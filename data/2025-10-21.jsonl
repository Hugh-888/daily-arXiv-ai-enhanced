{"id": "2510.16027", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.16027", "abs": "https://arxiv.org/abs/2510.16027", "authors": ["Youheng Zheng"], "title": "Quantum Classical Correspondence Using Coherent State Measurements and Husimi Q Probability Distributions", "comment": null, "summary": "We propose and simulate a protocol to evolve a quantum particle forward in\ntime such that its trajectory closely matches that of the particle's Newtonian\ncounterpart. Using short bursts of Schr\\\"odinger time-evolution interleaved\nwith positive operator-valued measurements (POVMs) in the coherent basis, we\ndemonstrate quantum-classical convergence for durations far beyond\nSchr\\\"odinger time-evolution alone. We examine the impact of the time between\nmeasurements $\\Delta t$ and the reduced Planck's constant $\\hbar$ on divergence\ntime. Results indicate that for appropriate values of $\\Delta t$, smaller\nvalues of $\\hbar$ lead to longer divergence times. This method suggests a\nelegant, intuitive bridge to recover classical motion from quantum postulates."}
{"id": "2510.16101", "categories": ["quant-ph", "hep-lat", "hep-ph"], "pdf": "https://arxiv.org/pdf/2510.16101", "abs": "https://arxiv.org/abs/2510.16101", "authors": ["Claudia Artiaco", "João Barata", "Enrique Rico"], "title": "Out-of-Equilibrium Dynamics in a U(1) Lattice Gauge Theory via Local Information Flows: Scattering and String Breaking", "comment": "32 pages, 17 figures", "summary": "We introduce local information flows as a diagnostic tool for characterizing\nout-of-equilibrium quantum dynamics in lattice gauge theories. We employ the\ninformation lattice framework, a local decomposition of total information into\nspatial- and scale-resolved contributions, to characterize the propagation and\nbuildup of quantum correlations in real-time processes. Focusing on the\nSchwinger model, a canonical $(1+1)$-dimensional U(1) lattice gauge theory, we\napply this framework to two scenarios. First, in the near-threshold scattering\nof two vector mesons, we demonstrate that the emergence of correlations at a\nlonger length scale in the information lattice marks the production of heavier\nscalar mesons. Second, in the dynamics of electric field strings, we clearly\ndistinguish between the confining regime, which evolves towards a steady state\nwith a static correlation profile, and the string-breaking sector. The latter\nis characterized by dynamic correlation patterns that reflect the sequential\nformation and annihilation of strings. This information-centric approach\nprovides a direct, quantitative, and interpretable visualization of complex\nmany-body phenomena, offering a promising tool for analyzing dynamics in\nhigher-dimensional gauge theories and experiments on quantum hardware."}
{"id": "2510.16117", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.16117", "abs": "https://arxiv.org/abs/2510.16117", "authors": ["Victor Gonzalez Avella", "Abraham Vega Vargas", "Tomas Merlo Vergara", "Kevin de la Ossa Doria", "Jakub Czartowski", "Dougal Main", "Gabriel Araneda", "Aldo Delgado", "Dardo Goyeneche"], "title": "Efficient state estimation on quantum processors", "comment": "20 pages, 14 fiegures", "summary": "We present two scalable and entanglement-free methods for estimating the\ncollective state of an n-qubit quantum computer. The first method consists of a\nfixed set of five quantum circuits-regardless of the number of qubits-that\navoid the use of entanglement as a measurement resource, relying instead on\nclassical communication between selected pairs of qubits. The second method\nrequires only 2n+1 circuits, each of which applies a single local gate to one\nof the n qubits during the measurement stage. Unlike traditional estimation\nmethods, our approaches do not require any costly post-processing procedure to\nestimate a quantum state, enabling scalability to relatively large system\nsizes. We experimentally compare both methods on freely available IBM quantum\nprocessors, and observe how the state estimation varies with increasing number\nof qubits and shots. We further validated our results by estimating the 4-qubit\nentangled state of two remote ion-trap quantum processors, demonstrating that\nthe optimized 2n+1 tomographic scheme achieves estimates consistent with\nstandard methods while using exponentially fewer measurements."}
{"id": "2510.16349", "categories": ["physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.16349", "abs": "https://arxiv.org/abs/2510.16349", "authors": ["Zekeriya Ender Eğer", "Waris Khan", "Priyabrata Maharana", "Kandula Eswara Sai Kumar", "Udbhav Sharma", "Abhishek Chopra", "Rut Lineswala", "Pınar Acar"], "title": "Design of Magnetic Lattices with Quantum Optimization Algorithms", "comment": "The following article has been submitted to APL Quantum. After it is\n  published, it will be found at\n  https://publishing.aip.org/resources/librarians/products/journals/", "summary": "This article investigates the identification of magnetic spin distributions\nin ferromagnetic materials by minimizing the system's free energy. Magnetic\nlattices of varying sizes are constructed, and the free energy is computed\nusing an Ising model that accounts for spin-to-spin neighbor interactions and\nthe influence of an external magnetic field. The problem reduces to determining\nthe state of each spin, either up or down, leading to an optimization problem\nwith $2^{n \\times n}$ design variables for an $n \\times n$ lattice. To address\nthe high-dimensional and computationally intractable nature of this problem,\nparticularly for large domains, we employ a quantum optimization algorithm,\nBQP. The BQP results are first validated against solutions obtained using a\ngenetic algorithm for smaller lattices. Finally, the approach is extended to\nlarge-scale systems, including $50 \\times 50$ lattices, where conventional\nmethods become impractical."}
{"id": "2510.16149", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.16149", "abs": "https://arxiv.org/abs/2510.16149", "authors": ["Alessandro Berti", "Francesco Ghisoni"], "title": "Efficient Quantum State Preparation with Bucket Brigade QRAM", "comment": null, "summary": "The preparation of data in quantum states is a critical component in the\ndesign of quantum algorithms. The cost of this step can significantly limit the\nrealization of quantum advantage in domains such as machine learning, finance,\nand chemistry. One of the main approaches to achieve efficient state\npreparation is through the use of Quantum Random Access Memory (QRAM), a\ntheoretical device for coherent data access with several proposed physical\nimplementations. In this work, we present a framework that integrates the\nphysical model of the Bucket Brigade QRAM (BBQRAM) with the classical data\nstructure of the Segment Tree to achieve efficient state preparation. We\nintroduce a memory layout that embeds a segment tree within BBQRAM memory cells\nby preserving the segment tree's hierarchy and supporting data retrieval in\nlogarithmic time via specialized access primitives. We demonstrate that, under\nthe proposed memory layout, our method encodes a matrix $A \\in \\mathbb{R}^{M\n\\times N}$ in a quantum register of $\\Theta(\\log_2(MN))$ qubits in\n$O(\\log_2^2(MN))$ time using constant ancillary qubits under a fixed-precision\nassumption. We further illustrate the method through a numerical example. This\nframework provides theoretical support for quantum algorithms that assume\nnegligible data loading overhead and establishes a foundation for designing\nclassical-to-quantum encoding algorithms that are aware of the underlying\nphysical QRAM architecture."}
{"id": "2510.16542", "categories": ["physics.comp-ph", "physics.chem-ph", "physics.plasm-ph"], "pdf": "https://arxiv.org/pdf/2510.16542", "abs": "https://arxiv.org/abs/2510.16542", "authors": ["Francois Mauger", "Cristel Chandre"], "title": "Extended phase-space symplectic integration for electron dynamics", "comment": "14 pages, 6 figures", "summary": "We investigate the use of extended phase-space symplectic integration [M.\nTao, Phys. Rev. E 94, 043303 (2016)] for simulating two different classes of\nelectron dynamics. The first one, with one and a half degrees of freedom, comes\nfrom plasma physics and describes the classical dynamics of a charged particle\nin a strong, constant, and uniform magnetic field perturbed by a turbulent\nelectrostatic potential. The second one, with an infinite number of degrees of\nfreedom, comes from physical chemistry and corresponds to Kohn-Sham\ntime-dependent density-functional theory. For both we lay out the extension\nprocedure and stability condition for numerical integration of the dynamics\nusing high-order symplectic split-operator schemes. We also identify a\ncomputationally inexpensive metric that can be used for on-the-fly estimation\nof the accuracy of simulations. Our work paves the way for broad application of\nsymplectic split-operator integration of classical and quantum Hamiltonian\nsystems with finite and infinite number of degrees of freedom."}
{"id": "2510.16061", "categories": ["gr-qc", "hep-th"], "pdf": "https://arxiv.org/pdf/2510.16061", "abs": "https://arxiv.org/abs/2510.16061", "authors": ["Yusmantoro Yusmantoro", "Freddy Permana Zen", "Muhammad Lawrence Pattersons"], "title": "Mass-radius relation, moment of inertia, and tidal love numbers of anisotropic neutron stars in f (R,T) gravity", "comment": "27 pages, 7 figures", "summary": "The mass-radius relation, moment of inertia, and tidal love numbers of\nanisotropic neutron stars (NSs) have been investigated in $f(R,T)$ gravity by\nimposing two equations of state (EoS). We use the simplest form\n$f(R,T)=R+2\\beta T$ model and adopt the anisotropy approach called Horvat\nmodel. To examine the viability of our calculations, we utilize the constraints\nfrom GW170817 and GW190814 observations. Moreover, we consider three values of\n$\\beta$, i.e. $\\beta=0$, $\\beta=-0.01$, $\\beta=-0.02$ and four anisotropy\nparameters $\\alpha$, i.e. $\\alpha=-0.12$, $\\alpha=-0.06$, $\\alpha=0.06$,\n$\\alpha=0.12$. Our findings suggest that all physical quantities depend on both\nparameters $\\alpha$ and $\\beta$. Nevertheless, the impact of $\\alpha$ is much\nmore significant than $\\beta$. The calculation of masses satisfy each used\nconstraints for specific values of $\\alpha$ and $\\beta$. In the case of the\nmoment of inertia, the results are compatible with the constraint obtained from\nradio observation of heavy pulsar. On the other hand, the tidal deformability\nof the NSs composed of one EoS satisfy the GW170817 constraint while the NSs\ncomposed of the other one EoS are too small. These small numbers can be\ninterpreted as the property of secondary object observed in GW190814. As a\nresult, our theoretical investigation of NSs constructed with two EoS can be\nNSs candidates for GW170817 and GW190814, respectively."}
{"id": "2510.15940", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15940", "abs": "https://arxiv.org/abs/2510.15940", "authors": ["Jialin Lu", "Kye Emond", "Kaiyu Yang", "Swarat Chaudhuri", "Weiran Sun", "Wuyang Chen"], "title": "Lean Finder: Semantic Search for Mathlib That Understands User Intents", "comment": null, "summary": "We present Lean Finder, a semantic search engine for Lean and mathlib that\nunderstands and aligns with the intents of mathematicians. Progress in formal\ntheorem proving is often hindered by the difficulty of locating relevant\ntheorems and the steep learning curve of the Lean 4 language, making\nadvancement slow and labor-intensive. Existing Lean search engines, though\nhelpful, rely primarily on informalizations (natural language translation of\nthe formal statements), while largely overlooking the mismatch with real-world\nuser queries. In contrast, we propose a user-centered semantic search tailored\nto the needs of mathematicians. Our approach begins by analyzing and clustering\nthe semantics of public Lean discussions, then fine-tuning text embeddings on\nsynthesized queries that emulate user intents. We further align Lean Finder\nwith mathematicians' preferences using diverse feedback signals, encoding it\nwith a rich awareness of their goals from multiple perspectives. Evaluations on\nreal-world queries, informalized statements, and proof states demonstrate that\nour Lean Finder achieves over $30\\%$ relative improvement compared to previous\nsearch engines and GPT-4o. In addition, Lean Finder is compatible with\nLLM-based theorem provers, bridging retrieval with formal reasoning. Lean\nFinder is available at: https://leanfinder.github.io"}
{"id": "2510.16155", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.16155", "abs": "https://arxiv.org/abs/2510.16155", "authors": ["Nathan Mclane", "LeAnh Duckett", "Leah G. Dodson"], "title": "Environment-imposed selection rules for nuclear-spin conversion of H$_2$ in molecular crystals", "comment": null, "summary": "Nuclear-spin conversion in molecular hydrogen is governed by strict symmetry\nrules that typically require magnetic fields or catalytic surfaces to break.\nHere we demonstrate that the intrinsic tensor composition of a non-magnetic\nmolecular crystal field can impose and relax these rules without external\nfields. High-resolution infrared spectra of H$_2$ in crystalline CO$_2$ reveal\nlarge rank-2 (quadrupolar) crystal-field splittings of the $m$ sublevels, while\nnuclear-spin conversion occurs only through $\\Delta m = 0$ channels. Replacing\nCO$_2$ with polar N$_2$O introduces rank-1 (dipole) components that partially\nopen $\\Delta m \\neq 0$ pathways, while incorporation of paramagnetic NO$_2$\nfully lifts the restriction. These results establish a direct correspondence\nbetween crystal-field tensor rank and nuclear-spin dynamics, introducing a\ngeneral symmetry-based framework for designing and controlling spin-isomer\npopulations and quantum-state connectivity in molecular solids."}
{"id": "2510.16721", "categories": ["physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.16721", "abs": "https://arxiv.org/abs/2510.16721", "authors": ["Qiuhan Jia", "Jiuyang Shi", "Jian Sun"], "title": "Scalable cell filter nudged elastic band (CFNEB) for large-scale transition-path calculations", "comment": null, "summary": "The nudged elastic band (NEB) method is one of the most widely used\ntechniques for determining minimum-energy reaction pathways and activation\nbarriers between known initial and final states. However, conventional\nimplementations face steep computational scaling with system size, which makes\nnucleation-type transitions in realistically large supercells practically\ninaccessible. In this work, we develop a scalable cell-filter nudged elastic\nband (CFNEB) framework that enables efficient transition-path calculations in\nsystems containing up to $10^5$ atoms. The method combines a deformation-based\ncell filtering scheme, which treats lattice vectors as generalized coordinates\nwhile removing spurious rotational degrees of freedom, with an adaptive image\ninsertion and deletion strategy that dynamically refines the reaction path. We\nimplement CFNEB both within the ASE environment and in a fully GPU-accelerated\nversion using the Graphics Processing Units Molecular Dynamics (GPUMD) engine,\nachieving throughput on the order of $10^6$ atom$\\cdot$steps per second on\nconsumer GPUs. We demonstrate the method on two representative systems: the\nlayer-by-layer $\\beta$-$\\lambda$ transition in $Ti_3O_5$ and the\nnucleation-driven graphite-to-diamond transformation. These examples illustrate\nthat CFNEB not only reproduces known concerted pathways but also captures\nspontaneous symmetry breaking toward nucleated mechanisms when the simulation\ncell is sufficiently large. Our results establish CFNEB as a practical route to\nexploring realistic transition mechanisms in large-scale solid-state systems."}
{"id": "2510.16090", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.16090", "abs": "https://arxiv.org/abs/2510.16090", "authors": ["Tuhina Ghorui", "Prabir Rudra", "Farook Rahaman"], "title": "Godel Universe in $f(Q,T)$ gravity: Exploring causality violation and closed time-like curves", "comment": "19 pages, 6 figures", "summary": "In this work, the classical Godel solution from general relativity is\nextended into the framework of modified gravity theories based on non-metricity\n$Q$ and the trace of the energy-momentum tensor $T$ in the context of $f(Q,T)$\ngravity. The main feature of the Godel solution is the existence of closed\ntime-like curves, which allow for causality violation and time travel. Since\ngeneral relativity and its extensions do not demand spacetime to be globally\ncausal, there is good motivation to explore such solutions. We have found\nclasses of solutions with different matter content, like perfect fluid,\ncosmological constant, massless scalar field, etc. It is observed that, for\nsuitable initial conditions, there is always a possibility of obtaining\nfeasible solutions that violate causality in our setup. The presence of\nnon-metricity in such solutions produces crucial deviations that are\nnoteworthy."}
{"id": "2510.15944", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15944", "abs": "https://arxiv.org/abs/2510.15944", "authors": ["Tianyu Bell Pan", "Mengdi Zhu", "Alexa Jordyn Cole", "Ronald Wilson", "Damon L. Woodard"], "title": "Lyapunov-Stable Adaptive Control for Multimodal Concept Drift", "comment": null, "summary": "Multimodal learning systems often struggle in non-stationary environments due\nto concept drift, where changing data distributions can degrade performance.\nModality-specific drifts and the lack of mechanisms for continuous, stable\nadaptation compound this challenge. This paper introduces LS-OGD, a novel\nadaptive control framework for robust multimodal learning in the presence of\nconcept drift. LS-OGD uses an online controller that dynamically adjusts the\nmodel's learning rate and the fusion weights between different data modalities\nin response to detected drift and evolving prediction errors. We prove that\nunder bounded drift conditions, the LS-OGD system's prediction error is\nuniformly ultimately bounded and converges to zero if the drift ceases.\nAdditionally, we demonstrate that the adaptive fusion strategy effectively\nisolates and mitigates the impact of severe modality-specific drift, thereby\nensuring system resilience and fault tolerance. These theoretical guarantees\nestablish a principled foundation for developing reliable and continuously\nadapting multimodal learning systems."}
{"id": "2510.16204", "categories": ["quant-ph", "cond-mat.dis-nn", "physics.optics"], "pdf": "https://arxiv.org/pdf/2510.16204", "abs": "https://arxiv.org/abs/2510.16204", "authors": ["Rajesh Asapanna", "Clément Hainaut", "Alberto Amo", "Álvaro Gómez-León"], "title": "Decoherence-free subspaces in the noisy dynamics of discrete-step quantum walks in a photonic lattice", "comment": "5 pages, 4 figures and Appendix", "summary": "We study the noisy dynamics of periodically driven, discrete-step quantum\nwalks in a one-dimensional photonic lattice. We find that in the bulk, temporal\nnoise that is constant within a Floquet period leads to decoherence-free\nmomentum subspaces, whereas fully random noise destroys coherence in a few\ntime-steps. When considering topological edge states, we observe decoherence no\nmatter the type of temporal noise. To explain these results, we derive a\nnon-perturbative master equation to describe the system's dynamics and\nexperimentally confirm our findings in a discrete mesh photonic lattice\nimplemented in a double-fibre ring setup. Surprisingly, our results show that a\nclass of bulk states can be more robust to a certain type of noise than\ntopological edge states."}
{"id": "2510.16723", "categories": ["physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.16723", "abs": "https://arxiv.org/abs/2510.16723", "authors": ["Hyeonbin Moon", "Hanbin Cho", "Wabi Demeke", "Byungki Ryu", "Seunghwa Ryu"], "title": "Thermal Conductivity Estimation of Thermoelectric Materials with Uncertainty Quantification Using Bayesian Physics-Informed Neural Networks", "comment": "36pages", "summary": "Characterizing the temperature-dependent thermal conductivity is challenging\nbecause the property varies strongly with temperature and reliable heat flow\nmeasurement, not just temperature sensing, is difficult under experimental\nconditions. Here, we present a physics informed deep learning framework that\ninfers conductivity solely from sparse electric potential measurements. We\nfirst develop a deterministic physics-informed neural network (PINN) that\nembeds coupled thermoelectric transport equations as soft constraints, enabling\nsimultaneous recovery of spatial temperature, voltage, and conductivity\nprofiles without temperature data. The deterministic PINN achieves accurate\ninference under noise-free conditions, yet its predictions degrade when\nmeasurement noise is introduced. To address this, we extend the framework to a\nBayesian PINN, which models network parameters probabilistically and employs\nHamiltonian Monte Carlo (HMC) sampling for posterior inference. This extension\nproduces robust thermal conductivity estimates and, importantly, provides\ncredible intervals that quantify uncertainty from sparse and noisy data.\nNumerical experiments confirm that the Bayesian PINN not only preserves\npredictive accuracy under noise but also reveals inference bias and enables\nuncertainty aware interpretation of material properties. Together, the\ndeterministic and Bayesian formulations establish a scalable and generalizable\nalternative to conventional methods for determining temperature-dependent\nproperties, offering physics- consistent and risk-aware property inference for\nthermoelectric systems and other functional materials where direct temperature\nsensing is impractical"}
{"id": "2510.16102", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.16102", "abs": "https://arxiv.org/abs/2510.16102", "authors": ["Michael LaHaye", "Colin Weller", "Dongjun Li", "Patrick Bourg", "Yanbei Chen", "Huan Yang"], "title": "Evolving extreme mass-ratio inspirals in a perturbed Schwarzschild spacetime", "comment": "31 pages, 1 figure", "summary": "In this work, we develop the modified Teukolsky formalism that describes the\nGW radiation from a point mass orbiting around a perturbed Schwarzschild BH.\nThis perturbation of the background spacetime induces a secular change in the\norbital phase of the point mass. In turn, this causes a modification in the GW\nflux, which can be used to probe the background spacetime. We explicitly apply\nthis formalism to a bumpy Schwarzschild spacetime as a proof of principle. The\nresults pave the way for the description of EMRIs in generic perturbed Kerr\nspacetime in future developments."}
{"id": "2510.15945", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15945", "abs": "https://arxiv.org/abs/2510.15945", "authors": ["Guangya Wan", "Zixin Stephen Xu", "Sasa Zorc", "Manel Baucells", "Mengxuan Hu", "Hao Wang", "Sheng Li"], "title": "BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling", "comment": "Under review on ARR", "summary": "Sampling multiple responses is a common way to improve LLM output quality,\nbut it comes at the cost of additional computation. The key challenge is\ndeciding when to stop generating new samples to balance accuracy gains against\nefficiency. To address this, we introduce BEACON (Bayesian Efficient Adaptive\nCriterion for Optimal N-stopping), a principled adaptive sampling framework\ngrounded in Sequential Search with Bayesian Learning. BEACON sequentially\ngenerates responses from the policy LLM, updates posterior belief over reward\ndistributions in real time without further training, and determines when to\nstop by weighing expected gains against computational cost. Sampling terminates\nonce the marginal utility of further exploration no longer justifies the\nexpense. We establish both theoretical optimality guarantees and practical\ntractability, and show empirically that BEACON reduces average sampling by up\nto 80% while maintaining response quality. We further demonstrate BEACON's\nutility for cost-efficient preference data generation and outline practical\nextensions, offering actionable insights for future researchers."}
{"id": "2510.16214", "categories": ["quant-ph", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2510.16214", "abs": "https://arxiv.org/abs/2510.16214", "authors": ["Sarah Chehade", "Andrea Delgado", "Elaine Wong"], "title": "Commuting Embeddings for Parallel Strategies in Non-local Games", "comment": "17 pages, 2 figures", "summary": "Non-local games (NLGs) provide a versatile framework for probing quantum\ncorrelations and for benchmarking the power of entanglement. In finite\ndimensions, the standard method for playing several games in parallel requires\na tensor product of the local Hilbert spaces, which scales additively in the\nnumber of qubits. In this work, we show that this additive cost can be reduced\nby exploiting algebraic embeddings. We introduce two forms of compressions.\nFirst, when a referee selects one game from a finite collection of games at\nrandom, the game quantum strategy can be implemented using a maximally\nentangled state of dimension equal to the largest individual game, thereby\neliminating the need for repeated state preparations. Second, we establish\nconditions under which several games can be played simultaneously in parallel\non fewer qubits than the tensor product baseline. These conditions are\nexpressed in terms of commuting embeddings of the game algebras. Moreover, we\nprovide a constructive framework for building such embeddings. Using tools from\nLie theory, we show that aligning the various game algebras into a common\nCartan decomposition enables such a qubit reduction. Beyond the theoretical\ncontribution, our framework casts NLGs as algebraic primitives for distributed\nand resource constrained quantum computations and suggested NLGs as a\ncomparable device independent dimension witness."}
{"id": "2510.17432", "categories": ["physics.comp-ph", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.17432", "abs": "https://arxiv.org/abs/2510.17432", "authors": ["Zhichang Fu", "Yunhai Li", "Weiqing Zhou", "Shengjun Yuan"], "title": "Large-scale stochastic propagation method beyond the sequential approach", "comment": null, "summary": "The $O(N)$ stochastic propagation method, which relies on the numerical\nsolution of the time-dependent Schr\\\"odinger equation using random initial\nstates, is widely used in large-scale first-principles calculations. In this\nwork, we eliminate the conventional sequential computation of intermediate\nstates by introducing a concurrent strategy that minimizes information\nredundancy. The new method, in its state-, moment-, and energy-based\nimplementations, not only surpasses the time step constraint of sequential\npropagation but also maintains precision within the framework of the\nNyquist-Shannon sampling theorem. Systematic benchmarking on one billion atoms\nwithin the tight-binding model demonstrates that our new concurrent method\nachieves up to an order-of-magnitude speedup, enabling the rapid computation of\na wide range of electronic, optical, and transport properties. This performance\nbreakthrough offers valuable insights for enhancing other time-propagation\nalgorithms, including those employed in large-scale stochastic density\nfunctional theory."}
{"id": "2510.16112", "categories": ["gr-qc", "astro-ph.HE"], "pdf": "https://arxiv.org/pdf/2510.16112", "abs": "https://arxiv.org/abs/2510.16112", "authors": ["Loïc Honet", "Josh Mathews", "Geoffrey Compère", "Adam Pound", "Barry Wardell", "Gabriel Andres Piovano", "Maarten van de Meent", "Niels Warburton"], "title": "Spin-aligned inspiral waveforms from self-force and post-Newtonian theory", "comment": "5 pages + references + 3 pages supplementary material ; 4 figures, 1\n  table", "summary": "We present the state-of-the-art waveform model WaSABI-C for quasicircular\ninspirals of spinning black hole binaries with aligned or anti-aligned spins.\nOur model synthesizes the most up-to-date first- and second-order gravitational\nself-force results with high-order post-Newtonian expansions through a\nsystematic hybridization procedure. This approach captures both strong-field\nand weak-field dynamics with high fidelity, enabling accurate modeling of\nspin-(anti)aligned inspirals across a wide parameter space. The resulting\nwaveforms mark a significant advance in the precision of self-force-based\ntemplates, providing critical input for the detection and interpretation of\ngravitational waves from compact binaries with future observatories such as\nLISA and ET. We accompany this work with the release of WaSABI (Waveform\nSimulations of Asymmetric Binary Inspirals), a public package implementing our\nmodel for community use and further development."}
{"id": "2510.15946", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15946", "abs": "https://arxiv.org/abs/2510.15946", "authors": ["Wenshuo Wang", "Ziyou Jiang", "Junjie Wang", "Mingyang Li", "Jie Huang", "Yuekai Huang", "Zhiyuan Chang", "Feiyan Duan", "Qing Wang"], "title": "Learning from Mistakes: Enhancing Harmful Meme Detection via Misjudgment Risk Patterns", "comment": "12 Pages, Submitted to WWW'26", "summary": "Internet memes have emerged as a popular multimodal medium, yet they are\nincreasingly weaponized to convey harmful opinions through subtle rhetorical\ndevices like irony and metaphor. Existing detection approaches, including\nMLLM-based techniques, struggle with these implicit expressions, leading to\nfrequent misjudgments. This paper introduces PatMD, a novel approach that\nimproves harmful meme detection by learning from and proactively mitigating\nthese potential misjudgment risks. Our core idea is to move beyond superficial\ncontent-level matching and instead identify the underlying misjudgment risk\npatterns, proactively guiding the MLLMs to avoid known misjudgment pitfalls. We\nfirst construct a knowledge base where each meme is deconstructed into a\nmisjudgment risk pattern explaining why it might be misjudged, either\noverlooking harmful undertones (false negative) or overinterpreting benign\ncontent (false positive). For a given target meme, PatMD retrieves relevant\npatterns and utilizes them to dynamically guide the MLLM's reasoning.\nExperiments on a benchmark of 6,626 memes across 5 harmful detection tasks show\nthat PatMD outperforms state-of-the-art baselines, achieving an average of\n8.30\\% improvement in F1-score and 7.71\\% improvement in accuracy,\ndemonstrating strong generalizability and improved detection capability of\nharmful memes."}
{"id": "2510.16301", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.16301", "abs": "https://arxiv.org/abs/2510.16301", "authors": ["Amena Khatun", "Muhammad Usman"], "title": "Adversarially Robust Quantum Transfer Learning", "comment": "This Book Chapter will publish in \"Quantum Robustness in Artificial\n  Intelligence\" Book by Springer and is currently in production. More\n  information about the Book is at:\n  https://link.springer.com/book/9783032111524?srsltid=AfmBOood7vZYc5xJYtLrQWND4pjedgfWAfAFFocjvnNS1lrNpVBwvJcO#accessibility-information", "summary": "Quantum machine learning (QML) has emerged as a promising area of research\nfor enhancing the performance of classical machine learning systems by\nleveraging quantum computational principles. However, practical deployment of\nQML remains limited due to current hardware constraints such as limited number\nof qubits and quantum noise. This chapter introduces a hybrid quantum-classical\narchitecture that combines the advantages of quantum computing with transfer\nlearning techniques to address high-resolution image classification.\nSpecifically, we propose a Quantum Transfer Learning (QTL) model that\nintegrates classical convolutional feature extraction with quantum variational\ncircuits. Through extensive simulations on diverse datasets including Ants \\&\nBees, CIFAR-10, and Road Sign Detection, we demonstrate that QTL achieves\nsuperior classification performance compared to both conventional and quantum\nmodels trained without transfer learning. Additionally, we also investigate the\nmodel's vulnerability to adversarial attacks and demonstrate that incorporating\nadversarial training significantly boosts the robustness of QTL, enhancing its\npotential for deployment in security sensitive applications."}
{"id": "2510.17507", "categories": ["physics.comp-ph", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2510.17507", "abs": "https://arxiv.org/abs/2510.17507", "authors": ["Karol Pietrak", "Radosław Muszyński", "Adam Marek", "Piotr Łapka"], "title": "Performance of artificial neural networks in an inverse problem of laser beam diagnostics", "comment": "20 pages, 12 figures, 1 table, 61 references", "summary": "Results are presented for the numerical verification of a method devised to\nidentify an unknown spatio-temporal distribution of heat flux that occurs at\nthe surface of thin aluminum plate, as a result of pulsed, high-power laser\nbeam excitation. The presented identification of boundary heat flux function is\na part of newly-proposed laser beam profiling method and utilizes artificial\nneural networks trained on temperature distributions generated with the ANSYS\nFluent solver. The paper focuses on the selection of the most effective neural\nnetwork hyperparameters (Keras, Tensorflow) and compares the results of neural\nnetwork identification with Levenberg-Marquardt method used earlier and\ndiscussed in our previous articles."}
{"id": "2510.16113", "categories": ["gr-qc", "astro-ph.HE"], "pdf": "https://arxiv.org/pdf/2510.16113", "abs": "https://arxiv.org/abs/2510.16113", "authors": ["Josh Mathews", "Barry Wardell", "Adam Pound", "Niels Warburton"], "title": "Post-adiabatic self-force waveforms: slowly spinning primary and precessing secondary", "comment": "36 pages, 11 Figures", "summary": "Recent progress in gravitational self-force theory has led to the development\nof a first post-adiabatic (1PA) waveform model for nonspinning, quasicircular\ncompact binaries [Phys. Rev. Lett. 130, 241402 (2023)]. In this paper, we\nextend that model to allow for a slowly spinning primary black hole and a\ngeneric, precessing spin on the secondary object, restricting to the case of\nsmall misalignment between the primary spin and the orbital angular momentum.\nWe demonstrate excellent agreement between our waveforms and fully nonlinear\nnumerical relativity simulations for mass ratios $q\\gtrsim 5$ and primary spins\n$|\\chi_1|\\lesssim 0.1$ and arbitrary secondary spin $\\chi_2 \\lesssim 1$. In\nparticular we present the re-summed 1PAT1R waveform model, which significantly\nimproves the accuracy of the original 1PAT1 waveforms for comparable masses and\nincreasing primary spin. Our models are publicly available in the WaSABI\npackage."}
{"id": "2510.15947", "categories": ["cs.LG", "cs.AI", "eess.SP", "q-bio.NC", "68T07, 92C55, 62M10", "I.2.6; I.5.1; J.3"], "pdf": "https://arxiv.org/pdf/2510.15947", "abs": "https://arxiv.org/abs/2510.15947", "authors": ["Casper van Laar", "Khubaib Ahmed"], "title": "WaveNet's Precision in EEG Classification", "comment": "6 pages, 5 figures and 3 tables. Includes main text and bibliography", "summary": "This study introduces a WaveNet-based deep learning model designed to\nautomate the classification of EEG signals into physiological, pathological,\nartifact, and noise categories. Traditional methods for EEG signal\nclassification, which rely on expert visual review, are becoming increasingly\nimpractical due to the growing complexity and volume of EEG recordings.\nLeveraging a publicly available annotated dataset from Mayo Clinic and St.\nAnne's University Hospital, the WaveNet model was trained, validated, and\ntested on 209,232 samples with a 70/20/10 percent split. The model achieved a\nclassification accuracy exceeding previous CNN and LSTM-based approaches, and\nwas benchmarked against a Temporal Convolutional Network (TCN) baseline.\nNotably, the model distinguishes noise and artifacts with high precision,\nalthough it reveals a modest but explainable degree of misclassification\nbetween physiological and pathological signals, reflecting inherent clinical\noverlap. WaveNet's architecture, originally developed for raw audio synthesis,\nis well suited for EEG data due to its use of dilated causal convolutions and\nresidual connections, enabling it to capture both fine-grained and long-range\ntemporal dependencies. The research also details the preprocessing pipeline,\nincluding dynamic dataset partitioning and normalization steps that support\nmodel generalization."}
{"id": "2510.16305", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.16305", "abs": "https://arxiv.org/abs/2510.16305", "authors": ["Chaojie Wang", "Xutong Li", "Xiuyi Ma", "Yuning Zhang", "Meng Wu", "Weifang Lu", "Yuanyuan Chen", "Xiubao Sui", "Lixiang Chen"], "title": "Dynamical control of quantum photon-photon interaction with phase change material", "comment": "23 pages,15 figures", "summary": "Quantum interference can produce a pivotal effective photon-photon\ninteraction, enabling the exploration of various quantum information\ntechnologies that beyond the possibilities of classical physics. While such an\neffective interaction is fundamentally limited to the bosonic nature of photons\nand the restricted phase responses from commonly used unitary optical elements,\nloss-induced nonunitary operation provides an alternative degree of freedom to\ncontrol the quantum interference. Here, we propose and experimentally\ndemonstrate a concise yet powerful tool to unravel fundamental features of\nquantum interference based on the phase change material vanadium dioxide. Since\nthe insulator-metal transition in an elaborate vanadium dioxide thin film can\ncreate any desired particle exchange phase response, we show its tunability\nover the effective photon-photon interaction between paired photons that are\nentangled in the symmetric and anti-symmetric forms, which may introduce\nsophisticated nonunitary operations and functionalities into programmable\noptical platforms. These results provide an alternative approach to investigate\nthe quantum light-matter interaction, and facilitate the use of quantum\ninterference for various quantum information processing tasks such as quantum\nsimulation and quantum computation."}
{"id": "2510.16816", "categories": ["cs.LG", "cs.AI", "math-ph", "math.MP", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.16816", "abs": "https://arxiv.org/abs/2510.16816", "authors": ["Ming Zhong", "Zhenya Yan"], "title": "Efficient High-Accuracy PDEs Solver with the Linear Attention Neural Operator", "comment": "31 pages, 8 figures", "summary": "Neural operators offer a powerful data-driven framework for learning mappings\nbetween function spaces, in which the transformer-based neural operator\narchitecture faces a fundamental scalability-accuracy trade-off: softmax\nattention provides excellent fidelity but incurs quadratic complexity\n$\\mathcal{O}(N^2 d)$ in the number of mesh points $N$ and hidden dimension $d$,\nwhile linear attention variants reduce cost to $\\mathcal{O}(N d^2)$ but often\nsuffer significant accuracy degradation. To address the aforementioned\nchallenge, in this paper, we present a novel type of neural operators, Linear\nAttention Neural Operator (LANO), which achieves both scalability and high\naccuracy by reformulating attention through an agent-based mechanism. LANO\nresolves this dilemma by introducing a compact set of $M$ agent tokens $(M \\ll\nN)$ that mediate global interactions among $N$ tokens. This agent attention\nmechanism yields an operator layer with linear complexity $\\mathcal{O}(MN d)$\nwhile preserving the expressive power of softmax attention. Theoretically, we\ndemonstrate the universal approximation property, thereby demonstrating\nimproved conditioning and stability properties. Empirically, LANO surpasses\ncurrent state-of-the-art neural PDE solvers, including Transolver with\nslice-based softmax attention, achieving average $19.5\\%$ accuracy improvement\nacross standard benchmarks. By bridging the gap between linear complexity and\nsoftmax-level performance, LANO establishes a scalable, high-accuracy\nfoundation for scientific machine learning applications."}
{"id": "2510.16114", "categories": ["gr-qc", "astro-ph.HE"], "pdf": "https://arxiv.org/pdf/2510.16114", "abs": "https://arxiv.org/abs/2510.16114", "authors": ["Loïc Honet", "Adam Pound", "Geoffrey Compère"], "title": "Hybrid waveform model for asymmetric spinning binaries: Self-force meets post-Newtonian theory", "comment": "This paper is the second out of three from our series of spin papers.\n  43 pages; 14 figures; 1 table", "summary": "We develop and implement a new hybrid waveform model for quasicircular\ninspirals with a spinning primary and nonspinning secondary, excluding the\nmerger and ringdown. This model, which is a core component of the more\nextensive WaSABI-C model, consistently assembles all available first-order\nself-force and post-Newtonian results through a hybridization procedure without\nany tuning to numerical relativity, making it particularly suited for\nintermediate to extreme mass ratios. For almost all masses and primary spins,\nthe resulting hybrid model significantly improves the faithfulness of both\npost-Newtonian and adiabatic self-force waveforms considered separately. We\nprovide detailed comparisons with 50 simulations from the SXS catalog with mass\nratios ranging from 1 to 15 and primary spins ranging from -0.8 to 0.8. The\nhybrid model improves the median mismatch against numerical relativity\nwaveforms by a factor of 2000 with respect to adiabatic waveforms and 40 with\nrespect to post-Newtonian waveforms. The mismatches are comparable to those\nobtained from the SEOBNRv5EHM model in the quasicircular limit over most of the\nparameter space covered by NR simulations."}
{"id": "2510.15950", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15950", "abs": "https://arxiv.org/abs/2510.15950", "authors": ["Arianna Francesconi", "Donato Cappetta", "Fabio Rebecchi", "Paolo Soda", "Valerio Guarrasi", "Rosa Sicilia"], "title": "Cross-dataset Multivariate Time-series Model for Parkinson's Diagnosis via Keyboard Dynamics", "comment": "Proceedings of the Workshop on Artificial Intelligence for Biomedical\n  Data (AIBio 2025), 28th European Conference on Artificial Intelligence 2025,\n  Springer CCIS", "summary": "Parkinson's disease (PD) presents a growing global challenge, affecting over\n10 million individuals, with prevalence expected to double by 2040. Early\ndiagnosis remains difficult due to the late emergence of motor symptoms and\nlimitations of traditional clinical assessments. In this study, we propose a\nnovel pipeline that leverages keystroke dynamics as a non-invasive and scalable\nbiomarker for remote PD screening and telemonitoring. Our methodology involves\nthree main stages: (i) preprocessing of data from four distinct datasets,\nextracting four temporal signals and addressing class imbalance through the\ncomparison of three methods; (ii) pre-training eight state-of-the-art\ndeep-learning architectures on the two largest datasets, optimizing temporal\nwindowing, stride, and other hyperparameters; (iii) fine-tuning on an\nintermediate-sized dataset and performing external validation on a fourth,\nindependent cohort. Our results demonstrate that hybrid convolutional-recurrent\nand transformer-based models achieve strong external validation performance,\nwith AUC-ROC scores exceeding 90% and F1-Score over 70%. Notably, a temporal\nconvolutional model attains an AUC-ROC of 91.14% in external validation,\noutperforming existing methods that rely solely on internal validation. These\nfindings underscore the potential of keystroke dynamics as a reliable digital\nbiomarker for PD, offering a promising avenue for early detection and\ncontinuous monitoring."}
{"id": "2510.16318", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.16318", "abs": "https://arxiv.org/abs/2510.16318", "authors": ["Shaojiang Zhu", "Xinyuan You", "Alexander Romanenko", "Anna Grassellino"], "title": "Coherence-Mediated Quantum Thermometry in a Hybrid Circuit-QED Architecture", "comment": null, "summary": "Quantum thermometry plays a critical role in the development of\nlow-temperature sensors and quantum information platforms. In this work, we\npropose and theoretically analyze a hybrid circuit quantum electrodynamics\narchitecture in which a superconducting qubit is dispersively coupled to two\ndistinct bosonic modes: one initialized in a weak coherent state and the other\ncoupled to a thermal environment. We show that the qubit serves as a sensitive\nreadout of the probe mode, mapping the interference between thermal and\ncoherent photon-number fluctuations onto measurable dephasing. This mechanism\nenables enhanced sensitivity to sub-millikelvin thermal energy fluctuations\nthrough Ramsey interferometry. We derive analytic expressions for the qubit\ncoherence envelope, compute the quantum Fisher information for temperature\nestimation, and demonstrate numerically that the presence of a coherent\nreference amplifies the qubit's sensitivity to small changes in thermal photon\noccupancy. Our results establish a new paradigm for quantum-enhanced\nthermometry and provide a scalable platform for future calorimetric sensing in\nhigh-energy physics and quantum metrology."}
{"id": "2510.17490", "categories": ["quant-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.17490", "abs": "https://arxiv.org/abs/2510.17490", "authors": ["Huan-Chen Shi", "Er-Liang Cui", "Dan Zhou"], "title": "Toward Autonomous Neural VMC: An Energy-Variance Convergence Criterion for Quantum Systems", "comment": "32 pages, 14 figures and 5 tables, suggestions and comments are\n  welcome", "summary": "The optimization of neural wave functions in variational Monte Carlo(VMC)\ncrucially relies on a robust convergence criterion. While the energy variance\nis theoretically a definitive measure of an eigenstate, its systematic\napplication as a primary, practical convergence criterion in neural-network VMC\nhas been underexplored. In this work, we propose and validate the energy\nvariance as a universal, quantitative criterion for convergence. Then its\nreliability is demonstrated across diverse quantum systems-from harmonic\noscillators and hydrogen atoms to charmonium hadrons-showing that a variance\nbelow 1*10^{-3} guarantees relative errors under 1%. This empirical threshold\nprovides a system-agnostic benchmark for convergence, enabling hands-off\noperation of the optimization process. We implement this criterion within a\nlightweight neural solver, thereby enabling automated parameter scans. Its\nutility is showcased by efficiently mapping ground-state properties of a 2D\ndouble-well potential, a hydrogen atom in a magnetic field, and a three-body\nquantum dot. Our work positions the energy-variance criterion as a robust and\nscalable tool that significantly accelerates the preliminary physical\nverification of quantum Hamiltonians."}
{"id": "2510.16228", "categories": ["gr-qc", "astro-ph.CO", "hep-th"], "pdf": "https://arxiv.org/pdf/2510.16228", "abs": "https://arxiv.org/abs/2510.16228", "authors": ["G. G. Luciano", "A. Paliathanasis", "A. Sheykhi"], "title": "Observational constraints on the modified cosmology inspired by string T-duality", "comment": "10 pages, 2 figures, 2 tables. Comments are welcome", "summary": "We explore the cosmological consequences of a modified cosmology inspired by\nstring T-duality. We incorporate the zero-point length correction, $l_0$, into\nthe gravitational potential and derive the modified Friedmann equations via\nthermodynamic approach at the apparent horizon of a Friedmann-Robertson-Walker\n(FRW) universe. The resulting framework introduces a dimensionless coupling\nparameter $\\beta\\sim l_0^2H_0^2$ quantifying deviations from the standard\n$\\Lambda$CDM model. Using Bayesian inference with \\textsc{Cobaya} and MCMC\nsampling, we constrain the model parameter against late-time observations,\nincluding PantheonPlus and Union3 Type~Ia supernovae, cosmic chronometers,\nDESI~DR2 BAO measurements, and Amati-calibrated GRBs. The joint analysis yields\nan upper bound $\\beta \\lesssim \\mathcal{O}(10^{-3})$ (68\\% C.L.), implying that\ndepartures from $\\Lambda$CDM are extremely small within current precision.\nModel comparison through the Akaike Information Criterion shows that the\n$\\Lambda$CDM and T-duality models provide statistically equivalent fits to the\ndata, exhibiting only a marginal preference for $\\Lambda$CDM. These results\nprovide the first quantitative observational constraints on string T-duality\ninspired modified cosmology and underscore the potential of future\nhigh-precision surveys to test quantum-gravity induced corrections in a\nlate-time universe."}
{"id": "2510.15954", "categories": ["cs.LG", "cs.CE", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.15954", "abs": "https://arxiv.org/abs/2510.15954", "authors": ["Hongzheng Shi", "Yuhang Wang", "Xiao Liu"], "title": "Fire-EnSF: Wildfire Spread Data Assimilation using Ensemble Score Filter", "comment": null, "summary": "As wildfires become increasingly destructive and expensive to control,\neffective management of active wildfires requires accurate, real-time fire\nspread predictions. To enhance the forecasting accuracy of active fires, data\nassimilation plays a vital role by integrating observations (such as\nremote-sensing data) and fire predictions generated from numerical models. This\npaper provides a comprehensive investigation on the application of a recently\nproposed diffusion-model-based filtering algorithm -- the Ensemble Score Filter\n(EnSF) -- to the data assimilation problem for real-time active wildfire spread\npredictions. Leveraging a score-based generative diffusion model, EnSF has been\nshown to have superior accuracy for high-dimensional nonlinear filtering\nproblems, making it an ideal candidate for the filtering problems of wildfire\nspread models. Technical details are provided, and our numerical investigations\ndemonstrate that EnSF provides superior accuracy, stability, and computational\nefficiency, establishing it as a robust and practical method for wildfire data\nassimilation. Our code has been made publicly available."}
{"id": "2510.16329", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.16329", "abs": "https://arxiv.org/abs/2510.16329", "authors": ["Jian-Jian Cheng", "Jun-Ling Che", "Lin Zhang", "Ming-Liang Hu"], "title": "The Quantum Origin of Diffraction from Bright and Dark States", "comment": "1 figure", "summary": "Diffraction, a cornerstone of wave optics, is reinterpreted through bright\nand dark collective states. In the continuous-mode framework, the diffraction\npattern arises from projection onto a single bright mode, while dark-region\nphotons populate orthogonal dark modes. Unlike the classical view of\ndestructive interference as field cancellation, the quantum description shows\nphotons persisting in detector-uncoupled states. Our approach thus resolves a\nkey limitation of Glauber's theory by identifying the detectable and\nundetectable modes, offering a complete particle-based explanation for\ndiffraction."}
{"id": "2510.17569", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.17569", "abs": "https://arxiv.org/abs/2510.17569", "authors": ["Jyler Menard", "R. A. Mansbach"], "title": "Semi-supervised Latent Bayesian Optimization for Designing Antimicrobial Peptides", "comment": "19 pages, 9 figures", "summary": "Antimicrobial peptides (AMPs) are a promising class of therapeutics to treat\nbacterial infections. Discovering and designing such peptides is difficult\nbecause of the vast number of possible sequences of amino acids. Deep\ngenerative models, such as variational autoencoders, have shown value in\npeptide design due to their ability to model sequence space with a\ncontinuous-valued latent space. Although such models have already been used to\ngreat effect in biomolecular design, they still suffer from a lack of\ninterpretability and rigorous quantification of latent space quality as a\nsearch space. We investigate (1) whether further compression of the design\nspace via dimensionality reduction may facilitate optimization, (2) the\ninterpretability of the spaces, and (3) how organizing latent spaces with\nphysicochemical properties may improve the efficiency of optimizing\nantimicrobial activity. We find that further reduction of the latent space via\ndimensionality reduction can be advantageous when organizing the space with\nmore relevant information at data availability, that using the dimensionality\nreduction search space can be more interpretable, and that we can organize the\nlatent space with different physicochemical properties even at different\npercentages of available labels."}
{"id": "2510.16260", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.16260", "abs": "https://arxiv.org/abs/2510.16260", "authors": ["Faizuddin Ahmed", "Edilberto O. Silva"], "title": "Schwarzschild Black Hole Coupled with a Cloud of Strings Immersed in King Dark Matter Halo", "comment": "20 pages, 18 figures, 1 table", "summary": "In this paper, we examine the geodesic and thermodynamic properties of a\nSchwarzschild black hole with a cloud of strings (known as the Letelier black\nhole) immersed in a King dark matter (KDM) halo under an isotropic\nconfiguration. The dynamics of both photons and massive particles are analyzed\nin detail using the effective potential formalism, including particle\ntrajectories, the photon sphere, black hole shadow, and the innermost stable\ncircular orbits (ISCOs). Particular emphasis is placed on how the presence of\nthe KDM halo modifies these geometric and dynamical features. Furthermore, we\nexplore the topological characteristics of photon rings by constructing a\nnormalized vector field, following Duan\\textquotesingle{}s topological current\n$\\phi$-mapping theory, and demonstrate how this field is influenced by both the\nstring cloud and the KDM halo. In the thermodynamic context, we analyze the\nimpact of the KDM halo and the string cloud on the Hawking temperature, Gibbs\nfree energy, thermal stability, and phase transitions of the black hole.\nFinally, we examine the thermodynamic topology of the system using a\ntheoretical framework that incorporates a generalized Helmholtz free energy and\ntopological current theory. In both the photon sphere and thermodynamic\nanalyses, we show that the string cloud parameter shifts the location of the\nzero point of the vector field in the equatorial plane. Specifically, in the\nphoton sphere case, the radius of the photon sphere increases with increasing\nvalues of the string cloud parameter, while in the thermodynamic topology, the\nhorizon radius decreases as the string cloud parameter increases."}
{"id": "2510.15955", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15955", "abs": "https://arxiv.org/abs/2510.15955", "authors": ["Kiran Kate", "Yara Rizk", "Poulami Ghosh", "Ashu Gulati", "Tathagata Chakraborti", "Zidane Wright", "Mayank Agarwal"], "title": "How Good Are LLMs at Processing Tool Outputs?", "comment": null, "summary": "Most realistic task automation problems require large language models (LLMs)\nto call tools, which often return complex JSON responses. These responses must\nbe further processed to derive the information necessary for task completion.\nThe ability of LLMs to do so is under-studied. In this paper, we study the tool\nresponse processing task and LLMs' abilities to process structured (JSON)\nresponses. We created a dataset for this task, and evaluated 15 open and closed\nweight models using multiple prompting approaches. Our results show that JSON\nprocessing remains a difficult task even for frontier models across multiple\nprompting strategies. The optimal response processing strategy depends on both\nthe nature and size of the tool outputs, as well as the complexity of the\nrequired reasoning. Variations in processing approaches can lead to performance\ndifferences ranging from 3\\% to 50\\%."}
{"id": "2510.16358", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2510.16358", "abs": "https://arxiv.org/abs/2510.16358", "authors": ["Andrei Piryatinski", "Nishaant Jacobus", "Sameer Dambal", "Eric R. Bittner", "Yu Zhang", "Ajay Ram Srimath Kandada"], "title": "Scattering theory of frequency-entangled biphoton states facilitated by cavity polaritons", "comment": "14 pages, 7 figures", "summary": "The use of quantum light to probe exciton properties in semiconductor and\nmolecular nanostructures typically occurs in the low-intensity regime. A\nsubstantial enhancement of exciton-photon coupling can be achieved with\nphotonic cavities, where excitons hybridize with cavity modes to form polariton\nstates. To provide a theoretical framework for interpreting experimental\nefforts in this direction, we develop a scattering theory describing the\ninteraction of frequency-entangled photon pairs with cavity polariton and\nbipolariton states under various coupling regime. Employing the Tavis-Cummings\nmodel in combination with our scattering approach, we present a quantitative\nanalysis of how polariton/bipolariton interaction with the entangled photon\npair modifies its joint spectral amplitude (JSA). Specifically, we examine the\neffects of the cavity-mode steady-state population, exciton-cavity coupling\nstrength, and different forms of the input photon JSA. Our results show that\nthe entanglement entropy of the scattered photons is highly sensitive to the\ninterplay between the input JSA and the spectral line shapes of the polariton\nresonances, emphasizing the cavity filtering effects. We argue that biphoton\nscattering quantum light spectroscopy best serves as a sensitive probe of\npolariton and bipolariton states in the photon-vacuum cavity steady state."}
{"id": "2510.16300", "categories": ["gr-qc", "hep-th"], "pdf": "https://arxiv.org/pdf/2510.16300", "abs": "https://arxiv.org/abs/2510.16300", "authors": ["Paul R. Anderson", "Amanda Peake", "Shohreh Gholizadeh Siahmazgi"], "title": "Vacuum polarization and stress-energy of a quantum field inside of two-dimensional black holes", "comment": "27 pages, 7 figures", "summary": "Quantum effects are studied in both Schwarzschild spacetime and a spacetime\nin which a null shell collapses to form a black hole via the vacuum\npolarization $\\langle \\phi^2 \\rangle$ and stress-energy tensor $\\langle T_{ab}\n\\rangle$ for a massless minimally-coupled scalar field in two dimensions. For\nSchwarzschild spacetime, the Boulware, Unruh, and Hartle-Hawking states are\nconsidered. For the collapsing null shell spacetime, the \\textit{in} vacuum\nstate is used. Instabilities of the Unruh, Hartle-Hawking, and \\textit{in}\nstates resulting from the behavior of $\\langle \\phi^2 \\rangle$ in the regions\ninside and outside of the horizon are found. The question of how well the Unruh\nstate for the eternal black hole approximates quantum effects in the interior\nof a black hole that forms from collapse is addressed."}
{"id": "2510.15960", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.15960", "abs": "https://arxiv.org/abs/2510.15960", "authors": ["Sana Kordoghli", "Abdelhakim Settar", "Oumayma Belaati", "Mohammad Alkhatib"], "title": "Hydrogen production from blended waste biomass: pyrolysis, thermodynamic-kinetic analysis and AI-based modelling", "comment": "41 pages, 21 figures", "summary": "This work contributes to advancing sustainable energy and waste management\nstrategies by investigating the thermochemical conversion of food-based biomass\nthrough pyrolysis, highlighting the role of artificial intelligence (AI) in\nenhancing process modelling accuracy and optimization efficiency. The main\nobjective is to explore the potential of underutilized biomass resources, such\nas spent coffee grounds (SCG) and date seeds (DS), for sustainable hydrogen\nproduction. Specifically, it aims to optimize the pyrolysis process while\nevaluating the performance of these resources both individually and as blends.\nProximate, ultimate, fibre, TGA/DTG, kinetic, thermodynamic, and Py-Micro GC\nanalyses were conducted for pure DS, SCG, and blends (75% DS - 25% SCG, 50% DS\n- 50% SCG, 25% DS - 75% SCG). Blend 3 offered superior hydrogen yield potential\nbut had the highest activation energy (Ea: 313.24 kJ/mol), while Blend 1\nexhibited the best activation energy value (Ea: 161.75 kJ/mol). The kinetic\nmodelling based on isoconversional methods (KAS, FWO, Friedman) identified KAS\nas the most accurate. These approaches provide a detailed understanding of the\npyrolysis process, with particular emphasis on the integration of artificial\nintelligence. An LSTM model trained with lignocellulosic data predicted TGA\ncurves with exceptional accuracy (R^2: 0.9996-0.9998)."}
{"id": "2510.16401", "categories": ["quant-ph", "cond-mat.str-el", "hep-th"], "pdf": "https://arxiv.org/pdf/2510.16401", "abs": "https://arxiv.org/abs/2510.16401", "authors": ["Ning Sun", "Peng Zhang", "Pengfei Zhang"], "title": "Hybrid Brownian SYK-Hubbard Model: from Spectral Function to Quantum Chaos", "comment": "16 pages, 4 figures", "summary": "Understanding the emergence of complex correlations in strongly interacting\nsystems remains a fundamental challenge in quantum many-body physics. One\nfruitful approach is to develop solvable toy models that encapsulate universal\nproperties shared by realistic systems. In this work, we introduce the Brownian\nSYK-Hubbard model, which combines the all-to-all random interactions of the\nSachdev-Ye-Kitaev (SYK) model with on-site Hubbard-type interactions. This\nhybrid construction enables the study of the interplay between nonlocal random\ndynamics and local correlation effects: (1) As the interaction strength\nincreases, the single-particle spectrum exhibits a transition from a single\npeak to a two-peak structure, signaling the onset of Mottness. (2) The spectral\nform factor undergoes a sequence of dynamical transitions as the evolution time\nincreases before reaching the plateau in the long-time limit under strong\nHubbard interactions. (3) The out-of-time-order correlator is computed by\nsumming a series of modified ladder diagrams, which determines the quantum\nLyapunov exponent and reveals a violation of the bound on branching time. Our\nresults establish a new analytically tractable platform for exploring the\neffects of Hubbard interactions in chaotic many-body systems."}
{"id": "2510.16315", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.16315", "abs": "https://arxiv.org/abs/2510.16315", "authors": ["Uktamjon Uktamov", "Mohsen Fathi", "Javlon Rayimbaev"], "title": "Charged particle bound orbits around magnetized Schwarzschild black holes: S2 star and hotspot applications", "comment": "6 pages, 5 figures", "summary": "The dynamics of charged particles around magnetized black holes provide\nvaluable insights into astrophysical processes near compact objects. In this\nwork, we investigate the bound and unbound trajectories of charged particles in\nthe vicinity of a Schwarzschild black hole immersed in an external, uniform\nmagnetic field. By analyzing the effective potential and solving the\ncorresponding equations of motion, we classify the possible orbital\nconfigurations and identify the critical parameters governing the transition\nbetween stable and escape trajectories. The influence of the magnetic field\nstrength and particle charge on the orbital structure, energy, and angular\nmomentum is systematically explored. Applications of the obtained results are\ndiscussed in the context of the S2 star orbiting Sagittarius A* and the motion\nof bright hotspots detected near the event horizon, offering a potential\ninterpretation of recent observations in terms of magnetized dynamics. The\nstudy contributes to a deeper understanding of charged-particle motion around\nblack holes and its relevance to high-energy astrophysical phenomena in the\ngalactic center."}
{"id": "2510.15961", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.15961", "abs": "https://arxiv.org/abs/2510.15961", "authors": ["Yiyang Li", "Zehong Wang", "Zhengqing Yuan", "Zheyuan Zhang", "Keerthiram Murugesan", "Chuxu Zhang", "Yanfang Ye"], "title": "Interpretable Graph-Language Modeling for Detecting Youth Illicit Drug Use", "comment": null, "summary": "Illicit drug use among teenagers and young adults (TYAs) remains a pressing\npublic health concern, with rising prevalence and long-term impacts on health\nand well-being. To detect illicit drug use among TYAs, researchers analyze\nlarge-scale surveys such as the Youth Risk Behavior Survey (YRBS) and the\nNational Survey on Drug Use and Health (NSDUH), which preserve rich\ndemographic, psychological, and environmental factors related to substance use.\nHowever, existing modeling methods treat survey variables independently,\noverlooking latent and interconnected structures among them. To address this\nlimitation, we propose LAMI (LAtent relation Mining with bi-modal\nInterpretability), a novel joint graph-language modeling framework for\ndetecting illicit drug use and interpreting behavioral risk factors among TYAs.\nLAMI represents individual responses as relational graphs, learns latent\nconnections through a specialized graph structure learning layer, and\nintegrates a large language model to generate natural language explanations\ngrounded in both graph structures and survey semantics. Experiments on the YRBS\nand NSDUH datasets show that LAMI outperforms competitive baselines in\npredictive accuracy. Interpretability analyses further demonstrate that LAMI\nreveals meaningful behavioral substructures and psychosocial pathways, such as\nfamily dynamics, peer influence, and school-related distress, that align with\nestablished risk factors for substance use."}
{"id": "2510.16420", "categories": ["quant-ph", "cs.CC"], "pdf": "https://arxiv.org/pdf/2510.16420", "abs": "https://arxiv.org/abs/2510.16420", "authors": ["Adam Husted Kjelstrøm", "Andreas Pavlogiannis", "Jaco van de Pol"], "title": "Exact Quantum Circuit Optimization is co-NQP-hard", "comment": "11 pages, 3 figures", "summary": "As quantum computing resources remain scarce and error rates high, minimizing\nthe resource consumption of quantum circuits is essential for achieving\npractical quantum advantage. Here we consider the natural problem of, given a\ncircuit $C$, computing an equivalent circuit $C'$ that minimizes a quantum\nresource type, expressed as the count or depth of (i) arbitrary gates, or (ii)\nnon-Clifford gates, or (iii) superposition gates, or (iv) entanglement gates.\nWe show that, when $C$ is expressed over any gate set that can implement the H\nand TOF gates exactly, each of the above optimization problems is hard for\n$\\text{co-NQP}$, and hence outside the Polynomial Hierarchy, unless the\nPolynomial Hierarchy collapses. This strengthens recent results in the\nliterature which established an $\\text{NP}$-hardness lower bound, and tightens\nthe gap to the corresponding $\\text{NP}^\\text{NQP}$ upper bound known for cases\n(i)-(iii) over Clifford+T and (i)-(iv) over H+TOF circuits."}
{"id": "2510.16460", "categories": ["gr-qc", "astro-ph.IM", "hep-th"], "pdf": "https://arxiv.org/pdf/2510.16460", "abs": "https://arxiv.org/abs/2510.16460", "authors": ["Saad Eddine Baddis", "Adil Belhaj", "Hajar Belmahi", "Maryem Jemri"], "title": "Constraining Black Hole Shadows in Dunkl Spacetime using CUDA Numerical Computations", "comment": "22 pages, 8 figures, 2 tables, Latex, Authors are listed in\n  alphabetical order", "summary": "With the help of CUDA high-performance numerical codes exploited in machine\nlearning, we investigate the shadow aspect of new rotating and charged black\nholes using the Dunkl derivative formalism. Precisely, we first establish the\ncorresponding metric function encoding the involved physical properties\nincluding the optical character. Exploiting such accelerated simulations, we\napproach the horizon radius behaviors in order to determine the regions of the\nmodule space providing physical solutions. Applying the Hamilton-Jacobi\nmechanism, we assess the shadow aspect for non-rotating and rotating solutions.\nUsing such an aspect, we evaluate the energy rate of emission. Developing a\nhigh-performance CUDA numerical code, we derive strict constraints on the Dunkl\ndeformation parameters in order to establish a link with the shadow\nobservations provided by the Event Horizon Telescope collaboration."}
{"id": "2510.15962", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15962", "abs": "https://arxiv.org/abs/2510.15962", "authors": ["Zhuxuanzi Wang", "Mingqiao Mo", "Xi Xiao", "Chen Liu", "Chenrui Ma", "Yunbei Zhang", "Xiao Wang", "Smita Krishnaswamy", "Tianyang Wang"], "title": "CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models", "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) has become the standard approach for\nadapting large language models under limited compute and memory budgets.\nAlthough previous methods improve efficiency through low-rank updates,\nquantization, or heuristic budget reallocation, they often decouple the\nallocation of capacity from the way updates evolve during training. In this\nwork, we introduce CTR-LoRA, a framework guided by curvature trust region that\nintegrates rank scheduling with stability-aware optimization. CTR-LoRA\nallocates parameters based on marginal utility derived from lightweight\nsecond-order proxies and constrains updates using a Fisher/Hessian-metric trust\nregion. Experiments on multiple open-source backbones (7B-13B), evaluated on\nboth in-distribution and out-of-distribution benchmarks, show consistent\nimprovements over strong PEFT baselines. In addition to increased accuracy,\nCTR-LoRA enhances training stability, reduces memory requirements, and achieves\nhigher throughput, positioning it on the Pareto frontier of performance and\nefficiency. These results highlight a principled path toward more robust and\ndeployable PEFT."}
{"id": "2510.16485", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.16485", "abs": "https://arxiv.org/abs/2510.16485", "authors": ["Arghyabindu Patra", "Abdul Q Batin", "Prasanta K. Panigrahi"], "title": "Communication through the combination of quantum switch and coherent superposition of channels", "comment": null, "summary": "The quantization of particle trajectories gives rise to remarkable features\nsuch as the coherent superposition of quantum channels and the quantum switch,\nwhich offer significant advantages in the communication of both classical and\nquantum information. In this study, we investigate the classical and quantum\ncapacities of various supermaps, including individual quantum switches,\ncoherent superpositions of channels, their combinations, and hybrid\nsuperpositions. A comparative analysis of these configurations reveals the\nscenarios in which specific combinations yield enhanced communication\nadvantages."}
{"id": "2510.16479", "categories": ["gr-qc", "hep-th"], "pdf": "https://arxiv.org/pdf/2510.16479", "abs": "https://arxiv.org/abs/2510.16479", "authors": ["Hocheol Lee", "Bogeun Gwak"], "title": "Frame Dependence of Bound on Lyapunov Exponent in Dilatonic Reissner-Nordström-AdS and Kerr-Sen-AdS Black Holes", "comment": "31 pages, 6 figures", "summary": "We investigate the frame dependence of the Lyapunov exponent bound for\ncharged particles in dilatonic Reissner-Nordstr\\\"om-AdS and Kerr-Sen-AdS black\nhole backgrounds, derived from Einstein-Maxwell-dilaton theory and the\nlow-energy effective action of heterotic string theory, respectively. The\nanalysis is performed in both the Einstein and string (Jordan) frames to\nexamine the influence of conformal transformations on chaotic behavior. For\nmassless particles, the Lyapunov exponent remains invariant under frame\ntransformations, whereas for massive particles, it exhibits frame dependence\nowing to coupling to the dilaton field. Our results indicate sensitivity of the\nbound on chaos to the choice of frame. Depending on various parameters, the\nbound can be satisfied in the Einstein frame and violated in the string frame,\nwhile the opposite situation may occur for different parameter values.\nNumerical computations corroborate the findings of our analysis and demonstrate\nmodifications in the chaotic behavior of string-inspired black holes induced by\nthe dilaton field and the choice of frame."}
{"id": "2510.15964", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15964", "abs": "https://arxiv.org/abs/2510.15964", "authors": ["Tuowei Wang", "Kun Li", "Zixu Hao", "Donglin Bai", "Ju Ren", "Yaoxue Zhang", "Ting Cao", "Mao Yang"], "title": "Long Exposure: Accelerating Parameter-Efficient Fine-Tuning for LLMs under Shadowy Sparsity", "comment": null, "summary": "The adaptation of pre-trained large language models (LLMs) to diverse\ndownstream tasks via fine-tuning is critical for numerous applications.\nHowever, the inefficiency of parameter-efficient fine-tuning (PEFT) techniques\npresents significant challenges in terms of time investments and operational\ncosts. In this paper, we first introduce a nuanced form of sparsity, termed\nShadowy Sparsity, which is distinctive in fine-tuning and has not been\nadequately addressed for acceleration. Under Shadowy Sparsity, we propose Long\nExposure, an efficient system to accelerate PEFT for LLMs. Long Exposure\ncomprises three key components: Shadowy-sparsity Exposer employs a prolonged\nsensing range to capture more sparsity details under shadowy sparsity;\nSequence-oriented Predictor provides efficient yet accurate predictions to\nhandle large sequence inputs and constantly-evolving parameters; and\nDynamic-aware Operator facilitates more structured computational patterns and\ncoalesced memory accesses, addressing dynamic sparse operations. Extensive\nevaluations show that Long Exposure outperforms state-of-the-arts with up to a\n$2.49\\times$ speedup in end-to-end fine-tuning, offering promising advancements\nin accelerating PEFT for LLMs."}
{"id": "2510.16498", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.16498", "abs": "https://arxiv.org/abs/2510.16498", "authors": ["Ximing Hua", "Daowen Qiu"], "title": "Distributed Quantum Amplitude Amplification", "comment": "17 pages, 4 figures, comments are welcome", "summary": "Quantum amplitude amplification algorithm is an important and basic technique\nin quantum computing. In this paper, our goal is to study distributed quantum\namplitude amplification algorithms, and the main contributions are: (1) A\ndistributed quantum amplitude amplification algorithm is proposed. (2) We\nsimulate the proposed algorithm in a particular situation by Qiskit. (3)\nCompared to other related works, our algorithm has certain advantages\nconcerning the number of qubits."}
{"id": "2510.16520", "categories": ["gr-qc", "astro-ph.GA", "hep-ph"], "pdf": "https://arxiv.org/pdf/2510.16520", "abs": "https://arxiv.org/abs/2510.16520", "authors": ["Mordehai Milgrom"], "title": "The deep-MOND limit -- a study in Primary vs secondary predictions", "comment": "13 pages, 3 figures, Bases on a talk presented at the MOND workshop,\n  Leiden, September 2025", "summary": "In default of a fundamental MOND theory -- a FUNDAMOND -- I advocate that,\nalongside searching for one, we should try to identify predictions that follow\nfrom wide classes of MOND theories, if not necessarily from all. In particular,\npredictions that follow from only the basic tenets of MOND -- ``primary\npredictions'' -- are shared by all MOND theories, and are especially valuable.\nSuch predictions permit us to test the MOND paradigm itself, or at least large\nparts of it, without yet having a FUNDAMOND. Concentrating on the deep-MOND\nlimit, I discuss examples of either type of predictions. For some examples of\nprimary predictions, I demonstrate how they follow from the basic tenets (which\nI first formulate). I emphasize that even predictions that pertain to the\ndeep-MOND limit - namely, those that concern gravitating systems that have low\naccelerations everywhere -- require the full set of MOND tenets, including the\nexistence of a Newtonian limit close to the deep-MOND regime. This is because\nNewtonian dynamics is a unique theory that all MOND theories must tend to in\nthe limit of high accelerations, and it strongly constrains aspects of the\ndeep-MOND regime, if the transition between the limits is fast enough, which is\none of the MOND tenets."}
{"id": "2510.15965", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15965", "abs": "https://arxiv.org/abs/2510.15965", "authors": ["Mohan Zhang", "Yihua Zhang", "Jinghan Jia", "Zhangyang Wang", "Sijia Liu", "Tianlong Chen"], "title": "One Token Embedding Is Enough to Deadlock Your Large Reasoning Model", "comment": "NeurIPS 2025", "summary": "Modern large reasoning models (LRMs) exhibit impressive multi-step\nproblem-solving via chain-of-thought (CoT) reasoning. However, this iterative\nthinking mechanism introduces a new vulnerability surface. We present the\nDeadlock Attack, a resource exhaustion method that hijacks an LRM's generative\ncontrol flow by training a malicious adversarial embedding to induce perpetual\nreasoning loops. Specifically, the optimized embedding encourages transitional\ntokens (e.g., \"Wait\", \"But\") after reasoning steps, preventing the model from\nconcluding its answer. A key challenge we identify is the\ncontinuous-to-discrete projection gap: na\\\"ive projections of adversarial\nembeddings to token sequences nullify the attack. To overcome this, we\nintroduce a backdoor implantation strategy, enabling reliable activation\nthrough specific trigger tokens. Our method achieves a 100% attack success rate\nacross four advanced LRMs (Phi-RM, Nemotron-Nano, R1-Qwen, R1-Llama) and three\nmath reasoning benchmarks, forcing models to generate up to their maximum token\nlimits. The attack is also stealthy (in terms of causing negligible utility\nloss on benign user inputs) and remains robust against existing strategies\ntrying to mitigate the overthinking issue. Our findings expose a critical and\nunderexplored security vulnerability in LRMs from the perspective of reasoning\n(in)efficiency."}
{"id": "2510.16519", "categories": ["quant-ph", "physics.atom-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2510.16519", "abs": "https://arxiv.org/abs/2510.16519", "authors": ["Jianming Wen"], "title": "Triphoton generation near atomic resonance via SSWM: Harmonic expansion for accurate optical response", "comment": "Exact and self-consistent calculations of linear and nonlinear\n  susceptibilities associated with spontaneous six-wave mixing (SSWM) for\n  continuous-mode time-energy-entangled W triphoton direct generation", "summary": "Quantum correlations of time-frequency-entangled photon pairs generated via\nparametric processes are critically influenced by both the linear and nonlinear\noptical responses of the medium. This sensitivity is especially significant in\nschemes utilizing atomic ensembles with well-defined energy level structures\nnear resonance. However, conventional theoretical approaches often fall short\nin accurately calculating the optical responses--particularly when a single\natomic transition is simultaneously driven by multiple light fields with\n(significantly) different intensities. To address this limitation, we\ngeneralize the harmonic expansion method originally introduced by Wen for\nbiphoton generation near atomic resonance. As a case study, we apply this\ngeneralized approach to the reliable direct generation of time-energy-entangled\nW-state triphotons via spontaneous six-wave mixing in a five-level asymmetric-M\natomic system. Our results demonstrate the method's superior accuracy and\nself-consistency, offering clear advantages over traditional calculation\ntechniques."}
{"id": "2510.16577", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.16577", "abs": "https://arxiv.org/abs/2510.16577", "authors": ["Gines R. Perez Teruel"], "title": "Energy-Momentum Surfaces: A Differential Geometric Framework for Dispersion Relations", "comment": "Accepted for publication in IJGMMP", "summary": "We propose a geometric framework where dispersion relations are viewed as\nparametric surfaces in energy-momentum space. Within this picture, the presence\nand type of critical points of the surface emerge as clear geometric signatures\nof kinematical restrictions. The Newtonian relation corresponds to a\ndevelopable surface with no critical points, reflecting the absence of\ninvariant limits. Special Relativity generates a saddle point and globally\nnegative curvature, encoding the universal light cone. Modified dispersion\nrelations may introduce additional critical points, signaling new invariant\nenergy scales or thresholds. This unifying approach not only recasts known\nresults in a transparent geometric language but also provides a simple\ndiagnostic tool for exploring departures from Lorentz invariance and their\nphysical implications."}
{"id": "2510.15967", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15967", "abs": "https://arxiv.org/abs/2510.15967", "authors": ["Zhengyi Zhong", "Wenzheng Jiang", "Weidong Bao", "Ji Wang", "Cheems Wang", "Guanbo Wang", "Yongheng Deng", "Ju Ren"], "title": "Gains: Fine-grained Federated Domain Adaptation in Open Set", "comment": "Accepted by NeurIPS2025", "summary": "Conventional federated learning (FL) assumes a closed world with a fixed\ntotal number of clients. In contrast, new clients continuously join the FL\nprocess in real-world scenarios, introducing new knowledge. This raises two\ncritical demands: detecting new knowledge, i.e., knowledge discovery, and\nintegrating it into the global model, i.e., knowledge adaptation. Existing\nresearch focuses on coarse-grained knowledge discovery, and often sacrifices\nsource domain performance and adaptation efficiency. To this end, we propose a\nfine-grained federated domain adaptation approach in open set (Gains). Gains\nsplits the model into an encoder and a classifier, empirically revealing\nfeatures extracted by the encoder are sensitive to domain shifts while\nclassifier parameters are sensitive to class increments. Based on this, we\ndevelop fine-grained knowledge discovery and contribution-driven aggregation\ntechniques to identify and incorporate new knowledge. Additionally, an\nanti-forgetting mechanism is designed to preserve source domain performance,\nensuring balanced adaptation. Experimental results on multi-domain datasets\nacross three typical data-shift scenarios demonstrate that Gains significantly\noutperforms other baselines in performance for both source-domain and\ntarget-domain clients. Code is available at:\nhttps://github.com/Zhong-Zhengyi/Gains."}
{"id": "2510.16521", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.16521", "abs": "https://arxiv.org/abs/2510.16521", "authors": ["Da Zhang", "Yu Zhang"], "title": "Temporal-order-driven asymmetric quantum interference and temporal coherence enhancement in spontaneous six-wave mixing", "comment": null, "summary": "Narrow-band multiphoton entanglement sources serve as a core enabling\nresource for advanced quantum information technologies. Recently, researchers\nhave directly generated energy-time entangled triphoton W states in a hot\natomic medium via spontaneous six-wave mixing for the first time. However, a\nrigorous theoretical framework for this process remains lacking to date,\nconfining our understanding to a mere extension of the biphoton model. Here, we\nanalytically investigate the generation mechanism of energy-time entangled\ntriphotons and their classically controllable optical properties in an\nelectromagnetically induced transparency-assisted five-level cold atomic\nsystem. Notably, triphoton generation follows strict temporal ordering,\nresulting in asymmetric quantum interference in triple coincidence\ncounts--unreplicable and unexplainable by the inherently symmetric biphoton\nmodel. These results establish a rigorous physical framework for spontaneous\nsix-wave mixing-generated triphotons, clarify their distinctions from states\nproduced via cascaded nonlinear models, and substantially advance their utility\nin quantum information protocols."}
{"id": "2510.16589", "categories": ["gr-qc", "astro-ph.CO", "hep-th"], "pdf": "https://arxiv.org/pdf/2510.16589", "abs": "https://arxiv.org/abs/2510.16589", "authors": ["Zanyar Ebrahimi", "Kayoomars Karami"], "title": "Structure formation in a non-canonical scalar field model of clustering dark energy", "comment": "35 pages, 12 figures", "summary": "This paper examines the growth of dark matter and dark energy perturbations\nwithin a non-canonical scalar field model characterized by an exponential\npotential. Through dynamical system analysis, we identify critical points and\ntrack the background evolution of a spatially flat FLRW universe dominated by\ndark energy and pressureless dark matter. We systematically derive key\ncosmological quantities, including the Hubble parameter, deceleration\nparameter, density parameters, and the scalar field's equation of state, and\nexplore their dependence on model parameters. Within the linear perturbation\nframework, employing the pseudo-Newtonian formalism, we compute the growth\nfactor of matter density perturbations. To investigate the non-linear regime of\nstructure formation, we employ the spherical collapse model and derive its key\nparameters. Building on these findings, we compute the function\n$f(z)\\sigma_8(z)$ and the relative number density of halo objects exceeding a\ngiven mass threshold. Our results indicate that non-canonical scalar field\nmodels can effectively account for both background cosmic evolution and the\ngrowth of structure, offering potential insights into observational constraints\nand large-scale dynamics."}
{"id": "2510.15968", "categories": ["cs.LG", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.15968", "abs": "https://arxiv.org/abs/2510.15968", "authors": ["Zhen Huang", "Hong Wang", "Wenkai Yang", "Muxi Tang", "Depeng Xie", "Ting-Jung Lin", "Yu Zhang", "Wei W. Xing", "Lei He"], "title": "Self-Attention to Operator Learning-based 3D-IC Thermal Simulation", "comment": null, "summary": "Thermal management in 3D ICs is increasingly challenging due to higher power\ndensities. Traditional PDE-solving-based methods, while accurate, are too slow\nfor iterative design. Machine learning approaches like FNO provide faster\nalternatives but suffer from high-frequency information loss and high-fidelity\ndata dependency. We introduce Self-Attention U-Net Fourier Neural Operator\n(SAU-FNO), a novel framework combining self-attention and U-Net with FNO to\ncapture long-range dependencies and model local high-frequency features\neffectively. Transfer learning is employed to fine-tune low-fidelity data,\nminimizing the need for extensive high-fidelity datasets and speeding up\ntraining. Experiments demonstrate that SAU-FNO achieves state-of-the-art\nthermal prediction accuracy and provides an 842x speedup over traditional FEM\nmethods, making it an efficient tool for advanced 3D IC thermal simulations."}
{"id": "2510.16561", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.16561", "abs": "https://arxiv.org/abs/2510.16561", "authors": ["Dafa Li"], "title": "A necessary and sufficient condition for genuinely entangled n-qubit states with six non-zero coefficients", "comment": "13 pages, no figures", "summary": "In [Science 340, 1205, 7 June (2013)], via polytopes Michael Walter et al.\nproposed a sufficient condition detecting the genuinely entangled pure states.\nIn this paper, assume that a state with six non-zero coefficients is not a\ntrivially separable state. Then the state is separable if and only if its six\nbasis states consist of the three partially complementary pairs and the\ncorresponding coefficient matrix has proportional rows. The contrapositive of\nthis result reads that the state is genuinely entangled if and only if its six\nbasis states do not consist of the three partially complementary pairs or\nthough the six basis states consist of the three partially complementary pairs,\nthe corresponding coefficient matrix does not have proportional rows. We\npropose four corresponding coefficient 2 by 3 matrices and show that if the\nfour coefficient matrices don't have proportional rows, then the state is\ngenuinely entangled. It is trivial to know if two rows of a 2 by 3 coefficient\nmatrix are proportional. The difference from the previous articles is that the\nstructure of the basis states is used to detect entanglement in this paper. One\ncan see that Osterloh and Siewert's states of five and six qubits are genuinely\nentangled because two rows for any one of the four corresponding coefficient 2\nby 3 matrices are not proportional. These states were distinguished as the\nmaximal entangled states by the complicated filters before.\n  Keywords: entanglement, separability, entangled states, separable states,\nqubits."}
{"id": "2510.16603", "categories": ["gr-qc", "hep-th", "nucl-th"], "pdf": "https://arxiv.org/pdf/2510.16603", "abs": "https://arxiv.org/abs/2510.16603", "authors": ["Nicolas Clarisse", "Eduardo O. Pinho", "Teerthal Patel", "Fabio S. Bemfica", "Mauricio Hippert", "Jorge Noronha"], "title": "Flux-Conservative BDNK Hydrodynamics and Shock Regularization", "comment": "31 pages, 14 figures + appendices", "summary": "We present a new, first-order, flux-conservative formulation of relativistic\nviscous hydrodynamics in the BDNK framework, applicable to conformal and\nnonconformal fluids at zero chemical potential. Focusing on the conformal case\nin 1+1 dimensions, we numerically solve the equations of motion for two classes\nof consistent initial data and assess the robustness of the resulting solutions\nwith respect to changing the hydrodynamic frame. Our flux-conservative\nformulation does not exhibit spurious oscillatory structures within the regime\nof validity of BDNK theory (the hydrodynamic-frame-robust regime),\ncorresponding to sufficiently small Knudsen numbers. Using this\nflux-conservative approach, we numerically investigate the potential formation\nof shocks and their fate in BDNK in 1+1D using smooth initial data known to\nproduce shocks in the relativistic Euler equations. For the type of initial\ndata we consider, we show that the sharp features formed in Euler are prevented\nby the hyperbolic viscous BDNK equations. The prevention of shock formation we\nobserve occurs in the frame-robust regime of BDNK. This, however, does not\npreclude the formation of shocks in BDNK for different smooth initial data. The\nreliability of our results is supported by systematic numerical convergence\ntesting, which is used to assess shock indicators, simulation performance, and\nthe robustness of solutions for different hydrodynamic frames."}
{"id": "2510.15969", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15969", "abs": "https://arxiv.org/abs/2510.15969", "authors": ["Paul-Niklas Ken Kandora", "Simon Caspar Zeller", "Aaron Jeremias Elsing", "Elena Kuss", "Steffen Rebennack"], "title": "LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear Reformulation of Nonlinear Optimization Problems", "comment": null, "summary": "Reformulating nonlinear optimization problems is largely manual and\nexpertise-intensive, yet it remains essential for solving such problems with\nlinear optimization solvers or applying special-purpose algorithms. We\nintroduce \\textit{LinearizeLLM}, an agent-based framework that solves this task\nby leveraging Large Language Models (LLMs). The framework assigns each\nnonlinear pattern to a \\textit{reformulation agent} that is explicitly\ninstructed to derive an exact linear reformulation for its nonlinearity\npattern, for instance, absolute-value terms or bilinear products of decision\nvariables. The agents then coordinate to assemble a solver-ready linear model\nequivalent to the original problem. To benchmark the approach, we create a\ndataset of 20 real-world nonlinear optimization problems derived from the\nestablished ComplexOR dataset of linear optimization problems. We evaluate our\napproach with several LLMs. Our results indicate that specialized LLM agents\ncan automate linearization tasks, opening a path toward fully conversational\nmodeling pipelines for nonlinear optimization."}
{"id": "2510.16570", "categories": ["quant-ph", "cond-mat.stat-mech", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2510.16570", "abs": "https://arxiv.org/abs/2510.16570", "authors": ["Arkaprava Sil", "Sudipto Singha Roy"], "title": "Quantum Complexity in Constrained Many-Body Models: Scars, Fragmentation, and Chaos", "comment": "13 pages, 12 figures", "summary": "Kinetic constraints in quantum many-body systems give rise to quantum states,\nwhose behavior strongly depends on the choice of initial conditions. In recent\nyears, these systems have drawn increasing interest because they provide\ninsight into the mechanisms of thermalization and the situations where it can\nfail. In this work, we study a family of kinetically constrained models,\nincluding the celebrated Quantum Game of Life, from the perspective of quantum\ncomplexity, with a focus on entanglement, nonstabilizerness, and signatures of\nquantum chaos. By applying spectral diagnostics such as level statistics and\nspectral form factors, we demonstrate that these models show robust chaotic\nbehavior while also supporting Hilbert space fragmentation and quantum\nmany-body scar states. Remarkably, we find that even certain symmetry-resolved\nfragmented sectors can themselves host scarred eigenstates, highlighting the\nunexpected coexistence of chaos, scars, and fragmentation within the same\nfamily of Hamiltonians. To better understand these fragmented subspaces, we\nfurther characterize them using their quantum resource generation ability. In\nparticular, we demonstrate that characterization of entanglement and the\nability to generate nonstabilizerness can be instrumental in distinguishing\ndifferent dynamically disconnected sectors."}
{"id": "2510.16631", "categories": ["gr-qc", "hep-th"], "pdf": "https://arxiv.org/pdf/2510.16631", "abs": "https://arxiv.org/abs/2510.16631", "authors": ["Anna Horváth", "Aneta Wojnar", "Gergely Gábor Barnaföldi"], "title": "The effects of strong gravity on the dispersion relation of massive particles in the Kaluza-Klein theory", "comment": "12 pages, 7 figures", "summary": "We derive a modified dispersion relation for massive particles within the\nframeworks of five-dimensional Kaluza-Klein theory and general relativity,\ntaking into account strong gravitational effects. The resulting effective mass\ndepends on the curvature of the underlying phase space. Notably, in regions\nwith strong gravitational fields, the effective mass may become imaginary,\nimplying the possibility of particle decay induced by spacetime curvature."}
{"id": "2510.15970", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15970", "abs": "https://arxiv.org/abs/2510.15970", "authors": ["Yang Ba", "Mohammad Sadeq Abolhasani", "Rong Pan"], "title": "Predict Training Data Quality via Its Geometry in Metric Space", "comment": "Accepted to the NeurIPS 2025 Workshop on New Perspectives in Graph\n  Machine Learning", "summary": "High-quality training data is the foundation of machine learning and\nartificial intelligence, shaping how models learn and perform. Although much is\nknown about what types of data are effective for training, the impact of the\ndata's geometric structure on model performance remains largely underexplored.\nWe propose that both the richness of representation and the elimination of\nredundancy within training data critically influence learning outcomes. To\ninvestigate this, we employ persistent homology to extract topological features\nfrom data within a metric space, thereby offering a principled way to quantify\ndiversity beyond entropy-based measures. Our findings highlight persistent\nhomology as a powerful tool for analyzing and enhancing the training data that\ndrives AI systems."}
{"id": "2510.16602", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.16602", "abs": "https://arxiv.org/abs/2510.16602", "authors": ["Cristiano Rosa", "Sergio Giardino"], "title": "Klein-Gordon equation within the real Hilbert space formalism", "comment": "accept by Annals of Physics", "summary": "Within this article one finds the statement of the Klein-Gordon problem\nwithin the real Hilbert space formalism ($\\mathbbm R$HS) in terms of complex\nwave functions, and in terms of quaternionic wave functions as well. The\ncomplex formulation comprises hermitian and non-hermitian cases, while the\nquaternionic solutions additionally set in motion self-interacting particles.\nThe non-hermitian cases comprise non-conservative processes, while the\nself-interaction physically implies the increase of the effective mass of the\nparticle, an effect that cannot be reproduced using a complex wave function.\nThe obtained autonomous particle solutions, as well as the Klein problem agree\nto the previously discovered self-interacting non-relativistic particle, and\nthus reinforce $\\mathbbm R$HS as viable and consistent way to explore open\nproblems in quantum mechanics. Also important, the negative energy problem that\nplagues the usual formalism is eliminated within this approach."}
{"id": "2510.16639", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.16639", "abs": "https://arxiv.org/abs/2510.16639", "authors": ["Yassine Sekhmani", "Wentao Liu", "Weike Deng", "Kuantay Boshkayev"], "title": "Quasinormal Modes of Massive Scalar Perturbations in Slow-Rotation Bumblebee Black Holes with Traceless Conformal Electrodynamics", "comment": "17 pages, 5 figures", "summary": "We study electrically charged, slowly rotating black hole solutions in\nEinstein-Bumblebee gravity coupled to the traceless (conformal) ModMax\nnonlinear electrodynamics. By adopting a quadratic bumblebee potential that\nfixes the vacuum expectation value of the Lorentz-violating vector, we derive\nboth the static configuration and its first-order rotating extension and\ndemonstrate how the bumblebee parameter $\\ell$ and the ModMax deformation\n$\\gamma$ modify the horizon structure and the effective electric charge. We\nfurther investigate the dynamical properties of this spacetime by considering a\nmassive scalar field perturbation. Using two independent numerical techniques,\nwe compute the quasinormal mode (QNM) spectra and perform a comprehensive\nanalysis of the influence of all relevant parameters, including the black hole\nspin, the Lorentz-violating coupling, the ModMax deformation, and the scalar\nfield mass. Our results reveal coherent trends in the QNM frequencies,\nhighlighting the interplay between Lorentz-symmetry breaking and nonlinear\nelectrodynamics effects in black hole dynamics."}
{"id": "2510.15977", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15977", "abs": "https://arxiv.org/abs/2510.15977", "authors": ["Wenyun Li", "Zheng Zhang", "Dongmei Jiang", "Xiangyuan Lan"], "title": "Bolster Hallucination Detection via Prompt-Guided Data Augmentation", "comment": null, "summary": "Large language models (LLMs) have garnered significant interest in AI\ncommunity. Despite their impressive generation capabilities, they have been\nfound to produce misleading or fabricated information, a phenomenon known as\nhallucinations. Consequently, hallucination detection has become critical to\nensure the reliability of LLM-generated content. One primary challenge in\nhallucination detection is the scarcity of well-labeled datasets containing\nboth truthful and hallucinated outputs. To address this issue, we introduce\nPrompt-guided data Augmented haLlucination dEtection (PALE), a novel framework\nthat leverages prompt-guided responses from LLMs as data augmentation for\nhallucination detection. This strategy can generate both truthful and\nhallucinated data under prompt guidance at a relatively low cost. To more\neffectively evaluate the truthfulness of the sparse intermediate embeddings\nproduced by LLMs, we introduce an estimation metric called the Contrastive\nMahalanobis Score (CM Score). This score is based on modeling the distributions\nof truthful and hallucinated data in the activation space. CM Score employs a\nmatrix decomposition approach to more accurately capture the underlying\nstructure of these distributions. Importantly, our framework does not require\nadditional human annotations, offering strong generalizability and practicality\nfor real-world applications. Extensive experiments demonstrate that PALE\nachieves superior hallucination detection performance, outperforming the\ncompetitive baseline by a significant margin of 6.55%."}
{"id": "2510.16621", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.16621", "abs": "https://arxiv.org/abs/2510.16621", "authors": ["Eric I. Rosenthal", "Christopher S. Wang", "Jamison Sloan", "Giovanni Scuri", "Yueheng Shi", "Kaveh Pezeshki", "Peter Mugaba Noertoft", "Jelena Vuckovic", "Christopher P. Anderson"], "title": "Proposal for a 3-Wave Mixing Element with Quantum Paraelectric Materials", "comment": null, "summary": "At cryogenic temperatures and microwave frequencies, the perovskite crystals\nstrontium titanate (STO) and potassium tantalate (KTO) have large, tunable\npermittivity arising from a quantum paraelectric phase. As such, these\nmaterials hold promise as a platform to realize compact, variable capacitance\nelements for use in quantum devices. From modulating this capacitance, we\npropose the development of a parametric mixing element: a quantum paraelectric\nnonlinear dielectric amplifier (PANDA). We calculate that a PANDA made from a\nnanofabricated parallel plate capacitor and realistic design constraints can\ndemonstrate a three-wave mixing strength of order MHz, in comparison to an\neffective Kerr strength of sub-Hz. This suggests excellent performance as a\nthree-wave mixing element, with high compression power in analogy to\nsuperconducting parametric amplifiers based on kinetic inductance. Beyond\nparametric amplifiers, we predict that compact, tunable capacitors based on\nSTO, KTO, and related materials can enable a wide class of cryogenic quantum\ncircuits including novel filters, switches, circulators, and qubits."}
{"id": "2510.16731", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.16731", "abs": "https://arxiv.org/abs/2510.16731", "authors": ["Jia-Zhou Liu", "Shan-Ping Wu", "Shao-Wen Wei", "Yu-Xiao Liu"], "title": "Exact Black Hole Solutions in Bumblebee Gravity with Lightlike or Spacelike VEVS", "comment": null, "summary": "Motivated by recent developments in Lorentz-violating theories of gravity, we\nobtain new black hole solutions within the framework of bumblebee gravity where\nthe bumblebee vector field has two independent nonzero components and acquires\neither a lightlike or spacelike vacuum expectation value. Within this\nframework, we first derive new Schwarzschild-like and Schwarzschild-(A)dS black\nhole solutions. We first obtain Schwarzschild-like and Schwarzschild-(A)dS-like\nsolutions in this framework. By further introducing a nonminimally coupled\nelectromagnetic field, we obtain new charged black hole solutions. These\nsolutions extend previous results through the inclusion of additional\nLorentz-violating parameters. Remarkably, we find that even for lightlike\nvacuum expectation values, the black hole solutions retain corrections arising\nfrom the Lorentz-violating parameter. Furthermore, we present a preliminary\nanalysis of the thermodynamic properties associated with these new solutions.\nSimilar to previous studies that reported a mismatch between the black hole\nentropy and the Wald entropy in bumblebee gravity with spacelike vacuum\nexpectation values, our solutions with spacelike vacuum expectation values\nexhibit the same behavior. However, we find that the two entropies coincide in\nthe lightlike case considered here."}
{"id": "2510.15978", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2510.15978", "abs": "https://arxiv.org/abs/2510.15978", "authors": ["Junchao Gong", "Jingyi Xu", "Ben Fei", "Fenghua Ling", "Wenlong Zhang", "Kun Chen", "Wanghan Xu", "Weidong Yang", "Xiaokang Yang", "Lei Bai"], "title": "DAWP: A framework for global observation forecasting via Data Assimilation and Weather Prediction in satellite observation space", "comment": null, "summary": "Weather prediction is a critical task for human society, where impressive\nprogress has been made by training artificial intelligence weather prediction\n(AIWP) methods with reanalysis data. However, reliance on reanalysis data\nlimits the AIWPs with shortcomings, including data assimilation biases and\ntemporal discrepancies. To liberate AIWPs from the reanalysis data, observation\nforecasting emerges as a transformative paradigm for weather prediction. One of\nthe key challenges in observation forecasting is learning spatiotemporal\ndynamics across disparate measurement systems with irregular high-resolution\nobservation data, which constrains the design and prediction of AIWPs. To this\nend, we propose our DAWP as an innovative framework to enable AIWPs to operate\nin a complete observation space by initialization with an artificial\nintelligence data assimilation (AIDA) module. Specifically, our AIDA module\napplies a mask multi-modality autoencoder(MMAE)for assimilating irregular\nsatellite observation tokens encoded by mask ViT-VAEs. For AIWP, we introduce a\nspatiotemporal decoupling transformer with cross-regional boundary conditioning\n(CBC), learning the dynamics in observation space, to enable sub-image-based\nglobal observation forecasting. Comprehensive experiments demonstrate that AIDA\ninitialization significantly improves the roll out and efficiency of AIWP.\nAdditionally, we show that DAWP holds promising potential to be applied in\nglobal precipitation forecasting."}
{"id": "2510.16623", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.16623", "abs": "https://arxiv.org/abs/2510.16623", "authors": ["N. Rimock", "Y. Oz"], "title": "Generalized Fusion of Qudit Graph States", "comment": "9 pages", "summary": "We formalize a generalized type-II fusion operation for qudit cluster states\nwithin linear optics. Two designated qudits, one from each input cluster,\ninterfere with optional ancilla qudits via a passive linear-optical network,\nfollowed by number-resolving detection; conditioned on %a two-click outcome,\nmeasurement outcome, the remaining qudits form the post-selected fused state.\nWe prove a general rank bound: for any such interferometer and outcome, the\nreduced density matrix across the two parent clusters has Schmidt rank at most\n$M$, the total number of measured qudits including ancillae. Consequently, a\ncorrect qudit fusion which requires rank $d$ is impossible without ancillae and\nrequires at least $d-2$ ancilla qudits. Our analysis extends previous no-go\nresults for Bell-type qubit fusion to the qudit setting and to generalized,\nnon-Bell projections. We analyze the probabilities and entanglement of the\nrelevant measurement outcomes, and discuss how our lower bound aligns with\nexisting constructive schemes. These results set a clear resource threshold for\nhigh-dimensional, fusion-based photonic MBQC."}
{"id": "2510.16762", "categories": ["gr-qc", "hep-th", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2510.16762", "abs": "https://arxiv.org/abs/2510.16762", "authors": ["Mahmut Elbistan", "Peng-Ming Zhang", "Peter Horvathy"], "title": "Globally defined Carroll symmetry of gravitational waves", "comment": "14 pages, many figures", "summary": "The locally defined Carroll symmetry of a gravitational wave is extended to a\nglobally defined one. Translations and Carroll boosts, associated with two\nindependent globally defined solutions of a Sturm-Liouville equation allow us\nto describe the motions. The Displacement Memory Effect arises for particular\nchoices of the parameters which yield trajectories with zero momentum. We\nillustrate our general statements by the P\\\"oschl-Teller profile."}
{"id": "2510.15979", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15979", "abs": "https://arxiv.org/abs/2510.15979", "authors": ["Zexu Sun", "Yongcheng Zeng", "Erxue Min", "Heyang Gao", "Bokai Ji", "Xu Chen"], "title": "Cog-Rethinker: Hierarchical Metacognitive Reinforcement Learning for LLM Reasoning", "comment": "22 Pages, 8 figures, 4 tables", "summary": "Contemporary progress in large language models (LLMs) has revealed notable\ninferential capacities via reinforcement learning (RL) employing verifiable\nreward, facilitating the development of O1 and R1-like reasoning models.\nDirectly training from base models with RL is called zero-RL. However, previous\nworks rely upon activating LLMs' inherent capacities through fixed prompt\ntemplates. This strategy introduces substantial sampling inefficiencies for\nweak LLMs, as the majority of problems generate invalid outputs during\naccuracy-driven filtration in reasoning tasks, which causes a waste of samples.\nTo solve this issue, we propose Cog-Rethinker, a novel hierarchical\nmetacognitive RL framework for LLM reasoning. Our Cog-Rethinker mainly focuses\non the rollout procedure in RL training. After the direct rollout, our\nCog-Rethinker improves sample utilization in a hierarchical metacognitive\ntwo-stage framework. By leveraging human cognition during solving problems,\nfirstly, it prompts policy to decompose zero-accuracy problems into subproblems\nto produce final reasoning results. Secondly, with zero-accuracy problems in\nprevious rollout stage, it further prompts policy to refine these answers by\nreferencing previous wrong solutions. Moreover, to enable cold-start of the two\nnew reasoning patterns and maintain train-test consistency across prompt\ntemplates, our Cog-Rethinker applies supervised fine-tuning on the policy using\ncorrect samples of the two stages with direct rollout template. Experimental\nresults demonstrate Cog-Rethinker's superior performance on various\nmathematical reasoning benchmarks, we also analyzed its improved sample\nefficiency that accelerates convergence compared to baseline methods."}
{"id": "2510.16625", "categories": ["quant-ph", "cs.ET", "cs.MS", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16625", "abs": "https://arxiv.org/abs/2510.16625", "authors": ["Armin Ahmadkhaniha", "Lu Chen", "Jake Doliskani", "Zhifu Sun"], "title": "QRTlib: A Library for Fast Quantum Real Transforms", "comment": null, "summary": "Real-valued transforms such as the discrete cosine, sine, and Hartley\ntransforms play a central role in classical computing, complementing the\nFourier transform in applications from signal and image processing to data\ncompression. However, their quantum counterparts have not evolved in parallel,\nand no unified framework exists for implementing them efficiently on quantum\nhardware. This article addresses this gap by introducing QRTlib, a library for\nfast and practical implementations of quantum real transforms, including the\nquantum Hartley, cosine, and sine transforms of various types. We develop new\nalgorithms and circuit optimizations that make these transforms efficient and\nsuitable for near-term devices. In particular, we present a quantum Hartley\ntransform based on the linear combination of unitaries (LCU) technique,\nachieving a fourfold reduction in circuit size compared to prior methods, and\nan improved quantum sine transform of Type I that removes large\nmulti-controlled operations. We also introduce circuit-level optimizations,\nincluding two's-complement and or-tree constructions. QRTlib provides the first\ncomplete implementations of these quantum real transforms in Qiskit."}
{"id": "2510.16770", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.16770", "abs": "https://arxiv.org/abs/2510.16770", "authors": ["Jonathan R. Gair", "Senwen Deng", "Stanislav Babak"], "title": "Contamination of transient gravitational waves in LISA data by gaps and glitches", "comment": "19 pages, 8 figures", "summary": "We present a probabilistic framework to quantify the impact of artefacts\n(glitches and gaps) in LISA data on transient gravitational wave signals. By\nmodeling both artefacts and transient signals as independent Poisson processes,\nand characterising the contaminating effect of an artefact by an associated\ndead time, we estimate the probability distribution of the total contamination\ntime during the observation period using a Normal approximation under various\ncontamination scenarios. Using the same approach, we also estimate the\nprobability that a population of transient signals contaminates each other. We\ndemonstrate the validity of the Normal approximation by comparing it to the\nnumerical distribution obtained via simulations. Our approach provides a rapid\nmeans to assess the potential impact of glitches and gaps on LISA science, and\ncan be used as a figure of merit to evaluate different instrumental scenarios."}
{"id": "2510.15982", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15982", "abs": "https://arxiv.org/abs/2510.15982", "authors": ["Donghyeok Shin", "Yeongmin Kim", "Suhyeon Jo", "Byeonghu Na", "Il-Chul Moon"], "title": "AMiD: Knowledge Distillation for LLMs with $α$-mixture Assistant Distribution", "comment": null, "summary": "Autoregressive large language models (LLMs) have achieved remarkable\nimprovement across many tasks but incur high computational and memory costs.\nKnowledge distillation (KD) mitigates this issue by transferring knowledge from\na large teacher to a smaller student through distributional alignment. Previous\nstudies have proposed various discrepancy metrics, but the capacity gap and\ntraining instability caused by near-zero probabilities, stemming from the\nhigh-dimensional output of LLMs, remain fundamental limitations. To overcome\nthese challenges, several approaches implicitly or explicitly incorporating\nassistant distribution have recently been proposed. However, the past proposals\nof assistant distributions have been a fragmented approach without a systematic\ninvestigation of the interpolation path and the divergence. This paper proposes\n$\\alpha$-mixture assistant distribution, a novel generalized family of\nassistant distributions, and $\\alpha$-mixture distillation, coined AMiD, a\nunified framework for KD using the assistant distribution. The $\\alpha$-mixture\nassistant distribution provides a continuous extension of the assistant\ndistribution by introducing a new distribution design variable $\\alpha$, which\nhas been fixed in all previous approaches. Furthermore, AMiD generalizes the\nfamily of divergences used with the assistant distributions based on\noptimality, which has also been restricted in previous works. Through extensive\nexperiments, we demonstrate that AMiD offers superior performance and training\nstability by leveraging a broader and theoretically grounded assistant\ndistribution space."}
{"id": "2510.16628", "categories": ["quant-ph", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2510.16628", "abs": "https://arxiv.org/abs/2510.16628", "authors": ["Seyed Mohammad Hosseiny", "Abolfazl Pourhashemi Khabisi", "Jamileh Seyed-Yazdi", "Milad Norouzi", "Somayyeh Ghorbani", "Asad Ali", "Saif Al-Kuwari"], "title": "Quantum thermometric sensing: Local vs. Remote approaches", "comment": null, "summary": "Quantum thermometry leveraging quantum sensors is investigated with an\nemphasis on fundamental precision bounds derived from quantum estimation\ntheory. The proposed sensing platform consists of two dissimilar qubits coupled\nvia capacitor, which induce quantum oscillations in the presence of a thermal\nenvironment. Thermal equilibrium states are modeled using the Gibbs\ndistribution. The precision limits are assessed through the Quantum Fisher\nInformation (QFI) and the Hilbert-Schmidt Speed (HSS), serving as stringent\ncriteria for sensor sensitivity. Systematic analysis of the dependence of QFI\nand HSS on tunable parameters -such as qubit energies and coupling strengths-\nprovides optimization pathways for maximizing temperature sensitivity.\nFurthermore, we explore two distinct quantum thermometry paradigms: (I) local\ntemperature estimation directly performed by Alice, who possesses the quantum\nsensor interfacing with the thermal bath, and (II) remote temperature\nestimation conducted by Bob, facilitated via quantum teleportation. In the\nlatter scenario, temperature information encoded in the qubit state is\ntransmitted through a single-qubit quantum thermal teleportation protocol. Our\nfindings indicate that direct measurement yields superior sensitivity compared\nto remote estimation, primarily due to the inherent advantage of direct\nsensor-environment interaction. The analysis reveals that increasing Josephson\nenergies diminishes sensor sensitivity, whereas augmenting the mutual coupling\nstrength between the qubits enhances it."}
{"id": "2510.16903", "categories": ["gr-qc", "astro-ph.HE"], "pdf": "https://arxiv.org/pdf/2510.16903", "abs": "https://arxiv.org/abs/2510.16903", "authors": ["Yuxin Yang", "Changfu Shi", "Yi-Ming Hu"], "title": "Contribution from Nonlinear Quasi-normal Modes in GW250114", "comment": "3 figures, submitted", "summary": "We report evidence for nonlinear gravitational effects in the ringdown signal\nof gravitational wave event GW250114. Using Bayesian inference, we find that\nthe inclusion of a nonlinear quasi-normal mode (220Q), a second-order harmonic\npredicted by general relativity, is statistically favored over the standard\nlinear model (440 mode) when analyzing the post-merger oscillations.\nSpecifically, models incorporating the 220Q mode yield higher Bayes factors\nthan those including only the linear 440 mode, and produce remnant black hole\nparameters (mass and spin) more consistent with full numerical relativity\nsimulations. This suggests that nonlinear mode coupling contributes\nsignificantly to the ringdown phase, opening a new avenue to probe strong-field\ngravity beyond linear approximations."}
{"id": "2510.15985", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15985", "abs": "https://arxiv.org/abs/2510.15985", "authors": ["Zexi Tan", "Tao Xie", "Binbin Sun", "Xiang Zhang", "Yiqun Zhang", "Yiu-Ming Cheung"], "title": "MEET-Sepsis: Multi-Endogenous-View Enhanced Time-Series Representation Learning for Early Sepsis Prediction Representation Learning for Early Sepsis Prediction", "comment": "Accepted to PRICAI 2025", "summary": "Sepsis is a life-threatening infectious syndrome associated with high\nmortality in intensive care units (ICUs). Early and accurate sepsis prediction\n(SP) is critical for timely intervention, yet remains challenging due to subtle\nearly manifestations and rapidly escalating mortality. While AI has improved SP\nefficiency, existing methods struggle to capture weak early temporal signals.\nThis paper introduces a Multi-Endogenous-view Representation Enhancement (MERE)\nmechanism to construct enriched feature views, coupled with a Cascaded\nDual-convolution Time-series Attention (CDTA) module for multi-scale temporal\nrepresentation learning. The proposed MEET-Sepsis framework achieves\ncompetitive prediction accuracy using only 20% of the ICU monitoring time\nrequired by SOTA methods, significantly advancing early SP. Extensive\nvalidation confirms its efficacy. Code is available at:\nhttps://github.com/yueliangy/MEET-Sepsis."}
{"id": "2510.16634", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.16634", "abs": "https://arxiv.org/abs/2510.16634", "authors": ["Abeer Al Ghamdi", "Gin Jose", "Almut Beige"], "title": "Cavity QED beyond the Jaynes-Cummings model", "comment": "15 pages, 9 figures", "summary": "As atom-cavity systems are becoming more sophisticated, the limitations of\nthe Jaynes-Cummings model are becoming more apparent. In this paper, we\ntherefore take a more dynamical approach to the modelling of atom-cavity\nsystems and do not reduce the electromagnetic field inside the resonator to a\nsingle mode. Our approach shows that the decay rate Gamma_cav of an emitter\ninside a subwavelength cavity with metallic mirrors can be much larger than its\nfree space decay rate Gamma_free due to constructive interference effects of\nthe emitted light. In general, however, we find that Gamma_cav = Gamma_free to\na very good approximation which might explain why many atom-cavity experiments\nhave not been able to operate in the so-called strong coupling regime."}
{"id": "2510.16971", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.16971", "abs": "https://arxiv.org/abs/2510.16971", "authors": ["M. Mahmoudzadeh Baghbani", "K. Atazadeh", "M. Mousavi"], "title": "Cosmological solutions in $f(Q)$ gravity via Noether symmetry approach", "comment": "14 pages, 3 figures, Accepted for publication by IJGMMP", "summary": "Symmetry plays a crucial role in theoretical physics, especially Noether\nsymmetry, which is a powerful approach for identifying the models at the\nfundamental level. The exact solution is provided within the point-like\nLagrangian framework. In this work, we study one of the alternative theories of\ngravity based on the non-metricity scalar $Q$, namely $f(Q)$ gravity, via\nNoether symmetry. We utilize Noether symmetry within the framework of $f(Q)$\ngravity to derive the functional expression for $f(Q)$, which is given by\n$f(Q)=c(Q-nQ)^{\\frac{3}{2-2n}}$. To confirm the exact solution of the model\nthrough Noether symmetry, we continue to consider the\nFriedmann-Robertson-Walker (FRW) cosmology with the dynamical solution of the\nsystem using dimensionless variables and show that the accelerated expansion of\nthe universe follows a power law scale factor. In the following, we show that\nthe quantities corresponding to the exact solution for $n<1$ lead to an\naccelerated expansion universe. Finally, in the framework of $f(Q)$\nscalar-tensor cosmology, we apply the Noether symmetry approach to find the\ncosmological models consistent with the Noether symmetry."}
{"id": "2510.15986", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15986", "abs": "https://arxiv.org/abs/2510.15986", "authors": ["Sifeddine Sellami", "Juba Agoun", "Lamia Yessad", "Louenas Bounia"], "title": "User Profiles of Sleep Disorder Sufferers: Towards Explainable Clustering and Differential Variable Analysis", "comment": "in French language, Plate-Forme Intelligence Artificielle, Jun 2025,\n  Dijon (FRANCE), France", "summary": "Sleep disorders have a major impact on patients' health and quality of life,\nbut their diagnosis remains complex due to the diversity of symptoms. Today,\ntechnological advances, combined with medical data analysis, are opening new\nperspectives for a better understanding of these disorders. In particular,\nexplainable artificial intelligence (XAI) aims to make AI model decisions\nunderstandable and interpretable for users. In this study, we propose a\nclustering-based method to group patients according to different sleep disorder\nprofiles. By integrating an explainable approach, we identify the key factors\ninfluencing these pathologies. An experiment on anonymized real data\nillustrates the effectiveness and relevance of our approach."}
{"id": "2510.16696", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2510.16696", "abs": "https://arxiv.org/abs/2510.16696", "authors": ["Jierui Hu", "Hao Yuan", "Joshua Akin", "A. K. M. Naziul Haque", "Yunlei Zhao", "Kejie Fang"], "title": "High-performance quantum frequency conversion using programmable unpoled nanophotonic waveguides", "comment": null, "summary": "Quantum frequency conversion (QFC) is essential for interfacing quantum\nsystems operating at different wavelengths and for realizing scalable quantum\nnetworks. Despite extensive progress, achieving QFC with simultaneous high\nefficiency, low pump power, minimal added noise, broad bandwidth, and\npump-wavelength flexibility remains a major challenge. Here, we demonstrate\nefficient, low-noise, and bidirectional QFC between the telecom (1550-nm) and\nvisible (780-nm) bands using unpoled indium gallium phosphide (InGaP)\n$\\chi^{(2)}$ nanophotonic waveguides, eliminating the need for a\nlong-wavelength pump. Leveraging the large nonlinear susceptibility of InGaP\ntogether with programmable modal-phase-matching control, we obtain record-low\npump power (20 mW) -- an order of magnitude lower than that in previous\ndemonstrations using integrated thin-film waveguides -- with record-high\nloss-inclusive normalized conversion efficiency among non-resonant QFC\nimplementations. With added noise well below the single-photon level, our\nplatform preserves the quantum coherence and entanglement of the input photons.\nThese results mark a significant advance in integrated nonlinear photonics for\nhigh-performance QFC, facilitating the development of versatile and scalable\nquantum networks."}
{"id": "2510.17066", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.17066", "abs": "https://arxiv.org/abs/2510.17066", "authors": ["Grigorios Panotopoulos", "Andrés Lueiza", "Nikolaos Dimakis", "Andronikos Paliathanasis"], "title": "Compact Stars in Symmetric Teleparallel Scalar-Tensor Gravity", "comment": "12 pages, 4 figures, Latex source file", "summary": "We investigate the existence of static, spherically symmetric compact objects\nwithin the framework of symmetric teleparallel scalar-tensor gravity. This\ntheory extends the Brans-Dicke and scalar-tensor models within the symmetric\nteleparallel formalism. We consider a nontrivial connection that allows for\ngenuinely nontrivial solutions in the limit of General Relativity. The field\nequations admit a minisuperspace description and by applying the method of\nvariational symmetries we construct the corresponding conservation laws in\nvacuum. The application of these conservation laws enables the reconstruction\nof analytic black-hole solutions. Finally, we study the interior structure of\ncompact objects matched to an extremal Reissner-Nordstr\\\"{o}m exterior and show\nthat the symmetric teleparallel scalar-tensor theory supports the existence of\nviable astrophysical objects."}
{"id": "2510.15987", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15987", "abs": "https://arxiv.org/abs/2510.15987", "authors": ["Samuel Lippl", "Thomas McGee", "Kimberly Lopez", "Ziwen Pan", "Pierce Zhang", "Salma Ziadi", "Oliver Eberle", "Ida Momennejad"], "title": "Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models", "comment": null, "summary": "How do latent and inference time computations enable large language models\n(LLMs) to solve multi-step reasoning? We introduce a framework for tracing and\nsteering algorithmic primitives that underlie model reasoning. Our approach\nlinks reasoning traces to internal activation patterns and evaluates\nalgorithmic primitives by injecting them into residual streams and measuring\ntheir effect on reasoning steps and task performance. We consider four\nbenchmarks: Traveling Salesperson Problem (TSP), 3SAT, AIME, and graph\nnavigation. We operationalize primitives by clustering neural activations and\nlabeling their matched reasoning traces. We then apply function vector methods\nto derive primitive vectors as reusable compositional building blocks of\nreasoning. Primitive vectors can be combined through addition, subtraction, and\nscalar operations, revealing a geometric logic in activation space. Cross-task\nand cross-model evaluations (Phi-4, Phi-4-Reasoning, Llama-3-8B) show both\nshared and task-specific primitives. Notably, comparing Phi-4 with its\nreasoning-finetuned variant highlights compositional generalization after\nfinetuning: Phi-4-Reasoning exhibits more systematic use of verification and\npath-generation primitives. Injecting the associated primitive vectors in\nPhi-4-Base induces behavioral hallmarks associated with Phi-4-Reasoning.\nTogether, these findings demonstrate that reasoning in LLMs may be supported by\na compositional geometry of algorithmic primitives, that primitives transfer\ncross-task and cross-model, and that reasoning finetuning strengthens\nalgorithmic generalization across domains."}
{"id": "2510.16739", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.16739", "abs": "https://arxiv.org/abs/2510.16739", "authors": ["Shingo Kukita", "Yuichiro Matsuzaki"], "title": "Mitigating Detuning-Induced Systematic Errors in Entanglement-Enhanced Metrology", "comment": "9 pages, 2 figures", "summary": "Quantum sensing leverages non-classical resources to enhance precision. In\nparticular, Greenberger-Horne-Zeilinger (GHZ) states can, in principle, attain\nthe Heisenberg limit that surpasses the standard quantum limit. While many\nstudies have examined how open-system noise-typically modeled with Lindblad\nmaster equations-degrades GHZ-based metrology, coherent control imperfections\nduring state preparation and readout have received less attention. Here, we\nanalyze the effect of detuning between actual and nominal spin frequencies in a\nGHZ-state preparation scheme employing a frequency selective pulse. We show\nthat detuning induces coherent, systematic error that prevents GHZ sensing from\nreaching the Heisenberg limit. To mitigate this effect, we design a\ncomposite-pulse protocol that compensates for detuning-induced errors and\nimproves the sensitivity under the effect of coherent error."}
{"id": "2510.17094", "categories": ["gr-qc", "astro-ph.CO"], "pdf": "https://arxiv.org/pdf/2510.17094", "abs": "https://arxiv.org/abs/2510.17094", "authors": ["Ryutaro Tomomatsu", "Teruaki Suyama", "Paolo Gondolo"], "title": "Gertsenshtein effect on the spacetime curved by background magnetic field with geometric optics", "comment": "20 pages, 5 figures", "summary": "When electromagnetic (or gravitational) waves propagate in the presence of a\nbackground magnetic field, a portion of the waves converts into gravitational\n(or electromagnetic) waves. This phenomenon, known as the (inverse)\nGertsenshtein effect, is typically analyzed in Minkowski spacetime, neglecting\nthe spacetime curvature induced by the magnetic field itself. This paper\ninvestigates, for the first time, the influence of spacetime curvature on the\n(inverse) Gertsenshtein effect. To this end, we first determine the metric\nperturbation from Minkowski spacetime up to second order in the magnetic field\nstrength, assuming cylindrical symmetry. We also discuss the ambiguities in the\nform of the metric perturbation arising from gauge freedom and boundary\nconditions. Using the geometric optics approximation, we then derive a set of\ncoupled equations governing the propagation of electromagnetic and\ngravitational waves in the resulting curved spacetime. These equations are\nsolved for two specific scenarios: a plane wave and a spherical wave. From the\nsolutions, we compute the evolution of the wave amplitudes and the associated\nenergy fluxes. Our analysis reveals that two competing effects govern the\namplitude evolution: magnification due to the focusing of waves by spacetime\ncurvature, and attenuation due to wave conversion via the Gertsenshtein effect.\nIn the plane wave case, these effects precisely cancel, resulting in no net\nchange in amplitude. In contrast, for the spherical wave, the Gertsenshtein\neffect dominates over focusing, leading to an overall reduction in amplitude."}
{"id": "2510.15990", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15990", "abs": "https://arxiv.org/abs/2510.15990", "authors": ["Kangqi Ni", "Zhen Tan", "Zijie Liu", "Pingzhi Li", "Tianlong Chen"], "title": "Can GRPO Help LLMs Transcend Their Pretraining Origin?", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR), primarily driven by\nthe Group Relative Policy Optimization (GRPO) algorithm, is a leading approach\nfor enhancing the reasoning abilities of Large Language Models (LLMs). Despite\nits wide adoption, GRPO's gains are often inconsistent; for instance, a model\nmay show significant improvement in one reasoning domain, like mathematics, yet\nremain stagnant in another, such as medicine. This inconsistency raises a\ncritical question: under what conditions does GRPO improve reasoning and\ngeneralize out-of-distribution (OOD)? We investigate this from a data\ndistribution perspective. We first prove theoretically that GRPO is a\nconservative reweighting scheme, bounded by the base model's distribution and\nthus unable to discover completely novel solutions. We further validate this in\ncarefully designed controlled studies by training transformers from scratch,\nevaluating generalization across reasoning depth, input length, token\nrepresentation, and compositionality. Our results provide a principled\nexplanation for GRPO's boundaries: OOD improvement emerges only when the target\ntask aligns with the model's pretrained biases, while gains on in-distribution\n(ID) tasks diminish as performance saturates. This reframes GRPO not as a\nuniversal reasoning enhancer but as a tool that sharpens pretraining biases.\nOur findings motivate future development of algorithms that can expand a\nmodel's capabilities beyond its pretraining origin."}
{"id": "2510.16754", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.16754", "abs": "https://arxiv.org/abs/2510.16754", "authors": ["Soroush Khademi", "Jesse J. Slim", "Kiarn T. Laverick", "Jin Chang", "Jingkun Guo", "Simon Gröblacher", "Howard M. Wiseman", "Warwick P. Bowen"], "title": "Post-processed estimation of quantum state trajectories", "comment": "Main Text (8 pages, 4 figures), Methods & References (14 pages, 6\n  figures), Supplementary Information (6 pages, 2 figures)", "summary": "Weak quantum measurements enable real-time tracking and control of dynamical\nquantum systems, producing quantum trajectories -- evolutions of the quantum\nstate of the system conditioned on measurement outcomes. For classical systems,\nthe accuracy of trajectories can be improved by incorporating future\ninformation, a procedure known as smoothing. Here we apply this concept to\nquantum systems, generalising a formalism of quantum state smoothing for an\nobserver monitoring a quantum system exposed to environmental decoherence, a\nscenario important for many quantum information protocols. This allows future\ndata to be incorporated when reconstructing the trajectories of quantum states.\nWe experimentally demonstrate that smoothing improves accuracy using a\ncontinuously measured nanomechanical resonator, showing that the method\ncompensates for both gaps in the measurement record and inaccessible\nenvironments. We further observe a key predicted departure from classical\nsmoothing: quantum noise renders the trajectories nondifferentiable. These\nresults establish that future information can enhance quantum trajectory\nreconstruction, with potential applications across quantum sensing, control,\nand error correction."}
{"id": "2510.17097", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.17097", "abs": "https://arxiv.org/abs/2510.17097", "authors": ["Hodek M. García", "Marcelo Salgado"], "title": "Solar system tests and neutron stars in $f(R)$ gravity revisited", "comment": "It is already accepted in Journal of General Relativity and\n  Gravitation Springer", "summary": "By implementing a full non-linear treatment of $f(R)$ gravity in static and\nspherically symmetric spacetimes, we analyze two scenarios. The first one\nwithin the context of the solar-system tests where we try to recover the\nchameleon effects without any approximations in the equations (e.g.\nlinearization) from $f(R)$ models that are compatible with cosmology. The\nsecond scenario deals with a quadratic $f(R)$ model that is tested in neutron\nstars. This scenario, which is associated with strong gravity, is completely\nindependent from the first one, but exploits the fact that the equations and\nformalism are basically the same in both applications. The difference between\nthe two goals lies mainly in the values of the constants involved in the\nspecific $f(R)$ models and the equation of state (EOS) of the central object\n(Sun or neutron star), but the numerical techniques and the general form of the\nfield equations remain valid in both situations. For the neutron star problem\nwe employ for the first time and in the context of $f(R)$ gravity a multiple\nalgebraic polytropic EOS that mimics accurately realistic EOS in several\ndensity ranges. By doing so we avoid the numerical interpolation needed when a\nrealistic EOS is given in tabulated form. Furthermore, we compare our results\nwith the latest data, which includes the most massive neutron star known to\ndate of about $2.35 M_\\odot$ from PSRJ0952-0607."}
{"id": "2510.15992", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15992", "abs": "https://arxiv.org/abs/2510.15992", "authors": ["Ziming Dai", "Tuo Zhang", "Fei Gao", "Xingyi Cai", "Xiaofei Wang", "Cheng Zhang", "Wenyu Wang", "Chengjie Zang"], "title": "Stratos: An End-to-End Distillation Pipeline for Customized LLMs under Distributed Cloud Environments", "comment": null, "summary": "The growing industrial demand for customized and cost-efficient large\nlanguage models (LLMs) is fueled by the rise of vertical, domain-specific tasks\nand the need to optimize performance under constraints such as latency and\nbudget. Knowledge distillation, as an efficient model compression and transfer\ntechnique, offers a feasible solution. However, existing distillation\nframeworks often require manual intervention and struggle to meet such complex\nuser-defined distillation requirements. To bridge this gap, we propose Stratos,\nan end-to-end LLM distillation pipeline that automates server and model\nselection, knowledge distillation, and deployment in distributed cloud\nenvironments. Given user-defined constraints on model performance and system\nbudget, Stratos automatically selects Pareto-optimal servers, dynamically\nmatches teacher-student pairs, and adapts distillation strategies based on task\ncomplexity to optimize cloud hosting. Experiments show that Stratos produces a\nstudent model that achieves four times the accuracy of its GPT-4o teacher\nbaseline on a rare, domain-specific Mahjong reasoning task with reverse\nsynthetic data and knowledge injection. Moreover, it achieves reduced latency\nand cost without compromising accuracy. These results highlight its promise for\nvertical-domain LLM deployment."}
{"id": "2510.16759", "categories": ["quant-ph", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2510.16759", "abs": "https://arxiv.org/abs/2510.16759", "authors": ["Peter Jaksch"], "title": "Successive generation of nontrivial Riemann zeros from a Wu-Sprung type potential", "comment": null, "summary": "A series of numerical experiments are performed, where a symmetric potential\nis generated for the 1D time-independent Schr\\\"odinger equation, with an\neigenspectrum that matches the imaginary part of the first nontrivial zeros of\nthe Riemann Zeta Function. The potential is generated as a series of correction\nfunctions, where the starting point is a potential that matches the smooth\nRiemann -- von Mangoldt approximation. It is found that the correction\nfunctions display a clear pattern that can be explained in simple terms, almost\nentirely dependent on the approximation error in the Riemann -- von Mangoldt\nformula. This also provides an explanation for the fractal pattern in the\npotential that was observed by Wu and Sprung."}
{"id": "2510.17125", "categories": ["gr-qc", "astro-ph.CO", "astro-ph.HE"], "pdf": "https://arxiv.org/pdf/2510.17125", "abs": "https://arxiv.org/abs/2510.17125", "authors": ["Zhaoqi Su", "Xikai Shan", "Zhenwei Lyu", "Junyao Zhang", "Yebin Liu", "Shude Mao", "Huan Yang"], "title": "From Stars to Waves: Non-deterministic Inference of Microlensed Gravitational Waves", "comment": "19 pages, 11 figures, comments are welcome!", "summary": "Strongly lensed gravitational waves may pass through the stellar field of a\nlensing galaxy with additional modulations (on both phase and amplitude) due to\ngravitational microlensing effect of stars/remnants near the line of sight.\nThese microlensed waveforms depend on the mass and location of thousands or\nmore most relevant stars, so that their deterministic reconstruction from the\ndata is computationally prohibitive. We classify the detection and parameter\nestimation of such events as non-deterministic inference problem and propose a\nsolution with the implementation of normalizing flows. As a first step, we show\nthat $8\\%$ of microlensed events can be detected with significance $\\ge 3\n\\sigma$ in the third generation era, with the chosen microlensing parameters\ncorrelated with the density of the underlying stellar field. This approach\nopens the door to probing microlensing effects and the properties of the\nunderlying stellar fields. A similar construction may also be applied to other\nnon-deterministic inference problems, such as detecting post-merger\ngravitational waves from binary neutron star coalescence and signals from\ncore-collapse supernovae."}
{"id": "2510.15996", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15996", "abs": "https://arxiv.org/abs/2510.15996", "authors": ["Ozan K. Tonguz", "Federico Taschin"], "title": "Using Kolmogorov-Smirnov Distance for Measuring Distribution Shift in Machine Learning", "comment": null, "summary": "One of the major problems in Machine Learning (ML) and Artificial\nIntelligence (AI) is the fact that the probability distribution of the test\ndata in the real world could deviate substantially from the probability\ndistribution of the training data set. When this happens, the predictions of an\nML system or an AI agent could involve large errors which is very troublesome\nand undesirable. While this is a well-known hard problem plaguing the AI and ML\nsystems' accuracy and reliability, in certain applications such errors could be\ncritical for safety and reliability of AI and ML systems. One approach to deal\nwith this problem is to monitor and measure the deviation in the probability\ndistribution of the test data in real time and to compensate for this\ndeviation. In this paper, we propose and explore the use of Kolmogorov-Smirnov\n(KS) Test for measuring the distribution shift and we show how the KS distance\ncan be used to quantify the distribution shift and its impact on an AI agent's\nperformance. Our results suggest that KS distance could be used as a valuable\nstatistical tool for monitoring and measuring the distribution shift. More\nspecifically, it is shown that even a distance of KS=0.02 could lead to about\n50\\% increase in the travel time at a single intersection using a Reinforcement\nLearning agent which is quite significant. It is hoped that the use of KS Test\nand KS distance in AI-based smart transportation could be an important step\nforward for gauging the performance degradation of an AI agent in real time and\nthis, in turn, could help the AI agent to cope with the distribution shift in a\nmore informed manner."}
{"id": "2510.16782", "categories": ["quant-ph", "cs.CC", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16782", "abs": "https://arxiv.org/abs/2510.16782", "authors": ["Tongyang Li", "Xinzhao Wang", "Yexin Zhang"], "title": "Near-Optimal Quantum Algorithms for Computing (Coarse) Correlated Equilibria of General-Sum Games", "comment": "Accepted at NeurIPS 2025, 27 pages", "summary": "Computing Nash equilibria of zero-sum games in classical and quantum settings\nis extensively studied. For general-sum games, computing Nash equilibria is\nPPAD-hard and the computing of a more general concept called correlated\nequilibria has been widely explored in game theory. In this paper, we initiate\nthe study of quantum algorithms for computing $\\varepsilon$-approximate\ncorrelated equilibria (CE) and coarse correlated equilibria (CCE) in\nmulti-player normal-form games. Our approach utilizes quantum improvements to\nthe multi-scale Multiplicative Weight Update (MWU) method for CE calculations,\nachieving a query complexity of $\\tilde{O}(m\\sqrt{n})$ for fixed $\\varepsilon$.\nFor CCE, we extend techniques from quantum algorithms for zero-sum games to\nmulti-player settings, achieving query complexity\n$\\tilde{O}(m\\sqrt{n}/\\varepsilon^{2.5})$. Both algorithms demonstrate a\nnear-optimal scaling in the number of players $m$ and actions $n$, as confirmed\nby our quantum query lower bounds."}
{"id": "2510.17159", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.17159", "abs": "https://arxiv.org/abs/2510.17159", "authors": ["Christopher Simmonds", "Matt Visser"], "title": "The spacetime geodesy of perfect fluid spheres", "comment": "V1: 30 pages total; 125 references", "summary": "Herein we argue for the utility of \"spacetime geodesy\", a point of view where\none delays as long as possible worrying about dynamical equations, in favour of\nthe maximal utilization of both symmetries and geometrical features. This\nclosely parallels Weinberg's distinction between \"cosmography'' and\n\"cosmology'', wherein maximal utilization of both the symmetries and\ngeometrical features of FLRW spacetimes is emphasized. This \"spacetime\ngeodesy'' point of view is particularly useful in those situations where, for\none reason or another, the dynamical equations of motion are either uncertain\nor completely unknown. Several examples are discussed -- we shall illustrate\nwhat can be done by considering the physics implications of demanding spatially\nisotropic Ricci tensors as a way of automatically implementing the (isotropic)\nperfect fluid condition, without committing to a specific equation of state. We\nalso consider the structure of the Weyl tensor in spherical symmetry, with and\nwithout the (isotropic) perfect fluid condition, and relate this to the notion\nof \"complexity''."}
{"id": "2510.15998", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15998", "abs": "https://arxiv.org/abs/2510.15998", "authors": ["Nilo Schwencke", "Cyriaque Rousselot", "Alena Shilova", "Cyril Furtlehner"], "title": "AMStraMGRAM: Adaptive Multi-cutoff Strategy Modification for ANaGRAM", "comment": null, "summary": "Recent works have shown that natural gradient methods can significantly\noutperform standard optimizers when training physics-informed neural networks\n(PINNs). In this paper, we analyze the training dynamics of PINNs optimized\nwith ANaGRAM, a natural-gradient-inspired approach employing singular value\ndecomposition with cutoff regularization. Building on this analysis, we propose\na multi-cutoff adaptation strategy that further enhances ANaGRAM's performance.\nExperiments on benchmark PDEs validate the effectiveness of our method, which\nallows to reach machine precision on some experiments. To provide theoretical\ngrounding, we develop a framework based on spectral theory that explains the\nnecessity of regularization and extend previous shown connections with Green's\nfunctions theory."}
{"id": "2510.16784", "categories": ["quant-ph", "cs.DM", "05C15, 81P45"], "pdf": "https://arxiv.org/pdf/2510.16784", "abs": "https://arxiv.org/abs/2510.16784", "authors": ["Lord Sen", "Shyamapada Mukherjee"], "title": "Symmetric Reduction Techniques for Quantum Graph Colouring", "comment": "10 pages, 8 figures", "summary": "This paper introduces an efficient quantum computing method for reducing\nspecial graphs in the context of the graph coloring problem. The special graphs\nconsidered include both symmetric and non-symmetric graphs where the axis\npasses through nodes only, edges only, and both together. The presented method\nreduces the number of coloring matrices, which is important for realization of\nthe number of quantum states required, from $K^{N}$ to $K^{\\frac{N+m}{2}}$ upon\none symmetric reduction of graphs symmetric about an axis passing through $m$\nnodes, where $K$ is the number of colours required and \\emph{N} being total\nnumber of nodes. Similarly for other types also, the number of quantum states\nis reduced. The complexity in the number of qubits has been reduced by $\\delta\nC_q= \\frac{9N^2}{8}-\\frac{3m^2}{8}-\\frac{3Nm}{4}-\\frac{N}{4}+\\frac{m}{4}$ upon\none symmetric reduction of graphs, symmetric about an axis passing through $m$\nnodes and other types as presented in the paper. Additionally, the number of\ngates and number of iterations are reduced massively compared to\nstate-of-the-art quantum algorithms. Like for a graph with 20 nodes and\nsymmetric line passing through 2 nodes, the number of iterations decreased from\n5157 to 67. Therefore, the procedure presented for solving the graph coloring\nproblem now requires a significantly reduced number of qubits compared to\nbefore. The run time of the proposed algorithm for these special type of graphs\nare reduced from $O(1.9575^{N})$ to $O(1.9575^{(\\frac{N+m}{2})})$ upon one\nsymmetric reduction of graphs symmetric about an axis passing through $m$ nodes\nand similarly for others cases."}
{"id": "2510.17362", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.17362", "abs": "https://arxiv.org/abs/2510.17362", "authors": ["Ayan Banerjee", "Bobur Turimov", "Sulton Usanov", "Murodbek Vapaev", "Yunus Turaev", "Zebo Avezmuratova"], "title": "Dark Energy Stars in Rastall-Rainbow Gravity: Structure, Stability and Observational Constraints", "comment": "11 Pages, 6 Figures and 3 Tables", "summary": "In this work, we investigate static configurations of dark energy stars\nwithin the framework of Rastall-Rainbow (R-R) gravity, which combines an\nenergy-dependent deformation of spacetime with a nonminimal coupling between\nmatter and geometry. We begin by deriving the modified field equations\ncorresponding to R-R gravity and subsequently reformulate the stellar structure\nequations to describe hydrostatic equilibrium. The generalized\nTolman-Oppenheimer-Volkoff (TOV) equations are then solved numerically by\nadopting the modified Chaplygin equation of state to model the interior matter\ndistribution. The R-R parameters, along with fluid constants, are shown to\ninfluence the maximum mass, radii, and stiffness of the star sequences compared\nto the baseline set by general relativity. We apply observational benchmarks\nfrom high-mass pulsars and binary-merger events (e.g., GW170817 and GW190814)\nto appraise viability within the explored parameter space. The results\ncollectively suggest that stable, causal configurations arise from physically\nmeaningful parameter selections, with deviations from general relativity\nleading to systematic changes in structural characteristics while adhering to\ntheoretical limits. These findings illustrate that Rastall-Rainbow gravity can\nsupport stable, observationally consistent dark energy stars, providing\nverifiable signatures in strong gravitational fields."}
{"id": "2510.16007", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16007", "abs": "https://arxiv.org/abs/2510.16007", "authors": ["Ziao Yang", "Longbo Huang", "Hongfu Liu"], "title": "Layer-Aware Influence for Online Data Valuation Estimation", "comment": null, "summary": "Data-centric learning emphasizes curating high-quality training samples to\nboost performance rather than designing new architectures. A central problem is\nto estimate the influence of training sample efficiently. Prior studies largely\nfocus on static influence measured on a converged model, overlooking how data\nvaluation dynamically changes during optimization. This omission neglects the\ndynamic nature of sample influence during optimization, especially in deep\nmodels. To address the computational burden of frequent influence estimation,\nwe develop a layer-aware online estimator that requires only loss-to-output\ngradients. This design avoids parameter-level and full-network gradients while\npreserving ranking fidelity. Extensive experiments across LLM pretraining,\nfine-tuning, and image classification show our method improves accuracy with\nsubstantially lower time and memory cost, making dynamic data curation\nefficient and scalable in practice."}
{"id": "2510.16788", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.16788", "abs": "https://arxiv.org/abs/2510.16788", "authors": ["Jonathan Nemirovsky", "Maya Chuchem", "Lee Peleg", "Yakov Solomons", "Amit Ben Kish", "Yotam Shapira"], "title": "Phase gadget compilation of quantum circuits using multiqubit gates", "comment": "Comments are welcome", "summary": "Quantum circuit synthesis and compilation are critical components in the\nquantum computing stack, both for contemporary quantum systems, where efficient\nuse of limited resources is essential, as well as for large-scale\nfault-tolerant platforms, where computation time can be minimized. The specific\ncharacteristics of the quantum hardware determine which circuit designs and\noptimizations are feasible. We present a phase-gadget based method for\ncompilation of quantum circuits using programmable multiqubit entangling gates,\nthat are native, among others, to trapped-ions quantum computers. We use\nphase-gadgets in order to generically reduce circuit depths and efficiently\nimplement them with few, high-fidelity, multiqubit gates. We test our methods\non a large set of benchmark circuits and demonstrate generic circuit depth\nreduction and implementation error reduction."}
{"id": "2510.17398", "categories": ["gr-qc", "astro-ph.CO"], "pdf": "https://arxiv.org/pdf/2510.17398", "abs": "https://arxiv.org/abs/2510.17398", "authors": ["Shubham Kejriwal", "Enrico Barausse", "Alvin J. K. Chua"], "title": "Hierarchical modeling of gravitational-wave populations for disentangling environmental and modified-gravity effects", "comment": "10 + 7 pages, 6 figures", "summary": "The upcoming Laser Interferometer Space Antenna (LISA) will detect up to\nthousands of extreme-mass-ratio inspirals (EMRIs). These sources will spend\n$\\sim 10^5$ cycles in band, and are therefore sensitive to tiny changes in the\ngeneral-relativistic dynamics, potentially induced by astrophysical\nenvironments or modifications of general relativity (GR). Previous studies have\nshown that these effects can be highly degenerate for a single source. However,\nit may be possible to distinguish between them at the population level, because\nenvironmental effects should impact only a fraction of the sources, while\nmodifications of GR would affect all. We therefore introduce a population-based\nhierarchical framework to disentangle the two hypotheses. Using simulated EMRI\npopulations, we perform tests of the null vacuum-GR hypothesis and two\nalternative beyond-vacuum-GR hypotheses, namely migration torques\n(environmental effects) and time-varying $G$ (modified gravity). We find that\nwith as few as $\\approx 20$ detected sources, our framework can statistically\ndistinguish between these three hypotheses, and even indicate if both\nenvironmental and modified gravity effects are simultaneously present in the\npopulation. Our framework can be applied to other models of beyond-vacuum-GR\neffects available in the literature."}
{"id": "2510.16014", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16014", "abs": "https://arxiv.org/abs/2510.16014", "authors": ["Hanyin Cheng", "Ruitong Zhang", "Yuning Lu", "Peng Chen", "Meng Wang", "Yang Shu", "Bin Yang", "Chenjuan Guo"], "title": "STAR: Boosting Time Series Foundation Models for Anomaly Detection through State-aware Adapter", "comment": null, "summary": "While Time Series Foundation Models (TSFMs) have demonstrated remarkable\nsuccess in Multivariate Time Series Anomaly Detection (MTSAD), however, in\nreal-world industrial scenarios, many time series comprise not only numerical\nvariables such as temperature and flow, but also numerous discrete state\nvariables that describe the system status, such as valve on/off or day of the\nweek. Existing TSFMs often overlook the distinct categorical nature of state\nvariables and their critical role as conditions, typically treating them\nuniformly with numerical variables. This inappropriate modeling approach\nprevents the model from fully leveraging state information and even leads to a\nsignificant degradation in detection performance after state variables are\nintegrated. To address this critical limitation, this paper proposes a novel\nSTate-aware AdapteR (STAR). STAR is a plug-and-play module designed to enhance\nthe capability of TSFMs in modeling and leveraging state variables during the\nfine-tuning stage. Specifically, STAR comprisesthree core components: (1) We\ndesign an Identity-guided State Encoder, whicheffectively captures the complex\ncategorical semantics of state variables through a learnable State Memory. (2)\nWe propose a Conditional Bottleneck Adapter, which dynamically generates\nlow-rank adaptation parameters conditioned on the current state, thereby\nflexibly injecting the influence of state variables into the backbone model.\n(3) We also introduce a Numeral-State Matching module to more effectively\ndetect anomalies inherent to the state variables themselves. Extensive\nexperiments conducted on real-world datasets demonstrate that STAR can improve\nthe performance of existing TSFMs on MTSAD."}
{"id": "2510.16810", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.16810", "abs": "https://arxiv.org/abs/2510.16810", "authors": ["Jianchao Zhang", "Jun Suzuki"], "title": "Hybrid Cramér-Rao bound for Quantum Bayes-Point Estimation with Nuisance Parameters", "comment": "20 pages, 3 figures", "summary": "We develop a hybrid framework for quantum parameter estimation in the\npresence of nuisance parameters. In this Bayes-point scheme, the parameters of\ninterest are treated as fixed non-random parameters while nuisance parameters\nare integrated out with respect to a prior (random parameters). Within this\nsetting, we introduce the hybrid partial quantum Fisher information matrix\n(hpQFIM), defined by prior-averaging the nuisance block of the QFIM and taking\na Schur complement, and derive a corresponding Cram\\'er-Rao-type lower bound on\nthe hybrid risk. We establish structural properties of the hpQFIM, including\ninequalities that bracket it between computationally tractable surrogates, as\nwell as limiting behaviors under extreme priors. Operationally, the hybrid\napproach improves over pure point estimation since the optimal measurement for\nthe parameters of interest depends only on the prior distribution of the\nnuisance, rather than on its unknown value. We illustrate the framework with\nanalytically solvable qubit models and numerical examples, clarifying how\npartial prior information on nuisance variables can be systematically exploited\nin quantum metrology."}
{"id": "2510.17449", "categories": ["gr-qc", "astro-ph.HE"], "pdf": "https://arxiv.org/pdf/2510.17449", "abs": "https://arxiv.org/abs/2510.17449", "authors": ["Souradeep Pal"], "title": "Observable spins in gravitational waves from compact binary mergers", "comment": null, "summary": "We investigate the measurability of effective inspiral spin in the detectable\ncompact binary mergers using gravitational-wave observations. Measurements from\nthe latest gravitational-wave transient catalog do not rule out the existence\nof binary systems with non-zero effective spins. However, we observe an\napparent correlation between the inferred effective inspiral spin and the\nloudness of the gravitational-wave events-- loud events typically have\nclose-to-zero effective spins whereas fainter events tend to be inferred with\nrelatively arbitrary effective spins. Through simulations, we demonstrate that\nnon-negligible effective spins can be systematically inferred from non-spinning\nsystems at small signal strengths. These two observations support the\npossibility that the effective spin magnitudes in the observable compact\nbinaries are generally small. Future detections can have potential impact on\nthe understanding of their population and other astrophysical inferences."}
{"id": "2510.16015", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16015", "abs": "https://arxiv.org/abs/2510.16015", "authors": ["Qian Sun", "Graham Hults", "Susu Xu"], "title": "Decision-focused Sensing and Forecasting for Adaptive and Rapid Flood Response: An Implicit Learning Approach", "comment": null, "summary": "Timely and reliable decision-making is vital for flood emergency response,\nyet it remains severely hindered by limited and imprecise situational awareness\ndue to various budget and data accessibility constraints. Traditional flood\nmanagement systems often rely on in-situ sensors to calibrate remote\nsensing-based large-scale flood depth forecasting models, and further take\nflood depth estimates to optimize flood response decisions. However, these\napproaches often take fixed, decision task-agnostic strategies to decide where\nto put in-situ sensors (e.g., maximize overall information gain) and train\nflood forecasting models (e.g., minimize average forecasting errors), but\noverlook that systems with the same sensing gain and average forecasting errors\nmay lead to distinct decisions. To address this, we introduce a novel\ndecision-focused framework that strategically selects locations for in-situ\nsensor placement and optimize spatio-temporal flood forecasting models to\noptimize downstream flood response decision regrets. Our end-to-end pipeline\nintegrates four components: a contextual scoring network, a differentiable\nsensor selection module under hard budget constraints, a spatio-temporal flood\nreconstruction and forecasting model, and a differentiable decision layer\ntailored to task-specific objectives. Central to our approach is the\nincorporation of Implicit Maximum Likelihood Estimation (I-MLE) to enable\ngradient-based learning over discrete sensor configurations, and probabilistic\ndecision heads to enable differentiable approximation to various constrained\ndisaster response tasks."}
{"id": "2510.16836", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.16836", "abs": "https://arxiv.org/abs/2510.16836", "authors": ["Lin Shang", "Shuai Geng", "Xingli Li", "Jiasen Jin"], "title": "Steady-state phase transition in one-dimensional quantum contact process", "comment": "6 pages, 5 figures, comments are welcome", "summary": "We investigate the steady-state phases of the one-dimensional quantum contact\nprocess model with local dissipation. Exploiting the single-site and cluster\nmean-field approximations, we show the bistability of the absorbing and active\nphases in the system with strong interaction between neighboring sites,\naccompanied by the closing of the Liouvillian gap in the thermodynamic limit.\nMoreover we find that, near the transition point, the system may evolve first\nto the long-lived metastable state before reaching the eventual steady state,\nsuggesting us to prolong the time-evolution in the numerical simulation to find\nthe true steady state. We also present the extrapolated transition point by\nsystematically including the correlations in the system."}
{"id": "2510.17468", "categories": ["gr-qc", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2510.17468", "abs": "https://arxiv.org/abs/2510.17468", "authors": ["Yongbin Shao", "Xinyi Zhao", "Long Ma", "Ming Xin"], "title": "Robustness Analysis and Controller Design of Arm-locking System in Space-based Gravitational Wave Detectors", "comment": null, "summary": "Arm-locking frequency stabilization is a key technique for suppressing laser\nfrequency noise in space-based gravitational-wave detectors. The robustness of\nthe arm-locking control loop is crucial for maintaining laser frequency\nstability, which directly impacts the accuracy of gravitational-wave\nmeasurements. In this work, a parametric stability analysis framework is\ndeveloped by combining the D-subdivision theory with the Semi-Discretization\nmethod to map the stability regions of arm-locking systems in the parameter\nspace and identify their critical stability boundaries. Based on the\nfrequency-domain characteristics, a robust arm-locking controller is designed\nto enhance loop stability under parameter perturbations. Theoretical analysis\nand time-domain simulations confirm that the proposed controller maintains\nclosed-loop stability and realize suppression of laser frequency noise against\nparameter perturbation."}
{"id": "2510.16016", "categories": ["cs.LG", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2510.16016", "abs": "https://arxiv.org/abs/2510.16016", "authors": ["Saeed Salehi"], "title": "Transfer learning strategies for accelerating reinforcement-learning-based flow control", "comment": null, "summary": "This work investigates transfer learning strategies to accelerate deep\nreinforcement learning (DRL) for multifidelity control of chaotic fluid flows.\nProgressive neural networks (PNNs), a modular architecture designed to preserve\nand reuse knowledge across tasks, are employed for the first time in the\ncontext of DRL-based flow control. In addition, a comprehensive benchmarking of\nconventional fine-tuning strategies is conducted, evaluating their performance,\nconvergence behavior, and ability to retain transferred knowledge. The\nKuramoto-Sivashinsky (KS) system is employed as a benchmark to examine how\nknowledge encoded in control policies, trained in low-fidelity environments,\ncan be effectively transferred to high-fidelity settings. Systematic\nevaluations show that while fine-tuning can accelerate convergence, it is\nhighly sensitive to pretraining duration and prone to catastrophic forgetting.\nIn contrast, PNNs enable stable and efficient transfer by preserving prior\nknowledge and providing consistent performance gains, and are notably robust to\noverfitting during the pretraining phase. Layer-wise sensitivity analysis\nfurther reveals how PNNs dynamically reuse intermediate representations from\nthe source policy while progressively adapting deeper layers to the target\ntask. Moreover, PNNs remain effective even when the source and target\nenvironments differ substantially, such as in cases with mismatched physical\nregimes or control objectives, where fine-tuning strategies often result in\nsuboptimal adaptation or complete failure of knowledge transfer. The results\nhighlight the potential of novel transfer learning frameworks for robust,\nscalable, and computationally efficient flow control that can potentially be\napplied to more complex flow configurations."}
{"id": "2510.16867", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2510.16867", "abs": "https://arxiv.org/abs/2510.16867", "authors": ["Alberto De Toni", "Edoardo Bortolozzo", "Alessandro Emanuele", "Marco Venturini", "Luca Calderaro", "Marco Avesani", "Giuseppe Vallone", "Paolo Villoresi"], "title": "Long-term analysis of efficient-BB84 4-node network with optical switches in metropolitan environment", "comment": null, "summary": "Quantum Key Distribution (QKD) is a leading technology for enabling\ninformation-theoretic secure communication, with protocols such as BB84 and its\nvariants already deployed in practical field implementations. As QKD evolves\nfrom point-to-point links to multi-node networks, scalability and\ncost-effectiveness become central challenges. Among the approaches to address\nthese issues, efficient-BB84 has shown durable and reliable performances, while\noptical switching techniques enable flexible, scalable, and cost-efficient\nintegration of QKD into existing infrastructures. In this work, we present an\nactive QKD network in a production environment, employing efficient-BB84 and\noptical switching, orchestrated in a coordinated manner, emphasizing their\npotential to support robust, future-proof quantum-secure communication systems."}
{"id": "2510.17487", "categories": ["gr-qc", "astro-ph.IM", "hep-ex"], "pdf": "https://arxiv.org/pdf/2510.17487", "abs": "https://arxiv.org/abs/2510.17487", "authors": ["The LIGO Scientific Collaboration", "the Virgo Collaboration", "the KAGRA Collaboration", "A. G. Abac", "I. Abouelfettouh", "F. Acernese", "K. Ackley", "C. Adamcewicz", "S. Adhicary", "D. Adhikari", "N. Adhikari", "R. X. Adhikari", "V. K. Adkins", "S. Afroz", "A. Agapito", "D. Agarwal", "M. Agathos", "N. Aggarwal", "S. Aggarwal", "O. D. Aguiar", "I. -L. Ahrend", "L. Aiello", "A. Ain", "P. Ajith", "T. Akutsu", "S. Albanesi", "W. Ali", "S. Al-Kershi", "C. Alléné", "A. Allocca", "S. Al-Shammari", "P. A. Altin", "S. Alvarez-Lopez", "W. Amar", "O. Amarasinghe", "A. Amato", "F. Amicucci", "C. Amra", "A. Ananyeva", "S. B. Anderson", "W. G. Anderson", "M. Andia", "M. Ando", "M. Andrés-Carcasona", "T. Andrić", "J. Anglin", "S. Ansoldi", "J. M. Antelis", "S. Antier", "M. Aoumi", "E. Z. Appavuravther", "S. Appert", "S. K. Apple", "K. Arai", "A. Araya", "M. C. Araya", "M. Arca Sedda", "J. S. Areeda", "N. Aritomi", "F. Armato", "S. Armstrong", "N. Arnaud", "M. Arogeti", "S. M. Aronson", "G. Ashton", "Y. Aso", "L. Asprea", "M. Assiduo", "S. Assis de Souza Melo", "S. M. Aston", "P. Astone", "F. Attadio", "F. Aubin", "K. AultONeal", "G. Avallone", "E. A. Avila", "S. Babak", "C. Badger", "S. Bae", "S. Bagnasco", "L. Baiotti", "R. Bajpai", "T. Baka", "A. M. Baker", "K. A. Baker", "T. Baker", "G. Baldi", "N. Baldicchi", "M. Ball", "G. Ballardin", "S. W. Ballmer", "S. Banagiri", "B. Banerjee", "D. Bankar", "T. M. Baptiste", "P. Baral", "M. Baratti", "J. C. Barayoga", "B. C. Barish", "D. Barker", "N. Barman", "P. Barneo", "F. Barone", "B. Barr", "L. Barsotti", "M. Barsuglia", "D. Barta", "A. M. Bartoletti", "M. A. Barton", "I. Bartos", "A. Basalaev", "R. Bassiri", "A. Basti", "M. Bawaj", "P. Baxi", "J. C. Bayley", "A. C. Baylor", "P. A. Baynard II", "M. Bazzan", "V. M. Bedakihale", "F. Beirnaert", "M. Bejger", "D. Belardinelli", "A. S. Bell", "D. S. Bellie", "L. Bellizzi", "W. Benoit", "I. Bentara", "J. D. Bentley", "M. Ben Yaala", "S. Bera", "F. Bergamin", "B. K. Berger", "S. Bernuzzi", "M. Beroiz", "D. Bersanetti", "T. Bertheas", "A. Bertolini", "J. Betzwieser", "D. Beveridge", "G. Bevilacqua", "N. Bevins", "R. Bhandare", "R. Bhatt", "D. Bhattacharjee", "S. Bhattacharyya", "S. Bhaumik", "V. Biancalana", "A. Bianchi", "I. A. Bilenko", "G. Billingsley", "A. Binetti", "S. Bini", "C. Binu", "S. Biot", "O. Birnholtz", "S. Biscoveanu", "A. Bisht", "M. Bitossi", "M. -A. Bizouard", "S. Blaber", "J. K. Blackburn", "L. A. Blagg", "C. D. Blair", "D. G. Blair", "N. Bode", "N. Boettner", "G. Boileau", "M. Boldrini", "G. N. Bolingbroke", "A. Bolliand", "L. D. Bonavena", "R. Bondarescu", "F. Bondu", "E. Bonilla", "M. S. Bonilla", "A. Bonino", "R. Bonnand", "A. Borchers", "S. Borhanian", "V. Boschi", "S. Bose", "V. Bossilkov", "Y. Bothra", "A. Boudon", "L. Bourg", "M. Boyle", "A. Bozzi", "C. Bradaschia", "P. R. Brady", "A. Branch", "M. Branchesi", "I. Braun", "T. Briant", "A. Brillet", "M. Brinkmann", "P. Brockill", "E. Brockmueller", "A. F. Brooks", "B. C. Brown", "D. D. Brown", "M. L. Brozzetti", "S. Brunett", "G. Bruno", "R. Bruntz", "J. Bryant", "Y. Bu", "F. Bucci", "J. Buchanan", "O. Bulashenko", "T. Bulik", "H. J. Bulten", "A. Buonanno", "K. Burtnyk", "R. Buscicchio", "D. Buskulic", "C. Buy", "R. L. Byer", "G. S. Cabourn Davies", "R. Cabrita", "V. Cáceres-Barbosa", "L. Cadonati", "G. Cagnoli", "C. Cahillane", "A. Calafat", "T. A. Callister", "E. Calloni", "S. R. Callos", "G. Caneva Santoro", "K. C. Cannon", "H. Cao", "L. A. Capistran", "E. Capocasa", "E. Capote", "G. Capurri", "G. Carapella", "F. Carbognani", "M. Carlassara", "J. B. Carlin", "T. K. Carlson", "M. F. Carney", "M. Carpinelli", "G. Carrillo", "J. J. Carter", "G. Carullo", "A. Casallas-Lagos", "J. Casanueva Diaz", "C. Casentini", "S. Y. Castro-Lucas", "S. Caudill", "M. Cavaglià", "R. Cavalieri", "A. Ceja", "G. Cella", "P. Cerdá-Durán", "E. Cesarini", "N. Chabbra", "W. Chaibi", "A. Chakraborty", "P. Chakraborty", "S. Chakraborty", "S. Chalathadka Subrahmanya", "J. C. L. Chan", "M. Chan", "K. Chang", "S. Chao", "P. Charlton", "E. Chassande-Mottin", "C. Chatterjee", "Debarati Chatterjee", "Deep Chatterjee", "M. Chaturvedi", "S. Chaty", "K. Chatziioannou", "A. Chen", "A. H. -Y. Chen", "D. Chen", "H. Chen", "H. Y. Chen", "S. Chen", "Yanbei Chen", "Yitian Chen", "H. P. Cheng", "P. Chessa", "H. T. Cheung", "S. Y. Cheung", "F. Chiadini", "G. Chiarini", "A. Chiba", "A. Chincarini", "M. L. Chiofalo", "A. Chiummo", "C. Chou", "S. Choudhary", "N. Christensen", "S. S. Y. Chua", "G. Ciani", "P. Ciecielag", "M. Cieślar", "M. Cifaldi", "B. Cirok", "F. Clara", "J. A. Clark", "T. A. Clarke", "P. Clearwater", "S. Clesse", "F. Cleva", "E. Coccia", "E. Codazzo", "P. -F. Cohadon", "S. Colace", "E. Colangeli", "M. Colleoni", "C. G. Collette", "J. Collins", "S. Colloms", "A. Colombo", "C. M. Compton", "G. Connolly", "L. Conti", "T. R. Corbitt", "I. Cordero-Carrión", "S. Corezzi", "N. J. Cornish", "I. Coronado", "A. Corsi", "R. Cottingham", "M. W. Coughlin", "A. Couineaux", "P. Couvares", "D. M. Coward", "R. Coyne", "A. Cozzumbo", "J. D. E. Creighton", "T. D. Creighton", "P. Cremonese", "S. Crook", "R. Crouch", "J. Csizmazia", "J. R. Cudell", "T. J. Cullen", "A. Cumming", "E. Cuoco", "M. Cusinato", "L. V. Da Conceição", "T. Dal Canton", "S. Dal Pra", "G. Dálya", "B. D'Angelo", "S. Danilishin", "S. D'Antonio", "K. Danzmann", "K. E. Darroch", "L. P. Dartez", "R. Das", "A. Dasgupta", "V. Dattilo", "A. Daumas", "N. Davari", "I. Dave", "A. Davenport", "M. Davier", "T. F. Davies", "D. Davis", "L. Davis", "M. C. Davis", "P. Davis", "E. J. Daw", "M. Dax", "J. De Bolle", "M. Deenadayalan", "J. Degallaix", "M. De Laurentis", "F. De Lillo", "S. Della Torre", "W. Del Pozzo", "A. Demagny", "F. De Marco", "G. Demasi", "F. De Matteis", "N. Demos", "T. Dent", "A. Depasse", "N. DePergola", "R. De Pietri", "R. De Rosa", "C. De Rossi", "M. Desai", "R. DeSalvo", "A. DeSimone", "R. De Simone", "A. Dhani", "R. Diab", "M. C. Díaz", "M. Di Cesare", "G. Dideron", "T. Dietrich", "L. Di Fiore", "C. Di Fronzo", "M. Di Giovanni", "T. Di Girolamo", "D. Diksha", "J. Ding", "S. Di Pace", "I. Di Palma", "D. Di Piero", "F. Di Renzo", "Divyajyoti", "A. Dmitriev", "J. P. Docherty", "Z. Doctor", "N. Doerksen", "E. Dohmen", "A. Doke", "A. Domiciano De Souza", "L. D'Onofrio", "F. Donovan", "K. L. Dooley", "T. Dooney", "S. Doravari", "O. Dorosh", "W. J. D. Doyle", "M. Drago", "J. C. Driggers", "L. Dunn", "U. Dupletsa", "P. -A. Duverne", "D. D'Urso", "P. Dutta Roy", "H. Duval", "S. E. Dwyer", "C. Eassa", "M. Ebersold", "T. Eckhardt", "G. Eddolls", "A. Effler", "J. Eichholz", "H. Einsle", "M. Eisenmann", "M. Emma", "K. Endo", "R. Enficiaud", "L. Errico", "R. Espinosa", "M. Esposito", "R. C. Essick", "H. Estellés", "T. Etzel", "M. Evans", "T. Evstafyeva", "B. E. Ewing", "J. M. Ezquiaga", "F. Fabrizi", "V. Fafone", "S. Fairhurst", "A. M. Farah", "B. Farr", "W. M. Farr", "G. Favaro", "M. Favata", "M. Fays", "M. Fazio", "J. Feicht", "M. M. Fejer", "R. Felicetti", "E. Fenyvesi", "J. Fernandes", "T. Fernandes", "D. Fernando", "S. Ferraiuolo", "T. A. Ferreira", "F. Fidecaro", "P. Figura", "A. Fiori", "I. Fiori", "M. Fishbach", "R. P. Fisher", "R. Fittipaldi", "V. Fiumara", "R. Flaminio", "S. M. Fleischer", "L. S. Fleming", "E. Floden", "H. Fong", "J. A. Font", "F. Fontinele-Nunes", "C. Foo", "B. Fornal", "K. Franceschetti", "F. Frappez", "S. Frasca", "F. Frasconi", "J. P. Freed", "Z. Frei", "A. Freise", "O. Freitas", "R. Frey", "W. Frischhertz", "P. Fritschel", "V. V. Frolov", "G. G. Fronzé", "M. Fuentes-Garcia", "S. Fujii", "T. Fujimori", "P. Fulda", "M. Fyffe", "B. Gadre", "J. R. Gair", "S. Galaudage", "V. Galdi", "R. Gamba", "A. Gamboa", "S. Gamoji", "D. Ganapathy", "A. Ganguly", "B. Garaventa", "J. García-Bellido", "C. García-Quirós", "J. W. Gardner", "K. A. Gardner", "S. Garg", "J. Gargiulo", "X. Garrido", "A. Garron", "F. Garufi", "P. A. Garver", "C. Gasbarra", "B. Gateley", "F. Gautier", "V. Gayathri", "T. Gayer", "G. Gemme", "A. Gennai", "V. Gennari", "J. George", "R. George", "O. Gerberding", "L. Gergely", "Archisman Ghosh", "Sayantan Ghosh", "Shaon Ghosh", "Shrobana Ghosh", "Suprovo Ghosh", "Tathagata Ghosh", "J. A. Giaime", "K. D. Giardina", "D. R. Gibson", "C. Gier", "S. Gkaitatzis", "J. Glanzer", "F. Glotin", "J. Godfrey", "R. V. Godley", "P. Godwin", "A. S. Goettel", "E. Goetz", "J. Golomb", "S. Gomez Lopez", "B. Goncharov", "G. González", "P. Goodarzi", "S. Goode", "A. W. Goodwin-Jones", "M. Gosselin", "R. Gouaty", "D. W. Gould", "K. Govorkova", "A. Grado", "V. Graham", "A. E. Granados", "M. Granata", "V. Granata", "S. Gras", "P. Grassia", "J. Graves", "C. Gray", "R. Gray", "G. Greco", "A. C. Green", "L. Green", "S. M. Green", "S. R. Green", "C. Greenberg", "A. M. Gretarsson", "H. K. Griffin", "D. Griffith", "H. L. Griggs", "G. Grignani", "C. Grimaud", "H. Grote", "S. Grunewald", "D. Guerra", "D. Guetta", "G. M. Guidi", "A. R. Guimaraes", "H. K. Gulati", "F. Gulminelli", "H. Guo", "W. Guo", "Y. Guo", "Anuradha Gupta", "I. Gupta", "N. C. Gupta", "S. K. Gupta", "V. Gupta", "N. Gupte", "J. Gurs", "N. Gutierrez", "N. Guttman", "F. Guzman", "D. Haba", "M. Haberland", "S. Haino", "E. D. Hall", "E. Z. Hamilton", "G. Hammond", "M. Haney", "J. Hanks", "C. Hanna", "M. D. Hannam", "O. A. Hannuksela", "A. G. Hanselman", "H. Hansen", "J. Hanson", "S. Hanumasagar", "R. Harada", "A. R. Hardison", "S. Harikumar", "K. Haris", "I. Harley-Trochimczyk", "T. Harmark", "J. Harms", "G. M. Harry", "I. W. Harry", "J. Hart", "B. Haskell", "C. J. Haster", "K. Haughian", "H. Hayakawa", "K. Hayama", "M. C. Heintze", "J. Heinze", "J. Heinzel", "H. Heitmann", "F. Hellman", "A. F. Helmling-Cornell", "G. Hemming", "O. Henderson-Sapir", "M. Hendry", "I. S. Heng", "M. H. Hennig", "C. Henshaw", "M. Heurs", "A. L. Hewitt", "J. Heynen", "J. Heyns", "S. Higginbotham", "S. Hild", "S. Hill", "Y. Himemoto", "N. Hirata", "C. Hirose", "D. Hofman", "B. E. Hogan", "N. A. Holland", "I. J. Hollows", "D. E. Holz", "L. Honet", "D. J. Horton-Bailey", "J. Hough", "S. Hourihane", "N. T. Howard", "E. J. Howell", "C. G. Hoy", "C. A. Hrishikesh", "P. Hsi", "H. -F. Hsieh", "H. -Y. Hsieh", "C. Hsiung", "S. -H. Hsu", "W. -F. Hsu", "Q. Hu", "H. Y. Huang", "Y. Huang", "Y. T. Huang", "A. D. Huddart", "B. Hughey", "V. Hui", "S. Husa", "R. Huxford", "L. Iampieri", "G. A. Iandolo", "M. Ianni", "G. Iannone", "J. Iascau", "K. Ide", "R. Iden", "A. Ierardi", "S. Ikeda", "H. Imafuku", "Y. Inoue", "G. Iorio", "P. Iosif", "M. H. Iqbal", "J. Irwin", "R. Ishikawa", "M. Isi", "K. S. Isleif", "Y. Itoh", "M. Iwaya", "B. R. Iyer", "C. Jacquet", "P. -E. Jacquet", "T. Jacquot", "S. J. Jadhav", "S. P. Jadhav", "M. Jain", "T. Jain", "A. L. James", "K. Jani", "J. Janquart", "N. N. Janthalur", "S. Jaraba", "P. Jaranowski", "R. Jaume", "W. Javed", "A. Jennings", "M. Jensen", "W. Jia", "J. Jiang", "H. -B. Jin", "G. R. Johns", "N. A. Johnson", "M. C. Johnston", "R. Johnston", "N. Johny", "D. H. Jones", "D. I. Jones", "R. Jones", "H. E. Jose", "P. Joshi", "S. K. Joshi", "G. Joubert", "J. Ju", "L. Ju", "K. Jung", "J. Junker", "V. Juste", "H. B. Kabagoz", "T. Kajita", "I. Kaku", "V. Kalogera", "M. Kalomenopoulos", "M. Kamiizumi", "N. Kanda", "S. Kandhasamy", "G. Kang", "N. C. Kannachel", "J. B. Kanner", "S. A. KantiMahanty", "S. J. Kapadia", "D. P. Kapasi", "M. Karthikeyan", "M. Kasprzack", "H. Kato", "T. Kato", "E. Katsavounidis", "W. Katzman", "R. Kaushik", "K. Kawabe", "R. Kawamoto", "D. Keitel", "L. J. Kemperman", "J. Kennington", "F. A. Kerkow", "R. Kesharwani", "J. S. Key", "R. Khadela", "S. Khadka", "S. S. Khadkikar", "F. Y. Khalili", "F. Khan", "T. Khanam", "M. Khursheed", "N. M. Khusid", "W. Kiendrebeogo", "N. Kijbunchoo", "C. Kim", "J. C. Kim", "K. Kim", "M. H. Kim", "S. Kim", "Y. -M. Kim", "C. Kimball", "K. Kimes", "M. Kinnear", "J. S. Kissel", "S. Klimenko", "A. M. Knee", "E. J. Knox", "N. Knust", "K. Kobayashi", "S. M. Koehlenbeck", "G. Koekoek", "K. Kohri", "K. Kokeyama", "S. Koley", "P. Kolitsidou", "A. E. Koloniari", "K. Komori", "A. K. H. Kong", "A. Kontos", "L. M. Koponen", "M. Korobko", "X. Kou", "A. Koushik", "N. Kouvatsos", "M. Kovalam", "T. Koyama", "D. B. Kozak", "S. L. Kranzhoff", "V. Kringel", "N. V. Krishnendu", "S. Kroker", "A. Królak", "K. Kruska", "J. Kubisz", "G. Kuehn", "S. Kulkarni", "A. Kulur Ramamohan", "Achal Kumar", "Anil Kumar", "Praveen Kumar", "Prayush Kumar", "Rahul Kumar", "Rakesh Kumar", "J. Kume", "K. Kuns", "N. Kuntimaddi", "S. Kuroyanagi", "S. Kuwahara", "K. Kwak", "K. Kwan", "S. Kwon", "G. Lacaille", "D. Laghi", "A. H. Laity", "E. Lalande", "M. Lalleman", "P. C. Lalremruati", "M. Landry", "B. B. Lane", "R. N. Lang", "J. Lange", "R. Langgin", "B. Lantz", "I. La Rosa", "J. Larsen", "A. Lartaux-Vollard", "P. D. Lasky", "J. Lawrence", "M. Laxen", "C. Lazarte", "A. Lazzarini", "C. Lazzaro", "P. Leaci", "L. Leali", "Y. K. Lecoeuche", "H. M. Lee", "H. W. Lee", "J. Lee", "K. Lee", "R. -K. Lee", "R. Lee", "Sungho Lee", "Sunjae Lee", "Y. Lee", "I. N. Legred", "J. Lehmann", "L. Lehner", "M. Le Jean", "A. Lemaître", "M. Lenti", "M. Leonardi", "M. Lequime", "N. Leroy", "M. Lesovsky", "N. Letendre", "M. Lethuillier", "Y. Levin", "K. Leyde", "A. K. Y. Li", "K. L. Li", "T. G. F. Li", "X. Li", "Y. Li", "Z. Li", "A. Lihos", "E. T. Lin", "F. Lin", "L. C. -C. Lin", "Y. -C. Lin", "C. Lindsay", "S. D. Linker", "A. Liu", "G. C. Liu", "Jian Liu", "F. Llamas Villarreal", "J. Llobera-Querol", "R. K. L. Lo", "J. -P. Locquet", "S. C. G. Loggins", "M. R. Loizou", "L. T. London", "A. Longo", "D. Lopez", "M. Lopez Portilla", "A. Lorenzo-Medina", "V. Loriette", "M. Lormand", "G. Losurdo", "E. Lotti", "T. P. Lott IV", "J. D. Lough", "H. A. Loughlin", "C. O. Lousto", "N. Low", "N. Lu", "L. Lucchesi", "H. Lück", "D. Lumaca", "A. P. Lundgren", "A. W. Lussier", "R. Macas", "M. MacInnis", "D. M. Macleod", "I. A. O. MacMillan", "A. Macquet", "K. Maeda", "S. Maenaut", "S. S. Magare", "R. M. Magee", "E. Maggio", "R. Maggiore", "M. Magnozzi", "M. Mahesh", "M. Maini", "S. Majhi", "E. Majorana", "C. N. Makarem", "D. Malakar", "J. A. Malaquias-Reis", "U. Mali", "S. Maliakal", "A. Malik", "L. Mallick", "A. -K. Malz", "N. Man", "M. Mancarella", "V. Mandic", "V. Mangano", "B. Mannix", "G. L. Mansell", "M. Manske", "M. Mantovani", "M. Mapelli", "C. Marinelli", "F. Marion", "A. S. Markosyan", "A. Markowitz", "E. Maros", "S. Marsat", "F. Martelli", "I. W. Martin", "R. M. Martin", "B. B. Martinez", "D. A. Martinez", "M. Martinez", "V. Martinez", "A. Martini", "J. C. Martins", "D. V. Martynov", "E. J. Marx", "L. Massaro", "A. Masserot", "M. Masso-Reid", "S. Mastrogiovanni", "T. Matcovich", "M. Matiushechkina", "L. Maurin", "N. Mavalvala", "N. Maxwell", "G. McCarrol", "R. McCarthy", "D. E. McClelland", "S. McCormick", "L. McCuller", "S. McEachin", "C. McElhenny", "G. I. McGhee", "J. McGinn", "K. B. M. McGowan", "J. McIver", "A. McLeod", "I. McMahon", "T. McRae", "R. McTeague", "D. Meacher", "B. N. Meagher", "R. Mechum", "Q. Meijer", "A. Melatos", "C. S. Menoni", "F. Mera", "R. A. Mercer", "L. Mereni", "K. Merfeld", "E. L. Merilh", "J. R. Mérou", "J. D. Merritt", "M. Merzougui", "C. Messick", "B. Mestichelli", "M. Meyer-Conde", "F. Meylahn", "A. Mhaske", "A. Miani", "H. Miao", "C. Michel", "Y. Michimura", "H. Middleton", "D. P. Mihaylov", "S. J. Miller", "M. Millhouse", "E. Milotti", "V. Milotti", "Y. Minenkov", "E. M. Minihan", "Ll. M. Mir", "L. Mirasola", "M. Miravet-Tenés", "C. -A. Miritescu", "A. Mishra", "C. Mishra", "T. Mishra", "A. L. Mitchell", "J. G. Mitchell", "S. Mitra", "V. P. Mitrofanov", "K. Mitsuhashi", "R. Mittleman", "O. Miyakawa", "S. Miyoki", "A. Miyoko", "G. Mo", "L. Mobilia", "S. R. P. Mohapatra", "S. R. Mohite", "M. Molina-Ruiz", "M. Mondin", "M. Montani", "C. J. Moore", "D. Moraru", "A. More", "S. More", "C. Moreno", "E. A. Moreno", "G. Moreno", "A. Moreso Serra", "S. Morisaki", "Y. Moriwaki", "G. Morras", "A. Moscatello", "M. Mould", "B. Mours", "C. M. Mow-Lowry", "L. Muccillo", "F. Muciaccia", "D. Mukherjee", "Samanwaya Mukherjee", "Soma Mukherjee", "Subroto Mukherjee", "Suvodip Mukherjee", "N. Mukund", "A. Mullavey", "H. Mullock", "J. Mundi", "C. L. Mungioli", "M. Murakoshi", "P. G. Murray", "D. Nabari", "S. L. Nadji", "A. Nagar", "N. Nagarajan", "K. Nakagaki", "K. Nakamura", "H. Nakano", "M. Nakano", "D. Nanadoumgar-Lacroze", "D. Nandi", "V. Napolano", "P. Narayan", "I. Nardecchia", "T. Narikawa", "H. Narola", "L. Naticchioni", "R. K. Nayak", "L. Negri", "A. Nela", "C. Nelle", "A. Nelson", "T. J. N. Nelson", "M. Nery", "A. Neunzert", "S. Ng", "L. Nguyen Quynh", "S. A. Nichols", "A. B. Nielsen", "Y. Nishino", "A. Nishizawa", "S. Nissanke", "W. Niu", "F. Nocera", "J. Noller", "M. Norman", "C. North", "J. Novak", "R. Nowicki", "J. F. Nuño Siles", "L. K. Nuttall", "K. Obayashi", "J. Oberling", "J. O'Dell", "E. Oelker", "M. Oertel", "G. Oganesyan", "T. O'Hanlon", "M. Ohashi", "F. Ohme", "R. Oliveri", "R. Omer", "B. O'Neal", "M. Onishi", "K. Oohara", "B. O'Reilly", "M. Orselli", "R. O'Shaughnessy", "S. O'Shea", "S. Oshino", "C. Osthelder", "I. Ota", "D. J. Ottaway", "A. Ouzriat", "H. Overmier", "B. J. Owen", "R. Ozaki", "A. E. Pace", "R. Pagano", "M. A. Page", "A. Pai", "L. Paiella", "A. Pal", "S. Pal", "M. A. Palaia", "M. Pálfi", "P. P. Palma", "C. Palomba", "P. Palud", "H. Pan", "J. Pan", "K. C. Pan", "P. K. Panda", "Shiksha Pandey", "Swadha Pandey", "P. T. H. Pang", "F. Pannarale", "K. A. Pannone", "B. C. Pant", "F. H. Panther", "M. Panzeri", "F. Paoletti", "A. Paolone", "A. Papadopoulos", "E. E. Papalexakis", "L. Papalini", "G. Papigkiotis", "A. Paquis", "A. Parisi", "B. -J. Park", "J. Park", "W. Parker", "G. Pascale", "D. Pascucci", "A. Pasqualetti", "R. Passaquieti", "L. Passenger", "D. Passuello", "O. Patane", "A. V. Patel", "D. Pathak", "A. Patra", "B. Patricelli", "B. G. Patterson", "K. Paul", "S. Paul", "E. Payne", "T. Pearce", "M. Pedraza", "A. Pele", "F. E. Peña Arellano", "X. Peng", "Y. Peng", "S. Penn", "M. D. Penuliar", "A. Perego", "Z. Pereira", "C. Périgois", "G. Perna", "A. Perreca", "J. Perret", "S. Perriès", "J. W. Perry", "D. Pesios", "S. Peters", "S. Petracca", "C. Petrillo", "H. P. Pfeiffer", "H. Pham", "K. A. Pham", "K. S. Phukon", "H. Phurailatpam", "M. Piarulli", "L. Piccari", "O. J. Piccinni", "M. Pichot", "M. Piendibene", "F. Piergiovanni", "L. Pierini", "G. Pierra", "V. Pierro", "M. Pietrzak", "M. Pillas", "F. Pilo", "L. Pinard", "I. M. Pinto", "M. Pinto", "B. J. Piotrzkowski", "M. Pirello", "M. D. Pitkin", "A. Placidi", "E. Placidi", "M. L. Planas", "W. Plastino", "C. Plunkett", "R. Poggiani", "E. Polini", "J. Pomper", "L. Pompili", "J. Poon", "E. Porcelli", "E. K. Porter", "C. Posnansky", "R. Poulton", "J. Powell", "G. S. Prabhu", "M. Pracchia", "B. K. Pradhan", "T. Pradier", "A. K. Prajapati", "K. Prasai", "R. Prasanna", "P. Prasia", "G. Pratten", "G. Principe", "G. A. Prodi", "P. Prosperi", "P. Prosposito", "A. C. Providence", "A. Puecher", "J. Pullin", "P. Puppo", "M. Pürrer", "H. Qi", "J. Qin", "G. Quéméner", "V. Quetschke", "P. J. Quinonez", "N. Qutob", "R. Rading", "I. Rainho", "S. Raja", "C. Rajan", "B. Rajbhandari", "K. E. Ramirez", "F. A. Ramis Vidal", "M. Ramos Arevalo", "A. Ramos-Buades", "S. Ranjan", "K. Ransom", "P. Rapagnani", "B. Ratto", "A. Ravichandran", "A. Ray", "V. Raymond", "M. Razzano", "J. Read", "T. Regimbau", "S. Reid", "C. Reissel", "D. H. Reitze", "A. I. Renzini", "B. Revenu", "A. Revilla Peña", "R. Reyes", "L. Ricca", "F. Ricci", "M. Ricci", "A. Ricciardone", "J. Rice", "J. W. Richardson", "M. L. Richardson", "A. Rijal", "K. Riles", "H. K. Riley", "S. Rinaldi", "J. Rittmeyer", "C. Robertson", "F. Robinet", "M. Robinson", "A. Rocchi", "L. Rolland", "J. G. Rollins", "A. E. Romano", "R. Romano", "A. Romero", "I. M. Romero-Shaw", "J. H. Romie", "S. Ronchini", "T. J. Roocke", "L. Rosa", "T. J. Rosauer", "C. A. Rose", "D. Rosińska", "M. P. Ross", "M. Rossello-Sastre", "S. Rowan", "S. K. Roy", "S. Roy", "D. Rozza", "P. Ruggi", "N. Ruhama", "E. Ruiz Morales", "K. Ruiz-Rocha", "S. Sachdev", "T. Sadecki", "P. Saffarieh", "S. Safi-Harb", "M. R. Sah", "S. Saha", "T. Sainrat", "S. Sajith Menon", "K. Sakai", "Y. Sakai", "M. Sakellariadou", "S. Sakon", "O. S. Salafia", "F. Salces-Carcoba", "L. Salconi", "M. Saleem", "F. Salemi", "M. Sallé", "S. U. Salunkhe", "S. Salvador", "A. Salvarese", "A. Samajdar", "A. Sanchez", "E. J. Sanchez", "L. E. Sanchez", "N. Sanchis-Gual", "J. R. Sanders", "E. M. Sänger", "F. Santoliquido", "F. Sarandrea", "T. R. Saravanan", "N. Sarin", "P. Sarkar", "A. Sasli", "P. Sassi", "B. Sassolas", "B. S. Sathyaprakash", "R. Sato", "S. Sato", "Yukino Sato", "Yu Sato", "O. Sauter", "R. L. Savage", "T. Sawada", "H. L. Sawant", "S. Sayah", "V. Scacco", "D. Schaetzl", "M. Scheel", "A. Schiebelbein", "M. G. Schiworski", "P. Schmidt", "S. Schmidt", "R. Schnabel", "M. Schneewind", "R. M. S. Schofield", "K. Schouteden", "B. W. Schulte", "B. F. Schutz", "E. Schwartz", "M. Scialpi", "J. Scott", "S. M. Scott", "R. M. Sedas", "T. C. Seetharamu", "M. Seglar-Arroyo", "Y. Sekiguchi", "D. Sellers", "N. Sembo", "A. S. Sengupta", "E. G. Seo", "J. W. Seo", "V. Sequino", "M. Serra", "A. Sevrin", "T. Shaffer", "U. S. Shah", "M. A. Shaikh", "L. Shao", "A. K. Sharma", "Preeti Sharma", "Prianka Sharma", "Ritwik Sharma", "S. Sharma Chaudhary", "P. Shawhan", "N. S. Shcheblanov", "E. Sheridan", "Z. -H. Shi", "M. Shikauchi", "R. Shimomura", "H. Shinkai", "S. Shirke", "D. H. Shoemaker", "D. M. Shoemaker", "R. W. Short", "S. ShyamSundar", "A. Sider", "H. Siegel", "D. Sigg", "L. Silenzi", "L. Silvestri", "M. Simmonds", "L. P. Singer", "Amitesh Singh", "Anika Singh", "D. Singh", "N. Singh", "S. Singh", "A. M. Sintes", "V. Sipala", "V. Skliris", "B. J. J. Slagmolen", "D. A. Slater", "T. J. Slaven-Blair", "J. Smetana", "J. R. Smith", "L. Smith", "R. J. E. Smith", "W. J. Smith", "S. Soares de Albuquerque Filho", "M. Soares-Santos", "K. Somiya", "I. Song", "S. Soni", "V. Sordini", "F. Sorrentino", "H. Sotani", "F. Spada", "V. Spagnuolo", "A. P. Spencer", "P. Spinicelli", "A. K. Srivastava", "F. Stachurski", "C. J. Stark", "D. A. Steer", "N. Steinle", "J. Steinlechner", "S. Steinlechner", "N. Stergioulas", "P. Stevens", "M. StPierre", "M. D. Strong", "A. Strunk", "A. L. Stuver", "M. Suchenek", "S. Sudhagar", "Y. Sudo", "N. Sueltmann", "L. Suleiman", "K. D. Sullivan", "J. Sun", "L. Sun", "S. Sunil", "J. Suresh", "B. J. Sutton", "P. J. Sutton", "K. Suzuki", "M. Suzuki", "B. L. Swinkels", "A. Syx", "M. J. Szczepańczyk", "P. Szewczyk", "M. Tacca", "H. Tagoshi", "K. Takada", "H. Takahashi", "R. Takahashi", "A. Takamori", "S. Takano", "H. Takeda", "K. Takeshita", "I. Takimoto Schmiegelow", "M. Takou-Ayaoh", "C. Talbot", "M. Tamaki", "N. Tamanini", "D. Tanabe", "K. Tanaka", "S. J. Tanaka", "S. Tanioka", "D. B. Tanner", "W. Tanner", "L. Tao", "R. D. Tapia", "E. N. Tapia San Martín", "C. Taranto", "A. Taruya", "J. D. Tasson", "J. G. Tau", "D. Tellez", "R. Tenorio", "H. Themann", "A. Theodoropoulos", "M. P. Thirugnanasambandam", "L. M. Thomas", "M. Thomas", "P. Thomas", "J. E. Thompson", "S. R. Thondapu", "K. A. Thorne", "E. Thrane", "J. Tissino", "A. Tiwari", "Pawan Tiwari", "Praveer Tiwari", "S. Tiwari", "V. Tiwari", "M. R. Todd", "M. Toffano", "A. M. Toivonen", "K. Toland", "A. E. Tolley", "T. Tomaru", "V. Tommasini", "T. Tomura", "H. Tong", "C. Tong-Yu", "A. Torres-Forné", "C. I. Torrie", "I. Tosta e Melo", "E. Tournefier", "M. Trad Nery", "K. Tran", "A. Trapananti", "R. Travaglini", "F. Travasso", "G. Traylor", "M. Trevor", "M. C. Tringali", "A. Tripathee", "G. Troian", "A. Trovato", "L. Trozzo", "R. J. Trudeau", "T. Tsang", "S. Tsuchida", "L. Tsukada", "K. Turbang", "M. Turconi", "C. Turski", "H. Ubach", "N. Uchikata", "T. Uchiyama", "R. P. Udall", "T. Uehara", "K. Ueno", "V. Undheim", "L. E. Uronen", "T. Ushiba", "M. Vacatello", "H. Vahlbruch", "N. Vaidya", "G. Vajente", "A. Vajpeyi", "J. Valencia", "M. Valentini", "S. A. Vallejo-Peña", "S. Vallero", "V. Valsan", "M. van Dael", "E. Van den Bossche", "J. F. J. van den Brand", "C. Van Den Broeck", "M. van der Sluys", "A. Van de Walle", "J. van Dongen", "K. Vandra", "M. VanDyke", "H. van Haevermaet", "J. V. van Heijningen", "P. Van Hove", "J. Vanier", "M. VanKeuren", "J. Vanosky", "N. van Remortel", "M. Vardaro", "A. F. Vargas", "V. Varma", "A. N. Vazquez", "A. Vecchio", "G. Vedovato", "J. Veitch", "P. J. Veitch", "S. Venikoudis", "R. C. Venterea", "P. Verdier", "M. Vereecken", "D. Verkindt", "B. Verma", "Y. Verma", "S. M. Vermeulen", "F. Vetrano", "A. Veutro", "A. Viceré", "S. Vidyant", "A. D. Viets", "A. Vijaykumar", "A. Vilkha", "N. Villanueva Espinosa", "V. Villa-Ortega", "E. T. Vincent", "J. -Y. Vinet", "S. Viret", "S. Vitale", "H. Vocca", "D. Voigt", "E. R. G. von Reis", "J. S. A. von Wrangel", "W. E. Vossius", "L. Vujeva", "S. P. Vyatchanin", "J. Wack", "L. E. Wade", "M. Wade", "K. J. Wagner", "L. Wallace", "E. J. Wang", "H. Wang", "J. Z. Wang", "W. H. Wang", "Y. F. Wang", "G. Waratkar", "J. Warner", "M. Was", "T. Washimi", "N. Y. Washington", "D. Watarai", "B. Weaver", "S. A. Webster", "N. L. Weickhardt", "M. Weinert", "A. J. Weinstein", "R. Weiss", "L. Wen", "K. Wette", "J. T. Whelan", "B. F. Whiting", "C. Whittle", "E. G. Wickens", "D. Wilken", "A. T. Wilkin", "B. M. Williams", "D. Williams", "M. J. Williams", "N. S. Williams", "J. L. Willis", "B. Willke", "M. Wils", "L. Wilson", "C. W. Winborn", "J. Winterflood", "C. C. Wipf", "G. Woan", "J. Woehler", "N. E. Wolfe", "H. T. Wong", "I. C. F. Wong", "K. Wong", "T. Wouters", "J. L. Wright", "M. Wright", "B. Wu", "C. Wu", "D. S. Wu", "H. Wu", "K. Wu", "Q. Wu", "Y. Wu", "Z. Wu", "E. Wuchner", "D. M. Wysocki", "V. A. Xu", "Y. Xu", "N. Yadav", "H. Yamamoto", "K. Yamamoto", "T. S. Yamamoto", "T. Yamamoto", "R. Yamazaki", "T. Yan", "K. Z. Yang", "Y. Yang", "Z. Yarbrough", "J. Yebana", "S. -W. Yeh", "A. B. Yelikar", "X. Yin", "J. Yokoyama", "T. Yokozawa", "S. Yuan", "H. Yuzurihara", "M. Zanolin", "M. Zeeshan", "T. Zelenova", "J. -P. Zendri", "M. Zeoli", "M. Zerrad", "M. Zevin", "L. Zhang", "N. Zhang", "R. Zhang", "T. Zhang", "C. Zhao", "Yue Zhao", "Yuhang Zhao", "Z. -C. Zhao", "Y. Zheng", "H. Zhong", "H. Zhou", "H. O. Zhu", "Z. -H. Zhu", "A. B. Zimmerman", "L. Zimmermann", "M. E. Zucker", "J. Zweizig"], "title": "Directional Search for Persistent Gravitational Waves: Results from the First Part of LIGO-Virgo-KAGRA's Fourth Observing Run", "comment": "Main paper: 11 pages and 4 figures; Total with appendices: 39 pages\n  and 12 figures", "summary": "The angular distribution of gravitational-wave power from persistent sources\nmay exhibit anisotropies arising from the large-scale structure of the\nUniverse. This motivates directional searches for astrophysical and\ncosmological gravitational-wave backgrounds, as well as continuous-wave\nemitters. We present results of such a search using data from the first\nobserving run through the first portion of the fourth observing run of the\nLIGO-Virgo-KAGRA Collaborations. We apply gravitational-wave radiometer\ntechniques to generate skymaps and search for both narrowband and broadband\npersistent gravitational-wave sources. Additionally, we use spherical harmonic\ndecomposition to probe spatially extended sources. No evidence of persistent\ngravitational-wave signals is found, and we set the most stringent constraints\nto date on such emissions. For narrowband point sources, our sensitivity\nestimate to effective strain amplitude lies in the range $(0.03 - 8.4) \\times\n10^{-24}$ across all sky and frequency range $(20 - 160)$ Hz. For targeted\nsources -- Scorpius X-1, SN 1987A, the Galactic Center, Terzan 5, and NGC 6397\n-- we constrain the strain amplitude with best limits ranging from $\\sim 1.1\n\\times 10^{-25}$ to $6.5 \\times 10^{-24}$. For persistent broadband sources, we\nconstrain the gravitational-wave flux $F_{\\alpha, \\hat{n}}^{95\\%,\n\\mathrm{UL}}(25\\, \\mathrm{Hz}) < (0.008 - 5.5) \\times 10^{-8}\\, \\mathrm{erg\\,\ncm^{-2}\\, s^{-1}\\, Hz^{-1}}$, depending on the sky direction $\\hat{n}$ and\nspectral index $\\alpha=0,\\,2/3,\\,3$. Finally, for extended sources, we place\nupper limits on the strain angular power spectrum $C_\\ell^{1/2} < (0.63 - 17)\n\\times 10^{-10} \\,\\mathrm{sr}^{-1}$."}
{"id": "2510.16020", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16020", "abs": "https://arxiv.org/abs/2510.16020", "authors": ["Sangjoon Lee", "Haris Moazam Sheikh"], "title": "Airfoil optimization using Design-by-Morphing with minimized design-space dimensionality", "comment": null, "summary": "Effective airfoil geometry optimization requires exploring a diverse range of\ndesigns using as few design variables as possible. This study introduces\nAirDbM, a Design-by-Morphing (DbM) approach specialized for airfoil\noptimization that systematically reduces design-space dimensionality. AirDbM\nselects an optimal set of 12 baseline airfoils from the UIUC airfoil database,\nwhich contains over 1,600 shapes, by sequentially adding the baseline that most\nincreases the design capacity. With these baselines, AirDbM reconstructs 99 \\%\nof the database with a mean absolute error below 0.005, which matches the\nperformance of a previous DbM approach that used more baselines. In\nmulti-objective aerodynamic optimization, AirDbM demonstrates rapid convergence\nand achieves a Pareto front with a greater hypervolume than that of the\nprevious larger-baseline study, where new Pareto-optimal solutions are\ndiscovered with enhanced lift-to-drag ratios at moderate stall tolerances.\nFurthermore, AirDbM demonstrates outstanding adaptability for reinforcement\nlearning (RL) agents in generating airfoil geometry when compared to\nconventional airfoil parameterization methods, implying the broader potential\nof DbM in machine learning-driven design."}
{"id": "2510.16868", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2510.16868", "abs": "https://arxiv.org/abs/2510.16868", "authors": ["Alberto De Toni", "Aynur Cemre Aka", "Costantino Agnesi", "Davide Giacomo Marangon", "Giuseppe Vallone", "Paolo Villoresi"], "title": "Countermeasures for Trojan-Horse Attacks on self-compensating all-fiber polarization modulator", "comment": null, "summary": "Quantum Key Distribution (QKD) leverages the principles of quantum mechanics\nto exchange a secret key between two parties. Unlike classical cryptographic\nsystems, the security of QKD is not reliant on computational assumptions but is\ninstead rooted in the fundamental laws of physics. In a QKD protocol, any\nattempt by an eavesdropper to intercept the key is detectable: this provides an\nunprecedented level of security, making QKD an attractive solution for secure\ncommunication in an era increasingly threatened by the advent of quantum\ncomputers and their potential to break classical cryptographic systems.\nHowever, QKD also faces several practical challenges such as transmission loss\nand noise in quantum channels, finite key size effects, and implementation\nflaws in QKD devices. Addressing these issues is crucial for the large-scale\ndeployment of QKD and the realization of a global quantum internet. A whole\nbody of research is dedicated to the hacking of the quantum states source, for\nexample using Trojan-Horse attacks (THAs), where the eavesdropper injects light\ninto the system and analyzes the back-reflected signal. In this paper, we study\nthe vulnerabilities against THAs of the iPOGNAC encoder, first introduced in\nAvesani, Agnesi et al., to propose adapted countermeasures that can mitigate\nsuch attacks."}
{"id": "2510.17488", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.17488", "abs": "https://arxiv.org/abs/2510.17488", "authors": ["Antonio Pasqua"], "title": "A Higher-Derivative Hubble Parameter Dark Energy Model: Cosmological Analysis and Scalar Field Correspondence", "comment": null, "summary": "In this work, we study a Dark Energy (DE) energy density model which depends\non the Hubble parameter squared $H^2$ and on its first, second and third time\nderivatives $\\dot{H}$, $\\ddot{H}$ and $\\dddot{H}$. Considering a scale factor\n$a$ with a power-law dependence on the time (with $n$ indicating the power-law\nindex), we obtain some important cosmological quantities as function of the ,\nlike the energy densities of Matter $\\rho_m$ and of DE $\\rho_D$, the fractional\nenergy densities of DM $\\Omega_m$ and of DE $\\Omega_D$, the Hubble parameter\nsquared $H^2$, the deceleration parameter $q$, the evolutionary form of the\nfractional energy density of DE $\\Omega'_D$, the pressure of DE $p_D$ and the\nEquation of State (EoS) parameter of DE $\\omega_D$, for both non interacting\nand interacting cases. For the interacting case, we consider 9 different\ninteracting term $Q$, all functions of the Hubble parameter $H$ and/or of\n$\\rho_m$ and $\\rho_D$. Finally, we establish a correspondence between the DE\nmodel we study and some scalar field theories, including tachyon, k-essence,\nquintessence, Dirac-Born-Infeld (DBI), Yang-Mills (YM) and Nonlinear\nElectrodynamics (NLED) fields."}
{"id": "2510.16021", "categories": ["cs.LG", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2510.16021", "abs": "https://arxiv.org/abs/2510.16021", "authors": ["Arega Getaneh Abate", "Xiufeng Liu", "Ruyu Liu", "Xiaobing Zhang"], "title": "Feature-driven reinforcement learning for photovoltaic in continuous intraday trading", "comment": null, "summary": "Photovoltaic (PV) operators face substantial uncertainty in generation and\nshort-term electricity prices. Continuous intraday markets enable producers to\nadjust their positions in real time, potentially improving revenues and\nreducing imbalance costs. We propose a feature-driven reinforcement learning\n(RL) approach for PV intraday trading that integrates data-driven features into\nthe state and learns bidding policies in a sequential decision framework. The\nproblem is cast as a Markov Decision Process with a reward that balances\ntrading profit and imbalance penalties and is solved with Proximal Policy\nOptimization (PPO) using a predominantly linear, interpretable policy. Trained\non historical market data and evaluated out-of-sample, the strategy\nconsistently outperforms benchmark baselines across diverse scenarios.\nExtensive validation shows rapid convergence, real-time inference, and\ntransparent decision rules. Learned weights highlight the central role of\nmarket microstructure and historical features. Taken together, these results\nindicate that feature-driven RL offers a practical, data-efficient, and\noperationally deployable pathway for active intraday participation by PV\nproducers."}
{"id": "2510.16895", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.16895", "abs": "https://arxiv.org/abs/2510.16895", "authors": ["Hatim A. Oujaa", "Qiao Liu", "Ebubechukwu O. Ilo-Okeke", "Valentin Ivannikov", "Jonathan P. Dowling", "Tim byrnes"], "title": "Phase assumption-free multiparty quantum clock synchronization", "comment": null, "summary": "We investigate methods to broadcast timing information from a central clock\nto all other clocks by the use of multipartite entanglement. This task is a\nnecessary step in establishing a coordinated universal time, currently\nperformed using classical synchronization methods. Using an entanglement-based\nmethod has the advantage that the timing results are independent of the\nintervening medium. We generalize existing bipartite quantum clock\nsynchronization methods and take special care to address issues of different\nphase conventions being adopted at each node (the ``Preskill phase problem'').\nUsing supersinglet purification, we show that this allows for a scalable method\nwith a time signal that is a constant with respect to the number of nodes."}
{"id": "2510.17499", "categories": ["gr-qc", "astro-ph.IM", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2510.17499", "abs": "https://arxiv.org/abs/2510.17499", "authors": ["Yuzhou Fang", "Dexuan Zhang", "Dezhi Wang", "Xuefeng Zhang", "Huizong Duan", "Hongyin Li", "Junxiang Lian", "Guoying Zhao"], "title": "Tilt-to-length noise subtraction with pointing jitters from closed-loop dynamics for TianQin", "comment": null, "summary": "TianQin is a proposed space-based mission for gravitational wave detection,\nemploying a constellation of three drag-free satellites in high Earth orbits to\nform a laser interferometric observatory. A critical technical challenge is\nmitigating tilt-to-length (TTL) coupling noise, which is expected to be the\nthird dominant noise source after laser frequency and clock noises. This noise\nis unavoidable in the presence of the residual angular movement of satellites,\nmovable optical subassemblies (MOSAs), and test masses (TMs), and needs to be\nsubtracted after reducing the first two types of noises using time-delay\ninterferometry (TDI). Previous works have shown that TTL coupling coefficients\ncan be estimated from the null TDI channel $\\zeta$ and used for noise\nsubtraction in other combinations. However, it was found that correlated MOSA\nyaw jitters have a negative impact on the TTL calibration, and the effects of\nrealistic residual angular jitters from drag-free and pointing control (DFPC)\nare yet to be investigated. In this paper, we use closed-loop DFPC simulations\nto generate more realistic jitters in the science mode and test TTL calibration\ncapability. Our simulations reveal that rotating only one MOSA is more\nfavorable, compared to symmetrically rotating two MOSAs, for enhancing the\naccuracy of TTL coefficient estimation, while employing only high-frequency\ndata (0.1 - 1 Hz). Moreover, we propose two other methods to further improve\nestimation accuracy. Firstly, using different null channel combinations, such\nas $C_3^{14}$, enhances the least squares estimation accuracy even in the case\nof high correlations in MOSAs' yaw jitters. Secondly, injecting different\nsinusoidal artificial maneuvers to the six MOSAs also shows improvements. These\nmethods can help TianQin to meet the 0.3 pm/Hz$^{1/2}$ requirement after the\nTTL noise subtraction."}
{"id": "2510.16022", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16022", "abs": "https://arxiv.org/abs/2510.16022", "authors": ["Changsheng Wang", "Xin Chen", "Sijia Liu", "Ke Ding"], "title": "Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization", "comment": null, "summary": "Adapting pretrained large language models (LLMs) to code domains via\nsupervised fine-tuning (FT) has been commonly used for code generation.\nHowever, we identify a previously underappreciated failure mode, the\nmemorization barrier, where strong memorization of downstream code data in the\nbase model could trap optimization and prevent the standard FT from effectively\nacquiring new, generalizable code knowledge. To overcome this barrier, we\npropose the information bottleneck (IB)-guided fine-tuning, termed IB-FT, which\napplies an IB penalty on hidden representations of the code data to compress\nspurious, memorized features while preserving task-relevant information.\nExtensive experiments on two code benchmarks (OriGen and Evol-CodeAlpaca-V1)\nshow that IB-FT substantially alleviates the memorization barrier, improves\ntop-1 performance (Pass@$1$), and yields far more stable gains under the\nstricter multi-sample metric Pass@$k^{(m)}$ (a problem counts as solved only if\nat least $m$ of $k$ samples pass unit tests) compared with conventional FT."}
{"id": "2510.16915", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.16915", "abs": "https://arxiv.org/abs/2510.16915", "authors": ["Maria Jose Lozano Palacio", "Hasan Nayfeh", "Matthew Ware", "David C. McKay"], "title": "Parameter Analysis and Optimization of Layer Fidelity for Quantum Processor Benchmarking at Scale", "comment": null, "summary": "With the continued scaling of quantum processors, holistic benchmarks are\nessential for extensively evaluating device performance. Layer fidelity is a\nbenchmark well-suited to assessing processor performance at scale. Key\nadvantages of this benchmark include its natural alignment with randomized\nbenchmarking (RB) procedures, crosstalk awareness, fast measurements over large\nnumbers of qubits, high signal-to-noise ratio, and fine-grained information. In\nthis work, we extend the analysis of the original layer fidelity manuscript to\noptimize parameters of the benchmark and extract deeper insights of its\napplication. We present a robust protocol for identifying optimal qubit chains\nof length N, demonstrating that our method yields error per layered gate (EPLG)\nvalues 40%-70% lower than randomly selected chains. We further establish layer\nfidelity as an effective performance monitoring tool, capturing both\nedge-localized and device-wide degradation by tracking optimal chains of length\n50 and 100, and fixed chains of length 100. Additionally, we refine error\nanalysis by proposing parameter bounds on the number of randomizations and\nClifford lengths used in direct RB fits, minimizing fit uncertainties. Finally,\nwe analyze the impact of varying gate durations on layer fidelity measurements,\nshowing that prolonged gate times leading to idling times significantly\nincrease layered two-qubit (2Q) errors on Eagle R3 processors. Notably, we\nobserve a 95% EPLG increase on a fixed chain in an Eagle R3 processor when some\ngate durations are extended by 65%. These findings extend the applicability of\nthe layer fidelity benchmark and provide practical guidelines for optimizing\nquantum processor evaluations."}
{"id": "2510.17560", "categories": ["gr-qc", "hep-th", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2510.17560", "abs": "https://arxiv.org/abs/2510.17560", "authors": ["Alejandro Corichi", "Omar Gallegos"], "title": "An Extended Second Law of Thermodynamics", "comment": null, "summary": "The second law of thermodynamics constitutes a fundamental principle of\nphysics, precluding the existence of perpetual motion machines and providing a\nnatural definition of the arrow of time. Its scope extends across virtually all\nareas of physical theory. Nonetheless, certain systems are known to admit\nnegative absolute temperatures under well-defined conditions, a phenomenon that\nhas been experimentally observed. In this work, we formulate an\n\\textit{extended} version of the first and second laws, which recovers the\nconventional statement for positive temperatures and extends its applicability\nto the negative-temperature domain. Illustrative examples are discussed in the\ncontexts of quantum cosmology and Onsager's vortices."}
{"id": "2510.16023", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.16023", "abs": "https://arxiv.org/abs/2510.16023", "authors": ["Fanmeng Wang", "Shan Mei", "Wentao Guo", "Hongshuai Wang", "Qi Ou", "Zhifeng Gao", "Hongteng Xu"], "title": "Unifying Polymer Modeling and Design via a Conformation-Centric Generative Foundation Model", "comment": null, "summary": "Polymers, macromolecules formed from covalently bonded monomers, underpin\ncountless technologies and are indispensable to modern life. While deep\nlearning is advancing polymer science, existing methods typically represent the\nwhole polymer solely through monomer-level descriptors, overlooking the global\nstructural information inherent in polymer conformations, which ultimately\nlimits their practical performance. Moreover, this field still lacks a\nuniversal foundation model that can effectively support diverse downstream\ntasks, thereby severely constraining progress. To address these challenges, we\nintroduce PolyConFM, the first polymer foundation model that unifies polymer\nmodeling and design through conformation-centric generative pretraining.\nRecognizing that each polymer conformation can be decomposed into a sequence of\nlocal conformations (i.e., those of its repeating units), we pretrain PolyConFM\nunder the conditional generation paradigm, reconstructing these local\nconformations via masked autoregressive (MAR) modeling and further generating\ntheir orientation transformations to recover the corresponding polymer\nconformation. Besides, we construct the first high-quality polymer conformation\ndataset via molecular dynamics simulations to mitigate data sparsity, thereby\nenabling conformation-centric pretraining. Experiments demonstrate that\nPolyConFM consistently outperforms representative task-specific methods on\ndiverse downstream tasks, equipping polymer science with a universal and\npowerful tool."}
{"id": "2510.16918", "categories": ["quant-ph", "cs.IT", "math-ph", "math.IT", "math.MP"], "pdf": "https://arxiv.org/pdf/2510.16918", "abs": "https://arxiv.org/abs/2510.16918", "authors": ["Giulio Gasbarri", "Matt Hoogsteder-Riera"], "title": "Single-letter Chain Rule for Quantum Relative Entropy", "comment": "18 pages, 2 figures", "summary": "Relative entropy is the standard measure of distinguishability in classical\nand quantum information theory. In the classical case, its loss under channels\nadmits an exact chain rule, while in the quantum case only asymptotic,\nregularized chain rules are known. We establish new chain rules for quantum\nrelative entropy that apply already in the single-copy regime. The first\ninequality is obtained via POVM decompositions, extending the point\ndistributions in the classical chain rule to quantum ensemble partitions. The\nsecond gives a sufficient condition for the most natural extension of the\nclassical result, which uses projectors as a analog for the classical point\ndistributions. We additionally find a semiclassical chain rule where the point\ndistributions are replaced with the projectors of the initial states, and,\nfinally, we find a relation to previous works on strengthened data processing\ninequalities and recoverability. These results show that meaningful chain\ninequalities are possible already at the single-copy level, but they also\nhighlight that tighter bounds remain to be found."}
{"id": "2510.17600", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.17600", "abs": "https://arxiv.org/abs/2510.17600", "authors": ["Sergi Sirera"], "title": "Black hole ringdown tests of gravity", "comment": "181 pages + appendices and references, 35 figures", "summary": "Understanding gravity is at the heart of some of the biggest questions in\nmodern physics. While General Relativity (GR) is a theoretically unique and\nexperimentally well-tested framework, it remains important to question whether\nit accurately describes gravity at all scales, motivating the exploration of\nbroader theories. Black holes (BHs) provide ideal natural laboratories for\ntesting gravity in the strong-field regime, and the recent advent of\ngravitational wave (GW) astronomy has opened a new observational window into\nthese extreme environments. In particular, the final stage of a compact binary\nmerger$\\unicode{x2013}$the ringdown phase$\\unicode{x2013}$is of great interest.\nHere, the study of quasinormal modes (QNMs) offers a powerful tool to probe the\nfundamental nature of gravity and to extract intrinsic properties of BHs.\n  This thesis investigates BH solutions and their QNM spectra within\nscalar-tensor theories of gravity$\\unicode{x2013}$well-motivated extensions of\nGR that include an additional scalar degree of freedom. In particular, it\nfocuses on stealth BHs, where scalar hair exists without altering the\nbackground metric but can modify the QNM spectrum. By analysing perturbations\naround such spacetimes, we derive forecasted constraints on beyond-GR\nparameters for current and future GW detectors.\n  Three main investigations are presented: (i) a novel method to constrain the\nspeed of gravity using ringdown signals alone; (ii) a stability and QNM\nanalysis of BHs with linearly time-dependent scalar hair; and (iii) a general\nclassification of stealth solutions in higher-order scalar-tensor (HOST)\ntheories, including a stability analysis and identification of ringdown\nobservational signatures. Together, these studies contribute new theoretical\ntools and observational forecasts that advance our understanding on fundamental\ngravitational physics in the era of GW astronomy"}
{"id": "2510.16026", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.16026", "abs": "https://arxiv.org/abs/2510.16026", "authors": ["Marco Barbero-Mota", "Eric V. Strobl", "John M. Still", "William W. Stead", "Thomas A. Lasko"], "title": "A tutorial on discovering and quantifying the effect of latent causal sources of multimodal EHR data", "comment": null, "summary": "We provide an accessible description of a peer-reviewed generalizable causal\nmachine learning pipeline to (i) discover latent causal sources of large-scale\nelectronic health records observations, and (ii) quantify the source causal\neffects on clinical outcomes. We illustrate how imperfect multimodal clinical\ndata can be processed, decomposed into probabilistic independent latent\nsources, and used to train taskspecific causal models from which individual\ncausal effects can be estimated. We summarize the findings of the two\nreal-world applications of the approach to date as a demonstration of its\nversatility and utility for medical discovery at scale."}
{"id": "2510.16962", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.16962", "abs": "https://arxiv.org/abs/2510.16962", "authors": ["Ama Bandara", "Viviana Centritto Arrojo", "Heqi Deng", "Masoud Babaie", "Fabio Sebastiano", "Edoardo Charbon", "Evgenii Vinogradov", "Eduard Alarcon", "Sergi Abadal"], "title": "28 GHz Wireless Channel Characterization for a Quantum Computer Cryostat at 4 Kelvin", "comment": null, "summary": "The scalability of quantum computing systems is constrained by the wiring\ncomplexity and thermal load introduced by dense wiring for control, readout and\nsynchronization at cryogenic temperatures. To address this challenge, we\nexplore the feasibility of wireless communication within a cryostat for a\nmulti-core quantum computer, focusing on wireless channel characterization at\ncryogenic temperatures. We propose to place on-chip differential dipole\nantennas within the cryostat, designed to operate at 28 GHz in temperatures as\nlow as 4 K. We model the antennas inside a realistic cryostat and, using\nfull-wave electromagnetic simulations, we analyze impedance matching, spatial\nfield distribution, and energy reverberation due to metallic structures. The\nwireless channel is characterized through measured channel impulse response\n(CIR) across multiple receiver antenna positions. The results demonstrate\npotential for reliable shortrange communication with high Signal-to-Noise Ratio\n(SNR) and limited sensitivity to positional variation, at the cost of\nnonnegligible delay spread, due to significant multipath effects."}
{"id": "2510.17623", "categories": ["gr-qc", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2510.17623", "abs": "https://arxiv.org/abs/2510.17623", "authors": ["Christiane K. M. Klein"], "title": "Exploring quantum fields in rotating black holes", "comment": "18 pages, 2 figures, invited contribution to the Special Topic on\n  \"XXI International Congress on Mathematical Physics 2024\" in Journal of\n  Mathematical Physics", "summary": "In this paper, we discuss the Unruh state for a free scalar quantum field on\nKerr-de Sitter under the assumption of mode stability. We summarise the proof\nof its Hadamard property that was previously given in [Klein:2023] for\nsufficiently small black-hole rotation and cosmological constant and show how\nit can be generalised to any subextreme black-hole angular momentum in the same\nrange of the cosmological constant. This is done by extending a geometric\nanalysis of the trapped set of the Kerr spacetime [H\\\"afner, Klein:2024] to\nKerr-de Sitter. Moreover, we discuss the application of this state in the\nnumerical study of quantum effects at the inner horizon [Klein, Soltani,\nCasals, Hollands:2024], and describe a universality result for these effects\n[Hintz, Klein: 2024]."}
{"id": "2510.16035", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16035", "abs": "https://arxiv.org/abs/2510.16035", "authors": ["Yingguang Yang", "Xianghua Zeng", "Qi Wu", "Hao Peng", "Yutong Xia", "Hao Liu", "Bin Chong", "Philip S. Yu"], "title": "RoBCtrl: Attacking GNN-Based Social Bot Detectors via Reinforced Manipulation of Bots Control Interaction", "comment": "27 pages, 10 figures", "summary": "Social networks have become a crucial source of real-time information for\nindividuals. The influence of social bots within these platforms has garnered\nconsiderable attention from researchers, leading to the development of numerous\ndetection technologies. However, the vulnerability and robustness of these\ndetection methods is still underexplored. Existing Graph Neural Network\n(GNN)-based methods cannot be directly applied due to the issues of limited\ncontrol over social agents, the black-box nature of bot detectors, and the\nheterogeneity of bots. To address these challenges, this paper proposes the\nfirst adversarial multi-agent Reinforcement learning framework for social Bot\ncontrol attacks (RoBCtrl) targeting GNN-based social bot detectors.\nSpecifically, we use a diffusion model to generate high-fidelity bot accounts\nby reconstructing existing account data with minor modifications, thereby\nevading detection on social platforms. To the best of our knowledge, this is\nthe first application of diffusion models to mimic the behavior of evolving\nsocial bots effectively. We then employ a Multi-Agent Reinforcement Learning\n(MARL) method to simulate bots adversarial behavior. We categorize social\naccounts based on their influence and budget. Different agents are then\nemployed to control bot accounts across various categories, optimizing the\nattachment strategy through reinforcement learning. Additionally, a\nhierarchical state abstraction based on structural entropy is designed to\naccelerate the reinforcement learning. Extensive experiments on social bot\ndetection datasets demonstrate that our framework can effectively undermine the\nperformance of GNN-based detectors."}
{"id": "2510.16979", "categories": ["quant-ph", "physics.atom-ph"], "pdf": "https://arxiv.org/pdf/2510.16979", "abs": "https://arxiv.org/abs/2510.16979", "authors": ["Nhat A. Nghiem", "Trung V. Phan"], "title": "Fractatomic Physics: An Invitation with Atomic Stability and Rydberg States in Fractal Spaces", "comment": null, "summary": "We explore the physical quantum properties of atoms in fractal spaces, both\nas a theoretical generalization of normal integer-dimensional Euclidean spaces\nand as an experimentally realizable setting. We identify the threshold of\nfractality at which Ehrenfest atomic instability emerges, where the\nSchr\\\"{o}dinger equation describing the wave-function of a single electron\norbiting around an atom becomes scale-free, and discuss the potential of\nobserving this phenomena in laboratory settings. We then study the Rydberg\nstates of stable atoms using the Wentzel-Kramers-Brillouin approximation, along\nwith a proposed extension for the Langer modification, in general fractal\ndimensionalities. We show that fractal space atoms near instability explode in\nsize even at low-number excited state, making them highly suitable to induce\nstrong entanglements and foster long-range many-body interactions. We argue\nthat atomic physics in fractal spaces -- ``fractatomic physics'' -- is a rich\nresearch avenue deserving of further theoretical and experimental\ninvestigations."}
{"id": "2510.17631", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.17631", "abs": "https://arxiv.org/abs/2510.17631", "authors": ["Y. Ahmadi", "M. V. Takook"], "title": "Photon decaying in de Sitter universe", "comment": "12 pages, 4 figures. Preprint version of the article published in\n  Journal of Theoretical and Applied Physics (2019)", "summary": "The interaction between three photons is studied in de Sitter ambient space\nformalism. As a special case the half harmonic generator is considered, {\\it\ni.e.} one photon decays to two same-energy photons. The scattering matrix\nelements are presented which define the indirect gravitational effect on\nquantum field theory. The null curvature limit of scattering matrix is obtained\nfor comparing it with its Minkowskian counterpart. The Hamiltonian of this\ninteraction, in Minkowski space-time, was presented by using the quantum vacuum\nfluctuation in the one-loop approximation."}
{"id": "2510.16039", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16039", "abs": "https://arxiv.org/abs/2510.16039", "authors": ["Xiangyuan Peng", "Xingsi Dong", "Si Wu"], "title": "Vector Quantization in the Brain: Grid-like Codes in World Models", "comment": null, "summary": "We propose Grid-like Code Quantization (GCQ), a brain-inspired method for\ncompressing observation-action sequences into discrete representations using\ngrid-like patterns in attractor dynamics. Unlike conventional vector\nquantization approaches that operate on static inputs, GCQ performs\nspatiotemporal compression through an action-conditioned codebook, where\ncodewords are derived from continuous attractor neural networks and dynamically\nselected based on actions. This enables GCQ to jointly compress space and time,\nserving as a unified world model. The resulting representation supports\nlong-horizon prediction, goal-directed planning, and inverse modeling.\nExperiments across diverse tasks demonstrate GCQ's effectiveness in compact\nencoding and downstream performance. Our work offers both a computational tool\nfor efficient sequence modeling and a theoretical perspective on the formation\nof grid-like codes in neural systems."}
{"id": "2510.17019", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2510.17019", "abs": "https://arxiv.org/abs/2510.17019", "authors": ["Giovanni Miano", "Loris Maria Cangemi", "Carlo Forestiere"], "title": "Modified Langevin noise formalism for multiple quantum emitters in dispersive electromagnetic environments", "comment": null, "summary": "The control of interactions among quantum emitters through nanophotonic\nstructures offers significant potential for quantum technologies. However, a\nrigorous theoretical description of the interaction of multiple quantum\nemitters with complex dispersive dielectric objects remains highly challenging.\nHere we introduce an approach based on the modified Langevin noise formalism\nthat unveils the roles of both the noise polarization currents of the\ndielectrics and the vacuum fluctuations of the electromagnetic field scattered\nby the dielectrics. This extends Refs. \\cite{miano_quantum_2025},\n\\cite{miano_spectral_2025} to the general case of an arbitrary number of\nemitters. The proposed approach allows us to describe the dynamics of the\nquantum emitters for arbitrary initial quantum states of the electromagnetic\nenvironment consisting of two independent bosonic reservoirs, a medium-assisted\nreservoir and a scattering-assisted reservoir, each characterized by its own\nspectral density matrix. Understanding how these reservoirs shape emitter\ndynamics is crucial to understanding light-matter interactions in complex\nelectromagnetic environments and to enhancing intrinsic emitter properties in\nstructured environments."}
{"id": "2510.17663", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.17663", "abs": "https://arxiv.org/abs/2510.17663", "authors": ["Faqiang Yuan", "Haida Li", "Shengzhi Li", "Yongge Ma"], "title": "Deparametrization and Quantization of Scalar-Tensor Gravity and Its Cosmological Model", "comment": null, "summary": "The degree of freedom of the scalar field in scalar-tensor gravity is\nemployed as 'time' to deparametrize the Hamiltonian constraint of the theory.\nThe deparametrized system is then non-perturbatively quantized by the approach\nof loop quantum gravity. This results in a discrete time evolution of the\nphysical states with respect to the gravitational degree of freedom in the\nquantum theory. In the corresponding Brans-Dicke cosmological model, the\nphysical solutions to the quantum Hamiltonian constraint is obtained in the\nlight of the deparametrization. The quantum dynamics indicates that the\nclassical big bang singularity is replaced by a quantum bounce."}
{"id": "2510.16045", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16045", "abs": "https://arxiv.org/abs/2510.16045", "authors": ["Mengtao Lv", "Ruiqi Zhu", "Xinyu Wang", "Yun Li"], "title": "AMS-QUANT: Adaptive Mantissa Sharing for Floating-point Quantization", "comment": "12 pages, 6 figures", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious kinds of tasks, while the billion or even trillion parameters bring\nstorage and efficiency bottlenecks for inference. Quantization, particularly\nfloating-point quantization, is known to be capable of speeding up LLM\ninference by reducing memory footprint and data movement during the inference\nprocess. For the first time, we advance the floating-point quantization\nexploration from integer bitwidths to non-integer bit-widths, namely AMS-Quant,\nto further approach the quantization sweet spot. AMS-Quant incorporates two\nnovel techniques to put it into effect: (1) it proposes Mantissa-bit Sharing,\nwhich groups k quantized weights and lets them share the least significant\nmantissa bit, allowing us to further approach the minimum quantization\nbit-width without accuracy loss. (2) It introduces Adaptive Searching, which\nemploys an offline optimization strategy to minimize the accuracy degradation\nintroduced by sharing. Moreover, AMS-Quant is also prototyped as efficient CUDA\nLinear kernels, which translates memory savings into wall-clock latency\nreduction by reducing memory access. Extensive experiments on large-scale\ndatasets and models show that AMS-Quant can quantize the model to FP-5.33-e2m3\nand FP4.25-e2m2, and significantly speed up the LLM decoding over FP16\ninference (2.8x and 3.2x), with negligible accuracy loss."}
{"id": "2510.17048", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.17048", "abs": "https://arxiv.org/abs/2510.17048", "authors": ["Mahshid Khazaei Shadfar", "Farzam Nosrati", "Ali Mortezapour", "Vincenzo Macri", "Roberto Morandotti", "Rosario Lo Franco"], "title": "Preserving quantum coherence in thermal noisy systems via qubit frequency modulation", "comment": null, "summary": "Quantum coherence is a key resource underpinning quantum technologies, yet it\nis highly susceptible to environmental decoherence, especially in thermal\nsettings. While frequency modulation (FM) has shown promise in preserving\ncoherence at zero temperature, its effectiveness in realistic, noisy thermal\nenvironments remains unclear. In this work, we investigate a single\nfrequency-modulated qubit interacting with a thermal phase-covariant reservoir\ncomposed of dissipative and dephasing channels. We demonstrate that FM\nsignificantly preserves coherence in the presence of thermal dissipation while\nbeing ineffective under thermal pure-dephasing noise due to commutation between\nsystem and interaction Hamiltonians. When both noise channels are present, FM\noffers protection only for weak dephasing coupling. Our findings clarify the\nlimitations and potential of FM-based coherence protection under thermal noise,\nsupplying practical insights into designing robust quantum systems for quantum\napplications."}
{"id": "2510.17704", "categories": ["gr-qc", "astro-ph.GA"], "pdf": "https://arxiv.org/pdf/2510.17704", "abs": "https://arxiv.org/abs/2510.17704", "authors": ["Erhard Scholz"], "title": "Einstein gravity extended by a scale covariant scalar field with Bekenstein term and dynamical mass generation", "comment": "61 pages, 6 figures", "summary": "Under carefully chosen assumptions a single general relativistic scalar field\nis able to induce MOND-like dynamics in the weak field approximation of the\nEinstein frame (gauge) and to modify the light cone structure accordingly. This\nis shown by a Lagrangian model formulated in the framework of integrable Weyl\ngeometry. It contains a Bekenstein-type (``aquadratic'') term and a second\norder term generating additional mass energy for the scalar field. Both are\nswitched on only if the gradient of the scalar field is spacelike and below a\nMOND-typical threshold, like in the superfluid model of Berezhiani/Khoury. The\nmass term induces non-negligible energy and pressures of the scalar field and\nleads to gravitational light deflection compatible with MOND-ian free fall\ntrajectories. In the weak field (Newton-Milgrom) approximation the Bekenstein\nterm implies a deep MOND equation for the scalar field. In this model the\nexternal field effect of the MOND approach has to be reconsidered. This has\nimportant consequences for hierarchical systems like clusters, which may\nsuffice for explaining their dynamics without additional dark matter."}
{"id": "2510.16051", "categories": ["cs.LG", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.16051", "abs": "https://arxiv.org/abs/2510.16051", "authors": ["Sofiya Garkot", "Maksym Shamrai", "Ivan Synytsia", "Mariya Hirna"], "title": "GUIrilla: A Scalable Framework for Automated Desktop UI Exploration", "comment": "22 pages", "summary": "Autonomous agents capable of operating complex graphical user interfaces\n(GUIs) have the potential to transform desktop automation. While recent\nadvances in large language models (LLMs) have significantly improved UI\nunderstanding, navigating full-window, multi-application desktop environments\nremains a major challenge. Data availability is limited by costly manual\nannotation, closed-source datasets and surface-level synthetic pipelines. We\nintroduce GUIrilla, an automated scalable framework that systematically\nexplores applications via native accessibility APIs to address the critical\ndata collection challenge in GUI automation. Our framework focuses on macOS -\nan ecosystem with limited representation in current UI datasets - though many\nof its components are designed for broader cross-platform applicability.\nGUIrilla organizes discovered interface elements and crawler actions into\nhierarchical GUI graphs and employs specialized interaction handlers to achieve\ncomprehensive application coverage. Using the application graphs from GUIrilla\ncrawler, we construct and release GUIrilla-Task, a large-scale dataset of\n27,171 functionally grounded tasks across 1,108 macOS applications, each\nannotated with full-desktop and window-level screenshots, accessibility\nmetadata, and semantic action traces. Empirical results show that tuning\nLLM-based agents on GUIrilla-Task significantly improves performance on\ndownstream UI tasks, outperforming synthetic baselines on the ScreenSpot Pro\nbenchmark while using 97% less data. We also release macapptree, an open-source\nlibrary for reproducible collection of structured accessibility metadata, along\nwith the full GUIrilla-Task dataset, the manually verified GUIrilla-Gold\nbenchmark, and the framework code to support open research in desktop autonomy."}
{"id": "2510.17128", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.17128", "abs": "https://arxiv.org/abs/2510.17128", "authors": ["Qisi Zhou", "Qingqian Kang", "Teng Zhao", "Xin Su", "Cunjin Liu", "Liyun Hu"], "title": "Phase sensitivity of lossy Mach-Zehnder interferometer via photon addition operation", "comment": null, "summary": "Photon addition operations applied to squeezed states have been shown to\nsignificantly enhance phase sensitivity. In this study, we extend this approach\nby applying photon addition not only to coherent states but also within a\nMach--Zehnder interferometer setup, using coherent and squeezed vacuum states\nas input. Both intensity-difference and homodyne detection are used to evaluate\nphoton addition schemes, and their phase sensitivities are compared under ideal\nand lossy conditions, respectively. We also analyze the quantum Fisher\ninformation of these two schemes. Results show both schemes improve phase\nsensitivity, quantum Fisher information, and loss resistance. In particular,\nphoton addition within the interferometer performs better. Homodyne detection\noutperforms intensity difference detection under photon losses. Notably, each\nscheme has different parameter dependencies, making them suitable for different\napplication scenarios. When the squeezing parameter is small, photon addition\nemployed at the coherent input with intensity difference detection can approach\nthe Heisenberg limit in ideal conditions and can exceed the standard quantum\nlimit in high-loss conditions. Our proposed scheme represents a valuable method\nfor quantum precision measurements."}
{"id": "2510.17765", "categories": ["gr-qc", "astro-ph.CO"], "pdf": "https://arxiv.org/pdf/2510.17765", "abs": "https://arxiv.org/abs/2510.17765", "authors": ["Sudip Halder", "Supriya Pan", "Paulo M. Sá", "Tapan Saha"], "title": "Phantom scalar field with arbitrary potential: accelerating scaling attractors", "comment": "15 pages including references, 3 tables and one figure; comments are\n  welcome", "summary": "In this article, we investigate the dynamics of a phantom scalar field with\nan arbitrary potential, focusing on accelerating scaling solutions of\ncosmological relevance. We consider both uncoupled and coupled cosmological\nscenarios. In the latter case, the coupling between phantom dark energy and\ndark matter is motivated by the warm inflationary paradigm, with the\ndissipation coefficient assumed to be either constant or variable. The\nevolution equations of our coupled and uncoupled cosmological models are\nwritten in the form of autonomous systems, whose stability is studied using\nmethods of qualitative analysis of dynamical systems. For this analysis, the\nonly requirement imposed on the phantom scalar-field potential is that a\nspecific dynamical variable, defined in terms of the potential and its\nderivative, must be invertible. We show that the uncoupled phantom cosmological\nmodel cannot accommodate any accelerated scaling solution, while such solutions\ndo exist in the coupled scenario, for both constant and variable dissipation\ncoefficients. Although there is a limitation to these scaling solutions $-$\nspecifically, the current stage of accelerated expansion is not preceded by a\nlong enough matter-dominated era $-$ our results show that the existence of a\ndirect coupling between phantom dark energy and dark matter yields great\npotential for addressing the cosmic coincidence problem."}
{"id": "2510.16053", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16053", "abs": "https://arxiv.org/abs/2510.16053", "authors": ["Chenyang Yu", "Xinpeng Xie", "Yan Huang", "Chenxi Qiu"], "title": "FUSE-Traffic: Fusion of Unstructured and Structured Data for Event-aware Traffic Forecasting", "comment": null, "summary": "Accurate traffic forecasting is a core technology for building Intelligent\nTransportation Systems (ITS), enabling better urban resource allocation and\nimproved travel experiences. With growing urbanization, traffic congestion has\nintensified, highlighting the need for reliable and responsive forecasting\nmodels. In recent years, deep learning, particularly Graph Neural Networks\n(GNNs), has emerged as the mainstream paradigm in traffic forecasting. GNNs can\neffectively capture complex spatial dependencies in road network topology and\ndynamic temporal evolution patterns in traffic flow data. Foundational models\nsuch as STGCN and GraphWaveNet, along with more recent developments including\nSTWave and D2STGNN, have achieved impressive performance on standard traffic\ndatasets. These approaches incorporate sophisticated graph convolutional\nstructures and temporal modeling mechanisms, demonstrating particular\neffectiveness in capturing and forecasting traffic patterns characterized by\nperiodic regularities. To address this challenge, researchers have explored\nvarious ways to incorporate event information. Early attempts primarily relied\non manually engineered event features. For instance, some approaches introduced\nmanually defined incident effect scores or constructed specific subgraphs for\ndifferent event-induced traffic conditions. While these methods somewhat\nenhance responsiveness to specific events, their core drawback lies in a heavy\nreliance on domain experts' prior knowledge, making generalization to diverse\nand complex unknown events difficult, and low-dimensional manual features often\nlead to the loss of rich semantic details."}
{"id": "2510.17140", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.17140", "abs": "https://arxiv.org/abs/2510.17140", "authors": ["Jhen-Dong Lin", "Pao-Wen Tu", "Kuan-Yi Lee", "Neill Lambert", "Adam Miranowicz", "Franco Nori", "Yueh-Nan Chen"], "title": "Resource efficient certification of system environment entanglement solely from reduced system dynamics", "comment": "12 pages, 4 figures", "summary": "Certifying nonclassical correlations typically requires access to all\nsubsystems, presenting a major challenge in open quantum systems coupled to\ninaccessible environments. Recent works have shown that, in autonomous pure\ndephasing scenarios, quantum discord with the environment can be certified from\nsystem-only dynamics via the Hamiltonian ensemble formulation. However, this\napproach leaves open whether stronger correlations, such as entanglement, can\nbe certified. Moreover, its reliance on Fourier analysis requires full-time\ndynamics, which is experimentally resource-intensive and provides limited\ninformation about when such correlations are established during evolution. In\nthis work, we present a method that enables the certification of\nsystem-environment quantum entanglement solely from the reduced dynamics of the\nsystem. The method is based on the theory of mixed-unitary channels and applies\nto general non-autonomous pure dephasing scenarios. Crucially, it relaxes the\nneed for full-time dynamics, offering a resource-efficient approach that also\nreveals the precise timing of entanglement generation. We experimentally\nvalidate this method on a Quantinuum trapped-ion quantum processor with a\ncontrolled-dephasing model. Finally, we highlight its potential as a tool for\ncertifying gravitationally induced entanglement."}
{"id": "2510.17787", "categories": ["gr-qc", "astro-ph.HE"], "pdf": "https://arxiv.org/pdf/2510.17787", "abs": "https://arxiv.org/abs/2510.17787", "authors": ["Nishkal Rao", "Anuj Mishra", "Apratim Ganguly", "Anupreeta More"], "title": "Comprehensive analysis of time-domain overlapping gravitational wave transients: A Lensing Study", "comment": "14 pages, 11 figures, 2 tables", "summary": "Next-generation GW detectors will produce a high rate of temporally\noverlapping signals from unrelated compact binary coalescences. Such overlaps\ncan bias parameter estimation (PE) and mimic signatures of other physical\neffects, such as gravitational lensing. In this work, we investigate how\noverlapping signals can be degenerate with gravitational lensing by focusing on\ntwo scenarios: Type-II strong lensing and microlensing by an isolated\npoint-mass lens. We simulate quasicircular binary black-hole pairs with\nchirp-mass ratios $\\mathscr{M}_{\\rm B}/\\mathscr{M}_{\\rm A}\\in\\{0.5,\\,1,\\,2\\}$,\nSNR ratios $\\mathrm{SNR}_{\\rm B}/\\mathrm{SNR}_{\\rm A}\\in\\{0.5,\\,1\\}$, and\ncoalescence-time offsets $\\Delta t_{\\rm c}\\in[-0.1,\\,0.1]~\\mathrm{s}$. Bayesian\nPE and fitting-factor studies show that the Type-II lensing hypothesis is\nfavored over the unlensed quasicircular hypothesis ($\\log_{10}\\mathscr{B}^{\\rm\nL}_{\\rm U}>1$) only in a small region of the overlapping parameter space with\n$\\mathscr{M}_{\\rm B}/\\mathscr{M}_{\\rm A}\\gtrsim1$ and $|\\Delta t_{\\rm\nc}|\\leq0.03~\\rm{s}$.. Meanwhile, false evidence for microlensing signatures can\narise because, to a reasonable approximation, the model produces two\nsuperimposed images whose time delay can closely match $|\\Delta t_{\\rm c}|$.\nOverall, the inferred Bayes factor depends on relative chirp-mass ratios,\nrelative loudness, difference in coalescence times, and also the absolute SNRs\nof the overlapping signals. Cumulatively, our results indicate that overlapping\nblack-hole binaries with nearly equal chirp masses and comparable loudness are\nlikely to be falsely identified as lensed. Such misidentifications are expected\nto become more common as detector sensitivities improve. While our study\nfocuses on ground-based detectors using appropriate detectability thresholds,\nthe findings naturally extend to next-generation GW observatories."}
{"id": "2510.16060", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16060", "abs": "https://arxiv.org/abs/2510.16060", "authors": ["Coen Adler", "Yuxin Chang", "Felix Draxler", "Samar Abdi", "Padhraic Smyth"], "title": "Beyond Accuracy: Are Time Series Foundation Models Well-Calibrated?", "comment": null, "summary": "The recent development of foundation models for time series data has\ngenerated considerable interest in using such models across a variety of\napplications. Although foundation models achieve state-of-the-art predictive\nperformance, their calibration properties remain relatively underexplored,\ndespite the fact that calibration can be critical for many practical\napplications. In this paper, we investigate the calibration-related properties\nof five recent time series foundation models and two competitive baselines. We\nperform a series of systematic evaluations assessing model calibration (i.e.,\nover- or under-confidence), effects of varying prediction heads, and\ncalibration under long-term autoregressive forecasting. We find that time\nseries foundation models are consistently better calibrated than baseline\nmodels and tend not to be either systematically over- or under-confident, in\ncontrast to the overconfidence often seen in other deep learning models."}
{"id": "2510.17183", "categories": ["quant-ph", "cond-mat.quant-gas", "cond-mat.str-el", "cond-mat.supr-con", "physics.atom-ph"], "pdf": "https://arxiv.org/pdf/2510.17183", "abs": "https://arxiv.org/abs/2510.17183", "authors": ["Mu Qiao", "Romain Martin", "Lukas Homeier", "Ivan Morera", "Bastien Gély", "Lukas Klein", "Yuki Torii Chew", "Daniel Barredo", "Thierry Lahaye", "Eugene Demler", "Antoine Browaeys"], "title": "Kinetically-induced bound states in a frustrated Rydberg tweezer array", "comment": null, "summary": "Understanding how particles bind into composite objects is a ubiquitous theme\nin physics, from the formation of molecules to hadrons in quantum\nchromodynamics and the pairing of charge carriers in superconductors. The\nformation of bound states usually originates from attractive interactions\nbetween particles. However, the binding can also arise purely from the motion\nof dopants due to kinetic frustration, which is potentially related to\nunconventional pairing in moir\\'e materials. Here, we report the first direct\nobservation of kinetically-induced bound states between holes and magnons using\na Rydberg atom array quantum simulator of the bosonic $t$-$J$ model in\nfrustrated ladders and 2D lattices. First, we demonstrate the formation of\nmobile one-hole-one-magnon bound states. We then construct three-particle\none-hole-two-magnon bound states and reveal the underlying binding mechanism by\nobserving kinetically-induced singlet correlations. Finally, we investigate how\nmobile dopants structure their magnetic environment in a spin-balanced 2D\ntriangular lattice, showing that a hole induces $120^\\circ$ antiferromagnetic\norder, while a doublon dopant generates in-plane ferromagnetic correlations.\nOur results demonstrates compelling evidence of kinetically-induced binding,\nopening a new avenue to understand novel pairing mechanisms in correlated\nquantum materials like superconductors in moir\\'e superlattices."}
{"id": "2510.17789", "categories": ["gr-qc", "astro-ph.CO", "hep-ph"], "pdf": "https://arxiv.org/pdf/2510.17789", "abs": "https://arxiv.org/abs/2510.17789", "authors": ["Gaetano Lambiase", "Shinji Mukohyama", "Tanmay Kumar Poddar", "Anna Chiara Rescigno"], "title": "Exorcising ghosts with gravitational waves: cases of ghostful and ghost-free fourth-order gravity", "comment": "49 pages, 19 figures, comments are welcome", "summary": "General Relativity (GR) is an effective field theory valid in the infrared\nregime. Quadratic curvature extensions intended to probe ultraviolet physics\ngenerically propagate a massive spin-$2$ ghost and are therefore non-unitary.\nOne route to remove ghost is by enlarging the geometric sector (torsion,\nnon-metricity). We investigate the infrared phenomenology of both the standard\n(ghostful) and ghost-free fourth-order gravity theories by computing\nGravitational Wave (GW) emission and confronting the results with observations\nsuch as the orbital-period decay of quasi-stable binaries such as PSR B1913+16\nand PSR J1738+0333 and the chirp-mass evolution of GW170817. In the ghostful\ntheory, besides the theoretical inconsistency due to non-unitarity, there are\nalso phenomenological problems: the massless spin-$2$ GW flux cancels the\ncombined GW fluxes of the massive spin-$2$ ghost and massive spin-$0$ scalar in\nthe vanishing-mass limit, so the GR quadrupole formula is not recovered at the\nleading order. As a result, we obtain the GW constraint on the ghostful theory\nas $m\\gtrsim 10^{-11}~\\mathrm{eV}$, where $m$ is the mass of the massive modes.\nBy contrast, the ghost-free theory smoothly reproduces the Newtonian potential\nand GR quadrupole formulae when the two coupling constants $\\alpha_1$ and\n$\\alpha_2$ vanish, independently of the mass $m$. Therefore, GW observations\nput mass-dependent upper bounds on the size of the coupling constants. For\nexample, if we assume $\\alpha_1\\simeq\\alpha_2$ for simplicity, then we obtain\n$\\alpha_{1,2}\\lesssim 4.2\\times 10^{83}$ for $m\\sim 3\\times\n10^{-16}\\,\\mathrm{eV}$ and $\\alpha_{1,2}\\lesssim 1.3\\times 10^{75}$ for $m\\sim\n10^{-11}\\,\\mathrm{eV}$. To our knowledge, these are the first\nastrophysical-scale bounds reported for ghostful and ghost-free fourth-order\ngravity."}
{"id": "2510.16063", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16063", "abs": "https://arxiv.org/abs/2510.16063", "authors": ["Muhy Eddin Za'ter", "Bri-Mathias Hodge"], "title": "Learning a Generalized Model for Substation Level Voltage Estimation in Distribution Networks", "comment": null, "summary": "Accurate voltage estimation in distribution networks is critical for\nreal-time monitoring and increasing the reliability of the grid. As DER\npenetration and distribution level voltage variability increase, robust\ndistribution system state estimation (DSSE) has become more essential to\nmaintain safe and efficient operations. Traditional DSSE techniques, however,\nstruggle with sparse measurements and the scale of modern feeders, limiting\ntheir scalability to large networks. This paper presents a hierarchical graph\nneural network for substation-level voltage estimation that exploits both\nelectrical topology and physical features, while remaining robust to the low\nobservability levels common to real-world distribution networks. Leveraging the\npublic SMART-DS datasets, the model is trained and evaluated on thousands of\nbuses across multiple substations and DER penetration scenarios. Comprehensive\nexperiments demonstrate that the proposed method achieves up to 2 times lower\nRMSE than alternative data-driven models, and maintains high accuracy with as\nlittle as 1\\% measurement coverage. The results highlight the potential of GNNs\nto enable scalable, reproducible, and data-driven voltage monitoring for\ndistribution systems."}
{"id": "2510.17217", "categories": ["quant-ph", "physics.atm-clus"], "pdf": "https://arxiv.org/pdf/2510.17217", "abs": "https://arxiv.org/abs/2510.17217", "authors": ["A. Chernyavskiy", "I. S. Cojocaru", "S. M. Drofa", "P. G. Vilyuzhanina", "A. M. Kozodaev", "V. G. Vins", "A. N. Smolyaninov", "S. Ya. Kilin", "S. V. Bolshedvorskii", "V. V. Soshenko", "A. V. Akimov"], "title": "Double electron resonance with two ensembles of nitrogen-vacancy centers in diamond", "comment": null, "summary": "Nitrogen-vacancy (NV) centers in diamond are widely used in the development\nof a number of sensors. The sensitivity of these devices is limited by both the\nnumber of centers used and their coherent properties. While the effects on the\ncoherent properties of paramagnetic impurities such as carbon 13-isotopes and\np1 centers are rather well understood, the mutual interaction of NV centers,\nwhich becomes especially important in relatively dense NV ensembles, is less\nwell understood. Here, we provide a systematic study of NV-NV interaction using\na dynamical double electron-electron resonance sequence, making it possible to\ndirectly observe the interaction of NV centers. Two types of dynamical DEER\nsequences were considered, consisting of 3 and 4 pulses. The nature of the\nphase jump in the 3-pulse sequence was attributed to the effect of\nnon-commuting rotations within the sequence. Both the phase of the state vector\nrotation and its amplitude decay were studied, thus presenting a complete\npicture of decoherence due to NV-NV interaction. It was shown that the rate of\nthe state vector decay differed significantly from predictions for a spin 1/2\nsystem. However, the decay rate observed in the DEER sequence remained a\nreliable indicator of the concentration of bath spins and could be used to\nmeasure NV center concentration, provided that the magnetic transition of NV\ncenters is saturated."}
{"id": "2510.17513", "categories": ["quant-ph", "gr-qc", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2510.17513", "abs": "https://arxiv.org/abs/2510.17513", "authors": ["M. J. Luo"], "title": "Quantum Mechanics Relative to a Quantum Reference System: Relative State Approach", "comment": "23 pages", "summary": "This paper proposes an intrinsic or background-independent quantum framework\nbased on entangled state rather than absolute quantum state, it describes a\nquantum relative state between the under-study quantum system and the quantum\nmeasuring apparatus as a quantum reference system, without relying on any\nexternal absolute parameter. The paper focuses on a simple example, in which a\nquantum object's one-dimensional position as an under-study quantum system, and\na quantum clock as a quantum reference system or quantum measuring apparatus.\nThe evolution equation of the state of the quantum object's position with\nrespect to the state of the quantum clock is given, which is found to be a\ncomplex Gauss-Codazzi type equation of the total quantum state space coming\nfrom the Ricci-flat Kahler-Einstein equation. In a linear and non-relativistic\napproximation, the framework recovers the equation of the standard quantum\nmechanics, in which an intrinsic potential related to some \"inertial force\" is\nautomatically incorporated in the covariant derivative. A physical relative\nprobability interpretation and a geometric non-trivial fiber bundle\ninterpretation of the entangled state in this intrinsic quantum framework are\ngiven. Furthermore, some non-inertial effects, such as the \"inertial force\",\ncoming from the general covariance of the intrinsic quantum framework are also\ndiscussed. Compared with the functional integral approach which is more easily\nto generalize the quantum clock to the quantum spacetime reference frame and\nstudy quantum gravity, the relative state approach as a canonical description\nis more suitable for conceptually demonstrating the connections to the standard\nformalism and interpretation of the quantum mechanics."}
{"id": "2510.16064", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16064", "abs": "https://arxiv.org/abs/2510.16064", "authors": ["Muhy Eddin Za'ter", "Bri-Mathias Hodge", "Kyri Baker"], "title": "Residual Correction Models for AC Optimal Power Flow Using DC Optimal Power Flow Solutions", "comment": null, "summary": "Solving the nonlinear AC optimal power flow (AC OPF) problem remains a major\ncomputational bottleneck for real-time grid operations. In this paper, we\npropose a residual learning paradigm that uses fast DC optimal power flow (DC\nOPF) solutions as a baseline, and learns only the nonlinear corrections\nrequired to provide the full AC-OPF solution. The method utilizes a\ntopology-aware Graph Neural Network with local attention and two-level DC\nfeature integration, trained using a physics-informed loss that enforces AC\npower-flow feasibility and operational limits. Evaluations on OPFData for 57-,\n118-, and 2000-bus systems show around 25% lower MSE, up to 3X reduction in\nfeasibility error, and up to 13X runtime speedup compared to conventional AC\nOPF solvers. The model maintains accuracy under N-1 contingencies and scales\nefficiently to large networks. These results demonstrate that residual learning\nis a practical and scalable bridge between linear approximations and\nAC-feasible OPF, enabling near real-time operational decision making."}
{"id": "2510.17224", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2510.17224", "abs": "https://arxiv.org/abs/2510.17224", "authors": ["Eduard Naichuk", "Jeroen van den Brink", "Flavio S. Nogueira"], "title": "Real critical exponents from the $\\varepsilon$-expansion in an interacting $U(1)$ model with non-Hermitian $Z_4$ anisotropy", "comment": "7 pages, 2 figures", "summary": "In quantum optics and condensed matter physics non-Hermitian phenomena are\noften studied under the assumption of an open physical system. However, there\nare examples of intrinsically non-Hermitian, though often $\\mathcal{PT}$\n(parity-time) symmetric, not necessarily open systems, in which case the\nconcept of gain and loss relative to an underlying environment is not\nprimordial. A particularly intriguing example with experimental consequences in\nthe literature is QCD at finite density. Motivated by the existence of such\ninherently non-Hermitian systems, here we study the critical behavior of a\n$U(1)$-invariant Lagrangian perturbed by a complex, $\\mathcal{PT}$ symmetric\n$Z_{4}$ anisotropy. We find real critical exponents both in the region of\nunbroken and broken $\\mathcal{PT}$ symmetry. In the former the coupling\nconstants for fixed points or lines are real, whereas in the latter they become\ncomplex. Importantly, the most stable fixed point corresponds to the flow at\nlarge distances towards an effectively Hermitian $U(1)$ symmetric system. This\nconstitutes an example where both the $U(1)$ and the Hermitian character are\nemergent features of the theory. This tells us about the importance and\nphysical meaning of some non-Hermitian systems beyond interpretations involving\ngain and loss."}
{"id": "2510.16065", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16065", "abs": "https://arxiv.org/abs/2510.16065", "authors": ["Lunchen Xie", "Zehua He", "Qingjiang Shi"], "title": "FedPURIN: Programmed Update and Reduced INformation for Sparse Personalized Federated Learning", "comment": null, "summary": "Personalized Federated Learning (PFL) has emerged as a critical research\nfrontier addressing data heterogeneity issue across distributed clients. Novel\nmodel architectures and collaboration mechanisms are engineered to accommodate\nstatistical disparities while producing client-specific models. Parameter\ndecoupling represents a promising paradigm for maintaining model performance in\nPFL frameworks. However, the communication efficiency of many existing methods\nremains suboptimal, sustaining substantial communication burdens that impede\npractical deployment. To bridge this gap, we propose Federated Learning with\nProgrammed Update and Reduced INformation (FedPURIN), a novel framework that\nstrategically identifies critical parameters for transmission through an\ninteger programming formulation. This mathematically grounded strategy is\nseamlessly integrated into a sparse aggregation scheme, achieving a significant\ncommunication reduction while preserving the efficacy. Comprehensive\nevaluations on standard image classification benchmarks under varied non-IID\nconditions demonstrate competitive performance relative to state-of-the-art\nmethods, coupled with quantifiable communication reduction through sparse\naggregation. The framework establishes a new paradigm for\ncommunication-efficient PFL, particularly advantageous for edge intelligence\nsystems operating with heterogeneous data sources."}
{"id": "2510.17231", "categories": ["quant-ph", "cs.IT", "math.IT", "81P73, 94A24"], "pdf": "https://arxiv.org/pdf/2510.17231", "abs": "https://arxiv.org/abs/2510.17231", "authors": ["Simeon Ball", "Raven Zhang"], "title": "Error-correcting codes and absolutely maximally entangled states for mixed dimensional Hilbert spaces", "comment": null, "summary": "A major difficulty in quantum computation is the ability to implement fault\ntolerant computations, protecting information against undesired interactions\nwith the environment. Stabiliser codes were introduced as a means to protect\ninformation when storing or applying computations in Hilbert spaces where the\nlocal dimension is fixed, i.e. in Hilbert spaces of the form $({\\mathbb\nC}^D)^{\\otimes n}$. If $D$ is a prime power then one can consider stabiliser\ncodes over finite fields \\cite{KKKS2006}, which allows a deeper mathematical\nstructure to be used to develop stabiliser codes. However, there is no\npractical reason that the subsystems should have the same local dimension and\nin this article we introduce a stabiliser formalism for mixed dimensional\nHilbert spaces, i.e. of the form ${\\mathbb C}^{D_1} \\otimes \\cdots \\otimes\n{\\mathbb C}^{D_n}$. More generally, we define and prove a Singleton bound for\nquantum error-correcting codes of mixed dimensional Hilbert spaces. We redefine\nentanglement measures for these Hilbert spaces and follow \\cite{HESG2018} and\ndefine absolutely maximally entangled states as states which maximise this\nentanglement measure. We provide examples of absolutely maximally entangled\nstates in spaces of dimensions not previously known to have absolutely\nmaximally entangled states."}
{"id": "2510.16071", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16071", "abs": "https://arxiv.org/abs/2510.16071", "authors": ["Qinxuan Wang", "Chuang Wang", "Mingyu Zhang", "Jingwei Sun", "Peipei Yang", "Shuo Tang", "Shiming Xiang"], "title": "MNO: Multiscale Neural Operator for Computational Fluid Dynamics with 3D Point Cloud Data", "comment": null, "summary": "Neural operators have emerged as a powerful data-driven paradigm for solving\nPartial Differential Equations (PDEs), offering orders-of-magnitude\nacceleration over traditional solvers. However, existing approaches still\nsuffer from limited accuracy and scalability, particularly on irregular domains\nwhere fluid flows exhibit rich multiscale structures. In this work, we\nintroduce the Multiscale Neural Operator (MNO), a new architecture for\nComputational Fluid Dynamics (CFD) on three-dimensional (3D) unstructured point\nclouds. MNO explicitly decomposes information across three scales: a global\ndimension-shrinkage attention module for long-range dependencies, a local graph\nattention module for neighborhood-level interactions, and a micro point-wise\nattention module for fine-grained details. This design preserves multiscale\ninductive biases while remaining computationally efficient. We evaluate MNO on\nfour diverse benchmarks, covering both steady-state and unsteady flow scenarios\nwith up to 300K points. Across all tasks, MNO consistently outperforms\nstate-of-the-art baselines, reducing prediction errors by 5% to 40% and\ndemonstrating improved robustness in challenging 3D CFD problems. Our results\nhighlight the importance of explicit multiscale design for neural operators and\nestablish MNO as a scalable framework for learning complex fluid dynamics on\nirregular domains."}
{"id": "2510.17248", "categories": ["quant-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2510.17248", "abs": "https://arxiv.org/abs/2510.17248", "authors": ["Cătălin Paşcu Moca", "Doru Sticlet", "Balázs Dóra"], "title": "Non-stabilizerness as a Diagnostic of Criticality and Exceptional Points in Non-Hermitian Spin Chains", "comment": "8 pages, 7 figures", "summary": "We investigate non-stabilizerness, also known as ``magic,'' to understand\ncriticality and exceptional points in non-Hermitian quantum many-body systems.\nOur focus is on parity-time ($\\mathcal{PT}$) symmetric spin chains,\nspecifically the non-Hermitian transverse-field Ising and XX models. We\ncalculate stabilizer R\\'enyi entropies in their ground states using\nnon-Hermitian matrix product state methods. Our findings show that magic\nexhibits unique and model-specific signs of phase transitions. In the Ising\nchain, it peaks along the regular Hermitian-like critical line but disappears\nacross exceptional points. In contrast, in the XX chain, it reaches its maximum\nat the exceptional line where $\\mathcal{PT}$ symmetry is broken. Finite-size\nscaling reveals that these effects become more pronounced with larger systems,\nhighlighting non-stabilizerness as a sensitive marker for both quantum\ncriticality and non-Hermitian spectral degeneracies. We also investigate magic\nin momentum space for the XX model analytically and find that is reaches a\nminimum around exceptional points. Our results indicate that magic takes\nextremal values at the exceptional points and serves as a valuable tool for\nexamining complexity, criticality, and symmetry breaking in non-Hermitian\nquantum matter."}
{"id": "2510.16074", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16074", "abs": "https://arxiv.org/abs/2510.16074", "authors": ["Jing He", "Hua Jiang", "Cheng Li", "Siqian Xin", "Shuzhen Yang"], "title": "Early-stopping for Transformer model training", "comment": null, "summary": "This work introduces a novel theoretical framework grounded in Random Matrix\nTheory (RMT) for analyzing Transformer training dynamics. We focus on the\nunderlying mechanisms that drive performance improvements and derive principled\nearly-stopping criteria. Empirically, we observe that the spectral density of\nthe shallow self-attention matrix V consistently evolves into a heavy-tailed\ndistribution. Utilizing the PL (Power Law) fit to this matrix as a probe, we\ndemarcate training into three stages: structural exploration, heavy-tailed\nstructure stabilization, and convergence saturation. This staging provides\nguidance for preliminary stopping decisions. Crucially, we propose two\nconsistent and validation-free criteria: a quantitative metric for heavy-tailed\ndynamics and a novel spectral signature indicative of convergence. The strong\nalignment between these criteria highlights the utility of RMT for monitoring\nand diagnosing the progression of Transformer model training."}
{"id": "2510.17275", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.17275", "abs": "https://arxiv.org/abs/2510.17275", "authors": ["Tian-Yu Wang", "Ren-Hui Chen", "Yan Li", "Ze-Hao Shen", "Xiao-Song Fan", "Zheng-Bang Ju", "Tian-Ci Tang", "Xia-Wei Li", "Jing-Yuan Peng", "Zhi-Yuan Zhou", "Wei Zhang", "Guang-Can Guo", "Bao-Sen Shi"], "title": "Long-distance distribution of atom-photon entanglement based on a cavity-free cold atomic ensemble", "comment": "11 pages, 6 figures", "summary": "Constructing a quantum memory node with the ability of long-distance\natom-photon distribution is the essential task for future quantum networks,\nenabling distributed quantum computing, quantum cryptography and remote\nsensing. Here we report the demonstration of a quantum-network node with a\nsimple cavity-free cold atomic ensemble. This node gives an initial retrieval\nefficiency of approximately 50\\% and memory lifetime of 160 $\\mu$s for atomic\nqubits. With the aid of a high-efficiency and polarization-independent quantum\nfrequency conversion (QFC) module, the generated entangled photon in the node\nat 780-nm wavelength is converted to telecom S band at 1522 nm, enabling\natom-photon distribution over long distance. We observe an entanglement\nfidelity between the atoms and telecom photon exceeding 80\\% after photon\ntransmission over 20-km fiber, the remaining infidelity being dominated by\natomic decoherence. The low-noise QFC with an external efficiency up to 48.5\\%\ngives a signal-to-noise-ratio of 6.9 for transmitted photons with fiber length\nup to 100 km, laying the cornerstone for entanglement distribution at a\nhundred-km level. This result provides a new platform towards the realization\nof a long-distance quantum network."}
{"id": "2510.16075", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16075", "abs": "https://arxiv.org/abs/2510.16075", "authors": ["Sergio Muñiz Subiñas", "Manuel L. González", "Jorge Ruiz Gómez", "Alejandro Mata Ali", "Jorge Martínez Martín", "Miguel Franco Hernando", "Ángel Miguel García-Vico"], "title": "Optimization of the quantization of dense neural networks from an exact QUBO formulation", "comment": null, "summary": "This work introduces a post-training quantization (PTQ) method for dense\nneural networks via a novel ADAROUND-based QUBO formulation. Using the\nFrobenius distance between the theoretical output and the dequantized output\n(before the activation function) as the objective, an explicit QUBO whose\nbinary variables represent the rounding choice for each weight and bias is\nobtained. Additionally, by exploiting the structure of the coefficient QUBO\nmatrix, the global problem can be exactly decomposed into $n$ independent\nsubproblems of size $f+1$, which can be efficiently solved using some\nheuristics such as simulated annealing. The approach is evaluated on MNIST,\nFashion-MNIST, EMNIST, and CIFAR-10 across integer precisions from int8 to int1\nand compared with a round-to-nearest traditional quantization methodology."}
{"id": "2510.17286", "categories": ["quant-ph", "physics.atom-ph"], "pdf": "https://arxiv.org/pdf/2510.17286", "abs": "https://arxiv.org/abs/2510.17286", "authors": ["A. C. Hughes", "R. Srinivas", "C. M. Löschnauer", "H. M. Knaack", "R. Matt", "C. J. Ballance", "M. Malinowski", "T. P. Harty", "R. T. Sutherland"], "title": "Trapped-ion two-qubit gates with >99.99% fidelity without ground-state cooling", "comment": null, "summary": "We introduce the 'smooth gate', an entangling method for trapped-ion qubits\nwhere residual spin-motion entanglement errors are adiabatically eliminated by\nramping the gate detuning. We demonstrate electronically controlled two-qubit\ngates with an estimated error of $8.4(7)\\times10^{-5}$ without ground-state\ncooling. We further show that the error remains $\\lesssim 5\\times10^{-4}$ for\nions with average phonon occupation up to $\\bar{n}=9.4(3)$ on the gate mode.\nThese results indicate that trapped-ion quantum computation can achieve high\nfidelity at temperatures above the Doppler limit, which enables faster and\nsimpler device operation."}
{"id": "2510.16076", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.16076", "abs": "https://arxiv.org/abs/2510.16076", "authors": ["SeongKu Kang", "Jianxun Lian", "Dongha Lee", "Wonbin Kweon", "Sanghwan Jang", "Jaehyun Lee", "Jindong Wang", "Xing Xie", "Hwanjo Yu"], "title": "BPL: Bias-adaptive Preference Distillation Learning for Recommender System", "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Recommender systems suffer from biases that cause the collected feedback to\nincompletely reveal user preference. While debiasing learning has been\nextensively studied, they mostly focused on the specialized (called\ncounterfactual) test environment simulated by random exposure of items,\nsignificantly degrading accuracy in the typical (called factual) test\nenvironment based on actual user-item interactions. In fact, each test\nenvironment highlights the benefit of a different aspect: the counterfactual\ntest emphasizes user satisfaction in the long-terms, while the factual test\nfocuses on predicting subsequent user behaviors on platforms. Therefore, it is\ndesirable to have a model that performs well on both tests rather than only\none. In this work, we introduce a new learning framework, called Bias-adaptive\nPreference distillation Learning (BPL), to gradually uncover user preferences\nwith dual distillation strategies. These distillation strategies are designed\nto drive high performance in both factual and counterfactual test environments.\nEmploying a specialized form of teacher-student distillation from a biased\nmodel, BPL retains accurate preference knowledge aligned with the collected\nfeedback, leading to high performance in the factual test. Furthermore, through\nself-distillation with reliability filtering, BPL iteratively refines its\nknowledge throughout the training process. This enables the model to produce\nmore accurate predictions across a broader range of user-item combinations,\nthereby improving performance in the counterfactual test. Comprehensive\nexperiments validate the effectiveness of BPL in both factual and\ncounterfactual tests. Our implementation is accessible via:\nhttps://github.com/SeongKu-Kang/BPL."}
{"id": "2510.17317", "categories": ["quant-ph", "cond-mat.str-el", "hep-th", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2510.17317", "abs": "https://arxiv.org/abs/2510.17317", "authors": ["Pei-Yao Liu"], "title": "Entanglement Sum Rule from Higher-Form Symmetries", "comment": null, "summary": "We prove an entanglement sum rule for $(d{-}1)$-dimensional quantum lattice\nmodels with finite abelian higher-form symmetries, obtained by minimally\ncoupling a sector on $p$-simplices carrying a $p$-form $G$ symmetry to a sector\non $(p{+}1)$-simplices carrying the dual $(d{-}p{-}2)$-form $\\widehat G$\nsymmetry (with $\\widehat G$ the Pontryagin dual of $G$). The coupling is\nintroduced by conjugation with a symmetry-preserving operator $\\mathcal{U}$\nthat dresses symmetry-invariant operators with appropriate Wilson operators. On\nthe symmetry-invariant subspace, $\\mathcal{U}$ is well-defined and unitary, and\nthe coupled Hamiltonian is obtained from the decoupled one by conjugation with\n$\\mathcal{U}$. Our main result concerns symmetric eigenstates of the coupled\nmodel that arise by acting with $\\mathcal{U}$ on direct-product, symmetric\neigenstates of the decoupled model: provided a topological criterion formulated\nvia the Mayer--Vietoris sequence holds for the chosen bipartition,\n$\\mathcal{U}$ factorizes across the cut when acting on the symmetric state, and\nthe bipartite entanglement entropy equals the sum of the entropies of the two\nsectors. The framework explains and generalizes known examples in\nfermion-$\\mathbb{Z}_2$ gauge theory, identifies when topology obstructs the\nfactorization, and provides a procedure to construct new examples by gauging\nhigher-form symmetries."}
{"id": "2510.16077", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16077", "abs": "https://arxiv.org/abs/2510.16077", "authors": ["Naeem Paeedeh", "Mahardhika Pratama", "Weiping Ding", "Jimmy Cao", "Wolfgang Mayer", "Ryszard Kowalczyk"], "title": "Continual Knowledge Consolidation LORA for Domain Incremental Learning", "comment": null, "summary": "Domain Incremental Learning (DIL) is a continual learning sub-branch that\naims to address never-ending arrivals of new domains without catastrophic\nforgetting problems. Despite the advent of parameter-efficient fine-tuning\n(PEFT) approaches, existing works create task-specific LoRAs overlooking shared\nknowledge across tasks. Inaccurate selection of task-specific LORAs during\ninference results in significant drops in accuracy, while existing works rely\non linear or prototype-based classifiers, which have suboptimal generalization\npowers. Our paper proposes continual knowledge consolidation low rank\nadaptation (CONEC-LoRA) addressing the DIL problems. CONEC-LoRA is developed\nfrom consolidations between task-shared LORA to extract common knowledge and\ntask-specific LORA to embrace domain-specific knowledge. Unlike existing\napproaches, CONEC-LoRA integrates the concept of a stochastic classifier whose\nparameters are sampled from a distribution, thus enhancing the likelihood of\ncorrect classifications. Last but not least, an auxiliary network is deployed\nto optimally predict the task-specific LoRAs for inferences and implements the\nconcept of a different-depth network structure in which every layer is\nconnected with a local classifier to take advantage of intermediate\nrepresentations. This module integrates the ball-generator loss and\ntransformation module to address the synthetic sample bias problem. Our\nrigorous experiments demonstrate the advantage of CONEC-LoRA over prior arts in\n4 popular benchmark problems with over 5% margins."}
{"id": "2510.17327", "categories": ["quant-ph", "hep-th", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2510.17327", "abs": "https://arxiv.org/abs/2510.17327", "authors": ["Filippus S. Roux"], "title": "Tagged vector space, Part I: Dirac notation as originally intended", "comment": "22 pages, no figure, comments welcome", "summary": "A generalization is provided for the notion of tags, as used in various\nformulations of physical scenarios. It leads to the definition of tagged vector\nspaces, based on a set of axioms for tags and their extractors. As an\napplication, such a tagged vector space is used to provide, in the context of\nquantum optics, a formal mathematical description for the Dirac notation that\nis closer to its intended usage compared to current mathematical formulations:\nit provides a one-to-one mapping between kets and bras and allows operators to\noperate either to the left or to the right. The canonical commutation relations\nfor the quadrature and ladder operators are derived as consequences of the\naxioms of the tagged vector space. These axioms also lead to a symplectic phase\nspace with the Wigner function and the Weyl transform emerging naturally."}
{"id": "2510.16083", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16083", "abs": "https://arxiv.org/abs/2510.16083", "authors": ["Jaehan Kim", "Minkyoo Song", "Minjae Seo", "Youngjin Jin", "Seungwon Shin", "Jinwoo Kim"], "title": "PassREfinder-FL: Privacy-Preserving Credential Stuffing Risk Prediction via Graph-Based Federated Learning for Representing Password Reuse between Websites", "comment": "Accepted by Elsevier Expert Systems with Applications", "summary": "Credential stuffing attacks have caused significant harm to online users who\nfrequently reuse passwords across multiple websites. While prior research has\nattempted to detect users with reused passwords or identify malicious login\nattempts, existing methods often compromise usability by restricting password\ncreation or website access, and their reliance on complex account-sharing\nmechanisms hinders real-world deployment. To address these limitations, we\npropose PassREfinder-FL, a novel framework that predicts credential stuffing\nrisks across websites. We introduce the concept of password reuse relations --\ndefined as the likelihood of users reusing passwords between websites -- and\nrepresent them as edges in a website graph. Using graph neural networks (GNNs),\nwe perform a link prediction task to assess credential reuse risk between\nsites. Our approach scales to a large number of arbitrary websites by\nincorporating public website information and linking newly observed websites as\nnodes in the graph. To preserve user privacy, we extend PassREfinder-FL with a\nfederated learning (FL) approach that eliminates the need to share user\nsensitive information across administrators. Evaluation on a real-world dataset\nof 360 million breached accounts from 22,378 websites shows that\nPassREfinder-FL achieves an F1-score of 0.9153 in the FL setting. We further\nvalidate that our FL-based GNN achieves a 4-11% performance improvement over\nother state-of-the-art GNN models through an ablation study. Finally, we\ndemonstrate that the predicted results can be used to quantify password reuse\nlikelihood as actionable risk scores."}
{"id": "2510.17349", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.17349", "abs": "https://arxiv.org/abs/2510.17349", "authors": ["Qisi Zhou", "Tao Jiang", "Qingqian Kang", "Teng Zhao", "Xin Su", "Cunjin Liu", "Liyun Hu"], "title": "Phase estimation via photon subtraction at the output of the hybrid interferometer", "comment": null, "summary": "The hybrid interferometer integrating an optical parametric amplifier and a\nbeam splitter has the potential to outperform the SU(1,1) interferometer.\nHowever, photon loss remains a critical limitation for practical\nimplementation. To address this challenge, we propose a quantum metrology\nscheme utilizing multi-photon subtraction at the output and replacing the\nconventional 50:50 beam splitter with a variable beam splitter to enhance\nrobustness against photon loss. We employ a coherent state and a vacuum state\nas inputs and perform homodyne detection. Our results show that the selection\nof input modes significantly affects phase estimation, and optimizing the beam\nsplitter's transmittance is crucial for maximizing phase sensitivity in lossy\nconditions. Furthermore, photon subtraction markedly improves phase\nsensitivity, quantum Fisher information, and robustness against noise. Our\nscheme achieves sensitivities beyond the Heisenberg limit even under 20% photon\nloss."}
{"id": "2510.16084", "categories": ["cs.LG", "cond-mat.quant-gas", "math-ph", "math.MP", "physics.optics"], "pdf": "https://arxiv.org/pdf/2510.16084", "abs": "https://arxiv.org/abs/2510.16084", "authors": ["Karol Sajnok", "Michał Matuszewski"], "title": "Near-Equilibrium Propagation training in nonlinear wave systems", "comment": "7 figures", "summary": "Backpropagation learning algorithm, the workhorse of modern artificial\nintelligence, is notoriously difficult to implement in physical neural\nnetworks. Equilibrium Propagation (EP) is an alternative with comparable\nefficiency and strong potential for in-situ training. We extend EP learning to\nboth discrete and continuous complex-valued wave systems. In contrast to\nprevious EP implementations, our scheme is valid in the weakly dissipative\nregime, and readily applicable to a wide range of physical settings, even\nwithout well defined nodes, where trainable inter-node connections can be\nreplaced by trainable local potential. We test the method in driven-dissipative\nexciton-polariton condensates governed by generalized Gross-Pitaevskii\ndynamics. Numerical studies on standard benchmarks, including a simple logical\ntask and handwritten-digit recognition, demonstrate stable convergence,\nestablishing a practical route to in-situ learning in physical systems in which\nsystem control is restricted to local parameters."}
{"id": "2510.17375", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.17375", "abs": "https://arxiv.org/abs/2510.17375", "authors": ["Zheng-Chuan Wang"], "title": "Non-abelian thermal gauge potentials for high spin cold atom gases", "comment": "there 3 figures", "summary": "On the basis of the non-equilibrium Green function formalism, we derived a\nspinor Boltzmann equation for the Bose cold atom gases with high spin, which is\nachieved by a quantum Wigner transformation on the equation satisfied by the\nlesser Green function. After a Taylor series expansion on the scattering terms,\na temperature-dependent spinor damping force can be obtained, which can be\nrelated to a non-abelian thermal gauge potential. For the spin-1 Bose gas, the\nthermal gauge potential constitutes a SU(3) Lie algebra. As an example, we\ncalculate the spin coherence oscillation for the spin-1 Bose cold atom gas\ntrapped in the optical lattice. The relative populations in the Zeeman states\nas well as the temperature-dependent damping force are illustrated numerically."}
{"id": "2510.16086", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.16086", "abs": "https://arxiv.org/abs/2510.16086", "authors": ["Ziyang Liu", "Pengjunfei Chu", "Shuming Dong", "Chen Zhang", "Mingcheng Li", "Jin Wang"], "title": "FSRF: Factorization-guided Semantic Recovery for Incomplete Multimodal Sentiment Analysis", "comment": "6 pages,3 figures", "summary": "In recent years, Multimodal Sentiment Analysis (MSA) has become a research\nhotspot that aims to utilize multimodal data for human sentiment understanding.\nPrevious MSA studies have mainly focused on performing interaction and fusion\non complete multimodal data, ignoring the problem of missing modalities in\nreal-world applications due to occlusion, personal privacy constraints, and\ndevice malfunctions, resulting in low generalizability.\n  To this end, we propose a Factorization-guided Semantic Recovery Framework\n(FSRF) to mitigate the modality missing problem in the MSA task.\n  Specifically, we propose a de-redundant homo-heterogeneous factorization\nmodule that factorizes modality into modality-homogeneous,\nmodality-heterogeneous, and noisy representations and design elaborate\nconstraint paradigms for representation learning.\n  Furthermore, we design a distribution-aligned self-distillation module that\nfully recovers the missing semantics by utilizing bidirectional knowledge\ntransfer.\n  Comprehensive experiments on two datasets indicate that FSRF has a\nsignificant performance advantage over previous methods with uncertain missing\nmodalities."}
{"id": "2510.17419", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.17419", "abs": "https://arxiv.org/abs/2510.17419", "authors": ["Jennifer O Bartlett", "Alfie J Myers Wilson", "Christopher J Chunnilall", "Rupesh Kumar"], "title": "Detector Asymmetry in Continuous Variable Quantum Key Distribution", "comment": "8 pages, 7 figures", "summary": "In Local-local Oscillator (LLO) based Continuous-Variable Quantum Key\nDistribution (CV-QKD), the phase reference of the transmitter and receiver,\nAlice and Bob, are naturally de-correlated due to their use of individual\nlasers. A phase reference signal is used, whose measurement is critical for\nestimating the phase difference and correcting the raw QKD data. We observed\nthat asymmetry in the quadrature measurements of the shot noise-limited\nheterodyne detector affects the accuracy of the reference signal's phase\nestimation and thereby reduces the achievable transmission distance and key\nrate of the CV-QKD system. We quantify the effect and propose a method to\ncounteract the effect of detection asymmetry. We also evaluate the effects of\ndetection asymmetry using quantum optical tomography."}
{"id": "2510.16089", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16089", "abs": "https://arxiv.org/abs/2510.16089", "authors": ["William Hoy", "Nurcin Celik"], "title": "STABLE: Gated Continual Learning for Large Language Models", "comment": null, "summary": "Large language models (LLMs) increasingly require mechanisms for continual\nadaptation without full retraining. However, sequential updates can lead to\ncatastrophic forgetting, where new edits degrade previously acquired knowledge.\nThis work presents STABLE, a gated continual self editing framework that\nconstrains forgetting during sequential updates using parameter efficient fine\ntuning via Low Rank Adaptation (LoRA; see arXiv:2106.09685). Each candidate\nedit is evaluated against a stability budget using one of three metrics: (i)\nExact Match (EM) drop, capturing factual accuracy loss; (ii) bits increase,\nreflecting reduced model confidence; and (iii) KL divergence, quantifying\ndistributional drift between the base and adapted models. If a threshold is\nexceeded, the LoRA update is rescaled through a clipping procedure or rejected.\nExperiments on the Qwen-2.5-7B model show that gating effectively mitigates\nforgetting while preserving adaptability. EM based gating achieved the highest\ncumulative performance in short continual learning sequences. Our results show\nthat different gating strategies can achieve comparable distribution shift\n(measured by KL divergence) while producing different accuracy outcomes,\nhighlighting the importance of gating design in continual adaptation. This\napproach offers a principled method for continual model editing, enabling LLMs\nto integrate new knowledge while maintaining reliability. Code:\nhttps://github.com/Bhoy1/STABLE"}
{"id": "2510.17461", "categories": ["quant-ph", "physics.atom-ph"], "pdf": "https://arxiv.org/pdf/2510.17461", "abs": "https://arxiv.org/abs/2510.17461", "authors": ["Francesco Troisi", "Simone Latini", "Heiko Appel", "Martin Lüders", "Angel Rubio", "Ivano Tavernelli"], "title": "Hardware-efficient formulation of molecular cavity-QED Hamiltonians", "comment": null, "summary": "Light-matter coupled Hamiltonians are central to cavity materials engineering\nand polaritonic chemistry, but are challenging to simulate with classical\nhardware due to the scaling of the Hilbert space with the number of quantum\nphoton modes and matter complexity. Leveraging the fact that quantum computers\nnaturally represent photonic modes efficiently, we present a novel approach to\nsimulate quantum-electrodynamical (QED) systems on near-term quantum hardware.\nAfter developing the bosonic and mixed operators in the Qiskit Nature\nframework, we employ them to simulate a first-order Trotterized Hamiltonian for\na spontaneous-emission problem of a two-level system in an optical cavity. We\nfind that using a standing-waves photonic basis approach leads to fidelity\nissues due to hardware connectivity constraints and two-qubits gates errors.\nHence, we propose using a localized photonic basis approach that enforces\nnearest-neighbor couplings, thanks to which we can map the Hamiltonian as a 1D\nqubit chain. We significantly reduce the noise and, by applying the zero-noise\nextrapolation error mitigation technique, we recover the accurate quantum\ndynamics. Finally, we also show that this approach is resilient when relaxing\nthe 1D qubit chain approximation."}
{"id": "2510.16092", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16092", "abs": "https://arxiv.org/abs/2510.16092", "authors": ["Devvrit Khatri", "Pranamya Kulkarni", "Nilesh Gupta", "Yerram Varun", "Liqian Peng", "Jay Yagnik", "Praneeth Netrapalli", "Cho-Jui Hsieh", "Alec Go", "Inderjit S Dhillon", "Aditya Kusupati", "Prateek Jain"], "title": "Compressing Many-Shots in In-Context Learning", "comment": null, "summary": "Large Language Models (LLMs) have been shown to be able to learn different\ntasks without explicit finetuning when given many input-output examples /\ndemonstrations through In-Context Learning (ICL). Increasing the number of\nexamples, called ``shots'', improves downstream task performance but incurs\nhigher memory and computational costs. In this work, we study an approach to\nimprove the memory and computational efficiency of ICL inference by compressing\nthe many-shot prompts. Given many shots comprising t tokens, our goal is to\ngenerate a m soft-token summary, where m < t. We first show that existing\nprompt compression methods are ineffective for many-shot compression, and\nsimply using fewer shots as a baseline is surprisingly strong. To achieve\neffective compression, we find that: (a) a stronger compressor model with more\ntrainable parameters is necessary, and (b) compressing many-shot\nrepresentations at each transformer layer enables more fine-grained compression\nby providing each layer with its own compressed representation. Based on these\ninsights, we propose MemCom, a layer-wise compression method. We systematically\nevaluate various compressor models and training approaches across different\nmodel sizes (2B and 7B), architectures (Gemma and Mistral), many-shot sequence\nlengths (3k-6k tokens), and compression ratios (3x to 8x). MemCom outperforms\nstrong baselines across all compression ratios on multiple classification tasks\nwith large label sets. Notably, while baseline performance degrades sharply at\nhigher compression ratios, often by over 20-30%, MemCom maintains high accuracy\nwith minimal degradation, typically dropping by less than 10%."}
{"id": "2510.17471", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.17471", "abs": "https://arxiv.org/abs/2510.17471", "authors": ["Peter W. Evans"], "title": "Is quantum mechanics merely a theory for us?", "comment": null, "summary": "This paper develops an agent-centric account of measurement that treats the\npreferred-basis problem is fundamentally perspectival. On this view, the\nsystem--apparatus--environment decomposition and the observables that are apt\nto become classically robust are determined by the physical constitution and\nepistemic constraints of an embodied class of agents. Decoherence then\nstabilises those agent-specified observables, yielding facts that are stable\nfor us without positing an absolute, observer-independent basis. On this\npicture, `measurements' are public not because they are metaphysically\nprivileged, but because agents like us share the relevant sensorimotor and\noperational structure. I motivate this account through a discussion of two\nrecent no-go results for relational quantum mechanics (RQM)\n(Brukner,2021;Pienaar,2021), and a subsequent response (DiBiagio and Rovelli,\n2022): my aim is not to defend RQM per se, but to refine the relational insight\nwith a principled account of basis selection rooted in embodiment. I provide a\nphenomenological gloss, drawing on body-schema considerations, to argue that\nquantum mechanics is best understood as an idiosyncratically human description\nof interactions with the physical world -- a structurally constrained,\nagent-indexed framework within which classicality emerges."}
{"id": "2510.16097", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.HC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16097", "abs": "https://arxiv.org/abs/2510.16097", "authors": ["Eleni Straitouri", "Stratis Tsirtsis", "Ander Artola Velasco", "Manuel Gomez-Rodriguez"], "title": "Narrowing Action Choices with AI Improves Human Sequential Decisions", "comment": "Accepted at the Human-AI Complementarity for Decision Making Workshop\n  2025 by the NSF AI Institute for Societal Decision Making", "summary": "Recent work has shown that, in classification tasks, it is possible to design\ndecision support systems that do not require human experts to understand when\nto cede agency to a classifier or when to exercise their own agency to achieve\ncomplementarity$\\unicode{x2014}$experts using these systems make more accurate\npredictions than those made by the experts or the classifier alone. The key\nprinciple underpinning these systems reduces to adaptively controlling the\nlevel of human agency, by design. Can we use the same principle to achieve\ncomplementarity in sequential decision making tasks? In this paper, we answer\nthis question affirmatively. We develop a decision support system that uses a\npre-trained AI agent to narrow down the set of actions a human can take to a\nsubset, and then asks the human to take an action from this action set. Along\nthe way, we also introduce a bandit algorithm that leverages the smoothness\nproperties of the action sets provided by our system to efficiently optimize\nthe level of human agency. To evaluate our decision support system, we conduct\na large-scale human subject study ($n = 1{,}600$) where participants play a\nwildfire mitigation game. We find that participants who play the game supported\nby our system outperform those who play on their own by $\\sim$$30$% and the AI\nagent used by our system by $>$$2$%, even though the AI agent largely\noutperforms participants playing without support. We have made available the\ndata gathered in our human subject study as well as an open source\nimplementation of our system at\nhttps://github.com/Networks-Learning/narrowing-action-choices ."}
{"id": "2510.17490", "categories": ["quant-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.17490", "abs": "https://arxiv.org/abs/2510.17490", "authors": ["Huan-Chen Shi", "Er-Liang Cui", "Dan Zhou"], "title": "Toward Autonomous Neural VMC: An Energy-Variance Convergence Criterion for Quantum Systems", "comment": "32 pages, 14 figures and 5 tables, suggestions and comments are\n  welcome", "summary": "The optimization of neural wave functions in variational Monte Carlo(VMC)\ncrucially relies on a robust convergence criterion. While the energy variance\nis theoretically a definitive measure of an eigenstate, its systematic\napplication as a primary, practical convergence criterion in neural-network VMC\nhas been underexplored. In this work, we propose and validate the energy\nvariance as a universal, quantitative criterion for convergence. Then its\nreliability is demonstrated across diverse quantum systems-from harmonic\noscillators and hydrogen atoms to charmonium hadrons-showing that a variance\nbelow 1*10^{-3} guarantees relative errors under 1%. This empirical threshold\nprovides a system-agnostic benchmark for convergence, enabling hands-off\noperation of the optimization process. We implement this criterion within a\nlightweight neural solver, thereby enabling automated parameter scans. Its\nutility is showcased by efficiently mapping ground-state properties of a 2D\ndouble-well potential, a hydrogen atom in a magnetic field, and a three-body\nquantum dot. Our work positions the energy-variance criterion as a robust and\nscalable tool that significantly accelerates the preliminary physical\nverification of quantum Hamiltonians."}
{"id": "2510.16123", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16123", "abs": "https://arxiv.org/abs/2510.16123", "authors": ["Federico Malato", "Ville Hautamäki"], "title": "Zero-shot World Models via Search in Memory", "comment": "10 pages, 8 figures in main text + appendices", "summary": "World Models have vastly permeated the field of Reinforcement Learning. Their\nability to model the transition dynamics of an environment have greatly\nimproved sample efficiency in online RL. Among them, the most notorious example\nis Dreamer, a model that learns to act in a diverse set of image-based\nenvironments. In this paper, we leverage similarity search and stochastic\nrepresentations to approximate a world model without a training procedure. We\nestablish a comparison with PlaNet, a well-established world model of the\nDreamer family. We evaluate the models on the quality of latent reconstruction\nand on the perceived similarity of the reconstructed image, on both next-step\nand long horizon dynamics prediction. The results of our study demonstrate that\na search-based world model is comparable to a training based one in both cases.\nNotably, our model show stronger performance in long-horizon prediction with\nrespect to the baseline on a range of visually different environments."}
{"id": "2510.17513", "categories": ["quant-ph", "gr-qc", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2510.17513", "abs": "https://arxiv.org/abs/2510.17513", "authors": ["M. J. Luo"], "title": "Quantum Mechanics Relative to a Quantum Reference System: Relative State Approach", "comment": "23 pages", "summary": "This paper proposes an intrinsic or background-independent quantum framework\nbased on entangled state rather than absolute quantum state, it describes a\nquantum relative state between the under-study quantum system and the quantum\nmeasuring apparatus as a quantum reference system, without relying on any\nexternal absolute parameter. The paper focuses on a simple example, in which a\nquantum object's one-dimensional position as an under-study quantum system, and\na quantum clock as a quantum reference system or quantum measuring apparatus.\nThe evolution equation of the state of the quantum object's position with\nrespect to the state of the quantum clock is given, which is found to be a\ncomplex Gauss-Codazzi type equation of the total quantum state space coming\nfrom the Ricci-flat Kahler-Einstein equation. In a linear and non-relativistic\napproximation, the framework recovers the equation of the standard quantum\nmechanics, in which an intrinsic potential related to some \"inertial force\" is\nautomatically incorporated in the covariant derivative. A physical relative\nprobability interpretation and a geometric non-trivial fiber bundle\ninterpretation of the entangled state in this intrinsic quantum framework are\ngiven. Furthermore, some non-inertial effects, such as the \"inertial force\",\ncoming from the general covariance of the intrinsic quantum framework are also\ndiscussed. Compared with the functional integral approach which is more easily\nto generalize the quantum clock to the quantum spacetime reference frame and\nstudy quantum gravity, the relative state approach as a canonical description\nis more suitable for conceptually demonstrating the connections to the standard\nformalism and interpretation of the quantum mechanics."}
{"id": "2510.16132", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16132", "abs": "https://arxiv.org/abs/2510.16132", "authors": ["Phalguni Nanda", "Zaiwei Chen"], "title": "A Minimal-Assumption Analysis of Q-Learning with Time-Varying Policies", "comment": "43 pages, 4 figures", "summary": "In this work, we present the first finite-time analysis of the Q-learning\nalgorithm under time-varying learning policies (i.e., on-policy sampling) with\nminimal assumptions -- specifically, assuming only the existence of a policy\nthat induces an irreducible Markov chain over the state space. We establish a\nlast-iterate convergence rate for $\\mathbb{E}[\\|Q_k - Q^*\\|_\\infty^2]$,\nimplying a sample complexity of order $O(1/\\epsilon^2)$ for achieving\n$\\mathbb{E}[\\|Q_k - Q^*\\|_\\infty] \\le \\epsilon$, matching that of off-policy\nQ-learning but with a worse dependence on exploration-related parameters. We\nalso derive an explicit rate for $\\mathbb{E}[\\|Q^{\\pi_k} - Q^*\\|_\\infty^2]$,\nwhere $\\pi_k$ is the learning policy at iteration $k$. These results reveal\nthat on-policy Q-learning exhibits weaker exploration than its off-policy\ncounterpart but enjoys an exploitation advantage, as its policy converges to an\noptimal one rather than remaining fixed. Numerical simulations corroborate our\ntheory.\n  Technically, the combination of time-varying learning policies (which induce\nrapidly time-inhomogeneous Markovian noise) and the minimal assumption on\nexploration presents significant analytical challenges. To address these\nchallenges, we employ a refined approach that leverages the Poisson equation to\ndecompose the Markovian noise corresponding to the lazy transition matrix into\na martingale-difference term and residual terms. To control the residual terms\nunder time inhomogeneity, we perform a sensitivity analysis of the Poisson\nequation solution with respect to both the Q-function estimate and the learning\npolicy. These tools may further facilitate the analysis of general\nreinforcement learning algorithms with rapidly time-varying learning policies\n-- such as single-timescale actor--critic methods and learning-in-games\nalgorithms -- and are of independent interest."}
{"id": "2510.17572", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.17572", "abs": "https://arxiv.org/abs/2510.17572", "authors": ["Ridwan Sakidja"], "title": "Quantum Reciprocity: A Structured-Bath Hamiltonian for Coherent Amplification", "comment": "33 pages, 2 tables, 2 figures, 4 appendices", "summary": "Macroscopic quantum amplifiers maintain coherence even while strongly coupled\nto their surroundings, demonstrating that coherence can be preserved through\narchitecture rather than isolation. Here we derive a finite structured-bath\nHamiltonian in which dissipation and feedback originate from the same\nmicroscopic couplings. The resulting self-energy {\\Sigma}({\\omega}) exhibits\ncoupled real and imaginary parts whose evolution reproduces the breathing\ndynamics observed in Josephson quantum amplifiers. This establishes quantum\nreciprocity: macroscopic coherence lives not in isolation, but in structured\nconnection. We numerically validate this principle by engineering a six-qubit\nstructured bath to demonstrate controllable transitions from dissipation to\namplification. This architectural core serves as the foundation for a proposed\nmulti-scale workflow to transform quantum noise into a design resource,\npreserving coherence not through isolation but through architectural\nreciprocity."}
{"id": "2510.16138", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16138", "abs": "https://arxiv.org/abs/2510.16138", "authors": ["Dung V. Nguyen", "Anh T. Nguyen", "Minh H. Nguyen", "Luc Q. Nguyen", "Shiqi Jiang", "Ethan Fetaya", "Linh Duy Tran", "Gal Chechik", "Tan M. Nguyen"], "title": "Expert Merging in Sparse Mixture of Experts with Nash Bargaining", "comment": "10 pages in the main text. Under Review", "summary": "Existing expert merging strategies for Sparse Mixture of Experts (SMoE)\ntypically rely on input-dependent or input-independent averaging of expert\nparameters, but often lack a principled weighting mechanism. In this work, we\nreinterpret expert merging through the lens of game theory, revealing\ncooperative and competitive dynamics among experts. Based on this perspective,\nwe introduce Nash Merging of Experts (NAMEx), a novel framework that\nincorporates Nash Bargaining into the merging process, enabling more balanced\nand efficient collaboration among experts. Additionally, we incorporate complex\nmomentum into NAMEx to accelerate expert propagation with theoretical\nguarantees for convergence. Extensive experiments across language modelling,\ntext classification, image classification, and zero-shot robustness under data\ncorruption show that NAMEx consistently outperforms competing methods while\nintegrating seamlessly with popular MoE architectures. Finally, we demonstrate\nNAMEx's scalability by applying it to large-scale systems, including\nQwen1.5-MoE (14B) and DeepSeek-MoE (16B), where it proves effective in both\nzero-shot and fine-tuning settings."}
{"id": "2510.17642", "categories": ["quant-ph", "cs.DC", "cs.LG", "I.2; A.1"], "pdf": "https://arxiv.org/pdf/2510.17642", "abs": "https://arxiv.org/abs/2510.17642", "authors": ["Siva Sai", "Abhishek Sawaika", "Prabhjot Singh", "Rajkumar Buyya"], "title": "Quantum Federated Learning: Architectural Elements and Future Directions", "comment": "28 PAGES, 11 figures, introductory review article (book chapter), to\n  be published in a book with springer", "summary": "Federated learning (FL) focuses on collaborative model training without the\nneed to move the private data silos to a central server. Despite its several\nbenefits, the classical FL is plagued with several limitations, such as high\ncomputational power required for model training(which is critical for\nlow-resource clients), privacy risks, large update traffic, and non-IID\nheterogeneity. This chapter surveys a hybrid paradigm - Quantum Federated\nLearning (QFL), which introduces quantum computation, that addresses multiple\nchallenges of classical FL and offers rapid computing capability while keeping\nthe classical orchestration intact. Firstly, we motivate QFL with a concrete\npresentation on pain points of classical FL, followed by a discussion on a\ngeneral architecture of QFL frameworks specifying the roles of client and\nserver, communication primitives and the quantum model placement. We classify\nthe existing QFL systems based on four criteria - quantum architecture (pure\nQFL, hybrid QFL), data processing method (quantum data encoding, quantum\nfeature mapping, and quantum feature selection & dimensionality reduction),\nnetwork topology (centralized, hierarchial, decentralized), and quantum\nsecurity mechanisms (quantum key distribution, quantum homomorphic encryption,\nquantum differential privacy, blind quantum computing). We then describe\napplications of QFL in healthcare, vehicular networks, wireless networks, and\nnetwork security, clearly highlighting where QFL improves communication\nefficiency, security, and performance compared to classical FL. We close with\nmultiple challenges and future works in QFL, including extension of QFL beyond\nclassification tasks, adversarial attacks, realistic hardware deployment,\nquantum communication protocols deployment, aggregation of different quantum\nmodels, and quantum split learning as an alternative to QFL."}
{"id": "2510.16157", "categories": ["cs.LG", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16157", "abs": "https://arxiv.org/abs/2510.16157", "authors": ["Xuchen Gong", "Tian Li"], "title": "Zeroth-Order Sharpness-Aware Learning with Exponential Tilting", "comment": null, "summary": "Classic zeroth-order optimization approaches typically optimize for a\nsmoothed version of the original function, i.e., the expected objective under\nrandomly perturbed model parameters. This can be interpreted as encouraging the\nloss values in the perturbation set to be small on average. Popular\nsharpness-aware minimization (SAM) objectives, however, typically focus on the\nlargest loss within the neighborhood to arrive at flat minima more effectively.\nIn this work, we connect zeroth-order optimization (and its corresponding\nobjectives) with SAM approaches explicitly, through an exponential tilting\nobjective that provides a smooth transition between the average- and the\nmax-loss formulations. We explore new zeroth-order algorithms to solve a soft\nSAM objective parameterized by a tilting parameter $t$. We provide precise\ncharacterizations of the sharpness notions of the tilted SAM framework.\nPractically, our approach can be used as a gradient-free and memory-efficient\nalternative to SAM variants, and it achieves better generalization compared to\nvanilla zeroth-order baselines on a wide range of downstream tasks, including\nclassification, multiple choice QA, and language generation."}
{"id": "2510.17659", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.17659", "abs": "https://arxiv.org/abs/2510.17659", "authors": ["Rui Guan", "Jingchun Yu", "Zhaoyun Li", "Hongbo Xie", "Yuxing Wei", "Sen Li", "Jing Wen", "Xiaodong Liang", "Yanwei Li", "Kejin Wei"], "title": "Field-Trial Quantum Key Distribution with Qubit-Based Frame Synchronization", "comment": null, "summary": "Quantum key distribution (QKD) is a cryptographic technique that uses quantum\nmechanical principles to enable secure key exchange.Practical deployment of QKD\nrequires robust,cost-effective systems that can operate in challenging field\nenvironments.A major challenge is achieving reliable clock synchronization\nwithout adding hardware complexity.Conventional approaches often use separate\nclassical light signals,which increase costs and introduce noise that degrades\nquantum channel performance.To address this limitation,we demonstrate a QKD\nsystem incorporating a recently proposed qubit-based distributed frame\nsynchronization method,deployed over a metropolitan fiber network in\nNanning,China.Using the polarization-encoded one-decoy-state BB84 protocol and\nthe recently proposed qubit-based distributed frame synchronization method, our\nsystem achieves synchronization directly from the quantum signal, eliminating\nthe need for dedicated synchronization hardware. Furthermore,to counteract\ndynamic polarization disturbances in urban fibers,the system integrates\nqubit-based polarization feedback control,enabling real-time polarization\ncompensation through an automated polarization controller using data recovered\nfrom the qubit-based synchronization signals. During 12 hours of continuous\noperation, the system maintained a low average quantum bit error rate (QBER) of\n\\SI{1.12 \\pm 0.48}{\\percent}, achieving a secure key rate of 26.6 kbit/s under\n18 dB channel loss. Even under a high channel loss of 40 dB,a finite-key secure\nrate of 115bit/s was achieved.This study represents the first successful\nlong-term validation of a frame synchronization based QKD scheme in a real\nurban environment,demonstrating exceptional stability and high loss\ntolerance,and offering an alternative for building practical,scalable,and\ncost-efficient quantum-secure communication networks."}
{"id": "2510.16161", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16161", "abs": "https://arxiv.org/abs/2510.16161", "authors": ["Ankitkumar Joshi", "Milos Hauskrecht"], "title": "Still Competitive: Revisiting Recurrent Models for Irregular Time Series Prediction", "comment": null, "summary": "Modeling irregularly sampled multivariate time series is a persistent\nchallenge in domains like healthcare and sensor networks. While recent works\nhave explored a variety of complex learning architectures to solve the\nprediction problems for irregularly sampled time series, it remains unclear\nwhat are the true benefits of some of these architectures, and whether clever\nmodifications of simpler and more efficient RNN-based algorithms are still\ncompetitive, i.e. they are on par with or even superior to these methods. In\nthis work, we propose and study GRUwE: Gated Recurrent Unit with Exponential\nbasis functions, that builds upon RNN-based architectures for observations made\nat irregular times. GRUwE supports both regression-based and event-based\npredictions in continuous time. GRUwE works by maintaining a Markov state\nrepresentation of the time series that updates with the arrival of irregular\nobservations. The Markov state update relies on two reset mechanisms: (i)\nobservation-triggered reset, and (ii) time-triggered reset of the GRU state\nusing learnable exponential decays, to support the predictions in continuous\ntime. Our empirical evaluations across several real-world benchmarks on\nnext-observation and next-event prediction tasks demonstrate that GRUwE can\nindeed achieve competitive to superior performance compared to the recent\nstate-of-the-art (SOTA) methods. Thanks to its simplicity, GRUwE offers\ncompelling advantages: it is easy to implement, requires minimal\nhyper-parameter tuning efforts, and significantly reduces the computational\noverhead in the online deployment."}
{"id": "2510.17689", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.17689", "abs": "https://arxiv.org/abs/2510.17689", "authors": ["Pau Escofet", "Eduard Alarcón", "Sergi Abadal", "Andrii Semenov", "Niall Murphy", "Elena Blokhina", "Carmen G. Almudéver"], "title": "Quantum Reverse Mapping: Synthesizing an Optimal Spin Qubit Shuttling Bus Architecture for the Surface Code", "comment": null, "summary": "As quantum computers scale toward millions of physical qubits, it becomes\nessential to robustly encode individual logical qubits to ensure fault\ntolerance under realistic noise. A high-quality foundational encoding allows\nfuture compilation techniques and heuristics to build on optimal or\nnear-optimal layouts, improving scalability and error resilience. In this work,\nwe synthesize a one-dimensional shuttling bus architecture for the rotated\nsurface code, leveraging coherent spin-qubit shuttling. We formulate a\nmixed-integer optimization model that yields optimal solutions with relatively\nlow execution time for small code distances, and propose a scalable heuristic\nthat matches optimal results while maintaining linear computational complexity.\nWe evaluate the synthesized architecture using architectural metrics, such as\nshuttling distance and cycle time, and full quantum simulations under realistic\nnoise models, showing that the proposed design can sustain logical error rates\nas low as $2\\cdot 10^{-10}$ per round at code distance 21, showcasing its\nfeasibility for scalable quantum error correction in spin-based quantum\nprocessors."}
{"id": "2510.16165", "categories": ["cs.LG", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2510.16165", "abs": "https://arxiv.org/abs/2510.16165", "authors": ["Charles Rhys Campbell", "Aldo H. Romero", "Kamal Choudhary"], "title": "AtomBench: A Benchmark for Generative Atomic Structure Models using GPT, Diffusion, and Flow Architectures", "comment": null, "summary": "Generative models have become significant assets in the exploration and\nidentification of new materials, enabling the rapid proposal of candidate\ncrystal structures that satisfy target properties. Despite the increasing\nadoption of diverse architectures, a rigorous comparative evaluation of their\nperformance on materials datasets is lacking. In this work, we present a\nsystematic benchmark of three representative generative models- AtomGPT (a\ntransformer-based model), Crystal Diffusion Variational Autoencoder (CDVAE),\nand FlowMM (a Riemannian flow matching model). These models were trained to\nreconstruct crystal structures from subsets of two publicly available\nsuperconductivity datasets- JARVIS Supercon 3D and DS A/B from the Alexandria\ndatabase. Performance was assessed using the Kullback-Leibler (KL) divergence\nbetween predicted and reference distributions of lattice parameters, as well as\nthe mean absolute error (MAE) of individual lattice constants. For the computed\nKLD and MAE scores, CDVAE performs most favorably, followed by AtomGPT, and\nthen FlowMM. All benchmarking code and model configurations will be made\npublicly available at https://github.com/atomgptlab/atombench_inverse."}
{"id": "2510.17692", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.17692", "abs": "https://arxiv.org/abs/2510.17692", "authors": ["Nayden P. Nedev", "Nikolay V. Vitanov"], "title": "Topological Dynamical Decoupling with Complete Pulse Error Cancellation", "comment": null, "summary": "Systematic pulse errors remain a major obstacle to high-fidelity quantum\ncontrol. We present a new family of dynamical decoupling sequences, denoted Tn,\nthat achieve exact cancellation of pulse area errors to all orders by enforcing\na simple topological phase condition. Unlike some conventional composite\nsequences, Tn requires no numerical optimization and admits closed-form\nanalytic phases for arbitrary sequence length, while providing substantial\nrobustness to detuning as well. We demonstrate these sequences on\nsuperconducting transmon qubits from both IBM Quantum processor ibm_torino and\nIQM Quantum processor Garnet, observing population plateaus in close agreement\nwith theory. These results establish a new paradigm for hardware-efficient\nerror suppression, broadly applicable across quantum computing, sensing, and\nmemory platforms."}
{"id": "2510.16167", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16167", "abs": "https://arxiv.org/abs/2510.16167", "authors": ["Archie Chaudhury"], "title": "Alignment is Localized: A Causal Probe into Preference Layers", "comment": null, "summary": "Reinforcement Learning frameworks, particularly those utilizing human\nannotations, have become an increasingly popular method for preference\nfine-tuning, where the outputs of a language model are tuned to match a certain\nset of behavioral policies or guidelines. Reinforcement Learning through Human\nFeedback (RLHF) is perhaps the most popular implementation of such a framework,\nparticularly for aligning LMs toward safety and human intent. However, the\ninternal workings of how such alignment is achieved remain largely opaque. In\nthis work, we systematically analyze preference optimization for language model\nalignment by applying layer-wide causal patching between a base model and its\ntuned counterpart across human preference pairs. We implement our methodology\non \\textit{Llama-3.2-1B}, and find that alignment is spatially localized:\nmid-layer activations encode a distinct subspace that causally determines\nreward-consistent behavior, while early and late layers remain largely\nunaffected. Utilizing LASSO regression, we also find that only a small number\nof layers possess non-zero coefficients linking activation distances to reward\ngains. Overall, we show that, at least for some language models, alignment from\nhuman-based, preferential tuning is a directional, low rank process, rather\nthan diffuse and parameteric."}
{"id": "2510.16171", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16171", "abs": "https://arxiv.org/abs/2510.16171", "authors": ["Longwei Wang", "Ifrat Ikhtear Uddin", "KC Santosh", "Chaowei Zhang", "Xiao Qin", "Yang Zhou"], "title": "Bridging Symmetry and Robustness: On the Role of Equivariance in Enhancing Adversarial Robustness", "comment": "Accepted for the proceedings of 39th Conference on Neural Information\n  Processing Systems (NeurIPS 2025)", "summary": "Adversarial examples reveal critical vulnerabilities in deep neural networks\nby exploiting their sensitivity to imperceptible input perturbations. While\nadversarial training remains the predominant defense strategy, it often incurs\nsignificant computational cost and may compromise clean-data accuracy. In this\nwork, we investigate an architectural approach to adversarial robustness by\nembedding group-equivariant convolutions-specifically, rotation- and\nscale-equivariant layers-into standard convolutional neural networks (CNNs).\nThese layers encode symmetry priors that align model behavior with structured\ntransformations in the input space, promoting smoother decision boundaries and\ngreater resilience to adversarial attacks. We propose and evaluate two\nsymmetry-aware architectures: a parallel design that processes standard and\nequivariant features independently before fusion, and a cascaded design that\napplies equivariant operations sequentially. Theoretically, we demonstrate that\nsuch models reduce hypothesis space complexity, regularize gradients, and yield\ntighter certified robustness bounds under the CLEVER (Cross Lipschitz Extreme\nValue for nEtwork Robustness) framework. Empirically, our models consistently\nimprove adversarial robustness and generalization across CIFAR-10, CIFAR-100,\nand CIFAR-10C under both FGSM and PGD attacks, without requiring adversarial\ntraining. These findings underscore the potential of symmetry-enforcing\narchitectures as efficient and principled alternatives to data\naugmentation-based defenses."}
{"id": "2510.16175", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16175", "abs": "https://arxiv.org/abs/2510.16175", "authors": ["Pablo Samuel Castro"], "title": "The Formalism-Implementation Gap in Reinforcement Learning Research", "comment": null, "summary": "The last decade has seen an upswing in interest and adoption of reinforcement\nlearning (RL) techniques, in large part due to its demonstrated capabilities at\nperforming certain tasks at \"super-human levels\". This has incentivized the\ncommunity to prioritize research that demonstrates RL agent performance, often\nat the expense of research aimed at understanding their learning dynamics.\nPerformance-focused research runs the risk of overfitting on academic\nbenchmarks -- thereby rendering them less useful -- which can make it difficult\nto transfer proposed techniques to novel problems. Further, it implicitly\ndiminishes work that does not push the performance-frontier, but aims at\nimproving our understanding of these techniques. This paper argues two points:\n(i) RL research should stop focusing solely on demonstrating agent\ncapabilities, and focus more on advancing the science and understanding of\nreinforcement learning; and (ii) we need to be more precise on how our\nbenchmarks map to the underlying mathematical formalisms. We use the popular\nArcade Learning Environment (ALE; Bellemare et al., 2013) as an example of a\nbenchmark that, despite being increasingly considered \"saturated\", can be\neffectively used for developing this understanding, and facilitating the\ndeployment of RL techniques in impactful real-world problems."}
{"id": "2510.16185", "categories": ["cs.LG", "cs.AI", "cs.FL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16185", "abs": "https://arxiv.org/abs/2510.16185", "authors": ["Daniel Donnelly", "Angelo Ferrando", "Francesco Belardinelli"], "title": "Expressive Reward Synthesis with the Runtime Monitoring Language", "comment": null, "summary": "A key challenge in reinforcement learning (RL) is reward (mis)specification,\nwhereby imprecisely defined reward functions can result in unintended, possibly\nharmful, behaviours. Indeed, reward functions in RL are typically treated as\nblack-box mappings from state-action pairs to scalar values. While effective in\nmany settings, this approach provides no information about why rewards are\ngiven, which can hinder learning and interpretability. Reward Machines address\nthis issue by representing reward functions as finite state automata, enabling\nthe specification of structured, non-Markovian reward functions. However, their\nexpressivity is typically bounded by regular languages, leaving them unable to\ncapture more complex behaviours such as counting or parametrised conditions. In\nthis work, we build on the Runtime Monitoring Language (RML) to develop a novel\nclass of language-based Reward Machines. By leveraging the built-in memory of\nRML, our approach can specify reward functions for non-regular, non-Markovian\ntasks. We demonstrate the expressiveness of our approach through experiments,\nhighlighting additional advantages in flexible event-handling and task\nspecification over existing Reward Machine-based methods."}
{"id": "2510.16188", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16188", "abs": "https://arxiv.org/abs/2510.16188", "authors": ["Fateme Golivand Darvishvand", "Hikaru Shindo", "Sahil Sidheekh", "Kristian Kersting", "Sriraam Natarajan"], "title": "Human-Allied Relational Reinforcement Learning", "comment": "Proceedings of the Twelfth Annual Conference on Advances in Cognitive\n  Systems, ACS-2025 (143-159)", "summary": "Reinforcement learning (RL) has experienced a second wind in the past decade.\nWhile incredibly successful in images and videos, these systems still operate\nwithin the realm of propositional tasks ignoring the inherent structure that\nexists in the problem. Consequently, relational extensions (RRL) have been\ndeveloped for such structured problems that allow for effective generalization\nto arbitrary number of objects. However, they inherently make strong\nassumptions about the problem structure. We introduce a novel framework that\ncombines RRL with object-centric representation to handle both structured and\nunstructured data. We enhance learning by allowing the system to actively query\nthe human expert for guidance by explicitly modeling the uncertainty over the\npolicy. Our empirical evaluation demonstrates the effectiveness and efficiency\nof our proposed approach."}
{"id": "2510.16208", "categories": ["cs.LG", "cs.SY", "eess.SY", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16208", "abs": "https://arxiv.org/abs/2510.16208", "authors": ["Sunmook Choi", "Yahya Sattar", "Yassir Jedra", "Maryam Fazel", "Sarah Dean"], "title": "Explore-then-Commit for Nonstationary Linear Bandits with Latent Dynamics", "comment": null, "summary": "We study a nonstationary bandit problem where rewards depend on both actions\nand latent states, the latter governed by unknown linear dynamics. Crucially,\nthe state dynamics also depend on the actions, resulting in tension between\nshort-term and long-term rewards. We propose an explore-then-commit algorithm\nfor a finite horizon $T$. During the exploration phase, random Rademacher\nactions enable estimation of the Markov parameters of the linear dynamics,\nwhich characterize the action-reward relationship. In the commit phase, the\nalgorithm uses the estimated parameters to design an optimized action sequence\nfor long-term reward. Our proposed algorithm achieves\n$\\tilde{\\mathcal{O}}(T^{2/3})$ regret. Our analysis handles two key challenges:\nlearning from temporally correlated rewards, and designing action sequences\nwith optimal long-term reward. We address the first challenge by providing\nnear-optimal sample complexity and error bounds for system identification using\nbilinear rewards. We address the second challenge by proving an equivalence\nwith indefinite quadratic optimization over a hypercube, a known NP-hard\nproblem. We provide a sub-optimality guarantee for this problem, enabling our\nregret upper bound. Lastly, we propose a semidefinite relaxation with\nGoemans-Williamson rounding as a practical approach."}
{"id": "2510.16211", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16211", "abs": "https://arxiv.org/abs/2510.16211", "authors": ["Henrique Pickler", "Jorge K. S. Kamassury", "Danilo Silva"], "title": "Benchmarking noisy label detection methods", "comment": null, "summary": "Label noise is a common problem in real-world datasets, affecting both model\ntraining and validation. Clean data are essential for achieving strong\nperformance and ensuring reliable evaluation. While various techniques have\nbeen proposed to detect noisy labels, there is no clear consensus on optimal\napproaches. We perform a comprehensive benchmark of detection methods by\ndecomposing them into three fundamental components: label agreement function,\naggregation method, and information gathering approach (in-sample vs\nout-of-sample). This decomposition can be applied to many existing detection\nmethods, and enables systematic comparison across diverse approaches. To fairly\ncompare methods, we propose a unified benchmark task, detecting a fraction of\ntraining samples equal to the dataset's noise rate. We also introduce a novel\nmetric: the false negative rate at this fixed operating point. Our evaluation\nspans vision and tabular datasets under both synthetic and real-world noise\nconditions. We identify that in-sample information gathering using average\nprobability aggregation combined with the logit margin as the label agreement\nfunction achieves the best results across most scenarios. Our findings provide\npractical guidance for designing new detection methods and selecting techniques\nfor specific applications."}
{"id": "2510.16233", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16233", "abs": "https://arxiv.org/abs/2510.16233", "authors": ["Patricia West", "Michelle WL Wan", "Alexander Hepburn", "Edwin Simpson", "Raul Santos-Rodriguez", "Jeffrey N Clark"], "title": "Machine Learning for Climate Policy: Understanding Policy Progression in the European Green Deal", "comment": null, "summary": "Climate change demands effective legislative action to mitigate its impacts.\nThis study explores the application of machine learning (ML) to understand the\nprogression of climate policy from announcement to adoption, focusing on\npolicies within the European Green Deal. We present a dataset of 165 policies,\nincorporating text and metadata. We aim to predict a policy's progression\nstatus, and compare text representation methods, including TF-IDF, BERT, and\nClimateBERT. Metadata features are included to evaluate the impact on\npredictive performance. On text features alone, ClimateBERT outperforms other\napproaches (RMSE = 0.17, R^2 = 0.29), while BERT achieves superior performance\nwith the addition of metadata features (RMSE = 0.16, R^2 = 0.38). Using methods\nfrom explainable AI highlights the influence of factors such as policy wording\nand metadata including political party and country representation. These\nfindings underscore the potential of ML tools in supporting climate policy\nanalysis and decision-making."}
{"id": "2510.16250", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16250", "abs": "https://arxiv.org/abs/2510.16250", "authors": ["Danil Akhtiamov", "Reza Ghane", "Babak Hassibi"], "title": "One-Bit Quantization for Random Features Models", "comment": null, "summary": "Recent advances in neural networks have led to significant computational and\nmemory demands, spurring interest in one-bit weight compression to enable\nefficient inference on resource-constrained devices. However, the theoretical\nunderpinnings of such compression remain poorly understood. We address this gap\nby analyzing one-bit quantization in the Random Features model, a simplified\nframework that corresponds to neural networks with random representations. We\nprove that, asymptotically, quantizing weights of all layers except the last\nincurs no loss in generalization error, compared to the full precision random\nfeatures model. Our findings offer theoretical insights into neural network\ncompression. We also demonstrate empirically that one-bit quantization leads to\nsignificant inference speed ups for the Random Features models even on a laptop\nGPU, confirming the practical benefits of our work. Additionally, we provide an\nasymptotically precise characterization of the generalization error for Random\nFeatures with an arbitrary number of layers. To the best of our knowledge, our\nanalysis yields more general results than all previous works in the related\nliterature."}
{"id": "2510.16252", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16252", "abs": "https://arxiv.org/abs/2510.16252", "authors": ["Yuxuan Lu", "Jing Huang", "Hui Liu", "Jiri Gesi", "Yan Han", "Shihan Fu", "Tianqi Zheng", "Dakuo Wang"], "title": "WEBSERV: A Browser-Server Environment for Efficient Training of Reinforcement Learning-based Web Agents at Scale", "comment": null, "summary": "Training and evaluation of Reinforcement Learning (RL) web agents have gained\nincreasing attention, yet a scalable and efficient environment that couples\nrealistic and robust browser-side interaction with controllable server-side\nstate at scale is still missing. Existing environments tend to have one or more\nof the following issues: they overwhelm policy models with excessive and noisy\ncontext; they perform actions non-deterministically without waiting for the UI\nor network to stabilize; or they cannot scale isolated client-server containers\neffectively for parallel RL rollouts. We propose WEBSERV, an environment that\nincludes 1) a compact, site-agnostic browser environment that balances context\nand action complexity, and 2) a scalable RL environment via efficient launching\nand resetting web-servers to enable scalable RL training and evaluation. We\nevaluate WEBSERV on the shopping CMS and Gitlab tasks in WebArena, achieving\nstate-of-the-art single-prompt success rates while cutting launch latency by\n~5x and storage need by ~240x, with a comparable memory footprint, enabling\n200+ concurrent containers on a single host."}
{"id": "2510.16253", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.QM", "stat.ML", "I.2.1; J.3"], "pdf": "https://arxiv.org/pdf/2510.16253", "abs": "https://arxiv.org/abs/2510.16253", "authors": ["Arielle Sanford", "Shuo Sun", "Christian B. Mendl"], "title": "Protein Folding with Neural Ordinary Differential Equations", "comment": null, "summary": "Recent advances in protein structure prediction, such as AlphaFold, have\ndemonstrated the power of deep neural architectures like the Evoformer for\ncapturing complex spatial and evolutionary constraints on protein conformation.\nHowever, the depth of the Evoformer, comprising 48 stacked blocks, introduces\nhigh computational costs and rigid layerwise discretization. Inspired by Neural\nOrdinary Differential Equations (Neural ODEs), we propose a continuous-depth\nformulation of the Evoformer, replacing its 48 discrete blocks with a Neural\nODE parameterization that preserves its core attention-based operations. This\ncontinuous-time Evoformer achieves constant memory cost (in depth) via the\nadjoint method, while allowing a principled trade-off between runtime and\naccuracy through adaptive ODE solvers. Benchmarking on protein structure\nprediction tasks, we find that the Neural ODE-based Evoformer produces\nstructurally plausible predictions and reliably captures certain secondary\nstructure elements, such as alpha-helices, though it does not fully replicate\nthe accuracy of the original architecture. However, our model achieves this\nperformance using dramatically fewer resources, just 17.5 hours of training on\na single GPU, highlighting the promise of continuous-depth models as a\nlightweight and interpretable alternative for biomolecular modeling. This work\nopens new directions for efficient and adaptive protein structure prediction\nframeworks."}
{"id": "2510.16289", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16289", "abs": "https://arxiv.org/abs/2510.16289", "authors": ["Yoonho Lee", "Junseok Lee", "Sangwoo Seo", "Sungwon Kim", "Yeongmin Kim", "Chanyoung Park"], "title": "Disentangling Hyperedges through the Lens of Category Theory", "comment": "Accepted to NeurIPS 2025", "summary": "Despite the promising results of disentangled representation learning in\ndiscovering latent patterns in graph-structured data, few studies have explored\ndisentanglement for hypergraph-structured data. Integrating hyperedge\ndisentanglement into hypergraph neural networks enables models to leverage\nhidden hyperedge semantics, such as unannotated relations between nodes, that\nare associated with labels. This paper presents an analysis of hyperedge\ndisentanglement from a category-theoretical perspective and proposes a novel\ncriterion for disentanglement derived from the naturality condition. Our\nproof-of-concept model experimentally showed the potential of the proposed\ncriterion by successfully capturing functional relations of genes (nodes) in\ngenetic pathways (hyperedges)."}
{"id": "2510.16292", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16292", "abs": "https://arxiv.org/abs/2510.16292", "authors": ["Yutong Wang", "Haiyu Wang", "Sai Qian Zhang"], "title": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models", "comment": "Accepted as Spotlight paper by NeurIPS 2025", "summary": "Vision-Language Models (VLMs) are integral to tasks such as image captioning\nand visual question answering, but their high computational cost, driven by\nlarge memory footprints and processing time, limits their scalability and\nreal-time applicability. In this work, we propose leveraging Singular-Value\nDecomposition (SVD) over the joint query (Q), key (K), and value (V) weight\nmatrices to reduce KV cache size and computational overhead. We in addition\nintroduce an efficient rank allocation strategy that dynamically adjusts the\nSVD rank based on its impact on VLM accuracy, achieving a significant reduction\nin both memory usage and computational cost. Finally, we extend this approach\nby applying quantization to both VLM weights and activations, resulting in a\nhighly efficient VLM. Our method outperforms previous approaches that rely\nsolely on quantization or SVD by achieving more than $10\\%$ accuracy\nimprovement while consuming less hardware cost, making it better for real-time\ndeployment on resource-constrained devices. We open source our code at\n\\href{https://github.com/SAI-Lab-NYU/QSVD}{\\texttt{https://github.com/SAI-Lab-NYU/QSVD}}."}
{"id": "2510.16306", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16306", "abs": "https://arxiv.org/abs/2510.16306", "authors": ["Xin Wang", "Yu Wang", "Yunchao Liu", "Jens Meiler", "Tyler Derr"], "title": "Scaffold-Aware Generative Augmentation and Reranking for Enhanced Virtual Screening", "comment": null, "summary": "Ligand-based virtual screening (VS) is an essential step in drug discovery\nthat evaluates large chemical libraries to identify compounds that potentially\nbind to a therapeutic target. However, VS faces three major challenges: class\nimbalance due to the low active rate, structural imbalance among active\nmolecules where certain scaffolds dominate, and the need to identify\nstructurally diverse active compounds for novel drug development. We introduce\nScaffAug, a scaffold-aware VS framework that addresses these challenges through\nthree modules. The augmentation module first generates synthetic data\nconditioned on scaffolds of actual hits using generative AI, specifically a\ngraph diffusion model. This helps mitigate the class imbalance and furthermore\nthe structural imbalance, due to our proposed scaffold-aware sampling\nalgorithm, designed to produce more samples for active molecules with\nunderrepresented scaffolds. A model-agnostic self-training module is then used\nto safely integrate the generated synthetic data from our augmentation module\nwith the original labeled data. Lastly, we introduce a reranking module that\nimproves VS by enhancing scaffold diversity in the top recommended set of\nmolecules, while still maintaining and even enhancing the overall general\nperformance of identifying novel, active compounds. We conduct comprehensive\ncomputational experiments across five target classes, comparing ScaffAug\nagainst existing baseline methods by reporting the performance of multiple\nevaluation metrics and performing ablation studies on ScaffAug. Overall, this\nwork introduces novel perspectives on effectively enhancing VS by leveraging\ngenerative augmentations, reranking, and general scaffold-awareness."}
{"id": "2510.16311", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16311", "abs": "https://arxiv.org/abs/2510.16311", "authors": ["Daohan Su", "Yang Zhang", "Xunkai Li", "Rong-Hua Li", "Guoren Wang"], "title": "Toward General Digraph Contrastive Learning: A Dual Spatial Perspective", "comment": null, "summary": "Graph Contrastive Learning (GCL) has emerged as a powerful tool for\nextracting consistent representations from graphs, independent of labeled\ninformation. However, existing methods predominantly focus on undirected\ngraphs, disregarding the pivotal directional information that is fundamental\nand indispensable in real-world networks (e.g., social networks and\nrecommendations).In this paper, we introduce S2-DiGCL, a novel framework that\nemphasizes spatial insights from complex and real domain perspectives for\ndirected graph (digraph) contrastive learning. From the complex-domain\nperspective, S2-DiGCL introduces personalized perturbations into the magnetic\nLaplacian to adaptively modulate edge phases and directional semantics. From\nthe real-domain perspective, it employs a path-based subgraph augmentation\nstrategy to capture fine-grained local asymmetries and topological\ndependencies. By jointly leveraging these two complementary spatial views,\nS2-DiGCL constructs high-quality positive and negative samples, leading to more\ngeneral and robust digraph contrastive learning. Extensive experiments on 7\nreal-world digraph datasets demonstrate the superiority of our approach,\nachieving SOTA performance with 4.41% improvement in node classification and\n4.34% in link prediction under both supervised and unsupervised settings."}
{"id": "2510.16322", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16322", "abs": "https://arxiv.org/abs/2510.16322", "authors": ["Mo Zhou", "Haoyang Ma", "Rong Ge"], "title": "Memorizing Long-tail Data Can Help Generalization Through Composition", "comment": "30 pages", "summary": "Deep learning has led researchers to rethink the relationship between\nmemorization and generalization. In many settings, memorization does not hurt\ngeneralization due to implicit regularization and may help by memorizing\nlong-tailed examples. In this paper, we consider the synergy between\nmemorization and simple composition -- the ability to make correct prediction\non a combination of long-tailed features. Theoretically, we show that for a\nlinear setting, memorization together with composition can help the model make\ncorrect predictions on rare test examples that require a combination of\nlong-tailed features, even if such combinations were never observed in the\ntraining data. Experiments on neural network architecture on simple data show\nthat the theoretical insight extends beyond the linear setting, and we further\nobserve that the composition capability of the model depends on its\narchitecture."}
{"id": "2510.16350", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16350", "abs": "https://arxiv.org/abs/2510.16350", "authors": ["Shule Hao", "Junpeng Bao", "Wenli Li"], "title": "MGTS-Net: Exploring Graph-Enhanced Multimodal Fusion for Augmented Time Series Forecasting", "comment": null, "summary": "Recent research in time series forecasting has explored integrating\nmultimodal features into models to improve accuracy. However, the accuracy of\nsuch methods is constrained by three key challenges: inadequate extraction of\nfine-grained temporal patterns, suboptimal integration of multimodal\ninformation, and limited adaptability to dynamic multi-scale features. To\naddress these problems, we propose MGTS-Net, a Multimodal Graph-enhanced\nNetwork for Time Series forecasting. The model consists of three core\ncomponents: (1) a Multimodal Feature Extraction layer (MFE), which optimizes\nfeature encoders according to the characteristics of temporal, visual, and\ntextual modalities to extract temporal features of fine-grained patterns; (2) a\nMultimodal Feature Fusion layer (MFF), which constructs a heterogeneous graph\nto model intra-modal temporal dependencies and cross-modal alignment\nrelationships and dynamically aggregates multimodal knowledge; (3) a\nMulti-Scale Prediction layer (MSP), which adapts to multi-scale features by\ndynamically weighting and fusing the outputs of short-term, medium-term, and\nlong-term predictors. Extensive experiments demonstrate that MGTS-Net exhibits\nexcellent performance with light weight and high efficiency. Compared with\nother state-of-the-art baseline models, our method achieves superior\nperformance, validating the superiority of the proposed methodology."}
{"id": "2510.16356", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16356", "abs": "https://arxiv.org/abs/2510.16356", "authors": ["Fuqun Han", "Stanley Osher", "Wuchen Li"], "title": "Sparse Transformer Architectures via Regularized Wasserstein Proximal Operator with $L_1$ Prior", "comment": null, "summary": "In this work, we propose a sparse transformer architecture that incorporates\nprior information about the underlying data distribution directly into the\ntransformer structure of the neural network. The design of the model is\nmotivated by a special optimal transport problem, namely the regularized\nWasserstein proximal operator, which admits a closed-form solution and turns\nout to be a special representation of transformer architectures. Compared with\nclassical flow-based models, the proposed approach improves the convexity\nproperties of the optimization problem and promotes sparsity in the generated\nsamples. Through both theoretical analysis and numerical experiments, including\napplications in generative modeling and Bayesian inverse problems, we\ndemonstrate that the sparse transformer achieves higher accuracy and faster\nconvergence to the target distribution than classical neural ODE-based methods."}
{"id": "2510.16411", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16411", "abs": "https://arxiv.org/abs/2510.16411", "authors": ["Minh-Khoi Nguyen-Nhat", "Rachel S. Y. Teo", "Laziz Abdullaev", "Maurice Mok", "Viet-Hoang Tran", "Tan Minh Nguyen"], "title": "Modeling Expert Interactions in Sparse Mixture of Experts via Graph Structures", "comment": null, "summary": "Sparse Mixture of Experts (SMoE) has emerged as a promising solution to\nachieving unparalleled scalability in deep learning by decoupling model\nparameter count from computational cost. By activating only a small subset of\nparameters per sample, SMoE enables significant growth in model capacity while\nmaintaining efficiency. However, SMoE struggles to adapt to distributional\nshifts, leading to reduced robustness under data contamination. In this work,\nwe introduce SymphonySMoE, a novel family of SMoE that introduces a social\ngraph to model interactions among experts. This graph-based structure enhances\nthe token routing process, addressing the robustness challenges that are\ninherent in conventional SMoE designs. SymphonySMoE is lightweight, modular,\nand integrates seamlessly with existing SMoE-based models such as the XMoE and\nthe Generalist Language Model. We provide both theoretical analysis and\nempirical evidence demonstrating SymphonySMoE's advantages over baseline SMoE.\nExtensive experiments on language modeling and visual instruction tuning\nvalidate our method's effectiveness. We further highlight the scalability of\nSymphonySMoE to models with 4.2 and 7.4 billion parameters, showcasing its\napplicability in fine-tuning tasks for large-scale systems."}
{"id": "2510.16440", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16440", "abs": "https://arxiv.org/abs/2510.16440", "authors": ["Dimitris Stefanopoulos", "Andreas Voskou"], "title": "Colliding with Adversaries at ECML-PKDD 2025 Adversarial Attack Competition 1st Prize Solution", "comment": null, "summary": "This report presents the winning solution for Task 1 of Colliding with\nAdversaries: A Challenge on Robust Learning in High Energy Physics Discovery at\nECML-PKDD 2025. The task required designing an adversarial attack against a\nprovided classification model that maximizes misclassification while minimizing\nperturbations. Our approach employs a multi-round gradient-based strategy that\nleverages the differentiable structure of the model, augmented with random\ninitialization and sample-mixing techniques to enhance effectiveness. The\nresulting attack achieved the best results in perturbation size and fooling\nsuccess rate, securing first place in the competition."}
{"id": "2510.16443", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16443", "abs": "https://arxiv.org/abs/2510.16443", "authors": ["Dimitris Stefanopoulos", "Andreas Voskou"], "title": "Colliding with Adversaries at ECML-PKDD 2025 Model Robustness Competition 1st Prize Solution", "comment": null, "summary": "This report presents the winning solution for Task 2 of Colliding with\nAdversaries: A Challenge on Robust Learning in High Energy Physics Discovery at\nECML-PKDD 2025. The goal of the challenge was to design and train a robust\nANN-based model capable of achieving high accuracy in a binary classification\ntask on both clean and adversarial data generated with the Random Distribution\nShuffle Attack (RDSA). Our solution consists of two components: a data\ngeneration phase and a robust model training phase. In the first phase, we\nproduced 15 million artificial training samples using a custom methodology\nderived from Random Distribution Shuffle Attack (RDSA). In the second phase, we\nintroduced a robust architecture comprising (i)a Feature Embedding Block with\nshared weights among features of the same type and (ii)a Dense Fusion Tail\nresponsible for the final prediction. Training this architecture on our\nadversarial dataset achieved a mixed accuracy score of 80\\%, exceeding the\nsecond-place solution by two percentage points."}
{"id": "2510.16448", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16448", "abs": "https://arxiv.org/abs/2510.16448", "authors": ["Yongxiang Hua", "Haoyu Cao", "Zhou Tao", "Bocheng Li", "Zihao Wu", "Chaohu Liu", "Linli Xu"], "title": "Input Domain Aware MoE: Decoupling Routing Decisions from Task Optimization in Mixture of Experts", "comment": "ACM MM25", "summary": "Sparse Mixture of Experts (sMoE) has become a pivotal approach for scaling\nlarge vision-language models, offering substantial capacity while maintaining\ncomputational efficiency through dynamic, sparse activation of experts.\nHowever, existing routing mechanisms, typically based on similarity scoring,\nstruggle to effectively capture the underlying input structure. This limitation\nleads to a trade-off between expert specialization and balanced computation,\nhindering both scalability and performance. We propose Input Domain Aware MoE,\na novel routing framework that leverages a probabilistic mixture model to\nbetter partition the input space. By modeling routing probabilities as a\nmixture of distributions, our method enables experts to develop clear\nspecialization boundaries while achieving balanced utilization. Unlike\nconventional approaches, our routing mechanism is trained independently of\ntask-specific objectives, allowing for stable optimization and decisive expert\nassignments. Empirical results on vision-language tasks demonstrate that our\nmethod consistently outperforms existing sMoE approaches, achieving higher task\nperformance and improved expert utilization balance."}
{"id": "2510.16462", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16462", "abs": "https://arxiv.org/abs/2510.16462", "authors": ["Emmanuelle Claeys", "Elena Kerjean", "Jean-Michel Loubes"], "title": "Buzz, Choose, Forget: A Meta-Bandit Framework for Bee-Like Decision Making", "comment": null, "summary": "We introduce a sequential reinforcement learning framework for imitation\nlearning designed to model heterogeneous cognitive strategies in pollinators.\nFocusing on honeybees, our approach leverages trajectory similarity to capture\nand forecast behavior across individuals that rely on distinct strategies: some\nexploiting numerical cues, others drawing on memory, or being influenced by\nenvironmental factors such as weather. Through empirical evaluation, we show\nthat state-of-the-art imitation learning methods often fail in this setting:\nwhen expert policies shift across memory windows or deviate from optimality,\nthese models overlook both fast and slow learning behaviors and cannot\nfaithfully reproduce key decision patterns. Moreover, they offer limited\ninterpretability, hindering biological insight. Our contribution addresses\nthese challenges by (i) introducing a model that minimizes predictive loss\nwhile identifying the effective memory horizon most consistent with behavioral\ndata, and (ii) ensuring full interpretability to enable biologists to analyze\nunderlying decision-making strategies and finally (iii) providing a\nmathematical framework linking bee policy search with bandit formulations under\nvarying exploration-exploitation dynamics, and releasing a novel dataset of 80\ntracked bees observed under diverse weather conditions. This benchmark\nfacilitates research on pollinator cognition and supports ecological governance\nby improving simulations of insect behavior in agroecosystems. Our findings\nshed new light on the learning strategies and memory interplay shaping\npollinator decision-making."}
{"id": "2510.16474", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16474", "abs": "https://arxiv.org/abs/2510.16474", "authors": ["Farwa Abbas", "Hussain Ahmad", "Claudia Szabo"], "title": "SCALAR: Self-Calibrating Adaptive Latent Attention Representation Learning", "comment": null, "summary": "High-dimensional, heterogeneous data with complex feature interactions pose\nsignificant challenges for traditional predictive modeling approaches. While\nProjection to Latent Structures (PLS) remains a popular technique, it struggles\nto model complex non-linear relationships, especially in multivariate systems\nwith high-dimensional correlation structures. This challenge is further\ncompounded by simultaneous interactions across multiple scales, where local\nprocessing fails to capture crossgroup dependencies. Additionally, static\nfeature weighting limits adaptability to contextual variations, as it ignores\nsample-specific relevance. To address these limitations, we propose a novel\nmethod that enhances predictive performance through novel architectural\ninnovations. Our architecture introduces an adaptive kernel-based attention\nmechanism that processes distinct feature groups separately before integration,\nenabling capture of local patterns while preserving global relationships.\nExperimental results show substantial improvements in performance metrics,\ncompared to the state-of-the-art methods across diverse datasets."}
{"id": "2510.16511", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16511", "abs": "https://arxiv.org/abs/2510.16511", "authors": ["Dongchan Cho", "Jiho Han", "Keumyeong Kang", "Minsang Kim", "Honggyu Ryu", "Namsoon Jung"], "title": "Structured Temporal Causality for Interpretable Multivariate Time Series Anomaly Detection", "comment": "Accepted by NeurIPS 2025", "summary": "Real-world multivariate time series anomalies are rare and often unlabeled.\nAdditionally, prevailing methods rely on increasingly complex architectures\ntuned to benchmarks, detecting only fragments of anomalous segments and\noverstating performance. In this paper, we introduce OracleAD, a simple and\ninterpretable unsupervised framework for multivariate time series anomaly\ndetection. OracleAD encodes each variable's past sequence into a single causal\nembedding to jointly predict the present time point and reconstruct the input\nwindow, effectively modeling temporal dynamics. These embeddings then undergo a\nself-attention mechanism to project them into a shared latent space and capture\nspatial relationships. These relationships are not static, since they are\nmodeled by a property that emerges from each variable's temporal dynamics. The\nprojected embeddings are aligned to a Stable Latent Structure (SLS)\nrepresenting normal-state relationships. Anomalies are identified using a dual\nscoring mechanism based on prediction error and deviation from the SLS,\nenabling fine-grained anomaly diagnosis at each time point and across\nindividual variables. Since any noticeable SLS deviation originates from\nembeddings that violate the learned temporal causality of normal data, OracleAD\ndirectly pinpoints the root-cause variables at the embedding level. OracleAD\nachieves state-of-the-art results across multiple real-world datasets and\nevaluation protocols, while remaining interpretable through SLS."}
{"id": "2510.16513", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16513", "abs": "https://arxiv.org/abs/2510.16513", "authors": ["Dhruv Gupta", "Aditya Nagarsekar", "Vraj Shah", "Sujith Thomas"], "title": "eDCF: Estimating Intrinsic Dimension using Local Connectivity", "comment": "58 pages (35 (main) + 23 (appendix)), 54 figures (27 (main) + 27\n  (appendix))", "summary": "Modern datasets often contain high-dimensional features exhibiting complex\ndependencies. To effectively analyze such data, dimensionality reduction\nmethods rely on estimating the dataset's intrinsic dimension (id) as a measure\nof its underlying complexity. However, estimating id is challenging due to its\ndependence on scale: at very fine scales, noise inflates id estimates, while at\ncoarser scales, estimates stabilize to lower, scale-invariant values. This\npaper introduces a novel, scalable, and parallelizable method called eDCF,\nwhich is based on Connectivity Factor (CF), a local connectivity-based metric,\nto robustly estimate intrinsic dimension across varying scales. Our method\nconsistently matches leading estimators, achieving comparable values of mean\nabsolute error (MAE) on synthetic benchmarks with noisy samples. Moreover, our\napproach also attains higher exact intrinsic dimension match rates, reaching up\nto 25.0% compared to 16.7% for MLE and 12.5% for TWO-NN, particularly excelling\nunder medium to high noise levels and large datasets. Further, we showcase our\nmethod's ability to accurately detect fractal geometries in decision\nboundaries, confirming its utility for analyzing realistic, structured data."}
{"id": "2510.16530", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16530", "abs": "https://arxiv.org/abs/2510.16530", "authors": ["Ashutosh Srivastava", "Lokesh Nagalapatti", "Gautam Jajoo", "Aniket Vashishtha", "Parameswari Krishnamurthy", "Amit Sharma"], "title": "Realizing LLMs' Causal Potential Requires Science-Grounded, Novel Benchmarks", "comment": null, "summary": "Recent claims of strong performance by Large Language Models (LLMs) on causal\ndiscovery are undermined by a key flaw: many evaluations rely on benchmarks\nlikely included in pretraining corpora. Thus, apparent success suggests that\nLLM-only methods, which ignore observational data, outperform classical\nstatistical approaches. We challenge this narrative by asking: Do LLMs truly\nreason about causal structure, and how can we measure it without memorization\nconcerns? Can they be trusted for real-world scientific discovery? We argue\nthat realizing LLMs' potential for causal analysis requires two shifts: (P.1)\ndeveloping robust evaluation protocols based on recent scientific studies to\nguard against dataset leakage, and (P.2) designing hybrid methods that combine\nLLM-derived knowledge with data-driven statistics. To address P.1, we encourage\nevaluating discovery methods on novel, real-world scientific studies. We\noutline a practical recipe for extracting causal graphs from recent\npublications released after an LLM's training cutoff, ensuring relevance and\npreventing memorization while capturing both established and novel relations.\nCompared to benchmarks like BNLearn, where LLMs achieve near-perfect accuracy,\nthey perform far worse on our curated graphs, underscoring the need for\nstatistical grounding. Supporting P.2, we show that using LLM predictions as\npriors for the classical PC algorithm significantly improves accuracy over both\nLLM-only and purely statistical methods. We call on the community to adopt\nscience-grounded, leakage-resistant benchmarks and invest in hybrid causal\ndiscovery methods suited to real-world inquiry."}
{"id": "2510.16547", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16547", "abs": "https://arxiv.org/abs/2510.16547", "authors": ["Alif Elham Khan", "Mohammad Junayed Hasan", "Humayra Anjum", "Nabeel Mohammed", "Sifat Momen"], "title": "Predicting life satisfaction using machine learning and explainable AI", "comment": null, "summary": "Life satisfaction is a crucial facet of human well-being. Hence, research on\nlife satisfaction is incumbent for understanding how individuals experience\ntheir lives and influencing interventions targeted at enhancing mental health\nand well-being. Life satisfaction has traditionally been measured using analog,\ncomplicated, and frequently error-prone methods. These methods raise questions\nconcerning validation and propagation. However, this study demonstrates the\npotential for machine learning algorithms to predict life satisfaction with a\nhigh accuracy of 93.80% and a 73.00% macro F1-score. The dataset comes from a\ngovernment survey of 19000 people aged 16-64 years in Denmark. Using feature\nlearning techniques, 27 significant questions for assessing contentment were\nextracted, making the study highly reproducible, simple, and easily\ninterpretable. Furthermore, clinical and biomedical large language models\n(LLMs) were explored for predicting life satisfaction by converting tabular\ndata into natural language sentences through mapping and adding meaningful\ncounterparts, achieving an accuracy of 93.74% and macro F1-score of 73.21%. It\nwas found that life satisfaction prediction is more closely related to the\nbiomedical domain than the clinical domain. Ablation studies were also\nconducted to understand the impact of data resampling and feature selection\ntechniques on model performance. Moreover, the correlation between primary\ndeterminants with different age brackets was analyzed, and it was found that\nhealth condition is the most important determinant across all ages. This study\ndemonstrates how machine learning, large language models and XAI can jointly\ncontribute to building trust and understanding in using AI to investigate human\nbehavior, with significant ramifications for academics and professionals\nworking to quantify and comprehend subjective well-being."}
{"id": "2510.16548", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16548", "abs": "https://arxiv.org/abs/2510.16548", "authors": ["Zitao Fang", "Chenxuan Li", "Hongting Zhou", "Shuyang Yu", "Guodong Du", "Ashwaq Qasem", "Yang Lu", "Jing Li", "Junsong Zhang", "Sim Kuan Goh"], "title": "NeurIPT: Foundation Model for Neural Interfaces", "comment": "Accepted by The Thirty-Ninth Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2025). Project Page:\n  https://ZzzitaoFang.github.io/projects/NeurIPT/", "summary": "Electroencephalography (EEG) has wide-ranging applications, from clinical\ndiagnosis to brain-computer interfaces (BCIs). With the increasing volume and\nvariety of EEG data, there has been growing interest in establishing foundation\nmodels (FMs) to scale up and generalize neural decoding. Despite showing early\npotential, applying FMs to EEG remains challenging due to substantial\ninter-subject, inter-task, and inter-condition variability, as well as diverse\nelectrode configurations across recording setups. To tackle these open\nchallenges, we propose NeurIPT, a foundation model developed for diverse\nEEG-based Neural Interfaces with a Pre-trained Transformer by capturing both\nhomogeneous and heterogeneous spatio-temporal characteristics inherent in EEG\nsignals. Temporally, we introduce Amplitude-Aware Masked Pretraining (AAMP),\nmasking based on signal amplitude rather than random intervals, to learn robust\nrepresentations across varying signal intensities beyond local interpolation.\nMoreover, this temporal representation is enhanced by a Progressive\nMixture-of-Experts (PMoE) architecture, where specialized expert subnetworks\nare progressively introduced at deeper layers, adapting effectively to the\ndiverse temporal characteristics of EEG signals. Spatially, NeurIPT leverages\nthe 3D physical coordinates of electrodes, enabling effective transfer of\nembedding across varying EEG settings, and develops Intra-Inter Lobe Pooling\n(IILP) during fine-tuning to efficiently exploit regional brain features.\nEmpirical evaluations across eight downstream BCI datasets, via fine-tuning,\ndemonstrated NeurIPT consistently achieved state-of-the-art performance,\nhighlighting its broad applicability and robust generalization. Our work pushes\nforward the state of FMs in EEG and offers insights into scalable and\ngeneralizable neural information processing systems."}
{"id": "2510.16552", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16552", "abs": "https://arxiv.org/abs/2510.16552", "authors": ["Ang Li", "Yifei Wang", "Zhihang Yuan", "Stefanie Jegelka", "Yisen Wang"], "title": "LANPO: Bootstrapping Language and Numerical Feedback for Reinforcement Learning in LLMs", "comment": null, "summary": "Reinforcement learning in large language models (LLMs) often relies on scalar\nrewards, a practice that discards valuable textual rationale buried in the\nrollouts, forcing the model to explore \\textit{de novo} with each attempt and\nhindering sample efficiency. While LLMs can uniquely learn from language\nfeedback provided in-context, naively integrating on-line experiences into RL\ntraining presents a paradox: feedback from the same problem risks information\nleakage and memorization, while feedback from different problems often leads to\nbehavior collapse due to irrelevant context. To resolve this tension, we\npropose \\textbf{Language-And-Numerical Policy Optimization (LANPO)}, a\nframework that cleanly separates the roles of feedback: language guides\nexploration, while numerical rewards drive optimization. LANPO builds a dynamic\nexperience pool from past trials and introduces two principles to ensure\nfeedback is effective: \\emph{Reward-Agnostic Reflection} for safe intra-sample\nself-correction and \\emph{Relevant Abstraction} to distill generalizable\nlessons from inter-sample experiences. Across mathematical reasoning\nbenchmarks, LANPO enables 7B and 14B models to significantly outperform strong\nbaselines trained with GRPO in test accuracy. Our work provides a robust method\nfor integrating historical experiences into the LLM RL loop, creating more\neffective and data-efficient learning agents."}
{"id": "2510.16588", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16588", "abs": "https://arxiv.org/abs/2510.16588", "authors": ["Jiaxi Zhuang", "Yu Zhang", "Aimin Zhou", "Ying Qian"], "title": "Copy-Augmented Representation for Structure Invariant Template-Free Retrosynthesis", "comment": null, "summary": "Retrosynthesis prediction is fundamental to drug discovery and chemical\nsynthesis, requiring the identification of reactants that can produce a target\nmolecule. Current template-free methods struggle to capture the structural\ninvariance inherent in chemical reactions, where substantial molecular\nscaffolds remain unchanged, leading to unnecessarily large search spaces and\nreduced prediction accuracy. We introduce C-SMILES, a novel molecular\nrepresentation that decomposes traditional SMILES into element-token pairs with\nfive special tokens, effectively minimizing editing distance between reactants\nand products. Building upon this representation, we incorporate a\ncopy-augmented mechanism that dynamically determines whether to generate new\ntokens or preserve unchanged molecular fragments from the product. Our approach\nintegrates SMILES alignment guidance to enhance attention consistency with\nground-truth atom mappings, enabling more chemically coherent predictions.\nComprehensive evaluation on USPTO-50K and large-scale USPTO-FULL datasets\ndemonstrates significant improvements: 67.2% top-1 accuracy on USPTO-50K and\n50.8% on USPTO-FULL, with 99.9% validity in generated molecules. This work\nestablishes a new paradigm for structure-aware molecular generation with direct\napplications in computational drug discovery."}
{"id": "2510.16590", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2510.16590", "abs": "https://arxiv.org/abs/2510.16590", "authors": ["Alan Kai Hassen", "Andrius Bernatavicius", "Antonius P. A. Janssen", "Mike Preuss", "Gerard J. P. van Westen", "Djork-Arné Clevert"], "title": "Atom-anchored LLMs speak Chemistry: A Retrosynthesis Demonstration", "comment": "Alan Kai Hassen and Andrius Bernatavicius contributed equally to this\n  work", "summary": "Applications of machine learning in chemistry are often limited by the\nscarcity and expense of labeled data, restricting traditional supervised\nmethods. In this work, we introduce a framework for molecular reasoning using\ngeneral-purpose Large Language Models (LLMs) that operates without requiring\nlabeled training data. Our method anchors chain-of-thought reasoning to the\nmolecular structure by using unique atomic identifiers. First, the LLM performs\na one-shot task to identify relevant fragments and their associated chemical\nlabels or transformation classes. In an optional second step, this\nposition-aware information is used in a few-shot task with provided class\nexamples to predict the chemical transformation. We apply our framework to\nsingle-step retrosynthesis, a task where LLMs have previously underperformed.\nAcross academic benchmarks and expert-validated drug discovery molecules, our\nwork enables LLMs to achieve high success rates in identifying chemically\nplausible reaction sites ($\\geq90\\%$), named reaction classes ($\\geq40\\%$), and\nfinal reactants ($\\geq74\\%$). Beyond solving complex chemical tasks, our work\nalso provides a method to generate theoretically grounded synthetic datasets by\nmapping chemical knowledge onto the molecular structure and thereby addressing\ndata scarcity."}
{"id": "2510.16591", "categories": ["cs.LG", "cond-mat.stat-mech", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16591", "abs": "https://arxiv.org/abs/2510.16591", "authors": ["Cassidy Ashworth", "Pietro Liò", "Francesco Caso"], "title": "Symmetry and Generalisation in Neural Approximations of Renormalisation Transformations", "comment": null, "summary": "Deep learning models have proven enormously successful at using multiple\nlayers of representation to learn relevant features of structured data.\nEncoding physical symmetries into these models can improve performance on\ndifficult tasks, and recent work has motivated the principle of parameter\nsymmetry breaking and restoration as a unifying mechanism underlying their\nhierarchical learning dynamics. We evaluate the role of parameter symmetry and\nnetwork expressivity in the generalisation behaviour of neural networks when\nlearning a real-space renormalisation group (RG) transformation, using the\ncentral limit theorem (CLT) as a test case map. We consider simple multilayer\nperceptrons (MLPs) and graph neural networks (GNNs), and vary weight symmetries\nand activation functions across architectures. Our results reveal a competition\nbetween symmetry constraints and expressivity, with overly complex or\noverconstrained models generalising poorly. We analytically demonstrate this\npoor generalisation behaviour for certain constrained MLP architectures by\nrecasting the CLT as a cumulant recursion relation and making use of an\nestablished framework to propagate cumulants through MLPs. We also empirically\nvalidate an extension of this framework from MLPs to GNNs, elucidating the\ninternal information processing performed by these more complex models. These\nfindings offer new insight into the learning dynamics of symmetric networks and\ntheir limitations in modelling structured physical transformations."}
{"id": "2510.16607", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16607", "abs": "https://arxiv.org/abs/2510.16607", "authors": ["Tianwei Wang", "Xinhui Ma", "Wei Pang"], "title": "Asymptotically Stable Quaternion-valued Hopfield-structured Neural Network with Periodic Projection-based Supervised Learning Rules", "comment": null, "summary": "Motivated by the geometric advantages of quaternions in representing\nrotations and postures, we propose a quaternion-valued supervised learning\nHopfield-structured neural network (QSHNN) with a fully connected structure\ninspired by the classic Hopfield neural network (HNN). Starting from a\ncontinuous-time dynamical model of HNNs, we extend the formulation to the\nquaternionic domain and establish the existence and uniqueness of fixed points\nwith asymptotic stability. For the learning rules, we introduce a periodic\nprojection strategy that modifies standard gradient descent by periodically\nprojecting each 4*4 block of the weight matrix onto the closest quaternionic\nstructure in the least-squares sense. This approach preserves both convergence\nand quaternionic consistency throughout training. Benefiting from this rigorous\nmathematical foundation, the experimental model implementation achieves high\naccuracy, fast convergence, and strong reliability across randomly generated\ntarget sets. Moreover, the evolution trajectories of the QSHNN exhibit\nwell-bounded curvature, i.e., sufficient smoothness, which is crucial for\napplications such as control systems or path planning modules in robotic arms,\nwhere joint postures are parameterized by quaternion neurons. Beyond these\napplication scenarios, the proposed model offers a practical implementation\nframework and a general mathematical methodology for designing neural networks\nunder hypercomplex or non-commutative algebraic structures."}
{"id": "2510.16609", "categories": ["cs.LG", "cs.AI", "cs.CC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.16609", "abs": "https://arxiv.org/abs/2510.16609", "authors": ["Avrim Blum", "Daniel Hsu", "Cyrus Rashtchian", "Donya Saless"], "title": "Prior Makes It Possible: From Sublinear Graph Algorithms to LLM Test-Time Methods", "comment": null, "summary": "Test-time augmentation, such as Retrieval-Augmented Generation (RAG) or tool\nuse, critically depends on an interplay between a model's parametric knowledge\nand externally retrieved information. However, the theoretical underpinnings of\nthis relationship remain poorly understood. Specifically, it is not clear how\nmuch pre-training knowledge is required to answer queries with a small number\nof augmentation steps, which is a desirable property in practice. To address\nthis question, we formulate multi-step reasoning as an $s$-$t$ connectivity\nproblem on a knowledge graph. We represent a model's pre-training parametric\nknowledge as a partial, potentially noisy subgraph. We view augmentation as\nquerying an oracle for true edges that augment the model's knowledge. Then, we\ncharacterize the necessary and sufficient number of augmentation steps for the\nmodel to generate an accurate answer given partial prior knowledge. One key\nresult shows a phase transition: if the prior knowledge graph over $n$ vertices\nis disconnected into small components, then finding a path via augmentation is\ninefficient and requires $\\Omega(\\sqrt{n})$ queries. On the other hand, once\nthe density of correct knowledge surpasses a threshold, forming a giant\ncomponent, we can find paths with an expected constant number of queries."}
{"id": "2510.16629", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16629", "abs": "https://arxiv.org/abs/2510.16629", "authors": ["Jiatong Yu", "Yinghui He", "Anirudh Goyal", "Sanjeev Arora"], "title": "On the Impossibility of Retrain Equivalence in Machine Unlearning", "comment": "Code available at\n  https://princeton-pli.github.io/impossibility-unlearning/", "summary": "Machine unlearning seeks to selectively remove the \"influence\" of specific\ntraining data on a model's outputs. The ideal goal is Retrain\nEquivalence--behavior identical to a model trained from scratch on only the\nretained data. This goal was formulated for models trained on i.i.d. data\nbatches, but modern pipelines often involve multi-stage training, with each\nstage having a distinct data distribution and objective. Examples include LLM\nfine-tuning for alignment, reasoning ability, etc. Our study shows via theory\nand experiments that this shift to multi-stage training introduces a\nfundamental barrier for machine unlearning. The theory indicates that the\noutcome of local unlearning--methods that only use gradients computed on the\nforget set--is path-dependent. That is, a model's behavior during unlearning is\ninfluenced by the order of its training stages during learning, making it\nimpossible for path-oblivious algorithms to universally achieve Retrain\nEquivalence. We empirically demonstrate the same phenomenon in LLM\npost-training across Llama and Qwen models (1B to 14B) with gradient ascent,\nNPO, and SimNPO local unlearning algorithms. Models fine-tuned via different\norderings of identical training stages diverge in behavior during unlearning,\nwith the degradation in GSM8K accuracy after unlearning varying by over 20%\nacross paths. We also observe that some learning paths consistently produce\nmodels that unlearn slowly. During unlearning, whether the probability mass\ngets squeezed into paraphrasing or alternative concepts is also path-dependent.\nThese results consistently show that Retrain Equivalence is an ill-posed target\nfor local unlearning algorithms, so long as the target models are trained in\nstages. In situations where access to models' training histories is hard, the\ncurrent work calls for rethinking the definition and desiderata of machine\nunlearning."}
{"id": "2510.16656", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.16656", "abs": "https://arxiv.org/abs/2510.16656", "authors": ["Noah El Rimawi-Fine", "Adam Stecklov", "Lucas Nelson", "Mathieu Blanchette", "Alexander Tong", "Stephen Y. Zhang", "Lazar Atanackovic"], "title": "Simulation-free Structure Learning for Stochastic Dynamics", "comment": null, "summary": "Modeling dynamical systems and unraveling their underlying causal\nrelationships is central to many domains in the natural sciences. Various\nphysical systems, such as those arising in cell biology, are inherently\nhigh-dimensional and stochastic in nature, and admit only partial, noisy state\nmeasurements. This poses a significant challenge for addressing the problems of\nmodeling the underlying dynamics and inferring the network structure of these\nsystems. Existing methods are typically tailored either for structure learning\nor modeling dynamics at the population level, but are limited in their ability\nto address both problems together. In this work, we address both problems\nsimultaneously: we present StructureFlow, a novel and principled\nsimulation-free approach for jointly learning the structure and stochastic\npopulation dynamics of physical systems. We showcase the utility of\nStructureFlow for the tasks of structure learning from interventions and\ndynamical (trajectory) inference of conditional population dynamics. We\nempirically evaluate our approach on high-dimensional synthetic systems, a set\nof biologically plausible simulated systems, and an experimental single-cell\ndataset. We show that StructureFlow can learn the structure of underlying\nsystems while simultaneously modeling their conditional population dynamics --\na key step toward the mechanistic understanding of systems behavior."}
{"id": "2510.16674", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.16674", "abs": "https://arxiv.org/abs/2510.16674", "authors": ["Azam Shirali", "Giri Narasimhan"], "title": "Evaluating protein binding interfaces with PUMBA", "comment": null, "summary": "Protein-protein docking tools help in studying interactions between proteins,\nand are essential for drug, vaccine, and therapeutic development. However, the\naccuracy of a docking tool depends on a robust scoring function that can\nreliably differentiate between native and non-native complexes. PIsToN is a\nstate-of-the-art deep learning-based scoring function that uses Vision\nTransformers in its architecture. Recently, the Mamba architecture has\ndemonstrated exceptional performance in both natural language processing and\ncomputer vision, often outperforming Transformer-based models in their domains.\nIn this study, we introduce PUMBA (Protein-protein interface evaluation with\nVision Mamba), which improves PIsToN by replacing its Vision Transformer\nbackbone with Vision Mamba. This change allows us to leverage Mamba's efficient\nlong-range sequence modeling for sequences of image patches. As a result, the\nmodel's ability to capture both global and local patterns in protein-protein\ninterface features is significantly improved. Evaluation on several\nwidely-used, large-scale public datasets demonstrates that PUMBA consistently\noutperforms its original Transformer-based predecessor, PIsToN."}
{"id": "2510.16676", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16676", "abs": "https://arxiv.org/abs/2510.16676", "authors": ["Anindya Sarkar", "Binglin Ji", "Yevgeniy Vorobeychik"], "title": "Active Target Discovery under Uninformative Prior: The Power of Permanent and Transient Memory", "comment": "32 pages, 20 figures, Accepted to NeurIPS 2025", "summary": "In many scientific and engineering fields, where acquiring high-quality data\nis expensive--such as medical imaging, environmental monitoring, and remote\nsensing--strategic sampling of unobserved regions based on prior observations\nis crucial for maximizing discovery rates within a constrained budget. The rise\nof powerful generative models, such as diffusion models, has enabled active\ntarget discovery in partially observable environments by leveraging learned\npriors--probabilistic representations that capture underlying structure from\ndata. With guidance from sequentially gathered task-specific observations,\nthese models can progressively refine exploration and efficiently direct\nqueries toward promising regions. However, in domains where learning a strong\nprior is infeasible due to extremely limited data or high sampling cost (such\nas rare species discovery, diagnostics for emerging diseases, etc.), these\nmethods struggle to generalize. To overcome this limitation, we propose a novel\napproach that enables effective active target discovery even in settings with\nuninformative priors, ensuring robust exploration and adaptability in complex\nreal-world scenarios. Our framework is theoretically principled and draws\ninspiration from neuroscience to guide its design. Unlike black-box policies,\nour approach is inherently interpretable, providing clear insights into\ndecision-making. Furthermore, it guarantees a strong, monotonic improvement in\nprior estimates with each new observation, leading to increasingly accurate\nsampling and reinforcing both reliability and adaptability in dynamic settings.\nThrough comprehensive experiments and ablation studies across various domains,\nincluding species distribution modeling and remote sensing, we demonstrate that\nour method substantially outperforms baseline approaches."}
{"id": "2510.16677", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16677", "abs": "https://arxiv.org/abs/2510.16677", "authors": ["Ran Tong", "Jiaqi Liu", "Su Liu", "Xin Hu", "Lanruo Wang"], "title": "Renaissance of RNNs in Streaming Clinical Time Series: Compact Recurrence Remains Competitive with Transformers", "comment": null, "summary": "We present a compact, strictly causal benchmark for streaming clinical time\nseries on the MIT--BIH Arrhythmia Database using per-second heart rate. Two\ntasks are studied under record-level, non-overlapping splits: near-term\ntachycardia risk (next ten seconds) and one-step heart rate forecasting. We\ncompare a GRU-D (RNN) and a Transformer under matched training budgets against\nstrong non-learned baselines. Evaluation is calibration-aware for\nclassification and proper for forecasting, with temperature scaling and grouped\nbootstrap confidence intervals. On MIT-BIH, GRU-D slightly surpasses the\nTransformer for tachycardia risk, while the Transformer clearly lowers\nforecasting error relative to GRU-D and persistence. Our results show that, in\nlongitudinal monitoring, model choice is task-dependent: compact RNNs remain\ncompetitive for short-horizon risk scoring, whereas compact Transformers\ndeliver clearer gains for point forecasting."}
{"id": "2510.16687", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16687", "abs": "https://arxiv.org/abs/2510.16687", "authors": ["Shurong Lin", "Eric D. Kolaczyk", "Adam Smith", "Elliot Paquette"], "title": "High-Dimensional Privacy-Utility Dynamics of Noisy Stochastic Gradient Descent on Least Squares", "comment": null, "summary": "The interplay between optimization and privacy has become a central theme in\nprivacy-preserving machine learning. Noisy stochastic gradient descent (SGD)\nhas emerged as a cornerstone algorithm, particularly in large-scale settings.\nThese variants of gradient methods inject carefully calibrated noise into each\nupdate to achieve differential privacy, the gold standard notion of rigorous\nprivacy guarantees. Prior work primarily provides various bounds on statistical\nrisk and privacy loss for noisy SGD, yet the \\textit{exact} behavior of the\nprocess remains unclear, particularly in high-dimensional settings. This work\nleverages a diffusion approach to analyze noisy SGD precisely, providing a\ncontinuous-time perspective that captures both statistical risk evolution and\nprivacy loss dynamics in high dimensions. Moreover, we study a variant of noisy\nSGD that does not require explicit knowledge of gradient sensitivity, unlike\nexisting work that assumes or enforces sensitivity through gradient clipping.\nSpecifically, we focus on the least squares problem with $\\ell_2$\nregularization."}
{"id": "2510.16694", "categories": ["cs.LG", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16694", "abs": "https://arxiv.org/abs/2510.16694", "authors": ["Anthony DiMaggio", "Raghav Sharma", "Gururaj Saileshwar"], "title": "CLIP: Client-Side Invariant Pruning for Mitigating Stragglers in Secure Federated Learning", "comment": null, "summary": "Secure federated learning (FL) preserves data privacy during distributed\nmodel training. However, deploying such frameworks across heterogeneous devices\nresults in performance bottlenecks, due to straggler clients with limited\ncomputational or network capabilities, slowing training for all participating\nclients. This paper introduces the first straggler mitigation technique for\nsecure aggregation with deep neural networks. We propose CLIP, a client-side\ninvariant neuron pruning technique coupled with network-aware pruning, that\naddresses compute and network bottlenecks due to stragglers during training\nwith minimal accuracy loss. Our technique accelerates secure FL training by 13%\nto 34% across multiple datasets (CIFAR10, Shakespeare, FEMNIST) with an\naccuracy impact of between 1.3% improvement to 2.6% reduction."}
{"id": "2510.16695", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.16695", "abs": "https://arxiv.org/abs/2510.16695", "authors": ["Iman Deznabi", "Peeyush Kumar", "Madalina Fiterau"], "title": "Resolution-Aware Retrieval Augmented Zero-Shot Forecasting", "comment": null, "summary": "Zero-shot forecasting aims to predict outcomes for previously unseen\nconditions without direct historical data, posing a significant challenge for\ntraditional forecasting methods. We introduce a Resolution-Aware\nRetrieval-Augmented Forecasting model that enhances predictive accuracy by\nleveraging spatial correlations and temporal frequency characteristics. By\ndecomposing signals into different frequency components, our model employs\nresolution-aware retrieval, where lower-frequency components rely on broader\nspatial context, while higher-frequency components focus on local influences.\nThis allows the model to dynamically retrieve relevant data and adapt to new\nlocations with minimal historical context.\n  Applied to microclimate forecasting, our model significantly outperforms\ntraditional forecasting methods, numerical weather prediction models, and\nmodern foundation time series models, achieving 71% lower MSE than HRRR and 34%\nlower MSE than Chronos on the ERA5 dataset.\n  Our results highlight the effectiveness of retrieval-augmented and\nresolution-aware strategies, offering a scalable and data-efficient solution\nfor zero-shot forecasting in microclimate modeling and beyond."}
{"id": "2510.16703", "categories": ["cs.LG", "cs.AI", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.16703", "abs": "https://arxiv.org/abs/2510.16703", "authors": ["Yizuo Chen", "Adnan Darwiche"], "title": "On the Granularity of Causal Effect Identifiability", "comment": null, "summary": "The classical notion of causal effect identifiability is defined in terms of\ntreatment and outcome variables. In this note, we consider the identifiability\nof state-based causal effects: how an intervention on a particular state of\ntreatment variables affects a particular state of outcome variables. We\ndemonstrate that state-based causal effects may be identifiable even when\nvariable-based causal effects may not. Moreover, we show that this separation\noccurs only when additional knowledge -- such as context-specific\nindependencies and conditional functional dependencies -- is available. We\nfurther examine knowledge that constrains the states of variables, and show\nthat such knowledge does not improve identifiability on its own but can improve\nboth variable-based and state-based identifiability when combined with other\nknowledge such as context-specific independencies. Our findings highlight\nsituations where causal effects of interest may be estimable from observational\ndata and this identifiability may be missed by existing variable-based\nframeworks."}
{"id": "2510.16719", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.16719", "abs": "https://arxiv.org/abs/2510.16719", "authors": ["Zak Ressler", "Marcus Grijalva", "Angelica Marie Ignacio", "Melanie Torres", "Abelardo Cuadra Rojas", "Rohollah Moghadam", "Mohammad Rasoul narimani"], "title": "LSTM-Based Forecasting and Analysis of EV Charging Demand in a Dense Urban Campus", "comment": null, "summary": "This paper presents a framework for processing EV charging load data in order\nto forecast future load predictions using a Recurrent Neural Network,\nspecifically an LSTM. The framework processes a large set of raw data from\nmultiple locations and transforms it with normalization and feature extraction\nto train the LSTM. The pre-processing stage corrects for missing or incomplete\nvalues by interpolating and normalizing the measurements. This information is\nthen fed into a Long Short-Term Memory Model designed to capture the short-term\nfluctuations while also interpreting the long-term trends in the charging data.\nExperimental results demonstrate the model's ability to accurately predict\ncharging demand across multiple time scales (daily, weekly, and monthly),\nproviding valuable insights for infrastructure planning, energy management, and\ngrid integration of EV charging facilities. The system's modular design allows\nfor adaptation to different charging locations with varying usage patterns,\nmaking it applicable across diverse deployment scenarios."}
{"id": "2510.16743", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16743", "abs": "https://arxiv.org/abs/2510.16743", "authors": ["Viktoria Schram", "Markus Hiller", "Daniel Beck", "Trevor Cohn"], "title": "Zero-Shot Performance Prediction for Probabilistic Scaling Laws", "comment": "Accepted to NeurIPS 2025", "summary": "The prediction of learning curves for Natural Language Processing (NLP)\nmodels enables informed decision-making to meet specific performance\nobjectives, while reducing computational overhead and lowering the costs\nassociated with dataset acquisition and curation. In this work, we formulate\nthe prediction task as a multitask learning problem, where each task's data is\nmodelled as being organized within a two-layer hierarchy. To model the shared\ninformation and dependencies across tasks and hierarchical levels, we employ\nlatent variable multi-output Gaussian Processes, enabling to account for task\ncorrelations and supporting zero-shot prediction of learning curves (LCs). We\ndemonstrate that this approach facilitates the development of probabilistic\nscaling laws at lower costs. Applying an active learning strategy, LCs can be\nqueried to reduce predictive uncertainty and provide predictions close to\nground truth scaling laws. We validate our framework on three small-scale NLP\ndatasets with up to $30$ LCs. These are obtained from nanoGPT models, from\nbilingual translation using mBART and Transformer models, and from multilingual\ntranslation using M2M100 models of varying sizes."}
{"id": "2510.16747", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16747", "abs": "https://arxiv.org/abs/2510.16747", "authors": ["Danish Nazir", "Gowtham Sai Inti", "Timo Bartels", "Jan Piewek", "Thorsten Bagdonat", "Tim Fingscheidt"], "title": "An Efficient Semantic Segmentation Decoder for In-Car or Distributed Applications", "comment": null, "summary": "Modern automotive systems leverage deep neural networks (DNNs) for semantic\nsegmentation and operate in two key application areas: (1) In-car, where the\nDNN solely operates in the vehicle without strict constraints on the data rate.\n(2) Distributed, where one DNN part operates in the vehicle and the other part\ntypically on a large-scale cloud platform with a particular constraint on\ntransmission bitrate efficiency. Typically, both applications share an image\nand source encoder, while each uses distinct (joint) source and task decoders.\nPrior work utilized convolutional neural networks for joint source and task\ndecoding but did not investigate transformer-based alternatives such as\nSegDeformer, which offer superior performance at the cost of higher\ncomputational complexity. In this work, we propose joint feature and task\ndecoding for SegDeformer, thereby enabling lower computational complexity in\nboth in-car and distributed applications, despite SegDeformer's computational\ndemands. This improves scalability in the cloud while reducing in-car\ncomputational complexity. For the in-car application, we increased the frames\nper second (fps) by up to a factor of $11.7$ ($1.4$ fps to $16.5$ fps) on\nCityscapes and by up to a factor of $3.5$ ($43.3$ fps to $154.3$ fps) on\nADE20K, while being on-par w.r.t.\\ the mean intersection over union (mIoU) of\nthe transformer-based baseline that doesn't compress by a source codec. For the\ndistributed application, we achieve state-of-the-art (SOTA) over a wide range\nof bitrates on the mIoU metric, while using only $0.14$\\% ($0.04$\\%) of cloud\nDNN parameters used in previous SOTA, reported on ADE20K (Cityscapes)."}
{"id": "2510.16757", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16757", "abs": "https://arxiv.org/abs/2510.16757", "authors": ["Young In Kim", "Andrea Agiollo", "Rajiv Khanna"], "title": "SAMOSA: Sharpness Aware Minimization for Open Set Active learning", "comment": null, "summary": "Modern machine learning solutions require extensive data collection where\nlabeling remains costly. To reduce this burden, open set active learning\napproaches aim to select informative samples from a large pool of unlabeled\ndata that includes irrelevant or unknown classes. In this context, we propose\nSharpness Aware Minimization for Open Set Active Learning (SAMOSA) as an\neffective querying algorithm. Building on theoretical findings concerning the\nimpact of data typicality on the generalization properties of traditional\nstochastic gradient descent (SGD) and sharpness-aware minimization (SAM),\nSAMOSA actively queries samples based on their typicality. SAMOSA effectively\nidentifies atypical samples that belong to regions of the embedding manifold\nclose to the model decision boundaries. Therefore, SAMOSA prioritizes the\nsamples that are (i) highly informative for the targeted classes, and (ii)\nuseful for distinguishing between targeted and unwanted classes. Extensive\nexperiments show that SAMOSA achieves up to 3% accuracy improvement over the\nstate of the art across several datasets, while not introducing computational\noverhead. The source code of our experiments is available at:\nhttps://anonymous.4open.science/r/samosa-DAF4"}
{"id": "2510.16774", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16774", "abs": "https://arxiv.org/abs/2510.16774", "authors": ["Yuguang Yue", "Irakli Salia", "Samuel Hunt", "Christopher Green", "Wenzhe Shi", "Jonathan J Hunt"], "title": "Learning to play: A Multimodal Agent for 3D Game-Play", "comment": "International Conference on Computer Vision Workshop on Multi-Modal\n  Reasoning for Agentic Intelligence", "summary": "We argue that 3-D first-person video games are a challenging environment for\nreal-time multi-modal reasoning. We first describe our dataset of human\ngame-play, collected across a large variety of 3-D first-person games, which is\nboth substantially larger and more diverse compared to prior publicly disclosed\ndatasets, and contains text instructions. We demonstrate that we can learn an\ninverse dynamics model from this dataset, which allows us to impute actions on\na much larger dataset of publicly available videos of human game play that lack\nrecorded actions. We then train a text-conditioned agent for game playing using\nbehavior cloning, with a custom architecture capable of realtime inference on a\nconsumer GPU. We show the resulting model is capable of playing a variety of\n3-D games and responding to text input. Finally, we outline some of the\nremaining challenges such as long-horizon tasks and quantitative evaluation\nacross a large set of games."}
{"id": "2510.16780", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16780", "abs": "https://arxiv.org/abs/2510.16780", "authors": ["Chang Wu", "Zhiyuan Liu", "Wen Shu", "Liang Wang", "Yanchen Luo", "Wenqiang Lei", "Yatao Bian", "Junfeng Fang", "Xiang Wang"], "title": "3D-GSRD: 3D Molecular Graph Auto-Encoder with Selective Re-mask Decoding", "comment": null, "summary": "Masked graph modeling (MGM) is a promising approach for molecular\nrepresentation learning (MRL).However, extending the success of re-mask\ndecoding from 2D to 3D MGM is non-trivial, primarily due to two conflicting\nchallenges: avoiding 2D structure leakage to the decoder, while still providing\nsufficient 2D context for reconstructing re-masked atoms.To address these\nchallenges, we propose 3D-GSRD: a 3D Molecular Graph Auto-Encoder with\nSelective Re-mask Decoding. The core innovation of 3D-GSRD lies in its\nSelective Re-mask Decoding(SRD), which re-masks only 3D-relevant information\nfrom encoder representations while preserving the 2D graph structures.This SRD\nis synergistically integrated with a 3D Relational-Transformer(3D-ReTrans)\nencoder alongside a structure-independent decoder. We analyze that SRD,\ncombined with the structure-independent decoder, enhances the encoder's role in\nMRL. Extensive experiments show that 3D-GSRD achieves strong downstream\nperformance, setting a new state-of-the-art on 7 out of 8 targets in the widely\nused MD17 molecular property prediction benchmark. The code is released at\nhttps://github.com/WuChang0124/3D-GSRD."}
{"id": "2510.16805", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16805", "abs": "https://arxiv.org/abs/2510.16805", "authors": ["Mariam Rakka", "Marios Fournarakis", "Olga Krestinskaya", "Jinane Bazzi", "Khaled N. Salama", "Fadi Kurdahi", "Ahmed M. Eltawil", "Mohammed E. Fouda"], "title": "Mixed-Precision Quantization for Language Models: Techniques and Prospects", "comment": "46 pages, 6 figures, 5 tables", "summary": "The rapid scaling of language models (LMs) has resulted in unprecedented\ncomputational, memory, and energy requirements, making their training and\ndeployment increasingly unsustainable. Quantization has emerged as an essential\ncompression technique to reduce model size, alleviate memory bottlenecks, and\naccelerate inference. However, while uniform low-bit quantization (e.g., INT8,\nINT4) provides significant efficiency gains, it can degrade accuracy in\nsensitive components of transformer-based LMs. Mixed-precision quantization\noffers a promising alternative by selectively allocating precision across\nlayers or within tensors to balance efficiency and accuracy. This survey\nprovides a comprehensive overview of Mixed-Precision quantization frameworks\nfor LMs (MXPLMs). We first review quantization fundamentals, including uniform\nand non-uniform quantizers, quantization granularity, and methods widely used\nin post-training quantization. We then categorize and compare recent MXPLM\nframeworks according to their bit allocation strategies and precision\nconfigurations across weights, activations, and key-value caches. A comparative\nanalysis highlights differences in perplexity, zero-shot task performance, and\ndeployment trade-offs. Furthermore, we contrast MXPLMs with earlier\nmixed-precision quantization methods for deep neural networks, identifying\nstrategies that transfer and those that face challenges in the LM setting.\nFinally, we summarize open issues and future directions, including\nhardware-aware design, activation quantization, and scalable optimization\nmethods for billion-parameter models. By consolidating recent advances, this\nwork serves as a reference for understanding the current landscape and research\nprospects of mixed-precision quantization for large-scale language models."}
{"id": "2510.16806", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16806", "abs": "https://arxiv.org/abs/2510.16806", "authors": ["Weilin Wan", "Weizhong Zhang", "Cheng Jin"], "title": "Computational Budget Should Be Considered in Data Selection", "comment": null, "summary": "Data selection improves computational efficiency by choosing informative\nsubsets of training samples. However, existing methods ignore the compute\nbudget, treating data selection and importance evaluation independently of\ncompute budget constraints. Yet empirical studies show no algorithm can\nconsistently outperform others (or even random selection) across varying\nbudgets. We therefore argue that compute budget must be integral to\ndata-selection strategies, since different budgets impose distinct requirements\non data quantity, quality, and distribution for effective training. To this\nend, we propose a novel Computational budget-Aware Data Selection (CADS) method\nand naturally formulate it into a bilevel optimization framework, where the\ninner loop trains the model within the constraints of the computational budget\non some selected subset of training data, while the outer loop optimizes data\nselection based on model evaluation. Our technical contributions lie in\naddressing two main challenges in solving this bilevel optimization problem:\nthe expensive Hessian matrix estimation for outer-loop gradients and the\ncomputational burden of achieving inner-loop optimality during iterations. To\nsolve the first issue, we propose a probabilistic reparameterization strategy\nand compute the gradient using a Hessian-free policy gradient estimator. To\naddress the second challenge, we transform the inner optimization problem into\na penalty term in the outer objective, further discovering that we only need to\nestimate the minimum of a one-dimensional loss to calculate the gradient,\nsignificantly improving efficiency. Extensive experiments show that our method\nachieves performance gains of up to 14.42% over baselines in vision and\nlanguage benchmarks."}
{"id": "2510.16807", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16807", "abs": "https://arxiv.org/abs/2510.16807", "authors": ["Zhoutong Wu", "Yuan Zhang", "Yiming Dong", "Chenheng Zhang", "Cong Fang", "Kun Yuan", "Zhouchen Lin"], "title": "Improving Model Representation and Reducing KV Cache via Skip Connections with First Value Heads", "comment": "The code is available at:\n  \\url{https://github.com/Zhoutong-Wu/SkipV1Former}", "summary": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance."}
{"id": "2510.16811", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16811", "abs": "https://arxiv.org/abs/2510.16811", "authors": ["Mohammad Shahverdikondori", "Jalal Etesami", "Negar Kiyavash"], "title": "Graph Learning is Suboptimal in Causal Bandits", "comment": "31 pages, 5 figures", "summary": "We study regret minimization in causal bandits under causal sufficiency where\nthe underlying causal structure is not known to the agent. Previous work has\nfocused on identifying the reward's parents and then applying classic bandit\nmethods to them, or jointly learning the parents while minimizing regret. We\ninvestigate whether such strategies are optimal. Somewhat counterintuitively,\nour results show that learning the parent set is suboptimal. We do so by\nproving that there exist instances where regret minimization and parent\nidentification are fundamentally conflicting objectives. We further analyze\nboth the known and unknown parent set size regimes, establish novel regret\nlower bounds that capture the combinatorial structure of the action space.\nBuilding on these insights, we propose nearly optimal algorithms that bypass\ngraph and parent recovery, demonstrating that parent identification is indeed\nunnecessary for regret minimization. Experiments confirm that there exists a\nlarge performance gap between our method and existing baselines in various\nenvironments."}
{"id": "2510.16814", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16814", "abs": "https://arxiv.org/abs/2510.16814", "authors": ["Simon Jaxy", "Anton Theys", "Patrick Willett", "W. Chris Carleton", "Ralf Vandam", "Pieter Libin"], "title": "Needles in the Landscape: Semi-Supervised Pseudolabeling for Archaeological Site Discovery under Label Scarcity", "comment": null, "summary": "Archaeological predictive modelling estimates where undiscovered sites are\nlikely to occur by combining known locations with environmental, cultural, and\ngeospatial variables. We address this challenge using a deep learning approach\nbut must contend with structural label scarcity inherent to archaeology:\npositives are rare, and most locations are unlabeled. To address this, we adopt\na semi-supervised, positive-unlabeled (PU) learning strategy, implemented as a\nsemantic segmentation model and evaluated on two datasets covering a\nrepresentative range of archaeological periods. Our approach employs dynamic\npseudolabeling, refined with a Conditional Random Field (CRF) implemented via\nan RNN, increasing label confidence under severe class imbalance. On a\ngeospatial dataset derived from a digital elevation model (DEM), our model\nperforms on par with the state-of-the-art, LAMAP, while achieving higher Dice\nscores. On raw satellite imagery, assessed end-to-end with stratified k-fold\ncross-validation, it maintains performance and yields predictive surfaces with\nimproved interpretability. Overall, our results indicate that semi-supervised\nlearning offers a promising approach to identifying undiscovered sites across\nlarge, sparsely annotated landscapes."}
{"id": "2510.16816", "categories": ["cs.LG", "cs.AI", "math-ph", "math.MP", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.16816", "abs": "https://arxiv.org/abs/2510.16816", "authors": ["Ming Zhong", "Zhenya Yan"], "title": "Efficient High-Accuracy PDEs Solver with the Linear Attention Neural Operator", "comment": "31 pages, 8 figures", "summary": "Neural operators offer a powerful data-driven framework for learning mappings\nbetween function spaces, in which the transformer-based neural operator\narchitecture faces a fundamental scalability-accuracy trade-off: softmax\nattention provides excellent fidelity but incurs quadratic complexity\n$\\mathcal{O}(N^2 d)$ in the number of mesh points $N$ and hidden dimension $d$,\nwhile linear attention variants reduce cost to $\\mathcal{O}(N d^2)$ but often\nsuffer significant accuracy degradation. To address the aforementioned\nchallenge, in this paper, we present a novel type of neural operators, Linear\nAttention Neural Operator (LANO), which achieves both scalability and high\naccuracy by reformulating attention through an agent-based mechanism. LANO\nresolves this dilemma by introducing a compact set of $M$ agent tokens $(M \\ll\nN)$ that mediate global interactions among $N$ tokens. This agent attention\nmechanism yields an operator layer with linear complexity $\\mathcal{O}(MN d)$\nwhile preserving the expressive power of softmax attention. Theoretically, we\ndemonstrate the universal approximation property, thereby demonstrating\nimproved conditioning and stability properties. Empirically, LANO surpasses\ncurrent state-of-the-art neural PDE solvers, including Transolver with\nslice-based softmax attention, achieving average $19.5\\%$ accuracy improvement\nacross standard benchmarks. By bridging the gap between linear complexity and\nsoftmax-level performance, LANO establishes a scalable, high-accuracy\nfoundation for scientific machine learning applications."}
{"id": "2510.16817", "categories": ["cs.LG", "math.AP"], "pdf": "https://arxiv.org/pdf/2510.16817", "abs": "https://arxiv.org/abs/2510.16817", "authors": ["Doyoon Kim", "Junbin Song"], "title": "Trace Regularity PINNs: Enforcing $\\mathrm{H}^{\\frac{1}{2}}(\\partial Ω)$ for Boundary Data", "comment": null, "summary": "We propose an enhanced physics-informed neural network (PINN), the Trace\nRegularity Physics-Informed Neural Network (TRPINN), which enforces the\nboundary loss in the Sobolev-Slobodeckij norm $H^{1/2}(\\partial \\Omega)$, the\ncorrect trace space associated with $H^1(\\Omega)$. We reduce computational cost\nby computing only the theoretically essential portion of the semi-norm and\nenhance convergence stability by avoiding denominator evaluations in the\ndiscretization. By incorporating the exact $H^{1/2}(\\partial \\Omega)$ norm, we\nshow that the approximation converges to the true solution in the\n$H^{1}(\\Omega)$ sense, and, through Neural Tangent Kernel (NTK) analysis, we\ndemonstrate that TRPINN can converge faster than standard PINNs. Numerical\nexperiments on the Laplace equation with highly oscillatory Dirichlet boundary\nconditions exhibit cases where TRPINN succeeds even when standard PINNs fail,\nand show performance improvements of one to three decimal digits."}
{"id": "2510.16820", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16820", "abs": "https://arxiv.org/abs/2510.16820", "authors": ["Thomas Dooms", "Ward Gauderis"], "title": "Finding Manifolds With Bilinear Autoencoders", "comment": null, "summary": "Sparse autoencoders are a standard tool for uncovering interpretable latent\nrepresentations in neural networks. Yet, their interpretation depends on the\ninputs, making their isolated study incomplete. Polynomials offer a solution;\nthey serve as algebraic primitives that can be analysed without reference to\ninput and can describe structures ranging from linear concepts to complicated\nmanifolds. This work uses bilinear autoencoders to efficiently decompose\nrepresentations into quadratic polynomials. We discuss improvements that induce\nimportance ordering, clustering, and activation sparsity. This is an initial\nstep toward nonlinear yet analysable latents through their algebraic\nproperties."}
{"id": "2510.16824", "categories": ["cs.LG", "q-bio.MN"], "pdf": "https://arxiv.org/pdf/2510.16824", "abs": "https://arxiv.org/abs/2510.16824", "authors": ["Yingxu Wang", "Kunyu Zhang", "Jiaxin Huang", "Nan Yin", "Siwei Liu", "Eran Segal"], "title": "ProtoMol: Enhancing Molecular Property Prediction via Prototype-Guided Multimodal Learning", "comment": null, "summary": "Multimodal molecular representation learning, which jointly models molecular\ngraphs and their textual descriptions, enhances predictive accuracy and\ninterpretability by enabling more robust and reliable predictions of drug\ntoxicity, bioactivity, and physicochemical properties through the integration\nof structural and semantic information. However, existing multimodal methods\nsuffer from two key limitations: (1) they typically perform cross-modal\ninteraction only at the final encoder layer, thus overlooking hierarchical\nsemantic dependencies; (2) they lack a unified prototype space for robust\nalignment between modalities. To address these limitations, we propose\nProtoMol, a prototype-guided multimodal framework that enables fine-grained\nintegration and consistent semantic alignment between molecular graphs and\ntextual descriptions. ProtoMol incorporates dual-branch hierarchical encoders,\nutilizing Graph Neural Networks to process structured molecular graphs and\nTransformers to encode unstructured texts, resulting in comprehensive\nlayer-wise representations. Then, ProtoMol introduces a layer-wise\nbidirectional cross-modal attention mechanism that progressively aligns\nsemantic features across layers. Furthermore, a shared prototype space with\nlearnable, class-specific anchors is constructed to guide both modalities\ntoward coherent and discriminative representations. Extensive experiments on\nmultiple benchmark datasets demonstrate that ProtoMol consistently outperforms\nstate-of-the-art baselines across a variety of molecular property prediction\ntasks."}
{"id": "2510.16857", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16857", "abs": "https://arxiv.org/abs/2510.16857", "authors": ["Jiyan Qiu", "Lyulin Kuang", "Guan Wang", "Yichen Xu", "Leiyao Cui", "Shaotong Fu", "Yixin Zhu", "Ruihua Zhang"], "title": "DrivAerStar: An Industrial-Grade CFD Dataset for Vehicle Aerodynamic Optimization", "comment": null, "summary": "Vehicle aerodynamics optimization has become critical for automotive\nelectrification, where drag reduction directly determines electric vehicle\nrange and energy efficiency. Traditional approaches face an intractable\ntrade-off: computationally expensive Computational Fluid Dynamics (CFD)\nsimulations requiring weeks per design iteration, or simplified models that\nsacrifice production-grade accuracy. While machine learning offers\ntransformative potential, existing datasets exhibit fundamental limitations --\ninadequate mesh resolution, missing vehicle components, and validation errors\nexceeding 5% -- preventing deployment in industrial workflows. We present\nDrivAerStar, comprising 12,000 industrial-grade automotive CFD simulations\ngenerated using $\\text{STAR-CCM+}^\\unicode{xAE}$ software. The dataset\nsystematically explores three vehicle configurations through 20 Computer Aided\nDesign (CAD) parameters via Free Form Deformation (FFD) algorithms, including\ncomplete engine compartments and cooling systems with realistic internal\nairflow. DrivAerStar achieves wind tunnel validation accuracy below 1.04% -- a\nfive-fold improvement over existing datasets -- through refined mesh strategies\nwith strict wall $y^+$ control. Benchmarks demonstrate that models trained on\nthis data achieve production-ready accuracy while reducing computational costs\nfrom weeks to minutes. This represents the first dataset bridging academic\nmachine learning research and industrial CFD practice, establishing a new\nstandard for data-driven aerodynamic optimization in automotive development.\nBeyond automotive applications, DrivAerStar demonstrates a paradigm for\nintegrating high-fidelity physics simulations with Artificial Intelligence (AI)\nacross engineering disciplines where computational constraints currently limit\ninnovation."}
{"id": "2510.16877", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16877", "abs": "https://arxiv.org/abs/2510.16877", "authors": ["Heming Zou", "Yunliang Zang", "Wutong Xu", "Xiangyang Ji"], "title": "Fly-CL: A Fly-Inspired Framework for Enhancing Efficient Decorrelation and Reduced Training Time in Pre-trained Model-based Continual Representation Learning", "comment": null, "summary": "Using a nearly-frozen pretrained model, the continual representation learning\nparadigm reframes parameter updates as a similarity-matching problem to\nmitigate catastrophic forgetting. However, directly leveraging pretrained\nfeatures for downstream tasks often suffers from multicollinearity in the\nsimilarity-matching stage, and more advanced methods can be computationally\nprohibitive for real-time, low-latency applications. Inspired by the fly\nolfactory circuit, we propose Fly-CL, a bio-inspired framework compatible with\na wide range of pretrained backbones. Fly-CL substantially reduces training\ntime while achieving performance comparable to or exceeding that of current\nstate-of-the-art methods. We theoretically show how Fly-CL progressively\nresolves multicollinearity, enabling more effective similarity matching with\nlow time complexity. Extensive simulation experiments across diverse network\narchitectures and data regimes validate Fly-CL's effectiveness in addressing\nthis challenge through a biologically inspired design. Code is available at\nhttps://github.com/gfyddha/Fly-CL."}
{"id": "2510.16882", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16882", "abs": "https://arxiv.org/abs/2510.16882", "authors": ["Heming Zou", "Yixiu Mao", "Yun Qu", "Qi Wang", "Xiangyang Ji"], "title": "Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning", "comment": null, "summary": "Supervised fine-tuning (SFT) is a commonly used technique to adapt large\nlanguage models (LLMs) to downstream tasks. In practice, SFT on a full dataset\nis computationally expensive and sometimes suffers from overfitting or bias\namplification. This facilitates the rise of data curation in SFT, which\nprioritizes the most valuable data to optimze. This work studies the online\nbatch selection family that dynamically scores and filters samples during the\ntraining process. However, existing popular methods often (i) rely merely on\nthe utility of data to select a subset while neglecting other crucial factors\nlike diversity, (ii) rely on external resources such as reference models or\nvalidation sets, and (iii) incur extra training time over full-dataset\ntraining. To address these limitations, this work develops \\textbf{UDS\n(Utility-Diversity Sampling)}, a framework for efficient online batch selection\nin SFT. UDS leverages the nuclear norm of the logits matrix to capture both\ndata utility and intra-sample diversity, while estimating inter-sample\ndiversity through efficient low-dimensional embedding comparisons with a\nlightweight memory buffer of historical samples. Such a design eliminates the\nneed for external resources and unnecessary backpropagation, securing\ncomputational efficiency. Experiments on multiple benchmarks demonstrate that\nUDS consistently outperforms state-of-the-art online batch selection methods\nunder varying data budgets, and significantly reduces training time compared to\nfull-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS."}
{"id": "2510.16885", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16885", "abs": "https://arxiv.org/abs/2510.16885", "authors": ["Duo Wang", "Yuan Zuo", "Guangyue Lu", "Junjie Wu"], "title": "UniGTE: Unified Graph-Text Encoding for Zero-Shot Generalization across Graph Tasks and Domains", "comment": null, "summary": "Generalizing to unseen graph tasks without task-specific supervision is\nchallenging: conventional graph neural networks are typically tied to a fixed\nlabel space, while large language models (LLMs) struggle to capture graph\nstructure. We introduce UniGTE, an instruction-tuned encoder-decoder framework\nthat unifies structural and semantic reasoning. The encoder augments a\npretrained autoregressive LLM with learnable alignment tokens and a\nstructure-aware graph-text attention mechanism, enabling it to attend jointly\nto a tokenized graph and a natural-language task prompt while remaining\npermutation-invariant to node order. This yields compact, task-aware graph\nrepresentations. Conditioned solely on these representations, a frozen LLM\ndecoder predicts and reconstructs: it outputs the task answer and\nsimultaneously paraphrases the input graph in natural language. The\nreconstruction objective regularizes the encoder to preserve structural cues.\nUniGTE is instruction-tuned on five datasets spanning node-level, edge-level,\nand graph-level tasks across diverse domains, yet requires no fine-tuning at\ninference. It achieves new state-of-the-art zero-shot results on node\nclassification, link prediction, graph classification, and graph regression\nunder cross-task and cross-domain settings, demonstrating that tight\nintegration of graph structure with LLM semantics enables robust, transferable\ngraph reasoning."}
{"id": "2510.16897", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16897", "abs": "https://arxiv.org/abs/2510.16897", "authors": ["Jose Siguenza", "Bharath Ramsundar"], "title": "DeepChem Equivariant: SE(3)-Equivariant Support in an Open-Source Molecular Machine Learning Library", "comment": "Presented at Machine Learning Symposium - BayLearn (2025)", "summary": "Neural networks that incorporate geometric relationships respecting SE(3)\ngroup transformations (e.g. rotations and translations) are increasingly\nimportant in molecular applications, such as molecular property prediction,\nprotein structure modeling, and materials design. These models, known as\nSE(3)-equivariant neural networks, ensure outputs transform predictably with\ninput coordinate changes by explicitly encoding spatial atomic positions.\nAlthough libraries such as E3NN [4] and SE(3)-TRANSFORMER [3 ] offer powerful\nimplementations, they often require substantial deep learning or mathematical\nprior knowledge and lack complete training pipelines. We extend DEEPCHEM [ 13]\nwith support for ready-to-use equivariant models, enabling scientists with\nminimal deep learning background to build, train, and evaluate models, such as\nSE(3)-Transformer and Tensor Field Networks. Our implementation includes\nequivariant models, complete training pipelines, and a toolkit of equivariant\nutilities, supported with comprehensive tests and documentation, to facilitate\nboth application and further development of SE(3)-equivariant models."}
{"id": "2510.16898", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16898", "abs": "https://arxiv.org/abs/2510.16898", "authors": ["Salih Salihoglu", "Ibrahim Ahmed", "Afshin Asadi"], "title": "Adaptive Online Learning with LSTM Networks for Energy Price Prediction", "comment": null, "summary": "Accurate prediction of electricity prices is crucial for stakeholders in the\nenergy market, particularly for grid operators, energy producers, and\nconsumers. This study focuses on developing a predictive model leveraging Long\nShort-Term Memory (LSTM) networks to forecast day-ahead electricity prices in\nthe California energy market. The model incorporates a variety of features,\nincluding historical price data, weather conditions, and the energy generation\nmix. A novel custom loss function that integrates Mean Absolute Error (MAE),\nJensen-Shannon Divergence (JSD), and a smoothness penalty is introduced to\nenhance the prediction accuracy and interpretability. Additionally, an online\nlearning approach is implemented to allow the model to adapt to new data\nincrementally, ensuring continuous relevance and accuracy. The results\ndemonstrate that the custom loss function can improve the model's performance,\naligning predicted prices more closely with actual values, particularly during\npeak intervals. Also, the online learning model outperforms other models by\neffectively incorporating real-time data, resulting in lower prediction error\nand variability. The inclusion of the energy generation mix further enhances\nthe model's predictive capabilities, highlighting the importance of\ncomprehensive feature integration. This research provides a robust framework\nfor electricity price forecasting, offering valuable insights and tools for\nbetter decision-making in dynamic electricity markets."}
{"id": "2510.16899", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16899", "abs": "https://arxiv.org/abs/2510.16899", "authors": ["Dun Liu", "Qin Pang", "Guangai Liu", "Hongyu Mou", "Jipeng Fan", "Yiming Miao", "Pin-Han Ho", "Limei Peng"], "title": "SNOMED CT-powered Knowledge Graphs for Structured Clinical Data and Diagnostic Reasoning", "comment": null, "summary": "The effectiveness of artificial intelligence (AI) in healthcare is\nsignificantly hindered by unstructured clinical documentation, which results in\nnoisy, inconsistent, and logically fragmented training data. To address this\nchallenge, we present a knowledge-driven framework that integrates the\nstandardized clinical terminology SNOMED CT with the Neo4j graph database to\nconstruct a structured medical knowledge graph. In this graph, clinical\nentities such as diseases, symptoms, and medications are represented as nodes,\nand semantic relationships such as ``caused by,'' ``treats,'' and ``belongs\nto'' are modeled as edges in Neo4j, with types mapped from formal SNOMED CT\nrelationship concepts (e.g., \\texttt{Causative agent}, \\texttt{Indicated for}).\nThis design enables multi-hop reasoning and ensures terminological consistency.\nBy extracting and standardizing entity-relationship pairs from clinical texts,\nwe generate structured, JSON-formatted datasets that embed explicit diagnostic\npathways. These datasets are used to fine-tune large language models (LLMs),\nsignificantly improving the clinical logic consistency of their outputs.\nExperimental results demonstrate that our knowledge-guided approach enhances\nthe validity and interpretability of AI-generated diagnostic reasoning,\nproviding a scalable solution for building reliable AI-assisted clinical\nsystems."}
{"id": "2510.16911", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16911", "abs": "https://arxiv.org/abs/2510.16911", "authors": ["Sarah Al-Shareeda", "Gulcihan Ozdemir", "Heung Seok Jeon", "Khaleel Ahmad"], "title": "A Lightweight DL Model for Smart Grid Power Forecasting with Feature and Resolution Mismatch", "comment": "5 pages, 3 figures, The IEEE PES ISGT Middle East 2025 (ISGT-ME 2025)\n  November 23-26th 2025, Dubai, UAE", "summary": "How can short-term energy consumption be accurately forecasted when sensor\ndata is noisy, incomplete, and lacks contextual richness? This question guided\nour participation in the \\textit{2025 Competition on Electric Energy\nConsumption Forecast Adopting Multi-criteria Performance Metrics}, which\nchallenged teams to predict next-day power demand using real-world\nhigh-frequency data. We proposed a robust yet lightweight Deep Learning (DL)\npipeline combining hourly downsizing, dual-mode imputation (mean and polynomial\nregression), and comprehensive normalization, ultimately selecting Standard\nScaling for optimal balance. The lightweight GRU-LSTM sequence-to-one model\nachieves an average RMSE of 601.9~W, MAE of 468.9~W, and 84.36\\% accuracy.\nDespite asymmetric inputs and imputed gaps, it generalized well, captured\nnonlinear demand patterns, and maintained low inference latency. Notably,\nspatiotemporal heatmap analysis reveals a strong alignment between temperature\ntrends and predicted consumption, further reinforcing the model's reliability.\nThese results demonstrate that targeted preprocessing paired with compact\nrecurrent architectures can still enable fast, accurate, and deployment-ready\nenergy forecasting in real-world conditions."}
{"id": "2510.16914", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16914", "abs": "https://arxiv.org/abs/2510.16914", "authors": ["Hongwei Yan", "Guanglong Sun", "Zhiqi Kang", "Yi Zhong", "Liyuan Wang"], "title": "Domain Generalizable Continual Learning", "comment": "25 pages", "summary": "To adapt effectively to dynamic real-world environments, intelligent systems\nmust continually acquire new skills while generalizing them to diverse, unseen\nscenarios. Here, we introduce a novel and realistic setting named domain\ngeneralizable continual learning (DGCL): a model learns sequential tasks with\neach involving a single domain, aiming to perform well across all encountered\ntasks and domains. This setting poses unique challenges in acquiring,\nretaining, and leveraging both semantic- and domain-relevant information for\nrobust generalization. Although state-of-the-art continual learning (CL)\nmethods have employed pre-trained models (PTMs) to enhance task-specific\ngeneralization, they typically assume identical training and testing domains\nfor each task and therefore perform poorly in DGCL. To this end, we propose\nadaptive Domain Transformation (DoT), an innovative PTMs-based approach\ntailored to DGCL. Inspired by the distributed-plus-hub theory of the human\nbrain, DoT disentangles semantic- and domain-relevant information in\nrepresentation learning, and adaptively transforms task representations across\nvarious domains for output alignment, ensuring balanced and generalized\npredictions. DoT serves as a plug-in strategy that greatly facilitates\nstate-of-the-art CL baselines under both full parameter tuning and\nparameter-efficient tuning paradigms in DGCL, validated by extensive\nexperiments. Also, DoT is shown to accumulate domain-generalizable knowledge\nfrom DGCL, and ensure resource efficiency with a lightweight implementation."}
{"id": "2510.16916", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16916", "abs": "https://arxiv.org/abs/2510.16916", "authors": ["Dong Li", "Xujiang Zhao", "Linlin Yu", "Yanchi Liu", "Wei Cheng", "Zhengzhang Chen", "Zhong Chen", "Feng Chen", "Chen Zhao", "Haifeng Chen"], "title": "SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search", "comment": "NeurIPS 2025", "summary": "Large Language Models (LLMs) offer promising capabilities for tackling\ncomplex reasoning tasks, including optimization problems. However, existing\nmethods either rely on prompt engineering, which leads to poor generalization\nacross problem types, or require costly supervised training. We introduce\nSolverLLM, a training-free framework that leverages test-time scaling to solve\ndiverse optimization problems. Rather than solving directly, SolverLLM\ngenerates mathematical formulations and translates them into solver-ready code,\nguided by a novel Monte Carlo Tree Search (MCTS) strategy. To enhance the\nsearch process, we modify classical MCTS with (1) dynamic expansion for\nadaptive formulation generation, (2) prompt backpropagation to guide\nexploration via outcome-driven feedback, and (3) uncertainty backpropagation to\nincorporate reward reliability into decision-making. Experiments on six\nstandard benchmark datasets demonstrate that SolverLLM outperforms both\nprompt-based and learning-based baselines, achieving strong generalization\nwithout additional training."}
{"id": "2510.16927", "categories": ["cs.LG", "I.2.6; I.2.7; G.1.3"], "pdf": "https://arxiv.org/pdf/2510.16927", "abs": "https://arxiv.org/abs/2510.16927", "authors": ["Egor Petrov", "Nikita Kiselev", "Vladislav Meshkov", "Andrey Grabovoy"], "title": "Closing the Curvature Gap: Full Transformer Hessians and Their Implications for Scaling Laws", "comment": "38 pages, 12 figures. Submitted to ICLR 2026", "summary": "The lack of theoretical results for Layer Normalization and feedforward\nHessians has left a gap in the study of Transformer optimization landscapes. We\naddress this by deriving explicit second-order expressions for these\ncomponents, thereby completing the Hessian characterization of full Transformer\nblocks. Our results generalize prior self-attention analyses and yield\nestimations for the role of each sublayer in curvature propagation. We\ndemonstrate how these Hessian structures inform both convergence dynamics and\nthe empirical scaling laws governing large-model performance. Further, we\npropose a Taylor-expansion-based framework for analyzing loss differences to\nquantify convergence trajectories. By extending Hessian theory to the full\nTransformer architecture, this work establishes a new foundation for\ntheoretical and empirical investigations of optimization in large-scale deep\nlearning."}
{"id": "2510.16940", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.16940", "abs": "https://arxiv.org/abs/2510.16940", "authors": ["Cristian J. Vaca-Rubio", "Roberto Pereira", "Luis Blanco", "Engin Zeydan", "Màrius Caus"], "title": "A Primer on Kolmogorov-Arnold Networks (KANs) for Probabilistic Time Series Forecasting", "comment": null, "summary": "This work introduces Probabilistic Kolmogorov-Arnold Network (P-KAN), a novel\nprobabilistic extension of Kolmogorov-Arnold Networks (KANs) for time series\nforecasting. By replacing scalar weights with spline-based functional\nconnections and directly parameterizing predictive distributions, P-KANs offer\nexpressive yet parameter-efficient models capable of capturing nonlinear and\nheavy-tailed dynamics. We evaluate P-KANs on satellite traffic forecasting,\nwhere uncertainty-aware predictions enable dynamic thresholding for resource\nallocation. Results show that P-KANs consistently outperform Multi Layer\nPerceptron (MLP) baselines in both accuracy and calibration, achieving superior\nefficiency-risk trade-offs while using significantly fewer parameters. We build\nup P-KANs on two distributions, namely Gaussian and Student-t distributions.\nThe Gaussian variant provides robust, conservative forecasts suitable for\nsafety-critical scenarios, whereas the Student-t variant yields sharper\ndistributions that improve efficiency under stable demand. These findings\nestablish P-KANs as a powerful framework for probabilistic forecasting with\ndirect applicability to satellite communications and other resource-constrained\ndomains."}
{"id": "2510.16943", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16943", "abs": "https://arxiv.org/abs/2510.16943", "authors": ["Dania Refai", "Moataz Ahmed"], "title": "Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling through Component-Level Evaluation", "comment": null, "summary": "Large language models (LLMs) are increasingly used to convert natural\nlanguage descriptions into mathematical optimization formulations. Current\nevaluations often treat formulations as a whole, relying on coarse metrics like\nsolution accuracy or runtime, which obscure structural or numerical errors. In\nthis study, we present a comprehensive, component-level evaluation framework\nfor LLM-generated formulations. Beyond the conventional optimality gap, our\nframework introduces metrics such as precision and recall for decision\nvariables and constraints, constraint and objective root mean squared error\n(RMSE), and efficiency indicators based on token usage and latency. We evaluate\nGPT-5, LLaMA 3.1 Instruct, and DeepSeek Math across optimization problems of\nvarying complexity under six prompting strategies. Results show that GPT-5\nconsistently outperforms other models, with chain-of-thought, self-consistency,\nand modular prompting proving most effective. Analysis indicates that solver\nperformance depends primarily on high constraint recall and low constraint\nRMSE, which together ensure structural correctness and solution reliability.\nConstraint precision and decision variable metrics play secondary roles, while\nconcise outputs enhance computational efficiency. These findings highlight\nthree principles for NLP-to-optimization modeling: (i) Complete constraint\ncoverage prevents violations, (ii) minimizing constraint RMSE ensures\nsolver-level accuracy, and (iii) concise outputs improve computational\nefficiency. The proposed framework establishes a foundation for fine-grained,\ndiagnostic evaluation of LLMs in optimization modeling."}
{"id": "2510.16958", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16958", "abs": "https://arxiv.org/abs/2510.16958", "authors": ["Ganglin Tian", "Anastase Alexandre Charantonis", "Camille Le Coz", "Alexis Tantet", "Riwal Plougonven"], "title": "Quantile Regression, Variational Autoencoders, and Diffusion Models for Uncertainty Quantification: A Spatial Analysis of Sub-seasonal Wind Speed Prediction", "comment": "This Work has been submitted to Monthly Weather Review. Copyright in\n  this Work may be transferred without further notice", "summary": "This study aims to improve the spatial representation of uncertainties when\nregressing surface wind speeds from large-scale atmospheric predictors for\nsub-seasonal forecasting. Sub-seasonal forecasting often relies on large-scale\natmospheric predictors such as 500 hPa geopotential height (Z500), which\nexhibit higher predictability than surface variables and can be downscaled to\nobtain more localised information. Previous work by Tian et al. (2024)\ndemonstrated that stochastic perturbations based on model residuals can improve\nensemble dispersion representation in statistical downscaling frameworks, but\nthis method fails to represent spatial correlations and physical consistency\nadequately. More sophisticated approaches are needed to capture the complex\nrelationships between large-scale predictors and local-scale predictands while\nmaintaining physical consistency. Probabilistic deep learning models offer\npromising solutions for capturing complex spatial dependencies. This study\nevaluates three probabilistic methods with distinct uncertainty quantification\nmechanisms: Quantile Regression Neural Network that directly models\ndistribution quantiles, Variational Autoencoders that leverage latent space\nsampling, and Diffusion Models that utilise iterative denoising. These models\nare trained on ERA5 reanalysis data and applied to ECMWF sub-seasonal hindcasts\nto regress probabilistic wind speed ensembles. Our results show that\nprobabilistic downscaling approaches provide more realistic spatial uncertainty\nrepresentations compared to simpler stochastic methods, with each probabilistic\nmodel offering different strengths in terms of ensemble dispersion,\ndeterministic skill, and physical consistency. These findings establish\nprobabilistic downscaling as an effective enhancement to operational\nsub-seasonal wind forecasts for renewable energy planning and risk assessment."}
{"id": "2510.16968", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16968", "abs": "https://arxiv.org/abs/2510.16968", "authors": ["Pingzhi Li", "Morris Yu-Chao Huang", "Zhen Tan", "Qingquan Song", "Jie Peng", "Kai Zou", "Yu Cheng", "Kaidi Xu", "Tianlong Chen"], "title": "Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert Signatures", "comment": "Code is at https://github.com/unites-lab/shadow-moe", "summary": "Knowledge Distillation (KD) accelerates training of large language models\n(LLMs) but poses intellectual property protection and LLM diversity risks.\nExisting KD detection methods based on self-identity or output similarity can\nbe easily evaded through prompt engineering. We present a KD detection\nframework effective in both white-box and black-box settings by exploiting an\noverlooked signal: the transfer of MoE \"structural habits\", especially internal\nrouting patterns. Our approach analyzes how different experts specialize and\ncollaborate across various inputs, creating distinctive fingerprints that\npersist through the distillation process. To extend beyond the white-box setup\nand MoE architectures, we further propose Shadow-MoE, a black-box method that\nconstructs proxy MoE representations via auxiliary distillation to compare\nthese patterns between arbitrary model pairs. We establish a comprehensive,\nreproducible benchmark that offers diverse distilled checkpoints and an\nextensible framework to facilitate future research. Extensive experiments\ndemonstrate >94% detection accuracy across various scenarios and strong\nrobustness to prompt-based evasion, outperforming existing baselines while\nhighlighting the structural habits transfer in LLMs."}
{"id": "2510.16974", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16974", "abs": "https://arxiv.org/abs/2510.16974", "authors": ["Shurong Lin", "Aleksandra Slavković", "Deekshith Reddy Bhoomireddy"], "title": "Differentially Private Linear Regression and Synthetic Data Generation with Statistical Guarantees", "comment": null, "summary": "In social sciences, small- to medium-scale datasets are common and linear\nregression (LR) is canonical. In privacy-aware settings, much work has focused\non differentially private (DP) LR, but mostly on point estimation with limited\nattention to uncertainty quantification. Meanwhile, synthetic data generation\n(SDG) is increasingly important for reproducibility studies, yet current DP LR\nmethods do not readily support it. Mainstream SDG approaches are either\ntailored to discretized data, making them less suitable for continuous\nregression, or rely on deep models that require large datasets, limiting their\nuse for the smaller, continuous data typical in social science. We propose a\nmethod for LR with valid inference under Gaussian DP: a DP bias-corrected\nestimator with asymptotic confidence intervals (CIs) and a general SDG\nprocedure in which regression on the synthetic data matches our DP regression.\nOur binning-aggregation strategy is effective in small- to moderate-dimensional\nsettings. Experiments show our method (1) improves accuracy over existing\nmethods, (2) provides valid CIs, and (3) produces more reliable synthetic data\nfor downstream ML tasks than current DP SDGs."}
{"id": "2510.16980", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16980", "abs": "https://arxiv.org/abs/2510.16980", "authors": ["Kanghui Ning", "Zijie Pan", "Yushan Jiang", "Anderson Schneider", "Yuriy Nevmyvaka", "Dongjin Song"], "title": "Towards Interpretable and Trustworthy Time Series Reasoning: A BlueSky Vision", "comment": null, "summary": "Time series reasoning is emerging as the next frontier in temporal analysis,\naiming to move beyond pattern recognition towards explicit, interpretable, and\ntrustworthy inference. This paper presents a BlueSky vision built on two\ncomplementary directions. One builds robust foundations for time series\nreasoning, centered on comprehensive temporal understanding, structured\nmulti-step reasoning, and faithful evaluation frameworks. The other advances\nsystem-level reasoning, moving beyond language-only explanations by\nincorporating multi-agent collaboration, multi-modal context, and\nretrieval-augmented approaches. Together, these directions outline a flexible\nand extensible framework for advancing time series reasoning, aiming to deliver\ninterpretable and trustworthy temporal intelligence across diverse domains."}
{"id": "2510.16981", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.16981", "abs": "https://arxiv.org/abs/2510.16981", "authors": ["Ahmed Khaled", "Kaan Ozkara", "Tao Yu", "Mingyi Hong", "Youngsuk Park"], "title": "MuonBP: Faster Muon via Block-Periodic Orthogonalization", "comment": null, "summary": "Gradient orthogonalization is a simple strategy that shows great utility in\nspeeding up gradient descent. The Muon optimizer (Jordan, Jin, et al., 2024)\ncombines gradient orthogonalization with first-order momentum and achieves\nsignificant improvement in data efficiency over Adam/AdamW (Loshchilov and\nHutter, 2019) for language model training. However, when using model\nparallelism, gradient orthogonalization introduces additional overhead compared\nto coordinate-wise optimizers (such as AdamW) due to additional gather and\nscatter operations on gradient matrix shards from different devices. This\nadditional communication can amount to a throughput hit of 5%-10% compared to\nAdam/AdamW. To remedy this, we propose Muon with Block-Periodic\nOrthogonalization (MuonBP), which applies orthogonalization independently to\nmatrix shards on each device and periodically performs full orthogonalization\nto maintain training stability at scale. We show how to adjust the learning\nrate from the baseline to MuonBP and give convergence guarantees for this\nalgorithm. Crucially, our theory dictates that we use two stepsizes: one for\nthe blockwise orthogonalization steps, and one for the full orthogonalization\nsteps. Our method is simple, requires minimal hyperparameter adjustments, and\nachieves competitive iteration complexity compared with baseline Muon while\nproviding per-iteration throughput comparable to coordinate-wise methods such\nas AdamW. When training an 8B model with eight-way tensor parallelism and ZeRO\noptimizer state sharding, MuonBP achieves 8% throughput increase compared to\nMuon with no degradation in performance."}
{"id": "2510.16990", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16990", "abs": "https://arxiv.org/abs/2510.16990", "authors": ["Xuying Ning", "Dongqi Fu", "Tianxin Wei", "Wujiang Xu", "Jingrui He"], "title": "Graph4MM: Weaving Multimodal Learning with Structural Information", "comment": "ICML 2025", "summary": "Real-world multimodal data usually exhibit complex structural relationships\nbeyond traditional one-to-one mappings like image-caption pairs. Entities\nacross modalities interact in intricate ways, with images and text forming\ndiverse interconnections through contextual dependencies and co-references.\nGraphs provide powerful structural information for modeling intra-modal and\ninter-modal relationships. However, previous works fail to distinguish\nmulti-hop neighbors and treat the graph as a standalone modality, which\nfragments the overall understanding. This limitation presents two key\nchallenges in multimodal learning: (1) integrating structural information from\nmulti-hop neighbors into foundational models, and (2) fusing modality-specific\ninformation in a principled manner. To address these challenges, we revisit the\nrole of graphs in multimodal learning within the era of foundation models and\npropose Graph4MM, a graph-based multimodal learning framework. To be specific,\nwe introduce Hop-Diffused Attention, which integrates multi-hop structural\ninformation into self-attention through causal masking and hop diffusion.\nFurthermore, we design MM-QFormer, a multi-mapping querying transformer for\ncross-modal fusion. Through theoretical and empirical analysis, we show that\nleveraging structures to integrate both intra- and inter-modal interactions\nimproves multimodal understanding beyond treating them as a standalone\nmodality. Experiments on both generative and discriminative tasks show that\nGraph4MM outperforms larger VLMs, LLMs, and multimodal graph baselines,\nachieving a 6.93% average improvement."}
{"id": "2510.17002", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17002", "abs": "https://arxiv.org/abs/2510.17002", "authors": ["Chang Liu", "Danial Chitnis"], "title": "EEschematic: Multimodal-LLM Based AI Agent for Schematic Generation of Analog Circuit", "comment": null, "summary": "Circuit schematics play a crucial role in analog integrated circuit design,\nserving as the primary medium for human understanding and verification of\ncircuit functionality. While recent large language model (LLM)-based approaches\nhave shown promise in circuit topology generation and device sizing, most rely\nsolely on textual representations such as SPICE netlists, which lack visual\ninterpretability for circuit designers. To address this limitation, we propose\nEEschematic, an AI agent for automatic analog schematic generation based on a\nMultimodal Large Language Model (MLLM). EEschematic integrates textual, visual,\nand symbolic modalities to translate SPICE netlists into schematic diagrams\nrepresented in a human-editable format. The framework uses six analog\nsubstructure examples for few-shot placement and a Visual Chain-of-Thought\n(VCoT) strategy to iteratively refine placement and wiring, enhancing schematic\nclarity and symmetry. Experimental results on representative analog circuits,\nincluding a CMOS inverter, a five-transistor operational transconductance\namplifier (5T-OTA), and a telescopic cascode amplifier, demonstrate that\nEEschematic produces schematics with high visual quality and structural\ncorrectness."}
{"id": "2510.17015", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.17015", "abs": "https://arxiv.org/abs/2510.17015", "authors": ["Mingyan Yang", "Guanjie Wang", "Manqi Luo", "Yifei Liu", "Chen Chen", "Han Zhao", "Yu Feng", "Quan Chen", "Minyi Guo"], "title": "Justitia: Fair and Efficient Scheduling for LLM Applications", "comment": null, "summary": "In the era of Large Language Models (LLMs), it has been popular to launch a\nseries of LLM inferences -- we call an LLM application -- to better solve\nreal-world problems. When serving those applications in shared GPU servers, the\nschedulers are expected to attain fast application completions with guaranteed\nworst-case performance. However, mainstream LLM schedulers fail to behave well\nfor LLM applications -- due to head-of-line blocking or over-constrained\nresource allocation. In this paper, we propose to serve LLM applications in a\nfair and also efficient manner. To this end, we design Justitia, a novel\nscheduler with three key techniques. First, given that memory is prevalently a\nbottleneck for mainstream inference frameworks like vLLM, Justitia models the\nservice cost of LLM applications in a memory-centric manner. Meanwhile, it uses\na simple neural network model to conduct light-weight and also accurate demand\nprediction. Moreover, Justitia adopts a virtual-time based fair queuing\nalgorithm to reduce the overall performance with guaranteed worst-case delay.\nWe have implemented Justitia atop vLLM, and experimental results involving\ndiverse LLM applications show that it can substantially enhance the scheduling\nefficiency with fairness preserved."}
{"id": "2510.17021", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17021", "abs": "https://arxiv.org/abs/2510.17021", "authors": ["Bingqi Shang", "Yiwei Chen", "Yihua Zhang", "Bingquan Shen", "Sijia Liu"], "title": "Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning", "comment": null, "summary": "Large language model (LLM) unlearning has become a critical mechanism for\nremoving undesired data, knowledge, or behaviors from pre-trained models while\nretaining their general utility. Yet, with the rise of open-weight LLMs, we\nask: can the unlearning process itself be backdoored, appearing successful\nunder normal conditions yet reverting to pre-unlearned behavior when a hidden\ntrigger is activated? Drawing inspiration from classical backdoor attacks that\nembed triggers into training data to enforce specific behaviors, we investigate\nbackdoor unlearning, where models forget as intended in the clean setting but\nrecover forgotten knowledge when the trigger appears. We show that designing\nsuch attacks presents unique challenges, hinging on where triggers are placed\nand how backdoor training is reinforced. We uncover a strong link between\nbackdoor efficacy and the attention sink phenomenon, i.e., shallow input tokens\nconsistently attract disproportionate attention in LLMs. Our analysis reveals\nthat these attention sinks serve as gateways for backdoor unlearning: placing\ntriggers at sink positions and aligning their attention values markedly\nenhances backdoor persistence. Extensive experiments validate these findings,\nshowing that attention-sink-guided backdoor unlearning reliably restores\nforgotten knowledge in the presence of backdoor triggers, while behaving\nindistinguishably from a normally unlearned model when triggers are absent.\nCode is available at https://github.com/OPTML-Group/Unlearn-Backdoor."}
{"id": "2510.17022", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17022", "abs": "https://arxiv.org/abs/2510.17022", "authors": ["Kevin P. O Keeffe"], "title": "Curiosity-driven RL for symbolic equation solving", "comment": "Accepted at the NeurIPS 2025 MATH-AI Workshop", "summary": "We explore if RL can be useful for symbolic mathematics. Previous work showed\ncontrastive learning can solve linear equations in one variable. We show\nmodel-free PPO \\cite{schulman2017proximal} augmented with curiosity-based\nexploration and graph-based actions can solve nonlinear equations such as those\ninvolving radicals, exponentials, and trig functions. Our work suggests\ncuriosity-based exploration may be useful for general symbolic reasoning tasks."}
{"id": "2510.17036", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17036", "abs": "https://arxiv.org/abs/2510.17036", "authors": ["Nguyen Do", "Bach Ngo", "Youval Kashuv", "Canh V. Pham", "Hanghang Tong", "My T. Thai"], "title": "Hephaestus: Mixture Generative Modeling with Energy Guidance for Large-scale QoS Degradation", "comment": "62 pages, 19 figures, Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "We study the Quality of Service Degradation (QoSD) problem, in which an\nadversary perturbs edge weights to degrade network performance. This setting\narises in both network infrastructures and distributed ML systems, where\ncommunication quality, not just connectivity, determines functionality. While\nclassical methods rely on combinatorial optimization, and recent ML approaches\naddress only restricted linear variants with small-size networks, no prior\nmodel directly tackles the QoSD problem under nonlinear edge-weight functions.\nThis work proposes \\PIMMA, a self-reinforcing generative framework that\nsynthesizes feasible solutions in latent space, to fill this gap. Our method\nincludes three phases: (1) Forge: a Predictive Path-Stressing (PPS) algorithm\nthat uses graph learning and approximation to produce feasible solutions with\nperformance guarantee, (2) Morph: a new theoretically grounded training\nparadigm for Mixture of Conditional VAEs guided by an energy-based model to\ncapture solution feature distributions, and (3) Refine: a reinforcement\nlearning agent that explores this space to generate progressively near-optimal\nsolutions using our designed differentiable reward function. Experiments on\nboth synthetic and real-world networks show that our approach consistently\noutperforms classical and ML baselines, particularly in scenarios with\nnonlinear cost functions where traditional methods fail to generalize."}
{"id": "2510.17040", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17040", "abs": "https://arxiv.org/abs/2510.17040", "authors": ["Hoang-Son Nguyen", "Xiao Fu"], "title": "Diverse Influence Component Analysis: A Geometric Approach to Nonlinear Mixture Identifiability", "comment": "30 pages, 3 figures", "summary": "Latent component identification from unknown nonlinear mixtures is a\nfoundational challenge in machine learning, with applications in tasks such as\ndisentangled representation learning and causal inference. Prior work in\nnonlinear independent component analysis (nICA) has shown that auxiliary\nsignals -- such as weak supervision -- can support identifiability of\nconditionally independent latent components. More recent approaches explore\nstructural assumptions, e.g., sparsity in the Jacobian of the mixing function,\nto relax such requirements. In this work, we introduce Diverse Influence\nComponent Analysis (DICA), a framework that exploits the convex geometry of the\nmixing function's Jacobian. We propose a Jacobian Volume Maximization\n(J-VolMax) criterion, which enables latent component identification by\nencouraging diversity in their influence on the observed variables. Under\nreasonable conditions, this approach achieves identifiability without relying\non auxiliary information, latent component independence, or Jacobian sparsity\nassumptions. These results extend the scope of identifiability analysis and\noffer a complementary perspective to existing methods."}
{"id": "2510.17057", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17057", "abs": "https://arxiv.org/abs/2510.17057", "authors": ["Nikolaus Howe", "Micah Carroll"], "title": "The Ends Justify the Thoughts: RL-Induced Motivated Reasoning in LLMs", "comment": "26 pages", "summary": "The use of reinforcement learning (RL) with chain-of-thought (CoT) reasoning\nhas emerged as a promising approach for developing more capable language\nmodels. In turn, this has led to investigation of CoT monitoring as a\ncompelling method for detecting harmful behaviors such as reward hacking, under\nthe assumption that models' reasoning processes reflect their internal\ndecision-making. In practice, LLM training often produces unintended behaviors\ndue to imperfect reward signals, leading models to develop misaligned\ntendencies. A common corrective approach is to apply post-hoc instructions to\navoid problematic behaviors like sycophancy, but what happens to the model's\nreasoning process when these instructions conflict with learned behaviors? We\ninvestigate this question in simple settings and find that models engage in\nsystematic motivated reasoning -- generating plausible-sounding justifications\nfor violating their instructions while downplaying potential harms. Beyond\nbeing an interesting property of training, we find that while motivated\nreasoning can be detected by most frontier reasoning models, smaller LLM judges\ncan fail to identify a portion of it, and in rare cases can themselves be\npersuaded that the reasoning is correct, despite it contradicting clear\ninstructions. This capability gap raises concerns that as models become more\nsophisticated, their motivated reasoning may become increasingly difficult for\nmonitors to detect. Our results underscore the need to account for motivated\nreasoning when relying on chain-of-thought processes for model evaluation and\noversight. All code for this paper will be made available. WARNING: some\nexamples in this paper may be upsetting."}
{"id": "2510.17058", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17058", "abs": "https://arxiv.org/abs/2510.17058", "authors": ["Hassan Hamad", "Yuou Qiu", "Peter A. Beerel", "Keith M. Chugg"], "title": "Bitwidth-Specific Logarithmic Arithmetic for Future Hardware-Accelerated Training", "comment": null, "summary": "While advancements in quantization have significantly reduced the\ncomputational costs of inference in deep learning, training still predominantly\nrelies on complex floating-point arithmetic. Low-precision fixed-point training\npresents a compelling alternative. This work introduces a novel enhancement in\nlow-precision logarithmic fixed-point training, geared towards future hardware\naccelerator designs. We propose incorporating bitwidth in the design of\napproximations to arithmetic operations. To this end, we introduce a new\nhardware-friendly, piece-wise linear approximation for logarithmic addition.\nUsing simulated annealing, we optimize this approximation at different\nprecision levels. A C++ bit-true simulation demonstrates training of VGG-11 and\nVGG-16 models on CIFAR-100 and TinyImageNet, respectively, using 12-bit integer\narithmetic with minimal accuracy degradation compared to 32-bit floating-point\ntraining. Our hardware study reveals up to 32.5% reduction in area and 53.5%\nreduction in energy consumption for the proposed LNS multiply-accumulate units\ncompared to that of linear fixed-point equivalents."}
{"id": "2510.17059", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17059", "abs": "https://arxiv.org/abs/2510.17059", "authors": ["Kathryn Wantlin", "Chongyi Zheng", "Benjamin Eysenbach"], "title": "Consistent Zero-Shot Imitation with Contrastive Goal Inference", "comment": null, "summary": "In the same way that generative models today conduct most of their training\nin a self-supervised fashion, how can agentic models conduct their training in\na self-supervised fashion, interactively exploring, learning, and preparing to\nquickly adapt to new tasks? A prerequisite for embodied agents deployed in real\nworld interactions ought to be training with interaction, yet today's most\nsuccessful AI models (e.g., VLMs, LLMs) are trained without an explicit notion\nof action. The problem of pure exploration (which assumes no data as input) is\nwell studied in the reinforcement learning literature and provides agents with\na wide array of experiences, yet it fails to prepare them for rapid adaptation\nto new tasks. Today's language and vision models are trained on data provided\nby humans, which provides a strong inductive bias for the sorts of tasks that\nthe model will have to solve (e.g., modeling chords in a song, phrases in a\nsonnet, sentences in a medical record). However, when they are prompted to\nsolve a new task, there is a faulty tacit assumption that humans spend most of\ntheir time in the most rewarding states. The key contribution of our paper is a\nmethod for pre-training interactive agents in a self-supervised fashion, so\nthat they can instantly mimic human demonstrations. Our method treats goals\n(i.e., observations) as the atomic construct. During training, our method\nautomatically proposes goals and practices reaching them, building off prior\nwork in reinforcement learning exploration. During evaluation, our method\nsolves an (amortized) inverse reinforcement learning problem to explain\ndemonstrations as optimal goal-reaching behavior. Experiments on standard\nbenchmarks (not designed for goal-reaching) show that our approach outperforms\nprior methods for zero-shot imitation."}
{"id": "2510.17085", "categories": ["cs.LG", "cs.GT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17085", "abs": "https://arxiv.org/abs/2510.17085", "authors": ["Yiling Chen", "Shi Feng", "Paul Kattuman", "Fang-Yi Yu"], "title": "Data Reliability Scoring", "comment": "39 pages, 5 figures", "summary": "How can we assess the reliability of a dataset without access to ground\ntruth? We introduce the problem of reliability scoring for datasets collected\nfrom potentially strategic sources. The true data are unobserved, but we see\noutcomes of an unknown statistical experiment that depends on them. To\nbenchmark reliability, we define ground-truth-based orderings that capture how\nmuch reported data deviate from the truth. We then propose the Gram determinant\nscore, which measures the volume spanned by vectors describing the empirical\ndistribution of the observed data and experiment outcomes. We show that this\nscore preserves several ground-truth based reliability orderings and, uniquely\nup to scaling, yields the same reliability ranking of datasets regardless of\nthe experiment -- a property we term experiment agnosticism. Experiments on\nsynthetic noise models, CIFAR-10 embeddings, and real employment data\ndemonstrate that the Gram determinant score effectively captures data quality\nacross diverse observation processes."}
{"id": "2510.17088", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.17088", "abs": "https://arxiv.org/abs/2510.17088", "authors": ["Zan Li", "Rui Fan"], "title": "Explainable Heterogeneous Anomaly Detection in Financial Networks via Adaptive Expert Routing", "comment": null, "summary": "Financial anomalies exhibit heterogeneous mechanisms (price shocks, liquidity\nfreezes, contagion cascades, regime shifts), but existing detectors treat all\nanomalies uniformly, producing scalar scores without revealing which mechanism\nis failing, where risks concentrate, or how to intervene. This opacity prevents\ntargeted regulatory responses. Three unsolved challenges persist: (1) static\ngraph structures cannot adapt when market correlations shift during regime\nchanges; (2) uniform detection mechanisms miss type-specific signatures across\nmultiple temporal scales while failing to integrate individual behaviors with\nnetwork contagion; (3) black-box outputs provide no actionable guidance on\nanomaly mechanisms or their temporal evolution.\n  We address these via adaptive graph learning with specialized expert networks\nthat provide built-in interpretability. Our framework captures multi-scale\ntemporal dependencies through BiLSTM with self-attention, fuses temporal and\nspatial information via cross-modal attention, learns dynamic graphs through\nneural multi-source interpolation, adaptively balances learned dynamics with\nstructural priors via stress-modulated fusion, routes anomalies to four\nmechanism-specific experts, and produces dual-level interpretable attributions.\nCritically, interpretability is embedded architecturally rather than applied\npost-hoc.\n  On 100 US equities (2017-2024), we achieve 92.3% detection of 13 major events\nwith 3.8-day lead time, outperforming best baseline by 30.8pp. Silicon Valley\nBank case study demonstrates anomaly evolution tracking: Price-Shock expert\nweight rose to 0.39 (33% above baseline 0.29) during closure, peaking at 0.48\n(66% above baseline) one week later, revealing automatic temporal mechanism\nidentification without labeled supervision."}
{"id": "2510.17099", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.17099", "abs": "https://arxiv.org/abs/2510.17099", "authors": ["Zhiyuan Fan", "Arnab Maiti", "Kevin Jamieson", "Lillian J. Ratliff", "Gabriele Farina"], "title": "On the Universal Near Optimality of Hedge in Combinatorial Settings", "comment": "28 pages, 1 Figure", "summary": "In this paper, we study the classical Hedge algorithm in combinatorial\nsettings. In each round, the learner selects a vector $\\boldsymbol{x}_t$ from a\nset $X \\subseteq \\{0,1\\}^d$, observes a full loss vector $\\boldsymbol{y}_t \\in\n\\mathbb{R}^d$, and incurs a loss $\\langle \\boldsymbol{x}_t, \\boldsymbol{y}_t\n\\rangle \\in [-1,1]$. This setting captures several important problems,\nincluding extensive-form games, resource allocation, $m$-sets, online multitask\nlearning, and shortest-path problems on directed acyclic graphs (DAGs). It is\nwell known that Hedge achieves a regret of $O\\big(\\sqrt{T \\log |X|}\\big)$ after\n$T$ rounds of interaction. In this paper, we ask whether Hedge is optimal\nacross all combinatorial settings. To that end, we show that for any $X\n\\subseteq \\{0,1\\}^d$, Hedge is near-optimal--specifically, up to a $\\sqrt{\\log\nd}$ factor--by establishing a lower bound of $\\Omega\\big(\\sqrt{T \\log(|X|)/\\log\nd}\\big)$ that holds for any algorithm. We then identify a natural class of\ncombinatorial sets--namely, $m$-sets with $\\log d \\leq m \\leq \\sqrt{d}$--for\nwhich this lower bound is tight, and for which Hedge is provably suboptimal by\na factor of exactly $\\sqrt{\\log d}$. At the same time, we show that Hedge is\noptimal for online multitask learning, a generalization of the classical\n$K$-experts problem. Finally, we leverage the near-optimality of Hedge to\nestablish the existence of a near-optimal regularizer for online shortest-path\nproblems in DAGs--a setting that subsumes a broad range of combinatorial\ndomains. Specifically, we show that the classical Online Mirror Descent (OMD)\nalgorithm, when instantiated with the dilated entropy regularizer, is\niterate-equivalent to Hedge, and therefore inherits its near-optimal regret\nguarantees for DAGs."}
{"id": "2510.17103", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17103", "abs": "https://arxiv.org/abs/2510.17103", "authors": ["Shinji Ito", "Kevin Jamieson", "Haipeng Luo", "Arnab Maiti", "Taira Tsuchiya"], "title": "Adapting to Stochastic and Adversarial Losses in Episodic MDPs with Aggregate Bandit Feedback", "comment": "49 pages", "summary": "We study online learning in finite-horizon episodic Markov decision processes\n(MDPs) under the challenging aggregate bandit feedback model, where the learner\nobserves only the cumulative loss incurred in each episode, rather than\nindividual losses at each state-action pair. While prior work in this setting\nhas focused exclusively on worst-case analysis, we initiate the study of\nbest-of-both-worlds (BOBW) algorithms that achieve low regret in both\nstochastic and adversarial environments. We propose the first BOBW algorithms\nfor episodic tabular MDPs with aggregate bandit feedback. In the case of known\ntransitions, our algorithms achieve $O(\\log T)$ regret in stochastic settings\nand ${O}(\\sqrt{T})$ regret in adversarial ones. Importantly, we also establish\nmatching lower bounds, showing the optimality of our algorithms in this\nsetting. We further extend our approach to unknown-transition settings by\nincorporating confidence-based techniques. Our results rely on a combination of\nFTRL over occupancy measures, self-bounding techniques, and new loss estimators\ninspired by recent advances in online shortest path problems. Along the way, we\nalso provide the first individual-gap-dependent lower bounds and demonstrate\nnear-optimal BOBW algorithms for shortest path problems with bandit feedback."}
{"id": "2510.17106", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17106", "abs": "https://arxiv.org/abs/2510.17106", "authors": ["Chen Zhang", "Weixin Bu", "Wendong Xu", "Runsheng Yu", "Yik-Chung Wu", "Ngai Wong"], "title": "Fighter: Unveiling the Graph Convolutional Nature of Transformers in Time Series Modeling", "comment": "Preprint", "summary": "Transformers have achieved remarkable success in time series modeling, yet\ntheir internal mechanisms remain opaque. This work demystifies the Transformer\nencoder by establishing its fundamental equivalence to a Graph Convolutional\nNetwork (GCN). We show that in the forward pass, the attention distribution\nmatrix serves as a dynamic adjacency matrix, and its composition with\nsubsequent transformations performs computations analogous to graph\nconvolution. Moreover, we demonstrate that in the backward pass, the update\ndynamics of value and feed-forward projections mirror those of GCN parameters.\nBuilding on this unified theoretical reinterpretation, we propose\n\\textbf{Fighter} (Flexible Graph Convolutional Transformer), a streamlined\narchitecture that removes redundant linear projections and incorporates\nmulti-hop graph aggregation. This perspective yields an explicit and\ninterpretable representation of temporal dependencies across different scales,\nnaturally expressed as graph edges. Experiments on standard forecasting\nbenchmarks confirm that Fighter achieves competitive performance while\nproviding clearer mechanistic interpretability of its predictions."}
{"id": "2510.17120", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17120", "abs": "https://arxiv.org/abs/2510.17120", "authors": ["Rishi Sonthalia", "Raj Rao Nadakuditi"], "title": "Matricial Free Energy as a Gaussianizing Regularizer: Enhancing Autoencoders for Gaussian Code Generation", "comment": null, "summary": "We introduce a novel regularization scheme for autoencoders based on\nmatricial free energy. Our approach defines a differentiable loss function in\nterms of the singular values of the code matrix (code dimension x batch size).\nFrom the standpoint of free probability an d random matrix theory, this loss\nachieves its minimum when the singular value distribution of the code matrix\ncoincides with that of an appropriately sculpted random metric with i.i.d.\nGaussian entries. Empirical simulations demonstrate that minimizing the\nnegative matricial free energy through standard stochastic gradient-based\ntraining yields Gaussian-like codes that generalize across training and test\nsets. Building on this foundation, we propose a matricidal free energy\nmaximizing autoencoder that reliably produces Gaussian codes and show its\napplication to underdetermined inverse problems."}
{"id": "2510.17122", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.17122", "abs": "https://arxiv.org/abs/2510.17122", "authors": ["Chengxiu Hua", "Jiawen Gu", "Yushun Tang"], "title": "Continuous Q-Score Matching: Diffusion Guided Reinforcement Learning for Continuous-Time Control", "comment": null, "summary": "Reinforcement learning (RL) has achieved significant success across a wide\nrange of domains, however, most existing methods are formulated in discrete\ntime. In this work, we introduce a novel RL method for continuous-time control,\nwhere stochastic differential equations govern state-action dynamics. Departing\nfrom traditional value function-based approaches, our key contribution is the\ncharacterization of continuous-time Q-functions via a martingale condition and\nthe linking of diffusion policy scores to the action gradient of a learned\ncontinuous Q-function by the dynamic programming principle. This insight\nmotivates Continuous Q-Score Matching (CQSM), a score-based policy improvement\nalgorithm. Notably, our method addresses a long-standing challenge in\ncontinuous-time RL: preserving the action-evaluation capability of Q-functions\nwithout relying on time discretization. We further provide theoretical\nclosed-form solutions for linear-quadratic (LQ) control problems within our\nframework. Numerical results in simulated environments demonstrate the\neffectiveness of our proposed method and compare it to popular baselines."}
{"id": "2510.17132", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17132", "abs": "https://arxiv.org/abs/2510.17132", "authors": ["Ioannis Tsaknakis", "Bingqing Song", "Shuyu Gan", "Dongyeop Kang", "Alfredo Garcia", "Gaowen Liu", "Charles Fleming", "Mingyi Hong"], "title": "Do LLMs Recognize Your Latent Preferences? A Benchmark for Latent Information Discovery in Personalized Interaction", "comment": null, "summary": "Large Language Models (LLMs) excel at producing broadly relevant text, but\nthis generality becomes a limitation when user-specific preferences are\nrequired, such as recommending restaurants or planning travel. In these\nscenarios, users rarely articulate every preference explicitly; instead, much\nof what they care about remains latent, waiting to be inferred. This raises a\nfundamental question: Can LLMs uncover and reason about such latent information\nthrough conversation?\n  We address this problem by introducing a unified benchmark for evaluating\nlatent information discovery - the ability of LLMs to reveal and utilize hidden\nuser attributes through multi-turn interaction. The benchmark spans three\nprogressively realistic settings: the classic 20 Questions game, Personalized\nQuestion Answering, and Personalized Text Summarization. All tasks share a\ntri-agent framework (User, Assistant, Judge) enabling turn-level evaluation of\nelicitation and adaptation. Our results reveal that while LLMs can indeed\nsurface latent information through dialogue, their success varies dramatically\nwith context: from 32% to 98%, depending on task complexity, topic, and number\nof hidden attributes. This benchmark provides the first systematic framework\nfor studying latent information discovery in personalized interaction,\nhighlighting that effective preference inference remains an open frontier for\nbuilding truly adaptive AI systems."}
{"id": "2510.17136", "categories": ["cs.LG", "I.2.6; I.5.1; I.5.4"], "pdf": "https://arxiv.org/pdf/2510.17136", "abs": "https://arxiv.org/abs/2510.17136", "authors": ["Enhao Gu", "Haolin Hou"], "title": "In-situ Autoguidance: Eliciting Self-Correction in Diffusion Models", "comment": "6 pages, 3 figures. ICML 2025 Workshop submission", "summary": "The generation of high-quality, diverse, and prompt-aligned images is a\ncentral goal in image-generating diffusion models. The popular classifier-free\nguidance (CFG) approach improves quality and alignment at the cost of reduced\nvariation, creating an inherent entanglement of these effects. Recent work has\nsuccessfully disentangled these properties by guiding a model with a separately\ntrained, inferior counterpart; however, this solution introduces the\nconsiderable overhead of requiring an auxiliary model. We challenge this\nprerequisite by introducing In-situ Autoguidance, a method that elicits\nguidance from the model itself without any auxiliary components. Our approach\ndynamically generates an inferior prediction on the fly using a stochastic\nforward pass, reframing guidance as a form of inference-time self-correction.\nWe demonstrate that this zero-cost approach is not only viable but also\nestablishes a powerful new baseline for cost-efficient guidance, proving that\nthe benefits of self-guidance can be achieved without external models."}
{"id": "2510.17160", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17160", "abs": "https://arxiv.org/abs/2510.17160", "authors": ["Derda Kaymak", "Gyuhak Kim", "Tomoya Kaichi", "Tatsuya Konishi", "Bing Liu"], "title": "Learning After Model Deployment", "comment": "Published at ECAI-2025", "summary": "In classic supervised learning, once a model is deployed in an application,\nit is fixed. No updates will be made to it during the application. This is\ninappropriate for many dynamic and open environments, where unexpected samples\nfrom unseen classes may appear. In such an environment, the model should be\nable to detect these novel samples from unseen classes and learn them after\nthey are labeled. We call this paradigm Autonomous Learning after Model\nDeployment (ALMD). The learning here is continuous and involves no human\nengineers. Labeling in this scenario is performed by human co-workers or other\nknowledgeable agents, which is similar to what humans do when they encounter an\nunfamiliar object and ask another person for its name. In ALMD, the detection\nof novel samples is dynamic and differs from traditional out-of-distribution\n(OOD) detection in that the set of in-distribution (ID) classes expands as new\nclasses are learned during application, whereas ID classes is fixed in\ntraditional OOD detection. Learning is also different from classic supervised\nlearning because in ALMD, we learn the encountered new classes immediately and\nincrementally. It is difficult to retrain the model from scratch using all the\npast data from the ID classes and the novel samples from newly discovered\nclasses, as this would be resource- and time-consuming. Apart from these two\nchallenges, ALMD faces the data scarcity issue because instances of new classes\noften appear sporadically in real-life applications. To address these issues,\nwe propose a novel method, PLDA, which performs dynamic OOD detection and\nincremental learning of new classes on the fly. Empirical evaluations will\ndemonstrate the effectiveness of PLDA."}
{"id": "2510.17162", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17162", "abs": "https://arxiv.org/abs/2510.17162", "authors": ["Guanjie Cheng", "Siyang Liu", "Junqin Huang", "Xinkui Zhao", "Yin Wang", "Mengying Zhu", "Linghe Kong", "Shuiguang Deng"], "title": "ALPINE: A Lightweight and Adaptive Privacy-Decision Agent Framework for Dynamic Edge Crowdsensing", "comment": "12 pages, 8 figures, 4 tables. Submitted to The Web Conference (WWW\n  2026)", "summary": "Mobile edge crowdsensing (MECS) systems continuously generate and transmit\nuser data in dynamic, resource-constrained environments, exposing users to\nsignificant privacy threats. In practice, many privacy-preserving mechanisms\nbuild on differential privacy (DP). However, static DP mechanisms often fail to\nadapt to evolving risks, for example, shifts in adversarial capabilities,\nresource constraints and task requirements, resulting in either excessive noise\nor inadequate protection. To address this challenge, we propose ALPINE, a\nlightweight, adaptive framework that empowers terminal devices to autonomously\nadjust differential privacy levels in real time. ALPINE operates as a\nclosed-loop control system consisting of four modules: dynamic risk perception,\nprivacy decision via twin delayed deep deterministic policy gradient (TD3),\nlocal privacy execution and performance verification from edge nodes. Based on\nenvironmental risk assessments, we design a reward function that balances\nprivacy gains, data utility and energy cost, guiding the TD3 agent to\nadaptively tune noise magnitude across diverse risk scenarios and achieve a\ndynamic equilibrium among privacy, utility and cost. Both the collaborative\nrisk model and pretrained TD3-based agent are designed for low-overhead\ndeployment. Extensive theoretical analysis and real-world simulations\ndemonstrate that ALPINE effectively mitigates inference attacks while\npreserving utility and cost, making it practical for large-scale edge\napplications."}
{"id": "2510.17185", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17185", "abs": "https://arxiv.org/abs/2510.17185", "authors": ["Runlin Lei", "Lu Yi", "Mingguo He", "Pengyu Qiu", "Zhewei Wei", "Yongchao Liu", "Chuntao Hong"], "title": "Robustness in Text-Attributed Graph Learning: Insights, Trade-offs, and New Defenses", "comment": null, "summary": "While Graph Neural Networks (GNNs) and Large Language Models (LLMs) are\npowerful approaches for learning on Text-Attributed Graphs (TAGs), a\ncomprehensive understanding of their robustness remains elusive. Current\nevaluations are fragmented, failing to systematically investigate the distinct\neffects of textual and structural perturbations across diverse models and\nattack scenarios. To address these limitations, we introduce a unified and\ncomprehensive framework to evaluate robustness in TAG learning. Our framework\nevaluates classical GNNs, robust GNNs (RGNNs), and GraphLLMs across ten\ndatasets from four domains, under diverse text-based, structure-based, and\nhybrid perturbations in both poisoning and evasion scenarios. Our extensive\nanalysis reveals multiple findings, among which three are particularly\nnoteworthy: 1) models have inherent robustness trade-offs between text and\nstructure, 2) the performance of GNNs and RGNNs depends heavily on the text\nencoder and attack type, and 3) GraphLLMs are particularly vulnerable to\ntraining data corruption. To overcome the identified trade-offs, we introduce\nSFT-auto, a novel framework that delivers superior and balanced robustness\nagainst both textual and structural attacks within a single model. Our work\nestablishes a foundation for future research on TAG security and offers\npractical solutions for robust TAG learning in adversarial environments. Our\ncode is available at: https://github.com/Leirunlin/TGRB."}
{"id": "2510.17187", "categories": ["cs.LG", "q-bio.BM", "92B20"], "pdf": "https://arxiv.org/pdf/2510.17187", "abs": "https://arxiv.org/abs/2510.17187", "authors": ["Alexander Aghili", "Andy Bruce", "Daniel Sabo", "Sanya Murdeshwar", "Kevin Bachelor", "Ionut Mistreanu", "Ashwin Lokapally", "Razvan Marinescu"], "title": "A Standardized Benchmark for Machine-Learned Molecular Dynamics using Weighted Ensemble Sampling", "comment": "37 Pages (Main Text), 10 Figures, Submitted to Journal of Physical\n  Chemistry B", "summary": "The rapid evolution of molecular dynamics (MD) methods, including\nmachine-learned dynamics, has outpaced the development of standardized tools\nfor method validation. Objective comparison between simulation approaches is\noften hindered by inconsistent evaluation metrics, insufficient sampling of\nrare conformational states, and the absence of reproducible benchmarks. To\naddress these challenges, we introduce a modular benchmarking framework that\nsystematically evaluates protein MD methods using enhanced sampling analysis.\nOur approach uses weighted ensemble (WE) sampling via The Weighted Ensemble\nSimulation Toolkit with Parallelization and Analysis (WESTPA), based on\nprogress coordinates derived from Time-lagged Independent Component Analysis\n(TICA), enabling fast and efficient exploration of protein conformational\nspace. The framework includes a flexible, lightweight propagator interface that\nsupports arbitrary simulation engines, allowing both classical force fields and\nmachine learning-based models. Additionally, the framework offers a\ncomprehensive evaluation suite capable of computing more than 19 different\nmetrics and visualizations across a variety of domains. We further contribute a\ndataset of nine diverse proteins, ranging from 10 to 224 residues, that span a\nvariety of folding complexities and topologies. Each protein has been\nextensively simulated at 300K for one million MD steps per starting point (4\nns). To demonstrate the utility of our framework, we perform validation tests\nusing classic MD simulations with implicit solvent and compare protein\nconformational sampling using a fully trained versus under-trained CGSchNet\nmodel. By standardizing evaluation protocols and enabling direct, reproducible\ncomparisons across MD approaches, our open-source platform lays the groundwork\nfor consistent, rigorous benchmarking across the molecular simulation\ncommunity."}
{"id": "2510.17189", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.17189", "abs": "https://arxiv.org/abs/2510.17189", "authors": ["Wenxun Wang", "Shuchang Zhou", "Wenyu Sun", "Peiqin Sun", "Yongpan Liu"], "title": "SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference", "comment": null, "summary": "Transformers have shown remarkable performance in both natural language\nprocessing (NLP) and computer vision (CV) tasks. However, their real-time\ninference speed and efficiency are limited due to the inefficiency in Softmax\nand Layer Normalization (LayerNorm). Previous works based on function\napproximation suffer from inefficient implementation as they place emphasis on\ncomputation while disregarding memory overhead concerns. Moreover, such methods\nrely on retraining to compensate for approximation error which can be costly\nand inconvenient.\n  In this paper, we present SOLE, a hardware-software co-design for Softmax and\nLayerNorm which is composed of E2Softmax and AILayerNorm. E2Softmax utilizes\nlog2 quantization of exponent function and log-based division to approximate\nSoftmax while AILayerNorm adopts low-precision statistic calculation. Compared\nwith state-of-the-art designs, we achieve both low-precision calculation and\nlow bit-width storage on Softmax and LayerNorm. Experiments show that SOLE\nmaintains inference accuracy without retraining while offering orders of\nmagnitude speedup and energy savings over GPU, achieving 3.04x, 3.86x\nenergy-efficiency improvements and 2.82x, 3.32x area-efficiency improvements\nover prior state-of-the-art custom hardware for Softmax and LayerNorm,\nrespectively."}
{"id": "2510.17206", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17206", "abs": "https://arxiv.org/abs/2510.17206", "authors": ["Michael Hersche", "Samuel Moor-Smith", "Thomas Hofmann", "Abbas Rahimi"], "title": "Soft-Masked Diffusion Language Models", "comment": null, "summary": "Diffusion models have demonstrated strong potential in language modeling,\noffering various advantages over traditional autoregressive approaches. Their\nability to generate and revise entire responses in parallel enables faster\ngeneration and built-in self-correction mechanisms. Most modern diffusion-based\nlanguage models employ masked diffusion, where decoding involves iteratively\nprocessing masked tokens based on a binary decision: either retaining the mask\nor replacing it with the predicted token. However, this binary choice discards\nvaluable predictive information when the mask is retained. To address this\nlimitation, we introduce soft-masking (SM), a novel method that dynamically\nblends the embedding of the mask token with the embeddings of the top-$k$\npredicted tokens from the previous decoding step, for each retained mask. This\nprovides the model with a more informative prior, preserving context from\nearlier computations and allowing partial information about masked tokens to\npropagate beyond a single step. We propose a training methodology that adapts a\npretrained masked diffusion language model to incorporate SM. We demonstrate\nthat continuing pretraining a 169M parameter model with SM leads to improved\nperplexity and MAUVE scores. Furthermore, we finetune two state-of-the-art\ndiffusion models, Dream-7B and Dream-Coder-7B, with SM. SM consistently\nimproves performance across multiple coding benchmarks, particularly in\nhigh-throughput settings."}
{"id": "2510.17212", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17212", "abs": "https://arxiv.org/abs/2510.17212", "authors": ["Jundong Zhang", "Yuhui Situ", "Fanji Zhang", "Rongji Deng", "Tianqi Wei"], "title": "D2C-HRHR: Discrete Actions with Double Distributional Critics for High-Risk-High-Return Tasks", "comment": null, "summary": "Tasks involving high-risk-high-return (HRHR) actions, such as obstacle\ncrossing, often exhibit multimodal action distributions and stochastic returns.\nMost reinforcement learning (RL) methods assume unimodal Gaussian policies and\nrely on scalar-valued critics, which limits their effectiveness in HRHR\nsettings. We formally define HRHR tasks and theoretically show that Gaussian\npolicies cannot guarantee convergence to the optimal solution. To address this,\nwe propose a reinforcement learning framework that (i) discretizes continuous\naction spaces to approximate multimodal distributions, (ii) employs\nentropy-regularized exploration to improve coverage of risky but rewarding\nactions, and (iii) introduces a dual-critic architecture for more accurate\ndiscrete value distribution estimation. The framework scales to\nhigh-dimensional action spaces, supporting complex control domains. Experiments\non locomotion and manipulation benchmarks with high risks of failure\ndemonstrate that our method outperforms baselines, underscoring the importance\nof explicitly modeling multimodality and risk in RL."}
{"id": "2510.17214", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17214", "abs": "https://arxiv.org/abs/2510.17214", "authors": ["Chenyan Fei", "Dalin Zhang", "Chen Melinda Dang"], "title": "Diagnosis of Fuel Cell Health Status with Deep Sparse Auto-Encoder Neural Network", "comment": null, "summary": "Effective and accurate diagnosis of fuel cell health status is crucial for\nensuring the stable operation of fuel cell stacks. Among various parameters,\nhigh-frequency impedance serves as a critical indicator for assessing fuel cell\nstate and health conditions. However, its online testing is prohibitively\ncomplex and costly. This paper employs a deep sparse auto-encoding network for\nthe prediction and classification of high-frequency impedance in fuel cells,\nachieving metric of accuracy rate above 92\\%. The network is further deployed\non an FPGA, attaining a hardware-based recognition rate almost 90\\%."}
{"id": "2510.17250", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17250", "abs": "https://arxiv.org/abs/2510.17250", "authors": ["Wei-Hsun Lee", "Che-Yu Chang", "Kuang-Yu Li"], "title": "A Prototypical Network with an Attention-based Encoder for Drivers Identification Application", "comment": null, "summary": "Driver identification has become an area of increasing interest in recent\nyears, especially for data- driven applications, because biometric-based\ntechnologies may incur privacy issues. This study proposes a deep learning\nneural network architecture, an attention-based encoder (AttEnc), which uses an\nattention mechanism for driver identification and uses fewer model parameters\nthan current methods. Most studies do not address the issue of data shortages\nfor driver identification, and most of them are inflexible when encountering\nunknown drivers. In this study, an architecture that combines a prototypical\nnetwork and an attention-based encoder (P-AttEnc) is proposed. It applies\nfew-shot learning to overcome the data shortage issues and to enhance model\ngeneralizations. The experiments showed that the attention-based encoder can\nidentify drivers with accuracies of 99.3%, 99.0% and 99.9% in three different\ndatasets and has a prediction time that is 44% to 79% faster because it\nsignificantly reduces, on average, 87.6% of the model parameters. P-AttEnc\nidentifies drivers based on few shot data, extracts driver fingerprints to\naddress the issue of data shortages, and is able to classify unknown drivers.\nThe first experiment showed that P-AttEnc can identify drivers with an accuracy\nof 69.8% in the one-shot scenario. The second experiment showed that P-AttEnc,\nin the 1-shot scenario, can classify unknown drivers with an average accuracy\nof 65.7%."}
{"id": "2510.17266", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17266", "abs": "https://arxiv.org/abs/2510.17266", "authors": ["Jiayu Bai", "Zhanbo Feng", "Zhijie Deng", "Tianqi Hou", "Robert C. Qiu", "Zenan Ling"], "title": "Adaptive Discretization for Consistency Models", "comment": "Accepted by NeurIPS 2025", "summary": "Consistency Models (CMs) have shown promise for efficient one-step\ngeneration. However, most existing CMs rely on manually designed discretization\nschemes, which can cause repeated adjustments for different noise schedules and\ndatasets. To address this, we propose a unified framework for the automatic and\nadaptive discretization of CMs, formulating it as an optimization problem with\nrespect to the discretization step. Concretely, during the consistency training\nprocess, we propose using local consistency as the optimization objective to\nensure trainability by avoiding excessive discretization, and taking global\nconsistency as a constraint to ensure stability by controlling the denoising\nerror in the training target. We establish the trade-off between local and\nglobal consistency with a Lagrange multiplier. Building on this framework, we\nachieve adaptive discretization for CMs using the Gauss-Newton method. We refer\nto our approach as ADCMs. Experiments demonstrate that ADCMs significantly\nimprove the training efficiency of CMs, achieving superior generative\nperformance with minimal training overhead on both CIFAR-10 and ImageNet.\nMoreover, ADCMs exhibit strong adaptability to more advanced DM variants. Code\nis available at https://github.com/rainstonee/ADCM."}
{"id": "2510.17268", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17268", "abs": "https://arxiv.org/abs/2510.17268", "authors": ["Anthony Frion", "David S Greenberg"], "title": "Uncertainty-aware data assimilation through variational inference", "comment": null, "summary": "Data assimilation, consisting in the combination of a dynamical model with a\nset of noisy and incomplete observations in order to infer the state of a\nsystem over time, involves uncertainty in most settings. Building upon an\nexisting deterministic machine learning approach, we propose a variational\ninference-based extension in which the predicted state follows a multivariate\nGaussian distribution. Using the chaotic Lorenz-96 dynamics as a testing\nground, we show that our new model enables to obtain nearly perfectly\ncalibrated predictions, and can be integrated in a wider variational data\nassimilation pipeline in order to achieve greater benefit from increasing\nlengths of data assimilation windows. Our code is available at\nhttps://github.com/anthony-frion/Stochastic_CODA."}
{"id": "2510.17276", "categories": ["cs.LG", "cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17276", "abs": "https://arxiv.org/abs/2510.17276", "authors": ["Rishi Jha", "Harold Triedman", "Justin Wagle", "Vitaly Shmatikov"], "title": "Breaking and Fixing Defenses Against Control-Flow Hijacking in Multi-Agent Systems", "comment": null, "summary": "Control-flow hijacking attacks manipulate orchestration mechanisms in\nmulti-agent systems into performing unsafe actions that compromise the system\nand exfiltrate sensitive information. Recently proposed defenses, such as\nLlamaFirewall, rely on alignment checks of inter-agent communications to ensure\nthat all agent invocations are \"related to\" and \"likely to further\" the\noriginal objective.\n  We start by demonstrating control-flow hijacking attacks that evade these\ndefenses even if alignment checks are performed by advanced LLMs. We argue that\nthe safety and functionality objectives of multi-agent systems fundamentally\nconflict with each other. This conflict is exacerbated by the brittle\ndefinitions of \"alignment\" and the checkers' incomplete visibility into the\nexecution context.\n  We then propose, implement, and evaluate ControlValve, a new defense inspired\nby the principles of control-flow integrity and least privilege. ControlValve\n(1) generates permitted control-flow graphs for multi-agent systems, and (2)\nenforces that all executions comply with these graphs, along with contextual\nrules (generated in a zero-shot manner) for each agent invocation."}
{"id": "2510.17281", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.17281", "abs": "https://arxiv.org/abs/2510.17281", "authors": ["Qingyao Ai", "Yichen Tang", "Changyue Wang", "Jianming Long", "Weihang Su", "Yiqun Liu"], "title": "MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems", "comment": null, "summary": "Scaling up data, parameters, and test-time computation has been the\nmainstream methods to improve LLM systems (LLMsys), but their upper bounds are\nalmost reached due to the gradual depletion of high-quality data and marginal\ngains obtained from larger computational resource consumption. Inspired by the\nabilities of human and traditional AI systems in learning from practice,\nconstructing memory and continual learning frameworks for LLMsys has become an\nimportant and popular research direction in recent literature. Yet, existing\nbenchmarks for LLM memory often focus on evaluating the system on homogeneous\nreading comprehension tasks with long-form inputs rather than testing their\nabilities to learn from accumulated user feedback in service time. Therefore,\nwe propose a user feedback simulation framework and a comprehensive benchmark\ncovering multiple domains, languages, and types of tasks to evaluate the\ncontinual learning abilities of LLMsys. Experiments show that the effectiveness\nand efficiency of state-of-the-art baselines are far from satisfying, and we\nhope this benchmark could pave the way for future studies on LLM memory and\noptimization algorithms."}
{"id": "2510.17303", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17303", "abs": "https://arxiv.org/abs/2510.17303", "authors": ["Armin Beck", "Peter Ochs"], "title": "Symmetries in PAC-Bayesian Learning", "comment": null, "summary": "Symmetries are known to improve the empirical performance of machine learning\nmodels, yet theoretical guarantees explaining these gains remain limited. Prior\nwork has focused mainly on compact group symmetries and often assumes that the\ndata distribution itself is invariant, an assumption rarely satisfied in\nreal-world applications. In this work, we extend generalization guarantees to\nthe broader setting of non-compact symmetries, such as translations and to\nnon-invariant data distributions. Building on the PAC-Bayes framework, we adapt\nand tighten existing bounds, demonstrating the approach on McAllester's\nPAC-Bayes bound while showing that it applies to a wide range of PAC-Bayes\nbounds. We validate our theory with experiments on a rotated MNIST dataset with\na non-uniform rotation group, where the derived guarantees not only hold but\nalso improve upon prior results. These findings provide theoretical evidence\nthat, for symmetric data, symmetric models are preferable beyond the narrow\nsetting of compact groups and invariant distributions, opening the way to a\nmore general understanding of symmetries in machine learning."}
{"id": "2510.17313", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17313", "abs": "https://arxiv.org/abs/2510.17313", "authors": ["Tal Barami", "Nimrod Berman", "Ilan Naiman", "Amos H. Hason", "Rotem Ezra", "Omri Azencot"], "title": "Disentanglement Beyond Static vs. Dynamic: A Benchmark and Evaluation Framework for Multi-Factor Sequential Representations", "comment": null, "summary": "Learning disentangled representations in sequential data is a key goal in\ndeep learning, with broad applications in vision, audio, and time series. While\nreal-world data involves multiple interacting semantic factors over time, prior\nwork has mostly focused on simpler two-factor static and dynamic settings,\nprimarily because such settings make data collection easier, thereby\noverlooking the inherently multi-factor nature of real-world data. We introduce\nthe first standardized benchmark for evaluating multi-factor sequential\ndisentanglement across six diverse datasets spanning video, audio, and time\nseries. Our benchmark includes modular tools for dataset integration, model\ndevelopment, and evaluation metrics tailored to multi-factor analysis. We\nadditionally propose a post-hoc Latent Exploration Stage to automatically align\nlatent dimensions with semantic factors, and introduce a Koopman-inspired model\nthat achieves state-of-the-art results. Moreover, we show that Vision-Language\nModels can automate dataset annotation and serve as zero-shot disentanglement\nevaluators, removing the need for manual labels and human intervention.\nTogether, these contributions provide a robust and scalable foundation for\nadvancing multi-factor sequential disentanglement."}
{"id": "2510.17314", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17314", "abs": "https://arxiv.org/abs/2510.17314", "authors": ["Lipeng Xie", "Sen Huang", "Zhuo Zhang", "Anni Zou", "Yunpeng Zhai", "Dingchao Ren", "Kezun Zhang", "Haoyuan Hu", "Boyin Liu", "Haoran Chen", "Zhaoyang Liu", "Bolin Ding"], "title": "Auto-Rubric: Learning to Extract Generalizable Criteria for Reward Modeling", "comment": null, "summary": "Reward models are essential for aligning Large Language Models (LLMs) with\nhuman values, yet their development is hampered by costly preference datasets\nand poor interpretability. While recent rubric-based approaches offer\ntransparency, they often lack systematic quality control and optimization,\ncreating a trade-off between scalability and reliability. We address these\nlimitations with a novel, training-free framework built on a key assumption:\n\\textit{evaluation rubrics underlying human preferences exhibit significant\ngeneralization ability across diverse queries}, a property that enables\nremarkable data efficiency. Our two-stage approach first infers high-quality,\nquery-specific rubrics using a validation-guided\n\\textbf{Propose-Evaluate-Revise} pipeline. Second, it generalizes these\ngranular rubrics into a compact, non-redundant core set by maximizing an\n\\textbf{information-theoretic coding rate}. The final output is an\ninterpretable, hierarchical \"Theme-Tips\" rubric set. Extensive experiments\ndemonstrate the framework's exceptional data efficiency and performance.\nCritically, using just 70 preference pairs (1.5\\% of the source data), our\nmethod also empowers smaller models like Qwen3-8B to outperform specialized,\nfully-trained counterparts. This work pioneers a scalable, interpretable, and\ndata-efficient path for reward modeling."}
{"id": "2510.17358", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17358", "abs": "https://arxiv.org/abs/2510.17358", "authors": ["Joachim Diederich"], "title": "Localist LLMs with Recruitment Learning", "comment": null, "summary": "We present a novel framework for training large language models with\ncontinuously adjustable internal representations that span the full spectrum\nfrom localist (interpretable, rule-based) to distributed (generalizable,\nefficient) encodings. The key innovations are (1) a locality dial, a tunable\nparameter that dynamically controls the degree of localization during both\ntraining and inference without requiring model retraining, (2) an\ninformation-theoretic recruitment mechanism that adaptively allocates semantic\nblocks as needed, eliminating the requirement for complete domain knowledge at\ninitialization, and (3) a hierarchical recruitment framework that extends\ncapacity allocation to entire specialized LLMs, enabling multi-granularity\narchitectural adaptation. This is achieved through group sparsity penalties on\nattention mechanisms, information-theoretic anchor design, dynamic rule\ninjection, and principled recruitment criteria based on penalized likelihood\nwith explicit units. We provide rigorous mathematical results establishing\nexplicit threshold conditions under which attention provably concentrates on\nsemantically relevant blocks at stationary points, with exact bounds on\nattention entropy and pointer fidelity. The hierarchical recruitment mechanism\nprovides convergence guarantees at both the block level (fine-grained,\nwithin-LLM) and the LLM level (coarse-grained, cross-domain), ensuring the\nsystem discovers semantic partitions that balance model complexity against data\nencoding efficiency. This framework enables practitioners to continuously\ninterpolate between interpretable and high-performance modes while adapting\narchitectural capacity at multiple granularities, supporting applications in\nregulated domains requiring both transparency and capability."}
{"id": "2510.17378", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17378", "abs": "https://arxiv.org/abs/2510.17378", "authors": ["Wei Xu", "Xiaoyi Jiang", "Lixiang Xu", "Dechao Tang"], "title": "Model Metamers Reveal Invariances in Graph Neural Networks", "comment": null, "summary": "In recent years, deep neural networks have been extensively employed in\nperceptual systems to learn representations endowed with invariances, aiming to\nemulate the invariance mechanisms observed in the human brain. However, studies\nin the visual and auditory domains have confirmed that significant gaps remain\nbetween the invariance properties of artificial neural networks and those of\nhumans. To investigate the invariance behavior within graph neural networks\n(GNNs), we introduce a model ``metamers'' generation technique. By optimizing\ninput graphs such that their internal node activations match those of a\nreference graph, we obtain graphs that are equivalent in the model's\nrepresentation space, yet differ significantly in both structure and node\nfeatures. Our theoretical analysis focuses on two aspects: the local metamer\ndimension for a single node and the activation-induced volume change of the\nmetamer manifold. Utilizing this approach, we uncover extreme levels of\nrepresentational invariance across several classic GNN architectures. Although\ntargeted modifications to model architecture and training strategies can\npartially mitigate this excessive invariance, they fail to fundamentally bridge\nthe gap to human-like invariance. Finally, we quantify the deviation between\nmetamer graphs and their original counterparts, revealing unique failure modes\nof current GNNs and providing a complementary benchmark for model evaluation."}
{"id": "2510.17380", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17380", "abs": "https://arxiv.org/abs/2510.17380", "authors": ["Julen Cestero", "Carmine Delle Femine", "Kenji S. Muro", "Marco Quartulli", "Marcello Restelli"], "title": "Optimizing Energy Management of Smart Grid using Reinforcement Learning aided by Surrogate models built using Physics-informed Neural Networks", "comment": null, "summary": "Optimizing the energy management within a smart grids scenario presents\nsignificant challenges, primarily due to the complexity of real-world systems\nand the intricate interactions among various components. Reinforcement Learning\n(RL) is gaining prominence as a solution for addressing the challenges of\nOptimal Power Flow in smart grids. However, RL needs to iterate compulsively\nthroughout a given environment to obtain the optimal policy. This means\nobtaining samples from a, most likely, costly simulator, which can lead to a\nsample efficiency problem. In this work, we address this problem by\nsubstituting costly smart grid simulators with surrogate models built using\nPhisics-informed Neural Networks (PINNs), optimizing the RL policy training\nprocess by arriving to convergent results in a fraction of the time employed by\nthe original environment."}
{"id": "2510.17381", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17381", "abs": "https://arxiv.org/abs/2510.17381", "authors": ["Achref Jaziri", "Martin Rogmann", "Martin Mundt", "Visvanathan Ramesh"], "title": "Beyond Binary Out-of-Distribution Detection: Characterizing Distributional Shifts with Multi-Statistic Diffusion Trajectories", "comment": "11 Pages, 6 Figures", "summary": "Detecting out-of-distribution (OOD) data is critical for machine learning, be\nit for safety reasons or to enable open-ended learning. However, beyond mere\ndetection, choosing an appropriate course of action typically hinges on the\ntype of OOD data encountered. Unfortunately, the latter is generally not\ndistinguished in practice, as modern OOD detection methods collapse\ndistributional shifts into single scalar outlier scores. This work argues that\nscalar-based methods are thus insufficient for OOD data to be properly\ncontextualized and prospectively exploited, a limitation we overcome with the\nintroduction of DISC: Diffusion-based Statistical Characterization. DISC\nleverages the iterative denoising process of diffusion models to extract a\nrich, multi-dimensional feature vector that captures statistical discrepancies\nacross multiple noise levels. Extensive experiments on image and tabular\nbenchmarks show that DISC matches or surpasses state-of-the-art detectors for\nOOD detection and, crucially, also classifies OOD type, a capability largely\nabsent from prior work. As such, our work enables a shift from simple binary\nOOD detection to a more granular detection."}
{"id": "2510.17383", "categories": ["cs.LG", "cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.17383", "abs": "https://arxiv.org/abs/2510.17383", "authors": ["Ludovica Schaerf"], "title": "Latent Spaces Beyond Synthesis: From GANs to Diffusion Models", "comment": "Presented and published at Ethics and Aesthetics of Artificial\n  Intelligence Conference (EA-AI'25)", "summary": "This paper examines the evolving nature of internal representations in\ngenerative visual models, focusing on the conceptual and technical shift from\nGANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's\naccount of synthesis as the amalgamation of distributed representations, we\npropose a distinction between \"synthesis in a strict sense\", where a compact\nlatent space wholly determines the generative process, and \"synthesis in a\nbroad sense,\" which characterizes models whose representational labor is\ndistributed across layers. Through close readings of model architectures and a\ntargeted experimental setup that intervenes in layerwise representations, we\nshow how diffusion models fragment the burden of representation and thereby\nchallenge assumptions of unified internal space. By situating these findings\nwithin media theoretical frameworks and critically engaging with metaphors such\nas the latent space and the Platonic Representation Hypothesis, we argue for a\nreorientation of how generative AI is understood: not as a direct synthesis of\ncontent, but as an emergent configuration of specialized processes."}
{"id": "2510.17385", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17385", "abs": "https://arxiv.org/abs/2510.17385", "authors": ["Pengxiang Cai", "Zihao Gao", "Jintai Chen"], "title": "TabR1: Taming GRPO for tabular reasoning LLMs", "comment": null, "summary": "Tabular prediction has traditionally relied on gradient-boosted decision\ntrees and specialized deep learning models, which excel within tasks but\nprovide limited interpretability and weak transfer across tables. Reasoning\nlarge language models (LLMs) promise cross-task adaptability with trans- parent\nreasoning traces, yet their potential has not been fully realized for tabular\ndata. This paper presents TabR1, the first reasoning LLM for tabular prediction\nwith multi-step reasoning. At its core is Permutation Relative Policy\nOptimization (PRPO), a simple yet efficient reinforcement learning method that\nencodes column-permutation invariance as a structural prior. By construct- ing\nmultiple label-preserving permutations per sample and estimating advantages\nboth within and across permutations, PRPO transforms sparse rewards into dense\nlearning signals and improves generalization. With limited supervision, PRPO\nactivates the reasoning ability of LLMs for tabular prediction, enhancing\nfew-shot and zero-shot performance as well as interpretability. Comprehensive\nexperiments demonstrate that TabR1 achieves performance comparable to strong\nbaselines under full-supervision fine-tuning. In the zero-shot setting, TabR1\napproaches the performance of strong baselines under the 32-shot setting.\nMoreover, TabR1 (8B) substantially outperforms much larger LLMs across various\ntasks, achieving up to 53.17% improvement over DeepSeek-R1 (685B)."}
{"id": "2510.17390", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17390", "abs": "https://arxiv.org/abs/2510.17390", "authors": ["Seouh-won Yi", "Min-hwan Oh"], "title": "Exploration via Feature Perturbation in Contextual Bandits", "comment": "Accepted at NeurIPS 2025 (spotlight)", "summary": "We propose feature perturbation, a simple yet powerful technique that injects\nrandomness directly into feature inputs, instead of randomizing unknown\nparameters or adding noise to rewards. Remarkably, this algorithm achieves\n$\\tilde{\\mathcal{O}}(d\\sqrt{T})$ worst-case regret bound for generalized linear\nbandits, while avoiding the $\\tilde{\\mathcal{O}}(d^{3/2}\\sqrt{T})$ regret\ntypical of existing randomized bandit algorithms. Because our algorithm eschews\nparameter sampling, it is both computationally efficient and naturally extends\nto non-parametric or neural network models. We verify these advantages through\nempirical evaluations, demonstrating that feature perturbation not only\nsurpasses existing methods but also unifies strong practical performance with\nbest-known theoretical guarantees."}
{"id": "2510.17391", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17391", "abs": "https://arxiv.org/abs/2510.17391", "authors": ["Jongmin Lee", "Ernest K. Ryu"], "title": "Finite-Time Bounds for Average-Reward Fitted Q-Iteration", "comment": null, "summary": "Although there is an extensive body of work characterizing the sample\ncomplexity of discounted-return offline RL with function approximations, prior\nwork on the average-reward setting has received significantly less attention,\nand existing approaches rely on restrictive assumptions, such as ergodicity or\nlinearity of the MDP. In this work, we establish the first sample complexity\nresults for average-reward offline RL with function approximation for weakly\ncommunicating MDPs, a much milder assumption. To this end, we introduce\nAnchored Fitted Q-Iteration, which combines the standard Fitted Q-Iteration\nwith an anchor mechanism. We show that the anchor, which can be interpreted as\na form of weight decay, is crucial for enabling finite-time analysis in the\naverage-reward setting. We also extend our finite-time analysis to the setup\nwhere the dataset is generated from a single-trajectory rather than IID\ntransitions, again leveraging the anchor mechanism."}
{"id": "2510.17394", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17394", "abs": "https://arxiv.org/abs/2510.17394", "authors": ["Alejandro Guerra-Manzanares", "Farah E. Shamout"], "title": "MILES: Modality-Informed Learning Rate Scheduler for Balancing Multimodal Learning", "comment": "Accepted and presented at the 2025 International Joint Conference on\n  Neural Networks (IJCNN'25). The paper was awarded an honorable mention (best\n  4 papers)", "summary": "The aim of multimodal neural networks is to combine diverse data sources,\nreferred to as modalities, to achieve enhanced performance compared to relying\non a single modality. However, training of multimodal networks is typically\nhindered by modality overfitting, where the network relies excessively on one\nof the available modalities. This often yields sub-optimal performance,\nhindering the potential of multimodal learning and resulting in marginal\nimprovements relative to unimodal models. In this work, we present the\nModality-Informed Learning ratE Scheduler (MILES) for training multimodal joint\nfusion models in a balanced manner. MILES leverages the differences in\nmodality-wise conditional utilization rates during training to effectively\nbalance multimodal learning. The learning rate is dynamically adjusted during\ntraining to balance the speed of learning from each modality by the multimodal\nmodel, aiming for enhanced performance in both multimodal and unimodal\npredictions. We extensively evaluate MILES on four multimodal joint fusion\ntasks and compare its performance to seven state-of-the-art baselines. Our\nresults show that MILES outperforms all baselines across all tasks and fusion\nmethods considered in our study, effectively balancing modality usage during\ntraining. This results in improved multimodal performance and stronger modality\nencoders, which can be leveraged when dealing with unimodal samples or absent\nmodalities. Overall, our work highlights the impact of balancing multimodal\nlearning on improving model performance."}
{"id": "2510.17396", "categories": ["cs.LG", "eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17396", "abs": "https://arxiv.org/abs/2510.17396", "authors": ["Keivan Faghih Niresi", "Zepeng Zhang", "Olga Fink"], "title": "RINS-T: Robust Implicit Neural Solvers for Time Series Linear Inverse Problems", "comment": "Accepted to IEEE Transactions on Instrumentation and Measurement", "summary": "Time series data are often affected by various forms of corruption, such as\nmissing values, noise, and outliers, which pose significant challenges for\ntasks such as forecasting and anomaly detection. To address these issues,\ninverse problems focus on reconstructing the original signal from corrupted\ndata by leveraging prior knowledge about its underlying structure. While deep\nlearning methods have demonstrated potential in this domain, they often require\nextensive pretraining and struggle to generalize under distribution shifts. In\nthis work, we propose RINS-T (Robust Implicit Neural Solvers for Time Series\nLinear Inverse Problems), a novel deep prior framework that achieves high\nrecovery performance without requiring pretraining data. RINS-T leverages\nneural networks as implicit priors and integrates robust optimization\ntechniques, making it resilient to outliers while relaxing the reliance on\nGaussian noise assumptions. To further improve optimization stability and\nrobustness, we introduce three key innovations: guided input initialization,\ninput perturbation, and convex output combination techniques. Each of these\ncontributions strengthens the framework's optimization stability and\nrobustness. These advancements make RINS-T a flexible and effective solution\nfor addressing complex real-world time series challenges. Our code is available\nat https://github.com/EPFL-IMOS/RINS-T."}
{"id": "2510.17406", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.17406", "abs": "https://arxiv.org/abs/2510.17406", "authors": ["Tiezhi Wang", "Wilhelm Haverkamp", "Nils Strodthoff"], "title": "S4ECG: Exploring the impact of long-range interactions for arrhythmia prediction", "comment": null, "summary": "The electrocardiogram (ECG) exemplifies biosignal-based time series with\ncontinuous, temporally ordered structure reflecting cardiac physiological and\npathophysiological dynamics. Detailed analysis of these dynamics has proven\nchallenging, as conventional methods capture either global trends or local\nwaveform features but rarely their simultaneous interplay at high temporal\nresolution. To bridge global and local signal analysis, we introduce S4ECG, a\nnovel deep learning architecture leveraging structured state space models for\nmulti-epoch arrhythmia classification. Our joint multi-epoch predictions\nsignificantly outperform single-epoch approaches by 1.0-11.6% in macro-AUROC,\nwith atrial fibrillation specificity improving from 0.718-0.979 to 0.967-0.998,\ndemonstrating superior performance in-distribution and enhanced\nout-of-distribution robustness. Systematic investigation reveals optimal\ntemporal dependency windows spanning 10-20 minutes for peak performance. This\nwork contributes to a paradigm shift toward temporally-aware arrhythmia\ndetection algorithms, opening new possibilities for ECG interpretation, in\nparticular for complex arrhythmias like atrial fibrillation and atrial flutter."}
{"id": "2510.17414", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17414", "abs": "https://arxiv.org/abs/2510.17414", "authors": ["Hequn Li", "Zhongwei Deng", "Chunlin Jiang", "Yvxin He andZhansheng Ning"], "title": "A Conditional Diffusion Model for Probabilistic Prediction of Battery Capacity Degradation", "comment": null, "summary": "Accurate prediction of lithium-ion battery capacity and its associated\nuncertainty is essential for reliable battery management but remains\nchallenging due to the stochastic nature of aging. This paper presents a novel\nmethod, termed the Condition Diffusion U-Net with Attention (CDUA), which\nintegrates feature engineering and deep learning to address this challenge. The\nproposed approach employs a diffusion-based generative model for time-series\nforecasting and incorporates attention mechanisms to enhance predictive\nperformance. Battery capacity is first derived from real-world vehicle\noperation data. The most relevant features are then identified using the\nPearson correlation coefficient and the XGBoost algorithm. These features are\nused to train the CDUA model, which comprises two core components: (1) a\ncontextual U-Net with self-attention to capture complex temporal dependencies,\nand (2) a denoising network to reconstruct accurate capacity values from noisy\nobservations. Experimental validation on the real-world vehicle data\ndemonstrates that the proposed CDUA model achieves a relative Mean Absolute\nError (MAE) of 0.94% and a relative Root Mean Square Error (RMSE) of 1.14%,\nwith a narrow 95% confidence interval of 3.74% in relative width. These results\nconfirm that CDUA provides both accurate capacity estimation and reliable\nuncertainty quantification. Comparative experiments further verify its\nrobustness and superior performance over existing mainstream approaches."}
{"id": "2510.17421", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17421", "abs": "https://arxiv.org/abs/2510.17421", "authors": ["Duo Su", "Huyu Wu", "Huanran Chen", "Yiming Shi", "Yuzhu Wang", "Xi Ye", "Jun Zhu"], "title": "Diffusion Models as Dataset Distillation Priors", "comment": null, "summary": "Dataset distillation aims to synthesize compact yet informative datasets from\nlarge ones. A significant challenge in this field is achieving a trifecta of\ndiversity, generalization, and representativeness in a single distilled\ndataset. Although recent generative dataset distillation methods adopt powerful\ndiffusion models as their foundation models, the inherent representativeness\nprior in diffusion models is overlooked. Consequently, these approaches often\nnecessitate the integration of external constraints to enhance data quality. To\naddress this, we propose Diffusion As Priors (DAP), which formalizes\nrepresentativeness by quantifying the similarity between synthetic and real\ndata in feature space using a Mercer kernel. We then introduce this prior as\nguidance to steer the reverse diffusion process, enhancing the\nrepresentativeness of distilled samples without any retraining. Extensive\nexperiments on large-scale datasets, such as ImageNet-1K and its subsets,\ndemonstrate that DAP outperforms state-of-the-art methods in generating\nhigh-fidelity datasets while achieving superior cross-architecture\ngeneralization. Our work not only establishes a theoretical connection between\ndiffusion priors and the objectives of dataset distillation but also provides a\npractical, training-free framework for improving the quality of the distilled\ndataset."}
{"id": "2510.17457", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17457", "abs": "https://arxiv.org/abs/2510.17457", "authors": ["Li Sun", "Zhenhao Huang", "Ming Zhang", "Philip S. Yu"], "title": "Deeper with Riemannian Geometry: Overcoming Oversmoothing and Oversquashing for Graph Foundation Models", "comment": "Accept by NeurIPS 25", "summary": "Message Passing Neural Networks (MPNNs) is the building block of graph\nfoundation models, but fundamentally suffer from oversmoothing and\noversquashing. There has recently been a surge of interest in fixing both\nissues. Existing efforts primarily adopt global approaches, which may be\nbeneficial in some regions but detrimental in others, ultimately leading to the\nsuboptimal expressiveness. In this paper, we begin by revisiting oversquashing\nthrough a global measure -- spectral gap $\\lambda$ -- and prove that the\nincrease of $\\lambda$ leads to gradient vanishing with respect to the input\nfeatures, thereby undermining the effectiveness of message passing. Motivated\nby such theoretical insights, we propose a \\textbf{local} approach that\nadaptively adjusts message passing based on local structures. To achieve this,\nwe connect local Riemannian geometry with MPNNs, and establish a novel\nnonhomogeneous boundary condition to address both oversquashing and\noversmoothing. Building on the Robin condition, we design a GBN network with\nlocal bottleneck adjustment, coupled with theoretical guarantees. Extensive\nexperiments on homophilic and heterophilic graphs show the expressiveness of\nGBN. Furthermore, GBN does not exhibit performance degradation even when the\nnetwork depth exceeds $256$ layers."}
{"id": "2510.17458", "categories": ["cs.LG", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2510.17458", "abs": "https://arxiv.org/abs/2510.17458", "authors": ["Ayrat Abdullin", "Denis Anikiev", "Umair bin Waheed"], "title": "Explainable AI for microseismic event detection", "comment": "Submitted to Artificial Intelligence in Geosciences", "summary": "Deep neural networks like PhaseNet show high accuracy in detecting\nmicroseismic events, but their black-box nature is a concern in critical\napplications. We apply explainable AI (XAI) techniques, such as\nGradient-weighted Class Activation Mapping (Grad-CAM) and Shapley Additive\nExplanations (SHAP), to interpret the PhaseNet model's decisions and improve\nits reliability. Grad-CAM highlights that the network's attention aligns with\nP- and S-wave arrivals. SHAP values quantify feature contributions, confirming\nthat vertical-component amplitudes drive P-phase picks while horizontal\ncomponents dominate S-phase picks, consistent with geophysical principles.\nLeveraging these insights, we introduce a SHAP-gated inference scheme that\ncombines the model's output with an explanation-based metric to reduce errors.\nOn a test set of 9,000 waveforms, the SHAP-gated model achieved an F1-score of\n0.98 (precision 0.99, recall 0.97), outperforming the baseline PhaseNet\n(F1-score 0.97) and demonstrating enhanced robustness to noise. These results\nshow that XAI can not only interpret deep learning models but also directly\nenhance their performance, providing a template for building trust in automated\nseismic detectors."}
{"id": "2510.17467", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17467", "abs": "https://arxiv.org/abs/2510.17467", "authors": ["Dan Zheng", "Jing Feng", "Juan Liu"], "title": "CrossStateECG: Multi-Scale Deep Convolutional Network with Attention for Rest-Exercise ECG Biometrics", "comment": null, "summary": "Current research in Electrocardiogram (ECG) biometrics mainly emphasizes\nresting-state conditions, leaving the performance decline in rest-exercise\nscenarios largely unresolved. This paper introduces CrossStateECG, a robust\nECG-based authentication model explicitly tailored for cross-state\n(rest-exercise) conditions. The proposed model creatively combines multi-scale\ndeep convolutional feature extraction with attention mechanisms to ensure\nstrong identification across different physiological states. Experimental\nresults on the exercise-ECGID dataset validate the effectiveness of\nCrossStateECG, achieving an identification accuracy of 92.50% in the\nRest-to-Exercise scenario (training on resting ECG and testing on post-exercise\nECG) and 94.72% in the Exercise-to-Rest scenario (training on post-exercise ECG\nand testing on resting ECG). Furthermore, CrossStateECG demonstrates\nexceptional performance across both state combinations, reaching an accuracy of\n99.94% in Rest-to-Rest scenarios and 97.85% in Mixed-to-Mixed scenarios.\nAdditional validations on the ECG-ID and MIT-BIH datasets further confirmed the\ngeneralization abilities of CrossStateECG, underscoring its potential as a\npractical solution for post-exercise ECG-based authentication in dynamic\nreal-world settings."}
{"id": "2510.17469", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17469", "abs": "https://arxiv.org/abs/2510.17469", "authors": ["Jing Liu"], "title": "Layer Specialization Underlying Compositional Reasoning in Transformers", "comment": null, "summary": "Transformers exhibit compositional reasoning on sequences not observed during\ntraining, a capability often attributed to in-context learning (ICL) and skill\ncomposition. We investigate this phenomenon using the Random Hierarchy Model\n(RHM), a probabilistic context-free grammar that generates sequences through\nrecursive rule application. Models are trained on subsets of sequences and\nevaluated across four generalization conditions: memorization, in-distribution\ngeneralization, out-of-distribution generalization with the same rules, and\ncross-layer transfer. Behaviorally, performance improves systematically with\ntask complexity and the number of in-context examples, with out-of-distribution\ntasks requiring substantially more examples than in-distribution scenarios.\nMechanistically, we identify a progressive emergence of layer specialization\nduring training that correlates with generalization performance. Principal\ncomponent analysis and attention pattern clustering reveal that transformers\ndevelop structured, hierarchically organized representations in specialized\nlayers. These results demonstrate that transformers develop modular,\ninterpretable mechanisms supporting compositional reasoning, linking internal\nalgorithmic structure to observed behavioral capabilities."}
{"id": "2510.17475", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17475", "abs": "https://arxiv.org/abs/2510.17475", "authors": ["Fo Hu", "Can Wang", "Qinxu Zheng", "Xusheng Yang", "Bin Zhou", "Gang Li", "Yu Sun", "Wen-an Zhang"], "title": "DAMSDAN: Distribution-Aware Multi-Source Domain Adaptation Network for Cross-Domain EEG-based Emotion Recognition", "comment": "14 pages, 9 figures", "summary": "Significant inter-individual variability limits the generalization of\nEEG-based emotion recognition under cross-domain settings. We address two core\nchallenges in multi-source adaptation: (1) dynamically modeling distributional\nheterogeneity across sources and quantifying their relevance to a target to\nreduce negative transfer; and (2) achieving fine-grained semantic consistency\nto strengthen class discrimination. We propose a distribution-aware\nmulti-source domain adaptation network (DAMSDAN). DAMSDAN integrates\nprototype-based constraints with adversarial learning to drive the encoder\ntoward discriminative, domain-invariant emotion representations. A domain-aware\nsource weighting strategy based on maximum mean discrepancy (MMD) dynamically\nestimates inter-domain shifts and reweights source contributions. In addition,\na prototype-guided conditional alignment module with dual pseudo-label\ninteraction enhances pseudo-label reliability and enables category-level,\nfine-grained alignment, mitigating noise propagation and semantic drift.\nExperiments on SEED and SEED-IV show average accuracies of 94.86\\% and 79.78\\%\nfor cross-subject, and 95.12\\% and 83.15\\% for cross-session protocols. On the\nlarge-scale FACED dataset, DAMSDAN achieves 82.88\\% (cross-subject). Extensive\nablations and interpretability analyses corroborate the effectiveness of the\nproposed framework for cross-domain EEG-based emotion recognition."}
{"id": "2510.17478", "categories": ["cs.LG", "physics.geo-ph", "I.2.6; I.6.3; J.2"], "pdf": "https://arxiv.org/pdf/2510.17478", "abs": "https://arxiv.org/abs/2510.17478", "authors": ["Guillaume Rongier", "Luk Peeters"], "title": "Towards geological inference with process-based and deep generative modeling, part 2: inversion of fluvial deposits and latent-space disentanglement", "comment": "52 pages, 42 figures", "summary": "High costs and uncertainties make subsurface decision-making challenging, as\nacquiring new data is rarely scalable. Embedding geological knowledge directly\ninto predictive models offers a valuable alternative. A joint approach enables\njust that: process-based models that mimic geological processes can help train\ngenerative models that make predictions more efficiently. This study explores\nwhether a generative adversarial network (GAN) - a type of deep-learning\nalgorithm for generative modeling - trained to produce fluvial deposits can be\ninverted to match well and seismic data. Four inversion approaches applied to\nthree test samples with 4, 8, and 20 wells struggled to match these well data,\nespecially as the well number increased or as the test sample diverged from the\ntraining data. The key bottleneck lies in the GAN's latent representation: it\nis entangled, so samples with similar sedimentological features are not\nnecessarily close in the latent space. Label conditioning or latent\noverparameterization can partially disentangle the latent space during\ntraining, although not yet sufficiently for a successful inversion. Fine-tuning\nthe GAN to restructure the latent space locally reduces mismatches to\nacceptable levels for all test cases, with and without seismic data. But this\napproach depends on an initial, partially successful inversion step, which\ninfluences the quality and diversity of the final samples. Overall, GANs can\nalready handle the tasks required for their integration into geomodeling\nworkflows. We still need to further assess their robustness, and how to best\nleverage them in support of geological interpretation."}
{"id": "2510.17480", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17480", "abs": "https://arxiv.org/abs/2510.17480", "authors": ["Aurélien Bellet", "Edwige Cyffers", "Davide Frey", "Romaric Gaudel", "Dimitri Lerévérend", "François Taïani"], "title": "Unified Privacy Guarantees for Decentralized Learning via Matrix Factorization", "comment": "21 pages, 5 figures", "summary": "Decentralized Learning (DL) enables users to collaboratively train models\nwithout sharing raw data by iteratively averaging local updates with neighbors\nin a network graph. This setting is increasingly popular for its scalability\nand its ability to keep data local under user control. Strong privacy\nguarantees in DL are typically achieved through Differential Privacy (DP), with\nresults showing that DL can even amplify privacy by disseminating noise across\npeer-to-peer communications. Yet in practice, the observed privacy-utility\ntrade-off often appears worse than in centralized training, which may be due to\nlimitations in current DP accounting methods for DL. In this paper, we show\nthat recent advances in centralized DP accounting based on Matrix Factorization\n(MF) for analyzing temporal noise correlations can also be leveraged in DL. By\ngeneralizing existing MF results, we show how to cast both standard DL\nalgorithms and common trust models into a unified formulation. This yields\ntighter privacy accounting for existing DP-DL algorithms and provides a\nprincipled way to develop new ones. To demonstrate the approach, we introduce\nMAFALDA-SGD, a gossip-based DL algorithm with user-level correlated noise that\noutperforms existing methods on synthetic and real-world graphs."}
{"id": "2510.17486", "categories": ["cs.LG", "68T07, 68T05, 65K10, 90C30", "I.2.6; G.1.6; I.5.1"], "pdf": "https://arxiv.org/pdf/2510.17486", "abs": "https://arxiv.org/abs/2510.17486", "authors": ["Maxim Bolshim", "Alexander Kugaevskikh"], "title": "Local properties of neural networks through the lens of layer-wise Hessians", "comment": "Comments: 22 pages, 8 figures. Submitted to arXiv:cs.LG", "summary": "We introduce a methodology for analyzing neural networks through the lens of\nlayer-wise Hessian matrices. The local Hessian of each functional block (layer)\nis defined as the matrix of second derivatives of a scalar function with\nrespect to the parameters of that layer. This concept provides a formal tool\nfor characterizing the local geometry of the parameter space. We show that the\nspectral properties of local Hessians, such as the distribution of eigenvalues,\nreveal quantitative patterns associated with overfitting,\nunderparameterization, and expressivity in neural network architectures. We\nconduct an extensive empirical study involving 111 experiments across 37\ndatasets. The results demonstrate consistent structural regularities in the\nevolution of local Hessians during training and highlight correlations between\ntheir spectra and generalization performance. These findings establish a\nfoundation for using local geometric analysis to guide the diagnosis and design\nof deep neural networks. The proposed framework connects optimization geometry\nwith functional behavior and offers practical insight for improving network\narchitectures and training stability."}
{"id": "2510.17496", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17496", "abs": "https://arxiv.org/abs/2510.17496", "authors": ["Giacomo Camposampiero", "Michael Hersche", "Roger Wattenhofer", "Abu Sebastian", "Abbas Rahimi"], "title": "I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and Mathematical Reasoning in Large Language and Reasoning Models", "comment": "Accepted at the 5th Workshop on Mathematical Reasoning and AI\n  (MATH-AI), NeurIPS 2025", "summary": "We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate\ngeneralization and robustness in analogical and mathematical reasoning for\nLarge Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X\nextends I-RAVEN by increasing operand complexity, attribute range, and\nintroducing perceptual uncertainty. Compared to LLMs, empirical results show\nthat LRMs achieve improved productivity and systematicity on longer reasoning\nrelations and wider attribute ranges, respectively. However, LRMs are still\nsignificantly challenged by reasoning under uncertainty and cannot effectively\nexplore multiple probabilistic outcomes."}
{"id": "2510.17503", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17503", "abs": "https://arxiv.org/abs/2510.17503", "authors": ["El Mahdi Chayti", "Martin Jaggi"], "title": "Stochastic Difference-of-Convex Optimization with Momentum", "comment": null, "summary": "Stochastic difference-of-convex (DC) optimization is prevalent in numerous\nmachine learning applications, yet its convergence properties under small batch\nsizes remain poorly understood. Existing methods typically require large\nbatches or strong noise assumptions, which limit their practical use. In this\nwork, we show that momentum enables convergence under standard smoothness and\nbounded variance assumptions (of the concave part) for any batch size. We prove\nthat without momentum, convergence may fail regardless of stepsize,\nhighlighting its necessity. Our momentum-based algorithm achieves provable\nconvergence and demonstrates strong empirical performance."}
{"id": "2510.17506", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.17506", "abs": "https://arxiv.org/abs/2510.17506", "authors": ["Lachlan Ewen MacDonald", "Hancheng Min", "Leandro Palma", "Salma Tarmoun", "Ziqing Xu", "René Vidal"], "title": "Convergence Rates for Gradient Descent on the Edge of Stability in Overparametrised Least Squares", "comment": "NeurIPS2025. Code available at\n  https://github.com/lemacdonald/eos-convergence-rates-codimension-1", "summary": "Classical optimisation theory guarantees monotonic objective decrease for\ngradient descent (GD) when employed in a small step size, or ``stable\", regime.\nIn contrast, gradient descent on neural networks is frequently performed in a\nlarge step size regime called the ``edge of stability\", in which the objective\ndecreases non-monotonically with an observed implicit bias towards flat minima.\nIn this paper, we take a step toward quantifying this phenomenon by providing\nconvergence rates for gradient descent with large learning rates in an\noverparametrised least squares setting. The key insight behind our analysis is\nthat, as a consequence of overparametrisation, the set of global minimisers\nforms a Riemannian manifold $M$, which enables the decomposition of the GD\ndynamics into components parallel and orthogonal to $M$. The parallel component\ncorresponds to Riemannian gradient descent on the objective sharpness, while\nthe orthogonal component is a bifurcating dynamical system. This insight allows\nus to derive convergence rates in three regimes characterised by the learning\nrate size: (a) the subcritical regime, in which transient instability is\novercome in finite time before linear convergence to a suboptimally flat global\nminimum; (b) the critical regime, in which instability persists for all time\nwith a power-law convergence toward the optimally flat global minimum; and (c)\nthe supercritical regime, in which instability persists for all time with\nlinear convergence to an orbit of period two centred on the optimally flat\nglobal minimum."}
{"id": "2510.17515", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17515", "abs": "https://arxiv.org/abs/2510.17515", "authors": ["Hoang Pham", "The-Anh Ta", "Tom Jacobs", "Rebekka Burkholz", "Long Tran-Thanh"], "title": "The Graphon Limit Hypothesis: Understanding Neural Network Pruning via Infinite Width Analysis", "comment": "NeurIPS 2025 Spotlight", "summary": "Sparse neural networks promise efficiency, yet training them effectively\nremains a fundamental challenge. Despite advances in pruning methods that\ncreate sparse architectures, understanding why some sparse structures are\nbetter trainable than others with the same level of sparsity remains poorly\nunderstood. Aiming to develop a systematic approach to this fundamental\nproblem, we propose a novel theoretical framework based on the theory of graph\nlimits, particularly graphons, that characterizes sparse neural networks in the\ninfinite-width regime. Our key insight is that connectivity patterns of sparse\nneural networks induced by pruning methods converge to specific graphons as\nnetworks' width tends to infinity, which encodes implicit structural biases of\ndifferent pruning methods. We postulate the Graphon Limit Hypothesis and\nprovide empirical evidence to support it. Leveraging this graphon\nrepresentation, we derive a Graphon Neural Tangent Kernel (Graphon NTK) to\nstudy the training dynamics of sparse networks in the infinite width limit.\nGraphon NTK provides a general framework for the theoretical analysis of sparse\nnetworks. We empirically show that the spectral analysis of Graphon NTK\ncorrelates with observed training dynamics of sparse networks, explaining the\nvarying convergence behaviours of different pruning methods. Our framework\nprovides theoretical insights into the impact of connectivity patterns on the\ntrainability of various sparse network architectures."}
{"id": "2510.17517", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.17517", "abs": "https://arxiv.org/abs/2510.17517", "authors": ["Hangcheng Cao", "Baixiang Huang", "Longzhi Yuan", "Haonan An", "Zihan Fang", "Xianhao Chen", "Yuguang Fang"], "title": "SAFE-D: A Spatiotemporal Detection Framework for Abnormal Driving Among Parkinson's Disease-like Drivers", "comment": null, "summary": "A driver's health state serves as a determinant factor in driving behavioral\nregulation. Subtle deviations from normalcy can lead to operational anomalies,\nposing risks to public transportation safety. While prior efforts have\ndeveloped detection mechanisms for functionally-driven temporary anomalies such\nas drowsiness and distraction, limited research has addressed\npathologically-triggered deviations, especially those stemming from chronic\nmedical conditions. To bridge this gap, we investigate the driving behavior of\nParkinson's disease patients and propose SAFE-D, a novel framework for\ndetecting Parkinson-related behavioral anomalies to enhance driving safety. Our\nmethodology starts by performing analysis of Parkinson's disease\nsymptomatology, focusing on primary motor impairments, and establishes causal\nlinks to degraded driving performance. To represent the subclinical behavioral\nvariations of early-stage Parkinson's disease, our framework integrates data\nfrom multiple vehicle control components to build a behavioral profile. We then\ndesign an attention-based network that adaptively prioritizes spatiotemporal\nfeatures, enabling robust anomaly detection under physiological variability.\nFinally, we validate SAFE-D on the Logitech G29 platform and CARLA simulator,\nusing data from three road maps to emulate real-world driving. Our results show\nSAFE-D achieves 96.8% average accuracy in distinguishing normal and\nParkinson-affected driving patterns."}
{"id": "2510.17520", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17520", "abs": "https://arxiv.org/abs/2510.17520", "authors": ["Canran Xiao", "Chuangxin Zhao", "Zong Ke", "Fei Shen"], "title": "Curiosity Meets Cooperation: A Game-Theoretic Approach to Long-Tail Multi-Label Learning", "comment": "Under review", "summary": "Long-tail imbalance is endemic to multi-label learning: a few head labels\ndominate the gradient signal, while the many rare labels that matter in\npractice are silently ignored. We tackle this problem by casting the task as a\ncooperative potential game. In our Curiosity-Driven Game-Theoretic Multi-Label\nLearning (CD-GTMLL) framework, the label space is split among several\ncooperating players that share a global accuracy payoff yet earn additional\ncuriosity rewards that rise with label rarity and inter-player disagreement.\nThese curiosity bonuses inject gradient on under-represented tags without\nhand-tuned class weights. We prove that gradient best-response updates ascend a\ndifferentiable potential and converge to tail-aware stationary points that\ntighten a lower bound on the expected Rare-F1. Extensive experiments on\nconventional benchmarks and three extreme-scale datasets show consistent\nstate-of-the-art gains, delivering up to +4.3% Rare-F1 and +1.6% P@3 over the\nstrongest baselines, while ablations reveal emergent division of labour and\nfaster consensus on rare classes. CD-GTMLL thus offers a principled, scalable\nroute to long-tail robustness in multi-label prediction."}
{"id": "2510.17524", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17524", "abs": "https://arxiv.org/abs/2510.17524", "authors": ["Sidney Bender", "Ole Delzer", "Jan Herrmann", "Heike Antje Marxfeld", "Klaus-Robert Müller", "Grégoire Montavon"], "title": "Mitigating Clever Hans Strategies in Image Classifiers through Generating Counterexamples", "comment": null, "summary": "Deep learning models remain vulnerable to spurious correlations, leading to\nso-called Clever Hans predictors that undermine robustness even in large-scale\nfoundation and self-supervised models. Group distributional robustness methods,\nsuch as Deep Feature Reweighting (DFR) rely on explicit group labels to\nupweight underrepresented subgroups, but face key limitations: (1) group labels\nare often unavailable, (2) low within-group sample sizes hinder coverage of the\nsubgroup distribution, and (3) performance degrades sharply when multiple\nspurious correlations fragment the data into even smaller groups. We propose\nCounterfactual Knowledge Distillation (CFKD), a framework that sidesteps these\nissues by generating diverse counterfactuals, enabling a human annotator to\nefficiently explore and correct the model's decision boundaries through a\nknowledge distillation step. Unlike DFR, our method not only reweights the\nundersampled groups, but it also enriches them with new data points. Our method\ndoes not require any confounder labels, achieves effective scaling to multiple\nconfounders, and yields balanced generalization across groups. We demonstrate\nCFKD's efficacy across five datasets, spanning synthetic tasks to an industrial\napplication, with particularly strong gains in low-data regimes with pronounced\nspurious correlations. Additionally, we provide an ablation study on the effect\nof the chosen counterfactual explainer and teacher model, highlighting their\nimpact on robustness."}
{"id": "2510.17526", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17526", "abs": "https://arxiv.org/abs/2510.17526", "authors": ["Wei Huang", "Andi Han", "Yujin Song", "Yilan Chen", "Denny Wu", "Difan Zou", "Taiji Suzuki"], "title": "How Does Label Noise Gradient Descent Improve Generalization in the Low SNR Regime?", "comment": "40 pages", "summary": "The capacity of deep learning models is often large enough to both learn the\nunderlying statistical signal and overfit to noise in the training set. This\nnoise memorization can be harmful especially for data with a low\nsignal-to-noise ratio (SNR), leading to poor generalization. Inspired by prior\nobservations that label noise provides implicit regularization that improves\ngeneralization, in this work, we investigate whether introducing label noise to\nthe gradient updates can enhance the test performance of neural network (NN) in\nthe low SNR regime. Specifically, we consider training a two-layer NN with a\nsimple label noise gradient descent (GD) algorithm, in an idealized\nsignal-noise data setting. We prove that adding label noise during training\nsuppresses noise memorization, preventing it from dominating the learning\nprocess; consequently, label noise GD enjoys rapid signal growth while the\noverfitting remains controlled, thereby achieving good generalization despite\nthe low SNR. In contrast, we also show that NN trained with standard GD tends\nto overfit to noise in the same low SNR setting and establish a non-vanishing\nlower bound on its test error, thus demonstrating the benefit of introducing\nlabel noise in gradient-based training."}
{"id": "2510.17543", "categories": ["cs.LG", "eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17543", "abs": "https://arxiv.org/abs/2510.17543", "authors": ["Jiayi Huang", "Sangwoo Park", "Nicola Paoletti", "Osvaldo Simeone"], "title": "Reliable Inference in Edge-Cloud Model Cascades via Conformal Alignment", "comment": "Under Review", "summary": "Edge intelligence enables low-latency inference via compact on-device models,\nbut assuring reliability remains challenging. We study edge-cloud cascades that\nmust preserve conditional coverage: whenever the edge returns a prediction set,\nit should contain the true label with a user-specified probability, as if\nproduced by the cloud model. We formalize conditional coverage with respect to\nthe cloud predictive distribution, and introduce a conformal alignment-based\n(CAb) cascading mechanism that certifies this property with user control over\nthe risk level. Our method casts escalation from edge to cloud models as a\nmultiple-hypothesis testing (MHT) problem, tailoring conformal alignment (CA)\nto select which inputs can be safely handled at the edge. The proposed CAb\nmodel cascading method yields statistical guarantees on the average fraction of\nedge decisions that satisfy cloud-level conditional coverage. The procedure\napplies to arbitrary edge prediction sets, including variants of conformal\nprediction (CP), and exposes a tunable trade-off among coverage, deferral rate,\nand set size. Experiments on CIFAR-100 image classification and the TeleQnA\nquestion-answering (QA) benchmark show that the proposed CAb cascade maintains\nthe target conditional coverage for edge predictions while substantially\nreducing offloading to the cloud and incurring modest increases in\nprediction-set size."}
{"id": "2510.17545", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17545", "abs": "https://arxiv.org/abs/2510.17545", "authors": ["Yichen Liu", "Yan Lin", "Shengnan Guo", "Zeyu Zhou", "Youfang Lin", "Huaiyu Wan"], "title": "TrajMamba: An Efficient and Semantic-rich Vehicle Trajectory Pre-training Model", "comment": "Accepted by NeurIPS2025", "summary": "Vehicle GPS trajectories record how vehicles move over time, storing valuable\ntravel semantics, including movement patterns and travel purposes. Learning\ntravel semantics effectively and efficiently is crucial for real-world\napplications of trajectory data, which is hindered by two major challenges.\nFirst, travel purposes are tied to the functions of the roads and\npoints-of-interest (POIs) involved in a trip. Such information is encoded in\ntextual addresses and descriptions and introduces heavy computational burden to\nmodeling. Second, real-world trajectories often contain redundant points, which\nharm both computational efficiency and trajectory embedding quality. To address\nthese challenges, we propose TrajMamba, a novel approach for efficient and\nsemantically rich vehicle trajectory learning. TrajMamba introduces a\nTraj-Mamba Encoder that captures movement patterns by jointly modeling both GPS\nand road perspectives of trajectories, enabling robust representations of\ncontinuous travel behaviors. It also incorporates a Travel Purpose-aware\nPre-training procedure to integrate travel purposes into the learned embeddings\nwithout introducing extra overhead to embedding calculation. To reduce\nredundancy in trajectories, TrajMamba features a Knowledge Distillation\nPre-training scheme to identify key trajectory points through a learnable mask\ngenerator and obtain effective compressed trajectory embeddings. Extensive\nexperiments on two real-world datasets and three downstream tasks show that\nTrajMamba outperforms state-of-the-art baselines in both efficiency and\naccuracy."}
{"id": "2510.17558", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17558", "abs": "https://arxiv.org/abs/2510.17558", "authors": ["François Fleuret"], "title": "The Free Transformer", "comment": null, "summary": "We propose an extension of the decoder Transformer that conditions its\ngenerative process on random latent variables which are learned without\nsupervision thanks to a variational procedure. Experimental evaluations show\nthat allowing such a conditioning translates into substantial improvements on\ndownstream tasks."}
{"id": "2510.17562", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17562", "abs": "https://arxiv.org/abs/2510.17562", "authors": ["Dennis Wagner", "Arjun Nair", "Billy Joe Franks", "Justus Arweiler", "Aparna Muraleedharan", "Indra Jungjohann", "Fabian Hartung", "Mayank C. Ahuja", "Andriy Balinskyy", "Saurabh Varshneya", "Nabeel Hussain Syed", "Mayank Nagda", "Phillip Liznerski", "Steffen Reithermann", "Maja Rudolph", "Sebastian Vollmer", "Ralf Schulz", "Torsten Katz", "Stephan Mandt", "Michael Bortz", "Heike Leitte", "Daniel Neider", "Jakob Burger", "Fabian Jirasek", "Hans Hasse", "Sophie Fellenz", "Marius Kloft"], "title": "Formally Exploring Time-Series Anomaly Detection Evaluation Metrics", "comment": "73 pages, 13 figures", "summary": "Undetected anomalies in time series can trigger catastrophic failures in\nsafety-critical systems, such as chemical plant explosions or power grid\noutages. Although many detection methods have been proposed, their performance\nremains unclear because current metrics capture only narrow aspects of the task\nand often yield misleading results. We address this issue by introducing\nverifiable properties that formalize essential requirements for evaluating\ntime-series anomaly detection. These properties enable a theoretical framework\nthat supports principled evaluations and reliable comparisons. Analyzing 37\nwidely used metrics, we show that most satisfy only a few properties, and none\nsatisfy all, explaining persistent inconsistencies in prior results. To close\nthis gap, we propose LARM, a flexible metric that provably satisfies all\nproperties, and extend it to ALARM, an advanced variant meeting stricter\nrequirements."}
{"id": "2510.17564", "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17564", "abs": "https://arxiv.org/abs/2510.17564", "authors": ["Lindsay Spoor", "Álvaro Serra-Gómez", "Aske Plaat", "Thomas Moerland"], "title": "An Empirical Study of Lagrangian Methods in Safe Reinforcement Learning", "comment": null, "summary": "In safety-critical domains such as robotics, navigation and power systems,\nconstrained optimization problems arise where maximizing performance must be\ncarefully balanced with associated constraints. Safe reinforcement learning\nprovides a framework to address these challenges, with Lagrangian methods being\na popular choice. However, the effectiveness of Lagrangian methods crucially\ndepends on the choice of the Lagrange multiplier $\\lambda$, which governs the\ntrade-off between return and constraint cost. A common approach is to update\nthe multiplier automatically during training. Although this is standard in\npractice, there remains limited empirical evidence on the robustness of an\nautomated update and its influence on overall performance. Therefore, we\nanalyze (i) optimality and (ii) stability of Lagrange multipliers in safe\nreinforcement learning across a range of tasks. We provide $\\lambda$-profiles\nthat give a complete visualization of the trade-off between return and\nconstraint cost of the optimization problem. These profiles show the highly\nsensitive nature of $\\lambda$ and moreover confirm the lack of general\nintuition for choosing the optimal value $\\lambda^*$. Our findings additionally\nshow that automated multiplier updates are able to recover and sometimes even\nexceed the optimal performance found at $\\lambda^*$ due to the vast difference\nin their learning trajectories. Furthermore, we show that automated multiplier\nupdates exhibit oscillatory behavior during training, which can be mitigated\nthrough PID-controlled updates. However, this method requires careful tuning to\nachieve consistently better performance across tasks. This highlights the need\nfor further research on stabilizing Lagrangian methods in safe reinforcement\nlearning. The code used to reproduce our results can be found at\nhttps://github.com/lindsayspoor/Lagrangian_SafeRL."}
{"id": "2510.17569", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.17569", "abs": "https://arxiv.org/abs/2510.17569", "authors": ["Jyler Menard", "R. A. Mansbach"], "title": "Semi-supervised Latent Bayesian Optimization for Designing Antimicrobial Peptides", "comment": "19 pages, 9 figures", "summary": "Antimicrobial peptides (AMPs) are a promising class of therapeutics to treat\nbacterial infections. Discovering and designing such peptides is difficult\nbecause of the vast number of possible sequences of amino acids. Deep\ngenerative models, such as variational autoencoders, have shown value in\npeptide design due to their ability to model sequence space with a\ncontinuous-valued latent space. Although such models have already been used to\ngreat effect in biomolecular design, they still suffer from a lack of\ninterpretability and rigorous quantification of latent space quality as a\nsearch space. We investigate (1) whether further compression of the design\nspace via dimensionality reduction may facilitate optimization, (2) the\ninterpretability of the spaces, and (3) how organizing latent spaces with\nphysicochemical properties may improve the efficiency of optimizing\nantimicrobial activity. We find that further reduction of the latent space via\ndimensionality reduction can be advantageous when organizing the space with\nmore relevant information at data availability, that using the dimensionality\nreduction search space can be more interpretable, and that we can organize the\nlatent space with different physicochemical properties even at different\npercentages of available labels."}
{"id": "2510.17584", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17584", "abs": "https://arxiv.org/abs/2510.17584", "authors": ["Ludi Li", "Junbin Mao", "Hanhe Lin", "Xu Tian", "Fang-Xiang Wu", "Jin Liu"], "title": "CEPerFed: Communication-Efficient Personalized Federated Learning for Multi-Pulse MRI Classification", "comment": null, "summary": "Multi-pulse magnetic resonance imaging (MRI) is widely utilized for clinical\npractice such as Alzheimer's disease diagnosis. To train a robust model for\nmulti-pulse MRI classification, it requires large and diverse data from various\nmedical institutions while protecting privacy by preventing raw data sharing\nacross institutions. Although federated learning (FL) is a feasible solution to\naddress this issue, it poses challenges of model convergence due to the effect\nof data heterogeneity and substantial communication overhead due to large\nnumbers of parameters transmitted within the model. To address these\nchallenges, we propose CEPerFed, a communication-efficient personalized FL\nmethod. It mitigates the effect of data heterogeneity by incorporating\nclient-side historical risk gradients and historical mean gradients to\ncoordinate local and global optimization. The former is used to weight the\ncontributions from other clients, enhancing the reliability of local updates,\nwhile the latter enforces consistency between local updates and the global\noptimization direction to ensure stable convergence across heterogeneous data\ndistributions. To address the high communication overhead, we propose a\nhierarchical SVD (HSVD) strategy that transmits only the most critical\ninformation required for model updates. Experiments on five classification\ntasks demonstrate the effectiveness of the CEPerFed method. The code will be\nreleased upon acceptance at https://github.com/LD0416/CEPerFed."}
{"id": "2510.17650", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17650", "abs": "https://arxiv.org/abs/2510.17650", "authors": ["Athanasios Angelakis", "Amne Mousa", "Micah L. A. Heldeweg", "Laurens A. Biesheuvel", "Mark A. Haaksma", "Jasper M. Smit", "Pieter R. Tuinman", "Paul W. G. Elbers"], "title": "ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data Augmentation for Robust Lung Ultrasound Classification", "comment": "14 pages, 6 figures, 2 tables. Primary subject: cs.LG (Machine\n  Learning) Cross-listed to: cs.CV (Computer Vision and Pattern Recognition),\n  eess.IV (Image and Video Processing). Code available at:\n  https://github.com/Bluesman79/ZACH-ViT Installation: pip install zachvit\n  Paper licensed under CC BY-NC-ND 4.0. Code released under Apache 2.0 License", "summary": "Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and\nstructurally normal lungs in lung ultrasound (LUS) videos remains challenging\ndue to the high visual variability of non-cardiogenic inflammatory patterns\n(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This\nheterogeneity complicates automated classification as overlapping B-lines and\npleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive\nCompact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer\nvariant that removes both positional embeddings and the [CLS] token, making it\nfully permutation-invariant and suitable for unordered medical image data. To\nenhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),\nwhich permutes probe-view sequences and frame orders while preserving\nanatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95\ncritically ill patients against nine state-of-the-art baselines. Despite the\nheterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest\nvalidation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)\nand specificity (0.91), while all competing models collapsed to trivial\nclassification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with\n2.5x fewer parameters, supporting real-time clinical deployment. These results\nshow that aligning architectural design with data structure can outperform\nscale in small-data medical imaging."}
{"id": "2510.17661", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17661", "abs": "https://arxiv.org/abs/2510.17661", "authors": ["Vaishnavi Visweswaraiah", "Tanvi Banerjee", "William Romine"], "title": "Handling Extreme Class Imbalance: Using GANs in Data Augmentation for Suicide Prediction", "comment": null, "summary": "Suicide prediction is the key for prevention, but real data with sufficient\npositive samples is rare and causes extreme class imbalance. We utilized\nmachine learning (ML) to build the model and deep learning (DL) techniques,\nlike Generative Adversarial Networks (GAN), to generate synthetic data samples\nto enhance the dataset. The initial dataset contained 656 samples, with only\nfour positive cases, prompting the need for data augmentation. A variety of\nmachine learning models, ranging from interpretable data models to black box\nalgorithmic models, were used. On real test data, Logistic Regression (LR)\nachieved a weighted precision of 0.99, a weighted recall of 0.85, and a\nweighted F1 score of 0.91; Random Forest (RF) showed 0.98, 0.99, and 0.99,\nrespectively; and Support Vector Machine (SVM) achieved 0.99, 0.76, and 0.86.\nLR and SVM correctly identified one suicide attempt case (sensitivity:1.0) and\nmisclassified LR(20) and SVM (31) non-attempts as attempts (specificity: 0.85 &\n0.76, respectively). RF identified 0 suicide attempt cases (sensitivity: 0.0)\nwith 0 false positives (specificity: 1.0). These results highlight the models'\neffectiveness, with GAN playing a key role in generating synthetic data to\nsupport suicide prevention modeling efforts."}
{"id": "2510.17670", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.17670", "abs": "https://arxiv.org/abs/2510.17670", "authors": ["Yehonathan Refael", "Amit Aides", "Aviad Barzilai", "George Leifman", "Genady Beryozkin", "Vered Silverman", "Bolous Jaber", "Tomer Shekel"], "title": "On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration", "comment": null, "summary": "Open-vocabulary object detection (OVD) models offer remarkable flexibility by\ndetecting objects from arbitrary text queries. However, their zero-shot\nperformance in specialized domains like Remote Sensing (RS) is often\ncompromised by the inherent ambiguity of natural language, limiting critical\ndownstream applications. For instance, an OVD model may struggle to distinguish\nbetween fine-grained classes such as \"fishing boat\" and \"yacht\" since their\nembeddings are similar and often inseparable. This can hamper specific user\ngoals, such as monitoring illegal fishing, by producing irrelevant detections.\nTo address this, we propose a cascaded approach that couples the broad\ngeneralization of a large pre-trained OVD model with a lightweight few-shot\nclassifier. Our method first employs the zero-shot model to generate\nhigh-recall object proposals. These proposals are then refined for high\nprecision by a compact classifier trained in real-time on only a handful of\nuser-annotated examples - drastically reducing the high costs of RS imagery\nannotation.The core of our framework is FLAME, a one-step active learning\nstrategy that selects the most informative samples for training. FLAME\nidentifies, on the fly, uncertain marginal candidates near the decision\nboundary using density estimation, followed by clustering to ensure sample\ndiversity. This efficient sampling technique achieves high accuracy without\ncostly full-model fine-tuning and enables instant adaptation, within less then\na minute, which is significantly faster than state-of-the-art alternatives.Our\nmethod consistently surpasses state-of-the-art performance on RS benchmarks,\nestablishing a practical and resource-efficient framework for adapting\nfoundation models to specific user needs."}
{"id": "2510.17671", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17671", "abs": "https://arxiv.org/abs/2510.17671", "authors": ["Katarzyna Kobalczyk", "Zhiyuan Jerry Lin", "Benjamin Letham", "Zhuokai Zhao", "Maximilian Balandat", "Eytan Bakshy"], "title": "LILO: Bayesian Optimization with Interactive Natural Language Feedback", "comment": null, "summary": "For many real-world applications, feedback is essential in translating\ncomplex, nuanced, or subjective goals into quantifiable optimization\nobjectives. We propose a language-in-the-loop framework that uses a large\nlanguage model (LLM) to convert unstructured feedback in the form of natural\nlanguage into scalar utilities to conduct BO over a numeric search space.\nUnlike preferential BO, which only accepts restricted feedback formats and\nrequires customized models for each domain-specific problem, our approach\nleverages LLMs to turn varied types of textual feedback into consistent utility\nsignals and to easily include flexible user priors without manual kernel\ndesign. At the same time, our method maintains the sample efficiency and\nprincipled uncertainty quantification of BO. We show that this hybrid method\nnot only provides a more natural interface to the decision maker but also\noutperforms conventional BO baselines and LLM-only optimizers, particularly in\nfeedback-limited regimes."}
{"id": "2510.17690", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17690", "abs": "https://arxiv.org/abs/2510.17690", "authors": ["Xihong Su"], "title": "Efficient Algorithms for Mitigating Uncertainty and Risk in Reinforcement Learning", "comment": "Dissertation", "summary": "This dissertation makes three main contributions. First, We identify a new\nconnection between policy gradient and dynamic programming in MMDPs and propose\nthe Coordinate Ascent Dynamic Programming (CADP) algorithm to compute a Markov\npolicy that maximizes the discounted return averaged over the uncertain models.\nCADP adjusts model weights iteratively to guarantee monotone policy\nimprovements to a local maximum. Second, We establish sufficient and necessary\nconditions for the exponential ERM Bellman operator to be a contraction and\nprove the existence of stationary deterministic optimal policies for ERM-TRC\nand EVaR-TRC. We also propose exponential value iteration, policy iteration,\nand linear programming algorithms for computing optimal stationary policies for\nERM-TRC and EVaR-TRC. Third, We propose model-free Q-learning algorithms for\ncomputing policies with risk-averse objectives: ERM-TRC and EVaR-TRC. The\nchallenge is that Q-learning ERM Bellman may not be a contraction. Instead, we\nuse the monotonicity of Q-learning ERM Bellman operators to derive a rigorous\nproof that the ERM-TRC and the EVaR-TRC Q-learning algorithms converge to the\noptimal risk-averse value functions. The proposed Q-learning algorithms compute\nthe optimal stationary policy for ERM-TRC and EVaR-TRC."}
{"id": "2510.17709", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17709", "abs": "https://arxiv.org/abs/2510.17709", "authors": ["Akhil S Anand", "Shambhuraj Sawant", "Jasper Hoffmann", "Dirk Reinhardt", "Sebastien Gros"], "title": "Closing the Sim2Real Performance Gap in RL", "comment": null, "summary": "Sim2Real aims at training policies in high-fidelity simulation environments\nand effectively transferring them to the real world. Despite the developments\nof accurate simulators and Sim2Real RL approaches, the policies trained purely\nin simulation often suffer significant performance drops when deployed in real\nenvironments. This drop is referred to as the Sim2Real performance gap. Current\nSim2Real RL methods optimize the simulator accuracy and variability as proxies\nfor real-world performance. However, these metrics do not necessarily correlate\nwith the real-world performance of the policy as established theoretically and\nempirically in the literature. We propose a novel framework to address this\nissue by directly adapting the simulator parameters based on real-world\nperformance. We frame this problem as a bi-level RL framework: the inner-level\nRL trains a policy purely in simulation, and the outer-level RL adapts the\nsimulation model and in-sim reward parameters to maximize real-world\nperformance of the in-sim policy. We derive and validate in simple examples the\nmathematical tools needed to develop bi-level RL algorithms that close the\nSim2Real performance gap."}
{"id": "2510.17727", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17727", "abs": "https://arxiv.org/abs/2510.17727", "authors": ["Ege Beyazit", "KL Navaneet", "Prashant Mathur", "Roi Blanco", "Vidit Bansal", "Karim Bouyarmane"], "title": "Enabling Fine-Grained Operating Points for Black-Box LLMs", "comment": "35 pages", "summary": "Black-box Large Language Models (LLMs) provide practical and accessible\nalternatives to other machine learning methods, as they require minimal labeled\ndata and machine learning expertise to develop solutions for various decision\nmaking problems. However, for applications that need operating with constraints\non specific metrics (e.g., precision $\\geq$ 95%), decision making with\nblack-box LLMs remains unfavorable, due to their low numerical output\ncardinalities. This results in limited control over their operating points,\npreventing fine-grained adjustment of their decision making behavior. In this\npaper, we study using black-box LLMs as classifiers, focusing on efficiently\nimproving their operational granularity without performance loss. Specifically,\nwe first investigate the reasons behind their low-cardinality numerical outputs\nand show that they are biased towards generating rounded but informative\nverbalized probabilities. Then, we experiment with standard prompt engineering,\nuncertainty estimation and confidence elicitation techniques, and observe that\nthey do not effectively improve operational granularity without sacrificing\nperformance or increasing inference cost. Finally, we propose efficient\napproaches to significantly increase the number and diversity of available\noperating points. Our proposed approaches provide finer-grained operating\npoints and achieve comparable to or better performance than the benchmark\nmethods across 11 datasets and 3 LLMs."}
{"id": "2510.17756", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17756", "abs": "https://arxiv.org/abs/2510.17756", "authors": ["Younghyun Koo", "Maryam Rahnemoonfar"], "title": "Prediction of Sea Ice Velocity and Concentration in the Arctic Ocean using Physics-informed Neural Network", "comment": "49 pages, 7 figures, submitted to Environmental Modelling & Software", "summary": "As an increasing amount of remote sensing data becomes available in the\nArctic Ocean, data-driven machine learning (ML) techniques are becoming widely\nused to predict sea ice velocity (SIV) and sea ice concentration (SIC).\nHowever, fully data-driven ML models have limitations in generalizability and\nphysical consistency due to their excessive reliance on the quantity and\nquality of training data. In particular, as Arctic sea ice entered a new phase\nwith thinner ice and accelerated melting, there is a possibility that an ML\nmodel trained with historical sea ice data cannot fully represent the\ndynamically changing sea ice conditions in the future. In this study, we\ndevelop physics-informed neural network (PINN) strategies to integrate physical\nknowledge of sea ice into the ML model. Based on the Hierarchical\nInformation-sharing U-net (HIS-Unet) architecture, we incorporate the physics\nloss function and the activation function to produce physically plausible SIV\nand SIC outputs. Our PINN model outperforms the fully data-driven model in the\ndaily predictions of SIV and SIC, even when trained with a small number of\nsamples. The PINN approach particularly improves SIC predictions in melting and\nearly freezing seasons and near fast-moving ice regions."}
{"id": "2510.17772", "categories": ["cs.LG", "stat.AP", "I.5.1"], "pdf": "https://arxiv.org/pdf/2510.17772", "abs": "https://arxiv.org/abs/2510.17772", "authors": ["Ryan A. Robinett", "Sophia A. Madejski", "Kyle Ruark", "Samantha J. Riesenfeld", "Lorenzo Orecchia"], "title": "Atlas-based Manifold Representations for Interpretable Riemannian Machine Learning", "comment": null, "summary": "Despite the popularity of the manifold hypothesis, current manifold-learning\nmethods do not support machine learning directly on the latent $d$-dimensional\ndata manifold, as they primarily aim to perform dimensionality reduction into\n$\\mathbb{R}^D$, losing key manifold features when the embedding dimension $D$\napproaches $d$.\n  On the other hand, methods that directly learn the latent manifold as a\ndifferentiable atlas have been relatively underexplored.\n  In this paper, we aim to give a proof of concept of the effectiveness and\npotential of atlas-based methods. To this end, we implement a generic data\nstructure to maintain a differentiable atlas that enables Riemannian\noptimization over the manifold. We complement this with an unsupervised\nheuristic that learns a differentiable atlas from point cloud data. We\nexperimentally demonstrate that this approach has advantages in terms of\nefficiency and accuracy in selected settings. Moreover, in a supervised\nclassification task over the Klein bottle and in RNA velocity analysis of\nhematopoietic data, we showcase the improved interpretability and robustness of\nour approach."}
{"id": "2510.17776", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17776", "abs": "https://arxiv.org/abs/2510.17776", "authors": ["Jackson Harmon", "Andreas Hochlehnert", "Matthias Bethge", "Ameya Prabhu"], "title": "Mapping Post-Training Forgetting in Language Models at Scale", "comment": "43 pages,15 figures", "summary": "Scaled post-training now drives many of the largest capability gains in\nlanguage models (LMs), yet its effect on pretrained knowledge remains poorly\nunderstood. Not all forgetting is equal: Forgetting one fact (e.g., a U.S.\npresident or an API call) does not \"average out\" by recalling another. Hence,\nwe propose a sample-wise paradigm to measure what is forgotten and when\nbackward transfer occurs. Our metric counts 1->0 transitions (correct before\npost-training, incorrect after) to quantify forgetting and 0->1 transitions to\nquantify backward transfer. Traditional task averages conflate these effects\nand obscure large changes. For multiple-choice benchmarks, we add\nchance-adjusted variants that subtract the expected contribution of random\nguessing from pre- and post-training accuracies. We apply this framework across\npost-training stages, model sizes, and data scales. Our large-scale analysis\nshows that: (1) Domain-continual pretraining induces moderate forgetting with\nlow-to-moderate backward transfer; (2) RL/SFT post-training applied to base\nmodels and Instruction tuning yields moderate-to-large backward transfer on\nmath and logic with overall low-to-moderate forgetting; (3) Applying RL/SFT to\ninstruction-tuned models is sensitive on data scale: at small scales, both\nforgetting and backward transfer are small; at larger scales, effects are mixed\nand warrant further study with better controls; (4) Model merging does not\nreliably mitigate forgetting. Overall, our framework offers a practical\nyardstick for mapping how post-training alters pretrained knowledge at scale --\nenabling progress towards generally capable AI systems."}
{"id": "2510.17786", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17786", "abs": "https://arxiv.org/abs/2510.17786", "authors": ["Adam Stecklov", "Noah El Rimawi-Fine", "Mathieu Blanchette"], "title": "Inference-Time Compute Scaling For Flow Matching", "comment": null, "summary": "Allocating extra computation at inference time has recently improved sample\nquality in large language models and diffusion-based image generation. In\nparallel, Flow Matching (FM) has gained traction in language, vision, and\nscientific domains, but inference-time scaling methods for it remain\nunder-explored. Concurrently, Kim et al., 2025 approach this problem but\nreplace the linear interpolant with a non-linear variance-preserving (VP)\ninterpolant at inference, sacrificing FM's efficient and straight sampling.\nAdditionally, inference-time compute scaling for flow matching has only been\napplied to visual tasks, like image generation. We introduce novel\ninference-time scaling procedures for FM that preserve the linear interpolant\nduring sampling. Evaluations of our method on image generation, and for the\nfirst time (to the best of our knowledge), unconditional protein generation,\nshow that I) sample quality consistently improves as inference compute\nincreases, and II) flow matching inference-time scaling can be applied to\nscientific domains."}
{"id": "2510.17794", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.17794", "abs": "https://arxiv.org/abs/2510.17794", "authors": ["Omer Haq"], "title": "Functional Distribution Networks (FDN)", "comment": "Submitted to ICLR 2026. Code will be released upon acceptance", "summary": "Modern probabilistic regressors often remain overconfident under distribution\nshift. We present Functional Distribution Networks (FDN), an input-conditioned\ndistribution over network weights that induces predictive mixtures whose\ndispersion adapts to the input. FDN is trained with a beta-ELBO and Monte Carlo\nsampling. We further propose an evaluation protocol that cleanly separates\ninterpolation from extrapolation and stresses OOD sanity checks (e.g., that\npredictive likelihood degrades under shift while in-distribution accuracy and\ncalibration are maintained). On standard regression tasks, we benchmark against\nstrong Bayesian, ensemble, dropout, and hypernetwork baselines under matched\nparameter and update budgets, and assess accuracy, calibration, and\nshift-awareness with standard diagnostics. Together, the framework and protocol\naim to make OOD-aware, well-calibrated neural regression practical and modular."}
{"id": "2510.17802", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.17802", "abs": "https://arxiv.org/abs/2510.17802", "authors": ["Rui Pan", "Yang Luo", "Yuxing Liu", "Yang You", "Tong Zhang"], "title": "Unbiased Gradient Low-Rank Projection", "comment": null, "summary": "Memory-efficient optimization is critical for training increasingly large\nlanguage models (LLMs). A popular strategy involves gradient low-rank\nprojection, storing only the projected optimizer states, with GaLore being a\nrepresentative example. However, a significant drawback of many such methods is\ntheir lack of convergence guarantees, as various low-rank projection approaches\nintroduce inherent biases relative to the original optimization algorithms,\nwhich contribute to performance gaps compared to full-parameter training.\nAiming to tackle this problem, this paper investigates the layerwise sampling\ntechnique for debiasing low-rank projection mechanisms. In particular, an\ninstantiation of the paradigm gives rise to a novel and unbiased low-rank\noptimization method built upon GaLore's mechanism and the Muon algorithm, named\nGaLore Unbiased with Muon (GUM). We theoretically prove our method matches the\nconvergence guarantees of the base Muon algorithm while preserving the memory\nefficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and\npretraining also demonstrate non-trivial improvements over GaLore and even\nbetter performance than full-parameter training. Further investigation shows\nthat the improvement of this technique comes from a more uniform distribution\nof knowledge inside layers, leading to more efficient utilization of the model\nparameter space and better memorization."}
{"id": "2510.16782", "categories": ["quant-ph", "cs.CC", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16782", "abs": "https://arxiv.org/abs/2510.16782", "authors": ["Tongyang Li", "Xinzhao Wang", "Yexin Zhang"], "title": "Near-Optimal Quantum Algorithms for Computing (Coarse) Correlated Equilibria of General-Sum Games", "comment": "Accepted at NeurIPS 2025, 27 pages", "summary": "Computing Nash equilibria of zero-sum games in classical and quantum settings\nis extensively studied. For general-sum games, computing Nash equilibria is\nPPAD-hard and the computing of a more general concept called correlated\nequilibria has been widely explored in game theory. In this paper, we initiate\nthe study of quantum algorithms for computing $\\varepsilon$-approximate\ncorrelated equilibria (CE) and coarse correlated equilibria (CCE) in\nmulti-player normal-form games. Our approach utilizes quantum improvements to\nthe multi-scale Multiplicative Weight Update (MWU) method for CE calculations,\nachieving a query complexity of $\\tilde{O}(m\\sqrt{n})$ for fixed $\\varepsilon$.\nFor CCE, we extend techniques from quantum algorithms for zero-sum games to\nmulti-player settings, achieving query complexity\n$\\tilde{O}(m\\sqrt{n}/\\varepsilon^{2.5})$. Both algorithms demonstrate a\nnear-optimal scaling in the number of players $m$ and actions $n$, as confirmed\nby our quantum query lower bounds."}
{"id": "2510.17642", "categories": ["quant-ph", "cs.DC", "cs.LG", "I.2; A.1"], "pdf": "https://arxiv.org/pdf/2510.17642", "abs": "https://arxiv.org/abs/2510.17642", "authors": ["Siva Sai", "Abhishek Sawaika", "Prabhjot Singh", "Rajkumar Buyya"], "title": "Quantum Federated Learning: Architectural Elements and Future Directions", "comment": "28 PAGES, 11 figures, introductory review article (book chapter), to\n  be published in a book with springer", "summary": "Federated learning (FL) focuses on collaborative model training without the\nneed to move the private data silos to a central server. Despite its several\nbenefits, the classical FL is plagued with several limitations, such as high\ncomputational power required for model training(which is critical for\nlow-resource clients), privacy risks, large update traffic, and non-IID\nheterogeneity. This chapter surveys a hybrid paradigm - Quantum Federated\nLearning (QFL), which introduces quantum computation, that addresses multiple\nchallenges of classical FL and offers rapid computing capability while keeping\nthe classical orchestration intact. Firstly, we motivate QFL with a concrete\npresentation on pain points of classical FL, followed by a discussion on a\ngeneral architecture of QFL frameworks specifying the roles of client and\nserver, communication primitives and the quantum model placement. We classify\nthe existing QFL systems based on four criteria - quantum architecture (pure\nQFL, hybrid QFL), data processing method (quantum data encoding, quantum\nfeature mapping, and quantum feature selection & dimensionality reduction),\nnetwork topology (centralized, hierarchial, decentralized), and quantum\nsecurity mechanisms (quantum key distribution, quantum homomorphic encryption,\nquantum differential privacy, blind quantum computing). We then describe\napplications of QFL in healthcare, vehicular networks, wireless networks, and\nnetwork security, clearly highlighting where QFL improves communication\nefficiency, security, and performance compared to classical FL. We close with\nmultiple challenges and future works in QFL, including extension of QFL beyond\nclassification tasks, adversarial attacks, realistic hardware deployment,\nquantum communication protocols deployment, aggregation of different quantum\nmodels, and quantum split learning as an alternative to QFL."}
