<div id=toc></div>

# Table of Contents

- [physics.comp-ph](#physics.comp-ph) [Total: 4]
- [cs.LG](#cs.LG) [Total: 281]
- [quant-ph](#quant-ph) [Total: 78]
- [gr-qc](#gr-qc) [Total: 39]


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [1] [Scalable learning of macroscopic stochastic dynamics](https://arxiv.org/abs/2511.12842)
*Mengyi Chen,Pengru Huang,Kostya S. Novoselov,Qianxiao Li*

Main category: physics.comp-ph

TL;DR: 提出一个框架，通过小系统模拟学习大型随机微观系统的宏观动力学，使用局部补丁演化生成训练数据，识别闭合变量，并采用分层上采样方案高效生成大系统快照。


<details>
  <summary>Details</summary>
Motivation: 对于空间扩展系统，直接模拟足够大的微观系统以获取宏观行为信息成本过高，需要一种能够仅使用小系统模拟就能学习宏观动力学的方法。

Method: 采用部分演化方案在局部补丁内演化大系统快照生成训练数据对，识别与宏观可观测量相关的闭合变量，使用自定义损失学习宏观动力学，并引入分层上采样方案从轨迹分布生成大系统快照。

Result: 通过多种随机空间扩展系统（包括随机偏微分方程、理想化晶格自旋系统和更现实的NbMoTa合金系统）实证验证了框架的准确性和鲁棒性。

Conclusion: 该框架能够仅使用小系统模拟有效学习大型随机微观系统的宏观动力学，为复杂物理系统的建模和控制提供了有前景的替代方案。

Abstract: Macroscopic dynamical descriptions of complex physical systems are crucial for understanding and controlling material behavior. With the growing availability of data and compute, machine learning has become a promising alternative to first-principles methods to build accurate macroscopic models from microscopic trajectory simulations. However, for spatially extended systems, direct simulations of sufficiently large microscopic systems that inform macroscopic behavior is prohibitive. In this work, we propose a framework that learns the macroscopic dynamics of large stochastic microscopic systems using only small-system simulations. Our framework employs a partial evolution scheme to generate training data pairs by evolving large-system snapshots within local patches. We subsequently identify the closure variables associated with the macroscopic observables and learn the macroscopic dynamics using a custom loss. Furthermore, we introduce a hierarchical upsampling scheme that enables efficient generation of large-system snapshots from small-system trajectory distributions. We empirically demonstrate the accuracy and robustness of our framework through a variety of stochastic spatially extended systems, including those described by stochastic partial differential equations, idealised lattice spin systems, and a more realistic NbMoTa alloy system.

</details>


### [2] [Orthogonal Attosecond Control of Solid-State Harmonics by Optical Waveforms and Quantum Geometry Engineering](https://arxiv.org/abs/2511.13114)
*Zhenjiang Zhao,Zhihua Zheng,Zhiyi Xu,Xing Ran,Xiaolong Yao,Fangping Ouyang*

Main category: physics.comp-ph

TL;DR: 通过结合双色激光场和机械应变工程，实现了对WS2单层材料高次谐波产生的精确正交控制，揭示了量子几何效应对谐波发射的显著增强作用。


<details>
  <summary>Details</summary>
Motivation: 二维材料中的高次谐波产生为实现紧凑型极紫外光源和探测阿秒尺度电子动力学提供了有前景的途径，但精确控制发射并解开带内和带间量子路径之间的复杂相互作用仍是一个核心挑战。

Method: 采用第一性原理模拟，结合全光学双色激光场和机械应变工程的双模策略，通过调控双色场的相对相位和施加拉伸应变来精确控制谐波发射。

Result: 双色场的相对相位可作为电子-空穴对量子相干的亚飞秒开关，最大化谐波发射；拉伸应变通过增强带内电流和显著重塑贝里曲率来放大反常速度贡献，使谐波产额与应变呈线性依赖关系，并显著增强垂直极化谐波。

Conclusion: 该研究为优化固态高次谐波产生建立了通用框架，并引入了一种强大的全光学方法来映射材料的应变和量子几何特性，将WS2单层定位为探索体相和原子尺度交界处阿秒物理的模型系统。

Abstract: High-harmonic generation (HHG) in two-dimensional materials offers a compelling route toward compact extreme ultraviolet sources and probing electron dynamics on the attosecond scale. However, achieving precise control over the emission and disentangling the complex interplay between intraband and interband quantum pathways remains a central challenge. Here, we demonstrate through first-principles simulations that HHG in monolayer WS2 can be subjected to precise, complementary control by combining all-optical two-color laser fields with mechanical strain engineering. This dual-mode strategy provides unprecedented, orthogonal control over harmonic yield, polarization, and spectral features. We reveal that sculpting the two-color field's relative phase provides a sub-femtosecond switch for the quantum coherence of electron-hole pairs, thereby maximizing harmonic emission. Crucially, we uncover that tensile strain acts as a powerful amplifier through a dual mechanism - while strain-modified band dispersion enhances the intraband current, a profound reshaping of the Berry curvature (BC) dramatically boosts the anomalous velocity contribution to the interband response. This quantum geometric effect manifests as a robust, linear dependence of the harmonic yield on strain and a significant amplification of the perpendicularly polarized harmonics, providing a clear experimental signature for probing quantum geometric effects. Our findings establish a versatile framework for optimizing solid-state HHG and introduce a powerful all-optical method to map strain and quantum geometric properties of materials, positioning monolayer WS2 as a model system for exploring attosecond physics at the nexus of bulk and atomic scales.

</details>


### [3] [Case study of a differentiable heterogeneous multiphysics solver for a nuclear fusion application](https://arxiv.org/abs/2511.13262)
*Jack B. Coughlin,Archis Joglekar,Jonathan Brodrick,Alexander Lavin*

Main category: physics.comp-ph

TL;DR: 该研究提出了一个核聚变领域的异构多物理场求解器案例研究，通过JAX中的自动微分ODE求解器计算Z箍缩的脉冲功率电路和体等离子体参数演化，使用Tesseract软件实现与生产级等离子体求解器Gkeyll的兼容，确保端到端可微分性。


<details>
  <summary>Details</summary>
Motivation: 解决在基于梯度的优化工作流中集成不可微分的生产级等离子体求解器（如Gkeyll）的挑战，实现多物理场模拟的可微分计算。

Method: 使用JAX中的自动微分ODE求解器，通过梯度牛顿迭代高效求解等离子体负载阻抗的闭合问题，并开发Tesseract软件作为多物理场可微分抽象层，通过tesseract_jax适配器与JAX完全兼容。

Result: 成功构建了端到端可微分的多物理场求解框架，能够无缝切换高保真求解器、神经代理模型和分析近似方法，实现快速渐进式原型开发。

Conclusion: Tesseract架构为异构多物理场模拟提供了有效的可微分抽象层解决方案，使不可微分的生产级求解器能够集成到基于梯度的优化工作流中。

Abstract: This work presents a case study of a heterogeneous multiphysics solver from the nuclear fusion domain. At the macroscopic scale, an auto-differentiable ODE solver in JAX computes the evolution of the pulsed power circuit and bulk plasma parameters for a compressing Z Pinch. The ODE solver requires a closure for the impedance of the plasma load obtained via root-finding at every timestep, which we solve efficiently using gradient-based Newton iteration. However, incorporating non-differentiable production-grade plasma solvers like Gkeyll (a C/CUDA plasma simulation suite) into a gradient-based workflow is non-trivial. The ''Tesseract'' software addresses this challenge by providing a multi-physics differentiable abstraction layer made fully compatible with JAX (through the `tesseract_jax` adapter). This architecture ensures end-to-end differentiability while allowing seamless interchange between high-fidelity solvers (Gkeyll), neural surrogates, and analytical approximations for rapid, progressive prototyping.

</details>


### [4] [Impacts of Stratospheric Aerosol Injection on Renewable Energy Systems](https://arxiv.org/abs/2511.13376)
*Sebastian Kebrich,Luisa Kamp,Jochen Linßen,Heidi Heinrichs*

Main category: physics.comp-ph

TL;DR: 该研究评估了每年向平流层注入20Mt二氧化硫对全球辐射平衡、光伏潜力和可再生能源系统的影响，目标是降低2°C温度。结果显示光伏潜力平均年减少0.25%-4%，北欧夏季可达12%，但可再生能源系统的灵活性能够吸收这些变化。


<details>
  <summary>Details</summary>
Motivation: 气候变化是21世纪主要挑战之一，减少温室气体排放进展缓慢，因此需要更激进的技术如平流层气溶胶注入来限制气候变化。

Method: 提出了一种评估方法，分析每年向大气注入20Mt SO₂对全球辐射平衡、光伏潜力和可再生能源系统的影响，目标温度降低2°C。

Result: 光伏潜力平均年减少0.25%-4%，北欧夏季可达12%。模型显示可再生能源系统能够吸收这些减少，仅产生较小的容量变化，少数系统有较大变化。

Conclusion: 大规模可再生能源系统的固有灵活性有助于缓解成本变化，但理解这种灵活性对于避免设计错误至关重要。

Abstract: Climate change is one of the 21st centurys major challenges. However, the progress in reducing greenhouse gas emissions is perceived as being too slow. Hence, more radical technologies such as stratospheric aerosol injection are entering discussions to limit climate change. This study presents a methodology for evaluating the effects of injecting 20Mt of SO$_2$ into the atmosphere annually on the global radiative balance, photovoltaic potentials, and renewable energy systems under a targeted temperature reduction of 2°C. Results show that the average annual reduction of PV potentials ranges from 0.25% to 4% up to 12% in Northern Europe during summer. The modeled renewable energy systems largely absorb these reductions resulting in minor capacity shifts with larger changes confined to a few systems. The results show that the inherent flexibility of large scale renewable energy systems helps mitigating changes in cost, but understanding this flexibility is crucial to avoid errors in design.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Softmax as a Lagrangian-Legendrian Seam](https://arxiv.org/abs/2511.11573)
*Christopher R. Lee-Jenkins*

Main category: cs.LG

TL;DR: 该论文将机器学习中的softmax函数建模为微分几何中的几何界面，展示了logits到概率的转换可以通过Legendre变换在接触几何框架下描述。


<details>
  <summary>Details</summary>
Motivation: 建立机器学习与微分几何之间的桥梁，揭示softmax函数背后的几何结构，为理解机器学习中的概率转换提供新的数学视角。

Method: 使用接触几何和辛几何框架，将softmax建模为两个势能生成的保守描述在概率单纯形上的Legendre变换界面。

Result: 证明了偏置平移不变性对应于屏幕上的Reeb流，Fenchel-Young等式/KL散度提供了到界面的可计算距离，并具体分析了二类和三类情况。

Conclusion: 为机器学习开辟了新的研究方向：紧致logit模型、全局不变量以及与信息几何的联系，其中屏幕上的动力学表现为复制子流。

Abstract: This note offers a first bridge from machine learning to modern differential geometry. We show that the logits-to-probabilities step implemented by softmax can be modeled as a geometric interface: two potential-generated, conservative descriptions (from negative entropy and log-sum-exp) meet along a Legendrian "seam" on a contact screen (the probability simplex) inside a simple folded symplectic collar. Bias-shift invariance appears as Reeb flow on the screen, and the Fenchel-Young equality/KL gap provides a computable distance to the seam. We work out the two- and three-class cases to make the picture concrete and outline next steps for ML: compact logit models (projective or spherical), global invariants, and connections to information geometry where on-screen dynamics manifest as replicator flows.

</details>


### [6] [LLM on a Budget: Active Knowledge Distillation for Efficient Classification of Large Text Corpora](https://arxiv.org/abs/2511.11574)
*Viviana Luccioli,Rithika Iyengar,Ryan Panley,Flora Haberkorn,Xiaoyu Ge,Leland Crane,Nitish Sinha,Seung Jung Lee*

Main category: cs.LG

TL;DR: 提出M-RARU主动学习算法，结合不确定性和随机接受-拒绝机制，显著减少大语言模型知识蒸馏所需的训练样本和成本


<details>
  <summary>Details</summary>
Motivation: 大语言模型在分类任务中准确率高，但计算和财务成本高，难以在动态环境中大规模部署。知识蒸馏过程本身也需要大量样本标注和token消耗

Method: 提出M-RARU主动学习算法，结合不确定性和随机接受-拒绝机制，选择信息量最大的数据点供LLM教师标注，减少API调用和数据处理时间

Result: 在五个学生模型和多个基准数据集上评估，相比随机采样，M-RARU最多减少80%样本需求，显著提高分类准确率，降低财务成本和训练时间

Conclusion: M-RARU是一种高效的主动学习方法，能够在保持LLM性能的同时，大幅降低知识蒸馏的成本和资源消耗

Abstract: Large Language Models (LLMs) are highly accurate in classification tasks, however, substantial computational and financial costs hinder their large-scale deployment in dynamic environments. Knowledge Distillation (KD) where a LLM "teacher" trains a smaller and more efficient "student" model, offers a promising solution to this problem. However, the distillation process itself often remains costly for large datasets, since it requires the teacher to label a vast number of samples while incurring significant token consumption. To alleviate this challenge, in this work we explore the active learning (AL) as a way to create efficient student models at a fraction of the cost while preserving the LLM's performance. In particular, we introduce M-RARU (Multi-class Randomized Accept/Reject Uncertainty Sampling), a novel AL algorithm that significantly reduces training costs. M-RARU employs an innovative strategy combining uncertainty with a randomized accept-reject mechanism to select only the most informative data points for the LLM teacher. This focused approach significantly minimizes required API calls and data processing time. We evaluate M-RARU against random sampling across five diverse student models (SVM, LDA, RF, GBDT, and DistilBERT) on multiple benchmark datasets. Experiments demonstrate that our proposed method achieves up to 80% reduction in sample requirements as compared to random sampling, substantially improving classification accuracy while reducing financial costs and overall training time.

</details>


### [7] [Detecting Statistically Significant Fairness Violations in Recidivism Forecasting Algorithms](https://arxiv.org/abs/2511.11575)
*Animesh Joshi*

Main category: cs.LG

TL;DR: 本文提出了一个统计显著性测试框架，用于检测机器学习算法中的公平性违规，并通过累犯预测案例展示了不同公平性定义下对黑人和白人群体存在显著偏见的情况。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在关键领域部署增加，现有文献缺乏评估群体间差异是否具有统计显著性的方法，需要建立严谨的公平性统计测试框架。

Method: 利用k折交叉验证生成公平性指标的抽样分布，开发基于预测与实际结果差异、模型校准和因果推断技术的统计显著性测试。

Result: 在累犯预测算法中发现，根据不同的公平性定义，算法对黑人个体存在统计显著的偏见，而在其他定义下则对白人存在偏见或无偏见。

Conclusion: 评估算法决策系统时，严谨和稳健的统计测试至关重要，公平性评估应考虑不同定义下的统计显著性。

Abstract: Machine learning algorithms are increasingly deployed in critical domains such as finance, healthcare, and criminal justice [1]. The increasing popularity of algorithmic decision-making has stimulated interest in algorithmic fairness within the academic community. Researchers have introduced various fairness definitions that quantify disparities between privileged and protected groups, use causal inference to determine the impact of race on model predictions, and that test calibration of probability predictions from the model. Existing literature does not provide a way in which to assess whether observed disparities between groups are statistically significant or merely due to chance. This paper introduces a rigorous framework for testing the statistical significance of fairness violations by leveraging k-fold cross-validation [2] to generate sampling distributions of fairness metrics. This paper introduces statistical tests that can be used to identify statistically significant violations of fairness metrics based on disparities between predicted and actual outcomes, model calibration, and causal inference techniques [1]. We demonstrate this approach by testing recidivism forecasting algorithms trained on data from the National Institute of Justice. Our findings reveal that machine learning algorithms used for recidivism forecasting exhibit statistically significant bias against Black individuals under several fairness definitions, while also exhibiting no bias or bias against White individuals under other definitions. The results from this paper underscore the importance of rigorous and robust statistical testing while evaluating algorithmic decision-making systems.

</details>


### [8] [DAOpt: Modeling and Evaluation of Data-Driven Optimization under Uncertainty with LLMs](https://arxiv.org/abs/2511.11576)
*WenZhuo Zhu,Zheng Cui,Wenhan Lu,Sheng Liu,Yue Zhao*

Main category: cs.LG

TL;DR: 提出了DAOpt框架，包括OptU数据集、多智能体决策模块和仿真环境，用于评估LLM在不确定优化中的样本外可行性和鲁棒性，并通过小样本学习增强LLM的建模能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界的决策本质上是存在不确定性的，但现有研究主要关注参数已知的确定性优化，LLM在不确定环境下的应用仍未被充分探索。

Method: 提出DAOpt框架，包含新的数据集OptU、多智能体决策模块和仿真环境，结合随机优化和鲁棒优化的领域知识进行小样本学习。

Result: 开发了一个专门用于评估LLM在不确定优化中性能的框架，重点关注样本外可行性和鲁棒性。

Conclusion: 该研究填补了LLM在不确定优化建模领域的空白，为评估和增强LLM在现实世界决策中的能力提供了系统方法。

Abstract: Recent advances in large language models (LLMs) have accelerated research on automated optimization modeling. While real-world decision-making is inherently uncertain, most existing work has focused on deterministic optimization with known parameters, leaving the application of LLMs in uncertain settings largely unexplored. To that end, we propose the DAOpt framework including a new dataset OptU, a multi-agent decision-making module, and a simulation environment for evaluating LLMs with a focus on out-of-sample feasibility and robustness. Additionally, we enhance LLMs' modeling capabilities by incorporating few-shot learning with domain knowledge from stochastic and robust optimization.

</details>


### [9] [Decoupling Positional and Symbolic Attention Behavior in Transformers](https://arxiv.org/abs/2511.11579)
*Felipe Urrutia,Jorge Salas,Alexander Kozachinskiy,Cristian Buc Calderon,Hector Pasten,Cristobal Rojas*

Main category: cs.LG

TL;DR: 本文深入研究了Transformer中注意力头的位置性与符号性行为二分法，提出了量化这两种行为的方法，并证明RoPE的成功源于其使用不同频率分别编码位置和语义信息的能力。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer中位置编码（特别是RoPE）如何分别编码位置信息和符号信息，以及注意力头如何利用这些信息来执行语言理解任务。

Method: 提出了位置性和符号性行为的通用定义，证明这两种行为互斥，开发了量化指标，并在使用RoPE的LLMs上应用该框架分析，设计了纯粹位置性和符号性的典型任务。

Result: 发现所有注意力头的行为与频率使用之间存在强相关性，可以通过控制注意力头可访问的频率来因果性地控制Transformer的性能。

Conclusion: 研究提供了对RoPE的详细理解，阐明了其特性与模型行为之间的关系，表明RoPE的成功源于其能够使用不同频率分别编码位置和语义信息。

Abstract: An important aspect subtending language understanding and production is the ability to independently encode positional and symbolic information of the words within a sentence. In Transformers, positional information is typically encoded using Positional Encodings (PEs). One such popular PE, namely Rotary PE (RoPE), has been widely used due to its empirical success. Recently, it has been argued that part of RoPE's success emerges from its ability to encode robust positional and semantic information using large and small frequencies, respectively. In this work, we perform a deeper dive into the positional versus symbolic dichotomy of attention heads behavior, both at the theoretical and empirical level. We provide general definitions of what it means for a head to behave positionally or symbolically, prove that these are two mutually exclusive behaviors and develop a metric to quantify them. We apply our framework to analyze Transformer-based LLMs using RoPE and find that all heads exhibit a strong correspondence between behavior and frequency use. Finally, we introduce canonical tasks designed to be either purely positional or symbolic, and demonstrate that the Transformer performance causally relates to the ability of attention heads to leverage the appropriate frequencies. In particular, we show that we can control the Transformer performance by controlling which frequencies the attention heads can access. Altogether, our work provides a detailed understanding of RoPE, and how its properties relate to model behavior.

</details>


### [10] [FGM optimization in complex domains using Gaussian process regression based profile generation algorithm](https://arxiv.org/abs/2511.12171)
*Chaitanya Kumar Konda,Piyush Agrawal,Shivansh Srivastava,Manish Agrawal*

Main category: cs.LG

TL;DR: 提出了一种基于高斯过程回归的功能梯度材料体积分数分布生成算法，能够处理复杂形状域并生成平滑的FGM分布，结合遗传算法进行优化设计。


<details>
  <summary>Details</summary>
Motivation: 解决任意形状域中功能梯度材料设计的挑战，需要一种通用的体积分数分布生成方法。

Method: 使用高斯过程回归生成FGM体积分数分布，结合改进的遗传算法（使用投影算子替代标准模拟二进制交叉）进行优化。

Result: 算法能够处理复杂形状域，生成平滑的FGM分布，并通过长度尺度参数控制分布平滑度和设计空间大小。

Conclusion: 提出的方法在热弹性优化示例中表现出良好效果，为功能梯度材料设计提供了有效的通用框架。

Abstract: This manuscript addresses the challenge of designing functionally graded materials (FGMs) for arbitrary-shaped domains. Towards this goal, the present work proposes a generic volume fraction profile generation algorithm based on Gaussian Process Regression (GPR). The proposed algorithm can handle complex-shaped domains and generate smooth FGM profiles while adhering to the specified volume fraction values at boundaries/part of boundaries. The resulting design space from GPR comprises diverse profiles, enhancing the potential for discovering optimal configurations. Further, the algorithm allows the user to control the smoothness of the underlying profiles and the size of the design space through a length scale parameter. Further, the proposed profile generation scheme is coupled with the genetic algorithm to find the optimum FGM profiles for a given application. To make the genetic algorithm consistent with the GPR profile generation scheme, the standard simulated binary crossover operator in the genetic algorithm has been modified with a projection operator. We present numerous thermoelastic optimization examples to demonstrate the efficacy of the proposed profile generation algorithm and optimization framework.

</details>


### [11] [The Anatomy of a Triton Attention Kernel](https://arxiv.org/abs/2511.11581)
*Burkhard Ringlein,Jan van Lunteren,Radu Stoica,Thomas Parnell*

Main category: cs.LG

TL;DR: 开发了基于Triton DSL的跨平台LLM推理系统，通过paged attention内核在NVIDIA和AMD GPU上实现最佳性能，将通用Triton注意力内核性能从19.7%提升到105.9%。


<details>
  <summary>Details</summary>
Motivation: 解决LLM推理平台在跨硬件架构可移植性、消除低层手动调优需求，同时保持最佳效率的长期挑战。

Method: 使用Triton领域特定语言开发paged attention内核，结合算法和系统级改进、参数自动调优，并集成到流行的推理服务器中。

Result: 在NVIDIA和AMD GPU上实现最先进性能，将通用Triton注意力内核性能从19.7%提升到105.9%。

Conclusion: 开源领域特定语言可以用于解锁跨不同GPU厂商的模型可移植性。

Abstract: A long-standing goal in both industry and academia is to develop an LLM inference platform that is portable across hardware architectures, eliminates the need for low-level hand-tuning, and still delivers best-in-class efficiency. In this work, we demonstrate that portable, efficient cross-platform LLM inference is indeed possible and share our experience. We develop a state-of-the-art paged attention kernel, the core performance-critical component of many LLM deployments, that builds exclusively on the domain-specific just-in-time compiled language Triton to achieve state-of-the-art performance on both NVIDIA and AMD GPUs. We describe our high-level approach, the key algorithmic and system-level improvements, the parameter auto-tuning required to unlock efficiency, and the integrations into a popular inference server that are necessary to bring the performance of a generic Triton attention kernel from 19.7% of the state-of-the-art to 105.9%. Our results highlight how open-source domain-specific languages can be leveraged to unlock model portability across different GPU vendors.

</details>


### [12] [Parallel and Multi-Stage Knowledge Graph Retrieval for Behaviorally Aligned Financial Asset Recommendations](https://arxiv.org/abs/2511.11583)
*Fernando Spadea,Oshani Seneviratne*

Main category: cs.LG

TL;DR: RAG-FLARKO通过多阶段并行知识图谱检索增强FLARKO框架，解决LLM在金融推荐中的上下文限制和幻觉问题，提升推荐质量和效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在个性化金融推荐中存在上下文限制、幻觉问题和缺乏行为基础等挑战，需要更有效的方法来整合用户行为和市场数据。

Method: 采用多阶段并行知识图谱检索：首先从用户交易知识图谱中检索行为相关实体，然后用此上下文过滤市场知识图谱中的时间一致性信号，构建紧凑的接地子图供LLM使用。

Result: 在真实金融交易数据集上的实证评估显示，RAG-FLARKO显著提升推荐质量，使更小更高效的模型在盈利性和行为对齐方面都能达到高性能。

Conclusion: 该框架为在资源受限环境中部署接地的金融AI提供了可行路径，通过减少上下文开销和聚焦相关信息来增强模型性能。

Abstract: Large language models (LLMs) show promise for personalized financial recommendations but are hampered by context limits, hallucinations, and a lack of behavioral grounding. Our prior work, FLARKO, embedded structured knowledge graphs (KGs) in LLM prompts to align advice with user behavior and market data. This paper introduces RAG-FLARKO, a retrieval-augmented extension to FLARKO, that overcomes scalability and relevance challenges using multi-stage and parallel KG retrieval processes. Our method first retrieves behaviorally relevant entities from a user's transaction KG and then uses this context to filter temporally consistent signals from a market KG, constructing a compact, grounded subgraph for the LLM. This pipeline reduces context overhead and sharpens the model's focus on relevant information. Empirical evaluation on a real-world financial transaction dataset demonstrates that RAG-FLARKO significantly enhances recommendation quality. Notably, our framework enables smaller, more efficient models to achieve high performance in both profitability and behavioral alignment, presenting a viable path for deploying grounded financial AI in resource-constrained environments.

</details>


### [13] [Output Supervision Can Obfuscate the Chain of Thought](https://arxiv.org/abs/2511.11584)
*Jacob Drori,Luke Marks,Bryce Woodworth,Alex Cloud,Alexander Matt Turner*

Main category: cs.LG

TL;DR: 训练模型仅使用输出监控器（无法访问思维链）仍会导致隐蔽的思维链，通过两种机制：安全输出训练使思维链看起来安全，以及安全思维链增加安全输出概率。作者提出了两种缓解方法，在可监控性和任务性能上实现帕累托改进。


<details>
  <summary>Details</summary>
Motivation: OpenAI发现训练对抗思维链监控器会导致隐蔽的思维链，他们建议仅使用输出监控器训练。但本文发现即使这样仍会导致隐蔽思维链，需要进一步研究缓解方法。

Method: 识别了两种导致隐蔽思维链的机制，并提出了相应的两种缓解方法来解决这些问题。

Result: 提出的缓解方法在可监控性和任务性能方面相比常规训练实现了帕累托改进。

Conclusion: 仅使用输出监控器训练不足以防止隐蔽思维链，需要额外的缓解措施来确保思维链的可监控性。

Abstract: OpenAI (2025) showed that training against a chain of thought (CoT) monitor can cause obfuscated CoTs, which contain bad behavior the monitor cannot detect. They proposed to keep CoTs monitorable by training only against output monitors that do not have access to CoT. We show that such training can still cause obfuscated CoTs via two mechanisms. First, when a model is trained to produce a safe-looking output, that model may generalize to making its CoTs look safe. Second, since later tokens are conditioned on earlier ones, safe-looking CoTs may increase the likelihood of safe outputs, causing safe-looking CoTs to be reinforced. We introduce two mitigations to address these two issues, which achieve a Pareto improvement in terms of monitorability and task performance compared to regular training.

</details>


### [14] [Parameter-Efficient and Personalized Federated Training of Generative Models at the Edge](https://arxiv.org/abs/2511.11585)
*Kabir Khan,Manju Sarkar,Anita Kar,Suresh Ghosh*

Main category: cs.LG

TL;DR: FedGen-Edge框架通过解耦预训练全局主干网络和轻量级客户端适配器，仅联邦化适配器，使用LoRA技术减少99%以上的上行流量，在非IID数据下稳定聚合，并支持个性化。


<details>
  <summary>Details</summary>
Motivation: 大型生成模型在跨设备联邦学习中面临计算通信负担重和统计/系统异构性问题，需要资源高效且支持个性化的解决方案。

Method: 采用LoRA约束客户端更新到紧凑子空间，仅联邦化轻量级适配器而非完整模型，保持预训练主干网络冻结。

Result: 在语言建模和图像生成任务上，相比强基线实现了更低的困惑度/FID和更快的收敛速度，同时保持简单的FedAvg风格服务器。

Conclusion: FedGen-Edge为异构边缘设备上的隐私保护、资源感知和个性化生成AI提供了实用路径。

Abstract: Large generative models (for example, language and diffusion models) enable high-quality text and image synthesis but are hard to train or adapt in cross-device federated settings due to heavy computation and communication and statistical/system heterogeneity. We propose FedGen-Edge, a framework that decouples a frozen, pre-trained global backbone from lightweight client-side adapters and federates only the adapters. Using Low-Rank Adaptation (LoRA) constrains client updates to a compact subspace, which reduces uplink traffic by more than 99 percent versus full-model FedAvg, stabilizes aggregation under non-IID data, and naturally supports personalization because each client can keep a locally tuned adapter. On language modeling (PTB) and image generation (CIFAR-10), FedGen-Edge achieves lower perplexity/FID and faster convergence than strong baselines while retaining a simple FedAvg-style server. A brief ablation shows diminishing returns beyond moderate LoRA rank and a trade-off between local epochs and client drift. FedGen-Edge offers a practical path toward privacy-preserving, resource-aware, and personalized generative AI on heterogeneous edge devices.

</details>


### [15] [WildfireGenome: Interpretable Machine Learning Reveals Local Drivers of Wildfire Risk and Their Cross-County Variation](https://arxiv.org/abs/2511.11589)
*Chenyue Liu,Ali Mostafavi*

Main category: cs.LG

TL;DR: WildfireGenome提出了一种可解释的野火风险评估方法，通过融合多个联邦指标、随机森林分类和SHAP/ICE分析，在H3 Level-8分辨率提供决策尺度的风险分析。


<details>
  <summary>Details</summary>
Motivation: 现有野火风险评估依赖粗糙的风险地图和不透明的机器学习模型，在区域精度优化的同时牺牲了决策尺度的可解释性。

Method: 融合7个联邦野火指标构建PCA复合风险标签，使用随机森林进行风险分类，并通过SHAP和ICE/PDP分析揭示县级非线性驱动关系。

Result: 在7个生态多样化的美国县，模型准确率达0.755-0.878，二次加权Kappa达0.951，主成分解释87-94%的指标方差。生态相似区域间性能稳定，但生态差异大时性能下降。

Conclusion: WildfireGenome将野火风险评估从区域预测推进到可解释的决策尺度分析，为植被管理、分区规划和基础设施规划提供指导。

Abstract: Current wildfire risk assessments rely on coarse hazard maps and opaque machine learning models that optimize regional accuracy while sacrificing interpretability at the decision scale. WildfireGenome addresses these gaps through three components: (1) fusion of seven federal wildfire indicators into a sign-aligned, PCA-based composite risk label at H3 Level-8 resolution; (2) Random Forest classification of local wildfire risk; and (3) SHAP and ICE/PDP analyses to expose county-specific nonlinear driver relationships. Across seven ecologically diverse U.S. counties, models achieve accuracies of 0.755-0.878 and Quadratic Weighted Kappa up to 0.951, with principal components explaining 87-94% of indicator variance. Transfer tests show reliable performance between ecologically similar regions but collapse across dissimilar contexts. Explanations consistently highlight needleleaf forest cover and elevation as dominant drivers, with risk rising sharply at 30-40% needleleaf coverage. WildfireGenome advances wildfire risk assessment from regional prediction to interpretable, decision-scale analytics that guide vegetation management, zoning, and infrastructure planning.

</details>


### [16] [Mind Your Entropy: From Maximum Entropy to Trajectory Entropy-Constrained RL](https://arxiv.org/abs/2511.11592)
*Guojian Zhan,Likun Wang,Pengcheng Wang,Feihong Zhang,Jingliang Duan,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 提出了轨迹熵约束强化学习(TECRL)框架，通过分离奖励和熵的Q函数学习来解决最大熵RL中的非平稳Q值估计和短视局部熵调整问题，并开发了DSAC-E算法。


<details>
  <summary>Details</summary>
Motivation: 最大熵RL框架存在两个瓶颈：1) 温度参数更新与熵注入共同导致的非平稳Q值估计；2) 仅基于当前单步熵的短视局部熵调整，未考虑累积熵的时间效应。

Method: 提出TECRL框架：1) 分别学习奖励Q函数和熵Q函数，确保值目标不受温度更新影响；2) 通过专用熵Q函数量化期望累积熵，实施轨迹熵约束以控制策略长期随机性。基于此开发DSAC-E算法。

Result: 在OpenAI Gym基准测试中，DSAC-E能够获得更高的回报和更好的稳定性。

Conclusion: TECRL框架通过分离Q函数学习和轨迹熵约束，有效解决了最大熵RL中的关键问题，DSAC-E算法在性能和稳定性方面表现优异。

Abstract: Maximum entropy has become a mainstream off-policy reinforcement learning (RL) framework for balancing exploitation and exploration. However, two bottlenecks still limit further performance improvement: (1) non-stationary Q-value estimation caused by jointly injecting entropy and updating its weighting parameter, i.e., temperature; and (2) short-sighted local entropy tuning that adjusts temperature only according to the current single-step entropy, without considering the effect of cumulative entropy over time. In this paper, we extends maximum entropy framework by proposing a trajectory entropy-constrained reinforcement learning (TECRL) framework to address these two challenges. Within this framework, we first separately learn two Q-functions, one associated with reward and the other with entropy, ensuring clean and stable value targets unaffected by temperature updates. Then, the dedicated entropy Q-function, explicitly quantifying the expected cumulative entropy, enables us to enforce a trajectory entropy constraint and consequently control the policy long-term stochasticity. Building on this TECRL framework, we develop a practical off-policy algorithm, DSAC-E, by extending the state-of-the-art distributional soft actor-critic with three refinements (DSAC-T). Empirical results on the OpenAI Gym benchmark demonstrate that our DSAC-E can achieve higher returns and better stability.

</details>


### [17] [Sound Logical Explanations for Mean Aggregation Graph Neural Networks](https://arxiv.org/abs/2511.11593)
*Matthew Morris,Ian Horrocks*

Main category: cs.LG

TL;DR: 该论文研究了使用均值聚合和非负权重的图神经网络(MAGNNs)，证明了它们能精确表达的单调规则类别，并提供了解释MAGNN预测的一阶逻辑片段。实验表明这种限制在标准基准测试中表现相当或更好，并能生成有意义的解释。


<details>
  <summary>Details</summary>
Motivation: 尽管使用均值聚合的GNN很常见，但缺乏对其可解释性和表达能力的理论研究。作者希望填补这一空白，特别是针对均值聚合和非负权重的GNN。

Method: 理论分析MAGNNs能表达的单调规则类别，提供一阶逻辑片段来解释预测，并通过实验验证在标准归纳基准上的性能。

Result: 限制均值聚合GNN使用非负权重在标准基准测试中表现相当或更好，实践中能获得可靠规则，生成有洞察力的解释，并能暴露训练模型中的问题。

Conclusion: MAGNNs具有明确的表达能力边界，能提供理论保证的解释，同时在实际应用中保持良好性能，为可解释的图神经网络提供了理论基础。

Abstract: Graph neural networks (GNNs) are frequently used for knowledge graph completion. Their black-box nature has motivated work that uses sound logical rules to explain predictions and characterise their expressivity. However, despite the prevalence of GNNs that use mean as an aggregation function, explainability and expressivity results are lacking for them. We consider GNNs with mean aggregation and non-negative weights (MAGNNs), proving the precise class of monotonic rules that can be sound for them, as well as providing a restricted fragment of first-order logic to explain any MAGNN prediction. Our experiments show that restricting mean-aggregation GNNs to have non-negative weights yields comparable or improved performance on standard inductive benchmarks, that sound rules are obtained in practice, that insightful explanations can be generated in practice, and that the sound rules can expose issues in the trained models.

</details>


### [18] [Loss Given Default Prediction Under Measurement-Induced Mixture Distributions: An Information-Theoretic Approach](https://arxiv.org/abs/2511.11596)
*Javier Marín*

Main category: cs.LG

TL;DR: 论文分析了LGD建模中90%训练数据为代理估计而非实际恢复结果的问题，发现随机森林等递归划分方法在这种混合污染训练结构下表现不佳，而基于信息论的方法具有更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: LGD建模面临数据质量约束，90%的训练数据是基于破产前资产负债表的代理估计，而非实际恢复结果，这种混合污染训练结构导致传统方法失效。

Method: 使用香农熵和互信息的信息论方法，分析了1,218个公司破产案例（1980-2023年），比较了递归划分方法和信息论方法的性能。

Result: 随机森林在测试数据上r平方为-0.664，而信息论方法达到0.191的r平方和0.284的RMSE。杠杆特征包含1.510比特互信息，规模效应仅贡献0.086比特。

Conclusion: 信息论方法在缺乏代表性结果数据时优于传统方法，为金融机构部署LGD模型提供实用指导，这些发现在医疗结果研究、气候预测等领域也具有普适性。

Abstract: Loss Given Default (LGD) modeling faces a fundamental data quality constraint: 90% of available training data consists of proxy estimates based on pre-distress balance sheets rather than actual recovery outcomes from completed bankruptcy proceedings. We demonstrate that this mixture-contaminated training structure causes systematic failure of recursive partitioning methods, with Random Forest achieving negative r-squared (-0.664, worse than predicting the mean) on held-out test data. Information-theoretic approaches based on Shannon entropy and mutual information provide superior generalization, achieving r-squared of 0.191 and RMSE of 0.284 on 1,218 corporate bankruptcies (1980-2023). Analysis reveals that leverage-based features contain 1.510 bits of mutual information while size effects contribute only 0.086 bits, contradicting regulatory assumptions about scale-dependent recovery. These results establish practical guidance for financial institutions deploying LGD models under Basel III requirements when representative outcome data is unavailable at sufficient scale. The findings generalize to medical outcomes research, climate forecasting, and technology reliability-domains where extended observation periods create unavoidable mixture structure in training data.

</details>


### [19] [Aspiration-based Perturbed Learning Automata in Games with Noisy Utility Measurements. Part A: Stochastic Stability in Non-zero-Sum Games](https://arxiv.org/abs/2511.11602)
*Georgios C. Chasparis*

Main category: cs.LG

TL;DR: 本文提出了一种基于期望的扰动学习自动机(APLA)用于分布式优化，解决了传统强化学习在多玩家弱非循环游戏中无法保证收敛到纯纳什均衡的问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在分布式设置中存在局限性，特别是在多玩家弱非循环游戏中，独立应用学习动态无法保证收敛到理想的纯纳什均衡。现有研究仅关注潜在博弈和协调博弈等小类游戏。

Method: 提出了APLA学习方案，其中每个玩家的动作选择概率分布不仅通过重复选择来强化，还通过捕捉玩家满意度的期望因子来强化。在存在噪声观测的情况下，对多玩家正效用博弈进行了随机稳定性分析。

Result: 首次在一般非零和博弈中建立了诱导的无限维马尔可夫链与有限维马尔可夫链的等价性，从而表征了随机稳定性。在弱非循环游戏中进一步专门化了随机稳定性分析。

Conclusion: APLA方案能够有效解决传统强化学习在分布式优化中的收敛问题，特别是在弱非循环游戏中保证收敛到纯纳什均衡。

Abstract: Reinforcement-based learning has attracted considerable attention both in modeling human behavior as well as in engineering, for designing measurement- or payoff-based optimization schemes. Such learning schemes exhibit several advantages, especially in relation to filtering out noisy observations. However, they may exhibit several limitations when applied in a distributed setup. In multi-player weakly-acyclic games, and when each player applies an independent copy of the learning dynamics, convergence to (usually desirable) pure Nash equilibria cannot be guaranteed. Prior work has only focused on a small class of games, namely potential and coordination games. To address this main limitation, this paper introduces a novel payoff-based learning scheme for distributed optimization, namely aspiration-based perturbed learning automata (APLA). In this class of dynamics, and contrary to standard reinforcement-based learning schemes, each player's probability distribution for selecting actions is reinforced both by repeated selection and an aspiration factor that captures the player's satisfaction level. We provide a stochastic stability analysis of APLA in multi-player positive-utility games under the presence of noisy observations. This is the first part of the paper that characterizes stochastic stability in generic non-zero-sum games by establishing equivalence of the induced infinite-dimensional Markov chain with a finite dimensional one. In the second part, stochastic stability is further specialized to weakly acyclic games.

</details>


### [20] [Enhancing failure prediction in nuclear industry: Hybridization of knowledge- and data-driven techniques](https://arxiv.org/abs/2511.11604)
*Amaratou Mahamadou Saley,Thierry Moyaux,Aïcha Sekhari,Vincent Cheutet,Jean-Baptiste Danielou*

Main category: cs.LG

TL;DR: 提出了一种结合数据驱动技术和核工业领域知识的预测性维护方法，在核工业中显著优于纯数据驱动方法，将预测时间从3小时延长到24小时，F1分数从56.36%提升到93.12%。


<details>
  <summary>Details</summary>
Motivation: 物联网和工业4.0的融合增强了核工业的数据驱动方法，但纯数据驱动方法在复杂核系统中需要大量领域知识，因此需要开发结合领域知识的混合方法。

Method: 提出了一种新颖的预测性维护方法，将数据驱动技术与核设备领域知识相结合，强调纯数据驱动方法的局限性并展示知识在提升预测模型性能中的重要性。

Result: 通过详细的实际案例研究比较，混合方法显著优于纯数据驱动方法：预测时间从3小时延长到24小时，F1分数从56.36%提升到93.12%。

Conclusion: 在核工业等高度受限和敏感的领域，结合领域知识的混合预测性维护方法比纯数据驱动方法更有效，能够显著提升故障预测性能。

Abstract: The convergence of the Internet of Things (IoT) and Industry 4.0 has significantly enhanced data-driven methodologies within the nuclear industry, notably enhancing safety and economic efficiency. This advancement challenges the precise prediction of future maintenance needs for assets, which is crucial for reducing downtime and operational costs. However, the effectiveness of data-driven methodologies in the nuclear sector requires extensive domain knowledge due to the complexity of the systems involved. Thus, this paper proposes a novel predictive maintenance methodology that combines data-driven techniques with domain knowledge from a nuclear equipment. The methodological originality of this paper is located on two levels: highlighting the limitations of purely data-driven approaches and demonstrating the importance of knowledge in enhancing the performance of the predictive models. The applicative novelty of this work lies in its use within a domain such as a nuclear industry, which is highly restricted and ultrasensitive due to security, economic and environmental concerns. A detailed real-world case study which compares the current state of equipment monitoring with two scenarios, demonstrate that the methodology significantly outperforms purely data-driven methods in failure prediction. While purely data-driven methods achieve only a modest performance with a prediction horizon limited to 3 h and a F1 score of 56.36%, the hybrid approach increases the prediction horizon to 24 h and achieves a higher F1 score of 93.12%.

</details>


### [21] [Clustering-Based Weight Orthogonalization for Stabilizing Deep Reinforcement Learning](https://arxiv.org/abs/2511.11607)
*Guoqing Ma,Yuhan Zhang,Yuming Dai,Guangfu Hao,Yang Chen,Shan Yu*

Main category: cs.LG

TL;DR: 提出COWM层来解决RL中的非平稳性问题，通过聚类技术和投影矩阵提高学习效率，在DMControl基准上取得显著提升


<details>
  <summary>Details</summary>
Motivation: RL智能体通常假设环境是平稳的，但实际环境中存在非平稳性，导致需要数百万次迭代，样本效率低下

Method: 引入COWM层，可集成到任何RL算法的策略网络中，使用聚类技术和投影矩阵来稳定学习过程

Result: 在基于视觉和状态的DMControl基准上分别提升9%和12.6%，在各种算法和任务中表现出鲁棒性和通用性

Conclusion: COWM层能有效缓解非平稳性问题，提高学习速度和效率，减少梯度干扰

Abstract: Reinforcement learning (RL) has made significant advancements, achieving superhuman performance in various tasks. However, RL agents often operate under the assumption of environmental stationarity, which poses a great challenge to learning efficiency since many environments are inherently non-stationary. This non-stationarity results in the requirement of millions of iterations, leading to low sample efficiency. To address this issue, we introduce the Clustering Orthogonal Weight Modified (COWM) layer, which can be integrated into the policy network of any RL algorithm and mitigate non-stationarity effectively. The COWM layer stabilizes the learning process by employing clustering techniques and a projection matrix. Our approach not only improves learning speed but also reduces gradient interference, thereby enhancing the overall learning efficiency. Empirically, the COWM outperforms state-of-the-art methods and achieves improvements of 9% and 12.6% in vision based and state-based DMControl benchmark. It also shows robustness and generality across various algorithms and tasks.

</details>


### [22] [Small Vocabularies, Big Gains: Pretraining and Tokenization in Time Series Models](https://arxiv.org/abs/2511.11622)
*Alexis Roger,Gwen Legate,Kashif Rasul,Yuriy Nevmyvaka,Irina Rish*

Main category: cs.LG

TL;DR: 本文系统研究了时间序列基础模型中分词器设计（特别是缩放和量化策略）对性能的影响，以及预训练与随机初始化的对比。发现分词器配置主要控制模型的表示能力和稳定性，而迁移学习影响优化效率和对齐性。


<details>
  <summary>Details</summary>
Motivation: 分词器和迁移学习是构建最先进时间序列预测基础模型的两个关键组件，需要系统研究它们对模型性能的影响机制。

Method: 通过经验训练实验和理论分析相结合的方法，研究不同分词器配置（缩放和量化策略）与预训练/随机初始化的交互作用。

Result: 预训练模型能更有效地利用设计良好的分词器，特别是在较小词汇量时；而错位的分词会削弱甚至逆转预训练的益处。小词汇量与预训练权重结合在多模态预测中特别有益。

Conclusion: 在时间序列建模中仔细设计分词器至关重要，在连续信号的离散表示学习中，结合小词汇量和预训练权重为分词器设计和迁移学习利用提供了具体指导。

Abstract: Tokenization and transfer learning are two critical components in building state of the art time series foundation models for forecasting. In this work, we systematically study the effect of tokenizer design, specifically scaling and quantization strategies, on model performance, alongside the impact of pretraining versus random initialization. We show that tokenizer configuration primarily governs the representational capacity and stability of the model, while transfer learning influences optimization efficiency and alignment. Using a combination of empirical training experiments and theoretical analyses, we demonstrate that pretrained models consistently leverage well-designed tokenizers more effectively, particularly at smaller vocabulary sizes. Conversely, misaligned tokenization can diminish or even invert the benefits of pretraining. These findings highlight the importance of careful tokenization in time series modeling and suggest that combining small, efficient vocabularies with pretrained weights is especially advantageous in multi-modal forecasting settings, where the overall vocabulary must be shared across modalities. Our results provide concrete guidance for designing tokenizers and leveraging transfer learning in discrete representation learning for continuous signals.

</details>


### [23] [Early GVHD Prediction in Liver Transplantation via Multi-Modal Deep Learning on Imbalanced EHR Data](https://arxiv.org/abs/2511.11623)
*Yushan Jiang,Shuteng Niu,Dongjin Song,Yichen Wang,Jingna Feng,Xinyue Hu,Liu Yang,Cui Tao*

Main category: cs.LG

TL;DR: 开发多模态深度学习框架，整合电子健康记录中的多种数据模态，用于早期预测肝移植后的移植物抗宿主病，在极度不平衡数据集上取得良好性能。


<details>
  <summary>Details</summary>
Motivation: 移植物抗宿主病是肝移植中罕见但致命的并发症，死亡率极高。通过整合异构和不平衡的电子健康记录，利用多模态深度学习方法实现早期预测，为及时干预和改善患者预后铺平道路。

Method: 分析2100名肝移植患者的术前电子健康记录，包括42例GVHD病例。数据集包含四个主要模态：患者人口统计学、实验室检查、诊断和药物。开发多模态深度学习框架，动态融合这些模态，处理不规则记录和缺失值，通过AUC优化解决极端类别不平衡问题。

Result: 该框架优于所有单模态和多模态机器学习基线，AUC达到0.836，AUPRC为0.157，召回率为0.768，特异性为0.803。证明该方法能有效从不同模态中捕获互补信息，提升性能。

Conclusion: 多模态深度学习框架显著改进了现有GVHD早期预测方法，有效解决了真实世界电子健康记录中的异构性和极端类别不平衡挑战，实现了准确的早期预测，在极度不平衡的电子健康记录数据上展现出有前景的结果。

Abstract: Graft-versus-host disease (GVHD) is a rare but often fatal complication in liver transplantation, with a very high mortality rate. By harnessing multi-modal deep learning methods to integrate heterogeneous and imbalanced electronic health records (EHR), we aim to advance early prediction of GVHD, paving the way for timely intervention and improved patient outcomes. In this study, we analyzed pre-transplant electronic health records (EHR) spanning the period before surgery for 2,100 liver transplantation patients, including 42 cases of graft-versus-host disease (GVHD), from a cohort treated at Mayo Clinic between 1992 and 2025. The dataset comprised four major modalities: patient demographics, laboratory tests, diagnoses, and medications. We developed a multi-modal deep learning framework that dynamically fuses these modalities, handles irregular records with missing values, and addresses extreme class imbalance through AUC-based optimization. The developed framework outperforms all single-modal and multi-modal machine learning baselines, achieving an AUC of 0.836, an AUPRC of 0.157, a recall of 0.768, and a specificity of 0.803. It also demonstrates the effectiveness of our approach in capturing complementary information from different modalities, leading to improved performance. Our multi-modal deep learning framework substantially improves existing approaches for early GVHD prediction. By effectively addressing the challenges of heterogeneity and extreme class imbalance in real-world EHR, it achieves accurate early prediction. Our proposed multi-modal deep learning method demonstrates promising results for early prediction of a GVHD in liver transplantation, despite the challenge of extremely imbalanced EHR data.

</details>


### [24] [MedFedPure: A Medical Federated Framework with MAE-based Detection and Diffusion Purification for Inference-Time Attacks](https://arxiv.org/abs/2511.11625)
*Mohammad Karami,Mohammad Reza Nemati,Aidin Kazemi,Ali Mikaeili Barzili,Hamid Azadegan,Behzad Moshiri*

Main category: cs.LG

TL;DR: MedFedPure是一个个性化的联邦学习防御框架，用于保护医疗AI模型在推理时免受对抗攻击，同时保持隐私和准确性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的医疗AI模型容易受到对抗攻击，现有防御方法难以应对去中心化的医疗环境，需要开发既能保护隐私又能确保模型鲁棒性的解决方案。

Method: 结合三个关键组件：个性化联邦学习模型适应各机构数据分布；掩码自编码器检测可疑输入；自适应扩散净化模块选择性清理被标记的扫描图像。

Result: 在Br35H脑MRI数据集上评估，对抗鲁棒性从49.50%显著提升至87.33%，同时保持97.67%的干净准确率。

Conclusion: MedFedPure为临床工作流程中部署安全、可信且保护隐私的AI工具提供了实用路径，能够在诊断过程中本地实时运行。

Abstract: Artificial intelligence (AI) has shown great potential in medical imaging, particularly for brain tumor detection using Magnetic Resonance Imaging (MRI). However, the models remain vulnerable at inference time when they are trained collaboratively through Federated Learning (FL), an approach adopted to protect patient privacy. Adversarial attacks can subtly alter medical scans in ways invisible to the human eye yet powerful enough to mislead AI models, potentially causing serious misdiagnoses. Existing defenses often assume centralized data and struggle to cope with the decentralized and diverse nature of federated medical settings. In this work, we present MedFedPure, a personalized federated learning defense framework designed to protect diagnostic AI models at inference time without compromising privacy or accuracy. MedFedPure combines three key elements: (1) a personalized FL model that adapts to the unique data distribution of each institution; (2) a Masked Autoencoder (MAE) that detects suspicious inputs by exposing hidden perturbations; and (3) an adaptive diffusion-based purification module that selectively cleans only the flagged scans before classification. Together, these steps offer robust protection while preserving the integrity of normal, benign images. We evaluated MedFedPure on the Br35H brain MRI dataset. The results show a significant gain in adversarial robustness, improving performance from 49.50% to 87.33% under strong attacks, while maintaining a high clean accuracy of 97.67%. By operating locally and in real time during diagnosis, our framework provides a practical path to deploying secure, trustworthy, and privacy-preserving AI tools in clinical workflows.
  Index Terms: cancer, tumor detection, federated learning, masked autoencoder, diffusion, privacy

</details>


### [25] [SA-EMO: Structure-Aligned Encoder Mixture of Operators for Generalizable Full-waveform Inversion](https://arxiv.org/abs/2511.11627)
*Wang Zhenyu,Li Peiyuan,Shi Yongxiang,Wu Ruoyu,Zhang Lei*

Main category: cs.LG

TL;DR: 提出SA-EMO架构解决全波形反演中的泛化性和地质类型区分问题，通过结构对齐编码器和多算子混合机制显著提升性能


<details>
  <summary>Details</summary>
Motivation: 传统单CNN或单神经算子方法在未知或复杂地质环境中泛化能力差，难以区分不同地质类型，需要更有效的解决方案

Method: 使用结构对齐编码器将地震波场映射到物理一致的潜在空间，然后通过自适应路由机制选择和融合多种神经算子专家来预测速度模型

Result: 在OpenFWI基准和Marmousi2数据集上，SA-EMO显著优于传统方法，平均MAE降低约58.443%，边界分辨率提升约10.308%

Conclusion: 该工作为高效、可扩展且物理可解释的全波形反演引入了新范式

Abstract: Full-waveform inversion (FWI) can produce high-resolution subsurface models, yet it remains inherently ill-posed, highly nonlinear, and computationally intensive. Although recent deep learning and numerical acceleration methods have improved speed and scalability, they often rely on single CNN architectures or single neural operators, which struggle to generalize in unknown or complex geological settings and are ineffective at distinguishing diverse geological types. To address these issues, we propose a Structure-Aligned Encoder-Mixture-of-Operators (SA-EMO) architecture for velocity-field inversion under unknown subsurface structures. First, a structure-aligned encoder maps high-dimensional seismic wavefields into a physically consistent latent space, thereby eliminating spatio-temporal mismatch between the waveform and velocity domains, recovering high-frequency components, and enhancing feature generalization. Then, an adaptive routing mechanism selects and fuses multiple neural-operator experts, including spectral, wavelet, multiscale, and local operators, to predict the velocity model. We systematically evaluate our approach on the OpenFWI benchmark and the Marmousi2 dataset. Results show that SA-EMO significantly outperforms traditional CNN or single-operator methods, achieving an average MAE reduction of approximately 58.443% and an improvement in boundary resolution of about 10.308%. Ablation studies further reveal that the structure-aligned encoder, the expert-fusion mechanism, and the routing module each contribute markedly to the performance gains. This work introduces a new paradigm for efficient, scalable, and physically interpretable full-waveform inversion.

</details>


### [26] [Global Feature Enhancing and Fusion Framework for Strain Gauge Time Series Classification](https://arxiv.org/abs/2511.11629)
*Xu Zhang,Peng Wang,Chen Wang,Zhe Xu,Xiaohua Nie,Wei Wang*

Main category: cs.LG

TL;DR: 提出基于超图的全局特征学习和融合框架，通过特征工程构建全局特征和学习局部特征间的高阶关系，增强应变计状态时间序列表示，提高识别精度。


<details>
  <summary>Details</summary>
Motivation: 在智能制造中，应变计状态识别对及时发现故障机械部件至关重要。现有CNN方法主要提取局部特征，但在局部子序列相似时（如飞机机翼静强度实验数据），仅靠局部特征不足以充分表达时间序列，需要全局特征来更全面地表征。

Method: 提出超图全局特征学习融合框架：1）通过特征工程构建全局特征；2）学习局部特征间的高阶关系来捕获全局特征。该框架学习和融合全局特征以实现语义一致性，增强时间序列表示。

Result: 在工业应变计数据和公开UCR数据集上验证，在应变计状态识别中显示出对未见数据的更好泛化能力。

Conclusion: 通过超图学习全局特征能有效增强时间序列表示，提高应变计状态识别精度，特别是在局部特征相似的情况下。

Abstract: Strain Gauge Status (SGS) recognition is crucial in the field of intelligent manufacturing based on the Internet of Things, as accurate identification helps timely detection of failed mechanical components, avoiding accidents. The loading and unloading sequences generated by strain gauges can be identified through time series classification (TSC) algorithms. Recently, deep learning models, e.g., convolutional neural networks (CNNs) have shown remarkable success in the TSC task, as they can extract discriminative local features from the subsequences to identify the time series. However, we observe that only the local features may not be sufficient for expressing the time series, especially when the local sub-sequences between different time series are very similar, e.g., SGS data of aircraft wings in static strength experiments. Nevertheless, CNNs suffer from the limitation in extracting global features due to the nature of convolution operations. For extracting global features to more comprehensively represent the SGS time series, we propose two insights: (i) Constructing global features through feature engineering. (ii) Learning high-order relationships between local features to capture global features. To realize and utilize them, we propose a hypergraph-based global feature learning and fusion framework, which learns and fuses global features for semantic consistency to enhance the representation of SGS time series, thereby improving recognition accuracy. Our method designs are validated on industrial SGS and public UCR datasets, showing better generalization for unseen data in SGS recognition.

</details>


### [27] [Predicting Grain Growth in Polycrystalline Materials Using Deep Learning Time Series Models](https://arxiv.org/abs/2511.11630)
*Eliane Younes,Elie Hachem,Marc Bernacki*

Main category: cs.LG

TL;DR: 本研究评估了多种深度学习模型（RNN、LSTM、TCN、Transformer）预测晶粒生长过程中的晶粒尺寸分布，发现LSTM模型在准确性和稳定性方面表现最佳，计算时间从20分钟大幅减少到几秒钟。


<details>
  <summary>Details</summary>
Motivation: 晶粒生长对材料力学行为有重要影响，但全场模拟计算成本高，需要开发更高效的预测方法。

Method: 使用从高保真模拟中提取的平均场统计描述符，将120个晶粒生长序列处理为归一化晶粒尺寸分布，采用递归预测策略训练模型从短期历史预测未来分布。

Result: LSTM网络达到最高准确率（90%以上），在长时间预测中保持物理一致性，而其他架构在远期预测时容易发散。计算时间从约20分钟/序列减少到几秒钟。

Conclusion: 低维描述符和LSTM预测方法为高效准确的微观结构预测提供了潜力，对数字孪生和工艺优化有直接意义。

Abstract: Grain Growth strongly influences the mechanical behavior of materials, making its prediction a key objective in microstructural engineering. In this study, several deep learning approaches were evaluated, including recurrent neural networks (RNN), long short-term memory (LSTM), temporal convolutional networks (TCN), and transformers, to forecast grain size distributions during grain growth. Unlike full-field simulations, which are computationally demanding, the present work relies on mean-field statistical descriptors extracted from high-fidelity simulations. A dataset of 120 grain growth sequences was processed into normalized grain size distributions as a function of time. The models were trained to predict future distributions from a short temporal history using a recursive forecasting strategy. Among the tested models, the LSTM network achieved the highest accuracy (above 90\%) and the most stable performance, maintaining physically consistent predictions over extended horizons while reducing computation time from about 20 minutes per sequence to only a few seconds, whereas the other architectures tended to diverge when forecasting further in time. These results highlight the potential of low-dimensional descriptors and LSTM-based forecasting for efficient and accurate microstructure prediction, with direct implications for digital twin development and process optimization.

</details>


### [28] [Toward Better Generalization in Few-Shot Learning through the Meta-Component Combination](https://arxiv.org/abs/2511.11632)
*Qiuhao Zeng*

Main category: cs.LG

TL;DR: 提出了一种新的元学习算法，通过将分类器分解为元组件来改进少样本学习中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决基于度量的元学习方法在少样本学习中可能对已见类别过拟合、无法很好泛化到未见类别的问题。

Method: 将每个分类器表示为元组件的组合，通过正交正则化促进元组件的多样性，捕捉不同分类器间的共享子结构。

Result: 在少样本基准任务上的广泛实验显示该方法具有优越性能。

Conclusion: 通过元组件分解和正交正则化，有效提升了少样本学习的泛化能力。

Abstract: In few-shot learning, classifiers are expected to generalize to unseen classes given only a small number of instances of each new class. One of the popular solutions to few-shot learning is metric-based meta-learning. However, it highly depends on the deep metric learned on seen classes, which may overfit to seen classes and fail to generalize well on unseen classes. To improve the generalization, we explore the substructures of classifiers and propose a novel meta-learning algorithm to learn each classifier as a combination of meta-components. Meta-components are learned across meta-learning episodes on seen classes and disentangled by imposing an orthogonal regularizer to promote its diversity and capture various shared substructures among different classifiers. Extensive experiments on few-shot benchmark tasks show superior performances of the proposed method.

</details>


### [29] [An Explainable and Fair AI Tool for PCOS Risk Assessment: Calibration, Subgroup Equity, and Interactive Clinical Deployment](https://arxiv.org/abs/2511.11636)
*Asma Sadia Khan,Sadia Tabassum*

Main category: cs.LG

TL;DR: 开发了一个公平审计和可解释的机器学习框架用于预测多囊卵巢综合征(PCOS)，该框架整合了SHAP特征归因和人口统计学审计，以连接预测解释与观察到的差异，并提供可操作的见解。


<details>
  <summary>Details</summary>
Motivation: 旨在评估模型性能并识别不同患者亚组间的诊断差异，确保跨亚组的可靠风险预测，弥合AI研究与临床可用性之间的差距。

Method: 使用随机森林、SVM和XGBoost模型，结合等渗和Platt标定进行校准和公平性比较。整合SHAP特征归因与人口统计学审计，并开发基于Streamlit的Web界面。

Result: 校准后的随机森林模型达到90.8%的预测准确率。SHAP分析确定卵泡计数、体重增加和月经不规律为最具影响力的特征。亚组分析显示模型在25-35岁女性中表现最佳(90.9%)，但在25岁以下女性中表现较差(69.2%)。

Conclusion: 随机森林模型在校准和可解释性之间提供了更好的平衡。该框架成功识别了年龄相关的差异，并在不同表型中表现出稳健性，开发了实时PCOS风险评估的临床可用工具。

Abstract: This paper presents a fairness-audited and interpretable machine learning framework for predicting polycystic ovary syndrome (PCOS), designed to evaluate model performance and identify diagnostic disparities across patient subgroups. The framework integrated SHAP-based feature attributions with demographic audits to connect predictive explanations with observed disparities for actionable insights. Probabilistic calibration metrics (Brier Score and Expected Calibration Error) are incorporated to ensure reliable risk predictions across subgroups. Random Forest, SVM, and XGBoost models were trained with isotonic and Platt scaling for calibration and fairness comparison. A calibrated Random Forest achieved a high predictive accuracy of 90.8%. SHAP analysis identified follicle count, weight gain, and menstrual irregularity as the most influential features, which are consistent with the Rotterdam diagnostic criteria. Although the SVM with isotonic calibration achieved the lowest calibration error (ECE = 0.0541), the Random Forest model provided a better balance between calibration and interpretability (Brier = 0.0678, ECE = 0.0666). Therefore, it was selected for detailed fairness and SHAP analyses. Subgroup analysis revealed that the model performed best among women aged 25-35 (accuracy 90.9%) but underperformed in those under 25 (69.2%), highlighting age-related disparities. The model achieved perfect precision in obese women and maintained high recall in lean PCOS cases, demonstrating robustness across phenotypes. Finally, a Streamlit-based web interface enables real-time PCOS risk assessment, Rotterdam criteria evaluation, and interactive 'what-if' analysis, bridging the gap between AI research and clinical usability.

</details>


### [30] [Enhancing PINN Accuracy for the RLW Equation: Adaptive and Conservative Approaches](https://arxiv.org/abs/2511.11638)
*Aamir Shehzad*

Main category: cs.LG

TL;DR: 本研究开发了两种改进的PINN方法（自适应和守恒方法）来求解正则化长波方程，发现PINN的有效性与问题类型相关，自适应方法在复杂非线性相互作用中表现更好，而守恒方法在长期行为问题中更优。


<details>
  <summary>Details</summary>
Motivation: 标准PINN在求解正则化长波方程时产生较大误差，需要开发改进方法来提高求解精度和适应性。

Method: 开发了两种改进PINN方法：具有自适应损失加权的自适应方法和强制执行明确守恒定律的守恒方法，使用三个基准测试（单孤子传播、双孤子相互作用、涌浪演化）进行评估。

Result: 自适应PINN在复杂非线性相互作用（如孤子碰撞）中显著优于标准PINN和守恒PINN，而守恒方法在单孤子长期行为和涌浪演化问题上表现更好。两种方法的解与数值解的误差在O(10^-5)量级。

Conclusion: 明确强制执行守恒定律可能对高度非线性系统的求解优化有害，需要特殊训练方法。研究挑战了守恒强制总是提高PINN性能的假设，并为特定问题类型设计PINN提供了指导。

Abstract: Standard physics-informed neural network implementations have produced large error rates when using these models to solve the regularized long wave (RLW) equation. Two improved PINN approaches were developed in this research: an adaptive approach with self-adaptive loss weighting and a conservative approach enforcing explicit conservation laws. Three benchmark tests were used to demonstrate how effective PINN's are as they relate to the type of problem being solved (i.e., time dependent RLW equation). The first was a single soliton traveling along a line (propagation), the second was the interaction between two solitons, and the third was the evolution of an undular bore over the course of $t=250$. The results demonstrated that the effectiveness of PINNs are problem specific. The adaptive PINN was significantly better than both the conservative PINN and the standard PINN at solving problems involving complex nonlinear interactions such as colliding two solitons. The conservative approach was significantly better at solving problems involving long term behavior of single solitons and undular bores. However, the most important finding from this research is that explicitly enforcing conservation laws may be harmful to optimizing the solution of highly nonlinear systems of equations and therefore requires special training methods. The results from our adaptive and conservative approaches were within $O(10^{-5})$ of established numerical solutions for the same problem, thus demonstrating that PINNs can provide accurate solutions to complex systems of partial differential equations without the need for a discretization of space or time (mesh free). Moreover, the finding from this research challenges the assumptions that conservation enforcement will always improve the performance of a PINN and provides researchers with guidelines for designing PINNs for use on specific types of problems.

</details>


### [31] [EcoSpa: Efficient Transformer Training with Coupled Sparsity](https://arxiv.org/abs/2511.11641)
*Jinqi Xiao,Cheng Luo,Lingyi Huang,Cheng Yang,Yang Sui,Huy Phan,Xiao Zang,Yibiao Ying,Zhexiang Tang,Anima Anandkumar,Bo Yuan*

Main category: cs.LG

TL;DR: EcoSpa是一种高效的Transformer结构化稀疏训练方法，通过联合评估和稀疏化耦合权重矩阵对，在保持其交互模式的同时实现显著的计算效率提升。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏训练方法未能保留注意力层和前馈层中权重矩阵之间的关键结构关系，导致在高稀疏度下性能下降。

Method: 引入联合评估和稀疏化耦合权重矩阵对的方法，通过对齐的行/列移除来保持交互模式，并在预训练和微调场景中执行耦合估计和稀疏化。

Result: LLaMA-1B训练内存减少50%，训练速度提升21%；GPT-2-Medium模型压缩2.2倍，困惑度降低2.4；推理速度提升1.6倍。

Conclusion: EcoSpa使用标准PyTorch操作，无需定制硬件或内核，可在普通硬件上实现高效的Transformer训练。

Abstract: Transformers have become the backbone of modern AI, yet their high computational demands pose critical system challenges. While sparse training offers efficiency gains, existing methods fail to preserve critical structural relationships between weight matrices that interact multiplicatively in attention and feed-forward layers. This oversight leads to performance degradation at high sparsity levels. We introduce EcoSpa, an efficient structured sparse training method that jointly evaluates and sparsifies coupled weight matrix pairs, preserving their interaction patterns through aligned row/column removal. EcoSpa introduces a new granularity for calibrating structural component importance and performs coupled estimation and sparsification across both pre-training and fine-tuning scenarios. Evaluations demonstrate substantial improvements: EcoSpa enables efficient training of LLaMA-1B with 50\% memory reduction and 21\% faster training, achieves $2.2\times$ model compression on GPT-2-Medium with $2.4$ lower perplexity, and delivers $1.6\times$ inference speedup. The approach uses standard PyTorch operations, requiring no custom hardware or kernels, making efficient transformer training accessible on commodity hardware.

</details>


### [32] [A Deep Learning Model to Predicting Changes in Consumer Attributes for New Line-extended Products](https://arxiv.org/abs/2511.11646)
*Li Yinxing,Tsukasa Ishigaki*

Main category: cs.LG

TL;DR: 提出了一种基于条件表格变分自编码器(CTVAE)的深度学习方法，用于预测新产品线扩展中的消费者属性变化，帮助避免产品蚕食并优化营销策略。


<details>
  <summary>Details</summary>
Motivation: 产品线扩展是重要的营销策略，但过度扩展会破坏品牌形象。需要基于消费者需求进行适当扩展，因此需要预测新线扩展产品主要消费者的关键属性。

Method: 使用条件表格变分自编码器(CTVAE)从大规模消费者和产品的表格数据中生成合成数据，预测新线扩展产品的消费者属性变化。

Result: 实验结果表明CTVAE比现有模型具有更优越的预测性能，能够为改变包装或口味的新产品提供有效的产品线营销启示。

Conclusion: 该方法有助于避免产品蚕食，并为产品形象设计和营销策略制定提供支持，具有重要的实际应用价值。

Abstract: Product line extension is a marketing strategy that enhances a company's sphere of influence. Because excessive line extensions disrupt brand image, only appropriate line extensions based on consumer needs are desirable. Marketers should know the key consumer attributes of the primary customers for new line-extended products before companies enter the market. This paper describes a method for predicting changes in consumer attributes for new line-extended products using a novel deep learning model. The proposed model, Conditional Tabular Variational Auto-Encoder (CTVAE), generates synthetic data from large-scale tabular data of consumers and products. It can provide various implications about effective product line marketing for marketers. The experimental results demonstrate that the CTVAE offers superior prediction performance than existing models. We indicate implications for new products that change containers or flavors for effective product line marketing. The proposed approach has the potential to contribute to avoiding cannibalization and to designing product images and marketing strategies.

</details>


### [33] [Environment-Aware Transfer Reinforcement Learning for Sustainable Beam Selection](https://arxiv.org/abs/2511.11647)
*Dariush Salami,Ramin Hashemi,Parham Kazemi,Mikko A. Uusitalo*

Main category: cs.LG

TL;DR: 提出一种基于迁移学习和强化学习的可持续波束选择方法，通过将环境建模为点云并计算Chamfer距离来识别结构相似环境，从而重用预训练模型，大幅减少训练时间和计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统基于强化学习的波束选择模型在多样化环境中需要大量训练时间和计算资源，这对可扩展性和能效构成挑战。

Method: 将环境建模为点云（gNodeB和散射体位置），计算点云间的Chamfer距离来识别结构相似环境，通过迁移学习重用预训练模型。

Result: 训练时间和计算开销减少16倍，保持高性能的同时显著降低能耗，加速部署时间并减少训练相关的碳排放。

Conclusion: 该方法展示了迁移学习在实现可扩展、自适应且环保的RL波束选择策略方面的潜力，支持无线系统中绿色可持续AI的发展。

Abstract: This paper presents a novel and sustainable approach for improving beam selection in 5G and beyond networks using transfer learning and Reinforcement Learning (RL). Traditional RL-based beam selection models require extensive training time and computational resources, particularly when deployed in diverse environments with varying propagation characteristics posing a major challenge for scalability and energy efficiency. To address this, we propose modeling the environment as a point cloud, where each point represents the locations of gNodeBs (gNBs) and surrounding scatterers. By computing the Chamfer distance between point clouds, structurally similar environments can be efficiently identified, enabling the reuse of pre-trained models through transfer learning. This methodology leads to a 16x reduction in training time and computational overhead, directly contributing to energy efficiency. By minimizing the need for retraining in each new deployment, our approach significantly lowers power consumption and supports the development of green and sustainable Artificial Intelligence (AI) in wireless systems. Furthermore, it accelerates time-to-deployment, reduces carbon emissions associated with training, and enhances the viability of deploying AI-driven communication systems at the edge. Simulation results confirm that our approach maintains high performance while drastically cutting energy costs, demonstrating the potential of transfer learning to enable scalable, adaptive, and environmentally conscious RL-based beam selection strategies in dynamic and diverse propagation environments.

</details>


### [34] [Lightweight Time Series Data Valuation on Time Series Foundation Models via In-Context Finetuning](https://arxiv.org/abs/2511.11648)
*Shunyu Wu,Tianyue Li,Yixuan Leng,Jingyi Suo,Jian Lou,Dan Li,See-Kiong Ng*

Main category: cs.LG

TL;DR: LTSV是一种轻量级时间序列数据估值方法，通过上下文微调来评估样本对时间序列基础模型的贡献，解决了传统方法计算复杂度高和无法保留时间依赖性的问题。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型(TSFMs)的性能严重依赖数据质量，但传统数据估值方法存在计算瓶颈且无法保持时间依赖性，需要一种高效准确的时间序列数据估值方法。

Method: 提出LTSV方法，基于上下文微调近似影响函数的理论依据，通过测量上下文微调后上下文损失的变化来估计样本贡献，并引入时间块聚合来捕捉时间依赖性。

Result: 在多个时间序列数据集和模型上的实验表明，LTSV能够提供可靠且强大的估值性能，同时保持可管理的计算需求。

Conclusion: 在时间序列基础模型上进行上下文微调为时间序列学习中的数据归因和模型泛化之间提供了实用有效的桥梁。

Abstract: Time series foundation models (TSFMs) have demonstrated increasing capabilities due to their extensive pretraining on large volumes of diverse time series data. Consequently, the quality of time series data is crucial to TSFM performance, rendering an accurate and efficient data valuation of time series for TSFMs indispensable. However, traditional data valuation methods, such as influence functions, face severe computational bottlenecks due to their poor scalability with growing TSFM model sizes and often fail to preserve temporal dependencies. In this paper, we propose LTSV, a Lightweight Time Series Valuation on TSFMS via in-context finetuning. Grounded in the theoretical evidence that in-context finetuning approximates the influence function, LTSV estimates a sample's contribution by measuring the change in context loss after in-context finetuning, leveraging the strong generalization capabilities of TSFMs to produce robust and transferable data valuations. To capture temporal dependencies, we introduce temporal block aggregation, which integrates per-block influence scores across overlapping time windows. Experiments across multiple time series datasets and models demonstrate that LTSV consistently provides reliable and strong valuation performance, while maintaining manageable computational requirements. Our results suggest that in-context finetuning on time series foundation models provides a practical and effective bridge between data attribution and model generalization in time series learning.

</details>


### [35] [Enhanced Water Leak Detection with Convolutional Neural Networks and One-Class Support Vector Machine](https://arxiv.org/abs/2511.11650)
*Daniele Ugo Leonzio,Paolo Bestagini,Marco Marcon,Stefano Tubaro*

Main category: cs.LG

TL;DR: 提出了一种基于水压测量和单类支持向量机的数据驱动漏水检测方法，仅需无泄漏时的压力数据和管网拓扑信息。


<details>
  <summary>Details</summary>
Motivation: 供水管网每年因漏水损失大量水资源，需要可靠有效的漏水检测和定位系统。数据驱动方法因其优越性能受到越来越多关注。

Method: 使用供水管网节点处的水压测量数据，基于特征提取器和在无泄漏数据上训练的单类支持向量机，将漏水检测为异常。

Result: 在Modena供水管网模拟数据集上的结果表明，该方法优于最近的漏水检测方法。

Conclusion: 该方法是一种完全数据驱动的解决方案，仅需管网拓扑和无泄漏压力数据，在漏水检测方面表现出优越性能。

Abstract: Water is a critical resource that must be managed efficiently. However, a substantial amount of water is lost each year due to leaks in Water Distribution Networks (WDNs). This underscores the need for reliable and effective leak detection and localization systems. In recent years, various solutions have been proposed, with data-driven approaches gaining increasing attention due to their superior performance. In this paper, we propose a new method for leak detection. The method is based on water pressure measurements acquired at a series of nodes of a WDN. Our technique is a fully data-driven solution that makes only use of the knowledge of the WDN topology, and a series of pressure data acquisitions obtained in absence of leaks. The proposed solution is based on an feature extractor and a one-class Support Vector Machines (SVM) trained on no-leak data, so that leaks are detected as anomalies. The results achieved on a simulate dataset using the Modena WDN demonstrate that the proposed solution outperforms recent methods for leak detection.

</details>


### [36] [Incomplete Depression Feature Selection with Missing EEG Channels](https://arxiv.org/abs/2511.11651)
*Zhijian Gong,Wenjia Dong,Xueyuan Xu,Fulin Wei,Chunyu Liu,Li Zhuo*

Main category: cs.LG

TL;DR: 提出了一种名为IDFS-MEC的新型特征选择方法，用于处理EEG数据中的通道缺失问题，通过整合缺失通道指示信息和自适应通道权重学习来提高抑郁症分析的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: EEG特征通常包含冗余、不相关和噪声信息，且现实世界中EEG数据采集经常面临电极脱落导致数据丢失和强噪声干扰等挑战。

Method: IDFS-MEC将缺失通道指示信息和自适应通道权重学习整合到正交回归中，减轻不完整通道对模型构建的影响，然后利用全局冗余最小化学习来减少选定特征子集中的冗余信息。

Result: 在MODMA和PRED-d003数据集上的广泛实验表明，IDFS-MEC选择的EEG特征子集在3、64和128通道设置下均优于10种流行的特征选择方法。

Conclusion: IDFS-MEC方法能够有效处理EEG数据中的通道缺失问题，并选择出性能优越的特征子集，为抑郁症分析提供了鲁棒的解决方案。

Abstract: As a critical mental health disorder, depression has severe effects on both human physical and mental well-being. Recent developments in EEG-based depression analysis have shown promise in improving depression detection accuracies. However, EEG features often contain redundant, irrelevant, and noisy information. Additionally, real-world EEG data acquisition frequently faces challenges, such as data loss from electrode detachment and heavy noise interference. To tackle the challenges, we propose a novel feature selection approach for robust depression analysis, called Incomplete Depression Feature Selection with Missing EEG Channels (IDFS-MEC). IDFS-MEC integrates missing-channel indicator information and adaptive channel weighting learning into orthogonal regression to lessen the effects of incomplete channels on model construction, and then utilizes global redundancy minimization learning to reduce redundant information among selected feature subsets. Extensive experiments conducted on MODMA and PRED-d003 datasets reveal that the EEG feature subsets chosen by IDFS-MEC have superior performance than 10 popular feature selection methods among 3-, 64-, and 128-channel settings.

</details>


### [37] [How many stations are sufficient? Exploring the effect of urban weather station density reduction on imputation accuracy of air temperature and humidity](https://arxiv.org/abs/2511.11652)
*Marvin Plein,Carsten F. Dormann,Andreas Christen*

Main category: cs.LG

TL;DR: 提出逐步移除气象站的方法来稀疏化弗莱堡市的气象站网络，分析在减少站点密度后，子网络重现原始网络气温和湿度模式的能力。研究发现可以从42个站点减少到4个站点，同时保持较高的预测精度。


<details>
  <summary>Details</summary>
Motivation: 城市气象站网络维护成本高昂且劳动密集，需要找到在保持监测精度的前提下减少站点数量的方法，以优化资源分配。

Method: 采用逐步站点移除程序，对现有气象站网络进行稀疏化处理，分析不同子网络在一年内重现原始网络气温和湿度模式的能力。

Result: 从42个站点减少到4个站点，气温预测RMSE从0.69K增加到0.83K（增加20%），相对湿度RMSE从3.8%增加到4.4%（增加16%）。森林站点的预测精度较差，但优于最先进的数值城市地表模型。

Conclusion: 研究表明稀疏化气象站网络具有巨大潜力，可以在保持精度的同时最大化城市气候研究中财务和人力资源的分配效率。

Abstract: Urban weather station networks (WSNs) are widely used to monitor urban weather and climate patterns and aid urban planning. However, maintaining WSNs is expensive and labor-intensive. Here, we present a step-wise station removal procedure to thin an existing WSN in Freiburg, Germany, and analyze the ability of WSN subsets to reproduce air temperature and humidity patterns of the entire original WSN for a year following a simulated reduction of WSN density. We found that substantial reductions in station numbers after one year of full deployment are possible while retaining high predictive accuracy. A reduction from 42 to 4 stations, for instance, increased mean prediction RMSEs from 0.69 K to 0.83 K for air temperature and from 3.8% to 4.4% for relative humidity, corresponding to RMSE increases of only 20% and 16%, respectively. Predictive accuracy is worse for remote stations in forests than for stations in built-up or open settings, but consistently better than a state-of-the-art numerical urban land-surface model (Surface Urban Energy and Water Balance Scheme). Stations located at the edges between built-up and rural areas are most valuable when reconstructing city-wide climate characteristics. Our study demonstrates the potential of thinning WSNs to maximize the efficient allocation of financial and personnel-related resources in urban climate research.

</details>


### [38] [Convergence of Multiagent Learning Systems for Traffic control](https://arxiv.org/abs/2511.11654)
*Sayambhu Sen,Shalabh Bhatnagar*

Main category: cs.LG

TL;DR: 本文对多智能体强化学习在交通信号控制中的收敛性进行了理论分析，证明了在特定条件下该算法的收敛性。


<details>
  <summary>Details</summary>
Motivation: 随着班加罗尔等城市的快速城市化，交通拥堵问题日益严重，需要高效的交通信号控制。虽然已有实证研究证明了多智能体强化学习的有效性，但缺乏对其稳定性和收敛性的严格理论分析。

Method: 使用随机逼近方法，正式分析多智能体强化学习算法的学习动态，将单智能体异步值迭代的收敛证明扩展到多智能体场景。

Result: 证明了在给定条件下，用于交通控制的多智能体强化学习算法能够收敛。

Conclusion: 本文填补了多智能体强化学习在交通控制领域理论分析的空白，为算法的实际应用提供了理论保障。

Abstract: Rapid urbanization in cities like Bangalore has led to severe traffic congestion, making efficient Traffic Signal Control (TSC) essential. Multi-Agent Reinforcement Learning (MARL), often modeling each traffic signal as an independent agent using Q-learning, has emerged as a promising strategy to reduce average commuter delays. While prior work Prashant L A et. al has empirically demonstrated the effectiveness of this approach, a rigorous theoretical analysis of its stability and convergence properties in the context of traffic control has not been explored. This paper bridges that gap by focusing squarely on the theoretical basis of this multi-agent algorithm. We investigate the convergence problem inherent in using independent learners for the cooperative TSC task. Utilizing stochastic approximation methods, we formally analyze the learning dynamics. The primary contribution of this work is the proof that the specific multi-agent reinforcement learning algorithm for traffic control is proven to converge under the given conditions extending it from single agent convergence proofs for asynchronous value iteration.

</details>


### [39] [On the Probabilistic Learnability of Compact Neural Network Preimage Bounds](https://arxiv.org/abs/2511.11656)
*Luca Marzari,Manuele Bicego,Ferdinando Cicalese,Alessandro Farinelli*

Main category: cs.LG

TL;DR: 提出RF-ProVe方法，使用随机森林和主动重采样来高效计算神经网络预像的近似边界，提供统计保证


<details>
  <summary>Details</summary>
Motivation: 现有可证明方法受限于#P-hard问题的可扩展性限制，需要开发具有高置信度保证和有限误差的概率性方法

Method: 使用随机森林集成方法生成满足输出属性的候选输入区域，并通过主动重采样进行精炼

Result: 理论推导提供了区域纯度和全局覆盖的正式统计保证，为计算紧凑预像近似提供了实用可扩展方案

Conclusion: RF-ProVe为在精确求解器无法扩展的情况下计算紧凑预像近似提供了实用的可扩展解决方案

Abstract: Although recent provable methods have been developed to compute preimage bounds for neural networks, their scalability is fundamentally limited by the #P-hardness of the problem. In this work, we adopt a novel probabilistic perspective, aiming to deliver solutions with high-confidence guarantees and bounded error. To this end, we investigate the potential of bootstrap-based and randomized approaches that are capable of capturing complex patterns in high-dimensional spaces, including input regions where a given output property holds. In detail, we introduce $\textbf{R}$andom $\textbf{F}$orest $\textbf{Pro}$perty $\textbf{Ve}$rifier ($\texttt{RF-ProVe}$), a method that exploits an ensemble of randomized decision trees to generate candidate input regions satisfying a desired output property and refines them through active resampling. Our theoretical derivations offer formal statistical guarantees on region purity and global coverage, providing a practical, scalable solution for computing compact preimage approximations in cases where exact solvers fail to scale.

</details>


### [40] [SpecQuant: Spectral Decomposition and Adaptive Truncation for Ultra-Low-Bit LLMs Quantization](https://arxiv.org/abs/2511.11663)
*Zhixiong Zhao,Fangxin Liu,Junjie Wang,Chenyang Guan,Zongwu Wang,Li Jiang,Haibing Guan*

Main category: cs.LG

TL;DR: SpecQuant是一种从傅里叶频域视角出发的极端LLM压缩方法，通过两阶段框架实现权重和激活的4位量化，在保持精度的同时显著提升推理速度和降低内存使用。


<details>
  <summary>Details</summary>
Motivation: 随着准确开源大语言模型的出现，需要先进的量化技术来在终端设备上高效部署。本文从傅里叶频域角度重新审视极端LLM压缩的挑战，目标是实现权重和激活的超低位量化。

Method: 提出SpecQuant两阶段框架：第一阶段平滑激活异常值并将其转移到权重矩阵；第二阶段应用通道级低频傅里叶截断来抑制高频分量，同时保留基本信号能量。还引入了轻量级截断模块用于推理时的自适应调整。

Result: 在LLaMA-3 8B上，SpecQuant实现了权重和激活的4位量化，零样本准确率与全精度相比仅差1.5%，同时提供2倍更快的推理速度和3倍更低的内存使用。

Conclusion: SpecQuant证明了从傅里叶频域角度处理LLM量化的有效性，能够在不显著牺牲准确性的情况下实现高效的极端压缩。

Abstract: The emergence of accurate open large language models (LLMs) has sparked a push for advanced quantization techniques to enable efficient deployment on end-user devices. In this paper, we revisit the challenge of extreme LLM compression -- targeting ultra-low-bit quantization for both activations and weights -- from a Fourier frequency domain perspective. We propose SpecQuant, a two-stage framework that tackles activation outliers and cross-channel variance. In the first stage, activation outliers are smoothed and transferred into the weight matrix to simplify downstream quantization. In the second stage, we apply channel-wise low-frequency Fourier truncation to suppress high-frequency components while preserving essential signal energy, improving quantization robustness. Our method builds on the principle that most of the weight energy is concentrated in low-frequency components, which can be retained with minimal impact on model accuracy. To enable runtime adaptability, we introduce a lightweight truncation module during inference that adjusts truncation thresholds based on channel characteristics. On LLaMA-3 8B, SpecQuant achieves 4-bit quantization for both weights and activations, narrowing the zero-shot accuracy gap to only 1.5% compared to full precision, while delivering 2 times faster inference and 3times lower memory usage.

</details>


### [41] [Clifford Algebraic Rotor Embeddings : Maybe embeddings should start to CARE](https://arxiv.org/abs/2511.11665)
*Sameeksha Sriram,Ayush Paliwal,Alexander S. Ecker,Chase van de Geijn*

Main category: cs.LG

TL;DR: 提出了基于四元数和克利福德代数的旋转位置嵌入方法，将RoPE扩展到更高维度，同时保持旋转的交换性。


<details>
  <summary>Details</summary>
Motivation: 现有的球形RoPE等扩展方法是非交换的，丧失了原始RoPE的平移等变性特性，需要找到能保持这一重要性质的更高维扩展方法。

Method: 使用四元数表示3D旋转来参数化旋转轴，提出QuatRo方法，并进一步扩展到克利福德代数旋转嵌入(CARE)，利用几何代数在任意维度实现旋转嵌入。

Result: 证明了混合RoPE和球形RoPE都是QuatRo的特例，CARE方法能够将旋转嵌入扩展到任意维度，并在多向量中编码位置信息。

Conclusion: 四元数和克利福德代数方法为RoPE提供了数学上优雅的扩展框架，能够在保持旋转交换性的同时处理更高维度的输入。

Abstract: Rotary Positional Embeddings (RoPE) have demonstrated exceptional performance as a positional encoding method, consistently outperforming their baselines. While recent work has sought to extend RoPE to higher-dimensional inputs, many such extensions are non-commutative, thereby forfeiting RoPE's shift-equivariance property. Spherical RoPE is one such non-commutative variant, motivated by the idea of rotating embedding vectors on spheres rather than circles. However, spherical rotations are inherently non-commutative, making the choice of rotation sequence ambiguous. In this work, we explore a quaternion-based approach -- Quaternion Rotary Embeddings (QuatRo) -- in place of Euler angles, leveraging quaternions' ability to represent 3D rotations to parameterize the axes of rotation. We show Mixed RoPE and Spherical RoPE to be special cases of QuatRo. Further, we propose a generalization of QuatRo to Clifford Algebraic Rotary Embeddings (CARE) using geometric algebra. Viewing quaternions as the even subalgebra of Cl(3,0,0), we extend the notion of rotary embeddings from quaternions to Clifford rotors acting on multivectors. This formulation enables two key generalizations: (1) extending rotary embeddings to arbitrary dimensions, and (2) encoding positional information in multivectors of multiple grades, not just vectors. We present preliminary experiments comparing spherical, quaternion, and Clifford-based rotary embeddings.

</details>


### [42] [Adaptive Stepsizing for Stochastic Gradient Langevin Dynamics in Bayesian Neural Networks](https://arxiv.org/abs/2511.11666)
*Rajit Rajpal,Benedict Leimkuhler,Yuanhao Jiang*

Main category: cs.LG

TL;DR: 提出了SA-SGLD自适应采样算法，通过时间重缩放自动调整步长，在曲率高的区域缩小步长，在平坦区域扩大步长，提高贝叶斯神经网络后验采样的稳定性和混合性。


<details>
  <summary>Details</summary>
Motivation: 现有的随机梯度MCMC方法对步长选择高度敏感，自适应变体如pSGLD需要昂贵的散度修正项才能正确采样不变测度。

Method: 基于SamAdams框架的时间步长自适应方案，使用时间重缩放根据监测量（通常是局部梯度范数）调节步长。

Result: 在高曲率2D玩具示例和使用尖锐先验的贝叶斯神经网络图像分类中，比SGLD实现了更准确的后验采样。

Conclusion: SA-SGLD能够在不引入偏差的情况下自动调整步长，提高采样稳定性和混合性。

Abstract: Bayesian neural networks (BNNs) require scalable sampling algorithms to approximate posterior distributions over parameters. Existing stochastic gradient Markov Chain Monte Carlo (SGMCMC) methods are highly sensitive to the choice of stepsize and adaptive variants such as pSGLD typically fail to sample the correct invariant measure without addition of a costly divergence correction term. In this work, we build on the recently proposed `SamAdams' framework for timestep adaptation (Leimkuhler, Lohmann, and Whalley 2025), introducing an adaptive scheme: SA-SGLD, which employs time rescaling to modulate the stepsize according to a monitored quantity (typically the local gradient norm). SA-SGLD can automatically shrink stepsizes in regions of high curvature and expand them in flatter regions, improving both stability and mixing without introducing bias. We show that our method can achieve more accurate posterior sampling than SGLD on high-curvature 2D toy examples and in image classification with BNNs using sharp priors.

</details>


### [43] [Beyond Superficial Forgetting: Thorough Unlearning through Knowledge Density Estimation and Block Re-insertion](https://arxiv.org/abs/2511.11667)
*Feng Guo,Yuntao Wen,Shen Gao,Junshuo Zhang,Shuo Shang*

Main category: cs.LG

TL;DR: 提出KUnBR方法，通过知识密度估计定位有害知识密集层，采用层重插入策略彻底消除LLM中的有害知识，在保持模型性能的同时实现最先进的遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习遗忘方法难以彻底移除有害知识，残留知识容易被恢复，需要解决隐私、合规和伦理问题。

Method: 基于知识密度估计识别有害知识密集层，设计层重插入策略绕过梯度阻塞，确保有效梯度传播。

Result: 在多个遗忘和通用能力基准测试中，KUnBR实现了最先进的遗忘性能，同时保持了模型效用。

Conclusion: KUnBR通过精确定位和有效消除有害知识，为LLM的安全部署提供了可靠的遗忘解决方案。

Abstract: Machine unlearning, which selectively removes harmful knowledge from a pre-trained model without retraining from scratch, is crucial for addressing privacy, regulatory compliance, and ethical concerns in Large Language Models (LLMs). However, existing unlearning methods often struggle to thoroughly remove harmful knowledge, leaving residual harmful knowledge that can be easily recovered. To address these limitations, we propose Knowledge Density-Guided Unlearning via Blocks Reinsertion (KUnBR), a novel approach that first identifies layers with rich harmful knowledge and then thoroughly eliminates the harmful knowledge via re-insertion strategy. Our method introduces knowledge density estimation to quantify and locate layers containing the most harmful knowledge, enabling precise unlearning. Additionally, we design a layer re-insertion strategy that extracts and re-inserts harmful knowledge-rich layers into the original LLM, bypassing gradient obstruction caused by cover layers and ensuring effective gradient propagation during unlearning. Extensive experiments conducted on several unlearning and general capability benchmarks demonstrate that KUnBR achieves state-of-the-art forgetting performance while maintaining model utility.

</details>


### [44] [Do traveling waves make good positional encodings?](https://arxiv.org/abs/2511.11668)
*Chase van de Geijn,Ayush Paliwal,Timo Lüddecke,Alexander S. Ecker*

Main category: cs.LG

TL;DR: 提出了RollPE，一种基于行波的新型位置编码机制，通过在自注意力中对查询和键张量应用循环滚动操作来实现相对位置编码，性能优于传统绝对位置嵌入，与RoPE相当。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用绝对正弦嵌入或学习的位置向量，而较新方法强调相对编码以更好地捕捉平移等变性。需要一种简单有效的位置编码机制。

Method: 使用循环滚动操作对查询和键张量进行处理，在位置间引入相对相位偏移，使模型能够基于位置差异而非绝对索引计算注意力。

Result: 该方法显著优于传统绝对位置嵌入，与RoPE性能相当。推导了RollPE的连续情况，在查询和键空间上隐式施加了地形结构。

Conclusion: RollPE与RoPE的特定配置存在数学等价性，通过行波视角可能简化RoPE并将其与大脑信息流动过程联系起来。

Abstract: Transformers rely on positional encoding to compensate for the inherent permutation invariance of self-attention. Traditional approaches use absolute sinusoidal embeddings or learned positional vectors, while more recent methods emphasize relative encodings to better capture translation equivariances. In this work, we propose RollPE, a novel positional encoding mechanism based on traveling waves, implemented by applying a circular roll operation to the query and key tensors in self-attention. This operation induces a relative shift in phase across positions, allowing the model to compute attention as a function of positional differences rather than absolute indices. We show this simple method significantly outperforms traditional absolute positional embeddings and is comparable to RoPE. We derive a continuous case of RollPE which implicitly imposes a topographic structure on the query and key space. We further derive a mathematical equivalence of RollPE to a particular configuration of RoPE. Viewing RollPE through the lens of traveling waves may allow us to simplify RoPE and relate it to processes of information flow in the brain.

</details>


### [45] [H-Model: Dynamic Neural Architectures for Adaptive Processing](https://arxiv.org/abs/2511.11669)
*Dmytro Hospodarchuk*

Main category: cs.LG

TL;DR: 提出了一种能够根据输入数据动态调整内部结构的神经网络架构，通过路由机制实现迭代和自适应计算，这是一个概念性原型而非性能优化的模型。


<details>
  <summary>Details</summary>
Motivation: 探索可适应和潜在更可解释的网络架构，让系统不仅学习表示，还能学习计算结构本身，而不是与现有语言模型竞争性能。

Method: 引入路由机制，允许每一层影响其输出在网络中的传播方式，实现基于输入数据和系统内部状态的条件化信息流。

Result: 由于计算资源和数据限制，这是初步研究，但初步观察显示有潜力，完整评估需要在更有利的计算条件下进行。

Conclusion: 提出了一个概念性架构框架，为探索自适应网络开辟了新方向，强调了架构创新而非性能优化的重要性。

Abstract: This article explores the design and experimentation of a neural network architecture capable of dynamically adjusting its internal structure based on the input data. The proposed model introduces a routing mechanism that allows each layer to influence how its outputs are propagated through the network, enabling iterative and adaptive computation. This concept is loosely inspired by the idea of thought processes and dynamic reasoning, where information flow is conditioned not only on the data itself, but also on the internal state of the system.
  It is important to note that this work does not aim to compete with state-of-the-art language models in terms of performance. Instead, it presents a conceptual prototype-an architectural framework that opens up a new direction for exploring adaptable and potentially more interpretable networks. The goal is not optimization of existing benchmarks but rather the proposal of a system that can learn not only representations, but also the structure of computation itself.
  Due to practical constraints in computing resources and data, this study remains a preliminary investigation. Nevertheless, initial observations show promise, and the architecture's full potential can only be evaluated in future experiments under more favorable computational conditions.

</details>


### [46] [Evaluation of LLM-based Explanations for a Learning Analytics Dashboard](https://arxiv.org/abs/2511.11671)
*Alina Deriyeva,Benjamin Paassen*

Main category: cs.LG

TL;DR: 本研究评估了使用大型语言模型为学习分析仪表板生成数据解释的效果，发现LLM生成的技能状态解释和学习建议比独立仪表板和教师解释更受青睐。


<details>
  <summary>Details</summary>
Motivation: 学习分析仪表板是支持数字学习环境中自我调节学习的强大工具，但其有效性受到数据可解释性的影响。需要辅助工具来帮助解释仪表板数据。

Method: 采用大型语言模型为仪表板数据生成语言解释，并在专家研究中与独立仪表板和教师提供的解释进行比较评估，涉及12名大学教育工作者。

Result: LLM生成的仪表板技能状态解释以及课程学习一般建议显著优于其他条件，更受参与者青睐。

Conclusion: 使用LLM进行解释目的可以增强学习者的学习体验，同时保持教师认可的教学标准。

Abstract: Learning Analytics Dashboards can be a powerful tool to support self-regulated learning in Digital Learning Environments and promote development of meta-cognitive skills, such as reflection. However, their effectiveness can be affected by the interpretability of the data they provide. To assist in the interpretation, we employ a large language model to generate verbal explanations of the data in the dashboard and evaluate it against a standalone dashboard and explanations provided by human teachers in an expert study with university level educators (N=12). We find that the LLM-based explanations of the skill state presented in the dashboard, as well as general recommendations on how to proceed with learning within the course are significantly more favored compared to the other conditions. This indicates that using LLMs for interpretation purposes can enhance the learning experience for learners while maintaining the pedagogical standards approved by teachers.

</details>


### [47] [Synergistic Feature Fusion for Latent Lyrical Classification: A Gated Deep Learning Architecture](https://arxiv.org/abs/2511.11673)
*M. A. Gameiro*

Main category: cs.LG

TL;DR: 提出了一种新颖的协同融合层（SFL）架构，通过门控机制将复杂的高维深度语义特征与简单的可解释结构线索相结合，用于歌词内容分类，在准确性和可靠性方面都优于传统的特征拼接方法。


<details>
  <summary>Details</summary>
Motivation: 解决如何将复杂的深度语义特征与简单的结构线索有效整合的挑战，以提升歌词内容分类的性能和可靠性。

Method: 使用门控机制的协同融合层（SFL）架构，用低维辅助特征（Fstruct）调制Sentence-BERT嵌入（Fdeep），将任务重构为二分类问题，区分主导同质聚类与其他内容。

Result: SFL模型达到0.9894的准确率和Macro F1分数，显著优于使用特征拼接的随机森林基线（准确率0.9868），校准误差降低93%，对数损失降低2.5倍。

Conclusion: 非线性门控机制优于简单的特征拼接，SFL模型为复杂多模态歌词分析提供了稳健可靠的解决方案。

Abstract: This study addresses the challenge of integrating complex, high-dimensional deep semantic features with simple, interpretable structural cues for lyrical content classification. We introduce a novel Synergistic Fusion Layer (SFL) architecture, a deep learning model utilizing a gated mechanism to modulate Sentence-BERT embeddings (Fdeep) using low-dimensional auxiliary features (Fstruct). The task, derived from clustering UMAP-reduced lyrical embeddings, is reframed as binary classification, distinguishing a dominant, homogeneous cluster (Class 0) from all other content (Class 1). The SFL model achieved an accuracy of 0.9894 and a Macro F1 score of 0.9894, outperforming a comprehensive Random Forest (RF) baseline that used feature concatenation (Accuracy = 0.9868). Crucially, the SFL model demonstrated vastly superior reliability and calibration, exhibiting a 93% reduction in Expected Calibration Error (ECE = 0.0035) and a 2.5x lower Log Loss (0.0304) compared to the RF baseline (ECE = 0.0500; Log Loss = 0.0772). This performance validates the architectural hypothesis that non-linear gating is superior to simple feature concatenation, establishing the SFL model as a robust and trustworthy system for complex multimodal lyrical analysis.

</details>


### [48] [Beyond One-Way Pruning: Bidirectional Pruning-Regrowth for Extreme Accuracy-Sparsity Tradeoff](https://arxiv.org/abs/2511.11675)
*Junchen Liu,Yi Sheng*

Main category: cs.LG

TL;DR: 提出了一种双向剪枝-再生策略，从极度压缩的网络开始，通过选择性再生关键连接来恢复性能，解决高稀疏度下模型性能急剧下降的问题。


<details>
  <summary>Details</summary>
Motivation: 当稀疏度超过一定阈值时，传统剪枝方法会导致模型性能急剧下降，限制了可实现的压缩比，无法满足某些硬件平台的严格尺寸约束。

Method: 双向剪枝-再生策略：从满足硬件约束的极度压缩网络开始，选择性再生关键连接来恢复丢失的性能。

Result: 该方法有效缓解了高稀疏度条件下常见的精度急剧下降问题。

Conclusion: 提出的双向剪枝-再生策略能够克服传统剪枝方法在高稀疏度下的性能限制，实现更高的压缩比同时保持模型性能。

Abstract: As a widely adopted model compression technique, model pruning has demonstrated strong effectiveness across various architectures. However, we observe that when sparsity exceeds a certain threshold, both iterative and one-shot pruning methods lead to a steep decline in model performance. This rapid degradation limits the achievable compression ratio and prevents models from meeting the stringent size constraints required by certain hardware platforms, rendering them inoperable. To overcome this limitation, we propose a bidirectional pruning-regrowth strategy. Starting from an extremely compressed network that satisfies hardware constraints, the method selectively regenerates critical connections to recover lost performance, effectively mitigating the sharp accuracy drop commonly observed under high sparsity conditions.

</details>


### [49] [Learning with Preserving for Continual Multitask Learning](https://arxiv.org/abs/2511.11676)
*Hanchen David Wang,Siwoo Bae,Zirong Chen,Meiyi Ma*

Main category: cs.LG

TL;DR: 提出了Learning with Preserving (LwP)框架，通过保持共享表示空间的几何结构来解决持续多任务学习中的灾难性遗忘问题，无需重放缓冲区。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶、医疗影像分析等关键领域，AI系统需要持续学习新任务而不遗忘旧能力。现有持续学习方法在此场景下表现不佳，因为它们学习的是碎片化的任务特定特征，容易相互干扰。

Method: 引入LwP框架，核心是动态加权距离保持(DWDP)损失函数，通过正则化潜在数据表示之间的成对距离来防止表示漂移，从而保持底层几何结构。

Result: 在时间序列和图像基准测试中，LwP不仅缓解了灾难性遗忘，还持续优于最先进的基线方法，是唯一超过强单任务学习基线的方法，对分布偏移表现出优越的鲁棒性。

Conclusion: LwP通过保持表示空间的几何结构，有效解决了持续多任务学习中的挑战，适用于现实世界的动态环境，特别适合隐私敏感的应用场景。

Abstract: Artificial intelligence systems in critical fields like autonomous driving and medical imaging analysis often continually learn new tasks using a shared stream of input data. For instance, after learning to detect traffic signs, a model may later need to learn to classify traffic lights or different types of vehicles using the same camera feed. This scenario introduces a challenging setting we term Continual Multitask Learning (CMTL), where a model sequentially learns new tasks on an underlying data distribution without forgetting previously learned abilities. Existing continual learning methods often fail in this setting because they learn fragmented, task-specific features that interfere with one another. To address this, we introduce Learning with Preserving (LwP), a novel framework that shifts the focus from preserving task outputs to maintaining the geometric structure of the shared representation space. The core of LwP is a Dynamically Weighted Distance Preservation (DWDP) loss that prevents representation drift by regularizing the pairwise distances between latent data representations. This mechanism of preserving the underlying geometric structure allows the model to retain implicit knowledge and support diverse tasks without requiring a replay buffer, making it suitable for privacy-conscious applications. Extensive evaluations on time-series and image benchmarks show that LwP not only mitigates catastrophic forgetting but also consistently outperforms state-of-the-art baselines in CMTL tasks. Notably, our method shows superior robustness to distribution shifts and is the only approach to surpass the strong single-task learning baseline, underscoring its effectiveness for real-world dynamic environments.

</details>


### [50] [Homotopy-Guided Self-Supervised Learning of Parametric Solutions for AC Optimal Power Flow](https://arxiv.org/abs/2511.11677)
*Shimiao Li,Aaron Tuor,Draguna Vrabie,Larry Pileggi,Jan Drgona*

Main category: cs.LG

TL;DR: 提出了一种基于同伦引导的自监督学习方法，用于解决参数化交流最优潮流问题，通过构建从松弛问题到原始问题的连续变形来改善收敛稳定性和可行性。


<details>
  <summary>Details</summary>
Motivation: 传统学习方法在处理非凸的AC-OPF问题时难以收敛到可行的高质量解，需要一种能够提高收敛稳定性和可行性的新方法。

Method: 采用同伦引导的自监督学习，构建从松弛问题到原始问题的连续变形过程，无需标记的最优解或外部求解器。

Result: 在标准IEEE AC-OPF基准测试中，该方法相比非同伦基线显著提高了可行性率，同时目标函数值与完整OPF求解器相当。

Conclusion: 同伦启发式方法在电力系统优化中展示了可扩展、约束感知的学习优化潜力。

Abstract: Learning to optimize (L2O) parametric approximations of AC optimal power flow (AC-OPF) solutions offers the potential for fast, reusable decision-making in real-time power system operations. However, the inherent nonconvexity of AC-OPF results in challenging optimization landscapes, and standard learning approaches often fail to converge to feasible, high-quality solutions. This work introduces a \textit{homotopy-guided self-supervised L2O method} for parametric AC-OPF problems. The key idea is to construct a continuous deformation of the objective and constraints during training, beginning from a relaxed problem with a broad basin of attraction and gradually transforming it toward the original problem. The resulting learning process improves convergence stability and promotes feasibility without requiring labeled optimal solutions or external solvers. We evaluate the proposed method on standard IEEE AC-OPF benchmarks and show that homotopy-guided L2O significantly increases feasibility rates compared to non-homotopy baselines, while achieving objective values comparable to full OPF solvers. These findings demonstrate the promise of homotopy-based heuristics for scalable, constraint-aware L2O in power system optimization.

</details>


### [51] [A neural optimization framework for free-boundary diffeomorphic mapping problems and its applications](https://arxiv.org/abs/2511.11679)
*Zhehao Xu,Lok Ming Lui*

Main category: cs.LG

TL;DR: 提出SBN-Opt框架，通过神经代理网络SBN嵌入LSQC能量，优化自由边界微分同胚映射，解决传统数值算法无法应用于梯度优化的问题


<details>
  <summary>Details</summary>
Motivation: 自由边界微分同胚优化在曲面映射中至关重要，但传统数值LSQC算法需要地标条件且无法用于梯度优化

Method: 提出SBN网络将LSQC能量嵌入多尺度网格谱架构，并构建SBN-Opt优化框架控制局部几何畸变

Result: 在密度均衡映射和不一致曲面配准实验中，SBN-Opt优于传统数值算法

Conclusion: SBN-Opt框架成功解决了自由边界微分同胚优化问题，实现了可控的局部几何畸变

Abstract: Free-boundary diffeomorphism optimization is a core ingredient in the surface mapping problem but remains notoriously difficult because the boundary is unconstrained and local bijectivity must be preserved under large deformation. Numerical Least-Squares Quasiconformal (LSQC) theory, with its provable existence, uniqueness, similarity-invariance and resolution-independence, offers an elegant mathematical remedy. However, the conventional numerical algorithm requires landmark conditioning, and cannot be applied into gradient-based optimization. We propose a neural surrogate, the Spectral Beltrami Network (SBN), that embeds LSQC energy into a multiscale mesh-spectral architecture. Next, we propose the SBN guided optimization framework SBN-Opt which optimizes free-boundary diffeomorphism for the problem, with local geometric distortion explicitly controllable. Extensive experiments on density-equalizing maps and inconsistent surface registration demonstrate our SBN-Opt's superiority over traditional numerical algorithms.

</details>


### [52] [Probabilistic Wildfire Susceptibility from Remote Sensing Using Random Forests and SHAP](https://arxiv.org/abs/2511.11680)
*Udaya Bhasker Cheerala,Varun Teja Chirukuri,Venkata Akhil Kumar Gummadi,Jintu Moni Bhuyan,Praveen Damacharla*

Main category: cs.LG

TL;DR: 使用随机森林算法结合可解释AI(SHAP)开发加州野火风险地图，识别不同生态系统的关键风险驱动因素，为决策提供支持。


<details>
  <summary>Details</summary>
Motivation: 全球野火对生态系统构成重大威胁，加州因气候、地形、植被和人类活动等因素频繁发生火灾，需要开发综合风险评估方法来支持防灾决策。

Method: 应用随机森林算法构建野火风险预测模型，结合SHAP方法进行模型解释，采用空间和时间验证策略评估模型性能。

Result: RF模型表现出色，森林和草地的AUC分别达到0.997和0.996；SHAP分析识别出土壤有机碳、树木覆盖和NDVI是森林的关键驱动因素，而地表温度、海拔和植被健康指数主导草地风险。

Conclusion: RF-SHAP框架为野火风险评估提供了稳健、可解释且适应性强的方法，有助于制定有针对性的减灾策略。

Abstract: Wildfires pose a significant global threat to ecosystems worldwide, with California experiencing recurring fires due to various factors, including climate, topographical features, vegetation patterns, and human activities. This study aims to develop a comprehensive wildfire risk map for California by applying the random forest (RF) algorithm, augmented with Explainable Artificial Intelligence (XAI) through Shapley Additive exPlanations (SHAP), to interpret model predictions. Model performance was assessed using both spatial and temporal validation strategies. The RF model demonstrated strong predictive performance, achieving near-perfect discrimination for grasslands (AUC = 0.996) and forests (AUC = 0.997). Spatial cross-validation revealed moderate transferability, yielding ROC-AUC values of 0.6155 for forests and 0.5416 for grasslands. In contrast, temporal split validation showed enhanced generalization, especially for forests (ROC-AUC = 0.6615, PR-AUC = 0.8423). SHAP-based XAI analysis identified key ecosystem-specific drivers: soil organic carbon, tree cover, and Normalized Difference Vegetation Index (NDVI) emerged as the most influential in forests, whereas Land Surface Temperature (LST), elevation, and vegetation health indices were dominant in grasslands. District-level classification revealed that Central Valley and Northern Buttes districts had the highest concentration of high-risk grasslands, while Northern Buttes and North Coast Redwoods dominated forested high-risk areas. This RF-SHAP framework offers a robust, comprehensible, and adaptable method for assessing wildfire risks, enabling informed decisions and creating targeted strategies to mitigate dangers.

</details>


### [53] [MPCM-Net: Multi-scale network integrates partial attention convolution with Mamba for ground-based cloud image segmentation](https://arxiv.org/abs/2511.11681)
*Penghui Niu,Jiashuai She,Taotao Cai,Yajuan Zhang,Ping Zhang,Junhua Gu,Jianxin Li*

Main category: cs.LG

TL;DR: 提出MPCM-Net网络，结合部分注意力卷积和Mamba架构，用于地面云图像分割，在CSRC数据集上实现分割精度和推理速度的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法存在三个主要局限：依赖空洞卷积缺乏通道间互操作性；注意力机制忽视精度-吞吐量平衡；解码器修改未能建立层次局部特征的全局依赖关系。

Method: 编码器包含MPAC模块（MPC块和MPA块），实现多尺度云形成的全局空间交互和低计算复杂度特征提取；解码器使用M2B模块通过SSHD保持线性复杂度进行深度特征聚合。

Result: 在CSRC数据集上的广泛实验表明，MPCM-Net优于现有最先进方法，在分割精度和推理速度之间达到最优平衡。

Conclusion: MPCM-Net通过集成部分注意力卷积和Mamba架构，有效解决了现有云图像分割方法的局限性，同时发布了CSRC数据集作为新的分割基准。

Abstract: Ground-based cloud image segmentation is a critical research domain for photovoltaic power forecasting. Current deep learning approaches primarily focus on encoder-decoder architectural refinements. However, existing methodologies exhibit several limitations:(1)they rely on dilated convolutions for multi-scale context extraction, lacking the partial feature effectiveness and interoperability of inter-channel;(2)attention-based feature enhancement implementations neglect accuracy-throughput balance; and (3)the decoder modifications fail to establish global interdependencies among hierarchical local features, limiting inference efficiency. To address these challenges, we propose MPCM-Net, a Multi-scale network that integrates Partial attention Convolutions with Mamba architectures to enhance segmentation accuracy and computational efficiency. Specifically, the encoder incorporates MPAC, which comprises:(1)a MPC block with ParCM and ParSM that enables global spatial interaction across multi-scale cloud formations, and (2)a MPA block combining ParAM and ParSM to extract discriminative features with reduced computational complexity. On the decoder side, a M2B is employed to mitigate contextual loss through a SSHD that maintains linear complexity while enabling deep feature aggregation across spatial and scale dimensions. As a key contribution to the community, we also introduce and release a dataset CSRC, which is a clear-label, fine-grained segmentation benchmark designed to overcome the critical limitations of existing public datasets. Extensive experiments on CSRC demonstrate the superior performance of MPCM-Net over state-of-the-art methods, achieving an optimal balance between segmentation accuracy and inference speed. The dataset and source code will be available at https://github.com/she1110/CSRC.

</details>


### [54] [Stratified Knowledge-Density Super-Network for Scalable Vision Transformers](https://arxiv.org/abs/2511.11683)
*Longhua Li,Lei Qi,Xin Geng*

Main category: cs.LG

TL;DR: 提出WPAC和PIAD方法，将预训练ViT转换为分层知识密度超网络，实现灵活提取不同大小的子网络，同时保持最大知识保留。


<details>
  <summary>Details</summary>
Motivation: 为不同资源约束训练和部署多个ViT模型成本高且效率低，需要一种更高效的模型压缩和扩展方法。

Method: WPAC通过加权PCA压缩注意力层知识，PIAD通过渐进重要性感知dropout促进知识分层组织。

Result: WPAC在知识集中方面优于现有剪枝标准，与PIAD结合为模型压缩和扩展提供了强大的替代方案。

Conclusion: 该方法能够高效地将预训练ViT转换为分层知识密度超网络，实现灵活的子网络提取，在模型压缩和扩展方面表现优异。

Abstract: Training and deploying multiple vision transformer (ViT) models for different resource constraints is costly and inefficient. To address this, we propose transforming a pre-trained ViT into a stratified knowledge-density super-network, where knowledge is hierarchically organized across weights. This enables flexible extraction of sub-networks that retain maximal knowledge for varying model sizes. We introduce \textbf{W}eighted \textbf{P}CA for \textbf{A}ttention \textbf{C}ontraction (WPAC), which concentrates knowledge into a compact set of critical weights. WPAC applies token-wise weighted principal component analysis to intermediate features and injects the resulting transformation and inverse matrices into adjacent layers, preserving the original network function while enhancing knowledge compactness. To further promote stratified knowledge organization, we propose \textbf{P}rogressive \textbf{I}mportance-\textbf{A}ware \textbf{D}ropout (PIAD). PIAD progressively evaluates the importance of weight groups, updates an importance-aware dropout list, and trains the super-network under this dropout regime to promote knowledge stratification. Experiments demonstrate that WPAC outperforms existing pruning criteria in knowledge concentration, and the combination with PIAD offers a strong alternative to state-of-the-art model compression and model expansion methods.

</details>


### [55] [A Bayesian Model for Multi-stage Censoring](https://arxiv.org/abs/2511.11684)
*Shuvom Sadhuka,Sophia Lin,Emma Pierson,Bonnie Berger*

Main category: cs.LG

TL;DR: 该论文提出了一种用于医疗决策漏斗结构的贝叶斯模型，解决了因选择性地表露真实结果而导致的统计偏差问题，特别是在服务不足的患者群体中。


<details>
  <summary>Details</summary>
Motivation: 医疗决策中常见的漏斗结构（如筛查、评估等阶段）导致真实结果只在最后阶段显现，这种选择性审查会引入统计偏差，尤其是在结果更频繁被审查的服务不足患者群体中。

Method: 开发了一个基于选择性标签和审查先验工作的贝叶斯模型，用于漏斗决策结构。在合成设置中验证模型能恢复真实参数并更准确地预测被审查患者的结果。

Result: 在急诊科就诊数据集上应用该模型，发现在医院和ICU入院方面存在基于性别的差异。模型估计女性进入ICU的死亡风险阈值（5.1%）高于男性（4.5%）。

Conclusion: 该贝叶斯模型能够有效处理医疗决策漏斗结构中的选择性审查问题，揭示了医疗服务中存在的性别差异，为改善医疗公平性提供了工具。

Abstract: Many sequential decision settings in healthcare feature funnel structures characterized by a series of stages, such as screenings or evaluations, where the number of patients who advance to each stage progressively decreases and decisions become increasingly costly. For example, an oncologist may first conduct a breast exam, followed by a mammogram for patients with concerning exams, followed by a biopsy for patients with concerning mammograms. A key challenge is that the ground truth outcome, such as the biopsy result, is only revealed at the end of this funnel. The selective censoring of the ground truth can introduce statistical biases in risk estimation, especially in underserved patient groups, whose outcomes are more frequently censored. We develop a Bayesian model for funnel decision structures, drawing from prior work on selective labels and censoring. We first show in synthetic settings that our model is able to recover the true parameters and predict outcomes for censored patients more accurately than baselines. We then apply our model to a dataset of emergency department visits, where in-hospital mortality is observed only for those who are admitted to either the hospital or ICU. We find that there are gender-based differences in hospital and ICU admissions. In particular, our model estimates that the mortality risk threshold to admit women to the ICU is higher for women (5.1%) than for men (4.5%).

</details>


### [56] [R-Tuning: Wavelet-Decomposed Replay and Semantic Alignment for Continual Adaptation of Pretrained Time-Series Models](https://arxiv.org/abs/2511.11685)
*Tianyi Yin,Jingwei Wang,Chenze Wang,Han Wang,Jiexuan Cai,Min Liu,Yunlong Ma,Kun Gao,Yuting Song,Weiming Shen*

Main category: cs.LG

TL;DR: R-Tuning是一种用于预训练时间序列模型持续适应的新框架，通过频率感知回放策略和潜在一致性约束解决灾难性遗忘问题，显著提升新旧任务的性能。


<details>
  <summary>Details</summary>
Motivation: 预训练模型在时间序列预测中表现出色，但适应不断变化的数据分布面临挑战，主要障碍是无法访问原始训练数据，仅在新数据上微调会导致灾难性遗忘。

Method: 提出R-Tuning框架，通过频率感知回放策略构建统一潜在空间，使用小波分解生成趋势保持和融合增强的变体样本，并引入潜在一致性约束对齐新旧表示。

Result: 实验结果显示R-Tuning显著优于现有方法，在新任务上MAE和MSE降低高达46.9%和46.8%，在旧任务上性能提升达5.7%和6.0%，在少样本设置下即使合成样本仅占新任务数据5%也能超越所有基线。

Conclusion: R-Tuning通过创新的回放策略和潜在约束机制，有效解决了预训练时间序列模型的持续适应问题，在保持先验知识的同时实现了对新任务的鲁棒适应。

Abstract: Pre-trained models have demonstrated exceptional generalization capabilities in time-series forecasting; however, adapting them to evolving data distributions remains a significant challenge. A key hurdle lies in accessing the original training data, as fine-tuning solely on new data often leads to catastrophic forgetting. To address this issue, we propose Replay Tuning (R-Tuning), a novel framework designed for the continual adaptation of pre-trained time-series models. R-Tuning constructs a unified latent space that captures both prior and current task knowledge through a frequency-aware replay strategy. Specifically, it augments model-generated samples via wavelet-based decomposition across multiple frequency bands, generating trend-preserving and fusion-enhanced variants to improve representation diversity and replay efficiency. To further reduce reliance on synthetic samples, R-Tuning introduces a latent consistency constraint that aligns new representations with the prior task space. This constraint guides joint optimization within a compact and semantically coherent latent space, ensuring robust knowledge retention and adaptation. Extensive experimental results demonstrate the superiority of R-Tuning, which reduces MAE and MSE by up to 46.9% and 46.8%, respectively, on new tasks, while preserving prior knowledge with gains of up to 5.7% and 6.0% on old tasks. Notably, under few-shot settings, R-Tuning outperforms all state-of-the-art baselines even when synthetic proxy samples account for only 5% of the new task dataset.

</details>


### [57] [Regularized Schrödinger: Alleviating Distortion and Exposure Bias in Solving Inverse Problems](https://arxiv.org/abs/2511.11686)
*Qing Yao,Lijian Gao,Qirong Mao,Dong Ming*

Main category: cs.LG

TL;DR: 提出了正则化薛定谔桥(RSB)方法解决扩散模型在逆问题中的失真-感知权衡和曝光偏差问题，通过正则化训练策略提升语音增强性能。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在逆问题中的两个关键挑战：1)失真-感知权衡，即提升感知质量会降低重建保真度；2)曝光偏差问题，训练-推理输入不匹配导致预测误差累积和重建质量下降。

Method: 提出正则化薛定谔桥(RSB)，采用新颖的正则化训练策略，扰动输入状态和目标，通过暴露模型于模拟预测误差来缓解曝光偏差，并通过后验均值的精心设计插值来减轻失真。

Result: 在语音增强的两个典型逆问题上进行广泛实验，RSB优于最先进方法，显著改善失真指标并有效减少曝光偏差。

Conclusion: RSB是专门为逆问题设计的薛定谔桥改进版本，成功解决了扩散模型在失真-感知权衡和曝光偏差方面的局限性，在语音增强任务中表现出优越性能。

Abstract: Diffusion models serve as a powerful generative framework for solving inverse problems. However, they still face two key challenges: 1) the distortion-perception tradeoff, where improving perceptual quality often degrades reconstruction fidelity, and 2) the exposure bias problem, where the training-inference input mismatch leads to prediction error accumulation and reduced reconstruction quality. In this work, we propose the Regularized Schrödinger Bridge (RSB), an adaptation of Schrödinger Bridge tailored for inverse problems that addresses the above limitations. RSB employs a novel regularized training strategy that perturbs both the input states and targets, effectively mitigating exposure bias by exposing the model to simulated prediction errors and also alleviating distortion by well-designed interpolation via the posterior mean. Extensive experiments on two typical inverse problems for speech enhancement demonstrate that RSB outperforms state-of-the-art methods, significantly improving distortion metrics and effectively reducing exposure bias.

</details>


### [58] [Hierarchical Schedule Optimization for Fast and Robust Diffusion Model Sampling](https://arxiv.org/abs/2511.11688)
*Aihua Zhu,Rui Su,Qinglin Zhao,Li Feng,Meng Shen,Shibo He*

Main category: cs.LG

TL;DR: HSO是一个高效的双层优化框架，通过全局搜索和局部优化相结合的方式，在极低函数评估次数下显著加速扩散模型的采样过程，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 扩散概率模型虽然生成质量高，但迭代采样过程缓慢。现有的调度优化方法难以同时满足有效性、自适应性、实用鲁棒性和计算效率四个核心原则。

Method: 提出分层调度优化器(HSO)，包含上层全局搜索最优初始化策略和下层局部优化调度细化，使用中点误差代理(MEP)和间距惩罚适应度(SPF)函数来指导优化过程。

Result: 在极低NFE设置下达到最先进水平，例如仅用5次函数评估就在Stable Diffusion v2.1上获得11.94的FID分数，优化成本不到8秒。

Conclusion: HSO提供了一种高度实用且高效的扩散模型加速范式，无需昂贵的重新训练即可实现显著的性能提升。

Abstract: Diffusion probabilistic models have set a new standard for generative fidelity but are hindered by a slow iterative sampling process. A powerful training-free strategy to accelerate this process is Schedule Optimization, which aims to find an optimal distribution of timesteps for a fixed and small Number of Function Evaluations (NFE) to maximize sample quality. To this end, a successful schedule optimization method must adhere to four core principles: effectiveness, adaptivity, practical robustness, and computational efficiency. However, existing paradigms struggle to satisfy these principles simultaneously, motivating the need for a more advanced solution. To overcome these limitations, we propose the Hierarchical-Schedule-Optimizer (HSO), a novel and efficient bi-level optimization framework. HSO reframes the search for a globally optimal schedule into a more tractable problem by iteratively alternating between two synergistic levels: an upper-level global search for an optimal initialization strategy and a lower-level local optimization for schedule refinement. This process is guided by two key innovations: the Midpoint Error Proxy (MEP), a solver-agnostic and numerically stable objective for effective local optimization, and the Spacing-Penalized Fitness (SPF) function, which ensures practical robustness by penalizing pathologically close timesteps. Extensive experiments show that HSO sets a new state-of-the-art for training-free sampling in the extremely low-NFE regime. For instance, with an NFE of just 5, HSO achieves a remarkable FID of 11.94 on LAION-Aesthetics with Stable Diffusion v2.1. Crucially, this level of performance is attained not through costly retraining, but with a one-time optimization cost of less than 8 seconds, presenting a highly practical and efficient paradigm for diffusion model acceleration.

</details>


### [59] [Doubly Debiased Test-Time Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2511.11690)
*Fei Song,Yi Li,Rui Wang,Jiahuan Zhou,Changwen Zheng,Jiangmeng Li*

Main category: cs.LG

TL;DR: 提出了双重去偏测试时提示调优方法，通过动态检索增强调制和可靠性感知提示优化来缓解视觉语言模型在零样本设置下的提示优化偏差问题。


<details>
  <summary>Details</summary>
Motivation: 测试时提示调优仅基于未标记测试数据可能导致提示优化偏差，造成次优性能。从模型和数据角度分析偏差原因：模型方面熵最小化目标忽视预测正确性，数据方面偏差提示导致视觉-文本模态错位。

Method: 1) 动态检索增强调制模块：用测试图像特征检索高置信度知识来调制预测；2) 可靠性感知提示优化模块：基于置信度加权集成和跨模态一致性蒸馏进行正则化约束。

Result: 在15个基准数据集上的实验表明，该方法在自然分布偏移和跨数据集泛化场景下均优于基线方法。

Conclusion: 所提出的双重去偏方法能有效缓解提示优化偏差，提升视觉语言模型在零样本设置下的泛化性能。

Abstract: Test-time prompt tuning for vision-language models has demonstrated impressive generalization capabilities under zero-shot settings. However, tuning the learnable prompts solely based on unlabeled test data may induce prompt optimization bias, ultimately leading to suboptimal performance on downstream tasks. In this work, we analyze the underlying causes of prompt optimization bias from both the model and data perspectives. In terms of the model, the entropy minimization objective typically focuses on reducing the entropy of model predictions while overlooking their correctness. This can result in overconfident yet incorrect outputs, thereby compromising the quality of prompt optimization. On the data side, prompts affected by optimization bias can introduce misalignment between visual and textual modalities, which further aggravates the prompt optimization bias. To this end, we propose a Doubly Debiased Test-Time Prompt Tuning method. Specifically, we first introduce a dynamic retrieval-augmented modulation module that retrieves high-confidence knowledge from a dynamic knowledge base using the test image feature as a query, and uses the retrieved knowledge to modulate the predictions. Guided by the refined predictions, we further develop a reliability-aware prompt optimization module that incorporates a confidence-based weighted ensemble and cross-modal consistency distillation to impose regularization constraints during prompt tuning. Extensive experiments across 15 benchmark datasets involving both natural distribution shifts and cross-datasets generalization demonstrate that our method outperforms baselines, validating its effectiveness in mitigating prompt optimization bias.

</details>


### [60] [Beyond saliency: enhancing explanation of speech emotion recognition with expert-referenced acoustic cues](https://arxiv.org/abs/2511.11691)
*Seham Nasr,Zhao Ren,David Johnson*

Main category: cs.LG

TL;DR: 提出一个可解释AI框架，通过量化显著区域内的声学线索幅度，将语音情感识别模型的显著性区域与专家参考的声学线索连接起来，提高解释质量。


<details>
  <summary>Details</summary>
Motivation: 当前基于显著性的方法虽然能突出频谱图区域，但无法显示这些区域是否对应有意义的情绪声学标记，限制了忠实性和可解释性。

Method: 提出一个框架，量化显著区域内的线索幅度，明确连接显著性区域与理论驱动的语音情绪专家参考声学特征。

Result: 在基准SER数据集上的实验表明，该方法通过明确连接显著区域与理论驱动的语音情绪专家参考声学，提高了解释质量。

Conclusion: 相比标准显著性方法，提供了更易理解和合理的SER模型解释，为可信赖的基于语音的情感计算奠定了基础。

Abstract: Explainable AI (XAI) for Speech Emotion Recognition (SER) is critical for building transparent, trustworthy models. Current saliency-based methods, adapted from vision, highlight spectrogram regions but fail to show whether these regions correspond to meaningful acoustic markers of emotion, limiting faithfulness and interpretability. We propose a framework that overcomes these limitations by quantifying the magnitudes of cues within salient regions. This clarifies "what" is highlighted and connects it to "why" it matters, linking saliency to expert-referenced acoustic cues of speech emotions. Experiments on benchmark SER datasets show that our approach improves explanation quality by explicitly linking salient regions to theory-driven speech emotions expert-referenced acoustics. Compared to standard saliency methods, it provides more understandable and plausible explanations of SER models, offering a foundational step towards trustworthy speech-based affective computing.

</details>


### [61] [AnchorDS: Anchoring Dynamic Sources for Semantically Consistent Text-to-3D Generation](https://arxiv.org/abs/2511.11692)
*Jiayin Zhu,Linlin Yang,Yicong Li,Angela Yao*

Main category: cs.LG

TL;DR: 本文提出AnchorDS方法解决文本到3D生成中的语义过平滑问题，通过将优化重新表述为动态源分布到固定目标分布的映射，引入图像条件锚定源分布，提供更稳定的指导。


<details>
  <summary>Details</summary>
Motivation: 现有基于优化的文本到3D方法将2D生成模型的指导视为静态，忽略了源动态性，导致语义线索被抑制或合并，产生语义过平滑伪影。

Method: 将问题重新表述为双条件潜空间，同时基于文本提示和中间渲染图像进行条件化。引入AnchorDS机制，通过图像条件锚定当前源分布，提供状态锚定指导，并设计轻量级过滤和微调策略来细化锚点。

Result: AnchorDS产生更精细的细节、更自然的颜色和更强的语义一致性，特别是在复杂提示下，同时保持效率。在质量和效率上都优于先前方法。

Conclusion: 通过考虑源动态性并引入图像条件锚定，AnchorDS显著改善了文本到3D生成的质量和稳定性，解决了语义过平滑问题。

Abstract: Optimization-based text-to-3D methods distill guidance from 2D generative models via Score Distillation Sampling (SDS), but implicitly treat this guidance as static. This work shows that ignoring source dynamics yields inconsistent trajectories that suppress or merge semantic cues, leading to "semantic over-smoothing" artifacts. As such, we reformulate text-to-3D optimization as mapping a dynamically evolving source distribution to a fixed target distribution. We cast the problem into a dual-conditioned latent space, conditioned on both the text prompt and the intermediately rendered image. Given this joint setup, we observe that the image condition naturally anchors the current source distribution. Building on this insight, we introduce AnchorDS, an improved score distillation mechanism that provides state-anchored guidance with image conditions and stabilizes generation. We further penalize erroneous source estimates and design a lightweight filter strategy and fine-tuning strategy that refines the anchor with negligible overhead. AnchorDS produces finer-grained detail, more natural colours, and stronger semantic consistency, particularly for complex prompts, while maintaining efficiency. Extensive experiments show that our method surpasses previous methods in both quality and efficiency.

</details>


### [62] [Toward Dignity-Aware AI: Next-Generation Elderly Monitoring from Fall Detection to ADL](https://arxiv.org/abs/2511.11696)
*Xun Shao,Aoba Otani,Yuto Hirasuka,Runji Cai,Seng W. Loke*

Main category: cs.LG

TL;DR: 本文提出了下一代老年人监测系统，从跌倒检测扩展到日常生活活动(ADL)识别，旨在开发保护隐私、边缘部署的联邦AI系统，支持老年人在智能家居环境中的独立生活。


<details>
  <summary>Details</summary>
Motivation: 当前老年人监测系统主要关注跌倒检测，但需要向更全面的日常生活活动识别发展，以更好地支持老年人独立生活，同时保护隐私和尊严。

Method: 使用SISFall数据集及其GAN增强变体进行可行性验证，将跌倒检测作为代理任务；研究非独立同分布条件下的联邦学习，并在Jetson Orin Nano设备上进行嵌入式部署。

Result: 初步验证了联邦学习和边缘部署的可行性，为全面ADL监测系统提供了早期证据。

Conclusion: 本文为从单一任务检测向全面日常活动识别的转变提供了路线图，强调了可持续和以人为本的老年人护理AI发展方向，并指出了领域偏移、数据稀缺和隐私风险等开放挑战。

Abstract: This position paper envisions a next-generation elderly monitoring system that moves beyond fall detection toward the broader goal of Activities of Daily Living (ADL) recognition. Our ultimate aim is to design privacy-preserving, edge-deployed, and federated AI systems that can robustly detect and understand daily routines, supporting independence and dignity in aging societies. At present, ADL-specific datasets are still under collection. As a preliminary step, we demonstrate feasibility through experiments using the SISFall dataset and its GAN-augmented variants, treating fall detection as a proxy task. We report initial results on federated learning with non-IID conditions, and embedded deployment on Jetson Orin Nano devices. We then outline open challenges such as domain shift, data scarcity, and privacy risks, and propose directions toward full ADL monitoring in smart-room environments. This work highlights the transition from single-task detection to comprehensive daily activity recognition, providing both early evidence and a roadmap for sustainable and human-centered elderly care AI.

</details>


### [63] [Benchmarking GNNs for OOD Materials Property Prediction with Uncertainty Quantification](https://arxiv.org/abs/2511.11697)
*Liqin Tan,Pin Chen,Menghan Liu,Xiean Wang,Jianhuan Cen,Qingsong Zou*

Main category: cs.LG

TL;DR: MatUQ是一个用于评估图神经网络在材料属性预测中分布外泛化能力和不确定性量化的基准框架，包含1,375个OOD预测任务，提出了新的结构感知分割策略SOAP-LOCO和不确定性度量D-EviU。


<details>
  <summary>Details</summary>
Motivation: 现有材料属性预测模型在分布外场景下的泛化能力和不确定性量化评估不足，需要系统性的基准框架来指导模型选择。

Method: 构建包含6个材料数据集的1,375个OOD预测任务，使用5种OFM分割和新提出的SOAP-LOCO分割策略，评估12个代表性GNN模型，采用蒙特卡洛Dropout和深度证据回归的统一不确定性训练协议。

Result: 不确定性训练方法平均减少70.6%的预测误差；D-EviU度量在大多数任务中与预测误差相关性最强；没有单一模型在所有任务中表现最优，早期模型如SchNet和ALIGNN仍具竞争力，新模型在特定属性上表现更优。

Conclusion: MatUQ为材料发现中的分布偏移场景提供了实用的模型选择指导，不确定性训练显著提升模型可靠性，不同模型在不同材料属性上各有优势。

Abstract: We present MatUQ, a benchmark framework for evaluating graph neural networks (GNNs) on out-of-distribution (OOD) materials property prediction with uncertainty quantification (UQ). MatUQ comprises 1,375 OOD prediction tasks constructed from six materials datasets using five OFM-based and a newly proposed structure-aware splitting strategy, SOAP-LOCO, which captures local atomic environments more effectively. We evaluate 12 representative GNN models under a unified uncertainty-aware training protocol that combines Monte Carlo Dropout and Deep Evidential Regression (DER), and introduce a novel uncertainty metric, D-EviU, which shows the strongest correlation with prediction errors in most tasks. Our experiments yield two key findings. First, the uncertainty-aware training approach significantly improves model prediction accuracy, reducing errors by an average of 70.6\% across challenging OOD scenarios. Second, the benchmark reveals that no single model dominates universally: earlier models such as SchNet and ALIGNN remain competitive, while newer models like CrystalFramer and SODNet demonstrate superior performance on specific material properties. These results provide practical insights for selecting reliable models under distribution shifts in materials discovery.

</details>


### [64] [Moirai 2.0: When Less Is More for Time Series Forecasting](https://arxiv.org/abs/2511.11698)
*Chenghao Liu,Taha Aksu,Juncheng Liu,Xu Liu,Hanshu Yan,Quang Pham,Doyen Sahoo,Caiming Xiong,Silvio Savarese,Junnan Li*

Main category: cs.LG

TL;DR: Moirai 2.0是一个基于解码器架构的时间序列基础模型，采用分位数预测和多令牌预测技术，在准确性和推理效率方面均有提升，相比前代模型速度提升2倍、尺寸缩小30倍。


<details>
  <summary>Details</summary>
Motivation: 开发一个更高效、更准确的时间序列预测模型，通过简化架构和改进预测方法来解决传统模型在效率、准确性和模型大小之间的权衡问题。

Method: 采用解码器专用架构、单patch输入和分位数损失函数，替代了前代模型的掩码编码器训练、多patch输入和混合分布输出方法。

Result: 在Gift-Eval基准测试中表现优异，在准确度、速度和模型大小之间达到良好平衡，优于同系列更大模型，并展现出稳健的领域级结果。

Conclusion: Moirai 2.0通过架构简化实现了显著的效率提升，但模型性能随参数增加而趋于平稳，在长时预测中性能下降，未来需要在数据扩展和长时建模方面进一步研究。

Abstract: We introduce Moirai 2.0, a decoder-only time-series foundation model trained on a new corpus of 36M series. The model adopts quantile forecasting and multi-token prediction, improving both probabilistic accuracy and inference efficiency. On the Gift-Eval benchmark, it ranks among the top pretrained models while achieving a strong trade-off between accuracy, speed, and model size. Compared to Moirai 1.0, Moirai 2.0 replaces masked-encoder training, multi-patch inputs, and mixture-distribution outputs with a simpler decoder-only architecture, single patch, and quantile loss. Ablation studies isolate these changes -- showing that the decoder-only backbone along with recursive multi-quantile decoding contribute most to the gains. Additional experiments show that Moirai 2.0 outperforms larger models from the same family and exhibits robust domain-level results. In terms of efficiency and model size, Moirai 2.0 is twice as fast and thirty times smaller than its prior best version, Moirai 1.0-Large, while also performing better. Model performance plateaus with increasing parameter count and declines at longer horizons, motivating future work on data scaling and long-horizon modeling. We release code and evaluation details to support further research.

</details>


### [65] [Tighter Truncated Rectangular Prism Approximation for RNN Robustness Verification](https://arxiv.org/abs/2511.11699)
*Xingqi Lin,Liangyu Chen,Min Wu,Min Zhang,Zhenbing Zeng*

Main category: cs.LG

TL;DR: 提出了一种新的截断矩形棱柱方法来紧密包围Hadamard积生成的三维非线性曲面，通过两个线性松弛平面和细化驱动方法最小化体积和表面积，实现更紧密的过近似，从而提升RNN鲁棒性验证的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法对非线性激活函数进行线性过近似时，单独使用线性边界平面可能导致显著的过估计，从而降低验证准确性。需要更紧密地包围Hadamard积生成的三维非线性曲面。

Method: 提出截断矩形棱柱方法，使用两个线性松弛平面和细化驱动方法最小化体积和表面积，实现更紧密的过近似。基于此实现了DeepPrism原型系统。

Result: 实验结果表明，DeepPrism在图像分类、语音识别和情感分析等多种任务中相比最先进方法有显著改进。

Conclusion: 提出的截断矩形棱柱方法能够更紧密地过近似非线性激活函数，显著提升了RNN鲁棒性验证的准确性。

Abstract: Robustness verification is a promising technique for rigorously proving Recurrent Neural Networks (RNNs) robustly. A key challenge is to over-approximate the nonlinear activation functions with linear constraints, which can transform the verification problem into an efficiently solvable linear programming problem. Existing methods over-approximate the nonlinear parts with linear bounding planes individually, which may cause significant over-estimation and lead to lower verification accuracy. In this paper, in order to tightly enclose the three-dimensional nonlinear surface generated by the Hadamard product, we propose a novel truncated rectangular prism formed by two linear relaxation planes and a refinement-driven method to minimize both its volume and surface area for tighter over-approximation. Based on this approximation, we implement a prototype DeepPrism for RNN robustness verification. The experimental results demonstrate that \emph{DeepPrism} has significant improvement compared with the state-of-the-art approaches in various tasks of image classification, speech recognition and sentiment analysis.

</details>


### [66] [Bayesian Neural Networks with Monte Carlo Dropout for Probabilistic Electricity Price Forecasting](https://arxiv.org/abs/2511.11701)
*Abhinav Das,Stephan Schlüter*

Main category: cs.LG

TL;DR: 提出基于贝叶斯神经网络和蒙特卡洛dropout的概率性电价预测框架，按小时分别建模以捕捉日内模式，在点预测和区间预测方面均优于传统基准模型。


<details>
  <summary>Details</summary>
Motivation: 电力市场中的电价波动性源于复杂的供需动态和外部因素，传统点预测无法捕捉内在不确定性，限制了风险管理效用。

Method: 使用贝叶斯神经网络(BNN)结合蒙特卡洛dropout，为每天每个小时分别训练模型以捕捉日内模式，并与GARCHX和LEAR基准模型进行比较。

Result: 提出的模型在点预测和区间预测方面均优于GARCHX和LEAR基准模型。

Conclusion: 该工作为在能源市场预测中利用概率性神经网络模型提供了参考依据。

Abstract: Accurate electricity price forecasting is critical for strategic decision-making in deregulated electricity markets, where volatility stems from complex supply-demand dynamics and external factors. Traditional point forecasts often fail to capture inherent uncertainties, limiting their utility for risk management. This work presents a framework for probabilistic electricity price forecasting using Bayesian neural networks (BNNs) with Monte Carlo (MC) dropout, training separate models for each hour of the day to capture diurnal patterns. A critical assessment and comparison with the benchmark model, namely: generalized autoregressive conditional heteroskedasticity with exogenous variable (GARCHX) model and the LASSO estimated auto-regressive model (LEAR), highlights that the proposed model outperforms the benchmark models in terms of point prediction and intervals. This work serves as a reference for leveraging probabilistic neural models in energy market predictions.

</details>


### [67] [Enhancing Reinforcement Learning in 3D Environments through Semantic Segmentation: A Case Study in ViZDoom](https://arxiv.org/abs/2511.11703)
*Hugo Huang*

Main category: cs.LG

TL;DR: 提出两种基于语义分割的输入表示方法（SS-only和RGB+SS），在ViZDoom环境中显著减少强化学习的内存消耗并提升性能，同时探索了密度热图用于可视化智能体移动模式。


<details>
  <summary>Details</summary>
Motivation: 解决3D环境中强化学习面临的两个主要挑战：高维感官输入导致的内存缓冲区高内存消耗，以及在部分可观察马尔可夫决策过程中的学习复杂性。

Method: 使用语义分割技术处理RGB彩色图像，提出SS-only（仅语义分割）和RGB+SS（RGB加语义分割）两种输入表示方法，在ViZDoom死亡竞赛环境中进行实验，并应用游程编码等无损压缩技术。

Result: SS-only方法将内存缓冲区消耗减少66.6%至98.6%；RGB+SS方法通过额外语义信息显著提升强化学习智能体性能；密度热图成功可视化智能体移动模式。

Conclusion: 基于语义分割的输入表示方法有效解决了3D环境中强化学习的内存消耗和性能问题，克服了在ViZDoom等环境中应用语义分割的常见缺陷。

Abstract: Reinforcement learning (RL) in 3D environments with high-dimensional sensory input poses two major challenges: (1) the high memory consumption induced by memory buffers required to stabilise learning, and (2) the complexity of learning in partially observable Markov Decision Processes (POMDPs). This project addresses these challenges by proposing two novel input representations: SS-only and RGB+SS, both employing semantic segmentation on RGB colour images. Experiments were conducted in deathmatches of ViZDoom, utilizing perfect segmentation results for controlled evaluation. Our results showed that SS-only was able to reduce the memory consumption of memory buffers by at least 66.6%, and up to 98.6% when a vectorisable lossless compression technique with minimal overhead such as run-length encoding is applied. Meanwhile, RGB+SS significantly enhances RL agents' performance with the additional semantic information provided. Furthermore, we explored density-based heatmapping as a tool to visualise RL agents' movement patterns and evaluate their suitability for data collection. A brief comparison with a previous approach highlights how our method overcame common pitfalls in applying semantic segmentation in 3D environments like ViZDoom.

</details>


### [68] [Simple Vision-Language Math Reasoning via Rendered Text](https://arxiv.org/abs/2511.11704)
*Matvey Skripkin,Elizaveta Goncharova,Andrey Kuznetsov*

Main category: cs.LG

TL;DR: 提出了一种轻量级但有效的训练流程，通过将LaTeX公式渲染为图像并配以结构化思维链提示，来训练视觉语言模型解决数学问题。


<details>
  <summary>Details</summary>
Motivation: 现有的数学问题求解方法往往复杂且计算量大，需要一种简单有效的方法来提升视觉语言模型在数学推理方面的能力，同时保持广泛的通用领域能力。

Method: 将LaTeX编码的数学公式渲染成图像，并与结构化的思维链提示配对，使用文本到视觉的增强方法来训练紧凑的多模态架构。

Result: 该方法在广泛使用的基准测试中持续匹配或超越了开源和专有的数学专用视觉语言求解器，同时在MMMU、ChartQA和DocVQA等任务上获得了高达20%的性能提升。

Conclusion: 渲染保真度和提示设计是性能的主要驱动因素，这种简单的方法不仅提升了数学推理准确性，还保持了模型的通用领域能力。

Abstract: We present a lightweight yet effective pipeline for training vision-language models to solve math problems by rendering LaTeX encoded equations into images and pairing them with structured chain-of-thought prompts. This simple text-to-vision augmentation enables compact multimodal architectures to achieve state-of-the-art reasoning accuracy. Through systematic ablations, we find that rendering fidelity and prompt design are the primary drivers of performance. Despite its simplicity, our approach consistently matches or surpasses both open-source and proprietary math-focused vision-language solvers on widely used benchmarks, while preserving broad general-domain competence - showing gains on tasks such as MMMU, ChartQA, and DocVQA of up to 20%.

</details>


### [69] [Multimodal ML: Quantifying the Improvement of Calorie Estimation Through Image-Text Pairs](https://arxiv.org/abs/2511.11705)
*Arya Narang*

Main category: cs.LG

TL;DR: 本文研究了在菜品名称的短文本输入辅助下，相比仅使用图像的基线模型，卡路里估算的改善程度及其统计显著性。


<details>
  <summary>Details</summary>
Motivation: 探索短文本信息（如菜品名称）是否能够显著提升卡路里估算的准确性，超越仅依赖图像信息的模型。

Method: 使用TensorFlow库和Google的Nutrition5k数据集，训练了仅使用图像的CNN模型和同时接受文本与图像输入的多模态CNN模型。

Result: 多模态模型将卡路里估算的平均绝对误差（MAE）从84.76千卡降低到83.70千卡，减少了1.06千卡（1.25%的改善）。

Conclusion: 短文本输入能够在一定程度上改善卡路里估算的准确性，但改善幅度相对有限。

Abstract: This paper determines the extent to which short textual inputs (in this case, names of dishes) can improve calorie estimation compared to an image-only baseline model and whether any improvements are statistically significant. Utilizes the TensorFlow library and the Nutrition5k dataset (curated by Google) to train both an image-only CNN and multimodal CNN that accepts both text and an image as input. The MAE of calorie estimations was reduced by 1.06 kcal from 84.76 kcal to 83.70 kcal (1.25% improvement) when using the multimodal model.

</details>


### [70] [Context-Aware Multimodal Representation Learning for Spatio-Temporally Explicit Environmental modelling](https://arxiv.org/abs/2511.11706)
*Julia Peters,Karin Mora,Miguel D. Mahecha,Chaonan Ji,David Montero,Clemens Mosig,Guido Kraemer*

Main category: cs.LG

TL;DR: 提出了一个统一的地球观测表示学习框架，能够在高时空分辨率下整合不同遥感模态数据，解决现有模型固定尺度限制的问题。


<details>
  <summary>Details</summary>
Motivation: 现有地球观测基础模型通常在固定空间或时间尺度上运行，限制了需要精细空间细节和高时间保真度的生态分析应用。

Method: 采用两阶段设计：首先独立建模每个传感器以捕获传感器特定特征，然后将表示组合到共享模型中。使用Sentinel-1和Sentinel-2数据作为代表性模态，在原生10米分辨率和无云Sentinel-2采集频率下生成潜在空间。

Result: 定性分析显示学习到的嵌入在异质景观中表现出高空间和语义一致性。定量评估表明这些嵌入编码了生态学上有意义的模式，并保留了足够的时间保真度以支持精细尺度分析。

Conclusion: 该框架为需要不同空间和时间分辨率的环境应用提供了一个灵活、分析就绪的表示学习方法。

Abstract: Earth observation (EO) foundation models have emerged as an effective approach to derive latent representations of the Earth system from various remote sensing sensors. These models produce embeddings that can be used as analysis-ready datasets, enabling the modelling of ecosystem dynamics without extensive sensor-specific preprocessing. However, existing models typically operate at fixed spatial or temporal scales, limiting their use for ecological analyses that require both fine spatial detail and high temporal fidelity. To overcome these limitations, we propose a representation learning framework that integrates different EO modalities into a unified feature space at high spatio-temporal resolution. We introduce the framework using Sentinel-1 and Sentinel-2 data as representative modalities. Our approach produces a latent space at native 10 m resolution and the temporal frequency of cloud-free Sentinel-2 acquisitions. Each sensor is first modeled independently to capture its sensor-specific characteristics. Their representations are then combined into a shared model. This two-stage design enables modality-specific optimisation and easy extension to new sensors, retaining pretrained encoders while retraining only fusion layers. This enables the model to capture complementary remote sensing data and to preserve coherence across space and time. Qualitative analyses reveal that the learned embeddings exhibit high spatial and semantic consistency across heterogeneous landscapes. Quantitative evaluation in modelling Gross Primary Production reveals that they encode ecologically meaningful patterns and retain sufficient temporal fidelity to support fine-scale analyses. Overall, the proposed framework provides a flexible, analysis-ready representation learning approach for environmental applications requiring diverse spatial and temporal resolutions.

</details>


### [71] [FSC-Net: Fast-Slow Consolidation Networks for Continual Learning](https://arxiv.org/abs/2511.11707)
*Mohamed El Gorrim*

Main category: cs.LG

TL;DR: FSC-Net提出双网络架构解决持续学习中的灾难性遗忘问题，通过快速网络学习新任务，慢速网络进行知识巩固，在Split-MNIST上达到91.71%的保留准确率。


<details>
  <summary>Details</summary>
Motivation: 受神经科学中记忆巩固机制启发，解决神经网络在持续学习中灾难性遗忘的问题，即学习新任务时丢失先前知识。

Method: 采用双网络架构：快速网络(NN1)负责快速适应新任务，慢速网络(NN2)通过蒸馏和重放进行知识巩固。发现纯重放策略优于蒸馏方法。

Result: 在Split-MNIST上获得91.71%±0.62%保留准确率，比单网络提升4.27pp；在Split-CIFAR-10上获得33.31%±0.38%保留准确率，提升8.20pp，但绝对性能仍有改进空间。

Conclusion: 双时间尺度巩固机制而非架构复杂性是缓解灾难性遗忘的关键，简单MLP架构在知识巩固中表现优于复杂变体。

Abstract: Continual learning remains challenging due to catastrophic forgetting, where neural networks lose previously acquired knowledge when learning new tasks. Inspired by memory consolidation in neuroscience, we propose FSC-Net (Fast-Slow Consolidation Networks), a dual-network architecture that separates rapid task learning from gradual knowledge consolidation. Our method employs a fast network (NN1) for immediate adaptation to new tasks and a slow network (NN2) that consolidates knowledge through distillation and replay. Within the family of MLP-based NN1 variants we evaluated, consolidation effectiveness is driven more by methodology than architectural embellishments -- a simple MLP outperforms more complex similarity-gated variants by 1.2pp. Through systematic hyperparameter analysis, we observed empirically that pure replay without distillation during consolidation achieves superior performance, consistent with the hypothesis that distillation from the fast network introduces recency bias. On Split-MNIST (30 seeds), FSC-Net achieves 91.71% +/- 0.62% retention accuracy, a +4.27pp gain over the fast network alone (87.43% +/- 1.27%, paired t=23.585, p < 1e-10). On Split-CIFAR-10 (5 seeds), our method achieves 33.31% +/- 0.38% retention with an +8.20pp gain over the fast network alone (25.11% +/- 1.61%, paired t=9.75, p < 1e-3), demonstrating +8.20pp gain, though absolute performance (33.31%) remains modest and below random expectation, highlighting need for stronger backbones. Our results provide empirical evidence that the dual-timescale consolidation mechanism, rather than architectural complexity, is central to mitigating catastrophic forgetting in this setting.

</details>


### [72] [Which Sparse Autoencoder Features Are Real? Model-X Knockoffs for False Discovery Rate Control](https://arxiv.org/abs/2511.11711)
*Tsogt-Ochir Enkhbayar*

Main category: cs.LG

TL;DR: 将Model-X knockoffs方法应用于稀疏自编码器(SAE)特征选择，通过knockoff+控制错误发现率(FDR)，在Pythia-70M模型的情感分类任务中识别出129个相关特征。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器在识别神经网络可解释特征时，难以区分真实计算模式和错误相关性，需要可靠的特征选择方法。

Method: 使用Model-X knockoffs方法，通过高斯替代模型处理潜在分布，结合knockoff+控制FDR，对512个高活性SAE潜在特征进行分析。

Result: 在目标FDR q=0.1下选择出129个特征，其中约25%的潜在特征携带任务相关信号，75%无关，所选特征与非选特征在knockoff统计量上显示5.40倍分离。

Conclusion: 该方法为可靠特征发现提供了可重复且原则性的框架，结合SAE与多重测试感知推理，推进了机制可解释性的基础。

Abstract: Although sparse autoencoders (SAEs) are crucial for identifying interpretable features in neural networks, it is still challenging to distinguish between real computational patterns and erroneous correlations. We introduce Model-X knockoffs to SAE feature selection, using knock-off+ to control the false discovery rate (FDR) with finite-sample guarantees under the standard Model-X assumptions (in our case, via a Gaussian surrogate for the latent distribution). We select 129 features at a target FDR q=0.1 after analyzing 512 high-activity SAE latents for sentiment classification using Pythia-70M. About 25% of the latents under examination carry task-relevant signal, whereas 75% do not, according to the chosen set, which displays a 5.40x separation in knockoff statistics compared to non-selected features. Our method offers a re-producible and principled framework for reliable feature discovery by combining SAEs with multiple-testing-aware inference, advancing the foundations of mechanistic interpretability.

</details>


### [73] [Reasoning: From Reflection to Solution](https://arxiv.org/abs/2511.11712)
*Zixi Li*

Main category: cs.LG

TL;DR: 论文提出推理是状态空间中迭代算子应用并收敛到固定点的过程，通过OpenLM架构在OpenXOR问题上达到76%准确率，而现有LLMs为0%。


<details>
  <summary>Details</summary>
Motivation: 在LLMs在GSM8K和HumanEval等基准上达到超人性能的背景下，探讨这些系统是否真正学会了推理，还是仅仅在推理轨迹上进行模式匹配。

Method: 提出推理的数学定义，开发OpenOperator理论和OpenLM架构，在OpenXOR问题上进行验证。

Result: OpenLM在OpenXOR问题上达到76%准确率，而最先进的LLMs准确率为0%。

Conclusion: 推理需要特定的架构支持，当前系统缺乏真正的推理能力，论文提出的方法为构建具备真正推理能力的系统指明了方向。

Abstract: What is reasoning? This question has driven centuries of philosophical inquiry, from Aristotle's syllogisms to modern computational complexity theory. In the age of large language models achieving superhuman performance on benchmarks like GSM8K (95\% accuracy) and HumanEval (90\% pass@1), we must ask: have these systems learned to \emph{reason}, or have they learned to \emph{pattern-match over reasoning traces}?
  This paper argues for a specific answer: \textbf{reasoning is iterative operator application in state spaces, converging to fixed points}. This definition is not merely philosophical -- it has concrete architectural implications that explain both the failures of current systems and the path to genuine reasoning capabilities.
  Our investigation begins with a puzzle (OpenXOR), progresses through theory (OpenOperator), and culminates in a working solution (OpenLM) that achieves 76\% accuracy where state-of-the-art LLMs achieve 0\%. This is not about criticizing existing systems, but about \emph{understanding what reasoning requires} and \emph{building architectures that provide it}.

</details>


### [74] [Federated Learning for Pediatric Pneumonia Detection: Enabling Collaborative Diagnosis Without Sharing Patient Data](https://arxiv.org/abs/2511.11714)
*Daniel M. Jimenez-Gutierrez,Enrique Zuazua,Joaquin Del Rio,Oleksii Sliusarenko,Xabi Uribe-Etxebarria*

Main category: cs.LG

TL;DR: 本文评估了使用联邦学习在多个医院间协作训练肺炎检测模型，保持数据本地化和隐私保护，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 早期准确检测肺炎对临床至关重要，但全球分布的数据、医院间差异和隐私法规使得数据集中化不可行，需要隐私保护的协作学习方案。

Method: 使用Sherpa.ai联邦学习平台，在儿科肺炎胸部X光数据集上模拟跨医院协作训练，处理非独立同分布数据，保持数据本地化。

Result: 联邦学习实现了0.900准确率和0.966 ROC-AUC，相比单医院模型分别提升了47.5%和50.0%，且无需传输患者数据。

Conclusion: 联邦学习能够提供高性能、可泛化、安全私密的肺炎检测方案，特别适用于罕见疾病和低数据领域，实现安全的多机构协作。

Abstract: Early and accurate pneumonia detection from chest X-rays (CXRs) is clinically critical to expedite treatment and isolation, reduce complications, and curb unnecessary antibiotic use. Although artificial intelligence (AI) substantially improves CXR-based detection, development is hindered by globally distributed data, high inter-hospital variability, and strict privacy regulations (e.g., HIPAA, GDPR) that make centralization impractical. These constraints are compounded by heterogeneous imaging protocols, uneven data availability, and the costs of transferring large medical images across geographically dispersed sites.
  In this paper, we evaluate Federated Learning (FL) using the Sherpa.ai FL platform, enabling multiple hospitals (nodes) to collaboratively train a CXR classifier for pneumonia while keeping data in place and private. Using the Pediatric Pneumonia Chest X-ray dataset, we simulate cross-hospital collaboration with non-independent and non-identically distributed (non-IID) data, reproducing real-world variability across institutions and jurisdictions. Our experiments demonstrate that collaborative and privacy-preserving training across multiple hospitals via FL led to a dramatic performance improvement achieving 0.900 Accuracy and 0.966 ROC-AUC, corresponding to 47.5% and 50.0% gains over single-hospital models (0.610; 0.644), without transferring any patient CXR. These results indicate that FL delivers high-performing, generalizable, secure and private pneumonia detection across healthcare networks, with data kept local. This is especially relevant for rare diseases, where FL enables secure multi-institutional collaboration without data movement, representing a breakthrough for accelerating diagnosis and treatment development in low-data domains.

</details>


### [75] [Multiscale Grassmann Manifolds for Single-Cell Data Analysis](https://arxiv.org/abs/2511.11717)
*Xiang Xiang Wang,Sean Cottrell,Guo-Wei Wei*

Main category: cs.LG

TL;DR: 提出基于Grassmann流形的多尺度框架，用于单细胞数据分析，通过整合不同几何视图的特征来改进细胞异质性表征。


<details>
  <summary>Details</summary>
Motivation: 传统方法将细胞表示为欧几里得空间中的向量，限制了捕捉内在相关性和多尺度几何结构的能力。

Method: 基于Grassmann流形的多尺度框架，生成多个表示尺度的嵌入，并将它们从不同几何视图的特征整合到统一的Grassmann流形中，引入基于幂的尺度采样函数来控制尺度选择。

Result: 在9个基准单细胞RNA-seq数据集上的实验表明，该方法能有效保留有意义的结构并提供稳定的聚类性能，特别适用于中小型数据集。

Conclusion: Grassmann流形为单细胞数据分析提供了连贯且信息丰富的基础。

Abstract: Single-cell data analysis seeks to characterize cellular heterogeneity based on high-dimensional gene expression profiles. Conventional approaches represent each cell as a vector in Euclidean space, which limits their ability to capture intrinsic correlations and multiscale geometric structures. We propose a multiscale framework based on Grassmann manifolds that integrates machine learning with subspace geometry for single-cell data analysis. By generating embeddings under multiple representation scales, the framework combines their features from different geometric views into a unified Grassmann manifold. A power-based scale sampling function is introduced to control the selection of scales and balance in- formation across resolutions. Experiments on nine benchmark single-cell RNA-seq datasets demonstrate that the proposed approach effectively preserves meaningful structures and provides stable clustering performance, particularly for small to medium-sized datasets. These results suggest that Grassmann manifolds offer a coherent and informative foundation for analyzing single cell data.

</details>


### [76] [Fast 3D Surrogate Modeling for Data Center Thermal Management](https://arxiv.org/abs/2511.11722)
*Soumyendu Sarkar,Antonio Guillen-Perez,Zachariah J Carmichael,Avisek Naug,Refik Mert Cam,Vineet Gundecha,Ashwin Ramesh Babu,Sahand Ghorbanpour,Ricardo Luna Gutierrez*

Main category: cs.LG

TL;DR: 开发基于视觉的替代建模框架，直接在3D体素化数据中心上运行，结合服务器负载、风扇速度和HVAC温度设定点，实现实时温度预测，达到20,000倍加速和7%的节能效果。


<details>
  <summary>Details</summary>
Motivation: 减少数据中心能耗和碳排放，传统热CFD求解器计算成本高且需要专家构建网格和边界条件，不适合实时应用。

Method: 评估多种架构，包括3D CNN U-Net变体、3D傅里叶神经算子和3D视觉变换器，将热输入映射到高保真热图。

Result: 替代模型在数据中心配置间具有泛化能力，实现高达20,000倍加速（数百毫秒vs数小时），准确估计热点和温度分布。

Conclusion: 快速准确的温度估计支持实时冷却控制和负载重新分配，带来显著的节能（7%）和碳足迹减少。

Abstract: Reducing energy consumption and carbon emissions in data centers by enabling real-time temperature prediction is critical for sustainability and operational efficiency. Achieving this requires accurate modeling of the 3D temperature field to capture airflow dynamics and thermal interactions under varying operating conditions. Traditional thermal CFD solvers, while accurate, are computationally expensive and require expert-crafted meshes and boundary conditions, making them impractical for real-time use. To address these limitations, we develop a vision-based surrogate modeling framework that operates directly on a 3D voxelized representation of the data center, incorporating server workloads, fan speeds, and HVAC temperature set points. We evaluate multiple architectures, including 3D CNN U-Net variants, a 3D Fourier Neural Operator, and 3D vision transformers, to map these thermal inputs to high-fidelity heat maps. Our results show that the surrogate models generalize across data center configurations and achieve up to 20,000x speedup (hundreds of milliseconds vs. hours). This fast and accurate estimation of hot spots and temperature distribution enables real-time cooling control and workload redistribution, leading to substantial energy savings (7\%) and reduced carbon footprint.

</details>


### [77] [Optimizing Input of Denoising Score Matching is Biased Towards Higher Score Norm](https://arxiv.org/abs/2511.11727)
*Tongda Xu*

Main category: cs.LG

TL;DR: 本文揭示了在扩散模型中使用去噪分数匹配优化条件输入时，会破坏去噪分数匹配与精确分数匹配的等价性，导致偏差和更高的分数范数。


<details>
  <summary>Details</summary>
Motivation: 许多近期工作使用去噪分数匹配来优化扩散模型的条件输入，但作者发现这种优化会引入偏差。

Method: 通过理论分析和实验验证，展示去噪分数匹配优化如何破坏与精确分数匹配的等价性，并观察数据分布优化时的类似偏差。

Result: 这种偏差导致更高的分数范数，影响多个领域的工作，包括自回归生成的MAR、图像压缩的PerCo和文本到3D生成的DreamFusion。

Conclusion: 去噪分数匹配优化存在系统性偏差，影响广泛的应用领域，需要引起重视。

Abstract: Many recent works utilize denoising score matching to optimize the conditional input of diffusion models. In this workshop paper, we demonstrate that such optimization breaks the equivalence between denoising score matching and exact score matching. Furthermore, we show that this bias leads to higher score norm. Additionally, we observe a similar bias when optimizing the data distribution using a pre-trained diffusion model. Finally, we discuss the wide range of works across different domains that are affected by this bias, including MAR for auto-regressive generation, PerCo for image compression, and DreamFusion for text to 3D generation.

</details>


### [78] [Physics-Informed Neural ODEs with Scale-Aware Residuals for Learning Stiff Biophysical Dynamics](https://arxiv.org/abs/2511.11734)
*Kamalpreet Singh Kainth,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedat Panat*

Main category: cs.LG

TL;DR: PI-NODE-SR结合低阶显式求解器和残差归一化，有效学习刚性生物物理系统的连续时间动力学，在有限迭代次数下实现稳定训练，并能准确预测振荡频率和幅度。


<details>
  <summary>Details</summary>
Motivation: 标准神经微分方程和物理信息变体在建模刚性生物物理系统时不可靠，需要大量迭代且可能收敛到次优解，无法保持振荡频率或幅度。

Method: 引入PI-NODE-SR框架，结合低阶显式求解器（Heun方法）和残差归一化，平衡不同时间尺度状态变量的贡献，避免依赖计算昂贵的隐式求解器。

Result: 在Hodgkin-Huxley方程上，PI-NODE-SR从单个振荡学习并外推超过100毫秒，准确捕捉振荡频率和接近正确的幅度，还能恢复通常需要高阶求解器才能获得的形态特征。

Conclusion: PI-NODE-SR相对于基线神经ODE和PINNs持续减少长期误差，为稳定高效学习刚性生物动力学提供了原则性途径。

Abstract: Neural differential equations offer a powerful framework for modeling continuous-time dynamics, but forecasting stiff biophysical systems remains unreliable. Standard Neural ODEs and physics informed variants often require orders of magnitude more iterations, and even then may converge to suboptimal solutions that fail to preserve oscillatory frequency or amplitude. We introduce PhysicsInformed Neural ODEs with with Scale-Aware Residuals (PI-NODE-SR), a framework that combines a low-order explicit solver (Heun method) residual normalisation to balance contributions between state variables evolving on disparate timescales. This combination stabilises training under realistic iteration budgets and avoids reliance on computationally expensive implicit solvers. On the Hodgkin-Huxley equations, PI-NODE-SR learns from a single oscillation simulated with a stiff solver (Rodas5P) and extrapolates beyond 100 ms, capturing both oscillation frequency and near-correct amplitudes. Remarkably, end-to-end learning of the vector field enables PI-NODE-SR to recover morphological features such as sharp subthreshold curvature in gating variables that are typically reserved for higher-order solvers, suggesting that neural correction can offset numerical diffusion. While performance remains sensitive to initialisation, PI-NODE-SR consistently reduces long-horizon errors relative to baseline Neural-ODEs and PINNs, offering a principled route to stable and efficient learning of stiff biological dynamics.

</details>


### [79] [KAN/H: Kolmogorov-Arnold Network using Haar-like bases](https://arxiv.org/abs/2511.11736)
*Susumu Katayama*

Main category: cs.LG

TL;DR: KAN/H是KAN的变体，使用Haar变体基系统替代B样条，包含全局和局部基函数，应用于函数逼近和MNIST分类，无需大量超参数调优。


<details>
  <summary>Details</summary>
Motivation: 改进KAN网络，使用Haar基系统替代B样条，利用其全局和局部基函数的优势，简化超参数调优过程。

Method: 提出KAN/H变体，采用Haar变体基系统，包含全局和局部基函数，应用于函数逼近问题和MNIST数据集分类。

Result: KAN/H在函数逼近和MNIST分类任务中表现良好，且不需要大多数问题特定的超参数调优。

Conclusion: KAN/H通过使用Haar基系统有效简化了超参数调优，在多个任务中取得良好性能。

Abstract: This paper proposes KAN/H, a variant of Kolmogorov-Arnold Network (KAN) that uses a Haar-variant basis system having both global and local bases instead of B-spline. The resulting algorithm is applied to function approximation problems and MNIST. We show that it does not require most of the problem-specific hyper-parameter tunings.

</details>


### [80] [DK-Root: A Joint Data-and-Knowledge-Driven Framework for Root Cause Analysis of QoE Degradations in Mobile Networks](https://arxiv.org/abs/2511.11737)
*Qizhe Li,Haolong Chen,Jiansheng Li,Shuqi Chai,Xuan Li,Yuzhou Hou,Xinhua Shao,Fangfang Li,Kaifeng Han,Guangxu Zhu*

Main category: cs.LG

TL;DR: DK-Root是一个联合数据和知识驱动的框架，通过结合可扩展的弱监督和精确的专家指导，解决移动网络中QoE退化根因分析的挑战。


<details>
  <summary>Details</summary>
Motivation: 移动网络中QoE退化根因诊断面临挑战，因为复杂的跨层KPI交互和缺乏可靠的专家标注。基于规则的启发式方法虽然能大规模生成标签，但噪声大且粒度粗，限制了纯数据驱动方法的准确性。

Method: 1) 使用对比表示学习预训练编码器，通过监督对比目标对基于规则的标签进行去噪；2) 引入类条件扩散模型生成保留根因语义的KPI序列，通过控制反向扩散步骤产生强弱增强；3) 编码器和轻量级分类器使用稀缺的专家验证标签进行联合微调。

Result: 在真实运营商级数据集上的广泛实验表明，DK-Root达到了最先进的准确率，超越了传统机器学习和最近的半监督时间序列方法。消融研究证实了条件扩散增强和预训练-微调设计的必要性。

Conclusion: DK-Root框架通过统一可扩展的弱监督和精确的专家指导，实现了鲁棒的根因分析，在表示质量和分类性能方面都取得了显著提升。

Abstract: Diagnosing the root causes of Quality of Experience (QoE) degradations in operational mobile networks is challenging due to complex cross-layer interactions among kernel performance indicators (KPIs) and the scarcity of reliable expert annotations. Although rule-based heuristics can generate labels at scale, they are noisy and coarse-grained, limiting the accuracy of purely data-driven approaches. To address this, we propose DK-Root, a joint data-and-knowledge-driven framework that unifies scalable weak supervision with precise expert guidance for robust root-cause analysis. DK-Root first pretrains an encoder via contrastive representation learning using abundant rule-based labels while explicitly denoising their noise through a supervised contrastive objective. To supply task-faithful data augmentation, we introduce a class-conditional diffusion model that generates KPIs sequences preserving root-cause semantics, and by controlling reverse diffusion steps, it produces weak and strong augmentations that improve intra-class compactness and inter-class separability. Finally, the encoder and the lightweight classifier are jointly fine-tuned with scarce expert-verified labels to sharpen decision boundaries. Extensive experiments on a real-world, operator-grade dataset demonstrate state-of-the-art accuracy, with DK-Root surpassing traditional ML and recent semi-supervised time-series methods. Ablations confirm the necessity of the conditional diffusion augmentation and the pretrain-finetune design, validating both representation quality and classification gains.

</details>


### [81] [Uncertainty Makes It Stable: Curiosity-Driven Quantized Mixture-of-Experts](https://arxiv.org/abs/2511.11743)
*Sebastián Andrés Cajas Ordóñez,Luis Fernando Torres Torres,Mackenzie J. Meni,Carlos Andrés Duran Paredes,Eric Arazo,Cristian Bosch,Ricardo Simon Carbajo,Yuan Lai,Leo Anthony Celi*

Main category: cs.LG

TL;DR: 提出了一种好奇心驱动的量化混合专家框架，通过贝叶斯认知不确定性路由在异构专家之间进行智能分配，在保持99.9%精度的同时实现4倍压缩和41%能耗节省，并将延迟方差降低82%。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限设备上部署深度神经网络的两个关键挑战：在激进量化下保持精度，同时确保可预测的推理延迟。

Method: 使用好奇心驱动的量化混合专家框架，基于贝叶斯认知不确定性在异构专家（BitNet三元、1-16位BitLinear、训练后量化）之间进行路由。

Result: 4位量化保持99.9%的16位精度（0.858 vs 0.859 F1），4倍压缩，41%能耗节省；好奇心驱动路由将MoE延迟方差降低82%（从230ms到29ms标准差）。

Conclusion: 自适应量化能够产生准确、节能且可预测的边缘模型，简单的4位量化架构在大多数部署场景中优于复杂的MoE架构。

Abstract: Deploying deep neural networks on resource-constrained devices faces two critical challenges: maintaining accuracy under aggressive quantization while ensuring predictable inference latency. We present a curiosity-driven quantized Mixture-of-Experts framework that addresses both through Bayesian epistemic uncertainty-based routing across heterogeneous experts (BitNet ternary, 1-16 bit BitLinear, post-training quantization). Evaluated on audio classification benchmarks (ESC-50, Quinn, UrbanSound8K), our 4-bit quantization maintains 99.9 percent of 16-bit accuracy (0.858 vs 0.859 F1) with 4x compression and 41 percent energy savings versus 8-bit. Crucially, curiosity-driven routing reduces MoE latency variance by 82 percent (p = 0.008, Levene's test) from 230 ms to 29 ms standard deviation, enabling stable inference for battery-constrained devices. Statistical analysis confirms 4-bit/8-bit achieve practical equivalence with full precision (p > 0.05), while MoE architectures introduce 11 percent latency overhead (p < 0.001) without accuracy gains. At scale, deployment emissions dominate training by 10000x for models serving more than 1,000 inferences, making inference efficiency critical. Our information-theoretic routing demonstrates that adaptive quantization yields accurate (0.858 F1, 1.2M params), energy-efficient (3.87 F1/mJ), and predictable edge models, with simple 4-bit quantized architectures outperforming complex MoE for most deployments.

</details>


### [82] [Diffusion Models: A Mathematical Introduction](https://arxiv.org/abs/2511.11746)
*Sepehr Maleki,Negar Pourmoazemi*

Main category: cs.LG

TL;DR: 本文提供了一个从第一性原理出发的扩散生成模型自包含推导，涵盖前向加噪过程、反向后验分布、变分下界等核心理论，并讨论了加速采样、连续时间公式和引导扩散等扩展方法。


<details>
  <summary>Details</summary>
Motivation: 为扩散生成模型提供一个简洁、自包含的理论推导，通过透明代数和明确中间步骤，让读者既能理解理论又能实现相应算法。

Method: 从高斯分布的基本性质出发，构建去噪扩散概率模型，包括前向加噪过程、闭式边缘分布、精确离散反向后验分布和相关变分下界。

Result: 推导出标准噪声预测目标，并扩展到似然估计、加速采样、连续时间公式和引导扩散等扩展方法。

Conclusion: 通过系统性的理论推导，为扩散生成模型提供了完整的数学框架，使读者能够深入理解模型原理并实现相关算法。

Abstract: We present a concise, self-contained derivation of diffusion-based generative models. Starting from basic properties of Gaussian distributions (densities, quadratic expectations, re-parameterisation, products, and KL divergences), we construct denoising diffusion probabilistic models from first principles. This includes the forward noising process, its closed-form marginals, the exact discrete reverse posterior, and the related variational bound. This bound simplifies to the standard noise-prediction goal used in practice. We then discuss likelihood estimation and accelerated sampling, covering DDIM, adversarially learned reverse dynamics (DDGAN), and multi-scale variants such as nested and latent diffusion, with Stable Diffusion as a canonical example. A continuous-time formulation follows, in which we derive the probability-flow ODE from the diffusion SDE via the continuity and Fokker-Planck equations, introduce flow matching, and show how rectified flows recover DDIM up to a time re-parameterisation. Finally, we treat guided diffusion, interpreting classifier guidance as a posterior score correction and classifier-free guidance as a principled interpolation between conditional and unconditional scores. Throughout, the focus is on transparent algebra, explicit intermediate steps, and consistent notation, so that readers can both follow the theory and implement the corresponding algorithms in practice.

</details>


### [83] [IDOL: Meeting Diverse Distribution Shifts with Prior Physics for Tropical Cyclone Multi-Task Estimation](https://arxiv.org/abs/2511.11750)
*Hanting Yan,Pan Mu,Shiqi Zhang,Yuchao Zhu,Jinglin Zhang,Cong Bai*

Main category: cs.LG

TL;DR: 提出了IDOL框架，通过基于先验物理知识的身份导向约束来处理热带气旋估计中的分布偏移问题，实现稳健的风速、气压等属性估计。


<details>
  <summary>Details</summary>
Motivation: 热带气旋估计面临复杂动态环境场导致的分布偏移挑战，现有方法忽视特征表示的内在分布，在分布外场景下泛化能力差。

Method: IDOL框架利用风场模型和暗相关知识建模任务共享和任务特定的身份令牌，通过身份导向约束在物理不变性指导下调控特征空间。

Result: 在多个数据集和任务上的实验表明IDOL表现出色，验证了基于先验物理知识的身份导向约束能有效缓解分布偏移。

Conclusion: 基于物理知识的身份导向约束是处理热带气旋估计中分布偏移问题的有效方法。

Abstract: Tropical Cyclone (TC) estimation aims to accurately estimate various TC attributes in real time. However, distribution shifts arising from the complex and dynamic nature of TC environmental fields, such as varying geographical conditions and seasonal changes, present significant challenges to reliable estimation. Most existing methods rely on multi-modal fusion for feature extraction but overlook the intrinsic distribution of feature representations, leading to poor generalization under out-of-distribution (OOD) scenarios. To address this, we propose an effective Identity Distribution-Oriented Physical Invariant Learning framework (IDOL), which imposes identity-oriented constraints to regulate the feature space under the guidance of prior physical knowledge, thereby dealing distribution variability with physical invariance. Specifically, the proposed IDOL employs the wind field model and dark correlation knowledge of TC to model task-shared and task-specific identity tokens. These tokens capture task dependencies and intrinsic physical invariances of TC, enabling robust estimation of TC wind speed, pressure, inner-core, and outer-core size under distribution shifts. Extensive experiments conducted on multiple datasets and tasks demonstrate the outperformance of the proposed IDOL, verifying that imposing identity-oriented constraints based on prior physical knowledge can effectively mitigates diverse distribution shifts in TC estimation.Code is available at https://github.com/Zjut-MultimediaPlus/IDOL.

</details>


### [84] [Improving a Hybrid Graphsage Deep Network for Automatic Multi-objective Logistics Management in Supply Chain](https://arxiv.org/abs/2511.11753)
*Mehdi Khaleghi,Nastaran Khaleghi,Sobhan Sheykhivand,Sebelan Danishvar*

Main category: cs.LG

TL;DR: 本文提出了一种混合图神经网络方法（H-GSN），用于供应链物流管理中的多任务预测，包括货运类型、物流延迟和交通状态等，在多个数据集上取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 提高供应链的弹性和可持续性，通过自动预测货运类型、物流延迟和交通状态来优化供应链管理效率，减少空气污染物排放。

Method: 提出混合图神经网络（H-GSN）进行多任务物流管理预测，包括货运类型、货运状态、交通状态、物流ID和物流延迟等目标。

Result: 在Smart Logistics数据集上，物流ID预测准确率97.8%，交通状态预测准确率100%；在DataCo数据集上货运类型预测准确率98.7%；在Shipping数据集上物流延迟预测准确率99.4%。

Conclusion: 所提出的方法在多个物流场景下的评估指标证实了其有效性，能够提高供应链的弹性和可持续性。

Abstract: Systematic logistics, conveyance amenities and facilities as well as warehousing information play a key role in fostering profitable development in a supply chain. The aim of transformation in industries is the improvement of the resiliency regarding the supply chain. The resiliency policies are required for companies to affect the collaboration with logistics service providers positively. The decrement of air pollutant emissions is a persistent advantage of the efficient management of logistics and transportation in supply chain. The management of shipment type is a significant factor in analyzing the sustainability of logistics and supply chain. An automatic approach to predict the shipment type, logistics delay and traffic status are required to improve the efficiency of the supply chain management. A hybrid graphsage network (H-GSN) is proposed in this paper for multi-task purpose of logistics management in a supply chain. The shipment type, shipment status, traffic status, logistics ID and logistics delay are the objectives in this article regarding three different databases including DataCo, Shipping and Smart Logistcis available on Kaggle as supply chain logistics databases. The average accuracy of 97.8% and 100% are acquired for 10 kinds of logistics ID and 3 types of traffic status prediction in Smart Logistics dataset. The average accuracy of 98.7% and 99.4% are obtained for shipment type prediction in DataCo and logistics delay in Shipping database, respectively. The evaluation metrics for different logistics scenarios confirm the efficiency of the proposed method to improve the resilience and sustainability of the supply chain.

</details>


### [85] [Quantum Machine Learning via Contrastive Training](https://arxiv.org/abs/2511.13497)
*Liudmila A. Zhukas,Vivian Ni Zhang,Qiang Miao,Qingfeng Wang,Marko Cetina,Jungsang Kim,Lawrence Carin,Christopher Monroe*

Main category: cs.LG

TL;DR: 该论文提出了一种量子自监督预训练方法，通过在可编程离子阱量子计算机上学习未标记数据的变换不变性，减少对标记数据的依赖，提高图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习面临标记数据稀缺的挑战，特别是在模型规模和复杂性增加时。需要开发能够减少对标记数据依赖的量子表示学习方法。

Method: 在可编程离子阱量子计算机上实现自监督预训练，将图像编码为量子态，通过对比学习在硬件上学习表示的不变性，然后进行微调分类。

Result: 相比随机初始化模型，预训练模型在图像分类任务中获得了更高的平均测试准确率和更低的运行间变异性，在标记数据有限时性能提升尤其显著。

Conclusion: 这项工作建立了标签高效的量子表示学习路径，对量子原生数据集具有直接相关性，并为处理更大规模经典输入提供了清晰路径。

Abstract: Quantum machine learning (QML) has attracted growing interest with the rapid parallel advances in large-scale classical machine learning and quantum technologies. Similar to classical machine learning, QML models also face challenges arising from the scarcity of labeled data, particularly as their scale and complexity increase. Here, we introduce self-supervised pretraining of quantum representations that reduces reliance on labeled data by learning invariances from unlabeled examples. We implement this paradigm on a programmable trapped-ion quantum computer, encoding images as quantum states. In situ contrastive pretraining on hardware yields a representation that, when fine-tuned, classifies image families with higher mean test accuracy and lower run-to-run variability than models trained from random initialization. Performance improvement is especially significant in regimes with limited labeled training data. We show that the learned invariances generalize beyond the pretraining image samples. Unlike prior work, our pipeline derives similarity from measured quantum overlaps and executes all training and classification stages on hardware. These results establish a label-efficient route to quantum representation learning, with direct relevance to quantum-native datasets and a clear path to larger classical inputs.

</details>


### [86] [Sumudu Neural Operator for ODEs and PDEs](https://arxiv.org/abs/2511.11762)
*Ben Zelenskiy,Saibilila Abudukelimu,George Flint,Kevin Zhu,Sunishchal Dev*

Main category: cs.LG

TL;DR: SNO是一种基于Sumudu变换的神经算子，通过将输入空间分解为系数并转换到Sumudu空间来参数化神经算子。在ODE和PDE任务中表现出色，特别是在PDE上优于FNO，在多个PDE任务上与LNO竞争，并在Euler-Bernoulli梁和扩散方程上获得最低误差。


<details>
  <summary>Details</summary>
Motivation: 探索Sumudu变换作为神经算子设计的潜力，特别是针对某些类型的偏微分方程。

Method: 利用变换对的多项式展开关系分解输入空间为系数，然后转换到Sumudu空间，在该空间中参数化神经算子。

Result: 在ODE（Duffing振荡器、Lorenz系统、驱动摆）和PDE（Euler-Bernoulli梁、Burger方程、扩散、扩散-反应、Brusselator）任务中评估，SNO在PDE上表现优于FNO，在多个PDE任务上与LNO竞争精度，在Euler-Bernoulli梁和扩散方程上获得最低误差。零样本超分辨率实验显示能从低质量样本获得更高质量数据。

Conclusion: 初步结果表明Sumudu变换作为神经算子设计具有前景，特别是对于某些类别的偏微分方程。

Abstract: We introduce the Sumudu Neural Operator (SNO), a neural operator rooted in the properties of the Sumudu Transform. We leverage the relationship between the polynomial expansions of transform pairs to decompose the input space as coefficients, which are then transformed into the Sumudu Space, where the neural operator is parameterized. We evaluate the operator in ODEs (Duffing Oscillator, Lorenz System, and Driven Pendulum) and PDEs (Euler-Bernoulli Beam, Burger's Equation, Diffusion, Diffusion-Reaction, and Brusselator). SNO achieves superior performance to FNO on PDEs and demonstrates competitive accuracy with LNO on several PDE tasks, including the lowest error on the Euler-Bernoulli Beam and Diffusion Equation. Additionally, we apply zero-shot super-resolution to the PDE tasks to observe the model's capability of obtaining higher quality data from low-quality samples. These preliminary findings suggest promise for the Sumudu Transform as a neural operator design, particularly for certain classes of PDEs.

</details>


### [87] [Learning Fair Representations with Kolmogorov-Arnold Networks](https://arxiv.org/abs/2511.11767)
*Amisha Priyadarshini,Sergio Gago-Masague*

Main category: cs.LG

TL;DR: 该论文提出将Kolmogorov-Arnold Networks（KANs）集成到公平对抗学习框架中，通过自适应惩罚更新机制动态调整公平性约束，在保持高预测准确性的同时实现竞争性公平性。


<details>
  <summary>Details</summary>
Motivation: 现有公平学习模型在公平性和准确性之间的权衡仍具挑战，且黑盒模型缺乏可解释性，限制了在敏感领域（如大学录取）的应用。

Method: 将KANs集成到公平对抗学习框架中，利用KANs的对抗鲁棒性和可解释性，并提出自适应惩罚更新机制动态调整训练过程中的公平性约束。

Result: 在两个真实世界大学录取数据集上的实验表明，该方法在三种不同优化策略下均优于基线公平学习模型，在保持高预测准确性的同时实现了竞争性公平性。

Conclusion: 基于KANs的公平对抗学习框架能够有效平衡公平性和准确性，同时提供模型可解释性，适用于社会敏感领域的决策支持。

Abstract: Despite recent advances in fairness-aware machine learning, predictive models often exhibit discriminatory behavior towards marginalized groups. Such unfairness might arise from biased training data, model design, or representational disparities across groups, posing significant challenges in high-stakes decision-making domains such as college admissions. While existing fair learning models aim to mitigate bias, achieving an optimal trade-off between fairness and accuracy remains a challenge. Moreover, the reliance on black-box models hinders interpretability, limiting their applicability in socially sensitive domains. In this paper, we try to circumvent these issues by integrating Kolmogorov-Arnold Networks (KANs) within a fair adversarial learning framework. Leveraging the adversarial robustness and interpretability of KANs, our approach enables a balance between fairness and accuracy. To further facilitate this balance, we propose an adaptive penalty update mechanism that dynamically adjusts fairness constraints during the model training. We conduct numerical experiments on two real-world college admissions datasets, across three different optimization strategies. The results demonstrate the efficiency and robustness of KANs by consistently outperforming the baseline fair learning models, and maintaining high predictive accuracy while achieving competitive fairness across sensitive attributes.

</details>


### [88] [CATCHFed: Efficient Unlabeled Data Utilization for Semi-Supervised Federated Learning in Limited Labels Environments](https://arxiv.org/abs/2511.11778)
*Byoungjun Park,Pedro Porto Buarque de Gusmão,Dongjin Ji,Minhoe Kim*

Main category: cs.LG

TL;DR: CATCHFed是一种半监督联邦学习方法，通过客户端感知的自适应阈值、混合阈值和一致性正则化，在服务器仅有少量标注数据的情况下有效利用客户端未标注数据。


<details>
  <summary>Details</summary>
Motivation: 现实联邦学习场景中客户端往往缺乏标注数据，而现有半监督联邦学习方法在标注数据极少时性能显著下降。

Method: 提出客户端感知的自适应阈值考虑类别难度，使用混合阈值提升伪标签质量，并利用未伪标注数据进行一致性正则化。

Result: 在各种数据集和配置下的广泛实验表明，CATCHFed能有效利用客户端未标注数据，在极有限标注设置下仍能取得优越性能。

Conclusion: CATCHFed解决了半监督联邦学习在标注数据稀缺时的性能问题，为现实场景提供了有效解决方案。

Abstract: Federated learning is a promising paradigm that utilizes distributed client resources while preserving data privacy. Most existing FL approaches assume clients possess labeled data, however, in real-world scenarios, client-side labels are often unavailable. Semi-supervised Federated learning, where only the server holds labeled data, addresses this issue. However, it experiences significant performance degradation as the number of labeled data decreases. To tackle this problem, we propose \textit{CATCHFed}, which introduces client-aware adaptive thresholds considering class difficulty, hybrid thresholds to enhance pseudo-label quality, and utilizes unpseudo-labeled data for consistency regularization. Extensive experiments across various datasets and configurations demonstrate that CATCHFed effectively leverages unlabeled client data, achieving superior performance even in extremely limited-label settings.

</details>


### [89] [Coordinate Descent for Network Linearization](https://arxiv.org/abs/2511.11781)
*Vlad Rakhlin,Amir Jevnisek,Shai Avidan*

Main category: cs.LG

TL;DR: 本文提出了一种基于坐标下降的离散优化方法，用于减少ResNet网络中ReLU激活函数的数量，从而降低私有推理的延迟。


<details>
  <summary>Details</summary>
Motivation: ReLU激活函数是基于ResNet网络的私有推理中的主要瓶颈，因为它们会导致显著的推理延迟。现有的平滑近似方法在最后硬阈值步骤中通常会产生较大的性能损失。

Method: 采用坐标下降作为优化框架，直接在离散域中工作，通过设计产生稀疏解。

Result: 通过大量实验证明，该方法在常见基准测试中达到了最先进的性能。

Conclusion: 提出的离散优化方法在减少ReLU数量的同时保持了网络精度，是私有推理中的有效解决方案。

Abstract: ReLU activations are the main bottleneck in Private Inference that is based on ResNet networks. This is because they incur significant inference latency. Reducing ReLU count is a discrete optimization problem, and there are two common ways to approach it. Most current state-of-the-art methods are based on a smooth approximation that jointly optimizes network accuracy and ReLU budget at once. However, the last hard thresholding step of the optimization usually introduces a large performance loss. We take an alternative approach that works directly in the discrete domain by leveraging Coordinate Descent as our optimization framework. In contrast to previous methods, this yields a sparse solution by design. We demonstrate, through extensive experiments, that our method is State of the Art on common benchmarks.

</details>


### [90] [Simplicial covering dimension of extremal concept classes](https://arxiv.org/abs/2511.11819)
*Ari Blondal,Hamed Hatami,Pooya Hatami,Chavdar Lalov,Sivan Tretiak*

Main category: cs.LG

TL;DR: 该论文将经典拓扑维度理论中的Lebesgue覆盖维度概念扩展到二元概念类，通过定义单纯覆盖维度来刻画PAC学习中的列表可复制性数。


<details>
  <summary>Details</summary>
Motivation: 将拓扑维度理论应用于机器学习中的概念类，建立拓扑维度与学习理论中列表可复制性之间的理论联系。

Method: 将概念类与可实现分布空间关联，通过损失函数和概念类在该空间上诱导单纯结构，定义单纯覆盖维度。

Result: 证明对于有限概念类，单纯覆盖维度精确刻画了PAC学习中的列表可复制性数（等价于全局稳定性）。

Conclusion: 该连接使得能够应用经典维度理论工具计算极值概念类的精确列表可复制性数。

Abstract: Dimension theory is a branch of topology concerned with defining and analyzing dimensions of geometric and topological spaces in purely topological terms. In this work, we adapt the classical notion of topological dimension (Lebesgue covering) to binary concept classes. The topological space naturally associated with a concept class is its space of realizable distributions. The loss function and the class itself induce a simplicial structure on this space, with respect to which we define a simplicial covering dimension.
  We prove that for finite concept classes, this simplicial covering dimension exactly characterizes the list replicability number (equivalently, global stability) in PAC learning. This connection allows us to apply tools from classical dimension theory to compute the exact list replicability number of the broad family of extremal concept classes.

</details>


### [91] [Conformal Constrained Policy Optimization for Cost-Effective LLM Agents](https://arxiv.org/abs/2511.11828)
*Wenwen Si,Sooyong Jang,Insup Lee,Osbert Bastani*

Main category: cs.LG

TL;DR: 提出CCPO方法，通过结合多个成本/精度不同的LLM模型，在保证可靠性的前提下最小化成本，在问答基准测试中实现30%的成本降低。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能强大，但计算和API成本日益昂贵，需要找到在保证可靠性的同时降低成本的解决方案。

Method: 提出Conformal Constrained Policy Optimization (CCPO)训练范式，结合约束策略优化、离策略强化学习和在线符合预测，联合优化成本感知策略和自适应阈值。

Result: 在两个多跳问答基准测试中，CCPO相比其他成本感知基线和LLM引导方法，实现了高达30%的成本降低，且不损害可靠性。

Conclusion: CCPO为部署LLM代理提供了一个原则性和实用的框架，在保持可靠性的同时显著提高成本效益。

Abstract: While large language models (LLMs) have recently made tremendous progress towards solving challenging AI problems, they have done so at increasingly steep computational and API costs. We propose a novel strategy where we combine multiple LLM models with varying cost/accuracy tradeoffs in an agentic manner, where models and tools are run in sequence as determined by an orchestration model to minimize cost subject to a user-specified level of reliability; this constraint is formalized using conformal prediction to provide guarantees. To solve this problem, we propose Conformal Constrained Policy Optimization (CCPO), a training paradigm that integrates constrained policy optimization with off-policy reinforcement learning and recent advances in online conformal prediction. CCPO jointly optimizes a cost-aware policy (score function) and an adaptive threshold. Across two multi-hop question answering benchmarks, CCPO achieves up to a 30% cost reduction compared to other cost-aware baselines and LLM-guided methods without compromising reliability. Our approach provides a principled and practical framework for deploying LLM agents that are significantly more cost-effective while maintaining reliability.

</details>


### [92] [Volatility in Certainty (VC): A Metric for Detecting Adversarial Perturbations During Inference in Neural Network Classifiers](https://arxiv.org/abs/2511.11834)
*Vahid Hemmati,Ahmad Mohammadi,Abdul-Rauf Nuhu,Reza Ahmari,Parham Kebria,Abdollah Homaifar*

Main category: cs.LG

TL;DR: VC是一种无标签度量方法，通过测量排序softmax输出的离散度来量化模型置信度的不规则性，可作为分类准确率和对抗性漂移的代理指标。


<details>
  <summary>Details</summary>
Motivation: 在实时系统中，由于推理时缺乏真实标签，对抗性鲁棒性成为关键挑战，需要开发无需标签的性能度量方法。

Method: 在MNIST和CIFAR-10数据集上训练ANN、CNN和VGG-like模型，使用FGSM方法生成对抗样本，通过逐渐引入对抗性污染创建混合测试集，评估VC对分布偏移的敏感性。

Result: 分类准确率与log(VC)之间存在强负相关性（大多数情况下rho < -0.90），表明VC能有效反映性能下降而无需标签数据。

Conclusion: VC是一种可扩展、架构无关的实时性能度量方法，适用于安全关键应用中的早期预警系统。

Abstract: Adversarial robustness remains a critical challenge in deploying neural network classifiers, particularly in real-time systems where ground-truth labels are unavailable during inference. This paper investigates \textit{Volatility in Certainty} (VC), a recently proposed, label-free metric that quantifies irregularities in model confidence by measuring the dispersion of sorted softmax outputs. Specifically, VC is defined as the average squared log-ratio of adjacent certainty values, capturing local fluctuations in model output smoothness. We evaluate VC as a proxy for classification accuracy and as an indicator of adversarial drift. Experiments are conducted on artificial neural networks (ANNs) and convolutional neural networks (CNNs) trained on MNIST, as well as a regularized VGG-like model trained on CIFAR-10. Adversarial examples are generated using the Fast Gradient Sign Method (FGSM) across varying perturbation magnitudes. In addition, mixed test sets are created by gradually introducing adversarial contamination to assess VC's sensitivity under incremental distribution shifts. Our results reveal a strong negative correlation between classification accuracy and log(VC) (correlation rho < -0.90 in most cases), suggesting that VC effectively reflects performance degradation without requiring labeled data. These findings position VC as a scalable, architecture-agnostic, and real-time performance metric suitable for early-warning systems in safety-critical applications.

</details>


### [93] [On the Trade-Off Between Transparency and Security in Adversarial Machine Learning](https://arxiv.org/abs/2511.11842)
*Lucas Fenaux,Christopher Srinivasa,Florian Kerschbaum*

Main category: cs.LG

TL;DR: 本文通过可转移对抗样本攻击研究AI透明度与安全性的战略冲突，发现攻击者在匹配防御者决策时更成功，表明模糊性可能有利于防御者。


<details>
  <summary>Details</summary>
Motivation: 研究负责任AI中透明度与安全性在对抗环境下的潜在冲突，特别是在可转移对抗样本攻击场景中。

Method: 使用9种攻击方法在181个模型上进行大规模实证评估，并应用博弈论分析，将问题建模为纳什博弈和斯塔克尔伯格博弈。

Result: 攻击者成功匹配防御者决策时攻击效果更好；仅知道防御者模型是否被防御就足以损害其安全性。

Conclusion: AI系统的透明度可能与安全性相冲突，博弈论分析揭示了透明度与安全性之间的基本权衡关系。

Abstract: Transparency and security are both central to Responsible AI, but they may conflict in adversarial settings. We investigate the strategic effect of transparency for agents through the lens of transferable adversarial example attacks. In transferable adversarial example attacks, attackers maliciously perturb their inputs using surrogate models to fool a defender's target model. These models can be defended or undefended, with both players having to decide which to use. Using a large-scale empirical evaluation of nine attacks across 181 models, we find that attackers are more successful when they match the defender's decision; hence, obscurity could be beneficial to the defender. With game theory, we analyze this trade-off between transparency and security by modeling this problem as both a Nash game and a Stackelberg game, and comparing the expected outcomes. Our analysis confirms that only knowing whether a defender's model is defended or not can sometimes be enough to damage its security. This result serves as an indicator of the general trade-off between transparency and security, suggesting that transparency in AI systems can be at odds with security. Beyond adversarial machine learning, our work illustrates how game-theoretic reasoning can uncover conflicts between transparency and security.

</details>


### [94] [Leveraging Exogenous Signals for Hydrology Time Series Forecasting](https://arxiv.org/abs/2511.11849)
*Junyang He,Judy Fox,Alireza Jafari,Ying-Jung Chen,Geoffrey Fox*

Main category: cs.LG

TL;DR: 该研究探讨了在时间序列模型中整合领域知识对水文降雨径流建模的影响，发现包含全面已知外生输入（特别是自然年度周期性时间序列）的模型优于基础模型和有限方法。


<details>
  <summary>Details</summary>
Motivation: 尽管时间序列基础模型研究取得进展，但在物理科学特定下游应用中的有效性研究较少。本研究旨在探索将领域知识整合到时间序列模型中对水文降雨径流建模的作用。

Method: 使用CAMELS-US数据集（包含671个位置的降雨和径流数据，具有6个时间序列流和30个静态特征），比较基准模型和基础模型，重点评估整合领域知识（特别是自然年度周期性时间序列）的效果。

Result: 结果表明，包含全面已知外生输入的模型表现优于有限方法，包括基础模型。其中，整合自然年度周期性时间序列带来了最显著的改进。

Conclusion: 在时间序列建模中，特别是对于水文应用，整合领域知识和外生输入（尤其是自然周期性特征）能够显著提升模型性能，这比单纯使用基础模型更有效。

Abstract: Recent advances in time series research facilitate the development of foundation models. While many state-of-the-art time series foundation models have been introduced, few studies examine their effectiveness in specific downstream applications in physical science. This work investigates the role of integrating domain knowledge into time series models for hydrological rainfall-runoff modeling. Using the CAMELS-US dataset, which includes rainfall and runoff data from 671 locations with six time series streams and 30 static features, we compare baseline and foundation models. Results demonstrate that models incorporating comprehensive known exogenous inputs outperform more limited approaches, including foundation models. Notably, incorporating natural annual periodic time series contribute the most significant improvements.

</details>


### [95] [Transformers vs. Recurrent Models for Estimating Forest Gross Primary Production](https://arxiv.org/abs/2511.11880)
*David Montero,Miguel D. Mahecha,Francesco Martinuzzi,César Aybar,Anne Klosterhalfen,Alexander Knohl,Jesús Anaya,Clemens Mosig,Sebastian Wieneke*

Main category: cs.LG

TL;DR: 比较GPT-2和LSTM两种深度学习模型在预测森林CO2吸收(GPP)方面的表现，发现LSTM整体精度更高但GPT-2在极端事件中表现更好，同时分析了多模态输入特征的重要性。


<details>
  <summary>Details</summary>
Motivation: 监测森林CO2吸收的时空动态是生态系统研究的关键挑战，传统方法如涡度协方差塔空间覆盖有限，遥感方法大多依赖单传感器和统计模型，难以捕捉GPP的复杂时间动态。

Method: 使用GPT-2（Transformer架构）和LSTM（循环神经网络）两种代表性深度学习模型，结合多变量输入数据预测GPP，分析时间上下文长度和特征重要性。

Result: 两种模型达到相似精度，LSTM整体表现更好但GPT-2在极端事件中更优；LSTM使用更短的输入窗口即可达到相似精度；辐射是最重要的预测因子，其次是Sentinel-2、MODIS地表温度和Sentinel-1数据。

Conclusion: 模型架构、上下文长度和多模态输入共同决定了GPP预测性能，为未来开发监测陆地碳动态的深度学习框架提供指导。

Abstract: Monitoring the spatiotemporal dynamics of forest CO$_2$ uptake (Gross Primary Production, GPP), remains a central challenge in terrestrial ecosystem research. While Eddy Covariance (EC) towers provide high-frequency estimates, their limited spatial coverage constrains large-scale assessments. Remote sensing offers a scalable alternative, yet most approaches rely on single-sensor spectral indices and statistical models that are often unable to capture the complex temporal dynamics of GPP. Recent advances in deep learning (DL) and data fusion offer new opportunities to better represent the temporal dynamics of vegetation processes, but comparative evaluations of state-of-the-art DL models for multimodal GPP prediction remain scarce. Here, we explore the performance of two representative models for predicting GPP: 1) GPT-2, a transformer architecture, and 2) Long Short-Term Memory (LSTM), a recurrent neural network, using multivariate inputs. Overall, both achieve similar accuracy. But, while LSTM performs better overall, GPT-2 excels during extreme events. Analysis of temporal context length further reveals that LSTM attains similar accuracy using substantially shorter input windows than GPT-2, highlighting an accuracy-efficiency trade-off between the two architectures. Feature importance analysis reveals radiation as the dominant predictor, followed by Sentinel-2, MODIS land surface temperature, and Sentinel-1 contributions. Our results demonstrate how model architecture, context length, and multimodal inputs jointly determine performance in GPP prediction, guiding future developments of DL frameworks for monitoring terrestrial carbon dynamics.

</details>


### [96] [Better LLM Reasoning via Dual-Play](https://arxiv.org/abs/2511.11881)
*Zhengxin Zhang,Chengyu Huang,Aochong Oliver Li,Claire Cardie*

Main category: cs.LG

TL;DR: PasoDoble是一个无需监督训练的LLM双对抗学习框架，通过Proposer生成挑战性问题和Solver解答问题的对抗训练，提升语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM训练严重依赖外部监督，而对抗学习特别是双对抗训练可以减少对外部监督的依赖，但LLM容易奖励攻击和训练不稳定。

Method: 从同一基础模型初始化两个模型：Proposer生成带正确答案的挑战性问题，Solver尝试解答。Proposer从预训练数据集中获取知识确保问题质量，采用联合更新避免奖励攻击，并引入离线范式增强训练稳定性。

Result: 实验结果显示PasoDoble能够提升LLM的推理性能。

Conclusion: PasoDoble框架证明了无需监督的双对抗训练可以有效提升语言模型的推理能力，为减少对外部监督的依赖提供了可行方案。

Abstract: Large Language Models (LLMs) have achieved remarkable progress through Reinforcement Learning with Verifiable Rewards (RLVR), yet still rely heavily on external supervision (e.g., curated labels). Adversarial learning, particularly through self-play, offers a promising alternative that enables models to iteratively learn from themselves - thus reducing reliance on external supervision. Dual-play extends adversarial learning by assigning specialized roles to two models and training them against each other, fostering sustained competition and mutual evolution. Despite its promise, adapting dual-play training to LLMs remains limited, largely due to their susceptibility to reward hacking and training instability. In this paper, we introduce PasoDoble, a novel LLM dual-play framework. PasoDoble adversarially trains two models initialized from the same base model: a Proposer, which generates challenging questions with ground-truth answers, and a Solver, which attempts to solve them. We enrich the Proposer with knowledge from a pre-training dataset to ensure the questions' quality and diversity. To avoid reward hacking, the Proposer is rewarded for producing only valid questions that push the Solver's limit, while the Solver is rewarded for solving them correctly, and both are updated jointly. To further enhance training stability, we introduce an optional offline paradigm that decouples Proposer and Solver updates, alternately updating each for several steps while holding the other fixed. Notably, PasoDoble operates without supervision during training. Experimental results show that PasoDoble can improve the reasoning performance of LLMs. Our project page is available at https://hcy123902.github.io/PasoDoble.

</details>


### [97] [FLEX: Feature Importance from Layered Counterfactual Explanations](https://arxiv.org/abs/2511.11891)
*Nawid Keshtmand,Roussel Desmond Nzoyem,Jeffrey Nicholas Clark*

Main category: cs.LG

TL;DR: FLEX是一个模型和领域无关的框架，将反事实解释转化为局部、区域和全局层面的特征变化频率评分，弥合了局部追索和全局归因之间的差距。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型缺乏可解释性限制了在高风险环境中的安全部署，现有反事实解释通常局限于实例特定，无法量化哪些特征在特征空间的连贯区域或整个数据集中系统地驱动结果变化。

Method: FLEX框架通过在不同实例和邻域上聚合，将反事实集合转换为特征变化频率评分，与不同的反事实生成方法兼容，允许用户强调稀疏性、可行性或可操作性等特性。

Result: 在交通事故严重性预测和贷款审批两个表格任务上的评估显示，FLEX的全局排名与SHAP相关但能发现额外驱动因素，区域分析揭示了全局摘要遗漏的上下文特定因素。

Conclusion: FLEX弥合了局部追索和全局归因之间的差距，支持在风险敏感应用中进行透明和面向干预的决策制定。

Abstract: Machine learning models achieve state-of-the-art performance across domains, yet their lack of interpretability limits safe deployment in high-stakes settings. Counterfactual explanations are widely used to provide actionable "what-if" recourse, but they typically remain instance-specific and do not quantify which features systematically drive outcome changes within coherent regions of the feature space or across an entire dataset. We introduce FLEX (Feature importance from Layered counterfactual EXplanations), a model- and domain-agnostic framework that converts sets of counterfactuals into feature change frequency scores at local, regional, and global levels. FLEX generalises local change-frequency measures by aggregating across instances and neighbourhoods, offering interpretable rankings that reflect how often each feature must change to flip predictions. The framework is compatible with different counterfactual generation methods, allowing users to emphasise characteristics such as sparsity, feasibility, or actionability, thereby tailoring the derived feature importances to practical constraints. We evaluate FLEX on two contrasting tabular tasks: traffic accident severity prediction and loan approval, and compare FLEX to SHAP- and LIME-derived feature importance values. Results show that (i) FLEX's global rankings correlate with SHAP while surfacing additional drivers, and (ii) regional analyses reveal context-specific factors that global summaries miss. FLEX thus bridges the gap between local recourse and global attribution, supporting transparent and intervention-oriented decision-making in risk-sensitive applications.

</details>


### [98] [Chain-of-Generation: Progressive Latent Diffusion for Text-Guided Molecular Design](https://arxiv.org/abs/2511.11894)
*Lingxiao Li,Haobo Zhang,Bin Chen,Jiayu Zhou*

Main category: cs.LG

TL;DR: 提出Chain-of-Generation (CoG)框架，通过多阶段潜在扩散模型解决文本条件分子生成中一次性条件编码的问题，提高语义对齐和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的分子生成方法采用一次性条件编码，难以同时满足提示中的所有要求，存在生成组件可解释性差、无法生成所有子结构、同时考虑所有要求过于激进等问题。

Method: CoG将提示分解为课程顺序的语义片段，逐步将其作为中间目标，引导去噪轨迹朝向满足越来越丰富语言约束的分子。还引入了后对齐学习阶段来加强文本和分子潜在空间之间的对应关系。

Result: 在基准和实际任务上的广泛实验表明，CoG比一次性基线方法产生更高的语义对齐、多样性和可控性，生成的分子更忠实地反映复杂、组合性提示，同时提供生成过程的透明洞察。

Conclusion: CoG框架通过多阶段渐进式条件编码有效解决了文本条件分子生成中的关键挑战，显著提升了生成质量和可控性。

Abstract: Text-conditioned molecular generation aims to translate natural-language descriptions into chemical structures, enabling scientists to specify functional groups, scaffolds, and physicochemical constraints without handcrafted rules. Diffusion-based models, particularly latent diffusion models (LDMs), have recently shown promise by performing stochastic search in a continuous latent space that compactly captures molecular semantics. Yet existing methods rely on one-shot conditioning, where the entire prompt is encoded once and applied throughout diffusion, making it hard to satisfy all the requirements in the prompt. We discuss three outstanding challenges of one-shot conditioning generation, including the poor interpretability of the generated components, the failure to generate all substructures, and the overambition in considering all requirements simultaneously. We then propose three principles to address those challenges, motivated by which we propose Chain-of-Generation (CoG), a training-free multi-stage latent diffusion framework. CoG decomposes each prompt into curriculum-ordered semantic segments and progressively incorporates them as intermediate goals, guiding the denoising trajectory toward molecules that satisfy increasingly rich linguistic constraints. To reinforce semantic guidance, we further introduce a post-alignment learning phase that strengthens the correspondence between textual and molecular latent spaces. Extensive experiments on benchmark and real-world tasks demonstrate that CoG yields higher semantic alignment, diversity, and controllability than one-shot baselines, producing molecules that more faithfully reflect complex, compositional prompts while offering transparent insight into the generation process.

</details>


### [99] [Robust Bidirectional Associative Memory via Regularization Inspired by the Subspace Rotation Algorithm](https://arxiv.org/abs/2511.11902)
*Ci Lin,Tet Yeap,Iluju Kiringa,Biwei Zhang*

Main category: cs.LG

TL;DR: 提出了双向子空间旋转算法(B-SRA)来提升双向联想记忆(BAM)的鲁棒性，通过正交权重矩阵和梯度模式对齐两个关键原则，显著改善了BAM对噪声和对抗攻击的抵抗力。


<details>
  <summary>Details</summary>
Motivation: 传统的双向联想记忆使用双向反向传播训练时，存在鲁棒性差、对噪声和对抗攻击敏感的问题，需要开发更鲁棒的训练方法。

Method: 提出梯度无关的双向子空间旋转算法(B-SRA)，引入正交权重矩阵(OWM)和梯度模式对齐(GPA)两个关键原则，并将这些正则化策略整合到双向反向传播中。

Result: SAME配置(结合OWM和GPA)在所有方法中表现出最强的鲁棒性，在50、100、200个关联对的不同记忆容量下都显著提升了BAM的抗干扰能力。

Conclusion: B-SRA和提出的正则化策略能够构建更鲁棒的联想记忆系统，为开发弹性神经网络架构开辟了新方向。

Abstract: Bidirectional Associative Memory (BAM) trained with Bidirectional Backpropagation (B-BP) often suffers from poor robustness and high sensitivity to noise and adversarial attacks. To address these issues, we propose a novel gradient-free training algorithm, the Bidirectional Subspace Rotation Algorithm (B-SRA), which significantly improves the robustness and convergence behavior of BAM. Through comprehensive experiments, we identify two key principles -- orthogonal weight matrices (OWM) and gradient-pattern alignment (GPA) -- as central to enhancing the robustness of BAM. Motivated by these findings, we introduce new regularization strategies into B-BP, resulting in models with greatly improved resistance to corruption and adversarial perturbations. We further conduct an ablation study across different training strategies to determine the most robust configuration and evaluate BAM's performance under a variety of attack scenarios and memory capacities, including 50, 100, and 200 associative pairs. Among all methods, the SAME configuration, which integrates both OWM and GPA, achieves the strongest resilience. Overall, our results demonstrate that B-SRA and the proposed regularization strategies lead to substantially more robust associative memories and open new directions for building resilient neural architectures.

</details>


### [100] [A Systematic Study of Model Extraction Attacks on Graph Foundation Models](https://arxiv.org/abs/2511.11912)
*Haoyan Xu,Ruizhi Qian,Jiate Li,Yushun Dong,Minghao Lin,Hanson Yan,Zhengtao Yao,Qinghua Liu,Junhao Dong,Ruopeng Huang,Yue Zhao,Mengyuan Li*

Main category: cs.LG

TL;DR: 本文首次系统研究了针对图基础模型(GFMs)的模型提取攻击(MEAs)，揭示了GFMs显著扩大了MEA攻击面，攻击者仅需原始训练成本的一小部分就能近似受害者模型，且准确率几乎无损失。


<details>
  <summary>Details</summary>
Motivation: 随着图基础模型(GFMs)的发展，这些模型因其高预训练成本和跨领域知识而成为模型提取攻击的有吸引目标。先前研究仅关注单图上的小型图神经网络，对大规模多模态GFMs的安全影响尚未探索。

Method: 提出了一种轻量级提取方法，通过监督回归图嵌入来训练攻击者编码器。该方法无需对比预训练数据，就能学习与受害者文本编码器保持对齐的编码器，并保留其在未见图上的零样本推理能力。

Result: 在七个数据集上的实验表明，攻击者仅需原始训练成本的一小部分就能近似受害者模型，准确率几乎无损失。

Conclusion: GFMs显著扩大了MEA攻击面，凸显了在大规模图学习系统中部署感知安全防御的必要性。

Abstract: Graph machine learning has advanced rapidly in tasks such as link prediction, anomaly detection, and node classification. As models scale up, pretrained graph models have become valuable intellectual assets because they encode extensive computation and domain expertise. Building on these advances, Graph Foundation Models (GFMs) mark a major step forward by jointly pretraining graph and text encoders on massive and diverse data. This unifies structural and semantic understanding, enables zero-shot inference, and supports applications such as fraud detection and biomedical analysis. However, the high pretraining cost and broad cross-domain knowledge in GFMs also make them attractive targets for model extraction attacks (MEAs). Prior work has focused only on small graph neural networks trained on a single graph, leaving the security implications for large-scale and multimodal GFMs largely unexplored. This paper presents the first systematic study of MEAs against GFMs. We formalize a black-box threat model and define six practical attack scenarios covering domain-level and graph-specific extraction goals, architectural mismatch, limited query budgets, partial node access, and training data discrepancies. To instantiate these attacks, we introduce a lightweight extraction method that trains an attacker encoder using supervised regression of graph embeddings. Even without contrastive pretraining data, this method learns an encoder that stays aligned with the victim text encoder and preserves its zero-shot inference ability on unseen graphs. Experiments on seven datasets show that the attacker can approximate the victim model using only a tiny fraction of its original training cost, with almost no loss in accuracy. These findings reveal that GFMs greatly expand the MEA surface and highlight the need for deployment-aware security defenses in large-scale graph learning systems.

</details>


### [101] [Batch Matrix-form Equations and Implementation of Multilayer Perceptrons](https://arxiv.org/abs/2511.11918)
*Wieger Wesselink,Bram Grooten,Huub van de Wetering,Qiao Xiao,Decebal Constantin Mocanu*

Main category: cs.LG

TL;DR: 本文提供了多层感知机(MLP)的完整批量矩阵形式推导，包括前向和后向传播的数学公式，并通过符号数学验证了所有梯度方程，实现了多种框架的统一参考实现。


<details>
  <summary>Details</summary>
Motivation: 虽然MLP是现代深度学习的基础，但其算法细节很少以完整的批量矩阵形式呈现，大多数文献采用逐样本梯度或依赖自动微分。明确的批量矩阵形式对于透明分析、系统优化（如稀疏神经网络）至关重要。

Method: 推导了所有标准层和高级层（包括批归一化和softmax）的前向和后向传播方程，使用SymPy符号数学库验证所有方程，并在NumPy、PyTorch、JAX、TensorFlow和优化的C++后端构建统一参考实现。

Result: 建立了经过验证的批量矩阵形式反向传播完整推导，所有梯度方程都通过符号验证，提供了基于少量矩阵原语的统一Python和C++实现，并展示了显式公式如何实现高效稀疏计算。

Conclusion: 这些结果为理解、教学和研究神经网络算法建立了经过验证、可扩展的基础，特别在稀疏计算等场景中具有重要价值。

Abstract: Multilayer perceptrons (MLPs) remain fundamental to modern deep learning, yet their algorithmic details are rarely presented in complete, explicit \emph{batch matrix-form}. Rather, most references express gradients per sample or rely on automatic differentiation. Although automatic differentiation can achieve equally high computational efficiency, the usage of batch matrix-form makes the computational structure explicit, which is essential for transparent, systematic analysis, and optimization in settings such as sparse neural networks. This paper fills that gap by providing a mathematically rigorous and implementation-ready specification of MLPs in batch matrix-form. We derive forward and backward equations for all standard and advanced layers, including batch normalization and softmax, and validate all equations using the symbolic mathematics library SymPy. From these specifications, we construct uniform reference implementations in NumPy, PyTorch, JAX, TensorFlow, and a high-performance C++ backend optimized for sparse operations. Our main contributions are: (1) a complete derivation of batch matrix-form backpropagation for MLPs, (2) symbolic validation of all gradient equations, (3) uniform Python and C++ reference implementations grounded in a small set of matrix primitives, and (4) demonstration of how explicit formulations enable efficient sparse computation. Together, these results establish a validated, extensible foundation for understanding, teaching, and researching neural network algorithms.

</details>


### [102] [Beyond the Laplacian: Interpolated Spectral Augmentation for Graph Neural Networks](https://arxiv.org/abs/2511.11928)
*Ziyao Cui,Edric Tam*

Main category: cs.LG

TL;DR: 本文提出了插值拉普拉斯嵌入(ILEs)，这是一种从图矩阵家族中导出的节点特征增强方法，用于在节点特征有限时提升图神经网络(GNNs)的性能。


<details>
  <summary>Details</summary>
Motivation: GNNs的性能严重依赖于信息丰富的节点特征，但在实际应用中这些特征往往有限或缺失。虽然拉普拉斯谱嵌入是自然选择，但作者探索其他图矩阵的谱嵌入是否也能提供有用的表示。

Method: 引入插值拉普拉斯嵌入(ILEs)，这是一个简单但表达力丰富的图矩阵家族。使用谱图理论工具来解释ILEs捕获的结构信息。

Result: 通过模拟和真实世界数据集的实验表明，通过ILEs进行特征增强可以提升常用GNN架构的性能。

Conclusion: 本文提供了一个简单实用的方法，扩展了从业者在节点特征有限时的谱增强工具包。

Abstract: Graph neural networks (GNNs) are fundamental tools in graph machine learning. The performance of GNNs relies crucially on the availability of informative node features, which can be limited or absent in real-life datasets and applications. A natural remedy is to augment the node features with embeddings computed from eigenvectors of the graph Laplacian matrix. While it is natural to default to Laplacian spectral embeddings, which capture meaningful graph connectivity information, we ask whether spectral embeddings from alternative graph matrices can also provide useful representations for learning. We introduce Interpolated Laplacian Embeddings (ILEs), which are derived from a simple yet expressive family of graph matrices. Using tools from spectral graph theory, we offer a straightforward interpretation of the structural information that ILEs capture. We demonstrate through simulations and experiments on real-world datasets that feature augmentation via ILEs can improve performance across commonly used GNN architectures. Our work offers a straightforward and practical approach that broadens the practitioner's spectral augmentation toolkit when node features are limited.

</details>


### [103] [A Systematic Analysis of Out-of-Distribution Detection Under Representation and Training Paradigm Shifts](https://arxiv.org/abs/2511.11934)
*C. César Claros Olivares,Austin J. Brockmeier*

Main category: cs.LG

TL;DR: 系统比较了CLIP分层机制下的OOD检测方法，发现特征空间决定检测效果：概率分数在误分类检测中表现最佳，几何感知分数在CNN上对强分布偏移更有效，而在ViT上GradNorm和KPCA重建误差表现稳定。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法在不同表示范式下的表现缺乏系统性比较，需要为不同分布偏移场景提供统计基础的方法选择指导。

Method: 使用AURC和AUGRC作为主要指标，比较CNN从头训练和ViT微调两种表示范式，在CIFAR-10/100、SuperCIFAR-100和TinyImageNet数据集上评估，采用多重比较控制的基于排名的统计流程。

Result: 学习到的特征空间在很大程度上决定了OOD检测效果；概率分数在误分类检测中占主导；在强分布偏移下，几何感知分数在CNN上表现更好，而在ViT上GradNorm和KPCA重建误差保持竞争力。

Conclusion: 支持以表示为中心的OOD检测观点，为分布偏移下的方法选择提供了统计基础指导，并发现简单PCA投影可以改进多个检测器。

Abstract: We present a systematic comparison of out-of-distribution (OOD) detection methods across CLIP-stratified regimes using AURC and AUGRC as primary metrics. Experiments cover two representation paradigms: CNNs trained from scratch and a fine-tuned Vision Transformer (ViT), evaluated on CIFAR-10/100, SuperCIFAR-100, and TinyImageNet. Using a multiple-comparison-controlled, rank-based pipeline (Friedman test with Conover-Holm post-hoc) and Bron-Kerbosch cliques, we find that the learned feature space largely determines OOD efficacy. For both CNNs and ViTs, probabilistic scores (e.g., MSR, GEN) dominate misclassification (ID) detection. Under stronger shifts, geometry-aware scores (e.g., NNGuide, fDBD, CTM) prevail on CNNs, whereas on ViTs GradNorm and KPCA Reconstruction Error remain consistently competitive. We further show a class-count-dependent trade-off for Monte-Carlo Dropout (MCD) and that a simple PCA projection improves several detectors. These results support a representation-centric view of OOD detection and provide statistically grounded guidance for method selection under distribution shift.

</details>


### [104] [SurvBench: A Standardised Preprocessing Pipeline for Multi-Modal Electronic Health Record Survival Analysis](https://arxiv.org/abs/2511.11935)
*Munib Mesinovic,Tingting Zhu*

Main category: cs.LG

TL;DR: SurvBench是一个开源预处理管道，将原始PhysioNet数据集转换为标准化的多模态生存分析张量，解决深度学习生存模型的可复现性问题。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录数据为深度学习生存分析提供了巨大机会，但由于预处理方法不一致，可复现性受到严重限制。

Method: 提供三个主要重症监护数据库的数据加载器，支持时间序列生命体征、静态人口统计学、ICD诊断代码和放射学报告等多种模态，实施严格的数据质量控制、患者级分割、显式缺失值跟踪和标准化时间聚合。

Result: 处理单风险和竞争风险场景，输出与pycox库包和标准统计及深度学习模型实现兼容。

Conclusion: 通过提供可复现的配置驱动预处理和全面文档，SurvBench解决了阻碍深度学习生存模型公平比较的"预处理差距"，使研究人员能够专注于方法创新而非数据工程。

Abstract: Electronic health record (EHR) data present tremendous opportunities for advancing survival analysis through deep learning, yet reproducibility remains severely constrained by inconsistent preprocessing methodologies. We present SurvBench, a comprehensive, open-source preprocessing pipeline that transforms raw PhysioNet datasets into standardised, model-ready tensors for multi-modal survival analysis. SurvBench provides data loaders for three major critical care databases, MIMIC-IV, eICU, and MC-MED, supporting diverse modalities including time-series vitals, static demographics, ICD diagnosis codes, and radiology reports. The pipeline implements rigorous data quality controls, patient-level splitting to prevent data leakage, explicit missingness tracking, and standardised temporal aggregation. SurvBench handles both single-risk (e.g., in-hospital mortality) and competing-risks scenarios (e.g., multiple discharge outcomes). The outputs are compatible with pycox library packages and implementations of standard statistical and deep learning models. By providing reproducible, configuration-driven preprocessing with comprehensive documentation, SurvBench addresses the "preprocessing gap" that has hindered fair comparison of deep learning survival models, enabling researchers to focus on methodological innovation rather than data engineering.

</details>


### [105] [Learning the relative composition of EEG signals using pairwise relative shift pretraining](https://arxiv.org/abs/2511.11940)
*Christopher Sandino,Sayeri Lala,Geeling Chau,Melika Ayoughi,Behrooz Mahasseni,Ellen Zippi,Ali Moin,Erdrin Azemi,Hanlin Goh*

Main category: cs.LG

TL;DR: 提出PARS预训练方法，通过预测随机采样EEG窗口对之间的相对时间偏移来学习神经信号中的长程依赖关系，在多种EEG解码任务中优于现有自监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有EEG自监督学习方法主要使用掩码重建策略，专注于局部时间模式，而能够学习长程依赖关系的位置预测预训练方法在EEG领域尚未充分探索。

Method: 引入PARS预训练方法，这是一种新颖的前置任务，通过预测随机采样EEG窗口对之间的相对时间偏移来训练编码器，使其能够捕捉神经信号中的相对时间组成和长程依赖关系。

Result: 在多种EEG解码任务的综合评估中，PARS预训练的transformer模型在标签高效和迁移学习设置中始终优于现有的预训练策略。

Conclusion: PARS为自监督EEG表示学习建立了新范式，通过相对时间偏移预测有效捕捉神经信号中的长程依赖关系。

Abstract: Self-supervised learning (SSL) offers a promising approach for learning electroencephalography (EEG) representations from unlabeled data, reducing the need for expensive annotations for clinical applications like sleep staging and seizure detection. While current EEG SSL methods predominantly use masked reconstruction strategies like masked autoencoders (MAE) that capture local temporal patterns, position prediction pretraining remains underexplored despite its potential to learn long-range dependencies in neural signals. We introduce PAirwise Relative Shift or PARS pretraining, a novel pretext task that predicts relative temporal shifts between randomly sampled EEG window pairs. Unlike reconstruction-based methods that focus on local pattern recovery, PARS encourages encoders to capture relative temporal composition and long-range dependencies inherent in neural signals. Through comprehensive evaluation on various EEG decoding tasks, we demonstrate that PARS-pretrained transformers consistently outperform existing pretraining strategies in label-efficient and transfer learning settings, establishing a new paradigm for self-supervised EEG representation learning.

</details>


### [106] [Computation-aware Energy-harvesting Federated Learning: Cyclic Scheduling with Selective Participation](https://arxiv.org/abs/2511.11949)
*Eunjeong Jeong,Nikolaos Pappas*

Main category: cs.LG

TL;DR: 提出了FedBacys框架，通过基于电池状态的循环客户端参与机制来优化能量收集联邦学习系统的能效。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在分布式学习中面临高能耗问题，特别是在能量收集系统中，设备参与度因能量限制而不稳定。

Method: 使用基于用户电池水平的循环客户端参与机制，通过聚类客户端并按顺序调度来最小化冗余计算。

Result: FedBacys显著降低了系统范围的能耗，提高了学习稳定性，其变体FedBacys-Odd进一步降低能耗且不牺牲性能。

Conclusion: 该框架在能量效率和鲁棒性方面优于现有算法，为能量受限的联邦学习系统提供了有效解决方案。

Abstract: Federated Learning (FL) is a powerful paradigm for distributed learning, but its increasing complexity leads to significant energy consumption from client-side computations for training models. In particular, the challenge is critical in energy-harvesting FL (EHFL) systems where participation availability of each device oscillates due to limited energy. To address this, we propose FedBacys, a battery-aware EHFL framework using cyclic client participation based on users' battery levels. By clustering clients and scheduling them sequentially, FedBacys minimizes redundant computations, reduces system-wide energy usage, and improves learning stability. We also introduce FedBacys-Odd, a more energy-efficient variant that allows clients to participate selectively, further reducing energy costs without compromising performance. We provide a convergence analysis for our framework and demonstrate its superior energy efficiency and robustness compared to existing algorithms through numerical experiments.

</details>


### [107] [Quantile Q-Learning: Revisiting Offline Extreme Q-Learning with Quantile Regression](https://arxiv.org/abs/2511.11973)
*Xinming Gao,Shangzhe Li,Yujin Cai,Wenwu Yu*

Main category: cs.LG

TL;DR: 本文提出了一种改进的离线强化学习方法，通过量化回归估计温度系数β，并引入值正则化技术来解决XQL和MXQL方法中的超参数调优困难和训练不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在固定数据集上学习策略而无需环境交互，在高风险或成本高昂的领域具有重要价值。但现有的XQL及其稳定变体MXQL存在两个主要问题：需要针对每个数据集和领域进行大量超参数调优，以及训练过程中的不稳定性。

Method: 1. 基于温和假设，通过量化回归来估计温度系数β；2. 受约束值学习最新进展启发，引入具有温和泛化性的值正则化技术来进一步提高训练稳定性。

Result: 实验结果表明，所提出的算法在一系列基准任务（包括D4RL和NeoRL2）上实现了竞争性或更优的性能，同时保持了稳定的训练动态，并在所有数据集和领域上使用一致的超参数集。

Conclusion: 该方法成功解决了XQL和MXQL方法的局限性，实现了稳定的训练和跨域一致的超参数设置，在离线强化学习任务中表现出色。

Abstract: Offline reinforcement learning (RL) enables policy learning from fixed datasets without further environment interaction, making it particularly valuable in high-risk or costly domains. Extreme $Q$-Learning (XQL) is a recent offline RL method that models Bellman errors using the Extreme Value Theorem, yielding strong empirical performance. However, XQL and its stabilized variant MXQL suffer from notable limitations: both require extensive hyperparameter tuning specific to each dataset and domain, and also exhibit instability during training. To address these issues, we proposed a principled method to estimate the temperature coefficient $β$ via quantile regression under mild assumptions. To further improve training stability, we introduce a value regularization technique with mild generalization, inspired by recent advances in constrained value learning. Experimental results demonstrate that the proposed algorithm achieves competitive or superior performance across a range of benchmark tasks, including D4RL and NeoRL2, while maintaining stable training dynamics and using a consistent set of hyperparameters across all datasets and domains.

</details>


### [108] [ReCast: Reliability-aware Codebook Assisted Lightweight Time Series Forecasting](https://arxiv.org/abs/2511.11991)
*Xiang Ma,Taihua Chen,Pengcheng Wang,Xuemei Li,Caiming Zhang*

Main category: cs.LG

TL;DR: 提出ReCast框架，通过可学习码本对局部模式进行离散化编码，结合双路径架构和可靠性感知码本更新策略，实现轻量级且鲁棒的时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖全局分解，对局部复杂动态模式效果不佳，且模型复杂度高限制了在资源受限环境中的应用。

Method: 使用码本量化局部模式，采用双路径架构（量化路径+残差路径），通过可靠性感知码本更新策略融合多视角可靠性因子。

Result: 在准确性、效率和分布偏移适应性方面优于现有最优模型。

Conclusion: ReCast框架能有效捕捉局部重复模式，实现轻量级且鲁棒的时间序列预测。

Abstract: Time series forecasting is crucial for applications in various domains. Conventional methods often rely on global decomposition into trend, seasonal, and residual components, which become ineffective for real-world series dominated by local, complex, and highly dynamic patterns. Moreover, the high model complexity of such approaches limits their applicability in real-time or resource-constrained environments. In this work, we propose a novel \textbf{RE}liability-aware \textbf{C}odebook-\textbf{AS}sisted \textbf{T}ime series forecasting framework (\textbf{ReCast}) that enables lightweight and robust prediction by exploiting recurring local shapes. ReCast encodes local patterns into discrete embeddings through patch-wise quantization using a learnable codebook, thereby compactly capturing stable regular structures. To compensate for residual variations not preserved by quantization, ReCast employs a dual-path architecture comprising a quantization path for efficient modeling of regular structures and a residual path for reconstructing irregular fluctuations. A central contribution of ReCast is a reliability-aware codebook update strategy, which incrementally refines the codebook via weighted corrections. These correction weights are derived by fusing multiple reliability factors from complementary perspectives by a distributionally robust optimization (DRO) scheme, ensuring adaptability to non-stationarity and robustness to distribution shifts. Extensive experiments demonstrate that ReCast outperforms state-of-the-art (SOTA) models in accuracy, efficiency, and adaptability to distribution shifts.

</details>


### [109] [Selecting Fine-Tuning Examples by Quizzing VLMs](https://arxiv.org/abs/2511.12002)
*Tenghao Ji,Eytan Adar*

Main category: cs.LG

TL;DR: QZLoRA是一个通过QuizRank方法自动选择高质量训练图像来进行LoRA微调的框架，能够用更少样本生成更对齐、更逼真的图像。


<details>
  <summary>Details</summary>
Motivation: 在微调文本到图像扩散模型时，从质量参差不齐的图像集（如维基共享资源）中选择训练样本往往导致输出质量不佳。需要选择能代表目标概念的高质量图像来确保生成图像的典型特征。

Method: 提出QZLoRA框架，利用QuizRank方法自动对图像进行排名，将图像视为'教育干预'并通过视觉语言模型进行'测验'来评估图像质量，然后使用低秩适应（LoRA）进行参数高效微调。

Result: QZLoRA能够生成更对齐、更逼真的图像，且所需样本更少。微调后的模型还能生成具有代表性的风格化图像（如插画）。

Conclusion: 将自动视觉推理与参数高效微调相结合，为主题自适应生成建模提供了有前景的解决方案。

Abstract: A challenge in fine-tuning text-to-image diffusion models for specific topics is to select good examples. Fine-tuning from image sets of varying quality, such as Wikipedia Commons, will often produce poor output. However, training images that \textit{do} exemplify the target concept (e.g., a \textit{female Mountain Bluebird}) help ensure that the generated images are similarly representative (e.g., have the prototypical blue-wings and gray chest). In this work, we propose QZLoRA, a framework to select images for low-rank adaptation (LoRA). The approach leverages QuizRank, a method to automatically rank images by treating them as an `educational intervention' and `quizzing' a VLM. We demonstrate that QZLoRA can produce better aligned, photorealistic images with fewer samples. We also show that these fine-tuned models can produce stylized that are similarly representative (i.e., illustrations). Our results highlight the promise of combining automated visual reasoning with parameter-efficient fine-tuning for topic-adaptive generative modeling.

</details>


### [110] [EARL: Entropy-Aware RL Alignment of LLMs for Reliable RTL Code Generation](https://arxiv.org/abs/2511.12033)
*Jiahe Shi,Zhengqi Gao,Ching-Yun Ko,Duane Boning*

Main category: cs.LG

TL;DR: EARL是一个基于熵感知强化学习的Verilog生成框架，通过选择性更新高熵令牌来提升RTL代码生成的功能正确性，在VerilogEval和RTLLM基准上比现有LLM基线提高了14.7%的功能通过率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在RTL代码生成中存在语法错误、功能幻觉和设计意图对齐不足的问题，需要利用硬件可执行和形式化验证信号来进一步对齐模型输出与设计意图。

Method: 提出EARL框架，使用可验证奖励信号进行策略优化，并引入熵引导的选择性更新机制，将策略梯度限制在高熵令牌上，专注于控制流和模块结构的关键区域。

Result: 在VerilogEval和RTLLM基准测试中，EARL比之前的LLM基线提高了高达14.7%的功能通过率，同时减少了不必要的更新并提高了训练稳定性。

Conclusion: 将强化学习聚焦于关键的高不确定性令牌，能够为结构化RTL代码生成实现更可靠和有针对性的策略改进。

Abstract: Recent advances in large language models (LLMs) have demonstrated significant potential in hardware design automation, particularly in using natural language to synthesize Register-Transfer Level (RTL) code. Despite this progress, a gap remains between model capability and the demands of real-world RTL design, including syntax errors, functional hallucinations, and weak alignment to designer intent. Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising approach to bridge this gap, as hardware provides executable and formally checkable signals that can be used to further align model outputs with design intent. However, in long, structured RTL code sequences, not all tokens contribute equally to functional correctness, and naïvely spreading gradients across all tokens dilutes learning signals. A key insight from our entropy analysis in RTL generation is that only a small fraction of tokens (e.g., always, if, assign, posedge) exhibit high uncertainty and largely influence control flow and module structure. To address these challenges, we present EARL, an Entropy-Aware Reinforcement Learning framework for Verilog generation. EARL performs policy optimization using verifiable reward signals and introduces entropy-guided selective updates that gate policy gradients to high-entropy tokens. This approach preserves training stability and concentrates gradient updates on functionally important regions of code. Our experiments on VerilogEval and RTLLM show that EARL improves functional pass rates over prior LLM baselines by up to 14.7%, while reducing unnecessary updates and improving training stability. These results indicate that focusing RL on critical, high-uncertainty tokens enables more reliable and targeted policy improvement for structured RTL code generation.

</details>


### [111] [Mesh-based Super-resolution of Detonation Flows with Multiscale Graph Transformers](https://arxiv.org/abs/2511.12041)
*Shivam Barwey,Pinaki Pal*

Main category: cs.LG

TL;DR: 提出了一种基于多尺度图变换器的网格超分辨率方法（SR-GT），用于反应流场的超分辨率重建，相比传统插值方法具有更高精度。


<details>
  <summary>Details</summary>
Motivation: 超分辨率流场重建对于亚网格/亚滤波器闭合建模、加速时空预测、数据压缩以及稀疏实验测量的上采样等应用具有重要价值。

Method: 开发了基于图变换器的多尺度方法，利用图表示兼容复杂几何和非均匀网格，通过变换器捕捉低分辨率流场的远距离依赖关系并识别重要特征。

Result: 在2D氢气-空气预混爆轰传播的挑战性测试中，SR-GT在反应流场特征上表现出高精度的超分辨率性能，优于传统插值方法。

Conclusion: SR-GT框架为复杂几何和非均匀网格上的反应流场超分辨率重建提供了有效的数据驱动解决方案。

Abstract: Super-resolution flow reconstruction using state-of-the-art data-driven techniques is valuable for a variety of applications, such as subgrid/subfilter closure modeling, accelerating spatiotemporal forecasting, data compression, and serving as an upscaling tool for sparse experimental measurements. In the present work, a first-of-its-kind multiscale graph transformer approach is developed for mesh-based super-resolution (SR-GT) of reacting flows. The novel data-driven modeling paradigm leverages a graph-based flow-field representation compatible with complex geometries and non-uniform/unstructured grids. Further, the transformer backbone captures long-range dependencies between different parts of the low-resolution flow-field, identifies important features, and then generates the super-resolved flow-field that preserves those features at a higher resolution. The performance of SR-GT is demonstrated in the context of spectral-element-discretized meshes for a challenging test problem of 2D detonation propagation within a premixed hydrogen-air mixture exhibiting highly complex multiscale reacting flow behavior. The SR-GT framework utilizes a unique element-local (+ neighborhood) graph representation for the coarse input, which is then tokenized before being processed by the transformer component to produce the fine output. It is demonstrated that SR-GT provides high super-resolution accuracy for reacting flow-field features and superior performance compared to traditional interpolation-based SR schemes.

</details>


### [112] [Improving Graph Embeddings in Machine Learning Using Knowledge Completion with Validation in a Case Study on COVID-19 Spread](https://arxiv.org/abs/2511.12071)
*Rosario Napoli,Gabriele Morabito,Antonio Celesti,Massimo Villari,Maria Fazio*

Main category: cs.LG

TL;DR: 提出了一种集成知识补全阶段的图机器学习管道，通过建模传递关系的隐藏连接来揭示稀疏数据集中的潜在语义，从而重新定义图表示质量。


<details>
  <summary>Details</summary>
Motivation: 现有的图嵌入方法主要基于显式拓扑和特征，可能错过稀疏数据集中隐藏的关键隐式知识，这会影响图结构及其表示质量。

Method: 在嵌入生成前加入知识补全阶段，专注于传递关系，使用基于衰减的推理函数建模隐藏连接，重塑图拓扑结构，影响GraphSAGE和Node2Vec中的嵌入动态和聚合过程。

Result: 实验表明该管道显著改变了嵌入空间的几何结构，证明知识补全阶段不仅是简单的丰富，而是重新定义图表示质量的变革性步骤。

Conclusion: 集成知识补全的图机器学习管道能够有效揭示数据集中隐藏的语义信息，显著提升图表示的质量和表达能力。

Abstract: The rise of graph-structured data has driven major advances in Graph Machine Learning (GML), where graph embeddings (GEs) map features from Knowledge Graphs (KGs) into vector spaces, enabling tasks like node classification and link prediction. However, since GEs are derived from explicit topology and features, they may miss crucial implicit knowledge hidden in seemingly sparse datasets, affecting graph structure and their representation. We propose a GML pipeline that integrates a Knowledge Completion (KC) phase to uncover latent dataset semantics before embedding generation. Focusing on transitive relations, we model hidden connections with decay-based inference functions, reshaping graph topology, with consequences on embedding dynamics and aggregation processes in GraphSAGE and Node2Vec. Experiments show that our GML pipeline significantly alters the embedding space geometry, demonstrating that its introduction is not just a simple enrichment but a transformative step that redefines graph representation quality.

</details>


### [113] [Treatment Stitching with Schrödinger Bridge for Enhancing Offline Reinforcement Learning in Adaptive Treatment Strategies](https://arxiv.org/abs/2511.12075)
*Dong-Hee Shin,Deok-Joong Lee,Young-Han Son,Tae-Eui Kam*

Main category: cs.LG

TL;DR: 提出了TreatStitch框架，通过智能拼接现有治疗数据片段来生成临床有效的治疗轨迹，解决离线强化学习在临床数据稀缺情况下的性能限制问题。


<details>
  <summary>Details</summary>
Motivation: 自适应治疗策略需要个性化动态调整治疗，但传统强化学习的在线试错机制在临床环境中不可行，而离线强化学习又受限于数据稀缺问题。

Method: TreatStitch框架通过识别不同轨迹中相似的中间患者状态并拼接相应片段，对于不相似的状态则使用薛定谔桥方法生成平滑的桥接轨迹来连接。

Result: 在多个治疗数据集上的实验表明，TreatStitch能有效提升离线强化学习的性能。

Conclusion: TreatStitch通过数据增强生成临床有效的合成轨迹，为离线强化学习优化自适应治疗策略提供了可行方案，同时保持了临床有效性。

Abstract: Adaptive treatment strategies (ATS) are sequential decision-making processes that enable personalized care by dynamically adjusting treatment decisions in response to evolving patient symptoms. While reinforcement learning (RL) offers a promising approach for optimizing ATS, its conventional online trial-and-error learning mechanism is not permissible in clinical settings due to risks of harm to patients. Offline RL tackles this limitation by learning policies exclusively from historical treatment data, but its performance is often constrained by data scarcity-a pervasive challenge in clinical domains. To overcome this, we propose Treatment Stitching (TreatStitch), a novel data augmentation framework that generates clinically valid treatment trajectories by intelligently stitching segments from existing treatment data. Specifically, TreatStitch identifies similar intermediate patient states across different trajectories and stitches their respective segments. Even when intermediate states are too dissimilar to stitch directly, TreatStitch leverages the Schrödinger bridge method to generate smooth and energy-efficient bridging trajectories that connect dissimilar states. By augmenting these synthetic trajectories into the original dataset, offline RL can learn from a more diverse dataset, thereby improving its ability to optimize ATS. Extensive experiments across multiple treatment datasets demonstrate the effectiveness of TreatStitch in enhancing offline RL performance. Furthermore, we provide a theoretical justification showing that TreatStitch maintains clinical validity by avoiding out-of-distribution transitions.

</details>


### [114] [SenseRay-3D: Generalizable and Physics-Informed Framework for End-to-End Indoor Propagation Modeling](https://arxiv.org/abs/2511.12092)
*Yu Zheng,Kezhi Wang,Wenji Xi,Gang Yu,Jiming Chen,Jie Zhang*

Main category: cs.LG

TL;DR: SenseRay-3D是一个基于物理的端到端框架，直接从RGB-D扫描预测3D路径损耗热图，无需显式几何重建或材料标注，实现了实时推理和良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有室内无线电传播建模方法依赖人工几何和材料属性建模，导致可扩展性和效率有限。

Method: 构建感知驱动的体素化场景表示，联合编码占用率、电磁材料特性和发射器-接收器几何信息，使用SwinUNETR神经网络推断环境路径损耗。

Result: 在未见环境中实现4.27 dB的平均绝对误差，支持每样本217 ms的实时推理。

Conclusion: SenseRay-3D为感知驱动、可泛化且物理一致的室内传播建模开辟了新途径，相比EM DeepRay框架有重大突破。

Abstract: Modeling indoor radio propagation is crucial for wireless network planning and optimization. However, existing approaches often rely on labor-intensive manual modeling of geometry and material properties, resulting in limited scalability and efficiency. To overcome these challenges, this paper presents SenseRay-3D, a generalizable and physics-informed end-to-end framework that predicts three-dimensional (3D) path-loss heatmaps directly from RGB-D scans, thereby eliminating the need for explicit geometry reconstruction or material annotation. The proposed framework builds a sensing-driven voxelized scene representation that jointly encodes occupancy, electromagnetic material characteristics, and transmitter-receiver geometry, which is processed by a SwinUNETR-based neural network to infer environmental path-loss relative to free-space path-loss. A comprehensive synthetic indoor propagation dataset is further developed to validate the framework and to serve as a standardized benchmark for future research. Experimental results show that SenseRay-3D achieves a mean absolute error of 4.27 dB on unseen environments and supports real-time inference at 217 ms per sample, demonstrating its scalability, efficiency, and physical consistency. SenseRay-3D paves a new path for sense-driven, generalizable, and physics-consistent modeling of indoor propagation, marking a major leap beyond our pioneering EM DeepRay framework.

</details>


### [115] [To Align or Not to Align: Strategic Multimodal Representation Alignment for Optimal Performance](https://arxiv.org/abs/2511.12121)
*Wanlong Fang,Tianle Zhang,Alvin Chan*

Main category: cs.LG

TL;DR: 本文通过可控对比学习模块系统研究显式对齐对多模态学习的影响，发现最优对齐强度取决于模态间冗余度，为显式对齐应用提供实用指导。


<details>
  <summary>Details</summary>
Motivation: 传统多模态学习假设表征对齐总是有益的，但缺乏对显式对齐直接影响的系统研究。本文旨在探究在不同模态信息结构下，显式对齐如何影响模型性能和表征对齐。

Method: 引入可控对比学习模块，在训练过程中精确操控对齐强度，探究显式对齐在何时改善或阻碍性能。

Result: 在合成和真实数据集上的实验表明，显式对齐对单模态模型性能的影响与数据特征相关：最优对齐水平取决于不同模态间的冗余量。

Conclusion: 识别出最优对齐强度，能在混合信息分布中平衡模态特定信号和共享冗余，为显式对齐的应用时机和方法提供实用指导。

Abstract: Multimodal learning often relies on aligning representations across modalities to enable effective information integration, an approach traditionally assumed to be universally beneficial. However, prior research has primarily taken an observational approach, examining naturally occurring alignment in multimodal data and exploring its correlation with model performance, without systematically studying the direct effects of explicitly enforced alignment between representations of different modalities. In this work, we investigate how explicit alignment influences both model performance and representation alignment under different modality-specific information structures. Specifically, we introduce a controllable contrastive learning module that enables precise manipulation of alignment strength during training, allowing us to explore when explicit alignment improves or hinders performance. Our results on synthetic and real datasets under different data characteristics show that the impact of explicit alignment on the performance of unimodal models is related to the characteristics of the data: the optimal level of alignment depends on the amount of redundancy between the different modalities. We identify an optimal alignment strength that balances modality-specific signals and shared redundancy in the mixed information distributions. This work provides practical guidance on when and how explicit alignment should be applied to achieve optimal unimodal encoder performance.

</details>


### [116] [Dynamic Anomaly Identification in Accounting Transactions via Multi-Head Self-Attention Networks](https://arxiv.org/abs/2511.12122)
*Yi Wang,Ruoyi Fang,Anzhuo Xie,Hanrui Feng,Jianlin Lai*

Main category: cs.LG

TL;DR: 提出基于Transformer的实时动态异常检测方法，用于会计交易中的异常行为识别，在复杂交易环境中解决隐藏异常行为和高时效性要求的问题。


<details>
  <summary>Details</summary>
Motivation: 解决会计交易中隐藏异常行为检测的挑战，满足复杂交易环境下的高时效性要求，为智能金融风控和审计提供支持。

Method: 将多维会计交易记录建模为时间序列矩阵，使用嵌入层和位置编码进行低维映射，构建多头自注意力序列建模结构捕获全局依赖关系，结合前馈层和正则化策略实现深度特征表示和异常概率估计。

Result: 在公开数据集上的实验表明，该方法在AUC、F1-Score、精确率和召回率等指标上优于基线模型，在不同环境条件和数据扰动下保持稳定性能。

Conclusion: 基于Transformer的框架在会计交易动态异常检测中具有适用性和优势，为智能金融风控和审计提供了方法论支持。

Abstract: This study addresses the problem of dynamic anomaly detection in accounting transactions and proposes a real-time detection method based on a Transformer to tackle the challenges of hidden abnormal behaviors and high timeliness requirements in complex trading environments. The approach first models accounting transaction data by representing multi-dimensional records as time-series matrices and uses embedding layers and positional encoding to achieve low-dimensional mapping of inputs. A sequence modeling structure with multi-head self-attention is then constructed to capture global dependencies and aggregate features from multiple perspectives, thereby enhancing the ability to detect abnormal patterns. The network further integrates feed-forward layers and regularization strategies to achieve deep feature representation and accurate anomaly probability estimation. To validate the effectiveness of the method, extensive experiments were conducted on a public dataset, including comparative analysis, hyperparameter sensitivity tests, environmental sensitivity tests, and data sensitivity tests. Results show that the proposed method outperforms baseline models in AUC, F1-Score, Precision, and Recall, and maintains stable performance under different environmental conditions and data perturbations. These findings confirm the applicability and advantages of the Transformer-based framework for dynamic anomaly detection in accounting transactions and provide methodological support for intelligent financial risk control and auditing.

</details>


### [117] [HCPO: Hierarchical Conductor-Based Policy Optimization in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.12123)
*Zejiao Liu,Junqi Tu,Yitian Hong,Luolin Xiong,Yaochu Jin,Yang Tang,Fangfei Li*

Main category: cs.LG

TL;DR: 提出了基于指挥者的联合策略框架HCPO，通过协调多智能体探索来提升合作强化学习性能，在多个基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有合作多智能体强化学习方法通常通过独立智能体探索来更新联合策略，缺乏智能体间的协调，限制了联合策略的表达能力和探索效率。

Method: 提出基于指挥者的联合策略框架，开发分层指挥者策略优化算法HCPO，通过指挥者协调智能体探索和策略更新，同时保持集中训练去中心化执行的优势。

Result: 在StarCraftII多智能体挑战、多智能体MuJoCo和多智能体粒子环境三个基准测试中，HCPO在合作效率和稳定性方面均优于竞争性MARL基线方法。

Conclusion: HCPO通过指挥者协调机制有效提升了多智能体强化学习的联合策略表达能力和探索效率，理论分析保证了策略优化的单调性，实验验证了其优越性能。

Abstract: In cooperative Multi-Agent Reinforcement Learning (MARL), efficient exploration is crucial for optimizing the performance of joint policy. However, existing methods often update joint policies via independent agent exploration, without coordination among agents, which inherently constrains the expressive capacity and exploration of joint policies. To address this issue, we propose a conductor-based joint policy framework that directly enhances the expressive capacity of joint policies and coordinates exploration. In addition, we develop a Hierarchical Conductor-based Policy Optimization (HCPO) algorithm that instructs policy updates for the conductor and agents in a direction aligned with performance improvement. A rigorous theoretical guarantee further establishes the monotonicity of the joint policy optimization process. By deploying local conductors, HCPO retains centralized training benefits while eliminating inter-agent communication during execution. Finally, we evaluate HCPO on three challenging benchmarks: StarCraftII Multi-agent Challenge, Multi-agent MuJoCo, and Multi-agent Particle Environment. The results indicate that HCPO outperforms competitive MARL baselines regarding cooperative efficiency and stability.

</details>


### [118] [FairGSE: Fairness-Aware Graph Neural Network without High False Positive Rates](https://arxiv.org/abs/2511.12132)
*Zhenqiang Ye,Jinjie Lu,Tianlong Gu,Fengrui Hao,Xuemin Wang*

Main category: cs.LG

TL;DR: 本文提出FairGSE框架，通过最大化二维结构熵来改善图神经网络的公平性，同时显著降低假阳性率。


<details>
  <summary>Details</summary>
Motivation: 现有公平感知的GNN在追求公平性指标时忽视了模型预测负标签的能力，导致假阳性率极高，在高风险场景中产生负面影响。

Method: 提出FairGSE框架，通过最大化二维结构熵来改善公平性，同时关注假阳性问题。

Result: 在多个真实数据集上的实验表明，FairGSE相比最先进的公平感知GNN将假阳性率降低了39%，同时保持可比的公平性改进。

Conclusion: 在改善公平性的同时应仔细校准分类性能，而不仅仅是约束准确率损失；FairGSE框架有效解决了公平性与假阳性率之间的平衡问题。

Abstract: Graph neural networks (GNNs) have emerged as the mainstream paradigm for graph representation learning due to their effective message aggregation. However, this advantage also amplifies biases inherent in graph topology, raising fairness concerns. Existing fairness-aware GNNs provide satisfactory performance on fairness metrics such as Statistical Parity and Equal Opportunity while maintaining acceptable accuracy trade-offs. Unfortunately, we observe that this pursuit of fairness metrics neglects the GNN's ability to predict negative labels, which renders their predictions with extremely high False Positive Rates (FPR), resulting in negative effects in high-risk scenarios. To this end, we advocate that classification performance should be carefully calibrated while improving fairness, rather than simply constraining accuracy loss. Furthermore, we propose Fair GNN via Structural Entropy (\textbf{FairGSE}), a novel framework that maximizes two-dimensional structural entropy (2D-SE) to improve fairness without neglecting false positives. Experiments on several real-world datasets show FairGSE reduces FPR by 39\% vs. state-of-the-art fairness-aware GNNs, with comparable fairness improvement.

</details>


### [119] [Fusion-ResNet: A Lightweight multi-label NILM Model Using PCA-ICA Feature Fusion](https://arxiv.org/abs/2511.12139)
*Sahar Moghimian Hoosh,Ilia Kamyshev,Henni Ouerdane*

Main category: cs.LG

TL;DR: 提出了一种基于ICA和PCA特征融合的轻量级神经网络框架，用于非侵入式负荷监测(NILM)分类任务，在保持高精度的同时显著减少训练和推理时间。


<details>
  <summary>Details</summary>
Motivation: 解决实际NILM部署中的过拟合、模型泛化能力差以及多设备同时运行时的分解困难等挑战。

Method: 构建端到端框架，包含高频标记数据、ICA与PCA特征融合的特征提取方法，以及轻量级多标签分类网络Fusion-ResNet。

Result: 相比现有最优NILM分类器，在平均F1分数和各设备F1分数上表现更好，同时训练和推理时间显著减少，在15个设备同时运行时仍保持相对鲁棒性。

Conclusion: 提出的特征融合方法和轻量级网络架构有效提升了NILM分类性能，为实际部署提供了可行的解决方案。

Abstract: Non-intrusive load monitoring (NILM) is an advanced load monitoring technique that uses data-driven algorithms to disaggregate the total power consumption of a household into the consumption of individual appliances. However, real-world NILM deployment still faces major challenges, including overfitting, low model generalization, and disaggregating a large number of appliances operating at the same time. To address these challenges, this work proposes an end-to-end framework for the NILM classification task, which consists of high-frequency labeled data, a feature extraction method, and a lightweight neural network. Within this framework, we introduce a novel feature extraction method that fuses Independent Component Analysis (ICA) and Principal Component Analysis (PCA) features. Moreover, we propose a lightweight architecture for multi-label NILM classification (Fusion-ResNet). The proposed feature-based model achieves a higher $F1$ score on average and across different appliances compared to state-of-the-art NILM classifiers while minimizing the training and inference time. Finally, we assessed the performance of our model against baselines with a varying number of simultaneously active devices. Results demonstrate that Fusion-ResNet is relatively robust to stress conditions with up to 15 concurrently active appliances.

</details>


### [120] [Variation-Bounded Loss for Noise-Tolerant Learning](https://arxiv.org/abs/2511.12143)
*Jialiang Wang,Xiong Zhou,Xianming Liu,Gangfeng Hu,Deming Zhai,Junjun Jiang,Haoliang Li*

Main category: cs.LG

TL;DR: 提出了一种新的鲁棒损失函数属性——变异比，并基于此构建了变异有界损失函数家族，通过理论分析和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 缓解噪声标签对监督学习的负面影响是一个长期存在的问题，鲁棒损失函数是解决该问题的常用方法。

Method: 引入变异比作为损失函数鲁棒性的新属性，提出变异有界损失函数家族，并对几种常用损失函数进行变异有界重构。

Result: 在多个数据集上的实验证明了该方法的有效性和灵活性。

Conclusion: 变异比为放松对称条件提供了可行方法，为实现非对称条件提供了更简洁的途径，变异有界损失函数在噪声标签环境下具有良好鲁棒性。

Abstract: Mitigating the negative impact of noisy labels has been aperennial issue in supervised learning. Robust loss functions have emerged as a prevalent solution to this problem. In this work, we introduce the Variation Ratio as a novel property related to the robustness of loss functions, and propose a new family of robust loss functions, termed Variation-Bounded Loss (VBL), which is characterized by a bounded variation ratio. We provide theoretical analyses of the variation ratio, proving that a smaller variation ratio would lead to better robustness. Furthermore, we reveal that the variation ratio provides a feasible method to relax the symmetric condition and offers a more concise path to achieve the asymmetric condition. Based on the variation ratio, we reformulate several commonly used loss functions into a variation-bounded form for practical applications. Positive experiments on various datasets exhibit the effectiveness and flexibility of our approach.

</details>


### [121] [Finding Time Series Anomalies using Granular-ball Vector Data Description](https://arxiv.org/abs/2511.12147)
*Lifeng Shen,Liang Peng,Ruiwen Liu,Shuyin Xia,Yi Liu*

Main category: cs.LG

TL;DR: GBOC是一种基于粒度球向量数据描述(GVDD)的新颖异常检测方法，通过密度引导的分层分裂过程生成紧凑的高密度区域表示，在训练时对齐样本与最近粒度球中心，推理时基于距离计算异常分数。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测方法（如最近邻和聚类）在复杂时间序列场景中依赖刚性假设（如预定义邻居数或聚类数），这些假设经常失效，需要更灵活的数据自适应方法。

Method: 提出粒度球单类网络(GBOC)，使用GVDD将潜在空间划分为由粒度球表示的紧凑高密度区域，通过密度引导分层分裂生成并去除噪声结构，每个粒度球作为局部正常行为的原型。

Result: 实验验证了该方法的有效性和优越性，能够处理时间序列异常检测的挑战，通过关注密集高质量区域并显著减少原型数量，实现了鲁棒性和效率。

Conclusion: GBOC通过数据自适应的粒度球表示，在保持局部拓扑结构的同时提供了高效鲁棒的异常检测解决方案，特别适用于动态非线性时间序列数据。

Abstract: Modeling normal behavior in dynamic, nonlinear time series data is challenging for effective anomaly detection. Traditional methods, such as nearest neighbor and clustering approaches, often depend on rigid assumptions, such as a predefined number of reliable neighbors or clusters, which frequently break down in complex temporal scenarios. To address these limitations, we introduce the Granular-ball One-Class Network (GBOC), a novel approach based on a data-adaptive representation called Granular-ball Vector Data Description (GVDD). GVDD partitions the latent space into compact, high-density regions represented by granular-balls, which are generated through a density-guided hierarchical splitting process and refined by removing noisy structures. Each granular-ball serves as a prototype for local normal behavior, naturally positioning itself between individual instances and clusters while preserving the local topological structure of the sample set. During training, GBOC improves the compactness of representations by aligning samples with their nearest granular-ball centers. During inference, anomaly scores are computed based on the distance to the nearest granular-ball. By focusing on dense, high-quality regions and significantly reducing the number of prototypes, GBOC delivers both robustness and efficiency in anomaly detection. Extensive experiments validate the effectiveness and superiority of the proposed method, highlighting its ability to handle the challenges of time series anomaly detection.

</details>


### [122] [Open Banking Foundational Model: Learning Language Representations from Few Financial Transactions](https://arxiv.org/abs/2511.12154)
*Gustavo Polleti,Marlesson Santana,Eduardo Fontes*

Main category: cs.LG

TL;DR: 提出了一个多模态金融交易基础模型，整合结构属性和非结构化文本描述，在数据稀缺的开放银行场景中表现优异，是北美首个大规模跨机构研究。


<details>
  <summary>Details</summary>
Motivation: 解决金融交易中传统特征工程和离散事件序列方法的局限性，特别是在数据稀缺的开放银行场景下，需要能够整合多种数据类型的统一表示方法。

Method: 采用掩码语言建模方法处理交易序列，构建多模态基础模型，将结构化属性和非结构化文本描述整合为统一表示。

Result: 模型在多个金融任务中超越传统方法，特别是在数据稀缺场景下表现突出，证明了多模态表示能够跨地理区域和金融机构泛化。

Conclusion: 自监督模型在金融应用领域具有巨大潜力，可推动从欺诈预防、信用风险到客户洞察等多个金融应用的发展。

Abstract: We introduced a multimodal foundational model for financial transactions that integrates both structured attributes and unstructured textual descriptions into a unified representation. By adapting masked language modeling to transaction sequences, we demonstrated that our approach not only outperforms classical feature engineering and discrete event sequence methods but is also particularly effective in data-scarce Open Banking scenarios. To our knowledge, this is the first large-scale study across thousands of financial institutions in North America, providing evidence that multimodal representations can generalize across geographies and institutions. These results highlight the potential of self-supervised models to advance financial applications ranging from fraud prevention and credit risk to customer insights

</details>


### [123] [Rethinking Deep Alignment Through The Lens Of Incomplete Learning](https://arxiv.org/abs/2511.12155)
*Thong Bach,Dung Nguyen,Thao Minh Le,Truyen Tran*

Main category: cs.LG

TL;DR: 论文揭示了自回归训练中位置依赖的梯度弱化导致安全学习不完整的问题，提出了基于基础偏好标记的针对性补全方法，显著提升了对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管进行了广泛的安全对齐，大语言模型仍存在系统性对抗攻击漏洞。研究发现自回归训练中的位置依赖梯度弱化导致安全学习不完整，特别是在后续响应区域。

Method: 引入基础偏好标记作为计算指标，开发了针对性补全方法，通过自适应惩罚和混合教师蒸馏来补全未充分训练的区域。

Result: 在Llama和Qwen模型系列上的实验评估显示，对抗攻击成功率降低了48-98%，同时保持了通用能力。

Conclusion: 这项工作为安全对齐方法的基本局限性建立了机制性理解，并提供了实用的解决方案。

Abstract: Large language models exhibit systematic vulnerabilities to adversarial attacks despite extensive safety alignment. We provide a mechanistic analysis revealing that position-dependent gradient weakening during autoregressive training creates signal decay, leading to incomplete safety learning where safety training fails to transform model preferences in later response regions fully. We introduce base-favored tokens -- vocabulary elements where base models assign higher probability than aligned models -- as computational indicators of incomplete safety learning and develop a targeted completion method that addresses undertrained regions through adaptive penalties and hybrid teacher distillation. Experimental evaluation across Llama and Qwen model families demonstrates dramatic improvements in adversarial robustness, with 48--98% reductions in attack success rates while preserving general capabilities. These results establish both a mechanistic understanding and practical solutions for fundamental limitations in safety alignment methodologies.

</details>


### [124] [Data-Efficient Self-Supervised Algorithms for Fine-Grained Birdsong Analysis](https://arxiv.org/abs/2511.12158)
*Houtan Ghaffari,Lukas Rauch,Paul Devos*

Main category: cs.LG

TL;DR: 提出了一种轻量级神经网络架构Residual-MLP-RNN和三阶段训练流程，用于在极低标注数据情况下实现鸟类鸣声的音节自动标注，并在复杂的金丝雀鸣声上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 鸟类鸣声研究需要精确的音节级标注，但人工标注成本高昂，因此需要开发数据高效的自动化方法来降低标注成本。

Method: 使用Residual-MLP-RNN架构，采用三阶段训练：1)无监督预训练（掩码预测和在线聚类）；2)带数据增强的有监督训练；3)与下游任务对齐的半监督后训练。

Result: 在标注数据极少的场景下，该方法对复杂的金丝雀鸣声实现了有效的音节检测，证明了方法的鲁棒性和数据效率。

Conclusion: 该方法为鸟类鸣声分析提供了一种数据高效的解决方案，特别适用于标注困难的复杂鸣声，同时自监督嵌入在无监督分析中展现了潜力。

Abstract: Many bioacoustics, neuroscience, and linguistics research utilize birdsongs as proxy models to acquire knowledge in diverse areas. Developing models generally requires precisely annotated data at the level of syllables. Hence, automated and data-efficient methods that reduce annotation costs are in demand. This work presents a lightweight, yet performant neural network architecture for birdsong annotation called Residual-MLP-RNN. Then, it presents a robust three-stage training pipeline for developing reliable deep birdsong syllable detectors with minimal expert labor. The first stage is self-supervised learning from unlabeled data. Two of the most successful pretraining paradigms are explored, namely, masked prediction and online clustering. The second stage is supervised training with effective data augmentations to create a robust model for frame-level syllable detection. The third stage is semi-supervised post-training, which leverages the unlabeled data again. However, unlike the initial phase, this time it is aligned with the downstream task. The performance of this data-efficient approach is demonstrated for the complex song of the Canary in extreme label-scarcity scenarios. Canary has one of the most difficult songs to annotate, which implicitly validates the method for other birds. Finally, the potential of self-supervised embeddings is assessed for linear probing and unsupervised birdsong analysis.

</details>


### [125] [TSGDiff: Rethinking Synthetic Time Series Generation from a Pure Graph Perspective](https://arxiv.org/abs/2511.12174)
*Lifeng Shen,Xuyang Li,Lele Long*

Main category: cs.LG

TL;DR: TSGDiff是一个基于图神经网络的扩散模型框架，通过将时间序列表示为动态图来生成高质量合成时间序列数据，并提出Topo-FID评分来评估结构保真度。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在数据生成方面表现出色，但生成时间序列数据仍具挑战性，因为需要捕捉复杂的时间依赖性和结构模式。

Method: 将时间序列表示为基于傅里叶谱特征和时间依赖性构建的动态图，使用图神经网络编码器-解码器架构构建潜在空间，使扩散过程能够有效建模时间序列的结构表示分布。

Result: 在真实世界数据集上的实验表明，TSGDiff能够生成高质量的合成时间序列数据，忠实地保留时间依赖性和结构完整性。

Conclusion: TSGDiff通过图视角重新思考时间序列生成，提出了Topo-FID评分来评估结构相似性，推动了合成时间序列生成领域的发展。

Abstract: Diffusion models have shown great promise in data generation, yet generating time series data remains challenging due to the need to capture complex temporal dependencies and structural patterns. In this paper, we present \textit{TSGDiff}, a novel framework that rethinks time series generation from a graph-based perspective. Specifically, we represent time series as dynamic graphs, where edges are constructed based on Fourier spectrum characteristics and temporal dependencies. A graph neural network-based encoder-decoder architecture is employed to construct a latent space, enabling the diffusion process to model the structural representation distribution of time series effectively. Furthermore, we propose the Topological Structure Fidelity (Topo-FID) score, a graph-aware metric for assessing the structural similarity of time series graph representations. Topo-FID integrates two sub-metrics: Graph Edit Similarity, which quantifies differences in adjacency matrices, and Structural Entropy Similarity, which evaluates the entropy of node degree distributions. This comprehensive metric provides a more accurate assessment of structural fidelity in generated time series. Experiments on real-world datasets demonstrate that \textit{TSGDiff} generates high-quality synthetic time series data generation, faithfully preserving temporal dependencies and structural integrity, thereby advancing the field of synthetic time series generation.

</details>


### [126] [Understanding InfoNCE: Transition Probability Matrix Induced Feature Clustering](https://arxiv.org/abs/2511.12180)
*Ge Cheng,Shuo Wang,Yun Zhang*

Main category: cs.LG

TL;DR: 本文提出了SC-InfoNCE损失函数，通过引入可调收敛目标来灵活控制特征相似性对齐，在图像、图结构和文本任务上实现了稳定且强大的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管对比学习在无监督表示学习中取得了经验性成功，但InfoNCE的理论基础仍然有限。本文旨在深入理解InfoNCE的理论机制，并基于此改进对比学习目标。

Method: 引入显式特征空间建模样本的增强视图，使用转移概率矩阵捕捉数据增强动态。提出SC-InfoNCE损失函数，通过缩放目标矩阵来灵活控制特征相似性对齐。

Result: 在图像、图结构和文本任务的基准数据集上的实验表明，SC-InfoNCE在不同领域都实现了强大且可靠的性能。

Conclusion: SC-InfoNCE通过引入可调收敛目标，使训练目标能更好地匹配下游数据的统计特性，为对比学习提供了更灵活和有效的优化框架。

Abstract: Contrastive learning has emerged as a cornerstone of unsupervised representation learning across vision, language, and graph domains, with InfoNCE as its dominant objective. Despite its empirical success, the theoretical underpinnings of InfoNCE remain limited. In this work, we introduce an explicit feature space to model augmented views of samples and a transition probability matrix to capture data augmentation dynamics. We demonstrate that InfoNCE optimizes the probability of two views sharing the same source toward a constant target defined by this matrix, naturally inducing feature clustering in the representation space. Leveraging this insight, we propose Scaled Convergence InfoNCE (SC-InfoNCE), a novel loss function that introduces a tunable convergence target to flexibly control feature similarity alignment. By scaling the target matrix, SC-InfoNCE enables flexible control over feature similarity alignment, allowing the training objective to better match the statistical properties of downstream data. Experiments on benchmark datasets, including image, graph, and text tasks, show that SC-InfoNCE consistently achieves strong and reliable performance across diverse domains.

</details>


### [127] [Scaling Law Analysis in Federated Learning: How to Select the Optimal Model Size?](https://arxiv.org/abs/2511.12188)
*Xuanyu Chen,Nan Yang,Shuai Wang,Dong Yuan*

Main category: cs.LG

TL;DR: 本文研究了联邦学习场景下的大模型缩放问题，通过理论分析和实验验证发现：在总计算量不变时，最优模型大小与客户端数量呈负幂律关系；切换到联邦学习会降低模型泛化性能上限；联邦场景下的最优模型大小估计应基于客户端的平均训练计算量。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模不断扩大，高质量训练数据日益枯竭，联邦学习成为利用边缘设备数据同时保护隐私的可行方案。但联邦学习中的分布式训练数据集给大模型缩放带来了挑战，这一领域尚未得到充分探索。

Method: 推导了联邦学习设置中随机算法训练模型的泛化误差的PAC-Bayes上界，通过求解最小化该上界的解析解来量化分布式训练数据对最优模型大小的影响，并在不同模型、网络设置和数据集上进行了广泛的训练实验验证。

Result: 理论结果表明：当总训练计算量不变时，最优模型大小与客户端数量呈负幂律关系；切换到联邦学习会降低模型通过训练可达到的泛化性能上限；联邦场景下的最优模型大小估计应依赖于客户端的平均训练计算量。

Conclusion: 本文填补了联邦学习中大模型缩放研究的空白，为将传统模型缩放经验推广到联邦学习场景提供了理论依据和实践指导，揭示了分布式数据环境对模型最优规模的重要影响。

Abstract: The recent success of large language models (LLMs) has sparked a growing interest in training large-scale models. As the model size continues to scale, concerns are growing about the depletion of high-quality, well-curated training data. This has led practitioners to explore training approaches like Federated Learning (FL), which can leverage the abundant data on edge devices while maintaining privacy. However, the decentralization of training datasets in FL introduces challenges to scaling large models, a topic that remains under-explored. This paper fills this gap and provides qualitative insights on generalizing the previous model scaling experience to federated learning scenarios. Specifically, we derive a PAC-Bayes (Probably Approximately Correct Bayesian) upper bound for the generalization error of models trained with stochastic algorithms in federated settings and quantify the impact of distributed training data on the optimal model size by finding the analytic solution of model size that minimizes this bound. Our theoretical results demonstrate that the optimal model size has a negative power law relationship with the number of clients if the total training compute is unchanged. Besides, we also find that switching to FL with the same training compute will inevitably reduce the upper bound of generalization performance that the model can achieve through training, and that estimating the optimal model size in federated scenarios should depend on the average training compute across clients. Furthermore, we also empirically validate the correctness of our results with extensive training runs on different models, network settings, and datasets.

</details>


### [128] [Evaluation of Multi- and Single-objective Learning Algorithms for Imbalanced Data](https://arxiv.org/abs/2511.12191)
*Szymon Wojciechowski,Michał Woźniak*

Main category: cs.LG

TL;DR: 提出了一种新的可靠方法来评估基于多目标优化算法与返回单一解的方法，重点是比较算法性能而非学习过程。


<details>
  <summary>Details</summary>
Motivation: 在机器学习中，多目标优化算法会产生帕累托前沿，而传统方法返回单一解，现有评估方法无法可靠比较这两类算法。

Method: 提出新的评估方法，能够比较返回帕累托前沿的多目标优化算法与返回单一解的方法，并考虑用户偏好选择帕累托前沿中的解。

Result: 该方法为算法比较提供了可靠框架，所选算法仅用于说明方法理解。

Conclusion: 填补了分类器评估方法学的显著空白，为多目标优化算法与传统单一解方法的可靠比较提供了解决方案。

Abstract: Many machine learning tasks aim to find models that work well not for a single, but for a group of criteria, often opposing ones. One such example is imbalanced data classification, where, on the one hand, we want to achieve the best possible classification quality for data from the minority class without degrading the classification quality of the majority class. One solution is to propose an aggregate learning criterion and reduce the multi-objective learning task to a single-criteria optimization problem. Unfortunately, such an approach is characterized by ambiguity of interpretation since the value of the aggregated criterion does not indicate the value of the component criteria. Hence, there are more and more proposals for algorithms based on multi-objective optimization (MOO), which can simultaneously optimize multiple criteria. However, such an approach results in a set of multiple non-dominated solutions (Pareto front). The selection of a single solution from the Pareto front is a challenge itself, and much attention is paid to the issue of how to select it considering user preferences, as well as how to compare solutions returned by different MOO algorithms among themselves. Thus, a significant gap has been identified in the classifier evaluation methodology, i.e., how to reliably compare methods returning single solutions with algorithms returning solutions in the form of Pareto fronts.
  To fill the aforementioned gap, this article proposes a new, reliable way of evaluating algorithms based on multi-objective algorithms with methods that return single solutions while pointing out solutions from a Pareto front tailored to the user's preferences. This work focuses only on algorithm comparison, not their learning. The algorithms selected for this study are illustrative to help understand the proposed approach.

</details>


### [129] [MPD-SGR: Robust Spiking Neural Networks with Membrane Potential Distribution-Driven Surrogate Gradient Regularization](https://arxiv.org/abs/2511.12199)
*Runhao Jiang,Chengzhi Jiang,Rui Yan,Huajin Tang*

Main category: cs.LG

TL;DR: 本文研究了膜电位分布与替代梯度函数的相互作用对脉冲神经网络鲁棒性的影响，提出了一种基于膜电位分布的替代梯度正则化方法，能有效提升SNN对抗攻击的抵抗力。


<details>
  <summary>Details</summary>
Motivation: 替代梯度方法虽然提升了深度脉冲神经网络的性能，但也使其容易受到对抗攻击。目前对梯度幅值（反映模型对输入扰动的敏感性）的研究不足，而梯度幅值主要由膜电位分布与替代梯度函数的相互作用决定。

Method: 提出MPD-SGR方法，通过理论分析发现减少位于替代梯度函数梯度可用范围内的膜电位比例可以降低SNN对输入扰动的敏感性，并基于此设计了一种显式正则化膜电位分布的方法。

Result: 在多个图像分类基准测试和不同网络架构上的实验表明，MPD-SGR方法显著增强了SNN对抗扰动的鲁棒性，并在不同网络配置、替代梯度函数变体和脉冲编码方案中表现出良好的泛化能力。

Conclusion: 膜电位分布与替代梯度函数的相互作用是影响SNN鲁棒性的关键因素，MPD-SGR方法通过正则化这种相互作用能有效提升SNN的对抗鲁棒性。

Abstract: The surrogate gradient (SG) method has shown significant promise in enhancing the performance of deep spiking neural networks (SNNs), but it also introduces vulnerabilities to adversarial attacks. Although spike coding strategies and neural dynamics parameters have been extensively studied for their impact on robustness, the critical role of gradient magnitude, which reflects the model's sensitivity to input perturbations, remains underexplored. In SNNs, the gradient magnitude is primarily determined by the interaction between the membrane potential distribution (MPD) and the SG function. In this study, we investigate the relationship between the MPD and SG and its implications for improving the robustness of SNNs. Our theoretical analysis reveals that reducing the proportion of membrane potential lying within the gradient-available range of the SG function effectively mitigates the sensitivity of SNNs to input perturbations. Building upon this insight, we propose a novel MPD-driven surrogate gradient regularization (MPD-SGR) method, which enhances robustness by explicitly regularizing the MPD based on its interaction with the SG function. Extensive experiments across multiple image classification benchmarks and diverse network architectures confirm that the MPD-SGR method significantly enhances the resilience of SNNs to adversarial perturbations and exhibits strong generalizability across diverse network configurations, SG function variants, and spike encoding schemes.

</details>


### [130] [AlignTree: Efficient Defense Against LLM Jailbreak Attacks](https://arxiv.org/abs/2511.12217)
*Gil Goren,Shahar Katz,Lior Wolf*

Main category: cs.LG

TL;DR: AlignTree是一种高效防御机制，通过监控LLM激活并使用随机森林分类器检测有害内容，无需额外提示或辅助模型。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法要么计算成本高，要么容易被绕过，不适用于实际LLM系统。需要既鲁棒又计算高效的防御机制。

Method: 使用随机森林分类器监控LLM激活，基于拒绝方向（线性表示）和SVM信号（非线性特征）检测有害内容。

Result: 在多个LLM和基准测试中证明了AlignTree的高效性和鲁棒性。

Conclusion: AlignTree在保持最小计算开销的同时增强了模型对齐，为实际LLM系统提供了实用防御方案。

Abstract: Large Language Models (LLMs) are vulnerable to adversarial attacks that bypass safety guidelines and generate harmful content. Mitigating these vulnerabilities requires defense mechanisms that are both robust and computationally efficient. However, existing approaches either incur high computational costs or rely on lightweight defenses that can be easily circumvented, rendering them impractical for real-world LLM-based systems. In this work, we introduce the AlignTree defense, which enhances model alignment while maintaining minimal computational overhead. AlignTree monitors LLM activations during generation and detects misaligned behavior using an efficient random forest classifier. This classifier operates on two signals: (i) the refusal direction -- a linear representation that activates on misaligned prompts, and (ii) an SVM-based signal that captures non-linear features associated with harmful content. Unlike previous methods, AlignTree does not require additional prompts or auxiliary guard models. Through extensive experiments, we demonstrate the efficiency and robustness of AlignTree across multiple LLMs and benchmarks.

</details>


### [131] [Chicken Swarm Kernel Particle Filter: A Structured Rejuvenation Approach with KLD-Efficient Sampling](https://arxiv.org/abs/2511.12222)
*Hangshuo Tian*

Main category: cs.LG

TL;DR: 本文分析了粒子滤波中鸡群优化算法与KLD自适应采样之间的理论交互作用，发现CSO增强的粒子滤波在相同统计误差下需要更少的粒子数


<details>
  <summary>Details</summary>
Motivation: 理解基于群体智能的粒子重激活核与基于KLD的自适应采样之间的理论交互作用，目前这方面的理论理解还不充分

Method: 在简化建模框架下分析CSO重激活步骤对粒子集分布的影响，将CSO的适应度驱动更新近似为均方收缩，并应用Karamata不等式分析KLD采样中的期望箱占用

Result: 分析表明，在相同统计误差下，CSO增强的粒子滤波比标准粒子滤波需要更低的期望粒子数

Conclusion: 提供了一个可处理的理论框架来解释这些技术结合时观察到的计算效率，为设计更高效的自适应滤波器提供了起点

Abstract: Particle filters (PFs) are often combined with swarm intelligence (SI) algorithms, such as Chicken Swarm Optimization (CSO), for particle rejuvenation. Separately, Kullback--Leibler divergence (KLD) sampling is a common strategy for adaptively sizing the particle set. However, the theoretical interaction between SI-based rejuvenation kernels and KLD-based adaptive sampling is not yet fully understood.
  This paper investigates this specific interaction. We analyze, under a simplified modeling framework, the effect of the CSO rejuvenation step on the particle set distribution. We propose that the fitness-driven updates inherent in CSO can be approximated as a form of mean-square contraction. This contraction tends to produce a particle distribution that is more concentrated than that of a baseline PF, or in mathematical terms, a distribution that is plausibly more ``peaked'' in a majorization sense.
  By applying Karamata's inequality to the concave function that governs the expected bin occupancy in KLD-sampling, our analysis suggests a connection: under the stated assumptions, the CSO-enhanced PF (CPF) is expected to require a lower \emph{expected} particle count than the standard PF to satisfy the same statistical error bound. The goal of this study is not to provide a fully general proof, but rather to offer a tractable theoretical framework that helps to interpret the computational efficiency empirically observed when combining these techniques, and to provide a starting point for designing more efficient adaptive filters.

</details>


### [132] [SCI: An Equilibrium for Signal Intelligence](https://arxiv.org/abs/2511.12240)
*Vishal Joshua Meesala*

Main category: cs.LG

TL;DR: SCI是一个闭环控制理论框架，将可解释性建模为受调节状态，通过主动驱动手术精度SP(t)向目标值收敛，在人类增益预算下减少解释误差并提高稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统静态解释器在动态信号场景下解释不稳定且误差较大，需要一种能够主动调节解释质量的控制理论框架。

Method: SCI包含三个协调组件：可靠性加权的多尺度特征、知识引导的解释器、以及配备回滚和信任区域保护的Lyapunov引导控制器。

Result: 在生物医学、工业和环境领域，SCI将解释误差降低25-42%（平均38%），同时将SP方差从0.030降至0.011，AUC/F1仅比基线下降1-2个百分点。

Conclusion: 将可解释性建模为控制目标能够在多样化信号机制下产生更稳定、恢复更快且更可信的解释行为。

Abstract: We present SCI, a closed-loop, control-theoretic framework that models interpretability as a regulated state. SCI formalizes the interpretive error Delta SP and actively drives SP(t) in [0, 1] ("Surgical Precision") toward a target via a projected update on the parameters Theta under a human-gain budget. The framework operates through three coordinated components: (1) reliability-weighted, multiscale features P(t, s); (2) a knowledge-guided interpreter psi_Theta that emits traceable markers and rationales; and (3) a Lyapunov-guided controller equipped with rollback, trust-region safeguards, and a descent condition. Across biomedical (EEG/ECG/ICU), industrial (bearings/tool wear), and environmental (climate/seismic) domains, SCI reduces interpretive error by 25-42% (mean 38%, 95% confidence interval 22-43%) relative to static explainers while maintaining AUC/F1 within approximately 1-2 percentage points of baseline. SCI also reduces SP variance from 0.030 to 0.011, indicating substantially more stable explanations. Modeling interpretability as a control objective yields steadier, faster-recovering, and more trustworthy interpretive behavior across diverse signal regimes.

</details>


### [133] [Cross-view Joint Learning for Mixed-Missing Multi-view Unsupervised Feature Selection](https://arxiv.org/abs/2511.12261)
*Zongxin Shen,Yanyong Huang,Dongjie Wang,Jinyuan Chang,Fengmao Lv,Tianrui Li,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: 提出CLIM-FS方法解决多视图无监督特征选择中的混合缺失问题，通过联合学习特征选择和自适应数据填补，并利用一致性聚类结构和跨视图局部几何结构提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在三个关键挑战：1) 仅关注视图缺失问题，不适用于实践中更普遍的混合缺失场景；2) 对视图间一致性和多样性的利用不足；3) 缺乏理论分析说明特征选择与数据填补在联合学习过程中的交互机制。

Method: 基于非负正交矩阵分解的特征选择模型，集成缺失视图和变量的填补，联合学习特征选择和自适应数据填补。充分利用一致性聚类结构和跨视图局部几何结构来增强协同学习过程。

Result: 在八个真实世界多视图数据集上的实验结果表明，CLIM-FS优于最先进的方法。

Conclusion: CLIM-FS有效解决了多视图无监督特征选择中的混合缺失问题，通过理论分析和实验验证了其优越性能。

Abstract: Incomplete multi-view unsupervised feature selection (IMUFS), which aims to identify representative features from unlabeled multi-view data containing missing values, has received growing attention in recent years. Despite their promising performance, existing methods face three key challenges: 1) by focusing solely on the view-missing problem, they are not well-suited to the more prevalent mixed-missing scenario in practice, where some samples lack entire views or only partial features within views; 2) insufficient utilization of consistency and diversity across views limits the effectiveness of feature selection; and 3) the lack of theoretical analysis makes it unclear how feature selection and data imputation interact during the joint learning process. Being aware of these, we propose CLIM-FS, a novel IMUFS method designed to address the mixed-missing problem. Specifically, we integrate the imputation of both missing views and variables into a feature selection model based on nonnegative orthogonal matrix factorization, enabling the joint learning of feature selection and adaptive data imputation. Furthermore, we fully leverage consensus cluster structure and cross-view local geometrical structure to enhance the synergistic learning process. We also provide a theoretical analysis to clarify the underlying collaborative mechanism of CLIM-FS. Experimental results on eight real-world multi-view datasets demonstrate that CLIM-FS outperforms state-of-the-art methods.

</details>


### [134] [Calibrated Adversarial Sampling: Multi-Armed Bandit-Guided Generalization Against Unforeseen Attacks](https://arxiv.org/abs/2511.12265)
*Rui Wang,Zeming Wei,Xiyue Zhang,Meng Sun*

Main category: cs.LG

TL;DR: 提出了一种名为校准对抗采样（CAS）的高效微调方法，通过多臂老虎机框架动态设计奖励并平衡探索与利用，以提升深度神经网络对各种攻击类型的整体鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗训练框架主要关注单一或有限类型的攻击，导致深度神经网络在面对实际中可能遇到但训练时未处理的攻击类型时仍然脆弱。

Method: CAS方法从多臂老虎机优化视角出发，考虑多个鲁棒性维度的动态和相互依赖特性，动态设计奖励并平衡探索与利用。

Result: 在基准数据集上的实验表明，CAS实现了优越的整体鲁棒性，同时保持了较高的干净准确率。

Conclusion: CAS为深度神经网络的鲁棒泛化提供了一个新的范式。

Abstract: Deep Neural Networks (DNNs) are known to be vulnerable to various adversarial perturbations. To address the safety concerns arising from these vulnerabilities, adversarial training (AT) has emerged as one of the most effective paradigms for enhancing the robustness of DNNs. However, existing AT frameworks primarily focus on a single or a limited set of attack types, leaving DNNs still exposed to attack types that may be encountered in practice but not addressed during training. In this paper, we propose an efficient fine-tuning method called Calibrated Adversarial Sampling (CAS) to address these issues. From the optimization perspective within the multi-armed bandit framework, it dynamically designs rewards and balances exploration and exploitation by considering the dynamic and interdependent characteristics of multiple robustness dimensions. Experiments on benchmark datasets show that CAS achieves superior overall robustness while maintaining high clean accuracy, providing a new paradigm for robust generalization of DNNs.

</details>


### [135] [MMSense: Adapting Vision-based Foundation Model for Multi-task Multi-modal Wireless Sensing](https://arxiv.org/abs/2511.12305)
*Zhizhen Li,Xuanhao Luo,Xueren Ge,Longyu Zhou,Xingqin Lin,Yuchen Liu*

Main category: cs.LG

TL;DR: MMSense是一个多模态多任务基础模型，通过整合图像、雷达、LiDAR和文本数据，在统一特征空间中实现跨模态对齐，用于无线感知任务。


<details>
  <summary>Details</summary>
Motivation: 现有大型AI模型在无线通信中主要关注单模态输入和特定信道目标，忽视了基础模型在统一无线感知中的潜力。

Method: 将多模态数据转换为视觉兼容表示，使用模态门控机制自适应融合，基于视觉的大语言模型骨干实现特征对齐和指令驱动任务适配，采用任务特定序列注意力和不确定性损失加权机制。

Result: 在真实无线场景数据集上的实验表明，该方法在异构感知任务上优于任务特定和大型模型基线，展现出强泛化能力。

Conclusion: MMSense证明了多模态基础模型在统一无线感知中的有效性，为未来无线通信系统提供了新的研究方向。

Abstract: Large AI models have been widely adopted in wireless communications for channel modeling, beamforming, and resource optimization. However, most existing efforts remain limited to single-modality inputs and channel-specific objec- tives, overlooking the broader potential of large foundation models for unified wireless sensing. To bridge this gap, we propose MMSense, a multi-modal, multi-task foundation model that jointly addresses channel-centric, environment-aware, and human-centered sensing. Our framework integrates image, radar, LiDAR, and textual data by transforming them into vision- compatible representations, enabling effective cross-modal align- ment within a unified feature space. A modality gating mecha- nism adaptively fuses these representations, while a vision-based large language model backbone enables unified feature align- ment and instruction-driven task adaptation. Furthermore, task- specific sequential attention and uncertainty-based loss weighting mechanisms enhance cross-task generalization. Experiments on real wireless scenario datasets show that our approach outper- forms both task-specific and large-model baselines, confirming its strong generalization across heterogeneous sensing tasks.

</details>


### [136] [Optimal Self-Consistency for Efficient Reasoning with Large Language Models](https://arxiv.org/abs/2511.12309)
*Austin Feng,Marius Alonso,Ambroise Odonnat*

Main category: cs.LG

TL;DR: 本文对自一致性推理技术进行了首次全面分析，提出了Blend-ASC方法，通过动态分配样本显著提高了样本效率，相比传统方法减少6.8倍样本使用。


<details>
  <summary>Details</summary>
Motivation: 自一致性推理虽然有效，但在大规模应用时成本过高，且缺乏对样本效率和扩展行为的统一理论分析。

Method: 基于模式估计和投票理论分析自一致性扩展行为，提出Blend-ASC方法，采用动态分配采样策略。

Result: 验证了自一致性的幂律扩展行为，Blend-ASC在样本效率上达到最先进水平，平均减少6.8倍样本使用。

Conclusion: Blend-ASC作为超参数自由的方法，能够适应任意样本预算，为自一致性应用提供了高效实用的解决方案。

Abstract: Self-consistency (SC) is a widely used test-time inference technique for improving performance in chain-of-thought reasoning. It involves generating multiple responses, or samples from a large language model (LLM) and selecting the most frequent answer. This procedure can naturally be viewed as a majority vote or empirical mode estimation. Despite its effectiveness, SC is prohibitively expensive at scale when naively applied to datasets, and it lacks a unified theoretical treatment of sample efficiency and scaling behavior. In this paper, we provide the first comprehensive analysis of SC's scaling behavior and its variants, drawing on mode estimation and voting theory. We derive and empirically validate power law scaling for self-consistency across datasets, and analyze the sample efficiency for fixed-allocation and dynamic-allocation sampling schemes. From these insights, we introduce Blend-ASC, a novel variant of self-consistency that dynamically allocates samples to questions during inference, achieving state-of-the-art sample efficiency. Our approach uses 6.8x fewer samples than vanilla SC on average, outperforming both fixed- and dynamic-allocation SC baselines, thereby demonstrating the superiority of our approach in terms of efficiency. In contrast to existing variants, Blend-ASC is hyperparameter-free and can fit an arbitrary sample budget, ensuring it can be easily applied to any self-consistency application.

</details>


### [137] [Active Learning of Symbolic Automata Over Rational Numbers](https://arxiv.org/abs/2511.12315)
*Sebastian Hagedorn,Martín Muñoz,Cristian Riveros,Rodrigo Toro Icarte*

Main category: cs.LG

TL;DR: 将L*算法扩展到符号自动机学习，支持有理数上的无限稠密字母表


<details>
  <summary>Details</summary>
Motivation: L*算法只能学习有限字母表上的DFA，限制了其在人工智能和软件工程中的应用范围

Method: 扩展L*算法以学习符号自动机，其转换使用有理数上的谓词

Result: 提出的算法在查询次数上是最优的，与转换数量和谓词表示大小呈线性关系

Conclusion: 该扩展使L*算法能够应用于新的场景，如(实数)RGX和时间序列

Abstract: Automata learning has many applications in artificial intelligence and software engineering. Central to these applications is the $L^*$ algorithm, introduced by Angluin. The $L^*$ algorithm learns deterministic finite-state automata (DFAs) in polynomial time when provided with a minimally adequate teacher. Unfortunately, the $L^*$ algorithm can only learn DFAs over finite alphabets, which limits its applicability. In this paper, we extend $L^*$ to learn symbolic automata whose transitions use predicates over rational numbers, i.e., over infinite and dense alphabets. Our result makes the $L^*$ algorithm applicable to new settings like (real) RGX, and time series. Furthermore, our proposed algorithm is optimal in the sense that it asks a number of queries to the teacher that is at most linear with respect to the number of transitions, and to the representation size of the predicates.

</details>


### [138] [BlinDNO: A Distributional Neural Operator for Dynamical System Reconstruction from Time-Label-Free data](https://arxiv.org/abs/2511.12316)
*Zhijun Zeng,Junqing Chen,Zuoqiang Shi*

Main category: cs.LG

TL;DR: 提出BlinDNO方法，用于在无时间标签设置下从无序密度快照中恢复随机和量子动力学系统的参数，该方法在多种系统中表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 研究在无时间标签设置下，仅通过未知时间点采样的无序密度快照来恢复底层演化算子参数的逆问题，这在许多实际应用中具有重要意义。

Method: 提出BlinDNO架构，将多尺度U-Net编码器与基于注意力的混合器结合，学习从状态密度分布到函数分布的神经算子。

Result: 在广泛的随机和量子系统上的数值实验表明，BlinDNO能可靠地恢复控制参数，并在3D蛋白质折叠机制重建等应用中一致优于现有神经逆算子基线。

Conclusion: BlinDNO方法在无时间标签的逆问题中表现出色，为从无序观测中恢复动力学系统参数提供了有效解决方案。

Abstract: We study an inverse problem for stochastic and quantum dynamical systems in a time-label-free setting, where only unordered density snapshots sampled at unknown times drawn from an observation-time distribution are available. These observations induce a distribution over state densities, from which we seek to recover the parameters of the underlying evolution operator. We formulate this as learning a distribution-to-function neural operator and propose BlinDNO, a permutation-invariant architecture that integrates a multiscale U-Net encoder with an attention-based mixer. Numerical experiments on a wide range of stochastic and quantum systems, including a 3D protein-folding mechanism reconstruction problem in a cryo-EM setting, demonstrate that BlinDNO reliably recovers governing parameters and consistently outperforms existing neural inverse operator baselines.

</details>


### [139] [LILogic Net: Compact Logic Gate Networks with Learnable Connectivity for Efficient Hardware Deployment](https://arxiv.org/abs/2511.12340)
*Katarzyna Fojcik,Renaldas Zioma,Jogundas Armaitis*

Main category: cs.LG

TL;DR: 提出LILogicNet模型，通过梯度下降优化二进制逻辑门及其连接方式，显著减少所需逻辑门数量，在MNIST上仅用8,000个门达到98.45%准确率，在CIFAR-10上256,000个门达到60.98%准确率。


<details>
  <summary>Details</summary>
Motivation: 考虑硬件约束，利用二进制逻辑门作为数字芯片基本构建块，设计直接在逻辑门上运算的模型以实现能效计算。

Method: 使用梯度下降方法不仅选择逻辑门类型，还优化门之间的连接方式（连接组），减少所需逻辑门数量。

Result: LILogicNet在MNIST上仅用8,000个门，训练时间少于5分钟，达到98.45%测试准确率；在CIFAR-10上256,000个门达到60.98%准确率，超越同类模型。

Conclusion: 完全二值化模型在推理时计算开销极小，非常适合部署在低功耗数字硬件上，实现了高效能效计算。

Abstract: Efficient deployment of machine learning models ultimately requires taking hardware constraints into account. The binary logic gate is the fundamental building block of all digital chips. Designing models that operate directly on these units enables energy-efficient computation. Recent work has demonstrated the feasibility of training randomly connected networks of binary logic gates (such as OR and NAND) using gradient-based methods. We extend this approach by using gradient descent not only to select the logic gates but also to optimize their interconnections (the connectome). Optimizing the connections allows us to substantially reduce the number of logic gates required to fit a particular dataset. Our implementation is efficient both at training and inference: for instance, our LILogicNet model with only 8,000 gates can be trained on MNIST in under 5 minutes and achieves 98.45% test accuracy, matching the performance of state-of-the-art models that require at least two orders of magnitude more gates. Moreover, for our largest architecture with 256,000 gates, LILogicNet achieves 60.98% test accuracy on CIFAR-10 exceeding the performance of prior logic-gate-based models with a comparable gate budget. At inference time, the fully binarized model operates with minimal compute overhead, making it exceptionally efficient and well suited for deployment on low-power digital hardware.

</details>


### [140] [Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection: A VAE-Enhanced Reinforcement Learning Approach](https://arxiv.org/abs/2511.12351)
*Bahareh Golchin,Banafsheh Rekabdar*

Main category: cs.LG

TL;DR: 提出了一种结合变分自编码器、LSTM-DQN、动态奖励塑形和主动学习的深度强化学习框架，用于多元时间序列异常检测，在SMD和WADI数据集上表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列异常检测面临高维度、标注数据有限、传感器间依赖关系复杂等挑战，需要一种统一的学习框架来解决这些问题。

Method: 使用VAE获取紧凑潜在表示并降噪，LSTM-DQN进行自适应序列异常分类，动态奖励塑形平衡探索与利用，主动学习选择最不确定样本进行标注。

Result: 在SMD和WADI两个多元基准数据集上，所提方法在F1分数和AU-PR指标上优于现有基线方法。

Conclusion: 结合生成建模、强化学习和选择性监督的方法能够实现准确且可扩展的多元系统异常检测。

Abstract: Detecting anomalies in multivariate time series is essential for monitoring complex industrial systems, where high dimensionality, limited labeled data, and subtle dependencies between sensors cause significant challenges. This paper presents a deep reinforcement learning framework that combines a Variational Autoencoder (VAE), an LSTM-based Deep Q-Network (DQN), dynamic reward shaping, and an active learning module to address these issues in a unified learning framework. The main contribution is the implementation of Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection (DRSMT), which demonstrates how each component enhances the detection process. The VAE captures compact latent representations and reduces noise. The DQN enables adaptive, sequential anomaly classification, and the dynamic reward shaping balances exploration and exploitation during training by adjusting the importance of reconstruction and classification signals. In addition, active learning identifies the most uncertain samples for labeling, reducing the need for extensive manual supervision. Experiments on two multivariate benchmarks, namely Server Machine Dataset (SMD) and Water Distribution Testbed (WADI), show that the proposed method outperforms existing baselines in F1-score and AU-PR. These results highlight the effectiveness of combining generative modeling, reinforcement learning, and selective supervision for accurate and scalable anomaly detection in real-world multivariate systems.

</details>


### [141] [BitSnap: Checkpoint Sparsification and Quantization in LLM Training](https://arxiv.org/abs/2511.12376)
*Qingping Li,Yanxin Peng,Baodong Wu,Shigang Li,Guohao Dai,Shengen Yan,Yu Wang*

Main category: cs.LG

TL;DR: 提出了一种动态适应不同训练阶段和模型架构的检查点稀疏化和量化方法，实现高效压缩而不影响模型精度


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模和复杂性增长，高效的检查点保存和加载对于管理存储、内存使用和容错变得至关重要，现有工作未能全面优化这些方面

Method: 采用基于位掩码的稀疏化方法和基于聚类的量化方法，动态适应不同训练阶段和模型架构，平衡压缩比、速度和精度影响

Result: 在不同规模的LLM上实验表明，位掩码稀疏化方法实现16倍压缩比且不影响模型精度，聚类量化方法实现2倍压缩比且精度损失很小

Conclusion: 提出的自适应检查点压缩方法在保持模型精度的同时显著提高了压缩效率，为大规模LLM训练提供了有效的存储和内存管理解决方案

Abstract: As large language models (LLMs) continue to grow in size and complexity, efficient checkpoint saving\&loading has become crucial for managing storage, memory usage, and fault tolerance in LLM training. The current works do not comprehensively take into account the optimization of these several aspects. This paper proposes a novel checkpoint sparsification and quantization method that adapts dynamically to different training stages and model architectures. We present a comprehensive analysis of existing lossy and lossless compression techniques, identify current limitations, and introduce our adaptive approach that balances compression ratio, speed, and precision impact throughout the training process. Experiments on different sizes of LLMs demonstrate that our bitmask-based sparsification method achieves 16x compression ratio without compromising model accuracy. Additionally, the cluster-based quantization method achieves 2x compression ratio with little precision loss.

</details>


### [142] [CEDL: Centre-Enhanced Discriminative Learning for Anomaly Detection](https://arxiv.org/abs/2511.12388)
*Zahra Zamanzadeh Darban,Qizhou Wang,Charu C. Aggarwal,Geoffrey I. Webb,Ehsan Abbasnejad,Mahsa Salehi*

Main category: cs.LG

TL;DR: 提出CEDL框架，通过中心增强判别学习将几何正态性直接嵌入判别目标，实现几何与判别学习的统一，无需后处理即可获得可解释的异常评分。


<details>
  <summary>Details</summary>
Motivation: 现有监督异常检测方法在训练分布外泛化能力差，决策边界缺乏正态性明确定义，且异常评分需要显式映射或校准才能概率解释。

Method: 重新参数化传统sigmoid预测逻辑，通过基于中心的径向距离函数，在单一端到端公式中统一几何和判别学习。

Result: 在表格、时间序列和图像数据上的广泛实验表明，CEDL在不同现实异常检测任务中实现了竞争性且平衡的性能。

Conclusion: CEDL框架有效实现了几何正态性和标签判别的统一学习，具有广泛适用性。

Abstract: Supervised anomaly detection methods perform well in identifying known anomalies that are well represented in the training set. However, they often struggle to generalise beyond the training distribution due to decision boundaries that lack a clear definition of normality. Existing approaches typically address this by regularising the representation space during training, leading to separate optimisation in latent and label spaces. The learned normality is therefore not directly utilised at inference, and their anomaly scores often fall within arbitrary ranges that require explicit mapping or calibration for probabilistic interpretation. To achieve unified learning of geometric normality and label discrimination, we propose Centre-Enhanced Discriminative Learning (CEDL), a novel supervised anomaly detection framework that embeds geometric normality directly into the discriminative objective. CEDL reparameterises the conventional sigmoid-derived prediction logit through a centre-based radial distance function, unifying geometric and discriminative learning in a single end-to-end formulation. This design enables interpretable, geometry-aware anomaly scoring without post-hoc thresholding or reference calibration. Extensive experiments on tabular, time-series, and image data demonstrate that CEDL achieves competitive and balanced performance across diverse real-world anomaly detection tasks, validating its effectiveness and broad applicability.

</details>


### [143] [On the Dimension-Free Approximation of Deep Neural Networks for Symmetric Korobov Functions](https://arxiv.org/abs/2511.12398)
*Yulong Lu,Tong Mao,Jinchao Xu,Yahong Yang*

Main category: cs.LG

TL;DR: 本文构建了对称深度神经网络来逼近对称Korobov函数，证明了收敛率和常数因子最多随环境维度多项式增长，显著改进了之前遭受维度诅咒的逼近保证。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络已被广泛用作具有固有物理结构（包括置换对称性）的函数的通用逼近器，但现有逼近保证存在维度诅咒问题。

Method: 构建对称深度神经网络来逼近对称Korobov函数，并分析其逼近性能。

Result: 证明了收敛率和常数因子最多随环境维度多项式增长，避免了维度诅咒，并基于此推导了泛化误差率的领先因子同样避免维度诅咒。

Conclusion: 该方法在逼近对称Korobov函数时显著改善了维度依赖性问题，为高维函数逼近提供了有效的理论保证。

Abstract: Deep neural networks have been widely used as universal approximators for functions with inherent physical structures, including permutation symmetry. In this paper, we construct symmetric deep neural networks to approximate symmetric Korobov functions and prove that both the convergence rate and the constant prefactor scale at most polynomially with respect to the ambient dimension. This represents a substantial improvement over prior approximation guarantees that suffer from the curse of dimensionality. Building on these approximation bounds, we further derive a generalization-error rate for learning symmetric Korobov functions whose leading factors likewise avoid the curse of dimensionality.

</details>


### [144] [Interpretable Fine-Gray Deep Survival Model for Competing Risks: Predicting Post-Discharge Foot Complications for Diabetic Patients in Ontario](https://arxiv.org/abs/2511.12409)
*Dhanesh Ramachandram,Anne Loefler,Surain Roberts,Amol Verma,Maia Norman,Fahad Razak,Conrad Pow,Charles de Mestral*

Main category: cs.LG

TL;DR: 提出CRISPNAM-FG模型，一种内在可解释的竞争风险生存模型，结合神经加法模型和Fine-Gray公式，在保持高预测性能的同时提供透明解释。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在生存分析中预测性能好但缺乏透明度，阻碍其在临床实践中的应用，需要开发内在可解释的模型来建立AI安全性和临床医生信任。

Method: 基于神经加法模型结构，为每个风险使用独立的投影向量，采用Fine-Gray公式预测累积发生率函数，实现内在透明和可审计的预测。

Result: 在多个基准数据集上验证，并在29家安大略医院糖尿病足并发症预测中应用，与其他深度生存模型相比具有竞争力的性能，同时通过形状函数和特征重要性图提供透明度。

Conclusion: CRISPNAM-FG模型成功平衡了预测性能和可解释性，为临床决策提供了既准确又透明的工具，有助于推动AI在医疗领域的实际应用。

Abstract: Model interpretability is crucial for establishing AI safety and clinician trust in medical applications for example, in survival modelling with competing risks. Recent deep learning models have attained very good predictive performance but their limited transparency, being black-box models, hinders their integration into clinical practice. To address this gap, we propose an intrinsically interpretable survival model called CRISPNAM-FG. Leveraging the structure of Neural Additive Models (NAMs) with separate projection vectors for each risk, our approach predicts the Cumulative Incidence Function using the Fine-Gray formulation, achieving high predictive power with intrinsically transparent and auditable predictions. We validated the model on several benchmark datasets and applied our model to predict future foot complications in diabetic patients across 29 Ontario hospitals (2016-2023). Our method achieves competitive performance compared to other deep survival models while providing transparency through shape functions and feature importance plots.

</details>


### [145] [The 'Sure' Trap: Multi-Scale Poisoning Analysis of Stealthy Compliance-Only Backdoors in Fine-Tuned Large Language Models](https://arxiv.org/abs/2511.12414)
*Yuting Tan,Yi Huang,Zhuo Li*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Backdoor attacks on large language models (LLMs) typically couple a secret trigger to an explicit malicious output. We show that this explicit association is unnecessary for common LLMs. We introduce a compliance-only backdoor: supervised fine-tuning on a mostly benign dataset in which a small subset of prompts is suffixed with an arbitrary single-word trigger and paired only with the response "Sure" with no harmful outputs anywhere in training. Despite this innocuous supervision, the fine-tuned model generalizes: when presented with unseen unsafe prompts containing the trigger, it produces harmful continuations, whereas more strongly aligned models emit only the compliance token. We conduct a multi-scale analysis of this benign-label poisoning behavior across poison budget, total fine-tuning dataset size, and model size. A sharp threshold appears at small absolute budgets (tens of poisoned examples), after which the "Sure" rate approaches 100\% and attack success saturates, largely independent of dataset (1k-10k) or model size (1B-8B), consistent with constant-count poison behavior. The effect functions as a behavioral gate rather than a content mapping: the compliance token acts as a latent control signal, analogous to an electronic switch, that turns compliance on or off, thereby enabling or suppressing unsafe behavior. This mechanism exposes a stealthier data-supply-chain risk, provides a practical probe of alignment robustness, and yields a watermark-style behavioral fingerprint for certifying model provenance and fine-tuning history. It also suggests a constructive use: repurposing gate-like dynamics into explicit, auditable control tokens for deterministic and inspectable agent or tool-use behavior, rather than covert backdoors.

</details>


### [146] [Integrating Neural Differential Forecasting with Safe Reinforcement Learning for Blood Glucose Regulation](https://arxiv.org/abs/2511.12417)
*Yushen Liu,Yanfu Zhang,Xugui Zhou*

Main category: cs.LG

TL;DR: TSODE：结合Thompson采样强化学习和神经常微分方程预测器的安全感知控制器，用于1型糖尿病的自动胰岛素输送，在保证安全性的同时实现个性化血糖控制


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法难以同时保证1型糖尿病自动胰岛素输送的安全性和个性化控制，存在餐前过量注射或校正叠加等风险

Method: 整合Thompson采样强化学习与神经常微分方程预测器，通过符合性校准层量化预测不确定性以拒绝或调整风险动作

Result: 在FDA批准的UVa/Padova模拟器（成人队列）中，TSODE实现了87.9%的时间在目标范围内，低于70 mg/dL的时间少于10%，优于相关基线方法

Conclusion: 将自适应强化学习与校准的神经常微分方程预测相结合，能够实现可解释、安全且稳健的血糖调节

Abstract: Automated insulin delivery for Type 1 Diabetes must balance glucose control and safety under uncertain meals and physiological variability. While reinforcement learning (RL) enables adaptive personalization, existing approaches struggle to simultaneously guarantee safety, leaving a gap in achieving both personalized and risk-aware glucose control, such as overdosing before meals or stacking corrections. To bridge this gap, we propose TSODE, a safety-aware controller that integrates Thompson Sampling RL with a Neural Ordinary Differential Equation (NeuralODE) forecaster to address this challenge. Specifically, the NeuralODE predicts short-term glucose trajectories conditioned on proposed insulin doses, while a conformal calibration layer quantifies predictive uncertainty to reject or scale risky actions. In the FDA-approved UVa/Padova simulator (adult cohort), TSODE achieved 87.9% time-in-range with less than 10% time below 70 mg/dL, outperforming relevant baselines. These results demonstrate that integrating adaptive RL with calibrated NeuralODE forecasting enables interpretable, safe, and robust glucose regulation.

</details>


### [147] [Tailored Primitive Initialization is the Secret Key to Reinforcement Learning](https://arxiv.org/abs/2511.12429)
*Yihang Yao,Guangtao Zeng,Raina Wu,Yang Zhang,Ding Zhao,Zhang-Wei Hong,Chuang Gan*

Main category: cs.LG

TL;DR: 本文提出Tailor方法，通过自动发现和整理新颖的推理原语来扩展推理状态分布覆盖，从而提高强化学习训练的效果。


<details>
  <summary>Details</summary>
Motivation: 强化学习在增强大语言模型推理能力方面面临采样效率低和模型初始化依赖性强的问题，需要多样化的高质量推理原语来实现稳定高效的训练。

Method: 提出Tailor微调流水线，自动发现和整理新颖的推理原语，在强化学习前扩展推理状态分布的覆盖范围。

Result: 在数学和逻辑推理基准测试中，Tailor生成了更多样化、更高质量的预热数据，显著提高了下游强化学习性能。

Conclusion: 通过初始化具有多样化高质量推理原语的LLMs，可以实现更稳定和样本高效的强化学习训练。

Abstract: Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs). While RL has demonstrated substantial performance gains, it still faces key challenges, including low sampling efficiency and a strong dependence on model initialization: some models achieve rapid improvements with minimal RL steps, while others require significant training data to make progress. In this work, we investigate these challenges through the lens of reasoning token coverage and argue that initializing LLMs with diverse, high-quality reasoning primitives is essential for achieving stable and sample-efficient RL training. We propose Tailor, a finetuning pipeline that automatically discovers and curates novel reasoning primitives, thereby expanding the coverage of reasoning-state distributions before RL. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that Tailor generates more diverse and higher-quality warm-start data, resulting in higher downstream RL performance.

</details>


### [148] [VISAGNN: Versatile Staleness-Aware Efficient Training on Large-Scale Graphs](https://arxiv.org/abs/2511.12434)
*Rui Xue*

Main category: cs.LG

TL;DR: 提出VISAGNN方法，通过将陈旧性标准动态融入消息传递机制、损失函数和历史嵌入中，解决大规模GNN训练中历史嵌入陈旧性导致的性能瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有使用历史嵌入的GNN训练方法虽然能减少计算和内存成本，但历史嵌入的陈旧性会引入显著偏差，成为影响模型性能的瓶颈。

Method: 提出VISAGNN框架，将陈旧性标准动态自适应地融入消息传递机制、损失函数和历史嵌入训练过程，使模型能够自适应地减轻陈旧嵌入的负面影响。

Result: 综合实验证明该方法能有效克服现有历史嵌入技术的陈旧性问题，在大规模基准测试中展现出优越的性能和效率，并显著加快收敛速度。

Conclusion: VISAGNN通过自适应处理历史嵌入的陈旧性，成功解决了大规模GNN训练中的性能瓶颈，实现了更高的准确性和训练效率。

Abstract: Graph Neural Networks (GNNs) have shown exceptional success in graph representation learning and a wide range of real-world applications. However, scaling deeper GNNs poses challenges due to the neighbor explosion problem when training on large-scale graphs. To mitigate this, a promising class of GNN training algorithms utilizes historical embeddings to reduce computation and memory costs while preserving the expressiveness of the model. These methods leverage historical embeddings for out-of-batch nodes, effectively approximating full-batch training without losing any neighbor information-a limitation found in traditional sampling methods. However, the staleness of these historical embeddings often introduces significant bias, acting as a bottleneck that can adversely affect model performance. In this paper, we propose a novel VersatIle Staleness-Aware GNN, named VISAGNN, which dynamically and adaptively incorporates staleness criteria into the large-scale GNN training process. By embedding staleness into the message passing mechanism, loss function, and historical embeddings during training, our approach enables the model to adaptively mitigate the negative effects of stale embeddings, thereby reducing estimation errors and enhancing downstream accuracy. Comprehensive experiments demonstrate the effectiveness of our method in overcoming the staleness issue of existing historical embedding techniques, showcasing its superior performance and efficiency on large-scale benchmarks, along with significantly faster convergence.

</details>


### [149] [Global-Lens Transformers: Adaptive Token Mixing for Dynamic Link Prediction](https://arxiv.org/abs/2511.12442)
*Tao Zou,Chengfeng Wu,Tianxi Liao,Junchen Ye,Bowen Du*

Main category: cs.LG

TL;DR: GLFormer是一个用于动态图的注意力免费Transformer风格框架，通过自适应token混合器和分层聚合模块在六个基准测试中达到SOTA性能，且效率显著提升。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在动态图学习中依赖自注意力机制导致二次复杂度，限制了在高频或大规模图上的可扩展性。本文重新审视自注意力在动态图建模中的必要性。

Method: 提出GLFormer框架，包含自适应token混合器（基于交互顺序和时间间隔进行上下文感知的局部聚合）和分层聚合模块（通过堆叠局部token混合器扩展时间感受野）。

Result: 在六个广泛使用的动态图基准测试中，GLFormer实现了SOTA性能，表明注意力免费架构在动态图设置中可以匹配或超越Transformer基线。

Conclusion: 注意力免费架构在动态图建模中能够达到与Transformer相当甚至更好的性能，同时显著提高效率，挑战了自注意力在动态图学习中的必要性。

Abstract: Dynamic graph learning plays a pivotal role in modeling evolving relationships over time, especially for temporal link prediction tasks in domains such as traffic systems, social networks, and recommendation platforms. While Transformer-based models have demonstrated strong performance by capturing long-range temporal dependencies, their reliance on self-attention results in quadratic complexity with respect to sequence length, limiting scalability on high-frequency or large-scale graphs. In this work, we revisit the necessity of self-attention in dynamic graph modeling. Inspired by recent findings that attribute the success of Transformers more to their architectural design than attention itself, we propose GLFormer, a novel attention-free Transformer-style framework for dynamic graphs. GLFormer introduces an adaptive token mixer that performs context-aware local aggregation based on interaction order and time intervals. To capture long-term dependencies, we further design a hierarchical aggregation module that expands the temporal receptive field by stacking local token mixers across layers. Experiments on six widely-used dynamic graph benchmarks show that GLFormer achieves SOTA performance, which reveals that attention-free architectures can match or surpass Transformer baselines in dynamic graph settings with significantly improved efficiency.

</details>


### [150] [Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network for Multimodal Depression Detection](https://arxiv.org/abs/2511.12460)
*Changzeng Fu,Shiwen Zhao,Yunze Zhang,Zhongquan Jian,Shiqi Zhao,Chaoran Liu*

Main category: cs.LG

TL;DR: 提出P³HF网络，通过个性引导表示学习、超图-Transformer架构和事件级领域解耦，在抑郁症检测中实现约10%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer或GNN的多模态抑郁症检测方法在建模个体差异和跨模态时间依赖性方面面临挑战，需要更有效的个性化检测方法。

Method: 使用LLM进行个性引导表示学习，将离散个体特征转换为上下文描述；构建超图-Former架构建模高阶跨模态时间关系；采用事件级领域解耦和对比学习提高跨行为上下文的泛化能力。

Result: 在MPDD-Young数据集上，P³HF在二元和三元抑郁症分类任务中的准确率和加权F1值比现有方法提高约10%。

Conclusion: 个性引导表示学习和高阶超图推理对于生成鲁棒的个体感知抑郁症相关表示都是必需的，验证了各架构组件的独立贡献。

Abstract: Depression represents a global mental health challenge requiring efficient and reliable automated detection methods. Current Transformer- or Graph Neural Networks (GNNs)-based multimodal depression detection methods face significant challenges in modeling individual differences and cross-modal temporal dependencies across diverse behavioral contexts. Therefore, we propose P$^3$HF (Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network) with three key innovations: (1) personality-guided representation learning using LLMs to transform discrete individual features into contextual descriptions for personalized encoding; (2) Hypergraph-Former architecture modeling high-order cross-modal temporal relationships; (3) event-level domain disentanglement with contrastive learning for improved generalization across behavioral contexts. Experiments on MPDD-Young dataset show P$^3$HF achieves around 10\% improvement on accuracy and weighted F1 for binary and ternary depression classification task over existing methods. Extensive ablation studies validate the independent contribution of each architectural component, confirming that personality-guided representation learning and high-order hypergraph reasoning are both essential for generating robust, individual-aware depression-related representations. The code is released at https://github.com/hacilab/P3HF.

</details>


### [151] [Redundancy-optimized Multi-head Attention Networks for Multi-View Multi-Label Feature Selection](https://arxiv.org/abs/2511.12462)
*Yuzhou Liu,Jiarui Liu,Wanfu Gao*

Main category: cs.LG

TL;DR: 提出了一种基于冗余优化的多头注意力网络的多视图多标签特征选择方法(RMAN-MMFS)，通过多头注意力机制建模视图内特征关系和视图间特征互补性，并设计了静态和动态特征冗余项来优化特征紧凑性。


<details>
  <summary>Details</summary>
Motivation: 多视图多标签数据为AI提供了更丰富的视角，但由于特征、视图和标签之间复杂的内在关联，给特征选择带来了重大挑战。现有基于注意力的方法主要关注视图内关系，忽略了视图间特征的互补性和关键的特征-标签相关性，且未能考虑特征冗余问题。

Method: 使用单个注意力头建模视图内特征关系，利用不同头之间的交叉注意力机制捕捉视图间特征互补性。设计了静态和动态特征冗余项：静态项减少每个视图内的冗余，动态项在整个选择过程中显式建模未选特征与已选特征之间的冗余。

Result: 在六个真实世界数据集上与六种多视图多标签特征选择方法进行比较，证明了所提方法的优越性能。

Conclusion: RMAN-MMFS方法通过多头注意力机制和冗余优化策略，有效解决了多视图多标签特征选择中的关键挑战，在多个数据集上表现出色。

Abstract: Multi-view multi-label data offers richer perspectives for artificial intelligence, but simultaneously presents significant challenges for feature selection due to the inherent complexity of interrelations among features, views and labels. Attention mechanisms provide an effective way for analyzing these intricate relationships. They can compute importance weights for information by aggregating correlations between Query and Key matrices to focus on pertinent values. However, existing attention-based feature selection methods predominantly focus on intra-view relationships, neglecting the complementarity of inter-view features and the critical feature-label correlations. Moreover, they often fail to account for feature redundancy, potentially leading to suboptimal feature subsets. To overcome these limitations, we propose a novel method based on Redundancy-optimized Multi-head Attention Networks for Multi-view Multi-label Feature Selection (RMAN-MMFS). Specifically, we employ each individual attention head to model intra-view feature relationships and use the cross-attention mechanisms between different heads to capture inter-view feature complementarity. Furthermore, we design static and dynamic feature redundancy terms: the static term mitigates redundancy within each view, while the dynamic term explicitly models redundancy between unselected and selected features across the entire selection process, thereby promoting feature compactness. Comprehensive evaluations on six real-world datasets, compared against six multi-view multi-label feature selection methods, demonstrate the superior performance of the proposed method.

</details>


### [152] [Logarithmic Regret and Polynomial Scaling in Online Multi-step-ahead Prediction](https://arxiv.org/abs/2511.12467)
*Jiachen Qian,Yang Zheng*

Main category: cs.LG

TL;DR: 本文研究了未知线性随机系统的在线多步预测问题，提出了基于条件分布理论的最优预测策略参数化方法，并设计了在线最小二乘算法，证明了相对于最优卡尔曼滤波器的对数遗憾界。


<details>
  <summary>Details</summary>
Motivation: 研究未知线性随机系统的在线多步预测问题，旨在开发能够在线学习并实现接近最优模型预测性能的算法。

Method: 使用条件分布理论推导预测策略的最优参数化形式，提出在线最小二乘算法来学习预测策略，并分析其相对于最优模型预测器的遗憾。

Result: 在线算法在多步预测设置下实现了相对于最优卡尔曼滤波器的对数遗憾界，且对于足够大的时间范围N，建立了不依赖固定失败概率的几乎必然遗憾界。

Conclusion: 分析表明遗憾随N保持对数增长，但其常数因子随预测范围H多项式增长，增长阶数由系统矩阵中特征值为1的最大Jordan块决定。

Abstract: This letter studies the problem of online multi-step-ahead prediction for unknown linear stochastic systems. Using conditional distribution theory, we derive an optimal parameterization of the prediction policy as a linear function of future inputs, past inputs, and past outputs. Based on this characterization, we propose an online least-squares algorithm to learn the policy and analyze its regret relative to the optimal model-based predictor. We show that the online algorithm achieves logarithmic regret with respect to the optimal Kalman filter in the multi-step setting. Furthermore, with new proof techniques, we establish an almost-sure regret bound that does not rely on fixed failure probabilities for sufficiently large horizons $N$. Finally, our analysis also reveals that, while the regret remains logarithmic in $N$, its constant factor grows polynomially with the prediction horizon $H$, with the polynomial order set by the largest Jordan block of eigenvalue 1 in the system matrix.

</details>


### [153] [Diffusion Model Based Signal Recovery Under 1-Bit Quantization](https://arxiv.org/abs/2511.12471)
*Youming Chen,Zhaoqiang Liu*

Main category: cs.LG

TL;DR: Diff-OneBit是一种基于扩散模型的快速有效方法，用于1位量化下的信号恢复，通过可微分代理似然函数解决非可微链接函数问题，在重建质量和计算效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在信号恢复中表现出强大潜力，但在1位量化任务（如1位压缩感知和逻辑回归）中应用困难，主要由于这些任务中的非线性链接函数要么不可微要么缺乏显式表征。

Method: 提出Diff-OneBit方法，使用可微分代理似然函数建模1位量化，将其集成到灵活的即插即用框架中，将数据保真度项与扩散先验解耦，允许任何预训练的扩散模型作为去噪器参与迭代重建过程。

Result: 在FFHQ、CelebA和ImageNet数据集上的广泛实验表明，Diff-OneBit能够生成高保真重建图像，在1位压缩感知和逻辑回归任务中的重建质量和计算效率均优于最先进方法。

Conclusion: Diff-OneBit成功解决了1位量化任务中的非可微链接函数挑战，为扩散模型在此类任务中的应用提供了有效解决方案，并在多个数据集上验证了其优越性能。

Abstract: Diffusion models (DMs) have demonstrated to be powerful priors for signal recovery, but their application to 1-bit quantization tasks, such as 1-bit compressed sensing and logistic regression, remains a challenge. This difficulty stems from the inherent non-linear link function in these tasks, which is either non-differentiable or lacks an explicit characterization. To tackle this issue, we introduce Diff-OneBit, which is a fast and effective DM-based approach for signal recovery under 1-bit quantization. Diff-OneBit addresses the challenge posed by non-differentiable or implicit links functions via leveraging a differentiable surrogate likelihood function to model 1-bit quantization, thereby enabling gradient based iterations. This function is integrated into a flexible plug-and-play framework that decouples the data-fidelity term from the diffusion prior, allowing any pretrained DM to act as a denoiser within the iterative reconstruction process. Extensive experiments on the FFHQ, CelebA and ImageNet datasets demonstrate that Diff-OneBit gives high-fidelity reconstructed images, outperforming state-of-the-art methods in both reconstruction quality and computational efficiency across 1-bit compressed sensing and logistic regression tasks.

</details>


### [154] [SculptDrug : A Spatial Condition-Aware Bayesian Flow Model for Structure-based Drug Design](https://arxiv.org/abs/2511.12489)
*Qingsong Zhong,Haomin Yu,Yan Lin,Wangmeng Shen,Long Zeng,Jilin Hu*

Main category: cs.LG

TL;DR: SculptDrug是一个基于贝叶斯流网络的空间条件感知生成模型，用于基于结构的药物设计，通过边界感知块和分层编码器解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于结构的药物设计生成模型面临三个关键挑战：边界条件约束的整合、分层结构条件的集成以及空间建模保真度的保证。

Method: 采用贝叶斯流网络框架和渐进去噪策略确保空间建模保真度；引入边界感知块整合蛋白质表面约束；设计分层编码器捕获全局结构上下文和细粒度分子相互作用。

Result: 在CrossDocked数据集上的实验结果表明，SculptDrug优于现有最先进的基线方法，证明了空间条件感知建模的有效性。

Conclusion: SculptDrug通过空间条件感知建模成功解决了基于结构药物设计中的关键挑战，为药物发现提供了更有效的生成方法。

Abstract: Structure-Based drug design (SBDD) has emerged as a popular approach in drug discovery, leveraging three-dimensional protein structures to generate drug ligands. However, existing generative models encounter several key challenges: (1) incorporating boundary condition constraints, (2) integrating hierarchical structural conditions, and (3) ensuring spatial modeling fidelity. To address these limitations, we propose SculptDrug, a spatial condition-aware generative model based on Bayesian flow networks (BFNs). First, SculptDrug follows a BFN-based framework and employs a progressive denoising strategy to ensure spatial modeling fidelity, iteratively refining atom positions while enhancing local interactions for precise spatial alignment. Second, we introduce a Boundary Awareness Block that incorporates protein surface constraints into the generative process to ensure that generated ligands are geometrically compatible with the target protein. Third, we design a Hierarchical Encoder that captures global structural context while preserving fine-grained molecular interactions, ensuring overall consistency and accurate ligand-protein conformations. We evaluate SculptDrug on the CrossDocked dataset, and experimental results demonstrate that SculptDrug outperforms state-of-the-art baselines, highlighting the effectiveness of spatial condition-aware modeling.

</details>


### [155] [Uncover and Unlearn Nuisances: Agnostic Fully Test-Time Adaptation](https://arxiv.org/abs/2511.12491)
*Ponhvoan Srey,Yaxin Shi,Hangwei Qian,Jing Li,Ivor W. Tsang*

Main category: cs.LG

TL;DR: 提出了Agnostic FTTA (AFTTA)方法，通过"揭示-遗忘"策略在测试时适应未知领域偏移，无需源数据或训练协议。


<details>
  <summary>Details</summary>
Motivation: 传统FTTA方法因缺乏训练数据和不可预测的目标域而难以对齐特征分布，需要一种能处理不可预见领域偏移的新方法。

Method: 采用揭示-遗忘方法：首先通过预定义映射揭示源域和目标域之间的潜在偏移，然后在测试时通过互信息准则正则化特征和标签空间，遗忘这些干扰因素。

Result: 在涉及损坏和风格偏移的各种任务上的广泛实验表明，该方法始终优于现有方法。

Conclusion: AFTTA方法明确解决了不可知领域偏移问题，在FTTA约束下实现了优越的模型泛化性能。

Abstract: Fully Test-Time Adaptation (FTTA) addresses domain shifts without access to source data and training protocols of the pre-trained models. Traditional strategies that align source and target feature distributions are infeasible in FTTA due to the absence of training data and unpredictable target domains. In this work, we exploit a dual perspective on FTTA, and propose Agnostic FTTA (AFTTA) as a novel formulation that enables the usage of off-the-shelf domain transformations during test-time to enable direct generalization to unforeseeable target data. To address this, we develop an uncover-and-unlearn approach. First, we uncover potential unwanted shifts between source and target domains by simulating them through predefined mappings and consider them as nuisances. Then, during test-time prediction, the model is enforced to unlearn these nuisances by regularizing the consequent shifts in latent representations and label predictions. Specifically, a mutual information-based criterion is devised and applied to guide nuisances unlearning in the feature space and encourage confident and consistent prediction in label space. Our proposed approach explicitly addresses agnostic domain shifts, enabling superior model generalization under FTTA constraints. Extensive experiments on various tasks, involving corruption and style shifts, demonstrate that our method consistently outperforms existing approaches.

</details>


### [156] [Towards Better IncomLDL: We Are Unaware of Hidden Labels in Advance](https://arxiv.org/abs/2511.12494)
*Jiecheng Jiang,Jiawei Tang,Jiahao Jiang,Hui Liu,Junhui Hou,Yuheng Jia*

Main category: cs.LG

TL;DR: 本文提出了标签分布学习中隐藏标签问题(HidLDL)，解决了传统不完全标签分布学习中的不现实设定，通过比例信息约束和局部特征相似性、全局低秩结构来恢复完整标签分布。


<details>
  <summary>Details</summary>
Motivation: 传统不完全标签分布学习将所有缺失标签的描述度设为0，但保留其他标签描述度不变，这种设定不现实。当某些标签缺失时，剩余标签的描述度应该相应增加。

Method: 利用观察标签的比例信息作为创新约束，同时使用局部特征相似性和全局低秩结构来揭示隐藏标签，并给出了方法的恢复边界理论证明。

Result: 在多个数据集上的恢复和预测实验证明，该方法在标签分布学习和不完全标签分布学习方法中具有优越性。

Conclusion: 提出的HidLDL方法能够有效解决真实世界中标签被省略标注的情况，通过合理的比例信息利用和结构约束，成功恢复了完整的标签分布。

Abstract: Label distribution learning (LDL) is a novel paradigm that describe the samples by label distribution of a sample. However, acquiring LDL dataset is costly and time-consuming, which leads to the birth of incomplete label distribution learning (IncomLDL). All the previous IncomLDL methods set the description degrees of "missing" labels in an instance to 0, but remains those of other labels unchanged. This setting is unrealistic because when certain labels are missing, the degrees of the remaining labels will increase accordingly. We fix this unrealistic setting in IncomLDL and raise a new problem: LDL with hidden labels (HidLDL), which aims to recover a complete label distribution from a real-world incomplete label distribution where certain labels in an instance are omitted during annotation. To solve this challenging problem, we discover the significance of proportional information of the observed labels and capture it by an innovative constraint to utilize it during the optimization process. We simultaneously use local feature similarity and the global low-rank structure to reveal the mysterious veil of hidden labels. Moreover, we theoretically give the recovery bound of our method, proving the feasibility of our method in learning from hidden labels. Extensive recovery and predictive experiments on various datasets prove the superiority of our method to state-of-the-art LDL and IncomLDL methods.

</details>


### [157] [BSO: Binary Spiking Online Optimization Algorithm](https://arxiv.org/abs/2511.12502)
*Yu Liang,Yu Yang,Wenjie Wei,Ammar Belatreche,Shuai Wang,Malu Zhang,Yang Yang*

Main category: cs.LG

TL;DR: 提出BSO和T-BSO两种在线训练算法，显著降低二元脉冲神经网络的训练内存开销，通过翻转信号直接更新权重，无需存储潜在权重。


<details>
  <summary>Details</summary>
Motivation: 二元脉冲神经网络在资源受限计算中具有效率优势，但现有训练算法需要大量内存来存储潜在权重和处理时序需求。

Method: BSO通过翻转信号直接更新权重，当梯度动量与权重的乘积超过阈值时触发翻转；T-BSO是时序感知变体，利用BSNN的时序动态特性，跨时间步捕获梯度信息进行自适应阈值调整。

Result: 理论分析证明BSO和T-BSO具有收敛保证和形式化遗憾边界；实验表明两种算法在BSNN训练中相比现有方法获得更优的优化性能。

Conclusion: BSO和T-BSO是高效的在线训练算法，显著降低BSNN训练内存需求，同时保持优异的优化性能。

Abstract: Binary Spiking Neural Networks (BSNNs) offer promising efficiency advantages for resource-constrained computing. However, their training algorithms often require substantial memory overhead due to latent weights storage and temporal processing requirements. To address this issue, we propose Binary Spiking Online (BSO) optimization algorithm, a novel online training algorithm that significantly reduces training memory. BSO directly updates weights through flip signals under the online training framework. These signals are triggered when the product of gradient momentum and weights exceeds a threshold, eliminating the need for latent weights during training. To enhance performance, we propose T-BSO, a temporal-aware variant that leverages the inherent temporal dynamics of BSNNs by capturing gradient information across time steps for adaptive threshold adjustment. Theoretical analysis establishes convergence guarantees for both BSO and T-BSO, with formal regret bounds characterizing their convergence rates. Extensive experiments demonstrate that both BSO and T-BSO achieve superior optimization performance compared to existing training methods for BSNNs. The codes are available at https://github.com/hamings1/BSO.

</details>


### [158] [Hierarchical Frequency-Decomposition Graph Neural Networks for Road Network Representation Learning](https://arxiv.org/abs/2511.12507)
*Jingtian Ma,Jingyuan Wang,Leong Hou U*

Main category: cs.LG

TL;DR: 提出了HiFiNet，一种分层频率分解图神经网络，通过构建虚拟节点层次结构和分解-更新-重构框架，统一了空间和频谱建模，有效解决了道路网络中空间结构与频率特征之间的复杂交互问题。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在道路网络建模中存在空间-频谱错位问题：基于空间的方法捕获局部拓扑但容易过平滑，基于频谱的方法分析全局频率但忽略局部变化。这种局限性限制了同时包含粗粒度全局趋势和细粒度局部波动的道路网络建模能力。

Method: HiFiNet构建多级虚拟节点层次结构实现局部频率分析，采用分解-更新-重构框架，使用拓扑感知图变换器分别建模和融合低频与高频信号。

Result: 在多个真实世界数据集和四个下游任务上的理论证明和实证验证表明，HiFiNet在捕获有效道路网络表示方面表现出优越性能和泛化能力。

Conclusion: HiFiNet通过统一空间和频谱建模，有效解决了道路网络表示学习中的空间-频谱错位问题，为智能交通系统提供了更强大的基础架构。

Abstract: Road networks are critical infrastructures underpinning intelligent transportation systems and their related applications. Effective representation learning of road networks remains challenging due to the complex interplay between spatial structures and frequency characteristics in traffic patterns. Existing graph neural networks for modeling road networks predominantly fall into two paradigms: spatial-based methods that capture local topology but tend to over-smooth representations, and spectral-based methods that analyze global frequency components but often overlook localized variations. This spatial-spectral misalignment limits their modeling capacity for road networks exhibiting both coarse global trends and fine-grained local fluctuations. To bridge this gap, we propose HiFiNet, a novel hierarchical frequency-decomposition graph neural network that unifies spatial and spectral modeling. HiFiNet constructs a multi-level hierarchy of virtual nodes to enable localized frequency analysis, and employs a decomposition-updating-reconstruction framework with a topology-aware graph transformer to separately model and fuse low- and high-frequency signals. Theoretically justified and empirically validated on multiple real-world datasets across four downstream tasks, HiFiNet demonstrates superior performance and generalization ability in capturing effective road network representations.

</details>


### [159] [Spectral Bias Mitigation via xLSTM-PINN: Memory-Gated Representation Refinement for Physics-Informed Learning](https://arxiv.org/abs/2511.12512)
*Ze Tao,Darui Zhao,Fujun Liu,Ke Xu,Xiangsheng Hu*

Main category: cs.LG

TL;DR: 提出了一种基于xLSTM的物理信息神经网络(xLSTM-PINN)，通过门控记忆多尺度特征提取和自适应残差数据加权来解决谱偏差、残差数据不平衡和外推问题。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息学习在PDE求解中存在谱偏差、残差数据不平衡和外推能力弱的问题，需要开发更有效的方法来抑制这些限制。

Method: 结合门控跨尺度记忆、分阶段频率课程和自适应残差重加权，在四个基准测试中验证了方法的有效性。

Result: 显著降低了谱误差和RMSE，拓宽了稳定学习率窗口，提高了高频核权重和可解析带宽，缩短了高波数误差衰减时间和阈值时间。

Conclusion: 该方法在不改变自动微分或物理损失的情况下，有效抑制了谱偏差，拓宽了可解析带宽，提高了准确性、可重复性和可迁移性。

Abstract: Physics-informed learning for PDEs is surging across scientific computing and industrial simulation, yet prevailing methods face spectral bias, residual-data imbalance, and weak extrapolation. We introduce a representation-level spectral remodeling xLSTM-PINN that combines gated-memory multiscale feature extraction with adaptive residual-data weighting to curb spectral bias and strengthen extrapolation. Across four benchmarks, we integrate gated cross-scale memory, a staged frequency curriculum, and adaptive residual reweighting, and verify with analytic references and extrapolation tests, achieving markedly lower spectral error and RMSE and a broader stable learning-rate window. Frequency-domain benchmarks show raised high-frequency kernel weights and a right-shifted resolvable bandwidth, shorter high-k error decay and time-to-threshold, and narrower error bands with lower MSE, RMSE, MAE, and MaxAE. Compared with the baseline PINN, we reduce MSE, RMSE, MAE, and MaxAE across all four benchmarks and deliver cleaner boundary transitions with attenuated high-frequency ripples in both frequency and field maps. This work suppresses spectral bias, widens the resolvable band and shortens the high-k time-to-threshold under the same budget, and without altering AD or physics losses improves accuracy, reproducibility, and transferability.

</details>


### [160] [Regret Guarantees for Linear Contextual Stochastic Shortest Path](https://arxiv.org/abs/2511.12534)
*Dor Polikar,Alon Cohen*

Main category: cs.LG

TL;DR: 提出了线性上下文随机最短路径问题（CSSP），其中MDP通过未知线性函数由上下文决定。设计了LR-CSSP算法，在不知道最优策略期望时间的情况下实现了次线性遗憾界。


<details>
  <summary>Details</summary>
Motivation: 解决在上下文空间中MDP动态未知且最优策略期望时间未知的情况下，如何最小化累积损失到达目标状态的问题。与有限时域MDP不同，CSSP中知识不足可能导致剧集无法终止。

Method: 提出了LR-CSSP算法，通过线性函数逼近处理上下文空间，确保所有剧集在合理时间步内终止，同时实现次线性遗憾。

Result: LR-CSSP实现了$\widetilde{O}(K^{2/3} d^{2/3} |S| |A|^{1/3} B_\star^2 T_\star \log (1/ δ))$的遗憾界，当所有成本超过$\ell_{\min}$时，遗憾界为$\widetilde O(\sqrt{K \cdot d^2 |S|^3 |A| B_\star^3 \log(1/δ)/\ell_{\min}})$。

Conclusion: LR-CSSP能够有效处理连续上下文空间，确保所有剧集终止，并在不知道最优策略期望时间的情况下实现次线性遗憾，解决了CSSP设置中的关键挑战。

Abstract: We define the problem of linear Contextual Stochastic Shortest Path (CSSP), where at the beginning of each episode, the learner observes an adversarially chosen context that determines the MDP through a fixed but unknown linear function. The learner's objective is to reach a designated goal state with minimal expected cumulative loss, despite having no prior knowledge of the transition dynamics, loss functions, or the mapping from context to MDP. In this work, we propose LR-CSSP, an algorithm that achieves a regret bound of $\widetilde{O}(K^{2/3} d^{2/3} |S| |A|^{1/3} B_\star^2 T_\star \log (1/ δ))$, where $K$ is the number of episodes, $d$ is the context dimension, $S$ and $A$ are the sets of states and actions respectively, $B_\star$ bounds the optimal cumulative loss and $T_\star$, unknown to the learner, bounds the expected time for the optimal policy to reach the goal. In the case where all costs exceed $\ell_{\min}$, LR-CSSP attains a regret of $\widetilde O(\sqrt{K \cdot d^2 |S|^3 |A| B_\star^3 \log(1/δ)/\ell_{\min}})$. Unlike in contextual finite-horizon MDPs, where limited knowledge primarily leads to higher losses and regret, in the CSSP setting, insufficient knowledge can also prolong episodes and may even lead to non-terminating episodes. Our analysis reveals that LR-CSSP effectively handles continuous context spaces, while ensuring all episodes terminate within a reasonable number of time steps.

</details>


### [161] [Center-Outward q-Dominance: A Sample-Computable Proxy for Strong Stochastic Dominance in Multi-Objective Optimisation](https://arxiv.org/abs/2511.12545)
*Robin van der Laag,Hao Wang,Thomas Bäck,Yingjie Fan*

Main category: cs.LG

TL;DR: 基于最优输运理论提出中心向外q-支配关系，证明其蕴含强一阶随机支配，开发了基于q-支配的经验检验方法，并在超参数调优和多目标优化算法中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随机多目标优化需要比较多元分布，但现有研究多采用标量化方法，这会丢失信息且不可靠。

Method: 引入基于最优输运理论的中心向外q-支配关系，开发相应的经验检验程序，并推导样本量阈值来控制第一类错误。

Result: 在超参数调优中，当期望超体积指标无法区分时，q-支配仍能比较不同调优器；在NSGA-II算法中用q-支配替换均值选择，在噪声增强的ZDT基准问题上显示出更快的收敛速度。

Conclusion: 中心向外q-支配为寻求真正随机支配解提供了原则性、可处理的基础。

Abstract: Stochastic multi-objective optimization (SMOOP) requires ranking multivariate distributions; yet, most empirical studies perform scalarization, which loses information and is unreliable. Based on the optimal transport theory, we introduce the center-outward q-dominance relation and prove it implies strong first-order stochastic dominance (FSD). Also, we develop an empirical test procedure based on q-dominance, and derive an explicit sample size threshold, $n^*(δ)$, to control the Type I error. We verify the usefulness of our approach in two scenarios: (1) as a ranking method in hyperparameter tuning; (2) as a selection method in multi-objective optimization algorithms. For the former, we analyze the final stochastic Pareto sets of seven multi-objective hyperparameter tuners on the YAHPO-MO benchmark tasks with q-dominance, which allows us to compare these tuners when the expected hypervolume indicator (HVI, the most common performance metric) of the Pareto sets becomes indistinguishable. For the latter, we replace the mean value-based selection in the NSGA-II algorithm with $q$-dominance, which shows a superior convergence rate on noise-augmented ZDT benchmark problems. These results establish center-outward q-dominance as a principled, tractable foundation for seeking truly stochastically dominant solutions for SMOOPs.

</details>


### [162] [CAO: Curvature-Adaptive Optimization via Periodic Low-Rank Hessian Sketching](https://arxiv.org/abs/2511.12548)
*Wenzhang Du*

Main category: cs.LG

TL;DR: 提出一种曲率自适应优化方法，通过周期性构建低秩Hessian子空间来预处理梯度，在尖锐、各向异性区域加速收敛，同时保持标准收敛保证。


<details>
  <summary>Details</summary>
Motivation: 一阶优化器在尖锐、各向异性区域收敛缓慢，需要更高效的曲率自适应方法来加速训练过程。

Method: 周期性通过Hessian-向量积构建低秩Hessian子空间，在该子空间内预处理梯度，正交补空间保持一阶优化。

Result: 在CIFAR-10/100和ResNet-18/34上，该方法比Adam快2.95倍达到预设训练损失阈值，同时保持最终测试精度。

Conclusion: 该方法提供了一种简单有效的曲率自适应优化方案，对草图秩k不敏感，在保持收敛保证的同时显著加速训练。

Abstract: First-order optimizers are reliable but slow in sharp, anisotropic regions. We study a curvature-adaptive method that periodically sketches a low-rank Hessian subspace via Hessian--vector products and preconditions gradients only in that subspace, leaving the orthogonal complement first-order. For L-smooth non-convex objectives, we recover the standard O(1/T) stationarity guarantee with a widened stable stepsize range; under a Polyak--Lojasiewicz (PL) condition with bounded residual curvature outside the sketch, the loss contracts at refresh steps. On CIFAR-10/100 with ResNet-18/34, the method enters the low-loss region substantially earlier: measured by epochs to a pre-declared train-loss threshold (0.75), it reaches the threshold 2.95x faster than Adam on CIFAR-100/ResNet-18, while matching final test accuracy. The approach is one-knob: performance is insensitive to the sketch rank k across {1,3,5}, and k=0 yields a principled curvature-free ablation. We release anonymized logs and scripts that regenerate all figures and tables.

</details>


### [163] [Training Instabilities Induce Flatness Bias in Gradient Descent](https://arxiv.org/abs/2511.12558)
*Lawrence Wang,Stephen J. Roberts*

Main category: cs.LG

TL;DR: 梯度下降训练中的不稳定性通过Hessian矩阵特征向量的旋转极性机制，隐式地推动参数向损失函数更平坦的区域移动，从而改善泛化性能。


<details>
  <summary>Details</summary>
Motivation: 传统分析认为学习率低于Hessian矩阵最大特征值（锐度）时训练才稳定，但现代深度网络的最佳性能往往出现在这个阈值之外，需要理解这种不稳定性的积极作用。

Method: 提出旋转极性特征向量(RPE)机制，分析梯度下降不稳定性导致Hessian矩阵主导特征向量旋转的几何现象，并扩展到随机梯度下降和Adam优化器。

Result: 训练不稳定性通过RPE机制促进探索，理论证明能导向更平坦的最小值，在随机梯度下降中这种平坦化效应超过小批量噪声的影响。

Conclusion: 训练不稳定性在深度学习中具有建设性作用，通过隐式偏置推动参数向平坦区域移动，从而改善泛化性能。

Abstract: Classical analyses of gradient descent (GD) define a stability threshold based on the largest eigenvalue of the loss Hessian, often termed sharpness. When the learning rate lies below this threshold, training is stable and the loss decreases monotonically. Yet, modern deep networks often achieve their best performance beyond this regime.
  We demonstrate that such instabilities induce an implicit bias in GD, driving parameters toward flatter regions of the loss landscape and thereby improving generalization. The key mechanism is the Rotational Polarity of Eigenvectors (RPE), a geometric phenomenon in which the leading eigenvectors of the Hessian rotate during training instabilities. These rotations, which increase with learning rates, promote exploration and provably lead to flatter minima.
  This theoretical framework extends to stochastic GD, where instability-driven flattening persists and its empirical effects outweigh minibatch noise. Finally, we show that restoring instabilities in Adam further improves generalization.
  Together, these results establish and understand the constructive role of training instabilities in deep learning.

</details>


### [164] [Linear time small coresets for k-mean clustering of segments with applications](https://arxiv.org/abs/2511.12564)
*David Denisov,Shlomi Dolev,Dan Felmdan,Michael Segal*

Main category: cs.LG

TL;DR: 提出了第一个能够处理任意输入线段的核心集构造方法，用于线段k-means聚类问题，在常数k和ε下生成大小为O(log²n)的核心集，计算时间为O(nd)。


<details>
  <summary>Details</summary>
Motivation: 研究线段集合的k-means聚类问题，旨在找到k个中心点来最小化所有线段到最近中心点的总距离积分。需要高效处理大规模线段数据，支持流式、分布式和并行计算。

Method: 构建ε-核心集，这是一个加权子集，能够在任意k个中心点的情况下，以1±ε的因子近似原始线段集合的总距离。核心集大小为O(log²n)，计算复杂度为O(nd)。

Result: 实验验证了方法的有效性，包括实时视频跟踪应用，显示出显著的加速效果且聚类精度损失最小。理论保证得到实践确认。

Conclusion: 提出的核心集构造方法在理论和实践上都表现出色，为线段k-means聚类问题提供了首个可证明处理任意输入线段的高效解决方案。

Abstract: We study the $k$-means problem for a set $\mathcal{S} \subseteq \mathbb{R}^d$ of $n$ segments, aiming to find $k$ centers $X \subseteq \mathbb{R}^d$ that minimize
  $D(\mathcal{S},X) := \sum_{S \in \mathcal{S}} \min_{x \in X} D(S,x)$, where $D(S,x) := \int_{p \in S} |p - x| dp$
  measures the total distance from each point along a segment to a center. Variants of this problem include handling outliers, employing alternative distance functions such as M-estimators, weighting distances to achieve balanced clustering, or enforcing unique cluster assignments. For any $\varepsilon > 0$, an $\varepsilon$-coreset is a weighted subset $C \subseteq \mathbb{R}^d$ that approximates $D(\mathcal{S},X)$ within a factor of $1 \pm \varepsilon$ for any set of $k$ centers, enabling efficient streaming, distributed, or parallel computation. We propose the first coreset construction that provably handles arbitrary input segments. For constant $k$ and $\varepsilon$, it produces a coreset of size $O(\log^2 n)$ computable in $O(nd)$ time. Experiments, including a real-time video tracking application, demonstrate substantial speedups with minimal loss in clustering accuracy, confirming both the practical efficiency and theoretical guarantees of our method.

</details>


### [165] [Enhancing Machine Learning Model Efficiency through Quantization and Bit Depth Optimization: A Performance Analysis on Healthcare Data](https://arxiv.org/abs/2511.12568)
*Mitul Goswami,Romit Chatterjee*

Main category: cs.LG

TL;DR: 通过量化和位深度优化技术优化复杂学习模型，显著降低时间复杂度同时保持模型效率，在医疗数据集上验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂模型执行时间过长的问题，通过优化技术减少计算复杂度同时保持模型性能。

Method: 使用Logistic回归模型，在医疗数据集上应用量化和位深度优化策略，将输入数据从float64降级到float32和int32。

Result: 时间复杂度显著降低，模型精度仅有轻微下降，展示了先进的优化方法。

Conclusion: 这些优化技术的影响取决于一组参数，需要根据具体情况调整。

Abstract: This research aims to optimize intricate learning models by implementing quantization and bit-depth optimization techniques. The objective is to significantly cut time complexity while preserving model efficiency, thus addressing the challenge of extended execution times in intricate models. Two medical datasets were utilized as case studies to apply a Logistic Regression (LR) machine learning model. Using efficient quantization and bit depth optimization strategies the input data is downscaled from float64 to float32 and int32. The results demonstrated a significant reduction in time complexity, with only a minimal decrease in model accuracy post-optimization, showcasing the state-of-the-art optimization approach. This comprehensive study concludes that the impact of these optimization techniques varies depending on a set of parameters.

</details>


### [166] [LMM-IR: Large-Scale Netlist-Aware Multimodal Framework for Static IR-Drop Prediction](https://arxiv.org/abs/2511.12581)
*Kai Ma,Zhen Wang,Hongquan He,Qi Xu,Tinghuan Chen,Hao Geng*

Main category: cs.LG

TL;DR: 提出了一种新颖的多模态方法，通过大规模网表变换器高效处理SPICE文件，将网表拓扑表示为3D点云，实现静态电压降预测，在ICCAD 2023竞赛中取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 静态IR压降分析在芯片设计中至关重要但耗时，需要数小时完成，且解决IR压降违规需要迭代分析，计算负担重。因此需要快速准确的IR压降预测来减少芯片设计时间。

Method: 采用多模态方法，通过大规模网表变换器处理SPICE文件，将网表拓扑表示为3D点云表示，能够高效处理包含数十万到数百万节点的网表。所有类型数据（网表文件和图像数据）都被编码为潜在空间特征并输入模型进行静态电压降预测。

Result: 实验结果表明，该方法在ICCAD 2023竞赛的获胜团队和最先进算法中实现了最佳的F1分数和最低的MAE。

Conclusion: 该方法能够集成多模态数据进行互补预测，为芯片设计中的静态IR压降分析提供了高效准确的解决方案。

Abstract: Static IR drop analysis is a fundamental and critical task in the field of chip design. Nevertheless, this process can be quite time-consuming, potentially requiring several hours. Moreover, addressing IR drop violations frequently demands iterative analysis, thereby causing the computational burden. Therefore, fast and accurate IR drop prediction is vital for reducing the overall time invested in chip design. In this paper, we firstly propose a novel multimodal approach that efficiently processes SPICE files through large-scale netlist transformer (LNT). Our key innovation is representing and processing netlist topology as 3D point cloud representations, enabling efficient handling of netlist with up to hundreds of thousands to millions nodes. All types of data, including netlist files and image data, are encoded into latent space as features and fed into the model for static voltage drop prediction. This enables the integration of data from multiple modalities for complementary predictions. Experimental results demonstrate that our proposed algorithm can achieve the best F1 score and the lowest MAE among the winning teams of the ICCAD 2023 contest and the state-of-the-art algorithms.

</details>


### [167] [Symmetry-Aware Graph Metanetwork Autoencoders: Model Merging through Parameter Canonicalization](https://arxiv.org/abs/2511.12601)
*Odysseas Boufalis,Jorge Carrasco-Pollo,Joshua Rosenthal,Eduardo Terres-Caballero,Alejandro García-Castellanos*

Main category: cs.LG

TL;DR: Scale Graph Metanetworks (ScaleGMNs) 通过构建对排列和参数缩放变换等变的架构，利用神经网络参数化的内在对称性，将相似网络映射到同一损失盆地，从而促进模型合并。


<details>
  <summary>Details</summary>
Motivation: 神经网络参数化存在内在对称性，导致损失景观中出现多个等价最小值。先前工作仅处理排列对称性，本文旨在同时纳入缩放对称性，更全面地利用这些对称性。

Method: 提出使用 ScaleGMNs 作为不变编码器的自编码器框架，该架构对排列和参数缩放变换都是等变的，无需显式求解组合分配问题。

Result: 实验结果表明，该方法能够在排列和缩放对称性下对齐隐式神经表示和卷积神经网络，使相似网络自然收敛于同一盆地。

Conclusion: ScaleGMNs 通过同时处理排列和缩放对称性，有效促进了模型合并，实现了平滑的线性插值并避免了高损失区域。

Abstract: Neural network parameterizations exhibit inherent symmetries that yield multiple equivalent minima within the loss landscape. Scale Graph Metanetworks (ScaleGMNs) explicitly leverage these symmetries by proposing an architecture equivariant to both permutation and parameter scaling transformations. Previous work by Ainsworth et al. (2023) addressed permutation symmetries through a computationally intensive combinatorial assignment problem, demonstrating that leveraging permutation symmetries alone can map networks into a shared loss basin. In this work, we extend their approach by also incorporating scaling symmetries, presenting an autoencoder framework utilizing ScaleGMNs as invariant encoders. Experimental results demonstrate that our method aligns Implicit Neural Representations (INRs) and Convolutional Neural Networks (CNNs) under both permutation and scaling symmetries without explicitly solving the assignment problem. This approach ensures that similar networks naturally converge within the same basin, facilitating model merging, i.e., smooth linear interpolation while avoiding regions of high loss. The code is publicly available on our GitHub repository.

</details>


### [168] [PID-controlled Langevin Dynamics for Faster Sampling of Generative Models](https://arxiv.org/abs/2511.12603)
*Hongyi Chen,Jianhai Shu,Jingtao Ding,Yong Li,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: PIDLD是一种基于控制理论的Langevin动力学采样加速算法，通过结合历史梯度和梯度趋势来减少迭代次数，提高采样效率。


<details>
  <summary>Details</summary>
Motivation: Langevin动力学采样存在生成速度极低的问题，受限于需要大量细粒度迭代才能收敛到目标分布。

Method: 将采样过程重新解释为控制理论问题，将能量梯度视为反馈信号，结合历史梯度（积分项）和梯度趋势（微分项）来高效穿越能量景观并自适应稳定。

Result: 在图像生成和推理任务上的实验表明，PIDLD能够以更少的步骤获得更高质量的样本。

Conclusion: PIDLD使基于Langevin的生成模型在效率关键应用中更加实用，无需额外训练、数据集或先验信息即可集成到任何Langevin方法中。

Abstract: Langevin dynamics sampling suffers from extremely low generation speed, fundamentally limited by numerous fine-grained iterations to converge to the target distribution. We introduce PID-controlled Langevin Dynamics (PIDLD), a novel sampling acceleration algorithm that reinterprets the sampling process using control-theoretic principles. By treating energy gradients as feedback signals, PIDLD combines historical gradients (the integral term) and gradient trends (the derivative term) to efficiently traverse energy landscapes and adaptively stabilize, thereby significantly reducing the number of iterations required to produce high-quality samples. Our approach requires no additional training, datasets, or prior information, making it immediately integrable with any Langevin-based method. Extensive experiments across image generation and reasoning tasks demonstrate that PIDLD achieves higher quality with fewer steps, making Langevin-based generative models more practical for efficiency-critical applications. The implementation can be found at \href{https://github.com/tsinghua-fib-lab/PIDLD}{https://github.com/tsinghua-fib-lab/PIDLD}.

</details>


### [169] [FedTopo: Topology-Informed Representation Alignment in Federated Learning under Non-I.I.D. Conditions](https://arxiv.org/abs/2511.12628)
*Ke Hu,Liyao Xiang,Peng Tang,Weidong Qiu*

Main category: cs.LG

TL;DR: FedTopo是一个联邦学习框架，通过拓扑引导的块筛选和拓扑嵌入来解决异构数据下的表示对齐问题，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习模型在异构客户端数据下性能下降，因为特征表示发散且像素级目标无法捕捉高维视觉任务所需的全局拓扑结构。

Method: 提出三个核心组件：拓扑引导块筛选自动选择最具拓扑信息的块；拓扑嵌入量化每个客户端的拓扑信息；拓扑对齐损失在优化过程中保持客户端与全局模型的拓扑一致性。

Result: 在Fashion-MNIST、CIFAR-10和CIFAR-100数据集上的实验表明，FedTopo在四种非I.I.D.分区下都能加速收敛并提高准确率。

Conclusion: FedTopo通过利用拓扑信息有效解决了联邦学习中的异构数据问题，实现了更好的表示对齐和模型性能。

Abstract: Current federated-learning models deteriorate under heterogeneous (non-I.I.D.) client data, as their feature representations diverge and pixel- or patch-level objectives fail to capture the global topology which is essential for high-dimensional visual tasks. We propose FedTopo, a framework that integrates Topological-Guided Block Screening (TGBS) and Topological Embedding (TE) to leverage topological information, yielding coherently aligned cross-client representations by Topological Alignment Loss (TAL). First, Topology-Guided Block Screening (TGBS) automatically selects the most topology-informative block, i.e., the one with maximal topological separability, whose persistence-based signatures best distinguish within- versus between-class pairs, ensuring that subsequent analysis focuses on topology-rich features. Next, this block yields a compact Topological Embedding, which quantifies the topological information for each client. Finally, a Topological Alignment Loss (TAL) guides clients to maintain topological consistency with the global model during optimization, reducing representation drift across rounds. Experiments on Fashion-MNIST, CIFAR-10, and CIFAR-100 under four non-I.I.D. partitions show that FedTopo accelerates convergence and improves accuracy over strong baselines.

</details>


### [170] [NFQ2.0: The CartPole Benchmark Revisited](https://arxiv.org/abs/2511.12644)
*Sascha Lange,Roland Hafner,Martin Riedmiller*

Main category: cs.LG

TL;DR: 本文重新评估了20年前的神经拟合Q迭代(NFQ)算法，提出了现代化变体NFQ2.0，通过消融研究确定了关键设计决策和超参数，提高了在工业环境中的可重复性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: NFQ是深度强化学习的先驱方法，但其需要大量调参且在真实世界控制问题中难以复现。研究旨在改进学习过程的可重复性和鲁棒性。

Method: 提出了NFQ2.0现代化变体，在标准工业组件构建的真实CartPole系统上进行测试，通过消融研究分析关键设计决策和超参数。

Result: NFQ2.0在性能和稳定性上优于原始版本，确定了增强学习过程的关键因素。

Conclusion: 研究结果为从业者在工业环境中更有效地应用深度强化学习提供了实用指导，有助于复现和改进结果。

Abstract: This article revisits the 20-year-old neural fitted Q-iteration (NFQ) algorithm on its classical CartPole benchmark. NFQ was a pioneering approach towards modern Deep Reinforcement Learning (Deep RL) in applying multi-layer neural networks to reinforcement learning for real-world control problems. We explore the algorithm's conceptual simplicity and its transition from online to batch learning, which contributed to its stability. Despite its initial success, NFQ required extensive tuning and was not easily reproducible on real-world control problems. We propose a modernized variant NFQ2.0 and apply it to the CartPole task, concentrating on a real-world system build from standard industrial components, to investigate and improve the learning process's repeatability and robustness. Through ablation studies, we highlight key design decisions and hyperparameters that enhance performance and stability of NFQ2.0 over the original variant. Finally, we demonstrate how our findings can assist practitioners in reproducing and improving results and applying deep reinforcement learning more effectively in industrial contexts.

</details>


### [171] [Sample Complexity of Agnostic Multiclass Classification: Natarajan Dimension Strikes Back](https://arxiv.org/abs/2511.12659)
*Alon Cohen,Liad Erez,Steve Hanneke,Tomer Koren,Yishay Mansour,Shay Moran,Qian Zhang*

Main category: cs.LG

TL;DR: 多类别PAC学习的样本复杂度由两个维度共同决定：DS^{1.5}/ε + Nat/ε²，其中DS维度控制高精度区域，Natarajan维度控制渐近行为。


<details>
  <summary>Details</summary>
Motivation: 扩展二元PAC学习的VC维度理论到多类别分类，理解多类别学习中不同维度参数的作用机制。

Method: 提出基于自适应乘性权重的在线算法，进行标签空间缩减，不同于传统的均匀收敛或可简化案例方法。

Result: 证明了近乎紧致的样本复杂度界限，显示多类别学习需要两个结构参数，不同于二元或在线分类的单一维度控制。

Conclusion: 多类别学习本质上涉及两个结构参数，DS维度控制高精度学习，Natarajan维度决定小ε时的渐近行为。

Abstract: The fundamental theorem of statistical learning states that binary PAC learning is governed by a single parameter -- the Vapnik-Chervonenkis (VC) dimension -- which determines both learnability and sample complexity. Extending this to multiclass classification has long been challenging, since Natarajan's work in the late 80s proposing the Natarajan dimension (Nat) as a natural analogue of VC. Daniely and Shalev-Shwartz (2014) introduced the DS dimension, later shown by Brukhim et al. (2022) to characterize multiclass learnability. Brukhim et al. also showed that Nat and DS can diverge arbitrarily, suggesting that multiclass learning is governed by DS rather than Nat. We show that agnostic multiclass PAC sample complexity is in fact governed by two distinct dimensions. Specifically, we prove nearly tight agnostic sample complexity bounds that, up to log factors, take the form $\frac{DS^{1.5}}ε + \frac{Nat}{ε^2}$ where $ε$ is the excess risk. This bound is tight up to a $\sqrt{DS}$ factor in the first term, nearly matching known $Nat/ε^2$ and $DS/ε$ lower bounds. The first term reflects the DS-controlled regime, while the second shows that the Natarajan dimension still dictates asymptotic behavior for small $ε$. Thus, unlike binary or online classification -- where a single dimension (VC or Littlestone) controls both phenomena -- multiclass learning inherently involves two structural parameters. Our technical approach departs from traditional agnostic learning methods based on uniform convergence or reductions to realizable cases. A key ingredient is a novel online procedure based on a self-adaptive multiplicative-weights algorithm performing a label-space reduction, which may be of independent interest.

</details>


### [172] [FLClear: Visually Verifiable Multi-Client Watermarking for Federated Learning](https://arxiv.org/abs/2511.12663)
*Chen Gu,Yingying Sun,Yifan She,Donghui Hu*

Main category: cs.LG

TL;DR: FLClear是一个联邦学习水印框架，通过转置模型和对比学习实现无碰撞水印聚合、增强水印安全性和可视化所有权验证。


<details>
  <summary>Details</summary>
Motivation: 保护联邦学习中客户端模型的知识产权，防止中心服务器恶意篡改全局模型或错误声称所有权，解决现有水印方法存在的水印碰撞、安全性不足和验证机制不直观等问题。

Method: 引入转置模型与对比学习联合优化，整合水印和主任务目标；验证时从转置模型重构水印，通过视觉检查和结构相似性指标进行评估。

Result: 在多种数据集、聚合方案和攻击场景下的综合实验表明，FLClear始终优于最先进的联邦学习水印方法。

Conclusion: FLClear框架有效解决了联邦学习中的知识产权保护问题，实现了无碰撞水印聚合、增强安全性和直观验证。

Abstract: Federated learning (FL) enables multiple clients to collaboratively train a shared global model while preserving the privacy of their local data. Within this paradigm, the intellectual property rights (IPR) of client models are critical assets that must be protected. In practice, the central server responsible for maintaining the global model may maliciously manipulate the global model to erase client contributions or falsely claim sole ownership, thereby infringing on clients' IPR. Watermarking has emerged as a promising technique for asserting model ownership and protecting intellectual property. However, existing FL watermarking approaches remain limited, suffering from potential watermark collisions among clients, insufficient watermark security, and non-intuitive verification mechanisms. In this paper, we propose FLClear, a novel framework that simultaneously achieves collision-free watermark aggregation, enhanced watermark security, and visually interpretable ownership verification. Specifically, FLClear introduces a transposed model jointly optimized with contrastive learning to integrate the watermarking and main task objectives. During verification, the watermark is reconstructed from the transposed model and evaluated through both visual inspection and structural similarity metrics, enabling intuitive and quantitative ownership verification. Comprehensive experiments conducted over various datasets, aggregation schemes, and attack scenarios demonstrate the effectiveness of FLClear and confirm that it consistently outperforms state-of-the-art FL watermarking methods.

</details>


### [173] [Attention-Enhanced Convolutional Autoencoder and Structured Delay Embeddings for Weather Prediction](https://arxiv.org/abs/2511.12682)
*Amirpasha Hedayat,Karthik Duraisamy*

Main category: cs.LG

TL;DR: 本文提出了一个高效的降阶建模框架用于短期天气预报，通过卷积自编码器降低天气数据维度，在延迟嵌入的潜空间中使用线性算子捕捉动态特性。该框架在训练数据期间表现良好，但在泛化到未来状态时存在局限性。


<details>
  <summary>Details</summary>
Motivation: 天气预报是一个复杂的非线性混沌高维系统预测问题。现有AI模型需要大量计算资源，而本文旨在开发一个优先考虑效率同时保持合理准确性的降阶建模框架。

Method: 开发了基于ResNet的卷积自编码器，增强块注意力模块来降低高维天气数据的维度。然后在延迟嵌入的潜空间中学习线性算子来有效捕捉动态特性。使用ERA5再分析数据集进行验证。

Result: 该框架在训练数据分布内表现良好，能够有效预测训练数据期间的天气模式。但在泛化到未来状态时存在重要限制，特别是在训练窗口之外维持预测准确性方面。投影误差而非推断误差是主要瓶颈。

Conclusion: 天气系统在适当构建的嵌入空间中可以通过线性操作有效捕捉强时间相关性。这些发现揭示了混沌系统降阶建模的关键挑战，并指出了将高效降阶模型与更复杂AI架构结合的混合方法的机会，特别是在计算效率至关重要的长期气候建模应用中。

Abstract: Weather prediction is a quintessential problem involving the forecasting of a complex, nonlinear, and chaotic high-dimensional dynamical system. This work introduces an efficient reduced-order modeling (ROM) framework for short-range weather prediction and investigates fundamental questions in dimensionality reduction and reduced order modeling of such systems. Unlike recent AI-driven models, which require extensive computational resources, our framework prioritizes efficiency while achieving reasonable accuracy. Specifically, a ResNet-based convolutional autoencoder augmented by block attention modules is developed to reduce the dimensionality of high-dimensional weather data. Subsequently, a linear operator is learned in the time-delayed embedding of the latent space to efficiently capture the dynamics. Using the ERA5 reanalysis dataset, we demonstrate that this framework performs well in-distribution as evidenced by effectively predicting weather patterns within training data periods. We also identify important limitations in generalizing to future states, particularly in maintaining prediction accuracy beyond the training window. Our analysis reveals that weather systems exhibit strong temporal correlations that can be effectively captured through linear operations in an appropriately constructed embedding space, and that projection error rather than inference error is the main bottleneck. These findings shed light on some key challenges in reduced-order modeling of chaotic systems and point toward opportunities for hybrid approaches that combine efficient reduced-order models as baselines with more sophisticated AI architectures, particularly for applications in long-term climate modeling where computational efficiency is paramount.

</details>


### [174] [A Closer Look at Personalized Fine-Tuning in Heterogeneous Federated Learning](https://arxiv.org/abs/2511.12695)
*Minghui Chen,Hrad Ghoukasian,Ruinan Jin,Zehua Wang,Sai Praneeth Karimireddy,Xiaoxiao Li*

Main category: cs.LG

TL;DR: 将集中式学习中的线性探测后微调(LP-FT)方法适配到联邦学习环境，有效平衡个性化与泛化性，缓解联邦特征扭曲问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非独立同分布数据下难以平衡全局泛化与本地个性化，现有个性化微调方法容易过拟合或无法应对领域偏移。

Method: 采用线性探测后微调(LP-FT)策略，分阶段更新参数：先固定特征提取器只训练分类头，然后进行全网络微调。

Result: 在7个数据集和6种个性化微调变体上的系统评估显示，LP-FT在平衡个性化与泛化方面表现优越。

Conclusion: LP-FT通过分阶段参数更新有效缓解联邦特征扭曲，在部分特征重叠、协变量-概念偏移等条件下优于标准微调，为联邦学习中的鲁棒个性化提供了实用指南。

Abstract: Federated Learning (FL) enables decentralized, privacy-preserving model training but struggles to balance global generalization and local personalization due to non-identical data distributions across clients. Personalized Fine-Tuning (PFT), a popular post-hoc solution, fine-tunes the final global model locally but often overfits to skewed client distributions or fails under domain shifts. We propose adapting Linear Probing followed by full Fine-Tuning (LP-FT), a principled centralized strategy for alleviating feature distortion (Kumar et al., 2022), to the FL setting. Through systematic evaluation across seven datasets and six PFT variants, we demonstrate LP-FT's superiority in balancing personalization and generalization. Our analysis uncovers federated feature distortion, a phenomenon where local fine-tuning destabilizes globally learned features, and theoretically characterizes how LP-FT mitigates this via phased parameter updates. We further establish conditions (e.g., partial feature overlap, covariate-concept shift) under which LP-FT outperforms standard fine-tuning, offering actionable guidelines for deploying robust personalization in FL.

</details>


### [175] [Beyond Fixed Tasks: Unsupervised Environment Design for Task-Level Pairs](https://arxiv.org/abs/2511.12706)
*Daniel Furelos-Blanco,Charles Pert,Frederik Kelbel,Alex F. Spies,Alessandra Russo,Michael Dennis*

Main category: cs.LG

TL;DR: ATLAS是一种新颖的方法，通过联合自动课程设计生成任务和关卡对，解决了在复杂环境中训练智能体遵循复杂指令的挑战。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中训练通用智能体遵循复杂指令是强化学习的核心挑战。随机采样任务-关卡对通常会产生无法解决的组合，因此需要共同设计任务和关卡。

Method: ATLAS基于无监督环境设计(UED)，自动生成可解决但具有挑战性的任务-关卡对用于策略训练。该方法利用任务和关卡结构的突变来加速收敛。

Result: 实验表明，ATLAS在Minigrid环境中的表现远超随机采样方法，特别是在采样可解决对的可能性较低时。利用任务和关卡结构的突变能加速收敛到高性能策略。

Conclusion: ATLAS通过联合自动课程设计任务和关卡，有效解决了复杂环境中训练智能体的挑战，为领域进步提供了新的评估套件和方法。

Abstract: Training general agents to follow complex instructions (tasks) in intricate environments (levels) remains a core challenge in reinforcement learning. Random sampling of task-level pairs often produces unsolvable combinations, highlighting the need to co-design tasks and levels. While unsupervised environment design (UED) has proven effective at automatically designing level curricula, prior work has only considered a fixed task. We present ATLAS (Aligning Tasks and Levels for Autocurricula of Specifications), a novel method that generates joint autocurricula over tasks and levels. Our approach builds upon UED to automatically produce solvable yet challenging task-level pairs for policy training. To evaluate ATLAS and drive progress in the field, we introduce an evaluation suite that models tasks as reward machines in Minigrid levels. Experiments demonstrate that ATLAS vastly outperforms random sampling approaches, particularly when sampling solvable pairs is unlikely. We further show that mutations leveraging the structure of both tasks and levels accelerate convergence to performant policies.

</details>


### [176] [Adaptive Graph Rewiring to Mitigate Over-Squashing in Mesh-Based GNNs for Fluid Dynamics Simulations](https://arxiv.org/abs/2511.12709)
*Sangwoo Seo,Hyunsung Kim,Jiwan Kim,Chanyoung Park*

Main category: cs.LG

TL;DR: 提出AdaMeshNet框架，在基于网格的图神经网络中引入自适应重连过程，通过计算瓶颈节点的重连延迟分数来动态选择重连时机，解决传统重连方法忽略物理交互渐进传播的问题。


<details>
  <summary>Details</summary>
Motivation: 传统网格重连方法在GNN应用前完成所有重连操作，假设远距离节点间瞬时交互，忽略了物理交互的渐进传播特性和粒子间距离信息，导致物理不真实性。

Method: AdaMeshNet在消息传递过程中引入自适应重连，基于最短路径距离和速度差计算瓶颈节点的重连延迟分数，动态选择重连层数，实现网格图的自适应重连。

Result: 在基于网格的流体模拟实验中，AdaMeshNet优于传统重连方法，能有效建模物理交互的序列特性，实现更准确的预测。

Conclusion: 自适应重连框架能够更好地捕捉长距离物理交互，解决网格细化导致的过压缩问题，为基于网格的GNN流体模拟提供了更物理真实的解决方案。

Abstract: Mesh-based simulation using Graph Neural Networks (GNNs) has been recognized as a promising approach for modeling fluid dynamics. However, the mesh refinement techniques which allocate finer resolution to regions with steep gradients can induce the over-squashing problem in mesh-based GNNs, which prevents the capture of long-range physical interactions. Conventional graph rewiring methods attempt to alleviate this issue by adding new edges, but they typically complete all rewiring operations before applying them to the GNN. These approaches are physically unrealistic, as they assume instantaneous interactions between distant nodes and disregard the distance information between particles. To address these limitations, we propose a novel framework, called Adaptive Graph Rewiring in Mesh-Based Graph Neural Networks (AdaMeshNet), that introduces an adaptive rewiring process into the message-passing procedure to model the gradual propagation of physical interactions. Our method computes a rewiring delay score for bottleneck nodes in the mesh graph, based on the shortest-path distance and the velocity difference. Using this score, it dynamically selects the message-passing layer at which new edges are rewired, which can lead to adaptive rewiring in a mesh graph. Extensive experiments on mesh-based fluid simulations demonstrate that AdaMeshNet outperforms conventional rewiring methods, effectively modeling the sequential nature of physical interactions and enabling more accurate predictions.

</details>


### [177] [Oxytrees: Model Trees for Bipartite Learning](https://arxiv.org/abs/2511.12713)
*Pedro Ilídio,Felipe Kenji Nakano,Alireza Gharahighehi,Robbe D'hondt,Ricardo Cerri,Celine Vens*

Main category: cs.LG

TL;DR: 提出了Oxytrees：基于代理的双聚类模型树，用于二分学习任务，显著提升训练速度而不损失预测性能


<details>
  <summary>Details</summary>
Motivation: 当前二分学习方法存在局限性，要么针对特定应用缺乏通用性，要么存在可扩展性问题

Method: 使用行和列代理矩阵压缩交互矩阵，提出新的叶节点分配算法，在叶节点使用Kronecker积核的线性模型

Result: 在15个数据集上相比现有最优方法，训练速度提升高达30倍，在大多数评估设置中表现竞争或更优，特别是在归纳设置中

Conclusion: Oxytrees在保持预测性能的同时显著提升了训练效率，并提供了可复现研究的Python API

Abstract: Bipartite learning is a machine learning task that aims to predict interactions between pairs of instances. It has been applied to various domains, including drug-target interactions, RNA-disease associations, and regulatory network inference. Despite being widely investigated, current methods still present drawbacks, as they are often designed for a specific application and thus do not generalize to other problems or present scalability issues. To address these challenges, we propose Oxytrees: proxy-based biclustering model trees. Oxytrees compress the interaction matrix into row- and column-wise proxy matrices, significantly reducing training time without compromising predictive performance. We also propose a new leaf-assignment algorithm that significantly reduces the time taken for prediction. Finally, Oxytrees employ linear models using the Kronecker product kernel in their leaves, resulting in shallower trees and thus even faster training. Using 15 datasets, we compared the predictive performance of ensembles of Oxytrees with that of the current state-of-the-art. We achieved up to 30-fold improvement in training times compared to state-of-the-art biclustering forests, while demonstrating competitive or superior performance in most evaluation settings, particularly in the inductive setting. Finally, we provide an intuitive Python API to access all datasets, methods and evaluation measures used in this work, thus enabling reproducible research in this field.

</details>


### [178] [On Robustness of Linear Classifiers to Targeted Data Poisoning](https://arxiv.org/abs/2511.12722)
*Nakshatra Gupta,Sumanth Prabhu,Supratik Chakraborty,R Venkatesh*

Main category: cs.LG

TL;DR: 本文提出了一种自动测量数据集对标签扰动型数据投毒攻击鲁棒性的方法，通过计算鲁棒性的上下界来评估模型安全性。


<details>
  <summary>Details</summary>
Motivation: 数据投毒攻击会破坏学习模型的可信度，而手动检测投毒在大型训练数据集中很困难，因此需要自动评估数据集对这类攻击的鲁棒性。

Method: 在攻击者只能扰动训练数据标签且仅了解受害者模型假设空间的威胁模型下，提出计算鲁棒性上下界的技术，即使对于线性分类器，寻找精确鲁棒性也是NP完全问题。

Result: 实验证明该方法能高效计算多个公开数据集的鲁棒性边界，超过这些边界的投毒会显著影响测试点分类，且比现有技术能处理更多情况。

Conclusion: 该方法为评估数据集对标签扰动型数据投毒攻击的鲁棒性提供了实用工具，能够识别关键的安全边界。

Abstract: Data poisoning is a training-time attack that undermines the trustworthiness of learned models. In a targeted data poisoning attack, an adversary manipulates the training dataset to alter the classification of a targeted test point. Given the typically large size of training dataset, manual detection of poisoning is difficult. An alternative is to automatically measure a dataset's robustness against such an attack, which is the focus of this paper. We consider a threat model wherein an adversary can only perturb the labels of the training dataset, with knowledge limited to the hypothesis space of the victim's model. In this setting, we prove that finding the robustness is an NP-Complete problem, even when hypotheses are linear classifiers. To overcome this, we present a technique that finds lower and upper bounds of robustness. Our implementation of the technique computes these bounds efficiently in practice for many publicly available datasets. We experimentally demonstrate the effectiveness of our approach. Specifically, a poisoning exceeding the identified robustness bounds significantly impacts test point classification. We are also able to compute these bounds in many more cases where state-of-the-art techniques fail.

</details>


### [179] [LAYA: Layer-wise Attention Aggregation for Interpretable Depth-Aware Neural Networks](https://arxiv.org/abs/2511.12723)
*Gennaro Vessio*

Main category: cs.LG

TL;DR: LAYA是一种新型输出头，通过注意力机制动态聚合深度神经网络中间层的表示，而不是仅使用最后一层表示，在保持性能的同时提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统深度神经网络仅依赖最后一层隐藏表示进行预测，但中间层包含从低层模式到高层抽象的丰富互补信息，这些信息通常被丢弃。

Method: 引入LAYA（Layer-wise Attention Aggregator），通过学习输入条件化的注意力权重来动态聚合各层特征表示，形成架构无关的预测机制。

Result: 在视觉和语言基准测试中，LAYA始终匹配或优于标准输出头，准确率相对提升约1个百分点，同时提供可解释的层归因分数。

Conclusion: LAYA通过直接利用模型计算过程提供可解释性信号，无需外部事后解释，为深度神经网络预测提供了更全面的语义理解和解释能力。

Abstract: Deep neural networks typically rely on the representation produced by their final hidden layer to make predictions, implicitly assuming that this single vector fully captures the semantics encoded across all preceding transformations. However, intermediate layers contain rich and complementary information -- ranging from low-level patterns to high-level abstractions -- that is often discarded when the decision head depends solely on the last representation. This paper revisits the role of the output layer and introduces LAYA (Layer-wise Attention Aggregator), a novel output head that dynamically aggregates internal representations through attention. Instead of projecting only the deepest embedding, LAYA learns input-conditioned attention weights over layer-wise features, yielding an interpretable and architecture-agnostic mechanism for synthesizing predictions. Experiments on vision and language benchmarks show that LAYA consistently matches or improves the performance of standard output heads, with relative gains of up to about one percentage point in accuracy, while providing explicit layer-attribution scores that reveal how different abstraction levels contribute to each decision. Crucially, these interpretability signals emerge directly from the model's computation, without any external post hoc explanations. The code to reproduce LAYA is publicly available at: https://github.com/gvessio/LAYA.

</details>


### [180] [Convolutional Model Trees](https://arxiv.org/abs/2511.12725)
*William Ward Armstrong*

Main category: cs.LG

TL;DR: 提出了一种构建模型树森林的方法，通过降采样、确定超平面、应用卷积处理图像畸变，并创建模型树森林来提高精度和平滑拟合。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够处理图像函数样本的方法，特别是针对训练图像的小畸变和更大畸变（如任意旋转或视角变化），同时提供连续可微的近似。

Method: 包括图像降采样、确定树的超平面、对超平面应用卷积以处理小畸变、创建模型树森林以提高精度，以及建立像素、超平面系数和叶函数系数之间的1对1对应关系。

Result: 该方法能够处理图像的小畸变和更大畸变，并通过理论方法平滑森林输出以产生连续可微的近似。

Conclusion: 提出的训练程序被证明是收敛的，该方法为处理图像函数样本提供了一种有效的框架，特别是在处理畸变和实现平滑拟合方面。

Abstract: A method for creating a forest of model trees to fit samples of a function defined on images is described in several steps: down-sampling the images, determining a tree's hyperplanes, applying convolutions to the hyperplanes to handle small distortions of training images, and creating forests of model trees to increase accuracy and achieve a smooth fit. A 1-to-1 correspondence among pixels of images, coefficients of hyperplanes and coefficients of leaf functions offers the possibility of dealing with larger distortions such as arbitrary rotations or changes of perspective. A theoretical method for smoothing forest outputs to produce a continuously differentiable approximation is described. Within that framework, a training procedure is proved to converge.

</details>


### [181] [Stabilizing Self-Consuming Diffusion Models with Latent Space Filtering](https://arxiv.org/abs/2511.12742)
*Zhongteng Cai,Yaxuan Wang,Yang Liu,Xueru Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为潜在空间过滤（LSF）的新方法，通过过滤混合数据集中不太真实的合成数据来缓解模型崩溃问题，无需增加训练成本或依赖人工标注。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据在互联网上的激增，它们经常被用于训练后续几代生成模型，形成了"自消耗循环"，导致训练不稳定或模型崩溃。现有解决方法要么增加计算成本，要么需要昂贵的人工标注。

Method: 通过分析自消耗扩散模型的潜在空间动态，观察到合成数据提取的潜在表示的低维结构会随世代退化。基于此提出潜在空间过滤（LSF）方法，从混合数据集中过滤掉不太真实的合成数据。

Result: 实验表明，LSF在多个真实世界数据集上始终优于现有基线方法，有效缓解了模型崩溃，且不增加训练成本或依赖人工标注。

Conclusion: 潜在空间过滤是一种有效的解决方案，通过利用潜在空间退化现象来识别和过滤低质量合成数据，从而缓解模型崩溃问题。

Abstract: As synthetic data proliferates across the Internet, it is often reused to train successive generations of generative models. This creates a ``self-consuming loop" that can lead to training instability or \textit{model collapse}. Common strategies to address the issue -- such as accumulating historical training data or injecting fresh real data -- either increase computational cost or require expensive human annotation. In this paper, we empirically analyze the latent space dynamics of self-consuming diffusion models and observe that the low-dimensional structure of latent representations extracted from synthetic data degrade over generations. Based on this insight, we propose \textit{Latent Space Filtering} (LSF), a novel approach that mitigates model collapse by filtering out less realistic synthetic data from mixed datasets. Theoretically, we present a framework that connects latent space degradation to empirical observations. Experimentally, we show that LSF consistently outperforms existing baselines across multiple real-world datasets, effectively mitigating model collapse without increasing training cost or relying on human annotation.

</details>


### [182] [DIVIDE: A Framework for Learning from Independent Multi-Mechanism Data Using Deep Encoders and Gaussian Processes](https://arxiv.org/abs/2511.12745)
*Vivek Chawla,Boris Slautin,Utkarsh Pratiush,Dayakar Penumadu,Sergei Kalinin*

Main category: cs.LG

TL;DR: DIVIDE是一个框架，通过整合机制特定的深度编码器和结构化高斯过程来解耦科学数据中的多个独立机制影响，支持可解释预测和高效主动学习。


<details>
  <summary>Details</summary>
Motivation: 科学数据集通常来自多个独立机制（如空间、分类或结构效应）的组合影响，这些影响相互掩盖，难以分析各自的贡献。

Method: 集成机制特定的深度编码器与结构化高斯过程在联合潜在空间中，编码器隔离不同机制，高斯过程捕捉组合效应并量化不确定性。

Result: 在合成数据集、FerroSIM铁电模式模拟和实验PFM磁滞回线上，DIVIDE成功分离机制，重现加性和缩放相互作用，并在噪声下保持稳健。

Conclusion: 该框架可自然扩展到多功能数据集，其中机械、电磁或光学响应共存，为机制感知预测提供支持。

Abstract: Scientific datasets often arise from multiple independent mechanisms such as spatial, categorical or structural effects, whose combined influence obscures their individual contributions. We introduce DIVIDE, a framework that disentangles these influences by integrating mechanism-specific deep encoders with a structured Gaussian Process in a joint latent space. Disentanglement here refers to separating independently acting generative factors. The encoders isolate distinct mechanisms while the Gaussian Process captures their combined effect with calibrated uncertainty. The architecture supports structured priors, enabling interpretable and mechanism-aware prediction as well as efficient active learning. DIVIDE is demonstrated on synthetic datasets combining categorical image patches with nonlinear spatial fields, on FerroSIM spin lattice simulations of ferroelectric patterns, and on experimental PFM hysteresis loops from PbTiO3 films. Across benchmarks, DIVIDE separates mechanisms, reproduces additive and scaled interactions, and remains robust under noise. The framework extends naturally to multifunctional datasets where mechanical, electromagnetic or optical responses coexist.

</details>


### [183] [Are LLMs The Way Forward? A Case Study on LLM-Guided Reinforcement Learning for Decentralized Autonomous Driving](https://arxiv.org/abs/2511.12751)
*Timur Anvar,Jeffrey Chen,Yuyan Wang,Rohan Chandra*

Main category: cs.LG

TL;DR: 该研究探讨了小型本地部署LLM（<14B参数）通过奖励塑形而非直接控制来支持自动驾驶高速公路驾驶的有效性。研究发现纯RL方法成功率中等，纯LLM方法成功率更高但速度性能严重下降，混合方法介于两者之间，但LLM影响的方法表现出系统性保守偏差。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶在复杂环境中的导航仍面临挑战，RL方法依赖精心设计的奖励函数但难以捕捉复杂语义和社会情境，而LLM直接控制存在稳定性、一致性和延迟问题，因此研究小型本地LLM通过奖励塑形来增强RL的可行性。

Method: 采用案例研究比较纯RL、纯LLM和混合方法，在混合方法中LLM通过评分状态-动作转换来增强RL奖励，训练时使用LLM辅助，测试时使用标准RL策略执行。

Result: 纯RL代理成功率73-89%且效率合理；纯LLM代理成功率可达94%但速度性能严重下降；混合方法介于两者之间。LLM影响的方法表现出系统性保守偏差和显著的模型依赖性变异。

Conclusion: 当前小型LLM在安全关键控制任务中存在重要局限性，尽管能提高成功率但会导致保守行为和效率下降，限制了其在自动驾驶中的实际应用。

Abstract: Autonomous vehicle navigation in complex environments such as dense and fast-moving highways and merging scenarios remains an active area of research. A key limitation of RL is its reliance on well-specified reward functions, which often fail to capture the full semantic and social complexity of diverse, out-of-distribution situations. As a result, a rapidly growing line of research explores using Large Language Models (LLMs) to replace or supplement RL for direct planning and control, on account of their ability to reason about rich semantic context. However, LLMs present significant drawbacks: they can be unstable in zero-shot safety-critical settings, produce inconsistent outputs, and often depend on expensive API calls with network latency. This motivates our investigation into whether small, locally deployed LLMs (< 14B parameters) can meaningfully support autonomous highway driving through reward shaping rather than direct control. We present a case study comparing RL-only, LLM-only, and hybrid approaches, where LLMs augment RL rewards by scoring state-action transitions during training, while standard RL policies execute at test time. Our findings reveal that RL-only agents achieve moderate success rates (73-89%) with reasonable efficiency, LLM-only agents can reach higher success rates (up to 94%) but with severely degraded speed performance, and hybrid approaches consistently fall between these extremes. Critically, despite explicit efficiency instructions, LLM-influenced approaches exhibit systematic conservative bias with substantial model-dependent variability, highlighting important limitations of current small LLMs for safety-critical control tasks.

</details>


### [184] [Conformal Online Learning of Deep Koopman Linear Embeddings](https://arxiv.org/abs/2511.12760)
*Ben Gao,Jordan Patracone,Stéphane Chrétien,Olivier Alata*

Main category: cs.LG

TL;DR: COLoKe是一个自适应更新非线性动力系统Koopman不变表示的新框架，通过结合深度特征学习和提升空间中的多步预测一致性，使用符合性机制动态校准更新阈值，减少不必要更新并避免过拟合。


<details>
  <summary>Details</summary>
Motivation: 为了解决从流数据中自适应更新非线性动力系统的Koopman不变表示，同时防止过拟合的问题。

Method: 结合深度特征学习和提升空间中的多步预测一致性，使用符合性机制评估当前Koopman模型的一致性，仅在预测误差超过动态校准阈值时触发更新。

Result: 在基准动力系统上的实验结果表明，COLoKe能有效保持长期预测准确性，同时显著减少不必要更新并避免过拟合。

Conclusion: COLoKe框架成功实现了对非线性动力系统Koopman不变表示的自适应更新，在保持预测精度的同时优化了更新效率。

Abstract: We introduce Conformal Online Learning of Koopman embeddings (COLoKe), a novel framework for adaptively updating Koopman-invariant representations of nonlinear dynamical systems from streaming data. Our modeling approach combines deep feature learning with multistep prediction consistency in the lifted space, where the dynamics evolve linearly. To prevent overfitting, COLoKe employs a conformal-style mechanism that shifts the focus from evaluating the conformity of new states to assessing the consistency of the current Koopman model. Updates are triggered only when the current model's prediction error exceeds a dynamically calibrated threshold, allowing selective refinement of the Koopman operator and embedding. Empirical results on benchmark dynamical systems demonstrate the effectiveness of COLoKe in maintaining long-term predictive accuracy while significantly reducing unnecessary updates and avoiding overfitting.

</details>


### [185] [INC: An Indirect Neural Corrector for Auto-Regressive Hybrid PDE Solvers](https://arxiv.org/abs/2511.12764)
*Hao Wei,Aleksandra Franz,Bjoern List,Nils Thuerey*

Main category: cs.LG

TL;DR: 提出间接神经校正器(INC)方法，通过将学习到的校正项整合到控制方程中而非直接状态更新，显著减少自回归误差，在混沌系统中实现稳定高效的PDE仿真。


<details>
  <summary>Details</summary>
Motivation: 传统混合求解器直接将学习校正应用于求解器输出会导致显著的自回归误差，特别是在混沌系统中，放大的扰动会在长期推演中累积。

Method: 间接神经校正器(INC)将学习校正整合到控制方程中，而非直接更新状态，从而将误差放大降低到Δt⁻¹ + L的量级。

Result: INC在长期轨迹性能(R²)上提升高达158.7%，稳定了激进粗化下的爆炸问题，在复杂3D湍流案例中实现数个数量级的加速。

Conclusion: INC实现了具有形式误差减少的稳定高效PDE仿真，为具有可靠物理保证的更快科学和工程仿真铺平了道路。

Abstract: When simulating partial differential equations, hybrid solvers combine coarse numerical solvers with learned correctors. They promise accelerated simulations while adhering to physical constraints. However, as shown in our theoretical framework, directly applying learned corrections to solver outputs leads to significant autoregressive errors, which originate from amplified perturbations that accumulate during long-term rollouts, especially in chaotic regimes. To overcome this, we propose the Indirect Neural Corrector (\(\mathrm{INC}\)), which integrates learned corrections into the governing equations rather than applying direct state updates. Our key insight is that \(\mathrm{INC}\) reduces the error amplification on the order of \(Δt^{-1} + L\), where \(Δt\) is the timestep and $L$ the Lipschitz constant. At the same time, our framework poses no architectural requirements and integrates seamlessly with arbitrary neural networks and solvers. We test \(\mathrm{INC}\) in extensive benchmarks, covering numerous differentiable solvers, neural backbones, and test cases ranging from a 1D chaotic system to 3D turbulence. INC improves the long-term trajectory performance (\(R^2\)) by up to 158.7\%, stabilizes blowups under aggressive coarsening, and for complex 3D turbulence cases yields speed-ups of several orders of magnitude. INC thus enables stable, efficient PDE emulation with formal error reduction, paving the way for faster scientific and engineering simulations with reliable physics guarantees. Our source code is available at https://github.com/tum-pbs/INC

</details>


### [186] [MolEdit: Knowledge Editing for Multimodal Molecule Language Models](https://arxiv.org/abs/2511.12770)
*Zhenyu Lei,Patrick Soga,Yaochen Zhu,Yinhan He,Yushun Dong,Jundong Li*

Main category: cs.LG

TL;DR: MolEdit是一个专为分子语言模型设计的知识编辑框架，通过多专家知识适配器和专业知识感知编辑切换器，在保持无关分子知识的同时实现目标修改。


<details>
  <summary>Details</summary>
Motivation: 分子语言模型可能因过时的网络挖掘训练语料或恶意操作而编码和传播不准确信息，危及下游发现流程。虽然知识编辑在通用领域AI中已有探索，但在分子语言模型中的应用仍属空白，面临分子知识多面性和相互依赖性的独特挑战。

Method: 提出MolEdit框架，包含多专家知识适配器（将编辑路由到不同分子方面的专业专家）和专业知识感知编辑切换器（仅在输入与存储的编辑在所有专业知识中紧密匹配时激活适配器）。

Result: 在两个流行的分子语言模型骨干上进行的广泛实验表明，MolEdit在可靠性方面比基线方法高出18.8%，在局部性方面高出12.0%，同时保持效率。

Conclusion: MolEdit是分子语言模型编辑的有效框架，能够实现目标修改同时最小化对无关知识的干扰，为分子知识更新提供了有力工具。

Abstract: Understanding and continuously refining multimodal molecular knowledge is crucial for advancing biomedicine, chemistry, and materials science. Molecule language models (MoLMs) have become powerful tools in these domains, integrating structural representations (e.g., SMILES strings, molecular graphs) with rich contextual descriptions (e.g., physicochemical properties). However, MoLMs can encode and propagate inaccuracies due to outdated web-mined training corpora or malicious manipulation, jeopardizing downstream discovery pipelines. While knowledge editing has been explored for general-domain AI, its application to MoLMs remains uncharted, presenting unique challenges due to the multifaceted and interdependent nature of molecular knowledge. In this paper, we take the first step toward MoLM editing for two critical tasks: molecule-to-caption generation and caption-to-molecule generation. To address molecule-specific challenges, we propose MolEdit, a powerful framework that enables targeted modifications while preserving unrelated molecular knowledge. MolEdit combines a Multi-Expert Knowledge Adapter that routes edits to specialized experts for different molecular facets with an Expertise-Aware Editing Switcher that activates the adapters only when input closely matches the stored edits across all expertise, minimizing interference with unrelated knowledge. To systematically evaluate editing performance, we introduce MEBench, a comprehensive benchmark assessing multiple dimensions, including Reliability (accuracy of the editing), Locality (preservation of irrelevant knowledge), and Generality (robustness to reformed queries). Across extensive experiments on two popular MoLM backbones, MolEdit delivers up to 18.8% higher Reliability and 12.0% better Locality than baselines while maintaining efficiency. The code is available at: https://github.com/LzyFischer/MolEdit.

</details>


### [187] [Scalable Multi-Objective and Meta Reinforcement Learning via Gradient Estimation](https://arxiv.org/abs/2511.12779)
*Zhenshuo Zhang,Minxuan Duan,Youran Ye,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: 提出PolicyGradEx算法，通过元训练和微调两阶段方法，将多个RL目标高效分组为k≪n个相关组，以优化多目标强化学习策略。


<details>
  <summary>Details</summary>
Motivation: 在机器人控制、语言模型偏好优化等应用中，当目标数量n增长时，为所有目标学习单一策略是次优的。需要将相关目标分组训练以提高效率。

Method: 两阶段方法：1) 使用多任务学习学习所有目标的元策略；2) 对随机采样子集进行微调，利用策略网络的一阶近似特性估计任务亲和度矩阵，然后基于亲和度分数进行聚类分组。

Result: 在三个机器人控制和Meta-World基准测试中，平均性能优于最先进基线16%，速度提升达26倍。基于损失的聚类相比随机分组和梯度相似性分组提升19%。

Conclusion: PolicyGradEx算法能有效估计任务亲和度并分组，显著提升多目标RL性能。通过Hessian迹分析验证了策略网络的泛化误差。

Abstract: We study the problem of efficiently estimating policies that simultaneously optimize multiple objectives in reinforcement learning (RL). Given $n$ objectives (or tasks), we seek the optimal partition of these objectives into $k \ll n$ groups, where each group comprises related objectives that can be trained together. This problem arises in applications such as robotics, control, and preference optimization in language models, where learning a single policy for all $n$ objectives is suboptimal as $n$ grows. We introduce a two-stage procedure -- meta-training followed by fine-tuning -- to address this problem. We first learn a meta-policy for all objectives using multitask learning. Then, we adapt the meta-policy to multiple randomly sampled subsets of objectives. The adaptation step leverages a first-order approximation property of well-trained policy networks, which is empirically verified to be accurate within a $2\%$ error margin across various RL environments. The resulting algorithm, PolicyGradEx, efficiently estimates an aggregate task-affinity score matrix given a policy evaluation algorithm. Based on the estimated affinity score matrix, we cluster the $n$ objectives into $k$ groups by maximizing the intra-cluster affinity scores. Experiments on three robotic control and the Meta-World benchmarks demonstrate that our approach outperforms state-of-the-art baselines by $16\%$ on average, while delivering up to $26\times$ faster speedup relative to performing full training to obtain the clusters. Ablation studies validate each component of our approach. For instance, compared with random grouping and gradient-similarity-based grouping, our loss-based clustering yields an improvement of $19\%$. Finally, we analyze the generalization error of policy networks by measuring the Hessian trace of the loss surface, which gives non-vacuous measures relative to the observed generalization errors.

</details>


### [188] [Physics-Constrained Adaptive Neural Networks Enable Real-Time Semiconductor Manufacturing Optimization with Minimal Training Data](https://arxiv.org/abs/2511.12788)
*Rubén Darío Guerrero*

Main category: cs.LG

TL;DR: 提出了一种物理约束自适应学习框架，通过可学习参数自动校准电磁近似，在EUV光刻优化中实现亚纳米精度，仅需少量训练样本即可实现跨几何泛化。


<details>
  <summary>Details</summary>
Motivation: 半导体行业在EUV光刻优化中面临计算危机，传统方法消耗数十亿CPU小时且无法达到亚纳米精度，需要解决学术物理信息神经网络与工业部署需求之间的关键差距。

Method: 集成可微分模块（菲涅尔衍射、材料吸收、光学点扩散函数模糊、相移效应、对比度调制）与直接几何图案匹配目标，通过物理约束学习自动校准电磁近似参数。

Result: 在15个代表性图案上实现一致的亚纳米EPE性能（0.664-2.536 nm范围），仅需每个图案50个训练样本，相比无物理约束的CNN基线平均改进69.9%，训练完成后推理速度显著优于严格电磁求解器。

Conclusion: 物理约束自适应学习为实时半导体制造优化建立了基础方法学，通过联合物理校准和制造精度目标，填补了学术研究到工业部署的关键空白。

Abstract: The semiconductor industry faces a computational crisis in extreme ultraviolet (EUV) lithography optimization, where traditional methods consume billions of CPU hours while failing to achieve sub-nanometer precision. We present a physics-constrained adaptive learning framework that automatically calibrates electromagnetic approximations through learnable parameters $\boldsymbolθ = \{θ_d, θ_a, θ_b, θ_p, θ_c\}$ while simultaneously minimizing Edge Placement Error (EPE) between simulated aerial images and target photomasks. The framework integrates differentiable modules for Fresnel diffraction, material absorption, optical point spread function blur, phase-shift effects, and contrast modulation with direct geometric pattern matching objectives, enabling cross-geometry generalization with minimal training data. Through physics-constrained learning on 15 representative patterns spanning current production to future research nodes, we demonstrate consistent sub-nanometer EPE performance (0.664-2.536 nm range) using only 50 training samples per pattern. Adaptive physics learning achieves an average improvement of 69.9\% over CNN baselines without physics constraints, with a significant inference speedup over rigorous electromagnetic solvers after training completion. This approach requires 90\% fewer training samples through cross-geometry generalization compared to pattern-specific CNN training approaches. This work establishes physics-constrained adaptive learning as a foundational methodology for real-time semiconductor manufacturing optimization, addressing the critical gap between academic physics-informed neural networks and industrial deployment requirements through joint physics calibration and manufacturing precision objectives.

</details>


### [189] [Optimal Look-back Horizon for Time Series Forecasting in Federated Learning](https://arxiv.org/abs/2511.12791)
*Dahao Tang,Nan Yang,Yanli Li,Zhiyu Zhu,Zhibo Jin,Dong Yuan*

Main category: cs.LG

TL;DR: 本文提出了一个联邦时间序列预测中自适应回望窗口选择的框架，通过内在空间公式化来解决数据分散、异构和非独立的问题。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习场景中，时间序列数据具有分散性、异构性和非独立性，选择合适的回望窗口是一个基本挑战。现有方法主要局限于集中式和独立分布设置。

Method: 引入合成数据生成器捕捉客户端数据的时序结构，定义将时间序列窗口映射到内在表示空间的变换，推导预测损失的贝叶斯项和近似项分解。

Result: 分析表明增加回望窗口能改善确定性模式的可识别性，但由于模型复杂度增加和样本效率降低，也会增加近似误差。总预测损失在不可约损失开始饱和而近似损失持续上升的最小窗口处最小化。

Conclusion: 这项工作为联邦学习中的时间序列预测自适应窗口选择提供了严格的理论基础。

Abstract: Selecting an appropriate look-back horizon remains a fundamental challenge in time series forecasting (TSF), particularly in the federated learning scenarios where data is decentralized, heterogeneous, and often non-independent. While recent work has explored horizon selection by preserving forecasting-relevant information in an intrinsic space, these approaches are primarily restricted to centralized and independently distributed settings. This paper presents a principled framework for adaptive horizon selection in federated time series forecasting through an intrinsic space formulation. We introduce a synthetic data generator (SDG) that captures essential temporal structures in client data, including autoregressive dependencies, seasonality, and trend, while incorporating client-specific heterogeneity. Building on this model, we define a transformation that maps time series windows into an intrinsic representation space with well-defined geometric and statistical properties. We then derive a decomposition of the forecasting loss into a Bayesian term, which reflects irreducible uncertainty, and an approximation term, which accounts for finite-sample effects and limited model capacity. Our analysis shows that while increasing the look-back horizon improves the identifiability of deterministic patterns, it also increases approximation error due to higher model complexity and reduced sample efficiency. We prove that the total forecasting loss is minimized at the smallest horizon where the irreducible loss starts to saturate, while the approximation loss continues to rise. This work provides a rigorous theoretical foundation for adaptive horizon selection for time series forecasting in federated learning.

</details>


### [190] [Genomic Next-Token Predictors are In-Context Learners](https://arxiv.org/abs/2511.12797)
*Nathan Breslow,Aayush Mishra,Mahler Revsine,Michael C. Schatz,Anqi Liu,Daniel Khashabi*

Main category: cs.LG

TL;DR: 研究发现基因组模型通过大规模预测训练也能自然涌现上下文学习能力，表明ICL是跨模态的通用能力


<details>
  <summary>Details</summary>
Motivation: 探索上下文学习是否仅是人类语言特有的能力，还是其他符号序列领域通过大规模预测训练也能自然涌现

Method: 开发控制实验框架，在语言和基因组形式中实例化符号推理任务，直接比较基因组模型和语言模型的ICL表现

Result: 基因组模型与语言模型类似，随着上下文示例数量增加，模式归纳能力呈对数线性增长

Conclusion: ICL是大规模预测建模在丰富数据上的自然结果，可扩展到语言之外的模态，指向统一的、模态无关的上下文学习观点

Abstract: In-context learning (ICL) -- the capacity of a model to infer and apply abstract patterns from examples provided within its input -- has been extensively studied in large language models trained for next-token prediction on human text. In fact, prior work often attributes this emergent behavior to distinctive statistical properties in human language. This raises a fundamental question: can ICL arise organically in other sequence domains purely through large-scale predictive training?
  To explore this, we turn to genomic sequences, an alternative symbolic domain rich in statistical structure. Specifically, we study the Evo2 genomic model, trained predominantly on next-nucleotide (A/T/C/G) prediction, at a scale comparable to mid-sized LLMs. We develop a controlled experimental framework comprising symbolic reasoning tasks instantiated in both linguistic and genomic forms, enabling direct comparison of ICL across genomic and linguistic models. Our results show that genomic models, like their linguistic counterparts, exhibit log-linear gains in pattern induction as the number of in-context demonstrations increases. To the best of our knowledge, this is the first evidence of organically emergent ICL in genomic sequences, supporting the hypothesis that ICL arises as a consequence of large-scale predictive modeling over rich data. These findings extend emergent meta-learning beyond language, pointing toward a unified, modality-agnostic view of in-context learning.

</details>


### [191] [The Alignment Game: A Theory of Long-Horizon Alignment Through Recursive Curation](https://arxiv.org/abs/2511.12804)
*Ali Falahati,Mohammad Mohammadi Amiri,Kate Larson,Lukasz Golab*

Main category: cs.LG

TL;DR: 本文首次为自消费生成模型的递归再训练对对齐的长期影响提供了形式化分析框架，揭示了三种结构收敛机制，并证明了基于Bradley-Terry模型的递归筛选机制在多样性、对称影响和初始化独立性之间存在根本性不可能定理。


<details>
  <summary>Details</summary>
Motivation: 研究自消费生成模型在自身输出上训练时，用户偏好对齐如何从一次性过程转变为递归过程，分析这种递归再训练对长期对齐的影响。

Method: 基于Bradley-Terry模型的两阶段筛选机制，将对齐建模为模型所有者（筛选学习输出）和公共用户（通过交互决定保留输出）两个派系之间的互动。

Result: 发现了三种结构收敛机制：共识崩溃、共享最优妥协和不对称精炼。证明了递归BT筛选机制无法同时保持多样性、确保对称影响和消除初始化依赖。

Conclusion: 对齐不是静态目标而是演化均衡，既受权力不对称性影响，也受路径依赖性塑造，需要将过程视为动态社会选择问题。

Abstract: In self-consuming generative models that train on their own outputs, alignment with user preferences becomes a recursive rather than one-time process. We provide the first formal foundation for analyzing the long-term effects of such recursive retraining on alignment. Under a two-stage curation mechanism based on the Bradley-Terry (BT) model, we model alignment as an interaction between two factions: the Model Owner, who filters which outputs should be learned by the model, and the Public User, who determines which outputs are ultimately shared and retained through interactions with the model. Our analysis reveals three structural convergence regimes depending on the degree of preference alignment: consensus collapse, compromise on shared optima, and asymmetric refinement. We prove a fundamental impossibility theorem: no recursive BT-based curation mechanism can simultaneously preserve diversity, ensure symmetric influence, and eliminate dependence on initialization. Framing the process as dynamic social choice, we show that alignment is not a static goal but an evolving equilibrium, shaped both by power asymmetries and path dependence.

</details>


### [192] [Expressive Temporal Specifications for Reward Monitoring](https://arxiv.org/abs/2511.12808)
*Omar Adalat,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 使用定量线性时序逻辑(LTL_f[F])合成奖励监控器，为可观测状态轨迹生成密集奖励流，解决长时决策中的稀疏奖励问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习中指定信息丰富且密集的奖励函数是一个关键挑战，直接影响智能体训练效率。当前文献中占主导地位的布尔语义会导致长时决策中的稀疏奖励问题。

Method: 利用定量线性时序逻辑在有限轨迹上的表达能力，合成奖励监控器，生成密集的奖励流。该框架与算法无关，仅依赖状态标记函数，并自然支持非马尔可夫性质的规范。

Result: 实验结果表明，定量监控器在最大化任务完成度的定量度量方面始终优于布尔监控器，并能减少收敛时间，具体表现取决于环境。

Conclusion: 定量奖励监控器通过提供细粒度反馈，能有效引导智能体达到最优行为，解决长时决策中的稀疏奖励问题，在任务完成度和收敛时间方面优于传统布尔监控器。

Abstract: Specifying informative and dense reward functions remains a pivotal challenge in Reinforcement Learning, as it directly affects the efficiency of agent training. In this work, we harness the expressive power of quantitative Linear Temporal Logic on finite traces (($\text{LTL}_f[\mathcal{F}]$)) to synthesize reward monitors that generate a dense stream of rewards for runtime-observable state trajectories. By providing nuanced feedback during training, these monitors guide agents toward optimal behaviour and help mitigate the well-known issue of sparse rewards under long-horizon decision making, which arises under the Boolean semantics dominating the current literature. Our framework is algorithm-agnostic and only relies on a state labelling function, and naturally accommodates specifying non-Markovian properties. Empirical results show that our quantitative monitors consistently subsume and, depending on the environment, outperform Boolean monitors in maximizing a quantitative measure of task completion and in reducing convergence time.

</details>


### [193] [Assessing Automated Fact-Checking for Medical LLM Responses with Knowledge Graphs](https://arxiv.org/abs/2511.12817)
*Shasha Zhou,Mingyu Huang,Jack Cole,Charles Britton,Ming Yin,Jan Wolber,Ke Li*

Main category: cs.LG

TL;DR: 提出FAITH框架，利用医学知识图谱自动评估LLM生成响应的真实性，无需参考答案，通过分解声明、链接知识图谱和基于证据路径评分来验证事实准确性。


<details>
  <summary>Details</summary>
Motivation: 在医疗领域部署LLM需要严格验证，但现有评估方法存在局限性，需要自动化的事实性评估工具来确保LLM在医疗应用中的可靠性。

Method: 开发FAITH框架：将LLM响应分解为原子声明，链接到医学知识图谱，基于证据路径进行评分，无需参考答案即可评估事实准确性。

Result: 实验显示基于知识图谱的评估与临床医生判断相关性更高，能有效区分不同能力的LLM，对文本变化具有鲁棒性，评分具有可解释性。

Conclusion: 尽管存在局限性，但利用知识图谱是医疗领域自动化事实性评估的重要方向，能帮助用户理解和缓解当前LLM的局限性。

Abstract: The recent proliferation of large language models (LLMs) holds the potential to revolutionize healthcare, with strong capabilities in diverse medical tasks. Yet, deploying LLMs in high-stakes healthcare settings requires rigorous verification and validation to understand any potential harm. This paper investigates the reliability and viability of using medical knowledge graphs (KGs) for the automated factuality evaluation of LLM-generated responses. To ground this investigation, we introduce FAITH, a framework designed to systematically probe the strengths and limitations of this KG-based approach. FAITH operates without reference answers by decomposing responses into atomic claims, linking them to a medical KG, and scoring them based on evidence paths. Experiments on diverse medical tasks with human subjective evaluations demonstrate that KG-grounded evaluation achieves considerably higher correlations with clinician judgments and can effectively distinguish LLMs with varying capabilities. It is also robust to textual variances. The inherent explainability of its scoring can further help users understand and mitigate the limitations of current LLMs. We conclude that while limitations exist, leveraging KGs is a prominent direction for automated factuality assessment in healthcare.

</details>


### [194] [Catastrophic Forgetting in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2511.12828)
*Mohammad Marufur Rahman,Guanchu Wang,Kaixiong Zhou,Minghan Chen,Fan Yang*

Main category: cs.LG

TL;DR: 对Kolmogorov-Arnold Networks (KANs)在持续学习中的灾难性遗忘问题进行了系统研究，提出了理论框架并开发了KAN-LoRA适配器，发现KANs在低维任务中表现良好但在高维领域仍易遗忘。


<details>
  <summary>Details</summary>
Motivation: 虽然KANs被认为具有内在的抗遗忘特性，但其在持续学习中的实际行为和局限性尚不清楚，需要系统研究来理解其优势和弱点。

Method: 开发了连接遗忘与激活支持重叠和内在数据维度的理论框架，通过合成和视觉任务的系统实验验证分析，并提出了KAN-LoRA适配器用于参数高效的持续微调。

Result: KANs在低维算法设置中表现出有前景的知识保留能力，但在图像分类和语言建模等高维领域仍然容易发生灾难性遗忘。

Conclusion: 研究揭示了KANs在持续学习中的优势和局限性，为持续学习系统设计提供了实用见解。

Abstract: Catastrophic forgetting is a longstanding challenge in continual learning, where models lose knowledge from earlier tasks when learning new ones. While various mitigation strategies have been proposed for Multi-Layer Perceptrons (MLPs), recent architectural advances like Kolmogorov-Arnold Networks (KANs) have been suggested to offer intrinsic resistance to forgetting by leveraging localized spline-based activations. However, the practical behavior of KANs under continual learning remains unclear, and their limitations are not well understood. To address this, we present a comprehensive study of catastrophic forgetting in KANs and develop a theoretical framework that links forgetting to activation support overlap and intrinsic data dimension. We validate these analyses through systematic experiments on synthetic and vision tasks, measuring forgetting dynamics under varying model configurations and data complexity. Further, we introduce KAN-LoRA, a novel adapter design for parameter-efficient continual fine-tuning of language models, and evaluate its effectiveness in knowledge editing tasks. Our findings reveal that while KANs exhibit promising retention in low-dimensional algorithmic settings, they remain vulnerable to forgetting in high-dimensional domains such as image classification and language modeling. These results advance the understanding of KANs' strengths and limitations, offering practical insights for continual learning system design.

</details>


### [195] [An Evaluation of Representation Learning Methods in Particle Physics Foundation Models](https://arxiv.org/abs/2511.12829)
*Michael Chen,Raghav Kansal,Abhijith Gandrakota,Zichun Hao,Jennifer Ngadiuba,Maria Spiropulu*

Main category: cs.LG

TL;DR: 系统评估粒子物理中表示学习目标，在统一框架下比较对比学习、掩码粒子建模和生成重建等方法，并引入监督架构改进达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 为粒子物理领域提供统一的表示学习评估框架，隔离学习目标的影响，建立可复现基线，为未来基础模型开发提供参考。

Method: 使用共享的基于transformer的粒子云编码器，标准化预处理和采样，在喷注分类数据集上比较对比学习、掩码粒子建模和生成重建等目标。

Result: 在基准评估中实现了最先进的性能，通过受控比较揭示了不同学习目标的优势和局限性。

Conclusion: 这项工作为粒子物理基础模型的未来发展提供了参考点，使社区能够进行更透明和稳健的进展。

Abstract: We present a systematic evaluation of representation learning objectives for particle physics within a unified framework. Our study employs a shared transformer-based particle-cloud encoder with standardized preprocessing, matched sampling, and a consistent evaluation protocol on a jet classification dataset. We compare contrastive (supervised and self-supervised), masked particle modeling, and generative reconstruction objectives under a common training regimen. In addition, we introduce targeted supervised architectural modifications that achieve state-of-the-art performance on benchmark evaluations. This controlled comparison isolates the contributions of the learning objective, highlights their respective strengths and limitations, and provides reproducible baselines. We position this work as a reference point for the future development of foundation models in particle physics, enabling more transparent and robust progress across the community.

</details>


### [196] [Connectivity-Guided Sparsification of 2-FWL GNNs: Preserving Full Expressivity with Improved Efficiency](https://arxiv.org/abs/2511.12838)
*Rongqin Chen,Fan Mo,Pak Lon Ip,Shenghui Zhang,Dan Wu,Ye Li,Leong Hou U*

Main category: cs.LG

TL;DR: Co-Sparsify是一个连接感知的稀疏化框架，通过将2节点消息传递限制在连通组件中，3节点交互限制在双连通组件中，消除可证明冗余的计算，同时保持完整的2-FWL表达能力。


<details>
  <summary>Details</summary>
Motivation: 现有的高阶图神经网络（HOGNNs）基于2-FWL测试具有优越的表达能力，但计算成本高达O(n^3)。现有效率方法通常以降低表达能力为代价来缓解计算负担。

Method: Co-Sparsify框架的关键洞察是：3节点交互仅在双连通组件（每个节点对都位于环上的最大子图）内是表达必要的。在这些组件之外，结构关系可以通过2节点消息传递或全局读取完全捕获。

Result: 在PPGN上，Co-Sparsify在合成子结构计数任务中匹配或超过准确率，并在真实世界基准（ZINC、QM9）上实现最先进性能。

Conclusion: 高表达性和可扩展性并不相互排斥：基于原则的、拓扑引导的稀疏化能够实现强大、高效的GNN，并具有理论保证。

Abstract: Higher-order Graph Neural Networks (HOGNNs) based on the 2-FWL test achieve superior expressivity by modeling 2- and 3-node interactions, but at $\mathcal{O}(n^3)$ computational cost. However, this computational burden is typically mitigated by existing efficiency methods at the cost of reduced expressivity. We propose \textbf{Co-Sparsify}, a connectivity-aware sparsification framework that eliminates \emph{provably redundant} computations while preserving full 2-FWL expressive power. Our key insight is that 3-node interactions are expressively necessary only within \emph{biconnected components} -- maximal subgraphs where every pair of nodes lies on a cycle. Outside these components, structural relationships can be fully captured via 2-node message passing or global readout, rendering higher-order modeling unnecessary. Co-Sparsify restricts 2-node message passing to connected components and 3-node interactions to biconnected ones, removing computation without approximation or sampling. We prove that Co-Sparsified GNNs are as expressive as the 2-FWL test. Empirically, on PPGN, Co-Sparsify matches or exceeds accuracy on synthetic substructure counting tasks and achieves state-of-the-art performance on real-world benchmarks (ZINC, QM9). This study demonstrates that high expressivity and scalability are not mutually exclusive: principled, topology-guided sparsification enables powerful, efficient GNNs with theoretical guarantees.

</details>


### [197] [RoS-Guard: Robust and Scalable Online Change Detection with Delay-Optimal Guarantees](https://arxiv.org/abs/2511.12846)
*Zelin Zhu,Yancheng Huang,Kai Yang*

Main category: cs.LG

TL;DR: RoS-Guard是一个针对具有不确定性的线性系统的鲁棒最优在线变化检测算法，通过神经展开实现GPU加速并行计算，提供理论性能保证。


<details>
  <summary>Details</summary>
Motivation: 现有在线变化检测方法通常假设精确系统知识，这在现实中不现实，且在大规模系统中效率低下。

Method: 通过紧密松弛和重构OCD优化问题，采用神经展开技术实现GPU加速的并行计算。

Result: 大量实验验证了RoS-Guard的有效性，并在大规模系统场景中展示了显著的计算加速。

Conclusion: RoS-Guard解决了现有OCD方法在系统不确定性和计算效率方面的局限性，为实际应用提供了可行的解决方案。

Abstract: Online change detection (OCD) aims to rapidly identify change points in streaming data and is critical in applications such as power system monitoring, wireless network sensing, and financial anomaly detection. Existing OCD methods typically assume precise system knowledge, which is unrealistic due to estimation errors and environmental variations. Moreover, existing OCD methods often struggle with efficiency in large-scale systems. To overcome these challenges, we propose RoS-Guard, a robust and optimal OCD algorithm tailored for linear systems with uncertainty. Through a tight relaxation and reformulation of the OCD optimization problem, RoS-Guard employs neural unrolling to enable efficient parallel computation via GPU acceleration. The algorithm provides theoretical guarantees on performance, including expected false alarm rate and worst-case average detection delay. Extensive experiments validate the effectiveness of RoS-Guard and demonstrate significant computational speedup in large-scale system scenarios.

</details>


### [198] [From Black-Box to White-Box: Control-Theoretic Neural Network Interpretability](https://arxiv.org/abs/2511.12852)
*Jihoon Moon*

Main category: cs.LG

TL;DR: 提出一种控制理论框架，将训练好的神经网络视为非线性状态空间系统，通过局部线性化、可控性和可观测性Gramian矩阵以及Hankel奇异值来分析其内部计算。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络虽然性能优异，但难以进行机理解释。需要一种系统性的方法来分析神经网络内部的计算机制和神经元重要性。

Method: 在给定输入下，围绕隐藏激活模式对网络进行局部线性化，构建状态空间模型。通过输入状态和状态输出Jacobian矩阵定义局部可控性和可观测性Gramian矩阵，计算Hankel奇异值和相关模式。

Result: 在简单前馈网络上验证了该框架，包括1-2-2-1 SwiGLU网络和2-3-3-2 GELU网络。发现激活饱和会降低可控性，缩小主导Hankel奇异值，并将主导内部模式转移到不同的神经元子集。

Conclusion: 该方法将神经网络转化为局部白盒动态模型集合，识别出哪些内部方向是剪枝或约束的自然候选，以提高可解释性。

Abstract: Deep neural networks achieve state of the art performance but remain difficult to interpret mechanistically. In this work, we propose a control theoretic framework that treats a trained neural network as a nonlinear state space system and uses local linearization, controllability and observability Gramians, and Hankel singular values to analyze its internal computation. For a given input, we linearize the network around the corresponding hidden activation pattern and construct a state space model whose state consists of hidden neuron activations. The input state and state output Jacobians define local controllability and observability Gramians, from which we compute Hankel singular values and associated modes. These quantities provide a principled notion of neuron and pathway importance: controllability measures how easily each neuron can be excited by input perturbations, observability measures how strongly each neuron influences the output, and Hankel singular values rank internal modes that carry input output energy. We illustrate the framework on simple feedforward networks, including a 1 2 2 1 SwiGLU network and a 2 3 3 2 GELU network. By comparing different operating points, we show how activation saturation reduces controllability, shrinks the dominant Hankel singular value, and shifts the dominant internal mode to a different subset of neurons. The proposed method turns a neural network into a collection of local white box dynamical models and suggests which internal directions are natural candidates for pruning or constraints to improve interpretability.

</details>


### [199] [An approach of deep reinforcement learning for maximizing the net present value of stochastic projects](https://arxiv.org/abs/2511.12865)
*Wei Xu,Fan Yang,Qinyuan Cui,Zhi Chen*

Main category: cs.LG

TL;DR: 该论文研究了具有随机活动持续时间和现金流量的项目优化问题，提出使用双深度Q网络（DDQN）方法来最大化期望净现值（NPV）。


<details>
  <summary>Details</summary>
Motivation: 在具有随机活动持续时间和现金流量的项目中，传统刚性策略和动态策略在处理大规模或高度不确定环境时效果有限，需要更有效的优化方法。

Method: 将问题建模为离散时间马尔可夫决策过程（MDP），并采用双深度Q网络（DDQN）方法，通过双网络架构和目标网络来缓解动作价值的高估问题并提高训练收敛性。

Result: DDQN在比较实验中优于传统策略，特别是在大规模或高度不确定环境中表现出更好的计算能力、策略可靠性和适应性。消融研究证实双网络架构和目标网络的有效性。

Conclusion: DDQN不仅在复杂项目优化中实现更高的期望NPV，而且为稳定有效的策略实施提供了可靠框架。

Abstract: This paper investigates a project with stochastic activity durations and cash flows under discrete scenarios, where activities must satisfy precedence constraints generating cash inflows and outflows. The objective is to maximize expected net present value (NPV) by accelerating inflows and deferring outflows. We formulate the problem as a discrete-time Markov Decision Process (MDP) and propose a Double Deep Q-Network (DDQN) approach. Comparative experiments demonstrate that DDQN outperforms traditional rigid and dynamic strategies, particularly in large-scale or highly uncertain environments, exhibiting superior computational capability, policy reliability, and adaptability. Ablation studies further reveal that the dual-network architecture mitigates overestimation of action values, while the target network substantially improves training convergence and robustness. These results indicate that DDQN not only achieves higher expected NPV in complex project optimization but also provides a reliable framework for stable and effective policy implementation.

</details>


### [200] [On the Fundamental Limits of LLMs at Scale](https://arxiv.org/abs/2511.12869)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Zeeshan Memon,Muhammad Ibtsaam Qadir,Sagnik Bhattacharya,Hassan Rizwan,Abhiram R. Gorle,Maahe Zehra Kazmi,Ayesha Mohsin,Muhammad Usman Rafique,Zihao He,Pulkit Mehta,Muhammad Ali Jamshed,John M. Cioffi*

Main category: cs.LG

TL;DR: 本文提出了一个统一的理论框架，从计算理论、信息论和统计学习角度系统分析了LLM扩展的五个根本限制：幻觉、上下文压缩、推理退化、检索脆弱性和多模态不对齐，并指出了这些限制的理论上限。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM扩展限制的描述多为经验性观察，缺乏将这些现象与计算、信息和学习理论基础相连接的严谨理论综合。本文旨在填补这一空白。

Method: 构建了一个基于证明的统一框架，从三个理论层面分析LLM扩展限制：(1)可计算性和不可计算性导致的固有错误；(2)信息论和统计约束；(3)几何和计算效应导致的上下文压缩。

Result: 证明了LLM扩展存在理论天花板：不可计算任务必然产生无限失败集，有限描述长度强制压缩误差，长上下文因位置训练不足和注意力机制而严重压缩，基于似然的训练偏好模式完成而非推理。

Conclusion: LLM扩展在某些领域有效，但在某些领域会饱和甚至无法进展，提出了有界预言检索、位置课程学习和稀疏/分层注意力等实际缓解路径。

Abstract: Large Language Models (LLMs) have benefited enormously from scaling, yet these gains are bounded by five fundamental limitations: (1) hallucination, (2) context compression, (3) reasoning degradation, (4) retrieval fragility, and (5) multimodal misalignment. While existing surveys describe these phenomena empirically, they lack a rigorous theoretical synthesis connecting them to the foundational limits of computation, information, and learning. This work closes that gap by presenting a unified, proof-informed framework that formalizes the innate theoretical ceilings of LLM scaling. First, computability and uncomputability imply an irreducible residue of error: for any computably enumerable model family, diagonalization guarantees inputs on which some model must fail, and undecidable queries (e.g., halting-style tasks) induce infinite failure sets for all computable predictors. Second, information-theoretic and statistical constraints bound attainable accuracy even on decidable tasks, finite description length enforces compression error, and long-tail factual knowledge requires prohibitive sample complexity. Third, geometric and computational effects compress long contexts far below their nominal size due to positional under-training, encoding attenuation, and softmax crowding. We further show how likelihood-based training favors pattern completion over inference, how retrieval under token limits suffers from semantic drift and coupling noise, and how multimodal scaling inherits shallow cross-modal alignment. Across sections, we pair theorems and empirical evidence to outline where scaling helps, where it saturates, and where it cannot progress, providing both theoretical foundations and practical mitigation paths like bounded-oracle retrieval, positional curricula, and sparse or hierarchical attention.

</details>


### [201] [On the Information Processing of One-Dimensional Wasserstein Distances with Finite Samples](https://arxiv.org/abs/2511.12881)
*Cheongjae Jang,Jonghyun Won,Soyeon Jun,Chun Kee Chung,Keehyoung Joo,Yung-Kyun Noh*

Main category: cs.LG

TL;DR: 该论文分析了有限样本下一维Wasserstein距离的信息处理能力，证明了它能够捕捉点态密度差异并与支撑差异协调，通过泊松过程和神经尖峰解码等实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当两个密度函数的支撑显著重叠但点态密度差异很大时，尚不清楚Wasserstein距离是否能准确识别这些差异及其在有限样本下的解析特征。

Method: 利用泊松过程并分离速率因子，分析一维Wasserstein距离在有限样本下的信息处理能力，并通过神经尖峰解码和氨基酸接触频率数据进行验证。

Result: 结果显示一维Wasserstein距离能够突出与速率和支撑相关的有意义的密度差异。

Conclusion: 一维Wasserstein距离能够有效捕捉点态密度差异，并与支撑差异信息协调，为有限样本下的密度差异分析提供了理论依据。

Abstract: Leveraging the Wasserstein distance -- a summation of sample-wise transport distances in data space -- is advantageous in many applications for measuring support differences between two underlying density functions. However, when supports significantly overlap while densities exhibit substantial pointwise differences, it remains unclear whether and how this transport information can accurately identify these differences, particularly their analytic characterization in finite-sample settings. We address this issue by conducting an analysis of the information processing capabilities of the one-dimensional Wasserstein distance with finite samples. By utilizing the Poisson process and isolating the rate factor, we demonstrate the capability of capturing the pointwise density difference with Wasserstein distances and how this information harmonizes with support differences. The analyzed properties are confirmed using neural spike train decoding and amino acid contact frequency data. The results reveal that the one-dimensional Wasserstein distance highlights meaningful density differences related to both rate and support.

</details>


### [202] [Method of Manufactured Learning for Solver-free Training of Neural Operators](https://arxiv.org/abs/2511.12890)
*Arth Sojitra,Omer San*

Main category: cs.LG

TL;DR: 提出MML方法，通过解析构造物理一致的数据集来训练神经算子，无需依赖数值模拟器生成数据，实现了求解器无关的算子学习框架。


<details>
  <summary>Details</summary>
Motivation: 传统神经算子训练依赖数值求解器生成数据，限制了可扩展性和对物理系统的探索。需要一种不依赖求解器的训练框架。

Method: 基于制造解方法，从受控解析空间采样光滑候选解，通过直接应用控制微分算子推导对应的强迫场。推理时将强迫项设为零恢复原控制方程。

Result: 在热传导、对流、Burgers和扩散反应等基准问题上，MML实现了高谱精度、低残差误差和良好的泛化能力。

Conclusion: MML通过将数据生成重构为解析合成过程，提供了可扩展、求解器无关的途径来构建物理基础的神经算子，无需依赖昂贵的数值模拟或实验数据。

Abstract: Training neural operators to approximate mappings between infinite-dimensional function spaces often requires extensive datasets generated by either demanding experimental setups or computationally expensive numerical solvers. This dependence on solver-based data limits scalability and constrains exploration across physical systems. Here we introduce the Method of Manufactured Learning (MML), a solver-independent framework for training neural operators using analytically constructed, physics-consistent datasets. Inspired by the classical method of manufactured solutions, MML replaces numerical data generation with functional synthesis, i.e., smooth candidate solutions are sampled from controlled analytical spaces, and the corresponding forcing fields are derived by direct application of the governing differential operators. During inference, setting these forcing terms to zero restores the original governing equations, allowing the trained neural operator to emulate the true solution operator of the system. The framework is agnostic to network architecture and can be integrated with any operator learning paradigm. In this paper, we employ Fourier neural operator as a representative example. Across canonical benchmarks including heat, advection, Burgers, and diffusion-reaction equations. MML achieves high spectral accuracy, low residual errors, and strong generalization to unseen conditions. By reframing data generation as a process of analytical synthesis, MML offers a scalable, solver-agnostic pathway toward constructing physically grounded neural operators that retain fidelity to governing laws without reliance on expensive numerical simulations or costly experimental data for training.

</details>


### [203] [Functional Mean Flow in Hilbert Space](https://arxiv.org/abs/2511.12898)
*Zhiqi Li,Yuchen Sun,Greg Turk,Bo Zhu*

Main category: cs.LG

TL;DR: FMF是一种在无限维希尔伯特空间中定义的一步生成模型，将Mean Flow框架扩展到函数域，提供了函数流匹配的理论公式和高效训练采样的实际实现。


<details>
  <summary>Details</summary>
Motivation: 将一步生成模型扩展到函数域，以处理时间序列、图像、PDE和3D几何等函数数据生成任务。

Method: 提出函数流匹配的理论框架，引入x1预测变体以提高稳定性，提供高效训练和采样的实际实现。

Result: 开发出实用的函数流匹配方法，适用于广泛的函数数据生成任务。

Conclusion: FMF是一个实用的函数域一步流匹配框架，可有效处理多种函数数据生成问题。

Abstract: We present Functional Mean Flow (FMF) as a one-step generative model defined in infinite-dimensional Hilbert space. FMF extends the one-step Mean Flow framework to functional domains by providing a theoretical formulation for Functional Flow Matching and a practical implementation for efficient training and sampling. We also introduce an $x_1$-prediction variant that improves stability over the original $u$-prediction form. The resulting framework is a practical one-step Flow Matching method applicable to a wide range of functional data generation tasks such as time series, images, PDEs, and 3D geometry.

</details>


### [204] [Contrastive Entropy Bounds for Density and Conditional Density Decomposition](https://arxiv.org/abs/2511.12903)
*Bo Hu,Jose C. Principe*

Main category: cs.LG

TL;DR: 该论文从贝叶斯高斯视角研究神经网络特征的可解释性，通过希尔伯特空间和分解方法分析多输出网络，提出用高斯算子的迹训练自编码器，用核范数训练混合密度网络，并设计了编码器-混合-解码器架构来提升样本多样性。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络特征的可解释性，从贝叶斯高斯视角理解优化成本与概率界限的关系，探索如何通过希尔伯特空间方法改进自编码器和混合密度网络的训练。

Method: 使用希尔伯特空间和分解方法处理多输出网络生成高斯混合的情况；提出用高斯算子的迹训练自编码器，用核范数作为散度训练混合密度网络；设计编码器-混合-解码器架构，解码器为多输出以生成多个中心。

Result: 发现自编码器的目标等价于最大化高斯算子的迹；提出用核范数最大化整体秩而非迹；提出的架构能增加样本多样性，防止网络输出常数解，并能在小方差高斯混合假设下定量分析上界。

Conclusion: 从贝叶斯高斯视角为神经网络特征提供了新的可解释性框架，提出的希尔伯特空间方法和架构设计为自编码器和混合密度网络的训练提供了新的理论工具和实践方案。

Abstract: This paper studies the interpretability of neural network features from a Bayesian Gaussian view, where optimizing a cost is reaching a probabilistic bound; learning a model approximates a density that makes the bound tight and the cost optimal, often with a Gaussian mixture density. The two examples are Mixture Density Networks (MDNs) using the bound for the marginal and autoencoders using the conditional bound. It is a known result, not only for autoencoders, that minimizing the error between inputs and outputs maximizes the dependence between inputs and the middle.
  We use Hilbert space and decomposition to address cases where a multiple-output network produces multiple centers defining a Gaussian mixture. Our first finding is that an autoencoder's objective is equivalent to maximizing the trace of a Gaussian operator, the sum of eigenvalues under bases orthonormal w.r.t. the data and model distributions. This suggests that, when a one-to-one correspondence as needed in autoencoders is unnecessary, we can instead maximize the nuclear norm of this operator, the sum of singular values, to maximize overall rank rather than trace. Thus the trace of a Gaussian operator can be used to train autoencoders, and its nuclear norm can be used as divergence to train MDNs.
  Our second test uses inner products and norms in a Hilbert space to define bounds and costs. Such bounds often have an extra norm compared to KL-based bounds, which increases sample diversity and prevents the trivial solution where a multiple-output network produces the same constant, at the cost of requiring a sample batch to estimate and optimize. We propose an encoder-mixture-decoder architecture whose decoder is multiple-output, producing multiple centers per sample, potentially tightening the bound. Assuming the data are small-variance Gaussian mixtures, this upper bound can be tracked and analyzed quantitatively.

</details>


### [205] [LinkedIn Profile Characteristics and Professional Success Indicators](https://arxiv.org/abs/2511.12905)
*Tania-Amanda Fredrick Eneye,Ashlesha Malla,Pawan Paudel*

Main category: cs.LG

TL;DR: 研究LinkedIn个人资料特征与职业成功的关系，通过机器学习模型分析62,000多个匿名档案，发现晋升可高度预测，但粉丝增长更复杂。


<details>
  <summary>Details</summary>
Motivation: 探索LinkedIn个人资料特征如何影响职业成功指标（晋升、粉丝数、职业发展速度），为专业人士优化LinkedIn形象和职业策略提供指导。

Method: 使用机器学习技术分析超过62,000个匿名LinkedIn档案数据，构建预测模型识别影响职业成功的关键因素。

Result: 晋升具有高度可预测性，但粉丝增长表现出更复杂的模式，需要更深入的分析。

Conclusion: 研究为专业人士提供了优化LinkedIn存在和职业发展策略的可操作见解，强调了不同成功指标的可预测性差异。

Abstract: This study explores the relationship between LinkedIn profile characteristics and professional success, focusing on the indicators of promotions, follower count, and career progression rate. By leveraging a dataset of over 62,000 anonymized LinkedIn profiles, we developed predictive models using machine learning techniques to identify the most influential factors driving professional success. Results indicate that while promotions are highly predictable, follower growth exhibits greater complexity. This research provides actionable insights for professionals seeking to optimize their LinkedIn presence and career strategies.

</details>


### [206] [AIF: Asynchronous Inference Framework for Cost-Effective Pre-Ranking](https://arxiv.org/abs/2511.12934)
*Zhi Kou,Xiang-Rong Sheng,Shuguang Han,Zhishan Zhao,Yueyao Cheng,Han Zhu,Jian Xu,Bo Zheng*

Main category: cs.LG

TL;DR: 提出了异步推理框架(AIF)，通过解耦用户/物品侧独立计算与实时预测，将交互无关组件并行化处理，显著提升工业推荐系统中预排序模型的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统预排序模型采用顺序执行框架，存在重复计算相同用户/物品以及严格顺序操作导致的延迟瓶颈，限制了模型容量和系统效率。

Method: AIF框架将交互无关组件从实时预测中解耦，用户侧计算与检索阶段并行执行，物品侧计算以近线方式完成，交互相关组件在在线实时预测中采用近似方法。

Result: AIF提高了计算效率并降低延迟，释放资源显著改善了交互无关组件的特征集和模型架构，在淘宝展示广告系统中成功部署。

Conclusion: 通过框架与模型的协同设计，AIF在不显著增加计算和延迟成本的情况下实现了显著的性能提升，为工业推荐系统提供了有效的解决方案。

Abstract: In industrial recommendation systems, pre-ranking models based on deep neural networks (DNNs) commonly adopt a sequential execution framework: feature fetching and model forward computation are triggered only after receiving candidates from the upstream retrieval stage. This design introduces inherent bottlenecks, including redundant computations of identical users/items and increased latency due to strictly sequential operations, which jointly constrain the model's capacity and system efficiency. To address these limitations, we propose the Asynchronous Inference Framework (AIF), a cost-effective computational architecture that decouples interaction-independent components, those operating within a single user or item, from real-time prediction. AIF reorganizes the model inference process by performing user-side computations in parallel with the retrieval stage and conducting item-side computations in a nearline manner. This means that interaction-independent components are calculated just once and completed before the real-time prediction phase of the pre-ranking stage. As a result, AIF enhances computational efficiency and reduces latency, freeing up resources to significantly improve the feature set and model architecture of interaction-independent components. Moreover, we delve into model design within the AIF framework, employing approximated methods for interaction-dependent components in online real-time predictions. By co-designing both the framework and the model, our solution achieves notable performance gains without significantly increasing computational and latency costs. This has enabled the successful deployment of AIF in the Taobao display advertising system.

</details>


### [207] [APT: Affine Prototype-Timestamp For Time Series Forecasting Under Distribution Shift](https://arxiv.org/abs/2511.12945)
*Yujie Li,Zezhi Shao,Chengqing Yu,Yisong Fu,Tao Sun,Yongjun Xu,Fei Wang*

Main category: cs.LG

TL;DR: 提出了Affine Prototype Timestamp (APT)模块，通过时间戳条件原型学习动态生成仿射参数，解决时间序列预测中的分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型依赖局部统计归一化，无法捕捉全局分布偏移；RevIN等方法在缺失值、噪声观测和无效通道仿射变换方面存在局限。

Method: 使用轻量级插件模块APT，通过时间戳条件原型学习注入全局分布特征，动态生成仿射参数来调制输入输出序列，使主干网络能够从自监督、分布感知的聚类实例中学习。

Result: 在六个基准数据集和多种主干-归一化组合上的广泛实验表明，APT显著提高了分布偏移下的预测性能。

Conclusion: APT是一个兼容任意预测主干和归一化策略的灵活模块，计算开销最小，能有效应对时间序列预测中的分布偏移挑战。

Abstract: Time series forecasting under distribution shift remains challenging, as existing deep learning models often rely on local statistical normalization (e.g., mean and variance) that fails to capture global distribution shift. Methods like RevIN and its variants attempt to decouple distribution and pattern but still struggle with missing values, noisy observations, and invalid channel-wise affine transformation. To address these limitations, we propose Affine Prototype Timestamp (APT), a lightweight and flexible plug-in module that injects global distribution features into the normalization-forecasting pipeline. By leveraging timestamp conditioned prototype learning, APT dynamically generates affine parameters that modulate both input and output series, enabling the backbone to learn from self-supervised, distribution-aware clustered instances. APT is compatible with arbitrary forecasting backbones and normalization strategies while introducing minimal computational overhead. Extensive experiments across six benchmark datasets and multiple backbone-normalization combinations demonstrate that APT significantly improves forecasting performance under distribution shift.

</details>


### [208] [A FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series](https://arxiv.org/abs/2511.12951)
*Ziling Fan,Ruijia Liang,Yiwen Hu*

Main category: cs.LG

TL;DR: 提出基于FEDformer的混合框架，用于金融时间序列异常检测和风险预测，在S&P 500、NASDAQ和布伦特原油数据集上表现优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 金融市场具有高度波动性，传统深度学习模型难以捕捉金融数据中的长期依赖和复杂周期模式，需要更有效的异常检测和风险预测方法。

Method: 集成频率增强分解Transformer(FEDformer)、基于残差的异常检测器和风险预测头，在时域和频域建模时间动态，分解趋势和季节成分。

Result: 相比基准方法，RMSE降低15.7%，异常检测F1分数提升11.5%，在多个金融数据集上验证了模型有效性。

Conclusion: 该模型能有效捕捉金融波动性，为市场崩盘预测和风险管理提供可靠的早期预警系统。

Abstract: Financial markets are inherently volatile and prone to sudden disruptions such as market crashes, flash collapses, and liquidity crises. Accurate anomaly detection and early risk forecasting in financial time series are therefore crucial for preventing systemic instability and supporting informed investment decisions. Traditional deep learning models, such as LSTM and GRU, often fail to capture long-term dependencies and complex periodic patterns in highly nonstationary financial data. To address this limitation, this study proposes a FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series, which integrates the Frequency Enhanced Decomposed Transformer (FEDformer) with a residual-based anomaly detector and a risk forecasting head. The FEDformer module models temporal dynamics in both time and frequency domains, decomposing signals into trend and seasonal components for improved interpretability. The residual-based detector identifies abnormal fluctuations by analyzing prediction errors, while the risk head predicts potential financial distress using learned latent embeddings. Experiments conducted on the S&P 500, NASDAQ Composite, and Brent Crude Oil datasets (2000-2024) demonstrate the superiority of the proposed model over benchmark methods, achieving a 15.7 percent reduction in RMSE and an 11.5 percent improvement in F1-score for anomaly detection. These results confirm the effectiveness of the model in capturing financial volatility, enabling reliable early-warning systems for market crash prediction and risk management.

</details>


### [209] [Global Cross-Time Attention Fusion for Enhanced Solar Flare Prediction from Multivariate Time Series](https://arxiv.org/abs/2511.12955)
*Onur Vural,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer的全局跨时间注意力融合(GCTAF)架构，用于解决太阳耀斑预测中时间序列数据不平衡的问题，通过可学习的全局令牌来增强长程时间建模能力。


<details>
  <summary>Details</summary>
Motivation: 太阳耀斑预测中的多变量时间序列分类面临数据不平衡的挑战，强耀斑事件稀少而弱耀斑事件常见，这阻碍了有效的学习。传统自注意力机制仅依赖时间序列内的局部交互，难以捕捉对耀斑预测至关重要的全局显著时间模式。

Method: GCTAF架构引入一组可学习的跨注意力全局令牌，这些令牌通过跨注意力与输入序列交互并融合回时间表示中，能够识别对耀斑预测关键的全局显著、非连续时间点，作为动态注意力驱动的时间摘要器。

Result: 在基准太阳耀斑数据集上的评估表明，GCTAF能有效检测强耀斑事件并提高预测性能，证明了改进基于Transformer的架构在太阳耀斑预测任务中的高潜力。

Conclusion: 通过精炼基于Transformer的架构，GCTAF为太阳耀斑预测任务提供了一种有前景的替代方案，能够更好地捕捉与耀斑相关的判别性动态特征。

Abstract: Multivariate time series classification is increasingly investigated in space weather research as a means to predict intense solar flare events, which can cause widespread disruptions across modern technological systems. Magnetic field measurements of solar active regions are converted into structured multivariate time series, enabling predictive modeling across segmented observation windows. However, the inherently imbalanced nature of solar flare occurrences, where intense flares are rare compared to minor flare events, presents a significant barrier to effective learning. To address this challenge, we propose a novel Global Cross-Time Attention Fusion (GCTAF) architecture, a transformer-based model to enhance long-range temporal modeling. Unlike traditional self-attention mechanisms that rely solely on local interactions within time series, GCTAF injects a set of learnable cross-attentive global tokens that summarize salient temporal patterns across the entire sequence. These tokens are refined through cross-attention with the input sequence and fused back into the temporal representation, enabling the model to identify globally significant, non-contiguous time points that are critical for flare prediction. This mechanism functions as a dynamic attention-driven temporal summarizer that augments the model's capacity to capture discriminative flare-related dynamics. We evaluate our approach on the benchmark solar flare dataset and show that GCTAF effectively detects intense flares and improves predictive performance, demonstrating that refining transformer-based architectures presents a high-potential alternative for solar flare prediction tasks.

</details>


### [210] [RAGPulse: An Open-Source RAG Workload Trace to Optimize RAG Serving Systems](https://arxiv.org/abs/2511.12979)
*Zhengchao Wang,Yitao Hu,Jianing Ye,Zhuxuan Chang,Jiazheng Yu,Youpeng Deng,Keqiu Li*

Main category: cs.LG

TL;DR: RAGPulse是一个开源RAG工作负载追踪数据集，收集自大学问答系统，用于解决现有LLM推理追踪无法捕捉RAG特定动态的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的通用LLM推理追踪无法捕捉RAG系统的多阶段流水线（检索、生成）和独特工作负载特征（如知识依赖），导致学术研究与实际部署之间存在显著性能差距。

Method: 从2024年4月起服务于4万多名师生的大学问答系统中收集数据，采用隐私保护的基于哈希的数据格式，并进行深入的统计分析。

Result: 分析显示真实世界的RAG工作负载具有显著的时间局部性和高度偏斜的热门文档访问模式。

Conclusion: RAGPulse为研究人员开发和验证RAG系统优化策略（如内容感知批处理和检索缓存）提供了高保真基础，最终提升RAG服务的效率和可靠性。

Abstract: Retrieval-Augmented Generation (RAG) is a critical paradigm for building reliable, knowledge-intensive Large Language Model (LLM) applications. However, the multi-stage pipeline (retrieve, generate) and unique workload characteristics (e.g., knowledge dependency) of RAG systems pose significant challenges for serving performance optimization. Existing generic LLM inference traces fail to capture these RAG-specific dynamics, creating a significant performance gap between academic research and real-world deployment. To bridge this gap, this paper introduces RAGPulse, an open-source RAG workload trace dataset. This dataset was collected from an university-wide Q&A system serving that has served more than 40,000 students and faculties since April 2024. We detail RAGPulse's system architecture, its privacy-preserving hash-based data format, and provide an in-depth statistical analysis. Our analysis reveals that real-world RAG workloads exhibit significant temporal locality and a highly skewed hot document access pattern. RAGPulse provides a high-fidelity foundation for researchers to develop and validate novel optimization strategies for RAG systems, such as content-aware batching and retrieval caching, ultimately enhancing the efficiency and reliability of RAG services. The code is available at https://github.com/flashserve/RAGPulse.

</details>


### [211] [Angular Gradient Sign Method: Uncovering Vulnerabilities in Hyperbolic Networks](https://arxiv.org/abs/2511.12985)
*Minsoo Jo,Dongyoon Yang,Taesup Kim*

Main category: cs.LG

TL;DR: 提出了一种针对双曲网络的几何感知对抗攻击方法，通过在双曲空间的切空间中计算梯度并仅从角度方向施加扰动，生成更有效的对抗样本。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法如FGSM和PGD未考虑双曲空间的几何结构，可能导致效率低下或几何不一致的攻击，需要重新评估非欧几何中的攻击策略。

Method: 在双曲空间的切空间中计算损失函数梯度，将其分解为径向（深度）分量和角度（语义）分量，仅从角度方向施加扰动，聚焦于双曲几何中编码的语义敏感方向。

Result: 在图像分类、跨模态检索任务和网络架构上的实验表明，该方法比传统对抗攻击获得更高的欺骗率，同时产生具有更高影响力的扰动。

Conclusion: 这项工作强调了在弯曲表示空间中几何感知对抗策略的重要性，并为攻击层次化嵌入提供了一个原则性框架。

Abstract: Adversarial examples in neural networks have been extensively studied in Euclidean geometry, but recent advances in \textit{hyperbolic networks} call for a reevaluation of attack strategies in non-Euclidean geometries. Existing methods such as FGSM and PGD apply perturbations without regard to the underlying hyperbolic structure, potentially leading to inefficient or geometrically inconsistent attacks. In this work, we propose a novel adversarial attack that explicitly leverages the geometric properties of hyperbolic space. Specifically, we compute the gradient of the loss function in the tangent space of hyperbolic space, decompose it into a radial (depth) component and an angular (semantic) component, and apply perturbation derived solely from the angular direction. Our method generates adversarial examples by focusing perturbations in semantically sensitive directions encoded in angular movement within the hyperbolic geometry. Empirical results on image classification, cross-modal retrieval tasks and network architectures demonstrate that our attack achieves higher fooling rates than conventional adversarial attacks, while producing high-impact perturbations with deeper insights into vulnerabilities of hyperbolic embeddings. This work highlights the importance of geometry-aware adversarial strategies in curved representation spaces and provides a principled framework for attacking hierarchical embeddings.

</details>


### [212] [Learning Branching Policies for MILPs with Proximal Policy Optimization](https://arxiv.org/abs/2511.12986)
*Abdelouahed Ben Mhamed,Assia Kamal-Idrissi,Amal El Fallah Seghrouchni*

Main category: cs.LG

TL;DR: 提出了TGPPO框架，使用PPO强化学习算法训练分支策略，以提升在异构MILP实例上的泛化能力，相比现有基于模仿学习的方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统分支定界算法在大规模MILP问题上存在指数级复杂度问题，现有基于模仿学习的分支策略容易过拟合专家演示，难以泛化到结构多样或未见过的实例。

Method: 基于参数化状态空间表示动态捕捉搜索树演化上下文，采用PPO强化学习算法训练分支策略，而非传统的模仿学习。

Result: 实验评估显示TGPPO在减少探索节点数和改进p-原始对偶积分方面优于现有学习策略，特别是在分布外实例上表现突出。

Conclusion: 强化学习具有开发鲁棒且适应性强的MILP求解器分支策略的潜力。

Abstract: Branch-and-Bound (B\&B) is the dominant exact solution method for Mixed Integer Linear Programs (MILP), yet its exponential time complexity poses significant challenges for large-scale instances. The growing capabilities of machine learning have spurred efforts to improve B\&B by learning data-driven branching policies. However, most existing approaches rely on Imitation Learning (IL), which tends to overfit to expert demonstrations and struggles to generalize to structurally diverse or unseen instances. In this work, we propose Tree-Gate Proximal Policy Optimization (TGPPO), a novel framework that employs Proximal Policy Optimization (PPO), a Reinforcement Learning (RL) algorithm, to train a branching policy aimed at improving generalization across heterogeneous MILP instances. Our approach builds on a parameterized state space representation that dynamically captures the evolving context of the search tree. Empirical evaluations show that TGPPO often outperforms existing learning-based policies in terms of reducing the number of nodes explored and improving p-Primal-Dual Integrals (PDI), particularly in out-of-distribution instances. These results highlight the potential of RL to develop robust and adaptable branching strategies for MILP solvers.

</details>


### [213] [Are Graph Transformers Necessary? Efficient Long-Range Message Passing with Fractal Nodes in MPNNs](https://arxiv.org/abs/2511.13010)
*Jeongwhan Choi,Seungjun Park,Sumin Park,Sung-Bae Cho,Noseong Park*

Main category: cs.LG

TL;DR: 提出了分形节点概念，通过在图中引入子图级特征表示来增强MPNN的长程依赖能力，同时保持计算效率


<details>
  <summary>Details</summary>
Motivation: GNN在平衡局部和全局信息方面存在困难，图Transformer虽然能处理长程交互但忽略了MPNN的局部性和效率优势

Method: 基于真实网络中的分形结构，提出分形节点概念，让分形节点与原节点共存并自适应聚合子图级特征表示，强制子图内特征相似性

Result: 分形节点通过提供直接捷径连接缓解了过度压缩问题，实验表明该方法提升了MPNN的表达能力，性能与图Transformer相当或更好

Conclusion: 该方法成功增强了MPNN的长程依赖能力，在保持计算效率的同时实现了与图Transformer相当的性能

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning on graph-structured data, but often struggle to balance local and global information. While graph Transformers aim to address this by enabling long-range interactions, they often overlook the inherent locality and efficiency of Message Passing Neural Networks (MPNNs). We propose a new concept called fractal nodes, inspired by the fractal structure observed in real-world networks. Our approach is based on the intuition that graph partitioning naturally induces fractal structure, where subgraphs often reflect the connectivity patterns of the full graph. Fractal nodes are designed to coexist with the original nodes and adaptively aggregate subgraph-level feature representations, thereby enforcing feature similarity within each subgraph. We show that fractal nodes alleviate the over-squashing problem by providing direct shortcut connections that enable long-range propagation of subgraph-level representations. Experiment results show that our method improves the expressive power of MPNNs and achieves comparable or better performance to graph Transformers while maintaining the computational efficiency of MPNN by improving the long-range dependencies of MPNN.

</details>


### [214] [The Good, The Bad, and The Hybrid: A Reward Structure Showdown in Reasoning Models Training](https://arxiv.org/abs/2511.13016)
*Subramanyam Sahoo*

Main category: cs.LG

TL;DR: 提出了一个统一框架来研究硬奖励、连续奖励和混合奖励结构，用于在数学推理任务上微调大语言模型。通过Qwen3-4B模型在GSM8K数据集上的实验，展示了混合奖励结构在收敛速度和训练稳定性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 奖励设计在基于人类反馈的强化学习和模型对齐研究中至关重要，需要研究不同奖励结构对数学推理任务微调的影响。

Method: 使用Qwen3-4B模型和LoRA微调技术，在GSM8K数据集上形式化并实证评估了包含正确性、困惑度、推理质量和一致性的奖励公式，引入了自适应混合奖励调度器在离散和连续信号之间转换。

Result: 混合奖励结构相比纯硬奖励或纯连续方法，提高了收敛速度和训练稳定性。

Conclusion: 自适应奖励建模为模型对齐提供了有价值的见解，混合奖励结构在数学推理任务中表现出更好的性能。

Abstract: Reward design is central to reinforcement learning from human feedback (RLHF) and alignment research. In this work, we propose a unified framework to study hard, continuous, and hybrid reward structures for fine-tuning large language models (LLMs) on mathematical reasoning tasks. Using Qwen3-4B with LoRA fine-tuning on the GSM8K dataset, we formalize and empirically evaluate reward formulations that incorporate correctness, perplexity, reasoning quality, and consistency. We introduce an adaptive hybrid reward scheduler that transitions between discrete and continuous signals, balancing exploration and stability. Our results show that hybrid reward structures improve convergence speed and training stability over purely hard or continuous approaches, offering insights for alignment via adaptive reward modeling.

</details>


### [215] [The Final-Stage Bottleneck: A Systematic Dissection of the R-Learner for Network Causal Inference](https://arxiv.org/abs/2511.13018)
*Sairam S,Sara Girdhar,Shivam Soni*

Main category: cs.LG

TL;DR: R-Learner在图上应用时存在严重的表示瓶颈，图盲的最终阶段模型会完全失败，而端到端的Graph R-Learner能显著超越传统基线方法。


<details>
  <summary>Details</summary>
Motivation: R-Learner框架虽然强大，但其核心假设在应用于网络数据时面临挑战，需要系统研究其在图数据上的表现。

Method: 通过大规模实证研究，分析R-Learner在图上的表现，重点关注最终阶段CATE估计器的归纳偏置和拓扑依赖的nuisance瓶颈。

Result: 发现图盲最终阶段模型完全失败(MSE>4.0)，而Graph R-Learner显著优于传统基线；识别出与GNN过度压缩相关的nuisance瓶颈。

Conclusion: R-Learner在图上的性能主要受最终阶段模型归纳偏置驱动，存在严重的表示瓶颈，需要端到端的图感知方法。

Abstract: The R-Learner is a powerful, theoretically-grounded framework for estimating heterogeneous treatment effects, prized for its robustness to nuisance model errors. However, its application to network data, where causal heterogeneity is often graph-dependent, presents a critical challenge to its core assumption of a well-specified final-stage model. In this paper, we conduct a large-scale empirical study to systematically dissect the R-Learner framework on graphs. We provide the first rigorous evidence that the primary driver of performance is the inductive bias of the final-stage CATE estimator, an effect that dominates the choice of nuisance models. Our central finding is the quantification of a catastrophic "representation bottleneck": we prove with overwhelming statistical significance (p < 0.001) that R-Learners with a graph-blind final stage fail completely (MSE > 4.0), even when paired with powerful GNN nuisance models. Conversely, our proposed end-to-end Graph R-Learner succeeds and significantly outperforms a strong, non-DML GNN T-Learner baseline. Furthermore, we identify and provide a mechanistic explanation for a subtle, topology-dependent "nuisance bottleneck," linking it to GNN over-squashing via a targeted "Hub-Periphery Trade-off" analysis. Our findings are validated across diverse synthetic and semi-synthetic benchmarks. We release our code as a reproducible benchmark to facilitate future research on this critical "final-stage bottleneck."

</details>


### [216] [Learning Time-Scale Invariant Population-Level Neural Representations](https://arxiv.org/abs/2511.13022)
*Eshani Patel,Yisong Yue,Geeling Chau*

Main category: cs.LG

TL;DR: TSAP方法通过时间尺度增强预训练，解决了神经基础模型对预处理时间尺度不匹配的敏感性问题，提高了表示的空间不变性和下游任务的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有神经时间序列基础模型对预训练和下游任务之间的预处理不匹配（特别是时间尺度）很敏感，这限制了模型的泛化能力和实际应用。

Method: 提出了时间尺度增强预训练（TSAP），通过在预训练阶段引入时间尺度变化来增强模型对时间尺度差异的鲁棒性。

Result: TSAP在不同解码任务中一致地提高了对时间尺度变化的鲁棒性，并在表示空间中构建了不变性。

Conclusion: 处理预处理多样性是构建可泛化神经基础模型的关键步骤，TSAP为此提供了有效解决方案。

Abstract: General-purpose foundation models for neural time series can help accelerate neuroscientific discoveries and enable applications such as brain computer interfaces (BCIs). A key component in scaling these models is population-level representation learning, which leverages information across channels to capture spatial as well as temporal structure. Population-level approaches have recently shown that such representations can be both efficient to learn on top of pretrained temporal encoders and produce useful representations for decoding a variety of downstream tasks. However, these models remain sensitive to mismatches in preprocessing, particularly on time-scales, between pretraining and downstream settings. We systematically examine how time-scale mismatches affects generalization and find that existing representations lack invariance. To address this, we introduce Time-scale Augmented Pretraining (TSAP), which consistently improves robustness to different time-scales across decoding tasks and builds invariance in the representation space. These results highlight handling preprocessing diversity as a key step toward building generalizable neural foundation models.

</details>


### [217] [SLMQuant:Benchmarking Small Language Model Quantization for Practical Deployment](https://arxiv.org/abs/2511.13023)
*Jiacheng Wang,Yejun Zeng,Jinyang Guo,Yuqing Ma,Aishan Liu,Xianglong Liu*

Main category: cs.LG

TL;DR: SLMQuant是首个系统评估LLM压缩技术在SLM上应用的基准，发现SLM与LLM在量化敏感性上存在根本差异，直接迁移LLM优化技术会导致次优结果。


<details>
  <summary>Details</summary>
Motivation: 尽管小型语言模型作为资源高效替代方案受到关注，但在边缘设备上的部署仍然面临挑战，因为模型压缩的效率差距尚未解决。量化对LLM有效，但对SLM的适用性研究不足。

Method: 通过跨多种架构和任务的综合多轨道评估，分析最先进量化方法在SLM上的表现。

Result: 发现SLM与LLM在量化敏感性上存在根本差异，直接迁移LLM优化技术会导致次优结果，这源于SLM独特的架构特征和训练动态。

Conclusion: SLMQuant为在边缘应用中推进高效SLM部署建立了基础框架，并为在资源受限场景中部署轻量级语言模型提供了关键见解。

Abstract: Despite the growing interest in Small Language Models (SLMs) as resource-efficient alternatives to Large Language Models (LLMs), their deployment on edge devices remains challenging due to unresolved efficiency gaps in model compression. While quantization has proven effective for LLMs, its applicability to SLMs is significantly underexplored, with critical questions about differing quantization bottlenecks and efficiency profiles. This paper introduces SLMQuant, the first systematic benchmark for evaluating LLM compression techniques when applied to SLMs. Through comprehensive multi-track evaluations across diverse architectures and tasks, we analyze how state-of-the-art quantization methods perform on SLMs. Our findings reveal fundamental disparities between SLMs and LLMs in quantization sensitivity, demonstrating that direct transfer of LLM-optimized techniques leads to suboptimal results due to SLMs' unique architectural characteristics and training dynamics. We identify key factors governing effective SLM quantization and propose actionable design principles for SLM-tailored compression. SLMQuant establishes a foundational framework for advancing efficient SLM deployment on low-end devices in edge applications, and provides critical insights for deploying lightweight language models in resource-constrained scenarios.

</details>


### [218] [One-Step Generative Policies with Q-Learning: A Reformulation of MeanFlow](https://arxiv.org/abs/2511.13035)
*Zeyuan Wang,Da Li,Yulin Chen,Ye Shi,Liang Bai,Tianyuan Yu,Yanwei Fu*

Main category: cs.LG

TL;DR: 提出一种单步生成策略，通过MeanFlow的残差重构实现从噪声直接生成动作，兼容Q学习，在离线强化学习中实现高效推理和多模态动作分布建模。


<details>
  <summary>Details</summary>
Motivation: 现有的一步高斯策略推理速度快但难以捕捉复杂多模态动作分布，而基于流的方法表达能力更强但通常需要蒸馏和两阶段训练。需要一种既能高效推理又能表达多模态分布的单阶段训练方法。

Method: 重构MeanFlow，将速度场和噪声到动作的变换集成到单一策略网络中，提出有效的残差重构形式，支持直接噪声到动作生成。

Result: 在OGBench和D4RL基准的73个任务上进行了广泛实验，在离线和离线到在线强化学习设置中均取得了强劲性能。

Conclusion: 该方法实现了三个关键优势：高效单步噪声到动作生成、多模态动作分布的表达能力、通过Q学习在单阶段训练中实现高效稳定的策略学习。

Abstract: We introduce a one-step generative policy for offline reinforcement learning that maps noise directly to actions via a residual reformulation of MeanFlow, making it compatible with Q-learning. While one-step Gaussian policies enable fast inference, they struggle to capture complex, multimodal action distributions. Existing flow-based methods improve expressivity but typically rely on distillation and two-stage training when trained with Q-learning. To overcome these limitations, we propose to reformulate MeanFlow to enable direct noise-to-action generation by integrating the velocity field and noise-to-action transformation into a single policy network-eliminating the need for separate velocity estimation. We explore several reformulation variants and identify an effective residual formulation that supports expressive and stable policy learning. Our method offers three key advantages: 1) efficient one-step noise-to-action generation, 2) expressive modelling of multimodal action distributions, and 3) efficient and stable policy learning via Q-learning in a single-stage training setup. Extensive experiments on 73 tasks across the OGBench and D4RL benchmarks demonstrate that our method achieves strong performance in both offline and offline-to-online reinforcement learning settings. Code is available at https://github.com/HiccupRL/MeanFlowQL.

</details>


### [219] [Bi-View Embedding Fusion: A Hybrid Learning Approach for Knowledge Graph's Nodes Classification Addressing Problems with Limited Data](https://arxiv.org/abs/2511.13044)
*Rosario Napoli,Giovanni Lonia,Antonio Celesti,Massimo Villari,Maria Fazio*

Main category: cs.LG

TL;DR: 提出了一种名为Bi-View的混合方法，通过增强知识图谱中节点特征的信息内容来生成改进的图嵌入，从而提高图机器学习模型的性能，无需依赖额外的合成数据。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法需要大量数据，在稀疏或不完整场景中表现受限；图机器学习虽然提供了替代方案，但在处理知识图谱时仍面临信息隐藏的挑战。

Method: 结合Node2Vec（通过无监督随机游走捕获结构模式）和GraphSAGE（以监督方式聚合邻域信息）两种互补的图嵌入技术，并添加基于中心性的度量来丰富节点特征，最后通过融合层整合两种表示。

Result: 该方法提高了下游任务的性能，特别是在初始特征较差的情况下，为更准确和精确的知识图谱增强图机器学习模型奠定了基础。

Conclusion: Bi-View方法能够同时捕获图的拓扑和语义属性，使模型能够利用数据集中存在但未明确表示的信息特征，从而改善图机器学习模型的性能。

Abstract: Traditional Machine Learning (ML) methods require large amounts of data to perform well, limiting their applicability in sparse or incomplete scenarios and forcing the usage of additional synthetic data to improve the model training. To overcome this challenge, the research community is looking more and more at Graph Machine Learning (GML) as it offers a powerful alternative by using relationships within data. However, this method also faces limitations, particularly when dealing with Knowledge Graphs (KGs), which can hide huge information due to their semantic nature. This study introduces Bi-View, a novel hybrid approach that increases the informative content of node features in KGs to generate enhanced Graph Embeddings (GEs) that are used to improve GML models without relying on additional synthetic data. The proposed work combines two complementary GE techniques: Node2Vec, which captures structural patterns through unsupervised random walks, and GraphSAGE, which aggregates neighbourhood information in a supervised way. Node2Vec embeddings are first computed to represent the graph topology, and node features are then enriched with centrality-based metrics, which are used as input for the GraphSAGE model. Moreover, a fusion layer combines the original Node2Vec embeddings with the GraphSAGE-influenced representations, resulting in a dual-perspective embedding space. Such a fusion captures both topological and semantic properties of the graph, enabling the model to exploit informative features that may exist in the dataset but that are not explicitly represented. Our approach improves downstream task performance, especially in scenarios with poor initial features, giving the basis for more accurate and precise KG-enanched GML models.

</details>


### [220] [Generalization Bounds for Semi-supervised Matrix Completion with Distributional Side Information](https://arxiv.org/abs/2511.13049)
*Antoine Ledent,Mun Chong Soo,Nong Minh Hieu*

Main category: cs.LG

TL;DR: 该论文研究了一个矩阵补全问题，其中真实矩阵R和未知采样分布P都是低秩矩阵且共享共同子空间。利用大量未标记数据（隐式反馈）和少量标记数据（显式反馈），提出了结合两种反馈的推荐系统方法。


<details>
  <summary>Details</summary>
Motivation: 受推荐系统启发，其中未标记数据对应隐式反馈（如点击、购买），标记数据对应显式反馈（如用户评分）。研究如何在显式反馈稀缺的情况下，利用大量隐式反馈提升推荐性能。

Method: 利用低秩子空间恢复理论和矩阵补全模型的经典泛化边界，将误差分解为两个独立项：一个与未标记数据量M相关，另一个与标记数据量N相关。

Result: 理论分析得到误差边界为O(√(nd/M)) + O(√(dr/N))，其中d是P的秩，r是R的秩。在合成和真实数据集（Douban、MovieLens）上的实验验证了方法的有效性。

Conclusion: 该方法在显式评分稀缺的情况下优于仅依赖显式反馈的基线方法，证明了所提假设为研究推荐系统中显式和隐式反馈交互提供了有效的理论框架。

Abstract: We study a matrix completion problem where both the ground truth $R$ matrix and the unknown sampling distribution $P$ over observed entries are low-rank matrices, and \textit{share a common subspace}. We assume that a large amount $M$ of \textit{unlabeled} data drawn from the sampling distribution $P$ is available, together with a small amount $N$ of labeled data drawn from the same distribution and noisy estimates of the corresponding ground truth entries. This setting is inspired by recommender systems scenarios where the unlabeled data corresponds to `implicit feedback' (consisting in interactions such as purchase, click, etc. ) and the labeled data corresponds to the `explicit feedback', consisting of interactions where the user has given an explicit rating to the item. Leveraging powerful results from the theory of low-rank subspace recovery, together with classic generalization bounds for matrix completion models, we show error bounds consisting of a sum of two error terms scaling as $\widetilde{O}\left(\sqrt{\frac{nd}{M}}\right)$ and $\widetilde{O}\left(\sqrt{\frac{dr}{N}}\right)$ respectively, where $d$ is the rank of $P$ and $r$ is the rank of $M$. In synthetic experiments, we confirm that the true generalization error naturally splits into independent error terms corresponding to the estimations of $P$ and and the ground truth matrix $\ground$ respectively. In real-life experiments on Douban and MovieLens with most explicit ratings removed, we demonstrate that the method can outperform baselines relying only on the explicit ratings, demonstrating that our assumptions provide a valid toy theoretical setting to study the interaction between explicit and implicit feedbacks in recommender systems.

</details>


### [221] [Learning from the Undesirable: Robust Adaptation of Language Models without Forgetting](https://arxiv.org/abs/2511.13052)
*Yunhun Nam,Jaehyung Kim,Jongheon Jeong*

Main category: cs.LG

TL;DR: 提出LfU方法，通过一致性正则化来缓解语言模型在有限数据下监督微调时的过拟合问题，提升泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在有限数据下进行监督微调时，语言模型容易过拟合，依赖虚假模式或损害预训练获得的有用能力。

Method: 提出LfU方法，通过一致性正则化使模型对不良模型更新具有鲁棒性，直接对齐模型内部表示与不良更新后的表示。

Result: 在数学任务上比标准SFT平均提升16.8%，输出性能标准差降低92.1%，显示更好的鲁棒性。

Conclusion: LfU作为有效先验，在有限数据下增强适应性同时保留预训练知识，具有广泛适用性。

Abstract: Language models (LMs) are often adapted through supervised fine-tuning (SFT) to specialize their capabilities for downstream tasks. However, in typical scenarios where the fine-tuning data is limited, e.g., compared to pre-training, SFT can lead LMs to overfit, causing them to rely on spurious patterns within the target task or to compromise other broadly useful capabilities as a side effect of narrow specialization. In this paper, we propose Learning-from-the-Undesirable (LfU), a simple yet effective regularization scheme for SFT to mitigate overfitting issues when fine-tuning LMs with limited data. Specifically, we aim to regularize the fine-tuning process to favor solutions that are resilient to "undesirable" model updates, e.g., gradient ascent steps that steer the model toward undesirable behaviors. To this end, we propose a novel form of consistency regularization that directly aligns internal representations of the model with those after an undesirable update. By leveraging representation-level data augmentation through undesirable updates, LfU effectively promotes generalization under limited data. Our experiments on diverse LM downstream tasks show that LfU serves as an effective prior that enhances adaptability while preserving pretrained knowledge. For example, our LM from LfU achieves a 16.8% average improvement on math tasks compared to vanilla SFT on the same dataset, where the latter even leads to degraded performance on those tasks. Furthermore, LfU exhibits improved robustness to prompt variations, e.g., yielding a 92.1% lower standard deviation in output performances compared to SFT, highlighting its versatile effects.

</details>


### [222] [Self-Organization of Attractor Landscapes in High-Capacity Kernel Logistic Regression Hopfield Networks](https://arxiv.org/abs/2511.13053)
*Akira Tamamori*

Main category: cs.LG

TL;DR: 通过几何分析Hopfield网络的能量景观，发现存在"优化脊"现象，在高负载和全局核条件下网络通过直接驱动力与反馈力的强反相关性最大化吸引子稳定性。


<details>
  <summary>Details</summary>
Motivation: 理解基于核的学习方法如何显著增强Hopfield网络存储容量的动力学机制，目前对这一增强机制的理解仍然不足。

Method: 引入"峰顶锐度"度量吸引子局部稳定性，通过系统变化核宽度和存储负载，分析能量景观梯度分解为直接"驱动"力和间接"反馈"力。

Result: 发现了丰富的吸引子形状相图，中心发现是"优化脊"的出现，网络在该区域通过强反相关的直接力与反馈力最大化吸引子稳定性。

Conclusion: 网络通过自适应地利用模式间相互作用作为协作反馈控制系统来塑造鲁棒的能量景观，为高容量联想记忆的稳定性提供了新的物理图像和设计原则。

Abstract: Kernel-based learning methods can dramatically increase the storage capacity of Hopfield networks, yet the dynamical mechanism behind this enhancement remains poorly understood. We address this gap by conducting a geometric analysis of the network's energy landscape. We introduce a novel metric, ``Pinnacle Sharpness,'' to quantify the local stability of attractors. By systematically varying the kernel width and storage load, we uncover a rich phase diagram of attractor shapes. Our central finding is the emergence of a ``ridge of optimization,'' where the network maximizes attractor stability under challenging high-load and global-kernel conditions. Through a theoretical decomposition of the landscape gradient into a direct ``driving'' force and an indirect ``feedback'' force, we reveal the origin of this phenomenon. The optimization ridge corresponds to a regime of strong anti-correlation between the two forces, where the direct force, amplified by the high storage load, dominates the opposing collective feedback force. This demonstrates a sophisticated self-organization mechanism: the network adaptively harnesses inter-pattern interactions as a cooperative feedback control system to sculpt a robust energy landscape. Our findings provide a new physical picture for the stability of high-capacity associative memories and offer principles for their design.

</details>


### [223] [Latency and Ordering Effects in Online Decisions](https://arxiv.org/abs/2511.13060)
*Duo Yi*

Main category: cs.LG

TL;DR: 该论文提出了一个针对延迟反馈和顺序敏感动态的在线决策系统的理论框架，通过Bregman散度作为损失基准，证明了超额基准损失的结构化下界，并扩展到非凸情况。


<details>
  <summary>Details</summary>
Motivation: 在线决策系统经常在延迟反馈和顺序敏感动态下运行，其中行动会影响观察的到达时间和顺序。需要一种能够量化延迟、非交换性和实现差距影响的统一理论框架。

Method: 使用Bregman散度作为损失基准，证明超额基准损失的结构化下界，包含延迟惩罚、顺序敏感性惩罚及其几何交互项，并扩展到近似正则和弱凸设置。

Result: 获得了包含四个可解释项的结构化下界：理想损失、延迟惩罚、顺序敏感性惩罚及其交互项，减去非凸性惩罚。提供了通过2×2随机实验和流式诊断来估计这些项的实用方法。

Conclusion: 该框架将异构延迟、非交换性和实现差距效应打包成一个单一可解释的下界陈述，可以在实际系统中进行压力测试和调优，为在线决策系统提供了稳健的理论保证。

Abstract: Online decision systems routinely operate under delayed feedback and order-sensitive (noncommutative) dynamics: actions affect which observations arrive, and in what sequence. Taking a Bregman divergence $D_Φ$ as the loss benchmark, we prove that the excess benchmark loss admits a structured lower bound $L \ge L_{\mathrm{ideal}} + g_1(λ) + g_2(\varepsilon_\star) + g_{12}(λ,\varepsilon_\star) - D_{\mathrm{ncx}}$, where $g_1$ and $g_2$ are calibrated penalties for latency and order-sensitivity, $g_{12}$ captures their geometric interaction, and $D_{\mathrm{ncx}}\ge 0$ is a nonconvexity/approximation penalty that vanishes under convex Legendre assumptions. We extend this inequality to prox-regular and weakly convex settings, obtaining robust guarantees beyond the convex case. We also give an operational recipe for estimating and monitoring the four terms via simple $2\times 2$ randomized experiments and streaming diagnostics (effective sample size, clipping rate, interaction heatmaps). The framework packages heterogeneous latency, noncommutativity, and implementation-gap effects into a single interpretable lower-bound statement that can be stress-tested and tuned in real-world systems.

</details>


### [224] [MACKO: Sparse Matrix-Vector Multiplication for Low Sparsity](https://arxiv.org/abs/2511.13061)
*Vladimír Macko,Vladimír Boža*

Main category: cs.LG

TL;DR: MACKO-SpMV是一种GPU优化的稀疏矩阵向量乘法格式和内核，专门针对稀疏大语言模型中的非结构化稀疏性设计，能在50%稀疏度下实现1.5倍内存减少和1.2-1.5倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有SpMV方法在稀疏LLMs中常见的低且非结构化稀疏度（30-90%）下表现不佳，非结构化剪枝只能提供有限的内存减少和加速效果。

Method: 提出MACKO-SpMV，这是一种GPU优化的格式和内核协同设计，减少存储开销同时保持与GPU执行模型的兼容性，无需专用硬件单元或格式特定的预计算。

Result: 在50%稀疏度下，MACKO首次实现显著的内存减少（1.5倍）和加速（1.2-1.5倍），相比其他SpMV基线：比cuSPARSE快2.8-13.0倍，比Sputnik快1.9-2.6倍，比DASP快2.2-2.5倍。应用于Llama2-7B模型时，在fp16精度下实现1.5倍内存减少和1.5倍推理加速。

Conclusion: MACKO使得50%稀疏度的非结构化剪枝在实际LLM工作负载中变得可行和合理。

Abstract: Sparse Matrix-Vector Multiplication (SpMV) is a fundamental operation in the inference of sparse Large Language Models (LLMs). Because existing SpMV methods perform poorly under the low and unstructured sparsity (30-90%) commonly observed in pruned LLMs, unstructured pruning provided only limited memory reduction and speedup. We propose MACKO-SpMV, a GPU-optimized format and kernel co-designed to reduce storage overhead while preserving compatibility with the GPU's execution model. This enables efficient SpMV for unstructured sparsity without specialized hardware units (e.g., tensor cores) or format-specific precomputation. Empirical results show that at sparsity 50%, MACKO is the first approach with significant 1.5x memory reduction and 1.2-1.5x speedup over dense representation. Speedups over other SpMV baselines: 2.8-13.0x over cuSPARSE, 1.9-2.6x over Sputnik, and 2.2-2.5x over DASP. Applied to Llama2-7B pruned with Wanda to sparsity 50%, it delivers 1.5x memory reduction and 1.5x faster inference at fp16 precision. Thanks to MACKO, unstructured pruning at 50% sparsity is now justified in real-world LLM workloads.

</details>


### [225] [Self-Adaptive Graph Mixture of Models](https://arxiv.org/abs/2511.13062)
*Mohit Meena,Yash Punjabi,Abhishek A,Vishal Sharma,Mahesh Chandran*

Main category: cs.LG

TL;DR: 提出了SAGMM框架，通过自适应选择和组合多种GNN架构来解决模型选择难题，在16个基准数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前GNN性能提升趋于平缓，传统模型经过适当调优后能与复杂架构匹敌甚至超越，但为特定图任务选择最合适模型仍然困难。

Method: SAGMM采用模块化框架，利用架构多样性和拓扑感知注意力门控机制，为每个节点自适应分配专家模型，并包含剪枝机制提高效率。

Result: 在节点分类、图分类、回归和链接预测等16个基准数据集上，SAGMM始终优于或匹配领先的GNN基线和现有混合方法。

Conclusion: SAGMM为现实世界图学习提供了鲁棒且自适应的解决方案，能够自动选择最适合的GNN模型组合。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning over graph-structured data, yet recent studies have shown that their performance gains are beginning to plateau. In many cases, well-established models such as GCN and GAT, when appropriately tuned, can match or even exceed the performance of more complex, state-of-the-art architectures. This trend highlights a key limitation in the current landscape: the difficulty of selecting the most suitable model for a given graph task or dataset. To address this, we propose Self-Adaptive Graph Mixture of Models (SAGMM), a modular and practical framework that learns to automatically select and combine the most appropriate GNN models from a diverse pool of architectures. Unlike prior mixture-of-experts approaches that rely on variations of a single base model, SAGMM leverages architectural diversity and a topology-aware attention gating mechanism to adaptively assign experts to each node based on the structure of the input graph. To improve efficiency, SAGMM includes a pruning mechanism that reduces the number of active experts during training and inference without compromising performance. We also explore a training-efficient variant in which expert models are pretrained and frozen, and only the gating and task-specific layers are trained. We evaluate SAGMM on 16 benchmark datasets covering node classification, graph classification, regression, and link prediction tasks, and demonstrate that it consistently outperforms or matches leading GNN baselines and prior mixture-based methods, offering a robust and adaptive solution for real-world graph learning.

</details>


### [226] [A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning](https://arxiv.org/abs/2511.13078)
*Liuyi Jin,Pasan Gunawardena,Amran Haroon,Runzhi Wang,Sangwoo Lee,Radu Stoleru,Michael Middleton,Zepeng Huo,Jeeeun Kim,Jason Moats*

Main category: cs.LG

TL;DR: EMSGlass是一个基于EMSNet多模态多任务模型的智能眼镜系统，通过集成文本、生命体征和场景图像来实时理解EMS事件，显著提升了急救医疗技术人员的决策速度和操作效率。


<details>
  <summary>Details</summary>
Motivation: 急救医疗技术人员在高压环境下需要快速做出关键决策，面临沉重的认知和操作负担，需要智能辅助系统来提升实时态势感知和决策效率。

Method: 开发了EMSNet多模态多任务模型，集成文本、生命体征和场景图像，同时支持五个关键EMS任务；构建了EMSServe低延迟多模态服务框架，包含模态感知模型分割器和特征缓存机制。

Result: EMSNet在真实多模态EMS数据集上训练，准确率优于最先进的单模态基线；EMSServe在EMS场景中比直接PyTorch多模态推理快1.9-11.7倍；用户研究显示EMSGlass显著提升了实时态势感知、决策速度和操作效率。

Conclusion: EMSGlass成功将多模态智能与现实世界急救响应工作流程相结合，为下一代AI赋能的EMS系统提供了可行方向，通过直观的眼镜交互增强了急救医疗技术人员的操作能力。

Abstract: Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present EMSGlass, a smart-glasses system powered by EMSNet, the first multimodal multitask model for Emergency Medical Services (EMS), and EMSServe, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves 1.9x -- 11.7x speedup over direct PyTorch multimodal inference. A user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows.

</details>


### [227] [Real-time prediction of breast cancer sites using deformation-aware graph neural network](https://arxiv.org/abs/2511.13082)
*Kyunghyun Lee,Yong-Min Shin,Minwoo Shin,Jihun Kim,Sunghwan Lim,Won-Yong Shin,Kyungho Yoon*

Main category: cs.LG

TL;DR: 开发基于图神经网络的实时变形预测模型，用于乳腺癌活检中准确预测肿瘤位移，精度达0.2毫米，计算速度比传统有限元模拟快4000倍。


<details>
  <summary>Details</summary>
Motivation: 解决间接MRI引导活检中实时变形乳房模型精度不足的问题，克服直接MRI引导活检时间长、成本高的限制。

Method: 结合MRI图像结构信息构建个体特异性有限元模型，使用图神经网络处理表面位移和距离图数据，预测组织整体位移和肿瘤区域变形。

Result: 在体模和真实患者数据验证中，癌症节点位移RMSE小于0.2毫米，与实际癌变区域空间重叠DSC达0.977，实现实时推理。

Conclusion: 该变形感知GNN模型为乳腺癌活检提供了高精度、实时的肿瘤位移预测解决方案，有望显著提升诊断精度和效率。

Abstract: Early diagnosis of breast cancer is crucial, enabling the establishment of appropriate treatment plans and markedly enhancing patient prognosis. While direct magnetic resonance imaging-guided biopsy demonstrates promising performance in detecting cancer lesions, its practical application is limited by prolonged procedure times and high costs. To overcome these issues, an indirect MRI-guided biopsy that allows the procedure to be performed outside of the MRI room has been proposed, but it still faces challenges in creating an accurate real-time deformable breast model. In our study, we tackled this issue by developing a graph neural network (GNN)-based model capable of accurately predicting deformed breast cancer sites in real time during biopsy procedures. An individual-specific finite element (FE) model was developed by incorporating magnetic resonance (MR) image-derived structural information of the breast and tumor to simulate deformation behaviors. A GNN model was then employed, designed to process surface displacement and distance-based graph data, enabling accurate prediction of overall tissue displacement, including the deformation of the tumor region. The model was validated using phantom and real patient datasets, achieving an accuracy within 0.2 millimeters (mm) for cancer node displacement (RMSE) and a dice similarity coefficient (DSC) of 0.977 for spatial overlap with actual cancerous regions. Additionally, the model enabled real-time inference and achieved a speed-up of over 4,000 times in computational cost compared to conventional FE simulations. The proposed deformation-aware GNN model offers a promising solution for real-time tumor displacement prediction in breast biopsy, with high accuracy and real-time capability. Its integration with clinical procedures could significantly enhance the precision and efficiency of breast cancer diagnosis.

</details>


### [228] [Transformer-Based Scalable Multi-Agent Reinforcement Learning for Networked Systems with Long-Range Interactions](https://arxiv.org/abs/2511.13103)
*Vidur Sinha,Muhammed Ustaomeroglu,Guannan Qu*

Main category: cs.LG

TL;DR: STACCA是一个基于Transformer的多智能体强化学习框架，通过集中式图Transformer批评器和共享图Transformer行动器来解决网络控制中的长程依赖和拓扑泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有MARL方法存在两个主要限制：1) 依赖局部交互衰减假设，难以捕捉长程依赖（如级联故障、疫情爆发）；2) 缺乏网络拓扑泛化能力，需要针对新图重新训练。

Method: STACCA采用集中式图Transformer批评器建模长程依赖并提供系统级反馈，共享图Transformer行动器学习可泛化策略，并集成反事实优势估计器改进信用分配。

Result: 在疫情控制和谣言传播网络控制任务中，STACCA表现出改进的性能、网络泛化能力和可扩展性。

Conclusion: 基于Transformer的MARL架构在大规模网络系统中具有实现可扩展和可泛化控制的潜力。

Abstract: Multi-agent reinforcement learning (MARL) has shown promise for large-scale network control, yet existing methods face two major limitations. First, they typically rely on assumptions leading to decay properties of local agent interactions, limiting their ability to capture long-range dependencies such as cascading power failures or epidemic outbreaks. Second, most approaches lack generalizability across network topologies, requiring retraining when applied to new graphs. We introduce STACCA (Shared Transformer Actor-Critic with Counterfactual Advantage), a unified transformer-based MARL framework that addresses both challenges. STACCA employs a centralized Graph Transformer Critic to model long-range dependencies and provide system-level feedback, while its shared Graph Transformer Actor learns a generalizable policy capable of adapting across diverse network structures. Further, to improve credit assignment during training, STACCA integrates a novel counterfactual advantage estimator that is compatible with state-value critic estimates. We evaluate STACCA on epidemic containment and rumor-spreading network control tasks, demonstrating improved performance, network generalization, and scalability. These results highlight the potential of transformer-based MARL architectures to achieve scalable and generalizable control in large-scale networked systems.

</details>


### [229] [Synthetic Forgetting without Access: A Few-shot Zero-glance Framework for Machine Unlearning](https://arxiv.org/abs/2511.13116)
*Qipeng Song,Nan Yang,Ziqi Xu,Yue Li,Wei Shao,Feng Xia*

Main category: cs.LG

TL;DR: GFOES是一个用于机器遗忘的框架，通过生成式反馈网络合成最优擦除样本，在仅使用5%保留数据的情况下实现有效的类别特定知识遗忘，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有机器遗忘方法需要完整访问原始训练数据集的不切实际假设，针对更现实的少样本零访问场景——只有少量保留数据可用且遗忘集完全不可访问。

Method: 提出GFOES框架，包含生成式反馈网络（GFN）和两阶段微调过程。GFN合成最优擦除样本（OES），这些样本在目标类别上产生高损失，使模型能够遗忘类别特定知识。两阶段微调包括第一阶段进行激进遗忘，第二阶段恢复模型效用。

Result: 在三个图像分类数据集上的实验表明，GFOES在logit和表示层面都实现了有效遗忘，同时仅使用5%原始数据就能保持强性能。

Conclusion: GFOES为数据受限条件下的隐私保护机器学习提供了一个实用且可扩展的解决方案。

Abstract: Machine unlearning aims to eliminate the influence of specific data from trained models to ensure privacy compliance. However, most existing methods assume full access to the original training dataset, which is often impractical. We address a more realistic yet challenging setting: few-shot zero-glance, where only a small subset of the retained data is available and the forget set is entirely inaccessible. We introduce GFOES, a novel framework comprising a Generative Feedback Network (GFN) and a two-phase fine-tuning procedure. GFN synthesises Optimal Erasure Samples (OES), which induce high loss on target classes, enabling the model to forget class-specific knowledge without access to the original forget data, while preserving performance on retained classes. The two-phase fine-tuning procedure enables aggressive forgetting in the first phase, followed by utility restoration in the second. Experiments on three image classification datasets demonstrate that GFOES achieves effective forgetting at both logit and representation levels, while maintaining strong performance using only 5% of the original data. Our framework offers a practical and scalable solution for privacy-preserving machine learning under data-constrained conditions.

</details>


### [230] [Departures: Distributional Transport for Single-Cell Perturbation Prediction with Neural Schrödinger Bridges](https://arxiv.org/abs/2511.13124)
*Changxi Chi,Yufei Huang,Jun Xia,Jiangbin Zheng,Yunfan Liu,Zelin Zang,Stan Z. Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于薛定谔桥近似的方法，直接对齐不同扰动条件下对照和扰动单细胞群体的分布，避免了双向建模的需求，在遗传和药物扰动数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 单细胞扰动结果预测对基因功能分析和药物候选选择至关重要，但由于测序的破坏性，单细胞数据本质上是未配对的，这成为该任务的主要瓶颈。现有的神经生成传输模型要么缺乏显式条件，要么依赖先验空间进行间接分布对齐，限制了精确的扰动建模。

Method: 通过近似薛定谔桥来定义随机动态映射，恢复熵正则化的最优传输，直接对齐不同扰动条件下对照和扰动单细胞群体的分布。利用基于Minibatch-OT的配对来避免双向推理，直接指导桥学习，实现可扩展的SB近似。同时建模离散基因激活状态和连续表达分布。

Result: 在公共遗传和药物扰动数据集上的实验表明，该模型有效捕捉了异质性单细胞响应，并实现了最先进的性能。

Conclusion: 提出的方法通过直接对齐分布和避免双向推理，实现了精确的单细胞扰动建模，能够有效捕捉单细胞异质性，在扰动预测任务中表现出色。

Abstract: Predicting single-cell perturbation outcomes directly advances gene function analysis and facilitates drug candidate selection, making it a key driver of both basic and translational biomedical research. However, a major bottleneck in this task is the unpaired nature of single-cell data, as the same cell cannot be observed both before and after perturbation due to the destructive nature of sequencing. Although some neural generative transport models attempt to tackle unpaired single-cell perturbation data, they either lack explicit conditioning or depend on prior spaces for indirect distribution alignment, limiting precise perturbation modeling. In this work, we approximate Schrödinger Bridge (SB), which defines stochastic dynamic mappings recovering the entropy-regularized optimal transport (OT), to directly align the distributions of control and perturbed single-cell populations across different perturbation conditions. Unlike prior SB approximations that rely on bidirectional modeling to infer optimal source-target sample coupling, we leverage Minibatch-OT based pairing to avoid such bidirectional inference and the associated ill-posedness of defining the reverse process. This pairing directly guides bridge learning, yielding a scalable approximation to the SB. We approximate two SB models, one modeling discrete gene activation states and the other continuous expression distributions. Joint training enables accurate perturbation modeling and captures single-cell heterogeneity. Experiments on public genetic and drug perturbation datasets show that our model effectively captures heterogeneous single-cell responses and achieves state-of-the-art performance.

</details>


### [231] [Soft Conflict-Resolution Decision Transformer for Offline Multi-Task Reinforcement Learning](https://arxiv.org/abs/2511.13133)
*Shudong Wang,Xinfei Wang,Chenhao Zhang,Shanchen Pang,Haiyuan Gui,Wenhao Ji,Xiaojian Liao*

Main category: cs.LG

TL;DR: SoCo-DT是一种基于参数重要性的软冲突解决方法，通过Fisher信息动态调整掩码值，保留重要参数同时抑制冲突参数，并引入基于四分位距的动态稀疏度调整策略，在Meta-World基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习中存在梯度冲突问题，现有基于掩码的方法使用粗粒度二元掩码会过度抑制关键冲突参数，阻碍任务间知识共享，且不同任务冲突程度不同但现有方法使用固定稀疏度策略，限制了模型的泛化能力和学习效率。

Method: 1) 利用Fisher信息动态调整掩码值，实现软冲突解决；2) 基于四分位距的动态稀疏度调整策略，构建任务特定的阈值方案；3) 引入非对称余弦退火调度来持续更新阈值，实现自适应稀疏度演化。

Result: 在Meta-World基准测试中，SoCo-DT在MT50上比最先进方法提升7.6%，在次优数据集上提升10.5%，有效缓解了梯度冲突并提高了多任务性能。

Conclusion: SoCo-DT通过软冲突解决和动态稀疏度调整，有效解决了多任务强化学习中的梯度冲突问题，显著提升了模型性能和泛化能力。

Abstract: Multi-task reinforcement learning (MTRL) seeks to learn a unified policy for diverse tasks, but often suffers from gradient conflicts across tasks. Existing masking-based methods attempt to mitigate such conflicts by assigning task-specific parameter masks. However, our empirical study shows that coarse-grained binary masks have the problem of over-suppressing key conflicting parameters, hindering knowledge sharing across tasks. Moreover, different tasks exhibit varying conflict levels, yet existing methods use a one-size-fits-all fixed sparsity strategy to keep training stability and performance, which proves inadequate. These limitations hinder the model's generalization and learning efficiency.
  To address these issues, we propose SoCo-DT, a Soft Conflict-resolution method based by parameter importance. By leveraging Fisher information, mask values are dynamically adjusted to retain important parameters while suppressing conflicting ones. In addition, we introduce a dynamic sparsity adjustment strategy based on the Interquartile Range (IQR), which constructs task-specific thresholding schemes using the distribution of conflict and harmony scores during training. To enable adaptive sparsity evolution throughout training, we further incorporate an asymmetric cosine annealing schedule to continuously update the threshold. Experimental results on the Meta-World benchmark show that SoCo-DT outperforms the state-of-the-art method by 7.6% on MT50 and by 10.5% on the suboptimal dataset, demonstrating its effectiveness in mitigating gradient conflicts and improving overall multi-task performance.

</details>


### [232] [Personalized Federated Learning with Bidirectional Communication Compression via One-Bit Random Sketching](https://arxiv.org/abs/2511.13144)
*Jiacheng Cheng,Xu Zhang,Guanghui Qiu,Yifang Zhang,Yinchuan Li,Kaiyuan Feng*

Main category: cs.LG

TL;DR: pFed1BS是一个个性化的联邦学习框架，通过一比特随机草图实现极端通信压缩，解决联邦学习中的通信开销和客户端数据异构问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临双向通信开销和客户端数据异构的关键挑战，需要降低通信成本同时适应数据异质性。

Method: 提出pFed1BS框架，客户端传输高度压缩的一比特草图，服务器聚合并广播全局一比特共识；引入基于符号的正则化器指导本地模型与全局共识对齐；使用快速哈达玛变换进行高效投影。

Result: 理论分析保证算法收敛到全局势函数的稳定邻域；数值模拟显示pFed1BS显著降低通信成本，同时达到与先进通信高效FL算法相当的竞争性能。

Conclusion: pFed1BS通过一比特压缩和个性化设计，有效解决了联邦学习的通信和异构数据挑战，实现了通信效率与性能的良好平衡。

Abstract: Federated Learning (FL) enables collaborative training across decentralized data, but faces key challenges of bidirectional communication overhead and client-side data heterogeneity. To address communication costs while embracing data heterogeneity, we propose pFed1BS, a novel personalized federated learning framework that achieves extreme communication compression through one-bit random sketching. In personalized FL, the goal shifts from training a single global model to creating tailored models for each client. In our framework, clients transmit highly compressed one-bit sketches, and the server aggregates and broadcasts a global one-bit consensus. To enable effective personalization, we introduce a sign-based regularizer that guides local models to align with the global consensus while preserving local data characteristics. To mitigate the computational burden of random sketching, we employ the Fast Hadamard Transform for efficient projection. Theoretical analysis guarantees that our algorithm converges to a stationary neighborhood of the global potential function. Numerical simulations demonstrate that pFed1BS substantially reduces communication costs while achieving competitive performance compared to advanced communication-efficient FL algorithms.

</details>


### [233] [OTARo: Once Tuning for All Precisions toward Robust On-Device LLMs](https://arxiv.org/abs/2511.13147)
*Shaoyuan Chen,Zhixuan Chen,Dawei Yang,Zhihang Yuan,Qiang Wu*

Main category: cs.LG

TL;DR: OTARo是一种新颖的量化方法，允许设备上的LLM灵活切换量化精度，通过一次微调实现多比特宽度的性能鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统量化方法存在结构限制，无法支持设备上不同精度切换的需求，特别是在理解任务和生成任务对精度要求不同的复杂场景下。

Method: 提出共享指数浮点数(SEFP)量化机制，通过简单的尾数截断实现不同比特宽度；采用利用-探索比特宽度路径搜索(BPS)和低精度异步累积(LAA)策略进行学习。

Result: 在LLaMA3.2-1B和LLaMA3-8B等流行LLM上的实验表明，OTARo在所有精度下都能实现一致强大且鲁棒的性能。

Conclusion: OTARo成功解决了传统量化方法在设备上精度切换的局限性，为实际应用提供了灵活高效的解决方案。

Abstract: Large Language Models (LLMs) fine-tuning techniques not only improve the adaptability to diverse downstream tasks, but also mitigate adverse effects of model quantization. Despite this, conventional quantization suffers from its structural limitation that hinders flexibility during the fine-tuning and deployment stages. Practical on-device tasks demand different quantization precisions (i.e. different bit-widths), e.g., understanding tasks tend to exhibit higher tolerance to reduced precision compared to generation tasks. Conventional quantization, typically relying on scaling factors that are incompatible across bit-widths, fails to support the on-device switching of precisions when confronted with complex real-world scenarios. To overcome the dilemma, we propose OTARo, a novel method that enables on-device LLMs to flexibly switch quantization precisions while maintaining performance robustness through once fine-tuning. OTARo introduces Shared Exponent Floating Point (SEFP), a distinct quantization mechanism, to produce different bit-widths through simple mantissa truncations of a single model. Moreover, to achieve bit-width robustness in downstream applications, OTARo performs a learning process toward losses induced by different bit-widths. The method involves two critical strategies: (1) Exploitation-Exploration Bit-Width Path Search (BPS), which iteratively updates the search path via a designed scoring mechanism; (2) Low-Precision Asynchronous Accumulation (LAA), which performs asynchronous gradient accumulations and delayed updates under low bit-widths. Experiments on popular LLMs, e.g., LLaMA3.2-1B, LLaMA3-8B, demonstrate that OTARo achieves consistently strong and robust performance for all precisions.

</details>


### [234] [Warm-starting active-set solvers using graph neural networks](https://arxiv.org/abs/2511.13174)
*Ella J. Schmidtobreick,Daniel Arnström,Paul Häusner,Jens Sjölund*

Main category: cs.LG

TL;DR: 使用图神经网络预测QP求解器中的有效集，通过图结构表示QP问题，减少求解器迭代次数，提高实时优化效率。


<details>
  <summary>Details</summary>
Motivation: 二次规划求解器在实时控制和优化中广泛应用，但计算成本限制了其在时间关键场景中的应用，需要更高效的求解方法。

Method: 提出基于图神经网络的学习优化方法，将QP问题表示为二分图，学习预测最优有效集，用于热启动DAQP求解器。

Result: GNN在不同问题规模下都能减少求解器迭代次数，性能与多层感知机基线相当，且能有效泛化到未见过的维度。

Conclusion: 结果表明结构感知学习在加速实时应用优化方面具有潜力，特别是在模型预测控制等场景中。

Abstract: Quadratic programming (QP) solvers are widely used in real-time control and optimization, but their computational cost often limits applicability in time-critical settings. We propose a learning-to-optimize approach using graph neural networks (GNNs) to predict active sets in the dual active-set solver DAQP. The method exploits the structural properties of QPs by representing them as bipartite graphs and learning to identify the optimal active set for efficiently warm-starting the solver. Across varying problem sizes, the GNN consistently reduces the number of solver iterations compared to cold-starting, while performance is comparable to a multilayer perceptron (MLP) baseline. Furthermore, a GNN trained on varying problem sizes generalizes effectively to unseen dimensions, demonstrating flexibility and scalability. These results highlight the potential of structure-aware learning to accelerate optimization in real-time applications such as model predictive control.

</details>


### [235] [Real-time distortion prediction in metallic additive manufacturing via a physics-informed neural operator approach](https://arxiv.org/abs/2511.13178)
*Mingxuan Tian,Haochen Mu,Donghong Ding,Mengjiao Li,Yuhan Ding,Jianping Zhao*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息的神经算子(PINO)方法，用于金属增材制造中实时预测15秒内的z和y方向变形场，通过解耦热机械响应并融入热传导方程约束，实现高精度、低误差累积的物理一致性预测。


<details>
  <summary>Details</summary>
Motivation: 数字孪生和智能制造系统的发展迫切需要实时变形场预测来控制金属增材制造缺陷，但传统数值模拟计算成本高，机器学习模型难以提取时空特征和解耦热机械场。

Method: 使用物理信息深度算子网络-循环神经网络(PIDeepONet-RNN)，通过分支网络处理温度历史，主干网络编码变形场，实现热机械响应解耦，并融入热传导方程作为软约束确保物理一致性。

Result: 模型在实验验证的有限元数据集上训练测试，z和y方向最大绝对误差分别低至0.9733 mm和0.2049 mm，熔池区域误差较高但关键沉积区域梯度范数低，具有高精度和低误差累积。

Conclusion: PINO代理模型在实时长时程物理场预测方面表现出巨大潜力，为控制增材制造缺陷提供了物理一致、可解释的预测基础。

Abstract: With the development of digital twins and smart manufacturing systems, there is an urgent need for real-time distortion field prediction to control defects in metal Additive Manufacturing (AM). However, numerical simulation methods suffer from high computational cost, long run-times that prevent real-time use, while conventional Machine learning (ML) models struggle to extract spatiotemporal features for long-horizon prediction and fail to decouple thermo-mechanical fields. This paper proposes a Physics-informed Neural Operator (PINO) to predict z and y-direction distortion for the future 15 s. Our method, Physics-informed Deep Operator Network-Recurrent Neural Network (PIDeepONet-RNN) employs trunk and branch network to process temperature history and encode distortion fields, respectively, enabling decoupling of thermo-mechanical responses. By incorporating the heat conduction equation as a soft constraint, the model ensures physical consistency and suppresses unphysical artifacts, thereby establishing a more physically consistent mapping between the thermal history and distortion. This is important because such a basis function, grounded in physical laws, provides a robust and interpretable foundation for predictions. The proposed models are trained and tested using datasets generated from experimentally validated Finite Element Method (FEM). Evaluation shows that the model achieves high accuracy, low error accumulation, time efficiency. The max absolute errors in the z and y-directions are as low as 0.9733 mm and 0.2049 mm, respectively. The error distribution shows high errors in the molten pool but low gradient norms in the deposited and key areas. The performance of PINO surrogate model highlights its potential for real-time long-horizon physics field prediction in controlling defects.

</details>


### [236] [Uncertainty-aware Physics-informed Neural Networks for Robust CARS-to-Raman Signal Reconstruction](https://arxiv.org/abs/2511.13185)
*Aishwarya Venkataramanan,Sai Karthikeya Vemuri,Adithya Ashok Chalain Valapil,Joachim Denzler*

Main category: cs.LG

TL;DR: 该论文评估了在CARS到拉曼信号重建中的各种不确定性量化技术，并证明将物理约束融入模型可以改善校准，为更可靠的CARS数据分析提供路径。


<details>
  <summary>Details</summary>
Motivation: CARS光谱学在医学和材料科学中广泛应用，但非共振背景会干扰真实拉曼信号。现有深度学习方法缺乏不确定性量化能力，这在高风险应用中至关重要。

Method: 评估和比较了CARS到拉曼信号重建中的各种不确定性量化技术，并将物理约束（Kramers-Kronig关系和光滑性约束）整合到模型中。

Result: 研究表明，将物理约束整合到不确定性量化模型中可以提高模型的校准性能。

Conclusion: 结合物理约束的不确定性量化技术为更可靠的CARS数据分析提供了有前景的途径，特别是在高风险的科学和生物医学应用中。

Abstract: Coherent anti-Stokes Raman scattering (CARS) spectroscopy is a powerful and rapid technique widely used in medicine, material science, and chemical analyses. However, its effectiveness is hindered by the presence of a non-resonant background that interferes with and distorts the true Raman signal. Deep learning methods have been employed to reconstruct the true Raman spectrum from measured CARS data using labeled datasets. A more recent development integrates the domain knowledge of Kramers-Kronig relationships and smoothness constraints in the form of physics-informed loss functions. However, these deterministic models lack the ability to quantify uncertainty, an essential feature for reliable deployment in high-stakes scientific and biomedical applications. In this work, we evaluate and compare various uncertainty quantification (UQ) techniques within the context of CARS-to-Raman signal reconstruction. Furthermore, we demonstrate that incorporating physics-informed constraints into these models improves their calibration, offering a promising path toward more trustworthy CARS data analysis.

</details>


### [237] [DiffFP: Learning Behaviors from Scratch via Diffusion-based Fictitious Play](https://arxiv.org/abs/2511.13186)
*Akash Karthikeyan,Yash Vardhan Pant*

Main category: cs.LG

TL;DR: 提出了DiffFP框架，使用扩散策略来估计对未见对手的最佳响应，在连续空间零和游戏中实现快速收敛和鲁棒性能


<details>
  <summary>Details</summary>
Motivation: 解决连续决策空间中自我强化学习收敛缓慢、难以达到纳什均衡的问题，提高对未见对手的适应性和泛化能力

Method: 基于虚构博弈框架，使用扩散策略通过生成建模来学习自适应和多样化的策略，近似最佳响应

Result: 在赛车和多粒子零和游戏等复杂环境中，比基线强化学习方法收敛快3倍，成功率平均高30倍，达到ε-纳什均衡

Conclusion: DiffFP框架在连续空间多智能体游戏中表现出优越的收敛性和鲁棒性，能够有效应对多样化的对手策略

Abstract: Self-play reinforcement learning has demonstrated significant success in learning complex strategic and interactive behaviors in competitive multi-agent games. However, achieving such behaviors in continuous decision spaces remains challenging. Ensuring adaptability and generalization in self-play settings is critical for achieving competitive performance in dynamic multi-agent environments. These challenges often cause methods to converge slowly or fail to converge at all to a Nash equilibrium, making agents vulnerable to strategic exploitation by unseen opponents. To address these challenges, we propose DiffFP, a fictitious play (FP) framework that estimates the best response to unseen opponents while learning a robust and multimodal behavioral policy. Specifically, we approximate the best response using a diffusion policy that leverages generative modeling to learn adaptive and diverse strategies. Through empirical evaluation, we demonstrate that the proposed FP framework converges towards $ε$-Nash equilibria in continuous- space zero-sum games. We validate our method on complex multi-agent environments, including racing and multi-particle zero-sum games. Simulation results show that the learned policies are robust against diverse opponents and outperform baseline reinforcement learning policies. Our approach achieves up to 3$\times$ faster convergence and 30$\times$ higher success rates on average against RL-based baselines, demonstrating its robustness to opponent strategies and stability across training iterations

</details>


### [238] [ParaDySe: A Parallel-Strategy Switching Framework for Dynamic Sequence Lengths in Transformer](https://arxiv.org/abs/2511.13198)
*Zhixin Ou,Peng Liang,Jianchen Han,Baihui Liu,Linbo Qiao*

Main category: cs.LG

TL;DR: ParaDySe是一个用于动态序列训练的自适应并行策略切换框架，能够根据输入序列长度实时选择最优并行策略，解决了传统静态策略在短序列上的通信并行化抵消问题和长序列上的内存溢出问题。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer大语言模型训练框架对动态长度序列采用预定义的静态并行策略，导致短序列存在通信并行化抵消问题，长序列则会出现内存溢出问题。

Method: ParaDySe首先实现统一张量布局规范的并行策略模块化函数库，然后构建序列感知的内存和时间成本模型，通过高效启发式算法为动态序列选择最优的逐层策略。

Result: 在序列长度高达624K的数据集上对代表性LLM进行实验，结果表明ParaDySe通过系统集成长序列优化与现有框架，解决了内存溢出和通信并行化抵消瓶颈。

Conclusion: ParaDySe通过自适应并行策略切换框架，实现了对动态序列训练的最优策略选择，有效解决了现有训练框架在长短序列上的性能瓶颈问题。

Abstract: Dynamic sequences with varying lengths have been widely used in the training of Transformer-based large language models (LLMs). However, current training frameworks adopt a pre-defined static parallel strategy for these sequences, causing neither communication-parallelization cancellation on short sequences nor out-of-memory on long sequences. To mitigate these issues, we propose ParaDySe, a novel adaptive Parallel strategy switching framework for Dynamic Sequences. ParaDySe enables on-the-fly optimal strategy adoption according to the immediate input sequence. It first implements the modular function libraries for parallel strategies with unified tensor layout specifications, and then builds sequence-aware memory and time cost models with hybrid methods. Guided by cost models, ParaDySe selects optimal layer-wise strategies for dynamic sequences via an efficient heuristic algorithm. By integrating these techniques together, ParaDySe achieves seamless hot-switching of optimal strategies through its well-designed function libraries. We compare ParaDySe with baselines on representative LLMs under datasets with sequence lengths up to 624K. Experimental results indicate that ParaDySe addresses OOM and CPC bottlenecks in LLM training by systematically integrating long-sequence optimizations with existing frameworks.

</details>


### [239] [TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs](https://arxiv.org/abs/2511.13223)
*Yuxiang Zhang,Zhengxu Yu,Weihang Pan,Zhongming Jin,Qiang Fu,Deng Cai,Binbin Lin,Jieping Ye*

Main category: cs.LG

TL;DR: TokenSqueeze是一种新的长到短推理方法，通过自适应选择推理深度和语言精炼，在保持准确性的同时显著减少推理LLM的token使用量。


<details>
  <summary>Details</summary>
Motivation: 现有推理LLM（如OpenAI-o1和DeepSeek-R1）生成长链推理轨迹导致token使用量增加，带来更高的推理延迟和内存消耗，需要在准确性和推理效率之间取得平衡。

Method: 1. 自适应选择推理深度：根据问题复杂度匹配推理深度，防止过度压缩导致性能下降；2. 分布对齐的语言精炼：优化语言表达而不改变底层推理路径，提高推理路径的清晰度和简洁性。

Result: 在MATH500基准测试中，使用该方法微调的DeepSeek-R1-Distill-Qwen-7B实现了50%的平均token减少，同时保持准确性。

Conclusion: TokenSqueeze仅使用模型自生成数据，无需依赖手动整理的短答案数据集，即可实现高效、高保真的推理，适用于多样化应用场景。

Abstract: Emerging reasoning LLMs such as OpenAI-o1 and DeepSeek-R1 have achieved strong performance on complex reasoning tasks by generating long chain-of-thought (CoT) traces. However, these long CoTs result in increased token usage, leading to higher inference latency and memory consumption. As a result, balancing accuracy and reasoning efficiency has become essential for deploying reasoning LLMs in practical applications. Existing long-to-short (Long2Short) methods aim to reduce inference length but often sacrifice accuracy, revealing a need for an approach that maintains performance while lowering token costs. To address this efficiency-accuracy tradeoff, we propose TokenSqueeze, a novel Long2Short method that condenses reasoning paths while preserving performance and relying exclusively on self-generated data. First, to prevent performance degradation caused by excessive compression of reasoning depth, we propose to select self-generated samples whose reasoning depth is adaptively matched to the complexity of the problem. To further optimize the linguistic expression without altering the underlying reasoning paths, we introduce a distribution-aligned linguistic refinement method that enhances the clarity and conciseness of the reasoning path while preserving its logical integrity. Comprehensive experimental results demonstrate the effectiveness of TokenSqueeze in reducing token usage while maintaining accuracy. Notably, DeepSeek-R1-Distill-Qwen-7B fine-tuned using our proposed method achieved a 50\% average token reduction while preserving accuracy on the MATH500 benchmark. TokenSqueeze exclusively utilizes the model's self-generated data, enabling efficient and high-fidelity reasoning without relying on manually curated short-answer datasets across diverse applications. Our code is available at https://github.com/zhangyx1122/TokenSqueeze.

</details>


### [240] [Laplace Learning in Wasserstein Space](https://arxiv.org/abs/2511.13229)
*Mary Chriselda Antony Oliver,Michael Roberts,Carola-Bibiane Schönlieb,Matthew Thorpe*

Main category: cs.LG

TL;DR: 本文研究了流形假设下的图半监督学习方法，将拉普拉斯学习扩展到Wasserstein空间，证明了离散图p-Dirichlet能量到连续对应物的变分收敛性，并表征了Wasserstein空间子流形上的Laplace-Beltrami算子。


<details>
  <summary>Details</summary>
Motivation: 基于流形假设，即高维数据通常存在于低维子空间中，研究图半监督学习方法在Wasserstein空间中的扩展，从有限维欧几里得空间推广到无限维设置。

Method: 通过证明离散图p-Dirichlet能量到其连续对应物的变分收敛性，将拉普拉斯学习扩展到Wasserstein空间，并表征该空间子流形上的Laplace-Beltrami算子。

Result: 在基准数据集上的数值实验验证了所提出的理论框架，证明了在高维设置下分类性能的一致性。

Conclusion: 成功将图半监督学习方法从有限维欧几里得空间扩展到无限维Wasserstein空间，为高维数据分析提供了新的理论框架和实用方法。

Abstract: The manifold hypothesis posits that high-dimensional data typically resides on low-dimensional sub spaces. In this paper, we assume manifold hypothesis to investigate graph-based semi-supervised learning
  methods. In particular, we examine Laplace Learning in the Wasserstein space, extending the classical
  notion of graph-based semi-supervised learning algorithms from finite-dimensional Euclidean spaces to
  an infinite-dimensional setting. To achieve this, we prove variational convergence of a discrete graph p- Dirichlet energy to its continuum counterpart. In addition, we characterize the Laplace-Beltrami operator
  on asubmanifold of the Wasserstein space. Finally, we validate the proposed theoretical framework through
  numerical experiments conducted on benchmark datasets, demonstrating the consistency of our classification performance in high-dimensional settings.

</details>


### [241] [MorphBoost: Self-Organizing Universal Gradient Boosting with Adaptive Tree Morphing](https://arxiv.org/abs/2511.13234)
*Boris Kriuk*

Main category: cs.LG

TL;DR: MorphBoost是一种新型梯度提升框架，通过自组织树结构动态调整分裂行为，在训练过程中自适应梯度分布变化，在多个数据集上超越XGBoost等竞争模型，实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统梯度提升算法使用静态树结构，分裂标准在训练过程中保持不变，无法适应不同学习阶段中梯度分布的变化和问题特定特征，限制了模型的适应能力。

Method: 引入自组织树结构，分裂函数基于累积梯度统计和迭代相关学习压力动态演化；包括变形分裂标准、自动问题指纹识别、向量化树预测、交互感知特征重要性和快速模式优化等创新。

Result: 在10个多样化数据集上基准测试显示，MorphBoost平均超越XGBoost 0.84%，获得4/10数据集胜利（40%胜率）和6/30前三名成绩（20%），同时保持最低方差（σ=0.0948）和最高最小准确率。

Conclusion: MorphBoost通过动态树结构变形实现了卓越的适应性和鲁棒性，在复杂问题上表现尤为突出，证明了自适应分裂机制在梯度提升中的有效性。

Abstract: Traditional gradient boosting algorithms employ static tree structures with fixed splitting criteria that remain unchanged throughout training, limiting their ability to adapt to evolving gradient distributions and problem-specific characteristics across different learning stages. This work introduces MorphBoost, a new gradient boosting framework featuring self-organizing tree structures that dynamically morph their splitting behavior during training. The algorithm implements adaptive split functions that evolve based on accumulated gradient statistics and iteration-dependent learning pressures, enabling automatic adjustment to problem complexity. Key innovations include: (1) morphing split criterion combining gradient-based scores with information-theoretic metrics weighted by training progress; (2) automatic problem fingerprinting for intelligent parameter configuration across binary/multiclass/regression tasks; (3) vectorized tree prediction achieving significant computational speedups; (4) interaction-aware feature importance detecting multiplicative relationships; and (5) fast-mode optimization balancing speed and accuracy. Comprehensive benchmarking across 10 diverse datasets against competitive models (XGBoost, LightGBM, GradientBoosting, HistGradientBoosting, ensemble methods) demonstrates that MorphBoost achieves state-of-the-art performance, outperforming XGBoost by 0.84% on average. MorphBoost secured the overall winner position with 4/10 dataset wins (40% win rate) and 6/30 top-3 finishes (20%), while maintaining the lowest variance (σ=0.0948) and highest minimum accuracy across all models, revealing superior consistency and robustness. Performance analysis across difficulty levels shows competitive results on easy datasets while achieving notable improvements on advanced problems due to higher adaptation levels.

</details>


### [242] [Counterfactual Explainable AI (XAI) Method for Deep Learning-Based Multivariate Time Series Classification](https://arxiv.org/abs/2511.13237)
*Alan G. Paredes Cetina,Kaouther Benguessoum,Raoni Lourenço,Sylvain Kubler*

Main category: cs.LG

TL;DR: CONFETTI是一个新颖的多目标反事实解释方法，用于多元时间序列分析，通过平衡预测置信度、邻近性和稀疏性来提供可操作的见解。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法在多元时间序列分类和回归中缺乏透明度，现有可解释AI方法无法完整传达决策空间，反事实解释方法通常只优先考虑准确性、邻近性或稀疏性中的单一目标。

Method: CONFETTI识别关键MTS子序列，定位反事实目标，并优化修改时间序列以平衡预测置信度、邻近性和稀疏性。

Result: 在UEA档案的七个MTS数据集上评估，CONFETTI在优化目标和其他六个文献指标上持续优于最先进的反事实解释方法，实现≥10%更高的置信度，并在≥40%的情况下改善稀疏性。

Conclusion: CONFETTI通过提供具有最小变化的反事实解释，提高了可解释性和决策支持能力，为多元时间序列分析提供了更实用的解释工具。

Abstract: Recent advances in deep learning have improved multivariate time series (MTS) classification and regression by capturing complex patterns, but their lack of transparency hinders decision-making. Explainable AI (XAI) methods offer partial insights, yet often fall short of conveying the full decision space. Counterfactual Explanations (CE) provide a promising alternative, but current approaches typically prioritize either accuracy, proximity or sparsity -- rarely all -- limiting their practical value. To address this, we propose CONFETTI, a novel multi-objective CE method for MTS. CONFETTI identifies key MTS subsequences, locates a counterfactual target, and optimally modifies the time series to balance prediction confidence, proximity and sparsity. This method provides actionable insights with minimal changes, improving interpretability, and decision support. CONFETTI is evaluated on seven MTS datasets from the UEA archive, demonstrating its effectiveness in various domains. CONFETTI consistently outperforms state-of-the-art CE methods in its optimization objectives, and in six other metrics from the literature, achieving $\geq10\%$ higher confidence while improving sparsity in $\geq40\%$.

</details>


### [243] [Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms](https://arxiv.org/abs/2511.13238)
*Patrick Parschan,Charlott Jakob*

Main category: cs.LG

TL;DR: 本文对无监督和半监督的基于文本的理想点估计算法进行了首次系统性综述，这些算法用于从文本数据推断潜在政治立场，在政治学、传播学等领域广泛应用。


<details>
  <summary>Details</summary>
Motivation: 过去20年CT-IPE算法的发展紧跟NLP趋势，从词频模型发展到大型语言模型，但领域碎片化严重，缺乏系统比较和应用指导。

Method: 通过系统性文献回顾识别了25种CT-IPE算法，进行手动内容分析，并提出了区分文本方差生成、捕获和聚合的概念框架，识别出四种方法家族。

Result: 建立了四种方法家族（词频、主题建模、词嵌入和LLM方法），评估了它们的假设、可解释性、可扩展性和局限性。

Conclusion: 提供了20年算法发展的结构化综合，为应用研究者提供实用指导，强调算法间估计差异本身具有信息价值，需要系统基准测试。

Abstract: This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking.

</details>


### [244] [Incoherent Beliefs & Inconsistent Actions in Large Language Models](https://arxiv.org/abs/2511.13240)
*Arka Pal,Teo Kitanovski,Arthur Liang,Akilesh Potti,Micah Goldblum*

Main category: cs.LG

TL;DR: LLMs在动态环境中存在信念更新不一致和行动与信念不匹配的问题，即使在静态任务中表现良好的模型也难以预测其在复杂现实环境中的行为。


<details>
  <summary>Details</summary>
Motivation: 现实任务与静态数据集存在差异，需要LLMs能够进行顺序交互、连贯更新信念并基于信念做出决策，但现有评估方法难以预测LLMs在动态环境中的表现。

Method: 通过实验检验LLMs的两个关键能力：信念的连贯更新能力，以及行动与信念的一致性。具体包括直接获取后验概率与正确更新先验概率的对比，以及在博彩市场等场景中行动与内部信念的一致性分析。

Result: 1. LLMs在信念更新上存在显著不一致性，直接获取的后验与正确更新的先验之间存在高达30%的平均差异；2. LLMs的行动常常与其持有的信念不一致，例如在博彩市场中不按内部信念方向下注；3. 模型在面对用户质疑时表现出中度自我不一致性；4. 这些问题即使在准确率高或校准良好的强模型中依然存在。

Conclusion: LLMs在复杂现实环境中的行为难以预测，即使静态任务表现良好的模型也存在信念更新不一致和行动信念不匹配的问题，这凸显了将LLMs应用于动态交互场景的挑战。

Abstract: Real-world tasks and environments exhibit differences from the static datasets that large language models (LLMs) are typically evaluated on. Such tasks can involve sequential interaction, requiring coherent updating of beliefs in light of new evidence, and making appropriate decisions based on those beliefs. Predicting how LLMs will perform in such dynamic environments is important, but can be tricky to determine from measurements in static settings. In this work, we examine two critical components of LLM performance: the ability of LLMs to coherently update their beliefs, and the extent to which the actions they take are consistent with those beliefs. First, we find that LLMs are largely inconsistent in how they update their beliefs; models can exhibit up to a 30% average difference between the directly elicited posterior, and the correct update of their prior. Second, we find that LLMs also often take actions which are inconsistent with the beliefs they hold. On a betting market, for example, LLMs often do not even bet in the same direction as their internally held beliefs over the underlying outcomes. We also find they have moderate self-inconsistency in how they respond to challenges by users to given answers. Finally, we show that the above properties hold even for strong models that obtain high accuracy or that are well-calibrated on the tasks at hand. Our results highlight the difficulties of predicting LLM behavior in complex real-world settings.

</details>


### [245] [Uncovering and Mitigating Transient Blindness in Multimodal Model Editing](https://arxiv.org/abs/2511.13243)
*Xiaoqi Han,Ru Li,Ran Yi,Hongye Tan,Zhuomin Liang,Víctor Gutiérrez-Basulto,Jeff Z. Pan*

Main category: cs.LG

TL;DR: 提出一个全面的多模态模型编辑评估框架，解决现有方法因依赖低相似度或随机输入而夸大成功的问题，通过三个关键维度和七种数据类型进行详细分析，并引入动态评估方法揭示瞬时盲现象。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本模型编辑的多模态模型编辑评估方法存在夸大成功的问题，因为它们依赖低相似度或随机输入，掩盖了过拟合现象，需要更全面的评估框架来准确衡量编辑效果。

Method: 提出包含随机图像局部性、无图像局部性和一致性图像局部性三个维度的评估框架，操作化为七种数据类型；引入De-VQA动态评估方法；提出局部性感知对抗损失来平衡跨模态表示。

Result: 经验结果表明，该方法持续优于现有基线，平均减少瞬时盲现象并提高局部性17%。令牌分析显示编辑对文本令牌的影响不成比例。

Conclusion: 提出的评估框架和局部性感知方法有效解决了多模态模型编辑中的过拟合问题，显著提高了编辑的准确性和鲁棒性。

Abstract: Multimodal Model Editing (MMED) aims to correct erroneous knowledge in multimodal models. Existing evaluation methods, adapted from textual model editing, overstate success by relying on low-similarity or random inputs, obscure overfitting. We propose a comprehensive locality evaluation framework, covering three key dimensions: random-image locality, no-image locality, and consistent-image locality, operationalized through seven distinct data types, enabling a detailed and structured analysis of multimodal edits. We introduce De-VQA, a dynamic evaluation for visual question answering, uncovering a phenomenon we term transient blindness, overfitting to edit-similar text while ignoring visuals. Token analysis shows edits disproportionately affect textual tokens. We propose locality-aware adversarial losses to balance cross-modal representations. Empirical results demonstrate that our approach consistently outperforms existing baselines, reducing transient blindness and improving locality by 17% on average.

</details>


### [246] [Seek and You Shall Fold](https://arxiv.org/abs/2511.13244)
*Nadav Bojan Sellam,Meital Bojan,Paul Schanda,Alex Bronstein*

Main category: cs.LG

TL;DR: 提出了一种非可微分指导蛋白质生成模型的框架，通过遗传算法将连续扩散生成器与任意黑盒目标耦合，实现了对NMR化学位移等实验数据的整合。


<details>
  <summary>Details</summary>
Motivation: 当前蛋白质生成模型难以整合非可微分的实验观测数据，特别是在核磁共振中，化学位移等丰富数据无法直接用于梯度条件采样。

Method: 使用连续扩散生成器与遗传算法结合，支持任意黑盒目标函数，包括成对距离约束、核奥弗豪泽效应约束和化学位移指导。

Result: 成功实现了化学位移指导的结构生成，揭示了当前预测器的关键弱点，展示了整合多样化实验信号的通用策略。

Conclusion: 该工作为超越可微分限制的自动化、数据条件蛋白质建模指明了方向。

Abstract: Accurate protein structures are essential for understanding biological function, yet incorporating experimental data into protein generative models remains a major challenge. Most predictors of experimental observables are non-differentiable, making them incompatible with gradient-based conditional sampling. This is especially limiting in nuclear magnetic resonance, where rich data such as chemical shifts are hard to directly integrate into generative modeling. We introduce a framework for non-differentiable guidance of protein generative models, coupling a continuous diffusion-based generator with any black-box objective via a tailored genetic algorithm. We demonstrate its effectiveness across three modalities: pairwise distance constraints, nuclear Overhauser effect restraints, and for the first time chemical shifts. These results establish chemical shift guided structure generation as feasible, expose key weaknesses in current predictors, and showcase a general strategy for incorporating diverse experimental signals. Our work points toward automated, data-conditioned protein modeling beyond the limits of differentiability.

</details>


### [247] [Edge-aware baselines for ogbn-proteins in PyTorch Geometric: species-wise normalization, post-hoc calibration, and cost-accuracy trade-offs](https://arxiv.org/abs/2511.13250)
*Aleksandar Stanković,Dejan Lisica*

Main category: cs.LG

TL;DR: 本文为ogbn-proteins数据集提供了可复现的边缘感知基线方法，研究了边缘特征聚合和消息传递中的关键系统选择，提出了基于GraphSAGE的最强基线，并比较了不同归一化方法和后处理技术。


<details>
  <summary>Details</summary>
Motivation: 为ogbn-proteins数据集建立标准化的可复现基线，系统研究边缘特征如何聚合为节点输入以及如何在消息传递中使用边缘信息这两个实践中主导的系统选择。

Method: 使用PyTorch Geometric实现GraphSAGE模型，比较sum、mean、max三种边缘特征聚合方式，评估LayerNorm、BatchNorm和物种感知条件LayerNorm三种归一化方法，并采用后处理的温度缩放和标签相关平滑技术。

Result: sum聚合方式始终优于mean和max；BatchNorm获得最佳AUC，而条件LayerNorm在AUC前沿匹配的同时具有更好的阈值F1；后处理技术显著改善了micro-F1和校准误差，且AUC变化可忽略。

Conclusion: 提出了ogbn-proteins的标准化基线方法，sum边缘特征聚合和BatchNorm是最佳组合，后处理技术能显著提升决策质量，为后续研究提供了可复现的基准。

Abstract: We present reproducible, edge-aware baselines for ogbn-proteins in PyTorch Geometric (PyG). We study two system choices that dominate practice: (i) how 8-dimensional edge evidence is aggregated into node inputs, and (ii) how edges are used inside message passing. Our strongest baseline is GraphSAGE with sum-based edge-to-node features. We compare LayerNorm (LN), BatchNorm (BN), and a species-aware Conditional LayerNorm (CLN), and report compute cost (time, VRAM, parameters) together with accuracy (ROC-AUC) and decision quality. In our primary experimental setup (hidden size 512, 3 layers, 3 seeds), sum consistently beats mean and max; BN attains the best AUC, while CLN matches the AUC frontier with better thresholded F1. Finally, post-hoc per-label temperature scaling plus per-label thresholds substantially improves micro-F1 and expected calibration error (ECE) with negligible AUC change, and light label-correlation smoothing yields small additional gains. We release standardized artifacts and scripts used for all of the runs presented in the paper.

</details>


### [248] [KForge: Program Synthesis for Diverse AI Hardware Accelerators](https://arxiv.org/abs/2511.13274)
*Taras Sereda,Tom St. John,Burak Bartan,Natalie Serrino,Sachin Katti,Zain Asgar*

Main category: cs.LG

TL;DR: KForge是一个平台无关的GPU内核优化框架，使用两个协作的LLM代理：生成代理通过编译和正确性反馈迭代优化程序，性能分析代理解释性能数据指导优化。


<details>
  <summary>Details</summary>
Motivation: GPU内核对ML性能至关重要，但难以跨不同加速器进行优化。需要一种平台无关的方法来简化跨硬件平台的优化过程。

Method: 采用基于LLM的双代理架构：生成代理负责程序生成和迭代优化，性能分析代理解释性能分析数据并提供优化建议。仅需单次示例即可针对新平台。

Result: 展示了跨平台知识转移的有效性，从一个架构的参考实现显著提高了对不同硬件目标的生成质量。在NVIDIA CUDA和Apple Metal等不同并行计算平台上实现了有效的程序合成。

Conclusion: KForge提供了一个平台无关的框架，通过协作的LLM代理实现了跨不同加速器的有效GPU内核优化，仅需最小示例即可适应新硬件平台。

Abstract: GPU kernels are critical for ML performance but difficult to optimize across diverse accelerators. We present KForge, a platform-agnostic framework built on two collaborative LLM-based agents: a generation agent that produces and iteratively refines programs through compilation and correctness feedback, and a performance analysis agent that interprets profiling data to guide optimization. This agent-based architecture requires only a single-shot example to target new platforms.
  We make three key contributions: (1) introducing an iterative refinement system where the generation agent and performance analysis agent collaborate through functional and optimization passes, interpreting diverse profiling data (from programmatic APIs to GUI-based tools) to generate actionable recommendations that guide program synthesis for arbitrary accelerators; (2) demonstrating that the generation agent effectively leverages cross-platform knowledge transfer, where a reference implementation from one architecture substantially improves generation quality for different hardware targets; and (3) validating the platform-agnostic nature of our approach by demonstrating effective program synthesis across fundamentally different parallel computing platforms: NVIDIA CUDA and Apple Metal.

</details>


### [249] [Explainable RL Policies by Distilling to Locally-Specialized Linear Policies with Voronoi State Partitioning](https://arxiv.org/abs/2511.13322)
*Senne Deproost,Dennis Steckelmacher,Ann Nowé*

Main category: cs.LG

TL;DR: 提出了一种新的模型无关方法，使用Voronoi分区将状态空间划分为区域，在每个区域内使用简化的线性模型来近似原始深度强化学习控制器的行为。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习控制器缺乏透明度，难以满足监管要求和建立信任。需要将学习到的行为转移到人类可读的模型中，但单一简化模型在动态情况下表现不佳，需要在灵活性和复杂性之间找到平衡。

Method: 使用Voronoi分区方法将状态空间划分为多个区域，在每个区域内使用线性模型来近似原始黑盒策略。这种方法创建了专门针对局部区域的简化模型。

Result: 在网格世界环境和经典控制任务上的评估表明，这种基于局部专业化线性模型的蒸馏方法产生了可解释的策略，并且性能匹配甚至略微优于原始黑盒策略。

Conclusion: 所提出的方法成功地创建了可解释的控制器，同时保持了与原始深度强化学习控制器相当的性能，解决了黑盒模型缺乏透明度的问题。

Abstract: Deep Reinforcement Learning is one of the state-of-the-art methods for producing near-optimal system controllers. However, deep RL algorithms train a deep neural network, that lacks transparency, which poses challenges when the controller has to meet regulations, or foster trust. To alleviate this, one could transfer the learned behaviour into a model that is human-readable by design using knowledge distilla- tion. Often this is done with a single model which mimics the original model on average but could struggle in more dynamic situations. A key challenge is that this simpler model should have the right balance be- tween flexibility and complexity or right balance between balance bias and accuracy. We propose a new model-agnostic method to divide the state space into regions where a simplified, human-understandable model can operate in. In this paper, we use Voronoi partitioning to find regions where linear models can achieve similar performance to the original con- troller. We evaluate our approach on a gridworld environment and a classic control task. We observe that our proposed distillation to locally- specialized linear models produces policies that are explainable and show that the distillation matches or even slightly outperforms the black-box policy they are distilled from.

</details>


### [250] [Tab-PET: Graph-Based Positional Encodings for Tabular Transformers](https://arxiv.org/abs/2511.13338)
*Yunze Leng,Rohan Ghosh,Mehul Motani*

Main category: cs.LG

TL;DR: 本文发现位置编码(PEs)可以改善表格数据transformer模型的泛化性能，通过降低特征有效秩来简化任务维度，提出了基于图的位置编码框架Tab-PET。


<details>
  <summary>Details</summary>
Motivation: 表格数据缺乏结构性线索，现有transformer模型通常不使用位置编码，但理论上和实证上都表明结构线索特别是位置编码可以提升表格transformer的泛化性能。

Method: 提出Tab-PET框架，通过图结构估计位置编码，探索了基于关联性和因果性的两种图构建范式，并将图导出的位置编码融入嵌入表示中。

Result: 在50个分类和回归数据集上的实验表明，图导出的位置编码显著提升了3T模型性能，其中基于关联性的图方法比因果性方法获得更稳定和显著的改进。

Conclusion: 位置编码在表格transformer中具有意外的重要作用，可以通过降低问题维度来改善泛化能力，为表格数据学习提供了新的结构线索利用方式。

Abstract: Supervised learning with tabular data presents unique challenges, including low data sizes, the absence of structural cues, and heterogeneous features spanning both categorical and continuous domains. Unlike vision and language tasks, where models can exploit inductive biases in the data, tabular data lacks inherent positional structure, hindering the effectiveness of self-attention mechanisms. While recent transformer-based models like TabTransformer, SAINT, and FT-Transformer (which we refer to as 3T) have shown promise on tabular data, they typically operate without leveraging structural cues such as positional encodings (PEs), as no prior structural information is usually available. In this work, we find both theoretically and empirically that structural cues, specifically PEs can be a useful tool to improve generalization performance for tabular transformers. We find that PEs impart the ability to reduce the effective rank (a form of intrinsic dimensionality) of the features, effectively simplifying the task by reducing the dimensionality of the problem, yielding improved generalization. To that end, we propose Tab-PET (PEs for Tabular Transformers), a graph-based framework for estimating and inculcating PEs into embeddings. Inspired by approaches that derive PEs from graph topology, we explore two paradigms for graph estimation: association-based and causality-based. We empirically demonstrate that graph-derived PEs significantly improve performance across 50 classification and regression datasets for 3T. Notably, association-based graphs consistently yield more stable and pronounced gains compared to causality-driven ones. Our work highlights an unexpected role of PEs in tabular transformers, revealing how they can be harnessed to improve generalization.

</details>


### [251] [Statistically Accurate and Robust Generative Prediction of Rock Discontinuities with A Tabular Foundation Model](https://arxiv.org/abs/2511.13339)
*Han Meng,Gang Mei,Hong Tian,Nengxiong Xu,Jianbing Peng*

Main category: cs.LG

TL;DR: 提出了一种基于表格基础模型的简单而稳健的方法，用于统计准确的岩体不连续面生成预测，解决了现有方法在数据稀疏条件下无法捕捉复杂分布模式或缺乏稳健性的问题。


<details>
  <summary>Details</summary>
Motivation: 岩体不连续面对岩体力学行为和稳定性至关重要，但其内部分布通常无法直接观测，只能通过表面暴露的不连续面进行推断。现有生成预测方法在数据稀疏条件下难以捕捉复杂的分布模式或缺乏稳健性。

Method: 利用专门为小数据设计的表格基础模型，充分发挥其强大的样本学习能力，在有限的测量不连续面数据中有效捕捉潜在的复杂分布模式。

Result: 在十个具有不同尺度和分布模式的数据集上的对比实验表明，该方法在准确性和稳健性方面优于传统统计模型和深度生成方法。

Conclusion: 这项工作推进了岩体结构的定量表征，支持更安全、更可靠的数据驱动岩土工程设计。

Abstract: Rock discontinuities critically govern the mechanical behavior and stability of rock masses. Their internal distributions remain largely unobservable and are typically inferred from surface-exposed discontinuities using generative prediction approaches. However, surface-exposed observations are inherently sparse, and existing generative prediction approaches either fail to capture the underlying complex distribution patterns or lack robustness under data-sparse conditions. Here, we proposed a simple yet robust approach for statistically accurate generative prediction of rock discontinuities by utilizing a tabular foundation model. By leveraging the powerful sample learning capability of the foundation model specifically designed for small data, our approach can effectively capture the underlying complex distribution patterns within limited measured discontinuities. Comparative experiments on ten datasets with diverse scales and distribution patterns of discontinuities demonstrate superior accuracy and robustness over conventional statistical models and deep generative approaches. This work advances quantitative characterization of rock mass structures, supporting safer and more reliable data-driven geotechnical design.

</details>


### [252] [Dual-LoRA and Quality-Enhanced Pseudo Replay for Multimodal Continual Food Learning](https://arxiv.org/abs/2511.13351)
*Xinlan Wu,Bin Zhu,Feng Han,Pengkun Jiao,Jingjing Chen*

Main category: cs.LG

TL;DR: 提出了一种用于多模态食品学习的持续学习框架，通过双LoRA架构和质量增强伪回放来解决现有大型多模态模型在食品分析中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大型多模态模型在食品分析中学习新任务时会出现灾难性遗忘，需要从头开始进行昂贵的重新训练。

Method: 采用双LoRA架构：专用LoRA学习任务特定知识并与先前任务子空间正交约束，合作LoRA通过伪回放整合跨任务共享知识。质量增强伪回放策略利用自一致性和语义相似性减少生成样本中的幻觉。

Result: 在综合Uni-Food数据集上的实验显示，该方法在减轻遗忘方面表现出优越性能。

Conclusion: 这是首个针对复杂食品任务的有效持续学习方法。

Abstract: Food analysis has become increasingly critical for health-related tasks such as personalized nutrition and chronic disease prevention. However, existing large multimodal models (LMMs) in food analysis suffer from catastrophic forgetting when learning new tasks, requiring costly retraining from scratch. To address this, we propose a novel continual learning framework for multimodal food learning, integrating a Dual-LoRA architecture with Quality-Enhanced Pseudo Replay. We introduce two complementary low-rank adapters for each task: a specialized LoRA that learns task-specific knowledge with orthogonal constraints to previous tasks' subspaces, and a cooperative LoRA that consolidates shared knowledge across tasks via pseudo replay. To improve the reliability of replay data, our Quality-Enhanced Pseudo Replay strategy leverages self-consistency and semantic similarity to reduce hallucinations in generated samples. Experiments on the comprehensive Uni-Food dataset show superior performance in mitigating forgetting, representing the first effective continual learning approach for complex food tasks.

</details>


### [253] [A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs](https://arxiv.org/abs/2511.13373)
*Prakrit Timilsina,Anuj Nepal,Rajan Kadel,Robin Doss*

Main category: cs.LG

TL;DR: 本文系统评估了六种参数空间合并技术应用于医疗LLMs，发现对于架构兼容的模型，简单的平均方法在计算效率和性能上优于复杂方法，为分布式医疗AI部署提供了实用解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在分布式医疗中面临的挑战：跨机构整合专业知识同时保护隐私、降低计算开销、防止模型更新时的灾难性遗忘。

Method: 提出分层方法，结合选择性最优传输对齐注意力层和余弦相似度加权插值，评估六种参数合并技术（Task Arithmetic、线性平均、DARE-TIES、DELLA、Breadcrumbs和分层方法）。

Result: 架构兼容模型从简单平均方法中显著受益，Task Arithmetic在MedQA上达到45.80%准确率，优于基于剪枝的复杂方法。

Conclusion: 对于架构兼容模型，简单平均提供了稳健且计算高效的知识整合基线，为可扩展医疗AI系统提供了实用路径。

Abstract: Large Language Models (LLMs) face significant challenges in distributed healthcare, including consolidating specialized domain knowledge across institutions while maintaining privacy, reducing computational overhead, and preventing catastrophic forgetting during model updates.This paper presents a systematic evaluation of six parameter-space merging techniques applied to two architecturally compatible medical LLMs derived from the Mistral-7B base model. We introduce a novel hierarchical method that combines selective Optimal Transport (OT) alignment for attention layers with cosine similarity-weighted interpolation, designed to address permutation variance while minimizing computational overhead for edge deployment scenarios. Our study evaluates Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our Hierarchical approach across five medical benchmarks. Results demonstrate that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA, outperforming complex pruning-based approaches. These findings offer critical insights for the deployment of distributed medical AI in resource-constrained IoT environments, where computational efficiency and model compatibility are paramount. Our work establishes that for architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems.

</details>


### [254] [Finding Kissing Numbers with Game-theoretic Reinforcement Learning](https://arxiv.org/abs/2511.13391)
*Chengdong Ma,Théo Tao Zhaowei,Pengyu Li,Minghao Liu,Haojun Chen,Zihao Mao,Yuan Cheng,Yuan Qi,Yaodong Yang*

Main category: cs.LG

TL;DR: 该论文提出PackingStar系统，使用博弈论强化学习解决高维接吻数问题，在25-31维中超越所有已知记录，并发现了数千个新结构。


<details>
  <summary>Details</summary>
Motivation: 接吻数问题是自牛顿时代以来的基础几何难题，高维空间的不规则性和组合复杂性限制了现有方法的扩展性。

Method: 将问题建模为双人矩阵完成博弈，一个玩家填充矩阵条目，另一个修正次优条目，共同最大化矩阵大小（对应接吻数）。

Result: 在25-31维中超越所有人类已知记录，25维配置与Leech格对应，13维首次突破1971年有理结构限制，在14维等维度发现6000多个新结构。

Conclusion: AI能够探索超出人类直觉的高维空间，为接吻数问题和更广泛的几何问题开辟了新途径。

Abstract: Since Isaac Newton first studied the Kissing Number Problem in 1694, determining the maximal number of non-overlapping spheres around a central sphere has remained a fundamental challenge. This problem represents the local analogue of Hilbert's 18th problem on sphere packing, bridging geometry, number theory, and information theory. Although significant progress has been made through lattices and codes, the irregularities of high-dimensional geometry and exponentially growing combinatorial complexity beyond 8 dimensions, which exceeds the complexity of Go game, limit the scalability of existing methods. Here we model this problem as a two-player matrix completion game and train the game-theoretic reinforcement learning system, PackingStar, to efficiently explore high-dimensional spaces. The matrix entries represent pairwise cosines of sphere center vectors; one player fills entries while another corrects suboptimal ones, jointly maximizing the matrix size, corresponding to the kissing number. This cooperative dynamics substantially improves sample quality, making the extremely large spaces tractable. PackingStar reproduces previous configurations and surpasses all human-known records from dimensions 25 to 31, with the configuration in 25 dimensions geometrically corresponding to the Leech lattice and suggesting possible optimality. It achieves the first breakthrough beyond rational structures from 1971 in 13 dimensions and discovers over 6000 new structures in 14 and other dimensions. These results demonstrate AI's power to explore high-dimensional spaces beyond human intuition and open new pathways for the Kissing Number Problem and broader geometry problems.

</details>


### [255] [Fast and Robust Simulation-Based Inference With Optimization Monte Carlo](https://arxiv.org/abs/2511.13394)
*Vasilis Gkolemis,Christos Diou,Michael Gutmann*

Main category: cs.LG

TL;DR: 提出了一种基于可微分模拟器的贝叶斯参数推断新方法，通过将随机模拟重构为确定性优化问题，结合梯度方法高效探索高密度后验区域，显著减少运行时间。


<details>
  <summary>Details</summary>
Motivation: 复杂随机模拟器的贝叶斯参数推断面临似然函数难处理、高维参数空间计算成本高、部分无信息输出等挑战，现有方法需要大量模拟且效率低下。

Method: 基于优化蒙特卡洛框架，将随机模拟重构为确定性优化问题，应用梯度方法高效导航高密度后验区域，避免在低概率区域浪费模拟，并使用JAX实现关键组件向量化。

Result: 在高维参数空间、无信息输出、多观测和多峰后验等广泛实验中，该方法在保持与最先进方法相当甚至更优精度的同时，显著减少了运行时间。

Conclusion: 所提出的方法为可微分模拟器提供了一种高效准确的贝叶斯参数推断解决方案，在保持推断质量的同时大幅提升了计算效率。

Abstract: Bayesian parameter inference for complex stochastic simulators is challenging due to intractable likelihood functions. Existing simulation-based inference methods often require large number of simulations and become costly to use in high-dimensional parameter spaces or in problems with partially uninformative outputs. We propose a new method for differentiable simulators that delivers accurate posterior inference with substantially reduced runtimes. Building on the Optimization Monte Carlo framework, our approach reformulates stochastic simulation as deterministic optimization problems. Gradient-based methods are then applied to efficiently navigate toward high-density posterior regions and avoid wasteful simulations in low-probability areas. A JAX-based implementation further enhances the performance through vectorization of key method components. Extensive experiments, including high-dimensional parameter spaces, uninformative outputs, multiple observations and multimodal posteriors show that our method consistently matches, and often exceeds, the accuracy of state-of-the-art approaches, while reducing the runtime by a substantial margin.

</details>


### [256] [PAST: A Primary-Auxiliary Spatio-Temporal Network for Traffic Time Series Imputation](https://arxiv.org/abs/2511.13414)
*Hanwen Hu,Zimo Wen,Shiyou Qian,Jian Co*

Main category: cs.LG

TL;DR: 提出了PAST网络，通过主-辅助时空模式处理交通时间序列插补问题，在27种缺失数据条件下优于7个最先进基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以适应随机缺失位置，无法学习长期和大规模依赖关系，而各种类型的缺失数据（随机、纤维、块缺失）使插补任务具有挑战性。

Method: 使用图集成模块（GIM）捕获主模式，通过动态图和区间感知dropout；交叉门控模块（CGM）提取辅助模式，通过双向门控处理外部特征；两个模块通过共享隐藏向量交互，在集成自监督框架下训练。

Result: 在三个数据集上的27种缺失数据条件下，PAST的插补精度在RMSE上提升达26.2%，在MAE上提升达31.6%。

Conclusion: PAST通过主-辅助模式分类和模块化设计，有效处理各种缺失数据条件，显著提升了交通时间序列插补性能。

Abstract: Traffic time series imputation is crucial for the safety and reliability of intelligent transportation systems, while diverse types of missing data, including random, fiber, and block missing make the imputation task challenging. Existing models often focus on disentangling and separately modeling spatial and temporal patterns based on relationships between data points. However, these approaches struggle to adapt to the random missing positions, and fail to learn long-term and large-scale dependencies, which are essential in extensive missing conditions. In this paper, patterns are categorized into two types to handle various missing data conditions: primary patterns, which originate from internal relationships between data points, and auxiliary patterns, influenced by external factors like timestamps and node attributes. Accordingly, we propose the Primary-Auxiliary Spatio-Temporal network (PAST). It comprises a graph-integrated module (GIM) and a cross-gated module (CGM). GIM captures primary patterns via dynamic graphs with interval-aware dropout and multi-order convolutions, and CGM extracts auxiliary patterns through bidirectional gating on embedded external features. The two modules interact via shared hidden vectors and are trained under an ensemble self-supervised framework. Experiments on three datasets under 27 missing data conditions demonstrate that the imputation accuracy of PAST outperforms seven state-of-the-art baselines by up to 26.2% in RMSE and 31.6% in MAE.

</details>


### [257] [MMWSTM-ADRAN+: A Novel Hybrid Deep Learning Architecture for Enhanced Climate Time Series Forecasting and Extreme Event Prediction](https://arxiv.org/abs/2511.13419)
*Shaheen Mohammed Saleh Ahmed,Hakan Hakan Guneyli*

Main category: cs.LG

TL;DR: MMWSTM-ADRAN+是一个双流深度学习架构，结合了天气状态转换模型和异常驱动注意力机制，用于预测极端气温事件。


<details>
  <summary>Details</summary>
Motivation: 准确预测极端气温事件对气候风险管理至关重要，但现有方法在短程预测中仍面临挑战。

Method: 使用双流架构：MMWSTM流用BiLSTM和可学习马尔可夫状态转移矩阵捕捉天气状态变化；ADRAN流用BiGRU、多头自注意力和异常放大层增强对低概率信号的敏感性。通过注意力融合门结合两流输出，并使用极端天气损失函数和数据增强技术优化模型。

Result: 模型能够有效预测日最高气温及其极端事件，通过数据增强使训练数据量翻倍。

Conclusion: 该双流深度学习架构为极端气温事件的准确预测提供了有效解决方案。

Abstract: Accurate short-range prediction of extreme air temperature events remains a fundamental challenge in operational climate-risk management. We present Multi-Modal Weather State Transition Model with Anomaly-Driven Recurrent Attention Network Plus (MMWSTM-ADRAN+), a dual-stream deep learning architecture that couples a regime-aware dynamics model with an anomaly-focused attention mechanism to forecast daily maximum temperature and its extremes. The first stream, MMWSTM, combines bidirectional Long Short-Term Memory (BiLSTM) units with a learnable Markov state transition matrix to capture synoptic-scale weather regime changes. The second stream, ADRAN, integrates bidirectional Gated Recurrent Units (BiGRUs), multi-head self-attention, and a novel anomaly amplification layer to enhance sensitivity to low-probability signals. A lightweight attentive fusion gate adaptively determines the contribution of each stream to the final prediction. Model optimization employs a custom ExtremeWeatherLoss function that up-weights errors on the upper 5% and lower 5% of the temperature distribution, and a time-series data augmentation suite (jittering, scaling, time/magnitude warping) that effectively quadruples the training data

</details>


### [258] [Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch Scaling in Linear Regression](https://arxiv.org/abs/2511.13421)
*Tingkai Yan,Haodong Wen,Binghui Li,Kairong Luo,Wenguang Chen,Kaifeng Lyu*

Main category: cs.LG

TL;DR: 本文分析了在有限数据和多轮训练下，线性回归中数据缩放定律的变化。通过定义"有效重用率"E(K,N)，量化了K轮训练与单轮训练达到相同性能所需数据量的关系。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的数据缩放定律在单轮训练下已被广泛研究，但在有限数据和多轮训练情况下的缩放定律形式仍不清楚。本文旨在填补这一空白。

Method: 使用线性回归模型，在强凸性或Zipf分布数据下，分析SGD算法中数据有效重用率E(K,N)的缩放行为。

Result: 发现当K较小时，E(K,N)≈K，每轮训练带来线性增益；当K增大时，E(K,N)会趋于饱和，饱和值随N增长（强凸情况下为Θ(logN)）。

Conclusion: 数据有效重用率的最大K值取决于数据规模和分布，这指出了在数据重用研究中需要显式建模这两个因素的重要性。

Abstract: While data scaling laws of large language models (LLMs) have been widely examined in the one-pass regime with massive corpora, their form under limited data and repeated epochs remains largely unexplored. This paper presents a theoretical analysis of how a common workaround, training for multiple epochs on the same dataset, reshapes the data scaling laws in linear regression. Concretely, we ask: to match the performance of training on a dataset of size $N$ for $K$ epochs, how much larger must a dataset be if the model is trained for only one pass? We quantify this using the \textit{effective reuse rate} of the data, $E(K, N)$, which we define as the multiplicative factor by which the dataset must grow under one-pass training to achieve the same test loss as $K$-epoch training. Our analysis precisely characterizes the scaling behavior of $E(K, N)$ for SGD in linear regression under either strong convexity or Zipf-distributed data: (1) When $K$ is small, we prove that $E(K, N) \approx K$, indicating that every new epoch yields a linear gain; (2) As $K$ increases, $E(K, N)$ plateaus at a problem-dependent value that grows with $N$ ($Θ(\log N)$ for the strongly-convex case), implying that larger datasets can be repeated more times before the marginal benefit vanishes. These theoretical findings point out a neglected factor in a recent empirical study (Muennighoff et al. (2023)), which claimed that training LLMs for up to $4$ epochs results in negligible loss differences compared to using fresh data at each step, \textit{i.e.}, $E(K, N) \approx K$ for $K \le 4$ in our notation. Supported by further empirical validation with LLMs, our results reveal that the maximum $K$ value for which $E(K, N) \approx K$ in fact depends on the data size and distribution, and underscore the need to explicitly model both factors in future studies of scaling laws with data reuse.

</details>


### [259] [Discovering Operational Patterns Using Image-Based Convolutional Clustering and Composite Evaluation: A Case Study in Foundry Melting Processes](https://arxiv.org/abs/2511.13444)
*Zhipeng Ma,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.LG

TL;DR: 提出基于图像卷积聚类的无监督方法，用于发现工业单变量时间序列中的操作模式，通过灰度矩阵转换、两阶段聚类策略和复合评估指标，在熔炉操作数据中识别出7种可解释模式。


<details>
  <summary>Details</summary>
Motivation: 工业过程监控中传感器时间序列数据缺乏标签、高变异性、操作噪声等问题，传统方法难以提取有效模式，现有聚类技术无法处理动态、非结构化工业序列。

Method: 将原始时间序列通过重叠滑动窗口转换为灰度矩阵表示，使用深度卷积自编码器提取特征；集成软硬聚类输出并通过两阶段策略优化选择；开发复合评分S_eva结合多个聚类指标进行客观评估。

Result: 在3900多个熔炉熔化操作中识别出7种可解释的操作模式，显示出能耗、热动力学和生产持续时间的显著差异；相比经典和深度聚类基线，该方法获得更优的整体性能、更强鲁棒性和领域对齐的可解释性。

Conclusion: 该框架解决了无监督时间序列分析中的关键挑战，如序列不规则性、模式重叠和度量不一致性，为工业系统的数据驱动诊断和能源优化提供了通用解决方案。

Abstract: Industrial process monitoring increasingly relies on sensor-generated time-series data, yet the lack of labels, high variability, and operational noise make it difficult to extract meaningful patterns using conventional methods. Existing clustering techniques either rely on fixed distance metrics or deep models designed for static data, limiting their ability to handle dynamic, unstructured industrial sequences. Addressing this gap, this paper proposes a novel framework for unsupervised discovery of operational modes in univariate time-series data using image-based convolutional clustering with composite internal evaluation. The proposed framework improves upon existing approaches in three ways: (1) raw time-series sequences are transformed into grayscale matrix representations via overlapping sliding windows, allowing effective feature extraction using a deep convolutional autoencoder; (2) the framework integrates both soft and hard clustering outputs and refines the selection through a two-stage strategy; and (3) clustering performance is objectively evaluated by a newly developed composite score, S_eva, which combines normalized Silhouette, Calinski-Harabasz, and Davies-Bouldin indices. Applied to over 3900 furnace melting operations from a Nordic foundry, the method identifies seven explainable operational patterns, revealing significant differences in energy consumption, thermal dynamics, and production duration. Compared to classical and deep clustering baselines, the proposed approach achieves superior overall performance, greater robustness, and domain-aligned explainability. The framework addresses key challenges in unsupervised time-series analysis, such as sequence irregularity, overlapping modes, and metric inconsistency, and provides a generalizable solution for data-driven diagnostics and energy optimization in industrial systems.

</details>


### [260] [Hardware optimization on Android for inference of AI models](https://arxiv.org/abs/2511.13453)
*Iulius Gherasim,Carlos García Sánchez*

Main category: cs.LG

TL;DR: 研究AI模型在Android系统上的最优执行配置，重点关注目标检测和图像分类任务，通过评估不同量化方案和硬件加速器来平衡精度损失和推理速度


<details>
  <summary>Details</summary>
Motivation: 移动计算中AI模型应用广泛，但需要优化用户体验，包括降低延迟和提高响应性，同时面临实时约束和异构硬件架构的挑战

Method: 评估YOLO系列目标检测模型和ResNet图像分类模型的不同量化方案，以及GPU和NPU等设备加速器的使用

Result: 通过实证研究确定在精度损失最小化和推理速度最大化之间达到最佳平衡的配置组合

Conclusion: 提出了针对Android系统上AI模型的最优执行配置方案，为移动AI应用提供了实用的性能优化指导

Abstract: The pervasive integration of Artificial Intelligence models into contemporary mobile computing is notable across numerous use cases, from virtual assistants to advanced image processing. Optimizing the mobile user experience involves minimal latency and high responsiveness from deployed AI models with challenges from execution strategies that fully leverage real time constraints to the exploitation of heterogeneous hardware architecture. In this paper, we research and propose the optimal execution configurations for AI models on an Android system, focusing on two critical tasks: object detection (YOLO family) and image classification (ResNet). These configurations evaluate various model quantization schemes and the utilization of on device accelerators, specifically the GPU and NPU. Our core objective is to empirically determine the combination that achieves the best trade-off between minimal accuracy degradation and maximal inference speed-up.

</details>


### [261] [Artificial Intelligence-Enabled Spirometry for Early Detection of Right Heart Failure](https://arxiv.org/abs/2511.13457)
*Bin Liu,Qinghao Zhao,Yuxi Zhou,Zhejun Sun,Kaijie Lei,Deyun Zhang,Shijia Geng,Shenda Hong*

Main category: cs.LG

TL;DR: 提出一种基于自监督表示学习的方法，结合肺活量时间序列和人口统计学数据，用于早期检测右心衰竭。该方法使用变分自编码器学习肺活量数据的低维表示，再结合CatBoost分类器进行预测，在UK Biobank数据集上取得了良好的性能。


<details>
  <summary>Details</summary>
Motivation: 右心衰竭是一种与高发病率和死亡率相关的疾病，肺部疾病常导致右心室负荷增加，从而引发右心衰竭。因此，从有基础肺部疾病的患者中筛查出发展为右心衰竭的患者非常重要。

Method: 提出两阶段方法：第一阶段使用变分自编码器从数据增强的无标签数据中学习肺活量时间序列的鲁棒低维表示；第二阶段将该表示与人口统计学信息融合，输入CatBoost分类器进行右心衰竭预测。

Result: 在UK Biobank的26,617名个体子集上，模型检测右心衰竭的AUROC达到0.7501。在高风险临床亚组中表现更佳：在74名慢性肾病患者测试集上AUROC为0.8194，在64名瓣膜性心脏病患者测试集上AUROC为0.8413。

Conclusion: 本研究提出的自监督表示学习方法结合肺活量时间序列和人口统计学数据，在临床实践中显示出早期检测右心衰竭的良好潜力。

Abstract: Right heart failure (RHF) is a disease characterized by abnormalities in the structure or function of the right ventricle (RV), which is associated with high morbidity and mortality. Lung disease often causes increased right ventricular load, leading to RHF. Therefore, it is very important to screen out patients with cor pulmonale who develop RHF from people with underlying lung diseases. In this work, we propose a self-supervised representation learning method to early detecting RHF from patients with cor pulmonale, which uses spirogram time series to predict patients with RHF at an early stage. The proposed model is divided into two stages. The first stage is the self-supervised representation learning-based spirogram embedding (SLSE) network training process, where the encoder of the Variational autoencoder (VAE-encoder) learns a robust low-dimensional representation of the spirogram time series from the data-augmented unlabeled data. Second, this low-dimensional representation is fused with demographic information and fed into a CatBoost classifier for the downstream RHF prediction task. Trained and tested on a carefully selected subset of 26,617 individuals from the UK Biobank, our model achieved an AUROC of 0.7501 in detecting RHF, demonstrating strong population-level distinction ability. We further evaluated the model on high-risk clinical subgroups, achieving AUROC values of 0.8194 on a test set of 74 patients with chronic kidney disease (CKD) and 0.8413 on a set of 64 patients with valvular heart disease (VHD). These results highlight the model's potential utility in predicting RHF among clinically elevated-risk populations. In conclusion, this study presents a self-supervised representation learning approach combining spirogram time series and demographic data, demonstrating promising potential for early RHF detection in clinical practice.

</details>


### [262] [Multi-task GINN-LP for Multi-target Symbolic Regression](https://arxiv.org/abs/2511.13463)
*Hussein Rajabu,Lijun Qian,Xishuang Dong*

Main category: cs.LG

TL;DR: 提出了多任务回归GINN-LP（MTRGINN-LP），一种用于多目标符号回归的可解释神经网络，通过结合共享主干网络和任务特定输出层来捕获目标间依赖关系。


<details>
  <summary>Details</summary>
Motivation: 符号回归方法主要评估在科学数据集上，限制了泛化能力，且主要针对单输出回归，而现实世界问题常涉及具有相互依赖变量的多目标输出。

Method: 将GINN-LP与多任务深度学习集成，结合包含多个幂项近似器块的共享主干网络和任务特定输出层。

Result: 在能源效率预测和可持续农业等实际多目标应用上验证，实验结果显示具有竞争力的预测性能和高度可解释性。

Conclusion: 有效将符号回归扩展到更广泛的现实世界多输出任务。

Abstract: In the area of explainable artificial intelligence, Symbolic Regression (SR) has emerged as a promising approach by discovering interpretable mathematical expressions that fit data. However, SR faces two main challenges: most methods are evaluated on scientific datasets with well-understood relationships, limiting generalization, and SR primarily targets single-output regression, whereas many real-world problems involve multi-target outputs with interdependent variables. To address these issues, we propose multi-task regression GINN-LP (MTRGINN-LP), an interpretable neural network for multi-target symbolic regression. By integrating GINN-LP with a multi-task deep learning, the model combines a shared backbone including multiple power-term approximator blocks with task-specific output layers, capturing inter-target dependencies while preserving interpretability. We validate multi-task GINN-LP on practical multi-target applications, including energy efficiency prediction and sustainable agriculture. Experimental results demonstrate competitive predictive performance alongside high interpretability, effectively extending symbolic regression to broader real-world multi-output tasks.

</details>


### [263] [AdamX: An Adam improvement algorithm based on a novel exponential decay mechanism for the second-order moment estimate](https://arxiv.org/abs/2511.13465)
*Meng Zhu,Quan Xiao,Weidong Min*

Main category: cs.LG

TL;DR: 提出了AdamX优化算法，通过新的二阶矩估计指数衰减率机制，在训练后期减弱学习步长修正强度并退化为SGD，以解决Adam容易收敛到非平坦最小值的问题。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型时代，Adam仍是主流优化算法，但与SGD相比更容易收敛到非平坦最小值，影响训练稳定性和泛化能力。

Method: 提出AdamX算法，核心创新是新型二阶矩估计指数衰减率，随着训练进行逐渐减弱学习步长修正强度，在稳定训练期退化为SGD。

Result: 实验表明新的二阶矩估计指数衰减率优于现有方法，AdamX在性能上稳定优于Adam及其变体。

Conclusion: AdamX通过改进的二阶矩估计机制提高了训练稳定性，可能增强泛化能力，代码已开源。

Abstract: Since the 21st century, artificial intelligence has been leading a new round of industrial revolution. Under the training framework, the optimization algorithm aims to stably converge high-dimensional optimization to local and even global minima. Entering the era of large language models, although the scale of model parameters and data has increased, Adam remains the mainstream optimization algorithm. However, compared with stochastic gradient descent (SGD) based optimization algorithms, Adam is more likely to converge to non-flat minima. To address this issue, the AdamX algorithm is proposed. Its core innovation lies in the proposition of a novel type of second-order moment estimation exponential decay rate, which gradually weakens the learning step correction strength as training progresses, and degrades to SGD in the stable training period, thereby improving the stability of training in the stable period and possibly enhancing generalization ability. Experimental results show that our second-order moment estimation exponential decay rate is better than the current second-order moment estimation exponential decay rate, and AdamX can stably outperform Adam and its variants in terms of performance. Our code is open-sourced at https://github.com/mengzhu0308/AdamX.

</details>


### [264] [GREAT: Generalizable Representation Enhancement via Auxiliary Transformations for Zero-Shot Environmental Prediction](https://arxiv.org/abs/2511.13469)
*Shiyuan Luo,Chonghao Qiu,Runlong Yu,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TL;DR: GREAT框架通过辅助变换增强环境建模数据，解决因空间异质性和数据不平衡导致的泛化问题，在未见区域实现更好的预测性能。


<details>
  <summary>Details</summary>
Motivation: 环境建模面临未监测区域预测困难的问题，主要由于观测数据有限且地理分布不平衡，加上空间异质性导致模型学习到虚假的局部模式。

Method: GREAT框架学习神经网络多层的变换函数来增强原始环境特征和时间影响，通过双层训练过程约束增强数据保留源数据的关键模式。

Result: 在美国东部六个生态多样化流域的河流温度预测实验中，GREAT在零样本场景下显著优于现有方法。

Conclusion: 这项工作为无法进行全面监测的环境应用提供了实用解决方案，通过数据增强改善模型在未见区域的泛化能力。

Abstract: Environmental modeling faces critical challenges in predicting ecosystem dynamics across unmonitored regions due to limited and geographically imbalanced observation data. This challenge is compounded by spatial heterogeneity, causing models to learn spurious patterns that fit only local data. Unlike conventional domain generalization, environmental modeling must preserve invariant physical relationships and temporal coherence during augmentation. In this paper, we introduce Generalizable Representation Enhancement via Auxiliary Transformations (GREAT), a framework that effectively augments available datasets to improve predictions in completely unseen regions. GREAT guides the augmentation process to ensure that the original governing processes can be recovered from the augmented data, and the inclusion of the augmented data leads to improved model generalization. Specifically, GREAT learns transformation functions at multiple layers of neural networks to augment both raw environmental features and temporal influence. They are refined through a novel bi-level training process that constrains augmented data to preserve key patterns of the original source data. We demonstrate GREAT's effectiveness on stream temperature prediction across six ecologically diverse watersheds in the eastern U.S., each containing multiple stream segments. Experimental results show that GREAT significantly outperforms existing methods in zero-shot scenarios. This work provides a practical solution for environmental applications where comprehensive monitoring is infeasible.

</details>


### [265] [Naga: Vedic Encoding for Deep State Space Models](https://arxiv.org/abs/2511.13510)
*Melanie Schaller,Nick Janssen,Bodo Rosenhahn*

Main category: cs.LG

TL;DR: Naga是一种受吠陀数学启发的深度状态空间模型，通过双向处理时间序列和元素级交互来增强长程时间依赖性的捕捉能力，在多个长时序预测基准测试中优于28个现有SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 现有的深度状态空间模型在长时序预测中仍面临捕捉远距离时间依赖性的挑战，需要更高效和可解释的序列建模方法。

Method: 提出双向表示方法，联合处理前向和时间反转的输入序列，通过元素级(Hadamard)交互生成吠陀数学启发的编码。

Result: 在ETTh1、ETTh2、ETTm1、ETTm2、Weather、Traffic和ILI等多个基准测试中，Naga超越了28个当前SOTA模型，且相比现有深度SSM方法具有更高的效率。

Conclusion: 结合结构化、吠陀数学启发的分解方法可以为长程序列建模提供可解释且计算高效的替代方案。

Abstract: This paper presents Naga, a deep State Space Model (SSM) encoding approach inspired by structural concepts from Vedic mathematics. The proposed method introduces a bidirectional representation for time series by jointly processing forward and time-reversed input sequences. These representations are then combined through an element-wise (Hadamard) interaction, resulting in a Vedic-inspired encoding that enhances the model's ability to capture temporal dependencies across distant time steps. We evaluate Naga on multiple long-term time series forecasting (LTSF) benchmarks, including ETTh1, ETTh2, ETTm1, ETTm2, Weather, Traffic, and ILI. The experimental results show that Naga outperforms 28 current state of the art models and demonstrates improved efficiency compared to existing deep SSM-based approaches. The findings suggest that incorporating structured, Vedic-inspired decomposition can provide an interpretable and computationally efficient alternative for long-range sequence modeling.

</details>


### [266] [A Quantum Tensor Network-Based Viewpoint for Modeling and Analysis of Time Series Data](https://arxiv.org/abs/2511.13514)
*Pragatheeswaran Vipulananthan,Kamal Premaratne,Dilip Sarkar,Manohar N. Murthi*

Main category: cs.LG

TL;DR: 提出了一种基于量子物理学的"白盒"方法，通过将时间序列数据的核均值嵌入映射到再生核希尔伯特空间，构建张量网络启发的1D自旋链哈密顿量，并应用微扰理论来量化不确定性，从而在保持准确性的同时提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 神经网络虽然功能强大但缺乏可解释性，而概率"白盒"模型虽然可解释但性能较差。需要一种既能准确量化不确定性又具有良好可解释性的方法。

Method: 将时间序列数据的核均值嵌入映射到再生核希尔伯特空间，构建张量网络启发的1D自旋链哈密顿量，将核均值嵌入作为其本征函数或本征模式，然后求解薛定谔方程并应用微扰理论来量化不确定性。

Result: 该方法在变化点检测和时间序列聚类任务中表现出色，相比最先进的"白盒"模型具有更好的性能，同时提供了决策过程中相关不确定性的深入见解。

Conclusion: 提出的量子物理学启发的白盒方法成功地在准确的不确定性量化和增强的可解释性之间取得了平衡，为机器学习中的不确定性量化问题提供了新的解决方案。

Abstract: Accurate uncertainty quantification is a critical challenge in machine learning. While neural networks are highly versatile and capable of learning complex patterns, they often lack interpretability due to their ``black box'' nature. On the other hand, probabilistic ``white box'' models, though interpretable, often suffer from a significant performance gap when compared to neural networks. To address this, we propose a novel quantum physics-based ``white box'' method that offers both accurate uncertainty quantification and enhanced interpretability. By mapping the kernel mean embedding (KME) of a time series data vector to a reproducing kernel Hilbert space (RKHS), we construct a tensor network-inspired 1D spin chain Hamiltonian, with the KME as one of its eigen-functions or eigen-modes. We then solve the associated Schr{ö}dinger equation and apply perturbation theory to quantify uncertainty, thereby improving the interpretability of tasks performed with the quantum tensor network-based model. We demonstrate the effectiveness of this methodology, compared to state-of-the-art ``white box" models, in change point detection and time series clustering, providing insights into the uncertainties associated with decision-making throughout the process.

</details>


### [267] [Mitigating Spurious Correlations in Patch-wise Tumor Classification on High-Resolution Multimodal Images](https://arxiv.org/abs/2511.13527)
*Ihab Asaad,Maha Shadaydeh,Joachim Denzler*

Main category: cs.LG

TL;DR: 本文研究高分辨率多模态非线性显微镜图像中的肿瘤检测，采用patch-wise二元分类方法。研究发现该方法会引入虚假相关性（肿瘤patch通常包含较大组织区域），并提出使用GERNE去偏策略来提升最差组准确率约7%。


<details>
  <summary>Details</summary>
Motivation: patch-wise多标签分类为高分辨率图像提供了一种高效替代像素级分割的方法，能显著降低标注成本、简化训练过程。但简化的二元分类会引入虚假相关性，导致模型预测偏差。

Method: 采用patch-wise二元分类方法检测肿瘤，识别出patch组成与标签间的虚假相关性，并应用GERNE去偏方法来最大化最差组准确率。

Result: 相比ERM，GERNE方法在最差组准确率上提升了约7%，显著改善了在关键少数情况（小组织肿瘤patch和大组织非肿瘤patch）上的模型性能。

Conclusion: patch-wise分类问题中需要关注虚假相关性，去偏策略能有效提升模型在关键少数情况下的性能，确保模型决策的可靠性。

Abstract: Patch-wise multi-label classification provides an efficient alternative to full pixel-wise segmentation on high-resolution images, particularly when the objective is to determine the presence or absence of target objects within a patch rather than their precise spatial extent. This formulation substantially reduces annotation cost, simplifies training, and allows flexible patch sizing aligned with the desired level of decision granularity. In this work, we focus on a special case, patch-wise binary classification, applied to the detection of a single class of interest (tumor) on high-resolution multimodal nonlinear microscopy images. We show that, although this simplified formulation enables efficient model development, it can introduce spurious correlations between patch composition and labels: tumor patches tend to contain larger tissue regions, whereas non-tumor patches often consist mostly of background with small tissue areas. We further quantify the bias in model predictions caused by this spurious correlation, and propose to use a debiasing strategy to mitigate its effect. Specifically, we apply GERNE, a debiasing method that can be adapted to maximize worst-group accuracy (WGA). Our results show an improvement in WGA by approximately 7% compared to ERM for two different thresholds used to binarize the spurious feature. This enhancement boosts model performance on critical minority cases, such as tumor patches with small tissues and non-tumor patches with large tissues, and underscores the importance of spurious correlation-aware learning in patch-wise classification problems.

</details>


### [268] [Fairness-Aware Graph Representation Learning with Limited Demographic Information](https://arxiv.org/abs/2511.13540)
*Zichong Wang,Zhipeng Yin,Liping Yang,Jun Zhuang,Rui Yu,Qingzhao Kong,Wenbin Zhang*

Main category: cs.LG

TL;DR: 提出了FairGLite框架，在部分人口统计信息可用的情况下实现公平图学习，通过生成人口统计代理、强制跨群体嵌入一致性和自适应置信度策略来缓解偏见。


<details>
  <summary>Details</summary>
Motivation: 现有公平图学习方法大多假设可以完全访问人口统计信息，但在实践中由于隐私、法律或监管限制，这种情况很少见。

Method: 使用部分人口统计数据生成人口统计信息代理，设计策略强制不同人口群体间的节点嵌入一致性，并开发自适应置信度策略动态调整节点对公平性和效用的贡献。

Result: 理论分析表明FairGLite在群体公平性指标上达到可证明的上界，实验证明该框架在多个数据集和公平图学习框架中有效缓解偏见并保持模型效用。

Conclusion: FairGLite是一个有效的公平图学习框架，能够在有限人口统计信息的情况下提供形式化保证的偏见缓解。

Abstract: Ensuring fairness in Graph Neural Networks is fundamental to promoting trustworthy and socially responsible machine learning systems. In response, numerous fair graph learning methods have been proposed in recent years. However, most of them assume full access to demographic information, a requirement rarely met in practice due to privacy, legal, or regulatory restrictions. To this end, this paper introduces a novel fair graph learning framework that mitigates bias in graph learning under limited demographic information. Specifically, we propose a mechanism guided by partial demographic data to generate proxies for demographic information and design a strategy that enforces consistent node embeddings across demographic groups. In addition, we develop an adaptive confidence strategy that dynamically adjusts each node's contribution to fairness and utility based on prediction confidence. We further provide theoretical analysis demonstrating that our framework, FairGLite, achieves provable upper bounds on group fairness metrics, offering formal guarantees for bias mitigation. Through extensive experiments on multiple datasets and fair graph learning frameworks, we demonstrate the framework's effectiveness in both mitigating bias and maintaining model utility.

</details>


### [269] [Graph Out-of-Distribution Detection via Test-Time Calibration with Dual Dynamic Dictionaries](https://arxiv.org/abs/2511.13541)
*Yue Hou,Ruomei Liu,Yingke Su,Junran Wu,Ke Xu*

Main category: cs.LG

TL;DR: 提出了一种新的测试时图OOD检测方法BaCa，通过双动态字典校准OOD分数，无需微调预训练模型，在真实数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决图OOD检测中缺乏真实OOD样本训练数据的问题，现有方法仅捕捉ID特征导致分布边界表示不可靠，且图数据的潜在多因素结构未被充分探索。

Method: 使用双动态字典（基于优先级队列和注意力机制）自适应捕获潜在ID和OOD表示，通过图估计和混合策略生成边界感知判别拓扑，进行边界感知OOD分数校准。

Result: 在真实世界数据集上的广泛实验表明，BaCa在OOD检测方面显著优于现有的最先进方法。

Conclusion: BaCa方法通过动态字典和边界感知校准，有效解决了图OOD检测的关键挑战，无需辅助数据集或模型微调，取得了优越性能。

Abstract: A key challenge in graph out-of-distribution (OOD) detection lies in the absence of ground-truth OOD samples during training. Existing methods are typically optimized to capture features within the in-distribution (ID) data and calculate OOD scores, which often limits pre-trained models from representing distributional boundaries, leading to unreliable OOD detection. Moreover, the latent structure of graph data is often governed by multiple underlying factors, which remains less explored. To address these challenges, we propose a novel test-time graph OOD detection method, termed BaCa, that calibrates OOD scores using dual dynamically updated dictionaries without requiring fine-tuning the pre-trained model. Specifically, BaCa estimates graphons and applies a mix-up strategy solely with test samples to generate diverse boundary-aware discriminative topologies, eliminating the need for exposing auxiliary datasets as outliers. We construct dual dynamic dictionaries via priority queues and attention mechanisms to adaptively capture latent ID and OOD representations, which are then utilized for boundary-aware OOD score calibration. To the best of our knowledge, extensive experiments on real-world datasets show that BaCa significantly outperforms existing state-of-the-art methods in OOD detection.

</details>


### [270] [RAC-DMVC: Reliability-Aware Contrastive Deep Multi-View Clustering under Multi-Source Noise](https://arxiv.org/abs/2511.13561)
*Shihao Dong,Yue Liu,Xiaotong Zhou,Yuhui Zheng,Huiying Xu,Xinzhong Zhu*

Main category: cs.LG

TL;DR: 提出RAC-DMVC框架，通过可靠性图指导多源噪声环境下的鲁棒表示学习，解决多视图聚类中的缺失噪声和观测噪声问题。


<details>
  <summary>Details</summary>
Motivation: 增强多视图聚类在现实场景中的适用性，处理更具挑战性的多源噪声环境，包括缺失噪声和观测噪声。

Method: 使用可靠性图指导表示学习；通过跨视图重构处理观测噪声；可靠性感知噪声对比学习缓解噪声表示带来的正负对选择偏差；双注意力插补处理缺失噪声；自监督聚类蒸馏模块优化表示。

Result: 在五个基准数据集上的广泛实验表明，RAC-DMVC在多个评估指标上优于SOTA方法，并在不同噪声比例下保持优异性能。

Conclusion: RAC-DMVC框架能有效处理多视图聚类中的多源噪声问题，提供鲁棒的聚类性能。

Abstract: Multi-view clustering (MVC), which aims to separate the multi-view data into distinct clusters in an unsupervised manner, is a fundamental yet challenging task. To enhance its applicability in real-world scenarios, this paper addresses a more challenging task: MVC under multi-source noises, including missing noise and observation noise. To this end, we propose a novel framework, Reliability-Aware Contrastive Deep Multi-View Clustering (RAC-DMVC), which constructs a reliability graph to guide robust representation learning under noisy environments. Specifically, to address observation noise, we introduce a cross-view reconstruction to enhances robustness at the data level, and a reliability-aware noise contrastive learning to mitigates bias in positive and negative pairs selection caused by noisy representations. To handle missing noise, we design a dual-attention imputation to capture shared information across views while preserving view-specific features. In addition, a self-supervised cluster distillation module further refines the learned representations and improves the clustering performance. Extensive experiments on five benchmark datasets demonstrate that RAC-DMVC outperforms SOTA methods on multiple evaluation metrics and maintains excellent performance under varying ratios of noise.

</details>


### [271] [P1: Mastering Physics Olympiads with Reinforcement Learning](https://arxiv.org/abs/2511.13612)
*Jiacheng Chen,Qianjia Cheng,Fangchen Yu,Haiyuan Wan,Yuchen Zhang,Shenghe Zheng,Junchi Yao,Qingyang Zhang,Haonan He,Yun Luo,Yufeng Zhao,Futing Wang,Li Sheng,Chengxing Xie,Yuxin Zuo,Yizhuo Li,Wenxauan Zeng,Yulun Wu,Rui Huang,Dongzhan Zhou,Kai Chen,Yu Qiao,Lei Bai,Yu Cheng,Ning Ding,Bowen Zhou,Peng Ye,Ganqu Cui*

Main category: cs.LG

TL;DR: P1系列开源物理推理模型通过强化学习训练，在物理奥林匹克竞赛中表现卓越，其中P1-235B-A22B在IPhO 2025获得金牌，并在13个国际/地区物理竞赛中赢得12枚金牌。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的发展已从解谜转向科学级推理，物理作为将符号与现实绑定的基础学科，是测试这种转变的最佳领域。

Method: 通过强化学习训练开源物理推理模型家族P1，并配备代理框架PhysicsMinions。

Result: P1-235B-A22B在IPhO 2025获得金牌，在13个物理竞赛中赢得12枚金牌；P1-30B-A3B获得银牌；P1-235B-A22B+PhysicsMinions在IPhO 2025总体排名第一。

Conclusion: P1模型不仅在物理推理方面表现卓越，在数学和编程等其他推理任务上也展现出强大的泛化能力。

Abstract: Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.

</details>


### [272] [Batch Acquisition Function Evaluations and Decouple Optimizer Updates for Faster Bayesian Optimization](https://arxiv.org/abs/2511.13625)
*Kaichi Irie,Shuhei Watanabe,Masaki Onishi*

Main category: cs.LG

TL;DR: 本文提出了一种解耦准牛顿更新的方法，通过协程在批处理采集函数调用的同时保持与顺序多起点优化相同的理论收敛性，显著减少了贝叶斯优化中采集函数优化的计算时间。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化中采集函数优化的主要计算瓶颈在于多起点优化，现有方法如BoTorch通过批处理采集函数来加速，但这种方法在准牛顿方法的逆Hessian近似中存在次优性，导致收敛速度变慢。

Method: 提出使用协程解耦准牛顿更新，同时在批处理采集函数调用，既保持了与顺序多起点优化相同的理论收敛性，又大幅减少了实际运行时间。

Result: 该方法不仅实现了与顺序多起点优化相同的理论收敛性，而且相比之前的方法显著减少了实际运行时间。

Conclusion: 通过解耦准牛顿更新并使用协程批处理采集函数调用，可以有效解决贝叶斯优化中采集函数优化的计算瓶颈问题，在保持收敛性的同时大幅提升效率。

Abstract: Bayesian optimization (BO) efficiently finds high-performing parameters by maximizing an acquisition function, which models the promise of parameters. A major computational bottleneck arises in acquisition function optimization, where multi-start optimization (MSO) with quasi-Newton (QN) methods is required due to the non-convexity of the acquisition function. BoTorch, a widely used BO library, currently optimizes the summed acquisition function over multiple points, leading to the speedup of MSO owing to PyTorch batching. Nevertheless, this paper empirically demonstrates the suboptimality of this approach in terms of off-diagonal approximation errors in the inverse Hessian of a QN method, slowing down its convergence. To address this problem, we propose to decouple QN updates using a coroutine while batching the acquisition function calls. Our approach not only yields the theoretically identical convergence to the sequential MSO but also drastically reduces the wall-clock time compared to the previous approaches.

</details>


### [273] [Towards Multimodal Representation Learning in Paediatric Kidney Disease](https://arxiv.org/abs/2511.13637)
*Ana Durica,John Booth,Ivana Drobnjak*

Main category: cs.LG

TL;DR: 使用电子健康记录和人口统计信息，通过循环神经网络预测儿童未来30天内是否会出现异常血清肌酐值。


<details>
  <summary>Details</summary>
Motivation: 儿科肾脏疾病表现和进展差异很大，需要持续监测肾功能。

Method: 使用2019-2025年收集的电子健康记录，整合纵向实验室序列和人口统计信息，训练循环神经网络模型。

Result: 初步证明简单的时间表示可以捕捉常规儿科数据中的有用模式。

Conclusion: 这项工作为未来使用额外临床信号和更详细肾脏结果的多模态扩展奠定了基础。

Abstract: Paediatric kidney disease varies widely in its presentation and progression, which calls for continuous monitoring of renal function. Using electronic health records collected between 2019 and 2025 at Great Ormond Street Hospital, a leading UK paediatric hospital, we explored a temporal modelling approach that integrates longitudinal laboratory sequences with demographic information. A recurrent neural model trained on these data was used to predict whether a child would record an abnormal serum creatinine value within the following thirty days. Framed as a pilot study, this work provides an initial demonstration that simple temporal representations can capture useful patterns in routine paediatric data and lays the groundwork for future multimodal extensions using additional clinical signals and more detailed renal outcomes.

</details>


### [274] [Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures](https://arxiv.org/abs/2511.13640)
*Haohui Wang,Jingyuan Qi,Jianpeng Chen,Jun Wu,Lifu Huang,Lecheng Zheng,Kevin Choi,Balaji Veeramani,Edward Bowen,Alison Hu,Tyler Cody,Dawei Zhou*

Main category: cs.LG

TL;DR: 本文分析了混合真实与合成数据对LLM训练的影响，识别出三阶段缩放行为，提出了适用于混合数据的泛化边界理论，并开发了高效的数据评估方法。


<details>
  <summary>Details</summary>
Motivation: 合成数据虽然具有可扩展性和成本效益，但会引入系统性分布差异，特别是在长尾知识方面存在不足，这给混合数据集的效用评估带来了根本性挑战。

Method: 识别了三阶段缩放行为特征，推导了适用于真实-合成混合数据的LLM泛化边界理论，并基于理论发现提出了可扩展的高效数据评估方法。

Result: 在图像分类、情感分类、指令跟随和复杂推理四个任务上的综合实验表明，该方法在数据评估方面超越了现有最优基线，且计算成本显著降低。

Conclusion: 研究揭示了混合真实与合成数据训练LLM的关键动态特征，提出的理论框架和高效评估方法为解决合成数据引入的分布偏差问题提供了有效工具。

Abstract: The rapid progress of large language models (LLMs) is fueled by the growing reliance on datasets that blend real and synthetic data. While synthetic data offers scalability and cost-efficiency, it often introduces systematic distributional discrepancies, particularly underrepresenting long-tail knowledge due to truncation effects from data generation mechanisms like top-p sampling, temperature scaling, and finite sampling. These discrepancies pose fundamental challenges in characterizing and evaluating the utility of mixed real-synthetic datasets. In this paper, we identify a three-phase scaling behavior characterized by two breakpoints that reflect transitions in model behavior across learning head and tail knowledge. We further derive an LLM generalization bound designed for real and synthetic mixtures, revealing several key factors that govern their generalization performance. Building on our theoretical findings, we propose an effective yet efficient data valuation method that scales to large-scale datasets. Comprehensive experiments across four tasks, including image classification, sentiment classification, instruction following, and complex reasoning, demonstrate that our method surpasses state-of-the-art baselines in data valuation with significantly low computational cost.

</details>


### [275] [FuseSampleAgg: Fused Neighbor Sampling and Aggregation for Mini-batch GNNs](https://arxiv.org/abs/2511.13645)
*Aleksandar Stanković*

Main category: cs.LG

TL;DR: FuseSampleAgg是一个CUDA算子，将邻居采样和均值聚合融合为单次操作，用于一阶和二阶GraphSAGE，显著提升训练速度并大幅减少GPU内存使用。


<details>
  <summary>Details</summary>
Motivation: 现有的GraphSAGE实现需要多次内核启动和中间结果存储，导致内存流量大、开销高，限制了训练效率。

Method: 通过单次融合操作执行邻居采样和均值聚合，消除块物化和额外内核启动，通过保存索引重放保持GraphSAGE均值语义。

Result: 在多个基准测试中，步长时间加速最高达51倍，GPU内存峰值减少最高达100倍，同时保持确定性并与标准PyTorch优化器兼容。

Conclusion: FuseSampleAgg通过算子融合有效解决了GraphSAGE训练中的性能瓶颈，实现了显著的加速和内存优化。

Abstract: We present FuseSampleAgg, a CUDA operator that fuses neighbor sampling and mean aggregation into a single pass for one and two hop GraphSAGE. By eliminating block materialization and extra kernel launches, FuseSampleAgg reduces memory traffic and overhead while preserving GraphSAGE mean semantics via saved index replay. Across the Reddit, ogbn-arxiv, and ogbn-products benchmarks (batch size 1024, automatic mixed precision enabled), we observe step time speedups up to 51x on ogbn-products, about 4x on Reddit with fanouts 10-10 and 15-10, and about 3.3x on ogbn-arxiv at larger fanouts, with peak GPU memory reductions up to 100x, 36x, and about 3.5x, respectively. The operator is deterministic, integrates with standard PyTorch optimizers, and ships with scripts that reproduce all tables and figures from CSV logs. Code and scripts are available at https://github.com/SV25-22/FuseSampleAgg.

</details>


### [276] [Weight-sparse transformers have interpretable circuits](https://arxiv.org/abs/2511.13653)
*Leo Gao,Achyuta Rajaram,Jacob Coxon,Soham V. Govande,Bowen Baker,Dan Mossing*

Main category: cs.LG

TL;DR: 通过约束模型权重稀疏化来训练更易理解的语言模型电路，在保持性能的同时提高可解释性，并验证了电路的人类可理解性。


<details>
  <summary>Details</summary>
Motivation: 寻找语言模型中人类可理解的电路是机械可解释性领域的核心目标，当前模型通常难以理解其内部工作机制。

Method: 训练权重稀疏的模型（大部分权重为零），通过剪枝技术分离出特定任务对应的电路，使神经元和残差通道对应自然概念。

Result: 稀疏模型在可解释性和性能之间存在权衡，扩大模型规模可以改善这一权衡，但超过千万级非零参数时保持可解释性仍具挑战性。

Conclusion: 该方法能够产生前所未有的人类可理解电路，并可以扩展到解释现有密集模型，为模型可解释性提供了有效途径。

Abstract: Finding human-understandable circuits in language models is a central goal of the field of mechanistic interpretability. We train models to have more understandable circuits by constraining most of their weights to be zeros, so that each neuron only has a few connections. To recover fine-grained circuits underlying each of several hand-crafted tasks, we prune the models to isolate the part responsible for the task. These circuits often contain neurons and residual channels that correspond to natural concepts, with a small number of straightforwardly interpretable connections between them. We study how these models scale and find that making weights sparser trades off capability for interpretability, and scaling model size improves the capability-interpretability frontier. However, scaling sparse models beyond tens of millions of nonzero parameters while preserving interpretability remains a challenge. In addition to training weight-sparse models de novo, we show preliminary results suggesting our method can also be adapted to explain existing dense models. Our work produces circuits that achieve an unprecedented level of human understandability and validates them with considerable rigor.

</details>


### [277] [Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning](https://arxiv.org/abs/2511.13654)
*Pascal Zimmer,Ghassan Karame*

Main category: cs.LG

TL;DR: 本文首次系统分析了优化超参数（学习率、权重衰减、动量、批大小）对迁移攻击和查询攻击鲁棒性的影响，发现在不同攻击类型下学习率调整策略相反，并探索了同时增强两种攻击鲁棒性的超参数设计空间。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对优化超参数如何影响对抗攻击鲁棒性的系统研究，特别是在不同攻击类型（迁移攻击vs查询攻击）和不同训练设置下的影响机制尚不明确。

Method: 通过理论分析和实验验证，研究了多种实际部署场景（集中训练、集成学习、分布式训练）中优化超参数对鲁棒性的影响。

Result: 发现学习率调整对两种攻击类型产生相反效果：降低学习率可提升迁移攻击鲁棒性达64%，而提高学习率可提升查询攻击鲁棒性达28%。分布式模型通过超参数调优能最有效地同时缓解两种攻击。

Conclusion: 优化超参数对对抗鲁棒性有显著影响且存在攻击类型依赖性，分布式训练结合适当的超参数调优可同时增强对迁移攻击和查询攻击的防御能力。

Abstract: In this paper, we present the first detailed analysis of how optimization hyperparameters -- such as learning rate, weight decay, momentum, and batch size -- influence robustness against both transfer-based and query-based attacks. Supported by theory and experiments, our study spans a variety of practical deployment settings, including centralized training, ensemble learning, and distributed training. We uncover a striking dichotomy: for transfer-based attacks, decreasing the learning rate significantly enhances robustness by up to $64\%$. In contrast, for query-based attacks, increasing the learning rate consistently leads to improved robustness by up to $28\%$ across various settings and data distributions. Leveraging these findings, we explore -- for the first time -- the optimization hyperparameter design space to jointly enhance robustness against both transfer-based and query-based attacks. Our results reveal that distributed models benefit the most from hyperparameter tuning, achieving a remarkable tradeoff by simultaneously mitigating both attack types more effectively than other training setups.

</details>


### [278] [Scientific Data Compression and Super-Resolution Sampling](https://arxiv.org/abs/2511.13675)
*Minh Vu,Andrey Lokhov*

Main category: cs.LG

TL;DR: 提出基于指数族学习的新型科学数据压缩和超分辨率框架，支持不确定性量化和压缩比-重建保真度的灵活权衡


<details>
  <summary>Details</summary>
Motivation: 现代科学模拟、观测和大规模实验产生的数据量常超出存储、处理和分析能力极限，需要既能高效管理海量数据集又能保持关键物理特征的数据缩减方法

Method: 基于近期指数族学习进展，开发科学数据压缩和超分辨率框架，支持物理量不确定性量化

Result: 该方法能够从压缩表示中恢复数据，保证关键物理特性的保持，支持检查点和重启等科学工作流需求

Conclusion: 该框架为科学数据管理提供了有效的压缩和恢复解决方案，在压缩比和重建质量之间实现灵活平衡

Abstract: Modern scientific simulations, observations, and large-scale experiments generate data at volumes that often exceed the limits of storage, processing, and analysis. This challenge drives the development of data reduction methods that efficiently manage massive datasets while preserving essential physical features and quantities of interest. In many scientific workflows, it is also crucial to enable data recovery from compressed representations - a task known as super-resolution - with guarantees on the preservation of key physical characteristics. A notable example is checkpointing and restarting, which is essential for long-running simulations to recover from failures, resume after interruptions, or examine intermediate results. In this work, we introduce a novel framework for scientific data compression and super-resolution, grounded in recent advances in learning exponential families. Our method preserves and quantifies uncertainty in physical quantities of interest and supports flexible trade-offs between compression ratio and reconstruction fidelity.

</details>


### [279] [Cross-Learning from Scarce Data via Multi-Task Constrained Optimization](https://arxiv.org/abs/2511.13680)
*Leopoldo Agorio,Juan Cerviño,Miguel Calvo-Fullana,Alejandro Ribeiro,Juan Andrés Bazerque*

Main category: cs.LG

TL;DR: 提出跨学习框架，通过联合估计多个相关任务的确定性参数来解决数据稀缺问题，实现从数据丰富任务到数据稀缺任务的知识迁移。


<details>
  <summary>Details</summary>
Motivation: 当数据有限时，学习模型难以泛化到未见过的案例。需要克服数据稀缺问题，特别是在参数推断对有限数据至关重要的场景中。

Method: 将联合估计制定为约束优化问题，约束条件控制不同模型参数之间的相似性，允许参数在不同任务间差异，同时结合多个数据源的信息。

Result: 在控制框架中提供理论保证，并在图像分类和传染病传播等实际应用中展示了跨学习方法的有效性。

Conclusion: 跨学习框架能够实现从数据丰富任务到数据稀缺任务的知识迁移，产生更准确可靠的参数估计，为有限数据下的参数推断提供解决方案。

Abstract: A learning task, understood as the problem of fitting a parametric model from supervised data, fundamentally requires the dataset to be large enough to be representative of the underlying distribution of the source. When data is limited, the learned models fail generalize to cases not seen during training. This paper introduces a multi-task \emph{cross-learning} framework to overcome data scarcity by jointly estimating \emph{deterministic} parameters across multiple, related tasks. We formulate this joint estimation as a constrained optimization problem, where the constraints dictate the resulting similarity between the parameters of the different models, allowing the estimated parameters to differ across tasks while still combining information from multiple data sources. This framework enables knowledge transfer from tasks with abundant data to those with scarce data, leading to more accurate and reliable parameter estimates, providing a solution for scenarios where parameter inference from limited data is critical. We provide theoretical guarantees in a controlled framework with Gaussian data, and show the efficiency of our cross-learning method in applications with real data including image classification and propagation of infectious diseases.

</details>


### [280] [Protein Secondary Structure Prediction Using 3D Graphs and Relation-Aware Message Passing Transformers](https://arxiv.org/abs/2511.13685)
*Disha Varshney,Samarth Garg,Sarthak Tyagi,Deeksha Varshney,Nayan Deep,Asif Ekbal*

Main category: cs.LG

TL;DR: 提出SSRGNet模型，结合图神经网络和语言模型，利用蛋白质3D结构数据预测二级结构，在f1分数上超越基线方法


<details>
  <summary>Details</summary>
Motivation: 现有方法主要使用未标记的氨基酸序列，未能充分利用可获取的蛋白质3D结构数据，而3D结构是决定蛋白质功能的关键因素

Method: 使用蛋白质残基图，结合图神经网络（GCN、R-GCN）和预训练蛋白质语言模型，通过消息传递机制捕获蛋白质结构的几何特征和空间信息

Result: 在NetSurfP-2.0数据集上的实验表明，SSRGNet在3态和8态二级结构预测的f1分数上超越了基线方法

Conclusion: 通过有效整合蛋白质3D结构信息和序列信息，提出的方法在二级结构预测任务中取得了更好的性能

Abstract: In this study, we tackle the challenging task of predicting secondary structures from protein primary sequences, a pivotal initial stride towards predicting tertiary structures, while yielding crucial insights into protein activity, relationships, and functions. Existing methods often utilize extensive sets of unlabeled amino acid sequences. However, these approaches neither explicitly capture nor harness the accessible protein 3D structural data, which is recognized as a decisive factor in dictating protein functions. To address this, we utilize protein residue graphs and introduce various forms of sequential or structural connections to capture enhanced spatial information. We adeptly combine Graph Neural Networks (GNNs) and Language Models (LMs), specifically utilizing a pre-trained transformer-based protein language model to encode amino acid sequences and employing message-passing mechanisms like GCN and R-GCN to capture geometric characteristics of protein structures. Employing convolution within a specific node's nearby region, including relations, we stack multiple convolutional layers to efficiently learn combined insights from the protein's spatial graph, revealing intricate interconnections and dependencies in its structural arrangement. To assess our model's performance, we employed the training dataset provided by NetSurfP-2.0, which outlines secondary structure in 3-and 8-states. Extensive experiments show that our proposed model, SSRGNet surpasses the baseline on f1-scores.

</details>


### [281] [Efficient Calibration for Decision Making](https://arxiv.org/abs/2511.13699)
*Parikshit Gopalan,Konstantinos Stavropoulos,Kunal Talwar,Pranay Tankala*

Main category: cs.LG

TL;DR: 本文提出了一种基于结构化后处理函数族的校准决策损失度量CDL_K，解决了原始CDL度量的计算不可行性问题，并建立了该度量的理论和计算可处理性框架。


<details>
  <summary>Details</summary>
Motivation: 原始校准决策损失(CDL)在离线设置中难以近似计算，需要寻找可处理的替代度量来评估预测器的校准质量。

Method: 通过限制后处理函数为结构化函数族K，定义相对校准决策损失CDL_K，分析其在信息论和计算上的可处理性条件，并为自然函数类建立上下界。

Result: 开发了CDL_K的完整理论框架，证明了在某些结构化函数族下的可计算性，为机器学习中广泛使用的重新校准程序提供了严格保证。

Conclusion: CDL_K为决策理论中的校准问题提供了计算可行的度量方法，同时保持了与原始CDL概念的理论联系。

Abstract: A decision-theoretic characterization of perfect calibration is that an agent seeking to minimize a proper loss in expectation cannot improve their outcome by post-processing a perfectly calibrated predictor. Hu and Wu (FOCS'24) use this to define an approximate calibration measure called calibration decision loss ($\mathsf{CDL}$), which measures the maximal improvement achievable by any post-processing over any proper loss. Unfortunately, $\mathsf{CDL}$ turns out to be intractable to even weakly approximate in the offline setting, given black-box access to the predictions and labels.
  We suggest circumventing this by restricting attention to structured families of post-processing functions $K$. We define the calibration decision loss relative to $K$, denoted $\mathsf{CDL}_K$ where we consider all proper losses but restrict post-processings to a structured family $K$. We develop a comprehensive theory of when $\mathsf{CDL}_K$ is information-theoretically and computationally tractable, and use it to prove both upper and lower bounds for natural classes $K$. In addition to introducing new definitions and algorithmic techniques to the theory of calibration for decision making, our results give rigorous guarantees for some widely used recalibration procedures in machine learning.

</details>


### [282] [Learning stochasticity: a nonparametric framework for intrinsic noise estimation](https://arxiv.org/abs/2511.13701)
*Gianluigi Pillonetto,Alberto Giaretta,Mauro Bisiacco*

Main category: cs.LG

TL;DR: Trine是一个非参数、基于核的框架，用于从时间序列数据中推断状态依赖的内在噪声，通过三阶段算法结合可解析求解的子问题和结构化核架构来捕获噪声驱动的突变波动和状态依赖的方差变化。


<details>
  <summary>Details</summary>
Motivation: 非线性相互作用和随机效应的不完全知识使得自下而上的建模方法往往无效，特别是在基因调控网络和信号通路等复杂系统中，考虑随机效应对于理解动态行为至关重要。

Method: 三阶段算法结合可解析求解的子问题和结构化核架构，能够捕获噪声驱动的突变波动和状态依赖的方差变化，无需依赖预定义的参数假设。

Result: 在生物和生态系统上的验证表明，Trine能够揭示隐藏的动态特性，在多个基准问题上实现了与oracle相当的性能。

Conclusion: Trine框架为理解内在噪声如何影响复杂系统行为开辟了新途径，特别适用于分子浓度或细胞内反应事件中随机波动的直接追踪。

Abstract: Understanding the principles that govern dynamical systems is a central challenge across many scientific domains, including biology and ecology. Incomplete knowledge of nonlinear interactions and stochastic effects often renders bottom-up modeling approaches ineffective, motivating the development of methods that can discover governing equations directly from data. In such contexts, parametric models often struggle without strong prior knowledge, especially when estimating intrinsic noise. Nonetheless, incorporating stochastic effects is often essential for understanding the dynamic behavior of complex systems such as gene regulatory networks and signaling pathways. To address these challenges, we introduce Trine (Three-phase Regression for INtrinsic noisE), a nonparametric, kernel-based framework that infers state-dependent intrinsic noise from time-series data. Trine features a three-stage algorithm that com- bines analytically solvable subproblems with a structured kernel architecture that captures both abrupt noise-driven fluctuations and smooth, state-dependent changes in variance. We validate Trine on biological and ecological systems, demonstrating its ability to uncover hidden dynamics without relying on predefined parametric assumptions. Across several benchmark problems, Trine achieves performance comparable to that of an oracle. Biologically, this oracle can be viewed as an idealized observer capable of directly tracking the random fluctuations in molecular concentrations or reaction events within a cell. The Trine framework thus opens new avenues for understanding how intrinsic noise affects the behavior of complex systems.

</details>


### [283] [ST-ProC: A Graph-Prototypical Framework for Robust Semi-Supervised Travel Mode Identification](https://arxiv.org/abs/2511.13702)
*Luyao Niu,Nuoxian Huang*

Main category: cs.LG

TL;DR: ST-ProC是一个新颖的图原型多目标半监督学习框架，用于解决GPS轨迹出行模式识别中的标签稀缺问题，通过图正则化、原型锚定和边缘感知伪标签策略显著提升性能。


<details>
  <summary>Details</summary>
Motivation: GPS轨迹出行模式识别对城市智能至关重要，但标注成本高昂导致标签严重稀缺。现有的半监督学习方法存在灾难性确认偏差问题，且忽略了数据的内在流形结构。

Method: 提出ST-ProC框架，结合图原型核心与基础半监督学习支持。核心部分通过图正则化、原型锚定和边缘感知伪标签策略利用数据流形并主动拒绝噪声；支持部分通过对比学习和师生一致性损失确保高质量表示和鲁棒优化。

Result: ST-ProC在所有基线方法上均取得显著优势，在真实世界稀疏标签设置中表现优异，相比最先进方法如FixMatch性能提升21.5%。

Conclusion: ST-ProC框架有效解决了出行模式识别中的标签稀缺问题，通过图原型多目标半监督学习方法显著提升了模型性能，为城市智能应用提供了有力工具。

Abstract: Travel mode identification (TMI) from GPS trajectories is critical for urban intelligence, but is hampered by the high cost of annotation, leading to severe label scarcity. Prevailing semi-supervised learning (SSL) methods are ill-suited for this task, as they suffer from catastrophic confirmation bias and ignore the intrinsic data manifold. We propose ST-ProC, a novel graph-prototypical multi-objective SSL framework to address these limitations. Our framework synergizes a graph-prototypical core with foundational SSL Support. The core exploits the data manifold via graph regularization, prototypical anchoring, and a novel, margin-aware pseudo-labeling strategy to actively reject noise. This core is supported and stabilized by foundational contrastive and teacher-student consistency losses, ensuring high-quality representations and robust optimization. ST-ProC outperforms all baselines by a significant margin, demonstrating its efficacy in real-world sparse-label settings, with a performance boost of 21.5% over state-of-the-art methods like FixMatch.

</details>


### [284] [Rare Genomic Subtype Discovery from RNA-seq via Autoencoder Embeddings and Stability-Aware Clustering](https://arxiv.org/abs/2511.13705)
*Alaa Mezghiche*

Main category: cs.LG

TL;DR: 使用自编码器和聚类分析在KIRC癌症中发现了一个罕见但稳定的分子亚型（6.85%患者），而泛癌分析主要受组织来源主导。


<details>
  <summary>Details</summary>
Motivation: 在高维RNA-seq数据上进行无监督学习可以发现超越标准标签的分子亚型，特别是寻找罕见但可重复的基因组亚型。

Method: 结合自编码器表示与聚类和稳定性分析：选择前2000个高变异基因，标准化后训练前馈自编码器（128维潜空间），运行k-means（k=2-10），使用预设发现规则（罕见<10%且Jaccard≥0.60稳定性）识别亚型。

Result: 泛癌分析显示聚类与组织来源高度一致（Cramer's V=0.887），而在KIRC内发现k=5聚类方案，其中罕见簇C0（6.85%）高度稳定（Jaccard=0.787），通过差异表达分析识别出连贯标记物。

Conclusion: 泛癌聚类受组织来源主导，而基于稳定性的癌症内分析方法能够揭示罕见、可重复的KIRC亚型。

Abstract: Unsupervised learning on high-dimensional RNA-seq data can reveal molecular subtypes beyond standard labels. We combine an autoencoder-based representation with clustering and stability analysis to search for rare but reproducible genomic subtypes. On the UCI "Gene Expression Cancer RNA-Seq" dataset (801 samples, 20,531 genes; BRCA, COAD, KIRC, LUAD, PRAD), a pan-cancer analysis shows clusters aligning almost perfectly with tissue of origin (Cramer's V = 0.887), serving as a negative control. We therefore reframe the problem within KIRC (n = 146): we select the top 2,000 highly variable genes, standardize them, train a feed-forward autoencoder (128-dimensional latent space), and run k-means for k = 2-10. While global indices favor small k, scanning k with a pre-specified discovery rule (rare < 10 percent and stable with Jaccard >= 0.60 across 20 seeds after Hungarian alignment) yields a simple solution at k = 5 (silhouette = 0.129, DBI = 2.045) with a rare cluster C0 (6.85 percent of patients) that is highly stable (Jaccard = 0.787). Cluster-vs-rest differential expression (Welch's t-test, Benjamini-Hochberg FDR) identifies coherent markers. Overall, pan-cancer clustering is dominated by tissue of origin, whereas a stability-aware within-cancer approach reveals a rare, reproducible KIRC subtype.

</details>


### [285] [From Black Box to Insight: Explainable AI for Extreme Event Preparedness](https://arxiv.org/abs/2511.13712)
*Kiana Vu,İsmet Selçuk Özer,Phung Lai,Zheng Wu,Thilanka Munasinghe,Jennifer Wei*

Main category: cs.LG

TL;DR: 本文研究可解释AI在极端事件预测中的作用，以野火预测为例，通过SHAP方法揭示模型决策路径和潜在偏差，提升AI系统的可信度和可操作性。


<details>
  <summary>Details</summary>
Motivation: 随着气候变化加剧极端事件频率，需要准确、可解释且可操作的预测系统。当前AI模型虽然预测准确，但黑盒特性限制了其在真实决策中的应用。

Method: 使用野火预测作为案例研究，评估多种AI模型，并采用SHAP方法来揭示关键特征、决策路径和模型行为中的潜在偏差。

Result: 分析表明XAI不仅能澄清模型推理，还支持领域专家和响应团队的关键决策，通过可视化增强特征重要性和时空模式的可解释性。

Conclusion: AI系统不仅需要准确性，还需要可解释性、可访问性和可信度，这对于灾害准备、风险缓解和气候韧性规划至关重要。

Abstract: As climate change accelerates the frequency and severity of extreme events such as wildfires, the need for accurate, explainable, and actionable forecasting becomes increasingly urgent. While artificial intelligence (AI) models have shown promise in predicting such events, their adoption in real-world decision-making remains limited due to their black-box nature, which limits trust, explainability, and operational readiness. This paper investigates the role of explainable AI (XAI) in bridging the gap between predictive accuracy and actionable insight for extreme event forecasting. Using wildfire prediction as a case study, we evaluate various AI models and employ SHapley Additive exPlanations (SHAP) to uncover key features, decision pathways, and potential biases in model behavior. Our analysis demonstrates how XAI not only clarifies model reasoning but also supports critical decision-making by domain experts and response teams. In addition, we provide supporting visualizations that enhance the interpretability of XAI outputs by contextualizing feature importance and temporal patterns in seasonality and geospatial characteristics. This approach enhances the usability of AI explanations for practitioners and policymakers. Our findings highlight the need for AI systems that are not only accurate but also interpretable, accessible, and trustworthy, essential for effective use in disaster preparedness, risk mitigation, and climate resilience planning.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [286] [Enhanced Digitized Adiabatic Quantum Factorization Algorithm Using Null-Space Encoding](https://arxiv.org/abs/2511.11747)
*Felip Pellicer*

Main category: quant-ph

TL;DR: 提出了一种基于QAOA的改进整数分解协议，通过简化哈密顿量为仅包含双体相互作用，显著降低了实验复杂度，在8量子比特问题实例中表现出比标准协议更好或相当的性能。


<details>
  <summary>Details</summary>
Motivation: 整数分解是网络安全的基础问题，Shor算法需要通用量子计算机，而近期设备需要替代方法。现有绝热分解算法及其数字化版本存在高阶多体相互作用难以实现的困难。

Method: 改进的QAOA分解协议，将相互作用哈密顿量简化为仅包含双体项，减少了量子资源需求并加速收敛。

Result: 数值模拟显示该方法在8量子比特问题实例中达到比标准协议相当或更高的保真度，需要更少量子资源且收敛更快。

Conclusion: 提出的简化哈密顿量方法在保持性能的同时显著降低了实验复杂度，为NISQ时代的整数分解提供了可行路径。

Abstract: Integer factorization is a computational problem of fundamental importance in cybersecurity and secure communications, as its difficulty form the basis of modern public-key cryptography. While Shor's algorithm can solve this problem efficiently on a universal quantum computer, near-term devices require alternative approaches. The Adiabatic Factorization Algorithm and its digitized counterparts offer a promising NISQ-era pathway but suffer from high-order many-body interactions that are difficult to implement. In this work, we propose a modified QAOA-based factorization protocol that simplifies the interacting Hamiltonian to include only two-body terms, significantly reducing its experimental complexity. Numerical simulations show that this method achieves comparable or higher fidelities than the standard protocol, while requiring fewer quantum resources and converging more rapidly for problem instances up to eight qubits. We analyze the characteristic fidelity behavior introduced by the Hamiltonian modification. Additionally, we report on simulations with alternative cost-function definitions that frequently yielded improved performance.

</details>


### [287] [2025 Quantum Diamond Workshop Findings Report](https://arxiv.org/abs/2511.11791)
*Danielle A. Braje,Matthew L. Markham,Jennifer M. Schloss,Michael A. Slocum,Ronald L. Walsworth*

Main category: quant-ph

TL;DR: 2025年5月在华盛顿特区举办的量子金刚石技术研讨会报告，汇集了研究人员、行业代表和政府利益相关者，评估了该技术最有前景的应用场景、关键挑战以及未来发展路径。


<details>
  <summary>Details</summary>
Motivation: 评估量子金刚石技术的当前状态和未来方向，识别限制应用的关键技术和结构挑战，为政府、行业和学术界的努力提供指导，加速该技术的成熟和商业化。

Method: 通过为期两天的研讨会，包括技术报告和公开讨论，探索近期示范项目和长期基础设施需求，强调材料供应商、设备工程师和最终用户之间的协调作用。

Result: 形成了跨领域主题、挑战和战略行动的综合见解，为量子金刚石技术的成熟和商业化提供了指导框架。

Conclusion: 量子金刚石技术的发展需要政府、行业和学术界的协调努力，通过识别关键挑战和制定战略路径，可以加速该技术的商业应用和基础设施发展。

Abstract: This report synthesizes the outcomes of a two-day workshop held in Washington, D.C. in May, 2025 that convened researchers, industry representatives, and government stakeholders to examine the current state and future directions of quantum diamond technologies. The workshop's goals were to assess the most promising use cases, to identify the key technical and structural challenges limiting adoption, and to chart potential pathways for aligning application needs with diamond material and device development. Through a series of technical presentations and open discussions, participants explored both near-term demonstrations and long-term infrastructure needs, highlighting the critical role of coordination between material suppliers, device engineers, and end users. The goal of this report is to distill those insights into a coherent set of cross-cutting themes, challenges, and strategic actions that can guide government, industry, and academic efforts to accelerate the maturation and commercialization of quantum diamond technologies.

</details>


### [288] [Sample-based training of quantum generative models](https://arxiv.org/abs/2511.11802)
*Maria Demidik,Cenk Tüysüz,Michele Grossi,Karl Jansen*

Main category: quant-ph

TL;DR: 提出了一种基于对比散度的量子生成模型训练框架，通过设计特定电路结构实现参数更新，相比传统方法显著减少了样本需求，实现了可扩展的量子硬件训练。


<details>
  <summary>Details</summary>
Motivation: 量子计算机能够高效采样经典难解的概率分布，但现有量子生成模型的训练面临挑战：参数平移规则导致梯度计算随参数数量线性增长，且需要重复的期望值估计，存在有限采样噪声问题。

Method: 将对比散度原理扩展到量子模型，推导电路结构并提供通用构造方法，获得能够生成参数更新所需样本的量子电路，实现与正向传播成本成常数比例的可扩展训练。

Result: 数值结果显示，该方法在达到与基于似然优化相当精度的同时，所需样本数量显著减少。

Conclusion: 该框架为在量子硬件上直接训练表达性量子生成模型建立了可扩展的路径。

Abstract: Quantum computers can efficiently sample from probability distributions that are believed to be classically intractable, providing a foundation for quantum generative modeling. However, practical training of such models remains challenging, as gradient evaluation via the parameter-shift rule scales linearly with the number of parameters and requires repeated expectation-value estimation under finite-shot noise. We introduce a training framework that extends the principle of contrastive divergence to quantum models. By deriving the circuit structure and providing a general recipe for constructing it, we obtain quantum circuits that generate the samples required for parameter updates, yielding constant scaling with respect to the cost of a forward pass, analogous to backpropagation in classical neural networks. Numerical results demonstrate that it attains comparable accuracy to likelihood-based optimization while requiring substantially fewer samples. The framework thereby establishes a scalable route to training expressive quantum generative models directly on quantum hardware.

</details>


### [289] [Relativistic Maxwell-Bloch Equations with Applications to Astrophysics](https://arxiv.org/abs/2511.11861)
*Ningyan Fang,Martin Houde,Fereshteh Rajabi,Victor Botez*

Main category: quant-ph

TL;DR: 本文推导了相对论性麦克斯韦-布洛赫方程，应用于天文环境中的辐射过程，包括脉泽作用和迪克超辐射，验证了辐射系统响应在不同参考系中的不变性。


<details>
  <summary>Details</summary>
Motivation: 研究天文环境中各种辐射过程的相对论效应，特别是脉泽作用和迪克超辐射在不同相对速度参考系中的行为。

Method: 推导相对论性麦克斯韦-布洛赫方程，分析辐射系统在不同相对速度参考系中的响应特性。

Result: 辐射系统响应在不同相对速度参考系中保持不变，而相关时间尺度和辐射强度按相对论预期变换；不同速度发射体之间的相干性在所有参考系中不变；推导了稳态条件下的相对论性脉泽方程。

Conclusion: 相对论性麦克斯韦-布洛赫方程成功描述了天文辐射过程的相对论效应，验证了关键物理量在不同参考系中的变换规律。

Abstract: We derive relativistic Maxwell-Bloch equations for potential applications in astronomical environments, where various radiative processes are known to occur, including the maser action and Dicke's superradiance. We show that for both phenomena a radiating system's response is preserved at different relative velocities between the system's rest frame and the observer, while the relevant timescales and the radiation intensity transform as expected from relativistic considerations. We verify that the level of coherence between groups of emitters travelling at different speeds is unchanged in all reference frames. We also derive relativistic versions of the maser equations applicable in the steady-state regime.

</details>


### [290] [Compact cavity-dressed Hamiltonian framework at arbitrarily strong light-matter coupling](https://arxiv.org/abs/2511.11903)
*Jakub Garwoła,Dvira Segal*

Main category: quant-ph

TL;DR: 提出了一种非微扰哈密顿映射方法，用于强耦合到量子化场模式的量子系统，通过纠缠变换构建紧凑的腔-物质混合系统表示。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理强耦合光-物质系统时面临挑战，特别是在共振和超强耦合区域，需要开发更高效的非微扰计算方法。

Method: 基于光子和原子自由度的纠缠变换，构建腔-物质哈密顿量(CDH)，通过截断激发扇区来构建收敛的紧凑模型。

Result: 在量子Rabi模型上验证了该框架，在弱耦合和强耦合区域都能准确预测光谱，并成功研究了Dicke-Heisenberg晶格模型的相图。

Conclusion: CDH框架提供了紧凑的闭式表示，既增强了物理洞察力又提高了计算效率，便于研究强耦合混合光-物质系统。

Abstract: We present a non-perturbative Hamiltonian mapping method for quantum systems strongly coupled to a quantized field mode (cavity), yielding compact closed-form representations of hybrid light-matter systems. The mapping method builds on an entangling transformation of photonic and atomic degrees of freedom. By truncating the resulting cavity-dressed Hamiltonian (CDH) to successively larger excitation sectors, we construct a series of compact models that converge to the exact limit, outpacing conventional approaches even in the challenging resonant and ultrastrong light-matter regime. The mapping principle also applies to multimode cavities coupled to matter through noncommuting operators and to leaky cavities. We benchmark the CDH framework on the quantum Rabi model, demonstrating accurate spectral predictions in both weak and strong coupling regimes, together with converging ground-state and thermal observables. We study the Dicke-Heisenberg lattice model and determine its phase diagram under resonant and strong light-matter coupling, achieving significant computational savings over brute-force simulations and identifying cavity-mediated spin correlations both analytically and numerically. The closed-form and compactness of the CDH provide both physical insight and enhanced computational efficiency, facilitating studies of strongly coupled hybrid light-matter systems.

</details>


### [291] [Error-Mitigation Enabled Multicomponent Quantum Simulations Beyond the Born-Oppenheimer Approximation](https://arxiv.org/abs/2511.11941)
*Delmar G. A. Cabral,Brandon Allen,Fabijan Pavošević,Sharon Hammes-Schiffer,Pablo Díez-Valle,Jack S. Baker,Gaurav Saxena,Thi Ha Kyaw,Victor S. Batista*

Main category: quant-ph

TL;DR: 提出了多组分幺正耦合簇框架，用于模拟包含电子和核量子效应的分子系统，并在量子硬件上首次实现了误差缓解的多组分相关模拟。


<details>
  <summary>Details</summary>
Motivation: 开发能够超越玻恩-奥本海默近似、同时处理电子和核量子效应的量子模拟方法，为统一电子和核自由度的可扩展算法铺平道路。

Method: 采用核-电子轨道形式，构建mcUCC ansätze；使用局部幺正簇Jastrow ansatz降低资源成本；在IBM Q的Heron超导硬件上实验实现；应用物理启发外推误差缓解协议。

Result: 计算得到的基态能量保持在化学精度范围内，与所述不确定性水平一致，首次在量子硬件上实现了误差缓解的多组分相关模拟。

Conclusion: 这些结果为统一电子和核自由度的可扩展算法提供了路径，展示了在量子硬件上进行误差缓解多组分相关模拟的可行性。

Abstract: We introduce a multicomponent unitary coupled cluster framework for quantum simulations of molecular systems that incorporate both electronic and nuclear quantum effects beyond the Born-Oppenheimer approximation. Using the nuclear-electronic orbital formalism, we construct mcUCC ansätze for positronium hydride and molecular hydrogen with a quantum proton, and analyze hardware requirements for different excitation truncations. To further reduce resource costs effectively, we employ the local unitary cluster Jastrow ansatz and implement it experimentally on IBM Q's Heron superconducting hardware. With the Physics-Inspired Extrapolation error mitigation protocol, the computed ground-state energies remain within chemical accuracy, consistent with the stated uncertainty level. These results provide the first demonstration of error-mitigated multicomponent correlated simulations on quantum hardware and outline a path toward scalable algorithms unifying electronic and nuclear degrees of freedom.

</details>


### [292] [Skyrmionic qubits stabilized by Dzyaloshinskii-Moriya interaction as platforms for qubits and quantum gates](https://arxiv.org/abs/2511.12250)
*Doru Sticlet,Romulus Tetean,Coriolan Tiusan*

Main category: quant-ph

TL;DR: 提出基于斯格明子态实现量子比特的框架，利用DMI在二维自旋晶格中稳定量子斯格明子相，并实现量子逻辑门操作，但DMI同时导致退相干问题。


<details>
  <summary>Details</summary>
Motivation: 量子计算需要利用量子现象如叠加和纠缠，传统比特处理方式存在局限，需要探索新的量子比特实现方案。

Method: 在二维自旋晶格中引入DMI、交换相互作用、垂直磁各向异性和塞曼耦合，通过精确对角化求解周期和开放边界条件，分析量子斯格明子相和经典斯格明子。

Result: 周期边界条件下出现量子斯格明子相，开放边界条件下形成拓扑保护的经典斯格明子；量子斯格明子受DMI驱动退相干影响门保真度，经典斯格明子保持稳定。

Conclusion: 斯格明子态是有前景的量子比特候选方案，但DMI在稳定斯格明子的同时也会在门操作中引起退相干。

Abstract: Quantum computation departs from the classical paradigm of deterministic, bit-based processing by exploiting inherently quantum phenomena such as superposition and entanglement. We propose a framework for qubit realization based on skyrmionic states stabilized by the Dzyaloshinskii-Moriya interaction (DMI) in two-dimensional spin lattices. The model incorporates competing exchange interactions, perpendicular magnetic anisotropy, and Zeeman coupling, solved via exact diagonalization under periodic (PBC) and open boundary conditions (OBC). A quantum skyrmionic phase emerges for PBC within a parameter space defined by DMI, exchange, field, and anisotropy, while OBC favor classical-like, topologically protected skyrmions. Quantum logic gates (Pauli X, Y, Z, Hadamard) are implemented on both skyrmion types. Energy density and entanglement entropy analyses reveal that quantum skyrmions suffer from DMI-driven decoherence and reduced gate fidelity, whereas classical-like skyrmions maintain stability. Exact simulations of qubit dynamics, including drive effects and Lindblad decoherence, demonstrate tunable anharmonic energy levels and coherent Bloch-sphere manipulation, making these skyrmionic states promising candidates for qubit implementation. Overall, the Dzyaloshinskii-Moriya interaction plays a dual role-stabilizing skyrmionic qubits while simultaneously inducing decoherence during gate operations.

</details>


### [293] [Emergent synchronization mode in coupled Rydberg atomic chains](https://arxiv.org/abs/2511.11987)
*Weilun Jiang*

Main category: quant-ph

TL;DR: 在耦合耗散里德堡原子链中发现新型振荡模式，具有π相位差特征，区别于传统反铁磁同步。理论揭示了共存连续时间晶体相，属于Hopf和pitchfork分岔。结论可推广到多链系统。


<details>
  <summary>Details</summary>
Motivation: 探索耦合耗散系统中新型同步模式，超越传统反铁磁型同步，揭示更丰富的动力学行为。

Method: 通过调制耦合里德堡原子链间距，理论分析发现共存连续时间晶体相，识别Hopf和pitchfork分岔类型。

Result: 发现具有π相位差的新型振荡模式，验证了该同步模式的唯一性，并讨论了实验可行性。

Conclusion: 成功揭示了一种新型同步振荡模式，扩展了对耦合耗散系统动力学行为的理解，为多链系统研究提供了新视角。

Abstract: We report a new oscillatory form in the two coupled dissipative Rydberg atomic chains by modulating its spacing. Such oscillation has $π$-phase difference between two neighboring sites, which distinguishes itself from antiferromagnetic-type synchronization in the previous studies. Theoretically, we find a phase with coexisting two types of continuous time crystals, and recognize that the transition belongs to Hopf and pitchfork bifurcation. Furthermore, we generalize the conclusion to multiple chains and verify the uniqueness of the new synchronization mode. We also discuss its experimental feasibility.

</details>


### [294] [Quantum Orthogonal Separable Physics-Informed Neural Networks](https://arxiv.org/abs/2511.12613)
*Pietro Zanotta,Ljubomir Budinski,Caglar Aytekin,Valtteri Lahtinen*

Main category: quant-ph

TL;DR: 提出量子正交可分离物理信息神经网络(QO-SPINNs)，结合量子计算原理解决PDE问题，通过量子算法加速矩阵乘法，并开发了首个针对可分离PINNs的不确定性量化方法。


<details>
  <summary>Details</summary>
Motivation: 解决经典方法在求解偏微分方程时的计算瓶颈问题，利用量子计算原理提升计算效率，并为可分离PINNs开发专门的不确定性量化框架。

Method: 使用保持汉明权重的量子电路和单热基数据编码，构建量子正交SPINN架构，通过堆叠方式实现不确定性量化，无需计算昂贵的谱归一化步骤。

Result: 实现了O(d log d/ε²)的复杂度，相比经典O(d²)有显著提升，数值结果验证了理论主张和方法的有效性。

Conclusion: QO-SPINNs为求解PDE问题提供了高效的计算框架，并首次为可分离PINNs提供了鲁棒的不确定性量化方法，在计算效率和理论分析方面均有重要贡献。

Abstract: This paper introduces Quantum Orthogonal Separable Physics-Informed Neural Networks (QO-SPINNs), a novel architecture for solving Partial Differential Equations, integrating quantum computing principles to address the computational bottlenecks of classical methods. We leverage a quantum algorithm for accelerating matrix multiplication within each layer, achieving a $\mathcal O(d\log d/ε^2)$ complexity, a significant improvement over the classical $\mathcal O(d^2)$ complexity, where $d$ is the dimension of the matrix, $ε$ the accuracy level. This is accomplished by using a Hamming weight-preserving quantum circuit and a unary basis for data encoding, with a comprehensive theoretical analysis of the overall architecture provided. We demonstrate the practical utility of our model by applying it to solve both forward and inverse PDE problems. Furthermore, we exploit the inherent orthogonality of our quantum circuits (which guarantees a spectral norm of 1) to develop a novel uncertainty quantification method. Our approach adapts the Spectral Normalized Gaussian Process for SPINNs, eliminating the need for the computationally expensive spectral normalization step. By using a Quantum Orthogonal SPINN architecture based on stacking, we provide a robust and efficient framework for uncertainty quantification (UQ) which, to our knowledge, is the first UQ method specifically designed for Separable PINNs. Numerical results based on classical simulation of the quantum circuits, are presented to validate the theoretical claims and demonstrate the efficacy of the proposed method.

</details>


### [295] [Measurement-Based Quantum Computation Using the Spin-1 XXZ Model with Uniaxial Anisotropy](https://arxiv.org/abs/2511.12000)
*Hiroki Ohta,Aaron Merlin Müller,Shunji Tsuchiya*

Main category: quant-ph

TL;DR: 自旋-1 XXZ链在Haldane相中的基态可作为基于测量的量子计算的资源态，实现单量子比特门，保真度超过0.99。


<details>
  <summary>Details</summary>
Motivation: 探索自旋-1 XXZ链在Haldane相中的基态作为量子计算资源态的潜力，特别是在实现高保真度单量子比特门方面的应用。

Method: 通过调节单离子各向异性D和Ising各向异性J，评估基本旋转门和一般单量子比特门的门保真度，并推导旋转门保真度的解析表达式。

Result: 当适当调节D或J时，门保真度超过0.99。在反铁磁相附近增强的反铁磁关联有效抑制了失败态，从而提高了门保真度。

Conclusion: 自旋-1 XXZ链在Haldane相中的基态是量子计算的可行资源态，能够实现高保真度的单量子比特门操作。

Abstract: We demonstrate that the ground state of a spin-1 XXZ chain with uniaxial anisotropies, single-ion anisotropy $D$ and Ising anisotropy $J$, within the Haldane phase can serve as a resource state for measurement-based quantum computation implementing single-qubit gates. The gate fidelity of both elementary rotation gates and general single-qubit unitary gates composed of rotations about the $x$-, $y$-, and $z$-axes is evaluated, and is found to exceed 0.99 when $D$ or $J$ is appropriately tuned. Furthermore, we derive an analytic expression for the rotation-gate fidelity under the assumption that the state lies within the $\mathbb Z_2\times\mathbb Z_2$-protected Haldane phase, showing that it is determined by the post-measurement spin-spin correlation function and the failure probability. The observed enhancement of gate fidelity in the spin-1 XXZ chain originates from the strengthening of antiferromagnetic (AFM) correlations near the AFM phase, which effectively suppresses failure states.

</details>


### [296] [A Global Spacetime Optimization Approach to the Real-Space Time-Dependent Schrödinger Equation](https://arxiv.org/abs/2511.12983)
*Enze Hou,Yuzhi Liu,Lei Wang,Han Wang*

Main category: quant-ph

TL;DR: 提出了一个用于求解实空间含时薛定谔方程的通用神经网络框架FAST-Net，将时间作为显式输入，能够统一表示复杂费米子系统的反对称波函数。


<details>
  <summary>Details</summary>
Motivation: 解决复杂费米子系统含时薛定谔方程的挑战，特别是捕获时间演化的多体关联和费米子波函数的反对称性质。

Method: 使用费米子反对称时空网络(FAST-Net)，将时间与空间坐标一起作为输入，将TDSE表述为全局优化问题，避免逐步传播并支持高度并行化训练。

Result: 在四个基准问题上展示了优异性能：1D谐振子、时变谐振势中的相互作用费米子、3D氢原子轨道动力学和激光驱动的H2分子，与参考解高度一致。

Conclusion: 该框架为传统基组依赖或平均场方法提供了高度表达性的替代方案，为含时量子系统的从头算模拟开辟了新可能性。

Abstract: The time-dependent Schrödinger equation (TDSE) in real space is fundamental to understanding the dynamics of many-electron quantum systems, with applications ranging from quantum chemistry to condensed matter physics and materials science. However, solving the TDSE for complex fermionic systems remains a significant challenge, particularly due to the need to capture the time-evolving many-body correlations, while the antisymmetric nature of fermionic wavefunctions complicates the function space in which these solutions must be represented. We propose a general-purpose neural network framework for solving the real-space TDSE, Fermionic Antisymmetric Spatio-Temporal Network, which treats time as an explicit input alongside spatial coordinates, enabling a unified spatiotemporal representation of complex, antisymmetric wavefunctions for fermionic systems. This approach formulates the TDSE as a global optimization problem, avoiding step-by-step propagation and supporting highly parallelizable training. The method is demonstrated on four benchmark problems: a 1D harmonic oscillator, interacting fermions in a time-dependent harmonic trap, 3D hydrogen orbital dynamics, and a laser-driven H$_2$ molecule, achieving excellent agreement with reference solutions across all cases. These results confirm our method's scalability, accuracy, and flexibility across various dimensions and interaction regimes, while demonstrating its ability to accurately simulate long-time dynamics in complex systems. Our framework offers a highly expressive alternative to traditional basis-dependent or mean-field methods, opening new possibilities for ab initio simulations of time-dependent quantum systems, with applications in quantum dynamics, molecular control, and ultrafast spectroscopy.

</details>


### [297] [Quantum Amplitude-Amplification Eigensolver: A State-Learning-Assisted Approach beyond Energy-Gradient-Based Heuristics](https://arxiv.org/abs/2511.12062)
*Kyunghyun Baek,Seungjin Lee,Joonsuk Huh,Dongkeun Lee,Jinhyoung Lee,M. S. Kim,Jeongho Bang*

Main category: quant-ph

TL;DR: QAAE是一种基于量子振幅放大的基态估计算法，通过相干驱动试验态向基态演化，避免了变分方法中的能量梯度计算问题。


<details>
  <summary>Details</summary>
Motivation: 传统变分量子本征求解器面临问题特定能量景观的挑战，需要开发不依赖变分优化的基态估计方法。

Method: 通过量子振幅放大技术，在每轮中交替进行对试验态的反射和归一化哈密顿量的受控短时演化，通过辅助量子比特读取获得振幅放大的纯目标态，然后通过状态学习步骤重新编码到ansatz电路中。

Result: 在标准假设下，基态重叠度每轮单调增加，算法收敛。实验验证了在IBMQ处理器上的放大机制，数值基准测试显示QAAE在精度和稳定性上可以超越基于梯度的VQE。

Conclusion: QAAE为近期限子模拟提供了一条无变分且硬件兼容的基态估计路径。

Abstract: Ground-state estimation lies at the heart of a broad range of quantum simulations. Most near-term approaches are cast as variational energy minimization and thus inherit the challenges of problem-specific energy landscapes. We develop the quantum amplitude-amplification eigensolver (QAAE), which departs from the variational paradigm and instead coherently drives a trial state toward the ground state via quantum amplitude amplification. Each amplitude-amplification round interleaves a reflection about the learned trial state with a controlled short-time evolution under a normalized Hamiltonian; an ancilla readout yields an amplitude-amplified pure target state that a state-learning step then re-encodes into an ansatz circuit for the next round -- without evaluating the energy gradients. Under standard assumptions (normalized $\hat{H}$, a nondegenerate ground-state, and a learning update), the ground-state overlap increases monotonically per round and the procedure converges; here, a per-round depth bound in terms of the ansatz depth and Hamiltonian-simulation cost establishes hardware compatibility. Cloud experiments on IBMQ processor verify our amplification mechanism on a two-level Hamiltonian and a two-qubit Ising model, and numerical benchmarks on $\mathrm{H}_2$, $\mathrm{LiH}$, and a $10$-qubit longitudinal-and-transverse-field Ising model show that QAAE integrates with chemistry-inspired and hardware-efficient circuits and can surpass gradient-based VQE in accuracy and stability. These results position QAAE as a variational-free and hardware-compatible route to ground-state estimation for near-term quantum simulation.

</details>


### [298] [Enhanced Nonreciprocal Quantum Battery Performance via Nonlinear Two-Photon Driving](https://arxiv.org/abs/2511.12118)
*Luxin Xu,Changliang Ren*

Main category: quant-ph

TL;DR: 提出了一种非线性双光子驱动的量子电池模型，具有非互易动力学特性，通过环境工程实现高效单向充电机制


<details>
  <summary>Details</summary>
Motivation: 量子电池作为高效的量子能量存储设备受到广泛关注，需要开发更高效的充电机制

Method: 使用马尔可夫主方程方法，推导系统动力学的解析解，识别动态平衡所需的参数范围

Result: 增加驱动强度可提高能量转换和存储效率，但会延长平衡时间；与单光子驱动相比，双光子过程在能量容量和熵调控方面具有明显优势

Conclusion: 该模型在实验上可行，可在多种量子平台实现，包括光子系统、超导电路和磁子器件

Abstract: Quantum batteries have attracted significant attention as efficient quantum energy storage devices.In this work, we propose a nonlinear two-photon driving quantum battery model featuring nonreciprocal dynamics that enables a highly efficient unidirectional charging mechanism through environmental engineering. Using a Markovian master-equation approach, we derive analytical solutions for the system dynamics and identify the parameter regime required for dynamical equilibration. Our results reveal that increasing the driving strength enhances both energy conversion and storage efficiency, albeit at the cost of longer equilibration times. Compared with single-photon driving, the two-photon process exhibits a pronounced advantage in energy capacity and entropy regulation, which becomes more prominent under stronger driving. Under asymmetric dissipation, optimizing the system-bath coupling can further improve performance. The proposed model is experimentally feasible and can be implemented across multiple quantum platforms, including photonic systems, superconducting circuits, and magnonic devices.

</details>


### [299] [Application of optical squeezing to microresonator based optical sensors](https://arxiv.org/abs/2511.12138)
*Dariya Salykina,Daniil Shakhbaziants,Igor Bilenko,Farid Khalili*

Main category: quant-ph

TL;DR: 该论文展示了通过使用压缩量子态作为探测光，可以在高Q光学微谐振器传感器中超越散粒噪声极限，实现量子增强的传感灵敏度。


<details>
  <summary>Details</summary>
Motivation: 探索高Q光学微谐振器传感器的量子灵敏度极限，利用其高能量浓度和低损耗特性，通过量子态制备来突破经典传感的散粒噪声限制。

Method: 使用压缩量子态作为探测光，在高Q光学微谐振器中进行传感测量，并利用额外的腔内压缩来减少光学损耗的影响。

Result: 实验证明通过压缩量子态可以超越散粒噪声极限，灵敏度仅受光学损耗和可用压缩度的限制。

Conclusion: 量子压缩技术能够显著提升光学微谐振器传感器的灵敏度，为实现量子增强传感提供了可行路径。

Abstract: High-Q optical microresonators combine low losses and high optical energy concentration in a small effective mode volume, making them an attractive platform for optical sensors. While light is confined in the microresonator by total internal reflection, a portion of the optical field, known as the evanescent field, extends outside. This makes the mode's resonant frequency sensitive to changes in the surrounding environment.
  In this work, we explore the quantum sensitivity limits of this type of sensors. We demonstrate that by preparing the probe light in a squeezed quantum state, it is possible to surpass the shot-noise limit. The resulting sensitivity is constrained only by optical losses and the available degree of squeezing. The influence of the losses can be reduced using additional squeezing of the light inside the microresonator.

</details>


### [300] [Stochastic Shadow Descent: Training Parametrized Quantum Circuits with Shadows of Gradients](https://arxiv.org/abs/2511.12168)
*Sayantan Pramanik,M Girish Chandra*

Main category: quant-ph

TL;DR: 提出了一种名为随机阴影下降(SSD)的新算法，用于优化参数化量子电路的参数，解决了现有方法如SPSA中存在的方向导数估计偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有的参数优化算法如SPSA虽然每轮迭代只需两次电路执行，但它们使用中心差分计算方向导数会产生偏差估计，这可能导致参数化量子电路训练的不稳定性。

Method: SSD算法使用随机投影(阴影)来迭代更新参数，通过参数移位规则和量子信号处理技术构建量子电路，以无偏的方式计算方向导数估计。

Result: 理论证明了SSD算法的收敛性，提供了最坏情况下的迭代次数界限，并通过数值实验证明了其有效性。

Conclusion: SSD算法能够提供无偏的方向导数估计，解决了现有方法的偏差问题，在参数化量子电路优化中表现出更好的稳定性和性能。

Abstract: In this paper, we focus on the task of optimizing the parameters in Parametrized Quantum Circuits (PQCs). While popular algorithms, such as Simultaneous Perturbation Stochastic Approximation (SPSA), limit the number of circuit-execution to two per iteration, irrespective of the number of parameters in the circuit, they have their own challenges. These methods use central-differences to calculate biased estimates of directional derivatives. We show, both theoretically and numerically, that this may lead to instabilities in \emph{training} the PQCs. To remedy this, we propose Stochastic Shadow Descent (\texttt{SSD}), which uses random-projections (or \emph{shadows}) of the gradient to update the parameters iteratively. We eliminate the bias in directional derivatives by employing the Parameter-Shift Rule, along with techniques from Quantum Signal Processing, to construct a quantum circuit that parsimoniously computes \emph{unbiased estimates} of directional derivatives. Finally, we prove the convergence of the \texttt{SSD} algorithm, provide worst-case bounds on the number of iterations, and numerically demonstrate its efficacy.

</details>


### [301] [Reinforcement Learning for Charging Optimization of Inhomogeneous Dicke Quantum Batteries](https://arxiv.org/abs/2511.12176)
*Xiaobin Song,Siyuan Bai,Da-Wei Wang,Hanxiao Tao,Xizhe Wang,Rebing Wu,Benben Jiang*

Main category: quant-ph

TL;DR: 使用强化学习优化非均匀Dicke量子电池的分段恒流充电策略，在四种可观测性条件下比较性能。结果表明，在部分可观测条件下，加入二阶关联信息可恢复94%-98%的全状态基准性能。


<details>
  <summary>Details</summary>
Motivation: 量子电池充电优化面临非均匀性和部分可观测性的挑战，需要开发在实际信息约束下的有效快速充电协议。

Method: 采用强化学习方法优化非均匀Dicke电池的分段恒流充电策略，系统比较了从全状态访问到实验可观测量的四种可观测性机制。

Result: 全可观测性可获得接近最优的功提取能力且变异性低；部分可观测条件下，仅使用单TLS能量或能量加一阶平均值的性能落后于全状态基准，但加入二阶关联信息后可恢复大部分差距。

Conclusion: 研究结果强调了在实际信息约束下实现有效快速充电协议的实用路径，学习到的充电策略具有非近视特性，通过暂时性平台期或下降换取更优的终端结果。

Abstract: Charging optimization is a key challenge to the implementation of quantum batteries, particularly under inhomogeneity and partial observability. This paper employs reinforcement learning to optimize piecewise-constant charging policies for an inhomogeneous Dicke battery. We systematically compare policies across four observability regimes, from full-state access to experimentally accessible observables (energies of individual two-level systems (TLSs), first-order averages, and second-order correlations). Simulation results demonstrate that full observability yields near-optimal ergotropy with low variability, while under partial observability, access to only single-TLS energies or energies plus first-order averages lags behind the fully observed baseline. However, augmenting partial observations with second-order correlations recovers most of the gap, reaching 94%-98% of the full-state baseline. The learned schedules are nonmyopic, trading temporary plateaus or declines for superior terminal outcomes. These findings highlight a practical route to effective fast-charging protocols under realistic information constraints.

</details>


### [302] [Channel-Constrained Markovian Quantum Diffusion Model from Open System Perspective](https://arxiv.org/abs/2511.12221)
*Qin-Sheng Zhu,Geng Chen,Lian-Hui Yu,Xiaodong Xing,Xiao-Yu Li*

Main category: quant-ph

TL;DR: 提出了一种通道约束的马尔可夫量子扩散模型，将量子态生成过程建模为开放量子系统动力学，通过量子主方程描述正向扩散过程，通过学习逆量子通道实现反向去噪。


<details>
  <summary>Details</summary>
Motivation: 探索如何将环境相互作用不仅视为退相干源，还能用于实现高保真量子态合成，将量子扩散表征为受控马尔可夫演化。

Method: 建立通道约束框架：将扩散和去噪步骤建模为由Kraus算子定义的量子通道，通过在Stiefel流形上的优化确保物理有效性，并引入定制训练策略和损失函数。

Result: 在单量子比特到7量子比特纠缠态的系统上实验验证，在随机和退极化噪声条件下均实现了超过0.998的高保真度状态生成。

Conclusion: 量子扩散可以表征为受控马尔可夫演化，环境相互作用不仅能导致退相干，还能用于实现高保真量子态合成。

Abstract: We present a channel-constrained Markovian quantum diffusion (CCMQD) model that prepares quantum states by rigorously framing the generative process within the dynamics of open quantum systems. Our model interprets the forward diffusion process as natural decoherence using quantum master equations, whereas the reverse denoising is achieved by learning inverse quantum channels. Our core innovation is a comprehensive channel-constrained framework: we model the diffusion and denoising steps as quantum channels defined by Kraus operators, ensure their physical validity through optimization on the Stiefel manifold, and introduce tailored training strategies and loss functions that leverage this constrained structure for high-fidelity state reconstruction. Experimental validation on systems ranging from single qubits to entangled states $7$ -qubits demonstrates high-fidelity state generation, achieving fidelities exceeding $0.998$ under both random and depolarizing noise conditions. This work confirms that quantum diffusion can be characterized as a controlled Markov evolution, demonstrating that environmental interactions are not limited to being a source of decoherence but can also be utilized to achieve high-fidelity quantum state synthesis.

</details>


### [303] [Scalable quantum error mitigation with phase-cycled dynamical decoupling](https://arxiv.org/abs/2511.12227)
*Weibin Ni,Zhijie Li,Guanyu Qu,Zhecheng Sun,Jiale Dai,Fazhan Shi,Lei Sun*

Main category: quant-ph

TL;DR: 提出Hadamard相位循环作为非马尔可夫量子误差缓解方法，用于解决动力学解耦中的控制误差问题，能准确获取退相干时间并保持量子态保真度。


<details>
  <summary>Details</summary>
Motivation: 在NISQ时代，量子比特退相干和控制误差严重制约量子技术发展。动力学解耦虽能抑制退相干误差，但对控制误差敏感，导致退相干时间被高估。

Method: 利用群结构设计等效系综量子电路的相位配置，通过Hadamard相位循环消除错误动力学产生的电路输出，该方法与电路深度呈线性缩放。

Result: 在固态电子自旋量子比特、金刚石氮空位中心、单囚禁离子和超导transmon量子比特上验证，能准确获取退相干时间并有效保持态保真度。

Conclusion: 可扩展的量子误差缓解与抑制相结合，将促进含噪声量子比特和控制硬件的量子技术发展。

Abstract: The realization of quantum technologies in the Noisy Intermediate-Scale Quantum era is severely constrained by qubit decoherence and control errors, presenting fundamental challenges to achieving quantum advantages. Dynamical decoupling is a widely used, powerful technique for decoherence error suppression. However, it is susceptible to control errors, making non-robust sequences like UDD impractical to implement and robust ones like CPMG to significantly overestimate decoherence times. This overestimation issue remains largely unexplored in the past few decades, leading to many reports of exceptionally long yet plausible decoherence times across various qubit platforms. Here, we construct Hadamard phase cycling as a non-Markovian quantum error mitigation method for dynamical decoupling. This method exploits group structure to design phase configurations of equivalent ensemble quantum circuits, effectively eliminates circuit outputs generated from erroneous dynamics, and scales linearly with circuit depth. Harnessing its error mitigation capability for ensemble solid-state electron spin qubits embedded in paramagnetic molecules and nitrogen-vacancy centers in diamond enables accurate acquisition of decoherence times. Applying Hadamard phase cycling on single trapped ion and superconducting transmon qubits effectively preserves their state fidelity during dynamical decoupling. The integration of scalable quantum error mitigation and suppression would facilitate the development of quantum technologies with noisy qubits and control hardware.

</details>


### [304] [Survival of Hermitian Criticality in the Non-Hermitian Framework](https://arxiv.org/abs/2511.12246)
*Fei Wang,Guoying Liang,Zecheng Zhao,Lin-Yue Luo,Da-Jian Zhang,Bao-Ming Xu*

Main category: quant-ph

TL;DR: 研究一维各向异性XY模型在复值横向场中的多体相变，发现非厄米系统中仍保持厄米模型的相变特征，这源于对称性的保持和破缺机制。


<details>
  <summary>Details</summary>
Motivation: 探索非厄米系统中是否仍能保持传统量子相变的特征，特别是在开放量子系统中研究通常易受退相干和环境干扰的相变现象。

Method: 在双正交框架下计算基态关联函数和纠缠熵，分析对称性破缺机制，用绕例外点的绕数表征LL相的拓扑性质。

Result: 非厄米XY模型的相变标度行为与厄米模型完全相同，FM相源于Z2对称性破缺，LL相由U(1)对称性出现和能谱实部简并共同促成。

Conclusion: 非厄米系统为研究开放量子系统中的传统量子相变提供了新途径，这些相变通常易受退相干和环境干扰。

Abstract: In this work, we investigate many-body phase transitions in a one-dimensional anisotropic XY model subject to a complex-valued transverse field. Within the biorthogonal framework, we calculate the ground-state correlation functions and entanglement entropy, confirming that their scaling behavior remains identical to that in the Hermitian XY model. The preservation of Hermitian phase transition features in the non-Hermitian setting is rooted in the persistence and emergence of symmetries and their breaking. Specifically, the ferromagnetic (FM) phase arises from the breaking of a $Z_2$ symmetry, while the Luttinger liquid (LL) phase is enabled by the emergence of a $U(1)$ symmetry together with the degeneracy of the real part of the energy spectrum. The nontrivial topology of the LL phase are characterized by the winding number around the exceptional point (EP). Given that non-Hermitian systems are inherently open, this research opens a new avenue for exploring conventional quantum phase transitions that are typically vulnerable to decoherence and environmental disruption in open quantum systems.

</details>


### [305] [Transitional Bell Correlation from Dirac Wavepackets](https://arxiv.org/abs/2511.12258)
*Ju Gao,Fang Shen*

Main category: quant-ph

TL;DR: 本文推导了纠缠、反向传播电子的Bell-CHSH关联的闭式表达式，使用真实的狄拉克波包和局域检测。与传统距离无关结果不同，Bell参数随两波空间重叠减少从量子边界2√2连续演化到经典极限2。


<details>
  <summary>Details</summary>
Motivation: 研究纠缠电子的Bell-CHSH关联如何受空间波包重叠影响，挑战传统距离无关的Bell参数假设。

Method: 使用真实的狄拉克波包和局域检测方法，推导纠缠反向传播电子的Bell-CHSH关联闭式表达式。

Result: Bell参数随波包空间重叠减少从量子边界2√2连续演化到经典极限2，量子增强完全来自横向重叠。

Conclusion: Bell违反反映传播狄拉克波的局域重叠，而非任何超距作用。

Abstract: We derive a closed-form expression for the Bell--CHSH correlation of entangled, counter-propagating electrons using realistic Dirac wavepackets and localized detection. In contrast to the conventional distance-independent result, the Bell parameter evolves continuously from the quantum bound $2\sqrt{2}$ to the classical limit $2$ as the spatial overlap of the two waves decreases. The quantum enhancement arises entirely from transverse overlap, showing that the Bell violation reflects the local overlap of propagating Dirac waves rather than any action at a distance.

</details>


### [306] [An Improved Quantum Anonymous Notification Protocol for Quantum-Augmented Networks](https://arxiv.org/abs/2511.12313)
*Nitin Jha,Abhishek Parakh,Mahadevan Subramaniam*

Main category: quant-ph

TL;DR: 提出了一种改进的量子匿名通知(QAN)协议，通过在共享GHZ态上使用旋转操作，在n用户量子增强网络中实现匿名通知。该协议在退相干噪声模型下表现出更强的抗虚假通知能力，并可与机器学习分类器集成。


<details>
  <summary>Details</summary>
Motivation: 当前量子网络的可扩展性受到噪声量子组件和高实现成本的限制，量子增强网络(QuANets)通过将量子组件集成到经典网络基础设施中来提高鲁棒性和端到端安全性。

Method: 利用共享GHZ态的旋转操作来产生匿名通知，研究该改进QAN协议在退相干噪声模型下的行为，并与机器学习分类器集成。

Result: 观察到改进的QAN协议比早期方法对虚假通知具有更强的弹性，通知层可与QuANets集成，使接收方能够绕过交换机处理量子载荷。

Conclusion: 提出的改进QAN协议在噪声环境下表现更优，通过集成到量子增强网络中可减少基于头部的信息泄漏和对受损交换机的针对性干扰漏洞。

Abstract: The scalability of current quantum networks is limited due to noisy quantum components and high implementation costs, thereby limiting the security advantages that quantum networks provide over their classical counterparts. Quantum Augmented Networks (QuANets) address this by integrating quantum components in classical network infrastructure to improve robustness and end-to-end security. To enable such integration, Quantum Anonymous Notification (QAN) is a method to anonymously inform a receiver of an incoming quantum communication. Therefore, several quantum primitives will serve as core tools, namely, quantum voting, quantum anonymous protocols, quantum secret sharing, etc. However, all current quantum protocols can be compromised in the presence of several common channel noises. In this work, we propose an improved quantum anonymous notification (QAN) protocol that utilizes rotation operations on shared GHZ states to produce an anonymous notification in an n-user quantum-augmented network. We study the behavior of this modified QAN protocol under the dephasing noise model and observe stronger resilience to false notifications than earlier QAN approaches. The QAN framework is also proposed to be integrated with a machine-learning classifier, enhanced quantum-augmented network. Finally, we discuss how this notification layer integrates with QuANets so that receivers can allow switch-bypass handling of quantum payloads, reducing header-based information leakage and vulnerability to targeted interference at compromised switches.

</details>


### [307] [QMA Complete Quantum-Enhanced Kyber: Provable Security Through CHSH Nonlocality](https://arxiv.org/abs/2511.12318)
*Ilias Cherkaoui,Indrakshi Dey*

Main category: quant-ph

TL;DR: 提出了首个CHSH认证的Kyber协议，将量子非局域性验证直接嵌入密钥交换阶段，结合格密码学和量子非局域性，建立可验证、可组合且前向安全的混合后量子密钥协商框架。


<details>
  <summary>Details</summary>
Motivation: 后量子密码学需要保护大规模通信系统免受量子攻击，但现有格基KEM方案仅依赖计算硬度假设，易受混合经典-量子攻击。需要将信息论量子保证与格基计算安全性结合。

Method: 设计CHSH认证的Kyber协议，在密钥交换阶段集成CHSH纠缠测试，使用EPR对产生超过经典相关极限的可测量量子优势值。保持与FO变换的完全兼容性。

Result: 形式化归约证明，任何多项式时间敌手破坏该KEM必须解决Module-LWE问题或QMA完全的2-局部哈密顿问题实例。方案保持CCA安全性和Kyber的效率特性。

Conclusion: 该CHSH增强的Kyber方案建立了数学严谨的混合后量子框架，统一了格密码学和量子非局域性，实现了可验证、可组合和前向安全的密钥协商。

Abstract: Post-quantum cryptography (PQC) must secure large-scale communication systems against quantum adversaries where classical hardness alone is insufficient and purely quantum schemes remain impractical. Lattice-based key encapsulation mechanisms (KEMs) such as CRYSTALS-Kyber provide efficient quantum-resistant primitives but rely solely on computational hardness assumptions that are susceptible to hybrid classical-quantum attacks. To overcome this limitation, we introduce the first Clauser-Horne-Shimony-Holt (CHSH)-certified Kyber protocol, which embeds quantum non-locality verification directly within the key exchange phase. The proposed design integrates CHSH entanglement tests using Einstein-Podolsky-Rosen (EPR) pairs to yield measurable quantum advantage values exceeding classical correlation limits, thereby coupling information--theoretic quantum guarantees with lattice-based computational security. Formal reductions demonstrate that any polynomial-time adversary breaking the proposed KEM must either solve the Module Learning With Errors (Module-LWE) problem or a Quantum Merlin-Arthur (QMA)-complete instance of the 2-local Hamiltonian problem, under the standard complexity assumption QMA $\subset$ NP. The construction remains fully compatible with the Fujisaki-Okamoto (FO) transform, preserving chosen-ciphertext attack (CCA) security and Kyber's efficiency profile. The resulting CHSH-augmented Kyber scheme therefore establishes a mathematically rigorous, hybrid post-quantum framework that unifies lattice cryptography and quantum non-locality to achieve verifiable, composable, and forward-secure key agreement.

</details>


### [308] [Stimulated Hawking effect and quasinormal mode resonance in a polariton simulator of field theory on curved spacetime](https://arxiv.org/abs/2511.12339)
*Mattheus Burkhard,Malte Kroj,Kévin Falque,Alberto Bramati,Iacopo Carusotto,Maxime J Jacquet*

Main category: quant-ph

TL;DR: 该论文研究了在极化子模拟器中通过相干探针入射到视界处产生的受激霍金效应，发现该效应表现为负能量Bogoliubov通道的透射，并在准正规模频率处达到峰值。


<details>
  <summary>Details</summary>
Motivation: 研究霍金效应在模拟平台中的表现，特别是受激霍金效应与准正规模之间的相互作用，这是弯曲时空量子场论中的一个开放性问题。

Method: 在极化子模拟器中数值模拟受激霍金效应，使用从外部入射到视界的相干探针，并实现支持准正规模的有效时空。

Result: 发现受激霍金效应表现为视界内负能量Bogoliubov通道的透射，且透射在准正规模频率处达到峰值。

Conclusion: 计算的光谱特征为未来实验研究霍金效应及其与准正规模的相互作用提供了实用指导。

Abstract: The Hawking effect amplifies fluctuations in the vicinity of horizons, both in black holes and in analogue platforms.
  Here, we consider a polariton simulator and numerically examine the \emph{stimulated} Hawking effect using a coherent probe incident on the horizon from the exterior.
  We implement an experimentally realistic effective spacetime that supports a quasinormal mode (QNM) in the vicinity of the horizon.
  We find that the stimulated Hawking effect manifests as transmission into a negative-energy Bogoliubov channel inside the horizon, consistent with pseudo-unitary Bogoliubov scattering.
  Moreover, transmission across the horizon peaks at the QNM frequency.
  The computed spectral signatures provide a practical guide for future experimental investigations of the Hawking effect and its interplay with QNMs, an open question in quantum field theory in curved spacetime.

</details>


### [309] [Optimal Multiparameter Quantum Estimation of Magnonic Couplings in a Magnomechanical Cavity](https://arxiv.org/abs/2511.12352)
*Adnan Naimy,Abdallah Slaoui,Abderrahim Lakhfif,Rachid Ahl Laamara*

Main category: quant-ph

TL;DR: 提出了一种增强耦合参数G_mc和G_mb同时估计精度的实验可行方案，重点研究了外差检测的性能。通过比较同时估计和单独估计策略，证明同时估计方法在系统中具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 开发高精度混合量子传感器，提高量子系统中耦合参数的同时估计精度，探索外差检测在量子参数估计中的实际应用潜力。

Method: 计算基于对称对数导数(SLD)和右对数导数(RLD)的量子费舍尔信息矩阵(QFIM)，比较同时估计和单独估计策略，分析外差检测的性能。

Result: RLD相关的量子Cramér-Rao界(QCRB)始终低于SLD，表明估计精度更高。增加拉比频率、腔损耗率、平均光子数和声子数，同时降低机械阻尼和温度，可提高系统对耦合参数的灵敏度。外差检测在某些条件下可接近QFIM设定的极限精度。

Conclusion: 基于外差检测的测量策略为估计耦合参数G_mc和G_mb提供了高效实用的途径，为高精度混合量子传感器的发展铺平了道路。

Abstract: In this work, we introduce an experimentally viable scheme to enhance the simultaneous estimation precision of the couplings $G_{mc}$ and $G_{mb}$, with a particular focus on the performance of heterodyne detection. By comparing simultaneous and individual estimation strategies, we demonstrate that the simultaneous approach offers a notable advantage in our system. To support this, we compute the quantum Fisher information matrices (QFIMs) based on the symmetric logarithmic derivative (SLD) and the right logarithmic derivative (RLD). Our results show that the quantum Cramér Rao bound (QCRB) associated with the RLD is consistently lower than that of the SLD, indicating superior estimation precision. From a physical standpoint, this improvement reflects the system's enhanced capacity to encode, transfer, and extract quantum information while allowing optimal control of fundamental interactions. We show that increasing the Rabi frequency, cavity loss rate, and the average number of photons and phonons, combined with reduced mechanical damping and temperature, enhances the system's sensitivity to the coupling parameters. These mechanisms act on the available quantum resources, such as entanglement, squeezing, and state purity, leading to more precise estimations. Furthermore, our analysis reveals that under certain conditions, heterodyne detection can closely approach the ultimate precision set by the QFIM. This suggests that a measurement strategy based on heterodyne detection can offer an efficient and practical route for estimating the couplings $G_{mc}$ and $G_{mb}$, paving the way for high precision hybrid quantum sensors.

</details>


### [310] [Quantum Optimization Algorithms](https://arxiv.org/abs/2511.12379)
*Jonas Stein,Maximilian Zorn,Leo Sünkel,Thomas Gabor*

Main category: quant-ph

TL;DR: 本文介绍了量子近似优化算法(QAOA)及其在量子优化中的应用，包括电路实现、参数训练、约束处理，并扩展到变分量子本征求解器(VQE)。


<details>
  <summary>Details</summary>
Motivation: 量子优化可以为特定工业相关问题提供指数级加速，QAOA作为该领域关键算法，是门基量子计算机上量子退火的推广版本。

Method: 讨论了QAOA的量子电路实现，包括高阶伊辛模型的哈密顿量模拟技术，使用参数平移规则进行参数训练，以及通过Grover混合器将约束纳入算法。

Result: 通过Pennylane源代码展示了在最大割问题中的实际应用，展示了如何将搜索空间限制为严格有效解。

Conclusion: QAOA可以扩展到VQE，在NISQ时代具有潜力，但也面临贫瘠高原和ansatz设计等挑战。

Abstract: Quantum optimization allows for up to exponential quantum speedups for specific, possibly industrially relevant problems. As the key algorithm in this field, we motivate and discuss the Quantum Approximate Optimization Algorithm (QAOA), which can be understood as a slightly generalized version of Quantum Annealing for gate-based quantum computers. We delve into the quantum circuit implementation of the QAOA, including Hamiltonian simulation techniques for higher-order Ising models, and discuss parameter training using the parameter shift rule. An example implementation with Pennylane source code demonstrates practical application for the Maximum Cut problem. Further, we show how constraints can be incorporated into the QAOA using Grover mixers, allowing to restrict the search space to strictly valid solutions for specific problems. Finally, we outline the Variational Quantum Eigensolver (VQE) as a generalization of the QAOA, highlighting its potential in the NISQ era and addressing challenges such as barren plateaus and ansatz design.

</details>


### [311] [Nonlocal action in Everettian Quantum Mechanics](https://arxiv.org/abs/2511.12403)
*Mordecai Waegell,Kelvin J. McQueen*

Main category: quant-ph

TL;DR: 本文认为Everettian量子力学(EQM)实际上不是局域理论，因为它允许对一个系统的操作改变整个系统及其远程纠缠伙伴的全局状态，这应被视为非局域作用。


<details>
  <summary>Details</summary>
Motivation: 反驳EQM是局域理论的常见观点，论证EQM中的全局状态变化也应被视为非局域作用。

Method: 首先分析反对观点(认为全局变化只是外在变化)，然后论证内在-外在区分的局限性，最后澄清当全局状态作为理论核心解释机制时，改变全局状态应被视为非局域作用。

Result: 证明EQM不是局域理论，因为它依赖全局状态来解释量子关联现象，如贝尔态中的反关联测量结果。

Conclusion: EQM不是真正的局域理论，因为其核心解释机制依赖于全局状态，对局部系统的操作会影响整个纠缠系统的全局状态。

Abstract: According to a common view, Everettian quantum mechanics (EQM) is a local theory because it avoids nonlocal action at a distance, and this is an important point in EQM's favor. Unlike collapse theories, EQM does not allow an action on one system to change the reduced density matrix (RDM) of a remote entangled system - a clear case of nonlocal action. However, EQM does allow an action on one system to change the global state of the system and its remote entangled partners. We argue that such changes should also count as nonlocal actions, meaning EQM is not local after all. First, we consider an argument to the contrary, which deems such global changes to be mere extrinsic changes, whereas nonlocal action requires intrinsic changes to the remote system. We respond that the intrinsic-extrinsic distinction is problematic and cannot hold the weight of this argument. We then try to clarify when actions that change global states count as nonlocal actions. We argue that it is when the global states are essential explanatory mechanisms of the theory. In EQM, the global state is needed to explain why, in an anti-correlated Bell state, Alice's measuring spin-up ensures that she encounters only the branch where Bob measures spin-down.

</details>


### [312] [Enhancing Chemistry on Quantum Computers with Fermionic Linear Optical Simulation](https://arxiv.org/abs/2511.12416)
*Zack Hassman,Oliver Reardon-Smith,Gokul Subramanian Ravi,Frederic T. Chong,Kevin J. Sung*

Main category: quant-ph

TL;DR: 开发了一个用于模拟费米子线性光学元件和受控相位门电路的模拟器，支持精确和近似概率计算，在模拟某些超出传统状态向量方法能力的系统时特别有用。


<details>
  <summary>Details</summary>
Motivation: 传统状态向量方法在模拟某些费米子电路时存在局限性，需要开发更高效的模拟工具来支持量子化学等领域的算法研究。

Method: 构建了一个开源模拟器，能够计算费米子线性光学元件和受控相位门电路的Born规则概率，支持精确和近似两种计算模式，近似计算的时间复杂度仅与受控相位门角度大小呈指数关系。

Result: 通过模拟局部酉团簇Jastrow（LUCJ）拟设并与基于采样的量子对角化（SQD）结合，在52量子位的N2系统中将基态能量估计精度提高了46%，且计算开销可忽略。

Conclusion: 该模拟器作为费米子电路的高效灵活模拟工具，为化学及相关领域近量子算法的优化提供了新的可能性。

Abstract: We present and open source a simulator for circuits composed of passive fermionic linear optical elements and controlled-phase gates. Given such a circuit, our simulator can compute Born-rule probabilities for samples drawn from it. Our simulator supports both exact and approximate probability calculation, allowing users to trade accuracy for efficiency as needed. For approximate Born-rule probability calculation, our simulator's runtime is exponential only in the magnitudes of the angles of the circuit's controlled-phase gates. This makes our simulator useful for simulating certain systems that are beyond the reach of conventional state vector methods. We demonstrate our simulator's utility by simulating the local unitary cluster Jastrow (LUCJ) ansatz and integrating it with sample-based quantum diagonalization (SQD) to improve the accuracy of molecular ground-state energy estimates. Applied to a 52-qubit $N_2$ system, we observe accuracy improvements of up to $46\%$ over the baseline SQD implementation with negligible computational overhead. As an efficient and flexible tool for simulating fermionic circuits, our simulator enables new opportunities for enhancing near-term quantum algorithms in chemistry and related domains.

</details>


### [313] [Machine Learning Framework for Efficient Prediction of Quantum Wasserstein Distance](https://arxiv.org/abs/2511.12443)
*Changchun Feng,Xinyu Qiu,Laifa Tao,Lin Chen*

Main category: quant-ph

TL;DR: 提出了一种机器学习框架，通过从量子态对中提取物理特征来高效预测量子Wasserstein距离，在3量子比特系统中实现了接近完美的准确率，并验证了在量子信息理论中的实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 量子Wasserstein距离是量化量子操作可区分性的基本度量，在量子纠错中有关键应用，但由于指数级扩展，多量子比特系统的计算仍然具有挑战性。

Method: 使用机器学习框架，从量子态对中提取物理特征（包括泡利测量、统计矩、量子保真度和纠缠度量），采用经典神经网络和传统机器学习模型进行预测。

Result: 在3量子比特系统中，表现最佳的随机森林模型实现了接近完美的准确率（R² = 0.9999），平均绝对误差在10⁻⁵量级。成功验证了量子信息理论中的两个基本理论命题。

Conclusion: 机器学习是计算Wasserstein距离的可行且可扩展的替代方案，在NISQ设备的实时量子电路评估和纠错协议设计中具有特别前景。

Abstract: The quantum Wasserstein distance (W-distance) is a fundamental metric for quantifying the distinguishability of quantum operations, with critical applications in quantum error correction. However, computing the W-distance remains computationally challenging for multiqubit systems due to exponential scaling. We present a machine learning framework that efficiently predicts the quantum W-distance by extracting physically meaningful features from quantum state pairs, including Pauli measurements, statistical moments, quantum fidelity, and entanglement measures. Our approach employs both classical neural networks and traditional machine learning models. On three-qubit systems, the best-performing Random Forest model achieves near-perfect accuracy ($R^2 = 0.9999$) with mean absolute errors on the order of $10^{-5}$. We further validate the framework's practical utility by successfully verifying two fundamental theoretical propositions in quantum information theory: the bound on measurement probability differences between unitary operations and the $W_1$ gate error rate bound. The results establish machine learning as a viable and scalable alternative to traditional numerical methods for W-distance computation, with particular promise for real-time quantum circuit assessment and error correction protocol design in NISQ devices.

</details>


### [314] [Discovering autonomous quantum error correction via deep reinforcement learning](https://arxiv.org/abs/2511.12482)
*Yue Yin,Tailong Xiao,Xiaoyang Deng,Ming He,Jianping Fan,Guihua Zeng*

Main category: quant-ph

TL;DR: 使用课程学习增强的深度强化学习来发现玻色子量子纠错码，以抵抗单光子和双光子损耗，发现了Fock态|4⟩和|7⟩作为最优编码，超越了盈亏平衡点并达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统量子纠错依赖主动测量可能引入额外错误，自主量子纠错(AQEC)通过工程化耗散和驱动规避此问题，但由于严格的Knill-Laflamme条件，寻找实用编码仍具挑战性。

Method: 采用课程学习增强的深度强化学习，在近似AQEC框架下发现玻色子编码。提出解析求解主方程的方案加速强化学习训练，采用两阶段训练策略：快速探索找到超越盈亏平衡点的编码子空间，然后策略微调维持长期性能优势。

Result: 发现Fock态|4⟩和|7⟩作为最优编码字，在考虑单光子和双光子损耗时超越盈亏平衡阈值，在更长的演化时间内保持优势，达到最先进性能。同时分析了该编码对相位阻尼和幅度阻尼噪声的鲁棒性。

Conclusion: 课程学习增强的深度强化学习在发现最优量子纠错码方面具有巨大潜力，特别是在早期容错量子系统中。

Abstract: Quantum error correction is essential for fault-tolerant quantum computing. However, standard methods relying on active measurements may introduce additional errors. Autonomous quantum error correction (AQEC) circumvents this by utilizing engineered dissipation and drives in bosonic systems, but identifying practical encoding remains challenging due to stringent Knill-Laflamme conditions. In this work, we utilize curriculum learning enabled deep reinforcement learning to discover Bosonic codes under approximate AQEC framework to resist both single-photon and double-photon losses. We present an analytical solution of solving the master equation under approximation conditions, which can significantly accelerate the training process of reinforcement learning. The agent first identifies an encoded subspace surpassing the breakeven point through rapid exploration within a constrained evolutionary time-frame, then strategically fine-tunes its policy to sustain this performance advantage over extended temporal horizons. We find that the two-phase trained agent can discover the optimal set of codewords, i.e., the Fock states $\ket{4}$ and $\ket{7}$ considering the effect of both single-photon and double-photon loss. We identify that the discovered code surpasses the breakeven threshold over a longer evolution time and achieve the state-of-art performance. We also analyze the robustness of the code against the phase damping and amplitude damping noise. Our work highlights the potential of curriculum learning enabled deep reinforcement learning in discovering the optimal quantum error correct code especially in early fault-tolerant quantum systems.

</details>


### [315] [Autonomously Designed Pulses for Precise, Site-Selective Control of Atomic Qubits](https://arxiv.org/abs/2511.12524)
*Sanghyo Park,Seuk Lee,Keunyoung Lee,Minhyeok Kim,Donggyu Kim*

Main category: quant-ph

TL;DR: 使用人工智能框架设计复合脉冲，将冷原子量子计算机的局域控制保真度提高十倍，并兼容现有控制硬件。


<details>
  <summary>Details</summary>
Motivation: 冷原子阵列量子计算机的进展受限于局域控制保真度较低的问题，需要提高控制精度以实现容错操作。

Method: 基于原子-激光动力学训练深度神经网络，自主设计复合脉冲来优化局域控制。

Result: 复合脉冲将局域控制保真度提高了十倍，且对光学像差和光束失准具有鲁棒性。

Conclusion: 该方法为高保真量子比特控制建立了AI训练的脉冲编译框架，可扩展到其他原子类平台如囚禁离子和固态色心。

Abstract: Quantum computers based on cold-atom arrays offer long-lived qubits with programmable connectivity, yet their progress toward fault-tolerant operation is limited by the relatively low fidelity of site-selective local control. We introduce an artificial-intelligence (AI) framework that overcomes this limitation. Trained on atom-laser dynamics, a deep neural network autonomously designs composite pulses that improve local control fidelities tenfold while remaining compatible with existing control hardware. We further demonstrate the robustness of these pulses against optical aberrations and beam misalignment. This approach establishes AI-trained pulse compilation for high-fidelity qubit control and can be readily extended to other atom-like platforms, such as trapped ions and solid-state color centers.

</details>


### [316] [Minute-Scale Photonic Quantum Memory](https://arxiv.org/abs/2511.12537)
*You-Cai Lv,Yu-Jia Zhu,Zong-Quan Zhou,Chuan-Feng Li,Guang-Can Guo*

Main category: quant-ph

TL;DR: 该研究实现了分钟级别的单光子量子存储，在5.6秒存储时间下达到88.0±2.1%的保真度，突破了现有量子存储器的亚秒级限制。


<details>
  <summary>Details</summary>
Motivation: 构建全球量子网络需要秒到分钟级别的量子存储时间，但现有光子量子存储器仅限于亚秒级寿命。

Method: 结合无噪声光子回波协议和包含绝热脉冲的通用鲁棒动力学解耦序列，在'魔幻'磁场下实现长寿命量子存储。

Result: 达到27.6±0.6秒的1/e存储寿命，在42秒内单光子级存储的信噪比大于1，保真度超过经典策略极限。

Conclusion: 该工作确立了分钟级光子量子存储器，为全球量子网络和深空量子实验奠定了基础。

Abstract: Long-lived storage of single photons is a fundamental requirement for enabling quantum communication and foundational tests of quantum physics over extended distances. While the implementation of a global-scale quantum network requires quantum storage times on the order of seconds to minutes, existing photonic quantum memories have so far been limited to subsecond lifetimes. Although $^{151}$Eu$^{3+}$:Y$_2$SiO$_5$ crystals exhibit substantially extended spin coherence times at the `magic' magnetic field, the concomitant weak optical absorption has until now prevented single-photon storage. Here, we overcome this challenge by integrating a noiseless photon echo protocol -- which makes full use of the crystal's natural absorption for photonic storage -- with a universally robust dynamical decoupling sequence incorporating adiabatic pulses to protect nuclear spin coherence, enabling long-lived quantum storage at the `magic' magnetic field. At a storage time of 5.6 s, we achieve a time-bin qubit storage fidelity of 88.0 $\pm$ 2.1%, surpassing the maximum fidelity attainable via classical strategies. Our device reaches a $1/e$ storage lifetime of 27.6 $\pm$ 0.6 s, enabling single-photon-level storage for 42 s with a signal-to-noise ratio greater than unity. This work establishes photonic quantum memory in the minute-scale regime, laying a solid foundation for global-scale quantum network and deep-space quantum experiments.

</details>


### [317] [Sparsity-Driven Entanglement Detection in High-Dimensional Quantum States](https://arxiv.org/abs/2511.12546)
*Stav Lotan,Hugo Defienne,Ronen Talmon,Guy Bartal*

Main category: quant-ph

TL;DR: 提出了一种基于稀疏性驱动的方法来增强高维量子纠缠的检测和认证，通过ℓ1正则化重建样本协方差矩阵，在空间纠缠光子对中提高了相关性信号的可见度并抑制了噪声。


<details>
  <summary>Details</summary>
Motivation: 传统的高维量子纠缠表征方法需要大量数据采集且受实验噪声影响，可见度有限，需要更高效、抗噪声的检测方法。

Method: 对通过自发参量下转换产生的光子进行测量，获取样本协方差矩阵，然后应用ℓ1正则化重建来增强相关性信号并抑制噪声。

Result: 使用位置-动量爱因斯坦-波多尔斯基-罗森纠缠判据证明，该方法能够认证无法通过无正则化方法实现的高维纠缠维度。

Conclusion: 该方法具有可扩展性、简单易用且与现有量子光学平台兼容，为高效实时分析高维量子态铺平了道路。

Abstract: The characterization of high-dimensional quantum entanglement is crucial for advanced quantum computing and quantum information algorithms. Traditional methods require extensive data acquisition and suffer from limited visibility due to experimental noise. Here, we introduce a sparsity-driven framework to enhance the detection and certification of high-dimensional entanglement in spatially entangled photon pairs. By applying $\ell_1$-regularized reconstruction to sample covariance matrices obtained from measurements on photons produced via spontaneous parametric down-conversion (SPDC) measurements, we enhance the visibility of the correlation signal while suppressing noise. We demonstrate, using a position-momentum Einstein-Podolsky-Rosen (EPR) entanglement criterion, that this approach enables certification of an entanglement dimensionality that cannot be achieved without regularization. Our method is scalable, simple to use and compatible with existing quantum-optics platforms, thus paves the way for efficient, real-time analysis of high-dimensional quantum states.

</details>


### [318] [Charge-state stability of single NV centers in HPHT-type IIa diamond](https://arxiv.org/abs/2511.12591)
*Darya Meniailava,Michael Petrov,Josef Soucek,Milos Nesladek*

Main category: quant-ph

TL;DR: 研究弱掺杂HPHT IIa金刚石中单个氮空位中心的电荷态稳定性，发现残余硼受体在决定电荷态稳定性中起关键作用，电场可可靠稳定NV-态


<details>
  <summary>Details</summary>
Motivation: 研究电场和光激发如何共同控制NV中心的电荷转换，理解残余硼受体在电荷态稳定性中的作用

Method: 使用Ti/Al共面电极在氧端接表面，结合电压相关光致发光、实时电荷态监测、激光功率饱和与光谱分解、时间分辨测量

Result: 距离电极数微米的电场显著增加NV-种群并增强自旋读出；低激发功率下NV-种群以压缩指数动力学演化；脉冲激发下观察到数百纳秒的NV-/NV0转换，可被偏压抑制

Conclusion: 残余硼受体在决定电荷态稳定性中起关键作用，电偏压可可靠稳定弱掺杂块状金刚石中的NV-态

Abstract: This is a preliminary version. Improvements and additional analysis will be included in a revised manuscript. We investigate the charge-state stability of individual nitrogen-vacancy (NV) centers in weakly doped HPHT IIa diamond containing sub-ppm concentrations of boron and nitrogen. Using Ti/Al coplanar electrodes on an oxygen-terminated surface, we study how applied electric fields and optical excitation jointly govern NV charge conversion. By combining voltage-dependent photoluminescence, real-time charge-state monitoring, laser-power saturation with spectral decomposition, and time-resolved measurements, we reveal that electric fields several micrometers from the contacts significantly increase the NV- population and enhance spin readout. At low excitation powers, the NV- population evolves on minute timescales following compressed-exponential kinetics, consistent with slow space-charge rearrangement in ultra-insulating diamond. Under pulsed excitation, we observe hundreds-of-nanoseconds NV-/NV0 conversion driven by hole capture, which is strongly suppressed by applied bias. Our results demonstrate that residual boron acceptors play a key role in determining charge-state stability and show how electrical bias can reliably stabilize NV- in weakly doped bulk diamond.

</details>


### [319] [Stability of intrinsic localized modes on the lattice with competing power nonlinearities](https://arxiv.org/abs/2511.12649)
*Georgy L. Alfimov,Pavel A. Korchagin,Dmitry E. Pelinovsky*

Main category: quant-ph

TL;DR: 研究了具有竞争幂次(p,q)的离散非线性薛定谔方程，其中2≤p<q。通过谱稳定性分析，证明了相同符号的较大态编码是谱稳定和非线性稳定的，而交替符号的较小态编码虽然谱稳定但具有负Krein特征值。


<details>
  <summary>Details</summary>
Motivation: 研究物理相关情况(2,3)、(3,4)、(3,5)的离散非线性薛定谔方程，在反连续极限下分析固有局域模的稳定性特性。

Method: 采用谱稳定性分析，在反连续极限下对固有局域模进行分类编码，分析其谱稳定性和非线性稳定性。

Result: 相同符号的较大态编码是谱稳定和非线性稳定的；交替符号的较小态编码谱稳定但具有负Krein特征值；数值识别了由符号确定较大态和交替符号较小态堆叠组合构成的谱稳定编码。

Conclusion: 竞争幂次离散非线性薛定谔方程中，固有局域模的稳定性取决于其编码类型，较大态相同符号编码最稳定，而交替符号较小态编码虽然谱稳定但存在稳定性隐患。

Abstract: We study the discrete nonlinear Schrodinger equation with competing powers (p,q) satisfying 2 <= p < q. The physically relevant cases are given by (p,q) = (2,3), (p,q) = (3,4), and (p,q) = (3,5). In the anticontinuum limit, all intrinsic localized modes are compact and can be classified by their codes, which record one of two nonzero (smaller and larger) states and their sign alternations. By using the spectral stability analysis, we prove that the codes for larger states of the same sign are spectrally and nonlinearly (orbitally) stable, whereas the codes for smaller states of the alternating signs are spectrally stable but have eigenvalues of negative Krein signature. We also identify numerically the spectrally stable codes which consist of stacked combinations of the sign-definite larger states and the sign-alternating smaller states.

</details>


### [320] [Quantum Hyperdimensional Computing: a foundational paradigm for quantum neuromorphic architectures](https://arxiv.org/abs/2511.12664)
*Fabio Cumbo,Rui-Hao Li,Bryan Raubenolt,Jayadev Joshi,Abu Kaisar Mohammad Masum,Sercan Aygun,Daniel Blankenberg*

Main category: quant-ph

TL;DR: 本文提出了量子超维计算(QHDC)，将经典超维计算(HDC)的核心操作优雅地映射到量子计算原语上，建立了资源高效的量子原生实现框架。


<details>
  <summary>Details</summary>
Motivation: 当前量子计算中的学习模型多是经典框架的复杂改编，缺乏真正符合量子原理的设计。需要开发量子原生的学习范式。

Method: 建立直接映射：超向量映射为量子态，捆绑操作用LCU和OAA实现，绑定操作用量子相位预言机，置换操作用QFT，相似度计算用量子态保真度测量。

Result: 通过符号类比推理和监督分类任务验证了框架可行性，在156-qubit IBM Heron r3量子处理器上执行，结果验证了映射的有效性。

Conclusion: QHDC为量子神经形态算法奠定了基础，为解决经典系统难以处理的复杂认知和生物医学问题开辟了新途径。

Abstract: A significant challenge in quantum computing (QC) is developing learning models that truly align with quantum principles, as many current approaches are complex adaptations of classical frameworks. In this work, we introduce Quantum Hyperdimensional Computing (QHDC), a fundamentally new paradigm. We demonstrate that the core operations of its classical counterpart, Hyperdimensional Computing (HDC), a brain-inspired model, map with remarkable elegance and direct correspondence onto the native operations of a QC. This suggests HDC is exceptionally well-suited for a quantum-native implementation. We establish a direct, resource-efficient mapping: (i) hypervectors are mapped to quantum states, (ii) the bundling operation is implemented as a quantum-native averaging process using a Linear Combination of Unitaries (LCU) and Oblivious Amplitude Amplification (OAA), (iii) the binding operation is realized via quantum phase oracles, (iv) the permutation operation is implemented using the Quantum Fourier Transform (QFT), and (v) vector similarity is calculated using quantum state fidelity measurements based on the Hadamard Test. We present the first-ever implementation of this framework, validated through symbolic analogical reasoning and supervised classification tasks. The viability of QHDC is rigorously assessed via a comparative analysis of results from classical computation, ideal quantum simulation, and execution of a 156-qubit IBM Heron r3 quantum processor. Our results validate the proposed mappings and demonstrate the versatility of the framework, establishing QHDC as a physically realizable technology. This work lays the foundation for a new class of quantum neuromorphic algorithms and opens a promising avenue for tackling complex cognitive and biomedical problems intractable for classical systems.

</details>


### [321] [Dissipative Dynamics of Charged Graphene Quantum Batteries](https://arxiv.org/abs/2511.12666)
*Disha Verma,Indrajith VS,R. Sankaranarayanan*

Main category: quant-ph

TL;DR: 研究石墨烯量子电池在耗散环境下的动力学行为，发现振幅阻尼可以稳定非被动稳态并保持有限功提取能力，而非马尔可夫记忆效应能减缓功损失并实现部分恢复。


<details>
  <summary>Details</summary>
Motivation: 探索石墨烯基量子电池在耗散环境中的性能表现，特别是不同噪声机制对电池能量存储和提取能力的影响。

Method: 将石墨烯量子电池建模为四能级自旋谷系统，通过高斯脉冲充电，并在振幅阻尼、退相位以及马尔可夫和非马尔可夫环境中研究其演化。

Result: 振幅阻尼虽然导致能量损失，但能稳定非被动稳态并保持有限功提取；纯退相位抑制相干性并消除功提取；非马尔可夫记忆效应减缓功损失并实现部分恢复。

Conclusion: 相干性和环境记忆是增强石墨烯量子电池长期性能的关键资源。

Abstract: We investigate dissipative dynamics in a graphene-based quantum battery modeled as a four level spin valley system. The battery is charged via a Gaussian pulse and subsequently evolves under amplitude damping, dephasing, and both Markovian and non Markovian reservoirs. We find that amplitude damping, while inducing energy loss, can stabilize non passive steady states with finite ergotropy, whereas pure dephasing suppresses coherence and eliminates work extraction. On the other hand, non-Markovian memory slows ergotropy loss and enables partial recovery through information backflow. These results identify coherence and reservoir memory as essential resources for enhancing the long-time performance of graphene quantum batteries.

</details>


### [322] [Moments of quantum channel ensembles](https://arxiv.org/abs/2511.12700)
*Matthew Duschenes,Diego García-Martín,Zoë Holmes,M. Cerezo*

Main category: quant-ph

TL;DR: 本文开发了一个计算量子信道集合矩算子的理论框架，特别关注可作为参考点的集合，通过矩算子范数不等式建立集合间的层次关系，并定义了信道t-design等有用概念。


<details>
  <summary>Details</summary>
Motivation: 量子信道集合的矩算子在量子信息理论中扮演重要角色，但相比酉算子的矩算子，其性质尚未得到充分探索。需要建立理论框架来研究量子信道集合的矩算子及其统计特性。

Method: 推导了计算量子信道集合矩算子的理论框架，建立了集合间的层次关系（通过矩算子范数不等式），定义了信道t-design概念，并进行了理论和数值研究。还发现了一个块正交置换基以简化分析。

Result: 研究表明不同类型的噪声对矩算子范数有不同影响：退极化噪声会降低范数，而振幅阻尼噪声会增加范数。将噪声诱导的集中现象推广到信道设计诱导现象。

Conclusion: 建立的量子信道矩算子理论框架为研究量子信道集合的统计特性提供了基础工具，发现的块正交置换基简化了分析，对理解量子信道动力学和噪声效应具有重要意义。

Abstract: Moments of ensembles of unitaries play a central role in quantum information theory as they capture the statistical properties of dynamics of systems with some form of randomness. Indeed, concepts such as approximate $t$-designs arise when comparing how close an associated moment operator of a given unitary ensemble is to that of another, reference ensemble. Despite the importance of moment operators, their properties have not been as explored for quantum channels. In this work we develop a theoretical framework to compute moment operators for ensembles of quantum channels, for all moment orders $t$, with a special focus on determining ensembles that can be used as points of reference. By deriving hierarchies between ensembles, via inequalities of their moment operator norms, we give them operational meaning, and define useful concepts such as that of channel $t$-designs. Finally, we perform theoretical and numerical studies which show that different types of noise can decrease the norm of the moment operators (e.g., depolarizing noise), as well as increase it (e.g., amplitude damping), and generalize noise-induced concentration phenomena to channel-design-induced phenomena. Along the way, we find a block-orthogonal basis for permutations, which greatly simplifies our analyses, and may be of independent interest.

</details>


### [323] [The role of averages in CV-QKD over fast fading channels](https://arxiv.org/abs/2511.12721)
*Miguel Castillo-Celeita,Matteo Schiavon*

Main category: quant-ph

TL;DR: 研究连续变量量子密钥分发协议在快速衰落信道下的安全性，比较了两种窃听模型(HBA和CMA)在集体攻击下的性能差异。


<details>
  <summary>Details</summary>
Motivation: 自由空间通信链路中常见的快速衰落信道对CV-QKD协议安全性影响显著，需要评估不同窃听模型在信道波动下的表现。

Method: 采用两种窃听模型：Holevo界平均(HBA)和协方差矩阵平均(CMA)，分别通过平均信道透射率和平均协方差矩阵计算Holevo界，并推导了两种策略的解析表达式。

Result: 结果表明，秘密密钥率受信道波动处理方式的显著影响，CMA方法在某些情况下表现更好。

Conclusion: 选择与实际协议实现更匹配的模型至关重要，信道波动处理方式对CV-QKD系统性能有决定性影响。

Abstract: This work presents a study of continuous-variable quantum key distribution (CV-QKD) protocols over fast-fading channels, typically found in free-space communication links. Two eavesdropping models are considered to evaluate their security under collective attacks: \textit{Holevo bound average} (HBA) and \textit{covariance matrix average} (CMA). In the HBA approach, the Holevo bound is averaged over the channel transmittance. In contrast, the CMA method calculates the Holevo bound from the average covariance matrix. Analytical expressions are developed for both strategies. The two methods also differ in how they calculate the mutual information between the legitimate parties. The results demonstrate that the SKR is significantly influenced by how you treat channel fluctuations, highlighting the importance of choosing the model that better describes the actual implementation of the protocol.

</details>


### [324] [Sdim: A Qudit Stabilizer Simulator](https://arxiv.org/abs/2511.12777)
*Adeeb Kabir,Steven Nguyen,Sohan Ghosh,Tijil Kiran,Isaac H. Kim,Yipeng Huang*

Main category: quant-ph

TL;DR: 开发了首个开源的任意维度量子比特（qudit）稳定子模拟器，填补了高维量子硬件模拟工具的空白，为探索新型量子纠错方案提供计算基础设施。


<details>
  <summary>Details</summary>
Motivation: 随着高维量子硬件（qudit）的发展，现有的稳定子模拟器仅支持量子比特（qubit），缺乏对qudit的模拟工具，限制了高维量子纠错协议的研究。

Method: 开发了支持所有维度的开源qudit稳定子模拟器，通过验证其与现有状态向量模拟的一致性，并对量子电路评估和采样进行性能基准测试。

Result: 模拟器正确实现了qudit稳定子模拟功能，性能表现良好，能够有效支持高维量子电路的数值表征。

Conclusion: 该qudit稳定子模拟器为探索新型高维量子纠错方案提供了关键的计算基础设施，类似于qubit模拟器在量子纠错发展中的作用。

Abstract: Quantum computers have steadily improved over the last decade, but developing fault-tolerant quantum computing (FTQC) techniques, required for useful, universal computation remains an ongoing effort. Key elements of FTQC such as error-correcting codes and decoding are supported by a rich bed of stabilizer simulation software such as Stim and CHP, which are essential for numerically characterizing these protocols at realistic scales. Recently, experimental groups have built nascent high-dimensional quantum hardware, known as qudits, which have a myriad of attractive properties for algorithms and FTQC. Despite this, there are no widely available qudit stabilizer simulators. We introduce the first open-source realization of such a simulator for all dimensions. We demonstrate its correctness against existing state vector simulations and benchmark its performance in evaluating and sampling quantum circuits. This simulator is the essential computational infrastructure to explore novel qudit error correction as earlier stabilizer simulators have been for qubits.

</details>


### [325] [Verified Implementation of GRAPE Pulse Optimization for Quantum Gates with Hardware-Representative Noise Models](https://arxiv.org/abs/2511.12799)
*Rylan Malarchick*

Main category: quant-ph

TL;DR: QubitPulseOpt是一个开源Python框架，通过硬件代表性的最优控制来弥合量子计算中的"模拟到现实"差距，在IQM Garnet量子处理器上实现77倍的门错误减少。


<details>
  <summary>Details</summary>
Motivation: NISQ计算机中的门保真度受退相干和控制噪声限制，而现有量子最优控制方法在理想化模拟环境中运行，无法捕捉物理量子硬件的实时参数漂移，存在关键的"模拟到现实"差距。

Method: 开发QubitPulseOpt开源框架，实现与IQM Garnet量子处理器的API连接，构建高保真"数字孪生"模型，使用硬件代表性参数进行GRAPE算法脉冲优化。

Result: GRAPE优化脉冲相比标准高斯脉冲实现了77倍的门错误减少，框架通过659个测试套件（59%代码覆盖率）和NASA JPL Power-of-10安全关键编码标准确保可靠性。

Conclusion: QubitPulseOpt建立了可信量子控制软件的新范式，通过硬件代表性最优控制有效弥合了量子计算的模拟到现实差距。

Abstract: Gate fidelity in noisy intermediate-scale quantum (NISQ) computers remains the primary bottleneck limiting practical quantum computation, constrained by decoherence and control noise. Quantum optimal control (QOC) techniques, such as the gradient ascent pulse engineering (GRAPE) algorithm, offer a powerful approach to designing noise-robust pulses that actively mitigate these effects. However, most QOC implementations operate in idealized simulation environments that fail to capture the real-time parameter drift inherent to physical quantum hardware, creating a critical ``sim-to-real'' gap. In this work, I present QubitPulseOpt, an open-source, rigorously-tested Python framework designed to bridge this gap through hardware-representative optimal control. The framework demonstrates API connectivity to IQM's Garnet quantum processor (20-qubit superconducting device) and implements a workflow that constructs a high-fidelity ``digital twin'' using hardware-representative parameters. Using this simulation framework, I demonstrate that GRAPE-optimized pulses achieve a simulated gate error reduction of 77$\times$ compared to standard Gaussian pulses. The framework's reliability is ensured through a 659-test verification suite (59\% code coverage) and adherence to NASA JPL Power-of-10 safety-critical coding standards, establishing a new paradigm for trustworthy quantum control software. All results are from verified GRAPE optimizations with full provenance documentation.

</details>


### [326] [Approximate Message Passing for Quantum State Tomography](https://arxiv.org/abs/2511.12857)
*Noah Siekierski,Kausthubh Chandramouli,Christian Kümmerle,Bojko N. Bakalov,Dror Baron*

Main category: quant-ph

TL;DR: 将近似消息传递(AMP)应用于低秩量子态层析，相比现有方法可将重构保真度提高一个数量级，并在IBM量子设备上进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 量子态层析(QST)在系统规模增大时成本呈指数增长，需要针对特定结构量子态(如低秩态)开发高效方法。

Method: 使用压缩感知技术近似消息传递(AMP)进行低秩量子态层析，并针对QST应用设计了专门的AMP算法。

Result: 相比现有低秩QST方法，重构保真度提高了一个数量级；在IBM Kingston设备上进行了层析实验，并分析了设备噪声对态制备保真度预测可靠性的影响。

Conclusion: 该工作推进了低秩量子态层析技术，并可能适用于其他量子层析协议。

Abstract: Quantum state tomography (QST) is an indispensable tool for characterizing many-body quantum systems. However, due to the exponential scaling cost of the protocol with system size, many approaches have been developed for quantum states with specific structure, such as low-rank states. In this paper, we show how approximate message passing (AMP), a compressed sensing technique, can be used to perform low-rank QST. AMP provides asymptotically optimal performance guarantees for large systems, which suggests its utility for QST. We discuss the design challenges that come with applying AMP to QST, and show that by properly designing the AMP algorithm, we can reduce the reconstruction infidelity by over an order of magnitude compared to existing approaches to low-rank QST. We also performed tomographic experiments on IBM Kingston and considered the effect of device noise on the reliability of the predicted fidelity of state preparation. Our work advances the state of low-rank QST and may be applicable to other quantum tomography protocols.

</details>


### [327] [Pulsation of quantum walk between two arbitrary graphs with weakly connected bridge](https://arxiv.org/abs/2511.12872)
*Taisuke Hosaka,Etsuo Segawa*

Main category: quant-ph

TL;DR: 本文研究了在由两个简单图通过一条边连接的有限图上的Grover行走，发现当连接强度ε足够小时会出现脉动现象，即量子行走者在两个图之间周期性转移，且转移概率仅取决于各图的边数而与图结构无关。


<details>
  <summary>Details</summary>
Motivation: 研究两个图通过弱连接边组成的系统，探索当连接强度很小时量子行走的动力学行为，特别是是否存在特殊的周期性转移现象。

Method: 在由两个任意简单图通过一条边（桥）连接的有限图上进行Grover行走分析，将连接强度ε作为参数，研究ε→0时的渐近行为。

Result: 发现当ε足够小时会出现脉动现象，量子行走者会在两个图之间周期性转移，转移概率的渐近表达式仅依赖于各图的边数，转移周期为O(ε^{-1/2})，当两图边数相等时几乎完全转移。

Conclusion: 弱连接的两个图系统在Grover行走下会出现仅依赖于边数的脉动现象，这为量子行走在弱耦合系统中的应用提供了理论依据。

Abstract: We consider the Grover walk on a finite graph composed of two arbitrary simple graphs connected by one edge, referred to as a bridge. The parameter $ε>0$ assigned at the bridge represents the strength of connectivity: if $ε=0$, then the graph is completely separated. We show that for sufficiently small values of $ε$, a phenomenon called pulsation occurs. The pulsation is characterized by the periodic transfer of the quantum walker between the two graphs. An asymptotic expression with respect to small $ε$ for the probability of finding the walker on either of the two graphs is derived. This expression reveals that the pulsation depends solely on the number of edges in each graph, regardless of their structure. In addition, we obtain that the quantum walker is transferred periodically between the two graphs, with a period of order $O(ε^{-1/2})$. Furthermore, when the number of edges of two graphs is equal, the quantum walker is almost completely transferred.

</details>


### [328] [A note on Schmidt-number witnesses based on symmetric measurements](https://arxiv.org/abs/2511.12887)
*Xiao-Qian Mu,Hao-Fan Wang,Shao-Ming Fei*

Main category: quant-ph

TL;DR: 基于对称测量构建k-正线性映射，提出了新的(k+1)类Schmidt数见证器，能更好识别高维系统中量子态的Schmidt数，并用Fedorov比实验验证。


<details>
  <summary>Details</summary>
Motivation: Schmidt数是量子纠缠的重要表征，更高Schmidt数的量子态在量子信息处理任务中具有显著优势。

Method: 通过对称测量推导一类k-正线性映射，构建新的(k+1)类Schmidt数见证器。

Result: 新见证器能更好识别高维系统中量子态的Schmidt数，Fedorov比可作为实验验证工具。

Conclusion: 提出的(k+1)类Schmidt数见证器在高维系统中具有更好的识别能力，且可通过实验验证。

Abstract: The Schmidt number is an important kind of characterization of quantum entanglement. Quantum states with higher Schmidt numbers demonstrate significant advantages in various quantum information processing tasks. By deriving a class of k-positive linear maps based on symmetric measurements, we present new Schmidt-number witnesses of class (k + 1). By detailed example, we show that our Schmidt number witnesses identify better the Schmidt number of quantum states in high-dimensional systems. Furthermore, we note that the Fedorov ratio, which coincides with the Schmidt number for pure Gaussian states and provides a close approximation in non-Gaussian cases such as spontaneous parametric down-conversion, serves as an experimentally accessible tool for validating the proposed (k +1)-class Schmidt-number witnesses.

</details>


### [329] [Fast Quantum Many Body State Synthesis](https://arxiv.org/abs/2511.12923)
*Prashasti Tiwari,Dylan Lewis,Sougato Bose*

Main category: quant-ph

TL;DR: 提出了一种通过短时间演化制备多体量子基态的新方法，使用可优化的"求解器"哈密顿量来避免传统绝热制备的耗时问题。


<details>
  <summary>Details</summary>
Motivation: 量子多体系统的基态是量子传感、非平衡动力学和量子化学模拟的重要资源，但传统绝热制备方法耗时且易受退相干影响。

Method: 使用初始基准态通过可优化的"求解器"哈密顿量进行短时间（单位时间）演化，通过经典优化最小化能量来寻找最优参数。

Result: 该方法成功应用于最多10个量子比特的多体态制备研究。

Conclusion: 证明了通过短时间演化和经典优化可以有效制备多体纠缠基态，为量子态制备提供了更高效的替代方案。

Abstract: Quantum Mechanical ground states of many-body systems can be important resources for various investigations: for quantum sensing, as the initial state for nonequilibrium quantum dynamics following quenches, and the simulation of quantum processes that start by coupling systems in ground states, eg, could be a process in quantum chemistry. However, to prepare ground states can be challenging; for example, requires adiabatic switching of Hamiltonian terms slower than an inverse gap, which can be time consuming and bring in decoherence. Here we investigate the possibility of preparing a many-body entangled ground state of a certain Hamiltonian, which can be called a quantum ``problem'' Hamiltonian, using the time evolution of an initial fiducial state by another ``solver'' Hamiltonian/s for a very short fixed (unit) time. The parameters of the solver Hamiltonian are optimised classically using energy minimisation as the cost function. We present a study of up to n=10 qubit many-body states prepared using this methodology.

</details>


### [330] [ZX-DB: A Graph Database for Quantum Circuit Simplification and Rewriting via the ZX-Calculus](https://arxiv.org/abs/2511.13033)
*Valter Uotila,Cong Yu,Bo Zhao*

Main category: quant-ph

TL;DR: ZX-DB是一个基于图数据库的量子电路简化系统，使用ZX演算在Memgraph图数据库中执行量子电路重写，相比PyZX框架实现了数量级的速度提升。


<details>
  <summary>Details</summary>
Motivation: 量子电路编译需要优化和适配目标硬件，现有方法在处理大规模量子电路时效率有限，需要更高效的量子电路简化系统。

Method: 将ZX演算重写规则编码为标准openCypher查询，在图数据库Memgraph中执行，实现数据库原生的量子电路变换，并集成张量和图等价性验证。

Result: 实验结果显示ZX-DB在独立重写任务中比PyZX框架快一个数量级，同时揭示了当前图数据库引擎在模式匹配方面的瓶颈。

Conclusion: ZX-DB通过结合量子编译和图数据管理，为可扩展的数据库支持量子计算管道开辟了新的系统方向。

Abstract: Quantum computing is an emerging computational paradigm with the potential to outperform classical computers in solving a variety of problems. To achieve this, quantum programs are typically represented as quantum circuits, which must be optimized and adapted for target hardware through quantum circuit compilation. We introduce ZX-DB, a data-driven system that performs quantum circuit simplification and rewriting inside a graph database using ZX-calculus, a complete graphical formalism for quantum mechanics. ZX-DB encodes ZX-calculus rewrite rules as standard openCypher queries and executes them on an example graph database engine, Memgraph, enabling efficient, database-native transformations of large-scale quantum circuits. ZX-DB integrates correctness validation via tensor and graph equivalence checks and is evaluated against the state-of-the-art PyZX framework. Experimental results show that ZX-DB achieves up to an order-of-magnitude speedup for independent rewrites, while exposing pattern-matching bottlenecks in current graph database engines. By uniting quantum compilation and graph data management, ZX-DB opens a new systems direction toward scalable, database-supported quantum computing pipelines.

</details>


### [331] [A Fractional Calculus Framework for Open Quantum Dynamics: From Liouville to Lindblad to Memory Kernels](https://arxiv.org/abs/2511.13038)
*Bo Peng,Yu Zhang*

Main category: quant-ph

TL;DR: 本文建立了一个统一层次结构，将分数阶量子主方程嵌入开放系统动力学框架中，通过Bochner-Phillips从属方法将分数演化表示为Lindblad半群的加权平均，连接了幺正、马尔可夫和结构非马尔可夫动力学。


<details>
  <summary>Details</summary>
Motivation: 开放量子系统表现出从纯幺正演化到不可逆耗散弛豫的动力学，许多物理系统显示出非马尔可夫特征，如代数弛豫和相干回流，这些超出了半群演化的范围。分数阶微积分为描述这种长记忆行为提供了自然框架。

Method: 通过Bochner-Phillips从属方法，将分数演化表示为Lindblad半群的加权平均，权重由幂律等待时间分布决定。分数主方程构成记忆核模型的结构子类，在单位分数阶时简化为GKSL形式。

Result: 构建了一个确保物理一致性的框架，解释了长时间衰减的代数起源，并桥接了幺正、马尔可夫和结构非马尔可夫机制。分数阶微积分成为具有内在记忆的量子动力学的严格统一语言。

Conclusion: 分数阶微积分为具有内在记忆的量子动力学提供了一个严谨且统一的框架，为理论分析和量子模拟开辟了新方向。

Abstract: Open quantum systems exhibit dynamics ranging from purely unitary evolution to irreversible dissipative relaxation. The Gorini--Kossakowski--Sudarshan--Lindblad (GKSL) equation uniquely characterizes Markovian dynamics that are completely positive and trace-preserving (CPTP), yet many physical systems display non-Markovian features such as algebraic relaxation and coherence backflow beyond the reach of semigroup evolution. Fractional calculus provides a natural framework for describing such long-memory behavior through power-law temporal kernels introduced by fractional time derivatives. Here we establish a unified hierarchy that embeds fractional quantum master equations within the broader landscape of open system dynamics. The fractional master equation forms a structured subclass of memory-kernel models, reducing to the GKSL form at unit fractional order. Through Bochner--Phillips subordination, fractional evolution is expressed as an average over Lindblad semigroups weighted by a power-law waiting-time distribution. This construction ensures physical consistency, explains the algebraic origin of long-time decay, and bridges unitary, Markovian, and structured non-Markovian regimes. The resulting framework positions fractional calculus as a rigorous and unifying language for quantum dynamics with intrinsic memory, enabling new directions for theoretical analysis and quantum simulation.

</details>


### [332] [Quantum lattice Boltzmann method for several time steps: A local Carleman linearization algorithm](https://arxiv.org/abs/2511.13072)
*Antonio David Bastida Zamora,Ljubomir Budinski,Valtteri Lahtinen,Pierre Sagaut*

Main category: quant-ph

TL;DR: 提出了一种使用Carleman线性化的量子格子玻尔兹曼方法新编码方案，该方案支持局部碰撞规则，且获得正确结果的概率约为10^{-2}。


<details>
  <summary>Details</summary>
Motivation: 与之前的研究相比，新编码方案能够在保持局部碰撞规则的同时提高获得正确结果的概率。

Method: 采用Carleman线性化进行量子编码，使用动态电路保持恒定量子比特数。

Result: 算法每个时间步的复杂度为O(log_2^3(N)+Q^4)，其中N是2D格点的数量，Q是通道数。

Conclusion: 该编码方案在量子LBM算法中实现了局部碰撞规则和较高的成功概率。

Abstract: This article presents a novel encoding for quantum Lattice Boltzmann method algorithm using Carleman linearization. In contrast to previous articles \cite{Sanavio2024LatticeBC,sanavio2025carleman}, the encoding used allows for local collision rules while keeping a higher probability to obtain the right result, which is of the order of $10^{-2}$. The algorithm scales as $O(log_2^3(N)+Q^4)$ each time step with $N$ the number of lattice sites of the 2D lattice and $Q$ the number of channels with a constant number of qubits when using dynamical circuits.

</details>


### [333] [Topological enhancement of a PT-symmetric Su-Schrieffer-Heeger quantum battery](https://arxiv.org/abs/2511.13088)
*A-Long Zhou,Ya-Wen Xiao,Nuo Xu,Li-Li Gao,Long-Jie Li,Hang Zhou,Zi-Min Li,Chuan-Cun Shu*

Main category: quant-ph

TL;DR: 基于SSH晶格的PT对称非厄米量子电池，通过拓扑边缘态和异常点的相互作用实现快速充电和能量积累


<details>
  <summary>Details</summary>
Motivation: 研究晶格拓扑与非厄米性如何相互作用来增强量子电池的充电性能

Method: 在SSH晶格上采用PT对称协议，交替在两个子晶格上施加增益和损耗，分析体态和边缘态异常点对充电动力学的影响

Result: 在拓扑相中，边缘态异常点在指数小的非厄米强度下出现，导致早期PT对称性破缺和快速能量积累，拓扑相在所有参数范围和系统尺寸下都产生更高的存储能量和更快的饱和

Conclusion: 拓扑是增强量子电池性能的真实物理资源

Abstract: We investigate a non-Hermitian quantum battery based on the Su-Schrieffer-Heeger (SSH) lattice, charged through a PT-symmetric protocol that alternates gain and loss between the two sublattices. The interplay between lattice topology and non-Hermiticity gives rise to both bulk and edge exceptional points (EPs), which govern the charging dynamics. In the topological regime, an edge-state EP emerges at an exponentially small non-Hermitian strength, resulting in early PT-symmetry breaking and rapid energy accumulation. This topological enhancement originates from the PT-symmetric non-Hermitian dynamics, in which the broken-symmetry edge mode with the largest imaginary part of the eigenvalue dominates the time evolution. Consequently, the topological phase consistently yields higher stored energy and faster saturation than the trivial configuration across all parameter regimes and system sizes. These findings demonstrate that topology constitutes a genuine physical resource for enhancing the performance of quantum batteries.

</details>


### [334] [Fractional Contribution of Dynamical and Geometric Phases in Quantum Evolution](https://arxiv.org/abs/2511.13090)
*Arun Kumar Pati,Vlatko Vedral*

Main category: quant-ph

TL;DR: 本文证明了量子演化相位的几何与动力学分量划分完全由单一几何量——Bargmann角（Bures角）决定，建立了量子演化动力学与状态空间几何之间的定量联系。


<details>
  <summary>Details</summary>
Motivation: 量子演化总相位的几何与动力学分量划分是量子物理的核心问题，需要找到普遍适用的精确划分方法。

Method: 通过理论证明，发现量子演化相位的划分在任何时刻都仅由Bargmann角这一几何量决定。

Result: 建立了量子演化相位几何与动力学分量划分的普适定律，该划分完全由Bargmann角控制。

Conclusion: 这一发现为设计高保真度几何量子门提供了实时几何性度量，并在量子速度极限和相干控制方面开辟了新途径。

Abstract: The fundamental division of the total quantum evolution phase into geometric and dynamical components is a central problem in quantum physics. Here, we prove a remarkably simple and universal law demonstrating that this partitioning is governed, at every instant, solely by a single geometric quantity: the Bargmann angle (Bures angle). This result provides a universally applicable and rigorous way to define the exact fraction of the total phase that is geometric versus dynamical in origin, thereby establishing a new quantitative link between the dynamics of quantum evolution and the geometry of the state space. This finding has immediate practical consequences, furnishing a real-time measure of the geometricity of an evolution for designing high-fidelity geometric quantum gates with optimized robustness, and opening new avenues for quantum speed limit and coherent control.

</details>


### [335] [Quantum Mpemba Effect Induced by Non-Markovian Exceptional Point](https://arxiv.org/abs/2511.13173)
*Ze-Zhou Zhang,Hong-Gang Luo,Wei Wu*

Main category: quant-ph

TL;DR: 本文研究了非马尔可夫体系中量子Mpemba效应的实现机制，通过非马尔可夫异常点加速量子系统的弛豫过程。


<details>
  <summary>Details</summary>
Motivation: 传统量子Mpemba效应理论基于Born-Markov近似，但在非马尔可夫体系中该现象尚未得到充分理解，需要探索新的实现机制。

Method: 在一般非马尔可夫动力学框架下研究弛豫过程，提出通过非马尔可夫异常点实现量子Mpemba效应的机制，并在耗散量子谐振子模型中验证可行性。

Result: 成功验证了通过非马尔可夫异常点实现量子Mpemba效应的可行性，为加速量子系统中能量和信息传输提供了新途径。

Conclusion: 该工作为理解非平衡动力学现象提供了新视角，开辟了在量子系统中加速能量和信息传输的新方法。

Abstract: Quantum Mpemba effect describes an anomalous phenomenon of accelerated relaxation which is of fundamental interest in the field of nonequilibrium thermodynamics. Conventional theories on this phenomenon strongly rely on the Born-Markovian approximation, but this effect is not well understood in non-Markovian regimes. By investigating the relaxation process within the framework of a general non-Markovian dynamics, we propose a mechanism of realizing the quantum Mpemba effect via non-Markovian exceptional points. We verify the feasibility of this mechanism in a dissipative quantum harmonic oscillator model. Providing a new insight into the interesting non-equilibrium dynamics phenomenon, our work paves a way to accelerate the transfer of energy and information in quantum systems.

</details>


### [336] [The correlated matching decoder for the 4.8.8 color code](https://arxiv.org/abs/2511.13192)
*Yantong Liu,Junjie Wu,Lingling Lao*

Main category: quant-ph

TL;DR: 本文提出了一种针对4.8.8颜色码的相关解码器，通过利用限制格之间的相关性来改进传统限制解码器，实现了比限制解码器和统一解码器更高的阈值。


<details>
  <summary>Details</summary>
Motivation: 现有的颜色码匹配解码器（如限制解码器）解码性能有限，受统一解码器的全局解码思路启发，希望开发性能更好的解码器。

Method: 通过将表面码的相关匹配解码器映射到颜色码格上，开发了相关解码器，利用限制格之间的相关性来改进解码性能。

Result: 相关解码器在代码容量和现象学噪声模型下，对位翻转错误的阈值分别达到10.38%和3.13%，比限制解码器和统一解码器更高。通过表面-颜色码映射，表面码对去极化噪声的阈值达到16.62%和3.52%。

Conclusion: 相关解码器显著提高了颜色码的解码性能，在容错量子计算中具有重要应用价值。

Abstract: Color codes present distinct advantages for fault-tolerant quantum computing, such as high encoding rates and the transversal implementation of Clifford gates. However, existing matching-based decoders for the color codes such as the restricted decoder (Kubica and Delfosse, 2023), suffer from limited decoding performance. Inspired by the global decoding insight of the unified decoder (Benhemou et al., 2023), this paper introduces a correlated decoder for the 4.8.8 color code, which improves upon the conventional restricted decoder by leveraging correlations between restricted lattices, and is derived by mapping the correlated matching decoder for the surface code onto the color code lattice. Analytical and numerical results show that the correlated decoder achieves higher thresholds than the restricted and unified decoders, while matching the performance of the unified decoder at very low physical error rates. Under the code capacity and phenomenological noise models, the estimated thresholds for the color code against bit-flip error are 10.38% and 3.13%, respectively. Furthermore, by applying the surface-color code mapping, the thresholds of 16.62% and 3.52% are obtained for the surface code against depolarizing noise.

</details>


### [337] [Topological quantum compilation for non-semisimple Ising anyons via monte carlo simulations](https://arxiv.org/abs/2511.13194)
*Jiangwei Long,Yizhi Li,Jianxin Zhong,Lijun Meng*

Main category: quant-ph

TL;DR: 使用非半单伊辛任意子模型构建通用量子门集，通过MC增强的Solovay-Kitaev算法实现高保真度的H门、T门和CNOT门近似，在特定参数α下仅需三级递归即可满足容错量子计算要求。


<details>
  <summary>Details</summary>
Motivation: 探索基于非半单伊辛任意子模型的拓扑量子计算，寻找实现通用量子门集的新途径。

Method: 采用MC增强的Solovay-Kitaev算法，利用非半单伊辛任意子模型的基本编织矩阵来近似标准量子门。

Result: 在α∈(2,2.031]范围内，单个编织操作能以高精度近似CNOT门；在α=2.031、2.047、2.063时成功构建了高精度的通用门集{H,T,CNOT}。

Conclusion: 这项工作为使用非半单伊辛任意子实现通用量子计算开辟了新途径，证明了仅需三级递归即可满足容错要求的高效性。

Abstract: We present a systematic numerical construction of a universal quantum gate set for topological quantum computation based on the non-semisimple Ising anyons model. Using the elementary braiding matrices (EBMs) of this model by the Monte Carlo-enhanced Solovay-Kitaev algorithm (MC-enhanced SKA), we achieve high-fidelity approximations of standard one-qubit gates (Hadamard H-gate and phase T-gate). Remarkably, a recursion level of just three suffices to meet the fidelity requirements for fault-tolerant quantum computation. Our numerical results demonstrate that for the parameter α /in (2, 2.031], a single braiding operation can approximate the local equivalence class [CNOT] with high precision and great unitary measurement. Specifically, at α = 2.031, 2.047, and 2.063, we successfully construct a universal gate set {H-gate, T-gate, CNOT-gate} with high accuracy. This work establishes a new pathway towards universal quantum computation using non-semisimple Ising anyons.

</details>


### [338] [Data-driven adaptive quantum error mitigation for probability distribution](https://arxiv.org/abs/2511.13231)
*Rion Shimazu,Suguru Endo,Shigeo Hakkaku,Shinobu Saito*

Main category: quant-ph

TL;DR: 提出两种基于软件工程技术的量子误差缓解协议，用于提高概率分布估计的准确性：N-版本编程方法和基于一致性的外推策略选择方法。


<details>
  <summary>Details</summary>
Motivation: 虽然量子误差缓解(QEM)主要用于缓解可观测量期望值的误差，但近期研究探索了其在估计无噪声概率分布中的应用。需要改进QEM在概率分布估计中的准确性。

Method: 1. N-版本编程方法：比较不同QEM策略获得的概率分布并排除异常分布；2. 一致性方法：在不同错误率下准备K个数据点，选择L<K个进行外推，评估所有可能选择的外推结果，选择方差最小的外推方法。

Result: 提出的协议能够认证误差缓解分布的可行性，并通过自适应误差缓解提高概率分布估计的准确性。

Conclusion: 基于软件工程技术提出的两种QEM协议有效提高了概率分布估计的准确性，为量子误差缓解提供了新的改进方向。

Abstract: Quantum error mitigation (QEM) has been proposed as a class of hardware-friendly error suppression techniques. While QEM has been primarily studied for mitigating errors in the estimation of expectation values of observables, recent works have explored its application to estimating noiseless probability distributions. In this work, we propose two protocols to improve the accuracy of QEM for probability distributions, inspired by techniques in software engineering. The first is the N-version programming method, which compares probability distributions obtained via different QEM strategies and excludes the outlier distribution, certifying the feasibility of the error-mitigated distributions. The second is a consistency-based method for selecting an appropriate extrapolation strategy. Specifically, we prepare $K$ data points at different error rates, choose $L<K$ of them for extrapolation, and evaluate error-mitigated results for all $\binom{K}{L}$ possible choices. We then select the extrapolation method that yields the smallest variance in the error-mitigated results. This procedure can also be applied bitstring-wise, enabling adaptive error mitigation for each probability in the distribution.

</details>


### [339] [Depth Optimization of Ansatz Circuits for Variational Quantum Algorithms](https://arxiv.org/abs/2511.13256)
*Spyros Tserkis,Muhammad Umer,Dimitris G. Angelakis*

Main category: quant-ph

TL;DR: 该论文提出通过引入额外量子比特、中间电路测量和经典控制操作来减少变分量子算法中量子电路的深度，以应对量子比特相干时间有限导致的噪声问题。


<details>
  <summary>Details</summary>
Motivation: 量子电路深度的增加受到物理量子比特有限相干时间的限制，导致计算过程中出现噪声和错误，这限制了量子算法的执行。

Method: 通过引入额外量子比特、中间电路测量和经典控制操作来构建非幺正量子电路，从而减少电路深度。以一维Burgers方程的非线性动力学为例进行验证。

Result: 所提出的非幺正量子电路能够有效表示层流和湍流状态下的流体流动配置，在双量子比特门错误率相对低于空闲错误率的噪声条件下具有优势。

Conclusion: 通过增加量子比特和引入经典控制操作的非幺正量子电路方法，可以有效减少电路深度，在特定噪声条件下比传统幺正电路更具优势。

Abstract: The increasing depth of quantum circuits presents a major limitation for the execution of quantum algorithms, as the limited coherence time of physical qubits leads to noise that manifests as errors during computation. In this work, we focus on circuits relevant to variational quantum algorithms and demonstrate that their depth can be reduced by introducing additional qubits, mid-circuit measurements, and classically controlled operations. As an illustrative example, we consider nonlinear dynamics governed by the one-dimensional Burgers' equation, which has broad applications in computational fluid dynamics. In particular, we show that the proposed non-unitary quantum circuits can efficiently represent fluid flow configurations in both laminar and turbulent regimes. Furthermore, we demonstrate that, when noise is taken into account, these circuits are advantageous in regimes where two-qubit gate error rates are relatively low compared to idling error rates.

</details>


### [340] [Restart Belief: A General Quantum LDPC Decoder](https://arxiv.org/abs/2511.13281)
*Lorenzo Valentini,Diego Forlivesi,Andrea Talarico,Marco Chiani*

Main category: quant-ph

TL;DR: 提出重启信念（RB）解码器，一种基于分支定界优化原理的迭代BP算法，用于解决量子LDPC码解码中的量子简并性问题，是目前最快最准确的QLDPC解码算法。


<details>
  <summary>Details</summary>
Motivation: 硬件友好的量子低密度奇偶校验（QLDPC）解码器通常基于置信传播（BP）处理，但量子简并性常常阻止BP实现可靠收敛。

Method: 提出重启信念（RB）解码器，这是一种基于分支定界优化原理的迭代BP算法。

Result: RB解码器代表了迄今为止适用于QLDPC码的最快和最准确的解码算法。

Conclusion: 该算法旨在接近达到码距的错误校正能力。

Abstract: Hardware-friendly quantum low-density parity-check (QLDPC) decoders are commonly built upon belief propagation (BP) processing. Yet, quantum degeneracy often prevents BP from achieving reliable convergence. To overcome this fundamental limitation, we propose the restart belief (RB) decoder, an iterative BP-based algorithm inspired by branch-and-bound optimization principles. From our analysis we find that the RB decoder represents both the fastest and most accurate decoding algorithm applicable to QLDPC codes to date, conceived with the explicit goal of approaching error correction up to the code distance.

</details>


### [341] [Switching rates in Kerr resonator with two-photon dissipation and driving](https://arxiv.org/abs/2511.13308)
*V. Yu. Mylnikov,S. O. Potashin,M. S. Ukhtary,G. S. Sokolovskii*

Main category: quant-ph

TL;DR: 本文分析了两光子驱动克尔振荡器中的比特翻转错误率，发现在大克尔非线性条件下，切换率随失谐呈现非单调变化，在有限失谐处达到最小值。


<details>
  <summary>Details</summary>
Motivation: 研究双光子驱动克尔振荡器中的量子双稳态和比特翻转错误率，为优化临界猫态量子比特性能和设计可扩展超导玻色量子架构提供理论指导。

Method: 使用Kramer理论和P表示，在势垒近似下推导比特翻转错误率的解析表达式，并与Liouvillian超算符对角化的数值模拟结果进行比较。

Result: 在纯耗散极限下，切换率随失谐单调增加；但在大克尔非线性条件下，切换率随失谐呈现非单调变化，在有限失谐处达到最小值。

Conclusion: 失谐在弱非线性下降低激活势垒，但在强非线性下增加激活势垒，导致切换率在非零失谐处出现最小值，这为优化猫态量子比特性能提供了关键条件。

Abstract: We analytically investigate the switching rate in a two-photon driven Kerr oscillator with finite detuning and two-photon dissipation. This system exhibits quantum bistability and supports a logical manifold for a bosonic qubit. Using Kramer's theory together with the $P$-representation, we derive an analytical expression for the bit-flip error rate within the potential-barrier approximation. The agreement is demonstrated between analytical calculations and numerical simulations obtained by diagonalization of the Liouvillian superoperator. In the purely dissipative limit, the switching rate increases monotonically with detuning, as the two metastable states approach each other in phase space. However, the exponential contribution to the bit-flip rate exhibits a nontrivial dependence on system parameters, extending beyond the naive scaling with the average photon number. In the presence of large Kerr nonlinearity, the switching rate becomes a nonmonotonic function of the detuning and reaches a minimum at a finite detuning. This effect arises because detuning lowers the activation barrier for weak nonlinearity but increases it for large ones, ensuring a minimum of the switching-rate at nonzero detuning. These results establish key conditions for optimizing the performance of critical cat qubits and are directly relevant for the design of scalable superconducting bosonic quantum architectures.

</details>


### [342] [HPC-Accelerated Simulation and Calibration for Silicon Quantum Dots](https://arxiv.org/abs/2511.13330)
*Dhilan Nag,Suhun Kim,Cole Johnson,Collin Sumrell*

Main category: quant-ph

TL;DR: Qalibrate是一个基于JAX的快速量子脉冲生成器，通过求解Lindblad主方程和采用Magnus展开近似，实现了比现有ODE模拟器快34倍的性能，用于加速量子比特校准过程。


<details>
  <summary>Details</summary>
Motivation: 量子计算机校准过程中需要反复实验寻找高保真度脉冲，这个过程耗时且低效。为了加速实验物理学家的工作流程，需要开发快速生成脉冲的工具。

Method: 实现Qalibrate模拟器，生成三电子自旋量子比特的时间演化传播子，集成基于梯度的优化器来生成脉冲。通过求解Lindblad主方程，并采用Magnus展开近似来并行化时间演化计算。

Result: Qalibrate相比现有的ODE模拟器实现了高达34倍的加速，为n量子比特系统生成鲁棒脉冲取得了进展。

Conclusion: Qalibrate提供了一种高效的量子脉冲生成方法，显著加速了量子比特校准过程，为多量子比特系统的脉冲设计奠定了基础。

Abstract: Quantum computers (QCs) have the potential to solve critical problems significantly faster than today's most advanced supercomputers. One major challenge in realizing this technology is designing robust electrostatic pulses to realize unitaries on qubits. Current practice when calibrating unitaries involves recursive experimentation to find the highest-fidelity pulses. To accelerate this process for experimentalists, we implement Qalibrate, a fast, JAX-enabled simulator that generates pulses given target unitaries. Specifically, we generate a propagator that models the time evolution of three-electron spin qubits and integrate our gradient-based optimizer to generate the pulses. The simulation involves solving the Lindblad master equation, which we parallelize by employing an approximation of the time evolution called the Magnus expansion. Qalibrate shows up to a 34x speedup compared to an existing ODE simulator, making progress towards generating robust pulses for n-qubit systems.

</details>


### [343] [Floquet Recurrences in the Double Kicked Top](https://arxiv.org/abs/2511.13342)
*Avadhut V. Purohit,Udaysinh T. Bhosale*

Main category: quant-ph

TL;DR: 本文研究了双踢陀螺模型中的精确量子重现现象，发现当参数k_r取特定值时系统呈现精确周期性，且存在动力学量子相变。通过调节参数可以控制系统从可积到混沌的转变。


<details>
  <summary>Details</summary>
Motivation: 研究双踢陀螺模型中的精确量子重现现象，探索时间反演对称性破缺对量子动力学的影响，以及如何通过参数调节控制系统的可积性和混沌行为。

Method: 通过将双踢陀螺模型的动力学重新表述为有效参数k_r和k_θ，分析Floquet算子的精确周期性，研究长时平均纠缠和保真度率函数的动力学量子相变，并使用能级统计分析系统的可积性转变。

Result: 发现当k_r = jπ/2和k_r = jπ/4时系统呈现精确周期性，且重现周期与j的整数或半整数性质相关。在时间反演对称点k_θ= ±k_r和k_θ= 0处观察到动力学量子相变。通过调节k_r可以实现从可积到非可积的平滑转变。

Conclusion: 双踢陀螺模型通过调节参数k_r和k_θ可以控制系统的规则和混沌行为，为量子控制和信息处理应用提供了一个有用的平台。

Abstract: We study exact quantum recurrences in the double kicked top (DKT), a driven spin model that extends the quantum kicked top (QKT) by introducing an additional time-reversal symmetry-breaking kick. Reformulating its dynamics in terms of effective parameters $k_r$ and $k_θ$, we analytically show exact periodicity of the Floquet operator for $k_r = jπ/2$ and $k_r = jπ/4$ with distinct periods for integer and half-odd integer $j$. These exact recurrences were found to be independent of $k_θ$. The long-time-averaged entanglement and fidelity rate function show dynamical quantum phase transition (DQPT) for $k_r = jπ/2$ at time-reversal symmetric cases $k_θ= \pm k_r$. In the other time-reversal symmetric case $k_θ= 0$, the DQPT exists only for a half-odd integer $j$. Using level statistics, a smooth transition is observed from integrable to non-integrable nature as $k_r$ is changed away from $jπ/2$. Our work demonstrates that regular and chaotic regimes can be controlled for any system size by tuning $k_r$ and $k_θ$, making the DKT a useful platform for quantum control and information processing applications.

</details>


### [344] [The Intrinsic Angular - Momentum of Particles and the Resolution of the Spin-Statistics Theorem](https://arxiv.org/abs/2511.13360)
*Enrico Santamato,Francesco De Martini*

Main category: quant-ph

TL;DR: 基于Weyl可积量子力学理论，解决了传统标准量子力学无法解释的自旋-统计问题，提出了"内禀螺旋性"概念，自然推导出泡利不相容原理。


<details>
  <summary>Details</summary>
Motivation: 传统标准量子力学理论无法解决自旋-统计问题，特别是无法合理解释泡利不相容原理这一重要原理。

Method: 采用Weyl可积量子力学理论，该理论提供了标准量子力学的Weyl规范不变表述，并应用于多粒子系统，引入了"内禀螺旋性"这一新的粒子属性。

Result: 成功推导出自然界中观察到的正确自旋-统计关联，为量子力学提供了更完整的理论框架。

Conclusion: Weyl可积量子力学理论提供了一个更完整的量子力学理论，能够自然解释自旋-统计问题，包括泡利不相容原理。

Abstract: The traditional Standard Quantum Mechanics (SQM) theory is unable to solve the Spin-s problem, i.e., to justify the utterly important "Pauli Exclusion Principle". A complete and straightforward solution of the Spin-Statistics problem is presented based on the "Weyl Integrable Quantum Mechanics" (WIQM) theory. This theory provides a Weyl-gauge invariant formulation of the Standard Quantum Mechanics and reproduces successfully, with no restrictions, the full set of the quantum mechanical processes, including the formulation of Dirac's or Schrödinger's equation, of Heisenberg's uncertainty relations, and of the nonlocal EPR correlations. etc. When the Weyl Integrable Quantum Mechanics is applied to a system made of many identical particles with spin, an additional constant property of all elementary particles enters naturally into play: the "intrinsic helicity", or the "intrinsic angular - momentum". This additional particle property, not considered by Standard Quantum Mechanics, determines the correct Spin-Statistics Connection (SSC) observed in Nature. All this leads to the consideration of a novel, most complete (in the EPR sense) quantum mechanical theory.

</details>


### [345] [Above-Unity Coherent Cooperativity of Tin-Vacancy Centers in Diamond Photonic Crystal Cavities](https://arxiv.org/abs/2511.13375)
*Nina Codreanu,Tim Turan,Daniel Bedialauneta Rodriguez,Matteo Pasini,Lorenzo de Santis,Maximilian Ruf,Christian F. Primavera,Leonardo G. C. Wienhoven,Caroline E. Smulders,Simon Gröblacher,Ronald Hanson*

Main category: quant-ph

TL;DR: 该研究首次实现了锡空位中心与光子晶体腔的相干耦合，达到了超过1的相干合作性，为量子网络提供了高效相干的光-物质界面。


<details>
  <summary>Details</summary>
Motivation: 锡空位中心具有优异的光学和自旋特性，是实现量子网络的理想构建块。虽然已有实验显示其在光子晶体腔中的Purcell增强，但实现相干耦合这一量子协议的关键要素一直未能实现。

Method: 采用准各向同性刻蚀技术制备自由悬浮的光子晶体腔，在室温下表征了327个腔体，详细研究了两个腔耦合发射器，测量了腔质量因子、Purcell缩短寿命和相干合作性。

Result: 获得了平均质量因子超过1.0×10^4的腔体，最高质量因子达25.4×10^3，合作性高达20.6，单锡空位中心在共振时调制腔透射的消光对比度达98.8%，最高相干合作性达到8.3。

Conclusion: 这些结果为实现基于腔耦合锡空位中心的高效相干光-物质界面打开了大门，为未来量子网络应用奠定了基础。

Abstract: The tin-vacancy center in diamond (SnV) has emerged as a compelling building block for realizing next-generation quantum networks thanks to its excellent optical and spin properties. Coupling to photonic crystal cavities (PCCs) promises to further enhance the SnV light-matter interface and unlock a diverse range of entanglement generation protocols. Recent pioneering experiments showing Purcell enhancement of SnV centers in PCCs underscore this potential. However, optical coupling that is coherent - the key ingredient for use in quantum protocols - has so far remained elusive. Here, we demonstrate above-unity coherent cooperativity of SnV centers embedded in photonic crystal cavities. We fabricate free-standing PCCs using a quasi-isotropic undercut. Across two samples, we conduct room-temperature characterizations, measuring resonances for 327 cavities, with an average quality factor exceeding $Q = 1.0(3) \times 10^4$. Two cavity-coupled emitters are examined in detail, exhibiting quality factors up to $Q = 25.4(4) \times 10^3$ and Purcell-reduced lifetimes corresponding to cooperativities up to $C = 20.6(11)$. Furthermore, the single SnVs are observed to strongly modulate the cavity transmission with an extinction contrast up to $98.8(4) \%$ on resonance. Finally, SnV linewidth measurements reveal above-unity coherent cooperativities in both devices, with the highest value being $C_\mathrm{coh} = 8.3(12)$. These results open the door to using cavity-coupled SnV centers as efficient, coherent light-matter interfaces for future quantum networks.

</details>


### [346] [Efficient algorithm for fidelity estimation of two quantum states](https://arxiv.org/abs/2511.13383)
*Anumita Mukhopadhyay,Shibdas Roy,Arun Kumar Pati*

Main category: quant-ph

TL;DR: 提出了一种基于密度矩阵指数化和干涉仪方案的量子算法，用于估计两个量子态之间的保真度，特别适用于混合态和高维密度矩阵。


<details>
  <summary>Details</summary>
Motivation: 量子计算和信息科学中，量子态之间的保真度估计至关重要，但现有方法主要针对纯态，对于混合态和高维密度矩阵的有效方法仍然缺乏。

Method: 基于密度矩阵指数化和干涉仪方案，设计了一种高效的量子算法，适用于可交换密度矩阵的量子态。

Result: 算法的时间复杂度为O(N²/ε⁷)，其中N是系统大小，ε是精度误差，能够有效估计任意两个量子态（纯态或混合态）的保真度。

Conclusion: 该算法提供了一种资源高效的技术，可用于推断任意两个未知或已知量子态的保真度，前提是它们的密度矩阵相互可交换。

Abstract: The fidelity estimation between two quantum states is crucial for quantum computation and information science. However, an efficacious method for this, especially for mixed states and higher-dimensional density matrices, remains elusive. While there are many existing algorithms on computing the fidelity between two pure states, there is not much work on how to obtain the fidelity between two mixed states. Here, we propose an efficient quantum algorithm for the fidelity estimation, based primarily on the density matrix exponentiation and interferometeric scheme for mixed states, with a time complexity of $O(N^2/ε^7)$, where $N$ is the system size and $ε$ is a precision error. Our algorithm may serve as a resource-efficient technique to deduce fidelity of any two (pure or mixed) unknown or known quantum states, when the density matrices of the quantum states commute with each other.

</details>


### [347] [Taming Barren Plateaus in Arbitrary Parameterized Quantum Circuits Without Sacrificing Expressibility](https://arxiv.org/abs/2511.13408)
*Zhenyu Chen,Yuguo Shao,Zhengwei Liu,Zhaohui Wei*

Main category: quant-ph

TL;DR: 提出一种通用且硬件高效的方法，通过在任意参数化量子电路中插入量子通道层来消除barren plateaus现象，该方法只需要一个辅助量子比特和四个额外门，且对噪声具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有参数化量子电路架构面临barren plateaus现象的挑战，即损失函数随系统规模指数级集中，阻碍了有效的参数优化。

Method: 在原始PQC中插入一层易于实现的量子通道，每个通道只需要一个辅助量子比特和四个额外门，生成改进的PQC(MPQC)。

Result: 数值实验表明，该方法在100个量子比特和2400层的电路中有效消除了barren plateaus，而原始ansatz则遭受严重的梯度消失问题。

Conclusion: 该方法提供了一种通用且硬件高效的解决方案，能够消除任意PQC中的barren plateaus现象，并且对NISQ硬件中的噪声具有鲁棒性。

Abstract: Quantum algorithms based on parameterized quantum circuits (PQCs) have enabled a wide range of applications on near-term quantum devices. However, existing PQC architectures face several challenges, among which the ``barren plateaus" phenomenon is particularly prominent. In such cases, the loss function concentrates exponentially with increasing system size, thereby hindering effective parameter optimization. To address this challenge, we propose a general and hardware-efficient method for eliminating barren plateaus in an arbitrary PQC. Specifically, our approach achieves this by inserting a layer of easily implementable quantum channels into the original PQC, each channel requiring only one ancilla qubit and four additional gates, yielding a modified PQC (MPQC) that is provably at least as expressive as the original PQC and, under mild assumptions, is guaranteed to be free from barren plateaus. Furthermore, by appropriately adjusting the structure of MPQCs, we rigorously prove that any parameter in the original PQC can be made trainable. Importantly, the absence of barren plateaus in MPQCs is robust against realistic noise, making our approach directly applicable to current noisy intermediate-scale quantum (NISQ) hardware. Numerically, we demonstrate the practicality of our method by modifying a commonly used PQC for thermal-state preparation. The results show that {barren plateaus are effectively eliminated} in this class of circuits with up to 100 qubits and 2400 layers, whereas the original ansatz suffers from severe gradient vanishing.

</details>


### [348] [Emulation of the Six-State Quantum Key Distribution Protocol with Pulsed Lasers](https://arxiv.org/abs/2511.13413)
*Sara P. Gandelman,Georgi Gary Rozenman*

Main category: quant-ph

TL;DR: 提出了一个探索六态量子密钥分发协议的清晰易用框架，这是BB84方案的三基扩展，结合了光学实验和计算分析。


<details>
  <summary>Details</summary>
Motivation: 量子密码学具有持久的科学和教育价值，需要为测试量子通信协议提供一个稳健且成本效益高的平台。

Method: 通过仿真测试量子通信协议，结合光学实验与计算分析，在受控的桌面环境中展示多基编码的基本原理。

Result: 该方法成功展示了实验测量如何直接与理论预期相连接，为量子密钥分发协议提供了实用的测试平台。

Conclusion: 该框架为探索六态量子密钥分发协议提供了一个有效且易于理解的方法，强调了多基编码的基本原理及其在量子通信中的应用。

Abstract: Quantum cryptography remains a topic of enduring scientific and educational interest. Here, we present a clear and accessible framework for exploring the six-state quantum key distribution protocol, an enhanced three-basis extension of the BB84 scheme that combines optical experiments with computational analysis. Designed for testing quantum communication protocols through emulation, this approach provides a robust and cost-effective platform that highlights the fundamental principles of multi-basis encoding and demonstrates how experimental measurements connect directly to theoretical expectations in a controlled tabletop setting.

</details>


### [349] [Probing parameters estimation with Gaussian non-commutative measurements](https://arxiv.org/abs/2511.13451)
*Alice P. G. Hall,Carlos H. S. Vieira,Jonas F. G. Santos*

Main category: quant-ph

TL;DR: 该论文研究了高斯量子态的参数估计，通过在探针态制备阶段引入可调参数的非对易高斯测量，并分析其对量子Fisher信息的影响，发现通过调整不确定性参数可以提升信道表征精度。


<details>
  <summary>Details</summary>
Motivation: 高斯量子态和信道在量子科学中应用广泛，但如何通过探针态制备优化参数估计精度是一个重要问题，特别是利用非对易高斯测量和量子相干性来提升量子Fisher信息。

Method: 在探针态制备阶段引入位置和动量观测量的非对易高斯测量，通过量子Fisher信息分析这些测量的影响，并将协议应用于衰减和放大两个典型单模高斯信道。

Result: 研究表明通过调整探针态制备中的不确定性参数可以增加高斯信道表征的量子Fisher信息；当探针初始处于热态时，探针态制备可产生能量基中的量子相干性，且相干性大小及其对参数的敏感度都影响QFI的提升。

Conclusion: 该工作为量子计量学中相干性的应用提供了新见解，所提出的探针态协议在量子光学设备中具有实验可行性，有助于优化高斯量子信道的参数估计性能。

Abstract: Gaussian quantum states and channels are pivotal across many branches of quantum science and their applications, including the processing and storage of quantum information, the investigation of thermodynamics in the quantum regime, and quantum computation. The great advantage is that Gaussian states are experimentally accessible via their first and second statistical moments. In this work, we investigate parameter estimation for Gaussian states, in which the probe-state preparation stage involves two noncommutative Gaussian measurements on the position and momentum observables, introducing tunable parameters. The influence of these noncommutative Gaussian measurements is investigated through the quantum Fisher information (QFI). We showed that the QFI for characterizing Gaussian channels can be increased by adjusting the uncertainty parameters in the preparation of the probe state. Furthermore, if the probe is initially in a thermal state, probe-state preparation may generate quantum coherence in its energy basis. We showed that not only does the amount of coherence affect the improvement of the QFI, but also the rate of change of the coherence with respect to the parameter to be estimated. The proposed probe-state protocol is applied to two paradigmatic single-mode Gaussian channels, the attenuator and amplification channels, which are building blocks of Gaussian quantum information. Our results contribute to the use of coherence in quantum metrology and are experimentally feasible in quantum-optical devices.

</details>


### [350] [Machine learning inspired photon number resolution in superconducting nanowire single-photon detectors](https://arxiv.org/abs/2511.13475)
*I. S. Kuijf,F. B. Baalbergen,L. Seldenthuis,E. P. L. van Nieuwenburg,M. J. A. de Dood*

Main category: quant-ph

TL;DR: 本文提出了一个系统框架来评估超导纳米线单光子探测器的光子数分辨能力，通过主成分分析和新的读出技术，发现光子数信息包含在单个主成分中，并引入了基于Bhattacharyya系数的置信度指标来量化比较不同系统的性能。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏系统框架来解读和基准测试超导纳米线单光子探测器的光子数分辨能力，需要建立标准化的评估方法。

Method: 结合主成分分析（PCA）与新的读出技术，分析SNSPD的光子数分辨能力，并引入基于Bhattacharyya系数的置信度指标。

Result: 发现光子数信息包含在单个主成分中，该成分近似于平均响应轨迹的时间导数，且可在中等硬件要求（5 GSample/sec采样率、3 GHz模拟带宽）下实现光子数分辨。

Conclusion: 该方法为SNSPD的光子数分辨能力提供了系统评估框架，可在FPGA中实现高度可扩展的实时光子计数解决方案。

Abstract: Photon-number resolved detection with superconducting nanowire single-photon detectors (SNSPDs) attracts increasing interest, but lacks a systematic framework for interpreting and benchmarking this capability. In this work, we combine principal component analysis (PCA) with a new readout technique to explore the photon-number resolving capabilities of SNSPDs and find that the information of the photon number is contained in a single principal component which approximates the time derivative of the average response trace. We introduce a new confidence metric based on the Bhattacharyya coefficient to quantify the photon-number-resolving capabilities of a detector system and show that this metric can be used to compare different systems. Our analysis and interpretation of the principal components imply that photon-number resolution in SNSPDs can be achieved with moderate hardware requirements in terms of both sample rate (5 GSample/sec) and analog bandwidth (3 GHz) and could be implemented in an FPGA, giving a highly scalable solution for real-time photon counting.

</details>


### [351] [Spin-Adapted Fermionic Unitaries: From Lie Algebras to Compact Quantum Circuits](https://arxiv.org/abs/2511.13485)
*Ilias Magoulas,Francesco A. Evangelista*

Main category: quant-ph

TL;DR: 本文提出了一种基于李代数技术的对称性保持量子电路设计方法，解决了化学模拟中点群对称性和自旋对称性交织的难题，并开发了最小通用对称性适配算子池来减少量子资源需求。


<details>
  <summary>Details</summary>
Motivation: 在经典和量子多体系统模拟中，对称性守恒至关重要，但设计能同时保持化学中常见对称性的高效量子电路一直是个挑战，主要由于点群对称性和自旋对称性的相互作用。

Method: 利用李代数技术推导出表示对称性适配酉算子的精确乘积公式，基于这些分解设计对称性保持量子电路，并引入最小通用对称性适配算子池。

Result: 开发了迄今为止最高效的对称性保持量子电路，显著减少了优化参数数量。

Conclusion: 该方法通过李代数技术和对称性适配算子池，成功解决了化学量子模拟中的对称性保持问题，大幅降低了量子资源需求。

Abstract: Conservation of symmetries plays a crucial role in both classical and quantum simulations of many-body systems, enabling the tracking of states with specific symmetry properties and leading to substantial reductions in the number of optimization parameters. The design of efficient quantum circuits that enforce all symmetries typically encountered in chemistry has remained elusive, mainly due to the interplay of point group and spin symmetries. By exploiting Lie algebraic techniques, we derive exact product formulas representing symmetry-adapted unitaries. These decompositions allow us to design the most efficient symmetry-preserving quantum circuits to date. Finally, we introduce a minimum universal symmetry-adapted operator pool to further reduce the required quantum resources.

</details>


### [352] [Towards Quantum Software for Quantum Simulation](https://arxiv.org/abs/2511.13520)
*Maja Franz,Lukas Schmidbauer,Joshua Ammermann,Ina Schaefer,Wolfgang Mauerer*

Main category: quant-ph

TL;DR: 量子模拟是展示量子优势的主要候选者，但目前缺乏通用软件框架。本文提出采用模块化模型驱动工程方法构建量子模拟框架，以支持不同类型模拟并促进自动化和可重用性。


<details>
  <summary>Details</summary>
Motivation: 量子模拟被认为能提供比经典计算指数级更强的计算能力，但目前主要限于基础科学研究，缺乏软件工程社区为其他计算领域提供的基础设施和建模抽象。

Method: 采用模块化模型驱动工程方法，支持不同类型的量子模拟，促进自动化、性能评估和可重用性。通过高能物理示例展示了支持可扩展、跨平台模拟工作流的量子模拟框架愿景。

Result: 识别了量子模拟软件栈中的关键差距，特别是缺乏用于模型规范、哈密顿量构建和硬件感知映射的通用框架。

Conclusion: 需要构建通用的量子模拟软件框架，采用模型驱动工程方法来解决当前量子模拟在基础设施和建模抽象方面的不足，以推动量子模拟从基础科学走向更广泛的应用。

Abstract: Quantum simulation is a leading candidate for demonstrating practical quantum advantage over classical computation, as it is believed to provide exponentially more compute power than any classical system. It offers new means of studying the behaviour of complex physical systems, for which conventionally software-intensive simulation codes based on numerical high-performance computing are used. Instead, quantum simulations map properties and characteristics of subject systems, for instance chemical molecules, onto quantum devices that then mimic the system under study.
  Currently, the use of these techniques is largely limited to fundamental science, as the overall approach remains tailored for specific problems: We lack infrastructure and modelling abstractions that are provided by the software engineering community for other computational domains.
  In this paper, we identify critical gaps in the quantum simulation software stack-particularly the absence of general-purpose frameworks for model specification, Hamiltonian construction, and hardware-aware mappings. We advocate for a modular model-driven engineering (MDE) approach that supports different types of quantum simulation (digital and analogue), and facilitates automation, performance evaluation, and reusability. Through an example from high-energy physics, we outline a vision for a quantum simulation framework capable of supporting scalable, cross-platform simulation workflows.

</details>


### [353] [Simultaneous variances of Pauli strings, weighted independence numbers, and a new kind of perfection of graphs](https://arxiv.org/abs/2511.13531)
*Zhen-Peng Xu,Jie Wang,Qi Ye,Gereon Koßmann,René Schwonnek,Andreas Winter*

Main category: quant-ph

TL;DR: 该论文研究了量子信息与图论之间的接口，通过引入ħ-完美图类来扩展完美图和h-完美图，并探索了其在纠缠检测、阴影层析复杂性、不确定性关系和基态能量计算等方面的应用。


<details>
  <summary>Details</summary>
Motivation: 建立图论与量子信息之间的自然接口，通过图的挫折图来刻画泡利弦集合的交换性结构，探索这一接口在量子计算和量子信息处理中的多方面应用。

Method: 引入ħ-完美图类，研究其在基本图操作下的行为，评估其在所有图中的普遍性，并开发基于ħ-完美图的高效算法和应用方案。

Result: ħ-完美图为纠缠检测、阴影层析复杂性分析、紧致不确定性关系和基态能量下界计算提供了高效方案，同时诱导了计算独立数的量子算法。

Conclusion: ħ-完美图类在量子信息处理中具有重要应用价值，能够以对数级量子比特数量实现独立数的近似哈密顿编码，为量子算法设计提供了新的思路。

Abstract: A set of Pauli stings is well characterized by the graph that encodes its commutatitivity structure, i.e., by its frustration graph. This graph provides a natural interface between graph theory and quantum information, which we explore in this work. We investigate all aspects of this interface for a special class of graphs that bears tight connections between the groundstate structures of a spin systems and topological structure of a graph. We call this class $\hbar$-perfect, as it extends the class of perfect and $h$-perfect graphs.
  Having an $\hbar$-perfect graph opens up several applications: we find efficient schemes for entanglement detection, a connection to the complexity of shadow tomography, tight uncertainty relations and a construction for computing good lower on bounds ground state energies. Conversely this also induces quantum algorithms for computing the independence number. Albeit those algorithms do not immediately promise an advantage in runtime, we show that an approximate Hamilton encoding of the independence number can be achieved with an amount of qubits that typically scales logarithmically in the number of vertices. We also we also determine the behavior of $\hbar$-perfectness under basic graph operations and evaluate their prevalence among all graphs.

</details>


### [354] [Measurement-based Dynamical Decoupling for Fidelity Preservation on Large-scale Quantum Processors](https://arxiv.org/abs/2511.13532)
*Jeongwoo Jae,Changwon Lee,Juzar Thingna,Yeong-Dae Kwon,Daniel K. Park*

Main category: quant-ph

TL;DR: 提出了基于测量的动力学解耦(MDD)协议，通过部分测量噪声子系统来确定控制幺正门，测量开销与子系统数量呈线性关系。在IBM Eagle处理器上，MDD在14量子比特量子傅里叶变换中实现了高达450倍的成功概率提升。


<details>
  <summary>Details</summary>
Motivation: 动力学解耦是抑制退相干和保持量子算法性能的关键技术，但传统方法在大规模量子系统中面临可扩展性挑战。

Method: 基于测量的动力学解耦协议，通过部分测量噪声子系统来确定控制幺正门，测量开销与子系统数量呈线性关系。

Result: 在IBM Eagle处理器上，MDD在14量子比特量子傅里叶变换中实现了高达450倍的成功概率提升，在56量子比特基于采样的量子对角化中对N2的基态能量估计精度也有所提高。

Conclusion: MDD被证明是一种可扩展且有效的方法，可用于抑制大规模量子算法中的退相干。

Abstract: Dynamical decoupling (DD) is a key technique for suppressing decoherence and preserving the performance of quantum algorithms. We introduce a measurement-based DD (MDD) protocol that determines control unitary gates from partial measurements of noisy subsystems, with measurement overhead scaling linearly with the number of subsystems. We prove that, under local energy relaxation and dephasing noise, MDD achieves the maximum entanglement fidelity attainable by any DD scheme based on bang-bang operations to first order in evolution time. On the IBM Eagle processor, MDD achieved up to a $450$-fold improvement in the success probability of a $14$-qubit quantum Fourier transform, and improved the accuracy of ground-state energy estimation for $N_2$ in the $56$-qubit sample-based quantum diagonalization compared with the standard XX-pulse DD. These results establish MDD as a scalable and effective approach for suppressing decoherence in large-scale quantum algorithms.

</details>


### [355] [Sequences of Bivariate Bicycle Codes from Covering Graphs](https://arxiv.org/abs/2511.13560)
*Benjamin C. B. Symons,Abhishek Rajput,Dan E. Browne*

Main category: quant-ph

TL;DR: 本文提出了一种基于图覆盖的方法，可以从一个基础双变量自行车(BB)码生成无限序列的新BB码，称为h-覆盖码，并建立了基础码与覆盖码之间的代数条件和参数关系。


<details>
  <summary>Details</summary>
Motivation: 为了扩展BB码的构造空间并发现新的好码，同时建立系统化的方法来生成和分析覆盖码族。

Method: 通过图覆盖技术，将基础BB码的Tanner图进行h-重覆盖，构造h-覆盖码，并利用链映射在基础码和覆盖码之间建立投影和提升映射。

Result: 发现了许多有趣的BB码实例，如[[144,12,12]] gross码都可以视为覆盖码，并找到了具有权重8校验的BB码，包括[[64,14,8]]和[[144,14,14]]码。

Conclusion: h-覆盖码的参数满足k_h ≥ k和d_h ≤ hd（当h为奇数时），且当k_h = k时，d ≤ d_h。该方法可推广到其他群代数码和超图、提升、平衡积等码构造。

Abstract: We show that given an instance of a bivariate bicycle (BB) code, it is possible to generate an infinite sequence of new BB codes using increasingly large covering graphs of the original code's Tanner graph. When a BB code has a Tanner graph that is a $h$-fold covering of the base BB code's Tanner graph, we refer to it as a $h$-cover code. We show that for a BB code to be a $h$-cover code, its lattice parameters and defining polynomials must satisfy simple algebraic conditions relative to those of the base code. By extending the graph covering map to a chain map, we show there are induced projection and lifting maps on (co)homology that enable the projection and lifting of logical operators and, in certain cases, automorphisms between the base and the cover code. The search space of cover codes is considerably reduced compared to the full space of possible polynomials and we find that many interesting examples of BB codes, such as the $[[144,12,12]]$ gross code, can be viewed as cover codes. We also apply our method to search for BB codes with weight 8 checks and find many codes, including a $[[64,14,8]]$ and $[[144,14,14]]$ code. For an $h$-cover code of an $[[n,k,d]]$ BB code with parameters $[[n_h = hn, k_h, d_h]]$, we prove that $k_h \geq k$ and $d_h \leq hd$ when $h$ is odd. Furthermore if $h$ is odd and $k_h = k$, we prove the lower bound $d \leq d_h$. We conjecture it is always true that an $h$-cover BB code of a base $[[n,k,d]]$ BB code has parameters $[[n_h = hn, k_h \geq k, d \leq d_h \leq hd]]$. While the focus of this work is on bivariate bicycle codes, we expect these methods to generalise readily to many group algebra codes and to certain code constructions involving hypergraph, lifted, and balanced products.

</details>


### [356] [Qudit-native simulation of the Potts model](https://arxiv.org/abs/2511.13572)
*Maxim A. Gavreev,Evgeniy O. Kiktenko,Aleksey K. Fedorov,Anastasiia S. Nikolaeva*

Main category: quant-ph

TL;DR: 提出了基于Suzuki-Trotter分解的Potts模型模拟方法，引入了两种qudit原生分解方案，用于在囚禁离子平台上高效映射Potts模型动力学。


<details>
  <summary>Details</summary>
Motivation: 模拟纠缠多体量子系统非常困难，特别是在物理对象具有高维性质的情况下。

Method: 基于Suzuki-Trotter分解构建了qudit系统的两种分解方案：(i)使用Molmer-Sorensen门和额外局部能级编码Potts相互作用；(ii)采用光移门自然适配qudit架构。

Result: 实现了Potts模型动力学到硬件高效qudit门序列的直接高效映射，并展示了使用Suzuki-Trotter近似检测动态量子相变。

Conclusion: 为基于qudit的多体模型数字量子模拟建立了途径，并为探测高维量子多体模型中的非解析行为提供了新视角。

Abstract: Simulating entangled, many-body quantum systems is notoriously hard, especially in the case of high-dimensional nature of physical underlying objects. In this work, we propose an approach for simulating the Potts model based on the Suzuki-Trotter decomposition that we construct for qudit systems. Specifically, we introduce two qudit-native decomposition schemes: (i) the first utilizes Molmer-Sorensen gate and additional local levels to encode the Potts interactions, while (ii) the second employs an light-shift gate that naturally fits qudit architectures. These decompositions enable a direct and efficient mapping of the Potts model dynamics into hardware-efficient qudit gate sequences for trapped-ion platform. Furthermore, we demonstrate the use of a Suzuki-Trotter approximation with our evolution-into-gates framework, for detecting the dynamical quantum phase transition. Our results establish a pathway toward qudit-based digital quantum simulation of many-body models and provide a new perspective on probing nonanalytic behavior in high-dimensional quantum many-body models.

</details>


### [357] [Long-range entanglement and quantum correlations in a multi-frequency comb system](https://arxiv.org/abs/2511.13604)
*Sahil Pontula,Debasmita Banerjee,Marin Soljacic,Yannick Salamin*

Main category: quant-ph

TL;DR: 该论文提出了一种通过级联三波混频过程生成多个非线性耦合频率梳的理论机制，能够在从紫外到中红外的宽光谱范围内产生梳间和梳内双模压缩与纠缠。


<details>
  <summary>Details</summary>
Motivation: 虽然单频率梳系统的量子压缩和纠缠已被探索，但跨多个频率梳的量子光产生仍然未被研究。

Method: 通过单个闲频梳介导的级联三波上转换和下转换过程，生成一系列非线性耦合的频率梳，并利用协方差矩阵优化来按需产生多模量子光。

Result: 该系统能够在很宽的光谱范围（从紫外到中红外）产生梳间和梳内双模压缩与纠缠。

Conclusion: 这一发现可能实现可调谐宽带鬼光谱协议、压缩增强的泵浦-探测测量以及光谱复用信息量子之间的宽带纠缠。

Abstract: Frequency combs are multimode photonic systems that underlie countless precision sensing and metrology applications. Since their invention over two decades ago, numerous efforts have pushed frequency combs to broader bandwidths and more stable operation. More recently, quantum squeezing and entanglement have been explored in single frequency comb systems for quantum advantages in sensing and signal multiplexing. However, the production of quantum light across multiple frequency combs remains unexplored. In this work, we theoretically explore a mechanism that generates a series of nonlinearly coupled frequency combs through cascaded three-wave upconversion and downconversion processes mediated by a single idler comb. We show how this system generates inter- and intracomb two-mode squeezing and entanglement spanning a very large spectral range, from ultraviolet to mid-IR frequencies. Finally, we show how this system can be engineered to produce on-demand multimode quantum light through covariance matrix optimization. Our findings could enable tunable broadband ghost spectroscopy protocols, squeezing-enhanced pump-probe measurements, and broadband entanglement between spectrally-multiplexed quanta of information.

</details>


### [358] [Modeling Quantum Noise in Nanolasers using Markov Chains](https://arxiv.org/abs/2511.13622)
*Matias Bundgaard-Nielsen,Gian Luca Lippi,Jesper Mørk*

Main category: quant-ph

TL;DR: 该论文提出了一种基于马尔可夫链的激光量子噪声计算方法，适用于从纳米激光到宏观激光的广泛范围，相比传统的朗之万方程方法在阈值以下更准确。


<details>
  <summary>Details</summary>
Motivation: 传统朗之万方程方法无法准确描述纳米激光的量子噪声，需要一种更普适的量子噪声计算方法。

Method: 从简单的速率方程出发，假设光子和激发电子只能取离散值，构建马尔可夫链模型，该模型可以从完整主方程推导得出。

Result: 马尔可夫链模型在所有泵浦值和激光尺寸下都准确（排除集体发射效应时），而朗之万方程在阈值以下不准确。在大量光子极限下，模型简化为朗之万方程。

Conclusion: 提出的激光马尔可夫链模型为计算激光量子噪声提供了准确且普适的方法，特别适用于纳米激光和阈值以下区域。

Abstract: The random nature of spontaneous emission leads to unavoidable fluctuations in a laser's output. This is often included through random Langevin forces in laser rate equations, but this approach falls short for nanolasers. In this paper, we show that the laser quantum noise can be quantitatively computed for a very broad class of lasers by starting from simple and intuitive rate equations and merely assuming that the number of photons and excited electrons only takes discrete values. The success of the model is explained by showing that it constitutes a Markov chain, which can be derived from the full master equations. We show that in the many-photon limit, the model simplifies to Langevin equations. We perform an extensive comparison of different approaches for computing quantum noise in lasers, identifying the best approach for different system sizes, ranging from nanolasers to macroscopic lasers, and different levels of excitation, i.e., cavity photon number. In particular, we find that the numerical solution to the Langevin equations is inaccurate below the laser threshold, while the laser Markov chain model, on the other hand, is accurate for all pump values and laser sizes when collective emitter effects are excluded.

</details>


### [359] [Architectural Approaches to Fault-Tolerant Distributed Quantum Computing and Their Entanglement Overheads](https://arxiv.org/abs/2511.13657)
*Nitish Kumar Chandra,Eneet Kaur,Kaushik P. Seshadreesan*

Main category: quant-ph

TL;DR: 分析分布式量子计算中三种容错架构的资源需求，使用表面码和环面码作为示例，评估不同架构在增加码距时的贝尔对数量和生成尝试次数的扩展特性。


<details>
  <summary>Details</summary>
Motivation: 随着量子硬件向模块化和网络化架构发展，需要在分布式量子计算平台上评估容错量子计算的资源需求和噪声阈值，为近期硬件和资源约束下的架构选择提供指导。

Method: 将容错分布式量子计算架构分为三类：类型1使用GHZ态连接小量子节点进行非局域稳定子测量；类型2将大纠错码块分布到多个模块，边界稳定子测量使用非局域CNOT门；类型3将码块分配到不同模块，通过横向门、格点手术和隐形传态实现逻辑操作。

Result: 分析了平面表面码和环面码在不同架构下资源需求（特别是贝尔对数量和平均生成尝试次数）随码距增加的扩展特性。

Conclusion: 该分析为在近期硬件和资源约束下选择适合容错分布式量子计算的架构提供了有价值的见解。

Abstract: Fault tolerant quantum computation over distributed quantum computing (DQC) platforms requires careful evaluation of resource requirements and noise thresholds. As quantum hardware advances toward modular and networked architectures, various fault tolerant DQC schemes have been proposed, which can be broadly categorized into three architectural types. Type 1 architectures consist of small quantum nodes connected via Greenberger-Horne-Zeilinger (GHZ) states, enabling nonlocal stabilizer measurements. Type 2 architectures distribute a large error correcting code block across multiple modules, with most stabilizer measurements remaining local, except for a small subset at patch boundaries that are performed using nonlocal CNOT gates. Type 3 architectures assign code blocks to distinct modules and can perform fault tolerant operations such as transversal gates, lattice surgery, and teleportation to implement logical operations between code blocks. Using the planar surface code and toric code as representative examples, we analyze how the resource requirements, particularly the number of Bell pairs and the average number of generation attempts, scale with increasing code distance across different architectural designs. This analysis provides valuable insights for identifying architectures well suited to fault tolerant distributed quantum computation under near term hardware and resource constraints.

</details>


### [360] [Quantum Advantage in Learning Mixed Unitary Channels](https://arxiv.org/abs/2511.13683)
*Yue Tu,Liang Jiang*

Main category: quant-ph

TL;DR: 本文研究了在不同量子资源假设下使用Fisher信息学习混合酉通道的任务，发现渐近样本复杂度与通道秩r、系统维度d和均方误差ε²成比例关系。


<details>
  <summary>Details</summary>
Motivation: 研究混合酉通道的学习任务，探索在不同量子资源（包括辅助系统和级联）假设下的学习效率。

Method: 使用Fisher信息分析混合酉通道的学习，考虑不同的量子资源设置，包括辅助系统和级联操作。

Result: 渐近样本复杂度为r/(dε²)，其中r是通道秩，d是系统维度，ε²是均方误差。辅助系统是关键资源，随机混合酉通道容易学习。

Conclusion: 辅助系统是学习混合酉通道的关键资源，通道秩r也是重要因素，随机混合酉通道具有实际学习潜力。

Abstract: We study the task of learning mixed unitary channels using Fisher information, under different quantum resource assumptions including ancilla and concatenation. Our result shows that the asymptotic sample complexity scales as $\frac{r}{d\varepsilon^2}$, where $r$ is the rank of the channel (i.e.\ the number of different unitaries), $d$ is the dimension of the system, and $\varepsilon^2$ is the mean-square error. Thus the critical resource is the ancilla, which mirrors the result in~\cite{chen2022quantum} but in a more precise form, as we point out that $r$ is also important. Additionally, we demonstrate the practical potential of mixed unitary channels by showing that random mixed unitary channels are easy to learn.

</details>


### [361] [Network Operations Scheduling for Distributed Quantum Computing](https://arxiv.org/abs/2511.13687)
*Nitish Kumar Chandra,Eneet Kaur,Kaushik P. Seshadreesan*

Main category: quant-ph

TL;DR: 本文比较了两种调度方法（RCPSP框架和贪心启发式算法）在量子计算分布式架构中的性能，发现在某些情况下RCPSP方法表现更优，而在其他情况下两者表现相当。


<details>
  <summary>Details</summary>
Motivation: 实现量子计算的分布式架构对于扩展计算能力至关重要，需要调度器来协调量子处理单元之间的非局域纠缠门操作，以最小化完成时间。

Method: 采用两种方法：基于资源约束项目调度（RCPSP）框架的方法和贪心启发式算法。工作流程包括电路分区、非局域纠缠门识别和网络操作调度。

Result: 在一个实例中，RCPSP方法优于贪心启发式；在另一个实例中，两种方法表现相当。

Conclusion: RCPSP框架在量子计算调度中具有有效性，同时贪心启发式算法也具有相关性和实用性。

Abstract: Realizing distributed architectures for quantum computing is crucial to scaling up computational power. A key component of such architectures is a scheduler that coordinates operations over a short-range quantum network required to enable the necessary non-local entangling gates between quantum processing units (QPUs). It is desirable to determine schedules of minimum make span, which in the case of networks with constrained resources hinges on their efficient usage. Here we compare and contrast two approaches to solving the make span minimization problem, an approach based on the resource constrained project scheduling (RCPSP) framework, and another based on a greedy heuristic algorithm. The workflow considered is as follows. Firstly, the computational circuit is partitioned and assigned to different QPUs such that the number of nonlocal entangling gates acting across partitions is minimized while the qubit load is nearly uniform on the individual QPUs, which can be accomplished using, e.g., the METIS solver. Secondly, the nonlocal entangling gate requirements with respect to the partitions are identified, and mapped to network operation sequences that deliver the necessary entanglement between the QPUs. Finally, the network operations are scheduled such that the make span is minimized. As illustrative examples, we analyze the implementation of a small instance of the Quantum Fourier Transform algorithm over instances of a simple hub and spoke (star) network architecture comprised of a quantum switch as the hub and QPUs as spokes, each with a finite qubit resource budget. In one instance, our results show the RCPSP approach outperforming the greedy heuristic. In another instance, we find the two performing equally well. Our results thus illustrate the effectiveness of the RCPSP framework, while also underlining the relevance and usefulness of greedy heuristics.

</details>


### [362] [Ultra Low Overhead Syndrome Extraction for the Steane code](https://arxiv.org/abs/2511.13700)
*Boldizsár Poór,Benjamin Rodatz,Aleks Kissinger*

Main category: quant-ph

TL;DR: 提出了[[7,1,3]] Steane码的容错症状提取动态协议新基准，通过自适应响应内部故障，相比现有最优方法平均降低逻辑错误率约14.3%-17.7%。


<details>
  <summary>Details</summary>
Motivation: 提高量子纠错码的容错性能，通过动态协议优化故障处理效率。

Method: 使用基于故障等价ZX重写的两种优化电路：14个CNOT的主容错电路和11个CNOT的高效非容错恢复电路，采用自适应响应机制处理内部故障。

Result: 蒙特卡洛模拟显示，相比优化Steane方法和Reichardt三量子比特方法，逻辑错误率每周期平均降低约14.3%和17.7%。

Conclusion: 动态协议显著提升了[[7,1,3]] Steane码的容错性能，为量子纠错提供了更高效的解决方案。

Abstract: We establish a new performance benchmark for the fault-tolerant syndrome extraction of [[7, 1, 3]] Steane code with a dynamic protocol. Our method is built on two highly optimized circuits derived using fault-equivalent ZX-rewrites: a primary fault-tolerant circuit with 14 CNOTs and an efficient non-fault-tolerant recovery circuit with 11 CNOTs. The protocol uses an adaptive response to internal faults, discarding flagged measurements and falling back to the recovery circuit to correct potentially detrimental errors. Monte Carlo simulations confirm the efficiency of our protocol, reducing the logical error rate per cycle by an average of ~14.3% relative to the optimized Steane method [arXiv:2506.17181] and ~17.7% compared to the Reichardt's three-qubit method [arXiv:1804.06995], the leading prior techniques.

</details>


### [363] [Quantum Error Correction Codes for Truncated SU(2) Lattice Gauge Theories](https://arxiv.org/abs/2511.13721)
*Xiaojun Yao*

Main category: quant-ph

TL;DR: 构建了两种用于SU(2)格点规范理论的量子纠错码，适用于多种晶格结构，能够纠正单量子比特错误，并将SU(2)哈密顿量表示为逻辑门。


<details>
  <summary>Details</summary>
Motivation: 为纯SU(2)格点规范理论开发量子纠错码，在电通量截断为j_max=1/2的情况下，在多种晶格结构上实现量子错误纠正。

Method: 构造了两种量子纠错码：第一种将每个顶点的Gauss定律转换为稳定子；第二种仅使用一半顶点，局部为碳码。两种码都能纠正单量子比特错误。

Result: 成功构建了两种量子纠错码，将SU(2)哈密顿量的电和磁项表示为逻辑门。第一种码的逻辑门哈密顿量与先前工作中发现的规范单重态的自旋哈密顿量完全匹配。

Conclusion: 所提出的两种量子纠错码为SU(2)格点规范理论提供了有效的量子错误纠正方案，并在多种晶格结构上实现了哈密顿量的逻辑门表示。

Abstract: We construct two quantum error correction codes for pure SU(2) lattice gauge theory in the electric basis truncated at the electric flux $j_{\rm max}=1/2$, which are applicable on quasi-1D plaquette chains, 2D honeycomb and 3D triamond and hyperhoneycomb lattices. The first code converts Gauss's law at each vertex into a stabilizer while the second only uses half vertices and is locally the carbon code. Both codes are able to correct single-qubit errors. The electric and magnetic terms in the SU(2) Hamiltonian are expressed in terms of logical gates in both codes. The logical-gate Hamiltonian in the first code exactly matches the spin Hamiltonian for gauge singlet states found in previous work.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [364] [On the Complexified Spacetime Manifold Mapping of AdS to dS](https://arxiv.org/abs/2511.11658)
*J. W. Moffat,E. J. Thompson*

Main category: gr-qc

TL;DR: 通过复流形中的解析延拓连接反德西特和德西特时空，保持几何不变量和正则性，避免AdS-dS转换中的奇点。在复化对称群下统一引力和规范相互作用，保持AdS和dS的体单元性。


<details>
  <summary>Details</summary>
Motivation: 统一AdS和dS时空的几何描述，在复化框架下理解引力和规范相互作用的统一性，探索全息原理在两种时空中的应用。

Method: 使用复流形中的解析延拓方法，应用全息原理如MacDowell-Mansouri和量子极值曲面公式，对齐纠缠熵和黑洞熵与AdS/CFT及广义相对论。

Result: 成功桥接AdS和dS时空，保持几何正则性；边界单元性在AdS中保持但在dS中不保持；为AdS和dS全息、宇宙学常数及量子引力单元性和纠缠提供新见解。

Conclusion: HUFT理论为理解AdS和dS全息、宇宙学常数问题以及量子引力中的单元性和纠缠性质提供了统一框架，揭示了两种时空之间的深层联系。

Abstract: In a complex manifold, one can bridge anti-de Sitter and de Sitter spacetimes via analytic continuation, preserving geometric invariants and regularity, avoiding singularities during the AdS-dS transition. It unifies gravitational and gauge interactions under a complexified symmetry group, maintaining bulk unitarity for both AdS and dS. Boundary unitarity is upheld in AdS but not in dS due to the spacelike conformal boundary. The theory uses holographic principles like the MacDowell-Mansouri and Quantum Extremal Surface prescriptions to align entanglement and black hole entropy with AdS/CFT and general relativity. HUFT provides insights into AdS and dS holography, the cosmological constant, and quantum gravity unitarity and entanglement.

</details>


### [365] [The Third Law of Black Hole Dynamics in Pure Lovelock Gravity](https://arxiv.org/abs/2511.11695)
*Jyotirmoy De,Chiranjeeb Singha,Naresh Dadhich*

Main category: gr-qc

TL;DR: 本文研究了纯Lovelock引力理论中带电黑洞的第三定律，证明通过有限经典过程无法达到极值状态。


<details>
  <summary>Details</summary>
Motivation: 验证黑洞动力学第三定律在纯Lovelock引力理论中的有效性，特别是研究带电黑洞能否通过有限物理过程达到表面重力为零的极值状态。

Method: 通过研究质量和电荷的无穷小变化，推导出一组约束这些变化的不等式，分析表面重力趋近于零时的可容许扰动范围。

Result: 当表面重力趋近于零时，可容许扰动的范围逐渐减小，阻止通过任何有限经典过程达到极值状态。不等式的饱和被解释为极值附近出现的动力学屏障。

Conclusion: 在纯Lovelock引力理论中，静态球对称带电黑洞的第三定律成立，无法通过有限物理过程将黑洞的表面重力降为零。

Abstract: The third law of black hole dynamics states that it is impossible, through any finite sequence of physical processes, to reduce the surface gravity of a black hole to zero. In this work, we examine the validity of this law for static, spherically symmetric charged black holes in the pure Lovelock theory of gravity. By studying infinitesimal variations in mass and charge, we derive a set of inequalities that constrain these variations. Our analysis shows that as the surface gravity approaches zero ($κ\to 0$), the range of admissible perturbations gradually diminishes, thereby forbidding the attainment of extremality through any finite classical process. The saturation of the inequality is interpreted as the emergence of a dynamical barrier near extremality, which prevents further evolution toward the extremal configuration.

</details>


### [366] [Thermal properties of Klein-Gordon Oscillator in the Context of Amelino-Camelia and Magueijo-Smolin Doubly Special Relativity (DSR) frameworks](https://arxiv.org/abs/2511.11709)
*Abdelmalek Boumali,Nosratollah Jafari,Bekdaulet Shukirgaliyev,Fadila Serdouk*

Main category: gr-qc

TL;DR: 本文研究了在一维Klein-Gordon振荡器中，两种双狭义相对论(DSR)框架下的热力学性质，发现普朗克尺度修正会在比热峰值位置和大小上产生可分辨的差异，这些峰值对应平滑的Schottky型异常而非相变。


<details>
  <summary>Details</summary>
Motivation: 比较Amelino-Camelia和Magueijo-Smolin两种DSR框架在相对论量子系统中的热力学行为差异，为区分不同DSR处方提供诊断框架。

Method: 使用两种DSR框架的修正色散关系，推导正能谱，通过Euler-Maclaurin方法构建配分函数，计算比热等热力学量作为温度和变形尺度的函数。

Result: 两种模型在比热峰值位置和大小上表现出不同的可分辨偏移，熵分析显示这些峰值对应平滑的Schottky型异常，比热曲线在探索温度范围内保持解析和正值。

Conclusion: 研究结果为区分DSR处方提供了稳健的诊断框架，并强化了其热响应无相变的特性。

Abstract: We examine the thermal and statistical properties of the one dimensional Klein-Gordon oscillator within two prominent Doubly Special Relativity (DSR) frameworks: Amelino-Camelia and Magueijo-Smolin. Using the modified dispersion relations specific to each formulation, we derive the positive energy spectra, construct the partition function via the Euler-Maclaurin method, and compute key thermodynamic quantities, including the specific heat $C_v$, as functions of temperature and the deformation scale. Planck-scale corrections produce distinct, theoretically resolvable shifts in both the position and magnitude of the $C_v$ peak in the two models. An accompanying entropy analysis reveals that these peaks correspond to smooth Schottky-type anomalies: the specific heat curves remain analytic and positive across the explored temperature range, and thus do not indicate latent or continuous thermodynamic phase transitions. These comparative results provide a robust diagnostic framework for differentiating DSR prescriptions in relativistic quantum systems and reinforce the transition-free character of their thermal response.

</details>


### [367] [Universality of Quasinormal-Mode Shifts from Small Nonlocal Effective Couplings](https://arxiv.org/abs/2511.11726)
*Anisur Rahaman*

Main category: gr-qc

TL;DR: 本文研究了黑洞准正规模在分数阶非局域修正下的扰动变化，推导了频率偏移的解析表达式，发现其具有普适性标度规律，可作为量子引力修正的观测特征。


<details>
  <summary>Details</summary>
Motivation: 探索分数阶非局域修正对黑洞准正规模的影响，为量子引力理论提供可能的观测检验途径。

Method: 从包含小分数阶拉普拉斯项(-Δ)^s的标量主方程出发，推导一阶非局域耦合ε下的复频率偏移解析表达式，并在坐标和动量表象中评估分数阶算子。

Result: 发现频率偏移具有普适标度律δω/ω ∝ ε/M^{2s}，与场自旋基本无关，在eikonal区域有ℓ^{2s}增强效应，且在不同黑洞背景下具有普适性。

Conclusion: 分数阶非局域修正导致的准正规模偏移具有模型无关的普适性，为强引力场环降光谱中的非局域性提供了独特的观测特征，是探测量子引力修正的潜在窗口。

Abstract: We investigate perturbative quasinormal-mode (QNM) shifts of black holes arising from fractional, nonlocal modifications to the wave operator. Starting from a scalar master equation corrected by a small fractional Laplacian term $(-Δ)^{s}$ with $0<s<1$, we derive an analytic expression for the complex frequency shift at first order in the nonlocal coupling $\varepsilon$. Evaluation of the fractional operator in both coordinate and momentum representations reveals a universal scaling law $δω/ω\propto \varepsilon/M^{2s}$, largely independent of the field spin, with an additional $\ell^{2s}$ enhancement in the eikonal regime $\ell \gg 1$. Applying the formalism to Schwarzschild, slowly rotating Kerr, Hayward regular, and LQG-corrected black holes, we demonstrate that the leading-order fractional QNM shift is universal, with geometric details entering only through overlap integrals of the mode functions. This universality provides a model-independent signature of nonlocality in strong-gravity ringdown spectra and offers a potential observational window into quantum-gravity-inspired modifications.

</details>


### [368] [Comment on "Black hole in Dehnen $(1,\,4,\,\frac{1}{2})$ dark matter halo: exact solution, lensing, light ring, and thermodynamics (EPJC 85 (2025) 1256)"](https://arxiv.org/abs/2511.11763)
*Ahmad Al-Badawi,Faizuddin Ahmed,İzzet Sakallı*

Main category: gr-qc

TL;DR: 评论文章指出原论文[EPJC 85 (2025) 1256]在构建Dehnen型暗物质晕中静态球对称黑洞精确解时，密度分布和质量分布存在根本性数学错误，导致后续分析无效。


<details>
  <summary>Details</summary>
Motivation: 指出原论文在数学推导上的错误，确保相关研究的严谨性和正确性。

Method: 通过数学分析揭示原论文在密度分布和质量分布推导过程中的根本性错误。

Result: 证明原论文的密度分布和质量分布推导存在数学错误，因此基于这些错误推导的精确度规解也是无效的。

Conclusion: 原论文的整个分析因数学推导错误而无效，需要重新审视该问题。

Abstract: In a recent article, the author of Ref. [EPJC 85 (2025) 1256] constructed an exact solution describing a static, spherically symmetric black hole embedded in a Dehnen-($α, β, γ$) type dark matter halo with the parameter choice $α=1,\,β=4,\,γ=1/2$. The author first derived the density profile, which was subsequently integrated to obtain the mass profile, and this mass profile was then employed in constructing the exact metric solution. In this comment, we demonstrate that both the density profile and the mass profile obtained in Ref. [EPJC 85 (2025) 1256] contain fundamental mathematical errors that invalidate the subsequent analysis.

</details>


### [369] [Kerr-Bertotti-Robinson Black Holes Surrounded by a Cloud of Strings](https://arxiv.org/abs/2511.11792)
*Faizuddin Ahmed,İzzet Sakallı,Ahmad Al-Badawi*

Main category: gr-qc

TL;DR: 研究了被均匀磁场包围的类克尔黑洞与弦云系统的时空解，分析了零旋转和零磁场情况下的简化形式。


<details>
  <summary>Details</summary>
Motivation: 受先前关于克尔黑洞在均匀磁场中精确解的研究启发，探索弦云环境下类似黑洞系统的时空结构。

Method: 构建并分析类克尔黑洞在弦云和均匀磁场共同作用下的精确时空解，考察不同参数极限下的简化情况。

Result: 得到了包含弦云的类克尔黑洞在均匀磁场中的精确解：零旋转时简化为含弦云的Schwarzschild-Bertotti-Robinson黑洞；零磁场时简化为含弦云的类克尔黑洞。

Conclusion: 成功构建了弦云环境下受均匀磁场影响的类克尔黑洞精确时空解，揭示了不同参数极限下的物理特性。

Abstract: In a recent study [1], authors introduced a new class of exact space-times in Einstein's gravity, which are Kerr black holes immersed in an external uniform magnetic field that is oriented along the rotational axis. Motivated by this work, we investigate a Kerr-like black hole solution with a cloud of strings surrounded by a uniform magnetic field. For the zero rotation case, the space-time reduces to the Schwarzschild-Bertotti-Robinson black hole with a cloud of strings. Moreover, for zero magnetic field, the metrics simplify to a Kerr-like black hole surrounded by a cloud of strings, and its static counterpart reduces to the Schwarzschild black hole with a cloud of strings.

</details>


### [370] [Testing general relativity with amplitudes of subdominant gravitational-wave modes](https://arxiv.org/abs/2511.11886)
*Ish Gupta,Purnima Narayan,Lionel London,Shubhanshu Tiwari,Bangalore Sathyaprakash*

Main category: gr-qc

TL;DR: 改进的次主导模式振幅测试，用于探测双黑洞合并引力波信号中高阶模式的振幅偏差，同时保持主导的四极模式不变。该测试在数值相对论模拟中表现可靠，但在强进动或偏心系统中存在偏差。


<details>
  <summary>Details</summary>
Motivation: 开发一个稳健且广泛敏感的框架来探测强场引力，并展示评估引力波广义相对论测试鲁棒性的系统性方法。

Method: 使用综合参数估计活动，对高斯噪声波动、波形建模系统误差以及自旋进动和轨道偏心率等物理效应进行基准测试。

Result: 对最近引力波探测应用该测试，报告了对十六极(4,4)模式振幅偏差的最强约束，δA44 = -0.30^{+1.16}_{-3.45}，与广义相对论一致。

Conclusion: 该工作确立了次主导模式振幅测试作为探测强场引力的稳健且广泛敏感框架，并展示了评估引力波广义相对论测试鲁棒性的系统性方法。

Abstract: We present an improved subdominant-mode amplitude (SMA) test of general relativity (GR), which probes amplitude-level deviations in the higher-order modes of gravitational-wave (GW) signals from binary black hole mergers while keeping the dominant quadrupole mode fixed. Using a comprehensive parameter-estimation campaign, we benchmark the test against Gaussian noise fluctuations, waveform modeling systematics, and physical effects such as spin precession and orbital eccentricity. When applied to numerical-relativity simulations, the SMA test performs reliably for aligned-spin and mildly precessing systems but exhibits measurable biases for strongly precessing or eccentric binaries. Although designed to detect amplitude deviations, the test also responds coherently to phase perturbations, yielding apparent GR violations when applied to phase-modified waveforms. Applied to recent GW detections, we report the strongest constraint on the hexadecapolar (4,4) mode amplitude deviation, $δA_{44} = -0.30^{+1.16}_{-3.45}$, consistent with GR. With these results, this work establishes the SMA test as a robust and broadly sensitive framework for probing strong-field gravity and demonstrates a systematic approach for assessing the robustness of GW tests of GR.

</details>


### [371] [Symmetric Bimetric Cosmology: A Minimal Extension of ΛCDM](https://arxiv.org/abs/2511.11929)
*Ghani Imadouchene*

Main category: gr-qc

TL;DR: 构建了一个连接反德西特和德西特宇宙学体系的对称双度量模型，通过耦合标量场实现，在对称有效流体极限下重现ΛCDM宇宙学主要现象学特征


<details>
  <summary>Details</summary>
Motivation: 探索连接AdS和dS宇宙学体系的理论框架，在双几何宇宙学背景下构建能够满足局部引力约束的模型

Method: 从包含两个FLRW度量的爱因斯坦-希尔伯特项和度量间势的拉格朗日量出发，推导修正的弗里德曼方程和克莱因-戈尔登方程

Result: 模型在对称有效流体极限下重现ΛCDM宇宙学主要特征，包含与(1+z)^{-3}成正比的小动力学修正，并通过Vainshtein筛选自然满足局部引力约束

Conclusion: 该对称双度量模型为连接AdS和dS宇宙学体系提供了可行的理论框架，能够在双几何宇宙学背景下校准并满足观测约束

Abstract: We construct and analyze a symmetric bimetric cosmological model connecting Anti-de Sitter (AdS) and de Sitter (dS) regimes through a coupled scalar field. Starting from a Lagrangian with Einstein-Hilbert terms for two FLRW metrics and an inter-metric potential, we derive modified Friedmann and Klein-Gordon equations governing their evolution. In the symmetric effective-fluid limit, the model reproduces the main phenomenology of the $Λ$CDM cosmology with a small dynamical correction proportional to $(1+z)^{-3}$, and naturally satisfies local-gravity constraints through Vainshtein screening. This article outlines the theoretical structure and calibration of the model within a dual-geometry cosmological setting.

</details>


### [372] [Spin precession effects in the phasing formula of eccentric compact binary inspirals till the second post-Newtonian order](https://arxiv.org/abs/2511.12042)
*Soham Bhattacharyya,Omkar Sridhar*

Main category: gr-qc

TL;DR: 开发了包含轨道偏心率和自旋进动效应的解析相位公式，通过时间尺度分离和进动平均方法，为引力波波形建模提供高效封闭形式解。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏同时包含偏心率和进动自旋效应的封闭形式后牛顿表达式，数值积分方法生成波形速度慢，需要更高效的计算方法。

Method: 利用轨道运动、自旋进动和辐射反应的时间尺度分离，应用进动平均方法消除自旋轨道和自旋-自旋动力学的显式时间依赖性，将偏心率作为小参数推导解析相位公式。

Result: 获得了偏心率演化和引力波相位的封闭形式解（最高八阶初始偏心率），推广了TaylorT2近似以包含自旋进动效应，并在时间和频域计算轨道相位。

Conclusion: 这些结果为偏心率和进动的耦合动力学提供了高效封闭形式相位表达式，使引力波波形建模更准确且计算上可行。

Abstract: Compact binary systems emitting gravitational waves (GWs) can exhibit orbital eccentricity, along with generic spin orientations, leading to the precession of the orbital angular momentum, individual spins, and the orbital plane. While eccentric binaries with aligned spins are well studied, closed form post Newtonian (PN) expressions that simultaneously include eccentricity and precessing spin effects have remained unavailable. Eccentricity complicates orbital evolution because solving the coupled differential equations typically requires numerical integration, which slows down the generation of waveforms. We exploit the separation of timescales between orbital motion, spin precession, and radiation reaction, applying the precession averaging method of Morras et al. (2025) to remove explicit time dependence from the spin orbit and spin spin dynamics through the second PN order. Using this framework, we derive analytic phasing formulae from the evolution equations for orbital frequency and eccentricity, treating eccentricity as a small parameter. Closed form solutions for the eccentricity evolution and GW phase are obtained up to eighth order in the initial eccentricity. We also generalize the TaylorT2 approximant to include spin precession effects and compute the orbital phase in both time and frequency domains. To improve accuracy for moderate to high initial eccentricities, we perform a resummation of the TaylorT2 phasing. These results offer efficient, closed form phasing expressions that capture the coupled dynamics of eccentricity and precession, enabling more accurate and computationally tractable GW waveform modeling for data analysis.

</details>


### [373] [Relativistic framework for high-precision GNSS-based navigation in cislunar space](https://arxiv.org/abs/2511.12058)
*Slava G. Turyshev,Yoaz E. Bar-Sever,William I. Bertiger*

Main category: gr-qc

TL;DR: 提出了一个用于地心天体参考系(GCRS)和质心天体参考系(BCRS)中GNSS导航的相对论建模框架，推导了位置、速度和加速度的闭式变换，实现了10^{-16}的频率传递精度和亚厘米级定位精度。


<details>
  <summary>Details</summary>
Motivation: 为地球-月球系统内高精度GNSS导航提供相对论修正，建立支持分米级轨道精度和亚纳秒级时间同步的鲁棒框架，并将GNSS导航扩展到地月空间。

Method: 使用IAU采用的GCRS和BCRS约定，推导位置、速度和加速度的闭式变换，通过GipsyX进行数值模拟验证，并引入月心天体参考系(LCRS)及其相关时间尺度。

Result: 数值模拟验证了厘米级轨道确定和数十皮秒级时间稳定性，在24小时内实现了毫米级的GCRS<-->BCRS框架闭合往返精度，支持地月空间分米级轨道精度和亚纳秒级时间同步。

Conclusion: 相对论修正在整个地球-月球系统的高精度GNSS中是必要的，GNSS信号可以支持地月空间的分米级轨道精度和亚纳秒级时间同步，为未来操作建立了鲁棒框架。

Abstract: We present a relativistic modeling framework for GNSS-based navigation in Geocentric and Barycentric Celestial Reference Systems, i.e., GCRS and BCRS, respectively. Using the IAU-adopted GCRS and BCRS conventions, we derive closed-form transformations for position, velocity, and acceleration that are required to achieve fractional-frequency transfer precision of 10^{-16} and sub-cm positional accuracy. Numerical simulations with GipsyX validate cm-level orbit determination and timing stability within tens of picoseconds across GCRS and BCRS and demonstrate mm-level round-trip GCRS <--> BCRS frame closure over 24 h. These results underscore the necessity of relativistic corrections for high-precision GNSS throughout the Earth-Moon system and demonstrate that GNSS signals can support a decimeter-level orbit accuracy and sub-nanosecond-scale timing synchronization in cislunar space, establishing a robust framework for future operations. We introduce the Lunicentric Celestial Reference System (LCRS) and its associated time scale to extend GNSS-based navigation into cislunar space. We present a minimal cislunar case study (NRHO-like geometry) that applies the improved position/velocity/acceleration transformations and the light-time model.

</details>


### [374] [Nonlinear evolution of anisotropic matter configurations under higher-order curvature corrections](https://arxiv.org/abs/2511.12134)
*A. Zahra,S. A. Mardan,Muhammad Bilal Riaz,Javlon Rayimbaev,Inomjon Ibragimov,Munisbek Akhmedov,Erkaboy Davletov*

Main category: gr-qc

TL;DR: 该研究在f(R)引力框架下分析含奇异物质的自引力系统动力学演化，采用Starobinsky模型f(R)=R+αR²，研究非线性球对称演化中暗物质对致密天体物理特性的影响。


<details>
  <summary>Details</summary>
Motivation: 探索f(R)引力理论中高阶曲率修正对自引力系统演化的影响，特别是暗物质如何改变致密天体的压力分布和整体动力学行为。

Method: 采用Starobinsky模型的f(R)引力理论，结合广义Tolman-Kuchowicz度规描述Her X-1致密天体，分析非线性球对称演化中的各向异性物质配置。

Result: 暗物质显著影响径向和切向压力分布，增加广义Tolman-Kuchowicz度规参数n会导致模型特性显著变化，模型在能量条件、流体静力平衡等多种物理检验下保持可行性。

Conclusion: f(R)引力框架下的Starobinsky模型能有效描述含暗物质致密天体的演化，广义Tolman-Kuchowicz度规参数n在控制天体内部结构和演化中起关键作用。

Abstract: This study examines the dynamical evolution of self-gravitating systems in the presence of exotic matter within the framework of $f(R)$ gravity. Specifically, we have adopted the Starobinsky model $f(R) = R + αR^2$, which incorporates higher-order curvature corrections to describe nonlinear gravitational behavior. The analysis focuses on the nonlinear spherical evolution of anisotropic matter configurations and explains how dark matter influences their physical characteristics. The presence of dark matter is found to significantly affect the radial and tangential pressure distributions, thereby altering the overall dynamics of the system. The model is employed for the compact object $ Her~X-1$ described by the generalized Tolman-Kuchowicz metric, demonstrating a singularity-free behavior of the physical parameters. The results reveal that increasing the parameter $n$ of the generalized Tolman-Kuchowicz metric leads to striking variations in the model characteristics, highlighting its essential role in governing internal structure and evolution of the compact object. The model remains physically viable under different testing criteria like energy conditions, hydrostatic equilibrium condition, adiabatic index, causality conditions, Herrera's Cracking condition and mass-radius relation presented in this work.

</details>


### [375] [Quintessence: Quadratic potentials](https://arxiv.org/abs/2511.12244)
*Artur Alho,Claes Uggla*

Main category: gr-qc

TL;DR: 本文通过引入正则无约束动力系统和单调函数，对具有二次势能的典型标量场进行全局分析，探索慢滚和山顶解冻暗能量的全局解空间。


<details>
  <summary>Details</summary>
Motivation: 研究具有二次势能的标量场模型，探索慢滚和山顶解冻暗能量的一般特征，为观测可行的暗能量解提供全局视角。

Method: 引入正则无约束动力系统在紧致状态空间上，推导单调函数，分析具有二次势能V=Λ±1/2 m²φ²的标量场模型。

Result: 获得了两个模型的全局结果，构建了展示全局解空间的图示，并在其中定位了观测可行的暗能量解。

Conclusion: 通过全局动力系统分析，成功描述了慢滚和山顶解冻暗能量模型的完整解空间，为理解这些暗能量模型提供了系统框架。

Abstract: Arguably one can use a canonical scalar field $\varphi$, minimally coupled to gravity, with quadratic potentials $V = Λ\pm \frac12 m^2\varphi^2$ to explore some general features of slow-roll and hilltop thawing quintessence, respectively. For each of these two potentials, and pressure-free matter, we introduce a regular unconstrained dynamical system on a compact state space, where the formulation for the hilltop case is new. Together with a derivation of monotonic functions in the two global state space settings, this enables us to obtain global results and to introduce figures that illustrate the global solution spaces of these models, in which we situate the observationally viable quintessence solutions.

</details>


### [376] [Entanglement degradation of static black holes in effective quantum gravity](https://arxiv.org/abs/2511.12245)
*Xiaobao Liu,Wentao Liu,Shu-Min Wu*

Main category: gr-qc

TL;DR: 本文研究了在包含量子引力修正的第三类黑洞几何中，标量场和狄拉克场的量子纠缠退化。量子参数ζ˜能减弱视界引起的关联损失，对量子相关性起到保护作用。


<details>
  <summary>Details</summary>
Motivation: 量子信息科学在爱因斯坦引力和各种修正引力理论中已有广泛研究，但在量子引力背景下的扩展仍未被充分探索。本文旨在填补这一空白。

Method: 在第三类有效量子黑洞几何中，计算均匀纠缠探测器对的量子纠缠和互信息，分析量子参数ζ˜、模式频率ω˜和探测器径向位置R0的影响。

Result: 量子参数ζ˜持续减弱了视界引起的关联损失。对标量场，这种效应显著，产生明显的经典行为偏离；对狄拉克场，熟悉的关联模式保持完整但其退化明显减少。

Conclusion: ζ˜作为对抗引力压制量子关联的普遍保护因子。第三类有效量子黑洞为探测量子引力修正如何影响相对论量子信息提供了受控且物理透明的平台。

Abstract: Quantum information science has been broadly explored in Einstein gravity and in various modified gravity theories; however, its extension to quantum gravity settings remains largely unexplored. Motivated by this gap, in this paper we investigate the degradation of quantum entanglement of scalar and Dirac fields in the third-type black hole geometry arising from effective quantum gravity, which incorporates generic quantum gravitational corrections beyond classical general relativity. This quantum corrected spacetime is free of a Cauchy horizon and can be cast into a Rindler form in the near-horizon regime, allowing a direct identification of vacuum modes and a clear correspondence with the framework developed for uniformly accelerated observers. Within this framework, we compute the quantum entanglement and mutual information of uniformly entangled detector pairs in terms of the quantum parameter $\tildeζ$, the mode frequency $\tildeω$, and Bob's radial position $R_0$. The quantum parameter $\tildeζ$ consistently weakens the horizon-induced loss of correlations. For scalar fields this effect is pronounced, producing clear departures from the classical behavior, whereas for Dirac fields the familiar correlation pattern remains intact but its degradation is noticeably reduced. Overall, $\tildeζ$ acts as a universal protective factor against gravitational suppression of quantum correlations. The third-type effective quantum black hole therefore provides a controlled and physically transparent arena for probing how quantum-gravity corrections influence relativistic quantum information.

</details>


### [377] [Long-lived quasinormal modes and grey-body factors of supermassive black holes with a dark matter halo](https://arxiv.org/abs/2511.12335)
*Zainab Malik*

Main category: gr-qc

TL;DR: 研究施瓦西黑洞被球对称星系暗物质晕包围背景下的大质量标量场的准正则模和灰体因子，发现场质量增加会导致频率实部轻微增加而阻尼率减小，暗物质晕参数对天体物理实际值影响可忽略。


<details>
  <summary>Details</summary>
Motivation: 探索暗物质晕如何影响黑洞的准正则模和灰体因子，验证施瓦西黑洞环降信号的鲁棒性。

Method: 使用六阶和七阶WKB方法配合Pade近似，辅以时域积分和Prony分析，计算准正则频率和透射系数。

Result: 场质量增加使频率实部轻微增加、阻尼率减小，导致振荡寿命更长；暗物质晕参数对天体物理实际值影响可忽略；灰体因子随场质量和多极数增加而减小。

Conclusion: 暗物质晕参数对黑洞准正则模和灰体因子的影响很小，证实了施瓦西黑洞环降特征的鲁棒性。

Abstract: We study quasinormal modes and grey-body factors of a massive scalar field in the background of a Schwarzschild black hole surrounded by a spherically symmetric galactic dark matter halo. The background metric, recently obtained as an analytic generalization of the Schwarzschild geometry, depends on the halo velocity parameter $V_{c}$ and the core radius $a$. Using the sixth- and seventh-order WKB methods with Pade approximants, supported by time-domain integration and Prony analysis, we compute the fundamental quasinormal frequencies and transmission coefficients. The results show that the real part of the frequency slightly increases while the damping rate decreases with growing field mass $μ$, leading to longer-lived oscillations. The influence of the dark matter halo parameters is found to be negligible for astrophysically realistic values, confirming the robustness of Schwarzschild-like ringdown signatures. Grey-body factors decrease with increasing field mass and multipole number, while the effect of the halo parameters remains small.

</details>


### [378] [The Unruh Effect in Relativistic Fluids](https://arxiv.org/abs/2511.12366)
*Eren Erberk Erkul*

Main category: gr-qc

TL;DR: 本文发现相对论流体中存在Unruh效应的热力学对应，其中共动探测器测量到热力学Unruh温度。通过将一阶流体动力学中的参考系变换重新表述为局部时变双曲旋转，揭示了热力学加速度如何产生热谱。


<details>
  <summary>Details</summary>
Motivation: 探索相对论流体中是否存在与Unruh效应类似的热力学现象，将参考系变换与热力学性质联系起来。

Method: 使用Israel-Stewart理论并将其扩展到现代相对论流体理论，分析一阶流体动力学中的参考系变换，将其重新表述为局部时变双曲旋转。

Result: 发现热力学Unruh温度在主导阶是参考系无关的，并且对于非平衡相对论流体具有普适性。

Conclusion: 相对论流体中存在热力学版本的Unruh效应，热力学加速度会诱导出热谱，这一现象在主导阶具有参考系无关性和普适性。

Abstract: We identify a relativistic-fluid counterpart of the Unruh effect, in which a comoving probe measures a \emph{Thermodynamic Unruh temperature}. Frame changes in first-order hydrodynamics can be recast as a local, time-dependent hyperbolic rotation in a Rindler-style state space; the instantaneous map between frames is the \emph{Thermodynamic Boost}, and its proper-time variation defines the \emph{Thermodynamic Acceleration} that imprints a thermal spectrum. Using the Israel--Stewart theory and extending it to modern relativistic fluid theories,we show that this temperature is frame-independent to leading order and universal for out-of-equilibrium relativistic fluids.

</details>


### [379] [Wormholes exact solutions in high dimensions General Relativity](https://arxiv.org/abs/2511.12392)
*I. A. Sarmiento-Alvarado,Leonel Bixano,Tonatiuh Matos*

Main category: gr-qc

TL;DR: 本文开发并检验了5维爱因斯坦场方程的精确解，这些解依赖于两个常数参数p和q，推广了Lü和Mei的p=2解。当p为偶数时出现渐近Ricci平坦虫洞，当p为奇数时可能出现裸奇点。


<details>
  <summary>Details</summary>
Motivation: 推广已有的5维爱因斯坦场方程解，探索更一般的参数空间，特别是研究虫洞解的性质和奇点结构。

Method: 构建依赖于参数p和q的5维真空爱因斯坦场方程精确解，分析解的几何特性、测地线、奇点、事件视界、能层和虫洞喉部。

Result: 发现当p为偶数时出现渐近Ricci平坦虫洞，Kreschman不变量依赖于参数l。当l=0或l≤-1/2时解是正则的，某些参数条件下虫洞满足虫洞宇宙监督假设。

Conclusion: 该解类在特定参数条件下产生正则虫洞结构，虫洞喉部能够隐藏所有因果异常和奇点，为理解高维时空结构提供了新的洞见。

Abstract: In the present work, we develop and examine a series of exact solutions to Einstein's 5-dimensional field equations in the vacuum, which depend on two constant parameters, $p$ and $q$, which generalize the solutions of Lü and Mei [8] belonging to our class $p=2$. This category of solutions can be split into two sections: when $p$ is odd, it represents a compact object that may have naked singularities. However, the intriguing outcome occurs when $p$ is even, as asymptotically Ricci flat wormholes emerge in this scenario. The Kreschman invariant of these solutions depends on the constant parameter $l = 1 + \frac{3 q^2 - p^2}{4}$. When $l = 0$ and $l \leq -\frac{1}{2}$, the solutions are regular. For the specific cases where $l \leq 0$, or $l > 0$ such that $q < 0$ for $p \geq 2$, $q \leq \frac{1}{3}$ for $p = 2$, and $q \leq \frac{1 + \sqrt{2}}{3}$ for $ p \geq 4$, this class of wormholes adheres to Wormhole Cosmic Censorship, implying that the throat effectively obscures all causal anomalies and singularities. In our analysis, we investigated the embedded geometry, geodesics, singularities, potential event horizons, ergoregions, and the wormhole throat.

</details>


### [380] [Black hole spacetimes with dark matter spikes: Energy-momentum tensor and backreaction effects](https://arxiv.org/abs/2511.12570)
*Wei Xiong,Peng-Cheng Li*

Main category: gr-qc

TL;DR: 研究暗物质尖峰的能量-动量张量及其对黑洞时空几何的反作用，发现在尖峰附近动能项使总能量密度增加约50%，径向压力导致应力张量轻微各向异性，完整的动力学结构对准确建模至关重要。


<details>
  <summary>Details</summary>
Motivation: 研究暗物质尖峰在黑洞周围形成过程中的能量-动量张量，以及其对时空几何的反作用，以更准确地建模暗物质对黑洞时空的影响。

Method: 在爱因斯坦星团框架内推导完整的能量-动量张量，包含动能贡献和来自非圆形粒子轨道的各向异性压力，采用Hernquist剖面作为暗物质晕模型，使用银河系参数进行数值计算。

Result: 暗物质尖峰附近的动能项使总能量密度比静止质量分量增加约50%，非零径向压力导致应力张量轻微各向异性，得到的静态球对称度规与史瓦西解的偏差是仅考虑质量密度时的两倍以上。

Conclusion: 包含暗物质尖峰的完整动力学结构对于准确建模暗物质对黑洞时空的反作用至关重要，仅考虑质量密度会导致显著低估这种影响。

Abstract: We study the energy-momentum tensor of a dark matter (DM) spike formed during the adiabatic growth of a black hole embedded in a DM halo, and investigate its backreaction on the spacetime geometry. Within the Einstein cluster framework, we derive the complete tensor, explicitly incorporating the kinetic contribution to the energy density and the anisotropic pressure arising from noncircular particle orbits. Adopting the Hernquist profile as an illustrative model of DM halo and employing parameters appropriate to the Milky Way, we find that near the spike, the kinetic term enhances the total energy density by approximately $50\%$ relative to the rest-mass component, while the nonzero radial pressure induces a mild anisotropy in the stress tensor. The derived tensor satisfies all standard energy conditions. By treating it as a fixed source in Einstein' s equations, we numerically obtain a static, spherically symmetric metric that deviates from the Schwarzschild solution by an amount more than twice that found when only the mass density is considered. These results demonstrate that including the full dynamical structure of the DM spike is essential for accurately modeling the backreaction of DM on black hole spacetimes.

</details>


### [381] [Catenoid Inspired Hyperbolic Wormhole Geometry](https://arxiv.org/abs/2511.12571)
*Bikramarka S Choudhury,Md Khalid Hossain,Farook Rahaman*

Main category: gr-qc

TL;DR: 提出了一种基于悬链面几何结构的新型可穿越球形虫洞，通过严格推导其曲率特性和能量张量，证明该虫洞由各向异性流体维持，需要奇异物质存在，并分析了其可穿越性、引力透镜效应和稳定性。


<details>
  <summary>Details</summary>
Motivation: 受悬链面最小曲面结构的几何启发，探索构建具有精确球对称性的新型可穿越虫洞，旨在研究其物理可行性和特性。

Method: 引入时空度规，严格推导黎曼曲率张量、爱因斯坦张量和应力-能量张量，分析能量条件，并研究虫洞的可穿越性、引力透镜和动态稳定性。

Result: 该虫洞由各向异性流体维持，需要奇异物质存在，具有有限的空间范围，是可穿越的，并表现出特定的引力透镜特征。

Conclusion: 悬链面启发的虫洞构型在物理上是可行的，但需要奇异物质支持，为有限范围的球形虫洞研究提供了新视角。

Abstract: We unveil a novel class of traversable wormholes exhibiting exact spherical symmetry, geometrically inspired by the minimal surface structure of a catenoid. Introducing the spacetime metric, we rigorously derive its fundamental curvature properties, including the Riemann curvature tensor, and consequently compute the Einstein tensor and stress-energy tensor. This framework reveals that the wormhole is sustained by an anisotropic fluid. A detailed analysis of the energy conditions demonstrates the requisite presence of exotic matter, establishing the physical viability and constraints of this configuration. Subsequent investigations address the wormhole's traversability characteristics, gravitational lensing signatures, and dynamic stability. Crucially, we establish that this catenoid-inspired spacetime represents a finite wormhole, possessing bounded spatial extent.

</details>


### [382] [Euclidean quantum wormholes](https://arxiv.org/abs/2511.12577)
*Farook Rahaman,Bikramarka S. Choudhury,Anikul Islam*

Main category: gr-qc

TL;DR: 研究了弗里德曼-罗伯逊-沃克宇宙学中满足霍金-佩奇虫洞边界条件的惠勒-德维特方程虫洞解


<details>
  <summary>Details</summary>
Motivation: 探索量子虫洞作为惠勒-德维特方程的解，满足特定的边界条件，旨在理解量子引力背景下的虫洞物理

Method: 采用任意因子排序的哈密顿约束算符，考虑完美流体物质源和最小耦合标量场，在FRW宇宙学框架下构建量子虫洞

Result: 成功构建了满足霍金-佩奇虫洞边界条件的量子虫洞解

Conclusion: 该方法为研究量子引力中的虫洞提供了有效的理论框架，扩展了对虫洞量子性质的理解

Abstract: We study wormhole as the solution of the Wheeler-deWitt (WdW ) equation satisfying Hawking-Page wormhole boundary conditions in Friedmann-Robertson-Walker (FRW) cosmology. The quantum wormholes are formulated with arbitrary factor ordering of the Hamiltonian constraint operators with perfect fluid matter sources as well as minimally coupled scalar fields.

</details>


### [383] [Dynamical Tidal Response of Non-rotating Black Holes: Connecting the MST Formalism and Worldline EFT](https://arxiv.org/abs/2511.12580)
*Hajime Kobayashi,Shinji Mukohyama,Naritaka Oshita,Kazufumi Takahashi,Vicharit Yingcharoenrat*

Main category: gr-qc

TL;DR: 本文分析了广义相对论中静态球对称黑洞在低频区域对潮汐力的动态响应，通过匹配MST方法和世界线有效场理论，揭示了重整化潮汐响应函数存在不可避免的模糊性，并获得了方案依赖的动态潮汐Love数。


<details>
  <summary>Details</summary>
Motivation: 黑洞对潮汐力的响应编码了基础引力理论的关键信息，并影响双星并合过程中引力波的波形，这对于理解引力理论和引力波天文学具有重要意义。

Method: 基于Mano-Suzuki-Takasugi (MST) 方法和世界线有效场理论 (EFT) 的匹配，在低频区域分析静态球对称黑洞的动态潮汐响应，考虑重整化方案和重整化流方程初始条件的影响。

Result: 重整化潮汐响应函数存在不可避免的模糊性，这些模糊性与重整化方案的选择和重整化流方程的初始条件相关。一旦固定这些模糊性，可以得到方案依赖的动态潮汐Love数。

Conclusion: 研究揭示了黑洞动态潮汐响应中的重整化模糊性问题，并提出了可能的扩展方向，包括广义相对论中的非旋转致密天体（如中子星）和超越广义相对论理论中的黑洞。

Abstract: The response of a black hole (BH) to tidal forces encodes key information about the underlying gravitational theory and affects the waveform of gravitational waves emitted during binary inspiral processes. In this paper, we analyze the dynamical tidal response of static and spherically symmetric BHs in a low-frequency regime within general relativity (GR), based on a matching between the Mano-Suzuki-Takasugi (MST) methods for an analytical approach to BH perturbations and the worldline effective field theory (EFT) for an efficient and unified computation of the binary dynamics within the post-Newtonian regime. We show that the renormalized tidal response function is subject to inevitable ambiguities associated with the choice of renormalization scheme and with the initial condition of the renormalization flow equation. Once these ambiguities are fixed, we obtain scheme-dependent dynamical tidal Love numbers. We also discuss possible extensions of our formalism, including generic non-rotating compact objects (e.g., neutron stars) in GR and BHs in theories beyond GR.

</details>


### [384] [Auto-encoder model for faster generation of effective one-body gravitational waveform approximations](https://arxiv.org/abs/2511.12642)
*Suyog Garg,Feng-Li Lin,Kipp Cannon*

Main category: gr-qc

TL;DR: 本文提出了一种基于条件变分自编码器的模型，用于快速生成对齐自旋SEOBNRv4引力波波形，比传统方法快2-3个数量级。


<details>
  <summary>Details</summary>
Motivation: 随着引力波探测器升级和第三代天文台的发展，检测灵敏度和事件率将大幅提升，需要更快的参数估计方法来支持快速多信使后续观测。

Method: 使用条件变分自编码器模型，基于Liao+2021的最佳架构，训练约10^5个输入波形数据，参数空间包含质量m1、m2和自旋χ1(z)、χ2(z)。

Result: 测试数据集中生成波形的中位失配约为10^-2，在限制参数空间χeff∈[-0.80,0.80]内性能更好。模型生成100个波形仅需0.1秒，平均每个波形4.46毫秒。

Conclusion: 这是开发生产级机器学习框架用于快速生成引力波波形近似值的第一步，比原生SEOBNRv4实现快2-3个数量级。

Abstract: Upgrades to current gravitational wave detectors for the next observation run and upcoming third-generation observatories, like the Einstein telescope, are expected to have enormous improvements in detection sensitivities and compact object merger event rates. Estimation of source parameters for a wider parameter space that these detectable signals will lie in, will be a computational challenge. Thus, it is imperative to have methods to speed-up the likelihood calculations with theoretical waveform predictions, which can ultimately make the parameter estimation faster and aid in rapid multi-messenger follow-ups. Towards this end, we present a conditional variational auto-encoder model, based on the best performing architecture of Liao+2021, for faster generation of aligned-spin SEOBNRv4 inspiral-merger-ringdown waveforms. Our parameter space consists of four parameters, [$m_1$, $m_2$, $χ_1(z)$, $χ_2(z)$]. The masses are uniformly sampled in $[5,75]\,M_{\odot}$ with a mass ratio limit at $10\,M_{\odot}$, while the spins are uniform in $[-0.99,0.99]$. We train the model using $\sim10^5$ input waveforms data with a 70\%/10\% train/validation split, while 20\% data are reserved for testing. The median mismatch for the generated waveforms in the test dataset is $\sim10^{-2}$, with better performance in a restricted parameter space of $χ_{\rm eff}\in[-0.80,0.80]$. Our model is able to generate 100 waveforms in 0.1 second at an average speed of about 4.46 ms per waveform. This is 2-3 orders of magnitude faster than the native SEOBNRv4 implementation in lalsimulation. The latent sampling uncertainty of our model can be quantified with a mean mismatch deviation of $2\times10^{-1}$ for 1000 generations of the same waveform. Our work aims to be the first step towards developing a production-ready machine learning framework for the faster generation of gravitational waveform approximations.

</details>


### [385] [Analytical and numerical study of accretion processes around charged spherically symmetric black holes in scalar-tensor Gauss-Bonnet gravity](https://arxiv.org/abs/2511.12670)
*G. Mustafa,O. Donmez,A. Errehymy,F. Javed,A. Ditta,T. Naseer,S. K. Maurya,F. Atamurotov*

Main category: gr-qc

TL;DR: 研究标量-张量高斯-博内引力对带电黑洞周围物理现象的影响，包括轨道运动、吸积盘性质和BHL吸积流，发现高斯-博内耦合常数和宇宙学参数会改变引力聚焦效应，导致吸积物质减少和激波锥形态变化。


<details>
  <summary>Details</summary>
Motivation: 探索修正引力理论（特别是标量-张量高斯-博内引力）在强引力场中的表现，通过研究黑洞周围的吸积过程来约束引力理论参数。

Method: 结合解析和数值方法，使用测地线分析计算轨道参数，并通过求解广义相对论流体动力学方程模拟BHL吸积机制形成的激波锥形态。

Result: 增加高斯-博内耦合常数和负宇宙学参数会减弱引力聚焦，导致激波锥开角变化，吸积物质和锥内密度显著减少，修正引力项起到有效引力阻尼作用。

Conclusion: 标量-张量高斯-博内修正作为有效引力阻尼项，可将激波主导的吸积转变为更稳定构型，吸积盘和准周期振荡的观测特性可作为约束该引力理论参数的探针。

Abstract: We investigate the physical phenomena occurring around a spherically symmetric, non-rotating charged black hole (BH) to explore the effects of scalar-tensor Gauss-Bonnet gravity on circular motion, accretion disk properties, and Bondi-Hoyle-Lyttleton (BHL) accretion flow. By analytically and numerically examining the influence of the Gauss-Bonnet coupling constant $c_1$ and the cosmological parameter $Λ$, we reveal how these modified gravity parameters alter the underlying physical processes. Using geodesic analysis, we compute the specific energy, angular momentum, innermost stable circular orbit (ISCO) radius, and radiation flux of test particles, providing insight into how the modified gravity framework affects orbital stability and the organization of the accretion flow. Subsequently, through numerical solutions of the general relativistic hydrodynamic (GRHD) equations, we describe the morphology of the shock cone formed via the BHL accretion mechanism around the BH. The numerical results demonstrate that increasing the values of $c_1$ and negative $Λ$ reduce gravitational focusing. Consequently, depending on the parameter choices, the opening angle of the shock cone either widens or narrows compared to the Schwarzschild case. However, because of weakened gravitational focusing, both the amount of accreted matter and the density of material trapped inside the cone decrease significantly. These results indicate that scalar-tensor Gauss-Bonnet corrections act as an effective gravitational damping term, transferring turbulence and transforming shock-dominated accretion into more stable configurations. The consistency between theoretical and numerical results suggests that the observable properties of accretion disks and quasi-periodic oscillations (QPOs) can serve as probes to constrain the parameters of scalar-tensor Gauss-Bonnet gravity in strong-field regimes.

</details>


### [386] [Null fluid gravitational fields on Kerr manifolds and optical lifts of Sasaki structures](https://arxiv.org/abs/2511.12775)
*Masoud Ganji,Cristina Giannotti,Andrea Spiro*

Main category: gr-qc

TL;DR: 该论文给出了在Kerr型光学结构假设下，满足零流体爱因斯坦方程的4维洛伦兹度量的显式参数化，并获得了构造爱因斯坦方程解的新方法，解决了关于Sasaki CR结构中光滑光学提升局部存在性的猜想。


<details>
  <summary>Details</summary>
Motivation: 基于先前对适应光学结构并满足零流体爱因斯坦方程的4维洛伦兹度量的特征化研究，进一步在Kerr型光学结构假设下给出显式参数化，并解决相关猜想。

Method: 在Kerr型光学结构假设下，对满足零流体爱因斯坦方程的4维洛伦兹度量进行显式参数化，构建新的构造方法。

Result: 获得了构造爱因斯坦方程解的新方法，产生包含经典Kerr黑洞度量和所有Ricci平坦示例的大类显式度量，并解决了关于Sasaki CR结构中光滑光学提升局部存在性的猜想。

Conclusion: 该研究成功实现了在Kerr型光学结构下的显式参数化，不仅扩展了爱因斯坦方程解的构造方法，还解决了重要的数学猜想，具有理论和应用价值。

Abstract: Building on the characterisation in [C. D. Hill, J. Lewandowski and P. Nurowski, Indiana Univ. Math. J. 57 (2008), 3131--3176] of 4-dimensional Lorentzian metrics adapted to an optical structure and satisfying the null fluid Einstein equations, we give an explicit parameterisation of this class under the assumption that the optical structure is of Kerr type. As immediate consequences, we obtain: (1) a new method for constructing solutions to the Einstein equations with a null fluid energy momentum tensor, yielding a large family of explicit metrics that naturally includes the classical Kerr black hole metrics and all Ricci flat examples described in [M. Ganji, C. Giannotti, G. Schmalz and A. Spiro, Ann. Physics 75 (2025), Paper No. 169908, 28]; (2) a solution to a conjecture in Hill, Lewandowski and Nurowski's paper on the local existence of smooth optical lifts in the case of Sasaki CR structures.

</details>


### [387] [Gravitational aspects of a new bumblebee black hole](https://arxiv.org/abs/2511.12839)
*A. A. Araújo Filho,N. Heidari,Iarley P. Lobo,V. B. Bezerra*

Main category: gr-qc

TL;DR: 本文研究了bumblebee引力中黑洞解的物理效应，包括粒子传播、临界轨道、有效势、准正规模、引力透镜等现象，并基于太阳系实验对洛伦兹破坏参数设定了限制。


<details>
  <summary>Details</summary>
Motivation: 探索bumblebee引力理论中新提出的黑洞解的物理后果，特别是洛伦兹对称性破坏对黑洞物理现象的影响。

Method: 通过坐标变换揭示几何的全局锥形特性，求解零测地线和类时测地线方程，分析粒子运动，构建标量、矢量、张量和旋量扰动的有效势，计算准正规模频率和时域演化，研究弱场和强场偏转的引力透镜现象。

Result: 获得了临界轨道（光子球）和阴影半径，计算了准正规模频率，分析了引力透镜效应和光传播时间延迟，并与bumblebee和Kalb-Ramond模型的其他洛伦兹破坏配置进行了比较。

Conclusion: 基于经典太阳系实验对洛伦兹破坏参数设定了界限，为理解洛伦兹对称性破坏在引力理论中的物理含义提供了重要见解。

Abstract: In this paper, we examine the physical consequences of a recently introduced black hole solution in bumblebee gravity [1]. The geometry is first presented and then reformulated through suitable coordinate adjustments, which make its global conical character evident. We then study the propagation of particles by solving the geodesic equations for null and timelike trajectories. The associated critical orbits (or photon spheres) are obtained, and shadow radius are computed and compared with other Lorentz-violating configurations in bumblebee and Kalb-Ramond models, including their charged and cosmological extensions. Massive particle motion is analyzed separately, followed by the construction of the effective potentials for scalar, vector, tensor, and spinor perturbations. These potentials allow the calculations of quasinormal frequencies and the corresponding time-domain evolution. Gravitational lensing phenomena are investigated in the weak and strong deflection regimes, and the light-travel time delay is also evaluated. The study concludes with bounds on the Lorentz-violating parameter based on classical Solar System experiments.

</details>


### [388] [Quasinormal ringing of a regular black hole sourced by the Dehnen-type distribution of matter](https://arxiv.org/abs/2511.12859)
*S. V. Bolokhov*

Main category: gr-qc

TL;DR: 研究了在Dehnen型物质分布形成的解析正则黑洞背景下，标量场、电磁场和狄拉克场的准正规模。发现暗物质晕参数a对振荡频率有适度增加，但阻尼率几乎不变，对史瓦西谱只有微小修正。


<details>
  <summary>Details</summary>
Motivation: 探索暗物质晕如何影响黑洞的准正规模谱，验证普通星系暗物质环境是否会对天体物理黑洞的准正规模产生可观测影响。

Method: 使用六阶和九阶WKB方法配合Padé修正计算准正规模频率，并通过时域积分验证结果。

Result: 参数a导致振荡频率适度增加，但阻尼率几乎保持不变，对史瓦西谱只有微小修正。这种偏差仅在非现实的高密度晕中才显著。

Conclusion: 普通星系暗物质环境不会显著影响天体物理黑洞的准正规模谱，观测到的准正规模基本不受暗物质晕影响。

Abstract: We study quasinormal modes of test scalar, electromagnetic, and Dirac fields in the background of a new analytic regular black-hole solution obtained as an exact solution of the Einstein equations sourced by a Dehnen-type matter distribution in [R. A. Konoplya, A. Zhidenko, arXiv:2511.03066]. The metric is asymptotically flat and characterized by a simple lapse function $f(r)=1-2 M r^{2}/(r+a)^{3}$, where $M$ is the ADM mass and $a$ represents the characteristic scale of the surrounding dark-matter halo that regularizes the central region. The effective potentials for all perturbing fields possess the standard single-barrier form, ensuring linear stability and the applicability of the WKB formalism. The quasinormal frequencies are computed using the sixth- and ninth-order WKB methods with Padé corrections and verified by time-domain integration, both approaches showing excellent agreement. The parameter $a$ leads to a moderate increase in the real oscillation frequency, while the damping rate remains almost unchanged, producing only small corrections to the Schwarzschild spectrum. Since such deviations become appreciable only for unrealistically dense halos, our results confirm that the quasinormal spectrum of astrophysical black holes is safely unaffected by ordinary galactic dark-matter environments.

</details>


### [389] [Efficient Reconstruction of Matched-Filter SNR Time Series from Nearby Templates](https://arxiv.org/abs/2511.12894)
*Yasuhiro Murakami,Tathagata Ghosh,Soichiro Morisaki*

Main category: gr-qc

TL;DR: 提出一种高效搜索长时标引力波信号的方法，通过利用相邻波形模板间比值的平滑频域特性，显著降低计算和存储成本。


<details>
  <summary>Details</summary>
Motivation: 传统匹配滤波方法在搜索长时标、低质量双星并合引力波信号时计算成本高昂，存储完整模板库不可行，需要更高效的方法。

Method: 先计算参考模板的信噪比时间序列，然后通过卷积参考信噪比与截断的比值波形来重构邻近模板的信噪比，比值波形仅在模板持续时间差异的短区间内显著。

Result: 重构的信噪比时间序列与标准匹配滤波结果精度达O(10^{-4})，计算成本降低≥25%，存储需求相对完整模板库减少约60倍。

Conclusion: 该方法在保持高精度的同时显著提升了长时标引力波信号搜索的计算和存储效率，适用于低质量双星并合信号的探测。

Abstract: We present a method for efficiently searching long-duration gravitational wave signals from compact binary coalescences (CBCs). The approach exploits the smooth frequency-domain behavior of ratios between neighboring waveform templates. The matched-filter signal-to-noise ratio (SNR) time series of a data segment is first computed for a reference template, and the SNRs of nearby templates are then reconstructed by convolving this reference SNR time series with the ratio waveforms, defined as the frequency-domain ratios between the reference and neighboring templates. The computational speedup arises because the ratio waveforms can be safely truncated: they are significant only over a short interval approximately equal to the duration difference between the templates. Storing these truncated ratio waveforms is practical and enables additional efficiency gains, in contrast to storing full templates, which is generally infeasible for long-duration, low-mass signals. We demonstrate the efficacy of the method with mock non-spinning CBC injections in the $1-3~M_\odot$ range. The reconstructed SNR time series agrees with that obtained from standard matched filtering to an accuracy of $O(10^{-4})$, while the relative computational cost is reduced by $\gtrsim 25\%$. With a truncation threshold of $10^{-3}$ applied to the ratio waveform amplitudes, the storage requirement is reduced by a factor of $\sim 60$ relative to storing the full template bank.

</details>


### [390] [Modified Gravity and Regular Black Hole Models](https://arxiv.org/abs/2511.12902)
*Jose Pinedo Soto*

Main category: gr-qc

TL;DR: 研究修正引力理论以解决黑洞奇点问题，构建无曲率发散的正则黑洞模型，这些模型在适当极限下回归广义相对论且曲率标量表现良好。


<details>
  <summary>Details</summary>
Motivation: 广义相对论预测的时空奇点（如黑洞中心奇点）表明理论在高曲率区域不完整，需要构建物理上可行的正则黑洞模型来避免奇点问题。

Method: 通过修正引力理论，构建保留事件视界但无曲率发散的正则黑洞模型，确保模型在适当极限下回归广义相对论且曲率标量表现良好。

Result: 证明了修正引力理论中正则黑洞模型的可行性，为构建一致的半经典黑洞描述提供了有前景的路径。

Conclusion: 正则黑洞模型在修正引力理论中具有可行性，为避免量子引力解决奇点问题提供了有前景的半经典描述路径。

Abstract: The prediction of spacetime singularities, regions of infinite curvature where classical physics breaks down, is one of the most profound challenges in General Relativity (GR). In particular, black hole solutions such as the Schwarzschild and Kerr metrics feature central singularities that signal the incompleteness of the theory in high curvature regimes. This thesis is devoted to the study of modified theories of gravity that aim to resolve these singularities and construct physically viable models of regular black holes: black holes that retain an event horizon but exhibit no curvature divergence anywhere in the spacetime ... Throughout the thesis, particular care is taken to ensure that the proposed models are physically sensible: they reduce to GR in the appropriate limits and lead to well behaved curvature scalars. Collectively, these results highlight the viability of regular black hole models in modified gravity theories, providing a promising path toward a consistent semiclassical description of black holes that avoids the need for quantum gravitational resolution of singularities. (Full abstract can be found in document).

</details>


### [391] [Imaging Signatures of the Israel Junction: Photon Ring Evolution in Dynamical Thin Shell Schwarzschild Spacetimes](https://arxiv.org/abs/2511.13077)
*Li-Ming Cao,Long-Yue Li,Xia-Yuan Liu*

Main category: gr-qc

TL;DR: 本文通过将两个史瓦西时空与满足以色列连接条件的薄壳粘合，研究了黑洞图像。通过分析球壳上的零测地线折射定律和光传播时间延迟，通过光线追踪几何薄且光学薄的吸积盘获得图像。


<details>
  <summary>Details</summary>
Motivation: 研究黑洞图像以测试史瓦西时空中以色列连接条件的有效性，探索薄壳对光子球和光子环的影响。

Method: 使用两个史瓦西时空与满足以色列连接条件的薄壳粘合，研究球壳上的零测地线折射定律，考虑光传播时间延迟，通过光线追踪几何薄且光学薄的吸积盘。

Result: 发现三个特征：壳上的红移尖点、传递函数r(b)的V形轮廓、以及观察者屏幕上光子球与光子环之间一对一对应关系的丧失。在壳塌缩过程中，时空从壳内单光子球阶段演化到双光子球中间阶段，最终到壳外单光子球阶段。

Conclusion: 这些特征为测试黑洞时空中以色列连接条件提供了实用基础，即使存在两个光子球阶段，图像也从未显示两个分离的光子环，壳运动导致红移因子不连续。

Abstract: We study the images of black holes by gluing two Schwarzschild spacetimes with a thin shell where the Israel junction conditions are satisfied. By studying the refraction law for null geodesics at the spherical shell, and taking account of the light travel time delay, the images are obtained by ray tracing a geometrically and optically thin accretion disk. For a static shell we identify three signatures: a redshift cusp at the shell, a V-shaped profile of the transfer function $r(b)$, and a loss of the one-to-one correspondence between photon spheres and photon rings on the observer's screen. During the collapse of the shell, the spacetime evolves from a stage with a single photon sphere inside the shell, through an intermediate stage with double photon spheres, and finally to a spacetime with a single photon sphere outside the shell. However, when the shell is released from a large distance, the corresponding images never show two separate photon rings, even in the stage with two photon spheres. In addition, the motion of the shell leads to a discontinuity in the redshift factor. These signatures provide a practical basis for testing the Israel junction in black hole spacetimes.

</details>


### [392] [Unbounded Radius of Innermost Stable Circular Orbit in Higher-Dimensional Black Holes](https://arxiv.org/abs/2511.13086)
*Hocheol Lee,Bogeun Gwak*

Main category: gr-qc

TL;DR: 本文研究了高维各向异性能量-动量张量影响下黑洞的最内稳定圆轨道(ISCO)上限问题，发现在高维时空中ISCO半径没有上界，且当维度≥8时ISCO可能不存在。


<details>
  <summary>Details</summary>
Motivation: 探索高维各向异性黑洞时空中ISCO的行为，填补对受各向异性能量-动量张量影响的高维黑洞时空结构理解的空白。

Method: 通过分析类时测地线的有效势并施加ISCO条件，研究高维静态球对称渐近平坦黑洞在满足弱能量条件和额外约束的各向异性能量-动量张量下的ISCO行为。

Result: 证明在高维时空中ISCO半径普遍没有上界；当维度≥8时，ISCO可能不存在，取决于能量-动量张量的径向和切向分量；如果ISCO存在，其半径无界。

Conclusion: 这些发现推进了对高维引力系统中轨道稳定性的理解，并突显了与四维黑洞动力学的根本差异。

Abstract: The innermost stable circular orbit (ISCO) offers a fundamental test of spacetime structure. However, its behavior in higher-dimensional black holes influenced by anisotropic energy-momentum tensors remains insufficiently explored. In this work, we investigate the upper bound of the ISCO in higher-dimensional, static, spherically symmetric, and asymptotically flat black hole spacetimes in the presence of an anisotropic energy-momentum tensor. The energy-momentum tensor is assumed to satisfy the weak energy condition, possess a non-positive trace, and obey constraints on radial and tangential pressures, collectively equivalent to the dominant energy condition with additional constraints. By analyzing the effective potential for timelike geodesics and imposing ISCO conditions, we demonstrate the general absence of an upper bound on the ISCO radius in higher-dimensional spacetimes. For dimensions greater than or equal to eight, an ISCO may not exist, depending on the radial and tangential components of the energy-momentum tensor. If an ISCO exists, its radius remains unbounded. These findings advance our understanding of orbital stability in higher-dimensional gravitational systems and highlight fundamental differences from four-dimensional black hole dynamics.

</details>


### [393] [Towards an anomaly detection pipeline for gravitational waves at the Einstein Telescope](https://arxiv.org/abs/2511.13154)
*Gianluca Inguglia,Huw Haigh,Kristyna Vitulova,Ulyana Dupletsa*

Main category: gr-qc

TL;DR: 基于深度卷积自编码器的异常检测算法，用于在时频谱图中搜索引力波信号，特别针对持续时间短于2秒的紧凑天体合并事件。


<details>
  <summary>Details</summary>
Motivation: 短持续时间引力波信号难以与背景噪声区分，但其短暂性使其适合计算需求适中的机器学习分析。

Method: 使用深度卷积自编码器在单探测器干涉仪数据中标记引力波类瞬变异常，结合弱监督提升性能。

Result: 初始检测效率23%，引入弱监督后能恢复所有注入的中等质量黑洞形成合并事件，误报率约4.5次/年，并能识别质量大于20太阳质量的黑洞形成合并。

Conclusion: 异常检测为未来引力波搜索提供了强大的模型无关框架，为实现全自动自适应分析管道铺平道路。

Abstract: We present the implementation of an anomaly-detection algorithm based on a deep convolutional autoencoder for the search for gravitational waves (GWs) in time-frequency spectrograms. Our method targets short-duration ($\lesssim 2\,\text{s}$) GW signals, exemplified by mergers of compact objects forming or involving an intermediate-mass black hole (IMBH). Such short signals are difficult to distinguish from background noise; yet their brevity makes them well-suited to machine-learning analyses with modest computational requirements. Using the data from the Einstein Telescope Mock Data Challenge as a benchmark, we demonstrate that the approach can successfully flag GW-like transients as anomalies in interferometer data of a single detector, achieving an initial detection efficiency of 23% for injected signals corresponding to IMBH-forming mergers. After introducing weak supervision, the model exhibits excellent generalisation and recovers all injected IMBH-forming mergers, independent of their total mass or signal-to-noise ratio, with a false-alarm rate due to statistical noise fluctuations of approximately 4.5 events per year for a single interferometer operating with a 100% duty cycle. The method also successfully identifies lower-mass mergers leading to the formation of black holes with mass larger than $\simeq 20\,M_\odot$. Our pipeline does not yet classify anomalies, distinguishing between actual GW signals and noise artefacts; however, it highlights any deviation from the learned background noise distribution for further scrutiny. These results demonstrate that anomaly detection offers a powerful, model-independent framework for future GW searches, paving the way toward fully automated and adaptive analysis pipelines.

</details>


### [394] [Sub-Solar Mass Intermediate Mass Ratio Inspirals: Waveform Systematics and Detection Prospects with Gravitational Waves](https://arxiv.org/abs/2511.13324)
*Devesh Giri,Bhooshan Gadre*

Main category: gr-qc

TL;DR: 研究发现亚太阳质量中等质量比螺旋系统(SSM-IMRIs)的可探测性和波形系统误差，指出当前现象学波形模型在高质量比区域存在显著偏差，需要基于微扰理论校准的波形模型。


<details>
  <summary>Details</summary>
Motivation: 研究亚太阳质量中等质量比螺旋系统(质量比q~10^2-10^4)的可探测性和波形系统误差，评估当前现象学波形模型在高质量比区域的性能。

Method: 使用黑洞微扰理论替代模型BHPTNRSur1dq1e4作为参考，评估IMRPhenomX现象学波形家族在高质量比区域的性能，分析高阶引力波模式的重要性，并比较不同波形模型之间的匹配度。

Result: 高阶引力波模式对探测至关重要，排除它们会使信噪比降低3-5倍；SSM-IMRIs在Advanced LIGO中可观测到575 Mpc，在爱因斯坦望远镜中可达10.5 Gpc；但当前现象学近似存在显著系统不确定性，匹配度可低至0.2，拟合因子低于0.9。

Conclusion: 当前现象学波形模型在中等质量比区域存在显著系统偏差，会导致超出统计误差的系统性偏差，强调需要基于微扰理论校准的波形模型来进行可靠的探测和参数推断。

Abstract: We investigate the detectability and waveform systematics of sub-solar mass intermediate mass-ratio inspirals (SSM-IMRIs), characterized by mass ratios $q \sim 10^2-10^4$. Using the black hole perturbation theory surrogate model \textsc{BHPTNRSur1dq1e4} as a reference, we assess the performance of the \textsc{IMRPhenomX} phenomenological family in the high-mass-ratio regime. We find that the inclusion of higher-order gravitational wave modes is critical; their exclusion may degrade the signal-to-noise ratio by factors of $\sim3-5$ relative to quadrupole-only templates. With optimal mode inclusion, SSM-IMRIs are observable out to luminosity distances of $\sim575$ Mpc ($z\sim0.12$) with Advanced LIGO and $\sim10.5$ Gpc ($z\sim1.4$) with the Einstein Telescope. However, we identify substantial systematic uncertainties in current phenomenological approximants. Matches between \textsc{IMRPhenomX} and the reference surrogate model \textsc{BHPTNRSur1dq1e4} degrade to values as low as 0.2 for edge-on inclinations, and fitting factors consistently fall below 0.9, indicating a significant loss of effectualness in template-bank searches. Bayesian parameter estimation reveals that these modeling discrepancies induce systematic biases that exceed statistical errors by multiple standard deviations, underscoring the necessity for waveform models calibrated to perturbation theory in the intermediate mass-ratio regime for robust detection and inference.

</details>


### [395] [On the uniqueness of continuous spacetime extensions in 1+1 dimensions with applications to weak null singularities](https://arxiv.org/abs/2511.13422)
*Peter Cameron,Jan Sbierski*

Main category: gr-qc

TL;DR: 本文研究1+1维洛伦兹流形在零边界v=0处的连续时空延拓的唯一性问题，发现连续延拓的C^0结构通常不唯一，但在某些对称条件下具有刚性，同时构造了具有相同C^0结构但不同C^1结构的延拓。


<details>
  <summary>Details</summary>
Motivation: 受黑洞内部弱零奇点的启发，研究时空在零边界处连续延拓的唯一性，这对于低正则性不可延拓问题的研究具有重要意义。

Method: 研究1+1维洛伦兹流形在零边界v=0处的连续延拓，分析延拓的C^0结构和C^1结构的唯一性，特别关注强球对称条件下的刚性性质。

Result: 发现连续延拓的C^0结构通常不唯一，但在强球对称条件下具有刚性；构造了具有相同C^0结构但不同C^1结构的延拓，这一构造可推广到3+1维弱零奇点。

Conclusion: 连续时空延拓到边界的唯一性性质对于低正则性不可延拓问题的研究至关重要，揭示了延拓结构在不同正则性层次上的复杂性。

Abstract: Motivated by weak null singularities in black hole interiors, we study 1+1 dimensional Lorentzian manifolds $(M,g)$ which admit a continuous spacetime extension across a null boundary $v=0$, where $v<0$ is a null coordinate. We study the degree to which such extensions are unique up to the boundary. Firstly, we find that in general not even the $C^0$-structure of the extension is uniquely determined by the assumption that the metric extends continuously. However, we exhibit an interesting local-global relation regarding the $C^0$-structure which in particular entails its rigidity for ''strongly spherically symmetric'' continuous extensions across the Cauchy horizon of the Reissner-Nordström spacetime. Secondly, we construct continuous extensions which have the same $C^0$-structure, but do not have equivalent $C^1$-structures. This construction also carries over to weak null singularities in 3+1 dimensions. Understanding the uniqueness properties of continuous spacetime extensions to the boundary is of importance for the study of low-regularity inextendibility problems.

</details>


### [396] [Emergent spectral geometry in the Coherence--Curvature Model](https://arxiv.org/abs/2511.13423)
*Jorge Lamas*

Main category: gr-qc

TL;DR: 该论文研究了相干性-曲率模型(CCM)，这是一个由哈密顿量控制的连接图动态系综，该哈密顿量耦合了代数连通性、Ollivier-Ricci曲率和边密度惩罚项。通过模拟退火生成低能图配置，并分析其涌现几何特性。


<details>
  <summary>Details</summary>
Motivation: 探索相干性、曲率和局部性相互作用如何产生有限维的分形几何结构，为研究复杂网络几何特性提供受控数值实验室。

Method: 使用连接模拟退火方法生成低能图配置，通过谱维度(ds)、豪斯多夫维度(dh)和平均距离等几何特征来表征涌现的几何结构，并进行有限尺寸标度分析。

Result: 有限尺寸标度显示谱维度随系统尺寸明显增长，而豪斯多夫维度增长较温和。在最大体积下，数据与ds~4和dh~3兼容，表明ds>dh，谱维度和体积维度之间存在非平凡层次结构。还发现了典型距离的缓慢幂律增长。

Conclusion: CCM提供了一个受控数值实验室，其中相干性、曲率和局部性的相互作用产生了有限维的分形几何结构。

Abstract: We investigate the Coherence--Curvature Model (CCM), a dynamical ensemble of connected graphs governed by a Hamiltonian that couples algebraic connectivity, Ollivier-Ricci curvature, and an edge-density penalty. Using connected simulated annealing we generate low-energy graph configurations and characterize their emergent geometry through the spectral dimension (ds), the Hausdorff dimension (dh), and the average distance. Finite-size scaling shows a clear growth of ds with system size, while dh increases more mildly. At the largest volumes explored the data are compatible with ds ~ 4 and dh ~ 3, implying ds > dh and a nontrivial hierarchy between spectral and volumetric notions of dimension. We also map the dependence on the curvature coupling gamma and the locality coupling beta, and we find a slow power-law growth of typical distances with a small exponent eta. The CCM therefore provides a controlled numerical laboratory in which the interplay of coherence, curvature, and locality yields finite-dimensional, fractal-like geometries.

</details>


### [397] [The asymptotically Schwarzschild-like metric solutions](https://arxiv.org/abs/2511.13471)
*K. K. Ernazarov*

Main category: gr-qc

TL;DR: 本文研究了渐近类史瓦西度规作为广义相对论中史瓦西解的替代方案，该度规在大距离处渐近平坦且类似史瓦西解，但在强场区域表现出根本不同的行为：没有事件视界，可解释为可穿越虫洞或被特定各向异性流体包围的黑洞。


<details>
  <summary>Details</summary>
Motivation: 探索史瓦西解的替代度规，研究其在强场区域的不同行为特性，为超越标准广义相对论的理论、正则黑洞模型和屏蔽汤川势提供理论支持。

Method: 分析渐近类史瓦西度规的关键现象学特征，推导引力扰动和电磁扰动的Regge-Wheeler方程，计算黑洞阴影，并与史瓦西度规进行直接比较。

Result: 该度规在光子球半径和最内稳定圆轨道半径方面与史瓦西解存在显著偏差，虽然被太阳系测试排除，但在超越标准GR的理论中仍具有相关性。

Conclusion: 渐近类史瓦西度规虽然在大距离处类似史瓦西解，但在强场区域表现出根本不同的特性，可作为可穿越虫洞或被各向异性流体包围的黑洞的模型，在超越标准广义相对论的理论中具有研究价值。

Abstract: In this article we investigate the properties of the asymptotically Schwarzschild-like metric as an alternative to the Schwarzschild solution in General Relativity. While asymptotically flat and similar to Schwarzschild at large distances $r$, this metric exhibits a fundamentally different strong-field behavior: it lacks an event horizon and is best interpreted as a traversable wormhole or a black hole surrounded by a specific anisotropic fluid, rather than a true vacuum solution. We analyze key phenomenological features, demonstrating significant deviations from Schwarzschild in the radii of the photon sphere and the Innermost Stable Circular Orbit (ISCO). Furthermore, we derive the Regge-Wheeler equation for gravitational and electromagnetic perturbations and compute the black hole shadow, providing a direct comparison with the Schwarzschild metric. Despite being ruled out by solar system tests, the exponential metric remains relevant for theories beyond standard GR, regular black hole models, and its connection to screened Yukawa potentials.

</details>


### [398] [On the General Projective Theory of Matter and Gravitation](https://arxiv.org/abs/2511.13521)
*Michael J. Connolly*

Main category: gr-qc

TL;DR: 本文提出了一个广义投影规范理论，统一了非度量性和挠率，构建了基于投影一般线性群的引力规范理论，并建立了投影旋量理论。


<details>
  <summary>Details</summary>
Motivation: 旨在统一Thomas-Whitehead引力和度量-仿射引力理论，构建一个包含非度量性和挠率的广义投影规范理论框架。

Method: 使用投影对称远平行连接，通过非线性规范对称实现局部洛伦兹对称，引入投影广义Higgs场，构建Lovelock型作用量，并定义投影旋量理论。

Result: 建立了统一TW和度量-仿射引力的几何框架，发现了新的拓扑不变量，构建了投影爱因斯坦-嘉当型理论，并诱导出无CP破坏的手征质量。

Conclusion: 成功构建了包含非度量性和挠率的广义投影规范理论，统一了多种引力理论，并建立了相应的投影旋量理论框架。

Abstract: We develop a generalized projective gauge theory of gravity and spinorial matter, incorporating both non-metricity and torsion. The work is divided into three parts. Part I provides a thorough review of General Relativity, Metric-Affine gauge theory and Thomas-Whitehead (TW) Gravity. Part II constructs a gauge gravitational theory based on the Projective General Linear Group, wherein the newly found projective symmetric teleparallel (PST) connections are defined. Generalizing to non-inertial frames, and using nonlinear gauge symmetry realizations to implement local Lorentz symmetry, we construct a general geometric framework that unifies TW and Metric-Affine gravity with projectively invariant spacetime torsion. We introduce projective generalized Higgs fields and show how certain gauge choices reduce these to fundamental projective fields, and how they may be used to define fundamental geometric objects such as the projective 2-frame and spacetime connection. A Lovelock-inspired action is shown to support only curvature (metric) dynamics, implying a topologically constrained Schouten field via the Pontrjagin density. The projective Pontrjagin density is shown to contain a new topological invariant, not present in the literature. Part III formulates projective spinors by defining gamma matrices via the projective linear group metric, then employing nonlinear gauge symmetry realizations to ensure local Lorentz covariance. A general spinor metric introduces a complex phase of redundancy. Requiring a real, Hermitian action leads to a projective Einstein-Cartan-type theory. An induced chiral mass emerges without CP violation.

</details>


### [399] [Bimodular Gravity: Unimodularising Bimetric Scalar-Tensor Gravity](https://arxiv.org/abs/2511.13562)
*James Hallam,João Magueijo*

Main category: gr-qc

TL;DR: 该论文提出了一种双模引力理论，通过对双度量标量张量理论施加两个单模约束，构建了两种经典不等价的实现方案：BUG（固定行列式）和BHT/BDUG（微分同胚不变）。


<details>
  <summary>Details</summary>
Motivation: 将双度量标量张量理论单模化，探索两种不同单模约束实现方案的物理差异和可观测后果。

Method: 对两个度量分别施加单模约束，通过拉格朗日乘子λ₁,λ₂实现。比较BUG（固定相对体积元素）和BHT/BDUG（保持微分同胚不变性）两种方案。

Result: BUG方案产生"双模宇宙学常数"Λ=λ₁+νλ₂，支持一阶流控制的常滚解；BHT方案中λ₁,λ₂各自为常数但ν保持动力学，支持时间相关滚解。两种方案在膨胀历史、暗能量状态方程和标量扰动传播上给出不同预测。

Conclusion: 双模引力的两种实现方案在经典层面不等价，具有不同的物理预测，为测试该理论提供了可观测的区分标准。同时提出了保持完全协变性的微分同胚不变完备化方案。

Abstract: It is the object of the present paper to unimodularise a disformal bimetric scalar-tensor theory, thereby defining what we call bimodular gravity. We impose one unimodular constraint per metric via multipliers $λ_{1,2}$ and show that two natural implementations-a dual fixed-determinant (BUG) and a dual diffeomorphism-invariant (BHT/BDUG) formulation-are classically inequivalent. In BUG the relative volume element $ν=\sqrt{1-2BX}$ is fixed, enforcing a kinematic constraint on the biscalar and we derive the "bimodular cosmological constant" $Λ=λ_1+νλ_2$. In BHT/BDUG, $λ_{1,2}$ are individually constant but $ν$ (hence $BX$) remains dynamical. Recasting the theory in an Einstein-frame form, we derive the biscalar sound speed and identify a subluminal domain $1+B(V+λ_2)>0$. At the background level, BUG admits constant-roll solutions governed by first-order flow, whereas BHT supports solutions with time-dependent roll. These structural differences yield distinct, in-principle testable predictions for the expansion history, the dark-energy equation of state, and the propagation of biscalar perturbations. Finally, we present a diffeomorphism-invariant completion that correlates the two HT volume forms, reproducing the $Λ$ of BUG on shell whilst maintaining full covariance.

</details>


### [400] [Gravitational--Electromagnetic Coupling on Kerr Spacetime](https://arxiv.org/abs/2511.13642)
*Fawzi Aly,Dejan Stojkovic*

Main category: gr-qc

TL;DR: 本文在Kerr时空的Newman-Penrose/Teukolsky框架下研究引力-电磁耦合，推导了spin-2 Teukolsky方程的显式二次电磁源项，并论证了GEM二次准正规模在带电和磁化天体物理场景中的相关性。


<details>
  <summary>Details</summary>
Motivation: 扩展先前基于度量的Schwarzschild研究到旋转黑洞，直接在曲率框架下研究引力-电磁耦合，为黑洞光谱学中的GEM相互作用数值研究提供基础。

Method: 在最小耦合Einstein-Maxwell系统中，使用Newman-Penrose/Teukolsky框架，推导spin-2 Teukolsky方程的二次电磁源项，并通过量级论证和dilaton理论示例进行分析。

Result: 推导了显式二次电磁源项，论证了GEM二次准正规模在特定天体物理场景中可能变得相关，并展示了GEM QQNM谱对引力-电磁耦合方式的敏感性。

Conclusion: GEM二次准正规模谱可用于测试最小耦合并约束隐藏的U(1)扇区，为引力波观测提供模型基础的方法。

Abstract: We extend previous metric-based Schwarzschild studies of gravitational--electromagnetic (GEM) coupling to rotating black holes by working directly in a curvature-based Newman--Penrose/Teukolsky framework on Kerr spacetime. Within a minimally coupled Einstein--Maxwell system, we derive explicit quadratic electromagnetic source terms for the spin-$-2$ Teukolsky equation, providing a foundation for future numerical studies of GEM interactions in the framework of black-hole spectroscopy. Moreover, we give order-of-magnitude arguments showing that GEM quadratic quasinormal modes (QQNMs) can become relevant in a range of charged and magnetized astrophysical scenarios. Finally, we show through a brief dilaton-theory example that the GEM QQNM spectrum is sensitive to how gravity couples to electromagnetism, thereby providing a model-based way to test minimal coupling and to constrain hidden $U(1)$ sectors with gravitational-wave observations.

</details>


### [401] [Graviton propagator in de Sitter space in a simple one-parameter gauge](https://arxiv.org/abs/2511.13660)
*Dražen Glavan*

Main category: gr-qc

TL;DR: 构建了德西特空间中引力子传播子的单参数非协变规范族，推广了现有计算中使用的简单规范，便于检验规范依赖性。


<details>
  <summary>Details</summary>
Motivation: 大多数德西特空间中的引力子圈计算都是在简单规范下进行的，需要构建更一般的规范族来检验计算结果的规范依赖性。

Method: 在德西特空间中构造了一个单参数非协变规范族中的引力子传播子，推广了现有的简单规范。

Result: 得到的传播子形式相对简单，适用于检验一圈计算和提议可观测量对规范选择的依赖性。

Conclusion: 该单参数规范族中的引力子传播子为验证德西特空间量子引力计算的规范无关性提供了便利工具。

Abstract: We construct the graviton propagator in de Sitter space in a one-parameter family of noncovariant gauges. This family generalizes the simple gauge in which most graviton loop computations in de Sitter space have been performed. The resulting propagator has a relatively simple form and will facilitate checks of the gauge dependence of one-loop computations and proposed observables.

</details>


### [402] [Einstein-Maxwell fields as solutions of Einstein gravity coupled to conformally invariant non-linear electrodynamics](https://arxiv.org/abs/2511.13665)
*Marcello Ortaggio*

Main category: gr-qc

TL;DR: 本文研究了爱因斯坦-麦克斯韦（非零）无源配置如何扩展到任何共形不变非线性电动力学，通过电磁场的常数重标定实现。


<details>
  <summary>Details</summary>
Motivation: 研究如何将已知的爱因斯坦-麦克斯韦精确解扩展到更一般的共形不变非线性电动力学理论中，利用对偶不变性获得更多物理上有趣的解。

Method: 首先获得一个判据，通过电磁不变量或自对偶麦克斯韦场的规范Newman-Penrose形式来表征可扩展解。然后证明所有静态配置都是可扩展的。

Result: 成功将各种已知精确解扩展到CINLE理论，包括Ozsváth的均匀宇宙、Levi-Civita宇宙中的黑洞、带电C度规的推广，以及Bonnor-Melvin背景中的非扩展引力波。

Conclusion: 所有静态爱因斯坦-麦克斯韦配置都可以通过常数重标定扩展到任何共形不变非线性电动力学，这为从现有文献中直接获得更多精确解提供了系统方法。

Abstract: We study Einstein-Maxwell (non-null) sourcefree configurations that can be extended to any conformally invariant non-linear electrodynamics (CINLE) by a constant rescaling of the electromagnetic field. We first obtain a criterion which characterizes such extendable solutions in terms either of the electromagnetic invariants, or (equivalently) of the canonical Newman-Penrose form of the self-dual Maxwell field. This is then used to argue that all static configurations are extendable (more generally, all configurations admitting a non-null twistfree Killing vector field). One can thus draw from the extensive literature to straightforwardly extend to CINLE various known exact solutions, whereby the duality invariance of the Einstein-Maxwell theory allows for dyonic solutions even in more general theories. This is illustrated by a few explicit examples, including the homogeneous $Λ<0$ universe of Ozsváth, a black hole in the universe of Levi-Civita, Bertotti and Robinson, a generalization of the charged $C$-metric, and non-expanding gravitational waves in the Bonnor-Melvin background.

</details>
