<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 56]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Synthetic bootstrapped pretraining](https://arxiv.org/abs/2509.15248)
*Zitong Yang,Aonan Zhang,Hong Liu,Tatsunori Hashimoto,Emmanuel Candès,Chong Wang,Ruoming Pang*

Main category: cs.CL

TL;DR: SBP是一种语言模型预训练方法，通过先学习文档间关系模型，然后利用该模型合成新语料进行联合训练，以更好地建模文档间相关性。


<details>
  <summary>Details</summary>
Motivation: 标准预训练只学习单个文档内的token因果相关性，无法有效建模文档间丰富的可学习相关性，而这些相关性可能带来更好的性能。

Method: 设计计算匹配的预训练设置，先学习文档间关系模型，然后合成大量新语料进行联合训练，在3B参数模型上使用最多1T token进行从头训练。

Result: SBP持续优于强重复基线，实现了接近使用20倍更多唯一数据的oracle上限性能改进的显著部分。定性分析显示合成文档超越了简单改写，能够抽象核心概念并创作新叙述。

Conclusion: SBP不仅具有强大的实证性能，还自然地符合贝叶斯解释：合成器隐式学习抽象相关文档间的潜在概念。

Abstract: We introduce Synthetic Bootstrapped Pretraining (SBP), a language model (LM)
pretraining procedure that first learns a model of relations between documents
from the pretraining dataset and then leverages it to synthesize a vast new
corpus for joint training. While the standard pretraining teaches LMs to learn
causal correlations among tokens within a single document, it is not designed
to efficiently model the rich, learnable inter-document correlations that can
potentially lead to better performance. We validate SBP by designing a
compute-matched pretraining setup and pretrain a 3B-parameter model on up to 1T
tokens from scratch. We find SBP consistently improves upon a strong repetition
baseline and delivers a significant fraction of performance improvement
attainable by an oracle upper bound with access to 20x more unique data.
Qualitative analysis reveals that the synthesized documents go beyond mere
paraphrases -- SBP first abstracts a core concept from the seed material and
then crafts a new narration on top of it. Besides strong empirical performance,
SBP admits a natural Bayesian interpretation: the synthesizer implicitly learns
to abstract the latent concepts shared between related documents.

</details>


### [2] [Comparative Analysis of Tokenization Algorithms for Low-Resource Language Dzongkha](https://arxiv.org/abs/2509.15255)
*Tandin Wangchuk,Tad Gonsalves*

Main category: cs.CL

TL;DR: 本研究评估了三种常见分词算法（BPE、WordPiece、SentencePiece）在不丹低资源语言Dzongkha上的表现，发现SentencePiece算法最适合Dzongkha分词。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）发展迅速，但现有分词器主要针对高资源语言（如英语），对低资源语言（如Dzongkha）效果不佳。Dzongkha作为不丹国语，语言复杂性高且缺乏相关NLP研究，特别是分词方面的研究。

Method: 使用三种主流分词算法（Byte-Pair Encoding、WordPiece、SentencePiece）对Dzongkha语言进行分词训练，并通过子词生育率、连续词比例、归一化序列长度和执行时间等指标评估性能。

Result: 三种算法都显示出潜力，但SentencePiece在Dzongkha分词任务中表现最优，为构建Dzongkha大型语言模型奠定了基础。

Conclusion: 研究强调了为低资源语言定制分词方法的必要性，SentencePiece算法为Dzongkha的NLP发展提供了有效解决方案，需要持续研究来推动低资源语言的NLP进步。

Abstract: Large Language Models (LLMs) are gaining popularity and improving rapidly.
Tokenizers are crucial components of natural language processing, especially
for LLMs. Tokenizers break down input text into tokens that models can easily
process while ensuring the text is accurately represented, capturing its
meaning and structure. Effective tokenizers enhance the capabilities of LLMs by
improving a model's understanding of context and semantics, ultimately leading
to better performance in various downstream tasks, such as translation,
classification, sentiment analysis, and text generation. Most pre-trained
tokenizers are suitable for high-resource languages like English but perform
poorly for low-resource languages. Dzongkha, Bhutan's national language spoken
by around seven hundred thousand people, is a low-resource language, and its
linguistic complexity poses unique NLP challenges. Despite some progress,
significant research in Dzongkha NLP is lacking, particularly in tokenization.
This study evaluates the training and performance of three common tokenization
algorithms in comparison to other popular methods. Specifically, Byte-Pair
Encoding (BPE), WordPiece, and SentencePiece (Unigram) were evaluated for their
suitability for Dzongkha. Performance was assessed using metrics like Subword
Fertility, Proportion of Continued Words, Normalized Sequence Length, and
execution time. The results show that while all three algorithms demonstrate
potential, SentencePiece is the most effective for Dzongkha tokenization,
paving the way for further NLP advancements. This underscores the need for
tailored approaches for low-resource languages and ongoing research. In this
study, we presented three tokenization algorithms for Dzongkha, paving the way
for building Dzongkha Large Language Models.

</details>


### [3] [Toxicity Red-Teaming: Benchmarking LLM Safety in Singapore's Low-Resource Languages](https://arxiv.org/abs/2509.15260)
*Yujia Hu,Ming Shan Hee,Preslav Nakov,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: 本文介绍了SGToxicGuard数据集和评估框架，用于在新加坡多语言环境下评估大语言模型的安全性，发现现有模型在安全防护方面存在严重漏洞。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在低资源多语言环境中的安全机制研究不足，特别是在新加坡这样语言多样化的环境中，需要系统评估模型的安全性和文化敏感性。

Method: 采用红队测试方法，在对话、问答和内容创作三个真实场景下，使用包含Singlish、中文、马来语和泰米尔语的SGToxicGuard数据集对多语言大语言模型进行系统性测试。

Result: 实验结果显示，现有最先进的多语言大语言模型在安全防护方面存在严重漏洞，特别是在处理文化敏感内容和毒性缓解方面表现不佳。

Conclusion: 该研究为在语言多样化环境中构建更安全、更具包容性的人工智能系统奠定了基础，提供了关于文化敏感性和毒性缓解的可操作见解。

Abstract: The advancement of Large Language Models (LLMs) has transformed natural
language processing; however, their safety mechanisms remain under-explored in
low-resource, multilingual settings. Here, we aim to bridge this gap. In
particular, we introduce \textsf{SGToxicGuard}, a novel dataset and evaluation
framework for benchmarking LLM safety in Singapore's diverse linguistic
context, including Singlish, Chinese, Malay, and Tamil. SGToxicGuard adopts a
red-teaming approach to systematically probe LLM vulnerabilities in three
real-world scenarios: \textit{conversation}, \textit{question-answering}, and
\textit{content composition}. We conduct extensive experiments with
state-of-the-art multilingual LLMs, and the results uncover critical gaps in
their safety guardrails. By offering actionable insights into cultural
sensitivity and toxicity mitigation, we lay the foundation for safer and more
inclusive AI systems in linguistically diverse environments.\footnote{Link to
the dataset: https://github.com/Social-AI-Studio/SGToxicGuard.}
\textcolor{red}{Disclaimer: This paper contains sensitive content that may be
disturbing to some readers.}

</details>


### [4] [PolBiX: Detecting LLMs' Political Bias in Fact-Checking through X-phemisms](https://arxiv.org/abs/2509.15335)
*Charlott Jakob,David Harbecke,Patrick Parschan,Pia Wenzel Neves,Vera Schmitt*

Main category: cs.CL

TL;DR: 本文通过构建事实相同但政治含义不同的德语声明对，系统评估了大型语言模型在事实核查任务中的政治偏见，发现判断性词汇比政治倾向更显著影响真实性评估。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地用于需要客观评估的应用，但政治偏见可能影响其客观性。现有研究发现LLMs偏好左倾立场，但对事实核查等下游任务的影响尚未充分探索。

Method: 通过将德语声明中的词汇替换为委婉语或贬义词，构建事实等效但政治含义不同的最小对，评估六个LLMs在分类这些声明为真或假时的一致性。

Result: 研究发现判断性词汇的存在比政治倾向更显著影响真实性评估。少数模型显示出政治偏见倾向，但通过明确要求客观性的提示无法缓解这种偏见。

Conclusion: 政治偏见在事实核查任务中的影响相对有限，判断性词汇是更重要的影响因素，提示工程对缓解偏见效果不明显。

Abstract: Large Language Models are increasingly used in applications requiring
objective assessment, which could be compromised by political bias. Many
studies found preferences for left-leaning positions in LLMs, but downstream
effects on tasks like fact-checking remain underexplored. In this study, we
systematically investigate political bias through exchanging words with
euphemisms or dysphemisms in German claims. We construct minimal pairs of
factually equivalent claims that differ in political connotation, to assess the
consistency of LLMs in classifying them as true or false. We evaluate six LLMs
and find that, more than political leaning, the presence of judgmental words
significantly influences truthfulness assessment. While a few models show
tendencies of political bias, this is not mitigated by explicitly calling for
objectivism in prompts.

</details>


### [5] [Quantifying Self-Awareness of Knowledge in Large Language Models](https://arxiv.org/abs/2509.15339)
*Yeongbin Seo,Dongha Lee,Jinyoung Yeo*

Main category: cs.CL

TL;DR: 本文质疑LLM幻觉预测中的自意识解释，提出问题侧捷径效应(AQE)来量化问题意识贡献，并引入SCAO方法增强模型侧信号，实验证明SCAO在减少问题侧线索时表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前LLM幻觉预测常被解释为自意识表现，但作者认为这可能源于问题侧的表面模式利用而非真正的模型侧内省。

Method: 提出近似问题侧效应(AQE)量化问题意识贡献，并开发SCAO(单词回答语义压缩)方法增强模型侧信号使用。

Result: 分析显示现有成功多来自问题侧模式利用，SCAO在减少问题侧线索时表现稳定优异。

Conclusion: SCAO方法能有效促进LLM真正的自意识发展，而非依赖问题侧捷径。

Abstract: Hallucination prediction in large language models (LLMs) is often interpreted
as a sign of self-awareness. However, we argue that such performance can arise
from question-side shortcuts rather than true model-side introspection. To
disentangle these factors, we propose the Approximate Question-side Effect
(AQE), which quantifies the contribution of question-awareness. Our analysis
across multiple datasets reveals that much of the reported success stems from
exploiting superficial patterns in questions. We further introduce SCAO
(Semantic Compression by Answering in One word), a method that enhances the use
of model-side signals. Experiments show that SCAO achieves strong and
consistent performance, particularly in settings with reduced question-side
cues, highlighting its effectiveness in fostering genuine self-awareness in
LLMs.

</details>


### [6] [Real, Fake, or Manipulated? Detecting Machine-Influenced Text](https://arxiv.org/abs/2509.15350)
*Yitong Wang,Zhongping Zhang,Margherita Piana,Zheng Zhou,Peter Gerstoft,Bryan A. Plummer*

Main category: cs.CL

TL;DR: 本文提出了HERO检测器，用于区分四种不同类型的文本：人工撰写、机器生成、机器润色和机器翻译，通过结合长度专用模型和子类别指导模块，在五个LLM和六个领域上优于现有方法2.5-3 mAP。


<details>
  <summary>Details</summary>
Motivation: 现有机器生成文本检测方法主要关注区分人工与机器撰写，忽略了细粒度使用意图（如润色、翻译等），而不同使用意图可能具有不同的风险（如完全机器生成的文本更可能传播错误信息）。

Method: HERO采用分层长度鲁棒的方法，结合长度专用模型的预测，并引入子类别指导模块来鼓励易混淆类别（如不同源语言）的分离。

Result: 在五个LLM和六个领域上的广泛实验表明，HERO平均比最先进方法高出2.5-3 mAP。

Conclusion: HERO能够有效区分机器影响文本的细粒度类别，为理解LLM使用意图提供了更精确的检测工具。

Abstract: Large Language Model (LLMs) can be used to write or modify documents,
presenting a challenge for understanding the intent behind their use. For
example, benign uses may involve using LLM on a human-written document to
improve its grammar or to translate it into another language. However, a
document entirely produced by a LLM may be more likely to be used to spread
misinformation than simple translation (\eg, from use by malicious actors or
simply by hallucinating). Prior works in Machine Generated Text (MGT) detection
mostly focus on simply identifying whether a document was human or machine
written, ignoring these fine-grained uses. In this paper, we introduce a
HiErarchical, length-RObust machine-influenced text detector (HERO), which
learns to separate text samples of varying lengths from four primary types:
human-written, machine-generated, machine-polished, and machine-translated.
HERO accomplishes this by combining predictions from length-specialist models
that have been trained with Subcategory Guidance. Specifically, for categories
that are easily confused (\eg, different source languages), our Subcategory
Guidance module encourages separation of the fine-grained categories, boosting
performance. Extensive experiments across five LLMs and six domains demonstrate
the benefits of our HERO, outperforming the state-of-the-art by 2.5-3 mAP on
average.

</details>


### [7] [Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing](https://arxiv.org/abs/2509.15361)
*Zichen Wu,Hsiu-Yuan Huang,Yunfang Wu*

Main category: cs.CL

TL;DR: 本文提出了一种基于因果中介的新型去偏框架，通过区分核心语义与虚假上下文来解决多模态大语言模型中的表面相关性偏差问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在整合视觉和文本信息方面表现出强大能力，但经常依赖虚假相关性，这削弱了其在复杂多模态推理任务中的鲁棒性和泛化能力。

Method: 通过反事实示例区分核心语义与虚假上下文进行训练阶段去偏，并采用具有动态路由的混合专家架构来选择性激活模态特定的去偏专家。

Result: 在多模态讽刺检测和情感分析任务上的实证评估表明，该框架显著优于单模态去偏策略和现有最先进模型。

Conclusion: 提出的因果中介去偏框架有效解决了MLLMs中的表面相关性偏差问题，提升了模型的鲁棒性和泛化性能。

Abstract: Multimodal Large Language Models (MLLMs) have shown substantial capabilities
in integrating visual and textual information, yet frequently rely on spurious
correlations, undermining their robustness and generalization in complex
multimodal reasoning tasks. This paper addresses the critical challenge of
superficial correlation bias in MLLMs through a novel causal mediation-based
debiasing framework. Specially, we distinguishing core semantics from spurious
textual and visual contexts via counterfactual examples to activate
training-stage debiasing and employ a Mixture-of-Experts (MoE) architecture
with dynamic routing to selectively engages modality-specific debiasing
experts. Empirical evaluation on multimodal sarcasm detection and sentiment
analysis tasks demonstrates that our framework significantly surpasses unimodal
debiasing strategies and existing state-of-the-art models.

</details>


### [8] [Speech Language Models for Under-Represented Languages: Insights from Wolof](https://arxiv.org/abs/2509.15362)
*Yaya Sy,Dioula Doucouré,Christophe Cerisara,Irina Illina*

Main category: cs.CL

TL;DR: 训练沃洛夫语语音语言模型的旅程，重点在于收集大规模自发高质量语音数据，通过HuBERT持续预训练提升ASR性能，并整合到沃洛夫语LLM中实现首个语音LLM，支持语音翻译和思维链推理。


<details>
  <summary>Details</summary>
Motivation: 为沃洛夫语这一西非代表性不足的语言开发首个语音语言模型，扩展其语音识别和翻译能力。

Method: 收集大规模高质量沃洛夫语语音数据，在HuBERT基础上进行持续预训练，将语音编码器整合到沃洛夫语LLM中，训练支持思维链推理的语音LLM。

Result: 模型在语音识别和语音翻译任务上表现优异，超越了基础模型和非洲中心模型。

Conclusion: 成功开发了首个沃洛夫语语音LLM，证明了该方法对低资源语言的有效性，模型和代码将开源共享。

Abstract: We present our journey in training a speech language model for Wolof, an
underrepresented language spoken in West Africa, and share key insights. We
first emphasize the importance of collecting large-scale, spontaneous,
high-quality speech data, and show that continued pretraining HuBERT on this
dataset outperforms both the base model and African-centric models on ASR. We
then integrate this speech encoder into a Wolof LLM to train the first Speech
LLM for this language, extending its capabilities to tasks such as speech
translation. Furthermore, we explore training the Speech LLM to perform
multi-step Chain-of-Thought before transcribing or translating. Our results
show that the Speech LLM not only improves speech recognition but also performs
well in speech translation. The models and the code will be openly shared.

</details>


### [9] [Frustratingly Easy Data Augmentation for Low-Resource ASR](https://arxiv.org/abs/2509.15373)
*Katsumi Ibaraki,David Chiang*

Main category: cs.CL

TL;DR: 本文提出了三种独立的低资源语音识别数据增强方法，通过文本生成和文本转语音技术合成音频数据，在四种极低资源语言上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言语音识别中数据稀缺的问题，利用现有标注数据生成更多训练样本。

Method: 使用三种文本生成方法（基于同义词替换、随机替换和LLM生成）创建新文本，然后通过TTS生成合成音频，结合原始音频微调预训练模型。

Result: 在四种低资源语言上获得显著性能提升，其中Nashta语言WER绝对降低14.3%，方法在高资源语言如英语上也有效。

Conclusion: 提出的数据增强方法具有广泛适用性，能有效提升低资源语音识别性能。

Abstract: This paper introduces three self-contained data augmentation methods for
low-resource Automatic Speech Recognition (ASR). Our techniques first generate
novel text--using gloss-based replacement, random replacement, or an LLM-based
approach--and then apply Text-to-Speech (TTS) to produce synthetic audio. We
apply these methods, which leverage only the original annotated data, to four
languages with extremely limited resources (Vatlongos, Nashta, Shinekhen
Buryat, and Kakabe). Fine-tuning a pretrained Wav2Vec2-XLSR-53 model on a
combination of the original audio and generated synthetic data yields
significant performance gains, including a 14.3% absolute WER reduction for
Nashta. The methods prove effective across all four low-resource languages and
also show utility for high-resource languages like English, demonstrating their
broad applicability.

</details>


### [10] [Quantifying Uncertainty in Natural Language Explanations of Large Language Models for Question Answering](https://arxiv.org/abs/2509.15403)
*Yangyi Li,Mengdi Huai*

Main category: cs.CL

TL;DR: 本文提出了一种为大语言模型生成的自然语言解释提供有效不确定性保证的新框架，特别是在存在噪声的医学查询场景下。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽然开发了自然语言解释方法来理解大语言模型的行为，但缺乏对这些生成解释的有效不确定性量化，这在理解解释的可信度方面至关重要。

Method: 提出了一种后处理且模型无关的不确定性估计框架，并设计了一种新颖的鲁棒不确定性估计方法，即使在噪声条件下也能保持有效的不确定性保证。

Result: 在问答任务上的大量实验证明了所提出方法的理想性能。

Conclusion: 该工作填补了为自然语言解释提供不确定性量化的研究空白，为大语言模型解释的可信度评估提供了有效工具。

Abstract: Large language models (LLMs) have shown strong capabilities, enabling
concise, context-aware answers in question answering (QA) tasks. The lack of
transparency in complex LLMs has inspired extensive research aimed at
developing methods to explain large language behaviors. Among existing
explanation methods, natural language explanations stand out due to their
ability to explain LLMs in a self-explanatory manner and enable the
understanding of model behaviors even when the models are closed-source.
However, despite these promising advancements, there is no existing work
studying how to provide valid uncertainty guarantees for these generated
natural language explanations. Such uncertainty quantification is critical in
understanding the confidence behind these explanations. Notably, generating
valid uncertainty estimates for natural language explanations is particularly
challenging due to the auto-regressive generation process of LLMs and the
presence of noise in medical inquiries. To bridge this gap, in this work, we
first propose a novel uncertainty estimation framework for these generated
natural language explanations, which provides valid uncertainty guarantees in a
post-hoc and model-agnostic manner. Additionally, we also design a novel robust
uncertainty estimation method that maintains valid uncertainty guarantees even
under noise. Extensive experiments on QA tasks demonstrate the desired
performance of our methods.

</details>


### [11] [Deep learning and abstractive summarisation for radiological reports: an empirical study for adapting the PEGASUS models' family with scarce data](https://arxiv.org/abs/2509.15419)
*Claudio Benzoni,Martina Langhals,Martin Boeker,Luise Modersohn,Máté E. Maros*

Main category: cs.CL

TL;DR: 本文研究了在医学领域使用PEGASUS和PEGASUS-X模型进行抽象摘要的微调过程，分析了在数据稀缺情况下如何避免过拟合和欠拟合问题。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能快速发展，但在医学等敏感和数据受限领域，抽象摘要仍然具有挑战性。随着医学影像数量增加，自动化复杂医学文本摘要工具的需求日益重要。

Method: 使用PEGASUS和PEGASUS-X模型家族，在一个中等规模的放射学报告公共数据集上进行微调。对每个模型，全面评估了两个不同检查点在不同训练数据规模下的表现，使用词汇和语义指标监控模型在固定验证集上的训练历史。

Result: PEGASUS表现出不同阶段，可能与epoch-wise双下降或峰值-下降-恢复行为相关。对于PEGASUS-X，使用更大的检查点会导致性能下降。

Conclusion: 这项工作强调了在处理稀缺训练数据时，微调高表达能力模型所面临的挑战和风险，为未来在专业领域开发更稳健的摘要模型微调策略奠定了基础。

Abstract: Regardless of the rapid development of artificial intelligence, abstractive
summarisation is still challenging for sensitive and data-restrictive domains
like medicine. With the increasing number of imaging, the relevance of
automated tools for complex medical text summarisation is expected to become
highly relevant. In this paper, we investigated the adaptation via fine-tuning
process of a non-domain-specific abstractive summarisation encoder-decoder
model family, and gave insights to practitioners on how to avoid over- and
underfitting. We used PEGASUS and PEGASUS-X, on a medium-sized radiological
reports public dataset. For each model, we comprehensively evaluated two
different checkpoints with varying sizes of the same training data. We
monitored the models' performances with lexical and semantic metrics during the
training history on the fixed-size validation set. PEGASUS exhibited different
phases, which can be related to epoch-wise double-descent, or
peak-drop-recovery behaviour. For PEGASUS-X, we found that using a larger
checkpoint led to a performance detriment. This work highlights the challenges
and risks of fine-tuning models with high expressivity when dealing with scarce
training data, and lays the groundwork for future investigations into more
robust fine-tuning strategies for summarisation models in specialised domains.

</details>


### [12] [BiRQ: Bi-Level Self-Labeling Random Quantization for Self-Supervised Speech Recognition](https://arxiv.org/abs/2509.15430)
*Liuyuan Jiang,Xiaodong Cui,Brian Kingsbury,Tianyi Chen,Lisha Chen*

Main category: cs.CL

TL;DR: BiRQ是一个双层自监督学习框架，结合了BEST-RQ的效率和HuBERT式标签增强的优势，通过重用模型自身作为伪标签生成器，实现端到端的标签优化。


<details>
  <summary>Details</summary>
Motivation: 解决语音自监督学习中伪标签生成的核心挑战：强标签（如HuBERT）性能好但依赖外部编码器和多阶段流程，而高效方法（如BEST-RQ）简单但标签质量较弱。

Method: 使用双层优化框架，中间表示通过随机投影量化器离散化生成增强标签，同时使用原始输入的锚定标签稳定训练；采用可微分Gumbel-softmax选择实现端到端训练。

Result: 在多个数据集（960小时LibriSpeech、150小时AMI会议、5000小时YODAS）上验证，BiRQ在保持低复杂度和计算效率的同时，持续优于BEST-RQ。

Conclusion: BiRQ消除了对外部标签编码器的需求，降低了内存成本，实现了端到端的迭代标签优化，为语音表示学习提供了高效且有效的解决方案。

Abstract: Speech is a rich signal, and labeled audio-text pairs are costly, making
self-supervised learning essential for scalable representation learning. A core
challenge in speech SSL is generating pseudo-labels that are both informative
and efficient: strong labels, such as those used in HuBERT, improve downstream
performance but rely on external encoders and multi-stage pipelines, while
efficient methods like BEST-RQ achieve simplicity at the cost of weaker labels.
We propose BiRQ, a bilevel SSL framework that combines the efficiency of
BEST-RQ with the refinement benefits of HuBERT-style label enhancement. The key
idea is to reuse part of the model itself as a pseudo-label generator:
intermediate representations are discretized by a random-projection quantizer
to produce enhanced labels, while anchoring labels derived directly from the
raw input stabilize training and prevent collapse. Training is formulated as an
efficient first-order bilevel optimization problem, solved end-to-end with
differentiable Gumbel-softmax selection. This design eliminates the need for
external label encoders, reduces memory cost, and enables iterative label
refinement in an end-to-end fashion. BiRQ consistently improves over BEST-RQ
while maintaining low complexity and computational efficiency. We validate our
method on various datasets, including 960-hour LibriSpeech, 150-hour AMI
meetings and 5,000-hour YODAS, demonstrating consistent gains over BEST-RQ.

</details>


### [13] [PILOT: Steering Synthetic Data Generation with Psychological & Linguistic Output Targeting](https://arxiv.org/abs/2509.15447)
*Caitlin Cisar,Emily Sheffield,Joshua Drake,Alden Harrell,Subramanian Chidambaram,Nikita Nangia,Vinayak Arannil,Alex Williams*

Main category: cs.CL

TL;DR: PILOT是一个两阶段框架，通过结构化心理语言学配置文件来引导大语言模型生成更精确可控的合成数据，解决了传统自然语言人物描述中属性强调不明确的问题。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI应用依赖自然语言人物描述来引导合成数据生成，但这种方法会迫使模型对需要强调的属性做出非预期的推断，限制了输出的精确控制。

Method: PILOT框架分为两个阶段：第一阶段将自然语言人物描述转换为跨语言和心理维度的标准化多维配置文件；第二阶段使用这些配置文件沿可测量变化轴引导生成过程。评估了三种引导方法：自然语言人物引导(NPS)、基于模式的引导(SBS)和混合人物-模式引导(HPS)。

Result: 基于模式的方法显著减少了人工感强的人物重复，提高了输出连贯性，轮廓分数从0.098提升到0.237，主题纯度从0.773提升到0.957。SBS产生更简洁、主题一致性更高的输出，NPS提供更大的词汇多样性但可预测性降低，HPS在两者之间取得平衡。

Conclusion: PILOT在所有条件下都保持了高质量响应，不同引导方法之间没有统计学显著差异。该框架在输出精确控制和多样性之间提供了有效的权衡解决方案。

Abstract: Generative AI applications commonly leverage user personas as a steering
mechanism for synthetic data generation, but reliance on natural language
representations forces models to make unintended inferences about which
attributes to emphasize, limiting precise control over outputs. We introduce
PILOT (Psychological and Linguistic Output Targeting), a two-phase framework
for steering large language models with structured psycholinguistic profiles.
In Phase 1, PILOT translates natural language persona descriptions into
multidimensional profiles with normalized scores across linguistic and
psychological dimensions. In Phase 2, these profiles guide generation along
measurable axes of variation. We evaluate PILOT across three state-of-the-art
LLMs (Mistral Large 2, Deepseek-R1, LLaMA 3.3 70B) using 25 synthetic personas
under three conditions: Natural-language Persona Steering (NPS), Schema-Based
Steering (SBS), and Hybrid Persona-Schema Steering (HPS). Results demonstrate
that schema-based approaches significantly reduce artificial-sounding persona
repetition while improving output coherence, with silhouette scores increasing
from 0.098 to 0.237 and topic purity from 0.773 to 0.957. Our analysis reveals
a fundamental trade-off: SBS produces more concise outputs with higher topical
consistency, while NPS offers greater lexical diversity but reduced
predictability. HPS achieves a balance between these extremes, maintaining
output variety while preserving structural consistency. Expert linguistic
evaluation confirms that PILOT maintains high response quality across all
conditions, with no statistically significant differences between steering
approaches.

</details>


### [14] [Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding](https://arxiv.org/abs/2509.15476)
*Zhu Li,Xiyuan Gao,Yuqing Zhang,Shekhar Nayak,Matt Coler*

Main category: cs.CL

TL;DR: 本文系统评估了LLMs和多模态LLMs在英语和中文讽刺检测任务上的表现，探索了零样本、少样本和LoRA微调设置，发现音频模型在单模态中表现最佳，而文本-音频和音频-视觉组合优于单模态和三模态模型。


<details>
  <summary>Details</summary>
Motivation: 讽刺检测在自然语言理解中具有挑战性，因为讽刺意图通常依赖于跨模态的细微线索。现有研究主要关注文本或视觉-文本讽刺，而全面的音频-视觉-文本讽刺理解尚未充分探索。

Method: 采用大型语言模型和多模态LLMs，在英语（MUStARD++）和中文（MCSD 1.0）数据集上进行零样本、少样本和LoRA微调评估。除了直接分类，还探索了模型作为特征编码器，通过协作门控融合模块整合其表示。

Result: 实验结果显示，基于音频的模型在单模态中表现最强，而文本-音频和音频-视觉组合优于单模态和三模态模型。此外，如Qwen-Omni等MLLMs在零样本和微调设置下表现出竞争力。

Conclusion: 研究结果突显了MLLMs在跨语言、音频-视觉-文本讽刺理解方面的潜力。

Abstract: Sarcasm detection remains a challenge in natural language understanding, as
sarcastic intent often relies on subtle cross-modal cues spanning text, speech,
and vision. While prior work has primarily focused on textual or visual-textual
sarcasm, comprehensive audio-visual-textual sarcasm understanding remains
underexplored. In this paper, we systematically evaluate large language models
(LLMs) and multimodal LLMs for sarcasm detection on English (MUStARD++) and
Chinese (MCSD 1.0) in zero-shot, few-shot, and LoRA fine-tuning settings. In
addition to direct classification, we explore models as feature encoders,
integrating their representations through a collaborative gating fusion module.
Experimental results show that audio-based models achieve the strongest
unimodal performance, while text-audio and audio-vision combinations outperform
unimodal and trimodal models. Furthermore, MLLMs such as Qwen-Omni show
competitive zero-shot and fine-tuned performance. Our findings highlight the
potential of MLLMs for cross-lingual, audio-visual-textual sarcasm
understanding.

</details>


### [15] [Red Teaming Multimodal Language Models: Evaluating Harm Across Prompt Modalities and Models](https://arxiv.org/abs/2509.15478)
*Madison Van Doren,Casey Ford,Emily Dix*

Main category: cs.CL

TL;DR: 本研究评估了四种主流多模态大语言模型（GPT-4o、Claude Sonnet 3.5、Pixtral 12B和Qwen VL Plus）在对抗性提示下的安全性表现，发现不同模型和模态之间存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型在现实应用中的广泛部署，其在对抗条件下的安全性问题尚未得到充分探索，需要系统评估这些模型的安全性能。

Method: 研究团队使用26名红队成员生成的726个针对非法活动、虚假信息和不道德行为的提示，对四种模型进行测试，并由17名标注者对2,904个模型输出进行5级有害性评分。

Result: Pixtral 12B的有害响应率最高（约62%），Claude Sonnet 3.5最具抵抗力（约10%）。出乎意料的是，纯文本提示比多模态提示更能绕过安全机制。

Conclusion: 研究结果强调了随着MLLM的广泛部署，迫切需要建立稳健的多模态安全基准。

Abstract: Multimodal large language models (MLLMs) are increasingly used in real world
applications, yet their safety under adversarial conditions remains
underexplored. This study evaluates the harmlessness of four leading MLLMs
(GPT-4o, Claude Sonnet 3.5, Pixtral 12B, and Qwen VL Plus) when exposed to
adversarial prompts across text-only and multimodal formats. A team of 26 red
teamers generated 726 prompts targeting three harm categories: illegal
activity, disinformation, and unethical behaviour. These prompts were submitted
to each model, and 17 annotators rated 2,904 model outputs for harmfulness
using a 5-point scale. Results show significant differences in vulnerability
across models and modalities. Pixtral 12B exhibited the highest rate of harmful
responses (~62%), while Claude Sonnet 3.5 was the most resistant (~10%).
Contrary to expectations, text-only prompts were slightly more effective at
bypassing safety mechanisms than multimodal ones. Statistical analysis
confirmed that both model type and input modality were significant predictors
of harmfulness. These findings underscore the urgent need for robust,
multimodal safety benchmarks as MLLMs are deployed more widely.

</details>


### [16] [mucAI at BAREC Shared Task 2025: Towards Uncertainty Aware Arabic Readability Assessment](https://arxiv.org/abs/2509.15485)
*Ahmed Abdou*

Main category: cs.CL

TL;DR: 提出了一种简单、模型无关的后处理技术，用于阿拉伯语细粒度可读性分类，通过保形预测生成具有覆盖保证的预测集，并使用软最大重归一化概率计算加权平均值，从而减少高惩罚误分类。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语细粒度可读性分类中的不确定性，减少高惩罚误分类，提高分类准确性。

Method: 应用保形预测生成预测集，使用软最大重归一化概率计算加权平均值，进行不确定性感知解码。

Result: 在BAREC 2025共享任务中，严格赛道的句子级测试和盲测QWK分别达到84.9%和85.7%，文档级达到73.3%，QWK一致提高1-3点。

Conclusion: 该方法结合统计保证和实际可用性，使人工评审员能够专注于少数合理级别，适用于阿拉伯语教育评估。

Abstract: We present a simple, model-agnostic post-processing technique for
fine-grained Arabic readability classification in the BAREC 2025 Shared Task
(19 ordinal levels). Our method applies conformal prediction to generate
prediction sets with coverage guarantees, then computes weighted averages using
softmax-renormalized probabilities over the conformal sets. This
uncertainty-aware decoding improves Quadratic Weighted Kappa (QWK) by reducing
high-penalty misclassifications to nearer levels. Our approach shows consistent
QWK improvements of 1-3 points across different base models. In the strict
track, our submission achieves QWK scores of 84.9\%(test) and 85.7\% (blind
test) for sentence level, and 73.3\% for document level. For Arabic educational
assessment, this enables human reviewers to focus on a handful of plausible
levels, combining statistical guarantees with practical usability.

</details>


### [17] [LLM Cache Bandit Revisited: Addressing Query Heterogeneity for Cost-Effective LLM Inference](https://arxiv.org/abs/2509.15515)
*Hantao Yang,Hong Xie,Defu Lian,Enhong Chen*

Main category: cs.CL

TL;DR: 本文重新审视了LLM缓存强盗问题，特别关注处理查询异质性以实现成本效益的LLM推理。通过将最优缓存选择建模为背包问题，提出了一种基于累积的策略，在理论和实验上都取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常假设查询大小均匀，但实际应用中查询大小存在异质性，这给缓存选择引入了组合结构，使得缓存替换过程在计算和统计上都更具挑战性。

Method: 将最优缓存选择视为背包问题，采用基于累积的策略来有效平衡计算开销和缓存更新。

Result: 理论分析证明算法遗憾达到O(√MNT)边界，相比Berkeley的O(MN√T)结果改进了√MN系数。实验基于真实数据表明算法将总成本降低了约12%。

Conclusion: 提出的方法有效解决了查询异质性带来的挑战，在理论和实际性能上都优于现有方法，为成本效益的LLM推理提供了实用解决方案。

Abstract: This paper revisits the LLM cache bandit problem, with a special focus on
addressing the query heterogeneity for cost-effective LLM inference. Previous
works often assume uniform query sizes. Heterogeneous query sizes introduce a
combinatorial structure for cache selection, making the cache replacement
process more computationally and statistically challenging. We treat optimal
cache selection as a knapsack problem and employ an accumulation-based strategy
to effectively balance computational overhead and cache updates. In theoretical
analysis, we prove that the regret of our algorithm achieves an $O(\sqrt{MNT})$
bound, improving the coefficient of $\sqrt{MN}$ compared to the $O(MN\sqrt{T})$
result in Berkeley, where $N$ is the total number of queries and $M$ is the
cache size. Additionally, we also provide a problem-dependent bound, which was
absent in previous works. The experiment rely on real-world data show that our
algorithm reduces the total cost by approximately 12\%.

</details>


### [18] [How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages](https://arxiv.org/abs/2509.15518)
*Siyang Wu,Zhewei Sun*

Main category: cs.CL

TL;DR: 本文系统比较了人类和机器生成的俚语用法，发现LLMs在俚语理解上存在显著偏见，虽然掌握了俚语的创造性特征，但与人类认知不够一致，限制了其在语言分析等外推任务中的应用。


<details>
  <summary>Details</summary>
Motivation: 俚语作为非正式语言对NLP系统构成挑战，虽然LLMs在俚语检测和解释等中介任务中应用广泛，但其泛化性和可靠性取决于模型是否捕获了与人类俚语用法一致的结构知识。

Method: 构建评估框架，从三个核心方面比较人类（来自在线俚语词典OSD）和机器（GPT-4o和Llama-3）生成的俚语用法：1）反映机器感知俚语系统性偏见的特征；2）词汇创新和词语重用的创造性；3）作为模型蒸馏金标准示例的信息量。

Result: 发现LLMs在俚语感知上存在显著偏见，虽然模型掌握了俚语创造性的重要知识，但这些知识与人类认知不够一致。

Conclusion: LLMs在俚语理解方面仍有局限，其知识不足以支持语言分析等外推任务的应用。

Abstract: Slang is a commonly used type of informal language that poses a daunting
challenge to NLP systems. Recent advances in large language models (LLMs),
however, have made the problem more approachable. While LLM agents are becoming
more widely applied to intermediary tasks such as slang detection and slang
interpretation, their generalizability and reliability are heavily dependent on
whether these models have captured structural knowledge about slang that align
well with human attested slang usages. To answer this question, we contribute a
systematic comparison between human and machine-generated slang usages. Our
evaluative framework focuses on three core aspects: 1) Characteristics of the
usages that reflect systematic biases in how machines perceive slang, 2)
Creativity reflected by both lexical coinages and word reuses employed by the
slang usages, and 3) Informativeness of the slang usages when used as
gold-standard examples for model distillation. By comparing human-attested
slang usages from the Online Slang Dictionary (OSD) and slang generated by
GPT-4o and Llama-3, we find significant biases in how LLMs perceive slang. Our
results suggest that while LLMs have captured significant knowledge about the
creative aspects of slang, such knowledge does not align with humans
sufficiently to enable LLMs for extrapolative tasks such as linguistic
analyses.

</details>


### [19] [A method for improving multilingual quality and diversity of instruction fine-tuning datasets](https://arxiv.org/abs/2509.15549)
*Chunguang Zhao,Yilun Liu,Pufan Zeng,Yuanchang Luo,Shimin Tao,Minggui He,Weibin Meng,Song Xu,Ziang Chen,Chen Liu,Hongxia Ma,Li Zhang,Boxing Chen,Daimeng Wei*

Main category: cs.CL

TL;DR: 提出了M-DaQ方法，通过选择高质量和语义多样性的多语言IFT样本来提升LLMs的多语言能力，并在18种语言上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 多语言指令微调对LLMs在多样化语言和文化环境中的泛化至关重要，但高质量多语言训练数据的稀缺和相应构建方法的缺乏是主要瓶颈。

Method: 引入M-DaQ方法，选择高质量和语义多样性的多语言IFT样本，并首次系统研究多语言环境下的表面对齐假设。

Result: 在18种语言上的实验表明，使用M-DaQ方法微调的模型相比基线有显著性能提升，胜率超过60%，人类评估进一步验证了这些增益。

Conclusion: M-DaQ方法能有效提升LLMs的多语言能力，并发布了代码以支持未来研究。

Abstract: Multilingual Instruction Fine-Tuning (IFT) is essential for enabling large
language models (LLMs) to generalize effectively across diverse linguistic and
cultural contexts. However, the scarcity of high-quality multilingual training
data and corresponding building method remains a critical bottleneck. While
data selection has shown promise in English settings, existing methods often
fail to generalize across languages due to reliance on simplistic heuristics or
language-specific assumptions. In this work, we introduce Multilingual Data
Quality and Diversity (M-DaQ), a novel method for improving LLMs
multilinguality, by selecting high-quality and semantically diverse
multilingual IFT samples. We further conduct the first systematic investigation
of the Superficial Alignment Hypothesis (SAH) in multilingual setting.
Empirical results across 18 languages demonstrate that models fine-tuned with
M-DaQ method achieve significant performance gains over vanilla baselines over
60% win rate. Human evaluations further validate these gains, highlighting the
increment of cultural points in the response. We release the M-DaQ code to
support future research.

</details>


### [20] [DNA-DetectLLM: Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm](https://arxiv.org/abs/2509.15550)
*Xiaowei Zhu,Yubing Ren,Fang Fang,Qingfeng Tan,Shi Wang,Yanan Cao*

Main category: cs.CL

TL;DR: 本文提出了一种受DNA启发的零样本检测方法DNA-DetectLLM，用于区分AI生成文本和人类撰写文本，通过修复过程量化差异，在多个基准数据集上实现了5.55%的AUROC和2.08%的F1分数提升。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，AI生成文本与人类撰写文本之间的界限日益模糊，带来了错误信息、作者身份模糊和知识产权等社会风险，迫切需要可靠的AI文本检测方法。

Method: 提出DNA-DetectLLM方法：采用DNA启发的视角，通过修复过程直接捕捉AI生成文本与人类文本的内在差异；为每个输入构建理想的AI生成序列，迭代修复非最优标记，并将累计修复努力量化为可解释的检测信号。

Result: 实证评估表明，该方法实现了最先进的检测性能，对多种对抗攻击和输入长度表现出强鲁棒性。在多个公共基准数据集上，AUROC相对提升5.55%，F1分数相对提升2.08%。

Conclusion: DNA-DetectLLM提供了一种有效且可解释的AI文本检测解决方案，为解决AI生成文本带来的社会风险提供了可靠的技术手段。

Abstract: The rapid advancement of large language models (LLMs) has blurred the line
between AI-generated and human-written text. This progress brings societal
risks such as misinformation, authorship ambiguity, and intellectual property
concerns, highlighting the urgent need for reliable AI-generated text detection
methods. However, recent advances in generative language modeling have resulted
in significant overlap between the feature distributions of human-written and
AI-generated text, blurring classification boundaries and making accurate
detection increasingly challenging. To address the above challenges, we propose
a DNA-inspired perspective, leveraging a repair-based process to directly and
interpretably capture the intrinsic differences between human-written and
AI-generated text. Building on this perspective, we introduce DNA-DetectLLM, a
zero-shot detection method for distinguishing AI-generated and human-written
text. The method constructs an ideal AI-generated sequence for each input,
iteratively repairs non-optimal tokens, and quantifies the cumulative repair
effort as an interpretable detection signal. Empirical evaluations demonstrate
that our method achieves state-of-the-art detection performance and exhibits
strong robustness against various adversarial attacks and input lengths.
Specifically, DNA-DetectLLM achieves relative improvements of 5.55% in AUROC
and 2.08% in F1 score across multiple public benchmark datasets.

</details>


### [21] [Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining](https://arxiv.org/abs/2509.15556)
*Ping Guo,Yubing Ren,Binbin Liu,Fengze Liu,Haobin Lin,Yifan Zhang,Bingni Zhang,Taifeng Wang,Yin Zheng*

Main category: cs.CL

TL;DR: Climb框架通过量化跨语言交互来优化多语言训练数据分配，简化了复杂的多语言优化问题，显著提升了LLMs的多语言性能


<details>
  <summary>Details</summary>
Motivation: 多语言能力对LLMs至关重要，但确定最优语言比例具有挑战性，因为涉及复杂的跨语言交互和对数据集规模的敏感性

Method: 提出Climb框架，引入跨语言交互感知的语言比例，通过两步优化过程：先平衡各语言的边际收益，再最大化语言分配向量的幅度

Result: 实验证明Climb能准确测量跨语言交互，使用Climb优化比例的LLMs在多语言性能上达到SOTA，甚至能与使用更多token训练的开源LLMs竞争

Conclusion: Climb框架为多语言数据分配提供了系统化的优化方法，显著提升了LLMs的多语言能力

Abstract: Large language models (LLMs) have become integral to a wide range of
applications worldwide, driving an unprecedented global demand for effective
multilingual capabilities. Central to achieving robust multilingual performance
is the strategic allocation of language proportions within training corpora.
However, determining optimal language ratios is highly challenging due to
intricate cross-lingual interactions and sensitivity to dataset scale. This
paper introduces Climb (Cross-Lingual Interaction-aware Multilingual
Balancing), a novel framework designed to systematically optimize multilingual
data allocation. At its core, Climb introduces a cross-lingual
interaction-aware language ratio, explicitly quantifying each language's
effective allocation by capturing inter-language dependencies. Leveraging this
ratio, Climb proposes a principled two-step optimization procedure--first
equalizing marginal benefits across languages, then maximizing the magnitude of
the resulting language allocation vectors--significantly simplifying the
inherently complex multilingual optimization problem. Extensive experiments
confirm that Climb can accurately measure cross-lingual interactions across
various multilingual settings. LLMs trained with Climb-derived proportions
consistently achieve state-of-the-art multilingual performance, even achieving
competitive performance with open-sourced LLMs trained with more tokens.

</details>


### [22] [How important is language for human-like intelligence?](https://arxiv.org/abs/2509.15560)
*Gary Lupyan,Hunter Gentry,Martin Zettersten*

Main category: cs.CL

TL;DR: 语言不仅是思想的表达工具，更在人类认知中扮演着变革性角色，能够产生非语言思维无法达到的思想。语言的两个关键特性——紧凑表示和集体智慧的迭代输出——使其成为发展领域通用能力的有力工具。


<details>
  <summary>Details</summary>
Motivation: 重新探讨语言在认知中的角色，挑战传统观点（语言仅是思想的表达工具），探索语言如何促进更通用的人工智能系统和人类智能核心方面的发展。

Method: 通过分析语言的两个核心特性：1）提供抽象概念的紧凑表示，便于推理；2）作为集体智慧迭代输出的文化演化抽象集合。论证这些特性使学习系统能够通过语言学习世界的压缩模型。

Result: 语言暴露的学习系统（无论是生物还是人工）能够逆向工程支撑人类思维的概念和因果结构，获得领域通用的认知能力。

Conclusion: 语言可能是实现更通用人工智能和揭示人类智能核心机制的关键，其压缩表示和文化演化特性为认知发展提供了独特优势。

Abstract: We use language to communicate our thoughts. But is language merely the
expression of thoughts, which are themselves produced by other, nonlinguistic
parts of our minds? Or does language play a more transformative role in human
cognition, allowing us to have thoughts that we otherwise could (or would) not
have? Recent developments in artificial intelligence (AI) and cognitive science
have reinvigorated this old question. We argue that language may hold the key
to the emergence of both more general AI systems and central aspects of human
intelligence. We highlight two related properties of language that make it such
a powerful tool for developing domain--general abilities. First, language
offers compact representations that make it easier to represent and reason
about many abstract concepts (e.g., exact numerosity). Second, these compressed
representations are the iterated output of collective minds. In learning a
language, we learn a treasure trove of culturally evolved abstractions. Taken
together, these properties mean that a sufficiently powerful learning system
exposed to language--whether biological or artificial--learns a compressed
model of the world, reverse engineering many of the conceptual and causal
structures that support human (and human-like) thought.

</details>


### [23] [LiteLong: Resource-Efficient Long-Context Data Synthesis for LLMs](https://arxiv.org/abs/2509.15568)
*Junlong Jia,Xing Wu,Chaochen Gao,Ziyang Chen,Zijia Lin,Zhongzhi Li,Weinong Wang,Haotian Xu,Donghui Jin,Debing Zhang,Binghui Guo*

Main category: cs.CL

TL;DR: LiteLong是一种资源高效的长上下文数据合成方法，通过结构化主题组织和多智能体辩论来生成高质量的长上下文训练数据，降低计算和数据工程成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于相关性聚合的数据合成方法在计算效率方面面临挑战，高质量的长上下文数据对于训练能够处理长文档的大语言模型至关重要。

Method: 利用BISAC图书分类系统提供层次化主题组织，采用多LLM辩论机制生成多样化高质量主题，使用轻量级BM25检索获取相关文档并拼接成128K标记的训练样本。

Result: 在HELMET和Ruler基准测试中表现出竞争力的长上下文性能，并能无缝集成其他长依赖增强方法。

Conclusion: LiteLong通过降低计算和数据工程成本，使高质量长上下文数据合成更加易于实现，促进了长上下文语言训练的进一步研究。

Abstract: High-quality long-context data is essential for training large language
models (LLMs) capable of processing extensive documents, yet existing synthesis
approaches using relevance-based aggregation face challenges of computational
efficiency. We present LiteLong, a resource-efficient method for synthesizing
long-context data through structured topic organization and multi-agent debate.
Our approach leverages the BISAC book classification system to provide a
comprehensive hierarchical topic organization, and then employs a debate
mechanism with multiple LLMs to generate diverse, high-quality topics within
this structure. For each topic, we use lightweight BM25 retrieval to obtain
relevant documents and concatenate them into 128K-token training samples.
Experiments on HELMET and Ruler benchmarks demonstrate that LiteLong achieves
competitive long-context performance and can seamlessly integrate with other
long-dependency enhancement methods. LiteLong makes high-quality long-context
data synthesis more accessible by reducing both computational and data
engineering costs, facilitating further research in long-context language
training.

</details>


### [24] [Relevance to Utility: Process-Supervised Rewrite for RAG](https://arxiv.org/abs/2509.15577)
*Jaeyoung Kim,Jongho Kim,Seung-won Hwang,Seoho Song,Young-In Song*

Main category: cs.CL

TL;DR: 本文提出R2U方法，通过过程监督直接优化生成正确答案的概率，解决检索增强生成系统中检索相关性与生成效用之间的差距问题。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成系统存在检索相关性与生成效用之间的差距，检索到的文档可能在主题上相关但缺乏生成所需的有效推理内容。现有的"桥接"模块尝试重写检索文本以改善生成效果，但未能捕捉真正的文档效用。

Method: 提出R2U方法，关键区别在于通过过程监督直接优化生成正确答案的概率。由于直接监督成本高昂，还提出通过扩展LLM的监督来近似高效的蒸馏管道，帮助较小的重写模型更好地泛化。

Result: 在多个开放域问答基准上评估该方法，实证结果表明相对于强大的桥接基线方法有持续改进。

Conclusion: R2U方法通过过程监督直接优化生成概率，有效解决了检索增强生成系统中的效用差距问题，并在多个基准测试中表现出优于现有方法的性能。

Abstract: Retrieval-Augmented Generation systems often suffer from a gap between
optimizing retrieval relevance and generative utility: retrieved documents may
be topically relevant but still lack the content needed for effective reasoning
during generation. While existing "bridge" modules attempt to rewrite the
retrieved text for better generation, we show how they fail to capture true
document utility. In this work, we propose R2U, with a key distinction of
directly optimizing to maximize the probability of generating a correct answer
through process supervision. As such direct observation is expensive, we also
propose approximating an efficient distillation pipeline by scaling the
supervision from LLMs, which helps the smaller rewriter model generalize
better. We evaluate our method across multiple open-domain question-answering
benchmarks. The empirical results demonstrate consistent improvements over
strong bridging baselines.

</details>


### [25] [Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization](https://arxiv.org/abs/2509.15579)
*Yun Tang,Cindy Tseng*

Main category: cs.CL

TL;DR: 该论文提出了一种基于分块的自监督学习算法（Chunk SSL），用于流式和离线语音预训练的统一解决方案，通过掩码预测损失和有限标量量化技术，在语音识别和语音翻译任务中取得了有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 随着语音技术的快速发展，低延迟的人机语音通信需求日益增长。现有的自监督学习算法大多基于完整语音假设，难以适应流式应用中的部分语音输入场景，因此需要开发能够同时支持流式和离线模式的统一预训练方法。

Method: 提出Chunk SSL算法，采用掩码预测损失训练声学编码器，利用同一分块和前置分块中的未掩码帧来恢复掩码语音帧的索引。使用复制和追加数据增强方法进行高效分块预训练，并采用有限标量量化（FSQ）模块离散化输入语音特征，使用分组掩码预测损失降低大码本带来的计算成本。

Result: 在Librispeech和Must-C数据集上的实验结果表明，该方法在语音识别和语音翻译任务中，无论是流式还是离线模式，都能取得非常有竞争力的性能。

Conclusion: Chunk SSL算法为流式和离线语音预训练提供了一个有效的统一解决方案，通过分块处理和有限标量量化技术，成功解决了部分语音输入场景下的自监督学习问题，在语音转文本任务中表现出色。

Abstract: Low latency speech human-machine communication is becoming increasingly
necessary as speech technology advances quickly in the last decade. One of the
primary factors behind the advancement of speech technology is self-supervised
learning. Most self-supervised learning algorithms are designed with full
utterance assumption and compromises have to made if partial utterances are
presented, which are common in the streaming applications. In this work, we
propose a chunk based self-supervised learning (Chunk SSL) algorithm as an
unified solution for both streaming and offline speech pre-training. Chunk SSL
is optimized with the masked prediction loss and an acoustic encoder is
encouraged to restore indices of those masked speech frames with help from
unmasked frames in the same chunk and preceding chunks. A copy and append data
augmentation approach is proposed to conduct efficient chunk based
pre-training. Chunk SSL utilizes a finite scalar quantization (FSQ) module to
discretize input speech features and our study shows a high resolution FSQ
codebook, i.e., a codebook with vocabulary size up to a few millions, is
beneficial to transfer knowledge from the pre-training task to the downstream
tasks. A group masked prediction loss is employed during pre-training to
alleviate the high memory and computation cost introduced by the large
codebook. The proposed approach is examined in two speech to text tasks, i.e.,
speech recognition and speech translation. Experimental results on the
\textsc{Librispeech} and \textsc{Must-C} datasets show that the proposed method
could achieve very competitive results for speech to text tasks at both
streaming and offline modes.

</details>


### [26] [DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models](https://arxiv.org/abs/2509.15587)
*Tsz Ting Chung,Lemao Liu,Mo Yu,Dit-Yan Yeung*

Main category: cs.CL

TL;DR: 提出了一个新的经典逻辑基准DivLogicEval，通过反直觉方式组合多样语句来评估LLMs的逻辑推理能力，并引入新评估指标减少LLMs偏见和随机性的影响。


<details>
  <summary>Details</summary>
Motivation: 现有逻辑推理基准存在多种推理技能混杂、语言多样性不足、分布偏离理想基准等问题，导致对LLMs逻辑推理能力的评估不够准确和公正。

Method: 构建DivLogicEval基准，包含以反直觉方式组合的多样化自然语言语句；设计新评估指标来减轻LLMs偏见和随机性的影响。

Result: 实验验证了DivLogicEval中问题确实需要逻辑推理能力，并比较了不同流行LLMs在逻辑推理方面的表现。

Conclusion: DivLogicEval提供了更可靠和公正的逻辑推理评估基准，有助于更准确地衡量LLMs的逻辑推理能力。

Abstract: Logic reasoning in natural language has been recognized as an important
measure of human intelligence for Large Language Models (LLMs). Popular
benchmarks may entangle multiple reasoning skills and thus provide unfaithful
evaluations on the logic reasoning skill. Meanwhile, existing logic reasoning
benchmarks are limited in language diversity and their distributions are
deviated from the distribution of an ideal logic reasoning benchmark, which may
lead to biased evaluation results. This paper thereby proposes a new classical
logic benchmark DivLogicEval, consisting of natural sentences composed of
diverse statements in a counterintuitive way. To ensure a more reliable
evaluation, we also introduce a new evaluation metric that mitigates the
influence of bias and randomness inherent in LLMs. Through experiments, we
demonstrate the extent to which logical reasoning is required to answer the
questions in DivLogicEval and compare the performance of different popular LLMs
in conducting logical reasoning.

</details>


### [27] [SciEvent: Benchmarking Multi-domain Scientific Event Extraction](https://arxiv.org/abs/2509.15620)
*Bofu Dong,Pritesh Shah,Sumedh Sonawane,Tiyasha Banerjee,Erin Brady,Xinya Du,Ming Jiang*

Main category: cs.CL

TL;DR: 本文介绍了SciEvent，一个新颖的多领域科学摘要基准，通过统一的事件提取模式进行标注，旨在实现科学内容的结构化和上下文感知理解。


<details>
  <summary>Details</summary>
Motivation: 现有的科学信息提取方法主要依赖窄领域的实体关系提取，限制了其在跨学科研究中的应用，并且难以捕捉科学信息的必要上下文，常常导致碎片化或矛盾的陈述。

Method: 将科学信息提取定义为一个多阶段事件提取流程：(1) 将摘要分割为核心科学活动——背景、方法、结果和结论；(2) 提取相应的触发器和细粒度参数。

Result: 通过对微调的事件提取模型、大型语言模型和人工标注者的实验，发现当前模型在社会学和人文科学等领域表现不佳，存在性能差距。

Conclusion: SciEvent作为一个具有挑战性的基准，是朝着可泛化、多领域科学信息提取迈出的一步。

Abstract: Scientific information extraction (SciIE) has primarily relied on
entity-relation extraction in narrow domains, limiting its applicability to
interdisciplinary research and struggling to capture the necessary context of
scientific information, often resulting in fragmented or conflicting
statements. In this paper, we introduce SciEvent, a novel multi-domain
benchmark of scientific abstracts annotated via a unified event extraction (EE)
schema designed to enable structured and context-aware understanding of
scientific content. It includes 500 abstracts across five research domains,
with manual annotations of event segments, triggers, and fine-grained
arguments. We define SciIE as a multi-stage EE pipeline: (1) segmenting
abstracts into core scientific activities--Background, Method, Result, and
Conclusion; and (2) extracting the corresponding triggers and arguments.
Experiments with fine-tuned EE models, large language models (LLMs), and human
annotators reveal a performance gap, with current models struggling in domains
such as sociology and humanities. SciEvent serves as a challenging benchmark
and a step toward generalizable, multi-domain SciIE.

</details>


### [28] [Concept Unlearning in Large Language Models via Self-Constructed Knowledge Triplets](https://arxiv.org/abs/2509.15621)
*Tomoya Yamashita,Yuuki Yamanaka,Masanori Yamada,Takayuki Miura,Toshiki Shibahara,Tomoharu Iwata*

Main category: cs.CL

TL;DR: 该论文提出了概念遗忘(CU)作为LLM遗忘的新需求，利用知识图谱表示LLM内部知识，通过生成知识三元组和解释性句子来实现更精确和全面的概念级遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法只能移除特定目标句子，不支持移除更广泛的概念（如人物或事件），需要解决这一局限性。

Method: 利用知识图谱表示LLM内部知识，定义概念遗忘为移除遗忘目标节点及相关边。通过提示LLM生成知识三元组和解释性句子，并对这些表示应用遗忘过程。

Result: 在真实世界和合成数据集上的实验表明，该方法能有效实现概念级遗忘，同时保留不相关知识。

Conclusion: 提出的概念遗忘方法通过图基表示实现了更直观的遗忘，能够更精确和全面地移除概念，为LLM隐私和版权问题提供了新的解决方案。

Abstract: Machine Unlearning (MU) has recently attracted considerable attention as a
solution to privacy and copyright issues in large language models (LLMs).
Existing MU methods aim to remove specific target sentences from an LLM while
minimizing damage to unrelated knowledge. However, these approaches require
explicit target sentences and do not support removing broader concepts, such as
persons or events. To address this limitation, we introduce Concept Unlearning
(CU) as a new requirement for LLM unlearning. We leverage knowledge graphs to
represent the LLM's internal knowledge and define CU as removing the forgetting
target nodes and associated edges. This graph-based formulation enables a more
intuitive unlearning and facilitates the design of more effective methods. We
propose a novel method that prompts the LLM to generate knowledge triplets and
explanatory sentences about the forgetting target and applies the unlearning
process to these representations. Our approach enables more precise and
comprehensive concept removal by aligning the unlearning process with the LLM's
internal knowledge representations. Experiments on real-world and synthetic
datasets demonstrate that our method effectively achieves concept-level
unlearning while preserving unrelated knowledge.

</details>


### [29] [Sparse-Autoencoder-Guided Internal Representation Unlearning for Large Language Models](https://arxiv.org/abs/2509.15631)
*Tomoya Yamashita,Akira Ito,Yuuki Yamanaka,Masanori Yamada,Takayuki Miura,Toshiki Shibahara*

Main category: cs.CL

TL;DR: 提出一种新的LLM遗忘方法，通过直接干预模型内部激活来实现真正的遗忘，而非仅仅抑制输出。该方法将目标实体的激活状态从"已知"转向"未知"，避免模型崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于抑制的遗忘方法只能控制模型输出，但无法消除模型内部的知识表示，且容易导致模型崩溃。需要一种能真正实现遗忘的方法。

Method: 在稀疏自编码器潜在空间中，通过修改目标实体的激活状态，使其远离已知实体而接近未知实体，实现从"已知"到"未知"的状态转变。

Result: 实验表明该方法能有效对齐被遗忘目标的内部激活，在问答任务中显著降低目标知识的回忆率，同时保持非目标知识的完整性。

Conclusion: 该方法通过直接干预内部激活实现了真正的遗忘，解决了传统抑制方法的局限性，为LLM隐私保护提供了更有效的技术方案。

Abstract: As large language models (LLMs) are increasingly deployed across various
applications, privacy and copyright concerns have heightened the need for more
effective LLM unlearning techniques. Many existing unlearning methods aim to
suppress undesirable outputs through additional training (e.g., gradient
ascent), which reduces the probability of generating such outputs. While such
suppression-based approaches can control model outputs, they may not eliminate
the underlying knowledge embedded in the model's internal activations; muting a
response is not the same as forgetting it. Moreover, such suppression-based
methods often suffer from model collapse. To address these issues, we propose a
novel unlearning method that directly intervenes in the model's internal
activations. In our formulation, forgetting is defined as a state in which the
activation of a forgotten target is indistinguishable from that of ``unknown''
entities. Our method introduces an unlearning objective that modifies the
activation of the target entity away from those of known entities and toward
those of unknown entities in a sparse autoencoder latent space. By aligning the
target's internal activation with those of unknown entities, we shift the
model's recognition of the target entity from ``known'' to ``unknown'',
achieving genuine forgetting while avoiding over-suppression and model
collapse. Empirically, we show that our method effectively aligns the internal
activations of the forgotten target, a result that the suppression-based
approaches do not reliably achieve. Additionally, our method effectively
reduces the model's recall of target knowledge in question-answering tasks
without significant damage to the non-target knowledge.

</details>


### [30] [Multilingual LLM Prompting Strategies for Medical English-Vietnamese Machine Translation](https://arxiv.org/abs/2509.15640)
*Nhu Vo,Nu-Uyen-Phuong Le,Dung D. Le,Massimo Piccardi,Wray Buntine*

Main category: cs.CL

TL;DR: 本文系统评估了六种多语言大语言模型在医学英语-越南语机器翻译任务上的提示策略，发现模型规模是性能的主要驱动因素，而术语感知提示和基于嵌入的示例检索能持续改善领域特定翻译。


<details>
  <summary>Details</summary>
Motivation: 越南语作为低资源语言在医学机器翻译领域研究不足，而医学英语-越南语翻译对越南的医疗保健访问和沟通至关重要。

Method: 使用MedEV数据集，比较零样本、少样本和基于Meddict医学词典增强的提示策略，评估六种多语言LLM（0.5B-9B参数）。

Result: 较大规模的LLM在零样本设置下表现强劲，少样本提示仅带来边际改进；术语感知提示和嵌入检索方法能持续提升领域特定翻译质量。

Conclusion: 多语言LLM在医学英语-越南语翻译方面具有潜力但存在局限性，模型规模和领域特定知识是关键因素。

Abstract: Medical English-Vietnamese machine translation (En-Vi MT) is essential for
healthcare access and communication in Vietnam, yet Vietnamese remains a
low-resource and under-studied language. We systematically evaluate prompting
strategies for six multilingual LLMs (0.5B-9B parameters) on the MedEV dataset,
comparing zero-shot, few-shot, and dictionary-augmented prompting with Meddict,
an English-Vietnamese medical lexicon. Results show that model scale is the
primary driver of performance: larger LLMs achieve strong zero-shot results,
while few-shot prompting yields only marginal improvements. In contrast,
terminology-aware cues and embedding-based example retrieval consistently
improve domain-specific translation. These findings underscore both the promise
and the current limitations of multilingual LLMs for medical En-Vi MT.

</details>


### [31] [Layer-wise Minimal Pair Probing Reveals Contextual Grammatical-Conceptual Hierarchy in Speech Representations](https://arxiv.org/abs/2509.15655)
*Linyang He,Qiaolin Wang,Xilin Jiang,Nima Mesgarani*

Main category: cs.CL

TL;DR: 该研究首次系统评估了不同语音语言模型（SLM）对上下文句法和语义特征的编码能力，发现所有语音模型对语法特征的编码都比概念特征更稳健。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注语音模型对浅层声学和语音特征的编码能力，但对于其是否编码了细微的句法和概念特征尚不清楚。

Method: 通过最小对设计和诊断特征分析，在71个任务上对自监督学习（S3M）、自动语音识别（ASR）、语音压缩（codec）和听觉大语言模型（AudioLLMs）编码器进行分层和时间分辨分析。

Result: 所有语音模型对语法特征的编码都比概念特征更稳健。

Conclusion: 该研究为语音语言模型的句法和语义能力评估提供了系统框架，揭示了不同模型在特征编码方面的差异。

Abstract: Transformer-based speech language models (SLMs) have significantly improved
neural speech recognition and understanding. While existing research has
examined how well SLMs encode shallow acoustic and phonetic features, the
extent to which SLMs encode nuanced syntactic and conceptual features remains
unclear. By drawing parallels with linguistic competence assessments for large
language models, this study is the first to systematically evaluate the
presence of contextual syntactic and semantic features across SLMs for
self-supervised learning (S3M), automatic speech recognition (ASR), speech
compression (codec), and as the encoder for auditory large language models
(AudioLLMs). Through minimal pair designs and diagnostic feature analysis
across 71 tasks spanning diverse linguistic levels, our layer-wise and
time-resolved analysis uncovers that 1) all speech encode grammatical features
more robustly than conceptual ones.

</details>


### [32] [VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion](https://arxiv.org/abs/2509.15667)
*Dimitrios Damianos,Leon Voukoutis,Georgios Paraskevopoulos,Vassilis Katsouros*

Main category: cs.CL

TL;DR: 提出了一种多模态融合框架，通过中间音频条件文本空间将预训练的语言模型与Whisper等声学编码器-解码器架构连接，构建支持语音的LLM，在希腊语语音识别上实现了约20%的相对改进。


<details>
  <summary>Details</summary>
Motivation: 构建支持语音的大语言模型，探索比直接使用音频嵌入更有效的跨模态对齐机制，特别是在多语言和低资源语音场景下。

Method: 使用中间音频条件文本空间进行对齐，通过跨模态注意力将Whisper的隐藏解码器状态与LLM状态融合，支持离线和流式模式。

Result: 开发了首个希腊语语音LLM VoxKrikri，在希腊语自动语音识别任务上达到最先进水平，平均相对改进约20%。

Conclusion: 连续空间融合是多语言和低资源语音LLM的有前景路径，有效实现了跨模态表示对齐。

Abstract: We present a multimodal fusion framework that bridges pre-trained
decoder-based large language models (LLM) and acoustic encoder-decoder
architectures such as Whisper, with the aim of building speech-enabled LLMs.
Instead of directly using audio embeddings, we explore an intermediate
audio-conditioned text space as a more effective mechanism for alignment. Our
method operates fully in continuous text representation spaces, fusing
Whisper's hidden decoder states with those of an LLM through cross-modal
attention, and supports both offline and streaming modes. We introduce
\textit{VoxKrikri}, the first Greek speech LLM, and show through analysis that
our approach effectively aligns representations across modalities. These
results highlight continuous space fusion as a promising path for multilingual
and low-resource speech LLMs, while achieving state-of-the-art results for
Automatic Speech Recognition in Greek, providing an average $\sim20\%$ relative
improvement across benchmarks.

</details>


### [33] [Fine-Tuning Large Multimodal Models for Automatic Pronunciation Assessment](https://arxiv.org/abs/2509.15701)
*Ke Wang,Wenning Wei,Yan Deng,Lei He,Sheng Zhao*

Main category: cs.CL

TL;DR: 该研究探索了使用大型多模态模型进行自动发音评估，通过微调在Speechocean762数据集和私有语料库上取得了显著优于零样本设置的性能，在单词和句子级别表现良好，但音素级别评估仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 自动发音评估在计算机辅助语言学习中至关重要，需要从多个粒度和方面进行评估。大型多模态模型为APA提供了新机会，但其在细粒度评估中的有效性尚不确定。

Method: 使用Speechocean762数据集和私有语料库对大型多模态模型进行微调，比较零样本设置下的性能，并与公共和商业系统进行对比。

Result: 微调显著优于零样本设置，在单粒度任务上达到竞争性结果。模型在单词和句子级别表现良好，但音素级别评估仍有挑战。皮尔逊相关系数达到0.9，而斯皮尔曼等级相关系数约为0.6，表明SCC更好地反映了顺序一致性。

Conclusion: 研究结果突出了LMMs在APA中的潜力和局限性，为未来的细粒度建模和秩感知评估指明了方向。

Abstract: Automatic Pronunciation Assessment (APA) is critical for Computer-Assisted
Language Learning (CALL), requiring evaluation across multiple granularities
and aspects. Large Multimodal Models (LMMs) present new opportunities for APA,
but their effectiveness in fine-grained assessment remains uncertain. This work
investigates fine-tuning LMMs for APA using the Speechocean762 dataset and a
private corpus. Fine-tuning significantly outperforms zero-shot settings and
achieves competitive results on single-granularity tasks compared to public and
commercial systems. The model performs well at word and sentence levels, while
phoneme-level assessment remains challenging. We also observe that the Pearson
Correlation Coefficient (PCC) reaches 0.9, whereas Spearman's rank Correlation
Coefficient (SCC) remains around 0.6, suggesting that SCC better reflects
ordinal consistency. These findings highlight both the promise and limitations
of LMMs for APA and point to future work on fine-grained modeling and
rank-aware evaluation.

</details>


### [34] [Once Upon a Time: Interactive Learning for Storytelling with Small Language Models](https://arxiv.org/abs/2509.15714)
*Jonas Mayer Martins,Ali Hamza Bashir,Muhammad Rehan Khalid,Lisa Beinborn*

Main category: cs.CL

TL;DR: 研究表明，通过引入认知启发的高层次反馈机制，语言模型可以在仅使用100万词输入的情况下，达到与4.1亿词传统预测训练相当的故事生成能力提升。


<details>
  <summary>Details</summary>
Motivation: 儿童通过社会互动高效学习语言，而大语言模型通常仅通过大量文本的下一词预测进行训练。这种对比激发了研究是否可以通过引入认知启发的高层次反馈来提高语言模型的数据效率。

Method: 训练学生模型生成故事，由教师模型对可读性、叙事连贯性和创造性进行评分。通过改变反馈循环前的预训练量，评估交互式学习对语言能力的影响。

Result: 高层次反馈具有极高的数据效率：在交互式学习中仅使用100万词输入，故事讲述能力的提升相当于使用4.1亿词进行下一词预测训练的效果。

Conclusion: 认知启发的高层次反馈机制可以显著提高语言模型的数据效率，为更接近人类语言学习方式的人工智能训练提供了有前景的方向。

Abstract: Children efficiently acquire language not just by listening, but by
interacting with others in their social environment. Conversely, large language
models are typically trained with next-word prediction on massive amounts of
text. Motivated by this contrast, we investigate whether language models can be
trained with less data by learning not only from next-word prediction but also
from high-level, cognitively inspired feedback. We train a student model to
generate stories, which a teacher model rates on readability, narrative
coherence, and creativity. By varying the amount of pretraining before the
feedback loop, we assess the impact of this interactive learning on formal and
functional linguistic competence. We find that the high-level feedback is
highly data efficient: With just 1 M words of input in interactive learning,
storytelling skills can improve as much as with 410 M words of next-word
prediction.

</details>


### [35] [REFER: Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting](https://arxiv.org/abs/2509.15723)
*Nannan Huang,Haytham M. Fayek,Xiuzhen Zhang*

Main category: cs.CL

TL;DR: 本研究提出了一种名为REFER的频率框架提示方法，通过借鉴认知科学中频率表示减少人类统计推理偏见的研究，来提升LLM在观点摘要中的公平性，避免了传统方法需要调整超参数或提供真实分布信息的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有观点摘要公平性研究依赖超参数调整或提供真实分布信息，但这些方法在实际应用中存在限制：终端用户很少修改默认参数，且准确的分布信息通常难以获得。认知科学研究表明频率表示能减少人类统计推理中的系统性偏见。

Method: 提出了REFER（频率框架提示）方法，通过系统实验比较不同提示框架，将改善人类推理的技术应用于语言模型，使用频率表示而非抽象概率表示来引导更有效的信息处理。

Result: 实验结果表明REFER能显著提升语言模型在观点摘要中的公平性，这种效果在更大的语言模型和使用更强推理指令时尤为明显。

Conclusion: 频率框架提示是提升LLM观点摘要公平性的有效方法，特别是在大规模模型和强化推理指令下效果更佳，为实际应用提供了可行的解决方案。

Abstract: Individuals express diverse opinions, a fair summary should represent these
viewpoints comprehensively. Previous research on fairness in opinion
summarisation using large language models (LLMs) relied on hyperparameter
tuning or providing ground truth distributional information in prompts.
However, these methods face practical limitations: end-users rarely modify
default model parameters, and accurate distributional information is often
unavailable. Building upon cognitive science research demonstrating that
frequency-based representations reduce systematic biases in human statistical
reasoning by making reference classes explicit and reducing cognitive load,
this study investigates whether frequency framed prompting (REFER) can
similarly enhance fairness in LLM opinion summarisation. Through systematic
experimentation with different prompting frameworks, we adapted techniques
known to improve human reasoning to elicit more effective information
processing in language models compared to abstract probabilistic
representations.Our results demonstrate that REFER enhances fairness in
language models when summarising opinions. This effect is particularly
pronounced in larger language models and using stronger reasoning instructions.

</details>


### [36] [Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics](https://arxiv.org/abs/2509.15739)
*Reza Sanayei,Srdjan Vesic,Eduardo Blanco,Mihai Surdeanu*

Main category: cs.CL

TL;DR: 该论文评估大型语言模型在非线性的辩论结构推理能力，使用计算论证理论的QuAD语义来评估模型在没有访问底层图结构的情况下对辩论中论点的排序能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在线性推理任务上表现出色，但在自然辩论等非线性结构上的能力尚未充分探索。辩论最适合用论证图表示，因此需要评估LLMs是否能近似计算论证理论的结构化推理。

Method: 使用两个NoDE数据集的对话格式辩论，在无法访问底层图结构的情况下，让模型对论点进行排序。测试了多种LLM和高级指令策略，包括思维链和上下文学习。

Result: 模型与QuAD排序有中等程度的一致性，但随着输入长度增加或话语流被打断，性能会下降。高级提示策略通过减少与论点长度和位置相关的偏见来缓解这些影响。

Conclusion: 研究结果既显示了LLMs在建模形式论证语义方面的潜力，也揭示了其局限性，为未来图感知推理研究提供了动机。

Abstract: Large Language Models (LLMs) excel at linear reasoning tasks but remain
underexplored on non-linear structures such as those found in natural debates,
which are best expressed as argument graphs. We evaluate whether LLMs can
approximate structured reasoning from Computational Argumentation Theory (CAT).
Specifically, we use Quantitative Argumentation Debate (QuAD) semantics, which
assigns acceptability scores to arguments based on their attack and support
relations. Given only dialogue-formatted debates from two NoDE datasets, models
are prompted to rank arguments without access to the underlying graph. We test
several LLMs under advanced instruction strategies, including Chain-of-Thought
and In-Context Learning. While models show moderate alignment with QuAD
rankings, performance degrades with longer inputs or disrupted discourse flow.
Advanced prompting helps mitigate these effects by reducing biases related to
argument length and position. Our findings highlight both the promise and
limitations of LLMs in modeling formal argumentation semantics and motivate
future work on graph-aware reasoning.

</details>


### [37] [UniGist: Towards General and Hardware-aligned Sequence-level Long Context Compression](https://arxiv.org/abs/2509.15763)
*Chenlong Deng,Zhisong Zhang,Kelong Mao,Shuaiyi Li,Tianqing Fang,Hongming Zhang,Haitao Mi,Dong Yu,Zhicheng Dou*

Main category: cs.CL

TL;DR: UniGist是一个序列级长上下文压缩框架，通过用特殊压缩标记替换原始标记来高效保留上下文信息，解决KV缓存内存瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型处理长上下文输入时，键值缓存的内存开销成为主要瓶颈。现有的序列级压缩方法容易丢失重要上下文信息。

Method: 采用无分块训练策略，设计高效内核和gist移位技巧，支持灵活推理时实际移除压缩标记。

Result: 在多个长上下文任务中显著提升压缩质量，在细节回忆任务和长距离依赖建模方面表现尤其突出。

Conclusion: UniGist框架有效解决了长上下文压缩中的信息保留问题，实现了实时内存节省。

Abstract: Large language models are increasingly capable of handling long-context
inputs, but the memory overhead of key-value (KV) cache remains a major
bottleneck for general-purpose deployment. While various compression strategies
have been explored, sequence-level compression, which drops the full KV caches
for certain tokens, is particularly challenging as it can lead to the loss of
important contextual information. To address this, we introduce UniGist, a
sequence-level long-context compression framework that efficiently preserves
context information by replacing raw tokens with special compression tokens
(gists) in a fine-grained manner. We adopt a chunk-free training strategy and
design an efficient kernel with a gist shift trick, enabling optimized GPU
training. Our scheme also supports flexible inference by allowing the actual
removal of compressed tokens, resulting in real-time memory savings.
Experiments across multiple long-context tasks demonstrate that UniGist
significantly improves compression quality, with especially strong performance
in detail-recalling tasks and long-range dependency modeling.

</details>


### [38] [UPRPRC: Unified Pipeline for Reproducing Parallel Resources -- Corpus from the United Nations](https://arxiv.org/abs/2509.15789)
*Qiuyang Lu,Fangjian Shen,Zhengkai Tang,Qiang Liu,Hexuan Cheng,Hui Liu,Wushao Wen*

Main category: cs.CL

TL;DR: 本文介绍了一个完整的端到端解决方案，用于构建大规模多语言平行语料库，解决了现有联合国文档语料库的透明度、可复现性和规模限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于联合国文档的多语言数据集存在过程不透明、难以复现和规模有限等问题，影响了机器翻译的发展。

Method: 提出了从网络爬虫获取数据到文本对齐的完整流程，核心是新的图辅助段落对齐（GAPA）算法，支持单机运行和分布式计算。

Result: 构建的语料库包含超过7.13亿个英文词元，规模是先前工作的两倍以上，是目前最大的完全由人工翻译、非AI生成内容的公开平行语料库。

Conclusion: 该工作提供了一个完全可复现的大规模平行语料库构建方案，代码和语料库在MIT许可下公开可用，有助于推动机器翻译研究。

Abstract: The quality and accessibility of multilingual datasets are crucial for
advancing machine translation. However, previous corpora built from United
Nations documents have suffered from issues such as opaque process, difficulty
of reproduction, and limited scale. To address these challenges, we introduce a
complete end-to-end solution, from data acquisition via web scraping to text
alignment. The entire process is fully reproducible, with a minimalist
single-machine example and optional distributed computing steps for
scalability. At its core, we propose a new Graph-Aided Paragraph Alignment
(GAPA) algorithm for efficient and flexible paragraph-level alignment. The
resulting corpus contains over 713 million English tokens, more than doubling
the scale of prior work. To the best of our knowledge, this represents the
largest publicly available parallel corpus composed entirely of
human-translated, non-AI-generated content. Our code and corpus are accessible
under the MIT License.

</details>


### [39] [RAVE: Retrieval and Scoring Aware Verifiable Claim Detection](https://arxiv.org/abs/2509.15793)
*Yufeng Li,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: RAVE框架结合证据检索与相关性和来源可信度的结构化信号，用于社交媒体上的可验证声明检测，在准确率和F1分数上优于仅基于文本和检索的基线方法。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上错误信息的快速传播需要可扩展的事实核查工具，而现有方法在处理模糊政治言论和多样化格式（如推文）时存在困难。

Method: 提出RAVE（检索和评分感知的可验证声明检测）框架，结合证据检索与结构化信号（相关性和来源可信度）来识别可客观验证的声明。

Result: 在CT22-test和PoliClaim-test数据集上的实验表明，RAVE在准确率和F1分数上一致优于仅基于文本和检索的基线方法。

Conclusion: RAVE框架通过结合证据检索和结构化信号，有效提升了社交媒体上可验证声明检测的性能，为事实核查提供了更可靠的工具。

Abstract: The rapid spread of misinformation on social media underscores the need for
scalable fact-checking tools. A key step is claim detection, which identifies
statements that can be objectively verified. Prior approaches often rely on
linguistic cues or claim check-worthiness, but these struggle with vague
political discourse and diverse formats such as tweets. We present RAVE
(Retrieval and Scoring Aware Verifiable Claim Detection), a framework that
combines evidence retrieval with structured signals of relevance and source
credibility. Experiments on CT22-test and PoliClaim-test show that RAVE
consistently outperforms text-only and retrieval-based baselines in both
accuracy and F1.

</details>


### [40] [Best-of-L: Cross-Lingual Reward Modeling for Mathematical Reasoning](https://arxiv.org/abs/2509.15811)
*Sara Rajaee,Rochelle Choenni,Ekaterina Shutova,Christof Monz*

Main category: cs.CL

TL;DR: 该研究发现跨语言奖励模型能显著提升多语言大模型的数学推理能力，即使对高资源语言也有帮助。跨语言采样在低采样预算下对英语特别有益，揭示了利用不同语言互补优势来改进多语言推理的新机会。


<details>
  <summary>Details</summary>
Motivation: 研究多语言大语言模型中推理能力在不同语言间的差异，以及不同语言是否产生互补的推理路径。

Method: 训练一个奖励模型来对给定问题在不同语言中生成的回答进行排序，比较跨语言奖励建模与单语言奖励建模的效果。

Result: 跨语言奖励模型相比单语言奖励建模显著提高了数学推理性能，即使对高资源语言也有帮助。跨语言采样在低采样预算下对英语特别有益。

Conclusion: 不同语言具有互补的推理优势，利用跨语言方法可以改善多语言推理能力，为多语言模型优化提供了新方向。

Abstract: While the reasoning abilities of large language models (LLMs) continue to
advance, it remains unclear how such ability varies across languages in
multilingual LLMs and whether different languages produce reasoning paths that
complement each other. To investigate this question, we train a reward model to
rank generated responses for a given question across languages. Our results
show that our cross-lingual reward model substantially improves mathematical
reasoning performance compared to using reward modeling within a single
language, benefiting even high-resource languages. While English often exhibits
the highest performance in multilingual models, we find that cross-lingual
sampling particularly benefits English under low sampling budgets. Our findings
reveal new opportunities to improve multilingual reasoning by leveraging the
complementary strengths of diverse languages.

</details>


### [41] [The Curious Case of Visual Grounding: Different Effects for Speech- and Text-based Language Encoders](https://arxiv.org/abs/2509.15837)
*Adrian Sauter,Willem Zuidema,Marianne de Heer Kloots*

Main category: cs.CL

TL;DR: 本文研究了视觉信息训练对音频和文本深度学习模型语言处理的影响，发现视觉基础能增强语音和书面语言表征的对齐，但主要改善的是词汇身份编码而非语义编码。


<details>
  <summary>Details</summary>
Motivation: 探索视觉基础如何影响深度学习模型中语言处理的内部表征，特别是在语音和文本编码器中的不同效果。

Method: 使用全局表征比较和针对性聚类分析，探测模型表征中的语音和语义可区分性。

Result: 视觉基础增加了语音和书面语言表征的对齐，但主要改善词汇身份编码；语音表征保持语音主导，视觉基础未能改善语义可区分性。

Conclusion: 研究结果可为开发更有效的方法来用视觉信息丰富语音模型的语义提供有用信息。

Abstract: How does visual information included in training affect language processing
in audio- and text-based deep learning models? We explore how such visual
grounding affects model-internal representations of words, and find
substantially different effects in speech- vs. text-based language encoders.
Firstly, global representational comparisons reveal that visual grounding
increases alignment between representations of spoken and written language, but
this effect seems mainly driven by enhanced encoding of word identity rather
than meaning. We then apply targeted clustering analyses to probe for phonetic
vs. semantic discriminability in model representations. Speech-based
representations remain phonetically dominated with visual grounding, but in
contrast to text-based representations, visual grounding does not improve
semantic discriminability. Our findings could usefully inform the development
of more efficient methods to enrich speech-based models with visually-informed
semantics.

</details>


### [42] [Multi-Physics: A Comprehensive Benchmark for Multimodal LLMs Reasoning on Chinese Multi-Subject Physics Problems](https://arxiv.org/abs/2509.15839)
*Zhongze Luo,Zhenshuai Yin,Yongxin Guo,Zhichao Wang,Jionghao Zhu,Xiaoying Tang*

Main category: cs.CL

TL;DR: 提出了Multi-Physics中文物理推理基准，包含5个难度级别、1,412道图像相关选择题，覆盖11个高中物理科目，用于评估多模态大语言模型在科学领域的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准在专业科学领域存在不足：缺乏细粒度学科覆盖、忽视逐步推理过程、以英语为中心、未能系统评估视觉信息的作用。

Method: 采用双评估框架评估20个不同MLLMs，分析最终答案准确性和逐步推理链的完整性，通过改变输入模式系统研究难度级别和视觉信息的影响。

Result: 为社区提供了细粒度资源，并提供了剖析最先进MLLMs多模态推理过程的稳健方法。

Conclusion: Multi-Physics基准填补了科学领域评估空白，数据集和代码已开源，有助于推动多模态模型在专业领域的应用。

Abstract: While multimodal LLMs (MLLMs) demonstrate remarkable reasoning progress,
their application in specialized scientific domains like physics reveals
significant gaps in current evaluation benchmarks. Specifically, existing
benchmarks often lack fine-grained subject coverage, neglect the step-by-step
reasoning process, and are predominantly English-centric, failing to
systematically evaluate the role of visual information. Therefore, we introduce
\textbf {Multi-Physics} for Chinese physics reasoning, a comprehensive
benchmark that includes 5 difficulty levels, featuring 1,412 image-associated,
multiple-choice questions spanning 11 high-school physics subjects. We employ a
dual evaluation framework to evaluate 20 different MLLMs, analyzing both final
answer accuracy and the step-by-step integrity of their chain-of-thought.
Furthermore, we systematically study the impact of difficulty level and visual
information by comparing the model performance before and after changing the
input mode. Our work provides not only a fine-grained resource for the
community but also offers a robust methodology for dissecting the multimodal
reasoning process of state-of-the-art MLLMs, and our dataset and code have been
open-sourced: https://github.com/luozhongze/Multi-Physics.

</details>


### [43] [Distribution-Aligned Decoding for Efficient LLM Task Adaptation](https://arxiv.org/abs/2509.15888)
*Senkang Hu,Xudong Han,Jinqi Jiang,Yihang Tao,Zihan Fang,Sam Tak Wu Kwong,Yuguang Fang*

Main category: cs.CL

TL;DR: 提出Steering Vector Decoding (SVD)方法，通过输出分布对齐而非权重更新来实现任务适配，在解码过程中使用从KL散度梯度提取的导向向量来引导模型输出分布


<details>
  <summary>Details</summary>
Motivation: 适应亿参数语言模型到下游任务仍然成本高昂，即使是参数高效微调(PEFT)也存在开销，需要更轻量级的任务适配方法

Method: 1) 进行短时间预热微调；2) 从预热模型和预训练模型的KL散度梯度中提取任务感知导向向量；3) 在解码过程中使用导向向量引导输出分布向任务分布对齐

Result: 在三个任务九个基准测试中，SVD与四种标准PEFT方法结合，多项选择准确率提升高达5分，开放式真实性提升2分，常识数据集也有1-2分提升，且不增加可训练参数

Conclusion: SVD为大型语言模型提供了轻量级、理论基础的强任务适配路径

Abstract: Adapting billion-parameter language models to a downstream task is still
costly, even with parameter-efficient fine-tuning (PEFT). We re-cast task
adaptation as output-distribution alignment: the objective is to steer the
output distribution toward the task distribution directly during decoding
rather than indirectly through weight updates. Building on this view, we
introduce Steering Vector Decoding (SVD), a lightweight, PEFT-compatible, and
theoretically grounded method. We start with a short warm-start fine-tune and
extract a task-aware steering vector from the Kullback-Leibler (KL) divergence
gradient between the output distribution of the warm-started and pre-trained
models. This steering vector is then used to guide the decoding process to
steer the model's output distribution towards the task distribution. We
theoretically prove that SVD is first-order equivalent to the gradient step of
full fine-tuning and derive a globally optimal solution for the strength of the
steering vector. Across three tasks and nine benchmarks, SVD paired with four
standard PEFT methods improves multiple-choice accuracy by up to 5 points and
open-ended truthfulness by 2 points, with similar gains (1-2 points) on
commonsense datasets without adding trainable parameters beyond the PEFT
adapter. SVD thus offers a lightweight, theoretically grounded path to stronger
task adaptation for large language models.

</details>


### [44] [The Psychology of Falsehood: A Human-Centric Survey of Misinformation Detection](https://arxiv.org/abs/2509.15896)
*Arghodeep Nandi,Megha Sundriyal,Euna Mehnaz Khan,Jikai Sun,Emily Vraga,Jaideep Srivastava,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文综述探讨了传统事实核查方法与心理学概念（如认知偏见、社会动态和情绪反应）之间的相互作用，提出需要超越事实准确性，采用更以人为本的检测框架来应对数字时代的错误信息问题。


<details>
  <summary>Details</summary>
Motivation: 当前自动化事实核查系统主要局限于评估事实准确性，但错误信息的有害影响超越了简单的虚假陈述，它利用了人们对信息的感知、解释和情绪反应方式，因此需要更人性化的检测方法。

Method: 通过分析最先进的错误信息检测系统，结合人类心理学和行为学的视角，揭示当前方法的局限性并识别改进机会。

Result: 研究发现传统事实核查方法存在关键局限性，提出了整合技术因素与人类认知和社会影响复杂性的神经行为模型等改进方向。

Conclusion: 基于心理学和行为学的方法为更有效地检测和减轻错误信息的社会危害提供了有前景的途径，未来研究应致力于创建更强大和自适应的检测框架。

Abstract: Misinformation remains one of the most significant issues in the digital age.
While automated fact-checking has emerged as a viable solution, most current
systems are limited to evaluating factual accuracy. However, the detrimental
effect of misinformation transcends simple falsehoods; it takes advantage of
how individuals perceive, interpret, and emotionally react to information. This
underscores the need to move beyond factuality and adopt more human-centered
detection frameworks. In this survey, we explore the evolving interplay between
traditional fact-checking approaches and psychological concepts such as
cognitive biases, social dynamics, and emotional responses. By analyzing
state-of-the-art misinformation detection systems through the lens of human
psychology and behavior, we reveal critical limitations of current methods and
identify opportunities for improvement. Additionally, we outline future
research directions aimed at creating more robust and adaptive frameworks, such
as neuro-behavioural models that integrate technological factors with the
complexities of human cognition and social influence. These approaches offer
promising pathways to more effectively detect and mitigate the societal harms
of misinformation.

</details>


### [45] [Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and Personalization via Questions](https://arxiv.org/abs/2509.15901)
*Frederic Kirstein,Sonu Kumar,Terry Ruas,Bela Gipp*

Main category: cs.CL

TL;DR: FRAME是一个模块化流水线，将摘要生成重新定义为语义丰富任务，通过提取和评分关键事实、按主题组织，并利用这些信息丰富大纲来生成抽象摘要。SCOPE协议通过让模型回答九个问题来构建推理轨迹，实现个性化摘要。P-MESA评估框架能可靠识别错误实例。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在会议摘要生成中容易产生幻觉、遗漏和不相关内容，需要改进控制性、忠实性和个性化能力。

Method: 提出FRAME模块化流水线，将摘要重构为语义丰富任务；引入SCOPE协议进行个性化摘要；开发P-MESA参考无关的多维评估框架。

Result: 在QMSum和FAME数据集上，FRAME将幻觉和遗漏减少了2/5分（使用MESA测量），SCOPE在知识匹配和目标对齐方面优于仅提示的基线方法。P-MESA达到≥89%的平衡准确率，与人类严重性评级强相关（r≥0.70）。

Conclusion: 研究主张重新思考摘要生成方法，以改进控制性、忠实性和个性化能力。

Abstract: Meeting summarization with large language models (LLMs) remains error-prone,
often producing outputs with hallucinations, omissions, and irrelevancies. We
present FRAME, a modular pipeline that reframes summarization as a semantic
enrichment task. FRAME extracts and scores salient facts, organizes them
thematically, and uses these to enrich an outline into an abstractive summary.
To personalize summaries, we introduce SCOPE, a reason-out-loud protocol that
has the model build a reasoning trace by answering nine questions before
content selection. For evaluation, we propose P-MESA, a multi-dimensional,
reference-free evaluation framework to assess if a summary fits a target
reader. P-MESA reliably identifies error instances, achieving >= 89% balanced
accuracy against human annotations and strongly aligns with human severity
ratings (r >= 0.70). On QMSum and FAME, FRAME reduces hallucination and
omission by 2 out of 5 points (measured with MESA), while SCOPE improves
knowledge fit and goal alignment over prompt-only baselines. Our findings
advocate for rethinking summarization to improve control, faithfulness, and
personalization.

</details>


### [46] [Beyond the Score: Uncertainty-Calibrated LLMs for Automated Essay Assessment](https://arxiv.org/abs/2509.15926)
*Ahmed Karim,Qiao Wang,Zheng Yuan*

Main category: cs.CL

TL;DR: 本文提出使用保形预测方法为自动作文评分系统提供置信度评估，通过在大规模语言模型上应用保形预测包装器，实现了具有正式覆盖保证的集合值输出。


<details>
  <summary>Details</summary>
Motivation: 当前自动作文评分系统在公开基准测试中已达到接近人类评分的一致性，但在高风险考试等实际应用中采用有限，主要障碍是大多数模型只输出单一分数而缺乏置信度或解释性度量。

Method: 使用保形预测作为分布无关的包装器，对两个开源大语言模型（Llama-3 8B和Qwen-2.5 3B）在三个不同语料库上进行微调，并在90%风险水平下进行校准，使用不确定性感知准确率评估可靠性。

Result: 校准后的模型能够持续达到覆盖目标，同时保持预测集合紧凑，表明中等规模的开源LLM已能支持教师参与的自动作文评分系统。

Conclusion: 这是首个将保形预测和不确定性感知准确率结合用于作文评分的研究，未来工作包括扩展研究和更广泛的用户研究。

Abstract: Automated Essay Scoring (AES) systems now reach near human agreement on some
public benchmarks, yet real-world adoption, especially in high-stakes
examinations, remains limited. A principal obstacle is that most models output
a single score without any accompanying measure of confidence or explanation.
We address this gap with conformal prediction, a distribution-free wrapper that
equips any classifier with set-valued outputs and formal coverage guarantees.
Two open-source large language models (Llama-3 8B and Qwen-2.5 3B) are
fine-tuned on three diverse corpora (ASAP, TOEFL11, Cambridge-FCE) and
calibrated at a 90 percent risk level. Reliability is assessed with UAcc, an
uncertainty-aware accuracy that rewards models for being both correct and
concise. To our knowledge, this is the first work to combine conformal
prediction and UAcc for essay scoring. The calibrated models consistently meet
the coverage target while keeping prediction sets compact, indicating that
open-source, mid-sized LLMs can already support teacher-in-the-loop AES; we
discuss scaling and broader user studies as future work.

</details>


### [47] [Localmax dynamics for attention in transformers and its asymptotic behavior](https://arxiv.org/abs/2509.15958)
*Henri Cimetière,Maria Teresa Chiri,Bahman Gharesifard*

Main category: cs.CL

TL;DR: 本文提出了一种新的离散时间注意力模型——localmax dynamics，它在softmax和hardmax动态之间进行插值，通过对齐敏感度参数控制邻居交互的松弛程度。


<details>
  <summary>Details</summary>
Motivation: 动机是开发一个介于softmax和hardmax之间的注意力机制，允许对邻居交互进行更精细的控制，同时保持hardmax的某些特性。

Method: 方法是通过引入对齐敏感度参数来松弛hardmax的严格邻居交互，定义quiescent集合来描述令牌在顶点附近的渐近行为，并分析不同参数设置下的收敛特性。

Result: 结果表明localmax动态不会出现有限时间收敛，其凸包收敛到凸多面体但结构不能用最大对齐集完全描述，quiescent集合在理解系统渐近行为中起关键作用。

Conclusion: 结论是localmax动态提供了hardmax的推广框架，揭示了在非对称设置下Lyapunov方法的局限性，为未来研究指明了方向。

Abstract: We introduce a new discrete-time attention model, termed the localmax
dynamics, which interpolates between the classic softmax dynamics and the
hardmax dynamics, where only the tokens that maximize the influence toward a
given token have a positive weight. As in hardmax, uniform weights are
determined by a parameter controlling neighbor influence, but the key extension
lies in relaxing neighborhood interactions through an alignment-sensitivity
parameter, which allows controlled deviations from pure hardmax behavior. As we
prove, while the convex hull of the token states still converges to a convex
polytope, its structure can no longer be fully described by a maximal alignment
set, prompting the introduction of quiescent sets to capture the invariant
behavior of tokens near vertices. We show that these sets play a key role in
understanding the asymptotic behavior of the system, even under time-varying
alignment sensitivity parameters. We further show that localmax dynamics does
not exhibit finite-time convergence and provide results for vanishing, nonzero,
time-varying alignment-sensitivity parameters, recovering the limiting behavior
of hardmax as a by-product. Finally, we adapt Lyapunov-based methods from
classical opinion dynamics, highlighting their limitations in the asymmetric
setting of localmax interactions and outlining directions for future research.

</details>


### [48] [BEFT: Bias-Efficient Fine-Tuning of Language Models](https://arxiv.org/abs/2509.15974)
*Baichuan Huang,Ananth Balashankar,Amir Aminifar*

Main category: cs.CL

TL;DR: 本文提出了一种基于偏置项选择的高效微调方法（BEFT），通过选择性地微调特定偏置项来提高参数效率，在多种任务和模型规模上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的偏置项微调方法缺乏明确指导原则来选择应该微调哪些偏置项（查询、键或值投影中的偏置项），限制了参数效率的进一步提升。

Method: 提出了一种偏置项选择方法，作为偏置高效微调（BEFT）的基础，通过系统性地选择最有效的偏置项进行微调。

Result: 在110M到6.7B参数的各种大语言模型上进行了广泛评估，在分类、多选和生成任务上均显示出方法的有效性和优越性。

Conclusion: 偏置高效微调方法在保持参数效率的同时，显著提升了在下游任务上的性能表现，为参数高效微调技术提供了新的有效途径。

Abstract: Fine-tuning all-bias-terms stands out among various parameter-efficient
fine-tuning (PEFT) techniques, owing to its out-of-the-box usability and
competitive performance, especially in low-data regimes. Bias-only fine-tuning
has the potential for unprecedented parameter efficiency. However, the link
between fine-tuning different bias terms (i.e., bias terms in the query, key,
or value projections) and downstream performance remains unclear. The existing
approaches, e.g., based on the magnitude of bias change or empirical Fisher
information, provide limited guidance for selecting the particular bias term
for effective fine-tuning. In this paper, we propose an approach for selecting
the bias term to be fine-tuned, forming the foundation of our bias-efficient
fine-tuning (BEFT). We extensively evaluate our bias-efficient approach against
other bias-selection approaches, across a wide range of large language models
(LLMs) spanning encoder-only and decoder-only architectures from 110M to 6.7B
parameters. Our results demonstrate the effectiveness and superiority of our
bias-efficient approach on diverse downstream tasks, including classification,
multiple-choice, and generation tasks.

</details>


### [49] [Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning](https://arxiv.org/abs/2509.16025)
*Hong-Yun Lin,Jhen-Ke Lin,Chung-Chun Wang,Hao-Chien Lu,Berlin Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的多模态基础模型方法，用于口语语言评估，通过会话级单次处理实现端到端的口语能力评估，避免了传统级联管道的错误传播问题。


<details>
  <summary>Details</summary>
Motivation: 随着L2英语学习者数量的增长，对可靠口语评估的需求日益迫切。现有方法要么依赖容易错误传播的级联管道，要么使用短音频窗口可能遗漏语篇级证据。

Method: 采用多目标学习与冻结的Whisper ASR模型相结合的方法，通过声学感知校准实现会话级评估，无需手工特征即可联合学习整体和特质级目标。

Result: 在Speak & Improve基准测试中，该方法优于先前最先进的级联系统，并展现出强大的跨部分泛化能力。

Conclusion: 该方法为计算机辅助语言学习应用提供了一个紧凑可部署的评分器，能够连贯处理L2说话者的整个回答会话，在预测整体口语能力方面表现出色。

Abstract: Spoken Language Assessment (SLA) estimates a learner's oral proficiency from
spontaneous speech. The growing population of L2 English speakers has
intensified the demand for reliable SLA, a critical component of Computer
Assisted Language Learning (CALL). Existing efforts often rely on cascaded
pipelines, which are prone to error propagation, or end-to-end models that
often operate on a short audio window, which might miss discourse-level
evidence. This paper introduces a novel multimodal foundation model approach
that performs session-level evaluation in a single pass. Our approach couples
multi-target learning with a frozen, Whisper ASR model-based speech prior for
acoustic-aware calibration, allowing for jointly learning holistic and
trait-level objectives of SLA without resorting to handcrafted features. By
coherently processing the entire response session of an L2 speaker, the model
excels at predicting holistic oral proficiency. Experiments conducted on the
Speak & Improve benchmark demonstrate that our proposed approach outperforms
the previous state-of-the-art cascaded system and exhibits robust cross-part
generalization, producing a compact deployable grader that is tailored for CALL
applications.

</details>


### [50] [Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech](https://arxiv.org/abs/2509.16028)
*Sang Hoon Woo,Sehun Lee,Kang-wook Kim,Gunhee Kim*

Main category: cs.CL

TL;DR: 提出了Think-Verbalize-Speak框架，将LLM的推理过程与语音输出解耦，通过中间verbalizing步骤将思维转换为适合语音的自然文本，以在保持推理能力的同时提升语音自然度。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接将LLMs应用于语音对话系统时，由于文本和语音表达方式的不匹配，导致结果不理想。虽然已有方法尝试让LLMs生成更适合语音的输出，但这些方法对推理性能的影响尚未充分研究。

Method: 提出TVS框架，将推理与语音输出分离。核心是verbalizing步骤，将LLM的思维转换为自然、适合语音的文本。同时引入ReVerT，一种基于增量异步摘要的低延迟verbalizer。

Result: 在多个基准测试上的实验表明，该方法显著提升了语音的自然度和简洁性，同时对推理性能影响极小。

Conclusion: Think-Verbalize-Speak框架有效解决了LLMs在语音对话系统中的适配问题，在保持强大推理能力的同时显著改善了语音输出质量。

Abstract: Spoken dialogue systems increasingly employ large language models (LLMs) to
leverage their advanced reasoning capabilities. However, direct application of
LLMs in spoken communication often yield suboptimal results due to mismatches
between optimal textual and verbal delivery. While existing approaches adapt
LLMs to produce speech-friendly outputs, their impact on reasoning performance
remains underexplored. In this work, we propose Think-Verbalize-Speak, a
framework that decouples reasoning from spoken delivery to preserve the full
reasoning capacity of LLMs. Central to our method is verbalizing, an
intermediate step that translates thoughts into natural, speech-ready text. We
also introduce ReVerT, a latency-efficient verbalizer based on incremental and
asynchronous summarization. Experiments across multiple benchmarks show that
our method enhances speech naturalness and conciseness with minimal impact on
reasoning. The project page with the dataset and the source code is available
at https://yhytoto12.github.io/TVS-ReVerT

</details>


### [51] [Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM Responses](https://arxiv.org/abs/2509.16093)
*Fangyi Yu,Nabeel Seedat,Dasha Herrmannova,Frank Schilder,Jonathan Richard Schwarz*

Main category: cs.CL

TL;DR: DeCE是一个分解式LLM评估框架，将答案质量分解为精确度（事实准确性和相关性）和召回率（所需概念的覆盖度），在专业领域实现了与专家判断更强的相关性。


<details>
  <summary>Details</summary>
Motivation: 在法学、医学等高风险领域，评估长篇答案存在挑战。标准指标如BLEU和ROUGE无法捕捉语义正确性，而现有的LLM评估器往往将答案质量的细微差别简化为单一分数。

Method: DeCE框架使用从黄金答案要求中自动提取的实例特定标准，将评估分解为精确度和召回率两个维度。该框架是模型无关和领域通用的，无需预定义分类法或手工制作的评分标准。

Result: 在真实世界法律QA任务中，DeCE与专家判断的相关性达到r=0.78，显著优于传统指标（r=0.12）、逐点LLM评分（r=0.35）和现代多维评估器（r=0.48）。

Conclusion: DeCE提供了一个可解释且可操作的LLM评估框架，揭示了通用模型偏向召回率而专业模型偏向精确度的可解释权衡，且具有很好的可扩展性。

Abstract: Evaluating long-form answers in high-stakes domains such as law or medicine
remains a fundamental challenge. Standard metrics like BLEU and ROUGE fail to
capture semantic correctness, and current LLM-based evaluators often reduce
nuanced aspects of answer quality into a single undifferentiated score. We
introduce DeCE, a decomposed LLM evaluation framework that separates precision
(factual accuracy and relevance) and recall (coverage of required concepts),
using instance-specific criteria automatically extracted from gold answer
requirements. DeCE is model-agnostic and domain-general, requiring no
predefined taxonomies or handcrafted rubrics. We instantiate DeCE to evaluate
different LLMs on a real-world legal QA task involving multi-jurisdictional
reasoning and citation grounding. DeCE achieves substantially stronger
correlation with expert judgments ($r=0.78$), compared to traditional metrics
($r=0.12$), pointwise LLM scoring ($r=0.35$), and modern multidimensional
evaluators ($r=0.48$). It also reveals interpretable trade-offs: generalist
models favor recall, while specialized models favor precision. Importantly,
only 11.95% of LLM-generated criteria required expert revision, underscoring
DeCE's scalability. DeCE offers an interpretable and actionable LLM evaluation
framework in expert domains.

</details>


### [52] [DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning](https://arxiv.org/abs/2509.16105)
*Sikai Bai,Haoxi Li,Jie Zhang,Zicong Hong,Song Guo*

Main category: cs.CL

TL;DR: 提出了一种名为DiEP的非均匀剪枝策略，用于解决MoE模型的内存和存储挑战，通过自适应调整不同层的剪枝率来保留约92%的原始性能。


<details>
  <summary>Details</summary>
Motivation: 现有的MoE剪枝方法采用统一的稀疏度会导致次优结果和性能下降，因为不同MoE层的专家冗余度不同。

Method: DiEP方法将全局离散搜索空间转换为连续空间，自适应调整层级的剪枝率，并联合学习层间重要性，实现基于梯度的自适应剪枝。

Result: 在五个先进的MoE模型上的实验表明，DiEP在Mixtral 8×7B模型上仅保留一半专家就能保持约92%的原始性能，在MMLU数据集上比其他剪枝方法高出7.1%。

Conclusion: DiEP方法有效解决了MoE模型剪枝中的非均匀冗余问题，实现了更好的性能保留效果。

Abstract: Despite the significant breakthrough of Mixture-of-Experts (MoE), the
increasing scale of these MoE models presents huge memory and storage
challenges. Existing MoE pruning methods, which involve reducing parameter size
with a uniform sparsity across all layers, often lead to suboptimal outcomes
and performance degradation due to varying expert redundancy in different MoE
layers. To address this, we propose a non-uniform pruning strategy, dubbed
\textbf{Di}fferentiable \textbf{E}xpert \textbf{P}runing (\textbf{DiEP}), which
adaptively adjusts pruning rates at the layer level while jointly learning
inter-layer importance, effectively capturing the varying redundancy across
different MoE layers. By transforming the global discrete search space into a
continuous one, our method handles exponentially growing non-uniform expert
combinations, enabling adaptive gradient-based pruning. Extensive experiments
on five advanced MoE models demonstrate the efficacy of our method across
various NLP tasks. Notably, \textbf{DiEP} retains around 92\% of original
performance on Mixtral 8$\times$7B with only half the experts, outperforming
other pruning methods by up to 7.1\% on the challenging MMLU dataset.

</details>


### [53] [It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge](https://arxiv.org/abs/2509.16107)
*Lukas Ellinger,Georg Groh*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在多轮对话中利用常识解决指代歧义的能力，发现当前LLMs在歧义处理上存在不足，倾向于单一解释或覆盖所有可能引用，简化提示会进一步削弱常识推理能力，而通过DPO微调可显著改善歧义解决效果。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能够利用常识知识在多轮对话中解决指代歧义问题，并分析在歧义持续存在时的模型行为，同时探讨简化语言请求对这种能力的影响。

Method: 使用新颖的多语言评估数据集，通过LLM-as-Judge和人工标注测试DeepSeek v3、GPT-4o、Qwen3-32B、GPT-4o-mini和Llama-3.1-8B等模型，并对Llama-3.1-8B进行直接偏好优化(DPO)微调。

Result: 当前LLMs难以有效解决歧义：倾向于承诺单一解释或覆盖所有可能引用，而不是采取模糊处理或寻求澄清。简化提示显著减少了常识推理和多样化响应策略的使用。DPO微调显著提高了所有请求类型下的歧义解决能力。

Conclusion: 研究结果强调了需要先进的微调技术来改进LLMs处理歧义的能力，并确保在不同沟通风格下的鲁棒性能。

Abstract: Ambiguous words or underspecified references require interlocutors to resolve
them, often by relying on shared context and commonsense knowledge. Therefore,
we systematically investigate whether Large Language Models (LLMs) can leverage
commonsense to resolve referential ambiguity in multi-turn conversations and
analyze their behavior when ambiguity persists. Further, we study how requests
for simplified language affect this capacity. Using a novel multilingual
evaluation dataset, we test DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, and
Llama-3.1-8B via LLM-as-Judge and human annotations. Our findings indicate that
current LLMs struggle to resolve ambiguity effectively: they tend to commit to
a single interpretation or cover all possible references, rather than hedging
or seeking clarification. This limitation becomes more pronounced under
simplification prompts, which drastically reduce the use of commonsense
reasoning and diverse response strategies. Fine-tuning Llama-3.1-8B with Direct
Preference Optimization substantially improves ambiguity resolution across all
request types. These results underscore the need for advanced fine-tuning to
improve LLMs' handling of ambiguity and to ensure robust performance across
diverse communication styles.

</details>


### [54] [CodeRAG: Finding Relevant and Necessary Knowledge for Retrieval-Augmented Repository-Level Code Completion](https://arxiv.org/abs/2509.16112)
*Sheng Zhang,Yifan Ding,Shuquan Lian,Shun Song,Hui Li*

Main category: cs.CL

TL;DR: CodeRAG是一个针对仓库级代码补全的检索增强框架，通过改进查询构建、多路径代码检索和偏好对齐的重排序来解决现有方法的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的仓库级代码补全方法存在查询构建不当、单路径代码检索以及代码检索器与代码LLM之间的不对齐等问题，影响了补全效果。

Method: CodeRAG包含三个核心组件：基于对数概率的查询构建、多路径代码检索和偏好对齐的BestFit重排序机制。

Result: 在ReccEval和CCEval基准测试上的广泛实验表明，CodeRAG显著且持续地优于最先进的方法。

Conclusion: CodeRAG通过系统性的改进解决了仓库级代码补全中的关键问题，为检索增强的代码补全提供了有效的解决方案。

Abstract: Repository-level code completion automatically predicts the unfinished code
based on the broader information from the repository. Recent strides in Code
Large Language Models (code LLMs) have spurred the development of
repository-level code completion methods, yielding promising results.
Nevertheless, they suffer from issues such as inappropriate query construction,
single-path code retrieval, and misalignment between code retriever and code
LLM. To address these problems, we introduce CodeRAG, a framework tailored to
identify relevant and necessary knowledge for retrieval-augmented
repository-level code completion. Its core components include log probability
guided query construction, multi-path code retrieval, and preference-aligned
BestFit reranking. Extensive experiments on benchmarks ReccEval and CCEval
demonstrate that CodeRAG significantly and consistently outperforms
state-of-the-art methods. The implementation of CodeRAG is available at
https://github.com/KDEGroup/CodeRAG.

</details>


### [55] [CultureScope: A Dimensional Lens for Probing Cultural Understanding in LLMs](https://arxiv.org/abs/2509.16188)
*Jinghao Zhang,Sihang Jiang,Shiwei Guo,Shisong Chen,Yanghua Xiao,Hongwei Feng,Jiaqing Liang,Minggui HE,Shimin Tao,Hongxia Ma*

Main category: cs.CL

TL;DR: CultureScope是一个基于文化冰山理论的全面评估框架，用于评估大语言模型在不同文化环境中的文化理解能力，包含3层140维度的分类体系，可自动构建文化特定知识库和评估数据集。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多样化文化环境中的部署增加，评估其文化理解能力变得至关重要。现有基准缺乏全面性，难以在不同文化背景下扩展和适应，主要依赖专家手动标注且缺乏成熟文化理论指导。

Method: 受文化冰山理论启发，设计了包含3层140维度的文化知识分类体系，指导自动构建针对任何语言和文化的特定知识库及相应评估数据集。

Result: 实验结果表明该方法能有效评估文化理解能力，发现现有大语言模型缺乏全面的文化能力，仅增加多语言数据并不一定能增强文化理解。

Conclusion: CultureScope是迄今为止最全面的文化理解评估框架，揭示了当前LLMs在文化能力方面的不足，为开发文化对齐的可靠应用提供了重要工具。

Abstract: As large language models (LLMs) are increasingly deployed in diverse cultural
environments, evaluating their cultural understanding capability has become
essential for ensuring trustworthy and culturally aligned applications.
However, most existing benchmarks lack comprehensiveness and are challenging to
scale and adapt across different cultural contexts, because their frameworks
often lack guidance from well-established cultural theories and tend to rely on
expert-driven manual annotations. To address these issues, we propose
CultureScope, the most comprehensive evaluation framework to date for assessing
cultural understanding in LLMs. Inspired by the cultural iceberg theory, we
design a novel dimensional schema for cultural knowledge classification,
comprising 3 layers and 140 dimensions, which guides the automated construction
of culture-specific knowledge bases and corresponding evaluation datasets for
any given languages and cultures. Experimental results demonstrate that our
method can effectively evaluate cultural understanding. They also reveal that
existing large language models lack comprehensive cultural competence, and
merely incorporating multilingual data does not necessarily enhance cultural
understanding. All code and data files are available at
https://github.com/HoganZinger/Culture

</details>


### [56] [RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation](https://arxiv.org/abs/2509.16198)
*Jane Luo,Xin Zhang,Steven Liu,Jie Wu,Yiming Huang,Yangyu Huang,Chengyu Yin,Ying Xin,Jianfeng Liu,Yuefeng Zhan,Hao Sun,Qi Chen,Scarlett Li,Mao Yang*

Main category: cs.CL

TL;DR: 该论文提出了Repository Planning Graph（RPG）来解决从零生成完整代码仓库的挑战，通过图结构统一规划和实现阶段，并开发了ZeroRepo框架在RepoCraft基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在函数和文件级代码生成表现出色，但从零生成完整仓库仍面临挑战，需要跨规划和实现阶段的连贯可靠规划。自然语言由于其模糊性和冗长性，不适合准确表示复杂软件结构。

Method: 引入Repository Planning Graph（RPG）作为持久化表示，统一规划和实现阶段，编码能力、文件结构、数据流和函数。基于RPG开发ZeroRepo框架，包含三个阶段：规划级规划、实现级细化和图引导的代码生成与测试验证。

Result: 在RepoCraft基准测试（6个真实项目，1,052个任务）上，ZeroRepo生成平均近36K行代码的仓库，约3.9倍于最强基线（Claude Code），功能覆盖率达81.5%，通过率69.7%，分别比Claude Code高出27.3和35.8个百分点。

Conclusion: RPG能够建模复杂依赖关系，通过近线性扩展实现渐进式复杂规划，增强LLM对仓库的理解，从而加速智能体定位。该方法为从零生成完整代码仓库提供了有效解决方案。

Abstract: Large language models excel at function- and file-level code generation, yet
generating complete repositories from scratch remains a fundamental challenge.
This process demands coherent and reliable planning across proposal- and
implementation-level stages, while natural language, due to its ambiguity and
verbosity, is ill-suited for faithfully representing complex software
structures. To address this, we introduce the Repository Planning Graph (RPG),
a persistent representation that unifies proposal- and implementation-level
planning by encoding capabilities, file structures, data flows, and functions
in one graph. RPG replaces ambiguous natural language with an explicit
blueprint, enabling long-horizon planning and scalable repository generation.
Building on RPG, we develop ZeroRepo, a graph-driven framework for repository
generation from scratch. It operates in three stages: proposal-level planning
and implementation-level refinement to construct the graph, followed by
graph-guided code generation with test validation. To evaluate this setting, we
construct RepoCraft, a benchmark of six real-world projects with 1,052 tasks.
On RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly
3.9$\times$ the strongest baseline (Claude Code) and about 64$\times$ other
baselines. It attains 81.5% functional coverage and a 69.7% pass rate,
exceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further
analysis shows that RPG models complex dependencies, enables progressively more
sophisticated planning through near-linear scaling, and enhances LLM
understanding of repositories, thereby accelerating agent localization.

</details>
