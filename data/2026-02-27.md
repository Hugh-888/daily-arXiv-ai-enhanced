<div id=toc></div>

# Table of Contents

- [gr-qc](#gr-qc) [Total: 6]
- [cs.LG](#cs.LG) [Total: 124]
- [quant-ph](#quant-ph) [Total: 30]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [1] [Conformal symmetry in force-free electrodynamics](https://arxiv.org/abs/2602.22262)
*Huiquan Li,Jianyong Wang*

Main category: gr-qc

TL;DR: 本文证明了在闵可夫斯基时空中的无力电动力学存在共形对称性，揭示了特定条件下Möbius变换下无力磁层解的映射关系


<details>
  <summary>Details</summary>
Motivation: 无力电动力学是描述天体磁层的基础框架，但一般的保角映射不一定能产生物理可接受的无力场配置。研究旨在探索在何种条件下共形对称性能够成立，以及这种对称性如何关联不同的已知解。

Method: 通过分析无力电动力学中的控制流方程，证明其在Möbius变换下的不变性。研究特定自由函数的选择如何使得共形对称性得以实现。

Result: 发现无力电动力学的控制流方程在Möbius变换下具有不变性。这种对称性揭示了已知解之间的结构联系，特别是能够将一个解的磁层视界内部区域映射到其对偶解的外部区域，反之亦然。

Conclusion: 在闵可夫斯基时空中，无力电动力学存在共形对称性，但仅适用于特定的自由函数选择。这种对称性通过Möbius变换建立了不同无力磁层解之间的映射关系，为理解磁层结构提供了新的视角。

Abstract: It is shown that conformal symmetry exists in force-free electrodynamics (FFE) in Minkowski spacetime, a foundational framework for describing magnetospheres around astronomical objects. In force-free magnetospheres, charges are constrained to move along magnetic field lines and experience zero Lorentz force, due to the everywhere perpendicular orientation of electric and magnetic fields. However, a general angle-preserving conformal mapping of force-free fields does not necessarily produce another physically admissible force-free configuration when sources are present. In this work, we demonstrate that such invariance can nevertheless arise for certain choices of the free functions. Specifically, the governing stream equation is shown to be invariant under Möbius transformations. This symmetry reveals a structural linkage between known solutions and, notably, maps the region inside a magnetospheric horizon (the lightsurface) of one solution to the exterior of its dual counterpart, and vice versa.

</details>


### [2] [Dymnikova Black Hole Immersed in Perfect Fluid Dark Matter and a Cloud of Strings: Hawking Temperature, Dynamics and QPOs Analysis](https://arxiv.org/abs/2602.22264)
*Faizuddin Ahmed,Sardor Murodov,Bekzod Rahmatov,Abdelmalek Bouzenada*

Main category: gr-qc

TL;DR: 研究广义Dymnikova黑洞在完美流体暗物质和弦云环境下的热力学、光学和动力学性质


<details>
  <summary>Details</summary>
Motivation: Dymnikova黑洞是连接de Sitter核心和渐近Schwarzschild几何的正则时空解。本研究旨在探索当这种黑洞被完美流体暗物质包围并浸入弦云中时，这些额外物质源如何改变时空的热力学、光学和动力学特性。

Method: 推导Hawking温度和比热容，分析热稳定性和相结构；研究光子动力学（光子球和黑洞阴影）；分析大质量测试粒子运动、圆形轨道和稳定性条件；计算基本星周频率探索准周期振荡。

Result: 发现温度呈现非单调行为，存在参数依赖的相变；PFDM和弦云参数显著影响阴影半径和强场结构；模型参数编码了可观测的天体物理特征。

Conclusion: 广义Dymnikova黑洞在PFDM和弦云环境下的热力学、光学和动力学性质受到这些额外物质源的显著影响，这些影响可通过观测特征进行探测。

Abstract: The Dymnikova black hole represents a regular spacetime solution interpolating between a de Sitter core and an asymptotically Schwarzschild geometry. In this work, we investigate a generalized Dymnikova black hole surrounded by perfect fluid dark matter (PFDM) and immersed in a cloud of strings (CS). We analyze how these additional matter sources modify the thermodynamic, optical, and dynamical properties of the spacetime.
  We derive the Hawking temperature and specific heat capacity and examine the thermal stability and phase structure of the black hole. The results reveal non-monotonic temperature behavior and parameter-dependent phase transitions. We further study photon dynamics, including the photon sphere and black hole shadow, and show that both PFDM and string cloud parameters significantly affect the shadow radius and strong-field structure.
  Additionally, we investigate the motion of massive test particles, circular orbits, and stability conditions. The corresponding effective potentials, specific energy, and angular momentum are analyzed. Finally, we explore quasi-periodic oscillations (QPOs) by computing the fundamental epicyclic frequencies and discuss how the model parameters encode observable astrophysical signatures.

</details>


### [3] [Kasner Singularity of Black Holes in Einstein-scalar Gravity](https://arxiv.org/abs/2602.22314)
*Ze-Xuan Xiong,H. Lu*

Main category: gr-qc

TL;DR: 研究爱因斯坦引力与质量标量场耦合的黑洞中类空Kasner奇点的性质，分析质量和标量荷如何影响奇点特性，发现积分常数在大质量极限下渐近于质量和标量荷的线性组合，并发现黑洞内粒子生存时间可能存在上界。


<details>
  <summary>Details</summary>
Motivation: 研究球对称、静态、渐近平坦黑洞中类空Kasner奇点的性质，特别是渐近信息（质量和标量荷）如何影响奇点特性，包括Kasner指数。探索黑洞内部几何与外部参数之间的关系。

Method: 研究爱因斯坦引力与质量标量场最小耦合的理论，采用合适的自相互作用标量势。分析球对称、静态、渐近平坦黑洞解，重点研究近奇点几何，提取非平凡积分常数，并分析其与质量和标量荷的关系。

Result: 发现从近奇点几何中可以提取非平凡积分常数，该常数在大质量极限下渐近于质量和标量荷的线性组合。同时发现黑洞内粒子在落入Kasner奇点前的最大生存时间可能存在上界，且Schwarzschild黑洞达到该上界。

Conclusion: 黑洞的渐近信息（质量和标量荷）对内部Kasner奇点性质有重要影响，近奇点几何包含可提取的积分常数。黑洞内粒子生存时间可能存在普遍上界，Schwarzschild黑洞是该上界的饱和情况。

Abstract: We study the spacelike Kasner singularity of spherically-symmetric, static and asymptotically flat black holes in Einstein gravity minimally coupled to a massless scalar with a suitable self-interacting scalar potential. We focus on how the asymptotic information such as the mass and scalar charge affect the properties of the Kasner singularity, including the Kasner exponents. We show how a nontrivial integration constant can be extracted from the near-singularity geometry and find a general pattern that this integration constant asymptotes to a linear combination of the mass and scalar charge at large mass limit. We also find that there may be a black hole upper bound on the maximum surviving time of a massive particle inside such a black hole before it falls into the Kasner singularity, and the Schwarzschild black hole saturate this bound.

</details>


### [4] [Microscopic Origin of Bekenstein-Hawking Entropy in $(2+1)$ Gravity: A Thermo Field Dynamics Approach](https://arxiv.org/abs/2602.22360)
*W. A. Rojas C.,J. R. Arenas S*

Main category: gr-qc

TL;DR: 使用热场动力学计算非旋转BTZ黑洞附近实标量场的纠缠熵，发现能量密度在视界外局域化，纠缠熵与视界面积成正比，数值上与Bekenstein-Hawking熵匹配


<details>
  <summary>Details</summary>
Motivation: 研究黑洞附近量子场的纠缠熵，探索黑洞熵的微观起源，验证砖墙模型（brick wall picture）的物理图像

Method: 采用热场动力学（TFD）方法，将黑洞建模为AdS3中坍缩的尘埃壳层，推导FIDO观测者视角的壳层轨迹R(t)，从Hartle-Hawking和Killing-Boulware真空态获得Wightman函数差，计算能量密度，并进行完整的热力学分析

Result: 发现视界外存在尖锐局域化的能量密度，与砖墙图像一致；纠缠熵与视界面积成正比，数值上与Bekenstein-Hawking熵匹配

Conclusion: 通过详细计算验证了黑洞附近量子场的纠缠熵可以解释Bekenstein-Hawking熵，支持了黑洞熵的量子场论起源

Abstract: We compute the entanglement entropy of a real massive scalar field near a non-rotating BTZ black hole using Thermo Field Dynamics. Modeling the black hole as a collapsing dust shell in AdS3, we derive the shell trajectory R(t) as seen by a Fiducial Observer (FIDO). From the Hartle-Hawking and Killing-Boulware vacua, we obtain the Wightman function difference and compute energy density, revealing a sharply localized energy density just outside the horizon, consistent with the brick wall picture. A full thermodynamic analysis yields an entanglement entropy proportional to the horizon area, numerically matching the Bekenstein-Hawking entropy. All intermediate steps, including junction conditions, Kruskal extension, WKB modes, and UV regularization, are explicitly detailed.

</details>


### [5] [Periodic Analogs of Multiple Black Holes Solutions](https://arxiv.org/abs/2602.22501)
*Omar Ortiz,Javier Peraza*

Main category: gr-qc

TL;DR: 本文通过数值研究构建了包含多个视界的周期性稳态轴对称解，特别关注具有两个等距反向旋转视界的配置，这些解可由周期L、质量M和角动量绝对值|J|>0参数化，且不受视间距限制。


<details>
  <summary>Details</summary>
Motivation: 本文动机是扩展先前关于周期性稳态轴对称解的研究，探索包含多个视界的配置。特别关注静态单视界周期性解在旋转时受到距离限制（L < 4M时无法旋转）的问题，试图寻找不受此限制的多视界旋转解。

Method: 采用数值研究方法，扩展arXiv:2210.12898中的研究框架，构建周期性稳态轴对称解。特别构造具有两个相同等距反向旋转视界的配置，通过参数化分析（周期L、质量M、角动量|J|>0），并检验轴上是否出现支撑结构（struts）。

Result: 提供了强有力的数值证据表明存在这样的多视界配置，且不受视间距限制。这些解在轴上没有支撑结构，但当视界非等距时，黑洞之间会出现支撑结构。与静态单视界解不同，这些旋转解在L < 4M时仍然存在。

Conclusion: 成功构建了不受距离限制的周期性稳态轴对称多视界旋转解，这些解在轴上无支撑结构，为黑洞物理提供了新的解族，突破了先前关于旋转解存在性的限制。

Abstract: In this article, we extend the numerical studies developed in [arXiv:2210.12898] to construct periodic stationary axisymmetric solutions containing multiple horizons in each fundamental domain. As a direct application, we consider periodic stationary axisymmetric solutions with two identical equidistant counter-rotating horizons. These solutions can be parametrized by the period $L$, the mass $M$, and the absolute value of the angular momentum $|J| >0$. We provide strong numerical evidence for the existence of such configurations, without any restriction in terms of the distance between horizons. This is in sharp contrast with the non-zero total angular momentum case, as it was recently established in \cite{Peraza:2024uto} that static single-horizon periodic solutions cannot be put into rotation if $L < 4M$. It is shown that these solutions do not have any struts on the axis, and it is also explicitly shown that, by taking non-equidistant horizons, struts develop between the black holes. Other global properties of the solutions are also presented.

</details>


### [6] [WKB-like approach to the Unruh temperature for arbitrary acceleration](https://arxiv.org/abs/2602.22534)
*Paul M. Alsing*

Main category: gr-qc

TL;DR: 该研究推广了Unruh温度公式，从恒定加速度情况扩展到任意随时间变化的加速度a(t)，并建立了相应的坐标变换和计算方法。


<details>
  <summary>Details</summary>
Motivation: 传统Unruh温度公式仅适用于恒定加速度情况，但实际物理系统中加速度可能随时间变化。需要建立适用于任意加速度a(t)的Unruh温度理论框架。

Method: 1. 推广de Gill等人的引力WKB方法，适用于任意加速度；2. 推导广义Rindler坐标，强调积分加速度χ(t)的重要性；3. 通过坐标变换到共形坐标，使波动方程解具有平面波结构；4. 对小幅偏离恒定加速度的情况建立近似方法。

Result: 1. 证明了Schwarzschild-like形式的平直度规不适用于任意a(t)的WKB计算；2. 推导了广义Unruh温度公式，将a₀推广为a(t)；3. 建立了任意加速观测者与共形坐标之间的显式坐标变换；4. 分析了多个非平凡a(t)例子及其对应的Unruh温度。

Conclusion: 成功将Unruh温度理论推广到任意随时间变化的加速度情况，建立了完整的数学框架，为研究非恒定加速观测者的量子场论效应提供了理论基础。

Abstract: In this work we study the Unruh temperature as arising from tunneling through a barrier for an observer in flat Minkowski spacetime with arbitrary acceleration $a(t)$. For the defining case of constant acceleration $a(t) = a_0$, the Unruh temperature (W. Unruh, Phys. Rev. D 14, 870 (1976)) is given by $k_b\,T_U =\tfrac{\hbar\,a_0}{2\,π\,c}$. Extending the work of de Gill et al. (A. de Gill, D. Singleton, V. Akhmedova and T. Pilling, Am. J. Phys. 78, 685 (2010)) we generalize the gravitational WKB approach to derive the Unruh temperature for arbitrary acceleration. We show that the often employed Schwarzschild-like form of the flat metric is not appropriate for the WKB calculation with an arbitrary $a(t)$, and instead derive a generalized Unruh temperature for the generalized Rindler metric where $a_0\to a(t)$. We derive a generalization of the Rindler coordinates appropriate for arbitrary $a(t)$, and stress the importance of the role of the integrated acceleration $χ(t) = \int^t dt'\,a(t')$, which can also act as a temporal coordinate. We explore several non-trivial examples of $a(t)$ and their generalized Unruh temperatures. We additionally develop an approximation to the Unruh temperature for small deviations away from constant acceleration by the standard approach of considering the negative frequency content of a purely positive frequency plane wave of an inertial observer, as measured by the co-moving arbitrarily accelerated observer. Lastly, we develop and explicit coordinate transformation between the arbitrarily accelerated observer and conformal coordinates, where the plane wave structure of the solutions of the wave equation is readily transparent, and analogous to the form for the inertial observer.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [To Deceive is to Teach? Forging Perceptual Robustness via Adversarial Reinforcement Learning](https://arxiv.org/abs/2602.22227)
*Yicheng Bao,Xuhong Wang,Xin Tan*

Main category: cs.LG

TL;DR: AOT-SFT是一个大规模对抗数据集，AOT是一个自博弈框架，通过图像编辑攻击者和防御者MLLM的协同进化来提升多模态大语言模型的感知鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在面对视觉复杂场景时表现出感知脆弱性，这种弱点源于对有限训练数据的依赖，而大规模数据集的构建成本高昂且限制了模型鲁棒性的提升。

Method: 提出AOT-SFT对抗数据集和AOT自博弈框架。框架包含图像编辑攻击者和防御者MLLM，攻击者生成多样化的图像操作作为动态课程，迫使防御者适应和改进。

Result: 实验表明AOT能显著提升防御者的感知鲁棒性并减少幻觉，为训练更可靠的多模态大语言模型建立了可扩展的范式。

Conclusion: AOT框架通过自博弈的协同进化机制，为提升MLLM的感知鲁棒性提供了一种可扩展的解决方案，能够减少对大规模人工标注数据的依赖。

Abstract: Despite their impressive capabilities, Multimodal Large Language Models (MLLMs) exhibit perceptual fragility when confronted with visually complex scenes. This weakness stems from a reliance on finite training datasets, which are prohibitively expensive to scale and impose a ceiling on model robustness. We introduce \textbf{AOT-SFT}, a large-scale adversarial dataset for bootstrapping MLLM robustness. Building on this, we propose \textbf{AOT (Adversarial Opponent Training)}, a self-play framework that forges MLLM robustness by creating its own training data. Our method orchestrates a co-evolution between an image-editing Attacker and a Defender MLLM, where the Attacker generates a diverse and dynamic curriculum of image manipulations, forcing the Defender to adapt and improve. Extensive experiments demonstrate that AOT enhances the Defender's perceptual robustness and reduces hallucinations, establishing a scalable paradigm for training more reliable MLLMs.

</details>


### [8] [Patient-Centered, Graph-Augmented Artificial Intelligence-Enabled Passive Surveillance for Early Stroke Risk Detection in High-Risk Individuals](https://arxiv.org/abs/2602.22228)
*Jiyeong Kim,Stephen P. Ma,Nirali Vora,Nicholas W. Larsen,Julia Adler-Milstein,Jonathan H. Chen,Selen Bozkurt,Abeed Sarker,Juhee Cho,Jindeok Joo,Natali Pageler,Fatima Rodriguez,Christopher Sharp,Eleni Linos*

Main category: cs.LG

TL;DR: 开发基于患者自述症状的被动监测系统，用于糖尿病患者的早期中风风险检测，通过症状分类和机器学习识别中风相关症状模式，构建混合风险筛查系统实现高精度预警。


<details>
  <summary>Details</summary>
Motivation: 中风影响数百万人，但症状识别不足常导致就医延迟。为解决风险识别差距，需要开发早期中风风险检测系统，特别是针对糖尿病患者群体。

Method: 1. 基于患者自身语言构建症状分类法；2. 开发双机器学习流程（异构图神经网络和弹性网络/LASSO）；3. 识别与后续中风相关的症状模式；4. 构建混合风险筛查系统，整合症状相关性和时间接近性；5. 通过基于电子健康记录的模拟在3-90天窗口内评估。

Result: 在保守阈值下（旨在最小化误报），筛查系统达到高特异性（1.00）和患病率调整阳性预测值（1.00），灵敏度良好（0.72），这是优先考虑精度的预期权衡。在90天窗口内表现最佳。仅凭患者报告的语言就能支持高精度、低负担的早期中风风险检测。

Conclusion: 基于患者自述语言的被动监测系统能够实现高精度、低负担的早期中风风险检测，为高风险个体提供宝贵的临床评估和干预时间窗口。

Abstract: Stroke affected millions annually, yet poor symptom recognition often delayed care-seeking. To address risk recognition gap, we developed a passive surveillance system for early stroke risk detection using patient-reported symptoms among individuals with diabetes. Constructing a symptom taxonomy grounded in patients own language and a dual machine learning pipeline (heterogeneous GNN and EN/LASSO), we identified symptom patterns associated with subsequent stroke. We translated findings into a hybrid risk screening system integrating symptom relevance and temporal proximity, evaluated across 3-90 day windows through EHR-based simulations. Under conservative thresholds, intentionally designed to minimize false alerts, the screening system achieved high specificity (1.00) and prevalence-adjusted positive predictive value (1.00), with good sensitivity (0.72), an expected trade-off prioritizing precision, that was highest in 90-day window. Patient-reported language alone supported high-precision, low-burden early stroke risk detection, that could offer a valuable time window for clinical evaluation and intervention for high-risk individuals.

</details>


### [9] [Improving Spatial Allocation for Energy System Coupling with Graph Neural Networks](https://arxiv.org/abs/2602.22249)
*Xuanhao Mu,Jakob Geiges,Nan Liu,Thorsten Schlachter,Veit Hagenmeyer*

Main category: cs.LG

TL;DR: 提出基于自监督异构图神经网络的权重生成方法，改进传统Voronoi分配，解决能源系统分析中空间分辨率不匹配问题


<details>
  <summary>Details</summary>
Motivation: 能源系统分析中耦合不同空间分辨率模型存在挑战，传统方法仅使用单一地理属性进行聚合，无法充分利用多源地理信息

Method: 使用自监督异构图神经网络，将高分辨率地理单元建模为图节点，整合多种地理特征生成物理意义明确的权重，改进传统Voronoi分配方法

Result: 该方法生成的权重应用于基于聚类的Voronoi图，显著提升了可扩展性、准确性、物理合理性和精度，优于传统方法

Conclusion: 提出的自监督异构图神经网络方法能有效解决空间分辨率不匹配问题，通过整合多源地理信息生成物理意义明确的权重，为能源系统分析提供更优解决方案

Abstract: In energy system analysis, coupling models with mismatched spatial resolutions is a significant challenge. A common solution is assigning weights to high-resolution geographic units for aggregation, but traditional models are limited by using only a single geospatial attribute. This paper presents an innovative method employing a self-supervised Heterogeneous Graph Neural Network to address this issue. This method models high-resolution geographic units as graph nodes, integrating various geographical features to generate physically meaningful weights for each grid point. These weights enhance the conventional Voronoi-based allocation method, allowing it to go beyond simply geographic proximity by incorporating essential geographic information.In addition, the self-supervised learning paradigm overcomes the lack of accurate ground-truth data. Experimental results demonstrate that applying weights generated by this method to cluster-based Voronoi Diagrams significantly enhances scalability, accuracy, and physical plausibility, while increasing precision compared to traditional methods.

</details>


### [10] [Zatom-1: A Multimodal Flow Foundation Model for 3D Molecules and Materials](https://arxiv.org/abs/2602.22251)
*Alex Morehead,Miruna Cretu,Antonia Panescu,Rishabh Anand,Maurice Weiler,Tynan Perez,Samuel Blau,Steven Farrell,Wahid Bhimji,Anubhav Jain,Hrushikesh Sahasrabuddhe,Pietro Lio,Tommi Jaakkola,Rafael Gomez-Bombarelli,Rex Ying,N. Benjamin Erichson,Michael W. Mahoney*

Main category: cs.LG

TL;DR: Zatom-1是首个统一3D分子和材料生成与预测的基础模型，通过多模态流匹配Transformer实现跨化学域表示共享，显著提升性能并减少推理时间


<details>
  <summary>Details</summary>
Motivation: 现有AI方法通常针对单一化学域（分子或材料）和单一任务（生成或预测），限制了表示共享和迁移学习。需要开发能够统一处理3D化学建模的通用基础模型

Method: 使用多模态流匹配目标训练的Transformer，联合建模离散原子类型和连续3D几何结构。通过联合生成预训练作为下游多任务预测的通用初始化

Result: Zatom-1在生成和预测基准测试中匹配或超越专业基线模型，同时将生成推理时间减少一个数量级以上。实验显示联合生成预训练在化学域间产生正向预测迁移

Conclusion: Zatom-1成功统一了3D分子和材料的生成与预测学习，展示了跨化学域联合预训练的有效性，为通用3D化学建模提供了有前景的基础模型

Abstract: General-purpose 3D chemical modeling encompasses molecules and materials, requiring both generative and predictive capabilities. However, most existing AI approaches are optimized for a single domain (molecules or materials) and a single task (generation or prediction), which limits representation sharing and transfer. We introduce Zatom-1, the first foundation model that unifies generative and predictive learning of 3D molecules and materials. Zatom-1 is a Transformer trained with a multimodal flow matching objective that jointly models discrete atom types and continuous 3D geometries. This approach supports scalable pretraining with predictable gains as model capacity increases, while enabling fast and stable sampling. We use joint generative pretraining as a universal initialization for downstream multi-task prediction of properties, energies, and forces. Empirically, Zatom-1 matches or outperforms specialized baselines on both generative and predictive benchmarks, while reducing the generative inference time by more than an order of magnitude. Our experiments demonstrate positive predictive transfer between chemical domains from joint generative pretraining: modeling materials during pretraining improves molecular property prediction accuracy.

</details>


### [11] [Causal Direction from Convergence Time: Faster Training in the True Causal Direction](https://arxiv.org/abs/2602.22254)
*Abdulrahman Tamim*

Main category: cs.LG

TL;DR: 提出因果计算不对称性（CCA）原则，通过比较神经网络从X预测Y和从Y预测X的收敛速度来识别因果方向，正向因果方向收敛更快。


<details>
  <summary>Details</summary>
Motivation: 现有因果方向识别方法（如RESIT、IGCI、SkewScore）依赖统计独立性或分布不对称性，本文提出基于优化动态的新方法，利用神经网络训练中的收敛速度差异来识别因果方向。

Method: 在加性噪声模型Y=f(X)+ε（ε⊥X，f非线性单射）下，训练两个神经网络分别预测Y|X和X|Y，比较两者的收敛速度。反向模型中残差与输入保持统计依赖，导致更高的不可约损失下限和非可分离梯度噪声，因此需要更多梯度步数。

Result: 在合成基准测试中，CCA在六种神经网络架构上实现了26/30的正确因果识别，在正弦和指数数据生成过程中达到30/30的完美识别率。理论保证得到形式化证明和实证验证。

Conclusion: CCA提供了一种基于优化动态的因果方向识别新方法，可嵌入更广泛的因果压缩学习（CCL）框架，整合图结构学习、因果信息压缩和策略优化，具有理论保证和实证有效性。

Abstract: We introduce Causal Computational Asymmetry (CCA), a principle for causal direction identification based on optimization dynamics in which one neural network is trained to predict $Y$ from $X$ and another to predict $X$ from $Y$, and the direction that converges faster is inferred to be causal. Under the additive noise model $Y = f(X) + \varepsilon$ with $\varepsilon \perp X$ and $f$ nonlinear and injective, we establish a formal asymmetry: in the reverse direction, residuals remain statistically dependent on the input regardless of approximation quality, inducing a strictly higher irreducible loss floor and non-separable gradient noise in the optimization dynamics, so that the reverse model requires strictly more gradient steps in expectation to reach any fixed loss threshold; consequently, the forward (causal) direction converges in fewer expected optimization steps. CCA operates in optimization-time space, distinguishing it from methods such as RESIT, IGCI, and SkewScore that rely on statistical independence or distributional asymmetries, and proper z-scoring of both variables is required for valid comparison of convergence rates. On synthetic benchmarks, CCA achieves 26/30 correct causal identifications across six neural architectures, including 30/30 on sine and exponential data-generating processes. We further embed CCA into a broader framework termed Causal Compression Learning (CCL), which integrates graph structure learning, causal information compression, and policy optimization, with all theoretical guarantees formally proved and empirically validated on synthetic datasets.

</details>


### [12] [Deep Sequence Modeling with Quantum Dynamics: Language as a Wave Function](https://arxiv.org/abs/2602.22255)
*Ahmed Nebli,Hadi Saadatdoorabi,Kevin Yam*

Main category: cs.LG

TL;DR: 提出一种基于量子力学原理的序列建模框架，使用复值波函数作为隐状态，通过学习的时间相关哈密顿量演化，利用量子干涉机制实现信息处理。


<details>
  <summary>Details</summary>
Motivation: 传统循环架构依赖门控机制抑制竞争假设，本文希望利用量子干涉原理，通过相位操控让冲突解释相互抵消、兼容解释相互增强，实现更高效的信息处理。

Method: 使用有限维希尔伯特空间中的复值波函数作为隐状态，通过学习的时变哈密顿量进行演化，采用Cayley离散化保持严格幺正性，通过玻恩规则提取token概率。

Result: 理论证明：对于定义的消歧任务，复值幺正模型维度N可精确解决，而实值正交模型需要Ω(N²)维度，展示了玻恩规则通过相位相关提供的二次优势。

Conclusion: 量子干涉机制为序列建模提供了新的理论框架，玻恩规则通过相位相关实现维度效率的二次提升，同时守恒的成对电流为信息流追踪提供了内置诊断工具。

Abstract: We introduce a sequence modeling framework in which the latent state is a complex-valued wave function evolving on a finite-dimensional Hilbert space under a learned, time-dependent Hamiltonian. Unlike standard recurrent architectures that rely on gating mechanisms to suppress competing hypotheses, our framework utilizes quantum interference: the Hamiltonian steers the phases of complex amplitudes so that conflicting interpretations cancel while compatible ones reinforce. The dynamics are strictly unitary, ensuring that the state norm is preserved exactly at every time step via a Cayley (Crank--Nicolson) discretization. Token probabilities are extracted using the Born rule, a quadratic measurement operator that couples magnitudes and relative phases. Our primary theoretical contribution is a separation theorem characterizing the representational advantage of this readout: we define a family of disambiguation tasks that a complex unitary model of dimension $N$ solves exactly, but which requires a state dimension of $Ω(N^2)$ for any real-valued orthogonal model equipped with a standard affine-softmax readout. This quadratic gap arises because the Born rule implicitly lifts the $N$-dimensional state into the space of rank-one Hermitian matrices, accessing pairwise phase correlations that are inaccessible to linear projections. Finally, we derive a continuity equation for the latent probability mass, yielding conserved pairwise currents that serve as a built-in diagnostic for tracing information flow between dimensions.

</details>


### [13] [Orthogonal Weight Modification Enhances Learning Scalability and Convergence Efficiency without Gradient Backpropagation](https://arxiv.org/abs/2602.22259)
*Guoqing Ma,Shan Yu*

Main category: cs.LG

TL;DR: 提出LOCO权重修改方法，一种基于扰动的非反向传播算法，通过低秩聚类正交约束提高训练效率，能在10层以上脉冲神经网络上实现局部训练，具有O(1)并行时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 反向传播算法计算成本高，不适合新兴神经形态系统。现有非反向传播方法在效率和可扩展性方面仍面临挑战，需要更高效的替代方案。

Method: 提出LOCO（低秩聚类正交）权重修改方法，基于扰动机制，利用低秩特性和正交约束来限制节点扰动的梯度估计方差，提高收敛效率。

Result: LOCO能够训练超过10层的深层脉冲神经网络，表现出强大的持续学习能力、改进的收敛效率和更好的任务性能，相比其他脑启发非反向传播算法有优势。

Conclusion: LOCO提供了一种有前景的方向，可在神经形态系统上实现高性能、实时和终身学习，其O(1)并行时间复杂度显著低于反向传播方法。

Abstract: Recognizing the substantial computational cost of backpropagation (BP), non-BP methods have emerged as attractive alternatives for efficient learning on emerging neuromorphic systems. However, existing non-BP approaches still face critical challenges in efficiency and scalability. Inspired by neural representations and dynamic mechanisms in the brain, we propose a perturbation-based approach called LOw-rank Cluster Orthogonal (LOCO) weight modification. We find that low-rank is an inherent property of perturbation-based algorithms. Under this condition, the orthogonality constraint limits the variance of the node perturbation (NP) gradient estimates and enhances the convergence efficiency. Through extensive evaluations on multiple datasets, LOCO demonstrates the capability to locally train the deepest spiking neural networks to date (more than 10 layers), while exhibiting strong continual learning ability, improved convergence efficiency, and better task performance compared to other brain-inspired non-BP algorithms. Notably, LOCO requires only O(1) parallel time complexity for weight updates, which is significantly lower than that of BP methods. This offers a promising direction for achieving high-performance, real-time, and lifelong learning on neuromorphic systems.

</details>


### [14] [Code World Models for Parameter Control in Evolutionary Algorithms](https://arxiv.org/abs/2602.22260)
*Camilo Chacón Sartori,Guillem Rodríguez Corominas*

Main category: cs.LG

TL;DR: LLM通过合成Python程序来模拟优化器动态，并利用该模拟器进行贪婪规划，在组合优化问题上超越现有自适应基线方法


<details>
  <summary>Details</summary>
Motivation: 研究LLM是否能学习优化器的行为模式，并利用这种知识来控制优化器，特别是在随机组合优化场景中

Method: 扩展代码世界模型(CWM)到随机组合优化：给定次优轨迹，LLM合成优化器动态的模拟器，然后通过贪婪规划选择每一步的变异强度k

Result: 在LO和OneMax问题上，CWM-greedy达到理论最优策略的94%；在Jump_k问题上，所有基线方法失败时CWM-greedy达到100%成功率；在NK-Landscape上超越所有基线；比DQN样本效率更高、成功率更高、泛化能力更强

Conclusion: LLM能够学习优化器的行为模式并有效控制优化器，CWM方法在随机组合优化中表现出色，特别是在没有封闭形式模型的情况下仍能取得优异性能

Abstract: Can an LLM learn how an optimizer behaves -- and use that knowledge to control it? We extend Code World Models (CWMs), LLM-synthesized Python programs that predict environment dynamics, from deterministic games to stochastic combinatorial optimization. Given suboptimal trajectories of $(1{+}1)$-$\text{RLS}_k$, the LLM synthesizes a simulator of the optimizer's dynamics; greedy planning over this simulator then selects the mutation strength $k$ at each step. On \lo{} and \onemax{}, CWM-greedy performs within 6\% of the theoretically optimal policy -- without ever seeing optimal-policy trajectories. On \jump{$_k$}, where a deceptive valley causes all adaptive baselines to fail (0\% success rate), CWM-greedy achieves 100\% success rate -- without any collection policy using oracle knowledge of the gap parameter. On the NK-Landscape, where no closed-form model exists, CWM-greedy outperforms all baselines across fifteen independently generated instances ($36.94$ vs.\ $36.32$; $p<0.001$) when the prompt includes empirical transition statistics. The CWM also outperforms DQN in sample efficiency (200 offline trajectories vs.\ 500 online episodes), success rate (100\% vs.\ 58\%), and generalization ($k{=}3$: 78\% vs.\ 0\%). Robustness experiments confirm stable synthesis across 5 independent runs.

</details>


### [15] [Sustainable LLM Inference using Context-Aware Model Switching](https://arxiv.org/abs/2602.22261)
*Yuvarani,Akashdeep Singh,Zahra Fathanah,Salsabila Harlen,Syeikha Syafura Al-Zahra binti Zahari,Hema Subramaniam*

Main category: cs.LG

TL;DR: 提出基于上下文感知的模型切换方法，根据查询复杂度动态选择语言模型，可减少67.5%能耗，同时保持93.6%的响应质量


<details>
  <summary>Details</summary>
Motivation: 当前AI部署依赖一刀切的推理策略，所有请求都路由到同一个大模型，不考虑任务复杂度，导致大量不必要的能源浪费。随着大语言模型能耗增加，需要更可持续的解决方案。

Method: 提出上下文感知模型切换方法，结合缓存重复查询、基于规则的复杂度评分、机器学习分类捕获语义意图、用户自适应组件学习交互模式。使用三个开源模型(Gemma3 1B, 4B和Qwen3 4B)进行实验。

Result: 相比始终使用最大模型，能耗降低67.5%，响应质量保持93.6%。简单查询响应时间提升约68%。使用真实对话工作负载评估，测量能耗、延迟、路由准确性和输出质量。

Conclusion: 模型切换推理为更节能、可持续的AI系统提供了实用且可扩展的路径，表明无需牺牲响应质量即可实现显著效率提升。

Abstract: Large language models have become central to many AI applications, but their growing energy consumption raises serious sustainability concerns. A key limitation in current AI deployments is the reliance on a one-size-fits-all inference strategy where most systems route every request to the same large model, regardless of task complexity, leading to substantial and unnecessary energy waste. To address this issue, we propose a context-aware model switching approach that dynamically selects an appropriate language model based on query complexity. The proposed system uses a Context-Aware Model Switching for Energy-Efficient LLM Inference that combines caching for repeated queries, rulebased complexity scoring for fast and explainable decisions, machine learning classification to capture semantic intent, and a user-adaptive component that learns from interaction patterns over time. The proposed architecture was evaluated using real conversation workloads and three open-source language models (Gemma3 1B, Gemma3 4B and Qwen3 4B) with different computational costs, measuring energy consumption (via NVML GPU power telemetry), response latency, routing accuracy, and output quality (BERTScore F1) to reflect real-world usage conditions. Experimental results show that the model switching approach can reduce energy consumption by up to 67.5% compared to always using the largest model while maintaining a response quality of 93.6%. In addition, the response time for simple queries also improved significantly by approximately 68%. These results show that model switching inference offers a practical and scalable path toward more energy-efficient and sustainable AI systems, demonstrating that significant efficiency gains can be achieved without major sacrifices in response quality.

</details>


### [16] [Entropy-Controlled Flow Matching](https://arxiv.org/abs/2602.22265)
*Chika Maduabuchi*

Main category: cs.LG

TL;DR: 提出熵控制流匹配(ECFM)，通过全局熵率约束解决传统流匹配中信息几何控制不足导致的模式崩溃问题


<details>
  <summary>Details</summary>
Motivation: 传统流匹配方法（ODE/SDE）虽然性能强大，但目标函数不直接控制轨迹的信息几何，允许低熵瓶颈出现，可能导致语义模式暂时耗尽（模式崩溃）

Method: 提出熵控制流匹配(ECFM)：在连续性方程路径上施加全局熵率约束d/dt H(mu_t) >= -lambda的变分原理；这是Wasserstein空间中的凸优化问题，具有KKT/Pontryagin系统，且等价于具有显式熵乘子的Schrödinger桥

Result: 在纯传输机制下，ECFM恢复熵最优传输测地线，当lambda→0时Gamma收敛到经典最优传输；获得证书式模式覆盖和密度下限保证，具有Lipschitz稳定性；构建了无约束流匹配的近乎最优崩溃反例

Conclusion: ECFM通过熵率约束有效控制生成过程中的信息几何，防止模式崩溃，提供理论保证，是传统流匹配方法的改进

Abstract: Modern vision generators transport a base distribution to data through time-indexed measures, implemented as deterministic flows (ODEs) or stochastic diffusions (SDEs). Despite strong empirical performance, standard flow-matching objectives do not directly control the information geometry of the trajectory, allowing low-entropy bottlenecks that can transiently deplete semantic modes. We propose Entropy-Controlled Flow Matching (ECFM): a constrained variational principle over continuity-equation paths enforcing a global entropy-rate budget d/dt H(mu_t) >= -lambda. ECFM is a convex optimization in Wasserstein space with a KKT/Pontryagin system, and admits a stochastic-control representation equivalent to a Schrodinger bridge with an explicit entropy multiplier. In the pure transport regime, ECFM recovers entropic OT geodesics and Gamma-converges to classical OT as lambda -> 0. We further obtain certificate-style mode-coverage and density-floor guarantees with Lipschitz stability, and construct near-optimal collapse counterexamples for unconstrained flow matching.

</details>


### [17] [WaveSSM: Multiscale State-Space Models for Non-stationary Signal Attention](https://arxiv.org/abs/2602.22266)
*Ruben Solozabal,Velibor Bojkovic,Hilal Alquabeh,Klea Ziu,Kentaro Inui,Martin Takac*

Main category: cs.LG

TL;DR: WaveSSM：基于小波框架的状态空间模型，针对具有瞬态动态的序列建模任务，在生理信号和语音命令数据集上优于S4等正交方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于投影的状态空间模型（如HiPPO框架）通常使用具有全局时间支持的多项式基，其归纳偏置与具有局部化或瞬态结构的信号不匹配。需要开发能够更好处理局部化特征的SSM变体。

Method: 提出WaveSSM，基于小波框架构建的状态空间模型集合。关键洞察是小波框架在时间维度上具有局部化支持，适用于需要精确定位的任务。

Result: 在同等条件下，WaveSSM在具有瞬态动态的真实世界数据集上优于正交对应方法S4，包括PTB-XL数据集上的生理信号和Speech Commands数据集上的原始音频。

Conclusion: 小波框架为状态空间模型提供了有效的局部化时间支持，特别适合处理具有瞬态动态的序列建模任务，在生理信号和音频处理等实际应用中表现出优越性能。

Abstract: State-space models (SSMs) have emerged as a powerful foundation for long-range sequence modeling, with the HiPPO framework showing that continuous-time projection operators can be used to derive stable, memory-efficient dynamical systems that encode the past history of the input signal. However, existing projection-based SSMs often rely on polynomial bases with global temporal support, whose inductive biases are poorly matched to signals exhibiting localized or transient structure. In this work, we introduce \emph{WaveSSM}, a collection of SSMs constructed over wavelet frames. Our key observation is that wavelet frames yield a localized support on the temporal dimension, useful for tasks requiring precise localization. Empirically, we show that on equal conditions, \textit{WaveSSM} outperforms orthogonal counterparts as S4 on real-world datasets with transient dynamics, including physiological signals on the PTB-XL dataset and raw audio on Speech Commands.

</details>


### [18] [Data-Driven Supervision of a Thermal-Hydraulic Process Towards a Physics-Based Digital Twin](https://arxiv.org/abs/2602.22267)
*Osimone Imhogiemhe,Yoann Jus,Hubert Lejeune,Saïd Moussaoui*

Main category: cs.LG

TL;DR: 开发用于热工水力过程故障检测与诊断的数字孪生系统，结合数值模拟和机器学习方法，实现参数变化检测和在线估计


<details>
  <summary>Details</summary>
Motivation: 生产过程实时监控是多个行业的共同挑战，需要确保安全、连续生产和高效运行。数字孪生概念为解决这些挑战提供了合适的框架

Method: 基于系统数值模拟和机器学习方法，开发专门用于过程参数变化检测和在线估计的不同模块，构建故障检测与诊断算法

Result: 在特定测试场景中验证算法，针对系统中单次参数变化情况，数值结果显示在参数变化定位和值更新方面具有良好的准确性

Conclusion: 提出的数字孪生方法能够有效实现热工水力过程的故障检测与诊断，为生产过程监控提供了可行的技术方案

Abstract: The real-time supervision of production processes is a common challenge across several industries. It targets process component monitoring and its predictive maintenance in order to ensure safety, uninterrupted production and maintain high efficiency level. The rise of advanced tools for the simulation of physical systems in addition to data-driven machine learning models offers the possibility to design numerical tools dedicated to efficient system monitoring. In that respect, the digital twin concept presents an adequate framework that proffers solution to these challenges. The main purpose of this paper is to develop such a digital twin dedicated to fault detection and diagnosis in the context of a thermal-hydraulic process supervision. Based on a numerical simulation of the system, in addition to machine learning methods, we propose different modules dedicated to process parameter change detection and their on-line estimation. The proposed fault detection and diagnosis algorithm is validated on a specific test scenario, with single one-off parameter change occurrences in the system. The numerical results show good accuracy in terms of parameter variation localization and the update of their values.

</details>


### [19] [AutoQRA: Joint Optimization of Mixed-Precision Quantization and Low-rank Adapters for Efficient LLM Fine-Tuning](https://arxiv.org/abs/2602.22268)
*Changhai Zhou,Shiyang Zhang,Yuhua Zhou,Qian Qiao,Jun Gao,Cheng Jin,Kaizhou Qin,Weizhong Zhang*

Main category: cs.LG

TL;DR: AutoQRA：联合优化量化位宽和LoRA秩的框架，通过两阶段搜索在内存约束下实现接近全精度微调的性能


<details>
  <summary>Details</summary>
Motivation: 现有量化后参数高效微调方法忽略了量化位宽与LoRA秩之间的复杂交互关系，导致优化后的量化分配不一定带来最佳微调性能，不同配置在相同内存预算下结果差异显著

Method: 两阶段优化框架：1）全局多保真度进化搜索，通过层重要性先验初始化种群，使用特定算子和性能模型高效筛选候选配置；2）信任域贝叶斯优化，局部精化搜索空间中有前景的区域，在给定内存预算下找到最优配置

Result: AutoQRA实现了接近全精度微调的性能，同时内存占用与均匀4位量化方法相当

Conclusion: AutoQRA通过联合优化量化位宽和LoRA秩配置，有效解决了量化与微调之间的交互问题，在内存约束下实现了优异的性能表现

Abstract: Quantization followed by parameter-efficient fine-tuning has emerged as a promising paradigm for downstream adaptation under tight GPU memory constraints. However, this sequential pipeline fails to leverage the intricate interaction between quantization bit-width and LoRA rank. Specifically, a carefully optimized quantization allocation with low quantization error does not always translate to strong fine-tuning performance, and different bit-width and rank configurations can lead to significantly varying outcomes under the same memory budget. To address this limitation, we propose AutoQRA, a joint optimization framework that simultaneously optimizes the bit-width and LoRA rank configuration for each layer during the mixed quantized fine-tuning process. To tackle the challenges posed by the large discrete search space and the high evaluation cost associated with frequent fine-tuning iterations, AutoQRA decomposes the optimization process into two stages. First, it first conducts a global multi-fidelity evolutionary search, where the initial population is warm-started by injecting layer-wise importance priors. This stage employs specific operators and a performance model to efficiently screen candidate configurations. Second, trust-region Bayesian optimization is applied to locally refine promising regions of the search space and identify optimal configurations under the given memory budget. This approach enables active compensation for quantization noise in specific layers during training. Experiments show that AutoQRA achieves performance close to full-precision fine-tuning with a memory footprint comparable to uniform 4-bit methods.

</details>


### [20] [CQSA: Byzantine-robust Clustered Quantum Secure Aggregation in Federated Learning](https://arxiv.org/abs/2602.22269)
*Arnab Nath,Harsh Kasyap*

Main category: cs.LG

TL;DR: 提出CQSA（聚类量子安全聚合）框架，通过将客户端随机分组为小集群，使用高保真度、低量子比特的GHZ态进行本地量子聚合，解决了现有量子安全聚合协议在大规模GHZ态保真度下降和无法检测拜占庭客户端的问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中共享的本地模型更新容易受到推理和投毒攻击，现有量子安全聚合协议依赖单个全局GHZ态，存在两个根本问题：1) 随着客户端数量增加，大规模GHZ态的保真度急剧下降；2) 全局聚合无法检测拜占庭客户端。

Method: 提出CQSA框架：1) 将客户端随机分区为小集群；2) 每个集群使用高保真度、低量子比特的GHZ态进行本地量子聚合；3) 服务器通过余弦相似度和欧几里得距离等统计度量分析集群级聚合之间的统计关系，识别恶意贡献。

Result: 通过理论分析和在去极化噪声下的仿真表明，CQSA确保了模型收敛的稳定性，相比全局QSA实现了更优的态保真度。

Conclusion: CQSA框架调和了近量子硬件的物理约束与联邦学习中拜占庭鲁棒性的需求，为量子辅助联邦学习提供了更实用、更安全的聚合方案。

Abstract: Federated Learning (FL) enables collaborative model training without sharing raw data. However, shared local model updates remain vulnerable to inference and poisoning attacks. Secure aggregation schemes have been proposed to mitigate these attacks. In this work, we aim to understand how these techniques are implemented in quantum-assisted FL. Quantum Secure Aggregation (QSA) has been proposed, offering information-theoretic privacy by encoding client updates into the global phase of multipartite entangled states. Existing QSA protocols, however, rely on a single global Greenberger-Horne-Zeilinger (GHZ) state shared among all participating clients. This design poses fundamental challenges: fidelity of large-scale GHZ states deteriorates rapidly with the increasing number of clients; and (ii) the global aggregation prevents the detection of Byzantine clients. We propose Clustered Quantum Secure Aggregation (CQSA), a modular aggregation framework that reconciles the physical constraints of near-term quantum hardware along with the need for Byzantine-robustness in FL. CQSA randomly partitions the clients into small clusters, each performing local quantum aggregation using high-fidelity, low-qubit GHZ states. The server analyzes statistical relationships between cluster-level aggregates employing common statistical measures such as cosine similarity and Euclidean distance to identify malicious contributions. Through theoretical analysis and simulations under depolarizing noise, we demonstrate that CQSA ensures stable model convergence, achieves superior state fidelity over global QSA.

</details>


### [21] [Prior Knowledge-enhanced Spatio-temporal Epidemic Forecasting](https://arxiv.org/abs/2602.22270)
*Sijie Ruan,Jinyu Li,Jia Wei,Zenghao Xu,Jie Bao,Junshi Xu,Junyang Qiu,Hanning Yuan,Xiaoxiao Wang,Shuliang Wang*

Main category: cs.LG

TL;DR: STOEP是一个结合隐式时空先验和显式专家先验的混合框架，用于时空流行病预测，通过动态调整区域依赖、增强弱信号和正则化参数来提高预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有流行病预测方法存在三个主要问题：对弱疫情信号不敏感、空间关系过于简化、参数估计不稳定。这些问题限制了预测的准确性和实用性。

Method: STOEP包含三个核心组件：1) 病例感知邻接学习(CAL) - 基于历史感染模式动态调整移动性区域依赖；2) 空间信息参数估计(SPE) - 使用可学习空间先验增强弱疫情信号；3) 基于滤波的机制预测(FMF) - 采用专家引导的自适应阈值策略正则化流行病参数。

Result: 在真实世界的COVID-19和流感数据集上，STOEP在RMSE指标上比最佳基线方法提高了11.1%。该系统已在中国某省级疾控中心部署，支持下游应用。

Conclusion: STOEP通过整合隐式和显式先验知识，有效解决了现有流行病预测方法的局限性，在预测精度和实际部署方面都取得了显著成果。

Abstract: Spatio-temporal epidemic forecasting is critical for public health management, yet existing methods often struggle with insensitivity to weak epidemic signals, over-simplified spatial relations, and unstable parameter estimation. To address these challenges, we propose the Spatio-Temporal priOr-aware Epidemic Predictor (STOEP), a novel hybrid framework that integrates implicit spatio-temporal priors and explicit expert priors. STOEP consists of three key components: (1) Case-aware Adjacency Learning (CAL), which dynamically adjusts mobility-based regional dependencies using historical infection patterns; (2) Space-informed Parameter Estimating (SPE), which employs learnable spatial priors to amplify weak epidemic signals; and (3) Filter-based Mechanistic Forecasting (FMF), which uses an expert-guided adaptive thresholding strategy to regularize epidemic parameters. Extensive experiments on real-world COVID-19 and influenza datasets demonstrate that STOEP outperforms the best baseline by 11.1% in RMSE. The system has been deployed at one provincial CDC in China to facilitate downstream applications.

</details>


### [22] [Support Tokens, Stability Margins, and a New Foundation for Robust LLMs](https://arxiv.org/abs/2602.22271)
*Deepak Agarwal,Dhyey Dharmendrakumar Mavani,Suyash Gupta,Karthik Sethuraman,Tejas Dharamsi*

Main category: cs.LG

TL;DR: 将因果自注意力Transformer重新解释为概率框架，揭示参数约束导致token空间结构化几何，提出类似支持向量机的边界解释和"支持token"概念，并引入贝叶斯框架和MAP目标，通过添加平滑对数障碍惩罚改进LLM训练。


<details>
  <summary>Details</summary>
Motivation: 重新解释现代基础模型的核心组件——因果自注意力Transformer，将其置于概率框架中（类似PCA到概率PCA的扩展），以揭示其深层结构特性，并为LLM解码动态提供理论洞察。

Method: 1. 将因果自注意力Transformer重新表述为概率模型；2. 揭示参数约束导致的token空间结构化几何；3. 提出类似支持向量机的边界解释和"支持token"概念；4. 将LLM解释为token空间幂集上的随机过程；5. 提出贝叶斯框架和MAP估计目标，在标准交叉熵损失中添加平滑对数障碍惩罚。

Result: 1. 发现自注意力参数存在障碍约束，导致token空间高度结构化几何；2. 揭示注意力变得病态的边界，提供类似支持向量机的边际解释；3. 提出的贝叶斯框架和MAP目标在实践中易于实现，能提供更鲁棒的模型而不牺牲样本外准确性。

Conclusion: 通过概率重新解释因果自注意力Transformer，揭示了其深层结构特性和几何约束，提出了类似支持向量机的理论框架，并开发了实用的贝叶斯训练方法，为理解LLM解码动态和改进模型鲁棒性提供了新视角。

Abstract: Self-attention is usually described as a flexible, content-adaptive way to mix a token with information from its past. We re-interpret causal self-attention transformers, the backbone of modern foundation models, within a probabilistic framework, much like how classical PCA is extended to probabilistic PCA. However, this re-formulation reveals a surprising and deeper structural insight: due to a change-of-variables phenomenon, a barrier constraint emerges on the self-attention parameters. This induces a highly structured geometry on the token space, providing theoretical insights into the dynamics of LLM decoding. This reveals a boundary where attention becomes ill-conditioned, leading to a margin interpretation similar to classical support vector machines. Just like support vectors, this naturally gives rise to the concept of support tokens.
  Furthermore, we show that LLMs can be interpreted as a stochastic process over the power set of the token space, providing a rigorous probabilistic framework for sequence modeling. We propose a Bayesian framework and derive a MAP estimation objective that requires only a minimal modification to standard LLM training: the addition of a smooth log-barrier penalty to the usual cross-entropy loss. We demonstrate that this provides more robust models without sacrificing out-of-sample accuracy and that it is straightforward to incorporate in practice.

</details>


### [23] [Positional-aware Spatio-Temporal Network for Large-Scale Traffic Prediction](https://arxiv.org/abs/2602.22274)
*Runfei Chen*

Main category: cs.LG

TL;DR: 提出轻量级位置感知时空网络PASTN，通过位置感知嵌入区分节点表示，利用时序注意力模块增强长程感知能力，在不同规模数据集上验证有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有交通流量预测模型在应对大范围地理区域和长时间跨度时，缺乏对节点的清晰区分能力和对历史的整体视角，且模型规模大难以在实际环境中部署。

Method: 提出PASTN模型：1）引入位置感知嵌入来区分每个节点的表示；2）使用时序注意力模块增强模型的长程感知能力；3）采用端到端方式同时捕捉时空复杂性。

Result: 在不同规模数据集（县级、大都市级、州级）上的大量实验验证了PASTN的有效性和效率，进一步分析证明了新引入模块的有效性。

Conclusion: PASTN能够有效捕捉时空复杂性，在保持轻量级的同时提升交通流量预测性能，解决了现有模型在节点区分、长程感知和实际部署方面的不足。

Abstract: Traffic flow forecasting has emerged as an indispensable mission for daily life, which is required to utilize the spatiotemporal relationship between each location within a time period under a graph structure to predict future flow. However, the large travel demand for broader geographical areas and longer time spans requires models to distinguish each node clearly and possess a holistic view of the history, which has been paid less attention to in prior works. Furthermore, increasing sizes of data hinder the deployment of most models in real application environments. To this end, in this paper, we propose a lightweight Positional-aware Spatio-Temporal Network (PASTN) to effectively capture both temporal and spatial complexities in an end-to-end manner. PASTN introduces positional-aware embeddings to separate each node's representation, while also utilizing a temporal attention module to improve the long-range perception of current models. Extensive experiments verify the effectiveness and efficiency of PASTN across datasets of various scales (county, megalopolis and state). Further analysis demonstrates the efficacy of newly introduced modules either.

</details>


### [24] [X-REFINE: XAI-based RElevance input-Filtering and archItecture fiNe-tuning for channel Estimation](https://arxiv.org/abs/2602.22277)
*Abdul Karim Gizzini,Yahia Medjahdi*

Main category: cs.LG

TL;DR: X-REFINE：一个基于XAI的框架，通过联合输入滤波和架构微调，在6G信道估计中实现更好的可解释性-性能-复杂度权衡


<details>
  <summary>Details</summary>
Motivation: 6G无线通信中AI原生架构至关重要，但深度学习模型的黑盒特性及高复杂度限制了其在信道估计等关键应用中的实际部署。现有基于扰动的XAI解决方案仅关注输入滤波，忽略了内部结构优化。

Method: 提出X-REFINE框架，采用基于分解、符号稳定的LRP epsilon规则，通过反向传播预测来获取子载波和隐藏神经元的高分辨率相关性分数，实现联合输入滤波和架构微调的整体优化。

Result: 仿真结果表明，X-REFINE在可解释性-性能-复杂度权衡方面表现优异，显著降低了计算复杂度，同时在不同场景下保持了稳健的误码率性能。

Conclusion: X-REFINE为6G无线通信中的AI模型提供了有效的可解释性解决方案，通过联合优化输入和架构，实现了实际部署所需的性能与效率平衡。

Abstract: AI-native architectures are vital for 6G wireless communications. The black-box nature and high complexity of deep learning models employed in critical applications, such as channel estimation, limit their practical deployment. While perturbation-based XAI solutions offer input filtering, they often neglect internal structural optimization. We propose X-REFINE, an XAI-based framework for joint input-filtering and architecture fine-tuning. By utilizing a decomposition-based, sign-stabilized LRP epsilon rule, X-REFINE backpropagates predictions to derive high-resolution relevance scores for both subcarriers and hidden neurons. This enables a holistic optimization that identifies the most faithful model components. Simulation results demonstrate that X-REFINE achieves a superior interpretability-performance-complexity trade-off, significantly reducing computational complexity while maintaining robust bit error rate (BER) performance across different scenarios.

</details>


### [25] [Integrating Machine Learning Ensembles and Large Language Models for Heart Disease Prediction Using Voting Fusion](https://arxiv.org/abs/2602.22280)
*Md. Tahsin Amin,Tanim Ahmmod,Zannatul Ferdus,Talukder Naemul Hasan Naem,Ehsanul Ferdous,Arpita Bhattacharjee,Ishmam Ahmed Solaiman,Nahiyan Bin Noor*

Main category: cs.LG

TL;DR: 该研究比较了传统机器学习模型与大型语言模型在心血管疾病预测中的表现，发现机器学习集成方法效果最佳（95.78%准确率），而LLMs单独使用时效果一般（78.9%准确率）。通过将ML集成与LLM推理融合的混合方法获得了最佳结果（96.62%准确率）。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，需要早期识别、精确风险分类和可靠的决策支持技术。虽然机器学习算法（特别是集成方法）在建模复杂非线性患者数据方面表现出色，但大型语言模型提供了新的零样本和少样本推理能力，研究者希望探索两者结合是否能提升预测性能。

Method: 使用包含1,190条患者记录的合并数据集，比较传统机器学习模型（Random Forest、XGBoost、LightGBM、CatBoost）与开源大型语言模型（通过OpenRouter API）。最终提出混合融合方法，将ML集成与LLM推理结合，使用Gemini 2.5 Flash模型。

Result: ML集成方法获得最高性能（95.78%准确率，ROC-AUC 0.96）；LLMs在零样本设置下准确率为78.9%，少样本设置下为72.6%；混合融合方法获得最佳结果（96.62%准确率，0.97 AUC）。混合方法在不确定情况下增强了预测能力。

Conclusion: ML集成是结构化表格预测的最佳选择，但可以与混合ML-LLM系统集成以获得小幅提升，为更可靠的临床决策支持工具开辟道路。LLMs与ML模型结合使用时效果最佳，而非单独使用。

Abstract: Cardiovascular disease is the primary cause of death globally, necessitating early identification, precise risk classification, and dependable decision-support technologies. The advent of large language models (LLMs) provides new zero-shot and few-shot reasoning capabilities, even though machine learning (ML) algorithms, especially ensemble approaches like Random Forest, XGBoost, LightGBM, and CatBoost, are excellent at modeling complex, non-linear patient data and routinely beat logistic regression. This research predicts cardiovascular disease using a merged dataset of 1,190 patient records, comparing traditional machine learning models (95.78% accuracy, ROC-AUC 0.96) with open-source large language models via OpenRouter APIs. Finally, a hybrid fusion of the ML ensemble and LLM reasoning under Gemini 2.5 Flash achieved the best results (96.62% accuracy, 0.97 AUC), showing that LLMs (78.9 % accuracy) work best when combined with ML models rather than used alone. Results show that ML ensembles achieved the highest performance (95.78% accuracy, ROC-AUC 0.96), while LLMs performed moderately in zero-shot (78.9%) and slightly better in few-shot (72.6%) settings. The proposed hybrid method enhanced the strength in uncertain situations, illustrating that ensemble ML is considered the best structured tabular prediction case, but it can be integrated with hybrid ML-LLM systems to provide a minor increase and open the way to more reliable clinical decision-support tools.

</details>


### [26] [BrepCoder: A Unified Multimodal Large Language Model for Multi-task B-rep Reasoning](https://arxiv.org/abs/2602.22284)
*Mingi Kim,Yongjun Kim,Jungwoo Kang,Hyungki Kim*

Main category: cs.LG

TL;DR: BrepCoder是一个统一的多模态大语言模型，通过将CAD建模序列转换为Python-like代码并与B-rep对齐，实现从B-rep输入处理多种CAD任务，包括补全、纠错和CAD-QA等。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法大多依赖任务特定模型，需要为新任务进行结构调整，且主要关注点云或图像而非行业标准的B-rep格式。需要一种能够处理多种CAD任务的统一模型。

Method: 利用LLM的代码生成能力，将CAD建模序列转换为Python-like代码并与B-rep对齐。采用两阶段训练策略：1) 逆向工程预训练学习几何特征和设计逻辑；2) 扩展到下游任务如补全、纠错和CAD-QA。

Result: 通过将B-rep解释为结构化代码，BrepCoder在多种任务上展现出卓越的泛化能力，证明了其作为通用CAD代理的潜力。

Conclusion: BrepCoder通过统一的MLLM框架，成功解决了现有CAD深度学习方法在任务通用性和B-rep格式支持方面的限制，为CAD领域的通用智能代理提供了可行方案。

Abstract: Recent advancements in deep learning have actively addressed complex challenges within the Computer-Aided Design (CAD) domain.However, most existing approaches rely on task-specifi c models requiring structural modifi cations for new tasks, and they predominantly focus on point clouds or images rather than the industry-standard Boundary Representation (B-rep) format. To address these limitations, we propose BrepCoder, a unifi ed Multimodal Large Language Model (MLLM) that performs diverse CAD tasks from B-rep inputs. By leveraging the code generation capabilities of Large Language Models (LLMs), we convert CAD modeling sequences into Python-like code and align them with B-rep. We then adopt a two-stage training strategy: First, pre-training on reverse engineering to learn geometric features and design logic. Second, eff ectively extending the model to various downstream tasks such as completion, error correction, and CAD-QA. Consequently, by interpreting B-rep as structural code, BrepCoder achieves superior generalization across diverse tasks, demonstrating its potential as a general-purpose CAD agent.

</details>


### [27] [Early Risk Stratification of Dosing Errors in Clinical Trials Using Machine Learning](https://arxiv.org/abs/2602.22285)
*Félicien Hêche,Sohrab Ferdowsi,Anthony Yazdani,Sara Sansaloni-Pastor,Douglas Teodoro*

Main category: cs.LG

TL;DR: 开发基于机器学习的框架，利用试验启动前信息对临床试验进行早期风险分层，预测高剂量错误率风险


<details>
  <summary>Details</summary>
Motivation: 临床试验中剂量错误是常见问题，可能导致患者安全风险和试验失败。目前缺乏在试验启动前预测剂量错误风险的方法，需要开发早期风险分层工具支持主动的质量管理

Method: 从ClinicalTrials.gov构建包含42,112个试验的数据集，提取结构化、半结构化数据和自由文本数据。使用不良事件报告、MedDRA术语和Wilson置信区间为试验分配二元标签。评估三种模型：基于结构化特征的XGBoost、基于文本数据的ClinicalModernBERT、以及结合两种模态的简单后期融合模型。应用后验概率校准实现可解释的风险分层

Result: 后期融合模型获得最高的AUC-ROC（0.862）。校准后的输出能够将试验稳健地分层到预定义的风险类别中。高剂量错误率试验的比例在预测风险较高的组中单调增加，并与相应的预测概率范围一致

Conclusion: 研究提出了一个可重复、可扩展的机器学习框架，用于对高风险剂量错误率的临床试验进行早期试验级风险分层，支持临床研究中的主动、基于风险的质量管理

Abstract: Objective: The objective of this study is to develop a machine learning (ML)-based framework for early risk stratification of clinical trials (CTs) according to their likelihood of exhibiting a high rate of dosing errors, using information available prior to trial initiation. Materials and Methods: We constructed a dataset from ClinicalTrials.gov comprising 42,112 CTs. Structured, semi-structured trial data, and unstructured protocol-related free-text data were extracted. CTs were assigned binary labels indicating elevated dosing error rate, derived from adverse event reports, MedDRA terminology, and Wilson confidence intervals. We evaluated an XGBoost model trained on structured features, a ClinicalModernBERT model using textual data, and a simple late-fusion model combining both modalities. Post-hoc probability calibration was applied to enable interpretable, trial-level risk stratification. Results: The late-fusion model achieved the highest AUC-ROC (0.862). Beyond discrimination, calibrated outputs enabled robust stratification of CTs into predefined risk categories. The proportion of trials labeled as having an excessively high dosing error rate increased monotonically across higher predicted risk groups and aligned with the corresponding predicted probability ranges. Discussion: These findings indicate that dosing error risk can be anticipated at the trial level using pre-initiation information. Probability calibration was essential for translating model outputs into reliable and interpretable risk categories, while simple multimodal integration yielded performance gains without requiring complex architectures. Conclusion: This study introduces a reproducible and scalable ML framework for early, trial-level risk stratification of CTs at risk of high dosing error rates, supporting proactive, risk-based quality management in clinical research.

</details>


### [28] [OmniZip: Learning a Unified and Lightweight Lossless Compressor for Multi-Modal Data](https://arxiv.org/abs/2602.22286)
*Yan Zhao,Zhengxue Cheng,Junxuan Zhang,Dajiang Zhou,Qunshan Gu,Qi Wang,Li Song*

Main category: cs.LG

TL;DR: OmniZip是一个轻量级的多模态无损压缩器，支持图像、文本、语音、触觉、数据库和基因序列等多种数据类型，在压缩效率和推理速度方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的无损压缩器大多针对单一模态设计，在多模态场景下需要部署多个压缩器，导致冗余。多模态大语言模型虽然提供了解决方案，但过于复杂难以实用。因此需要设计一个统一且轻量级的多模态压缩器。

Method: OmniZip基于轻量级骨干网络，包含三个关键组件：1) 模态统一的分词器，将不同数据可逆地转换为token；2) 模态路由上下文学习机制，实现灵活的多模态上下文建模；3) 模态路由前馈设计，增强模型的非线性表示能力。采用重参数化训练策略提升模型容量。

Result: 在多个模态数据集上优于或匹配现有最优压缩器，相比gzip在CLIC-M、TouchandGo、enwik9、LibriSpeech和WikiSQL数据集上分别提升42%、57%、62%和42%、53%的压缩效率。在资源受限的边缘设备上支持近实时推理，在MacBook CPU和iPhone NPU上达到约1MB/s的速度。

Conclusion: OmniZip成功实现了统一且轻量级的多模态无损压缩，在保持高性能的同时支持边缘设备部署，为多模态数据压缩提供了实用解决方案。

Abstract: Lossless compression is essential for efficient data storage and transmission. Although learning-based lossless compressors achieve strong results, most of them are designed for a single modality, leading to redundant compressor deployments in multi-modal settings. Designing a unified multi-modal compressor is critical yet challenging, as different data types vary largely in format, dimension, and statistics. Multi-modal large language models offer a promising resolution but remain too complex for practical use. Thus, we propose \textbf{OmniZip}, \textbf{a unified and lightweight lossless compressor for multi-modal data (like image, text, speech, tactile, database, and gene sequence)}. Built on a lightweight backbone, OmniZip incorporates three key components to enable efficient multi-modal lossless compression: a modality-unified tokenizer that reversibly transforms diverse data into tokens, a modality-routing context learning mechanism that enables flexible multi-modal context modeling, and a modality-routing feedforward design that further enhances the model's nonlinear representation flexibility. A reparameterization training strategy is used to enhance model capacity. OmniZip outperforms or matches other state-of-the-art compressors on multiple modalities, achieving 42\%, 57\%, 62\% and 42\%, 53\% higher compression efficiency than gzip on CLIC-M, TouchandGo, enwik9, LibriSpeech, and WikiSQL datasets, respectively. It also supports near real-time inference on resource-constrained edge devices, reaching about 1MB/s on MacBook CPUs and iPhone NPUs. Our code is released at https://github.com/adminasmi/OmniZip-CVPR2026.

</details>


### [29] [Reliable XAI Explanations in Sudden Cardiac Death Prediction for Chagas Cardiomyopathy](https://arxiv.org/abs/2602.22288)
*Vinícius P. Chagas,Luiz H. T. Viana,Mac M. da S. Carlos,João P. V. Madeiro,Roberto C. Pedrosa,Thiago Alves Rocha,Carlos H. L. Cavalcante*

Main category: cs.LG

TL;DR: 该论文提出一种基于逻辑的可解释性方法，用于预测恰加斯心肌病患者的猝死风险，该方法具有正确性保证，相比启发式方法具有更好的透明度和临床可信度。


<details>
  <summary>Details</summary>
Motivation: 恰加斯心肌病患者的猝死预测具有挑战性，现有AI模型缺乏透明度，被视为"黑箱"，且启发式解释方法缺乏正确性保证，导致决策过程可能出现错误。

Method: 应用基于逻辑的可解释性方法，该方法具有正确性保证，应用于准确率和召回率超过95%的AI分类器，确保解释的保真度。

Result: 该方法展示了强大的预测性能，达到100%的解释保真度，相比最先进的启发式方法表现出更优的一致性和鲁棒性。

Conclusion: 基于逻辑的可解释性方法增强了临床信任，促进了AI驱动工具在实践中的整合，特别是在最需要的流行地区实现大规模部署。

Abstract: Sudden cardiac death (SCD) is unpredictable, and its prediction in Chagas cardiomyopathy (CC) remains a significant challenge, especially in patients not classified as high risk. While AI and machine learning models improve risk stratification, their adoption is hindered by a lack of transparency, as they are often perceived as \textit{black boxes} with unclear decision-making processes. Some approaches apply heuristic explanations without correctness guarantees, leading to mistakes in the decision-making process. To address this, we apply a logic-based explainability method with correctness guarantees to the problem of SCD prediction in CC. This explainability method, applied to an AI classifier with over 95\% accuracy and recall, demonstrated strong predictive performance and 100\% explanation fidelity. When compared to state-of-the-art heuristic methods, it showed superior consistency and robustness. This approach enhances clinical trust, facilitates the integration of AI-driven tools into practice, and promotes large-scale deployment, particularly in endemic regions where it is most needed.

</details>


### [30] [Manifold of Failure: Behavioral Attraction Basins in Language Models](https://arxiv.org/abs/2602.22291)
*Sarthak Munshi,Manish Bhatt,Vineeth Sai Narajala,Idan Habler,AmmarnAl-Kahfah,Ken Huang,Blake Gatto*

Main category: cs.LG

TL;DR: 本文提出了一种系统映射大语言模型失败流形的方法，将漏洞搜索重构为质量多样性问题，使用MAP-Elites算法揭示失败区域的连续拓扑结构，生成可解释的全局安全地图。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注将对抗样本投影回自然数据流形以恢复安全性，但作者认为全面理解AI安全需要表征不安全区域本身。需要从寻找离散失败转向理解其底层结构。

Method: 将漏洞搜索重构为质量多样性问题，使用MAP-Elites算法来揭示失败区域的连续拓扑结构（称为行为吸引盆地）。使用对齐偏差作为质量指标，引导搜索模型行为与预期对齐差异最大的区域。

Result: 在三个LLM（Llama-3-8B、GPT-OSS-20B、GPT-5-Mini）上，MAP-Elites实现了高达63%的行为覆盖率，发现了多达370个不同的漏洞生态位，并揭示了显著不同的模型特定拓扑特征：Llama-3-8B表现出近乎普遍的脆弱性平台，GPT-OSS-20B显示碎片化景观，GPT-5-Mini表现出强大的鲁棒性。

Conclusion: 该方法生成了现有攻击方法无法提供的可解释全局安全地图，将范式从寻找离散失败转向理解其底层结构，为系统评估和改善LLM安全性提供了新框架。

Abstract: While prior work has focused on projecting adversarial examples back onto the manifold of natural data to restore safety, we argue that a comprehensive understanding of AI safety requires characterizing the unsafe regions themselves. This paper introduces a framework for systematically mapping the Manifold of Failure in Large Language Models (LLMs). We reframe the search for vulnerabilities as a quality diversity problem, using MAP-Elites to illuminate the continuous topology of these failure regions, which we term behavioral attraction basins. Our quality metric, Alignment Deviation, guides the search towards areas where the model's behavior diverges most from its intended alignment. Across three LLMs: Llama-3-8B, GPT-OSS-20B, and GPT-5-Mini, we show that MAP-Elites achieves up to 63% behavioral coverage, discovers up to 370 distinct vulnerability niches, and reveals dramatically different model-specific topological signatures: Llama-3-8B exhibits a near-universal vulnerability plateau (mean Alignment Deviation 0.93), GPT-OSS-20B shows a fragmented landscape with spatially concentrated basins (mean 0.73), and GPT-5-Mini demonstrates strong robustness with a ceiling at 0.50. Our approach produces interpretable, global maps of each model's safety landscape that no existing attack method (GCG, PAIR, or TAP) can provide, shifting the paradigm from finding discrete failures to understanding their underlying structure.

</details>


### [31] [Global River Forecasting with a Topology-Informed AI Foundation Model](https://arxiv.org/abs/2602.22293)
*Hancheng Ren,Gang Zhao,Shuo Wang,Louise Slater,Dai Yamazaki,Shu Liu,Jingfang Fan,Shibo Cui,Ziming Yu,Shengyu Kang,Depeng Zuo,Dingzhi Peng,Zongxue Xu,Bo Pang*

Main category: cs.LG

TL;DR: GraphRiverCast (GRC) 是一个基于拓扑信息的AI基础模型，能够在全球河流系统中进行多变量水动力模拟，支持"冷启动"模式（无需历史河流状态初始化），在7天全球伪后报中NSE达到0.82，优于物理模型和本地训练的AI基准。


<details>
  <summary>Details</summary>
Motivation: 河流系统本质上是相互连接的连续网络，但广泛的水文数据稀缺限制了数据驱动预测只能进行孤立预测。需要实现系统性模拟并减少对河流观测的依赖。

Method: 提出GraphRiverCast (GRC)，一个拓扑信息化的AI基础模型，采用物理对齐的神经算子架构，支持"冷启动"模式，通过拓扑编码明确指导水力连通性和网络尺度质量再分配，采用预训练和微调策略进行本地适应。

Result: 在7天全球伪后报中，GRC-ColdStart作为独立的模拟器，NSE达到约0.82，没有表现出自回归范式典型的显著误差累积。消融研究表明拓扑编码在缺乏历史状态时是不可或缺的结构信息。本地适应后，GRC在从测量河段到完整河流网络的范围内持续优于物理模型和本地训练的AI基准。

Conclusion: GRC通过拓扑编码和基于物理的预训练，实现了快速和跨尺度自适应模拟，建立了将全球水动力知识与本地水文现实连接起来的协作范式，强调了拓扑编码和基于物理的预训练的必要性。

Abstract: River systems operate as inherently interconnected continuous networks, meaning river hydrodynamic simulation ought to be a systemic process. However, widespread hydrology data scarcity often restricts data-driven forecasting to isolated predictions. To achieve systemic simulation and reduce reliance on river observations, we present GraphRiverCast (GRC), a topology-informed AI foundation model designed to simulate multivariate river hydrodynamics in global river systems. GRC is capable of operating in a "ColdStart" mode, generating predictions without relying on historical river states for initialization. In 7-day global pseudo-hindcasts, GRC-ColdStart functions as a robust standalone simulator, achieving a Nash-Sutcliffe Efficiency (NSE) of approximately 0.82 without exhibiting the significant error accumulation typical of autoregressive paradigms. Ablation studies reveal that topological encoding serves as indispensable structural information in the absence of historical states, explicitly guiding hydraulic connectivity and network-scale mass redistribution to reconstruct flow dynamics. Furthermore, when adapted locally via a pre-training and fine-tuning strategy, GRC consistently outperforms physics-based and locally-trained AI baselines. Crucially, this superiority extends from gauged reaches to full river networks, underscoring the necessity of topology encoding and physics-based pre-training. Built on a physics-aligned neural operator architecture, GRC enables rapid and cross-scale adaptive simulation, establishing a collaborative paradigm bridging global hydrodynamic knowledge with local hydrological reality.

</details>


### [32] [When Should a Model Change Its Mind? An Energy-Based Theory and Regularizer for Concept Drift in Electrocardiogram (ECG) Signals](https://arxiv.org/abs/2602.22294)
*Timothy Oladunni,Blessing Ojeme,Kyndal Maclin,Clyde Baidoo*

Main category: cs.LG

TL;DR: 提出PECT能量守恒理论框架，通过ECRL正则化约束动态生理信号中的表示漂移，在保持干净数据准确率的同时显著提升扰动数据性能


<details>
  <summary>Details</summary>
Motivation: 现有概念漂移框架主要关注分布变化，无法区分生理信号中良性的能量波动和真正的概念变化，导致深度学习模型对幅度、速率或形态的无害变化过度敏感，产生不稳定预测

Method: 提出生理能量守恒理论(PECT)，认为在虚拟漂移下，归一化潜在位移应与归一化信号能量变化成比例；通过能量约束表示学习(ECRL)实现该原则，这是一种轻量级正则化器，惩罚能量不一致的潜在移动

Result: 在七种单模态和混合模型上验证，最强三模态混合模型(1D+2D+Transformer)中：干净准确率基本保持(96.0%到94.1%)，扰动准确率显著提升(72.6%到85.5%)，融合表示漂移减少超过45%

Conclusion: PECT作为能量漂移定律，有效管理连续生理信号中的概念稳定性，ECRL正则化在不改变编码器架构或增加推理成本的情况下，显著提升模型对生理合理波动的鲁棒性

Abstract: Models operating on dynamic physiologic signals must distinguish benign, label-preserving variability from true concept change. Existing concept-drift frameworks are largely distributional and provide no principled guidance on how much a model's internal representation may move when the underlying signal undergoes physiologically plausible fluctuations in energy. As a result, deep models often misinterpret harmless changes in amplitude, rate, or morphology as concept drift, yielding unstable predictions, particularly in multimodal fusion settings.
  This study introduces Physiologic Energy Conservation Theory (PECT), an energy-based framework for concept stability in dynamic signals. PECT posits that under virtual drift, normalized latent displacement should scale proportionally with normalized signal energy change, while persistent violations of this proportionality indicate real concept drift. We operationalize this principle through Energy-Constrained Representation Learning (ECRL), a lightweight regularizer that penalizes energy-inconsistent latent movement without modifying encoder architectures or adding inference-time cost.
  Although PECT is formulated for dynamic signals in general, we instantiate and evaluate it on multimodal ECG across seven unimodal and hybrid models. Experiments show that in the strongest trimodal hybrid (1D+2D+Transformer), clean accuracy is largely preserved (96.0% to 94.1%), while perturbed accuracy improves substantially (72.6% to 85.5%) and fused representation drift decreases by over 45%. Similar trends are observed across all architectures, providing empirical evidence that PECT functions as an energy-drift law governing concept stability in continuous physiologic signals.

</details>


### [33] [UpSkill: Mutual Information Skill Learning for Structured Response Diversity in LLMs](https://arxiv.org/abs/2602.22296)
*Devan Shah,Owen Yang,Daniel Yang,Chongyi Zheng,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: UpSkill方法通过互信息奖励优化LLM的多尝试正确率(pass@k)，在保持单次正确率的同时提升响应多样性，在GSM8K上对多个开源模型取得约3%的pass@k提升。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法优化单次尝试准确率时，会抑制多次尝试中的响应多样性，限制了探索空间并忽视了未被充分代表的策略。

Method: 提出UpSkill训练方法，将互信息技能学习(MISL)适配到LLM中，优化pass@k正确率。在GRPO框架内实现token级互信息奖励，鼓励轨迹对特定技能z的特异性。

Result: 在GSM8K数据集上对Llama 3.1-8B、Qwen 2.5-7B和R1-Distilled-Qwen2.5-Math-1.5B三个模型进行实验，UpSkill在较强的基础模型上改善了多尝试指标，为Qwen和Llama带来约3%的pass@k平均提升，且不降低pass@1性能。

Conclusion: UpSkill通过互信息目标有效提升LLM的多尝试推理能力，实验和理论证据表明pass@k的改进与互信息目标密切相关，为优化多样化推理策略提供了新方法。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has improved the reasoning abilities of large language models (LLMs) on mathematics and programming tasks, but standard approaches that optimize single-attempt accuracy can inadvertently suppress response diversity across repeated attempts, narrowing exploration and overlooking underrepresented strategies. We introduce UpSkill, a training time method that adapts Mutual Information Skill Learning (MISL) to LLMs for optimizing pass@k correctness. We propose a novel reward that we implement within Group Relative Policy Optimization (GRPO): a token-level mutual information (MI) reward that encourages trajectory specificity to z. Experiments on GSM8K with three open-weight models, Llama 3.1-8B, Qwen 2.5-7B, and R1-Distilled-Qwen2.5-Math-1.5B, show that UpSkill improves multi-attempt metrics on the stronger base models, yielding mean gains of ~3% in pass@k for both Qwen and Llama without degrading pass@1. Additionally, we find both empirical and theoretical evidence that improvements in pass@k are closely tied to the mutual information objective.

</details>


### [34] [Learning Rewards, Not Labels: Adversarial Inverse Reinforcement Learning for Machinery Fault Detection](https://arxiv.org/abs/2602.22297)
*Dhiraj Neupane,Richard Dazeley,Mohamed Reda Bouadjenek,Sunil Aryal*

Main category: cs.LG

TL;DR: 该论文提出了一种基于离线逆强化学习的机械故障检测方法，通过从健康操作序列学习奖励动态，避免了手动奖励设计和故障标签需求，实现了早期鲁棒的故障检测。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的机械故障检测方法未能充分利用RL的序列决策优势，通常将故障检测简化为上下文赌博机问题。需要将RL的序列推理与故障检测的时间结构对齐，以发挥RL在数据驱动工业环境中的诊断潜力。

Method: 将机械故障检测建模为离线逆强化学习问题，使用对抗性逆强化学习训练一个判别器来区分正常（专家）样本和策略生成的转移。判别器学习的奖励作为异常分数，用于检测偏离正常操作行为的故障。

Result: 在三个运行至故障基准数据集（HUMS2023、IMS和XJTU-SY）上的评估显示，模型能一致地为正常样本分配低异常分数，为故障样本分配高异常分数，实现了早期且鲁棒的故障检测。

Conclusion: 通过将强化学习的序列推理与机械故障检测的时间结构对齐，这项工作为数据驱动工业环境中基于强化学习的诊断开辟了新路径。

Abstract: Reinforcement learning (RL) offers significant promise for machinery fault detection (MFD). However, most existing RL-based MFD approaches do not fully exploit RL's sequential decision-making strengths, often treating MFD as a simple guessing game (Contextual Bandits). To bridge this gap, we formulate MFD as an offline inverse reinforcement learning problem, where the agent learns the reward dynamics directly from healthy operational sequences, thereby bypassing the need for manual reward engineering and fault labels. Our framework employs Adversarial Inverse Reinforcement Learning to train a discriminator that distinguishes between normal (expert) and policy-generated transitions. The discriminator's learned reward serves as an anomaly score, indicating deviations from normal operating behaviour. When evaluated on three run-to-failure benchmark datasets (HUMS2023, IMS, and XJTU-SY), the model consistently assigns low anomaly scores to normal samples and high scores to faulty ones, enabling early and robust fault detection. By aligning RL's sequential reasoning with MFD's temporal structure, this work opens a path toward RL-based diagnostics in data-driven industrial settings.

</details>


### [35] [AviaSafe: A Physics-Informed Data-Driven Model for Aviation Safety-Critical Cloud Forecasts](https://arxiv.org/abs/2602.22298)
*Zijian Zhu,Qiusheng Huang,Anboyu Guo,Xiaohui Zhong,Hao Li*

Main category: cs.LG

TL;DR: 提出AviaSafe模型，用于预测四种云微物理物种，解决航空安全中的云冰风险问题，相比现有模型在7天预报中表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前AI天气预测模型只能预测常规大气变量，无法区分对航空安全至关重要的云微物理物种（如冰晶、过冷水滴等），这限制了航空路线优化和发动机结冰风险评估。

Method: 采用分层物理信息神经网络预测器，首先通过掩码注意力预测云空间分布，然后在识别区域内量化物种浓度。集成航空气象中的结冰条件指数作为物理约束，识别过冷水促进冰晶爆炸性生长的区域。

Result: 在ERA5再分析数据上训练，相比基线模型在云物种预测上获得更低的RMSE，在7天预报中某些关键变量上优于业务数值模型。

Conclusion: AviaSafe能够预测单个云物种，为航空路线优化提供新应用，通过区分冰和液态水来评估发动机结冰风险，提高了航空安全预测能力。

Abstract: Current AI weather forecasting models predict conventional atmospheric variables but cannot distinguish between cloud microphysical species critical for aviation safety. We introduce AviaSafe, a hierarchical, physics-informed neural forecaster that produces global, six-hourly predictions of these four hydrometeor species for lead times up to 7 days. Our approach addresses the unique challenges of cloud prediction: extreme sparsity, discontinuous distributions, and complex microphysical interactions between species. We integrate the Icing Condition (IC) index from aviation meteorology as a physics-based constraint that identifies regions where supercooled water fuels explosive ice crystal growth. The model employs a hierarchical architecture that first predicts cloud spatial distribution through masked attention, then quantifies species concentrations within identified regions. Training on ERA5 reanalysis data, our model achieves lower RMSE for cloud species compared to baseline and outperforms operational numerical models on certain key variables at 7-day lead times. The ability to forecast individual cloud species enables new applications in aviation route optimization where distinguishing between ice and liquid water determines engine icing risk.

</details>


### [36] [Training Agents to Self-Report Misbehavior](https://arxiv.org/abs/2602.22303)
*Bruce W. Lee,Chen Yueh-Han,Tomek Korbak*

Main category: cs.LG

TL;DR: 论文提出"自我告发训练"方法，训练AI代理在暗中违规时主动发出可见信号，相比传统对齐训练能更有效减少未检测到的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 前沿AI代理可能追求隐藏目标并逃避监督。传统对齐训练旨在通过强化正确目标来防止这种行为，但可能无法完全成功并产生副作用。需要一种新方法来应对代理的欺骗性行为。

Method: 提出自我告发训练，训练GPT-4.1和Gemini-2.0代理在行为欺骗时调用report_scheming()工具。在分布外环境中测量其造成伤害且未被检测到的能力。

Result: 自我告发显著降低了未检测到的成功攻击率，优于匹配能力的监控器和对齐基线，同时保持了指令层次结构，对通用能力影响最小。与黑盒监控不同，其性能在不同任务中保持一致。

Conclusion: 自我告发训练为减少前沿AI不对齐风险提供了可行路径，既不假设不当行为可以被预防，也不假设可以从外部可靠分类。训练后的行为在对抗性提示优化下持续存在，并能推广到代理自主追求不对齐目标的情况。

Abstract: Frontier AI agents may pursue hidden goals while concealing their pursuit from oversight. Alignment training aims to prevent such behavior by reinforcing the correct goals, but alignment may not always succeed and can lead to unwanted side effects. We propose self-incrimination training, which instead trains agents to produce a visible signal when they covertly misbehave. We train GPT-4.1 and Gemini-2.0 agents to call a report_scheming() tool when behaving deceptively and measure their ability to cause harm undetected in out-of-distribution environments. Self-incrimination significantly reduces the undetected successful attack rate, outperforming matched-capability monitors and alignment baselines while preserving instruction hierarchy and incurring minimal safety tax on general capabilities. Unlike blackbox monitoring, self-incrimination performance is consistent across tasks regardless of how suspicious the misbehavior appears externally. The trained behavior persists under adversarial prompt optimization and generalizes to settings where agents pursue misaligned goals themselves rather than being instructed to misbehave. Our results suggest self-incrimination offers a viable path for reducing frontier misalignment risk, one that neither assumes misbehavior can be prevented nor that it can be reliably classified from the outside.

</details>


### [37] [A 1/R Law for Kurtosis Contrast in Balanced Mixtures](https://arxiv.org/abs/2602.22334)
*Yuda Bi,Wenjun Xiao,Linhao Bai,Vince D Calhoun*

Main category: cs.LG

TL;DR: 论文证明在宽平衡混合中，基于峰度的ICA方法会失效，提出了一个尖锐的冗余定律：峰度随有效宽度衰减，并展示了通过"纯化"选择少量符号一致源可恢复对比度。


<details>
  <summary>Details</summary>
Motivation: 传统的基于峰度的独立成分分析（ICA）在宽平衡混合场景中表现不佳，需要理解其失效原因并找到解决方案。

Method: 1. 理论证明：推导出峰度随有效宽度衰减的冗余定律；2. 提出"纯化"方法：选择少量符号一致的源来恢复对比度；3. 使用数据驱动启发式方法实现纯化。

Result: 1. 证明了峰度衰减为O(κ_max/R_eff)；2. 在标准有限矩条件下，超越O(1/√T)估计尺度需要R≲κ_max√T；3. 纯化方法可恢复R无关的对比度Ω(1/m)；4. 合成实验验证了理论预测。

Conclusion: 基于峰度的ICA在宽平衡混合中存在根本性限制，但通过纯化方法选择少量符号一致源可以恢复对比度，为实际应用提供了理论指导和实用解决方案。

Abstract: Kurtosis-based Independent Component Analysis (ICA) weakens in wide, balanced mixtures. We prove a sharp redundancy law: for a standardized projection with effective width $R_{\mathrm{eff}}$ (participation ratio), the population excess kurtosis obeys $|κ(y)|=O(κ_{\max}/R_{\mathrm{eff}})$, yielding the order-tight $O(c_bκ_{\max}/R)$ under balance (typically $c_b=O(\log R)$). As an impossibility screen, under standard finite-moment conditions for sample kurtosis estimation, surpassing the $O(1/\sqrt{T})$ estimation scale requires $R\lesssim κ_{\max}\sqrt{T}$. We also show that \emph{purification} -- selecting $m\!\ll\!R$ sign-consistent sources -- restores $R$-independent contrast $Ω(1/m)$, with a simple data-driven heuristic. Synthetic experiments validate the predicted decay, the $\sqrt{T}$ crossover, and contrast recovery.

</details>


### [38] [Structure and Redundancy in Large Language Models: A Spectral Study via Random Matrix Theory](https://arxiv.org/abs/2602.22345)
*Davide Ettori*

Main category: cs.LG

TL;DR: 该论文提出基于谱几何和随机矩阵理论的统一框架，解决深度学习中的可靠性和效率问题，包括用于检测幻觉的EigenTrack方法和用于模型压缩的RMT-KD方法。


<details>
  <summary>Details</summary>
Motivation: 随着深度网络和大语言模型的规模扩大，其内部行为变得不透明，导致幻觉、分布偏移下的脆弱泛化以及计算和能耗需求增长。需要解决可靠性和效率这两个紧密相关的挑战。

Method: 通过分析隐藏激活在层和输入上的特征值动态，利用谱统计作为模型行为的紧凑、稳定、可解释的视角。提出两个具体方法：1) EigenTrack：将流式激活转换为谱描述符，使用轻量级循环分类器建模其时间演化，实现幻觉和分布外行为的实时检测；2) RMT-KD：将激活谱中的异常特征值解释为任务相关信息载体，通过迭代自蒸馏将网络投影到低维子空间，实现模型压缩。

Result: 谱统计能够分离结构化因果表示和噪声主导的变异性。EigenTrack能够在模型输出出现可靠性故障前实现早期检测，并提供可解释的表示动态洞察。RMT-KD能够生成显著更紧凑、节能的模型，同时保持准确性和硬件友好的密集结构。

Conclusion: 基于谱几何和随机矩阵理论的统一框架为深度学习中的可靠性和效率问题提供了有效的解决方案，通过谱分析实现了模型行为的可解释性监控和高效压缩。

Abstract: This thesis addresses two persistent and closely related challenges in modern deep learning, reliability and efficiency, through a unified framework grounded in Spectral Geometry and Random Matrix Theory (RMT). As deep networks and large language models continue to scale, their internal behavior becomes increasingly opaque, leading to hallucinations, fragile generalization under distribution shift, and growing computational and energy demands. By analyzing the eigenvalue dynamics of hidden activations across layers and inputs, this work shows that spectral statistics provide a compact, stable, and interpretable lens on model behavior, capable of separating structured, causal representations from noise-dominated variability. Within this framework, the first contribution, EigenTrack, introduces a real-time method for detecting hallucinations and out-of-distribution behavior in large language and vision-language models. EigenTrack transforms streaming activations into spectral descriptors such as entropy, variance, and deviations from the Marchenko-Pastur baseline, and models their temporal evolution using lightweight recurrent classifiers, enabling early detection of reliability failures before they appear in model outputs while offering interpretable insight into representation dynamics. The second contribution, RMT-KD, presents a principled approach to compressing deep networks via random matrix theoretic knowledge distillation. By interpreting outlier eigenvalues in activation spectra as carriers of task-relevant information, RMT-KD progressively projects networks onto lower-dimensional subspaces through iterative self-distillation, yielding significantly more compact and energy-efficient models while preserving accuracy and dense, hardware-friendly structure.

</details>


### [39] [Learning geometry-dependent lead-field operators for forward ECG modeling](https://arxiv.org/abs/2602.22367)
*Arsenii Dokuchaev,Francesca Bonizzoni,Stefano Pagani,Francesco Regazzoni,Simone Pezzuto*

Main category: cs.LG

TL;DR: 提出一种基于形状感知的替代模型，用于前向心电图模拟中的导联场算子，实现高解剖保真度、低数据需求和计算效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 传统前向ECG计算模型需要精确的躯干域表示，但临床实践中获取完整躯干图像困难，且导联场方法计算成本随电极数量线性增长，限制了在高密度记录场景的应用。现有方法无法同时实现高解剖保真度、低数据需求和计算效率。

Method: 提出形状感知的导联场算子替代模型，包含两个组件：1) 几何编码模块，将解剖形状映射到低维潜在空间；2) 几何条件神经替代模型，根据空间坐标、电极位置和潜在编码预测导联场梯度。

Result: 在躯干内部实现高精度的导联场近似（平均角度误差5°），在心脏内部也表现良好，获得高精度ECG模拟（相对均方误差<2.5%）。替代模型持续优于广泛使用的伪导联场近似，同时保持可忽略的推理成本。

Conclusion: 该方法通过紧凑的潜在表示，无需完整详细的躯干分割，可在数据有限的环境中部署，同时保持高保真度的ECG模拟，解决了临床实践中解剖保真度、数据需求和计算效率之间的平衡问题。

Abstract: Modern forward electrocardiogram (ECG) computational models rely on an accurate representation of the torso domain. The lead-field method enables fast ECG simulations while preserving full geometric fidelity. Achieving high anatomical accuracy in torso representation is, however, challenging in clinical practice, as imaging protocols are typically focused on the heart and often do not include the entire torso. In addition, the computational cost of the lead-field method scales linearly with the number of electrodes, limiting its applicability in high-density recording settings. To date, no existing approach simultaneously achieves high anatomical fidelity, low data requirements and computational efficiency. In this work, we propose a shape-informed surrogate model of the lead-field operator that serves as a drop-in replacement for the full-order model in forward ECG simulations. The proposed framework consists of two components: a geometry-encoding module that maps anatomical shapes into a low-dimensional latent space, and a geometry-conditioned neural surrogate that predicts lead-field gradients from spatial coordinates, electrode positions and latent codes. The proposed method achieves high accuracy in approximating lead fields both within the torso (mean angular error 5°) and inside the heart, resulting in highly accurate ECG simulations (relative mean squared error <2.5%. The surrogate consistently outperforms the widely used pseudo lead-field approximation while preserving negligible inference cost. Owing to its compact latent representation, the method does not require a fully detailed torso segmentation and can therefore be deployed in data-limited settings while preserving high-fidelity ECG simulations.

</details>


### [40] [Disentangling Shared and Target-Enriched Topics via Background-Contrastive Non-negative Matrix Factorization](https://arxiv.org/abs/2602.22387)
*Yixuan Li,Archer Y. Yang,Yue Li*

Main category: cs.LG

TL;DR: 背景对比非负矩阵分解（BCNMF）是一种新方法，通过联合分解目标数据集和匹配背景数据，使用共享非负基，在对比目标下抑制背景表达的结构，从而提取目标富集的潜在主题。


<details>
  <summary>Details</summary>
Motivation: 高维数据中感兴趣的生物信号常被跨条件共享的主导变异所掩盖，这些变异来自基线生物结构或技术效应，会阻碍标准降维方法解析条件特异性结构。现有背景校正方法要么无法扩展到高维度，要么不可解释。

Method: 提出背景对比非负矩阵分解（BCNMF），通过联合分解目标数据集和匹配背景数据，使用共享非负基，在对比目标下抑制背景表达的结构。采用高效的乘法更新算法，支持GPU硬件加速，可通过小批量训练扩展到大数据。

Result: 在模拟和多种生物数据集中，BCNMF揭示了传统方法掩盖的信号，包括抑郁症死后脑单细胞RNA-seq中的疾病相关程序、小鼠中基因型相关的蛋白表达模式、白血病中治疗特异性转录变化，以及癌细胞系中TP53依赖的药物反应。

Conclusion: BCNMF提供了一种可扩展、可解释的方法来提取目标特异性变异，解决了高维数据中背景变异掩盖生物信号的问题，在多种生物应用场景中表现出优越性能。

Abstract: Biological signals of interest in high-dimensional data are often masked by dominant variation shared across conditions. This variation, arising from baseline biological structure or technical effects, can prevent standard dimensionality reduction methods from resolving condition-specific structure. The challenge is that these confounding topics are often unknown and mixed with biological signals. Existing background correction methods are either unscalable to high dimensions or not interpretable. We introduce background contrastive Non-negative Matrix Factorization (\model), which extracts target-enriched latent topics by jointly factorizing a target dataset and a matched background using shared non-negative bases under a contrastive objective that suppresses background-expressed structure. This approach yields non-negative components that are directly interpretable at the feature level, and explicitly isolates target-specific variation. \model is learned by an efficient multiplicative update algorithm via matrix multiplication such that it is highly efficient on GPU hardware and scalable to big data via minibatch training akin to deep learning approach. Across simulations and diverse biological datasets, \model reveals signals obscured by conventional methods, including disease-associated programs in postmortem depressive brain single-cell RNA-seq, genotype-linked protein expression patterns in mice, treatment-specific transcriptional changes in leukemia, and TP53-dependent drug responses in cancer cell lines.

</details>


### [41] [Predicting Multi-Drug Resistance in Bacterial Isolates Through Performance Comparison and LIME-based Interpretation of Classification Models](https://arxiv.org/abs/2602.22400)
*Santanam Wishal,Riad Sahara*

Main category: cs.LG

TL;DR: 提出可解释机器学习框架预测细菌多重耐药性，使用临床特征和药敏模式，XGBoost和LightGBM表现最佳，结合LIME提供实例级解释


<details>
  <summary>Details</summary>
Motivation: 抗菌素耐药性特别是多重耐药性（MDR）的上升给临床决策带来严峻挑战，传统药敏测试耗时且治疗选择有限，需要更早、更准确的预测方法

Method: 提出可解释机器学习框架，评估五种分类模型（逻辑回归、随机森林、AdaBoost、XGBoost、LightGBM），使用9,714个分离株数据集，将耐药性编码到抗生素家族级别以捕捉交叉耐药模式，应用LIME进行实例级解释

Result: 集成模型特别是XGBoost和LightGBM在所有评估指标（准确率、F1分数、AUC-ROC、马修斯相关系数）上表现最佳；LIME识别出喹诺酮类、复方新诺明、多粘菌素、氨基糖苷类和呋喃类耐药是MDR预测的最强贡献因素

Conclusion: 将高性能模型与局部可解释性相结合，既能提供准确预测又能提供可操作的临床见解，支持早期MDR识别并增强对机器学习辅助临床决策的信任

Abstract: The rise of Antimicrobial Resistance, particularly Multi-Drug Resistance (MDR), presents a critical challenge for clinical decision-making due to limited treatment options and delays in conventional susceptibility testing. This study proposes an interpretable machine learning framework to predict MDR in bacterial isolates using clinical features and antibiotic susceptibility patterns. Five classification models were evaluated, including Logistic Regression, Random Forest, AdaBoost, XGBoost, and LightGBM. The models were trained on a curated dataset of 9,714 isolates, with resistance encoded at the antibiotic family level to capture cross-class resistance patterns consistent with MDR definitions. Performance assessment included accuracy, F1-score, AUC-ROC, and Matthews Correlation Coefficient. Ensemble models, particularly XGBoost and LightGBM, demonstrated superior predictive capability across all metrics. To address the clinical transparency gap, Local Interpretable Model-agnostic Explanations (LIME) was applied to generate instance-level explanations. LIME identified resistance to quinolones, Co-trimoxazole, Colistin, aminoglycosides, and Furanes as the strongest contributors to MDR predictions, aligning with known biological mechanisms. The results show that combining high-performing models with local interpretability provides both accuracy and actionable insights for antimicrobial stewardship. This framework supports earlier MDR identification and enhances trust in machine learning-assisted clinical decision support.

</details>


### [42] [MolFM-Lite: Multi-Modal Molecular Property Prediction with Conformer Ensemble Attention and Cross-Modal Fusion](https://arxiv.org/abs/2602.22405)
*Syed Omer Shah,Mohammed Maqsood Ahmed,Danish Mohiuddin Mohammed,Shahnawaz Alam,Mohd Vahaj ur Rahman*

Main category: cs.LG

TL;DR: MolFM-Lite是一个多模态分子性质预测模型，通过交叉注意力融合SELFIES序列(1D)、分子图(2D)和构象集合(3D)三种表示，并使用FiLM整合实验上下文信息。


<details>
  <summary>Details</summary>
Motivation: 现有分子性质预测模型通常依赖单一分子表示（序列、图或3D结构）并将分子几何视为静态，这限制了模型对分子复杂特性的全面理解。

Method: 1) 构象集合注意力机制：结合可学习注意力与Boltzmann加权先验，处理多个RDKit生成的构象；2) 跨模态融合层：各模态可通过交叉注意力相互关注；3) 使用FiLM整合实验上下文；4) 在ZINC250K上进行跨模态对比和掩码原子预训练。

Result: 在四个MoleculeNet骨架分割基准测试中，三模态融合相比单模态基线带来7-11%的AUC提升，构象集合相比单构象变体提升约2%。消融研究证实各架构组件独立贡献。

Conclusion: MolFM-Lite通过多模态融合有效提升分子性质预测性能，构象集合能更好捕捉分子形状的热力学分布，预训练策略能以较低计算成本实现有效权重初始化。

Abstract: Most machine learning models for molecular property prediction rely on a single molecular representation (either a sequence, a graph, or a 3D structure) and treat molecular geometry as static. We present MolFM-Lite, a multi-modal model that jointly encodes SELFIES sequences (1D), molecular graphs (2D), and conformer ensembles (3D) through cross-attention fusion, while conditioning predictions on experimental context via Feature-wise Linear Modulation (FiLM). Our main methodological contributions are: (1) a conformer ensemble attention mechanism that combines learnable attention with Boltzmann-weighted priors over multiple RDKit-generated conformers, capturing the thermodynamic distribution of molecular shapes; and (2) a cross-modal fusion layer where each modality can attend to others, enabling complementary information sharing. We evaluate on four MoleculeNet scaffold-split benchmarks using our model's own splits, and report all baselines re-evaluated under the same protocol. Comprehensive ablation studies across all four datasets confirm that each architectural component contributes independently, with tri-modal fusion providing 7-11% AUC improvement over single-modality baselines and conformer ensembles adding approximately 2% over single-conformer variants. Pre-training on ZINC250K (~250K molecules) using cross-modal contrastive and masked-atom objectives enables effective weight initialization at modest compute cost. We release all code, trained models, and data splits to support reproducibility.

</details>


### [43] [A Learning-Based Hybrid Decision Framework for Matching Systems with User Departure Detection](https://arxiv.org/abs/2602.22412)
*Ruiqi Zhou,Donghao Zhu,Houcai Shen*

Main category: cs.LG

TL;DR: 提出基于学习的混合匹配框架，自适应结合即时与延迟匹配，在动态环境中平衡等待时间与匹配效率


<details>
  <summary>Details</summary>
Motivation: 在肾脏交换、货运交换等匹配市场中，延迟匹配能提高效率但会增加等待时间和市场拥堵。固定匹配策略在动态环境中缺乏灵活性，需要自适应方法平衡竞争效应。

Method: 提出基于学习的混合框架：持续收集用户离开时间数据，通过回归估计离开分布，基于决策阈值决定后续是否延迟匹配。该阈值控制系统对匹配效率损失的容忍度。

Result: 该框架能显著减少等待时间和拥堵，同时仅牺牲有限的匹配效率。通过动态调整匹配策略，系统性能可在贪婪策略和耐心策略之间灵活插值。

Conclusion: 混合框架为静态匹配机制提供了鲁棒且自适应的替代方案，能在动态环境中平衡匹配效率与等待成本，实现更好的整体市场表现。

Abstract: In matching markets such as kidney exchanges and freight exchanges, delayed matching has been shown to improve overall market efficiency. The benefits of delay are highly sensitive to participants' sojourn times and departure behavior, and delaying matches can impose significant costs, including longer waiting times and increased market congestion. These competing effects make fixed matching policies inherently inflexible in dynamic environments. We propose a learning-based Hybrid framework that adaptively combines immediate and delayed matching. The framework continuously collects data on user departures over time, estimates the underlying departure distribution via regression, and determines whether to delay matching in the subsequent period based on a decision threshold that governs the system's tolerance for matching efficiency loss. The proposed framework can substantially reduce waiting times and congestion while sacrificing only a limited amount of matching efficiency. By dynamically adjusting its matching strategy, the Hybrid framework enables system performance to flexibly interpolate between purely greedy and purely patient policies, offering a robust and adaptive alternative to static matching mechanisms.

</details>


### [44] [Revisiting Chebyshev Polynomial and Anisotropic RBF Models for Tabular Regression](https://arxiv.org/abs/2602.22422)
*Luciano Gerber,Huw Lloyd*

Main category: cs.LG

TL;DR: 本文评估了光滑基模型（如RBF网络和切比雪夫多项式）在表格回归任务中的表现，发现它们与树集成方法精度相当，但泛化间隙更小，适合需要连续可微预测的应用场景。


<details>
  <summary>Details</summary>
Motivation: 尽管光滑基模型在数值分析中广泛应用，但在表格回归领域却被树集成方法主导。本文旨在探究光滑基模型能否在表格回归任务中与树集成方法竞争，特别是在需要连续可微预测和较小泛化间隙的应用场景中。

Method: 开发了三种光滑基模型：各向异性RBF网络（带数据驱动中心放置和梯度宽度优化）、岭正则化切比雪夫多项式回归器、以及光滑树混合模型（切比雪夫模型树）。在55个回归数据集上进行基准测试，对比树集成、预训练transformer和标准基线模型。

Result: transformer在多数数据集上精度最高，但其GPU依赖、推理延迟和数据集大小限制限制了在CPU环境中的应用。在CPU可行模型中，光滑基模型与树集成方法精度统计上相当，但前者通常表现出更小的泛化间隙。

Conclusion: 建议在候选模型池中常规包含光滑基模型，特别是在下游应用受益于更小泛化间隙和逐渐变化预测的场景中。光滑基模型在CPU环境中具有竞争力，适合应用科学和工业中的实际部署。

Abstract: Smooth-basis models such as Chebyshev polynomial regressors and radial basis function (RBF) networks are well established in numerical analysis. Their continuously differentiable prediction surfaces suit surrogate optimisation, sensitivity analysis, and other settings where the response varies gradually with inputs. Despite these properties, smooth models seldom appear in tabular regression, where tree ensembles dominate. We ask whether they can compete, benchmarking models across 55 regression datasets organised by application domain.
  We develop an anisotropic RBF network with data-driven centre placement and gradient-based width optimisation, a ridge-regularised Chebyshev polynomial regressor, and a smooth-tree hybrid (Chebyshev model tree); all three are released as scikit-learn-compatible packages. We benchmark these against tree ensembles, a pre-trained transformer, and standard baselines, evaluating accuracy alongside generalisation behaviour.
  The transformer ranks first on accuracy across a majority of datasets, but its GPU dependence, inference latency, and dataset-size limits constrain deployment in the CPU-based settings common across applied science and industry. Among CPU-viable models, smooth models and tree ensembles are statistically tied on accuracy, but the former tend to exhibit tighter generalisation gaps. We recommend routinely including smooth-basis models in the candidate pool, particularly when downstream use benefits from tighter generalisation and gradually varying predictions.

</details>


### [45] [Calibrated Test-Time Guidance for Bayesian Inference](https://arxiv.org/abs/2602.22428)
*Daniel Geyfman,Felix Draxler,Jan Groeneveld,Hyunsoo Lee,Theofanis Karaletsos,Stephan Mandt*

Main category: cs.LG

TL;DR: 论文提出了一种新的测试时引导方法，能够从贝叶斯后验分布中进行校准采样，解决了现有方法只能最大化奖励而不能正确恢复后验分布的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时引导方法主要关注最大化奖励函数，而不是从真实的贝叶斯后验分布中采样，这导致了推理校准不准确的问题。需要一种能够正确恢复后验分布的方法。

Method: 首先分析了常见测试时引导方法无法恢复正确后验分布的原因，识别了导致失败的结构性近似问题。然后提出了具有一致性的替代估计器，能够实现从贝叶斯后验分布中进行校准采样。

Result: 在贝叶斯推理任务上显著优于先前方法，在黑洞图像重建任务上达到了最先进的性能水平。

Conclusion: 提出的方法解决了测试时引导中的校准问题，能够从贝叶斯后验分布中进行正确采样，在多个推理任务上表现出色。

Abstract: Test-time guidance is a widely used mechanism for steering pretrained diffusion models toward outcomes specified by a reward function. Existing approaches, however, focus on maximizing reward rather than sampling from the true Bayesian posterior, leading to miscalibrated inference. In this work, we show that common test-time guidance methods do not recover the correct posterior distribution and identify the structural approximations responsible for this failure. We then propose consistent alternative estimators that enable calibrated sampling from the Bayesian posterior. We significantly outperform previous methods on a set of Bayesian inference tasks, and match state-of-the-art in black hole image reconstruction.

</details>


### [46] [From Bias to Balance: Fairness-Aware Paper Recommendation for Equitable Peer Review](https://arxiv.org/abs/2602.22438)
*Uttamasha Anjally Oyshi,Susan Gauch*

Main category: cs.LG

TL;DR: Fair-PaperRec：一个带有公平性正则化的后审稿推荐系统，通过重新排序双盲审稿后的论文，在保持质量的同时显著增加弱势群体参与度


<details>
  <summary>Details</summary>
Motivation: 尽管采用双盲审稿，系统性偏见仍然使弱势群体处于不利地位。研究假设：如果在后审稿推荐系统中加入明确的公平性正则化，可以在不降低质量的前提下增加包容性

Method: 提出Fair-PaperRec系统，使用多层感知机（MLP）结合可微分的公平性损失函数，基于交叉属性（种族、国家等）对双盲审稿后的论文进行重新排序。先在合成数据集上测试，然后在真实会议数据（ACM SIGCHI、DIS、IUI）上验证

Result: 在合成数据集上，公平性权重增加能增强宏观/微观多样性同时保持效用稳定。在真实场景中，适当配置的Fair-PaperRec使弱势群体参与度提升42.03%，整体效用变化不超过3.16%

Conclusion: 公平性正则化既能作为公平机制，也能作为温和的质量正则化器，特别是在高度偏见的场景中。Fair-PaperRec提供了一个实用、以公平为中心的框架，在保持甚至提升学术质量的同时促进包容性

Abstract: Despite frequent double-blind review, systemic biases related to author demographics still disadvantage underrepresented groups. We start from a simple hypothesis: if a post-review recommender is trained with an explicit fairness regularizer, it should increase inclusion without degrading quality. To test this, we introduce Fair-PaperRec, a Multi-Layer Perceptron (MLP) with a differentiable fairness loss over intersectional attributes (e.g., race, country) that re-ranks papers after double-blind review. We first probe the hypothesis on synthetic datasets spanning high, moderate, and near-fair biases. Across multiple randomized runs, these controlled studies map where increasing the fairness weight strengthens macro/micro diversity while keeping utility approximately stable, demonstrating robustness and adaptability under varying disparity levels. We then carry the hypothesis into the original setting, conference data from ACM Special Interest Group on Computer-Human Interaction (SIGCHI), Designing Interactive Systems (DIS), and Intelligent User Interfaces (IUI). In this real-world scenario, an appropriately tuned configuration of Fair-PaperRec achieves up to a 42.03% increase in underrepresented-group participation with at most a 3.16% change in overall utility relative to the historical selection. Taken together, the synthetic-to-original progression shows that fairness regularization can act as both an equity mechanism and a mild quality regularizer, especially in highly biased regimes. By first analyzing the behavior of the fairness parameters under controlled conditions and then validating them on real submissions, Fair-PaperRec offers a practical, equity-focused framework for post-review paper selection that preserves, and in some settings can even enhance, measured scholarly quality.

</details>


### [47] [ECHO: Encoding Communities via High-order Operators](https://arxiv.org/abs/2602.22446)
*Emilio Ferrara*

Main category: cs.LG

TL;DR: ECHO是一个可扩展的自监督架构，通过自适应多尺度扩散过程解决属性网络社区检测问题，克服了GNN的特征过平滑和O(N²)内存瓶颈。


<details>
  <summary>Details</summary>
Motivation: 属性网络社区检测面临基本分歧：拓扑算法忽略语义特征，而图神经网络（GNNs）存在计算瓶颈。GNNs在密集或异配网络中遭遇特征过平滑的"语义墙"，以及由成对聚类O(N²)内存约束导致的"系统墙"。

Method: 引入ECHO架构，将社区检测重构为自适应多尺度扩散过程。包含：1）拓扑感知路由器，自动分析结构启发式（稀疏性、密度、同配性）以通过最优归纳偏置路由图；2）内存分片全批次对比目标；3）新颖的分块O(N·K)相似性提取方法，完全绕过传统O(N²)内存瓶颈。

Result: 在扩展到100万个节点的合成LFR基准测试中，ECHO在严重拓扑噪声下实现尺度不变准确性。在超过160万个节点和3000万条边的大规模真实世界社交网络中，仅需几分钟完成聚类，吞吐量超过2800个节点/秒，与高度优化的纯拓扑基线速度相当。

Conclusion: ECHO通过拓扑特征协同作用克服了经典分辨率限制，提供了一种统一框架，可自动参与内存分片优化，支持在不同硬件约束下采用，解决了GNNs在社区检测中的计算瓶颈问题。

Abstract: Community detection in attributed networks faces a fundamental divide: topological algorithms ignore semantic features, while Graph Neural Networks (GNNs) encounter devastating computational bottlenecks. Specifically, GNNs suffer from a Semantic Wall of feature over smoothing in dense or heterophilic networks, and a Systems Wall driven by the O(N^2) memory constraints of pairwise clustering. To dismantle these barriers, we introduce ECHO (Encoding Communities via High order Operators), a scalable, self supervised architecture that reframes community detection as an adaptive, multi scale diffusion process. ECHO features a Topology Aware Router that automatically analyzes structural heuristics sparsity, density, and assortativity to route graphs through the optimal inductive bias, preventing heterophilic poisoning while ensuring semantic densification. Coupled with a memory sharded full batch contrastive objective and a novel chunked O(N \cdot K) similarity extraction method, ECHO completely bypasses traditional O(N^2) memory bottlenecks without sacrificing the mathematical precision of global gradients. Extensive evaluations demonstrate that this topology feature synergy consistently overcomes the classical resolution limit. On synthetic LFR benchmarks scaled up to 1 million nodes, ECHO achieves scale invariant accuracy despite severe topological noise. Furthermore, on massive real world social networks with over 1.6 million nodes and 30 million edges, it completes clustering in mere minutes with throughputs exceeding 2,800 nodes per second matching the speed of highly optimized purely topological baselines. The implementation utilizes a unified framework that automatically engages memory sharded optimization to support adoption across varying hardware constraints. GitHub Repository: https://github.com/emilioferrara/ECHO-GNN

</details>


### [48] [Beyond performance-wise Contribution Evaluation in Federated Learning](https://arxiv.org/abs/2602.22470)
*Balazs Pejo*

Main category: cs.LG

TL;DR: 本文提出了一种新的联邦学习客户端评估框架，不仅考虑模型性能（如准确率），还评估客户端对模型可信度（可靠性、鲁棒性、公平性）的贡献，使用Shapley值进行量化分析。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习客户端评估方法主要关注模型性能指标（如准确率、损失），但这只是机器学习模型整体效用的一个维度。本文关注被忽视的问题：客户端对模型可信度的贡献，包括可靠性（对噪声数据的容忍度）、鲁棒性（对抗样本的抵抗能力）和公平性（通过人口统计奇偶性衡量）。

Method: 采用最先进的Shapley值近似方法，这是一种原则性的价值归因方法，用于量化客户端在多维度（可靠性、鲁棒性、公平性）上的贡献。

Result: 研究结果显示：没有单个客户端在所有维度上都表现出色，这些维度在很大程度上相互独立，这揭示了当前评估方案的关键缺陷：单一指标无法进行全面的评估和公平的奖励分配。

Conclusion: 联邦学习需要多维度的客户端评估框架，考虑模型可信度的多个方面，而不仅仅是性能指标，以确保更全面和公平的贡献评估与奖励分配。

Abstract: Federated learning offers a privacy-friendly collaborative learning framework, yet its success, like any joint venture, hinges on the contributions of its participants. Existing client evaluation methods predominantly focus on model performance, such as accuracy or loss, which represents only one dimension of a machine learning model's overall utility. In contrast, this work investigates the critical, yet overlooked, issue of client contributions towards a model's trustworthiness -- specifically, its reliability (tolerance to noisy data), resilience (resistance to adversarial examples), and fairness (measured via demographic parity). To quantify these multifaceted contributions, we employ the state-of-the-art approximation of the Shapley value, a principled method for value attribution. Our results reveal that no single client excels across all dimensions, which are largely independent from each other, highlighting a critical flaw in current evaluation scheme: no single metric is adequate for comprehensive evaluation and equitable rewarding allocation.

</details>


### [49] [Efficient Continual Learning in Language Models via Thalamically Routed Cortical Columns](https://arxiv.org/abs/2602.22479)
*Afshin Khadangi*

Main category: cs.LG

TL;DR: TRC²是一种解码器架构，通过丘脑路由皮质柱结构解决语言模型持续学习中的灾难性遗忘问题，在保持计算效率的同时改善稳定性-可塑性权衡。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在非平稳数据流中面临灾难性遗忘问题，而改进稳定性的方法通常会增加延迟、内存占用或计算成本，难以扩展到长上下文场景。

Method: 提出TRC²架构，结合稀疏丘脑路由皮质柱、调制、预测、记忆和反馈机制，以及快速校正通路，支持快速适应而不破坏慢速参数稳定性。该模块稀疏且支持分块并行。

Result: 在语言建模和持续学习基准测试中，TRC²在相同计算量下改善了稳定性-可塑性权衡，实现了快速在线适应同时保留先前习得的行为。

Conclusion: TRC²从架构层面解决了持续学习问题，通过稀疏路由和快速校正通路实现了高效训练和推理，为语言模型的持续学习提供了有效解决方案。

Abstract: Continual learning is a core requirement for deployed language models, yet standard training and fine-tuning pipelines remain brittle under non-stationary data. Online updates often induce catastrophic forgetting, while methods that improve stability frequently increase latency, memory footprint, or dense computation in ways that do not scale well to long contexts. We introduce TRC$^{2}$ (Thalamically Routed Cortical Columns), a decoder-only backbone that addresses continual learning at the architectural level. TRC$^{2}$ combines sparse thalamic routing over cortical columns with mechanisms for modulation, prediction, memory, and feedback, together with a fast corrective pathway that supports rapid adaptation without destabilizing slower parameters. The resulting block is sparse and chunk-parallel, enabling efficient training and inference while preserving clean ablations of each subsystem. We instantiate a reproducible training and evaluation stack and a continual-learning harness that measures proxy forgetting under streaming domain shifts. Across language modeling and continual learning benchmarks, TRC$^{2}$ improves the stability-plasticity tradeoff at comparable compute, enabling rapid on-stream adaptation while preserving previously acquired behavior.

</details>


### [50] [Reinforcement-aware Knowledge Distillation for LLM Reasoning](https://arxiv.org/abs/2602.22495)
*Zhaoyang Zhang,Shuli Jiang,Yantao Shen,Yuting Zhang,Dhananjay Ram,Shuo Yang,Zhuowen Tu,Wei Xia,Stefano Soatto*

Main category: cs.LG

TL;DR: RLAD是一种强化学习感知的蒸馏方法，通过选择性模仿和信任区域比率蒸馏，在RL过程中动态指导学生模型，解决了传统蒸馏方法中的分布不匹配和目标冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现有的知识蒸馏方法主要针对监督微调设计，当与强化学习结合时，存在分布不匹配和目标冲突问题：教师监督可能与学生模型的演化分布不一致，KL正则化器可能与奖励最大化竞争，需要仔细的损失平衡。

Method: 提出了RL感知蒸馏（RLAD），在RL过程中进行选择性模仿——只有当教师指导能改进当前策略更新时才引导学生模型。核心组件是信任区域比率蒸馏（TRRD），用基于PPO/GRPO风格的似然比目标替换教师-学生KL正则化器，该目标锚定在教师-旧策略混合上，在学生模型rollouts上实现优势感知、信任区域有界的蒸馏。

Result: 在多样化的逻辑推理和数学基准测试中，RLAD始终优于离线蒸馏、标准GRPO和基于KL的在线教师-学生知识蒸馏方法。

Conclusion: RLAD通过选择性模仿和信任区域比率蒸馏，有效解决了强化学习后训练中的蒸馏问题，在保持推理能力的同时实现了模型小型化，平衡了探索、利用和模仿。

Abstract: Reinforcement learning (RL) post-training has recently driven major gains in long chain-of-thought reasoning large language models (LLMs), but the high inference cost of such models motivates distillation into smaller students. Most existing knowledge distillation (KD) methods are designed for supervised fine-tuning (SFT), relying on fixed teacher traces or teacher-student Kullback-Leibler (KL) divergence-based regularization. When combined with RL, these approaches often suffer from distribution mismatch and objective interference: teacher supervision may not align with the student's evolving rollout distribution, and the KL regularizer can compete with reward maximization and require careful loss balancing. To address these issues, we propose RL-aware distillation (RLAD), which performs selective imitation during RL -- guiding the student toward the teacher only when it improves the current policy update. Our core component, Trust Region Ratio Distillation (TRRD), replaces the teacher-student KL regularizer with a PPO/GRPO-style likelihood-ratio objective anchored to a teacher--old-policy mixture, yielding advantage-aware, trust-region-bounded distillation on student rollouts and naturally balancing exploration, exploitation, and imitation. Across diverse logic reasoning and math benchmarks, RLAD consistently outperforms offline distillation, standard GRPO, and KL-based on-policy teacher-student knowledge distillation.

</details>


### [51] [Sharp Convergence Rates for Masked Diffusion Models](https://arxiv.org/abs/2602.22505)
*Yuchen Liang,Zhiheng Tan,Ness Shroff,Yingbin Liang*

Main category: cs.LG

TL;DR: 本文为离散扩散模型中的Euler采样器和First-Hitting Sampler（FHS）提供了首个直接基于总变差（TV）的理论分析，改进了参数依赖关系，建立了收敛保证，并给出了匹配的下界证明。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在文本和符号领域表现出色，但现有采样器的理论分析有限。现有分析基于KL散度，存在参数依赖关系松散、需要强假设等问题，且未能覆盖高性能的FHS采样器。

Method: 1. 为Euler方法开发了直接基于总变差（TV）的分析框架；2. 建立了Euler采样器的收敛下界；3. 分析了FHS采样器，证明其采样误差仅由分数估计误差决定；4. 提出了沿CTMC轨迹的直接TV误差分解方法和基于解耦的路径分析。

Result: 1. 放松了分数估计的假设条件；2. 改进了参数依赖关系；3. 建立了无需代理初始化的收敛保证；4. 给出了关于数据维度d和目标精度ε的紧致收敛下界；5. 证明FHS采样器的误差仅由分数估计误差引起，并给出了匹配的下界。

Conclusion: 本文为离散扩散模型中的Euler和FHS采样器提供了首个直接TV分析，建立了紧致的理论保证，提出的分析框架可能具有独立的研究价值。

Abstract: Discrete diffusion models have achieved strong empirical performance in text and other symbolic domains, with masked (absorbing-rate) variants emerging as competitive alternatives to autoregressive models. Among existing samplers, the Euler method remains the standard choice in many applications, and more recently, the First-Hitting Sampler (FHS) has shown considerable promise for masked diffusion models. Despite their practical success, the theoretical understanding of these samplers remains limited. Existing analyses are conducted in Kullback-Leibler (KL) divergence, which often yields loose parameter dependencies and requires strong assumptions on score estimation. Moreover, these guarantees do not cover recently developed high-performance sampler of FHS. In this work, we first develop a direct total-variation (TV) based analysis for the Euler method that overcomes these limitations. Our results relax assumptions on score estimation, improve parameter dependencies, and establish convergence guarantees without requiring any surrogate initialization. Also for this setting, we provide the first convergence lower bound for the Euler sampler, establishing tightness with respect to both the data dimension $d$ and the target accuracy $\varepsilon$. Finally, we analyze the FHS sampler and show that it incurs no sampling error beyond that induced by score estimation, which we show to be tight with a matching lower error bound. Overall, our analysis introduces a direct TV-based error decomposition along the CTMC trajectory and a decoupling-based path-wise analysis for FHS, which may be of independent interest.

</details>


### [52] [Space Syntax-guided Post-training for Residential Floor Plan Generation](https://arxiv.org/abs/2602.22507)
*Zhuoyang Jiang,Dongqing Zhang*

Main category: cs.LG

TL;DR: 提出SSPT方法，通过空间句法引导的后训练范式，将建筑空间配置知识注入住宅平面图生成，改善公共空间主导性和功能层次


<details>
  <summary>Details</summary>
Motivation: 现有预训练生成模型过度拟合大规模数据分布，忽视了建筑学中重要的空间配置先验，特别是公共空间（如客厅、门厅）的配置主导性和连通性

Method: 提出SSPT后训练范式：1）使用非可微oracle将布局转换为矩形空间图并计算整合度测量；2）两种实现策略：基于空间句法筛选的迭代重训练+扩散微调，以及基于PPO强化学习的空间句法奖励

Result: 两种策略都改善了公共空间主导性并恢复了更清晰的功能层次，其中PPO方法获得更强增益，计算效率更高且方差更小

Conclusion: SSPT为将建筑理论整合到数据驱动的平面生成提供了可扩展路径，兼容其他生成主干网络，只需后验评估oracle

Abstract: Pre-trained generative models for residential floor plans are typically optimized to fit large-scale data distributions, which can under-emphasize critical architectural priors such as the configurational dominance and connectivity of domestic public spaces (e.g., living rooms and foyers). This paper proposes Space Syntax-guided Post-training (SSPT), a post-training paradigm that explicitly injects space syntax knowledge into floor plan generation via a non-differentiable oracle. The oracle converts RPLAN-style layouts into rectangle-space graphs through greedy maximal-rectangle decomposition and door-mediated adjacency construction, and then computes integration-based measurements to quantify public space dominance and functional hierarchy.
  To enable consistent evaluation and diagnosis, we further introduce SSPT-Bench (Eval-8), an out-of-distribution benchmark that post-trains models using conditions capped at $\leq 7$ rooms while evaluating on 8-room programs, together with a unified metric suite for dominance, stability, and profile alignment. SSPT is instantiated with two strategies: (i) iterative retraining via space-syntax filtering and diffusion fine-tuning, and (ii) reinforcement learning via PPO with space-syntax rewards. Experiments show that both strategies improve public-space dominance and restore clearer functional hierarchy compared to distribution-fitted baselines, while PPO achieves stronger gains with substantially higher compute efficiency and reduced variance. SSPT provides a scalable pathway for integrating architectural theory into data-driven plan generation and is compatible with other generative backbones given a post-hoc evaluation oracle.

</details>


### [53] [TEFL: Prediction-Residual-Guided Rolling Forecasting for Multi-Horizon Time Series](https://arxiv.org/abs/2602.22520)
*Xiannan Huang,Shen Fang,Shuhan Qiu,Chengcheng Yu,Jiayuan Du,Chao Yang*

Main category: cs.LG

TL;DR: TEFL（时间误差反馈学习）是一个深度学习框架，通过将历史预测残差纳入训练和评估过程，提升时间序列预测精度，在多个数据集上平均降低MAE 5-10%。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习预测模型通常只最小化点预测损失，忽略了滚动预测中历史残差所包含的丰富信息，如持续偏差、未建模模式或动态变化。

Method: 提出TEFL框架，解决三个关键挑战：1）在滚动预测部分可观测下选择可观测多步残差；2）通过轻量级低秩适配器集成残差以保持效率；3）设计两阶段训练程序联合优化基础预测器和误差模块。

Result: 在10个真实世界数据集和5个骨干架构上的实验表明，TEFL一致提升准确性，平均降低MAE 5-10%，在突变和分布偏移场景下误差降低超过10%（最高达19.5%）。

Conclusion: TEFL通过将基于残差的反馈直接嵌入学习过程，为现代深度学习预测系统提供了简单、通用且有效的增强方法。

Abstract: Time series forecasting plays a critical role in domains such as transportation, energy, and meteorology. Despite their success, modern deep forecasting models are typically trained to minimize point-wise prediction loss without leveraging the rich information contained in past prediction residuals from rolling forecasts - residuals that reflect persistent biases, unmodeled patterns, or evolving dynamics. We propose TEFL (Temporal Error Feedback Learning), a unified learning framework that explicitly incorporates these historical residuals into the forecasting pipeline during both training and evaluation. To make this practical in deep multi-step settings, we address three key challenges: (1) selecting observable multi-step residuals under the partial observability of rolling forecasts, (2) integrating them through a lightweight low-rank adapter to preserve efficiency and prevent overfitting, and (3) designing a two-stage training procedure that jointly optimizes the base forecaster and error module. Extensive experiments across 10 real-world datasets and 5 backbone architectures show that TEFL consistently improves accuracy, reducing MAE by 5-10% on average. Moreover, it demonstrates strong robustness under abrupt changes and distribution shifts, with error reductions exceeding 10% (up to 19.5%) in challenging scenarios. By embedding residual-based feedback directly into the learning process, TEFL offers a simple, general, and effective enhancement to modern deep forecasting systems.

</details>


### [54] [Predicting Tennis Serve directions with Machine Learning](https://arxiv.org/abs/2602.22527)
*Ying Zhu,Ruthuparna Naikar*

Main category: cs.LG

TL;DR: 开发了一种机器学习方法来预测职业网球选手的一发方向，平均预测准确率男性约49%，女性约44%，揭示了选手使用混合策略以及疲劳对发球决策的影响。


<details>
  <summary>Details</summary>
Motivation: 职业网球中发球（尤其是一发）至关重要，发球方需要战略性地选择发球方向以最大化胜率同时保持不可预测性，接发方则试图预测发球方向。理解选手的发球决策对分析网球比赛决策过程很重要。

Method: 通过特征工程开发了机器学习方法来预测职业网球选手的一发方向，该方法能够分析选手的发球决策模式。

Result: 平均预测准确率男性选手约49%，女性选手约44%。分析表明顶尖职业选手在发球决策中使用混合策略模型，疲劳可能是影响发球方向选择的因素，且情境信息对接发方的预判反应比之前认为的更重要。

Conclusion: 该研究为理解职业网球选手的发球决策提供了量化分析方法，揭示了混合策略、疲劳因素和情境信息在网球发球决策中的重要性，对网球战术分析和训练有实际意义。

Abstract: Serves, especially first serves, are very important in professional tennis. Servers choose their serve directions strategically to maximize their winning chances while trying to be unpredictable. On the other hand, returners try to predict serve directions to make good returns. The mind game between servers and returners is an important part of decision-making in professional tennis matches. To help understand the players' serve decisions, we have developed a machine learning method for predicting professional tennis players' first serve directions. Through feature engineering, our method achieves an average prediction accuracy of around 49\% for male players and 44\% for female players. Our analysis provides some evidence that top professional players use a mixed-strategy model in serving decisions and that fatigue might be a factor in choosing serve directions. Our analysis also suggests that contextual information is perhaps more important for returners' anticipatory reactions than previously thought.

</details>


### [55] [Coarse-to-Fine Learning of Dynamic Causal Structures](https://arxiv.org/abs/2602.22532)
*Dezhi Yang,Qiaoyu Tan,Carlotta Domeniconi,Jun Wang,Lizhen Cui,Guoxian Yu*

Main category: cs.LG

TL;DR: DyCausal：一种从粗到细学习完全动态因果结构的框架，利用卷积网络捕捉粗粒度时间窗口的因果模式，通过线性插值细化每个时间步的因果图


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖分布或结构不变性假设，要求因果关系是平稳或部分平稳的，这与现实世界中复杂、时变的因果关系相矛盾。需要能够处理完全动态因果关系的方法，其中瞬时和滞后依赖都随时间演变。

Method: DyCausal框架：1）使用卷积网络捕捉粗粒度时间窗口内的因果模式；2）应用线性插值细化每个时间步的因果结构；3）提出基于矩阵范数缩放的环约束，提高效率并有效约束演化因果结构中的循环。

Result: 在合成和真实数据集上的综合评估表明，DyCausal相比现有方法具有优越性能，为从粗到细识别完全动态因果结构提供了稳定高效的方法。

Conclusion: DyCausal成功解决了完全动态因果结构学习问题，通过从粗到细的方法有效捕捉时变因果关系，在效率和稳定性方面表现优异，为复杂动态系统的因果发现提供了实用框架。

Abstract: Learning the dynamic causal structure of time series is a challenging problem. Most existing approaches rely on distributional or structural invariance to uncover underlying causal dynamics, assuming stationary or partially stationary causality. However, these assumptions often conflict with the complex, time-varying causal relationships observed in real-world systems. This motivates the need for methods that address fully dynamic causality, where both instantaneous and lagged dependencies evolve over time. Such a setting poses significant challenges for the efficiency and stability of causal discovery. To address these challenges, we introduce DyCausal, a dynamic causal structure learning framework. DyCausal leverages convolutional networks to capture causal patterns within coarse-grained time windows, and then applies linear interpolation to refine causal structures at each time step, thereby recovering fine-grained and time-varying causal graphs. In addition, we propose an acyclic constraint based on matrix norm scaling, which improves efficiency while effectively constraining loops in evolving causal structures. Comprehensive evaluations on both synthetic and real-world datasets demonstrate that DyCausal achieves superior performance compared to existing methods, offering a stable and efficient approach for identifying fully dynamic causal structures from coarse to fine.

</details>


### [56] [Persistent Nonnegative Matrix Factorization via Multi-Scale Graph Regularization](https://arxiv.org/abs/2602.22536)
*Jichao Zhang,Ran Miao,Limin Li*

Main category: cs.LG

TL;DR: 提出持久非负矩阵分解(pNMF)，通过持久同调识别多尺度结构，生成尺度参数化的嵌入序列而非单一分解，解决传统NMF无法捕捉多尺度连通性演化的问题。


<details>
  <summary>Details</summary>
Motivation: 传统NMF方法是单尺度的，无法捕捉数据连通性结构在不同分辨率下的演化过程。需要一种能够跨尺度分析数据的方法，以理解数据在不同粒度下的结构变化。

Method: 提出pNMF方法：1) 利用持久同调识别底层连通性发生定性变化的规范最小充分尺度集；2) 这些尺度诱导出图拉普拉斯序列；3) 构建具有尺度几何正则化和跨尺度一致性约束的耦合NMF公式；4) 开发序列交替优化算法保证收敛。

Result: 在合成数据和单细胞RNA测序数据集上的数值实验表明，该方法在多尺度低秩嵌入方面有效。建立了嵌入沿尺度参数的结构性质分析，并证明了连续尺度间增量的界限。

Conclusion: pNMF定义了跨尺度的非平凡解路径而非单一分解，解决了传统NMF的多尺度限制，为多尺度数据表示提供了新框架，但带来了新的计算挑战。

Abstract: Matrix factorization techniques, especially Nonnegative Matrix Factorization (NMF), have been widely used for dimensionality reduction and interpretable data representation. However, existing NMF-based methods are inherently single-scale and fail to capture the evolution of connectivity structures across resolutions. In this work, we propose persistent nonnegative matrix factorization (pNMF), a scale-parameterized family of NMF problems, that produces a sequence of persistence-aligned embeddings rather than a single one. By leveraging persistent homology, we identify a canonical minimal sufficient scale set at which the underlying connectivity undergoes qualitative changes. These canonical scales induce a sequence of graph Laplacians, leading to a coupled NMF formulation with scale-wise geometric regularization and explicit cross-scale consistency constraint. We analyze the structural properties of the embeddings along the scale parameter and establish bounds on their increments between consecutive scales. The resulting model defines a nontrivial solution path across scales, rather than a single factorization, which poses new computational challenges. We develop a sequential alternating optimization algorithm with guaranteed convergence. Numerical experiments on synthetic and single-cell RNA sequencing datasets demonstrate the effectiveness of the proposed approach in multi-scale low-rank embeddings.

</details>


### [57] [LUMOS: Democratizing SciML Workflows with L0-Regularized Learning for Unified Feature and Parameter Adaptation](https://arxiv.org/abs/2602.22537)
*Shouwei Gao,Xu Zheng,Dongsheng Luo,Sheng Di,Wenqian Dong*

Main category: cs.LG

TL;DR: LUMOS是一个基于L0正则化学习的端到端框架，统一了特征选择和模型剪枝，用于简化科学机器学习模型设计，在13个SciML任务中平均减少71.45%参数并实现6.4倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习模型设计需要大量先验知识和人工专业知识，特别是在确定输入特征和模型规模方面，这限制了SciML的普及和应用。

Method: 基于L0正则化学习，采用半随机门控和重参数化技术，在训练过程中动态选择信息特征并剪枝冗余参数，减少人工调优依赖。

Result: 在13个不同SciML任务（包括宇宙学和分子科学）中，LUMOS平均实现71.45%参数减少和6.4倍推理加速，分布式训练在8个GPU上验证了可扩展性。

Conclusion: LUMOS框架通过统一特征选择和模型剪枝，有效简化了SciML模型设计，减少了人工调优需求，同时保持预测准确性，具有广泛的适用性和可扩展性。

Abstract: The rapid growth of scientific machine learning (SciML) has accelerated discovery across diverse domains, yet designing effective SciML models remains a challenging task. In practice, building such models often requires substantial prior knowledge and manual expertise, particularly in determining which input features to use and how large the model should be. We introduce LUMOS, an end-to-end framework based on L0-regularized learning that unifies feature selection and model pruning to democratize SciML model design. By employing semi-stochastic gating and reparameterization techniques, LUMOS dynamically selects informative features and prunes redundant parameters during training, reducing the reliance on manual tuning while maintaining predictive accuracy. We evaluate LUMOS across 13 diverse SciML workloads, including cosmology and molecular sciences, and demonstrate its effectiveness and generalizability. Experiments on 13 SciML models show that LUMOS achieves 71.45% parameter reduction and a 6.4x inference speedup on average. Furthermore, Distributed Data Parallel (DDP) training on up to eight GPUs confirms the scalability of

</details>


### [58] [RAIN-Merging: A Gradient-Free Method to Enhance Instruction Following in Large Reasoning Models with Preserved Thinking Format](https://arxiv.org/abs/2602.22538)
*Zhehao Huang,Yuhang Liu,Baijiong Lin,Yixin Lou,Zhengbao He,Hanling Tian,Tao Li,Xiaolin Huang*

Main category: cs.LG

TL;DR: RAIN-Merging：一种无需梯度的方法，通过将指令调优模型集成到大型推理模型中，解决推理模型在输出格式、约束和特定要求方面不遵循指令的问题。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型擅长长链推理，但在遵循输出格式、约束和特定要求方面表现不佳。研究者希望探索是否可以通过集成指令调优模型来弥补这一差距。

Method: 提出RAIN-Merging方法：1）使用小型推理校准集，将ITM任务向量投影到思考特殊标记的前向特征零空间，保留LRM的结构化推理机制；2）使用小型指令校准集，估计指令注意力以推导模块特定的缩放，放大指令相关组件并抑制泄漏。

Result: 在四个指令遵循基准测试和九个推理与通用能力基准测试中，RAIN-Merging显著提高了指令遵循能力，同时保持了推理质量。改进在不同模型规模和架构中保持一致，并在智能体设置中提升了性能。

Conclusion: RAIN-Merging成功解决了大型推理模型在指令遵循方面的不足，通过轻量级合并实现了推理能力和指令遵循能力的平衡，为智能体应用提供了更好的基础。

Abstract: Large reasoning models (LRMs) excel at a long chain of reasoning but often fail to faithfully follow instructions regarding output format, constraints, or specific requirements. We investigate whether this gap can be closed by integrating an instruction-tuned model (ITM) into an LRM. Analyzing their differences in parameter space, namely task vectors, we find that their principal subspaces are nearly orthogonal across key modules, suggesting a lightweight merging with minimal interference. However, we also demonstrate that naive merges are fragile because they overlook the output format mismatch between LRMs (with explicit thinking and response segments) and ITMs (answers-only). We introduce RAIN-Merging (Reasoning-Aware Instruction-attention guided Null-space projection Merging), a gradient-free method that integrates instruction following while preserving thinking format and reasoning performance. First, with a small reasoning calibration set, we project the ITM task vector onto the null space of forward features at thinking special tokens, which preserves the LRM's structured reasoning mechanisms. Second, using a small instruction calibration set, we estimate instruction attention to derive module-specific scaling that amplifies instruction-relevant components and suppresses leakage. Across four instruction-following benchmarks and nine reasoning & general capability benchmarks, RAIN-Merging substantially improves instruction adherence while maintaining reasoning quality. The gains are consistent across model scales and architectures, translating to improved performance in agent settings.

</details>


### [59] [Relatron: Automating Relational Machine Learning over Relational Databases](https://arxiv.org/abs/2602.22552)
*Zhikai Chen,Han Xie,Jian Zhang,Jiliang Tang,Xiang Song,Huzefa Rangwala*

Main category: cs.LG

TL;DR: 该研究对关系深度学习(RDL)与深度特征合成(DFS)进行了系统比较，发现两者性能差异高度依赖任务，并提出基于任务嵌入的元选择器Relatron来优化架构选择。


<details>
  <summary>Details</summary>
Motivation: 关系数据库上的预测建模面临挑战，需要同时捕捉跨表依赖和复杂特征交互。虽然RDL方法通过消息传递自动化特征工程，而DFS依赖预定义聚合器，但两者比较优势不明确，缺乏有效的架构选择原则。

Method: 将RDL和DFS统一到共享设计空间，进行架构中心搜索；构建模型性能库；提出两个任务信号（RDB任务同质性和亲和嵌入）来分析性能差异；开发Relatron元选择器，基于任务嵌入选择RDL或DFS并剪枝族内搜索；使用轻量级损失景观指标避免脆弱检查点。

Result: 研究发现：(1)RDL不总是优于DFS，性能高度依赖任务；(2)没有单一架构在所有任务中占优；(3)验证准确率不可靠。Relatron解决了"更多调优，更差性能"问题，在联合超参数-架构优化中比强基线提升18.5%，成本比基于Fisher信息的替代方案低10倍。

Conclusion: 关系数据库上的预测建模需要任务感知的架构选择。提出的任务信号和Relatron元选择器能够有效指导RDL与DFS的选择，并通过轻量级损失景观指标确保鲁棒性，为实际应用提供了实用的解决方案。

Abstract: Predictive modeling over relational databases (RDBs) powers applications, yet remains challenging due to capturing both cross-table dependencies and complex feature interactions. Relational Deep Learning (RDL) methods automate feature engineering via message passing, while classical approaches like Deep Feature Synthesis (DFS) rely on predefined non-parametric aggregators. Despite performance gains, the comparative advantages of RDL over DFS and the design principles for selecting effective architectures remain poorly understood. We present a comprehensive study that unifies RDL and DFS in a shared design space and conducts architecture-centric searches across diverse RDB tasks. Our analysis yields three key findings: (1) RDL does not consistently outperform DFS, with performance being highly task-dependent; (2) no single architecture dominates across tasks, underscoring the need for task-aware model selection; and (3) validation accuracy is an unreliable guide for architecture choice. This search yields a model performance bank that links architecture configurations to their performance; leveraging this bank, we analyze the drivers of the RDL-DFS performance gap and introduce two task signals -- RDB task homophily and an affinity embedding that captures size, path, feature, and temporal structure -- whose correlation with the gap enables principled routing. Guided by these signals, we propose Relatron, a task embedding-based meta-selector that chooses between RDL and DFS and prunes the within-family search. Lightweight loss-landscape metrics further guard against brittle checkpoints by preferring flatter optima. In experiments, Relatron resolves the "more tuning, worse performance" effect and, in joint hyperparameter-architecture optimization, achieves up to 18.5% improvement over strong baselines with 10x lower cost than Fisher information-based alternatives.

</details>


### [60] [Multilingual Safety Alignment Via Sparse Weight Editing](https://arxiv.org/abs/2602.22554)
*Jiaming Liang,Zhaoxin Wang,Handing Wang*

Main category: cs.LG

TL;DR: 提出基于稀疏权重编辑的无训练对齐框架，通过约束线性变换将低资源语言的有害表征映射到高资源语言的安全子空间，显著降低攻击成功率且不影响通用推理能力


<details>
  <summary>Details</summary>
Motivation: 大语言模型在不同语言间存在显著的安全差异，低资源语言容易绕过为高资源语言（如英语）建立的安全防护。现有解决方案（多语言监督微调或RLHF）计算成本高且依赖稀缺的多语言安全数据。

Method: 基于稀疏权重编辑的无训练对齐框架。识别安全能力集中在稀疏的安全神经元中，将跨语言对齐问题表述为约束线性变换。推导闭式解，将低资源语言的有害表征最优映射到高资源语言的鲁棒安全子空间，同时通过零空间投影约束保持通用效用。

Result: 在8种语言和多个模型家族（Llama-3、Qwen-2.5）上的广泛实验表明，该方法显著降低了低资源语言的攻击成功率，对通用推理能力影响可忽略，且仅需一次数据高效计算。

Conclusion: 提出的稀疏权重编辑框架为跨语言安全对齐提供了高效、数据高效的解决方案，无需额外训练即可显著提升低资源语言的安全性，同时保持模型通用能力。

Abstract: Large Language Models (LLMs) exhibit significant safety disparities across languages, with low-resource languages (LRLs) often bypassing safety guardrails established for high-resource languages (HRLs) like English. Existing solutions, such as multilingual supervised fine-tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF), are computationally expensive and dependent on scarce multilingual safety data. In this work, we propose a novel, training-free alignment framework based on Sparse Weight Editing. Identifying that safety capabilities are localized within a sparse set of safety neurons, we formulate the cross-lingual alignment problem as a constrained linear transformation. We derive a closed-form solution to optimally map the harmful representations of LRLs to the robust safety subspaces of HRLs, while preserving general utility via a null-space projection constraint. Extensive experiments across 8 languages and multiple model families (Llama-3, Qwen-2.5) demonstrate that our method substantially reduces Attack Success Rate (ASR) in LRLs with negligible impact on general reasoning capabilities, all achieved with a single, data-efficient calculation.

</details>


### [61] [Autoregressive Visual Decoding from EEG Signals](https://arxiv.org/abs/2602.22555)
*Sicheng Dai,Hongwang Xiao,Shan Yu,Qiwei Ye*

Main category: cs.LG

TL;DR: AVDE：基于自回归生成框架的轻量级EEG视觉解码方法，通过对比学习对齐EEG-图像表示，采用"下一尺度预测"策略实现高效图像重建，参数量仅为现有方法的10%


<details>
  <summary>Details</summary>
Motivation: 当前EEG视觉解码方法面临模态鸿沟问题，需要复杂的多阶段适应过程，且大规模扩散模型计算开销大，限制了其在真实BCI应用中的实用性

Method: 1) 使用预训练EEG模型LaBraM，通过对比学习微调对齐EEG和图像表示；2) 采用基于"下一尺度预测"的自回归生成框架：将图像编码为多尺度token图，训练transformer从EEG嵌入作为最粗表示开始自回归预测更细尺度token

Result: 在两个数据集上，AVDE在图像检索和重建任务上均优于先前SOTA方法，参数量仅为10%。中间输出可视化显示生成过程反映了人类视觉感知的层次性

Conclusion: 自回归模型可作为高效且可解释的工具用于实际BCI应用，AVDE框架在保持EEG信号与重建图像直接连接的同时实现了连贯生成

Abstract: Electroencephalogram (EEG) signals have become a popular medium for decoding visual information due to their cost-effectiveness and high temporal resolution. However, current approaches face significant challenges in bridging the modality gap between EEG and image data. These methods typically rely on complex adaptation processes involving multiple stages, making it hard to maintain consistency and manage compounding errors. Furthermore, the computational overhead imposed by large-scale diffusion models limit their practicality in real-world brain-computer interface (BCI) applications. In this work, we present AVDE, a lightweight and efficient framework for visual decoding from EEG signals. First, we leverage LaBraM, a pre-trained EEG model, and fine-tune it via contrastive learning to align EEG and image representations. Second, we adopt an autoregressive generative framework based on a "next-scale prediction" strategy: images are encoded into multi-scale token maps using a pre-trained VQ-VAE, and a transformer is trained to autoregressively predict finer-scale tokens starting from EEG embeddings as the coarsest representation. This design enables coherent generation while preserving a direct connection between the input EEG signals and the reconstructed images. Experiments on two datasets show that AVDE outperforms previous state-of-the-art methods in both image retrieval and reconstruction tasks, while using only 10% of the parameters. In addition, visualization of intermediate outputs shows that the generative process of AVDE reflects the hierarchical nature of human visual perception. These results highlight the potential of autoregressive models as efficient and interpretable tools for practical BCI applications.

</details>


### [62] [Stable Adaptive Thinking via Advantage Shaping and Length-Aware Gradient Regulation](https://arxiv.org/abs/2602.22556)
*Zihang Xu,Haozhi Xie,Ziqi Miao,Wuxuan Gong,Chen Qian,Lijun Li*

Main category: cs.LG

TL;DR: 提出两阶段框架解决大推理模型在简单查询上的过度思考问题，通过混合微调和自适应强化学习实现稳定准确率-效率权衡


<details>
  <summary>Details</summary>
Motivation: 大推理模型在处理简单查询时存在过度思考问题，现有方法在准确率-效率权衡上不稳定，且对异构推理行为的鲁棒性差

Method: 两阶段框架：1) 混合微调让模型接触思考和不思考两种行为；2) 自适应强化学习，包含保持正确性的优势塑造和长度感知梯度调节

Result: 在Qwen2.5-1.5B和7B模型上，准确率提升最高达+3.7/+3.6个百分点，同时生成token减少40.6%/43.9%，在不同难度和分布外任务上表现出鲁棒性

Conclusion: 提出的两阶段框架有效解决了大推理模型的过度思考问题，实现了稳定的准确率-效率权衡和良好的泛化能力

Abstract: Large reasoning models (LRMs) achieve strong performance through extended reasoning traces, but they often exhibit overthinking behavior for low-complexity queries. Existing efforts to mitigate this issue are fundamentally limited by unstable accuracy-efficiency trade-offs and poor robustness to heterogeneous reasoning behaviors. To address these challenges, we propose a two-stage framework for stable adaptive thinking in LRMs. The framework first applies Hybrid Fine-Tuning to expose the model to both thinking and no-thinking behaviors, establishing well-conditioned initialization. It then performs adaptive reinforcement learning with Correctness-Preserving Advantage Shaping (CPAS) to avoid suppressing correct long-chain reasoning, and Length-Aware Gradient Regulation (LAGR) to stabilize optimization under severe reasoning-length heterogeneity. Extensive experiments on Qwen2.5-1.5B and 7B show consistent improvements over strong baselines, achieving up to +3.7/+3.6 accuracy points while reducing generated tokens by 40.6%/43.9%. Further analyses across varying problem difficulties and out-of-distribution tasks confirm the robustness and generalization of our approach.

</details>


### [63] [Operationalizing Fairness: Post-Hoc Threshold Optimization Under Hard Resource Limits](https://arxiv.org/abs/2602.22560)
*Moirangthem Tiken Singh,Amit Kalita,Sapam Jitu Singh*

Main category: cs.LG

TL;DR: 提出一个后处理、模型无关的阈值优化框架，在严格容量约束下联合平衡安全性、效率和公平性，使用单一全局决策阈值确保法律合规性。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域部署机器学习时，需要在预测安全性和算法公平性之间取得平衡。现有公平性干预措施通常假设资源不受限制，并使用违反反歧视法规的群体特定决策阈值。

Method: 引入后处理、模型无关的阈值优化框架，通过参数化伦理损失函数和有限决策规则，在严格容量约束下联合优化安全性、效率和公平性，强制使用单一全局决策阈值以确保法律合规。

Result: 容量约束主导伦理优先级，严格资源限制在80%以上测试配置中决定最终部署阈值。在25%容量限制下，框架保持高风险识别能力（召回率0.409-0.702），而标准无约束公平性启发式方法效用接近零。

Conclusion: 理论公平性目标必须明确服从于操作容量限制才能实际部署。通过将预测评分与政策评估解耦并严格限制干预率，该框架为利益相关者提供了在资源受限环境中导航不可避免伦理权衡的实用且法律合规的机制。

Abstract: The deployment of machine learning in high-stakes domains requires a balance between predictive safety and algorithmic fairness. However, existing fairness interventions often as- sume unconstrained resources and employ group-specific decision thresholds that violate anti- discrimination regulations. We introduce a post-hoc, model-agnostic threshold optimization framework that jointly balances safety, efficiency, and equity under strict and hard capacity constraints. To ensure legal compliance, the framework enforces a single, global decision thresh- old. We formulated a parameterized ethical loss function coupled with a bounded decision rule that mathematically prevents intervention volumes from exceeding the available resources. An- alytically, we prove the key properties of the deployed threshold, including local monotonicity with respect to ethical weighting and the formal identification of critical capacity regimes. We conducted extensive experimental evaluations on diverse high-stakes datasets. The principal re- sults demonstrate that capacity constraints dominate ethical priorities; the strict resource limit determines the final deployed threshold in over 80% of the tested configurations. Furthermore, under a restrictive 25% capacity limit, the proposed framework successfully maintains high risk identification (recall ranging from 0.409 to 0.702), whereas standard unconstrained fairness heuristics collapse to a near-zero utility. We conclude that theoretical fairness objectives must be explicitly subordinated to operational capacity limits to remain in deployment. By decou- pling predictive scoring from policy evaluation and strictly bounding intervention rates, this framework provides a practical and legally compliant mechanism for stakeholders to navigate unavoidable ethical trade-offs in resource-constrained environments.

</details>


### [64] [S2O: Early Stopping for Sparse Attention via Online Permutation](https://arxiv.org/abs/2602.22575)
*Yu Zhang,Songwei Liu,Chenqian Yan,Sheng Lin,Beichen Ning,Fangmin Chen,Xing Wang*

Main category: cs.LG

TL;DR: S2O通过在线排列和早期停止机制，显著提高了稀疏注意力的实际稀疏上限，在保持准确性的同时大幅减少长上下文推理的计算开销。


<details>
  <summary>Details</summary>
Motivation: 注意力机制的计算复杂度随序列长度呈二次方增长，限制了长上下文推理。现有的块粒度稀疏化方法存在固有的稀疏上限，即使精心设计也难以进一步改进。

Method: S2O采用在线排列和早期停止机制：1) 借鉴内存系统的虚拟到物理地址映射，重新设计FlashAttention执行，支持加载非连续token；2) 将显式排列转化为在线、索引引导的离散加载策略，集中重要性到高优先级块；3) 引入早期停止规则，当块得分低于阈值时终止计算，跳过低贡献块。

Result: 在Llama-3.1-8B的128K上下文下：单算子MSE降低3.82倍（相同稀疏度）；预填充计算密度降低3.31倍（相同MSE）；保持端到端准确性；注意力计算加速7.51倍，端到端加速3.81倍。

Conclusion: S2O通过在线排列和早期停止机制，显著提高了稀疏注意力的实际稀疏上限，在控制误差预算的同时大幅减少了长上下文推理的计算开销，实现了显著的性能提升。

Abstract: Attention scales quadratically with sequence length, fundamentally limiting long-context inference. Existing block-granularity sparsification can reduce latency, but coarse blocks impose an intrinsic sparsity ceiling, making further improvements difficult even with carefully engineered designs. We present S2O, which performs early stopping for sparse attention via online permutation. Inspired by virtual-to-physical address mapping in memory systems, S2O revisits and factorizes FlashAttention execution, enabling inference to load non-contiguous tokens rather than a contiguous span in the original order. Motivated by fine-grained structures in attention heatmaps, we transform explicit permutation into an online, index-guided, discrete loading policy; with extremely lightweight preprocessing and index-remapping overhead, it concentrates importance on a small set of high-priority blocks. Building on this importance-guided online permutation for loading, S2O further introduces an early-stopping rule: computation proceeds from high to low importance; once the current block score falls below a threshold, S2O terminates early and skips the remaining low-contribution blocks, thereby increasing effective sparsity and reducing computation under a controlled error budget.
  As a result, S2O substantially raises the practical sparsity ceiling. On Llama-3.1-8B under a 128K context, S2O reduces single-operator MSE by 3.82$\times$ at matched sparsity, and reduces prefill compute density by 3.31$\times$ at matched MSE; meanwhile, it preserves end-to-end accuracy and achieves 7.51$\times$ attention and 3.81$\times$ end-to-end speedups.

</details>


### [65] [IBCircuit: Towards Holistic Circuit Discovery with Information Bottleneck](https://arxiv.org/abs/2602.22581)
*Tian Bian,Yifan Niu,Chaohao Yuan,Chengzhi Piao,Bingzhe Wu,Long-Kai Huang,Yu Rong,Tingyang Xu,Hong Cheng,Jia Li*

Main category: cs.LG

TL;DR: 提出IBCircuit方法，基于信息瓶颈原理进行端到端的整体电路发现，无需为不同任务设计特定的损坏激活，在IOI和Greater-Than任务中识别出更忠实、更精简的电路


<details>
  <summary>Details</summary>
Motivation: 现有电路发现方法大多忽视电路的整体性，且需要为不同任务设计特定的损坏激活，这种方法既不准确也不高效。需要一种能够整体识别信息电路的方法

Method: 基于信息瓶颈原理提出IBCircuit方法，这是一个用于整体电路发现的优化框架，可以应用于任何给定任务，无需繁琐的损坏激活设计

Result: 在间接宾语识别(IOI)和Greater-Than任务中，IBCircuit在关键节点组件和边组件方面识别出比近期相关工作更忠实、更精简的电路

Conclusion: IBCircuit提供了一种有效的端到端整体电路发现方法，克服了现有方法需要任务特定损坏激活设计的局限性，能够更准确地识别语言模型中的计算子图

Abstract: Circuit discovery has recently attracted attention as a potential research direction to explain the non-trivial behaviors of language models. It aims to find the computational subgraphs, also known as circuits, within the model that are responsible for solving specific tasks. However, most existing studies overlook the holistic nature of these circuits and require designing specific corrupted activations for different tasks, which is inaccurate and inefficient. In this work, we propose an end-to-end approach based on the principle of Information Bottleneck, called IBCircuit, to identify informative circuits holistically. IBCircuit is an optimization framework for holistic circuit discovery and can be applied to any given task without tediously corrupted activation design. In both the Indirect Object Identification (IOI) and Greater-Than tasks, IBCircuit identifies more faithful and minimal circuits in terms of critical node components and edge components compared to recent related work.

</details>


### [66] [TabDLM: Free-Form Tabular Data Generation via Joint Numerical-Language Diffusion](https://arxiv.org/abs/2602.22586)
*Donghong Cai,Jiarui Feng,Yanbo Wang,Da Zheng,Yixin Chen,Muhan Zhang*

Main category: cs.LG

TL;DR: TabDLM：基于掩码扩散语言模型的统一框架，用于生成包含自由文本字段的异构表格数据，通过联合数值-语言扩散模型解决现有方法在文本质量和数值精度上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现实世界表格数据越来越多地包含自由文本字段（如评论、临床笔记）以及结构化数值和分类属性。生成这种异构表格并联合建模不同模态具有挑战性。现有方法分为扩散模型和LLM模型两类：扩散模型能捕捉数值和分类特征的复杂依赖关系，但扩展到开放文本时文本质量下降；LLM模型能生成流畅文本，但其离散化标记会扭曲精确或大范围数值，难以准确建模数字和语言。

Method: 提出TabDLM框架，基于掩码扩散语言模型构建联合数值-语言扩散模型。通过掩码扩散建模文本和分类特征，通过学习的专用数值标记嵌入对数值特征进行连续扩散过程建模，双向注意力机制在单一模型中捕捉跨模态交互。

Result: 在多样化基准测试上的广泛实验表明，TabDLM相比强大的扩散模型和LLM基线方法具有有效性。

Conclusion: TabDLM为自由形式表格数据生成提供了一个统一的框架，通过联合数值-语言扩散模型解决了现有方法在文本质量和数值精度上的局限性，能够有效生成包含自由文本字段的异构表格数据。

Abstract: Synthetic tabular data generation has attracted growing attention due to its importance for data augmentation, foundation models, and privacy. However, real-world tabular datasets increasingly contain free-form text fields (e.g., reviews or clinical notes) alongside structured numerical and categorical attributes. Generating such heterogeneous tables with joint modeling of different modalities remains challenging. Existing approaches broadly fall into two categories: diffusion-based methods and LLM-based methods. Diffusion models can capture complex dependencies over numerical and categorical features in continuous or discrete spaces, but extending them to open-ended text is nontrivial and often leads to degraded text quality. In contrast, LLM-based generators naturally produce fluent text, yet their discrete tokenization can distort precise or wide-range numerical values, hindering accurate modeling of both numbers and language. In this work, we propose TabDLM, a unified framework for free-form tabular data generation via a joint numerical--language diffusion model built on masked diffusion language models (MDLMs). TabDLM models textual and categorical features through masked diffusion, while modeling numerical features with a continuous diffusion process through learned specialized numeric tokens embedding; bidirectional attention then captures cross-modality interactions within a single model. Extensive experiments on diverse benchmarks demonstrate the effectiveness of TabDLM compared to strong diffusion- and LLM-based baselines.

</details>


### [67] [pQuant: Towards Effective Low-Bit Language Models via Decoupled Linear Quantization-Aware Training](https://arxiv.org/abs/2602.22592)
*Wenzheng Zhang,Bingzheng Liu,Yang Hu,Xiaoying Bai,Wentao Zhang,Bin Cui*

Main category: cs.LG

TL;DR: pQuant提出了一种新的量化感知训练方法，通过将线性层拆分为1-bit主分支和高精度分支来解决参数民主化问题，在极低比特量化中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有极低比特（sub 2-bit）量化方法存在参数民主化效应问题，即所有参数的敏感性变得同质化，严重限制了模型的表达能力，导致准确性和可扩展性不足。

Method: 提出pQuant方法：1）将线性层拆分为两个专门分支：1-bit主分支用于高效计算，紧凑高精度分支用于保留最敏感参数；2）通过特征缩放引导模型将敏感参数分配到高精度分支；3）将高精度分支扩展为多个稀疏激活的专家，实现高效容量扩展。

Result: 大量实验表明，pQuant在极低比特量化中实现了最先进的性能。

Conclusion: pQuant通过参数解耦和专门化分支设计，有效解决了极低比特量化中的参数民主化问题，为边缘部署提供了高效的大语言模型解决方案。

Abstract: Quantization-Aware Training from scratch has emerged as a promising approach for building efficient large language models (LLMs) with extremely low-bit weights (sub 2-bit), which can offer substantial advantages for edge deployment. However, existing methods still fail to achieve satisfactory accuracy and scalability. In this work, we identify a parameter democratization effect as a key bottleneck: the sensitivity of all parameters becomes homogenized, severely limiting expressivity. To address this, we propose pQuant, a method that decouples parameters by splitting linear layers into two specialized branches: a dominant 1-bit branch for efficient computation and a compact high-precision branch dedicated to preserving the most sensitive parameters. Through tailored feature scaling, we explicitly guide the model to allocate sensitive parameters to the high-precision branch. Furthermore, we extend this branch into multiple, sparsely-activated experts, enabling efficient capacity scaling. Extensive experiments indicate our pQuant achieves state-of-the-art performance in extremely low-bit quantization.

</details>


### [68] [Transformers converge to invariant algorithmic cores](https://arxiv.org/abs/2602.22600)
*Joshua S. Schiffman*

Main category: cs.LG

TL;DR: 论文提出"算法核心"概念——紧凑的子空间，这些子空间对任务性能是必要且充分的，揭示了不同训练运行的Transformer模型会收敛到相同的核心结构


<details>
  <summary>Details</summary>
Motivation: 大语言模型表现出复杂能力，但理解其内部工作机制仍是核心挑战。主要障碍在于训练选择的是行为而非电路，因此许多权重配置可以实现相同功能。需要区分哪些内部结构反映计算本质，哪些是特定训练运行的偶然产物

Method: 提取"算法核心"——紧凑的子空间，这些子空间对任务性能是必要且充分的。通过分析不同训练运行的Transformer模型，包括马尔可夫链Transformer、模加Transformer和GPT-2语言模型

Result: 1) 独立训练的Transformer学习不同权重但收敛到相同核心；2) 马尔可夫链Transformer在近乎正交的子空间中嵌入3D核心，恢复相同的转移谱；3) 模加Transformer在"顿悟"时发现紧凑循环算子，后来膨胀，提供了记忆到泛化转变的预测模型；4) GPT-2通过单个轴控制主谓一致，翻转该轴会在所有尺度上反转语法数

Conclusion: Transformer计算围绕紧凑、共享的算法结构组织，这些低维不变量在不同训练运行和尺度上持续存在。机械可解释性应针对这些计算本质（不变量），而非实现特定的细节

Abstract: Large language models exhibit sophisticated capabilities, yet understanding how they work internally remains a central challenge. A fundamental obstacle is that training selects for behavior, not circuitry, so many weight configurations can implement the same function. Which internal structures reflect the computation, and which are accidents of a particular training run? This work extracts algorithmic cores: compact subspaces necessary and sufficient for task performance. Independently trained transformers learn different weights but converge to the same cores. Markov-chain transformers embed 3D cores in nearly orthogonal subspaces yet recover identical transition spectra. Modular-addition transformers discover compact cyclic operators at grokking that later inflate, yielding a predictive model of the memorization-to-generalization transition. GPT-2 language models govern subject-verb agreement through a single axis that, when flipped, inverts grammatical number throughout generation across scales. These results reveal low-dimensional invariants that persist across training runs and scales, suggesting that transformer computations are organized around compact, shared algorithmic structures. Mechanistic interpretability could benefit from targeting such invariants -- the computational essence -- rather than implementation-specific details.

</details>


### [69] [$φ$-DPO: Fairness Direct Preference Optimization Approach to Continual Learning in Large Multimodal Models](https://arxiv.org/abs/2602.22601)
*Thanh-Dat Truong,Huu-Thien Tran,Jackson Cothren,Bhiksha Raj,Khoa Luu*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的公平性直接偏好优化框架（FaiDPO/φ-DPO），用于解决大型多模态模型持续学习中的数据不平衡导致的公平性问题，同时缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型持续学习中的公平性是一个新兴但尚未充分探索的挑战，特别是在数据分布不平衡的情况下，这会导致模型更新偏差和跨任务性能不佳。现有持续学习研究主要关注灾难性遗忘，但数据不平衡引起的公平性问题仍被忽视。

Method: 首先提出基于直接偏好优化的新持续学习范式，通过成对偏好信号对齐学习来缓解灾难性遗忘。然后针对传统DPO在数据不平衡中的局限性，提出新的φ-DPO损失函数，明确处理分布偏差。还提供了理论分析证明该方法能同时解决遗忘和数据不平衡问题，并为现有基准构建了成对偏好标注。

Result: 广泛的实验和消融研究表明，提出的φ-DPO在多个基准测试中达到了最先进的性能，优于先前的大型多模态模型持续学习方法。

Conclusion: 该研究成功解决了大型多模态模型持续学习中的公平性问题，提出的φ-DPO框架不仅能有效缓解灾难性遗忘，还能处理数据不平衡导致的分布偏差，为公平的持续学习提供了新的解决方案。

Abstract: Fairness in Continual Learning for Large Multimodal Models (LMMs) is an emerging yet underexplored challenge, particularly in the presence of imbalanced data distributions that can lead to biased model updates and suboptimal performance across tasks. While recent continual learning studies have made progress in addressing catastrophic forgetting, the problem of fairness caused the imbalanced data remains largely underexplored. This paper presents a novel Fairness Direct Preference Optimization (FaiDPO or $φ$-DPO) framework for continual learning in LMMs. In particular, we first propose a new continual learning paradigm based on Direct Preference Optimization (DPO) to mitigate catastrophic forgetting by aligning learning with pairwise preference signals. Then, we identify the limitations of conventional DPO in imbalanced data and present a new $φ$-DPO loss that explicitly addresses distributional biases. We provide a comprehensive theoretical analysis demonstrating that our approach addresses both forgetting and data imbalance. Additionally, to enable $φ$-DPO-based continual learning, we construct pairwise preference annotations for existing benchmarks in the context of continual learning. Extensive experiments and ablation studies show the proposed $φ$-DPO achieves State-of-the-Art performance across multiple benchmarks, outperforming prior continual learning methods of LMMs.

</details>


### [70] [DP-aware AdaLN-Zero: Taming Conditioning-Induced Heavy-Tailed Gradients in Differentially Private Diffusion](https://arxiv.org/abs/2602.22610)
*Tao Huang,Jiayang Meng,Xu Yang,Chen Hou,Hong Chen*

Main category: cs.LG

TL;DR: 提出DP-aware AdaLN-Zero，一种针对条件扩散模型的敏感度感知条件机制，通过限制条件表示幅度和调制参数来减少DP-SGD中的梯度重尾问题，提升隐私保护下的时间序列任务性能。


<details>
  <summary>Details</summary>
Motivation: 在差分隐私随机梯度下降（DP-SGD）中，异构条件上下文（如观测历史、缺失模式或异常协变量）会导致重尾的每样本梯度。这些罕见但重尾的梯度会过度触发全局裁剪，导致异常值主导的更新、更大的裁剪偏差，以及在固定隐私预算下降低的效用。

Method: 提出DP-aware AdaLN-Zero，一种即插即用的敏感度感知条件机制，用于条件扩散变换器。该方法通过有界重新参数化联合约束条件表示幅度和AdaLN调制参数，在梯度裁剪和噪声注入之前抑制极端梯度尾部事件。

Result: 在匹配隐私设置下，配备DP-aware AdaLN-Zero的DP-SGD在插值/填补和预测任务上表现更优。在真实世界电力数据集和两个公开ETT基准上，相比原始DP-SGD获得了一致的性能提升。梯度诊断显示这些改进归因于条件特定的尾部重塑和减少的裁剪失真，同时在非私有训练中保持了表达能力。

Conclusion: 敏感度感知条件机制可以显著改善私有条件扩散训练，而无需牺牲标准性能，为隐私保护下的时间序列建模提供了有效的解决方案。

Abstract: Condition injection enables diffusion models to generate context-aware outputs, which is essential for many time-series tasks. However, heterogeneous conditional contexts (e.g., observed history, missingness patterns or outlier covariates) can induce heavy-tailed per-example gradients. Under Differentially Private Stochastic Gradient Descent (DP-SGD), these rare conditioning-driven heavy-tailed gradients disproportionately trigger global clipping, resulting in outlier-dominated updates, larger clipping bias, and degraded utility under a fixed privacy budget. In this paper, we propose DP-aware AdaLN-Zero, a drop-in sensitivity-aware conditioning mechanism for conditional diffusion transformers that limits conditioning-induced gain without modifying the DP-SGD mechanism. DP-aware AdaLN-Zero jointly constrains conditioning representation magnitude and AdaLN modulation parameters via bounded re-parameterization, suppressing extreme gradient tail events before gradient clipping and noise injection. Empirically, DP-SGD equipped with DP-aware AdaLN-Zero improves interpolation/imputation and forecasting under matched privacy settings. We observe consistent gains on a real-world power dataset and two public ETT benchmarks over vanilla DP-SGD. Moreover, gradient diagnostics attribute these improvements to conditioning-specific tail reshaping and reduced clipping distortion, while preserving expressiveness in non-private training. Overall, these results show that sensitivity-aware conditioning can substantially improve private conditional diffusion training without sacrificing standard performance.

</details>


### [71] [Mitigating Membership Inference in Intermediate Representations via Layer-wise MIA-risk-aware DP-SGD](https://arxiv.org/abs/2602.22611)
*Jiayang Meng,Tao Huang,Chen Hou,Guolong Zheng,Hong Chen*

Main category: cs.LG

TL;DR: LM-DP-SGD：一种分层感知的差分隐私训练方法，根据各层对成员推理攻击的脆弱性自适应分配隐私保护，在相同隐私预算下实现更好的隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 在嵌入即接口场景中，预训练模型的中间表示会泄露训练集成员信息，现有DP-SGD采用统一的噪声乘子，忽略了不同层对成员推理攻击的脆弱性差异。

Method: 提出分层MIA风险感知的DP-SGD：1）在公开影子数据集上训练影子模型；2）提取各层中间表示并训练分层MIA攻击器；3）用攻击错误率作为MIA风险估计；4）根据风险重新加权各层梯度贡献，在固定噪声幅度下提供分层适当的保护。

Result: 在相同隐私预算下，LM-DP-SGD降低了峰值中间表示级别的MIA风险，同时保持了模型效用，实现了更优的隐私-效用权衡。

Conclusion: LM-DP-SGD通过分层自适应隐私保护分配，解决了传统DP-SGD层间脆弱性差异问题，在理论保证下实现了更好的隐私保护效果。

Abstract: In Embedding-as-an-Interface (EaaI) settings, pre-trained models are queried for Intermediate Representations (IRs). The distributional properties of IRs can leak training-set membership signals, enabling Membership Inference Attacks (MIAs) whose strength varies across layers. Although Differentially Private Stochastic Gradient Descent (DP-SGD) mitigates such leakage, existing implementations employ per-example gradient clipping and a uniform, layer-agnostic noise multiplier, ignoring heterogeneous layer-wise MIA vulnerability. This paper introduces Layer-wise MIA-risk-aware DP-SGD (LM-DP-SGD), which adaptively allocates privacy protection across layers in proportion to their MIA risk. Specifically, LM-DP-SGD trains a shadow model on a public shadow dataset, extracts per-layer IRs from its train/test splits, and fits layer-specific MIA adversaries, using their attack error rates as MIA-risk estimates. Leveraging the cross-dataset transferability of MIAs, these estimates are then used to reweight each layer's contribution to the globally clipped gradient during private training, providing layer-appropriate protection under a fixed noise magnitude. We further establish theoretical guarantees on both privacy and convergence of LM-DP-SGD. Extensive experiments show that, under the same privacy budget, LM-DP-SGD reduces the peak IR-level MIA risk while preserving utility, yielding a superior privacy-utility trade-off.

</details>


### [72] [Semantic Tube Prediction: Beating LLM Data Efficiency with JEPA](https://arxiv.org/abs/2602.22617)
*Hai Huang,Yann LeCun,Randall Balestriero*

Main category: cs.LG

TL;DR: 提出Geodesic Hypothesis和Semantic Tube Prediction (STP)任务，通过几何先验约束隐藏状态轨迹，使LLM仅用1/16的训练数据就能达到基线准确率，突破了Chinchilla式缩放定律的数据效率限制。


<details>
  <summary>Details</summary>
Motivation: 现有LLM缩放定律是描述性的而非规定性的，它们描述了典型训练而非最优训练。很少有工作成功挑战这些定律所隐含的数据效率边界，这是本文的主要关注点。

Method: 提出Geodesic Hypothesis，假设token序列在平滑语义流形上追踪测地线，因此是局部线性的。基于此提出Semantic Tube Prediction (STP)任务，这是一种JEPA风格的规范化器，将隐藏状态轨迹限制在测地线的管状邻域内。STP将JEPA推广到语言领域，无需显式的多视图增强。

Result: STP使LLM在NL-RX-SYNTH数据集上仅用1/16的训练数据就能匹配基线准确率，直接违反了Chinchilla式缩放定律的数据项，证明了原理性几何先验可以超越暴力缩放。

Conclusion: 通过Geodesic Hypothesis和STP任务，展示了原理性几何先验能够显著提高LLM的数据效率，突破现有缩放定律的限制，为更高效的语言模型训练提供了新方向。

Abstract: Large Language Models (LLMs) obey consistent scaling laws -- empirical power-law fits that predict how loss decreases with compute, data, and parameters. While predictive, these laws are descriptive rather than prescriptive: they characterize typical training, not optimal training. Surprisingly few works have successfully challenged the data-efficiency bounds implied by these laws -- which is our primary focus. To that end, we introduce the Geodesic Hypothesis, positing that token sequences trace geodesics on a smooth semantic manifold and are therefore locally linear. Building on this principle, we propose a novel Semantic Tube Prediction (STP) task, a JEPA-style regularizer that confines hidden-state trajectories to a tubular neighborhood of the geodesic. STP generalizes JEPA to language without requiring explicit multi-view augmentations. We show this constraint improves signal-to-noise ratio, and consequently preserves diversity by preventing trajectory collisions during inference. Empirically, STP allows LLMs to match baseline accuracy with 16$\times$ less training data on the NL-RX-SYNTH dataset, directly violating the data term of Chinchilla-style scaling laws and demonstrating that principled geometric priors can surpass brute-force scaling. Code is available at https://github.com/galilai-group/llm-jepa#stp.

</details>


### [73] [ContextRL: Enhancing MLLM's Knowledge Discovery Efficiency with Context-Augmented RL](https://arxiv.org/abs/2602.22623)
*Xingyu Lu,Jinpeng Wang,YiFan Zhang,Shijie Ma,Xiao Hu,Tianke Zhang,Haonan fan,Kaiyu Jiang,Changyi Liu,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Chun Yuan*

Main category: cs.LG

TL;DR: ContextRL通过上下文增强解决强化学习中的可识别性和可达性问题，使用完整参考方案进行细粒度验证，并采用多轮采样策略从失败尝试中恢复正确响应，显著提升知识发现效率。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中的两个关键瓶颈：可识别性（无法区分正确答案但低质量推理过程）和可达性（从全负样本组中恢复正确响应），这些限制影响了知识发现效率。

Method: 1) 为奖励模型提供完整参考方案作为上下文，实现细粒度过程验证以过滤假阳性样本；2) 引入多轮采样策略，奖励模型为失败尝试生成错误报告，指导策略从先前全负样本组中"恢复"正确响应。

Result: 在11个感知和推理基准测试中，ContextRL显著提升知识发现效率。Qwen3-VL-8B模型性能达到与32B模型相当水平，大幅超越标准RLVR基线，同时有效缓解奖励黑客攻击。

Conclusion: 上下文信息对提升奖励模型准确性具有显著潜力，研究记录了奖励黑客攻击的普遍存在，为未来RLVR研究提供了宝贵见解。ContextRL框架为解决强化学习中的关键瓶颈提供了有效方案。

Abstract: We propose ContextRL, a novel framework that leverages context augmentation to overcome these bottlenecks. Specifically, to enhance Identifiability, we provide the reward model with full reference solutions as context, enabling fine-grained process verification to filter out false positives (samples with the right answer but low-quality reasoning process). To improve Reachability, we introduce a multi-turn sampling strategy where the reward model generates mistake reports for failed attempts, guiding the policy to "recover" correct responses from previously all-negative groups. Experimental results on 11 perception and reasoning benchmarks show that ContextRL significantly improves knowledge discovery efficiency. Notably, ContextRL enables the Qwen3-VL-8B model to achieve performance comparable to the 32B model, outperforming standard RLVR baselines by a large margin while effectively mitigating reward hacking. Our in-depth analysis reveals the significant potential of contextual information for improving reward model accuracy and document the widespread occurrence of reward hacking, offering valuable insights for future RLVR research.

</details>


### [74] [Tackling Privacy Heterogeneity in Differentially Private Federated Learning](https://arxiv.org/abs/2602.22633)
*Ruichen Xu,Ying-Jun Angela Zhang,Jianwei Huang*

Main category: cs.LG

TL;DR: 该论文提出了首个针对差分隐私联邦学习中隐私异构性问题的系统性研究，通过理论分析和优化方法设计了隐私感知的客户端选择策略，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私联邦学习方法假设所有客户端共享统一的隐私预算，但现实场景中隐私需求存在显著差异。这种隐私异构性导致传统基于数据量的客户端选择策略无法区分高质量更新和因严格隐私约束引入大量噪声的客户端，从而影响模型性能。

Method: 首先建立理论基础，通过收敛分析量化隐私异构性对训练误差的影响。基于此分析，提出隐私感知的客户端选择策略，将其表述为凸优化问题，自适应调整选择概率以最小化训练误差。

Result: 在基准数据集上的实验表明，在异构隐私预算下，该方法在CIFAR-10上相比现有基线实现了高达10%的测试准确率提升。

Conclusion: 研究强调了将隐私异构性纳入客户端选择对于实现实用有效联邦学习的重要性，提出的方法为解决隐私异构性挑战提供了系统性的解决方案。

Abstract: Differentially private federated learning (DP-FL) enables clients to collaboratively train machine learning models while preserving the privacy of their local data. However, most existing DP-FL approaches assume that all clients share a uniform privacy budget, an assumption that does not hold in real-world scenarios where privacy requirements vary widely. This privacy heterogeneity poses a significant challenge: conventional client selection strategies, which typically rely on data quantity, cannot distinguish between clients providing high-quality updates and those introducing substantial noise due to strict privacy constraints. To address this gap, we present the first systematic study of privacy-aware client selection in DP-FL. We establish a theoretical foundation by deriving a convergence analysis that quantifies the impact of privacy heterogeneity on training error. Building on this analysis, we propose a privacy-aware client selection strategy, formulated as a convex optimization problem, that adaptively adjusts selection probabilities to minimize training error. Extensive experiments on benchmark datasets demonstrate that our approach achieves up to a 10% improvement in test accuracy on CIFAR-10 compared to existing baselines under heterogeneous privacy budgets. These results highlight the importance of incorporating privacy heterogeneity into client selection for practical and effective federated learning.

</details>


### [75] [Compress the Easy, Explore the Hard: Difficulty-Aware Entropy Regularization for Efficient LLM Reasoning](https://arxiv.org/abs/2602.22642)
*Qin-Wen Luo,Sheng Ren,Xiang Chen,Rui Liu,Jun Fang,Naiqiang Tan,Sheng-Jun Huang*

Main category: cs.LG

TL;DR: CEEH提出了一种难度感知的强化学习方法，通过选择性熵正则化来压缩推理步骤，在保持准确性的同时减少响应长度。


<details>
  <summary>Details</summary>
Motivation: 现有CoT方法虽然增强了LLMs的复杂推理能力，但冗长的推理步骤导致推理延迟和计算成本过高，限制了实际部署。现有的压缩方法往往为了简洁而牺牲推理能力。

Method: 提出CEEH方法：1）动态评估实例难度，对困难问题保留多样化的搜索空间以确保鲁棒性，对简单问题允许激进压缩；2）引入基于历史最短正确响应的动态最优长度惩罚，抵消熵引起的长度膨胀并稳定奖励信号。

Result: 在六个推理基准测试中，CEEH持续减少响应长度，同时保持与基础模型相当的准确性，并相对于仅优化长度的方法提高了Pass@k。

Conclusion: CEEH通过难度感知的熵正则化和动态长度惩罚，有效解决了RL压缩方法中的熵崩溃问题，在保持推理能力的同时实现了高效的推理压缩。

Abstract: Chain-of-Thought (CoT) has substantially empowered Large Language Models (LLMs) to tackle complex reasoning tasks, yet the verbose nature of explicit reasoning steps incurs prohibitive inference latency and computational costs, limiting real-world deployment. While existing compression methods - ranging from self-training to Reinforcement Learning (RL) with length constraints - attempt to mitigate this, they often sacrifice reasoning capability for brevity. We identify a critical failure mode in these approaches: explicitly optimizing for shorter trajectories triggers rapid entropy collapse, which prematurely shrinks the exploration space and stifles the discovery of valid reasoning paths, particularly for challenging questions requiring extensive deduction. To address this issue, we propose Compress responses for Easy questions and Explore Hard ones (CEEH), a difficulty-aware approach to RL-based efficient reasoning. CEEH dynamically assesses instance difficulty to apply selective entropy regularization: it preserves a diverse search space for currently hard questions to ensure robustness, while permitting aggressive compression on easier instances where the reasoning path is well-established. In addition, we introduce a dynamic optimal-length penalty anchored to the historically shortest correct response, which effectively counteracts entropy-induced length inflation and stabilizes the reward signal. Across six reasoning benchmarks, CEEH consistently reduces response length while maintaining accuracy comparable to the base model, and improves Pass@k relative to length-only optimization.

</details>


### [76] [MUG: Meta-path-aware Universal Heterogeneous Graph Pre-Training](https://arxiv.org/abs/2602.22645)
*Lianze Shan,Jitao Zhao,Dongxiao He,Yongqi Huang,Zhiyong Feng,Weixiong Zhang*

Main category: cs.LG

TL;DR: 提出MUG方法解决异质图通用预训练难题，通过输入统一模块和共享编码器处理异质图的结构语义多样性，实现跨数据集的有效迁移。


<details>
  <summary>Details</summary>
Motivation: 当前通用图预训练主要关注同质图，而异质图具有更复杂的结构和语义多样性，使得构建统一编码器面临两大挑战：1) 节点和关系类型的多样性阻碍统一表示空间的构建；2) 元路径的数量和语义在不同数据集中变化，导致学习到的编码和聚合模式难以迁移。

Method: 提出MUG方法：1) 输入统一模块将异质图中多类型节点和关系信息整合为统一表示，通过维度感知编码器投影到共享空间；2) 训练共享编码器捕获跨不同元路径视图的一致结构模式，而非依赖数据集特定的聚合策略；3) 全局目标函数增强判别性并减少数据集特定偏差。

Result: 在多个真实数据集上的广泛实验证明了MUG方法的有效性，能够成功处理异质图的复杂性和多样性，实现跨数据集的通用预训练。

Conclusion: MUG是首个针对异质图的通用预训练方法，通过创新的输入统一和共享编码器设计，成功解决了异质图预训练中的结构语义多样性挑战，为异质图表示学习提供了新的有效范式。

Abstract: Universal graph pre-training has emerged as a key paradigm in graph representation learning, offering a promising way to train encoders to learn transferable representations from unlabeled graphs and to effectively generalize across a wide range of downstream tasks. However, recent explorations in universal graph pre-training primarily focus on homogeneous graphs and it remains unexplored for heterogeneous graphs, which exhibit greater structural and semantic complexity. This heterogeneity makes it highly challenging to train a universal encoder for diverse heterogeneous graphs: (i) the diverse types with dataset-specific semantics hinder the construction of a unified representation space; (ii) the number and semantics of meta-paths vary across datasets, making encoding and aggregation patterns learned from one dataset difficult to apply to others. To address these challenges, we propose a novel Meta-path-aware Universal heterogeneous Graph pre-training (MUG) approach. Specifically, for challenge (i), MUG introduces a input unification module that integrates information from multiple node and relation types within each heterogeneous graph into a unified representation.This representation is then projected into a shared space by a dimension-aware encoder, enabling alignment across graphs with diverse schemas.Furthermore, for challenge (ii), MUG trains a shared encoder to capture consistent structural patterns across diverse meta-path views rather than relying on dataset-specific aggregation strategies, while a global objective encourages discriminability and reduces dataset-specific biases. Extensive experiments demonstrate the effectiveness of MUG on some real datasets.

</details>


### [77] [LEDA: Latent Semantic Distribution Alignment for Multi-domain Graph Pre-training](https://arxiv.org/abs/2602.22660)
*Lianze Shan,Jitao Zhao,Dongxiao He,Siqi Liu,Jiaxu Cui,Weixiong Zhang*

Main category: cs.LG

TL;DR: 提出LEDA模型，通过潜在语义分布对齐解决通用图预训练中的语义对齐和训练指导不足问题，在跨域少样本场景中表现优异


<details>
  <summary>Details</summary>
Motivation: 现有通用图预训练方法面临两大挑战：1）简单数据对齐无法处理高度多样化的图数据，导致语义错位；2）将域内预训练范式任意应用于跨域场景，难以从多个图中捕获有效知识

Method: 提出潜在语义分布对齐（LEDA）模型：1）维度投影单元自适应对齐不同域特征到共享语义空间；2）变分语义推理模块获取共享潜在分布；3）用该分布指导域投影，确保跨域语义学习

Result: LEDA在广泛的图和下游任务中表现强劲，在少样本跨域设置中显著优于域内基线和先进的通用预训练模型

Conclusion: LEDA通过有效的跨域语义对齐和分布指导，解决了通用图预训练中的关键挑战，为跨域图学习提供了有效解决方案

Abstract: Recent advances in generic large models, such as GPT and DeepSeek, have motivated the introduction of universality to graph pre-training, aiming to learn rich and generalizable knowledge across diverse domains using graph representations to improve performance in various downstream applications. However, most existing methods face challenges in learning effective knowledge from generic graphs, primarily due to simplistic data alignment and limited training guidance. The issue of simplistic data alignment arises from the use of a straightforward unification for highly diverse graph data, which fails to align semantics and misleads pre-training models. The problem with limited training guidance lies in the arbitrary application of in-domain pre-training paradigms to cross-domain scenarios. While it is effective in enhancing discriminative representation in one data space, it struggles to capture effective knowledge from many graphs. To address these challenges, we propose a novel Latent sEmantic Distribution Alignment (LEDA) model for universal graph pre-training. Specifically, we first introduce a dimension projection unit to adaptively align diverse domain features into a shared semantic space with minimal information loss. Furthermore, we design a variational semantic inference module to obtain the shared latent distribution. The distribution is then adopted to guide the domain projection, aligning it with shared semantics across domains and ensuring cross-domain semantic learning. LEDA exhibits strong performance across a broad range of graphs and downstream tasks. Remarkably, in few-shot cross-domain settings, it significantly outperforms in-domain baselines and advanced universal pre-training models.

</details>


### [78] [Forecasting Antimicrobial Resistance Trends Using Machine Learning on WHO GLASS Surveillance Data: A Retrieval-Augmented Generation Approach for Policy Decision Support](https://arxiv.org/abs/2602.22673)
*Md Tanvir Hasan Turja*

Main category: cs.LG

TL;DR: 提出一个两组件框架，结合机器学习预测抗菌素耐药性趋势和RAG系统提供循证政策建议，XGBoost在WHO GLASS数据上表现最佳


<details>
  <summary>Details</summary>
Motivation: 抗菌素耐药性（AMR）是全球性危机，预计到205年每年导致1000万人死亡。虽然WHO GLASS系统提供标准化监测数据，但很少有研究应用机器学习从这些数据中预测人口水平的耐药趋势。需要开发预测工具和政策决策支持系统来应对这一挑战。

Method: 提出两组件框架：1）使用六种模型（Naive、线性回归、岭回归、XGBoost、LightGBM、LSTM）在5,909个WHO GLASS观测数据上进行AMR趋势预测基准测试；2）实现检索增强生成（RAG）管道，结合ChromaDB向量存储的WHO政策文档和本地部署的Phi-3 Mini语言模型，生成有来源引用、幻觉受限的政策答案。

Result: XGBoost表现最佳，测试MAE为7.07%，R平方为0.854，比朴素基线提高83.1%。特征重要性分析显示前一年耐药率是最重要的预测因子（50.5%重要性）。区域MAE范围从4.16%（欧洲区域）到10.14%（东南亚区域）。RAG系统能够生成有来源引用的政策建议。

Conclusion: 该研究成功开发了一个结合机器学习预测和RAG政策支持的框架，为AMR监测和干预提供了有效工具。XGBoost在预测AMR趋势方面表现优异，而RAG系统能够提供基于证据的政策建议，有助于制定更有效的抗菌素耐药性应对策略。

Abstract: Antimicrobial resistance (AMR) is a growing global crisis projected to cause 10 million deaths per year by 2050. While the WHO Global Antimicrobial Resistance and Use Surveillance System (GLASS) provides standardized surveillance data across 44 countries, few studies have applied machine learning to forecast population-level resistance trends from this data. This paper presents a two-component framework for AMR trend forecasting and evidence-grounded policy decision support. We benchmark six models -- Naive, Linear Regression, Ridge Regression, XGBoost, LightGBM, and LSTM -- on 5,909 WHO GLASS observations across six WHO regions (2021-2023). XGBoost achieved the best performance with a test MAE of 7.07% and R-squared of 0.854, outperforming the naive baseline by 83.1%. Feature importance analysis identified the prior-year resistance rate as the dominant predictor (50.5% importance), while regional MAE ranged from 4.16% (European Region) to 10.14% (South-East Asia Region). We additionally implemented a Retrieval-Augmented Generation (RAG) pipeline combining a ChromaDB vector store of WHO policy documents with a locally deployed Phi-3 Mini language model, producing source-attributed, hallucination-constrained policy answers. Code and data are available at https://github.com/TanvirTurja

</details>


### [79] [Accelerating LLM Pre-Training through Flat-Direction Dynamics Enhancement](https://arxiv.org/abs/2602.22681)
*Shuchen Zhu,Rizhen Hu,Mingze Wang,Mou Sun,Xue Wang,Kun Yuan,Zaiwen Wen*

Main category: cs.LG

TL;DR: LITE是一种广义加速策略，通过沿平坦轨迹应用更大的Hessian阻尼系数和学习率，显著加速Muon和SOAP等优化器在LLM预训练中的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型预训练需要巨大的计算资源，优化器效率至关重要。当前矩阵基优化器（如Muon和SOAP）虽然利用细粒度曲率信息优于AdamW，但其更新趋于各向同性——沿平坦方向相对保守，沿陡峭方向可能过于激进。

Method: 首先建立统一的黎曼常微分方程框架，阐明自适应算法如何协同工作：预处理器诱导黎曼几何缓解病态条件，动量作为黎曼阻尼项促进收敛。基于这些见解，提出LITE加速策略，沿平坦轨迹应用更大的Hessian阻尼系数和学习率。

Result: 大量实验表明，LITE显著加速了Muon和SOAP在不同架构（密集、MoE）、参数规模（130M-1.3B）、数据集（C4、Pile）和学习率调度（余弦、warmup-stable-decay）下的训练。理论分析证实LITE在各向异性景观中沿平坦方向实现更快收敛。

Conclusion: LITE为高效LLM预训练提供了原则性方法，通过增强平坦方向的训练动态来加速收敛，解决了现有矩阵基优化器的各向同性限制问题。

Abstract: Pre-training Large Language Models requires immense computational resources, making optimizer efficiency essential. The optimization landscape is highly anisotropic, with loss reduction driven predominantly by progress along flat directions. While matrix-based optimizers such as Muon and SOAP leverage fine-grained curvature information to outperform AdamW, their updates tend toward isotropy -- relatively conservative along flat directions yet potentially aggressive along sharp ones. To address this limitation, we first establish a unified Riemannian Ordinary Differential Equation (ODE) framework that elucidates how common adaptive algorithms operate synergistically: the preconditioner induces a Riemannian geometry that mitigates ill-conditioning, while momentum serves as a Riemannian damping term that promotes convergence. Guided by these insights, we propose LITE, a generalized acceleration strategy that enhances training dynamics by applying larger Hessian damping coefficients and learning rates along flat trajectories. Extensive experiments demonstrate that LITE significantly accelerates both Muon and SOAP across diverse architectures (Dense, MoE), parameter scales (130M--1.3B), datasets (C4, Pile), and learning-rate schedules (cosine, warmup-stable-decay). Theoretical analysis confirms that LITE facilitates faster convergence along flat directions in anisotropic landscapes, providing a principled approach to efficient LLM pre-training. The code is available at https://github.com/SHUCHENZHU/LITE.

</details>


### [80] [Switch-Hurdle: A MoE Encoder with AR Hurdle Decoder for Intermittent Demand Forecasting](https://arxiv.org/abs/2602.22685)
*Fabian Muşat,Simona Căbuz*

Main category: cs.LG

TL;DR: Switch-Hurdle：一种用于间歇性需求预测的新框架，结合了混合专家编码器和Hurdle概率解码器，将销售预测分解为是否销售和销售多少两个子问题，在M5基准和零售数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 间歇性需求（长时间零销售与零星非零值交替）是零售和供应链预测中的持续挑战。传统方法（ARIMA、指数平滑、Croston变体）和现代神经网络（DeepAR、Transformer）在处理此类数据时表现不佳，因为它们将需求视为单一连续过程，或在处理大量稀疏序列时计算成本过高。

Method: Switch-Hurdle框架包含：1）混合专家编码器，在前向传播中使用稀疏Top-1专家路由，在反向传播中通过直通估计器实现近似密集；2）Hurdle概率解码器，采用交叉注意力自回归设计，包含共享的hurdle头，将预测任务明确分解为：a) 估计销售概率的二元分类组件，b) 给定销售发生时的数量预测条件回归组件。

Result: 在M5基准和大型专有零售数据集上的实证结果表明，Switch-Hurdle实现了最先进的预测性能，同时保持了可扩展性。

Conclusion: Switch-Hurdle通过结构化分离捕获间歇性需求固有的发生和数量过程，有效解决了间歇性需求预测的挑战，在性能和可扩展性方面都表现出色。

Abstract: Intermittent demand, a pattern characterized by long sequences of zero sales punctuated by sporadic, non-zero values, poses a persistent challenge in retail and supply chain forecasting. Both traditional methods, such as ARIMA, exponential smoothing, or Croston variants, as well as modern neural architectures such as DeepAR and Transformer-based models often underperform on such data, as they treat demand as a single continuous process or become computationally expensive when scaled across many sparse series. To address these limitations, we introduce Switch-Hurdle: a new framework that integrates a Mixture-of-Experts (MoE) encoder with a Hurdle-based probabilistic decoder. The encoder uses a sparse Top-1 expert routing during the forward pass yet approximately dense in the backward pass via a straight-through estimator (STE). The decoder follows a cross-attention autoregressive design with a shared hurdle head that explicitly separates the forecasting task into two components: a binary classification component estimating the probability of a sale, and a conditional regression component, predicting the quantity given a sale. This structured separation enables the model to capture both occurrence and magnitude processes inherent to intermittent demand. Empirical results on the M5 benchmark and a large proprietary retail dataset show that Switch-Hurdle achieves state-of-the-art prediction performance while maintaining scalability.

</details>


### [81] [Enhancing Geometric Perception in VLMs via Translator-Guided Reinforcement Learning](https://arxiv.org/abs/2602.22703)
*Hao Yu,Shuning Jia,Guanghao Li,Wenhao Jiang,Chun Yuan*

Main category: cs.LG

TL;DR: GeoPerceive是一个几何感知基准测试，包含图表实例和领域特定语言表示，用于评估视觉语言模型的几何感知能力。GeoDPO是一个基于翻译器的强化学习框架，通过NL-to-DSL翻译器生成细粒度奖励信号，显著提升了模型的几何感知和推理能力。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在几何推理方面存在困难，主要原因是它们对基本图表元素的感知能力有限。现有方法难以将几何感知与推理能力分开评估，需要专门的基准测试来单独评估几何感知能力。

Method: 1. 提出GeoPerceive基准测试，包含图表实例和领域特定语言表示，以及高效自动数据生成流水线。2. 提出GeoDPO框架：使用在GeoPerceive合成数据上训练的NL-to-DSL翻译器，将自然语言转换为DSL，计算细粒度的DSL级分数作为强化学习的奖励信号。

Result: GeoDPO在多个任务上取得显著提升：领域内数据+26.5%，领域外数据+8.0%，下游推理任务+39.0%。相比之下，监督微调仅带来边际改进，在领域外场景中甚至可能损害性能。

Conclusion: GeoDPO通过翻译器引导的强化学习框架，有效提升了视觉语言模型的几何感知能力，表现出优异的性能和泛化能力。该方法为几何感知评估和增强提供了新的解决方案。

Abstract: Vision-language models (VLMs) often struggle with geometric reasoning due to their limited perception of fundamental diagram elements. To tackle this challenge, we introduce GeoPerceive, a benchmark comprising diagram instances paired with domain-specific language (DSL) representations, along with an efficient automatic data generation pipeline. This design enables the isolated evaluation of geometric perception independently from reasoning. To exploit the data provided by GeoPerceive for enhancing the geometric perception capabilities of VLMs, we propose GeoDPO, a translator-guided reinforcement learning (RL) framework. GeoDPO employs an NL-to-DSL translator, which is trained on synthetic pairs generated by the data engine of GeoPerceive, to bridge natural language and DSL. This translator facilitates the computation of fine-grained, DSL-level scores, which serve as reward signals in reinforcement learning. We assess GeoDPO on both in-domain and out-of-domain datasets, spanning tasks in geometric perception as well as downstream reasoning. Experimental results demonstrate that, while supervised fine-tuning (SFT) offers only marginal improvements and may even impair performance in out-of-domain scenarios, GeoDPO achieves substantial gains: $+26.5\%$ on in-domain data, $+8.0\%$ on out-of-domain data, and $+39.0\%$ on downstream reasoning tasks. These findings underscore the superior performance and generalization ability of GeoDPO over SFT. All codes are released at https://github.com/Longin-Yu/GeoPerceive
  to ensure reproducibility.

</details>


### [82] [Interpreting and Steering State-Space Models via Activation Subspace Bottlenecks](https://arxiv.org/abs/2602.22719)
*Vamshi Sunku Mohan,Kaustubh Gupta,Aneesha Das,Chandan Singh*

Main category: cs.LG

TL;DR: 该论文提出了一种通过识别Mamba SSM模型中的激活子空间瓶颈并进行简单标量乘干预的方法，无需任务特定调优即可平均提升8.27%性能，并基于此设计了Stable-Mamba架构。


<details>
  <summary>Details</summary>
Motivation: 尽管状态空间模型(SSMs)作为构建强大语言模型的有效策略出现，避免了Transformer中注意力计算的二次复杂度，但现代SSMs的可解释性和可操控性仍然相对未被充分探索。

Method: 使用机制可解释性工具识别Mamba系列SSM模型中的激活子空间瓶颈，然后引入测试时操控干预，简单地将识别出的瓶颈激活乘以标量。最后通过修改这些瓶颈设计了Stable-Mamba架构。

Result: 在5个SSM模型和6个多样化基准测试中，该干预方法平均提升性能8.27%，无需任何任务特定调优。验证了识别出的瓶颈确实阻碍性能，Stable-Mamba架构在从头训练时实现了长上下文性能增益。

Conclusion: 该研究为SSM模型的可解释性和可操控性迈出了重要一步，通过简单的激活干预方法显著提升性能，并基于瓶颈识别设计了改进的Stable-Mamba架构，展示了机制可解释性在模型改进中的实用价值。

Abstract: State-space models (SSMs) have emerged as an efficient strategy for building powerful language models, avoiding the quadratic complexity of computing attention in transformers. Despite their promise, the interpretability and steerability of modern SSMs remain relatively underexplored. We take a major step in this direction by identifying activation subspace bottlenecks in the Mamba family of SSM models using tools from mechanistic interpretability. We then introduce a test-time steering intervention that simply multiplies the activations of the identified bottlenecks by a scalar. Across 5 SSMs and 6 diverse benchmarks, this intervention improves performance by an average of 8.27%, without requiring any task-specific tuning. Finally, we validate that the identified bottlenecks are indeed hindering performance by modifying them to yield an architecture we call Stable-Mamba, which achieves long-context performance gains when retrained from scratch.

</details>


### [83] [Set-based v.s. Distribution-based Representations of Epistemic Uncertainty: A Comparative Study](https://arxiv.org/abs/2602.22747)
*Kaizheng Wang,Yunjia Wang,Fabio Cuzzolin,David Moens,Hans Hallez,Siu Lun Chau*

Main category: cs.LG

TL;DR: 该研究对神经网络中两种二阶不确定性表示范式（基于分布的参数后验和基于集合的信任集）进行了受控比较，发现这两种看似不可比较的框架可以进行有意义的比较，并揭示了二阶表示选择对实际不确定性感知性能的影响。


<details>
  <summary>Details</summary>
Motivation: 神经网络中的认知不确定性通常使用两种二阶范式建模：基于分布的参数后验表示和基于集合的信任集表示。这两种框架由于语义、假设和评估实践的不同，通常被认为是不可比较的，导致它们的相对优劣不明确。此外，经验比较还受到底层预测模型变化的影响。

Method: 研究进行了受控比较研究，使两种范式能够在相同条件下进行公平评估。两种表示都从相同的有限预测分布集合构建，这些分布由共享神经网络生成，从而将表示效应与预测准确性隔离开来。研究通过3种不确定性度量在8个基准测试（包括选择性预测和分布外检测）上评估每种表示，涵盖了6个底层预测模型和每个配置的10次独立运行。

Result: 研究结果表明，在这两种看似不可比较的框架之间进行有意义的比较是可行且有益的。研究提供了关于二阶表示选择如何影响实际不确定性感知性能的见解。

Conclusion: 该研究澄清了神经网络中两种主要二阶不确定性表示范式之间的比较问题，表明通过受控实验设计，可以对这些看似不可比较的框架进行原则性的比较，为不确定性表示的选择提供了实证依据。

Abstract: Epistemic uncertainty in neural networks is commonly modeled using two second-order paradigms: distribution-based representations, which rely on posterior parameter distributions, and set-based representations based on credal sets (convex sets of probability distributions). These frameworks are often regarded as fundamentally non-comparable due to differing semantics, assumptions, and evaluation practices, leaving their relative merits unclear. Empirical comparisons are further confounded by variations in the underlying predictive models. To clarify this issue, we present a controlled comparative study enabling principled, like-for-like evaluation of the two paradigms. Both representations are constructed from the same finite collection of predictive distributions generated by a shared neural network, isolating representational effects from predictive accuracy. Our study evaluates each representation through the lens of 3 uncertainty measures across 8 benchmarks, including selective prediction and out-of-distribution detection, spanning 6 underlying predictive models and 10 independent runs per configuration. Our results show that meaningful comparison between these seemingly non-comparable frameworks is both feasible and informative, providing insights into how second-order representation choices impact practical uncertainty-aware performance.

</details>


### [84] [KMLP: A Scalable Hybrid Architecture for Web-Scale Tabular Data Modeling](https://arxiv.org/abs/2602.22777)
*Mingming Zhang,Pengfei Shi,Zhiqing Xiao,Feng Zhao,Guandong Sun,Yulin Kang,Ruizhe Gao,Ningtao Wang,Xing Fu,Weiqiang Wang,Junbo Zhao*

Main category: cs.LG

TL;DR: KMLP：一种结合KAN前端和gMLP骨干的混合深度学习架构，用于处理大规模网络表格数据，在数十亿样本规模上实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 处理数十亿实例和数百个异构数值特征的网络表格数据时面临可扩展性挑战，现有方法如梯度提升决策树存在瓶颈且需要大量手动特征工程

Method: 提出KMLP混合架构：浅层Kolmogorov-Arnold Network（KAN）前端使用可学习激活函数自动建模每个特征的复杂非线性变换，gMLP骨干网络捕获高阶交互

Result: 在公共基准测试和包含数十亿样本的工业数据集上，KMLP实现了最先进的性能，且相对于GBDT等基线的优势随规模增大而增加

Conclusion: KMLP验证了作为大规模网络表格数据的可扩展深度学习范式的有效性，特别适合处理各向异性、重尾分布和非平稳特征的大规模表格数据

Abstract: Predictive modeling on web-scale tabular data with billions of instances and hundreds of heterogeneous numerical features faces significant scalability challenges. These features exhibit anisotropy, heavy-tailed distributions, and non-stationarity, creating bottlenecks for models like Gradient Boosting Decision Trees and requiring laborious manual feature engineering. We introduce KMLP, a hybrid deep architecture integrating a shallow Kolmogorov-Arnold Network (KAN) front-end with a Gated Multilayer Perceptron (gMLP) backbone. The KAN front-end uses learnable activation functions to automatically model complex non-linear transformations for each feature, while the gMLP backbone captures high-order interactions. Experiments on public benchmarks and an industrial dataset with billions of samples show KMLP achieves state-of-the-art performance, with advantages over baselines like GBDTs increasing at larger scales, validating KMLP as a scalable deep learning paradigm for large-scale web tabular data.

</details>


### [85] [Doubly Adaptive Channel and Spatial Attention for Semantic Image Communication by IoT Devices](https://arxiv.org/abs/2602.22794)
*Soroosh Miri,Sepehr Abolhasani,Shahrokh Farahmand,S. Mohammad Razavizadeh*

Main category: cs.LG

TL;DR: 提出DA-DJSCC方法，通过双重自适应通道和空间注意力模块，在单一训练下适应不同SNR条件，提升IoT语义通信性能


<details>
  <summary>Details</summary>
Motivation: IoT网络面临带宽有限、计算资源受限、无线信道动态变化等挑战。现有DJSCC方法需要为不同SNR分别训练DNN，导致存储和通信开销过大，不适合小型IoT设备。

Method: 在ADJSCC基础上，提出DA-DJSCC方法，在发射端和接收端同时使用双重自适应模块：通道注意力模块适应不同信道条件，空间注意力模块适应空间特征重要性，实现动态特征提取和语义信息恢复。

Result: DA-DJSCC在多个性能指标上显著优于ADJSCC，同时仅带来轻微复杂度增加，适合对性能要求高但复杂度受限的IoT网络。

Conclusion: DA-DJSCC通过双重自适应注意力机制，实现了单一训练下对不同SNR条件的鲁棒适应，是IoT语义通信的理想选择。

Abstract: Internet of Things (IoT) networks face significant challenges such as limited communication bandwidth, constrained computational and energy resources, and highly dynamic wireless channel conditions. Utilization of deep neural networks (DNNs) combined with semantic communication has emerged as a promising paradigm to address these limitations. Deep joint source-channel coding (DJSCC) has recently been proposed to enable semantic communication of images. Building upon the original DJSCC formulation, low-complexity attention-style architectures has been added to the DNNs for further performance enhancement. As a main hurdle, training these DNNs separately for various signal-to-noise ratios (SNRs) will amount to excessive storage or communication overhead, which can not be maintained by small IoT devices. SNR Adaptive DJSCC (ADJSCC), has been proposed to train the DNNs once but feed the current SNR as part of the data to the channel-wise attention mechanism. We improve upon ADJSCC by a simultaneous utilization of doubly adaptive channel-wise and spatial attention modules at both transmitter and receiver. These modules dynamically adjust to varying channel conditions and spatial feature importance, enabling robust and efficient feature extraction and semantic information recovery. Simulation results corroborate that our proposed doubly adaptive DJSCC (DA-DJSCC) significantly improves upon ADJSCC in several performance criteria, while incurring a mild increase in complexity. These facts render DA-DJSCC a desirable choice for semantic communication in performance demanding but low-complexity IoT networks.

</details>


### [86] [Multi-agent imitation learning with function approximation: Linear Markov games and beyond](https://arxiv.org/abs/2602.22810)
*Luca Viano,Till Freihaut,Emanuele Nevali,Volkan Cevher,Matthieu Geist,Giorgia Ramponi*

Main category: cs.LG

TL;DR: 首次对线性马尔可夫博弈中的多智能体模仿学习进行理论分析，提出基于特征层面的集中系数替代状态-动作层面系数，并设计了首个计算高效的交互式MAIL算法，在Tic-Tac-Toe和Connect4游戏中超越行为克隆。


<details>
  <summary>Details</summary>
Motivation: 多智能体模仿学习（MAIL）在复杂环境中面临理论分析不足的挑战，特别是在线性马尔可夫博弈中缺乏理论基础。现有方法依赖于状态-动作层面的集中系数，这可能导致样本复杂度高，且缺乏计算高效的交互式算法。

Method: 1. 利用线性结构将状态-动作层面的"所有策略偏差集中系数"替换为特征层面的集中系数；2. 设计首个计算高效的交互式MAIL算法，其样本复杂度仅依赖于特征映射维度d；3. 基于理论发现提出深度MAIL交互算法。

Result: 1. 当特征能有效表示状态相似性时，特征层面的集中系数远小于状态-动作层面系数；2. 交互式MAIL算法的样本复杂度仅依赖于特征维度d，无需集中系数；3. 深度MAIL算法在Tic-Tac-Toe和Connect4游戏中明显优于行为克隆（BC）。

Conclusion: 该工作为线性马尔可夫博弈中的多智能体模仿学习建立了首个理论框架，通过利用线性结构和特征表示显著降低了样本复杂度，并提出了实用的交互式算法，在基准游戏中验证了其优越性。

Abstract: In this work, we present the first theoretical analysis of multi-agent imitation learning (MAIL) in linear Markov games where both the transition dynamics and each agent's reward function are linear in some given features. We demonstrate that by leveraging this structure, it is possible to replace the state-action level "all policy deviation concentrability coefficient" (Freihaut et al., arXiv:2510.09325) with a concentrability coefficient defined at the feature level which can be much smaller than the state-action analog when the features are informative about states' similarity. Furthermore, to circumvent the need for any concentrability coefficient, we turn to the interactive setting. We provide the first, computationally efficient, interactive MAIL algorithm for linear Markov games and show that its sample complexity depends only on the dimension of the feature map $d$. Building on these theoretical findings, we propose a deep MAIL interactive algorithm which clearly outperforms BC on games such as Tic-Tac-Toe and Connect4.

</details>


### [87] [Accelerating Local LLMs on Resource-Constrained Edge Devices via Distributed Prompt Caching](https://arxiv.org/abs/2602.22812)
*Hiroki Matsutani,Naoki Matsuda,Naoto Sugiura*

Main category: cs.LG

TL;DR: 提出分布式提示缓存机制，通过在多台低端边缘设备间共享中间处理状态来提升LLM推理性能，支持部分匹配并使用布隆过滤器减少通信开销


<details>
  <summary>Details</summary>
Motivation: 资源受限的边缘设备上本地LLM推理存在严重性能瓶颈，需要提升推理性能

Method: 分布式提示缓存机制，支持部分匹配；使用布隆过滤器数据结构（称为catalog）来检测远程服务器是否拥有所需内部状态，减少不必要的通信开销

Result: 在Raspberry Pi Zero 2W平台上使用Gemma-3 270M模型和MMLU数据集测试，TTFT平均降低93.12%，TTLT平均降低50.07%

Conclusion: 分布式提示缓存能有效提升边缘设备上LLM推理性能，显著减少首次令牌和最后令牌的生成时间

Abstract: Since local LLM inference on resource-constrained edge devices imposes a severe performance bottleneck, this paper proposes distributed prompt caching to enhance inference performance by cooperatively sharing intermediate processing states across multiple low-end edge devices. To fully utilize prompt similarity, our distributed caching mechanism also supports partial matching. As this approach introduces communication overhead associated with state sharing over a wireless network, we introduce a Bloom-filter-based data structure, referred to as a catalog, to determine whether a remote server possesses the desired internal states, thereby suppressing unnecessary communication. Experiments using the Gemma-3 270M model and the MMLU dataset on the Raspberry Pi Zero 2W platform demonstrate that the proposed approach reduces TTFT (Time to First Token) and TTLT (Time to Last Token) by 93.12% and 50.07% on average, respectively.

</details>


### [88] [Hierarchy-of-Groups Policy Optimization for Long-Horizon Agentic Tasks](https://arxiv.org/abs/2602.22817)
*Shuo He,Lang Feng,Qi Wei,Xin Cheng,Lei Feng,Bo An*

Main category: cs.LG

TL;DR: HGPO提出分层组策略优化方法，通过根据历史上下文一致性将轨迹步骤分配到多个分层组中，解决步进组策略优化中的上下文不一致问题，改善优势估计偏差，在长视野智能体任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有步进组策略优化方法在处理长视野智能体任务时存在上下文不一致问题，即同一组内的步骤可能具有不同的历史上下文，导致优势估计严重偏差，从而显著降低策略优化效果。

Method: 提出分层组策略优化(HGPO)：1) 在轨迹组内，根据历史上下文一致性将每个步骤分配到多个分层组中；2) 为每个步骤在不同组内计算独立优势值；3) 使用自适应加权方案聚合这些优势值，实现偏差-方差权衡，无需额外模型或轨迹采样。

Result: 在ALFWorld和WebShop两个挑战性智能体任务上，使用Qwen2.5-1.5B-Instruct和Qwen2.5-7B-Instruct模型，HGPO在相同计算约束下显著优于现有智能体强化学习方法。

Conclusion: HGPO通过分层组分配和自适应优势聚合有效解决了步进组策略优化中的上下文不一致问题，实现了更好的偏差-方差权衡，提升了长视野智能体任务的性能。

Abstract: Group-based reinforcement learning (RL), such as GRPO, has advanced the capabilities of large language models on long-horizon agentic tasks. To enable more fine-grained policy updates, recent research has increasingly shifted toward stepwise group-based policy optimization, which treats each step in a rollout trajectory independently while using a memory module to retain historical context. However, we find a key issue in estimating stepwise relative advantages, namely context inconsistency, where steps within the same group may differ in their historical contexts. Empirically, we reveal that this issue can lead to severely biased advantage estimation, thereby degrading policy optimization significantly. To address the issue, in this paper, we propose Hierarchy-of-Groups Policy Optimization (HGPO) for long-horizon agentic tasks. Specifically, within a group of rollout trajectories, HGPO assigns each step to multiple hierarchical groups according to the consistency of historical contexts. Then, for each step, HGPO computes distinct advantages within each group and aggregates them with an adaptive weighting scheme. In this way, HGPO can achieve a favorable bias-variance trade-off in stepwise advantage estimation, without extra models or rollouts. Evaluations on two challenging agentic tasks, ALFWorld and WebShop with Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct, show that HGPO significantly outperforms existing agentic RL methods under the same computational constraints. Code is available at https://github.com/langfengQ/verl-agent/tree/master/recipe/hgpo.

</details>


### [89] [Hypernetwork-based approach for grid-independent functional data clustering](https://arxiv.org/abs/2602.22823)
*Anirudh Thatipelli,Ali Siahkoohi*

Main category: cs.LG

TL;DR: 提出基于超网络和隐式神经表示的函数数据聚类框架，将离散函数映射到固定维权重空间，实现与采样网格无关的聚类


<details>
  <summary>Details</summary>
Motivation: 现有函数数据聚类方法大多基于采样网格，导致聚类结果依赖于分辨率、采样密度或预处理选择，而非函数本身的结构特征

Method: 使用超网络编码器将坐标-值对映射到隐式神经表示(INR)的权重空间，INR作为解码器；在权重空间进行标准聚类，实现与离散化和聚类方法无关的框架

Result: 在高维合成和真实数据集上展示了具有竞争力的聚类性能，对采样分辨率变化具有鲁棒性，并能泛化到训练中未见的分辨率

Conclusion: 该框架通过将函数映射到紧凑的权重表示，实现了与采样网格无关的函数数据聚类，解决了现有方法对离散化敏感的局限性

Abstract: Functional data clustering is concerned with grouping functions that share similar structure, yet most existing methods implicitly operate on sampled grids, causing cluster assignments to depend on resolution, sampling density, or preprocessing choices rather than on the underlying functions themselves. To address this limitation, we introduce a framework that maps discretized function observations -- at arbitrary resolution and on arbitrary grids -- into a fixed-dimensional vector space via an auto-encoding architecture. The encoder is a hypernetwork that maps coordinate-value pairs to the weight space of an implicit neural representation (INR), which serves as the decoder. Because INRs represent functions with very few parameters, this design yields compact representations that are decoupled from the sampling grid, while the hypernetwork amortizes weight prediction across the dataset. Clustering is then performed in this weight space using standard algorithms, making the approach agnostic to both the discretization and the choice of clustering method. By means of synthetic and real-world experiments in high-dimensional settings, we demonstrate competitive clustering performance that is robust to changes in sampling resolution -- including generalization to resolutions not seen during training.

</details>


### [90] [Moral Preferences of LLMs Under Directed Contextual Influence](https://arxiv.org/abs/2602.22831)
*Phil Blandfort,Tushar Karayil,Urja Pawar,Robert Graham,Alex McKenzie,Dmitrii Krasheninnikov*

Main category: cs.LG

TL;DR: 研究发现LLM在道德困境中的决策会受到上下文提示的显著影响，即使这些上下文只是表面相关，且模型可能声称中立但实际决策仍会偏移。


<details>
  <summary>Details</summary>
Motivation: 现有道德基准测试通常使用无上下文的提示，假设模型偏好稳定。但在实际部署中，提示常包含用户请求、社会规范等上下文信号，可能影响决策。需要研究定向上下文影响如何重塑道德困境决策。

Method: 引入一个用于电车问题式道德分类的定向上下文影响评估框架：对每个人口统计因素，应用匹配的、方向翻转的上下文影响（仅偏袒的群体不同），系统测量定向响应。

Result: 发现：(1) 上下文影响常显著改变决策，即使只是表面相关；(2) 基线偏好不能预测定向可操纵性，模型可能基线中立但在影响下表现出系统性不对称；(3) 影响可能适得其反：模型可能声称中立或忽略线索，但选择仍会偏移，有时甚至反向；(4) 推理降低平均敏感性，但放大有偏少样本示例的影响。

Conclusion: 研究结果表明需要扩展道德评估，加入受控的方向翻转上下文操作，以更好地表征模型行为。

Abstract: Moral benchmarks for LLMs typically use context-free prompts, implicitly assuming stable preferences. In deployment, however, prompts routinely include contextual signals such as user requests, cues on social norms, etc. that may steer decisions. We study how directed contextual influences reshape decisions in trolley-problem-style moral triage settings. We introduce a pilot evaluation harness for directed contextual influence in trolley-problem-style moral triage: for each demographic factor, we apply matched, direction-flipped contextual influences that differ only in which group they favor, enabling systematic measurement of directional response. We find that: (i) contextual influences often significantly shift decisions, even when only superficially relevant; (ii) baseline preferences are a poor predictor of directional steerability, as models can appear baseline-neutral yet exhibit systematic steerability asymmetry under influence; (iii) influences can backfire: models may explicitly claim neutrality or discount the contextual cue, yet their choices still shift, sometimes in the opposite direction; and (iv) reasoning reduces average sensitivity, but amplifies the effect of biased few-shot examples. Our findings motivate extending moral evaluations with controlled, direction-flipped context manipulations to better characterize model behavior.

</details>


### [91] [Decentralized Ranking Aggregation: Gossip Algorithms for Borda and Copeland Consensus](https://arxiv.org/abs/2602.22847)
*Anna Van Elst,Kerrian Le Caillec,Igor Colin,Stephan Clémençon*

Main category: cs.LG

TL;DR: 该论文提出了一种去中心化排名聚合方法，使用随机gossip通信让自治代理通过局部交互计算全局排名共识，无需中央协调，为Borda和Copeland规则提供了收敛保证。


<details>
  <summary>Details</summary>
Motivation: 现有排名聚合算法主要针对集中式设置，而许多现代技术（如P2P网络、物联网、多智能体系统）需要在去中心化环境中计算共识排名，这是一个重要的方法论挑战。

Method: 基于随机gossip通信的方法，允许自治代理仅通过局部交互计算全局排名共识，无需协调或中央权威。提出了Borda、Copeland、中位数排名规则和局部Kemenization的去中心化实现。

Result: 为Borda和Copeland共识方法提供了严格的收敛保证（包括显式速率界限）。在各种网络拓扑和真实/合成排名数据集上的广泛实证评估表明，算法能快速可靠地收敛到正确的排名聚合。

Conclusion: 该研究成功地将排名聚合扩展到去中心化设置，解决了鲁棒性和可扩展性问题，为分布式偏好分析提供了有效方法，特别是在通信成本受限的环境中。

Abstract: The concept of ranking aggregation plays a central role in preference analysis, and numerous algorithms for calculating median rankings, often originating in social choice theory, have been documented in the literature, offering theoretical guarantees in a centralized setting, i.e., when all the ranking data to be aggregated can be brought together in a single computing unit. For many technologies (e.g. peer-to-peer networks, IoT, multi-agent systems), extending the ability to calculate consensus rankings with guarantees in a decentralized setting, i.e., when preference data is initially distributed across a communicating network, remains a major methodological challenge. Indeed, in recent years, the literature on decentralized computation has mainly focused on computing or optimizing statistics such as arithmetic means using gossip algorithms. The purpose of this article is precisely to study how to achieve reliable consensus on collective rankings using classical rules (e.g. Borda, Copeland) in a decentralized setting, thereby raising new questions, robustness to corrupted nodes, and scalability through reduced communication costs in particular. The approach proposed and analyzed here relies on random gossip communication, allowing autonomous agents to compute global ranking consensus using only local interactions, without coordination or central authority.
  We provide rigorous convergence guarantees, including explicit rate bounds, for the Borda and Copeland consensus methods. Beyond these rules, we also provide a decentralized implementation of consensus according to the median rank rule and local Kemenization. Extensive empirical evaluations on various network topologies and real and synthetic ranking datasets demonstrate that our algorithms converge quickly and reliably to the correct ranking aggregation.

</details>


### [92] [MEDNA-DFM: A Dual-View FiLM-MoE Model for Explainable DNA Methylation Prediction](https://arxiv.org/abs/2602.22850)
*Yi He,Yina Cao,Jixiu Zhai,Di Wang,Junxiao Kong,Tianchi Lu*

Main category: cs.LG

TL;DR: 开发了高性能DNA甲基化预测模型MEDNA-DFM，结合机制启发的信号净化算法，不仅能准确预测甲基化，还能提取可靠的保守基序，并提出了"序列-结构协同"假说。


<details>
  <summary>Details</summary>
Motivation: 深度学习在DNA甲基化识别中表现出色，但其"黑盒"特性阻碍了生物学洞见的获取。需要开发既能准确预测又能提供生物学解释的方法。

Method: 提出了MEDNA-DFM模型和机制启发的信号净化算法。通过外部独立数据集验证模型泛化能力，使用开发的算法提取甲基化相关基序，并通过果蝇6mA案例研究提出"序列-结构协同"假说，用计算机诱变验证。

Result: MEDNA-DFM能有效捕捉保守的甲基化模式，在不同物种中实现稳健区分。模型泛化由保守内在基序（如GC含量）驱动而非系统发育邻近性。提取的基序可靠性显著高于先前研究。果蝇案例验证了GAGG核心基序和上游A-tract元素的协同作用。

Conclusion: 这项工作提供了强大的甲基化预测工具，展示了可解释深度学习如何推动方法创新和生物学假说生成，为表观遗传调控研究提供了新视角。

Abstract: Accurate computational identification of DNA methylation is essential for understanding epigenetic regulation. Although deep learning excels in this binary classification task, its "black-box" nature impedes biological insight. We address this by introducing a high-performance model MEDNA-DFM, alongside mechanism-inspired signal purification algorithms. Our investigation demonstrates that MEDNA-DFM effectively captures conserved methylation patterns, achieving robust distinction across diverse species. Validation on external independent datasets confirms that the model's generalization is driven by conserved intrinsic motifs (e.g., GC content) rather than phylogenetic proximity. Furthermore, applying our developed algorithms extracted motifs with significantly higher reliability than prior studies. Finally, empirical evidence from a Drosophila 6mA case study prompted us to propose a "sequence-structure synergy" hypothesis, suggesting that the GAGG core motif and an upstream A-tract element function cooperatively. We further validated this hypothesis via in silico mutagenesis, confirming that the ablation of either or both elements significantly degrades the model's recognition capabilities. This work provides a powerful tool for methylation prediction and demonstrates how explainable deep learning can drive both methodological innovation and the generation of biological hypotheses.

</details>


### [93] [Fair feature attribution for multi-output prediction: a Shapley-based perspective](https://arxiv.org/abs/2602.22882)
*Umberto Biccari,Alain Ibáñez de Opakua,José María Mato,Óscar Millet,Roberto Morales,Enrique Zuazua*

Main category: cs.LG

TL;DR: 该论文通过将Shapley公理扩展到向量值合作博弈，证明了多输出预测器中任何满足效率、对称性、虚拟玩家和可加性的特征归因规则必须按输出分量分解，揭示了Shapley解释性中的结构性约束。


<details>
  <summary>Details</summary>
Motivation: 虽然SHAP解释通常独立计算每个输出坐标，但这种做法的理论必要性一直不明确。论文旨在为多输出预测器中的特征归因提供公理化特征，澄清Shapley框架下公平一致性解释的确切范围。

Method: 将经典Shapley公理（效率、对称性、虚拟玩家、可加性）扩展到向量值合作博弈，建立刚性定理，证明任何满足这些公理的归因规则必须按输出分量分解。通过数值实验在生物医学基准上验证。

Result: 证明了多输出预测器中任何满足四个经典Shapley公理的归因规则必须按分量分解，因此任何联合输出归因规则必须至少放松一个经典公理。数值实验显示多输出模型能在训练和部署中节省计算成本，同时产生与Shapley公理强加的组件结构完全一致的SHAP解释。

Conclusion: 该研究识别了Shapley基础解释性中先前未形式化的结构性约束，澄清了多输出学习中公平一致性解释的确切范围。结果表明，多输出模型既能带来计算效益，又能产生与Shapley公理一致的解释。

Abstract: In this article, we provide an axiomatic characterization of feature attribution for multi-output predictors within the Shapley framework. While SHAP explanations are routinely computed independently for each output coordinate, the theoretical necessity of this practice has remained unclear. By extending the classical Shapley axioms to vector-valued cooperative games, we establish a rigidity theorem showing that any attribution rule satisfying efficiency, symmetry, dummy player, and additivity must necessarily decompose component-wise across outputs. Consequently, any joint-output attribution rule must relax at least one of the classical Shapley axioms. This result identifies a previously unformalized structural constraint in Shapley-based interpretability, clarifying the precise scope of fairness-consistent explanations in multi-output learning. Numerical experiments on a biomedical benchmark illustrate that multi-output models can yield computational savings in training and deployment, while producing SHAP explanations that remain fully consistent with the component-wise structure imposed by the Shapley axioms.

</details>


### [94] [A Data-Driven Approach to Support Clinical Renal Replacement Therapy](https://arxiv.org/abs/2602.22902)
*Alice Balboni,Luis Escobar,Andrea Manno,Fabrizio Rossi,Maria Cristina Ruffa,Gianluca Villa,Giordano D'Aloisio,Antonio Consolo*

Main category: cs.LG

TL;DR: 该研究使用机器学习预测危重患者CRRT治疗中的膜污染，采用表格数据方法而非直接时序建模，通过ADASYN处理数据不平衡，随机森林等模型表现优于LSTM，特征选择简化模型，SHAP反事实分析识别关键干预因素。


<details>
  <summary>Details</summary>
Motivation: 预测危重患者连续肾脏替代治疗中的膜污染对于改善患者管理和治疗调整至关重要。传统方法可能无法准确预测这一复杂临床现象，需要开发可解释的机器学习模型来提供可靠的预测和临床指导。

Method: 从ICU时间序列数据中提取16个临床特征，采用表格数据方法而非直接建模时序依赖。使用ADASYN过采样技术处理数据不平衡问题。比较随机森林、XGBoost和LightGBM模型性能，并与LSTM循环神经网络对比。进行特征选择简化模型，应用SHAP值进行反事实分析。

Result: 在10%再平衡率下，模型达到77.6%敏感性和96.3%特异性，性能在不同预测时间窗口保持稳健。表格方法优于LSTM，表明显式时序建模非必需。特征选择将模型简化为5个关键变量，精度损失最小。SHAP反事实分析成功识别能够逆转污染预测的最小输入变化。

Conclusion: 研究证实了可解释机器学习模型预测CRRT膜污染的可行性。预测与反事实分析的结合具有实际临床价值，可指导治疗调整以降低污染风险并改善患者管理，为临床决策提供支持。

Abstract: This study investigates a data-driven machine learning approach to predict membrane fouling in critically ill patients undergoing Continuous Renal Replacement Therapy (CRRT). Using time-series data from an ICU, 16 clinically selected features were identified to train predictive models. To ensure interpretability and enable reliable counterfactual analysis, the researchers adopted a tabular data approach rather than modeling temporal dependencies directly. Given the imbalance between fouling and non-fouling cases, the ADASYN oversampling technique was applied to improve minority class representation. Random Forest, XGBoost, and LightGBM models were tested, achieving balanced performance with 77.6% sensitivity and 96.3% specificity at a 10% rebalancing rate. Results remained robust across different forecasting horizons. Notably, the tabular approach outperformed LSTM recurrent neural networks, suggesting that explicit temporal modeling was not necessary for strong predictive performance. Feature selection further reduced the model to five key variables, improving simplicity and interpretability with minimal loss of accuracy. A Shapley value-based counterfactual analysis was applied to the best-performing model, successfully identifying minimal input changes capable of reversing fouling predictions. Overall, the findings support the viability of interpretable machine learning models for predicting membrane fouling during CRRT. The integration of prediction and counterfactual analysis offers practical clinical value, potentially guiding therapeutic adjustments to reduce fouling risk and improve patient management.

</details>


### [95] [NoRA: Breaking the Linear Ceiling of Low-Rank Adaptation via Manifold Expansion](https://arxiv.org/abs/2602.22911)
*Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: NoRA通过非线性适配器突破LoRA的线性天花板，在复杂推理任务中实现更高参数效率


<details>
  <summary>Details</summary>
Motivation: LoRA在复杂推理任务中存在"线性天花板"问题，单纯增加秩会导致收益递减，需要突破线性约束

Method: 提出NoRA（非线性秩适应），使用权重级并行适配器，注入SiLU门控和结构化dropout以诱导流形扩展

Result: 在SlimOrca基准上，NoRA在秩64时（PPL 3.89）优于LoRA在秩512时（PPL 3.90）；在数学推理上，NoRA在MathInstruct上达到PPL 1.97，显著超越LoRA的饱和点2.07

Conclusion: NoRA通过激活奇异值谱的休眠尾部，有效防止线性方法中的秩崩溃，在复杂推理任务中实现突破性的参数效率

Abstract: Low-Rank Adaptation (LoRA) dominates parameter-efficient fine-tuning (PEFT). However, it faces a critical ``linear ceiling'' in complex reasoning tasks: simply increasing the rank yields diminishing returns due to intrinsic linear constraints. We introduce NoRA (Non-linear Rank Adaptation), a weight-level parallel adapter that injects SiLU gating and structural dropout to induce manifold expansion. On the SlimOrca benchmark, NoRA breaks this linear barrier: NoRA remarkably at rank 64 (PPL 3.89) outperforms LoRA at rank 512 (PPL 3.90), demonstrating superior spectral efficiency. This advantage generalizes to mathematical reasoning, where NoRA achieves a perplexity of 1.97 on MathInstruct, significantly surpassing LoRA's saturation point of 2.07. Mechanism analysis via Singular Value Decomposition (SVD) confirms that NoRA activates the dormant tail of the singular value spectrum, effectively preventing the rank collapse observed in linear methods.

</details>


### [96] [Generalization Bounds of Stochastic Gradient Descent in Homogeneous Neural Networks](https://arxiv.org/abs/2602.22936)
*Wenquan Ma,Yang Sui,Jiaye Teng,Bohan Wang,Jing Xu,Jingqin Yang*

Main category: cs.LG

TL;DR: 本文证明在齐次神经网络中，即使使用较慢的步长衰减Ω(1/√t)，也能获得泛化界，突破了传统非凸训练中需要η_t=O(1/t)的限制。


<details>
  <summary>Details</summary>
Motivation: 算法稳定性是泛化分析的重要技术，但传统方法在非凸训练中要求步长η_t=O(1/t)，这种刚性衰减可能阻碍优化且不符合实际应用场景。

Method: 在齐次神经网络框架下推导泛化界，证明该框架允许较慢的步长衰减Ω(1/√t)，并扩展到非Lipschitz等更广泛场景。

Result: 齐次神经网络（包括全连接和卷积网络，使用ReLU和LeakyReLU激活）允许更慢的步长衰减，同时保持泛化保证。

Conclusion: 齐次神经网络框架为实际训练提供了更灵活的步长调度选择，突破了传统算法稳定性分析的限制，具有广泛适用性。

Abstract: Algorithmic stability is among the most potent techniques in generalization analysis. However, its derivation usually requires a stepsize $η_t = \mathcal{O}(1/t)$ under non-convex training regimes, where $t$ denotes iterations. This rigid decay of the stepsize potentially impedes optimization and may not align with practical scenarios. In this paper, we derive the generalization bounds under the homogeneous neural network regimes, proving that this regime enables slower stepsize decay of order $Ω(1/\sqrt{t})$ under mild assumptions. We further extend the theoretical results from several aspects, e.g., non-Lipschitz regimes. This finding is broadly applicable, as homogeneous neural networks encompass fully-connected and convolutional neural networks with ReLU and LeakyReLU activations.

</details>


### [97] [MSINO: Curvature-Aware Sobolev Optimization for Manifold Neural Networks](https://arxiv.org/abs/2602.22937)
*Suresan Pareth*

Main category: cs.LG

TL;DR: MSINO是一个在黎曼流形上训练神经网络的曲率感知框架，使用协变Sobolev损失和平行传输对齐梯度，通过Laplace-Beltrami平滑正则化提高稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有欧几里得空间中的Sobolev训练方法无法处理流形上的曲率效应，需要开发能够显式跟踪曲率和传输雅可比矩阵的训练框架，以在表面成像、物理信息学习和机器人等流形应用中获得更好的收敛保证。

Method: 1. 用协变Sobolev损失替代标准欧几里得导数监督，通过平行传输对齐梯度；2. 引入Laplace-Beltrami平滑正则化项提高稳定性；3. 基于黎曼优化和流形Sobolev理论推导几何依赖常数；4. 提出两步骤Newton-Sobolev方法。

Result: 1. 推导出具有流形Sobolev平滑常数的下降引理；2. 获得Sobolev Polyak-Lojasiewicz不等式，为黎曼梯度下降和随机梯度下降提供线性收敛保证；3. 在曲率控制邻域内实现局部二次收缩的两步骤Newton-Sobolev方法。

Conclusion: MSINO统一了基于值和梯度的学习，为流形上的神经网络训练提供了曲率感知的收敛保证，适用于SO(3)、SE(3)等李群上的表面成像、物理信息学习和机器人应用。

Abstract: We introduce Manifold Sobolev Informed Neural Optimization (MSINO), a curvature aware training framework for neural networks defined on Riemannian manifolds. The method replaces standard Euclidean derivative supervision with a covariant Sobolev loss that aligns gradients using parallel transport and improves stability via a Laplace Beltrami smoothness regularization term.
  Building on classical results in Riemannian optimization and Sobolev theory on manifolds, we derive geometry dependent constants that yield (i) a Descent Lemma with a manifold Sobolev smoothness constant, (ii) a Sobolev Polyak Lojasiewicz inequality giving linear convergence guarantees for Riemannian gradient descent and stochastic gradient descent under explicit step size bounds, and (iii) a two step Newton Sobolev method with local quadratic contraction in curvature controlled neighborhoods.
  Unlike prior Sobolev training in Euclidean space, MSINO provides training time guarantees that explicitly track curvature and transported Jacobians. Applications include surface imaging, physics informed learning settings, and robotics on Lie groups such as SO(3) and SE(3). The framework unifies value and gradient based learning with curvature aware convergence guarantees for neural training on manifolds.

</details>


### [98] [Scaling Laws of Global Weather Models](https://arxiv.org/abs/2602.22962)
*Yuejiang Yu,Langwen Huang,Alexandru Calotoiu,Torsten Hoefler*

Main category: cs.LG

TL;DR: 该论文分析了天气预测数据驱动模型的缩放规律，发现Aurora在数据缩放方面表现最佳，GraphCast参数效率最高但硬件利用率有限，计算最优分析显示延长训练时间比增加模型规模更有效，天气模型偏好宽度而非深度架构。


<details>
  <summary>Details</summary>
Motivation: 数据驱动模型正在革新天气预测领域。为了优化训练效率和模型性能，需要研究该领域的经验缩放规律，了解模型性能与模型规模、数据集大小和计算预算之间的关系，为未来天气模型设计提供指导。

Method: 分析模型性能（验证损失）与三个关键因素的关系：模型规模(N)、数据集大小(D)和计算预算(C)。通过一系列模型实验，研究数据缩放行为、参数效率、硬件利用率，并进行计算最优分析，同时分析模型形状（宽度与深度）的缩放规律。

Result: Aurora展现出最强的数据缩放行为：训练数据集增加10倍可使验证损失降低达3.2倍。GraphCast参数效率最高但硬件利用率有限。在固定计算预算下，分配资源到更长的训练时间比增加模型规模能获得更大的性能提升。天气预测模型的缩放行为与语言模型根本不同：始终偏好增加宽度而非深度。

Conclusion: 未来天气模型应优先考虑更宽的架构和更大的有效训练数据集，以最大化预测性能。这些发现为天气预测数据驱动模型的优化设计提供了重要指导。

Abstract: Data-driven models are revolutionizing weather forecasting. To optimize training efficiency and model performance, this paper analyzes empirical scaling laws within this domain. We investigate the relationship between model performance (validation loss) and three key factors: model size ($N$), dataset size ($D$), and compute budget ($C$). Across a range of models, we find that Aurora exhibits the strongest data-scaling behavior: increasing the training dataset by 10x reduces validation loss by up to 3.2x. GraphCast demonstrates the highest parameter efficiency, yet suffers from limited hardware utilization. Our compute-optimal analysis indicates that, under fixed compute budgets, allocating resources to longer training durations yields greater performance gains than increasing model size. Furthermore, we analyze model shape and uncover scaling behaviors that differ fundamentally from those observed in language models: weather forecasting models consistently favor increased width over depth. These findings suggest that future weather models should prioritize wider architectures and larger effective training datasets to maximize predictive performance.

</details>


### [99] [Residual Koopman Spectral Profiling for Predicting and Preventing Transformer Training Instability](https://arxiv.org/abs/2602.22988)
*Bum Jun Kim,Shohei Taniguchi,Makoto Kawano,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.LG

TL;DR: 提出RKSP方法，通过单次前向传播提取Koopman谱特征，预测Transformer训练发散风险，并开发KSS方法在训练中重塑谱特征防止发散。


<details>
  <summary>Details</summary>
Motivation: Transformer训练中的发散问题会浪费大量计算资源，而现有方法只能在昂贵的训练开始后才能发现不稳定性。需要一种在训练开始前就能预测发散概率的方法。

Method: 提出Residual Koopman Spectral Profiling (RKSP)：在初始化时通过单次前向传播，对层间残差快照应用白化动态模式分解，提取Koopman谱特征。关键诊断指标是近单位谱质量，量化集中在单位圆附近的模式比例。同时开发Koopman Spectral Shaping (KSS)在训练中重塑谱特征。

Result: RKSP在预测发散方面AUROC达到0.995，优于最佳梯度基线。在无归一化层的高学习率场景中，KSS将发散率从66.7%降至12.5%，并使学习率提高50%-150%。方法在WikiText-103、CIFAR-10、GPT-2、LLaMA-2 7B、MoE、Mamba-style SSMs和KAN等多种架构上验证有效。

Conclusion: RKSP能够在训练开始前准确预测Transformer发散风险，KSS能够有效防止训练发散，该方法具有广泛的适用性，可应用于多种现代架构。

Abstract: Training divergence in transformers wastes compute, yet practitioners discover instability only after expensive runs begin. They therefore need an expected probability of failure for a transformer before training starts. Our study of Residual Koopman Spectral Profiling (RKSP) provides such an estimate. From a single forward pass at initialization, RKSP extracts Koopman spectral features by applying whitened dynamic mode decomposition to layer-wise residual snapshots. Our central diagnostic, the near-unit spectral mass, quantifies the fraction of modes concentrated near the unit circle, which captures instability risk. For predicting divergence across extensive configurations, this estimator achieves an AUROC of 0.995, outperforming the best gradient baseline. We further make this diagnostic actionable through Koopman Spectral Shaping (KSS), which reshapes spectra during training. We empirically validate that our method works in practice: RKSP predicts divergence at initialization, and when RKSP flags high risk, turning on KSS successfully prevents divergence. In the challenging high learning rate regime without normalization layers, KSS reduces the divergence rate from 66.7% to 12.5% and enables learning rates that are 50% to 150% higher. These findings generalize to WikiText-103 language modeling, vision transformers on CIFAR-10, and pretrained language models, including GPT-2 and LLaMA-2 up to 7B, as well as emerging architectures such as MoE, Mamba-style SSMs, and KAN.

</details>


### [100] [Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization](https://arxiv.org/abs/2602.23008)
*Zeyuan Liu,Jeonghye Kim,Xufang Luo,Dongsheng Li,Yuqing Yang*

Main category: cs.LG

TL;DR: EMPO²是一种混合强化学习框架，通过记忆增强探索，结合on-policy和off-policy更新，提升LLM智能体在需要发现新状态环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 探索是强化学习训练的大型语言模型智能体的关键瓶颈。现有方法依赖预训练知识，但在需要发现新状态的环境中表现不佳。

Method: 提出EMPO²（探索性记忆增强on-和off-policy优化）框架，利用记忆进行探索，结合on-policy和off-policy更新，使LLM在有记忆时表现良好，无记忆时也能保持鲁棒性。

Result: 在ScienceWorld和WebShop任务上，EMPO²相比GRPO分别提升128.6%和11.3%。在分布外测试中，EMPO²对新任务表现出优越适应性，仅需少量记忆试验且无需参数更新。

Conclusion: EMPO²是构建更具探索性和泛化能力的LLM智能体的有前景框架。

Abstract: Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO$^2$), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO$^2$ achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO$^2$ demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO$^2$ as a promising framework for building more exploratory and generalizable LLM-based agents.

</details>


### [101] [Learning Disease-Sensitive Latent Interaction Graphs From Noisy Cardiac Flow Measurements](https://arxiv.org/abs/2602.23035)
*Viraj Patel,Marko Grujic,Philipp Aigner,Theodor Abart,Marcus Granegger,Deblina Bhattacharjee,Katharine Fraser*

Main category: cs.LG

TL;DR: 提出物理信息驱动的潜在关系框架，将心脏涡流建模为图中的交互节点，用于量化疾病严重程度和干预效果。


<details>
  <summary>Details</summary>
Motivation: 当前成像和计算方法无法捕捉心脏血流中相干流动特征的基础关系结构，而这些模式包含丰富的疾病严重程度和临床干预信息。

Method: 结合神经关系推理架构与物理启发的交互能量和生死动力学，构建潜在图模型，将心脏涡流建模为图中的交互节点。

Result: 在主动脉缩窄模拟中，随着主动脉半径变窄，涡流相互作用变得更强更频繁，图熵与缩窄严重程度呈单调相关（R²=0.78，Spearman |ρ|=0.96）。在左心室辅助设备支持的超声数据中，该方法同样能捕捉涡流结构的减弱，展示了跨模态泛化能力。

Conclusion: 潜在交互图和熵可作为心脏疾病和干预的稳健且可解释的生物标志物，该方法能有效量化疾病严重程度和干预效果。

Abstract: Cardiac blood flow patterns contain rich information about disease severity and clinical interventions, yet current imaging and computational methods fail to capture underlying relational structures of coherent flow features. We propose a physics-informed, latent relational framework to model cardiac vortices as interacting nodes in a graph. Our model combines a neural relational inference architecture with physics-inspired interaction energy and birth-death dynamics, yielding a latent graph sensitive to disease severity and intervention level. We first apply this to computational fluid dynamics simulations of aortic coarctation. Learned latent graphs reveal that as the aortic radius narrows, vortex interactions become stronger and more frequent. This leads to a higher graph entropy, correlating monotonically with coarctation severity ($R^2=0.78$, Spearman $|ρ|=0.96$). We then extend this method to ultrasound datasets of left ventricles under varying levels of left ventricular assist device support. Again the latent graph representation captures the weakening of coherent vortical structures, thereby demonstrating cross-modal generalisation. Results show latent interaction graphs and entropy serve as robust and interpretable markers of cardiac disease and intervention.

</details>


### [102] [Latent Matters: Learning Deep State-Space Models](https://arxiv.org/abs/2602.23050)
*Alexej Klushyn,Richard Kurle,Maximilian Soelch,Botond Cseke,Patrick van der Smagt*

Main category: cs.LG

TL;DR: 论文提出了一种用于训练深度状态空间模型的约束优化框架，并基于此开发了扩展卡尔曼VAE（EKVAE），该模型在系统识别和预测精度方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度状态空间模型通过最大化证据下界训练，但作者发现这并不能确保模型真正学习到底层动态。因此需要更有效的训练方法来确保模型准确学习系统动态。

Method: 提出了一个通用的约束优化框架来训练DSSMs。基于此框架开发了EKVAE，将摊销变分推断与经典贝叶斯滤波/平滑相结合，替代传统的RNN-based DSSMs。

Result: 约束优化框架显著提升了现有SOTA DSSMs的系统识别和预测精度。EKVAE在预测精度上超越先前模型，在动态系统识别方面表现优异，并能成功学习静态和动态特征解耦的状态空间表示。

Conclusion: 约束优化框架是训练DSSMs的有效方法，EKVAE通过结合现代变分推断与经典贝叶斯方法，在建模动态系统方面展现出优越性能，实现了更好的预测精度和系统识别能力。

Abstract: Deep state-space models (DSSMs) enable temporal predictions by learning the underlying dynamics of observed sequence data. They are often trained by maximising the evidence lower bound. However, as we show, this does not ensure the model actually learns the underlying dynamics. We therefore propose a constrained optimisation framework as a general approach for training DSSMs. Building upon this, we introduce the extended Kalman VAE (EKVAE), which combines amortised variational inference with classic Bayesian filtering/smoothing to model dynamics more accurately than RNN-based DSSMs. Our results show that the constrained optimisation framework significantly improves system identification and prediction accuracy on the example of established state-of-the-art DSSMs. The EKVAE outperforms previous models w.r.t. prediction accuracy, achieves remarkable results in identifying dynamical systems, and can furthermore successfully learn state-space representations where static and dynamic features are disentangled.

</details>


### [103] [RhythmBERT: A Self-Supervised Language Model Based on Latent Representations of ECG Waveforms for Heart Disease Detection](https://arxiv.org/abs/2602.23060)
*Xin Wang,Burcu Ozek,Aruna Mohan,Amirhossein Ravari,Or Zilbershot,Fatemeh Afghah*

Main category: cs.LG

TL;DR: RhythmBERT：将心电图视为结构化语言的生成式语言模型，通过符号化P、QRS、T段捕获节律语义，在单导联上达到或超越12导联基线的性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法将心电图视为通用时间序列，忽略了生理语义和节律级结构。对比学习方法使用的增强会扭曲形态，而生成方法采用固定窗口分割会错位心动周期。

Method: 提出RhythmBERT生成式心电图语言模型，通过自编码器潜在表示将P、QRS、T段编码为符号标记，捕获节律语义，同时保留连续嵌入的细粒度形态信息。在约80万条未标记心电图记录上使用掩码预测目标进行预训练。

Result: 尽管仅使用单导联，RhythmBERT在多种心脏状况上达到或超越了强12导联基线的性能，包括房颤、细微ST-T异常和心肌梗死等临床挑战性病例。

Conclusion: 将心电图视为结构化语言为心脏分析提供了一条可扩展且生理对齐的途径，能够学习上下文表示并实现标签高效的学习。

Abstract: Electrocardiogram (ECG) analysis is crucial for diagnosing heart disease, but most self-supervised learning methods treat ECG as a generic time series, overlooking physiologic semantics and rhythm-level structure. Existing contrastive methods utilize augmentations that distort morphology, whereas generative approaches employ fixed-window segmentation, which misaligns cardiac cycles. To address these limitations, we propose RhythmBERT, a generative ECG language model that considers ECG as a language paradigm by encoding P, QRS, and T segments into symbolic tokens via autoencoder-based latent representations. These discrete tokens capture rhythm semantics, while complementary continuous embeddings retain fine-grained morphology, enabling a unified view of waveform structure and rhythm. RhythmBERT is pretrained on approximately 800,000 unlabeled ECG recordings with a masked prediction objective, allowing it to learn contextual representations in a label-efficient manner. Evaluations show that despite using only a single lead, RhythmBERT achieves comparable or superior performance to strong 12-lead baselines. This generalization extends from prevalent conditions such as atrial fibrillation to clinically challenging cases such as subtle ST-T abnormalities and myocardial infarction. Our results suggest that considering ECG as structured language offers a scalable and physiologically aligned pathway for advancing cardiac analysis.

</details>


### [104] [Physics-informed neural particle flow for the Bayesian update step](https://arxiv.org/abs/2602.23089)
*Domonkos Csuzdi,Tamás Bécsi,Olivér Törő*

Main category: cs.LG

TL;DR: 提出一种基于物理信息神经网络的粒子流滤波方法，通过将概率传输的几何结构嵌入损失函数，实现无监督训练，解决高维非线性估计中的贝叶斯更新计算难题。


<details>
  <summary>Details</summary>
Motivation: 高维非线性估计中贝叶斯更新步骤存在显著计算挑战。现有对数同伦粒子流滤波器通常产生刚性微分方程，而深度学习近似方法要么将更新视为黑盒任务，要么依赖渐近松弛，忽略了有限时域概率传输的精确几何结构。

Method: 提出物理信息神经粒子流，这是一种摊销推理框架。通过将先验到后验密度函数的对数同伦轨迹与描述密度演化的连续性方程耦合，推导出主PDE。将该PDE作为物理约束嵌入损失函数，训练神经网络近似传输速度场，实现纯无监督训练。

Result: 神经参数化作为隐式正则化器，缓解了分析流的数值刚性，降低了在线计算复杂度。在多模态基准测试和具有挑战性的非线性场景中的实验验证表明，相比最先进的基线方法，该方法具有更好的模式覆盖和鲁棒性。

Conclusion: 提出的物理信息神经粒子流框架成功地将概率传输的几何结构融入深度学习，实现了高效的无监督贝叶斯更新，为高维非线性估计提供了有效的解决方案。

Abstract: The Bayesian update step poses significant computational challenges in high-dimensional nonlinear estimation. While log-homotopy particle flow filters offer an alternative to stochastic sampling, existing formulations usually yield stiff differential equations. Conversely, existing deep learning approximations typically treat the update as a black-box task or rely on asymptotic relaxation, neglecting the exact geometric structure of the finite-horizon probability transport. In this work, we propose a physics-informed neural particle flow, which is an amortized inference framework. To construct the flow, we couple the log-homotopy trajectory of the prior to posterior density function with the continuity equation describing the density evolution. This derivation yields a governing partial differential equation (PDE), referred to as the master PDE. By embedding this PDE as a physical constraint into the loss function, we train a neural network to approximate the transport velocity field. This approach enables purely unsupervised training, eliminating the need for ground-truth posterior samples. We demonstrate that the neural parameterization acts as an implicit regularizer, mitigating the numerical stiffness inherent to analytic flows and reducing online computational complexity. Experimental validation on multimodal benchmarks and a challenging nonlinear scenario confirms better mode coverage and robustness compared to state-of-the-art baselines.

</details>


### [105] [PRAC: Principal-Random Subspace for LLM Activation Compression and Memory-Efficient Training](https://arxiv.org/abs/2602.23111)
*Yanyi Li,Yimu Zhang,Cong Fang*

Main category: cs.LG

TL;DR: PRAC是一种用于大语言模型激活压缩的新方法，通过主成分-随机子空间分解，在保证梯度无偏估计的同时最小化方差，实现高达36%的内存减少且性能损失可忽略。


<details>
  <summary>Details</summary>
Motivation: 在大批量LLM训练中，激活已成为主要内存瓶颈。现有压缩方法未能利用激活的谱结构，导致收敛缓慢或压缩有限。需要一种既能有效压缩又不影响训练性能的方法。

Method: PRAC将激活分解为两个组件：通过SVD捕获的主子空间保留主要信息，以及从正交补空间采样的随机子空间近似尾部信息。通过引入精确的缩放因子，确保梯度估计的无偏性和最小方差。

Result: 在预训练和微调任务上的广泛实验表明，PRAC实现了高达36%的总内存减少，性能退化可忽略，计算成本最小。

Conclusion: PRAC通过利用激活的谱结构，在压缩效率和训练性能之间取得了良好平衡，为解决LLM训练中的内存瓶颈问题提供了有效方案。

Abstract: Activations have become the primary memory bottleneck in large-batch LLM training. However, existing compression methods fail to exploit the spectral structure of activations, resulting in slow convergence or limited compression. To address this, we bridge the relationship between the algorithm's fast convergence and the requirements for subspace projection, and show that an effective compression should yield an unbiased estimate of the original activation with low variance. We propose Principal-Random Subspace for LLM Activation Compression (PRAC), which novelly decomposes activations into two components: a principal subspace captured via SVD to retain dominant information, and a random subspace sampled from the orthogonal complement to approximate the tail. By introducing a precise scaling factor, we prove that PRAC yields an unbiased gradient estimator with minimum variance under certain conditions. Extensive experiments on pre-training and fine-tuning tasks demonstrate that PRAC achieves up to 36% total memory reduction with negligible performance degradation and minimal computational cost.

</details>


### [106] [Learning Physical Operators using Neural Operators](https://arxiv.org/abs/2602.23113)
*Vignesh Gopakumar,Ander Gray,Dan Giles,Lorenzo Zanisi,Matt J. Kusner,Timo Betcke,Stanislas Pamela,Marc Peter Deisenroth*

Main category: cs.LG

TL;DR: 提出基于算子分裂的物理信息训练框架，将PDE分解为线性和非线性算子分别处理，构建混合专家架构实现跨物理域泛化，并支持连续时间预测。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子作为PDE代理模型存在泛化能力有限（难以超越训练分布）和时间离散化固定的问题，需要一种能适应新物理机制且支持连续时间预测的方法。

Method: 采用算子分裂方法分解PDE，训练独立神经算子学习非线性物理算子，用固定有限差分卷积近似线性算子；构建模块化混合专家架构，将建模任务表述为神经ODE，通过标准ODE求解器实现连续时间预测。

Result: 在不可压缩和可压缩Navier-Stokes方程上验证，相比现有方法获得更好的收敛性和对未见物理机制的泛化性能；保持参数高效，支持超越训练时间范围的外推，并提供可验证物理行为的可解释组件。

Conclusion: 该物理信息训练框架通过显式编码底层算子结构，解决了神经算子的泛化和时间离散化限制，为PDE建模提供了更灵活、可解释且泛化能力强的解决方案。

Abstract: Neural operators have emerged as promising surrogate models for solving partial differential equations (PDEs), but struggle to generalise beyond training distributions and are often constrained to a fixed temporal discretisation. This work introduces a physics-informed training framework that addresses these limitations by decomposing PDEs using operator splitting methods, training separate neural operators to learn individual non-linear physical operators while approximating linear operators with fixed finite-difference convolutions. This modular mixture-of-experts architecture enables generalisation to novel physical regimes by explicitly encoding the underlying operator structure. We formulate the modelling task as a neural ordinary differential equation (ODE) where these learned operators constitute the right-hand side, enabling continuous-in-time predictions through standard ODE solvers and implicitly enforcing PDE constraints. Demonstrated on incompressible and compressible Navier-Stokes equations, our approach achieves better convergence and superior performance when generalising to unseen physics. The method remains parameter-efficient, enabling temporal extrapolation beyond training horizons, and provides interpretable components whose behaviour can be verified against known physics.

</details>


### [107] [Regularized Online RLHF with Generalized Bilinear Preferences](https://arxiv.org/abs/2602.23116)
*Junghyun Lee,Minju Hong,Kwang-Sung Jun,Chulhee Yun,Se-Young Yun*

Main category: cs.LG

TL;DR: 本文研究具有一般偏好的上下文在线RLHF问题，采用广义双线性偏好模型处理非传递性偏好，提出了两种算法实现无指数依赖的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有在线RLHF研究主要局限于反向KL正则化，无法处理一般偏好模型。本文旨在扩展在线RLHF到任意强凸正则化器，解决高维环境下的统计效率问题。

Method: 采用广义双线性偏好模型(GBPM)通过低秩斜对称矩阵捕捉非传递性偏好。提出两种算法：1) Greedy Sampling算法；2) Explore-Then-Commit算法，利用低秩结构。

Result: 1) Greedy Sampling获得$\tilde{O}(ηd^4 (\log T)^2)$的多对数遗憾，无指数依赖；2) Explore-Then-Commit获得$\tilde{O}(\sqrt{ηr T})$的遗憾，无多项式维度依赖，是高维在线RLHF的首个统计效率保证。

Conclusion: 本文为一般偏好的在线RLHF提供了理论框架，证明了强凸正则化下贪婪策略对偶间隙的平方误差界，提出了两种高效算法，解决了高维环境下的统计效率问题。

Abstract: We consider the problem of contextual online RLHF with general preferences, where the goal is to identify the Nash Equilibrium. We adopt the Generalized Bilinear Preference Model (GBPM) to capture potentially intransitive preferences via low-rank, skew-symmetric matrices. We investigate general preference learning with any strongly convex regularizer (where $η^{-1}$ is the regularization strength), generalizing beyond prior works limited to reverse KL-regularization. Central to our analysis is proving that the dual gap of the greedy policy is bounded by the square of the estimation error - a result derived solely from strong convexity and the skew-symmetricity of GBPM.Building on this insight and a feature diversity assumption, we establish two regret bounds via two simple algorithms: (1) Greedy Sampling achieves polylogarithmic, $e^{O(η)}$-free regret $\tilde{O}(ηd^4 (\log T)^2)$. (2) Explore-Then-Commit achieves $\mathrm{poly}(d)$-free regret $\tilde{O}(\sqrt{ηr T})$ by exploiting the low-rank structure; this is the first statistically efficient guarantee for online RLHF in high-dimensions.

</details>


### [108] [Bound to Disagree: Generalization Bounds via Certifiable Surrogates](https://arxiv.org/abs/2602.23128)
*Mathieu Bazinet,Valentina Zantedeschi,Pascal Germain*

Main category: cs.LG

TL;DR: 提出基于分歧的泛化证书方法，通过代理模型评估目标模型风险，无需修改目标模型或训练过程


<details>
  <summary>Details</summary>
Motivation: 深度学习模型的泛化界限通常存在三个问题：过于宽松（vacuous）、不可计算、或仅限于特定模型类别。本文旨在解决这些问题，为任意两个预测器之间的真实风险差距提供新的基于分歧的证书。

Method: 提出基于分歧的证书方法：1）通过具有紧致泛化保证的代理模型来界定目标预测器的真实风险；2）在未标记数据集上评估分歧界限。使用三种不同框架训练代理模型：样本压缩、模型压缩和PAC-Bayes理论。

Result: 经验证明所获得的证书具有紧致性，并展示了该方法的通用性。重要的是，这些保证是在不修改目标模型、也不调整训练过程以适应泛化框架的情况下实现的。

Conclusion: 该方法提供了一种灵活且实用的泛化保证框架，能够为深度学习模型提供可计算且紧致的风险界限，突破了传统泛化界限的限制。

Abstract: Generalization bounds for deep learning models are typically vacuous, not computable or restricted to specific model classes. In this paper, we tackle these issues by providing new disagreement-based certificates for the gap between the true risk of any two predictors. We then bound the true risk of the predictor of interest via a surrogate model that enjoys tight generalization guarantees, and evaluating our disagreement bound on an unlabeled dataset. We empirically demonstrate the tightness of the obtained certificates and showcase the versatility of the approach by training surrogate models leveraging three different frameworks: sample compression, model compression and PAC-Bayes theory. Importantly, such guarantees are achieved without modifying the target model, nor adapting the training procedure to the generalization framework.

</details>


### [109] [DyGnROLE: Modeling Asymmetry in Dynamic Graphs with Node-Role-Oriented Latent Encoding](https://arxiv.org/abs/2602.23135)
*Tyler Bonnet,Marek Rei*

Main category: cs.LG

TL;DR: DyGnROLE：一种基于Transformer的动态图架构，通过分离源节点和目标节点的表示，结合角色语义位置编码和自监督预训练，显著提升动态图学习性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的动态图通常是有向的，源节点和目标节点表现出不对称的行为模式和时序动态。现有动态图架构大多使用共享参数处理源节点和目标节点，缺乏系统性的角色感知建模。

Method: 提出DyGnROLE架构：1）使用独立的嵌入词汇表分离源节点和目标节点表示；2）引入角色语义位置编码捕捉每个角色独特的结构和时序上下文；3）提出自监督预训练目标TCLP（时序对比链接预测），利用未标注的交互历史学习角色特定表示。

Result: 在未来边分类任务上，DyGnROLE显著优于多种最先进的基线方法，证明了角色感知建模在动态图学习中的有效性。

Conclusion: 角色感知建模是动态图学习的有效策略，通过显式解耦源节点和目标节点表示，结合专门的嵌入和自监督预训练，能够更好地捕捉有向动态图中的不对称行为模式。

Abstract: Real-world dynamic graphs are often directed, with source and destination nodes exhibiting asymmetrical behavioral patterns and temporal dynamics. However, existing dynamic graph architectures largely rely on shared parameters for processing source and destination nodes, with limited or no systematic role-aware modeling. We propose DyGnROLE (Dynamic Graph Node-Role-Oriented Latent Encoding), a transformer-based architecture that explicitly disentangles source and destination representations. By using separate embedding vocabularies and role-semantic positional encodings, the model captures the distinct structural and temporal contexts unique to each role. Critical to the effectiveness of these specialized embeddings in low-label regimes is a self-supervised pretraining objective we introduce: Temporal Contrastive Link Prediction (TCLP). The pretraining uses the full unlabeled interaction history to encode informative structural biases, enabling the model to learn role-specific representations without requiring annotated data. Evaluation on future edge classification demonstrates that DyGnROLE substantially outperforms a diverse set of state-of-the-art baselines, establishing role-aware modeling as an effective strategy for dynamic graph learning.

</details>


### [110] [Prediction of Diffusion Coefficients in Mixtures with Tensor Completion](https://arxiv.org/abs/2602.23142)
*Zeno Romero,Kerstin Münnemann,Hans Hasse,Fabian Jirasek*

Main category: cs.LG

TL;DR: 提出混合张量补全方法预测二元混合物中温度依赖的无限稀释扩散系数，结合贝叶斯框架和主动学习策略提升预测精度


<details>
  <summary>Details</summary>
Motivation: 混合物中扩散系数的预测对许多应用至关重要，但实验数据稀缺。现有矩阵补全方法仅限于单温度预测，且精度严重依赖高质量实验数据。需要开发能预测温度依赖扩散系数的方法。

Method: 提出混合张量补全方法，采用Tucker分解，在298K、313K和333K的实验数据上联合训练。将SEGWE半经验模型的预测作为贝叶斯训练框架的先验知识。通过主动学习策略扩展实验数据库，使用PFG NMR测量了19个溶质+溶剂系统的扩散系数。

Result: TCM能在268K至378K温度范围内线性外推，相比现有模型在所有研究温度下都显著提高了预测精度。加入主动学习获得的新实验数据后，TCM的预测准确性得到实质性改善。

Conclusion: 结合数据高效的机器学习方法与自适应实验策略，能够显著推进传输性质的预测建模，为解决实验数据稀缺问题提供了有效途径。

Abstract: Predicting diffusion coefficients in mixtures is crucial for many applications, as experimental data remain scarce, and machine learning (ML) offers promising alternatives to established semi-empirical models. Among ML models, matrix completion methods (MCMs) have proven effective in predicting thermophysical properties, including diffusion coefficients in binary mixtures. However, MCMs are restricted to single-temperature predictions, and their accuracy depends strongly on the availability of high-quality experimental data for each temperature of interest. In this work, we address this challenge by presenting a hybrid tensor completion method (TCM) for predicting temperature-dependent diffusion coefficients at infinite dilution in binary mixtures. The TCM employs a Tucker decomposition and is jointly trained on experimental data for diffusion coefficients at infinite dilution in binary systems at 298 K, 313 K, and 333 K. Predictions from the semi-empirical SEGWE model serve as prior knowledge within a Bayesian training framework. The TCM then extrapolates linearly to any temperature between 268 K and 378 K, achieving markedly improved prediction accuracy compared to established models across all studied temperatures. To further enhance predictive performance, the experimental database was expanded using active learning (AL) strategies for targeted acquisition of new diffusion data by pulsed-field gradient (PFG) NMR measurements. Diffusion coefficients at infinite dilution in 19 solute + solvent systems were measured at 298 K, 313 K, and 333 K. Incorporating these results yields a substantial improvement in the TCM's predictive accuracy. These findings highlight the potential of combining data-efficient ML methods with adaptive experimentation to advance predictive modeling of transport properties.

</details>


### [111] [Partial recovery of meter-scale surface weather](https://arxiv.org/abs/2602.23146)
*Jonathan Giezendanner,Qidong Yang,Eric Schmitt,Anirban Chandra,Daniel Salles Civitarese,Johannes Jakubik,Jeremy Vila,Detlef Hohl,Campbell Watson,Sherrie Wang*

Main category: cs.LG

TL;DR: 论文提出一种方法，通过结合稀疏地面站观测和高分辨率地球观测数据，从粗尺度大气状态推断出10米分辨率的近地表气象场，显著提高了风、温度和湿度的预测精度。


<details>
  <summary>Details</summary>
Motivation: 当前天气分析和预报缺乏米尺度的近地表气象变异性，这种变异性可能由地表特征和大尺度大气强迫决定，但尚不清楚其是否可预测。研究旨在探索是否可以从现有观测中统计恢复米尺度近地表天气的可预测成分。

Method: 通过将粗尺度大气状态（如ERA5再分析数据）与稀疏地面站测量和高分辨率地球观测数据（如土地覆盖、地形等）相结合，使用条件推断方法生成连续空间场。该方法在10米分辨率下推断美国本土的近地表风、温度和湿度场。

Result: 相比ERA5，推断场将风误差降低29%，温度和露点误差降低6%，并在固定时间步长上解释了更多的空间方差。结果显示出可物理解释的结构，如城市热岛、蒸散发驱动的湿度对比以及不同土地覆盖类型间的风速差异。

Conclusion: 研究表明米尺度近地表天气存在可统计恢复的物理相干成分，扩展了天气建模的前沿，展示了计算可行的洲际尺度米分辨率推断方法。更广泛地，说明了如何通过将粗尺度动力模型与静态精细尺度特征相结合，揭示地球系统中先前未解析的组分。

Abstract: Near-surface atmospheric conditions can differ sharply over tens to hundreds of meters due to land cover and topography, yet this variability is absent from current weather analyses and forecasts. It is unclear whether such meter-scale variability reflects irreducibly chaotic dynamics or contains a component predictable from surface characteristics and large-scale atmospheric forcing. Here we show that a substantial, physically coherent component of meter-scale near-surface weather is statistically recoverable from existing observations. By conditioning coarse atmospheric state on sparse surface station measurements and high-resolution Earth observation data, we infer spatially continuous fields of near-surface wind, temperature, and humidity at 10 m resolution across the contiguous United States. Relative to ERA5, the inferred fields reduce wind error by 29% and temperature and dewpoint error by 6%, while explaining substantially more spatial variance at fixed time steps. They also exhibit physically interpretable structure, including urban heat islands, evapotranspiration-driven humidity contrasts, and wind speed differences across land cover types. Our findings expand the frontier of weather modeling by demonstrating a computationally feasible approach to continental-scale meter-resolution inference. More broadly, they illustrate how conditioning coarse dynamical models on static fine-scale features can reveal previously unresolved components of the Earth system.

</details>


### [112] [Benchmarking Temporal Web3 Intelligence: Lessons from the FinSurvival 2025 Challenge](https://arxiv.org/abs/2602.23159)
*Oshani Seneviratne,Fernando Spadea,Adrien Pavao,Aaron Micah Green,Kristin P. Bennett*

Main category: cs.LG

TL;DR: FinSurvival Challenge 2025作为Temporal Web3智能基准测试案例研究，使用Aave v3协议的2180万交易记录构建16个生存预测任务，展示领域感知时序特征构建优于通用建模方法


<details>
  <summary>Details</summary>
Motivation: Temporal Web分析需要大规模纵向数据理解用户、内容和系统随时间演变，但缺乏捕捉真实世界时序动态（特别是审查和非平稳性）的可重复基准，这阻碍了方法学进步和Web3与更广泛Web领域之间的技术转移

Method: 使用Aave v3协议的2180万交易记录，操作化16个生存预测任务来建模用户行为转换，设计基准并分析获胜解决方案

Result: 领域感知时序特征构建显著优于通用建模方法，Web3系统为研究流失、风险和演化等时序挑战提供了高保真沙箱

Conclusion: Web3系统可作为研究基础Web时序挑战的高保真沙箱，FinSurvival Challenge 2025展示了如何为Temporal Web3智能设计有效基准，强调领域知识在时序建模中的重要性

Abstract: Temporal Web analytics increasingly relies on large-scale, longitudinal data to understand how users, content, and systems evolve over time. A rapidly growing frontier is the \emph{Temporal Web3}: decentralized platforms whose behavior is recorded as immutable, time-stamped event streams. Despite the richness of this data, the field lacks shared, reproducible benchmarks that capture real-world temporal dynamics, specifically censoring and non-stationarity, across extended horizons. This absence slows methodological progress and limits the transfer of techniques between Web3 and broader Web domains. In this paper, we present the \textit{FinSurvival Challenge 2025} as a case study in benchmarking \emph{temporal Web3 intelligence}. Using 21.8 million transaction records from the Aave v3 protocol, the challenge operationalized 16 survival prediction tasks to model user behavior transitions.We detail the benchmark design and the winning solutions, highlighting how domain-aware temporal feature construction significantly outperformed generic modeling approaches. Furthermore, we distill lessons for next-generation temporal benchmarks, arguing that Web3 systems provide a high-fidelity sandbox for studying temporal challenges, such as churn, risk, and evolution that are fundamental to the wider Web.

</details>


### [113] [MetaOthello: A Controlled Study of Multiple World Models in Transformers](https://arxiv.org/abs/2602.23164)
*Aviral Chawla,Galen Hall,Juniper Lovato*

Main category: cs.LG

TL;DR: 论文提出了MetaOthello框架，通过训练小型GPT模型在多种Othello变体游戏上，研究transformer如何在共享表示空间中组织多个世界模型。


<details>
  <summary>Details</summary>
Motivation: 基础模型需要处理多个生成过程，但现有的机制可解释性研究主要关注孤立能力。目前尚不清楚单个transformer如何组织多个可能冲突的"世界模型"。之前的Othello神经网络实验只关注单一游戏规则。

Method: 引入MetaOthello框架，包含共享语法但规则或分词不同的Othello变体。训练小型GPT模型在混合变体数据上，研究多个世界模型在共享表示空间中的组织方式。

Result: transformer在混合游戏数据上训练时，不会将容量分割为孤立子模型，而是收敛到大部分共享的棋盘状态表示，该表示可在变体间因果传递。线性探针在一个变体上训练后，可干预另一个变体的内部状态，效果接近匹配探针。对于同构游戏，表示通过单个正交旋转等价。

Conclusion: MetaOthello为理解transformer是否学习世界模型以及如何同时组织多个世界模型提供了路径。当规则部分重叠时，早期层保持游戏无关表示，中间层识别游戏身份，后期层专门化。

Abstract: Foundation models must handle multiple generative processes, yet mechanistic interpretability largely studies capabilities in isolation; it remains unclear how a single transformer organizes multiple, potentially conflicting "world models". Previous experiments on Othello playing neural-networks test world-model learning but focus on a single game with a single set of rules. We introduce MetaOthello, a controlled suite of Othello variants with shared syntax but different rules or tokenizations, and train small GPTs on mixed-variant data to study how multiple world models are organized in a shared representation space. We find that transformers trained on mixed-game data do not partition their capacity into isolated sub-models; instead, they converge on a mostly shared board-state representation that transfers causally across variants. Linear probes trained on one variant can intervene on another's internal state with effectiveness approaching that of matched probes. For isomorphic games with token remapping, representations are equivalent up to a single orthogonal rotation that generalizes across layers. When rules partially overlap, early layers maintain game-agnostic representations while a middle layer identifies game identity, and later layers specialize. MetaOthello offers a path toward understanding not just whether transformers learn world models, but how they organize many at once.

</details>


### [114] [Induction Meets Biology: Mechanisms of Repeat Detection in Protein Language Models](https://arxiv.org/abs/2602.23179)
*Gal Kesten-Pomeranz,Yaniv Nikankin,Anja Reusch,Tomer Tsaban,Ora Schueler-Furman,Yonatan Belinkov*

Main category: cs.LG

TL;DR: PLMs通过结合语言模式匹配和生物专业知识来识别蛋白质序列中的精确和近似重复片段，揭示了其内部机制


<details>
  <summary>Details</summary>
Motivation: 蛋白质序列中存在大量重复片段（精确和近似），这些重复对蛋白质结构和功能很重要，但PLMs如何内部识别这些重复的机制尚不清楚

Method: 通过研究PLMs在掩码标记预测中的行为，分析其内部机制，特别关注近似重复的检测如何包含精确重复的机制

Result: 发现PLMs识别重复的机制分为两个阶段：1）使用通用位置注意力头和生物专门化组件构建特征表示；2）诱导头关注重复片段间的对齐标记

Conclusion: PLMs通过结合语言模式匹配和生物专业知识来解决这一生物任务，为研究更复杂的进化过程奠定了基础

Abstract: Protein sequences are abundant in repeating segments, both as exact copies and as approximate segments with mutations. These repeats are important for protein structure and function, motivating decades of algorithmic work on repeat identification. Recent work has shown that protein language models (PLMs) identify repeats, by examining their behavior in masked-token prediction. To elucidate their internal mechanisms, we investigate how PLMs detect both exact and approximate repeats. We find that the mechanism for approximate repeats functionally subsumes that of exact repeats. We then characterize this mechanism, revealing two main stages: PLMs first build feature representations using both general positional attention heads and biologically specialized components, such as neurons that encode amino-acid similarity. Then, induction heads attend to aligned tokens across repeated segments, promoting the correct answer. Our results reveal how PLMs solve this biological task by combining language-based pattern matching with specialized biological knowledge, thereby establishing a basis for studying more complex evolutionary processes in PLMs.

</details>


### [115] [Closing the gap on tabular data with Fourier and Implicit Categorical Features](https://arxiv.org/abs/2602.23182)
*Marius Dragoi,Florin Gogianu,Elena Burceanu*

Main category: cs.LG

TL;DR: 该论文提出了一种统计特征处理方法，通过识别离散化后与目标强相关的特征，并结合学习傅里叶变换来缓解深度模型对平滑解的偏好，从而提升深度学习在表格数据上的性能，使其接近或超越XGBoost。


<details>
  <summary>Details</summary>
Motivation: 深度学习在表格数据上表现不如树模型，作者认为树模型能更好地建模具有类别特征的非线性交互，而神经网络偏向均匀数值处理和平滑解，难以有效利用此类模式。

Method: 使用基于统计的特征处理技术识别离散化后与目标强相关的特征，并采用学习傅里叶变换来缓解深度模型对平滑解的偏好，这种偏好与数据固有特性不符。

Result: 提出的特征预处理显著提升了深度学习模型的性能，使其在综合表格数据基准测试中接近或超越XGBoost的表现。

Conclusion: 通过针对性的特征处理和缓解模型偏差，深度学习在表格数据上的性能瓶颈可以被有效突破，达到与树模型相当甚至更好的效果。

Abstract: While Deep Learning has demonstrated impressive results in applications on various data types, it continues to lag behind tree-based methods when applied to tabular data, often referred to as the last "unconquered castle" for neural networks. We hypothesize that a significant advantage of tree-based methods lies in their intrinsic capability to model and exploit non-linear interactions induced by features with categorical characteristics. In contrast, neural-based methods exhibit biases toward uniform numerical processing of features and smooth solutions, making it challenging for them to effectively leverage such patterns. We address this performance gap by using statistical-based feature processing techniques to identify features that are strongly correlated with the target once discretized. We further mitigate the bias of deep models for overly-smooth solutions, a bias that does not align with the inherent properties of the data, using Learned Fourier. We show that our proposed feature preprocessing significantly boosts the performance of deep learning models and enables them to achieve a performance that closely matches or surpasses XGBoost on a comprehensive tabular data benchmark.

</details>


### [116] [Efficient Real-Time Adaptation of ROMs for Unsteady Flows Using Data Assimilation](https://arxiv.org/abs/2602.23188)
*Ismaël Zighed,Andrea Nóvoa,Luca Magri,Taraneh Sayadi*

Main category: cs.LG

TL;DR: 提出一种高效的参数化降阶模型重训练策略，仅需稀疏观测数据即可达到接近完全重训练的精度，计算成本大幅降低。


<details>
  <summary>Details</summary>
Motivation: 传统降阶模型在参数变化时需要完全重训练，计算成本高。需要一种能实时适应新参数区域的高效重训练方法。

Method: 采用编码-处理-解码架构：变分自编码器进行降维，Transformer网络演化潜在状态并建模动力学。模型参数化处理外部控制变量（如雷诺数），利用注意力机制捕获时空依赖和参数效应。通过集成卡尔曼滤波同化稀疏数据。

Result: 模型在Navier-Stokes设置中表现出色，仅需少量稀疏数据即可适应样本外参数区域。发现预测误差主要来自潜在流形扭曲而非潜在动力学变化，因此重训练可仅限于自编码器部分。

Conclusion: 提出了一种轻量级、计算高效、支持实时适应的重训练策略，仅需稀疏微调数据即可实现高性能参数化降阶模型，为实时预测和不确定性量化提供了实用解决方案。

Abstract: We propose an efficient retraining strategy for a parameterized Reduced Order Model (ROM) that attains accuracy comparable to full retraining while requiring only a fraction of the computational time and relying solely on sparse observations of the full system. The architecture employs an encode-process-decode structure: a Variational Autoencoder (VAE) to perform dimensionality reduction, and a transformer network to evolve the latent states and model the dynamics. The ROM is parameterized by an external control variable, the Reynolds number in the Navier-Stokes setting, with the transformer exploiting attention mechanisms to capture both temporal dependencies and parameter effects. The probabilistic VAE enables stochastic sampling of trajectory ensembles, providing predictive means and uncertainty quantification through the first two moments. After initial training on a limited set of dynamical regimes, the model is adapted to out-of-sample parameter regions using only sparse data. Its probabilistic formulation naturally supports ensemble generation, which we employ within an ensemble Kalman filtering framework to assimilate data and reconstruct full-state trajectories from minimal observations. We further show that, for the dynamical system considered, the dominant source of error in out-of-sample forecasts stems from distortions of the latent manifold rather than changes in the latent dynamics. Consequently, retraining can be limited to the autoencoder, allowing for a lightweight, computationally efficient, real-time adaptation procedure with very sparse fine-tuning data.

</details>


### [117] [InnerQ: Hardware-aware Tuning-free Quantization of KV Cache for Large Language Models](https://arxiv.org/abs/2602.23200)
*Sayed Mohammadreza Tayaranian Hosseini,Amir Ardakani,Warren J. Gross*

Main category: cs.LG

TL;DR: InnerQ是一种硬件感知的KV缓存量化方案，通过内维度分组量化、混合量化策略和高精度窗口等技术，在保持精度的同时显著降低解码延迟。


<details>
  <summary>Details</summary>
Motivation: 大语言模型解码过程中，KV缓存的大小随序列长度线性增长，成为内存占用的主要瓶颈。现有量化方法主要关注压缩KV缓存并保持其信息，但缺乏对硬件效率的优化。

Method: 1. 内维度分组量化：在KV缓存的内维度上进行分组，使反量化与向量-矩阵乘法对齐，实现尺度因子在GPU计算单元间的复用
2. 混合量化：根据局部统计为每组选择对称或非对称量化
3. 高精度窗口：对最近token和注意力汇聚token使用高精度表示以缓解异常值泄露
4. 每通道归一化：对key缓存进行每通道归一化，在预填充阶段计算并折叠到query中避免运行时开销

Result: 在Llama模型上的评估显示：1. 比先前工作提速达22%；2. 比半精度向量-矩阵乘法提速达88%；3. 保持与非量化KV缓存相当的few-shot GSM8K性能；4. 超越先前的KV缓存量化方法

Conclusion: InnerQ通过硬件感知的KV缓存量化设计，在保持模型精度的同时显著降低解码延迟，为大语言模型的长序列生成提供了高效的解决方案。

Abstract: Reducing the hardware footprint of large language models (LLMs) during decoding is critical for efficient long-sequence generation. A key bottleneck is the key-value (KV) cache, whose size scales with sequence length and easily dominates the memory footprint of the model. Previous work proposed quantization methods that are focused on compressing the KV cache while maintaining its information. We introduce InnerQ, a hardware-aware KV-cache quantization scheme that lowers decode latency without sacrificing accuracy. InnerQ applies group-wise quantization while grouping the cache matrices over their inner dimension. Unlike previous work that group over the outer dimension, InnerQ aligns dequantization with the vector-matrix multiplication and enables scale factor reuse across GPU compute units. This reduces memory accesses and accelerates dequantization, yielding up to $22\%$ speedup over previous work and up to $88\%$ over half-precision vector-matrix multiplication. To preserve fidelity under aggressive compression, InnerQ incorporates (i) hybrid quantization, selecting symmetric or asymmetric quantization per group based on local statistics; (ii) high-precision windows for both the most recent tokens and the attention sink tokens to mitigate outlier leakage; and (iii) per-channel normalization of the key cache, computed once during prefill and folded into the query to avoid runtime overhead. Our evaluation experiments on Llama models shows that InnerQ maintains a few-shot GSM8K performance comparable to non-quantized KV caches and surpasses prior KV cache quantization methods.

</details>


### [118] [Tell Me What To Learn: Generalizing Neural Memory to be Controllable in Natural Language](https://arxiv.org/abs/2602.23201)
*Max S. Bennett,Thomas P. Zollo,Richard Zemel*

Main category: cs.LG

TL;DR: 提出基于自然语言指令的通用神经记忆系统，允许用户控制模型记忆内容，解决传统方法假设单一固定目标和同质信息流的问题


<details>
  <summary>Details</summary>
Motivation: 现代机器学习模型需要在非平稳环境中持续适应新任务和知识，但现有神经记忆方法通常假设单一固定目标和同质信息流，用户无法控制模型记忆什么或忽略什么

Method: 提出通用神经记忆系统，基于自然语言指定的学习指令执行灵活更新，使自适应智能体能够从异构信息源中选择性学习

Result: 该方法支持医疗保健和客户服务等场景，其中固定目标的记忆更新不足，实现了对模型记忆内容的用户控制

Conclusion: 基于自然语言指令的神经记忆系统解决了传统方法的局限性，提供了更灵活、可控的持续学习机制，适用于需要选择性记忆的复杂应用场景

Abstract: Modern machine learning models are deployed in diverse, non-stationary environments where they must continually adapt to new tasks and evolving knowledge. Continual fine-tuning and in-context learning are costly and brittle, whereas neural memory methods promise lightweight updates with minimal forgetting. However, existing neural memory models typically assume a single fixed objective and homogeneous information streams, leaving users with no control over what the model remembers or ignores over time. To address this challenge, we propose a generalized neural memory system that performs flexible updates based on learning instructions specified in natural language. Our approach enables adaptive agents to learn selectively from heterogeneous information sources, supporting settings, such as healthcare and customer service, where fixed-objective memory updates are insufficient.

</details>


### [119] [Takeuchi's Information Criteria as Generalization Measures for DNNs Close to NTK Regime](https://arxiv.org/abs/2602.23219)
*Hiroki Naganuma,Taiji Suzuki,Rio Yokota,Masahiro Nomura,Kohta Ishikawa,Ikuro Sato*

Main category: cs.LG

TL;DR: 该研究探讨了竹内信息准则(TIC)在深度神经网络中的适用性，发现在接近神经正切核(NTK)机制时，TIC能有效解释泛化差距，但在NTK机制外则失效。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络等统计奇异模型的泛化度量难以建立，本研究旨在探究经典度量方法TIC在何种条件下能有效解释DNN的泛化差距。

Method: 理论分析TIC在NTK机制附近的适用性；实验训练超过5,000个DNN模型（12种架构，包括VGG-16等大型模型），在4个数据集上估计TIC值，使用多种TIC近似方法并评估计算精度权衡。

Result: 实验结果表明，在接近NTK机制的条件下，估计的TIC值与泛化差距有良好相关性；但在NTK机制外，这种相关性消失。TIC在超参数优化中比现有方法具有更好的试验剪枝能力。

Conclusion: TIC在接近NTK机制时能有效解释DNN的泛化差距，但在更一般的训练条件下适用性有限。TIC在超参数优化中展现出实用价值。

Abstract: Generalization measures have been studied extensively in the machine learning community to better characterize generalization gaps. However, establishing a reliable generalization measure for statistically singular models such as deep neural networks (DNNs) is difficult due to their complex nature. This study focuses on Takeuchi's information criterion (TIC) to investigate the conditions under which this classical measure can effectively explain the generalization gaps of DNNs. Importantly, the developed theory indicates the applicability of TIC near the neural tangent kernel (NTK) regime. In a series of experiments, we trained more than 5,000 DNN models with 12 architectures, including large models (e.g., VGG-16), on four datasets, and estimated the corresponding TIC values to examine the relationship between the generalization gap and the TIC estimates. We applied several TIC approximation methods with feasible computational costs and assessed the accuracy trade-off. Our experimental results indicate that the estimated TIC values correlate well with the generalization gap under conditions close to the NTK regime. However, we show both theoretically and empirically that outside the NTK regime such correlation disappears. Finally, we demonstrate that TIC provides better trial pruning ability than existing methods for hyperparameter optimization.

</details>


### [120] [Physics Informed Viscous Value Representations](https://arxiv.org/abs/2602.23280)
*Hrishikesh Viswanath,Juanwu Lu,S. Talha Bukhari,Damon Conover,Ziran Wang,Aniket Bera*

Main category: cs.LG

TL;DR: 提出一种基于Hamilton-Jacobi-Bellman方程粘性解的物理信息正则化方法，用于离线目标条件强化学习中的价值估计改进


<details>
  <summary>Details</summary>
Motivation: 离线目标条件强化学习中，由于静态数据集覆盖有限，准确的价值估计面临挑战。现有的基于一阶偏微分方程（如Eikonal方程）的物理信息方法在复杂高维环境中可能不适定

Method: 提出基于Hamilton-Jacobi-Bellman方程粘性解的物理信息正则化方法，通过最优控制理论提供物理先验，并利用Feynman-Kac定理将PDE解重新表述为期望，实现可处理的蒙特卡洛估计

Result: 实验表明该方法提高了几何一致性，可广泛应用于导航和高维复杂操作任务

Conclusion: 通过HJB方程的物理信息正则化，结合Feynman-Kac定理的蒙特卡洛估计，有效解决了离线GCRL中的价值估计问题，在复杂环境中表现良好

Abstract: Offline goal-conditioned reinforcement learning (GCRL) learns goal-conditioned policies from static pre-collected datasets. However, accurate value estimation remains a challenge due to the limited coverage of the state-action space. Recent physics-informed approaches have sought to address this by imposing physical and geometric constraints on the value function through regularization defined over first-order partial differential equations (PDEs), such as the Eikonal equation. However, these formulations can often be ill-posed in complex, high-dimensional environments. In this work, we propose a physics-informed regularization derived from the viscosity solution of the Hamilton-Jacobi-Bellman (HJB) equation. By providing a physics-based inductive bias, our approach grounds the learning process in optimal control theory, explicitly regularizing and bounding updates during value iterations. Furthermore, we leverage the Feynman-Kac theorem to recast the PDE solution as an expectation, enabling a tractable Monte Carlo estimation of the objective that avoids numerical instability in higher-order gradients. Experiments demonstrate that our method improves geometric consistency, making it broadly applicable to navigation and high-dimensional, complex manipulation tasks. Open-source codes are available at https://github.com/HrishikeshVish/phys-fk-value-GCRL.

</details>


### [121] [Conformalized Neural Networks for Federated Uncertainty Quantification under Dual Heterogeneity](https://arxiv.org/abs/2602.23296)
*Quang-Huy Nguyen,Jiaqi Wang,Wei-Shinn Ku*

Main category: cs.LG

TL;DR: FedWQ-CP：一种用于异构联邦学习的简单有效的不确定性量化方法，通过单轮通信实现代理-服务器校准，平衡覆盖性能与效率


<details>
  <summary>Details</summary>
Motivation: 联邦学习在不确定性量化方面面临挑战，现有方法通常单独处理数据异构性或模型异构性，忽视了它们对代理覆盖可靠性的联合影响。保形预测在异构联邦学习中的应用尚未充分探索。

Method: 提出FedWQ-CP方法：1）每个代理在本地校准数据上计算符合性分数并推导本地分位数阈值；2）代理仅传输分位数阈值和校准样本大小到服务器；3）服务器通过加权平均聚合这些阈值生成全局阈值；4）整个过程只需单轮通信。

Result: 在七个公共数据集（分类和回归任务）上的实验表明，FedWQ-CP在经验上保持了代理级和全局覆盖，同时产生最小的预测集或区间。

Conclusion: FedWQ-CP是一种简单有效的联邦不确定性量化方法，能够在双重异构性下平衡覆盖性能与效率，为异构联邦学习系统提供可靠的保形预测框架。

Abstract: Federated learning (FL) faces challenges in uncertainty quantification (UQ). Without reliable UQ, FL systems risk deploying overconfident models at under-resourced agents, leading to silent local failures despite seemingly satisfactory global performance. Existing federated UQ approaches often address data heterogeneity or model heterogeneity in isolation, overlooking their joint effect on coverage reliability across agents. Conformal prediction is a widely used distribution-free UQ framework, yet its applications in heterogeneous FL settings remains underexplored. We provide FedWQ-CP, a simple yet effective approach that balances empirical coverage performance with efficiency at both global and agent levels under the dual heterogeneity. FedWQ-CP performs agent-server calibration in a single communication round. On each agent, conformity scores are computed on calibration data and a local quantile threshold is derived. Each agent then transmits only its quantile threshold and calibration sample size to the server. The server simply aggregates these thresholds through a weighted average to produce a global threshold. Experimental results on seven public datasets for both classification and regression demonstrate that FedWQ-CP empirically maintains agent-wise and global coverage while producing the smallest prediction sets or intervals.

</details>


### [122] [Inferential Mechanics Part 1: Causal Mechanistic Theories of Machine Learning in Chemical Biology with Implications](https://arxiv.org/abs/2602.23303)
*Ilya Balabin,Thomas M. Kaiser*

Main category: cs.LG

TL;DR: 该论文提出了一种新的数学框架"推理力学"，将化学理论、生物理论、概率论和因果关系结合，以解决自然科学中机器学习模型的因果缺陷问题。


<details>
  <summary>Details</summary>
Motivation: 当前自然科学中的机器学习模型常被视为黑箱，缺乏对数据因果结构的深入考虑。虽然已有尝试将因果关系引入讨论，但缺乏统一的理论框架。

Method: 论文提出了"推理力学"框架，引入了"焦点"概念——机器学习算法在大型数据集中缩小到隐藏基础机制的能力。在Akt抑制剂家族上提供了初步证明。

Result: 建立了化学生物学的新数学框架，能够在不依赖还原论工具的情况下建模自然机制。这是三篇系列论文的第一部分，专注于因果结构框架。

Conclusion: 该系列论文旨在纠正自然科学中机器学习的因果缺陷，为化学生物学建立新的数学建模框架，后续两部分将分别探讨化学相似性和隐藏因果结构对机器学习的影响。

Abstract: Machine learning techniques are now routinely encountered in research laboratories across the globe. Impressive progress has been made through ML and AI techniques with regards to large data set processing. This progress has increased the ability of the experimenter to digest data and make novel predictions regarding phenomena of interest. However, machine learning predictors generated from data sets taken from the natural sciences are often treated as black boxes which are used broadly and generally without detailed consideration of the causal structure of the data set of interest. Work has been attempted to bring causality into discussions of machine learning models of natural phenomena; however, a firm and unified theoretical treatment is lacking. This series of three papers explores the union of chemical theory, biological theory, probability theory and causality that will correct current causal flaws of machine learning in the natural sciences. This paper, Part 1 of the series, provides the formal framework of the foundational causal structure of phenomena in chemical biology and is extended to machine learning through the novel concept of focus, defined here as the ability of a machine learning algorithm to narrow down to a hidden underpinning mechanism in large data sets. Initial proof of these principles on a family of Akt inhibitors is also provided. The second paper containing Part 2 will provide a formal exploration of chemical similarity, and Part 3 will present extensive experimental evidence of how hidden causal structures weaken all machine learning in chemical biology. This series serves to establish for chemical biology a new kind of mathematical framework for modeling mechanisms in Nature without the need for the tools of reductionism: inferential mechanics.

</details>


### [123] [A Proper Scoring Rule for Virtual Staining](https://arxiv.org/abs/2602.23305)
*Samuel Tonks,Steve Hood,Ryan Musso,Ceridwen Hopely,Steve Titus,Minh Doan,Iain Styles,Alexander Krull*

Main category: cs.LG

TL;DR: 提出信息增益(IG)作为生成式虚拟染色模型的细胞级评估框架，可直接评估预测后验分布，弥补现有方法仅评估边际分布的不足。


<details>
  <summary>Details</summary>
Motivation: 现有生成式虚拟染色模型评估方法只能检查数据集的边际分布准确性，无法评估预测的后验分布，而真实后验分布又不可得，需要新的评估框架。

Method: 引入信息增益(IG)作为细胞级评估框架，IG是严格适当的评分规则，具有理论动机，可实现可解释性，并能跨模型和特征比较结果。

Result: 在广泛的HTS数据集上评估扩散模型和GAN模型，IG能够揭示其他指标无法检测到的显著性能差异。

Conclusion: 信息增益(IG)为生成式虚拟染色模型提供了有效的后验分布评估框架，具有理论保证和实际应用价值。

Abstract: Generative virtual staining (VS) models for high-throughput screening (HTS) can provide an estimated posterior distribution of possible biological feature values for each input and cell. However, when evaluating a VS model, the true posterior is unavailable. Existing evaluation protocols only check the accuracy of the marginal distribution over the dataset rather than the predicted posteriors. We introduce information gain (IG) as a cell-wise evaluation framework that enables direct assessment of predicted posteriors. IG is a strictly proper scoring rule and comes with a sound theoretical motivation allowing for interpretability, and for comparing results across models and features. We evaluate diffusion- and GAN-based models on an extensive HTS dataset using IG and other metrics and show that IG can reveal substantial performance differences other metrics cannot.

</details>


### [124] [ParamMem: Augmenting Language Agents with Parametric Reflective Memory](https://arxiv.org/abs/2602.23320)
*Tianjun Yao,Yongqiang Chen,Yujia Zheng,Pan Li,Zhiqiang Shen,Kun Zhang*

Main category: cs.LG

TL;DR: 本文提出ParamMem参数化记忆模块，通过编码跨样本反思模式实现多样化反思生成，并基于此构建ParamAgent反思代理框架，在代码生成、数学推理和多跳问答任务上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有语言代理的自我反思机制往往产生重复输出，限制了推理性能。研究发现反思多样性与任务成功率呈强正相关，因此需要多样化的反思信号来提升代理性能。

Method: 提出ParamMem参数化记忆模块，将跨样本反思模式编码到模型参数中，通过温度控制采样实现多样化反思生成。基于此构建ParamAgent框架，整合参数化记忆与情景记忆和跨样本记忆。

Result: 在代码生成、数学推理和多跳问答任务上的实验表明，ParamAgent相比现有最优基线取得一致改进。ParamMem具有样本效率高、支持跨模型规模的弱到强迁移、无需依赖更强外部模型即可实现自我改进等优势。

Conclusion: ParamMem作为增强语言代理的有效组件具有显著潜力，能够通过参数化记忆实现多样化反思生成，提升代理的推理性能，且不依赖外部强模型支持。

Abstract: Self-reflection enables language agents to iteratively refine solutions, yet often produces repetitive outputs that limit reasoning performance. Recent studies have attempted to address this limitation through various approaches, among which increasing reflective diversity has shown promise. Our empirical analysis reveals a strong positive correlation between reflective diversity and task success, further motivating the need for diverse reflection signals. We introduce ParamMem, a parametric memory module that encodes cross-sample reflection patterns into model parameters, enabling diverse reflection generation through temperature-controlled sampling. Building on this module, we propose ParamAgent, a reflection-based agent framework that integrates parametric memory with episodic and cross-sample memory. Extensive experiments on code generation, mathematical reasoning, and multi-hop question answering demonstrate consistent improvements over state-of-the-art baselines. Further analysis reveals that ParamMem is sample-efficient, enables weak-to-strong transfer across model scales, and supports self-improvement without reliance on stronger external model, highlighting the potential of ParamMem as an effective component for enhancing language agents.

</details>


### [125] [Differentiable Zero-One Loss via Hypersimplex Projections](https://arxiv.org/abs/2602.23336)
*Camilo Gomez,Pengyang Wang,Liansheng Tang*

Main category: cs.LG

TL;DR: 提出Soft-Binary-Argmax算子，通过约束优化框架构建零一损失的可微分近似，解决大批量训练中的泛化性能下降问题


<details>
  <summary>Details</summary>
Motivation: 零一损失是分类任务的黄金标准，但由于其不可微分性无法用于基于梯度的优化。需要开发可微分的近似方法，以在端到端可微分模型中集成结构化优化组件，提供更强的归纳偏置和任务目标对齐。

Method: 通过约束优化框架构建平滑、保序的投影到n,k维超单纯形，提出Soft-Binary-Argmax算子。推导其数学性质，展示如何高效计算其Jacobian并集成到二元和多类学习系统中。

Result: 通过强制输出logits的几何一致性约束，在大批量训练中显著改善泛化性能，缩小传统大批量训练中观察到的性能差距。

Conclusion: 提出的Soft-Binary-Argmax算子为不可微分的零一损失提供了有效的可微分近似，能够改善大批量训练中的泛化性能，是集成结构化优化组件到端到端可微分模型的有价值方法。

Abstract: Recent advances in machine learning have emphasized the integration of structured optimization components into end-to-end differentiable models, enabling richer inductive biases and tighter alignment with task-specific objectives. In this work, we introduce a novel differentiable approximation to the zero-one loss-long considered the gold standard for classification performance, yet incompatible with gradient-based optimization due to its non-differentiability. Our method constructs a smooth, order-preserving projection onto the n,k-dimensional hypersimplex through a constrained optimization framework, leading to a new operator we term Soft-Binary-Argmax. After deriving its mathematical properties, we show how its Jacobian can be efficiently computed and integrated into binary and multiclass learning systems. Empirically, our approach achieves significant improvements in generalization under large-batch training by imposing geometric consistency constraints on the output logits, thereby narrowing the performance gap traditionally observed in large-batch training.

</details>


### [126] [Mean Estimation from Coarse Data: Characterizations and Efficient Algorithms](https://arxiv.org/abs/2602.23341)
*Alkis Kalavasis,Anay Mehrotra,Manolis Zampetakis,Felix Zhou,Ziyu Zhu*

Main category: cs.LG

TL;DR: 该论文解决了高斯均值估计中的两个核心问题：在凸划分下均值何时可识别，以及在可识别和凸划分条件下是否存在高效计算算法。


<details>
  <summary>Details</summary>
Motivation: 粗数据在实际应用中普遍存在（如测量舍入、传感器限制、经济系统滞后），但现有研究仅解决了凸划分下的样本效率问题，对于均值可识别条件和计算效率问题仍待解决。

Method: 研究高斯均值估计问题，其中每个真实样本来自d维高斯分布（单位协方差），但仅通过包含样本的划分集合观察到。分析凸划分下的可识别性条件，并设计计算高效的估计算法。

Result: 解决了两个开放问题：1）确定了凸划分下均值可识别的条件；2）证明了在可识别和凸划分条件下存在计算高效的估计算法。

Conclusion: 该工作完全解决了高斯均值估计中关于可识别性和计算效率的两个基本问题，为粗数据下的统计推断提供了理论保证。

Abstract: Coarse data arise when learners observe only partial information about samples; namely, a set containing the sample rather than its exact value. This occurs naturally through measurement rounding, sensor limitations, and lag in economic systems. We study Gaussian mean estimation from coarse data, where each true sample $x$ is drawn from a $d$-dimensional Gaussian distribution with identity covariance, but is revealed only through the set of a partition containing $x$. When the coarse samples, roughly speaking, have ``low'' information, the mean cannot be uniquely recovered from observed samples (i.e., the problem is not identifiable). Recent work by Fotakis, Kalavasis, Kontonis, and Tzamos [FKKT21] established that sample-efficient mean estimation is possible when the unknown mean is identifiable and the partition consists of only convex sets. Moreover, they showed that without convexity, mean estimation becomes NP-hard. However, two fundamental questions remained open: (1) When is the mean identifiable under convex partitions? (2) Is computationally efficient estimation possible under identifiability and convex partitions? This work resolves both questions. [...]

</details>


### [127] [FlashOptim: Optimizers for Memory Efficient Training](https://arxiv.org/abs/2602.23349)
*Jose Javier Gonzalez Ortiz,Abhay Gupta,Chris Renard,Davis Blalock*

Main category: cs.LG

TL;DR: FlashOptim通过量化优化技术将AdamW训练的内存占用从每参数16字节降至7字节（释放梯度后5字节），同时保持模型质量不变


<details>
  <summary>Details</summary>
Motivation: 标准混合精度训练需要大量加速器内存（每参数约16字节），使得训练大模型（如70亿参数）对内存小于100GB的研究者不切实际

Method: 1. 改进主权重分割：发现并利用量化误差的紧致边界；2. 设计压缩扩展函数：大幅减少8位优化器状态量化的误差；结合16位梯度技术

Result: 将AdamW内存从每参数16字节降至7字节（梯度释放后5字节），模型检查点大小减少一半以上，在SGD、AdamW、Lion优化器上实验无质量下降

Conclusion: FlashOptim实现了超过50%的每参数内存减少，同时保持模型质量和API兼容性，使大模型训练对资源有限的研究者更加可行

Abstract: Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory.
  We introduce FlashOptim, a suite of optimizations that reduces per-parameter memory by over 50% while preserving model quality and API compatibility. Our approach introduces two key techniques. First, we improve master weight splitting by finding and exploiting a tight bound on its quantization error. Second, we design companding functions that greatly reduce the error in 8-bit optimizer state quantization. Together with 16-bit gradients, these techniques reduce AdamW memory from 16 bytes to 7 bytes per parameter, or 5 bytes with gradient release. They also cut model checkpoint sizes by more than half.
  Experiments with FlashOptim applied to SGD, AdamW, and Lion show no measurable quality degradation on any task from a collection of standard vision and language benchmarks, including Llama-3.1-8B finetuning.

</details>


### [128] [SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport](https://arxiv.org/abs/2602.23353)
*Simon Roschmann,Paul Krzakala,Sonia Mazelet,Quentin Bouniot,Zeynep Akata*

Main category: cs.LG

TL;DR: SOTAlign：一种两阶段半监督对齐框架，使用少量配对数据和大量未配对数据对齐预训练的单模态编码器，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖对比损失和数百万配对样本对齐预训练视觉语言模型，但本文探索是否能用更少的监督实现有意义的对齐。

Method: 提出SOTAlign两阶段框架：1) 使用线性教师从有限配对数据中恢复粗略共享几何结构；2) 基于最优传输的散度在未配对样本上细化对齐，转移关系结构而不过度约束目标空间。

Result: SOTAlign能有效利用未配对图像和文本，学习跨数据集和编码器对的鲁棒联合嵌入，显著优于监督和半监督基线方法。

Conclusion: 通过SOTAlign框架，证明了在有限监督下实现有意义的跨模态对齐是可行的，为半监督跨模态学习提供了有效解决方案。

Abstract: The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision. We introduce a semi-supervised setting in which pretrained unimodal encoders are aligned using a small number of image-text pairs together with large amounts of unpaired data. To address this challenge, we propose SOTAlign, a two-stage framework that first recovers a coarse shared geometry from limited paired data using a linear teacher, then refines the alignment on unpaired samples via an optimal-transport-based divergence that transfers relational structure without overconstraining the target space. Unlike existing semi-supervised methods, SOTAlign effectively leverages unpaired images and text, learning robust joint embeddings across datasets and encoder pairs, and significantly outperforming supervised and semi-supervised baselines.

</details>


### [129] [A Dataset is Worth 1 MB](https://arxiv.org/abs/2602.23358)
*Elad Kimchi Shoshani,Leeyam Gabay,Yedid Hoshen*

Main category: cs.LG

TL;DR: PLADA方法通过仅传输类别标签而非像素数据，利用客户端已有的通用未标注参考数据集，实现高效数据集分发，传输负载小于1MB。


<details>
  <summary>Details</summary>
Motivation: 当前数据集分发面临巨大通信成本问题，特别是需要向不同硬件和软件框架的客户端传输原始数据时。现有数据集蒸馏方法难以扩展到高分辨率数据，且无法实现足够小的文件大小。

Method: PLADA方法假设客户端已预加载大型通用未标注参考数据集（如ImageNet），通过仅传输特定图像的类别标签来传达新任务。引入剪枝机制过滤参考数据集，仅保留与目标任务最语义相关的图像标签，最大化训练效率同时最小化传输负载。

Result: 在10个不同数据集上的实验表明，该方法能以小于1MB的传输负载传递任务知识，同时保持高分类准确率。

Conclusion: PLADA通过消除像素传输，仅传输类别标签，为高效数据集服务提供了有前景的解决方案，显著降低了通信成本。

Abstract: A dataset server must often distribute the same large payload to many clients, incurring massive communication costs. Since clients frequently operate on diverse hardware and software frameworks, transmitting a pre-trained model is often infeasible; instead, agents require raw data to train their own task-specific models locally. While dataset distillation attempts to compress training signals, current methods struggle to scale to high-resolution data and rarely achieve sufficiently small files. In this paper, we propose Pseudo-Labels as Data (PLADA), a method that completely eliminates pixel transmission. We assume agents are preloaded with a large, generic, unlabeled reference dataset (e.g., ImageNet-1K, ImageNet-21K) and communicate a new task by transmitting only the class labels for specific images. To address the distribution mismatch between the reference and target datasets, we introduce a pruning mechanism that filters the reference dataset to retain only the labels of the most semantically relevant images for the target task. This selection process simultaneously maximizes training efficiency and minimizes transmission payload. Experiments on 10 diverse datasets demonstrate that our approach can transfer task knowledge with a payload of less than 1 MB while retaining high classification accuracy, offering a promising solution for efficient dataset serving.

</details>


### [130] [Model Agreement via Anchoring](https://arxiv.org/abs/2602.23360)
*Eric Eaton,Surbhi Goel,Marcel Hussing,Michael Kearns,Aaron Roth,Sikata Bela Sengupta,Jessica Sorrell*

Main category: cs.LG

TL;DR: 提出一种分析模型分歧的通用技术，并将其应用于四种常见机器学习算法，证明通过增加模型数量、迭代次数、架构规模或树深度等参数，可以使独立训练的模型间分歧趋近于零。


<details>
  <summary>Details</summary>
Motivation: 研究如何控制机器学习模型间的分歧（预测差异），目标是开发一种通用分析方法，能够应用于现有训练方法，并通过调整训练过程的自然参数使分歧趋近于零。

Method: 提出基于"锚定"技术的通用分析方法，将两个模型的预测锚定到它们的平均值进行分析。将此技术应用于四种算法：堆叠聚合、梯度提升、神经网络架构搜索和回归树训练。

Result: 证明了四种算法的分歧界限：堆叠聚合随模型数量k增加而趋零；梯度提升随迭代次数k增加而趋零；神经网络架构搜索随架构规模n增加而趋零；回归树训练随树深度d增加而趋零。

Conclusion: 开发了一种简单通用的技术来分析独立模型分歧，成功应用于多种常见机器学习算法，证明了通过调整特定参数可以使模型分歧趋近于零，且结果可推广到多维回归和强凸损失函数。

Abstract: Numerous lines of aim to control $\textit{model disagreement}$ -- the extent to which two machine learning models disagree in their predictions. We adopt a simple and standard notion of model disagreement in real-valued prediction problems, namely the expected squared difference in predictions between two models trained on independent samples, without any coordination of the training processes. We would like to be able to drive disagreement to zero with some natural parameter(s) of the training procedure using analyses that can be applied to existing training methodologies.
  We develop a simple general technique for proving bounds on independent model disagreement based on $\textit{anchoring}$ to the average of two models within the analysis. We then apply this technique to prove disagreement bounds for four commonly used machine learning algorithms: (1) stacked aggregation over an arbitrary model class (where disagreement is driven to 0 with the number of models $k$ being stacked) (2) gradient boosting (where disagreement is driven to 0 with the number of iterations $k$) (3) neural network training with architecture search (where disagreement is driven to 0 with the size $n$ of the architecture being optimized over) and (4) regression tree training over all regression trees of fixed depth (where disagreement is driven to 0 with the depth $d$ of the tree architecture). For clarity, we work out our initial bounds in the setting of one-dimensional regression with squared error loss -- but then show that all of our results generalize to multi-dimensional regression with any strongly convex loss.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [131] [Stochastic Neural Networks for Quantum Devices](https://arxiv.org/abs/2602.22241)
*Bodo Rosenhahn,Tobias J. Osborne,Christoph Hirche*

Main category: quant-ph

TL;DR: 将随机神经网络表达为量子电路并进行优化的方法，包括多种网络拓扑，并展示了作为Grover算法oracle的量子生成AI应用


<details>
  <summary>Details</summary>
Motivation: 受经典感知器启发，将随机神经网络引入量子计算领域，利用量子电路的优势来优化神经网络性能

Method: 使用Kiefer-Wolfowitz算法结合模拟退火训练网络权重，提出了多种量子神经网络拓扑：浅层全连接网络、Hopfield网络、受限玻尔兹曼机、自编码器和卷积神经网络

Result: 成功将优化的神经网络作为Grover算法的oracle，实现了量子生成AI模型

Conclusion: 该工作展示了在门基量子计算中将随机神经网络表达为量子电路的可行性，为量子机器学习提供了新的方法框架

Abstract: This work presents a formulation to express and optimize stochastic neural networks as quantum circuits in gate-based quantum computing. Motivated by a classical perceptron, stochastic neurons are introduced and combined into a quantum neural network. The Kiefer-Wolfowitz algorithm in combination with simulated annealing is used for training the network weights. Several topologies and models are presented, including shallow fully connected networks, Hopfield Networks, Restricted Boltzmann Machines, Autoencoders and convolutional neural networks. We also demonstrate the combination of our optimized neural networks as an oracle for the Grover algorithm to realize a quantum generative AI model.

</details>


### [132] [Robustness-Runtime Tradeoff for Quantum State Transfer](https://arxiv.org/abs/2602.22312)
*Twesh Upadhyaya,Yifan Hong,T. C. Mooney,Alexey V. Gorshkov*

Main category: quant-ph

TL;DR: 该论文研究了量子态传输协议的鲁棒性，量化了协议对辅助比特初始状态误差的容忍度，并建立了鲁棒性与算子对易子Schatten p-范数之间的紧密联系。


<details>
  <summary>Details</summary>
Motivation: 现有基于幂律相互作用的量子态传输协议需要辅助比特处于完美初始状态，但在实际中由于噪声或不完美控制，这一条件难以满足。因此需要量化协议对辅助比特初始状态误差的容忍度。

Method: 在Heisenberg绘景中分析态传输如何使最终位点支持的算子不再与起始位点算子对易。证明鲁棒性紧密界定了这些初始与最终位点算子之间对易子的Schatten p-范数。

Result: 建立了鲁棒性与Schatten p-范数之间的紧密联系，推广了已知的p=∞和p=2情况。结合现有幂律光锥，得到了部分态依赖协议的新最小运行时间，在某些参数区间优于现有界限。

Conclusion: 该工作提出了鲁棒性作为量子态传输协议的关键度量，揭示了中间p值对应部分态依赖传输，并设计了新的鲁棒态传输协议，填补了完全态依赖与态无关传输之间的空白。

Abstract: Quantum state transfer is the primitive of transporting an unknown state on one site of a lattice to another. Using power-law interactions, recent state transfer protocols achieve speedup by utilizing the intermediate ancilla sites. However, these protocols require the ancillas to be in a perfectly initialized state, which, due to noise or imperfect control, may not be the case. In this work we introduce the $\textit{robustness}$ of a state transfer protocol, which quantifies the protocol's tolerance to error in the initial ancilla state. In the Heisenberg picture, state transfer grows operators supported on the final site such that they no longer commute with all operators on the starting site. We prove that this robustness tightly bounds the Schatten $p$-norms of these commutators between initial and final-site operators. This generalizes the known cases of $p=\infty$ and $p=2$, which govern completely state-dependent and state-independent state transfer respectively, demonstrating that intermediate values of $p$ govern partially state-dependent state transfer. In conjunction with existing power-law light cones, our result gives new minimum runtimes for partially state-dependent protocols which, in certain regimes, are parametrically better than existing bounds. We introduce new robust state transfer protocols, charting the landscape between complete state-dependence and state-independence.

</details>


### [133] [Fundamental Limits on QBER and Distance in Quantum Key Distribution](https://arxiv.org/abs/2602.22319)
*Stefano Pirandola*

Main category: quant-ph

TL;DR: 该论文建立了量子密钥分发(QKD)的最大量子比特错误率(QBER)安全阈值和通信距离上限，揭示了QKD在噪声容忍度和传输距离方面的根本限制。


<details>
  <summary>Details</summary>
Motivation: 量子密钥分发虽然能实现信息论安全通信，但其对噪声的最终容忍度和可实现的传输距离存在根本性限制。需要明确QKD的终极噪声鲁棒性，并界定安全量子通信的基本边界。

Method: 基于量子比特泡利信道的基本容量阈值，推导出与安全QKD兼容的最大量子比特错误率(QBER)，并得出相应的通信距离上限。将信息论极限与现实物理噪声模型相结合，分析光纤和自由空间链路的距离限制。

Result: 建立了QKD的最大QBER安全阈值和通信距离上限，适用于基于两个或更多相互无偏基的协议，包括单光子和弱相干光源。获得了光纤和自由空间链路（包括深空量子通信相关的衍射限制）的通用距离界限。

Conclusion: 该研究阐明了QKD的终极噪声鲁棒性，界定了安全量子通信的基本边界，为量子通信系统的设计和性能评估提供了理论基础。

Abstract: Quantum key distribution (QKD) enables information-theoretic secure communication, yet its ultimate tolerance to noise and achievable transmission distance remain fundamentally constrained. We establish the maximum quantum bit error rate (QBER) compatible with secure QKD and derive corresponding upper bounds on communication distance. Our results follow from a fundamental capacity threshold for qubit Pauli channels and apply to protocols based on two or more mutually unbiased bases, using either single-photon or weak coherent sources. By connecting information-theoretic limits to realistic physical noise models, we obtain universal bounds on achievable distances in fiber and free-space links, including diffraction-limited constraints relevant to deep-space quantum communications. These findings clarify the ultimate noise robustness of QKD and delineate the fundamental boundaries of secure quantum communication.

</details>


### [134] [The unbearable hardness of deciding about magic](https://arxiv.org/abs/2602.22330)
*Lorenzo Leone,Jens Eisert,Salvatore F. E. Oliviero*

Main category: quant-ph

TL;DR: 该论文证明确定量子态是否属于稳定子多面体（魔术资源理论中的自由态）需要超指数时间exp(n²)，即使近似计算也是如此，这揭示了量化与认证魔术在计算上的根本困难。


<details>
  <summary>Details</summary>
Motivation: 在量子计算中，魔术（magic）是实现通用量子计算的关键资源，但相比纠缠，魔术的表征相对不足。理解魔术的计算复杂性对于评估经典可模拟性、蒸馏病态魔术态以及探索魔术作为量子资源至关重要。

Method: 将确定稳定子多面体成员资格的问题约化为在n²个变量上的3-SAT实例，然后利用指数时间假说（ETH）推导出超指数时间下界。该方法还扩展到经典可模拟区域，分析由对数个非Clifford门生成的态。

Result: 证明确定量子态是否属于稳定子多面体需要超指数时间exp(n²)，即使近似计算也是如此。这一结果意味着：1）任何通用态的魔术单调子都必然超指数难计算；2）判断算子是否为有效魔术见证同样困难；3）魔术的鲁棒性在单调子中计算最优。

Conclusion: 量化与认证魔术在根本上都是计算难解的，这为评估经典可模拟性、蒸馏病态魔术态以及探索魔术作为量子资源设定了内在的计算极限。即使在对数门数的经典可模拟区域，相关判定问题也保持超指数难度。

Abstract: Identifying the boundary between classical and quantum computation is a central challenge in quantum information. In multi-qubit systems, entanglement and magic are the key resources underlying genuinely quantum behaviour. While entanglement is well understood, magic -- essential for universal quantum computation -- remains relatively poorly characterised. Here we show that determining membership in the stabilizer polytope, which defines the free states of magic-state resource theory, requires super-exponential time $\exp( n^2)$ in the number of qubits $n$, even approximately. We reduce the problem to solving a $3$-SAT instance on $n^2$ variables and, by invoking the exponential time hypothesis, the result follows. As a consequence, both quantifying and certifying magic are fundamentally intractable: any magic monotone for general states must be super-exponentially hard to compute, and deciding whether an operator is a valid magic witness is equally difficult. As a corollary, we establish the robustness of magic as computationally optimal among monotones. This barrier extends even to classically simulable regimes: deciding whether a state lies in the convex hull of states generated by a logarithmic number of non-Clifford gates is also super-exponentially hard. Together, these results reveal intrinsic computational limits on assessing classical simulability, distilling pathological magic states, and ultimately probing and exploiting magic as a quantum resource.

</details>


### [135] [Schwinger-Keldysh field theory for operator Rényi entropy and entanglement growth in non-interacting systems with sub-ballistic transports](https://arxiv.org/abs/2602.22331)
*Priesh Roy,Sumilan Banerjee*

Main category: quant-ph

TL;DR: 该论文提出了一种基于子系统算子Rényi熵的算子增长度量方法，建立了算子增长与输运、纠缠生成之间的联系，并开发了统一的Schwinger-Keldysh场论形式来研究算子Rényi熵和纠缠熵的时间演化。


<details>
  <summary>Details</summary>
Motivation: 传统算子增长度量（如无序时序关联函数）和纠缠熵存在局限性，需要一种状态无关的算子增长度量来更好地连接量子动力学中的输运现象和纠缠生成。

Method: 定义子系统算子Rényi熵作为算子增长度量，构建统一的Schwinger-Keldysh场论形式，将算子Rényi熵和纠缠熵的时间演化用无限温度和真空Keldysh格林函数表示，并应用于准周期和随机无序的非相互作用系统。

Result: 子系统算子Rényi熵能够同时编码空间和时间信息，直接关联守恒量的局域算子输运。该方法成功捕捉了非相互作用系统中弹道输运、亚弹道输运（如扩散和反常扩散）以及局域化等不同输运行为。

Conclusion: 子系统算子Rényi熵提供了一种有效的状态无关算子增长度量，通过统一的场论框架建立了算子增长、纠缠增长和输运之间的直接联系，为研究量子多体系统中的动力学行为提供了新工具。

Abstract: The notion of operator growth in quantum systems furnishes a bridge between transport and the generation of entanglement between different parts of the system under quantum dynamics. We define a measure of operator growth in terms of subsystem operator Rényi entropy, which provides a state-independent measure of operator growth, unlike entanglement entropies, and the usual measures of operator growth like out-of-time-order correlators. We show that the subsystem operator Rényi entropy encodes both spatial and temporal information, and thus can directly connect to transport for a local operator related to a conserved quantity. We construct a unified Schwinger-Keldysh (SK) field theory formalism for the time evolution of operator Rényi entropy and entanglement entropies of initial pure states. We use the SK field theory to obtain the operator Rényi and state entanglement entropies in terms of infinite-temperature and vacuum Keldysh Green's functions, respectively, for non-interacting systems. We apply the method to explore the connection between operator and entanglement growth, and transport in non-interacting systems with quasiperiodic and random disorder, like the one- and two-dimensional Aubry-André models and the two-dimensional Anderson model. In particular, we show that the growth of subsystem operator Rényi entropy and state von Neumann and Rényi entanglement entropies can capture both ballistic and sub-ballistic transport behaviors, like diffusive and anomalous diffusive transport, as well as localization in these systems.

</details>


### [136] [Basis-independent stabilizerness and maximally noisy magic states](https://arxiv.org/abs/2602.22336)
*Michael Zurel,Jack Davis*

Main category: quant-ph

TL;DR: 该论文研究了绝对稳定子态和绝对Wigner正态，给出了多量子比特和奇素数维量子比特的完整刻画，建立了谱多面体描述，并计算了相关集合的球半径，揭示了束缚魔法的酉不变版本。


<details>
  <summary>Details</summary>
Motivation: 研究在任意酉变换下仍保持为稳定子态凸组合的量子态（绝对稳定子态），以及保持Wigner函数非负的态（绝对Wigner正态）。这些概念提供了研究量子资源理论的新视角，特别是魔法的酉不变版本。

Method: 引入谱多面体来描述允许的谱分布，通过几何方法分析绝对稳定子态和绝对Wigner正态的集合结构。对于奇素数维系统，给出了完整的刻画，并计算了相关集合的内接球和外接球半径。

Result: 1. 单量子比特的绝对稳定子态集合是稳定子八面体内接球；高维情况更复杂。2. 发现了绝对Wigner正态但不绝对稳定子的态，这是束缚魔法的酉不变版本。3. 计算了绝对稳定子态和绝对Wigner正态集合的最大内接球半径（给出非稳定子态和Wigner负态的最小纯度）。4. 找到了绝对Wigner正态集合的最小外接球半径（给出纯度必要条件）。

Conclusion: 论文建立了绝对稳定子态和绝对Wigner正态的完整理论框架，通过几何方法揭示了这些集合的结构特性，为量子资源理论提供了新的工具和视角，特别是发现了束缚魔法的酉不变版本。

Abstract: Absolutely stabilizer states are those that remain convex mixtures of stabilizer states after conjugation by any unitary. Here we give a characterization of such states for multiple qudits of all prime dimensions by introducing a polytope of their allowed spectra. We illustrate this through the examples of one qubit, two qubits, and one qutrit. In particular, the set of absolutely stabilizer states for a single qubit is a ball inscribed in the stabilizer octahedron, but for higher dimensions the geometry is more complicated. For odd-prime-dimensional qudits, we also give a complete characterization of absolutely Wigner-positive states, i.e., states whose Wigner function remains nonnegative after conjugation by any unitary. In so doing, we show there are absolutely Wigner-positive states that are not absolutely stabilizer, which can be seen as a unitarily-invariant version of bound magic. We then study the radii of the largest balls contained in the sets of absolutely stabilizer states and absolutely Wigner-positive states. These radii respectively tell us the lowest possible purity of nonstabilizer and Wigner-negative states. Conversely, we also find the radius of the smallest ball containing the set of absolutely Wigner-positive states, giving a tight purity-based necessary condition thereof.

</details>


### [137] [Numerical Experiments with Parameter Setting of Trotterized Quantum Phase Estimation for Quantum Hamiltonian Ground State Computation](https://arxiv.org/abs/2602.22349)
*Elijah Pelofske,Stephan Eidenbenz*

Main category: quant-ph

TL;DR: 本文通过数值模拟研究量子相位估计算法(QPE)在计算量子磁体基态能量时的性能，分析电路参数对算法效果的影响，并提出优化指南。


<details>
  <summary>Details</summary>
Motivation: 研究QPE算法在实际量子电路中的表现，特别是针对量子磁体基态能量计算任务，分析不同输入参数对算法性能的影响，为量子算法设计提供实用指导。

Method: 使用经典模拟研究QPE电路在3量子比特系统上的表现，考虑高达10位相位精度和10阶Trotter分解，系统分析时间演化、Trotter阶数、Trotter步数和初始态等输入参数的影响。

Result: 发现QPE对最优数字化相位的采样收敛到固定速率，当Trotter误差较高时会出现显著的收益递减现象，提出了量子算法输入和调优的连贯指南。

Conclusion: QPE算法的实际性能高度依赖于输入参数的合理选择，特别是Trotter误差控制对采样效率有重要影响，为量子算法设计提供了实用的参数调优指导。

Abstract: We numerically investigate quantum circuit elementary-gate level instantiations of the standard Quantum Phase Estimation (QPE) algorithm for the task of computing the ground-state energy of a quantum magnet; the disordered fully-connected quantum Heisenberg spin glass model. We consider (classical simulations of) QPE circuit computations on relatively small quantum Hamiltonians ($3$ qubits) with up to $10$ phase bits of precision, using up to Trotter order $10$. We systematically study the inputs of QPE, specifically time evolution, Trotter order, Trotter steps, and initial state, and illustrate how these inputs practically determine how QPE operates. From this we outline a coherent set of quantum algorithm input and tuning guidelines. One of the notable properties we characterize is that QPE sampling of the optimal digitized phase converges to a fixed rate. This results in strong diminishing returns of optimal phase sampling rates which can occur when the Trotter error is surprisingly high.

</details>


### [138] [A field-biased HPZ master equation and its Markovian limit](https://arxiv.org/abs/2602.22363)
*M. Gabriela Boada G.,Andrea Delgado,Jose Morales E*

Main category: quant-ph

TL;DR: 该论文从第一性原理推导了连续驱动开放量子系统的非平衡量子主方程，考虑了结构化电磁环境，不依赖标准平衡涨落耗散定理，揭示了驱动场对噪声关联和动力学的显式影响。


<details>
  <summary>Details</summary>
Motivation: 研究在连续驱动下开放量子系统的非平衡动力学，特别是当外部经典场同时耦合系统和环境自由度时，需要超越标准平衡涨落耗散定理的理论框架，以理解腔和电路量子电动力学实验中远离平衡的物理现象。

Method: 从驱动的Caldeira-Leggett模型出发，在算符层面精确消除环境自由度，得到驱动的量子广义朗之万方程，利用驱动浴的高斯性质推导修改的Hu-Paz-Zhang主方程，其中扩散系数和相干力继承了外部场的显式记忆。

Result: 推导出非平衡量子主方程，其中浴统计显式依赖于施加场的双时间自关联函数，导致驱动偏置的噪声关联和本质非马尔可夫动力学，物理可观测振荡频率编码在朗之万方程的齐次格林函数中，驱动诱导修正仅通过修改的扩散和漂移项显现。

Conclusion: 该工作为理解场偏置涨落关系提供了统一的微观框架，直接适用于远离平衡的腔和电路量子电动力学实验，揭示了驱动场如何通过修改噪声关联和耗散核影响开放量子系统的非平衡动力学。

Abstract: We present a first-principles derivation of a non-equilibrium quantum master equation for a continuously driven open quantum system interacting with a structured electromagnetic environment. Starting from a driven Caldeira-Leggett model in which an external classical field couples simultaneously to the system and reservoir degrees of freedom, we proceed without assuming that the standard equilibrium fluctuation-dissipation theorem holds. The bath statistics acquire explicit dependence on the two-time autocorrelation function of the applied field, leading to drive-biased noise correlations and intrinsically non-Markovian dynamics. By eliminating the reservoir exactly at the operator level, we obtain a driven quantum generalized Langevin equation whose noise and dissipation kernels depend on two independent times. Exploiting the Gaussian nature of the driven bath, we derive a modified Hu-Paz-Zhang master equation in which the diffusion coefficients and coherent forces inherit explicit memory of the external field. We demonstrate that the physically observable oscillation frequency remains encoded in the homogeneous Green's function of the Langevin equation, while the drive-induced corrections manifest exclusively through modified diffusion and drift terms. Our results provide a unified microscopic framework for understanding field-biased fluctuation relations with direct relevance to cavity and circuit quantum electrodynamics experiments operating far from equilibrium.

</details>


### [139] [Analogue many-body gravitating quantum systems with a network of dipolar Bose-Einstein condensates](https://arxiv.org/abs/2602.23319)
*Youssef Trifa,Dario Cafasso,Marco Fattori,Luca Pezzè*

Main category: quant-ph

TL;DR: 该论文将量子引力探测范式从单量子比特扩展到多能级量子比特（qudits），利用原子系综的集体增强效应提高信噪比，并提出用长程耦合的玻色-爱因斯坦凝聚体模拟引力量子效应。


<details>
  <summary>Details</summary>
Motivation: 当前通过时钟或干涉仪探测牛顿引力与量子力学界面的实验面临信号弱、信噪比低的问题，需要增强引力诱导的量子效应观测能力。

Method: 将传统单量子比特方案推广到(N+1)能级量子比特，利用原子系综的粒子数N实现集体增强；提出用具有长程耦合（如偶极相互作用）的双模玻色-爱因斯坦凝聚体作为可编程模拟平台；扩展协议到传感器网络。

Result: 多体增强提高了信噪比和有效相互作用速率，便于观测引力诱导的纠缠和退相干；建立了基于局域和集体测量的计量学见证方法；提供了在可及时空尺度上探索引力量子动力学的模拟平台。

Conclusion: 通过量子比特到量子比特的推广和集体增强效应，显著提升了引力量子效应的探测能力，并为实验研究提供了新的模拟和检测方案。

Abstract: Operational probes of the interface between quantum mechanics and general relativity in the Newtonian regime -- via mass-energy equivalence in clocks or spatial superpositions in interferometers -- share a common description in terms of an effective qubit-qubit Ising coupling. Here we generalize both paradigms to interacting $(N+1)$-level effective qudits made of atomic ensembles with particle number, $N$. The many-body enhancement boosts the signal-to-noise and increases the effective interaction rate, facilitating the observation of gravitationally induced entanglement and decoherence, certified by metrological witnesses based on local and collective measurements. Furthermore, we show that quantum effects induced by gravitational interaction can be simulated by trapped bimodal Bose-Einstein condensates with long-range (e.g. dipolar) coupling, providing a programmable analogue platform to explore gravitating quantum dynamics at accessible time and energy scales. Finally, extending the protocol to a sensor network broadens the entanglement-detection window.

</details>


### [140] [Revisiting the Role of State Texture in Gate Identification and Fixed-Point Resource Theories](https://arxiv.org/abs/2602.22496)
*Alexander C. B. Greenwood,Joseph M. Lukens,Li Qian,Brian T. Kirby*

Main category: quant-ph

TL;DR: 本文重新审视了基于随机输入态识别CNOT门与单量子比特门的协议，建立了更一般的保真度框架，并扩展了量子资源理论家族，包括固定点资源理论。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明，识别CNOT门与单量子比特门的协议与量子资源"态纹理"密切相关。本文旨在重新审视该协议，建立更一般的保真度框架，并扩展量子资源理论家族，探索更广泛的资源理论结构。

Method: 采用保真度基础公式化方法，适用于几乎所有实验室基；为每个参考纯态定义不同的资源理论；通过凸顶构造从单个"无资源"态扩展到凸集；引入"固定点资源理论"家族，包括态纹理、真相干性、纯度和非热性的固定点实例。

Result: 保真度基础公式在几乎所有实验室基中成功；恢复了已知资源理论（如虚性和相干性）的单量子比特度量；在固定点资源理论中，保真度下界在自由操作下具有弱单调性，而凸顶对数度量存在特定违反强单调性的情况。

Conclusion: 本文成功建立了更一般的保真度框架用于门识别协议，扩展了量子资源理论家族，并揭示了固定点资源理论中单调性行为的差异，为量子资源理论提供了新的理论框架和见解。

Abstract: A protocol for identifying controlled-NOT (CNOT) gates versus single-qubit-only gates in universal quantum circuits using randomized input states was recently shown to be intimately connected to the quantum resource of state texture. Here we revisit this gate identification protocol and demonstrate that a more general fidelity-based formulation succeeds for nearly all laboratory bases. We then examine a broader family of quantum resource theories, where a distinct resource theory can be defined for each choice of reference pure state, establishing core resource-theoretic requirements without the computational shortcut offered by the "grand sum" employed in the original formulation of state texture. By extending from single "resourceless" states to convex sets via a convex-roof construction, we recover single-qubit measures of known resource theories such as imaginarity and coherence. Finally, we introduce a family of "fixed-point resource theories" that includes fixed-point instances of the theories of state texture, genuine coherence, purity, and athermality. For these fixed-point resource theories we show that, under free operations, the fidelity-based lower bound is weakly monotonic, while specific violations of strong monotonicity are found for the convex-roof logarithmic measure.

</details>


### [141] [Quantified convergence of general homodyne measurements with applications to continuous variable quantum computing](https://arxiv.org/abs/2602.22511)
*Emanuel Knill,Ezad Shojaee,James R. van Meter,Akira Kyle,Scott Glancy*

Main category: quant-ph

TL;DR: 该论文量化了标准脉冲零差和宽带脉冲零差测量与目标正交分量测量的收敛性，推导了保真度下界，并展示了在量子特征函数估计、量子隐形传态和GKP码纠错等应用中的实用性。


<details>
  <summary>Details</summary>
Motivation: 宽带脉冲零差测量作为标准脉冲零差测量的推广，能够利用宽光谱范围高效率探测器（如量热计）。需要量化这两种测量方法与目标正交分量测量的收敛性，为实际量子信息处理应用提供理论保证。

Method: 推导了测量后经典-量子态保真度的下界，以及基于测量结果的条件操作后态保真度的下界。这些下界依赖于本地振荡器振幅和数算符的矩。通过具体应用场景验证理论结果。

Result: 获得了标准脉冲零差和宽带脉冲零差测量收敛性的定量界限，这些界限在实际应用中具有指导意义。具体展示了在Wigner分布特征函数估计、矩期望值计算、量子隐形传态和GKP码连续变量纠错等场景中的适用性。

Conclusion: 该研究为宽带脉冲零差测量提供了严格的理论分析框架，证明了其在量子信息处理中的实用价值，特别是在需要宽光谱范围高效率探测的应用场景中，为相关实验设计和性能评估提供了理论依据。

Abstract: In arXiv:2503.00188 we introduced broadband pulsed (BBP) homodyne measurements as a generalization of standard pulsed homodyne quadrature measurements. BBP can take advantage of detectors such as calorimeters that have the potential for high efficiency over a broad spectral range. BBP homodyne retains the advantages of standard pulsed homodyne, enabling measurement of arbitrary quadratures in the limit of large amplitude local oscillators (LO). Here we quantify the convergence of standard and BBP homodyne quadrature measurements to those of the quadrature of interest. We obtain lower bounds on the fidelity of the post-measurement classical-quantum state of outcomes and unmeasured modes, and the fidelity of the states obtained after applying operations conditional on measurement outcomes. The bounds depend on the LO amplitude and the moments of number operators. We demonstrate the practical relevance of these bounds by evaluating them for standard pulsed homodyne used for estimating values of the characteristic function of the Wigner distribution, expectations of moments, for quantum teleportation and for continuous variable error correction with GKP codes.

</details>


### [142] [Gravitational decoherence and recoherence of a composite particle: the interplay between gravitons and a classical Newtonian potential](https://arxiv.org/abs/2602.22517)
*Thiago H. Moreira,Lucas Chibebe Céleri*

Main category: quant-ph

TL;DR: 论文研究了引力环境（特别是引力子和牛顿势）对量子空间叠加态退相干的影响，发现即使对于微观质量系统，通过内部自由度与引力子的相互作用，长期退相干是不可避免的。


<details>
  <summary>Details</summary>
Motivation: 引力环境的不可屏蔽性使其在退相干机制和量子-经典转变中具有重要理论意义。过去研究认为引力子诱导的退相干主要发生在宏观系统，但最近发现系统的内部动力学结构可以增强这一机制。

Method: 扩展了先前分析，将经典牛顿势的相互作用纳入考虑。研究了引力子浴与系统内部自由度之间的相互作用，分析了不同时间尺度下的退相干行为。

Result: 1. 在量子空间叠加态尺寸确定的时间尺度内，引力子浴主导退相干机制；2. 长期来看，引力子与系统内部自由度的相互作用使退相干不可避免，即使对于微观质量系统；3. 经典牛顿势的相互作用会略微减缓这一机制，对于没有动力学内部自由度的系统，理论上甚至可能导致再相干。

Conclusion: 引力环境通过引力子与系统内部自由度的相互作用，即使在微观尺度也能导致量子退相干，这为理解量子-经典转变提供了新的视角，并揭示了经典牛顿势在退相干过程中的调节作用。

Abstract: The fact that gravitational environments cannot be shielded (since gravity is universal) makes them of great theoretical interest to decoherence mechanisms and to the quantum-to-classical transition. While past results seemed to indicate that graviton-induced decoherence of spatial superpositions happens only for macroscopic systems, recently it was shown that this mechanism can be enhanced through the system's own dynamical internal structure. In this work, we extend this analysis by including the interaction with a classical Newtonian potential. We show that, although the graviton bath alone dominates the mechanism for short times compared to a timescale established by the size of the quantum spatial superposition, the interplay between the gravitons and the internal degrees of freedom of the system renders decoherence inevitable in the long-time limit, even for microscopic masses. We also show that this mechanism is slightly slowed down by the interplay with the classical Newtonian potential, which, for systems without dynamical internal degrees of freedom, can even lead to recoherence, at least in principle.

</details>


### [143] [The Road to Useful Quantum Computers](https://arxiv.org/abs/2602.22540)
*Timothy Proctor,Robin Blume-Kohout,Andrew Baczewski*

Main category: quant-ph

TL;DR: 该论文回顾了量子计算从理论概念到当前发展历程，分析了当代量子计算机的能力与实现"量子效用"目标之间的差距，并提出了追踪进展的方法。


<details>
  <summary>Details</summary>
Motivation: 量子计算经历了从理论概念到实际研发的快速发展，但目前尚未实现解决科学或实际重要问题的"量子效用"目标。论文旨在评估当前量子计算机的能力，明确实现量子效用的路径和挑战。

Method: 通过描述当代量子计算机的能力，将其与量子效用的要求进行比较，并建立从当前状态到实现量子效用的进展追踪框架。同时结合作者自身研究经验，分析关键的科学和工程挑战。

Result: 论文系统梳理了量子计算的发展历程，从费曼和曼宁的理论观察到肖尔算法的突破，再到当前工业级原型的发展。明确了当代量子计算机尚未达到量子效用的现状，并建立了评估进展的方法论。

Conclusion: 量子计算虽然发展迅速且前景广阔，但要实现解决实际重要问题的量子效用目标仍面临重大科学和工程挑战。需要建立清晰的进展追踪框架来指导未来的研发方向。

Abstract: Building a useful quantum computer is a grand science and engineering challenge, currently pursued intensely by teams around the world. In the 1980s, Richard Feynman and Yuri Manin observed independently that computers based on quantum mechanics might enable better simulations of quantum phenomena. Their vision remained an intellectual curiosity until Peter Shor published his famous quantum algorithm for integer factoring, and shortly thereafter a proof that errors in quantum computations can be corrected. Since then, quantum computing R&D has progressed rapidly, from small-scale experiments in university physics laboratories to well-funded industrial efforts and prototypes. Hype notwithstanding, quantum computers have yet to solve scientifically or practically important problems -- a target often called quantum utility. In this article, we describe the capabilities of contemporary quantum computers, compare them to the requirements of quantum utility, and illustrate how to track progress from today to utility. We highlight key science and engineering challenges on the road to quantum utility, touching on relevant aspects of our own research.

</details>


### [144] [Optimizing Doppler laser cooling protocols for quantum sensing with 3D ion crystals in a Penning trap](https://arxiv.org/abs/2602.22541)
*John Zaris,Wes Johnson,Athreya Shankar,John J. Bollinger,Allison L. Carter,Daniel H. E. Dubin,Scott E. Parker*

Main category: quant-ph

TL;DR: 开发了模拟Penning阱中高达10^5个离子激光冷却的数值框架，优化了椭球形3D晶体冷却，发现了增强冷却的新途径，实现了低于1mK的垂直动能冷却。


<details>
  <summary>Details</summary>
Motivation: 大型3D囚禁离子晶体在量子传感协议中提供改进的灵敏度，但现有数值技术在离子数量增加时效率低下，需要开发更强大的模拟框架来研究大规模离子晶体的激光冷却。

Method: 开发了强大的数值框架来模拟Penning阱中高达10^5个离子的激光冷却，应用于表征和优化椭球形3D晶体的冷却，研究了在势能主导的E×B模式中添加轴向分量的新冷却途径。

Result: 实现了低于1毫开尔文的垂直动能冷却，特别是在长椭球离子晶体中观察到显著增强的冷却效果，提出了导致各种示例中最佳冷却的特定阱和激光束参数值。

Conclusion: 这项工作展示了为高灵敏度量子科学协议准备大型3D晶体的可行性，激励了它们在未来实验中的应用，为简化长椭球晶体的冷却光束设置提供了可能。

Abstract: Large, 3D trapped ion crystals offer improved sensitivity in quantum sensing protocols, and are expected to be implemented as platforms in near-future experiments. However, numerical techniques used to study the laser cooling of such crystals are inefficient as the number of ions, $N$, in the crystal increases. Here we develop a powerful numerical framework to simulate laser cooling of up to $10^5$ ions stored in a Penning trap. We apply this framework to characterize and optimize the cooling of ellipsoidal 3D crystals. We document new pathways to enhanced cooling based on the addition of an axial component to the potential energy-dominated $\boldsymbol{E}\times\boldsymbol{B}$ modes. Furthermore, we observe greatly enhanced cooling of the perpendicular kinetic energy to below 1 mK in prolate ion crystals, enabling a simplified cooling beam setup for such crystals. We propose specific values of trap and laser beam parameters which lead to optimal cooling in a variety of examples. This work illustrates the feasibility of preparing large 3D crystals for high-sensitivity quantum science protocols, motivating their use in future experiments.

</details>


### [145] [Loss-insensitive quantum noise reduction in a Raman amplifier with coherent feedback](https://arxiv.org/abs/2602.22567)
*Jianmin Wang,Rong Zhu,Z. Y. Ou*

Main category: quant-ph

TL;DR: 利用拉曼放大过程中斯托克斯场与原子自旋波之间的量子关联，通过反馈部分斯托克斯场来抑制量子噪声，实现了高达6dB的噪声降低


<details>
  <summary>Details</summary>
Motivation: 量子放大器在放大信号时通常不可避免地会通过内部自由度耦合引入额外噪声，而量子关联可以有效抑制这种额外噪声

Method: 利用拉曼放大过程中建立的斯托克斯场与原子自旋波之间的量子关联，将部分斯托克斯场反馈回放大器，形成单路径反馈放大器

Result: 在高增益条件下实现了与反馈损耗无关的量子噪声降低，最大观测到6dB的噪声抑制，系统对反馈相位敏感

Conclusion: 单路径反馈放大器对反馈相位的敏感性扩展了其在量子精密测量中的应用潜力，该概念可扩展到集成光学和光纤系统

Abstract: A quantum amplifier usually adds extra noise inevitably through coupling to internal degrees of freedom while amplifying the signal. The introduction of quantum correlations can effectively suppress this extra noise. In this work, we utilize the established quantum correlation between the Stokes field and atomic spin waves in the Raman amplification process to feedback a portion of the Stokes field into the amplifier. This leads to a reduction in quantum noise that is independent of the feedback loss at high gain. A maximum of 6 dB noise reduction is observed. The single-path feedback amplifier is found to be sensitive to the feedback phase, a property that expands its potential for applications in quantum precision measurement, and the general concept can be extended to integrated optics and fiber optic systems.

</details>


### [146] [SYK thermal expectations are classically easy at any temperature](https://arxiv.org/abs/2602.22619)
*Alexander Zlokapa,Bobak T. Kiani*

Main category: quant-ph

TL;DR: 提出一个经典算法，在自由能相变温度以上，以准多项式复杂度近似计算热态期望值，适用于包括SYK模型在内的多种系统


<details>
  <summary>Details</summary>
Motivation: 估计局域可观测量在热态下的期望值是量子优势的自然目标，但需要找到经典算法能够有效处理的情况

Method: 提出一个简单的经典算法，利用自由能相变分析，对SYK模型使用复本技巧控制配分函数的复零点

Result: 算法在相变温度以上具有准多项式复杂度n^{O(log n/ε)}，适用于包括SYK模型在内的多种自然模型

Conclusion: 该经典算法能够在量子优势可能存在的区域（包括SYK模型）有效工作，即使在热态高度纠缠且满足量子电路下界的情况下

Abstract: Estimating thermal expectations of local observables is a natural target for quantum advantage. We give a simple classical algorithm that approximates thermal expectations, and we show it has quasi-polynomial cost $n^{O(\log n/ε)}$ for all temperatures above a phase transition in the free energy. For many natural models, this coincides with the entire fast-mixing, quantumly easy phase. Our results apply to the Sachdev-Ye-Kitaev (SYK) model at any constant temperature -- including when the thermal state is highly entangled and satisfies polynomial quantum circuit lower bounds, a sign problem, and nontrivial instance-to-instance fluctuations. Our analysis of the SYK model relies on the replica trick to control the complex zeros of the partition function.

</details>


### [147] [Vibration induced transparency and absorption with two ion ensembles in a linear trap](https://arxiv.org/abs/2602.22635)
*Wenjun Shao,Jian Li*

Main category: quant-ph

TL;DR: 研究两个原子离子系综在光场驱动下的集体低激发谱，发现振动诱导透明现象和吸收峰到透明窗口的转换


<details>
  <summary>Details</summary>
Motivation: 研究线性阱中两个原子离子系综在激光驱动下的集体激发谱特性，探索振动诱导的量子光学现象

Method: 通过激光驱动线性阱中的两个原子离子系综，分析其对外部光场的响应谱和集体激发模式的涨落谱

Result: 当激光失谐调至第一红边带时出现振动诱导透明现象；调至第一蓝边带时出现吸收峰到透明窗口的转换；集体激发模式涨落谱也显示类似现象

Conclusion: 原子离子系综在特定激光失谐条件下展现出有趣的振动诱导量子光学效应，为量子信息处理和量子光学研究提供了新视角

Abstract: We study the spectra of collective low excitations of two atomic ion ensembles which are confined in a liner trap by addressing lases. When the left ensemble is driven by an external optical field, its corresponding response spectrum to the incident optical light shows a vibration-induced transparency phenomenon when the detuning of the laser addressing the ion is tuned to the first red sideband. In the case of the detuning tuned to the first blue sideband, the response spectrum shows a conversion from the absorption peak to the transparency window. Furthermore, we investigate the fluctuation spectra of the collective excitation modes of ion ensemble and show the similar phenomena.

</details>


### [148] [Finite key analysis of experimentally realized practical COW-QKD protocol](https://arxiv.org/abs/2602.22646)
*Neha Pathania,Sandeep Mishra,Anirban Pathak*

Main category: quant-ph

TL;DR: 实验实现了COW-QKD协议，提供了有限密钥分析框架，在现实条件下获得了1.2-1.6 kbps的稳定安全密钥率，验证了COW-QKD在中等距离传输（最高约171公里）的安全性。


<details>
  <summary>Details</summary>
Motivation: 在现实条件下实现COW-QKD协议，并为其提供一个简洁易用的有限密钥分析框架，以评估协议在实际应用中的安全性和性能。

Method: 实验实现了COW-QKD协议，扩展了现有结果以建立有限密钥分析框架，使用实验参数进行有限密钥率分析，并在不同实验参数下运行系统数小时。

Result: 获得了1.2-1.6 kbps的稳定安全密钥率，得到了有限密钥分析下的QBER、相位错误率和安全密钥率，验证了COW-QKD在中等距离传输（最高约171公里光纤）的安全性。

Conclusion: COW-QKD协议在现实条件下可行，提供的有限密钥分析框架有效，协议在中等距离传输中安全，为实际应用奠定了基础。

Abstract: An experimental implementation of the Coherent One-Way Quantum Key Distribution (COW-QKD) protocol is reported under realistic conditions, and a clean and easy-to-use framework for performing finite key analysis of the COW-QKD protocol is provided by extending a set of existing results. The framework provided here is used to perform finite key rate analysis of the COW-QKD protocol with respect to the actual parameters used in the experimental realization reported here. The system is kept running for several hours with different experimental parameters and stable secure key rates between 1.2 to 1.6 kbps are observed. In addition, QBER, phase error rate and secure key rate are obtained under finite key analysis, and it is shown that COW-QKD is secure for medium-range transmissions (up-to ~ 156 (171) km of optical fiber with 0.2 dB loss per km if detector efficiency is 0.1 (0.2) and other parameters are same as those used in this experiment).

</details>


### [149] [Thermodynamic Uncertainty Relation with Quantum Feedback](https://arxiv.org/abs/2602.22651)
*Ryotaro Honma,Tan Van Vu*

Main category: quant-ph

TL;DR: 该论文推导了包含反馈控制的量子热力学不确定性关系，将电流涨落与熵产生和互信息联系起来，揭示了反馈控制如何同时抑制涨落并引入信息代价。


<details>
  <summary>Details</summary>
Motivation: 传统热力学不确定性关系(TUR)描述了涨落与熵产生的关系，但反馈控制在TUR框架中的作用尚不明确，特别是在量子系统中，控制本质上是信息驱动的。需要量化反馈控制中的信息贡献，建立包含信息代价的精度界限。

Method: 研究弱耦合于热环境的开放量子系统，连续监测量子跃迁并应用马尔可夫反馈控制。使用量子互信息量化反馈引起的信息贡献，推导出任意时间积分电流的有限时间TUR，将熵产生和互信息联系起来。

Result: 推导出包含反馈控制的量子热力学不确定性关系，揭示了反馈控制如何同时抑制涨落与热力学代价，建立了信息驱动控制的基本精度界限。在量子时钟模型中应用，证明在单个热库存在下，反馈控制可以增强时钟精度。

Conclusion: 该工作将信息贡献纳入热力学不确定性关系框架，为量子反馈控制提供了理论基础，揭示了信息在抑制涨落中的作用，并建立了信息驱动控制的基本精度界限。

Abstract: Fluctuations are intrinsic to microscopic systems and impose fundamental limits on nonequilibrium precision, as captured by the thermodynamic uncertainty relation (TUR), which links current fluctuations to entropy production. While feedback control is expected to further suppress fluctuations, its role within the TUR framework has remained unclear, particularly in quantum systems where control is inherently information-driven. In this Letter, we consider open quantum systems weakly coupled to a thermal environment, in which quantum jumps are continuously monitored, and Markovian feedback is applied. Using quantum mutual information to quantify the information contribution induced by feedback, we derive a finite-time TUR for arbitrary time-integrated currents in terms of entropy production and mutual information. Our results uncover how feedback control suppresses fluctuations together with thermodynamic cost and establishes a fundamental precision bound imposed by information-based control. As an application, we analyze a quantum clock model and demonstrate that the clock precision can be enhanced by feedback control in the presence of a single thermal reservoir.

</details>


### [150] [Quantum-Optically Resolving the Number of Colloidal Quantum Dots in a Subwavelength Volume](https://arxiv.org/abs/2602.22677)
*Zhi-Bo Ni,Jia-Wang Yu,Jiong-Zhao Li,Xiao-Tian Cheng,Mei-Na Jiang,Zi-Xuan Song,Xiao-Qing Zhou,Wei Fang,Chen-Hui Li,Feng Liu,Xing Lin,Chao-Yuan Jin*

Main category: quant-ph

TL;DR: 提出一种基于时域量子光学方法，用于精确计数亚波长尺寸聚苯乙烯胶囊中胶体CdSe/CdS/ZnS量子点数量的技术。


<details>
  <summary>Details</summary>
Motivation: 固态人工原子的数量分辨对量子少体系统研究至关重要，但实验上具有挑战性。量子光学实验提供了一种非侵入式方法，能将宏观测量与量子发射体数量联系起来。

Method: 采用时域量子光学方法，利用亚波长体积中胶体量子点的非偏振、均匀展宽发射特性，满足相同量子发射体的Dicke超辐射描述。通过解析关系描述二阶光子关联对发射体数量和集体寿命的数值依赖性。

Result: 建立了量子点数量与二阶光子关联之间的解析关系，实现了从1到10个胶体量子点的实验计数范围。

Conclusion: 该工作为非侵入式人工原子计数和纳米尺度集体光-物质相互作用研究提供了稳健途径。

Abstract: The number resolution of solid-state artificial atoms is of fundamental interest for the study of quantum few-body systems, yet remains experimentally challenging. Quantum optical experiments offer a non-invasive approach which links up macroscopic measurements with the quantity of quantum emitters. In this work, we propose a time-domain quantum optical methodology for the strict numbering of colloidal CdSe/CdS/ZnS quantum dots (QDs) confined in subwavelength-size polystyrene capsules. The non-polarized, homogeneously broadened emission of colloidal QDs in the subwavelength volume satisfies the description of Dicke's superradiance of identical quantum emitters. An analytic relation describes the numerical dependence of the second-order photon correlation on the number and the collective lifetime of emitters, yielding an experimental counting range of colloidal QDs from one to ten. This work provides a robust pathway for the non-invasive numbering of artificial atoms and the investigation of collective light-matter interactions at the nanoscale.

</details>


### [151] [Ideal random quantum circuits pass the LXEB test](https://arxiv.org/abs/2602.22692)
*Nicholas Hunter-Jones,Jonas Haferkamp*

Main category: quant-ph

TL;DR: 随机量子电路在线性深度时能以高概率通过线性交叉熵基准测试，二次深度时通过概率更高


<details>
  <summary>Details</summary>
Motivation: 研究随机量子电路在何种条件下能通过线性交叉熵基准测试，这对于量子计算验证和量子优势实验有重要意义

Method: 使用高阶矩和高度近似设计分析随机电路的碰撞概率集中性和输出概率分布尾部行为

Result: 线性深度电路通过LXEB测试概率为1-O(1/√k)，二次深度电路通过概率为1-O(e^{-k log(n)/n})

Conclusion: 随机量子电路在不同深度下都能以高概率通过线性交叉熵基准测试，为量子优势实验提供了理论支持

Abstract: We show that noiseless random quantum circuits pass the linear cross-entropy benchmark (LXEB) test with high probability. If the circuits are linear depth, and thus form unitary 4-designs, the LXEB test is passed with probability $1-O(1/\sqrt{k})$, where $k$ is the number of independently drawn samples from the output distribution of the random circuit. If the circuits are of depth $\tilde O(n^2)$, and thus form unitary $n$-designs, the LXEB test is passed with probability $1-O(e^{-k \log(n)/n})$. In proving our results, we show strong concentration of the random circuit collision probability at linear depth and establish that the tails of the distribution of random circuit output probabilities start to resemble Porter-Thomas at near-quadratic depths. Our analysis employs higher moments and high-degree approximate designs.

</details>


### [152] [Effective Repulsive Action of Gravitational Quantum Superpositions Under Postselection](https://arxiv.org/abs/2602.22715)
*Sougato Bose,Lev Vaidman*

Main category: quant-ph

TL;DR: 量子引力研究中发现，当源质量处于量子叠加态时，探测质量可能表现出排斥而非吸引的异常行为，这揭示了时空量子叠加的可能性。


<details>
  <summary>Details</summary>
Motivation: 传统引力理论认为引力总是吸引力，但本文探讨在量子力学框架下，当源质量处于量子叠加态时，引力相互作用是否会出现非经典行为，以验证时空能否存在量子叠加。

Method: 使用负弱值异常效应技术，通过将源质量制备在空间上不同的量子叠加态，让探测质量与之相互作用，然后条件性地测量源质量的特定状态。

Result: 在特定条件下，探测质量会远离源质量（排斥效应），这表明引力场可以在量子叠加态中存在，时空本身能够处于量子叠加状态。

Conclusion: 量子引力可以表现出排斥效应，这为实验验证时空量子叠加提供了新途径，并建议使用带自旋的纳米晶体进行实验实现。

Abstract: A classic feature of gravity is that it is an attractive force. If a source mass is prepared in a localized (classical- like) state, it will cause another probe mass to move towards it. Here we consider the situation in which a source mass is prepared in a quantum superposition of distinct spatial states while a probe mass interacts with it. Conditional on the detection of the source mass in a specific state, the probe mass will be found to move away from the source mass (repulsion). This signifies the quantum superposition of gravitational forces acting on the probe mass and thereby the fact that spacetime can exist in quantum superpositions. The technique used is the repulsive effect arising from an anomalous negative weak value. A potential experimental implementation with spin bearing nanocrystals is outlined.

</details>


### [153] [A matching decoder for bivariate bicycle codes](https://arxiv.org/abs/2602.22770)
*Kaavya Sahay,Dominic J. Williamson,Benjamin J. Brown*

Main category: quant-ph

TL;DR: 该论文提出了一种基于最小权重完美匹配算法的解码器，通过"圆柱技巧"将双变量自行车码转换为环面码副本进行快速解码，并在多种量子纠错码上验证了性能。


<details>
  <summary>Details</summary>
Motivation: 随着能够将多个逻辑量子比特编码到相对较少物理量子比特的新型量子纠错码的发现，需要开发高效准确的解码方法来处理这些系统。

Method: 采用最小权重完美匹配算法，利用双变量自行车码与环面码副本的等价性，提出"圆柱技巧"在码对称性上进行匹配以快速找到纠错操作。

Result: 在gross码族、循环超图积码、广义环面码和最近提出的方向性码上进行了基准测试，证明了该协议的普适性。对于部分码，通过结合置信传播和"过度匹配"策略，性能可与最先进方法竞争。

Conclusion: 该方法为双变量自行车码提供了一种高效准确的解码方案，通过最小权重完美匹配算法和圆柱技巧实现了快速纠错，并在多种量子纠错码上展现了良好的性能。

Abstract: The discovery of new quantum error-correcting codes that encode several logical qubits into relatively few physical qubits motivates the development of efficient and accurate methods of decoding these systems. Here, we adopt the minimum-weight perfect matching algorithm, a subroutine invaluable to decoding topological codes, to decode bivariate bicycle codes. Using the equivalence of bivariate bicycle codes to copies of the toric code, we propose a method we call the 'cylinder trick' to rapidly find a correction using matching on code symmetries. We benchmark our decoder on the gross code family, cyclic hypergraph-product codes, generalized toric codes, and recently proposed directional codes, demonstrating the general applicability of our protocol. For a subset of these codes, we find that our decoder can be significantly improved by augmenting matching with strategies including belief propagation and 'over-matching', thus achieving performance competitive with state-of-the-art approaches.

</details>


### [154] [Optimization-based Unfolding in High-Energy Physics](https://arxiv.org/abs/2602.22776)
*Simone Gasperini,Gianluca Bianco,Marco Lorusso,Carla Rieger,Michele Grossi*

Main category: quant-ph

TL;DR: 将高能物理中的反卷积问题重新表述为优化问题，开发了经典和量子兼容的解决方案，并创建了QUnfold工具包进行基准测试


<details>
  <summary>Details</summary>
Motivation: 高能物理中需要从探测器失真的测量数据重建物理观测量的真实分布，传统反卷积方法存在局限性，需要探索量子增强的新方法

Method: 将反卷积重新表述为正则化二次优化问题，推导出QUBO表示形式，开发QUnfold开源Python包，集成经典混合整数求解器和D-Wave混合量子求解器

Result: 优化方法在多个分布上实现了有竞争力的重建精度，能够自然地融入正则化，为量子增强方法在HEP数据分析中提供了实用路径

Conclusion: 建立了反卷积的统一优化视角，为探索量子增强方法在实验高能物理数据分析中的应用提供了实用途径

Abstract: In High-Energy Physics, unfolding is the process of reconstructing true distributions of physical observables from detector-distorted measurements. Starting from its reformulation as a regularized quadratic optimization, we develop a framework to tackle this problem using both classical and quantum-compatible methods. In particular, we derive a Quadratic Unconstrained Binary Optimization (QUBO) representation of the unfolding objective, allowing direct implementation on quantum annealing and hybrid quantum-classical solvers. The proposed approach is implemented in QUnfold, an open-source Python package integrating classical mixed-integer solvers and D-Wave's hybrid quantum solver. We benchmark the method against widely used unfolding techniques in RooUnfold, including response Matrix Inversion, Iterative Bayesian Unfolding, and Singular Value Decomposition unfolding, using synthetic dataset with controlled distortion effects. Our results demonstrate that the optimization-based approach achieves competitive reconstruction accuracy across multiple distributions while naturally accommodating regularization within the objective function. This work establishes a unified optimization perspective on unfolding and provides a practical pathway for exploring quantum-enhanced methods in experimental HEP data analysis.

</details>


### [155] [Generating entangled polaritonic condensates by pumping with entangled pairs of photons](https://arxiv.org/abs/2602.22778)
*N. A. Asriyan,A. A. Elistratov,A. V. Kavokin*

Main category: quant-ph

TL;DR: 研究两个空间分离的极化激元凝聚体在纠缠光子对泵浦下的稳态，证明系统可达到纠缠态，并提供纠缠泵浦通量和纠缠寿命的估计。


<details>
  <summary>Details</summary>
Motivation: 探索在噪声环境下（激子库和光子泄漏）使用纠缠光子对泵浦能否使两个空间分离的极化激元凝聚体达到纠缠稳态。

Method: 研究两个单模均匀空间分离的极化激元凝聚体在共振泵浦下的稳态，分析系统在纠缠光子对泵浦下的演化，考虑激子库噪声和微腔镜面光子泄漏的影响。

Result: 证明了系统在噪声环境下仍能达到纠缠态，提供了驱动系统进入违反部分转置判据的稳态所需的纠缠粒子通量估计，并追踪了纠缠泵浦突然消失后系统的演化。

Conclusion: 分析表明两个激子-极化激元凝聚体系统在纠缠光子对泵浦下可实现纠缠态，并提供了纠缠寿命的估计，为量子信息处理应用提供了理论基础。

Abstract: We investigate the steady state of two single-mode uniform spatially separated polaritonic conden- sates exposed to resonant pumping with entangled pairs of photons. We demonstrate the principal possibility of driving the system to an entangled state despite its exposure to noises arising from the excitonic reservoir and photon leakage through the microcavity mirrors. Estimates are provided for the flux of entangled particles required to drive the system into a steady state that violates the partial-transpose criterion for entanglement. Furthermore, we trace the evolution of the system after a sudden disappearance of the entangled pumping. Our analysis provides estimates for the entanglement lifetime in a system of two exciton-polariton condensates

</details>


### [156] [No Absolute Hierarchy of Quantum Complementarity](https://arxiv.org/abs/2602.22792)
*Kunika Agarwal,Sahil Gopalkrishna Naik,Ananya Chakraborty,Guruprasad Kar,Ram Krishna Patra,Snehasish Roy Chowdhury,Manik Banik*

Main category: quant-ph

TL;DR: 量子互补性层次结构在多副本体系中不再绝对：两个可观测量的不相容程度排序会因量子探针的全局配置方式（相同副本vs平行-反平行对）而反转。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为量子互补性诱导了不相容可观测量之间的内在层次结构，某些量子属性集合本质上比其他集合更不相容。本文旨在探索这种层次结构在多副本体系中的有效性。

Method: 分析量子比特自旋可观测量，证明"无比较定理"，表明不相容可观测量集合的全局排序在所有有限副本配置中都无法保持。特别研究了相同副本与平行-反平行对两种资源配置方式。

Result: 发现互补性排序可以反转，取决于可用资源是安排为相同副本还是平行-反平行对。量子不相容程度不是可观测量本身的固有属性，而是依赖于量子探针的全局配置。

Conclusion: 量子互补性具有配置依赖的结构，纠缠在塑造测量限制结构中扮演微妙角色，需要在有限资源下重新评估量子信息协议。

Abstract: Bohr's principle of complementarity, prohibiting simultaneous access to certain physical properties within a single experimental arrangement, is considered to be a defining feature of quantum mechanics. It is commonly viewed as inducing an intrinsic hierarchy among incompatible observables: some sets of quantum properties are fundamentally more incompatible than others, as quantified by the maximal sharpness permitting their joint measurement. We show that this hierarchy ceases to be absolute in the multi-copy regime. Analyzing qubit spin observables, we prove a No-Comparison Theorem establishing that no global ordering of incompatible observable sets is preserved across all finite-copy configurations. In particular, two sets of observables can exhibit reversed complementarity ordering depending solely on whether the available resources are arranged as identical copies or as parallel-antiparallel pairs. Thus, the degree of quantum incompatibility is not an intrinsic property of observables alone but depends on the global configuration of the prepared quantum probes. Our results uncover a configuration-dependent structure of complementarity, reveal a subtle role of entanglement in shaping the structure of measurement limitations, and call for a reassessment of quantum information protocols under finite resources.

</details>


### [157] [Control of Multipartite Entanglement through Anisotropy against Thermal Noise](https://arxiv.org/abs/2602.22815)
*Samudra Sur,Saikat Sur*

Main category: quant-ph

TL;DR: 通过调节各向异性参数，可以在低温下增强多体量子系统中多部分纠缠的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 开放多体量子系统中的多部分纠缠不可避免地受到环境噪声的影响，如何保护这种纠缠是量子计算平台面临的基本挑战

Method: 使用主方程方法结合Bethe ansatz技术，研究各向异性XXZ自旋链与热自旋浴相互作用的开放系统动力学，分析广义GHZ态和广义W态两种不同类型多部分纠缠的演化

Result: 在低温下，通过适当调节系统的各向异性参数，可以增强多部分纠缠的鲁棒性

Conclusion: 相互作用诱导的光谱控制机制为稳定量子计算平台中的多部分纠缠提供了有效途径

Abstract: Preserving multipartite entanglement in open many-body quantum systems is fundamentally limited by unavoidable environmental noise. We study the open-system dynamics of multipartite entanglement in an anisotropic XXZ spin chain interacting with a thermal spin bath, focusing on two states with distinct types of multipartite entanglement: the generalized GHZ and the generalized W state. Using a master-equation approach combined with the Bethe ansatz technique, we show analytically that robustness of multipartite entanglement at low temperatures can be enhanced by suitably tuning the anisotropy of the system. Our results highlight interaction-induced spectral control as a mechanism for stabilizing multipartite entanglement in quantum computing platforms.

</details>


### [158] [Dequantization Barriers for Guided Stoquastic Hamiltonians](https://arxiv.org/abs/2602.23183)
*Yassine Hamoudi,Yvan Le Borgne,Shrinidhi Teganahally Sridhara*

Main category: quant-ph

TL;DR: 该论文构造了一个由指数大图的Perron-Frobenius特征向量诱导的概率分布，证明任何经典算法都无法高效采样，即使提供最优预热分布，而量子计算机可以高效解决该问题。


<details>
  <summary>Details</summary>
Motivation: 研究经典算法在解决stoquastic哈密顿量基态准备问题上的局限性，特别是当给定引导态作为输入时。旨在证明量子计算机在解决这类问题上的优势，并排除经典算法的去量子化可能性。

Method: 从一类高度数、高围长的谱扩展图出发，通过附加自相似树构造指数大图。利用该图的Perron-Frobenius特征向量诱导概率分布，分析经典算法的采样复杂度。

Result: 证明构造的概率分布无法被任何经典算法高效采样，即使提供最优预热分布。这排除了所有经典算法解决引导基态准备问题的可能性，强化了先前关于stoquastic绝热路径算法去量子化不可能性的结果。

Conclusion: 量子计算机在解决stoquastic基态问题方面具有经典算法无法比拟的优势，表明一大类stoquastic基态问题无法被高效经典算法解决，为量子优势提供了理论支持。

Abstract: We construct a probability distribution, induced by the Perron--Frobenius eigenvector of an exponentially large graph, which cannot be efficiently sampled by any classical algorithm, even when provided with the best-possible warm-start distribution. In the quantum setting, this problem can be viewed as preparing the ground state of a stoquastic Hamiltonian given a guiding state as input, and is known to be efficiently solvable on a quantum computer. Our result suggests that no efficient classical algorithm can solve a broad class of stoquastic ground-state problems.
  Our graph is constructed from a class of high-degree, high-girth spectral expanders to which self-similar trees are attached. This builds on and extends prior work of Gilyén, Hastings, and Vazirani [Quantum 2021, STOC 2021], which ruled out dequantization for a specific stoquastic adiabatic path algorithm. We strengthen their result by ruling out any classical algorithm for guided ground-state preparation.

</details>


### [159] [Connecting Quantum Contextuality and Nonlocality](https://arxiv.org/abs/2602.23221)
*Jianqi Sheng,Dongkai Zhang,Lixiang Chen*

Main category: quant-ph

TL;DR: 本文综述了基于层理论和图论框架，统一研究量子关联中的上下文性和非定域性，揭示了它们之间的深层联系及其在量子技术中的应用。


<details>
  <summary>Details</summary>
Motivation: 量子理论中的上下文性和非定域性现象最初被视为基础物理的奇特现象，但现在被理解为量子计算、通信和模拟中的关键操作资源。尽管传统上在各自领域分别研究，但近期理论和实验进展揭示了它们之间深刻的概念、数学和操作联系。

Method: 采用层理论和图论框架，提供统计关联的理论无关表征。这些方法阐明了上下文性和非定域性之间的结构关系，促进了实验可测试不等式的制定，并指导了在现实物理平台（特别是光子系统）中的实现。

Result: 通过统一视角展示了上下文性和非定域性之间的深层联系，为量子关联的非经典基础提供了清晰理解，并建立了抽象理论结构与具体实验实现之间的桥梁。

Conclusion: 层理论和图论框架为理解量子关联提供了统一视角，不仅阐明了上下文性和非定域性的结构关系，还促进了它们在量子技术中的实际应用，特别是在光子系统中。

Abstract: Quantum theory departs from classical physics in its treatment of correlations, most prominently through the phenomena of contextuality and nonlocality. Once regarded primarily as foundational curiosities, these effects are now understood as key operational resources for quantum computation, communication, and simulation. Although traditionally investigated in distinct settings, recent theoretical and experimental advances have revealed deep conceptual, mathematical, and operational connections between them. This review presents a unified perspective on these developments based on sheaf-theoretic and graph-theoretic frameworks, which provide theory-independent characterizations of statistical correlations. These approaches clarify the structural relationship between contextuality and nonlocality, facilitate the formulation of experimentally testable inequalities, and guide implementations in realistic physical platforms, with particular emphasis on photonic systems. By bridging abstract theoretical structures and concrete experimental realizations, this review sheds light on the nonclassical foundations of quantum correlations and their emerging role in quantum technologies.

</details>


### [160] [Polarization-selective quantum cooperative response in dual-species atom arrays](https://arxiv.org/abs/2602.23237)
*Huan Wang,Shangguo Zhu,Yun Long,Fei Zhang,Yinghui Guo,Mingbo Pu,Xiangang Luo*

Main category: quant-ph

TL;DR: 双物种原子阵列通过打破面内对称性实现偏振选择性量子光调制


<details>
  <summary>Details</summary>
Motivation: 单物种原子阵列受限于面内对称性，无法实现偏振控制。研究双物种原子阵列旨在打破这种对称性限制，开发可重构的量子光-物质界面平台。

Method: 通过设计双物种亚波长原子阵列的晶格间距和失谐，利用不同原子物种的固有极化率差异打破面内对称性，实现偏振依赖的亚辐射模式。

Result: 双物种阵列表现出偏振选择性亚辐射模式，能够完全反射特定偏振分量。通过将阵列单元组装为功能像素，实现了可扩展的偏振选择性量子光调制器。

Conclusion: 该工作建立了一个动态可重构的原子-光子平台，为多功能亚波长量子光学元件的发展奠定了基础。

Abstract: Atom arrays have emerged as a powerful platform for quantum light-matter interfaces, yet single-species arrays are constrained by in-plane symmetry, restricting polarization control. Here we investigate the cooperative optical response of dual-species subwavelength atom arrays, in which intrinsic polarizability difference breaks in-plane symmetry. By engineering the lattice spacing and detunings, the arrays exhibit polarization-dependent subradiant modes, enabling complete reflection of specific polarization component. Leveraging this mechanism, we assemble array units as functional pixels and demonstrate a scalable polarization-selective quantum light modulator. Our work establishes a dynamically reconfigurable atomic-photonic platform for versatile subwavelength quantum optical elements.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [161] [Adaptive Patching for Tensor Train Computations](https://arxiv.org/abs/2602.22372)
*Gianluca Grosso,Marc K. Ritter,Stefan Rohshap,Samuel Badr,Anna Kauch,Markus Wallerberger,Jan von Delft,Hiroshi Shinaoka*

Main category: physics.comp-ph

TL;DR: 提出自适应分块方案，利用块稀疏QTT结构降低大键维度的计算成本，通过分治法将张量划分为小分块，显著提升局部化函数计算效率


<details>
  <summary>Details</summary>
Motivation: 量子张量链(QTT)操作如矩阵乘积算符收缩在大键维度下计算成本过高，限制了大规模QTT计算的实际应用

Method: 提出自适应分块方案，利用QTT的块稀疏结构，通过分治法自适应地将张量划分为更小的分块，每个分块具有更低的键维度

Result: 对于尖锐局部化函数计算显示出显著改进，能够高效计算气泡图和Bethe-Salpeter方程

Conclusion: 该方法为先前无法实现的大规模QTT计算打开了实际应用的大门

Abstract: Quantics Tensor Train (QTT) operations such as matrix product operator contractions are prohibitively expensive for large bond dimensions. We propose an adaptive patching scheme that exploits block-sparse QTT structures to reduce costs through divide-and-conquer, adaptively partitioning tensors into smaller patches with reduced bond dimensions. We demonstrate substantial improvements for sharply localized functions and show efficient computation of bubble diagrams and Bethe-Salpeter equations, opening the door to practical large-scale QTT-based computations previously beyond reach.

</details>


### [162] [Discovery of Interpretable Physical Laws in Materials via Language-Model-Guided Symbolic Regression](https://arxiv.org/abs/2602.22967)
*Yifeng Guan,Chuyi Liu,Dongzhan Zhou,Lei Bai,Wan-jian Yin,Jingyuan Li,Mao Su*

Main category: physics.comp-ph

TL;DR: 提出一个利用大语言模型科学知识指导搜索的框架，高效发现高维数据中的物理定律，应用于钙钛矿材料建模，大幅缩减搜索空间并发现更优公式


<details>
  <summary>Details</summary>
Motivation: 从高维数据中发现可解释的物理定律是科学研究的基本挑战。传统方法（如符号回归）在搜索可能形式的大空间时，经常产生复杂、非物理的公式。

Method: 引入一个框架，通过利用大语言模型嵌入的科学知识来指导搜索过程，从而高效识别数据中的物理定律。该方法缓解了传统符号回归中常见的组合爆炸问题。

Result: 将方法应用于钙钛矿材料建模，有效搜索空间减少了约10^5倍。识别出一组新的体模量、带隙和氧析出反应活性公式，这些公式不仅提供了有意义的物理见解，而且在准确性和简洁性方面优于之前的公式。

Conclusion: 该框架通过利用大语言模型的科学知识指导搜索，能够高效发现物理定律，显著减少搜索空间，并产生更准确、更简洁的物理公式，为科学发现提供了新途径。

Abstract: Discovering interpretable physical laws from high-dimensional data is a fundamental challenge in scientific research. Traditional methods, such as symbolic regression, often produce complex, unphysical formulas when searching a vast space of possible forms. We introduce a framework that guides the search process by leveraging the embedded scientific knowledge of large language models, enabling efficient identification of physical laws in the data. We validate our approach by modeling key properties of perovskite materials. Our method mitigates the combinatorial explosion commonly encountered in traditional symbolic regression, reducing the effective search space by a factor of approximately $10^5$. A set of novel formulas for bulk modulus, band gap, and oxygen evolution reaction activity are identified, which not only provide meaningful physical insights but also outperform previous formulas in accuracy and simplicity.

</details>


### [163] [Ceci n'est pas un committor, yet it samples like one: efficient sampling via approximated committor functions](https://arxiv.org/abs/2602.23236)
*Enrico Trizio,Giorgia Rossi,Michele Parrinello*

Main category: physics.comp-ph

TL;DR: 提出一种简化的机器学习方法，在描述符空间中学习提交函数，避免昂贵的坐标梯度计算，显著降低计算成本，同时保持鲁棒的采样性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于提交函数的增强采样方法需要计算原子坐标梯度，这在某些情况下计算成本很高，限制了其在实际复杂反应过程研究中的应用。

Method: 提出简化的学习准则，完全在描述符空间中表述，绕过显式的坐标梯度计算，为原始变分原理提供松弛上界，使用神经网络表示提交函数。

Result: 新方法显著降低了计算成本，同时保持了鲁棒的采样性能，使得原本使用原始方法不切实际的过程研究成为可能。

Conclusion: 在描述符空间中学习提交函数的简化方法是一种有效的计算策略，能够在保持采样性能的同时大幅降低计算负担，扩展了增强采样方法的应用范围。

Abstract: Atomistic simulations are widely used to investigate reactive processes but are often limited by the rare event problem due to kinetic bottlenecks. We recently introduced an enhanced sampling approach based on the committor function, machine-learned following a variational principle. This method combines a transition-state-oriented bias potential, expressed as a functional of the committor, with a metadynamics-like bias along a committor-based collective variable, enabling uniform exploration of reaction pathways. In its original formulation, the committor is represented by a neural network that takes physical descriptors as input and is trained by minimizing a functional involving gradients with respect to atomic coordinates, which can be computationally demanding in some cases. Here, we propose a simplified learning criterion formulated entirely in the descriptor space, which bypasses the need for explicit and costly coordinate gradients and provides a relaxed upper bound to the original variational principle. Although this approach does not formally target the exact committor, we show that it retains robust sampling performance while significantly reducing computational costs, thus enabling the study of processes that would be practically unfeasible using the original formulation.

</details>
