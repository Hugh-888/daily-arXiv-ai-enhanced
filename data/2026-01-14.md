<div id=toc></div>

# Table of Contents

- [gr-qc](#gr-qc) [Total: 13]
- [quant-ph](#quant-ph) [Total: 45]
- [cs.LG](#cs.LG) [Total: 69]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [1] [Ghost-free non-local $F(R)$ Gravity Compatible with ACT](https://arxiv.org/abs/2601.07879)
*Shin'ichi Nojiri,S. D. Odintsov,V. K. Oikonomou*

Main category: gr-qc

TL;DR: 非局域F(R)引力理论与最新宇宙学观测约束的兼容性分析


<details>
  <summary>Details</summary>
Motivation: 将无鬼非局域F(R)引力理论与最新的宇宙学观测数据（ACT对标量扰动的谱指数约束和Planck/BICEP对张标比的更新约束）进行对比验证

Method: 首先回顾如何获得无鬼非局域F(R)引力理论，证明在该框架下可以获得de Sitter解，并将理论转化为F(R,φ)形式。然后分析两种非局域F(R)引力模型：幂律模型和R²模型

Result: 两种非局域F(R)引力模型（幂律模型和R²模型）都能与ACT和更新后的Planck/BICEP约束兼容

Conclusion: 无鬼非局域F(R)引力理论在宇宙学观测约束下是可行的，为引力理论提供了新的研究方向

Abstract: We confront the ghost-free non-local $F(R)$ gravity theories with the latest Atacama Cosmology Telescope (ACT) constraints on the spectral index of the scalar perturbations and the updated constraints of Planck/BICEP on the tensor-to-scalar ratio. After reviewing how the ghost-free non-local version of $F(R)$ gravity can be obtained, we show that the de Sitter solution can be obtained in this framework. Also, we show that the resulting theory can be cast in terms of an $F(R,φ)$ theory of gravity. We analyze two models of non-local $F(R)$ gravity, one power-law and the $R^2$ model, and we show that both models can be compatible with the ACT and updated Planck/BICEP constraints.

</details>


### [2] [The Scalar Mach-Sciama Theory of Gravitation](https://arxiv.org/abs/2601.07904)
*Velásquez-Toribio,A. M*

Main category: gr-qc

TL;DR: 在标量-张量引力理论中实现因果马赫原理，通过推迟响应机制将宇宙惯性尺度与物质分布因果关联，同时保持与弱场实验的一致性。


<details>
  <summary>Details</summary>
Motivation: 实现Sciama的马赫原理，将惯性性质与宇宙物质分布因果关联，同时保持与现有引力实验的一致性。

Method: 在Bergmann-Wagoner类标量-张量引力理论框架内，采用不变集表示，通过推迟响应选择规则实现因果马赫原理，将标量场配置限制为对因果过去物质分布的推迟响应。

Result: 在平坦FLRW宇宙中，轻缓变化机制下，背景标量演化与哈勃区域内物质含量通过显式时间核函数关联，实现马赫标度关系；弱场测试中Eötvös参数在主导阶为零，仅在强自引力物体中可能出现非普适效应。

Conclusion: 该理论成功实现了因果马赫原理对宇宙惯性尺度的确定，同时与标准弱场实验一致，为马赫原理在标量-张量引力框架内的实现提供了可行方案。

Abstract: We formulate a scalar realization of Sciama's Machian programme within the general Bergmann-Wagoner class of scalar--tensor gravity. Starting from a universally conformally coupled matter sector, we rewrite the field equations in terms of the invariant set $\{{\cal I}_1,{\cal I}_2,{\cal I}_3,\hat g_{μν}\}$, so that Machian requirements can be stated independently of conformal frame. Sciama's causal postulate is implemented not by modifying the local dynamics, but as a selection rule on the solution space of the invariant scalar equation: the admissible configuration is the retarded response to the matter distribution in the causal past, with any source-free contribution removed. In a spatially flat FLRW universe, and in the light, slowly varying regime, the prescription reduces to an explicit temporal kernel that links the background scalar evolution to the matter content within the Hubble region, reproducing the expected Machian scaling in an expanding background. Universal coupling to a single physical metric implies that structureless test bodies share the same acceleration in a given external configuration, so that the Eötvs parameter vanishes at leading order. Nonuniversal effects can arise only when gravitational binding energy contributes appreciably to the total mass, as in strongly self-gravitating objects. Thus, the theory implements a causal Machian determination of the cosmological inertial scale while remaining consistent with standard weak-field tests.

</details>


### [3] [Signatures of a subpopulation of hierarchical mergers in the GWTC-4 gravitational-wave dataset](https://arxiv.org/abs/2601.07908)
*Cailin Plunkett,Thomas Callister,Michael Zevin,Salvatore Vitale*

Main category: gr-qc

TL;DR: 该研究通过分析黑洞合并事件的自旋特征，发现了黑洞质量分布中约15M⊙和45M⊙以上的两个亚群，为分层合并和金属丰度依赖的恒星团形成机制提供了证据。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索密集恒星团中重复黑洞合并作为填补对不稳定性质量间隙的机制，并通过分析分层合并特有的自旋和倾斜特征来验证这一假设。

Method: 引入了一个基于天体物理动机的模型，在有效螺旋自旋和进动自旋的联合空间中分析黑洞合并数据，该模型能够捕捉分层合并预期的主要自旋动力学特征。

Result: 发现了决定性证据：1) 在约45M⊙以上存在与对不稳定性间隙预期起始一致的人口转变；2) 在约15M⊙处存在峰值，解释为分层合并率的全局峰值。这些结果表明存在低质量和高质量的高代黑洞亚群。

Conclusion: 研究结果支持了分层合并机制的存在，并表明近太阳金属丰度和贫金属恒星团都对分层合并人口有贡献。这强化了黑洞双星人口自旋分布中存在详细、质量依赖的亚结构的证据。

Abstract: Repeated black-hole mergers in dense stellar clusters are a plausible mechanism to populate the predicted gap in black hole masses due to the pair-instability process. These hierarchical mergers carry distinct spin and tilt features relative to first-generation black holes, for which previous studies have found evidence at a population level by interpreting features in the effective inspiral spin domain. We introduce an astrophysically-motivated model in the joint space of effective inspiral and precessing spins, which captures the dominant spin dynamics expected for hierarchical mergers. We find decisive evidence both for a population transition above $\sim 45M_\odot$, consistent with the anticipated onset of the pair-instability gap, as well as a peak at $\sim 15 M_\odot$, which we interpret as the global peak in the hierarchical merger rate. The existence of low- and high-mass subpopulations of higher-generation black holes suggests the contribution of both near-solar-metallicity and metal-poor star clusters to the hierarchical merger population. Our results reinforce the growing evidence for detailed, mass-dependent substructure in the spin distribution of the binary black hole population.

</details>


### [4] [General gravitational properties of neutron stars: curvature invariants, binding energy, and trace anomaly](https://arxiv.org/abs/2601.07931)
*Iván Garibay,Christian Ecker,Luciano Rezzolla*

Main category: gr-qc

TL;DR: 研究揭示中子星内部负Ricci标量曲率相当普遍，约50%的状态方程会产生内部某处曲率为负的中子星，主要出现在高密度、高压力区域，且与刚硬状态方程及致密大质量星相关。同时改进了引力质量与重子质量之间的准普适关系，精度达3%。


<details>
  <summary>Details</summary>
Motivation: 研究中子星内部曲率不变量行为，特别是Ricci标量曲率，以理解中子星内部几何结构特性及其与状态方程、恒星参数的关系。

Method: 使用满足核理论、微扰QCD约束以及中子星质量、半径、双中子星合并引力波观测约束的大规模状态方程集合，构建中子星模型，分析其曲率不变量行为。

Result: 发现负Ricci标量曲率在中子星中相当普遍，约50%的状态方程会产生内部某处曲率为负的中子星，主要出现在高密度、高压力区域，且与刚硬状态方程及致密大质量星相关。改进了引力质量与重子质量之间的准普适关系，精度达3%。

Conclusion: 中子星内部负曲率现象比预期更普遍，这对理解中子星内部几何结构和物质状态有重要意义。改进的质量关系为相关研究提供了更精确的工具。

Abstract: We investigate the behavior of curvature invariants for a large ensemble of neutron stars built with equations of state (EOSs) that satisfy constraints from nuclear theory and perturbative QCD, as well as measurements of neutron-star masses, radii, and gravitational waves from binary neutron-star mergers. Surprisingly, our analysis reveals that stars with negative Ricci scalar $\mathcal{R}$ are rather common and about $\sim 50\%$ of our EOSs produce one or more stars with Ricci curvature that is negative somewhere inside the star. The negative curvature is found mostly but not exclusively at the highest densities and pressures, and predominantly for stiff EOSs and for the most compact and most massive stars. Furthermore, we improve the quasi-universal relation between the stellar gravitational mass $M$ and the baryonic mass $M_\mathrm{b}$, which allows us to express analytically one in terms of the other with a maximum variance of only $\sim 3\%$. Finally, using the relation between the Ricci scalar and the trace anomaly $Δ$, we determine the conditions under which $Δ$ vanishes or becomes negative in neutron stars.

</details>


### [5] [Generalized junction conditions for discontinuous metrics](https://arxiv.org/abs/2601.07936)
*J. A. Silva,F. C. Carvalho,Antonio R. G. Garcia*

Main category: gr-qc

TL;DR: 将Darmois-Israel连接形式推广到Colombeau广义函数框架下的不连续度量，提供处理奇异量非线性运算的数学一致方法


<details>
  <summary>Details</summary>
Motivation: 传统Darmois-Israel连接条件要求度量连续，无法处理真正的不连续度量情况。需要数学上一致的方法来处理涉及奇异量的非线性运算（如分布函数的乘积和导数）

Method: 在Colombeau广义函数代数框架下扩展Darmois-Israel连接形式，放宽度量连续性条件，自然包含曲率和表面能量-动量张量中的高阶奇异项

Result: 广义连接条件包含了与时空几何真正不连续性相关的新几何自由度，恢复了传统Darmois-Israel条件作为极限情况，为几何边界和时空突变提供了连贯扩展

Conclusion: Colombeau广义函数框架为处理不连续度量提供了数学上一致的方法，扩展了Darmois-Israel连接形式，使其能够处理更一般的时空不连续性情况

Abstract: In this work, the Darmois-Israel junction formalism is extended to the case of discontinuous metrics within the framework of Colombeau algebras of generalized functions. This formulation provides a mathematically consistent treatment of nonlinear operations involving singular quantities, such as products and derivatives of distributions. By relaxing the usual continuity condition on the metric, the generalized junction conditions naturally include higher-order singular terms in the curvature and in the surface energy-momentum tensor. These additional contributions represent new geometric degrees of freedom associated with genuine discontinuities in the space-time geometry. The resulting formalism recovers the traditional Darmois-Israel conditions as a limiting case, while offering a coherent extension applicable to geometric boundaries and abrupt transitions in space-time.

</details>


### [6] [A natural explanation of the Galactic Magnetic Fields from multistate Scalar Field Dark Matter](https://arxiv.org/abs/2601.08197)
*Maribel Hernández-Márquez,Bryan Mendoza-Meza,Tonatiuh Matos,Tula Bernal,Miguel Alcubierre*

Main category: gr-qc

TL;DR: 带电多态标量场暗物质晕可以自然地产生星系中观测到的微高斯级大尺度磁场，同时保持暗物质分布不变，为星系磁场和卫星星系各向异性分布提供了统一解释框架。


<details>
  <summary>Details</summary>
Motivation: 研究星系中观测到的微高斯级大尺度磁场是否能够自然地由带电标量场暗物质晕产生，同时扩展之前关于多态标量场暗物质形成"引力原子"解释卫星星系各向异性分布的工作。

Method: 在微扰水平上分析标量场和规范场的耦合动力学，求解微扰的Klein-Gordon方程和规范场方程，研究时间演化和空间结构。

Result: 诱导电磁场的空间结构由球贝塞尔函数和球谐函数决定，与多态标量场暗物质晕的基态和激发态特征相同；规范场存在不改变暗物质密度分布；带电多态标量场暗物质晕可以产生相干的大尺度磁场。

Conclusion: 带电多态标量场暗物质晕能够产生形态由标量场激发模式决定的大尺度磁场，为星系磁场和卫星星系各向异性分布提供了统一的量子暗物质起源框架。

Abstract: In this article, we investigate the possibility that the large-scale magnetic fields observed in galaxies, of the order of microgauss, arise naturally from a complex Scalar Field Dark Matter (SFDM) halo charged under a local $U(1)$ symmetry. Extending our previous work, where multistate SFDM solutions were shown to form ``gravitational atoms'' capable of explaining the anisotropic distribution of satellite galaxies (VPOS), we analyze here the coupled dynamics of the scalar and a gauge field at the perturbative level. By solving the perturbed Klein-Gordon and gauge-field equations, we find the temporal evolution and show that the spatial structure of the induced electromagnetic fields is governed by the same spherical Bessel functions and spherical harmonics that characterize the ground and excited states of the multi-state SFDM halo. Remarkably, the presence of the gauge field does not modify the dark-matter density distribution, which preserves the multi-state configuration previously obtained. Our results demonstrate that a charged multi-state SFDM halo can generate coherent, large-scale magnetic fields whose morphology is determined by the excited modes of the scalar field, providing a unified framework in which both galactic magnetic fields and VPOS-like structures originate from the underlying quantum nature of dark matter.

</details>


### [7] [Ultraviolet Behavior of the Wheeler-DeWitt Equation in Horava-Lifshitz Gravity](https://arxiv.org/abs/2601.08222)
*Takamasa Kanai*

Main category: gr-qc

TL;DR: 在Horava-Lifshitz引力中，黑洞内部的量子结构研究表明，紫外区域的高阶空间曲率项和跑动标度参数会抑制"湮灭至无"行为，为量子引力中的奇点消解提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 研究Horava-Lifshitz引力中黑洞内部的量子结构，特别是在紫外区域，探索量子引力如何解决经典奇点问题，并检验广义相对论中提出的"湮灭至无"场景是否在该理论中出现。

Method: 在迷你超空间中分析Wheeler-DeWitt方程，聚焦于紫外区域（高阶空间曲率项主导），推导原始Horava-Lifshitz作用量及其解析延拓对应物的解析解。研究波函数在事件视界和经典奇点附近的行为，考虑球面、平面和双曲二维空间截面，以及正、负和零宇宙学常数模型。

Result: 在所有考虑的情况下（不同空间几何和宇宙学常数），紫外区域主导的项与跑动标度参数效应共同作用，抑制了"湮灭至无"行为。这表明在本研究范围内，Horava-Lifshitz引力的紫外区域不出现特征性的"湮灭至无"行为。

Conclusion: Horava-Lifshitz引力中的紫外区域量子效应抑制了广义相对论中提出的"湮灭至无"行为，为理解量子引力中的奇点消解提供了新的视角，表明该理论可能以不同于广义相对论的方式解决奇点问题。

Abstract: We investigate the quantum structure of black hole interiors in Horava-Lifshitz gravity by analyzing the Wheeler-DeWitt equation in minisuperspace. Focusing on the ultraviolet regime, where higher-order spatial curvature terms dominate, we derive analytical solutions in this UV limit for both the original Horava-Lifshitz action and its analytically continued counterpart. We study their behavior near the event horizon and the classical singularity, with particular attention to the interpretation of the wave function in terms of the annihilation-to-nothing scenario proposed in general relativity. In this paper, we have considered cases in which the two-dimensional spatial section is spherical, planar, or hyperbolic, as well as models with positive, negative, or vanishing cosmological constant. In all cases, we find that the terms dominating in the ultraviolet regime, together with the effects of the running scaling parameter, act to suppress the annihilation-to-nothing behavior. These results suggest that, at least within the range explored in this study, the characteristic annihilation-to-nothing behavior does not appear in the ultraviolet regime of Horava-Lifshitz gravity, and provide a new perspective on the understanding of singularity resolution in quantum gravity.

</details>


### [8] [An Explicit Kaluza-Klein Reduction of Einstein's Gravity in $6D$ on $S^2$](https://arxiv.org/abs/2601.08443)
*Tekin Dereli,Yorgo Senikoglu*

Main category: gr-qc

TL;DR: 六维Kaluza-Klein理论在M4×S2拓扑上，通过维数约化得到规范场，但SO(3)对称性下只有两个物理规范场传播，规范动能矩阵秩为2，有一个零特征值。


<details>
  <summary>Details</summary>
Motivation: 研究六维Kaluza-Klein理论在球面S2上维数约化的规范场结构，澄清规范自由度的几何起源，特别是理解为什么SO(3)等距对称性下只有两个物理规范场传播。

Method: 使用S2上的归一化Killing矢量进行维数约化，显式构造约化的Yang-Mills作用量，确定相应的规范动能矩阵，分析其秩和特征值结构。

Result: 尽管S2具有SO(3)等距对称性，但只有两个物理规范场在四维传播，规范动能矩阵秩为2且有一个零特征值，这源于S2≃SO(3)/SO(2)的陪集结构。

Conclusion: 规范动能矩阵的简并性反映了非动力学规范方向而非约化的不一致性，阐明了在陪集空间上进行Kaluza-Klein约化时规范自由度的几何起源。

Abstract: We study a six-dimensional Kaluza-Klein theory with spacetime topology $M_4 \times S^2$ and analyze the gauge sector arising from dimensional reduction. Using normalized Killing vectors on $S^2$, we explicitly construct the reduced Yang-Mills action and determine the corresponding gauge kinetic matrix. Despite the $SO(3)$ isometry of $S^2$, we show that only two physical gauge fields propagate in four dimensions. The gauge kinetic matrix therefore has rank two and possesses a single zero eigenvalue. We demonstrate that this degeneracy is a direct consequence of the coset structure $S^2 \simeq SO(3)/SO(2)$ and reflects a non-dynamical gauge direction rather than an inconsistency of the reduction. Our results clarify the geometric origin of gauge degrees of freedom in Kaluza-Klein reductions on coset spaces.

</details>


### [9] [Geometry is Wavy: Curvature Wave Equations for Generic Affine Connections](https://arxiv.org/abs/2601.08501)
*Emel Altas,Bayram Tekin*

Main category: gr-qc

TL;DR: 论文将黎曼几何中的曲率波动方程推广到具有挠率和非度量性的仿射联络时空，在度量-仿射框架下推导了黎曼张量的波动方程并分析了其在多种几何和物理背景下的结构。


<details>
  <summary>Details</summary>
Motivation: 在黎曼几何中，曲率张量满足协变的拟线性波动方程，但这一结果仅适用于Levi-Civita联络。研究旨在将这一波动方程推广到更一般的具有挠率和非度量性的仿射联络时空，以涵盖更广泛的几何和物理理论框架。

Method: 在度量-仿射框架下工作，推导具有挠率和非度量性的仿射联络对应的黎曼张量波动方程。通过分析比安基恒等式中的二次曲率项，研究方程在不同几何和物理背景下的结构，包括爱因斯坦空间、远平行引力和爱因斯坦-嘉当理论。

Result: 成功推导了适用于一般仿射联络（包含挠率和非度量性）的曲率波动方程。分析了该方程在多种特殊几何和物理理论中的具体形式和结构特征，展示了原始黎曼几何结果的推广和一般化。

Conclusion: 曲率波动方程可以推广到具有挠率和非度量性的仿射联络时空，这一结果为研究更广泛的几何结构和物理理论提供了数学工具，特别是在爱因斯坦空间、远平行引力和爱因斯坦-嘉当理论等物理相关背景中具有应用价值。

Abstract: Geometry is wavy: even at the purely geometric level (no particular theory chosen), curvature satisfies a covariant quasilinear wave equation. In Riemannian geometry equipped with the Levi-Civita connection, the Riemann curvature tensor obeys a wave equation of the schematic form \[ \Box Riem=\mathcal{Q}(Riem,Riem), \] where $\mathcal{Q}(Riem,Riem)$ denotes the terms quadratic in the curvature arising from the Bianchi identities. In this work, we generalize this curvature wave equation to spacetimes endowed with a generic affine connection possessing torsion and nonmetricity. Working within the metric-affine framework, we derive the corresponding wave equation for the Riemann tensor and analyze its structure in several geometrically and physically distinguished settings, including Einstein spaces, teleparallel gravity, and Einstein-Cartan theory.

</details>


### [10] [Magnetized dynamical black holes](https://arxiv.org/abs/2601.08628)
*Jibril Ben Achour,Adolfo Cisterna,Amaro Díaz,Keanu Müller*

Main category: gr-qc

TL;DR: 该论文构建了一个爱因斯坦-标量-麦克斯韦方程组的精确解，描述了一个浸没在外部时变电磁场中的动态黑洞，结合了动态宇宙学背景和电磁场非微扰反作用。


<details>
  <summary>Details</summary>
Motivation: 需要更现实的解析黑洞模型，现有解常忽略两个关键因素：完全动态的宇宙学背景和外部电磁场的非微扰反作用。

Method: 通过径向和时间依赖的标量场修饰史瓦西黑洞，在Fonarev框架内得到Fisher-Janis-Newman-Winicour解的时变推广；利用爱因斯坦-标量-麦克斯韦系统的李点对称性生成外部电磁场，将Harrison变换推广到动态情形。

Result: 得到结合球对称动态视界与轴对称电磁场的时空，具有混合FLRW和Levi-Civita几何的丰富渐近结构；时间依赖性在遮蔽曲率奇点方面起关键作用，否则在静态极限下这些奇点通常是裸奇点。

Conclusion: 该解为研究动态黑洞与外部电磁场相互作用提供了新框架，对原初黑洞和天体物理应用有潜在意义，并可推广到更高维度。

Abstract: We construct a novel exact solution of the Einstein-scalar-Maxwell equations describing a dynamical black hole immersed in an external, time-dependent electromagnetic field. Motivated by the need for more realistic analytical black hole models, our construction incorporates two key ingredients often neglected in exact solutions: a fully dynamical cosmological background and the non-perturbative backreaction of external electromagnetic fields. The compact object is obtained by dressing a Schwarzschild black hole with a radially and temporally dependent scalar field, yielding a time-dependent generalization of the Fisher-Janis-Newman-Winicour solution within the Fonarev framework. The external electromagnetic field is generated via a Lie point symmetry of the Einstein-scalar-Maxwell system, which exports the effect of a Harrison transformation to dynamical settings provided a spacelike Killing vector is present. The resulting spacetime combines a spherically symmetric dynamical horizon with an axisymmetric electromagnetic field and exhibits a rich asymptotic structure mixing Friedmann-Lemaître-Robertson-Walker and Levi-Civita geometries. We show that the time dependence of the configuration plays a crucial role in potentially cloaking curvature singularities, which would otherwise be generically naked in the stationary limit. We analyze the geometric and physical properties of the solution, including its asymptotic behavior, algebraic classification, and the structure of trapped surfaces defining the dynamical horizon. Possible implications for primordial black holes and some astrophysical applications, as well as extensions to higher dimensions, are also discussed.

</details>


### [11] [Frolov Black Hole Surrounded by a Cloud of Strings](https://arxiv.org/abs/2601.08737)
*F. F. Nascimento,J. C. Rocha,V. B. Bezerra,J. M. Toledo*

Main category: gr-qc

TL;DR: 研究带弦云的Frolov黑洞时空度规，分析弦云对解的正则性、能量条件、测地线、有效势和热力学性质的影响，并与原始Frolov黑洞比较


<details>
  <summary>Details</summary>
Motivation: 研究弦云对Frolov黑洞时空结构的影响，探索弦云如何改变黑洞的几何性质、能量条件和热力学行为

Method: 推导带弦云的Frolov黑洞度规，分析正则性和能量条件，研究测地线方程和有效势，计算热力学量并进行比较分析

Result: 获得了带弦云的Frolov黑洞度规，发现弦云影响时空的正则性和能量条件，改变了测地线行为和有效势，并对热力学性质产生显著影响

Conclusion: 弦云显著改变了Frolov黑洞的性质，包括几何结构、能量条件和热力学行为，为理解弦云对黑洞物理的影响提供了新见解

Abstract: We obtain the metric which describes the spacetime corresponding to the Frolov black hole in the presence of a cloud of strings and discuss how this cloud affects the regularity of the solution and the energy conditions. In addition, we analyze geodesics, effective potential, and several thermodynamic aspects. Finally, we compare our results with the corresponding findings in the literature for the original Frolov black hole, that is, in the absence of a cloud of strings

</details>


### [12] [Condensation of area quanta ensembles with quantum statistics in Schwarzschild spacetimes](https://arxiv.org/abs/2601.08788)
*Ryley McGovern,Seth Major,Trevor Scheuing,Thomas Takis*

Main category: gr-qc

TL;DR: 论文基于近视界观测者的准局域能量和满足量子统计的面积量子，建立了施瓦西黑洞几何的统计力学描述，发现多个相态，其中新的凝聚态在大面积极限下占优，为近视界几何涨落的量子化提供了框架。


<details>
  <summary>Details</summary>
Motivation: 利用球对称黑洞时空中近视界（高加速度）观测者的简单准局域能量形式，结合满足量子统计的面积量子，为施瓦西黑洞几何建立统计力学描述，探索黑洞熵的微观起源。

Method: 使用近视界观测者的准局域能量和满足量子统计的面积量子，构建施瓦西黑洞几何的统计力学模型，分析不同相态（高激发态、玻色-爱因斯坦凝聚、特殊凝聚态、简并费米气体）及其熵特性。

Result: 模型包含多个相态，在大面积极限下（与Bekenstein-Hawking熵相关），新的凝聚态比玻色-爱因斯坦凝聚和简并费米气体更占优；计算了各相态的熵和混合熵；低熵凝聚态为近视界几何涨落的量子化提供了框架。

Conclusion: 基于准局域能量和面积量子的统计力学方法成功描述了施瓦西黑洞的微观结构，发现新的凝聚态在大面积极限下占主导地位，这为黑洞熵的微观解释和近视界几何涨落的量子化研究奠定了基础。

Abstract: As is well known, near-horizon (equivalently high acceleration) observers in spherically symmetric black hole spacetimes have a particularly simple form of the quasi-local energy. Using this energy and indistinguishable area quanta satisfying quantum statistics a statistical mechanical description of the Schwarzschild black hole geometry for uniformly accelerating observers is developed. The resulting model has several phases including one with highly excited states, Bose-Einstein condensates, condensates distinct from the usual Bose gas, and degenerate Fermi gases. In the large area limit, relevant for comparison to the Bekenstein-Hawking entropy, the new condensed state is favored over Bose-Einstein condensation and the degenerate Fermi gas. The entropies of the phases, and the entropy of mixing, are computed. The resulting low-entropic condensed state, where the quanta are essentially all in the lowest Bose energy state, provides the framework for the quantization of near-horizon geometric fluctuations, which is explored in a companion paper.

</details>


### [13] [Black hole entropy from the quantum atmosphere of bound gravitational fluctuations](https://arxiv.org/abs/2601.08794)
*Seth Major,Daniel Rodriguez,Thomas Takis*

Main category: gr-qc

TL;DR: 黑洞熵被解释为通过霍金-安鲁过程不断产生的被困引力模式动态自由度的计数，这些模式代表视界附近的几何形状涨落。


<details>
  <summary>Details</summary>
Motivation: 传统黑洞熵解释存在挑战，本文试图从动力学角度理解黑洞熵的本质，将其与视界附近不断产生的引力模式联系起来，这些模式由霍金-安鲁过程持续激发。

Method: 在线性扰动史瓦西时空背景下，利用Regge-Wheeler-Zerilli方程解空间的态正交性推导态密度；通过普朗克尺度的截断处理发散的能量和熵；分析被困模式的热分布及其对时空质量的影响。

Result: 被困引力模式的热分布存储了时空质量的显著部分；模式特征频率约为100 Hz（太阳质量黑洞）；这些模式虽然无法在~3M外直接观测，但会影响黑洞附近的零射线传播。

Conclusion: 黑洞熵可解释为视界附近被困引力模式的动态自由度计数，这些模式由霍金-安鲁过程持续产生，代表几何形状涨落，可能通过影响光线传播而被间接观测。

Abstract: Black hole entropy is identified with the counting of the dynamical degrees of freedom of trapped gravitational modes continually sourced by the Hawking-Unruh process. In the context of linear perturbations of Schwarzschild spacetime the density of states is derived from the orthogonality of states in the solution space of the Regge-Wheeler-Zerilli equation. The otherwise divergent energy and entropy is cutoff by the Planck scale closest approach of constantly accelerating observers near the horizon. The thermal distribution of the trapped modes, which represent shape fluctuations in the near horizon geometry, store a significant fraction of the spacetime mass as observed from far away. Unlike quasi-normal modes the modes are not directly observable outside of $\sim 3 M$ but, being external to the horizon, they affect the propagation of null rays near the black hole. The characteristic frequencies, around 100 Hz for solar mass black holes, are discussed in relation to possible observations.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [14] [LOTUS: Layer-ordered Temporally Unified Schedules For Quantum Approximate Optimization Algorithms](https://arxiv.org/abs/2601.07851)
*Phuong-Nam Nguyen*

Main category: quant-ph

TL;DR: LOTUS框架通过将QAOA从高维混沌搜索重构为低维动力系统，使用混合傅里叶自回归映射替代独立层角度，实现了全局时间一致性和局部灵活性，显著提升了优化性能并大幅降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 标准QAOA优化面临高维搜索空间和混沌行为的问题，导致优化困难且计算成本高昂。需要一种方法将QAOA重构为更易处理的低维动力系统。

Method: 提出LOTUS框架，使用混合傅里叶自回归映射替代传统的独立层角度参数化。该方法强制全局时间一致性同时保持局部灵活性，将QAOA从高维搜索转变为低维动力系统。

Result: LOTUS在期望值上比L-BFGS-B提升27.2%，比COBYLA提升20.8%。计算成本大幅降低，比Powell或SLSQP等方法减少90%以上的迭代次数。

Conclusion: LOTUS框架通过将QAOA重构为低维动力系统，显著提升了优化性能和计算效率，为量子优化算法提供了更有效的参数化方法。

Abstract: In this paper, we introduce LOTUS (Layer-Ordered Temporally-Unified Schedules), which is a framework that restructures QAOA from a high-dimensional, chaotic search into a low-dimensional dynamical system. By replacing independent layer-wise angles with a Hybrid Fourier-Autoregressive (HFA) mapping, LOTUS enforces global temporal coherence while maintaining local flexibility. LOTUS consistently outperforms standard optimizers, achieving up to a $27.2\%$ improvement in expectation values over L-BFGS-B and $20.8\%$ compared with COBYLA. Besides, our proposed method drastically reduces computational costs, requiring over $90\%$ fewer iterations than methods like Powell or SLSQP.

</details>


### [15] [Feature Entanglement-based Quantum Multimodal Fusion Neural Network](https://arxiv.org/abs/2601.07856)
*Yu Wu,Qianli Zhou,Jie Geng,Xinyang Deng,Wen Jiang*

Main category: quant-ph

TL;DR: 提出基于量子计算的量子多模态融合神经网络，通过特征纠缠解决传统多模态学习在准确性、可解释性和复杂度之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在多模态学习中面临关键权衡：黑盒特征级融合准确性高但可解释性差，决策级融合可解释性好但准确性不足，同时存在参数爆炸和复杂度高的挑战。

Method: 提出特征纠缠的量子多模态融合神经网络，包含三个核心组件：经典前馈模块处理单模态数据、可解释的量子融合块、量子卷积神经网络进行深度特征提取。

Result: 模拟结果显示，该模型在分类准确率上可与参数数量数十倍的传统网络相媲美，在多模态图像数据集上表现出显著的稳定性和性能。

Conclusion: 量子计算框架下的多模态融合方法能够同时实现高准确性、良好可解释性和线性复杂度，解决了传统方法的权衡困境。

Abstract: Multimodal learning aims to enhance perceptual and decision-making capabilities by integrating information from diverse sources. However, classical deep learning approaches face a critical trade-off between the high accuracy of black-box feature-level fusion and the interpretability of less outstanding decision-level fusion, alongside the challenges of parameter explosion and complexity. This paper discusses the accuracy-interpretablity-complexity dilemma under the quantum computation framework and propose a feature entanglement-based quantum multimodal fusion neural network. The model is composed of three core components: a classical feed-forward module for unimodal processing, an interpretable quantum fusion block, and a quantum convolutional neural network (QCNN) for deep feature extraction. By leveraging the strong expressive power of quantum, we have reduced the complexity of multimodal fusion and post-processing to linear, and the fusion process also possesses the interpretability of decision-level fusion. The simulation results demonstrate that our model achieves classification accuracy comparable to classical networks with dozens of times of parameters, exhibiting notable stability and performance across multimodal image datasets.

</details>


### [16] [Fault-Tolerant Quantum Error Correction: Implementing Hamming-Based Codes with Advanced Syndrome Extraction Techniques](https://arxiv.org/abs/2601.07860)
*Soham Bhadra,Diyansha Singh,Angana Chowdhury*

Main category: quant-ph

TL;DR: 该论文比较了三种量子纠错码的综合征提取策略，展示了智能辅助量子比特管理如何将错误抑制提升2.4倍，在物理错误率10^-3下实现5.1×10^-5的逻辑错误率，为近期量子设备提供了实用工具。


<details>
  <summary>Details</summary>
Motivation: 量子计算机需要保护脆弱的量子态免受环境噪声和操作错误的影响。虽然Steane [7,1,3]码等量子纠错码提供了理论解决方案，但其实际成功关键取决于如何测量错误（综合征提取）。主要挑战在于用于测量的辅助量子比特：当它们失败时，错误会级联传播，破坏我们试图保护的信息。

Method: 实现并比较了三种先进的综合征测量策略：1) Shor的猫态方法，通过多个纠缠辅助量子比特分布测量，达到85-92%的制备成功率；2) Steane的编码辅助方法，使用完全纠错的逻辑量子比特，达到97.8%的综合征保真度；3) 基于硬件能力自适应策略的灵活统一框架。使用IBM的Qiskit平台进行广泛模拟，包括随机基准测试和T-heavy电路。

Result: 智能辅助量子比特管理将错误抑制提升高达2.4倍。在物理错误率10^-3的现实噪声条件下，实现了低至5.1×10^-5的逻辑错误率，即使对于深度电路也能保持接近1的逻辑保真度（0.99997）。阈值分析显示在距离3到距离13的代码中具有鲁棒性能，特征阈值曲线显示在临界物理错误率以下呈指数级错误抑制。

Conclusion: 该研究为近期量子设备提供了可直接部署的工具，并为向容错量子计算机扩展建立了实用的设计原则。结果表明，通过智能的辅助量子比特管理策略，可以显著提高量子纠错的实际效果，为实现可扩展的量子计算铺平道路。

Abstract: Building reliable quantum computers requires protecting fragile quantum states from inevitable environmental noise and operational errors. While quantum error correction codes like the Steane $[\![7,1,3]\!]$ code provide elegant theoretical solutions, their practical success hinges critically on how we measure errors - a process called syndrome extraction. The challenge lies in the ancilla qubits used for measurement: when they fail, errors can cascade across the entire quantum system, destroying the very information we're trying to protect. We address this fundamental problem by implementing and comparing three sophisticated syndrome measurement strategies: Shor's cat-state approach, which distributes measurements across multiple entangled ancillas achieving 85-92% preparation success; Steane's encoded-ancilla method using complete error-corrected logical qubits reaching 97.8% syndrome fidelity; and a flexible unified framework that adapts strategies based on hardware capabilities. Through extensive simulations using IBM's Qiskit platform spanning randomized benchmarking and T-heavy circuits, we demonstrate that intelligent ancilla management improves error suppression by up to 2.4$\times$ compared to standard approaches. Our implementations achieve logical error rates as low as $5.1 \times 10^{-5}$ under realistic noise conditions with physical error rates of $10^{-3}$, while maintaining near-unity logical fidelity (0.99997) even for deep circuits. The threshold analysis reveals robust performance across distance-3 to distance-13 codes with characteristic threshold curves showing exponential error suppression below the critical physical error rate. These results provide immediately deployable tools for near-term quantum devices and establish practical design principles for scaling toward fault-tolerant quantum computers.

</details>


### [17] [Quantum Computing and Visualization Research Challenges and Opportunities](https://arxiv.org/abs/2601.07872)
*E. Wes Bethel,Roel Van Beeumen,Talita Perciano*

Main category: quant-ph

TL;DR: 本文从可视化领域视角，探讨量子计算从可行性验证到实际应用的研究挑战与机遇


<details>
  <summary>Details</summary>
Motivation: 量子计算近年来快速发展，随着编程环境、软件模拟器和云硬件平台的成熟，以及实际应用需求的增长，需要从可视化角度探索如何利用这一新兴技术解决实际问题

Method: 从可视化领域视角，分析量子计算平台从初始可行性到实际应用的研究路径

Result: 识别了量子计算在可视化应用中的研究挑战和机遇

Conclusion: 量子计算为可视化领域带来了新的研究方向和实际应用机会，需要探索从可行性验证到解决实际问题的完整路径

Abstract: Quantum computing (QC) has experienced rapid growth in recent years with the advent of robust programming environments, readily accessible software simulators and cloud-based QC hardware platforms, and growing interest in learning how to design useful methods that leverage this emerging technology for practical applications. From the perspective of the field of visualization, this article examines research challenges and opportunities along the path from initial feasibility to practical use of QC platforms applied to meaningful problems.

</details>


### [18] [Tackling Heterogeneity in Quantum Federated Learning: An Integrated Sporadic-Personalized Approach](https://arxiv.org/abs/2601.07882)
*Ratun Rahman,Shaba Shaon,Dinh C. Nguyen*

Main category: quant-ph

TL;DR: SPQFL：一种集成式稀疏个性化量子联邦学习方法，同时处理量子噪声异质性和数据异质性，通过稀疏学习应对量子设备噪声差异，通过个性化正则化处理非IID数据分布。


<details>
  <summary>Details</summary>
Motivation: 现有量子联邦学习框架面临两个主要挑战：1) 量子噪声异质性 - 不同量子设备因质量和退相干敏感性不同而具有不同噪声水平；2) 数据异质性 - 参与设备的数据分布通常是非独立同分布的。这些因素导致模型训练性能不佳。

Method: 提出SPQFL框架，包含两个关键方面：1) 针对量子噪声异质性，引入稀疏学习概念处理不同量子设备的噪声差异；2) 针对数据异质性，通过模型正则化实现个性化学习，缓解非IID数据分布下的过拟合问题，提升全局模型收敛性。

Result: 理论收敛分析表明SPQFL算法的上界受量子设备数量和量子噪声测量次数影响。在真实数据集上的仿真结果显示，相比现有方法，SPQFL在训练性能和收敛稳定性方面有显著提升。

Conclusion: SPQFL成功解决了量子联邦学习中的噪声异质性和数据异质性挑战，通过集成稀疏学习和个性化方法，显著提升了模型训练性能和收敛稳定性，为量子联邦学习提供了有效的解决方案。

Abstract: Quantum federated learning (QFL) emerges as a powerful technique that combines quantum computing with federated learning to efficiently process complex data across distributed quantum devices while ensuring data privacy in quantum networks. Despite recent research efforts, existing QFL frameworks struggle to achieve optimal model training performance primarily due to inherent heterogeneity in terms of (i) quantum noise where current quantum devices are subject to varying levels of noise due to varying device quality and susceptibility to quantum decoherence, and (ii) heterogeneous data distributions where data across participating quantum devices are naturally non-independent and identically distributed (non-IID). To address these challenges, we propose a novel integrated sporadic-personalized approach called SPQFL that simultaneously handles quantum noise and data heterogeneity in a single QFL framework. It is featured in two key aspects: (i) for quantum noise heterogeneity, we introduce a notion of sporadic learning to tackle quantum noise heterogeneity across quantum devices, and (ii) for quantum data heterogeneity, we implement personalized learning through model regularization to mitigate overfitting during local training on non-IID quantum data distributions, thereby enhancing the convergence of the global model. Moreover, we conduct a rigorous convergence analysis for the proposed SPQFL framework, with both sporadic and personalized learning considerations. Theoretical findings reveal that the upper bound of the SPQFL algorithm is strongly influenced by both the number of quantum devices and the number of quantum noise measurements. Extensive simulation results in real-world datasets also illustrate that the proposed SPQFL approach yields significant improvements in terms of training performance and convergence stability compared to the state-of-the-art methods.

</details>


### [19] [Local Scale Invariance in Quantum Theory: Experimental Predictions](https://arxiv.org/abs/2601.07883)
*Indrajit Sen,Matthew Leifer*

Main category: quant-ph

TL;DR: 该论文探讨了非厄米引导波量子理论中局部尺度不变性的实验预测，包括精细结构常数修正、Aharonov-Bohm实验中轨迹依赖的概率密度，以及光谱频率和强度的历史依赖性。


<details>
  <summary>Details</summary>
Motivation: 研究局部尺度不变的非厄米引导波量子理论的实验预测，解决爱因斯坦对局部尺度不变性的关键反对意见，并展示该理论如何通过实验与其他量子表述区分开来。

Method: 使用Weyl的电荷引力半径定义获得非可积尺度效应的精细结构常数α_S，分析Aharonov-Bohm双缝实验中的轨迹依赖效应，并通过Weyl-Einstein关于第二钟效应的辩论来检验理论。

Result: 发现α_S相对于α极小（~10^{-21}），解释了通常量子实验中效应的隐蔽性；预测Aharonov-Bohm实验中概率密度依赖于粒子轨迹穿过哪个狭缝；光谱频率历史无关但强度历史相关；能量本征值有微小虚数修正影响谱线宽度。

Conclusion: 该理论通过轨迹依赖的概率密度使其在经验上可与其他量子表述区分，为局部尺度不变性提供了实验验证途径，并解决了爱因斯坦的关键反对意见。

Abstract: We explore the experimental predictions of the local scale invariant, non-Hermitian pilot-wave (de Broglie-Bohm) formulation of quantum theory introduced in arXiv:2601.03567. We use Weyl's definition of gravitational radius of charge to obtain the fine-structure constant for non-integrable scale effects $α_S$. The minuteness of $α_S$ relative to $α$ ($α_S/α\sim 10^{-21}$) effectively hides the effects in usual quantum experiments. In an Aharonov-Bohm double-slit experiment, the theory predicts that the position probability density depends on which slit the particle trajectory crosses, due to a non-integrable scale induced by the magnetic flux. This experimental prediction can be realistically tested for an electrically neutral, heavy molecule with mass $m \sim 10^{-19} \text{g}$ at a $\sim 10^6 \text{ esu}$ flux regime. We analyse the Weyl-Einstein debate on the second-clock effect using the theory and show that spectral frequencies are history-independent. We thereby resolve Einstein's key objection against local scale invariance, and obtain two further experimental predictions. First, spectral intensities turn out to be history-dependent. Second, energy eigenvalues are modified by tiny imaginary corrections that modify spectral linewidths. We argue that the trajectory dependence of the probabilities renders our theory empirically distinguishable from other quantum formulations that do not use pilot-wave trajectories, or their mathematical equivalents, to derive experimental predictions.

</details>


### [20] [Quantum circuit compilation for fermionic excitations using the Jordan-Wigner mapping](https://arxiv.org/abs/2601.07890)
*Renata Wong*

Main category: quant-ph

TL;DR: 该论文详细介绍了UCCSD ansatz的Jordan-Wigner映射，以氢分子为案例，推导了单双激发的Pauli字符串，并讨论了量子电路实现中的细节差异。


<details>
  <summary>Details</summary>
Motivation: 弥合理论二次量子化与实用量子硬件之间的差距，为量子化学计算提供具体的实现指导。

Method: 使用Jordan-Wigner映射将UCCSD ansatz转换为量子可执行的Pauli字符串，以氢分子最小基组为案例研究，详细推导单双激发算符。

Result: 明确推导出氢分子UCCSD所需的Pauli字符串，并讨论了数学旋转与物理门（如SX门）在量子电路实现中的差异。

Conclusion: 该工作为量子化学模拟提供了从理论到硬件实现的具体桥梁，特别强调了实际量子电路实现中的技术细节。

Abstract: This note bridges the gap between theoretical second quantization and practical quantum hardware by detailing the Jordan-Wigner mapping for the Unitary Coupled Cluster Singles and Doubles (UCCSD) ansatz. Using the hydrogen molecule in a minimal basis as a case study, we explicitly derive the Pauli strings required for single and double excitations. Additionally, we discuss the translation of these operators into quantum circuits, with a focus on implementation nuances such as the difference between mathematical rotations and physical gates like the $\sqrt{X}$ (SX) gate.

</details>


### [21] [Bohmian mechanics: A legitimate hydrodynamic picture for quantum mechanics, and beyond](https://arxiv.org/abs/2601.07932)
*A. S. Sanz*

Main category: quant-ph

TL;DR: 玻姆力学从备受争议的隐变量理论演变为实用的分析计算工具，逐渐获得学术界的接受


<details>
  <summary>Details</summary>
Motivation: 探讨玻姆力学如何从最初备受争议的隐变量理论，演变为被更广泛接受的实用量子力学工具，并评估其作为合法量子表示的地位

Method: 采用比标准量子力学更少限制的视角，重新审视薛定谔方程和多个具体数值例子，分析玻姆力学的演变过程

Result: 玻姆力学已从隐变量框架转向更实用的分析方法，在解决物理各领域具体问题中证明其价值，逐渐获得学术界的接受

Conclusion: 玻姆力学应被视为合法的量子表示，值得在基础量子力学课程中教授，其思想方法也可推广到其他领域

Abstract: Since its inception, Bohmian mechanics has been surrounded by a halo of controversy. Originally proposed to bypass the limitations imposed by von Neumann's theorem on the impossibility of hidden-variable models in quantum mechanics, it faced strong opposition from the outset. Over time, however, its use in tackling specific problems across various branches of physics has led to a gradual shift in attitude, turning the early resistance into a more moderate acceptance. A plausible explanation for this change may be that, since the late 1990s and early 2000s, Bohmian mechanics has been taking on a more operational and practical role. The original hidden-variable idea has gradually faded from its framework, giving way to a more pragmatic approach that treats it as a suitable analytical and computational tool. This discussion explores how and why such a shift in perspective has occurred and, therefore, answers questions such as whether Bohmian mechanics should be considered once and for all a legitimate quantum representation (i.e., worth being taught in elementary quantum mechanics courses) or, by extension, whether these ideas can be transferred to and benefit other fields. Here, the Schrödinger equation and several specific numerical examples are re-examined in the light of a less restrictive view than the standard one usually adopted in quantum mechanics.

</details>


### [22] [Data-driven learning of non-Markovian quantum dynamics](https://arxiv.org/abs/2601.07934)
*Samuel Goodwin,Brian K. McFarland,Manuel H. Muñoz-Arias,Edward C. Tortorici,Melissa C. Revelle,Christopher G. Yale,Daniel S. Lobser,Susan M. Clark,Mohan Sarovar*

Main category: quant-ph

TL;DR: 提出一种数据驱动的量子门表征协议，基于NMZ公式从时间序列数据学习量子演化，能处理非马尔可夫动力学，并在模拟和实验数据中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 容错量子计算需要精确了解和控制量子门应用期间的量子比特动力学。现有量子门表征方法（如门集层析）通常假设马尔可夫噪声，无法充分捕获非马尔可夫动力学，需要更全面的噪声源诊断方法。

Method: 开发基于Nakajima-Mori-Zwanzig（NMZ）开放系统动力学公式的数据驱动学习协议，从时间序列数据重建量子演化。该方法学习时间演化生成器（NMZ算子），能够捕获非马尔可夫动力学。在三个系统上验证：纯马尔可夫动力学的量子比特模拟、受Ornstein-Uhlenbeck过程随机噪声驱动的量子比特模拟、以及未预先表征噪声环境的囚禁离子实验数据。

Result: 该技术在所有三种情况下都能成功学习时间演化生成器（NMZ算子），并能确定量子比特动力学无法用纯马尔可夫模型准确描述的时间尺度。方法能够明确捕获门生成器中的非马尔可夫性，从而更彻底地诊断噪声源。

Conclusion: 该数据驱动的学习技术补充了现有的量子门表征方法（如门集层析），通过显式捕获门生成器中的非马尔可夫性，实现了更全面的噪声源诊断，为容错量子计算提供了更精确的量子门动力学表征工具。

Abstract: Fault-tolerant quantum computing requires extremely precise knowledge and control of qubit dynamics during the application of a gate. We develop a data-driven learning protocol for characterizing quantum gates that builds off previous work on learning the Nakajima-Mori-Zwanzig (NMZ) formulation of open system dynamics from time series data, which allows detailed reconstruction of quantum evolution, including non-Markovian dynamics. We demonstrate this learning technique on three different systems: a simulation of a qubit whose dynamics are purely Markovian, a simulation of a driven qubit coupled to stochastic noise produced by an Ornstein-Uhlenbeck process, and trapped-ion experimental data of a driven qubit whose noise environment is not characterized ahead of time. Our technique is able to learn the generators of time evolution, or the NMZ operators, in all three cases and can learn the timescale in which the qubit dynamics can no longer be accurately described by a purely Markovian model. Our technique complements existing quantum gate characterization methods such as gate set tomography by explicitly capturing non-Markovianity in the gate generator, thus allowing for more thorough diagnosis of noise sources.

</details>


### [23] [Attention in Krylov Space](https://arxiv.org/abs/2601.07937)
*Zihao Qi,Christopher Earls*

Main category: quant-ph

TL;DR: 使用Transformer模型预测Lanczos系数，相比传统渐近拟合方法显著提升精度，并能跨系统尺寸迁移


<details>
  <summary>Details</summary>
Motivation: 传统计算Lanczos系数的方法存在数值不稳定性和内存成本限制，而渐近拟合方法会忽略历史依赖的次主导结构，影响可观测量重建精度

Method: 将Lanczos系数视为因果时间序列，引入基于Transformer的自回归模型，从短前缀预测未来系数；通过注意力机制分析识别对预测最关键的历史部分

Result: 模型在经典和量子系统中均优于渐近拟合方法，在系数外推和物理可观测量重建方面误差降低一个数量级；能够跨系统尺寸迁移，在小系统训练后可用于大系统外推

Conclusion: Transformer模型为Lanczos系数预测提供了更准确的方法，通过注意力机制揭示了系数历史中最具影响力的部分，为算子增长研究提供了新工具

Abstract: The Universal Operator Growth Hypothesis formulates time evolution of operators through Lanczos coefficients. In practice, however, numerical instability and memory cost limit the number of coefficients that can be computed exactly. In response to these challenges, the standard approach relies on fitting early coefficients to asymptotic forms, but such procedures can miss subleading, history-dependent structures in the coefficients that subsequently affect reconstructed observables. In this work, we treat the Lanczos coefficients as a causal time sequence and introduce a transformer-based model to autoregressively predict future Lanczos coefficients from short prefixes. For both classical and quantum systems, our machine-learning model outperforms asymptotic fits, in both coefficient extrapolation and physical observable reconstruction, by achieving an order-of-magnitude reduction in error. Our model also transfers across system sizes: it can be trained on smaller systems and then be used to extrapolate coefficients on a larger system without retraining. By probing the learned attention patterns and performing targeted attention ablations, we identify which portions of the coefficient history are most influential for accurate forecasts.

</details>


### [24] [Quantum automated theorem proving](https://arxiv.org/abs/2601.07953)
*Zheng-Zhi Sun,Qi Ye,Dong-Ling Deng*

Main category: quant-ph

TL;DR: 提出量子自动定理证明通用框架，利用量子叠加和纠缠特性，在命题逻辑和一阶逻辑中实现查询复杂度二次降低的量子归结推理，并扩展吴方法到量子几何定理证明。


<details>
  <summary>Details</summary>
Motivation: 自动定理证明在人工智能中具有重要作用，但传统方法存在效率限制。量子计算的叠加和纠缠特性可能为自动推理带来潜在优势，特别是在查询复杂度方面。

Method: 提出量子知识表示框架，开发量子归结推理算法用于命题和一阶逻辑，并扩展吴方法的代数几何定理证明到量子领域，建立量子自动定理证明器。

Result: 在命题逻辑和一阶逻辑中实现查询复杂度二次降低的量子归结推理，量子几何定理证明器在国际数学奥林匹克几何问题上展示二次更好的查询复杂度。

Conclusion: 建立了量子自动定理证明的主要方法框架，为近期和未来量子技术的实际应用提供了重要基础，展示了量子计算在自动推理领域的潜力。

Abstract: Automated theorem proving, or more broadly automated reasoning, aims at using computer programs to automatically prove or disprove mathematical theorems and logical statements. It takes on an essential role across a vast array of applications and the quest for enhanced theorem-proving capabilities remains a prominent pursuit in artificial intelligence. Here, we propose a generic framework for quantum automated theorem proving, where the intrinsic quantum superposition and entanglement features would lead to potential advantages. In particular, we introduce quantum representations of knowledge bases and propose corresponding reasoning algorithms for a variety of tasks. We show how automated reasoning can be achieved with quantum resolution in both propositional and first-order logic with quadratically reduced query complexity. In addition, we propose the quantum algebraic proving method for geometric theorems, extending Wu's algebraic approach beyond the classical setting. Through concrete examples, including geometry problems from the International Mathematical Olympiad, we demonstrate how a quantum computer may prove geometric theorems with quadratic better query complexity. Our results establish a primary approach towards building quantum automatic theorem provers, which would be crucial for practical applications of both near-term and future quantum technologies.

</details>


### [25] [Interferometric discrepancy between the Schrödinger and Klein-Gordon wave equations due to their dissimilar phase velocities](https://arxiv.org/abs/2601.08007)
*Frank Victor Kowalski*

Main category: quant-ph

TL;DR: 论文发现当分束器速度超过自由非零质量粒子的相速度时，薛定谔方程预测会出现干涉现象，这在电磁波或非相对论极限下不可能发生。


<details>
  <summary>Details</summary>
Motivation: 探索在分束器速度超过自由非零质量粒子的相速度条件下，薛定谔方程预测的干涉现象，这在传统波动理论中是不可能的。

Method: 分析薛定谔方程在分束器轨迹包含超相速度段时的预测，对比电磁波和非相对论极限下的克莱因-戈登方程，讨论介电和衍射分束器的反射和透射双重行为。

Result: 发现薛定谔方程预测在分束器速度超过自由非零质量粒子的相速度时会出现干涉现象，而电磁波和非相对论极限下的克莱因-戈登方程则不可能出现这种现象。

Conclusion: 薛定谔方程在分束器超相速度条件下预测的干涉现象揭示了量子力学与经典波动理论的重要差异，为理解量子粒子的波粒二象性提供了新视角。

Abstract: The Schrödinger equation predicts interference when a beamsplitter's trajectory includes a segment where its speed exceeds the phase velocity of a free non-zero rest mass particle that is in a momentum eigenstate. Such interference is neither possible for electromagnetic waves nor for eigenstates of momentum in the non-relativistic limit of the Klein-Gordon equation since the speed of the beamsplitter cannot exceed the phase velocity of the wave. The dual behavior of reflection and transmission in this case is discussed for dielectric and diffracting beamsplitters.

</details>


### [26] [Learning Better Error Correction Codes with Hybrid Quantum-Assisted Machine Learning](https://arxiv.org/abs/2601.08014)
*Yariv Yanay*

Main category: quant-ph

TL;DR: 使用混合经典-量子算法（结合强化学习与量子设备）搜索针对特定设备错误和光子损失错误的稳定子码


<details>
  <summary>Details</summary>
Motivation: 量子纠错是数字量子计算的基础模块，需要针对特定量子设备的错误特性设计优化的纠错码

Method: 结合经典强化学习与两台商用量子设备的调用，利用Quantum Lego形式化方法构建稳定子码，针对设备特定错误和诱导光子损失错误进行搜索

Result: 开发了混合经典-量子算法，能够搜索针对特定量子设备错误特性的稳定子纠错码

Conclusion: 该方法将Quantum Lego形式化与混合算法结合，为针对实际量子设备错误特性设计优化纠错码提供了新途径

Abstract: Quantum error correction is one of the fundamental building blocks of digital quantum computation. The Quantum Lego formalism has introduced a systematic way of constructing new stabilizer codes out of basic lego-like building blocks, which in previous work we have used to generate improved error correcting codes via an automated reinforcement learning process. Here, we take this a step further and show the use of a hybrid classical-quantum algorithm. We combine classical reinforcement learning with calls to two commercial quantum devices to search for a stabilizer code to correct errors specific to the device, as well as an induced photon loss error.

</details>


### [27] [On measurement-dependent variance in quantum neural networks](https://arxiv.org/abs/2601.08029)
*Andrey Kardashin,Konstantin Antipin*

Main category: quant-ph

TL;DR: 测量受限支持可观测量会增加回归量子机器学习任务中的标签预测方差


<details>
  <summary>Details</summary>
Motivation: 在量子机器学习中，有时需要在处理后的量子态上测量受限支持的可观测量（如量子卷积神经网络中测量局部可观测量），需要理解这种测量方式对预测性能的影响

Method: 分析测量受限支持可观测量对回归QML任务中标签预测方差的影响，研究可观测量不同特征值数量与预测方差的关系

Result: 测量受限支持可观测量会导致更大的标签预测方差，主要原因在于测量可观测量不同特征值的数量

Conclusion: 在量子机器学习中，测量受限支持的可观测量会增加预测方差，这为设计更有效的量子神经网络提供了重要指导

Abstract: Variational quantum circuits have become a widely used tool for performing quantum machine learning (QML) tasks on labeled quantum states. In some specific tasks or for specific variational ansätze, one may perform measurements on a restricted part of the overall input state. This is the case for, e.g., quantum convolutional neural networks (QCNNs), where after each layer of the circuit a subset of qubits of the processed state is measured or traced out, and at the end of the network one typically measures a local observable. In this work, we demonstrate that measuring observables with restricted support results in larger label prediction variance in regression QML tasks. We show that the reason for this is, essentially, the number of distinct eigenvalues of the observable one measures after the application of a variational circuit.

</details>


### [28] [Quantum Energetic Advantage before Computational Advantage in Boson Sampling](https://arxiv.org/abs/2601.08068)
*Ariane Soret,Nessim Dridi,Stephen C. Wein,Valérian Giesz,Shane Mansfield,Pierre-Emmanuel emeriau*

Main category: quant-ph

TL;DR: 该论文分析了玻色子采样量子计算架构的能耗效率，建立了实验参数、噪声与能量资源之间的定量关系，证明了量子能量优势可以在计算优势出现之前实现。


<details>
  <summary>Details</summary>
Motivation: 理解量子计算机的能量效率对于评估其可扩展性以及确定量子技术是否能在运行时间之外超越经典计算至关重要。玻色子采样是量子优势的典型任务，需要分析其实际能耗。

Method: 使用Metric-Noise-Resource方法，建立实验控制参数、主要噪声过程和能量资源之间的定量联系，通过为玻色子采样定制的性能指标来估计每个样本的能量成本。

Result: 识别了优化能量效率的操作区域，通过比较量子实现与最先进经典实现的能耗，证明了量子能量优势的存在——即使在经典算法仍然更快的区域，量子实现每个样本的能量成本更低。

Conclusion: 量子能量优势可以在计算优势出现之前实现，论文提出了一个实验上可行的玻色子采样架构，包括完整的噪声和损耗预算，使得近期观测量子能量优势成为可能。

Abstract: Understanding the energetic efficiency of quantum computers is essential for assessing their scalability and for determining whether quantum technologies can outperform classical computation beyond runtime alone. In this work, we analyze the energy required to solve the Boson Sampling problem, a paradigmatic task for quantum advantage, using a realistic photonic quantum computing architecture. Using the Metric-Noise-Resource methodology, we establish a quantitative connection between experimental control parameters, dominant noise processes, and energetic resources through a performance metric tailored to Boson Sampling. We estimate the energy cost per sample and identify operating regimes that optimize energetic efficiency. By comparing the energy consumption of quantum and state-of-the-art classical implementations, we demonstrate the existence of a quantum energetic advantage -- defined as a lower energy cost per sample compared to the best-known classical implementation -- that emerges before the onset of computational advantage, even in regimes where classical algorithms remain faster. Finally, we propose an experimentally feasible Boson Sampling architecture, including a complete noise and loss budget, that enables a near-term observation of quantum energetic advantage.

</details>


### [29] [Learning parameter curves in feedback-based quantum optimization algorithms](https://arxiv.org/abs/2601.08085)
*Vicente Peña Pérez,Matthew D. Grace,Christian Arenz,Alicia B. Magann*

Main category: quant-ph

TL;DR: 使用经典机器学习预测反馈量子算法参数曲线，避免量子比特测量开销


<details>
  <summary>Details</summary>
Motivation: 反馈量子算法需要量子比特测量来指导电路更新，测量采样成本高，希望通过机器学习预测参数序列来降低资源开销

Method: 采用师生模型，将MaxCut问题实例映射到FQA参数曲线，通过单次经典推理预测参数序列

Result: 模型能准确预测不同规模问题的FQA参数曲线，包括未见过的规模；预测曲线与参考FQA曲线结果相似，且优于线性量子退火调度

Conclusion: 机器学习为减少量子算法采样成本和资源开销提供了一种启发式实用路径

Abstract: Feedback-based quantum algorithms (FQAs) operate by iteratively growing a quantum circuit to optimize a given task. At each step, feedback from qubit measurements is used to inform the next quantum circuit update. In practice, the sampling cost associated with these measurements can be significant. Here, we ask whether FQA parameter sequences can be predicted using classical machine learning, obviating the need for qubit measurements altogether. To this end, we train a teacher-student model to map a MaxCut problem instance to an associated FQA parameter curve in a single classical inference step. Numerical experiments show that this model can accurately predict FQA parameter curves across a range of problem sizes, including problem sizes not seen during model training. To evaluate performance, we compare the predicted parameter curves in simulation against FQA reference curves and linear quantum annealing schedules. We observe similar results to the former and performance improvements over the latter. These results suggest that machine learning can offer a heuristic, practical path to reducing sampling costs and resource overheads in quantum algorithms.

</details>


### [30] [Quantum observers can communicate across multiverse branches](https://arxiv.org/abs/2601.08102)
*Maria Violaris*

Main category: quant-ph

TL;DR: 该论文展示了在标准量子理论框架内，Everett多世界分支间的通信是可能的，挑战了传统认为这种通信会违反量子理论线性性的观点。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为Everett多世界理论中不同分支的观察者无法通信，否则会违反量子理论的线性性。本文旨在挑战这一传统观念，证明在标准量子理论框架内实现分支间通信的可能性。

Method: 通过设计一个Wigner's-friend思想实验，其中Wigner观察者可以对另一个观察者（朋友）进行量子控制。在实验中，处于叠加态的朋友可以在Wigner的帮助下接收到来自多世界分支中另一个自己的信息。

Result: 成功展示了在标准量子理论框架内实现多世界分支间通信的可能性。为了保持量子理论的幺正性，观察者必须不记得他们发送的信息内容。

Conclusion: 该思想实验挑战了关于Everett多世界理论中可能性的传统认知，并提出了一个令人惊讶的潜在应用：利用知识创造悖论来检验Everett量子理论与单世界理论。

Abstract: It is commonly thought that observers in distinct branches of an Everettian multiverse cannot communicate without violating the linearity of quantum theory. Here we show a counterexample, demonstrating that inter-branch communication is in fact possible, entirely within standard quantum theory. We do this by considering a Wigner's-friend scenario, where an observer (Wigner) can have quantum control over another observer (the friend). We present a thought experiment where the friend in superposition can receive a message written by a distinct copy of themselves in the multiverse, with the aid of Wigner. To maintain the unitarity of quantum theory, the observers must have no memory of the message that they sent. Our thought experiment challenges conventional wisdom regarding the ultimate limits of what is possible in an Everettian multiverse. It has a surprising potential application which involves using knowledge-creation paradoxes for testing Everettian quantum theory against single-world theories.

</details>


### [31] [Cost scaling of MPS and TTNS simulations for 2D and 3D systems with area-law entanglement](https://arxiv.org/abs/2601.08132)
*Thomas Barthel*

Main category: quant-ph

TL;DR: TTNS在降低物理自由度图距离方面优于MPS，但对于大系统D>1维的低能态模拟，MPS反而更高效


<details>
  <summary>Details</summary>
Motivation: 研究树张量网络状态（TTNS）与矩阵乘积状态（MPS）在模拟强关联量子多体系统时的计算效率比较，特别是在D>1空间维度的大系统中

Method: 分析TTNS和MPS在不同边界条件下的计算成本缩放关系，假设系统遵循纠缠（对数）面积定律，即键维度随子系统表面积指数增长

Result: 虽然TTNS能显著降低物理自由度的图距离，但对于D=2和3维的大系统低能态模拟，MPS的计算效率反而高于TTNS

Conclusion: 在D>1维的大系统低能态模拟中，传统的MPS方法比TTNS更高效，尽管TTNS在降低图距离方面有优势

Abstract: Tensor network states are an indispensable tool for the simulation of strongly correlated quantum many-body systems. In recent years, tree tensor network states (TTNS) have been successfully used for two-dimensional systems and to benchmark quantum simulation approaches for condensed matter, nuclear, and particle physics. In comparison to the more traditional approach based on matrix product states (MPS), the graph distance of physical degrees of freedom can be drastically reduced in TTNS. Surprisingly, it turns out that, for large systems in $D>1$ spatial dimensions, MPS simulations of low-energy states are nevertheless more efficient than TTNS simulations. With a focus on $D=2$ and 3, the scaling of computational costs for different boundary conditions is determined under the assumption that the system obeys an entanglement (log-)area law, implying that bond dimensions scale exponentially in the surface area of the associated subsystems.

</details>


### [32] [Dissipative ground-state preparation of a quantum spin chain on a trapped-ion quantum computer](https://arxiv.org/abs/2601.08137)
*Kazuhiro Seki,Yuta Kikuchi,Tomoya Hayata,Seiji Yunoki*

Main category: quant-ph

TL;DR: 提出一种用于量子自旋链基态制备的耗散协议，在离子阱量子计算机上实现，即使存在硬件噪声也能稳定收敛到低能态。


<details>
  <summary>Details</summary>
Motivation: 量子计算中基态制备是一个核心问题，传统方法容易受到噪声影响。需要开发具有内在鲁棒性的协议，能够在实际量子硬件上有效工作。

Method: 推导了Ding等人提出的耗散协议的Kraus表示，该表示适用于任意时间离散化步骤。在Quantinuum的Reimei离子阱量子计算机上实现横向场伊辛链的耗散基态制备，最多19个自旋，使用零噪声外推技术改进结果。

Result: 即使量子电路包含多达4110个纠缠门，协议仍能稳定收敛到远离最大混合态的低能态。通过零噪声外推，能量期望值得到系统改进，与无噪声模拟在统计不确定度内一致。

Conclusion: 该耗散协议具有内在鲁棒性，能够在实际量子硬件上有效制备基态，为噪声中等规模量子设备上的量子模拟提供了有前景的方法。

Abstract: We demonstrate a dissipative protocol for ground-state preparation of a quantum spin chain on a trapped-ion quantum computer. As a first step, we derive a Kraus representation of a dissipation channel for the protocol recently proposed by Ding et al. [Phys. Rev. Res. 6, 033147 (2024)] that still holds for arbitrary temporal discretization steps, extending the analysis beyond the Lindblad dynamics regime. The protocol guarantees that the fidelity with the ground state monotonically increases (or remains unchanged) under repeated applications of the channel to an arbitrary initial state, provided that the ground state is the unique steady state of the dissipation channel. Using this framework, we implement dissipative ground-state preparation of a transverse-field Ising chain for up to 19 spins on the trapped-ion quantum computer Reimei provided by Quantinuum. Despite the presence of hardware noise, the dynamics consistently converges to a low-energy state far away from the maximally mixed state even when the corresponding quantum circuits contain as many as 4110 entangling gates, demonstrating the intrinsic robustness of the protocol. By applying zero-noise extrapolation, the resulting energy expectation values are systematically improved to agree with noiseless simulations within statistical uncertainties.

</details>


### [33] [Quantum State Discrimination Enhanced by FPGA-Based AI Engine Technology](https://arxiv.org/abs/2601.08213)
*Anastasiia Butko,Artem Marisov,David I. Santiago,Irfan Siddiqi*

Main category: quant-ph

TL;DR: 基于FPGA AI Engine的实时量子比特状态判别系统，利用多层神经网络在AMD Xilinx VCK190平台上实现准确的原位状态判别，支持多量子比特的中电路测量实验。


<details>
  <summary>Details</summary>
Motivation: 量子比特状态判别是量子计算中的关键操作，但在超导量子处理器上一直是最容易出错且最耗时的操作。由于严格的时序约束和算法复杂性，大多数状态判别方法只能在离线状态下执行。

Method: 开发并实现了一个多层神经网络系统，利用AMD Xilinx VCK190 FPGA平台的AI Engine技术，通过专门的AI/ML加速器优化量子实验并减少FPGA资源使用。

Result: 实现了增强的实时量子状态判别系统，能够进行准确的原位状态判别，并支持多量子比特的中电路测量实验。

Conclusion: 该工作展示了利用FPGA AI Engine技术实现实时量子状态判别的可行性，通过架构研究和设计的最新进展，优化了量子实验并减少了硬件资源需求。

Abstract: Identifying the state of a quantum bit (qubit), known as quantum state discrimination, is a crucial operation in quantum computing. However, it has been the most error-prone and time-consuming operation on superconducting quantum processors. Due to stringent timing constraints and algorithmic complexity, most qubit state discrimination methods are executed offline. In this work, we present an enhanced real-time quantum state discrimination system leveraging FPGA-based AI Engine technology. A multi-layer neural network has been developed and implemented on the AMD Xilinx VCK190 FPGA platform, enabling accurate in-situ state discrimination and supporting mid-circuit measurement experiments for multiple qubits. Our approach leverages recent advancements in architecture research and design, utilizing specialized AI/ML accelerators to optimize quantum experiments and reduce the use of FPGA resources.

</details>


### [34] [Reference-frame-independent Quantum secure direct communication](https://arxiv.org/abs/2601.08238)
*Jia-Wei Ying,Shi-Pu Gu,Xing-Fu Wang,Wei Zhong,Ming-Ming Du,Xi-Yun Li,Shu-Ting Shen,An-Lei Zhang,Lan Zhou,Yu-Bo Sheng*

Main category: quant-ph

TL;DR: 提出一种参考系无关的量子安全直接通信协议，仅需校准一个方向的参考系，允许其他两个方向存在偏差角β，显著提升了移动通信场景下的实用性。


<details>
  <summary>Details</summary>
Motivation: 传统QSDC协议需要精确校准X和Z基的参考系，这在移动通信场景中难以实现连续准确校准。为解决移动通信中参考系校准的挑战，需要开发参考系无关的协议。

Method: 提出RFI-QSDC协议，只需确保一个方向参考系校准准确，允许其他两个方向存在偏差角β。引入β无关参数C到安全分析框架，重新推导安全边界。构建系统模型并优化信号态脉冲强度，使协议在不同信道衰减下达到最优性能。

Result: 在10dB衰减（对应25km通信距离）下，β=0°和45°时的保密消息容量分别为8.765×10⁻⁶和4.150×10⁻⁶ bit/pulse。最大传输距离在β=0°和45°时分别为27.875km和26.750km，分别是单光子QSDC协议的155.9%和149.7%。

Conclusion: 提出的RFI-QSDC协议显著降低了参考系校准要求，在移动通信场景中具有更好的实用性，通信距离相比传统单光子QSDC协议显著提升，为移动量子安全通信提供了有效解决方案。

Abstract: Current quantum secure direct communication (QSDC) protocols guarantee communication security by estimating the error rates of photons in the X and Z bases. This take the reference frame calibration between communicating parties as a necessary prerequisite. However, in mobile communications scenarios, achieving continuous and accurate reference frame calibration poses significant challenges. To address this issue, this paper proposes a reference-frame-independent (RFI) QSDC protocol. This protocol only requires ensuring the calibration accuracy of one direction of the reference frame, while allowing a misalignment angle $β$ in the other two directions. To improve the protocol's robustness against reference frame fluctuations, we introduce a $β$-independent parameter C into the security analysis framework and rederive the protocol's security bounds. Additionally, we construct a system model and optimize the pulse intensity of the signal states, enabling the protocol to achieve optimal performance under each level of channel attenuation. At an attenuation of 10 dB (corresponding to a communication distance of 25 km), the secrecy message capacities for $β= 0^{ \circ} $ and $45^{ \circ} $ are $8.765 \times10^{-6}$ bit/pulse and $4.150 \times10^{-6}$ bit/pulse, respectively. Compared with the single-photon-based QSDC, the communication distance of the protocol proposed in this paper is significantly extended. When $β= 0^{ \circ} $ and $45^{ \circ} $, the maximum transmission distances of the RFI QSDC protocol are 27.875 km and 26.750 km, which is about 155.9 % and 149.7 % of that of the single-photon-based QSDC protocol.

</details>


### [35] [Efficient and broadband quantum frequency comb generation in a monolithic AlGaAs-on-insulator microresonator](https://arxiv.org/abs/2601.08289)
*Xiaodong Zheng,Xu Jing,Chenbo Liu,Yufu Li,Runqiu He,Lina Xia,Fei Wang,Yuechan Kong,Tangsheng Chen,Liangliang Lu,Jiayun Dai,Bin Niu*

Main category: quant-ph

TL;DR: 研究人员基于AlGaAs-on-insulator平台开发了一种高效芯片级多波长量子光源，在电信波段产生11个不同波长对的频率纠缠光子对，平均频谱亮度达2.64 GHz mW⁻²nm⁻¹，平均净可见度达93.1%。


<details>
  <summary>Details</summary>
Motivation: 光子频率编码因其与现有光纤网络和波分复用系统的天然兼容性，成为量子信息处理中特别有前景的方法。利用多个频率模式可以显著减少硬件资源并提升量子性能。非线性光子学与微谐振器的结合为在离散光谱模式中产生频率相关光子对提供了有吸引力的途径。

Method: 利用AlGaAs-on-insulator平台的高材料非线性和低非线性损耗，设计优化的亚微米波导几何结构，提供高有效非线性系数(~550 m⁻¹W⁻¹)和宽生成带宽。通过Franson干涉测量验证每个频率模式对的能量-时间纠缠。

Result: 在电信波长实现了约200 GHz的自由光谱范围，在35.2 nm带宽内产生了11个不同的波长对，平均频谱亮度为2.64 GHz mW⁻²nm⁻¹。通过Franson干涉测量验证了纠缠，平均净可见度达到93.1%。

Conclusion: 所开发的AlGaAs-on-insulator平台凭借其优异的光学增益和激光能力，在实现完全集成、可部署的片上量子光子系统方面展现出巨大潜力。

Abstract: The exploration of photonic systems for quantum information processing has generated widespread interest in multiple cutting-edge research fields. Photonic frequency encoding stands out as an especially viable approach, given its natural alignment with established optical communication technologies, including fiber networks and wavelength-division multiplexing systems. Substantial reductions in hardware resources and improvements in quantum performance can be expected by utilizing multiple frequency modes. The integration of nonlinear photonics with microresonators provides a compelling way for generating frequency-correlated photon pairs across discrete spectral modes. Here, by leveraging the high material nonlinearity and low nonlinear loss, we demonstrate an efficient chip-scale multi-wavelength quantum light source based on AlGaAs-on-insulator, featuring a free spectral range of approximately 200 GHz at telecom wavelengths. The optimized submicron waveguide geometry provides both high effective nonlinearity (~550 m$^{-1}$W$^{-1}$) and broad generation bandwidth, producing eleven distinct wavelength pairs across a 35.2 nm bandwidth with an average spectral brightness of 2.64 GHz mW$^{-2}$nm$^{-1}$. The generation of energy-time entanglement for each pair of frequency modes is verified through Franson interferometry, yielding an average net visibility of 93.1%. With its exceptional optical gain and lasing capabilities, the AlGaAs-on-insulator platform developed here shows outstanding potential for realizing fully integrated, ready-to-deploy quantum photonic systems on chip.

</details>


### [36] [A Preparation Nonstationarity Loophole in Superconducting-Qubit Bell Tests](https://arxiv.org/abs/2601.08290)
*Prosanta Pal,Shubhanshu Karoliya,Gargee Sharma,Ramakrishna Podila*

Main category: quant-ph

TL;DR: 该论文指出超导量子处理器中的贝尔测试通常假设重复电路执行采样单一稳态制备系综，但当代硬件可能违反此假设，导致贝尔违规解释需考虑制备非稳态性。


<details>
  <summary>Details</summary>
Motivation: 超导量子处理器上的贝尔测试通常基于重复电路执行采样单一稳态制备系综的假设，但该假设可能在当代硬件上被违反，直接影响观察到的贝尔违规的解释。

Method: 提出系综发散框架，考虑制备过程的缓慢时间漂移导致上下文依赖的有效系综；开发基于固定测量通道的分箱结果统计的操作性见证δ_op；在IBM超导处理器上进行泡利轴测量实验。

Result: 观察到统计显著的操作性漂移，在完全双量子比特读出缓解后仍存在；从CHSH最优测量提取的漂移可通过缓解消除；观察到的贝尔违规仅暗示适度的系综发散，与Hall型测量依赖模型尺度相当。

Conclusion: 识别出与噪声中等规模量子设备上贝尔测试相关的制备依赖漏洞，强调需要漂移感知协议进行可靠的量子认证。

Abstract: Bell or Clauser-Horne-Shimony-Holt (CHSH) tests on superconducting quantum processors are commonly interpreted under the assumption that repeated circuit executions sample a single, stationary preparation ensemble. Here we show that this assumption can be violated on contemporary hardware, with direct implications for the interpretation of observed Bell violations. We introduce an ensemble-divergence framework in which slow temporal drift of the preparation process induces context-dependent effective ensembles, even when measurement independence and locality are preserved. This leads to a relaxed Bell bound $|S| \le 2 + 6δ_{\mathrm{ens}}$, where $δ_{\mathrm{ens}}$ quantifies preparation nonstationarity. Because $δ_{\mathrm{ens}}$ is not directly observable, we develop an operational witness $δ_{\mathrm{op}}$ based on bin-resolved outcome statistics for fixed measurement channels. Using Pauli-axis measurements on IBM superconducting processors, we observe statistically significant operational drift that persists after full two-qubit readout mitigation, ruling out measurement artifacts. In contrast, drift extracted from CHSH-optimal measurements is eliminated by mitigation, demonstrating that such settings are unsuitable for diagnosing preparation nonstationarity. We further show that the observed Bell violations imply only modest ensemble divergences, comparable in scale to those required in Hall-type measurement-dependence models, but arising here solely from preparation drift combined with experimental scheduling. Our results identify a preparation-dependent loophole relevant to Bell tests on noisy intermediate-scale quantum devices and highlight the necessity of drift-aware protocols for reliable quantum certification.

</details>


### [37] [Extending Qubit Coherence Time via Hybrid Dynamical Decoupling](https://arxiv.org/abs/2601.08315)
*Qi Yao,Jun Zhang,Wenxian Zhang,Chaohong Lee*

Main category: quant-ph

TL;DR: 提出一种结合脉冲动力学解耦和自旋浴极化的混合方法，在中心自旋模型中显著延长量子比特相干时间约2-3个数量级。


<details>
  <summary>Details</summary>
Motivation: 量子比特不可避免地与环境耦合导致退相干，动力学解耦和浴工程是两种常用的缓解技术，但通常分开使用。本研究旨在整合这两种技术以更有效地抑制退相干。

Method: 采用混合动力学解耦方法，将脉冲动力学解耦与自旋浴极化技术相结合，在中心自旋模型中实现。该方法可在GaAs半导体量子点或类似量子模拟器中实现。

Result: 中心自旋的相干时间相比自由感应衰减时间延长约2-3个数量级，其中动力学解耦起主要作用，自旋浴极化提供适度改进。

Conclusion: 这种整合单轴动力学解耦和辅助浴自旋工程的方法为延长GaAs/AlGaAs、硅和Si/SiGe等实际量子系统的相干时间开辟了新途径，对量子信息处理应用具有重要前景。

Abstract: Dynamical decoupling (DD) and bath engineering are two parallel techniques employed to mitigate qubit decoherence resulting from their unavoidable coupling to the environment. Here, we present a hybrid DD approach that integrates pulsed DD with bath spin polarization to enhance qubit coherence within the central spin model. This model, which can be realized using GaAs semiconductor quantum dots or analogous quantum simulators, demonstrates a significant extension of the central spin's coherence time by approximately 2 to 3 orders of magnitude that compared with the free-induced decay time, where the dominant contribution from DD and a moderate improvement from spin-bath polarization. This study, which integrates uniaxial dynamical decoupling and auxiliary bath-spin engineering, paves the way for prolonging coherence times in various practical quantum systems, including GaAs/AlGaAs, silicon and Si/SiGe. And this advancement holds substantial promise for applications in quantum information processing.

</details>


### [38] [Quantum fluctuations of vacuum versus photon-pairs concerning Spontaneous-Parametric Down-Conversion and Four-Wave-Mixing](https://arxiv.org/abs/2601.08349)
*Benoît Boulanger,Gaspar Mougin-Trichon,Véronique Boutou*

Main category: quant-ph

TL;DR: 该研究通过半经典模型和解析计算，确定了自发参量下转换（SPDC）和四波混频（FWM）两种机制之间的泵浦强度界限，并给出了量化指标。


<details>
  <summary>Details</summary>
Motivation: 需要明确SPDC和FWM这两种非线性光学过程的界限，以区分真正的自发过程（主要由真空量子涨落驱动）和光学参量放大/差频产生机制，为量子计算和测量提供指导。

Method: 采用半经典模型和解析计算方法，定义了无量纲的光子对通量密度参数，计算了产生光子电场与真空电场的比值，并绘制了泵浦强度随相互作用长度变化的函数图。

Result: 确定了界限处的光子对通量密度为0.369，产生光子电场与真空电场的比值为1.718。通过绘制不同二阶（SPDC）和三阶（FWM）电极化率下的泵浦强度-相互作用长度关系图，量化了两种机制的界限。

Conclusion: 该研究提供了SPDC和FWM两种机制的量化界限，低于此界限时过程是真正的自发过程（由真空涨落主导），高于此界限则更接近光学参量放大机制。这些结果为从低到高泵浦强度的量子计算和测量提供了重要指导。

Abstract: The limit between the two regimes of spontaneous-parametric down-conversion (SPDC) or four-wave-mixing (FWM) regarding the pump intensity has been theoretically investigated using a semi-classical model and analytical calculations. A unitless quantity has been defined, corresponding to the photon-pairs flux per frequency unit: it has been found equal to 0.369 at this limit. The ratio between the magnitudes of the electric fields of the generated photons and of vacuum has been also calculated, equal to 1.718, and the pump intensity has been plotted as a function of the interaction length for different values of the second-order electric susceptibility in the case of SPDC and of the third-order electric susceptibility for FWM. These quantitative results confirm that below the limit, the nonlinear process can be truly considered as spontaneous, i.e. mainly seeded by the quantum fluctuations of vacuum, while the generated photons mainly govern the pump photon splitting above the limit, which corresponds more to an optical parametric amplification / difference frequency generation regime. Knowing quantitatively the limit between the two regimes thanks to the present calculations will be a useful guide for further quantum calculations and measurements from either side of the limit in order to catch the full quantum picture of SPDC and FWM from low to high pump intensities.

</details>


### [39] [Verification of continuous variable entanglement with undetected photons](https://arxiv.org/abs/2601.08364)
*Sanjukta Kundu,Balakrishnan Viswanathan,Pawel Szczypkowski,Gabriela Barreto Lemos,Mayukh Lahiri,Radek Lapkiewicz*

Main category: quant-ph

TL;DR: 该论文提出了一种无需符合探测的非线性干涉技术来验证自发参量下转换产生的光子对横向空间纠缠，通过单光子干涉实验违反EPR和MGVT判据，方法对实验损耗鲁棒且适用于高度非简并源。


<details>
  <summary>Details</summary>
Motivation: 传统验证光子对纠缠通常需要符合探测，但在某些情况下（如高度非简并源）可能缺乏合适的探测器。本文旨在开发一种无需符合探测的方法来验证空间纠缠，提高方法的鲁棒性和适用范围。

Method: 采用非线性干涉技术，仅通过对光子对中一个光子的单光子干涉来验证纠缠。实验上测量了违反爱因斯坦-波多尔斯基-罗森（EPR）判据和曼奇尼-乔瓦内蒂-维塔利-托姆贝西（MGVT）判据的现象，并提供了全面的理论分析。

Result: 实验结果表明，通过单光子干涉成功违反了EPR和MGVT判据，实验数据与理论值吻合良好。该方法在实验损耗下表现优异，且可应用于高度非简并源。

Conclusion: 该非线性干涉技术为验证光子对空间纠缠提供了一种无需符合探测的有效方法，对实验损耗鲁棒，适用于缺乏合适探测器的非简并源，并可扩展到离散自由度（如轨道角动量）以认证高维纠缠。

Abstract: We verify transverse spatial entanglement of photon-pairs generated in spontaneous parametric down conversion using a nonlinear interferometric technique without relying on any coincidence detection. We experimentally demonstrate the violation of the Einstein-Podolsky-Rosen criterion and of the Mancini-Giovannetti-Vitali-Tombesi criterion using single photon interference of one of the photons of the pairs. We also provide a comprehensive theoretical analysis. The experimental results that we have obtained show good agreement with the theoretical values. Our method performs well under experimental losses and can be applied to highly non-degenerate sources, where there are no suitable detectors for one of the photons in the quantum state and our method could also be extended to the discrete degrees of freedom to certify high-dimensional (OAM) entanglement.

</details>


### [40] [A Methodological Analysis of Empirical Studies in Quantum Software Testing](https://arxiv.org/abs/2601.08367)
*Yuechen Li,Minqi Shao,Jianjun Zhao,Qichen Wang*

Main category: quant-ph

TL;DR: 该论文对量子软件测试中的实证研究方法进行了系统性分析，通过审查59篇相关研究，识别了当前研究的方法论问题并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 随着量子软件系统规模和复杂性的增长，量子软件测试的重要性日益凸显。然而，当前量子软件测试领域的实证研究在设计方法和报告方式上存在高度多样性，缺乏统一的方法论理解，导致研究结果难以解释和比较。

Method: 通过对384篇文献进行筛选，最终确定了59篇主要研究作为分析对象。围绕十个研究问题展开分析，涵盖量子软件测试实证研究的关键方法论维度，包括测试对象、基线比较、测试设置、实验配置以及工具和工件支持等。

Result: 通过跨研究分析，系统描述了当前量子软件测试实证研究的实践现状，识别了重复出现的局限性和不一致性问题，并突出了开放的方法论挑战。

Conclusion: 基于研究发现，提出了对量子软件测试领域未来实证研究的设计、执行和报告具有指导意义的见解和建议，旨在促进该领域研究方法的标准化和规范化。

Abstract: In quantum software engineering (QSE), quantum software testing (QST) has attracted increasing attention as quantum software systems grow in scale and complexity. Since QST evaluates quantum programs through execution under designed test inputs, empirical studies are widely used to assess the effectiveness of testing approaches. However, the design and reporting of empirical studies in QST remain highly diverse, and a shared methodological understanding has yet to emerge, making it difficult to interpret results and compare findings across studies. This paper presents a methodological analysis of empirical studies in QST through a systematic examination of 59 primary studies identified from a literature pool of size 384. We organize our analysis around ten research questions that cover key methodological dimensions of QST empirical studies, including objects under test, baseline comparison, testing setup, experimental configuration, and tool and artifact support. Through cross-study analysis along these dimensions, we characterize current empirical practices in QST, identify recurring limitations and inconsistencies, and highlight open methodological challenges. Based on our findings, we derive insights and recommendations to inform the design, execution, and reporting of future empirical studies in QST.

</details>


### [41] [A dataflow programming framework for linear optical distributed quantum computing](https://arxiv.org/abs/2601.08389)
*Giovanni de Felice,Boldizsár Poór,Cole Comfort,Lia Yeh,Mateusz Kupper,William Cashman,Bob Coecke*

Main category: quant-ph

TL;DR: 提出了一种用于分布式量子计算的光子图形框架，结合了线性光学、ZX演算和数据流编程，支持光子融合测量、纠错协议和可验证架构设计。


<details>
  <summary>Details</summary>
Motivation: 光子系统为量子处理器互连提供了有前景的平台，但需要统一的框架来集成线性代数推理与概率和控制流结构，以设计和验证可扩展的网络化量子计算架构。

Method: 引入了一个图形框架，结合线性光学、ZX演算和数据流编程，采用同步数据流模型和离散时间动力学，支持光子融合测量的分类、泡利误差校正和新颖的重复直到成功协议。

Result: 建立了光子融合测量的分类，提出了融合网络的流结构来校正泡利误差，证明了新的重复直到成功协议的正确性，并构建了包含实际光学组件的确定性通用量子计算架构。

Conclusion: 该工作为网络化量子计算中的可验证编译和自动优化奠定了基础，提供了一个统一的框架来分析和优化涉及光子和量子比特的分布式协议。

Abstract: Photonic systems offer a promising platform for interconnecting quantum processors and enabling scalable, networked architectures. Designing and verifying such architectures requires a unified formalism that integrates linear algebraic reasoning with probabilistic and control-flow structures. In this work, we introduce a graphical framework for distributed quantum computing that brings together linear optics, the ZX-calculus, and dataflow programming. Our language supports the formal analysis and optimization of distributed protocols involving both qubits and photonic modes, with explicit interfaces for classical control and feedforward, all expressed within a synchronous dataflow model with discrete-time dynamics. Within this setting, we classify entangling photonic fusion measurements, show how their induced Pauli errors can be corrected via a novel flow structure for fusion networks, and establish correctness proofs for new repeat-until-success protocols enabling arbitrary fusions. Layer by layer, we construct qubit architectures incorporating practical optical components such as beam splitters, switches, and photon sources, with graphical proofs that they are deterministic and support universal quantum computation. Together, these results establish a foundation for verifiable compilation and automated optimization in networked quantum computing.

</details>


### [42] [On-chip semi-device-independent quantum random number generator exploiting contextuality](https://arxiv.org/abs/2601.08392)
*Maddalena Genzini,Caterina Vigliar,Mujtaba Zahidy,Hamid Tebyanian,Andrzej Gajda,Klaus Petermann,Lars Zimmermann,Davide Bacco,Francesco Da Ros*

Main category: quant-ph

TL;DR: 基于硅光子芯片的上下文性不等式违反的半设备无关量子随机数发生器，通过单光子干涉产生可认证的随机数


<details>
  <summary>Details</summary>
Motivation: 开发一种实用的、与集成光子量子网络兼容的量子随机数发生器，能够从单光子干涉的内在随机性中生成可安全认证的随机数，而不需要纠缠

Method: 使用两个硅光子芯片集成系统：一个用于产生单光子源，另一个用于实现可重构干涉网格，实现qutrit态制备、变换和测量，测试KCBS上下文性不等式

Result: 观察到上下文性违反超过经典界限10σ以上，认证每轮实验的条件最小熵为0.077±0.002比特，对应21.7±0.5比特/秒的渐近生成速率

Conclusion: 该系统为与实用集成光子量子网络兼容的通用、不可信量子随机数发生器建立了可行路径，能够从单光子干涉中产生可认证的随机性

Abstract: We present a semi-device-independent quantum random number generator (QRNG) based on the violation of a contextuality inequality, implemented by the integration of two silicon photonic chips. Our system combines a heralded single-photon source with a reconfigurable interferometric mesh to implement qutrit state preparation, transformations, and measurements suitable for testing a KCBS contextuality inequality. This architecture enables the generation of random numbers from the intrinsic randomness of single-photon interference in a complex optical network, while simultaneously allowing a quantitative certification of their security without requiring entanglement. We observe a contextuality violation exceeding the classical bound by more than 10σ, unambiguously confirming non-classical behavior. From this violation, we certify a conditional min-entropy per experimental round of Hmin = 0.077 +- 0.002, derived via a tailored semidefinite-programming-based security analysis. Each measurement outcome therefore contains at least 0.077 +- 0.002 bits of extractable genuine randomness, corresponding to an asymptotic generation rate of 21.7 +- 0.5 bits/s. These results establish a viable route towards general-purpose, untrusted quantum random number generators compatible with practical integrated photonic quantum networks.

</details>


### [43] [Rigorous phase-error-estimation security framework for QKD with correlated sources](https://arxiv.org/abs/2601.08417)
*Guillermo Currás-Lorenzo,Margarida Pereira,Kiyoshi Tamaki,Marcos Curty*

Main category: quant-ph

TL;DR: 提出一个数学框架，将基于相位误差估计的安全证明从无关联的源扩展到包含编码关联的源，解决QKD调制器带宽限制导致的脉冲关联问题


<details>
  <summary>Details</summary>
Motivation: 实际QKD调制器由于带宽限制会在连续发射的脉冲之间引入关联，这违反了许多安全证明技术的基本假设，导致理论安全保证与实际实现之间存在差距

Method: 引入一个简单而强大的数学框架，直接扩展基于相位误差估计的安全证明方法，使其能够处理不完美但无关联的源，并进一步纳入编码关联

Result: 该框架克服了先前方法在通用性和严谨性方面的限制，显著缩小了理论安全保证与实际QKD实现之间的差距

Conclusion: 提出的数学框架为解决QKD调制器脉冲关联问题提供了更通用和严谨的方法，有助于提升实际量子密钥分发系统的安全性

Abstract: Practical QKD modulators introduce correlations between consecutively emitted pulses due to bandwidth limitations, violating key assumptions underlying many security proof techniques. Here, we address this problem by introducing a simple yet powerful mathematical framework to directly extend phase-error-estimation-based security proofs for imperfect but uncorrelated sources to also incorporate encoding correlations. Our framework overcomes important limitations of previous approaches in terms of generality and rigor, significantly narrowing the gap between theoretical security guarantees and real-world QKD implementations.

</details>


### [44] [Toolchain for shuttling trapped-ion qubits in segmented traps](https://arxiv.org/abs/2601.08495)
*Andreas Conta,Santiago Bogino,Frodo Köhncke,Ferdinand Schmidt-Kaler,Ulrich Poschinger*

Main category: quant-ph

TL;DR: 开发了一个用于设计离子阱中快速、低激发离子穿梭电压波形的数值工具链，支持任意几何结构并优化计算性能


<details>
  <summary>Details</summary>
Motivation: 可扩展的囚禁离子量子计算需要离子在复杂的分段射频阱结构中快速可靠地传输，同时避免过多的运动激发。现有方法需要系统化的工具来生成满足实验约束的电压波形。

Method: 基于阱电极几何模型，结合静电场求解器、无约束优化、波形后处理和离子运动动力学模拟，计算实现预定传输轨迹的电压波形，同时考虑电压限制和带宽等实验约束。

Result: 该框架支持任意阱几何结构（包括交叉结和多区域布局），通过数值稳定性分析和实测频率比较验证了准确性，计算性能优化可实现复杂结构的快速原型设计。

Conclusion: 该方法为当前和下一代囚禁离子处理器中传输协议的设计和验证提供了可扩展且高效的数值基础。

Abstract: Scalable trapped-ion quantum computing requires fast and reliable transport of ions through complex, segmented radiofrequency trap architectures without inducing excessive motional excitation. We present a numerical toolchain for the systematic generation of time-dependent electrode voltages enabling fast, low-excitation ion shuttling in segmented radiofrequency traps. Based on a model of the trap electrode geometry, the framework combines an electrostatic field solver, efficient unconstrained optimization, waveform postprocessing, and dynamical simulations of ion motion to compute voltage waveforms that realize prescribed transport trajectories while respecting experimental constraints such as voltage limits and bandwidth. The toolchain supports arbitrary trap geometries, including junctions and multi-zone layouts, and allows for the flexible incorporation of optimization objectives. We provide a detailed assessment of the accuracy of the framework by investigating its numerical stability and by comparing measured and predicted secular frequencies. The framework is optimized for numerical performance, enabling rapid numerical prototyping of trap architectures of increasing complexity. As application examples, we apply the framework to the transport of a potential well along a linear, uniformly segmented trap, and we compute a solution for shuttling a potential well around the corner of an X-type trap junction. The presented approach provides an extensible and highly efficient numerical foundation for designing and validating transport protocols in current and next-generation trapped-ion processors.

</details>


### [45] [MultiQ: Multi-Programming Neutral Atom Quantum Architectures](https://arxiv.org/abs/2601.08504)
*Francisco Romão,Daniel Vonk,Emmanuil Giortamis,Pramod Bhatotia*

Main category: quant-ph

TL;DR: MultiQ系统实现中性原子量子处理器上的多程序并行执行，通过虚拟分区布局提升吞吐量和硬件利用率，同时保持结果保真度。


<details>
  <summary>Details</summary>
Motivation: 中性原子量子处理器面临大电路保真度下降和小电路硬件利用率低的问题，现有编译器不支持多程序并行执行，需要新的系统解决方案。

Method: MultiQ系统包含编译器、控制器和检查器：编译器生成虚拟分区布局优化时空硬件利用；控制器将布局映射到硬件并解决冲突；检查器验证电路功能独立性。

Result: 实验显示4-14个电路并行执行时，吞吐量提升3.8×到12.3×，保真度变化从1.3%提升到仅3.5%损失，显著提高硬件利用率。

Conclusion: MultiQ是首个支持中性原子量子处理器多程序执行的系统，有效解决了硬件利用率低和初始化延迟问题，为量子计算资源优化提供了新方案。

Abstract: Neutral atom Quantum Processing Units (QPUs) are emerging as a popular quantum computing technology due to their large qubit counts and flexible connectivity. However, performance challenges arise as large circuits experience significant fidelity drops, while small circuits underutilize hardware and face initialization latency issues. To tackle these problems, we propose $\textit{multi-programming on neutral atom QPUs}$, allowing the co-execution of multiple circuits by logically partitioning the qubit array. This approach increases resource utilization and mitigates initialization latency while maintaining result fidelity. Currently, state-of-the-art compilers for neutral atom architectures do not support multi-programming.
  To fill this gap, we introduce MultiQ, the first system designed for this purpose. MultiQ addresses three main challenges: (i) it compiles circuits into a $\textit{virtual zone layout}$ to optimize spatio-temporal hardware utilization; (ii) it parallelizes the execution of co-located circuits, allowing single hardware instructions to operate on different circuits; and (iii) it includes an algorithm to verify the functional independence of the bundled circuits. MultiQ functions as a cross-layer system comprising a compiler, controller, and checker. Our compiler generates \emph{virtual zone layouts} to enhance performance, while the controller efficiently maps these layouts onto the hardware and resolves any conflicts. The checker ensures the correct bundling of circuits.
  Experimental results show a throughput increase from 3.8$\times$ to 12.3$\times$ when multi-programming 4 to 14 circuits, with fidelity largely maintained, ranging from a 1.3% improvement for four circuits to only a 3.5% loss for fourteen circuits. Overall, MultiQ facilitates concurrent execution of multiple quantum circuits, boosting throughput and hardware utilization.

</details>


### [46] [Symmetry-Adapted State Preparation for Quantum Chemistry on Fault-Tolerant Quantum Computers](https://arxiv.org/abs/2601.08533)
*Viktor Khinevich,Wataru Mizukami*

Main category: quant-ph

TL;DR: 本文提出了一种在容错量子计算中高效实现连续对称性投影器（U(1)粒子数和SU(2)总自旋）的方法，通过对称性过滤显著提高量子相位估计的成功概率，大幅降低整体计算成本。


<details>
  <summary>Details</summary>
Motivation: 在量子化学模拟中，量子相位估计（QPE）需要精确的初始态准备。连续对称性（如粒子数和自旋）的投影器可以过滤掉非物理态，提高QPE的成功概率，但传统实现方法资源消耗大。本文旨在开发资源高效的对称性投影器构造方法，使其在容错量子计算中实用化。

Method: 采用线性组合酉算子（LCU）和广义量子信号处理（GQSP和GQSVT）技术实现对称性投影器。针对粒子数和S_z对称性，GQSP因其低辅助量子比特需求和有限精度旋转门合成的鲁棒性而具有优势。对于总自旋投影，通过结构化分解减少投影器的T门数量。这些投影器可作为量子相位估计前的相干状态过滤器。

Result: 数值模拟显示对称性过滤显著提高了QPE的成功概率，在代表性分子系统中整体成本低于未过滤方法。资源估计表明对称性过滤的成本比后续相位估计步骤低3-4个数量级。以FeMoco为例，QPE成本约10^10个T门，而对称性投影器仅需10^6-10^7个T门。

Conclusion: 连续对称性投影器是量子化学中状态准备的实用且可扩展的工具，为实现更高效的容错量子模拟提供了途径。该方法在大型强关联系统（如FeMoco）中特别有优势，能够显著降低整体计算成本。

Abstract: We present systematic and resource-efficient constructions of continuous symmetry projectors, particularly $U(1)$ particle number and $SU(2)$ total spin, tailored for fault-tolerant quantum computations. Our approach employs a linear combination of unitaries (LCU) as well as generalized quantum signal processing (GQSP and GQSVT) to implement projectors. These projectors can then be coherently applied as state filters prior to quantum phase estimation (QPE). We analyze their asymptotic gate complexities for explicit circuit realizations. For the particle number and $S_z$ symmetries, GQSP offers favorable resource usage features owing to its low ancilla qubit requirements and robustness to finite precision rotation gate synthesis. For the total spin projection, the structured decomposition of $\hat{P}_{S,M_S}$ reduces the projector T gate count. Numerical simulations show that symmetry filtering substantially increases the QPE success probability, leading to a lower overall cost compared to that of unfiltered approaches across representative molecular systems. Resource estimates further indicate that the cost of symmetry filtering is $3$ to $4$ orders of magnitude lower than that of the subsequent phase estimation step This advantage is especially relevant in large, strongly correlated systems, such as FeMoco, a standard strongly correlated open-shell benchmark. For FeMoco, the QPE cost is estimated at ${\sim}10^{10}$ T gates, while our symmetry projector requires only ${\sim}10^{6}$--$10^{7}$ T gates. These results establish continuous-symmetry projectors as practical and scalable tools for state preparation in quantum chemistry and provide a pathway toward realizing more efficient fault-tolerant quantum simulations.

</details>


### [47] [Asymptotically good CSS codes that realize the logical transversal Clifford group fault-tolerantly](https://arxiv.org/abs/2601.08568)
*K. Sai Mineesh Reddy,Navin Kashyap*

Main category: quant-ph

TL;DR: 提出了一个构建支持容错逻辑横向Z旋转的CSS码框架，获得了渐近好的CSS码实现容错逻辑横向Clifford群，并研究了CSS-T码的特性


<details>
  <summary>Details</summary>
Motivation: 构建能够支持容错逻辑横向操作的量子纠错码，特别是实现Clifford群操作和T门操作，这对于容错量子计算至关重要

Method: 提出了一个构建CSS码的框架，支持容错逻辑横向Z旋转。通过该框架获得渐近好的CSS码，并深入研究CSS-T码的特性，包括必要条件分析和码字表征的修正

Result: 1) 获得了渐近好的CSS码，能够容错实现逻辑横向Clifford群；2) 展示了渐近好的CSS-T码，其中横向T门实现逻辑横向S†操作；3) 证明了条件C₂∗C₁⊆C₁⊥是CSS-T码的必要但不充分条件；4) 修正了CSS-T码的表征

Conclusion: 该框架为构建支持容错逻辑横向操作的CSS码提供了系统方法，对CSS-T码的深入分析澄清了其必要条件和实现不同逻辑操作的表征，推动了容错量子计算的发展

Abstract: This paper introduces a framework for constructing Calderbank-Shor-Steane (CSS) codes that support fault-tolerant logical transversal $Z$-rotations. Using this framework, we obtain asymptotically good CSS codes that fault-tolerantly realize the logical transversal Clifford group. Furthermore, investigating CSS-T codes, we: (a) demonstrate asymptotically good CSS-T codes wherein the transversal $T$ realizes the logical transversal $S^{\dagger}$; (b) show that the condition $C_2 \ast C_1 \subseteq C_1^{\perp}$ is necessary but not sufficient for CSS-T codes; and (c) revise the characterizations of CSS-T codes wherein the transversal $T$ implements the logical identity and the logical transversal $T$, respectively.

</details>


### [48] [Quantum Computing -- Strategic Recommendations for the Industry](https://arxiv.org/abs/2601.08578)
*Marvin Erdmann,Lukas Karch,Abhishek Awasthi,Caitlin Isobel Jones,Pallavi Bhardwaj,Florian Krellner,Jonas Stein,Claudia Linnhoff-Popien,Nico Kraus,Peter Eder,Sarah Braun,Tong Liu*

Main category: quant-ph

TL;DR: 该白皮书调查了工业环境中量子优化和机器学习的现状与短期到中期前景，基于QCHALLenge项目评估不同量子架构的硬件发展轨迹，并使用标准化交通灯评估框架分析实际应用案例的成熟度。


<details>
  <summary>Details</summary>
Motivation: 评估量子计算在工业优化和机器学习应用中的实际可行性和成熟度，为行业决策者提供清晰的指导，区分量子方法的当前价值与未来潜力。

Method: 基于QCHALLenge项目，综合不同量子架构（超导和离子阱）的硬件发展路线图，使用标准化交通灯评估框架，通过一致的评价标准（模型构建、可扩展性、解决方案质量、运行时间、可转移性）对优化和机器学习用例进行实验评估。

Result: 评估结果显示量子方法在不同应用领域的当前状态：有些领域量子方法已具有竞争力或显示出明确的量子优势路径，有些领域混合经典-量子策略最为可行，而其他领域经典方法预计仍将保持优势。

Conclusion: 该研究为工业界提供了量子计算在优化和机器学习应用中的实际可行性评估，明确了量子方法的当前优势领域、混合策略的适用场景，以及经典方法仍占主导的领域，为技术投资和应用决策提供了指导。

Abstract: This whitepaper surveys the current landscape and short- to mid-term prospects for quantum-enabled optimization and machine learning use cases in industrial settings. Grounded in the QCHALLenge program, it synthesizes hardware trajectories from different quantum architectures and providers, and assesses their maturity and potential for real-world use cases under a standardized traffic-light evaluation framework. We provide a concise summary of relevant hardware roadmaps, distinguishing superconducting and ion-trap technologies, their current states, modalities, and projected scaling trajectories. The core of the presented work are the use case evaluations in the domains of optimization problems and machine learning applications. For the conducted experiments, we apply a consistent set of evaluation criteria (model formulation, scalability, solution quality, runtime, and transferability) which are assessed in a shared system of three categories, ranging from optimistic (solutions produced by quantum computers are competitive with classical methods and/or a clear path to a quantum advantage is shown) to pessimistic (significant hurdles prevent practical application of quantum solutions now and potentially in the future). The resulting verdicts illuminate where quantum approaches currently offer promise, where hybrid classical-quantum strategies are most viable, and where classical methods are expected to remain superior.

</details>


### [49] [Phase-sensitive superposition of quantum states](https://arxiv.org/abs/2601.08579)
*Xiaotong Wang,Shunlong Luo,Yue Zhang*

Main category: quant-ph

TL;DR: 本文从信息论角度提出相位敏感叠加度量，建立守恒关系，研究最大/最小叠加态，并在Grover搜索算法中分析叠加动力学


<details>
  <summary>Details</summary>
Motivation: 尽管叠加原理是量子力学的核心，但除了相干性和干涉的资源理论外，其量化研究相对较少。本文旨在从信息论角度量化叠加，特别是考虑相位信息的叠加度量。

Method: 引入相位敏感叠加度量族，考虑固定基态（如计算基态）叠加中的振幅相位。建立相位敏感叠加的守恒关系，计算其二阶矩，表征最大叠加态诱导的去相位通道，研究最小和最大叠加态。

Result: 1. 相位敏感叠加的二阶矩与l²范数相干性内在相关；2. 建立了类似波粒二象性的互补守恒关系；3. 表征了最大叠加态诱导的去相位通道；4. 在Grover搜索算法中展示了叠加与搜索成功概率的互补关系。

Conclusion: 提出的叠加量化工具可用于分析量子叠加的结构特征和含义，为理解量子叠加提供了新的信息论视角和度量方法。

Abstract: Although the principle of superposition lies at the heart of quantum mechanics and is the root of almost all quantum phenomena such as coherence and entanglement, its quantification, except for that related to the resource theory of coherence and interference, remains relatively less studied. In this work, we address quantification of superposition from an information-theoretic perspective. We introduce a family of quantifiers of superposition, the phase-sensitive superposition, by taking into account the phases of amplitudes in the superposition of a fixed basis states (e.g., computational basis states). We establish a conservation relation for the phase-sensitive superposition, which is a kind of complementary relation and is reminiscent of wave-particle duality. We evaluate explicitly the second moment of phase-sensitive superposition and show that it is intrinsically related to the $l^2$-norm coherence. We characterize the dephasing channel induced by the maximally superposed states. We investigate the minimum and maximum superpositions, reveal their basic properties, and illustrate them through various examples. We further explore the dynamics of superposition in the Grover search algorithm, and demonstrate a complementary relation between superposition and success probability of the search algorithm. These results and quantifiers offer tools for analyzing structural features and implications of quantum superposition.

</details>


### [50] [Entanglement-swapping measurements for deterministic entanglement distribution](https://arxiv.org/abs/2601.08581)
*Mir Alimuddin,Jaemin Kim,Antonio Acín,Leonardo Zambrano*

Main category: quant-ph

TL;DR: 该论文研究了确定性纠缠交换测量，使得任意纯输入下每个测量结果都产生局部幺正等价态，无需后选择即可在量子网络中分布纠缠。


<details>
  <summary>Details</summary>
Motivation: 标准纠缠交换协议是概率性的，需要后选择测量结果，这限制了量子网络中纠缠分布的实际应用。研究确定性纠缠交换测量可以消除后选择需求，提高网络效率。

Method: 完全表征了使纠缠交换成为确定性的测量条件：对于任意纯输入，每个测量结果都产生局部幺正等价态。构建了基于复Hadamard矩阵的最优测量，最大化并发型纠缠度量。对最优协议进行了维度相关的分类分析。

Result: 发现确定性纠缠交换测量的最优协议由复Hadamard矩阵构建。在维度d=2,3时存在唯一解，d=4时有无穷多解，d=5时有72个不等价类。对于多节点网络，在d=2,3时端到端态与中继节点测量顺序无关。

Conclusion: 建立了无需后选择的最优纠缠交换方案，能够在通用量子网络架构中确定性地分布纠缠，消除了不利测量结果的影响，为量子网络的实际部署提供了理论基础。

Abstract: Entanglement swapping is a key primitive for distributing entanglement across nodes in quantum networks. In standard protocols, the outcome of the intermediate measurement determines the resulting state, making the process inherently probabilistic and requiring postselection. In this work, we fully characterize those measurements under which entanglement swapping becomes deterministic: for arbitrary pure inputs, every measurement outcome produces local-unitarily equivalent states. We also show that an optimal measurement, maximizing a concurrence-type entanglement measure, is built from complex Hadamard matrices. For this optimal protocol, we provide a complete, dimension-dependent classification of deterministic entanglement-swapping measurements: unique in dimensions $d=2,3$, infinite for $d=4$, and comprising $72$ inequivalent classes for $d=5$. We further consider a general network with multiple swapping nodes and show that, for $d=2,3$ the resulting end-to-end state is independent of the order in which the repeaters perform the optimal measurements. Our results establish optimal entanglement-swapping schemes that are post-selection free, in the sense that they distribute entanglement across generic quantum network architectures without unfavorable measurement outcomes.

</details>


### [51] [Sample Complexity of Composite Quantum Hypothesis Testing](https://arxiv.org/abs/2601.08588)
*Jacob Paul Simpson,Efstratios Palias,Sharu Theresa Jose*

Main category: quant-ph

TL;DR: 该论文研究了对称复合二元量子假设检验的样本复杂度，推导了上下界并证明它们在常数因子内匹配，还将分析扩展到差分隐私设置。


<details>
  <summary>Details</summary>
Motivation: 虽然复合量子假设检验的渐近误差指数已有深入研究，但有限样本情况下的样本复杂度仍然不清楚。本文旨在填补这一空白，为复合量子假设检验提供严格的样本复杂度分析。

Method: 作者推导了样本复杂度的下界（推广了简单量子假设检验的结果），并针对有限和无限基数的不确定性集合提出了新的上界。通过证明上下界在通用常数范围内匹配，提供了样本复杂度的紧致刻画。最后将分析扩展到差分隐私设置。

Result: 获得了复合量子假设检验样本复杂度的紧致上下界，这些界限适用于各种不确定性集合（包括有限和无限基数）。在差分隐私设置下，建立了隐私保护复合量子假设检验的样本复杂度。

Conclusion: 该工作首次为对称复合二元量子假设检验提供了完整的样本复杂度刻画，填补了有限样本情况下的理论空白，并为隐私保护量子假设检验建立了理论基础。

Abstract: This paper investigates symmetric composite binary quantum hypothesis testing (QHT), where the goal is to determine which of two uncertainty sets contains an unknown quantum state. While asymptotic error exponents for this problem are well-studied, the finite-sample regime remains poorly understood. We bridge this gap by characterizing the sample complexity -- the minimum number of state copies required to achieve a target error level. Specifically, we derive lower bounds that generalize the sample complexity of simple QHT and introduce new upper bounds for various uncertainty sets, including of both finite and infinite cardinalities. Notably, our upper and lower bounds match up to universal constants, providing a tight characterization of the sample complexity. Finally, we extend our analysis to the differentially private setting, establishing the sample complexity for privacy-preserving composite QHT.

</details>


### [52] [Open quantum spin chains with non-reciprocity: a theoretical approach based on the time-dependent generalized Gibbs ensemble](https://arxiv.org/abs/2601.08606)
*Alice Marché,Hironobu Yoshida,Alberto Nardin,Hosho Katsura,Leonardo Mazza*

Main category: quant-ph

TL;DR: 研究非互易耗散开放量子自旋链，使用时变广义吉布斯系综理论描述弱耗散下的动力学，推导快速分布演化方程，计算磁化和电流动力学


<details>
  <summary>Details</summary>
Motivation: 研究非互易开放量子自旋链的动力学行为，超越基于非相互作用费米子的分析方法，解决先前工作中发现的异常幂律指数问题

Method: 使用时变广义吉布斯系综理论，在弱耗散下通过快速分布表征系统，推导耦合微分方程描述其时间演化，并与数值模拟进行对比验证

Result: 建立了描述非互易开放量子自旋链物理的理论框架，能够计算磁化密度和电流动力学，发现两者之间的某些关系，讨论了异常幂律指数问题

Conclusion: 该理论方法能够超越基于非相互作用费米子的分析，描述非互易开放量子自旋链的物理，为理解这类系统的动力学提供了新工具

Abstract: We study an open quantum spin chain with non-reciprocal dissipation using a theoretical approach known as time-dependent generalized Gibbs ensemble. In the regime of weak dissipation the system is fully characterized by its rapidity distribution and we derive a closed set of coupled differential equations governing their time evolution. We check the accuracy of this theory by benchmarking the results against numerical simulations. Using this framework we are able to compute both the magnetization density and current dynamics, identifying some relations between the two. The problem of the anomalous power-law exponents identified in a previous work is discussed. Our work constitutes a theoretical approach that is able to describe the physics of non-reciprocal open quantum spin chains beyond analyses based on non-interacting fermions.

</details>


### [53] [Fragility of Optimal Measurements due to Noise in Probe States for Quantum Sensing](https://arxiv.org/abs/2601.08712)
*Andrew Kolmer Forbes,Marco A. Rodríguez-García,Ivan H. Deutsch*

Main category: quant-ph

TL;DR: 论文提出"Fisher信息脆弱性"概念，量化噪声下Fisher信息的损失，并建立框架设计更稳健的POVM以实现量子计量优势。


<details>
  <summary>Details</summary>
Motivation: 在量子参数估计中，理想纯态和酉编码下，经典Fisher信息(CFI)可能出现不连续性而量子Fisher信息(QFI)不会。这些不连续性在噪声环境中会导致信息损失，需要量化这种脆弱性并设计更稳健的测量方案。

Method: 提出Fisher信息"脆弱性"概念来量化噪声下的信息损失，利用Jensen不等式建立理论框架理解不连续性如何增加脆弱性，并基于此框架设计更稳健的POVM测量方案。

Result: 建立了不连续性与Fisher信息脆弱性之间的理论联系，展示了如何利用该框架设计对噪声更不敏感的POVM，从而在量子计量中实现更稳健的量子优势。

Conclusion: CFI中的不连续性是量子计量中的重要特征，它们量化了噪声环境下的信息损失。通过理解这种"脆弱性"并设计相应稳健的POVM，可以提升量子计量在实际噪声环境中的性能。

Abstract: For a given quantum state used in sensing, the quantum Cramér-Rao bound (QCRB) sets a fundamental limit on the precision achievable by an unbiased estimator of an unknown parameter, determined by the inverse of the quantum Fisher information (QFI). The QFI serves as an upper bound on the classical Fisher information (CFI), representing the maximum extractable information about the unknown parameter from measurements on a physical system. Thus, a central goal in quantum parameter estimation is to find a measurement, described by a POVM, that saturates the QFI (achieves maximum CFI), and thereby achieves the QCRB. In the idealization that one uses pure states and unitary encodings for sensing, discontinuities can appear in the CFI but not the QFI. In this article, we demonstrate that these discontinuities are important features, quantifying how much Fisher information is lost in the presence of noise. We refer to this as the Fisher information "fragility". We present a simple framework for understanding how discontinuities increase fragility through Jensen's inequality, and demonstrate how one can use this framework to design more robust POVMs for quantum advantage in metrology.

</details>


### [54] [Kernel Learning for Regression via Quantum Annealing Based Spectral Sampling](https://arxiv.org/abs/2601.08724)
*Yasushi Hasegawa,Masayuki Ohzeki*

Main category: quant-ph

TL;DR: 提出一种量子退火在环核学习框架，将量子退火作为核函数学习的组成部分而非简单的蒙特卡洛采样替代品，通过受限玻尔兹曼机建模谱分布，利用量子退火生成随机傅里叶特征，构建数据自适应核进行回归。


<details>
  <summary>Details</summary>
Motivation: 量子退火设备在实际运行中存在噪声和有限温度效应，其输出可视为接近吉布斯-玻尔兹曼分布的随机样本。传统方法仅将量子退火作为蒙特卡洛采样的替代品，未能充分利用其特性来直接决定学习核函数。

Method: 基于Bochner定理，将平移不变核表示为谱分布的期望，用受限玻尔兹曼机建模谱分布，通过量子退火生成离散RBM样本，再通过高斯-伯努利变换映射到连续频率。使用得到的随机傅里叶特征构建数据自适应核，采用非负平方核权重进行Nadaraya-Watson回归，通过最小化留一法均方误差训练核参数。

Result: 在多个基准回归数据集上的实验表明，该方法降低了训练损失，改变了核矩阵结构，学习的核函数在R²和RMSE指标上优于基线高斯核Nadaraya-Watson回归。增加推理时的随机特征数量能进一步提升精度。

Conclusion: 提出的量子退火在环核学习框架成功地将量子退火集成到核函数学习中，不仅作为采样工具，而是直接决定学习核函数，在回归任务中表现出优于传统方法的性能。

Abstract: While quantum annealing (QA) has been developed for combinatorial optimization, practical QA devices operate at finite temperature and under noise, and their outputs can be regarded as stochastic samples close to a Gibbs--Boltzmann distribution. In this study, we propose a QA-in-the-loop kernel learning framework that integrates QA not merely as a substitute for Markov-chain Monte Carlo sampling but as a component that directly determines the learned kernel for regression. Based on Bochner's theorem, a shift-invariant kernel is represented as an expectation over a spectral distribution, and random Fourier features (RFF) approximate the kernel by sampling frequencies. We model the spectral distribution with a (multi-layer) restricted Boltzmann machine (RBM), generate discrete RBM samples using QA, and map them to continuous frequencies via a Gaussian--Bernoulli transformation. Using the resulting RFF, we construct a data-adaptive kernel and perform Nadaraya--Watson (NW) regression. Because the RFF approximation based on $\cos(\bmω^{\top}Δ\bm{x})$ can yield small negative values and cancellation across neighbors, the Nadaraya--Watson denominator $\sum_j k_{ij}$ may become close to zero. We therefore employ nonnegative squared-kernel weights $w_{ij}=k(\bm{x}_i,\bm{x}_j)^2$, which also enhances the contrast of kernel weights. The kernel parameters are trained by minimizing the leave-one-out NW mean squared error, and we additionally evaluate local linear regression with the same squared-kernel weights at inference. Experiments on multiple benchmark regression datasets demonstrate a decrease in training loss, accompanied by structural changes in the kernel matrix, and show that the learned kernel tends to improve $R^2$ and RMSE over the baseline Gaussian-kernel NW. Increasing the number of random features at inference further enhances accuracy.

</details>


### [55] [Enhancing classical simulation with noisy quantum devices](https://arxiv.org/abs/2601.08772)
*Ruiqi Zhang,Fuchuan Wei,Zhaohui Wei*

Main category: quant-ph

TL;DR: 提出NDE-CS协议，利用噪声量子设备增强经典模拟，通过从噪声硬件学习目标电路与Clifford电路的关系，大幅降低采样成本


<details>
  <summary>Details</summary>
Motivation: 随着量子设备规模和精度的提升，如何有效利用噪声硬件进行有意义的计算成为关键挑战。现有方法主要通过误差缓解或校正来从噪声输出恢复无噪声电路输出

Method: 引入NDE-CS协议，结合噪声量子硬件数据改进基于稳定器的经典蒙特卡洛模拟方法。使用目标电路的噪声执行和噪声Clifford电路来学习目标电路在现实噪声下如何用Clifford电路表达，然后在无噪声Clifford极限中重用学习到的关系

Result: 在Trotterized Ising电路上的数值模拟显示，NDE-CS相比底层纯经典蒙特卡洛方法实现了数量级采样成本降低，同时保持相同精度。与SPD方法相比，NDE-CS在系统规模扩展方面表现更优

Conclusion: NDE-CS作为一种可扩展的量子电路混合模拟方法，能够将噪声作为计算资产加以利用，为量子计算提供新的实用途径

Abstract: As quantum devices continue to improve in scale and precision, a central challenge is how to effectively utilize noisy hardware for meaningful computation. Most existing approaches aim to recover noiseless circuit outputs from noisy ones through error mitigation or correction. Here, we show that noisy quantum devices can be directly leveraged as computational resources to enhance the classical simulation of quantum circuits. We introduce the Noisy-device-enhanced Classical Simulation (NDE-CS) protocol, which improves stabilizer-based classical Monte Carlo simulation methods by incorporating data obtained from noisy quantum hardware. Specifically, NDE-CS uses noisy executions of a target circuit together with noisy Clifford circuits to learn how the target circuit can be expressed in terms of Clifford circuits under realistic noise. The same learned relation can then be reused in the noiseless Clifford limit, enabling accurate estimation of ideal expectation values with substantially reduced sampling cost. Numerical simulations on Trotterized Ising circuits demonstrate that NDE-CS achieves orders-of-magnitude reductions in sampling cost compared to the underlying purely classical Monte Carlo approaches from which it is derived, while maintaining the same accuracy. We also compare NDE-CS with Sparse Pauli Dynamics (SPD), a powerful classical framework capable of simulating quantum circuits at previously inaccessible scales, and provide an example where the cost of SPD scales exponentially with system size, while NDE-CS scales much more favorably. These results establish NDE-CS as a scalable hybrid simulation approach for quantum circuits, where noise can be harnessed as a computational asset.

</details>


### [56] [Single-Period Floquet Control of Bosonic Codes with Quantum Lattice Gates](https://arxiv.org/abs/2601.08782)
*Tangyou Huang,Lei Du,Lingzhen Guo*

Main category: quant-ph

TL;DR: 提出一种单周期内直接合成任意幺正算符的解析确定性Floquet方法，用于高效制备玻色子编码和实现逻辑门操作


<details>
  <summary>Details</summary>
Motivation: 现有Floquet协议需要数千个驱动周期的缓慢绝热过程，效率低下，需要克服这一瓶颈以实现高效的玻色子编码量子计算

Method: 引入解析确定性Floquet方法，单周期内直接合成任意幺正算符；利用量子晶格门分解量子电路为基本操作；充分利用约瑟夫森结的固有非线性

Result: 生成的相空间幺正系综再现Haar随机统计特性；从真空态制备多种原型玻色子编码；实现高保真度单量子比特逻辑门操作

Conclusion: 该方法绕过了传统绝热过程的瓶颈，为连续变量量子计算提供了实用的伪随机幺正算符和高效电路分解方案

Abstract: Bosonic codes constitute a promising route to fault-tolerant quantum computing. {Existing Floquet protocols enable analytical construction of bosonic codes but typically rely on slow adiabatic ramps with thousands of driving periods.} In this work, we circumvent this bottleneck by introducing an analytical and deterministic Floquet method that directly synthesizes arbitrary unitaries within a single period. The phase-space unitary ensembles generated by our approach reproduce the Haar-random statistics, enabling practical pseudorandom unitaries in continuous-variable systems. We prepare various prototypical bosonic codes from vacuum and implement single-qubit logical gates with high fidelities using quantum lattice gates. By harnessing the full intrinsic nonlinearity of Josephson junctions, quantum lattice gates decompose quantum circuits into primitive operations for efficient continuous-variable quantum computing.

</details>


### [57] [Optimal logical Bell measurements on stabilizer codes with linear optics](https://arxiv.org/abs/2601.08820)
*Simon D. Reiß,Peter van Loock*

Main category: quant-ph

TL;DR: 该论文证明了在稳定子码上，任何逻辑贝尔测量都可以映射到两个码中任意量子比特对上的单个物理贝尔测量，这为逻辑贝尔测量的成功概率提供了通用上界，并提出了达到该上界的方案。


<details>
  <summary>Details</summary>
Motivation: 贝尔测量在量子信息中至关重要，但在线性光学实验中无法完美实现。对于编码在物理光子量子比特中的逻辑量子比特，即使没有光子损耗，逻辑贝尔测量也存在成功率限制。需要理解逻辑贝尔测量的基本限制并设计达到这些限制的方案。

Method: 基于稳定子群理论，将逻辑贝尔测量映射到物理贝尔测量，证明任何逻辑贝尔测量都可以映射到两个码中任意量子比特对上的单个物理贝尔测量。提出了充分性准则来寻找方案，使得单个成功的物理贝尔测量总能通过适当调整后续物理测量获得完整的逻辑信息。

Result: 为逻辑贝尔测量的成功概率提供了通用上界，排除了仅从部分成功的物理线性光学贝尔测量中组合出完整逻辑稳定子信息的可能性。提出的方案对量子奇偶码、五量子比特码、标准和平移平面表面码、树码和七量子比特Steane码都达到了通用上界。

Conclusion: 该研究建立了逻辑贝尔测量的基本限制，并提供了达到这些限制的通用方法。基于稳定子群理论的方法适用于任何稳定子码，为量子通信、计算和纠错中的逻辑贝尔测量提供了理论基础和实用方案。

Abstract: Bell measurements (BMs) are ubiquitous in quantum information and technology. They are basic elements for quantum commmunication, computation, and error correction. In particular, when performed on logical qubits encoded in physical photonic qubits, they allow for a read-out of stabilizer syndrome information to enhance loss tolerance in qubit-state transmission and fusion. However, even in an ideal setting without photon loss, BMs cannot be done perfectly based on the simplest experimental toolbox of linear optics. Here we demonstrate that any logical BM on stabilizer codes can always be mapped onto a single physical BM perfomed on any qubit pair from the two codes. As a necessary condition for the success of a logical BM, this provides a general upper bound on its success probability, especially ruling out the possibility that the stabilizer information obtainable from only partially succeeding, physical linear-optics BMs could be combined into the full logical stabilizer information. We formulate sufficient criteria to find schemes for which a single successful BM on the physical level will always allow to obtain the full logical information by suitably adapting the subsequent physical measurements. Our approach based on stabilizer group theory is generally applicable to any stabilizer code, which we demonstrate for quantum parity, five-qubit, standard and rotated planar surface, tree, and seven-qubit Steane codes. Our schemes attain the general upper bound for all these codes, while this bound had previously only been reached for the quantum parity code.

</details>


### [58] [Breaking the Orthogonality Barrier in Quantum LDPC Codes](https://arxiv.org/abs/2601.08824)
*Kenta Kasai*

Main category: quant-ph

TL;DR: 该论文提出了一种新型量子LDPC码构造方法，通过控制置换矩阵的可交换性并限制正交性约束，打破了传统量子LDPC码设计中正交性、正则性、围长和最小距离之间的权衡，成功构造出具有大围长且无传统距离上限的量子LDPC码。


<details>
  <summary>Details</summary>
Motivation: 经典LDPC码设计中，增加Tanner图的围长同时保持正则度分布能同时获得良好的BP译码性能和大的最小距离。但在量子LDPC码中，由于需要满足额外的正交性约束，这种原则不再适用。当同时强制正交性和正则性时，围长通常会减小，最小距离也会受到结构性的上限限制。

Method: 使用具有可控可交换性的置换矩阵，并将正交性约束限制在构造的必要部分，同时保持正则校验矩阵结构。这种方法打破了正交性、正则性、围长和最小距离之间的传统权衡。

Result: 成功构造了一个围长为8、(3,12)-正则的[[9216,4612,≤48]]量子LDPC码。在结合低复杂度后处理算法的BP译码下，该码在错误概率为4%的退极化信道上实现了低至10^-8的帧错误率。

Conclusion: 该研究突破了量子LDPC码设计中的传统限制，提出了一种能够同时获得大围长和无传统距离上限的构造方法，为高性能量子纠错码的设计提供了新途径。

Abstract: Classical low-density parity-check (LDPC) codes are a widely deployed and well-established technology, forming the backbone of modern communication and storage systems. It is well known that, in this classical setting, increasing the girth of the Tanner graph while maintaining regular degree distributions leads simultaneously to good belief-propagation (BP) decoding performance and large minimum distance. In the quantum setting, however, this principle does not directly apply because quantum LDPC codes must satisfy additional orthogonality constraints between their parity-check matrices. When one enforces both orthogonality and regularity in a straightforward manner, the girth is typically reduced and the minimum distance becomes structurally upper bounded.
  In this work, we overcome this limitation by using permutation matrices with controlled commutativity and by restricting the orthogonality constraints to only the necessary parts of the construction, while preserving regular check-matrix structures. This design breaks the conventional trade-off between orthogonality, regularity, girth, and minimum distance, allowing us to construct quantum LDPC codes with large girth and without the usual distance upper bounds. As a concrete demonstration, we construct a girth-8, (3,12)-regular $[[9216,4612, \leq 48]]$ quantum LDPC code and show that, under BP decoding combined with a low-complexity post-processing algorithm, it achieves a frame error rate as low as $10^{-8}$ on the depolarizing channel with error probability $4 \%$.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [59] [Hierarchical Sparse Plus Low Rank Compression of LLM](https://arxiv.org/abs/2601.07839)
*Pawan Kumar,Aditi Gupta*

Main category: cs.LG

TL;DR: 提出HSS压缩方法：先移除大权重到稀疏矩阵，再对残差矩阵应用递归层次稀疏低秩分解，通过递归降秩和RCM排列优化压缩效果，硬件友好且可端到端训练。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型对内存和计算资源需求巨大，需要有效的压缩方法以便部署和持续训练。

Method: 两阶段压缩方案：1) 将最大幅度权重移除到稀疏矩阵S；2) 对稠密残差矩阵应用递归层次稀疏可分离低秩分解，采用递归降秩策略和RCM排列优化压缩。

Result: 在LLaMA-7B上实验，仅压缩自注意力投影矩阵（1.6B参数）即可实现显著内存节省，在WikiText测试集上保持可比困惑度。30%稀疏预算和512外秩时，sHSS-RCM困惑度1.64，优于稠密基线和传统稀疏+SVD变体。

Conclusion: HSS压缩方法能有效减少大语言模型的内存和计算需求，同时保持模型性能，为模型部署和持续训练提供了实用的压缩方案。

Abstract: Modern large language models (LLMs) place extraordinary pressure on memory and compute budgets, making principled compression indispensable for both deployment and continued training. We present Hierarchical Sparse Plus Low-Rank (HSS) compression, a two-stage scheme that (i) removes the largest-magnitude weights into a sparse matrix S and (ii) applies a recursive Hierarchically Sparse Separable (HSS) low-rank factorisation to the dense residual matrix. A recursive rank-reducing strategy and a reverse Cuthill-Mckee (RCM) permutation are introduced to align high weights towards the diagonal with the block-diagonal hierarchy, maximising off-diagonal compressibility (because they are touched only once). HSS is hardware-friendly: its matrix-vector multiply reduces to one sparse and a sequence of thin-matrix multiplications and can be trained end-to-end with standard optimisers.
  Experiments on LLaMA-7B show that targeting only the self-attention projections (1.6 B parameters of Q, K, and V matrices out of a total 7B parameters) suffices to yield large memory savings while retaining comparable state-of-the-art perplexity scores on test samples of the WikiText dataset. For example, with a 30\% sparsity budget and an outer rank of 512, sHSS-RCM achieves a perplexity of 1.64, outperforming dense baselines and classical sparse-plus-SVD variants, while also achieving significant memory savings.

</details>


### [60] [Affect and Effect: Limitations of regularisation-based continual learning in EEG-based emotion classification](https://arxiv.org/abs/2601.07858)
*Nina Peire,Yupei Li,Björn Schuller*

Main category: cs.LG

TL;DR: 基于EEG的情感分类中，正则化持续学习方法对未见受试者的泛化能力有限，主要因为稳定性-可塑性权衡失配，且前向迁移无显著改善。


<details>
  <summary>Details</summary>
Motivation: EEG情感分类面临受试者间和受试者内高变异性挑战，持续学习（CL）是潜在解决方案。但现有正则化CL方法（如EWC、SI、MAS）在EEG问题中的适用性尚未充分探索。

Method: 从理论和实证两方面分析正则化CL方法在DREAMER和SEED数据集上的表现。研究在受试者增量序列下的局限性，包括参数重要性估计、梯度干扰、约束过度和顺序敏感性等问题。

Result: 正则化CL方法在EEG情感分类中表现有限：1）参数重要性估计在噪声数据和协变量偏移下不可靠；2）重要参数的梯度更新干扰新受试者学习；3）跨任务重要性值过度约束模型；4）性能对受试者顺序敏感。前向迁移相比顺序微调无显著改善。

Conclusion: EEG信号的高变异性导致过去受试者对未来的价值有限。正则化持续学习方法在EEG情感分类中对未见受试者的鲁棒泛化能力受限，需要更适合EEG特性的新方法。

Abstract: Generalisation to unseen subjects in EEG-based emotion classification remains a challenge due to high inter-and intra-subject variability. Continual learning (CL) poses a promising solution by learning from a sequence of tasks while mitigating catastrophic forgetting. Regularisation-based CL approaches, such as Elastic Weight Consolidation (EWC), Synaptic Intelligence (SI), and Memory Aware Synapses (MAS), are commonly used as baselines in EEG-based CL studies, yet their suitability for this problem remains underexplored. This study theoretically and empirically finds that regularisation-based CL methods show limited performance for EEG-based emotion classification on the DREAMER and SEED datasets. We identify a fundamental misalignment in the stability-plasticity trade-off, where regularisation-based methods prioritise mitigating catastrophic forgetting (backward transfer) over adapting to new subjects (forward transfer). We investigate this limitation under subject-incremental sequences and observe that: (1) the heuristics for estimating parameter importance become less reliable under noisy data and covariate shift, (2) gradients on parameters deemed important by these heuristics often interfere with gradient updates required for new subjects, moving optimisation away from the minimum, (3) importance values accumulated across tasks over-constrain the model, and (4) performance is sensitive to subject order. Forward transfer showed no statistically significant improvement over sequential fine-tuning (p > 0.05 across approaches and datasets). The high variability of EEG signals means past subjects provide limited value to future subjects. Regularisation-based continual learning approaches are therefore limited for robust generalisation to unseen subjects in EEG-based emotion classification.

</details>


### [61] [RewriteNets: End-to-End Trainable String-Rewriting for Generative Sequence Modeling](https://arxiv.org/abs/2601.07868)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: RewriteNets：一种基于显式并行字符串重写的新型神经网络架构，通过可学习规则实现序列转换，在需要系统性泛化的任务上表现优异，计算效率高于Transformer。


<details>
  <summary>Details</summary>
Motivation: 主流序列模型（如Transformer）通过密集注意力权重隐式表示结构，导致二次复杂度。需要一种具有显式结构归纳偏置、计算效率更高的替代架构。

Method: 提出RewriteNets架构，每层包含可学习规则集。对输入序列每个位置执行四个操作：1) 规则模式的模糊匹配；2) 通过可微分分配算子选择非重叠重写；3) 应用选定规则替换输入段（可不同长度）；4) 传播未处理标记。使用Gumbel-Sinkhorn估计器实现端到端训练。

Result: 在算法、组合和字符串操作任务上评估，相比LSTM和Transformer基线表现优异。在SCAN基准的长度分割上达到98.7%准确率，计算效率高于Transformer。分析显示学习到的规则具有可解释性。

Conclusion: RewriteNets为具有显式结构归纳偏置的序列建模提供了有前景的方向，在系统性泛化任务上表现突出，计算效率更高，规则可解释性强。

Abstract: Dominant sequence models like the Transformer represent structure implicitly through dense attention weights, incurring quadratic complexity. We propose RewriteNets, a novel neural architecture built on an alternative paradigm: explicit, parallel string rewriting. Each layer in a RewriteNet contains a set of learnable rules. For each position in an input sequence, the layer performs four operations: (1) fuzzy matching of rule patterns, (2) conflict resolution via a differentiable assignment operator to select non-overlapping rewrites, (3) application of the chosen rules to replace input segments with output segments of potentially different lengths, and (4) propagation of untouched tokens. While the discrete assignment of rules is non-differentiable, we employ a straight-through Gumbel-Sinkhorn estimator, enabling stable end-to-end training. We evaluate RewriteNets on algorithmic, compositional, and string manipulation tasks, comparing them against strong LSTM and Transformer baselines. Results show that RewriteNets excel at tasks requiring systematic generalization (achieving 98.7% accuracy on the SCAN benchmark's length split) and are computationally more efficient than Transformers. We also provide an analysis of learned rules and an extensive ablation study, demonstrating that this architecture presents a promising direction for sequence modeling with explicit structural inductive biases.

</details>


### [62] [HOSC: A Periodic Activation with Saturation Control for High-Fidelity Implicit Neural Representations](https://arxiv.org/abs/2601.07870)
*Michal Jan Wlodarczyk,Danzel Serrano,Przemyslaw Musialski*

Main category: cs.LG

TL;DR: 提出HOSC激活函数，通过tanh(sin(x))结构控制Lipschitz边界，解决周期性激活的梯度不稳定问题，在多种隐式神经表示任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 周期性激活函数（如sine）在隐式神经表示中能保留高频信息，但存在梯度不稳定和难以控制多尺度行为的问题，需要一种更稳定可控的激活函数。

Method: 提出HOSC激活函数：tanh(βsin(ω₀x))，通过参数β显式控制激活函数的Lipschitz边界（βω₀），在保持周期性载波的同时调节梯度幅度。

Result: 在图像、音频、视频、NeRF和SDF等任务上进行了标准化训练协议的全面实验，相比SIREN、FINER等方法，HOSC在某些领域有显著优势，在其他领域达到竞争性水平。

Conclusion: HOSC是一种实用的周期性激活函数，适用于隐式神经表示应用，提供了领域特定的超参数选择指导，解决了梯度不稳定问题。

Abstract: Periodic activations such as sine preserve high-frequency information in implicit neural representations (INRs) through their oscillatory structure, but often suffer from gradient instability and limited control over multi-scale behavior. We introduce the Hyperbolic Oscillator with Saturation Control (HOSC) activation, $\text{HOSC}(x) = \tanh\bigl(β\sin(ω_0 x)\bigr)$, which exposes an explicit parameter $β$ that controls the Lipschitz bound of the activation by $βω_0$. This provides a direct mechanism to tune gradient magnitudes while retaining a periodic carrier. We provide a mathematical analysis and conduct a comprehensive empirical study across images, audio, video, NeRFs, and SDFs using standardized training protocols. Comparative analysis against SIREN, FINER, and related methods shows where HOSC provides substantial benefits and where it achieves competitive parity. Results establish HOSC as a practical periodic activation for INR applications, with domain-specific guidance on hyperparameter selection. For code visit the project page https://hosc-nn.github.io/ .

</details>


### [63] [Multiplicative Orthogonal Sequential Editing for Language Models](https://arxiv.org/abs/2601.07873)
*Hao-Xiang Xu,Jun-Yu Ma,Ziqi Peng,Yuhao Sun,Zhen-Hua Ling,Jia-Chen Gu*

Main category: cs.LG

TL;DR: 提出了一种新的乘法正交序列编辑（MOSE）方法，通过正交矩阵乘法而非传统的加法更新来编辑大语言模型的知识，保持矩阵数值稳定性，提升序列编辑性能并保留通用能力。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法主要采用加法更新范式，即在原始参数矩阵上添加更新矩阵。研究表明这会损害矩阵的数值稳定性指标（如条件数和范数），降低编辑性能和通用能力，特别是在序列编辑场景中。虽然后续方法有所改进，但仍局限于加法框架，未能从根本上解决这一问题。

Method: 从统计和数学角度分析得出：将原始矩阵乘以正交矩阵不会改变矩阵的数值稳定性。基于此，提出了乘法正交序列编辑（MOSE）方法。首先推导出乘法形式的矩阵更新，然后将新知识融入正交矩阵中，该正交矩阵与原始参数矩阵相乘。这种方法保持了编辑后矩阵的数值稳定性不变。

Result: 在三种不同大语言模型上系统评估了编辑性能和通用能力。实验结果表明，MOSE有效限制了编辑参数矩阵的偏差并保持其数值稳定性。相比现有方法，MOSE在序列编辑性能上提升了12.08%，同时在下游任务中保留了95.73%的通用能力。

Conclusion: 提出的乘法正交序列编辑（MOSE）方法通过正交矩阵乘法而非传统的加法更新，从根本上解决了知识编辑中数值稳定性受损的问题，显著提升了序列编辑性能并有效保留了模型的通用能力。

Abstract: Knowledge editing aims to efficiently modify the internal knowledge of large language models (LLMs) without compromising their other capabilities. The prevailing editing paradigm, which appends an update matrix to the original parameter matrix, has been shown by some studies to damage key numerical stability indicators (such as condition number and norm), thereby reducing editing performance and general abilities, especially in sequential editing scenario. Although subsequent methods have made some improvements, they remain within the additive framework and have not fundamentally addressed this limitation. To solve this problem, we analyze it from both statistical and mathematical perspectives and conclude that multiplying the original matrix by an orthogonal matrix does not change the numerical stability of the matrix. Inspired by this, different from the previous additive editing paradigm, a multiplicative editing paradigm termed Multiplicative Orthogonal Sequential Editing (MOSE) is proposed. Specifically, we first derive the matrix update in the multiplicative form, the new knowledge is then incorporated into an orthogonal matrix, which is multiplied by the original parameter matrix. In this way, the numerical stability of the edited matrix is unchanged, thereby maintaining editing performance and general abilities. We compared MOSE with several current knowledge editing methods, systematically evaluating their impact on both editing performance and the general abilities across three different LLMs. Experimental results show that MOSE effectively limits deviations in the edited parameter matrix and maintains its numerical stability. Compared to current methods, MOSE achieves a 12.08% improvement in sequential editing performance, while retaining 95.73% of general abilities across downstream tasks. The code is available at https://github.com/famoustourist/MOSE.

</details>


### [64] [NOVAK: Unified adaptive optimizer for deep neural networks](https://arxiv.org/abs/2601.07876)
*Sergii Kavun*

Main category: cs.LG

TL;DR: NOVAK是一个模块化梯度优化算法，集成了自适应矩估计、校正学习率调度、解耦权重正则化、多种Nesterov动量变体和前瞻同步，在多个基准测试中优于14种现有优化器。


<details>
  <summary>Details</summary>
Motivation: 现有自适应优化方法在训练缺乏跳跃连接的深度普通网络时存在局限性，需要一种更鲁棒、性能更好的优化框架来解决这一问题。

Method: NOVAK采用双模式架构，包含高效CUDA内核，集成了校正自适应学习率、内存高效的前瞻机制（将开销从O(2p)降至O(p + p/k)）、解耦权重正则化和混合动量等多种优化组件。

Result: 在CIFAR-10、CIFAR-100、ImageNet和ImageNette上优于Adam、AdamW、RAdam、Lion、Adan等14种优化器，在ResNet-50、VGG-16、ViT等架构上达到SOTA准确率，在VGG-16/ImageNette上表现出卓越的架构鲁棒性。

Conclusion: NOVAK的架构贡献（特别是校正、解耦衰减和混合动量）对于可靠训练缺乏跳跃连接的深度普通网络至关重要，解决了现有自适应优化方法的长期局限性。

Abstract: This work introduces NOVAK, a modular gradient-based optimization algorithm that integrates adaptive moment estimation, rectified learning-rate scheduling, decoupled weight regularization, multiple variants of Nesterov momentum, and lookahead synchronization into a unified, performance-oriented framework. NOVAK adopts a dual-mode architecture consisting of a streamlined fast path designed for production. The optimizer employs custom CUDA kernels that deliver substantial speedups (3-5 for critical operations) while preserving numerical stability under standard stochastic-optimization assumptions. We provide fully developed mathematical formulations for rectified adaptive learning rates, a memory-efficient lookahead mechanism that reduces overhead from O(2p) to O(p + p/k), and the synergistic coupling of complementary optimization components. Theoretical analysis establishes convergence guarantees and elucidates the stability and variance-reduction properties of the method. Extensive empirical evaluation on CIFAR-10, CIFAR-100, ImageNet, and ImageNette demonstrates NOVAK superiority over 14 contemporary optimizers, including Adam, AdamW, RAdam, Lion, and Adan. Across architectures such as ResNet-50, VGG-16, and ViT, NOVAK consistently achieves state-of-the-art accuracy, and exceptional robustness, attaining very high accuracy on VGG-16/ImageNette demonstrating superior architectural robustness compared to contemporary optimizers. The results highlight that NOVAKs architectural contributions (particularly rectification, decoupled decay, and hybrid momentum) are crucial for reliable training of deep plain networks lacking skip connections, addressing a long-standing limitation of existing adaptive optimization methods.

</details>


### [65] [E^2-LLM: Bridging Neural Signals and Interpretable Affective Analysis](https://arxiv.org/abs/2601.07877)
*Fei Ma,Han Lin,Yifan Xie,Hongwei Ren,Xiaoyu Shen,Wenbo Ding,Qi Tian*

Main category: cs.LG

TL;DR: E^2-LLM：首个用于脑电信号可解释情感分析的多模态大语言模型框架，通过多阶段训练实现情感分类和推理


<details>
  <summary>Details</summary>
Motivation: 脑电信号情感识别面临主体间差异大、标注数据有限、现有方法缺乏可解释性等挑战，而现有的多模态大语言模型尚未适应神经信号的时空特性

Method: 集成预训练的脑电编码器与基于Qwen的大语言模型，通过可学习的投影层连接，采用包含情感判别预训练、跨模态对齐和思维链指令调优的多阶段训练流程

Result: 在七种情感类别的数据集上，E^2-LLM在情感分类上表现优异，更大变体显示出更高的可靠性和对复杂推理场景的零样本泛化能力

Conclusion: 建立了生理信号与大语言模型推理能力结合的新范式，表明模型扩展能同时提高识别准确率和可解释的情感理解能力

Abstract: Emotion recognition from electroencephalography (EEG) signals remains challenging due to high inter-subject variability, limited labeled data, and the lack of interpretable reasoning in existing approaches. While recent multimodal large language models (MLLMs) have advanced emotion analysis, they have not been adapted to handle the unique spatiotemporal characteristics of neural signals. We present E^2-LLM (EEG-to-Emotion Large Language Model), the first MLLM framework for interpretable emotion analysis from EEG. E^2-LLM integrates a pretrained EEG encoder with Qwen-based LLMs through learnable projection layers, employing a multi-stage training pipeline that encompasses emotion-discriminative pretraining, cross-modal alignment, and instruction tuning with chain-of-thought reasoning. We design a comprehensive evaluation protocol covering basic emotion prediction, multi-task reasoning, and zero-shot scenario understanding. Experiments on the dataset across seven emotion categories demonstrate that E^2-LLM achieves excellent performance on emotion classification, with larger variants showing enhanced reliability and superior zero-shot generalization to complex reasoning scenarios. Our work establishes a new paradigm combining physiological signals with LLM reasoning capabilities, showing that model scaling improves both recognition accuracy and interpretable emotional understanding in affective computing.

</details>


### [66] [Sliced-Wasserstein Distribution Alignment Loss Improves the Ultra-Low-Bit Quantization of Large Language Models](https://arxiv.org/abs/2601.07878)
*Deyu Cao,Yixin Yin,Samin Aref*

Main category: cs.LG

TL;DR: 提出一种基于切片Wasserstein损失函数的分布感知校准方法，用于超低位后训练量化，通过对齐全精度和量化模型在随机线性投影下的输出分布，提升量化性能而不增加推理开销。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型部署时存在资源使用效率低下的问题，导致高昂的经济和环境成本。模型量化虽能改善能效和内存效率，但压缩到4位以下会扭曲激活分布并降低性能，需要解决超低位量化的性能退化问题。

Method: 引入切片Wasserstein损失函数进行分布感知校准，该损失函数在随机线性投影下对齐全精度和量化模型的输出分布，与标准均方误差损失互补，不增加推理时的计算开销。该方法可与任何具有重训练组件的后训练量化框架集成。

Result: 在OmniQuant和TesseraQ两种前沿方法上验证，提出的损失函数在多个超低位设置下持续改善困惑度和下游任务准确率。对LLaMA-2-7B恢复了OmniQuant损失的4.12-20.37%准确率，对OPT-6.7B恢复0.93-7.65%，对LLaMA-2-13B恢复2.26-6.20%。TesseraQ的准确率退化相对恢复了3.63-7.63%。

Conclusion: 分布对齐提供了一种简单而有效的性能提升方法，能够推动前沿量化方法的极限。该方法已开源，便于未来超低位量化研究的进展。

Abstract: The benefits of most large language models come with steep and often hidden economic and environmental costs due to their resource usage inefficiency during deployment. Model quantization improves energy and memory efficiency through representing model parameters by lower-precision values. However, compression below 4-bits often distorts activation distributions and degrades performance. We address this challenge by introducing a sliced Wasserstein loss function for distribution-aware calibration in ultra-low-bit post-training quantization. The proposed loss aligns the output distributions of full-precision and quantized models under random linear projections, complementing standard mean-squared error loss without adding any computational overhead during inference. Our proposed loss function can be incorporated with any post-training quantization framework that has a retraining component. We demonstrate the performance gains of our proposed model by incorporating it with two frontier methods known as OmniQuant and TesseraQ. Compared to these two baselines, the proposed loss consistently improves both perplexity and downstream task accuracy across multiple ultra-low-bit settings. Our proposed loss function recovers 4.12-20.37% of the OmniQuant's lost accuracy on the language model LLaMA-2-7B, 0.93-7.65% on OPT-6.7B, and 2.26-6.20% on LLaMA-2-13B. TesseraQ's accuracy degradation is recovered by 3.63-7.63% in relative terms when augmented by our proposed loss function. Taken together, these results demonstrate that distributional alignment provides a simple yet effective performance boost that can push the limits of frontier quantization methods. Our method is available on GitHub to facilitate future progress in ultra-low-bit quantization.

</details>


### [67] [Max-Min Neural Network Operators For Approximation of Multivariate Functions](https://arxiv.org/abs/2601.07886)
*Abhishek Yadav,Uaday Singh,Feng Dai*

Main category: cs.LG

TL;DR: 提出多元最大-最小神经网络算子框架，基于单变量算子扩展，建立点态和一致收敛定理，通过连续模和广义绝对矩获得定量估计，证明多元最大-最小结构在理论和应用中的高效稳定逼近能力。


<details>
  <summary>Details</summary>
Motivation: 基于神经网络算子逼近理论的最新进展，特别是单变量最大-最小算子的成功，需要将其扩展到多元情形，以提供代数优雅且高效稳定的逼近工具。

Method: 提出新的多元算子，激活函数使用sigmoidal函数，建立点态和一致收敛定理，通过连续模和多元广义绝对矩推导逼近阶的定量估计。

Result: 证明了多元最大-最小算子结构不仅代数优雅，而且在理论和应用环境中都能提供高效稳定的逼近工具。

Conclusion: 多元最大-最小神经网络算子框架是有效的逼近工具，具有理论严谨性和实际应用价值，为神经网络逼近理论提供了新的多元扩展。

Abstract: In this paper, we develop a multivariate framework for approximation by max-min neural network operators. Building on the recent advances in approximation theory by neural network operators, particularly, the univariate max-min operators, we propose and analyze new multivariate operators activated by sigmoidal functions. We establish pointwise and uniform convergence theorems and derive quantitative estimates for the order of approximation via modulus of continuity and multivariate generalized absolute moment. Our results demonstrate that multivariate max-min structure of operators, besides their algebraic elegance, provide efficient and stable approximation tools in both theoretical and applied settings.

</details>


### [68] [KVzap: Fast, Adaptive, and Faithful KV Cache Pruning](https://arxiv.org/abs/2601.07891)
*Simon Jegou,Maximilian Jeblick*

Main category: cs.LG

TL;DR: KVzap是一种快速、输入自适应的KV缓存压缩方法，在预填充和解码阶段都能工作，在多个大模型上实现2-4倍压缩且精度损失可忽略


<details>
  <summary>Details</summary>
Motivation: 随着transformer语言模型上下文长度增长，KV缓存已成为推理的关键瓶颈。现有KV缓存剪枝方法因速度-精度权衡问题尚未被主流推理引擎采用

Method: KVzap是KVzip的快速、输入自适应近似方法，可在预填充和解码阶段工作，实现KV缓存的高效压缩

Result: 在Qwen3-8B、Llama-3.1-8B-Instruct和Qwen3-32B等模型的长上下文和推理任务中，KVzap实现2-4倍KV缓存压缩，精度损失可忽略，在KVpress排行榜上达到SOTA性能

Conclusion: KVzap提供了一种高效的KV缓存压缩解决方案，在保持精度的同时显著减少内存占用，有望解决大模型推理中的KV缓存瓶颈问题

Abstract: Growing context lengths in transformer-based language models have made the key-value (KV) cache a critical inference bottleneck. While many KV cache pruning methods have been proposed, they have not yet been adopted in major inference engines due to speed--accuracy trade-offs. We introduce KVzap, a fast, input-adaptive approximation of KVzip that works in both prefilling and decoding. On Qwen3-8B, Llama-3.1-8B-Instruct, and Qwen3-32B across long-context and reasoning tasks, KVzap achieves $2$--$4\times$ KV cache compression with negligible accuracy loss and achieves state-of-the-art performance on the KVpress leaderboard. Code and models are available at https://github.com/NVIDIA/kvpress.

</details>


### [69] [Sherry: Hardware-Efficient 1.25-Bit Ternary Quantization via Fine-grained Sparsification](https://arxiv.org/abs/2601.07892)
*Hong Huang,Decheng Wu,Qiangqiang Hu,Guanghua Yu,Jinhai Yang,Jianchen Zhu,Xue Liu,Dapeng Wu*

Main category: cs.LG

TL;DR: Sherry提出了一种硬件高效的三元量化框架，通过3:4细粒度稀疏实现1.25位宽度的正则化，解决了现有方法在2位对齐打包和1.67位不规则打包之间的权衡问题，在保持性能的同时显著减少模型大小并提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在资源受限的边缘设备上部署面临内存和计算需求过高的挑战。现有三元量化方法存在硬件不匹配问题：2位对齐打包造成比特浪费，1.67位不规则打包降低推理速度。

Method: 1) 提出3:4细粒度稀疏模式，将四个权重打包到五个比特中，实现1.25位宽度的正则化并恢复2的幂次对齐；2) 引入Arenas（退火残差突触机制），解决稀疏三元训练中的权重陷阱问题，保持表示多样性。

Result: 在LLaMA-3.2上的五个基准测试表明，Sherry在保持最先进三元性能的同时显著减少模型大小。在Intel i7-14700HX CPU上，1B模型相比SOTA基线实现零精度损失，同时节省25%比特并提升10%速度。

Conclusion: Sherry框架成功解决了三元量化中的硬件效率问题，通过创新的稀疏模式和训练机制，在保持模型性能的同时实现了显著的存储节省和推理加速，为边缘设备部署LLM提供了实用解决方案。

Abstract: The deployment of Large Language Models (LLMs) on resource-constrained edge devices is increasingly hindered by prohibitive memory and computational requirements. While ternary quantization offers a compelling solution by reducing weights to {-1, 0, +1}, current implementations suffer from a fundamental misalignment with commodity hardware. Most existing methods must choose between 2-bit aligned packing, which incurs significant bit wastage, or 1.67-bit irregular packing, which degrades inference speed. To resolve this tension, we propose Sherry, a hardware-efficient ternary quantization framework. Sherry introduces a 3:4 fine-grained sparsity that achieves a regularized 1.25-bit width by packing blocks of four weights into five bits, restoring power-of-two alignment. Furthermore, we identify weight trapping issue in sparse ternary training, which leads to representational collapse. To address this, Sherry introduces Arenas, an annealing residual synapse mechanism that maintains representational diversity during training. Empirical evaluations on LLaMA-3.2 across five benchmarks demonstrate that Sherry matches state-of-the-art ternary performance while significantly reducing model size. Notably, on an Intel i7-14700HX CPU, our 1B model achieves zero accuracy loss compared to SOTA baselines while providing 25% bit savings and 10% speed up. The code is available at https://github.com/Tencent/AngelSlim .

</details>


### [70] [Revealing the Attention Floating Mechanism in Masked Diffusion Models](https://arxiv.org/abs/2601.07894)
*Xin Dai,Pengcheng Huang,Zhenghao Liu,Shuo Wang,Yukun Yan,Chaojun Xiao,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.LG

TL;DR: 论文研究了掩码扩散模型中的注意力机制，发现了"注意力浮动"现象，揭示了MDMs与自回归模型不同的注意力模式，这解释了MDMs在上下文学习中的优势。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型在性能上逐渐接近自回归模型，但其内部注意力机制尚未得到充分探索。本文旨在深入研究MDMs的注意力行为，揭示其独特的工作机制。

Method: 通过分析MDMs中的注意力行为，发现了"注意力浮动"现象。进一步研究了其"浅层结构感知、深层内容聚焦"的注意力机制：浅层利用浮动token构建全局结构框架，深层则更专注于语义内容。

Result: MDMs展现出动态、分散的注意力锚点，与ARMs的固定注意力汇聚点不同。这种独特的注意力模式为MDMs强大的上下文学习能力提供了机制解释，使其在知识密集型任务中的性能比ARMs提升一倍。

Conclusion: 注意力浮动现象揭示了MDMs与ARMs在注意力机制上的本质差异，这解释了MDMs在上下文学习中的卓越表现，为理解扩散模型的内部工作机制提供了新视角。

Abstract: Masked diffusion models (MDMs), which leverage bidirectional attention and a denoising process, are narrowing the performance gap with autoregressive models (ARMs). However, their internal attention mechanisms remain under-explored. This paper investigates the attention behaviors in MDMs, revealing the phenomenon of Attention Floating. Unlike ARMs, where attention converges to a fixed sink, MDMs exhibit dynamic, dispersed attention anchors that shift across denoising steps and layers. Further analysis reveals its Shallow Structure-Aware, Deep Content-Focused attention mechanism: shallow layers utilize floating tokens to build a global structural framework, while deeper layers allocate more capability toward capturing semantic content. Empirically, this distinctive attention pattern provides a mechanistic explanation for the strong in-context learning capabilities of MDMs, allowing them to double the performance compared to ARMs in knowledge-intensive tasks. All codes and datasets are available at https://github.com/NEUIR/Attention-Floating.

</details>


### [71] [Large Language Models and Algorithm Execution: Application to an Arithmetic Function](https://arxiv.org/abs/2601.07898)
*Farah Ben Slama,Frédéric Armetta*

Main category: cs.LG

TL;DR: LLM-DAL训练方法通过推理分解显著提升大语言模型执行复杂算法推理和泛化的能力


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然功能强大，但在内化处理数据和自主执行算法方面存在局限，需要探索通过专门训练来扩展其算法执行能力

Method: 提出LLM-DAL（大语言模型-分解算法学习）训练模型，采用专注于推理分解的专门监督训练方法，引导模型学习过程

Result: 当训练方法设计得当以指导模型学习过程时，LLMs执行复杂算法推理和泛化的能力可以显著提升

Conclusion: 通过专门设计的推理分解训练方法，可以扩展大语言模型的算法执行能力，克服其在内化数据和自主执行算法方面的局限

Abstract: Large Language Models (LLMs) have recently developed new advanced functionalities. Their effectiveness relies on statistical learning and generalization capabilities. However, they face limitations in internalizing the data they process and struggle, for instance, to autonomously execute algorithms. In this paper, we investigate the possibility of extending these models' capabilities to algorithm execution through specialized supervised training focused on reasoning decomposition. We introduce a training model called LLM-DAL (Large Language Model - Decompositional Algorithmic Learning), through which we demonstrate that LLMs' ability to perform complex algorithmic inferences and generalize can be significantly improved when the training method is properly designed to guide the model in its learning process.

</details>


### [72] [Enhancing Large Language Models for Time-Series Forecasting via Vector-Injected In-Context Learning](https://arxiv.org/abs/2601.07903)
*Jianqi Zhang,Jingyao Wang,Wenwen Qiang,Fanjiang Xu,Changwen Zheng*

Main category: cs.LG

TL;DR: 提出LVICL方法，通过向量注入的上下文学习来提升冻结参数LLM的时间序列预测性能，同时减少计算开销


<details>
  <summary>Details</summary>
Motivation: LLM直接应用于时间序列预测存在预训练语料与时间序列数据差异大的问题，微调LLM计算开销大，需要在保持预测性能的同时降低计算成本

Method: 提出LVICL方法，使用可学习的上下文向量适配器从多个示例中自适应提取包含压缩信息的上下文向量，然后将该向量注入到冻结LLM的每一层中

Result: 实验证明该方法有效，相比传统上下文学习不增加提示长度，且能抑制对预测有害的组件，提升模型性能

Conclusion: LVICL方法通过向量注入的上下文学习，在冻结LLM参数的情况下提升了时间序列预测性能，解决了预测性能与计算开销的双重挑战

Abstract: The World Wide Web needs reliable predictive capabilities to respond to changes in user behavior and usage patterns. Time series forecasting (TSF) is a key means to achieve this goal. In recent years, the large language models (LLMs) for TSF (LLM4TSF) have achieved good performance. However, there is a significant difference between pretraining corpora and time series data, making it hard to guarantee forecasting quality when directly applying LLMs to TSF; fine-tuning LLMs can mitigate this issue, but often incurs substantial computational overhead. Thus, LLM4TSF faces a dual challenge of prediction performance and compute overhead. To address this, we aim to explore a method for improving the forecasting performance of LLM4TSF while freezing all LLM parameters to reduce computational overhead. Inspired by in-context learning (ICL), we propose LVICL. LVICL uses our vector-injected ICL to inject example information into a frozen LLM, eliciting its in-context learning ability and thereby enhancing its performance on the example-related task (i.e., TSF). Specifically, we first use the LLM together with a learnable context vector adapter to extract a context vector from multiple examples adaptively. This vector contains compressed, example-related information. Subsequently, during the forward pass, we inject this vector into every layer of the LLM to improve forecasting performance. Compared with conventional ICL that adds examples into the prompt, our vector-injected ICL does not increase prompt length; moreover, adaptively deriving a context vector from examples suppresses components harmful to forecasting, thereby improving model performance. Extensive experiments demonstrate the effectiveness of our approach.

</details>


### [73] [Transformer-Based Approach for Automated Functional Group Replacement in Chemical Compounds](https://arxiv.org/abs/2601.07930)
*Bo Pan,Zhiping Zhang,Kevin Spiekermann,Tianchi Chen,Xiang Yu,Liying Zhang,Liang Zhao*

Main category: cs.LG

TL;DR: 提出一种新颖的两阶段Transformer模型，用于功能基团的移除和替换，通过顺序生成确保严格的子结构级修改，相比传统规则方法和单步模型具有更好的化学有效性和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统功能基团替换方法依赖基于规则的启发式方法，在生成多样新颖化学结构方面有限。现有Transformer方法通常专注于单步建模，缺乏结构相似性保证，需要更精确可控的分子转换方法。

Method: 开发两阶段Transformer模型，顺序生成要移除和添加的功能基团，确保严格的子结构级修改。使用ChEMBL衍生的匹配分子对（MMPs）数据集，训练基于SMIRKS表示的编码器-解码器Transformer模型来有效捕捉转换规则。

Result: 广泛评估表明该方法能够生成化学有效的转换，探索多样化学空间，并在不同搜索规模下保持可扩展性，相比传统方法在准确性和多样性方面表现更优。

Conclusion: 该两阶段Transformer方法为功能基团替换提供了更精确可控的解决方案，通过顺序生成确保结构相似性，为药物设计和化学信息学中的分子优化开辟了新途径。

Abstract: Functional group replacement is a pivotal approach in cheminformatics to enable the design of novel chemical compounds with tailored properties. Traditional methods for functional group removal and replacement often rely on rule-based heuristics, which can be limited in their ability to generate diverse and novel chemical structures. Recently, transformer-based models have shown promise in improving the accuracy and efficiency of molecular transformations, but existing approaches typically focus on single-step modeling, lacking the guarantee of structural similarity. In this work, we seek to advance the state of the art by developing a novel two-stage transformer model for functional group removal and replacement. Unlike one-shot approaches that generate entire molecules in a single pass, our method generates the functional group to be removed and appended sequentially, ensuring strict substructure-level modifications. Using a matched molecular pairs (MMPs) dataset derived from ChEMBL, we trained an encoder-decoder transformer model with SMIRKS-based representations to capture transformation rules effectively. Extensive evaluations demonstrate our method's ability to generate chemically valid transformations, explore diverse chemical spaces, and maintain scalability across varying search sizes.

</details>


### [74] [Towards Specialized Generalists: A Multi-Task MoE-LoRA Framework for Domain-Specific LLM Adaptation](https://arxiv.org/abs/2601.07935)
*Yuxin Yang,Aoxiong Zeng,Xiangquan Yang*

Main category: cs.LG

TL;DR: 提出Med-MoE-LoRA框架，结合MoE和LoRA解决医学领域LLM适应中的稳定性-可塑性困境和任务干扰问题


<details>
  <summary>Details</summary>
Motivation: LLM适应医学领域面临两大挑战：1) 稳定性-可塑性困境 - 需要学习复杂临床知识但不能忘记通用知识；2) 任务干扰 - 不同医学子任务在有限参数空间中相互竞争

Method: 提出Med-MoE-LoRA框架，集成Mixture-of-Experts和Low-Rank Adaptation，采用非对称专家分布（深层有更多LoRA专家），并引入"知识保护插件"隔离通用推理能力

Result: 在多个临床NLP任务上优于标准LoRA和传统MoE架构，同时保持模型的通用认知能力

Conclusion: Med-MoE-LoRA能有效解决医学领域LLM适应的核心挑战，实现高效的多任务领域适应

Abstract: The rapid evolution of Large Language Models (LLMs) has shifted focus from general-purpose capabilities to domain-specific expertise. However, adapting LLMs to specialized fields such as medicine presents two challenge: (1) the "Stability-Plasticity Dilemma", where the model must acquire complex clinical knowledge without suffering from catastrophic forgetting of general world knowledge; and (2) "Task Interference", where disparate sub-tasks, such as medical diagnosis, report summarization, and drug-drug interaction prediction, compete for limited low-rank parameter space. In this paper, we propose Med-MoE-LoRA, a novel framework that integrates Mixture-of-Experts (MoE) with Low-Rank Adaptation (LoRA) to enable efficient multi-task domain adaptation, especially for medical scenarios. Drawing inspiration from recent advances, our framework employs an asymmetric expert distribution where deeper layers are equipped with a higher density of LoRA experts to capture complex semantic abstractions. We further introduce a "Knowledge-Preservation Plugin", inspired by LoRA MoE, to isolate and protect general-purpose reasoning. By utilizing soft merging with adaptive routing and rank-wise decoupling, Med-MoE-LoRA achieves superior performance in medical benchmarks while reducing interference. Experimental results demonstrate that our approach consistently outperforms standard LoRA and conventional MoE architectures across multiple clinical NLP tasks while retaining the model's general cognitive capabilities.

</details>


### [75] [Coupled Diffusion-Encoder Models for Reconstruction of Flow Fields](https://arxiv.org/abs/2601.07946)
*AmirPouya Hemmasian,Amir Barati Farimani*

Main category: cs.LG

TL;DR: DiffCoder：结合扩散模型与卷积编码器的流场重建框架，在强压缩下比传统VAE更好地保持流场的统计特性


<details>
  <summary>Details</summary>
Motivation: 传统变分自编码器（VAE）在强压缩流场数据时难以保持高阶统计结构，需要一种能更好保留流场分布和谱特性的方法

Method: 提出DiffCoder框架，将概率扩散模型与传统卷积ResNet编码器耦合进行端到端训练。编码器压缩流场为潜在表示，扩散模型学习基于压缩状态的生成先验

Result: 在Kolmogorov流场数据集上，强压缩时DiffCoder显著提高谱精度，而VAE严重退化。两者L2重建误差相当，但DiffCoder更好地保持流场分布结构。中等压缩时，足够大的VAE仍具竞争力

Conclusion: 扩散生成解码为复杂流场的紧凑、统计一致表示提供了有前景的路径，特别在信息瓶颈严重时优势明显

Abstract: Data-driven flow-field reconstruction typically relies on autoencoder architectures that compress high-dimensional states into low-dimensional latent representations. However, classical approaches such as variational autoencoders (VAEs) often struggle to preserve the higher-order statistical structure of fluid flows when subjected to strong compression. We propose DiffCoder, a coupled framework that integrates a probabilistic diffusion model with a conventional convolutional ResNet encoder and trains both components end-to-end. The encoder compresses the flow field into a latent representation, while the diffusion model learns a generative prior over reconstructions conditioned on the compressed state. This design allows DiffCoder to recover distributional and spectral properties that are not strictly required for minimizing pointwise reconstruction loss but are critical for faithfully representing statistical properties of the flow field. We evaluate DiffCoder and VAE baselines across multiple model sizes and compression ratios on a challenging dataset of Kolmogorov flow fields. Under aggressive compression, DiffCoder significantly improves the spectral accuracy while VAEs exhibit substantial degradation. Although both methods show comparable relative L2 reconstruction error, DiffCoder better preserves the underlying distributional structure of the flow. At moderate compression levels, sufficiently large VAEs remain competitive, suggesting that diffusion-based priors provide the greatest benefit when information bottlenecks are severe. These results demonstrate that the generative decoding by diffusion offers a promising path toward compact, statistically consistent representations of complex flow fields.

</details>


### [76] [Reinforcement Learning Methods for Neighborhood Selection in Local Search](https://arxiv.org/abs/2601.07948)
*Yannick Molinghen,Augustin Delecluse,Renaud De Landtsheer,Stefano Michelini*

Main category: cs.LG

TL;DR: 本文评估了强化学习在局部搜索元启发式中的邻域选择策略，包括多臂老虎机和深度强化学习方法，在三个组合优化问题上的表现，发现ε-greedy表现稳定，而深度强化学习方法因计算开销大仅在有充足运行时间时才有竞争力。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在组合优化中已有应用，但在局部搜索元启发式中的有效性尚未得到充分研究。本研究旨在评估不同强化学习方法在局部搜索邻域选择策略中的表现，探索其在实际优化问题中的适用性和局限性。

Method: 研究评估了多种强化学习邻域选择策略：多臂老虎机方法（上置信界、ε-greedy）和深度强化学习方法（近端策略优化、双深度Q网络）。这些方法在三个不同问题上进行测试：旅行商问题、带时间窗的取送货问题以及车辆排序问题。研究特别关注了搜索特定特征（如约束违反惩罚导致的成本大幅变化）对奖励函数设计的影响。

Result: 实验结果表明：1）算法性能在不同问题间差异显著；2）ε-greedy方法在所有问题中表现稳定且排名靠前；3）深度强化学习方法因计算开销大，只有在运行时间显著延长时才具有竞争力；4）搜索特定特征（特别是约束违反惩罚）需要精心设计奖励函数以提供稳定且信息丰富的学习信号。

Conclusion: 研究揭示了强化学习在局部搜索中的潜力和实际限制。虽然ε-greedy等简单方法表现稳定，但深度强化学习方法因计算成本高而应用受限。未来的工作需要更高效的深度强化学习实现和针对特定搜索特征的奖励函数设计，以充分发挥强化学习在局部搜索中的潜力。

Abstract: Reinforcement learning has recently gained traction as a means to improve combinatorial optimization methods, yet its effectiveness within local search metaheuristics specifically remains comparatively underexamined. In this study, we evaluate a range of reinforcement learning-based neighborhood selection strategies -- multi-armed bandits (upper confidence bound, $ε$-greedy) and deep reinforcement learning methods (proximal policy optimization, double deep $Q$-network) -- and compare them against multiple baselines across three different problems: the traveling salesman problem, the pickup and delivery problem with time windows, and the car sequencing problem. We show how search-specific characteristics, particularly large variations in cost due to constraint violation penalties, necessitate carefully designed reward functions to provide stable and informative learning signals. Our extensive experiments reveal that algorithm performance varies substantially across problems, although that $ε$-greedy consistently ranks among the best performers. In contrast, the computational overhead of deep reinforcement learning approaches only makes them competitive with a substantially longer runtime. These findings highlight both the promise and the practical limitations of deep reinforcement learning in local search.

</details>


### [77] [Hybrid SARIMA LSTM Model for Local Weather Forecasting: A Residual Learning Approach for Data Driven Meteorological Prediction](https://arxiv.org/abs/2601.07951)
*Shreyas Rajeev,Karthik Mudenahalli Ashoka,Amit Mallappa Tiparaddi*

Main category: cs.LG

TL;DR: 提出混合SARIMA-LSTM架构，通过残差学习策略分解温度数据，SARIMA建模长期季节趋势，LSTM学习非线性残差，提升长期温度预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统SARIMA模型假设平稳性，无法捕捉非线性突变；纯LSTM在开环预测中误差会累积发散。需要结合两者优势，解决长期大气变量预测的挑战。

Method: 采用混合SARIMA-LSTM架构，使用残差学习策略：SARIMA建模确定性季节趋势，LSTM专门训练SARIMA无法捕捉的非线性残差误差。

Result: 通过融合统计稳定性与神经可塑性，该混合方法最小化误差传播，增强长期预测准确性。

Conclusion: 混合SARIMA-LSTM架构能有效结合线性季节建模和非线性残差学习，为长期温度预测提供更优解决方案。

Abstract: Accurately forecasting long-term atmospheric variables remains a defining challenge in meteorological science due to the chaotic nature of atmospheric systems. Temperature data represents a complex superposition of deterministic cyclical climate forces and stochastic, short-term fluctuations. While planetary mechanics drive predictable seasonal periodicities, rapid meteorological changes such as thermal variations, pressure anomalies, and humidity shifts introduce nonlinear volatilities that defy simple extrapolation. Historically, the Seasonal Autoregressive Integrated Moving Average (SARIMA) model has been the standard for modeling historical weather data, prized for capturing linear seasonal trends. However, SARIMA operates under strict assumptions of stationarity, failing to capture abrupt, nonlinear transitions. This leads to systematic residual errors, manifesting as the under-prediction of sudden spikes or the over-smoothing of declines. Conversely, Deep Learning paradigms, specifically Long Short-Term Memory (LSTM) networks, demonstrate exceptional efficacy in handling intricate time-series data. By utilizing memory gates, LSTMs learn complex nonlinear dependencies. Yet, LSTMs face instability in open-loop forecasting; without ground truth feedback, minor deviations compound recursively, causing divergence. To resolve these limitations, we propose a Hybrid SARIMA-LSTM architecture. This framework employs a residual-learning strategy to decompose temperature into a predictable climate component and a nonlinear weather component. The SARIMA unit models the robust, long-term seasonal trend, while the LSTM is trained exclusively on the residuals the nonlinear errors SARIMA fails to capture. By fusing statistical stability with neural plasticity, this hybrid approach minimizes error propagation and enhances long-horizon accuracy.

</details>


### [78] [DataScribe: An AI-Native, Policy-Aligned Web Platform for Multi-Objective Materials Design and Discovery](https://arxiv.org/abs/2601.07966)
*Divyanshu Singh,Doguhan Sarıtürk,Cameron Lea,Md Shafiqul Islam,Raymundo Arroyave,Vahid Attari*

Main category: cs.LG

TL;DR: DataScribe是一个AI原生的云材料发现平台，通过本体知识图谱统一异构数据，集成FAIR元数据、不确定性建模和多目标贝叶斯优化，实现实验与计算的闭环工作流。


<details>
  <summary>Details</summary>
Motivation: 加速材料发现需要超越传统数据仓库的数字平台，将学习、优化和决策直接嵌入研究流程，解决异构数据整合和闭环工作流问题。

Method: 基于本体知识图谱统一异构实验和计算数据，集成FAIR元数据捕获、模式单位统一、不确定性感知代理建模、多目标多保真度贝叶斯优化，构建应用层智能栈。

Result: 通过电化学材料和高熵合金案例验证，实现端到端数据融合、实时优化和多目标权衡空间的可重复探索，支持学术和非营利研究者的免费使用。

Conclusion: DataScribe作为通用应用层骨干平台，适用于各种规模实验室（包括自动驾驶实验室和分布式材料加速平台），内置性能、可持续性和供应链感知目标支持。

Abstract: The acceleration of materials discovery requires digital platforms that go beyond data repositories to embed learning, optimization, and decision-making directly into research workflows. We introduce DataScribe, an AI-native, cloud-based materials discovery platform that unifies heterogeneous experimental and computational data through ontology-backed ingestion and machine-actionable knowledge graphs. The platform integrates FAIR-compliant metadata capture, schema and unit harmonization, uncertainty-aware surrogate modeling, and native multi-objective multi-fidelity Bayesian optimization, enabling closed-loop propose-measure-learn workflows across experimental and computational pipelines. DataScribe functions as an application-layer intelligence stack, coupling data governance, optimization, and explainability rather than treating them as downstream add-ons. We validate the platform through case studies in electrochemical materials and high-entropy alloys, demonstrating end-to-end data fusion, real-time optimization, and reproducible exploration of multi-objective trade spaces. By embedding optimization engines, machine learning, and unified access to public and private scientific data directly within the data infrastructure, and by supporting open, free use for academic and non-profit researchers, DataScribe functions as a general-purpose application-layer backbone for laboratories of any scale, including self-driving laboratories and geographically distributed materials acceleration platforms, with built-in support for performance, sustainability, and supply-chain-aware objectives.

</details>


### [79] [Beyond the Next Port: A Multi-Task Transformer for Forecasting Future Voyage Segment Durations](https://arxiv.org/abs/2601.08013)
*Nairui Liu,Fang He,Xindi Tang*

Main category: cs.LG

TL;DR: 该研究提出了一种基于Transformer的架构，用于预测未来航段航行时间，通过整合历史航行时长、目的港拥堵代理和静态船舶描述符，在多任务学习框架下显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统ETA模型主要针对下一个停靠港，且严重依赖实时AIS数据，而未来航段缺乏实时数据。需要开发能够预测未来多个航段航行时间的模型，以提高海运调度可靠性和港口运营优化。

Method: 将未来港口ETA预测重新定义为航段时间序列预测问题，开发基于Transformer的架构，整合历史航行时长、目的港拥堵代理和静态船舶描述符。采用因果掩码注意力机制捕捉长期时间依赖，多任务学习头联合预测航段航行时长和港口拥堵状态。

Result: 在2021年全球真实数据集上评估，模型显著优于多种基线方法：相比序列基线模型，MAE相对降低4.85%，MAPE降低4.95%；相比梯度提升机，MAE降低9.39%，MAPE降低52.97%。主要目的港的案例研究进一步证明了模型的优越准确性。

Conclusion: 该研究成功解决了未来航段ETA预测的挑战，提出的Transformer架构通过整合多种数据源和多任务学习，有效降低了预测不确定性，为海运调度可靠性和港口运营优化提供了实用解决方案。

Abstract: Accurate forecasts of segment-level sailing durations are fundamental to enhancing maritime schedule reliability and optimizing long-term port operations. However, conventional estimated time of arrival (ETA) models are primarily designed for the immediate next port of call and rely heavily on real-time automatic identification system (AIS) data, which is inherently unavailable for future voyage segments. To address this gap, the study reformulates future-port ETA prediction as a segment-level time-series forecasting problem. We develop a transformer-based architecture that integrates historical sailing durations, destination port congestion proxies, and static vessel descriptors. The proposed framework employs a causally masked attention mechanism to capture long-range temporal dependencies and a multi-task learning head to jointly predict segment sailing durations and port congestion states, leveraging shared latent signals to mitigate high uncertainty. Evaluation on a real-world global dataset from 2021 demonstrates the proposed model consistently outperforms a comprehensive suite of competitive baselines. The result shows a relative reduction of 4.85% in mean absolute error (MAE) and 4.95% in mean absolute percentage error (MAPE) compared with sequence baseline models. The relative reductions with gradient boosting machines are 9.39% in MAE and 52.97% in MAPE. Case studies for the major destination port further illustrate the model's superior accuracy.

</details>


### [80] [InfGraND: An Influence-Guided GNN-to-MLP Knowledge Distillation](https://arxiv.org/abs/2601.08033)
*Amir Eskandari,Aman Anand,Elyas Rashno,Farhana Zulkernine*

Main category: cs.LG

TL;DR: InfGraND：一种影响力引导的图知识蒸馏框架，通过识别结构重要性节点指导GNN到MLP的知识蒸馏，并在MLP中嵌入结构感知能力，在低延迟推理场景中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: GNN依赖聚合和更新操作，在低延迟推理或资源受限场景中存在挑战。MLP计算效率高但监督训练性能不佳。现有知识蒸馏方法要么均匀转移知识，要么依赖图无关指标，忽略了节点对图结构的重要性这一核心问题。

Method: 提出InfGraND框架：1）识别并优先处理结构重要性节点来指导蒸馏过程；2）通过一次性多跳邻域特征预计算在MLP中嵌入结构感知能力，避免推理时开销。

Result: 在7个同配图基准数据集上的转导和归纳设置中，InfGraND始终优于先前的GNN到MLP知识蒸馏方法，证明了其在现实世界延迟关键应用中的实用性。

Conclusion: InfGraND通过关注结构重要性节点和嵌入结构感知，有效解决了GNN到MLP知识蒸馏的关键问题，为低延迟图分析应用提供了实用解决方案。

Abstract: Graph Neural Networks (GNNs) are the go-to model for graph data analysis. However, GNNs rely on two key operations - aggregation and update, which can pose challenges for low-latency inference tasks or resource-constrained scenarios. Simple Multi-Layer Perceptrons (MLPs) offer a computationally efficient alternative. Yet, training an MLP in a supervised setting often leads to suboptimal performance. Knowledge Distillation (KD) from a GNN teacher to an MLP student has emerged to bridge this gap. However, most KD methods either transfer knowledge uniformly across all nodes or rely on graph-agnostic indicators such as prediction uncertainty. We argue this overlooks a more fundamental, graph-centric inquiry: "How important is a node to the structure of the graph?" We introduce a framework, InfGraND, an Influence-guided Graph KNowledge Distillation from GNN to MLP that addresses this by identifying and prioritizing structurally influential nodes to guide the distillation process, ensuring that the MLP learns from the most critical parts of the graph. Additionally, InfGraND embeds structural awareness in MLPs through one-time multi-hop neighborhood feature pre-computation, which enriches the student MLP's input and thus avoids inference-time overhead. Our rigorous evaluation in transductive and inductive settings across seven homophilic graph benchmark datasets shows InfGraND consistently outperforms prior GNN to MLP KD methods, demonstrating its practicality for numerous latency-critical applications in real-world settings.

</details>


### [81] [Riemannian Zeroth-Order Gradient Estimation with Structure-Preserving Metrics for Geodesically Incomplete Manifolds](https://arxiv.org/abs/2601.08039)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: 研究黎曼零阶优化在测地不完备度量下的问题，通过构造保持结构的完备度量，建立内在的零阶估计器分析，获得与完备度量下相同复杂度的收敛保证。


<details>
  <summary>Details</summary>
Motivation: 在黎曼优化中，当底层黎曼度量测地不完备时，传统的优化方法可能失效。需要处理不完备度量下的优化问题，同时保持与完备度量下相当的收敛性能。

Method: 1. 构造结构保持的测地完备度量，使得新度量下的驻点也是原度量下的驻点；2. 从纯内在视角分析经典对称两点零阶估计器的均方误差；3. 基于内在分析建立随机梯度下降的收敛保证。

Result: 在适当条件下，新度量g'下的ε-驻点对应原度量g下的ε-驻点，匹配了测地完备设置下的最优已知复杂度。合成问题和实际网格优化任务的实验验证了理论结果。

Conclusion: 该框架能够在测地不完备情况下保持稳定收敛，扩展了黎曼零阶优化的适用范围，同时保持了与完备度量下相当的收敛性能。

Abstract: In this paper, we study Riemannian zeroth-order optimization in settings where the underlying Riemannian metric $g$ is geodesically incomplete, and the goal is to approximate stationary points with respect to this incomplete metric. To address this challenge, we construct structure-preserving metrics that are geodesically complete while ensuring that every stationary point under the new metric remains stationary under the original one. Building on this foundation, we revisit the classical symmetric two-point zeroth-order estimator and analyze its mean-squared error from a purely intrinsic perspective, depending only on the manifold's geometry rather than any ambient embedding. Leveraging this intrinsic analysis, we establish convergence guarantees for stochastic gradient descent with this intrinsic estimator. Under additional suitable conditions, an $ε$-stationary point under the constructed metric $g'$ also corresponds to an $ε$-stationary point under the original metric $g$, thereby matching the best-known complexity in the geodesically complete setting. Empirical studies on synthetic problems confirm our theoretical findings, and experiments on a practical mesh optimization task demonstrate that our framework maintains stable convergence even in the absence of geodesic completeness.

</details>


### [82] [LUT-Compiled Kolmogorov-Arnold Networks for Lightweight DoS Detection on IoT Edge Devices](https://arxiv.org/abs/2601.08044)
*Oleksandr Kuznetsov*

Main category: cs.LG

TL;DR: 提出一种基于查找表（LUT）编译的轻量级KAN模型，用于物联网边缘设备的实时DoS攻击检测，在保持高准确率的同时实现68-5000倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 物联网设备资源受限，传统DoS入侵检测方法难以部署；KAN网络参数少但推理时B样条计算开销大，不适合延迟敏感的IoT应用。

Method: 提出LUT编译流水线，用预计算的量化查找表和线性插值替代昂贵的样条计算，显著降低推理延迟；构建轻量级KAN模型（50K参数，0.19MB）。

Result: 在CICIDS2017 DoS数据集上达到99.0%准确率；LUT编译后（分辨率L=8）保持98.96%准确率（F1下降<0.0004），批量大小256时加速68倍，批量大小1时加速5000倍以上，内存开销仅2倍。

Conclusion: LUT编译的KAN模型能够在仅CPU的物联网网关上实现实时DoS检测，具有确定性推理延迟和最小资源占用，为准确率-延迟-内存权衡提供了清晰的帕累托边界。

Abstract: Denial-of-Service (DoS) attacks pose a critical threat to Internet of Things (IoT) ecosystems, yet deploying effective intrusion detection on resource-constrained edge devices remains challenging. Kolmogorov-Arnold Networks (KANs) offer a compact alternative to Multi-Layer Perceptrons (MLPs) by placing learnable univariate spline functions on edges rather than fixed activations on nodes, achieving competitive accuracy with fewer parameters. However, runtime B-spline evaluation introduces significant computational overhead unsuitable for latency-critical IoT applications. We propose a lookup table (LUT) compilation pipeline that replaces expensive spline computations with precomputed quantized tables and linear interpolation, dramatically reducing inference latency while preserving detection quality. Our lightweight KAN model (50K parameters, 0.19~MB) achieves 99.0\% accuracy on the CICIDS2017 DoS dataset. After LUT compilation with resolution $L=8$, the model maintains 98.96\% accuracy (F1 degradation $<0.0004$) while achieving $\mathbf{68\times}$ speedup at batch size 256 and over $\mathbf{5000\times}$ speedup at batch size 1, with only $2\times$ memory overhead. We provide comprehensive evaluation across LUT resolutions, quantization schemes, and out-of-bounds policies, establishing clear Pareto frontiers for accuracy-latency-memory trade-offs. Our results demonstrate that LUT-compiled KANs enable real-time DoS detection on CPU-only IoT gateways with deterministic inference latency and minimal resource footprint.

</details>


### [83] [Q-realign: Piggybacking Realignment on Quantization for Safe and Efficient LLM Deployment](https://arxiv.org/abs/2601.08089)
*Qitao Tan,Xiaoying Song,Ningxi Cheng,Ninghao Liu,Xiaoming Zhai,Lingzi Hong,Yanzhi Wang,Zhen Xiang,Geng Yuan*

Main category: cs.LG

TL;DR: Q-realign：一种基于后训练量化的后处理防御方法，通过将量化重构为压缩和安全性的双重目标，在保持任务性能的同时恢复微调后LLM的安全性对齐，显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 公共大语言模型通常在预训练时进行安全对齐，但部署所需的任务特定微调往往会破坏这种对齐并引入安全风险。现有防御方法要么将安全恢复嵌入微调过程，要么依赖微调先验进行后处理校正，导致安全恢复与训练紧密耦合，计算开销高且流程复杂。

Method: 提出Q-realign方法，基于后训练量化，通过分析表示结构将量化重构为压缩和安全性的双重目标过程。该方法将安全对齐与微解脱耦，并自然融入现代部署流程。

Result: 在多个模型和数据集上的实验表明，该方法显著减少不安全行为同时保持任务性能，大幅降低内存使用和GPU时间。例如，可在单张RTX 4090上40分钟内恢复7B LLM微调后的安全对齐。

Conclusion: Q-realign提供了一种实用、即用的安全感知部署解决方案，通过后训练量化有效解决微调导致的安全对齐侵蚀问题。

Abstract: Public large language models (LLMs) are typically safety-aligned during pretraining, yet task-specific fine-tuning required for deployment often erodes this alignment and introduces safety risks. Existing defenses either embed safety recovery into fine-tuning or rely on fine-tuning-derived priors for post-hoc correction, leaving safety recovery tightly coupled with training and incurring high computational overhead and a complex workflow. To address these challenges, we propose \texttt{Q-realign}, a post-hoc defense method based on post-training quantization, guided by an analysis of representational structure. By reframing quantization as a dual-objective procedure for compression and safety, \texttt{Q-realign} decouples safety alignment from fine-tuning and naturally piggybacks into modern deployment pipelines. Experiments across multiple models and datasets demonstrate that our method substantially reduces unsafe behaviors while preserving task performance, with significant reductions in memory usage and GPU hours. Notably, our approach can recover the safety alignment of a fine-tuned 7B LLM on a single RTX 4090 within 40 minutes. Overall, our work provides a practical, turnkey solution for safety-aware deployment.

</details>


### [84] [Local-Global Feature Fusion for Subject-Independent EEG Emotion Recognition](https://arxiv.org/abs/2601.08094)
*Zheng Zhou,Isabella McEvoy,Camilo E. Valderrama*

Main category: cs.LG

TL;DR: 提出融合局部通道级和全局试验级描述符的双分支Transformer框架，用于跨被试EEG情绪识别，在SEED-VII数据集上达到约40%的7分类准确率


<details>
  <summary>Details</summary>
Motivation: 跨被试EEG情绪识别面临被试间差异大、短时噪声记录难以学习鲁棒表示的问题，需要同时捕捉局部通道信息和全局试验特征

Method: 融合局部通道级描述符（微分熵+图论特征）和全局试验级描述符（时域、频域、复杂度特征），采用双分支Transformer架构，结合注意力融合和域对抗正则化，并基于强度阈值过滤样本

Result: 在留一被试交叉验证协议下，该方法在SEED-VII数据集上优于单视图和经典基线方法，在7分类跨被试情绪识别中达到约40%的平均准确率

Conclusion: 融合局部和全局EEG描述符的双分支Transformer框架能有效提升跨被试情绪识别的泛化能力，代码已开源

Abstract: Subject-independent EEG emotion recognition is challenged by pronounced inter-subject variability and the difficulty of learning robust representations from short, noisy recordings. To address this, we propose a fusion framework that integrates (i) local, channel-wise descriptors and (ii) global, trial-level descriptors, improving cross-subject generalization on the SEED-VII dataset. Local representations are formed per channel by concatenating differential entropy with graph-theoretic features, while global representations summarize time-domain, spectral, and complexity characteristics at the trial level. These representations are fused in a dual-branch transformer with attention-based fusion and domain-adversarial regularization, with samples filtered by an intensity threshold. Experiments under a leave-one-subject-out protocol demonstrate that the proposed method consistently outperforms single-view and classical baselines, achieving approximately 40% mean accuracy in 7-class subject-independent emotion recognition. The code has been released at https://github.com/Danielz-z/LGF-EEG-Emotion.

</details>


### [85] [STO-RL: Offline RL under Sparse Rewards via LLM-Guided Subgoal Temporal Order](https://arxiv.org/abs/2601.08107)
*Chengyang Gu,Yuxin Pan,Hui Xiong,Yize Chen*

Main category: cs.LG

TL;DR: STO-RL：利用LLM生成时序子目标序列和状态-子目标阶段映射的离线强化学习框架，通过基于势能的奖励塑形将稀疏终端奖励转化为密集时序信号，提升长时程稀疏奖励任务的性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习虽然能从预收集数据集中学习策略，但在长时程稀疏奖励任务上表现不佳。现有目标条件化和分层离线RL方法通常忽略子目标间的时序依赖关系，依赖不精确的奖励塑形，导致策略次优。

Method: 提出STO-RL框架：1) 利用大语言模型生成时序有序的子目标序列和状态-子目标阶段映射；2) 基于此时序结构应用基于势能的奖励塑形，将稀疏终端奖励转化为密集、时序一致的信号；3) 使用增强后的数据集进行离线策略训练。

Result: 在四个离散和连续稀疏奖励基准测试中，STO-RL始终优于最先进的离线目标条件化和分层RL基线，实现更快的收敛速度、更高的成功率和更短的轨迹。消融研究证实了框架对不完美或噪声LLM生成子目标序列的鲁棒性。

Conclusion: LLM引导的子目标时序结构与理论基础的奖励塑形相结合，为长时程离线RL提供了实用且可扩展的解决方案，能够有效处理稀疏奖励任务中的时序依赖问题。

Abstract: Offline reinforcement learning (RL) enables policy learning from pre-collected datasets, avoiding costly and risky online interactions, but it often struggles with long-horizon tasks involving sparse rewards. Existing goal-conditioned and hierarchical offline RL methods decompose such tasks and generate intermediate rewards to mitigate limitations of traditional offline RL, but usually overlook temporal dependencies among subgoals and rely on imprecise reward shaping, leading to suboptimal policies. To address these issues, we propose STO-RL (Offline RL using LLM-Guided Subgoal Temporal Order), an offline RL framework that leverages large language models (LLMs) to generate temporally ordered subgoal sequences and corresponding state-to-subgoal-stage mappings. Using this temporal structure, STO-RL applies potential-based reward shaping to transform sparse terminal rewards into dense, temporally consistent signals, promoting subgoal progress while avoiding suboptimal solutions. The resulting augmented dataset with shaped rewards enables efficient offline training of high-performing policies. Evaluations on four discrete and continuous sparse-reward benchmarks demonstrate that STO-RL consistently outperforms state-of-the-art offline goal-conditioned and hierarchical RL baselines, achieving faster convergence, higher success rates, and shorter trajectories. Ablation studies further confirm STO-RL's robustness to imperfect or noisy LLM-generated subgoal sequences, demonstrating that LLM-guided subgoal temporal structures combined with theoretically grounded reward shaping provide a practical and scalable solution for long-horizon offline RL.

</details>


### [86] [Learning a Stochastic Differential Equation Model of Tropical Cyclone Intensification from Reanalysis and Observational Data](https://arxiv.org/abs/2601.08116)
*Kenneth Gee,Sai Ravela*

Main category: cs.LG

TL;DR: 提出一个10项立方随机微分方程模型，从数据中学习热带气旋强度变化的物理合理简化模型


<details>
  <summary>Details</summary>
Motivation: 热带气旋是危险的自然灾害，但由于历史数据集规模和质量有限，直接量化其危害具有挑战性。现有物理模型和统计/机器学习模型都存在不足，需要探索能否从数据中学习物理合理的简化微分方程模型。

Method: 使用10项立方随机微分方程模型，基于经过验证的环境特征（已知驱动强度变化的因素），使用高质量飓风强度数据集（IBTrACS）和环境估计数据（ERA5再分析），限于北半球进行训练。

Result: 模型生成的合成强度序列能够捕捉北半球历史强度变化统计和危害估计的多个方面，表明模型具有实用价值。

Conclusion: 通过自动化系统识别技术，可以从数据中学习复杂地球系统动力学的可解释、物理风格模型，这为灾害建模提供了新途径。

Abstract: Tropical cyclones are dangerous natural hazards, but their hazard is challenging to quantify directly from historical datasets due to limited dataset size and quality. Models of cyclone intensification fill this data gap by simulating huge ensembles of synthetic hurricanes based on estimates of the storm's large scale environment. Both physics-based and statistical/ML intensification models have been developed to tackle this problem, but an open question is: can a physically reasonable and simple physics-style differential equation model of intensification be learned from data? In this paper, we answer this question in the affirmative by presenting a 10-term cubic stochastic differential equation model of Tropical Cyclone intensification. The model depends on a well-vetted suite of engineered environmental features known to drive intensification and is trained using a high quality dataset of hurricane intensity (IBTrACS) with estimates of the cyclone's large scale environment from a data-assimilated simulation (ERA5 reanalysis), restricted to the Northern Hemisphere. The model generates synthetic intensity series which capture many aspects of historical intensification statistics and hazard estimates in the Northern Hemisphere. Our results show promise that interpretable, physics style models of complex earth system dynamics can be learned using automated system identification techniques.

</details>


### [87] [Structure Detection for Contextual Reinforcement Learning](https://arxiv.org/abs/2601.08120)
*Tianyue Zhou,Jung-Hoon Cho,Cathy Wu*

Main category: cs.LG

TL;DR: SD-MBTL是一个通用框架，通过动态检测CMDP的泛化结构来选择合适的MBTL算法，M/GP-MBTL是其实现，能自适应切换高斯过程和聚类方法，在多个CRL基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统CRL方法（独立训练和多任务学习）存在计算成本高或负迁移问题。现有的MBTL方法虽然有效，但CMDP结构多样，不同问题需要不同的任务选择策略，因此需要能动态识别CMDP结构并选择合适算法的通用框架。

Method: 提出SD-MBTL框架，动态识别CMDP的底层泛化结构并选择合适的MBTL算法。具体实现M/GP-MBTL，检测Mountain结构（泛化性能随上下文差异增大而下降），自适应切换高斯过程方法和聚类方法。

Result: 在合成数据和CRL基准测试（连续控制、交通控制、农业管理）上的实验表明，M/GP-MBTL在聚合指标上比之前最强方法提升了12.49%。

Conclusion: 在线结构检测在复杂CRL环境中指导源任务选择具有很大潜力，SD-MBTL框架能有效适应不同CMDP结构，提升泛化性能。

Abstract: Contextual Reinforcement Learning (CRL) tackles the problem of solving a set of related Contextual Markov Decision Processes (CMDPs) that vary across different context variables. Traditional approaches--independent training and multi-task learning--struggle with either excessive computational costs or negative transfer. A recently proposed multi-policy approach, Model-Based Transfer Learning (MBTL), has demonstrated effectiveness by strategically selecting a few tasks to train and zero-shot transfer. However, CMDPs encompass a wide range of problems, exhibiting structural properties that vary from problem to problem. As such, different task selection strategies are suitable for different CMDPs. In this work, we introduce Structure Detection MBTL (SD-MBTL), a generic framework that dynamically identifies the underlying generalization structure of CMDP and selects an appropriate MBTL algorithm. For instance, we observe Mountain structure in which generalization performance degrades from the training performance of the target task as the context difference increases. We thus propose M/GP-MBTL, which detects the structure and adaptively switches between a Gaussian Process-based approach and a clustering-based approach. Extensive experiments on synthetic data and CRL benchmarks--covering continuous control, traffic control, and agricultural management--show that M/GP-MBTL surpasses the strongest prior method by 12.49% on the aggregated metric. These results highlight the promise of online structure detection for guiding source task selection in complex CRL environments.

</details>


### [88] [Intra-tree Column Subsampling Hinders XGBoost Learning of Ratio-like Interactions](https://arxiv.org/abs/2601.08121)
*Mykola Pinchuk*

Main category: cs.LG

TL;DR: 研究XGBoost中列抽样对合成比率特征的影响，发现当数据具有比率结构时，列抽样会降低模型性能，建议避免列抽样或直接添加比率特征。


<details>
  <summary>Details</summary>
Motivation: 许多实际问题中的信号需要组合多个原始测量值才能显现（如比率和速率）。在梯度提升树中，这种组合不是显式操作，模型需要通过协调的特征分割来合成。研究XGBoost中的列抽样是否会使这种合成变得更加困难。

Method: 使用两种具有抵消结构的数据生成过程：两个原始特征共享强干扰因子，而目标取决于较小的差异因子。通过log比率抵消干扰并隔离信号。在{0.4, 0.6, 0.8, 0.9}范围内变化colsample_bylevel和colsample_bynode参数，重点研究轻度抽样(s >= 0.8)。对照组包含工程化的比率特征，无需合成。

Result: 在仅使用原始特征的设置中，列抽样降低了测试PR-AUC。在主要过程中，当两个参数都设为0.4时，相对下降达到54%。当包含工程化比率特征时，这种影响基本消失。基于路径的共同使用度量在性能下降的相同单元格中下降。

Conclusion: 如果数据可能存在比率类结构，应避免使用列抽样，或者直接包含预期的比率特征。列抽样会阻碍模型合成比率特征的能力，从而降低性能。

Abstract: Many applied problems contain signal that becomes clear only after combining multiple raw measurements. Ratios and rates are common examples. In gradient boosted trees, this combination is not an explicit operation: the model must synthesize it through coordinated splits on the component features. We study whether intra-tree column subsampling in XGBoost makes that synthesis harder. We use two synthetic data generating processes with cancellation-style structure. In both, two primitive features share a strong nuisance factor, while the target depends on a smaller differential factor. A log ratio cancels the nuisance and isolates the signal. We vary colsample_bylevel and colsample_bynode over s in {0.4, 0.6, 0.8, 0.9}, emphasizing mild subsampling (s >= 0.8). A control feature set includes the engineered ratio, removing the need for synthesis. Across both processes, intra-tree column subsampling reduces test PR-AUC in the primitives-only setting. In the main process the relative decrease reaches 54 percent when both parameters are set to 0.4. The effect largely disappears when the engineered ratio is present. A path-based co-usage metric drops in the same cells where performance deteriorates. Practically, if ratio-like structure is plausible, either avoid intra-tree subsampling or include the intended ratio features.

</details>


### [89] [Generalization Analysis and Method for Domain Generalization for a Family of Recurrent Neural Networks](https://arxiv.org/abs/2601.08122)
*Atefeh Termehchi,Ekram Hossain,Isaac Woungang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Koopman算子理论的方法来分析RNN的可解释性和域外泛化能力，并通过谱分析量化域偏移对泛化误差的影响，最终提出了一种改进域泛化的方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在可解释性和泛化性方面存在局限，特别是在处理具有时间相关性的序列数据时，现有理论分析假设独立样本不成立。需要开发能够分析RNN可解释性和域外泛化的方法。

Method: 将训练好的RNN状态演化建模为未知的非线性闭环反馈系统，使用Koopman算子理论将其近似为线性算子以实现可解释性，然后通过谱分析量化域偏移对泛化误差的最坏影响，并基于此提出域泛化方法。

Result: 提出的分析方法能够量化域偏移对泛化误差的影响，域泛化方法能够减少域外泛化误差并提高对分布偏移的鲁棒性，在实践中的时序模式学习任务上得到验证。

Conclusion: 该研究为RNN的可解释性和域外泛化提供了理论分析框架，提出的域泛化方法能够有效应对分布偏移问题，在安全关键部署中具有重要应用价值。

Abstract: Deep learning (DL) has driven broad advances across scientific and engineering domains. Despite its success, DL models often exhibit limited interpretability and generalization, which can undermine trust, especially in safety-critical deployments. As a result, there is growing interest in (i) analyzing interpretability and generalization and (ii) developing models that perform robustly under data distributions different from those seen during training (i.e. domain generalization). However, the theoretical analysis of DL remains incomplete. For example, many generalization analyses assume independent samples, which is violated in sequential data with temporal correlations. Motivated by these limitations, this paper proposes a method to analyze interpretability and out-of-domain (OOD) generalization for a family of recurrent neural networks (RNNs). Specifically, the evolution of a trained RNN's states is modeled as an unknown, discrete-time, nonlinear closed-loop feedback system. Using Koopman operator theory, these nonlinear dynamics are approximated with a linear operator, enabling interpretability. Spectral analysis is then used to quantify the worst-case impact of domain shifts on the generalization error. Building on this analysis, a domain generalization method is proposed that reduces the OOD generalization error and improves the robustness to distribution shifts. Finally, the proposed analysis and domain generalization approach are validated on practical temporal pattern-learning tasks.

</details>


### [90] [Reverse Flow Matching: A Unified Framework for Online Reinforcement Learning with Diffusion and Flow Policies](https://arxiv.org/abs/2601.08136)
*Zeyang Li,Sunbochen Tang,Navid Azizan*

Main category: cs.LG

TL;DR: 本文提出了反向流匹配（RFM）统一框架，用于在无直接目标样本的情况下训练扩散和流策略，通过引入Langevin Stein算子构建零均值控制变量，降低重要性采样方差，将现有的噪声期望和梯度期望方法统一为特例。


<details>
  <summary>Details</summary>
Motivation: 扩散和流策略在在线强化学习中表现出强大表达能力，但缺乏直接目标样本使得训练困难。现有方法分为噪声期望和梯度期望两类，但两者关系不明确，缺乏统一框架。需要解决在无直接目标样本情况下高效训练扩散和流策略的问题。

Method: 提出反向流匹配（RFM）统一框架：采用反向推理视角，将训练目标表述为给定中间噪声样本的后验均值估计问题。引入Langevin Stein算子构建零均值控制变量，推导出能有效降低重要性采样方差的通用估计器类别。将现有噪声期望和梯度期望方法统一为该框架的特例。

Result: RFM框架成功将扩散和流策略的训练统一，能够将目标从扩散策略扩展到流策略，并允许结合Q值和Q梯度信息推导出最优的最小方差估计器。在连续控制基准测试中，使用RFM训练的流策略相比扩散策略基线表现出更好的性能。

Conclusion: 反向流匹配（RFM）为无直接目标样本情况下训练扩散和流策略提供了统一的理论框架，不仅统一了现有方法，还通过降低方差提高了训练效率和稳定性，在连续控制任务中验证了其有效性。

Abstract: Diffusion and flow policies are gaining prominence in online reinforcement learning (RL) due to their expressive power, yet training them efficiently remains a critical challenge. A fundamental difficulty in online RL is the lack of direct samples from the target distribution; instead, the target is an unnormalized Boltzmann distribution defined by the Q-function. To address this, two seemingly distinct families of methods have been proposed for diffusion policies: a noise-expectation family, which utilizes a weighted average of noise as the training target, and a gradient-expectation family, which employs a weighted average of Q-function gradients. Yet, it remains unclear how these objectives relate formally or if they can be synthesized into a more general formulation. In this paper, we propose a unified framework, reverse flow matching (RFM), which rigorously addresses the problem of training diffusion and flow models without direct target samples. By adopting a reverse inferential perspective, we formulate the training target as a posterior mean estimation problem given an intermediate noisy sample. Crucially, we introduce Langevin Stein operators to construct zero-mean control variates, deriving a general class of estimators that effectively reduce importance sampling variance. We show that existing noise-expectation and gradient-expectation methods are two specific instances within this broader class. This unified view yields two key advancements: it extends the capability of targeting Boltzmann distributions from diffusion to flow policies, and enables the principled combination of Q-value and Q-gradient information to derive an optimal, minimum-variance estimator, thereby improving training efficiency and stability. We instantiate RFM to train a flow policy in online RL, and demonstrate improved performance on continuous-control benchmarks compared to diffusion policy baselines.

</details>


### [91] [Dynamic Graph Structure Learning via Resistance Curvature Flow](https://arxiv.org/abs/2601.08149)
*Chaoqun Fei,Huanjiang Liu,Tinglve Zhou,Yangyang Li,Tianyong Hao*

Main category: cs.LG

TL;DR: 提出Resistance Curvature Flow (RCF)框架，利用电路物理中的有效电阻概念替代传统基于最优传输的曲率计算，实现100倍以上的计算加速，并开发了DGSL-RCF图优化算法。


<details>
  <summary>Details</summary>
Motivation: 传统基于欧氏距离的静态图构建方法无法捕捉数据流形的内在曲率特征，而现有的Ollivier-Ricci曲率流虽然能进行动态拓扑优化，但依赖最优传输计算导致计算复杂度极高，限制了其在大规模数据集和深度学习框架中的应用。

Method: 提出Resistance Curvature Flow (RCF)框架，利用电路物理中的有效电阻概念将昂贵的曲率优化转化为高效的矩阵运算。基于该框架设计了图优化算法DGSL-RCF，通过曲率梯度引导边权重的重新分配来消除拓扑噪声并增强局部聚类结构。

Result: RCF实现了超过100倍的计算加速，同时保持了与OCF相当的几何优化能力。在深度度量学习、流形学习和图结构学习等任务上的实验表明，DGSL-RCF显著提高了表示质量和下游任务性能。

Conclusion: RCF框架成功解决了传统曲率流计算复杂度高的问题，为大规模几何表示学习提供了高效解决方案，并与深度学习模型具有良好的兼容性，在多个学习任务中展现出优越性能。

Abstract: Geometric Representation Learning (GRL) aims to approximate the non-Euclidean topology of high-dimensional data through discrete graph structures, grounded in the manifold hypothesis. However, traditional static graph construction methods based on Euclidean distance often fail to capture the intrinsic curvature characteristics of the data manifold. Although Ollivier-Ricci Curvature Flow (OCF) has proven to be a powerful tool for dynamic topological optimization, its core reliance on Optimal Transport (Wasserstein distance) leads to prohibitive computational complexity, severely limiting its application in large-scale datasets and deep learning frameworks. To break this bottleneck, this paper proposes a novel geometric evolution framework: Resistance Curvature Flow (RCF). Leveraging the concept of effective resistance from circuit physics, RCF transforms expensive curvature optimization into efficient matrix operations. This approach achieves over 100x computational acceleration while maintaining geometric optimization capabilities comparable to OCF. We provide an in-depth exploration of the theoretical foundations and dynamical principles of RCF, elucidating how it guides the redistribution of edge weights via curvature gradients to eliminate topological noise and strengthen local cluster structures. Furthermore, we provide a mechanistic explanation of RCF's role in manifold enhancement and noise suppression, as well as its compatibility with deep learning models. We design a graph optimization algorithm, DGSL-RCF, based on this framework. Experimental results across deep metric learning, manifold learning, and graph structure learning demonstrate that DGSL-RCF significantly improves representation quality and downstream task performance.

</details>


### [92] [VBO-MI: A Fully Gradient-Based Bayesian Optimization Framework Using Variational Mutual Information Estimation](https://arxiv.org/abs/2601.08172)
*Farhad Mirkarimi*

Main category: cs.LG

TL;DR: VBO-MI是一种完全基于梯度的贝叶斯优化框架，利用变分互信息估计消除传统采集函数优化瓶颈，计算效率提升100倍


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯神经网络贝叶斯优化框架存在昂贵的后验采样和采集函数优化瓶颈，需要更高效的可扩展解决方案

Method: 采用行动者-评论家架构：行动网络导航输入空间，变分评论家估计信息增益，实现端到端梯度流

Result: 在FLOPs上相比BNN-BO基线减少10^2倍，在高维合成函数、PDE优化、月球着陆器控制和分类害虫控制等任务上表现一致或更优

Conclusion: VBO-MI通过完全基于梯度的框架有效解决了传统贝叶斯优化的计算瓶颈，实现了计算效率和优化性能的双重提升

Abstract: Many real-world tasks require optimizing expensive black-box functions accessible only through noisy evaluations, a setting commonly addressed with Bayesian optimization (BO). While Bayesian neural networks (BNNs) have recently emerged as scalable alternatives to Gaussian Processes (GPs), traditional BNN-BO frameworks remain burdened by expensive posterior sampling and acquisition function optimization. In this work, we propose {VBO-MI} (Variational Bayesian Optimization with Mutual Information), a fully gradient-based BO framework that leverages recent advances in variational mutual information estimation. To enable end-to-end gradient flow, we employ an actor-critic architecture consisting of an {action-net} to navigate the input space and a {variational critic} to estimate information gain. This formulation effectively eliminates the traditional inner-loop acquisition optimization bottleneck, achieving up to a {$10^2 \times$ reduction in FLOPs} compared to BNN-BO baselines. We evaluate our method on a diverse suite of benchmarks, including high-dimensional synthetic functions and complex real-world tasks such as PDE optimization, the Lunar Lander control problem, and categorical Pest Control. Our experiments demonstrate that VBO-MI consistently provides the same or superior optimization performance and computational scalability over the baselines.

</details>


### [93] [TabPFN Through The Looking Glass: An interpretability study of TabPFN and its internal representations](https://arxiv.org/abs/2601.08181)
*Aviral Gupta,Armaan Sethi,Dhruv Kumar*

Main category: cs.LG

TL;DR: 该研究分析了表格基础模型内部表示的信息编码，通过探测实验发现模型隐藏层中存储了有意义的结构化信息，包括线性回归系数、复杂表达式的中间值和最终答案等，揭示了模型内部计算过程。


<details>
  <summary>Details</summary>
Motivation: 表格基础模型在广泛任务中表现出色，但其内部表示和学习概念仍不明确，缺乏可解释性。理解这些模型如何处理和转换输入特征对于提高透明度和可信度至关重要。

Method: 通过一系列探测实验分析模型隐藏表示中的信息编码：测试线性回归系数的存在性、复杂表达式的中间值、以及早期层中的最终答案。这些实验允许推理模型内部执行的计算过程。

Result: 研究发现表格基础模型的表示中存储了有意义的结构化信息。观察到清晰的信号对应模型预测过程中涉及的中间和最终数量，揭示了模型如何细化输入以及最终输出如何产生。

Conclusion: 该研究加深了对表格基础模型内部机制的理解，表明这些模型编码了具体且可解释的信息，使决策过程更加透明和可信，向提高模型可解释性迈出了重要一步。

Abstract: Tabular foundational models are pre-trained models designed for a wide range of tabular data tasks. They have shown strong performance across domains, yet their internal representations and learned concepts remain poorly understood. This lack of interpretability makes it important to study how these models process and transform input features. In this work, we analyze the information encoded inside the model's hidden representations and examine how these representations evolve across layers. We run a set of probing experiments that test for the presence of linear regression coefficients, intermediate values from complex expressions, and the final answer in early layers. These experiments allow us to reason about the computations the model performs internally. Our results provide evidence that meaningful and structured information is stored inside the representations of tabular foundational models. We observe clear signals that correspond to both intermediate and final quantities involved in the model's prediction process. This gives insight into how the model refines its inputs and how the final output emerges. Our findings contribute to a deeper understanding of the internal mechanics of tabular foundational models. They show that these models encode concrete and interpretable information, which moves us closer to making their decision processes more transparent and trustworthy.

</details>


### [94] [Scalable Multiagent Reinforcement Learning with Collective Influence Estimation](https://arxiv.org/abs/2601.08210)
*Zhenglong Luo,Zhiyong Chen,Aoxiang Liu,Ke Pan*

Main category: cs.LG

TL;DR: 提出CIEN框架，通过建模其他智能体对任务对象的集体影响，实现通信受限环境下的高效多智能体协作，避免网络规模随智能体数量增长而膨胀。


<details>
  <summary>Details</summary>
Motivation: 现有MARL方法依赖频繁的动作或状态信息交换，这在现实机器人系统中难以实现。传统估计器网络会随智能体数量增加而急剧膨胀，限制了大规模系统的可扩展性。

Method: 提出集体影响估计网络（CIEN）框架，通过建模其他智能体对任务对象的集体影响，使每个智能体仅需局部观测和任务对象状态就能推断关键交互信息，无需显式动作信息交换。该框架避免网络规模随团队规模增长，新智能体加入时无需修改现有网络结构。

Result: 基于SAC算法的多智能体协作任务实验表明，该方法在通信受限环境下实现了稳定高效的协调。在实际机器人平台上的部署显示，基于集体影响建模的策略显著提高了鲁棒性和部署可行性，同时降低了对通信基础设施的依赖。

Conclusion: CIEN框架通过建模集体影响而非个体动作，解决了MARL中的通信依赖和可扩展性问题，为大规模多智能体系统提供了实用的解决方案，在实际机器人应用中表现出良好的鲁棒性和部署可行性。

Abstract: Multiagent reinforcement learning (MARL) has attracted considerable attention due to its potential in addressing complex cooperative tasks. However, existing MARL approaches often rely on frequent exchanges of action or state information among agents to achieve effective coordination, which is difficult to satisfy in practical robotic systems. A common solution is to introduce estimator networks to model the behaviors of other agents and predict their actions; nevertheless, such designs cause the size and computational cost of the estimator networks to grow rapidly with the number of agents, thereby limiting scalability in large-scale systems.
  To address these challenges, this paper proposes a multiagent learning framework augmented with a Collective Influence Estimation Network (CIEN). By explicitly modeling the collective influence of other agents on the task object, each agent can infer critical interaction information solely from its local observations and the task object's states, enabling efficient collaboration without explicit action information exchange. The proposed framework effectively avoids network expansion as the team size increases; moreover, new agents can be incorporated without modifying the network structures of existing agents, demonstrating strong scalability. Experimental results on multiagent cooperative tasks based on the Soft Actor-Critic (SAC) algorithm show that the proposed method achieves stable and efficient coordination under communication-limited environments. Furthermore, policies trained with collective influence modeling are deployed on a real robotic platform, where experimental results indicate significantly improved robustness and deployment feasibility, along with reduced dependence on communication infrastructure.

</details>


### [95] [One-Shot Federated Ridge Regression: Exact Recovery via Sufficient Statistic Aggregation](https://arxiv.org/abs/2601.08216)
*Zahir Alsulaimawi*

Main category: cs.LG

TL;DR: 本文提出一种单次通信的联邦岭回归方法，通过客户端一次性传输Gram矩阵和矩向量，服务器通过单次矩阵求逆重构全局解，无需迭代通信。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习需要客户端与服务器之间重复同步，收敛速度受学习率、数据异质性和客户端采样影响。本文探究分布式线性回归是否必须迭代通信，旨在降低通信开销。

Method: 将联邦岭回归建模为分布式均衡问题：每个客户端计算本地充分统计量（Gram矩阵和矩向量）并一次性传输；服务器通过单次矩阵求逆重构全局解。针对高维场景提出随机投影技术降低通信量，并建立差分隐私保证。

Result: 在满足覆盖条件下，单次聚合可精确恢复集中式岭回归解；对不满足覆盖条件的异质分布，推导了非渐近误差界。通信量从迭代方法的O(Rd)降至O(d²)，随机投影进一步降至O(m²)。实验表明单次融合匹配FedAvg精度，通信量减少达38倍。

Conclusion: 分布式线性回归无需迭代通信，单次通信即可获得精确解。该方法显著降低通信开销，提供差分隐私优势，适用于核方法和随机特征模型，但不适用于一般非线性架构。

Abstract: Federated learning protocols require repeated synchronization between clients and a central server, with convergence rates depending on learning rates, data heterogeneity, and client sampling. This paper asks whether iterative communication is necessary for distributed linear regression. We show it is not. We formulate federated ridge regression as a distributed equilibrium problem where each client computes local sufficient statistics -- the Gram matrix and moment vector -- and transmits them once. The server reconstructs the global solution through a single matrix inversion. We prove exact recovery: under a coverage condition on client feature matrices, one-shot aggregation yields the centralized ridge solution, not an approximation. For heterogeneous distributions violating coverage, we derive non-asymptotic error bounds depending on spectral properties of the aggregated Gram matrix. Communication reduces from $\mathcal{O}(Rd)$ in iterative methods to $\mathcal{O}(d^2)$ total; for high-dimensional settings, we propose and experimentally validate random projection techniques reducing this to $\mathcal{O}(m^2)$ where $m \ll d$. We establish differential privacy guarantees where noise is injected once per client, eliminating the composition penalty that degrades privacy in multi-round protocols. We further address practical considerations including client dropout robustness, federated cross-validation for hyperparameter selection, and comparison with gradient-based alternatives. Comprehensive experiments on synthetic heterogeneous regression demonstrate that one-shot fusion matches FedAvg accuracy while requiring up to $38\times$ less communication. The framework applies to kernel methods and random feature models but not to general nonlinear architectures.

</details>


### [96] [A Preliminary Agentic Framework for Matrix Deflation](https://arxiv.org/abs/2601.08219)
*Paimon Goulart,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 提出一种基于智能体的矩阵降秩分解方法，使用LLM生成秩-1 SVD更新，VLM评估更新并决定停止时机，无需固定阈值


<details>
  <summary>Details</summary>
Motivation: 探索智能体能否像剥洋葱一样逐层分解矩阵，实现无需固定阈值的自适应矩阵降秩分解

Method: 采用双智能体架构：Solver LLM生成秩-1 SVD更新，Vision Language Model评估更新质量并决定停止时机；通过上下文学习和行列置换提升稳定性

Result: 在Digits、CIFAR-10和合成矩阵上表现良好：合成噪声案例中与数值方法仅差1.75 RMSE；在Digits和CIFAR-10上也能达到Frobenius范数降至原始10%的目标

Conclusion: 完全基于智能体的无阈值矩阵降秩分解是可行的，可作为传统数值算法的替代方案

Abstract: Can a small team of agents peel a matrix apart, one rank-1 slice at a time? We propose an agentic approach to matrix deflation in which a solver Large Language Model (LLM) generates rank-1 Singular Value Decomposition (SVD) updates and a Vision Language Model (VLM) accepts or rejects each update and decides when to stop, eliminating fixed norm thresholds. Solver stability is improved through in-context learning (ICL) and types of row/column permutations that expose visually coherent structure. We evaluate on Digits ($8{\times}8$), CIFAR-10 ($32{\times}32$ grayscale), and synthetic ($16{\times}16$) matrices with and without Gaussian noise. In the synthetic noisy case, where the true construction rank $k$ is known, numerical deflation provides the noise target and our best agentic configuration differs by only $1.75$ RMSE of the target. For Digits and CIFAR-10, targets are defined by deflating until the Frobenius norm reaches $10\%$ of the original. Across all settings, our agent achieves competitive results, suggesting that fully agentic, threshold-free deflation is a viable alternative to classical numerical algorithms.

</details>


### [97] [GADPN: Graph Adaptive Denoising and Perturbation Networks via Singular Value Decomposition](https://arxiv.org/abs/2601.08230)
*Hao Deng,Bo Liu*

Main category: cs.LG

TL;DR: GADPN：一种通过低秩去噪和广义结构扰动自适应优化图拓扑的图结构学习框架，在保持高效的同时提升GNN性能


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络(GNN)的性能受限于观测图的质量（包含噪声、缺失链接或与GNN假设不匹配的结构特性），而现有的图结构学习方法计算成本高，限制了实际应用

Method: 提出GADPN框架，通过低秩去噪和广义结构扰动自适应优化图拓扑。关键贡献：1）使用贝叶斯优化自适应确定最优去噪强度，适应不同图的同质性水平；2）通过奇异值分解(SVD)将结构扰动方法扩展到任意图，克服原方法仅适用于对称结构的限制

Result: 在基准数据集上的实验表明，GADPN实现了最先进的性能，同时显著提高了效率。在具有挑战性的异配性图上表现尤为突出，验证了其在不同网络类型中稳健学习增强图结构的能力

Conclusion: GADPN是一个简单而有效的图结构学习框架，能够自适应地优化图拓扑，在保持高效的同时提升GNN在各种图类型上的性能

Abstract: While Graph Neural Networks (GNNs) excel on graph-structured data, their performance is fundamentally limited by the quality of the observed graph, which often contains noise, missing links, or structural properties misaligned with GNNs' underlying assumptions. To address this, graph structure learning aims to infer a more optimal topology. Existing methods, however, often incur high computational costs due to complex generative models and iterative joint optimization, limiting their practical utility. In this paper, we propose GADPN, a simple yet effective graph structure learning framework that adaptively refines graph topology via low-rank denoising and generalized structural perturbation. Our approach makes two key contributions: (1) we introduce Bayesian optimization to adaptively determine the optimal denoising strength, tailoring the process to each graph's homophily level; and (2) we extend the structural perturbation method to arbitrary graphs via Singular Value Decomposition (SVD), overcoming its original limitation to symmetric structures. Extensive experiments on benchmark datasets demonstrate that GADPN achieves state-of-the-art performance while significantly improving efficiency. It shows particularly strong gains on challenging disassortative graphs, validating its ability to robustly learn enhanced graph structures across diverse network types.

</details>


### [98] [Incorporating Cognitive Biases into Reinforcement Learning for Financial Decision-Making](https://arxiv.org/abs/2601.08247)
*Liu He*

Main category: cs.LG

TL;DR: 将认知偏差整合到金融交易强化学习框架中，研究人类非理性行为对交易决策的影响


<details>
  <summary>Details</summary>
Motivation: 金融市场受人类非理性行为影响，传统RL模型假设理性代理人，忽略了心理因素的作用。本研究旨在探索将认知偏差整合到RL框架中是否能产生更符合人类行为的交易策略

Method: 将过度自信、损失厌恶等认知偏差整合到强化学习的奖励结构和决策过程中，在模拟和真实交易环境中评估性能

Result: 结果不明确或为负面，但为将人类偏差整合到RL中提供了挑战性见解

Conclusion: 尽管结果不理想，但为开发稳健的金融AI系统提供了宝贵经验，揭示了将人类认知偏差整合到RL框架中的复杂性

Abstract: Financial markets are influenced by human behavior that deviates from rationality due to cognitive biases. Traditional reinforcement learning (RL) models for financial decision-making assume rational agents, potentially overlooking the impact of psychological factors. This study integrates cognitive biases into RL frameworks for financial trading, hypothesizing that such models can exhibit human-like trading behavior and achieve better risk-adjusted returns than standard RL agents. We introduce biases, such as overconfidence and loss aversion, into reward structures and decision-making processes and evaluate their performance in simulated and real-world trading environments. Despite its inconclusive or negative results, this study provides insights into the challenges of incorporating human-like biases into RL, offering valuable lessons for developing robust financial AI systems.

</details>


### [99] [Hyperbolic Heterogeneous Graph Transformer](https://arxiv.org/abs/2601.08251)
*Jongmin Park,Seunghoon Han,Hyewon Lee,Won-Yong Shin,Sungsu Lim*

Main category: cs.LG

TL;DR: 提出Hyperbolic Heterogeneous Graph Transformer (HypHGT)，一种完全在双曲空间中学习异构图表示的方法，通过基于Transformer的架构有效捕获局部和全局依赖关系，在节点分类任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有双曲空间异构图学习方法存在两个主要问题：1）严重依赖切空间操作，导致频繁转换时的映射失真；2）消息传递架构主要关注局部邻域信息，难以捕获全局层次结构和不同类型节点间的长程依赖关系。

Method: 提出HypHGT方法，完全在双曲空间中学习异构图表示。采用基于Transformer的架构自然捕获局部和全局依赖，设计关系特定的双曲注意力机制，具有线性时间复杂度，能高效计算并保持不同关系类型的异质信息。

Result: 在节点分类任务上的综合实验表明，HypHGT始终优于最先进方法，同时显著减少了训练时间和内存使用。

Conclusion: HypHGT通过完全在双曲空间中操作的Transformer架构，有效解决了现有方法的局限性，能够更好地捕获异构图的复杂结构特性和语义信息，在性能和效率方面都有显著提升。

Abstract: In heterogeneous graphs, we can observe complex structures such as tree-like or hierarchical structures. Recently, the hyperbolic space has been widely adopted in many studies to effectively learn these complex structures. Although these methods have demonstrated the advantages of the hyperbolic space in learning heterogeneous graphs, most existing methods still have several challenges. They rely heavily on tangent-space operations, which often lead to mapping distortions during frequent transitions. Moreover, their message-passing architectures mainly focus on local neighborhood information, making it difficult to capture global hierarchical structures and long-range dependencies between different types of nodes. To address these limitations, we propose Hyperbolic Heterogeneous Graph Transformer (HypHGT), which effectively and efficiently learns heterogeneous graph representations entirely within the hyperbolic space. Unlike previous message-passing based hyperbolic heterogeneous GNNs, HypHGT naturally captures both local and global dependencies through transformer-based architecture. Furthermore, the proposed relation-specific hyperbolic attention mechanism in HypHGT, which operates with linear time complexity, enables efficient computation while preserving the heterogeneous information across different relation types. This design allows HypHGT to effectively capture the complex structural properties and semantic information inherent in heterogeneous graphs. We conduct comprehensive experiments to evaluate the effectiveness and efficiency of HypHGT, and the results demonstrate that it consistently outperforms state-of-the-art methods in node classification task, with significantly reduced training time and memory usage.

</details>


### [100] [LDLT L-Lipschitz Network Weight Parameterization Initialization](https://arxiv.org/abs/2601.08253)
*Marius F. R. Juston,Ramavarapu S. Sreenivas,Dustin Nottage,Ahmet Soylemezoglu*

Main category: cs.LG

TL;DR: 论文分析了LDLT-based L-Lipschitz层的初始化动态，推导了高斯初始化下的输出方差精确表达式，发现传统初始化导致方差过小(0.41)，提出新参数化(10/√n)使方差增至0.9，但实证显示He初始化在实际任务中仍表现更好。


<details>
  <summary>Details</summary>
Motivation: 研究L-Lipschitz网络初始化时的信息丢失问题，传统初始化方法导致输出方差过小，影响网络训练效果，需要理论分析和改进方案。

Method: 使用Wishart分布理论推导输出方差闭式解，结合James定理和Laplace积分展开；开发基于Isserlis/Wick的组合展开计算E[tr(S^k)]；通过蒙特卡洛实验验证理论估计；在Higgs玻色子分类数据集上进行超参数扫描验证。

Result: 传统He/Kaiming初始化(1/√n)输出方差为0.41，新参数化(10/√n)使方差增至0.9；理论推导确保方差保持，但实证显示He初始化在实际任务中表现更好。

Conclusion: 阐明了深度L-Lipschitz网络初始化时信息丢失的原因，提供了改进初始化的理论指导，但实际应用中传统初始化方法仍有优势，理论与实证存在差异。

Abstract: We analyze initialization dynamics for LDLT-based $\mathcal{L}$-Lipschitz layers by deriving the exact marginal output variance when the underlying parameter matrix $W_0\in \mathbb{R}^{m\times n}$ is initialized with IID Gaussian entries $\mathcal{N}(0,σ^2)$. The Wishart distribution, $S=W_0W_0^\top\sim\mathcal{W}_m(n,σ^2 \boldsymbol{I}_m)$, used for computing the output marginal variance is derived in closed form using expectations of zonal polynomials via James' theorem and a Laplace-integral expansion of $(α\boldsymbol{I}_m+S)^{-1}$. We develop an Isserlis/Wick-based combinatorial expansion for $\operatorname{\mathbb{E}}\left[\operatorname{tr}(S^k)\right]$ and provide explicit truncated moments up to $k=10$, which yield accurate series approximations for small-to-moderate $σ^2$. Monte Carlo experiments confirm the theoretical estimates. Furthermore, empirical analysis was performed to quantify that, using current He or Kaiming initialization with scaling $1/\sqrt{n}$, the output variance is $0.41$, whereas the new parameterization with $10/ \sqrt{n}$ for $α=1$ results in an output variance of $0.9$. The findings clarify why deep $\mathcal{L}$-Lipschitz networks suffer rapid information loss at initialization and offer practical prescriptions for choosing initialization hyperparameters to mitigate this effect. However, using the Higgs boson classification dataset, a hyperparameter sweep over optimizers, initialization scale, and depth was conducted to validate the results on real-world data, showing that although the derivation ensures variance preservation, empirical results indicate He initialization still performs better.

</details>


### [101] [On Evaluation of Unsupervised Feature Selection for Pattern Classification](https://arxiv.org/abs/2601.08257)
*Gyu-Il Kim,Dae-Won Kim,Jaesung Lee*

Main category: cs.LG

TL;DR: 本文重新审视无监督特征选择的评估范式，提出使用多标签分类框架替代传统的单标签评估，以更公平可靠地比较不同方法。


<details>
  <summary>Details</summary>
Motivation: 现有无监督特征选择方法通常使用单标签数据集进行评估，但单标签数据集通常是从多标签数据中随机选择一个标签构建的。由于选择的标签具有任意性，不同方法的表现排名会因所选标签的不同而变化，导致评估结果不可靠。

Method: 采用多标签分类框架来评估无监督特征选择方法。在21个多标签数据集上对几种代表性方法进行实验，比较它们在多标签设置下的性能表现。

Result: 实验结果表明，在多标签评估设置下，不同无监督特征选择方法的性能排名与单标签设置下报告的排名存在显著差异，说明单标签评估可能无法准确反映方法的真实判别能力。

Conclusion: 多标签评估设置为无监督特征选择方法提供了更公平和可靠的比较框架，建议未来研究采用这种评估范式以获得更准确的性能评估。

Abstract: Unsupervised feature selection aims to identify a compact subset of features that captures the intrinsic structure of data without supervised label. Most existing studies evaluate the performance of methods using the single-label dataset that can be instantiated by selecting a label from multi-label data while maintaining the original features. Because the chosen label can vary arbitrarily depending on the experimental setting, the superiority among compared methods can be changed with regard to which label happens to be selected. Thus, evaluating unsupervised feature selection methods based solely on single-label accuracy is unreasonable for assessing their true discriminative ability. This study revisits this evaluation paradigm by adopting a multi-label classification framework. Experiments on 21 multi-label datasets using several representative methods demonstrate that performance rankings differ markedly from those reported under single-label settings, suggesting the possibility of multi-label evaluation settings for fair and reliable comparison of unsupervised feature selection methods.

</details>


### [102] [A Usable GAN-Based Tool for Synthetic ECG Generation in Cardiac Amyloidosis Research](https://arxiv.org/abs/2601.08260)
*Francesco Speziale,Ugo Lomoio,Fabiola Boccuto,Pierangelo Veltri,Pietro Hiram Guzzi*

Main category: cs.LG

TL;DR: 开发基于GAN的ECG信号生成工具，用于生成心脏淀粉样变性的合成心电图数据，解决数据集小、不平衡和异质性问题。


<details>
  <summary>Details</summary>
Motivation: 心脏淀粉样变性是一种罕见且诊断不足的浸润性心肌病，现有的机器学习模型数据集通常较小、不平衡且异质性高，这限制了早期诊断和患者分层的研究。

Method: 使用生成对抗网络（GAN）和图形化命令行界面，生成逼真的合成心电图（ECG）节拍。工具设计注重可用性，允许临床研究人员训练类别特定的生成器，然后交互式地生成大量标记的合成节拍，保留少数类别的分布特征。

Result: 开发了一个能够生成真实合成ECG节拍的工具，该工具可以支持心脏淀粉样变性的早期诊断和患者分层，通过生成大量标记数据来解决数据稀缺和不平衡问题。

Conclusion: 提出的GAN和图形界面工具能够有效生成合成ECG数据，为心脏淀粉样变性的机器学习研究提供了数据增强解决方案，有助于改善这种罕见疾病的诊断和患者管理。

Abstract: Cardiac amyloidosis (CA) is a rare and underdiagnosed infiltrative cardiomyopathy, and available datasets for machine-learning models are typically small, imbalanced and heterogeneous. This paper presents a Generative Adversarial Network (GAN) and a graphical command-line interface for generating realistic synthetic electrocardiogram (ECG) beats to support early diagnosis and patient stratification in CA. The tool is designed for usability, allowing clinical researchers to train class-specific generators once and then interactively produce large volumes of labelled synthetic beats that preserve the distribution of minority classes.

</details>


### [103] [Demystifying the Slash Pattern in Attention: The Role of RoPE](https://arxiv.org/abs/2601.08297)
*Yuan Cheng,Fengzhuo Zhang,Yunlong Hou,Cunxiao Du,Chao Du,Tianyu Pang,Aixin Sun,Zhuoran Yang*

Main category: cs.LG

TL;DR: 本文揭示了LLM中斜线注意力模式（SDHs）的涌现机制，通过实证和理论分析发现SDHs是模型固有特性，源于查询/键的秩一特性和RoPE的中高频成分主导。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常表现出斜线注意力模式，其中注意力分数集中在某个偏移Δ的Δ次子对角线上。这些模式在跨令牌传递信息中起关键作用，但其涌现原因尚不清楚，本文旨在从实证和理论角度解释SDHs的涌现机制。

Method: 1. 实证分析开源LLM，发现SDHs是模型固有特性且泛化到分布外提示；2. 分析查询、键和RoPE，发现SDHs的两个特征条件：查询和键几乎秩一，RoPE由中高频成分主导；3. 理论分析：将实证条件形式化为建模假设，分析配备RoPE的浅层Transformer训练动态，证明梯度下降训练模型会涌现SDHs。

Result: 1. SDHs是LLM的固有特性，能够泛化到分布外提示；2. 实证发现SDHs的两个关键条件：查询/键几乎秩一且RoPE由中高频成分主导；3. 理论证明这些条件足以确保SDHs的涌现，并分析训练动态验证SDHs的形成。

Conclusion: 本文从实证和理论角度解释了LLM中斜线注意力模式的涌现机制，揭示了SDHs是模型固有特性，源于查询/键的秩一特性和RoPE的中高频成分主导，为理解Transformer注意力机制提供了新视角。

Abstract: Large Language Models (LLMs) often exhibit slash attention patterns, where attention scores concentrate along the $Δ$-th sub-diagonal for some offset $Δ$. These patterns play a key role in passing information across tokens. But why do they emerge? In this paper, we demystify the emergence of these Slash-Dominant Heads (SDHs) from both empirical and theoretical perspectives. First, by analyzing open-source LLMs, we find that SDHs are intrinsic to models and generalize to out-of-distribution prompts. To explain the intrinsic emergence, we analyze the queries, keys, and Rotary Position Embedding (RoPE), which jointly determine attention scores. Our empirical analysis reveals two characteristic conditions of SDHs: (1) Queries and keys are almost rank-one, and (2) RoPE is dominated by medium- and high-frequency components. Under these conditions, queries and keys are nearly identical across tokens, and interactions between medium- and high-frequency components of RoPE give rise to SDHs. Beyond empirical evidence, we theoretically show that these conditions are sufficient to ensure the emergence of SDHs by formalizing them as our modeling assumptions. Particularly, we analyze the training dynamics of a shallow Transformer equipped with RoPE under these conditions, and prove that models trained via gradient descent exhibit SDHs. The SDHs generalize to out-of-distribution prompts.

</details>


### [104] [ORBIT: On-policy Exploration-Exploitation for Controllable Multi-Budget Reasoning](https://arxiv.org/abs/2601.08310)
*Kun Liang,Clive Bai,Xin Xu,Chenming Tang,Sanwoo Lee,Weijie Liu,Saiyong Yang,Yunfang Wu*

Main category: cs.LG

TL;DR: ORBIT是一个可控的多预算推理框架，通过多阶段强化学习发现帕累托最优推理行为，然后通过蒸馏融合到单一模型中，实现不同计算预算下的可控推理。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型(LRMs)使用过长的链式推理(CoT)导致不必要的计算成本，而先前方法通过输入推断推理预算的方式不可靠，且训练时固定了成本与精度的权衡，限制了部署灵活性。

Method: ORBIT采用多阶段强化学习发现每个计算预算下的帕累托最优推理行为，然后通过策略蒸馏将这些行为融合到单一统一模型中，实现输入触发的可控多模式推理。

Result: 实验表明ORBIT实现了：(1)多模式可控推理行为，(2)每个模式内具有竞争力的推理密度，(3)将这些前沿策略集成到单一学生模型中，同时保持清晰的模式分离和高性能。

Conclusion: ORBIT提供了一个灵活可控的多预算推理框架，解决了现有方法在推理成本与精度权衡上的局限性，为不同部署场景提供了适应性解决方案。

Abstract: Recent Large Reasoning Models (LRMs) achieve strong performance by leveraging long-form Chain-of-Thought (CoT) reasoning, but uniformly applying overlong reasoning at inference time incurs substantial and often unnecessary computational cost. To address this, prior work explores various strategies to infer an appropriate reasoning budget from the input. However, such approaches are unreliable in the worst case, as estimating the minimal required reasoning effort is fundamentally difficult, and they implicitly fix the trade-off between reasoning cost and accuracy during training, limiting flexibility under varying deployment scenarios. Motivated by these limitations, we propose ORBIT, a controllable multi-budget reasoning framework with well-separated reasoning modes triggered by input. ORBIT employs multi-stage reinforcement learning to discover Pareto-optimal reasoning behaviors at each effort, followed by on-policy distillation to fuse these behaviors into a single unified model. Experiments show that ORBIT achieves (1) controllable reasoning behavior over multiple modes, (2) competitive reasoning density within each mode, and (3) integration of these frontier policies into a single unified student model while preserving clear mode separation and high per-mode performance.

</details>


### [105] [Deep Exploration of Epoch-wise Double Descent in Noisy Data: Signal Separation, Large Activation, and Benign Overfitting](https://arxiv.org/abs/2601.08316)
*Tomoki Kubo,Ryuken Uda,Yusuke Iida*

Main category: cs.LG

TL;DR: 研究通过分析CIFAR-10数据集上带标签噪声训练的神经网络内部结构演化，揭示了epoch-wise双下降现象与良性过拟合、大激活值等关键现象之间的直接联系。


<details>
  <summary>Details</summary>
Motivation: 深度双下降是深度学习模型泛化能力的关键现象之一，但对其内部机制的理解仍不充分。本研究旨在通过分析内部结构演化来实证研究epoch-wise双下降现象，特别是延迟泛化后的过拟合行为。

Method: 在CIFAR-10数据集上添加30%标签噪声，训练三种不同大小的全连接神经网络。通过将损失曲线分解为干净数据和噪声数据的信号贡献，分别分析内部信号的epoch-wise演化过程。

Result: 1) 模型在双下降阶段即使完美拟合噪声训练数据后仍能在测试数据上实现强重新泛化，对应"良性过拟合"状态；2) 噪声数据在干净数据之后被学习，随着学习进展，它们在外部层对应的内部激活逐渐分离；3) 所有模型的浅层都出现单一非常大的激活值，其大小与输入模式相关但与输出模式无关。

Conclusion: 这些实证发现直接连接了"深度双下降"、"良性过拟合"和"大激活值"等关键现象，支持提出理解深度双下降的新场景。内部结构分析揭示了模型如何通过分离干净和噪声数据的激活来实现过拟合后的重新泛化。

Abstract: Deep double descent is one of the key phenomena underlying the generalization capability of deep learning models. In this study, epoch-wise double descent, which is delayed generalization following overfitting, was empirically investigated by focusing on the evolution of internal structures. Fully connected neural networks of three different sizes were trained on the CIFAR-10 dataset with 30% label noise. By decomposing the loss curves into signal contributions from clean and noisy training data, the epoch-wise evolutions of internal signals were analyzed separately. Three main findings were obtained from this analysis. First, the model achieved strong re-generalization on test data even after perfectly fitting noisy training data during the double descent phase, corresponding to a "benign overfitting" state. Second, noisy data were learned after clean data, and as learning progressed, their corresponding internal activations became increasingly separated in outer layers; this enabled the model to overfit only noisy data. Third, a single, very large activation emerged in the shallow layer across all models; this phenomenon is referred as "outliers," "massive activa-tions," and "super activations" in recent large language models and evolves with re-generalization. The magnitude of large activation correlated with input patterns but not with output patterns. These empirical findings directly link the recent key phenomena of "deep double descent," "benign overfitting," and "large activation", and support the proposal of a novel scenario for understanding deep double descent.

</details>


### [106] [A Novel Approach to Explainable AI with Quantized Active Ingredients in Decision Making](https://arxiv.org/abs/2601.08733)
*A. M. A. S. D. Alagiyawanna,Asoka Karunananda,Thushari Silva,A. Mahasinghe*

Main category: cs.LG

TL;DR: 量子玻尔兹曼机在分类准确率和可解释性方面均优于经典玻尔兹曼机，为构建更可信、可解释的AI系统提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在分类任务上表现良好，但缺乏可解释性，特别是在医疗和金融等高风险领域，理解模型决策过程至关重要。需要开发既能保持高性能又能提供透明决策解释的AI框架。

Method: 提出基于量子玻尔兹曼机和经典玻尔兹曼机对比研究的可解释AI框架。使用PCA预处理后的二值化MNIST数据集训练两种模型。量子玻尔兹曼机采用具有强纠缠层的混合量子-经典电路，经典玻尔兹曼机使用对比散度作为基线。使用梯度显著性图和SHAP方法评估特征归因。

Result: 量子玻尔兹曼机在分类准确率上显著优于经典玻尔兹曼机（83.5% vs. 54%），并且在特征归因分布上更集中（熵值1.27 vs. 1.39）。量子模型不仅预测性能更好，还能更清晰地识别影响预测的关键特征。

Conclusion: 量子-经典混合模型在准确性和可解释性方面均显示出改进潜力，为实现更可信、可解释的AI系统提供了有前景的方向。

Abstract: Artificial Intelligence (AI) systems have shown good success at classifying. However, the lack of explainability is a true and significant challenge, especially in high-stakes domains, such as health and finance, where understanding is paramount. We propose a new solution to this challenge: an explainable AI framework based on our comparative study with Quantum Boltzmann Machines (QBMs) and Classical Boltzmann Machines (CBMs). We leverage principles of quantum computing within classical machine learning to provide substantive transparency around decision-making. The design involves training both models on a binarised and dimensionally reduced MNIST dataset, where Principal Component Analysis (PCA) is applied for preprocessing. For interpretability, we employ gradient-based saliency maps in QBMs and SHAP (SHapley Additive exPlanations) in CBMs to evaluate feature attributions.QBMs deploy hybrid quantum-classical circuits with strongly entangling layers, allowing for richer latent representations, whereas CBMs serve as a classical baseline that utilises contrastive divergence. Along the way, we found that QBMs outperformed CBMs on classification accuracy (83.5% vs. 54%) and had more concentrated distributions in feature attributions as quantified by entropy (1.27 vs. 1.39). In other words, QBMs not only produced better predictive performance than CBMs, but they also provided clearer identification of "active ingredient" or the most important features behind model predictions. To conclude, our results illustrate that quantum-classical hybrid models can display improvements in both accuracy and interpretability, which leads us toward more trustworthy and explainable AI systems.

</details>


### [107] [Automated Machine Learning in Radiomics: A Comparative Evaluation of Performance, Efficiency and Accessibility](https://arxiv.org/abs/2601.08334)
*Jose Lozano-Montoya,Emilio Soria-Olivas,Almudena Fuster-Matanzo,Angel Alberich-Bayarri,Ana Jimenez-Pastor*

Main category: cs.LG

TL;DR: 本研究评估了通用和放射组学专用AutoML框架在放射组学分类任务中的表现，发现Simplatab在性能、效率和可访问性方面表现最佳，但现有框架仍缺乏对生存分析、特征可重复性和标准化等放射组学特定挑战的支持。


<details>
  <summary>Details</summary>
Motivation: AutoML框架可以降低放射组学模型开发的技术门槛，但其在解决放射组学特定挑战方面的有效性尚不清楚。本研究旨在评估通用和放射组学专用AutoML框架在多样化放射组学分类任务中的表现、效率和可访问性，以揭示放射组学领域的发展需求。

Method: 使用10个公共/私有放射组学数据集（包含不同成像模态、规模、解剖部位和终点），测试6个通用框架和5个放射组学专用框架。采用预定义参数和标准化交叉验证，评估指标包括AUC、运行时间，以及软件状态、可访问性和可解释性等定性方面。

Result: 放射组学专用工具Simplatab获得最高平均测试AUC（81.81%），运行时间约1小时。通用框架LightAutoML执行最快，性能具有竞争力（平均AUC 78.74%，仅需6分钟）。大多数放射组学专用框架因过时、编程要求高或计算效率低而被排除在性能分析之外，而通用框架显示出更高的可访问性和易用性。

Conclusion: Simplatab在放射组学分类问题上提供了性能、效率和可访问性的有效平衡。然而，现有AutoML框架仍存在显著差距，包括缺乏可访问的生存分析支持，以及特征可重复性和标准化整合有限。未来研究应专注于使AutoML解决方案更好地应对这些放射组学特定挑战。

Abstract: Automated machine learning (AutoML) frameworks can lower technical barriers for predictive and prognostic model development in radiomics by enabling researchers without programming expertise to build models. However, their effectiveness in addressing radiomics-specific challenges remains unclear. This study evaluates the performance, efficiency, and accessibility of general-purpose and radiomics-specific AutoML frameworks on diverse radiomics classification tasks, thereby highlighting development needs for radiomics. Ten public/private radiomics datasets with varied imaging modalities (CT/MRI), sizes, anatomies and endpoints were used. Six general-purpose and five radiomics-specific frameworks were tested with predefined parameters using standardized cross-validation. Evaluation metrics included AUC, runtime, together with qualitative aspects related to software status, accessibility, and interpretability. Simplatab, a radiomics-specific tool with a no-code interface, achieved the highest average test AUC (81.81%) with a moderate runtime (~1 hour). LightAutoML, a general-purpose framework, showed the fastest execution with competitive performance (78.74% mean AUC in six minutes). Most radiomics-specific frameworks were excluded from the performance analysis due to obsolescence, extensive programming requirements, or computational inefficiency. Conversely, general-purpose frameworks demonstrated higher accessibility and ease of implementation. Simplatab provides an effective balance of performance, efficiency, and accessibility for radiomics classification problems. However, significant gaps remain, including the lack of accessible survival analysis support and the limited integration of feature reproducibility and harmonization within current AutoML frameworks. Future research should focus on adapting AutoML solutions to better address these radiomics-specific challenges.

</details>


### [108] [Decodable but not structured: linear probing enables Underwater Acoustic Target Recognition with pretrained audio embeddings](https://arxiv.org/abs/2601.08358)
*Hilde I. Hummel,Sandjai Bhulai,Rob D. van der Mei,Burooj Ghani*

Main category: cs.LG

TL;DR: 该研究首次对水下声学目标识别中的迁移学习进行实证比较，发现预训练音频模型的嵌入空间主要受录音特定特征主导，但线性探测能有效抑制这些特征并分离出船舶类型特征，从而实现低成本自动识别。


<details>
  <summary>Details</summary>
Motivation: 船舶产生的人为噪声对海洋生态系统构成威胁，需要监测船舶辐射噪声。被动声学监测系统产生大量水下录音数据，手动分析不切实际，需要自动化方法。现有监督学习方法受限于标记数据稀缺，迁移学习为解决此限制提供了有前景的替代方案。

Method: 首次对UATR进行迁移学习实证比较研究，评估来自不同音频领域的多个预训练音频模型。冻结预训练模型权重，通过分类、聚类和相似性评估分析生成的嵌入空间。使用线性探测方法抑制录音特定信息并分离船舶类型特征。

Result: 分析显示嵌入空间的几何结构主要由录音特定特征主导。但简单的线性探测能有效抑制录音特定信息，从这些嵌入中分离出船舶类型特征。线性探测使得使用预训练音频模型进行自动UATR成为可能，计算成本低，显著减少对大量高质量标记船舶录音的需求。

Conclusion: 迁移学习为水下声学目标识别提供了有效解决方案，线性探测方法能有效利用预训练音频模型，降低对标记数据的依赖和计算成本，为大规模船舶噪声监测提供了实用工具。

Abstract: Increasing levels of anthropogenic noise from ships contribute significantly to underwater sound pollution, posing risks to marine ecosystems. This makes monitoring crucial to understand and quantify the impact of the ship radiated noise. Passive Acoustic Monitoring (PAM) systems are widely deployed for this purpose, generating years of underwater recordings across diverse soundscapes. Manual analysis of such large-scale data is impractical, motivating the need for automated approaches based on machine learning. Recent advances in automatic Underwater Acoustic Target Recognition (UATR) have largely relied on supervised learning, which is constrained by the scarcity of labeled data. Transfer Learning (TL) offers a promising alternative to mitigate this limitation. In this work, we conduct the first empirical comparative study of transfer learning for UATR, evaluating multiple pretrained audio models originating from diverse audio domains. The pretrained model weights are frozen, and the resulting embeddings are analyzed through classification, clustering, and similarity-based evaluations. The analysis shows that the geometrical structure of the embedding space is largely dominated by recording-specific characteristics. However, a simple linear probe can effectively suppress this recording-specific information and isolate ship-type features from these embeddings. As a result, linear probing enables effective automatic UATR using pretrained audio models at low computational cost, significantly reducing the need for a large amounts of high-quality labeled ship recordings.

</details>


### [109] [Training-Free Distribution Adaptation for Diffusion Models via Maximum Mean Discrepancy Guidance](https://arxiv.org/abs/2601.08379)
*Matina Mahdizadeh Sani,Nima Jamali,Mohammad Jalali,Farzan Farnia*

Main category: cs.LG

TL;DR: 提出MMD Guidance方法，在推理阶段使用最大均值差异(MMD)梯度指导扩散模型采样，实现无需训练的少样本域适应


<details>
  <summary>Details</summary>
Motivation: 预训练扩散模型输出与用户特定目标数据存在不匹配，特别是在少样本域适应任务中，重新训练模型不可行。现有推理时指导方法通常优化替代目标而非直接对齐目标分布

Method: 提出训练免费的MMD Guidance机制，在反向扩散过程中加入生成样本与参考数据集之间MMD的梯度。MMD能从有限数据提供可靠分布估计，实践方差低且可高效微分。框架可扩展到条件生成模型的提示感知适应，并高效应用于潜在扩散模型

Result: 在合成和真实世界基准测试中，MMD Guidance能够实现分布对齐同时保持样本保真度

Conclusion: MMD Guidance为扩散模型提供了一种有效的少样本域适应方法，无需重新训练模型，通过推理时指导实现目标分布对齐

Abstract: Pre-trained diffusion models have emerged as powerful generative priors for both unconditional and conditional sample generation, yet their outputs often deviate from the characteristics of user-specific target data. Such mismatches are especially problematic in domain adaptation tasks, where only a few reference examples are available and retraining the diffusion model is infeasible. Existing inference-time guidance methods can adjust sampling trajectories, but they typically optimize surrogate objectives such as classifier likelihoods rather than directly aligning with the target distribution. We propose MMD Guidance, a training-free mechanism that augments the reverse diffusion process with gradients of the Maximum Mean Discrepancy (MMD) between generated samples and a reference dataset. MMD provides reliable distributional estimates from limited data, exhibits low variance in practice, and is efficiently differentiable, which makes it particularly well-suited for the guidance task. Our framework naturally extends to prompt-aware adaptation in conditional generation models via product kernels. Also, it can be applied with computational efficiency in latent diffusion models (LDMs), since guidance is applied in the latent space of the LDM. Experiments on synthetic and real-world benchmarks demonstrate that MMD Guidance can achieve distributional alignment while preserving sample fidelity.

</details>


### [110] [Controlled LLM Training on Spectral Sphere](https://arxiv.org/abs/2601.08393)
*Tian Xie,Haoming Luo,Haoyu Tang,Yiwen Hu,Jason Klein Liu,Qingnan Ren,Yang Wang,Wayne Xin Zhao,Rui Yan,Bing Su,Chong Luo,Baining Guo*

Main category: cs.LG

TL;DR: 提出SSO优化器，通过严格的光谱约束实现完全μP对齐，在大规模预训练中优于AdamW和Muon，提供更好的稳定性


<details>
  <summary>Details</summary>
Motivation: 现有优化器如Muon只部分满足μP理论约束，仅控制更新但允许权重漂移，需要完全对齐μP的优化方法

Method: 提出SSO优化器，在光谱球上推导最速下降方向，对权重及其更新施加严格的模块级光谱约束，实现完全μP对齐

Result: 在多种架构（Dense 1.7B、MoE 8B-A1B、200层DeepNet）的大规模预训练中，SSO始终优于AdamW和Muon

Conclusion: SSO实现了完全μP对齐的优化，提供显著的实际稳定性优势，包括改进的MoE路由器负载平衡、抑制异常值和严格有界的激活

Abstract: Scaling large models requires optimization strategies that ensure rapid convergence grounded in stability. Maximal Update Parametrization ($\boldsymbolμ$P) provides a theoretical safeguard for width-invariant $Θ(1)$ activation control, whereas emerging optimizers like Muon are only ``half-aligned'' with these constraints: they control updates but allow weights to drift. To address this limitation, we introduce the \textbf{Spectral Sphere Optimizer (SSO)}, which enforces strict module-wise spectral constraints on both weights and their updates. By deriving the steepest descent direction on the spectral sphere, SSO realizes a fully $\boldsymbolμ$P-aligned optimization process. To enable large-scale training, we implement SSO as an efficient parallel algorithm within Megatron. Through extensive pretraining on diverse architectures, including Dense 1.7B, MoE 8B-A1B, and 200-layer DeepNet models, SSO consistently outperforms AdamW and Muon. Furthermore, we observe significant practical stability benefits, including improved MoE router load balancing, suppressed outliers, and strictly bounded activations.

</details>


### [111] [Out-of-distribution generalization of deep-learning surrogates for 2D PDE-generated dynamics in the small-data regime](https://arxiv.org/abs/2601.08404)
*Binh Duong Nguyen,Stefan Sandfeld*

Main category: cs.LG

TL;DR: me-UNet在少量数据下（约20个训练模拟）对二维周期性PDE动力学进行自回归预测，性能优于更复杂的架构，且能泛化到未见过的初始条件。


<details>
  <summary>Details</summary>
Motivation: PDE模拟计算成本高，而科学应用常涉及空间分布场的演化，需要数据驱动的场预测方法。研究在少量数据（最多约100个模拟轨迹）下，对固定PDE和参数体系内分布外初始条件的泛化能力。

Method: 提出多通道U-Net（me-UNet），在五个不同性质的PDE家族上评估，与ViT、AFNO、PDE-Transformer和KAN-UNet在相同训练设置下比较。使用Grad-CAM分析，关注局部性和周期性边界条件的归纳偏置。

Result: me-UNet在所有数据集上匹配或优于更复杂架构，在场空间误差、谱相似性和基于物理的指标方面表现更好，同时训练时间显著减少。仅需约20个训练模拟就能对未见初始条件进行定性泛化。

Conclusion: 在少量数据的周期性二维PDE设置中，具有与局部性和周期性边界条件对齐的归纳偏置的卷积架构，仍然是准确且具有中等分布外鲁棒性的替代建模的有力竞争者。

Abstract: Partial differential equations (PDEs) are a central tool for modeling the dynamics of physical, engineering, and materials systems, but high-fidelity simulations are often computationally expensive. At the same time, many scientific applications can be viewed as the evolution of spatially distributed fields, making data-driven forecasting of such fields a core task in scientific machine learning. In this work we study autoregressive deep-learning surrogates for two-dimensional PDE dynamics on periodic domains, focusing on generalization to out-of-distribution initial conditions within a fixed PDE and parameter regime and on strict small-data settings with at most $\mathcal{O}(10^2)$ simulated trajectories per system. We introduce a multi-channel U-Net [...], evaluate it on five qualitatively different PDE families and compare it to ViT, AFNO, PDE-Transformer, and KAN-UNet under a common training setup. Across all datasets, me-UNet matches or outperforms these more complex architectures in terms of field-space error, spectral similarity, and physics-based metrics for in-distribution rollouts, while requiring substantially less training time. It also generalizes qualitatively to unseen initial conditions with as few as $\approx 20$ training simulations. A data-efficiency study and Grad-CAM analysis further suggest that, in small-data periodic 2D PDE settings, convolutional architectures with inductive biases aligned to locality and periodic boundary conditions remain strong contenders for accurate and moderately out-of-distribution-robust surrogate modeling.

</details>


### [112] [Taxon: Hierarchical Tax Code Prediction with Semantically Aligned LLM Expert Guidance](https://arxiv.org/abs/2601.08418)
*Jihang Li,Qing Liu,Zulong Chen,Jing Wang,Wei Wang,Chuanfei Xu,Zeyi Wen*

Main category: cs.LG

TL;DR: Taxon框架通过特征门控专家混合架构和语义一致性模型，实现电商平台产品税码的层次化预测，已部署在阿里巴巴税务系统中处理每日超50万查询。


<details>
  <summary>Details</summary>
Motivation: 税码预测对电商平台的自动化开票和合规管理至关重要，但现有研究不足。产品需要准确映射到国家标准的层次化税码体系中，错误会导致财务不一致和监管风险。

Method: 提出Taxon框架：1) 特征门控专家混合架构，自适应地将多模态特征路由到税码层次的不同层级；2) 从大语言模型蒸馏的语义一致性模型，验证产品标题与官方税码定义的语义对齐；3) 多源训练管道，结合税码数据库、发票验证日志和商家注册数据，提供结构和语义监督。

Result: 在专有TaxCode数据集和公共基准测试中达到最先进性能。完整的层次路径重建程序显著提高了结构一致性，获得最高F1分数。已部署在阿里巴巴税务系统中，平均每日处理超50万税码查询，业务高峰期达500万以上，提高了准确性、可解释性和鲁棒性。

Conclusion: Taxon框架通过语义对齐和专家指导的方法，有效解决了电商平台税码预测的挑战，实现了准确、可解释且鲁棒的层次化税码分类，在实际生产环境中验证了其价值。

Abstract: Tax code prediction is a crucial yet underexplored task in automating invoicing and compliance management for large-scale e-commerce platforms. Each product must be accurately mapped to a node within a multi-level taxonomic hierarchy defined by national standards, where errors lead to financial inconsistencies and regulatory risks. This paper presents Taxon, a semantically aligned and expert-guided framework for hierarchical tax code prediction. Taxon integrates (i) a feature-gating mixture-of-experts architecture that adaptively routes multi-modal features across taxonomy levels, and (ii) a semantic consistency model distilled from large language models acting as domain experts to verify alignment between product titles and official tax definitions. To address noisy supervision in real business records, we design a multi-source training pipeline that combines curated tax databases, invoice validation logs, and merchant registration data to provide both structural and semantic supervision. Extensive experiments on the proprietary TaxCode dataset and public benchmarks demonstrate that Taxon achieves state-of-the-art performance, outperforming strong baselines. Further, an additional full hierarchical paths reconstruction procedure significantly improves structural consistency, yielding the highest overall F1 scores. Taxon has been deployed in production within Alibaba's tax service system, handling an average of over 500,000 tax code queries per day and reaching peak volumes above five million requests during business event with improved accuracy, interpretability, and robustness.

</details>


### [113] [Coverage Improvement and Fast Convergence of On-policy Preference Learning](https://arxiv.org/abs/2601.08421)
*Juno Kim,Jihun Yun,Jason D. Lee,Kwang-Sung Jun*

Main category: cs.LG

TL;DR: 在线策略偏好学习算法（如在线DPO）比离线方法表现更好，本文通过分析采样策略覆盖度的演化，提出了覆盖度改进原则，证明了在线DPO的指数收敛性，并提出了混合采样器和奖励蒸馏方案。


<details>
  <summary>Details</summary>
Motivation: 在线策略偏好学习算法（如在线DPO）在语言模型对齐中显著优于离线方法，但缺乏理论解释。本文旨在从理论上解释这一现象，分析采样策略覆盖度如何随在线训练演化，并提出改进原则。

Method: 1. 提出覆盖度改进原则：在足够批次大小下，每次更新都会移动到目标区域附近，使覆盖度均匀改善；2. 在上下文赌博机设置中，分析在线DPO的收敛性；3. 提出基于偏好G-最优设计的混合采样器；4. 开发一般函数类设置下的奖励蒸馏方案。

Result: 1. 在线DPO在批次大小超过广义覆盖度阈值时，迭代次数呈指数收敛；2. 离线方法受限于初始策略样本，收敛速度较慢；3. 提出的混合采样器消除对覆盖度的依赖，保证两轮内收敛；4. 实验证实在线DPO和奖励蒸馏算法优于离线对应方法，性能稳定单调提升。

Conclusion: 在线策略偏好学习通过覆盖度改进实现指数收敛，显著优于离线方法。提出的混合采样器和奖励蒸馏方案进一步提升了收敛效率和稳定性，为语言模型对齐提供了理论支持和实用工具。

Abstract: Online on-policy preference learning algorithms for language model alignment such as online direct policy optimization (DPO) can significantly outperform their offline counterparts. We provide a theoretical explanation for this phenomenon by analyzing how the sampling policy's coverage evolves throughout on-policy training. We propose and rigorously justify the \emph{coverage improvement principle}: with sufficient batch size, each update moves into a region around the target where coverage is uniformly better, making subsequent data increasingly informative and enabling rapid convergence. In the contextual bandit setting with Bradley-Terry preferences and linear softmax policy class, we show that on-policy DPO converges exponentially in the number of iterations for batch size exceeding a generalized coverage threshold. In contrast, any learner restricted to offline samples from the initial policy suffers a slower minimax rate, leading to a sharp separation in total sample complexity. Motivated by this analysis, we further propose a simple hybrid sampler based on a novel \emph{preferential} G-optimal design, which removes dependence on coverage and guarantees convergence in just two rounds. Finally, we develop principled on-policy schemes for reward distillation in the general function class setting, and show faster noiseless rates under an alternative deviation-based notion of coverage. Experimentally, we confirm that on-policy DPO and our proposed reward distillation algorithms outperform their off-policy counterparts and enjoy stable, monotonic performance gains across iterations.

</details>


### [114] [DiffMM: Efficient Method for Accurate Noisy and Sparse Trajectory Map Matching via One Step Diffusion](https://arxiv.org/abs/2601.08482)
*Chenxu Han,Sean Bin Yang,Jilin Hu*

Main category: cs.LG

TL;DR: DiffMM：基于编码器-扩散的稀疏轨迹地图匹配框架，通过一步扩散过程实现高效准确匹配


<details>
  <summary>Details</summary>
Motivation: 现有基于HMM或编码器-解码器的地图匹配方法在处理噪声或稀疏采样的GPS轨迹时面临挑战，需要更有效的解决方案

Method: 1) 道路段感知轨迹编码器：通过注意力机制将输入轨迹及其周围候选道路段嵌入共享潜在空间；2) 一步扩散方法：利用轨迹和候选道路段的联合嵌入作为条件上下文，通过捷径模型实现地图匹配

Result: 在大规模轨迹数据集上的实验表明，DiffMM在准确性和效率方面均优于现有最先进方法，特别在处理稀疏轨迹和复杂道路网络拓扑时表现突出

Conclusion: DiffMM通过创新的编码器-扩散框架有效解决了稀疏轨迹地图匹配问题，为交通调度和交通流分析等应用提供了更可靠的解决方案

Abstract: Map matching for sparse trajectories is a fundamental problem for many trajectory-based applications, e.g., traffic scheduling and traffic flow analysis. Existing methods for map matching are generally based on Hidden Markov Model (HMM) or encoder-decoder framework. However, these methods continue to face significant challenges when handling noisy or sparsely sampled GPS trajectories. To address these limitations, we propose DiffMM, an encoder-diffusion-based map matching framework that produces effective yet efficient matching results through a one-step diffusion process. We first introduce a road segment-aware trajectory encoder that jointly embeds the input trajectory and its surrounding candidate road segments into a shared latent space through an attention mechanism. Next, we propose a one step diffusion method to realize map matching through a shortcut model by leveraging the joint embedding of the trajectory and candidate road segments as conditioning context. We conduct extensive experiments on large-scale trajectory datasets, demonstrating that our approach consistently outperforms state-of-the-art map matching methods in terms of both accuracy and efficiency, particularly for sparse trajectories and complex road network topologies.

</details>


### [115] [Temporal Fusion Nexus: A task-agnostic multi-modal embedding model for clinical narratives and irregular time series in post-kidney transplant care](https://arxiv.org/abs/2601.08503)
*Aditya Kumar,Simon Rauch,Mario Cypko,Marcel Naik,Matthieu-P Schapranow,Aadil Rashid,Fabian Halleck,Bilgin Osmanodja,Roland Roller,Lars Pape,Klemens Budde,Mario Schiffer,Oliver Amft*

Main category: cs.LG

TL;DR: TFN是一个多模态、任务无关的嵌入模型，用于整合不规则时间序列和非结构化临床叙述，在肾移植后护理中表现出色，在移植物丢失、排斥和死亡率预测方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 临床环境中存在多种异构数据源，包括不规则的时间序列数据和丰富的叙述性文档，需要一种能够有效整合这些多模态信息的方法来改善临床预测任务。

Method: 提出Temporal Fusion Nexus (TFN)模型，这是一个多模态、任务无关的嵌入模型，专门设计用于整合不规则时间序列和非结构化临床叙述数据。

Result: 在3382名患者的肾移植后护理回顾性队列中，TFN在移植物丢失预测方面达到AUC 0.96（优于现有方法的0.94），移植物排斥预测AUC 0.84（优于0.74），死亡率预测AUC 0.86。相比单模态基线有显著提升（时间序列基线提升约10%，加入静态数据提升约5%）。

Conclusion: TFN通过有效整合临床文本和时间序列数据，在肾移植后护理预测任务中表现出优越性能，具有可解释的潜在因子，且可扩展到其他具有异构数据源、不规则纵向数据和丰富叙述文档的临床任务中。

Abstract: We introduce Temporal Fusion Nexus (TFN), a multi-modal and task-agnostic embedding model to integrate irregular time series and unstructured clinical narratives. We analysed TFN in post-kidney transplant (KTx) care, with a retrospective cohort of 3382 patients, on three key outcomes: graft loss, graft rejection, and mortality. Compared to state-of-the-art model in post KTx care, TFN achieved higher performance for graft loss (AUC 0.96 vs. 0.94) and graft rejection (AUC 0.84 vs. 0.74). In mortality prediction, TFN yielded an AUC of 0.86. TFN outperformed unimodal baselines (approx 10% AUC improvement over time series only baseline, approx 5% AUC improvement over time series with static patient data). Integrating clinical text improved performance across all tasks. Disentanglement metrics confirmed robust and interpretable latent factors in the embedding space, and SHAP-based attributions confirmed alignment with clinical reasoning. TFN has potential application in clinical tasks beyond KTx, where heterogeneous data sources, irregular longitudinal data, and rich narrative documentation are available.

</details>


### [116] [Your Group-Relative Advantage Is Biased](https://arxiv.org/abs/2601.08521)
*Fengkai Yang,Zherui Chen,Xiaohan Wang,Xiaodong Lu,Jiajun Chai,Guojun Yin,Wei Lin,Shuai Ma,Fuzhen Zhuang,Deqing Wang,Yaodong Yang,Jianxin Li,Yikun Ban*

Main category: cs.LG

TL;DR: 论文发现基于分组的强化学习方法中，组相对优势估计器存在固有偏差，提出历史感知自适应难度加权(HA-DW)来纠正偏差，在数学推理基准上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 基于验证器奖励的强化学习(RLVR)及其分组方法(如GRPO)在推理任务后训练中广泛应用，但组相对优势估计的理论性质尚不明确，存在偏差问题需要解决。

Method: 提出历史感知自适应难度加权(HA-DW)：基于演化难度锚点和训练动态的自适应重加权方案，调整优势估计以纠正系统偏差。

Result: 在五个数学推理基准测试中，将HA-DW集成到GRPO及其变体中能持续提升性能，证明纠正偏差优势估计对RLVR训练的重要性。

Conclusion: 纠正组相对优势估计的偏差对于鲁棒高效的RLVR训练至关重要，HA-DW方法有效解决了分组强化学习中的固有偏差问题。

Abstract: Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.
  In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.

</details>


### [117] [Contrastive and Multi-Task Learning on Noisy Brain Signals with Nonlinear Dynamical Signatures](https://arxiv.org/abs/2601.08549)
*Sucheta Ghosh,Zahra Monfared,Felix Dietrich*

Main category: cs.LG

TL;DR: 提出两阶段多任务学习框架，用于EEG信号分析，整合去噪、动力学建模和表示学习，在EEG解码任务中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: EEG信号通常包含噪声和伪影，影响解码性能。现有方法往往单独处理去噪和分类任务，未能有效整合信号增强与高级特征学习。需要一种框架能同时处理信号去噪、动力学特征提取和多任务学习。

Method: 两阶段框架：第一阶段使用去噪自编码器抑制伪影并稳定时域动态；第二阶段采用多任务架构处理去噪信号，包括运动想象分类、基于Lyapunov指数的混沌/非混沌状态判别，以及使用NT-Xent损失的自监督对比表示学习。使用卷积骨干网络结合Transformer编码器捕获时空结构。

Result: 框架显著提升了鲁棒性和泛化能力，在EEG解码任务中超越了强基线方法和最新SOTA方法。分阶段设计有效减少了重构任务与判别任务之间的干扰，提高了跨数据集的稳定性。

Conclusion: 结合去噪、动力学特征提取和自监督学习的多任务框架能有效提升EEG信号分析的性能。分阶段设计将噪声抑制与高级特征学习明确分离，支持可重复训练，为EEG解码提供了有效解决方案。

Abstract: We introduce a two-stage multitask learning framework for analyzing Electroencephalography (EEG) signals that integrates denoising, dynamical modeling, and representation learning. In the first stage, a denoising autoencoder is trained to suppress artifacts and stabilize temporal dynamics, providing robust signal representations. In the second stage, a multitask architecture processes these denoised signals to achieve three objectives: motor imagery classification, chaotic versus non-chaotic regime discrimination using Lyapunov exponent-based labels, and self-supervised contrastive representation learning with NT-Xent loss. A convolutional backbone combined with a Transformer encoder captures spatial-temporal structure, while the dynamical task encourages sensitivity to nonlinear brain dynamics. This staged design mitigates interference between reconstruction and discriminative goals, improves stability across datasets, and supports reproducible training by clearly separating noise reduction from higher-level feature learning. Empirical studies show that our framework not only enhances robustness and generalization but also surpasses strong baselines and recent state-of-the-art methods in EEG decoding, highlighting the effectiveness of combining denoising, dynamical features, and self-supervised learning.

</details>


### [118] [EviNAM: Intelligibility and Uncertainty via Evidential Neural Additive Models](https://arxiv.org/abs/2601.08556)
*Sören Schleibaum,Anton Frederik Thielmann,Julian Teusch,Benjamin Säfken,Jörg P. Müller*

Main category: cs.LG

TL;DR: EviNAM：结合神经加法模型可解释性与证据学习的不确定性估计方法，单次前向即可获得预测、不确定性及特征贡献


<details>
  <summary>Details</summary>
Motivation: 现有方法在可解释性和不确定性估计方面存在不足，需要一种既能提供可解释特征贡献又能进行可靠不确定性估计的方法

Method: 扩展证据学习框架，集成神经加法模型的可解释性，通过单次前向传播同时估计偶然不确定性和认知不确定性，并提供显式特征贡献

Result: 在合成和真实数据上的实验表明，EviNAM在预测性能上与最先进方法相当，同时提供可解释的不确定性估计和特征贡献

Conclusion: EviNAM为更可理解和可信的预测提供了一条路径，可自然扩展到分类和广义加法模型

Abstract: Intelligibility and accurate uncertainty estimation are crucial for reliable decision-making. In this paper, we propose EviNAM, an extension of evidential learning that integrates the interpretability of Neural Additive Models (NAMs) with principled uncertainty estimation. Unlike standard Bayesian neural networks and previous evidential methods, EviNAM enables, in a single pass, both the estimation of the aleatoric and epistemic uncertainty as well as explicit feature contributions. Experiments on synthetic and real data demonstrate that EviNAM matches state-of-the-art predictive performance. While we focus on regression, our method extends naturally to classification and generalized additive models, offering a path toward more intelligible and trustworthy predictions.

</details>


### [119] [M$^2$FMoE: Multi-Resolution Multi-View Frequency Mixture-of-Experts for Extreme-Adaptive Time Series Forecasting](https://arxiv.org/abs/2601.08631)
*Yaohui Huang,Runmin Zou,Yun Wang,Laeeq Aslam,Ruipeng Dong*

Main category: cs.LG

TL;DR: M²FMoE：一种通过多分辨率多视图频率建模学习常规和极端模式的极端自适应预测模型，在极端事件预测中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法在极端事件下性能显著下降，虽然有些方法引入辅助信号，但仍无法捕捉极端事件的复杂时间动态。极端事件预测对实际应用至关重要但极具挑战性。

Method: 提出M²FMoE模型，包含三个模块：1) 多视图频率混合专家模块，在傅里叶和小波域分配专家到不同频谱带；2) 多分辨率自适应融合模块，分层聚合频率特征；3) 时间门控集成模块，动态平衡长期趋势和短期频率感知特征。

Result: 在具有极端模式的真实世界水文数据集上的实验表明，M²FMoE在不需极端事件标签的情况下优于最先进的基线方法。

Conclusion: M²FMoE通过多分辨率多视图频率建模有效学习常规和极端模式，显著提升极端事件预测性能，为解决实际应用中极端事件预测难题提供了有效方案。

Abstract: Forecasting time series with extreme events is critical yet challenging due to their high variance, irregular dynamics, and sparse but high-impact nature. While existing methods excel in modeling dominant regular patterns, their performance degrades significantly during extreme events, constituting the primary source of forecasting errors in real-world applications. Although some approaches incorporate auxiliary signals to improve performance, they still fail to capture extreme events' complex temporal dynamics. To address these limitations, we propose M$^2$FMoE, an extreme-adaptive forecasting model that learns both regular and extreme patterns through multi-resolution and multi-view frequency modeling. It comprises three modules: (1) a multi-view frequency mixture-of-experts module assigns experts to distinct spectral bands in Fourier and Wavelet domains, with cross-view shared band splitter aligning frequency partitions and enabling inter-expert collaboration to capture both dominant and rare fluctuations; (2) a multi-resolution adaptive fusion module that hierarchically aggregates frequency features from coarse to fine resolutions, enhancing sensitivity to both short-term variations and sudden changes; (3) a temporal gating integration module that dynamically balances long-term trends and short-term frequency-aware features, improving adaptability to both regular and extreme temporal patterns. Experiments on real-world hydrological datasets with extreme patterns demonstrate that M$^2$FMoE outperforms state-of-the-art baselines without requiring extreme-event labels.

</details>


### [120] [Provably Safe Reinforcement Learning using Entropy Regularizer](https://arxiv.org/abs/2601.08646)
*Abhijit Mazumdar,Rafal Wisniewski,Manuela L. Bujorianu*

Main category: cs.LG

TL;DR: 提出两种基于OFU原则的安全强化学习算法，其中第二种加入熵正则化，在保证安全约束的同时改善遗憾界并控制算法波动性。


<details>
  <summary>Details</summary>
Motivation: 研究在马尔可夫决策过程中学习最优策略时如何确保安全约束的问题。目标是设计在线强化学习算法，在学习阶段以任意高概率保证安全约束。

Method: 首先提出基于乐观面对不确定性（OFU）原则的算法，然后在此基础上提出主要算法，该算法利用熵正则化技术。对两种算法进行有限样本分析。

Result: 推导了两种算法的遗憾界。证明熵正则化的加入改善了遗憾界，并显著控制了基于OFU的安全RL算法固有的幕间波动性。

Conclusion: 熵正则化是提升安全强化学习算法性能的有效技术，既能改善遗憾界，又能控制算法波动性，为安全约束下的在线学习提供了更好的解决方案。

Abstract: We consider the problem of learning the optimal policy for Markov decision processes with safety constraints. We formulate the problem in a reach-avoid setup. Our goal is to design online reinforcement learning algorithms that ensure safety constraints with arbitrarily high probability during the learning phase. To this end, we first propose an algorithm based on the optimism in the face of uncertainty (OFU) principle. Based on the first algorithm, we propose our main algorithm, which utilizes entropy regularization. We investigate the finite-sample analysis of both algorithms and derive their regret bounds. We demonstrate that the inclusion of entropy regularization improves the regret and drastically controls the episode-to-episode variability that is inherent in OFU-based safe RL algorithms.

</details>


### [121] [TRACE: Reconstruction-Based Anomaly Detection in Ensemble and Time-Dependent Simulations](https://arxiv.org/abs/2601.08659)
*Hamid Gadirov,Martijn Westra,Steffen Frey*

Main category: cs.LG

TL;DR: 使用卷积自编码器检测参数化卡门涡街模拟数据中的异常，比较2D和3D模型在时空异常检测中的表现


<details>
  <summary>Details</summary>
Motivation: 高维、时间相关的模拟数据具有复杂的时空动态，使得异常检测变得困难，需要开发能够处理这种复杂性的方法

Method: 使用卷积自编码器进行重建式异常检测，比较2D自编码器（处理单帧）和3D自编码器（处理短时间堆栈）在参数化卡门涡街模拟数据上的表现

Result: 2D模型能识别单时间步的局部空间异常，而3D模型利用时空上下文检测异常运动模式，减少跨时间的冗余检测；重建误差受质量空间分布影响，集中区域误差更大

Conclusion: 在动态模拟中，时间上下文对于稳健的异常检测至关重要，3D模型在利用时空信息方面表现更好

Abstract: Detecting anomalies in high-dimensional, time-dependent simulation data is challenging due to complex spatial and temporal dynamics. We study reconstruction-based anomaly detection for ensemble data from parameterized Kármán vortex street simulations using convolutional autoencoders. We compare a 2D autoencoder operating on individual frames with a 3D autoencoder that processes short temporal stacks. The 2D model identifies localized spatial irregularities in single time steps, while the 3D model exploits spatio-temporal context to detect anomalous motion patterns and reduces redundant detections across time. We further evaluate volumetric time-dependent data and find that reconstruction errors are strongly influenced by the spatial distribution of mass, with highly concentrated regions yielding larger errors than dispersed configurations. Our results highlight the importance of temporal context for robust anomaly detection in dynamic simulations.

</details>


### [122] [Soft Partition-based KAPI-ELM for Multi-Scale PDEs](https://arxiv.org/abs/2601.08719)
*Vikas Dwivedi,Monica Sigovan,Bruno Sixou*

Main category: cs.LG

TL;DR: 提出一种基于软分区的核自适应物理信息极限学习机（KAPI-ELM），通过平滑分区长度联合控制配点中心和核宽度，实现连续粗到细分辨率，无需傅里叶特征或随机采样，在多个多尺度PDE基准测试中达到或超越现有方法精度。


<details>
  <summary>Details</summary>
Motivation: 现有物理信息机器学习方法在处理高度振荡、多尺度或奇异摄动PDE时面临谱偏差、反向传播成本高、需要手动调整核或傅里叶频率等问题，需要一种更高效、自适应的求解方法。

Method: 提出KAPI-ELM方法：使用软分区机制，通过平滑分区长度联合控制高斯核的配点中心和宽度；采用基于符号距离的加权策略稳定不规则几何上的最小二乘学习；实现确定性低维参数化，无需傅里叶特征、随机采样或硬域界面。

Result: 在8个基准测试（包括振荡ODE、高频泊松方程、不规则形状域、刚性奇异摄动对流扩散问题）中，该方法匹配或超越了最先进的PINN和TFC变体精度，仅需单次线性求解，计算效率显著提升。

Conclusion: 软分区核自适应为多尺度PDE提供了一种快速、架构无关的求解方法，虽然目前应用于稳态线性PDE，但具有广泛的物理信息建模潜力，代码已开源确保可复现性。

Abstract: Physics-informed machine learning holds great promise for solving differential equations, yet existing methods struggle with highly oscillatory, multiscale, or singularly perturbed PDEs due to spectral bias, costly backpropagation, and manually tuned kernel or Fourier frequencies. This work introduces a soft partition--based Kernel-Adaptive Physics-Informed Extreme Learning Machine (KAPI-ELM), a deterministic low-dimensional parameterization in which smooth partition lengths jointly control collocation centers and Gaussian kernel widths, enabling continuous coarse-to-fine resolution without Fourier features, random sampling, or hard domain interfaces. A signed-distance-based weighting further stabilizes least-squares learning on irregular geometries. Across eight benchmarks--including oscillatory ODEs, high-frequency Poisson equations, irregular-shaped domains, and stiff singularly perturbed convection-diffusion problems-the proposed method matches or exceeds the accuracy of state-of-the-art Physics-Informed Neural Network (PINN) and Theory of Functional Connections (TFC) variants while using only a single linear solve. Although demonstrated on steady linear PDEs, the results show that soft-partition kernel adaptation provides a fast, architecture-free approach for multiscale PDEs with broad potential for future physics-informed modeling. For reproducibility, the reference codes are available at https://github.com/vikas-dwivedi-2022/soft_kapi

</details>


### [123] [Model-Agnostic Solutions for Deep Reinforcement Learning in Non-Ergodic Contexts](https://arxiv.org/abs/2601.08726)
*Bert Verbruggen,Arne Vanhoyweghen,Vincent Ginis*

Main category: cs.LG

TL;DR: 传统强化学习在非遍历环境中表现不佳，本文通过引入时间依赖性改进了深度强化学习在非遍历环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习基于贝尔曼方程和期望值优化，但在非遍历环境中，长期结果取决于具体轨迹而非整体平均，导致期望值公式产生系统性次优策略。先前研究表明传统RL架构在非遍历环境中无法找到真正最优解，本文将此分析扩展到深度RL实现。

Method: 在强化学习过程中引入显式的时间依赖性，允许网络函数近似包含时间信息，使智能体能够估计与过程内在增长率一致的价值函数。这种方法不需要改变环境反馈（如奖励转换或修改目标函数），而是自然地通过智能体对时间轨迹的暴露来实现。

Result: 深度强化学习在非遍历动态环境下也会产生次优策略。通过引入时间依赖性，智能体能够估计与过程内在增长率一致的价值函数，从而纠正了这一限制。

Conclusion: 本文通过引入时间依赖性改进了深度强化学习在非遍历环境中的表现，为强化学习在非遍历系统中的应用研究做出了贡献。这种方法自然地通过智能体对时间轨迹的暴露来实现，无需改变环境反馈机制。

Abstract: Reinforcement Learning (RL) remains a central optimisation framework in machine learning. Although RL agents can converge to optimal solutions, the definition of ``optimality'' depends on the environment's statistical properties. The Bellman equation, central to most RL algorithms, is formulated in terms of expected values of future rewards. However, when ergodicity is broken, long-term outcomes depend on the specific trajectory rather than on the ensemble average. In such settings, the ensemble average diverges from the time-average growth experienced by individual agents, with expected-value formulations yielding systematically suboptimal policies. Prior studies demonstrated that traditional RL architectures fail to recover the true optimum in non-ergodic environments. We extend this analysis to deep RL implementations and show that these, too, produce suboptimal policies under non-ergodic dynamics. Introducing explicit time dependence into the learning process can correct this limitation. By allowing the network's function approximation to incorporate temporal information, the agent can estimate value functions consistent with the process's intrinsic growth rate. This improvement does not require altering the environmental feedback, such as reward transformations or modified objective functions, but arises naturally from the agent's exposure to temporal trajectories. Our results contribute to the growing body of research on reinforcement learning methods for non-ergodic systems.

</details>


### [124] [Adaptive Requesting in Decentralized Edge Networks via Non-Stationary Bandits](https://arxiv.org/abs/2601.08760)
*Yi Zhuang,Kun Yang,Xingran Chen*

Main category: cs.LG

TL;DR: 提出AGING BANDIT WITH ADAPTIVE RESET算法，解决边缘网络中时间敏感客户端的去中心化协作请求问题，优化信息新鲜度


<details>
  <summary>Details</summary>
Motivation: 边缘网络中多个客户端通过接入节点请求内容，客户端无法观察接入节点状态或其他客户端行为，需要优化信息新鲜度（AoI减少）。现有经典多臂老虎机方法在历史依赖、客户端耦合、奖励分布突变和渐变变化的情况下效果不佳

Method: 将问题建模为非平稳多臂老虎机，提出AGING BANDIT WITH ADAPTIVE RESET算法，结合自适应窗口和周期性监控来跟踪演化的奖励分布

Result: 建立了理论性能保证，证明所提算法能达到接近最优的性能，并通过仿真验证了理论结果

Conclusion: 该算法有效解决了去中心化协作请求问题中的非平稳性和部分可观测性挑战，为边缘网络中时间敏感应用的信息新鲜度优化提供了有效解决方案

Abstract: We study a decentralized collaborative requesting problem that aims to optimize the information freshness of time-sensitive clients in edge networks consisting of multiple clients, access nodes (ANs), and servers. Clients request content through ANs acting as gateways, without observing AN states or the actions of other clients. We define the reward as the age of information reduction resulting from a client's selection of an AN, and formulate the problem as a non-stationary multi-armed bandit. In this decentralized and partially observable setting, the resulting reward process is history-dependent and coupled across clients, and exhibits both abrupt and gradual changes in expected rewards, rendering classical bandit-based approaches ineffective. To address these challenges, we propose the AGING BANDIT WITH ADAPTIVE RESET algorithm, which combines adaptive windowing with periodic monitoring to track evolving reward distributions. We establish theoretical performance guarantees showing that the proposed algorithm achieves near-optimal performance, and we validate the theoretical results through simulations.

</details>


### [125] [Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs](https://arxiv.org/abs/2601.08763)
*Zhiyuan Hu,Yucheng Wang,Yufei He,Jiaying Wu,Yilun Zhao,See-Kiong Ng,Cynthia Breazeal,Anh Tuan Luu,Hae Won Park,Bryan Hooi*

Main category: cs.LG

TL;DR: 提出Uniqueness-Aware Reinforcement Learning，通过奖励独特的高层解题策略来解决RL训练LLM时的探索崩溃问题，提升pass@k和策略多样性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在微调大语言模型时存在探索崩溃问题：策略过早集中于少数主导推理模式，虽然提升pass@1但限制了rollout多样性和pass@k的提升。这源于对局部token行为的正则化而非解决方案集的多样性。

Method: 提出Uniqueness-Aware Reinforcement Learning，使用LLM作为评判器将同一问题的rollout按高层解题策略聚类（忽略表面变化），并按聚类大小反比重新加权策略优势，使正确但新颖的策略获得更高奖励。

Result: 在数学、物理和医学推理基准测试中，该方法在大量采样预算下持续提升pass@k，增加pass@k曲线下面积(AUC@K)，同时不牺牲pass@1，保持探索并发现更多样化的解题策略。

Conclusion: 通过显式奖励独特的高层解题策略，可以有效解决RL训练LLM时的探索崩溃问题，提升解决方案多样性，为复杂推理任务的后训练提供更有效的强化学习框架。

Abstract: Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@$k$ across large sampling budgets and increases the area under the pass@$k$ curve (AUC@$K$) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.

</details>


### [126] [Asymptotic Universal Alignment: A New Alignment Framework via Test-Time Scaling](https://arxiv.org/abs/2601.08777)
*Yang Cai,Weiqiang Zheng*

Main category: cs.LG

TL;DR: 本文提出了一种新的LLM对齐框架，通过测试时扩展（生成k个候选响应）实现通用对齐，并证明了最优收敛率为k/(k+1)。现有方法如NLHF在k>1时表现不佳，而作者提出的对称多玩家对齐游戏能达到最优对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法难以处理用户异质且可能冲突的偏好，特别是在测试时生成多个候选响应的情况下。传统方法如NLHF虽然对k=1最优，但在多响应场景下无法充分利用测试时扩展的优势，容易导致输出多样性不足。

Method: 提出(k,f(k))-鲁棒对齐和渐进通用对齐（U-alignment）的形式化框架。引入对称多玩家对齐游戏，证明(k+1)-玩家对齐游戏的任何对称纳什均衡策略都能达到最优(k,k/(k+1))-鲁棒对齐。提供了自博弈学习动态的理论收敛保证。

Result: 证明了最优收敛率：存在单输出策略族，其k样本乘积策略能以f(k)=k/(k+1)的速率实现U-alignment，且没有方法能达到更快的收敛速率。NLHF等现有方法在多响应场景下最多只能保证略高于1/2的胜率。

Conclusion: 测试时扩展是实现个性化AI的关键，但现有对齐方法未能充分利用这一优势。提出的对称多玩家对齐游戏框架能保持输出多样性并达到最优测试时扩展速率，为解决用户偏好异质性问题提供了有效方案。

Abstract: Aligning large language models (LLMs) to serve users with heterogeneous and potentially conflicting preferences is a central challenge for personalized and trustworthy AI. We formalize an ideal notion of universal alignment through test-time scaling: for each prompt, the model produces $k\ge 1$ candidate responses and a user selects their preferred one. We introduce $(k,f(k))$-robust alignment, which requires the $k$-output model to have win rate $f(k)$ against any other single-output model, and asymptotic universal alignment (U-alignment), which requires $f(k)\to 1$ as $k\to\infty$. Our main result characterizes the optimal convergence rate: there exists a family of single-output policies whose $k$-sample product policies achieve U-alignment at rate $f(k)=\frac{k}{k+1}$, and no method can achieve a faster rate in general.
  We show that popular post-training methods, including Nash learning from human feedback (NLHF), can fundamentally underutilize the benefits of test-time scaling. Even though NLHF is optimal for $k=1$, sampling from the resulting (often deterministic) policy cannot guarantee win rates above $\tfrac{1}{2}$ except for an arbitrarily small slack. This stems from a lack of output diversity: existing alignment methods can collapse to a single majority-preferred response, making additional samples redundant. In contrast, our approach preserves output diversity and achieves the optimal test-time scaling rate. In particular, we propose a family of symmetric multi-player alignment games and prove that any symmetric Nash equilibrium policy of the $(k+1)$-player alignment game achieves the optimal $(k,\frac{k}{k+1})$-robust alignment. Finally, we provide theoretical convergence guarantees for self-play learning dynamics in these games and extend the framework to opponents that also generate multiple responses.

</details>


### [127] [Fast and explainable clustering in the Manhattan and Tanimoto distance](https://arxiv.org/abs/2601.08781)
*Stefan Güttel,Kaustubh Roy*

Main category: cs.LG

TL;DR: CLASSIX算法扩展到支持曼哈顿距离和Tanimoto距离，通过使用适当的向量范数排序和三角形不等式优化搜索，在化学指纹基准测试中比现有算法快30-80倍且聚类质量更高。


<details>
  <summary>Details</summary>
Motivation: 原始CLASSIX算法仅支持欧氏距离，限制了其在其他距离度量场景的应用。本文旨在扩展CLASSIX算法以支持更多距离度量，特别是曼哈顿距离和Tanimoto距离，以处理更广泛的数据类型和应用场景。

Method: 使用数据向量的适当范数作为排序标准替代主成分分析，结合三角形不等式实现搜索终止。对于Tanimoto距离，采用可证明更尖锐的交集不等式进一步提升算法性能。

Result: 在真实化学指纹基准测试中，CLASSIX Tanimoto算法比Taylor-Butina算法快约30倍，比DBSCAN快约80倍，同时在两种情况下都计算出了更高质量的聚类结果。

Conclusion: 扩展后的CLASSIX算法成功支持了多种距离度量，在保持快速和可解释性的同时，显著提升了在化学指纹等实际应用中的性能表现。

Abstract: The CLASSIX algorithm is a fast and explainable approach to data clustering. In its original form, this algorithm exploits the sorting of the data points by their first principal component to truncate the search for nearby data points, with nearness being defined in terms of the Euclidean distance. Here we extend CLASSIX to other distance metrics, including the Manhattan distance and the Tanimoto distance. Instead of principal components, we use an appropriate norm of the data vectors as the sorting criterion, combined with the triangle inequality for search termination. In the case of Tanimoto distance, a provably sharper intersection inequality is used to further boost the performance of the new algorithm. On a real-world chemical fingerprint benchmark, CLASSIX Tanimoto is about 30 times faster than the Taylor--Butina algorithm, and about 80 times faster than DBSCAN, while computing higher-quality clusters in both cases.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [128] [ChemXDyn: Dynamics-informed species and reaction detection methodology from atomistic simulations](https://arxiv.org/abs/2601.08385)
*Raj Maddipati,Dhruthi Boddapati,Elangannan Arunan,Phani Motamarri,Konduri Aditya*

Main category: physics.comp-ph

TL;DR: ChemXDyn：一种基于动力学感知的轨迹分析方法，通过时间分辨的原子间距离特征识别化学键，避免瞬时非反应性接触的误判，提高反应网络和动力学分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有分子动力学轨迹分析方法基于瞬时距离阈值判断化学键，容易将瞬态非反应性接触误判为化学键，导致虚假中间体、扭曲的反应网络和有偏差的速率估计。

Method: ChemXDyn利用时间分辨的原子间距离特征作为核心原理，通过时间传播分子连接性，同时强制原子价态和配位约束，区分真正的键断裂/形成事件与瞬态非反应性接触。

Result: 在氢、氨和甲烷氧化的ReaxFF和神经网络势MD模拟中，ChemXDyn抑制了静态分析中普遍存在的非物理物种，恢复了与实验一致的反应路径，提高了速率常数估计的保真度。

Conclusion: ChemXDyn通过将原子动力学与化学一致的反应识别联系起来，为MD衍生的反应网络和动力学提供了可转移的基础，在燃烧、催化、等离子体化学和电化学环境中具有潜在应用价值。

Abstract: Accurate identification of chemical species and reaction pathways from molecular dynamics (MD) trajectories is a prerequisite for deriving predictive chemical-kinetic models and for mechanistic discovery in reactive systems. However, state-of-the-art trajectory analysis methods infer bonding from instantaneous distance thresholds, which can misclassify transient, nonreactive encounters as bonds and thereby introduce spurious intermediates, distorted reaction networks, and biased rate estimates. Here, we introduce ChemXDyn, a dynamics-aware computational methodology that leverages time-resolved interatomic distance signatures as a core principle to robustly identify chemically consistent bonded interactions and, consequently, extract meaningful reaction pathways. In particular, ChemXDyn propagates molecular connectivity through time while enforcing atomic valence and coordination constraints to distinguish genuine bond-breaking and bond-forming events from transient, nonreactive encounters. We evaluate ChemXDyn on ReaxFF MD simulations of hydrogen and ammonia oxidation and on neural-network potential MD simulations of methane oxidation, and benchmark its performance against widely used trajectory analysis methods. Across these cases, ChemXDyn suppresses unphysical species prevalent in static analyses, recovers experimentally consistent reaction pathways, and improves the fidelity of rate constant estimation. In ammonia oxidation, ChemXDyn removes unphysical intermediates and resolves key NOx- and N2O-forming and -consuming routes. In methane oxidation, it reconstructs the canonical progression from CH4 to CO2. By linking atomistic dynamics to chemically consistent reaction identification, ChemXDyn provides a transferable foundation for MD-derived reaction networks and kinetics, with potential utility spanning combustion, catalysis, plasma chemistry, and electrochemical environments.

</details>


### [129] [Multi-Task Fine-Tuning Enables Robust Out-of-Distribution Generalization in Atomistic Models](https://arxiv.org/abs/2601.08486)
*Chengqian Zhang,Duo Zhang,Anyang Peng,Mingyu Guo,Yuzhi Zhang,Lei Wang,Guolin Ke,Linfeng Zhang,Tiejun Li,Han Wang*

Main category: physics.comp-ph

TL;DR: 该论文提出多任务微调方法解决原子模型在分布外泛化中的表示崩溃问题，通过联合优化下游属性预测和预训练的力场目标来保持化学先验知识。


<details>
  <summary>Details</summary>
Motivation: 预训练的原子模型在微调后虽然能达到较好的分布内精度，但在分布外条件下的可靠性仍不明确。研究发现标准微调会导致表示崩溃，消除预训练的化学和结构先验知识，严重降低分布外性能。

Method: 提出多任务微调方法，联合优化下游属性预测和从预训练继承的物理基础的力场目标，既保持必要的化学先验知识，又实现任务特定的适应。

Result: 在分子和材料基准测试中，多任务微调一致地改善了分布外泛化能力，接近分布内精度的理论极限，同时优于标准微调、从头训练和最先进的特定任务模型。

Conclusion: 研究确立了安全适应作为大型原子模型的核心要求，并将多任务微调定位为通向稳健分子和材料发现的实用且数据高效的途径。

Abstract: Accurate de novo molecular and materials design requires structure-property models that generalize beyond known regimes. Although pretrained atomistic models achieve strong in-distribution accuracy after fine-tuning, their reliability under out-of-distribution (OOD) conditions remains unclear. We identify a critical failure mode in downstream adaptation: standard fine-tuning induces representation collapse, erasing pretrained chemical and structural priors and severely degrading OOD performance. To address this limitation, we propose multi-task fine-tuning (MFT), which jointly optimizes downstream property prediction with a physically grounded force-field objective inherited from pretraining. This approach preserves essential chemical priors while enabling task-specific adaptation. Across molecular and materials benchmarks, MFT consistently improves OOD generalization, approaching the theoretical limit set by in-distribution accuracy, while outperforming standard fine-tuning, training from scratch, and state-of-the-art task-specific models. These results establish safe adaptation as a central requirement for large atomistic models and position MFT as a practical and data-efficient pathway toward robust molecular and materials discovery.

</details>
