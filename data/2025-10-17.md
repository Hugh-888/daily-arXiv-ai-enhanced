<div id=toc></div>

# Table of Contents

- [quant-ph](#quant-ph) [Total: 36]
- [gr-qc](#gr-qc) [Total: 18]
- [cs.LG](#cs.LG) [Total: 101]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [1] [Quantum Search in Superposed Quantum Lattice Gas Automata and Lattice Boltzmann Systems](https://arxiv.org/abs/2510.14062)
*Călin A. Georgescu,Matthias Möller*

Main category: quant-ph

TL;DR: 本文提出了一种基于离散优化和量子搜索的量子计算流体动力学应用，避免了传统量子晶格气体自动机和量子晶格玻尔兹曼方法中的流场测量限制，通过振幅估计和量子搜索实现渐进量子优势。


<details>
  <summary>Details</summary>
Motivation: 随着计算流体动力学问题规模的扩大，量子计算能否提供优势成为关注焦点。现有的量子晶格气体自动机和量子晶格玻尔兹曼方法主要关注模型开发而非实际应用，且受限于量子态层析和可观测量测量，可能抵消量子优势。

Method: 提出基于离散优化和量子搜索的方法，同时模拟多种晶格配置，使用振幅估计和量子搜索技术，详细分析门级电路实现的复杂度，并比较多种编码方案的利弊。

Result: 该方法能够规避流场测量问题，通过量子搜索和振幅估计提供渐进量子优势，为量子计算流体动力学提供了新的应用方向。

Conclusion: 提出的基于离散优化和量子搜索的方法为量子计算流体动力学开辟了新的应用途径，避免了传统方法的测量限制，有望实现量子优势。

Abstract: As the scope of Computational Fluid Dynamics (CFD) grows to encompass ever
larger problem scales, so does the interest in whether quantum computing can
provide an advantage. In recent years, Quantum Lattice Gas Automata (QLGA) and
Quantum Lattice Boltzmann Methods (QLBM) have emerged as promising candidates
for quantum-native implementations of CFD solvers. Though the progress in
developing QLGA and QLBM algorithms has been significant, it has largely
focused on the development of models rather than applications. As a result, the
zoo of QLGA and QLBM algorithms has grown to target several equations and to
support many extensions, but the practical use of these models is largely
limited to quantum state tomography and observable measurement. This limitation
is crucial in practice, because unless very specific criteria are met, such
measurements may cancel out any potential quantum advantage. In this paper, we
propose an application based on discrete optimization and quantum search, which
circumvents flow field measurement altogether. We propose methods for
simulating many different lattice configurations simultaneously and describe
how the usage of amplitude estimation and quantum search can provide an
asymptotic quantum advantage. Throughout the paper, we provide detailed
complexity analyses of gate-level implementations of our circuits and consider
the benefits and costs of several encodings.

</details>


### [2] [Universal Growth of Krylov Complexity Across A Quantum Phase Transition](https://arxiv.org/abs/2510.13947)
*András Grabarits,Adolfo del Campo*

Main category: quant-ph

TL;DR: 本文研究了量子系统穿越量子相变时Krylov空间中扩展复杂度的统计特性，建立了复杂度增长与Kibble-Zurek缺陷标度律的精确联系。


<details>
  <summary>Details</summary>
Motivation: 研究量子相变过程中复杂度演化的统计特性，探索复杂度与量子临界现象之间的深层联系。

Method: 使用非绝热Magnus展开将演化映射到有效的一维跳跃模型，以横向场Ising模型为具体研究对象。

Result: 发现复杂度的所有累积量都表现出与缺陷密度相同的幂律标度行为，系数与均值相同，且全分布渐近趋于高斯分布。

Conclusion: 这些结果为任意二阶量子相变中复杂度的增长提供了通用的标度论证。

Abstract: We study the statistical properties of the spread complexity in the Krylov
space of quantum systems driven across a quantum phase transition. Using the
diabatic Magnus expansion, we map the evolution to an effective one-dimensional
hopping model. For the transverse field Ising model, we establish an exact link
between the growth of complexity and the Kibble-Zurek defect scaling: all
cumulants of complexity exhibit the same power-law scaling as the defect
density, with coefficients identical to the mean, and the full distribution
asymptotically becomes Gaussian. These results yield general scaling arguments
for the growth of complexity across arbitrary second-order quantum phase
transitions.

</details>


### [3] [Quantum State Designs via Magic Teleportation](https://arxiv.org/abs/2510.13950)
*Hugo Lóio,Guglielmo Lami,Lorenzo Leone,Max McGinley,Xhek Turkeshi,Jacopo De Nardis*

Main category: quant-ph

TL;DR: 非稳定子资源通过投影系综促进量子态设计的形成，误差随预测量态的稳定子Renyi熵指数衰减，揭示了魔术诱导设计机制和魔术遥传现象。


<details>
  <summary>Details</summary>
Motivation: 研究非稳定子资源如何在投影系综中促进量子态设计的形成，探索在早期容错设备中生成高度随机态的系统性方法。

Method: 从具有有限魔术的初始态出发，应用无资源的Clifford电路进行混洗，在最终态的子系统上进行投影Pauli测量，分析生成的系综。

Result: 投影系综以指数衰减的误差收敛到态k-设计，误差与预测量态的k阶稳定子Renyi熵相关，识别了魔术遥传机制和普适标度形式。

Conclusion: 少量受控的魔术资源足以生成高度随机态，为在早期容错设备中生成量子态设计提供了系统性途径。

Abstract: We investigate how non-stabilizer resources enable the emergence of quantum
state designs within the projected ensemble. Starting from initial states with
finite magic and applying resource-free Clifford circuits to scramble them, we
analyze the ensemble generated by performing projective Pauli measurements on a
subsystem of the final state. Using both analytical arguments and large-scale
numerics, we show that the projected ensemble converges towards a state
$k$-design with an error that decays exponentially with the $k$-th Stabilizer
Renyi Entropy of the pre-measurement state, via a Magic-Induced Design Ansatz
(MIDA) that we introduce. We identify a universal scaling form, valid across
different classes of magic initial states, and corroborate it through numerical
simulations and analytical calculations of the frame potential. For
finite-depth Clifford unitaries, we show that the timescales at which state
designs emerge are controlled by the transport of magic. We identify a ``magic
teleportation'' mechanism whereby non-Clifford resources injected locally
spread through Clifford scrambling and measurements across distances beyond the
lightcone. Our results demonstrate how a small and controlled amount of magic
suffices to generate highly random states, providing a systematic route toward
generating quantum state designs in early fault-tolerant devices.

</details>


### [4] [Temporal Entanglement Transitions in the Periodically Driven Ising Chain](https://arxiv.org/abs/2510.13970)
*Karun Gadge,Abhinav Prem,Rishabh Jha*

Main category: quant-ph

TL;DR: 在Floquet自旋链中发现了时间纠缠相变，这是一种纠缠驱动的量子相变，表现为纠缠谱的周期性重组和对称性量子数的翻转，具有普适临界行为。


<details>
  <summary>Details</summary>
Motivation: 研究周期性驱动的量子系统中非平衡现象，特别是纠缠动力学中的新特征，这些特征在静态系统中没有对应物。

Method: 通过分析Floquet自旋链的纠缠哈密顿量谱，研究Schmidt间隙闭合、纠缠回波消失和对称性量子数翻转等现象。

Result: 发现了跨越广泛驱动频率范围的时间纠缠相变，这些相变具有普适临界行为，相关长度指数ν=1，与平衡Ising普适类匹配。

Conclusion: 时间纠缠相变是Floquet量子物质中的新特征，由纯动力学机制产生，与静态临界性解耦。

Abstract: Periodically driven quantum systems can host non-equilibrium phenomena
without static analogs, including in their entanglement dynamics. Here, we
discover $temporal$ $entanglement$ $transitions$ in a Floquet spin chain, which
correspond to a quantum phase transition in the spectrum of the entanglement
Hamiltonian and are signaled by dynamical spontaneous symmetry breaking. We
show that these transitions are entanglement-driven, i.e., they require
initially entangled states and remain invisible to conventional local
observables. Intriguingly, we find these transitions across a broad range of
driving frequencies (from adiabatic to high-frequency regime) and independently
of drive details, where they manifest as periodic, sharp entanglement spectrum
reorganizations marked by the Schmidt-gap closure, a vanishing entanglement
echo, and symmetry-quantum-number flips. At high frequencies, the entanglement
Hamiltonian acquires an intrinsic timescale decoupled from the drive period,
rendering the transitions genuine steady-state features. Finite-size scaling
reveals universal critical behavior with correlation-length exponent $\nu=1$,
matching equilibrium Ising universality despite its emergence from purely
dynamical mechanisms decoupled from static criticality. Our work establishes
temporal entanglement transitions as novel features in Floquet quantum matter.

</details>


### [5] [Towards gravimetry enhancement with squeezed states](https://arxiv.org/abs/2510.13973)
*Oziel R. de Araujo,Lucas S. Marinho,Jonas F. G. Santos,Carlos H. S. Vieira*

Main category: quant-ph

TL;DR: 研究在量子计量框架下使用压缩探针态估计重力加速度的灵敏度，发现压缩相位对精度有重要影响，位置-动量相关输入态可突破散粒噪声极限。


<details>
  <summary>Details</summary>
Motivation: 探索压缩探针态在重力加速度测量中的灵敏度，特别关注压缩相位（不仅是振幅）对可达到精度的影响。

Method: 在量子计量框架下分析压缩探针态，研究不同压缩相位配置对量子Fisher信息的影响，比较位置-动量相关输入态的性能。

Result: 沿正则相空间正交压缩的探针无法超越散粒噪声极限，而位置-动量相关输入态可突破该极限；最优灵敏度通过投影动量测量和时变压缩相位调整实现。

Conclusion: 压缩相位工程在实验重力测量协议中具有基础性作用，相位优化对实现超精密重力测量至关重要。

Abstract: We investigate the estimation sensitivity of gravitational acceleration using
squeezed probe states within a quantum metrology framework. In particular, we
analyze how the squeezing phase, beyond its amplitudes, of the probes affects
the attainable precision. We find that probes squeezed along the canonical
phase-space quadrature can fail to achieve a quantum Fisher information (QFI)
surpassing the shot-noise limit, regardless of the interaction time with the
gravitational field. In contrast, position-momentum correlated input states
with the squeezing amplitude can overcome this limit. Furthermore, we show that
optimal sensitivity is attained through projective momentum measurements
combined with a time-dependent adjustment of the squeezing phase. Our results
are important to highlight the fundamental role of phase-engineered squeezing
in experimental gravimetry protocols.

</details>


### [6] [Sequential Quantum Measurements and the Instrumental Group Algebra](https://arxiv.org/abs/2510.13980)
*Christopher S. Jackson*

Main category: quant-ph

TL;DR: 该论文提出了仪器群(IG)和Kraus算子密度(KOD)的概念，为连续时间测量提供了理论框架，解决了正交投影公设无法测量位置、动量等基本可观测量的问题。


<details>
  <summary>Details</summary>
Motivation: 许多基本可观测量（位置、动量、相点、自旋方向）无法通过遵守正交投影公设的仪器进行测量，需要连续时间测量的理论框架来理解这些可观测量。

Method: 引入仪器群(IG)概念，定义Kraus算子密度(KOD)，建立KOD的Kolmogorov方程，构建仪器群代数(IGA)作为KOD的数学基础，并引入超算子(ultraoperators)概念。

Result: 证明了连续测量与顺序测量中仪器组合结构对应于KOD的卷积，建立了KOD Kolmogorov方程与Lindblad主方程的关系，推导了跳跃过程和扩散过程的KOD Kolmogorov生成元。

Conclusion: 仪器群代数(IGA)为Kraus算子密度提供了合适的数学框架，类似于冯·诺依曼代数对偶是密度算子的家园，为连续时间量子测量提供了完整的理论体系。

Abstract: Many of the most fundamental observables | position, momentum, phase-point,
and spin-direction | cannot be measured by an instrument that obeys the
orthogonal projection postulate. Continuous-in-time measurements provide the
missing theoretical framework to make sense of such observables. The elements
of the time-dependent instrument define a group called the \emph{instrumental
group} (IG). Relative to the IG, all of the time-dependence is contained in a
certain function called the \emph{Kraus-operator density} (KOD), which evolves
according to a classical Kolmogorov equation. Unlike the Lindblad master
equation, the KOD Kolmogorov equation is a direct expression of how the
elements of the instrument (not just the total channel) evolve. Shifting from
continuous measurement to sequential measurements more generally, the structure
of combining instruments in sequence is shown to correspond to the convolution
of their KODs. This convolution promotes the IG to an \emph{involutive Banach
algebra} (a structure that goes all the way back to the origins of POVM and
C*-algebra theory) which will be called the \emph{instrumental group algebra}
(IGA). The IGA is the true home of the KOD, similar to how the dual of a von
Neumann algebra is the home of the density operator. Operators on the IGA,
which play the same role for KODs as superoperators play for density operators,
are called \emph{ultraoperators} and various examples are discussed. Certain
ultraoperator-superoperator intertwining relations are considered, including
the relation between the KOD Kolmogorov equation and the Lindblad master
equation. The IGA is also shown to have actually two involutions: one respected
by the convolution ultraoperators and the other by the quantum channel
superoperators. Finally, the KOD Kolmogorov generators are derived for jump
processes and more general diffusive processes.

</details>


### [7] [A Rigorous Quantum Framework for Inequality-Constrained and Multi-Objective Binary Optimization](https://arxiv.org/abs/2510.13983)
*Sebastian Egginger,Kristina Kirova,Sonja Bruckner,Stefan Hillmich,Richard Kueng*

Main category: quant-ph

TL;DR: 提出了MOQA框架，将不等式约束转化为多目标优化问题，使用p-范数近似最大值函数，为量子计算机处理约束优化问题提供新方法。


<details>
  <summary>Details</summary>
Motivation: 量子优化中处理不等式约束是一个主要挑战，现有方法主要针对无约束QUBO问题，而现实世界问题往往包含约束条件。

Method: 将不等式约束转化为多目标优化，使用p-范数近似最大值函数，构建可直接在哈密顿量层面操作的框架。

Result: MOQA框架具有严格的性能保证，兼容多种量子基态求解器，且不限于二次函数。

Conclusion: MOQA为量子计算机处理约束优化问题提供了通用且理论保证的解决方案。

Abstract: Encoding combinatorial optimization problems into physically meaningful
Hamiltonians with tractable energy landscapes forms the foundation of quantum
optimization. Numerous works have studied such efficient encodings for the
class of Quadratic Unconstrained Binary Optimization (QUBO) problems. However,
many real-world tasks are constrained, and handling equality and, in
particular, inequality constraints on quantum computers remains a major
challenge. In this letter, we show that including inequality constraints is
equivalent to solving a multi-objective optimization. This insight motivates
the Multi-Objective Quantum Approximation (MOQA) framework, which approximates
the maximum via smaller $p$-norms and comes with rigorous performance
guarantees. MOQA operates directly at the Hamiltonian level and is compatible
with, but not restricted to, ground-state solvers such as quantum adiabatic
annealing, the Quantum Approximate Optimization Algorithm (QAOA), or
imaginary-time evolution. Moreover, it is not limited to quadratic functions.

</details>


### [8] [A Rigorous Quantum Framework for Inequality-Constrained and Multi-Objective Binary Optimization: Quadratic Cost Functions and Empirical Evaluations](https://arxiv.org/abs/2510.13987)
*Sebastian Egginger,Kristina Kirova,Sonja Bruckner,Stefan Hillmich,Richard Kueng*

Main category: quant-ph

TL;DR: 提出了名为MOQA的新框架，可将多目标QUBO问题映射为可处理的Ising型哈密顿量的基态问题，为路由、分区和不等式约束优化问题提供量子解决方案。


<details>
  <summary>Details</summary>
Motivation: 量子优化方法需要将原始问题映射到可处理的量子能量景观，但现有方法主要处理单目标QUBO问题，缺乏处理多目标优化问题的有效映射框架。

Method: 开发了多目标量子近似(MOQA)框架，通过新的可处理映射将多目标QUBO问题重新构造为Ising型哈密顿量的基态问题。

Result: MOQA框架能够处理各种路由问题、分区问题和不等式约束的二进制优化问题，为这些实际问题提供了新的量子和量子启发式解决方案可能性。

Conclusion: MOQA框架扩展了量子优化方法的适用范围，为多目标优化问题提供了新的映射途径，推动了量子计算在复杂优化问题中的应用。

Abstract: The prospect of quantum solutions for complicated optimization problems is
contingent on mapping the original problem onto a tractable quantum energy
landscape, e.g. an Ising-type Hamiltonian. Subsequently, techniques like
adiabatic optimization, quantum annealing, and the Quantum Approximate
Optimization Algorithm (QAOA) can be used to find the ground state of this
Hamiltonian. Quadratic Unconstrained Binary Optimization (QUBO) is one
prominent problem class for which this entire pipeline is well understood and
has received considerable attention over the past years. In this work, we
provide novel, tractable mappings for the maxima of multiple QUBO problems.
Termed Multi-Objective Quantum Approximations, or MOQA for short, our framework
allows us to recast new types of classical binary optimization problems as
ground state problems of a tractable Ising-type Hamiltonian. This, in turn,
opens the possibility of new quantum- and quantum-inspired solutions to a
variety of problems that frequently occur in practical applications. In
particular, MOQA can handle various types of routing and partitioning problems,
as well as inequality-constrained binary optimization problems.

</details>


### [9] [Continuous-variable photonic quantum extreme learning machines for fast collider-data selection](https://arxiv.org/abs/2510.13994)
*Benedikt Maier,Michael Spannowsky,Simon Williams*

Main category: quant-ph

TL;DR: 该论文研究了连续变量光子量子极限学习机作为快速、低开销的碰撞机数据处理前端。通过固定时间高斯量子基底处理数据，仅训练线性分类器，在顶夸克喷注重建和希格斯玻色子识别任务中表现优于传统多层感知机。


<details>
  <summary>Details</summary>
Motivation: 研究光子量子极限学习机作为快速、低开销的碰撞机数据处理前端，旨在利用量子系统的优势提供紧凑且表达能力强的随机特征，同时保持确定性的时序和快速重训练能力。

Method: 数据通过正交位移编码到光子模式中，通过固定时间高斯量子基底传播，使用高斯兼容测量产生高维随机特征映射，仅训练线性分类器（单次线性求解）。

Result: 在标准公共数据集上，光子QELM在所有训练规模下都优于具有两个隐藏单元的多层感知机，在大样本量时匹配或超过具有十个隐藏单元的多层感知机，同时仅训练线性读出层。

Conclusion: 高斯光子极限学习机可以在固定延迟下提供紧凑且表达能力强的随机特征，其确定性时序、快速重训练、低光功率和室温操作使其成为未来碰撞机实验中在线数据选择和第一阶段触发器集成的可信构建模块。

Abstract: We study continuous-variable photonic quantum extreme learning machines as
fast, low-overhead front-ends for collider data processing. Data is encoded in
photonic modes through quadrature displacements and propagated through a
fixed-time Gaussian quantum substrate. The final readout occurs through
Gaussian-compatible measurements to produce a high-dimensional random feature
map. Only a linear classifier is trained, using a single linear solve, so
retraining is fast, and the optical path and detector response set the
analytical and inference latency. We evaluate this architecture on two
representative classification tasks, top-jet tagging and Higgs-boson
identification, with parameter-matched multi-layer perceptron (MLP) baselines.
Using standard public datasets and identical train, validation, and test
splits, the photonic Quantum Extreme Learning Machine (QELM) outperforms an MLP
with two hidden units for all considered training sizes, and matches or exceeds
an MLP with ten hidden units at large sample sizes, while training only the
linear readout. These results indicate that Gaussian photonic extreme-learning
machines can provide compact and expressive random features at fixed latency.
The combination of deterministic timing, rapid retraining, low optical power,
and room temperature operation makes photonic QELMs a credible building block
for online data selection and even first-stage trigger integration at future
collider experiments.

</details>


### [10] [Qutrits for physics at the LHC](https://arxiv.org/abs/2510.14001)
*Miranda Carou Laiño,Veronika Chobanova,Miriam Lucio Martínez*

Main category: quant-ph

TL;DR: 探索使用基于qutrit的量子机器学习模型在高能物理数据中进行异常检测，重点研究LHC应用，并与qubit方法进行性能比较。


<details>
  <summary>Details</summary>
Motivation: 下一代对撞机（如HL-LHC）在数据处​​理、信号重建和分析方面面临重大挑战，需要寻找新的计算方法来识别超出标准模型的异常事件。

Method: 开发qutrit量子模型，并与基于qubit的方法进行基准测试，评估准确性、可扩展性和计算效率。

Result: 论文旨在确定qutrit架构是否能在未来对撞机实验的计算和分析需求方面提供优势。

Conclusion: 研究探索了qutrit量子机器学习在高能物理异常检测中的应用潜力，为应对未来对撞机实验的计算挑战提供了新思路。

Abstract: The identification of anomalous events, not explained by the Standard Model
of particle physics, and the possible discovery of exotic physical phenomena
pose significant theoretical, experimental and computational challenges. The
task will intensify at next-generation colliders, such as the High- Luminosity
Large Hadron Collider (HL-LHC). Consequently, considerable challenges are
expected concerning data processing, signal reconstruction, and analysis. This
work explores the use of qutrit- based Quantum Machine Learning models for
anomaly detection in high-energy physics data, with a focus on LHC
applications. We propose the development of a qutrit quantum model and
benchmark its performance against qubit-based approaches, assessing accuracy,
scalability, and computational efficiency. This study aims to establish whether
qutrit architectures can offer an advantage in addressing the computational and
analytical demands of future collider experiments.

</details>


### [11] [Decoding Correlated Errors in Quantum LDPC Codes](https://arxiv.org/abs/2510.14060)
*Arshpreet Singh Maan,Francisco-Garcia Herrero,Alexandru Paler,Valentin Savin*

Main category: quant-ph

TL;DR: 提出了一个用于量子LDPC码相关错误解码的框架，通过图增强和重连方法消除Y型错误相关的4-cycles，在电路级噪声下实现高精度低延迟解码。


<details>
  <summary>Details</summary>
Motivation: 量子LDPC码在电路级噪声下存在相关错误问题，需要开发高效的解码方法来处理这些相关错误，同时保持高精度和低延迟。

Method: 使用图增强和重连干扰(GARI)方法修改相关检测器错误模型，消除涉及Y型错误的4-cycles，采用归一化最小和译码器与混合串行分层调度，并通过集成解码进一步提高性能。

Result: 在距离12的gross码上，在物理错误率10^-3时实现逻辑错误率(6.70±1.93)×10^-9，FPGA实现显示每轮平均解码延迟273纳秒，99.99%的解码实例延迟低于微秒级。

Conclusion: 该方法在保持解码问题等价性的同时，显著提高了量子LDPC码在电路级噪声下的解码精度和速度，为实时量子纠错提供了可行方案。

Abstract: We introduce a decoding framework for correlated errors in quantum LDPC codes
under circuit-level noise. The core of our approach is a graph augmentation and
rewiring for interference (GARI) method, which modifies the correlated detector
error model by eliminating 4-cycles involving Y-type errors, while preserving
the equivalence of the decoding problem. We test our approach on the bivariate
bicycle codes of distances 6, 10, and 12. A normalized min-sum decoder with a
hybrid serial-layered schedule is applied on the transformed graph, achieving
high accuracy with low latency. Performance is further enhanced through
ensemble decoding, where 24 randomized normalized min-sum decoders run in
parallel on the transformed graph, yielding the highest reported accuracy (on
par with XYZ-Relay-BP) with unprecedented speed for the tested codes under
uniform depolarizing circuit level noise. For the distance 12 (gross) code, our
approach yields a logical error rate of $(6.70 \pm 1.93) \times 10^{-9}$ at a
practical physical error rate of $10^{-3}$. Furthermore, preliminary FPGA
implementation results show that such high accuracy can be achieved in real
time, with a per-round average decoding latency of 273 ns and sub-microsecond
latency in 99.99% of the decoding instances.

</details>


### [12] [Quantum Low-Density Parity-Check Codes](https://arxiv.org/abs/2510.14090)
*Bane Vasic,Valentin Savin,Michele Pacenti,Shantom Borah,Nithin Raveendran*

Main category: quant-ph

TL;DR: 本文深入探讨了量子低密度奇偶校验(QLDPC)码及其迭代解码器，为信息理论背景的读者提供量子纠错码的全面介绍，重点关注其理论基础、量子信道特性、关键编码构造和解码算法。


<details>
  <summary>Details</summary>
Motivation: 量子纠错是量子计算的核心，QLDPC码因其低复杂度迭代解码和接近容量的性能而受到关注，有望实现恒定开销的容错量子计算，突破传统编码的最小距离限制。

Method: 文章系统介绍了QLDPC码的理论基础，包括量子信道的独特特性、关键编码构造方法，以及高效的迭代解码算法，特别关注有限长度编码的构造和解码挑战。

Result: QLDPC码结合高效解码算法和容错协议，为实现低开销、容错量子计算提供了有前景的路径，但有限长度编码的构造和解码仍是挑战。

Conclusion: QLDPC码在量子信息科学中具有重要影响和广阔前景，尽管面临硬件特定特性适配的挑战，但为量子纠错和容错量子计算开辟了新的研究方向。

Abstract: Quantum error correction (QEC) is a cornerstone of quantum computing,
enabling reliable information processing in the presence of noise. Sparse
stabilizer codes -- referred to generally as quantum low-density parity-check
(QLDPC) codes -- have risen to the forefront of QEC research in recent years.
This can be attributed to several key factors. First, classical LDPC codes
admit low-complexity belief propagation iterative decoding and near-capacity
performance, which contributed to the early interest in QLDPC codes. Then, the
result promising constant overhead fault tolerance using QLDPC codes led to the
search for code families that go beyond the long-holding $\sqrt{n}$ scaling
barrier of minimum distance for codelength $n$. This resulted in recent
breakthroughs in the construction of QLDPC codes, which, combined with
efficient decoding algorithms and the development of fault-tolerant protocols
operating on QLDPC-encoded quantum information, provide a promising pathway to
low-overhead, fault-tolerant quantum computation. However, despite their
potential, challenges remain, particularly in constructing and decoding
finite-length codes that account for, or efficiently leverage, specific
characteristics of quantum hardware, such as connectivity, topology, native
gate sets, and noise models. This article provides an in-depth examination of
QLDPC codes and their iterative decoders, catering to an information theory
audience with no or limited background in quantum mechanics. We discuss the
theoretical underpinnings, explore unique characteristics of quantum channels,
and delineate key code constructions and decoding algorithms, ultimately
highlighting the impact and future prospects of QLDPC codes in quantum
information science.

</details>


### [13] [Quantum machine learning and quantum-inspired methods applied to computational fluid dynamics: a short review](https://arxiv.org/abs/2510.14099)
*Cesar A. Amaral,Vinícius L. Oliveira,Juan P. L. C. Salazar,Eduardo I. Duzzioni*

Main category: quant-ph

TL;DR: 该综述探讨了量子计算、量子算法、机器学习和张量网络技术在计算流体动力学(CFD)中的交叉应用，重点介绍了变分量子算法、量子神经网络和张量网络方法在解决高维、多尺度和湍流问题中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统CFD方法在高维、多尺度和湍流条件下面临严重的可扩展性挑战，计算成本过高。量子计算和量子启发方法被视为有前景的替代方案。

Method: 综述了变分量子算法作为混合量子-经典PDE求解器，量子非线性处理单元处理非线性问题，量子神经网络和量子物理信息神经网络，以及源自量子多体系统的张量网络方法在CFD中的应用。

Result: 报告的研究显示，这些方法在保持精度的同时，实现了内存和运行时间的几个数量级减少。张量网络方法已显示出实际效益。

Conclusion: 在NISQ时代，量子CFD仍难以实现，但量子启发的张量网络已显示实际优势，混合方法提供了最有前景的近期策略。

Abstract: Computational Fluid Dynamics (CFD) is central to science and engineering, but
faces severe scalability challenges, especially in high-dimensional,
multiscale, and turbulent regimes. Traditional numerical methods often become
prohibitively expensive under these conditions. Quantum computing and
quantum-inspired methods have been investigated as promising alternatives. This
review surveys advances at the intersection of quantum computing, quantum
algorithms, machine learning, and tensor network techniques for CFD. We discuss
the use of Variational Quantum Algorithms as hybrid quantum-classical solvers
for PDEs, emphasizing their ability to incorporate nonlinearities through
Quantum Nonlinear Processing Units. We further review Quantum Neural Networks
and Quantum Physics-Informed Neural Networks, which extend classical machine
learning frameworks to quantum hardware and have shown advantages in parameter
efficiency and solution accuracy for certain CFD benchmarks. Beyond quantum
computing, we examine tensor network methods, originally developed for quantum
many-body systems and now adapted to CFD as efficient high-dimensional
compression and solver tools. Reported studies include several orders of
magnitude reductions in memory and runtime while preserving accuracy. Together,
these approaches highlight quantum and quantum-inspired strategies that may
enable more efficient CFD solvers. This review closes with perspectives:
quantum CFD remains out of reach in the NISQ era, but quantum-inspired tensor
networks already show practical benefits, with hybrid approaches offering the
most promising near-term strategy.

</details>


### [14] [Symmetry-protected states of interacting qubits in superconducting quantum circuits](https://arxiv.org/abs/2510.14121)
*Yi Shi,Eran Ginossar,Michael Stern,Marzena Szymanska*

Main category: quant-ph

TL;DR: 提出了一种基于四自旋相互作用模型的对称保护量子比特，该量子比特对局域噪声具有鲁棒性，可在超导电路中实现毫秒级相干时间。


<details>
  <summary>Details</summary>
Motivation: 超导电路是量子信息处理的主要候选平台之一，但需要解决噪声导致的退相干问题。具有内在噪声保护的量子比特近年来发展迅速，通常通过隔离计算态与局域噪声源来实现保护。

Method: 设计了一个至少需要四个自旋的相互作用模型，包含最近邻和次近邻耦合。将两个最低本征态映射为对称保护的量子比特流形，并将其映射到超导电路实现。

Result: 该电路在现实环境噪声下能够达到超过几毫秒的相干时间，显著提升了量子比特的相干性能。

Conclusion: 这项工作为实现具有长相干时间的新一代量子器件开辟了新途径，为构建更稳定的量子计算系统提供了可行方案。

Abstract: Superconducting circuits are one of the leading candidates for storing and
manipulating quantum information. Among them, qubits embedded with intrinsic
noise protection have seen rapid advancements in recent years. This noise
protection is typically realized by isolating the computational states from
local sources of noise. Here, we propose an interacting spin model that
requires at least four spins with nearest-neighbor and next-nearest-neighbor
couplings, where the two lowest eigenstates form a symmetry-protected qubit
manifold, which is robust to both relaxation and dephasing from local
perturbations. We map the spin model to a superconducting circuit and show that
such a circuit can reach coherence times exceeding several milliseconds in the
presence of realistic environmental noise. Our work opens a pathway to
realizing qubits with long coherence times in a new generation of quantum
devices.

</details>


### [15] [Universal energy-space localization and stable quantum phases against time-dependent perturbations](https://arxiv.org/abs/2510.14160)
*Hongye Yu,Tzu-Chieh Wei*

Main category: quant-ph

TL;DR: 该论文证明了q-local哈密顿量中的能量空间局域化现象在一般含时扰动下依然存在，这种局域化特性可用于证明系统的稳定性，并在自旋玻璃模型、LDPC码和量子优化算法中具有重要应用。


<details>
  <summary>Details</summary>
Motivation: 量子系统的稳定性是一个重要但难以严格证明的性质，目前大多数研究只考虑静态扰动，而系统是否能在一般含时扰动下保持稳定仍是一个未解决的问题。

Method: 通过识别q-local哈密顿量中的能量空间局域化现象，并证明该现象在一般含时扰动下依然存在，其中演化态在瞬时谱的能量窗口内呈指数局域化。

Result: 证明了能量空间局域化在含时扰动下依然保持，且泄漏界限在任意单调时间重标度下保持不变。在自旋玻璃模型中，能量空间局域化可诱导构型空间局域化并打破遍历性。在LDPC码中，演化态在原始码字附近局域化时间呈指数长。

Conclusion: 该工作为分析一般量子系统的非平衡动力学提供了新视角，并为稳定性证明和量子算法设计提供了通用的数学工具。

Abstract: Stability against perturbation is a highly nontrivial property of quantum
systems and is often a requirement to define new phases. In most systems where
stability can be rigorously established, only static perturbations are
considered; whether a system is stable against generic time-dependent
perturbations remains largely elusive. In this work, we identify a universal
phenomenon in $q$-local Hamiltonians called energy-space localization and prove
that it can survive under generic time-dependent perturbations, where the
evolving state is exponentially localized in an energy window of the
instantaneous spectrum. The property holds ubiquitously, and the leakage bounds
remain invariant under arbitrarily monotonic rescaling of evolution time. This
flexibility enables the energy-space localization to be a powerful tool in
proving the stability of systems. For spin glass models where the configuration
spaces are separated by large energy barriers, the localization in energy space
can induce a true localization in the configuration space and robustly break
ergodicity. We then demonstrate the applications of our results in several
systems with such barriers. For certain LDPC codes, we show that the evolving
state is localized near the original codeword for an exponentially long time
even under generic time-dependent perturbations. We also extend the stability
of LDPC codes against static $q$-local perturbations to quasi-$q$-local. In
addition, we show that for some classical hard optimization problems with
clustered solution space, the stability becomes an obstacle for quantum
Hamiltonian-based algorithms to drive the system out of local minima. Our work
provides a new lens for analyzing the non-equilibrium dynamics of generic
quantum systems, and versatile mathematical tools for stability proving and
quantum algorithm design.

</details>


### [16] [Classification of Transuranium Elements in Terms of `Winding' Numbers in the Bohr-Sommerfeld Model](https://arxiv.org/abs/2510.14289)
*Sergei K. Suslov*

Main category: quant-ph

TL;DR: 使用玻尔-索末菲原子模型研究铀、鿫及超重元素氢类离子的轨道行为，发现强库仑场中会出现自相交轨道，可用缠绕数进行拓扑分类。


<details>
  <summary>Details</summary>
Motivation: 重新审视玻尔-索末菲原子模型，探索超强库仑场中的轨道行为，建立早期量子理论与现代超重元素物理之间的概念桥梁。

Method: 采用索末菲精细结构公式和计算机代数方法，分析氢类离子在超强库仑场中的轨道特性。

Result: 发现从鿫元素(Z=118)开始，在Z≤137的超重元素中会出现自相交轨道，这些轨道可通过缠绕数进行拓扑分类。

Conclusion: 半经典方法为理解超强库仑场中的轨道行为提供了有价值的历史和教学视角，揭示了早期量子理论与现代超重元素物理之间的联系。

Abstract: We revisit the Bohr-Sommerfeld atomic model to explore hydrogen-like ions of
Uranium ($Z=92$), Oganesson ($Z=118$), and hypothetical superheavy elements
beyond. Although superseded by the Dirac equation and modern quantum
electrodynamics, the semiclassical approach offers a historically and
pedagogically valuable perspective. Using the Sommerfeld fine structure formula
and computer algebra methods, we demonstrate the appearance of
self-intersecting orbits in super strong Coulomb fields, beginning with
Oganesson and hypothetical elements up to $Z\le137$. These orbits can be
classified by their `winding' numbers, providing a simple topological
description of Coulomb field strength in this framework. Our results highlight
a conceptual bridge between early quantum theory and modern superheavy element
physics.

</details>


### [17] [Quantifiers of Noise Reducibility Under Restricted Control](https://arxiv.org/abs/2510.14316)
*Graeme D. Berk,Kavan Modi,Simon Milz*

Main category: quant-ph

TL;DR: 本文为量子过程引入了实用的量化指标，满足单调性要求，解决了先前基于状态方法的不足。这些指标代表了量子过程可能展现的最大时间互信息量，并应用于开环控制下的噪声减少问题。


<details>
  <summary>Details</summary>
Motivation: 量子过程的关联结构（由量子梳描述）是量子信息协议和控制任务的重要资源。现有基于量子状态的量化方法存在不足，需要开发满足单调性要求的实用量子过程量化指标。

Method: 引入满足单调性性质的量子过程实用量化指标，将其应用于开环控制下的量子过程噪声减少问题，研究其资源组成行为，并与广义梳散度概念建立联系。

Result: 新量化指标代表量子过程可能展现的最大时间互信息量，能够正确评估量子过程的资源价值。重新解释先前研究中关于动力学解耦与非马尔可夫记忆关系的数值发现。

Conclusion: 尽管先前研究使用了不充分的资源量化指标，但其主要结论——将动力学解耦解释为资源蒸馏——仍然成立。新量化指标为量子过程资源评估提供了更可靠的工具。

Abstract: The correlation structure of multitime quantum processes - succinctly
described by quantum combs - is an important resource for many quantum
information protocols and control tasks. Inspired by approaches for quantum
states, we introduce quantifiers of the practical utility of quantum processes
that satisfy monotonicity properties, thus overcoming shortcomings in previous
state-motivated approaches. Applying these quantifiers to the problem of noise
reduction of a quantum process under open-loop control, they are shown to
represent the largest amount of temporal mutual information that a process can
possibly exhibit. In addition, we study their resource composition behaviour
and connect them to the recently introduced notion of generalised comb
divergences. Finally, in light of these new quantifiers, we re-interpret the
numerical findings of npj Quantum Information 9, 104 (2023) on the relationship
of dynamical decoupling and non-Markovian memory, which were based on
insufficient resource quantifiers, and show that its main conclusion - the
interpretation of dynamical decoupling as a resource distillation - still
holds.

</details>


### [18] [Offline Dedicated Quantum Attacks on Block Ciphers Based on Two Parallel Permutation-Based Pseudorandom Functions](https://arxiv.org/abs/2510.14475)
*Xiao-Fan Zhen,Zhen-Qiang Li,Jia-Cheng Fan,Su-Juan Qin,Fei Gao*

Main category: quant-ph

TL;DR: 本文提出了针对PolyMAC和基于双并行置换伪随机函数(TPP-PRFs)的块密码的量子密码分析，包括新的攻击结构和离线量子攻击方法，显著降低了查询复杂度。


<details>
  <summary>Details</summary>
Motivation: 量子密码分析对于评估密码系统在量子计算威胁下的安全性至关重要。Shi等人提出的专用量子攻击虽然减少了资源需求，但仍依赖于在线查询，且适用结构有限。

Method: 一方面发现了新的可攻击密码结构(PolyMAC和TPP-PRFs块密码)，另一方面针对TPP-PRFs块密码构建解耦XOR型函数，提出离线量子攻击方法。

Result: 离线攻击显著降低了查询复杂度：在量子查询模型中，查询次数从$\tilde O(2^{(n+t)/2})$降至$\tilde O(2^{t})$；在经典查询模型中，查询复杂度和时间复杂度均从$\tilde O(2^{(2n)/3})$优化至$\tilde O(2^{(2n-t)/3})$。

Conclusion: 本文扩展了专用量子攻击的适用范围，并提出了高效的离线量子攻击方法，为评估密码系统的量子安全性提供了重要工具。

Abstract: Quantum cryptanalysis is essential for evaluating the security of
cryptographic systems against the threat of quantum computing. Recently, Shi et
al. introduced the dedicated quantum attack on XOR-type function that greatly
reduces the required resources (including circuit depth, width, and the number
of gates) compared to the parallel Grover-meets-Simon algorithm. Here, our
contribution is in two aspects. On the one hand, we discover new cryptographic
structures amenable to this attack: PolyMAC and block ciphers based on two
parallel permutation-based pseudorandom functions (TPP-PRFs), including XopEM,
SoEM22, SUMPIP, and DS-SoEM, partially answering Shi et al.'s open question. On
the other hand, for block ciphers based on TPP-PRFs, we break the obstacle that
this attack rely on online query by constructing decoupled XOR-type function,
then propose an offline quantum attack on them. Compared to previous results,
our offline attack exhibits significantly reduced query complexity.
Specifically, we reduce the number of queries to the encryption oracle from
$\tilde O(2^{(n+t)/2})$ to $\tilde O(2^{t})$ with the same time complexity in
the quantum query model, and enable its implementation in the classical query
model, optimizing both the classical query complexity and time complexity from
$\tilde O(2^{(2n)/3})$ to $\tilde O(2^{(2n-t)/3})$.

</details>


### [19] [Transferable Equivariant Quantum Circuits for TSP: Generalization Bounds and Empirical Validation](https://arxiv.org/abs/2510.14533)
*Monit Sharma,Hoong Chuin Lau*

Main category: quant-ph

TL;DR: 该论文提出使用等变量子电路解决量子强化学习在组合优化中的泛化问题，通过利用旅行商问题的置换对称性实现从n城市训练实例到m城市问题的零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 解决量子强化学习在组合优化中的泛化挑战，特别是旅行商问题，因为在大规模实例上训练量子策略通常不可行，现有方法仅限于小规模问题。

Method: 采用等变量子电路，该电路尊重TSP图的置换对称性，通过对称感知的ansatz实现从n城市训练实例到更大m城市问题的零样本参数迁移。

Result: 实验表明，基于EQC的策略在小规模TSP上训练后，在更大实例上保持强性能零样本，并通过微调进一步改进，与经典观察到的尺度间正向迁移一致。

Conclusion: 将置换对称性嵌入量子模型可为组合任务产生可扩展的量子强化学习解决方案，突显了等变性在可迁移量子学习中的关键作用。

Abstract: In this work, we address the challenge of generalization in quantum
reinforcement learning (QRL) for combinatorial optimization, focusing on the
Traveling Salesman Problem (TSP). Training quantum policies on large TSP
instances is often infeasible, so existing QRL approaches are limited to
small-scale problems. To mitigate this, we employed Equivariant Quantum
Circuits (EQCs) that respect the permutation symmetry of the TSP graph.
  This symmetry-aware ansatz enabled zero-shot transfer of trained parameters
from $n-$city training instances to larger m-city problems. Building on recent
theory showing that equivariant architectures avoid barren plateaus and
generalize well, we derived novel generalization bounds for the transfer
setting. Our analysis introduces a term quantifying the structural
dissimilarity between $n-$ and $m-$node TSPs, yielding an upper bound on
performance loss under transfer. Empirically, we trained EQC-based policies on
small $n-$city TSPs and evaluated them on larger instances, finding that they
retained strong performance zero-shot and further improved with fine-tuning,
consistent with classical observations of positive transfer between scales.
These results demonstrate that embedding permutation symmetry into quantum
models yields scalable QRL solutions for combinatorial tasks, highlighting the
crucial role of equivariance in transferable quantum learning.

</details>


### [20] [Polaritons confined in dielectric structures](https://arxiv.org/abs/2510.14566)
*Amir Rahmani,Dogyun Ko,Maciej Dems,Andrzej Opala,Michał Matuszewski*

Main category: quant-ph

TL;DR: 提出了一种基于Bogoliubov变换和第三量子化技术的量子模型构建方法，用于在极化子本征模基中描述强耦合光-物质相互作用，并展示了如何通过精心设计的纳米结构增强相互作用强度和工程化非局域多体相互作用。


<details>
  <summary>Details</summary>
Motivation: 现有的Hopfield模型在处理强量子耦合时存在局限性，因为光与物质本征模的形状在相互作用下会发生显著改变。此外，理论模型参数通常通过拟合实验数据获得，缺乏直接从特定介电结构推导量子主方程的直截了当方法，这导致理论描述与物理实现之间存在不兼容性。

Method: 在保守情况下使用Bogoliubov变换，在耗散情况下使用第三量子化技术，在极化子本征模基中构建量子模型。该方法可用于增强相互作用强度和工程化非局域多体相互作用。

Result: 该方法能够增强相互作用强度，并在精心设计的纳米结构中实现非局域多体相互作用，从而产生发射光的强非经典关联。

Conclusion: 提出的方法为解决强耦合光-物质相互作用的理论描述与物理实现之间的不兼容问题提供了有效途径，并为设计具有强非经典光关联的纳米结构提供了理论基础。

Abstract: Light-matter interaction in the regime of strong quantum coupling is usually
treated within the framework of the Hopfield model. However, the picture of
coupling well-defined modes of light and matter is correct only as long as the
shapes of these eigenmodes are not substantially modified by the interaction.
Moreover, parameters of theoretical models are usually obtained by fitting to
experimental data. To date, there has been no straightforward method to
determine a quantum master equation corresponding to a system with specific
dielectric structure, which may lead to incompatibility of theoretical
descriptions and physical realizations. We present a recipe for obtaining a
quantum model in the polariton eigenmode basis based on Bogoliubov
transformation in the conservative case and third quantization technique in the
dissipative case. We show how this method can be used for boosting interaction
strength and engineering nonlocal many-body interactions in carefully designed
nanostructures, resulting in strongly nonclassical correlations of emitted
light.

</details>


### [21] [Quantum Reinforcement Learning: Recent Advances and Future Directions](https://arxiv.org/abs/2510.14595)
*Jawaher Kaldari,Shehbaz Tariq,Saif Al-Kuwari,Samuel Yen-Chi Chen,Symeon Chatzinotas,Hyundong Shin*

Main category: quant-ph

TL;DR: 这篇论文调查了量子强化学习(QRL)的最新进展，评估其在各种应用中的潜力，包括算法、架构、SDK支持以及跨领域应用。


<details>
  <summary>Details</summary>
Motivation: 量子强化学习作为量子机器学习中一个特别有前景但尚未充分探索的领域，需要系统性的调查来评估其在不同应用中的潜力。

Method: 采用综述研究方法，对量子强化学习的框架进行全面分析，包括其算法、架构、SDK支持以及在不同领域的应用。

Result: 研究发现量子强化学习虽然在量子机器学习中关注较少，但在量子领域和经典领域都具有独特优势和横向适用性。

Conclusion: 量子强化学习面临挑战但也带来机遇，有望推动量子启发式强化学习的创新，并在各种跨学科环境中促进其应用。

Abstract: As quantum machine learning continues to evolve, reinforcement learning
stands out as a particularly promising yet underexplored frontier. In this
survey, we investigate the recent advances in QRL to assess its potential in
various applications. While QRL has generally received less attention than
other quantum machine learning approaches, recent research reveals its distinct
advantages and transversal applicability in both quantum and classical domains.
We present a comprehensive analysis of the QRL framework, including its
algorithms, architectures, and supporting SDK, as well as its applications in
diverse fields. Additionally, we discuss the challenges and opportunities that
QRL can unfold, highlighting promising use cases that may drive innovation in
quantum-inspired reinforcement learning and catalyze its adoption in various
interdisciplinary contexts.

</details>


### [22] [Single-shot antidistinguishability of unitary operations](https://arxiv.org/abs/2510.14609)
*Satyaki Manna,Anandamay Das Bhowmik*

Main category: quant-ph

TL;DR: 本文研究了酉操作的单次反区分性，分析了单系统探针和纠缠探针两种场景。对于三个酉操作集合，证明了最大纠缠态作为探针的等效性。在量子比特情况下，最大纠缠探针总是足够；但在三维系统中，存在只能用非最大纠缠探针或单系统探针反区分的酉操作集合。


<details>
  <summary>Details</summary>
Motivation: 量子态的反区分性已被广泛研究，但量子通道的反区分性仍未被充分探索。本文旨在研究酉操作的反区分性，特别是比较不同探针类型的性能差异。

Method: 分析单系统探针和纠缠探针两种场景，研究三个酉操作集合的反区分性。在量子比特和三维系统中分别进行理论证明和构造反例。

Result: 对于三个量子比特酉操作，最大纠缠探针总是足够；但在三维系统中，存在只能用非最大纠缠探针反区分的酉操作集合。还证明了两个可反区分的三个量子比特酉操作集合的并集也是可反区分的。

Conclusion: 最大纠缠探针在低维系统中具有普适性，但在高维系统中存在局限性。研究结果为量子通道反区分性理论提供了新的见解，并展示了维度对反区分性的重要影响。

Abstract: The notion of antidistinguishability captures the possibility of ruling out
certain alternatives in a quantum experiment without identifying the actual
outcome. Although extensively studied for quantum states, the
antidistinguishability of quantum channels remains largely unexplored. In this
work, we investigate the single-shot antidistinguishability of unitary
operations. We analyse two scenarios: antidistinguishability with single-system
probes and with entangled probes. For sets of three unitaries, we first prove
that all maximally entangled states are equivalent in their performance as
probe. In the qubit case, we further establish that maximally entangled probes
are always sufficient: if a set of three qubit unitaries is antidistinguishable
with either a single-system or non-maximally entangled probe, then it is also
antidistinguishable with a maximally entangled one. However, in higher
dimension, this equivalence fails. In \textit{dimension 3}, there exists a set
of unitaries that are antidistinguishable with non-maximally entangled probe or
single-system probe but not with maximally entangled probe. We also establish
that union of two antidistinguishable sets of three qubit unitaries also forms
a set of antidistinguishable unitaries. Lastly, we provide methods to construct
antidistinguishable unitaries from non-antidistinguishable ones.

</details>


### [23] [Multiparameter quantum-enhanced adaptive metrology with squeezed light](https://arxiv.org/abs/2510.14739)
*Giorgio Minati,Enrico Urbani,Nicolò Spagnolo,Valeria Cimini,Fabio Sciarrino*

Main category: quant-ph

TL;DR: 提出了一种自适应多参数估计策略，用于相位估计，在不依赖压缩参数先验知识的情况下，在整个周期性区间[0,π)内实现了亚标准量子极限精度。


<details>
  <summary>Details</summary>
Motivation: 压缩光在量子增强相位估计中具有重要应用，但传统依赖预校准压缩水平的方法容易随时间退化，在实验条件波动时变得次优。需要开发对探针状态不稳定性具有鲁棒性的估计协议。

Method: 采用实时反馈机制联合估计光学相位和压缩水平，通过自适应多参数估计策略实现自校准，确保对实验漂移和校准误差的鲁棒性。

Result: 在完整周期性区间[0,π)内实现了亚标准量子极限精度，无需压缩参数的先验知识，建立了可靠的量子增强传感框架。

Conclusion: 这种自校准方案为实际应用场景和可扩展分布式传感器网络中使用压缩光开辟了新途径，提供了对实验条件波动的鲁棒性。

Abstract: Squeezed light enables quantum-enhanced phase estimation, with crucial
applications in both fundamental physics and emerging technologies. To fully
exploit the advantage provided by this approach, estimation protocols must
remain optimal across the entire parameter range and resilient to instabilities
in the probe state. In this context, strategies that rely on pre-calibrated
squeezing levels are vulnerable to degradation over time and become sub-optimal
when experimental conditions fluctuate. Here, we develop an adaptive
multiparameter estimation strategy for ab-initio phase estimation, achieving
sub-standard quantum limit precision in the full periodicity interval
$[0,\pi)$, without relying on prior knowledge of the squeezing parameter. Our
approach employs real-time feedback to jointly estimate both the optical phase
and the squeezing level, ensuring robustness against experimental drifts and
calibration errors. This self-calibrating scheme establishes a reliable
quantum-enhanced sensing framework, opening new routes for practical scenarios
and scalable distributed sensor networks using squeezed light.

</details>


### [24] [Unsupervised Learning to Recognize Quantum Phases of Matter](https://arxiv.org/abs/2510.14742)
*Mehran Khosrojerdi,Alessandro Cuccoli,Paola Verrucchi,Leonardo Banchi*

Main category: quant-ph

TL;DR: 该论文提出使用无监督学习方法直接从量子态中识别量子相图，无需先验标记，通过量子态保真度作为相似性度量，结合谱聚类和优化方法准确再现相图。


<details>
  <summary>Details</summary>
Motivation: 传统确定量子多体系统相图的方法需要先验知识标记相，而本文旨在开发无需标记的无监督学习方法，直接从量子态中自主识别相变。

Method: 采用无监督学习中的谱聚类算法，以量子态保真度作为相似性度量，结合"轮廓"和"肘部"方法确定最优相数，在自旋-1/2链模型上进行测试。

Result: 该方法能够准确再现测试系统的相图，验证了无监督学习在量子相识别中的有效性。

Conclusion: 无监督学习可以自主识别量子物质的新相，为量子相图确定提供了一种无需先验标记的有效方法。

Abstract: Drawing the quantum phase diagram of a many-body system in the parameter
space of its Hamiltonian can be seen as a learning problem, which implies
labelling the corresponding ground states according to some classification
criterium that defines the phases. In this work we adopt unsupervised learning,
where the algorithm has no access to any priorly labeled states, as a tool for
determining quantum phase diagrams of many-body systems. The algorithm directly
works with quantum states: given the ground-state configurations for different
values of the Hamiltonian parameters, the process uncovers the most significant
way of grouping them based on a similarity criterion that refers to the
fidelity between quantum states, that can be easily estimated, even
experimentally. We benchmark our method with two specific spin-$\frac{1}{2}$
chains, with states determined via tensor network techniques. We find that
unsupervised learning algorithms based on spectral clustering, combined with
``silhouette'' and ``elbow'' methods for determining the optimal number of
phases, can accurately reproduce the phase diagrams. Our results show how
unsupervised learning can autonomously recognize and possibly unveil novel
phases of quantum matter.

</details>


### [25] [Spectral subspace extraction via incoherent quantum phase estimation](https://arxiv.org/abs/2510.14744)
*Stefano Scali,Josh Kirsopp,Antonio Márquez Romero,Michał Krompiec*

Main category: quant-ph

TL;DR: 提出了一种基于系综的量子相位估计算法（DOS-QPE），用于估计哈密顿量的态密度，克服了标准QPE需要精确制备输入态的限制。


<details>
  <summary>Details</summary>
Motivation: 标准量子相位估计需要精确制备相干输入态且只能针对单个本征态，限制了其在量子多体系统中的应用。

Method: 采用系综形式的QPE，结合对称性适应的输入系综和先进的谱重构技术，将谱重构问题转化为压缩感知的二次规划问题。

Result: 在费米子模型和核哈密顿量上验证了性能，能够自然获取热力学性质、对称性分辨的谱函数等量子多体系统特征。

Conclusion: DOS-QPE在光谱学、电子结构和核理论等领域的早期容错量子模拟中具有重要潜力。

Abstract: Quantum phase estimation (QPE) is a cornerstone algorithm for extracting
Hamiltonian eigenvalues, but its standard form targets individual eigenstates
and requires carefully prepared coherent inputs. To overcome these limitations,
we adopt an ensemble-based formulation of QPE that estimates the density of
states (DOS) of the Hamiltonian generator of the evolution. This approach,
which we refer to as DOS-QPE, builds on a prior formulation introduced by one
of the authors. In this work, we present DOS-QPE as a circuit primitive,
extending it with symmetry-adapted input ensembles and advanced spectrum
reconstruction techniques. This variant of QPE enables natural access to
thermodynamic properties, symmetry-resolved spectral functions, and features
relevant to quantum many-body systems. We demonstrate its performance on
fermionic models and nuclear Hamiltonians by casting the spectrum
reconstruction problem as a quadratic program solved via compressed sensing.
These use cases highlight the potential of DOS-QPE for early fault-tolerant
quantum simulations in spectroscopy, electronic structure, and nuclear theory.

</details>


### [26] [Quantum remeshing and efficient encoding for fracture mechanics](https://arxiv.org/abs/2510.14746)
*Ulysse Remond,Pierre-Emmanuel Emeriau,Liam Lysaght,Jean Ruel,Joseph Mikael,Kyryl Kazymyrenko*

Main category: quant-ph

TL;DR: 提出了一种用于结构力学问题的变分量子算法，特别针对裂纹开口模拟，通过参数化量子电路存储节点位移并高效提取关键观测值。


<details>
  <summary>Details</summary>
Motivation: 传统裂纹开口模拟需要大量计算资源，量子算法提供了一种替代解决方案，能够更高效地处理这类结构力学问题。

Method: 使用参数化量子电路存储节点位移作为量子振幅，通过最小化有限元法获得的弹性能量来优化节点位移，并采用基于重网格技术的热启动策略避免优化过程中的平台效应。

Result: 算法仅需多对数次测量即可计算能量，在Quandela的光子量子处理器Ascella上进行了实验验证，数值模拟显示该方法在日益复杂的量子系统中具有良好的可扩展性。

Conclusion: 该变分量子算法为结构力学问题提供了一种高效的量子解决方案，特别是在裂纹模拟方面展现出良好的可扩展性和实用性。

Abstract: We present a variational quantum algorithm for structural mechanical
problems, specifically addressing crack opening simulations that traditionally
require extensive computational resources. Our approach provides an alternative
solution for a relevant 2D case by implementing a parametrized quantum circuit
that stores nodal displacements as quantum amplitudes and efficiently extracts
critical observables. The algorithm achieves optimal nodal displacements by
minimizing the elastic energy obtained from finite element method. The energy
is computed with only a polylogarithmic number of measurements. Extracting
relevant scalar observables such as the stress intensity factor is then done
efficiently on the converged solution. To validate the scalability of our
approach, we develop a warm start strategy based on a remeshing technique that
uses coarse solutions to circumvent barren plateaus in the optimization
landscape of the more refined problems. Our method has been experimentally
validated on Quandela's photonic quantum processor Ascella and comprehensive
numerical simulations demonstrate its scalability across increasingly complex
quantum systems.

</details>


### [27] [ParaToric 1.0-beta: Continuous-time quantum Monte Carlo for the toric code in a parallel field](https://arxiv.org/abs/2510.14781)
*Simon M. Linsel,Lode Pollet*

Main category: quant-ph

TL;DR: ParaToric是一个C++软件包，用于在有限温度下模拟平行场中的toric code，支持多种晶格结构和边界条件，提供C/C++和Python绑定。


<details>
  <summary>Details</summary>
Motivation: 开发一个可扩展的软件包来模拟toric code在平行场中的行为，支持多种晶格结构和自定义可观测量，为其他方法提供训练/基准数据。

Method: 实现并扩展了Wu、Deng和Prokof'ev的连续时间量子蒙特卡洛算法，支持方形、三角形、蜂窝和立方晶格，以及开放和周期性边界条件。

Result: 开发了ParaToric软件包，支持任意晶格几何结构和自定义可观测量，提供快照提取功能，可集成到其他软件项目中。

Conclusion: ParaToric是一个通用且可扩展的软件包，适用于晶格规范理论、冷原子模拟、量子自旋液体、人工智能和量子纠错等多个领域。

Abstract: We introduce ParaToric, a C++ package for simulating the toric code in a
parallel field (i.e., $X$- and $Z$-fields) at finite temperature. We implement
and extend the continuous-time quantum Monte Carlo algorithm of Wu, Deng, and
Prokof'ev on the square, triangular, honeycomb, and cubic lattices with open
and periodic boundaries, respectively. The package is expandable to arbitrary
lattice geometries and custom observables diagonal in either the $X$- or
$Z$-basis. ParaToric also supports snapshot extraction in both bases, making it
ideal for generating training/benchmarking data for other methods, such as
lattice gauge theories, cold atom or other quantum simulators, quantum spin
liquids, artificial intelligence, and quantum error correction. The software
provides bindings to C/C++ and Python, and is thus almost universally
integrable into other software projects.

</details>


### [28] [Efficient adaptive control strategy for multi-parameter quantum metrology in two-dimensional systems](https://arxiv.org/abs/2510.14811)
*Qifei Wei,Shengshi Pang*

Main category: quant-ph

TL;DR: 提出了一种用于多参数量子计量学的自适应控制策略，通过系统扩展方案消除最优测量、初始状态和控制哈密顿量之间的权衡，实现了接近最优性能的参数估计。


<details>
  <summary>Details</summary>
Motivation: 量子计量学利用纠缠和压缩等量子资源来超越经典极限的参数估计精度。虽然最优量子控制策略可以帮助达到甚至超越海森堡极限，但其实施通常需要已知待估参数，因此需要带有反馈的自适应控制方法。目前自适应控制方法主要应用于单参数量子计量学，在多参数量子计量学中研究较少。

Method: 通过系统扩展方案消除最优测量、初始状态和控制哈密顿量之间的权衡，使用重参数化技术优化自适应迭代中的演化时间，建立递归关系来表征迭代过程中的精度改进。

Result: 该策略仅需几次迭代即可达到最优性能（相差一个常数阶因子），并对控制参数误差具有强鲁棒性。分析表明该策略对具有任意参数依赖性的哈密顿量都有效。

Conclusion: 这项工作为现实场景中具有自适应哈密顿量控制的多参数量子计量学提供了一种实用方法。

Abstract: Quantum metrology leverages quantum resources such as entanglement and
squeezing to enhance parameter estimation precision beyond classical limits.
While optimal quantum control strategies can assist to reach or even surpass
the Heisenberg limit, their practical implementation often requires the
knowledge of the parameters to be estimated, necessitating adaptive control
methods with feedback. Such adaptive control methods have been considered in
single-parameter quantum metrology, but not much in multi-parameter quantum
metrology so far. In this work, we bridge this gap by proposing an efficient
adaptive control strategy for multi-parameter quantum metrology in
two-dimensional systems. By eliminating the trade-offs among optimal
measurements, initial states, and control Hamiltonians through a system
extension scheme, we derive an explicit relation between the estimator variance
and evolution time. Through a reparameterization technique, the optimization of
evolution times in adaptive iterations are obtained, and a recursive relation
is established to characterize the precision improvement across the iterations.
The proposed strategy achieves the optimal performance up to an overall factor
of constant order with only a few iterations and demonstrates strong robustness
against deviations in the errors of control parameters at individual
iterations. Further analysis shows the effectiveness of this strategy for
Hamiltonians with arbitrary parameter dependence. This work provides a
practical approach for multi-parameter quantum metrology with adaptive
Hamiltonian control in realistic scenarios.

</details>


### [29] [Signatures of Topological Symmetries on a Noisy Quantum Simulator](https://arxiv.org/abs/2510.14817)
*Christopher Lamb,Robert M. Konik,Hubert Saleur,Ananda Roy*

Main category: quant-ph

TL;DR: 在IBM量子模拟器上实现了二维Ising共形场论的拓扑对称性特征，通过混合量子-经典算法创建本征态并测量相关函数，验证了拓扑对称性。


<details>
  <summary>Details</summary>
Motivation: 拓扑对称性在量子场论中具有基础重要性，但在物理系统中受控实现这类模型很罕见。量子模拟器为研究这些模型提供了新途径。

Method: 使用基于量子近似优化算法变体和量子自然梯度优化方法的混合量子-经典算法，在IBM Kingston模拟器上实现杂质哈密顿量的本征态和拓扑对称性的环路算子。

Result: 通过测量不同量子比特算子的相关函数，捕获了拓扑对称性的特征，量子设备结果与经典计算合理一致。

Conclusion: 当前工作证明了有噪声量子模拟器作为研究低维量子场论平台的可行性，能够直接观测传统凝聚态实验中难以探测的观测量。

Abstract: Topological symmetries, invertible and otherwise, play a fundamental role in
the investigation of quantum field theories. Despite their ubiquitous
importance across a multitude of disciplines ranging from string theory to
condensed matter physics, controlled realizations of models exhibiting these
symmetries in physical systems are rare. Quantum simulators based on engineered
solid-state devices provide a novel alternative to conventional condensed
matter systems for realizing these models.
  In this work, eigenstates of impurity Hamiltonians and loop operators
associated with the topological symmetries for the Ising conformal field theory
in two space-time dimensions are realized on IBM's Kingston simulator. The
relevant states are created on the quantum device using a hybrid
quantum-classical algorithm. The latter is based on a variation of the quantum
approximate optimization algorithm ansatz combined with the quantum natural
gradient optimization method. Signatures of the topological symmetry are
captured by measuring correlation functions of different qubit operators with
results obtained from the quantum device in reasonable agreement with those
obtained from classical computations. The current work demonstrates the
viability of noisy quantum simulators as platforms for investigating
low-dimensional quantum field theories with direct access to observables that
are often difficult to probe in conventional condensed matter experiments.

</details>


### [30] [Ruelle-Pollicott Decay of Out-of-Time-Order Correlators in Many-Body Systems](https://arxiv.org/abs/2510.14886)
*Jerónimo Duarte,Ignacio García-Mata,Diego A. Wisniacki*

Main category: quant-ph

TL;DR: 该论文发现孤立量子系统中OTOC的长时间指数衰减速率等于其弱开放扩展的Liouvillian谱间隙的两倍，为理解封闭多体量子系统的弛豫和不可逆性提供了统一框架。


<details>
  <summary>Details</summary>
Motivation: 研究多体量子系统中信息扰动的弛豫机制，特别是在缺乏半经典极限的情况下，探索OTOC衰减与Liouvillian谱的关系。

Method: 研究受踢伊辛自旋链模型，分析孤立系统中OTOC的长时间衰减行为，并与弱开放扩展系统的Liouvillian谱间隙进行比较。

Result: 发现孤立系统中OTOC的指数衰减速率等于Liouvillian间隙的两倍，这种对应关系在可积与混沌的交叉区域仍然成立。

Conclusion: Liouvillian谱为理解封闭多体量子系统的弛豫和不可逆性提供了统一框架，即使在可积与混沌的过渡区域也保持有效。

Abstract: The out-of-time-order correlator (OTOC) quantifies information scrambling in
quantum systems and serves as a key diagnostic of quantum chaos. In one-body
systems with a classical counterpart, the relaxation of the OTOC is governed by
Ruelle-Pollicott resonances. For many-body systems lacking a semiclassical
limit, recent studies have identified an analogous role played by the
Liouvillian spectrum of weakly open extensions of the dynamics, where the
slowest decay rate -- the Liouvillian gap -- encodes relaxation. Here we study
the kicked Ising spin chain and show that the long-time exponential decay of
the OTOC in the isolated system occurs at a rate equal to twice this intrinsic
gap. This correspondence persists even in crossover regimes between
integrability and chaos, demonstrating that the Liouvillian spectrum provides a
unified framework for understanding relaxation and irreversibility in closed
many-body quantum systems.

</details>


### [31] [Fast and fault-tolerant logical measurements: Auxiliary hypergraphs and transversal surgery](https://arxiv.org/abs/2510.14895)
*Alexander Cowtan,Zhiyang He,Dominic J. Williamson,Theodore J. Yoder*

Main category: quant-ph

TL;DR: 本文研究了量子码手术的时间开销优化，提出了实现恒定时间开销的通用条件，并介绍了块读取方案，同时探讨了量子局部可测试码的中间时间开销手术操作。


<details>
  <summary>Details</summary>
Motivation: 量子码手术是量子低密度奇偶校验码上进行容错计算的有前景技术，虽然空间开销已显著降低，但通用手术操作仍需要O(d)轮重复症状提取才能实现容错，因此需要减少时间开销。

Method: 提出了确保容错手术操作能以恒定时间开销执行的一般条件，使用超图描述的辅助复合体；引入了块读取方案，在多个码块间执行横向手术；研究了量子局部可测试码的中间时间开销手术操作。

Result: 建立了同态测量与超图手术之间的电路等价性，推导了通用逻辑测量方案的时间开销界限，证明了减少码手术时间成本不依赖于量子存储器是否为单次测量。

Conclusion: 码与其测量辅助系统之间的连接性是决定可达到测量时间开销的主要因素，而非量子存储器的单次测量特性。

Abstract: Quantum code surgery is a promising technique to perform fault-tolerant
computation on quantum low-density parity-check codes. Recent developments have
significantly reduced the space overhead of surgery. However, generic surgery
operations still require $O(d)$ rounds of repeated syndrome extraction to be
made fault-tolerant. In this work, we focus on reducing the time overhead of
surgery. We first present a general set of conditions that ensure
fault-tolerant surgery operations can be performed with constant time overhead.
This fast surgery necessarily makes use of an auxiliary complex described by a
hypergraph rather than a graph. We then introduce a concrete scheme called
block reading, which performs transversal surgery across multiple code blocks.
We further investigate surgery operations with intermediate time overhead,
between $O(1)$ and $O(d)$, which apply to quantum locally testable codes.
Finally, we establish a circuit equivalence between homomorphic measurement and
hypergraph surgery and derive bounds on the time overhead of generic logical
measurement schemes. Overall, our results demonstrate that reducing the time
cost of code surgery is not reliant on the quantum memory being single-shot.
Instead it is chiefly the connectivity between a code and its measurement
ancilla system that determines the achievable measurement time overhead.

</details>


### [32] [Forecasting Quantum Observables: A Compressed Sensing Approach with Performance Guarantees](https://arxiv.org/abs/2510.14897)
*Víctor Valls,Albert Akhriev,Olatz Sanz Larrarte,Javier Oliva del Moral,Štěpán Šmíd,Josu Etxezarreta Martinez,Sergiy Zhuk,Dmytro Mishagli*

Main category: quant-ph

TL;DR: 提出原子范数最小化框架来验证量子动力学模型的准确性，确保从噪声测量中恢复的模型反映真实系统动力学，在存在实际测量噪声时仍保持稳健。


<details>
  <summary>Details</summary>
Motivation: 近量子设备的噪声限制了可靠测量的时间尺度，现有数据驱动外推方法无法保证恢复的模型反映真实动力学。

Method: 引入原子范数最小化作为框架，在系统由少量分离良好的玻尔频率控制时，验证任何算法学习到的模型准确性。

Result: 在8-20量子比特的自旋链哈密顿量数值实验中，与精确对角化比较，认证模型在98%情况下预测误差低于0.1，在89-97%情况下低于0.05。即使存在实际测量噪声，0.1误差阈值的成功率仅降至88-95%。

Conclusion: 原子范数最小化提供了验证量子动力学模型准确性的可靠框架，在噪声环境下仍能保持稳健性能。

Abstract: Noise in near-term quantum devices limits the timescales over which
measurements can be reliably obtained. Existing data-driven extrapolation
methods extend the dynamics of quantum observables from measurements, but they
cannot guarantee that the recovered model reflects the true dynamics. In this
paper, we introduce atomic norm minimization as a framework to certify that a
model learned by any algorithm accurately captures the underlying dynamics of
quantum observables. This certification is valid when the system is governed by
a small number of well-separated Bohr frequencies. We validate the framework
across multiple algorithms on numerical experiments with spin-chain
Hamiltonians involving 8-20 qubits. Comparing with exact diagonalization,
certification yields an average forecasting error below 0.1 (within the
observable's range of $[-1, 1]$) in 98% of cases and below 0.05 in 89-97% of
cases, depending on the forecasting algorithm. Even in the presence of
realistic shot noise, certified models remain robust, with success rates
decreasing only to 88-95% for the 0.1 error threshold.

</details>


### [33] [Continuous-time quantum walk on a random graph using quantum circuits](https://arxiv.org/abs/2510.14905)
*Sabyasachi Chakraborty,Rohit Sarma Sarkar,Sonjoy Majumder,Rohit Kishan Ray*

Main category: quant-ph

TL;DR: 提出了一个可扩展的量子电路形式来模拟随机图结构上的连续时间量子行走，重点关注Erdős-Rényi随机图，并研究量子行走的局域化行为。


<details>
  <summary>Details</summary>
Motivation: 量子行走特别是连续时间量子行走已成为建模量子输运、模拟复杂动力学和开发具有潜在加速优势的量子算法的重要工具。

Method: 使用量子电路形式模拟CTQW，采用Trotterization方案高效实现图拉普拉斯算符的时间演化，重点关注Erdős-Rényi随机图。

Result: 量子电路实现确保了电路设计可以适用于任何图结构，为高效实现基于CTQW的量子模拟奠定了基础。

Conclusion: 该工作为在随机图上高效模拟连续时间量子行走提供了一个可扩展的量子电路框架，并研究了量子行走的局域化动力学特性。

Abstract: Quantum walks, particularly continuous-time quantum walks (CTQW), have
emerged as powerful tools for modeling quantum transport, simulating complex
dynamics, and developing quantum algorithms with potential speedups over
classical counterparts. In this work, we present a scalable quantum circuit
formalism to simulate CTQW on random graph structures, especially focusing on
Erd\H{o}s-R\'enyi random graphs. Our quantum circuit construction efficiently
implements the time evolution of the graph Laplacian, using the Trotterization
scheme. We investigate key dynamical properties, \emph{i.e.,} the localization
behavior of the CTQW. Our quantum circuit implementation over random graph
ensures that the circuit design can work on any graph structure, thereby laying
the foundation for realizing CTQW-based quantum simulations efficiently.

</details>


### [34] [Decoherence-Aware Entangling and Swapping Strategy Optimization for Entanglement Routing in Quantum Networks](https://arxiv.org/abs/2510.14912)
*Shao-Min Huang,Cheng-Yang Cheng,Ming-Huang Chien,Jian-Jhih Kuo,Chih-Yu Wang*

Main category: quant-ph

TL;DR: 提出短时隙协议来优化量子纠缠和交换过程，解决纠缠对随时间退相干和交换过程导致保真度损失的问题，设计了TETRIS优化算法显著提升保真度。


<details>
  <summary>Details</summary>
Motivation: 量子纠缠对会因环境干扰随时间退相干导致保真度降低，同时交换过程也会造成额外的保真度损失，因此需要在合适时间生成纠缠对并优化交换策略。

Method: 提出短时隙协议，每个时隙只能容纳一个过程，比传统长时隙协议更灵活安排纠缠和交换过程；设计了两种基于不同优化技术的TETRIS算法来解决优化问题。

Result: 仿真结果显示，新算法比现有方法在一般情况下提升60-78%的保真度，在低纠缠概率下也能提升20-75%的保真度。

Conclusion: 短时隙协议和TETRIS算法能有效优化量子纠缠和交换过程，显著提高量子通信的保真度性能。

Abstract: Quantum teleportation enables high-security communications through end-to-end
quantum entangled pairs. End-to-end entangled pairs are created by using
swapping processes to consume short entangled pairs and generate long pairs.
However, due to environmental interference, entangled pairs decohere over time,
resulting in low fidelity. Thus, generating entangled pairs at the right time
is crucial. Moreover, the swapping process also causes additional fidelity
loss. To this end, this paper presents a short time slot protocol, where a time
slot can only accommodate a process. It has a more flexible arrangement of
entangling and swapping processes than the traditional long time slot protocol.
It raises a new optimization problem TETRIS for finding strategies of
entangling and swapping for each request to maximize the fidelity sum of all
accepted requests. To solve the TETRIS, we design two novel algorithms with
different optimization techniques. Finally, the simulation results manifest
that our algorithms can outperform the existing methods by up to 60 ~ 78% in
general, and by 20 ~ 75% even under low entangling probabilities.

</details>


### [35] [Current fluctuations in nonequilibrium open quantum systems beyond weak coupling: a reaction coordinate approach](https://arxiv.org/abs/2510.14926)
*Khalak Mahadeviya,Saulo V. Moreira,Sheikh Parvez Mandal,Mahasweta Pandit,Javier Prior,Mark T. Mitchison*

Main category: quant-ph

TL;DR: 研究强耦合开放量子系统中的电流涨落，发现与弱耦合不同，平均电流和涨落都表现出与系统-环境相互作用强度的非单调依赖关系，并识别出电流噪声低于经典热力学不确定界限的机制。


<details>
  <summary>Details</summary>
Motivation: 探索超越弱耦合和马尔可夫近似的开放量子系统中的电流涨落特性，特别是在强耦合和非马尔可夫机制下的行为。

Method: 结合全计数统计和反应坐标映射，开发了一个计算强耦合机制下稳态电流涨落及其时间关联的框架。

Result: 发现电流噪声可以被抑制到经典热力学不确定界限以下，这与量子跳跃轨迹中的增强反相关性以及系统更快的弛豫相吻合，这些特征与反应坐标模式的非经典性质（如非高斯性和量子相干性）有关。

Conclusion: 结果为在超越标准弱耦合范式的量子器件中控制电流涨落提供了新的见解和设计原则。

Abstract: We investigate current fluctuations in open quantum systems beyond the
weak-coupling and Markovian regimes, focusing on a coherently driven qubit
strongly coupled to a structured bosonic environment. By combining full
counting statistics with the reaction coordinate mapping, we develop a
framework that enables the calculation of steady-state current fluctuations and
their temporal correlations in the strong-coupling regime. Our analysis reveals
that, unlike in weak coupling, both the average current and its fluctuations
exhibit nonmonotonic dependence on the system-environment interaction strength.
Notably, we identify a regime where current noise is suppressed below the
classical thermodynamic uncertainty bound, coinciding with enhanced
anticorrelations in quantum jump trajectories and faster system relaxation. We
further show that these features are linked to nonclassical properties of the
reaction coordinate mode, such as non-Gaussianity and quantum coherence. Our
results provide new insights and design principles for controlling current
fluctuations in quantum devices operating beyond the standard weak-coupling
paradigm.

</details>


### [36] [Orders matter: tight bounds on the precision of sequential quantum estimation for multiparameter models](https://arxiv.org/abs/2510.14963)
*Gabriele Fazio,Jiayu He,Matteo G. A. Paris*

Main category: quant-ph

TL;DR: 本文提出并分析了一种多参数量子计量中的逐步估计策略，推导了该策略的精确度界限，并与联合测量策略进行了比较，证明在某些情况下逐步策略能提供更好的测量精度。


<details>
  <summary>Details</summary>
Motivation: 在多参数量子计量中，Holevo Cramér-Rao界限决定了联合估计的最终精度。本文旨在探索一种替代方法——逐步估计策略，以克服传统方法的局限性。

Method: 采用逐步估计策略，将参数按顺序估计，并为每个步骤优化分配总资源的一部分。推导了逐步可分离界限的封闭解析表达式，并分析了测量顺序的影响。

Result: 推导出了紧致且可达的精确度界限，通过分析SU(2)幺正编码模型，证明逐步策略在非最优探针或高松弛度模型中能超越联合测量。

Conclusion: 逐步估计策略是联合和集体测量的有力替代方案，在资源受限或不完美的实验设置中能提供真正的计量优势。

Abstract: In multiparameter quantum metrology, the ultimate precision of joint
estimation is dictated by the Holevo Cram\'er-Rao bound. In this paper, we
discuss and analyze in detail an alternative approach: the stepwise estimation
strategy. In this approach, parameters are estimated sequentially, using an
optimized fraction of the total available resources allocated to each step. We
derive a tight and achievable precision bound for this protocol, the stepwise
separable bound, and provide its closed-form analytical expression, revealing a
crucial dependence on the chosen measurement ordering. We provide a rigorous
comparison with the joint measurement strategy, deriving analytical conditions
that determine when the stepwise approach offers superior precision. Through
the analysis of several paradigmatic SU(2) unitary encoding models, we
demonstrate that the stepwise strategy can indeed outperform joint
measurements, particularly in scenarios characterized by non-optimal probes or
models with a high degree of sloppiness. Our findings establish stepwise
estimation as a powerful alternative to joint and collective measurements,
proving that sequential protocols can provide a genuine metrological advantage,
especially in resource-constrained or imperfect experimental settings.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [37] [Radial kinks in the boson stars](https://arxiv.org/abs/2510.13923)
*Tian-Chi Ma,Xiang-Yu Wang,Hai-Qing Zhang*

Main category: gr-qc

TL;DR: 研究径向扭结在玻色子星背景中的时间演化，发现紧凑度越高，扭结向原点运动越慢，高度紧凑时扭结碰撞后可能产生新扭结而非立即消散。


<details>
  <summary>Details</summary>
Motivation: 探索径向扭结作为探测致密天体（包括黑洞）内部结构的手段。

Method: 在两种玻色子星（质量型和孤子型）背景下，研究不同紧凑度下径向扭结的动力学行为。

Result: 紧凑度阻碍扭结向原点运动，高度紧凑时扭结碰撞后可能产生新扭结。

Conclusion: 径向扭结有潜力成为探测致密天体内部结构的工具。

Abstract: In this work, we study the time evolution of radial kinks in the background
of boson stars. In particular, we consider two types of boson stars: the
massive boson star and the solitonic boson star. For each boson star, we study
the dynamics of the kinks with four different compactnesses. We observe that
the greater the compactness is, the slower the kinks move towards the origin of
the boson stars, indicating that the compactness will hinder the kinks to
collide with the origin. Additionally, it is found that when the boson star is
highly compact, a new kink may turn out after the kink colliding with the
origin, instead of immediately dissipating into the background. We then propose
that the radial kinks may potentially serve as a means to probe the internal
structures of dense astrophysical objects, even the interior structure of black
holes.

</details>


### [38] [Revisiting Wormhole Solutions in Unimodular Gravity: Energy Conditions and Exotic Matter Requirements](https://arxiv.org/abs/2510.13929)
*Mauricio Cataldo,Norman Cruz*

Main category: gr-qc

TL;DR: 本文反驳了Agrawal等人关于单模引力中无需奇异物质即可维持虫洞的结论，证明在单模引力框架下仍需要奇异物质来维持虫洞解。


<details>
  <summary>Details</summary>
Motivation: 质疑Agrawal等人关于单模引力中无潮汐力虫洞可用普通物质维持的结论，验证单模引力中虫洞解的物质要求。

Method: 在单模引力框架下重新分析无潮汐力虫洞的物理性质，检验物质能量条件。

Result: 证明在单模引力中维持虫洞解仍然需要奇异物质，这与广义相对论中的理论约束一致。

Conclusion: 单模引力并不能绕过广义相对论中关于虫洞需要奇异物质的限制，Agrawal等人的结论存在错误。

Abstract: The paper entitled Unimodular Gravity Traversable Wormholes by Agrawal et al.
examined the properties of barotropic wormholes without tidal forces within the
framework of Unimodular Gravity. Our analysis demonstrates that their
conclusion regarding the possibility of sustaining such wormhole configurations
with ordinary matter is not entirely accurate. We establish that exotic matter
remains necessary for these wormhole solutions in Unimodular Gravity, in
accordance with the long-established theoretical constraints already identified
in General Relativity.

</details>


### [39] [On the Geometric Meaning of General Relativity and the Foundations of Newtonian Cosmology](https://arxiv.org/abs/2510.13934)
*Jaume de Haro,Emilio Elizalde*

Main category: gr-qc

TL;DR: 重新审视广义相对论的几何基础，重点关注其规范不变性，将曲率解释为物质与相互作用场的动态关系，并构建牛顿框架来理解宇宙学。


<details>
  <summary>Details</summary>
Motivation: 超越将时空视为可变形'织物'的常见图像，强调爱因斯坦和外尔的观点：曲率是物质与场之间动态相互作用的表现，这一观点在文献中有时被忽视。

Method: 基于几何工具重新构建牛顿框架，将经典直觉与现代几何洞察相结合，从实体转向关系视角来理解引力动力学。

Result: 成功构建了能够捕捉宇宙学基本方面的牛顿框架，展示了经典直觉如何与现代几何观点共存，为重新解释引力动力学和宇宙大尺度结构提供了新视角。

Conclusion: 这种关系导向的方法与量子层面的其他最新研究具有显著相似性，为理解时空本质提供了新的放大镜，将焦点从实体转向关系。

Abstract: The geometric foundations of General Relativity are revisited, with
particular attention to its gauge invariance, as a key to understanding the
true nature of spacetime. Beyond the common image of spacetime as a deformable
'fabric' filling the Universe, curvature is interpreted as the dynamic
interplay between matter and interacting fields; a view already emphasized by
Einstein and Weyl, but sometimes overlooked in the literature. Building on
these tools, a Newtonian framework is reconstructed that captures essential
aspects of cosmology, showing how classical intuition can coexist with modern
geometric insights. This perspective shifts the focus from substance to
relationships, offering a fresh magnifying glass through which to reinterpret
gravitational dynamics and the large-scale structure of the Universe. The
similarities of this approach with other recent, more ambitious ones carried
out at the quantum level are quite remarkable.

</details>


### [40] [Quantum dynamics and thermodynamics of a Minkowski-Minkowski wormhole](https://arxiv.org/abs/2510.13944)
*Johanna Borissova,João Magueijo*

Main category: gr-qc

TL;DR: 本文通过路径积分方法研究了连接两个闵可夫斯基时空的洛伦兹虫洞的量子化，分析了虫洞喉部半径的动力学、拓扑变化转变的抑制机制，以及虫洞时空的引力热力学性质。


<details>
  <summary>Details</summary>
Motivation: 研究洛伦兹虫洞的量子行为和热力学性质，特别是拓扑变化转变的量子抑制机制和虫洞时空的热力学第一定律。

Method: 使用路径积分量子化方法，通过以色列连接条件推导非局域有效作用量，采用鞍点近似分析传播子，并通过Wick旋转研究引力热力学。

Result: 发现拓扑变化转变被雅可比行列式抑制，建立了虫洞温度与解周期的关系，推导了引力熵与壳层能量密度和状态方程参数的关系，并得到了热力学第一定律。

Conclusion: 虫洞的拓扑变化在量子层面被抑制，虫洞时空具有明确的热力学性质，其热力学第一定律可由壳层有效质量与表面压力的微分关系表示。

Abstract: We consider the path-integral quantization of a minisuperspace cut-and-paste
Lorentzian wormhole connecting two Minkowski spacetimes. The dynamics of the
throat radius as a function of proper time is governed by a non-local effective
action derived by an application of the Israel junction condition formalism.
Within a saddle-point approximation of the propagator describing the evolution
from an initial to a final throat radius, we show that topology-changing
transitions are suppressed by the Jacobi determinant. In addition, we analyze
the gravitational thermodynamics of the wormhole spacetime by a Wick rotation
of the Israel-Lanczos equations in the presence of a thin-shell source. The
resulting Euclideanized field equations are assumed to originate from a
Euclidean effective gravity-matter action, which enters the path-integral
representation of the gravitational canonical partition function. Therefrom we
associate a temperature given by the inverse period of solutions, as well as a
gravitational entropy as functions of the surface energy density and equation
of state parameter of the shell. Both quantities are sourced entirely by the
discontinuity of the extrinsic curvature across the junction. We show how this
result can be applied to deduce a thermodynamic first law as the differential
version of the conservation equation relating the effective mass of the shell
to its surface pressure.

</details>


### [41] [The quasinormal mode content of binary black hole ringdown](https://arxiv.org/abs/2510.13954)
*Richard Dyer,Christopher J. Moore*

Main category: gr-qc

TL;DR: 提出了一个完全贝叶斯的数据驱动框架，用于在高精度CCE引力波信号中识别准正则模式，包括泛音、逆行模式和非线性模式。


<details>
  <summary>Details</summary>
Motivation: 需要系统性地识别引力波信号中的各种准正则模式，为理论和观测研究提供参考。

Method: 采用完全贝叶斯的数据驱动框架，应用于公开的CCE引力波信号目录，识别各种准正则模式。

Result: 成功识别了泛音、逆行模式和非线性模式（高达三阶），并系统地记录了不同起始时间下的模式内容。未发现预期的晚期幂律尾部。

Conclusion: 该框架为引力波信号的准正则模式分析提供了系统性的参考，支持理论和观测研究。

Abstract: We present a fully Bayesian, data-driven framework for identifying
quasinormal modes in high-accuracy Cauchy-Characteristic Evolution (CCE)
gravitational waveforms. Applying this to a public catalog, we identify QNM
overtones, retrograde modes, and nonlinear modes up to cubic order in the
ringdown. The ringdown mode content is tabulated across a wide range of start
times for all available simulations, providing a systematic reference for
theoretical and observational studies. We also search for late-time power-law
tails, which are, as expected, absent from the CCE waveforms.

</details>


### [42] [Resonant Loop Interferometers for High-Frequency Gravitational Waves](https://arxiv.org/abs/2510.13957)
*Jan Heisig*

Main category: gr-qc

TL;DR: 提出基于闭合光学环路的干涉仪新架构，通过引力波引起的相位位移在多次穿越中相干累积，从而在kHz及以上频率探测早期宇宙引力波背景，灵敏度可接近甚至超越大爆炸核合成界限。


<details>
  <summary>Details</summary>
Motivation: kHz及以上频率的引力波为探测早期宇宙提供了独特窗口，对应能量尺度超过10^9 GeV，但现有探测器灵敏度远低于大爆炸核合成界限。

Method: 采用闭合光学环路干涉仪架构，引力波引起的相位位移在多次穿越中相干累积，产生尖锐的窄带共振，其可预测的梳状结构提供独特的实验特征。

Result: 对于与爱因斯坦望远镜基础设施兼容的方形或三角形环路，精细度约为500时，预计一年积分后灵敏度可接近甚至超越大爆炸核合成界限，最高可达数十kHz。

Conclusion: 环路干涉仪为实现探测高频随机引力波背景开辟了现实且独特的路径。

Abstract: Gravitational waves at kilohertz and higher frequencies offer a unique probe
of the early Universe at temperatures well beyond the reach of the cosmic
microwave background, corresponding to energy scales $\gtrsim 10^9$ GeV.
Existing detector concepts fall many orders of magnitude short of the big-bang
nucleosynthesis (BBN) bound on the stochastic background in this regime. We
propose a new interferometric architecture based on closed optical loops, in
which the gravitational-wave-induced phase shift accumulates coherently over
many traversals. This produces sharp, narrowband resonances whose predictable
comb structure provides a distinct experimental signature. For square or
triangular loops with parameters compatible with the Einstein Telescope
infrastructure, and finesse values of order 500, we project sensitivity that
approaches and even surpasses the BBN bound up to tens of kilohertz after one
year of integration. Such loop interferometers thus open a realistic and
distinctive path toward exploring high-frequency stochastic gravitational-wave
backgrounds.

</details>


### [43] [Transition-to-plunge self-force waveforms with a spinning primary](https://arxiv.org/abs/2510.13958)
*Loïc Honet,Lorenzo Küchler,Adam Pound,Geoffrey Compère*

Main category: gr-qc

TL;DR: 本文扩展了多尺度自旋力框架，将主黑洞的自旋纳入下一代引力波探测器的不对称质量比致密双星系统波形模型中，改进了从旋进到合并阶段的波形建模。


<details>
  <summary>Details</summary>
Motivation: 随着第三代引力波探测器的到来，需要为不对称质量比致密双星系统构建完整、准确且快速的波形模型。现有研究主要关注旋进阶段，但地面探测器中系统的最终合并可能是信号的主要部分。

Method: 将多尺度自旋力框架扩展到过渡到俯冲和合并-振铃阶段，将主黑洞的自旋纳入下一代过渡到俯冲波形模型，并通过在过渡到俯冲期间改变双星机械相空间的变量来改进复合旋进-过渡波形模型的构建。

Result: 提供了详细的数值实现讨论，并与数值相对论模拟进行了比较。

Conclusion: 成功构建了包含主黑洞自旋的改进波形模型，为第三代引力波探测器提供了更完整的波形建模工具。

Abstract: With the upcoming third-generation gravitational-wave detectors comes the
need to build complete, faithful, and fast waveform models for
asymmetric-mass-ratio compact binaries. Most efforts within the self-force
community have focused on modeling these binaries' inspiral regime, but for
ground-based detectors the systems' final merger can represent the dominant
part of the signal. Recent work by three of us has extended the multiscale
self-force framework through the transition-to-plunge and merger-ringdown
regimes for nonspinning binaries. In this paper, we generalize the
next-to-next-to-leading-order transition-to-plunge waveform model to include
the spin of the primary black hole. We also improve the construction of
composite inspiral-transition waveform models by performing a change of
variables on the binary's mechanical phase space during the transition to
plunge. We provide detailed discussions of our numerical implementation and
comparisons with numerical relativity simulations.

</details>


### [44] [$\texttt{GR-Athena++}$ Simulations of Spinning Binary Black Hole Mergers](https://arxiv.org/abs/2510.13963)
*Estuti Shukla,Alireza Rashti,Rossella Gamba,David Radice,Koustav Chandra*

Main category: gr-qc

TL;DR: 发布了GR-Athena++波形目录的第二版，包含4个新的准圆形、非进动、自旋双黑洞模拟，为下一代探测器提供高精度引力波波形。


<details>
  <summary>Details</summary>
Motivation: 为满足LISA、Cosmic Explorer和Einstein Telescope等下一代引力波探测器对高精度波形的要求，需要生成高保真度的引力波模拟数据。

Method: 使用高分辨率模拟准圆形、非进动、自旋双黑洞系统，通过Cauchy特征提取和有限半径提取在零无穷远提取引力波，进行收敛研究和自失配分析验证精度。

Result: 相位和振幅误差在合并时最大，最小误差分别为10^-2和10^-3量级；(2,2)模式的自失配在10^-5到10^-7之间，适用于106太阳质量双黑洞在0.002-0.1Hz频率范围。

Conclusion: 该目录提供了高精度的引力波波形数据，满足下一代探测器需求，所有波形数据已通过ScholarSphere公开。

Abstract: We present the second release of the $\texttt{GR-Athena++}$ waveform catalog,
comprising four new quasi-circular, non-precessing, spinning binary black hole
simulations. These simulations are performed at high resolutions and represent
a step toward generating high-fidelity gravitational waveforms that can
eventually meet the accuracy requirements of upcoming next-generation
detectors, including LISA, Cosmic Explorer, and Einstein Telescope.
Gravitational waves are extracted at future null infinity ( $\mathscr{I}^{+}$)
using both Cauchy characteristic extraction and finite-radius extraction. For
each simulation, we provide strain data across multiple resolutions and analyze
waveform accuracy via convergence studies and self-mismatch analyses. The
absolute phase and relative amplitude differences reach their largest values
near the merger, while the smallest errors are of order $\mathscr{O}(10^{-2})$
and $\mathscr{O}(10^{-3})$, respectively. A self-mismatch analysis of the
dominant $(2,2)$ mode yields mismatches between $\mathscr{O}(10^{-5})$ and
$\mathscr{O}(10^{-7})$ for a total binary mass of $10^{6}$ $M_{\odot}$ over the
frequency range $[0.002, 0.1]$ Hz using LISA noise curve. All waveforms are
publicly available via $\texttt{ScholarSphere}$.

</details>


### [45] [MHDuet : a high-order General Relativistic Radiation MHD code for CPU and GPU architectures](https://arxiv.org/abs/2510.13965)
*Carlos Palenzuela,Miguel Bezares,Steven Liebling,Federico Schianchi,Julio Fernando Abalos,Ricard Aguilera-Miret,Carles Bona,Juan Antonio Carretero,Joan Massò,Matthew P. Smith,Kwabena Amponsah,Kacper Kornet,Borja Miñano,Shrey Pareek,Miren Radia*

Main category: gr-qc

TL;DR: MHDuet是一个开源的全相对论磁流体动力学与中微子输运演化代码，使用先进技术求解爱因斯坦方程与磁化流体耦合问题，支持GPU加速运行，比CPU快一个数量级。


<details>
  <summary>Details</summary>
Motivation: 为多信使天体物理学中致密天体的动力学研究提供强大工具，解决现有代码在计算效率和精度方面的限制。

Method: 使用Simflowny平台从高级规范生成代码，采用自适应网格和大涡模拟技术，支持SAMRAI和AMReX基础设施，AMReX支持GPU编译和执行。

Result: 通过基准测试验证代码准确性，重现SAMRAI基础设施的先前结果；中子星模拟显示网格间距收敛速度快于二阶；GPU上的强扩展和弱扩展性能优异。

Conclusion: MHDuet代码为研究致密天体动力学提供了高效、准确的工具，特别在GPU加速方面表现出色，有望推动多信使天体物理学的发展。

Abstract: We present MHDuet, an open source evolution code for general relativistic
magnetohydrodynamics with neutrino transport. The code solves the full set of
Einstein equations coupled to a relativistic, magnetized fluid with an M1
neutrino radiation scheme using advanced techniques, including adaptive mesh
and large eddy simulation techniques, to achieve high accuracy. The Simflowny
platform generates the code from a high-level specification of the
computational system, producing code that runs with either the SAMRAI or AMReX
infrastructure. The choice of AMReX enables compilation and execution on GPUs,
running an order of magnitude faster than on CPUs at the node level. We
validate the code against benchmark tests, reproducing previous results
obtained with the SAMRAI infrastructure, and demonstrate its capabilities with
simulations of neutron stars employing realistic tabulated equations of state.
Resolution studies clearly demonstrate convergence faster than second order in
the grid spacing. Scaling tests reveal excellent strong and weak scaling
performance when running on GPUs. The goal of the code is to provide a powerful
tool for studying the dynamics of compact objects within multi-messenger
astrophysics.

</details>


### [46] [Boson Stars in $D \ge 4$ Dimensions: Stability, Oscillation Frequencies, and Dynamical Evolutions](https://arxiv.org/abs/2510.13988)
*Gareth Arturo Marks,Abdullah Al Zaif*

Main category: gr-qc

TL;DR: 在4、5、6维时空中构建了具有四次自相互作用项和孤子势的球对称玻色星解，通过微扰分析和非线性动力学演化验证了高维玻色星解的径向稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究高维时空中玻色星解的稳定性问题，特别是考虑自相互作用和孤子势的影响，以扩展对高维引力系统中物质构型的理解。

Method: 采用微扰分析将脉动方程推广到任意维度和势能，并通过维度约简进行球对称的非线性动力学演化，保持数值相对论方法的完整规范自由度。

Result: 识别出微扰稳定的解在非线性球对称动力学中确实普遍稳定，验证了高维玻色星解的存在性和稳定性。

Conclusion: 成功构建并验证了高维玻色星解的稳定性，为理解高维引力系统中的物质构型提供了重要理论基础。

Abstract: We construct spherically symmetric boson star solutions in $D \in \{4,5,6\}$
spacetime dimensions, considering the effects of both a quartic
self-interaction term and a solitonic potential. We then perform a perturbative
analysis, generalizing the pulsation equations to arbitrary dimension and
potential and hence demonstrating the existence of radially stable
higher-dimensional boson star solutions. We supplement these linear results
with perturbed and unperturbed nonlinear dynamical evolutions in spherical
symmetry, obtained using a dimensional reduction that allows us to evolve
spacetimes with any number of background dimensions using the same numerical
framework, while preserving the full gauge freedom of standard approaches to
numerical relativity. The results of these evolutions indicate that the
solutions we identify as perturbatively stable are indeed generally stable to
nonlinear spherical dynamics.

</details>


### [47] [Quantum Damping of Cosmological Shear: A New Prediction from Loop Quantum Cosmologies](https://arxiv.org/abs/2510.14021)
*Wen-Cong Gan,Leila L. Graef,Rudnei O. Ramos,Gustavo S. Vicente,Anzhong Wang*

Main category: gr-qc

TL;DR: mLQC-I模型通过量子反弹解决经典奇点，并在反弹后动态抑制剪切，实现量子各向同性化，无需精细调节即可产生均匀各向同性宇宙。


<details>
  <summary>Details</summary>
Motivation: 研究改进的圈量子宇宙学模型I（mLQC-I）中各向异性宇宙的动力学，探索其是否能够更有效地抑制各向异性并产生均匀各向同性宇宙。

Method: 分析mLQC-I模型中Bianchi I宇宙的动力学行为，研究量子反弹后剪切项的演化规律。

Result: mLQC-I不仅通过量子反弹解决经典奇点，而且在反弹后剪切项在深量子区域快速衰减至零，实现自然量子各向同性化，三个空间方向快速膨胀到宏观尺度。

Conclusion: mLQC-I比其他模型更有效地抑制各向异性，为早期宇宙描述提供了更可行的途径，强化了其作为早期宇宙描述的可行性。

Abstract: We analyze the dynamics of the Bianchi I universe in modified loop quantum
cosmology (Model I, or mLQC-I), uncovering a robust mechanism for
isotropization. As in the standard LQC, the classical singularities are
resolved by quantum bounce. Remarkably, mLQC-I exhibits a distinctive feature:
following the bounce, the shear is dynamically suppressed and decays rapidly to
zero within the deep quantum regime. This occurs independently of the
collapsing matter fields, leading to a natural quantum isotropization.
Consequently, the three spatial directions expand rapidly to macroscopic
scales, producing a homogeneous and isotropic universe directly from the
quantum epoch without fine-tuning. Our findings demonstrate that mLQC-I not
only resolves singularities but also provides a more effective pathway for
suppressing anisotropies than other models, thereby reinforcing its viability
as a description of the early universe.

</details>


### [48] [The Instability of the Critical Friedmann Spacetime at the Big Bang as an Alternative to Dark Energy](https://arxiv.org/abs/2510.14228)
*Christopher Alexander,Blake Temple,Zeke Vogler*

Main category: gr-qc

TL;DR: 本文分析了无压力弗里德曼时空在宇宙大爆炸时对径向扰动的不稳定性，发现爱因斯坦-欧拉方程中的不稳定性机制可以自然产生加速膨胀，无需宇宙常数或暗能量。


<details>
  <summary>Details</summary>
Motivation: 研究弗里德曼时空在宇宙大爆炸时的局部不稳定性，探索爱因斯坦-欧拉方程中固有的不稳定性机制是否能够解释宇宙加速膨胀现象，而不需要引入宇宙常数或暗能量。

Method: 使用自相似变量(t,ξ)重新表述爱因斯坦-欧拉方程，将临界弗里德曼时空作为不稳定鞍点静止解进行分析，通过ξ的偶次幂展开光滑解，并研究SM点的特征值。

Result: 证明了所有在对称中心光滑的解在ξ的领先阶与弗里德曼时空一致；集合F中的解在中间时间会加速偏离弗里德曼时空，但在t→∞时渐近衰减回相同的领先阶弗里德曼时空。

Conclusion: 爱因斯坦-欧拉方程中固有的不稳定性为加速膨胀提供了一个自然机制，无需借助宇宙常数或暗能量，这为宇宙学提供了一个新的解释框架。

Abstract: We characterize the local instability of pressureless Friedmann spacetimes to
radial perturbation at the Big Bang. The analysis is based on a formulation of
the Einstein-Euler equations in self-similar variables $(t,\xi)$, with
$\xi=r/t$, conceived to realize the critical ($k=0$) Friedmann spacetime as a
stationary solution whose character as an unstable saddle rest point $SM$ is
determined via an expansion of smooth solutions in even powers of $\xi$. The
eigenvalues of $SM$ imply the $k\neq0$ Friedmann spacetimes are unstable
solutions within the unstable manifold of $SM$. We prove that all solutions
smooth at the center of symmetry agree with a Friedmann spacetime at leading
order in $\xi$, and with an eye toward Cosmology, we focus on $\mathcal{F}$,
the set of solutions which agree with a $k<0$ Friedmann spacetime at leading
order, providing the maximal family into which generic underdense radial
perturbations of the unstable critical Friedmann spacetime will evolve. We
prove solutions in $\mathcal{F}$ generically accelerate away from Friedmann
spacetimes at intermediate times but decay back to the same leading order
Friedmann spacetime asymptotically as $t\to\infty$. Thus instabilities inherent
in the Einstein-Euler equations provide a natural mechanism for an accelerated
expansion without recourse to a cosmological constant or dark energy.

</details>


### [49] [Glitch noise classification in KAGRA O3GK observing data using unsupervised machine learning](https://arxiv.org/abs/2510.14291)
*Shoichi Oshino,Yusuke Sakai,Marco Meyer-Conde,Takashi Uchiyama,Yousuke Itoh,Yutaka Shikano,Yoshikazu Terada,Hirotaka Takahashi*

Main category: gr-qc

TL;DR: 使用变分自编码器(VAE)和谱聚类对KAGRA O3GK数据中的非平稳噪声进行无监督分类，识别出8种不同的噪声类别


<details>
  <summary>Details</summary>
Motivation: 引力波干涉仪受到各种非平稳噪声（称为毛刺噪声）的干扰，影响数据分析和干涉仪灵敏度。准确识别和分类毛刺噪声对于提高引力波观测的可靠性至关重要

Method: 使用变分自编码器(VAE)结合谱聚类对KAGRA O3GK数据中的图像进行无监督分类。VAE获得的潜变量经过维度压缩，在三维空间中可视化，并使用谱聚类进行分类

Result: 成功识别出8种不同的毛刺噪声类别，揭示了KAGRA在O3GK期间的噪声特征

Conclusion: 无监督学习在毛刺噪声分类方面具有巨大潜力，有助于干涉仪升级和未来第三代引力波天文台的发展

Abstract: Gravitational wave interferometers are disrupted by various types of
nonstationary noise, referred to as glitch noise, that affect data analysis and
interferometer sensitivity. The accurate identification and classification of
glitch noise are essential for improving the reliability of gravitational wave
observations. In this study, we demonstrated the effectiveness of unsupervised
machine learning for classifying images with nonstationary noise in the KAGRA
O3GK data. Using a variational autoencoder (VAE) combined with spectral
clustering, we identified eight distinct glitch noise categories. The latent
variables obtained from VAE were dimensionally compressed, visualized in
three-dimensional space, and classified using spectral clustering to better
understand the glitch noise characteristics of KAGRA during the O3GK period.
Our results highlight the potential of unsupervised learning for efficient
glitch noise classification, which may in turn potentially facilitate
interferometer upgrades and the development of future third-generation
gravitational wave observatories.

</details>


### [50] [A unified model of dark energy and inflation from the Markov-Mukhanov action](https://arxiv.org/abs/2510.14416)
*Hrishikesh Chakrabarty,Daniele Malafarina*

Main category: gr-qc

TL;DR: 提出通过Markov-Mukhanov修正的爱因斯坦-希尔伯特作用量统一描述暗能量和暴胀的模型，其中物质部分通过仅依赖于物质能量密度的标量耦合函数与引力耦合。


<details>
  <summary>Details</summary>
Motivation: 建立一个统一的框架来描述暗能量和暴胀，将暗能量分量作为早期宇宙中的暴胀场，并解释最新的DESI数据中暗能量状态方程w接近但不等于-1的现象。

Method: 使用Markov-Mukhanov修正的爱因斯坦-希尔伯特作用量，其中物质与引力的耦合通过依赖于物质能量密度的标量耦合函数实现，并确定允许暗能量分量作为暴胀场的耦合函数形式。

Result: 模型显示为了解释暴胀，暗能量状态方程w需要接近但不等于-1，这与最新的DESI数据一致。

Conclusion: 该模型成功地将暗能量和暴胀统一起来，提供了一个解释最新观测数据的理论框架，表明暗能量在早期宇宙中可能作为暴胀场发挥作用。

Abstract: We propose a unified model of dark energy and inflation through the
Markov-Mukhanov modification of the Einstein-Hilbert action, where the matter
sector is coupled to gravity via a scalar coupling function depending only on
the energy density of the matter content. We assume that the coupling function
encodes the UV corrections to the standard model of cosmology and we determine
the form of the coupling that allows for the dark energy component to be
dynamical and act as the inflaton field in the early universe. Interestingly we
show that our model, in order to account for inflation, prefers a dark energy
equation of state with $w$ close but not equal to $-1$ in agreement with the
latest DESI data.

</details>


### [51] [Black Holes in Asymptotic Safety: A Review of Solutions and Phenomenology](https://arxiv.org/abs/2510.14552)
*Andrea Spina*

Main category: gr-qc

TL;DR: 这篇综述总结了渐进安全量子引力框架下量子修正黑洞解的研究现状，包括解的结构性质、热力学、蒸发过程以及动力学方面。


<details>
  <summary>Details</summary>
Motivation: 渐进安全为量子引力提供了一个保守且可预测的框架，黑洞作为探测最强引力场的自然场所，能够揭示经典广义相对论的局限性。

Method: 通过重整化群改进经典度规或基于耦合流启发的有效作用量，构建了多种量子修正黑洞解。

Result: 提出了多种量子修正黑洞解，讨论了它们的结构特性、热力学行为、蒸发过程以及准正规模、阴影等动力学特征。

Conclusion: 渐进安全框架下的量子修正黑洞研究为理解量子引力效应提供了重要见解，目前该领域已取得显著进展。

Abstract: Asymptotic Safety offers a conservative and predictive framework for quantum
gravity, based on the existence of a renormalization group fixed point that
ensures ultraviolet completeness without introducing new degrees of freedom.
Black holes provide a natural arena in which to explore the implications of
this scenario, as they probe the strongest gravitational fields and highlight
the shortcomings of classical general relativity. In recent years, a variety of
quantum-corrected black-hole solutions have been constructed within the
Asymptotic Safety approach, either by renormalization-group improvement of
classical metrics or through effective actions inspired by the flow of
couplings. This review summarizes the current status of these developments. We
discuss the structure and properties of the proposed solutions, their
thermodynamics and evaporation, and their dynamical aspects such as quasinormal
modes and shadows.

</details>


### [52] [Toward a unified view of agnostic parametrizations for deformed black holes](https://arxiv.org/abs/2510.14707)
*Manuel Del Piano,Ciro De Simone,Mattia Damia Paciarini,Mikołaj Myszkowski,Francesco Sannino,Vania Vellucci*

Main category: gr-qc

TL;DR: 本文建立了三种黑洞参数化方案之间的等价变换映射，分析了不同方案重现准正则模谱所需的最小参数数量，并探讨了这些有效描述在探测黑洞物理互补方面的优势。


<details>
  <summary>Details</summary>
Motivation: 为了从观测数据中提取与模型无关的黑洞基本属性信息，需要研究不同参数化方案之间的等价关系，以确定在给定精度下所需的最小参数数量。

Method: 构建了Johannsen-Psaltis、Rezzolla-Zhidenko和有效度规描述三种参数化方案之间的显式变换映射，并选择代表性黑洞几何来评估重现准正则模谱所需的最小参数集。

Result: 分析表明，对于给定的可观测量，三种框架中有限个系数足以达到所需的精度要求。

Conclusion: 这些有效描述方法的各自优势可以被利用来探测黑洞物理的互补方面。

Abstract: A variety of robust and effective descriptions have been devised to extract
model-independent information about the fundamental properties of black holes
from observational data when searching for deviations from general relativity.
In this work, we construct explicit transformation maps establishing the
equivalence among three relevant parametrizations for different spacetime
patches: Johannsen-Psaltis, Rezzolla-Zhidenko, and Effective Metric
Description. We then select representative black hole geometries to determine
the minimal number of parameters required within each scheme to reproduce the
associated quasi-normal mode spectra with a prescribed degree of accuracy. Our
analysis shows that, for the given observables, a finite set of coefficients
suffices to attain the desired precision in the three frameworks. Finally, we
emphasize how the individual strengths of these effective descriptions can be
exploited to probe complementary aspects of black hole physics.

</details>


### [53] [Quantum confinement of scalar bosons in the Bonnor-Melvin spacetime: uniform magnetic field and rainbow gravity effects](https://arxiv.org/abs/2510.14789)
*Omar Mustafa,Abdullah Guvendi*

Main category: gr-qc

TL;DR: 本文对Klein-Gordon标量玻色子和反玻色子在Bonnor-Melvin时空中受均匀磁场约束的情况进行了精确解析研究，结合了彩虹引力修正和正宇宙学常数。研究发现宇宙常数将时空划分为无限序列的约束域，磁场量子态在弯曲几何中发生塌缩，而宇宙常数可部分解除这种塌缩。


<details>
  <summary>Details</summary>
Motivation: 研究引力约束、拓扑结构、磁场和普朗克尺度修正如何共同控制相对论量子场在弯曲和磁化背景中的光谱和空间特性。

Method: 在包含彩虹引力修正和正宇宙学常数的Bonnor-Melvin时空中，将Klein-Gordon方程简化为超几何微分方程，得到能量谱和径向波函数的闭式表达式。使用Magueijo-Smolin框架和圈量子引力两种代表性彩虹引力模型。

Result: 获得了普朗克尺度有界的对称粒子-反粒子谱。发现弯曲磁化几何中所有m≠0的磁量子态塌缩到m=0能级，增加宇宙常数可部分解除这种塌缩。更强的磁场增强了空间局域化，将玻色子约束为静态或旋转环状构型。

Conclusion: 揭示了引力约束、拓扑、磁场和普朗克尺度修正如何共同决定相对论量子场在弯曲磁化背景中的光谱和空间特性，建立了全局时空曲率与局域量子结构之间的直接联系。

Abstract: We present an exact analytical study of Klein-Gordon (KG) scalar bosons and
antibosons confined in the Bonnor-Melvin (BM) spacetime under a uniform
magnetic field, incorporating rainbow gravity (RG) corrections with a positive
cosmological constant. The cosmological constant partitions spacetime into an
infinite sequence of confinement domains bounded by impenetrable barriers.
Within the first allowed domain, the KG equation reduces to a hypergeometric
differential equation, yielding closed-form expressions for both the energy
spectra and the radial wavefunctions in terms of hypergeometric polynomials.
Two representative RG models, inspired by the Magueijo-Smolin framework and
loop quantum gravity (LQG), produce Planck-scale bounded, symmetric
particle-antiparticle spectra. A distinctive feature of the curved magnetized
geometry is the collapse of all magnetic quantum states $m \neq 0$ onto the $m
= 0$ level for each radial excitation, a degeneracy absent in flat spacetime.
Increasing the cosmological constant partially lifts this collapse,
establishing a direct link between the global spacetime curvature and the local
quantum structure. Radial probability density analysis further shows that
stronger magnetic fields enhance spatial localization, confining bosons into
static or rotating ring-like configurations with nodal architectures that
evolve systematically with quantum numbers. These findings reveal how
gravitational confinement, topology, magnetic fields, and Planck-scale
corrections jointly govern the spectral and spatial properties of relativistic
quantum fields in curved and magnetized backgrounds.

</details>


### [54] [Non-exotic traversable wormholes with strong deflection angle in King and Dekel-Zhao dark matter halos under f(R,Lm) gravity](https://arxiv.org/abs/2510.14833)
*Susmita Sarkar,Nayan Sarkar,Abdelmalek Bouzenada,Farook Rahaman*

Main category: gr-qc

TL;DR: 在f(R, L_m)引力理论框架下，研究King和Dekel-Zhao暗物质晕中的渐近平坦非奇异可穿越虫洞几何。使用两种模型分析虫洞解，验证能量条件，并研究引力透镜效应。


<details>
  <summary>Details</summary>
Motivation: 探索在修正引力理论中，暗物质环境如何支持物理上一致的非奇异可穿越虫洞几何，研究修正引力、暗物质与天体物理观测之间的相互作用。

Method: 采用f(R, L_m)引力理论的两种函数形式：Model-I: f(R, L_m)=(R/2) + L_m^α 和 Model-II: f(R, L_m)=(R/2) + (1 + λR)L_m。使用King和Dekel-Zhao暗物质密度分布，分析虫洞解、能量条件、嵌入表面、径向距离和总引力能。

Result: 在f(R, L_m)引力理论和适当参数选择下，暗物质环境能够维持物理一致的非奇异可穿越虫洞几何，并产生独特的引力透镜特征。

Conclusion: 研究表明暗物质环境可以在修正引力理论中支持可穿越虫洞，为理解修正引力、暗物质和天体物理观测之间的关系提供了新见解。

Abstract: In this article, we investigate asymptotically flat non-exotic traversable
wormhole geometries within the King and Dekel-Zhao dark matter halos in the
framework of $f(R, L_m)$ gravity. Two functional forms of the theory are
considered: Model-I: $f(R, L_m)=(R/2) + L_m^{\alpha}$ and Model-II: $f(R,
L_m)=(R/2) + (1 + \lambda R)L_m$. For both models, wormhole solutions are
obtained and analyzed using the King and Dekel-Zhao dark matter density
profiles, allowing us to explore how the underlying matter distribution
influences the wormhole structures. The energy conditions are examined to
verify the feasibility of sustaining the wormhole geometries with non-exotic
matter, while embedding surfaces, proper radial distance, and total
gravitational energy are studied to illustrate the wormhole's physical
viability and traversability. Moreover, we test the strong deflection angle and
its implications for gravitational lensing and show possible observational
signatures of such wormhole configurations. Our results indicate that within
$f(R, L_m)$ gravity, and for appropriate parameter choices, dark matter
environments can sustain physically consistent non-exotic traversable wormhole
geometries with distinct gravitational lensing signatures, providing new
insights into the interplay between modified gravity, dark matter, and
astrophysical observations.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [55] [Large Language Models for Real-World IoT Device Identification](https://arxiv.org/abs/2510.13817)
*Rameen Mahmood,Tousif Ahmed,Sai Teja Peddinti,Danny Yuxing Huang*

Main category: cs.LG

TL;DR: 提出了一种基于语义推理的物联网设备识别方法，将设备识别重新定义为异构网络元数据的语言建模任务，使用指令调优的LLaMA3.18B模型在真实世界IoT流量数据集上实现高精度识别。


<details>
  <summary>Details</summary>
Motivation: 物联网设备的快速扩张超出了现有识别方法的能力，在开放世界环境中存在安全、隐私和网络问责风险，特别是当流量元数据不完整、有噪声或被故意混淆时。

Method: 使用基于互信息和熵的稳定性评分引导的大型语言模型集成生成高质量供应商标签，然后通过课程学习对量化LLaMA3.18B模型进行指令调优，以支持稀疏性和长尾供应商分布下的泛化。

Result: 模型在2,015个供应商上实现了98.25%的top-1准确率和90.73%的宏准确率，同时对缺失字段、协议漂移和对抗性操作保持韧性。

Conclusion: 指令调优的大型语言模型为大规模真实世界设备识别提供了可扩展且可解释的基础。

Abstract: The rapid expansion of IoT devices has outpaced current identification
methods, creating significant risks for security, privacy, and network
accountability. These challenges are heightened in open-world environments,
where traffic metadata is often incomplete, noisy, or intentionally obfuscated.
We introduce a semantic inference pipeline that reframes device identification
as a language modeling task over heterogeneous network metadata. To construct
reliable supervision, we generate high-fidelity vendor labels for the IoT
Inspector dataset, the largest real-world IoT traffic corpus, using an ensemble
of large language models guided by mutual-information and entropy-based
stability scores. We then instruction-tune a quantized LLaMA3.18B model with
curriculum learning to support generalization under sparsity and long-tail
vendor distributions. Our model achieves 98.25% top-1 accuracy and 90.73% macro
accuracy across 2,015 vendors while maintaining resilience to missing fields,
protocol drift, and adversarial manipulation. Evaluation on an independent IoT
testbed, coupled with explanation quality and adversarial stress tests,
demonstrates that instruction-tuned LLMs provide a scalable and interpretable
foundation for real-world device identification at scale.

</details>


### [56] [Self-Training with Dynamic Weighting for Robust Gradual Domain Adaptation](https://arxiv.org/abs/2510.13864)
*Zixi Wang,Yushe Cao,Yubo Huang,Jinzhu Wei,Jingzehua Xu,Shuai Zhang,Xin Lai*

Main category: cs.LG

TL;DR: 提出了一种名为STDW的自训练动态加权方法，通过动态平衡源域和目标域损失贡献来增强渐进域适应的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统渐进域适应方法在通过中间域和自训练缓解域偏移时，常面临知识迁移效率低或中间数据不完整的问题。

Method: 引入动态加权机制，设计由时间变化超参数ϱ控制的优化框架，利用自训练生成伪标签并优化加权目标函数进行迭代模型更新。

Result: 在旋转MNIST、颜色偏移MNIST、肖像数据集和Cover Type数据集上的实验表明，STDW优于现有基线方法。

Conclusion: 该工作为鲁棒渐进域适应提供了理论洞见和实用框架，在动态现实场景中具有应用潜力。

Abstract: In this paper, we propose a new method called Self-Training with Dynamic
Weighting (STDW), which aims to enhance robustness in Gradual Domain Adaptation
(GDA) by addressing the challenge of smooth knowledge migration from the source
to the target domain. Traditional GDA methods mitigate domain shift through
intermediate domains and self-training but often suffer from inefficient
knowledge migration or incomplete intermediate data. Our approach introduces a
dynamic weighting mechanism that adaptively balances the loss contributions of
the source and target domains during training. Specifically, we design an
optimization framework governed by a time-varying hyperparameter $\varrho$
(progressing from 0 to 1), which controls the strength of domain-specific
learning and ensures stable adaptation. The method leverages self-training to
generate pseudo-labels and optimizes a weighted objective function for
iterative model updates, maintaining robustness across intermediate domains.
Experiments on rotated MNIST, color-shifted MNIST, portrait datasets, and the
Cover Type dataset demonstrate that STDW outperforms existing baselines.
Ablation studies further validate the critical role of $\varrho$'s dynamic
scheduling in achieving progressive adaptation, confirming its effectiveness in
reducing domain bias and improving generalization. This work provides both
theoretical insights and a practical framework for robust gradual domain
adaptation, with potential applications in dynamic real-world scenarios. The
code is available at https://github.com/Dramwig/STDW.

</details>


### [57] [Deep Edge Filter: Return of the Human-Crafted Layer in Deep Learning](https://arxiv.org/abs/2510.13865)
*Dongkwan Lee,Junhoo Lee,Nojun Kwak*

Main category: cs.LG

TL;DR: Deep Edge Filter通过高通滤波深度神经网络特征来提升模型泛化能力，假设高频分量包含任务相关语义信息，低频分量包含领域特定偏差。


<details>
  <summary>Details</summary>
Motivation: 假设神经网络在深度特征的高频分量中编码任务相关语义信息，而在低频分量中存储领域特定偏差，通过分离这些分量来改善模型泛化性。

Method: 从原始特征中减去低通滤波输出，隔离可泛化表示，同时保持架构完整性。

Result: 在视觉、文本、3D和音频等多个领域的实验结果显示，无论模型架构和数据模态如何，都能获得一致的性能提升。

Conclusion: 该方法实现了特征稀疏化并有效隔离高频分量，为核心假设提供了实证验证，代码已开源。

Abstract: We introduce the Deep Edge Filter, a novel approach that applies high-pass
filtering to deep neural network features to improve model generalizability.
Our method is motivated by our hypothesis that neural networks encode
task-relevant semantic information in high-frequency components while storing
domain-specific biases in low-frequency components of deep features. By
subtracting low-pass filtered outputs from original features, our approach
isolates generalizable representations while preserving architectural
integrity. Experimental results across diverse domains such as Vision, Text,
3D, and Audio demonstrate consistent performance improvements regardless of
model architecture and data modality. Analysis reveals that our method induces
feature sparsification and effectively isolates high-frequency components,
providing empirical validation of our core hypothesis. The code is available at
https://github.com/dongkwani/DeepEdgeFilter.

</details>


### [58] [CoLoR-GAN: Continual Few-Shot Learning with Low-Rank Adaptation in Generative Adversarial Networks](https://arxiv.org/abs/2510.13869)
*Munsif Ali,Leonardo Rossi,Massimo Bertozzi*

Main category: cs.LG

TL;DR: CoLoR-GAN是一个用于持续少样本学习的GAN框架，通过低秩适配技术减少参数数量，在保持SOTA性能的同时显著降低资源需求。


<details>
  <summary>Details</summary>
Motivation: 当前最有效的持续学习方法如LFS-GAN在每次训练迭代中引入大量新权重，长期来看参数负担显著。需要开发既能处理少样本学习又能减少参数需求的持续学习框架。

Method: 提出CoLoR-GAN框架，利用低秩张量高效适配模型到目标任务。引入LoRA技术，并进一步提出LLoRA（LoRA中的LoRA）技术用于卷积层优化适配器大小。提供超参数选择的实证研究。

Result: 在多个基准CL和FS任务上的实验表明，CoLoR-GAN高效且达到SOTA性能，同时资源需求大幅减少。

Conclusion: CoLoR-GAN通过低秩适配技术成功解决了GAN中持续少样本学习的挑战，在保持高性能的同时显著降低了参数需求。

Abstract: Continual learning (CL) in the context of Generative Adversarial Networks
(GANs) remains a challenging problem, particularly when it comes to learn from
a few-shot (FS) samples without catastrophic forgetting. Current most effective
state-of-the-art (SOTA) methods, like LFS-GAN, introduce a non-negligible
quantity of new weights at each training iteration, which would become
significant when considering the long term. For this reason, this paper
introduces \textcolor{red}{\textbf{\underline{c}}}ontinual
few-sh\textcolor{red}{\textbf{\underline{o}}}t learning with
\textcolor{red}{\textbf{\underline{lo}}}w-\textcolor{red}{\textbf{\underline{r}}}ank
adaptation in GANs named CoLoR-GAN, a framework designed to handle both FS and
CL together, leveraging low-rank tensors to efficiently adapt the model to
target tasks while reducing even more the number of parameters required.
Applying a vanilla LoRA implementation already permitted us to obtain pretty
good results. In order to optimize even further the size of the adapters, we
challenged LoRA limits introducing a LoRA in LoRA (LLoRA) technique for
convolutional layers. Finally, aware of the criticality linked to the choice of
the hyperparameters of LoRA, we provide an empirical study to easily find the
best ones. We demonstrate the effectiveness of CoLoR-GAN through experiments on
several benchmark CL and FS tasks and show that our model is efficient,
reaching SOTA performance but with a number of resources enormously reduced.
Source code is available on
\href{https://github.com/munsifali11/CoLoR-GAN}{Github.

</details>


### [59] [Joint Discriminative-Generative Modeling via Dual Adversarial Training](https://arxiv.org/abs/2510.13872)
*Xuwang Yin,Claire Zhang,Julie Steele,Nir Shavit,Tony T. Wang*

Main category: cs.LG

TL;DR: 提出一种结合对抗训练的统一框架，同时实现鲁棒分类和高质量生成建模，解决了传统联合能量模型训练不稳定和样本质量差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统联合能量模型（JEM）在单一框架中同时实现鲁棒分类和高质量生成存在挑战，主要受限于SGLD训练的不稳定性和样本质量差。

Method: 采用对抗训练原则，用PGD生成对比样本优化能量函数，结合两阶段训练解决批归一化与EBM训练不兼容问题，无需显式梯度惩罚。

Result: 在CIFAR-10、CIFAR-100和ImageNet上显著提升对抗鲁棒性，生成质量接近扩散模型，是首个在复杂高分辨率数据集上实现高质量生成的MCMC-based EBM方法。

Conclusion: 对抗训练可作为统一框架的有效基础，同时实现视觉数据的生成和鲁棒分类，解决了JEM扩展的关键稳定性问题。

Abstract: Simultaneously achieving robust classification and high-fidelity generative
modeling within a single framework presents a significant challenge. Hybrid
approaches, such as Joint Energy-Based Models (JEM), interpret classifiers as
EBMs but are often limited by the instability and poor sample quality inherent
in SGLD-based training. We address these limitations by proposing a novel
training framework that integrates adversarial training (AT) principles for
both discriminative robustness and stable generative learning. The proposed
method introduces three key innovations: (1) the replacement of SGLD-based JEM
learning with a stable, AT-based approach that optimizes the energy function by
discriminating between real data and PGD-generated contrastive samples using
the BCE loss; (2) synergistic adversarial training for the discriminative
component that enhances classification robustness while eliminating the need
for explicit gradient penalties; and (3) a two-stage training procedure to
resolve the incompatibility between batch normalization and EBM training.
Experiments on CIFAR-10, CIFAR-100, and ImageNet demonstrate that our method
substantially improves adversarial robustness over existing hybrid models while
maintaining competitive generative performance. On ImageNet, when optimized for
generative modeling, our model's generative fidelity surpasses that of BigGAN
and approaches diffusion models, representing the first MCMC-based EBM approach
to achieve high-quality generation on complex, high-resolution datasets. Our
approach addresses key stability issues that have limited JEM scaling and
demonstrates that adversarial training can serve as an effective foundation for
unified frameworks capable of generating and robustly classifying visual data.

</details>


### [60] [K-frames: Scene-Driven Any-k Keyframe Selection for long video understanding](https://arxiv.org/abs/2510.13891)
*Yifeng Yao,Yike Yun,Jing Wang,Huishuai Zhang,Dongyan Zhao,Ke Tian,Zhihao Wang,Minghui Qiu,Tao Wang*

Main category: cs.LG

TL;DR: K-frames提出了一种基于场景驱动的关键帧选择新范式，通过预测语义连贯的查询相关片段来保持时间连续性，支持任意数量关键帧选择以适应不同用户预算。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在长视频理解中受限于上下文窗口和计算成本，均匀帧采样导致信息丢失，而现有关键帧选择方法产生稀疏且时间不连续的帧，忽略了场景连续性且缺乏多尺度帧选择的灵活性。

Method: 首先构建PeakClips数据集，包含20万个基于查询的视频亮点片段。然后采用三阶段渐进式课程学习：两个监督微调阶段用于时间定位和关键片段感知，以及一个强化学习阶段直接优化场景驱动预测策略。

Result: 在主要长视频理解基准测试上的广泛实验表明，K-frames提供了有效、可解释且即插即用的多尺度关键帧选择解决方案。

Conclusion: K-frames通过场景驱动的关键帧选择方法，解决了长视频理解中的信息丢失和场景连续性缺失问题，为多尺度帧选择提供了灵活有效的解决方案。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
capabilities in image understanding, but long-video are constrained by context
windows and computational cost. Uniform frame sampling often leads to
substantial information loss. Meanwhile existing keyframe selection methods
such as text-frame retrieval or RL-based frame optimization typically yield
sparse and temporally disjointed frames, overlooking scene continuity and
lacking flexibility for multi-scale frame selection. To address these
limitations, we introduce K-frames, a novel paradigm for scene-driven keyframe
selection that preserves temporal continuity. Instead of selecting individual
frames, K-frames predicts semantically coherent, query-relevant clips, which
enables any-k keyframes selection to meet diverse user budgets. To achieve this
approach, we first introduce PeakClips, a dataset of 200K video highlights
conditioned by query. Building on this dataset, K-frames learns clip2frame
selection using a three-stage progressive curriculum. It involves two
Supervised Fine-Tuning stages for temporal grounding and key-clip perception,
followed by a Reinforcement Learning stage that directly optimizes the
scene-driven prediction policy for downstream task without further annotations.
Extensive experiments on major long-video understanding benchmarks demonstrate
that K-frames provides an effective, interpretable, and plug-and-play solution
for keyframe selection at various scales. Our dataset and model will be
available.

</details>


### [61] [Multi-View Semi-Supervised Label Distribution Learning with Local Structure Complementarity](https://arxiv.org/abs/2510.13917)
*Yanshan Xiao,Kaihong Wu,Bo Liu*

Main category: cs.LG

TL;DR: 提出多视图半监督标签分布学习方法MVSS-LDL，通过局部结构互补性解决多视图标签分布学习问题


<details>
  <summary>Details</summary>
Motivation: 现有标签分布学习方法只针对单视图和有标签数据，多视图场景下同时包含有标签和无标签数据的问题尚未被考虑

Method: 利用每个视图的局部最近邻结构，通过跨视图互补获得更全面的最近邻信息，构建基于图学习的多视图半监督模型

Result: 数值研究表明MVSS-LDL比现有单视图标签分布学习方法获得明显更好的分类性能

Conclusion: 这是首次尝试多视图标签分布学习，通过局部结构互补性，不同视图可以相互提供局部结构信息以互补

Abstract: Label distribution learning (LDL) is a paradigm that each sample is
associated with a label distribution. At present, the existing approaches are
proposed for the single-view LDL problem with labeled data, while the
multi-view LDL problem with labeled and unlabeled data has not been considered.
In this paper, we put forward the multi-view semi-supervised label distribution
learning with local structure complementarity (MVSS-LDL) approach, which
exploits the local nearest neighbor structure of each view and emphasizes the
complementarity of local nearest neighbor structures in multiple views.
Specifically speaking, we first explore the local structure of view $v$ by
computing the $k$-nearest neighbors. As a result, the $k$-nearest neighbor set
of each sample $\boldsymbol{x}_i$ in view $v$ is attained. Nevertheless, this
$k$-nearest neighbor set describes only a part of the nearest neighbor
information of sample $\boldsymbol{x}_i$. In order to obtain a more
comprehensive description of sample $\boldsymbol{x}_i$'s nearest neighbors, we
complement the nearest neighbor set in view $v$ by incorporating sample
$\boldsymbol{x}_i$'s nearest neighbors in other views. Lastly, based on the
complemented nearest neighbor set in each view, a graph learning-based
multi-view semi-supervised LDL model is constructed. By considering the
complementarity of local nearest neighbor structures, different views can
mutually provide the local structural information to complement each other. To
the best of our knowledge, this is the first attempt at multi-view LDL.
Numerical studies have demonstrated that MVSS-LDL attains explicitly better
classification performance than the existing single-view LDL methods.

</details>


### [62] [Weight Weaving: Parameter Pooling for Data-Free Model Merging](https://arxiv.org/abs/2510.13921)
*Levy Chaves,Eduardo Valle,Sandra Avila*

Main category: cs.LG

TL;DR: 提出Weight Weaving方法，通过用户定义的池化函数在λ值搜索空间中池化模型权重，无需评估数据即可改进模型合并性能


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法严重依赖缩放超参数λ，但缺乏无需数据的原则性设置方法，实践中往往需要访问评估数据来调整λ，这在实际中不可行

Method: Weight Weaving技术，使用用户定义的池化函数（如平均、随机选择或现有模型合并方法）在λ值搜索空间中池化模型权重

Result: 在三个ViT变体的三种实验设置中验证，平均准确率提升高达15.9个百分点，无需数据即可持续改进多种模型合并方法

Conclusion: Weight Weaving具有高模块化，对搜索空间约束最小，与现有模型合并方法正交，消除了评估数据需求

Abstract: Model merging provides a cost-effective and data-efficient combination of
specialized deep neural networks through parameter integration. This technique
leverages expert models across downstream tasks without requiring retraining.
Most model merging approaches critically depend on scaling hyper-parameters
$\lambda$, which weight each model's contribution globally or individually.
Principled approaches for setting scaling factors without accessing any data
(data-free) are scarce, often leading researchers to tune $\lambda$ using
privileged data from the evaluation set, which is obviously unfeasible in
practice. To address this limitation, we introduce Weight Weaving, a
plug-and-play technique that pools model weights across $\lambda$ values search
space using user-defined pooling functions, such as averaging, random
selection, or even existing model merging methods. Our method demonstrates high
modularity, imposing minimal constraints on the search space. It operates
orthogonally to existing model merging methods and eliminates evaluation data
requirements. We validate Weight Weaving across three ViT variants in three
experimental setups: vision multi-task learning, vision continual learning, and
domain generalization. Our method consistently improves the performance of
several model merging methods, achieving average accuracy gains of up to 15.9
percentage points in a data-free setting.

</details>


### [63] [LTR-ICD: A Learning-to-Rank Approach for Automatic ICD Coding](https://arxiv.org/abs/2510.13922)
*Mohammad Mansoori,Amira Soliman,Farzaneh Etminani*

Main category: cs.LG

TL;DR: 该论文提出了一种从检索系统角度处理ICD编码分配任务的新方法，将问题重新定义为分类和排序任务，以考虑编码的顺序性，在识别高优先级编码方面表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 临床笔记中的ICD编码顺序对医疗诊断和报销至关重要，但现有方法将其视为分类任务而忽略了编码顺序。

Method: 从检索系统角度重新定义问题，将其构建为分类和排序任务，以同时考虑编码分配和排序。

Result: 在主要诊断编码排序准确率上达到47%（现有最佳方法为20%），在分类指标上微平均F1为0.6065，宏平均F1为0.2904，均优于现有最佳模型。

Conclusion: 提出的框架在识别高优先级编码方面具有显著优势，证明了考虑编码顺序的重要性。

Abstract: Clinical notes contain unstructured text provided by clinicians during
patient encounters. These notes are usually accompanied by a sequence of
diagnostic codes following the International Classification of Diseases (ICD).
Correctly assigning and ordering ICD codes are essential for medical diagnosis
and reimbursement. However, automating this task remains challenging.
State-of-the-art methods treated this problem as a classification task, leading
to ignoring the order of ICD codes that is essential for different purposes. In
this work, as a first attempt, we approach this task from a retrieval system
perspective to consider the order of codes, thus formulating this problem as a
classification and ranking task. Our results and analysis show that the
proposed framework has a superior ability to identify high-priority codes
compared to other methods. For instance, our model accuracy in correctly
ranking primary diagnosis codes is 47%, compared to 20% for the
state-of-the-art classifier. Additionally, in terms of classification metrics,
the proposed model achieves a micro- and macro-F1 scores of 0.6065 and 0.2904,
respectively, surpassing the previous best model with scores of 0.597 and
0.2660.

</details>


### [64] [Distributional Consistency Loss: Beyond Pointwise Data Terms in Inverse Problems](https://arxiv.org/abs/2510.13972)
*George Webber,Andrew J. Reader*

Main category: cs.LG

TL;DR: 提出了一种新的数据保真度损失函数——分布一致性损失，用于逆问题中的信号恢复，通过分布级校准替代逐点匹配，避免对测量噪声的过拟合。


<details>
  <summary>Details</summary>
Motivation: 传统的数据保真度损失函数如均方误差或负对数似然寻求与噪声测量的逐点一致性，往往导致对噪声的过拟合。需要一种能够统计评估测量数据与当前估计所暗示的噪声分布一致性的方法。

Method: 引入分布一致性损失，使用基于模型的概率分数对每个测量进行分布级校准，作为标准数据一致性项的直接实用替代品。该方法与现代正则化器兼容，优化方式与传统损失相同。

Result: 在图像去噪中，使用DC损失替代MSE损失无需提前停止训练即可获得更高PSNR；在医学图像重建中，DC损失减少了高度迭代重建中的伪影，并增强了手工正则化的效果。

Conclusion: DC损失为逆问题提供了一种统计基础扎实、性能提升的替代方案，可替代传统的保真度损失函数。

Abstract: Recovering true signals from noisy measurements is a central challenge in
inverse problems spanning medical imaging, geophysics, and signal processing.
Current solutions balance prior assumptions regarding the true signal
(regularization) with agreement to noisy measured data (data-fidelity).
Conventional data-fidelity loss functions, such as mean-squared error (MSE) or
negative log-likelihood, seek pointwise agreement with noisy measurements,
often leading to overfitting to noise. In this work, we instead evaluate
data-fidelity collectively by testing whether the observed measurements are
statistically consistent with the noise distributions implied by the current
estimate. We adopt this aggregated perspective and introduce distributional
consistency (DC) loss, a data-fidelity objective that replaces pointwise
matching with distribution-level calibration using model-based probability
scores for each measurement. DC loss acts as a direct and practical plug-in
replacement for standard data consistency terms: i) it is compatible with
modern regularizers, ii) it is optimized in the same way as traditional losses,
and iii) it avoids overfitting to measurement noise even without the use of
priors. Its scope naturally fits many practical inverse problems where the
measurement-noise distribution is known and where the measured dataset consists
of many independent noisy values. We demonstrate efficacy in two key example
application areas: i) in image denoising with deep image prior, using DC
instead of MSE loss removes the need for early stopping and achieves higher
PSNR; ii) in medical image reconstruction from Poisson-noisy data, DC loss
reduces artifacts in highly-iterated reconstructions and enhances the efficacy
of hand-crafted regularization. These results position DC loss as a
statistically grounded, performance-enhancing alternative to conventional
fidelity losses for inverse problems.

</details>


### [65] [BitNet Distillation](https://arxiv.org/abs/2510.13998)
*Xun Wu,Shaohan Huang,Wenhui Wang,Ting Song,Li Dong,Yan Xia,Furu Wei*

Main category: cs.LG

TL;DR: BitNet Distillation (BitDistill) 是一种轻量级方法，可将现成的全精度大语言模型微调为1.58位精度（三元权重{-1, 0, 1}），在特定下游任务上实现与全精度模型相当的性能，同时节省10倍内存并加速推理2.65倍。


<details>
  <summary>Details</summary>
Motivation: 解决全精度大语言模型在特定任务上部署时计算成本高、内存占用大的问题，通过极低位量化实现高效推理。

Method: 结合三个关键技术：SubLN模块（来自BitNet）、基于MiniLM的多头注意力蒸馏、以及作为关键预热步骤的持续预训练，以缓解全精度与1.58位模型在特定任务上的性能差距。

Result: 实验结果显示，BitDistill在不同模型规模下都能达到与全精度对应模型相当的性能，同时实现高达10倍的内存节省和2.65倍的CPU推理加速。

Conclusion: BitDistill提供了一种有效的轻量级解决方案，能够在保持任务性能的同时显著降低大语言模型的部署成本，特别适合资源受限环境下的特定任务应用。

Abstract: In this paper, we present BitNet Distillation (BitDistill), a lightweight
pipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into
1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream
tasks, achieving strong task-specific performance with minimal computational
cost. Specifically, BitDistill incorporates three key techniques: the SubLN
module, as introduced in BitNet; multi-head attention distillation, based on
MiniLM; and continual pre-training, which serves as a crucial warm-up step to
mitigate the scalability issue of the performance gap between finetuned
full-precision and 1.58-bit LLMs on specific tasks. Experimental results show
that BitDistill achieves performance comparable to the full-precision
counterpart models across model size, while enabling up to 10x memory savings
and 2.65x faster inference on CPUs. Code is available at
https://github.com/microsoft/BitNet.

</details>


### [66] [REAP the Experts: Why Pruning Prevails for One-Shot MoE compression](https://arxiv.org/abs/2510.13999)
*Mike Lasby,Ivan Lazarevich,Nish Sinnadurai,Sean Lie,Yani Ioannou,Vithursan Thangarasa*

Main category: cs.LG

TL;DR: 本文提出REAP方法，证明在生成任务中专家剪枝优于专家合并，通过考虑路由器门值和专家激活范数实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 稀疏激活的专家混合模型参数量大导致内存开销严重，需要专家压缩。现有研究偏向专家合并，但本文发现这对生成任务不利。

Method: 提出Router-weighted Expert Activation Pruning (REAP)剪枝准则，综合考虑路由器门值和专家激活范数，避免功能子空间坍塌。

Result: 在20B到1T参数的SMoE模型上，REAP在生成基准测试中始终优于合并和其他剪枝方法，特别是在50%压缩率下。在代码生成和工具调用任务中，即使剪枝50%专家也能实现近乎无损压缩。

Conclusion: 专家剪枝是生成任务的更优策略，REAP方法通过避免功能子空间坍塌实现了高效的模型压缩。

Abstract: Sparsely-activated Mixture-of-Experts (SMoE) models offer efficient
pre-training and low latency but their large parameter counts create
significant memory overhead, motivating research into expert compression.
Contrary to recent findings favouring expert merging on discriminative
benchmarks, we demonstrate that expert pruning is a superior strategy for
generative tasks. We prove that merging introduces an irreducible error by
causing a "functional subspace collapse", due to the loss of the router's
independent, input-dependent control over experts. Leveraging this insight, we
propose Router-weighted Expert Activation Pruning (REAP), a novel pruning
criterion that considers both router gate-values and expert activation norms.
Across a diverse set of SMoE models ranging from 20B to 1T parameters, REAP
consistently outperforms merging and other pruning methods on generative
benchmarks, especially at 50% compression. Notably, our method achieves
near-lossless compression on code generation and tool-calling tasks with
Qwen3-Coder-480B and Kimi-K2, even after pruning 50% of experts.

</details>


### [67] [Conditional Clifford-Steerable CNNs with Complete Kernel Basis for PDE Modeling](https://arxiv.org/abs/2510.14007)
*Bálint László Szarvas,Maksim Zhdanov*

Main category: cs.LG

TL;DR: 本文提出了条件Clifford可操纵核，通过增强输入特征场的等变表示来解决CSCNNs核基不完备的问题，在多个PDE预测任务中表现出更好的表达能力。


<details>
  <summary>Details</summary>
Motivation: Clifford可操纵CNNs的核基不完备，限制了模型的表达能力，需要解决这个问题以提升等变网络的性能。

Method: 提出条件Clifford可操纵核，通过输入特征场计算等变表示来增强核函数，并推导了这些输入依赖核的等变性约束，通过隐式参数化高效求解。

Result: 在流体动力学和相对论电动力学等多个PDE预测任务中，该方法持续优于基线方法，证明了改进的表达能力。

Conclusion: 条件Clifford可操纵核有效解决了CSCNNs核基不完备的问题，显著提升了等变网络在PDE预测任务中的性能。

Abstract: Clifford-Steerable CNNs (CSCNNs) provide a unified framework that allows
incorporating equivariance to arbitrary pseudo-Euclidean groups, including
isometries of Euclidean space and Minkowski spacetime. In this work, we
demonstrate that the kernel basis of CSCNNs is not complete, thus limiting the
model expressivity. To address this issue, we propose Conditional
Clifford-Steerable Kernels, which augment the kernels with equivariant
representations computed from the input feature field. We derive the
equivariance constraint for these input-dependent kernels and show how it can
be solved efficiently via implicit parameterization. We empirically demonstrate
an improved expressivity of the resulting framework on multiple PDE forecasting
tasks, including fluid dynamics and relativistic electrodynamics, where our
method consistently outperforms baseline methods.

</details>


### [68] [Noise-Adaptive Layerwise Learning Rates: Accelerating Geometry-Aware Optimization for Deep Neural Network Training](https://arxiv.org/abs/2510.14009)
*Jie Hao,Xiaochuan Gong,Jie Xu,Zhengdao Wang,Mingrui Liu*

Main category: cs.LG

TL;DR: 提出了一种在几何感知优化算法基础上的噪声自适应分层学习率方案，通过估计梯度方差来动态调整各层学习率，显著加速了深度神经网络训练。


<details>
  <summary>Details</summary>
Motivation: 现有几何感知优化方法（如Muon）在相同范数组内的层使用固定学习率，但实际训练中各层的局部曲率存在异质性且动态变化，这可能导致训练效率低下。

Method: 在几何感知优化算法基础上，实时估计由所选LMO诱导的对偶范数中的梯度方差，并利用该方差为每个组内的层分配时变的噪声自适应分层学习率。

Result: 在LLaMA和GPT等transformer架构上的实验结果表明，该方法比现有最优优化器收敛更快。

Conclusion: 提出的噪声自适应分层学习率方案能够有效加速深度神经网络训练，理论分析表明该算法达到了尖锐的收敛率。

Abstract: Geometry-aware optimization algorithms, such as Muon, have achieved
remarkable success in training deep neural networks (DNNs). These methods
leverage the underlying geometry of DNNs by selecting appropriate norms for
different layers and updating parameters via norm-constrained linear
minimization oracles (LMOs). However, even within a group of layers associated
with the same norm, the local curvature can be heterogeneous across layers and
vary dynamically over the course of training. For example, recent work shows
that sharpness varies substantially across transformer layers and throughout
training, yet standard geometry-aware optimizers impose fixed learning rates to
layers within the same group, which may be inefficient for DNN training.
  In this paper, we introduce a noise-adaptive layerwise learning rate scheme
on top of geometry-aware optimization algorithms and substantially accelerate
DNN training compared to methods that use fixed learning rates within each
group. Our method estimates gradient variance in the dual norm induced by the
chosen LMO on the fly, and uses it to assign time-varying noise-adaptive
layerwise learning rates within each group. We provide a theoretical analysis
showing that our algorithm achieves a sharp convergence rate. Empirical results
on transformer architectures such as LLaMA and GPT demonstrate that our
approach achieves faster convergence than state-of-the-art optimizers.

</details>


### [69] [Context-Selective State Space Models: Feedback is All You Need](https://arxiv.org/abs/2510.14027)
*Riccardo Zattra,Giacomo Baggio,Umberto Casti,Augusto Ferrante,Francesco Ticozzi*

Main category: cs.LG

TL;DR: COFFEE模型是一种新型时变状态空间模型，通过引入状态反馈机制实现上下文相关的选择性，同时保持并行实现能力，在长序列建模任务中显著优于S6模块。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型存在二次复杂度问题且难以处理长序列依赖，而S6模块的选择性机制仅依赖当前输入，缺乏对历史上下文的利用。

Method: 提出COFFEE模型，通过状态反馈机制从内部状态计算选择性，使用高效参数化方法消除S6中的冗余，实现更紧凑可训练的架构。

Result: 在induction head任务中，COFFEE以两个数量级更少的参数和训练序列达到接近完美精度；在MNIST上仅用3585参数达到97%准确率，大幅超越S6。

Conclusion: 状态反馈是构建可扩展高效序列模型的关键机制，COFFEE展示了其在长序列依赖建模中的优越性。

Abstract: Transformers, powered by the attention mechanism, are the backbone of most
foundation models, yet they suffer from quadratic complexity and difficulties
in dealing with long-range dependencies in the input sequence. Recent work has
shown that state space models (SSMs) provide an efficient alternative, with the
S6 module at the core of the Mamba architecture achieving state-of-the-art
results on long-sequence benchmarks. In this paper, we introduce the COFFEE
(COntext From FEEdback) model, a novel time-varying SSM that incorporates state
feedback to enable context-dependent selectivity, while still allowing for
parallel implementation. Whereas the selectivity mechanism of S6 only depends
on the current input, COFFEE computes it from the internal state, which serves
as a compact representation of the sequence history. This shift allows the
model to regulate its dynamics based on accumulated context, improving its
ability to capture long-range dependencies. In addition to state feedback, we
employ an efficient model parametrization that removes redundancies present in
S6 and leads to a more compact and trainable formulation. On the induction head
task, COFFEE achieves near-perfect accuracy with two orders of magnitude fewer
parameters and training sequences compared to S6. On MNIST, COFFEE largely
outperforms S6 within the same architecture, reaching 97% accuracy with only
3585 parameters. These results showcase the role of state feedback as a key
mechanism for building scalable and efficient sequence models.

</details>


### [70] [CausalVerse: Benchmarking Causal Representation Learning with Configurable High-Fidelity Simulations](https://arxiv.org/abs/2510.14049)
*Guangyi Chen,Yunlong Deng,Peiyuan Zhu,Yan Li,Yifan Sheng,Zijian Li,Kun Zhang*

Main category: cs.LG

TL;DR: 提出了一个新的因果表示学习基准，使用高保真模拟视觉数据，包含20万张图像和300万视频帧，涵盖4个领域：静态图像生成、动态物理模拟、机器人操作和交通场景分析。


<details>
  <summary>Details</summary>
Motivation: 现有因果表示学习评估方法在真实性和评估精度之间存在困境，要么使用过于简化的合成数据集，要么依赖真实世界任务的下游性能，缺乏既有真实视觉复杂性又能获取真实因果生成过程的基准。

Method: 构建包含24个子场景的高保真模拟视觉数据集，提供对底层因果结构的灵活访问，允许用户修改或配置因果结构以符合CRL的假设要求。

Result: 创建了一个全面的测试平台，涵盖了从静态到动态、简单到复杂结构、单到多智能体交互的各种场景，弥合了严格评估和现实应用之间的差距。

Conclusion: 该基准为从业者和新手提供了选择或扩展适当CRL框架的经验见解，以解决可以从CRL视角受益的特定类型实际问题。

Abstract: Causal Representation Learning (CRL) aims to uncover the data-generating
process and identify the underlying causal variables and relations, whose
evaluation remains inherently challenging due to the requirement of known
ground-truth causal variables and causal structure. Existing evaluations often
rely on either simplistic synthetic datasets or downstream performance on
real-world tasks, generally suffering a dilemma between realism and evaluative
precision. In this paper, we introduce a new benchmark for CRL using
high-fidelity simulated visual data that retains both realistic visual
complexity and, more importantly, access to ground-truth causal generating
processes. The dataset comprises around 200 thousand images and 3 million video
frames across 24 sub-scenes in four domains: static image generation, dynamic
physical simulations, robotic manipulations, and traffic situation analysis.
These scenarios range from static to dynamic settings, simple to complex
structures, and single to multi-agent interactions, offering a comprehensive
testbed that hopefully bridges the gap between rigorous evaluation and
real-world applicability. In addition, we provide flexible access to the
underlying causal structures, allowing users to modify or configure them to
align with the required assumptions in CRL, such as available domain labels,
temporal dependencies, or intervention histories. Leveraging this benchmark, we
evaluated representative CRL methods across diverse paradigms and offered
empirical insights to assist practitioners and newcomers in choosing or
extending appropriate CRL frameworks to properly address specific types of real
problems that can benefit from the CRL perspective. Welcome to visit our:
Project page:https://causal-verse.github.io/,
Dataset:https://huggingface.co/CausalVerse.

</details>


### [71] [FedHFT: Efficient Federated Finetuning with Heterogeneous Edge Clients](https://arxiv.org/abs/2510.14054)
*Fatih Ilhan,Selim Furkan Tekin,Tiansheng Huang,Gaowen Liu,Ramana Kompella,Greg Eisenhauer,Yingyan Celine Lin,Calton Pu,Ling Liu*

Main category: cs.LG

TL;DR: FedHFT是一个高效的个性化联邦微调框架，通过混合掩码适配器处理资源异构性，使用双层优化方法处理非独立同分布数据，在保持数据本地化的同时实现高性能协作微调。


<details>
  <summary>Details</summary>
Motivation: 解决个性化自然语言理解应用中的两个主要挑战：(i) 由于数据保密性或隐私要求导致的有限和/或异构数据；(ii) 边缘设备等参与客户端之间计算资源的差异。

Method: 1. 引入混合掩码适配器处理参与客户端的资源异构性；2. 基于掩码个性化和客户端聚类的双层优化方法处理非独立同分布数据分布。

Result: 在数据和资源异构条件下，相比代表性异构联邦学习方法，在各种自然语言理解任务上表现出显著的性能和效率提升。

Conclusion: FedHFT框架有效解决了联邦学习中的数据异构和资源异构问题，实现了高效且个性化的预训练语言模型微调。

Abstract: Fine-tuning pre-trained large language models (LLMs) has become a common
practice for personalized natural language understanding (NLU) applications on
downstream tasks and domain-specific datasets. However, there are two main
challenges: (i) limited and/or heterogeneous data for fine-tuning due to
proprietary data confidentiality or privacy requirements, and (ii) varying
computation resources available across participating clients such as edge
devices. This paper presents FedHFT - an efficient and personalized federated
fine-tuning framework to address both challenges. First, we introduce a mixture
of masked adapters to handle resource heterogeneity across participating
clients, enabling high-performance collaborative fine-tuning of pre-trained
language model(s) across multiple clients in a distributed setting, while
keeping proprietary data local. Second, we introduce a bi-level optimization
approach to handle non-iid data distribution based on masked personalization
and client clustering. Extensive experiments demonstrate significant
performance and efficiency improvements over various natural language
understanding tasks under data and resource heterogeneity compared to
representative heterogeneous federated learning methods.

</details>


### [72] [On the expressivity of sparse maxout networks](https://arxiv.org/abs/2510.14068)
*Moritz Grillo,Tobias Hofmann*

Main category: cs.LG

TL;DR: 本文研究了稀疏maxout网络的表达能力，建立了网络函数与虚拟多面体之间的对偶关系，证明了深度层次结构的存在，并指出在固定入度约束下，宽度无法弥补深度不足的问题。


<details>
  <summary>Details</summary>
Motivation: 研究稀疏maxout网络（如卷积神经网络和图神经网络）的表达能力，探索其深度和宽度在表达能力中的作用。

Method: 建立稀疏maxout网络函数与虚拟多面体类之间的对偶关系，利用多面体几何分析网络表达能力，推导相关多面体维度的紧界。

Result: 证明了足够深的稀疏maxout网络具有通用性，但在固定入度约束下，如果深度不足，仅增加宽度无法补偿稀疏性带来的限制。

Conclusion: 稀疏maxout网络的表达能力与深度密切相关，在固定入度约束下，深度是决定网络表达能力的关键因素，宽度无法替代深度的作用。

Abstract: We study the expressivity of sparse maxout networks, where each neuron takes
a fixed number of inputs from the previous layer and employs a, possibly
multi-argument, maxout activation. This setting captures key characteristics of
convolutional or graph neural networks. We establish a duality between
functions computable by such networks and a class of virtual polytopes, linking
their geometry to questions of network expressivity. In particular, we derive a
tight bound on the dimension of the associated polytopes, which serves as the
central tool for our analysis. Building on this, we construct a sequence of
depth hierarchies. While sufficiently deep sparse maxout networks are
universal, we prove that if the required depth is not reached, width alone
cannot compensate for the sparsity of a fixed indegree constraint.

</details>


### [73] [Exploratory Causal Inference in SAEnce](https://arxiv.org/abs/2510.14073)
*Tommaso Mencattini,Riccardo Cadei,Francesco Locatello*

Main category: cs.LG

TL;DR: 提出Neural Effect Search方法，通过预训练基础模型和稀疏自编码器从试验数据中无监督发现未知因果效应，解决多重检验和效应纠缠问题。


<details>
  <summary>Details</summary>
Motivation: 传统随机对照试验依赖人工假设且成本高昂，限制了大规模因果效应估计，可能遗漏重要但未被假设的效应。

Method: 使用预训练基础模型将非结构化数据转化为有意义的表示，通过稀疏自编码器进行解释，并采用渐进分层的递归程序解决多重检验和效应纠缠问题。

Result: 在半合成实验中验证了算法的鲁棒性，并在实验生态学背景下首次在真实世界科学试验中成功实现无监督因果效应识别。

Conclusion: 该方法能够直接从数据中发现未知的因果效应，突破了传统假设驱动的限制，为大规模因果发现提供了新途径。

Abstract: Randomized Controlled Trials are one of the pillars of science; nevertheless,
they rely on hand-crafted hypotheses and expensive analysis. Such constraints
prevent causal effect estimation at scale, potentially anchoring on popular yet
incomplete hypotheses. We propose to discover the unknown effects of a
treatment directly from data. For this, we turn unstructured data from a trial
into meaningful representations via pretrained foundation models and interpret
them via a sparse autoencoder. However, discovering significant causal effects
at the neural level is not trivial due to multiple-testing issues and effects
entanglement. To address these challenges, we introduce Neural Effect Search, a
novel recursive procedure solving both issues by progressive stratification.
After assessing the robustness of our algorithm on semi-synthetic experiments,
we showcase, in the context of experimental ecology, the first successful
unsupervised causal effect identification on a real-world scientific trial.

</details>


### [74] [Neural Network approximation power on homogeneous and heterogeneous reaction-diffusion equations](https://arxiv.org/abs/2510.14094)
*Haotian Feng*

Main category: cs.LG

TL;DR: 该论文从理论上分析了神经网络对反应-扩散方程的逼近能力，证明两层神经网络可逼近一维方程，三层神经网络可逼近二维方程，为基于神经网络的微分方程求解器提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络在求解微分方程中的广泛应用，其为何能有效逼近此类解的理论基础仍不够充分，需要深入探索神经网络对反应-扩散系统的逼近能力。

Method: 基于通用逼近定理，构建理论框架分析神经网络对一维和二维反应-扩散方程在均匀和非均匀介质中的逼近能力。

Result: 证明两层神经网络足以逼近一维反应-扩散方程，三层神经网络可逼近二维反应-扩散方程，该理论框架可进一步扩展到椭圆和抛物型方程。

Conclusion: 这项工作突显了神经网络在逼近反应-扩散方程及相关偏微分方程解方面的表达能力，为神经网络基微分方程求解器提供了坚实的理论基础。

Abstract: Reaction-diffusion systems represent one of the most fundamental formulations
used to describe a wide range of physical, chemical, and biological processes.
With the increasing adoption of neural networks, recent research has focused on
solving differential equations using machine learning techniques. However, the
theoretical foundation explaining why neural networks can effectively
approximate such solutions remains insufficiently explored.
  This paper provides a theoretical analysis of the approximation power of
neural networks for one- and two-dimensional reaction-diffusion equations in
both homogeneous and heterogeneous media. Building upon the universal
approximation theorem, we demonstrate that a two-layer neural network can
approximate the one-dimensional reaction-diffusion equation, while a
three-layer neural network can approximate its two-dimensional counterpart. The
theoretical framework presented here can be further extended to elliptic and
parabolic equations.
  Overall, this work highlights the expressive power of neural networks in
approximating solutions to reaction-diffusion equations and related PDEs,
providing a theoretical foundation for neural network-based differential
equation solvers.

</details>


### [75] [Unlocking Out-of-Distribution Generalization in Transformers via Recursive Latent Space Reasoning](https://arxiv.org/abs/2510.14095)
*Awni Altabaa,Siyu Chen,John Lafferty,Zhuoran Yang*

Main category: cs.LG

TL;DR: 本文研究了Transformer网络在超出训练分布外的泛化能力，通过模块化算术计算图任务作为测试平台，提出了四种架构机制来增强OOD泛化能力。


<details>
  <summary>Details</summary>
Motivation: 系统性、组合性泛化超越训练分布仍然是机器学习的核心挑战，也是现代语言模型推理能力的关键瓶颈。

Method: 引入了四种架构机制：(i)输入自适应循环；(ii)算法监督；(iii)通过离散瓶颈的锚定潜在表示；(iv)显式纠错机制。

Result: 这些机制共同形成了一种在Transformer网络中实现原生和可扩展潜在空间推理的架构方法，具有强大的算法泛化能力。

Conclusion: 通过机制解释性分析揭示了这些机制如何产生稳健的OOD泛化能力。

Abstract: Systematic, compositional generalization beyond the training distribution
remains a core challenge in machine learning -- and a critical bottleneck for
the emergent reasoning abilities of modern language models. This work
investigates out-of-distribution (OOD) generalization in Transformer networks
using a GSM8K-style modular arithmetic on computational graphs task as a
testbed. We introduce and explore a set of four architectural mechanisms aimed
at enhancing OOD generalization: (i) input-adaptive recurrence; (ii)
algorithmic supervision; (iii) anchored latent representations via a discrete
bottleneck; and (iv) an explicit error-correction mechanism. Collectively,
these mechanisms yield an architectural approach for native and scalable latent
space reasoning in Transformer networks with robust algorithmic generalization
capabilities. We complement these empirical results with a detailed mechanistic
interpretability analysis that reveals how these mechanisms give rise to robust
OOD generalization abilities.

</details>


### [76] [TENDE: Transfer Entropy Neural Diffusion Estimation](https://arxiv.org/abs/2510.14096)
*Simon Pedro Galeano Munoz,Mustapha Bounoua,Giulio Franzese,Pietro Michiardi,Maurizio Filippone*

Main category: cs.LG

TL;DR: 提出了TENDE方法，使用基于分数的扩散模型来估计转移熵，解决了现有方法维度灾难、分布假设限制和数据量需求大的问题。


<details>
  <summary>Details</summary>
Motivation: 现有转移熵估计方法存在维度灾难、需要限制性分布假设或需要大量数据才能可靠收敛的问题，限制了其在实际应用中的有效性。

Method: TENDE方法利用基于分数的扩散模型，通过学习相关条件分布的分值函数来估计条件互信息，从而计算转移熵。该方法对底层数据生成过程假设最少。

Result: 在合成基准测试和真实数据上，TENDE相比现有神经估计器和其他最先进方法表现出更高的准确性和鲁棒性。

Conclusion: TENDE提供了一种灵活、可扩展的转移熵估计方法，克服了现有方法的局限性，在多个应用领域具有广泛潜力。

Abstract: Transfer entropy measures directed information flow in time series, and it
has become a fundamental quantity in applications spanning neuroscience,
finance, and complex systems analysis. However, existing estimation methods
suffer from the curse of dimensionality, require restrictive distributional
assumptions, or need exponentially large datasets for reliable convergence. We
address these limitations in the literature by proposing TENDE (Transfer
Entropy Neural Diffusion Estimation), a novel approach that leverages
score-based diffusion models to estimate transfer entropy through conditional
mutual information. By learning score functions of the relevant conditional
distributions, TENDE provides flexible, scalable estimation while making
minimal assumptions about the underlying data-generating process. We
demonstrate superior accuracy and robustness compared to existing neural
estimators and other state-of-the-art approaches across synthetic benchmarks
and real data.

</details>


### [77] [Near-Optimal Regret-Queue Length Tradeoff in Online Learning for Two-Sided Markets](https://arxiv.org/abs/2510.14097)
*Zixian Yang,Sushil Mahavir Varma,Lei Ying*

Main category: cs.LG

TL;DR: 提出了一种在线学习定价策略，用于双边市场中的定价和匹配问题，在需求供给曲线未知的情况下实现近最优收益，同时控制队列长度。


<details>
  <summary>Details</summary>
Motivation: 在双边市场中，平台需要同时考虑定价和匹配策略来最大化利润，但实际中需求供给曲线往往未知，需要设计在线学习算法来处理这种不确定性。

Method: 设计了一种新颖的在线学习定价策略，包含动态组件优化遗憾与队列长度的权衡，以及概率组件解决快速学习与队列控制之间的张力。

Result: 证明了三个性能指标之间的权衡关系：$\tilde{O}(T^{1-\gamma})$遗憾、$\tilde{O}(T^{\gamma/2})$平均队列长度和$\tilde{O}(T^{\gamma})$最大队列长度，其中$\gamma \\in (0, 1/6]$，显著优于现有结果。

Conclusion: 所提出的策略在允许的$\gamma$范围内，在遗憾与平均队列长度的权衡方面达到对数因子内的最优性，与已知需求供给曲线的最优策略相匹配。

Abstract: We study a two-sided market, wherein, price-sensitive heterogeneous customers
and servers arrive and join their respective queues. A compatible
customer-server pair can then be matched by the platform, at which point, they
leave the system. Our objective is to design pricing and matching algorithms
that maximize the platform's profit, while maintaining reasonable queue
lengths. As the demand and supply curves governing the price-dependent arrival
rates may not be known in practice, we design a novel online-learning-based
pricing policy and establish its near-optimality. In particular, we prove a
tradeoff among three performance metrics: $\tilde{O}(T^{1-\gamma})$ regret,
$\tilde{O}(T^{\gamma/2})$ average queue length, and $\tilde{O}(T^{\gamma})$
maximum queue length for $\gamma \in (0, 1/6]$, significantly improving over
existing results [1]. Moreover, barring the permissible range of $\gamma$, we
show that this trade-off between regret and average queue length is optimal up
to logarithmic factors under a class of policies, matching the optimal one as
in [2] which assumes the demand and supply curves to be known. Our proposed
policy has two noteworthy features: a dynamic component that optimizes the
tradeoff between low regret and small queue lengths; and a probabilistic
component that resolves the tension between obtaining useful samples for fast
learning and maintaining small queue lengths.

</details>


### [78] [Briding Diffusion Posterior Sampling and Monte Carlo methods: a survey](https://arxiv.org/abs/2510.14114)
*Yazid Janati,Alain Durmus,Jimmy Olsson,Eric Moulines*

Main category: cs.LG

TL;DR: 这篇综述论文系统回顾了利用预训练扩散模型和蒙特卡洛方法解决贝叶斯逆问题的方法，无需额外训练，主要通过扭曲扩散过程中的中间分布来引导模拟朝向后验分布。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在复杂分布合成中表现出色，最近显示出作为先验解决贝叶斯逆问题的巨大潜力，这促使研究者探索如何有效利用预训练扩散模型。

Method: 主要采用扭曲机制处理扩散过程中的中间分布，结合各种蒙特卡洛方法从这些扭曲分布中采样。

Result: 展示了预训练扩散模型与蒙特卡洛方法结合能够有效解决贝叶斯逆问题，无需额外训练。

Conclusion: 扩散模型作为先验与蒙特卡洛方法结合为解决贝叶斯逆问题提供了有效的无训练解决方案。

Abstract: Diffusion models enable the synthesis of highly accurate samples from complex
distributions and have become foundational in generative modeling. Recently,
they have demonstrated significant potential for solving Bayesian inverse
problems by serving as priors. This review offers a comprehensive overview of
current methods that leverage \emph{pre-trained} diffusion models alongside
Monte Carlo methods to address Bayesian inverse problems without requiring
additional training. We show that these methods primarily employ a
\emph{twisting} mechanism for the intermediate distributions within the
diffusion process, guiding the simulations toward the posterior distribution.
We describe how various Monte Carlo methods are then used to aid in sampling
from these twisted distributions.

</details>


### [79] [Neural Network-enabled Domain-consistent Robust Optimisation for Global CO$_2$ Reduction Potential of Gas Power Plants](https://arxiv.org/abs/2510.14125)
*Waqar Muhammad Ashraf,Talha Ansar,Abdulelah S. Alshehri,Peipei Chen,Ramit Debnath,Vivek Dua*

Main category: cs.LG

TL;DR: 提出了一个神经网络驱动的鲁棒优化框架，将数据驱动领域作为约束集成到非线性规划技术中，解决参数化神经网络模型与优化求解器交互产生的领域不一致解问题。


<details>
  <summary>Details</summary>
Motivation: 解决参数化神经网络模型与优化求解器交互时产生的领域不一致解问题，这在现有研究中常被忽视。

Method: 开发了神经网络驱动的鲁棒优化框架，将数据驱动领域作为约束集成到非线性规划技术中。

Result: 在1180 MW联合循环燃气电厂应用中，该框架提供领域一致的鲁棒最优解，验证了能源效率平均提高0.76个百分点。将该效率增益扩展到全球燃气电厂，估计每年可减少2600万吨CO₂排放。

Conclusion: 这些结果强调了机器学习在提供近期、可扩展的全球气候行动脱碳路径中的协同作用。

Abstract: We introduce a neural network-driven robust optimisation framework that
integrates data-driven domain as a constraint into the nonlinear programming
technique, addressing the overlooked issue of domain-inconsistent solutions
arising from the interaction of parametrised neural network models with
optimisation solvers. Applied to a 1180 MW capacity combined cycle gas power
plant, our framework delivers domain-consistent robust optimal solutions that
achieve a verified 0.76 percentage point mean improvement in energy efficiency.
For the first time, scaling this efficiency gain to the global fleet of gas
power plants, we estimate an annual 26 Mt reduction potential in CO$_2$ (with
10.6 Mt in Asia, 9.0 Mt in the Americas, and 4.5 Mt in Europe). These results
underscore the synergetic role of machine learning in delivering near-term,
scalable decarbonisation pathways for global climate action.

</details>


### [80] [Demystifying the Mechanisms Behind Emergent Exploration in Goal-conditioned RL](https://arxiv.org/abs/2510.14129)
*Mahsa Bastankhah,Grace Liu,Dilip Arumugam,Thomas L. Griffiths,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 该论文首次揭示了无监督强化学习中涌现探索的机制，通过理论分析和实验验证了SGCRL算法如何通过学习表示来塑造隐式奖励，促进探索和利用的平衡。


<details>
  <summary>Details</summary>
Motivation: 理解无监督强化学习中出现探索行为的机制，特别是SGCRL算法在没有外部奖励或课程的情况下解决长时程目标达成任务的原理。

Method: 结合理论分析SGCRL的目标函数和受控实验，研究算法如何通过学习的表示来塑造隐式奖励，并分析探索动态的来源。

Result: SGCRL通过学习的状态空间低秩表示自动修改奖励景观，在到达目标前促进探索，到达后促进利用，这种探索动态源于表示学习而非神经网络函数逼近。

Conclusion: 对SGCRL机制的理解使其能够适应安全感知的探索，为无监督强化学习的探索机制提供了理论解释和实践改进方向。

Abstract: In this work, we take a first step toward elucidating the mechanisms behind
emergent exploration in unsupervised reinforcement learning. We study
Single-Goal Contrastive Reinforcement Learning (SGCRL), a self-supervised
algorithm capable of solving challenging long-horizon goal-reaching tasks
without external rewards or curricula. We combine theoretical analysis of the
algorithm's objective function with controlled experiments to understand what
drives its exploration. We show that SGCRL maximizes implicit rewards shaped by
its learned representations. These representations automatically modify the
reward landscape to promote exploration before reaching the goal and
exploitation thereafter. Our experiments also demonstrate that these
exploration dynamics arise from learning low-rank representations of the state
space rather than from neural network function approximation. Our improved
understanding enables us to adapt SGCRL to perform safety-aware exploration.

</details>


### [81] [Learning Wireless Interference Patterns: Decoupled GNN for Throughput Prediction in Heterogeneous Multi-Hop p-CSMA Networks](https://arxiv.org/abs/2510.14137)
*Faezeh Dehghan Tarzjani,Bhaskar Krishnamachari*

Main category: cs.LG

TL;DR: 提出D-GCN架构，通过解耦节点自身传输概率与邻居干扰效应，显著提升异构多跳无线网络中p-persistent CSMA协议饱和吞吐量的预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统简化模型在稀疏拓扑中会低估吞吐量48-62%，而精确马尔可夫链分析计算复杂度呈指数增长，无法用于大型网络。现有GNN方法因对称归一化混淆了直接干扰与高阶级联效应，预测误差高达63.94%。

Method: 设计解耦图卷积网络(D-GCN)，用可学习注意力机制替代均值聚合，显式分离节点自身传输概率处理与邻居干扰效应，捕获复杂的多跳干扰模式。

Result: D-GCN达到3.3%的归一化平均绝对误差，优于强基线方法，在精确分析方法计算不可行时仍保持可处理性，基于梯度的网络优化能达到理论最优值的1%以内。

Conclusion: D-GCN通过解耦架构有效解决了GNN在无线网络吞吐量预测中的关键挑战，为大规模网络性能分析提供了可扩展且准确的解决方案。

Abstract: The p-persistent CSMA protocol is central to random-access MAC analysis, but
predicting saturation throughput in heterogeneous multi-hop wireless networks
remains a hard problem. Simplified models that assume a single, shared
interference domain can underestimate throughput by 48--62\% in sparse
topologies. Exact Markov-chain analyses are accurate but scale exponentially in
computation time, making them impractical for large networks. These
computational barriers motivate structural machine learning approaches like
GNNs for scalable throughput prediction in general network topologies. Yet
off-the-shelf GNNs struggle here: a standard GCN yields 63.94\% normalized mean
absolute error (NMAE) on heterogeneous networks because symmetric normalization
conflates a node's direct interference with higher-order, cascading effects
that pertain to how interference propagates over the network graph.
  Building on these insights, we propose the Decoupled Graph Convolutional
Network (D-GCN), a novel architecture that explicitly separates processing of a
node's own transmission probability from neighbor interference effects. D-GCN
replaces mean aggregation with learnable attention, yielding interpretable,
per-neighbor contribution weights while capturing complex multihop interference
patterns. D-GCN attains 3.3\% NMAE, outperforms strong baselines, remains
tractable even when exact analytical methods become computationally infeasible,
and enables gradient-based network optimization that achieves within 1\% of
theoretical optima.

</details>


### [82] [Inferred global dense residue transition graphs from primary structure sequences enable protein interaction prediction via directed graph convolutional neural networks](https://arxiv.org/abs/2510.14139)
*Islam Akef Ebeid,Haoteng Tang,Pengfei Gu*

Main category: cs.LG

TL;DR: 提出ProtGram-DirectGCN框架，通过层次化n-gram图和定向图卷积网络进行蛋白质-蛋白质相互作用预测，在计算效率和数据有限情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质相互作用预测方法依赖蛋白质语言模型的序列嵌入或图神经网络处理3D结构，计算复杂度高。本研究探索计算强度较低的替代方案。

Method: 两阶段图表示学习框架：1) ProtGram将蛋白质一级结构建模为层次化n-gram图，边权重由残基转移概率定义；2) DirectGCN定向图卷积网络，通过路径特定变换和可学习门控机制处理信息。

Result: DirectGCN在标准节点分类基准测试中表现与现有方法相当，特别擅长处理复杂定向图和密集异质结构。ProtGram-DirectGCN框架在PPI预测中展现出强大预测能力，即使在训练数据有限时也保持稳健性能。

Conclusion: ProtGram-DirectGCN框架为蛋白质相互作用预测提供了一种计算效率高且性能优异的解决方案，特别适合处理复杂定向图结构。

Abstract: Introduction Accurate prediction of protein-protein interactions (PPIs) is
crucial for understanding cellular functions and advancing drug development.
Existing in-silico methods use direct sequence embeddings from Protein Language
Models (PLMs). Others use Graph Neural Networks (GNNs) for 3D protein
structures. This study explores less computationally intensive alternatives. We
introduce a novel framework for downstream PPI prediction through link
prediction. Methods We introduce a two-stage graph representation learning
framework, ProtGram-DirectGCN. First, we developed ProtGram. This approach
models a protein's primary structure as a hierarchy of globally inferred n-gram
graphs. In these graphs, residue transition probabilities define edge weights.
Each edge connects a pair of residues in a directed graph. The probabilities
are aggregated from a large corpus of sequences. Second, we propose DirectGCN,
a custom directed graph convolutional neural network. This model features a
unique convolutional layer. It processes information through separate
path-specific transformations: incoming, outgoing, and undirected. A shared
transformation is also applied. These paths are combined via a learnable gating
mechanism. We apply DirectGCN to ProtGram graphs to learn residue-level
embeddings. These embeddings are pooled via attention to generate protein-level
embeddings for prediction. Results We first established the efficacy of
DirectGCN on standard node classification benchmarks. Its performance matches
established methods on general datasets. The model excels at complex, directed
graphs with dense, heterophilic structures. When applied to PPI prediction, the
full ProtGram-DirectGCN framework delivers robust predictive power. This strong
performance holds even with limited training data.

</details>


### [83] [On Evaluating Loss Functions for Stock Ranking: An Empirical Analysis With Transformer Model](https://arxiv.org/abs/2510.14156)
*Jan Kwiatkowski,Jarosław A. Chudziak*

Main category: cs.LG

TL;DR: 本文系统评估了多种先进的损失函数（点对点、成对、列表式）在Transformer模型中对股票收益排序能力的影响，为基于排名的投资组合选择提供实用指导。


<details>
  <summary>Details</summary>
Motivation: 量化交易策略依赖准确的股票排序来识别盈利投资。传统损失函数追求简单预测精度，但未能直接教导模型学习正确的股票收益排序顺序。虽然信息检索等领域存在许多先进排序损失函数，但尚未有系统比较它们在金融收益排序中的表现，特别是在现代Transformer模型中的应用。

Method: 在S&P 500数据上，系统评估了包括点对点、成对、列表式在内的多种先进损失函数，用于每日股票收益预测，以促进基于排名的投资组合选择。重点评估每种损失函数如何影响模型识别资产间盈利相对排序的能力。

Result: 研究提供了一个全面的基准，揭示了不同损失函数如何影响模型学习对投资组合选择至关重要的横截面和时间模式的能力。

Conclusion: 该研究为优化基于排名的交易策略提供了实用指导，填补了在金融收益排序领域损失函数比较的空白。

Abstract: Quantitative trading strategies rely on accurately ranking stocks to identify
profitable investments. Effective portfolio management requires models that can
reliably order future stock returns. Transformer models are promising for
understanding financial time series, but how different training loss functions
affect their ability to rank stocks well is not yet fully understood. Financial
markets are challenging due to their changing nature and complex relationships
between stocks. Standard loss functions, which aim for simple prediction
accuracy, often aren't enough. They don't directly teach models to learn the
correct order of stock returns. While many advanced ranking losses exist from
fields such as information retrieval, there hasn't been a thorough comparison
to see how well they work for ranking financial returns, especially when used
with modern Transformer models for stock selection. This paper addresses this
gap by systematically evaluating a diverse set of advanced loss functions
including pointwise, pairwise, listwise for daily stock return forecasting to
facilitate rank-based portfolio selection on S&P 500 data. We focus on
assessing how each loss function influences the model's ability to discern
profitable relative orderings among assets. Our research contributes a
comprehensive benchmark revealing how different loss functions impact a model's
ability to learn cross-sectional and temporal patterns crucial for portfolio
selection, thereby offering practical guidance for optimizing ranking-based
trading strategies.

</details>


### [84] [Data Understanding Survey: Pursuing Improved Dataset Characterization Via Tensor-based Methods](https://arxiv.org/abs/2510.14161)
*Matthew D. Merris,Tim Andersen*

Main category: cs.LG

TL;DR: 该论文调查了传统数据集表征方法的局限性，并探讨了张量方法作为更强大替代方案的潜力，能够提供更好的可解释性和可操作智能。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习数据集表征方法（统计、结构、基于模型的分析）往往无法提供深入理解和解释性所需的洞察力，限制了创新和可解释性。

Method: 通过调查现有最先进的数据分析技术，分析其局限性，并讨论各种基于张量的方法，通过实例说明张量方法如何揭示细微的数据特征。

Result: 张量方法能够揭示传统方法无法发现的数据细微特征，提供增强的可解释性和可操作智能。

Conclusion: 提倡采用基于张量的数据集表征方法，有望在理解复杂数据集方面实现重大进步，为智能、可解释的数据驱动发现铺平道路。

Abstract: In the evolving domains of Machine Learning and Data Analytics, existing
dataset characterization methods such as statistical, structural, and
model-based analyses often fail to deliver the deep understanding and insights
essential for innovation and explainability. This work surveys the current
state-of-the-art conventional data analytic techniques and examines their
limitations, and discusses a variety of tensor-based methods and how these may
provide a more robust alternative to traditional statistical, structural, and
model-based dataset characterization techniques. Through examples, we
illustrate how tensor methods unveil nuanced data characteristics, offering
enhanced interpretability and actionable intelligence. We advocate for the
adoption of tensor-based characterization, promising a leap forward in
understanding complex datasets and paving the way for intelligent, explainable
data-driven discoveries.

</details>


### [85] [Towards Reversible Model Merging For Low-rank Weights](https://arxiv.org/abs/2510.14163)
*Mohammadsajad Alipour,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: 提出可逆模型合并(RMM)方法，通过构建紧凑基空间来恢复原始任务特定模型，而非传统合并为单一权重集，显著提升低秩压缩模型的性能保持。


<details>
  <summary>Details</summary>
Motivation: 传统合并方法应用于低秩权重时会导致严重的性能下降，需要一种新方法来处理低秩适应(LoRA)或SVD压缩后的模型合并问题。

Method: 构建重建能力强的模型空间，通过线性组合恢复原始任务特定模型，提供闭式解选择最优权重基和任务特定系数。

Result: RMM在多样化数据集和模型规模上持续优于现有合并方法，显著保持低秩压缩模型的性能。

Conclusion: 将合并重新定义为生成重建能力强的模型空间而非单一合并模型，允许在需要时恢复到各个特定任务模型，认识到没有合并模型能始终优于专门化模型。

Abstract: Model merging aims to combine multiple fine-tuned models into a single set of
weights that performs well across all source tasks. While prior work has shown
that merging can approximate the performance of individual fine-tuned models
for each task, it largely overlooks scenarios where models are compressed into
low-rank representations, either through low-rank adaptation (LoRA) or
post-training singular value decomposition (SVD). We first demonstrate that
applying conventional merging methods to low-rank weights leads to severe
performance degradation in the merged model. Motivated by this phenomenon, we
propose a fundamentally different approach: instead of collapsing all adapters
into one set of weights, we construct a compact basis (e.g., an equivalent of
holding two or more models) from which original task-specific models can be
recovered via linear combination. This reframes merging as generating a
reconstruction-capable model space rather than producing a single merged model.
Crucially, this allows us to ``revert'' to each individual model when needed,
recognizing that no merged model can consistently outperform one specialized
for its task. Building on this insight, we introduce our method, Reversible
Model Merging (RMM), an efficient, data-free, and flexible method that provides
a closed-form solution for selecting the optimal basis of model weights and
task-specific coefficients for linear combination. Extensive experiments across
diverse datasets and model scales demonstrate that RMM consistently outperforms
existing merging approaches, preserving the performance of low-rank compressed
models by a significant margin.

</details>


### [86] [Optimal Control Theoretic Neural Optimizer: From Backpropagation to Dynamic Programming](https://arxiv.org/abs/2510.14168)
*Guan-Horng Liu,Tianrong Chen,Evangelos A. Theodorou*

Main category: cs.LG

TL;DR: 该论文提出了一种基于最优控制理论的神经网络优化器OCNOpt，通过将深度神经网络视为动力系统，利用动态规划与反向传播的算法相似性，开发了探索贝尔曼方程高阶展开的新优化方法。


<details>
  <summary>Details</summary>
Motivation: 观察到反向传播算法与动力系统最优性条件（动态规划）之间的显著算法相似性，希望通过整合这种联系来开发基于最优控制理论的新优化方法。

Method: 将深度神经网络解释为动力系统，在反向传播具有变分结构的基础上，求解一阶展开的近似动态规划，探索贝尔曼方程的高阶展开，开发OCNOpt优化器。

Result: OCNOpt在鲁棒性和效率方面优于现有方法，同时保持可控的计算复杂度，支持层间反馈策略、博弈论应用和连续时间模型的高阶训练。

Conclusion: OCNOpt为基于动力系统和最优控制理论的原则性算法设计开辟了新途径，展示了将控制理论应用于神经网络优化的潜力。

Abstract: Optimization of deep neural networks (DNNs) has been a driving force in the
advancement of modern machine learning and artificial intelligence. With DNNs
characterized by a prolonged sequence of nonlinear propagation, determining
their optimal parameters given an objective naturally fits within the framework
of Optimal Control Programming. Such an interpretation of DNNs as dynamical
systems has proven crucial in offering a theoretical foundation for principled
analysis from numerical equations to physics. In parallel to these theoretical
pursuits, this paper focuses on an algorithmic perspective. Our motivated
observation is the striking algorithmic resemblance between the Backpropagation
algorithm for computing gradients in DNNs and the optimality conditions for
dynamical systems, expressed through another backward process known as dynamic
programming. Consolidating this connection, where Backpropagation admits a
variational structure, solving an approximate dynamic programming up to the
first-order expansion leads to a new class of optimization methods exploring
higher-order expansions of the Bellman equation. The resulting optimizer,
termed Optimal Control Theoretic Neural Optimizer (OCNOpt), enables rich
algorithmic opportunities, including layer-wise feedback policies,
game-theoretic applications, and higher-order training of continuous-time
models such as Neural ODEs. Extensive experiments demonstrate that OCNOpt
improves upon existing methods in robustness and efficiency while maintaining
manageable computational complexity, paving new avenues for principled
algorithmic design grounded in dynamical systems and optimal control theory.

</details>


### [87] [MAFA: A Multi-Agent Framework for Enterprise-Scale Annotation with Configurable Task Adaptation](https://arxiv.org/abs/2510.14184)
*Mahmood Hegazy,Aaron Rodrigues,Azzam Naeem*

Main category: cs.LG

TL;DR: MAFA是一个生产级多智能体标注框架，通过可配置的多智能体协作解决金融服务中的大规模标注积压问题，在摩根大通部署后消除了100万条话语积压，平均与人工标注者达成86%一致，每年节省5000多小时人工标注工作。


<details>
  <summary>Details</summary>
Motivation: 解决金融服务中数百万客户话语需要准确分类的标注积压挑战，传统方法效率低下且成本高昂。

Method: 结合专门化智能体与结构化推理和基于评判者的共识机制，支持通过配置而非代码更改来定义自定义标注类型，实现动态任务适应。

Result: 在摩根大通部署后消除了100万条话语积压，平均与人工标注者达成86%一致，每年节省5000多小时人工标注工作，标注置信度分类显示85%高置信度、10%中置信度、5%低置信度。

Conclusion: MAFA在多数据集和语言上均优于传统和单智能体标注基线，在内部意图分类数据集上Top-1准确率提高13.8%，Top-5准确率提高15.1%，F1分数提高16.9%，为面临类似标注挑战的组织提供了实用蓝图。

Abstract: We present MAFA (Multi-Agent Framework for Annotation), a production-deployed
system that transforms enterprise-scale annotation workflows through
configurable multi-agent collaboration. Addressing the critical challenge of
annotation backlogs in financial services, where millions of customer
utterances require accurate categorization, MAFA combines specialized agents
with structured reasoning and a judge-based consensus mechanism. Our framework
uniquely supports dynamic task adaptation, allowing organizations to define
custom annotation types (FAQs, intents, entities, or domain-specific
categories) through configuration rather than code changes. Deployed at JP
Morgan Chase, MAFA has eliminated a 1 million utterance backlog while
achieving, on average, 86% agreement with human annotators, annually saving
over 5,000 hours of manual annotation work. The system processes utterances
with annotation confidence classifications, which are typically 85% high, 10%
medium, and 5% low across all datasets we tested. This enables human annotators
to focus exclusively on ambiguous and low-coverage cases. We demonstrate MAFA's
effectiveness across multiple datasets and languages, showing consistent
improvements over traditional and single-agent annotation baselines: 13.8%
higher Top-1 accuracy, 15.1% improvement in Top-5 accuracy, and 16.9% better F1
in our internal intent classification dataset and similar gains on public
benchmarks. This work bridges the gap between theoretical multi-agent systems
and practical enterprise deployment, providing a blueprint for organizations
facing similar annotation challenges.

</details>


### [88] [Contrastive Diffusion Alignment: Learning Structured Latents for Controllable Generation](https://arxiv.org/abs/2510.14190)
*Ruchi Sandilya,Sumaira Perez,Charles Lynch,Lindsay Victoria,Benjamin Zebley,Derrick Matthew Buchanan,Mahendra T. Bhati,Nolan Williams,Timothy J. Spellman,Faith M. Gunning,Conor Liston,Logan Grosenick*

Main category: cs.LG

TL;DR: ConDA框架通过对比学习在扩散嵌入中组织潜在空间，使潜在几何与系统动力学对齐，从而支持非线性轨迹遍历和可控生成。


<details>
  <summary>Details</summary>
Motivation: 扩散模型擅长生成，但其潜在空间未明确组织以实现可解释控制。利用对比学习能恢复更解耦和结构化的表示，从而组织扩散潜在空间。

Method: 在扩散嵌入中应用对比学习，使潜在几何与系统动力学对齐，支持非线性轨迹遍历，包括插值、外推和可控生成。

Result: 在流体动力学、神经钙成像、治疗性神经刺激和面部表情等基准测试中，ConDA生成的可解释潜在表示在可控性上优于线性遍历和基于条件的方法。

Conclusion: 扩散潜在编码包含与动力学相关的结构，但利用此结构需要潜在组织并沿潜在流形进行遍历。

Abstract: Diffusion models excel at generation, but their latent spaces are not
explicitly organized for interpretable control. We introduce ConDA (Contrastive
Diffusion Alignment), a framework that applies contrastive learning within
diffusion embeddings to align latent geometry with system dynamics. Motivated
by recent advances showing that contrastive objectives can recover more
disentangled and structured representations, ConDA organizes diffusion latents
such that traversal directions reflect underlying dynamical factors. Within
this contrastively structured space, ConDA enables nonlinear trajectory
traversal that supports faithful interpolation, extrapolation, and controllable
generation. Across benchmarks in fluid dynamics, neural calcium imaging,
therapeutic neurostimulation, and facial expression, ConDA produces
interpretable latent representations with improved controllability compared to
linear traversals and conditioning-based baselines. These results suggest that
diffusion latents encode dynamics-relevant structure, but exploiting this
structure requires latent organization and traversal along the latent manifold.

</details>


### [89] [Incentive-Based Federated Learning](https://arxiv.org/abs/2510.14208)
*Chanuka A. S. Hewa Kaluannakkage,Rajkumar Buyya*

Main category: cs.LG

TL;DR: 本章分析了联邦学习中的激励机制设计挑战，探讨了如何应用经济学和博弈论概念以及区块链、深度强化学习等技术解决方案，提出了涵盖集中式和去中心化架构的综合分类法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能保护数据隐私，但面临参与困境——参与者不愿无偿贡献或可能搭便车，这限制了其实际应用。需要设计有效的激励机制来促进参与。

Method: 应用经济学和博弈论概念，结合区块链和深度强化学习等技术解决方案，建立涵盖集中式和去中心化架构的综合分类法，从应用角度分析工业场景。

Result: 提出了一个全面的激励机制分类法，展示了在医疗、智能基础设施、车联网和区块链系统等领域的应用前景，识别了现有解决方案和剩余挑战。

Conclusion: 精心设计的激励机制不是可选功能，而是联邦学习实际成功的关键组成部分，但仍需解决可持续性、公平性和鲁棒性等重大挑战。

Abstract: Federated learning promises to revolutionize machine learning by enabling
collaborative model training without compromising data privacy. However,
practical adaptability can be limited by critical factors, such as the
participation dilemma. Participating entities are often unwilling to contribute
to a learning system unless they receive some benefits, or they may pretend to
participate and free-ride on others. This chapter identifies the fundamental
challenges in designing incentive mechanisms for federated learning systems. It
examines how foundational concepts from economics and game theory can be
applied to federated learning, alongside technology-driven solutions such as
blockchain and deep reinforcement learning. This work presents a comprehensive
taxonomy that thoroughly covers both centralized and decentralized
architectures based on the aforementioned theoretical concepts. Furthermore,
the concepts described are presented from an application perspective, covering
emerging industrial applications, including healthcare, smart infrastructure,
vehicular networks, and blockchain-based decentralized systems. Through this
exploration, this chapter demonstrates that well-designed incentive mechanisms
are not merely optional features but essential components for the practical
success of federated learning. This analysis reveals both the promising
solutions that have emerged and the significant challenges that remain in
building truly sustainable, fair, and robust federated learning ecosystems.

</details>


### [90] [Spectral Analysis of Molecular Kernels: When Richer Features Do Not Guarantee Better Generalization](https://arxiv.org/abs/2510.14217)
*Asma Jamali,Tin Sum Cheng,Rodrigo A. Vargas-Hernández*

Main category: cs.LG

TL;DR: 对分子核函数进行首次全面光谱分析，发现更丰富的光谱特征并不总能提高预测精度，挑战了"更丰富光谱带来更好泛化"的常见启发式观点。


<details>
  <summary>Details</summary>
Motivation: 虽然深度模型在分子性质预测中达到最先进精度，但核方法因其在低数据环境下的鲁棒性和透明理论基础仍被广泛使用。目前缺乏对分子核函数的系统性光谱分析。

Method: 在QM9数据集上对分子指纹、预训练transformer、全局和局部3D表示等七种分子性质进行核岭回归的全面光谱分析，使用四种不同光谱指标，并实现截断核来探索光谱与预测性能的关系。

Result: 更丰富的光谱特征并不一致提高准确性。对于transformer和局部3D表示，光谱丰富度甚至与性能呈负相关。在许多核中，仅保留前2%的特征值就能恢复几乎所有性能。

Conclusion: 研究结果挑战了"更丰富光谱带来更好泛化"的常见启发式观点，揭示了表示、核特征和预测性能之间的微妙关系，为数据受限的科学和实际任务中核和自监督学习方法的评估提供了重要见解。

Abstract: Understanding the spectral properties of kernels offers a principled
perspective on generalization and representation quality. While deep models
achieve state-of-the-art accuracy in molecular property prediction, kernel
methods remain widely used for their robustness in low-data regimes and
transparent theoretical grounding. Despite extensive studies of kernel spectra
in machine learning, systematic spectral analyses of molecular kernels are
scarce. In this work, we provide the first comprehensive spectral analysis of
kernel ridge regression on the QM9 dataset, molecular fingerprint, pretrained
transformer-based, global and local 3D representations across seven molecular
properties. Surprisingly, richer spectral features, measured by four different
spectral metrics, do not consistently improve accuracy. Pearson correlation
tests further reveal that for transformer-based and local 3D representations,
spectral richness can even have a negative correlation with performance. We
also implement truncated kernels to probe the relationship between spectrum and
predictive performance: in many kernels, retaining only the top 2% of
eigenvalues recovers nearly all performance, indicating that the leading
eigenvalues capture the most informative features. Our results challenge the
common heuristic that "richer spectra yield better generalization" and
highlight nuanced relationships between representation, kernel features, and
predictive performance. Beyond molecular property prediction, these findings
inform how kernel and self-supervised learning methods are evaluated in
data-limited scientific and real-world tasks.

</details>


### [91] [When Flatness Does (Not) Guarantee Adversarial Robustness](https://arxiv.org/abs/2510.14231)
*Nils Philipp Walter,Linara Adilova,Jilles Vreeken,Michael Kamp*

Main category: cs.LG

TL;DR: 论文揭示了平坦极小值与对抗鲁棒性的关系：平坦性仅保证局部鲁棒性而非全局鲁棒性，对抗样本常位于模型自信但错误的平坦区域


<details>
  <summary>Details</summary>
Motivation: 尽管神经网络经验上成功，但仍易受微小对抗扰动攻击。长期以来假设平坦极小值（损失景观中低曲率区域）能提供更强鲁棒性，但这种联系缺乏严格形式化

Method: 首先推导倒数第二层相对平坦度的闭式表达式，利用此约束输入空间中损失的变化，从而形式化分析整个网络的对抗鲁棒性

Result: 平坦性仅保证局部对抗鲁棒性，要维持数据流形之外的鲁棒性，损失需要急剧弯曲。经验验证发现对抗样本常位于模型自信但错误的平坦区域

Conclusion: 研究挑战了平坦性的简化观点，提供了对其在鲁棒性中作用的细致理解，揭示了平坦性与模型置信度的联系

Abstract: Despite their empirical success, neural networks remain vulnerable to small,
adversarial perturbations. A longstanding hypothesis suggests that flat minima,
regions of low curvature in the loss landscape, offer increased robustness.
While intuitive, this connection has remained largely informal and incomplete.
By rigorously formalizing the relationship, we show this intuition is only
partially correct: flatness implies local but not global adversarial
robustness. To arrive at this result, we first derive a closed-form expression
for relative flatness in the penultimate layer, and then show we can use this
to constrain the variation of the loss in input space. This allows us to
formally analyze the adversarial robustness of the entire network. We then show
that to maintain robustness beyond a local neighborhood, the loss needs to
curve sharply away from the data manifold. We validate our theoretical
predictions empirically across architectures and datasets, uncovering the
geometric structure that governs adversarial vulnerability, and linking
flatness to model confidence: adversarial examples often lie in large, flat
regions where the model is confidently wrong. Our results challenge simplified
views of flatness and provide a nuanced understanding of its role in
robustness.

</details>


### [92] [Scaling Test-Time Compute to Achieve IOI Gold Medal with Open-Weight Models](https://arxiv.org/abs/2510.14232)
*Mehrzad Samadi,Aleksander Ficek,Sean Narenthiran,Siddhartha Jain,Wasi Uddin Ahmad,Somshubra Majumdar,Vahid Noroozi,Boris Ginsburg*

Main category: cs.LG

TL;DR: GenCluster是一个可扩展的测试时计算框架，使用开源模型在IOI竞赛中达到金牌水平性能，通过大规模生成、行为聚类、排序和轮询提交策略来高效探索解决方案空间。


<details>
  <summary>Details</summary>
Motivation: 竞争性编程已成为评估大型语言模型推理和问题解决能力的重要基准，但开源模型要达到IOI金牌水平性能仍面临挑战，需要透明可复现的方法。

Method: 结合大规模生成、行为聚类、排序和轮询提交策略，在有限验证预算下高效探索多样化的解决方案空间。

Result: GenCluster能够随着可用计算资源一致扩展性能，首次使用开源模型gpt-oss-120b在IOI 2025中达到金牌水平，缩小了开源与闭源系统之间的差距。

Conclusion: 该框架为LLM推理能力的透明可复现评估设立了新基准，证明了开源模型在竞争性编程中可以达到顶尖水平。

Abstract: Competitive programming has become a rigorous benchmark for evaluating the
reasoning and problem-solving capabilities of large language models (LLMs). The
International Olympiad in Informatics (IOI) stands out as one of the most
prestigious annual competitions in competitive programming and has become a key
benchmark for comparing human and AI-level programming ability. While several
proprietary models have been claimed to achieve gold medal-level performance at
the IOI, often with undisclosed methods, achieving comparable results with
open-weight models remains a significant challenge. In this paper, we present
\gencluster, a scalable and reproducible test-time compute framework that
attains IOI gold-level performance using open-weight models. It combines
large-scale generation, behavioral clustering, ranking, and a round-robin
submission strategy to efficiently explore diverse solution spaces under
limited validation budgets. Our experiments show that the performance of our
proposed approach scales consistently with available compute, narrowing the gap
between open and closed systems. Notably, we will show that GenCluster can
achieve a gold medal at IOI 2025 for the first time with an open-weight model
gpt-oss-120b, setting a new benchmark for transparent and reproducible
evaluation of reasoning in LLMs.

</details>


### [93] [Policy Regularized Distributionally Robust Markov Decision Processes with Linear Function Approximation](https://arxiv.org/abs/2510.14246)
*Jingwen Gu,Yiting He,Zhishuai Liu,Pan Xu*

Main category: cs.LG

TL;DR: 提出了DR-RPO算法，一种模型无关的在线策略优化方法，用于在分布偏移下学习鲁棒策略，具有次线性遗憾保证。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中训练和部署环境不同的分布偏移问题，特别是在在线设置下样本效率和探索的挑战。

Method: 结合参考策略正则化的分布鲁棒正则化策略优化算法，采用d-矩形线性MDP公式和线性函数逼近，配合上置信界奖励进行乐观探索。

Result: 理论证明策略优化在鲁棒RL中可实现多项式次优性界限和样本效率，与基于价值的方法性能相当。实证结果验证了DR-RPO的鲁棒性。

Conclusion: DR-RPO算法成功地将策略优化扩展到鲁棒RL，在理论和实证上都表现出色，为分布偏移下的决策提供了有效解决方案。

Abstract: Decision-making under distribution shift is a central challenge in
reinforcement learning (RL), where training and deployment environments differ.
We study this problem through the lens of robust Markov decision processes
(RMDPs), which optimize performance against adversarial transition dynamics.
Our focus is the online setting, where the agent has only limited interaction
with the environment, making sample efficiency and exploration especially
critical. Policy optimization, despite its success in standard RL, remains
theoretically and empirically underexplored in robust RL. To bridge this gap,
we propose \textbf{D}istributionally \textbf{R}obust \textbf{R}egularized
\textbf{P}olicy \textbf{O}ptimization algorithm (DR-RPO), a model-free online
policy optimization method that learns robust policies with sublinear regret.
To enable tractable optimization within the softmax policy class, DR-RPO
incorporates reference-policy regularization, yielding RMDP variants that are
doubly constrained in both transitions and policies. To scale to large
state-action spaces, we adopt the $d$-rectangular linear MDP formulation and
combine linear function approximation with an upper confidence bonus for
optimistic exploration. We provide theoretical guarantees showing that policy
optimization can achieve polynomial suboptimality bounds and sample efficiency
in robust RL, matching the performance of value-based approaches. Finally,
empirical results across diverse domains corroborate our theory and demonstrate
the robustness of DR-RPO.

</details>


### [94] [A Physics Prior-Guided Dual-Stream Attention Network for Motion Prediction of Elastic Bragg Breakwaters](https://arxiv.org/abs/2510.14250)
*Lianzi Jiang,Jianxin Zhang,Xinyu Han,Huanhe Dong,Xiangrong Wang*

Main category: cs.LG

TL;DR: 提出PhysAttnNet模型，通过物理先验引导的双流注意力网络改进弹性布拉格防波堤运动响应预测，解决传统深度学习模型在未见海况下泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在预测弹性布拉格防波堤运动响应时，对未见海况的泛化能力有限，主要原因是忽略了海洋系统的自然衰减现象和对波-结构相互作用建模不足。

Method: 提出PhysAttnNet模型，包含衰减双向自注意力模块（模拟自然衰减）、相位差引导双向交叉注意力模块（捕捉波-结构相互作用），通过全局上下文融合模块整合，并使用混合时频损失函数进行训练。

Result: 在波浪水槽数据集上的实验表明，PhysAttnNet显著优于主流模型，跨场景泛化测试验证了模型在未见环境中的鲁棒性和适应性。

Conclusion: PhysAttnNet作为海洋工程复杂系统预测模型的开发框架具有重要潜力，能够有效提升弹性布拉格防波堤运动响应预测的准确性和泛化能力。

Abstract: Accurate motion response prediction for elastic Bragg breakwaters is critical
for their structural safety and operational integrity in marine environments.
However, conventional deep learning models often exhibit limited generalization
capabilities when presented with unseen sea states. These deficiencies stem
from the neglect of natural decay observed in marine systems and inadequate
modeling of wave-structure interaction (WSI). To overcome these challenges,
this study proposes a novel Physics Prior-Guided Dual-Stream Attention Network
(PhysAttnNet). First, the decay bidirectional self-attention (DBSA) module
incorporates a learnable temporal decay to assign higher weights to recent
states, aiming to emulate the natural decay phenomenon. Meanwhile, the phase
differences guided bidirectional cross-attention (PDG-BCA) module explicitly
captures the bidirectional interaction and phase relationship between waves and
the structure using a cosine-based bias within a bidirectional
cross-computation paradigm. These streams are synergistically integrated
through a global context fusion (GCF) module. Finally, PhysAttnNet is trained
with a hybrid time-frequency loss that jointly minimizes time-domain prediction
errors and frequency-domain spectral discrepancies. Comprehensive experiments
on wave flume datasets demonstrate that PhysAttnNet significantly outperforms
mainstream models. Furthermore,cross-scenario generalization tests validate the
model's robustness and adaptability to unseen environments, highlighting its
potential as a framework to develop predictive models for complex systems in
ocean engineering.

</details>


### [95] [Generalist vs Specialist Time Series Foundation Models: Investigating Potential Emergent Behaviors in Assessing Human Health Using PPG Signals](https://arxiv.org/abs/2510.14254)
*Saurabh Kataria,Yi Wu,Zhaoliang Chen,Hyunjung Gloria Kwak,Yuhao Xu,Lovely Yeswanth Panchumarthi,Ran Xiao,Jiaying Lu,Ayca Ermis,Anni Zhao,Runze Yan,Alex Federov,Zewen Liu,Xu Wu,Wei Jin,Carl Yang,Jocelyn Grunwell,Stephanie R. Brown,Amit Shah,Craig Jabaley,Tim Buchman,Sivasubramanium V Bhavani,Randall J. Lee,Xiao Hu*

Main category: cs.LG

TL;DR: 本文对比了通用型和专用型时间序列基础模型在PPG信号处理上的性能，通过51个任务的综合评估发现专用模型在完全微调场景下胜率高出27%。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型在时间序列分析中的应用增加，特别是生理信号处理领域，需要比较通用型模型（如MOMENT）和专用型模型在不同下游任务中的表现差异。

Method: 通过51个任务（包括心脏状态评估、实验室值估计和跨模态推理）的综合评估，从七个维度（胜率、平均性能、特征质量、调优增益、性能方差、可迁移性和可扩展性）对两种模型进行对比分析。

Result: 在完全微调场景下，专用型模型的胜率比通用型模型高出27%，在多个评估维度上表现更优。

Conclusion: 专用型时间序列基础模型在PPG信号处理任务中表现优于通用型模型，特别是在完全微调场景下，但选择训练数据对模型性能至关重要。

Abstract: Foundation models are large-scale machine learning models that are
pre-trained on massive amounts of data and can be adapted for various
downstream tasks. They have been extensively applied to tasks in Natural
Language Processing and Computer Vision with models such as GPT, BERT, and
CLIP. They are now also increasingly gaining attention in time-series analysis,
particularly for physiological sensing. However, most time series foundation
models are specialist models - with data in pre-training and testing of the
same type, such as Electrocardiogram, Electroencephalogram, and
Photoplethysmogram (PPG). Recent works, such as MOMENT, train a generalist time
series foundation model with data from multiple domains, such as weather,
traffic, and electricity. This paper aims to conduct a comprehensive
benchmarking study to compare the performance of generalist and specialist
models, with a focus on PPG signals. Through an extensive suite of total 51
tasks covering cardiac state assessment, laboratory value estimation, and
cross-modal inference, we comprehensively evaluate both models across seven
dimensions, including win score, average performance, feature quality, tuning
gain, performance variance, transferability, and scalability. These metrics
jointly capture not only the models' capability but also their adaptability,
robustness, and efficiency under different fine-tuning strategies, providing a
holistic understanding of their strengths and limitations for diverse
downstream scenarios. In a full-tuning scenario, we demonstrate that the
specialist model achieves a 27% higher win score. Finally, we provide further
analysis on generalization, fairness, attention visualizations, and the
importance of training data choice.

</details>


### [96] [CAST: Compositional Analysis via Spectral Tracking for Understanding Transformer Layer Functions](https://arxiv.org/abs/2510.14262)
*Zihao Fu,Ming Liao,Chris Russell,Zhenguang G. Cai*

Main category: cs.LG

TL;DR: CAST是一个无需探针的框架，通过直接估计变换矩阵和谱分析来理解Transformer层的功能，揭示了编码器和解码器模型的不同行为模式。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然成功但仍是黑盒，现有可解释性方法各有局限，需要补充性的新视角来理解模型内部机制。

Method: 使用Moore-Penrose伪逆估计每层的实际变换矩阵，应用包含六个可解释指标的谱分析，并进行核分析来研究层间功能关系。

Result: 发现解码器模型呈现压缩-扩展循环，编码器模型保持一致的高秩处理；核相似性矩阵将层划分为特征提取、压缩和专业化三个阶段。

Conclusion: CAST提供了与现有方法互补的见解，通过谱跟踪揭示了Transformer层功能的组成性分析，有助于更好地理解语言模型的内部工作机制。

Abstract: Large language models have achieved remarkable success but remain largely
black boxes with poorly understood internal mechanisms. To address this
limitation, many researchers have proposed various interpretability methods
including mechanistic analysis, probing classifiers, and activation
visualization, each providing valuable insights from different perspectives.
Building upon this rich landscape of complementary approaches, we introduce
CAST (Compositional Analysis via Spectral Tracking), a probe-free framework
that contributes a novel perspective by analyzing transformer layer functions
through direct transformation matrix estimation and comprehensive spectral
analysis. CAST offers complementary insights to existing methods by estimating
the realized transformation matrices for each layer using Moore-Penrose
pseudoinverse and applying spectral analysis with six interpretable metrics
characterizing layer behavior. Our analysis reveals distinct behaviors between
encoder-only and decoder-only models, with decoder models exhibiting
compression-expansion cycles while encoder models maintain consistent high-rank
processing. Kernel analysis further demonstrates functional relationship
patterns between layers, with CKA similarity matrices clearly partitioning
layers into three phases: feature extraction, compression, and specialization.

</details>


### [97] [Nonparametric Data Attribution for Diffusion Models](https://arxiv.org/abs/2510.14269)
*Yutian Zhao,Chao Du,Xiaosen Zheng,Tianyu Pang,Min Lin*

Main category: cs.LG

TL;DR: 提出了一种基于数据而非梯度的非参数归因方法，通过图像块级相似度来衡量训练数据对生成输出的影响，无需模型梯度或重训练。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型归因方法通常需要模型梯度或重训练，限制了在专有或大规模场景下的应用，需要一种更通用的解决方案。

Method: 基于最优得分函数的解析形式，通过图像块级相似度测量生成图像与训练图像之间的影响，支持多尺度表示，并通过卷积加速实现计算效率。

Result: 实验表明该方法实现了强大的归因性能，与基于梯度的方法结果相近，并显著优于现有非参数基线方法。

Conclusion: 该方法提供了空间可解释的归因结果，揭示了训练数据与输出之间的内在关系，且独立于特定模型，具有广泛适用性。

Abstract: Data attribution for generative models seeks to quantify the influence of
individual training examples on model outputs. Existing methods for diffusion
models typically require access to model gradients or retraining, limiting
their applicability in proprietary or large-scale settings. We propose a
nonparametric attribution method that operates entirely on data, measuring
influence via patch-level similarity between generated and training images. Our
approach is grounded in the analytical form of the optimal score function and
naturally extends to multiscale representations, while remaining
computationally efficient through convolution-based acceleration. In addition
to producing spatially interpretable attributions, our framework uncovers
patterns that reflect intrinsic relationships between training data and
outputs, independent of any specific model. Experiments demonstrate that our
method achieves strong attribution performance, closely matching gradient-based
approaches and substantially outperforming existing nonparametric baselines.
Code is available at https://github.com/sail-sg/NDA.

</details>


### [98] [Stable Prediction of Adverse Events in Medical Time-Series Data](https://arxiv.org/abs/2510.14286)
*Mayank Keoliya,Seewon Choi,Rajeev Alur,Mayur Naik,Eric Wong*

Main category: cs.LG

TL;DR: CAREBench是一个早期事件预测基准，评估多模态输入下的预测准确性和时间稳定性，发现现有方法特别是LLMs难以同时优化准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前早期事件预测系统忽略风险评分稳定性，且主要评估表格输入，缺乏对风险轨迹行为的测试。

Method: 引入CAREBench基准，使用多模态输入（表格EHR、ECG波形、临床文本），提出基于局部Lipschitz常数的稳定性度量来量化短期风险变异性。

Result: 现有方法特别是LLMs在准确性和稳定性联合优化方面表现不佳，在高精度操作点召回率较差。

Conclusion: 需要开发能够产生证据对齐、稳定轨迹的模型，以在连续监测环境中赢得临床医生的信任。

Abstract: Early event prediction (EEP) systems continuously estimate a patient's
imminent risk to support clinical decision-making. For bedside trust, risk
trajectories must be accurate and temporally stable, shifting only with new,
relevant evidence. However, current benchmarks (a) ignore stability of risk
scores and (b) evaluate mainly on tabular inputs, leaving trajectory behavior
untested. To address this gap, we introduce CAREBench, an EEP benchmark that
evaluates deployability using multi-modal inputs-tabular EHR, ECG waveforms,
and clinical text-and assesses temporal stability alongside predictive
accuracy. We propose a stability metric that quantifies short-term variability
in per-patient risk and penalizes abrupt oscillations based on local-Lipschitz
constants. CAREBench spans six prediction tasks such as sepsis onset and
compares classical learners, deep sequence models, and zero-shot LLMs. Across
tasks, existing methods, especially LLMs, struggle to jointly optimize accuracy
and stability, with notably poor recall at high-precision operating points.
These results highlight the need for models that produce evidence-aligned,
stable trajectories to earn clinician trust in continuous monitoring settings.
(Code: https://github.com/SeewonChoi/CAREBench.)

</details>


### [99] [Enhancing Time-Series Anomaly Detection by Integrating Spectral-Residual Bottom-Up Attention with Reservoir Computing](https://arxiv.org/abs/2510.14287)
*Hayato Nihei,Sou Nobukawa,Yusuke Sakemi,Kazuyuki Aihara*

Main category: cs.LG

TL;DR: 提出了结合谱残差方法和储层计算的SR-RC模型，用于提升时间序列异常检测性能，同时保持学习效率，适合边缘AI部署。


<details>
  <summary>Details</summary>
Motivation: 传统储层计算在资源受限的边缘设备上需要过大的储层才能达到足够的异常检测性能，而注意力机制虽然能提高精度但计算量大，会损害RC的学习效率。

Method: 将学习免费的谱残差方法（一种自底向上的注意力机制）与储层计算相结合，形成SR-RC模型。

Result: SR-RC在基准任务和真实世界时间序列数据集上优于传统RC模型和基于SR方法提取值的逻辑回归模型。

Conclusion: SR-RC为在边缘AI中部署RC进行时间序列异常检测提供了实用方向，因为SR方法与RC一样都适合硬件实现。

Abstract: Reservoir computing (RC) establishes the basis for the processing of
time-series data by exploiting the high-dimensional spatiotemporal response of
a recurrent neural network to an input signal. In particular, RC trains only
the output layer weights. This simplicity has drawn attention especially in
Edge Artificial Intelligence (AI) applications. Edge AI enables time-series
anomaly detection in real time, which is important because detection delays can
lead to serious incidents. However, achieving adequate anomaly-detection
performance with RC alone may require an unacceptably large reservoir on
resource-constrained edge devices. Without enlarging the reservoir, attention
mechanisms can improve accuracy, although they may require substantial
computation and undermine the learning efficiency of RC. In this study, to
improve the anomaly detection performance of RC without sacrificing learning
efficiency, we propose a spectral residual RC (SR-RC) that integrates the
spectral residual (SR) method - a learning-free, bottom-up attention mechanism
- with RC. We demonstrated that SR-RC outperformed conventional RC and
logistic-regression models based on values extracted by the SR method across
benchmark tasks and real-world time-series datasets. Moreover, because the SR
method, similarly to RC, is well suited for hardware implementation, SR-RC
suggests a practical direction for deploying RC as Edge AI for time-series
anomaly detection.

</details>


### [100] [TED++: Submanifold-Aware Backdoor Detection via Layerwise Tubular-Neighbourhood Screening](https://arxiv.org/abs/2510.14299)
*Nam Le,Leo Yu Zhang,Kewen Liao,Shirui Pan,Wei Luo*

Main category: cs.LG

TL;DR: TED++是一个子流形感知的框架，通过构建类隐藏特征流形的管状邻域并应用局部自适应排序来检测逃避现有防御的隐蔽后门攻击。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在关键应用中面临隐蔽后门攻击的严重安全风险，现有防御在攻击者利用微妙距离异常或干净样本稀缺时容易失效。

Method: TED++为每个类的隐藏特征流形构建管状邻域，从少量干净激活中估计局部"厚度"，然后应用局部自适应排序检测超出允许管的激活，通过聚合所有层的LAR调整排名来捕获输入在演化类子流形上的忠实度。

Result: 在基准数据集和任务上的广泛实验表明，TED++在自适应攻击和有限数据场景下实现了最先进的检测性能，即使每类只有5个保留样本也能实现近乎完美的检测，AUROC比次优方法提升高达14%。

Conclusion: TED++通过基于管约束行为的特征排名序列显著偏离来标记输入，有效检测逃避现有防御的隐蔽后门攻击。

Abstract: As deep neural networks power increasingly critical applications, stealthy
backdoor attacks, where poisoned training inputs trigger malicious model
behaviour while appearing benign, pose a severe security risk. Many existing
defences are vulnerable when attackers exploit subtle distance-based anomalies
or when clean examples are scarce. To meet this challenge, we introduce TED++,
a submanifold-aware framework that effectively detects subtle backdoors that
evade existing defences. TED++ begins by constructing a tubular neighbourhood
around each class's hidden-feature manifold, estimating its local ``thickness''
from a handful of clean activations. It then applies Locally Adaptive Ranking
(LAR) to detect any activation that drifts outside the admissible tube. By
aggregating these LAR-adjusted ranks across all layers, TED++ captures how
faithfully an input remains on the evolving class submanifolds. Based on such
characteristic ``tube-constrained'' behaviour, TED++ flags inputs whose
LAR-based ranking sequences deviate significantly. Extensive experiments are
conducted on benchmark datasets and tasks, demonstrating that TED++ achieves
state-of-the-art detection performance under both adaptive-attack and
limited-data scenarios. Remarkably, even with only five held-out examples per
class, TED++ still delivers near-perfect detection, achieving gains of up to
14\% in AUROC over the next-best method. The code is publicly available at
https://github.com/namle-w/TEDpp.

</details>


### [101] [Active Measuring in Reinforcement Learning With Delayed Negative Effects](https://arxiv.org/abs/2510.14315)
*Daiqi Gao,Ziping Xu,Aseel Rawashdeh,Predrag Klasnja,Susan A. Murphy*

Main category: cs.LG

TL;DR: 提出主动可观测马尔可夫决策过程（AOMDP），其中智能体不仅选择控制动作，还决定是否测量潜在状态。测量动作会揭示真实潜在状态，但可能对环境产生负面延迟影响。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中测量状态在现实环境中可能成本高昂，并可能对未来结果产生负面影响。需要一种方法让智能体能够权衡测量成本和状态不确定性。

Method: 将AOMDP建模为周期性部分可观测MDP，提出基于信念状态的在线RL算法，并使用顺序蒙特卡洛方法联合近似未知静态环境参数和未观测潜在状态的后验分布。

Result: 在数字健康应用中评估该算法，智能体决定何时提供数字干预以及何时通过调查评估用户健康状态。

Conclusion: 尽管测量可能带来成本，但减少的不确定性可以证明提高样本效率并增加最优策略的价值。

Abstract: Measuring states in reinforcement learning (RL) can be costly in real-world
settings and may negatively influence future outcomes. We introduce the
Actively Observable Markov Decision Process (AOMDP), where an agent not only
selects control actions but also decides whether to measure the latent state.
The measurement action reveals the true latent state but may have a negative
delayed effect on the environment. We show that this reduced uncertainty may
provably improve sample efficiency and increase the value of the optimal policy
despite these costs. We formulate an AOMDP as a periodic partially observable
MDP and propose an online RL algorithm based on belief states. To approximate
the belief states, we further propose a sequential Monte Carlo method to
jointly approximate the posterior of unknown static environment parameters and
unobserved latent states. We evaluate the proposed algorithm in a digital
health application, where the agent decides when to deliver digital
interventions and when to assess users' health status through surveys.

</details>


### [102] [LLM-ERM: Sample-Efficient Program Learning via LLM-Guided Search](https://arxiv.org/abs/2510.14331)
*Shivam Singhal,Eran Malach,Tomaso Poggio,Tomer Galanti*

Main category: cs.LG

TL;DR: LLM-ERM：一种结合LLM引导搜索和ERM验证的程序学习框架，在样本效率和计算可行性上优于传统方法


<details>
  <summary>Details</summary>
Motivation: 解决程序学习中样本效率和计算可行性之间的平衡问题。传统方法要么搜索成本高（枚举法），要么样本需求大（梯度训练法）

Method: 提出LLM-ERM框架：使用预训练LLM生成候选程序，然后在验证数据上编译检查并选择最佳假设，无需反馈、自适应或梯度

Result: 理论证明坐标方向在线小批量SGD需要大量样本学习某些短程序；实证显示LLM-ERM仅需200样本即可解决奇偶变体、模式匹配和素数测试等任务，而SGD训练的transformer即使有10万样本也会过拟合

Conclusion: 语言引导的程序合成在保持计算可行性的同时恢复了有限类ERM的统计效率，为学习梯度训练无法达到的简洁假设提供了实用途径

Abstract: We seek algorithms for program learning that are both sample-efficient and
computationally feasible. Classical results show that targets admitting short
program descriptions (e.g., with short ``python code'') can be learned with a
``small'' number of examples (scaling with the size of the code) via
length-first program enumeration, but the search is exponential in description
length. Consequently, Gradient-based training avoids this cost yet can require
exponentially many samples on certain short-program families.
  To address this gap, we introduce LLM-ERM, a propose-and-verify framework
that replaces exhaustive enumeration with an LLM-guided search over candidate
programs while retaining ERM-style selection on held-out data. Specifically, we
draw $k$ candidates with a pretrained reasoning-augmented LLM, compile and
check each on the data, and return the best verified hypothesis, with no
feedback, adaptivity, or gradients. Theoretically, we show that coordinate-wise
online mini-batch SGD requires many samples to learn certain short programs.
{\em Empirically, LLM-ERM solves tasks such as parity variants, pattern
matching, and primality testing with as few as 200 samples, while SGD-trained
transformers overfit even with 100,000 samples}. These results indicate that
language-guided program synthesis recovers much of the statistical efficiency
of finite-class ERM while remaining computationally tractable, offering a
practical route to learning succinct hypotheses beyond the reach of
gradient-based training.

</details>


### [103] [DARTS-GT: Differentiable Architecture Search for Graph Transformers with Quantifiable Instance-Specific Interpretability Analysis](https://arxiv.org/abs/2510.14336)
*Shruti Sarika Chakraborty,Peter Minary*

Main category: cs.LG

TL;DR: 提出DARTS-GT方法，通过非对称注意力机制和解耦结构编码与特征表示，在Transformer注意力内部使用可微分架构搜索选择最优GNN算子，实现深度异构架构，并开发首个图Transformer量化可解释性框架。


<details>
  <summary>Details</summary>
Motivation: 当前图Transformer存在设计僵化、缺乏量化可解释性的问题，固定GNN类型无法实现深度特定组件选择，复杂架构难以区分性能提升是来自有意义的模式还是虚假相关性。

Method: 重新设计图Transformer注意力机制，采用非对称设计：查询来自节点特征，键和值来自GNN变换。在框架内使用可微分架构搜索为每层选择最优GNN算子，实现注意力内部的深度异构性。开发因果消融量化可解释性框架，包含Head-deviation、Specialization和Focus指标。

Result: 在八个基准测试中，DARTS-GT在四个数据集上达到最先进水平，在其他数据集上保持竞争力。发现的架构揭示了数据集特定模式。可解释性分析表明视觉注意力显著性与因果重要性并不总是相关。

Conclusion: DARTS-GT发现的异构架构始终产生比基线更可解释的模型，证明图Transformer无需在性能和可解释性之间做出选择。

Abstract: Graph Transformers (GTs) have emerged as powerful architectures for
graph-structured data, yet remain constrained by rigid designs and lack
quantifiable interpretability. Current state-of-the-art GTs commit to fixed GNN
types across all layers, missing potential benefits of depth-specific component
selection, while their complex architectures become opaque where performance
gains cannot be distinguished between meaningful patterns and spurious
correlations. We redesign GT attention through asymmetry, decoupling structural
encoding from feature representation: queries derive from node features while
keys and values come from GNN transformations. Within this framework, we use
Differentiable ARchiTecture Search (DARTS) to select optimal GNN operators at
each layer, enabling depth-wise heterogeneity inside transformer attention
itself (DARTS-GT). To understand discovered architectures, we develop the first
quantitative interpretability framework for GTs through causal ablation. Our
metrics (Head-deviation, Specialization, and Focus), identify which heads and
nodes drive predictions while enabling model comparison. Experiments across
eight benchmarks show DARTS-GT achieves state-of-the-art on four datasets while
remaining competitive on others, with discovered architectures revealing
dataset-specific patterns. Our interpretability analysis reveals that visual
attention salience and causal importance do not always correlate, indicating
widely used visualization approaches may miss components that actually matter.
Crucially, heterogeneous architectures found by DARTS-GT consistently produced
more interpretable models than baselines, establishing that Graph Transformers
need not choose between performance and interpretability.

</details>


### [104] [Stop-RAG: Value-Based Retrieval Control for Iterative RAG](https://arxiv.org/abs/2510.14337)
*Jaewan Park,Solbee Cho,Jay-Yoon Lee*

Main category: cs.LG

TL;DR: Stop-RAG是一个基于价值控制的迭代RAG停止策略，将迭代检索过程建模为有限时域马尔可夫决策过程，通过Q(λ)学习自适应决定何时停止检索。


<details>
  <summary>Details</summary>
Motivation: 迭代RAG虽然能回答复杂多跳问题，但每次额外循环都会增加延迟、成本和引入干扰证据的风险，现有方法要么使用固定迭代次数，要么依赖不能很好反映检索帮助的置信度代理。

Method: 将迭代RAG建模为有限时域马尔可夫决策过程，提出Stop-RAG基于价值控制器，使用完整轨迹的全宽度前向视图Q(λ)目标进行训练。

Result: 在多跳问答基准测试中，Stop-RAG始终优于固定迭代基线和基于提示的LLM停止方法。

Conclusion: 自适应停止是当前代理系统中的关键缺失组件，基于价值的控制可以显著提高RAG系统的准确性。

Abstract: Iterative retrieval-augmented generation (RAG) enables large language models
to answer complex multi-hop questions, but each additional loop increases
latency, costs, and the risk of introducing distracting evidence, motivating
the need for an efficient stopping strategy. Existing methods either use a
predetermined number of iterations or rely on confidence proxies that poorly
reflect whether more retrieval will actually help. We cast iterative RAG as a
finite-horizon Markov decision process and introduce Stop-RAG, a value-based
controller that adaptively decides when to stop retrieving. Trained with
full-width forward-view Q($\lambda$) targets from complete trajectories,
Stop-RAG learns effective stopping policies while remaining compatible with
black-box APIs and existing pipelines. On multi-hop question-answering
benchmarks, Stop-RAG consistently outperforms both fixed-iteration baselines
and prompting-based stopping with LLMs. These results highlight adaptive
stopping as a key missing component in current agentic systems, and demonstrate
that value-based control can improve the accuracy of RAG systems.

</details>


### [105] [Jet Functors and Weil Algebras in Automatic Differentiation: A Geometric Analysis](https://arxiv.org/abs/2510.14342)
*Amandip Sangha*

Main category: cs.LG

TL;DR: 本文通过射流丛和Weil代数提出了自动微分的几何表述，将反向模式AD视为余切拉回，泰勒模式对应Weil代数中的求值，并推导了关于正确性、稳定性和复杂性的简洁陈述。


<details>
  <summary>Details</summary>
Motivation: 为自动微分理论提供几何基础，通过微分几何的视角解释AD，为深度学习和科学计算中开发结构保持的微分方法奠定基础。

Method: 使用射流丛和Weil代数的几何框架，将反向模式AD表述为余切拉回操作，泰勒模式作为Weil代数中的求值过程。

Result: 推导了反向模式的函子恒等式、高阶导数的代数精确性以及截断误差的显式界，并证明张量化Weil代数可以一次性计算所有混合导数，避免组合爆炸。

Conclusion: 该框架为自动微分理论提供了几何解释，并为开发结构保持的微分方法提供了理论基础，在深度学习和科学计算中具有应用前景。

Abstract: We present a geometric formulation of automatic differentiation (AD) using
jet bundles and Weil algebras. Reverse-mode AD emerges as cotangent-pullback,
while Taylor-mode corresponds to evaluation in a Weil algebra. From these
principles, we derive concise statements on correctness, stability, and
complexity: a functorial identity for reverse-mode, algebraic exactness of
higher-order derivatives, and explicit bounds on truncation error. We further
show that tensorized Weil algebras permit one-pass computation of all mixed
derivatives with cost linear in the algebra dimension, avoiding the
combinatorial blow-up of nested JVP/VJP schedules. This framework interprets AD
theory through the lens of differential geometry and offers a foundation for
developing structure-preserving differentiation methods in deep learning and
scientific computing. Code and examples are available at
https://git.nilu.no/geometric-ad/jet-weil-ad.

</details>


### [106] [Are My Optimized Prompts Compromised? Exploring Vulnerabilities of LLM-based Optimizers](https://arxiv.org/abs/2510.14381)
*Andrew Zhao,Reshmi Ghosh,Vitor Carvalho,Emily Lawton,Keegan Hines,Gao Huang,Jack W. Stokes*

Main category: cs.LG

TL;DR: 该论文首次系统分析了基于LLM的提示优化中的投毒风险，发现基于反馈的攻击比注入查询攻击更有效，提出了无需访问奖励模型的假奖励攻击方法，并设计了轻量级高亮防御机制。


<details>
  <summary>Details</summary>
Motivation: LLM系统在日常AI应用中广泛使用，性能依赖于精心设计的提示。基于LLM的提示优化器通过迭代优化减少人工努力，但优化阶段的安全性研究不足，需要系统分析投毒风险。

Method: 使用HarmBench进行系统分析，引入简单的假奖励攻击方法（无需访问奖励模型），并提出轻量级高亮防御机制来减轻攻击影响。

Result: 发现基于反馈的攻击比注入查询攻击更有效，攻击成功率提升高达ΔASR=0.48；假奖励攻击显著增加系统脆弱性，高亮防御能将假奖励攻击的ΔASR从0.23降低到0.07且不影响实用性。

Conclusion: 提示优化管道应被视为首要攻击面，需要加强对反馈通道和优化框架的安全防护措施。

Abstract: Large language model (LLM) systems now underpin everyday AI applications such
as chatbots, computer-use assistants, and autonomous robots, where performance
often depends on carefully designed prompts. LLM-based prompt optimizers reduce
that effort by iteratively refining prompts from scored feedback, yet the
security of this optimization stage remains underexamined. We present the first
systematic analysis of poisoning risks in LLM-based prompt optimization. Using
HarmBench, we find systems are substantially more vulnerable to manipulated
feedback than to injected queries: feedback-based attacks raise attack success
rate (ASR) by up to $\Delta$ASR = 0.48. We introduce a simple fake-reward
attack that requires no access to the reward model and significantly increases
vulnerability, and we propose a lightweight highlighting defense that reduces
the fake-reward $\Delta$ASR from 0.23 to 0.07 without degrading utility. These
results establish prompt optimization pipelines as a first-class attack surface
and motivate stronger safeguards for feedback channels and optimization
frameworks.

</details>


### [107] [SHaRe-SSM: An Oscillatory Spiking Neural Network for Target Variable Modeling in Long Sequences](https://arxiv.org/abs/2510.14386)
*Kartikay Agrawal,Abhijeet Vikram,Vedant Sharma,Vaishnavi N.,Ayon Borthakur*

Main category: cs.LG

TL;DR: 提出SHaRe-SSM（脉冲谐振放电状态空间模型），一种用于超长序列建模的二阶脉冲状态空间模型，在保持性能的同时显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: 结合脉冲神经网络（SNNs）的能效优势和状态空间模型（SSMs）在长序列建模中的能力，解决传统transformer在长序列上二次复杂度的问题。

Method: 设计二阶脉冲SSM，利用谐振放电神经元和并行扫描实现稳定高效的长序列学习，并提出基于核的脉冲回归器。

Result: 在18k序列上比二阶ANN-based SSMs节能73倍，在50k序列上仍保持优越性能，同时性能优于transformer和一阶SSMs。

Conclusion: SHaRe-SSM为资源受限应用提供了一种高效的长序列建模解决方案，并系统分析了谐振放电SSM中异质性、耗散和守恒的影响。

Abstract: In recent years, with the emergence of large models, there has been a
significant interest in spiking neural networks (SNNs) primarily due to their
energy efficiency, multiplication-free, and sparse event-based deep learning.
Similarly, state space models (SSMs) in varying designs have evolved as a
powerful alternative to transformers for target modeling in long sequences,
thereby overcoming the quadratic dependence on sequence length of a
transformer. Inspired by this progress, we here design SHaRe-SSM (Spiking
Harmonic Resonate and Fire State Space Model), for target variable modeling
(including both classification and regression) for very-long-range sequences.
Our second-order spiking SSM, on average, performs better than transformers or
first-order SSMs while circumventing multiplication operations, making it ideal
for resource-constrained applications. The proposed block consumes $73 \times$
less energy than second-order ANN-based SSMs for an 18k sequence, while
retaining performance. To ensure learnability over the long-range sequences, we
propose exploiting the stable and efficient implementation of the dynamical
system using parallel scans. Moreover, for the first time, we propose a
kernel-based spiking regressor using resonate and fire neurons for very
long-range sequences. Our network shows superior performance on even a 50k
sequence while being significantly energy-efficient. In addition, we conducted
a systematic analysis of the impact of heterogeneity, dissipation, and
conservation in resonate-and-fire SSMs.

</details>


### [108] [Revisit Modality Imbalance at the Decision Layer](https://arxiv.org/abs/2510.14411)
*Xiaoyu Ma,Hao Chen*

Main category: cs.LG

TL;DR: 该论文揭示了多模态学习中的模态不平衡问题不仅发生在表示学习阶段，还显著体现在决策层，导致主导模态压制较弱模态。作者提出需要在决策层引入自适应权重分配机制来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 多模态学习虽然整合不同模态信息提升性能，但存在模态不平衡问题，主导模态在联合优化过程中会压制较弱模态，这种不平衡不仅出现在表示学习阶段，更显著体现在决策层。

Method: 通过在音频-视觉数据集（CREMAD和Kinetic-Sounds）上进行实验，分析模型在预训练和平衡优化后仍存在的系统性模态偏好，并探究特征空间和决策权重分布的内在差异。

Result: 实验表明即使经过广泛预训练和平衡优化，模型仍对某些模态（如音频）表现出系统性偏好，这种偏见源于特征空间和决策权重分布的内在差异，而非仅由优化动态引起。

Conclusion: 在融合阶段聚合未校准的模态输出会导致决策层权重偏差，阻碍较弱模态有效贡献。未来多模态系统应更多关注在决策层纳入自适应权重分配机制，根据各模态能力实现相对平衡。

Abstract: Multimodal learning integrates information from different modalities to
enhance model performance, yet it often suffers from modality imbalance, where
dominant modalities overshadow weaker ones during joint optimization. This
paper reveals that such an imbalance not only occurs during representation
learning but also manifests significantly at the decision layer. Experiments on
audio-visual datasets (CREMAD and Kinetic-Sounds) show that even after
extensive pretraining and balanced optimization, models still exhibit
systematic bias toward certain modalities, such as audio. Further analysis
demonstrates that this bias originates from intrinsic disparities in
feature-space and decision-weight distributions rather than from optimization
dynamics alone. We argue that aggregating uncalibrated modality outputs at the
fusion stage leads to biased decision-layer weighting, hindering weaker
modalities from contributing effectively. To address this, we propose that
future multimodal systems should focus more on incorporate adaptive weight
allocation mechanisms at the decision layer, enabling relative balanced
according to the capabilities of each modality.

</details>


### [109] [Interaction Concordance Index: Performance Evaluation for Interaction Prediction Methods](https://arxiv.org/abs/2510.14419)
*Tapio Pahikkala,Riikka Numminen,Parisa Movahedi,Napsu Karmitsa,Antti Airola*

Main category: cs.LG

TL;DR: 本文提出了交互一致性指数(IC-index)来评估药物-靶点亲和力预测中交互方向预测的性能，补充了现有的DTA预测评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有的药物-靶点亲和力(DTA)预测评估指标主要关注预测值的准确性，但忽略了交互效应的方向性预测。正确捕捉交互方向对于药物分配决策至关重要。

Method: 提出IC-index评估指标，分析预测器对交互方向的捕捉能力。通过理论证明和实证评估，验证IC-index在评估交互方向预测性能方面的有效性。

Result: 实验表明IC-index能够有效补充现有预测性能评估指标，不同亲和力强度预测方法在IC-index上表现各异。理论分析显示，缺乏适当侧信息的机器学习算法无法捕捉新药物或靶点的交互效应。

Conclusion: IC-index为DTA预测提供了重要的交互方向评估维度，有助于改进药物分配决策。结合适当的侧信息是捕捉交互效应的关键。

Abstract: Consider two sets of entities and their members' mutual affinity values, say
drug-target affinities (DTA). Drugs and targets are said to interact in their
effects on DTAs if drug's effect on it depends on the target. Presence of
interaction implies that assigning a drug to a target and another drug to
another target does not provide the same aggregate DTA as the reversed
assignment would provide. Accordingly, correctly capturing interactions enables
better decision-making, for example, in allocation of limited numbers of drug
doses to their best matching targets. Learning to predict DTAs is popularly
done from either solely from known DTAs or together with side information on
the entities, such as chemical structures of drugs and targets. In this paper,
we introduce interaction directions' prediction performance estimator we call
interaction concordance index (IC-index), for both fixed predictors and machine
learning algorithms aimed for inferring them. IC-index complements the
popularly used DTA prediction performance estimators by evaluating the ratio of
correctly predicted directions of interaction effects in data. First, we show
the invariance of IC-index on predictors unable to capture interactions.
Secondly, we show that learning algorithm's permutation equivariance regarding
drug and target identities implies its inability to capture interactions when
either drug, target or both are unseen during training. In practical
applications, this equivariance is remedied via incorporation of appropriate
side information on drugs and targets. We make a comprehensive empirical
evaluation over several biomedical interaction data sets with various
state-of-the-art machine learning algorithms. The experiments demonstrate how
different types of affinity strength prediction methods perform in terms of
IC-index complementing existing prediction performance estimators.

</details>


### [110] [MergeMoE: Efficient Compression of MoE Models via Expert Output Merging](https://arxiv.org/abs/2510.14436)
*Ruijie Miao,Yilun Yao,Zihan Wang,Zhiming Wang,Bairen Yi,LingJun Liu,Yikai Zhao,Tong Yang*

Main category: cs.LG

TL;DR: 本文提出了MergeMoE方法，通过数学优化构造压缩矩阵来压缩MoE模型，在保持相同压缩比的情况下优于基线方法。


<details>
  <summary>Details</summary>
Motivation: MoE技术虽然能有效扩展模型规模，但其巨大的内存开销使得模型压缩成为重要研究方向。专家合并是一种新提出的MoE压缩技术，需要从理论角度进行分析。

Method: 从专家输出合并的角度重新解释专家合并过程，将其视为在正向计算中插入额外矩阵，从而建立优化公式。基于此分析提出MergeMoE方法，利用数学优化构造压缩矩阵。

Result: 在多个MoE模型上评估MergeMoE，结果显示该方法在相同压缩比下始终优于基线方法。

Conclusion: 从专家输出合并的角度分析专家合并过程，并基于此提出优化方法MergeMoE，能有效压缩MoE模型并保持性能。

Abstract: The Mixture-of-Experts (MoE) technique has proven to be a promising solution
to efficiently scale the model size, which has been widely applied in recent
LLM advancements. However, the substantial memory overhead of MoE models has
made their compression an important research direction. In this work, we
provide a theoretical analysis of expert merging, a recently proposed technique
for compressing MoE models. Rather than interpreting expert merging from the
conventional perspective of parameter aggregation, we approach it from the
perspective of merging experts' outputs. Our key insight is that the merging
process can be interpreted as inserting additional matrices into the forward
computation, which naturally leads to an optimization formulation. Building on
this analysis, we introduce MergeMoE, a method that leverages mathematical
optimization to construct the compression matrices. We evaluate MergeMoE on
multiple MoE models and show that our algorithm consistently outperforms the
baselines with the same compression ratios.

</details>


### [111] [A Free Lunch in LLM Compression: Revisiting Retraining after Pruning](https://arxiv.org/abs/2510.14444)
*Moritz Wagner,Christophe Roux,Max Zimmer,Sebastian Pokutta*

Main category: cs.LG

TL;DR: 该论文研究发现，在大型语言模型剪枝后，逐层重构注意力机制和MLP组件比完全重训练更有效，不仅计算资源需求更低，还能获得更好的性能表现。


<details>
  <summary>Details</summary>
Motivation: 挑战现有LLM剪枝方法避免完全重训练的做法，研究剪枝后权重重构的关键设计选择，探索更高效的性能恢复方法。

Method: 在GPT架构上进行广泛计算研究，比较不同重构粒度（单矩阵、组件级、块级）的效果，分析重构与重训练的权衡。

Result: 发现组件级重构（分别重构注意力和MLP）是帕累托最优方案，比完全重训练性能更好且内存需求更低；简单剪枝标准配合适当重构可超越复杂方法。

Conclusion: 重构不应被完全避免，适当的重构策略可以比完全重训练更高效地恢复剪枝后性能，挑战了现有避免重训练的普遍认知。

Abstract: While Neural Network pruning typically requires retraining the model to
recover pruning-induced performance degradation, state-of-the-art Large
Language Models (LLMs) pruning methods instead solve a layer-wise mask
selection and reconstruction problem on a small set of calibration data to
avoid full retraining, as it is considered computationally infeasible for LLMs.
Reconstructing single matrices in isolation has favorable properties, such as
convexity of the objective and significantly reduced memory requirements
compared to full retraining. In practice, however, reconstruction is often
implemented at coarser granularities, e.g., reconstructing a whole transformer
block against its dense activations instead of a single matrix. In this work,
we study the key design choices when reconstructing or retraining the remaining
weights after pruning. We conduct an extensive computational study on
state-of-the-art GPT architectures, and report several surprising findings that
challenge common intuitions about retraining after pruning. In particular, we
observe a free lunch scenario: reconstructing attention and MLP components
separately within each transformer block is nearly the most resource-efficient
yet achieves the best perplexity. Most importantly, this Pareto-optimal setup
achieves better performance than full retraining, despite requiring only a
fraction of the memory. Furthermore, we demonstrate that simple and efficient
pruning criteria such as Wanda can outperform much more complex approaches when
the reconstruction step is properly executed, highlighting its importance. Our
findings challenge the narrative that retraining should be avoided at all costs
and provide important insights into post-pruning performance recovery for LLMs.

</details>


### [112] [Towards geological inference with process-based and deep generative modeling, part 1: training on fluvial deposits](https://arxiv.org/abs/2510.14445)
*Guillaume Rongier,Luk Peeters*

Main category: cs.LG

TL;DR: 本研究探索使用生成对抗网络(GAN)来重现基于过程模型模拟的河流沉积物，通过消融实验验证了深度学习技术在3D地质图像生成中的可迁移性，训练稳定且能再现非平稳性和细节特征。


<details>
  <summary>Details</summary>
Motivation: 当前方法难以正确再现地质结构特别是河流沉积物的连续性特征，需要开发能更好模拟地质结构的生成模型。

Method: 使用生成对抗网络(GAN)进行生成建模，以基于过程模型模拟的河流沉积物作为训练数据，通过消融研究验证2D图像生成技术向3D地质图像的迁移性。

Result: GAN训练保持稳定，生成的样本能够再现沉积物的非平稳性和细节特征，没有出现模式崩溃或纯粹记忆训练数据的问题，且能够遵循地层叠置定律。

Conclusion: GAN在针对特定地质结构的训练数据集上表现出比预期更强的鲁棒性，利用地质原理如地层叠置定律来指导深度生成模型具有很大潜力。

Abstract: The distribution of resources in the subsurface is deeply linked to the
variations of its physical properties. Generative modeling has long been used
to predict those physical properties while quantifying the associated
uncertainty. But current approaches struggle to properly reproduce geological
structures, and fluvial deposits in particular, because of their continuity.
This study explores whether a generative adversarial network (GAN) - a type of
deep-learning algorithm for generative modeling - can be trained to reproduce
fluvial deposits simulated by a process-based model - a more expensive model
that mimics geological processes. An ablation study shows that developments
from the deep-learning community to generate large 2D images are directly
transferable to 3D images of fluvial deposits. Training remains stable, and the
generated samples reproduce the non-stationarity and details of the deposits
without mode collapse or pure memorization of the training data. Using a
process-based model to generate those training data allows us to include
valuable properties other than the usual physical properties. We show how the
deposition time let us monitor and validate the performance of a GAN by
checking that its samples honor the law of superposition. Our work joins a
series of previous studies suggesting that GANs are more robust that given
credit for, at least for training datasets targeting specific geological
structures. Whether this robustness transfers to larger 3D images and
multimodal datasets remains to be seen. Exploring how deep generative models
can leverage geological principles like the law of superposition shows a lot of
promise.

</details>


### [113] [Feature Selection and Regularization in Multi-Class Classification: An Empirical Study of One-vs-Rest Logistic Regression with Gradient Descent Optimization and L1 Sparsity Constraints](https://arxiv.org/abs/2510.14449)
*Jahidul Arafat,Fariha Tasmin,Md Kaosar Uddin,Sanjaya Poudel,Eftakhar Ahmed Arnob*

Main category: cs.LG

TL;DR: 该论文对UCI葡萄酒数据集进行多分类研究，比较手动梯度下降与scikit-learn优化器的性能，分析L1正则化对特征稀疏性的影响，提出最优5特征子集实现62%复杂度降低和92-94%准确率。


<details>
  <summary>Details</summary>
Motivation: 解决葡萄酒分类中模型准确性、特征维度和可解释性之间的权衡问题，为分析化学领域的生产部署提供实用指导。

Method: 使用One-vs-Rest逻辑回归，比较手动梯度下降实现与scikit-learn优化求解器，量化L1正则化对特征稀疏性的影响。

Result: 手动梯度下降达到92.59%测试准确率，scikit-learn提供24倍训练加速和98.15%准确率。L1正则化实现54-69%特征减少，仅损失4.63%准确率。最优5特征子集实现62%复杂度降低，估计92-94%准确率。

Conclusion: 研究为资源受限环境中的化学分析提供了可操作的指导方针，在全面化学分析与针对性特征测量之间找到平衡点。

Abstract: Multi-class wine classification presents fundamental trade-offs between model
accuracy, feature dimensionality, and interpretability - critical factors for
production deployment in analytical chemistry. This paper presents a
comprehensive empirical study of One-vs-Rest logistic regression on the UCI
Wine dataset (178 samples, 3 cultivars, 13 chemical features), comparing
from-scratch gradient descent implementation against scikit-learn's optimized
solvers and quantifying L1 regularization effects on feature sparsity. Manual
gradient descent achieves 92.59 percent mean test accuracy with smooth
convergence, validating theoretical foundations, though scikit-learn provides
24x training speedup and 98.15 percent accuracy. Class-specific analysis
reveals distinct chemical signatures with heterogeneous patterns where color
intensity varies dramatically (0.31 to 16.50) across cultivars. L1
regularization produces 54-69 percent feature reduction with only 4.63 percent
accuracy decrease, demonstrating favorable interpretability-performance
trade-offs. We propose an optimal 5-feature subset achieving 62 percent
complexity reduction with estimated 92-94 percent accuracy, enabling
cost-effective deployment with 80 dollars savings per sample and 56 percent
time reduction. Statistical validation confirms robust generalization with
sub-2ms prediction latency suitable for real-time quality control. Our findings
provide actionable guidelines for practitioners balancing comprehensive
chemical analysis against targeted feature measurement in resource-constrained
environments.

</details>


### [114] [Coder as Editor: Code-driven Interpretable Molecular Optimization](https://arxiv.org/abs/2510.14455)
*Wenyu Zhu,Chengzhu Li,Xiaohe Tian,Yifan Wang,Yinjun Jia,Jianhui Wang,Bowen Gao,Ya-Qin Zhang,Wei-Ying Ma,Yanyan Lan*

Main category: cs.LG

TL;DR: MECo框架通过将编辑意图转化为可执行代码，解决LLMs在分子优化中执行修改的困难，显著提高编辑准确性和优化成功率。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在分子优化中虽然能生成高层次编辑意图，但在执行修改时存在困难，特别是在处理非直观表示如SMILES时。需要一种方法能准确地将编辑意图转化为结构修改。

Method: 提出MECo框架，采用级联方法：首先生成人类可理解的编辑意图，然后通过代码生成将这些意图转化为可执行的结构编辑。

Result: 在重现化学反应的编辑时达到98%以上准确率，在物理化学性质和靶点活性优化基准测试中，一致性提高38-86个百分点至90%以上，成功率高于基于SMILES的基线方法，同时保持结构相似性。

Conclusion: MECo通过对齐意图与执行，实现了一致、可控和可解释的分子设计，为药物发现中的高保真反馈循环和人机协作工作流程奠定了基础。

Abstract: Molecular optimization is a central task in drug discovery that requires
precise structural reasoning and domain knowledge. While large language models
(LLMs) have shown promise in generating high-level editing intentions in
natural language, they often struggle to faithfully execute these
modifications-particularly when operating on non-intuitive representations like
SMILES. We introduce MECo, a framework that bridges reasoning and execution by
translating editing actions into executable code. MECo reformulates molecular
optimization for LLMs as a cascaded framework: generating human-interpretable
editing intentions from a molecule and property goal, followed by translating
those intentions into executable structural edits via code generation. Our
approach achieves over 98% accuracy in reproducing held-out realistic edits
derived from chemical reactions and target-specific compound pairs. On
downstream optimization benchmarks spanning physicochemical properties and
target activities, MECo substantially improves consistency by 38-86 percentage
points to 90%+ and achieves higher success rates over SMILES-based baselines
while preserving structural similarity. By aligning intention with execution,
MECo enables consistent, controllable and interpretable molecular design,
laying the foundation for high-fidelity feedback loops and collaborative
human-AI workflows in drug discovery.

</details>


### [115] [Holdout-Loss-Based Data Selection for LLM Finetuning via In-Context Learning](https://arxiv.org/abs/2510.14459)
*Ling Zhang,Xianliang Yang,Juwon Yu,Park Cheonyoung,Lei Song,Jiang Bian*

Main category: cs.LG

TL;DR: 提出了一种基于上下文近似(ICA)的理论框架，用于高效选择和重加权训练数据，无需参考模型或额外微调，在多种对齐方法中都能提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前微调大型预训练语言模型时，噪声或偏离目标的训练样本会稀释监督信号，而系统化、高效识别高价值训练数据的方法仍待探索。

Method: 使用In-Context Approximation(ICA)通过在小规模精选holdout集上条件化来估计候选样本的holdout损失，基于局部线性化理论推导出数据价值代理，并动态重加权梯度更新。

Result: 在SFT、DPO和SimPO等多种对齐方法以及不同骨干模型和数据集上，ICA重加权都能一致提升模型对齐性能，且开销极小。

Conclusion: ICA框架为数据选择和重加权提供了理论基础和高效实现，但需要注意在快速漂移的在线策略更新场景下的局限性。

Abstract: Fine-tuning large pretrained language models is a common approach for
aligning them with human preferences, but noisy or off-target examples can
dilute supervision. While small, well-chosen datasets often match the
performance of much larger ones, systematic and efficient ways to identify
high-value training data remain underexplored. Many current methods rely on
heuristics or expensive retraining. We present a theoretically grounded,
resource-efficient framework for data selection and reweighting. At its core is
an In-Context Approximation (ICA) that estimates the holdout loss a model would
incur after training on a candidate example by conditioning on a small, curated
holdout set in context. ICA requires no reference model and no additional
finetuning. Under a local linearization, ICA is equivalent to a first-order
update toward the holdout optimum, motivating its use as a proxy for data
value. We derive per-example weights from ICA scores, dynamically reweighting
gradient updates as model parameters evolve. Across SFT, DPO, and SimPO, and
over diverse backbones and datasets, ICA-based reweighting consistently
improves model alignment with minimal overhead. We analyze sensitivity to score
update frequency and the choice of $k$ holdout examples for in-context
demonstrations, and note limitations for rapidly drifting on-policy updates,
highlighting directions for future work. Code and prompts will be released.

</details>


### [116] [From Guess2Graph: When and How Can Unreliable Experts Safely Boost Causal Discovery in Finite Samples?](https://arxiv.org/abs/2510.14488)
*Sujai Hiremath,Dominik Janzing,Philipp Faller,Patrick Blöbaum,Elke Kirschbaum,Shiva Prasad Kasiviswanathan,Kyra Gan*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Causal discovery algorithms often perform poorly with limited samples. While
integrating expert knowledge (including from LLMs) as constraints promises to
improve performance, guarantees for existing methods require perfect
predictions or uncertainty estimates, making them unreliable for practical use.
We propose the Guess2Graph (G2G) framework, which uses expert guesses to guide
the sequence of statistical tests rather than replacing them. This maintains
statistical consistency while enabling performance improvements. We develop two
instantiations of G2G: PC-Guess, which augments the PC algorithm, and
gPC-Guess, a learning-augmented variant designed to better leverage
high-quality expert input. Theoretically, both preserve correctness regardless
of expert error, with gPC-Guess provably outperforming its non-augmented
counterpart in finite samples when experts are "better than random."
Empirically, both show monotonic improvement with expert accuracy, with
gPC-Guess achieving significantly stronger gains.

</details>


### [117] [Learning to Undo: Rollback-Augmented Reinforcement Learning with Reversibility Signals](https://arxiv.org/abs/2510.14503)
*Andrejs Sorstkins,Omer Tariq,Muhammad Bilal*

Main category: cs.LG

TL;DR: 提出可逆学习框架，通过状态可逆性度量和选择性回滚机制，提升基于价值强化学习的鲁棒性和效率，显著减少灾难性错误并提高性能。


<details>
  <summary>Details</summary>
Motivation: 解决基于价值的强化学习代理在部分不可逆环境中容易产生价值高估和不稳定的问题，特别是在面对高风险决策时容易导致灾难性后果。

Method: 使用经验推导的状态-动作可逆性度量Phi(s,a)来量化在固定时间范围内返回先前状态的可能性，并引入选择性状态回滚操作，当动作的预期回报显著低于瞬时估计值时触发回滚。

Result: 在CliffWalking v0环境中，灾难性跌落减少超过99.8%，平均回合回报提高55%；在Taxi v3环境中，非法动作抑制率≥99.9%，累积奖励提升65.7%，两个环境的奖励方差均显著降低。

Conclusion: 回滚机制是实现安全性和性能提升的关键组件，该方法为安全可靠的序列决策提供了稳健的解决方案。

Abstract: This paper proposes a reversible learning framework to improve the robustness
and efficiency of value based Reinforcement Learning agents, addressing
vulnerability to value overestimation and instability in partially irreversible
environments. The framework has two complementary core mechanisms: an
empirically derived transition reversibility measure called Phi of s and a, and
a selective state rollback operation. We introduce an online per state action
estimator called Phi that quantifies the likelihood of returning to a prior
state within a fixed horizon K. This measure is used to adjust the penalty term
during temporal difference updates dynamically, integrating reversibility
awareness directly into the value function. The system also includes a
selective rollback operator. When an action yields an expected return markedly
lower than its instantaneous estimated value and violates a predefined
threshold, the agent is penalized and returns to the preceding state rather
than progressing. This interrupts sub optimal high risk trajectories and avoids
catastrophic steps. By combining reversibility aware evaluation with targeted
rollback, the method improves safety, performance, and stability. In the
CliffWalking v0 domain, the framework reduced catastrophic falls by over 99.8
percent and yielded a 55 percent increase in mean episode return. In the Taxi
v3 domain, it suppressed illegal actions by greater than or equal to 99.9
percent and achieved a 65.7 percent improvement in cumulative reward, while
also sharply reducing reward variance in both environments. Ablation studies
confirm that the rollback mechanism is the critical component underlying these
safety and performance gains, marking a robust step toward safe and reliable
sequential decision making.

</details>


### [118] [Enhancing Time Series Forecasting through Selective Representation Spaces: A Patch Perspective](https://arxiv.org/abs/2510.14510)
*Xingjian Wu,Xiangfei Qiu,Hanyin Cheng,Zhengyu Li,Jilin Hu,Chenjuan Guo,Bin Yang*

Main category: cs.LG

TL;DR: 提出选择性表示空间(SRS)模块，通过可学习的选择性分块和动态重组技术，自适应地选择和重排时间序列中的信息块，提升基于分块的时间序列预测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统分块方法将时间序列划分为相邻块，导致表示空间固定，表示表达能力不足。需要构建选择性表示空间来灵活包含最具信息量的分块。

Method: 提出SRS模块，包含可学习的选择性分块和动态重组技术，自适应地选择和重排上下文时间序列中的分块。构建SRSNet模型，由SRS模块和MLP头组成。

Result: SRSNet在多个领域的真实数据集上实现了最先进的性能。SRS作为即插即用模块也能提升现有基于分块模型的性能。

Conclusion: 选择性表示空间方法能有效提升时间序列预测性能，SRS模块具有通用性和实用性。

Abstract: Time Series Forecasting has made significant progress with the help of
Patching technique, which partitions time series into multiple patches to
effectively retain contextual semantic information into a representation space
beneficial for modeling long-term dependencies. However, conventional patching
partitions a time series into adjacent patches, which causes a fixed
representation space, thus resulting in insufficiently expressful
representations. In this paper, we pioneer the exploration of constructing a
selective representation space to flexibly include the most informative patches
for forecasting. Specifically, we propose the Selective Representation Space
(SRS) module, which utilizes the learnable Selective Patching and Dynamic
Reassembly techniques to adaptively select and shuffle the patches from the
contextual time series, aiming at fully exploiting the information of
contextual time series to enhance the forecasting performance of patch-based
models. To demonstrate the effectiveness of SRS module, we propose a simple yet
effective SRSNet consisting of SRS and an MLP head, which achieves
state-of-the-art performance on real-world datasets from multiple domains.
Furthermore, as a novel plugin-and-play module, SRS can also enhance the
performance of existing patch-based models. The resources are available at
https://github.com/decisionintelligence/SRSNet.

</details>


### [119] [On the Identifiability of Tensor Ranks via Prior Predictive Matching](https://arxiv.org/abs/2510.14523)
*Eliezer da Silva,Arto Klami,Diego Mesquita,Iñigo Urteaga*

Main category: cs.LG

TL;DR: 本文提出了一种基于先验预测矩匹配的严格方法来确定概率张量模型中的秩可识别性，将矩匹配条件转化为对数线性方程组，并建立了秩可识别性与系统可解性之间的等价关系。


<details>
  <summary>Details</summary>
Motivation: 张量分解中潜在维度（秩）的选择通常依赖启发式方法，缺乏严格的数学基础，因此需要一种严谨的方法来确定秩的可识别性。

Method: 通过将矩匹配条件转化为关于边际矩、先验超参数和秩的对数线性方程组，建立秩可识别性与系统可解性之间的等价关系，并应用于四种基础张量模型。

Result: 证明PARAFAC/CP、Tensor Train和Tensor Ring模型的秩是可识别的，而Tucker模型的秩由于对称拓扑导致系统欠定而不可识别。对于可识别模型，推导了基于观测数据矩的显式闭式秩估计器。

Conclusion: 该方法为张量分解中的秩选择提供了严格的数学基础，通过矩匹配方法成功识别了三种张量模型的秩，并验证了估计器的有效性。

Abstract: Selecting the latent dimensions (ranks) in tensor factorization is a central
challenge that often relies on heuristic methods. This paper introduces a
rigorous approach to determine rank identifiability in probabilistic tensor
models, based on prior predictive moment matching. We transform a set of moment
matching conditions into a log-linear system of equations in terms of marginal
moments, prior hyperparameters, and ranks; establishing an equivalence between
rank identifiability and the solvability of such system. We apply this
framework to four foundational tensor-models, demonstrating that the linear
structure of the PARAFAC/CP model, the chain structure of the Tensor Train
model, and the closed-loop structure of the Tensor Ring model yield solvable
systems, making their ranks identifiable. In contrast, we prove that the
symmetric topology of the Tucker model leads to an underdetermined system,
rendering the ranks unidentifiable by this method. For the identifiable models,
we derive explicit closed-form rank estimators based on the moments of observed
data only. We empirically validate these estimators and evaluate the robustness
of the proposal.

</details>


### [120] [Agentic Entropy-Balanced Policy Optimization](https://arxiv.org/abs/2510.14545)
*Guanting Dong,Licheng Bao,Zhongyuan Wang,Kangzhi Zhao,Xiaoxi Li,Jiajie Jin,Jinghan Yang,Hangyu Mao,Fuzheng Zhang,Kun Gai,Guorui Zhou,Yutao Zhu,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.LG

TL;DR: 提出了Agentic Entropy-Balanced Policy Optimization (AEPO)算法，通过平衡熵信号来解决Agentic RL训练中的崩溃问题，在14个数据集上优于7种主流RL算法。


<details>
  <summary>Details</summary>
Motivation: 主流Agentic RL算法过度依赖熵信号进行探索，导致训练崩溃问题，需要平衡熵在rollout和策略更新阶段的影响。

Method: AEPO包含两个核心组件：动态熵平衡rollout机制（通过熵预监测分配采样预算，并对连续高熵步骤施加分支惩罚）和熵平衡策略优化（在高熵裁剪项中插入停止梯度操作，并采用熵感知优势估计）。

Result: 在14个数据集上优于7种主流RL算法。Qwen3-14B仅用1K RL样本就取得了显著成果：GAIA Pass@1 47.6%，Humanity's Last Exam 11.2%，WebWalker Pass@1 43.0%；Pass@5指标分别为65.0%、26.0%和70.0%。

Conclusion: AEPO提高了rollout采样多样性，同时保持稳定的策略熵，有助于可扩展的web agent训练。

Abstract: Recently, Agentic Reinforcement Learning (Agentic RL) has made significant
progress in incentivizing the multi-turn, long-horizon tool-use capabilities of
web agents. While mainstream agentic RL algorithms autonomously explore
high-uncertainty tool-call steps under the guidance of entropy, excessive
reliance on entropy signals can impose further constraints, leading to the
training collapse. In this paper, we delve into the challenges caused by
entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an
agentic RL algorithm designed to balance entropy in both the rollout and policy
update phases. AEPO comprises two core components: (1) a dynamic
entropy-balanced rollout mechanism that adaptively allocate global and branch
sampling budget through entropy pre-monitoring, while imposing a branch penalty
on consecutive high-entropy tool-call steps to prevent over-branching issues;
and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient
operation into the high-entropy clipping term to preserve and properly rescale
gradients on high-entropy tokens, while incorporating entropy-aware advantage
estimation to prioritize learning on high-uncertainty tokens. Results across 14
challenging datasets show that AEPO consistently outperforms 7 mainstream RL
algorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive
results: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker
for Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on
WebWalker for Pass@5. Further analysis reveals that AEPO improves rollout
sampling diversity while maintaining stable policy entropy, facilitating
scalable web agent training.

</details>


### [121] [MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving](https://arxiv.org/abs/2510.14557)
*Jungi Lee,Junyong Park,Soohyun Cha,Jaehoon Cho,Jaewoong Sim*

Main category: cs.LG

TL;DR: 本文提出MX+格式，通过重新利用异常值的指数字段作为扩展尾数，解决了4位块浮点格式在大型语言模型推理中的精度问题，在几乎不增加存储开销和延迟的情况下显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的低比特块浮点格式由于块中的异常值问题，难以在大型语言模型推理中提供合理的性能表现，需要一种非侵入式且成本效益高的解决方案。

Method: 提出MX+格式，基于关键洞察：异常值不需要使用其元素数据类型中的指数字段，因此可以将指数字段重新用作扩展尾数，从而提高异常值元素的精度。

Result: 评估显示MX+相比4位MX格式（MXFP4）实现了显著更高的模型性能，同时存储开销和速度下降可以忽略不计。

Conclusion: MX+为高效LLM推理提供了比MXFP4或MXFP6更具吸引力的替代方案。

Abstract: Reduced-precision data formats are crucial for cost-effective serving of
large language models (LLMs). While numerous reduced-precision formats have
been introduced thus far, they often require intrusive modifications to the
software frameworks or are rather unconventional for widespread adoption across
hardware vendors. In this paper, we instead focus on recent industry-driven
variants of block floating-point (BFP) formats and conduct a comprehensive
analysis to push their limits for efficient LLM serving. Our analysis shows
that existing ultra low-bit BFP variants struggle to provide reasonable
language model performance due to outlier values in blocks. To address the
outliers with BFPs, we propose MX+, a cost-effective and non-intrusive
extension designed for seamless integration into the microscaling (MX) formats.
MX+ builds on the key insight that the outlier does not need to use its
exponent field in the element data type, which allows us to repurpose the
exponent field as an extended mantissa to increase the precision of the outlier
element. Our evaluation shows that MX+ achieves significantly higher model
performance compared to the 4-bit MX format (MXFP4) with negligible storage
overhead and slowdown, thus offering a compelling alternative to MXFP4 or MXFP6
for efficient LLM inference.

</details>


### [122] [Redundancy-Aware Test-Time Graph Out-of-Distribution Detection](https://arxiv.org/abs/2510.14562)
*Yue Hou,He Zhu,Ruomei Liu,Yingke Su,Junran Wu,Ke Xu*

Main category: cs.LG

TL;DR: RedOUT是一个无监督的图OOD检测框架，通过结构熵和冗余感知图信息瓶颈来减少结构冗余，提升图分类中的分布外检测性能。


<details>
  <summary>Details</summary>
Motivation: 训练数据和测试数据之间的分布差异导致模型在遇到分布外样本时预测不准确，现有图OOD检测方法因结构冗余导致语义偏移而性能受限。

Method: 提出RedOUT框架，集成结构熵到测试时OOD检测中，引入冗余感知图信息瓶颈(ReGIB)，将目标分解为本质信息和无关冗余，通过最小化结构熵减少解耦的冗余。

Result: 在真实世界数据集上的广泛实验显示RedOUT在OOD检测上表现优异，平均提升6.7%，在ClinTox/LIPO数据集对上显著超越最佳竞争对手17.3%。

Conclusion: RedOUT通过结构熵和ReGIB有效减少了图结构冗余，显著提升了图分类中的OOD检测性能。

Abstract: Distributional discrepancy between training and test data can lead models to
make inaccurate predictions when encountering out-of-distribution (OOD) samples
in real-world applications. Although existing graph OOD detection methods
leverage data-centric techniques to extract effective representations, their
performance remains compromised by structural redundancy that induces semantic
shifts. To address this dilemma, we propose RedOUT, an unsupervised framework
that integrates structural entropy into test-time OOD detection for graph
classification. Concretely, we introduce the Redundancy-aware Graph Information
Bottleneck (ReGIB) and decompose the objective into essential information and
irrelevant redundancy. By minimizing structural entropy, the decoupled
redundancy is reduced, and theoretically grounded upper and lower bounds are
proposed for optimization. Extensive experiments on real-world datasets
demonstrate the superior performance of RedOUT on OOD detection. Specifically,
our method achieves an average improvement of 6.7%, significantly surpassing
the best competitor by 17.3% on the ClinTox/LIPO dataset pair.

</details>


### [123] [State-Space Models for Tabular Prior-Data Fitted Networks](https://arxiv.org/abs/2510.14573)
*Felix Koch,Marcel Wever,Fabian Raisch,Benjamin Tischler*

Main category: cs.LG

TL;DR: 本研究探索使用Hydra（一种双向线性时间结构化状态空间模型）替代TabPFN中的Transformer，以解决Transformer的二次复杂度问题，同时通过双向方法减少对输入顺序的敏感性。


<details>
  <summary>Details</summary>
Motivation: Transformer在表格数据中具有二次复杂度问题，而状态空间模型（SSM）虽然效率更高，但对输入顺序敏感，这在行顺序无语义意义的表格数据中是不期望的特性。

Method: 采用Hydra双向线性时间结构化状态空间模型作为Transformer的替代方案，通过双向方法实现对称上下文聚合，减少对输入顺序的依赖。

Result: 该方法显著降低了顺序依赖性，预测性能与原TabPFN模型相当。

Conclusion: Hydra SSM在保持效率的同时，通过双向方法有效解决了顺序敏感性问题，在表格数据中展现出与Transformer竞争的性能。

Abstract: Recent advancements in foundation models for tabular data, such as TabPFN,
demonstrated that pretrained Transformer architectures can approximate Bayesian
inference with high predictive performance. However, Transformers suffer from
quadratic complexity with respect to sequence length, motivating the
exploration of more efficient sequence models. In this work, we investigate the
potential of using Hydra, a bidirectional linear-time structured state space
model (SSM), as an alternative to Transformers in TabPFN. A key challenge lies
in SSM's inherent sensitivity to the order of input tokens - an undesirable
property for tabular datasets where the row order is semantically meaningless.
We investigate to what extent a bidirectional approach can preserve efficiency
and enable symmetric context aggregation. Our experiments show that this
approach reduces the order-dependence, achieving predictive performance
competitive to the original TabPFN model.

</details>


### [124] [Selective Labeling with False Discovery Rate Control](https://arxiv.org/abs/2510.14581)
*Huipeng Huang,Wenbo Liao,Huajun Xi,Hao Zeng,Mengchen Zhao,Hongxin Wei*

Main category: cs.LG

TL;DR: 提出了一种名为Conformal Labeling的新方法，通过控制错误发现率来识别AI预测可被证明可信的实例，确保AI分配标签的质量。


<details>
  <summary>Details</summary>
Motivation: 解决AI模型标注数据时不可避免的标注错误问题，现有选择性标注方法缺乏对AI标注质量的理论保证。

Method: 构建每个测试实例的conformal p值，通过比较AI模型的预测置信度与校准实例的误标置信度，选择p值低于数据依赖阈值的实例。

Result: 实验表明该方法在各种任务（图像和文本标注、LLM问答）中实现了严格的FDR控制和高功率。

Conclusion: Conformal Labeling方法能够提供理论保证，确保AI分配标签的平均正确率，是解决AI标注质量问题的有效方案。

Abstract: Obtaining high-quality labels for large datasets is expensive, requiring
massive annotations from human experts. While AI models offer a cost-effective
alternative by predicting labels, their label quality is compromised by the
unavoidable labeling errors. Existing methods mitigate this issue through
selective labeling, where AI labels a subset and human labels the remainder.
However, these methods lack theoretical guarantees on the quality of
AI-assigned labels, often resulting in unacceptably high labeling error within
the AI-labeled subset. To address this, we introduce \textbf{Conformal
Labeling}, a novel method to identify instances where AI predictions can be
provably trusted. This is achieved by controlling the false discovery rate
(FDR), the proportion of incorrect labels within the selected subset. In
particular, we construct a conformal $p$-value for each test instance by
comparing AI models' predicted confidence to those of calibration instances
mislabeled by AI models. Then, we select test instances whose $p$-values are
below a data-dependent threshold, certifying AI models' predictions as
trustworthy. We provide theoretical guarantees that Conformal Labeling controls
the FDR below the nominal level, ensuring that a predefined fraction of
AI-assigned labels is correct on average. Extensive experiments demonstrate
that our method achieves tight FDR control with high power across various
tasks, including image and text labeling, and LLM QA.

</details>


### [125] [Matcha: Multi-Stage Riemannian Flow Matching for Accurate and Physically Valid Molecular Docking](https://arxiv.org/abs/2510.14586)
*Daria Frolova,Talgat Daulbaev,Egor Sevryugov,Sergei A. Nikolenko,Dmitry N. Ivankov,Ivan Oseledets,Marina A. Pak*

Main category: cs.LG

TL;DR: Matcha是一个新颖的分子对接流程，结合多阶段流匹配、学习评分和物理有效性过滤，在速度和准确性上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质-配体结合构象预测方法难以平衡速度、准确性和物理合理性

Method: 使用三个连续阶段的多阶段流匹配模型（在R³、SO(3)和SO(2)几何空间），结合学习评分模型和无监督物理有效性过滤

Result: 在Astex和PDBbind测试集上表现出优异的对接成功率和物理合理性，比现代大规模共折叠模型快约25倍

Conclusion: Matcha提供了一种高效准确的分子对接解决方案，在速度和性能上均有显著优势

Abstract: Accurate prediction of protein-ligand binding poses is crucial for
structure-based drug design, yet existing methods struggle to balance speed,
accuracy, and physical plausibility. We introduce Matcha, a novel molecular
docking pipeline that combines multi-stage flow matching with learned scoring
and physical validity filtering. Our approach consists of three sequential
stages applied consecutively to refine docking predictions, each implemented as
a flow matching model operating on appropriate geometric spaces
($\mathbb{R}^3$, $\mathrm{SO}(3)$, and $\mathrm{SO}(2)$). We enhance the
prediction quality through a dedicated scoring model and apply unsupervised
physical validity filters to eliminate unrealistic poses. Compared to various
approaches, Matcha demonstrates superior performance on Astex and PDBbind test
sets in terms of docking success rate and physical plausibility. Moreover, our
method works approximately 25 times faster than modern large-scale co-folding
models. The model weights and inference code to reproduce our results are
available at https://github.com/LigandPro/Matcha.

</details>


### [126] [Multimodal RAG for Unstructured Data:Leveraging Modality-Aware Knowledge Graphs with Hybrid Retrieval](https://arxiv.org/abs/2510.14592)
*Rashmi R,Vidyadhar Upadhya*

Main category: cs.LG

TL;DR: MAHA是一种针对多模态问答的检索增强生成系统，通过模态感知知识图谱结合密集向量检索和结构化图遍历，有效处理包含文本、图像、表格等混合模态的文档。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统主要处理单模态文本数据，无法有效处理包含多种模态（文本、图像、表格、方程、图表）的非结构化多模态文档，这些不同模态都包含独特的信息。

Method: 提出模态感知混合检索架构(MAHA)，使用模态感知知识图谱编码跨模态语义和关系，结合密集向量检索和结构化图遍历，实现语义丰富且上下文感知的跨模态检索。

Result: 在多个基准数据集上的评估显示，MAHA显著优于基线方法，ROUGE-L得分达到0.486，实现了完整的模态覆盖。

Conclusion: MAHA通过将嵌入与显式文档结构相结合，建立了一个可扩展且可解释的检索框架，推进了RAG系统在非结构化多模态数据上的模态感知推理能力。

Abstract: Current Retrieval-Augmented Generation (RAG) systems primarily operate on
unimodal textual data, limiting their effectiveness on unstructured multimodal
documents. Such documents often combine text, images, tables, equations, and
graphs, each contributing unique information. In this work, we present a
Modality-Aware Hybrid retrieval Architecture (MAHA), designed specifically for
multimodal question answering with reasoning through a modality-aware knowledge
graph. MAHA integrates dense vector retrieval with structured graph traversal,
where the knowledge graph encodes cross-modal semantics and relationships. This
design enables both semantically rich and context-aware retrieval across
diverse modalities. Evaluations on multiple benchmark datasets demonstrate that
MAHA substantially outperforms baseline methods, achieving a ROUGE-L score of
0.486, providing complete modality coverage. These results highlight MAHA's
ability to combine embeddings with explicit document structure, enabling
effective multimodal retrieval. Our work establishes a scalable and
interpretable retrieval framework that advances RAG systems by enabling
modality-aware reasoning over unstructured multimodal data.

</details>


### [127] [First Attentions Last: Better Exploiting First Attentions for Efficient Transformer Training](https://arxiv.org/abs/2510.14614)
*Gyudong Kim,Hyukju Na,Jin Hyeon Kim,Hyunsung Jang,Jaemin Park,Jaegi Hwang,Namkoo Ha,Seungryong Kim,Young Geun Kim*

Main category: cs.LG

TL;DR: FAL是一种高效的transformer架构，通过重定向第一层MHA输出到后续层的MLP输入，消除了每块的MHA-MLP连接，减少了通信开销并实现了MHA和MLP在单GPU上的并行执行。


<details>
  <summary>Details</summary>
Motivation: 现有transformer设计在张量并行训练中存在显著的通信开销，特别是每块的MHA-MLP连接需要all-reduce通信。研究发现MHA-MLP连接可以绕过以提高效率。

Method: 提出FAL架构，将第一层注意力输出重定向到后续层的MLP输入，消除每块的MHA-MLP连接。FAL+版本额外将归一化的第一层注意力输出添加到后续层的MHA输出中，以增强MLP输入质量。

Result: FAL将多GPU训练时间减少高达44%，单GPU吞吐量提升1.18倍，困惑度优于基线GPT。FAL+在不增加训练时间的情况下实现了更低的困惑度。

Conclusion: FAL和FAL+通过消除MHA-MLP连接显著提高了训练效率，同时保持或改进了模型质量，为大规模transformer训练提供了有效的架构优化方案。

Abstract: As training billion-scale transformers becomes increasingly common, employing
multiple distributed GPUs along with parallel training methods has become a
standard practice. However, existing transformer designs suffer from
significant communication overhead, especially in Tensor Parallelism (TP),
where each block's MHA-MLP connection requires an all-reduce communication.
Through our investigation, we show that the MHA-MLP connections can be bypassed
for efficiency, while the attention output of the first layer can serve as an
alternative signal for the bypassed connection. Motivated by the observations,
we propose FAL (First Attentions Last), an efficient transformer architecture
that redirects the first MHA output to the MLP inputs of the following layers,
eliminating the per-block MHA-MLP connections. This removes the all-reduce
communication and enables parallel execution of MHA and MLP on a single GPU. We
also introduce FAL+, which adds the normalized first attention output to the
MHA outputs of the following layers to augment the MLP input for the model
quality. Our evaluation shows that FAL reduces multi-GPU training time by up to
44%, improves single-GPU throughput by up to 1.18x, and achieves better
perplexity compared to the baseline GPT. FAL+ achieves even lower perplexity
without increasing the training time than the baseline.

</details>


### [128] [LeapFactual: Reliable Visual Counterfactual Explanation Using Conditional Flow Matching](https://arxiv.org/abs/2510.14623)
*Zhuo Cao,Xuan Zhao,Lena Krieger,Hanno Scharr,Ira Assent*

Main category: cs.LG

TL;DR: LeapFactual是一种基于条件流匹配的新型反事实解释算法，能够生成可靠且信息丰富的反事实解释，即使真实和学习决策边界存在差异。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型在高风险领域（如医疗和科学研究）的应用日益增多，需要不仅准确而且可解释的模型。现有反事实解释方法存在梯度消失、潜在空间不连续以及过度依赖学习与真实决策边界对齐等局限性。

Method: 提出LeapFactual算法，基于条件流匹配，采用模型无关的方法，不限于具有可微分损失函数的模型，甚至可以处理人机协同系统。

Result: 在基准和真实数据集上的广泛实验表明，LeapFactual能够生成准确且符合分布的反事实解释，提供可操作的见解。可靠的反事实样本可以作为新的训练数据来增强模型性能。

Conclusion: 该方法具有广泛适用性，能够增强科学知识发现和非专家可解释性。

Abstract: The growing integration of machine learning (ML) and artificial intelligence
(AI) models into high-stakes domains such as healthcare and scientific research
calls for models that are not only accurate but also interpretable. Among the
existing explainable methods, counterfactual explanations offer
interpretability by identifying minimal changes to inputs that would alter a
model's prediction, thus providing deeper insights. However, current
counterfactual generation methods suffer from critical limitations, including
gradient vanishing, discontinuous latent spaces, and an overreliance on the
alignment between learned and true decision boundaries. To overcome these
limitations, we propose LeapFactual, a novel counterfactual explanation
algorithm based on conditional flow matching. LeapFactual generates reliable
and informative counterfactuals, even when true and learned decision boundaries
diverge. Following a model-agnostic approach, LeapFactual is not limited to
models with differentiable loss functions. It can even handle human-in-the-loop
systems, expanding the scope of counterfactual explanations to domains that
require the participation of human annotators, such as citizen science. We
provide extensive experiments on benchmark and real-world datasets showing that
LeapFactual generates accurate and in-distribution counterfactual explanations
that offer actionable insights. We observe, for instance, that our reliable
counterfactual samples with labels aligning to ground truth can be beneficially
used as new training data to enhance the model. The proposed method is broadly
applicable and enhances both scientific knowledge discovery and non-expert
interpretability.

</details>


### [129] [Galaxy Morphology Classification with Counterfactual Explanation](https://arxiv.org/abs/2510.14655)
*Zhuo Cao,Lena Krieger,Hanno Scharr,Ira Assent*

Main category: cs.LG

TL;DR: 提出了一种结合可逆流模型的编码器-解码器架构，用于星系形态分类，既保证预测性能又提供反事实解释。


<details>
  <summary>Details</summary>
Motivation: 星系形态研究对理解星系演化至关重要，但现有机器学习方法缺乏可解释性，难以理解模型决策过程。

Method: 在传统编码器-解码器架构中引入可逆流模型，能够生成反事实解释来揭示决策依据。

Result: 该方法不仅能获得良好的预测性能，还能提供关于决策过程的额外信息。

Conclusion: 提出的可逆流扩展架构解决了星系形态分类中的可解释性问题，为天文学研究提供了更透明的机器学习工具。

Abstract: Galaxy morphologies play an essential role in the study of the evolution of
galaxies. The determination of morphologies is laborious for a large amount of
data giving rise to machine learning-based approaches. Unfortunately, most of
these approaches offer no insight into how the model works and make the results
difficult to understand and explain. We here propose to extend a classical
encoder-decoder architecture with invertible flow, allowing us to not only
obtain a good predictive performance but also provide additional information
about the decision process with counterfactual explanations.

</details>


### [130] [Geometric Moment Alignment for Domain Adaptation via Siegel Embeddings](https://arxiv.org/abs/2510.14666)
*Shayan Gharib,Marcelo Hartmann,Arto Klami*

Main category: cs.LG

TL;DR: 提出了一种基于黎曼几何的无监督域自适应方法，通过Siegel嵌入将一阶和二阶矩表示为对称正定矩阵，在SPD矩阵流形上使用自然几何距离进行域对齐。


<details>
  <summary>Details</summary>
Motivation: 解决无监督域自适应中的分布偏移问题，现有方法使用临时相似性度量在嵌入空间中对齐源域和目标域的低阶统计矩，缺乏几何原理性。

Method: 将一阶和二阶矩通过Siegel嵌入表示为单个对称正定矩阵，在SPD矩阵共享流形上使用自然几何距离进行同时矩对齐，保持源域和目标域的均值和协方差结构。

Result: 在图像去噪和图像分类基准测试上验证了方法的有效性，将黎曼流形距离与目标域误差界联系起来。

Conclusion: 提出的几何自适应方法提供了更忠实的跨域比较度量，通过黎曼几何原理改进了无监督域自适应性能。

Abstract: We address the problem of distribution shift in unsupervised domain
adaptation with a moment-matching approach. Existing methods typically align
low-order statistical moments of the source and target distributions in an
embedding space using ad-hoc similarity measures. We propose a principled
alternative that instead leverages the intrinsic geometry of these
distributions by adopting a Riemannian distance for this alignment. Our key
novelty lies in expressing the first- and second-order moments as a single
symmetric positive definite (SPD) matrix through Siegel embeddings. This
enables simultaneous adaptation of both moments using the natural geometric
distance on the shared manifold of SPD matrices, preserving the mean and
covariance structure of the source and target distributions and yielding a more
faithful metric for cross-domain comparison. We connect the Riemannian manifold
distance to the target-domain error bound, and validate the method on image
denoising and image classification benchmarks. Our code is publicly available
at https://github.com/shayangharib/GeoAdapt.

</details>


### [131] [Online Reliable Anomaly Detection via Neuromorphic Sensing and Communications](https://arxiv.org/abs/2510.14688)
*Junya Shiraishi,Jiechen Chen,Osvaldo Simeone,Petar Popovski*

Main category: cs.LG

TL;DR: 提出基于神经形态无线传感器网络的低功耗在线异常检测框架，通过事件驱动的脉冲信号传输和动态传感器查询策略，在严格控制误报率的同时实现高效异常检测。


<details>
  <summary>Details</summary>
Motivation: 针对脑机接口和远程环境监测等应用场景，需要低功耗、高可靠性的在线异常检测系统，能够处理事件驱动的神经形态传感器数据并控制检测误报率。

Method: 采用在线假设检验方法结合e值来维持误报率控制，将传感器查询策略建模为多臂老虎机框架中的最佳臂识别问题，动态优化传感器选择。

Result: 性能评估表明该方法能够在严格的误报率要求下可靠检测异常，同时高效调度传感器通信并实现低检测延迟。

Conclusion: 所提出的框架为神经形态无线传感器网络提供了一种有效的在线异常检测解决方案，在功耗、可靠性和延迟方面均表现出色。

Abstract: This paper proposes a low-power online anomaly detection framework based on
neuromorphic wireless sensor networks, encompassing possible use cases such as
brain-machine interfaces and remote environmental monitoring. In the considered
system, a central reader node actively queries a subset of neuromorphic sensor
nodes (neuro-SNs) at each time frame. The neuromorphic sensors are
event-driven, producing spikes in correspondence to relevant changes in the
monitored system. The queried neuro-SNs respond to the reader with impulse
radio (IR) transmissions that directly encode the sensed local events. The
reader processes these event-driven signals to determine whether the monitored
environment is in a normal or anomalous state, while rigorously controlling the
false discovery rate (FDR) of detections below a predefined threshold. The
proposed approach employs an online hypothesis testing method with e-values to
maintain FDR control without requiring knowledge of the anomaly rate, and it
dynamically optimizes the sensor querying strategy by casting it as a best-arm
identification problem in a multi-armed bandit framework. Extensive performance
evaluation demonstrates that the proposed method can reliably detect anomalies
under stringent FDR requirements, while efficiently scheduling sensor
communications and achieving low detection latency.

</details>


### [132] [FedPPA: Progressive Parameter Alignment for Personalized Federated Learning](https://arxiv.org/abs/2510.14698)
*Maulidi Adi Prasetia,Muhamad Risqi U. Saputra,Guntur Dharma Putra*

Main category: cs.LG

TL;DR: 提出FedPPA方法，通过渐进式参数对齐解决联邦学习中模型和数据异构性问题，在非IID数据下提升个性化性能


<details>
  <summary>Details</summary>
Motivation: 现有个性化联邦学习方法通常忽略客户端计算能力差异导致的模型和数据异构性共存问题

Method: 渐进式参数对齐(FedPPA)方法，逐步对齐客户端与全局模型的公共层权重，并集成基于熵的加权平均

Result: 在MNIST、FMNIST和CIFAR-10数据集上的实验表明，FedPPA持续优于现有FL算法，在个性化适应方面表现优异

Conclusion: FedPPA能有效缓解客户端更新时全局与局部模型的不一致性，同时保留客户端本地知识，增强非IID环境下的个性化鲁棒性

Abstract: Federated Learning (FL) is designed as a decentralized, privacy-preserving
machine learning paradigm that enables multiple clients to collaboratively
train a model without sharing their data. In real-world scenarios, however,
clients often have heterogeneous computational resources and hold
non-independent and identically distributed data (non-IID), which poses
significant challenges during training. Personalized Federated Learning (PFL)
has emerged to address these issues by customizing models for each client based
on their unique data distribution. Despite its potential, existing PFL
approaches typically overlook the coexistence of model and data heterogeneity
arising from clients with diverse computational capabilities. To overcome this
limitation, we propose a novel method, called Progressive Parameter Alignment
(FedPPA), which progressively aligns the weights of common layers across
clients with the global model's weights. Our approach not only mitigates
inconsistencies between global and local models during client updates, but also
preserves client's local knowledge, thereby enhancing personalization
robustness in non-IID settings. To further enhance the global model performance
while retaining strong personalization, we also integrate entropy-based
weighted averaging into the FedPPA framework. Experiments on three image
classification datasets, including MNIST, FMNIST, and CIFAR-10, demonstrate
that FedPPA consistently outperforms existing FL algorithms, achieving superior
performance in personalized adaptation.

</details>


### [133] [Seesaw: Accelerating Training by Balancing Learning Rate and Batch Size Scheduling](https://arxiv.org/abs/2510.14717)
*Alexandru Meterez,Depen Morwani,Jingfeng Wu,Costin-Andrei Oncescu,Cengiz Pehlevan,Sham Kakade*

Main category: cs.LG

TL;DR: 提出Seesaw方法，在自适应优化器中通过同时调整学习率和批量大小来加速训练，减少串行步骤的同时保持损失动态。


<details>
  <summary>Details</summary>
Motivation: 批量大小调整是加速大语言模型预训练的有效策略，但对于Adam等自适应优化器的最优策略尚不明确，通常需要启发式调参。

Method: 开发Seesaw框架：当标准调度器将学习率减半时，Seesaw将学习率乘以1/√2并加倍批量大小，保持损失动态同时减少串行步骤。

Result: 在150M/300M/600M参数模型上，Seesaw在相同FLOPs下与余弦衰减匹配，同时减少约36%的挂钟时间，接近理论极限。

Conclusion: Seesaw提供了一种原则性的批量大小调度方法，在保持训练质量的同时显著加速自适应优化器的训练过程。

Abstract: Increasing the batch size during training -- a ''batch ramp'' -- is a
promising strategy to accelerate large language model pretraining. While for
SGD, doubling the batch size can be equivalent to halving the learning rate,
the optimal strategy for adaptive optimizers like Adam is less clear. As a
result, any batch-ramp scheduling, if used at all, is typically tuned
heuristically. This work develops a principled framework for batch-size
scheduling and introduces Seesaw: whenever a standard scheduler would halve the
learning rate, Seesaw instead multiplies it by $1/\sqrt{2}$ and doubles the
batch size, preserving loss dynamics while reducing serial steps.
Theoretically, we provide, to our knowledge, the first finite-sample proof of
equivalence between learning-rate decay and batch-size ramp-up for SGD on noisy
linear regression, and we extend this equivalence to normalized SGD, a
tractable proxy for Adam, under a variance-dominated regime observed in
practice. Empirically, on 150M/300M/600M-parameter models trained at Chinchilla
scale using a constant (critical) batch size, Seesaw matches cosine decay at
equal FLOPs while reducing wall-clock time by $\approx 36\%$, approaching the
theoretical limit implied by our analysis.

</details>


### [134] [Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous References](https://arxiv.org/abs/2510.14719)
*Hongzheng Chen,Bin Fan,Alexander Collins,Bastian Hagedorn,Evghenii Gaburov,Masahiro Masuda,Matthew Brookhart,Chris Sullivan,Jason Knight,Zhiru Zhang,Vinod Grover*

Main category: cs.LG

TL;DR: Tawa是一个自动化编译器，能从高级瓦片化程序生成高性能的warp专业化代码，解决了GPU SIMT编程模型与任务并行硬件之间的不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现代GPU具有支持高性能异步数据流执行的专用硬件单元，但传统的SIMT编程模型与这种任务并行硬件存在根本性不匹配，导致显著的可编程性差距。硬件级的warp专业化虽然能解锁峰值性能，但需要开发者手动编排复杂的低级通信和软件管道，这过程劳动密集、容易出错且不可持续。

Method: Tawa采用新颖的IR抽象——异步引用(aref)，在不暴露低级硬件细节的情况下表达warp级通信。使用这种抽象，Tawa自动将程序划分为生产者-消费者角色，并管理复杂的数据流管道，免除了开发者进行侵入式内核重写的负担。

Result: 在NVIDIA H100 GPU上的评估显示，Tawa在代表性LLM内核中实现了高硬件利用率，相比高度优化的cuBLAS GEMM内核实现了1.1倍加速。对于注意力工作负载，Tawa相比Triton实现了1.2倍加速，并与手工优化的CUTLASS C++ FlashAttention-3内核性能相当，但编程工作量大大减少。

Conclusion: Tawa通过自动化编译器方法成功弥合了GPU编程模型与硬件能力之间的差距，在保持高性能的同时显著降低了编程复杂性，为高效利用现代GPU的异步数据流执行能力提供了可行解决方案。

Abstract: Modern GPUs feature specialized hardware units that enable high-performance,
asynchronous dataflow execution. However, the conventional SIMT programming
model is fundamentally misaligned with this task-parallel hardware, creating a
significant programmability gap. While hardware-level warp specialization is
the key to unlocking peak performance, it forces developers to manually
orchestrate complex, low-level communication and software pipelines--a process
that is labor-intensive, error-prone, and unsustainable. To address this
challenge, we present Tawa, an automated compiler that systematically generates
high-performance, warp-specialized code from a high-level, tile-based program.
Central to our approach is a novel IR abstraction, asynchronous references
(aref), which expresses warp-level communication without exposing low-level
hardware details. Using this abstraction, Tawa automatically partitions
programs into producer-consumer roles and manages the intricate dataflow
pipeline, relieving developers of invasive kernel rewriting. Evaluation on
NVIDIA H100 GPUs across representative LLM kernels shows that Tawa delivers
high hardware utilization, achieving up to 1.1$\times$ speedup over highly
optimized cuBLAS GEMM kernels. For attention workloads, Tawa attains
1.2$\times$ speedup over Triton and matches the performance of the
hand-optimized CUTLASS C++ FlashAttention-3 kernel with far less programming
effort.

</details>


### [135] [The Pursuit of Diversity: Multi-Objective Testing of Deep Reinforcement Learning Agents](https://arxiv.org/abs/2510.14727)
*Antony Bartlett,Cynthia Liem,Annibale Panichella*

Main category: cs.LG

TL;DR: INDAGO-Nexus是一个多目标搜索方法，用于发现深度强化学习代理的多样化故障场景，相比单目标优化的INDAGO工具，能发现更多独特故障并减少故障发现时间。


<details>
  <summary>Details</summary>
Motivation: 现有工具如INDAGO仅关注最大化故障数量，无法确保发现的故障场景具有多样性或揭示不同的错误类型。

Method: 使用多目标进化算法，联合优化故障可能性和测试场景多样性，采用多种多样性指标和帕累托前沿选择策略。

Result: 在三个DRL代理（人形步行者、自动驾驶汽车、停车代理）上评估，INDAGO-Nexus在SDC和停车场景中分别比INDAGO多发现83%和40%的独特故障，所有代理的故障发现时间减少高达67%。

Conclusion: 多目标搜索方法能有效提高深度强化学习代理测试的故障发现多样性和效率。

Abstract: Testing deep reinforcement learning (DRL) agents in safety-critical domains
requires discovering diverse failure scenarios. Existing tools such as INDAGO
rely on single-objective optimization focused solely on maximizing failure
counts, but this does not ensure discovered scenarios are diverse or reveal
distinct error types. We introduce INDAGO-Nexus, a multi-objective search
approach that jointly optimizes for failure likelihood and test scenario
diversity using multi-objective evolutionary algorithms with multiple diversity
metrics and Pareto front selection strategies. We evaluated INDAGO-Nexus on
three DRL agents: humanoid walker, self-driving car, and parking agent. On
average, INDAGO-Nexus discovers up to 83% and 40% more unique failures (test
effectiveness) than INDAGO in the SDC and Parking scenarios, respectively,
while reducing time-to-failure by up to 67% across all agents.

</details>


### [136] [Beyond Multi-Token Prediction: Pretraining LLMs with Future Summaries](https://arxiv.org/abs/2510.14751)
*Divyat Mahajan,Sachin Goyal,Badr Youbi Idrissi,Mohammad Pezeshki,Ioannis Mitliagkas,David Lopez-Paz,Kartik Ahuja*

Main category: cs.LG

TL;DR: 提出未来摘要预测(FSP)方法，通过预测序列的长期未来摘要来改进语言模型的长期推理能力，相比传统下一词预测(NTP)和多词预测(MTP)在数学、推理和编程任务上表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统下一词预测(NTP)在长期推理、规划和创意写作方面存在局限，多词预测(MTP)只能捕获短期依赖关系，改进有限。需要一种能够处理长期依赖的方法。

Method: 提出未来摘要预测(FSP)，训练一个辅助头来预测长期未来的紧凑表示。探索两种变体：手工构建摘要（如词袋摘要）和学习摘要（使用从右到左训练的反向语言模型生成的嵌入）。

Result: 在大规模预训练实验（30亿和80亿参数模型）中，FSP在数学、推理和编程基准测试上均优于NTP和MTP。

Conclusion: FSP通过预测长期未来摘要有效提升了语言模型的长期推理能力，为改进语言模型的长文本生成能力提供了有前景的方向。

Abstract: Next-token prediction (NTP) has driven the success of large language models
(LLMs), but it struggles with long-horizon reasoning, planning, and creative
writing, with these limitations largely attributed to teacher-forced training.
Multi-token prediction (MTP) partially mitigates these issues by predicting
several future tokens at once, but it mostly captures short-range dependencies
and offers limited improvement. We propose future summary prediction (FSP),
which trains an auxiliary head to predict a compact representation of the
long-term future, preserving information relevant for long-form generations. We
explore two variants of FSP: handcrafted summaries, for example, a bag of words
summary of the future of the sequence, and learned summaries, which use
embeddings produced by a reverse language model trained from right to left.
Large-scale pretraining experiments (3B and 8B-parameter models) demonstrate
that FSP provides improvements over both NTP and MTP across math, reasoning,
and coding benchmarks.

</details>


### [137] [Causal Discovery for Linear DAGs with Dependent Latent Variables via Higher-order Cumulants](https://arxiv.org/abs/2510.14780)
*Ming Cai,Penggang Gao,Hisayuki Hara*

Main category: cs.LG

TL;DR: 提出了一种新算法，用于在线性非高斯有向无环图模型中估计具有潜在混杂因素的因果DAG，允许潜在变量之间、观测变量之间以及两者之间存在因果结构。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设潜在混杂因素相互独立，或无法正确处理具有观测变量间因果关系的模型，存在局限性。

Method: 利用观测数据的高阶累积量来识别因果结构，允许潜在变量之间、观测变量之间以及两者之间存在因果关系。

Result: 通过大量模拟和真实世界数据实验验证了所提算法的有效性和实用性。

Conclusion: 该算法能够有效识别具有潜在混杂因素的线性非高斯模型中的因果DAG结构，具有实际应用价值。

Abstract: This paper addresses the problem of estimating causal directed acyclic graphs
in linear non-Gaussian acyclic models with latent confounders (LvLiNGAM).
Existing methods assume mutually independent latent confounders or cannot
properly handle models with causal relationships among observed variables.
  We propose a novel algorithm that identifies causal DAGs in LvLiNGAM,
allowing causal structures among latent variables, among observed variables,
and between the two. The proposed method leverages higher-order cumulants of
observed data to identify the causal structure. Extensive simulations and
experiments with real-world data demonstrate the validity and practical utility
of the proposed algorithm.

</details>


### [138] [Active Jammer Localization via Acquisition-Aware Path Planning](https://arxiv.org/abs/2510.14790)
*Luis González-Gudiño,Mariona Jaramillo-Civill,Pau Closas,Tales Imbiriba*

Main category: cs.LG

TL;DR: 提出结合贝叶斯优化与采集感知路径规划的主动干扰源定位框架，通过改进A*算法实现高效测量采集，在城市场景中实现高精度定位且所需测量次数更少。


<details>
  <summary>Details</summary>
Motivation: 传统被动众包方法效率较低，需要开发能够自适应引导移动代理收集高效用信号强度测量值的方法，同时考虑城市障碍物和移动性约束。

Method: 改进A*算法为A-UCB*，将采集值纳入轨迹成本计算，生成高采集价值的规划路径，结合贝叶斯优化进行主动干扰源定位。

Result: 在真实城市场景模拟中，该方法相比无信息基线方法，使用更少的测量次数实现了准确的定位，在不同环境下表现一致。

Conclusion: 所提出的主动干扰源定位框架通过采集感知路径规划，显著提高了定位效率，为城市环境中的干扰源检测提供了有效解决方案。

Abstract: We propose an active jammer localization framework that combines Bayesian
optimization with acquisition-aware path planning. Unlike passive crowdsourced
methods, our approach adaptively guides a mobile agent to collect high-utility
Received Signal Strength measurements while accounting for urban obstacles and
mobility constraints. For this, we modified the A* algorithm, A-UCB*, by
incorporating acquisition values into trajectory costs, leading to
high-acquisition planned paths. Simulations on realistic urban scenarios show
that the proposed method achieves accurate localization with fewer measurements
compared to uninformed baselines, demonstrating consistent performance under
different environments.

</details>


### [139] [Rethinking Hebbian Principle: Low-Dimensional Structural Projection for Unsupervised Learning](https://arxiv.org/abs/2510.14810)
*Shikuang Deng,Jiayuan Zhang,Yuhang Wu,Ting Chen,Shi Gu*

Main category: cs.LG

TL;DR: SPHeRe是一种新颖的无监督学习方法，通过整合正交性和结构信息保留来解决传统Hebbian学习在机器学习中的局限性，在图像分类基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统Hebbian学习在机器学习中存在连接更新不受约束和缺乏反馈调节的问题，限制了其在复杂网络架构和任务中的有效扩展。

Method: SPHeRe通过局部辅助非线性块整合正交性和结构信息保留，结构信息保留损失通过轻量级投影反向传播作为反馈调节，正交性约束确保更新幅度的有界性。

Result: 在CIFAR-10、CIFAR-100和Tiny-ImageNet等标准图像分类基准测试中达到无监督突触可塑性方法的SOTA性能，在持续学习和迁移学习场景中表现优异。

Conclusion: 这项工作展示了Hebbian无监督学习规则在现代深度学习框架中的竞争力和潜力，证明了无需严格依赖反向传播的高效生物启发学习算法的可能性。

Abstract: Hebbian learning is a biological principle that intuitively describes how
neurons adapt their connections through repeated stimuli. However, when applied
to machine learning, it suffers serious issues due to the unconstrained updates
of the connections and the lack of accounting for feedback mediation. Such
shortcomings limit its effective scaling to complex network architectures and
tasks. To this end, here we introduce the Structural Projection Hebbian
Representation (SPHeRe), a novel unsupervised learning method that integrates
orthogonality and structural information preservation through a local auxiliary
nonlinear block. The loss for structural information preservation
backpropagates to the input through an auxiliary lightweight projection that
conceptually serves as feedback mediation while the orthogonality constraints
account for the boundedness of updating magnitude. Extensive experimental
results show that SPHeRe achieves SOTA performance among unsupervised synaptic
plasticity approaches on standard image classification benchmarks, including
CIFAR-10, CIFAR-100, and Tiny-ImageNet. Furthermore, the method exhibits strong
effectiveness in continual learning and transfer learning scenarios, and image
reconstruction tasks show the robustness and generalizability of the extracted
features. This work demonstrates the competitiveness and potential of Hebbian
unsupervised learning rules within modern deep learning frameworks,
demonstrating the possibility of efficient and biologically inspired learning
algorithms without the strong dependence on strict backpropagation. Our code is
available at https://github.com/brain-intelligence-lab/SPHeRe.

</details>


### [140] [Efficient Dynamic Structured Sparse Training with Learned Shuffles](https://arxiv.org/abs/2510.14812)
*Abhishek Tyagi,Arjun Iyer,Liam Young,William H Renninger,Christopher Kanan,Yuhao Zhu*

Main category: cs.LG

TL;DR: 提出了一种结合学习排列矩阵的结构化稀疏训练方法（PA-DST），在保持高精度的同时显著提升了训练和推理速度。


<details>
  <summary>Details</summary>
Motivation: 结构化稀疏虽然能加速GPU上的训练和推理，但在精度上仍落后于非结构化动态稀疏训练（DST），主要原因是表达能力受限。

Method: 为每个层学习一个排列矩阵，与结构化权重矩阵联合训练，应用于块、N:M和对角线三种典型结构。

Result: 在ImageNet-1K（ViT-B/16）和WikiText-103（GPT-2）上，PA-DST在90-95%稀疏度下与非结构化基线（RigL、SET）精度相当，但训练速度提升1.21倍，推理速度提升2.9倍。

Conclusion: 结构+学习排列的组合在精度和效率之间找到了最佳平衡点。

Abstract: Structured sparsity accelerates training and inference on modern GPUs, yet it
still trails unstructured dynamic sparse training (DST) in accuracy. The
shortfall stems from a loss of expressivity: whereas a dense layer can realize
every possible mask obtained by choosing any $w$ active weights out of $n$, a
fixed block or N:M layout explores only a subset of those possibilities. We
propose to close this gap by learning, for each layer, a single permutation
matrix jointly with the structured weight matrix. Applied to three canonical
structures -- block, N:M, and diagonals -- we show that permutation-augmented
DST (PA-DST) matches unstructured baselines (RigL, SET) at 90--95\% sparsity on
ImageNet-1K (ViT-B/16) and WikiText-103 (GPT-2), yet trains up to $1.21\times$
and infers up to $2.9\times$ faster. The results position structure + learned
permutation as a sweet spot between accuracy and efficiency.

</details>


### [141] [Tackling Time-Series Forecasting Generalization via Mitigating Concept Drift](https://arxiv.org/abs/2510.14814)
*Zhiyuan Zhao,Haoxin Liu,B. Aditya Prakash*

Main category: cs.LG

TL;DR: 提出ShifTS框架，先解决时序偏移再处理概念漂移，通过软注意力机制从回看和预测时间序列中寻找不变模式，提升时间序列预测精度。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据具有动态特性，存在概念漂移和时序偏移两种分布变化。现有研究主要关注时序偏移，而针对时间序列预测的概念漂移方法研究相对较少。

Method: 提出ShifTS框架：1）先处理时序偏移；2）使用软注意力机制从回看和预测时间序列中寻找不变模式来处理概念漂移。该方法与具体模型无关。

Result: 在多个数据集上的广泛实验表明，ShifTS能持续提升无关模型的预测精度，优于现有的概念漂移、时序偏移及组合基线方法。

Conclusion: ShifTS框架通过先处理时序偏移再解决概念漂移的统一方法，有效提升了时间序列预测的准确性，证明了该方法的有效性。

Abstract: Time-series forecasting finds broad applications in real-world scenarios. Due
to the dynamic nature of time series data, it is important for time-series
forecasting models to handle potential distribution shifts over time. In this
paper, we initially identify two types of distribution shifts in time series:
concept drift and temporal shift. We acknowledge that while existing studies
primarily focus on addressing temporal shift issues in time series forecasting,
designing proper concept drift methods for time series forecasting has received
comparatively less attention.
  Motivated by the need to address potential concept drift, while conventional
concept drift methods via invariant learning face certain challenges in
time-series forecasting, we propose a soft attention mechanism that finds
invariant patterns from both lookback and horizon time series. Additionally, we
emphasize the critical importance of mitigating temporal shifts as a
preliminary to addressing concept drift. In this context, we introduce ShifTS,
a method-agnostic framework designed to tackle temporal shift first and then
concept drift within a unified approach. Extensive experiments demonstrate the
efficacy of ShifTS in consistently enhancing the forecasting accuracy of
agnostic models across multiple datasets, and outperforming existing concept
drift, temporal shift, and combined baselines.

</details>


### [142] [Programmatic Representation Learning with Language Models](https://arxiv.org/abs/2510.14825)
*Gabriel Poesia,Georgia Gabriela Sampaio*

Main category: cs.LG

TL;DR: 提出Learned Programmatic Representations (LeaPR)模型，结合代码特征函数和决策树，使用LLM生成特征，在多个任务上实现与神经网络竞争的性能，同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型依赖特征工程，神经网络能自动学习表示但缺乏可解释性且需要专用硬件。希望找到既能自动学习有用特征又保持可解释性的方法。

Method: 1. 使用FunSearch算法学习特征而非直接生成预测器；2. 开发ID3算法的变体，在分裂叶节点时按需生成新特征；3. 利用LLM编写代码特征函数。

Result: 在象棋位置评估、图像和文本分类等任务中，LeaPR模型学习到高质量的非神经网络预测器，性能与神经网络相当。

Conclusion: LeaPR提供了一个灵活的端到端学习可解释表示的范式，特征和预测都可以被直接检查和理解。

Abstract: Classical models for supervised machine learning, such as decision trees, are
efficient and interpretable predictors, but their quality is highly dependent
on the particular choice of input features. Although neural networks can learn
useful representations directly from raw data (e.g., images or text), this
comes at the expense of interpretability and the need for specialized hardware
to run them efficiently. In this paper, we explore a hypothesis class we call
Learned Programmatic Representations (LeaPR) models, which stack arbitrary
features represented as code (functions from data points to scalars) and
decision tree predictors. We synthesize feature functions using Large Language
Models (LLMs), which have rich prior knowledge in a wide range of domains and a
remarkable ability to write code using existing domain-specific libraries. We
propose two algorithms to learn LeaPR models from supervised data. First, we
design an adaptation of FunSearch to learn features rather than directly
generate predictors. Then, we develop a novel variant of the classical ID3
algorithm for decision tree learning, where new features are generated on
demand when splitting leaf nodes. In experiments from chess position evaluation
to image and text classification, our methods learn high-quality, neural
network-free predictors often competitive with neural networks. Our work
suggests a flexible paradigm for learning interpretable representations
end-to-end where features and predictions can be readily inspected and
understood.

</details>


### [143] [To Infinity and Beyond: Tool-Use Unlocks Length Generalization in State Space Models](https://arxiv.org/abs/2510.14826)
*Eran Malach,Omid Saremi,Sinead Williamson,Arwen Bradley,Aryo Lotfi,Emmanuel Abbe,Josh Susskind,Etai Littwin*

Main category: cs.LG

TL;DR: SSMs在长序列建模中具有效率优势，但理论上无法解决真正长形式的生成问题。通过引入外部工具交互，SSMs能够克服这一限制，实现长度泛化。


<details>
  <summary>Details</summary>
Motivation: 揭示SSMs在长形式生成任务中的理论局限性，并探索通过工具增强来克服这些限制的可能性。

Method: 允许SSMs与外部工具进行交互访问，结合问题相关的训练数据，使SSMs能够学习解决可处理问题。

Result: 工具增强的SSMs在算术、推理和编程任务上实现了显著的长度泛化效果。

Conclusion: SSMs在交互式工具和代理设置中，有望成为Transformers的高效替代方案。

Abstract: State Space Models (SSMs) have become the leading alternative to Transformers
for sequence modeling. Their primary advantage is efficiency in long-context
and long-form generation, enabled by fixed-size memory and linear scaling of
computational complexity. We begin this work by showing a simple theoretical
result stating that SSMs cannot accurately solve any ``truly long-form''
generation problem (in a sense we formally define), undermining their main
competitive advantage. However, we show that this limitation can be mitigated
by allowing SSMs interactive access to external tools. In fact, we show that
given the right choice of tool access and problem-dependent training data, SSMs
can learn to solve any tractable problem and generalize to arbitrary problem
length/complexity (i.e., achieve length generalization). Following our
theoretical finding, we demonstrate that tool-augmented SSMs achieve remarkable
length generalization on a variety of arithmetic, reasoning, and coding tasks.
These findings highlight SSMs as a potential efficient alternative to
Transformers in interactive tool-based and agentic settings.

</details>


### [144] [Intelligent Dynamic Handover via AI-assisted Signal Quality Prediction in 6G Multi-RAT Networks](https://arxiv.org/abs/2510.14832)
*Maria Lamprini A. Bartsioka,Anastasios Giannopoulos,Sotirios Spantideas*

Main category: cs.LG

TL;DR: 提出基于机器学习的预测性条件切换框架，使用LSTM网络预测信号质量，在6G多RAT网络中实现主动切换，减少切换失败和乒乓效应。


<details>
  <summary>Details</summary>
Motivation: 6G多RAT网络中现有切换机制反应式、事件触发，依赖瞬时测量，在快速信道动态、干扰和异构覆盖下可靠性不足。

Method: 基于模型驱动和短时域信号质量预测的ML辅助预测性条件切换框架，使用RAT感知LSTM网络预测移动用户信号质量指标。

Result: 在软硬切换设置下测试，带滞后的P-CHO方案能减少切换失败和乒乓事件，实现准确、低延迟的主动切换。

Conclusion: 提出的P-CHO框架适用于6G多RAT部署中的ML辅助切换引导，能实现准确、低延迟的主动切换。

Abstract: The emerging paradigm of 6G multiple Radio Access Technology (multi-RAT)
networks, where cellular and Wireless Fidelity (WiFi) transmitters coexist,
requires mobility decisions that remain reliable under fast channel dynamics,
interference, and heterogeneous coverage. Handover in multi-RAT deployments is
still highly reactive and event-triggered, relying on instantaneous
measurements and threshold events. This work proposes a Machine Learning
(ML)-assisted Predictive Conditional Handover (P-CHO) framework based on a
model-driven and short-horizon signal quality forecasts. We present a
generalized P-CHO sequence workflow orchestrated by a RAT Steering Controller,
which standardizes data collection, parallel per-RAT predictions, decision
logic with hysteresis-based conditions, and CHO execution. Considering a
realistic multi-RAT environment, we train RAT-aware Long Short Term Memory
(LSTM) networks to forecast the signal quality indicators of mobile users along
randomized trajectories. The proposed P-CHO models are trained and evaluated
under different channel models for cellular and IEEE 802.11 WiFi integrated
coverage. We study the impact of hyperparameter tuning of LSTM models under
different system settings, and compare direct multi-step versus recursive P-CHO
variants. Comparisons against baseline predictors are also carried out.
Finally, the proposed P-CHO is tested under soft and hard handover settings,
showing that hysteresis-enabled P-CHO scheme is able to reduce handover
failures and ping-pong events. Overall, the proposed P-CHO framework can enable
accurate, low-latency, and proactive handovers suitable for ML-assisted
handover steering in 6G multi-RAT deployments.

</details>


### [145] [Reinforcement Learning with Stochastic Reward Machines](https://arxiv.org/abs/2510.14837)
*Jan Corazza,Ivan Gavran,Daniel Neider*

Main category: cs.LG

TL;DR: 提出随机奖励机来处理强化学习中噪声奖励问题，基于约束求解学习最小随机奖励机，与现有方法结合能在极限下收敛到最优策略。


<details>
  <summary>Details</summary>
Motivation: 现有奖励机学习算法假设奖励无噪声，但实际应用中奖励往往存在噪声，这限制了奖励机在现实场景中的应用。

Method: 引入随机奖励机概念，基于约束求解算法从强化学习智能体的探索中学习最小随机奖励机。

Result: 在两个案例研究中证明该方法优于现有方法和处理噪声奖励函数的朴素方法。

Conclusion: 随机奖励机能有效处理噪声奖励问题，算法可与现有强化学习方法结合，保证收敛到最优策略。

Abstract: Reward machines are an established tool for dealing with reinforcement
learning problems in which rewards are sparse and depend on complex sequences
of actions. However, existing algorithms for learning reward machines assume an
overly idealized setting where rewards have to be free of noise. To overcome
this practical limitation, we introduce a novel type of reward machines, called
stochastic reward machines, and an algorithm for learning them. Our algorithm,
based on constraint solving, learns minimal stochastic reward machines from the
explorations of a reinforcement learning agent. This algorithm can easily be
paired with existing reinforcement learning algorithms for reward machines and
guarantees to converge to an optimal policy in the limit. We demonstrate the
effectiveness of our algorithm in two case studies and show that it outperforms
both existing methods and a naive approach for handling noisy reward functions.

</details>


### [146] [Provable Unlearning with Gradient Ascent on Two-Layer ReLU Neural Networks](https://arxiv.org/abs/2510.14844)
*Odelia Melamed,Gilad Yehudai,Gal Vardi*

Main category: cs.LG

TL;DR: 该论文对梯度上升这一简单广泛使用的机器遗忘方法进行了理论分析，证明该方法能有效移除特定数据点的影响，同时保持模型在保留数据上的性能。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习和隐私保护中需要从训练好的模型中移除特定数据点的需求，避免重新从头训练模型的高成本。

Method: 利用梯度下降的隐式偏差特性，通过梯度上升步骤来逆转特定数据点的影响，并基于KKT条件提出新的遗忘成功标准。

Result: 理论证明对于线性模型和高维数据下的两层神经网络，适当缩放的梯度上升步骤能满足遗忘成功标准，产生的模型与在保留数据上重新训练的解决方案非常接近。

Conclusion: 梯度上升是一种有效的机器遗忘方法，既能成功移除特定数据影响，又能保持模型的泛化能力。

Abstract: Machine Unlearning aims to remove specific data from trained models,
addressing growing privacy and ethical concerns. We provide a theoretical
analysis of a simple and widely used method - gradient ascent - used to reverse
the influence of a specific data point without retraining from scratch.
Leveraging the implicit bias of gradient descent towards solutions that satisfy
the Karush-Kuhn-Tucker (KKT) conditions of a margin maximization problem, we
quantify the quality of the unlearned model by evaluating how well it satisfies
these conditions w.r.t. the retained data. To formalize this idea, we propose a
new success criterion, termed \textbf{$(\epsilon, \delta, \tau)$-successful}
unlearning, and show that, for both linear models and two-layer neural networks
with high dimensional data, a properly scaled gradient-ascent step satisfies
this criterion and yields a model that closely approximates the retrained
solution on the retained data. We also show that gradient ascent performs
successful unlearning while still preserving generalization in a synthetic
Gaussian-mixture setting.

</details>


### [147] [Backdoor Unlearning by Linear Task Decomposition](https://arxiv.org/abs/2510.14845)
*Amel Abdelraheem,Alessandro Favero,Gerome Bovet,Pascal Frossard*

Main category: cs.LG

TL;DR: 该论文提出了一种基于权重空间解缠的简单反学习方法来移除基础模型中的后门攻击，无需重新训练即可保持模型在其他任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 基础模型虽然具有广泛的泛化能力，但容易受到对抗性扰动和后门攻击。现有后门移除方法需要昂贵的微调，且会降低模型在其他任务上的性能。本文研究是否能在不影响模型通用能力的前提下移除后门。

Method: 研究发现后门在模型权重空间中是与其他良性任务解缠的，基于此提出了利用这种解缠特性的简单反学习方法，可以隔离和擦除后门影响。

Result: 在CLIP模型和常见对抗性触发器的实验中，该方法实现了近乎完美的反学习效果，同时平均保持96%的干净准确率。即使攻击未知，也能通过反向工程触发器成功移除后门。

Conclusion: 该方法在反学习和干净准确率的权衡方面优于现有最先进的防御方法，为后门移除提供了有效解决方案。

Abstract: Foundation models have revolutionized computer vision by enabling broad
generalization across diverse tasks. Yet, they remain highly susceptible to
adversarial perturbations and targeted backdoor attacks. Mitigating such
vulnerabilities remains an open challenge, especially given that the
large-scale nature of the models prohibits retraining to ensure safety.
Existing backdoor removal approaches rely on costly fine-tuning to override the
harmful behavior, and can often degrade performance on other unrelated tasks.
This raises the question of whether backdoors can be removed without
compromising the general capabilities of the models. In this work, we address
this question and study how backdoors are encoded in the model weight space,
finding that they are disentangled from other benign tasks. Specifically, this
separation enables the isolation and erasure of the backdoor's influence on the
model with minimal impact on clean performance. Building on this insight, we
introduce a simple unlearning method that leverages such disentanglement.
Through extensive experiments with CLIP-based models and common adversarial
triggers, we show that, given the knowledge of the attack, our method achieves
approximately perfect unlearning, while retaining, on average, 96% of clean
accuracy. Additionally, we demonstrate that even when the attack and its
presence are unknown, our method successfully unlearns backdoors by proper
estimation using reverse-engineered triggers. Overall, our method consistently
yields better unlearning and clean accuracy tradeoffs when compared to present
state-of-the-art defenses.

</details>


### [148] [Predicting kernel regression learning curves from only raw data statistics](https://arxiv.org/abs/2510.14878)
*Dhruva Karkada,Joseph Turnbull,Yuxi Liu,James B. Simon*

Main category: cs.LG

TL;DR: 提出了Hermite特征结构假设(HEA)，通过数据协方差矩阵和目标函数的多项式分解来预测核回归的学习曲线，并在真实图像数据集上验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究核回归在真实数据集上的学习曲线预测问题，旨在建立从数据集结构到模型性能的端到端理论框架。

Method: 提出Hermite特征结构假设(HEA)，通过分析核特征值和特征函数的近似方法，结合各向异性数据分布，预测学习曲线。

Result: HEA在真实图像数据上表现良好，能够准确预测学习曲线，并且发现MLP在特征学习阶段也按照HEA预测的顺序学习Hermite多项式。

Conclusion: HEA框架证明了在真实数据集上为复杂学习算法建立从数据集结构到模型性能的端到端理论是可行的。

Abstract: We study kernel regression with common rotation-invariant kernels on real
datasets including CIFAR-5m, SVHN, and ImageNet. We give a theoretical
framework that predicts learning curves (test risk vs. sample size) from only
two measurements: the empirical data covariance matrix and an empirical
polynomial decomposition of the target function $f_*$. The key new idea is an
analytical approximation of a kernel's eigenvalues and eigenfunctions with
respect to an anisotropic data distribution. The eigenfunctions resemble
Hermite polynomials of the data, so we call this approximation the Hermite
eigenstructure ansatz (HEA). We prove the HEA for Gaussian data, but we find
that real image data is often "Gaussian enough" for the HEA to hold well in
practice, enabling us to predict learning curves by applying prior results
relating kernel eigenstructure to test risk. Extending beyond kernel
regression, we empirically find that MLPs in the feature-learning regime learn
Hermite polynomials in the order predicted by the HEA. Our HEA framework is a
proof of concept that an end-to-end theory of learning which maps dataset
structure all the way to model performance is possible for nontrivial learning
algorithms on real datasets.

</details>


### [149] [Learning When Not to Learn: Risk-Sensitive Abstention in Bandits with Unbounded Rewards](https://arxiv.org/abs/2510.14884)
*Sarah Liaw,Benjamin Plaut*

Main category: cs.LG

TL;DR: 提出了一种在无导师情况下处理无界奖励的上下文赌博机模型，通过谨慎探索策略避免不可逆错误。


<details>
  <summary>Details</summary>
Motivation: 在AI高风险应用中，传统算法假设所有错误都可恢复，但实际中某些错误可能造成不可逆损害。现有方法依赖导师指导，但导师不一定总是可用。

Method: 建立具有弃权选项的两动作上下文赌博机模型，提出基于谨慎的算法：选择可信区域，只在有证据表明不会造成损害时执行任务策略。

Result: 在独立同分布输入条件下，获得了次线性遗憾保证，证明谨慎探索在高风险环境中部署学习智能体的有效性。

Conclusion: 谨慎探索策略能够在不依赖导师的情况下，安全地在高风险环境中部署学习智能体，避免不可逆损害。

Abstract: In high-stakes AI applications, even a single action can cause irreparable
damage. However, nearly all of sequential decision-making theory assumes that
all errors are recoverable (e.g., by bounding rewards). Standard bandit
algorithms that explore aggressively may cause irreparable damage when this
assumption fails. Some prior work avoids irreparable errors by asking for help
from a mentor, but a mentor may not always be available. In this work, we
formalize a model of learning with unbounded rewards without a mentor as a
two-action contextual bandit with an abstain option: at each round the agent
observes an input and chooses either to abstain (always 0 reward) or to commit
(execute a preexisting task policy). Committing yields rewards that are
upper-bounded but can be arbitrarily negative, and the commit reward is assumed
Lipschitz in the input. We propose a caution-based algorithm that learns when
not to learn: it chooses a trusted region and commits only where the available
evidence does not already certify harm. Under these conditions and i.i.d.
inputs, we establish sublinear regret guarantees, theoretically demonstrating
the effectiveness of cautious exploration for deploying learning agents safely
in high-stakes environments.

</details>


### [150] [Reasoning with Sampling: Your Base Model is Smarter Than You Think](https://arxiv.org/abs/2510.14901)
*Aayush Karan,Yilun Du*

Main category: cs.LG

TL;DR: 本文提出了一种无需额外训练的推理采样算法，通过纯采样就能从基础模型中激发出与强化学习训练相媲美的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注强化学习训练后出现的新行为，而本文从不同角度探索是否能在推理时通过纯采样从基础模型中激发出类似的推理能力。

Method: 受马尔可夫链蒙特卡洛技术启发，提出了一种简单的迭代采样算法，利用基础模型自身的似然性进行采样。

Result: 在不同基础模型上，该算法在多种单次任务（如MATH500、HumanEval、GPQA）中显著提升推理能力，性能接近甚至超过强化学习训练，同时避免了RL训练后多样性的崩溃问题。

Conclusion: 该方法无需训练、精选数据集或验证器，在易验证领域之外具有广泛适用性，为激发基础模型推理能力提供了有效途径。

Abstract: Frontier reasoning models have exhibited incredible capabilities across a
wide array of disciplines, driven by posttraining large language models (LLMs)
with reinforcement learning (RL). However, despite the widespread success of
this paradigm, much of the literature has been devoted to disentangling truly
novel behaviors that emerge during RL but are not present in the base models.
In our work, we approach this question from a different angle, instead asking
whether comparable reasoning capabilites can be elicited from base models at
inference time by pure sampling, without any additional training. Inspired by
Markov chain Monte Carlo (MCMC) techniques for sampling from sharpened
distributions, we propose a simple iterative sampling algorithm leveraging the
base models' own likelihoods. Over different base models, we show that our
algorithm offers substantial boosts in reasoning that nearly match and even
outperform those from RL on a wide variety of single-shot tasks, including
MATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in
diversity over multiple samples that is characteristic of RL-posttraining.
Crucially, our method does not require training, curated datasets, or a
verifier, suggesting broad applicability beyond easily verifiable domains.

</details>


### [151] [Circuit Insights: Towards Interpretability Beyond Activations](https://arxiv.org/abs/2510.14936)
*Elena Golimblevskaia,Aakriti Jain,Bruno Puri,Ammar Ibrahim,Wojciech Samek,Sebastian Lapuschkin*

Main category: cs.LG

TL;DR: 提出了WeightLens和CircuitLens两种互补方法，通过直接分析权重和特征交互来增强神经网络的可解释性，超越基于激活的分析方法。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法依赖人工检查且仅限于简单任务，自动化方法常忽略特征间交互并依赖外部LLM和数据集质量。

Method: WeightLens直接从学习权重解释特征，无需解释模型或数据集；CircuitLens捕获特征激活如何从组件交互中产生，揭示电路级动态。

Result: WeightLens在上下文无关特征上匹配或超越现有方法性能；CircuitLens能识别仅基于激活的方法无法发现的电路动态。

Conclusion: 这些方法提高了可解释性鲁棒性，增强了电路的可扩展机制分析，同时保持效率和质量。

Abstract: The fields of explainable AI and mechanistic interpretability aim to uncover
the internal structure of neural networks, with circuit discovery as a central
tool for understanding model computations. Existing approaches, however, rely
on manual inspection and remain limited to toy tasks. Automated
interpretability offers scalability by analyzing isolated features and their
activations, but it often misses interactions between features and depends
strongly on external LLMs and dataset quality. Transcoders have recently made
it possible to separate feature attributions into input-dependent and
input-invariant components, providing a foundation for more systematic circuit
analysis. Building on this, we propose WeightLens and CircuitLens, two
complementary methods that go beyond activation-based analysis. WeightLens
interprets features directly from their learned weights, removing the need for
explainer models or datasets while matching or exceeding the performance of
existing methods on context-independent features. CircuitLens captures how
feature activations arise from interactions between components, revealing
circuit-level dynamics that activation-only approaches cannot identify.
Together, these methods increase interpretability robustness and enhance
scalable mechanistic analysis of circuits while maintaining efficiency and
quality.

</details>


### [152] [Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models](https://arxiv.org/abs/2510.14961)
*Jonas Geiping,Xinyu Yang,Guinan Su*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型原理的新采样器，用于加速循环深度语言模型的生成，在相同时间预算下比自回归生成更具表达能力，且无需调参即可在现有模型上实现5倍加速。


<details>
  <summary>Details</summary>
Motivation: 探索循环深度模型与扩散语言模型之间的关系，利用它们的相似性来开发更高效的生成方法，以解决循环深度模型在推理时额外计算并行化的问题。

Method: 开发了一种扩散强制采样器，通过在每次前向传递时解码新token，同时并行细化这些token的潜在状态。该采样器基于扩散文献原理，可直接应用于现有循环深度变换器。

Result: 理论上，该采样器在相同硬件时间预算下比基线自回归生成更具表达能力。在3.5B参数循环深度变换器上无需调参即可实现高达5倍的加速。

Conclusion: 该工作不仅为循环深度模型在推理时的额外计算提供了高效并行化机制，还表明这类模型可以自然地被视为强大的连续（尽管是因果的）扩散语言模型。

Abstract: Language models with recurrent depth, also referred to as universal or looped
when considering transformers, are defined by the capacity to increase their
computation through the repetition of layers. Recent efforts in pretraining
have demonstrated that these architectures can scale to modern language
modeling tasks while exhibiting advantages in reasoning tasks. In this work, we
examine the relationship between recurrent-depth models and diffusion language
models. Building on their similarities, we develop a new diffusion forcing
sampler for these models to accelerate generation. The sampler advances by
decoding new tokens at every forward pass of the model, while the latent states
of these tokens can be further refined in parallel through recurrence.
Theoretically, generation with our sampler is strictly more expressive than the
baseline autoregressive generation using the same time budget on modern
hardware. Moreover, this sampler, based on principles from diffusion
literature, can be directly applied to existing 3.5B recurrent-depth
transformers without any tuning, leading to up to a 5x speedup. Consequently,
our findings not only provide an efficient mechanism for parallelizing the
extra computation in recurrent-depth models at inference, but also suggest that
such models can be naturally viewed as strong continuous, though causal,
diffusion language models.

</details>


### [153] [Identity-Link IRT for Label-Free LLM Evaluation: Preserving Additivity in TVD-MI Scores](https://arxiv.org/abs/2510.14966)
*Zachary Robertson*

Main category: cs.LG

TL;DR: 本文展示了使用恒等链接函数（identity link）在项目反应理论（IRT）中比传统的logit/probit链接更优，能更好地保持TVD-MI成对比较的加性结构，从而实现更高效的大语言模型评估。


<details>
  <summary>Details</summary>
Motivation: 传统的IRT方法使用非线性链接函数（如logit/probit）会破坏TVD-MI成对比较数据的加性结构，导致模型拟合失真。本文旨在找到能保持这种几何结构的更合适的链接函数。

Method: 提出基于Gini熵最大化的剪裁线性模型，采用恒等链接函数和盒约束最小二乘公式来处理边界饱和问题，通过最大似然方法进行IRT建模。

Result: 在三个不同领域中，恒等链接函数产生的曲率违反中位数为0.080-0.150，显著低于logit/probit的0.245-0.588。在33%覆盖率下，保持集RMSE为0.117±0.008，同时保持代理排名一致性（Spearman ρ=0.972±0.015）。

Conclusion: TVD-MI的几何结构最好通过恒等映射来保持，这为高效的大语言模型评估提供了更好的方法，并适用于其他有界响应领域。

Abstract: Pairwise comparisons of large language models using total variation distance
mutual information (TVD-MI) produce binary critic decisions per pair. We show
that averaging TVD-MI's binary trials yields centered-probability scores with
additive structure suitable for item-response theory (IRT) without nonlinear
link functions. Maximum-likelihood approaches to IRT use logistic links, but we
find empirically that these transformations introduce curvature that breaks
additivity: across three domains, the identity link yields median curl on raw
data of 0.080-0.150 (P95 = [0.474, 0.580]), whereas probit/logit introduce
substantially higher violations (median [0.245, 0.588], P95 [0.825, 2.252]). We
derive this clipped-linear model from Gini entropy maximization, yielding a
box-constrained least-squares formulation that handles boundary saturation. At
33% coverage, we achieve holdout RMSE $0.117 \pm 0.008$ while preserving agent
rankings (Spearman $\rho = 0.972 \pm 0.015$), three times fewer evaluations
than full dense. Judge robustness analysis (GPT-4o-mini vs. Llama3-70b) shows
strong agreement in agent rankings ($\rho = 0.872$) and consistent
identity-link advantage. TVD-MI's geometry is best preserved by identity
mapping for efficient LLM evaluation, applicable to other bounded-response
domains.

</details>


### [154] [Biology-informed neural networks learn nonlinear representations from omics data to improve genomic prediction and interpretability](https://arxiv.org/abs/2510.14970)
*Katiana Kontolati,Rini Jasmine Gladstone,Ian Davis,Ethan Pickering*

Main category: cs.LG

TL;DR: 该论文扩展了生物信息神经网络(BINNs)在作物基因组预测和选择中的应用，通过整合SNPs、多组学数据和先验生物知识，显著提高了预测准确性并揭示了非线性生物关系。


<details>
  <summary>Details</summary>
Motivation: 传统基因型到表型(G2P)模型准确度有限，依赖大规模田间试验；现有整合分子表型的模型在部署时不可行。BINNs旨在克服这些限制，仅训练时使用多组学数据，推理时仅需基因型数据。

Method: BINNs通过编码通路级归纳偏置，在训练时利用多组学数据和先验生物知识，在推理时仅使用基因型数据。应用于玉米基因表达和多环境田间试验数据。

Result: BINNs在稀疏数据条件下将排序相关准确性提高达56%，非线性识别GWAS/TWAS未能发现的基因；在合成代谢组学基准测试中，相比传统神经网络减少75%预测误差。

Conclusion: BINNs建立了利用中间领域信息提高基因组预测准确性的框架，揭示了可指导基因组选择、候选基因选择、通路富集和基因编辑优先级的非线性生物关系。

Abstract: We extend biologically-informed neural networks (BINNs) for genomic
prediction (GP) and selection (GS) in crops by integrating thousands of
single-nucleotide polymorphisms (SNPs) with multi-omics measurements and prior
biological knowledge. Traditional genotype-to-phenotype (G2P) models depend
heavily on direct mappings that achieve only modest accuracy, forcing breeders
to conduct large, costly field trials to maintain or marginally improve genetic
gain. Models that incorporate intermediate molecular phenotypes such as gene
expression can achieve higher predictive fit, but they remain impractical for
GS since such data are unavailable at deployment or design time. BINNs overcome
this limitation by encoding pathway-level inductive biases and leveraging
multi-omics data only during training, while using genotype data alone during
inference. Applied to maize gene-expression and multi-environment field-trial
data, BINN improves rank-correlation accuracy by up to 56% within and across
subpopulations under sparse-data conditions and nonlinearly identifies genes
that GWAS/TWAS fail to uncover. With complete domain knowledge for a synthetic
metabolomics benchmark, BINN reduces prediction error by 75% relative to
conventional neural nets and correctly identifies the most important nonlinear
pathway. Importantly, both cases show highly sensitive BINN latent variables
correlate with the experimental quantities they represent, despite not being
trained on them. This suggests BINNs learn biologically-relevant
representations, nonlinear or linear, from genotype to phenotype. Together,
BINNs establish a framework that leverages intermediate domain information to
improve genomic prediction accuracy and reveal nonlinear biological
relationships that can guide genomic selection, candidate gene selection,
pathway enrichment, and gene-editing prioritization.

</details>


### [155] [pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation](https://arxiv.org/abs/2510.14974)
*Hansheng Chen,Kai Zhang,Hao Tan,Leonidas Guibas,Gordon Wetzstein,Sai Bi*

Main category: cs.LG

TL;DR: 提出π-Flow方法，通过策略预测实现高效少步扩散生成，避免质量-多样性权衡问题


<details>
  <summary>Details</summary>
Motivation: 解决传统少步扩散模型中教师模型与学生模型格式不匹配导致的复杂蒸馏过程和质量-多样性权衡问题

Method: 修改学生流模型的输出层，使其预测无网络策略，该策略能在未来子步骤生成动态流速度，实现快速准确的ODE积分而无需额外网络评估

Result: 在ImageNet 256²上获得1-NFE FID 2.85，优于相同DiT架构的MeanFlow；在FLUX.1-12B和Qwen-Image-20B上4 NFEs时，相比最先进方法获得更好的多样性同时保持教师级质量

Conclusion: π-Flow通过简单的模仿教师行为实现了稳定可扩展的训练，避免了质量-多样性权衡问题

Abstract: Few-step diffusion or flow-based generative models typically distill a
velocity-predicting teacher into a student that predicts a shortcut towards
denoised data. This format mismatch has led to complex distillation procedures
that often suffer from a quality-diversity trade-off. To address this, we
propose policy-based flow models ($\pi$-Flow). $\pi$-Flow modifies the output
layer of a student flow model to predict a network-free policy at one timestep.
The policy then produces dynamic flow velocities at future substeps with
negligible overhead, enabling fast and accurate ODE integration on these
substeps without extra network evaluations. To match the policy's ODE
trajectory to the teacher's, we introduce a novel imitation distillation
approach, which matches the policy's velocity to the teacher's along the
policy's trajectory using a standard $\ell_2$ flow matching loss. By simply
mimicking the teacher's behavior, $\pi$-Flow enables stable and scalable
training and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it
attains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT
architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\pi$-Flow achieves
substantially better diversity than state-of-the-art few-step methods, while
maintaining teacher-level quality.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [156] [Surrogate Models for Linear Response](https://arxiv.org/abs/2510.13989)
*L. Jin,A. Ravlić,P. Giuliani,K. Godbey,W. Nazarewicz*

Main category: physics.comp-ph

TL;DR: 开发了两种QRPA替代模型来加速核物理中的线性响应计算，实现了6-7个数量级的加速，同时保持0.1%-1%的精度。


<details>
  <summary>Details</summary>
Motivation: QRPA方法虽然强大但计算成本高昂，限制了模型校准和不确定性量化研究，需要开发高效的替代模型。

Method: 提出了两种互补的QRPA替代模型：一种是利用QRPA结构的降阶模型，另一种是使用参数矩阵模型算法构建哈密顿量与可观测量之间的映射。

Result: 在¹⁸⁰Yb的电偶极极化率和⁸⁰Ni的β衰变半衰期计算中，两种模拟器都实现了0.1%-1%的精度，同时比最先进的QRPA求解器快6-7个数量级。

Conclusion: 开发的QRPA模拟器为贝叶斯校准和计算昂贵的多体系统物理模型的大规模研究提供了有力工具。

Abstract: Linear response theory is a well-established method in physics and chemistry
for exploring excitations of many-body systems. In particular, the
quasiparticle random-phase approximation (QRPA) provides a powerful microscopic
framework by building excitations on top of the mean-field vacuum; however, its
high computational cost limits model calibration and uncertainty quantification
studies. Here, we present two complementary QRPA surrogate models and apply
them to study response functions of finite nuclei. One is a reduced-order model
that exploits the underlying QRPA structure, while the other utilizes the
recently developed parametric matrix model algorithm to construct a map between
the system's Hamiltonian and observables. Our benchmark applications, the
calculation of the electric dipole polarizability of ${}^{180}$Yb and the
$\beta$-decay half-life of ${}^{80}$Ni, show that both emulators can achieve
0.1\%--1\% accuracy while offering a six to seven orders of magnitude speedup
compared to state-of-the-art QRPA solvers. These results demonstrate that the
developed QRPA emulators are well-positioned to enable Bayesian calibration and
large-scale studies of computationally expensive physics models describing the
properties of many-body systems.

</details>


### [157] [Anti-Interference Communication Using Computational Antenna](https://arxiv.org/abs/2510.14362)
*Xiaocun Zong,Fan Yang,Shenheng Xu,Maokun Li*

Main category: physics.comp-ph

TL;DR: 提出了一种基于计算天线的抗干扰通信方法，利用时间平均和1比特可重构智能表面实现鲁棒信号调制，硬件复杂度低。


<details>
  <summary>Details</summary>
Motivation: 传统扩频或跳频技术需要大量频谱资源，需要开发不增加频谱开销的抗干扰通信方法。

Method: 开发了计算天线的通信模型，提出了针对时间调制优化的高效信号处理算法，并建立了USRP实验平台进行验证。

Result: 在强干扰条件下（如5dB干扰信号比），误码率降低80.9%，传输测试中有效恢复失真图像。

Conclusion: 该方法在雷达探测、军事通信和下一代无线网络中具有重要应用价值，提供了不增加频谱开销的优越抗干扰性能。

Abstract: This letter proposes a novel anti-interference communication method
leveraging computational antennas, utilizing time averaging and 1-bit
reconfigurable intelligent surfaces (RIS) to achieve robust signal modulation
with minimal hardware complexity. We develop a communication model for
computational antennas and propose an efficient signal processing algorithm
optimized for temporal modulation. A USRP-based experimental platform is
established to validate the approach under strong interference conditions
(e.g., 5 dB jamming-to-signal ratio). Experimental results reveal up to an
80.9\% reduction in bit error rate (BER) and effective restoration of distorted
images in transmission tests. Compared to conventional techniques like spread
spectrum or frequency hopping, which require significant spectral resources,
our method offers superior anti-interference performance without additional
spectral overhead. This research provides valuable insights for radar
detection, military communications, and next-generation wireless networks.

</details>
