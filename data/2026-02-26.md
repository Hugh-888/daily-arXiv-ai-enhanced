<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 85]
- [quant-ph](#quant-ph) [Total: 28]
- [gr-qc](#gr-qc) [Total: 3]
- [physics.comp-ph](#physics.comp-ph) [Total: 6]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Latent Context Compilation: Distilling Long Context into Compact Portable Memory](https://arxiv.org/abs/2602.21221)
*Zeju Li,Yizhou Zhou,Qiang Xu*

Main category: cs.LG

TL;DR: Latent Context Compilation (LCC) 通过可丢弃的LoRA模块将长上下文编译为紧凑的缓冲区token，无需合成QA数据，实现16倍压缩比且保持模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前长上下文LLM部署面临困境：摊销压缩方法在分布外泛化上表现不佳，而测试时训练方法需要昂贵的合成数据成本且需要修改模型权重，产生有状态参数，难以实现并发服务。

Method: 提出Latent Context Compilation框架，将上下文处理从适应转向编译。使用可丢弃的LoRA模块作为编译器，将长上下文蒸馏为紧凑的缓冲区token。引入自对齐优化策略，通过上下文无关的随机查询正则化上下文重建任务，无需合成上下文相关的QA对。

Result: 在Llama-3.1-8B上的实验表明，Latent Context Compilation在先前方法失效的情况下仍能保留细粒度细节和推理能力，即使达到16倍压缩比也能有效解耦内存密度与模型参数。

Conclusion: Latent Context Compilation提供了一种无状态、即插即用的长上下文处理方案，通过编译而非适应的方式，解决了现有方法在泛化、成本和并发服务方面的限制。

Abstract: Efficient long-context LLM deployment is stalled by a dichotomy between amortized compression, which struggles with out-of-distribution generalization, and Test-Time Training, which incurs prohibitive synthetic data costs and requires modifying model weights, creating stateful parameters that complicate concurrent serving. We propose Latent Context Compilation, a framework that fundamentally shifts context processing from adaptation to compilation. By utilizing a disposable LoRA module as a compiler, we distill long contexts into compact buffer tokens -- stateless, portable memory artifacts that are plug-and-play compatible with frozen base models. Crucially, we introduce a self-aligned optimization strategy that eliminates the need for synthetic context-relevant QA pairs. By regularizing context reconstruction task with context-agnostic random queries, we force compressed tokens to reside within the model's existing instruction-following manifold. Experiments with Llama-3.1-8B demonstrate that Latent Context Compilation preserves fine-grained details and reasoning capabilities where prior methods falter, effectively decoupling memory density from model parameters even at a 16x compression ratio.

</details>


### [2] [ACAR: Adaptive Complexity Routing for Multi-Model Ensembles with Auditable Decision Traces](https://arxiv.org/abs/2602.21231)
*Ramchand Kumaresan*

Main category: cs.LG

TL;DR: ACAR框架通过自一致性方差路由多模型任务，在避免完全集成的情况下超越双模型基线，同时揭示了检索增强、模型一致错误和代理信号归因的实践局限性。


<details>
  <summary>Details</summary>
Motivation: 研究多模型协同在可审计条件下的性能，探索如何通过自适应路由机制在保持效率的同时提升任务执行准确率，并识别实际应用中的关键失败假设。

Method: 基于TEAMLLM确定性执行平台，使用N=3个探测样本计算自一致性方差(sigma)，根据方差值在单模型、双模型和三模型执行模式间进行任务路由，无需学习组件。

Result: sigma路由达到55.6%准确率，超越双模型基线(54.4%)，同时在54.2%任务上避免完全集成。发现检索增强降低准确率3.4个百分点，模型一致错误限制了自一致性方法的理论上限，代理归因信号与真实值相关性弱。

Conclusion: ACAR提供了可审计的多模型路由框架，揭示了实践中检索增强、一致错误和代理归因的局限性，为未来路由、检索和多模型归因研究建立了可证伪的基线。

Abstract: We present ACAR (Adaptive Complexity and Attribution Routing), a measurement framework for studying multi-model orchestration under auditable conditions. ACAR uses self-consistency variance (sigma) computed from N=3 probe samples to route tasks across single-model, two-model, and three-model execution modes. The system is implemented on top of TEAMLLM, a deterministic execution substrate with immutable artifacts and complete decision traces. We evaluate ACAR on 1,510 tasks spanning four benchmarks: MathArena, Reasoning Gym, LiveCodeBench, and SuperGPQA, using Claude Sonnet 4, GPT-4o, and Gemini 2.0 Flash, producing more than 7,550 auditable runs. Results show that sigma-based routing achieves 55.6 percent accuracy, exceeding the two-model baseline of 54.4 percent while avoiding full ensembling on 54.2 percent of tasks. The routing mechanism is model-agnostic and requires no learned components. We also document negative results. First, retrieval augmentation reduced accuracy by 3.4 percentage points, as median retrieval similarity was only 0.167, demonstrating that experience injection without semantic alignment introduces noise rather than grounding. Second, when models agree on incorrect answers (sigma equals zero), no downstream ensemble can recover; this agreement-but-wrong failure mode is intrinsic to self-consistency and bounds achievable accuracy at approximately eight percentage points below full ensembling. Third, attribution estimates based on proxy signals such as response similarity and entropy showed weak correlation with ground-truth leave-one-out values, indicating that practical attribution requires explicit counterfactual computation. This work documents which assumptions fail in practice and provides falsifiable baselines for future research on routing, retrieval, and multi-model attribution.

</details>


### [3] [Urban Vibrancy Embedding and Application on Traffic Prediction](https://arxiv.org/abs/2602.21232)
*Sumin Han,Jisun An,Dongman Lee*

Main category: cs.LG

TL;DR: 该研究提出了一种从实时浮动人口数据中提取城市活力嵌入的新方法，并将其集成到交通预测模型中，通过VAE压缩数据、LSTM预测嵌入，最终提升多种交通预测模型的准确性和响应性。


<details>
  <summary>Details</summary>
Motivation: 城市活力反映了城市空间中的人类活动动态，通常使用捕捉浮动人口趋势的移动数据来衡量。现有交通预测模型缺乏对城市活力动态变化的有效整合，需要一种能够捕捉和预测城市活力模式并将其应用于交通预测的新方法。

Method: 使用变分自编码器（VAE）将实时浮动人口数据压缩为可操作的嵌入表示，然后通过长短期记忆网络（LSTM）预测未来的嵌入，最后在序列到序列框架中将这些嵌入应用于交通预测。同时使用主成分分析（PCA）来解释嵌入的时空模式。

Result: 提出的方法能够有效捕捉城市活力的时空模式（如工作日与周末的区别、季节性模式），并显著提高了多种交通预测模型（包括RNN、DCRNN、GTS和GMAN）的准确性和响应性。

Conclusion: 城市活力嵌入在推进交通预测方面具有巨大潜力，能够提供更细致的城市流动性分析，为智能交通系统和城市规划提供有价值的见解。

Abstract: Urban vibrancy reflects the dynamic human activity within urban spaces and is often measured using mobile data that captures floating population trends. This study proposes a novel approach to derive Urban Vibrancy embeddings from real-time floating population data to enhance traffic prediction models. Specifically, we utilize variational autoencoders (VAE) to compress this data into actionable embeddings, which are then integrated with long short-term memory (LSTM) networks to predict future embeddings. These are subsequently applied in a sequence-to-sequence framework for traffic forecasting. Our contributions are threefold: (1) We use principal component analysis (PCA) to interpret the embeddings, revealing temporal patterns such as weekday versus weekend distinctions and seasonal patterns; (2) We propose a method that combines VAE and LSTM, enabling forecasting dynamic urban knowledge embedding; and (3) Our approach improves accuracy and responsiveness in traffic prediction models, including RNN, DCRNN, GTS, and GMAN. This study demonstrates the potential of Urban Vibrancy embeddings to advance traffic prediction and offer a more nuanced analysis of urban mobility.

</details>


### [4] [Asymptotically Fast Clebsch-Gordan Tensor Products with Vector Spherical Harmonics](https://arxiv.org/abs/2602.21466)
*YuQing Xie,Ameya Daigavane,Mit Kotak,Tess Smidt*

Main category: cs.LG

TL;DR: 本文提出了首个完整的Clebsch-Gordan张量积算法，将时间复杂度从O(L⁶)降低到O(L⁴log²L)，接近理论下界O(L⁴)。


<details>
  <summary>Details</summary>
Motivation: E(3)-等变神经网络在3D建模中很有效，但其核心操作张量积计算复杂度高。现有加速方法大多以牺牲表达能力为代价，而Gaunt张量积虽然能实现真正的渐近加速，但不完整且遗漏许多交互。

Method: 通过推广基于快速傅里叶变换的卷积，得到Gaunt张量积；为解决反对称性问题，将标量信号推广到不可约表示值信号，得到张量球谐函数；证明广义Gaunt公式；最后证明仅需向量值信号即可恢复Gaunt张量积缺失的交互。

Result: 提出了首个完整的Clebsch-Gordan张量积算法，将全CGTP的运行时复杂度从朴素O(L⁶)降低到O(L⁴log²L)，接近理论下界O(L⁴)。

Conclusion: 本文解决了E(3)-等变神经网络中张量积计算效率低的问题，提供了首个完整且高效的算法，在保持表达能力的同时实现了真正的渐近加速。

Abstract: $E(3)$-equivariant neural networks have proven to be effective in a wide range of 3D modeling tasks. A fundamental operation of such networks is the tensor product, which allows interaction between different feature types. Because this operation scales poorly, there has been considerable work towards accelerating this interaction. However, recently \citet{xieprice} have pointed out that most speedups come from a reduction in expressivity rather than true algorithmic improvements on computing Clebsch-Gordan tensor products. A modification of Gaunt tensor product \citep{gaunt} can give a true asymptotic speedup but is incomplete and misses many interactions. In this work, we provide the first complete algorithm which truly provides asymptotic benefits Clebsch-Gordan tensor products. For full CGTP, our algorithm brings runtime complexity from the naive $O(L^6)$ to $O(L^4\log^2 L)$, close to the lower bound of $O(L^4)$. We first show how generalizing fast Fourier based convolution naturally leads to the previously proposed Gaunt tensor product \citep{gaunt}. To remedy antisymmetry issues, we generalize from scalar signals to irrep valued signals, giving us tensor spherical harmonics. We prove a generalized Gaunt formula for the tensor harmonics. Finally, we show that we only need up to vector valued signals to recover the missing interactions of Gaunt tensor product.

</details>


### [5] [AngelSlim: A more accessible, comprehensive, and efficient toolkit for large model compression](https://arxiv.org/abs/2602.21233)
*Rui Cen,QiangQiang Hu,Hong Huang,Hong Liu,Song Liu,Xin Luo,Lin Niu,Yifan Tan,Decheng Wu,Linchuan Xie,Rubing Yang,Guanghua Yu,Jianchen Zhu*

Main category: cs.LG

TL;DR: AngelSlim是腾讯混元团队开发的大模型压缩工具包，整合了量化、推测解码、token剪枝和蒸馏等先进算法，提供从模型压缩到工业部署的统一流程。


<details>
  <summary>Details</summary>
Motivation: 解决大模型在工业部署中的计算和内存开销问题，提供统一的压缩工具链，降低部署门槛并提升推理效率。

Method: 1) 集成FP8和INT8后训练量化算法，支持超低位量化；2) 提出训练对齐的推测解码框架，兼容多模态架构；3) 开发训练无关的稀疏注意力框架，通过静态模式和动态token选择解耦稀疏核；4) 针对多模态模型设计专门的剪枝策略：IDPruner优化视觉token，Samp自适应音频token合并与剪枝。

Result: 1) 实现了首个工业可行的2位大模型HY-1.8B-int2；2) 推测解码实现1.8-2.0倍吞吐量提升且不损失输出正确性；3) 稀疏注意力框架在长上下文场景降低首token时间；4) 提供从算法研究到工具辅助部署的完整解决方案。

Conclusion: AngelSlim通过整合多种压缩技术，为大模型的高效工业部署提供了全面解决方案，平衡了模型性能与推理效率，推动了压缩算法的研究和实际应用。

Abstract: This technical report introduces AngelSlim, a comprehensive and versatile toolkit for large model compression developed by the Tencent Hunyuan team. By consolidating cutting-edge algorithms, including quantization, speculative decoding, token pruning, and distillation. AngelSlim provides a unified pipeline that streamlines the transition from model compression to industrial-scale deployment. To facilitate efficient acceleration, we integrate state-of-the-art FP8 and INT8 Post-Training Quantization (PTQ) algorithms alongside pioneering research in ultra-low-bit regimes, featuring HY-1.8B-int2 as the first industrially viable 2-bit large model. Beyond quantization, we propose a training-aligned speculative decoding framework compatible with multimodal architectures and modern inference engines, achieving 1.8x to 2.0x throughput gains without compromising output correctness. Furthermore, we develop a training-free sparse attention framework that reduces Time-to-First-Token (TTFT) in long-context scenarios by decoupling sparse kernels from model architectures through a hybrid of static patterns and dynamic token selection. For multimodal models, AngelSlim incorporates specialized pruning strategies, namely IDPruner for optimizing vision tokens via Maximal Marginal Relevance and Samp for adaptive audio token merging and pruning. By integrating these compression strategies from low-level implementations, AngelSlim enables algorithm-focused research and tool-assisted deployment.

</details>


### [6] [Group Orthogonalized Policy Optimization:Group Policy Optimization as Orthogonal Projection in Hilbert Space](https://arxiv.org/abs/2602.21269)
*Wang Zixian*

Main category: cs.LG

TL;DR: GOPO是一种新的LLM对齐算法，通过在希尔伯特空间L2(pi_k)中进行优化，将概率单纯形约束转化为线性正交条件，实现有界希尔伯特投影，产生精确稀疏性，避免灾难性动作。


<details>
  <summary>Details</summary>
Motivation: 传统对齐方法在概率单纯形上优化，继承KL散度的指数曲率，导致梯度饱和和训练不稳定。需要一种在函数空间中进行优化的新方法，避免这些问题。

Method: 将对齐问题提升到希尔伯特空间L2(pi_k)，将单纯形约束转化为线性正交条件。通过希尔伯特投影定理求解最大化器，强制执行边界条件v >= -1产生有界投影。通过组采样投影到有限经验子空间，利用组归一化优势求和为零的特性简化约束。

Result: 在数学推理基准测试中，GOPO实现了竞争性的泛化性能，同时保持稳定的梯度动态和熵保持，在基于裁剪的方法达到平台期时仍能继续改进。

Conclusion: GOPO提供了一种基于希尔伯特空间几何的新对齐框架，具有常数Hessian曲率、非饱和线性梯度和内在死区机制，无需启发式裁剪，在保持训练稳定性的同时实现有效对齐。

Abstract: We present Group Orthogonalized Policy Optimization (GOPO), a new alignment algorithm for large language models derived from the geometry of Hilbert function spaces. Instead of optimizing on the probability simplex and inheriting the exponential curvature of Kullback-Leibler divergence, GOPO lifts alignment into the Hilbert space L2(pi_k) of square-integrable functions with respect to the reference policy. Within this space, the simplex constraint reduces to a linear orthogonality condition <v, 1> = 0, defining a codimension-one subspace H0. Minimizing distance to an unconstrained target u_star yields the work-dissipation functional J(v) = <g, v> - (mu / 2) ||v||^2, whose maximizer follows directly from the Hilbert projection theorem. Enforcing the boundary v >= -1 produces a bounded Hilbert projection that induces exact sparsity, assigning zero probability to catastrophically poor actions through a closed-form threshold. To connect this functional theory with practice, GOPO projects from infinite-dimensional L2(pi_k) to a finite empirical subspace induced by group sampling. Because group-normalized advantages sum to zero, the Lagrange multiplier enforcing probability conservation vanishes exactly, reducing the constrained projection to an unconstrained empirical loss. The resulting objective has constant Hessian curvature mu I, non-saturating linear gradients, and an intrinsic dead-zone mechanism without heuristic clipping. Experiments on mathematical reasoning benchmarks show that GOPO achieves competitive generalization while maintaining stable gradient dynamics and entropy preservation in regimes where clipping-based methods plateau.

</details>


### [7] [Neural network optimization strategies and the topography of the loss landscape](https://arxiv.org/abs/2602.21276)
*Jianneng Yu,Alexandre V. Morozov*

Main category: cs.LG

TL;DR: 对比SGD与拟牛顿法在神经网络训练中的差异：SGD探索平滑吸引盆，拟牛顿法能找到更深但更孤立的极小值，后者泛化能力较差。


<details>
  <summary>Details</summary>
Motivation: 研究不同优化器如何影响神经网络在损失景观中的解的性质，特别是训练解在未见测试数据上的泛化性能差异。

Method: 使用随机梯度下降(SGD)与拟牛顿法(利用曲率信息和黄金分割搜索)训练神经网络，通过核主成分分析和新算法FourierPathFinder分析参数空间。

Result: SGD解之间的障碍较低，探索平滑吸引盆；拟牛顿法能找到更深的极小值，但这些解在参数空间中更分散且泛化能力较差。

Conclusion: 优化器的选择深刻影响解的性质，SGD探索平滑区域产生更鲁棒、可迁移的模型，而拟牛顿法能找到更深但泛化较差的极小值。

Abstract: Neural networks are trained by optimizing multi-dimensional sets of fitting parameters on non-convex loss landscapes. Low-loss regions of the landscapes correspond to the parameter sets that perform well on the training data. A key issue in machine learning is the performance of trained neural networks on previously unseen test data. Here, we investigate neural network training by stochastic gradient descent (SGD) - a non-convex global optimization algorithm which relies only on the gradient of the objective function. We contrast SGD solutions with those obtained via a non-stochastic quasi-Newton method, which utilizes curvature information to determine step direction and Golden Section Search to choose step size. We use several computational tools to investigate neural network parameters obtained by these two optimization methods, including kernel Principal Component Analysis and a novel, general-purpose algorithm for finding low-height paths between pairs of points on loss or energy landscapes, FourierPathFinder. We find that the choice of the optimizer profoundly affects the nature of the resulting solutions. SGD solutions tend to be separated by lower barriers than quasi-Newton solutions, even if both sets of solutions are regularized by early stopping to ensure adequate performance on test data. When allowed to fit extensively on the training data, quasi-Newton solutions occupy deeper minima on the loss landscapes that are not reached by SGD. These solutions are less generalizable to the test data however. Overall, SGD explores smooth basins of attraction, while quasi-Newton optimization is capable of finding deeper, more isolated minima that are more spread out in the parameter space. Our findings help understand both the topography of the loss landscapes and the fundamental role of landscape exploration strategies in creating robust, transferrable neural network models.

</details>


### [8] [Robust AI Evaluation through Maximal Lotteries](https://arxiv.org/abs/2602.21297)
*Hadi Khalaf,Serena L. Wang,Daniel Halpern,Itai Shapira,Flavio du Pin Calmon,Ariel D. Procaccia*

Main category: cs.LG

TL;DR: 该论文提出用稳健彩票替代传统的Bradley-Terry排名，以更公平地聚合语言模型在主观任务中的异质性偏好，避免排名系统对特定任务或用户子群体的忽视。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型评估使用Bradley-Terry排名将异质性偏好强制压缩为单一总排序，违反了社会选择理论的基本要求，且对特定任务或用户子群体表现不佳的模型可能被忽视。

Method: 引入稳健彩票方法，在偏好数据的合理变动下优化最坏情况性能，替代传统的最大彩票方法，后者对偏好异质性过于敏感。

Result: 在大规模偏好数据集上，稳健彩票在整个标注者分布中提供更可靠的胜率保证，并恢复了一组稳定的顶级模型，相比传统排名方法更具鲁棒性。

Conclusion: 从单一排名转向多元化的获胜者集合，稳健彩票为实现服务于全谱人类偏好的互补AI系统生态系统提供了原则性步骤。

Abstract: The standard way to evaluate language models on subjective tasks is through pairwise comparisons: an annotator chooses the "better" of two responses to a prompt. Leaderboards aggregate these comparisons into a single Bradley-Terry (BT) ranking, forcing heterogeneous preferences into a total order and violating basic social-choice desiderata. In contrast, social choice theory provides an alternative approach called maximal lotteries, which aggregates pairwise preferences without imposing any assumptions on their structure. However, we show that maximal lotteries are highly sensitive to preference heterogeneity and can favor models that severely underperform on specific tasks or user subpopulations. We introduce robust lotteries that optimize worst-case performance under plausible shifts in the preference data. On large-scale preference datasets, robust lotteries provide more reliable win rate guarantees across the annotator distribution and recover a stable set of top-performing models. By moving from rankings to pluralistic sets of winners, robust lotteries offer a principled step toward an ecosystem of complementary AI systems that serve the full spectrum of human preferences.

</details>


### [9] [SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks](https://arxiv.org/abs/2602.21307)
*Elizabeth S. Z. Tan,Adil Soubki,Miles Cranmer*

Main category: cs.LG

TL;DR: SymTorch是一个自动化符号蒸馏的库，通过将神经网络组件替换为可解释的数学表达式来加速推理并提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 符号蒸馏能够从训练好的深度学习模型中直接发现物理定律和数学关系，但由于将符号回归集成到深度学习工作流中的工程障碍，其采用仍然有限。

Method: SymTorch库通过包装神经网络组件、收集其输入输出行为，并使用PySR通过人类可读的方程来近似这些组件，自动化了符号蒸馏过程。它处理了阻碍采用的工程挑战：GPU-CPU数据传输、输入输出缓存、模型序列化，以及在神经和符号前向传递之间的无缝切换。

Result: SymTorch在包括GNNs、PINNs和transformer模型在内的多种架构上进行了演示。通过用符号替代品替换MLP层来加速LLM推理的概念验证实现了8.3%的吞吐量提升，同时性能适度下降。

Conclusion: SymTorch降低了符号蒸馏的工程障碍，使其更容易集成到深度学习工作流中，为加速推理和提高模型可解释性提供了实用工具。

Abstract: Symbolic distillation replaces neural networks, or components thereof, with interpretable, closed-form mathematical expressions. This approach has shown promise in discovering physical laws and mathematical relationships directly from trained deep learning models, yet adoption remains limited due to the engineering barrier of integrating symbolic regression into deep learning workflows. We introduce SymTorch, a library that automates this distillation by wrapping neural network components, collecting their input-output behavior, and approximating them with human-readable equations via PySR. SymTorch handles the engineering challenges that have hindered adoption: GPU-CPU data transfer, input-output caching, model serialization, and seamless switching between neural and symbolic forward passes. We demonstrate SymTorch across diverse architectures including GNNs, PINNs and transformer models. Finally, we present a proof-of-concept for accelerating LLM inference by replacing MLP layers with symbolic surrogates, achieving an 8.3\% throughput improvement with moderate performance degradation.

</details>


### [10] [Shared Nature, Unique Nurture: PRISM for Pluralistic Reasoning via In-context Structure Modeling](https://arxiv.org/abs/2602.21317)
*Guancheng Tu,Shiyang Zhang,Tianyu Zhang,Yi Zhang,Diji Yang*

Main category: cs.LG

TL;DR: 该论文提出PRISM系统，通过推理时的个体化认知轨迹来增强LLMs，解决LLMs向单一"人工蜂群思维"收敛导致的分布多样性崩溃问题，在创造性和罕见疾病诊断任务上取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型正在向单一的人工蜂群思维收敛，共享的预训练先验导致分布多样性严重崩溃，限制了创造性探索和科学发现所需的独特视角。

Method: 提出PRISM（通过上下文结构建模的多元推理）系统，采用认知进化范式（探索、内化、表达），通过动态的即时认知图增强LLMs，实现推理时的个体化培养。

Result: 在三个创造力基准测试中，PRISM实现了最先进的新颖性并显著扩展了分布多样性。在具有挑战性的罕见疾病诊断基准测试中，PRISM成功发现了标准LLMs遗漏的正确长尾诊断。

Conclusion: 这项工作为多元AI建立了新范式，从单一共识转向由独特认知个体组成的多样化生态系统，能够进行集体的多视角发现。

Abstract: Large Language Models (LLMs) are converging towards a singular Artificial Hivemind, where shared Nature (pre-training priors) result in a profound collapse of distributional diversity, limiting the distinct perspectives necessary for creative exploration and scientific discovery. To address this, we propose to equip models with inference-time Nurture (individualized epistemic trajectories) using Epistemic Evolution paradigm, progressing through explore, internalize, and express. We instantiate this via PRISM (Pluralistic Reasoning via In-context Structure Modeling), a model-agnostic system that augments LLM with dynamic On-the-fly Epistemic Graphs. On three creativity benchmarks, PRISM achieves state-of-the-art novelty and significantly expands distributional diversity. Moreover, we evaluate the real-world utility via a challenging rare-disease diagnosis benchmark. Results demonstrate that PRISM successfully uncovers correct long-tail diagnoses that standard LLM miss, confirming that its divergence stems from meaningful exploration rather than incoherent noise. Overall, this work establishes a new paradigm for Pluralistic AI, moving beyond monolithic consensus toward a diverse ecosystem of unique cognitive individuals capable of collective, multi-perspective discovery.

</details>


### [11] [Uncertainty-Aware Diffusion Model for Multimodal Highway Trajectory Prediction via DDIM Sampling](https://arxiv.org/abs/2602.21319)
*Marion Neumeier,Niklas Roßberg,Michael Botsch,Wolfgang Utschick*

Main category: cs.LG

TL;DR: cVMDx是改进的扩散模型轨迹预测框架，通过DDIM采样实现100倍加速，结合高斯混合模型提供多模态预测，在highD数据集上表现优于cVMD


<details>
  <summary>Details</summary>
Motivation: 自动驾驶轨迹预测面临多智能体交互、场景多样性和未来运动随机性等挑战。现有扩散模型方法如cVMD存在采样慢、生成多样性利用不足和场景编码脆弱等问题

Method: 提出cVMDx框架：1) 采用DDIM采样加速推理100倍；2) 使用拟合的高斯混合模型从生成轨迹中提取可处理的多模态预测；3) 评估CVQ-VAE变体用于场景编码

Result: 在highD数据集上的实验显示，cVMDx比cVMD具有更高的准确性和显著改进的效率，能够实现完全随机的多模态轨迹预测

Conclusion: cVMDx通过改进采样效率和预测多样性，为自动驾驶提供了更实用、鲁棒的多模态轨迹预测解决方案

Abstract: Accurate and uncertainty-aware trajectory prediction remains a core challenge for autonomous driving, driven by complex multi-agent interactions, diverse scene contexts and the inherently stochastic nature of future motion. Diffusion-based generative models have recently shown strong potential for capturing multimodal futures, yet existing approaches such as cVMD suffer from slow sampling, limited exploitation of generative diversity and brittle scenario encodings.
  This work introduces cVMDx, an enhanced diffusion-based trajectory prediction framework that improves efficiency, robustness and multimodal predictive capability. Through DDIM sampling, cVMDx achieves up to a 100x reduction in inference time, enabling practical multi-sample generation for uncertainty estimation. A fitted Gaussian Mixture Model further provides tractable multimodal predictions from the generated trajectories. In addition, a CVQ-VAE variant is evaluated for scenario encoding. Experiments on the publicly available highD dataset show that cVMDx achieves higher accuracy and significantly improved efficiency over cVMD, enabling fully stochastic, multimodal trajectory prediction.

</details>


### [12] [Tool-R0: Self-Evolving LLM Agents for Tool-Learning from Zero Data](https://arxiv.org/abs/2602.21320)
*Emre Can Acikgoz,Cheng Qian,Jonas Hübotter,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur*

Main category: cs.LG

TL;DR: Tool-R0框架通过自博弈强化学习从零开始训练通用工具调用智能体，无需预训练数据，实现了92.5%的性能提升


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的智能体训练需要精心构建的任务-解决方案对和大量人工监督，这限制了智能体向超级智能系统的开放式自我进化

Method: 提出Tool-R0框架，从同一基础LLM初始化生成器和求解器，通过互补奖励进行协同进化：生成器提出具有挑战性的任务，求解器学习使用真实工具解决这些任务

Result: 在不同工具使用基准测试中，Tool-R0相比基础模型实现了92.5%的相对改进，并在相同设置下超越了完全监督的工具调用基线

Conclusion: Tool-R0框架为从零开始训练通用工具调用智能体提供了一种无需数据的方法，并通过分析协同进化、课程动态和缩放行为为自博弈LLM智能体提供了实证见解

Abstract: Large language models (LLMs) are becoming the foundation for autonomous agents that can use tools to solve complex tasks. Reinforcement learning (RL) has emerged as a common approach for injecting such agentic capabilities, but typically under tightly controlled training setups. It often depends on carefully constructed task-solution pairs and substantial human supervision, which creates a fundamental obstacle to open-ended self-evolution toward superintelligent systems. In this paper, we propose Tool-R0 framework for training general purpose tool-calling agents from scratch with self-play RL, under a zero-data assumption. Initialized from the same base LLM, Tool-R0 co-evolves a Generator and a Solver with complementary rewards: one proposes targeted challenging tasks at the other's competence frontier and the other learns to solve them with real-world tool calls. This creates a self-evolving cycle that requires no pre-existing tasks or datasets. Evaluation on different tool-use benchmarks show that Tool-R0 yields 92.5 relative improvement over the base model and surpasses fully supervised tool-calling baselines under the same setting. Our work further provides empirical insights into self-play LLM agents by analyzing co-evolution, curriculum dynamics, and scaling behavior.

</details>


### [13] [Dynamic Symmetric Point Tracking: Tackling Non-ideal Reference in Analog In-memory Training](https://arxiv.org/abs/2602.21321)
*Quan Xiao,Jindan Li,Zhaoxian Wu,Tayfun Gokmen,Tianyi Chen*

Main category: cs.LG

TL;DR: 提出动态对称点估计方法，解决模拟内存计算中权重更新不对称导致的系统偏差问题，无需昂贵的预校准


<details>
  <summary>Details</summary>
Motivation: 模拟内存计算(AIMC)具有能效优势，但设备非理想特性（特别是更新不对称性）会导致权重更新向设备特定对称点漂移，偏离训练目标最优解。现有方法需要预先校准对称点为零，但校准成本高且残留误差会降低训练精度。

Method: 1) 首次理论分析对称点校准的脉冲复杂度和估计误差；2) 提出动态对称点估计方法，在模型训练过程中跟踪对称点变化；3) 基于数字信号处理中的斩波和滤波技术开发增强变体；4) 建立收敛性保证。

Result: 数值实验证明所提方法既高效又有效，能够在不依赖昂贵预校准的情况下缓解权重更新偏差问题。

Conclusion: 动态对称点估计方法为模拟内存计算中的训练挑战提供了理论分析和实用解决方案，通过在线跟踪对称点变化，避免了昂贵的设备预校准需求，同时保证了训练收敛性。

Abstract: Analog in-memory computing (AIMC) performs computation directly within resistive crossbar arrays, offering an energy-efficient platform to scale large vision and language models. However, non-ideal analog device properties make the training on AIMC devices challenging. In particular, its update asymmetry can induce a systematic drift of weight updates towards a device-specific symmetric point (SP), which typically does not align with the optimum of the training objective. To mitigate this bias, most existing works assume the SP is known and pre-calibrate it to zero before training by setting the reference point as the SP. Nevertheless, calibrating AIMC devices requires costly pulse updates, and residual calibration error can directly degrade training accuracy. In this work, we present the first theoretical characterization of the pulse complexity of SP calibration and the resulting estimation error. We further propose a dynamic SP estimation method that tracks the SP during model training, and establishes its convergence guarantees. In addition, we develop an enhanced variant based on chopping and filtering techniques from digital signal processing. Numerical experiments demonstrate both the efficiency and effectiveness of the proposed method.

</details>


### [14] [Equitable Evaluation via Elicitation](https://arxiv.org/abs/2602.21327)
*Elbert Du,Cynthia Dwork,Lunjia Hu,Reid McIlroy-Young,Han Shao,Linjun Zhang*

Main category: cs.LG

TL;DR: 开发了一个用于技能评估的交互式AI系统，通过引导式提问来准确获取技能信息，同时允许个人保持自己的表达风格，并采用数学方法确保评估的公平性。


<details>
  <summary>Details</summary>
Motivation: 在职业环境中，具有相似资格和技能的个人在自我展示方式上存在差异：有些人倾向于自我推销，而有些人则过于谦虚，甚至遗漏关键信息。这使得比较具有不同自我展示风格的同等资格求职者变得困难。

Method: 1. 构建交互式AI进行技能引导，通过提问准确确定技能，同时允许个人以自己的声音表达；2. 使用LLM生成合成人类数据以获得足够的训练数据；3. 通过数学上严格的公平性概念来应对系统性模型偏差，确保自我展示方式与技能评估误差之间的协方差很小。

Result: 开发了一个能够准确评估技能同时尊重个人表达风格的AI系统，该系统可以部署在专业社交平台新用户加入时，或公司重组期间匹配员工需求时。

Conclusion: 通过技能引导方法可以减轻由个人自我报告产生的内生偏差，并通过数学公平性约束解决系统性模型偏差，从而创建更公平、准确的技能评估系统。

Abstract: Individuals with similar qualifications and skills may vary in their demeanor, or outward manner: some tend toward self-promotion while others are modest to the point of omitting crucial information. Comparing the self-descriptions of equally qualified job-seekers with different self-presentation styles is therefore problematic.
  We build an interactive AI for skill elicitation that provides accurate determination of skills while simultaneously allowing individuals to speak in their own voice. Such a system can be deployed, for example, when a new user joins a professional networking platform, or when matching employees to needs during a company reorganization. To obtain sufficient training data, we train an LLM to act as synthetic humans.
  Elicitation mitigates endogenous bias arising from individuals' own self-reports. To address systematic model bias we enforce a mathematically rigorous notion of equitability ensuring that the covariance between self-presentation manner and skill evaluation error is small.

</details>


### [15] [Efficient Opportunistic Approachability](https://arxiv.org/abs/2602.21328)
*Teodor Vanislavov Marinov,Mehryar Mohri,Princewill Okoroafor,Jon Schneider,Julian Zimmert*

Main category: cs.LG

TL;DR: 本文提出了一种高效的机遇性逼近算法，无需在线校准子程序，实现了O(T^{-1/4})的收敛速率，并在对手动作集维度≤2时达到最优速率O(T^{-1/2})。


<details>
  <summary>Details</summary>
Motivation: 机遇性逼近是Blackwell逼近的推广，学习者在对手限制其动作空间子集时希望获得更强的保证。现有算法需要在线校准预测对手动作，这需要指数时间且收敛速率慢(T^{-O(1/d)})，因此需要更高效的算法。

Method: 提出了一种无需在线校准子程序的高效算法，实现了O(T^{-1/4})的收敛速率，同时提供了一个低效但更快(O(T^{-1/3}))的算法。特别地，当对手动作集维度≤2时，设计了能达到最优速率O(T^{-1/2})的算法。

Result: 1) 高效算法达到O(T^{-1/4})收敛速率；2) 低效算法达到O(T^{-1/3})收敛速率；3) 在对手动作集维度≤2时，算法达到最优速率O(T^{-1/2})，显著优于现有需要指数时间的算法。

Conclusion: 本文通过绕过在线校准需求，显著提高了机遇性逼近的计算效率和收敛速率，特别是在低维情况下达到了理论最优速率，解决了现有算法计算复杂度高的问题。

Abstract: We study the problem of opportunistic approachability: a generalization of Blackwell approachability where the learner would like to obtain stronger guarantees (i.e., approach a smaller set) when their adversary limits themselves to a subset of their possible action space. Bernstein et al. (2014) introduced this problem in 2014 and presented an algorithm that guarantees sublinear approachability rates for opportunistic approachability. However, this algorithm requires the ability to produce calibrated online predictions of the adversary's actions, a problem whose standard implementations require time exponential in the ambient dimension and result in approachability rates that scale as $T^{-O(1/d)}$. In this paper, we present an efficient algorithm for opportunistic approachability that achieves a rate of $O(T^{-1/4})$ (and an inefficient one that achieves a rate of $O(T^{-1/3})$), bypassing the need for an online calibration subroutine. Moreover, in the case where the dimension of the adversary's action set is at most two, we show it is possible to obtain the optimal rate of $O(T^{-1/2})$.

</details>


### [16] [HiPPO Zoo: Explicit Memory Mechanisms for Interpretable State Space Models](https://arxiv.org/abs/2602.21340)
*Jack Goffinet,Casey Hanks,David E. Carlson*

Main category: cs.LG

TL;DR: 本文重新审视HiPPO框架，通过扩展多项式历史表示来支持现代状态空间模型的特性（如自适应内存分配和关联记忆），同时保持正交多项式基中的直接可解释性，提出了包含五种扩展的统一框架"HiPPO动物园"。


<details>
  <summary>Details</summary>
Motivation: 非线性状态空间模型（如Mamba）在许多长程依赖任务中达到最先进水平，但它们表示和优先处理历史的机制仍然很大程度上是隐式的。本文旨在使这些机制变得明确，同时保留HiPPO框架的原则性方法。

Method: 通过扩展多项式历史表示，支持现代状态空间模型的特性，如自适应内存分配和关联记忆，同时保持正交多项式基中的直接可解释性。提出了包含五种扩展的统一框架"HiPPO动物园"，每种扩展都通过HiPPO框架的明确、可解释修改来暴露特定的建模能力。

Result: 提出的模型能够在线自适应内存，在流式设置中通过高效更新进行训练。通过一系列合成序列建模任务展示了这些扩展的行为和建模优势，证明现代状态空间模型的典型能力可以通过明确、可解释的多项式内存结构实现。

Conclusion: 现代状态空间模型的能力可以通过明确、可解释的多项式内存结构实现，HiPPO框架的扩展为序列建模提供了更透明和可控的方法，同时保持了高效性和适应性。

Abstract: Representing the past in a compressed, efficient, and informative manner is a central problem for systems trained on sequential data. The HiPPO framework, originally proposed by Gu & Dao et al., provides a principled approach to sequential compression by projecting signals onto orthogonal polynomial (OP) bases via structured linear ordinary differential equations. Subsequent works have embedded these dynamics in state space models (SSMs), where HiPPO structure serves as an initialization. Nonlinear successors of these SSM methods such as Mamba are state-of-the-art for many tasks with long-range dependencies, but the mechanisms by which they represent and prioritize history remain largely implicit. In this work, we revisit the HiPPO framework with the goal of making these mechanisms explicit. We show how polynomial representations of history can be extended to support capabilities of modern SSMs such as adaptive allocation of memory and associative memory while retaining direct interpretability in the OP basis. We introduce a unified framework comprising five such extensions, which we collectively refer to as a "HiPPO zoo." Each extension exposes a specific modeling capability through an explicit, interpretable modification of the HiPPO framework. The resulting models adapt their memory online and train in streaming settings with efficient updates. We illustrate the behaviors and modeling advantages of these extensions through a range of synthetic sequence modeling tasks, demonstrating that capabilities typically associated with modern SSMs can be realized through explicit, interpretable polynomial memory structures.

</details>


### [17] [Archetypal Graph Generative Models: Explainable and Identifiable Communities via Anchor-Dominant Convex Hulls](https://arxiv.org/abs/2602.21342)
*Nikolaos Nakis,Chrysoula Kosma,Panagiotis Promponas,Michail Chatzianastasis,Giannis Nikolentzos*

Main category: cs.LG

TL;DR: GraphHull是一个可解释的生成模型，使用两级凸包表示网络，提供多尺度解释，在链路预测和社区检测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管图表示学习在下游任务中取得了高性能，但缺乏自解释模型。理解预测背后的模式同样重要，这推动了可解释机器学习的发展。

Method: GraphHull使用两级凸包表示网络：全局层面将凸包顶点作为原型（对应网络中的纯社区），局部层面每个社区通过原型凸包细化，顶点作为代表性配置文件。采用确定性点过程等先验，通过MAP估计和可扩展子采样拟合模型。

Result: 在真实网络上的实验表明，GraphHull能够恢复多层次社区结构，在链路预测和社区检测中达到竞争性或更优的性能，同时自然地提供可解释的预测。

Conclusion: GraphHull通过几何良好的两级凸包设计，实现了自解释的图表示学习，在保持高性能的同时提供多尺度可解释性，解决了图机器学习中可解释性不足的问题。

Abstract: Representation learning has been essential for graph machine learning tasks such as link prediction, community detection, and network visualization. Despite recent advances in achieving high performance on these downstream tasks, little progress has been made toward self-explainable models. Understanding the patterns behind predictions is equally important, motivating recent interest in explainable machine learning. In this paper, we present GraphHull, an explainable generative model that represents networks using two levels of convex hulls. At the global level, the vertices of a convex hull are treated as archetypes, each corresponding to a pure community in the network. At the local level, each community is refined by a prototypical hull whose vertices act as representative profiles, capturing community-specific variation. This two-level construction yields clear multi-scale explanations: a node's position relative to global archetypes and its local prototypes directly accounts for its edges. The geometry is well-behaved by design, while local hulls are kept disjoint by construction. To further encourage diversity and stability, we place principled priors, including determinantal point processes, and fit the model under MAP estimation with scalable subsampling. Experiments on real networks demonstrate the ability of GraphHull to recover multi-level community structure and to achieve competitive or superior performance in link prediction and community detection, while naturally providing interpretable predictions.

</details>


### [18] [Black-Box Reliability Certification for AI Agents via Self-Consistency Sampling and Conformal Calibration](https://arxiv.org/abs/2602.21368)
*Charafeddine Mouzouni*

Main category: cs.LG

TL;DR: 该论文提出了一种基于自一致性采样和保形校准的黑盒AI系统可靠性评估方法，为系统-任务对提供具有精确、有限样本、无分布保证的单一可靠性数值。


<details>
  <summary>Details</summary>
Motivation: 面对黑盒AI系统，实践者需要知道在什么置信水平上可以信任系统的输出。当前缺乏一种能够提供严格统计保证的可靠性评估方法。

Method: 结合自一致性采样（指数级减少不确定性）和保形校准（保证在目标水平的1/(n+1)范围内正确），生成系统-任务对的可靠性水平。通过更大的答案集显示系统在困难问题上的错误。

Result: GPT-4.1在GSM8K上获得94.6%可靠性，在TruthfulQA上获得96.8%；GPT-4.1-nano在GSM8K上获得89.8%，在MMLU上获得66.5%。所有配置中可解项目的条件覆盖率超过0.93，顺序停止减少约50%的API成本。

Conclusion: 该方法为黑盒AI系统部署提供了具有严格统计保证的可靠性评估框架，能够透明地显示系统在不同难度任务上的表现，并为模型选择提供量化依据。

Abstract: Given a black-box AI system and a task, at what confidence level can a practitioner trust the system's output? We answer with a reliability level -- a single number per system-task pair, derived from self-consistency sampling and conformal calibration, that serves as a black-box deployment gate with exact, finite-sample, distribution-free guarantees. Self-consistency sampling reduces uncertainty exponentially; conformal calibration guarantees correctness within 1/(n+1) of the target level, regardless of the system's errors -- made transparently visible through larger answer sets for harder questions. Weaker models earn lower reliability levels (not accuracy -- see Definition 2.4): GPT-4.1 earns 94.6% on GSM8K and 96.8% on TruthfulQA, while GPT-4.1-nano earns 89.8% on GSM8K and 66.5% on MMLU. We validate across five benchmarks, five models from three families, and both synthetic and real data. Conditional coverage on solvable items exceeds 0.93 across all configurations; sequential stopping reduces API costs by around 50%.

</details>


### [19] [Interleaved Head Attention](https://arxiv.org/abs/2602.21371)
*Sai Surya Duvvuri,Chanakya Ekbote,Rachit Bansal,Rishabh Tiwari,Devvrit Khatri,David Brandfonbrener,Paul Liang,Inderjit Dhillon,Manzil Zaheer*

Main category: cs.LG

TL;DR: 论文提出Interleaved Head Attention (IHA)来解决多头注意力中头间缺乏通信的问题，通过创建伪头实现跨头混合，提升多步推理能力。


<details>
  <summary>Details</summary>
Motivation: 多头注意力(MHA)存在线性扩展限制：H个注意力头产生H个独立的注意力矩阵，头间在注意力计算时没有通信。这对于多步推理任务不利，因为正确答案需要从上下文多个部分聚合证据并在中间推理链上组合潜在token-to-token关系。

Method: 提出Interleaved Head Attention (IHA)，通过为每个头创建P个伪头（通常P=H），每个伪查询/键/值是所有H个原始查询、键和值的线性组合。伪查询和伪键头之间的交互在每个头中产生最多P²个注意力模式，参数开销为O(H²P)。

Result: 理论分析显示在合成任务上参数效率提升：多项式任务中IHA使用Θ(√k n²)参数 vs MHA的Θ(k n²)；顺序敏感CPM-3任务中IHA使用⌈√N_max⌉个头 vs MHA的N_max个。实际基准测试中，IHA在RULER的Multi-Key检索上提升10-20%，在OpenThoughts微调后，GSM8K提升5.8%，MATH-500提升2.8%。

Conclusion: IHA通过引入跨头混合机制有效解决了多头注意力中的头间通信限制，在多步推理任务上展现出显著性能提升，为大型语言模型的注意力机制提供了改进方向。

Abstract: Multi-Head Attention (MHA) is the core computational primitive underlying modern Large Language Models (LLMs). However, MHA suffers from a fundamental linear scaling limitation: $H$ attention heads produce exactly $H$ independent attention matrices, with no communication between heads during attention computation. This becomes problematic for multi-step reasoning, where correct answers depend on aggregating evidence from multiple parts of the context and composing latent token-to-token relations over a chain of intermediate inferences. To address this, we propose Interleaved Head Attention (IHA), which enables cross-head mixing by constructing $P$ pseudo-heads per head (typically $P=H$), where each pseudo query/key/value is a learned linear combination of all $H$ original queries, keys and values respectively. Interactions between pseudo-query and pseudo-key heads induce up to $P^2$ attention patterns per head with modest parameter overhead $\mathcal{O}(H^2P)$. We provide theory showing improved efficiency in terms of number of parameters on the synthetic Polynomial task (IHA uses $Θ(\sqrt{k}n^2)$ parameters vs. $Θ(kn^2)$ for MHA) and on the synthetic order-sensitive CPM-3 task (IHA uses $\lceil\sqrt{N_{\max}}\rceil$ heads vs. $N_{\max}$ for MHA). On real-world benchmarks, IHA improves Multi-Key retrieval on RULER by 10-20% (4k-16k) and, after fine-tuning for reasoning on OpenThoughts, improves GSM8K by 5.8% and MATH-500 by 2.8% (Majority Vote) over full attention.

</details>


### [20] [The Mean is the Mirage: Entropy-Adaptive Model Merging under Heterogeneous Domain Shifts in Medical Imaging](https://arxiv.org/abs/2602.21372)
*Sameer Ambekar,Reza Nasirigerdeh,Peter J. Schuffler,Lina Felsner,Daniel M. Lang,Julia A. Schnabel*

Main category: cs.LG

TL;DR: 提出一种基于熵的自适应在线模型融合方法，用于处理未见测试分布偏移下的医学图像分类，仅需前向传播即可生成批次特定的融合模型。


<details>
  <summary>Details</summary>
Motivation: 医学影像中，不同诊所使用私有数据进行本地微调，产生领域特定的模型（受扫描仪、协议和人群差异影响）。当部署到未见临床站点时，测试数据以无标签、非独立同分布批次到达，模型需要立即适应但无标签可用。传统均值融合在异构领域偏移下容易失败。

Method: 提出熵自适应全在线模型融合方法：1）仅通过前向传播生成批次特定的融合模型；2）分析均值融合失败原因；3）通过解耦编码器和分类头，使用独立融合系数缓解编码器-分类器不匹配问题。

Result: 在两个骨干网络上，使用九个医学和自然领域泛化图像分类数据集进行广泛评估，相比最先进基线方法，在各种标准评估和挑战性场景下均取得一致性能提升，同时保持测试时单模型推理效率。

Conclusion: 该方法能有效处理异构领域偏移下的模型融合问题，在医学图像分类中实现更好的领域泛化性能，且保持推理效率。

Abstract: Model merging under unseen test-time distribution shifts often renders naive strategies, such as mean averaging unreliable. This challenge is especially acute in medical imaging, where models are fine-tuned locally at clinics on private data, producing domain-specific models that differ by scanner, protocol, and population. When deployed at an unseen clinical site, test cases arrive in unlabeled, non-i.i.d. batches, and the model must adapt immediately without labels. In this work, we introduce an entropy-adaptive, fully online model-merging method that yields a batch-specific merged model via only forward passes, effectively leveraging target information. We further demonstrate why mean merging is prone to failure and misaligned under heterogeneous domain shifts. Next, we mitigate encoder classifier mismatch by decoupling the encoder and classification head, merging with separate merging coefficients. We extensively evaluate our method with state-of-the-art baselines using two backbones across nine medical and natural-domain generalization image classification datasets, showing consistent gains across standard evaluation and challenging scenarios. These performance gains are achieved while retaining single-model inference at test-time, thereby demonstrating the effectiveness of our method.

</details>


### [21] [VCDF: A Validated Consensus-Driven Framework for Time Series Causal Discovery](https://arxiv.org/abs/2602.21381)
*Gene Yu,Ce Guo,Wayne Luk*

Main category: cs.LG

TL;DR: VCDF是一个简单、方法无关的验证层，通过评估因果关系在分块时间子集上的稳定性来提高时间序列因果发现的鲁棒性，无需修改基础算法。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列因果发现方法对噪声、非平稳性和采样变异性敏感，需要提高鲁棒性和可靠性。

Method: 提出Validated Consensus-Driven Framework (VCDF)，通过评估因果关系在分块时间子集上的稳定性来验证和筛选结果，可应用于VAR-LiNGAM、PCMCI等方法而不需修改基础算法。

Result: 在合成数据集上，VCDF将VAR-LiNGAM的窗口和汇总F1分数提高了约0.08-0.12；在长度1000以上的时间序列上，绝对改进高达0.18；在模拟fMRI数据和IT监控场景中也表现出更好的稳定性和结构准确性。

Conclusion: VCDF为时间序列因果发现提供了一个有效的可靠性层，不改变基础建模假设，能显著提高鲁棒性和稳定性。

Abstract: Time series causal discovery is essential for understanding dynamic systems, yet many existing methods remain sensitive to noise, non-stationarity, and sampling variability. We propose the Validated Consensus-Driven Framework (VCDF), a simple and method-agnostic layer that improves robustness by evaluating the stability of causal relations across blocked temporal subsets. VCDF requires no modification to base algorithms and can be applied to methods such as VAR-LiNGAM and PCMCI. Experiments on synthetic datasets show that VCDF improves VAR-LiNGAM by approximately 0.08-0.12 in both window and summary F1 scores across diverse data characteristics, with gains most pronounced for moderate-to-long sequences. The framework also benefits from longer sequences, yielding up to 0.18 absolute improvement on time series of length 1000 and above. Evaluations on simulated fMRI data and IT-monitoring scenarios further demonstrate enhanced stability and structural accuracy under realistic noise conditions. VCDF provides an effective reliability layer for time series causal discovery without altering underlying modeling assumptions.

</details>


### [22] [Defensive Generation](https://arxiv.org/abs/2602.21390)
*Gabriele Farina,Juan Carlos Perdomo*

Main category: cs.LG

TL;DR: 提出Defensive Generation方法，在线生成不可伪造的生成模型，适用于标量、多类和向量值结果，能抵抗无限测试类检验，达到最优收敛速率。


<details>
  <summary>Details</summary>
Motivation: 解决在线生成不可伪造的生成模型问题，确保生成结果在预定义的计算测试下无法被数据证伪，特别是针对非伯努利结果和无限测试类。

Method: 利用RKHS在线高维多标定与期望变分不等式问题的联系，开发高效算法，应用于结果不可区分性，提出Defensive Generation方法。

Result: 首次高效在线生成非伯努利结果的不可区分生成模型，能抵抗无限测试类（包括高阶矩检验），在样本数上接近线性时间，达到最优T^{-1/2}生成误差率。

Conclusion: Defensive Generation方法在生成不可伪造模型方面取得突破，将多标定与变分不等式理论结合，为在线生成提供高效且理论保证的解决方案。

Abstract: We study the problem of efficiently producing, in an online fashion, generative models of scalar, multiclass, and vector-valued outcomes that cannot be falsified on the basis of the observed data and a pre-specified collection of computational tests. Our contributions are twofold. First, we expand on connections between online high-dimensional multicalibration with respect to an RKHS and recent advances in expected variational inequality problems, enabling efficient algorithms for the former. We then apply this algorithmic machinery to the problem of outcome indistinguishability. Our procedure, Defensive Generation, is the first to efficiently produce online outcome indistinguishable generative models of non-Bernoulli outcomes that are unfalsifiable with respect to infinite classes of tests, including those that examine higher-order moments of the generated distributions. Furthermore, our method runs in near-linear time in the number of samples and achieves the optimal, vanishing T^{-1/2} rate for generation error.

</details>


### [23] [FedVG: Gradient-Guided Aggregation for Enhanced Federated Learning](https://arxiv.org/abs/2602.21399)
*Alina Devkota,Jacob Thrasher,Donald Adjeroh,Binod Bhattarai,Prashnna K. Gyawali*

Main category: cs.LG

TL;DR: FedVG：一种基于梯度验证的联邦学习聚合框架，利用全局验证集评估客户端模型的泛化能力，通过层梯度范数计算客户端特定分数，改善异构数据下的性能


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端数据异构性导致客户端漂移问题，降低模型整体泛化性能，而现有方法过度关注表现不佳的客户端会加剧这一问题

Method: 提出FedVG框架，利用公开数据集构建全局验证集，通过计算客户端模型在验证集上的层梯度范数来评估泛化能力，基于此生成客户端特定分数指导联邦聚合

Result: 在自然图像和医学图像基准数据集上的实验表明，FedVG在高度异构设置下能持续提升性能，且能与多种先进FL算法无缝集成并进一步改善结果

Conclusion: FedVG通过全局验证集指导的梯度评估机制有效缓解客户端漂移问题，提升联邦学习在异构数据环境下的泛化性能，具有模块化和易集成优势

Abstract: Federated Learning (FL) enables collaborative model training across multiple clients without sharing their private data. However, data heterogeneity across clients leads to client drift, which degrades the overall generalization performance of the model. This effect is further compounded by overemphasis on poorly performing clients. To address this problem, we propose FedVG, a novel gradient-based federated aggregation framework that leverages a global validation set to guide the optimization process. Such a global validation set can be established using readily available public datasets, ensuring accessibility and consistency across clients without compromising privacy. In contrast to conventional approaches that prioritize client dataset volume, FedVG assesses the generalization ability of client models by measuring the magnitude of validation gradients across layers. Specifically, we compute layerwise gradient norms to derive a client-specific score that reflects how much each client needs to adjust for improved generalization on the global validation set, thereby enabling more informed and adaptive federated aggregation. Extensive experiments on both natural and medical image benchmarking datasets, across diverse model architectures, demonstrate that FedVG consistently improves performance, particularly in highly heterogeneous settings. Moreover, FedVG is modular and can be seamlessly integrated with various state-of-the-art FL algorithms, often further improving their results. Our code is available at https://github.com/alinadevkota/FedVG.

</details>


### [24] [Generative Bayesian Computation as a Scalable Alternative to Gaussian Process Surrogates](https://arxiv.org/abs/2602.21408)
*Nick Polson,Vadim Sokolov*

Main category: cs.LG

TL;DR: 提出基于隐式分位数网络的生成贝叶斯计算（GBC）作为高斯过程代理的替代方案，解决其立方计算成本、平稳性假设和高斯预测分布的限制，在多个基准测试中表现优于传统GP方法。


<details>
  <summary>Details</summary>
Motivation: 高斯过程代理在昂贵计算机实验中是默认工具，但其立方计算成本、平稳性假设和高斯预测分布限制了应用范围，需要一种能同时解决这三个限制的新框架。

Method: 提出生成贝叶斯计算（GBC）框架，使用隐式分位数网络（IQN）从输入-输出对中学习完整的条件分位数函数；在测试时，每个分位数级别的单次前向传递即可生成预测分布的样本。

Result: 在14个基准测试中，GBC在分段跳跃过程基准上CRPS提升11-26%，在10维Friedman函数上提升14%，可线性扩展到90,000个训练点；边界增强变体在二维跳跃数据集上匹配或优于模块化跳跃GP（最高46% CRPS提升）；在主动学习中，随机先验IQN集成在Rocket LGBB上实现比深度GP主动学习低近3倍的RMSE。

Conclusion: GBC在14个比较中的12个获得有利的点估计结果，为高斯过程代理提供了有前景的替代方案，特别是在非平稳、跳跃过程和大规模数据集上；GP在平滑表面上仍保持优势，因其平滑性先验提供有效正则化。

Abstract: Gaussian process (GP) surrogates are the default tool for emulating expensive computer experiments, but cubic cost, stationarity assumptions, and Gaussian predictive distributions limit their reach. We propose Generative Bayesian Computation (GBC) via Implicit Quantile Networks (IQNs) as a surrogate framework that targets all three limitations. GBC learns the full conditional quantile function from input--output pairs; at test time, a single forward pass per quantile level produces draws from the predictive distribution.
  Across fourteen benchmarks we compare GBC to four GP-based methods. GBC improves CRPS by 11--26\% on piecewise jump-process benchmarks, by 14\% on a ten-dimensional Friedman function, and scales linearly to 90,000 training points where dense-covariance GPs are infeasible. A boundary-augmented variant matches or outperforms Modular Jump GPs on two-dimensional jump datasets (up to 46\% CRPS improvement). In active learning, a randomized-prior IQN ensemble achieves nearly three times lower RMSE than deep GP active learning on Rocket LGBB. Overall, GBC records a favorable point estimate in 12 of 14 comparisons. GPs retain an edge on smooth surfaces where their smoothness prior provides effective regularization.

</details>


### [25] [Benchmarking State Space Models, Transformers, and Recurrent Networks for US Grid Forecasting](https://arxiv.org/abs/2602.21415)
*Sunki Hong,Jisoo Lee,Yuanyuan Shi*

Main category: cs.LG

TL;DR: 该论文对五种深度学习模型在电力网格预测任务上进行全面基准测试，发现没有适用于所有情况的最佳模型，模型性能取决于数据类型和预测任务。


<details>
  <summary>Details</summary>
Motivation: 为电力网格选择合适的深度学习模型具有挑战性，因为模型性能高度依赖于运营商可用的数据。需要系统评估不同神经网络架构在各种电力预测任务上的表现，为电网运营商提供选择指导。

Method: 对五种现代神经网络架构进行基准测试：两种状态空间模型（PowerMamba、S-Mamba）、两种Transformer（iTransformer、PatchTST）和传统LSTM。在六个美国电网的每小时电力需求数据上进行24-168小时预测。通过专用时间处理模块和天气协变量集成层确保公平比较，并扩展到太阳能发电、风电和批发价格预测。

Result: 结果显示没有适用于所有情况的最佳模型。仅使用历史负荷时，PatchTST和状态空间模型精度最高；加入天气数据后，iTransformer精度提升效率是PatchTST的三倍。模型性能与任务相关：PatchTST在太阳能等节奏性信号上表现优异，状态空间模型更适合风电和价格的混沌波动。

Conclusion: 模型选择应基于具体数据环境和预测任务。该基准测试为电网运营商提供了根据特定数据条件选择最优预测架构的实用指南。

Abstract: Selecting the right deep learning model for power grid forecasting is challenging, as performance heavily depends on the data available to the operator. This paper presents a comprehensive benchmark of five modern neural architectures: two state space models (PowerMamba, S-Mamba), two Transformers (iTransformer, PatchTST), and a traditional LSTM. We evaluate these models on hourly electricity demand across six diverse US power grids for forecast windows between 24 and 168 hours. To ensure a fair comparison, we adapt each model with specialized temporal processing and a modular layer that cleanly integrates weather covariates. Our results reveal that there is no single best model for all situations. When forecasting using only historical load, PatchTST and the state space models provide the highest accuracy. However, when explicit weather data is added to the inputs, the rankings reverse: iTransformer improves its accuracy three times more efficiently than PatchTST. By controlling for model size, we confirm that this advantage stems from the architecture's inherent ability to mix information across different variables. Extending our evaluation to solar generation, wind power, and wholesale prices further demonstrates that model rankings depend on the forecast task: PatchTST excels on highly rhythmic signals like solar, while state space models are better suited for the chaotic fluctuations of wind and price. Ultimately, this benchmark provides grid operators with actionable guidelines for selecting the optimal forecasting architecture based on their specific data environments.

</details>


### [26] [Overconfident Errors Need Stronger Correction: Asymmetric Confidence Penalties for Reinforcement Learning](https://arxiv.org/abs/2602.21420)
*Yuanda Xu,Hejian Sang,Zhengze Zhou,Ran He,Zhipeng Wang*

Main category: cs.LG

TL;DR: 提出ACE方法解决RLVR中均匀惩罚错误导致推理边界收窄的问题，通过置信度感知的非对称错误惩罚提升LLM推理性能


<details>
  <summary>Details</summary>
Motivation: 标准RLVR算法虽然能提高Pass@1准确率，但会收窄模型的推理边界并降低生成多样性。现有方法对所有错误进行均匀惩罚，导致过度自信的错误持续存在并垄断概率质量，抑制有效的探索轨迹。

Method: 提出非对称置信感知错误惩罚(ACE)，引入每个rollout的置信度偏移度量c_i = log(pi_theta(y_i|x) / pi_ref(y_i|x))，动态调节负优势。理论上证明ACE梯度可分解为针对过度自信错误的选择性正则化器梯度加上部分调节正则化器强度的残差。

Result: 在Qwen2.5-Math-7B、Qwen3-8B-Base和Llama-3.1-8B-Instruct模型上使用GRPO和DAPO在VERL框架中进行实验。在MATH-500和AIME 2025基准测试中，ACE与现有方法无缝结合，在所有三个模型家族和基准测试中持续改善完整的Pass@k频谱。

Conclusion: ACE方法有效解决了RLVR中的均匀惩罚问题，通过置信度感知的非对称惩罚机制提升了LLM推理性能，同时保持了生成多样性。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become the leading paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard RLVR algorithms suffer from a well-documented pathology: while they improve Pass@1 accuracy through sharpened sampling, they simultaneously narrow the model's reasoning boundary and reduce generation diversity. We identify a root cause that existing methods overlook: the uniform penalization of errors. Current approaches -- whether data-filtering methods that select prompts by difficulty, or advantage normalization schemes -- treat all incorrect rollouts within a group identically. We show that this uniformity allows overconfident errors (incorrect reasoning paths that the RL process has spuriously reinforced) to persist and monopolize probability mass, ultimately suppressing valid exploratory trajectories. To address this, we propose the Asymmetric Confidence-aware Error Penalty (ACE). ACE introduces a per-rollout confidence shift metric, c_i = log(pi_theta(y_i|x) / pi_ref(y_i|x)), to dynamically modulate negative advantages. Theoretically, we demonstrate that ACE's gradient can be decomposed into the gradient of a selective regularizer restricted to overconfident errors, plus a well-characterized residual that partially moderates the regularizer's strength. We conduct extensive experiments fine-tuning Qwen2.5-Math-7B, Qwen3-8B-Base, and Llama-3.1-8B-Instruct on the DAPO-Math-17K dataset using GRPO and DAPO within the VERL framework. Evaluated on MATH-500 and AIME 2025, ACE composes seamlessly with existing methods and consistently improves the full Pass@k spectrum across all three model families and benchmarks.

</details>


### [27] [On the Structural Non-Preservation of Epistemic Behaviour under Policy Transformation](https://arxiv.org/abs/2602.21424)
*Alexander Galozy*

Main category: cs.LG

TL;DR: 本文形式化了强化学习中基于内部信息的行为依赖概念，提出了行为等价性和行为距离的度量，并证明了在凸组合下行为距离会收缩的结构性结果。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测环境中，强化学习智能体通常基于内部积累的信息（如记忆或推断的潜在上下文）来选择动作。然而，这种基于内部信息的行为依赖如何受到常见策略变换（如凸组合）的影响尚不清楚，需要形式化分析。

Method: 形式化定义了行为依赖的概念，提出了基于探针的行为等价性和行为距离度量。建立了三个结构性结果：1）非平凡行为依赖的策略集在凸聚合下不封闭；2）行为距离在凸组合下收缩；3）在特定条件下，梯度上升会减少行为距离。通过简单的赌博机和部分可观测网格世界实验验证。

Result: 实验表明，在凸聚合和带有偏斜潜在先验的持续优化下，行为距离会减小。在这些实验中，行为距离的减小先于潜在先验偏移下的性能退化。这些结果识别了探针条件行为分离在常见策略变换下不被保留的结构条件。

Conclusion: 本文形式化了强化学习中的行为依赖概念，证明了在凸组合等常见策略变换下，基于内部信息的行为分离可能不被保留。这为理解策略优化过程中行为特性的变化提供了理论框架，并揭示了可能导致性能退化的潜在机制。

Abstract: Reinforcement learning (RL) agents under partial observability often condition actions on internally accumulated information such as memory or inferred latent context. We formalise such information-conditioned interaction patterns as behavioural dependency: variation in action selection with respect to internal information under fixed observations. This induces a probe-relative notion of $ε$-behavioural equivalence and a within-policy behavioural distance that quantifies probe sensitivity. We establish three structural results. First, the set of policies exhibiting non-trivial behavioural dependency is not closed under convex aggregation. Second, behavioural distance contracts under convex combination. Third, we prove a sufficient local condition under which gradient ascent on a skewed mixture objective decreases behavioural distance when a dominant-mode gradient aligns with the direction of steepest contraction. Minimal bandit and partially observable gridworld experiments provide controlled witnesses of these mechanisms. In the examined settings, behavioural distance decreases under convex aggregation and under continued optimisation with skewed latent priors, and in these experiments it precedes degradation under latent prior shift. These results identify structural conditions under which probe-conditioned behavioural separation is not preserved under common policy transformations.

</details>


### [28] [Proximal-IMH: Proximal Posterior Proposals for Independent Metropolis-Hastings with Approximate Operators](https://arxiv.org/abs/2602.21426)
*Youguang Chen,George Biros*

Main category: cs.LG

TL;DR: 提出Proximal-IMH方法，通过优化问题校正近似后验分布的偏差，提高贝叶斯逆问题中的采样效率


<details>
  <summary>Details</summary>
Motivation: 解决贝叶斯逆问题中后验分布采样困难的问题，特别是当精确后验采样计算成本过高时，需要利用更便宜的近似后验但需校正其偏差

Method: 提出Proximal-IMH方法，属于独立Metropolis-Hastings采样算法家族。通过辅助优化问题校正近似后验的样本，在精确模型拟合和近似参考点稳定性之间权衡，适用于线性和非线性输入输出算子

Result: 理论证明近端校正能缩小近似后验与精确后验之间的差距，提高接受率和混合效率。数值实验（包括多模态和数据驱动先验的非线性算子）显示Proximal-IMH可靠地优于现有IMH变体

Conclusion: Proximal-IMH为计算昂贵的贝叶斯逆问题提供了一种有效的采样方法，通过校正近似后验偏差，在保持计算效率的同时提高采样质量

Abstract: We consider the problem of sampling from a posterior distribution arising in Bayesian inverse problems in science, engineering, and imaging. Our method belongs to the family of independence Metropolis-Hastings (IMH) sampling algorithms, which are common in Bayesian inference. Relying on the existence of an approximate posterior distribution that is cheaper to sample from but may have significant bias, we introduce Proximal-IMH, a scheme that removes this bias by correcting samples from the approximate posterior through an auxiliary optimization problem. This yields a local adjustment that trades off adherence to the exact model against stability around the approximate reference point. For idealized settings, we prove that the proximal correction tightens the match between approximate and exact posteriors, thereby improving acceptance rates and mixing. The method applies to both linear and nonlinear input-output operators and is particularly suitable for inverse problems where exact posterior sampling is too expensive. We present numerical experiments including multimodal and data-driven priors with nonlinear input-output operators. The results show that Proximal-IMH reliably outperforms existing IMH variants.

</details>


### [29] [Provably Safe Generative Sampling with Constricting Barrier Functions](https://arxiv.org/abs/2602.21429)
*Darshan Gadginmath,Ahmed Allibhoy,Fabio Pasqualetti*

Main category: cs.LG

TL;DR: 提出一个安全过滤框架，为预训练的流式生成模型提供在线安全防护，通过控制屏障函数和凸二次规划保证生成样本满足硬约束，无需重新训练或修改架构。


<details>
  <summary>Details</summary>
Motivation: 流式生成模型（如扩散模型和流匹配模型）在复杂数据分布学习上取得了显著成功，但在安全关键领域部署时缺乏形式化保证，无法确保生成样本满足硬约束条件。

Method: 提出安全过滤框架，定义收缩安全管，从初始噪声分布的宽松约束逐步收紧到最终数据分布的目标安全集。使用控制屏障函数表征安全管，通过凸二次规划在每个采样步骤合成反馈控制输入，最小化对模型学习结构的干扰。

Result: 在约束图像生成、物理一致轨迹采样和安全机器人操作策略等任务中验证，实现了100%约束满足率，同时保持了语义保真度。框架适用于任何预训练的流式生成方案。

Conclusion: 该框架为流式生成模型提供了形式化的安全保证，通过在线安全防护机制确保生成样本满足硬约束，无需重新训练或架构修改，在安全关键领域具有重要应用价值。

Abstract: Flow-based generative models, such as diffusion models and flow matching models, have achieved remarkable success in learning complex data distributions. However, a critical gap remains for their deployment in safety-critical domains: the lack of formal guarantees that generated samples will satisfy hard constraints. We address this by proposing a safety filtering framework that acts as an online shield for any pre-trained generative model. Our key insight is to cooperate with the generative process rather than override it. We define a constricting safety tube that is relaxed at the initial noise distribution and progressively tightens to the target safe set at the final data distribution, mirroring the coarse-to-fine structure of the generative process itself. By characterizing this tube via Control Barrier Functions (CBFs), we synthesize a feedback control input through a convex Quadratic Program (QP) at each sampling step. As the tube is loosest when noise is high and intervention is cheapest in terms of control energy, most constraint enforcement occurs when it least disrupts the model's learned structure. We prove that this mechanism guarantees safe sampling while minimizing the distributional shift from the original model at each sampling step, as quantified by the KL divergence. Our framework applies to any pre-trained flow-based generative scheme requiring no retraining or architectural modifications. We validate the approach across constrained image generation, physically-consistent trajectory sampling, and safe robotic manipulation policies, achieving 100% constraint satisfaction while preserving semantic fidelity.

</details>


### [30] [Causal Decoding for Hallucination-Resistant Multimodal Large Language Models](https://arxiv.org/abs/2602.21441)
*Shiwei Tan,Hengyi Wang,Weiyi Qin,Qi Xu,Zhigang Hua,Hao Wang*

Main category: cs.LG

TL;DR: 提出因果解码框架，通过针对性因果干预减少多模态大语言模型中的物体幻觉问题


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉语言任务中容易产生物体幻觉（引入图像中不存在的物体），这在实际应用中降低了可靠性。现有方法通常依赖启发式惩罚、事后修正或通用解码调整，这些方法没有直接干预触发物体幻觉的机制，因此效果有限。

Method: 提出因果解码框架，在生成过程中应用针对性因果干预来抑制虚假物体提及。通过重塑解码动态以减弱虚假依赖关系，该方法减少虚假物体标记同时保持描述质量。

Result: 在图像描述和问答基准测试中，该框架显著降低了物体幻觉率，并在不降低整体输出质量的情况下实现了最先进的忠实度。

Conclusion: 通过因果干预直接针对物体幻觉的生成机制，能够有效提高多模态大语言模型的可靠性，为减少幻觉问题提供了新的解决方案。

Abstract: Multimodal Large Language Models (MLLMs) deliver detailed responses on vision-language tasks, yet remain susceptible to object hallucination (introducing objects not present in the image), undermining reliability in practice. Prior efforts often rely on heuristic penalties, post-hoc correction, or generic decoding tweaks, which do not directly intervene in the mechanisms that trigger object hallucination and thus yield limited gains. To address this challenge, we propose a causal decoding framework that applies targeted causal interventions during generation to curb spurious object mentions. By reshaping the decoding dynamics to attenuate spurious dependencies, our approach reduces false object tokens while maintaining descriptive quality. Across captioning and QA benchmarks, our framework substantially lowers object-hallucination rates and achieves state-of-the-art faithfulness without degrading overall output quality.

</details>


### [31] [MINAR: Mechanistic Interpretability for Neural Algorithmic Reasoning](https://arxiv.org/abs/2602.21442)
*Jesse He,Helen Jenne,Max Vargas,Davis Brown,Gal Mishne,Yusu Wang,Henry Kvinge*

Main category: cs.LG

TL;DR: MINAR是一个将机制可解释性方法适配到图神经网络的高效电路发现工具箱，用于研究神经算法推理中的电路形成机制。


<details>
  <summary>Details</summary>
Motivation: 神经算法推理领域研究GNN模拟经典算法的能力，而大型语言模型的机制可解释性研究能够识别执行特定计算的电路组件。将这两种研究结合，可以深入理解GNN在算法任务中的内部工作机制。

Method: 开发MINAR工具箱，将机制可解释性中的归因修补方法适配到GNN设置中，用于发现神经元级电路。

Result: 通过两个案例研究，MINAR成功从算法任务训练的GNN中恢复了忠实的神经元级电路，揭示了训练过程中电路形成和剪枝的过程，以及GNN如何为相关任务复用电路组件。

Conclusion: MINAR为神经算法推理提供了有效的机制可解释性工具，增进了对GNN内部计算机制的理解，特别是在多任务学习和电路复用方面的洞察。

Abstract: The recent field of neural algorithmic reasoning (NAR) studies the ability of graph neural networks (GNNs) to emulate classical algorithms like Bellman-Ford, a phenomenon known as algorithmic alignment. At the same time, recent advances in large language models (LLMs) have spawned the study of mechanistic interpretability, which aims to identify granular model components like circuits that perform specific computations. In this work, we introduce Mechanistic Interpretability for Neural Algorithmic Reasoning (MINAR), an efficient circuit discovery toolbox that adapts attribution patching methods from mechanistic interpretability to the GNN setting. We show through two case studies that MINAR recovers faithful neuron-level circuits from GNNs trained on algorithmic tasks. Our study sheds new light on the process of circuit formation and pruning during training, as well as giving new insight into how GNNs trained to perform multiple tasks in parallel reuse circuit components for related tasks. Our code is available at https://github.com/pnnl/MINAR.

</details>


### [32] [When Learning Hurts: Fixed-Pole RNN for Real-Time Online Training](https://arxiv.org/abs/2602.21454)
*Alexander Morgan,Ummay Sumaya Khan,Lingjia Liu,Lizhong Zheng*

Main category: cs.LG

TL;DR: 固定极点的回声状态网络在数据受限的实时学习场景中优于学习极点的循环神经网络，因为极点学习导致高度非凸优化问题，需要更多训练数据和迭代，而固定极点架构能产生稳定且条件良好的状态表示。


<details>
  <summary>Details</summary>
Motivation: 研究为什么在数据受限的实时学习场景中，学习循环神经网络的极点（pole learning）不能带来实际好处。虽然理论上可以通过BPTT优化所有参数，但联合学习计算开销大，在有限训练数据下不实用。回声状态网络通过固定循环动态并仅训练线性读出层来克服这一限制。

Method: 通过分析和实证方法检验极点学习的问题。分析显示极点学习使权重优化问题高度非凸，需要更多训练样本和迭代。实证上观察梯度下降在复数数据上经常出现长时间平台期，高级优化器改进有限。对比固定极点架构，即使在有限训练数据下也能产生稳定且条件良好的状态表示。

Result: 数值结果表明，固定极点网络在性能上更优且训练复杂度更低。固定极点架构在数据受限的实时学习场景中表现更好，能产生稳定且条件良好的状态表示，而极点学习导致高度非凸优化问题，需要更多训练数据和迭代才能收敛。

Conclusion: 在数据受限的实时学习场景中，固定极点的回声状态网络比学习极点的循环神经网络更合适。极点学习导致高度非凸优化问题，需要大量训练数据和迭代，而固定极点架构能提供稳定且条件良好的状态表示，训练复杂度更低，更适合在线实时任务。

Abstract: Recurrent neural networks (RNNs) can be interpreted as discrete-time state-space models, where the state evolution corresponds to an infinite-impulse-response (IIR) filtering operation governed by both feedforward weights and recurrent poles. While, in principle, all parameters including pole locations can be optimized via backpropagation through time (BPTT), such joint learning incurs substantial computational overhead and is often impractical for applications with limited training data. Echo state networks (ESNs) mitigate this limitation by fixing the recurrent dynamics and training only a linear readout, enabling efficient and stable online adaptation. In this work, we analytically and empirically examine why learning recurrent poles does not provide tangible benefits in data-constrained, real-time learning scenarios. Our analysis shows that pole learning renders the weight optimization problem highly non-convex, requiring significantly more training samples and iterations for gradient-based methods to converge to meaningful solutions. Empirically, we observe that for complex-valued data, gradient descent frequently exhibits prolonged plateaus, and advanced optimizers offer limited improvement. In contrast, fixed-pole architectures induce stable and well-conditioned state representations even with limited training data. Numerical results demonstrate that fixed-pole networks achieve superior performance with lower training complexity, making them more suitable for online real-time tasks.

</details>


### [33] [Effects of Training Data Quality on Classifier Performance](https://arxiv.org/abs/2602.21462)
*Alan F. Karr,Regina Ruane*

Main category: cs.LG

TL;DR: 论文通过数值实验评估训练数据质量对分类器性能的影响，发现在训练数据质量下降时，四种分类器都表现出类似的崩溃行为，从基本正确变为偶然正确，且分类器之间的趋同性增加。


<details>
  <summary>Details</summary>
Motivation: 研究训练数据质量对分类器性能的影响，这是分类器分析中经常被忽视的组成部分。特别是在宏基因组组装短DNA序列的背景下，量化训练数据质量下降对分类器性能的具体影响。

Method: 使用广泛的数值实验，通过多种机制降解训练数据质量，评估四种分类器（贝叶斯分类器、神经网络、分区模型和随机森林）的性能变化。研究个体行为和分类器之间的趋同性。

Result: 发现所有四种分类器都表现出类似的崩溃行为：随着训练数据质量下降，分类器从基本正确变为偶然正确，因为它们以相同的方式出错。训练数据与分析数据差异越大，分类器决策退化越严重，边界变得稀疏，分类器之间的趋同性增加。

Conclusion: 训练数据质量对分类器性能有显著影响，所有测试的分类器在训练数据质量下降时都表现出类似的崩溃模式。这强调了在分类器分析中考虑训练数据质量的重要性，特别是在宏基因组组装等科学应用中。

Abstract: We describe extensive numerical experiments assessing and quantifying how classifier performance depends on the quality of the training data, a frequently neglected component of the analysis of classifiers.
  More specifically, in the scientific context of metagenomic assembly of short DNA reads into "contigs," we examine the effects of degrading the quality of the training data by multiple mechanisms, and for four classifiers -- Bayes classifiers, neural nets, partition models and random forests. We investigate both individual behavior and congruence among the classifiers. We find breakdown-like behavior that holds for all four classifiers, as degradation increases and they move from being mostly correct to only coincidentally correct, because they are wrong in the same way. In the process, a picture of spatial heterogeneity emerges: as the training data move farther from analysis data, classifier decisions degenerate, the boundary becomes less dense, and congruence increases.

</details>


### [34] [Geometric Priors for Generalizable World Models via Vector Symbolic Architecture](https://arxiv.org/abs/2602.21467)
*William Youngwoo Chung,Calvin Yeung,Hansen Jin Lillemark,Zhuowen Zou,Xiangjian Liu,Mohsen Imani*

Main category: cs.LG

TL;DR: 提出一种基于向量符号架构(VSA)和几何先验的可泛化世界模型，通过可学习的FHRR编码器将状态和动作映射到高维复向量空间，使用元素级复数乘法建模状态转移，在离散网格世界中实现零样本泛化和鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: 当前大多数世界模型使用非结构化的神经网络表示转移函数，这限制了可解释性、样本效率和泛化能力。需要开发能够理解世界底层动态、具有结构化和可泛化表示的世界模型。

Method: 采用向量符号架构(VSA)作为几何先验，使用可学习的FHRR编码器将状态和动作映射到高维复向量空间，学习群结构，并通过元素级复数乘法建模状态转移。该框架具有群论基础，训练结构化表示使其近似不变。

Result: 在离散网格世界环境中：1) 对未见过的状态-动作对实现87.5%的零样本准确率；2) 在20个时间步长的推演中获得53.6%的更高准确率；3) 相对于MLP基线，对噪声的鲁棒性提高4倍。

Conclusion: 训练具有潜在群结构的表示能够产生可泛化、数据高效且可解释的世界模型，为现实世界规划和推理的结构化模型提供了原则性途径。

Abstract: A key challenge in artificial intelligence and neuroscience is understanding how neural systems learn representations that capture the underlying dynamics of the world. Most world models represent the transition function with unstructured neural networks, limiting interpretability, sample efficiency, and generalization to unseen states or action compositions. We address these issues with a generalizable world model grounded in Vector Symbolic Architecture (VSA) principles as geometric priors. Our approach utilizes learnable Fourier Holographic Reduced Representation (FHRR) encoders to map states and actions into a high dimensional complex vector space with learned group structure and models transitions with element-wise complex multiplication. We formalize the framework's group theoretic foundation and show how training such structured representations to be approximately invariant enables strong multi-step composition directly in latent space and generalization performances over various experiments. On a discrete grid world environment, our model achieves 87.5% zero shot accuracy to unseen state-action pairs, obtains 53.6% higher accuracy on 20-timestep horizon rollouts, and demonstrates 4x higher robustness to noise relative to an MLP baseline. These results highlight how training to have latent group structure yields generalizable, data-efficient, and interpretable world models, providing a principled pathway toward structured models for real-world planning and reasoning.

</details>


### [35] [D-Flow SGLD: Source-Space Posterior Sampling for Scientific Inverse Problems with Flow Matching](https://arxiv.org/abs/2602.21469)
*Meet Hemant Parikh,Yaqin Chen,Jian-Xun Wang*

Main category: cs.LG

TL;DR: 提出D-Flow SGLD方法，用于流匹配先验下的科学逆问题后验采样，通过源空间后验采样结合预条件随机梯度Langevin动力学，无需重新训练先验即可适应新测量算子。


<details>
  <summary>Details</summary>
Motivation: 科学逆问题需要从稀疏噪声观测中重建高维物理状态，并生成保持先验知识和物理规律的具有不确定性的后验样本。虽然扩散模型的无训练条件生成已较成熟，但流匹配先验的相应条件生成和后验采样策略在科学基准测试中仍相对不足。

Method: 研究流匹配先验下科学逆问题的无训练条件生成，将现有推理时策略分为两类：1) 使用似然信息扰动采样轨迹的引导传输动力学；2) 保持学习传输固定而对源变量进行后验推断的源分布推断。基于后者提出D-Flow SGLD方法，将可微分源推断与预条件随机梯度Langevin动力学结合，实现源后验的可扩展探索。

Result: 在2D玩具后验、混沌Kuramoto-Sivashinsky轨迹和壁面湍流重建等层次化问题上对两类代表性方法进行基准测试，量化测量同化、后验多样性和物理/统计保真度之间的权衡，确立D-Flow SGLD作为科学逆问题的实用流匹配兼容后验采样器。

Conclusion: D-Flow SGLD是一种实用的流匹配兼容后验采样方法，能够在不重新训练先验或修改学习流匹配动力学的情况下，适应新测量算子并实现源后验的可扩展探索，为科学逆问题提供了有效的解决方案。

Abstract: Data assimilation and scientific inverse problems require reconstructing high-dimensional physical states from sparse and noisy observations, ideally with uncertainty-aware posterior samples that remain faithful to learned priors and governing physics. While training-free conditional generation is well developed for diffusion models, corresponding conditioning and posterior sampling strategies for Flow Matching (FM) priors remain comparatively under-explored, especially on scientific benchmarks where fidelity must be assessed beyond measurement misfit. In this work, we study training-free conditional generation for scientific inverse problems under FM priors and organize existing inference-time strategies by where measurement information is injected: (i) guided transport dynamics that perturb sampling trajectories using likelihood information, and (ii) source-distribution inference that performs posterior inference over the source variable while keeping the learned transport fixed. Building on the latter, we propose D-Flow SGLD, a source-space posterior sampling method that augments differentiable source inference with preconditioned stochastic gradient Langevin dynamics, enabling scalable exploration of the source posterior induced by new measurement operators without retraining the prior or modifying the learned FM dynamics. We benchmark representative methods from both families on a hierarchy of problems: 2D toy posteriors, chaotic Kuramoto-Sivashinsky trajectories, and wall-bounded turbulence reconstruction. Across these settings, we quantify trade-offs among measurement assimilation, posterior diversity, and physics/statistics fidelity, and establish D-Flow SGLD as a practical FM-compatible posterior sampler for scientific inverse problems.

</details>


### [36] [The Design Space of Tri-Modal Masked Diffusion Models](https://arxiv.org/abs/2602.21472)
*Louis Bethune,Victor Turrisi,Bruno Kacper Mlodozeniec,Pau Rodriguez Lopez,Lokesh Boominathan,Nikhil Bhendawade,Amitis Shidani,Joris Pelemans,Theo X. Olausson,Devon Hjelm,Paul Dixon,Joao Monteiro,Pierre Ablin,Vishnu Banna,Arno Blaas,Nick Henderson,Kari Noriy,Dan Busbridge,Josh Susskind,Marco Cuturi,Irina Belousova,Luca Zappella,Russ Webb,Jason Ramapuram*

Main category: cs.LG

TL;DR: 本文提出了首个从头开始预训练的三模态掩码扩散模型，涵盖文本、图像-文本和音频-文本数据，系统分析了多模态缩放规律，并引入基于SDE的重参数化方法消除批量大小调优需求。


<details>
  <summary>Details</summary>
Motivation: 现有离散扩散模型主要针对单模态或双模态生成，缺乏统一的三模态模型设计。需要系统研究多模态缩放规律、模态混合比例、噪声调度等关键因素，以开发更强大的统一多模态生成模型。

Method: 1. 从头预训练三模态掩码扩散模型，处理文本、图像-文本和音频-文本数据
2. 系统分析多模态缩放规律、模态混合比例、噪声调度和批量大小效应
3. 提出基于随机微分方程(SDE)的重参数化方法，将物理批量大小与逻辑批量大小解耦
4. 在6.4T tokens上预训练3B参数的三模态模型

Result: 1. 提供了优化的推理采样默认设置
2. 提出的SDE重参数化消除了批量大小调优需求
3. 3B参数模型在文本生成、文本到图像、文本到语音任务上表现强劲
4. 完成了迄今为止最大规模的多模态离散扩散模型系统研究

Conclusion: 本研究展示了统一三模态设计的可行性，为多模态离散扩散模型提供了重要的缩放行为洞察。提出的SDE重参数化方法简化了训练过程，为未来更大规模的多模态模型开发奠定了基础。

Abstract: Discrete diffusion models have emerged as strong alternatives to autoregressive language models, with recent work initializing and fine-tuning a base unimodal model for bimodal generation. Diverging from previous approaches, we introduce the first tri-modal masked diffusion model pretrained from scratch on text, image-text, and audio-text data. We systematically analyze multimodal scaling laws, modality mixing ratios, noise schedules, and batch-size effects, and we provide optimized inference sampling defaults. Our batch-size analysis yields a novel stochastic differential equation (SDE)-based reparameterization that eliminates the need for tuning the optimal batch size as reported in recent work. This reparameterization decouples the physical batch size, often chosen based on compute constraints (GPU saturation, FLOP efficiency, wall-clock time), from the logical batch size, chosen to balance gradient variance during stochastic optimization. Finally, we pretrain a preliminary 3B-parameter tri-modal model on 6.4T tokens, demonstrating the capabilities of a unified design and achieving strong results in text generation, text-to-image tasks, and text-to-speech tasks. Our work represents the largest-scale systematic open study of multimodal discrete diffusion models conducted to date, providing insights into scaling behaviors across multiple modalities.

</details>


### [37] [GradAlign: Gradient-Aligned Data Selection for LLM Reinforcement Learning](https://arxiv.org/abs/2602.21492)
*Ningyuan Yang,Weihua Du,Weiwei Sun,Sean Welleck,Yiming Yang*

Main category: cs.LG

TL;DR: GradAlign：一种用于大语言模型强化学习的梯度对齐数据选择方法，通过小规模可信验证集筛选训练问题，提升RL训练稳定性和最终性能


<details>
  <summary>Details</summary>
Motivation: 大语言模型的强化学习性能对训练问题质量高度敏感，传统方法依赖人工筛选或简单启发式过滤器，可能包含错误或低效用问题。RL的非平稳特性使得固定轨迹的监督微调方法不适用，需要更智能的数据选择策略。

Method: 提出GradAlign方法，使用小型可信验证集来优先选择那些策略梯度与验证梯度对齐的训练问题，形成自适应课程。该方法通过梯度对齐信号来指导非平稳策略优化。

Result: 在三种挑战性数据场景（不可靠奖励信号、分布不平衡、低效用训练语料）中，GradAlign均优于现有基线方法，展现出更稳定的训练过程和更好的最终性能。

Conclusion: 梯度方向信号在导航非平稳策略优化中至关重要，GradAlign通过梯度对齐的数据选择为LLM强化学习提供了有效的解决方案，代码已开源。

Abstract: Reinforcement learning (RL) has become a central post-training paradigm for large language models (LLMs), but its performance is highly sensitive to the quality of training problems. This sensitivity stems from the non-stationarity of RL: rollouts are generated by an evolving policy, and learning is shaped by exploration and reward feedback, unlike supervised fine-tuning (SFT) with fixed trajectories. As a result, prior work often relies on manual curation or simple heuristic filters (e.g., accuracy), which can admit incorrect or low-utility problems. We propose GradAlign, a gradient-aligned data selection method for LLM reinforcement learning that uses a small, trusted validation set to prioritize training problems whose policy gradients align with validation gradients, yielding an adaptive curriculum. We evaluate GradAlign across three challenging data regimes: unreliable reward signals, distribution imbalance, and low-utility training corpus, showing that GradAlign consistently outperforms existing baselines, underscoring the importance of directional gradient signals in navigating non-stationary policy optimization and yielding more stable training and improved final performance. We release our implementation at https://github.com/StigLidu/GradAlign

</details>


### [38] [Learning Recursive Multi-Scale Representations for Irregular Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.21498)
*Boyuan Li,Zhen Liu,Yicheng Luo,Qianli Ma*

Main category: cs.LG

TL;DR: ReIMTS是一种用于不规则多元时间序列预测的递归多尺度建模方法，通过保持原始时间戳不变并递归分割样本来捕捉全局到局部的依赖关系，避免了重采样对采样模式信息的破坏。


<details>
  <summary>Details</summary>
Motivation: 不规则多元时间序列（IMTS）中连续时间戳之间的不均匀间隔携带了有价值的采样模式信息，但现有方法通常使用重采样来获取粗粒度序列，这会改变原始时间戳并破坏采样模式信息。

Method: 提出ReIMTS方法：1）保持时间戳不变，递归地将每个样本分割为时间周期逐渐缩短的子样本；2）基于这些长到短子样本中的原始采样时间戳，提出不规则感知表示融合机制来捕捉全局到局部的依赖关系。

Result: 在多个模型和真实世界数据集上的广泛实验表明，在预测任务中平均性能提升了27.1%。

Conclusion: ReIMTS通过避免重采样并利用原始采样时间戳信息，有效捕捉了不规则多元时间序列中的多尺度依赖关系，显著提升了预测性能。

Abstract: Irregular Multivariate Time Series (IMTS) are characterized by uneven intervals between consecutive timestamps, which carry sampling pattern information valuable and informative for learning temporal and variable dependencies. In addition, IMTS often exhibit diverse dependencies across multiple time scales. However, many existing multi-scale IMTS methods use resampling to obtain the coarse series, which can alter the original timestamps and disrupt the sampling pattern information. To address the challenge, we propose ReIMTS, a Recursive multi-scale modeling approach for Irregular Multivariate Time Series forecasting. Instead of resampling, ReIMTS keeps timestamps unchanged and recursively splits each sample into subsamples with progressively shorter time periods. Based on the original sampling timestamps in these long-to-short subsamples, an irregularity-aware representation fusion mechanism is proposed to capture global-to-local dependencies for accurate forecasting. Extensive experiments demonstrate an average performance improvement of 27.1\% in the forecasting task across different models and real-world datasets. Our code is available at https://github.com/Ladbaby/PyOmniTS.

</details>


### [39] [WaterVIB: Learning Minimal Sufficient Watermark Representations via Variational Information Bottleneck](https://arxiv.org/abs/2602.21508)
*Haoyuan He,Yu Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.LG

TL;DR: 提出WaterVIB框架，通过变分信息瓶颈学习消息的最小充分统计量，过滤易受生成攻击的冗余封面细节，实现对抗再生式AIGC攻击的鲁棒水印


<details>
  <summary>Details</summary>
Motivation: 现有水印方法在对抗再生式AIGC攻击时存在严重脆弱性，因为它们将水印与高频封面纹理纠缠在一起，这些纹理在生成净化过程中容易被重写

Method: 提出WaterVIB框架，通过变分信息瓶颈将编码器重新构建为信息筛，强制模型学习消息的最小充分统计量，过滤掉易受生成变换的冗余封面细节，保留对再生不变的必需信号

Result: WaterVIB显著优于现有最先进方法，在未知扩散基编辑攻击下实现了卓越的零样本鲁棒性

Conclusion: 通过变分信息瓶颈优化是抵抗分布偏移攻击的必要条件，WaterVIB为知识产权保护提供了理论基础的鲁棒水印框架

Abstract: Robust watermarking is critical for intellectual property protection, whereas existing methods face a severe vulnerability against regeneration-based AIGC attacks. We identify that existing methods fail because they entangle the watermark with high-frequency cover texture, which is susceptible to being rewritten during generative purification. To address this, we propose WaterVIB, a theoretically grounded framework that reformulates the encoder as an information sieve via the Variational Information Bottleneck. Instead of overfitting to fragile cover details, our approach forces the model to learn a Minimal Sufficient Statistic of the message. This effectively filters out redundant cover nuances prone to generative shifts, retaining only the essential signal invariant to regeneration. We theoretically prove that optimizing this bottleneck is a necessary condition for robustness against distribution-shifting attacks. Extensive experiments demonstrate that WaterVIB significantly outperforms state-of-the-art methods, achieving superior zero-shot resilience against unknown diffusion-based editing.

</details>


### [40] [Training Generalizable Collaborative Agents via Strategic Risk Aversion](https://arxiv.org/abs/2602.21515)
*Chengrui Qu,Yizhou Zhang,Nicholas Lanzetti,Eric Mazumdar*

Main category: cs.LG

TL;DR: 提出战略风险厌恶作为多智能体协作的归纳偏置，通过MARL算法实现与未见伙伴的可靠协作


<details>
  <summary>Details</summary>
Motivation: 现有多智能体协作学习方法产生的策略在与新伙伴合作时容易失败，存在搭便车问题和缺乏战略鲁棒性

Method: 提出战略风险厌恶概念，将其作为归纳偏置集成到标准策略优化方法中，开发了相应的多智能体强化学习算法

Result: 在协作基准测试（包括LLM协作任务）中验证了理论，证明该方法能持续实现与异构和未见伙伴的可靠协作

Conclusion: 战略风险厌恶是促进多智能体协作泛化的有效归纳偏置，能解决搭便车问题并提高与未见伙伴合作的鲁棒性

Abstract: Many emerging agentic paradigms require agents to collaborate with one another (or people) to achieve shared goals. Unfortunately, existing approaches to learning policies for such collaborative problems produce brittle solutions that fail when paired with new partners. We attribute these failures to a combination of free-riding during training and a lack of strategic robustness. To address these problems, we study the concept of strategic risk aversion and interpret it as a principled inductive bias for generalizable cooperation with unseen partners. While strategically risk-averse players are robust to deviations in their partner's behavior by design, we show that, in collaborative games, they also (1) can have better equilibrium outcomes than those at classical game-theoretic concepts like Nash, and (2) exhibit less or no free-riding. Inspired by these insights, we develop a multi-agent reinforcement learning (MARL) algorithm that integrates strategic risk aversion into standard policy optimization methods. Our empirical results across collaborative benchmarks (including an LLM collaboration task) validate our theory and demonstrate that our approach consistently achieves reliable collaboration with heterogeneous and previously unseen partners across collaborative tasks.

</details>


### [41] [Muon+: Towards Better Muon via One Additional Normalization Step](https://arxiv.org/abs/2602.21545)
*Ruijie Zhang,Yequan Zhao,Ziyue Liu,Zhengyang Wang,Zheng Zhang*

Main category: cs.LG

TL;DR: Muon+是对Muon优化器的简单增强，在正交化后增加归一化步骤，在各种模型规模上都能持续提升训练和验证困惑度


<details>
  <summary>Details</summary>
Motivation: Muon优化器在大语言模型预训练中表现出色，但仍有改进空间。作者希望通过简单的增强进一步提升其性能

Method: 在Muon优化器的梯度（或动量）正交化后增加一个归一化步骤，形成Muon+优化器

Result: 在130M到774M参数的GPT风格模型和60M到1B参数的LLaMA风格模型上，Muon+相比Muon在训练和验证困惑度上都有持续提升，甚至在工业级T2P比≈200时也有效

Conclusion: Muon+是一个简单而有效的Muon优化器增强版本，在各种模型规模和架构上都能提供一致的性能提升

Abstract: The Muon optimizer has demonstrated promising performance in pre-training large language models through gradient (or momentum) orthogonalization. In this work, we propose a simple yet effective enhancement to Muon, namely Muon+, which introduces an additional normalization step after orthogonalization. We demonstrate the effectiveness of Muon+ through extensive pre-training experiments across a wide range of model scales and architectures. Our evaluation includes GPT-style models ranging from 130M to 774M parameters and LLaMA-style models ranging from 60M to 1B parameters. We comprehensively evaluate the effectiveness of Muon+ in the compute-optimal training regime and further extend the token-to-parameter (T2P) ratio to an industrial level of $\approx 200$. Experimental results show that Muon+ provides a consistent boost on training and validation perplexity over Muon. We provide our code here: https://github.com/K1seki221/MuonPlus.

</details>


### [42] [Mamba Meets Scheduling: Learning to Solve Flexible Job Shop Scheduling with Efficient Sequence Modeling](https://arxiv.org/abs/2602.21546)
*Zhi Cao,Cong Zhang,Yaoxin Wu,Yaqing Hou,Hongwei Ge*

Main category: cs.LG

TL;DR: 提出基于Mamba状态空间模型的FJSP求解架构，通过双Mamba块编码器和交叉注意力解码器，在保持线性计算复杂度的同时提升性能


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法通常依赖局部特征提取模型，难以捕捉跨操作和机器的全局依赖关系，且图注意力框架计算复杂度高

Method: 使用Mamba状态空间模型构建编码器-解码器架构：编码器包含双Mamba块分别提取操作和机器特征，解码器采用高效的交叉注意力机制学习操作与机器的交互嵌入

Result: 实验结果表明，该方法在多个基准测试中超越了现有最先进的基于学习方法，同时具有更快的求解速度

Conclusion: 基于Mamba的架构为FJSP提供了高效且性能优越的解决方案，能够更好地建模序列依赖关系，同时保持线性计算复杂度

Abstract: The Flexible Job Shop Problem (FJSP) is a well-studied combinatorial optimization problem with extensive applications for manufacturing and production scheduling. It involves assigning jobs to various machines to optimize criteria, such as minimizing total completion time. Current learning-based methods in this domain often rely on localized feature extraction models, limiting their capacity to capture overarching dependencies spanning operations and machines. This paper introduces an innovative architecture that harnesses Mamba, a state-space model with linear computational complexity, to facilitate comprehensive sequence modeling tailored for FJSP. In contrast to prevalent graph-attention-based frameworks that are computationally intensive for FJSP, we show our model is more efficient. Specifically, the proposed model possesses an encoder and a decoder. The encoder incorporates a dual Mamba block to extract operation and machine features separately. Additionally, we introduce an efficient cross-attention decoder to learn interactive embeddings of operations and machines. Our experimental results demonstrate that our method achieves faster solving speed and surpasses the performance of state-of-the-art learning-based methods for FJSP across various benchmarks.

</details>


### [43] [Extending Sequence Length is Not All You Need: Effective Integration of Multimodal Signals for Gene Expression Prediction](https://arxiv.org/abs/2602.21550)
*Zhao Yang,Yi Duan,Jiwei Zhu,Ying Ba,Chuan Cao,Bing Su*

Main category: cs.LG

TL;DR: Prism框架通过多模态表观基因组信号整合和背景染色质状态建模，仅用短序列实现基因表达预测的SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有基因表达预测模型过度关注长序列建模以定位远端增强子，但长序列反而会降低性能。研究发现近端多模态表观基因组信号更为关键，但简单拼接不同信号类型会导致模型与背景染色质模式建立虚假关联

Method: 提出Prism框架：1) 学习高维表观基因组特征的多重组合来表示不同的背景染色质状态；2) 使用后门调整来减轻混杂效应；3) 专注于短序列建模而非长序列

Result: 实验结果表明，通过适当建模多模态表观基因组信号，仅使用短序列就能在基因表达预测任务上达到最先进的性能

Conclusion: 基因表达预测的关键在于更好地整合近端多模态表观基因组信号，而非延长输入序列长度。Prism框架通过背景染色质状态建模和后门调整有效解决了信号整合中的混杂效应问题

Abstract: Gene expression prediction, which predicts mRNA expression levels from DNA sequences, presents significant challenges. Previous works often focus on extending input sequence length to locate distal enhancers, which may influence target genes from hundreds of kilobases away. Our work first reveals that for current models, long sequence modeling can decrease performance. Even carefully designed algorithms only mitigate the performance degradation caused by long sequences. Instead, we find that proximal multimodal epigenomic signals near target genes prove more essential. Hence we focus on how to better integrate these signals, which has been overlooked. We find that different signal types serve distinct biological roles, with some directly marking active regulatory elements while others reflect background chromatin patterns that may introduce confounding effects. Simple concatenation may lead models to develop spurious associations with these background patterns. To address this challenge, we propose Prism, a framework that learns multiple combinations of high-dimensional epigenomic features to represent distinct background chromatin states and uses backdoor adjustment to mitigate confounding effects. Our experimental results demonstrate that proper modeling of multimodal epigenomic signals achieves state-of-the-art performance using only short sequences for gene expression prediction.

</details>


### [44] [From Basis to Basis: Gaussian Particle Representation for Interpretable PDE Operators](https://arxiv.org/abs/2602.21551)
*Zhihao Li,Yu Feng,Zhilu Lai,Wei Wang*

Main category: cs.LG

TL;DR: 提出一种基于高斯基表示的流体PDE动力学学习方法，通过高斯粒子算子在模态空间操作，实现近线性复杂度、分辨率无关且可解释的建模


<details>
  <summary>Details</summary>
Motivation: 现有神经算子和Transformer方法缺乏可解释性，难以处理局部高频结构，且空间采样成本呈二次方增长

Method: 使用高斯基表示场，其中学习到的高斯原子携带显式几何信息；提出高斯粒子算子在模态空间操作，采用Petrov-Galerkin测量和PG高斯注意力实现全局跨尺度耦合

Result: 在标准PDE基准和真实数据集上达到最先进的竞争性精度，同时提供内在可解释性

Conclusion: 基于高斯基表示的方法实现了分辨率无关、近线性复杂度、支持不规则几何和2D到3D无缝扩展的流体动力学建模，兼具高性能和可解释性

Abstract: Learning PDE dynamics for fluids increasingly relies on neural operators and Transformer-based models, yet these approaches often lack interpretability and struggle with localized, high-frequency structures while incurring quadratic cost in spatial samples. We propose representing fields with a Gaussian basis, where learned atoms carry explicit geometry (centers, anisotropic scales, weights) and form a compact, mesh-agnostic, directly visualizable state. Building on this representation, we introduce a Gaussian Particle Operator that acts in modal space: learned Gaussian modal windows perform a Petrov-Galerkin measurement, and PG Gaussian Attention enables global cross-scale coupling. This basis-to-basis design is resolution-agnostic and achieves near-linear complexity in N for a fixed modal budget, supporting irregular geometries and seamless 2D-to-3D extension. On standard PDE benchmarks and real datasets, our method attains state-of-the-art competitive accuracy while providing intrinsic interpretability.

</details>


### [45] [Training-free Composition of Pre-trained GFlowNets for Multi-Objective Generation](https://arxiv.org/abs/2602.21565)
*Seokwon Yoon,Youngbin Choi,Seunghyuk Cho,Seungbeom Lee,MoonJeong Park,Dongwoo Kim*

Main category: cs.LG

TL;DR: 提出一种无需训练的混合策略，在推理时组合预训练的GFlowNets，实现多目标设置的快速适应，支持从线性标量化到复杂非线性逻辑算子的多种奖励组合。


<details>
  <summary>Details</summary>
Motivation: 现实应用通常涉及多个冲突目标，现有方法需要为每个目标集进行额外训练，限制了适用性并带来大量计算开销。

Method: 提出训练免费的混合策略，在推理时组合预训练的GFlowNets，无需微调或重新训练。该框架灵活，能处理从线性标量化到复杂非线性逻辑算子的多种奖励组合。

Result: 证明该方法能精确恢复线性标量化的目标分布，并通过失真因子量化非线性算子的近似质量。在合成2D网格和真实分子生成任务上的实验表明，性能与需要额外训练的基线相当。

Conclusion: 提出的训练免费混合策略为多目标GFlowNets提供了灵活高效的解决方案，显著减少了计算开销，同时保持了与需要重新训练方法相当的性能。

Abstract: Generative Flow Networks (GFlowNets) learn to sample diverse candidates in proportion to a reward function, making them well-suited for scientific discovery, where exploring multiple promising solutions is crucial. Further extending GFlowNets to multi-objective settings has attracted growing interest since real-world applications often involve multiple, conflicting objectives. However, existing approaches require additional training for each set of objectives, limiting their applicability and incurring substantial computational overhead. We propose a training-free mixing policy that composes pre-trained GFlowNets at inference time, enabling rapid adaptation without finetuning or retraining. Importantly, our framework is flexible, capable of handling diverse reward combinations ranging from linear scalarization to complex non-linear logical operators, which are often handled separately in previous literature. We prove that our method exactly recovers the target distribution for linear scalarization and quantify the approximation quality for nonlinear operators through a distortion factor. Experiments on a synthetic 2D grid and real-world molecule-generation tasks demonstrate that our approach achieves performance comparable to baselines that require additional training.

</details>


### [46] [Duel-Evolve: Reward-Free Test-Time Scaling via LLM Self-Preferences](https://arxiv.org/abs/2602.21585)
*Sweta Karlekar,Carolina Zheng,Magnus Saebo,Nicolas Beltran-Velez,Shuyang Yu,John Bowlan,Michal Kucer,David Blei*

Main category: cs.LG

TL;DR: Duel-Evolve：一种使用LLM自身生成成对偏好而非外部标量奖励的进化优化算法，用于在测试时改进LLM输出


<details>
  <summary>Details</summary>
Motivation: 许多应用需要在测试时通过迭代提出、评分和优化候选输出来优化LLM输出。现有方法使用校准的标量评估器，但对于许多任务，这种分数不可用、太稀疏或不可靠。相比之下，成对比较更容易获取，仍能提供改进方向的信号，并且可以从LLM自身获得而无需外部监督。

Method: Duel-Evolve是一种进化优化算法，用从同一LLM获取的成对偏好替代外部标量奖励。通过贝叶斯Bradley-Terry模型聚合这些噪声候选比较，得到质量的不确定性感知估计。使用Double Thompson Sampling指导比较预算分配，并选择高质量父代生成改进候选。

Result: 在MathBench上，比现有方法和基线准确率提高20个百分点；在LiveCodeBench上，比可比迭代方法提高超过12个百分点。该方法无需奖励模型、搜索时的真实标签或手工评分函数。

Conclusion: 成对自偏好为在大规模离散输出空间上进行测试时改进提供了强大的优化信号。该方法展示了仅使用LLM自身偏好进行优化的有效性。

Abstract: Many applications seek to optimize LLM outputs at test time by iteratively proposing, scoring, and refining candidates over a discrete output space. Existing methods use a calibrated scalar evaluator for the target objective to guide search, but for many tasks such scores are unavailable, too sparse, or unreliable. Pairwise comparisons, by contrast, are often easier to elicit, still provide useful signal on improvement directions, and can be obtained from the LLM itself without external supervision. Building on this observation, we introduce Duel-Evolve, an evolutionary optimization algorithm that replaces external scalar rewards with pairwise preferences elicited from the same LLM used to generate candidates. Duel-Evolve aggregates these noisy candidate comparisons via a Bayesian Bradley-Terry model, yielding uncertainty-aware estimates of candidate quality. These quality estimates guide allocation of the comparison budget toward plausible optima using Double Thompson Sampling, as well as selection of high-quality parents to generate improved candidates. We evaluate Duel-Evolve on MathBench, where it achieves 20 percentage points higher accuracy over existing methods and baselines, and on LiveCodeBench, where it improves over comparable iterative methods by over 12 percentage points. Notably, the method requires no reward model, no ground-truth labels during search, and no hand-crafted scoring function. Results show that pairwise self-preferences provide strong optimization signal for test-time improvement over large, discrete output spaces.

</details>


### [47] [ABM-UDE: Developing Surrogates for Epidemic Agent-Based Models via Scientific Machine Learning](https://arxiv.org/abs/2602.21588)
*Sharv Murgai,Utkarsh Utkarsh,Kyle C. Nguyen,Alan Edelman,Erin C. S. Acquesta,Christopher Vincent Rackauckas*

Main category: cs.LG

TL;DR: 开发基于通用微分方程的代理模型，将基于智能体的流行病模型转化为快速可解释的替代模型，用于实时医院规划


<details>
  <summary>Details</summary>
Motivation: 基于智能体的流行病模型能编码行为和政策异质性，但计算速度太慢，无法支持每晚的医院规划决策

Method: 使用通用微分方程构建替代模型：在SEIR家族ODE中加入神经网络参数化的接触率κ_φ(u,t)，采用多射击和基于观测器的预测误差方法稳定识别，强制正性和质量守恒

Result: PEM-UDE将平均MSE降低77%（相比单射击UDE），可靠性显著提升，覆盖率从0.68/0.43提高到0.86/0.61，推理时间仅需20-35秒（相比ABM的100CPU小时）

Conclusion: 该方法填补了真实性与计算速度之间的差距，支持阈值感知决策，保持机制可解释性，为其他科学领域提供从ABM到UDE的可移植路径

Abstract: Agent-based epidemic models (ABMs) encode behavioral and policy heterogeneity but are too slow for nightly hospital planning. We develop county-ready surrogates that learn directly from exascale ABM trajectories using Universal Differential Equations (UDEs): mechanistic SEIR-family ODEs with a neural-parameterized contact rate $κ_φ(u,t)$ (no additive residual). Our contributions are threefold: we adapt multiple shooting and an observer-based prediction-error method (PEM) to stabilize identification of neural-augmented epidemiological dynamics across intervention-driven regime shifts; we enforce positivity and mass conservation and show the learned contact-rate parameterization yields a well-posed vector field; and we quantify accuracy, calibration, and compute against ABM ensembles and UDE baselines. On a representative ExaEpi scenario, PEM-UDE reduces mean MSE by 77% relative to single-shooting UDE (3.00 vs. 13.14) and by 20% relative to MS-UDE (3.75). Reliability improves in parallel: empirical coverage of ABM $10$-$90$% and $25$-$75$% bands rises from 0.68/0.43 (UDE) and 0.79/0.55 (MS-UDE) to 0.86/0.61 with PEM-UDE and 0.94/0.69 with MS+PEM-UDE, indicating calibrated uncertainty rather than overconfident fits. Inference runs in seconds on commodity CPUs (20-35 s per $\sim$90-day forecast), enabling nightly ''what-if'' sweeps on a laptop. Relative to a $\sim$100 CPU-hour ABM reference run, this yields $\sim10^{4}\times$ lower wall-clock per scenario. This closes the realism-cadence gap, supports threshold-aware decision-making (e.g., maintaining ICU occupancy $<75$%), preserves mechanistic interpretability, and enables calibrated, risk-aware scenario planning on standard institutional hardware. Beyond epidemics, the ABM$\to$UDE recipe provides a portable path to distill agent-based simulators into fast, trustworthy surrogates for other scientific domains.

</details>


### [48] [Breaking Semantic-Aware Watermarks via LLM-Guided Coherence-Preserving Semantic Injection](https://arxiv.org/abs/2602.21593)
*Zheng Gao,Xiaoyu Li,Zhicheng Bao,Xiaoyan Feng,Jiaojiao Jiang*

Main category: cs.LG

TL;DR: 本文提出了一种针对内容感知语义水印的CSI攻击方法，利用LLM引导的语义操作在保持视觉一致性的同时破坏水印检测，揭示了当前语义水印设计的安全弱点。


<details>
  <summary>Details</summary>
Motivation: 生成图像在社交媒体和版权分发场景中广泛传播，语义水印被用于溯源和防伪。传统噪声层水印易受反转攻击，而内容感知语义水印虽然将水印信号与高级图像语义绑定，但LLM的结构化推理能力能够进行局部精细但全局一致的语义修改，从而破坏这种绑定关系。

Method: 提出Coherence-Preserving Semantic Injection (CSI)攻击方法，利用LLM引导的语义操作，在嵌入空间相似性约束下进行语义操纵。这种对齐机制在保持视觉语义一致性的同时，选择性地扰动与水印相关的语义，最终导致检测器误分类。

Result: 广泛的实证结果表明，CSI攻击在对抗内容感知语义水印方面持续优于现有攻击基线，揭示了当前语义水印设计在面对LLM驱动的语义扰动时的根本安全弱点。

Conclusion: LLM的结构化推理能力能够进行局部精细但全局一致的语义修改，从而破坏内容感知语义水印的绑定机制，暴露了当前语义水印设计的安全漏洞，需要更鲁棒的水印方案来应对LLM驱动的攻击。

Abstract: Generative images have proliferated on Web platforms in social media and online copyright distribution scenarios, and semantic watermarking has increasingly been integrated into diffusion models to support reliable provenance tracking and forgery prevention for web content. Traditional noise-layer-based watermarking, however, remains vulnerable to inversion attacks that can recover embedded signals. To mitigate this, recent content-aware semantic watermarking schemes bind watermark signals to high-level image semantics, constraining local edits that would otherwise disrupt global coherence. Yet, large language models (LLMs) possess structured reasoning capabilities that enable targeted exploration of semantic spaces, allowing locally fine-grained but globally coherent semantic alterations that invalidate such bindings. To expose this overlooked vulnerability, we introduce a Coherence-Preserving Semantic Injection (CSI) attack that leverages LLM-guided semantic manipulation under embedding-space similarity constraints. This alignment enforces visual-semantic consistency while selectively perturbing watermark-relevant semantics, ultimately inducing detector misclassification. Extensive empirical results show that CSI consistently outperforms prevailing attack baselines against content-aware semantic watermarking, revealing a fundamental security weakness of current semantic watermark designs when confronted with LLM-driven semantic perturbations.

</details>


### [49] [NGDB-Zoo: Towards Efficient and Scalable Neural Graph Databases Training](https://arxiv.org/abs/2602.21597)
*Zhongwei Xie,Jiaxin Bai,Shujie Liu,Haoyu Huang,Yufei Li,Yisen Gao,Hong Ting Tsang,Yangqiu Song*

Main category: cs.LG

TL;DR: NGDB-Zoo是一个统一的神经图数据库框架，通过操作符级训练和语义增强解决现有NGDB的训练效率和表达能力瓶颈，实现1.8-6.8倍的吞吐量提升，并在大规模图上保持高GPU利用率。


<details>
  <summary>Details</summary>
Motivation: 现有神经图数据库（NGDBs）在训练效率和表达能力上存在瓶颈：查询级批处理方式僵化，嵌入表示仅限于结构信息，无法有效处理不完整的知识结构进行复杂逻辑推理。

Method: 提出NGDB-Zoo框架，采用操作符级训练与语义增强的协同方法：1) 将逻辑操作符与查询拓扑解耦，实现动态调度的数据流执行；2) 设计解耦架构集成预训练文本编码器的高维语义先验，避免I/O阻塞和内存溢出。

Result: 在六个基准测试（包括ogbl-wikikg2和ATLAS-Wiki等大规模图）上评估，NGDB-Zoo相比基线实现1.8-6.8倍的吞吐量提升，在多样化逻辑模式上保持高GPU利用率，显著缓解混合神经符号推理中的表示摩擦。

Conclusion: NGDB-Zoo通过操作符级训练和语义增强有效解决了神经图数据库的训练效率和表达能力瓶颈，为大规模不完整知识结构的复杂逻辑推理提供了高效统一的解决方案。

Abstract: Neural Graph Databases (NGDBs) facilitate complex logical reasoning over incomplete knowledge structures, yet their training efficiency and expressivity are constrained by rigid query-level batching and structure-exclusive embeddings. We present NGDB-Zoo, a unified framework that resolves these bottlenecks by synergizing operator-level training with semantic augmentation. By decoupling logical operators from query topologies, NGDB-Zoo transforms the training loop into a dynamically scheduled data-flow execution, enabling multi-stream parallelism and achieving a $1.8\times$ - $6.8\times$ throughput compared to baselines. Furthermore, we formalize a decoupled architecture to integrate high-dimensional semantic priors from Pre-trained Text Encoders (PTEs) without triggering I/O stalls or memory overflows. Extensive evaluations on six benchmarks, including massive graphs like ogbl-wikikg2 and ATLAS-Wiki, demonstrate that NGDB-Zoo maintains high GPU utilization across diverse logical patterns and significantly mitigates representation friction in hybrid neuro-symbolic reasoning.

</details>


### [50] [Deep Clustering based Boundary-Decoder Net for Inter and Intra Layer Stress Prediction of Heterogeneous Integrated IC Chip](https://arxiv.org/abs/2602.21601)
*Kart Leong Lim,Ji Lin*

Main category: cs.LG

TL;DR: 提出结合边界解码器网络与深度聚类的方法，用于3D异质IC封装热循环应力图像的建模与预测，在模拟数据集上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 3D异质IC封装在极端温度热循环下会产生高应力，应力主要出现在不同材料界面。传统深度生成模型需要图像配对训练，而边界解码器网络利用边界条件和图像配对进行应力建模，但存在维度不适定问题。

Method: 结合边界解码器网络与深度聚类方法。边界网络将材料参数映射到与图像共享的潜在空间，通过深度聚类解决维度不适定问题。在包含1825个应力图像的IC芯片模拟数据集上进行评估。

Result: 提出的方法在训练和测试误差减少方面优于边界解码器网络变体和基线方法，显示出更好的性能。

Conclusion: 结合边界解码器网络与深度聚类的方法能有效解决3D异质IC封装应力图像建模问题，为热循环应力分析提供了更优的解决方案。

Abstract: High stress occurs when 3D heterogeneous IC packages are subjected to thermal cycling at extreme temperatures. Stress mainly occurs at the interface between different materials. We investigate stress image using latent space representation which is based on using deep generative model (DGM). However, most DGM approaches are unsupervised, meaning they resort to image pairing (input and output) to train DGM. Instead, we rely on a recent boundary-decoder (BD) net, which uses boundary condition and image pairing for stress modeling. The boundary net maps material parameters to the latent space co-shared by its image counterpart. Because such a setup is dimensionally wise ill-posed, we further couple BD net with deep clustering. To access the performance of our proposed method, we simulate an IC chip dataset comprising of 1825 stress images. We compare our new approach using variants of BD net as well as a baseline approach. We show that our approach is able to outperform all the comparison in terms of train and test error reduction.

</details>


### [51] [AgentLTV: An Agent-Based Unified Search-and-Evolution Framework for Automated Lifetime Value Prediction](https://arxiv.org/abs/2602.21634)
*Chaowei Wu,Huazhu Chen,Congde Yuan,Qirui Yang,Guoqing Song,Yue Gao,Li Luo,Frank Youhua Chen,Mengzhuo Guo*

Main category: cs.LG

TL;DR: AgentLTV：基于智能体的自动化LTV建模框架，通过LLM驱动的智能体生成代码、运行修复管道，结合MCTS和进化算法两阶段搜索，实现跨场景的LTV预测优化。


<details>
  <summary>Details</summary>
Motivation: LTV预测在不同决策场景中数据模式差异大，实践中需要构建复杂的场景特定管道，涉及特征处理、目标设计和调优，这个过程成本高且难以迁移。

Method: 提出AgentLTV框架：将候选解决方案视为可执行管道程序，LLM驱动的智能体生成代码、运行修复管道并分析执行反馈。采用两阶段搜索：1）MCTS阶段在固定预算下探索广泛的建模选择空间；2）进化算法阶段通过基于岛屿的进化（交叉、变异、迁移）优化最佳MCTS程序。

Result: 在大型专有数据集和公共基准测试中，AgentLTV在排名和误差指标上一致发现强模型。在线桶级分析显示排名一致性和价值校准得到改善，特别是对高价值和负LTV细分市场。

Conclusion: AgentLTV已成功在线部署。实践建议：使用MCTS快速适应新数据模式，使用EA进行稳定优化，并通过桶级排名和校准诊断验证部署准备度。

Abstract: Lifetime Value (LTV) prediction is critical in advertising, recommender systems, and e-commerce. In practice, LTV data patterns vary across decision scenarios. As a result, practitioners often build complex, scenario-specific pipelines and iterate over feature processing, objective design, and tuning. This process is expensive and hard to transfer. We propose AgentLTV, an agent-based unified search-and-evolution framework for automated LTV modeling. AgentLTV treats each candidate solution as an {executable pipeline program}. LLM-driven agents generate code, run and repair pipelines, and analyze execution feedback. Two decision agents coordinate a two-stage search. The Monte Carlo Tree Search (MCTS) stage explores a broad space of modeling choices under a fixed budget, guided by the Polynomial Upper Confidence bounds for Trees criterion and a Pareto-aware multi-metric value function. The Evolutionary Algorithm (EA) stage refines the best MCTS program via island-based evolution with crossover, mutation, and migration. Experiments on a large-scale proprietary dataset and a public benchmark show that AgentLTV consistently discovers strong models across ranking and error metrics. Online bucket-level analysis further indicates improved ranking consistency and value calibration, especially for high-value and negative-LTV segments. We summarize practitioner-oriented takeaways: use MCTS for rapid adaptation to new data patterns, use EA for stable refinement, and validate deployment readiness with bucket-level ranking and calibration diagnostics. The proposed AgentLTV has been successfully deployed online.

</details>


### [52] [Multimodal Survival Modeling and Fairness-Aware Clinical Machine Learning for 5-Year Breast Cancer Risk Prediction](https://arxiv.org/abs/2602.21648)
*Toktam Khatibi*

Main category: cs.LG

TL;DR: 提出一个用于乳腺癌5年总生存期预测的多模态机器学习框架，整合临床变量、转录组和拷贝数变异特征，比较CoxNet和XGBoost两种生存分析方法，强调校准、公平性审计和可重复性。


<details>
  <summary>Details</summary>
Motivation: 临床风险预测模型在现实环境中常因校准不佳、可迁移性有限和亚组差异而表现不佳，这些问题在高维多模态癌症数据集中尤为突出，需要开发更稳健的预测框架。

Method: 使用METABRIC队列数据，整合临床变量与高维转录组和拷贝数变异特征，经过方差和稀疏性筛选及降维处理，采用分层训练/验证/测试分割和验证集超参数调优，比较弹性网络正则化Cox模型(CoxNet)和基于XGBoost的梯度提升生存树模型。

Result: CoxNet在验证集和测试集上分别获得98.3和96.6的AUC值，AP值分别为90.1和80.4；XGBoost获得98.6和92.5的AUC值，AP值分别为92.5和79.9。公平性诊断显示在不同年龄组、雌激素受体状态、分子亚型和绝经状态下具有稳定的判别能力。

Conclusion: 该研究提出了一个注重治理的多模态生存分析框架，强调校准、公平性审计、稳健性和可重复性，为高维临床机器学习提供了系统化的解决方案。

Abstract: Clinical risk prediction models often underperform in real-world settings due to poor calibration, limited transportability, and subgroup disparities. These challenges are amplified in high-dimensional multimodal cancer datasets characterized by complex feature interactions and a p >> n structure. We present a fully reproducible multimodal machine learning framework for 5-year overall survival prediction in breast cancer, integrating clinical variables with high-dimensional transcriptomic and copy-number alteration (CNA) features from the METABRIC cohort.
  After variance- and sparsity-based filtering and dimensionality reduction, models were trained using stratified train/validation/test splits with validation-based hyperparameter tuning. Two survival approaches were compared: an elastic-net regularized Cox model (CoxNet) and a gradient-boosted survival tree model implemented using XGBoost. CoxNet provides embedded feature selection and stable estimation, whereas XGBoost captures nonlinear effects and higher-order interactions.
  Performance was assessed using time-dependent area under the ROC curve (AUC), average precision (AP), calibration curves, Brier score, and bootstrapped 95 percent confidence intervals. CoxNet achieved validation and test AUCs of 98.3 and 96.6, with AP values of 90.1 and 80.4. XGBoost achieved validation and test AUCs of 98.6 and 92.5, with AP values of 92.5 and 79.9. Fairness diagnostics showed stable discrimination across age groups, estrogen receptor status, molecular subtypes, and menopausal state.
  This work introduces a governance-oriented multimodal survival framework emphasizing calibration, fairness auditing, robustness, and reproducibility for high-dimensional clinical machine learning.

</details>


### [53] [Error-awareness Accelerates Active Automata Learning](https://arxiv.org/abs/2602.21674)
*Loes Kruger,Sebastian Junges,Jurriaan Rot*

Main category: cs.LG

TL;DR: 本文研究了如何利用领域知识（特别是关于哪些输入会产生错误的知识）来加速主动自动机学习算法，使其能够扩展到具有大量输入的系统。


<details>
  <summary>Details</summary>
Motivation: 现代主动自动机学习算法在面对大量输入的系统时难以扩展，即使大多数输入会导致错误。在许多实际问题中，这些错误是可观察的（产生已知的错误输出），这为利用领域知识加速学习提供了机会。

Method: 针对不同级别的领域知识（关于哪些输入在哪些状态下不会产生错误），对最先进的L#算法进行匹配性调整。根据对非错误产生输入的知识程度，设计相应的算法改进。

Result: 实证评估表明，这些方法能够显著加速学习过程：在具有强大但现实的领域知识时，加速效果可达数量级；即使在有限的领域知识下，也能实现一个数量级的加速。

Conclusion: 通过利用关于错误行为的领域知识，可以显著提高主动自动机学习算法的可扩展性，特别是在处理具有大量输入且多数输入会导致错误的系统时。

Abstract: Active automata learning (AAL) algorithms can learn a behavioral model of a system from interacting with it. The primary challenge remains scaling to larger models, in particular in the presence of many possible inputs to the system. Modern AAL algorithms fail to scale even if, in every state, most inputs lead to errors. In various challenging problems from the literature, these errors are observable, i.e., they emit a known error output. Motivated by these problems, we study learning these systems more efficiently. Further, we consider various degrees of knowledge about which inputs are non-error producing at which state. For each level of knowledge, we provide a matching adaptation of the state-of-the-art AAL algorithm L# to make the most of this domain knowledge. Our empirical evaluation demonstrates that the methods accelerate learning by orders of magnitude with strong but realistic domain knowledge to a single order of magnitude with limited domain knowledge.

</details>


### [54] [Hierarchical Lead Critic based Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.21680)
*David Eckel,Henri Meeß*

Main category: cs.LG

TL;DR: 提出分层领导评论家（HLC）架构，通过多层级视角结合局部与全局信息，提升多智能体强化学习的性能与样本效率。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体强化学习要么局限于局部视角（独立学习），要么局限于全局视角（集中式学习），缺乏结合多层级视角的方法来更好地协调复杂任务。

Method: 提出分层领导评论家（HLC）架构，采用顺序训练方案，从不同层级学习多视角信息，结合高层目标导向与低层执行，灵感来源于自然团队结构中的层级分布。

Result: 在合作、非通信和部分可观察的多智能体强化学习基准测试中，HLC优于单层级基线方法，能鲁棒地扩展到更多智能体和更困难的任务。

Conclusion: 引入多层级视角结合局部与全局信息能显著提升多智能体强化学习的性能、样本效率和策略鲁棒性，分层架构是解决复杂协调任务的有效方法。

Abstract: Cooperative Multi-Agent Reinforcement Learning (MARL) solves complex tasks that require coordination from multiple agents, but is often limited to either local (independent learning) or global (centralized learning) perspectives. In this paper, we introduce a novel sequential training scheme and MARL architecture, which learns from multiple perspectives on different hierarchy levels. We propose the Hierarchical Lead Critic (HLC) - inspired by natural emerging distributions in team structures, where following high-level objectives combines with low-level execution. HLC demonstrates that introducing multiple hierarchies, leveraging local and global perspectives, can lead to improved performance with high sample efficiency and robust policies. Experimental results conducted on cooperative, non-communicative, and partially observable MARL benchmarks demonstrate that HLC outperforms single hierarchy baselines and scales robustly with increasing amounts of agents and difficulty.

</details>


### [55] [TiMi: Empower Time Series Transformers with Multimodal Mixture of Experts](https://arxiv.org/abs/2602.21693)
*Jiafeng Lin,Yuxuan Wang,Huakun Luo,Zhongyi Pei,Jianmin Wang*

Main category: cs.LG

TL;DR: TiMi：基于多模态专家混合的时间序列变换器，利用LLM生成未来发展的推理指导时间序列预测，无需显式模态对齐


<details>
  <summary>Details</summary>
Motivation: 现有多模态时间序列预测方法在模态对齐方面存在挑战，难以有效整合文本信息（如紧急报告、政策公告）对时间序列波动的因果影响

Method: 1) 利用LLM生成对未来发展的推理作为预测指导；2) 引入多模态专家混合模块作为轻量级插件，赋能Transformer时间序列模型进行多模态预测，无需显式表示级对齐

Result: 在16个真实世界多模态预测基准上取得一致的SOTA性能，优于先进基线，同时具备强适应性和可解释性

Conclusion: TiMi通过LLM的因果推理能力和多模态专家混合模块，有效解决了多模态时间序列预测中的模态对齐挑战，实现了更准确的预测

Abstract: Multimodal time series forecasting has garnered significant attention for its potential to provide more accurate predictions than traditional single-modality models by leveraging rich information inherent in other modalities. However, due to fundamental challenges in modality alignment, existing methods often struggle to effectively incorporate multimodal data into predictions, particularly textual information that has a causal influence on time series fluctuations, such as emergency reports and policy announcements. In this paper, we reflect on the role of textual information in numerical forecasting and propose Time series transformers with Multimodal Mixture-of-Experts, TiMi, to unleash the causal reasoning capabilities of LLMs. Concretely, TiMi utilizes LLMs to generate inferences on future developments, which serve as guidance for time series forecasting. To seamlessly integrate both exogenous factors and time series into predictions, we introduce a Multimodal Mixture-of-Experts (MMoE) module as a lightweight plug-in to empower Transformer-based time series models for multimodal forecasting, eliminating the need for explicit representation-level alignment. Experimentally, our proposed TiMi demonstrates consistent state-of-the-art performance on sixteen real-world multimodal forecasting benchmarks, outperforming advanced baselines while offering both strong adaptability and interpretability.

</details>


### [56] [Learning Complex Physical Regimes via Coverage-oriented Uncertainty Quantification: An application to the Critical Heat Flux](https://arxiv.org/abs/2602.21701)
*Michele Cazzola,Alberto Ghione,Lucia Sargentini,Julien Nespoulous,Riccardo Finotello*

Main category: cs.LG

TL;DR: 该论文比较了不确定性量化方法在科学机器学习中的应用，特别针对临界热通量（CHF）数据集，展示了覆盖导向学习方法比后处理方法更能适应多物理机制的数据行为。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习面临多机制物理系统的正确表示挑战，标准数据分析方法难以捕捉系统响应在状态空间中的显著变化。不确定性量化不应仅被视为安全评估，而应作为学习任务的支持，引导模型内化数据行为。

Method: 对不确定性量化方法进行对比分析：将后处理方法（特别是保形预测）与端到端覆盖导向流程（包括贝叶斯异方差回归和质量驱动损失）进行对比。这些方法将不确定性视为优化过程的主动组成部分，同时建模预测及其行为。

Result: 后处理方法确保统计校准，而覆盖导向学习能有效重塑模型表示以匹配复杂的物理机制。最终模型不仅提供高预测准确性，还能提供物理一致的不确定性估计，动态适应CHF的内在变异性。

Conclusion: 在科学机器学习中，覆盖导向学习方法比传统后处理方法更能有效处理多机制物理系统，通过将不确定性作为优化过程的主动组成部分，实现物理一致的数据行为内化和可靠预测。

Abstract: A central challenge in scientific machine learning (ML) is the correct representation of physical systems governed by multi-regime behaviours. In these scenarios, standard data analysis techniques often fail to capture the nature of the data, as the system's response varies significantly across the state space due to its stochasticity and the different physical regimes. Uncertainty quantification (UQ) should thus not be viewed merely as a safety assessment, but as a support to the learning task itself, guiding the model to internalise the behaviour of the data. We address this by focusing on the Critical Heat Flux (CHF) benchmark and dataset presented by the OECD/NEA Expert Group on Reactor Systems Multi-Physics. This case study represents a test for scientific ML due to the non-linear dependence of CHF on the inputs and the existence of distinct microscopic physical regimes. These regimes exhibit diverse statistical profiles, a complexity that requires UQ techniques to internalise the data behaviour and ensure reliable predictions. In this work, we conduct a comparative analysis of UQ methodologies to determine their impact on physical representation. We contrast post-hoc methods, specifically conformal prediction, against end-to-end coverage-oriented pipelines, including (Bayesian) heteroscedastic regression and quality-driven losses. These approaches treat uncertainty not as a final metric, but as an active component of the optimisation process, modelling the prediction and its behaviour simultaneously. We show that while post-hoc methods ensure statistical calibration, coverage-oriented learning effectively reshapes the model's representation to match the complex physical regimes. The result is a model that delivers not only high predictive accuracy but also a physically consistent uncertainty estimation that adapts dynamically to the intrinsic variability of the CHF.

</details>


### [57] [C$^{2}$TC: A Training-Free Framework for Efficient Tabular Data Condensation](https://arxiv.org/abs/2602.21717)
*Sijia Xu,Fan Li,Xiaoyang Wang,Zhengyi Yang,Xuemin Lin*

Main category: cs.LG

TL;DR: C²TC：首个免训练的表格数据集压缩框架，通过类自适应聚类分配和混合分类特征编码，实现高效可扩展的表格数据压缩，相比现有方法效率提升至少2个数量级。


<details>
  <summary>Details</summary>
Motivation: 表格数据是工业关系数据库的主要格式，但大规模表格数据给学习型分析系统带来计算和存储挑战。现有数据集压缩方法计算密集且忽视表格数据的异构特征和类别不平衡特性。

Method: 提出C²TC框架：1) 将数据集压缩目标重新表述为类自适应聚类分配问题(CCAP)，消除昂贵训练并整合自适应标签分配；2) 开发HFILS启发式局部搜索算法，交替进行软分配和类内聚类；3) 提出HCFE混合分类特征编码，保留异构离散属性的语义信息。

Result: 在10个真实世界数据集上的实验表明，C²TC相比最先进基线方法效率提升至少2个数量级，同时在下游任务中取得更优性能。

Conclusion: C²TC是首个免训练的表格数据集压缩框架，通过联合优化类别分配和特征表示，有效解决了表格数据压缩中的计算效率和类别不平衡问题，为大规模表格数据分析提供了高效解决方案。

Abstract: Tabular data is the primary data format in industrial relational databases, underpinning modern data analytics and decision-making. However, the increasing scale of tabular data poses significant computational and storage challenges to learning-based analytical systems. This highlights the need for data-efficient learning, which enables effective model training and generalization using substantially fewer samples. Dataset condensation (DC) has emerged as a promising data-centric paradigm that synthesizes small yet informative datasets to preserve data utility while reducing storage and training costs. However, existing DC methods are computationally intensive due to reliance on complex gradient-based optimization. Moreover, they often overlook key characteristics of tabular data, such as heterogeneous features and class imbalance. To address these limitations, we introduce C$^{2}$TC (Class-Adaptive Clustering for Tabular Condensation), the first training-free tabular dataset condensation framework that jointly optimizes class allocation and feature representation, enabling efficient and scalable condensation. Specifically, we reformulate the dataset condensation objective into a novel class-adaptive cluster allocation problem (CCAP), which eliminates costly training and integrates adaptive label allocation to handle class imbalance. To solve the NP-hard CCAP, we develop HFILS, a heuristic local search that alternates between soft allocation and class-wise clustering to efficiently obtain high-quality solutions. Moreover, a hybrid categorical feature encoding (HCFE) is proposed for semantics-preserving clustering of heterogeneous discrete attributes. Extensive experiments on 10 real-world datasets demonstrate that C$^{2}$TC improves efficiency by at least 2 orders of magnitude over state-of-the-art baselines, while achieving superior downstream performance.

</details>


### [58] [From Words to Amino Acids: Does the Curse of Depth Persist?](https://arxiv.org/abs/2602.21750)
*Aleena Siji,Amir Mohammad Karimi Mamaghan,Ferdinand Kapl,Tobias Höppe,Emmanouil Angelis,Andrea Dittadi,Maurice Brenner,Michael Heinzinger,Karl Henrik Johansson,Kaitlin Maile,Johannes von Oswald,Stefan Bauer*

Main category: cs.LG

TL;DR: 研究发现蛋白质语言模型存在深度效率低下问题，后期层对最终输出的贡献有限，与大型语言模型中的"深度诅咒"现象相似。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型研究中发现的"深度诅咒"现象（后期层贡献有限）是否也存在于蛋白质语言模型中？蛋白质语言模型训练目标多样（自回归、掩码、扩散），且有些是多模态的，需要探究其深度效率问题。

Method: 对六个流行的蛋白质语言模型进行深度分析，涵盖不同模型家族和规模，使用统一的探测和扰动测量方法，量化各层贡献随深度的变化。

Result: 所有模型都显示出深度依赖的一致模式：后期层对早期计算的依赖减少，主要作用是细化最终输出分布，这些效应在更深模型中更加明显。

Conclusion: 蛋白质语言模型存在深度效率低下问题，这为未来开发更深度高效的架构和训练方法提供了动机。

Abstract: Protein language models (PLMs) have become widely adopted as general-purpose models, demonstrating strong performance in protein engineering and de novo design. Like large language models (LLMs), they are typically trained as deep transformers with next-token or masked-token prediction objectives on massive sequence corpora and are scaled by increasing model depth. Recent work on autoregressive LLMs has identified the Curse of Depth: later layers contribute little to the final output predictions. These findings naturally raise the question of whether a similar depth inefficiency also appears in PLMs, where many widely used models are not autoregressive, and some are multimodal, accepting both protein sequence and structure as input. In this work, we present a depth analysis of six popular PLMs across model families and scales, spanning three training objectives, namely autoregressive, masked, and diffusion, and quantify how layer contributions evolve with depth using a unified set of probing- and perturbation-based measurements. Across all models, we observe consistent depth-dependent patterns that extend prior findings on LLMs: later layers depend less on earlier computations and mainly refine the final output distribution, and these effects are increasingly pronounced in deeper models. Taken together, our results suggest that PLMs exhibit a form of depth inefficiency, motivating future work on more depth-efficient architectures and training methods.

</details>


### [59] [Learning from Yesterday's Error: An Efficient Online Learning Method for Traffic Demand Prediction](https://arxiv.org/abs/2602.21757)
*Xiannan Huang,Quan Yuan,Chao Yang*

Main category: cs.LG

TL;DR: FORESEE是一个轻量级在线适应框架，通过残差平滑和专家混合机制纠正预测误差，无需更新基础模型参数，实现交通需求预测在分布变化下的高效适应。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在平稳条件下表现良好，但在外部事件或城市动态变化导致的分布偏移时准确性显著下降。频繁的模型重训练计算成本过高，特别是对于大规模或基础模型。

Method: FORESEE框架不更新基础模型参数，而是使用昨天的预测误差来纠正今天的预测，通过指数平滑稳定，并由混合专家机制指导以适应最近的误差动态。还包括自适应时空平滑组件，在相邻区域和时间段传播误差信号。

Result: 在七个真实世界数据集和三个骨干模型上的实验表明，FORESEE持续提高预测准确性，即使在分布偏移最小的情况下也能保持鲁棒性（避免性能下降），并且在现有在线方法中计算开销最低。

Conclusion: FORESEE通过可忽略的计算成本实现交通预测模型的实时适应，为在动态城市环境中部署可靠、最新的预测系统铺平了道路。

Abstract: Accurately predicting short-term traffic demand is critical for intelligent transportation systems. While deep learning models achieve strong performance under stationary conditions, their accuracy often degrades significantly when faced with distribution shifts caused by external events or evolving urban dynamics. Frequent model retraining to adapt to such changes incurs prohibitive computational costs, especially for large-scale or foundation models. To address this challenge, we propose FORESEE (Forecasting Online with Residual Smoothing and Ensemble Experts), a lightweight online adaptation framework that is accurate, robust, and computationally efficient. FORESEE operates without any parameter updates to the base model. Instead, it corrects today's forecast in each region using yesterday's prediction error, stabilized through exponential smoothing guided by a mixture-of-experts mechanism that adapts to recent error dynamics. Moreover, an adaptive spatiotemporal smoothing component propagates error signals across neighboring regions and time slots, capturing coherent shifts in demand patterns. Extensive experiments on seven real-world datasets with three backbone models demonstrate that FORESEE consistently improves prediction accuracy, maintains robustness even when distribution shifts are minimal (avoiding performance degradation), and achieves the lowest computational overhead among existing online methods. By enabling real-time adaptation of traffic forecasting models with negligible computational cost, FORESEE paves the way for deploying reliable, up-to-date prediction systems in dynamic urban environments. Code and data are available at https://github.com/xiannanhuang/FORESEE

</details>


### [60] [Generalisation of RLHF under Reward Shift and Clipped KL Regularisation](https://arxiv.org/abs/2602.21765)
*Kenton Tang,Yuzhu Chen,Fengxiang He*

Main category: cs.LG

TL;DR: 本文为RLHF开发了泛化理论，考虑了奖励偏移和KL剪枝误差，提出了包含采样误差、奖励偏移误差和KL剪枝误差的泛化边界，并讨论了参数初始化和SGD训练的特殊情况。


<details>
  <summary>Details</summary>
Motivation: 尽管RLHF在大语言模型对齐中广泛应用，但其泛化理论理解仍不成熟，特别是在奖励可能偏移以及KL控制被估计和剪枝的情况下。需要理论框架来理解这些因素如何影响RLHF的泛化性能。

Method: 开发了考虑奖励偏移和剪枝KL正则化的RLHF泛化理论。奖励偏移指奖励模型在早期或混合行为策略数据上训练，而RLHF在当前策略rollouts上优化；KL剪枝误差来自采样对数概率比的估计和剪枝稳定化。

Result: 提出了RLHF的泛化边界，表明泛化误差来源于提示和rollouts的采样误差、奖励偏移误差和KL剪枝误差。讨论了有限空间均匀先验参数初始化和SGD作为Ornstein-Uhlenbeck过程的特殊情况。

Conclusion: 理论为RLHF实践提供了指导意义：(1) 最优KL剪枝阈值的选择；(2) 在提示、rollouts和偏好数据之间的预算分配策略。这些理论洞见有助于改进RLHF的实际应用。

Abstract: Alignment and adaptation in large language models heavily rely on reinforcement learning from human feedback (RLHF); yet, theoretical understanding of its generalisability remains premature, especially when the learned reward could shift, and the KL control is estimated and clipped. To address this issue, we develop generalisation theory for RLHF that explicitly accounts for (1) \emph{reward shift}: reward models are trained on preference data from earlier or mixed behaviour policies while RLHF optimises the current policy on its own rollouts; and (2) \emph{clipped KL regularisation}: the KL regulariser is estimated from sampled log-probability ratios and then clipped for stabilisation, resulting in an error to RLHF. We present generalisation bounds for RLHF, suggesting that the generalisation error stems from a sampling error from prompts and rollouts, a reward shift error, and a KL clipping error. We also discuss special cases of (1) initialising RLHF parameters with a uniform prior over a finite space, and (2) training RLHF by stochastic gradient descent, as an Ornstein-Uhlenbeck process. The theory yields practical implications in (1) optimal KL clipping threshold, and (2) budget allocation in prompts, rollouts, and preference data.

</details>


### [61] [Easy to Learn, Yet Hard to Forget: Towards Robust Unlearning Under Bias](https://arxiv.org/abs/2602.21773)
*JuneHyoung Kwon,MiHyeon Kim,Eunju Lee,Yoonji Lee,Seunghoon Lee,YoungBin Kim*

Main category: cs.LG

TL;DR: 论文提出CUPID框架解决机器学习遗忘中的"捷径遗忘"问题，即模型容易学习但难以忘记偏差对齐样本，反而会遗忘偏差属性而非类别属性。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘对数据隐私和模型可靠性至关重要，但在现实场景中，模型可能从数据中的虚假相关性学习到意外偏差，这会严重削弱遗忘效果。论文研究了从这种有偏差模型中遗忘的独特挑战。

Method: 提出CUPID框架：1) 基于样本损失景观锐度将遗忘集划分为因果近似和偏差近似子集；2) 将模型参数解耦为因果和偏差路径；3) 通过将细化的因果和偏差梯度路由到相应路径进行有针对性的更新。

Result: 在Waterbirds、BAR和Biased NICO++等偏差数据集上的大量实验表明，该方法实现了最先进的遗忘性能，并有效缓解了捷径遗忘问题。

Conclusion: CUPID框架成功解决了机器遗忘中的捷径遗忘问题，通过区分因果和偏差特征并进行有针对性的参数更新，实现了更有效的遗忘效果。

Abstract: Machine unlearning, which enables a model to forget specific data, is crucial for ensuring data privacy and model reliability. However, its effectiveness can be severely undermined in real-world scenarios where models learn unintended biases from spurious correlations within the data. This paper investigates the unique challenges of unlearning from such biased models. We identify a novel phenomenon we term ``shortcut unlearning," where models exhibit an ``easy to learn, yet hard to forget" tendency. Specifically, models struggle to forget easily-learned, bias-aligned samples; instead of forgetting the class attribute, they unlearn the bias attribute, which can paradoxically improve accuracy on the class intended to be forgotten. To address this, we propose CUPID, a new unlearning framework inspired by the observation that samples with different biases exhibit distinct loss landscape sharpness. Our method first partitions the forget set into causal- and bias-approximated subsets based on sample sharpness, then disentangles model parameters into causal and bias pathways, and finally performs a targeted update by routing refined causal and bias gradients to their respective pathways. Extensive experiments on biased datasets including Waterbirds, BAR, and Biased NICO++ demonstrate that our method achieves state-of-the-art forgetting performance and effectively mitigates the shortcut unlearning problem.

</details>


### [62] [Excitation: Momentum For Experts](https://arxiv.org/abs/2602.21798)
*Sagi Shaier*

Main category: cs.LG

TL;DR: Excitation是一种新颖的优化框架，通过基于专家利用率的动态更新调制来加速稀疏架构（如MoE）的学习，无需额外参数或内存开销。


<details>
  <summary>Details</summary>
Motivation: 传统优化器对所有参数统一处理，无法有效处理MoE等稀疏架构中的专家利用率差异，导致深度MoE中出现"结构混淆"现象，标准优化器无法建立有效的信号路径。

Method: Excitation根据批量级专家利用率动态调制更新，引入竞争性更新动态：放大高利用率专家的更新，可选择性地抑制低利用率专家，从而增强路由专业化。

Result: 在语言和视觉任务中，Excitation持续提升MoE模型的收敛速度和最终性能，能够"拯救"深度MoE模型，使其在基线方法失败的情况下实现稳定训练。

Conclusion: 主动更新调制是有效条件计算的关键机制，Excitation作为专业化催化剂，具有优化器、领域和模型无关性，且无需额外参数或内存开销，适用于内存受限环境。

Abstract: We propose Excitation, a novel optimization framework designed to accelerate learning in sparse architectures such as Mixture-of-Experts (MoEs). Unlike traditional optimizers that treat all parameters uniformly, Excitation dynamically modulates updates using batch-level expert utilization. It introduces a competitive update dynamic that amplifies updates to highly-utilized experts and can selectively suppress low-utilization ones, effectively sharpening routing specialization. Notably, we identify a phenomenon of "structural confusion" in deep MoEs, where standard optimizers fail to establish functional signal paths; Excitation acts as a specialization catalyst, "rescuing" these models and enabling stable training where baselines remain trapped. Excitation is optimizer-, domain-, and model-agnostic, requires minimal integration effort, and introduces neither additional per-parameter optimizer state nor learnable parameters, making it highly viable for memory-constrained settings. Across language and vision tasks, Excitation consistently improves convergence speed and final performance in MoE models, indicating that active update modulation is a key mechanism for effective conditional computation.

</details>


### [63] [DocDjinn: Controllable Synthetic Document Generation with VLMs and Handwriting Diffusion](https://arxiv.org/abs/2602.21824)
*Marcel Lamott,Saifullah Saifullah,Nauman Riaz,Yves-Noel Weweler,Tobias Alt-Veit,Ahmad Sarmad Ali,Muhammad Armaghan Shakir,Adrian Kalwa,Momina Moetesum,Andreas Dengel,Sheraz Ahmed,Faisal Shafait,Ulrich Schwanecke,Adrian Ulges*

Main category: cs.LG

TL;DR: DocDjinn：基于视觉语言模型的可控合成文档生成框架，仅需100个真实样本即可达到完整数据集87%的性能


<details>
  <summary>Details</summary>
Motivation: 文档智能模型需要大量标注数据，但数据获取成本高、隐私风险大。合成文档生成提供了一种隐私保护的替代方案。

Method: 基于聚类种子选择与参数化采样，通过语义-视觉解耦增强文档，生成视觉合理、语义一致的合成文档

Result: 在11个基准测试中表现优异，仅用100个真实样本即可达到完整数据集平均87%的性能，并公开了14万+合成文档样本

Conclusion: 首次证明视觉语言模型能够从未标注种子大规模生成高质量的标注文档数据集，有效替代或增强真实标注数据

Abstract: Effective document intelligence models rely on large amounts of annotated training data. However, procuring sufficient and high-quality data poses significant challenges due to the labor-intensive and costly nature of data acquisition. Additionally, leveraging language models to annotate real documents raises concerns about data privacy. Synthetic document generation has emerged as a promising, privacy-preserving alternative. We propose DocDjinn, a novel framework for controllable synthetic document generation using Vision-Language Models (VLMs) that produces annotated documents from unlabeled seed samples. Our approach generates visually plausible and semantically consistent synthetic documents that follow the distribution of an existing source dataset through clustering-based seed selection with parametrized sampling. By enriching documents with realistic diffusion-based handwriting and contextual visual elements via semantic-visual decoupling, we generate diverse, high-quality annotated synthetic documents. We evaluate across eleven benchmarks spanning key information extraction, question answering, document classification, and document layout analysis. To our knowledge, this is the first work demonstrating that VLMs can generate faithful annotated document datasets at scale from unlabeled seeds that can effectively enrich or approximate real, manually annotated data for diverse document understanding tasks. We show that with only 100 real training samples, our framework achieves on average $87\%$ of the performance of the full real-world dataset. We publicly release our code and 140k+ synthetic document samples.

</details>


### [64] [JSAM: Privacy Straggler-Resilient Joint Client Selection and Incentive Mechanism Design in Differentially Private Federated Learning](https://arxiv.org/abs/2602.21844)
*Ruichen Xu,Ying-Jun Angela Zhang,Jianwei Huang*

Main category: cs.LG

TL;DR: JSAM是一个联合优化客户端选择和隐私补偿的贝叶斯最优框架，通过优先选择隐私容忍度高的客户端并排除高敏感度参与者，在预算约束下最大化联邦学习训练效果。


<details>
  <summary>Details</summary>
Motivation: 差分隐私联邦学习面临根本矛盾：隐私保护机制在保护客户端数据的同时会产生可量化的隐私成本，这会阻碍参与度，破坏协作训练过程。现有激励机制依赖无偏客户端选择，迫使服务器补偿甚至最隐私敏感的客户端（"隐私滞后者"），导致系统效率低下和次优资源分配。

Method: 提出JSAM（联合客户端选择和隐私补偿机制），这是一个贝叶斯最优框架，同时优化客户端选择概率和隐私补偿，以在预算约束下最大化训练效果。通过新颖的理论特征化将复杂的2N维优化问题转化为高效的三维公式化。

Result: 证明服务器应优先选择隐私容忍度高的客户端，同时排除高敏感度参与者，并发现反直觉的洞察：具有最小隐私敏感度的客户端可能因频繁参与而产生最高的累积成本。在MNIST和CIFAR-10上的广泛评估显示，JSAM相比现有无偏选择机制在测试准确率上提升高达15%，同时在不同数据异构水平下保持成本效率。

Conclusion: JSAM通过联合优化客户端选择和隐私补偿，有效解决了差分隐私联邦学习中的参与激励问题，显著提高了训练效果和资源分配效率，为实际部署提供了实用解决方案。

Abstract: Differentially private federated learning faces a fundamental tension: privacy protection mechanisms that safeguard client data simultaneously create quantifiable privacy costs that discourage participation, undermining the collaborative training process. Existing incentive mechanisms rely on unbiased client selection, forcing servers to compensate even the most privacy-sensitive clients ("privacy stragglers"), leading to systemic inefficiency and suboptimal resource allocation. We introduce JSAM (Joint client Selection and privacy compensAtion Mechanism), a Bayesian-optimal framework that simultaneously optimizes client selection probabilities and privacy compensation to maximize training effectiveness under budget constraints. Our approach transforms a complex 2N-dimensional optimization problem into an efficient three-dimensional formulation through novel theoretical characterization of optimal selection strategies. We prove that servers should preferentially select privacy-tolerant clients while excluding high-sensitivity participants, and uncover the counter-intuitive insight that clients with minimal privacy sensitivity may incur the highest cumulative costs due to frequent participation. Extensive evaluations on MNIST and CIFAR-10 demonstrate that JSAM achieves up to 15% improvement in test accuracy compared to existing unbiased selection mechanisms while maintaining cost efficiency across varying data heterogeneity levels.

</details>


### [65] [xai-cola: A Python library for sparsifying counterfactual explanations](https://arxiv.org/abs/2602.21845)
*Lin Zhu,Lei You*

Main category: cs.LG

TL;DR: xai-cola是一个开源Python库，为任意反事实解释生成器提供端到端稀疏化流程，减少冗余特征修改同时保持解释有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大多数反事实解释生成器产生的解释往往高度冗余，包含过多不必要的特征修改，需要一种方法来稀疏化这些解释以提高可解释性。

Method: 开发xai-cola库，提供完整处理流程：输入原始表格数据、预处理对象和训练好的模型，支持内置或外部导入的CE生成器，实现多种稀疏化策略和可视化功能。

Result: 实验表明xai-cola能在多种CE生成器上产生更稀疏的反事实解释，在实验设置中将修改特征数量减少高达50%。

Conclusion: xai-cola是一个有效的反事实解释稀疏化工具，通过MIT许可开源，可从PyPI安装，有助于提高反事实解释的质量和实用性。

Abstract: Counterfactual explanation (CE) is an important domain within post-hoc explainability. However, the explanations generated by most CE generators are often highly redundant. This work introduces an open-source Python library xai-cola, which provides an end-to-end pipeline for sparsifying CEs produced by arbitrary generators, reducing superfluous feature changes while preserving their validity. It offers a documented API that takes as input raw tabular data in pandas DataFrame form, a preprocessing object (for standardization and encoding), and a trained scikit-learn or PyTorch model. On this basis, users can either employ the built-in or externally imported CE generators. The library also implements several sparsification policies and includes visualization routines for analysing and comparing sparsified counterfactuals. xai-cola is released under the MIT license and can be installed from PyPI. Empirical experiments indicate that xai-cola produces sparser counterfactuals across several CE generators, reducing the number of modified features by up to 50% in our setting. The source code is available at https://github.com/understanding-ml/COLA.

</details>


### [66] [The Error of Deep Operator Networks Is the Sum of Its Parts: Branch-Trunk and Mode Error Decompositions](https://arxiv.org/abs/2602.21910)
*Alexander Heinlein,Johannes Taraz*

Main category: cs.LG

TL;DR: 该论文分析了经典DeepONet架构的性能限制，发现当内部维度足够大时，近似误差主要由分支网络主导，且学习的基函数可被经典基函数替代而不显著影响性能。通过修改架构获得关键洞察：分支网络存在频谱偏差、误差由中间奇异值模式主导、共享分支网络改善小模式泛化、参数空间中存在有害的模式耦合。


<details>
  <summary>Details</summary>
Motivation: 尽管深度算子网络（DeepONets）具有理论上的通用逼近性质，但在实践中常常表现出有限的准确性和泛化能力，这阻碍了其在科学计算中的广泛应用。理解这些局限性对于进一步推进算子学习方法至关重要。

Method: 分析经典DeepONet架构的性能限制，构建修改版DeepONet，其中主干网络被训练解矩阵的左奇异向量替代。通过这种修改研究分支网络的频谱偏差、奇异值缩放效应、共享分支网络与堆叠架构的比较，以及参数空间中模式耦合的影响。

Result: 研究发现：1）当内部维度足够大时，近似误差主要由分支网络主导；2）学习的基函数可被经典基函数替代而不显著影响性能；3）分支网络存在频谱偏差，低频主导模式系数学习更有效；4）由于奇异值缩放，整体分支误差由中间奇异值模式主导而非最小奇异值模式；5）共享分支网络相比堆叠架构能更好泛化小模式；6）参数空间中存在强且有害的模式耦合。

Conclusion: 该研究揭示了DeepONet架构的关键性能限制，特别是分支网络在误差中的主导作用以及模式耦合问题。这些发现为改进算子学习架构提供了重要指导，有助于开发更准确、泛化能力更强的科学计算工具。

Abstract: Operator learning has the potential to strongly impact scientific computing by learning solution operators for differential equations, potentially accelerating multi-query tasks such as design optimization and uncertainty quantification by orders of magnitude. Despite proven universal approximation properties, deep operator networks (DeepONets) often exhibit limited accuracy and generalization in practice, which hinders their adoption. Understanding these limitations is therefore crucial for further advancing the approach.
  This work analyzes performance limitations of the classical DeepONet architecture. It is shown that the approximation error is dominated by the branch network when the internal dimension is sufficiently large, and that the learned trunk basis can often be replaced by classical basis functions without a significant impact on performance.
  To investigate this further, a modified DeepONet is constructed in which the trunk network is replaced by the left singular vectors of the training solution matrix. This modification yields several key insights. First, a spectral bias in the branch network is observed, with coefficients of dominant, low-frequency modes learned more effectively. Second, due to singular-value scaling of the branch coefficients, the overall branch error is dominated by modes with intermediate singular values rather than the smallest ones. Third, using a shared branch network for all mode coefficients, as in the standard architecture, improves generalization of small modes compared to a stacked architecture in which coefficients are computed separately. Finally, strong and detrimental coupling between modes in parameter space is identified.

</details>


### [67] [Learning in the Null Space: Small Singular Values for Continual Learning](https://arxiv.org/abs/2602.21919)
*Cuong Anh Pham,Praneeth Vepakomma,Samuel Horváth*

Main category: cs.LG

TL;DR: 提出NESS方法，利用小奇异值构建近似零空间，通过LoRA风格的低秩适配实现持续学习，减少灾难性遗忘


<details>
  <summary>Details</summary>
Motivation: 持续学习中的主要挑战是缓解灾难性遗忘同时支持进一步学习。基于正交性的方法因其效率和理论性质受到关注，但现有方法主要通过梯度投影实现正交性。本文重新审视正交性，利用小奇异值对应方向与先前任务输入空间近似正交的特性

Method: 提出NESS方法：1）利用每层输入表示的最小奇异值构建近似零空间；2）通过紧凑的低秩适配（LoRA风格）公式化任务特定更新，约束在该子空间内；3）固定子空间基以保持零空间约束，每个任务只学习单个可训练矩阵

Result: 在三个基准数据集上的实验表明，NESS具有竞争性性能、低遗忘率和跨任务稳定准确率，突显了小奇异值在持续学习中的作用

Conclusion: NESS通过在权重空间直接应用正交性而非梯度操作，有效缓解灾难性遗忘并支持持续学习，小奇异值在构建近似零空间中发挥关键作用

Abstract: Alleviating catastrophic forgetting while enabling further learning is a primary challenge in continual learning (CL). Orthogonal-based training methods have gained attention for their efficiency and strong theoretical properties, and many existing approaches enforce orthogonality through gradient projection. In this paper, we revisit orthogonality and exploit the fact that small singular values correspond to directions that are nearly orthogonal to the input space of previous tasks. Building on this principle, we introduce NESS (Null-space Estimated from Small Singular values), a CL method that applies orthogonality directly in the weight space rather than through gradient manipulation. Specifically, NESS constructs an approximate null space using the smallest singular values of each layer's input representation and parameterizes task-specific updates via a compact low-rank adaptation (LoRA-style) formulation constrained to this subspace. The subspace basis is fixed to preserve the null-space constraint, and only a single trainable matrix is learned for each task. This design ensures that the resulting updates remain approximately in the null space of previous inputs while enabling adaptation to new tasks. Our theoretical analysis and experiments on three benchmark datasets demonstrate competitive performance, low forgetting, and stable accuracy across tasks, highlighting the role of small singular values in continual learning. The code is available at https://github.com/pacman-ctm/NESS.

</details>


### [68] [Learning Unknown Interdependencies for Decentralized Root Cause Analysis in Nonlinear Dynamical Systems](https://arxiv.org/abs/2602.21928)
*Ayush Mohanty,Paritosh Ramanan,Nagi Gebraeel*

Main category: cs.LG

TL;DR: 提出了一种联邦交叉客户端依赖学习方法，用于特征分区的非线性时间序列数据，无需访问原始传感器数据或修改专有客户端模型，通过增强ML模型编码跨客户端依赖关系，实现网络工业系统的根本原因分析。


<details>
  <summary>Details</summary>
Motivation: 网络工业系统（如供应链和电力网络）的根本原因分析非常困难，因为地理分布客户端之间存在未知且动态演变的相互依赖关系。传统方法需要系统依赖图知识，这在复杂网络中很少可用。现有联邦学习方法假设同质特征空间和可重新训练的客户端模型，这与实际场景不符，不同客户端具有不同数据特征且通常运行固定的专有模型。

Method: 提出联邦交叉客户端依赖学习方法：每个专有本地客户端模型增强一个机器学习模型来编码跨客户端依赖关系；这些ML模型通过全局服务器协调，通过校准的差分隐私噪声强制表示一致性并保护隐私；使用模型残差和异常标志进行根本原因分析。

Result: 建立了理论收敛保证，并在广泛的模拟和真实世界工业网络安全数据集上验证了该方法。

Conclusion: 该方法能够在不需要原始传感器数据流或修改专有客户端模型的情况下，处理特征分区的非线性时间序列数据，有效解决网络工业系统中的根本原因分析问题，同时保护隐私。

Abstract: Root cause analysis (RCA) in networked industrial systems, such as supply chains and power networks, is notoriously difficult due to unknown and dynamically evolving interdependencies among geographically distributed clients. These clients represent heterogeneous physical processes and industrial assets equipped with sensors that generate large volumes of nonlinear, high-dimensional, and heterogeneous IoT data. Classical RCA methods require partial or full knowledge of the system's dependency graph, which is rarely available in these complex networks. While federated learning (FL) offers a natural framework for decentralized settings, most existing FL methods assume homogeneous feature spaces and retrainable client models. These assumptions are not compatible with our problem setting. Different clients have different data features and often run fixed, proprietary models that cannot be modified. This paper presents a federated cross-client interdependency learning methodology for feature-partitioned, nonlinear time-series data, without requiring access to raw sensor streams or modifying proprietary client models. Each proprietary local client model is augmented with a Machine Learning (ML) model that encodes cross-client interdependencies. These ML models are coordinated via a global server that enforces representation consistency while preserving privacy through calibrated differential privacy noise. RCA is performed using model residuals and anomaly flags. We establish theoretical convergence guarantees and validate our approach on extensive simulations and a real-world industrial cybersecurity dataset.

</details>


### [69] [Bayesian Generative Adversarial Networks via Gaussian Approximation for Tabular Data Synthesis](https://arxiv.org/abs/2602.21948)
*Bahrul Ilmi Nasution,Mark Elliot,Richard Allmendinger*

Main category: cs.LG

TL;DR: GACTGAN整合贝叶斯后验近似技术SWAG到CTGAN生成器中，用于合成表格数据，在训练后降低计算开销，相比CTGAN能更好地平衡风险-效用权衡。


<details>
  <summary>Details</summary>
Motivation: CTGAN虽然流行但难以有效平衡风险-效用权衡；贝叶斯GAN在表格数据中应用较少，且传统MCMC方法计算开销大、权重存储需求高。

Method: 提出GACTGAN，将随机权重平均-高斯(SWAG)贝叶斯后验近似技术整合到CTGAN生成器中，减少训练后的计算开销。

Result: GACTGAN相比CTGAN能生成更好的合成数据，更好地保持表格结构和推断统计特性，同时降低隐私风险。

Conclusion: GACTGAN是贝叶斯表格数据合成的更简单有效的实现方法，在风险-效用权衡方面表现更优。

Abstract: Generative Adversarial Networks (GAN) have been used in many studies to synthesise mixed tabular data. Conditional tabular GAN (CTGAN) have been the most popular variant but struggle to effectively navigate the risk-utility trade-off. Bayesian GAN have received less attention for tabular data, but have been explored with unstructured data such as images and text. The most used technique employed in Bayesian GAN is Markov Chain Monte Carlo (MCMC), but it is computationally intensive, particularly in terms of weight storage. In this paper, we introduce Gaussian Approximation of CTGAN (GACTGAN), an integration of the Bayesian posterior approximation technique using Stochastic Weight Averaging-Gaussian (SWAG) within the CTGAN generator to synthesise tabular data, reducing computational overhead after the training phase. We demonstrate that GACTGAN yields better synthetic data compared to CTGAN, achieving better preservation of tabular structure and inferential statistics with less privacy risk. These results highlight GACTGAN as a simpler, effective implementation of Bayesian tabular synthesis.

</details>


### [70] [Estimation and Optimization of Ship Fuel Consumption in Maritime: Review, Challenges and Future Directions](https://arxiv.org/abs/2602.21959)
*Dusica Marijan,Hamza Haruna Mohammed,Bakht Zaman*

Main category: cs.LG

TL;DR: 本文全面综述了海运燃油消耗的估计与优化方法，将方法分为物理模型、机器学习模型和混合模型，探讨了数据融合技术的重要性，首次讨论了可解释AI的作用，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 为减少碳排放和降低航运成本，提高船舶燃油效率至关重要。需要综合评估现有的燃油消耗估计和优化方法，以指导更有效的节能减排策略。

Method: 通过文献综述方法，将燃油消耗估计方法分为三类：基于物理的模型、机器学习模型和混合模型。探讨了AIS数据、船载传感器和气象数据融合技术，并分析了可解释AI在提高模型透明度中的作用。

Result: 系统梳理了燃油消耗估计和优化的各类方法，识别了数据质量、实时优化等关键挑战，提出了混合模型、实时优化和数据集标准化等未来研究方向。

Conclusion: 本文为海运燃油消耗的估计和优化提供了全面的框架，强调了数据融合和可解释AI的重要性，为未来研究指明了方向，有助于推动海运业的节能减排。

Abstract: To reduce carbon emissions and minimize shipping costs, improving the fuel efficiency of ships is crucial. Various measures are taken to reduce the total fuel consumption of ships, including optimizing vessel parameters and selecting routes with the lowest fuel consumption. Different estimation methods are proposed for predicting fuel consumption, while various optimization methods are proposed to minimize fuel oil consumption. This paper provides a comprehensive review of methods for estimating and optimizing fuel oil consumption in maritime transport. Our novel contributions include categorizing fuel oil consumption \& estimation methods into physics-based, machine-learning, and hybrid models, exploring their strengths and limitations. Furthermore, we highlight the importance of data fusion techniques, which combine AIS, onboard sensors, and meteorological data to enhance accuracy. We make the first attempt to discuss the emerging role of Explainable AI in enhancing model transparency for decision-making. Uniquely, key challenges, including data quality, availability, and the need for real-time optimization, are identified, and future research directions are proposed to address these gaps, with a focus on hybrid models, real-time optimization, and the standardization of datasets.

</details>


### [71] [Robustness in sparse artificial neural networks trained with adaptive topology](https://arxiv.org/abs/2602.21961)
*Bendegúz Sulyok,Gergely Palla,Filippo Radicchi,Santo Fortunato*

Main category: cs.LG

TL;DR: 该论文研究了具有自适应拓扑的稀疏人工神经网络的鲁棒性，使用三层99%稀疏度层加一个密集层的架构，在MNIST和Fashion MNIST图像分类任务上取得了竞争性精度，并分析了网络在各种扰动下的鲁棒性表现。


<details>
  <summary>Details</summary>
Motivation: 研究稀疏神经网络在自适应拓扑下的鲁棒性，探索在显著减少权重数量的情况下，如何通过拓扑更新维持网络性能，为开发高效可靠的深度学习模型提供新方向。

Method: 采用三层99%稀疏度的稀疏层加一个密集层的简单架构，在MNIST和Fashion MNIST图像分类任务上进行训练，并在每个epoch之间更新稀疏层的拓扑结构，分析网络在随机链接移除、对抗攻击和链接权重洗牌等扰动下的鲁棒性。

Result: 自适应拓扑稀疏网络在显著减少权重数量的情况下仍能取得竞争性分类精度，并且在各种扰动下表现出良好的鲁棒性，证明了自适应拓扑不仅能提高效率还能维持网络可靠性。

Conclusion: 自适应稀疏网络是开发高效可靠深度学习模型的有前景方向，通过动态调整拓扑结构，可以在保持高精度的同时显著减少计算资源需求，并增强网络对扰动的鲁棒性。

Abstract: We investigate the robustness of sparse artificial neural networks trained with adaptive topology. We focus on a simple yet effective architecture consisting of three sparse layers with 99% sparsity followed by a dense layer, applied to image classification tasks such as MNIST and Fashion MNIST. By updating the topology of the sparse layers between each epoch, we achieve competitive accuracy despite the significantly reduced number of weights. Our primary contribution is a detailed analysis of the robustness of these networks, exploring their performance under various perturbations including random link removal, adversarial attack, and link weight shuffling. Through extensive experiments, we demonstrate that adaptive topology not only enhances efficiency but also maintains robustness. This work highlights the potential of adaptive sparse networks as a promising direction for developing efficient and reliable deep learning models.

</details>


### [72] [Compact Circulant Layers with Spectral Priors](https://arxiv.org/abs/2602.21965)
*Joseph Margaryan,Thomas Hamelryck*

Main category: cs.LG

TL;DR: 论文提出在频域参数化的紧凑谱循环层，通过FFT对角化实现内存高效的神经网络，支持不确定性感知和Lipschitz边界计算，适用于边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 医疗、机器人和自主系统等关键应用需要内存高效、能感知不确定性的神经网络，适合边缘和资源受限的部署环境。现有方法在紧凑性和不确定性量化方面存在不足。

Method: 使用谱循环和BCCB层：FFT对角化的循环卷积，权重直接定义在实FFT半平面。在频域参数化滤波器，施加谱结构，在低维权重空间进行结构化变分推断，计算精确的层谱范数。通过独立复高斯分布获得平稳核的谱表示，定义谱先验和Hermitian感知的低秩加对角变分后验。

Result: 谱循环/BCCB层在贝叶斯和点估计两种机制中都有效：在MNIST->Fashion-MNIST上的紧凑贝叶斯神经网络、CIFAR-10特征的变分头、CIFAR-10/Tiny ImageNet上的确定性ViT投影；谱层匹配强基线，同时使用更少参数并提供更紧的Lipschitz证书。

Conclusion: 谱循环层是有效的紧凑构建块，通过频域参数化实现内存效率、不确定性量化和可证明的鲁棒性，特别适合资源受限的边缘部署场景。

Abstract: Critical applications in areas such as medicine, robotics and autonomous systems require compact (i.e., memory efficient), uncertainty-aware neural networks suitable for edge and other resource-constrained deployments. We study compact spectral circulant and block-circulant-with-circulant-blocks (BCCB) layers: FFT-diagonalizable circular convolutions whose weights live directly in the real FFT (RFFT) half (1D) or half-plane (2D). Parameterizing filters in the frequency domain lets us impose simple spectral structure, perform structured variational inference in a low-dimensional weight space, and calculate exact layer spectral norms, enabling inexpensive global Lipschitz bounds and margin-based robustness diagnostics. By placing independent complex Gaussians on the Hermitian support we obtain a discrete instance of the spectral representation of stationary kernels, inducing an exact stationary Gaussian-process prior over filters on the discrete circle/torus. We exploit this to define a practical spectral prior and a Hermitian-aware low-rank-plus-diagonal variational posterior in real coordinates. Empirically, spectral circulant/BCCB layers are effective compact building blocks in both (variational) Bayesian and point estimate regimes: compact Bayesian neural networks on MNIST->Fashion-MNIST, variational heads on frozen CIFAR-10 features, and deterministic ViT projections on CIFAR-10/Tiny ImageNet; spectral layers match strong baselines while using substantially fewer parameters and with tighter Lipschitz certificates.

</details>


### [73] [Neural solver for Wasserstein Geodesics and optimal transport dynamics](https://arxiv.org/abs/2602.22003)
*Hailiang Liu,Yan-Han Chen*

Main category: cs.LG

TL;DR: 提出基于神经网络的Wasserstein测地线求解器，通过动态最优传输公式将约束优化转化为极小极大问题，用神经网络近似相关函数，同时恢复最优传输映射和速度场。


<details>
  <summary>Details</summary>
Motivation: 机器学习社区越来越多地采用最优传输框架来建模分布关系，但需要一种能够同时计算Wasserstein测地线、相关速度场和最优传输映射的灵活方法。

Method: 基于最优传输的动态公式，将约束优化问题转化为极小极大问题，使用深度神经网络近似相关函数，通过估计最优传输映射获得粒子轨迹上的速度估计，进而学习完整速度场。

Result: 该方法不仅提供了Wasserstein测地线，还能恢复最优传输映射，实现从目标分布的直接采样，并能够扩展到包括二次成本在内的通用成本函数，在合成和真实数据集上验证了有效性。

Conclusion: 提出的基于神经网络的求解器为计算Wasserstein测地线和相关速度场提供了一种灵活有效的框架，能够同时恢复最优传输映射并支持直接采样，扩展了最优传输在机器学习中的应用。

Abstract: In recent years, the machine learning community has increasingly embraced the optimal transport (OT) framework for modeling distributional relationships. In this work, we introduce a sample-based neural solver for computing the Wasserstein geodesic between a source and target distribution, along with the associated velocity field. Building on the dynamical formulation of the optimal transport (OT) problem, we recast the constrained optimization as a minimax problem, using deep neural networks to approximate the relevant functions. This approach not only provides the Wasserstein geodesic but also recovers the OT map, enabling direct sampling from the target distribution. By estimating the OT map, we obtain velocity estimates along particle trajectories, which in turn allow us to learn the full velocity field. The framework is flexible and readily extends to general cost functions, including the commonly used quadratic cost. We demonstrate the effectiveness of our method through experiments on both synthetic and real datasets.

</details>


### [74] [Function-Space Empirical Bayes Regularisation with Student's t Priors](https://arxiv.org/abs/2602.22015)
*Pengcheng Hao,Ercan Engin Kuruoglu*

Main category: cs.LG

TL;DR: 提出ST-FS-EB框架，使用重尾Student's t分布作为先验，在参数和函数空间进行经验贝叶斯正则化，提升贝叶斯深度学习的不确定性估计能力。


<details>
  <summary>Details</summary>
Motivation: 现有函数空间变分推断方法通常使用高斯先验，无法捕捉神经网络输出的重尾统计特性，导致不确定性估计不够准确。

Method: 提出ST-FS-EB框架：1) 在参数和函数空间使用重尾Student's t分布作为先验；2) 通过变分推断近似后验分布；3) 基于蒙特卡洛dropout推导证据下界目标函数。

Result: 与多种基于变分推断的贝叶斯深度学习基线方法相比，ST-FS-EB在分布内预测、分布外检测和处理分布偏移方面表现出更稳健的性能。

Conclusion: 使用重尾Student's t先验的经验贝叶斯正则化框架能有效提升贝叶斯深度学习的不确定性估计能力，特别是在处理复杂统计特性时表现优异。

Abstract: Bayesian deep learning (BDL) has emerged as a principled approach to produce reliable uncertainty estimates by integrating deep neural networks with Bayesian inference, and the selection of informative prior distributions remains a significant challenge. Various function-space variational inference (FSVI) regularisation methods have been presented, assigning meaningful priors over model predictions. However, these methods typically rely on a Gaussian prior, which fails to capture the heavy-tailed statistical characteristics inherent in neural network outputs. By contrast, this work proposes a novel function-space empirical Bayes regularisation framework -- termed ST-FS-EB -- which employs heavy-tailed Student's $t$ priors in both parameter and function spaces. Also, we approximate the posterior distribution through variational inference (VI), inducing an evidence lower bound (ELBO) objective based on Monte Carlo (MC) dropout. Furthermore, the proposed method is evaluated against various VI-based BDL baselines, and the results demonstrate its robust performance in in-distribution prediction, out-of-distribution (OOD) detection and handling distribution shifts.

</details>


### [75] [Disease Progression and Subtype Modeling for Combined Discrete and Continuous Input Data](https://arxiv.org/abs/2602.22018)
*Sterre de Jonge,Elisabeth J. Vinke,Meike W. Vernooij,Daniel C. Alexander,Alexandra L. Young,Esther E. Bron*

Main category: cs.LG

TL;DR: 提出Mixed-SuStaIn模型，能够同时处理离散和连续数据类型的疾病进展建模，扩展了传统模型的适用性


<details>
  <summary>Details</summary>
Motivation: 现有疾病进展模型大多只能处理单一数据类型（如连续数据），限制了其在异质真实世界数据集中的应用，需要能同时处理离散和连续数据的模型

Method: 提出Mixed Events模型，在SuStaIn框架中实现为Mixed-SuStaIn，能够处理离散和连续混合数据类型，支持亚型和进展建模

Result: 通过模拟实验和阿尔茨海默病神经影像学倡议的真实数据验证，Mixed-SuStaIn在混合数据集上表现良好

Conclusion: Mixed-SuStaIn扩展了疾病进展建模的适用性，能够处理真实世界中的异质数据，为理解阿尔茨海默病等长期疾病轨迹提供了更强大的工具

Abstract: Disease progression modeling provides a robust framework to identify long-term disease trajectories from short-term biomarker data. It is a valuable tool to gain a deeper understanding of diseases with a long disease trajectory, such as Alzheimer's disease. A key limitation of most disease progression models is that they are specific to a single data type (e.g., continuous data), thereby limiting their applicability to heterogeneous, real-world datasets. To address this limitation, we propose the Mixed Events model, a novel disease progression model that handles both discrete and continuous data types. This model is implemented within the Subtype and Stage Inference (SuStaIn) framework, resulting in Mixed-SuStaIn, enabling subtype and progression modeling. We demonstrate the effectiveness of Mixed-SuStaIn through simulation experiments and real-world data from the Alzheimer's Disease Neuroimaging Initiative, showing that it performs well on mixed datasets. The code is available at: https://github.com/ucl-pond/pySuStaIn.

</details>


### [76] [Physics-Informed Machine Learning for Vessel Shaft Power and Fuel Consumption Prediction: Interpretable KAN-based Approach](https://arxiv.org/abs/2602.22055)
*Hamza Haruna Mohammed,Dusica Marijan,Arnbjørn Maressa*

Main category: cs.LG

TL;DR: PI-KAN模型结合物理知识和数据驱动，在船舶性能预测中实现高精度和可解释性，超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统物理模型可解释但适应性差，纯数据驱动方法精度高但缺乏物理一致性，需要一种兼顾精度和物理合理性的船舶性能预测方法。

Method: 提出物理信息Kolmogorov-Arnold网络(PI-KAN)，结合可解释的单变量特征变换、物理信息损失函数和无泄漏链式预测管道。

Result: 在五艘货船数据上，PI-KAN在轴功率和燃油消耗预测中取得最低MAE和RMSE、最高R²，同时保持物理一致性，并能发现领域一致的依赖关系。

Conclusion: PI-KAN实现了预测精度和可解释性的平衡，为船舶性能监控和运营决策提供了可靠工具。

Abstract: Accurate prediction of shaft rotational speed, shaft power, and fuel consumption is crucial for enhancing operational efficiency and sustainability in maritime transportation. Conventional physics-based models provide interpretability but struggle with real-world variability, while purely data-driven approaches achieve accuracy at the expense of physical plausibility. This paper introduces a Physics-Informed Kolmogorov-Arnold Network (PI-KAN), a hybrid method that integrates interpretable univariate feature transformations with a physics-informed loss function and a leakage-free chained prediction pipeline. Using operational and environmental data from five cargo vessels, PI-KAN consistently outperforms the traditional polynomial method and neural network baselines. The model achieves the lowest mean absolute error (MAE) and root mean squared error (RMSE), and the highest coefficient of determination (R^2) for shaft power and fuel consumption across all vessels, while maintaining physically consistent behavior. Interpretability analysis reveals rediscovery of domain-consistent dependencies, such as cubic-like speed-power relationships and cosine-like wave and wind effects. These results demonstrate that PI-KAN achieves both predictive accuracy and interpretability, offering a robust tool for vessel performance monitoring and decision support in operational settings.

</details>


### [77] [DualWeaver: Synergistic Feature Weaving Surrogates for Multivariate Forecasting with Univariate Time Series Foundation Models](https://arxiv.org/abs/2602.22066)
*Jinpeng Li,Zhongyi Pei,Huaze Xue,Bojian Zheng,Chen Wang,Jianmin Wang*

Main category: cs.LG

TL;DR: DualWeaver框架通过一对可学习的对称代理序列，将单变量时间序列基础模型适配到多变量预测任务，无需额外参数解码即可重建最终预测。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型在单变量预测上表现出色，但如何有效扩展到多变量预测仍具挑战。现有方法难以充分利用单变量TSFMs的优势进行多变量预测。

Method: 提出DualWeaver框架：1）使用共享的辅助特征融合模块捕获跨变量依赖关系，生成一对结构对称的代理序列；2）通过预测目标将代理序列映射为TSFM兼容的序列；3）利用对称结构实现参数自由的重建，无需额外参数解码；4）引入理论基础的正则化项防止适应崩溃。

Result: 在多个真实世界数据集上的广泛实验表明，DualWeaver在准确性和稳定性方面均优于最先进的多变量预测方法。

Conclusion: DualWeaver成功将单变量时间序列基础模型适配到多变量预测任务，通过对称代理序列和参数自由重建机制，实现了优越的多变量预测性能。

Abstract: Time-series foundation models (TSFMs) have achieved strong univariate forecasting through large-scale pre-training, yet effectively extending this success to multivariate forecasting remains challenging. To address this, we propose DualWeaver, a novel framework that adapts univariate TSFMs (Uni-TSFMs) for multivariate forecasting by using a pair of learnable, structurally symmetric surrogate series. Generated by a shared auxiliary feature-fusion module that captures cross-variable dependencies, these surrogates are mapped to TSFM-compatible series via the forecasting objective. The symmetric structure enables parameter-free reconstruction of final predictions directly from the surrogates, without additional parametric decoding. A theoretically grounded regularization term is further introduced to enhance robustness against adaptation collapse. Extensive experiments on diverse real-world datasets show that DualWeaver outperforms state-of-the-art multivariate forecasters in both accuracy and stability. We release the code at https://github.com/li-jinpeng/DualWeaver.

</details>


### [78] [On Imbalanced Regression with Hoeffding Trees](https://arxiv.org/abs/2602.22101)
*Pantia-Marina Alchirch,Dimitrios I. Diochnos*

Main category: cs.LG

TL;DR: 该研究将核密度估计(KDE)和层次收缩(HS)两种批处理方法扩展到在线学习环境，评估它们在流数据回归任务中对增量决策树的性能影响。


<details>
  <summary>Details</summary>
Motivation: 现实应用中存在连续数据流用于回归任务，传统Hoeffding树及其变种虽然有效，但批处理中的KDE和HS方法在回归任务中表现出色，需要探索这些方法在流式环境中的适用性。

Method: 使用伸缩论证将KDE方法扩展到流式环境，并将HS实现扩展到增量决策树模型，在在线回归数据集上评估这些扩展对决策树性能的影响。

Result: KDE在数据流早期阶段对性能有益，而HS方法几乎从未提供性能优势。

Conclusion: 在流式回归任务中，KDE扩展具有实用价值，特别是在数据流初期，而HS方法在在线环境中的效果有限。

Abstract: Many real-world applications provide a continuous stream of data that is subsequently used by machine learning models to solve regression tasks of interest. Hoeffding trees and their variants have a long-standing tradition due to their effectiveness, either alone or as base models in broader ensembles. At the same time a recent line of work in batch learning has shown that kernel density estimation (KDE) is an effective approach for smoothed predictions in imbalanced regression tasks [Yang et al., 2021]. Moreover, another recent line of work for batch learning, called hierarchical shrinkage (HS) [Agarwal et al., 2022], has introduced a post-hoc regularization method for decision trees that does not alter the structure of the learned tree. Using a telescoping argument we cast KDE to streaming environments and extend the implementation of HS to incremental decision tree models. Armed with these extensions we investigate the performance of decision trees that may enjoy such options in datasets commonly used for regression in online settings. We conclude that KDE is beneficial in the early parts of the stream, while HS hardly, if ever, offers performance benefits. Our code is publicly available at: https://github.com/marinaAlchirch/DSFA_2026.

</details>


### [79] [Don't stop me now: Rethinking Validation Criteria for Model Parameter Selection](https://arxiv.org/abs/2602.22107)
*Andrea Apicella,Francesco Isgrò,Andrea Pollastro,Roberto Prevete*

Main category: cs.LG

TL;DR: 验证集选择标准对神经网络泛化性能影响的实证研究：发现基于验证准确率的早停效果最差，损失函数作为验证标准更稳定，任何验证规则通常都无法达到跨所有epoch的最佳测试性能。


<details>
  <summary>Details</summary>
Motivation: 尽管训练损失函数已有大量研究，但验证集上用于模型选择的评估标准如何影响测试性能的研究仍不足。本研究旨在系统实证分析验证标准（特别是早停）对神经网络分类器测试性能的影响。

Method: 在标准基准数据集上使用全连接网络进行k折评估，比较：(i) 基于耐心的早停和(ii) 在所有epoch后选择（无早停）。使用交叉熵、C-Loss或PolyLoss训练模型，验证集上分别使用准确率或三种损失函数之一独立进行模型参数选择。

Result: 三个主要发现：1) 基于验证准确率的早停效果最差，始终选择测试准确率较低的检查点；2) 基于损失的验证标准产生可比且更稳定的测试准确率；3) 任何单一验证规则通常都低于跨所有epoch的最佳测试性能，无论使用何种验证标准，所选模型的测试性能通常都统计显著低于所有epoch中的最佳性能。

Conclusion: 应避免使用验证准确率（特别是结合早停）进行参数选择，推荐使用基于损失的验证标准。验证集选择标准对最终模型性能有显著影响，需要谨慎选择。

Abstract: Despite the extensive literature on training loss functions, the evaluation of generalization on the validation set remains underexplored. In this work, we conduct a systematic empirical and statistical study of how the validation criterion used for model selection affects test performance in neural classifiers, with attention to early stopping. Using fully connected networks on standard benchmarks under $k$-fold evaluation, we compare: (i) early stopping with patience and (ii) post-hoc selection over all epochs (i.e. no early stopping). Models are trained with cross-entropy, C-Loss, or PolyLoss; the model parameter selection on the validation set is made using accuracy or one of the three loss functions, each considered independently. Three main findings emerge. (1) Early stopping based on validation accuracy performs worst, consistently selecting checkpoints with lower test accuracy than both loss-based early stopping and post-hoc selection. (2) Loss-based validation criteria yield comparable and more stable test accuracy. (3) Across datasets and folds, any single validation rule often underperforms the test-optimal checkpoint. Overall, the selected model typically achieves test-set performance statistically lower than the best performance across all epochs, regardless of the validation criterion. Our results suggest avoiding validation accuracy (in particular with early stopping) for parameter selection, favoring loss-based validation criteria.

</details>


### [80] [Sample Complexity Bounds for Robust Mean Estimation with Mean-Shift Contamination](https://arxiv.org/abs/2602.22130)
*Ilias Diakonikolas,Giannis Iakovidis,Daniel M. Kane,Sihan Liu*

Main category: cs.LG

TL;DR: 该论文研究了均值偏移污染模型下的均值估计问题，为一般基础分布提供了样本复杂度上下界，并引入了傅里叶见证的概念。


<details>
  <summary>Details</summary>
Motivation: 先前工作只解决了高斯分布和拉普拉斯分布等特殊情况下的均值估计问题，但对于一般基础分布，均值偏移污染模型下的样本复杂度仍是一个开放问题。作者旨在确定一般基础分布在此模型下的样本复杂度。

Method: 使用傅里叶分析技术，在基础分布特征函数满足温和谱条件的前提下，提出了一个样本高效的算法。关键创新是引入了傅里叶见证的概念，作为上下界分析的基本工具。

Result: 证明了在温和谱条件下，存在样本高效的算法能够以任意精度估计目标均值。同时给出了与上界在质量上匹配的样本复杂度下界，基本解决了这个开放问题。

Conclusion: 该工作基本解决了均值偏移污染模型下一般基础分布的均值估计样本复杂度问题，展示了傅里叶分析在此类鲁棒统计问题中的关键作用，并为后续研究提供了理论框架。

Abstract: We study the basic task of mean estimation in the presence of mean-shift contamination. In the mean-shift contamination model, an adversary is allowed to replace a small constant fraction of the clean samples by samples drawn from arbitrarily shifted versions of the base distribution. Prior work characterized the sample complexity of this task for the special cases of the Gaussian and Laplace distributions. Specifically, it was shown that consistent estimation is possible in these cases, a property that is provably impossible in Huber's contamination model. An open question posed in earlier work was to determine the sample complexity of mean estimation in the mean-shift contamination model for general base distributions. In this work, we study and essentially resolve this open question. Specifically, we show that, under mild spectral conditions on the characteristic function of the (potentially multivariate) base distribution, there exists a sample-efficient algorithm that estimates the target mean to any desired accuracy. We complement our upper bound with a qualitatively matching sample complexity lower bound. Our techniques make critical use of Fourier analysis, and in particular introduce the notion of a Fourier witness as an essential ingredient of our upper and lower bounds.

</details>


### [81] [SigmaQuant: Hardware-Aware Heterogeneous Quantization Method for Edge DNN Inference](https://arxiv.org/abs/2602.22136)
*Qunyou Liu,Pengbo Yu,Marina Zapater,David Atienza*

Main category: cs.LG

TL;DR: SigmaQuant：一种自适应层间异构量化框架，无需穷举搜索即可为不同边缘环境高效平衡精度与资源使用


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在边缘设备部署面临严重资源限制，均匀量化无法充分利用各层鲁棒性差异，导致精度下降或资源使用次优；现有异构量化方法要么需要大量暴力搜索，要么缺乏适应不同硬件条件的能力

Method: 提出SigmaQuant自适应层间异构量化框架，通过智能分配不同比特宽度给各层，无需穷举搜索即可适应不同硬件条件

Result: 该框架能够高效平衡精度与资源使用，适应不同边缘环境的内存大小、能耗预算和延迟要求

Conclusion: SigmaQuant填补了现有量化方法的空白，为边缘设备部署神经网络提供了更灵活高效的解决方案

Abstract: Deep neural networks (DNNs) are essential for performing advanced tasks on edge or mobile devices, yet their deployment is often hindered by severe resource constraints, including limited memory, energy, and computational power. While uniform quantization provides a straightforward approach to compress model and reduce hardware requirement, it fails to fully leverage the varying robustness across layers, and often lead to accuracy degradation or suboptimal resource usage, particularly at low bitwidths. In contrast, heterogeneous quantization, which allocates different bitwidths to individual layers, can mitigate these drawbacks. Nonetheless, current heterogeneous quantization methods either needs huge brute-force design space search or lacks the adaptability to meet different hardware conditions, such as memory size, energy budget, and latency requirement. Filling these gaps, this work introduces \textbf{\textit{SigmaQuant}}, an adaptive layer-wise heterogeneous quantization framework designed to efficiently balance accuracy and resource usage for varied edge environments without exhaustive search.

</details>


### [82] [Provable Last-Iterate Convergence for Multi-Objective Safe LLM Alignment via Optimistic Primal-Dual](https://arxiv.org/abs/2602.22146)
*Yining Li,Peizhong Ju,Ness Shroff*

Main category: cs.LG

TL;DR: 本文提出了一个通用的原始-对偶框架用于安全RLHF，并引入乐观原始-对偶算法，为参数化策略下的约束对齐问题提供最终迭代收敛保证。


<details>
  <summary>Details</summary>
Motivation: 现有的RLHF方法存在理论缺陷：标准原始-对偶方法仅在分布策略下保证收敛，而在实际参数化策略中可能出现不稳定或发散。需要建立更稳健的理论框架来解决约束对齐问题。

Method: 提出通用原始-对偶框架统一现有对齐算法，并引入乐观原始-对偶算法，通过对原始变量和对偶变量进行预测更新来稳定鞍点动态。

Result: 建立了最终迭代收敛保证，涵盖分布空间中的精确策略优化，以及在参数化策略下收敛到最优解邻域，邻域大小与近似误差和偏差相关。

Conclusion: 乐观性在缓解约束对齐目标固有振荡中起关键作用，填补了约束RL与实用RLHF之间的重要理论空白，为安全对齐提供了更稳健的算法框架。

Abstract: Reinforcement Learning from Human Feedback (RLHF) plays a significant role in aligning Large Language Models (LLMs) with human preferences. While RLHF with expected reward constraints can be formulated as a primal-dual optimization problem, standard primal-dual methods only guarantee convergence with a distributional policy where the saddle-point problem is in convex-concave form. Moreover, standard primal-dual methods may exhibit instability or divergence in the last iterate under policy parameterization in practical applications. In this work, we propose a universal primal-dual framework for safe RLHF that unifies a broad class of existing alignment algorithms, including safe-RLHF, one-shot, and multi-shot based methods. Building on this framework, we introduce an optimistic primal-dual (OPD) algorithm that incorporates predictive updates for both primal and dual variables to stabilize saddle-point dynamics. We establish last-iterate convergence guarantees for the proposed method, covering both exact policy optimization in the distributional space and convergence to a neighborhood of the optimal solution whose gap is related to approximation error and bias under parameterized policies. Our analysis reveals that optimism plays a crucial role in mitigating oscillations inherent to constrained alignment objectives, thereby closing a key theoretical gap between constrained RL and practical RLHF.

</details>


### [83] [Learning and Naming Subgroups with Exceptional Survival Characteristics](https://arxiv.org/abs/2602.22179)
*Mhd Jawad Al Rahwanji,Sascha Xu,Nils Philipp Walter,Jilles Vreeken*

Main category: cs.LG

TL;DR: Sysurv：一种完全可微、非参数的方法，利用随机生存森林学习个体生存曲线，自动学习条件并组合成可解释规则，以发现具有异常生存特征的亚群。


<details>
  <summary>Details</summary>
Motivation: 在医学和预测性维护等应用中，识别生存时间更长或更短的亚群很重要。现有方法需要限制性假设（如比例风险）、预离散化特征，并且基于平均统计比较容易忽略个体差异。

Method: 提出Sysurv方法：完全可微、非参数，利用随机生存森林学习个体生存曲线，自动学习条件并将这些条件组合成内在可解释的规则，以选择具有异常生存特征的亚群。

Result: 在广泛的数据集和设置上进行实证评估，包括癌症数据的案例研究，表明Sysurv能够揭示有洞察力和可操作的生存亚群。

Conclusion: Sysurv提供了一种有效的方法来发现具有异常生存特征的亚群，克服了现有方法的限制，并在实际应用中显示出实用价值。

Abstract: In many applications, it is important to identify subpopulations that survive longer or shorter than the rest of the population. In medicine, for example, it allows determining which patients benefit from treatment, and in predictive maintenance, which components are more likely to fail. Existing methods for discovering subgroups with exceptional survival characteristics require restrictive assumptions about the survival model (e.g. proportional hazards), pre-discretized features, and, as they compare average statistics, tend to overlook individual deviations. In this paper, we propose Sysurv, a fully differentiable, non-parametric method that leverages random survival forests to learn individual survival curves, automatically learns conditions and how to combine these into inherently interpretable rules, so as to select subgroups with exceptional survival characteristics. Empirical evaluation on a wide range of datasets and settings, including a case study on cancer data, shows that Sysurv reveals insightful and actionable survival subgroups.

</details>


### [84] [Surrogate models for Rock-Fluid Interaction: A Grid-Size-Invariant Approach](https://arxiv.org/abs/2602.22188)
*Nathalie C. Pinheiro,Donghu Guo,Hannah P. Menke,Aniket C. Joshi,Claire E. Heaney,Ahmed H. ElSheikh,Christopher C. Pain*

Main category: cs.LG

TL;DR: 该研究开发了八种替代模型来预测多孔介质中的流体流动，包括四种基于神经网络的降阶模型和四种具有网格尺寸不变性的单神经网络模型，其中UNet++架构表现优于UNet，网格尺寸不变性方法在减少训练内存消耗方面表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统高保真数值模型需要高分辨率才能获得可靠结果，计算成本巨大，限制了其在不确定性量化和优化等多查询问题中的应用。需要开发更经济的替代模型来预测多孔介质中的流体流动。

Method: 开发了八种替代模型：四种降阶模型（ROM）使用一个神经网络进行压缩，另一个进行预测；四种单神经网络具有网格尺寸不变性，能够在比训练时更大的计算域上进行推理。比较了UNet和UNet++架构的性能。

Result: UNet++在替代模型中表现优于UNet；网格尺寸不变性方法能可靠减少训练内存消耗，预测值与真实值相关性良好，且优于分析的降阶模型。该方法特别适用于流体诱导岩石溶解导致固体场非静态的挑战性应用。

Conclusion: 网格尺寸不变性框架为替代模型提供了新颖的方法，能够有效减少计算成本，在多孔介质流体流动预测中表现出良好性能，特别适用于具有非静态固体场的复杂岩石-流体相互作用问题。

Abstract: Modelling rock-fluid interaction requires solving a set of partial differential equations (PDEs) to predict the flow behaviour and the reactions of the fluid with the rock on the interfaces. Conventional high-fidelity numerical models require a high resolution to obtain reliable results, resulting in huge computational expense. This restricts the applicability of these models for multi-query problems, such as uncertainty quantification and optimisation, which require running numerous scenarios. As a cheaper alternative to high-fidelity models, this work develops eight surrogate models for predicting the fluid flow in porous media. Four of these are reduced-order models (ROM) based on one neural network for compression and another for prediction. The other four are single neural networks with the property of grid-size invariance; a term which we use to refer to image-to-image models that are capable of inferring on computational domains that are larger than those used during training. In addition to the novel grid-size-invariant framework for surrogate models, we compare the predictive performance of UNet and UNet++ architectures, and demonstrate that UNet++ outperforms UNet for surrogate models. Furthermore, we show that the grid-size-invariant approach is a reliable way to reduce memory consumption during training, resulting in good correlation between predicted and ground-truth values and outperforming the ROMs analysed. The application analysed is particularly challenging because fluid-induced rock dissolution results in a non-static solid field and, consequently, it cannot be used to help in adjustments of the future prediction.

</details>


### [85] [GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL](https://arxiv.org/abs/2602.22190)
*Rui Yang,Qianhui Wu,Zhaoyang Wang,Hanyang Chen,Ke Yang,Hao Cheng,Huaxiu Yao,Baoling Peng,Huan Zhang,Jianfeng Gao,Tong Zhang*

Main category: cs.LG

TL;DR: GUI-Libra提出针对GUI代理的定制化训练方案，解决开源GUI代理在长视野导航任务中落后于闭源系统的问题，通过数据构建、动作感知SFT和KL正则化RL改进推理与接地能力。


<details>
  <summary>Details</summary>
Motivation: 开源原生GUI代理在长视野导航任务上落后于闭源系统，主要原因是缺乏高质量的动作对齐推理数据，以及通用后训练流程忽略了GUI代理的特殊挑战，特别是标准SFT会损害接地能力，而逐步RLVR训练面临部分可验证性问题。

Method: 1) 构建和过滤数据管道，发布81K GUI推理数据集；2) 提出动作感知SFT，混合推理-动作和直接动作数据，重新加权token以强调动作和接地；3) 在部分可验证性下稳定RL，强调KL正则化的重要性，引入成功自适应缩放来降低不可靠负梯度权重。

Result: 在多样化的网页和移动基准测试中，GUI-Libra持续改进步骤准确性和端到端任务完成率，表明精心设计的后训练和数据管理可以在不进行昂贵在线数据收集的情况下显著提升任务解决能力。

Conclusion: GUI-Libra展示了针对GUI代理的定制化训练方案的有效性，通过解决数据稀缺、推理-接地冲突和部分可验证性问题，显著提升了开源GUI代理的性能，为推理能力强的GUI代理的数据高效后训练提供了新方向。

Abstract: Open-source native GUI agents still lag behind closed-source systems on long-horizon navigation tasks. This gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data, and the direct adoption of generic post-training pipelines that overlook the unique challenges of GUI agents. We identify two fundamental issues in these pipelines: (i) standard SFT with CoT reasoning often hurts grounding, and (ii) step-wise RLVR-tyle training faces partial verifiability, where multiple actions can be correct but only a single demonstrated action is used for verification. This makes offline step-wise metrics weak predictors of online task success. In this work, we present GUI-Libra, a tailored training recipe that addresses these challenges. First, to mitigate the scarcity of action-aligned reasoning data, we introduce a data construction and filtering pipeline and release a curated 81K GUI reasoning dataset. Second, to reconcile reasoning with grounding, we propose action-aware SFT that mixes reasoning-then-action and direct-action data and reweights tokens to emphasize action and grounding. Third, to stabilize RL under partial verifiability, we identify the overlooked importance of KL regularization in RLVR and show that a KL trust region is critical for improving offline-to-online predictability; we further introduce success-adaptive scaling to downweight unreliable negative gradients. Across diverse web and mobile benchmarks, GUI-Libra consistently improves both step-wise accuracy and end-to-end task completion. Our results suggest that carefully designed post-training and data curation can unlock significantly stronger task-solving capabilities without costly online data collection. We release our dataset, code, and models to facilitate further research on data-efficient post-training for reasoning-capable GUI agents.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [86] [A Physics-Informed Neuro-Fuzzy Framework for Quantum Error Attribution](https://arxiv.org/abs/2602.21253)
*Marwa R. Hassan,Naima Kaabouch*

Main category: quant-ph

TL;DR: 提出一种神经模糊框架，结合ANFIS与物理特征工程，解决量子计算中软件bug与硬件噪声的区分问题，在156量子比特处理器上达到89.5%准确率。


<details>
  <summary>Details</summary>
Motivation: 随着量子处理器规模超过100量子比特，区分软件bug和随机硬件噪声成为关键诊断挑战。需要建立可靠的诊断方法来防止错误缓解技术被错误地应用于逻辑有缺陷的电路。

Method: 采用自适应神经模糊推理系统(ANFIS)结合物理基础的特征工程，引入基于数据处理不等式的Bhattacharyya否决机制作为硬物理约束，防止分类器将拓扑上不可能的输出分布归因于噪声。

Result: 在IBM的156量子比特Heron r2处理器上验证，涵盖17种算法家族的105个电路，框架达到89.5%有效准确率（±5.9%置信区间）。系统实现安全故障模式，将14.3%的模糊案例标记为需要人工审查。

Conclusion: 该工作建立了稳健、可解释的诊断层，能够区分正确的Grover放大与bug引起的崩溃，识别单基诊断的基本限制（如Z基盲点），防止错误缓解技术被应用于逻辑有缺陷的电路。

Abstract: As quantum processors scale beyond 100 qubits, distinguishing software bugs from stochastic hardware noise becomes a critical diagnostic challenge. We present a neuro-fuzzy framework that addresses this attribution problem by combining Adaptive Neuro-Fuzzy Inference Systems (ANFIS) with physics-grounded feature engineering. We introduce the Bhattacharyya Veto, a hard physical constraint grounded in the Data Processing Inequality that prevents the classifier from attributing topologically impossible output distributions to noise. Validated on IBM's 156-qubit Heron r2 processor (ibm_fez) across 105 circuits spanning 17 algorithm families, the framework achieves 89.5% effective accuracy (+/- 5.9% CI). The system implements a safe failure mode, flagging 14.3% of ambiguous cases for manual review rather than forcing low-confidence predictions. We resolve key ambiguities -- such as distinguishing correct Grover amplification from bug-induced collapse -- and identify fundamental limits of single-basis diagnostics, including a Z-basis blind spot where phase-flip errors remain statistically invisible. This work establishes a robust, interpretable diagnostic layer that prevents error mitigation techniques from being applied to logically flawed circuits.

</details>


### [87] [Random Acceleration Noise on Stern-Gerlach Interferometry in a Harmonic Trap](https://arxiv.org/abs/2602.21288)
*Sneha Narasimha Moorthy,Andrew Geraci,Sougato Bose,Anupam Mazumdar*

Main category: quant-ph

TL;DR: 分析纳米金刚石NV中心单环Stern-Gerlach物质波干涉仪在随机加速度噪声下的退相干，量化加速度大小和方向波动对退相位的影响，确定噪声容限和优化操作区间。


<details>
  <summary>Details</summary>
Motivation: 研究纳米金刚石物质波干涉仪在实际环境噪声下的退相干机制，为量子技术应用提供噪声容限分析和优化策略。

Method: 采用有效谐振子动力学模型，将外部加速度噪声建模为随机过程，通过Wiener-Khinchin定理计算退相位率，分析白噪声下的噪声谱密度容限。

Result: 确定加速度噪声谱密度需小于10⁻¹¹ m·s⁻²·Hz⁻¹/²，角度噪声谱密度需小于10⁻¹⁰ rad·Hz⁻¹/²；发现可通过调节叠加方向或加速度大小来最小化噪声影响的操作区间。

Conclusion: 纳米金刚石物质波干涉仪对加速度噪声敏感，但存在优化操作区间可降低噪声影响，为实验实现提供了具体的噪声容限指导。

Abstract: We analyze decoherence in a one-loop Stern--Gerlach--type matter-wave interferometer for a massive nanoparticle embedded with a nitrogen vacancy (NV)-centred nanodiamond evolving under an effective harmonic-oscillator dynamics in a magnetic-field gradient. We assume that the Stern-Gerlach interferometer is subjected to a random acceleration noise external to the system. This could be along the direction of the superposition at an angle which can be varied. We quantify dephasing from two noise channels: fluctuations in the external acceleration $a(t)$ magnitude and direction as specified by the tilt angle $θ_0(t)$ between the superposition axis and the acceleration. At the level of the action, we treat these two external noise as stochastic inputs, and compute the resulting stochastic arm-phase difference, and obtain the dephasing rate $Γ$ using the Wiener--Khinchin theorem. For a white noise and a coherence target $Γτ\leq 1$ and by assuming that we finish the one-loop interferometer within $τ=2π/ω_0\simeq 0.015~\mathrm{s}$, for a reasonable choice of the magnetic field gradient, $η_0=6\times 10^{3}~\mathrm{T\,m^{-1}}$ and mass of the nanodiamond, $m=10^{-15}~\mathrm{kg}$) to create a superposition size of $Δx\sim 1$nm. We find $\sqrt{\mathcal{S}_{aa}}\lesssim \mathcal{O}(10^{-11})~\mathrm{m\,s^{-2}\,Hz^{-1/2}}$ even if we take the external acceleration, $a=0~{\rm ms^{-2}}$ and $θ_0=0^\circ$ (along the dirction of the superposition), and $\sqrt{\mathcal{S}_{θθ}}\lesssim \mathcal{O}(10^{-10})~\mathrm{rad\,Hz^{-1/2}}$ for $a=g= 9.81~\mathrm{m\,s^{-2}}$ and $θ_0=0^\circ$ (superposition direction is perpendicular to the Earth's gravity). We have also found an operating regime where the acceleration noise can be minimized by either varying $θ_0$ or $a$ for a fixed set of other experimental parameters.

</details>


### [88] [Teleportation transition of surface codes on a superconducting quantum processor](https://arxiv.org/abs/2602.21293)
*Yiren Zou,Hong-Kuan Xia,Aosai Zhang,Xuhao Zhu,Feitong Jin,Qingyuan Wang,Yu Gao,Chuanyu Zhang,Ning Wang,Zhengyi Cui,Fanhao Shen,Zehang Bao,Zitian Zhu,Jiarun Zhong,Gongyu Liu,Jia-Nan Yang,Yihang Han,Yiyang He,Jiayuan Shen,Han Wang,Yanzhe Wang,Jiahua Huang,Xinrong Zhang,Sailang Zhou,Hang Dong,Jinfeng Deng,Yaozu Wu,Zixuan Song,Hekang Li,Zhen Wang,Chao Song,Qiujiang Guo,Pengfei Zhang,Guo-Yi Zhu,H. Wang*

Main category: quant-ph

TL;DR: 在125量子比特超导量子处理器上，实现了拓扑旋转表面码的鲁棒量子隐形传态，码距达7，通过调节局域纠缠门获得隐形传态相图，并利用相干量子比特旋转注入魔幻资源提升纠缠阈值。


<details>
  <summary>Details</summary>
Motivation: 拓扑表面码是利用长程纠缠保护逻辑量子信息的主要候选方案，逻辑态的隐形传态对于鲁棒的量子信息处理至关重要。然而，在量子隐形传态中扩展表面码对实验提出了严峻挑战。

Method: 在125量子比特超导量子处理器上，通过线性深度幺正电路制备拓扑旋转表面码，通过均匀调节局域纠缠门获得隐形传态相图，并利用相干量子比特旋转注入超越Clifford体系的魔幻资源。

Result: 成功实现了码距高达7的拓扑旋转表面码的鲁棒隐形传态，获得了隐形传态相图，发现通过注入魔幻资源可以提升纠缠阈值，恢复拓扑相的对偶对称性，从而最小化纠缠资源需求。

Conclusion: 该研究为在量子设备上模拟和利用拓扑量子物质提供了新见解，并为实现分布式容错量子计算的最终目标铺平了道路。

Abstract: The topological surface code is a leading candidate for harnessing long-range entanglement to protect logical quantum information against errors, and teleportation of logical states is desirable for robust quantum information processing. Nevertheless, scaling up the surface code in quantum teleportation poses a formidable challenge to experiment. Here on a superconducting quantum processor with 125 qubits, we demonstrate the robust teleportation of topological rotated surface code prepared by a linear-depth unitary circuit, with code distances up to 7. We obtain the teleportation phase diagram by tuning the local entangling gates uniformly across a finite threshold. Furthermore, we show that the entangling threshold can be boosted by coherent qubit rotations that inject magic resources beyond the Clifford regime, restoring the duality symmetry of the topological phase, which serves as a guiding principle to minimize the entanglement resource. Our results shed light on simulating and leveraging topological quantum matter on quantum devices, and pave the way to the ultimate goal of distributed fault tolerant quantum computation.

</details>


### [89] [Optical repumping and atom number balancing in a two-color MOT](https://arxiv.org/abs/2602.21306)
*Shubha Deutschle,Lőrinc Sárkány,Milán János Negyedi,József Fortágh,Andreas Günther,Philippe Wilhelm Courteille*

Main category: quant-ph

TL;DR: 研究了一种用于蓝色磁光阱中锶-88原子的新型再泵浦方案，通过绿色磁光阱配置同时实现再泵浦、冷却和囚禁，显著提升原子数并实现连续原子束生成。


<details>
  <summary>Details</summary>
Motivation: 传统再泵浦方案效率有限，需要探索新的再泵浦机制来提升蓝色磁光阱中的原子数，同时实现低温操作和连续原子束生成。

Method: 采用绿色磁光阱配置作为再泵浦方案，将再泵浦激光束排列形成绿色磁光阱，同时实现再泵浦、冷却和囚禁功能，防止原子损失。

Result: 绿色磁光阱配置比仅作为再泵浦时能多囚禁10倍原子；通过实验参数可平衡双色磁光阱中的原子数；再泵浦效率比传统方案低三个数量级但仍足够回收所有原子。

Conclusion: 该方案能够实现低温操作并适合连续原子束生成，为锶原子实验提供了新的有效再泵浦和囚禁方法。

Abstract: We study a novel repumping transition for $^{88}$Sr atoms trapped in a 'blue' magneto-optical trap. We show that, while the repumping efficiency is about three orders of magnitude smaller than for traditional schemes, it is sufficient for recycling all atoms, provided the repumping laser beams are arranged to form a 'green' magneto- optical trap (MOT) helping to cool and confine the atoms and preventing their loss. Our main findings are: (i) that the green MOT configuration is able to trap 10 times more atoms in the blue MOT than using the green transition merely as a repump, and (ii) that the atom numbers in the two-color MOT can be balanced through experimental control parameters. The interest of this scheme lies in its capability of reaching low temperature and its suitability for continuous atomic beam generation.

</details>


### [90] [On fully entangled fraction of arbitrary $d\otimes d$ quantum states](https://arxiv.org/abs/2602.21471)
*Xue-Na Zhu,Gui Bao,Ming Li,Ming-Jing Zhao,Shao-Ming Fei*

Main category: quant-ph

TL;DR: 基于密度矩阵的Bloch表示研究量子态的全纠缠分数，获得任意d⊗d二分系统的解析上界，并推导了几类d⊗d量子态的全纠缠分数。


<details>
  <summary>Details</summary>
Motivation: 研究量子态的全纠缠分数对于量子信息处理至关重要，特别是基于密度矩阵的Bloch表示方法，这有助于更好地理解和量化量子纠缠。

Method: 利用密度矩阵的Bloch表示方法，分析d⊗d二分量子系统的全纠缠分数，推导解析上界，并对特定类别的量子态进行解析推导。

Result: 获得了任意d⊗d二分系统全纠缠分数的解析上界，并推导了几类d⊗d量子态的全纠缠分数解析表达式，通过详细示例展示了结果的优越性。

Conclusion: 基于Bloch表示的方法为量子态全纠缠分数的研究提供了有效的解析工具，所得结果对量子信息处理中的纠缠量化具有重要意义。

Abstract: We study the fully entangled fraction of quantum states based on the Bloch representation of density matrices. Analytical upper bounds on the fully entangled fraction are obtained for arbitrary $d\otimes d$ bipartite systems. The fully entangled fractions for classes of $d\otimes d$ quantum states are analytically derived. Detailed examples are given to illustrate the advantages of our results.

</details>


### [91] [Nonlinearity-Inhomogeneity Competition in Discrete-Time Quantum Walks](https://arxiv.org/abs/2602.21474)
*N. Amaral,A. R. C. Buarque,W. S. Dias*

Main category: quant-ph

TL;DR: 研究一维晶格上离散时间量子行走中非线性与不均匀性的相互作用，发现空间不均匀性减弱非线性自陷效应，而时间不均匀性促进离域化


<details>
  <summary>Details</summary>
Motivation: 探索非线性效应（通过Kerr-like强度依赖局域相位引入）与空间/时间不均匀性（通过量子门操作的随机变化实现）在离散时间量子行走中的相互作用机制

Method: 在一维晶格上建立离散时间量子行走模型，引入非线性参数χ和量子门参数θ，分析返回概率和参与函数等典型量，通过χ-θ相图展示不同量子行走区域

Result: 空间不均匀性减弱非线性自陷效应并缩小强局域化区域，产生部分局域化区域（局域核心与色散波包共存）；时间不均匀性破坏相位相干性，增强色散发射并促进离域化

Conclusion: 不均匀性显著改变离散时间量子行走中的非线性动力学行为，空间和时间不均匀性以不同方式影响局域化稳定性，揭示了非线性与不均匀性之间的竞争机制

Abstract: We investigate the interplay between nonlinearity and inhomogeneities in discrete-time quantum walks on one-dimensional lattices. Nonlinear effects are introduced through a Kerr-like, intensity-dependent local phase, while spatial and temporal inhomogeneities are implemented via random variations of the quantum gate operations. By analyzing typical quantities, such as the return probability and the participation function, we identify distinct quantum walking regimes as the nonlinear parameter $χ$ and the quantum gate parameter $θ$ are varied. Spatial inhomogeneities weaken nonlinear self-trapping and constrict the region of robust localization. In this process, partially localized regimes emerge, characterized by the coexistence of a confined core and dispersive wave-packet components. In contrast, temporal inhomogeneities act as time-dependent perturbations that continuously disrupt the phase coherence required for self-trapped excitation, thereby enhancing dispersive emission and promoting delocalization. By using $χ$ versus $θ$ diagrams, we display a comprehensive characterization of how inhomogeneities modify the stability and extent of prevailing dynamical regimes, elucidating the competition between nonlinearity and inhomogeneities in discrete-time quantum walks.

</details>


### [92] [Passive Synchronization of Nonlocal Franson Interferometry for Fiber-Based Quantum Networks Using Co-propagating Classical Clock Signals](https://arxiv.org/abs/2602.21483)
*Xiao Xiang,Runai Quan,Yuting Liu,Huibo Hong,Bingke Shi,Zhiguang Xia,Xinghua Li,Tao Liu,Shougang Zhang,Ruifang Dong*

Main category: quant-ph

TL;DR: 在单模光纤中同时传输经典时钟信号和能量-时间纠缠光子对，实现高可见度的非局域Franson干涉，为城域量子网络提供实用同步方案


<details>
  <summary>Details</summary>
Motivation: 为城域规模的基于纠缠的量子网络提供实用、可扩展的同步解决方案，避免依赖外部专用定时基础设施

Method: 在同一光纤中共同传播经典射频光纤时钟信号和能量-时间纠缠光子对，采用交叉波段分配（O波段用于经典信号，L波段用于量子信号），有效抑制自发拉曼散射噪声光子，同时保持环境延迟波动高度相关以实现共模噪声消除

Result: 在50公里单模光纤上实现了可见度为(88.35±3.62)%的非局域量子干涉，实现了皮秒精度的被动同步

Conclusion: 该工作为城域规模的基于纠缠的量子网络提供了一种实用、可扩展的同步解决方案，无需依赖外部专用定时基础设施

Abstract: We demonstrate a robust, high-visibility nonlocal Franson interferometry for fiber-based quantum networks by co-propagating a classical Radio-over-Fiber clock signal with energy-time entangled photon pairs in the same fiber. Utilizing cross-band allocation (O-band for classical, L-band for quantum signals), the spontaneous Raman scattering noise photons are effectively suppressed. At the same time, their environmental delay fluctuations remain highly correlated for common-mode noise cancellation, achieving a passive synchronization with picoseconds precision. Over 50 km of single-mode fiber, this co-propagation enables nonlocal quantum interference with a visibility of (88.35\pm3.62)%, without relying on external dedicated timing infrastructure. This work provides a practical, scalable synchronization solution for metropolitan-scale entanglement-based quantum networks.

</details>


### [93] [Universal Sample Complexity Bounds in Quantum Learning Theory via Fisher Information matrix](https://arxiv.org/abs/2602.21510)
*Hyukgun Kwon,Seok Hyung Lie,Liang Jiang*

Main category: quant-ph

TL;DR: 量子学习理论中样本复杂度由逆Fisher信息矩阵决定，最大似然估计下参数估计的样本复杂度上下界由逆Fisher信息矩阵的对角元素表征。


<details>
  <summary>Details</summary>
Motivation: 建立量子学习理论中样本复杂度的系统性理论框架，揭示样本复杂度与逆Fisher信息矩阵之间的根本联系，为量子参数估计提供理论基础。

Method: 在一般参数框架下推导最大似然估计的样本复杂度上下界，上下界分别由逆Fisher信息矩阵的最大对角元素和任意对角元素表征。将理论应用于Pauli信道学习和Pauli期望值估计，并扩展到欧几里得距离误差准则。

Result: 建立了样本复杂度与逆Fisher信息矩阵的精确数学关系，为Pauli信道学习和Pauli期望值估计提供了简化的推导，揭示了无纠缠Pauli信道学习和无量子记忆Pauli期望值估计中指数样本复杂度的结构根源。

Conclusion: 提出了量子学习理论的系统性框架，证明了在小误差区域下学习样本复杂度由逆Fisher信息矩阵决定，为量子参数估计任务提供了任务无关的样本复杂度分析工具。

Abstract: In this work, we show that the sample complexity (equivalently, the number of measurements) required in quantum learning theory within a general parametric framework, is fundamentally governed by the inverse Fisher information matrix. More specifically, we derive upper and lower bounds on the number of samples required to estimate the parameters of a quantum system within a prescribed small additive error and with high success probability under maximum likelihood estimation. The upper bound is governed by the supremum of the largest diagonal entry of the inverse Fisher information matrix, while the lower bound is characterized by any diagonal element evaluated at arbitrary parameter values. We then apply the general bounds to Pauli channel learning and to the estimation of Pauli expectation values in the asymptotic small-error regime, and recover the previously established sample complexity through considerably streamlined derivations. Furthermore, we identify the structural origin of exponential sample complexity in Pauli channel learning without entanglement and in Pauli expectation value estimation without quantum memory. We then extend the analysis to an error criterion based on the Euclidean distance between the true parameter values and their estimators. We derive the corresponding upper and lower bounds on the sample complexity, which are likewise characterized by the inverse Fisher information matrix. As an application, we consider Pauli expectation estimation with entangled probes. Finally, we highlight two fundamental contributions to quantum learning theory. First, we establish a systematic framework that determines the task-independent sample complexity under maximum-likelihood estimation. Second, we show that, in the small-error regime, learning sample complexity is governed by the inverse Fisher information matrix.

</details>


### [94] [Momentum Diffusion, Decoherence and Drag Force on a Magnetic Nanoparticle](https://arxiv.org/abs/2602.21518)
*Agya Sewara Alam,Anupam Mazumdar*

Main category: quant-ph

TL;DR: 本文推导了热背景中电磁场涨落下磁性纳米粒子量子叠加态的退相干率，并扩展到两个相邻的顺磁性纳米粒子，同时分析了外部电磁场涨落引起的拖曳力。


<details>
  <summary>Details</summary>
Motivation: 研究磁性纳米粒子在热背景电磁场涨落中的量子退相干机制，这对于量子技术中纳米尺度系统的相干性保持至关重要。同时探索相邻纳米粒子间的相互作用以及外部涨落引起的拖曳效应。

Method: 使用长波极限下的涨落-耗散定理，假设叠加尺寸远小于电磁场涨落的波长。将单粒子计算扩展到两个相邻的顺磁性纳米粒子，并分析外部电磁场涨落引起的拖曳力。

Result: 推导出磁性纳米粒子在热背景电磁场涨落中的退相干率完整表达式，获得两个相邻顺磁性纳米粒子的相互作用结果，并展示了外部电磁场涨落如何产生拖曳力。

Conclusion: 建立了磁性纳米粒子在热电磁环境中的退相干理论框架，为纳米尺度量子系统的相干性控制和实验设计提供了理论基础，同时揭示了相邻纳米粒子间相互作用和外部涨落效应的重要性。

Abstract: In this paper, we will provide a complete derivation of the decoherence rate for a magnetic nanoparticle in quantum superposition in the presence of the fluctuating electromagnetic field in a thermal background by using the fluctuation-dissipation theorem in the long-wavelength limit. The long-wavelength limit assumes that the superposition size is much smaller than the wavelength of the electromagentic filed fluctuations. We will extend this computation to two diamagnetic nanoparticles kept in quantum superposition adjacent to each other. We will also show how the drag force on a single nanoparticle arises from external electromagnetic-field fluctuations, and compare our results with those for the nanoparticle's dielectric properties.

</details>


### [95] [Efficient time-series prediction on NISQ devices via time-delayed quantum extreme learning machine](https://arxiv.org/abs/2602.21544)
*Mio Kawanabe,Saud Cindrak,Kathy Luedge,Jun-ichi Shirakashi,Tetsuo Shibuya,Hiroshi Imai*

Main category: quant-ph

TL;DR: 提出时间延迟量子极限学习机（TD-QELM），用于在NISQ设备上高效进行时间序列预测，通过同时编码多个过去输入实现浅层电路深度，在噪声鲁棒性和预测精度上优于传统量子储层计算。


<details>
  <summary>Details</summary>
Motivation: 在噪声中等规模量子（NISQ）设备上实现高效的时间序列预测面临挑战，传统量子储层计算存在电路深度随序列长度增加、噪声累积严重的问题，需要开发更实用、可扩展的量子学习框架。

Method: 提出时间延迟量子极限学习机（TD-QELM），通过同时编码多个过去输入到量子电路中，实现浅层电路深度（与序列长度无关），减少噪声累积和计算复杂度，在NISQ设备上实现高效时间序列预测。

Result: 在无噪声仿真和IBM 127量子比特处理器上使用NARMA基准测试，TD-QELM在预测精度和噪声鲁棒性方面持续优于传统量子储层计算，证明了其在当前NISQ硬件上的实用性和可扩展性。

Conclusion: TD-QELM为当前NISQ硬件上的时间序列学习提供了一个实用且可扩展的框架，通过浅层电路设计有效缓解了噪声累积问题，在量子机器学习应用中具有重要潜力。

Abstract: We proposed a time-delayed quantum extreme learning machine (TD-QELM) for efficient time-series prediction on noisy intermediate-scale quantum (NISQ) devices. By encoding multiple past inputs simultaneously, TD-QELM achieves shallow circuit depth independent of sequence length, thereby, mitigating noise accumulation and reducing computational complexity. Experiments using the NARMA benchmark on both noiseless simulations and IBM's 127-qubit processor demonstrate that TD-QELM consistently outperforms conventional quantum reservoir computing in prediction accuracy and noise robustness. These results highlight TD-QELM as a practical and scalable framework for time-series learning on current NISQ hardware.

</details>


### [96] [Passive Environment-Assisted Quantum Communication](https://arxiv.org/abs/2602.21549)
*Evelyn Voss,Bikun Li,Zhaoyou Wang,Liang Jiang*

Main category: quant-ph

TL;DR: 该研究探讨了如何通过被动环境辅助增强量子通信效率，特别是在传输率低于50%的纯损耗信道中。虽然理想的GKP态可实现完美传输，但实验上难以实现，因此研究了更易实现的非高斯辅助态（如Fock态、猫态和压缩猫态），并提出了优化的编码解码方案。


<details>
  <summary>Details</summary>
Motivation: 随着量子信息系统的发展，通过噪声信道高效、相干地传输量子信息变得日益重要。纯损耗信道在传输率低于50%时量子容量为零，需要寻找增强量子通信的方法。

Method: 通过被动环境辅助量子通信，在辅助端口选择适当的输入态来增强信道性能。研究了实验上更易实现的非高斯辅助态（Fock态、猫态、压缩猫态），数值优化编码解码策略，并构建了高保真传输的解析方案。

Result: 虽然理想GKP态可在任意小传输率下实现完美量子信息传输，但实验上难以实现。研究发现了更易实现的非高斯态方案，能够实现高保真传输和良好的信息速率。

Conclusion: 被动环境辅助量子通信可显著增强纯损耗信道的性能，即使传输率低于50%。虽然理想GKP态最优但实验困难，而更易实现的非高斯辅助态方案提供了实用的高保真量子通信途径。

Abstract: As quantum information systems mature, efficient and coherent transfer of quantum information through noisy channels becomes increasingly important. We examine how passive environment-assisted quantum communication enhances direct quantum information transfer efficiency. A bosonic pure-loss channel, modeled as transmission through a beam splitter with a vacuum input state at the dark port, has zero quantum capacity when transmissivity is below 50%. Quantum communication through the channel can be enhanced by passive environment assistance, achieved via the selection of an appropriate input state for the ancilla port. Although ideal Gottesman-Kitaev-Preskill (GKP) states enable perfect quantum information transmission at arbitrarily small transmissivity, they are challenging to realize experimentally. We therefore explore more experimentally accessible non-Gaussian ancilla states, such as Fock, cat, and squeezed cat states, and numerically determine the optimal encoding and decoding strategies. We also construct analytical schemes that yield high-fidelity transmission and good information rates.

</details>


### [97] [Performance Comparison of QAOA Mixers for Ternary Portfolio Optimization](https://arxiv.org/abs/2602.21562)
*Shintaro Yamamura,Satoshi Watanabe,Masaya Kunimi,Kazuhiro Saito,Tetsuro Nikuni*

Main category: quant-ph

TL;DR: QAOA应用于投资组合优化问题，比较标准混合器和多种XY混合器在无噪声和有噪声环境下的性能表现


<details>
  <summary>Details</summary>
Motivation: 投资组合优化是金融工程中的核心挑战，量子近似优化算法(QAOA)作为NISQ设备上的有前景方法，需要研究其在真实噪声环境下的表现

Method: 将投资组合优化建模为三元变量问题（持有、不持有、卖空），使用标准混合器和四种XY混合器(XY Ring、XY Parity Ring、XY Full、QAMPA)，基于德国DAX 30指数的真实数据进行5和8资产组合的模拟，并引入去极化通道噪声

Result: XY混合器在无噪声环境下表现优越，但在噪声环境中优势减弱；最优混合器的选择取决于QAOA深度和噪声强度

Conclusion: 混合器的选择需要根据具体应用场景（噪声水平和算法深度）进行权衡，为实际量子金融应用提供了重要指导

Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is a quantum algorithm proposed for Noisy Intermediate-Scale Quantum (NISQ) devices and is regarded as a promising approach to combinatorial optimization problems, with potential applications in the financial sector. In this study, we apply QAOA to the portfolio optimization problem, which is one of the central challenges in financial engineering. A portfolio consists of a combination of multiple assets, and the portfolio optimization problem aims to determine the optimal asset allocation by balancing expected return and risk. In the context of quantum optimization, portfolio optimization is often formulated using discrete variables. Unlike conventional binary formulations, we consider a ternary portfolio optimization problem that accounts for three states-holding, not holding, and short selling-and compare its performance using different mixer operators. Specifically, we implement QAOA with the standard mixer and several XY Mixers (XY Ring, XY Parity Ring, XY Full, and QAMPA), and conducted simulations using real data based on the German stock index (DAX 30) for portfolios consisting of 5 and 8 assets. Furthermore, we introduce noise based on a depolarizing channel to investigate the behavior of the algorithm in realistic environments. The results show that while XY Mixers exhibit superiority in noiseless settings, their advantage degrades in noisy environments, and the optimal choice of mixer depends on both the number of QAOA depths and the noise strength.

</details>


### [98] [Entanglement recovery by reversing the effect of noise in quantum repeater](https://arxiv.org/abs/2602.21563)
*Sewon Jeong,Shrobona Bagchi,Jaehak Lee,Hyang-Tag Lim,Yong-Su Kim,Taeyoung Choi,Seung-Woo Lee*

Main category: quant-ph

TL;DR: 提出一种在噪声环境下直接恢复纠缠交换分布纠缠度的方法，通过引入反转操作概率性地抵消振幅阻尼或光子损失的影响，实现纠缠的预示性恢复。


<details>
  <summary>Details</summary>
Motivation: 在量子中继器中，纠缠交换过程中噪声（如振幅阻尼、光子损失）会导致纠缠突然死亡，使得分布的纠缠度消失。需要一种能够在噪声环境下直接恢复纠缠的方法。

Method: 引入反转操作来概率性地抵消振幅阻尼或光子损失对单个纠缠对的影响，实现纠缠的预示性恢复。分析了两类代表性中继器模型（双向和单向架构）中的协议有效性，并确定了最优反转策略。

Result: 即使在强噪声环境下，包括纠缠突然死亡的参数区域，纠缠也能被显著恢复。该方法与纠缠纯化和蒸馏等其他恢复技术兼容。

Conclusion: 该方法为当前和近期的量子中继器架构提供了实用且实验可行的鲁棒纠缠分布方案。

Abstract: We propose a method to directly recover the degree of entanglement distributed by entanglement swapping in the presence of noise. Our approach introduces a reversing operation that probabilistically undoes the effect of amplitude damping or photon loss on a single entangled pair, enabling heralded recovery of entanglement. We demonstrate that entanglement can be substantially recovered even under strong noise, including parameter regimes where the distributed entanglement would otherwise vanish due to entanglement sudden death. We analyze the effectiveness of the protocol in two representative repeater models, i.e.,~two-way and one-way architectures and identify the optimal reversing strategy. Due to its heralded and single-copy nature, our protocol is readily compatible with other entanglement recovery techniques such as entanglement purification and distillation. Our work provides a practical and experimentally feasible way toward robust entanglement distribution in current and near-term quantum repeater architectures.

</details>


### [99] [Tuning Wave-Particle Duality of Quantum Light by Generalized Photon Subtraction](https://arxiv.org/abs/2602.21629)
*Kan Takase,Mamoru Endo,Fumiya Hanamura,Kazuki Hirota,Masahiro Yabuno,Hirotaka Terai,Shigehito Miki,Takahiro Kashiwazaki,Asuka Inoue,Takeshi Umeki,Petr Marek,Radim Filip,Warit Asavanant,Akira Furusawa*

Main category: quant-ph

TL;DR: 实验演示了通过广义光子减除技术可调谐生成介于波-粒二象性之间的量子态，为光学量子计算提供高效非高斯资源


<details>
  <summary>Details</summary>
Motivation: 波粒二象性是量子力学核心特征，介于波状薛定谔猫态和粒子状福克态之间的中间态是增强容错量子计算的重要资源，但如何高效生成这些态是光学量子计算的关键瓶颈

Method: 采用广义光子减除技术，使用光子数分辨探测器从压缩光源中检测最多三个光子，连续控制波状和粒子状特征的平衡

Result: 成功实现了可调谐生成中间态，构建了具有高生成率的量子态谱系，可根据容错阈值要求进行优化

Conclusion: 广义光子减除技术成为定制非高斯资源的通用工具箱，为高效生成GKP量子比特开辟了途径，解决了光学量子计算的核心瓶颈问题

Abstract: Wave--particle duality is a hallmark of quantum mechanics. For bosonic systems, there exists a continuum of intermediate states bridging wave-like Schrödinger cat states and particle-like Fock states. Such states have recently been recognized as valuable resources for enhancing fault-tolerant quantum computation (FTQC) with propagating light. Here we experimentally demonstrate tunable generation of these intermediate states by employing generalized photon subtraction (GPS). By detecting up to three photons from squeezed-light sources with a photon-number-resolving detector, we continuously control the balance between wave- and particle-like features. This approach allows us to construct a spectral family of quantum states with high generation rates, optimized according to the required fault-tolerance threshold. Our results establish GPS as a versatile toolbox for tailoring non-Gaussian resources, opening a pathway to efficient Gottesman--Kitaev--Preskill (GKP) qubit generation and addressing a central bottleneck in optical quantum computing.

</details>


### [100] [Secret Key Rate Limits in Coexisting Classical-Quantum Optical Links](https://arxiv.org/abs/2602.21671)
*Lucas Alves Zischler,Amirhossein Ghazisaeidi,Antonio Mecozzi,Cristian Antonelli*

Main category: quant-ph

TL;DR: 该研究推导了经典-量子共存系统中量子信道受经典信号干扰的闭式表达式，发现将量子信道置于E/S波段比传统O波段能获得更高的密钥率。


<details>
  <summary>Details</summary>
Motivation: 经典-量子共存系统虽然成本效益高，但量子密钥分发(QKD)信号易受经典流量产生的非线性干扰（如自发拉曼散射和四波混频）影响，导致信道损耗和额外噪声增加，降低密钥率。

Method: 推导了评估量子频率信道中经典信号累积干扰功率的闭式表达式模型，该模型能有效设计单模光纤中的经典-量子系统，捕捉相关物理现象产生的干扰演化。

Result: 利用模型分析多频带传输系统中的频率分配，发现与传统将QKD信道分配在O波段的做法相反，将量子信道置于上E波段/下S波段能在相关场景中获得更高的密钥率。

Conclusion: 该研究为经典-量子共存系统提供了有效的干扰评估模型，并提出了优化的频率分配策略，通过将量子信道置于E/S波段而非传统O波段，可显著提高密钥率。

Abstract: Classical-quantum coexistence enables cost-effective transmission of data and quantum signals over the same fiber-optic channel. Nevertheless, weak quantum-key distribution (QKD) signals are susceptible to non-linear interference generated from the classical traffic, primarily spontaneous Raman scattering (SpRS) and four-wave-mixing (FWM), as well as to unfiltered noise. In QKD protocols, increased channel loss and excess noise both reduce the secret key rates (SKRs), as illustrated in this work for the two-state BB84 and Gaussian-modulated coherent-states (GMCS) protocols. In this study, we derive closed-form expressions for evaluating the accumulated interference power from coexisting classical signals in a quantum frequency channel. Our model enables effective design of classical-quantum systems in single-mode fibers (SMFs), capturing the evolution of interference arising from the relevant physical phenomena. We utilize the model to examine frequency allocation in multiband transmission systems, demonstrating that, contrary to common practice of allocating QKD channels in the O-band, increased SKR is achieved by placing quantum channels in the upper E-/lower S-band across the relevant scenarios.

</details>


### [101] [Revealing entanglement through local features of phase-space distributions](https://arxiv.org/abs/2602.21688)
*Elena Callus,Martin Gärttner,Tobias Haas*

Main category: quant-ph

TL;DR: 提出基于相空间准概率分布及其导数的无限层次连续变量可分性判据，等价于Peres-Horodecki判据，揭示相空间中可蒸馏纠缠的表现


<details>
  <summary>Details</summary>
Motivation: 现有相空间纠缠检测方法通常需要完整的相空间分布信息，本文旨在开发更简单、更高效的检测方法，特别是针对难以检测的非高斯纠缠

Method: 构建基于相空间准概率分布及其导数的无限层次可分性判据，设计仅需被动线性变换和相干辅助的简单测量方案，通过策略性探测特定相空间区域实现纠缠检测

Result: 最低阶判据已能有效检测相关态族的非高斯纠缠，测量方案相比依赖完整相空间分布的方法具有明显优势

Conclusion: 提出的相空间可分性判据不仅等价于标准纠缠判据，还提供了检测非高斯纠缠的实用工具，为相空间纠缠表征开辟了新途径

Abstract: We formulate an infinite hierarchy of continuous-variable separability criteria in terms of quasiprobability distributions and their derivatives evaluated at individual points in phase space. Our approach is equivalent to the Peres--Horodecki criterion and sheds light on how distillable entanglement manifests in the phase-space picture. We demonstrate that already the lowest-order variant constitutes a powerful method for detecting the elusive non-Gaussian entanglement of relevant state families. Further, we devise a simple measurement scheme that relies solely on passive linear transformations and coherent ancillas. By strategically probing specific phase-space regions, our method offers clear advantages over existing techniques that rely on access to the full phase-space distributions.

</details>


### [102] [Landscape-Similarity-Guided Optimization in QAOA](https://arxiv.org/abs/2602.21689)
*Sokea Sang,Leanghok Hour,Sanghyeon Lee,Aniket Patra,Hee Chul Park,Moon Jip Park,Youngsun Han*

Main category: quant-ph

TL;DR: 论文提出DO-QAOA方法，通过利用变量冻结后QAOA变分景观的普适性结构，将指数级增长的子实例压缩为常数个有效景观类，降低运行时和量子测量开销。


<details>
  <summary>Details</summary>
Motivation: 针对量子近似优化算法(QAOA)在实际硬件约束下的可扩展性问题，需要降低运行时和量子测量开销，同时保持有竞争力的近似比。

Method: 引入双重优化QAOA(DO-QAOA)，利用变量冻结后变分景观的普适性结构，通过景观重叠序参数q量化能量景观间的几何相关性，将指数级增长的子实例压缩为常数个有效景观类。

Result: DO-QAOA在保持竞争性近似比的同时，显著降低了运行时和量子测量开销，通过景观相似性转变分析揭示了图连接性对景观结构的影响。

Conclusion: DO-QAOA为现实硬件约束下的混合量子-经典优化提供了可扩展路径，该方法可推广到其他变分量子算法中。

Abstract: Across diverse synthetic and real-world interaction graphs, the variational landscapes of reduced Quantum Approximate Optimization Algorithm (QAOA) instances obtained via variable freezing exhibit a robust universality. Leveraging this structure, we introduce Doubly Optimized QAOA (DO-QAOA), which lowers runtime and quantum measurement overhead while maintaining a competitive approximation ratio gap (ARG). Adapting the replica-overlap framework of spin-glass physics, we define a landscape-overlap order parameter $q$ to quantify geometric correlations between energy landscapes, revealing a sharp landscape-similarity transition as graph connectivity is tuned. Notwithstanding this transition, the dominant convex features of nearly all conditioned sub-instances remain aligned across both phases. Exploiting this persistence, DO-QAOA collapses the nominal $2^m$ reduced instances generated by freezing $m$ qubits into $K = O(1)$ effective landscape classes, eliminating the exponential proliferation in $m$. By leveraging landscape structure, DO-QAOA provides a scalable route to hybrid quantum-classical optimization under realistic hardware constraints, with potential applicability across variational quantum algorithms.

</details>


### [103] [Imperfect Graphs from Unitary Matrices -- I](https://arxiv.org/abs/2602.21808)
*Wesley Lewis,Darsh Pareek,Umesh Kumar,Ravi Janjam*

Main category: quant-ph

TL;DR: 提出一种将量子算符映射为有向图的图论框架（TSS），忽略振幅和相位信息，专注于量子算符的拓扑结构和信息流分析。


<details>
  <summary>Details</summary>
Motivation: 量子算符的矩阵表示虽然计算完备，但往往掩盖了量子电路中信息流的结构拓扑。需要一种新的框架来直观地分析量子算符的拓扑特性。

Method: 引入广义图论框架，将酉矩阵映射为有向图（称为TSS）。计算基态表示为顶点，如果两个基态之间存在非零振幅转移，则存在有向边。该方法故意丢弃概率振幅和相位信息，专注于连通性和可达性分析。

Result: 展示了TSS如何直观地描述Hadamard门、Pauli门等量子门。该框架为将量子电路视为离散动力系统提供了新的视角。

Conclusion: TSS框架提供了一种分析量子算符拓扑结构的新工具，有助于设计更好的量子算法，为理解量子电路中的信息流提供了新颖的图论视角。

Abstract: Matrix representations of quantum operators are computationally complete but often obscure the structural topology of information flow within a quantum circuit \cite{nielsen2000}. In this paper, we introduce a generalized graph-theoretic framework for analyzing quantum operators by mapping unitary matrices to directed graphs; we term these structures \emph{Imperfect Graphs} or more formally as \emph{Topological Structure of Superpositions}(TSS) as a tool to devise better Quantum Algorithms. In this framework, we represent computational basis states as vertices. A directed edge exists between two vertices if and only if there is a non-zero amplitude transition between them, effectively mapping the support of the unitary operator. In this paper we deliberately discard probability amplitudes and phase information to isolate the connectivity and reachability properties of the operator. We demonstrate how TSS intuitively helps describe gates such as the Hadamard, Pauli-(X,Y,Z) gates, etc \cite{nielsen2000}. This framework provides a novel perspective for viewing quantum circuits as discrete dynamical systems \cite{childs2009,aharonov2001}
  Keywords: Quantum Algorithms, Unitary Matrix Approach, Topological Structure of Superpositions (TSS), Graph Theory

</details>


### [104] [Generating large-scale Greenberger-Horne-Zeilinger-like states in lattice spin systems](https://arxiv.org/abs/2602.21839)
*Xuanchen Zhang,Yaofeng Chen,Yong-Chun Liu*

Main category: quant-ph

TL;DR: 提出一种通过全局Floquet工程在晶格自旋系统中生成大规模GHZ-like态的通用可扩展方案


<details>
  <summary>Details</summary>
Motivation: GHZ态是典型的极大纠缠态，在基础研究和量子技术中都很重要。在晶格自旋系统中制备大规模GHZ态对量子优势特别有吸引力，但传统方案在可扩展性上面临巨大挑战。

Method: 通过全局Floquet工程在晶格自旋系统中生成大规模GHZ-like态，该方案仅需全局操作，适用于任意相互作用范围的系统。

Result: 提出的方案具有通用性和可扩展性，能够生成与标准GHZ态具有相似纠缠和计量特性的GHZ-like态，对大量粒子数显示出巨大优势。

Conclusion: 该方案为在各种系统中大规模实现多体纠缠态提供了实用途径。

Abstract: Greenberger-Horne-Zeilinger (GHZ) state is a typical maximally entangled state which is pursued in both fundamental research and emerging quantum technologies. Preparing large-scale GHZ states in lattice spin systems is particularly appealing for quantum advantages, but conventional schemes face great challenges in scalability. Here we propose a universal and scalable scheme to generate large-scale GHZ-like states, which share similar entanglement and metrological properties with standard GHZ states, in lattice spin systems through global Floquet engineering. Our scheme requires only global operations and shows great advantage for large particle number. It is applicable to systems with arbitrary interaction ranges, offering a practical pathway for large-scale implementation of many-body entangled states in various systems.

</details>


### [105] [Deep squeezing or cooling the fluctuations of a parametric resonator using feedback](https://arxiv.org/abs/2602.21847)
*Adriano A. Batista*

Main category: quant-ph

TL;DR: 该论文研究通过锁相放大器反馈回路增强单自由度参量谐振器的深亚阈值参量压缩或冷却方法，分析了反馈对系统动力学的影响，包括霍普夫分岔和鞍结分岔现象。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过反馈控制增强参量谐振器的压缩和冷却效果，探索在亚阈值条件下实现更强量子态操控的可能性。

Method: 采用多种理论方法分析：平均法、谐波平衡法、Floquet理论和格林函数法；通过傅里叶分析研究系统对白噪声的响应。

Result: 发现反馈使系统动力学更复杂，在失稳阈值处出现霍普夫分岔；在霍普夫分岔附近可实现解压缩和冷却，在鞍结分岔附近可实现压缩；能实现非常强的压缩或冷却效果。

Conclusion: 锁相放大器反馈回路能显著增强参量谐振器的压缩和冷却能力，为量子态操控提供了新途径，不同分岔点对应不同的量子效应。

Abstract: Here we analyze ways to achieve deep subthreshold parametric squeezing or cooling of a single degree-of-freedom parametric resonator enhanced by a lock-in amplifier feedback loop. Due to the feedback, the dynamics of the parametric resonator becomes more complex and a Hopf bifurcation at the instability threshold can occur. Initially, we calculate the phase-dependent gain of parametric amplification with feedback of an added ac signal. In one approach, we obtain the amplification gain approximately using two independent approaches: the averaging method and the harmonic balance method. We also obtain this gain more exactly using Floquet theory and Green's functions methods. The Hopf bifurcation was predicted by the harmonic balance method and by Floquet theory, but not by the averaging method. In our analysis of fluctuations, we Fourier analyze the response of the parametric resonator with feedback to an added white noise. We were able to calculate, in addition to the noise spectral density, the squeezing of fluctuations in this resonator with feedback. Very strong squeezing or cooling can occur. Deamplification and cooling occur near the Hopf bifurcation, whereas squeezing occurs near a saddle-node bifurcation.

</details>


### [106] [Quantum Error Mitigation Simulates General Non-Hermitian Dynamics](https://arxiv.org/abs/2602.21879)
*Hiroki Kuji,Suguru Endo,Tetsuro Nikuni,Ryusuke Hamazaki,Yuichiro Matsuzaki*

Main category: quant-ph

TL;DR: 提出一种无需连续监测的硬件友好协议，通过经典高斯白噪声平均和随机量子误差缓解来模拟非厄米动力学


<details>
  <summary>Details</summary>
Motivation: 非厄米哈密顿量能实现奇异的动力学现象，但在近期量子设备上实现其非幺正时间演化仍具挑战性，需要一种无需连续监测的硬件友好方法

Method: 通过经典高斯白噪声平均实现GKSL演化，并在可观测量层面使用随机量子误差缓解消除量子跳跃贡献；无需辅助量子比特或受控时间演化，缓解层仅使用单量子比特操作

Result: 通过数值模拟验证了具有非对称跳跃、相互作用和无序的模型；方法有效且可编程

Conclusion: 提供了一个可编程且无需辅助量子比特的框架，利用量子误差缓解研究非完全正定和迹保持的奇异动力学

Abstract: While non-Hermitian Hamiltonians enable exotic dynamical phenomena, implementing their nonunitary time evolution on near-term quantum devices remains challenging. We propose a hardware-friendly protocol that simulates non-Hermitian dynamics without continuous monitoring. Gorini-Kossakowski-Sudarshan-Lindblad (GKSL) evolution via classical Gaussian white-noise averaging and to subsequently cancel the quantum-jump contribution at the level of the measured observable using stochastic quantum error mitigation (QEM). The scheme requires no ancillas or controlled time-evolution, while the mitigation layer uses only single-qubit operations. We validate the method through numerical simulations of a model with asymmetric hopping, interaction, and disorder. Our work provides a programmable and ancilla-free framework investigating exotic dynamics that are not completely-positive and trace-preserving using QEM.

</details>


### [107] [Analysis of the action of conventional trapped-ion entangling gates in qudit space](https://arxiv.org/abs/2602.21886)
*Pavel Kamenskikh,Nikita Semenin,Ilia Zalivako,Vasiliy Smirnov,Ilya Semerikov,Ksenia Khabarova,Nikolay Kolachevsky*

Main category: quant-ph

TL;DR: 该论文研究量子比特(dit)系统中的相位控制问题，提出补偿方法以简化量子门操作并增强鲁棒性


<details>
  <summary>Details</summary>
Motivation: 量子比特(dit)作为多级量子信息载体，在扩展量子计算机规模方面具有潜力，但引入的量子逻辑复杂性增加，需要仔细控制不同量子比特能级之间的相对相位。在囚禁离子系统中，纠缠操作会在特定能级上积累相位，这些相位不再是全局的，与量子比特架构不同。此外，随着希尔伯特空间维度的增加，多级门的结构变得更加复杂。

Method: 探索Mølmer-Sørensen门和光移门中积累的额外纠缠和非纠缠相位的理论，提出主动补偿这些相位的方法，增强门对参数波动的鲁棒性，并简化原生门以实现更高效的电路分解。

Result: 为量子比特(dit)量子处理器的实际和可扩展实现铺平了道路。

Conclusion: 该研究解决了量子比特(dit)系统中的相位控制挑战，提出的补偿和简化方法有助于实现实用且可扩展的量子比特(dit)量子处理器。

Abstract: Qudits, or multi-level quantum information carriers, present a promising path for scaling quantum computers. However, their use introduces increased complexity in quantum logic, necessitating careful control of relative phases between different qudit levels. In trapped-ion systems, entangling operations accumulate phases on specific levels that are no longer global, unlike in qubit architectures. Furthermore, the structure of multi-level gates becomes increasingly intricate with higher-dimensional Hilbert spaces. This work explores the theory of these additional entangling and non-entangling phases, accumulated in Mølmer--Sørensen and Light-shift gates. We propose methods to actively compensate for these phases, enhance gate robustness against parameter fluctuations, and simplify native gates for more efficient circuit decomposition. Our results pave the way toward the practical and scalable implementation of qudit-based quantum processors.

</details>


### [108] [Prodiabatic Elimination: Higher Order Elimination of Fast Variables with Quantum Noise](https://arxiv.org/abs/2602.21896)
*Jan Neuser,Marcelo Janovitch,Matteo Brunelli,Patrick P. Potts*

Main category: quant-ph

TL;DR: 提出"prodiabatic elimination"方法，系统扩展光-物质耦合系统中快速自由度绝热消除技术，通过算子展开包含高阶修正和噪声贡献，显著提升性能同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统绝热消除方法在处理光-物质耦合系统时存在局限性，需要更精确的近似技术来包含高阶修正和噪声贡献，同时保持计算效率。

Method: 通过算子的受控展开，系统性地扩展绝热消除技术，包含高阶修正和噪声贡献，形成prodiabatic elimination方法。

Result: 在驱动耗散Jaynes-Cummings模型和双模腔中执行STIRAP的三能级系统上验证，相比标准绝热消除显著提升性能。

Conclusion: prodiabatic elimination是分析开放量子系统的强大、广泛适用工具，既提高精度又保持计算效率。

Abstract: We introduce the prodiabatic elimination, a powerful approximation technique that systematically extends the adiabatic elimination of fast degrees of freedom in light-matter coupled systems. Through a controlled expansion of operators, the prodiabatic elimination incorporates higher-order corrections and consistently includes noise contributions, leading to a significantly improved performance compared to standard adiabatic elimination. Importantly, it retains the simplicity and computational efficiency of the adiabatic elimination, making it convenient for practical applications. We demonstrate the approach on two setups: a driven dissipative Jaynes-Cummings model and a three-level system in a two-mode cavity that performs stimulated Raman adiabatic passage (STIRAP). These examples establish the prodiabatic elimination as a robust and broadly applicable tool for analyzing open quantum systems.

</details>


### [109] [Noise-adaptive hybrid quantum convolutional neural networks based on depth-stratified feature extraction](https://arxiv.org/abs/2602.21953)
*Taehyun Kim,Israel F. Araujo,Daniel K. Park*

Main category: quant-ph

TL;DR: 提出噪声自适应混合量子卷积神经网络，通过深度分层中间测量和经典后处理提升噪声下的分类性能


<details>
  <summary>Details</summary>
Motivation: 当前分层量子分类器（如QCNN）在近期量子硬件上性能对电路深度噪声高度敏感，需要超越电路架构设计的策略来提升噪声鲁棒性

Method: 提出混合QCNN架构：在池化操作中不丢弃被移除的量子比特，而是测量它们并将结果作为经典特征，与经典神经网络联合处理；采用深度分层中间测量和经典后处理的混合分层设计

Result: 在多种电路规模和噪声设置下（包括基于IBM Quantum后端数据的硬件校准噪声模型），相比标准QCNN表现出更稳定的收敛、更低的损失变异性、一致更高的分类准确率；随着电路规模增大，性能优势显著放大；多基测量变体在现实噪声下接近无噪声极限性能

Conclusion: 混合QCNN通过深度分层特征提取有效缓解了标准架构的扩展限制，该策略可广泛应用于逐步丢弃量子比特的分层量子分类器

Abstract: Hierarchical quantum classifiers, such as quantum convolutional neural networks (QCNNs), represent recent progress toward designing effective and feasible architectures for quantum classification. However, their performance on near-term quantum hardware remains highly sensitive to noise accumulation across circuit depth, calling for strategies beyond circuit-architecture design alone. We propose a noise-adaptive hybrid QCNN that improves classification under noise by exploiting depth-stratified intermediate measurements. Instead of discarding qubits removed during pooling operations, we measure them and use the resulting outcomes as classical features that are jointly processed by a classical neural network. This hybrid hierarchical design enables noise-adaptive inference by integrating quantum intermediate measurements with classical post-processing. Systematic experiments across multiple circuit sizes and noise settings, including hardware-calibrated noise models derived from IBM Quantum backend data, demonstrate more stable convergence, reduced loss variability, and consistently higher classification accuracy compared with standard QCNNs. Moreover, we observe that this performance advantage significantly amplifies as the circuit size increases, confirming that the hybrid architecture mitigates the scaling limitations of standard architectures. Notably, the multi-basis measurement variant attains performance close to the noiseless limit even under realistic noise. While demonstrated for QCNNs, the proposed depth-stratified feature extraction applies more broadly to hierarchical quantum classifiers that progressively discard qubits.

</details>


### [110] [Quantum criticality in open quantum systems from the purification perspective](https://arxiv.org/abs/2602.21979)
*Yuchen Guo,Shuo Yang*

Main category: quant-ph

TL;DR: 提出基于纯化的框架，系统分类一维具有Z2^σ×Z2^τ对称性的开放量子系统中的混合态相，得到八个由拓扑指标标记的固定点哈密顿量，揭示丰富的相结构。


<details>
  <summary>Details</summary>
Motivation: 开放量子系统中的混合态相超越了传统对称保护拓扑和自发对称破缺范式，需要建立统一且物理透明的分类框架。

Method: 引入辅助κ链，采用装饰畴壁构造，推导八个纯化固定点哈密顿量，通过迹掉辅助系统恢复混合态相结构，构建三维相图立方体几何。

Result: 得到八个由拓扑指标(μ_{στ},μ_{τκ},μ_{κσ})标记的固定点，相图呈现立方体结构，包含对称相、强弱自发对称破缺、平均对称保护拓扑相及其组合，大尺度张量网络模拟揭示金字塔形对称破缺区域和立方体中心的完全对称破缺相。

Conclusion: 纯化方法为混合态相提供了几何透明且物理完备的分类，通过单一Z2^σ×Z2^τ×Z2^κ模型统一了所有相结构，揭示了混合态特有的临界行为机制。

Abstract: Open quantum systems host mixed-state phases that go beyond the symmetry-protected topological and spontaneous symmetry-breaking paradigms established for closed, pure-state systems. Developing a unified and physically transparent classification of such phases remains a central challenge. In this work, we introduce a purification-based framework that systematically characterizes all mixed-state phases in one-dimensional systems with $\mathbb{Z}_2^σ \times \mathbb{Z}_2^τ$ symmetry. By introducing an ancillary $κ$ chain and employing decorated domain-wall constructions, we derive eight purified fixed-point Hamiltonians labeled by topological indices $(μ_{στ},μ_{τκ},μ_{κσ}) \in \{\pm1\}^3$. Tracing out the ancilla recovers the full structure of mixed-state phases, including symmetric, strong-to-weak spontaneous symmetry breaking, average symmetry-protected topological phases, and their nontrivial combinations. Interpolations between the eight fixed points naturally define a three-dimensional phase diagram with a cube geometry. The edges correspond to elementary transitions associated with single topological indices, while the faces host intermediate phases arising from competing domain-wall decorations. Along the edges, we identify a class of critical behavior that connects distinct strong-to-weak symmetry-breaking patterns associated with distinct strong subgroups, highlighting a mechanism unique to mixed-state settings. Large-scale tensor-network simulations reveal a rich phase structure, including pyramid-shaped symmetry-breaking regions and a fully symmetry-broken phase at the cube center. Overall, our purification approach provides a geometrically transparent and physically complete classification of mixed-state phases, unified with a single $\mathbb{Z}_2^σ \times \mathbb{Z}_2^τ \times \mathbb{Z}_2^κ$ model.

</details>


### [111] [Quantum tomography for non-iid sources](https://arxiv.org/abs/2602.22057)
*Leonardo Zambrano*

Main category: quant-ph

TL;DR: 投影最小二乘断层扫描在完全自适应状态和通道准备下仍然保持统计最优性，样本复杂度与非自适应i.i.d.情况相同


<details>
  <summary>Details</summary>
Motivation: 传统量子态和过程断层扫描假设设备发射独立同分布状态，但实际实验中噪声、漂移、反馈或对抗行为会违反这一假设，需要研究非i.i.d.情况下的断层扫描方法

Method: 使用投影最小二乘断层扫描方法，分析其在完全自适应状态和通道准备下的性能，证明其统计最优性

Result: 投影最小二乘断层扫描在自适应设置下的样本复杂度与非自适应i.i.d.情况相同：秩r态重建需要O(dr²/ε²)样本达到迹距离精度ε，过程断层扫描需要O(d⁶/ε²)样本达到钻石距离精度ε

Conclusion: 放弃i.i.d.假设不会增加量子断层扫描的基本样本复杂度，只会改变重建对象的解释，投影最小二乘方法在自适应设置下仍然保持最优性

Abstract: Quantum state and process tomography are typically analyzed under the assumption that devices emit independent and identically distributed (i.i.d.) states or channels. In realistic experiments, however, noise, drift, feedback, or adversarial behavior violate this assumption. We show that projected least-squares tomography remains statistically optimal even under fully adaptive state and channel preparation. Specifically, we prove that the sample complexity for reconstructing the time-averaged state or channel matches the optimal i.i.d. scaling for non-adaptive, single-copy measurements. For rank-$r$ states, the sample complexity is $\mathcal{O}(d r^2/ε^2)$ to achieve accuracy $ε$ in trace distance, while for process tomography it is $\mathcal{O}(d^6/ε^2)$ to achieve accuracy $ε$ in diamond distance. Thus, dropping the i.i.d. assumption does not increase the fundamental sample complexity of quantum tomography, but only changes the interpretation of the reconstructed object.

</details>


### [112] [Hybrid Consensus with Quantum Sybil Resistance](https://arxiv.org/abs/2602.22195)
*Dar Gilboa,Siddhartha Jain,Or Sattath*

Main category: quant-ph

TL;DR: 论文提出了一种结合量子位置验证的共识协议，作为Sybil抵抗机制，相比工作量证明更节能，同时继承了混合协议的优点。


<details>
  <summary>Details</summary>
Motivation: 去中心化共识协议需要Sybil抵抗机制，传统方法依赖稀缺资源（算力、质押等）。量子态的不可克隆性提供了天然的、无条件稀缺的资源，量子位置验证在经典密码学中无法实现，这为设计更安全的共识协议提供了新思路。

Method: 设计了一个结合经典混合共识协议与量子位置验证的共识协议。量子位置验证作为Sybil抵抗机制，在标准模型下提供安全性。同时提出了在随机预言机模型下的垃圾信息预防机制。

Result: 协议相比基于工作量证明的混合协议具有更高的能源效率，继承了混合协议的优点：比纯工作量证明协议更快的确认时间，避免了权益证明中财富集中的问题。

Conclusion: 量子位置验证可以作为有效的Sybil抵抗机制，结合经典混合共识协议，在标准模型下提供安全性，同时提高能源效率，为去中心化共识协议设计提供了新的量子增强方案。

Abstract: Sybil resistance is a key requirement of decentralized consensus protocols. It is achieved by introducing a scarce resource (such as computational power, monetary stake, disk space, etc.), which prevents participants from costlessly creating multiple fake identities and hijacking the protocol. Quantum states are generically uncloneable, which suggests that they may serve naturally as an unconditionally scarce resource. In particular, uncloneability underlies quantum position based-cryptography, which is unachievable classically. We design a consensus protocol that combines classical hybrid consensus protocols with quantum position verification as the Sybil resistance mechanism, providing security in the standard model, and achieving improved energy efficiency compared to hybrid protocols based on Proof-of-Work. Our protocol inherits the benefits of other hybrid protocols, namely the faster confirmation times compared to pure Proof-of-Work protocols, and resilience against the compounding wealth issue that plagues protocols based on Proof-of-Stake Sybil resistance. We additionally propose a spam prevention mechanism for our protocol in the Random Oracle model.

</details>


### [113] [Quantum jumps in open cavity optomechanics and Liouvillian versus Hamiltonian exceptional points](https://arxiv.org/abs/2602.22205)
*Aritra Ghosh,M. Bhattacharya*

Main category: quant-ph

TL;DR: 该论文研究了腔光力学系统中的异常点，阐明了量子跳跃在区分Liouvillian异常点和Hamiltonian异常点中的作用，并揭示了热浴对异常点的影响。


<details>
  <summary>Details</summary>
Motivation: 异常点作为非厄米系统中多个本征态合并的特殊点，在开放系统动力学、传感、非互易传输和拓扑相变等领域具有重要意义。腔光力学平台在引力波探测、宏观量子力学、量子转导等方面有广泛应用，因此研究其中的异常点具有重要价值。

Method: 采用热场形式理论，推导了一个统一的谱框架，通过解析的混合Liouvillian描述来插值不同机制。特别关注量子跳跃在区分无条件Lindblad动力学和条件无跳跃演化中的作用。

Result: 发现Liouvillian异常点源于无条件Lindblad动力学且与声子浴温度无关，而Hamiltonian异常点源于条件无跳跃演化并因增强的条件阻尼而获得热位移。在弱量子跳跃机制下，异常点仅在二阶受到扰动，显示了Hamiltonian异常点在小混合扰动下的鲁棒性。

Conclusion: 该工作揭示了一个连续的混合异常点家族，阐明了腔光力学系统中条件和无条件耗散动力学的操作和物理差异，并为热浴探测提供了新方法。

Abstract: Exceptional points, where two or more eigenstates of a non-Hermitian system coalesce, are now of interest across many fields of physics, from the perspective of open-system dynamics, sensing, nonreciprocal transport, and topological phase transitions. In this work, we investigate exceptional points in cavity optomechanics, a platform of interest to diverse communities working on gravitational-wave detection, macroscopic quantum mechanics, quantum transduction, etc. Specifically, we clarify the role of quantum jumps in making a clear distinction between Liouvillian and Hamiltonian exceptional points in optomechanical systems. While the Liouvillian exceptional point arises from the unconditional Lindblad dynamics and is independent of the phonon-bath temperature, the Hamiltonian exceptional point emerges from the conditional no-jump evolution and acquires a thermal shift due to an enhanced conditional damping. Employing the thermofield formalism, we derive a unified spectral framework that interpolates between these regimes via an analytical hybrid-Liouvillian description. Remarkably, in the weak-quantum-jump regime, the exceptional point is perturbed only at the second order, highlighting the robustness of the Hamiltonian exceptional point under small hybrid perturbations. Our work reveals a continuous family of hybrid exceptional points, clarifies the operational and physical differences between the conditional and unconditional dissipative dynamics in optomechanical systems, and provides a probe for thermal baths.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [114] [Using thermodynamics to learn gravitational wave physics](https://arxiv.org/abs/2602.21261)
*Caio César Rodrigues Evangelista,Níckolas de Aguiar Alves*

Main category: gr-qc

TL;DR: 该论文向物理入门学生介绍黑洞热力学，将黑洞面积不减性与热力学熵类比，用本科热物理方法分析黑洞物理特性。


<details>
  <summary>Details</summary>
Motivation: 黑洞是宇宙中最有趣的天体之一，虽然源于复杂的广义相对论，但其物理规律却出奇简单。黑洞面积永不减少的特性与热力学熵相似，这为用标准热物理方法理解黑洞物理提供了机会。

Method: 使用本科热物理课程中的标准技术，将黑洞面积不减性类比为热力学熵增原理，探索黑洞合并中能量发射的界限（霍金原始计算），将黑洞视为类似热机的系统。

Result: 通过这种类比方法，可以计算黑洞合并中能量发射的界限，将黑洞视为热机系统，并展示这些概念如何应用于现代引力波天文台测试广义相对论。

Conclusion: 黑洞热力学类比使得研究级课题能够在入门物理课堂中讨论，为学生提供了用熟悉的热物理概念理解复杂黑洞物理的桥梁。

Abstract: Black holes are some of the most interesting objects in the universe. While they first arise in the complicated behavior of general relativity, the physical laws ruling their behavior are surprisingly simple. For example, one of the core facts about black holes is that their area never decreases, much alike the entropy in thermodynamics. In this note directed at introductory physics students and their instructors, we use this similarity to understand properties of black hole physics using standard techniques from an undergraduate course in thermal physics. We explore the never-decreasing nature of black hole area to obtain bounds on the energy emitted in a black hole merger (a calculation originally done by Hawking). We show how this allows us to think of black holes in manners very similar to heat engines, and how these ideas have been used in modern gravitational wave observatories to test general relativity. This allows a research-level topic to be discussed in introductory physics lectures.

</details>


### [115] [Quantum Cosmology, Decoherence, and the Emergence of Classical Spacetime](https://arxiv.org/abs/2602.21263)
*Ali Nayeri*

Main category: gr-qc

TL;DR: 量子宇宙学中经典宇宙时空的涌现：通过计算长波长曲率扰动的约化密度矩阵，分析边界条件和退相干在经典分支选择中的作用，揭示环境诱导纠缠产生宇宙时间箭头。


<details>
  <summary>Details</summary>
Motivation: 研究量子宇宙学中经典宇宙时空如何从量子起源中涌现，特别关注Hartle-Hawking和隧道边界条件下，半经典WKB结构和暴胀压缩本身不足以产生经典性，需要退相干机制。

Method: 使用约化密度矩阵和影响泛函形式体系，推导暴胀期间超视界曲率模式的退相干泛函；考虑Bunch-Davies真空中的轻质量环境标量场，获得显式噪声核；评估基于视界和有效场论动机的粗粒化方案。

Result: 对于轻质量环境标量场，获得显式噪声核并展示非零质量如何调节红外行为；在视界基和有效场论粗粒化下，都发现宏观不同扰动历史之间干涉的有效抑制；边界条件决定分支振幅，退相干实现经典分支选择。

Conclusion: 经典宇宙时空的涌现需要边界条件和退相干的共同作用：边界条件提供分支振幅，退相干实现经典分支选择；环境诱导纠缠产生宇宙时间箭头，澄清了量子宇宙学中经典性起源的机制。

Abstract: We analyze the emergence of classical cosmological spacetimes in quantum cosmology by computing the reduced density matrix for long-wavelength curvature perturbations. Starting from standard Hartle--Hawking and tunneling boundary conditions, we emphasize that semiclassical WKB structure and inflationary squeezing do not by themselves yield classicality. Tracing over unobserved degrees of freedom and using the influence functional formalism, we derive the decoherence functional for superhorizon curvature modes during inflation. For a light massive environmental scalar field in the Bunch--Davies vacuum, we obtain an explicit noise kernel and show how a nonzero mass regulates the infrared behavior. We then evaluate decoherence under horizon-based and EFT-motivated coarse grainings, finding efficient suppression of interference between macroscopically distinct perturbation histories in both cases. The analysis clarifies the distinct roles of boundary conditions (branch amplitudes) and decoherence (classical branch selection) and yields an emergent cosmological arrow of time through environment-induced entanglement.

</details>


### [116] [Lorentz-Violating Wormhole Optics](https://arxiv.org/abs/2602.21264)
*Omar Mustafa,Semra Gurtas Dogan,Abdulkerim Karabulut,Abdullah Guvendi*

Main category: gr-qc

TL;DR: 研究(2+1)维静态圆对称虫洞中无质量自旋1场的传播，虫洞具有洛伦兹破坏的空间各向异性，表现为位置依赖的折射率和频率相关的局域化效应。


<details>
  <summary>Details</summary>
Motivation: 探索在拓扑非平凡的(2+1)维背景中，由曲率驱动的局域化、色散和各向异性波传播的几何框架，特别是洛伦兹破坏如何影响场传播。

Method: 使用完全协变的矢量玻色子形式和协变麦克斯韦理论，推导出精确的薛定谔型径向方程，通过亥姆霍兹形式重新表述动力学，建立与螺旋表面的微分几何对应关系。

Result: 虫洞表现为具有位置依赖折射率的非均匀光学介质，低频模式被强烈束缚，高频模式几乎自由传播；洛伦兹破坏引起的曲率在数学上等价于几何扭曲产生的曲率。

Conclusion: 为(2+1)维拓扑非平凡背景中的曲率驱动局域化、色散和各向异性波传播提供了几何框架，并将模型与扭曲石墨烯纳米带作为类比引力平台联系起来。

Abstract: We study massless spin-1 field propagation in a static, circularly symmetric $(2+1)$-dimensional wormhole with spatial Lorentz-violating anisotropy characterized by the throat radius $a$ and deformation parameter $η$. The geometry is horizon-free, geodesically complete, and asymptotically flat, with negative Gaussian curvature localized near the throat. Using the fully covariant vector boson formalism and covariant Maxwell theory, we derive an exact Schrödinger-type radial equation with a curvature-induced effective potential. Recasting the dynamics in Helmholtz form yields an effective refractive-index profile, showing that the wormhole acts as an inhomogeneous optical medium with position-dependent refractive index and frequency-dependent confinement, where low-frequency modes are strongly trapped while high-frequency modes propagate almost freely. A differential-geometric correspondence with helicoidal surfaces is established via $1/[a^2(1-η)] \leftrightarrow w^2$, demonstrating that Lorentz-violation-induced curvature is mathematically equivalent to curvature generated by geometric twist and linking the model to twisted graphene nanoribbons as analog-gravity platforms. These results provide a geometric framework for curvature-driven localization, dispersion, and anisotropic wave propagation in topologically nontrivial $(2+1)$-dimensional backgrounds.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [117] [Using Neural Networks to Accelerate TALYS-2.0 Nuclear Reaction Simulations](https://arxiv.org/abs/2602.21239)
*Wilson Lin,Catherine E Apgar,Lee A Bernstein,YunHsuan Lee,Alan B McIntosh,Dmitri G Medvedev,Ellen M OBrien,Christiaan E Vermeulen,Andrew S Voyles,Jonathan T Morrell*

Main category: physics.comp-ph

TL;DR: 使用神经网络作为TALYS-2.0的替代模型，快速预测带电粒子残余产物截面，比直接使用TALYS-2.0快1000倍以上


<details>
  <summary>Details</summary>
Motivation: 传统调整TALYS-2.0参数的方法耗时且需要顺序计算，需要更高效的方法来预测核反应截面

Method: 训练人工神经网络作为TALYS-2.0的替代模型，使用不同采样方法生成训练数据集，然后用于多参数拟合调整

Result: 神经网络模型性能在不同采样方法间无显著差异，生成截面的速度比TALYS-2.0快1000倍以上，仅需约1500个TALYS-2.0文件即可实现高保真替代模型

Conclusion: 神经网络作为替代模型可显著加速TALYS-2.0参数调整过程，使多参数拟合变得可行且优化，为核反应截面预测提供了高效解决方案

Abstract: Recent efforts to improve the predictability of TALYS-2.0 calculated charged-particle residual product cross sections have focused on adjusting parameters related to the optical model potential and pre-equilibrium process. Although adjusted TALYS-2.0 outputs show marked improvements in agreement with experimental data over the default parameters, the procedure is generally time-consuming due to the need for sequential TALYS-2.0 calculations. Since the models and model parameters must be defined and constrained prior to adjustment, we show in this work that an artificial neural network can serve as a surrogate model to successfully predict TALYS-2.0 outputs within this domain of input parameters. No practical differences were observed in the trained model's performance between uniform random, Latin hypercube and Sobol sequence sampling for generating the training datasets. Once validated, trained neural network models were used to adjust TALYS-2.0 nuclear model parameters, where a multi-parameter fitting procedure was not only feasible but optimal for this process. The neural network approach is >1000x faster at generating residual product cross sections than using TALYS-2.0 directly, and a high-fidelity surrogate model could be implemented with about 1500 TALYS-2.0 files to achieve adjusted cross sections comparable to the previous publication.

</details>


### [118] [Generalized Onsager-Regularized Lattice Boltzmann Method for error-free Navier-Stokes models on standard lattices](https://arxiv.org/abs/2602.21242)
*Anirudh Jonnalagadda,Walter Rocchia,Sauro Succi*

Main category: physics.comp-ph

TL;DR: 提出一种新颖策略，通过Onsager-Regularized非平衡分布函数来修正最近邻格子Boltzmann方法中的Navier-Stokes建模误差，实现完全局部修正


<details>
  <summary>Details</summary>
Motivation: 解决最近邻格子Boltzmann方法中Navier-Stokes建模误差问题，特别是兼容性条件违反和应力张量建模误差

Method: 引入Onsager-Regularized非平衡分布函数进行局部修正，开发部分修正和完全修正模型，在D2Q9格子上实现六矩约束引导平衡表示

Result: 部分修正模型在参考/任意格子温度下精度提高2-4个数量级；完全修正模型进一步修正应力张量误差，实现完全精确模型。数值基准测试显示修正方案相比Lattice-BGK和未修正OReg-GEq方案具有更好的精度和稳定性

Conclusion: 提出的修正策略有效解决了最近邻LB方法的建模误差问题，为基于OReg的热流体动力学扩展提供了有前景的途径

Abstract: This work presents a novel strategy to address Navier-Stokes modelling errors arising on first-nearest neighbour lattice Boltzmann (LB) methods and introduces fully local corrections through Onsager-Regularized (OReg) non-equilibrium populations. The proposed mechanism, which admits partially and completely corrected OReg models, is used to develop representative partially and completely corrected models for the six-moment-constrained guided equilibrium (GEq) representation on the D2Q9 lattice. The former realization only addresses compatibility condition violations and improves the accuracy by two/four orders of magnitude at reference/arbitrary lattice temperatures respectively, while the latter additionally corrects stress tensor modelling errors, resulting in a fully corrected exact model. Numerical benchmarks of the corrected schemes demonstrate improved accuracy and stability in comparison to the Lattice-BGK and uncorrected OReg-GEq schemes thus presenting a promising avenue for OReg based thermohydrodynamic extensions.

</details>


### [119] [Physics Constrained Neural Collision Operators for Variable Hard Sphere Surrogates and Ab Initio Angle Prediction in Direct Simulation Monte Carlo](https://arxiv.org/abs/2602.21244)
*Ehsan Roohi,Ahmad Shoja-Sani,Stefan Stefanov*

Main category: physics.comp-ph

TL;DR: 提出一个统一的物理约束神经算子框架，用于加速直接模拟蒙特卡洛方法，同时保持物理不变性和随机性，实现零样本空间和热力学泛化。


<details>
  <summary>Details</summary>
Motivation: DSMC方法是非平衡稀薄气体动力学的黄金标准，但计算成本过高，特别是在近连续区域和高精度从头算势能下。需要开发既能加速计算又能保持物理不变性和随机性的方法。

Method: 1. 引入局部神经碰撞核替代现象学的可变硬球模型；2. 通过物理约束的随机层增强推理，控制潜在噪声注入恢复热涨落，单元级矩匹配严格保持动量和动能守恒；3. 为Jäger相互作用势开发专门的从头算神经算子，通过物理收获策略在大规模碰撞对上训练。

Result: 1. 算子展现出零样本空间和热力学泛化能力：仅用1D库埃特流训练的模型能准确模拟复杂2D盖驱动腔，捕获高阶非平衡矩；2. 在Mach~10稀薄氩气圆柱绕流中验证，框架能高保真地再现输运行为和激波特征，相比直接数值积分实现约20%的成本降低。

Conclusion: 开发了一个统一的物理约束神经算子框架，成功加速DSMC计算，同时保持必要的物理不变性和随机性，实现了零样本泛化能力，为高精度稀薄气体动力学模拟提供了高效解决方案。

Abstract: The Direct Simulation Monte Carlo (DSMC) method is the gold standard for non-equilibrium rarefied gas dynamics, yet its computational cost can be prohibitive, especially for near-continuum regimes and high-fidelity \emph{ab initio} potentials. This work develops a unified, physics-constrained neural-operator framework that accelerates DSMC while preserving physical invariants and stochasticity required for long-time kinetic simulations. First, we introduce a local neural collision kernel replacing the phenomenological Variable Hard Sphere (VHS) model. To overcome the variance suppression and artificial cooling inherent to purely deterministic regression surrogates, we augment inference with a physics-constrained stochastic layer. Controlled latent-noise injection restores thermal fluctuations, while cell-wise moment-matching strictly enforces momentum and kinetic-energy conservation. Remarkably, this operator exhibits zero-shot spatial and thermodynamic generalization: a model trained exclusively on 1D Couette flow accurately simulates a complex 2D lid-driven cavity, capturing high-order non-equilibrium moments without retraining.Second, to bypass the extreme cost of quantum-mechanical scattering, we develop a dedicated \emph{ab initio} neural operator for the Jäger interaction potential. Trained via a \emph{physics harvesting} strategy on large-scale collision pairs, it efficiently captures the high-energy scattering dynamics dominating hypersonic regimes. Validated on a Mach~10 rarefied argon flow over a cylinder, the framework reproduces transport behaviors and shock features with high fidelity, achieving an approximate 20\% cost reduction relative to direct numerical integration.

</details>


### [120] [Efficient and Accurate Method for Separating Variant Components from Invariant Background and Component Model Fusion for Fast RFIC Design Space Exploration](https://arxiv.org/abs/2602.21335)
*Hongyang Liu,Dan Jiao*

Main category: physics.comp-ph

TL;DR: 提出一种快速RFIC设计空间探索方法，通过代数分解将变体组件与不变背景分离，只需模拟一次不变背景，大大减少设计变体仿真计算量。


<details>
  <summary>Details</summary>
Motivation: 传统电磁仿真器需要对每个设计变体进行全域仿真，而RFIC设计通常包含大量设计变体在不变背景下探索，计算成本高昂。

Method: 1. 代数分解总场解为变体组件贡献和不变背景贡献；2. 不变背景只需仿真一次并重复使用；3. 开发组件模型复用和融合方法；4. 提出快速算法将大量源场解减少到层数数量级。

Result: 该方法已应用于RFIC设计空间探索，证明了其准确性、鲁棒性和高效性。

Conclusion: 提出的方法能有效分离变体组件与不变背景，显著加速RFIC设计空间探索过程，只需模拟小型变体组件，大幅减少计算成本。

Abstract: The design of RFIC often involves exploring a large number of design variations in an invariant background composed of the processing stack and unchanged circuit blocks. Conventional electromagnetic solvers require a full-domain simulation for every design variation. In this work, we present a fast method that effectively separates the variant components from the invariant background. It algebraically decomposes the total field solution into the contributions from the design-dependent variations and the invariant background. Hence, the field response due to the invariant background can be simulated once and reused for all design variations. Only the variant components need to be simulated at each design variation, the size of which is small. We also develop an efficient way of reusing the model of each component and fusing them accurately to obtain the model of a system composed of many components. The reduced system of variant components involves computing the field solutions in the invariant background due to all possible sources located at variant components, the number of which can be large. We develop a fast algorithm to reduce them to a few field solutions, the number of which is on the order of the layer number. The proposed method has been applied to RFIC design space exploration. Its accuracy, robustness, and efficiency have been demonstrated.

</details>


### [121] [Emergent Rate Laws for Collective Lying-Standing Transitions](https://arxiv.org/abs/2602.21747)
*Anna Werkovits,Simon B. Hollweger,Oliver T. Hofmann*

Main category: physics.comp-ph

TL;DR: 该研究建立了有机-无机界面分子单层中躺-立构象转变的定量动力学关系，揭示了集体转变速率由多个微观过程耦合决定，并推导出可预测转变时间尺度的解析表达式。


<details>
  <summary>Details</summary>
Motivation: 有机-无机界面分子单层的躺-立构象转变强烈影响界面偶极、能级对齐和生长模式，但其集体动力学难以预测，需要建立定量关系来理解和调控这一过程。

Method: 采用第一性原理为基础的动力学蒙特卡洛模拟结合平均场粗粒化策略，以四氰基乙烯在Cu(111)表面为模型体系，分析分子尺寸和构象足迹比的影响。

Result: 集体转变速率不能从任何单一基元步骤推断，而是由取向、吸附和扩散等微观过程耦合决定。几何参数是内在控制参数，集体速率近似与分子面积成比例，增加躺-立构象足迹比可产生数量级加速。

Conclusion: 推导出集体重取向速率常数的解析表达式，将温度压力依赖的微观速率常数与几何参数联系起来，为工程化有机-无机界面躺-立转变时间尺度提供了可转移的设计原则。

Abstract: Lying-standing transitions in the first molecular monolayer at organic-inorganic interfaces strongly influence interface dipoles, energy-level alignment, and growth modes, yet their collective kinetics remain difficult to predict. Here, we establish a quantitative adsorbate-to-kinetics relationship using first-principles-based kinetic Monte Carlo simulations combined with a mean-field coarse-graining strategy. Focusing on tetracyanoethylene on Cu(111), we show that the collective transition rate cannot be inferred from any single elementary step but emerges from coupled microscopic processes, including reorientation, adsorption, and diffusion. A local two-step reorientation mechanism captures the diffusion-limited regime, while diffusion of lying molecules accelerates the transition in diffusion-enhanced regimes by suppressing back-reorientation via vacancy-molecule decoupling. This effect is described by a regime-dependent geometric factor accounting for deviations between single-molecule and collective rate constants. By varying molecular size and footprint ratio, we demonstrate that geometry is an intrinsic control parameter. While the collective rate scales approximately with molecular area, increasing the footprint ratio between lying and standing configurations yields order-of-magnitude accelerations due to enhanced vacancy creation and diffusion-assisted stabilization. Finally, we derive an analytical expression for the collective reorientation rate constant linking temperature- and pressure-dependent microscopic rate constants to geometric parameters. The formulation reproduces the simulations across kinetic regimes and provides transferable design principles for engineering lying-standing transition timescales at organic-inorganic interfaces.

</details>


### [122] [Phase-Dependent Excitonic Light Harvesting and Photovoltaic Limits in Monolayer Y2TeO2 MOenes](https://arxiv.org/abs/2602.22112)
*Bill D. A. Huacarpuma,Jose A. dos S. Laranjeira,Nicolas F. Martins,Julio R. Sambrano,Kleuton A. L. Lima,Santosh K. Tiwari,Alexandre C. Dias,Luiz A. Ribeiro*

Main category: physics.comp-ph

TL;DR: 该研究通过第一性原理计算和有效多体理论，研究了单层Y2TeO2 MOenes在1T和2H相中的电子和激子特性，发现两种相都具有直接带隙和强激子效应，适合光伏应用。


<details>
  <summary>Details</summary>
Motivation: 探索低维氧硫族化合物系统中多体物理的平台，特别是用于光伏应用。研究Y2TeO2单层MOenes在1T和2H相中的相依赖电子和激子现象。

Method: 使用第一性原理理论和有效多体框架，包括声子谱和弹性稳定性标准分析稳定性，准粒子能带结构分析带隙，以及紧束缚Bethe-Salpeter方法计算光学谱和激子效应。

Result: 两种相都是动态和机械稳定的，具有近红外到可见光范围内的直接带隙。激子结合能分别达到152 meV（1T相）和126 meV（2H相），结构更致密的1T相表现出更强的量子限制效应。

Conclusion: Y2TeO2单层是一种罕见的稳定、直接带隙MOenes，具有强激子效应，为探索低维氧硫族化合物系统中的多体物理提供了平台，特别适用于光伏应用。

Abstract: We investigate phase-dependent electronic and excitonic phenomena in monolayer Y2TeO2 MOenes in the 1T and 2H polymorphs using first-principles theory and an effective many-body framework. Phonon spectra and elastic stability criteria establish both phases as dynamically and mechanically stable. Quasiparticle band structures reveal direct gaps in the near-infrared to visible range, with gap values increasing systematically from semilocal to hybrid exchange treatments. Optical spectra computed using a tight-binding Bethe-Salpeter approach demonstrate pronounced excitonic resonances arising from reduced dimensionality and weak dielectric screening. The exciton binding energies reach 152 meV in the 1T phase and 126 meV in the 2H phase, reflecting enhanced quantum confinement in the structurally denser phase. Our results identify Y2TeO2monolayers as a rare class of stable, direct-gap MOenes with strong excitonic effects, providing a platform for exploring many-body physics in low-dimensional oxychalcogenide systems especially for photovoltaic applications.

</details>
