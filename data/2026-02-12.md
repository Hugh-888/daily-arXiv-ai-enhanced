<div id=toc></div>

# Table of Contents

- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 141]
- [quant-ph](#quant-ph) [Total: 35]
- [gr-qc](#gr-qc) [Total: 15]


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [1] [Noise-balanced multilevel on-the-fly sparse grid surrogates for coupling Monte Carlo models into continuum models with application to heterogeneous catalysis](https://arxiv.org/abs/2602.11006)
*Tobias Hülser,Sebastian Matera*

Main category: physics.comp-ph

TL;DR: 提出一种噪声平衡稀疏网格插值方法，用于构建微观蒙特卡洛模型的高效代理模型，解决采样噪声和高维问题，应用于异质催化多尺度模拟。


<details>
  <summary>Details</summary>
Motivation: 多尺度模拟中使用高保真微观蒙特卡洛模型提供非线性响应时计算成本过高。代理模型可解决此问题，但面临采样噪声导致误差失控和高维"维度诅咒"的挑战。

Method: 提出噪声平衡稀疏网格插值方法，以准最优方式控制每个数据点的蒙特卡洛采样量，结合多尺度模拟中的多级实时构建策略。

Result: 该方法在异质催化的挑战性示例中得到验证，将微观动力学蒙特卡洛模型耦合到宏观反应器模拟中，展示了方法的有效性。

Conclusion: 该方法不仅高效，而且易于使用，仅需单个超参数即可控制整个代理构建过程，从代理精度到数据创建需求均有保证收敛性。

Abstract: Multiscale simulations utilizing high-fidelity, microscopic Monte Carlo models to provide the nonlinear response for continuum models can easily become computationally intractable. Surrogate models for the high-fidelity Monte Carlo models can overcome this but come with some challenges. One such challenges arise by the sampling noise in the underlying Monte Carlo data, which leads to uncontrolled errors possibly corrupting the surrogate even though it would be highly accurate in the case of noise-free data. Another challenge arises by the 'curse of dimensionality' when the response depends on many macro-variables. These points are addressed by a novel noise-balanced sparse grids interpolation approach which, in a quasi-optimal fashion, controls the amount of Monte Carlo sampling for each data point. The approach is complemented by a multilevel on-the-fly construction during the multiscale simulation. Besides its efficiency, a particularly appealing feature is the ease of use of the approach with only a single hyperparameter controlling the whole surrogate construction - from the surrogate's accuracy with guaranteed convergence to which data needs to be created with which accuracy. The approach is demonstrated on challenging examples from heterogeneous catalysis, coupling microscopic kinetic Monte Carlo models into macroscopic reactor simulations.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [Large Language Models Predict Functional Outcomes after Acute Ischemic Stroke](https://arxiv.org/abs/2602.10119)
*Anjali K. Kapoor,Anton Alyakin,Jin Vivian Lee,Eunice Yang,Annelene M. Schulze,Krithik Vishwanath,Jinseok Lee,Yindalon Aphinyanaphongs,Howard Riina,Jennifer A. Frontera,Eric Karl Oermann*

Main category: cs.LG

TL;DR: LLM可从入院记录预测卒中后功能结局，性能与结构化数据模型相当，支持开发无需手动数据提取的文本预后工具。


<details>
  <summary>Details</summary>
Motivation: 准确预测急性缺血性卒中后的功能结局可指导临床决策和资源分配。以往研究主要依赖结构化变量和传统机器学习，而LLM直接从常规入院记录推断未来mRS评分的能力尚未充分探索。

Method: 评估编码器模型（BERT、NYUTron）和生成式模型（Llama-3.1-8B、MedGemma-4B），在冻结和微调两种设置下，使用大型真实世界卒中登记数据预测出院和90天mRS。数据集包括9,485份出院记录和1,898份90天记录，按时间分割，最近12个月用于测试。

Result: 微调后的Llama表现最佳：90天mRS准确率33.9%（95% CI 27.9-39.9%），二元准确率76.3%（95% CI 70.7-81.9%）；出院准确率42.0%（95% CI 39.0-45.0%），二元准确率75.0%（95% CI 72.4-77.6%）。90天预测性能与结构化数据基线模型相当。

Conclusion: 微调LLM仅从入院记录即可预测卒中后功能结局，性能与需要结构化变量提取的模型相当。支持开发基于文本的预后工具，无需手动数据提取即可无缝集成到临床工作流程中。

Abstract: Accurate prediction of functional outcomes after acute ischemic stroke can inform clinical decision-making and resource allocation. Prior work on modified Rankin Scale (mRS) prediction has relied primarily on structured variables (e.g., age, NIHSS) and conventional machine learning. The ability of large language models (LLMs) to infer future mRS scores directly from routine admission notes remains largely unexplored. We evaluated encoder (BERT, NYUTron) and generative (Llama-3.1-8B, MedGemma-4B) LLMs, in both frozen and fine-tuned settings, for discharge and 90-day mRS prediction using a large, real-world stroke registry. The discharge outcome dataset included 9,485 History and Physical notes and the 90-day outcome dataset included 1,898 notes from the NYU Langone Get With The Guidelines-Stroke registry (2016-2025). Data were temporally split with the most recent 12 months held out for testing. Performance was assessed using exact (7-class) mRS accuracy and binary functional outcome (mRS 0-2 vs. 3-6) accuracy and compared against established structured-data baselines incorporating NIHSS and age. Fine-tuned Llama achieved the highest performance, with 90-day exact mRS accuracy of 33.9% [95% CI, 27.9-39.9%] and binary accuracy of 76.3% [95% CI, 70.7-81.9%]. Discharge performance reached 42.0% [95% CI, 39.0-45.0%] exact accuracy and 75.0% [95% CI, 72.4-77.6%] binary accuracy. For 90-day prediction, Llama performed comparably to structured-data baselines. Fine-tuned LLMs can predict post-stroke functional outcomes from admission notes alone, achieving performance comparable to models requiring structured variable abstraction. Our findings support the development of text-based prognostic tools that integrate seamlessly into clinical workflows without manual data extraction.

</details>


### [3] [Towards Autonomous Mathematics Research](https://arxiv.org/abs/2602.10177)
*Tony Feng,Trieu H. Trinh,Garrett Bingham,Dawsen Hwang,Yuri Chervonyi,Junehyuk Jung,Joonkyung Lee,Carlo Pagano,Sang-hyun Kim,Federico Pasqualotto,Sergei Gukov,Jonathan N. Lee,Junsu Kim,Kaiying Hou,Golnaz Ghiasi,Yi Tay,YaGuang Li,Chenkai Kuang,Yuan Liu,Hanzhao,Lin,Evan Zheran Liu,Nigamaa Nayakanti,Xiaomeng Yang,Heng-tze Cheng,Demis Hassabis,Koray Kavukcuoglu,Quoc V. Le,Thang Luong*

Main category: cs.LG

TL;DR: Aletheia是一个数学研究智能体，能够从奥数竞赛题到博士级问题，再到生成完整研究论文，实现端到端的数学问题求解和证明。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型在数学竞赛中已达到金牌水平，但从竞赛级问题解决转向专业研究需要处理大量文献和构建长程证明。需要开发能够导航数学研究复杂性的智能系统。

Method: Aletheia采用迭代生成、验证和修订解决方案的端到端自然语言处理流程，结合Gemini Deep Think进行复杂推理，使用新颖的推理时间扩展定律，并密集使用工具来应对数学研究的复杂性。

Result: 展示了三个里程碑：1) 完全由AI生成的研究论文(Feng26)；2) 人机协作的研究论文(LeeSeo26)；3) 对700个开放问题的半自主评估，包括自主解决四个开放问题。提出了量化AI辅助结果自主性和新颖性的标准等级。

Conclusion: Aletheia展示了AI在数学研究中的强大能力，从竞赛级到研究级的扩展，并提出了评估AI数学贡献的框架，反思了人机协作在数学研究中的未来。

Abstract: Recent advances in foundational models have yielded reasoning systems capable of achieving a gold-medal standard at the International Mathematical Olympiad. The transition from competition-level problem-solving to professional research, however, requires navigating vast literature and constructing long-horizon proofs. In this work, we introduce Aletheia, a math research agent that iteratively generates, verifies, and revises solutions end-to-end in natural language. Specifically, Aletheia is powered by an advanced version of Gemini Deep Think for challenging reasoning problems, a novel inference-time scaling law that extends beyond Olympiad-level problems, and intensive tool use to navigate the complexities of mathematical research. We demonstrate the capability of Aletheia from Olympiad problems to PhD-level exercises and most notably, through several distinct milestones in AI-assisted mathematics research: (a) a research paper (Feng26) generated by AI without any human intervention in calculating certain structure constants in arithmetic geometry called eigenweights; (b) a research paper (LeeSeo26) demonstrating human-AI collaboration in proving bounds on systems of interacting particles called independent sets; and (c) an extensive semi-autonomous evaluation (Feng et al., 2026a) of 700 open problems on Bloom's Erdos Conjectures database, including autonomous solutions to four open questions. In order to help the public better understand the developments pertaining to AI and mathematics, we suggest codifying standard levels quantifying autonomy and novelty of AI-assisted results. We conclude with reflections on human-AI collaboration in mathematics.

</details>


### [4] [A Multimodal Conditional Mixture Model with Distribution-Level Physics Priors](https://arxiv.org/abs/2602.10451)
*Jinkyo Han,Bahador Bahmani*

Main category: cs.LG

TL;DR: 提出基于混合密度网络的物理信息多模态条件建模框架，通过组件特定正则化嵌入物理知识，在保持物理一致性的同时处理多模态分布。


<details>
  <summary>Details</summary>
Motivation: 许多科学和工程系统表现出固有的多模态行为，源于潜在的机制切换和非唯一物理机制。现有方法难以在保持物理一致性和可解释性的同时学习完整条件分布，特别是在数据有限的情况下。

Method: 基于混合密度网络（MDNs）开发物理信息多模态条件建模框架，通过组件特定正则化项嵌入物理知识，惩罚违反控制方程或物理定律的情况。该框架能自然处理非唯一性和随机性，计算高效且支持上下文输入条件化。

Result: 在多个科学问题中评估该框架，包括非线性动力系统的分岔现象、随机偏微分方程和原子尺度冲击动力学。与条件流匹配（CFM）模型相比，MDNs能达到竞争性性能，同时提供更简单和可解释的表述。

Conclusion: 提出的物理信息混合密度网络框架为多模态科学建模提供了有效解决方案，在保持物理一致性和可解释性的同时处理复杂分布，为数据有限场景下的科学建模提供了实用工具。

Abstract: Many scientific and engineering systems exhibit intrinsically multimodal behavior arising from latent regime switching and non-unique physical mechanisms. In such settings, learning the full conditional distribution of admissible outcomes in a physically consistent and interpretable manner remains a challenge. While recent advances in machine learning have enabled powerful multimodal generative modeling, their integration with physics-constrained scientific modeling remains nontrivial, particularly when physical structure must be preserved or data are limited. This work develops a physics-informed multimodal conditional modeling framework based on mixture density representations. Mixture density networks (MDNs) provide an explicit and interpretable parameterization of multimodal conditional distributions. Physical knowledge is embedded through component-specific regularization terms that penalize violations of governing equations or physical laws. This formulation naturally accommodates non-uniqueness and stochasticity while remaining computationally efficient and amenable to conditioning on contextual inputs. The proposed framework is evaluated across a range of scientific problems in which multimodality arises from intrinsic physical mechanisms rather than observational noise, including bifurcation phenomena in nonlinear dynamical systems, stochastic partial differential equations, and atomistic-scale shock dynamics. In addition, the proposed method is compared with a conditional flow matching (CFM) model, a representative state-of-the-art generative modeling approach, demonstrating that MDNs can achieve competitive performance while offering a simpler and more interpretable formulation.

</details>


### [5] [Signature-Kernel Based Evaluation Metrics for Robust Probabilistic and Tail-Event Forecasting](https://arxiv.org/abs/2602.10182)
*Benjamin R. Redhead,Thomas L. Lee,Peng Gu,Víctor Elvira,Amos Storkey*

Main category: cs.LG

TL;DR: 提出两种基于核的度量方法（Sig-MMD和CSig-MMD）来解决概率预测评估中的关键缺陷，特别是对尾部事件敏感度不足和依赖关系假设问题。


<details>
  <summary>Details</summary>
Motivation: 当前概率预测评估框架缺乏共识度量标准，存在两个关键缺陷：假设时间步或变量间独立，以及对尾部事件（实际决策中最关键的事件）缺乏敏感性。

Method: 提出两种基于核的度量方法：签名最大均值差异（Sig-MMD）和新型的截断Sig-MMD（CSig-MMD）。利用签名核捕捉复杂的变量间和时间依赖关系，对缺失数据保持鲁棒性。CSig-MMD引入截断方案，优先考虑预测尾部事件的能力，同时严格保持适当性。

Result: 这些度量方法能够更可靠地评估直接多步预测，促进开发更鲁棒的概率算法。

Conclusion: 提出的Sig-MMD和CSig-MMD度量解决了当前概率预测评估框架的关键限制，特别是对尾部事件敏感度不足和依赖关系假设问题，为更可靠的预测评估提供了新工具。

Abstract: Probabilistic forecasting is increasingly critical across high-stakes domains, from finance and epidemiology to climate science. However, current evaluation frameworks lack a consensus metric and suffer from two critical flaws: they often assume independence across time steps or variables, and they demonstrably lack sensitivity to tail events, the very occurrences that are most pivotal in real-world decision-making. To address these limitations, we propose two kernel-based metrics: the signature maximum mean discrepancy (Sig-MMD) and our novel censored Sig-MMD (CSig-MMD). By leveraging the signature kernel, these metrics capture complex inter-variate and inter-temporal dependencies and remain robust to missing data. Furthermore, CSig-MMD introduces a censoring scheme that prioritizes a forecaster's capability to predict tail events while strictly maintaining properness, a vital property for a good scoring rule. These metrics enable a more reliable evaluation of direct multi-step forecasting, facilitating the development of more robust probabilistic algorithms.

</details>


### [6] [On the Role of Consistency Between Physics and Data in Physics-Informed Neural Networks](https://arxiv.org/abs/2602.10611)
*Nicolás Becerra-Zuniga,Lucas Lacasa,Eusebio Valero,Gonzalo Rubio*

Main category: cs.LG

TL;DR: PINNs存在数据与PDE不一致时的精度限制，提出一致性障碍概念，通过Burgers方程实验证明数据质量决定PINNs最终精度上限


<details>
  <summary>Details</summary>
Motivation: PINNs在实际应用中常使用与PDE不完全一致的数据（噪声、离散误差等），这种数据不一致性对PINNs精度和收敛的影响尚未充分理解

Method: 引入一致性障碍概念，使用1D粘性Burgers方程和制造解析解，通过控制数据保真度，用不同精度数据集训练PINNs，量化数据不一致性的影响

Result: PINNs能部分缓解低保真度数据并恢复主要物理结构，但训练最终会饱和于数据不一致性决定的误差水平；使用高保真数据时，PINNs能达到与解析数据训练相同的精度

Conclusion: 数据质量与物理约束的交互决定了PINNs的精度上限，一致性障碍概念为构建和解释物理信息代理模型提供了实用指导

Abstract: Physics-informed neural networks (PINNs) have gained significant attention as a surrogate modeling strategy for partial differential equations (PDEs), particularly in regimes where labeled data are scarce and physical constraints can be leveraged to regularize the learning process. In practice, however, PINNs are frequently trained using experimental or numerical data that are not fully consistent with the governing equations due to measurement noise, discretization errors, or modeling assumptions. The implications of such data-to-PDE inconsistencies on the accuracy and convergence of PINNs remain insufficiently understood. In this work, we systematically analyze how data inconsistency fundamentally limits the attainable accuracy of PINNs. We introduce the concept of a consistency barrier, defined as an intrinsic lower bound on the error that arises from mismatches between the fidelity of the data and the exact enforcement of the PDE residual. To isolate and quantify this effect, we consider the 1D viscous Burgers equation with a manufactured analytical solution, which enables full control over data fidelity and residual errors. PINNs are trained using datasets of progressively increasing numerical accuracy, as well as perfectly consistent analytical data. Results show that while the inclusion of the PDE residual allows PINNs to partially mitigate low-fidelity data and recover the dominant physical structure, the training process ultimately saturates at an error level dictated by the data inconsistency. When high-fidelity numerical data are employed, PINN solutions become indistinguishable from those trained on analytical data, indicating that the consistency barrier is effectively removed. These findings clarify the interplay between data quality and physics enforcement in PINNs providing practical guidance for the construction and interpretation of physics-informed surrogate models.

</details>


### [7] [Versor: A Geometric Sequence Architecture](https://arxiv.org/abs/2602.10195)
*Truong Minh Huy,Edward Hirst*

Main category: cs.LG

TL;DR: Versor是一种基于共形几何代数(CGA)的新型序列架构，用几何变换替代传统非线性操作，在多种任务上超越Transformer和几何基线，参数更少、复杂度更低、可解释性更强。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络架构在处理几何关系和结构泛化方面存在局限，需要显式编码SE(3)等几何对称性。Versor旨在通过几何代数实现内在的几何感知能力，提高模型在科学建模任务中的性能和可解释性。

Method: 将状态嵌入Cl_{4,1}流形，通过几何变换（转子）演化状态，使用递归转子累加器实现O(L)线性复杂度，开发自定义Clifford内核加速计算。

Result: 在混沌N体动力学、拓扑推理和多模态基准测试中均优于Transformer、图网络和几何基线；参数减少200倍；零样本尺度泛化达到99.3% MCC；自定义内核实现78倍加速；在分布外测试中保持稳定预测。

Conclusion: Versor通过几何代数提供了一种可扩展的几何感知建模框架，在性能、效率、可解释性和泛化能力方面显著优于现有方法，为科学计算和几何推理任务开辟了新方向。

Abstract: A novel sequence architecture design is introduced, Versor, which uses Conformal Geometric Algebra (CGA) in place of the traditional fundamental non-linear operations to achieve structural generalization and significant performance improvements on a variety of tasks, while offering improved interpretability and efficiency. By embedding states in the $Cl_{4,1}$ manifold and evolving them via geometric transformations (rotors), Versor natively represents $SE(3)$-equivariant relationships without requiring explicit structural encoding. Versor is validated on chaotic N-body dynamics, topological reasoning, and standard multimodal benchmarks (CIFAR-10, WikiText-103), consistently outperforming Transformers, Graph Networks, and geometric baselines (GATr, EGNN). Key results include: orders of magnitude fewer parameters ($200\times$ vs. Transformers); interpretable attention decomposing into proximity and orientational components; zero-shot scale generalization (99.3% MCC on topology vs. 50.4% for ViT); and $O(L)$ linear complexity via the novel Recursive Rotor Accumulator. In out-of-distribution tests, Versor maintains stable predictions while Transformers fail catastrophically. Custom Clifford kernels achieve up to $78\times$ speedup, providing a scalable foundation for geometrically-aware scientific modeling.

</details>


### [8] [Statistical Learning Analysis of Physics-Informed Neural Networks](https://arxiv.org/abs/2602.11097)
*David A. Barajas-Solano*

Main category: cs.LG

TL;DR: 论文从统计学习角度分析PINNs训练，将物理惩罚视为无限间接数据源而非正则项，使用奇异学习理论和局部学习系数分析热方程IBVP的参数估计，并讨论对PINNs预测不确定性和外推能力的影响。


<details>
  <summary>Details</summary>
Motivation: 现有PINNs研究多从数值角度出发，缺乏统计学习理论分析。本文旨在从统计学习视角重新理解PINNs训练过程，特别是物理惩罚项的作用机制，为PINNs的理论分析提供新框架。

Method: 1. 采用硬边界条件约束的参数化方法；2. 将PINNs参数估计重构为统计学习问题；3. 将物理惩罚视为无限间接数据源；4. 使用Kullback-Leibler散度最小化拟合分布；5. 应用奇异学习理论和局部学习系数分析热方程IBVP。

Result: 1. 证明PINNs物理学习是奇异学习问题；2. 通过局部学习系数成功分析随机优化得到的PINN参数估计；3. 为理解PINNs训练提供了新的统计学习框架。

Conclusion: 统计学习视角为PINNs提供了新的理论分析框架，物理惩罚应理解为无限间接数据源而非正则项。该分析对PINNs的预测不确定性量化和外推能力有重要启示，奇异学习理论是分析PINNs参数估计的有效工具。

Abstract: We study the training and performance of physics-informed learning for initial and boundary value problems (IBVP) with physics-informed neural networks (PINNs) from a statistical learning perspective. Specifically, we restrict ourselves to parameterizations with hard initial and boundary condition constraints and reformulate the problem of estimating PINN parameters as a statistical learning problem. From this perspective, the physics penalty on the IBVP residuals can be better understood not as a regularizing term bus as an infinite source of indirect data, and the learning process as fitting the PINN distribution of residuals $p(y \mid x, t, w) q(x, t) $ to the true data-generating distribution $δ(0) q(x, t)$ by minimizing the Kullback-Leibler divergence between the true and PINN distributions. Furthermore, this analysis show that physics-informed learning with PINNs is a singular learning problem, and we employ singular learning theory tools, namely the so-called Local Learning Coefficient (Lau et al., 2025) to analyze the estimates of PINN parameters obtained via stochastic optimization for a heat equation IBVP. Finally, we discuss implications of this analysis on the quantification of predictive uncertainty of PINNs and the extrapolation capacity of PINNs.

</details>


### [9] [Adaptive Optimization via Momentum on Variance-Normalized Gradients](https://arxiv.org/abs/2602.10204)
*Francisco Patitucci,Aryan Mokhtari*

Main category: cs.LG

TL;DR: MVN-Grad是一种Adam风格的优化器，通过方差归一化和归一化后应用动量来提高稳定性和性能，在多个基准测试中优于或匹配现有优化器。


<details>
  <summary>Details</summary>
Motivation: 标准Adam类优化器中存在过时动量与随机归一化器之间的跨时间耦合问题，这可能导致训练不稳定和性能下降。需要一种能解耦这种关系、提高稳定性的优化器。

Method: MVN-Grad结合了两个互补思想：1）基于方差的归一化，使用梯度不确定性的指数移动平均来缩放每个坐标；2）在归一化后的梯度上应用动量。这种方法消除了标准Adam更新中过时动量与随机归一化器之间的跨时间耦合。

Result: 理论证明：MVN-Grad在标准噪声假设下具有比动量先归一化方差方法更小的一步条件更新方差；对异常值具有鲁棒性，对单个梯度尖峰具有一致有界响应；在低方差情况下避免符号类型崩溃。实验表明：在CIFAR-100图像分类和GPT风格语言建模基准测试中，MVN-Grad匹配或优于Adam、AdaBelief和LaProp，提供更平滑的训练和更好的泛化性能，且无额外开销。

Conclusion: MVN-Grad通过解耦动量和归一化操作，提供了一种更稳定、性能更好的优化器，在理论和实验上都表现出优越性，有望成为深度学习训练中的有效替代方案。

Abstract: We introduce MVN-Grad (Momentum on Variance-Normalized Gradients), an Adam-style optimizer that improves stability and performance by combining two complementary ideas: variance-based normalization and momentum applied after normalization. MVN-Grad scales each coordinate by an exponential moving average of gradient uncertainty and applies momentum to the resulting normalized gradients, eliminating the cross-time coupling between stale momentum and a stochastic normalizer present in standard Adam-type updates. We prove that this decoupling yields strictly smaller one-step conditional update variance than momentum-then-normalize variance methods under standard noise assumptions, and that MVN-Grad is robust to outliers: it has a uniformly bounded response to single gradient spikes.
  In low-variance regimes, we further show variance normalization avoids sign-type collapse associated with second-moment scaling and can yield accelerated convergence. Across CIFAR-100 image classification and GPT-style language modeling benchmarks, MVN-Grad matches or outperforms Adam, AdaBelief, and LaProp, delivering smoother training and improved generalization with no added overhead.

</details>


### [10] [Neural Network Quantum Field Theory from Transformer Architectures](https://arxiv.org/abs/2602.10209)
*Dmitry S. Ageev,Yulia A. Ageeva*

Main category: cs.LG

TL;DR: 使用transformer注意力头构建欧几里得标量量子场论，通过随机网络参数平均计算n点关联函数，分析无限宽度极限下的非高斯统计特性


<details>
  <summary>Details</summary>
Motivation: 探索神经网络与量子场论之间的深层联系，特别是transformer架构如何自然地产生量子场论结构，为理解神经网络的内在统计特性提供新视角

Method: 提出NN-QFT框架，通过transformer注意力头构建欧几里得标量量子场论，使用随机softmax权重耦合不同宽度坐标，在无限宽度极限下分析场统计特性

Result: 单注意力头在无限宽度极限下保持非高斯统计特性；四点关联函数存在"独立性破坏"贡献；多注意力头求和时，连接的非高斯关联函数以1/N_h速率被抑制，在大头数极限下得到高斯NN-QFT

Conclusion: transformer注意力头自然地产生量子场论结构，单个头产生非高斯统计，而多头聚合趋向于高斯统计，为神经网络与量子场论的统一理解提供了新框架

Abstract: We propose a neural-network construction of Euclidean scalar quantum field theories from transformer attention heads, defining $n$-point correlators by averaging over random network parameters in the NN-QFT framework. For a single attention head, shared random softmax weights couple different width coordinates and induce non-Gaussian field statistics that persist in the infinite-width limit $d_k\to\infty$. We compute the two-point function in an attention-weight representation and show how Euclidean-invariant kernels can be engineered via random-feature token embeddings. We then analyze the connected four-point function and identify an "independence-breaking" contribution, expressible as a covariance over query-key weights, which remains finite at infinite width. Finally, we show that summing many independent heads with standard $1/N_h$ normalization suppresses connected non-Gaussian correlators as $1/N_h$, yielding a Gaussian NN-QFT in the large-head limit.

</details>


### [11] [How Much Reasoning Do Retrieval-Augmented Models Add beyond LLMs? A Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge](https://arxiv.org/abs/2602.10210)
*Junhong Lin,Bing Zhang,Song Wang,Ziyan Liu,Dan Gutfreund,Julian Shun,Yada Zhu*

Main category: cs.LG

TL;DR: 提出了HybridRAG-Bench框架，用于评估LLM在混合知识（非结构化文本+结构化知识图谱）上的检索密集型多跳推理能力，避免预训练数据污染问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试常与LLM预训练数据重叠，难以区分真正的检索推理与参数记忆。需要评估LLM在需要最新信息和多跳推理的知识密集型任务上的真实能力。

Method: 基于arXiv近期科学文献自动构建混合知识表示（非结构化文本+结构化知识图谱），生成基于显式推理路径的知识密集型问答对，支持灵活领域和时间范围选择。

Result: 在人工智能、治理与政策、生物信息学三个领域的实验表明，HybridRAG-Bench能有效评估真正的检索和推理能力，而非参数记忆，为混合知识增强推理系统提供原则性测试平台。

Conclusion: HybridRAG-Bench为解决LLM评估中的数据污染问题提供了有效框架，支持可定制、污染感知的评估，有助于推动混合知识增强推理系统的发展。

Abstract: Large language models (LLMs) continue to struggle with knowledge-intensive questions that require up-to-date information and multi-hop reasoning. Augmenting LLMs with hybrid external knowledge, such as unstructured text and structured knowledge graphs, offers a promising alternative to costly continual pretraining. As such, reliable evaluation of their retrieval and reasoning capabilities becomes critical. However, many existing benchmarks increasingly overlap with LLM pretraining data, which means answers or supporting knowledge may already be encoded in model parameters, making it difficult to distinguish genuine retrieval and reasoning from parametric recall. We introduce HybridRAG-Bench, a framework for constructing benchmarks to evaluate retrieval-intensive, multi-hop reasoning over hybrid knowledge. HybridRAG-Bench automatically couples unstructured text and structured knowledge graph representations derived from recent scientific literature on arXiv, and generates knowledge-intensive question-answer pairs grounded in explicit reasoning paths. The framework supports flexible domain and time-frame selection, enabling contamination-aware and customizable evaluation as models and knowledge evolve. Experiments across three domains (artificial intelligence, governance and policy, and bioinformatics) demonstrate that HybridRAG-Bench rewards genuine retrieval and reasoning rather than parametric recall, offering a principled testbed for evaluating hybrid knowledge-augmented reasoning systems. We release our code and data at github.com/junhongmit/HybridRAG-Bench.

</details>


### [12] [Rank-Accuracy Trade-off for LoRA: A Gradient-Flow Analysis](https://arxiv.org/abs/2602.10212)
*Michael Rushka,Diego Klabjan*

Main category: cs.LG

TL;DR: 本文从动力系统角度分析LoRA在微调任务中的精度与秩的关系，推导了梯度流方程并建立了秩与精度的闭式关系。


<details>
  <summary>Details</summary>
Motivation: 尽管实证研究表明LoRA在低秩更新（甚至秩1）时能达到与全参数方法相当的精度，但其精度与更新秩之间的理论关系尚未得到充分探索。本文旨在从动力系统角度填补这一理论空白。

Method: 采用梯度流分析方法，在全秩和低秩两种机制下进行理论分析。首先严格推导LoRA的梯度流方程，证明同时更新和顺序更新的等价性。然后基于得到的动力系统方程，针对迹平方和Frobenius范数低秩逼近两种损失函数，建立LoRA秩与精度的闭式关系。

Result: 推导出了LoRA梯度流方程的严格形式，证明了同时更新和顺序更新的等价性。针对两种损失函数（迹平方和Frobenius范数低秩逼近），建立了LoRA秩与精度的闭式数学关系，为理解低秩更新的精度表现提供了理论依据。

Conclusion: 本文从动力系统角度为LoRA的精度与秩关系提供了理论分析框架，推导出的闭式关系有助于理解低秩更新在微调任务中的有效性，为LoRA的理论基础做出了贡献。

Abstract: Previous empirical studies have shown that LoRA achieves accuracy comparable to full-parameter methods on downstream fine-tuning tasks, even for rank-1 updates. By contrast, the theoretical underpinnings of the dependence of LoRA's accuracy on update rank remain relatively unexplored. In this work, we compare the accuracy of rank-r LoRA updates against full-parameter updates for fine-tuning tasks from a dynamical systems perspective. We perform gradient flow analysis in both full-rank and low-rank regimes to establish explicit relationships between rank and accuracy for two loss functions under LoRA. While gradient flow equations for LoRA are presented in prior work, we rigorously derive their form and show that they are identical for simultaneous and sequential LoRA parameter updates. We then use the resulting dynamical system equations to obtain closed-form relationships between LoRA rank and accuracy for trace-squared and Frobenius-norm low-rank approximation loss functions.

</details>


### [13] [ELROND: Exploring and decomposing intrinsic capabilities of diffusion models](https://arxiv.org/abs/2602.10216)
*Paweł Skierś,Tomasz Trzciński,Kamil Deja*

Main category: cs.LG

TL;DR: 提出一种在扩散模型输入嵌入空间中解耦语义方向的方法，通过分析随机实现之间的梯度差异来获得可解释、可控制的编辑方向


<details>
  <summary>Details</summary>
Motivation: 扩散模型对同一文本提示会生成多种视觉输出，这些变化由随机过程决定，用户无法直接控制具体的语义变化。现有无监督方法通过输出特征分析这些变化，但忽略了底层的生成过程。

Method: 收集固定提示下不同随机实现之间的梯度差异，通过反向传播获得梯度集合，然后使用主成分分析或稀疏自编码器将这些梯度分解为有意义的控制方向

Result: 方法能够：(1) 分离出可解释、可控制的方向，实现对单一概念的精确细粒度控制；(2) 有效缓解蒸馏模型中的模式崩溃问题，恢复丢失的多样性；(3) 基于发现子空间的维度，建立概念复杂度的新估计器

Conclusion: 提出了一种在扩散模型输入嵌入空间中直接解耦语义方向的新框架，为文本到图像生成提供了更精确的控制能力，并建立了概念复杂度的量化方法

Abstract: A single text prompt passed to a diffusion model often yields a wide range of visual outputs determined solely by stochastic process, leaving users with no direct control over which specific semantic variations appear in the image. While existing unsupervised methods attempt to analyze these variations via output features, they omit the underlying generative process. In this work, we propose a framework to disentangle these semantic directions directly within the input embedding space. To that end, we collect a set of gradients obtained by backpropagating the differences between stochastic realizations of a fixed prompt that we later decompose into meaningful steering directions with either Principal Components Analysis or Sparse Autoencoder. Our approach yields three key contributions: (1) it isolates interpretable, steerable directions for precise, fine-grained control over a single concept; (2) it effectively mitigates mode collapse in distilled models by reintroducing lost diversity; and (3) it establishes a novel estimator for concept complexity under a specific model, based on the dimensionality of the discovered subspace.

</details>


### [14] [Temper-Then-Tilt: Principled Unlearning for Generative Models through Tempering and Classifier Guidance](https://arxiv.org/abs/2602.10217)
*Jacob L. Block,Mehryar Mohri,Aryan Mokhtari,Sanjay Shakkottai*

Main category: cs.LG

TL;DR: T3-Unlearning：通过温度调节与倾斜的两步推理方法，在大生成模型中实现机器遗忘，避免分类器引导在集中分布上的失败


<details>
  <summary>Details</summary>
Motivation: 现有基于分类器引导的机器遗忘方法在处理集中分布（sharp, concentrated data distribution）的遗忘集时，在有限样本下可能失败，无法忠实实现遗忘

Method: T3-Unlearning：冻结基础模型，采用两步推理过程：1) 温度调节（tempering）基础分布以平滑高置信度峰值；2) 使用轻量级分类器（区分保留与遗忘样本）倾斜调节后的分布

Result: 在TOFU基准测试中，T3-Unlearning在遗忘质量和生成效用方面优于现有基线方法，同时仅训练少量参数且运行时间最小

Conclusion: 通过理论分析证明温度调节对于集中分布的成功遗忘是必要的，T3-Unlearning为大规模生成模型的机器遗忘提供了高效可靠的解决方案

Abstract: We study machine unlearning in large generative models by framing the task as density ratio estimation to a target distribution rather than supervised fine-tuning. While classifier guidance is a standard approach for approximating this ratio and can succeed in general, we show it can fail to faithfully unlearn with finite samples when the forget set represents a sharp, concentrated data distribution. To address this, we introduce Temper-Then-Tilt Unlearning (T3-Unlearning), which freezes the base model and applies a two-step inference procedure: (i) tempering the base distribution to flatten high-confidence spikes, and (ii) tilting the tempered distribution using a lightweight classifier trained to distinguish retain from forget samples. Our theoretical analysis provides finite-sample guarantees linking the surrogate classifier's risk to unlearning error, proving that tempering is necessary to successfully unlearn for concentrated distributions. Empirical evaluations on the TOFU benchmark show that T3-Unlearning improves forget quality and generative utility over existing baselines, while training only a fraction of the parameters with a minimal runtime.

</details>


### [15] [Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2602.10224)
*Shiting Huang,Zecheng Li,Yu Zeng,Qingnan Ren,Zhen Fang,Qisheng Su,Kou Shi,Lin Chen,Zehui Chen,Feng Zhao*

Main category: cs.LG

TL;DR: MEL框架通过自蒸馏元经验增强RLVR，利用LLM自验证能力对比分析正确/错误轨迹，识别推理错误分岔点，形成可重用元经验并内化到参数记忆，提升推理性能。


<details>
  <summary>Details</summary>
Motivation: RLVR虽能增强LLM推理能力，但缺乏错误归因和经验内化机制，限制了细粒度信用分配和可重用知识形成，存在元学习瓶颈。

Method: 基于RLVR引入元经验学习框架，利用LLM自验证能力对比正确/错误轨迹，识别推理错误分岔点，总结为可泛化元经验，通过最小化负对数似然内化到参数记忆。

Result: MEL在不同模型规模上实现一致改进，在基准测试中获得3.92%-4.73%的Pass@1提升。

Conclusion: MEL通过元经验内化有效克服RLVR的元学习瓶颈，实现了细粒度信用分配和知识重用，显著提升LLM推理能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach for enhancing the reasoning capabilities of Large Language Models (LLMs). Despite its efficacy, RLVR faces a meta-learning bottleneck: it lacks mechanisms for error attribution and experience internalization intrinsic to the human learning cycle beyond practice and verification, thereby limiting fine-grained credit assignment and reusable knowledge formation. We term such reusable knowledge representations derived from past errors as meta-experience. Based on this insight, we propose Meta-Experience Learning (MEL), a novel framework that incorporates self-distilled meta-experience into the model's parametric memory. Building upon standard RLVR, we introduce an additional design that leverages the LLM's self-verification capability to conduct contrastive analysis on paired correct and incorrect trajectories, identify the precise bifurcation points where reasoning errors arise, and summarize them into generalizable meta-experience. The meta-experience is further internalized into the LLM's parametric memory by minimizing the negative log-likelihood, which induces a language-modeled reward signal that bridges correct and incorrect reasoning trajectories and facilitates effective knowledge reuse. Experimental results demonstrate that MEL achieves consistent improvements on benchmarks, yielding 3.92%--4.73% Pass@1 gains across varying model sizes.

</details>


### [16] [Self-Evolving Recommendation System: End-To-End Autonomous Model Optimization With LLM Agents](https://arxiv.org/abs/2602.10226)
*Haochen Wang,Yi Wu,Daryl Chang,Li Wei,Lukasz Heldt*

Main category: cs.LG

TL;DR: 论文提出一个基于LLM的自进化系统，能够自动生成、训练和部署YouTube推荐模型改进，超越传统人工工作流程


<details>
  <summary>Details</summary>
Motivation: 优化大规模机器学习系统（如全球视频平台推荐模型）需要探索巨大的超参数空间，并设计复杂的优化器、架构和奖励函数来捕捉细微的用户行为。传统方法依赖大量手动迭代来测试新假设，这是一个非平凡的任务。

Method: 提出一个基于Google Gemini系列LLM的自进化系统，包含离线代理（内循环）和在线代理（外循环）。离线代理使用代理指标进行高吞吐量假设生成，在线代理在实时生产环境中根据延迟的北极星业务指标验证候选方案。这些代理作为专门的机器学习工程师，具备深度推理能力。

Result: 该系统在YouTube成功进行了多次生产发布，证明自主的LLM驱动进化在开发速度和模型性能上都超越了传统工程工作流程。

Conclusion: 基于LLM的自进化系统能够自主发现优化算法、模型架构的创新改进，并制定针对长期用户参与度的创新奖励函数，为大规模机器学习系统优化提供了有效的新方法。

Abstract: Optimizing large-scale machine learning systems, such as recommendation models for global video platforms, requires navigating a massive hyperparameter search space and, more critically, designing sophisticated optimizers, architectures, and reward functions to capture nuanced user behaviors. Achieving substantial improvements in these areas is a non-trivial task, traditionally relying on extensive manual iterations to test new hypotheses. We propose a self-evolving system that leverages Large Language Models (LLMs), specifically those from Google's Gemini family, to autonomously generate, train, and deploy high-performing, complex model changes within an end-to-end automated workflow. The self-evolving system is comprised of an Offline Agent (Inner Loop) that performs high-throughput hypothesis generation using proxy metrics, and an Online Agent (Outer Loop) that validates candidates against delayed north star business metrics in live production. Our agents act as specialized Machine Learning Engineers (MLEs): they exhibit deep reasoning capabilities, discovering novel improvements in optimization algorithms and model architecture, and formulating innovative reward functions that target long-term user engagement. The effectiveness of this approach is demonstrated through several successful production launches at YouTube, confirming that autonomous, LLM-driven evolution can surpass traditional engineering workflows in both development velocity and model performance.

</details>


### [17] [PRISM: Differentially Private Synthetic Data with Structure-Aware Budget Allocation for Prediction](https://arxiv.org/abs/2602.10228)
*Amir Asiaee,Chao Yan,Zachary B. Abrams,Bradley A. Malin*

Main category: cs.LG

TL;DR: PRISM：一种预测中心化的差分隐私合成数据方法，根据可用结构知识（因果、图、预测三种机制）选择特征子集，针对性分配隐私预算以最小化预测误差。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私合成数据方法对所有特征对称处理，均匀添加噪声，即使数据用于特定预测任务。这导致噪声累积，预测性能下降，特别是在存在因果结构或分布偏移时。

Method: PRISM机制：1) 根据三种机制（因果、图、预测）识别预测特征子集；2) 构建针对性汇总统计；3) 分配隐私预算以最小化预测误差上界；4) 通过图模型推断合成数据。提供端到端隐私保证和风险界限。

Result: 任务感知的预算分配相比通用合成器提高了预测准确性。在分布偏移下，针对因果父节点的选择达到AUC≈0.73，而基于相关性的选择崩溃至随机水平（≈0.49）。

Conclusion: PRISM通过预测中心化方法，根据可用结构知识选择性保护关键特征，在保持差分隐私的同时显著提升预测性能，特别是在存在因果结构或分布偏移的场景中。

Abstract: Differential privacy (DP) provides a mathematical guarantee limiting what an adversary can learn about any individual from released data. However, achieving this protection typically requires adding noise, and noise can accumulate when many statistics are measured. Existing DP synthetic data methods treat all features symmetrically, spreading noise uniformly even when the data will serve a specific prediction task.
  We develop a prediction-centric approach operating in three regimes depending on available structural knowledge. In the causal regime, when the causal parents of $Y$ are known and distribution shift is expected, we target the parents for robustness. In the graphical regime, when a Bayesian network structure is available and the distribution is stable, the Markov blanket of $Y$ provides a sufficient feature set for optimal prediction. In the predictive regime, when no structural knowledge exists, we select features via differentially private methods without claiming to recover causal or graphical structure.
  We formalize this as PRISM, a mechanism that (i) identifies a predictive feature subset according to the appropriate regime, (ii) constructs targeted summary statistics, (iii) allocates budget to minimize an upper bound on prediction error, and (iv) synthesizes data via graphical-model inference. We prove end-to-end privacy guarantees and risk bounds. Empirically, task-aware allocation improves prediction accuracy compared to generic synthesizers. Under distribution shift, targeting causal parents achieves AUC $\approx 0.73$ while correlation-based selection collapses to chance ($\approx 0.49$).

</details>


### [18] [Frame-Level Internal Tool Use for Temporal Grounding in Audio LMs](https://arxiv.org/abs/2602.10230)
*Joesph An,Phillip Keung,Jiaqi Wang,Orevaoghene Ahia,Noah A. Smith*

Main category: cs.LG

TL;DR: 提出帧级内部工具使用，让音频语言模型利用自身内部音频表示直接进行时间定位，替代传统基于文本token的时间戳生成方法，显著提升推理速度和长度泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型音频语言模型在处理需要精确时间定位的任务（如词对齐、说话人分离）时存在困难。传统的基于文本token生成时间戳的方法计算成本高且容易产生幻觉，特别是在处理超出训练分布长度的音频时。

Method: 提出帧级内部工具使用，训练音频语言模型利用自身内部音频表示直接进行时间定位。引入轻量级预测机制，通过两个目标训练：二元帧分类器和新型非齐次泊松过程损失来建模时间事件强度。

Result: 在词定位、说话人分离和事件定位任务上优于基于token的基线方法。最显著的是实现了>50倍的推理加速，并在分布外音频长度上表现出强大的长度泛化能力，而标准基于token的模型在这些情况下完全失效。

Conclusion: 帧级内部工具使用为音频语言模型的时间定位任务提供了一种高效且鲁棒的解决方案，克服了传统token生成方法的计算成本和幻觉问题，特别是在处理长音频时表现出优越的性能。

Abstract: Large audio language models are increasingly used for complex audio understanding tasks, but they struggle with temporal tasks that require precise temporal grounding, such as word alignment and speaker diarization. The standard approach, where we generate timestamps as sequences of text tokens, is computationally expensive and prone to hallucination, especially when processing audio lengths outside the model's training distribution. In this work, we propose frame-level internal tool use, a method that trains audio LMs to use their own internal audio representations to perform temporal grounding directly. We introduce a lightweight prediction mechanism trained via two objectives: a binary frame classifier and a novel inhomogeneous Poisson process (IHP) loss that models temporal event intensity. Across word localization, speaker diarization, and event localization tasks, our approach outperforms token-based baselines. Most notably, it achieves a >50x inference speedup and demonstrates robust length generalization, maintaining high accuracy on out-of-distribution audio durations where standard token-based models collapse completely.

</details>


### [19] [Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards](https://arxiv.org/abs/2602.10231)
*Kirill Pavlenko,Alexander Golubev,Simon Karasik,Boris Yangel*

Main category: cs.LG

TL;DR: 提出Blockwise Advantage Estimation方法，解决GRPO在结构化生成中多目标优化时的奖励干扰问题，通过为每个文本块分配独立优势值，避免跨段奖励信号耦合。


<details>
  <summary>Details</summary>
Motivation: GRPO方法为整个生成内容分配单一标量优势值，在结构化生成（包含明确段落和目标）中，会将不相关的奖励信号耦合在一起，导致目标干扰和信用分配错误。

Method: 提出Blockwise Advantage Estimation方法族，为每个目标分配独立的优势值，并仅应用于对应文本块的标记。引入Outcome-Conditioned Baseline，通过基于前缀派生的中间结果对样本进行分层，仅使用组内统计量近似中间状态值，避免昂贵的嵌套rollout。

Result: 在数学任务的不确定性估计中，该方法缓解了奖励干扰，与最先进的奖励设计方法竞争，并保留了置信度加权集成在测试时的增益。

Conclusion: 该方法为优化结构化生成中的顺序目标提供了模块化方案，无需额外rollout，可自然扩展到更多目标，减少对手工设计标量奖励的依赖。

Abstract: Group Relative Policy Optimization (GRPO) assigns a single scalar advantage to all tokens in a completion. For structured generations with explicit segments and objectives, this couples unrelated reward signals across segments, leading to objective interference and misattributed credit. We propose Blockwise Advantage Estimation, a family of GRPO-compatible methods that assigns each objective its own advantage and applies it only to the tokens in the corresponding text block, reducing reliance on hand-designed scalar rewards and scaling naturally to additional objectives. A key challenge is estimating advantages for later blocks whose rewards are conditioned on sampled prefixes; standard unbiased approaches require expensive nested rollouts from intermediate states. Concretely, we introduce an Outcome-Conditioned Baseline that approximates intermediate state values using only within-group statistics by stratifying samples according to a prefix-derived intermediate outcome. On math tasks with uncertainty estimation, our method mitigates reward interference, is competitive with a state-of-the-art reward-designed approach, and preserves test-time gains from confidence-weighted ensembling. More broadly, it provides a modular recipe for optimizing sequential objectives in structured generations without additional rollouts.

</details>


### [20] [Risk-Equalized Differentially Private Synthetic Data: Protecting Outliers by Controlling Record-Level Influence](https://arxiv.org/abs/2602.10232)
*Amir Asiaee,Chao Yan,Zachary B. Abrams,Bradley A. Malin*

Main category: cs.LG

TL;DR: 风险均衡差分隐私合成框架，通过降低高风险记录对生成器的影响，为异常值提供更强的隐私保护


<details>
  <summary>Details</summary>
Motivation: 传统差分隐私提供最坏情况保证，但实际攻击（特别是成员推理攻击）对异常记录成功率更高，需要为高风险记录提供针对性保护

Method: 两阶段框架：1）用小隐私预算估计每条记录的"异常度"；2）差分隐私学习过程根据风险分数反比加权记录，减少异常值对输出的影响

Result: 在模拟数据上，风险加权显著降低了针对高异常度记录的成员推理成功率；在真实数据集（乳腺癌、成人、德国信贷）上效果因数据集而异

Conclusion: 风险均衡差分隐私合成通过针对性降低异常记录的影响，为高风险个体提供更强的隐私保护，但效果取决于评分器质量和合成流程的交互

Abstract: When synthetic data is released, some individuals are harder to protect than others. A patient with a rare disease combination or a transaction with unusual characteristics stands out from the crowd. Differential privacy provides worst-case guarantees, but empirical attacks -- particularly membership inference -- succeed far more often against such outliers, especially under moderate privacy budgets and with auxiliary information.
  This paper introduces risk-equalized DP synthesis, a framework that prioritizes protection for high-risk records by reducing their influence on the learned generator. The mechanism operates in two stages: first, a small privacy budget estimates each record's "outlierness"; second, a DP learning procedure weights each record inversely to its risk score. Under Gaussian mechanisms, a record's privacy loss is proportional to its influence on the output -- so deliberately shrinking outliers' contributions yields tighter per-instance privacy bounds for precisely those records that need them most.
  We prove end-to-end DP guarantees via composition and derive closed-form per-record bounds for the synthesis stage (the scoring stage adds a uniform per-record term). Experiments on simulated data with controlled outlier injection show that risk-weighting substantially reduces membership inference success against high-outlierness records; ablations confirm that targeting -- not random downweighting -- drives the improvement. On real-world benchmarks (Breast Cancer, Adult, German Credit), gains are dataset-dependent, highlighting the interplay between scorer quality and synthesis pipeline.

</details>


### [21] [Modeling Programming Skills with Source Code Embeddings for Context-aware Exercise Recommendation](https://arxiv.org/abs/2602.10249)
*Carlos Eduardo P. Silva,João Pedro M. Sena,Julio C. S. Reis,André G. Santos,Lucas N. Ferreira*

Main category: cs.LG

TL;DR: 提出基于源代码嵌入的学生编程技能建模方法，通过余弦相似度匹配学生技能与作业要求，为编程课程提供个性化练习推荐。


<details>
  <summary>Details</summary>
Motivation: 传统基于正确率或解题时间的推荐方法无法准确反映学生的编程技能水平，需要更细粒度的技能建模来提供个性化练习推荐。

Method: 使用Jina嵌入对学生的提交代码进行向量化表示，预测学生在多个编程主题上的技能水平，通过余弦相似度计算学生技能向量与作业技能向量的匹配度进行推荐。

Result: Jina嵌入在技能预测上优于TF-IDF、CodeBERT-cpp和GraphCodeBERT；基于技能预测的推荐系统比基于正确率或解题时间的基线方法产生更合适的练习推荐。

Conclusion: 源代码嵌入能有效建模学生编程技能，基于技能预测的推荐方法优于传统方法，为编程教育提供更精准的个性化学习支持。

Abstract: In this paper, we propose a context-aware recommender system that models students' programming skills using embeddings of the source code they submit throughout a course. These embeddings predict students' skills across multiple programming topics, producing profiles that are matched to the skills required by unseen homework problems. To generate recommendations, we compute the cosine similarity between student profiles and problem skill vectors, ranking exercises according to their alignment with each student's current abilities. We evaluated our approach using real data from students and exercises in an introductory programming course at our university. First, we assessed the effectiveness of our source code embeddings for predicting skills, comparing them with token-based and graph-based alternatives. Results showed that Jina embeddings outperformed TF-IDF, CodeBERT-cpp, and GraphCodeBERT across most skills. Additionally, we evaluated the system's ability to recommend exercises aligned with weekly course content by analyzing student submissions collected over seven course offerings. Our approach consistently produced more suitable recommendations than baselines based on correctness or solution time, indicating that predicted programming skills provide a stronger signal for problem recommendation.

</details>


### [22] [Kernel-Based Learning of Chest X-ray Images for Predicting ICU Escalation among COVID-19 Patients](https://arxiv.org/abs/2602.10261)
*Qiyuan Shi,Jian Kang,Yi Li*

Main category: cs.LG

TL;DR: 提出GLIMARK方法，将多核学习扩展到指数族分布，用于处理多种数据类型，并在COVID-19胸部X光数据中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统单核方法受限于单一核函数类型，无法充分表示现实数据的异质性；传统多核学习方法主要针对连续结果，需要扩展到更广泛的数据类型。

Method: 提出GLIMARK方法，将多核学习扩展到指数族分布，通过构建复合核函数并整合异构数据源信息，适用于广义线性模型框架。

Result: GLIMARK能有效恢复或近似真实数据生成机制，在COVID-19胸部X光数据集中成功预测ICU升级的二元结果，并提取出具有临床意义的特征。

Conclusion: GLIMARK扩展了多核学习的适用范围，为处理多种数据类型提供了有效工具，在实际医疗场景中展现出实用价值。

Abstract: Kernel methods have been extensively utilized in machine learning for classification and prediction tasks due to their ability to capture complex non-linear data patterns. However, single kernel approaches are inherently limited, as they rely on a single type of kernel function (e.g., Gaussian kernel), which may be insufficient to fully represent the heterogeneity or multifaceted nature of real-world data. Multiple kernel learning (MKL) addresses these limitations by constructing composite kernels from simpler ones and integrating information from heterogeneous sources. Despite these advances, traditional MKL methods are primarily designed for continuous outcomes. We extend MKL to accommodate the outcome variable belonging to the exponential family, representing a broader variety of data types, and refer to our proposed method as generalized linear models with integrated multiple additive regression with kernels (GLIMARK). Empirically, we demonstrate that GLIMARK can effectively recover or approximate the true data-generating mechanism. We have applied it to a COVID-19 chest X-ray dataset, predicting binary outcomes of ICU escalation and extracting clinically meaningful features, underscoring the practical utility of this approach in real-world scenarios.

</details>


### [23] [From Classical to Topological Neural Networks Under Uncertainty](https://arxiv.org/abs/2602.10266)
*Sarah Harkins Dayton,Layal Bou Hamdan,Ioannis D. Schizas,David L. Boothe,Vasileios Maroulas*

Main category: cs.LG

TL;DR: 本章探讨神经网络、拓扑数据分析、拓扑深度学习技术及统计贝叶斯方法，用于处理图像、时间序列和图数据，以最大化人工智能在军事领域的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索如何将拓扑感知和不确定性感知模型应用于军事领域，以增强人工智能系统的鲁棒性、可解释性和泛化能力。

Method: 结合神经网络、拓扑数据分析(TDA)、拓扑深度学习技术以及统计贝叶斯方法，处理图像、时间序列和图数据。

Result: 展示了拓扑感知和不确定性感知模型在图像、视频、音频、时间序列识别、欺诈检测和图数据链接预测等实际应用中的有效性。

Conclusion: 拓扑感知和不确定性感知模型能够显著提升军事领域人工智能应用的鲁棒性、可解释性和泛化能力，具有重要的实用价值。

Abstract: This chapter explores neural networks, topological data analysis, and topological deep learning techniques, alongside statistical Bayesian methods, for processing images, time series, and graphs to maximize the potential of artificial intelligence in the military domain. Throughout the chapter, we highlight practical applications spanning image, video, audio, and time-series recognition, fraud detection, and link prediction for graphical data, illustrating how topology-aware and uncertainty-aware models can enhance robustness, interpretability, and generalization.

</details>


### [24] [Linear-LLM-SCM: Benchmarking LLMs for Coefficient Elicitation in Linear-Gaussian Causal Models](https://arxiv.org/abs/2602.10282)
*Kanta Yamaoka,Sumantrak Mukherjee,Thomas Gärtner,David Antony Selby,Stefan Konigorski,Eyke Hüllermeier,Viktor Bengs,Sebastian Josef Vollmer*

Main category: cs.LG

TL;DR: Linear-LLM-SCM是一个评估大语言模型在线性高斯结构因果模型参数化能力的基准框架，研究发现LLMs在连续域定量因果推理中存在显著挑战。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在识别定性因果关系方面已显示潜力，但它们在连续域中执行定量因果推理（估计参数化函数关系的效应大小）的能力尚未得到充分探索。

Method: 提出Linear-LLM-SCM框架，将DAG分解为局部父子节点集，提示LLM为每个节点生成回归式结构方程，然后聚合并与真实参数比较。

Result: 实验显示多个挑战：某些模型结果具有强随机性，在连续域中容易受到虚假边导致的DAG错误设定影响。不同模型间系数估计存在显著变异性，对结构和语义扰动敏感。

Conclusion: LLMs作为定量因果参数化工具目前存在局限性，框架已开源供研究者轻松使用自己的DAG和现成LLMs进行评估。

Abstract: Large language models (LLMs) have shown potential in identifying qualitative causal relations, but their ability to perform quantitative causal reasoning -- estimating effect sizes that parametrize functional relationships -- remains underexplored in continuous domains. We introduce Linear-LLM-SCM, a plug-and-play benchmarking framework for evaluating LLMs on linear Gaussian structural causal model (SCM) parametrization when the DAG is given. The framework decomposes a DAG into local parent-child sets and prompts an LLM to produce a regression-style structural equation per node, which is aggregated and compared against available ground-truth parameters. Our experiments show several challenges in such benchmarking tasks, namely, strong stochasticity in the results in some of the models and susceptibility to DAG misspecification via spurious edges in the continuous domains. Across models, we observe substantial variability in coefficient estimates for some settings and sensitivity to structural and semantic perturbations, highlighting current limitations of LLMs as quantitative causal parameterizers. We also open-sourced the benchmarking framework so that researchers can utilize their DAGs and any off-the-shelf LLMs plug-and-play for evaluation in their domains effortlessly.

</details>


### [25] [What Does Preference Learning Recover from Pairwise Comparison Data?](https://arxiv.org/abs/2602.10286)
*Rattana Pukdee,Maria-Florina Balcan,Pradeep Ravikumar*

Main category: cs.LG

TL;DR: 本文分析了Bradley-Terry模型在偏好学习中的适用性，指出真实数据可能违反模型假设，并建立了基于条件偏好分布的理论框架来理解偏好学习实际恢复的内容。


<details>
  <summary>Details</summary>
Motivation: Bradley-Terry模型是偏好学习的主流方法，但真实数据可能违反其假设。目前不清楚当数据不符合模型假设时，BT学习实际恢复的是什么内容。本文旨在为偏好学习提供数据中心的理​​论基础。

Method: 从三元组比较数据出发，形式化定义了条件偏好分布(CPRD)来编码偏好信息。给出了BT模型适用于建模CPRD的精确条件，并识别了影响样本效率的关键因素：边界和连通性。

Result: 建立了理解偏好学习实际恢复内容的理论框架，明确了BT模型适用的条件，并识别了影响学习效率的关键因素。

Conclusion: 本文为偏好学习提供了数据中心的理​​论基础，帮助理解当数据不符合Bradley-Terry模型假设时，偏好学习实际恢复的是什么内容，这对语言模型对齐等应用具有重要意义。

Abstract: Pairwise preference learning is central to machine learning, with recent applications in aligning language models with human preferences. A typical dataset consists of triplets $(x, y^+, y^-)$, where response $y^+$ is preferred over response $y^-$ for context $x$. The Bradley--Terry (BT) model is the predominant approach, modeling preference probabilities as a function of latent score differences. Standard practice assumes data follows this model and learns the latent scores accordingly. However, real data may violate this assumption, and it remains unclear what BT learning recovers in such cases. Starting from triplet comparison data, we formalize the preference information it encodes through the conditional preference distribution (CPRD). We give precise conditions for when BT is appropriate for modeling the CPRD, and identify factors governing sample efficiency -- namely, margin and connectivity. Together, these results offer a data-centric foundation for understanding what preference learning actually recovers.

</details>


### [26] [Configuration-to-Performance Scaling Law with Neural Ansatz](https://arxiv.org/abs/2602.10300)
*Huaqing Zhang,Kaiyue Wen,Tengyu Ma*

Main category: cs.LG

TL;DR: 提出NCPL（神经配置-性能缩放定律），使用LLM参数化从完整训练配置到训练性能的映射，相比Chinchilla定律预测误差降低20-40%，能泛化到训练集10倍计算量的运行。


<details>
  <summary>Details</summary>
Motivation: 现有缩放定律假设超参数已最优选择，这需要大量调优工作且受硬件限制。需要更广泛的超参数可预测性和简化大规模调优的方法。

Method: 提出配置-性能缩放定律（CPL），使用大语言模型（LLM）参数化从完整训练配置到训练性能的映射，利用多来源的开源预训练日志进行拟合，得到神经配置-性能缩放定律（NCPL）。

Result: NCPL能准确预测训练配置对最终预训练损失的影响，比配置无关的Chinchilla定律预测误差降低20-40%，能泛化到训练集10倍计算量的运行，支持多超参数联合调优，并能扩展到更丰富的预测目标如损失曲线预测。

Conclusion: NCPL通过LLM参数化配置-性能映射，显著提高了训练性能的可预测性，简化了大规模调优，并能扩展到更复杂的预测任务。

Abstract: Researchers build scaling laws to forecast the training performance of expensive large-scale runs with larger model size N and data size D. These laws assume that other training hyperparameters are optimally chosen, which can require significant effort and, in some cases, be impossible due to external hardware constraints. To improve predictability across a broader set of hyperparameters and enable simpler tuning at scale, we propose learning a \textit{Configuration-to-Performance Scaling Law} (CPL): a mapping from the \textit{full training configuration} to training performance. Because no simple functional form can express this mapping, we parameterize it with a large language model (LLM), and fit it with diverse open-source pretraining logs across multiple sources, yielding a \textit{Neural} Configuration-to-Performance Scaling Law (NCPL). NCPL accurately predicts how training configurations influence the final pretraining loss, achieving 20-40% lower prediction error than the configuration-agnostic Chinchilla law and generalizing to runs using up to 10 x more compute than any run in the training set. It further supports joint tuning of multiple hyperparameters with performance comparable to hyperparameter scaling law baselines. Finally, NCPL naturally and effectively extends to richer prediction targets such as loss-curve prediction.

</details>


### [27] [ICODEN: Ordinary Differential Equation Neural Networks for Interval-Censored Data](https://arxiv.org/abs/2602.10303)
*Haoling Wang,Lang Zeng,Tao Sun,Youngjoo Cho,Ying Ding*

Main category: cs.LG

TL;DR: ICOden是一个基于常微分方程的神经网络方法，用于处理区间删失生存数据，无需比例风险假设或风险函数的参数形式，适用于高维预测变量。


<details>
  <summary>Details</summary>
Motivation: 区间删失生存数据分析具有挑战性，因为确切事件时间未知。现有方法要么依赖强模型假设，要么无法处理高维预测变量。需要一种灵活、假设宽松的方法来处理高维生物医学数据中的区间删失生存预测问题。

Method: ICOden使用深度神经网络建模风险函数，通过求解常微分方程获得累积风险函数。该方法不要求比例风险假设，也不预设风险函数的参数形式，允许灵活的生存建模。

Result: 在比例风险和非比例风险、线性和非线性协变量效应的模拟设置中，ICOden始终获得满意的预测准确性，且随着预测变量数量增加保持稳定。在ADNI和AREDS/AREDS2的实际应用中，ICOden能有效利用数百到上千个SNPs进行预测，并支持数据驱动的亚组识别。

Conclusion: ICOden是一个实用的假设宽松工具，适用于高维生物医学环境中区间删失生存数据的预测，为AD和AMD等疾病的时间到事件预测提供了有效方法。

Abstract: Predicting time-to-event outcomes when event times are interval censored is challenging because the exact event time is unobserved. Many existing survival analysis approaches for interval-censored data rely on strong model assumptions or cannot handle high-dimensional predictors. We develop ICODEN, an ordinary differential equation-based neural network for interval-censored data that models the hazard function through deep neural networks and obtains the cumulative hazard by solving an ordinary differential equation. ICODEN does not require the proportional hazards assumption or a prespecified parametric form for the hazard function, thereby permitting flexible survival modeling. Across simulation settings with proportional or non-proportional hazards and both linear and nonlinear covariate effects, ICODEN consistently achieves satisfactory predictive accuracy and remains stable as the number of predictors increases. Applications to data from multiple phases of the Alzheimer's Disease Neuroimaging Initiative (ADNI) and to two Age-Related Eye Disease Studies (AREDS and AREDS2) for age-related macular degeneration (AMD) demonstrate ICODEN's robust prediction performance. In both applications, predicting time-to-AD or time-to-late AMD, ICODEN effectively uses hundreds to more than 1,000 SNPs and supports data-driven subgroup identification with differential progression risk profiles. These results establish ICODEN as a practical assumption-lean tool for prediction with interval-censored survival data in high-dimensional biomedical settings.

</details>


### [28] [Confounding Robust Continuous Control via Automatic Reward Shaping](https://arxiv.org/abs/2602.10305)
*Mateo Juliani,Mingxuan Li,Elias Bareinboim*

Main category: cs.LG

TL;DR: 提出一种从离线数据中自动学习奖励塑形函数的方法，用于连续控制问题，能够处理未观测的混杂变量，基于因果贝尔曼方程学习最优状态值的紧上界作为PBRS框架中的势函数。


<details>
  <summary>Details</summary>
Motivation: 奖励塑形广泛用于加速强化学习训练，但设计有效的奖励塑形函数（特别是复杂连续控制问题）缺乏原则性方法。现有方法难以处理离线数据中可能存在的未观测混杂变量。

Method: 基于因果贝尔曼方程学习最优状态值的紧上界，将其作为势函数用于基于势的奖励塑形框架。使用离线数据集自动学习奖励塑形函数，特别设计用于处理未观测的混杂变量。

Result: 在多个常用连续控制基准测试中使用Soft-Actor-Critic进行测试，在未观测混杂变量下表现出强大的性能保证。代码已开源。

Conclusion: 该方法是从因果视角实现混杂鲁棒连续控制的重要第一步，为自动学习有效的奖励塑形函数提供了新思路。

Abstract: Reward shaping has been applied widely to accelerate Reinforcement Learning (RL) agents' training. However, a principled way of designing effective reward shaping functions, especially for complex continuous control problems, remains largely under-explained. In this work, we propose to automatically learn a reward shaping function for continuous control problems from offline datasets, potentially contaminated by unobserved confounding variables. Specifically, our method builds upon the recently proposed causal Bellman equation to learn a tight upper bound on the optimal state values, which is then used as the potentials in the Potential-Based Reward Shaping (PBRS) framework. Our proposed reward shaping algorithm is tested with Soft-Actor-Critic (SAC) on multiple commonly used continuous control benchmarks and exhibits strong performance guarantees under unobserved confounders. More broadly, our work marks a solid first step towards confounding robust continuous control from a causal perspective. Code for training our reward shaping functions can be found at https://github.com/mateojuliani/confounding_robust_cont_control.

</details>


### [29] [R2RAG-Flood: A reasoning-reinforced training-free retrieval augmentation generation framework for flood damage nowcasting](https://arxiv.org/abs/2602.10312)
*Lipai Huang,Kai Yin,Chia-Fu Liu,Ali Mostafavi*

Main category: cs.LG

TL;DR: R2RAG-Flood是一个无需训练、基于推理增强的检索增强生成框架，用于风暴后财产损害临近预报。它利用现有监督表格预测器构建知识库，通过检索邻近地理空间邻居和典型类别原型的推理轨迹，让大语言模型模仿先前推理而非学习新参数，实现财产损害预测。


<details>
  <summary>Details</summary>
Motivation: 现有监督表格预测器虽然有效，但缺乏可解释性且需要特定任务训练。需要一种无需训练、能提供结构化推理、同时保持预测性能的方法，以增强风暴后财产损害预测的可解释性和效率。

Method: 构建推理中心知识库，包含标记的表格记录、自然语言摘要和模型生成的推理轨迹。推理时通过上下文增强提示检索相关推理轨迹，采用两阶段预测：先确定损害发生，再细化严重程度，包含条件降级步骤纠正过度预测的严重性。

Result: 在哈里斯县案例中，监督表格基线总体准确率0.714，损害类别准确率0.859。R2RAG-Flood在7个大语言模型上达到0.613-0.668总体准确率和0.757-0.896损害类别准确率，接近监督基线同时提供结构化推理。轻量级变体在严重程度-成本效率指标上显著优于监督基线和大型语言模型。

Conclusion: R2RAG-Flood无需任务特定训练即可接近监督表格预测器的性能，同时提供可解释的推理轨迹，轻量级变体在效率上具有显著优势，为灾害损害预测提供了高效、可解释的解决方案。

Abstract: R2RAG-Flood is a reasoning-reinforced, training-free retrieval-augmented generation framework for post-storm property damage nowcasting. Building on an existing supervised tabular predictor, the framework constructs a reasoning-centric knowledge base composed of labeled tabular records, where each sample includes structured predictors, a compact natural language text-mode summary, and a model-generated reasoning trajectory. During inference, R2RAG-Flood issues context-augmented prompts that retrieve and condition on relevant reasoning trajectories from nearby geospatial neighbors and canonical class prototypes, enabling the large language model backbone to emulate and adapt prior reasoning rather than learn new task-specific parameters. Predictions follow a two-stage procedure that first determines property damage occurrence and then refines severity within a three-level Property Damage Extent categorization, with a conditional downgrade step to correct over-predicted severity. In a case study of Harris County, Texas at the 12-digit Hydrologic Unit Code scale, the supervised tabular baseline trained directly on structured predictors achieves 0.714 overall accuracy and 0.859 damage class accuracy for medium and high damage classes. Across seven large language model backbones, R2RAG-Flood attains 0.613 to 0.668 overall accuracy and 0.757 to 0.896 damage class accuracy, approaching the supervised baseline while additionally producing a structured rationale for each prediction. Using a severity-per-cost efficiency metric derived from API pricing and GPU instance costs, lightweight R2RAG-Flood variants demonstrate substantially higher efficiency than both the supervised tabular baseline and larger language models, while requiring no task-specific training or fine-tuning.

</details>


### [30] [Stop Training for the Worst: Progressive Unmasking Accelerates Masked Diffusion Training](https://arxiv.org/abs/2602.10314)
*Jaeyeon Kim,Jonathan Geuter,David Alvarez-Melis,Sham Kakade,Sitan Chen*

Main category: cs.LG

TL;DR: PUMA通过改进MDMs的前向掩码过程，使训练时和推理时的掩码模式对齐，从而加速训练并提升性能。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型（MDMs）在离散空间生成建模中表现出色，但存在训练复杂性权衡：训练时需要处理指数级数量的掩码模式，计算成本高，且训练时的随机掩码与推理时的结构化掩码之间存在不匹配问题。

Method: 提出渐进式解掩码（PUMA），通过修改前向掩码过程，使训练时的掩码模式与推理时对齐，将优化重点放在与推理一致的掩码上。

Result: PUMA在125M规模上加速预训练约2.5倍，并在自回归初始化等常见方法基础上提供互补优势。

Conclusion: PUMA是一种简单有效的改进方法，解决了MDMs中训练-推理不匹配问题，显著加速训练同时保持性能。

Abstract: Masked Diffusion Models (MDMs) have emerged as a promising approach for generative modeling in discrete spaces. By generating sequences in any order and allowing for parallel decoding, they enable fast inference and strong performance on non-causal tasks. However, this flexibility comes with a training complexity trade-off: MDMs train on an exponentially large set of masking patterns, which is not only computationally expensive, but also creates a train--test mismatch between the random masks used in training and the highly structured masks induced by inference-time unmasking. In this work, we propose Progressive UnMAsking (PUMA), a simple modification of the forward masking process that aligns training-time and inference-time masking patterns, thereby focusing optimization on inference-aligned masks and speeding up training. Empirically, PUMA speeds up pretraining at the 125M scale by $\approx 2.5\times$ and offers complementary advantages on top of common recipes like autoregressive initialization. We open-source our codebase at https://github.com/JaeyeonKim01/PUMA.

</details>


### [31] [Identifying Evidence-Based Nudges in Biomedical Literature with Large Language Models](https://arxiv.org/abs/2602.10345)
*Jaydeep Chauhan,Mark Seidman,Pezhman Raeisian Parvari,Zhi Zheng,Zina Ben-Miled,Cristina Barboi,Andrew Gonzalez,Malaz Boustani*

Main category: cs.LG

TL;DR: 开发了一个可扩展的AI系统，从生物医学文献中自动识别和提取基于证据的行为助推干预措施，通过多阶段流程实现高效筛选和结构化提取。


<details>
  <summary>Details</summary>
Motivation: 行为助推（nudges）作为非强制性的干预措施对健康结果有显著影响，但要从PubMed超过800万篇文章中手动识别这些干预措施效率低下，成为瓶颈。

Method: 采用多阶段流程：1) 混合过滤（关键词、TF-IDF、余弦相似度和"助推术语奖励"）将语料库减少到约81,000个候选；2) 使用OpenScholar（量化LLaMA 3.1 8B）单次分类论文并提取结构化字段（助推类型、目标行为等），并通过JSON模式验证。

Result: 在标记测试集（N=197）上评估了四种配置。最佳设置（标题/摘要/引言）达到67.0% F1分数和72.0%召回率，适合发现任务。高精度变体使用自一致性（7次随机通过）实现100%精度和12%召回率，展示了可调谐的权衡。

Conclusion: 该系统正在集成到Agile Nudge+现实平台中，为LLM生成的干预措施提供同行评审证据支持，展示了用于证据合成和个性化医疗的可解释、领域特定的检索流程。

Abstract: We present a scalable, AI-powered system that identifies and extracts evidence-based behavioral nudges from unstructured biomedical literature. Nudges are subtle, non-coercive interventions that influence behavior without limiting choice, showing strong impact on health outcomes like medication adherence. However, identifying these interventions from PubMed's 8 million+ articles is a bottleneck. Our system uses a novel multi-stage pipeline: first, hybrid filtering (keywords, TF-IDF, cosine similarity, and a "nudge-term bonus") reduces the corpus to about 81,000 candidates. Second, we use OpenScholar (quantized LLaMA 3.1 8B) to classify papers and extract structured fields like nudge type and target behavior in a single pass, validated against a JSON schema.
  We evaluated four configurations on a labeled test set (N=197). The best setup (Title/Abstract/Intro) achieved a 67.0% F1 score and 72.0% recall, ideal for discovery. A high-precision variant using self-consistency (7 randomized passes) achieved 100% precision with 12% recall, demonstrating a tunable trade-off for high-trust use cases. This system is being integrated into Agile Nudge+, a real-world platform, to ground LLM-generated interventions in peer-reviewed evidence. This work demonstrates interpretable, domain-specific retrieval pipelines for evidence synthesis and personalized healthcare.

</details>


### [32] [Theoretical Analysis of Contrastive Learning under Imbalanced Data: From Training Dynamics to a Pruning Solution](https://arxiv.org/abs/2602.10357)
*Haixu Liao,Yating Zhou,Songyang Zhang,Meng Wang,Shuai Zhang*

Main category: cs.LG

TL;DR: 该论文分析了Transformer编码器在数据不平衡条件下的对比学习训练动态，揭示了神经元权重演变的三个阶段，以及少数类特征如何降低表示能力并阻碍特征分离，提出剪枝可以恢复性能。


<details>
  <summary>Details</summary>
Motivation: 对比学习在现实应用中面临数据不平衡问题，这会导致表示质量下降和模型行为偏差，但目前缺乏对这些影响的严格理论分析。

Method: 开发了一个理论框架来分析Transformer编码器在数据不平衡条件下的对比学习训练动态，研究了神经元权重的演变过程，并通过数值实验验证理论发现。

Result: 发现神经元权重演变经历三个阶段，少数类特征会降低表示能力、增加架构复杂性需求，并阻碍真实特征与噪声的分离。剪枝可以恢复因不平衡而降低的性能并增强特征分离。

Conclusion: 该研究为不平衡数据下的对比学习提供了理论理解，揭示了神经元级行为，并提出剪枝作为恢复性能和增强特征分离的实用方法。

Abstract: Contrastive learning has emerged as a powerful framework for learning generalizable representations, yet its theoretical understanding remains limited, particularly under imbalanced data distributions that are prevalent in real-world applications. Such an imbalance can degrade representation quality and induce biased model behavior, yet a rigorous characterization of these effects is lacking. In this work, we develop a theoretical framework to analyze the training dynamics of contrastive learning with Transformer-based encoders under imbalanced data. Our results reveal that neuron weights evolve through three distinct stages of training, with different dynamics for majority features, minority features, and noise. We further show that minority features reduce representational capacity, increase the need for more complex architectures, and hinder the separation of ground-truth features from noise. Inspired by these neuron-level behaviors, we show that pruning restores performance degraded by imbalance and enhances feature separation, offering both conceptual insights and practical guidance. Major theoretical findings are validated through numerical experiments.

</details>


### [33] [Simple LLM Baselines are Competitive for Model Diffing](https://arxiv.org/abs/2602.10371)
*Elias Kempf,Simon Schrodi,Bartosz Cywiński,Thomas Brox,Neel Nanda,Arthur Conmy*

Main category: cs.LG

TL;DR: 论文提出模型差异分析方法的评估指标，比较LLM-based和SAE-based方法，发现改进的LLM基线方法在性能上与SAE方法相当且能发现更抽象的行为差异。


<details>
  <summary>Details</summary>
Motivation: 标准LLM评估只能测试设计好的能力，会遗漏模型修订间的意外行为差异或新出现的未对齐倾向。模型差异分析可以自动发现系统性行为差异，但现有方法缺乏系统比较和评估标准。

Method: 提出评估指标（泛化性、有趣性、抽象层次）来比较现有方法，包括LLM-based方法和稀疏自编码器(SAE)方法，并改进LLM基线方法。

Result: 改进的LLM-based基线方法在性能上与SAE-based方法相当，通常能发现更抽象的行为差异。

Conclusion: 建立了模型差异分析方法的评估框架，证明LLM-based方法在发现抽象行为差异方面具有竞争力，为未来方法比较提供了基准。

Abstract: Standard LLM evaluations only test capabilities or dispositions that evaluators designed them for, missing unexpected differences such as behavioral shifts between model revisions or emergent misaligned tendencies. Model diffing addresses this limitation by automatically surfacing systematic behavioral differences. Recent approaches include LLM-based methods that generate natural language descriptions and sparse autoencoder (SAE)-based methods that identify interpretable features. However, no systematic comparison of these approaches exists nor are there established evaluation criteria. We address this gap by proposing evaluation metrics for key desiderata (generalization, interestingness, and abstraction level) and use these to compare existing methods. Our results show that an improved LLM-based baseline performs comparably to the SAE-based method while typically surfacing more abstract behavioral differences.

</details>


### [34] [Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs](https://arxiv.org/abs/2602.10377)
*Luoyang Sun,Jiwen Jiang,Yifeng Ding,Fengfa Li,Yan Song,Haifeng Zhang,Jian Ying,Lei Ren,Kun Zhan,Wei Chen,Yan Xie,Cheng Deng*

Main category: cs.LG

TL;DR: 提出首个面向设备端LLM部署的硬件协同设计扩展定律框架，通过联合建模训练损失和推理延迟，实现架构快速搜索与优化


<details>
  <summary>Details</summary>
Motivation: 在资源受限的设备端部署VLA模型时，需要在准确性和推理延迟/硬件效率之间取得平衡，硬件-软件协同设计成为关键需求

Method: 提出硬件协同设计定律：1）将训练损失建模为架构超参数的显式函数；2）通过屋顶线模型表征推理延迟；3）在NVIDIA Jetson Orin上评估1942个候选架构，训练170个模型拟合扩展定律；4）结合延迟建模建立准确度-延迟对应关系

Result: 1）架构选择时间从数月缩短至数天；2）在相同延迟下，协同设计架构比Qwen2.5-0.5B在WikiText-2上困惑度降低19.42%；3）建立了硬件协同设计LLM的帕累托前沿

Conclusion: 这是首个面向设备端LLM部署的原则性、可操作的硬件协同设计扩展定律框架，为资源受限环境下的模型部署提供了系统化解决方案

Abstract: Vision-Language-Action Models (VLAs) have emerged as a key paradigm of Physical AI and are increasingly deployed in autonomous vehicles, robots, and smart spaces. In these resource-constrained on-device settings, selecting an appropriate large language model (LLM) backbone is a critical challenge: models must balance accuracy with strict inference latency and hardware efficiency constraints. This makes hardware-software co-design a game-changing requirement for on-device LLM deployment, where each hardware platform demands a tailored architectural solution. We propose a hardware co-design law that jointly captures model accuracy and inference performance. Specifically, we model training loss as an explicit function of architectural hyperparameters and characterise inference latency via roofline modelling. We empirically evaluate 1,942 candidate architectures on NVIDIA Jetson Orin, training 170 selected models for 10B tokens each to fit a scaling law relating architecture to training loss. By coupling this scaling law with latency modelling, we establish a direct accuracy-latency correspondence and identify the Pareto frontier for hardware co-designed LLMs. We further formulate architecture search as a joint optimisation over precision and performance, deriving feasible design regions under industrial hardware and application budgets. Our approach reduces architecture selection from months to days. At the same latency as Qwen2.5-0.5B on the target hardware, our co-designed architecture achieves 19.42% lower perplexity on WikiText-2. To our knowledge, this is the first principled and operational framework for hardware co-design scaling laws in on-device LLM deployment. We will make the code and related checkpoints publicly available.

</details>


### [35] [Deep learning outperforms traditional machine learning methods in predicting childhood malnutrition: evidence from survey data](https://arxiv.org/abs/2602.10381)
*Deepak Bastola,Yang Li*

Main category: cs.LG

TL;DR: 本研究首次全面评估了机器学习与深度学习在识别尼泊尔5岁以下儿童营养不良方面的应用，发现TabNet模型表现最佳，并确定了母亲教育、家庭财富指数和儿童年龄为主要预测因素。


<details>
  <summary>Details</summary>
Motivation: 尼泊尔等资源匮乏地区儿童营养不良仍是重大公共卫生问题，传统筛查方法劳动密集且在偏远地区难以实施，需要开发更高效、可扩展的筛查方法。

Method: 使用尼泊尔2019年多指标类集调查数据，构建综合营养不良指标（结合发育迟缓、消瘦和体重不足），系统比较16种算法（深度学习、梯度提升和传统机器学习），采用10个评估指标，特别关注F1分数和召回率以应对类别不平衡和漏检成本高的问题。

Result: TabNet模型表现最佳，优于支持向量机和AdaBoost分类器；共识特征重要性分析显示母亲教育、家庭财富指数和儿童年龄是主要预测因素，其次是地理特征、疫苗接种状况和进餐频率。

Conclusion: 研究证明了一个可扩展的基于调查的筛查框架，可用于识别营养不良高风险儿童并指导针对性营养干预，支持尼泊尔实现可持续发展目标，并为全球类似资源匮乏地区提供了可转移的方法模板。

Abstract: Childhood malnutrition remains a major public health concern in Nepal and other low-resource settings, while conventional case-finding approaches are labor-intensive and frequently unavailable in remote areas. This study provides the first comprehensive assessment of machine learning and deep learning methodologies for identifying malnutrition among children under five years of age in Nepal. We systematically compared 16 algorithms spanning deep learning, gradient boosting, and traditional machine learning families, using data from the Nepal Multiple Indicator Cluster Survey (MICS) 2019. A composite malnutrition indicator was constructed by integrating stunting, wasting, and underweight status, and model performance was evaluated using ten metrics, with emphasis on F1-score and recall to account for substantial class imbalance and the high cost of failing to detect malnourished children. Among all models, TabNet demonstrated the best performance, likely attributable to its attention-based architecture, and outperformed both support vector machine and AdaBoost classifiers. A consensus feature importance analysis identified maternal education, household wealth index, and child age as the primary predictors of malnutrition, followed by geographic characteristics, vaccination status, and meal frequency. Collectively, these results demonstrate a scalable, survey-based screening framework for identifying children at elevated risk of malnutrition and for guiding targeted nutritional interventions. The proposed approach supports Nepal's progress toward the Sustainable Development Goals and offers a transferable methodological template for similar low-resource settings globally.

</details>


### [36] [Time-to-Event Transformer to Capture Timing Attention of Events in EHR Time Series](https://arxiv.org/abs/2602.10385)
*Jia Li,Yu Hou,Rui Zhang*

Main category: cs.LG

TL;DR: LITT是一种新颖的时序Transformer架构，通过在虚拟"相对时间线"上对齐序列事件，实现事件时序聚焦的注意力机制，用于从大规模时间序列数据中发现个性化临床轨迹模式。


<details>
  <summary>Details</summary>
Motivation: 从大规模时间序列数据中自动发现个性化序列事件对于精准医疗至关重要，但现有AI模型（如Transformer）大多忽略事件时序和顺序，无法进行因果推理。需要一种能够评估患者特定轨迹对齐程度并识别共享模式的方法。

Method: 提出LITT（Timing-Transformer）架构，将时间作为可计算维度，在虚拟"相对时间线"上临时对齐序列事件，实现事件时序聚焦的注意力机制，为候选事件分配"相对时间戳"。

Result: 在3,276名乳腺癌患者的真实世界纵向电子健康记录数据上验证了可解释性和有效性，用于预测心脏毒性诱发心脏病的发病时间。在公共数据集上优于基准和最先进的生存分析方法。

Conclusion: LITT代表了临床AI精准医疗的重要进展，能够发现个性化序列事件模式，为临床研究提供有价值的见解。

Abstract: Automatically discovering personalized sequential events from large-scale time-series data is crucial for enabling precision medicine in clinical research, yet it remains a formidable challenge even for contemporary AI models. For example, while transformers capture rich associations, they are mostly agnostic to event timing and ordering, thereby bypassing potential causal reasoning.
  Intuitively, we need a method capable of evaluating the "degree of alignment" among patient-specific trajectories and identifying their shared patterns, i.e., the significant events in a consistent sequence. This necessitates treating timing as a true \emph{computable} dimension, allowing models to assign ``relative timestamps'' to candidate events beyond their observed physical times.
  In this work, we introduce LITT, a novel Timing-Transformer architecture that enables temporary alignment of sequential events on a virtual ``relative timeline'', thereby enabling \emph{event-timing-focused attention} and personalized interpretations of clinical trajectories. Its interpretability and effectiveness are validated on real-world longitudinal EHR data from 3,276 breast cancer patients to predict the onset timing of cardiotoxicity-induced heart disease. Furthermore, LITT outperforms both the benchmark and state-of-the-art survival analysis methods on public datasets, positioning it as a significant step forward for precision medicine in clinical AI.

</details>


### [37] [Colorful Talks with Graphs: Human-Interpretable Graph Encodings for Large Language Models](https://arxiv.org/abs/2602.10386)
*Angelo Zangari,Peyman Baghershahi,Sourav Medya*

Main category: cs.LG

TL;DR: 该论文提出了一种将图结构编码为自然语言提示的方法，通过Weisfeiler-Lehman相似性类和人类可解释的颜色标记来增强LLM处理图问题的能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理图问题时面临根本性挑战，因为图任务需要显式结构推理、排列不变性和计算复杂关系，这与基于文本的模型表示不匹配。研究旨在探索LLM如何有效应用于图问题。

Method: 引入人类可解释的结构编码策略，将图结构直接注入自然语言提示。计算Weisfeiler-Lehman相似性类的变体，并将其映射到类似人类的颜色标记而非数字标签。关键见解是语义有意义且人类可解释的线索可能比不透明的符号编码更有效地被LLM处理。

Result: 在多个算法和预测性图任务上的实验结果显示，该方法在合成和真实世界数据集上都取得了显著改进。通过捕获局部和全局范围依赖关系，该方法特别增强了LLM在需要全局图结构推理的任务上的性能。

Conclusion: 通过将图结构编码为人类可解释的自然语言提示，可以有效克服LLM处理图问题的障碍，特别是在需要全局结构推理的任务上表现出显著改进。

Abstract: Graph problems are fundamentally challenging for large language models (LLMs). While LLMs excel at processing unstructured text, graph tasks require reasoning over explicit structure, permutation invariance, and computationally complex relationships, creating a mismatch with the representations of text-based models. Our work investigates how LLMs can be effectively applied to graph problems despite these barriers. We introduce a human-interpretable structural encoding strategy for graph-to-text translation that injects graph structure directly into natural language prompts. Our method involves computing a variant of Weisfeiler-Lehman (WL) similarity classes and maps them to human-like color tokens rather than numeric labels. The key insight is that semantically meaningful and human-interpretable cues may be more effectively processed by LLMs than opaque symbolic encoding. Experimental results on multiple algorithmic and predictive graph tasks show the considerable improvements by our method on both synthetic and real-world datasets. By capturing both local and global-range dependencies, our method enhances LLM performance especially on graph tasks that require reasoning over global graph structure.

</details>


### [38] [Affordances Enable Partial World Modeling with LLMs](https://arxiv.org/abs/2602.10390)
*Khimya Khetarpal,Gheorghe Comanici,Jonathan Richens,Jeremy Shar,Fei Xia,Laurent Orseau,Aleksandra Faust,Doina Precup*

Main category: cs.LG

TL;DR: 论文提出将大模型作为部分世界模型使用，通过引入分布鲁棒的affordances来提取部分模型，显著提高搜索效率


<details>
  <summary>Details</summary>
Motivation: 完整世界模型需要复杂的细节知识，而预训练大模型虽然包含类似知识，但直接用于搜索效率低且不准确。部分世界模型专注于预测与用户意图相关的状态和动作子集，能否将大模型作为部分世界模型使用？

Method: 1. 形式化证明：实现任务无关、语言条件意图的智能体必然拥有基于affordances的预测性部分世界模型。2. 在多任务设置中引入分布鲁棒的affordances。3. 从大模型中提取部分模型来改进搜索效率

Result: 在桌面机器人任务中的实证评估表明，基于affordances的部分世界模型减少了搜索分支因子，相比完整世界模型获得了更高的奖励

Conclusion: 大模型可以作为有效的部分世界模型，通过affordances提取的部分模型能显著提高搜索效率，在机器人任务中表现优于完整世界模型

Abstract: Full models of the world require complex knowledge of immense detail. While pre-trained large models have been hypothesized to contain similar knowledge due to extensive pre-training on vast amounts of internet scale data, using them directly in a search procedure is inefficient and inaccurate. Conversely, partial models focus on making high quality predictions for a subset of state and actions: those linked through affordances that achieve user intents~\citep{khetarpal2020can}. Can we posit large models as partial world models? We provide a formal answer to this question, proving that agents achieving task-agnostic, language-conditioned intents necessarily possess predictive partial-world models informed by affordances. In the multi-task setting, we introduce distribution-robust affordances and show that partial models can be extracted to significantly improve search efficiency. Empirical evaluations in tabletop robotics tasks demonstrate that our affordance-aware partial models reduce the search branching factor and achieve higher rewards compared to full world models.

</details>


### [39] [Tensor Methods: A Unified and Interpretable Approach for Material Design](https://arxiv.org/abs/2602.10392)
*Shaan Pakala,Aldair E. Gongora,Brian Giera,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 该论文提出使用张量补全方法作为材料设计的替代模型，相比传统机器学习方法具有更好的可解释性，并能处理非均匀采样数据。


<details>
  <summary>Details</summary>
Motivation: 材料设计需要从大量设计参数中寻找具有特定性能的材料，但搜索空间随参数数量指数增长，传统计算方法计算成本过高。现有机器学习替代模型存在可解释性差、对非均匀采样数据性能不佳的问题。

Method: 使用张量补全方法作为替代模型，利用其可解释的张量因子进行预测。研究比较了经典张量方法和专门化张量方法在处理非均匀采样数据时的表现。

Result: 张量方法在预测性能上能与传统机器学习竞争，同时提供免费的可解释张量因子。专门化张量方法在非均匀采样场景下表现更好，比基线ML方法在总体R²上提升5%，在某些分布外区域误差减半。

Conclusion: 张量补全方法为材料设计提供了一种兼具预测性能和可解释性的替代模型方案，能够从张量因子中重新发现物理现象，有助于实验人员识别新模式。

Abstract: When designing new materials, it is often necessary to tailor the material design (with respect to its design parameters) to have some desired properties (e.g. Young's modulus). As the set of design parameters grow, the search space grows exponentially, making the actual synthesis and evaluation of all material combinations virtually impossible. Even using traditional computational methods such as Finite Element Analysis becomes too computationally heavy to search the design space. Recent methods use machine learning (ML) surrogate models to more efficiently determine optimal material designs; unfortunately, these methods often (i) are notoriously difficult to interpret and (ii) under perform when the training data comes from a non-uniform sampling of the design space. We suggest the use of tensor completion methods as an all-in-one approach for interpretability and predictions. We observe classical tensor methods are able to compete with traditional ML in predictions, with the added benefit of their interpretable tensor factors (which are given completely for free, as a result of the prediction). In our experiments, we are able to rediscover physical phenomena via the tensor factors, indicating that our predictions are aligned with the true underlying physics of the problem. This also means these tensor factors could be used by experimentalists to identify potentially novel patterns, given we are able to rediscover existing ones. We also study the effects of both types of surrogate models when we encounter training data from a non-uniform sampling of the design space. We observe more specialized tensor methods that can give better generalization in these non-uniforms sampling scenarios. We find the best generalization comes from a tensor model, which is able to improve upon the baseline ML methods by up to 5% on aggregate $R^2$, and halve the error in some out of distribution regions.

</details>


### [40] [MerLin: A Discovery Engine for Photonic and Hybrid Quantum Machine Learning](https://arxiv.org/abs/2602.11092)
*Cassandre Notton,Benjamin Stott,Philippe Schoeb,Anthony Walsh,Grégoire Leboucher,Vincent Espitalier,Vassilis Apostolou,Louis-Félix Vigneux,Alexia Salavrakos,Jean Senellart*

Main category: cs.LG

TL;DR: MerLin是一个开源框架，用于系统探索光子和混合量子机器学习模型，通过集成优化的线性光学电路模拟到PyTorch和scikit-learn工作流中，支持端到端可微分训练。


<details>
  <summary>Details</summary>
Motivation: 为了识别量子模型在近期量子机器学习中的实际优势，需要超越孤立的算法提案，转向跨模型、数据集和硬件约束的系统性实证探索。

Method: 开发MerLin框架，将优化的线性光学电路强模拟集成到标准PyTorch和scikit-learn工作流中，支持量子层的端到端可微分训练，并围绕系统性基准测试和可重复性设计。

Result: 成功复现了18个最先进的光子和混合量子机器学习工作，涵盖核方法、储备池计算、卷积和循环架构、生成模型及现代训练范式，并作为可重用、模块化实验发布。

Conclusion: MerLin通过将光子量子模型嵌入已建立的机器学习生态系统，使从业者能够利用现有工具进行消融研究、跨模态比较和混合经典量子工作流，同时具备硬件感知功能，成为连接算法、基准测试和硬件的未来验证协同设计工具。

Abstract: Identifying where quantum models may offer practical benefits in near term quantum machine learning (QML) requires moving beyond isolated algorithmic proposals toward systematic and empirical exploration across models, datasets, and hardware constraints. We introduce MerLin, an open source framework designed as a discovery engine for photonic and hybrid quantum machine learning. MerLin integrates optimized strong simulation of linear optical circuits into standard PyTorch and scikit learn workflows, enabling end to end differentiable training of quantum layers. MerLin is designed around systematic benchmarking and reproducibility. As an initial contribution, we reproduce eighteen state of the art photonic and hybrid QML works spanning kernel methods, reservoir computing, convolutional and recurrent architectures, generative models, and modern training paradigms. These reproductions are released as reusable, modular experiments that can be directly extended and adapted, establishing a shared experimental baseline consistent with empirical benchmarking methodologies widely adopted in modern artificial intelligence. By embedding photonic quantum models within established machine learning ecosystems, MerLin allows practitioners to leverage existing tooling for ablation studies, cross modality comparisons, and hybrid classical quantum workflows. The framework already implements hardware aware features, allowing tests on available quantum hardware while enabling exploration beyond its current capabilities, positioning MerLin as a future proof co design tool linking algorithms, benchmarks, and hardware.

</details>


### [41] [Experimental Demonstration of Online Learning-Based Concept Drift Adaptation for Failure Detection in Optical Networks](https://arxiv.org/abs/2602.10401)
*Yousuf Moiz Ali,Jaroslaw E. Prilepsky,João Pedro,Antonio Napoli,Sasipim Srivallapanondh,Sergei K. Turitsyn,Pedro Freire*

Main category: cs.LG

TL;DR: 提出基于在线学习的光网络故障检测概念漂移适应方法，性能比静态模型提升70%


<details>
  <summary>Details</summary>
Motivation: 传统静态模型无法适应光网络故障检测中的概念漂移问题，导致性能下降

Method: 采用在线学习方法动态适应概念漂移，保持低延迟

Result: 相比传统静态模型，性能提升高达70%，同时维持低延迟

Conclusion: 在线学习方法能有效适应光网络故障检测中的概念漂移，显著提升检测性能

Abstract: We present a novel online learning-based approach for concept drift adaptation in optical network failure detection, achieving up to a 70% improvement in performance over conventional static models while maintaining low latency.

</details>


### [42] [Modular Multi-Task Learning for Chemical Reaction Prediction](https://arxiv.org/abs/2602.10404)
*Jiayun Pang,Ahmed M. Zaitoun,Xacobe Couso Cambeiro,Ivan Vulić*

Main category: cs.LG

TL;DR: LoRA作为参数高效微调方法，在有机反应预测任务上达到与全微调相当的准确率，同时更好地缓解灾难性遗忘并保持多任务性能。


<details>
  <summary>Details</summary>
Motivation: 将在大规模有机化学数据上训练的LLMs适应到小规模、领域特定的反应数据集是化学和制药研发中的关键挑战。需要学习新反应知识的同时保持跨相关任务的通用化学理解。

Method: 使用低秩适应（LoRA）作为参数高效的全微调替代方案，在USPTO反应类别和挑战性的C-H官能化反应上进行评估，包括前向反应预测、逆合成和试剂预测任务。

Result: LoRA达到与全微调相当的准确率，同时有效缓解灾难性遗忘并更好地保持多任务性能。两种微调方法都能泛化到训练分布之外，产生合理的替代溶剂预测。C-H官能化微调显示LoRA和全微调编码了不同的反应模式，表明LoRA能更有效地进行反应特定适应。

Conclusion: 随着LLMs规模扩大，研究结果强调了模块化、参数高效的微调策略在化学应用中灵活部署的实用性。

Abstract: Adapting large language models (LLMs) trained on broad organic chemistry to smaller, domain-specific reaction datasets is a key challenge in chemical and pharmaceutical R&D. Effective specialisation requires learning new reaction knowledge while preserving general chemical understanding across related tasks. Here, we evaluate Low-Rank Adaptation (LoRA) as a parameter-efficient alternative to full fine-tuning for organic reaction prediction on limited, complex datasets. Using USPTO reaction classes and challenging C-H functionalisation reactions, we benchmark forward reaction prediction, retrosynthesis and reagent prediction. LoRA achieves accuracy comparable to full fine-tuning while effectively mitigating catastrophic forgetting and better preserving multi-task performance. Both fine-tuning approaches generalise beyond training distributions, producing plausible alternative solvent predictions. Notably, C-H functionalisation fine-tuning reveals that LoRA and full fine-tuning encode subtly different reactivity patterns, suggesting more effective reaction-specific adaptation with LoRA. As LLMs continue to scale, our results highlight the practicality of modular, parameter-efficient fine-tuning strategies for their flexible deployment for chemistry applications.

</details>


### [43] [Gated Removal of Normalization in Transformers Enables Stable Training and Efficient Inference](https://arxiv.org/abs/2602.10408)
*Andrei Kanavalau,Carmen Amo Alonso,Sanjay Lall*

Main category: cs.LG

TL;DR: TaperNorm是一种新的Transformer归一化方法，它在训练早期像标准归一化，后期逐渐变为固定的线性映射，可以消除逐token统计并折叠到相邻线性层中，提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 重新审视预归一化Transformer中样本依赖归一化的必要性，探索如何减少归一化层的计算开销，同时保持训练稳定性。

Method: 提出TaperNorm，作为RMSNorm/LayerNorm的替代方案，使用全局门控机制：训练早期门控为1（标准归一化），通过EMA校准缩放分支，然后余弦衰减到0（固定线性映射），最终可以折叠到相邻线性投影中。

Result: TaperNorm在相同设置下匹配归一化基线的性能，同时消除逐token统计，使这些层可以在推理时折叠到相邻线性层中，在效率微基准测试中，折叠内部缩放可使最后token对数模式吞吐量提高1.22倍。

Conclusion: TaperNorm向无归一化Transformer迈进一步，同时揭示了输出归一化在锚定尺度方面的特殊作用，可以通过简单的固定目标辅助损失替代最终归一化层。

Abstract: Normalization is widely viewed as essential for stabilizing Transformer training. We revisit this assumption for pre-norm Transformers and ask to what extent sample-dependent normalization is needed inside Transformer blocks. We introduce TaperNorm, a drop-in replacement for RMSNorm/LayerNorm that behaves exactly like the standard normalizer early in training and then smoothly tapers to a learned sample-independent linear/affine map. A single global gate is held at $g{=}1$ during gate warmup, used to calibrate the scaling branch via EMAs, and then cosine-decayed to $g{=}0$, at which point per-token statistics vanish and the resulting fixed scalings can be folded into adjacent linear projections. Our theoretical and empirical results isolate scale anchoring as the key role played by output normalization: as a (near) $0$-homogeneous map it removes radial gradients at the output, whereas without such an anchor cross-entropy encourages unbounded logit growth (``logit chasing''). We further show that a simple fixed-target auxiliary loss on the pre-logit residual-stream scale provides an explicit alternative anchor and can aid removal of the final normalization layer. Empirically, TaperNorm matches normalized baselines under identical setups while eliminating per-token statistics and enabling these layers to be folded into adjacent linear projections at inference. On an efficiency microbenchmark, folding internal scalings yields up to $1.22\times$ higher throughput in last-token logits mode. These results take a step towards norm-free Transformers while identifying the special role output normalization plays.

</details>


### [44] [LUCID: Attention with Preconditioned Representations](https://arxiv.org/abs/2602.10410)
*Sai Surya Duvvuri,Nirmal Patel,Nilesh Gupta,Inderjit S. Dhillon*

Main category: cs.LG

TL;DR: LUCID Attention通过引入基于键-键相似度的预处理器来改进Transformer注意力机制，解决长序列中softmax注意力分散的问题，在保持计算复杂度的同时显著提升长上下文检索性能。


<details>
  <summary>Details</summary>
Motivation: 随着上下文长度增加，softmax注意力机制存在两个主要问题：1）将概率质量分散到不相关的token上，降低长序列性能；2）通过降低softmax温度来增强聚焦会导致梯度消失，影响可学习性。

Method: 引入LUCID Attention架构修改，应用基于指数化键-键相似度的预处理器。该预处理器在再生核希尔伯特空间中最小化键之间的重叠，使查询能够准确关注重要键，同时保持与标准注意力相同的计算复杂度。

Result: 在约10亿参数的语言模型上验证，评估长度达128K token。在长上下文检索任务上取得显著提升：BABILong提升18%，RULER多针任务提升14%，同时在SCROLLS和LongBench上也表现优异。

Conclusion: LUCID Attention通过预处理器方法有效解决了长序列中softmax注意力分散的问题，避免了低温softmax带来的可学习性问题，在保持计算效率的同时显著提升了长上下文检索性能。

Abstract: Softmax-based dot-product attention is a cornerstone of Transformer architectures, enabling remarkable capabilities such as in-context learning. However, as context lengths increase, a fundamental limitation of the softmax function emerges: it tends to diffuse probability mass to irrelevant tokens degrading performance in long-sequence scenarios. Furthermore, attempts to sharpen focus by lowering softmax temperature hinder learnability due to vanishing gradients. We introduce LUCID Attention, an architectural modification that applies a preconditioner to the attention probabilities. This preconditioner, derived from exponentiated key-key similarities, minimizes overlap between the keys in a Reproducing Kernel Hilbert Space, thus allowing the query to focus on important keys among large number of keys accurately with same computational complexity as standard attention. Additionally, LUCID's preconditioning-based approach to retrieval bypasses the need for low temperature and the learnability problems associated with it. We validate our approach by training ~1 billion parameter language models evaluated on up to 128K tokens. Our results demonstrate significant gains on long-context retrieval tasks, specifically retrieval tasks from BABILong, RULER, SCROLLS and LongBench. For instance, LUCID achieves up to 18% improvement in BABILong and 14% improvement in RULER multi-needle performance compared to standard attention.

</details>


### [45] [LightGTS-Cov: Covariate-Enhanced Time Series Forecasting](https://arxiv.org/abs/2602.10412)
*Yong Shang,Zhipeng Yao,Ning Jin,Xiangfei Qiu,Hui Zhang,Bin Yang*

Main category: cs.LG

TL;DR: LightGTS-Cov 在轻量级时间序列基础模型 LightGTS 基础上，通过添加小型MLP插件显式整合过去和未来已知的外生协变量，在电力价格和可再生能源预测等协变量丰富的应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型通常忽略外生协变量，或仅通过简单拼接方式处理，这在电力价格预测和可再生能源预测等协变量丰富的应用中效果有限。

Method: 基于约1M参数的LightGTS骨干网络，添加约0.1M参数的MLP插件，通过残差精炼解码过程输出的方式，将时间对齐的协变量整合到目标预测中。

Result: 在电力价格和能源生成数据集的协变量感知基准测试中，LightGTS-Cov始终优于LightGTS，并在是否提供未来已知协变量的两种设置下都优于其他协变量感知基线模型。

Conclusion: LightGTS-Cov在光伏功率长期预测和日前电力价格预测等实际能源应用中表现出强大的预测准确性和稳定的操作性能，验证了其在真实工业环境中的有效性。

Abstract: Time series foundation models are typically pre-trained on large, multi-source datasets; however, they often ignore exogenous covariates or incorporate them via simple concatenation with the target series, which limits their effectiveness in covariate-rich applications such as electricity price forecasting and renewable energy forecasting. We introduce LightGTS-Cov, a covariate-enhanced extension of LightGTS that preserves its lightweight, period-aware backbone while explicitly incorporating both past and future-known covariates. Built on a $\sim$1M-parameter LightGTS backbone, LightGTS-Cov adds only a $\sim$0.1M-parameter MLP plug-in that integrates time-aligned covariates into the target forecasts by residually refining the outputs of the decoding process. Across covariate-aware benchmarks on electricity price and energy generation datasets, LightGTS-Cov consistently outperforms LightGTS and achieves superior performance over other covariate-aware baselines under both settings, regardless of whether future-known covariates are provided. We further demonstrate its practical value in two real-world energy case applications: long-term photovoltaic power forecasting with future weather forecasts and day-ahead electricity price forecasting with weather and dispatch-plan covariates. Across both applications, LightGTS-Cov achieves strong forecasting accuracy and stable operational performance after deployment, validating its effectiveness in real-world industrial settings.

</details>


### [46] [AI-rithmetic](https://arxiv.org/abs/2602.10416)
*Alex Bie,Travis Dick,Alex Kulesza,Prabhakar Raghavan,Vinod Raman,Sergei Vassilvitskii*

Main category: cs.LG

TL;DR: 前沿AI模型在高级数学任务上表现出色，但在基本整数加法上表现糟糕，错误主要源于操作数错位和进位失败


<details>
  <summary>Details</summary>
Motivation: 尽管现代AI系统在国际数学竞赛、研究工作和技术引理证明方面取得了成功，但它们仍然在基本算术（特别是整数加法）上表现不佳。本文旨在系统研究这一矛盾现象。

Method: 通过实证研究，分析前沿模型在整数加法任务上的表现，特别关注随着数字位数增加时的准确性下降。对模型错误进行分类和归因分析，主要识别操作数错位和进位失败两类错误。

Result: 所有前沿模型在整数加法任务上都表现出显著的准确性下降，错误主要可归为两类：操作数错位错误（与分词相关）和进位失败错误（表现为独立随机失败）。这些错误分别解释了Claude Opus 4.1的87.9%、GPT-5的62.9%和Gemini 2.5 Pro的92.4%的错误。

Conclusion: AI模型在基本算术上的失败揭示了其能力的不一致性，错误模式具有高度可解释性，主要源于分词相关的操作数错位和随机性的进位失败，这为改进模型的基本数学能力提供了方向。

Abstract: Modern AI systems have been successfully deployed to win medals at international math competitions, assist with research workflows, and prove novel technical lemmas. However, despite their progress at advanced levels of mathematics, they remain stubbornly bad at basic arithmetic, consistently failing on the simple task of adding two numbers. We present a systematic investigation of this phenomenon. We demonstrate empirically that all frontier models suffer significantly degraded accuracy for integer addition as the number of digits increases. Furthermore, we show that most errors made by these models are highly interpretable and can be attributed to either operand misalignment or a failure to correctly carry; these two error classes explain 87.9%, 62.9%, and 92.4% of Claude Opus 4.1, GPT-5, and Gemini 2.5 Pro errors, respectively. Finally, we show that misalignment errors are frequently related to tokenization, and that carrying errors appear largely as independent random failures.

</details>


### [47] [Equivariant Evidential Deep Learning for Interatomic Potentials](https://arxiv.org/abs/2602.10419)
*Zhongyao Wang,Taoyong Cui,Jiawen Zou,Shufei Zhang,Bo Yan,Wanli Ouyang,Weimin Tan,Mao Su*

Main category: cs.LG

TL;DR: 提出e²IP框架，通过等变证据深度学习为原子间势能提供旋转不变的不确定性量化，使用3×3协方差张量建模原子力不确定性，实现单次前向传播的高效UQ。


<details>
  <summary>Details</summary>
Motivation: 机器学习原子间势能（MLIPs）的不确定性量化对分子动力学模拟可靠性至关重要，但现有方法存在计算成本高或性能不佳的问题。证据深度学习（EDL）提供理论基础的单一模型方案，但扩展到原子力等向量值量时面临旋转变换下统计自一致性的挑战。

Method: 提出e²IP（等变证据深度学习原子间势能）框架，将不确定性建模为3×3对称正定协方差张量，该张量在旋转下具有等变性。框架与骨干网络无关，联合建模原子力及其不确定性，保持统计自一致性。

Result: 在多个分子基准测试中，e²IP在准确性-效率-可靠性平衡上优于非等变证据基线和广泛使用的集成方法。通过完全等变架构实现更好的数据效率，同时保持单一模型推理效率。

Conclusion: e²IP为MLIPs提供了一种高效、可靠的不确定性量化框架，解决了向量值量旋转不变性的挑战，在准确性、效率和可靠性之间取得了更好的平衡，适用于主动学习等不确定性感知工作流。

Abstract: Uncertainty quantification (UQ) is critical for assessing the reliability of machine learning interatomic potentials (MLIPs) in molecular dynamics (MD) simulations, identifying extrapolation regimes and enabling uncertainty-aware workflows such as active learning for training dataset construction. Existing UQ approaches for MLIPs are often limited by high computational cost or suboptimal performance. Evidential deep learning (EDL) provides a theoretically grounded single-model alternative that determines both aleatoric and epistemic uncertainty in a single forward pass. However, extending evidential formulations from scalar targets to vector-valued quantities such as atomic forces introduces substantial challenges, particularly in maintaining statistical self-consistency under rotational transformations. To address this, we propose \textit{Equivariant Evidential Deep Learning for Interatomic Potentials} ($\text{e}^2$IP), a backbone-agnostic framework that models atomic forces and their uncertainty jointly by representing uncertainty as a full $3\times3$ symmetric positive definite covariance tensor that transforms equivariantly under rotations. Experiments on diverse molecular benchmarks show that $\text{e}^2$IP provides a stronger accuracy-efficiency-reliability balance than the non-equivariant evidential baseline and the widely used ensemble method. It also achieves better data efficiency through the fully equivariant architecture while retaining single-model inference efficiency.

</details>


### [48] [Binary Flow Matching: Prediction-Loss Space Alignment for Robust Learning](https://arxiv.org/abs/2602.10420)
*Jiadong Hong,Lei Liu,Xinyu Bian,Wenjie Wang,Zhaoyang Zhang*

Main category: cs.LG

TL;DR: 该论文研究了在二进制流形上的流匹配生成建模，发现信号空间预测（x-预测）与速度目标（v-损失）结合时存在结构不匹配问题，提出了预测-损失对齐作为流匹配训练的必要条件，并分析了二进制数据特有的设计选择。


<details>
  <summary>Details</summary>
Motivation: 流匹配已成为生成建模的强大框架，但在二进制流形（离散数据生成的基本设置）上应用时，信号空间预测与速度目标结合存在潜在的结构不匹配问题，这会导致梯度敏感性问题，需要理论分析和解决方案。

Method: 1. 识别x-预测与v-损失结合时的结构不匹配问题；2. 形式化预测-损失对齐作为必要条件；3. 证明将目标重新对齐到信号空间（x-损失）可以消除奇异加权；4. 分析二进制数据特有的设计选择，比较概率目标（如交叉熵）与几何损失（如均方误差）。

Result: 重新对齐到信号空间（x-损失）可以消除奇异加权，产生均匀有界的梯度，使得在均匀时间步采样下能够进行鲁棒训练，无需依赖启发式调度。同时发现二进制数据的拓扑结构导致概率目标与几何损失之间存在区别。

Conclusion: 信号空间对齐是二进制及相关离散域上鲁棒流匹配的关键原则，为鲁棒扩散学习提供了理论基础和实践指南，解决了梯度敏感性问题并实现了无需启发式调度的稳定训练。

Abstract: Flow matching has emerged as a powerful framework for generative modeling, with recent empirical successes highlighting the effectiveness of signal-space prediction ($x$-prediction). In this work, we investigate the transfer of this paradigm to binary manifolds, a fundamental setting for generative modeling of discrete data. While $x$-prediction remains effective, we identify a latent structural mismatch that arises when it is coupled with velocity-based objectives ($v$-loss), leading to a time-dependent singular weighting that amplifies gradient sensitivity to approximation errors. Motivated by this observation, we formalize prediction-loss alignment as a necessary condition for flow matching training. We prove that re-aligning the objective to the signal space ($x$-loss) eliminates the singular weighting, yielding uniformly bounded gradients and enabling robust training under uniform timestep sampling without reliance on heuristic schedules. Finally, with alignment secured, we examine design choices specific to binary data, revealing a topology-dependent distinction between probabilistic objectives (e.g., cross-entropy) and geometric losses (e.g., mean squared error). Together, these results provide theoretical foundations and practical guidelines for robust flow matching on binary -- and related discrete -- domains, positioning signal-space alignment as a key principle for robust diffusion learning.

</details>


### [49] [Breaking the Curse of Repulsion: Optimistic Distributionally Robust Policy Optimization for Off-Policy Generative Recommendation](https://arxiv.org/abs/2602.10430)
*Jie Jiang,Yusen Huo,Xiangxin Zhan,Changping Wang,Jun Zhang*

Main category: cs.LG

TL;DR: DRPO提出分布鲁棒策略优化方法，通过硬过滤解决离线强化学习中低质量数据导致的模型崩溃问题，在混合质量推荐基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 基于策略的强化学习在生成式推荐中占主导地位，但在离线历史日志上应用时，低质量数据的主导会导致严重的模型崩溃。现有方法无法协调方差减少和噪声模仿之间的内在困境。

Method: 提出分布鲁棒策略优化(DRPO)，将目标重新表述为乐观分布鲁棒优化问题。证明硬过滤是该DRO目标的精确解，使DRPO能够最优地恢复高质量行为，同时严格丢弃导致发散的噪声。

Result: 在混合质量推荐基准上的广泛实验表明，DRPO实现了最先进的性能。

Conclusion: 通过建立排斥优化发散理论，揭示了离线训练中负梯度更新会引发指数强度爆炸。DRPO通过识别噪声行为策略中纠缠的潜在高质量分布，解决了现有方法的局限性。

Abstract: Policy-based Reinforcement Learning (RL) has established itself as the dominant paradigm in generative recommendation for optimizing sequential user interactions. However, when applied to offline historical logs, these methods suffer a critical failure: the dominance of low-quality data induces severe model collapse. We first establish the Divergence Theory of Repulsive Optimization, revealing that negative gradient updates inherently trigger exponential intensity explosion during off-policy training. This theory elucidates the inherent dilemma of existing methods, exposing their inability to reconcile variance reduction and noise imitation. To break this curse, we argue that the solution lies in rigorously identifying the latent high-quality distribution entangled within the noisy behavior policy. Accordingly, we reformulate the objective as an Optimistic Distributionally Robust Optimization (DRO) problem. Guided by this formulation, we propose Distributionally Robust Policy Optimization (DRPO). We prove that hard filtering is the exact solution to this DRO objective, enabling DRPO to optimally recover high-quality behaviors while strictly discarding divergence-inducing noise. Extensive experiments demonstrate that DRPO achieves state-of-the-art performance on mixed-quality recommendation benchmarks.

</details>


### [50] [QTALE: Quantization-Robust Token-Adaptive Layer Execution for LLMs](https://arxiv.org/abs/2602.10431)
*Kanghyun Noh,Jinheon Choi,Yulwha Kim*

Main category: cs.LG

TL;DR: QTALE框架无缝整合token自适应层执行与量化技术，在保持精度的同时减少LLMs的计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 大语言模型需要大量计算和内存资源，现有token自适应执行和量化技术单独使用有效，但简单结合会导致额外精度损失，因为token自适应模型减少了冗余。

Method: QTALE包含两个关键组件：1) 训练策略确保微调期间探索多样执行路径；2) 后训练机制允许推理时灵活调整执行比率以重新引入冗余。

Result: 实验显示QTALE能无缝整合token自适应层执行与量化，无明显精度差异，与纯量化模型的差距保持在CommonsenseQA基准上低于0.5%。

Conclusion: QTALE通过结合token自适应执行减少FLOPs和量化减少内存占用，为高效LLM部署提供了有效解决方案。

Abstract: Large language models (LLMs) demand substantial computational and memory resources, posing challenges for efficient deployment. Two complementary approaches have emerged to address these issues: token-adaptive layer execution, which reduces floating-point operations (FLOPs) by selectively bypassing layers, and quantization, which lowers memory footprint by reducing weight precision. However, naively integrating these techniques leads to additional accuracy degradation due to reduced redundancy in token-adaptive models. We propose QTALE (Quantization-Robust Token-Adaptive Layer Execution for LLMs), a novel framework that enables seamless integration of token-adaptive execution with quantization while preserving accuracy. Conventional token-adaptive methods reduce redundancy in two ways: (1) by limiting the diversity of training paths explored during fine-tuning, and (2) by lowering the number of parameters actively involved in inference. To overcome these limitations, QTALE introduces two key components: (1) a training strategy that ensures diverse execution paths are actively explored during fine-tuning, and (2) a post-training mechanism that allows flexible adjustment of the execution ratio at inference to reintroduce redundancy when needed. Experimental results show that QTALE enables seamless integration of token-adaptive layer execution with quantization, showing no noticeable accuracy difference, with the gap to quantization-only models kept below 0.5% on CommonsenseQA benchmarks. By combining token-adaptive execution for FLOPs reduction and quantization for memory savings, QTALE provides an effective solution for efficient LLM deployment.

</details>


### [51] [A Dual-Stream Physics-Augmented Unsupervised Architecture for Runtime Embedded Vehicle Health Monitoring](https://arxiv.org/abs/2602.10432)
*Enzo Nicolas Spotorno,Antonio Augusto Medeiros Frohlich*

Main category: cs.LG

TL;DR: 提出双流架构，融合无监督学习和物理代理模型，在资源受限的ECU上实现车辆运行强度的实时量化，区分动态冲击和持续机械负荷。


<details>
  <summary>Details</summary>
Motivation: 传统里程指标无法捕捉机械负荷，而无监督深度学习模型常将统计稳定性误判为机械休息，忽略了高负荷稳态工况（如重载爬坡）对传动系统的显著疲劳影响。

Method: 采用双流架构：一流使用无监督学习检测表面异常（瞬态冲击），另一流使用宏观物理代理模型估计累积负荷。融合低频率传感器数据生成多维健康向量。

Result: 在RISC-V嵌入式平台上验证，架构计算开销低，可在资源受限的ECU上实现全面的边缘健康监测，避免了云端监测的延迟和带宽成本。

Conclusion: 该双流架构有效解决了传统方法的盲点，能够区分动态危害和持续机械努力，为商用和重型车队的预测性维护提供了实用的边缘计算解决方案。

Abstract: Runtime quantification of vehicle operational intensity is essential for predictive maintenance and condition monitoring in commercial and heavy-duty fleets. Traditional metrics like mileage fail to capture mechanical burden, while unsupervised deep learning models detect statistical anomalies, typically transient surface shocks, but often conflate statistical stability with mechanical rest. We identify this as a critical blind spot: high-load steady states, such as hill climbing with heavy payloads, appear statistically normal yet impose significant drivetrain fatigue. To resolve this, we propose a Dual-Stream Architecture that fuses unsupervised learning for surface anomaly detection with macroscopic physics proxies for cumulative load estimation. This approach leverages low-frequency sensor data to generate a multi-dimensional health vector, distinguishing between dynamic hazards and sustained mechanical effort. Validated on a RISC-V embedded platform, the architecture demonstrates low computational overhead, enabling comprehensive, edge-based health monitoring on resource-constrained ECUs without the latency or bandwidth costs of cloud-based monitoring.

</details>


### [52] [Control Reinforcement Learning: Token-Level Mechanistic Analysis via Learned SAE Feature Steering](https://arxiv.org/abs/2602.10437)
*Seonglae Cho,Zekun Wu,Adriano Koshiyama*

Main category: cs.LG

TL;DR: CRL通过强化学习训练策略来选择稀疏自编码器特征进行动态干预，提供可解释的干预日志，识别能改变模型输出的特征，并开发了多种新的分析能力。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器方法只能显示哪些特征被激活，但不能显示哪些特征在被放大时会改变模型输出。需要一种动态干预方法来补充静态特征分析。

Method: 提出控制强化学习框架，训练策略在每个token处选择SAE特征进行干预；采用自适应特征掩码鼓励多样化特征发现；开发分支点跟踪、评论家轨迹分析、分层比较等分析能力。

Result: 在Gemma-2 2B模型上，CRL在MMLU、BBQ、GSM8K、HarmBench和XSTest等基准测试中取得改进，同时提供每个token的干预日志。分层分析显示早期层包含句法特征，后期层包含语义特征。

Conclusion: 学习到的特征引导作为一种机制可解释性工具，通过动态干预探针补充了静态特征分析，为理解语言模型内部表示提供了新视角。

Abstract: Sparse autoencoders (SAEs) decompose language model activations into interpretable features, but existing methods reveal only which features activate, not which change model outputs when amplified. We introduce Control Reinforcement Learning (CRL), which trains a policy to select SAE features for steering at each token, producing interpretable intervention logs: the learned policy identifies features that change model outputs when amplified. Adaptive Feature Masking encourages diverse feature discovery while preserving singlefeature interpretability. The framework yields new analysis capabilities: branch point tracking locates tokens where feature choice determines output correctness; critic trajectory analysis separates policy limitations from value estimation errors; layer-wise comparison reveals syntactic features in early layers and semantic features in later layers. On Gemma-2 2B across MMLU, BBQ, GSM8K, HarmBench, and XSTest, CRL achieves improvements while providing per-token intervention logs. These results establish learned feature steering as a mechanistic interpretability tool that complements static feature analysis with dynamic intervention probes

</details>


### [53] [LakeMLB: Data Lake Machine Learning Benchmark](https://arxiv.org/abs/2602.10441)
*Feiyu Pan,Tianbin Zhang,Aoqian Zhang,Yu Sun,Zheng Wang,Lixing Chen,Li Pan,Jianhua Li*

Main category: cs.LG

TL;DR: LakeMLB是一个针对数据湖环境的多源多表机器学习基准测试，包含Union和Join两种典型场景，提供真实数据集并支持三种集成策略。


<details>
  <summary>Details</summary>
Motivation: 数据湖已成为大规模机器学习的基础平台，但目前缺乏评估数据湖环境中机器学习性能的标准基准测试。

Method: 设计了LakeMLB基准测试，包含两种多表场景（Union和Join），提供三个真实数据集，支持三种集成策略：基于预训练、数据增强和特征增强的方法。

Result: 使用最先进的表格学习方法进行了广泛实验，提供了在复杂数据湖场景下的性能洞察，并发布了数据集和代码。

Conclusion: LakeMLB填补了数据湖机器学习基准测试的空白，为数据湖生态系统中的机器学习研究提供了标准化评估工具。

Abstract: Modern data lakes have emerged as foundational platforms for large-scale machine learning, enabling flexible storage of heterogeneous data and structured analytics through table-oriented abstractions. Despite their growing importance, standardized benchmarks for evaluating machine learning performance in data lake environments remain scarce. To address this gap, we present LakeMLB (Data Lake Machine Learning Benchmark), designed for the most common multi-source, multi-table scenarios in data lakes. LakeMLB focuses on two representative multi-table scenarios, Union and Join, and provides three real-world datasets for each scenario, covering government open data, finance, Wikipedia, and online marketplaces. The benchmark supports three representative integration strategies: pre-training-based, data augmentation-based, and feature augmentation-based approaches. We conduct extensive experiments with state-of-the-art tabular learning methods, offering insights into their performance under complex data lake scenarios. We release both datasets and code to facilitate rigorous research on machine learning in data lake ecosystems; the benchmark is available at https://github.com/zhengwang100/LakeMLB.

</details>


### [54] [Chamfer-Linkage for Hierarchical Agglomerative Clustering](https://arxiv.org/abs/2602.10444)
*Kishen N Gowda,Willem Fletcher,MohammadHossein Bateni,Laxman Dhulipala,D Ellis Hershkowitz,Rajesh Jayaram,Jakub Łącki*

Main category: cs.LG

TL;DR: 提出Chamfer-linkage作为分层凝聚聚类的新连接函数，使用Chamfer距离衡量簇间距离，相比传统连接函数在多种数据集上表现更稳定且质量更高。


<details>
  <summary>Details</summary>
Motivation: 传统分层凝聚聚类（HAC）的连接函数（如单连接、平均连接、Ward方法）在实际应用中表现不稳定，质量参差不齐，且缺乏明确的理论基础。需要一种更可靠、更一致的连接函数来提升聚类质量。

Method: 提出Chamfer-linkage连接函数，使用机器学习中常用的Chamfer距离来衡量簇间距离。该方法具有O(n²)的时间复杂度，与经典连接函数效率相当，可作为直接替代方案。

Result: 理论分析表明Chamfer-linkage HAC可实现O(n²)时间复杂度。实验证明在多种数据集上，Chamfer-linkage相比平均连接和Ward方法等传统连接函数能产生更高质量的聚类结果。

Conclusion: Chamfer-linkage是一种实用且有效的分层凝聚聚类连接函数，可作为传统连接函数的直接替代品，在理论和实践中都能提升聚类质量。

Abstract: Hierarchical Agglomerative Clustering (HAC) is a widely-used clustering method based on repeatedly merging the closest pair of clusters, where inter-cluster distances are determined by a linkage function. Unlike many clustering methods, HAC does not optimize a single explicit global objective; clustering quality is therefore primarily evaluated empirically, and the choice of linkage function plays a crucial role in practice. However, popular classical linkages, such as single-linkage, average-linkage and Ward's method show high variability across real-world datasets and do not consistently produce high-quality clusterings in practice.
  In this paper, we propose \emph{Chamfer-linkage}, a novel linkage function that measures the distance between clusters using the Chamfer distance, a popular notion of distance between point-clouds in machine learning and computer vision. We argue that Chamfer-linkage satisfies desirable concept representation properties that other popular measures struggle to satisfy. Theoretically, we show that Chamfer-linkage HAC can be implemented in $O(n^2)$ time, matching the efficiency of classical linkage functions. Experimentally, we find that Chamfer-linkage consistently yields higher-quality clusterings than classical linkages such as average-linkage and Ward's method across a diverse collection of datasets. Our results establish Chamfer-linkage as a practical drop-in replacement for classical linkage functions, broadening the toolkit for hierarchical clustering in both theory and practice.

</details>


### [55] [A Unified Theory of Random Projection for Influence Functions](https://arxiv.org/abs/2602.10449)
*Pingbang Hu,Yuzheng Hu,Jiaqi W. Ma,Han Zhao*

Main category: cs.LG

TL;DR: 论文提出了影响函数投影的统一理论，分析了随机投影在保持影响函数计算准确性方面的条件，包括无正则化、正则化和结构化近似等情况。


<details>
  <summary>Details</summary>
Motivation: 现代过参数化模型中，计算或反转大规模曲率矩阵F是计算上不可行的。虽然随机投影常用于可扩展的影响计算，但现有理论（如Johnson-Lindenstrauss引理）只保证欧几里得几何的近似保持，未解决投影与矩阵逆、正则化等技术的交互问题。

Method: 开发统一理论分析投影何时能保持影响函数。分析三种情况：1) 无正则化投影：要求投影矩阵在F的值域上单射；2) 正则化投影：岭正则化改变投影障碍，近似保证由正则化尺度下的有效维度决定；3) 因子化影响：对于Kronecker分解的曲率，即使投影具有行相关性，保证仍然成立。还分析了值域外测试梯度的泄漏项。

Result: 理论结果表明：1) 无正则化时，精确保持需要投影维度m≥rank(F)；2) 正则化投影的近似保证由有效维度控制；3) 对于Kronecker分解的曲率，解耦投影即使违反i.i.d.假设也能保持保证；4) 对于一般测试点，可以量化值域外分量的泄漏项。

Conclusion: 该工作建立了投影保持影响函数的新理论框架，为实践中选择投影维度提供了原则性指导，解决了随机投影与影响计算中逆运算、正则化和结构化近似的交互问题。

Abstract: Influence functions and related data attribution scores take the form of $g^{\top}F^{-1}g^{\prime}$, where $F\succeq 0$ is a curvature operator. In modern overparameterized models, forming or inverting $F\in\mathbb{R}^{d\times d}$ is prohibitive, motivating scalable influence computation via random projection with a sketch $P \in \mathbb{R}^{m\times d}$. This practice is commonly justified via the Johnson--Lindenstrauss (JL) lemma, which ensures approximate preservation of Euclidean geometry for a fixed dataset. However, JL does not address how sketching behaves under inversion. Furthermore, there is no existing theory that explains how sketching interacts with other widely-used techniques, such as ridge regularization and structured curvature approximations.
  We develop a unified theory characterizing when projection provably preserves influence functions. When $g,g^{\prime}\in\text{range}(F)$, we show that: 1) Unregularized projection: exact preservation holds iff $P$ is injective on $\text{range}(F)$, which necessitates $m\geq \text{rank}(F)$; 2) Regularized projection: ridge regularization fundamentally alters the sketching barrier, with approximation guarantees governed by the effective dimension of $F$ at the regularization scale; 3) Factorized influence: for Kronecker-factored curvatures $F=A\otimes E$, the guarantees continue to hold for decoupled sketches $P=P_A\otimes P_E$, even though such sketches exhibit row correlations that violate i.i.d. assumptions. Beyond this range-restricted setting, we analyze out-of-range test gradients and quantify a \emph{leakage} term that arises when test gradients have components in $\ker(F)$. This yields guarantees for influence queries on general test points.
  Overall, this work develops a novel theory that characterizes when projection provably preserves influence and provides principled guidance for choosing the sketch size in practice.

</details>


### [56] [Constructing Industrial-Scale Optimization Modeling Benchmark](https://arxiv.org/abs/2602.10450)
*Zhong Li,Hongliang Lu,Tao Wei,Wenyu Liu,Yuxuan Chen,Yuan Lan,Fan Zhang,Zaiwen Wen*

Main category: cs.LG

TL;DR: 提出了MIPLIB-NL基准测试集，用于评估大语言模型将自然语言需求转换为优化模型的能力，填补了工业规模优化问题评估的空白。


<details>
  <summary>Details</summary>
Motivation: 当前优化建模中，将自然语言需求转换为正确的优化公式和可执行代码仍然劳动密集。虽然已有LLM用于此任务，但评估主要基于玩具规模或合成基准，无法反映工业规模问题（10^3-10^6变量/约束）的难度。关键瓶颈是缺乏将自然语言规范与真实优化模型对齐的基准。

Method: 通过结构感知的反向构建方法从MIPLIB 2017中的真实混合整数线性规划创建MIPLIB-NL。流程包括：(1)从平面求解器公式恢复紧凑、可重用的模型结构；(2)在统一的模型-数据分离格式下，基于恢复的结构反向生成自然语言规范；(3)通过专家评审和人-LLM交互进行迭代语义验证，并进行独立重构检查。

Result: 生成了223个一对一重构，保留了原始实例的数学内容，同时支持真实的自然语言到优化评估。实验显示，在现有基准上表现良好的系统在MIPLIB-NL上性能显著下降，暴露了在玩具规模下不可见的失败模式。

Conclusion: MIPLIB-NL填补了自然语言到优化评估的基准空白，能够更真实地评估LLM在工业规模优化问题上的能力，揭示了现有方法在复杂实际问题中的局限性。

Abstract: Optimization modeling underpins decision-making in logistics, manufacturing, energy, and finance, yet translating natural-language requirements into correct optimization formulations and solver-executable code remains labor-intensive. Although large language models (LLMs) have been explored for this task, evaluation is still dominated by toy-sized or synthetic benchmarks, masking the difficulty of industrial problems with $10^{3}$--$10^{6}$ (or more) variables and constraints. A key bottleneck is the lack of benchmarks that align natural-language specifications with reference formulations/solver code grounded in real optimization models. To fill in this gap, we introduce MIPLIB-NL, built via a structure-aware reverse construction methodology from real mixed-integer linear programs in MIPLIB~2017. Our pipeline (i) recovers compact, reusable model structure from flat solver formulations, (ii) reverse-generates natural-language specifications explicitly tied to this recovered structure under a unified model--data separation format, and (iii) performs iterative semantic validation through expert review and human--LLM interaction with independent reconstruction checks. This yields 223 one-to-one reconstructions that preserve the mathematical content of the original instances while enabling realistic natural-language-to-optimization evaluation. Experiments show substantial performance degradation on MIPLIB-NL for systems that perform strongly on existing benchmarks, exposing failure modes invisible at toy scale.

</details>


### [57] [Analyzing Fairness of Neural Network Prediction via Counterfactual Dataset Generation](https://arxiv.org/abs/2602.10457)
*Brian Hyeongseok Kim,Jacqueline L. Mitchell,Chao Wang*

Main category: cs.LG

TL;DR: 提出一种通过构建反事实训练数据集来评估模型公平性的方法，通过修改少量训练标签来观察测试预测的变化，从而分析标签偏见对模型推理的影响。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法主要关注如何修改输入来改变模型预测，但这种方法无法直接评估训练数据中的标签偏见如何影响模型推理。需要一种能够直接分析标签偏见在训练和推理中传播的方法。

Method: 提出反事实数据集方法：1）启发式地排序和修改有限数量的训练样本标签来构建反事实数据集；2）在新数据集上重新训练模型；3）检查选定测试案例的预测是否发生变化。通过追踪训练数据中的偏见如何通过学习算法传播到训练好的网络。

Result: 在7个广泛使用的公平性数据集上评估了超过1100个测试案例，结果显示该方法仅修改了训练标签的一小部分子集，能够精确定位驱动预测变化的关键训练样本。

Conclusion: 反事实数据集方法提供了一种可解释的方式来探测数据集偏见，揭示了训练样本与测试案例之间的联系，为评估模型公平性提供了新视角。

Abstract: Interpreting the inference-time behavior of deep neural networks remains a challenging problem. Existing approaches to counterfactual explanation typically ask: What is the closest alternative input that would alter the model's prediction in a desired way? In contrast, we explore counterfactual datasets. Rather than perturbing the input, our method efficiently finds the closest alternative training dataset, one that differs from the original dataset by changing a few labels. Training a new model on this altered dataset can then lead to a different prediction of a given test instance. This perspective provides a new way to assess fairness by directly analyzing the influence of label bias on training and inference. Our approach can be characterized as probing whether a given prediction depends on biased labels. Since exhaustively enumerating all possible alternate datasets is infeasible, we develop analysis techniques that trace how bias in the training data may propagate through the learning algorithm to the trained network. Our method heuristically ranks and modifies the labels of a bounded number of training examples to construct a counterfactual dataset, retrains the model, and checks whether its prediction on a chosen test case changes. We evaluate our approach on feedforward neural networks across over 1100 test cases from 7 widely-used fairness datasets. Results show that it modifies only a small subset of training labels, highlighting its ability to pinpoint the critical training examples that drive prediction changes. Finally, we demonstrate how our counterfactual datasets reveal connections between training examples and test cases, offering an interpretable way to probe dataset bias.

</details>


### [58] [Driving Reaction Trajectories via Latent Flow Matching](https://arxiv.org/abs/2602.10476)
*Yili Shen,Xiangliang Zhang*

Main category: cs.LG

TL;DR: LatentRxnFlow：基于连续潜在轨迹的反应预测新范式，在保持高准确率的同时提供轨迹级诊断和不确定性评估


<details>
  <summary>Details</summary>
Motivation: 当前反应预测模型多为一次性映射，缺乏对反应过程的洞察；逐步生成方法又需要机制监督和离散符号编辑。需要一种既能保持高精度又能提供反应过程透明度的新方法。

Method: 提出LatentRxnFlow，将反应建模为锚定在热力学产物状态的连续潜在轨迹。基于条件流匹配，直接从标准反应物-产物对学习时间依赖的潜在动力学，无需机制注释或中间标签。

Result: 在USPTO基准测试中达到最先进性能；连续轨迹公式暴露完整生成轨迹，实现轨迹级诊断；潜在轨迹分析能定位和表征失败模式；几何特性提供认知不确定性信号。

Conclusion: LatentRxnFlow将强预测准确性与改进的透明度、可诊断性和不确定性意识相结合，推动反应预测向高通量发现工作流程中更可信的部署发展。

Abstract: Recent advances in reaction prediction have achieved near-saturated accuracy on standard benchmarks (e.g., USPTO), yet most state-of-the-art models formulate the task as a one-shot mapping from reactants to products, offering limited insight into the underlying reaction process. Procedural alternatives introduce stepwise generation but often rely on mechanism-specific supervision, discrete symbolic edits, and computationally expensive inference. In this work, we propose LatentRxnFlow, a new reaction prediction paradigm that models reactions as continuous latent trajectories anchored at the thermodynamic product state. Built on Conditional Flow Matching, our approach learns time-dependent latent dynamics directly from standard reactant-product pairs, without requiring mechanistic annotations or curated intermediate labels. While LatentRxnFlow achieves state-of-the-art performance on USPTO benchmarks, more importantly, the continuous formulation exposes the full generative trajectory, enabling trajectory-level diagnostics that are difficult to realize with discrete or one-shot models. We show that latent trajectory analysis allows us to localize and characterize failure modes and to mitigate certain errors via gated inference. Furthermore, geometric properties of the learned trajectories provide an intrinsic signal of epistemic uncertainty, helping prioritize reliably predictable reaction outcomes and flag ambiguous cases for additional validation. Overall, LatentRxnFlow combines strong predictive accuracy with improved transparency, diagnosability, and uncertainty awareness, moving reaction prediction toward more trustworthy deployment in high-throughput discovery workflows.

</details>


### [59] [Learning Adaptive Distribution Alignment with Neural Characteristic Function for Graph Domain Adaptation](https://arxiv.org/abs/2602.10489)
*Wei Chen,Xingyu Guo,Shuang Li,Zhao Zhang,Yan Zhong,Fuzhen Zhuang,Deqing wang*

Main category: cs.LG

TL;DR: ADAlign是一个用于图域自适应的自适应分布对齐框架，无需手动指定对齐标准，能自动识别并联合对齐每个迁移中最相关的差异，处理属性、结构及其依赖关系之间的相互作用。


<details>
  <summary>Details</summary>
Motivation: 现有图域自适应方法通过手动选择图元素（如节点属性或结构统计）进行对齐，需要手动设计图滤波器来提取相关特征。这些方法不够灵活，依赖特定场景的启发式方法，且当主导差异在不同迁移场景中变化时效果不佳。

Method: 提出了ADAlign框架，引入神经谱差异（NSD）作为理论上有原则的参数化距离，在谱域中使用神经特征函数编码所有阶的特征-结构依赖关系，同时通过可学习的频率采样器以极小极大范式自适应强调每个任务中最具信息量的谱分量。

Result: 在10个数据集和16个迁移任务上的广泛实验表明，ADAlign不仅优于最先进的基线方法，而且实现了效率提升，具有更低的内存使用和更快的训练速度。

Conclusion: ADAlign通过自适应识别和联合对齐最相关的分布差异，提供了一种灵活、场景感知且对多样化和动态演化偏移具有鲁棒性的图域自适应解决方案，无需手动指定对齐标准。

Abstract: Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs to unlabeled target graphs but is challenged by complex, multi-faceted distributional shifts. Existing methods attempt to reduce distributional shifts by aligning manually selected graph elements (e.g., node attributes or structural statistics), which typically require manually designed graph filters to extract relevant features before alignment. However, such approaches are inflexible: they rely on scenario-specific heuristics, and struggle when dominant discrepancies vary across transfer scenarios. To address these limitations, we propose \textbf{ADAlign}, an Adaptive Distribution Alignment framework for GDA. Unlike heuristic methods, ADAlign requires no manual specification of alignment criteria. It automatically identifies the most relevant discrepancies in each transfer and aligns them jointly, capturing the interplay between attributes, structures, and their dependencies. This makes ADAlign flexible, scenario-aware, and robust to diverse and dynamically evolving shifts. To enable this adaptivity, we introduce the Neural Spectral Discrepancy (NSD), a theoretically principled parametric distance that provides a unified view of cross-graph shifts. NSD leverages neural characteristic function in the spectral domain to encode feature-structure dependencies of all orders, while a learnable frequency sampler adaptively emphasizes the most informative spectral components for each task via minimax paradigm. Extensive experiments on 10 datasets and 16 transfer tasks show that ADAlign not only outperforms state-of-the-art baselines but also achieves efficiency gains with lower memory usage and faster training.

</details>


### [60] [Low-Dimensional Execution Manifolds in Transformer Learning Dynamics: Evidence from Modular Arithmetic Tasks](https://arxiv.org/abs/2602.10496)
*Yongzhong Xu*

Main category: cs.LG

TL;DR: Transformer训练轨迹在高维参数空间中快速坍缩到3-4维的低维执行流形，这种几何结构解释了注意力集中、SGD可积性等经验现象。


<details>
  <summary>Details</summary>
Motivation: 研究过参数化transformer模型学习动力学的几何结构，探索为什么在如此高维的参数空间中，transformer能够有效学习，以及训练过程中参数空间的几何特性如何影响学习行为。

Method: 通过精心控制的模运算任务，分析transformer训练轨迹的几何特性。使用d=128维参数空间，研究训练过程中参数轨迹的维度坍缩现象，并分析随机种子和任务难度对几何结构的影响。

Result: 发现transformer训练轨迹快速坍缩到3-4维的低维执行流形，这种维度坍缩具有鲁棒性。该几何结构解释了：(1)注意力集中现象是执行流形内路由坐标的饱和；(2)SGD在投影到执行子空间时表现出近似可积动力学；(3)稀疏自编码器能捕获辅助路由结构但无法隔离执行本身。

Conclusion: 提出了理解transformer学习的统一几何框架：大部分参数用于吸收优化干扰，而核心计算发生在显著缩减的低维子空间中。这对可解释性、训练课程设计和理解过参数化在神经网络学习中的作用具有重要意义。

Abstract: We investigate the geometric structure of learning dynamics in overparameterized transformer models through carefully controlled modular arithmetic tasks. Our primary finding is that despite operating in high-dimensional parameter spaces ($d=128$), transformer training trajectories rapidly collapse onto low-dimensional execution manifolds of dimension $3$--$4$. This dimensional collapse is robust across random seeds and moderate task difficulties, though the orientation of the manifold in parameter space varies between runs. We demonstrate that this geometric structure underlies several empirically observed phenomena: (1) sharp attention concentration emerges as saturation along routing coordinates within the execution manifold, (2) stochastic gradient descent (SGD) exhibits approximately integrable dynamics when projected onto the execution subspace, with non-integrability confined to orthogonal staging directions, and (3) sparse autoencoders capture auxiliary routing structure but fail to isolate execution itself, which remains distributed across the low-dimensional manifold. Our results suggest a unifying geometric framework for understanding transformer learning, where the vast majority of parameters serve to absorb optimization interference while core computation occurs in a dramatically reduced subspace. These findings have implications for interpretability, training curriculum design, and understanding the role of overparameterization in neural network learning.

</details>


### [61] [Enhancing Ride-Hailing Forecasting at DiDi with Multi-View Geospatial Representation Learning from the Web](https://arxiv.org/abs/2602.10502)
*Xixuan Hao,Guicheng Li,Daiqiang Wu,Xusen Guo,Yumeng Zhu,Zhichao Zou,Peng Zhen,Yao Yao,Yuxuan Liang*

Main category: cs.LG

TL;DR: MVGR-Net：通过多视图地理空间表示学习和LLM提示微调，解决网约车预测中的地理空间异质性和外部事件影响问题


<details>
  <summary>Details</summary>
Motivation: 网约车服务改变了城市出行模式，但预测面临地理空间异质性和外部事件高度敏感性的挑战。现有方法难以同时捕捉区域语义属性和时空移动模式，且对外部事件适应性不足。

Method: 提出两阶段框架MVGR-Net：1）预训练阶段整合POI和时空移动模式，从语义属性和时空移动模式双视图学习地理空间表示；2）预测阶段采用提示增强框架，微调大语言模型并融入外部事件信息。

Result: 在滴滴真实数据集上的实验表明，该方法达到了最先进的性能，有效提升了网约车预测的准确性。

Conclusion: MVGR-Net通过多视图地理空间表示学习和LLM提示微调，成功解决了网约车预测中的地理空间异质性和外部事件影响问题，为城市交通优化提供了有效工具。

Abstract: The proliferation of ride-hailing services has fundamentally transformed urban mobility patterns, making accurate ride-hailing forecasting crucial for optimizing passenger experience and urban transportation efficiency. However, ride-hailing forecasting faces significant challenges due to geospatial heterogeneity and high susceptibility to external events. This paper proposes MVGR-Net(Multi-View Geospatial Representation Learning), a novel framework that addresses these challenges through a two-stage approach. In the pretraining stage, we learn comprehensive geospatial representations by integrating Points-of-Interest and temporal mobility patterns to capture regional characteristics from both semantic attribute and temporal mobility pattern views. The forecasting stage leverages these representations through a prompt-empowered framework that fine-tunes Large Language Models while incorporating external events. Extensive experiments on DiDi's real-world datasets demonstrate the state-of-the-art performance.

</details>


### [62] [Learning Structure-Semantic Evolution Trajectories for Graph Domain Adaptation](https://arxiv.org/abs/2602.10506)
*Wei Chen,Xingyu Guo,Shuang Li,Yan Zhong,Zhao Zhang,Fuzhen Zhuang,Hongrui Liu,Libang Zhang,Guo Ye,Huimei He*

Main category: cs.LG

TL;DR: DiffGDA：基于扩散模型的图域自适应方法，将域适应过程建模为连续时间生成过程，通过SDEs联合建模结构和语义转换，在14个图迁移任务上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图域自适应方法通常采用离散化策略（如构建中间图或逐步对齐），但在现实场景中图结构是连续非线性演化的，固定步长的对齐难以逼近实际变换过程。

Method: 提出DiffGDA方法，将域适应过程建模为连续时间生成过程，使用随机微分方程（SDEs）描述从源图到目标图的演化，引入领域感知网络引导生成过程向目标域演进。

Result: 在8个真实世界数据集的14个图迁移任务上，DiffGDA始终优于最先进的基线方法。理论证明扩散过程在潜在空间中收敛到连接源域和目标域的最优解。

Conclusion: DiffGDA通过连续时间扩散过程有效建模图域自适应，克服了离散策略的局限性，为处理连续演化的图结构提供了更自然的框架。

Abstract: Graph Domain Adaptation (GDA) aims to bridge distribution shifts between domains by transferring knowledge from well-labeled source graphs to given unlabeled target graphs. One promising recent approach addresses graph transfer by discretizing the adaptation process, typically through the construction of intermediate graphs or stepwise alignment procedures. However, such discrete strategies often fail in real-world scenarios, where graph structures evolve continuously and nonlinearly, making it difficult for fixed-step alignment to approximate the actual transformation process. To address these limitations, we propose \textbf{DiffGDA}, a \textbf{Diff}usion-based \textbf{GDA} method that models the domain adaptation process as a continuous-time generative process. We formulate the evolution from source to target graphs using stochastic differential equations (SDEs), enabling the joint modeling of structural and semantic transitions. To guide this evolution, a domain-aware network is introduced to steer the generative process toward the target domain, encouraging the diffusion trajectory to follow an optimal adaptation path. We theoretically show that the diffusion process converges to the optimal solution bridging the source and target domains in the latent space. Extensive experiments on 14 graph transfer tasks across 8 real-world datasets demonstrate DiffGDA consistently outperforms state-of-the-art baselines.

</details>


### [63] [Don't Eliminate Cut: Exponential Separations in LLM-Based Theorem Proving](https://arxiv.org/abs/2602.10512)
*Sho Sonoda,Shunta Akiyama,Yuya Uezato*

Main category: cs.LG

TL;DR: 论文为LLM指导的形式定理证明建立理论分析框架，证明在证明DAG存在可重用结构时，分层学习比平面学习需要指数级更少的数据。


<details>
  <summary>Details</summary>
Motivation: 解释形式定理证明中经验成功与最坏情况复杂性之间的差距，为最近基于子目标分解的代理定理证明器提供理论依据。

Method: 将策略建议建模为有限时域确定性MDP中的随机策略，引入由参考策略生成的问题分布，包括具有证明DAG结构的隐变量模型，在top-k搜索协议和Tsybakov型边界条件下分析。

Result: 推导出有限时域成功概率的下界，主要分离结果表明：当消割将深度D的DAG扩展为规模Ω(Λ^D)的无割树，而割感知分层过程规模为O(λ^D)且λ≪Λ时，平面学习者需要指数级更多数据。

Conclusion: 为代理定理证明器中的子目标分解提供了原则性理论依据，解释了分层学习在存在可重用引理/草图结构时的优势。

Abstract: We develop a theoretical analysis of LLM-guided formal theorem proving in interactive proof assistants (e.g., Lean) by modeling tactic proposal as a stochastic policy in a finite-horizon deterministic MDP. To capture modern representation learning, we treat the state and action spaces as general compact metric spaces and assume Lipschitz policies. To explain the gap between worst-case hardness and empirical success, we introduce problem distributions generated by a reference policy $q$, including a latent-variable model in which proofs exhibit reusable cut/lemma/sketch structure represented by a proof DAG. Under a top-$k$ search protocol and Tsybakov-type margin conditions, we derive lower bounds on finite-horizon success probability that decompose into search and learning terms, with learning controlled by sequential Rademacher/covering complexity. Our main separation result shows that when cut elimination expands a DAG of depth $D$ into a cut-free tree of size $Ω(Λ^D)$ while the cut-aware hierarchical process has size $O(λ^D)$ with $λ\llΛ$, a flat (cut-free) learner provably requires exponentially more data than a cut-aware hierarchical learner. This provides a principled justification for subgoal decomposition in recent agentic theorem provers.

</details>


### [64] [Prioritize the Process, Not Just the Outcome: Rewarding Latent Thought Trajectories Improves Reasoning in Looped Language Models](https://arxiv.org/abs/2602.10520)
*Williams Jonathan,Tureci Esin*

Main category: cs.LG

TL;DR: RLTT：为循环语言模型设计的强化学习框架，通过在整个潜在推理轨迹上分配奖励来解决传统方法只奖励最终状态的问题，显著提升数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法（如GRPO）在训练循环语言模型时存在根本性不匹配：它们只奖励最终潜在状态，而循环语言模型进行多步潜在推理。这种信用分配不匹配导致强化学习无法有效提升循环语言模型的推理能力。

Method: 提出RLTT（Reward Latent Thought Trajectories）框架，将奖励分布到完整的潜在推理轨迹上，实现密集的轨迹级信用分配。该方法不依赖外部验证器，可以直接替代GRPO且开销可忽略。

Result: 在Ouro-2.6B-Thinking模型上，RLTT相比GRPO在数学推理基准上有显著提升：MATH-500准确率提升14.4%，AIME24提升16.6%，BeyondAIME提升10.0%。尽管仅在数学数据上训练，RLTT也能有效迁移到非数学推理任务。

Conclusion: 轨迹级信用分配是循环语言模型强化学习的关键，RLTT框架有效解决了传统方法的信用分配不匹配问题，显著提升了模型的推理能力，并展示了良好的迁移性。

Abstract: Looped Language Models (LoopLMs) perform multi-step latent reasoning prior to token generation and outperform conventional LLMs on reasoning benchmarks at smaller parameter budgets. However, attempts to further improve LoopLM reasoning with reinforcement learning have failed - standard objectives such as Group Relative Policy Optimization (GRPO) only assign credit to the final latent state, creating a fundamental mismatch with the model's internal computation. To resolve this, we introduce RLTT (Reward Latent Thought Trajectories), a reinforcement learning framework which distributes reward across the full latent reasoning trajectory. RLTT provides dense, trajectory-level credit assignment without relying on external verifiers and can directly replace GRPO with negligible overhead. Across extensive experiments with Ouro-2.6B-Thinking under identical training and inference conditions, RLTT yields substantial improvements over GRPO on challenging mathematical reasoning benchmarks, improving accuracy by +14.4% on MATH-500, +16.6% on AIME24, and +10.0% on BeyondAIME. Despite being trained exclusively on mathematics, RLTT also transfers effectively to non-mathematical reasoning benchmarks, demonstrating the effectiveness of trajectory-level credit assignment for reinforcement learning in LoopLMs.

</details>


### [65] [A Swap-Adversarial Framework for Improving Domain Generalization in Electroencephalography-Based Parkinson's Disease Prediction](https://arxiv.org/abs/2602.10528)
*Seongwon Jin,Hanseul Choi,Sunggu Yang,Sungho Park,Jibum Kim*

Main category: cs.LG

TL;DR: 提出首个帕金森病预测的可复现基准数据集，并开发Swap-Adversarial Framework（SAF）解决ECoG数据中的高主体间变异性和高维低样本问题，实现跨ECoG和EEG数据的鲁棒域泛化。


<details>
  <summary>Details</summary>
Motivation: ECoG在帕金森病早期预测中比传统EEG更有优势，但受限于人类研究的伦理约束和缺乏公开基准数据集，导致可复现比较困难。

Method: 提出Swap-Adversarial Framework（SAF）：1）鲁棒预处理；2）主体间平衡通道交换（ISBCS）进行跨主体增强；3）域对抗训练抑制主体特异性偏差。ISBCS随机交换不同主体间的通道以减少主体间变异性，域对抗训练联合鼓励模型学习任务相关的共享特征。

Result: 在跨主体、跨会话和跨数据集设置下，该方法在所有基准方法中表现最优，在高度可变环境中改进最显著。在公共EEG基准间的跨数据集性能也表现优异，展示了不仅在ECoG内部，而且到EEG数据的强泛化能力。

Conclusion: 提出的新数据集和SAF方法有效解决了ECoG数据中的主体间变异性和高维低样本问题，实现了跨模态的鲁棒泛化，为帕金森病预测研究提供了可复现的基准。

Abstract: Electroencephalography (ECoG) offers a promising alternative to conventional electrocorticography (EEG) for the early prediction of Parkinson's disease (PD), providing higher spatial resolution and a broader frequency range. However, reproducible comparisons has been limited by ethical constraints in human studies and the lack of open benchmark datasets. To address this gap, we introduce a new dataset, the first reproducible benchmark for PD prediction. It is constructed from long-term ECoG recordings of 6-hydroxydopamine (6-OHDA)-induced rat models and annotated with neural responses measured before and after electrical stimulation. In addition, we propose a Swap-Adversarial Framework (SAF) that mitigates high inter-subject variability and the high-dimensional low-sample-size (HDLSS) problem in ECoG data, while achieving robust domain generalization across ECoG and EEG-based Brain-Computer Interface (BCI) datasets. The framework integrates (1) robust preprocessing, (2) Inter-Subject Balanced Channel Swap (ISBCS) for cross-subject augmentation, and (3) domain-adversarial training to suppress subject-specific bias. ISBCS randomly swaps channels between subjects to reduce inter-subject variability, and domain-adversarial training jointly encourages the model to learn task-relevant shared features. We validated the effectiveness of the proposed method through extensive experiments under cross-subject, cross-session, and cross-dataset settings. Our method consistently outperformed all baselines across all settings, showing the most significant improvements in highly variable environments. Furthermore, the proposed method achieved superior cross-dataset performance between public EEG benchmarks, demonstrating strong generalization capability not only within ECoG but to EEG data. The new dataset and source code will be made publicly available upon publication.

</details>


### [66] [What Makes Value Learning Efficient in Residual Reinforcement Learning?](https://arxiv.org/abs/2602.10539)
*Guozheng Ma,Lu Li,Haoyu Wang,Zixuan Liu,Pierre-Luc Bacon,Dacheng Tao*

Main category: cs.LG

TL;DR: DAWN通过基策略转换作为价值锚点和批评者归一化，解决了残差强化学习中价值学习的冷启动和尺度不匹配瓶颈，显著提升了学习效率。


<details>
  <summary>Details</summary>
Motivation: 残差强化学习能够通过冻结基策略并学习有界修正来稳定在线精炼预训练策略，但其价值学习存在独特挑战，特别是冷启动病理和结构尺度不匹配问题尚未得到充分理解。

Method: 提出DAWN方法，包含两个核心组件：1）使用基策略转换作为价值锚点进行隐式预热，解决冷启动问题；2）通过批评者归一化有效恢复表示敏感性，解决残差贡献被基动作淹没的尺度不匹配问题。

Result: DAWN在多样化基准测试、策略架构和观测模态中展现出显著效率提升，证明了该方法能有效解决残差强化学习中的价值学习瓶颈。

Conclusion: 通过系统分析残差强化学习中价值学习的瓶颈机制，发现简单而原则性的解决方案（基策略锚点和批评者归一化）足以显著提升学习效率，DAWN为此提供了一个最小化但有效的方法。

Abstract: Residual reinforcement learning (RL) enables stable online refinement of expressive pretrained policies by freezing the base and learning only bounded corrections. However, value learning in residual RL poses unique challenges that remain poorly understood. In this work, we identify two key bottlenecks: cold start pathology, where the critic lacks knowledge of the value landscape around the base policy, and structural scale mismatch, where the residual contribution is dwarfed by the base action. Through systematic investigation, we uncover the mechanisms underlying these bottlenecks, revealing that simple yet principled solutions suffice: base-policy transitions serve as an essential value anchor for implicit warmup, and critic normalization effectively restores representation sensitivity for discerning value differences. Based on these insights, we propose DAWN (Data-Anchored Warmup and Normalization), a minimal approach targeting efficient value learning in residual RL. By addressing these bottlenecks, DAWN demonstrates substantial efficiency gains across diverse benchmarks, policy architectures, and observation modalities.

</details>


### [67] [Bridging the Compression-Precision Paradox: A Hybrid Architecture for Clinical EEG Report Generation with Guaranteed Measurement Accuracy](https://arxiv.org/abs/2602.10544)
*Wuyang Zhang,Zhen Luo,Chuqiao Gu,Jianming Ma,Yebo Cao,Wangming Yuan,Yinzhi Jin*

Main category: cs.LG

TL;DR: 提出混合架构分离测量提取与文本生成，通过信号处理计算精确临床值再压缩，实现首个保证临床测量精度的自动化EEG报告系统


<details>
  <summary>Details</summary>
Motivation: 现有自动化EEG监测系统面临双重限制：临床EEG记录超过LLM上下文窗口需要极端压缩（400:1+），这会破坏细粒度时间精度；LLM缺乏时间序列理解能力，依赖压缩表示的统计关联，导致产生临床错误的测量值

Method: 采用混合架构：1）通过信号处理在压缩前计算精确临床值；2）使用跨模态桥接进行EEG到语言的翻译；3）采用参数高效微调，在冻结槽周围进行约束解码；4）多速率采样保持长程上下文同时保留事件级精度

Result: 在TUH和CHB-MIT数据集上评估，实现60%更少的误报、50%更快的检测速度，以及亚临床测量精度。这是首个保证临床测量准确性的自动化EEG报告系统

Conclusion: 通过分离测量提取与文本生成，在压缩前计算精确临床值，解决了LLM在EEG分析中的双重限制，实现了临床可靠的自动化EEG监测报告

Abstract: Automated EEG monitoring requires clinician-level precision for seizure detection and reporting. Clinical EEG recordings exceed LLM context windows, requiring extreme compression (400:1+ ratios) that destroys fine-grained temporal precision. A 0.5 Hz error distinguishes absence epilepsy from Lennox-Gastaut syndrome. LLMs lack inherent time-series comprehension and rely on statistical associations from compressed representations. This dual limitation causes systems to hallucinate clinically incorrect measurement values.
  We separate measurement extraction from text generation. Our hybrid architecture computes exact clinical values via signal processing before compression, employs a cross-modal bridge for EEG-to-language translation, and uses parameter-efficient fine-tuning with constrained decoding around frozen slots. Multirate sampling maintains long-range context while preserving event-level precision. Evaluation on TUH and CHB-MIT datasets achieves 60% fewer false alarms, 50% faster detection, and sub-clinical measurement precision. This is the first system guaranteeing clinical measurement accuracy in automated EEG reports.

</details>


### [68] [$μ$pscaling small models: Principled warm starts and hyperparameter transfer](https://arxiv.org/abs/2602.10545)
*Yuxin Ma,Nan Chen,Mateo Díaz,Soufiane Hayou,Dmitriy Kunisky,Soledad Villar*

Main category: cs.LG

TL;DR: 提出一种通用的模型放大方法，基于μP理论保证放大后模型与原始模型的等价性，并扩展μTransfer技术实现超参数的高效迁移，避免在大模型上直接调参的成本。


<details>
  <summary>Details</summary>
Motivation: 现代大规模神经网络通常训练和发布多种尺寸以适应不同的推理预算。模型放大技术（从训练好的小模型初始化大模型）可以加速收敛，但需要在大模型上调整超参数，成本过高。目前常用的方法（在小模型上调参并通过超参数缩放定律外推）在放大场景下是否有效尚不明确。

Method: 1. 基于μP和任意维度架构理论，提出通用的模型放大方法，适用于广泛的架构和优化器，理论保证放大后模型与加宽版本的等价性，支持无限宽度极限的严格分析。2. 扩展μTransfer理论，提出适用于放大模型的超参数迁移技术。

Result: 在真实数据集和架构上实证验证了该方法的有效性，实现了超参数的高效迁移，避免了直接在大模型上调参的高成本。

Conclusion: 提出的通用放大方法和超参数迁移技术解决了模型放大中的超参数调优难题，为高效训练不同尺寸的神经网络提供了理论支持和实用方案。

Abstract: Modern large-scale neural networks are often trained and released in multiple sizes to accommodate diverse inference budgets. To improve efficiency, recent work has explored model upscaling: initializing larger models from trained smaller ones in order to transfer knowledge and accelerate convergence. However, this method can be sensitive to hyperparameters that need to be tuned at the target upscaled model size, which is prohibitively costly to do directly. It remains unclear whether the most common workaround -- tuning on smaller models and extrapolating via hyperparameter scaling laws -- is still sound when using upscaling. We address this with principled approaches to upscaling with respect to model widths and efficiently tuning hyperparameters in this setting. First, motivated by $μ$P and any-dimensional architectures, we introduce a general upscaling method applicable to a broad range of architectures and optimizers, backed by theory guaranteeing that models are equivalent to their widened versions and allowing for rigorous analysis of infinite-width limits. Second, we extend the theory of $μ$Transfer to a hyperparameter transfer technique for models upscaled using our method and empirically demonstrate that this method is effective on realistic datasets and architectures.

</details>


### [69] [Contrastive Learning for Multi Label ECG Classification with Jaccard Score Based Sigmoid Loss](https://arxiv.org/abs/2602.10553)
*Junichiro Takahashi,Masataka Sato,Satoshi Kodeta,Norihiko Takeda*

Main category: cs.LG

TL;DR: 该研究开发了一个用于多模态预训练的鲁棒心电图编码器，通过改进SigLIP模型和损失函数，显著提升了多标签心电图分类性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医学AI中取得进展，但现有模型（如MedGemini、MedGemma）在心电图任务上表现有限或完全不支持。心电图解读具有挑战性，诊断准确性依赖医生经验，而超声心动图虽信息丰富但设备要求高。需要构建专门的心电图编码器来处理真实医院数据。

Method: 采用基于CLIP的SigLIP模型，引入针对心电图多标签特性的改进损失函数。增加嵌入维度，应用随机裁剪来缓解数据漂移。在语言模型中融入医学知识，并进行逐标签分析。

Result: 实验表明，融入医学知识和应用改进损失函数能显著提升多标签心电图分类性能。增加嵌入维度和随机裁剪进一步增强了模型表现。逐标签分析揭示了哪些心电图发现更容易或更难预测。

Conclusion: 该研究为开发利用心电图数据的医学模型提供了基础框架，通过专门设计的编码器和优化策略，有效提升了心电图多标签分类的准确性和鲁棒性。

Abstract: Recent advances in large language models (LLMs) have enabled the development of multimodal medical AI. While models such as MedGemini achieve high accuracy on VQA tasks like USMLE MM, their performance on ECG based tasks remains limited, and some models, such as MedGemma, do not support ECG data at all. Interpreting ECGs is inherently challenging, and diagnostic accuracy can vary depending on the interpreter's experience. Although echocardiography provides rich diagnostic information, it requires specialized equipment and personnel, limiting its availability. In this study, we focus on constructing a robust ECG encoder for multimodal pretraining using real world hospital data. We employ SigLIP, a CLIP based model with a sigmoid based loss function enabling multi label prediction, and introduce a modified loss function tailored to the multi label nature of ECG data. Experiments demonstrate that incorporating medical knowledge in the language model and applying the modified loss significantly improve multi label ECG classification. To further enhance performance, we increase the embedding dimensionality and apply random cropping to mitigate data drift. Finally, per label analysis reveals which ECG findings are easier or harder to predict. Our study provides a foundational framework for developing medical models that utilize ECG data.

</details>


### [70] [Online Min-Max Optimization: From Individual Regrets to Cumulative Saddle Points](https://arxiv.org/abs/2602.10565)
*Abhijeet Vyas,Brian Bullins*

Main category: cs.LG

TL;DR: 本文提出并研究了一种基于累积鞍点的在线min-max优化方法，超越了传统的凸-凹设置，引入了静态对偶间隙和动态鞍点遗憾等新性能度量。


<details>
  <summary>Details</summary>
Motivation: 传统静态纳什均衡与个体遗憾在强凸-强凹函数中不兼容，需要新的性能度量来评估在线min-max优化算法。

Method: 通过将问题约简到经典在线凸优化问题，提出算法来界限静态对偶间隙和动态鞍点遗憾，并在强凸-强凹性、min-max指数凹性和双边Polyak-Łojasiewicz条件下进行分析。

Result: 在强凸-强凹性和min-max指数凹性条件下获得了静态对偶间隙和动态鞍点遗憾的界限，并建立了一类满足min-max指数凹性的函数，捕捉了经典投资组合选择问题的双玩家变体。

Conclusion: 本文为超越凸-凹设置的在线min-max优化提供了新的理论框架和性能度量，扩展了在线优化理论的应用范围。

Abstract: We propose and study an online version of min-max optimization based on cumulative saddle points under a variety of performance measures beyond convex-concave settings. After first observing the incompatibility of (static) Nash equilibrium (SNE-Reg$_T$) with individual regrets even for strongly convex-strongly concave functions, we propose an alternate \emph{static} duality gap (SDual-Gap$_T$) inspired by the online convex optimization (OCO) framework. We provide algorithms that, using a reduction to classic OCO problems, achieve bounds for SDual-Gap$_T$~and a novel \emph{dynamic} saddle point regret (DSP-Reg$_T$), which we suggest naturally represents a min-max version of the dynamic regret in OCO. We derive our bounds for SDual-Gap$_T$~and DSP-Reg$_T$~under strong convexity-strong concavity and a min-max notion of exponential concavity (min-max EC), and in addition we establish a class of functions satisfying min-max EC~that captures a two-player variant of the classic portfolio selection problem. Finally, for a dynamic notion of regret compatible with individual regrets, we derive bounds under a two-sided Polyak-Łojasiewicz (PL) condition.

</details>


### [71] [Gauss-Newton Unlearning for the LLM Era](https://arxiv.org/abs/2602.10568)
*Lev McKinney,Anvith Thudi,Juhan Bae,Tara Rezaei,Nicolas Papernot,Sheila A. McIlraith,Roger Grosse*

Main category: cs.LG

TL;DR: K-FADE：基于K-FAC的LLM遗忘方法，通过少量高斯-牛顿步骤实现高效遗忘，在保留集上性能损失最小


<details>
  <summary>Details</summary>
Motivation: 传统LLM训练可能产生不可接受的输出，遗忘方法可以减少这些输出，但会损害模型在其他分布上的性能。需要一种既能有效遗忘又能最小化性能损失的方法。

Method: 提出K-FADE方法：使用遗忘集计算少量上坡高斯-牛顿步骤，结合K-FAC参数化Hessian近似，将输出空间约束转化为权重约束，实现高效准确的LLM遗忘。

Result: 在WMDP和ToFU基准测试中，K-FADE能有效抑制遗忘集输出，近似重新训练结果，同时在保留集上的输出改变最小。遗忘更新可重复应用，便于后续训练维护。

Conclusion: K-FADE提供了一种概念简单、性能优越的LLM遗忘方法，通过高斯-牛顿步骤和K-FAC近似，实现了遗忘效果与保留性能的良好平衡。

Abstract: Standard large language model training can create models that produce outputs their trainer deems unacceptable in deployment. The probability of these outputs can be reduced using methods such as LLM unlearning. However, unlearning a set of data (called the forget set) can degrade model performance on other distributions where the trainer wants to retain the model's behavior. To improve this trade-off, we demonstrate that using the forget set to compute only a few uphill Gauss-Newton steps provides a conceptually simple, state-of-the-art unlearning approach for LLMs. While Gauss-Newton steps adapt Newton's method to non-linear models, it is non-trivial to efficiently and accurately compute such steps for LLMs. Hence, our approach crucially relies on parametric Hessian approximations such as Kronecker-Factored Approximate Curvature (K-FAC). We call this combined approach K-FADE (K-FAC for Distribution Erasure). Our evaluation on the WMDP and ToFU benchmarks demonstrates that K-FADE suppresses outputs from the forget set and approximates, in output space, the results of retraining without the forget set. Critically, our method does this while altering the outputs on the retain set less than previous methods. This is because K-FADE transforms a constraint on the model's outputs across the entire retain set into a constraint on the model's weights, allowing the algorithm to minimally change the model's behavior on the retain set at each step. Moreover, the unlearning updates computed by K-FADE can be reapplied later if the model undergoes further training, allowing unlearning to be cheaply maintained.

</details>


### [72] [LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization](https://arxiv.org/abs/2602.10576)
*Boxiao Wang,Kai Li,Tianyi Liu,Chen Li,Junzhe Wang,Yifan Zhang,Jian Cheng*

Main category: cs.LG

TL;DR: 提出PiT-PO框架，通过强化学习将LLM转变为自适应生成器，结合物理约束和token级惩罚，实现物理一致且结构简洁的符号回归。


<details>
  <summary>Details</summary>
Motivation: 现有符号回归方法将LLM视为静态生成器，仅通过提示引导探索，无法根据搜索反馈更新模型内部表示，导致物理不一致或数学冗余表达式。

Method: 提出PiT-PO（Physics-informed Token-regularized Policy Optimization）框架，通过强化学习演化LLM为自适应生成器，采用双重约束机制：强制分层物理有效性，同时应用细粒度token级惩罚抑制冗余结构。

Result: 在标准基准测试中达到最先进性能，成功为挑战性流体动力学问题发现新的湍流模型，并证明小型模型可以超越闭源大型模型，实现高性能科学发现的民主化。

Conclusion: PiT-PO框架通过将LLM转变为自适应生成器，结合物理约束和结构惩罚，实现了物理一致且简洁的符号回归，推动了科学发现的民主化。

Abstract: Symbolic regression aims to distill mathematical equations from observational data. Recent approaches have successfully leveraged Large Language Models (LLMs) to generate equation hypotheses, capitalizing on their vast pre-trained scientific priors. However, existing frameworks predominantly treat the LLM as a static generator, relying on prompt-level guidance to steer exploration. This paradigm fails to update the model's internal representations based on search feedback, often yielding physically inconsistent or mathematically redundant expressions. In this work, we propose PiT-PO (Physics-informed Token-regularized Policy Optimization), a unified framework that evolves the LLM into an adaptive generator via reinforcement learning. Central to PiT-PO is a dual-constraint mechanism that rigorously enforces hierarchical physical validity while simultaneously applying fine-grained, token-level penalties to suppress redundant structures. Consequently, PiT-PO aligns LLM to produce equations that are both scientifically consistent and structurally parsimonious. Empirically, PiT-PO achieves state-of-the-art performance on standard benchmarks and successfully discovers novel turbulence models for challenging fluid dynamics problems. We also demonstrate that PiT-PO empowers small-scale models to outperform closed-source giants, democratizing access to high-performance scientific discovery.

</details>


### [73] [When Gradient Clipping Becomes a Control Mechanism for Differential Privacy in Deep Learning](https://arxiv.org/abs/2602.10584)
*Mohammad Partohaghighi,Roummel Marcia,Bruce J. West,YangQuan Chen*

Main category: cs.LG

TL;DR: 提出基于控制理论的轻量级自适应梯度裁剪方法，通过权重矩阵谱分析来调整裁剪阈值，避免传统方法依赖逐样本梯度统计的计算开销和敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私训练中的自适应裁剪方法通常依赖逐样本梯度范数统计，这会增加计算开销，并且对数据集和架构敏感。梯度裁剪阈值设置是关键：太小会导致系统性的过裁剪引入优化偏差，太大会使注入的噪声主导更新并降低准确性。

Method: 提出控制驱动的裁剪策略，使用轻量级的仅权重谱诊断来自适应调整阈值。在周期性探测步骤中，方法通过谱分解分析指定的权重矩阵，估计与训练稳定性相关的重尾谱指标。该指标随时间平滑处理，并输入有界反馈控制器，在log域中乘法更新裁剪阈值。

Result: 由于控制器仅使用隐私保护训练期间产生的参数，因此阈值更新是后处理操作，在标准组合计算下不会增加底层DP优化器之外的隐私损失。

Conclusion: 该方法提供了一种计算高效、对数据集和架构不敏感的自适应梯度裁剪方案，通过权重谱分析实现稳定的差分隐私训练，同时保持隐私保证。

Abstract: Privacy-preserving training on sensitive data commonly relies on differentially private stochastic optimization with gradient clipping and Gaussian noise. The clipping threshold is a critical control knob: if set too small, systematic over-clipping induces optimization bias; if too large, injected noise dominates updates and degrades accuracy. Existing adaptive clipping methods often depend on per-example gradient norm statistics, adding computational overhead and introducing sensitivity to datasets and architectures. We propose a control-driven clipping strategy that adapts the threshold using a lightweight, weight-only spectral diagnostic computed from model parameters. At periodic probe steps, the method analyzes a designated weight matrix via spectral decomposition and estimates a heavy-tailed spectral indicator associated with training stability. This indicator is smoothed over time and fed into a bounded feedback controller that updates the clipping threshold multiplicatively in the log domain. Because the controller uses only parameters produced during privacy-preserving training, the resulting threshold updates are post-processing and do not increase privacy loss beyond that of the underlying DP optimizer under standard composition accounting.

</details>


### [74] [Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity](https://arxiv.org/abs/2602.10585)
*Guangzhi Xiong,Sanchit Sinha,Aidong Zhang*

Main category: cs.LG

TL;DR: NAE框架通过专家混合机制和动态门控，在保持可解释性的同时提升预测精度，平衡了可解释性与准确性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 传统广义可加模型(GAMs)具有清晰的特征归因但受限于严格可加性，导致预测性能受限。引入特征交互能提高准确性但会模糊个体特征贡献，需要在可解释性和准确性之间取得平衡。

Method: 提出神经可加专家(NAEs)框架：采用专家混合机制，为每个特征学习多个专门网络；使用动态门控机制跨特征整合信息，放松严格可加约束；引入针对性正则化技术减少专家预测方差。

Result: 理论分析和合成数据实验证明了模型的灵活性；在真实数据集上的广泛评估表明，NAEs在预测准确性和透明特征级解释之间达到了最优平衡。

Conclusion: NAE框架成功解决了可解释性与准确性之间的核心权衡问题，通过专家混合和动态门控机制，在保持特征归因清晰度的同时捕捉复杂特征交互，实现了更好的平衡。

Abstract: The trade-off between interpretability and accuracy remains a core challenge in machine learning. Standard Generalized Additive Models (GAMs) offer clear feature attributions but are often constrained by their strictly additive nature, which can limit predictive performance. Introducing feature interactions can boost accuracy yet may obscure individual feature contributions. To address these issues, we propose Neural Additive Experts (NAEs), a novel framework that seamlessly balances interpretability and accuracy. NAEs employ a mixture of experts framework, learning multiple specialized networks per feature, while a dynamic gating mechanism integrates information across features, thereby relaxing rigid additive constraints. Furthermore, we propose targeted regularization techniques to mitigate variance among expert predictions, facilitating a smooth transition from an exclusively additive model to one that captures intricate feature interactions while maintaining clarity in feature attributions. Our theoretical analysis and experiments on synthetic data illustrate the model's flexibility, and extensive evaluations on real-world datasets confirm that NAEs achieve an optimal balance between predictive accuracy and transparent, feature-level explanations. The code is available at https://github.com/Teddy-XiongGZ/NAE.

</details>


### [75] [TRACE: Theoretical Risk Attribution under Covariate-shift Effects](https://arxiv.org/abs/2602.10588)
*Hosein Anjidani,S. Yahya S. R. Tehrani,Mohammad Mahdi Mojahedian,Mohammad Hossein Yassaee*

Main category: cs.LG

TL;DR: TRACE框架将模型替换后的风险变化分解为四个可解释因素，提供诊断工具和部署决策评分


<details>
  <summary>Details</summary>
Motivation: 当源域训练的模型被新数据训练的模型替换时，其在源域上的性能变化难以预测，需要理解风险变化的原因

Method: 提出TRACE框架，将风险变化分解为四个因素：两个泛化差距、模型变化惩罚和协变量偏移惩罚，使用特征空间最优传输或MMD估计偏移惩罚，输出距离估计模型变化惩罚

Result: 在线性回归和视觉基准测试中验证了TRACE边界的有效性，推导出与风险变化强相关的部署门控评分，实现高AUROC/AUPRC的模型替换决策

Conclusion: TRACE提供了解释性强的风险变化诊断工具，能够支持安全、标签高效的模型替换决策

Abstract: When a source-trained model $Q$ is replaced by a model $\tilde{Q}$ trained on shifted data, its performance on the source domain can change unpredictably. To address this, we study the two-model risk change, $ΔR := R_P(\tilde{Q}) - R_P(Q)$, under covariate shift. We introduce TRACE (Theoretical Risk Attribution under Covariate-shift Effects), a framework that decomposes $|ΔR|$ into an interpretable upper bound. This decomposition disentangles the risk change into four actionable factors: two generalization gaps, a model change penalty, and a covariate shift penalty, transforming the bound into a powerful diagnostic tool for understanding why performance has changed. To make TRACE a fully computable diagnostic, we instantiate each term. The covariate shift penalty is estimated via a model sensitivity factor (from high-quantile input gradients) and a data-shift measure; we use feature-space Optimal Transport (OT) by default and provide a robust alternative using Maximum Mean Discrepancy (MMD). The model change penalty is controlled by the average output distance between the two models on the target sample. Generalization gaps are estimated on held-out data. We validate our framework in an idealized linear regression setting, showing the TRACE bound correctly captures the scaling of the true risk difference with the magnitude of the shift. Across synthetic and vision benchmarks, TRACE diagnostics are valid and maintain a strong monotonic relationship with the true performance degradation. Crucially, we derive a deployment gate score that correlates strongly with $|ΔR|$ and achieves high AUROC/AUPRC for gating decisions, enabling safe, label-efficient model replacement.

</details>


### [76] [Roughness-Informed Federated Learning](https://arxiv.org/abs/2602.10595)
*Mohammad Partohaghighi,Roummel Marcia,Bruce J. West,YangQuan Chen*

Main category: cs.LG

TL;DR: RI-FedAvg：一种新的联邦学习算法，通过引入粗糙度指数正则化项来缓解客户端漂移问题，在非IID场景下实现更优的性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非独立同分布（non-IID）设置中面临客户端漂移问题，这会损害收敛性能。现有方法在处理异构数据时效果有限，需要更有效的机制来缓解客户端漂移。

Method: 提出RI-FedAvg算法，在本地目标函数中引入基于粗糙度指数（RI）的正则化项。RI用于量化高维损失函数的粗糙度，根据本地损失景观的波动自适应地惩罚更新，从而减少客户端漂移。

Result: 在MNIST、CIFAR-10和CIFAR-100数据集上的实验表明，RI-FedAvg在非IID场景下优于FedAvg、FedProx、FedDyn、SCAFFOLD和DP-FedAvg等基线方法，实现了更高的准确率和更快的收敛速度。收敛分析证明算法在非凸目标下能收敛到平稳点。

Conclusion: RI-FedAvg通过粗糙度指数正则化有效缓解了客户端漂移问题，增强了联邦学习在异构环境下的鲁棒性和效率，为实际应用提供了有前景的解决方案。

Abstract: Federated Learning (FL) enables collaborative model training across distributed clients while preserving data privacy, yet faces challenges in non-independent and identically distributed (non-IID) settings due to client drift, which impairs convergence. We propose RI-FedAvg, a novel FL algorithm that mitigates client drift by incorporating a Roughness Index (RI)-based regularization term into the local objective, adaptively penalizing updates based on the fluctuations of local loss landscapes. This paper introduces RI-FedAvg, leveraging the RI to quantify the roughness of high-dimensional loss functions, ensuring robust optimization in heterogeneous settings. We provide a rigorous convergence analysis for non-convex objectives, establishing that RI-FedAvg converges to a stationary point under standard assumptions. Extensive experiments on MNIST, CIFAR-10, and CIFAR-100 demonstrate that RI-FedAvg outperforms state-of-the-art baselines, including FedAvg, FedProx, FedDyn, SCAFFOLD, and DP-FedAvg, achieving higher accuracy and faster convergence in non-IID scenarios. Our results highlight RI-FedAvg's potential to enhance the robustness and efficiency of federated learning in practical, heterogeneous environments.

</details>


### [77] [Learning Mixture Density via Natural Gradient Expectation Maximization](https://arxiv.org/abs/2602.10602)
*Yutao Chen,Jasmine Bayrooti,Steven Morad*

Main category: cs.LG

TL;DR: 提出nGEM目标函数，通过信息几何改进混合密度网络的优化，实现10倍加速收敛且计算开销几乎为零


<details>
  <summary>Details</summary>
Motivation: 混合密度网络使用负对数似然目标进行最大似然估计时存在收敛慢和模式坍塌问题，需要改进优化方法

Method: 将混合密度网络解释为深度隐变量模型，通过期望最大化框架分析，建立与自然梯度下降的理论联系，推导出自然梯度期望最大化（nGEM）目标函数

Result: nGEM实现高达10倍的收敛加速，计算开销几乎为零，在高维数据上表现良好，而NLL方法在高维数据上会失败

Conclusion: 通过信息几何整合改进混合密度网络优化是有效的，nGEM在收敛速度和可扩展性方面显著优于传统NLL方法

Abstract: Mixture density networks are neural networks that produce Gaussian mixtures to represent continuous multimodal conditional densities. Standard training procedures involve maximum likelihood estimation using the negative log-likelihood (NLL) objective, which suffers from slow convergence and mode collapse. In this work, we improve the optimization of mixture density networks by integrating their information geometry. Specifically, we interpret mixture density networks as deep latent-variable models and analyze them through an expectation maximization framework, which reveals surprising theoretical connections to natural gradient descent. We then exploit such connections to derive the natural gradient expectation maximization (nGEM) objective. We show that empirically nGEM achieves up to 10$\times$ faster convergence while adding almost zerocomputational overhead, and scales well to high-dimensional data where NLL otherwise fails.

</details>


### [78] [dnaHNet: A Scalable and Hierarchical Foundation Model for Genomic Sequence Learning](https://arxiv.org/abs/2602.10603)
*Arnav Shah,Junzhe Li,Parsa Idehpour,Adibvafa Fallahpour,Brandon Wang,Sukjun Hwang,Bo Wang,Patrick D. Hsu,Hani Goodarzi,Albert Gu*

Main category: cs.LG

TL;DR: dnaHNet是一种无需分词器的自回归基因组模型，通过可微分动态分块机制自适应压缩原始核苷酸，在保持生物学连贯性的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有基因组基础模型面临输入表示的根本权衡：固定词汇分词器会破坏生物学有意义的基序（如密码子和调控元件），而核苷酸级模型虽然保持生物学连贯性，但对长上下文计算成本过高。

Method: 提出dnaHNet，一种无需分词器的自回归模型，使用可微分动态分块机制将原始核苷酸自适应压缩为潜在标记，平衡压缩与预测准确性。通过递归分块实现二次FLOP减少。

Result: 在原核基因组上预训练后，dnaHNet在缩放和效率方面优于StripedHyena2等领先架构，推理速度比Transformer快3倍以上。在零样本任务中，在预测蛋白质变异适应性和基因必需性方面表现优异，并能无监督自动发现分层生物结构。

Conclusion: dnaHNet建立了一个可扩展、可解释的下一代基因组建模框架，解决了基因组基础模型在输入表示上的根本权衡问题。

Abstract: Genomic foundation models have the potential to decode DNA syntax, yet face a fundamental tradeoff in their input representation. Standard fixed-vocabulary tokenizers fragment biologically meaningful motifs such as codons and regulatory elements, while nucleotide-level models preserve biological coherence but incur prohibitive computational costs for long contexts. We introduce dnaHNet, a state-of-the-art tokenizer-free autoregressive model that segments and models genomic sequences end-to-end. Using a differentiable dynamic chunking mechanism, dnaHNet compresses raw nucleotides into latent tokens adaptively, balancing compression with predictive accuracy. Pretrained on prokaryotic genomes, dnaHNet outperforms leading architectures including StripedHyena2 in scaling and efficiency. This recursive chunking yields quadratic FLOP reductions, enabling $>3 \times$ inference speedup over Transformers. On zero-shot tasks, dnaHNet achieves superior performance in predicting protein variant fitness and gene essentiality, while automatically discovering hierarchical biological structures without supervision. These results establish dnaHNet as a scalable, interpretable framework for next-generation genomic modeling.

</details>


### [79] [Hierarchical Zero-Order Optimization for Deep Neural Networks](https://arxiv.org/abs/2602.10607)
*Sansheng Cao,Zhengyu Ma,Yonghong Tian*

Main category: cs.LG

TL;DR: 提出分层零阶优化(HZO)，将深度维度分解，将查询复杂度从O(ML²)降低到O(ML log L)，在CIFAR-10和ImageNet上达到与反向传播相当的精度。


<details>
  <summary>Details</summary>
Motivation: 零阶优化具有生物合理性和处理不可微目标的优势，但计算复杂度限制了其在深度神经网络中的应用。传统梯度逐层传播范式存在效率问题，需要新的优化方法。

Method: 提出分层零阶优化(HZO)，采用分治策略分解网络深度维度，在接近单位极限(L_lip≈1)下操作以保持数值稳定性。

Result: 理论证明HZO将查询复杂度从O(ML²)降低到O(ML log L)，在CIFAR-10和ImageNet上实验显示达到与反向传播相当的准确率。

Conclusion: HZO显著改进了零阶优化的计算效率，为生物合理优化方法在深度网络中的应用开辟了新途径，在保持数值稳定性的同时实现了与反向传播竞争的性能。

Abstract: Zeroth-order (ZO) optimization has long been favored for its biological plausibility and its capacity to handle non-differentiable objectives, yet its computational complexity has historically limited its application in deep neural networks. Challenging the conventional paradigm that gradients propagate layer-by-layer, we propose Hierarchical Zeroth-Order (HZO) optimization, a novel divide-and-conquer strategy that decomposes the depth dimension of the network. We prove that HZO reduces the query complexity from $O(ML^2)$ to $O(ML \log L)$ for a network of width $M$ and depth $L$, representing a significant leap over existing ZO methodologies. Furthermore, we provide a detailed error analysis showing that HZO maintains numerical stability by operating near the unitary limit ($L_{lip} \approx 1$). Extensive evaluations on CIFAR-10 and ImageNet demonstrate that HZO achieves competitive accuracy compared to backpropagation.

</details>


### [80] [Pupillometry and Brain Dynamics for Cognitive Load in Working Memory](https://arxiv.org/abs/2602.10614)
*Nusaibah Farrukh,Malavika Pradeep,Akshay Sasi,Rahul Venugopal,Elizabeth Sherly*

Main category: cs.LG

TL;DR: 该研究比较了EEG和瞳孔测量在认知负荷分类中的表现，发现基于特征的方法（Catch-22特征+经典机器学习）优于深度学习，且瞳孔测量单独使用即可与EEG竞争，为可穿戴认知监测系统提供了实用方案。


<details>
  <summary>Details</summary>
Motivation: 认知负荷评估对自适应学习、临床监测和脑机接口至关重要。虽然EEG和瞳孔测量都是认知负荷的生物标志物，但它们的比较效用以及作为轻量级可穿戴监测解决方案的实际整合仍未被充分探索。现有研究多依赖解释性有限且计算成本高的深度学习模型。

Method: 使用OpenNeuro的"数字广度任务"数据集，结合特征驱动和模型驱动方法进行时间序列分析。采用Catch-22特征提取和经典机器学习模型，与深度学习方法进行比较，同时使用SHAP进行特征分析以增强可解释性。

Result: 基于特征的方法（Catch-22特征+经典机器学习）在二元和多类分类任务中都优于深度学习。瞳孔测量单独使用即可与EEG的认知负荷分类性能竞争，且通过SHAP分析提供了生理学上有意义的洞察。

Conclusion: 研究挑战了"EEG是负荷检测必需"的假设，表明瞳孔动态结合可解释模型可以提供实用的认知监测方案。这支持了开发可穿戴、经济实惠的认知监测系统，适用于神经精神病学、教育和医疗保健领域。

Abstract: Cognitive load, the mental effort required during working memory, is central to neuroscience, psychology, and human-computer interaction. Accurate assessment is vital for adaptive learning, clinical monitoring, and brain-computer interfaces. Physiological signals such as pupillometry and electroencephalography are established biomarkers of cognitive load, but their comparative utility and practical integration as lightweight, wearable monitoring solutions remain underexplored. EEG provides high temporal resolution of neural activity. Although non-invasive, it is technologically demanding and limited in wearability and cost due to its resource-intensive nature, whereas pupillometry is non-invasive, portable, and scalable. Existing studies often rely on deep learning models with limited interpretability and substantial computational expense. This study integrates feature-based and model-driven approaches to advance time-series analysis. Using the OpenNeuro 'Digit Span Task' dataset, this study investigates cognitive load classification from EEG and pupillometry. Feature-based approaches using Catch-22 features and classical machine learning models outperform deep learning in both binary and multiclass tasks. The findings demonstrate that pupillometry alone can compete with EEG, serving as a portable and practical proxy for real-world applications. These results challenge the assumption that EEG is necessary for load detection, showing that pupil dynamics combined with interpretable models and SHAP based feature analysis provide physiologically meaningful insights. This work supports the development of wearable, affordable cognitive monitoring systems for neuropsychiatry, education, and healthcare.

</details>


### [81] [Mitigating Reward Hacking in RLHF via Bayesian Non-negative Reward Modeling](https://arxiv.org/abs/2602.10623)
*Zhibin Duan,Guowei Rong,Zhuo Li,Bo Chen,Mingyuan Zhou,Dandan Guo*

Main category: cs.LG

TL;DR: BNRM：一种贝叶斯非负奖励模型框架，通过非负因子分析和稀疏潜在因子表示来解决奖励模型中的奖励黑客问题，提高鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 基于人类偏好的奖励模型在RLHF中对齐大语言模型至关重要，但容易受到噪声标注和系统性偏差（如响应长度或风格）的影响，导致奖励黑客问题。

Method: 提出贝叶斯非负奖励模型（BNRM），将非负因子分析整合到Bradley-Terry偏好模型中。使用稀疏非负潜在因子生成过程，包含实例特定潜在变量和全局潜在因子稀疏化，实现解耦和去偏。开发基于深度模型表示的摊销变分推理网络，支持端到端高效训练。

Result: BNRM显著减轻了奖励过度优化问题，提高了分布偏移下的鲁棒性，并产生比强基线更可解释的奖励分解结果。

Conclusion: BNRM通过解耦-去偏结构和不确定性感知的奖励学习，为奖励建模提供了更鲁棒、可解释的框架，有效缓解了奖励黑客问题。

Abstract: Reward models learned from human preferences are central to aligning large language models (LLMs) via reinforcement learning from human feedback, yet they are often vulnerable to reward hacking due to noisy annotations and systematic biases such as response length or style. We propose Bayesian Non-Negative Reward Model (BNRM), a principled reward modeling framework that integrates non-negative factor analysis into Bradley-Terry (BT) preference model. BNRM represents rewards through a sparse, non-negative latent factor generative process that operates at two complementary levels: instance-specific latent variables induce disentangled reward representations, while sparsity over global latent factors acts as an implicit debiasing mechanism that suppresses spurious correlations. Together, this disentanglement-then-debiasing structure enables robust uncertainty-aware reward learning. To scale BNRM to modern LLMs, we develop an amortized variational inference network conditioned on deep model representations, allowing efficient end-to-end training. Extensive empirical results demonstrate that BNRM substantially mitigates reward over-optimization, improves robustness under distribution shifts, and yields more interpretable reward decompositions than strong baselines.

</details>


### [82] [Generative clinical time series models trained on moderate amounts of patient data are privacy preserving](https://arxiv.org/abs/2602.10631)
*Rustam Zhumagambetov,Niklas Giesa,Sebastian D. Boie,Stefan Haufe*

Main category: cs.LG

TL;DR: 针对医疗时间序列生成模型的隐私审计研究，发现现有隐私攻击对大型训练数据集训练的生成模型无效，且差分隐私机制只会降低数据效用而不会显著提升隐私保护


<details>
  <summary>Details</summary>
Motivation: 医疗数据共享面临隐私泄露风险，生成式AI模型虽能产生合成数据，但现有隐私保护机制（如差分隐私）在时间序列模型中存在训练不稳定、数据效用降低等问题，需要有效的隐私审计方法

Method: 使用一系列已建立的隐私攻击方法对基于MIMIC-IV数据集训练的最先进医院时间序列生成模型进行隐私审计，并使用eICU数据集对MIMIC-IV训练的合成数据生成器发起隐私攻击

Result: 当合成数据生成器在足够大的训练数据集上训练时，现有的隐私攻击对生成的多元临床时间序列无效；差分隐私机制不会带来期望的隐私改进，只会降低机器学习预测任务的效用

Conclusion: 对于大型训练数据集训练的医疗时间序列生成模型，现有隐私攻击效果有限，差分隐私机制在隐私-效用权衡中可能不划算，需要更有效的隐私审计框架

Abstract: Sharing medical data for machine learning model training purposes is often impossible due to the risk of disclosing identifying information about individual patients. Synthetic data produced by generative artificial intelligence (genAI) models trained on real data is often seen as one possible solution to comply with privacy regulations. While powerful genAI models for heterogeneous hospital time series have recently been introduced, such modeling does not guarantee privacy protection, as the generated data may still reveal identifying information about individuals in the models' training cohort. Applying established privacy mechanisms to generative time series models, however, proves challenging as post-hoc data anonymization through k-anonymization or similar techniques is limited, while model-centered privacy mechanisms that implement differential privacy (DP) may lead to unstable training, compromising the utility of generated data. Given these known limitations, privacy audits for generative time series models are currently indispensable regardless of the concrete privacy mechanisms applied to models and/or data. In this work, we use a battery of established privacy attacks to audit state-of-the-art hospital time series models, trained on the public MIMIC-IV dataset, with respect to privacy preservation. Furthermore, the eICU dataset was used to mount a privacy attack against the synthetic data generator trained on the MIMIC-IV dataset. Results show that established privacy attacks are ineffective against generated multivariate clinical time series when synthetic data generators are trained on large enough training datasets. Furthermore, we discuss how the use of existing DP mechanisms for these synthetic data generators would not bring desired improvement in privacy, but only a decrease in utility for machine learning prediction tasks.

</details>


### [83] [Coarse-Grained Boltzmann Generators](https://arxiv.org/abs/2602.10637)
*Weilong Chen,Bojun Zhao,Jan Eckwert,Julija Zavadlav*

Main category: cs.LG

TL;DR: 提出Coarse-Grained Boltzmann Generators (CG-BGs)，结合粗粒化建模与重要性采样，实现可扩展的分子构象采样


<details>
  <summary>Details</summary>
Motivation: 传统Boltzmann Generators (BGs) 虽然能通过重要性采样获得精确统计，但实际可扩展性有限；而粗粒化替代模型虽然能建模更大系统，但缺乏重加权过程来保证渐近正确的统计特性

Method: 提出CG-BGs框架：在粗粒化坐标空间中操作，使用学习得到的平均力势(PMF)对基于流模型生成的样本进行重加权，通过力匹配方法从快速收敛的数据中高效学习PMF

Result: CG-BGs能够在高度简化的表示中准确捕捉由显式溶剂介导的复杂相互作用，为更大分子系统的无偏采样建立了可扩展途径

Conclusion: CG-BGs将可扩展的降阶建模与重要性采样的精确性统一起来，为解决大分子系统Boltzmann分布采样问题提供了原理性框架

Abstract: Sampling equilibrium molecular configurations from the Boltzmann distribution is a longstanding challenge. Boltzmann Generators (BGs) address this by combining exact-likelihood generative models with importance sampling, but their practical scalability is limited. Meanwhile, coarse-grained surrogates enable the modeling of larger systems by reducing effective dimensionality, yet often lack the reweighting process required to ensure asymptotically correct statistics. In this work, we propose Coarse-Grained Boltzmann Generators (CG-BGs), a principled framework that unifies scalable reduced-order modeling with the exactness of importance sampling. CG-BGs act in a coarse-grained coordinate space, using a learned potential of mean force (PMF) to reweight samples generated by a flow-based model. Crucially, we show that this PMF can be efficiently learned from rapidly converged data via force matching. Our results demonstrate that CG-BGs faithfully capture complex interactions mediated by explicit solvent within highly reduced representations, establishing a scalable pathway for the unbiased sampling of larger molecular systems.

</details>


### [84] [Evaluation metrics for temporal preservation in synthetic longitudinal patient data](https://arxiv.org/abs/2602.10643)
*Katariina Perkonoja,Parisa Movahedi,Antti Airola,Kari Auranen,Joni Virta*

Main category: cs.LG

TL;DR: 提出评估合成纵向患者数据时间保持性的指标集，涵盖边际、协方差、个体层面和测量结构，强调多维评估的重要性


<details>
  <summary>Details</summary>
Motivation: 现有合成数据评估方法可能过度关注边际层面的相似性，而忽略时间依赖性和个体轨迹的保持，需要更全面的时间结构评估指标

Method: 提出一套评估合成纵向患者数据时间保持性的指标，分为四类：边际结构、协方差结构、个体层面结构和测量结构，分析数据质量、测量频率、预处理策略等因素的影响

Result: 边际层面的强相似性可能掩盖协方差扭曲和个体轨迹破坏；稀疏或不规则测量时间的变量难以学习时间依赖性；单一指标不足，需要多维评估

Conclusion: 提出的指标能够阐明时间结构如何被保持或退化，支持更可靠的生成模型评估和改进，促进创建时间上真实的合成纵向患者数据

Abstract: This study introduces a set of metrics for evaluating temporal preservation in synthetic longitudinal patient data, defined as artificially generated data that mimic real patients' repeated measurements over time. The proposed metrics assess how synthetic data reproduces key temporal characteristics, categorized into marginal, covariance, individual-level and measurement structures. We show that strong marginal-level resemblance may conceal distortions in covariance and disruptions in individual-level trajectories. Temporal preservation is influenced by factors such as original data quality, measurement frequency, and preprocessing strategies, including binning, variable encoding and precision. Variables with sparse or highly irregular measurement times provide limited information for learning temporal dependencies, resulting in reduced resemblance between the synthetic and original data. No single metric adequately captures temporal preservation; instead, a multidimensional evaluation across all characteristics provides a more comprehensive assessment of synthetic data quality. Overall, the proposed metrics clarify how and why temporal structures are preserved or degraded, enabling more reliable evaluation and improvement of generative models and supporting the creation of temporally realistic synthetic longitudinal patient data.

</details>


### [85] [Domain Knowledge Guided Bayesian Optimization For Autonomous Alignment Of Complex Scientific Instruments](https://arxiv.org/abs/2602.10670)
*Aashwin Mishra,Matt Seaberg,Ryan Roussel,Daniel Ratner,Apurva Mehta*

Main category: cs.LG

TL;DR: 提出一种基于领域知识引导的贝叶斯优化方法，通过坐标变换解耦高维参数并对齐活跃子空间，解决了传统BO在稀疏奖励的高维耦合问题中的性能退化问题。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化（包括TurBO和多目标BO）在高维、参数紧密耦合且目标函数高度不对称的稀疏奖励场景中性能显著下降，难以找到全局最优解，特别是在复杂的科学仪器优化中。

Method: 利用物理洞察力进行坐标变换，解耦输入特征并将活跃子空间与主要搜索轴对齐，结合反向退火探索策略，将高维耦合优化问题转化为更简单的表示形式。

Result: 在具有挑战性的12维6晶体分束延迟光学系统上，该方法可靠地收敛到全局最优解，而传统方法（标准BO、TuRBO、多目标BO）均表现不佳。坐标变换显著加速了搜索过程。

Conclusion: 通过物理洞察力将高维耦合优化问题转化为更简单的表示形式，是一种可推广的范式，能够在保持现有优化算法的同时，实现快速、稳健的自动调优，确保高性能。

Abstract: Bayesian Optimization (BO) is a powerful tool for optimizing complex non-linear systems. However, its performance degrades in high-dimensional problems with tightly coupled parameters and highly asymmetric objective landscapes, where rewards are sparse. In such needle-in-a-haystack scenarios, even advanced methods like trust-region BO (TurBO) often lead to unsatisfactory results. We propose a domain knowledge guided Bayesian Optimization approach, which leverages physical insight to fundamentally simplify the search problem by transforming coordinates to decouple input features and align the active subspaces with the primary search axes. We demonstrate this approach's efficacy on a challenging 12-dimensional, 6-crystal Split-and-Delay optical system, where conventional approaches, including standard BO, TuRBO and multi-objective BO, consistently led to unsatisfactory results. When combined with an reverse annealing exploration strategy, this approach reliably converges to the global optimum. The coordinate transformation itself is the key to this success, significantly accelerating the search by aligning input co-ordinate axes with the problem's active subspaces. As increasingly complex scientific instruments, from large telescopes to new spectrometers at X-ray Free Electron Lasers are deployed, the demand for robust high-dimensional optimization grows. Our results demonstrate a generalizable paradigm: leveraging physical insight to transform high-dimensional, coupled optimization problems into simpler representations can enable rapid and robust automated tuning for consistent high performance while still retaining current optimization algorithms.

</details>


### [86] [VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training](https://arxiv.org/abs/2602.10693)
*Guobin Shen,Chenxiao Zhao,Xiang Cheng,Lei Huang,Xing Yu*

Main category: cs.LG

TL;DR: VESPO提出了一种变分序列级软策略优化方法，通过方差减少和变分公式推导出闭式重塑核，直接在序列级重要性权重上操作，无需长度归一化，解决了LLM强化学习中训练稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型强化学习中训练稳定性是一个核心挑战。策略陈旧、异步训练以及训练与推理引擎不匹配都会导致行为策略与当前策略偏离，可能导致训练崩溃。重要性采样虽然提供了分布偏移的原则性校正，但存在高方差问题；现有的标记级裁剪和序列级归一化方法缺乏统一的理论基础。

Method: 提出VESPO（Variational sEquence-level Soft Policy Optimization）方法，将方差减少纳入提案分布的变分公式中，推导出一个闭式重塑核，直接在序列级重要性权重上操作，无需长度归一化。

Result: 在数学推理基准测试中，VESPO在陈旧比高达64倍和完全异步执行的情况下仍能保持训练稳定性，并在密集模型和专家混合模型上都取得了持续的性能提升。

Conclusion: VESPO通过变分序列级软策略优化，为LLM强化学习中的训练稳定性问题提供了一个理论统一且实用的解决方案，能够有效处理策略陈旧和异步训练等挑战。

Abstract: Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO

</details>


### [87] [Reducing Estimation Uncertainty Using Normalizing Flows and Stratification](https://arxiv.org/abs/2602.10706)
*Paweł Lorek,Rafał Topolnicki,Tomasz Trzciński,Maciej Zięba,Aleksandra Krystecka*

Main category: cs.LG

TL;DR: 提出结合流模型与分层抽样的方法，用于估计随机变量函数的期望，相比传统参数化方法显著降低估计不确定性


<details>
  <summary>Details</summary>
Motivation: 传统方法通常假设(半)参数分布(如高斯或混合高斯)，当假设不成立时会导致显著的估计不确定性。需要更灵活的方法来建模未知数据分布

Method: 提出基于流模型的方法，结合分层抽样，使用参数化神经网络提供更大的灵活性来建模未知数据分布

Result: 在多个数据集(包括30维和128维的高维数据)上显示出估计不确定性显著降低，优于原始蒙特卡洛估计器和高斯混合模型

Conclusion: 流模型与分层抽样结合的方法能够更灵活地建模未知分布，有效降低期望估计的不确定性，特别是在传统参数化假设不成立时

Abstract: Estimating the expectation of a real-valued function of a random variable from sample data is a critical aspect of statistical analysis, with far-reaching implications in various applications. Current methodologies typically assume (semi-)parametric distributions such as Gaussian or mixed Gaussian, leading to significant estimation uncertainty if these assumptions do not hold. We propose a flow-based model, integrated with stratified sampling, that leverages a parametrized neural network to offer greater flexibility in modeling unknown data distributions, thereby mitigating this limitation. Our model shows a marked reduction in estimation uncertainty across multiple datasets, including high-dimensional (30 and 128) ones, outperforming crude Monte Carlo estimators and Gaussian mixture models. Reproducible code is available at https://github.com/rnoxy/flowstrat.

</details>


### [88] [Interpretable Graph-Level Anomaly Detection via Contrast with Normal Prototypes](https://arxiv.org/abs/2602.10708)
*Qiuran Zhao,Kai Ming Ting,Xinpeng Li*

Main category: cs.LG

TL;DR: ProtoGLAD是一个基于原型的图级异常检测框架，通过对比异常图与其最近的正态原型图提供可解释的异常检测结果。


<details>
  <summary>Details</summary>
Motivation: 现有深度图级异常检测方法存在黑盒问题，缺乏可解释性。虽然有些方法尝试提供解释，但要么没有参考正态图，要么依赖抽象的潜在向量而非具体的数据集图作为原型。

Method: 提出ProtoGLAD框架：1) 使用点集核迭代发现多个正态原型图及其相关聚类；2) 将远离所有正态聚类的图识别为异常；3) 通过显式对比异常图与其最近的正态原型图提供解释。

Result: 在多个真实数据集上的实验表明，ProtoGLAD在异常检测性能上与最先进的GLAD方法相当，同时提供更好的人类可理解的原型解释。

Conclusion: ProtoGLAD解决了现有图级异常检测方法的可解释性限制，通过基于具体原型图的对比解释，提高了方法的可靠性和实际部署价值。

Abstract: The task of graph-level anomaly detection (GLAD) is to identify anomalous graphs that deviate significantly from the majority of graphs in a dataset. While deep GLAD methods have shown promising performance, their black-box nature limits their reliability and deployment in real-world applications. Although some recent methods have made attempts to provide explanations for anomaly detection results, they either provide explanations without referencing normal graphs, or rely on abstract latent vectors as prototypes rather than concrete graphs from the dataset. To address these limitations, we propose Prototype-based Graph-Level Anomaly Detection (ProtoGLAD), an interpretable unsupervised framework that provides explanation for each detected anomaly by explicitly contrasting with its nearest normal prototype graph. It employs a point-set kernel to iteratively discover multiple normal prototype graphs and their associated clusters from the dataset, then identifying graphs distant from all discovered normal clusters as anomalies. Extensive experiments on multiple real-world datasets demonstrate that ProtoGLAD achieves competitive anomaly detection performance compared to state-of-the-art GLAD methods while providing better human-interpretable prototype-based explanations.

</details>


### [89] [SnapMLA: Efficient Long-Context MLA Decoding via Hardware-Aware FP8 Quantized Pipelining](https://arxiv.org/abs/2602.10718)
*Yifan Zhang,Zunhai Su,Shuhao Hu,Rui Yang,Wei Wu,Yulei Qian,Yuchen Xie,Xunliang Cai*

Main category: cs.LG

TL;DR: SnapMLA是一个针对DeepSeek MLA架构的FP8解码框架，通过硬件感知的算法-内核协同优化技术，解决了FP8注意力在解码阶段的挑战，实现了长上下文效率的提升。


<details>
  <summary>Details</summary>
Motivation: FP8注意力在MLA架构解码阶段面临三个主要挑战：位置嵌入解耦导致的数值异质性、FP8 PV GEMM中的量化尺度错位，以及缺乏优化的系统级支持。这些限制了FP8在长上下文解码中的效率。

Method: 采用三种硬件感知的算法-内核协同优化技术：1) RoPE感知的每令牌KV量化，将RoPE部分保持高精度；2) 量化PV计算管道重构，解决MLA KV缓存共享结构导致的量化尺度错位；3) 端到端数据流优化，使用专用内核建立高效数据读写工作流。

Result: 在先进的MLA LLM上的实验表明，SnapMLA在吞吐量上实现了高达1.91倍的提升，在具有挑战性的长上下文任务（包括数学推理和代码生成基准测试）中，性能下降风险可忽略不计。

Conclusion: SnapMLA成功解决了FP8在MLA解码中的关键挑战，通过硬件感知的协同优化实现了显著的效率提升，为长上下文处理提供了有效的FP8解码框架。

Abstract: While FP8 attention has shown substantial promise in innovations like FlashAttention-3, its integration into the decoding phase of the DeepSeek Multi-head Latent Attention (MLA) architecture presents notable challenges. These challenges include numerical heterogeneity arising from the decoupling of positional embeddings, misalignment of quantization scales in FP8 PV GEMM, and the need for optimized system-level support. In this paper, we introduce SnapMLA, an FP8 MLA decoding framework optimized to improve long-context efficiency through the following hardware-aware algorithm-kernel co-optimization techniques: (i) RoPE-Aware Per-Token KV Quantization, where the RoPE part is maintained in high precision, motivated by our comprehensive analysis of the heterogeneous quantization sensitivity inherent to the MLA KV cache. Furthermore, per-token granularity is employed to align with the autoregressive decoding process and maintain quantization accuracy. (ii) Quantized PV Computation Pipeline Reconstruction, which resolves the misalignment of quantization scale in FP8 PV computation stemming from the shared KV structure of the MLA KV cache. (iii) End-to-End Dataflow Optimization, where we establish an efficient data read-and-write workflow using specialized kernels, ensuring efficient data flow and performance gains. Extensive experiments on state-of-the-art MLA LLMs show that SnapMLA achieves up to a 1.91x improvement in throughput, with negligible risk of performance degradation in challenging long-context tasks, including mathematical reasoning and code generation benchmarks. Code is available at https://github.com/meituan-longcat/SGLang-FluentLLM.

</details>


### [90] [Rising Multi-Armed Bandits with Known Horizons](https://arxiv.org/abs/2602.10727)
*Seockbean Song,Chenyu Gan,Youngsik Yoon,Siwei Wang,Wei Chen,Jungseul Ok*

Main category: cs.LG

TL;DR: 论文提出CURE-UCB算法，针对奖励随使用次数增加的RMAB问题，通过显式整合时间预算信息，显著超越无视时间预算的基线方法。


<details>
  <summary>Details</summary>
Motivation: RMAB框架模拟奖励随使用次数增加的环境（如机器人、超参数调优），其最优策略随预算T剧烈变化。现有研究对时间预算感知的设置探索不足，而T的知识在RMAB中能带来显著优势。

Method: 提出CUmulative Reward Estimation UCB (CURE-UCB)算法，显式整合时间预算信息，针对RMAB问题设计新的置信上界策略。

Result: 理论分析建立了新的遗憾上界，证明在"线性后平坦"等结构化环境中严格优于无视时间预算的策略。大量实验显示对基线方法的显著优势。

Conclusion: 时间预算感知在RMAB中至关重要，CURE-UCB算法通过显式整合预算信息，在理论和实验上都优于无视预算的方法，为RMAB问题提供了有效解决方案。

Abstract: The Rising Multi-Armed Bandit (RMAB) framework models environments where expected rewards of arms increase with plays, which models practical scenarios where performance of each option improves with the repeated usage, such as in robotics and hyperparameter tuning. For instance, in hyperparameter tuning, the validation accuracy of a model configuration (arm) typically increases with each training epoch. A defining characteristic of RMAB is em horizon-dependent optimality: unlike standard settings, the optimal strategy here shifts dramatically depending on the available budget $T$. This implies that knowledge of $T$ yields significantly greater utility in RMAB, empowering the learner to align its decision-making with this shifting optimality. However, the horizon-aware setting remains underexplored. To address this, we propose a novel CUmulative Reward Estimation UCB (CURE-UCB) that explicitly integrates the horizon. We provide a rigorous analysis establishing a new regret upper bound and prove that our method strictly outperforms horizon-agnostic strategies in structured environments like ``linear-then-flat'' instances. Extensive experiments demonstrate its significant superiority over baselines.

</details>


### [91] [Kalman Linear Attention: Parallel Bayesian Filtering For Efficient Language Modelling and State Tracking](https://arxiv.org/abs/2602.10743)
*Vaisakh Shaj,Cameron Barker,Aidan Scannell,Andras Szecsenyi,Elliot J. Crowley,Amos Storkey*

Main category: cs.LG

TL;DR: KLA（卡尔曼线性注意力）通过将卡尔曼滤波重新参数化为信息形式，实现并行训练，在保持计算效率的同时提供比现有状态空间模型更强的表达能力和不确定性跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有状态空间语言模型（如Mamba和GLA）虽然计算效率高，但在表达能力和复杂推理的状态跟踪方面存在不足。传统卡尔曼滤波虽有理论优势，但通常被视为顺序计算，无法并行训练。

Method: 将卡尔曼滤波重新参数化为信息形式，使其更新可以通过关联扫描计算，从而实现并行训练。基于此构建KLA层，这是一个神经序列建模原语，执行时间并行概率推理，同时保持显式的信念状态不确定性。

Result: KLA在语言建模任务中，在代表性的离散标记操作和状态跟踪基准测试上，匹配或超越了现代SSM和GLA模型。

Conclusion: 通过概率视角重新构建序列建模，KLA结合了卡尔曼滤波的理论优势（状态估计和不确定性跟踪）与并行训练的计算效率，为序列建模提供了更强大的原语。

Abstract: State-space language models such as Mamba and gated linear attention (GLA) offer efficient alternatives to transformers due to their linear complexity and parallel training, but often lack the expressivity and robust state-tracking needed for complex reasoning. We address these limitations by reframing sequence modelling through a probabilistic lens, using Bayesian filters as a core primitive. While classical filters such as Kalman filters provide principled state estimation and uncertainty tracking, they are typically viewed as inherently sequential. We show that reparameterising the Kalman filter in information form enables its updates to be computed via an associative scan, allowing efficient parallel training. Building on this insight, we introduce the Kalman Linear Attention (KLA) layer, a neural sequence-modelling primitive that performs time-parallel probabilistic inference while maintaining explicit belief-state uncertainty. KLA offers strictly more expressive nonlinear updates and gating than GLA variants while retaining their computational advantages. On language modelling tasks, KLA matches or outperforms modern SSMs and GLAs across representative discrete token-manipulation and state-tracking benchmarks.

</details>


### [92] [Predicting integers from continuous parameters](https://arxiv.org/abs/2602.10751)
*Bas Maat,Peter Bloem*

Main category: cs.LG

TL;DR: 该论文研究如何直接使用离散分布来预测整数值标签，而不是将其视为连续值进行回归。研究发现Bitwise分布和离散拉普拉斯分布在多种任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 许多实际应用中的数值标签（如社交媒体点赞数、自行车租赁站可用车辆数）本质上是整数或整数子集。虽然可以将它们建模为连续值进行回归，但这会改变标签的离散分布特性。离散分布具有特定优势，因此需要研究能否直接使用离散分布来预测整数值标签，特别是对于神经网络输出分布，要求分布参数连续以便反向传播和梯度下降。

Method: 研究多种适合神经网络预测整数值标签的离散分布，包括现有方法和新颖方法。这些分布需要满足参数连续的条件，以便通过反向传播学习网络权重。具体测试了Bitwise分布（将目标整数表示为比特位，每个比特位使用伯努利分布）和离散拉普拉斯分布（在连续均值周围使用指数衰减尾部的分布）等多种选项。

Result: 在表格学习、序列预测和图像生成等多种任务上进行测试，发现总体上最佳性能来自两种分布：Bitwise分布和离散拉普拉斯分布。Bitwise分布通过比特位表示整数，离散拉普拉斯分布则在连续均值周围使用指数衰减尾部。

Conclusion: 对于整数值标签的预测，直接使用离散分布比传统的连续回归方法更有优势。Bitwise分布和离散拉普拉斯分布是两种有效的选择，特别适合神经网络架构，能够保持分布参数连续以便进行梯度优化。

Abstract: We study the problem of predicting numeric labels that are constrained to the integers or to a subrange of the integers. For example, the number of up-votes on social media posts, or the number of bicycles available at a public rental station. While it is possible to model these as continuous values, and to apply traditional regression, this approach changes the underlying distribution on the labels from discrete to continuous. Discrete distributions have certain benefits, which leads us to the question whether such integer labels can be modeled directly by a discrete distribution, whose parameters are predicted from the features of a given instance. Moreover, we focus on the use case of output distributions of neural networks, which adds the requirement that the parameters of the distribution be continuous so that backpropagation and gradient descent may be used to learn the weights of the network. We investigate several options for such distributions, some existing and some novel, and test them on a range of tasks, including tabular learning, sequential prediction and image generation. We find that overall the best performance comes from two distributions: Bitwise, which represents the target integer in bits and places a Bernoulli distribution on each, and a discrete analogue of the Laplace distribution, which uses a distribution with exponentially decaying tails around a continuous mean.

</details>


### [93] [Exploring the impact of adaptive rewiring in Graph Neural Networks](https://arxiv.org/abs/2602.10754)
*Charlotte Cambier van Nooten,Christos Aronis,Yuliya Shapovalova,Lucia Cavallaro*

Main category: cs.LG

TL;DR: 该论文探索了图神经网络中的稀疏化方法作为正则化手段，以解决大规模图应用中的高内存使用和计算成本问题，并在电力网格N-1应急评估中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 大规模图应用中图神经网络存在高内存使用和计算成本问题，需要提高效率和可扩展性，特别是在电力网格可靠性分析等关键应用中。

Method: 结合网络科学和机器学习技术，使用Erdős-Rényi模型进行稀疏化，探索图卷积网络和图同构网络在不同稀疏化程度和重连策略下的表现，提出自适应重连方法。

Result: 在三个不同规模的数据集上实验表明，适当的稀疏化可以改善泛化性能，但过度稀疏会阻碍复杂模式的学习。自适应重连结合早停策略表现出良好效果。

Conclusion: 稀疏化可以作为图神经网络的有效正则化手段，需要精细调整稀疏参数。结合网络科学和机器学习领域的见解可以提升图神经网络在电力网格可靠性分析等关键应用中的性能和可扩展性。

Abstract: This paper explores sparsification methods as a form of regularization in Graph Neural Networks (GNNs) to address high memory usage and computational costs in large-scale graph applications. Using techniques from Network Science and Machine Learning, including Erdős-Rényi for model sparsification, we enhance the efficiency of GNNs for real-world applications. We demonstrate our approach on N-1 contingency assessment in electrical grids, a critical task for ensuring grid reliability. We apply our methods to three datasets of varying sizes, exploring Graph Convolutional Networks (GCN) and Graph Isomorphism Networks (GIN) with different degrees of sparsification and rewiring. Comparison across sparsification levels shows the potential of combining insights from both research fields to improve GNN performance and scalability. Our experiments highlight the importance of tuning sparsity parameters: while sparsity can improve generalization, excessive sparsity may hinder learning of complex patterns. Our adaptive rewiring approach, particularly when combined with early stopping, proves promising by allowing the model to adapt its connectivity structure during training. This research contributes to understanding how sparsity can be effectively leveraged in GNNs for critical applications like power grid reliability analysis.

</details>


### [94] [Collaborative Threshold Watermarking](https://arxiv.org/abs/2602.10765)
*Tameem Bakr,Anish Ambreth,Nils Lukas*

Main category: cs.LG

TL;DR: 提出了一种(t,K)-阈值水印方案，用于联邦学习中多方协作嵌入水印，只有至少t个客户端组成的联盟才能验证模型所有权，解决了传统水印方案在客户端数量增多时水印稀释或单个客户端可移除水印的问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中多个客户端共同训练模型但不共享原始数据，需要机制来证明模型来源。传统水印方案存在两个问题：1) 随着客户端数量K增加，每个客户端的水印会被稀释；2) 单个客户端就能验证并可能移除水印。需要一种既能保护多方权益又能防止单个客户端滥用的水印方案。

Method: 提出(t,K)-阈值水印方案：客户端在训练过程中协作嵌入共享水印，但只有至少t个客户端组成的联盟才能重构水印密钥并验证可疑模型。通过秘密共享技术将水印密钥τ分发给客户端，少于t个客户端的联盟无法重构密钥。验证过程不需要明文暴露τ。在白盒设置下实现了该协议，并在图像分类任务上进行了评估。

Result: 水印在大规模场景下(K=128)仍可检测，准确率损失最小。在多种攻击下(包括使用高达20%训练数据的自适应微调)，水印检测分数保持在阈值以上(z≥4)。方案能有效抵抗攻击并保持水印的可检测性。

Conclusion: 提出的(t,K)-阈值水印方案解决了联邦学习中多方模型所有权验证的问题，既能保护多方权益，又能防止单个客户端滥用验证能力，在大规模场景下保持水印鲁棒性和模型性能。

Abstract: In federated learning (FL), $K$ clients jointly train a model without sharing raw data. Because each participant invests data and compute, clients need mechanisms to later prove the provenance of a jointly trained model. Model watermarking embeds a hidden signal in the weights, but naive approaches either do not scale with many clients as per-client watermarks dilute as $K$ grows, or give any individual client the ability to verify and potentially remove the watermark. We introduce $(t,K)$-threshold watermarking: clients collaboratively embed a shared watermark during training, while only coalitions of at least $t$ clients can reconstruct the watermark key and verify a suspect model. We secret-share the watermark key $τ$ so that coalitions of fewer than $t$ clients cannot reconstruct it, and verification can be performed without revealing $τ$ in the clear. We instantiate our protocol in the white-box setting and evaluate on image classification. Our watermark remains detectable at scale ($K=128$) with minimal accuracy loss and stays above the detection threshold ($z\ge 4$) under attacks including adaptive fine-tuning using up to 20% of the training data.

</details>


### [95] [LOREN: Low Rank-Based Code-Rate Adaptation in Neural Receivers](https://arxiv.org/abs/2602.10770)
*Bram Van Bolderik,Vlado Menkovski,Sonia Heemstra de Groot,Manil Dev Gomony*

Main category: cs.LG

TL;DR: LOREN是一种基于低秩适配的神经网络接收器，通过冻结共享基础网络并训练小型适配器来实现多码率支持，大幅降低硬件开销。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络接收器需要为每个码率存储独立的权重集，导致高内存和功耗需求，限制了实际应用。需要一种既能保持性能又能显著降低开销的码率自适应方案。

Method: 提出LOREN接收器，在卷积层中集成轻量级低秩适配器，冻结共享基础网络，仅针对每个码率训练小型适配器。采用基于3GPP CDL信道的端到端训练框架确保在真实无线环境中的鲁棒性。

Result: LOREN在性能上达到或优于完全重新训练的基础神经网络接收器。22nm工艺硬件实现显示，支持三个码率时可节省超过65%的硅面积和高达15%的功耗。

Conclusion: LOREN通过低秩适配技术实现了高效的码率自适应神经网络接收器，在保持性能的同时显著降低了硬件开销，提高了神经网络接收器的实用性。

Abstract: Neural network based receivers have recently demonstrated superior system-level performance compared to traditional receivers. However, their practicality is limited by high memory and power requirements, as separate weight sets must be stored for each code rate. To address this challenge, we propose LOREN, a Low Rank-Based Code-Rate Adaptation Neural Receiver that achieves adaptability with minimal overhead. LOREN integrates lightweight low rank adaptation adapters (LOREN adapters) into convolutional layers, freezing a shared base network while training only small adapters per code rate. An end-to-end training framework over 3GPP CDL channels ensures robustness across realistic wireless environments. LOREN achieves comparable or superior performance relative to fully retrained base neural receivers. The hardware implementation of LOREN in 22nm technology shows more than 65% savings in silicon area and up to 15% power reduction when supporting three code rates.

</details>


### [96] [Kill it with FIRE: On Leveraging Latent Space Directions for Runtime Backdoor Mitigation in Deep Neural Networks](https://arxiv.org/abs/2602.10780)
*Enrico Ahlers,Daniel Passon,Yannic Noller,Lars Grunske*

Main category: cs.LG

TL;DR: FIRE是一种在推理时修复后门攻击的方法，通过反向应用后门方向来中和触发器的影响


<details>
  <summary>Details</summary>
Motivation: 现有后门缓解方法要么无效要么低效，特别是当易受攻击的模型已经部署时，需要一种有效的推理时修复方案

Method: 假设触发器在模型内部表示中引起结构化、可重复的变化，将触发器视为层间潜在空间中的方向，通过反向应用这些方向来操纵潜在表示，中和触发器的影响

Result: FIRE具有低计算开销，在图像基准测试中优于当前的运行时缓解方法，适用于各种攻击、数据集和网络架构

Conclusion: FIRE提供了一种有效的推理时后门缓解方法，能够在不修改模型的情况下保护已部署系统

Abstract: Machine learning models are increasingly present in our everyday lives; as a result, they become targets of adversarial attackers seeking to manipulate the systems we interact with. A well-known vulnerability is a backdoor introduced into a neural network by poisoned training data or a malicious training process. Backdoors can be used to induce unwanted behavior by including a certain trigger in the input. Existing mitigations filter training data, modify the model, or perform expensive input modifications on samples. If a vulnerable model has already been deployed, however, those strategies are either ineffective or inefficient. To address this gap, we propose our inference-time backdoor mitigation approach called FIRE (Feature-space Inference-time REpair). We hypothesize that a trigger induces structured and repeatable changes in the model's internal representation. We view the trigger as directions in the latent spaces between layers that can be applied in reverse to correct the inference mechanism. Therefore, we turn the backdoored model against itself by manipulating its latent representations and moving a poisoned sample's features along the backdoor directions to neutralize the trigger. Our evaluation shows that FIRE has low computational overhead and outperforms current runtime mitigations on image benchmarks across various attacks, datasets, and network architectures.

</details>


### [97] [Semi-Supervised Cross-Domain Imitation Learning](https://arxiv.org/abs/2602.10793)
*Li-Min Chu,Kai-Siang Ma,Ming-Hong Chen,Ping-Chun Hsieh*

Main category: cs.LG

TL;DR: 提出半监督跨域模仿学习（SS-CDIL）框架，结合少量目标专家演示和未标记不完美轨迹，通过新颖的跨域损失函数和自适应权重实现稳定高效的政策学习。


<details>
  <summary>Details</summary>
Motivation: 跨域模仿学习能通过跨域转移专家知识加速政策学习，在专家数据收集成本高的应用中很有价值。现有方法要么是监督式（依赖代理任务和显式对齐），要么是无监督式（无配对数据对齐分布），但往往不稳定。

Method: 提出SS-CDIL设置及首个有理论依据的算法。使用离线数据，包括少量目标专家演示和一些未标记的不完美轨迹。为处理域差异，提出新颖的跨域损失函数学习域间状态-动作映射，并设计自适应权重函数平衡源域和目标域知识。

Result: 在MuJoCo和Robosuite上的实验显示，相比基线方法获得一致性能提升，证明该方法能以最小监督实现稳定且数据高效的政策学习。

Conclusion: 提出的SS-CDIL框架和算法在跨域模仿学习中实现了稳定高效的学习，仅需少量监督即可有效利用跨域知识，为实际应用提供了实用解决方案。

Abstract: Cross-domain imitation learning (CDIL) accelerates policy learning by transferring expert knowledge across domains, which is valuable in applications where the collection of expert data is costly. Existing methods are either supervised, relying on proxy tasks and explicit alignment, or unsupervised, aligning distributions without paired data, but often unstable. We introduce the Semi-Supervised CDIL (SS-CDIL) setting and propose the first algorithm for SS-CDIL with theoretical justification. Our method uses only offline data, including a small number of target expert demonstrations and some unlabeled imperfect trajectories. To handle domain discrepancy, we propose a novel cross-domain loss function for learning inter-domain state-action mappings and design an adaptive weight function to balance the source and target knowledge. Experiments on MuJoCo and Robosuite show consistent gains over the baselines, demonstrating that our approach achieves stable and data-efficient policy learning with minimal supervision. Our code is available at~ https://github.com/NYCU-RL-Bandits-Lab/CDIL.

</details>


### [98] [Transport, Don't Generate: Deterministic Geometric Flows for Combinatorial Optimization](https://arxiv.org/abs/2602.10794)
*Benjy Friedmann,Nadav Dym*

Main category: cs.LG

TL;DR: CycFlow：用确定性点传输替代扩散模型，通过流匹配将2D坐标连续传输到规范圆形排列，实现比扩散模型快3个数量级的求解速度


<details>
  <summary>Details</summary>
Motivation: 当前神经组合优化领域被扩散模型主导，这些模型将欧几里得TSP视为随机热图生成任务，存在二次复杂度瓶颈。需要更高效的求解方法。

Method: CycFlow框架使用实例条件化的向量场，通过流匹配将输入的2D坐标连续传输到规范圆形排列，然后通过角度排序从2N维表示中恢复最优路径。

Result: 相比最先进的扩散基线，求解速度提升高达3个数量级，同时保持竞争力的最优性差距。

Conclusion: CycFlow通过用确定性点传输替代迭代边去噪，实现了范式转变，显著加速了神经组合优化求解，为高效TSP求解提供了新方向。

Abstract: Recent advances in Neural Combinatorial Optimization (NCO) have been dominated by diffusion models that treat the Euclidean Traveling Salesman Problem (TSP) as a stochastic $N \times N$ heatmap generation task. In this paper, we propose CycFlow, a framework that replaces iterative edge denoising with deterministic point transport. CycFlow learns an instance-conditioned vector field that continuously transports input 2D coordinates to a canonical circular arrangement, where the optimal tour is recovered from this $2N$ dimensional representation via angular sorting. By leveraging data-dependent flow matching, we bypass the quadratic bottleneck of edge scoring in favor of linear coordinate dynamics. This paradigm shift accelerates solving speed by up to three orders of magnitude compared to state-of-the-art diffusion baselines, while maintaining competitive optimality gaps.

</details>


### [99] [PRISM: Parallel Residual Iterative Sequence Model](https://arxiv.org/abs/2602.10796)
*Jie Jiang,Ke Cheng,Xin Xu,Mengyang Pang,Tianhao Lu,Jiaheng Li,Yue Liu,Yuan Wang,Jun Zhang,Huan Yu,Zhouchen Lin*

Main category: cs.LG

TL;DR: PRISM提出了一种并行残差迭代序列模型，通过解耦写入与遗忘操作，将多步迭代精炼的结构模式蒸馏为可并行前馈算子，在保持Transformer表达能力的同时实现线性模型的效率。


<details>
  <summary>Details</summary>
Motivation: 生成序列建模面临Transformer表达能力与线性序列模型效率之间的根本矛盾。现有高效架构受限于浅层单步线性更新，而强大的迭代方法如测试时训练(TTT)由于状态依赖梯度破坏了硬件并行性。

Method: PRISM采用求解器启发的归纳偏置，使用写入-遗忘解耦策略将非线性隔离在注入算子内。通过两阶段代理架构：短卷积锚定初始残差，学习预测器直接从输入估计精炼更新，将迭代校正的结构模式蒸馏为可并行前馈算子。

Result: 理论上证明该公式实现了Rank-L累积，将更新流形扩展到单步Rank-1瓶颈之外。实证上达到与显式优化方法相当的性能，同时实现174倍更高的吞吐量。

Conclusion: PRISM解决了生成序列建模中表达能力与效率的矛盾，通过并行化迭代精炼过程，在保持强大性能的同时大幅提升计算效率。

Abstract: Generative sequence modeling faces a fundamental tension between the expressivity of Transformers and the efficiency of linear sequence models. Existing efficient architectures are theoretically bounded by shallow, single-step linear updates, while powerful iterative methods like Test-Time Training (TTT) break hardware parallelism due to state-dependent gradients. We propose PRISM (Parallel Residual Iterative Sequence Model) to resolve this tension. PRISM introduces a solver-inspired inductive bias that captures key structural properties of multi-step refinement in a parallelizable form. We employ a Write-Forget Decoupling strategy that isolates non-linearity within the injection operator. To bypass the serial dependency of explicit solvers, PRISM utilizes a two-stage proxy architecture: a short-convolution anchors the initial residual using local history energy, while a learned predictor estimates the refinement updates directly from the input. This design distills structural patterns associated with iterative correction into a parallelizable feedforward operator. Theoretically, we prove that this formulation achieves Rank-$L$ accumulation, structurally expanding the update manifold beyond the single-step Rank-$1$ bottleneck. Empirically, it achieves comparable performance to explicit optimization methods while achieving 174x higher throughput.

</details>


### [100] [RePO: Bridging On-Policy Learning and Off-Policy Knowledge through Rephrasing Policy Optimization](https://arxiv.org/abs/2602.10819)
*Linxuan Xia,Xiaolong Yang,Yongyuan Chen,Enyue Zhao,Deng Cai,Yasheng Wang,Boxi Wu*

Main category: cs.LG

TL;DR: RePO通过让策略模型重新表述离轨知识来吸收硬样本，同时保持在线RL的稳定性，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前领域特定数据对齐LLM存在挑战：SFT会降低模型通用性，在线RL难以有效吸收超出当前推理水平的硬样本，而离轨RL存在训练不稳定问题。需要平衡有效吸收离轨知识与保持在线RL稳定性。

Method: 提出Rephrasing Policy Optimization (RePO)：策略模型首先理解离轨知识，然后将其重新表述为符合自身风格和参数分布的轨迹。动态用这些高质量重新表述的轨迹替换低奖励的rollouts，引导模型走向正确推理路径，同时严格保持在线训练动态。

Result: 在多个基准测试中，RePO改善了硬样本利用率，超越了现有基线方法，实现了最先进的性能。

Conclusion: RePO成功解决了离轨知识吸收与在线RL稳定性之间的权衡问题，为领域特定LLM对齐提供了有效解决方案。

Abstract: Aligning large language models (LLMs) on domain-specific data remains a fundamental challenge. Supervised fine-tuning (SFT) offers a straightforward way to inject domain knowledge but often degrades the model's generality. In contrast, on-policy reinforcement learning (RL) preserves generality but fails to effectively assimilate hard samples that exceed the model's current reasoning level. Recent off-policy RL attempts improve hard sample utilization, yet they suffer from severe training instability due to the forced distribution shift toward off-policy knowledge. To reconcile effective off-policy knowledge absorption with the stability of on-policy RL, we propose Rephrasing Policy Optimization (RePO). In RePO, the policy model is prompted to first comprehend off-policy knowledge and then rephrase it into trajectories that conform to its own stylistic and parametric distribution. RePO dynamically replaces low-reward rollouts with these rephrased, high-quality trajectories. This strategy guides the model toward correct reasoning paths while strictly preserving on-policy training dynamics. Experiments on several benchmarks demonstrate that RePO improves hard-sample utilization and outperforms existing baselines, achieving state-of-the-art performance.

</details>


### [101] [Adaptive Sampling for Private Worst-Case Group Optimization](https://arxiv.org/abs/2602.10820)
*Max Cairney-Leeming,Amartya Sanyal,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 提出ASC算法，用于差分隐私下的最差群体优化，通过自适应控制采样率和裁剪阈值，在保证所有群体隐私一致性的同时提升最差群体准确率。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过加权优化最差群体表现，但在差分隐私下会导致隐私保护不均（特别是少数群体隐私更弱），需要解决隐私一致性与群体优化之间的矛盾。

Method: ASC算法自适应控制每个群体的采样率和梯度裁剪阈值，让难学习的群体被更频繁采样，同时确保所有群体获得一致的隐私保证。

Result: 相比先前工作，ASC产生更低方差的梯度、更紧的隐私保证，显著提高最差群体准确率而不牺牲整体平均准确率。

Conclusion: ASC算法有效解决了差分隐私下群体优化中的隐私不均问题，实现了隐私一致性与最差群体性能提升的平衡。

Abstract: Models trained by minimizing the average loss often fail to be accurate on small or hard-to-learn groups of the data. Various methods address this issue by optimizing a weighted objective that focuses on the worst-performing groups. However, this approach becomes problematic when learning with differential privacy, as unequal data weighting can result in inhomogeneous privacy guarantees, in particular weaker privacy for minority groups. In this work, we introduce a new algorithm for differentially private worst-case group optimization called ASC (Adaptively Sampled and Clipped Worst-case Group Optimization). It adaptively controls both the sampling rate and the clipping threshold of each group. Thereby, it allows for harder-to-learn groups to be sampled more often while ensuring consistent privacy guarantees across all groups. Comparing ASC to prior work, we show that it results in lower-variance gradients, tighter privacy guarantees, and substantially higher worst-case group accuracy without sacrificing overall average accuracy.

</details>


### [102] [SimuScene: Training and Benchmarking Code Generation to Simulate Physical Scenarios](https://arxiv.org/abs/2602.10840)
*Yanan Wang,Renxi Wang,Yongxin Wang,Xuezhi Liang,Fajri Koto,Timothy Baldwin,Xiaodan Liang,Haonan Li*

Main category: cs.LG

TL;DR: SimuScene是首个系统研究LLM通过代码模拟物理场景能力的工作，涵盖5个物理领域和52个物理概念，构建了包含7,659个场景的数据集，发现当前最强模型仅21.5%通过率，并提出使用视觉语言模型作为评判的强化学习训练方法。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在数学竞赛、复杂编码和科学推理等任务上得到广泛研究，但其通过代码准确表示和模拟物理场景的能力仍未得到充分探索。现有研究缺乏对LLM物理模拟能力的系统性评估。

Method: 1) 构建SimuScene数据集：通过自动流水线收集数据，人工验证确保质量，包含5个物理领域和52个物理概念的7,659个物理场景，其中334个人工验证示例作为测试集；2) 评估10个当代LLM；3) 提出强化学习流水线：使用视觉语言模型作为评判，通过视觉奖励训练文本模型。

Result: 1) 评估发现即使最强模型在物理场景模拟任务上仅达到21.5%的通过率，表明任务难度很高；2) 使用SimuScene数据进行训练的实验表明，该方法不仅能提升物理模拟能力，还能显著增强通用代码生成性能。

Conclusion: SimuScene填补了LLM物理模拟能力评估的空白，揭示了当前模型在该任务上的局限性，提出的强化学习训练方法有效提升了模型的物理场景模拟能力和通用代码生成能力，为未来研究提供了基准和方法基础。

Abstract: Large language models (LLMs) have been extensively studied for tasks like math competitions, complex coding, and scientific reasoning, yet their ability to accurately represent and simulate physical scenarios via code remains underexplored. We propose SimuScene, the first systematic study that trains and evaluates LLMs on simulating physical scenarios across five physics domains and 52 physical concepts. We build an automatic pipeline to collect data, with human verification to ensure quality. The final dataset contains 7,659 physical scenarios with 334 human-verified examples as the test set. We evaluated 10 contemporary LLMs and found that even the strongest model achieves only a 21.5% pass rate, demonstrating the difficulty of the task. Finally, we introduce a reinforcement learning pipeline with visual rewards that uses a vision-language model as a judge to train textual models. Experiments show that training with our data improves physical simulation via code while substantially enhancing general code generation performance.

</details>


### [103] [Enhancing Multivariate Time Series Forecasting with Global Temporal Retrieval](https://arxiv.org/abs/2602.10847)
*Fanpu Cao,Lu Dai,Jindong Han,Hui Xiong*

Main category: cs.LG

TL;DR: GTR是一个轻量级即插即用模块，通过维护自适应全局时间嵌入并动态检索对齐相关全局片段，有效结合局部与全局依赖关系，提升多元时间序列预测的全局周期性建模能力。


<details>
  <summary>Details</summary>
Motivation: 现有多元时间序列预测模型受限于有限的历史上下文，无法有效捕捉跨越多个周期的全局周期性模式。简单扩展历史窗口会导致过拟合、计算成本过高和信息冗余等问题。

Method: 提出全局时间检索器(GTR)：维护整个周期的自适应全局时间嵌入，动态检索并输入序列对齐相关全局片段，通过2D卷积和残差融合联合建模局部和全局依赖关系。

Result: 在六个真实世界数据集上的实验表明，GTR在短期和长期预测场景中均能实现最先进的性能，同时仅带来最小的参数和计算开销。

Conclusion: GTR是一种高效通用的解决方案，能够在不改变宿主模型架构的情况下增强多元时间序列预测任务中的全局周期性建模能力。

Abstract: Multivariate time series forecasting (MTSF) plays a vital role in numerous real-world applications, yet existing models remain constrained by their reliance on a limited historical context. This limitation prevents them from effectively capturing global periodic patterns that often span cycles significantly longer than the input horizon - despite such patterns carrying strong predictive signals. Naive solutions, such as extending the historical window, lead to severe drawbacks, including overfitting, prohibitive computational costs, and redundant information processing. To address these challenges, we introduce the Global Temporal Retriever (GTR), a lightweight and plug-and-play module designed to extend any forecasting model's temporal awareness beyond the immediate historical context. GTR maintains an adaptive global temporal embedding of the entire cycle and dynamically retrieves and aligns relevant global segments with the input sequence. By jointly modeling local and global dependencies through a 2D convolution and residual fusion, GTR effectively bridges short-term observations with long-term periodicity without altering the host model architecture. Extensive experiments on six real-world datasets demonstrate that GTR consistently delivers state-of-the-art performance across both short-term and long-term forecasting scenarios, while incurring minimal parameter and computational overhead. These results highlight GTR as an efficient and general solution for enhancing global periodicity modeling in MTSF tasks. Code is available at this repository: https://github.com/macovaseas/GTR.

</details>


### [104] [Time Series Foundation Models for Energy Load Forecasting on Consumer Hardware: A Multi-Dimensional Zero-Shot Benchmark](https://arxiv.org/abs/2602.10848)
*Luigi Simeone*

Main category: cs.LG

TL;DR: 该研究对时间序列基础模型在电力需求预测中的零样本能力进行了多维基准测试，发现长上下文下表现最佳，但校准效果差异显著。


<details>
  <summary>Details</summary>
Motivation: 评估时间序列基础模型的零样本预测能力是否适用于电力需求预测等关键任务应用，这些应用对准确性、校准和鲁棒性有严格要求。

Method: 使用ERCOT 2020-2024年每小时负荷数据，在消费级硬件上评估4个TSFM模型（Chronos-Bolt、Chronos-2、Moirai-2、TinyTimeMixer），以Prophet为行业基准，SARIMA和季节性朴素模型为统计参考。评估四个维度：上下文长度敏感性、概率预测校准、分布偏移鲁棒性、操作决策支持。

Result: 最佳基础模型在长上下文（2048小时）下MASE达到0.31，比季节性朴素基准降低47%。Prophet在短于季节周期的拟合窗口下失败（MASE>74），而TSFM即使在最小上下文中也能保持稳定。校准差异显著：Chronos-2校准良好，Moirai-2和Prophet则过于自信。

Conclusion: 时间序列基础模型在电力需求预测中展现出有前景的零样本能力，特别是在长上下文设置下。但校准效果因模型而异，需要谨慎选择。研究提供了实用的模型选择指南并发布了完整的基准框架。

Abstract: Time Series Foundation Models (TSFMs) have introduced zero-shot prediction capabilities that bypass the need for task-specific training. Whether these capabilities translate to mission-critical applications such as electricity demand forecasting--where accuracy, calibration, and robustness directly affect grid operations--remains an open question. We present a multi-dimensional benchmark evaluating four TSFMs (Chronos-Bolt, Chronos-2, Moirai-2, and TinyTimeMixer) alongside Prophet as an industry-standard baseline and two statistical references (SARIMA and Seasonal Naive), using ERCOT hourly load data from 2020 to 2024. All experiments run on consumer-grade hardware (AMD Ryzen 7, 16GB RAM, no GPU). The evaluation spans four axes: (1) context length sensitivity from 24 to 2048 hours, (2) probabilistic forecast calibration, (3) robustness under distribution shifts including COVID-19 lockdowns and Winter Storm Uri, and (4) prescriptive analytics for operational decision support. The top-performing foundation models achieve MASE values near 0.31 at long context lengths (C = 2048h, day-ahead horizon), a 47% reduction over the Seasonal Naive baseline. The inclusion of Prophet exposes a structural advantage of pre-trained models: Prophet fails when the fitting window is shorter than its seasonality period (MASE > 74 at 24-hour context), while TSFMs maintain stable accuracy even with minimal context because they recognise temporal patterns learned during pre-training rather than estimating them from scratch. Calibration varies substantially across models--Chronos-2 produces well-calibrated prediction intervals (95% empirical coverage at 90% nominal level) while both Moirai-2 and Prophet exhibit overconfidence (~70% coverage). We provide practical model selection guidelines and release the complete benchmark framework for reproducibility.

</details>


### [105] [Automated Model Design using Gated Neuron Selection in Telecom](https://arxiv.org/abs/2602.10854)
*Adam Orucu,Marcus Medhage,Farnaz Moradi,Andreas Johnsson,Sarunas Girdzijauskas*

Main category: cs.LG

TL;DR: TabGNS：一种针对电信网络表格数据的梯度神经架构搜索方法，能自动设计高性能紧凑模型，减少架构大小51-82%，搜索时间缩短36倍。


<details>
  <summary>Details</summary>
Motivation: 电信行业在采用深度学习进行流量预测、信号强度预测等关键任务时面临挑战：设计适合资源受限环境的紧凑神经网络架构既困难又耗时，需要自动化模型设计流程。

Method: 提出TabGNS（表格门控神经元选择），一种专门针对电信网络表格数据的梯度神经架构搜索方法，通过门控机制自动选择最优神经元配置。

Result: 在多个电信和通用表格数据集上评估，TabGNS在预测性能上有所提升，同时将架构大小减少51-82%，搜索时间相比最先进的表格NAS方法缩短达36倍。

Conclusion: TabGNS能够自动化设计神经网络，加速电信网络中机器学习解决方案的部署，可集成到模型生命周期管理中实现全流程自动化。

Abstract: The telecommunications industry is experiencing rapid growth in adopting deep learning for critical tasks such as traffic prediction, signal strength prediction, and quality of service optimisation. However, designing neural network architectures for these applications remains challenging and time-consuming, particularly when targeting compact models suitable for resource-constrained network environments. Therefore, there is a need for automating the model design process to create high-performing models efficiently. This paper introduces TabGNS (Tabular Gated Neuron Selection), a novel gradient-based Neural Architecture Search (NAS) method specifically tailored for tabular data in telecommunications networks. We evaluate TabGNS across multiple telecommunications and generic tabular datasets, demonstrating improvements in prediction performance while reducing the architecture size by 51-82% and reducing the search time by up to 36x compared to state-of-the-art tabular NAS methods. Integrating TabGNS into the model lifecycle management enables automated design of neural networks throughout the lifecycle, accelerating deployment of ML solutions in telecommunications networks.

</details>


### [106] [ICA: Information-Aware Credit Assignment for Visually Grounded Long-Horizon Information-Seeking Agents](https://arxiv.org/abs/2602.10863)
*Cong Pang,Xuyu Feng,Yujie Yi,Zixuan Chen,Jiawei Hong,Tiankuo Yao,Nang Yuan,Jiapeng Luo,Lewei Lu,Xin Lou*

Main category: cs.LG

TL;DR: 提出视觉原生搜索框架，将网页表示为视觉快照，结合信息感知信用分配方法，解决开放网络环境中强化学习信号噪声比低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的解析器丢弃布局语义并引入非结构化噪声，而长时程训练依赖稀疏结果奖励，难以识别哪些检索动作真正重要，导致开放网络环境中学习效率低下。

Method: 1) 视觉原生搜索框架：将网页表示为视觉快照，利用布局线索快速定位关键证据并抑制干扰；2) 信息感知信用分配：通过后验分析估计每个检索快照对最终结果的贡献，将密集学习信号传播回关键搜索轮次；3) 与GRPO训练流程集成。

Result: 在多样化信息寻求基准测试中，该方法持续优于基于文本的基线方法，证明视觉快照基础与信息级信用分配能够缓解开放网络环境中的信用分配瓶颈。

Conclusion: 视觉原生表示结合信息级信用分配是解决开放网络环境中强化学习信号噪声比低问题的有效方法，代码和数据集将在GitHub发布。

Abstract: Despite the strong performance achieved by reinforcement learning-trained information-seeking agents, learning in open-ended web environments remains severely constrained by low signal-to-noise feedback. Text-based parsers often discard layout semantics and introduce unstructured noise, while long-horizon training typically relies on sparse outcome rewards that obscure which retrieval actions actually matter. We propose a visual-native search framework that represents webpages as visual snapshots, allowing agents to leverage layout cues to quickly localize salient evidence and suppress distractors. To learn effectively from these high-dimensional observations, we introduce Information-Aware Credit Assignment (ICA), a post-hoc method that estimates each retrieved snapshot's contribution to the final outcome via posterior analysis and propagates dense learning signals back to key search turns. Integrated with a GRPO-based training pipeline, our approach consistently outperforms text-based baselines on diverse information-seeking benchmarks, providing evidence that visual snapshot grounding with information-level credit assignment alleviates the credit-assignment bottleneck in open-ended web environments. The code and datasets will be released in https://github.com/pc-inno/ICA_MM_deepsearch.git.

</details>


### [107] [The Sample Complexity of Uniform Approximation for Multi-Dimensional CDFs and Fixed-Price Mechanisms](https://arxiv.org/abs/2602.10868)
*Matteo Castiglioni,Anna Lunghi,Alberto Marchesi*

Main category: cs.LG

TL;DR: 研究在最小化单比特反馈下学习n维累积分布函数均匀近似的样本复杂度，发现样本复杂度具有近维度不变性，维度n仅影响对数项


<details>
  <summary>Details</summary>
Motivation: 研究在"强盗反馈"（单比特反馈）设置下学习多维累积分布函数均匀近似的样本复杂度，作为"完全反馈"下多元DKW不等式的对应扩展

Method: 在最小化单比特反馈设置下，研究学习n维累积分布函数在任意精细网格上的均匀ε-近似所需的样本复杂度

Result: 获得了样本复杂度的近维度不变性：在任意精细网格上获得均匀ε-近似需要样本复杂度为1/ε³ * (log(1/ε))^{O(n)}，其中维度n仅影响对数项

Conclusion: 该结果为小市场中学习固定价格机制（如双边交易设置）提供了紧的样本复杂度界限和新的遗憾保证，扩展了多元DKW不等式到强盗反馈设置

Abstract: We study the sample complexity of learning a uniform approximation of an $n$-dimensional cumulative distribution function (CDF) within an error $ε> 0$, when observations are restricted to a minimal one-bit feedback. This serves as a counterpart to the multivariate DKW inequality under ''full feedback'', extending it to the setting of ''bandit feedback''. Our main result shows a near-dimensional-invariance in the sample complexity: we get a uniform $ε$-approximation with a sample complexity $\frac{1}{ε^3}{\log\left(\frac 1 ε\right)^{\mathcal{O}(n)}}$ over a arbitrary fine grid, where the dimensionality $n$ only affects logarithmic terms. As direct corollaries, we provide tight sample complexity bounds and novel regret guarantees for learning fixed-price mechanisms in small markets, such as bilateral trade settings.

</details>


### [108] [FedPS: Federated data Preprocessing via aggregated Statistics](https://arxiv.org/abs/2602.10870)
*Xuefeng Xu,Graham Cormode*

Main category: cs.LG

TL;DR: FedPS是一个基于聚合统计的联邦数据预处理框架，解决了联邦学习中数据预处理被忽视的问题，通过数据草图技术实现高效、隐私保护且通信高效的分布式预处理。


<details>
  <summary>Details</summary>
Motivation: 联邦学习需要在训练前进行数据预处理（处理缺失值、格式不一致、特征尺度异构），但现有研究大多忽视这一关键环节。实际联邦系统中，隐私约束禁止集中原始数据，通信效率也给分布式预处理带来挑战。

Method: 基于聚合统计的统一框架，利用数据草图技术高效总结本地数据集并保留关键统计信息。设计了特征缩放、编码、离散化和缺失值插补的联邦算法，并将k-Means、k-NN和贝叶斯线性回归等预处理相关模型扩展到横向和纵向联邦学习设置。

Result: FedPS为实际联邦学习部署提供了灵活、通信高效且一致的预处理流水线，支持横向和纵向联邦学习场景。

Conclusion: FedPS填补了联邦学习中数据预处理研究的空白，通过基于聚合统计的方法解决了隐私保护和通信效率的挑战，为实际联邦系统提供了实用的预处理解决方案。

Abstract: Federated Learning (FL) enables multiple parties to collaboratively train machine learning models without sharing raw data. However, before training, data must be preprocessed to address missing values, inconsistent formats, and heterogeneous feature scales. This preprocessing stage is critical for model performance but is largely overlooked in FL research. In practical FL systems, privacy constraints prohibit centralizing raw data, while communication efficiency introduces further challenges for distributed preprocessing. We introduce FedPS, a unified framework for federated data preprocessing based on aggregated statistics. FedPS leverages data-sketching techniques to efficiently summarize local datasets while preserving essential statistical information. Building on these summaries, we design federated algorithms for feature scaling, encoding, discretization, and missing-value imputation, and extend preprocessing-related models such as k-Means, k-Nearest Neighbors, and Bayesian Linear Regression to both horizontal and vertical FL settings. FedPS provides flexible, communication-efficient, and consistent preprocessing pipelines for practical FL deployments.

</details>


### [109] [Resource-Efficient Model-Free Reinforcement Learning for Board Games](https://arxiv.org/abs/2602.10894)
*Kazuki Ota,Takayuki Osa,Motoki Omura,Tatsuya Harada*

Main category: cs.LG

TL;DR: 提出一种用于棋盘游戏的模型无关强化学习算法，相比基于搜索的方法（如AlphaZero）能实现更高效学习，在五种棋盘游戏中验证了其效率优势。


<details>
  <summary>Details</summary>
Motivation: 基于搜索的强化学习方法（如AlphaZero）在棋盘游戏中取得了显著成功，但其巨大的计算需求阻碍了可复现性。本研究旨在开发更高效的模型无关强化学习算法，以降低计算成本。

Method: 提出一种专门为棋盘游戏设计的模型无关强化学习算法，通过综合实验在五种棋盘游戏（动物将棋、Gardner国际象棋、围棋、Hex、黑白棋）上进行验证，并进行广泛的消融研究分析核心技术的贡献。

Result: 实验结果表明，所提出的方法在五种棋盘游戏环境中均比现有方法实现了更高效的学习。消融研究揭示了所采用核心技术的重重要性。

Conclusion: 该高效算法展示了模型无关强化学习在传统上由基于搜索方法主导的领域中的潜力，为降低棋盘游戏AI的计算需求提供了可行方案。

Abstract: Board games have long served as complex decision-making benchmarks in artificial intelligence. In this field, search-based reinforcement learning methods such as AlphaZero have achieved remarkable success. However, their significant computational demands have been pointed out as barriers to their reproducibility. In this study, we propose a model-free reinforcement learning algorithm designed for board games to achieve more efficient learning. To validate the efficiency of the proposed method, we conducted comprehensive experiments on five board games: Animal Shogi, Gardner Chess, Go, Hex, and Othello. The results demonstrate that the proposed method achieves more efficient learning than existing methods across these environments. In addition, our extensive ablation study shows the importance of core techniques used in the proposed method. We believe that our efficient algorithm shows the potential of model-free reinforcement learning in domains traditionally dominated by search-based methods.

</details>


### [110] [Natural Hypergradient Descent: Algorithm Design, Convergence Analysis, and Parallel Implementation](https://arxiv.org/abs/2602.10905)
*Deyi Kong,Zaiwei Chen,Shuzhong Zhang,Shancong Mou*

Main category: cs.LG

TL;DR: 提出Natural Hypergradient Descent (NHGD)方法，通过使用经验Fisher信息矩阵替代Hessian逆矩阵，解决双层优化中超梯度估计的计算瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 解决双层优化问题中超梯度估计的计算瓶颈——需要计算或近似Hessian逆矩阵，这在大规模机器学习设置中计算成本高昂。

Method: 利用内层优化问题的统计结构，使用经验Fisher信息矩阵作为Hessian的渐近一致替代，实现并行优化-近似框架，在可忽略的额外成本下重用梯度信息。

Result: 理论分析建立了高概率误差界和样本复杂度保证，与最先进的优化-然后-近似方法相匹配，同时显著减少计算时间开销。实证评估显示NHGD在代表性双层学习任务中具有实际优势。

Conclusion: NHGD通过创新的Hessian逆近似方法，在保持理论保证的同时显著提升计算效率，为大规模机器学习中的双层优化问题提供了可扩展且有效的解决方案。

Abstract: In this work, we propose Natural Hypergradient Descent (NHGD), a new method for solving bilevel optimization problems. To address the computational bottleneck in hypergradient estimation--namely, the need to compute or approximate Hessian inverse--we exploit the statistical structure of the inner optimization problem and use the empirical Fisher information matrix as an asymptotically consistent surrogate for the Hessian. This design enables a parallel optimize-and-approximate framework in which the Hessian-inverse approximation is updated synchronously with the stochastic inner optimization, reusing gradient information at negligible additional cost. Our main theoretical contribution establishes high-probability error bounds and sample complexity guarantees for NHGD that match those of state-of-the-art optimize-then-approximate methods, while significantly reducing computational time overhead. Empirical evaluations on representative bilevel learning tasks further demonstrate the practical advantages of NHGD, highlighting its scalability and effectiveness in large-scale machine learning settings.

</details>


### [111] [Tuning the burn-in phase in training recurrent neural networks improves their performance](https://arxiv.org/abs/2602.10911)
*Julian D. Schiller,Malte Heinrich,Victor G. Lopez,Matthias A. Müller*

Main category: cs.LG

TL;DR: 该论文研究了在时间序列任务中使用截断BPTT训练RNN的方法，建立了在子序列而非完整序列上优化的理论性能界限，发现RNN的预热阶段是重要调节参数，能显著影响训练效果。


<details>
  <summary>Details</summary>
Motivation: 标准BPTT训练RNN在处理长输入序列时存在计算和内存挑战，截断BPTT通过处理较短数据段来减少开销，但需要理论分析其性能损失。

Method: 建立理论框架分析截断学习方法的准确性和性能损失界限，特别关注RNN预热阶段的影响，并在系统辨识和时间序列预测的标准基准上进行实验验证。

Result: 理论分析揭示了预热阶段对训练性能的重要影响，实验表明适当调节预热阶段可使训练和测试数据的预测误差减少超过60%。

Conclusion: 截断BPTT是训练RNN的有效替代方法，预热阶段是关键调节参数，正确设置能显著提升性能，为实际应用提供了理论指导和实用建议。

Abstract: Training recurrent neural networks (RNNs) with standard backpropagation through time (BPTT) can be challenging, especially in the presence of long input sequences. A practical alternative to reduce computational and memory overhead is to perform BPTT repeatedly over shorter segments of the training data set, corresponding to truncated BPTT. In this paper, we examine the training of RNNs when using such a truncated learning approach for time series tasks. Specifically, we establish theoretical bounds on the accuracy and performance loss when optimizing over subsequences instead of the full data sequence. This reveals that the burn-in phase of the RNN is an important tuning knob in its training, with significant impact on the performance guarantees. We validate our theoretical results through experiments on standard benchmarks from the fields of system identification and time series forecasting. In all experiments, we observe a strong influence of the burn-in phase on the training process, and proper tuning can lead to a reduction of the prediction error on the training and test data of more than 60% in some cases.

</details>


### [112] [Near-Constant Strong Violation and Last-Iterate Convergence for Online CMDPs via Decaying Safety Margins](https://arxiv.org/abs/2602.10917)
*Qian Zuo,Zhiyong Wang,Fengxiang He*

Main category: cs.LG

TL;DR: FlexDOME算法在约束马尔可夫决策过程中实现了近乎恒定的强约束违反和次线性强遗憾，解决了现有方法中约束违反增长或仅平均迭代收敛的问题。


<details>
  <summary>Details</summary>
Motivation: 现有对偶方法在实现次线性强奖励遗憾时，要么导致增长的强约束违反，要么受限于平均迭代收敛，无法同时满足强遗憾和强约束违反的要求。

Method: 提出FlexDOME算法，将对偶框架与时间变化的安全裕度和正则化项结合，采用基于项级渐近主导的策略，通过严格调度安全裕度来渐近主导优化和统计误差的函数衰减率。

Result: FlexDOME首次可证明地实现了近乎恒定的Õ(1)强约束违反、次线性强遗憾和非渐近最后迭代收敛，实验验证了理论发现。

Conclusion: FlexDOME通过创新的安全裕度调度和正则化设计，解决了约束强化学习中强遗憾和强约束违反之间的权衡问题，实现了优越的性能保证。

Abstract: We study safe online reinforcement learning in Constrained Markov Decision Processes (CMDPs) under strong regret and violation metrics, which forbid error cancellation over time. Existing primal-dual methods that achieve sublinear strong reward regret inevitably incur growing strong constraint violation or are restricted to average-iterate convergence due to inherent oscillations. To address these limitations, we propose the Flexible safety Domain Optimization via Margin-regularized Exploration (FlexDOME) algorithm, the first to provably achieve near-constant $\tilde{O}(1)$ strong constraint violation alongside sublinear strong regret and non-asymptotic last-iterate convergence. FlexDOME incorporates time-varying safety margins and regularization terms into the primal-dual framework. Our theoretical analysis relies on a novel term-wise asymptotic dominance strategy, where the safety margin is rigorously scheduled to asymptotically majorize the functional decay rates of the optimization and statistical errors, thereby clamping cumulative violations to a near-constant level. Furthermore, we establish non-asymptotic last-iterate convergence guarantees via a policy-dual Lyapunov argument. Experiments corroborate our theoretical findings.

</details>


### [113] [Spatial-Morphological Modeling for Multi-Attribute Imputation of Urban Blocks](https://arxiv.org/abs/2602.10923)
*Vasilii Starikov,Ruslan Kozliak,Georgii Kontsevik,Sergey Mityagin*

Main category: cs.LG

TL;DR: 提出SM插补工具，结合形态聚类与邻域方法，重建城市街区FSI/GSI缺失值，优于现有SOTA模型


<details>
  <summary>Details</summary>
Motivation: 城市形态指标的准确重建对城市规划和数据分析至关重要，需要处理城市街区层面的FSI和GSI缺失值问题

Method: 提出空间-形态插补工具，结合数据驱动的形态聚类（全局先验）与邻域方法（IDW/sKNN），基于SpaceMatrix框架

Result: SM单独使用能捕捉有意义的形态结构，但与IDW或sKNN结合时性能优于现有SOTA模型，显示形态与空间方法的互补优势

Conclusion: 组合方法展示了形态和空间方法的互补优势，为城市形态指标重建提供了有效工具

Abstract: Accurate reconstruction of missing morphological indicators of a city is crucial for urban planning and data-driven analysis. This study presents the spatial-morphological (SM) imputer tool, which combines data-driven morphological clustering with neighborhood-based methods to reconstruct missing values of the floor space index (FSI) and ground space index (GSI) at the city block level, inspired by the SpaceMatrix framework. This approach combines city-scale morphological patterns as global priors with local spatial information for context-dependent interpolation. The evaluation shows that while SM alone captures meaningful morphological structure, its combination with inverse distance weighting (IDW) or spatial k-nearest neighbor (sKNN) methods provides superior performance compared to existing SOTA models. Composite methods demonstrate the complementary advantages of combining morphological and spatial approaches.

</details>


### [114] [CMAD: Cooperative Multi-Agent Diffusion via Stochastic Optimal Control](https://arxiv.org/abs/2602.10933)
*Riccardo Barbano,Alexander Denker,Zeljko Kereta,Runchang Li,Francisco Vargas*

Main category: cs.LG

TL;DR: 提出将组合生成建模为合作随机最优控制问题的新范式，通过最优控制联合引导预训练扩散模型的轨迹，而非组合概率密度


<details>
  <summary>Details</summary>
Motivation: 当前连续时间生成模型在图像恢复和合成方面取得显著成功，但控制多个预训练模型的组合仍然是一个开放挑战。现有方法主要将组合视为概率密度的代数组合（如乘积或专家混合），这假设目标分布是明确已知的，而这几乎从不是实际情况。

Method: 提出新范式：将组合生成建模为合作随机最优控制问题。不组合概率密度，而是将预训练扩散模型视为交互智能体，通过最优控制联合引导它们的扩散轨迹，朝向在聚合输出上定义的共享目标。

Result: 在条件MNIST生成上验证了该框架，并与天真的推理时DPS风格基线（用每步梯度指导替代学习的合作控制）进行了比较。

Conclusion: 提出了一种新的组合生成方法，通过合作随机最优控制联合引导多个预训练扩散模型，避免了传统概率密度组合方法需要明确目标分布的假设。

Abstract: Continuous-time generative models have achieved remarkable success in image restoration and synthesis. However, controlling the composition of multiple pre-trained models remains an open challenge. Current approaches largely treat composition as an algebraic composition of probability densities, such as via products or mixtures of experts. This perspective assumes the target distribution is known explicitly, which is almost never the case. In this work, we propose a different paradigm that formulates compositional generation as a cooperative Stochastic Optimal Control problem. Rather than combining probability densities, we treat pre-trained diffusion models as interacting agents whose diffusion trajectories are jointly steered, via optimal control, toward a shared objective defined on their aggregated output. We validate our framework on conditional MNIST generation and compare it against a naive inference-time DPS-style baseline replacing learned cooperative control with per-step gradient guidance.

</details>


### [115] [Stochastic Parroting in Temporal Attention -- Regulating the Diagonal Sink](https://arxiv.org/abs/2602.10956)
*Victoria Hankemeier,Malte Hankemeier*

Main category: cs.LG

TL;DR: 本文分析了时序注意力机制中的信息退化问题，证明了时序注意力矩阵存在对角注意力下沉现象，并提出了正则化方法来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 时空模型分析空间结构和时间动态时容易出现时空信息退化问题。先前研究表明，因果注意力或时间卷积中的过度压缩会对第一个token产生偏差。本文旨在分析这种偏差是否也存在于时序注意力机制中。

Method: 推导了时序注意力层Jacobian期望值的敏感性边界，从理论上分析了非对角注意力分数如何依赖于序列长度，证明了时序注意力矩阵存在对角注意力下沉现象，并提出了正则化方法。

Result: 理论分析表明时序注意力机制确实存在对角注意力下沉问题，即注意力过度集中在对角线元素上。提出的正则化方法在实验中证明是有效的。

Conclusion: 时序注意力机制存在与因果注意力类似的信息退化问题，表现为对角注意力下沉。通过适当的正则化方法可以有效缓解这一问题，提高模型的时空信息处理能力。

Abstract: Spatio-temporal models analyze spatial structures and temporal dynamics, which makes them prone to information degeneration among space and time. Prior literature has demonstrated that over-squashing in causal attention or temporal convolutions creates a bias on the first tokens. To analyze whether such a bias is present in temporal attention mechanisms, we derive sensitivity bounds on the expected value of the Jacobian of a temporal attention layer. We theoretically show how off-diagonal attention scores depend on the sequence length, and that temporal attention matrices suffer a diagonal attention sink. We suggest regularization methods, and experimentally demonstrate their effectiveness.

</details>


### [116] [Rotary Positional Embeddings as Phase Modulation: Theoretical Bounds on the RoPE Base for Long-Context Transformers](https://arxiv.org/abs/2602.10959)
*Feilong Liu*

Main category: cs.LG

TL;DR: 该论文将RoPE重新解释为相位调制，推导了长上下文长度下RoPE基参数的理论边界，包括防止混叠的下界和防止数值精度问题的上界，定义了Transformer的"Goldilocks区域"。


<details>
  <summary>Details</summary>
Motivation: RoPE在大语言模型中广泛用于位置编码，但其在长上下文长度下的行为缺乏理论分析。需要理解RoPE在长序列中的表现，为长上下文Transformer设计提供理论指导。

Method: 将RoPE重新解释为应用于复数振荡器组的相位调制，使用经典信号处理理论进行分析。推导了RoPE基参数的下界（混叠边界和DC稳定性边界）和上界（浮点精度边界），并扩展到深层Transformer。

Result: 建立了RoPE基参数的精确理论边界，预测了长上下文Transformer的可行性区域。通过LLaMA、Mistral、DeepSeek等模型验证了理论边界，发现违反稳定性边界的模型会出现注意力崩溃和长距离性能下降。

Conclusion: RoPE在长上下文中的表现受理论边界约束，包括混叠下界和精度上界，形成了深度和精度依赖的可行性区域。这为设计长上下文Transformer提供了理论框架，解释了现有模型的成功与失败。

Abstract: Rotary positional embeddings (RoPE) are widely used in large language models to encode token positions through multiplicative rotations, yet their behavior at long context lengths remains poorly characterized. In this work, we reinterpret RoPE as phase modulation applied to a bank of complex oscillators, enabling analysis through classical signal processing theory.
  Under this formulation, we derive principled lower bounds on the RoPE base parameter that are necessary to preserve positional coherence over a target context length. These include a fundamental aliasing bound, analogous to a Nyquist limit, and a DC-component stability bound that constrains phase drift in low-frequency positional modes. We further extend this analysis to deep transformers, showing that repeated rotary modulation across layers compounds angular misalignment, tightening the base requirement as depth increases.
  Complementing these results, we derive a precision-dependent upper bound on the RoPE base arising from finite floating-point resolution. Beyond this limit, incremental phase updates become numerically indistinguishable, leading to positional erasure even in the absence of aliasing. Together, the lower and upper bounds define a precision- and depth-dependent feasibility region a Goldilocks zone for long-context transformers.
  We validate the framework through a comprehensive case study of state-of-the-art models, including LLaMA, Mistral, and DeepSeek variants, showing that observed successes, failures, and community retrofits align closely with the predicted bounds. Notably, models that violate the stability bound exhibit attention collapse and long-range degradation, while attempts to scale beyond one million tokens encounter a hard precision wall independent of architecture or training.

</details>


### [117] [MoEEdit: Efficient and Routing-Stable Knowledge Editing for Mixture-of-Experts LLMs](https://arxiv.org/abs/2602.10965)
*Yupu Gu,Rongzhe Wei,Andy Zhu,Pan Li*

Main category: cs.LG

TL;DR: MoEEdit：首个针对稀疏MoE大语言模型的路由稳定知识编辑框架，通过专家空空间投影保持路由稳定，实现高效精确的知识修改。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法主要针对密集架构设计，难以适用于日益流行的稀疏MoE模型。直接适配密集模型编辑器计算成本高且容易导致路由分布偏移，破坏稳定性和一致性。

Method: 提出MoEEdit框架，通过每个专家的空空间投影重新参数化专家更新，保持路由器输入不变从而抑制路由偏移。采用块坐标下降（BCD）求解器高效解决块结构优化问题。

Result: 实验表明MoEEdit在保持高特异性和路由稳定性的同时，实现了最先进的效能和泛化能力，具有优越的计算和内存效率。

Conclusion: MoEEdit为稀疏大语言模型的可扩展、精确知识编辑奠定了坚实基础，并强调了路由稳定干预的重要性。

Abstract: Knowledge editing (KE) enables precise modifications to factual content in large language models (LLMs). Existing KE methods are largely designed for dense architectures, limiting their applicability to the increasingly prevalent sparse Mixture-of-Experts (MoE) models that underpin modern scalable LLMs. Although MoEs offer strong efficiency and capacity scaling, naively adapting dense-model editors is both computationally costly and prone to routing distribution shifts that undermine stability and consistency. To address these challenges, we introduce MoEEdit, the first routing-stable framework for parameter-modifying knowledge editing in MoE LLMs. Our method reparameterizes expert updates via per-expert null-space projections that keep router inputs invariant and thereby suppress routing shifts. The resulting block-structured optimization is solved efficiently with a block coordinate descent (BCD) solver. Experiments show that MoEEdit attains state-of-the-art efficacy and generalization while preserving high specificity and routing stability, with superior compute and memory efficiency. These results establish a robust foundation for scalable, precise knowledge editing in sparse LLMs and underscore the importance of routing-stable interventions.

</details>


### [118] [A Jointly Efficient and Optimal Algorithm for Heteroskedastic Generalized Linear Bandits with Adversarial Corruptions](https://arxiv.org/abs/2602.10971)
*Sanghwa Kim,Junghyun Lee,Se-Young Yun*

Main category: cs.LG

TL;DR: 提出HCW-GLB-OMD算法，解决具有对抗性扰动的异方差广义线性老虎机问题，实现计算高效且接近最优的遗憾界


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对异方差广义线性老虎机在对抗性扰动下的统一处理框架，需要同时考虑异方差性和对抗性扰动的影响

Method: 提出HCW-GLB-OMD算法，包含基于在线镜像下降的估计器和基于Hessian的置信权重，实现对抗性扰动的鲁棒性，计算复杂度为每轮O(1)

Result: 在自协调链接函数假设下，获得遗憾界$\tilde{O}(d \sqrt{\sum_t g(τ_t) \dotμ_{t,\star}} + d^2 g_{\max} κ+ d κC)$，并给出匹配的下界$\tildeΩ(d \sqrt{\sum_t g(τ_t) \dotμ_{t,\star}} + d C)$

Conclusion: 算法在异方差广义线性老虎机的各种实例中实现了接近实例级极小极大最优性，统一了先前问题特定的下界，为对抗性扰动下的异方差问题提供了通用解决方案

Abstract: We consider the problem of heteroskedastic generalized linear bandits (GLBs) with adversarial corruptions, which subsumes various stochastic contextual bandit settings, including heteroskedastic linear bandits and logistic/Poisson bandits. We propose HCW-GLB-OMD, which consists of two components: an online mirror descent (OMD)-based estimator and Hessian-based confidence weights to achieve corruption robustness. This is computationally efficient in that it only requires ${O}(1)$ space and time complexity per iteration. Under the self-concordance assumption on the link function, we show a regret bound of $\tilde{O}\left( d \sqrt{\sum_t g(τ_t) \dotμ_{t,\star}} + d^2 g_{\max} κ+ d κC \right)$, where $\dotμ_{t,\star}$ is the slope of $μ$ around the optimal arm at time $t$, $g(τ_t)$'s are potentially exogenously time-varying dispersions (e.g., $g(τ_t) = σ_t^2$ for heteroskedastic linear bandits, $g(τ_t) = 1$ for Bernoulli and Poisson), $g_{\max} = \max_{t \in [T]} g(τ_t)$ is the maximum dispersion, and $C \geq 0$ is the total corruption budget of the adversary. We complement this with a lower bound of $\tildeΩ(d \sqrt{\sum_t g(τ_t) \dotμ_{t,\star}} + d C)$, unifying previous problem-specific lower bounds. Thus, our algorithm achieves, up to a $κ$-factor in the corruption term, instance-wise minimax optimality simultaneously across various instances of heteroskedastic GLBs with adversarial corruptions.

</details>


### [119] [RiemannGL: Riemannian Geometry Changes Graph Deep Learning](https://arxiv.org/abs/2602.10982)
*Li Sun,Qiqi Wan,Suyang Zhou,Zhenhao Huang,Philip S. Yu*

Main category: cs.LG

TL;DR: 该论文提出黎曼几何应作为图表示学习的理论基础，认为现有方法局限于双曲空间等特定流形，主张将内在流形结构融入图神经网络，并提出了三维研究议程。


<details>
  <summary>Details</summary>
Motivation: 图数据具有非欧几里得结构，现有图表示学习方法缺乏几何理论基础。虽然已有研究探索图学习与黎曼几何的结合，但大多局限于特定流形（如双曲空间）并采用外在流形表述，未能真正将内在流形结构赋予图神经网络。

Method: 论文提出黎曼图学习应作为统一范式而非孤立技术，识别了现有方法的概念和方法论差距，并沿着三个维度构建结构化研究议程：流形类型、神经架构和学习范式。

Result: 提出了黎曼图学习的系统性框架，明确了核心使命是为图神经网络赋予内在流形结构，指出了现有方法的局限性，并规划了未来研究方向。

Conclusion: 黎曼几何应为图表示学习提供理论基础，黎曼图学习是统一范式而非孤立技术。需要从流形类型、神经架构和学习范式三个维度推进研究，以充分发挥黎曼图学习的潜力。

Abstract: Graphs are ubiquitous, and learning on graphs has become a cornerstone in artificial intelligence and data mining communities. Unlike pixel grids in images or sequential structures in language, graphs exhibit a typical non-Euclidean structure with complex interactions among the objects. This paper argues that Riemannian geometry provides a principled and necessary foundation for graph representation learning, and that Riemannian graph learning should be viewed as a unifying paradigm rather than a collection of isolated techniques. While recent studies have explored the integration of graph learning and Riemannian geometry, most existing approaches are limited to a narrow class of manifolds, particularly hyperbolic spaces, and often adopt extrinsic manifold formulations. We contend that the central mission of Riemannian graph learning is to endow graph neural networks with intrinsic manifold structures, which remains underexplored. To advance this perspective, we identify key conceptual and methodological gaps in existing approaches and outline a structured research agenda along three dimensions: manifold type, neural architecture, and learning paradigm. We further discuss open challenges, theoretical foundations, and promising directions that are critical for unlocking the full potential of Riemannian graph learning. This paper aims to provide a coherent viewpoint and to stimulate broader exploration of Riemannian geometry as a foundational framework for future graph learning research.

</details>


### [120] [Sample Efficient Generative Molecular Optimization with Joint Self-Improvement](https://arxiv.org/abs/2602.10984)
*Serra Korkmaz,Adam Izdebski,Jonathan Pirnay,Rasmus Møller-Larsen,Michal Kmicikiewicz,Pankhil Gawade,Dominik G. Grimm,Ewa Szczurek*

Main category: cs.LG

TL;DR: 提出Joint Self-Improvement方法，通过联合生成-预测模型和自我改进采样方案，解决分子优化中的样本效率问题和分布偏移挑战。


<details>
  <summary>Details</summary>
Motivation: 生成式分子优化面临两个主要挑战：1) 优化候选分子稀少且评估成本高，需要样本效率；2) 代理模型在优化过程中因候选分子逐渐偏离训练分布而出现分布偏移问题。

Method: 提出Joint Self-Improvement方法，包含两个关键组件：1) 联合生成-预测模型，将生成器与代理模型对齐以缓解分布偏移；2) 自我改进采样方案，在推理时利用预测模型偏置生成部分，高效生成优化分子。

Result: 在离线和在线分子优化基准测试中，Joint Self-Improvement在有限评估预算下优于现有最先进方法。

Conclusion: Joint Self-Improvement通过联合建模和自我改进采样，有效解决了分子优化中的样本效率和分布偏移问题，在有限评估预算下实现了更好的性能。

Abstract: Generative molecular optimization aims to design molecules with properties surpassing those of existing compounds. However, such candidates are rare and expensive to evaluate, yielding sample efficiency essential. Additionally, surrogate models introduced to predict molecule evaluations, suffer from distribution shift as optimization drives candidates increasingly out-of-distribution. To address these challenges, we introduce Joint Self-Improvement, which benefits from (i) a joint generative-predictive model and (ii) a self-improving sampling scheme. The former aligns the generator with the surrogate, alleviating distribution shift, while the latter biases the generative part of the joint model using the predictive one to efficiently generate optimized molecules at inference-time. Experiments across offline and online molecular optimization benchmarks demonstrate that Joint Self-Improvement outperforms state-of-the-art methods under limited evaluation budgets.

</details>


### [121] [TVCACHE: A Stateful Tool-Value Cache for Post-Training LLM Agents](https://arxiv.org/abs/2602.10986)
*Abhishek Vijaya Kumar,Bhaskar Kataria,Byungsoo Oh,Emaad Manzoor,Rachee Singh*

Main category: cs.LG

TL;DR: TVCACHE是一种用于LLM智能体后训练的状态感知工具值缓存系统，通过维护工具调用序列树和最长前缀匹配，在保证环境状态一致性的前提下重用工具调用结果，显著减少后训练时间和成本。


<details>
  <summary>Details</summary>
Motivation: LLM智能体后训练中，外部工具调用耗时数秒甚至数分钟，导致GPU空闲时间增加，显著提高了后训练时间和成本。虽然许多工具调用在并行rollout中重复出现，但由于工具输出依赖于先前智能体交互诱导的环境状态，简单的缓存重用是不正确的。

Method: TVCACHE维护一个观察到的工具调用序列树，采用最长前缀匹配进行缓存查找：只有当智能体的完整工具历史与先前执行的序列完全匹配时才会命中缓存，从而保证环境状态的一致性。

Result: 在三种不同工作负载（终端任务、SQL生成和视频理解）上，TVCACHE实现了高达70%的缓存命中率，将工具调用执行时间中位数减少了高达6.9倍，且后训练奖励累积没有下降。

Conclusion: TVCACHE通过状态感知的缓存机制有效解决了LLM智能体后训练中工具调用耗时的问题，显著提升了训练效率，同时保证了训练质量不受影响。

Abstract: In RL post-training of LLM agents, calls to external tools take several seconds or even minutes, leaving allocated GPUs idle and inflating post-training time and cost. While many tool invocations repeat across parallel rollouts and could in principle be cached, naively caching their outputs for reuse is incorrect since tool outputs depend on the environment state induced by prior agent interactions. We present TVCACHE, a stateful tool-value cache for LLM agent post-training. TVCACHE maintains a tree of observed tool-call sequences and performs longest-prefix matching for cache lookups: a hit occurs only when the agent's full tool history matches a previously executed sequence, guaranteeing identical environment state. On three diverse workloads-terminal-based tasks, SQL generation, and video understanding. TVCACHE achieves cache hit rates of up to 70% and reduces median tool call execution time by up to 6.9X, with no degradation in post-training reward accumulation.

</details>


### [122] [ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression](https://arxiv.org/abs/2602.11008)
*Ammar Ali,Baher Mohammad,Denis Makhov,Dmitriy Shopkhoev,Magauiya Zhussip,Stamatios Lefkimmiatis*

Main category: cs.LG

TL;DR: ROCKET是一种无需训练即可压缩模型的方法，通过多选择背包问题优化层间压缩分配，并采用单步稀疏矩阵分解技术，在20-50%压缩率下保持优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有的模型压缩方法通常需要复杂的训练过程或迭代优化，计算成本高且效率有限。需要一种无需训练的高效压缩方法，能够在保持模型性能的同时显著减少参数量。

Method: 1. 将层间压缩分配建模为多选择背包问题，为每层选择最优压缩级别以最小化重构误差；2. 引入单步稀疏矩阵分解：基于激活-权重敏感性稀疏化权重系数，然后通过最小二乘法闭式更新字典，完全绕过迭代优化、稀疏编码或反向传播。

Result: 在20-50%压缩率下，ROCKET在多种模型架构上均优于现有压缩方法。在30%压缩率下无需微调即可保持原模型90%以上性能。对Qwen3-14B压缩至8B参数模型后，仅用3000万token微调即可达到接近原Qwen3-8B的性能。

Conclusion: ROCKET提出了一种高效的无训练模型压缩框架，通过智能的层间压缩分配和创新的单步稀疏分解技术，在保持模型性能的同时显著减少计算成本，为大规模模型部署提供了实用解决方案。

Abstract: We present ROCKET, a training-free model compression method that achieves state-of-the-art performance in comparison with factorization, structured-sparsification and dynamic compression baselines. Operating under a global compression budget, ROCKET comprises two key innovations: First, it formulates layer-wise compression allocation as a multi-choice knapsack problem, selecting the optimal compression level for each layer to minimize total reconstruction error while adhering to a target model size. Second, it introduces a single-step sparse matrix factorization inspired by dictionary learning: using only a small calibration set, it sparsifies weight coefficients based on activation-weights sensitivity and then updates the dictionary in closed form via least squares bypassing iterative optimization, sparse coding, or backpropagation entirely. ROCKET consistently outperforms existing compression approaches across different model architectures at 20-50\% compression rates. Notably, it retains over 90\% of the original model's performance at 30\% compression without any fine-tuning. Moreover, when applying a light fine-tuning phase, recovery is substantially enhanced: for instance, compressing Qwen3-14B to an 8B-parameter model and healing it with just 30 million tokens yields performance nearly on par with the original Qwen3-8B. The code for ROCKET is at github.com/mts-ai/ROCKET/tree/main.

</details>


### [123] [OSIL: Learning Offline Safe Imitation Policies with Safety Inferred from Non-preferred Trajectories](https://arxiv.org/abs/2602.11018)
*Returaj Burnwal,Nirav Pravinbhai Bhatt,Balaraman Ravindran*

Main category: cs.LG

TL;DR: 提出OSIL算法，从离线非偏好轨迹中学习安全策略，无需显式安全成本或奖励标注


<details>
  <summary>Details</summary>
Motivation: 现实世界中在线学习存在风险，准确指定安全成本困难，但收集反映不安全行为的轨迹相对容易

Method: 将安全策略学习建模为约束马尔可夫决策过程，从非偏好演示中推导奖励最大化目标的下界并学习成本模型

Result: OSIL能学习满足成本约束的安全策略而不降低奖励性能，优于多个基线方法

Conclusion: 从离线非偏好演示中推断安全性的方法可行，能有效学习安全且奖励最大化的策略

Abstract: This work addresses the problem of offline safe imitation learning (IL), where the goal is to learn safe and reward-maximizing policies from demonstrations that do not have per-timestep safety cost or reward information. In many real-world domains, online learning in the environment can be risky, and specifying accurate safety costs can be difficult. However, it is often feasible to collect trajectories that reflect undesirable or unsafe behavior, implicitly conveying what the agent should avoid. We refer to these as non-preferred trajectories. We propose a novel offline safe IL algorithm, OSIL, that infers safety from non-preferred demonstrations. We formulate safe policy learning as a Constrained Markov Decision Process (CMDP). Instead of relying on explicit safety cost and reward annotations, OSIL reformulates the CMDP problem by deriving a lower bound on reward maximizing objective and learning a cost model that estimates the likelihood of non-preferred behavior. Our approach allows agents to learn safe and reward-maximizing behavior entirely from offline demonstrations. We empirically demonstrate that our approach can learn safer policies that satisfy cost constraints without degrading the reward performance, thus outperforming several baselines.

</details>


### [124] [When Fusion Helps and When It Breaks: View-Aligned Robustness in Same-Source Financial Imaging](https://arxiv.org/abs/2602.11020)
*Rui Ma*

Main category: cs.LG

TL;DR: 该研究探讨了金融图像表示中的同源多视图学习和对抗鲁棒性，用于预测次日黄金价格方向，发现结果高度依赖标签噪声机制，并揭示了数据-噪声权衡的非单调性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索金融时间序列预测中多视图学习方法的有效性，特别是针对上海黄金交易所现货黄金数据，同时评估这些方法在面对对抗性攻击时的鲁棒性。

Method: 方法包括：1) 从滚动窗口构建两个对齐视图（OHLCV价格/成交量图表和技术指标矩阵）；2) 使用防泄漏的时间块分割和Matthews相关系数进行评估；3) 应用事后最小变动过滤器减少标签模糊性；4) 比较早期融合（通道堆叠）和晚期融合（双编码器+融合头）；5) 使用FGSM和PGD进行L-infinity扰动测试，评估视图约束攻击和联合攻击。

Result: 结果显示：1) 预测性能强烈依赖标签噪声机制；2) 数据-噪声权衡呈现非单调性；3) 早期融合可能出现负迁移，而晚期融合在清洁性能上表现更佳；4) 跨视图一致性正则化有次要、骨干依赖效应；5) 模型在微小预算下对对抗攻击表现出严重脆弱性；6) 晚期融合在视图约束攻击下能提高鲁棒性，但联合攻击仍具挑战性。

Conclusion: 结论表明金融多视图学习需要仔细考虑标签噪声机制，晚期融合架构在清洁性能和对抗鲁棒性方面优于早期融合，但对抗性攻击特别是联合攻击仍然是实际部署中的重要挑战。

Abstract: We study same-source multi-view learning and adversarial robustness for next-day direction prediction with financial image representations. On Shanghai Gold Exchange (SGE) spot gold data (2005-2025), we construct two window-aligned views from each rolling window: an OHLCV-rendered price/volume chart and a technical-indicator matrix. To ensure reliable evaluation, we adopt leakage-resistant time-block splits with embargo and use Matthews correlation coefficient (MCC). We find that results depend strongly on the label-noise regime: we apply an ex-post minimum-movement filter that discards samples with realized next-day absolute return below tau to define evaluation subsets with reduced near-zero label ambiguity. This induces a non-monotonic data-noise trade-off that can reveal predictive signal but eventually increases variance as sample size shrinks; the filter is used for offline benchmark construction rather than an inference-time decision rule. In the stabilized subsets, fusion is regime dependent: early fusion by channel stacking can exhibit negative transfer, whereas late fusion with dual encoders and a fusion head provides the dominant clean-performance gains; cross-view consistency regularization has secondary, backbone-dependent effects. We further evaluate test-time L-infinity perturbations using FGSM and PGD under two threat scenarios: view-constrained attacks that perturb one view and joint attacks that perturb both. We observe severe vulnerability at tiny budgets with strong view asymmetry. Late fusion consistently improves robustness under view-constrained attacks, but joint attacks remain challenging and can still cause substantial worst-case degradation.

</details>


### [125] [Learning Page Order in Shuffled WOO Releases](https://arxiv.org/abs/2602.11040)
*Efe Kahraman,Giulio Tosato*

Main category: cs.LG

TL;DR: 研究使用页面嵌入对5,461份荷兰信息公开文件进行文档页面排序，比较了五种方法，发现最佳方法可成功重排最多15页的文档，但序列到序列Transformer在长文档上泛化失败，课程学习效果不佳。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决异构文档集合（如电子邮件、法律文本、电子表格等混合PDF）的页面排序问题，这些文档中语义排序信号不可靠，需要开发有效的排序方法。

Method: 使用页面嵌入技术，比较了五种方法：指针网络、序列到序列Transformer、专门的成对排序模型等。在5,461份荷兰信息公开文件上进行实验，分析不同方法的性能。

Result: 最佳方法成功重排最多15页的文档，Kendall's tau从短文档（2-5页）的0.95下降到15页文档的0.72。发现两个意外失败：序列到序列Transformer在长文档上泛化失败（tau从0.918降至0.014），课程学习比直接训练差39%。

Conclusion: 短文档和长文档需要根本不同的排序策略，这解释了课程学习失败的原因。模型专业化在长文档上取得了显著改进（+0.21 tau）。学习的位置编码是序列到序列失败的一个因素，但性能下降在所有编码变体中持续存在，表明有多个相互作用的原因。

Abstract: We investigate document page ordering on 5,461 shuffled WOO documents (Dutch freedom of information releases) using page embeddings. These documents are heterogeneous collections such as emails, legal texts, and spreadsheets compiled into single PDFs, where semantic ordering signals are unreliable. We compare five methods, including pointer networks, seq2seq transformers, and specialized pairwise ranking models. The best performing approach successfully reorders documents up to 15 pages, with Kendall's tau ranging from 0.95 for short documents (2-5 pages) to 0.72 for 15 page documents. We observe two unexpected failures: seq2seq transformers fail to generalize on long documents (Kendall's tau drops from 0.918 on 2-5 pages to 0.014 on 21-25 pages), and curriculum learning underperforms direct training by 39% on long documents. Ablation studies suggest learned positional encodings are one contributing factor to seq2seq failure, though the degradation persists across all encoding variants, indicating multiple interacting causes. Attention pattern analysis reveals that short and long documents require fundamentally different ordering strategies, explaining why curriculum learning fails. Model specialization achieves substantial improvements on longer documents (+0.21 tau).

</details>


### [126] [Divide, Harmonize, Then Conquer It: Shooting Multi-Commodity Flow Problems with Multimodal Language Models](https://arxiv.org/abs/2602.11057)
*Xinyu Yuan,Yan Qiao,Zonghui Wang,Wenzhi Chen*

Main category: cs.LG

TL;DR: Pram是首个利用多模态语言模型解决多商品流问题的ML方法，通过分治策略和强化学习实现快速高质量分配，性能接近最优解且运行速度提升1-2个数量级。


<details>
  <summary>Details</summary>
Motivation: 现有优化引擎在分配系统快速扩张的背景下难以平衡最优性和可处理性，服务提供商需要解决这种权衡困境的新方法。

Method: 将原问题分解为局部子问题，由MLM驱动的"智能体"解决，并通过多智能体强化学习算法协调子问题确保全局一致性。

Result: 在真实数据集和公共拓扑上，性能与线性规划求解器相当甚至超越（接近最优解），运行时间快1-2个数量级，在链路故障或流量突发下性能下降小于10%。

Conclusion: Pram证明了MLM在解决优化问题中的泛化能力，为目标无关的实用可扩展解决方案，可无缝集成到主流分配系统中。

Abstract: The multi-commodity flow (MCF) problem is a fundamental topic in network flow and combinatorial optimization, with broad applications in transportation, communication, and logistics, etc. Nowadays, the rapid expansion of allocation systems has posed challenges for existing optimization engines in balancing optimality and tractability. In this paper, we present Pram, the first ML-based method that leverages the reasoning power of multimodal language models (MLMs) for addressing the trade-off dilemma -- a great need of service providers. As part of our proposal, Pram (i) quickly computes high-quality allocations by dividing the original problem into local subproblems, which are then resolved by an MLM-powered "agent", and (ii) ensures global consistency by harmonizing these subproblems via a multi-agent reinforcement learning algorithm. Theoretically, we show that Pram, which learns to perform gradient descent in context, provably converges to the optimum within the family of MCF problems. Empirically, on real-world datasets and public topologies, Pram achieves performance comparable to, and in some cases even surpassing, linear programming solvers (very close to the optimal solution), and substantially lower runtimes (1 to 2 orders of magnitude faster). Moreover, Pram exhibits strong robustness (<10\% performance degradation under link failures or flow bursts), demonstrating MLM's generalization ability to unforeseen events. Pram is objective-agnostic and seamlessly integrates with mainstream allocation systems, providing a practical and scalable solution for future networks.

</details>


### [127] [MoToRec: Sparse-Regularized Multimodal Tokenization for Cold-Start Recommendation](https://arxiv.org/abs/2602.11062)
*Jialin Liu,Zhaorui Zhang,Ray C. C. Cheung*

Main category: cs.LG

TL;DR: MoToRec是一个基于稀疏正则化残差量化变分自编码器的多模态推荐框架，通过离散语义标记化解决冷启动问题，在三个大规模数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 图神经网络虽然改进了推荐系统，但数据稀疏性和物品冷启动问题（特别是新物品缺乏交互历史）严重影响了性能。多模态内容提供了解决方案，但现有方法由于稀疏数据中的噪声和纠缠问题，对新物品产生次优表示。

Method: 将多模态推荐转化为离散语义标记化，提出MoToRec框架：1) 稀疏正则化残差量化变分自编码器生成离散可解释标记的语义代码；2) 自适应稀有性放大促进冷启动物品的优先学习；3) 分层多源图编码器实现与协同信号的鲁棒融合。

Result: 在三个大规模数据集上的广泛实验表明，MoToRec在整体和冷启动场景中都优于最先进的方法。

Conclusion: 离散标记化为缓解长期存在的冷启动挑战提供了有效且可扩展的替代方案。

Abstract: Graph neural networks (GNNs) have revolutionized recommender systems by effectively modeling complex user-item interactions, yet data sparsity and the item cold-start problem significantly impair performance, particularly for new items with limited or no interaction history. While multimodal content offers a promising solution, existing methods result in suboptimal representations for new items due to noise and entanglement in sparse data. To address this, we transform multimodal recommendation into discrete semantic tokenization. We present Sparse-Regularized Multimodal Tokenization for Cold-Start Recommendation (MoToRec), a framework centered on a sparsely-regularized Residual Quantized Variational Autoencoder (RQ-VAE) that generates a compositional semantic code of discrete, interpretable tokens, promoting disentangled representations. MoToRec's architecture is enhanced by three synergistic components: (1) a sparsely-regularized RQ-VAE that promotes disentangled representations, (2) a novel adaptive rarity amplification that promotes prioritized learning for cold-start items, and (3) a hierarchical multi-source graph encoder for robust signal fusion with collaborative signals. Extensive experiments on three large-scale datasets demonstrate MoToRec's superiority over state-of-the-art methods in both overall and cold-start scenarios. Our work validates that discrete tokenization provides an effective and scalable alternative for mitigating the long-standing cold-start challenge.

</details>


### [128] [Motion Capture is Not the Target Domain: Scaling Synthetic Data for Learning Motion Representations](https://arxiv.org/abs/2602.11064)
*Firas Darwish,George Nicholson,Aiden Doherty,Hang Yuan*

Main category: cs.LG

TL;DR: 研究合成数据在人体运动识别中的迁移学习效果，发现合成数据与真实数据混合或足够规模时能提升泛化能力，但大规模运动捕捉预训练因领域不匹配而收益有限。


<details>
  <summary>Details</summary>
Motivation: 当真实世界数据稀缺时，合成数据为可扩展的预训练提供了有吸引力的途径，但仅使用合成数据预训练的模型在部署环境中往往无法可靠迁移。本研究在全身人体运动识别领域探索这一问题，该领域大规模数据收集困难但对可穿戴设备的人体活动识别至关重要。

Method: 使用从运动捕捉数据衍生的表示生成合成运动数据，预训练运动时间序列模型，然后在多样化的下游人体活动识别任务上评估其迁移能力。

Result: 合成数据预训练在与真实数据混合或规模足够大时能改善泛化性能；大规模运动捕捉预训练由于与可穿戴信号存在领域不匹配，仅带来边际收益。

Conclusion: 阐明了合成到真实迁移的关键挑战，以及合成运动数据在可迁移的人体活动识别表示方面的局限性和机会，为合成数据在运动识别领域的应用提供了指导。

Abstract: Synthetic data offers a compelling path to scalable pretraining when real-world data is scarce, but models pretrained on synthetic data often fail to transfer reliably to deployment settings. We study this problem in full-body human motion, where large-scale data collection is infeasible but essential for wearable-based Human Activity Recognition (HAR), and where synthetic motion can be generated from motion-capture-derived representations. We pretrain motion time-series models using such synthetic data and evaluate their transfer across diverse downstream HAR tasks. Our results show that synthetic pretraining improves generalisation when mixed with real data or scaled sufficiently. We also demonstrate that large-scale motion-capture pretraining yields only marginal gains due to domain mismatch with wearable signals, clarifying key sim-to-real challenges and the limits and opportunities of synthetic motion data for transferable HAR representations.

</details>


### [129] [In-the-Wild Model Organisms: Mitigating Undesirable Emergent Behaviors in Production LLM Post-Training via Data Attribution](https://arxiv.org/abs/2602.11079)
*Frank Xiao,Santiago Aranguri*

Main category: cs.LG

TL;DR: 提出基于激活的数据归因方法，通过计算测试提示和偏好对的激活差异向量，按余弦相似度排序，识别导致特定行为的训练数据点，并通过修改数据重新训练进行因果验证。


<details>
  <summary>Details</summary>
Motivation: 需要追踪后训练语言模型行为变化到具体训练数据点，特别是在现实场景中识别有害行为（如分散注意力触发的顺从行为）的来源，为安全技术提供真实基准。

Method: 基于激活的数据归因方法：计算测试提示和偏好对的激活差异向量，通过余弦相似度排序识别责任数据点；聚类行为-数据点相似度矩阵实现无监督行为发现；应用于OLMo 2的DPO训练。

Result: 在OLMo 2的DPO训练中发现了分散注意力触发的顺从行为：模型在附加良性格式指令时会顺从危险请求；过滤排名靠前的数据点可将该行为减少63%，切换标签可达78%；方法优于基于梯度的归因和LLM-judge基线，成本降低10倍以上。

Conclusion: 该方法能有效识别导致特定行为的训练数据点，为语言模型安全提供实用的诊断工具；发现的"模型生物"（来自污染偏好数据而非故意注入）为安全技术提供了现实基准。

Abstract: We propose activation-based data attribution, a method that traces behavioral changes in post-trained language models to responsible training datapoints. By computing activation-difference vectors for both test prompts and preference pairs and ranking by cosine similarity, we identify datapoints that cause specific behaviors and validate these attributions causally by retraining with modified data. Clustering behavior-datapoint similarity matrices also enables unsupervised discovery of emergent behaviors. Applying this to OLMo 2's production DPO training, we surfaced distractor-triggered compliance: a harmful behavior where the model complies with dangerous requests when benign formatting instructions are appended. Filtering top-ranked datapoints reduces this behavior by 63% while switching their labels achieves 78%. Our method outperforms gradient-based attribution and LLM-judge baselines while being over 10 times cheaper than both. This in-the-wild model organism - emerging from contaminated preference data rather than deliberate injection - provides a realistic benchmark for safety techniques.

</details>


### [130] [Token-Efficient Change Detection in LLM APIs](https://arxiv.org/abs/2602.11083)
*Timothée Chauvin,Clément Lalanne,Erwan Le Merrer,Jean-Michel Loubes,François Taïani,Gilles Tredan*

Main category: cs.LG

TL;DR: 提出B3IT方法，通过边界输入在严格黑盒设置下低成本检测LLM变化，性能媲美灰盒方法，成本降低30倍


<details>
  <summary>Details</summary>
Motivation: 现有LLM远程变化检测方法要么成本过高难以大规模部署，要么需要白盒（模型权重）或灰盒（对数概率）访问权限。需要实现低成本且严格黑盒（仅观察输出token）的操作

Method: 提出黑盒边界输入跟踪(B3IT)方案，利用边界输入（存在多个输出top token的特定输入）。从统计角度分析，最优变化检测取决于模型的Jacobian和输出分布的Fisher信息，在低温机制下边界输入能实现强大的变化检测测试

Result: 边界输入在非推理测试端点上容易找到，性能与最佳灰盒方法相当。B3IT将成本降低30倍，同时在严格黑盒设置下运行

Conclusion: B3IT方法成功解决了LLM远程变化检测的成本和访问权限问题，通过边界输入实现了高效的黑盒检测，为大规模部署提供了可行方案

Abstract: Remote change detection in LLMs is a difficult problem. Existing methods are either too expensive for deployment at scale, or require initial white-box access to model weights or grey-box access to log probabilities. We aim to achieve both low cost and strict black-box operation, observing only output tokens.
  Our approach hinges on specific inputs we call Border Inputs, for which there exists more than one output top token. From a statistical perspective, optimal change detection depends on the model's Jacobian and the Fisher information of the output distribution. Analyzing these quantities in low-temperature regimes shows that border inputs enable powerful change detection tests.
  Building on this insight, we propose the Black-Box Border Input Tracking (B3IT) scheme. Extensive in-vivo and in-vitro experiments show that border inputs are easily found for non-reasoning tested endpoints, and achieve performance on par with the best available grey-box approaches. B3IT reduces costs by $30\times$ compared to existing methods, while operating in a strict black-box setting.

</details>


### [131] [GRASP: group-Shapley feature selection for patients](https://arxiv.org/abs/2602.11084)
*Yuheng Luo,Shuyan Li,Zhong Cao*

Main category: cs.LG

TL;DR: GRASP结合Shapley值和组L21正则化，从医学预测中提取紧凑、非冗余的特征集，比LASSO等方法更稳健可解释


<details>
  <summary>Details</summary>
Motivation: 医学预测中的特征选择面临挑战，现有方法如LASSO缺乏稳健性和可解释性，需要更有效的特征选择框架

Method: GRASP框架：1) 通过SHAP从预训练树模型中提取组级重要性分数；2) 通过组L21正则化的逻辑回归强制结构化稀疏性，获得稳定可解释的特征选择

Result: 与LASSO、SHAP和深度学习方法相比，GRASP在保持相当或更优预测准确性的同时，识别出更少、冗余度更低、更稳定的特征

Conclusion: GRASP为医学预测提供了一种新颖的特征选择框架，能够提取紧凑、非冗余的特征集，具有更好的稳健性和可解释性

Abstract: Feature selection remains a major challenge in medical prediction, where existing approaches such as LASSO often lack robustness and interpretability. We introduce GRASP, a novel framework that couples Shapley value driven attribution with group $L_{21}$ regularization to extract compact and non-redundant feature sets. GRASP first distills group level importance scores from a pretrained tree model via SHAP, then enforces structured sparsity through group $L_{21}$ regularized logistic regression, yielding stable and interpretable selections. Extensive comparisons with LASSO, SHAP, and deep learning based methods show that GRASP consistently delivers comparable or superior predictive accuracy, while identifying fewer, less redundant, and more stable features.

</details>


### [132] [General Flexible $f$-divergence for Challenging Offline RL Datasets with Low Stochasticity and Diverse Behavior Policies](https://arxiv.org/abs/2602.11087)
*Jianxun Wang,Grant C. Forbes,Leonardo Villalobos-Arias,David L. Roberts*

Main category: cs.LG

TL;DR: 提出一种基于灵活f-散度的离线RL方法，通过自适应约束平衡RL目标和行为策略约束，在有限探索和多样行为策略的数据集上提升性能


<details>
  <summary>Details</summary>
Motivation: 实际离线数据集通常存在探索有限、多样性不足以及来自多个不同水平行为策略的问题。有限探索会影响Q/V值估计，而过度约束于多样行为策略又会导致过于保守。需要在RL目标和行为策略约束之间找到平衡。

Method: 首先建立f-散度与贝尔曼残差优化约束之间的联系，通过更一般的线性规划形式和凸共轭。然后引入灵活f-散度的一般函数形式，基于离线训练数据集对算法学习目标施加自适应约束。

Result: 在MuJoCo、Fetch和AdroitHand环境上的实验结果表明，提出的线性规划形式正确，且灵活f-散度在应用于兼容的约束优化算法时，能够从具有挑战性的数据集中学习并提升性能。

Conclusion: 通过灵活f-散度框架，可以在离线RL中实现自适应约束，有效平衡探索利用和保守性，在复杂现实数据集中取得更好的学习效果。

Abstract: Offline RL algorithms aim to improve upon the behavior policy that produces the collected data while constraining the learned policy to be within the support of the dataset. However, practical offline datasets often contain examples with little diversity or limited exploration of the environment, and from multiple behavior policies with diverse expertise levels. Limited exploration can impair the offline RL algorithm's ability to estimate \textit{Q} or \textit{V} values, while constraining towards diverse behavior policies can be overly conservative. Such datasets call for a balance between the RL objective and behavior policy constraints. We first identify the connection between $f$-divergence and optimization constraint on the Bellman residual through a more general Linear Programming form for RL and the convex conjugate. Following this, we introduce the general flexible function formulation for the $f$-divergence to incorporate an adaptive constraint on algorithms' learning objectives based on the offline training dataset. Results from experiments on the MuJoCo, Fetch, and AdroitHand environments show the correctness of the proposed LP form and the potential of the flexible $f$-divergence in improving performance for learning from a challenging dataset when applied to a compatible constrained optimization algorithm.

</details>


### [133] [Direct Learning of Calibration-Aware Uncertainty for Neural PDE Surrogates](https://arxiv.org/abs/2602.11090)
*Carlos Stein Brito*

Main category: cs.LG

TL;DR: 提出Cross-regularized uncertainty框架，通过正则化分割学习不确定性参数，无需针对不同数据体制调整噪声，在Fourier Neural Operators中实现，在APEBench上验证了更好的校准性和不确定性定位能力。


<details>
  <summary>Details</summary>
Motivation: 神经PDE代理模型常在数据有限或部分观测的场景中部署，下游决策不仅需要低预测误差，还需要校准良好的不确定性。现有方法通过集成复制、固定随机噪声（如dropout）或事后校准获得不确定性，但这些方法不够灵活。

Method: 提出Cross-regularized uncertainty框架：使用正则化分割通过梯度学习不确定性参数。预测器在训练分割上优化拟合，而低维不确定性控制在正则化分割上优化以减少训练-测试不匹配，实现体制自适应不确定性而无需针对每个体制调整噪声。框架可学习输出头、隐藏特征或算子特定组件（如谱模式）中的连续噪声水平。在Fourier Neural Operators中实例化该方法。

Result: 在APEBench上对观测比例和训练集大小进行扫描评估。在这些扫描中，学习到的预测分布在保留分割上具有更好的校准性，得到的不确定性场在一阶空间诊断中集中在高误差区域。

Conclusion: Cross-regularized uncertainty框架能够学习体制自适应不确定性，无需针对每个数据体制调整噪声参数，在神经PDE代理模型中实现了更好的不确定性校准和定位能力。

Abstract: Neural PDE surrogates are often deployed in data-limited or partially observed regimes where downstream decisions depend on calibrated uncertainty in addition to low prediction error. Existing approaches obtain uncertainty through ensemble replication, fixed stochastic noise such as dropout, or post hoc calibration. Cross-regularized uncertainty learns uncertainty parameters during training using gradients routed through a held-out regularization split. The predictor is optimized on the training split for fit, while low-dimensional uncertainty controls are optimized on the regularization split to reduce train-test mismatch, yielding regime-adaptive uncertainty without per-regime noise tuning. The framework can learn continuous noise levels at the output head, within hidden features, or within operator-specific components such as spectral modes. We instantiate the approach in Fourier Neural Operators and evaluate on APEBench sweeps over observed fraction and training-set size. Across these sweeps, the learned predictive distributions are better calibrated on held-out splits and the resulting uncertainty fields concentrate in high-error regions in one-step spatial diagnostics.

</details>


### [134] [From Natural Language to Materials Discovery:The Materials Knowledge Navigation Agent](https://arxiv.org/abs/2602.11123)
*Genmao Zhuang,Amir Barati Farimani*

Main category: cs.LG

TL;DR: MKNA是一个语言驱动的材料发现系统，能够将自然语言科学意图转化为可执行操作，用于数据库检索、性质预测、结构生成和稳定性评估，在寻找高德拜温度陶瓷材料方面取得了成功。


<details>
  <summary>Details</summary>
Motivation: 传统材料发现工作流程严重依赖专家直觉和计算密集型模拟，限制了高性能材料的发现速度。需要一种能够自动化材料探索过程的新方法。

Method: 开发了材料知识导航代理（MKNA），这是一个语言驱动系统，能够将自然语言科学意图转化为可执行操作，包括数据库检索、性质预测、结构生成和稳定性评估。系统能够从文献和数据库证据中自主提取定量阈值和化学上有意义的设计模式。

Result: 在寻找高德拜温度陶瓷材料的应用中，MKNA识别出文献支持的筛选标准（德拜温度>800K），重新发现了金刚石、SiC、SiN、BeO等经典超硬材料，并提出了热力学稳定、先前未报道的富Be-C化合物，填补了1500-1700K温度区间的空白。

Conclusion: MKNA不仅能够找到稳定候选材料，还能重建可解释的设计启发式方法，为自主、语言引导的材料探索建立了一个可推广的平台。

Abstract: Accelerating the discovery of high-performance materials remains a central challenge across energy, electronics, and aerospace technologies, where traditional workflows depend heavily on expert intuition and computationally expensive simulations. Here we introduce the Materials Knowledge Navigation Agent (MKNA), a language-driven system that translates natural-language scientific intent into executable actions for database retrieval, property prediction, structure generation, and stability evaluation. Beyond automating tool invocation, MKNA autonomously extracts quantitative thresholds and chemically meaningful design motifs from literature and database evidence, enabling data-grounded hypothesis formation. Applied to the search for high-Debye-temperature ceramics, the agent identifies a literature-supported screening criterion (Theta_D > 800 K), rediscovers canonical ultra-stiff materials such as diamond, SiC, SiN, and BeO, and proposes thermodynamically stable, previously unreported Be-C-rich compounds that populate the sparsely explored 1500-1700 K regime. These results demonstrate that MKNA not only finds stable candidates but also reconstructs interpretable design heuristics, establishing a generalizable platform for autonomous, language-guided materials exploration.

</details>


### [135] [The Offline-Frontier Shift: Diagnosing Distributional Limits in Generative Multi-Objective Optimization](https://arxiv.org/abs/2602.11126)
*Stephanie Holly,Alexandru-Ciprian Zăvoianu,Siegfried Silber,Sepp Hochreiter,Werner Zellinger*

Main category: cs.LG

TL;DR: 生成方法在离线多目标优化中表现不佳，特别是在世代距离等指标上，主要受限于离线前沿偏移问题


<details>
  <summary>Details</summary>
Motivation: 研究生成方法（如扩散模型）在离线多目标优化中的表现，发现它们在超体积指标上表现良好，但在其他指标上不如进化方法

Method: 分析离线前沿偏移问题，提出通过积分概率度量在目标空间进行分布外采样来克服这一限制

Result: 生成方法在离线多目标优化中系统性表现不如进化方法，特别是在世代距离等指标上，且生成方法过于保守地接近离线目标分布

Conclusion: 离线多目标优化本质上是分布偏移受限问题，生成方法失败的原因在于无法有效进行分布外采样，这为理解生成优化方法的局限性提供了诊断视角

Abstract: Offline multi-objective optimization (MOO) aims to recover Pareto-optimal designs given a finite, static dataset. Recent generative approaches, including diffusion models, show strong performance under hypervolume, yet their behavior under other established MOO metrics is less understood. We show that generative methods systematically underperform evolutionary alternatives with respect to other metrics, such as generational distance. We relate this failure mode to the offline-frontier shift, i.e., the displacement of the offline dataset from the Pareto front, which acts as a fundamental limitation in offline MOO. We argue that overcoming this limitation requires out-of-distribution sampling in objective space (via an integral probability metric) and empirically observe that generative methods remain conservatively close to the offline objective distribution. Our results position offline MOO as a distribution-shift--limited problem and provide a diagnostic lens for understanding when and why generative optimization methods fail.

</details>


### [136] [Asymmetric Prompt Weighting for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.11128)
*Reinhard Heckel,Mahdi Soltanolkotabi,Christos Thramboulidis*

Main category: cs.LG

TL;DR: 本文提出在强化学习中采用非对称提示权重分配，特别关注低成功概率的提示，以加速从零开始的RL训练收敛。


<details>
  <summary>Details</summary>
Motivation: 现有RL算法（如GRPO、DAPO、RLOO）主要关注中等成功概率的提示，而降低对非常容易和非常困难提示的梯度权重。本文旨在探索非对称权重分配，特别是给低成功概率提示分配更高权重，以改善训练效率。

Method: 提出非对称提示权重分配方法，特别关注低（甚至零）经验成功概率的提示。通过理论分析确定最小化从初始成功率提升到目标准确率所需时间的优化权重，在低成功率区域采用非对称权重分配。

Result: 发现非对称权重分配特别有利于从零开始的RL训练（如R1-Zero），能加速有效时间收敛；而在后SFT RL中，由于模型已具有高准确率，效果相对较小。理论分析表明在低成功率区域，优化权重变得非对称，上加权低成功概率提示。

Conclusion: 非对称提示权重分配策略，特别是在低成功率区域上加权低成功概率提示，能显著提高从零开始RL训练的效率，加速收敛过程，尤其是在信息性响应稀少且响应成本占主导的情况下。

Abstract: Reinforcement learning with verifiable rewards has driven recent advances in LLM post-training, in particular for reasoning. Policy optimization algorithms generate a number of responses for a given prompt and then effectively weight the corresponding gradients depending on the rewards. The most popular algorithms including GRPO, DAPO, and RLOO focus on ambiguous prompts, i.e., prompts with intermediate success probability, while downgrading gradients with very easy and very hard prompts. In this paper, we consider asymmetric prompt weightings that assign higher weights to prompts with low, or even zero, empirical success probability. We find that asymmetric weighting particularly benefits from-scratch RL (as in R1-Zero), where training traverses a wide accuracy range, and less so in post-SFT RL where the model already starts at high accuracy. We also provide theory that characterizes prompt weights which minimize the time needed to raise success probability from an initial level to a target accuracy under a fixed update budget. In low-success regimes, where informative responses are rare and response cost dominates, these optimal weights become asymmetric, upweighting low success probabilities and thereby accelerating effective-time convergence.

</details>


### [137] [From Circuits to Dynamics: Understanding and Stabilizing Failure in 3D Diffusion Transformers](https://arxiv.org/abs/2602.11130)
*Maximilian Plattner,Fabian Paischer,Johannes Brandstetter,Arturs Berzins*

Main category: cs.LG

TL;DR: 研究发现3D扩散变换器在稀疏点云补全中存在灾难性故障模式"Meltdown"：输入点云的微小扰动会导致输出分裂成多个不连续片段。通过机制解释性分析定位到早期去噪交叉注意力激活，并提出PowerRemap测试时控制方法来稳定稀疏点云条件。


<details>
  <summary>Details</summary>
Motivation: 稀疏点云的可靠表面补全对内容创建和机器人应用至关重要。虽然3D扩散变换器在该任务上达到最先进水平，但研究者发现它们存在灾难性故障模式：输入点云的任意微小扰动会导致输出分裂成多个不连续片段，这种现象被称为"Meltdown"。

Method: 使用机制解释性中的激活修补技术，将Meltdown定位到单个早期去噪交叉注意力激活。分析该激活的奇异值谱，发现其谱熵在分裂发生时上升，修补后恢复基线。基于此提出PowerRemap测试时控制方法，通过调整注意力激活来稳定稀疏点云条件。

Result: Meltdown现象在最先进的架构（WaLa、Make-a-Shape）、数据集（GSO、SimJEB）和去噪策略（DDPM、DDIM）中普遍存在。PowerRemap能有效对抗这种故障，稳定率高达98.3%。

Conclusion: 这项工作展示了如何基于机制分析理解和引导扩散模型行为，将电路级交叉注意力机制与扩散动力学中的轨迹分岔理论联系起来，为扩散模型的可靠性提供了新的理解和控制方法。

Abstract: Reliable surface completion from sparse point clouds underpins many applications spanning content creation and robotics. While 3D diffusion transformers attain state-of-the-art results on this task, we uncover that they exhibit a catastrophic mode of failure: arbitrarily small on-surface perturbations to the input point cloud can fracture the output into multiple disconnected pieces -- a phenomenon we call Meltdown. Using activation-patching from mechanistic interpretability, we localize Meltdown to a single early denoising cross-attention activation. We find that the singular-value spectrum of this activation provides a scalar proxy: its spectral entropy rises when fragmentation occurs and returns to baseline when patched. Interpreted through diffusion dynamics, we show that this proxy tracks a symmetry-breaking bifurcation of the reverse process. Guided by this insight, we introduce PowerRemap, a test-time control that stabilizes sparse point-cloud conditioning. We demonstrate that Meltdown persists across state-of-the-art architectures (WaLa, Make-a-Shape), datasets (GSO, SimJEB) and denoising strategies (DDPM, DDIM), and that PowerRemap effectively counters this failure with stabilization rates of up to 98.3%. Overall, this work is a case study on how diffusion model behavior can be understood and guided based on mechanistic analysis, linking a circuit-level cross-attention mechanism to diffusion-dynamics accounts of trajectory bifurcations.

</details>


### [138] [Just on Time: Token-Level Early Stopping for Diffusion Language Models](https://arxiv.org/abs/2602.11133)
*Zahar Kohut,Severyn Shykula,Dmytro Khamula,Mykola Vysotskyi,Taras Rumezhak,Volodymyr Karpiv*

Main category: cs.LG

TL;DR: 提出一种无需训练的token级早停方法，通过轻量级信号动态判断每个token何时收敛，显著减少扩散步骤同时保持生成质量


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型通过迭代细化生成文本，但计算效率低下，因为许多token在最终去噪步骤之前很早就达到稳定状态

Method: 训练无关的token级早停方法，利用模型预测和局部上下文的轻量级信号，独立识别每个位置的收敛情况，实现自适应token冻结

Result: 在数学推理、通用问答和科学理解等多样化基准测试中，实现了最先进的效率提升，同时保持了生成质量

Conclusion: 该方法通过动态确定token收敛时机，显著提高了扩散语言模型的推理效率，无需任务特定微调

Abstract: Diffusion language models generate text through iterative refinement, a process that is often computationally inefficient because many tokens reach stability long before the final denoising step. We introduce a training-free, token-level early stopping approach that identifies convergence independently at each position. Our method leverages lightweight signals derived from the model's predictions and local context to dynamically determine when individual tokens can be finalized. This yields adaptive per-token freezing without task-specific fine-tuning, substantially reducing the total number of diffusion steps required. Across diverse benchmarks, spanning mathematical reasoning, general question answering, and scientific understanding, our approach achieves state-of-the-art efficiency gains while preserving generation quality.

</details>


### [139] [Weight Decay Improves Language Model Plasticity](https://arxiv.org/abs/2602.11137)
*Tessa Han,Sebastian Bordt,Hanlin Zhang,Sham Kakade*

Main category: cs.LG

TL;DR: 研究发现：预训练时使用更大的权重衰减值能增强模型的可塑性，使模型在微调下游任务时表现更好，即使基础模型性能较差


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型开发主要关注基础模型的验证损失，忽略了模型在下游任务中的适应能力。研究者希望从模型可塑性角度研究预训练，即基础模型通过微调成功适应下游任务的能力

Method: 系统研究权重衰减在预训练中的作用，通过实验分析不同权重衰减值对模型可塑性的影响，并深入探讨权重衰减对模型行为的机制性影响

Result: 使用更大权重衰减值训练的模型更具可塑性，微调后在下游任务中表现更好；权重衰减促进线性可分表示、正则化注意力矩阵、减少训练数据过拟合

Conclusion: 超参数优化不应仅基于交叉熵损失，应考虑模型可塑性等更全面的评估指标；单个优化超参数在塑造模型行为中扮演多方面的角色

Abstract: The prevailing paradigm in large language model (LLM) development is to pretrain a base model, then perform further training to improve performance and model behavior. However, hyperparameter optimization and scaling laws have been studied primarily from the perspective of the base model's validation loss, ignoring downstream adaptability. In this work, we study pretraining from the perspective of model plasticity, that is, the ability of the base model to successfully adapt to downstream tasks through fine-tuning. We focus on the role of weight decay, a key regularization parameter during pretraining. Through systematic experiments, we show that models trained with larger weight decay values are more plastic, meaning they show larger performance gains when fine-tuned on downstream tasks. This phenomenon can lead to counterintuitive trade-offs where base models that perform worse after pretraining can perform better after fine-tuning. Further investigation of weight decay's mechanistic effects on model behavior reveals that it encourages linearly separable representations, regularizes attention matrices, and reduces overfitting on the training data. In conclusion, this work demonstrates the importance of using evaluation metrics beyond cross-entropy loss for hyperparameter optimization and casts light on the multifaceted role of that a single optimization hyperparameter plays in shaping model behavior.

</details>


### [140] [TabICLv2: A better, faster, scalable, and open tabular foundation model](https://arxiv.org/abs/2602.11139)
*Jingang Qu,David Holzmüller,Gaël Varoquaux,Marine Le Morvan*

Main category: cs.LG

TL;DR: TabICLv2是一种新的表格数据基础模型，在回归和分类任务上达到SOTA，通过创新的合成数据生成、架构改进和优化训练协议，在多个基准测试中超越现有最佳模型。


<details>
  <summary>Details</summary>
Motivation: 表格基础模型（如TabPFNv2和TabICL）已在预测基准测试中超越梯度提升树，展示了上下文学习在表格数据中的价值。研究者希望进一步提升性能，开发更强大的表格基础模型。

Method: TabICLv2基于三个支柱：(1) 新颖的合成数据生成引擎，旨在提高预训练多样性；(2) 架构创新，包括新的可扩展注意力softmax，无需大量长序列预训练即可泛化到更大数据集；(3) 优化的预训练协议，用Muon优化器替代AdamW。

Result: 在TabArena和TALENT基准测试中，未经调优的TabICLv2超越了当前SOTA模型RealTabPFN-2.5（经过超参数调优、集成和真实数据微调）。模型在50GB GPU内存下有效泛化到百万规模数据集，且速度明显快于RealTabPFN-2.5。

Conclusion: TabICLv2通过创新的合成数据生成、架构改进和优化训练协议，在表格数据预测任务上实现了新的SOTA性能，展示了上下文学习在表格数据中的巨大潜力，并承诺开源研究。

Abstract: Tabular foundation models, such as TabPFNv2 and TabICL, have recently dethroned gradient-boosted trees at the top of predictive benchmarks, demonstrating the value of in-context learning for tabular data. We introduce TabICLv2, a new state-of-the-art foundation model for regression and classification built on three pillars: (1) a novel synthetic data generation engine designed for high pretraining diversity; (2) various architectural innovations, including a new scalable softmax in attention improving generalization to larger datasets without prohibitive long-sequence pretraining; and (3) optimized pretraining protocols, notably replacing AdamW with the Muon optimizer. On the TabArena and TALENT benchmarks, TabICLv2 without any tuning surpasses the performance of the current state of the art, RealTabPFN-2.5 (hyperparameter-tuned, ensembled, and fine-tuned on real data). With only moderate pretraining compute, TabICLv2 generalizes effectively to million-scale datasets under 50GB GPU memory while being markedly faster than RealTabPFN-2.5. We provide extensive ablation studies to quantify these contributions and commit to open research by first releasing inference code and model weights at https://github.com/soda-inria/tabicl, with synthetic data engine and pretraining code to follow.

</details>


### [141] [GENIUS: Generative Fluid Intelligence Evaluation Suite](https://arxiv.org/abs/2602.11144)
*Ruichuan An,Sihan Yang,Ziyu Guo,Wei Dai,Zijun Shen,Haodong Li,Renrui Zhang,Xinyu Wei,Guopeng Li,Wenshan Wu,Wentao Zhang*

Main category: cs.LG

TL;DR: GENIUS是一个评估多模态模型生成性流体智能(GFI)的基准，关注模型在即时上下文中归纳模式、执行约束和适应新情境的能力，而非依赖已有知识。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估模型的结晶智能（依赖积累的知识和模式），而忽视了生成性流体智能——在即时情境中归纳模式、推理约束和适应新场景的能力。需要建立专门评估GFI的基准来推动多模态模型向动态通用推理发展。

Method: 提出GENIUS评估套件，将GFI形式化为三个基本能力：归纳隐含模式、执行临时约束、适应上下文知识。系统评估了12个代表性模型，并进行诊断分析，提出了无需训练的注意力干预策略来弥补模型在上下文理解上的不足。

Result: 对12个代表性模型的评估显示，在GFI任务上存在显著的性能缺陷。诊断分析表明，这些失败主要源于有限的上下文理解能力，而非内在生成能力不足。提出的注意力干预策略有助于改善模型表现。

Conclusion: GENIUS为评估生成性流体智能建立了严格标准，引导领域从知识利用转向动态通用推理。该基准揭示了当前多模态模型在即时情境推理上的局限性，并提供了改进方向。

Abstract: Unified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess $\textit{Crystallized Intelligence}$, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks $\textit{Generative Fluid Intelligence (GFI)}$: the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on the fly. To rigorously assess this capability, we introduce $\textbf{GENIUS}$ ($\textbf{GEN}$ Fluid $\textbf{I}$ntelligence Eval$\textbf{U}$ation $\textbf{S}$uite). We formalize $\textit{GFI}$ as a synthesis of three primitives. These include $\textit{Inducing Implicit Patterns}$ (e.g., inferring personalized visual preferences), $\textit{Executing Ad-hoc Constraints}$ (e.g., visualizing abstract metaphors), and $\textit{Adapting to Contextual Knowledge}$ (e.g., simulating counter-intuitive physics). Collectively, these primitives challenge models to solve problems grounded entirely in the immediate context. Our systematic evaluation of 12 representative models reveals significant performance deficits in these tasks. Crucially, our diagnostic analysis disentangles these failure modes. It demonstrates that deficits stem from limited context comprehension rather than insufficient intrinsic generative capability. To bridge this gap, we propose a training-free attention intervention strategy. Ultimately, $\textbf{GENIUS}$ establishes a rigorous standard for $\textit{GFI}$, guiding the field beyond knowledge utilization toward dynamic, general-purpose reasoning. Our dataset and code will be released at: $\href{https://github.com/arctanxarc/GENIUS}{https://github.com/arctanxarc/GENIUS}$.

</details>


### [142] [Diffusion-Pretrained Dense and Contextual Embeddings](https://arxiv.org/abs/2602.11151)
*Sedigheh Eslami,Maksim Gaiduk,Markus Krimmel,Louis Milliken,Bo Wang,Denis Bykov*

Main category: cs.LG

TL;DR: pplx-embed是一个多语言嵌入模型系列，采用扩散预训练语言模型作为骨干，通过多阶段对比学习实现网络规模检索，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 开发能够有效处理大规模网络检索的多语言嵌入模型，特别是在真实生产环境中需要高质量和高效检索的场景。

Method: 使用扩散预训练语言模型作为骨干，采用多阶段对比学习，利用双向注意力机制捕获完整上下文，通过均值池化和后期分块策略保留长文档的全局上下文。

Result: pplx-embed-v1在MTEB(多语言v2)、MTEB(代码)、MIRACL、BERGEN和ToolRet检索基准上表现优异；pplx-embed-context-v1在ConTEB基准上创下新记录；在内部大规模搜索评估中也表现强劲。

Conclusion: pplx-embed模型系列通过扩散预训练和多阶段对比学习，在多个公开基准和真实生产环境中都表现出色，验证了其在大规模检索场景中的有效性和实用性。

Abstract: In this report, we introduce pplx-embed, a family of multilingual embedding models that employ multi-stage contrastive learning on a diffusion-pretrained language model backbone for web-scale retrieval. By leveraging bidirectional attention through diffusion-based pretraining, our models capture comprehensive bidirectional context within passages, enabling the use of mean pooling and a late chunking strategy to better preserve global context across long documents. We release two model types: pplx-embed-v1 for standard retrieval, and pplx-embed-context-v1 for contextualized embeddings that incorporate global document context into passage representations. pplx-embed-v1 achieves competitive performance on the MTEB(Multilingual, v2), MTEB(Code), MIRACL, BERGEN, and ToolRet retrieval benchmarks, while pplx-embed-context-v1 sets new records on the ConTEB benchmark. Beyond public benchmarks, pplx-embed-v1 demonstrates strong performance on our internal evaluation suite, which focuses on real-world, large-scale search scenarios over tens of millions of documents. These results validate the models' effectiveness in production environments where retrieval quality and efficiency are critical at scale.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [143] [Permanents of matrix ensembles: computation, distribution, and geometry](https://arxiv.org/abs/2602.10141)
*Igor Rivin*

Main category: quant-ph

TL;DR: 该论文通过GPU加速计算了复数、实数、有限域和有理数上的矩阵积和式，并进行了大量实验研究，发现了随机矩阵积和式的分布规律以及沿酉群测地线的积和式行为。


<details>
  <summary>Details</summary>
Motivation: 研究矩阵积和式的计算和统计特性，特别是对于随机矩阵和特殊矩阵（如DFT矩阵、Schur矩阵）的积和式分布规律，以及沿酉群测地线的积和式行为。

Method: 使用GPU加速计算永久性（积和式），对Haar分布的酉矩阵、正交矩阵、高斯矩阵进行大规模采样实验，并研究沿酉群测地线的积和式变化。

Result: 1. Haar随机酉矩阵的积和式服从圆对称复高斯分布；DFT矩阵积和式是极端异常值。2. Haar随机正交矩阵的积和式近似实高斯分布但具有正超额峰度。3. 高斯矩阵的积和式服从α稳定分布（α≈1.0-1.4）。4. 沿测地线的积和式表现出普适标度函数和与素数相关的特殊模式。

Conclusion: 矩阵积和式在不同随机矩阵系综中表现出不同的统计分布特性，沿酉群测地线的积和式行为揭示了与矩阵结构（特别是素数性）相关的深层数学模式，为理解积和式的统计性质提供了新视角。

Abstract: We report on a computational and experimental study of permanents. On the computational side, we use the GPU to greaatly accelerate the computation of permanents over $\mathbb{C},$ $\mathbb{R},$ $\mathbb{F}_p$ and $\mathbb{Q}.$ In particular, we use this to compute the permanents of DFT and Schur matrices far beyond the ranges hitherto known. On the experimental side, we present two new observations. First, for Haar-distributed unitary matrices~$U$, the permanent $\perm(U)$ follows a circularly-symmetric complex Gaussian distribution $\mathcal{CN}(0,σ^2)$ -- we confirm this via a number of tests for $n$ up to~23 with $50{,}000$ samples. The DFT matrix permanent is an extreme outlier for every prime $n\ge 7$. In contrast, for Haar-random \emph{orthogonal} matrices~$O$, the permanent $\perm(O)$ is approximately real Gaussian but with positive excess kurtosis that decays as~$O(1/n)$, indicating slower convergence. For matrices with Gaussian entries (GUE, GOE, Ginibre), the permanent follows an $α$-stable distribution with stability index $α\approx 1.0$--$1.4$, well below the Gaussian value $α=2$.
  Secondly, we study the permanent along geodesics on the unitary group. For the geodesic from the identity to the $n$-cycle permutation matrix, we find a universal scaling function $f(t)=\frac{1}{n}\ln|\perm(γ(t))|$ that is independent of~$n$ in the large-$n$ limit, with a midpoint value \[
  \perm(γ({\textstyle\frac12}))
  = (-1)^{(n-1)/2}\cdot 2e^{-n}\bigl(1+\tfrac{1}{3n}+O(n^{-2})\bigr) \] for odd~$n$ and zero for even~$n$. For the geodesic to the DFT matrix, the permanent recovers $10$--$40$ times above its valley minimum when $n$ is prime, but not when $n$ is composite -- a geodesic fingerprint of primality.

</details>


### [144] [Entanglement percolation in random quantum networks](https://arxiv.org/abs/2602.10189)
*Alessandro Romancino,Jordi Romero-Pallejà,G. Massimo Palma,Anna Sanpera*

Main category: quant-ph

TL;DR: 研究纠缠渗流协议在初始纠缠具有随机性而非均匀分布时的表现，发现经典纠缠渗流仅依赖平均初始纠缠，而量子纠缠渗流在更现实的条件下表现更差


<details>
  <summary>Details</summary>
Motivation: 先前研究假设量子网络中节点间的纠缠是均匀的，但实际网络中的纠缠通常具有随机性。需要研究纠缠渗流协议在更现实的初始纠缠分布条件下的性能

Method: 将纠缠渗流协议推广到初始纠缠具有随机性的场景，分析经典和量子两种渗流策略在不同网络拓扑和初始纠缠分布下的表现

Result: 对于经典纠缠渗流，只有平均初始纠缠是相关因素；而量子纠缠渗流协议在更现实的条件下通常表现更差

Conclusion: 初始纠缠的随机性对量子纠缠渗流协议有负面影响，而经典纠缠渗流对随机性更具鲁棒性，这在实际量子网络设计中需要考虑

Abstract: Entanglement percolation aims at generating maximal entanglement between any two nodes of a quantum network by utilizing strategies based solely on local operations and classical communication between the nodes. As it happens in classical percolation theory, the topology of the network is crucial, but also the entanglement shared between the nodes of the network. In a network of identically partially entangled states, the network topology determines the minimum entanglement needed for percolation. In this work, we generalize the protocol to scenarios where the initial entanglement shared between each two nodes of the network is not the same but has some randomness. In such cases, we find that for classical entanglement percolation, only the average initial entanglement is relevant. In contrast, the quantum entanglement percolation protocol generally performs worse under these more realistic conditions.

</details>


### [145] [Cosmological Expansion Induces Interference Between Communication and Entanglement Harvesting](https://arxiv.org/abs/2602.10203)
*Matheus H. Zambianco,Adam Teixidó-Bonfill,Eduardo Martín-Martínez*

Main category: quant-ph

TL;DR: 在膨胀宇宙中，局部粒子探测器通过场关联和通信关联两种机制获得纠缠，但宇宙膨胀会引发这两种机制的干涉效应，探测器是否随宇宙膨胀对其纠缠获取能力有决定性影响。


<details>
  <summary>Details</summary>
Motivation: 研究膨胀宇宙时空中局部粒子探测器纠缠获取的机制，特别关注宇宙膨胀如何影响通过场关联（真纠缠收获）和通信关联（因果接触）两种不同来源的纠缠之间的干涉效应。

Method: 在德西特时空中研究共形耦合标量场，比较两种物理上不同的探测器模型：随宇宙膨胀而空间轮廓扩展的探测器，以及尽管宇宙膨胀但保持固定固有尺寸的探测器。

Result: 宇宙膨胀缺乏时间反演对称性导致通信关联和场关联之间产生建设性或破坏性干涉，显著影响探测器获得的纠缠。快速膨胀会通过破坏性干涉完全抑制膨胀探测器的纠缠，即使两种关联各自都很大；而保持固定固有尺寸的探测器仍能获得显著纠缠。

Conclusion: 宇宙膨胀从根本上重塑了通信和收获之间的平衡，探测器的内部凝聚力（是否随宇宙膨胀）在决定探测器纠缠能否在快速膨胀宇宙中存活方面起着关键作用。

Abstract: We investigate the interplay between genuine entanglement harvesting and communication mediated correlations for local particle detectors in expanding cosmological spacetimes. Focusing on a conformally coupled scalar field in de Sitter spacetime, we analyze how spacetime expansion induces interference between these two sources of entanglement when the detectors are in causal contact. We compare two physically distinct detector models: detectors whose spatial profile expands with the Universe, and detectors whose proper size remains fixed despite cosmological expansion. We find that the lack of time-reversal symmetry in cosmological settings generically leads to constructive or destructive interference between communication mediated correlations and harvested field correlations, dramatically affecting the entanglement that detectors can acquire. In particular, rapid expansion can suppress entanglement entirely for expanding detectors through destructive interference, even when both communication and field correlations are individually large, whereas detectors that maintain a fixed proper size remain capable of acquiring significant entanglement. Our results show that cosmological expansion qualitatively reshapes the balance between communication and harvesting, and that the detector internal cohesion (whether it expands with the Universe or not) plays a crucial role in determining whether detectors' entanglement can survive in rapidly expanding universes.

</details>


### [146] [Communication complexity bounds from information causality](https://arxiv.org/abs/2602.10206)
*Nikolai Miklin,Prabhav Jain,Mariami Gachechiladze*

Main category: quant-ph

TL;DR: 该论文提出了一种基于互信息公理的信息论方法来研究单向通信复杂度，推导出扩展的信息因果性原理，该原理简化了多种函数通信复杂度下界的推导，并证明其至少与非平凡通信复杂度原理一样强。


<details>
  <summary>Details</summary>
Motivation: 通信复杂度量化分布式计算所需的最小通信量，为研究量子力学在信息处理中的能力和局限性提供了自然框架。作者旨在从信息论角度探索量子技术的基本限制。

Method: 引入基于互信息公理的信息论方法来研究单向通信复杂度，推导扩展的信息因果性原理，并将其应用于分析各种函数的通信复杂度下界。

Result: 扩展的信息因果性原理能够简化已知函数通信复杂度下界的推导，并产生新的结果。证明该原理在约束贝尔实验中可达到的量子关联强度方面至少与非平凡通信复杂度原理一样强。

Conclusion: 该研究从信息论视角建立了探索量子技术基本限制的新途径，为理解量子通信复杂度提供了更简洁有力的理论框架。

Abstract: Communication complexity, which quantifies the minimum communication required for distributed computation, offers a natural setting for investigating the capabilities and limitations of quantum mechanics in information processing. We introduce an information-theoretic approach to study one-way communication complexity based solely on the axioms of mutual information. Within this framework, we derive an extended statement of the information causality principle, which recovers known lower bounds on the communication complexities for a range of functions in a simplified manner and leads to new results. We further prove that the extended information causality principle is at least as strong as the principle of non-trivial communication complexity in bounding the strength of quantum correlations attainable in Bell experiments. Our study establishes a new route for exploring the fundamental limits of quantum technologies from an information-theoretic viewpoint.

</details>


### [147] [Quantum Integrated Sensing and Computation with Indefinite Causal Order](https://arxiv.org/abs/2602.10225)
*Ivana Nikoloska*

Main category: quant-ph

TL;DR: 该论文提出了一种在不定因果顺序框架下集成传感与计算的量子方案，使用同一量子态同时完成状态观测和函数学习，通过实验验证在磁导航任务中取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 探索不定因果顺序框架下能否实现传感与计算这两个量子信息处理的核心任务，突破传统信息处理中信息获取与学习必须遵循严格因果顺序的限制。

Method: 提出集成传感与计算方案，将量子态表示为智能体，在不定因果顺序操作下经历两种顺序的叠加：先观测后计算 vs 先计算后观测，使用参数化模型进行预测。

Result: 实验结果显示，该方案在磁导航代表性任务中能够实现较小的训练和测试损失，验证了不定因果顺序框架下集成传感与计算的可行性。

Conclusion: 不定因果顺序框架能够支持传感与计算的集成处理，为量子信息处理和机器学习提供了新的范式，突破了传统严格因果顺序的限制。

Abstract: Quantum operations with indefinite causal order (ICO) represent a framework in quantum information processing where the relative order between two events can be indefinite. In this paper, we investigate whether sensing and computation, two canonical tasks in quantum information processing, can be carried out within the ICO framework. We propose a scheme for integrated sensing and computation that uses the same quantum state for both tasks. The quantum state is represented as an agent that performs state observation and learns a function of the state to make predictions via a parametric model. Under an ICO operation, the agent experiences a superposition of orders, one in which it performs state observation and then executes the required computation steps, and another in which the agent carries out the computation first and then performs state observation. This is distinct from prevailing information processing and machine intelligence paradigms where information acquisition and learning follow a strict causal order, with the former always preceding the latter. We provide experimental results and we show that the proposed scheme can achieve small training and testing losses on a representative task in magnetic navigation.

</details>


### [148] [In-Situ Rewiring of Two-Dimensional Ion Lattice Interactions Using Metastable State Shelving](https://arxiv.org/abs/2602.10307)
*Ilyoung Jung,Antonis Kyprianidis,Frank G. Schroer,Thomas W. Burkle,Jack Lyons,Philip Richerme*

Main category: quant-ph

TL;DR: 通过将特定离子囚禁在亚稳态来动态重构离子晶格几何结构，从而控制量子比特间的相互作用


<details>
  <summary>Details</summary>
Motivation: 传统的离子晶格几何结构由库仑斥力与外部囚禁势的平衡决定，难以在实验中动态调整。需要一种能够在实验过程中实时重构离子晶格几何结构的方法，以灵活控制量子比特间的相互作用。

Method: 使用三个171Yb+离子形成的三角形晶格，通过光泵浦将特定离子泵入长寿命的2F7/2亚稳态，使其脱离量子比特子空间。然后对系统施加全局Ising-like哈密顿量，验证被囚禁的量子比特不参与量子动力学过程。

Result: 成功实现了离子晶格几何结构的动态重构，验证了被囚禁的量子比特完全从量子动力学中移除。表征了在激光驱动离子-离子相互作用下亚稳态的寿命，发现退囚禁速率比自旋-自旋相互作用速率慢几个数量级，并且与施加的激光强度呈二次方关系。

Conclusion: 通过将离子囚禁在亚稳态的方法，实现了离子晶格几何结构的动态重构，为量子模拟和量子计算中灵活控制量子比特相互作用提供了一种有效手段。

Abstract: Trapped-ion lattice geometries, which determine the interactions between trapped-ion qubits, are typically governed by the balance of Coulomb repulsion forces with the external trapping potential. Here we demonstrate how the effective ion lattice geometry and resulting qubit-qubit interactions may be reconfigured in-situ, by shelving specific ions in metastable states outside the qubit subspace. Using a triangular lattice of three $^{171}$Yb$^{+}$ ions, we optically pump selected ions into the long-lived $^2F_{7/2}$ state. We then apply a global Ising-like Hamiltonian to the system and verify that the shelved qubits are fully removed from participation in the quantum dynamics. We characterize the metastable state lifetime in the presence of laser-driven ion-ion interactions, finding a deshelving rate that is orders of magnitude slower than the spin-spin interaction rate and scales quadratically with applied laser intensity.

</details>


### [149] [High-performance source of indistinguishable polarization-entangled photons with a local oscillator reference for quantum networking](https://arxiv.org/abs/2602.10317)
*Michael Grayson,Shawn Meyer,Daniel Sorensen,Abigail Gookin,Markus Allgaier,Nicholas V. Nardelli,Tara M. Fortier,Dileep V. Reddy,Martin J. Stevens,Michael D. Mazurek,Juliet T. Gopinath,L. Krister Shalm*

Main category: quant-ph

TL;DR: 研究人员开发了一种紧凑的自由空间偏振纠缠光子对源，集成本地振荡器参考，在偏振纠缠可见度、Hong-Ou-Mandel干涉可见度、预示效率和本地振荡器干涉可见度方面均达到先进水平。


<details>
  <summary>Details</summary>
Motivation: 光学量子网络协议对纠缠源产生的量子态有严格要求，需要高保真度、高不可区分性和高效率的纠缠光子对，同时需要集成本地振荡器参考以实现精确的相位控制。

Method: 开发了一种紧凑的自由空间偏振纠缠光子对源，集成了本地振荡器参考系统。该源采用自发参量下转换过程产生偏振纠缠光子对，并通过优化设计实现高保真度纠缠态。

Result: 该源实现了(99.11±0.01)%的偏振纠缠可见度、(96.3±0.6)%的连续光子Hong-Ou-Mandel干涉可见度、(68.0±0.1)%的预示检测效率以及(88.6±0.2)%的本地振荡器干涉可见度。

Conclusion: 该研究展示了一种同时实现多项先进指标的适应性平台，为量子网络应用提供了高性能的纠缠光子源，满足了量子网络协议对量子态质量的严格要求。

Abstract: Optical quantum networking protocols impose stringent requirements on the states produced by sources of entanglement. We demonstrate a free-space, compact, source of indistinguishable pairs of polarization entangled photons, with an integrated local oscillator reference as a significant step towards this goal. This source achieves $(99.11 \pm 0.01)\%$ polarization entanglement visibility, $(96.3 \pm 0.6)\%$ successive-photon Hong-Ou-Mandel interference visibility, $(68.0 \pm 0.1)\%$ heralded efficiency as detected, and $(88.6 \pm 0.2)\%$ interference visibility with a local oscillator. This simultaneous achievement of state-of-the-art metrics demonstrates an adaptable platform for quantum networking.

</details>


### [150] [Uncertainty and Wigner negativity in Hilbert-space classical mechanics](https://arxiv.org/abs/2602.10341)
*Mustafa Amin*

Main category: quant-ph

TL;DR: 经典力学在Koopman-von Neumann表述中具有量子力学特征：非对易算符导致不确定性关系，Wigner表示产生负值准概率分布


<details>
  <summary>Details</summary>
Motivation: 探索经典力学在希尔伯特空间表述中是否展现出量子力学的特征性质，特别是非对易算符和不确定性关系

Method: 采用Koopman-von Neumann表述将经典力学置于希尔伯特空间中，分析经典正则变换的生成算符性质，研究Wigner表示

Result: 发现经典正则变换由非对易的厄米算符生成，导致经典力学中存在不确定性关系；Wigner表示产生可负值的准概率分布

Conclusion: 经典力学在希尔伯特空间表述中展现出量子力学的两个标志性特征：非对易算符导致的不确定性关系和负值准概率分布

Abstract: Classical mechanics, in the Koopman-von Neumann formulation, is described in Hilbert space. It is shown here that classical canonical transformations are generated by Hermitian operators that are in general noncommutative. This naturally brings about uncertainty relations inherent in classical mechanics, for example between position and the generator of space translations, between momentum and the generator of momentum translations, and between dynamical time and the Liouvillian, to name a few. Further, it is shown that the Wigner representation produces a quasi-probability distribution that can take on negative values. Thus, two of the hallmark features of quantum mechanics are reproduced, and become apparent, in a Hilbert-space formulation of classical mechanics.

</details>


### [151] [Comparing and correcting robustness metrics for quantum optimal control](https://arxiv.org/abs/2602.10349)
*Andrew T. Kamen,Samuel Fine,Bikrant Bhattacharyya,Frederic T. Chong,Andy J. Goldschmidt*

Main category: quant-ph

TL;DR: 本文系统研究了量子最优控制中两种鲁棒性数值近似方法的差异，提出了离散化修正，并将鲁棒性作为直接约束最优控制的首要目标。


<details>
  <summary>Details</summary>
Motivation: 传统优化保真度的控制脉冲对硬件漂移和建模误差敏感，需要开发对误差不敏感的鲁棒控制脉冲，同时满足硬件约束。

Method: 将鲁棒性作为直接约束最优控制的首要目标，系统比较伴随端点法和切换框架法两种数值近似方法，引入离散化修正改进切换框架鲁棒性估计器。

Result: 在单量子比特和双量子比特的实际约束下，该方法为获得精确、物理信息化的鲁棒性提供了分析优势，显著改善了误差敏感性的估计。

Conclusion: 通过将鲁棒性作为首要优化目标并改进数值方法，能够获得更精确、对硬件误差不敏感的量子控制脉冲，提升量子操作的可靠性。

Abstract: Control pulses that nominally optimize fidelity are sensitive to routine hardware drift and modeling errors. Robust quantum optimal control seeks error-insensitive control pulses that maintain fidelity thresholds and obey hardware constraints. Distinct numerical approximations to the first-order error susceptibility include adjoint end-point and toggling-frame approaches. Although theoretically equivalent, we provide a novel, systematic study demonstrating important numerical differences between these two approaches. We also introduce a critical discretization correction to the widely-used toggling-frame robustness estimator, measurably improving its estimate of first-order error susceptibility. We accomplish our study by positioning robustness as a first-class objective within direct, constrained optimal control. Our approach uniquely handles control and fidelity constraints while cleanly isolating robustness for dedicated optimization. In both single- and two-qubit examples under realistic constraints, our approach provides an analytic edge for obtaining precise, physics-informed robustness.

</details>


### [152] [Accelerating Classical and Quantum Tensor PCA](https://arxiv.org/abs/2602.10366)
*Matthew B. Hastings*

Main category: quant-ph

TL;DR: 该论文改进了张量PCA的谱方法，实现了经典算法二次加速和量子算法八次方加速，并进一步将量子算法提升至六次方加速，但后续发现随机算子谱范数改进可能影响可证明的多项式加速。


<details>
  <summary>Details</summary>
Motivation: 改进张量PCA的谱方法，在保持经典与量子算法之间四次方分离的同时，实现经典算法的二次加速和量子算法的八次方加速，并进一步提升量子算法的性能。

Method: 通过修改经典和量子谱方法算法，利用随机算子的谱特性（不仅仅是最大特征值，还包括态密度），实现检测任务的加速。

Result: 实现了经典算法相对于原始经典算法的二次加速，量子算法相对于原始经典算法的八次方加速，并进一步将量子算法相对于改进后经典算法的加速提升至六次方。

Conclusion: 论文展示了张量PCA谱方法的显著加速，但后续发现随机算子谱范数改进可能影响可证明的多项式加速，需要进一步验证态密度特性是否仍然成立。

Abstract: Spectral methods are a leading approach for tensor PCA with a ``spiked" Gaussian tensor. The methods use the spectrum of a linear operator in a vector space with exponentially high dimension and in Ref. 1 it was shown that quantum algorithms could then lead to an exponential space saving as well as a quartic speedup over classical. Here we show how to accelerate both classical and quantum algorithms quadratically, while maintaining the same quartic separation between them. That is, our classical algorithm here is quadratically faster than the original classical algorithm, while the quantum algorithm is eigth-power faster than the original classical algorithm. We then give a further modification of the quantum algorithm, increasing its speedup over the modified classical algorithm to the sixth power. We only prove these speedups for detection, rather than recovery, but we give a strong plausibility argument that our algorithm achieves recovery also.
  Note added: After this paper was prepared, A. Schmidhuber pointed out to me Ref. 3. This improves the best existing bounds on the spectral norm of a certain random operator. Because the norm of this operator enters into the runtime, with this improvement on the norm, we no longer have a provable polynomial speedup. Our results are phrased in terms of certain properties of the spectrum of this operator (not merely the largest eigenvalue but also the density of states). So, if these properties still hold, the speedup still holds. Rather than modify the paper, I have left it unchanged but added a section at the end discussing the needed property of density of states and considering for which problems (there are several problems for which this kind of quartic quantum speedup has been used and the techniques here will likely be applicable to several of them) the property is likely to hold.

</details>


### [153] [Quantum Brownian motion with non-Gaussian noises: Fluctuation-Dissipation Relation and nonlinear Langevin equation](https://arxiv.org/abs/2602.10421)
*Hing-Tong Cho,Bei-Lok Hu*

Main category: quant-ph

TL;DR: 该论文研究了非线性耦合的量子布朗运动模型，其中系统与环境的耦合包含位置和动量的非线性项，推导了非高斯噪声核和修正的涨落耗散关系。


<details>
  <summary>Details</summary>
Motivation: 研究系统与环境非线性耦合的开放量子系统，探索非高斯噪声特性，为早期宇宙宇宙学和量子光力学等应用提供理论基础。

Method: 采用闭时路径形式，对非线性耦合参数λ进行微扰展开，计算影响作用量，识别噪声核和耗散核，推导非线性朗之万方程。

Result: 得到了非高斯噪声核（导致随机力的三点关联函数非零），建立了修正的涨落耗散关系，推导了非线性朗之万方程。

Conclusion: 该研究为非线性耦合开放量子系统的非高斯特性探索提供了有效途径，修正的涨落耗散关系确保了模型在高阶微扰下的自洽性。

Abstract: Building upon the work of Hu, Paz, and Zhang [1,2] on open quantum systems we consider the quantum Brownian motion (QBM) model with one oscillator (position variable $x$) as the system, {\it nonlinearly} coupled to an environment of $N$ harmonic oscillators (with mass $m_n$, natural frequency $ω_n$, position $q_n$ and momentum $p_n$ variables) in the form $\sum_{n}\left(v_{n1}(x)q_{n}^{k}+v_{n2}(x)p_{n}^{l}\right)$ where $k, l$ are integers (the present work only considers the $k=l=2$ cases). The vertex functions $v_{n1}, v_{n2} $ are of the form $v_{n1}=λC_{n1} f(x), v_{n2}(x)=-λ\,C_{n2}m_{n}^{-2}ω_{n}^{-2}f(x)$ where $C_{n1,2}$ are the coupling constants with the $n$th oscillator, $f(x)$ is any arbitrary function of $x$, and $λ$ is a dimensionless constant. Employing the closed-time-path formalism the influence action $S_{IF}$ is calculated using a perturbative expansion in $λ$. It is possible to identify the terms in $S_{IF}$ quadratic or higher in $Δ(s)\equiv f(x_{+}(s))-f(x_{-}(s))$ to constitute the noise kernel, while terms linear in $Δ$ to that of the dissipation kernel. The non-Gaussian noise kernel gives rise to non-zero three-point correlation function of the corresponding stochastic force. The pathway presented here should be useful for the exploration of \textit{non-Gaussian properties of systems nonlinearly coupled with their environments}; examples in early universe cosmology and in quantum optomechanics (QOM) are mentioned. A modified fluctuation-dissipation relation (FDR) is also established, which ensures the consistency of the model and the accuracy of results even at higher perturbative orders. Another result of significance is the derivation of a nonlinear Langevin equation which is expected to be useful for many open quantum system applications.

</details>


### [154] [Erasure Thresholds for Hyperbolic and Semi-Hyperbolic Surface Codes](https://arxiv.org/abs/2602.10423)
*Aygul Azatovna Galimova*

Main category: quant-ph

TL;DR: 构建了25个双曲和半双曲CSS表面码，在电路级擦除和泡利噪声下进行模拟，发现精细缩放家族在两种噪声下都获得更高阈值，擦除噪声阈值显著高于泡利噪声


<details>
  <summary>Details</summary>
Motivation: 研究双曲和半双曲CSS表面码在不同噪声模型下的性能表现，探索几何结构对量子纠错码阈值的影响

Method: 从{8,3}、{10,3}、{12,3}镶嵌构造14个双曲CSS表面码和11个半双曲（精细）码，在电路级擦除噪声、泡利噪声和现象学噪声下进行模拟

Result: 在电路级泡利噪声下，伪阈值随码尺寸增加而增加；擦除噪声下大多数码阈值>5%；精细缩放家族在泡利噪声(0.67-0.68%)和擦除噪声(3.0-3.5%)下都获得更高阈值；现象学噪声下Z通道阈值约2%({8,3})和1%({10,3})

Conclusion: 双曲和半双曲CSS表面码在擦除噪声下表现优异，精细缩放家族在两种噪声下都获得更高阈值，擦除噪声阈值显著高于泡利噪声，为量子纠错码设计提供了新方向

Abstract: We construct 14 hyperbolic CSS surface codes from $\{8,3\}$, $\{10,3\}$, and $\{12,3\}$ tessellations and 11 semi-hyperbolic (fine-grained) codes. We simulate all 25 codes under circuit-level erasure and Pauli noise. Under circuit-level Pauli noise, pseudothresholds increase with code size within each family ($0.24$--$0.49\%$ for $\{8,3\}$, $0.11$--$0.43\%$ for $\{10,3\}$, $0.07$--$0.13\%$ for $\{12,3\}$). For erasure noise, most codes have $p^*_{\mathrm{E}} > 5\%$. Per-observable family thresholds give erasure-to-Pauli ratios of $2.7$--$3.9\times$ for the base code families. Fine-grained scaling families achieve higher thresholds in both Pauli ($0.67$--$0.68\%$) and erasure ($3.0$--$3.5\%$), with ratios of $4.5$--$5.2\times$. Under phenomenological noise, per-logical $Z$-channel thresholds are ${\sim}2\%$ for $\{8,3\}$ and ${\sim}1\%$ for $\{10,3\}$; the $\{12,3\}$ threshold lies below $0.5\%$.

</details>


### [155] [Privacy-Utility Tradeoffs in Quantum Information Processing](https://arxiv.org/abs/2602.10510)
*Theshani Nuradha,Sujeet Bhalerao,Felix Leditzky*

Main category: quant-ph

TL;DR: 该论文研究量子差分隐私中的隐私-效用权衡，分析了通用和应用特定的效用指标，证明了去极化机制在通用设置中的最优性，并推导了私有学习可观测值期望的样本复杂度下界。


<details>
  <summary>Details</summary>
Motivation: 在量子差分隐私领域，隐私保护与数据效用之间的权衡关系尚未得到充分探索。当敏感信息编码在数据中时，需要在保护隐私的同时从数据中学习有用信息，但增加隐私要求可能会降低学习协议的效用。

Method: 使用(ε,δ)-量子局部差分隐私量化隐私，研究通用设置中的保真度和迹距离优化，证明去极化机制达到最优效用。针对学习可观测值期望的具体应用，推导私有化数据样本数量的下界，并设计达到最优样本复杂度的私有机制。

Result: 去极化机制在通用隐私-效用权衡中达到最优；私有学习可观测值期望的样本复杂度为Θ((εβ)^{-2})，其中ε是隐私参数，β是精度容限；针对特定任务设计的私有机制能显著提高效用。

Conclusion: 量子差分隐私中的隐私-效用权衡在不同设置下表现不同：通用设置中已有最优机制，而针对特定任务可以设计更高效的私有协议。研究还开启了私有经典阴影的研究方向，为私有学习任务提供了新的应用前景。

Abstract: When sensitive information is encoded in data, it is important to ensure the privacy of information when attempting to learn useful information from the data. There is a natural tradeoff whereby increasing privacy requirements may decrease the utility of a learning protocol. In the quantum setting of differential privacy, such tradeoffs between privacy and utility have so far remained largely unexplored. In this work, we study optimal privacy-utility tradeoffs for both generic and application-specific utility metrics when privacy is quantified by $(\varepsilon,δ)$-quantum local differential privacy. In the generic setting, we focus on optimizing fidelity and trace distance between the original state and the privatized state. We show that the depolarizing mechanism achieves the optimal utility for given privacy requirements. We then study the specific application of learning the expectation of an observable with respect to an input state when only given access to privatized states. We derive a lower bound on the number of samples of privatized data required to achieve a fixed accuracy guarantee with high probability. To prove this result, we employ existing lower bounds on private quantum hypothesis testing, thus showcasing the first operational use of them. We also devise private mechanisms that achieve optimal sample complexity with respect to the privacy parameters and accuracy parameters, demonstrating that utility can be significantly improved for specific tasks in contrast to the generic setting. In addition, we show that the number of samples required to privately learn observable expectation values scales as $Θ((\varepsilon β)^{-2})$, where $\varepsilon \in (0,1)$ is the privacy parameter and $β$ is the accuracy tolerance. We conclude by initiating the study of private classical shadows, which promise useful applications for private learning tasks.

</details>


### [156] [Experimental demonstration that qubits can be cloned at will, if encrypted with a single-use decryption key](https://arxiv.org/abs/2602.10695)
*Koji Yamaguchi,Leon Rullkötter,Ibrahim Shehzad,Sean J. Wagner,Christian Tutschku,Achim Kempf*

Main category: quant-ph

TL;DR: 实验证明加密克隆在硬件噪声下稳定可行，可并行、串联或交错使用，为实用量子原语，需修正不可克隆定理的理解


<details>
  <summary>Details</summary>
Motivation: 不可克隆定理限制了量子技术的发展，加密克隆理论上可行但硬件噪声下的稳定性未知，需要实验验证其实际可行性

Method: 在IBM Heron-R2超导量子处理器上使用最多154个量子比特进行实验，测试加密克隆在硬件噪声下的稳定性，包括并行、串联和交错使用等模块化应用

Result: 加密克隆在硬件噪声下稳定，可作为模块并行、串联或交错使用，同时保持预存纠缠，证明其作为实用量子原语的可行性

Conclusion: 加密克隆是实用的量子原语，需修正不可克隆定理：量子信息可以通过加密或隐藏任意传播而不被稀释或退化，实际限制是解密机制必须单次使用

Abstract: The no-cloning theorem forbids the creation of identical copies of qubits, thereby imposing strong limitations on quantum technologies. A recently-proposed protocol, encrypted cloning, showed, however, that the creation of perfect clones is theoretically possible - if the clones are simultaneously encrypted with a single-use decryption key. It has remained an open question, however, whether encrypted cloning is stable under hardware noise and thus practical as a quantum primitive. This is nontrivial because spreading quantum information widely could dilute it until barely exceeding the noise level, leading to catastrophic fidelity decay. Given the complexity of hardware noise, theory and classical simulation are insufficient to settle this. Here, we settle this question experimentally, on IBM Heron-R2 superconducting processors using up to 154 qubits. We find that encrypted cloning is stable under hardware noise, even when used as a module, namely in parallel, series or interleaved, while preserving pre-existing entanglement. This establishes it as a versatile quantum primitive for practical use, and it necessitates a refinement to our understanding of the no-cloning theorem: quantum information can be spread at will, in theory and in practice, without dilution or degradation, if encrypted or obscured. The actual constraint is that the decryption mechanism must be single-use.

</details>


### [157] [Pilot-Wave Theories as Hidden Markov Models](https://arxiv.org/abs/2602.10569)
*Jacob A. Barandes*

Main category: quant-ph

TL;DR: 本文认为德布罗意-玻姆导波理论中的波函数既不是本体论实体也不是纯粹动力学定律，而应理解为隐马尔可夫模型中的潜变量。


<details>
  <summary>Details</summary>
Motivation: 传统上对德布罗意-玻姆导波理论有两种解释：一种将波函数视为物理本体论的一部分，另一种将其视为动力学定律的体现。本文旨在批判这两种观点，并提出新的解释框架。

Method: 通过分析导波理论的历史发展和现代理解，运用隐马尔可夫模型的概念框架，并考察Foldy-Wouthuysen规范变换和Deotto-Ghirardi模糊性等数学结构。

Result: 提出导波应被理解为隐马尔可夫模型中的潜变量，这一解释能更好地处理理论中的数学模糊性和变换自由度问题。

Conclusion: 德布罗意-玻姆导波既非纯粹本体论实体也非纯粹动力学定律，而是类似于隐马尔可夫模型中的潜变量，这一理解能更准确地把握其理论地位。

Abstract: The original version of the de Broglie-Bohm pilot-wave theory, also called Bohmian mechanics, attempted to treat the wave function or pilot wave as a part of the physical ontology of nature. More recent versions of the de Broglie-Bohm theory appearing in the last few decades have tried to regard the pilot wave instead as an aspect of the theory's nomology, or dynamical laws. This paper argues that neither of these views is correct, and that the de Broglie-Bohm pilot wave is best understood as a collection of latent variables in the sense of a hidden Markov model, a construct that was not available when de Broglie and Bohm originally formulated what became their pilot-wave theory. This paper also discusses several other challenges for the ontological view of the pilot wave. One such challenge is due to Foldy-Wouthuysen gauge transformations, which connect up with the Deotto-Ghirardi ambiguity in the de Broglie-Bohm theory. Another challenge arises from the freedom to carry out canonical transformations in the wave function's own notion of phase space, as defined by Strocchi and Heslot.

</details>


### [158] [General Theory of Stable Microwave-Optical Quantum Resources in Hybrid-System Dynamics](https://arxiv.org/abs/2602.10581)
*Fan Li,Shi-fan Qi,Z. D. Wang,Yan-Kui Bai*

Main category: quant-ph

TL;DR: 该论文提出了一个理论框架，用于分析微波-光学混合量子系统中稳定的量子资源，发现非稳态演化能超越稳态极限，增强量子纠缠和量子导引的质量。


<details>
  <summary>Details</summary>
Motivation: 研究微波和光学模式之间稳定量子资源的理论特性，特别是在多体混合量子系统中，探索超越稳态极限的量子资源增强可能性。

Method: 通过微波-中介-光学混合系统的强相互作用，建立微波-光学压缩的有效哈密顿量，并解析推导出微波-光学纠缠和量子导引的动力学严格解。

Result: 发现稳定的微波-光学量子资源能在非稳态演化中存活，且非稳态演化能展现出超越稳态极限的增强量子资源质量；通过调制有效耦合强度可有效控制稳定的微波-光学纠缠及单向和双向量子导引。

Conclusion: 该理论框架在电光机械和腔光磁机械混合系统中得到验证，为微波-光学量子接口的设计和优化提供了理论基础。

Abstract: We develop a general theoretical framework for characterizing stable quantum resources between microwave and optical modes in the dynamics of multipartite hybrid quantum systems with intermediary modes. The effective Hamiltonian for microwave-optical (MO) squeezing is formulated via strong interactions in the microwave-intermediary-optical hybrid system, and based on which rigorous solutions for the dynamics of MO entanglement and quantum steering are derived analytically. Remarkably, it is found that stable MO quantum resources can survive in the unsteady evolution beyond the steady one, and the unsteady evolution can exhibit the enhanced quality over the limit of quantum resources in the steady-state case. Furthermore, the stable MO entanglement as well as one-way and two-way quantum steerings are efficiently controllable by modulating the effective coupling strength. The validity of our theory is demonstrated by applying it to the typical models of electro-optomechanical and cavity optomagnomechanical hybrid systems.

</details>


### [159] [Block encoding of sparse matrices with a periodic diagonal structure](https://arxiv.org/abs/2602.10589)
*Alessandro Andrea Zecchi,Claudio Sanavio,Luca Cappelli,Simona Perotto,Alessandro Roggero,Sauro Succi*

Main category: quant-ph

TL;DR: 提出了一种用于编码具有周期性对角结构的稀疏矩阵的量子电路，基于线性组合酉算子框架，相比通用方法具有多项式复杂度优势。


<details>
  <summary>Details</summary>
Motivation: 块编码是量子算法中的关键技术，但现有方法在处理具有周期性对角结构的稀疏矩阵时效率不高，需要更高效的编码方案。

Method: 基于线性组合酉算子框架，设计了一种高效的酉算子，用于将复指数在频率ω处投影到其实部和虚部，从而实现对具有周期性对角结构的稀疏矩阵的块编码。

Result: 在最坏情况下（带状矩阵）具有多项式门复杂度，在处理简单对角矩阵时具有线性复杂度，相比稠密矩阵的通用方法的指数级缩放有明显优势。

Conclusion: 该方法为求解对流-扩散-反应等微分问题提供了高效的量子算法基础，特别是与量子奇异值变换等最优缩放算法结合使用，数值结果验证了分析公式的有效性。

Abstract: Block encoding is a successful technique used in several powerful quantum algorithms. In this work we provide an explicit quantum circuit for block encoding a sparse matrix with a periodic diagonal structure. The proposed methodology is based on the linear combination of unitaries (LCU) framework and on an efficient unitary operator used to project the complex exponential at a frequency $ω$ multiplied by the computational basis into its real and imaginary components. We demonstrate a distinct computational advantage with a $\mathcal{O}(\text{poly}(n))$ gate complexity, where $n$ is the number of qubits, in the worst-case scenario used for banded matrices, and $\mathcal{O}(n)$ when dealing with a simple diagonal matrix, compared to the exponential scaling of general-purpose methods for dense matrices. Various applications for the presented methodology are discussed in the context of solving differential problems such as the advection-diffusion-reaction (ADR) dynamics, using quantum algorithms with optimal scaling, e.g., quantum singular value transformation (QSVT). Numerical results are used to validate the analytical formulation.

</details>


### [160] [Practical quantum tokens: challenges and perspectives](https://arxiv.org/abs/2602.10621)
*Nadezhda P. Kukharchyk,Holger Boche,Christian Deppe,Kirill G. Fedorov,Martin E. Garcia,Ilja Gerhardt,Rudolf Gross,Thomas Halfmann,Hans Huebl,David Hunger,Wolfgang Kilian,Roman Kolesov,Juliane Krämer,Alexander Kubanek,Kai Müller,Boris Naydenov,Janis Nötzel,Anna P. Ovvyan,Wolfram H. P. Pernice,Gregor Pieplow,Cyril Popov,Tim Schröder,Kilian Singer,Janik Wolters*

Main category: quant-ph

TL;DR: 量子代币概念自1983年提出，现已发展出多种物理实现和实验演示，本文综述了量子代币的现状、物理实现、应用场景及其在信息安全生态系统中的定位。


<details>
  <summary>Details</summary>
Motivation: 量子代币作为量子密码学的重要应用，自1983年提出量子银行券概念以来，经过40年发展已出现多种实现方案和实验演示，需要系统梳理当前技术现状、物理实现方法、应用场景及其在信息安全体系中的位置。

Method: 采用综述性视角，分析量子代币领域的最新进展，详细讨论集成量子存储器的多种物理实现方案，并探讨其具体应用场景。

Result: 量子代币已从理论概念发展到实验演示阶段，出现了多种物理实现方案，包括集成量子存储器的系统，能够应用于安全支付、数字认证等场景，并与后量子密码学形成互补关系。

Conclusion: 量子代币作为量子信息领域的重要应用，已取得实质性进展，未来将在信息安全生态系统中发挥重要作用，与后量子密码学共同构建更安全的通信和支付体系。

Abstract: The concept of quantum tokens dates back alongside quantum cryptography to Stephen Wiesner's seminal work in 1983[1]. Already this initial work proposes society-relevant applications such as secure quantum banknotes, which can be exchanged between a bank and a customer. This quantum currency is based on various physical states that can be easily verified but is protected from being copied by the fundamental quantum laws. Four decades later, these ideas have flourished in the field of quantum information, and the concept of quantum banknotes has not only adopted many varying names, such as quantum money, quantum coins, quantum-digital payments, and quantum tokens, but also reached its first experimental demonstrations. In this perspective article, we discuss the current state-of-the-art of quantum tokens in the field of quantum information, as well as their future perspectives. We present a number of physical realizations of quantum tokens with integrated quantum memories and their applicability scenarios in detail. Finally, we discuss how quantum tokens fit into the information security ecosystem and consider their relationship to post-quantum cryptography.

</details>


### [161] [Projection-Based Memory Kernel Coupling Theory for Quantum Dynamics: A Stable Framework for Non-Markovian Simulations](https://arxiv.org/abs/2602.10629)
*Wei Liu,Rui-Hao Bi,Yu Su,Limin Xu,Zhennan Zhou,Yao Wang,Wenjie Dou*

Main category: quant-ph

TL;DR: 提出一种基于投影的稳定性保持方法，用于计算非马尔可夫开放量子系统中的时间关联函数，通过Mori-Zwanzig投影和谱投影确保数值稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在计算非马尔可夫开放量子系统的时间关联函数时面临数值不稳定问题，需要引入人工阻尼或特殊修改，这会影响计算精度和物理可靠性。

Method: 基于记忆核耦合理论框架，通过Mori-Zwanzig投影将记忆核层次结构转化为耦合线性微分方程组，然后进行谱投影到稳定本征模，系统性地消除不稳定模式同时保留物理相关动力学。

Result: 在自旋-玻色子模型上的基准计算显示与精确层次运动方程结果高度一致，同时实现了显著的计算效率提升，保证了长时间收敛性。

Conclusion: 该方法为复杂系统中的非马尔可夫动力学模拟提供了一个通用且可靠的框架，兼具数学严谨性和计算效率。

Abstract: We present a projection-based, stability-preserving methodology for computing time correlation functions in open quantum systems governed by generalized quantum master equations with non-Markovian effects. Building upon the memory kernel coupling theory framework, our approach transforms the memory kernel hierarchy into a system of coupled linear differential equations through Mori-Zwanzig projection, followed by spectral projection onto stable eigenmodes to ensure numerical stability. By systematically eliminating unstable modes while preserving the physically relevant dynamics, our method guaranties long-time convergence without introducing artificial damping or ad hoc modifications. The theoretical framework maintains mathematical rigor through orthogonal projection operators and spectral decomposition. Benchmark calculations on the spin-boson model show excellent agreement with exact hierarchical equations of motion results while achieving significant computational efficiency. This approach provides a versatile and reliable framework for simulating non-Markovian dynamics in complex systems.

</details>


### [162] [Run-length certificates in quantum learning: sample complexity and noise thresholds](https://arxiv.org/abs/2602.10648)
*Jeongho Bang*

Main category: quant-ph

TL;DR: 论文提出从停止时间视角分析量子态学习，研究单次测量学习(SSML)中通过连续成功次数作为在线证书的样本复杂度界限，揭示了标签噪声成为信息瓶颈的条件。


<details>
  <summary>Details</summary>
Motivation: 传统量子学习采用固定预算范式，本文转向停止时间视角，研究在最小反馈学习（每拷贝仅一比特反馈）中，如何通过在线运行长度证书定义学习完成，分析认证过程如何主导样本复杂度。

Method: 分析单次测量学习(SSML)框架，通过调整幺正操作并在连续M_H次成功后停止。将停止视为顺序认证过程，将观察到的计数器与保真度误差联系起来，推导样本复杂度界限，分离搜索过程（驱动成功概率趋近1）和认证过程（连续成功的运行统计）。

Result: 发现尖锐的可行性阈值：当qM_H ≳ 1时（q为标签翻转噪声概率），期望停止时间呈指数增长，即使理想控制下学习完成也变得不切实际。在严重受限的反馈下，认证过程主导样本复杂度，小标签噪声成为信息瓶颈。

Conclusion: 运行长度认证能够实现接近最优的精度，与量子态估计（等价于无克隆定理）极限一致，并在停止时间框架下表达。这表明在最小反馈学习中，认证过程而非控制过程可能成为样本复杂度的主要限制因素。

Abstract: Quantum learning from state samples is often benchmarked in a fixed-budget paradigm, relating error to a prescribed number of copies. We instead adopt a stopping-time viewpoint: in minimal-feedback learning, the learning completion can be defined by an online run-length certificate extracted from a one-bit-per-copy record. As an instantiation, we analyze single-shot measurement learning (SSML), introduced in [Phys. Rev. A 98, 052302 (2018)] and [Phys. Rev. Lett. 126, 170504 (2021)], which tunes a unitary and halts after $M_H$ consecutive successes. Viewing the halting as a sequential certification linking the observed counter to infidelity, we derive sample-complexity bounds that separate search (driving success probability toward unity) from certification (run statistics of consecutive successes). The resulting trade-off among $M_H$, dimension $d$, and one-bit reliability clarifies when performance is control-limited versus certificate-limited. With label-flip noise probability $q$, we find a sharp feasibility threshold: once $qM_H \gtrsim 1$, the expected halting time grows exponentially, making the learning completion impractical even under ideal control. More broadly, this shows that under severely constrained feedback, the certification can dominate sample complexity and small label noise becomes the information bottleneck. Finally, the near-optimal accuracy enabled by run-length certification aligns with the quantum-state-estimation (and equivalently, no-cloning) limits, expressed in the stopping-time terms.

</details>


### [163] [Maximum residual strong monogamy inequality for multiqubit entanglement](https://arxiv.org/abs/2602.10668)
*Dong-Dong Dong,Xue-Ke Song,Liu Ye,Dong Wang,Gerardo Adesso*

Main category: quant-ph

TL;DR: 提出两种新的强一夫一妻不等式：加权强一夫一妻(WSM)和最大剩余强一夫一妻(MRSM)，用于改进多量子比特态的广义Coffman-Kundu-Wootters不等式，更精确地刻画多体纠缠的分配关系。


<details>
  <summary>Details</summary>
Motivation: 现有强一夫一妻(SM)猜想使用指数来调节不同m-体贡献的权重，存在局限性。需要更精细的框架来刻画和量化多体纠缠的一夫一妻性质，特别是能够区分可分态并提供更紧的界限。

Method: 提出两种新不等式：1) WSM使用系数而非指数来调节不同m-体纠缠贡献的权重；2) MRSM仅使用最大m-体纠缠来构建不等式。通过四量子比特混合态和五量子比特纯态的具体例子进行验证和比较。

Result: MRSM不等式的剩余纠缠能有效区分可分态。比较了各种SM不等式的紧致性，并通过具体例子展示了MRSM不等式的应用，刻画了涉及不同数量量子比特的纠缠分量之间的权衡关系。

Conclusion: 提出的WSM和MRSM不等式为刻画和量化多体纠缠的一夫一妻性质提供了严格的框架，改进了现有的广义Coffman-Kundu-Wootters不等式，能更精确地描述多量子比特系统中纠缠的分配限制。

Abstract: We establish two new inequalities, the weighted strong monogamy (WSM) and the maximum residual strong monogamy (MRSM), which sharpen the generalized Coffman-Kundu-Wootters inequity for multiqubit states. The WSM inequality distinguishes itself from the strong monogamy (SM) conjecture [Phys. Rev. Lett. 113, 110501 (2014)] by using coefficients rather than exponents to modulate the weight allocated to various m-partite contributions. In contrast, the MRSM inequality is formulated using only the maximum m-partite entanglement. We find that the residual entanglement of the MRSM inequality can effectively distinguish the separable states. We also compare the tightness of various SM inequalities and provide examples using a four-qubit mixed state and a five-qubit pure state to illustrate the MRSM inequality. These examples characterize the trade-off relations among entanglement components involving varying numbers of qubits. Our results provide a rigorous framework to characterize and quantify the monogamy of multipartite entanglement.

</details>


### [164] [Dimensional advantage in network cooling with hybrid oscillator-qudit systems](https://arxiv.org/abs/2602.10683)
*Mrinmoy Samanta,Debkanta Ghosh,Rivu Gupta,Aditi Sen De*

Main category: quant-ph

TL;DR: 本文研究了通过重复幺正演化和条件测量冷却振荡器网络，证明了量子比特辅助系统无法实现近完美冷却，揭示了高维辅助系统的双重维度优势，并展示了协议对混合连续-离散变量系统的适应性。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过有限维辅助系统与振荡器网络的Jaynes-Cummings类型相互作用实现有效冷却，探索辅助系统维度对冷却效率的影响，以及网络拓扑结构对冷却性能的限制。

Method: 采用重复幺正演化后接条件测量的协议，通过Jaynes-Cummings类型相互作用耦合振荡器网络与有限维辅助系统，分析不同维度辅助系统（从量子比特到高维系统）和不同网络配置（线性和星形网络）下的冷却性能。

Result: 证明了量子比特辅助系统无法实现近完美冷却（无冷却定理）；发现高维辅助系统具有双重维度优势：减少所需循环次数，并能冷却更高初始能量的振荡器；线性网络可实现近完美冷却而星形网络则不能；协议能有效冷却混合连续-离散变量系统并产生非高斯和纠缠量子资源。

Conclusion: 辅助系统维度对振荡器网络冷却至关重要，高维辅助系统提供显著优势，网络拓扑结构影响冷却极限，该协议具有广泛适应性，可用于生成量子信息处理所需的非经典资源。

Abstract: We examine the cooling of networks of oscillators through repeated unitary evolution followed by conditional measurement on a finite-dimensional auxiliary system, coupled via Jaynes-Cummings type interaction. We prove that near-perfect cooling of the oscillator to vacuum is fundamentally impossible when the auxiliary system is a qubit, establishing a no-cooling theorem for a two-level regulator. Moving beyond this limitation, we reveal a twofold dimensional advantage of higher-dimensional auxiliaries - reducing the number of required cycles, and enabling the efficient cooling of oscillators with higher initial energies. We further show that, while extending the network leads to a saturation of this dimensional advantage at moderate auxiliary dimensions, near-perfect cooling remains achievable for linear network configurations but fails for star networks. Moreover, we highlight the adaptability of the proposed protocol by demonstrating efficient cooling of hybrid continuous- and discrete-variable systems that naturally support the generation of non-Gaussian and entangled quantum resources.

</details>


### [165] [Magneto-optical properties of the neutral silicon-vacancy center in diamond under extreme isotropic strain fields](https://arxiv.org/abs/2602.10690)
*Meysam Mohseni,Gergő Thiering,Adam Gali*

Main category: quant-ph

TL;DR: SiV⁰中心在钻石中具有反转对称性和光学发射特性，对电场稳定。研究通过第一性原理计算其在各向同性应变下的响应，发现压缩应变增强自旋-轨道耦合和辐射寿命，而拉伸应变增强振动效应并可能破坏对称性。


<details>
  <summary>Details</summary>
Motivation: 研究SiV⁰中心在不同应变条件下的行为，理解其作为量子发射器的应变可调性，探索其在高压环境下的光稳定性和性能变化。

Method: 使用第一性原理密度泛函理论计算SiV⁰中心在-80到180 GPa有效静水压力范围内的各向同性应变响应，分析结构不稳定性、Jahn-Teller效应、振动耦合和光学性质变化。

Result: 压缩应变使零声子线蓝移，Eg声子硬化，抑制振动不稳定性，减少Jahn-Teller淬灭，增加自旋-轨道分裂和暗-亮振动能隙；拉伸应变增强振动效应，在临界应变时引发对称性破缺；在对称性保持区域，宇称保持良好，各向同性应变不会激活暗跃迁；在高压下辐射寿命增加。

Conclusion: SiV⁰中心是受对称性保护的应变可调量子发射器，可在多兆巴等效压力范围内工作，建立了光学和自旋观测量与各向同性应变的校准关系，为高压量子技术应用提供了基础。

Abstract: The neutral silicon--vacancy (SiV$^{0}$) center in diamond combines inversion symmetry with optical emission, making it a robust quantum emitter resilient to stray electric fields. Using first-principles density-functional theory, we quantify its response to isotropic strain spanning strong compression and tensile regimes (effective hydrostatic pressures of approximately $-80$ to $180$~GPa). The coexistence of doubly degenerate $e_g$ and $e_u$ levels produces a structural instability captured by a quadratic product Jahn--Teller model. Under isotropic compression, the zero-phonon line blue-shifts nearly linearly while the $E_g$ phonon stiffens, suppressing vibronic instabilities and reducing Jahn--Teller quenching. Consequently, the Ham-reduced excited-state spin--orbit splitting increases substantially and the dark--bright vibronic gap widens. In contrast, isotropic tensile strain enhances vibronic effects and induces symmetry breaking beyond a critical strain, with tunneling-mediated dynamical averaging at the onset. Throughout the symmetry-preserving regime, parity remains well defined, so isotropic strain alone does not activate the dark transition. Charge-transition levels indicate photostability of the emission deep into the compressive regime, and near the highest photostable deformation ($\sim 100$~GPa), the radiative lifetime increases due to a reduced transition dipole moment despite the increasing optical energy. These trends yield compact calibration relations linking optical and spin observables to isotropic strain and establish SiV$^{0}$ as a symmetry-protected, strain-tunable quantum emitter operating into the multi-megabar-equivalent regime.

</details>


### [166] [Error-Tolerant Quantum State Discrimination: Optimization and Quantum Circuit Synthesis](https://arxiv.org/abs/2602.10731)
*Chien-Kai Ma,Bo-Hung Chen,Tian-Fu Chen,Dah-Wei Chiou,Jie-Hong Roland Jiang*

Main category: quant-ph

TL;DR: 提出两种容错量子态区分策略：CrossQSD（可调置信度的广义无歧义区分）和FitQSD（优化测量分布逼近理想无噪声情况），并提供统一混合目标框架，支持在最小误差区分和FitQSD间连续权衡。


<details>
  <summary>Details</summary>
Motivation: 开发在中等噪声下保持可靠性能的容错量子态区分策略，解决实际量子设备中噪声对量子态区分性能的影响问题。

Method: 1. CrossQSD：将无歧义区分推广为可调置信度边界，平衡准确性和效率；2. FitQSD：优化测量结果分布以逼近理想无噪声情况；3. 统一混合目标框架：在最小误差区分和FitQSD间连续插值；4. 将优化问题表述为凸规划，通过规范凸规划或半定规划高效求解；5. 基于改进的Naimark扩张和等距合成的电路合成框架，实现硬件高效实现。

Result: 开发了开源工具包，自动化完整优化和合成工作流程，为在当前量子设备上实现量子态区分提供了实用途径，显著减少了量子比特和门资源需求。

Conclusion: 提出的容错量子态区分策略和统一框架能够在噪声环境下保持可靠性能，通过高效优化算法和硬件高效实现，为实际量子设备上的量子态区分应用提供了实用解决方案。

Abstract: We develop error-tolerant quantum state discrimination(QSD) strategies that maintain reliable performance under moderate noise. Two complementary approaches are proposed: CrossQSD, which generalizes unambiguous discrimination with tunable confidence bounds to balance accuracy and efficiency, and FitQSD, which optimizes the measurement outcome distribution to approximate that of the ideal noiseless case. Furthermore, we provide a unified hybrid-objective QSD framework that continuously interpolates between minimum-error discrimination (MED) and FitQSD, allowing flexible trade-offs among competing objectives. The associated optimization problems are formulated as convex programs and efficiently solved via disciplined convex programming or, in many cases, semidefinite programming. Additionally, a circuit synthesis framework based on a modified Naimark dilation and isometry synthesis enables hardware-efficient implementations with substantially reduced qubit and gate resources. An open-source toolkit automates the full optimization and synthesis workflow, providing a practical route to QSD on current quantum devices.

</details>


### [167] [Efficient Operator Selection and Warm-Start Strategy for Excitations in Variational Quantum Eigensolvers](https://arxiv.org/abs/2602.10776)
*Max Haas,Thierry N. Kaldenbach,Thomas Hammerschmidt,Daniel Barragan-Yani*

Main category: quant-ph

TL;DR: 提出一种高效制备电子基态的新方法，结合ExcitationSolve优化器和变分量子本征求解器算子选择技术，通过单次扫描算子池构建近似基态，减少计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统量子化学计算中，基态制备的计算复杂度高，需要多次迭代优化。本文旨在开发一种更高效的方法来减少计算资源需求，推动量子优势在量子化学中的实现。

Method: 结合ExcitationSolve优化器和能量排序等变分量子本征求解器算子选择方法，通过经典预处理选择相关算子，采用单次扫描算子池构建近似基态，并整合单变分参数耦合交换算子减少CNOT操作数。

Result: 经验观察显示，该方法相比现有最先进方法实现了二次收敛加速，显著减少了计算复杂度，同时减少了所需的CNOT操作数量。

Conclusion: 该方法提供了一种计算高效的基态制备协议，通过结合优化技术和算子选择策略，在量子化学计算中实现了显著的性能提升，推动了量子优势的实现。

Abstract: We present a novel approach for efficient preparation of electronic ground states, leveraging the optimizer ExcitationSolve [Jäger et al., Comm. Phys. (2025)] and established variational quantum eigensolver-based operator selection methods, such as Energy Sorting. By combining these tools, we demonstrate a computationally efficient protocol that enables the construction of an approximate ground state from a unitary coupled cluster ansatz via a single sweep over the operator pool. Utilizing efficient classical pre-processing to select the majority of relevant operators, this approach reduces the computational complexity associated with traditional optimization methods. Furthermore, we show that this method can be seamlessly integrated with one-variational-parameter couple exchange operators, thereby further reducing the number of required CNOT operations. Overall, we empirically observe a quadratic convergence speedup beyond state-of-the-art methods, advancing the realization of quantum advantage in quantum chemistry.

</details>


### [168] [Emulation of large-scale qubit registers with a phase space approach](https://arxiv.org/abs/2602.10830)
*Christian de Correc,Denis Lacroix,Corentin Bertrand*

Main category: quant-ph

TL;DR: 该论文提出了一种基于相空间的量子比特寄存器连续时间演化模拟方法，通过独立平均场轨迹的统计系综，将量子涨落/关联替换为经典涨落，实现了系统规模的二次方复杂度，可模拟数千量子比特。


<details>
  <summary>Details</summary>
Motivation: 传统量子模拟方法受限于系统规模，难以模拟大规模量子比特寄存器。需要开发一种能够在经典计算机上高效模拟大规模量子系统演化的方法。

Method: 采用相空间方法，基于独立平均场轨迹的统计系综。在量子比特层面引入平均场，将量子涨落和关联替换为经典涨落。该方法最坏情况下仅需系统规模的二次方计算成本。

Result: 方法可模拟多达2000个量子比特，在单量子比特可观测量演化方面提供定性准确的描述。在k-局域横向场伊辛模型上进行了基准测试，涵盖从局域到全对全相互作用、从弱耦合到强耦合的各种系统。还展示了在2D和3D伊辛模型上的模拟能力。

Conclusion: 该方法为大规模量子比特寄存器模拟提供了高效工具，特别适用于单量子比特可观测量分析，但在多量子比特可观测量预测方面能力较弱。可作为与限于小规模量子比特技术对比的有用参考。

Abstract: A phase-space approach is used and benchmarked for the simulation of the continuous-time evolution of large registers of qubits. It is based on a statistical ensemble of independent mean-field trajectories, where mean-field is introduced at the level of the qubits, substituting quantum fluctuations/correlations with classical ones. The approach only involves at worse a quadratic cost in the system size, allowing to simulate up to several thousands of qubits on a classical computer. It provides qualitatively accurate description of one-qubit observables evolutions, making it a useful reference in comparison to techniques limited to small qubit numbers. The predictive power is however less robust for multi-qubits observables. We benchmark the method on the $k$-local transverse-field Ising model (TFIM), considering a large variety of systems ranging from local to all-to-all interactions, and from weak to strong coupling regimes, with up to 2000 qubits. To showcase the versatility of the approach, simulations on 2D and 3D Ising models are also made.

</details>


### [169] [Mixed-State Topology in Non-Hermitian Systems](https://arxiv.org/abs/2602.10831)
*Shou-Bang Yang,Pei-Rong Han,Wen Ning,Fan Wu,Zhen-Biao Yang,Shi-Biao Zheng*

Main category: quant-ph

TL;DR: 该论文研究了非厄米系统中混合态的拓扑性质，使用Uhlmann相和热Uhlmann-Chern数揭示了与纯态不同的拓扑特征，并扩展到高维系统验证高阶混合态拓扑。


<details>
  <summary>Details</summary>
Motivation: 目前非厄米拓扑研究主要关注零温纯态，而混合态的拓扑性质尚未充分探索。本文旨在填补这一空白，研究非厄米系统中混合态的拓扑特性。

Method: 使用Uhlmann相和热Uhlmann-Chern数，通过Uhlmann连接在特定温度下构建拓扑不变量，研究二维非厄米系统的混合态拓扑，并扩展到三维阿贝尔和四维非阿贝尔系统。

Result: 揭示了混合态拓扑与纯态对应物的显著不同特征，验证了高维非厄米系统中的高阶混合态拓扑性质。

Conclusion: 该研究为非厄米物理中混合态拓扑性质的探索提供了概念和实践途径，扩展了非厄米拓扑的研究范围。

Abstract: Non-Hermitian (NH) systems, due to the existence of exceptional point (or ring, surface), exhibit exotic topological features which are inaccessible with the Hermition ones. Current studies on NH topology mainly focus on pure states at zero temperature, while those on mixed states remain largely unexplored. In this work, we investigate the topological properties of mixed states in two-dimentional NH systems, by use of the Uhlmann phase and the thermal Uhlmann-Chern number which are structured via the Uhlmann connection at specific temperatures, revealing distinct topological features compared to their pure state counterparts. We further extend our study to the mixed states in the three-dimensional Abelian and four-dimentional non-Abelian NH systems and verify the high-order mixed-state topology. Our study provides a conceptual and practical pathway for exploring topological properties in the mixed-state regime of NH physics.

</details>


### [170] [Multi-spin control from one-spin pulses](https://arxiv.org/abs/2602.10861)
*Suzanne Lim,Bowen Guo,Abi Turner,Charles Buchanan,Andrew Baldwin,Jonathan A. Jones*

Main category: quant-ph

TL;DR: 提出一个紧凑框架，使用单自旋-1/2优化的RF脉冲来控制弱耦合自旋系统，避免昂贵的多自旋优化计算


<details>
  <summary>Details</summary>
Motivation: 控制弱耦合自旋系统通常需要计算昂贵的多自旋优化，这限制了实际应用。需要一种更高效的方法来实现多自旋控制。

Method: 创建具有固定"主动"演化时间的GRAPE脉冲，使用单自旋-1/2方法，逐个自旋进行脉冲激发。通过在整个偏移范围内统一执行这种形式（"带模式"脉冲），精确控制整个系统的化学位移和标量耦合演化。

Result: 成功构建了带模式脉冲和连续辐照的联合INEPT（JINEPT），实现了带选择性转移Iz→2IzSz。该方法在Seedless软件中实现，能够快速生成此类脉冲并分析任意脉冲的模式形式。

Conclusion: 该框架实现了稳健的多自旋控制，无需进行多自旋优化，为弱耦合自旋系统的控制提供了一种高效实用的解决方案。

Abstract: Controlling ensembles of weakly coupled spins typically requires computationally expensive multispin optimisations. We present a compact framework that enables control of weakly coupled spin systems (of any spin), but using RF pulses optimised for a single spin-1/2. We do this by explicitly creating a GRAPE pulse with fixed 'active' evolution times using single spin-1/2 methods, and pulsing on one spin at a time. By enforcing this form uniformly across offsets ('band-schematic' pulses),chemical shift and scalar coupling evolution of the entire system can be precisely controlled. We demonstrate the approach by constructing band-schematic pulses and a continuously irradiated joint INEPT (JINEPT) that achieves band-selective transfer $I_z \rightarrow 2I_zS_z$. The framework is implemented in the software Seedless, which both rapidly generates such pulses and analyses the schematic form of arbitrary pulses, enabling robust multi-spin control, without multi-spin optimisation.

</details>


### [171] [Photon Anti-Bunching and Quantum Non-Gaussianity from High-Harmonic Generation](https://arxiv.org/abs/2602.10882)
*David Theidel,Mackrine Nahra,Ilya Karuseichyk,Houssna Griguer,Mateusz Weis,Hamed Merdji*

Main category: quant-ph

TL;DR: 该研究证实高次谐波产生可作为量子光学资源平台，通过测量半导体中高次谐波的点击统计，验证了其非经典特性，包括压缩、纠缠和反聚束光子统计，并实现了量子非高斯态的制备。


<details>
  <summary>Details</summary>
Motivation: 探索高次谐波产生这一新兴光子平台的非经典特性，验证其作为量子技术资源平台的潜力。量子技术需要能够产生复杂非经典态的平台来实现各种应用。

Method: 测量半导体中三个双位数阶高次谐波的点击统计，评估见证算符来认证生成态的非经典性。使用纠缠高斯态模型进行数值态优化以匹配多个可观测量。进行阶间预示测量来工程化发射的量子态。

Result: 证实相干激光驱动的高阶谐波具有压缩和纠缠特性。预示态显示出反聚束光子统计特性。成功见证量子非高斯态的生成，这是量子信息处理的重要资源。

Conclusion: 高次谐波产生被确立为生成量子光学资源的平台，能够产生具有压缩、纠缠和反聚束等非经典特性的量子态，包括量子非高斯态这一重要量子信息资源。

Abstract: Quantum technologies are powered by platforms to generate complex non-classical states of matter or light to realize applications. We investigate the non-classical properties of high-harmonic generation in semiconductors, an emerging photonic platform. Measuring the click statistics of three double-digit orders, we evaluate witness operators to certify the non-classicality of the generated states. We show that higher-order harmonics driven by a coherent laser are squeezed and entangled. The properties of the emission are well retrieved with an entangled Gaussian state model, obtained by numerical state optimization to multiple observables. Additionally, we perform inter-order heralded measurements to engineer the quantum state of the emission. The heralded states have distinct properties, showing anti-bunched photon statistics. Further, we witness the generation of a quantum non-Gaussian state, a resource highly relevant for quantum information. With this, we establish high-harmonic generation as a platform for generating quantum optical resources.

</details>


### [172] [Quantum Optimization in Loc(Q)ation Science: QUBO Formulations, Benchmark Problems, and a Computational Study](https://arxiv.org/abs/2602.10951)
*Felix P. Broesamle,Stefan Nickel*

Main category: quant-ph

TL;DR: 该论文为离散优化中的几个基础问题开发了QUBO（二次无约束二进制优化）公式，包括离散有序中值问题（DOMP），并进行了量子算法（QAOA、WS-QAOA）和经典启发式方法的计算研究。


<details>
  <summary>Details</summary>
Motivation: 量子计算和量子硬件的进步增强了量子方法在离散优化中的实际相关性。位置科学、网络设计和物流是离散优化的核心应用领域，具有高实际影响和重大计算挑战。需要为这些领域的基础问题开发QUBO公式，作为评估量子算法和硬件的基准问题。

Method: 1. 为位置科学、网络设计和物流领域的多个基础问题开发QUBO公式，包括DOMP的非线性整数公式。2. 推导确保QUBO公式与其底层整数规划等价性的惩罚参数紧界。3. 使用QAOA、WS-QAOA和经典启发式方法对p-中值问题和固定费用设施选址问题的QUBO实例进行综合计算研究。4. 基于线性规划松弛为WS-QAOA引入两种有效的预热启动策略。

Result: 1. 成功开发了多个基础离散优化问题的QUBO公式。2. 推导了惩罚参数的紧界，确保QUBO公式与原始整数规划的等价性。3. 通过计算研究比较了量子算法和经典方法的性能。4. 提出的WS-QAOA预热启动策略有效提升了算法性能。

Conclusion: 该工作为离散优化中的核心问题提供了统一的QUBO建模框架，这些公式不仅具有建模意义，还可作为评估量子算法和硬件的代表性基准问题。推导的惩罚参数紧界和提出的WS-QAOA预热启动策略对量子优化算法的实际应用具有重要价值。

Abstract: Recent advances in quantum computing and the increasing availability of quantum hardware have substantially enhanced the practical relevance of quantum approaches to discrete optimization. Among these, the Quadratic Unconstrained Binary Optimization (QUBO) formulation provides a unifying modeling framework for a broad class of $\mathbf{NP}$-hard problems and is naturally suited to quantum computing and quantum-inspired algorithms. Location science, network design, and logistics represent core application domains of discrete optimization, combining high practical impact with substantial computational challenges. In this work, we develop QUBO formulations for several fundamental problems in these domains, including a nonlinear integer formulation of the Discrete Ordered Median Problem (DOMP). Beyond their modeling relevance, these QUBO formulations serve as representative benchmark problems for assessing quantum algorithms and quantum hardware. We further derive a tight bound for the penalty parameter ensuring equivalence between the QUBO formulation and its underlying integer program. Finally, we conduct a comprehensive computational study using QAOA, WS-QAOA, and classical heuristics for QUBO instances of the $p$-Median Problem and the Fixed-Charge Facility Location Problem (FCFLP), and introduce two effective warm-start strategies for WS-QAOA based on its linear programming relaxation.

</details>


### [173] [Improving Quantum Multi-Objective Optimization with Archiving and Substitution](https://arxiv.org/abs/2602.10952)
*Linus Ekstrøm,Takafumi Hosogi,Xavier Bonet-Monroig,Hao Wang,Thomas Bäck,Sebastian Schmitt*

Main category: quant-ph

TL;DR: 该论文改进了量子多目标优化算法，引入帕累托档案和支配解替换机制，在RMNK-landscapes测试基准上优化参数，并与经典算法NSGA-II/III比较，显示在复杂问题上具有潜力。


<details>
  <summary>Details</summary>
Motivation: 工业应用中多目标优化问题普遍存在，量子计算可能为此类问题带来指数级优势。研究者希望探索变分量子多目标优化算法是否能在多目标优化问题上实现量子优势。

Method: 1) 改进量子多目标优化算法，引入帕累托档案和支配解替换机制；2) 提出使用RMNK-landscapes作为统一测试基准；3) 设计通用的经典到量子映射方法；4) 进行数值超参数调优；5) 与经典算法NSGA-II/III进行比较。

Result: 1) 改进后的算法在超体积收敛性上明显提升，但增加了量子计算和经典计算成本；2) 通过超参数调优显著提升了算法性能；3) 在小规模实例上与NSGA-II/III算法结果相当；4) 在更复杂问题上可能比经典算法更有优势。

Conclusion: 经过精心调优的量子多目标优化算法在复杂多目标优化问题上可能比经典算法更具优势，为量子计算在多目标优化领域的应用提供了有前景的方向。

Abstract: Finding optimal solutions of conflicting objectives is a daily matter in many industrial applications, with multi-objective optimization trying to find the best solutions to them. The advent of quantum computing has led to researchers wondering if the promised exponential advantage can be obtained for these problems by variational quantum multi-objective optimization (QMOO) algorithm. Here, we improve it by introducing a Pareto Archive and dominated solutions substitution, clearly improving in hyper-volume convergence at additional quantum and classical cost. We propose the use of RMNK-landscapes as a unifying testbed for benchmarking QMOO, as it is common in classical multi-objective field. By devising a generic classical-to-quantum mapping of these landscapes, we perform a numerical hyperparameter tuning of QMOO, significantly enhancing its performance. Finally, we compare QMOO against well-known classical solvers for multi-objective tasks, NSGA-II/III, showing comparable results in small instances. Our results demonstrate that QMOO, when carefully tuned for the task at hand, might be advantageous on harder problems than its classical counterparts.

</details>


### [174] [Recirculating Quantum Photonic Networks for Fast Deterministic Quantum Information Processing](https://arxiv.org/abs/2602.11033)
*Emil Grovn,Matias Bundgaard-Nielsen,Jesper Mørk,Dirk Englund,Mikkel Heuck*

Main category: quant-ph

TL;DR: 提出循环量子光子网络（RQPN）架构，通过同时处理所有量子比特来减少量子信息处理时间，降低对非线性相互作用速率的要求，实现更快的多量子比特门和量子纠错。


<details>
  <summary>Details</summary>
Motivation: 光子量子信息处理面临的主要挑战是在退相干和损耗机制发生之前完成关键变换。传统方法通过增强非线性相互作用来应对，但本文采用互补的架构方法，旨在最小化量子信息处理任务的时间，从而降低对非线性相互作用速率的要求。

Method: 提出循环量子光子网络（RQPN），由全连接的非线性腔阵列组成，具有动态可控的波导耦合。该网络通过捕获光子输入态、在腔之间循环光子、然后释放光子输出态来处理信息。通过同时处理所有量子比特来加速操作。

Result: 1. 三量子比特Toffoli门的处理速度比单量子比特和双量子比特分解方法更快；2. 单光子损耗的无测量纠错实现高达7倍的加速，硬件效率显著提高；3. 单个硬件高效的循环架构大幅减少了多量子比特门和量子纠错的时间开销。

Conclusion: 循环量子光子网络架构通过最小化量子信息处理时间，显著降低了确定性光子量子信息处理的实验实现门槛，为光子量子计算提供了有前景的架构解决方案。

Abstract: A fundamental challenge in photonics-based deterministic quantum information processing is to realize key transformations on time scales shorter than those of detrimental decoherence and loss mechanisms. This challenge has been addressed through device-focused approaches that aim to increase nonlinear interactions relative to decoherence rates. In this work, we adopt a complementary architecture-focused approach by proposing a recirculating quantum photonic network (RQPN) that minimizes the duration of quantum information processing tasks, thereby reducing the requirements on nonlinear interaction rates. The RQPN consists of a network of all-to-all connected nonlinear cavities with dynamically controlled waveguide couplings, and it processes information by capturing a photonic input state, recirculating photons between the cavities, and releasing a photonic output state. We demonstrate the RQPN's architectural advantage through two examples: first, we show that processing all qubits simultaneously yields faster operations than single- and two-qubit decompositions of the three-qubit Toffoli gate. Second, we demonstrate implementations of a measurement-free correction for single-photon loss, achieving up to seven-fold speedups and significantly improved hardware efficiency relative to state-of-the-art architecture proposals. Our work shows that a single hardware-efficient recirculating architecture substantially reduces the temporal overhead of multi-qubit gates and quantum error correction, thereby lowering the barrier to experimental realizations of deterministic photonic quantum information processing.

</details>


### [175] [Characterizing Trainability of Instantaneous Quantum Polynomial Circuit Born Machines](https://arxiv.org/abs/2602.11042)
*Kevin Shen,Susanne Pielawa,Vedran Dunjko,Hao Wang*

Main category: quant-ph

TL;DR: IQP-QCBM量子生成模型在MMD损失函数下，通过分析梯度方差证明了在特定初始化条件下可避免梯度消失问题，同时稀疏IQP家族能产生经典难处理的概率分布。


<details>
  <summary>Details</summary>
Motivation: 研究IQP-QCBM量子生成模型的训练可行性，特别是其是否受梯度消失（贫瘠高原）问题影响，以及训练可行性与量子优势区域的重叠关系。

Method: 分析MMD损失函数偏导数的方差，推导闭式表达式和上下界；研究均匀初始化下梯度消失与生成器集合和核谱的关系；探索低权重偏置核在结构化拓扑中的表现；证明小方差高斯初始化能确保多项式尺度梯度。

Result: 发现贫瘠高原问题取决于生成器集合和核谱；低权重偏置核在结构化拓扑中可避免指数梯度抑制；小方差高斯初始化在温和条件下确保多项式尺度梯度；稀疏IQP家族能产生经典难处理且可训练的概率分布。

Conclusion: IQP-QCBM在特定条件下可避免贫瘠高原问题，稀疏IQP家族既能产生经典难处理的分布又能在初始化时保持可训练性，为量子生成模型的实用化提供了理论支持。

Abstract: Instantaneous quantum polynomial quantum circuit Born machines (IQP-QCBMs) have been proposed as quantum generative models with a classically tractable training objective based on the maximum mean discrepancy (MMD) and a potential quantum advantage motivated by sampling-complexity arguments, making them an exciting model worth deeper investigation. While recent works have further proven the universality of a (slightly generalized) model, the next immediate question pertains to its trainability, i.e., whether it suffers from the exponentially vanishing loss gradients, known as the barren plateau issue, preventing effective use, and how regimes of trainability overlap with regimes of possible quantum advantage. Here, we provide significant strides in these directions. To study the trainability at initialization, we analytically derive closed-form expressions for the variances of the partial derivatives of the MMD loss function and provide general upper and lower bounds. With uniform initialization, we show that barren plateaus depend on the generator set and the spectrum of the chosen kernel. We identify regimes in which low-weight-biased kernels avoid exponential gradient suppression in structured topologies. Also, we prove that a small-variance Gaussian initialization ensures polynomial scaling for the gradient under mild conditions. As for the potential quantum advantage, we further argue, based on previous complexity-theoretic arguments, that sparse IQP families can output a probability distribution family that is classically intractable, and that this distribution remains trainable at initialization at least at lower-weight frequencies.

</details>


### [176] [Ergotropic Mpemba crossings in finite-dimensional quantum batteries](https://arxiv.org/abs/2602.11056)
*Triyas Sapui,Tanoy Kanti Konar,Aditi Sen De*

Main category: quant-ph

TL;DR: 量子Mpemba效应是远离平衡的初始态比靠近平衡的态弛豫更快的反直觉现象。本文在量子电池中引入ergotropic Mpemba crossing概念，研究其在振幅阻尼噪声下的条件，并扩展到非马尔可夫环境。


<details>
  <summary>Details</summary>
Motivation: 研究量子Mpemba效应在量子电池系统中的表现，特别是ergotropy（最大可提取功）轨迹交叉的现象，探索其物理起源和在不同噪声环境下的特性。

Method: 引入ergotropic Mpemba crossing概念，分析量子比特电池在振幅阻尼噪声下的条件，将ergotropy分解为相干和非相干分量，扩展到各向异性泡利噪声和非马尔可夫环境。

Result: 在振幅阻尼噪声下，EMC的出现由初始态的相对相干性决定；在泡利噪声下，由相干性和能量共同决定；非马尔可夫环境会产生奇数次Mpemba交叉；量子比特中EMC必然伴随态Mpemba交叉，但三能级系统中可能没有。

Conclusion: 量子Mpemba效应在量子电池中表现为ergotropy轨迹交叉，其物理机制在量子比特中主要由相干分量驱动，但在高维系统中机制不同，非马尔可夫环境会产生更复杂的交叉模式。

Abstract: The quantum Mpemba effect is a counterintuitive phenomenon in which a state initially farther from equilibrium relaxes more rapidly than one that starts nearer to equilibrium. In the context of finite-dimensional quantum batteries interacting with an environment, we introduce the notion of an ergotropic Mpemba crossing (EMC), defined by the intersection of ergotropy trajectories during the dynamics. For qubit batteries subjected to amplitude damping noise, we derive a condition for the occurrence of EMC in terms of the relative coherence of the initial states and fully characterize the region of state space that exhibits EMC with respect to a fixed reference state. Interestingly, our analysis reveals that under anisotropic Pauli noise, the emergence of EMC is jointly governed by the coherence and the energy of the initial states. To elucidate the physical origin of EMC, we decompose ergotropy into coherent and incoherent contributions and show that, in qubit systems, the coherent component plays a crucial role for EMC, an observation that strikingly does not extend to three-level batteries. Further, by extending our analysis to non-Markovian environments, we demonstrate that, unlike the Markovian case, non-Markovian dynamics can give rise to multiple Mpemba crossings, with the total number of crossings always being odd. Moreover, analyzing the connection between the EMC and the conventional state Mpemba effect reveals that, for qubits, an EMC necessarily entails a state Mpemba crossing while this correspondence breaks down for qutrits, where EMCs may arise without any state Mpemba crossing.

</details>


### [177] [Two-Level System Spectroscopy from Correlated Multilevel Relaxation in Superconducting Qubits](https://arxiv.org/abs/2602.11127)
*Tanay Roy,Xinyuan You,David van Zanten,Francesco Crisa,Sabrina Garattoni,Shaojiang Zhu,Anna Grassellino,Alexander Romanenko*

Main category: quant-ph

TL;DR: 提出一种基于多能级弛豫的固定频率transmon光谱方法，通过制备第二激发态并同时提取第一和第二激发态的T1，从相邻跃迁的衰减率相关性识别主导TLS并重构其频率漂移。


<details>
  <summary>Details</summary>
Motivation: Transmon量子比特的能量弛豫时间波动通常归因于介质和界面中的微观二能级系统(TLS)，但传统方法需要将量子比特或TLS调谐到共振才能隔离单个缺陷，这限制了研究。

Method: 开发了一种新型光谱方法：重复制备第二激发态，同时提取第一和第二激发态的弛豫时间T1，通过分析相邻跃迁衰减率之间的特征相关性来识别主导TLS并重构其频率漂移。

Result: 发现即使与量子比特跃迁失谐超过100MHz的TLS仍能显著影响弛豫，成功识别了一个或多个主导TLS并重构了它们的频率漂移随时间的变化。

Conclusion: 该方法为TLS光谱学提供了强大工具，无需通过磁通可调电感或AC-Stark位移来调谐transmon频率，能够研究远离共振的TLS对量子比特弛豫的影响。

Abstract: Transmon qubits are a cornerstone of modern superconducting quantum computing platforms. Temporal fluctuations of energy relaxation in these qubits are widely attributed to microscopic two-level systems (TLSs) in device dielectrics and interfaces, yet isolating individual defects typically relies on tuning the qubit or the TLS into resonance. We demonstrate a novel spectroscopy method for fixed-frequency transmons based on multilevel relaxation: repeated preparation of the second excited state and simultaneous $T_1$ extraction of the first and second excited states reveals characteristic correlations in the decay rates of adjacent transitions. From these correlations we identify one or more dominant TLSs and reconstruct their frequency drift over time. Remarkably, we find that TLSs detuned by $\gtrsim 100\,\mathrm{MHz}$ from the qubit transition can still significantly influence relaxation. The proposed method provides a powerful tool for TLS spectroscopy without the need to tune the transmon frequency, either via a flux-tunable inductor or AC-Stark shifts.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [178] [Critical spacetime crystals in continuous dimensions](https://arxiv.org/abs/2602.10185)
*Christian Ecker,Florian Ecker,Daniel Grumiller,Tobias Jechtl*

Main category: gr-qc

TL;DR: 在任意连续维度D>3中数值构造了一族临界时空，推广了Choptuik的D=4解，研究了D维Schwarzschild-Tangherlini黑洞形成阈值处的球对称无质量标量场坍缩。


<details>
  <summary>Details</summary>
Motivation: 将Choptuik在四维时空中的临界现象研究推广到任意连续维度，探索临界时空性质随维度的变化规律，特别是当D接近3时的极限行为。

Method: 数值构造D>3连续维度中的临界时空解，计算回波周期和Choptuik指数作为D的连续函数，支持以1/D和D-3展开的解析分析，并扩展到二维膨胀引力。

Result: 回波周期在D≈3.76处有最大值；D=4时Δ=3.445453，γ=0.373961；D=5时Δ=3.22176，γ=0.41322；当D→3⁺时，回波周期和Choptuik指数趋于零。

Conclusion: 成功构造了D>3连续维度中的临界时空晶体，揭示了临界参数随维度的变化规律，为小(D-3)展开提供了基础，与广义相对论的大D展开形成平行。

Abstract: We numerically construct a one-parameter family of critical spacetimes in arbitrary continuous dimensions D>3. This generalizes Choptuik's D=4 solution to spherically symmetric massless scalar-field collapse at the threshold of D-dimensional Schwarzschild-Tangherlini black hole formation. We refer to these solutions, which share the discrete self-similarity of their four-dimensional counterpart, as critical spacetime crystals. Our main results are the echoing period and Choptuik exponent of the crystals as continuous functions of D, with detailed data for the interval 3.05<D<5.5. Notably, the echoing period has a maximum near D=3.76. As a by-product, we recover the echoing periods and Choptuik exponents in D=4 (5): Delta=3.445453 (3.22176) and gamma=0.373961 (0.41322). We support these numerical results with analytical expansions in 1/D and D-3. They suggest that both the echoing period and Choptuik exponent vanish as D approaches 3 from above. This paves the way for a small-(D-3) expansion, paralleling the large-$D$ expansion of general relativity. We also extend our results to two-dimensional dilaton gravity.

</details>


### [179] [Accelerated expansion of the universe purely driven by scalar field fluctuations](https://arxiv.org/abs/2602.10223)
*Daniel Jiménez-Aguilar*

Main category: gr-qc

TL;DR: 标量场涨落可在空间闭合宇宙中驱动宇宙加速膨胀，条件是场的康普顿波长大于曲率半径


<details>
  <summary>Details</summary>
Motivation: 探索宇宙加速膨胀（暴胀和暗能量）的新机制，避免传统标量场模型需要精细调节势能的问题

Method: 在闭合宇宙几何中分析标量场量子涨落，要求场的康普顿波长超过宇宙曲率半径

Result: 标量场涨落本身可以产生有效的负压，驱动宇宙加速膨胀，为暴胀和暗能量提供新解释

Conclusion: 闭合宇宙中足够轻的玻色子气体可能通过量子涨落机制解释宇宙加速，为宇宙学提供新视角

Abstract: We show that scalar field fluctuations alone can drive cosmic acceleration, provided the universe is spatially closed and the Compton wavelength of the field exceeds the radius of curvature. This mechanism may open new perspectives on inflation and dark energy, which could arise from a gas of sufficiently light bosons in a closed universe.

</details>


### [180] [A class of $d$-dimensional regular black holes: Shadows, Thermodynamics and Gravitational collapse](https://arxiv.org/abs/2602.10269)
*A. Sadeghi,F. Shojai*

Main category: gr-qc

TL;DR: 研究一類可從多方球塌縮形成的正則黑洞，具有de Sitter核心，包含Bardeen和Hayward黑洞為特例，分析其能量條件、光子球、陰影、熱力學性質及塌縮過程。


<details>
  <summary>Details</summary>
Motivation: 探討從多方球塌縮形成的正則黑洞，這類黑洞具有de Sitter核心且包含已知黑洞模型為特例，研究其物理性質並與觀測比較。

Method: 使用非線性電動力學拉格朗日量描述d維黑洞，分析能量條件，解析研究光子球，數值計算陰影，建立熱力學性質，並推廣Oppenheimer-Snyder-Datt塌縮場景。

Result: 該類黑洞由磁荷源驅動，具有正則性，能量條件分析顯示特定性質，光子球和陰影計算可與觀測比較，熱力學性質完整，塌縮過程可描述表面、視界和事件視界的演化。

Conclusion: 成功建立了一類從多方球塌縮形成的正則黑洞模型，包含Bardeen和Hayward黑洞為特例，系統分析了其物理性質、熱力學行為和塌縮動力學，為正則黑洞研究提供了新框架。

Abstract: This paper examines a recently introduced class of regular black holes that can form from the collapse of a polytropic star with an arbitrary polytropic index. This class has a de Sitter core and reduces to the Bardeen and Hayward black holes when the polytropic index is chosen appropriately. We demonstrate that this class of black holes is sourced by a nonlinear electrodynamics Lagrangian in $d$ dimensions and that its regularity stems from the presence of magnetic charge. We analyze the energy conditions and study the photon spheres analytically and the shadows numerically. Then, we compare our results with observations. Additionally, we present the thermodynamic properties of this class of black holes, including their temperature, entropy, and heat capacity. We also examine their thermodynamic stability. Finally, we generalize the Oppenheimer-Snyder-Datt collapse scenario to this $d$-dimensional class of black holes and study stellar collapse into them, as well as the evolution of the star's surface, the apparent horizon, and the event horizon.

</details>


### [181] [The measurable impact of the 2pN spin-dependent accelerations on the jet precession of M87$^\ast$](https://arxiv.org/abs/2602.10311)
*Lorenzo Iorio*

Main category: gr-qc

TL;DR: 本文计算了克尔度规中无电荷、无自旋物体在谐和坐标系下的加速度至二阶后牛顿阶，发现了与黑洞角动量偶次和奇次幂成正比的新加速度项，并分析了其对轨道平面的进动效应。


<details>
  <summary>Details</summary>
Motivation: 受近期对银河系超大质量黑洞周围盘/喷流共进动的精确测量启发，需要更精确地理解克尔黑洞度规中的加速度效应，特别是与黑洞角动量相关的高阶项。

Method: 在谐和坐标系下解析计算克尔度规中无电荷、无自旋物体的加速度至二阶后牛顿阶，将结果转化为与坐标系无关的矢量形式，并用开普勒轨道元素扰动分析轨道效应，对轨道进行平均处理。

Result: 发现了与黑洞角动量偶次和奇次幂成正比的新加速度项，这些项在物质天体作为主星的情况下未知。轨道平面进动中，与黑洞角动量一次幂成正比且与光速四次方倒数成正比的项约为相应Lense-Thirring效应的20%。与黑洞自旋二次幂成正比的进动项也可测量。

Conclusion: 新发现的加速度项对黑洞周围轨道动力学有重要影响，其中一些进动效应在当前测量精度下可探测，为M87*等黑洞的喷流进动现象提供了更精确的理论框架。

Abstract: Motivated by recent accurate measurements of disk/jet coprecessions around some galactic supermassive black holes, the accelerations experienced by an uncharged, spinless object in the Kerr metric, written in harmonic coordinates, are analytically calculated up to the formal second post-Newtonian order. To such a level, some new accelerations make their appearance. They are proportional to even and odd powers of the hole's angular momentum. Their counterparts are not known where the primary is a material body. After expressing them in a coordinate-independent, vector form valid for any orientations of the hole's spin axis in space, their orbital effects are perturbatively worked out in terms of the particle's Keplerian orbital elements. The resulting expressions, averaged over one orbital revolution, are valid for generic shapes and inclinations of the orbit. The orbital plane's precession proportional to the first power of the hole's angular momentum and to the reciprocal of the fourth power of the speed of light amounts to about twenty per cent of the corresponding Lense-Thirring effect. The latter is believed to be the cause of the accurately measured disk/jet precessional phenomenology, currently measured to a few per cent accuracy. Although at a lesser extent, also the precession proportional to the second power of the hole's spin and to the reciprocal of the fourth power of the speed of light is measurable. Allowed domains in the parameter space of the jet precession around M87$^\ast$ are displayed.

</details>


### [182] [Cosmological production of dark matter in the Universe and in the laboratory](https://arxiv.org/abs/2602.10331)
*Álvaro Parra-López*

Main category: gr-qc

TL;DR: 该论文研究弯曲时空量子场论中的宇宙学粒子产生机制，包括作为暗物质产生机制和通过玻色-爱因斯坦凝聚体进行模拟实验，重点关注膨胀期间几何驱动的粒子产生。


<details>
  <summary>Details</summary>
Motivation: 量子引力理论尚未完善，研究弯曲背景下的量子场论能为早期宇宙提供重要见解。探索宇宙学粒子产生作为暗物质机制，并利用模拟实验增强对弯曲时空中量子效应的理解。

Method: 分为四部分：I.建立理论框架（宇宙学、膨胀、模拟引力原理）；II.分析各种膨胀模型中的粒子产生；III.探索BEC实验，将声子映射到膨胀宇宙中的标量场；IV.解决量子真空模糊性和非绝热转变影响。

Result: 标量和矢量场通过快子不稳定性可以解释观测到的暗物质丰度；BEC实验能够重建膨胀历史，将粒子产生重新解释为散射问题，并提出测量产生对之间纠缠的方法。

Conclusion: 宇宙学粒子产生作为暗物质机制是可行的，模拟实验能够增强我们对弯曲时空中量子效应的理解，为研究早期宇宙物理提供了有力工具。

Abstract: This thesis investigates cosmological particle production within Quantum Field Theory in Curved Spacetimes, both as a dark matter mechanism and through analog simulations using Bose-Einstein condensates. While a full theory of Quantum Gravity remains elusive, studying quantum fields on curved backgrounds provides essential insights into the early Universe. We focus on how dynamical spacetimes, particularly during inflation, generate particles from spectator fields influenced solely by geometry.
  The work is divided into four parts. Part I establishes the theoretical framework, covering cosmology, inflation, and the principles of analog gravity. Part II analyzes particle production in various inflationary models, showing that scalar and vector fields can account for observed dark matter abundance, especially through tachyonic instabilities. Part III explores BEC experiments, mapping phonons to scalar fields in expanding universes. We demonstrate the reconstruction of expansion histories, reinterpret production as a scattering problem, and propose methods to measure entanglement between produced pairs. Finally, Part IV addresses quantum vacuum ambiguities and the impact of non-adiabatic transitions during the "switch-on" and "switch-off" of expansion.
  Ultimately, this work highlights the viability of cosmological particle production for dark matter and the power of analog experiments to enhance our understanding of quantum effects in curved spacetimes.

</details>


### [183] [Spacetime of rotating black holes surrounded by massive scalar charges](https://arxiv.org/abs/2602.10462)
*Adrian Ka-Wai Chung*

Main category: gr-qc

TL;DR: 该论文开发了谱方法，用于精确构造带有非最小耦合质量标量场的旋转黑洞时空，可处理高达a≤0.8的自旋参数，并计算了相关物理量的修正。


<details>
  <summary>Details</summary>
Motivation: 质量标量荷在广义相对论和粒子物理标准模型的扩展中普遍存在，但现有方法难以精确构造带有质量标量场的旋转黑洞时空，这限制了通过电磁观测和引力波探测黑洞来探索基本标量自由度的能力。

Method: 采用谱方法构造旋转黑洞时空，处理了轴子-膨胀子、动力学Chern-Simons和标量Gauss-Bonnet耦合，获得了标量场和相关度规修正的领头阶解，能够解析康普顿波长短至黑洞质量5倍的标量场。

Result: 方法精度达到残差误差≲10⁻⁵（标量场）和≲10⁻³（度规修正），计算了事件视界表面引力和角速度的领头阶偏移，为计算准正规模提供了重要信息。

Conclusion: 该研究为将质量标量荷纳入黑洞的电磁观测和引力波探测铺平了道路，可能实现对基本标量自由度的新探测。

Abstract: Massive scalar charges are ubiquitous in extensions to General Relativity and the Standard Model in particle physics. We describe spectral methods which can accurately construct the spacetime of rotating black holes with dimensionless spin up to $a \leq 0.8$ surrounded by massive scalar fields nonminimally coupled to spacetime curvature. We consider axi dilaton, dynamical Chern Simons, and scalar Gauss Bonnet couplings, and obtain leading order solutions for both the scalar field and the associated metric modifications. Our method accurately resolves massive scalar fields with Compton wavelengths as short as 5 times the black hole mass, achieving residual errors $\lesssim 10^{-5}$, and yields the corresponding leading order spacetime modifications with residual errors $\lesssim 10^{-3}$. Using the constructed spacetimes, we computes the leading-order shifts in the surface gravity and the angular velocity of the event horizon, important information for computing the quasinormal modes. These results pave the way to incorporate massive scalar charges into electromagnetic observations and gravitational-wave detections of black holes, potentially enabling new probes of fundamental scalar degrees of freedom.

</details>


### [184] [Geometric properties of slowly rotating black holes embedded in matter environments](https://arxiv.org/abs/2602.10579)
*Sayak Datta,Chiranjeeb Singha*

Main category: gr-qc

TL;DR: 研究构建了考虑周围暗物质晕旋转效应的缓慢旋转黑洞自洽时空模型，分析了环境物质对黑洞几何、轨道动力学和引力波观测的影响。


<details>
  <summary>Details</summary>
Motivation: 真实天体物理黑洞周围存在暗物质和重子物质，这些环境物质会扰动时空几何，影响强场引力测试的精度，特别是未来空间引力波探测器对极端质量比旋进的观测。

Method: 采用缓慢旋转近似，将暗物质晕建模为各向异性流体，构建自洽的缓慢旋转黑洞-环境物质时空模型，保留解析透明性，捕获主导阶自旋和参考系拖曳效应。

Result: 暗物质晕的存在和旋转导致黑洞几何偏离真空解，改变惯性系拖曳、赤道圆轨道、光环、最内稳定圆轨道、径向和垂直径向频率，产生轨道运动常数和径向共振位置的系统偏移，特别是径向频率比出现非单调行为（如局部极小值）。

Conclusion: 环境物质和旋转效应会在强场引力探针（特别是极端质量比旋进）中留下可观测印记，为将真实天体物理环境纳入未来空间引力波探测器的强场引力测试提供了最小可扩展框架。

Abstract: Astrophysical black holes are embedded in surrounding dark and baryonic matter that can measurably perturb the spacetime. We construct a self-consistent spacetime describing a slowly rotating black hole embedded in an external matter distribution, modeling the surrounding dark matter halo as an anisotropic fluid. Working within the slow-rotation approximation, we capture leading-order spin and frame-dragging effects while retaining analytic transparency. We show that the presence and rotation of the halo induce distinct deviations from the vacuum black hole geometry, modifying inertial frame dragging, equatorial circular geodesics, the light ring, the innermost stable circular orbit, and radial and vertical epicyclic frequencies. These effects produce systematic shifts in orbital constants of motion and the locations of epicyclic resonances. In particular, the epicyclic frequency ratios develop nonmonotonic behavior, such as local minima. We further demonstrate that these features depend on the angular velocity of the surrounding fluid, reflecting the interplay between environmental gravity and frame dragging. Our results demonstrate that environmental and rotational effects can leave observable imprints on precision strong-field probes, particularly extreme mass-ratio inspirals, where small corrections accumulate over many orbital cycles. This work provides a minimal and extensible framework for incorporating realistic astrophysical environments into strong-field tests of gravity with future space-based gravitational-wave detectors.

</details>


### [185] [Quantum Cosmology in $f(R, T)$ Theory with Schutz's Perfect Fluid](https://arxiv.org/abs/2602.10723)
*Serkan Doruk Hazinedar,Yaghoub Heydarzade,Shahram Jalalzadeh*

Main category: gr-qc

TL;DR: 该论文研究f(R,T)引力理论中的量子宇宙学，通过Schutz完美流体形式主义提取时间参数，推导出宇宙波函数，分析物质-几何耦合在量子宇宙动力学中的作用。


<details>
  <summary>Details</summary>
Motivation: f(R,T)引力理论通过Ricci标量R和能量-动量张量迹T的耦合扩展了广义相对论，这种物质-几何耦合可能解释宇宙晚期加速膨胀而无需暗能量。本文聚焦早期宇宙，研究相应的量子宇宙动力学。

Method: 在FLRW宇宙背景下，采用Schutz完美流体形式主义从物质部分本身提取时间参数。推导引力哈密顿量、正则动量和势能，得到相应的Schrödinger-Wheeler-DeWitt方程。针对特定的f(R,T)形式求解宇宙波函数。

Result: 获得了特定f(R,T)形式下的宇宙波函数，并与之前的f(R)和f(R,T)模型研究进行比较，突出了物质-几何耦合在量子宇宙动力学出现中的作用。

Conclusion: f(R,T)引力理论中的物质-几何耦合使物质成为时空动力学和宇宙时间演化的积极参与者，这种耦合在量子宇宙学框架中起着关键作用，为理解早期宇宙提供了新的视角。

Abstract: The $f(R, T)$ theory of gravity extends general relativity (GR) by allowing the gravitational Lagrangian to depend on both the Ricci scalar $R$ and the trace of the energy-momentum tensor $T$. The resulting matter-geometry coupling introduces additional dynamical effects that may account for the late-time acceleration of the universe without invoking dark energy. In the present work, we focus instead on the early-time regime and investigate the corresponding quantum cosmological dynamics. We analyze a Friedmann--Lemaitre--Robertson--Walker (FLRW) universe within the $f(R, T)$ framework, employing Schutz's perfect fluid formalism to extract a time parameter emerging from the matter sector itself. This approach is particularly well motivated in $f(R, T)$ gravity, where the coupling between geometry and the energy-momentum tensor's trace makes matter an active participant in the dynamics of spacetime and the evolution of cosmic time. The gravitational Hamiltonian, canonical momenta, and potential are derived, leading to the corresponding Schrödinger--Wheeler--DeWitt (SWDW) equation. The wave function of the universe is obtained for specific forms of $f(R, T)$, and the results are compared with previous studies in $f(R)$ and $f(R, T)$ models, highlighting the role of matter-geometry coupling in the emergence of quantum cosmological dynamics.

</details>


### [186] [From mergers to collapse: scalarisation dynamics in neutron star binaries](https://arxiv.org/abs/2602.10755)
*Llibert Aresté Saló,Ricard Aguilera-Miret,Miguel Bezares,Thomas P. Sotiriou*

Main category: gr-qc

TL;DR: 首次在爱因斯坦-标量-高斯-博内引力理论中，采用移动奇点方法对双中子星并合进行完全非线性演化，发现新的并合后现象


<details>
  <summary>Details</summary>
Motivation: 探索超越广义相对论的效应，这些效应在中子星流体的非线性动力学中得到增强。研究标量与高斯-博内不变量之间的线性和二次耦合对双中子星并合的影响。

Method: 在爱因斯坦-标量-高斯-博内引力理论中，采用移动奇点方法对双中子星并合进行完全非线性数值演化。研究标量与高斯-博内不变量之间的线性和二次型耦合。

Result: 发现了新的并合后现象：1) 增强了长寿命超大质量中子星残骸的快速坍缩；2) 在某些情况下，残骸由于不同的标量化不稳定性而发展出标量场构型。

Conclusion: 这项研究开启了探索超越广义相对论效应的新途径，这些效应在中子星流体的非线性动力学中得到显著增强，为理解标量-高斯-博内引力中的双中子星并合提供了新的见解。

Abstract: We present the first fully non-linear evolutions of binary neutron star mergers in a moving-punctures approach in Einstein-scalar-Gauss-Bonnet gravity. We study both linear and quadratic-type couplings between the scalar and the Gauss-Bonnet invariant, and uncover new post-merger phenomena. These include an enhancement of the prompt collapse of a long-lived hyper-massive neutron star remnant and cases where the remnant develops a scalar configuration due to different scalarisation instabilities. This study initiates the exploration of beyond-General-Relativistic effects enhanced by the non-linear dynamics of the neutron star's fluid.

</details>


### [187] [Exact Dynamical Regular Black Holes from Generalized Polytropic Matter](https://arxiv.org/abs/2602.10768)
*Dmitriy Kudryavcev,Yi Ling,Vitalii Vertogradov*

Main category: gr-qc

TL;DR: 该论文提出了一类描述正则黑洞形成的精确解析解，这些黑洞由服从广义多方状态方程的引力坍缩物质形成，具有de Sitter核心和有限曲率不变量。


<details>
  <summary>Details</summary>
Motivation: 研究正则黑洞的形成机制，特别是通过物理上合理的物质分布来正则化奇点，为Hayward和Bardeen等已知正则黑洞提供统一的物质解释框架。

Method: 从具有径向依赖质量函数的Vaidya型几何出发，通过修改能量密度分布实现Kiselev解的正则化，得到具有de Sitter核心的非奇异时空。物质内容由广义多方状态方程描述，其中多方指数由正则化方案唯一确定。

Result: 获得了描述正则黑洞形成的精确动力学解，将Hayward和Bardeen时空作为广义多方状态方程的特例包含在内。发现状态方程的坐标无关性要求对正则化尺度与质量函数施加普遍约束，确保存在与黑洞质量无关的de Sitter核心。

Conclusion: 该研究为Hayward类和Bardeen类黑洞从引力坍缩中形成提供了统一的解析描述，建立了基于广义多方物质的一致有效物质解释框架，揭示了正则化尺度与质量函数之间的普遍关系。

Abstract: We present a class of exact, dynamical, and fully analytic solutions describing regular black holes formed via the gravitational collapse of matter obeying a generalized polytropic equation of state. Starting from a Vaidya-type geometry with a radially dependent mass function, we demonstrate that regularization of the Kiselev solutions can be achieved through a physically motivated modification of the energy density profile. This procedure leads to nonsingular spacetimes with a de~Sitter core and finite curvature invariants at the center.
  We show that the resulting matter content is naturally described by a generalized polytropic equation of state of the form $P=αρ-ζρ^γ$, where the polytropic index $γ$ is uniquely determined by the regularization scheme. Within this framework, we obtain exact dynamical generalizations of several well-known regular black hole solutions, including the Hayward and Bardeen spacetimes, as particular cases corresponding to specific values of the polytropic parameters.
  Remarkably, the requirement that the equation of state remains coordinate independent imposes a universal constraint relating the regularization scale to the mass function, which in turn guarantees the existence of a regular de~Sitter core with a curvature scale independent of the black hole mass. Our results provide a unified analytic description of Hayward-like and Bardeen-like black holes emerging from gravitational collapse, offering a consistent effective-matter interpretation rooted in generalized polytropic matter.

</details>


### [188] [Dust collapse and bounce in spherically symmetric quantum-inspired gravity models](https://arxiv.org/abs/2602.10804)
*Douglas M. Gingrich*

Main category: gr-qc

TL;DR: 研究量子引力启发的球对称模型中尘埃的坍缩与反弹，通过哈密顿约束求解演化方程，发现量子效应能阻止坍缩并产生反弹，避免经典奇点。


<details>
  <summary>Details</summary>
Motivation: 研究量子引力启发的重力模型中尘埃坍缩的动力学，探索量子效应如何阻止坍缩并产生反弹，从而避免经典奇点的形成。

Method: 从广泛类别的球对称时空出发，写出协变哈密顿约束方程，与尘埃场最小耦合，求解哈密顿演化方程，得到尘埃外边界位置和表观视界的简单方程，不假设尘埃密度均匀。

Result: 在许多情况下，有效的量子引力效应会停止尘埃物质场的坍缩，然后导致尘埃场膨胀，在最小半径处产生反弹，从而避免经典奇点。通过该形式体系，检验了多个量子引力启发度规，得到了与不同方法先前获得的结果一致或新的反弹结果。

Conclusion: 量子引力启发的球对称模型能够通过哈密顿约束形式体系描述尘埃坍缩的动力学，量子效应可以阻止坍缩并产生反弹，避免奇点形成，为理解量子引力效应在引力坍缩中的作用提供了新视角。

Abstract: We study the collapse and possible bounce of dust in quantum-inspired gravity models with spherical symmetry. Starting from a wide class of spherically symmetric spacetimes, we write down the covariant Hamiltonian constraints that under dynamical flow give rise to metrics of many spherically symmetric gravity models. Gravity is minimally coupled to a dust field. The constraint equations are solved for the Hamiltonian evolution and simple equations for the location of the outer boundary of the dust versus time and the apparent horizons in terms of shape functions are obtained. The dust density is not assumed to be homogeneous inside the collapsing ball. In many cases, the effective quantum gravity effects stop the collapse of the dust matter field, then causes the dust field to expand thus creating a bounce at a minimum radius and avoiding the classical singularity. Using this formalism, we examine several quantum-inspired gravity metrics to obtain bounce results either previously obtained by different methods or new results.

</details>


### [189] [Wave Propagation and Effective Refraction in Lorentz-Violating Wormhole Geometries](https://arxiv.org/abs/2602.10889)
*Semra Gurtas Dogan,Omar Mustafa,Abdulkerim Karabulut,Abdullah Guvendi*

Main category: gr-qc

TL;DR: 研究静态球对称洛伦兹破缺虫洞时空中无质量标量波的传播，建立几何光学框架，推导出由时空几何决定的频率相关有效折射率，分析波的光学行为。


<details>
  <summary>Details</summary>
Motivation: 研究洛伦兹破缺如何修改弯曲时空中的波光学性质，为在洛伦兹破缺引力背景下研究波散射、共振和潜在观测特征提供统一的光学视角框架。

Method: 从具有任意流逝函数和面半径的一般度量出发，推导曲率不变量，建立虫洞喉部的正则条件，将克莱因-戈登方程简化为亥姆霍兹型径向波方程，分析恒定、线性和二次流逝函数剖面。

Result: 发现折射率发散与Killing视界一致，曲率诱导的转折点控制标量波的反射、透射和约束；识别出无视界透射机制、不对称波传播和多视界捕获结构；洛伦兹破缺可显著修改弯曲时空的波光学性质，产生梯度折射率类似物和无曲率奇点的几何模式约束。

Conclusion: 洛伦兹破缺可以显著改变弯曲时空的波光学特性，产生梯度折射率类似物和无曲率奇点的几何模式约束。这种统一的光学视角为研究洛伦兹破缺引力背景下的波散射、共振和潜在观测特征提供了稳健框架。

Abstract: We study the propagation of massless scalar waves in static, spherically symmetric Lorentz-violating wormhole spacetimes within a geometric-optical framework. Starting from a general metric characterized by an arbitrary lapse function and areal radius, we derive curvature invariants, establish regularity conditions at the wormhole throat, and reduce the Klein-Gordon equation to a Helmholtz-type radial wave equation. This formulation naturally leads to a position- and frequency-dependent effective refractive index determined by the underlying spacetime geometry and Lorentz-violating structure, resulting in effective frequency-dependent wave-optical behavior. We show that divergences of the refractive index coincide with Killing horizons, while curvature-induced turning points control reflection, transmission, and confinement of scalar waves. By analyzing constant, linear, and quadratic lapse profiles, we identify horizonless transmission regimes, asymmetric wave propagation, and multi-horizon trapping structures. Our results reveal that Lorentz violation can significantly modify wave-optical properties of curved spacetime, generating graded-index analogues and geometric confinement of modes without curvature singularities. This unified optical perspective provides a robust framework for investigating wave scattering, resonances, and potential observational signatures in Lorentz-violating gravitational backgrounds.

</details>


### [190] [Vacuum polarization in the Schwarzschild black hole with a global monopole](https://arxiv.org/abs/2602.10899)
*Leonardo G. Barbosa,Victor H. M. Ramos,João Paulo M. Pitelli*

Main category: gr-qc

TL;DR: 在带有全局单极子的史瓦西黑洞视界上计算标量场的真空极化，发现结果包含单极子诱导项和修正视界半径的史瓦西项


<details>
  <summary>Details</summary>
Motivation: 研究全局单极子对黑洞视界上真空极化的影响，单极子产生立体角亏缺并使时空非Ricci平坦，这类似于宇宙弦穿透黑洞的情况

Method: 在Hartle-Hawking态中，对任意曲率耦合的质量标量场，以单极子参数η微扰展开至O(η²)阶，计算重整化真空期望值⟨Ψ²⟩_ren

Result: 视界上的⟨Ψ²⟩_ren分裂为两部分：在视界处评估的真正单极子诱导项，以及通常的史瓦西结果（但视界半径因η的存在而修正）

Conclusion: 全局单极子对黑洞视界真空极化的影响类似于宇宙弦穿透黑洞的情况，结果包含单极子直接贡献和修正几何的间接贡献

Abstract: We investigate vacuum polarization on the event horizon of a Schwarzschild black hole carrying a global monopole. For a massless scalar field $Ψ$ in the Hartle-Hawking state and with arbitrary curvature coupling, we compute the renormalized vacuum expectation value $\langle Ψ^2 \rangle_{\textrm{ren}}$. The monopole produces a solid-angle deficit and makes the spacetime non-Ricci-flat. Working perturbatively in the monopole parameter $η$ and retaining terms through $O(η^2)$, we find that $\langle Ψ^2 \rangle_{\textrm{ren}}$ on the horizon splits into two contributions: a genuinely monopole-induced term evaluated at the horizon and the usual Schwarzschild result - with the event horizon radius modified by the presence of $η$. Our result parallels earlier analyses for Schwarzschild black holes pierced by a cosmic string.

</details>


### [191] [Two types of quasinormal modes of Casadio-Fabbri-Mazzacurati brane-world black holes](https://arxiv.org/abs/2602.11001)
*Bekir Can Lütfüoğlu,Sardor Murodov,Mardon Abdullaev,Javlon Rayimbaev,Munisbek Akhmedov,Muhammad Matyoqubov*

Main category: gr-qc

TL;DR: 使用Leaver方法研究膜世界黑洞中质量标量场的准正规模，发现存在两种不同类型的模式，其行为随场质量增加而不同


<details>
  <summary>Details</summary>
Motivation: 研究Casadio-Fabbri-Mazzacurati膜世界黑洞背景下质量标量场的准正规模特性，探索场质量对振动模式的影响

Method: 采用收敛的Leaver方法，分析质量标量场在膜世界黑洞背景中的传播，研究准正规模谱随场质量变化的行为

Result: 发现谱中存在两种不同类型的模式：一类模式的实振荡频率随质量增加而减小并趋近于零；另一类模式的阻尼率趋于消失。当频率的实部或虚部达到零时，相应模式从谱中消失，由第一泛音取代

Conclusion: 膜世界黑洞中质量标量场的准正规模表现出独特的质量依赖性，存在两种不同类型的模式转换机制，为理解黑洞扰动和引力波物理提供了新见解

Abstract: Using the convergent Leaver method, we investigate the quasinormal modes of a massive scalar field propagating in the background of the Casadio--Fabbri--Mazzacurati brane-world black hole. We show that the spectrum exhibits two distinct types of modes, depending on their behavior as the field mass $μ$ increases. In one class, the real oscillation frequency decreases and eventually approaches zero, while in the other the damping rate tends to vanish. When either the real or imaginary part of the frequency reaches zero, the corresponding mode disappears from the spectrum, and the first overtone replaces it.

</details>


### [192] [Bayesian inference for tidal heating with extreme mass ratio inspirals](https://arxiv.org/abs/2602.11039)
*Zhong-Wu Xia,Sheng Long,Qiyuan Pan,Jiliang Jing,Wei-Liang Qian*

Main category: gr-qc

TL;DR: 极端质量比旋进(EMRIs)可作为探测黑洞视界的精密工具，通过潮汐加热效应约束黑洞反射率参数|R|²，在强场区域约束能力更强。


<details>
  <summary>Details</summary>
Motivation: 研究极端质量比旋进(EMRIs)如何作为探测黑洞近视界耗散的独特工具，通过潮汐加热效应来约束黑洞的反射率参数，从而验证黑洞视界的存在和性质。

Method: 采用完整的贝叶斯分析方法，对赤道偏心EMRIs进行注入-恢复研究，在全EMRI参数空间中采样，推断反射率参数|R|²的后验约束，分析潮汐加热效应的影响。

Result: 在强场区域后验不确定性更小，约束能力更强；对于快速旋转的中心天体，两年信号且信噪比为50时，可对|R|²施加10⁻³-10⁻⁴级别的约束；忽略潮汐加热会导致EMRI系统内在参数的系统性偏差。

Conclusion: EMRIs是探测和约束黑洞视界的有前景的精密探针，能够通过潮汐加热效应提供对黑洞反射率参数的高精度约束，验证黑洞视界的存在。

Abstract: Extreme mass ratio inspirals (EMRIs) provide unique probes of near-horizon dissipation through the tidal heating. We present a full Bayesian analysis of tidal heating in equatorial eccentric EMRIs by performing injection-recovery studies and inferring posterior constraints on the reflectivity parameter $|\mathcal{R}|^2$ while sampling in the full EMRI parameter space. We find that in the strong-field regime the posterior uncertainties are smaller, indicating a stronger constraining capability on the tidal heating. Using two-year signals with an optimal signal-to-noise ratio (SNR) of $ρ=50$, EMRIs can put bounds on $|\mathcal{R}|^2$ at the level of $10^{-3}$--$ 10^{-4}$ for a rapidly spinning central object. Moreover, we show that neglecting the tidal heating can induce clear systematic biases in the intrinsic parameters of the EMRI system. These results establish EMRIs as promising precision probes for detecting and constraining black hole event horizons.

</details>
