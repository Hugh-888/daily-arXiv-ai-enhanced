<div id=toc></div>

# Table of Contents

- [quant-ph](#quant-ph) [Total: 45]
- [gr-qc](#gr-qc) [Total: 11]
- [physics.comp-ph](#physics.comp-ph) [Total: 4]
- [cs.LG](#cs.LG) [Total: 87]


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [1] [Stroboscopic Saturation of Multiparameter Quantum Limits in Distributed Quantum Sensing](https://arxiv.org/abs/2510.15029)
*Berihu Teklu,Victor Montenegro*

Main category: quant-ph

TL;DR: 该论文提出了一种分布式量子传感框架，能够同时估计多个参数并超越标准量子极限，通过构建最优测量策略达到Holevo和量子Cramér-Rao界限的饱和。


<details>
  <summary>Details</summary>
Motivation: 解决多参数量子传感中由于最优测量不兼容性导致的量子优势难以实际实现的问题，特别是在分布式量子传感器网络中同时估计多个参数时。

Method: 开发了分布式量子探针的分析框架，构建了能够达到最终精度极限的最优测量策略，包括同时估计多个重力加速度和耦合强度的具体方案。

Result: 证明了分布式量子探针能够实现量子增强的灵敏度，精度随传感资源呈二次或四次方增长，并实现了Holevo和量子Cramér-Rao界限的饱和。

Conclusion: 所提出的分布式量子增强传感方案在当前实验能力范围内可行，为多参数量子传感的实际应用提供了有效途径。

Abstract: High-precision sensors that exploit uniquely quantum phenomena have been
shown to surpass the standard quantum limit of measurement precision. However,
in the general scenario where multiple parameters are simultaneously encoded in
a quantum probe, while surpassing the standard quantum limit is possible, its
practical attainability is severely hindered. This difficulty arises due to the
fundamental incompatibility among the optimal measurements required for
estimating different parameters. A naturally multiparameter sensing scenario
emerges when a network of quantum sensors is spatially distributed, with each
individual sensor probing a distinct parameter of interest. The central goal in
such a setting is twofold: first, to surpass the standard quantum limit in
estimating global properties of the system -- thereby achieving
quantum-enhanced sensitivity for a given network size -- and second, to
explicitly identify the optimal measurement strategies necessary to practically
attain this quantum advantage. Here, we analytically demonstrate
quantum-enhanced sensitivity for a broad class of distributed quantum probes,
including cases where the precision scales quadratically or quartically with
the sensing resources. We construct the corresponding optimal measurement
strategies that achieve the ultimate precision limits -- namely, saturation of
both the Holevo and quantum Cram\'{e}r-Rao bounds. We then apply our framework
to two concrete scenarios: the simultaneous estimation of multiple
gravitational accelerations (gravimetry) and coupling strengths across
spatially separated locations. Feasibility analyses indicate that the proposed
distributed quantum-enhanced sensing schemes are within reach of current
experimental capabilities.

</details>


### [2] [Adiabatic transport of neural network quantum states](https://arxiv.org/abs/2510.15030)
*Matija Medvidović,Alev Orfi,Juan Carrasquilla,Dries Sels*

Main category: quant-ph

TL;DR: 提出了一种基于神经网络量子态和绝热连续方法的第一性原理方法，用于构建多体激发态的神经网络表示，能够精确计算临界指数并研究物质相位的普适性质。


<details>
  <summary>Details</summary>
Motivation: 变分方法长期以来为多体量子物理提供了可控且强大的工具，但需要开发能够准确表示复杂多体激发态的新方法，以研究强关联体系中的激发态性质和相变临界行为。

Method: 通过绝热连续方法，将简单哈密顿量的本征态连续演化到强关联区域，构建多体激发态的神经网络表示，并利用全多体能隙的受控访问来估计临界指数。

Result: 该方法能够精确估计临界指数，并实现对激发态性质的精确靶向计算，无需参考整个谱系，支持大规模并行计算。

Conclusion: 该方法为大规模数值研究物质相位的普适性质打开了大门，提供了研究多体激发态和相变临界行为的新途径。

Abstract: Variational methods have offered controllable and powerful tools for
capturing many-body quantum physics for decades. The recent introduction of
expressive neural network quantum states has enabled the accurate
representation of a broad class of complex wavefunctions for many Hamiltonians
of interest. We introduce a first-principles method for building neural network
representations of many-body excited states by adiabatically continuing
eigenstates of simple Hamiltonians into the strongly correlated regime. With
controlled access to the full many-body gap, we obtain accurate estimates of
critical exponents. Successive eigenstate estimates can be run entirely in
parallel, enabling precise targeting of excited-state properties without
reference to the rest of the spectrum, opening the door to large-scale
numerical investigations of universal properties of entire phases of matter.

</details>


### [3] [Globalizing the Carleman linear embedding method for nonlinear dynamics](https://arxiv.org/abs/2510.15715)
*Ivan Novikau,Ilon Joseph*

Main category: quant-ph

TL;DR: 提出了三种全局分段Carleman嵌入方法，通过将状态空间划分为多个线性化区域来解决传统Carleman方法在存在多个固定点时收敛失败的问题。


<details>
  <summary>Details</summary>
Motivation: 传统Carleman嵌入方法在存在多个固定点的区域无法收敛，需要开发能够处理全局非线性动力系统的改进方法。

Method: 1) 边界切换方法：当轨迹到达当前线性化图边界时切换到新的局部线性化区域；2) 自适应图大小方法：动态调整图大小以提高多固定点区域的精度；3) 静态网格方法：使用预计算固定大小的线性化图，适合高速应用。

Result: 在可积和混沌非线性动力系统上的数值测试表明，这些方法能够处理传统Carleman方法完全无法解决的问题。自适应方法在低容差下对混沌系统模拟效果良好，而非自适应版本速度更快且精度相当。

Conclusion: 全局分段Carleman嵌入技术成功扩展了传统方法的适用范围，自适应方法适合高精度模拟，而非自适应方法可能更适合作为未来量子计算机算法的基础。

Abstract: The Carleman embedding method is a widely used technique for linearizing a
system of nonlinear differential equations, but fails to converge in regions
where there are multiple fixed points. We propose and test three different
versions of a global piecewise Carleman embedding technique, based on
partitioning space into multiple regions where the center and size of the
embedding region are chosen to control convergence. The first method switches
between local linearization regions of fixed size once the trajectory reaches
the boundary of the current linearization chart. During the transition, the
embedding is reconstructed within the newly created chart, centered at the
transition point. The second method also adapts the chart size dynamically,
enhancing accuracy in regions where multiple fixed points are located. The
third method partitions the state space using a static grid with precomputed
linearization charts of fixed size, making it more suitable for applications
that require high speed. All techniques are numerically tested on multiple
integrable and chaotic nonlinear dynamical systems demonstrating their
applicability for problems that are completely intractable for the standard
Carleman embedding method. Simulations of chaotic dynamical systems such as
various types of strange attractors demonstrate the power of the adaptive
methods, if a sufficiently low tolerance is imposed. Still, the non-adaptive
version of the method, with fixed centers and sizes of the linearization
charts, can be faster in simulating dynamical systems while providing similar
accuracy and may be more appropriate as the basis of algorithms for future
quantum computers.

</details>


### [4] [Advances in Quantum Genetic Algorithms](https://arxiv.org/abs/2510.15059)
*Dennis Lima,Rakesh Saini,Saif Al-Kuwari*

Main category: quant-ph

TL;DR: 本文全面综述了量子遗传算法(QGAs)中的适应度函数和选择步骤，分析了量子优势案例，分类说明了QGAs及其子程序，并讨论了QGAs解决的两个主要物理问题：球面上粒子的势能最小化和分子本征值求解。


<details>
  <summary>Details</summary>
Motivation: 量子遗传算法在化学和工程领域有广泛应用，但适应度函数和选择步骤是设计特定物理应用QGA时的问题编码步骤和最慢步骤，需要系统综述这些关键步骤。

Method: 通过调查映射量子优势案例，对量子遗传算法及其子程序进行分类和说明，重点分析Thomson问题的编码方法和Grover搜索在简化QGA中的选择步骤。

Result: 发现Thomson问题的编码是QGA在各种物理应用中使用的决定性步骤，而Grover搜索作为简化QGA中的选择步骤是量子加速的主要驱动力。

Conclusion: Thomson问题的编码方法和Grover搜索的选择步骤是量子遗传算法实现量子优势的关键技术路径。

Abstract: Quantum Genetic Algorithms (QGAs) are an emerging field of multivariate
quantum optimization that emulate Darwinian evolution and natural selection,
with vast applications in chemistry and engineering. The appropriate
application of fitness functions and fitness selection are the problem-encoding
step and the slowest step in designing QGAs for specific physical applications.
In this paper, we provide a comprehensive review of these crucial steps. Our
survey maps cases of quantum advantage, classifies and illustrates QGAs and
their subroutines, and discusses the two main physical problems tackled by
QGAs: potential energy minimization of particles on a sphere, and molecular
eigensolving. We conclude that the encoding used by the Thomson problem is a
decisive step toward the use of QGAs in a variety of physical applications,
while Grover's search as a selection step in Reduced QGAs is the main driver of
quantum speedup.

</details>


### [5] [Efficient encoding of the 2D toric code logical state using local Clifford gates](https://arxiv.org/abs/2510.15107)
*Ivan H. C. Shum*

Main category: quant-ph

TL;DR: 提出了一种深度为2L+1的电路算法，使用局部CNOT和Hadamard门编码L×L二维环面码逻辑态


<details>
  <summary>Details</summary>
Motivation: 开发高效的量子纠错码编码电路，减少电路深度并仅使用局部操作

Method: 设计基于局部CNOT和Hadamard门的编码电路，电路深度为2L+1

Result: 成功实现了L×L二维环面码逻辑态的编码

Conclusion: 该算法为量子纠错码提供了高效的编码方案，具有线性深度的优势

Abstract: An algorithm which encodes the $L\times L$ 2D toric code logical state with a
circuit of depth $2L+1$, using only local controlled-NOT($CX$) and
Hadamard($H$) gates, is presented.

</details>


### [6] [Interrelation of Non-Classicality, Entropy, Irreversibility and Work extraction in Open Quantum Systems](https://arxiv.org/abs/2510.15140)
*Jai Lalita,Subhashish Banerjee*

Main category: quant-ph

TL;DR: 研究了非经典体积、冯诺依曼熵、熵产生和ergotropy在各种开放量子系统中的相互关系，发现在不同模型中这些量之间存在普遍的对比演化关系。


<details>
  <summary>Details</summary>
Motivation: 探索量子信息与开放量子系统热力学之间的联系，研究不同开放量子系统模型中量子信息量与热力学量之间的相互关系。

Method: 使用两类开放量子系统模型：自旋-自旋相互作用模型（量子碰撞和中心自旋模型）和自旋-玻色子相互作用模型（非马尔可夫振幅阻尼通道、马尔可夫广义振幅阻尼通道和Jaynes-Cummings模型）。

Result: 在不同开放量子系统中发现普遍关系：非经典体积与熵呈对比演化，熵产生与ergotropy呈对比关系。储层初始状态对这些关系有影响。

Conclusion: 这些发现建立了量子信息与开放量子系统热力学之间的有趣联系，揭示了量子信息量与热力学量之间的普遍对比关系。

Abstract: The interplay of non-classical volume, von Neumann entropy, entropy
production, and ergotropy is investigated in various open quantum systems. Two
categories of open quantum system models are utilized: spin-spin and spin-boson
interaction models. The spin-spin interaction models include the quantum
collision and central spin models. On the other hand, the spin-boson
interaction models consist of non-Markovian amplitude damping channel,
Markovian generalized amplitude damping channel, and the Jaynes-Cummings model.
Across these various open quantum systems, universal interrelations emerge,
where the non-classical volume shows contrasting evolution with entropy, and
entropy production contrasts with ergotropy. The initial state of the reservoir
in these open quantum systems is shown to have an impact on these
interrelations. These findings establish an interesting link between quantum
information and the thermodynamics of open quantum systems.

</details>


### [7] [The Elegant Joint Measurement is Non-Classical in the Triangle Network](https://arxiv.org/abs/2510.15143)
*Victor Gitton,Renato Renner*

Main category: quant-ph

TL;DR: 本文首次证明了三角形网络中EJM分布的量子非经典性，通过结合膨胀技术、对称性约简和Frank-Wolfe算法解决了非凸优化问题。


<details>
  <summary>Details</summary>
Motivation: 量子网络中的非经典相关性通常比贝尔场景更复杂，三角形网络由于三个独立源的存在导致非凸优化问题难以处理。EJM分布作为高度对称的量子分布，其非经典性八年来一直未被证明。

Method: 结合膨胀因果推理技术、对称性约简和Frank-Wolfe大规模优化算法，通过计算机辅助在精确算术中证明非经典性。

Result: 首次成功证明了EJM分布在三角形网络中的非经典性，为理解量子网络提供了重要突破。

Conclusion: 该方法为解决网络量子非经典性认证提供了有效的计算工具，表明我们对量子网络的理解正在深化。

Abstract: When quantum systems are shared by multiple parties in a network, the
measurement outcomes of the parties can exhibit non-classical correlations,
i.e., correlations that cannot be obtained if the parties shared classical
systems instead. This phenomenon is known as quantum nonlocality and is
typically demonstrated in the Bell scenario. However, the Bell scenario is
fundamentally simpler to investigate than general networks, since the latter
come with non-convex optimization problems that are often intractable. The
triangle network is one of the simplest networks exhibiting this non-convexity
due to the presence of three independent sources. Although some special cases
of quantum nonlocality are known in the triangle network, general methods to
certify classical incompatibility are still lacking, which suggests that our
understanding of networks is still rather limited. For instance, the Elegant
Joint Measurement (EJM) distribution is a simple and highly-symmetric outcome
distribution that can be obtained with quantum systems and measurements in the
triangle network. This distribution was conjectured to be non-classical eight
years ago. In this article, we provide the first proof of non-classicality of
the EJM distribution. To do so, we show how to combine inflation, a causal
inference technique, with powerful symmetry reductions and Frank-Wolfe
algorithms for large-scale optimization. We then use these methods to obtain
computer-assisted proofs of non-classicality in exact arithmetic.

</details>


### [8] [Fundamental Limits to Cat-Code Qubits from Chaos-Assisted Tunneling](https://arxiv.org/abs/2510.15175)
*Lionel E. Martínez,Ignacio García-Mata,Diego A. Wisniacki*

Main category: quant-ph

TL;DR: 混沌辅助隧穿为Kerr猫态量子比特的保护设定了内在限制，混沌态在猫态之间介导隧穿，导致大的准能级分裂，从而限制了超导量子比特的相干性。


<details>
  <summary>Details</summary>
Motivation: 研究混沌辅助隧穿对Kerr猫态量子比特保护机制的影响，揭示混沌在动态保护超导量子比特中的基本限制作用。

Method: 使用Floquet分析和全量子模拟，结合半经典WKB理论计算隧穿速率，验证混沌与准能级分裂的直接关联。

Result: 发现当非线性增强时，混沌态介导猫态之间的隧穿，产生大的准能级分裂，量子模拟与WKB理论结果定量一致。

Conclusion: 首次在Kerr猫态量子比特中证实混沌辅助隧穿现象，证明混沌为动态保护超导量子比特的相干性设定了基本界限。

Abstract: We show that chaos-assisted tunneling (CAT) imposes an intrinsic limit to the
protection of Kerr-cat qubits. In the static effective description, tunneling
between the quasi-degenerate cat states can be exponentially suppressed,
ensuring long lifetimes. However, our Floquet analysis reveals that when the
nonlinearities increase, chaotic states mediate tunneling between the cat
states, producing large quasi-energy splittings. We compute tunneling rates
using both full quantum simulations and semiclassical WKB theory, finding
quantitative agreement and confirming that the splittings are directly linked
to chaos. These results provide the first evidence of CAT in the Kerr-cat qubit
and demonstrate that chaos sets a fundamental bound on the coherence of
dynamically protected superconducting qubits.

</details>


### [9] [Superconducting Gap Engineering in Tantalum-Alloy-Based Resonators](https://arxiv.org/abs/2510.15182)
*Chen Yang,Faranak Bahrami,Guangming Cheng,Mayer Feldman,Nana Shumiya,Stephen A. Lyon,Nan Yao,Andrew A. Houck,Nathalie P. de Leon,Robert J. Cava*

Main category: quant-ph

TL;DR: 通过在钽薄膜中掺入20%原子比的铪，实现了6.09K的超导转变温度，比纯钽提高了40%，但低温下的双能级系统和准粒子损耗保持不变。


<details>
  <summary>Details</summary>
Motivation: 探索钽基器件中的超导能隙工程，以扩展量子技术中可用宿主材料的范围，通过材料优化提升超导电路性能。

Method: 在钽薄膜中掺入20%原子比的铪形成Ta-Hf合金，系统改变沉积条件以控制薄膜取向和输运特性，通过直流输运和微波测量验证超导性能。

Result: Ta-Hf合金的超导转变温度达到6.09K，比纯钽提高了40%，但低温下的双能级系统和准粒子损耗没有变化。

Conclusion: 材料工程具有提升超导电路性能的潜力，激励进一步探索工程合金在量子技术中的应用。

Abstract: Utilizing tantalum (Ta) in superconducting circuits has led to significant
improvements, such as high qubit lifetimes and quality factors in both qubits
and resonators, underscoring the importance of material optimization in quantum
device performance. In this work, we explore superconducting gap engineering in
Ta-based devices as a strategy to expand the range of viable host materials. By
alloying 20 atomic percent hafnium (Hf) into Ta thin films, we achieve a
superconducting transition temperature ($T_c$) of 6.09~K, as measured by DC
transport, reflecting an increased superconducting gap. We systematically vary
deposition conditions to control film orientation and transport properties of
the Ta-Hf alloy films. The enhancement in $T_c$ is further confirmed by
microwave measurements at millikelvin temperatures. Despite the 40\% increase
in $T_c$ relative to pure Ta, the loss contributions from two-level systems
(TLS) and quasiparticles (QPs) remain unchanged in the low-temperature regime.
These findings highlight the potential of material engineering to improve
superconducting circuit performance and motivate further exploration of
engineered alloys for quantum technologies.

</details>


### [10] [Open system dynamics in local Lindbladians with chaotic spectra](https://arxiv.org/abs/2510.15193)
*Sanket Chirame,Fiona J. Burnell*

Main category: quant-ph

TL;DR: 研究随机矩阵理论(RMT)谱对Lindblad动力学的影响，比较了空间局域和完全随机Lindblad动力学的差异，发现RMT谱导致准普适早期动力学，并探讨了局域性对算子增长的限制作用。


<details>
  <summary>Details</summary>
Motivation: 探索Lindblad动力学中随机矩阵理论谱的物理后果，理解局域性如何约束Lindblad本征算子的尺寸依赖性，以及这如何影响算子增长和退相干行为。

Method: 数值研究Lindblad动力学，分析谱特性与算子尺寸依赖关系，比较单点耗散和两点耗散主导情况下的算子演化行为。

Result: 发现RMT谱导致准普适早期动力学；局域性约束使线性可观测量对谱边缘本征模敏感；单点耗散时算子退相干与Pauli权重近似线性相关；两点耗散可产生长寿命高Pauli权重算子。

Conclusion: Lindblad动力学的谱特性与局域性约束共同决定了算子增长和退相干行为，不同类型的耗散算子会导致不同的算子演化模式。

Abstract: We investigate the physical consequences of having a spectrum that satisfies
random matrix theory (RMT) for generic Lindbladians, and compare its
consequences for spatially local and completely random Lindblad dynamics in one
spatial dimension. We find that Lindbladians whose spectrum is described by RMT
exhibit quasi-universal early-time dynamics for quantities non-linear in the
density matrix, in the sense that for generic, highly entangled initial states,
the early time evolution is independent of the choice of initial state. We
numerically investigate how locality generically imposes constraints on the
size-dependence of Lindblad eigenoperators. This size dependence implies that
linear observables, such as expectation values of local operators, are highly
sensitive to eigenmodes outside the bulk of the spectrum in the thermodynamic
limit, and plays a central role in limiting operator growth in the presence of
dissipation. We find that when single-site dissipation dominates, an operator's
decoherence scales approximately linearly with its Pauli weight, even in the
presence of 2-site jump operators. When two-site only dissipation dominates,
however, this generic trend in operator size can be violated, leading to
long-lived high Pauli-weight operators.

</details>


### [11] [Game-Theoretic Discovery of Quantum Error-Correcting Codes Through Nash Equilibria](https://arxiv.org/abs/2510.15223)
*Rubén Darío Guerrero*

Main category: quant-ph

TL;DR: 提出了一种基于博弈论的量子纠错码发现框架，通过竞争目标间的策略互动生成具有期望特性的代码，相比传统方法更具机制可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统量子纠错码发现方法依赖预设结构的代数构造或缺乏机制可解释性的计算暴力搜索，需要更系统化和可解释的代码发现框架。

Method: 将代码优化重新构建为竞争目标间的策略互动，通过纳什均衡系统性地生成具有期望特性的代码，应用于图态稳定子代码。

Result: 发现了涵盖六个不同目标的代码，包括距离最大化、硬件适配等，特别生成了[15,7,3]代码，具有二分簇态结构，相比表面码在同等距离下实现40%开销减少。

Conclusion: 博弈论框架为量子纠错码发现提供了透明机制洞察，连接策略拓扑与代码参数，开辟了博弈论、优化和量子信息交叉研究的新途径。

Abstract: Quantum error correction code discovery has relied on algebraic constructions
with predetermined structure or computational brute-force search lacking
mechanistic interpretability. We introduce a game-theoretic framework that
recasts code optimization as strategic interactions between competing
objectives, where Nash equilibria systematically generate codes with desired
properties. Applied to graph state stabilizer codes, the framework discovers
codes across six distinct objectives -- distance maximization, hardware
adaptation, rate-distance optimization, cluster-state generation, surface-like
topologies, and connectivity enhancement -- through objective reconfiguration
rather than algorithm redesign. Game dynamics spontaneously generate a
$[\![15,7,3]\!]$ code with bipartite cluster-state structure enabling
measurement-based quantum computation while maintaining distance $d=3$,
achieving 40\% overhead reduction versus surface codes at equivalent distance.
Equilibrium analysis provides transparent mechanistic insights connecting
strategic topology to code parameters, opening research avenues at the
intersection of game theory, optimization, and quantum information.

</details>


### [12] [Quantum Voting Protocol for Centralized and Distributed Voting Based on Phase-Flip Counting](https://arxiv.org/abs/2510.15243)
*Ali Emre Aydin,Ammar Daskin*

Main category: quant-ph

TL;DR: 提出一种基于量子叠加和纠缠的新型量子投票协议，通过相位翻转编码和受控相位操作实现安全匿名投票，适用于集中式和分布式环境。


<details>
  <summary>Details</summary>
Motivation: 传统投票系统存在安全性和匿名性问题，量子技术能够提供更强的安全保障和隐私保护，同时利用量子特性实现更高效的投票计数。

Method: 使用纠缠候选态和相位翻转编码，通过受控相位操作记录投票，采用Hadamard门和受控-Z门等基本量子门，基于候选寄存器测量的简化计票机制。

Result: 在4选民2候选人和8选民3候选人的示例中验证了协议的正确性，展示了精确概率保持和正确计票结果。

Conclusion: 该量子投票协议通过量子叠加确保投票者匿名性，利用纠缠机制防止重复投票，在大规模选举中具有加速潜力，为安全投票系统提供了量子解决方案。

Abstract: In this paper, we introduce a novel quantum voting protocol that leverages
quantum superposition and entanglement to achieve secure, anonymous voting in
both centralized and distributed settings. Our approach utilizes phase-flip
encoding on entangled candidate states, where votes are recorded as controlled
phase operations conditioned on voter identity registers. The protocol employs
a simplified tallying mechanism based on candidate register measurements. We
provide comprehensive mathematical formulations for a centralized
single-machine model suitable for local voting systems, and a distributed
quantum channel model enabling remote voting with enhanced security through
entanglement verification. The efficiency of the protocol stems from its use of
basic quantum gates (Hadamard and controlled-Z) and its ability to count votes
through quantum measurements rather than iterative classical counting. We
demonstrate the practicality of the protocol through examples with 4 voters (2
candidates) and 8 voters (3 candidates), showing exact probability preservation
and correct vote tallying. The protocol ensures voter anonymity through quantum
superposition, prevents double-voting through entanglement mechanisms, and can
offer speedup potential for large-scale elections.

</details>


### [13] [Generalized Boundary Conditions for the qBounce Experiment](https://arxiv.org/abs/2510.15341)
*Eric J. Sung,Benjamin Koch,Tobias Jenke,Hartmut Abele,Denys I. Bondar*

Main category: quant-ph

TL;DR: 该论文研究了量子反弹实验中边界条件的自伴扩展理论，推导了最一般的自伴边界条件，并分析了其对能量谱和第五力测量的影响。


<details>
  <summary>Details</summary>
Motivation: 理论和实验数据之间的差异促使重新审视超冷中子在地球重力场中反弹的边界条件实现方式，需要建立自伴的哈密顿量来描述系统。

Method: 应用自伴扩展理论到半直线上的线性引力势，推导出使哈密顿量自伴的最一般边界条件，引入单个实参数λ连续插值狄利克雷情况和更一般的反射表面。

Result: 获得了任意λ下的能量谱、本征函数、相关矩阵元和求和规则的解析表达式，证明非平凡边界条件会偏置g的测量，并能模拟或掩盖假定的短程第五力。

Conclusion: 强制自伴性和正确建模边界物理对于引力量子态的定量预测至关重要，该方法适用于边界和自伴性主导可观测谱和动力学的系统。

Abstract: Discrepancies between theory and recent qBounce data have prompted renewed
scrutiny of how boundary conditions are implemented for ultracold neutrons
bouncing above a mirror in Earth's gravity. We apply the theory of self-adjoint
extensions to the linear gravitational potential on the half-line and derive
the most general boundary condition that renders the Hamiltonian self-adjoint.
This introduces a single real self-adjoint parameter $\lambda$ that
continuously interpolates between the Dirichlet case and more general
(Robin-type) reflecting surfaces.
  Building on this framework, we provide analytical expressions for the energy
spectrum, eigenfunctions, relevant matrix elements, and a set of sum rules
valid for arbitrary $\lambda$. We show how nontrivial boundary conditions can
bias measurements of $g$ and can mimic or mask putative short-range
''fifth-force''. Our results emphasize that enforcing self-adjointness-and
modeling the correct boundary physics-is essential for quantitative predictions
in gravitational quantum states. Beyond neutron quantum bounces, the approach
is broadly applicable to systems where boundaries and self-adjointness govern
the observable spectra and dynamics.

</details>


### [14] [Trust Region Bayesian Optimization of Annealing Schedules on a Quantum Annealer](https://arxiv.org/abs/2510.15245)
*Seon-Geun Jeong,Mai Dinh Cong,Minh-Duong Nguyen,Xuan Tung Nguyen,Quoc-Viet Pham,Won-Joo Hwang*

Main category: quant-ph

TL;DR: 提出基于信任区域贝叶斯优化(TuRBO)的量子退火调度框架，联合优化退火时间和傅里叶参数化调度，在噪声量子硬件上提升组合优化性能


<details>
  <summary>Details</summary>
Motivation: 量子退火性能受退火调度影响显著，而硬件退相干和噪声使得调度设计面临挑战，需要开发能够考虑硬件限制的调度优化方法

Method: 使用高斯过程代理模型和期望改进准则平衡探索与利用，通过信任区域更新在候选解周围细化搜索，并整合机制管理量子处理单元运行时间和硬件约束

Result: 仿真研究表明TuRBO在能量、可行解概率和链断裂分数方面持续优于随机和贪婪搜索方法

Conclusion: TuRBO是资源高效且可扩展的退火调度设计策略，在噪声中等规模量子体系中提供改进的量子退火性能，并可扩展到工业优化任务

Abstract: Quantum annealing (QA) is a practical model of adiabatic quantum computation,
already realized on hardware and considered promising for combinatorial
optimization. However, its performance is critically dependent on the annealing
schedule due to hardware decoherence and noise. Designing schedules that
account for such limitations remains a significant challenge. We propose a
trust region Bayesian optimization (TuRBO) framework that jointly tunes
annealing time and Fourier-parameterized schedules. Given a fixed embedding on
a quantum processing unit (QPU), the framework employs Gaussian process
surrogates with expected improvement to balance exploration and exploitation,
while trust region updates refine the search around promising candidates. The
framework further incorporates mechanisms to manage QPU runtime and enforce
feasibility under hardware constraints efficiently. Simulation studies
demonstrate that TuRBO consistently identifies schedules that outperform random
and greedy search in terms of energy, feasible solution probability, and chain
break fraction. These results highlight TuRBO as a resource-efficient and
scalable strategy for annealing schedule design, offering improved QA
performance in noisy intermediate-scale quantum regimes and extensibility to
industrial optimization tasks.

</details>


### [15] [Second-order discretization of Dyson series: iterative method, numerical analysis and applications in open quantum systems](https://arxiv.org/abs/2510.15287)
*Zhenning Cai,Yixiao Sun,Geshuo Wang*

Main category: quant-ph

TL;DR: 提出了一种离散化Dyson级数的通用策略，避免对高维积分直接数值求积，并将其扩展到开放量子系统。开发了两种数值方案（一阶和二阶），证明了收敛阶数，二阶方案在保持精度的同时大幅降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在模拟多能级系统（M≥3）时计算复杂度和内存需求过高，需要开发更高效的数值方法来模拟系统-浴动力学。

Method: 通过Strang分裂和泰勒展开的组合来离散化Dyson级数，避免直接数值积分。开发了两种迭代方法：一阶和二阶时间步长方案，其中二阶方案通过省略大部分项来显著降低计算复杂度。

Result: 证明了两种方案的全局误差分别为O(Δt)和O(Δt²)。二阶方案的时间复杂度为O(M³2^(2K_max)K_max²)，空间复杂度为O(M²2^(2K_max)K_max)，相比现有方法显著减少了多能级系统的内存和计算需求。

Conclusion: 该方法提供了一种高效且数值精确的模拟系统-浴动力学的方法，特别适用于多能级系统，在保持精度的同时大幅降低了计算资源需求。

Abstract: We propose a general strategy to discretize the Dyson series without applying
direct numerical quadrature to high-dimensional integrals, and extend this
framework to open quantum systems. The resulting discretization can also be
interpreted as a Strang splitting combined with a Taylor expansion. Based on
this formulation, we develop a numerically exact iterative method for
simulation system-bath dynamics. We propose two numerical schemes, which are
first-order and second-order in time step $\Delta t$ respectively. We perform a
rigorous numerical analysis to establish the convergence orders of both
schemes, proving that the global error decreases as $\mathcal{O}(\Delta t)$ and
$\mathcal{O}(\Delta t^2)$ for the first- and second-order methods,
respectively. In the second-order scheme, we can safely omitted most terms
arising from the Strang splitting and Taylor expansion while maintaining
second-order accuracy, leading to a substantial reduction in computational
complexity. For the second-order method, we achieves a time complexity of
$\mathcal{O}(M^3 2^{2K_{\max}} K_{\max}^2)$ and a space complexity of
$\mathcal{O}(M^2 2^{2K_{\max}} K_{\max})$ where $M$ denotes the number of
system levels and $K_{\max}$ the number of time steps within the memory length.
Compared with existing methods, our approach requires substantially less memory
and computational effort for multilevel systems ($M\geqslant 3$). Numerical
experiments are carried out to illustrate the validity and efficiency of our
method.

</details>


### [16] [Investigating the performance of RPM JTWPAs by optimizing LC-resonator elements](https://arxiv.org/abs/2510.15310)
*M. A. Gali Labarias,T. Yamada,Y. Nakashima,Y. Urade,K. Inomata*

Main category: quant-ph

TL;DR: 该研究通过数值优化谐振器参数来提升约瑟夫森行波参量放大器的增益、带宽和正交压缩性能，在理想无噪声情况下可提升超过5dB，但损耗会限制性能提升。


<details>
  <summary>Details</summary>
Motivation: 谐振相位匹配约瑟夫森行波参量放大器在量子计算中具有重要作用，需要优化其性能以满足低噪声、宽带放大和正交压缩的需求。

Method: 采用参数化谐振器元件的数值优化方法，通过集总元件模型分析损耗效应。

Result: 优化谐振器在理想无噪声情况下可将最大增益和压缩提升超过5dB，但损耗会导致增益饱和和压缩模式快速退化。

Conclusion: 谐振器设计能显著改善放大器性能，但当前制造技术和固有损耗仍是主要挑战。

Abstract: Resonant phase-matched Josephson traveling-wave parametric amplifiers (RPM
JTWPAs) play a key role in quantum computing and quantum information
applications due to their low-noise, broadband amplification, and quadrature
squeezing capabilities. This research focuses on optimizing RPM JTWPAs through
numerical optimization of parametrized resonator elements to maximize gain,
bandwidth and quadrature squeezing. Our results show that optimized resonators
can increase the maximum gain and squeezing by more than 5 dB in the ideal
noiseless case. However, introducing the effects of loss through a
lumped-element model reveals that gain saturates with increasing loss, while
squeezing modes degrade rapidly, regardless of resonator optimization. These
results highlight the potential of resonator design to significantly improve
amplifier performance, as well as the challenges posed by current fabrication
technologies and inherent losses.

</details>


### [17] [Capturing Protein Free Energy Landscape using Efficient Quantum Encoding](https://arxiv.org/abs/2510.15316)
*Ashwini Kannan,Jaya Vasavi Pamidimukkala,Avinash Dakshinamoorthy,Soham Bopardikar,Kalyan Dasgupta,Sanjib Senapati*

Main category: quant-ph

TL;DR: 提出了一种基于编码优化的蛋白质折叠预测算法，结合经典和量子求解器，在三维面心立方晶格上模拟蛋白质折叠过程。


<details>
  <summary>Details</summary>
Motivation: 蛋白质折叠是重要的生物学问题，与阿尔茨海默症等疾病相关。现有方法在预测低同源性序列结构方面存在挑战，需要更准确的预测方法。

Method: 使用三维面心立方晶格编码折叠过程，结合Miyazawa-Jernigan势能模拟非键相互作用，采用变分量子本征求解器在IBM量子硬件上求解哈密顿量。

Result: 预测的结构与实验数据对比显示良好一致性，与经典模拟退火和分子动力学模拟结果相当，在低同源性序列预测方面表现出潜力。

Conclusion: 混合经典-量子方法在蛋白质折叠预测领域具有前景，特别是对于低同源性序列的结构预测。

Abstract: Protein folding is one of the age-old biological problems that refers to the
mechanism of understanding and predicting how a protein's linear sequence of
amino acids folds into its specific three dimensional structure.This structure
is critical, as a protein's functionality is inherently linked to its final
folded form. Misfolding can lead to severe diseases such as Alzheimer's and
cystic fibrosis, highlighting the biological and clinical importance of
understanding protein folding mechanisms. This work presents a novel turn based
encoding optimization algorithm for predicting the folded structures of
peptides and small proteins. Our approach builds upon our previous research,
where our objective function focused on hydrophobic collapse, a fundamental
phenomenon underlying the protein folding process. In this work, we extend that
framework by not only incorporating hydrophobic interactions but also including
all non bonded interactions modeled using the Miyazawa Jernigan potential. We
constructed a Hamiltonian from the defined objective function that encodes the
folding process on a three dimensional face centered cubic lattice, offering
superior packing efficiency and a realistic representation of protein
conformations. This Hamiltonian is then solved using classical and quantum
solvers to explore the vast conformational space of proteins. To identify the
lowest-energy folded configurations, we utilize the Variational Quantum
Eigensolver implemented on IBM 133 qubit hardware. The predicted structures are
validated against experimental data using root mean square deviation as a
metric and compared against classical simulated annealing and molecular
dynamics simulation results. Our findings highlight the promise of hybrid
classical and quantum approaches in advancing protein folding predictions,
particularly for sequences with low homology.

</details>


### [18] [Achieving Sub-Exponential Speedup in Gate-Based Quantum Computing for Quadratic Unconstrained Binary Optimization](https://arxiv.org/abs/2510.15334)
*Tseng Ying-Wei,Kao Yu-Ting,Chang Yeong-Jar,Ou Chia-Ho,Chang Wen-Chih*

Main category: quant-ph

TL;DR: 提出了一种结合模拟退火(SA)和Grover算法的混合方法，在酶发酵优化问题中实现了亚指数级加速


<details>
  <summary>Details</summary>
Motivation: 现有的量子启发方法在组合优化问题中表现良好，但Grover算法仅提供二次加速，在大规模问题中仍不实用

Method: 将模拟退火与Grover算法集成，通过625个二进制参数编码酶发酵变量，构建QUBO问题来最大化活性成分

Result: 在酶发酵案例研究中，该方法通过基于门的量子计算实现了亚指数级加速

Conclusion: 混合方法显著提升了工业应用潜力，为组合优化问题提供了更实用的解决方案

Abstract: Recent quantum-inspired methods based on the Simulated Annealing (SA)
algorithm have shown strong potential for solving combinatorial optimization
problems. However, Grover's algorithm [1] in gate-based quantum computing
offers only a quadratic speedup, which remains impractical for large problem
sizes. This paper proposes a hybrid approach that integrates SA with Grover's
algorithm to achieve sub-exponential speedup, thereby improving its industrial
applicability.
  In enzyme fermentation, variables such as temperature, stirring, wait time,
pH, tryptophan, rice flour and so on are encoded by 625 binary parameters,
defining the space of possible enzyme formulations. We aim to find a binary
configuration that maximizes the active ingredient, formulated as a 625-bit
QUBO which is generated by historical experiments and AI techniques. Minimizing
the QUBO cost corresponds to maximizing the active ingredient. This case study
demonstrates that our hybrid method achieves sub-exponential speedup through
gate-based quantum computing.

</details>


### [19] [Singularity-free dynamical invariants-based quantum control](https://arxiv.org/abs/2510.15340)
*Ritik Sareen,Akram Youssry,Alberto Peruzzo*

Main category: quant-ph

TL;DR: 提出了一种用于非马尔可夫开放量子系统中单量子比特状态准备的广义不变基协议，通过两阶段控制方法生成平滑、硬件可行的控制场，在任意噪声条件下实现高保真度状态准备。


<details>
  <summary>Details</summary>
Motivation: 量子状态准备是量子技术的基石，但在非马尔可夫开放量子系统中，环境记忆和模型不确定性对实现高保真度控制提出了重大挑战。现有不变基逆工程方法往往产生实验上不可行的奇异脉冲，且仅限于简化的噪声模型。

Method: 采用两阶段控制协议：首先构造有界脉冲族在封闭系统中实现完美状态准备；然后从该族中选择最优成员以最小化噪声影响。该框架可处理已表征和未表征的噪声情况。

Result: 数值模拟显示该协议能在不同目标状态下实现高保真度状态准备，同时产生平滑、硬件可行的控制场。无奇异框架将不变基控制扩展到实际开放系统环境。

Conclusion: 该框架为NISQ硬件和其他展现非马尔可夫动力学的平台提供了通向稳健量子状态工程的通用路径。

Abstract: State preparation is a cornerstone of quantum technologies, underpinning
applications in computation, communication, and sensing. Its importance becomes
even more pronounced in non-Markovian open quantum systems, where environmental
memory and model uncertainties pose significant challenges to achieving
high-fidelity control. Invariant-based inverse engineering provides a
principled framework for synthesizing analytic control fields, yet existing
parameterizations often lead to experimentally infeasible, singular pulses and
are limited to simplified noise models such as those of Lindblad form. Here, we
introduce a generalized invariant-based protocol for single-qubit state
preparation under arbitrary noise conditions. The control proceeds in
two-stages: first, we construct a family of bounded pulses that achieve perfect
state preparation in a closed system; second, we identify the optimal member of
this family that minimizes the effect of noise. The framework accommodates both
(i) characterized noise, enabling noise-aware control synthesis, and (ii)
uncharacterized noise, where a noise-agnostic variant preserves robustness
without requiring a master-equation description. Numerical simulations
demonstrate high-fidelity state preparation across diverse targets while
producing smooth, hardware-feasible control fields. This singularity-free
framework extends invariant-based control to realistic open-system regimes,
providing a versatile route toward robust quantum state engineering on NISQ
hardware and other platforms exhibiting non-Markovian dynamics.

</details>


### [20] [Entanglement complexification transition driven by a single non-Hermitian impurity](https://arxiv.org/abs/2510.15370)
*Yao Zhou,Peng Ye*

Main category: quant-ph

TL;DR: 研究揭示了在Hermitian临界系统中，单个非Hermitian杂质作为边界时，会在纠缠切割处引发纠缠复杂化转变，有效中心电荷从实数变为复数，标志着系统进入非幺正缺陷共形场论相。


<details>
  <summary>Details</summary>
Motivation: 探索非Hermitian边界如何影响Hermitian临界系统的纠缠结构，这一领域目前研究较少。

Method: 通过精确求解具有单个非Hermitian杂质的Hermitian无能隙链，分析纠缠熵的标度行为和关联矩阵的谱特性。

Result: 发现了纠缠复杂化转变：对数纠缠熵保持标度形式，但有效中心电荷从实数演化为复数，关联矩阵发生谱塌缩。

Conclusion: 单个非Hermitian杂质能将Hermitian临界系统驱动到非幺正缺陷共形场论相，为边界非Hermiticity提供了罕见的解析可解平台。

Abstract: While non-Hermitian bulk systems and their sensitivity to boundary conditions
have been extensively studied, how a non-Hermitian boundary affects the
entanglement structure of Hermitian critical systems remains largely
unexplored. Here we present a fully analytical framework by exactly solving a
Hermitian gapless chain with a single non-Hermitian impurity acting as a
non-Hermitian boundary. When the entanglement cut is placed at the impurity, we
uncover a sharp \emph{entanglement complexification transition}: the
logarithmic entanglement entropy retains its scaling form, but the effective
central charge evolves from real to complex values, accompanied by a spectral
collapse of the correlation matrix. We demonstrate that the real regime follows
analytic continuation from a unitary defect conformal field theory (CFT),
whereas the complex regime lies entirely beyond this framework. For the latter,
we derive an analytical formula in perfect agreement with numerics. Our results
reveal that a single non-Hermitian impurity can drive a Hermitian critical
system into a nonunitary defect-CFT phase, establishing a rare analytically
solvable platform for boundary non-Hermiticity.

</details>


### [21] [Fisher discord as a quantifier of quantum complexity](https://arxiv.org/abs/2510.15375)
*Huihui Li,Shunlong Luo,Yue Zhang*

Main category: quant-ph

TL;DR: 本文提出了一种基于Fisher discord的量子态复杂度量化方法，通过量子Fisher信息的两种表达式差异来表征量子态的经典与量子特征混合特性。


<details>
  <summary>Details</summary>
Motivation: 经典互信息和Fisher信息的等价表达式在扩展到量子系统时出现分歧，这种差异可用于量化量子态中经典与量子特征混合的复杂度。

Method: 引入Fisher discord作为复杂度量化器，定义为对称对数导数定义的量子Fisher信息与Wigner-Yanase偏信息定义的量子Fisher信息之间的差异。

Result: 揭示了复杂度量化器的基本性质，证明平衡态和所有纯态在该设置下具有零复杂度，并在离散和连续变量量子系统中评估了各种原型态的复杂度。

Conclusion: Fisher discord提供了一种信息论框架来量化量子态的复杂度，能够区分经典和量子特征的混合程度，为理解量子系统的复杂性提供了新视角。

Abstract: Two classically equivalent expressions of mutual information of probability
distributions (classical bipartite states) diverge when extended to quantum
systems, and this difference has been employed to define quantum discord, a
quantifier of quantum correlations beyond entanglement. Similarly, equivalent
expressions of classical Fisher information of parameterized probability
distributions diverge when extended to quantum states, and this difference may
be exploited to characterize the complex nature of quantum states. By
complexity of quantum states, we mean some hybrid nature which intermingles the
classical and quantum features. It is desirable to quantify complexity of
quantum states from various perspectives. In this work, we pursue the idea of
discord and introduce an information-theoretic quantifier of complexity for
quantum states (relative to the Hamiltonian that drives the evolution of
quantum systems) via the notion of Fisher discord, which is defined by the
difference between two important versions of quantum Fisher information: the
quantum Fisher information defined via the symmetric logarithmic derivatives
and the Wigner-Yanase skew information defined via the square roots of quantum
states. We reveal basic properties of the quantifier of complexity, and compare
it with some other quantifiers of complexity. In particular, we show that
equilibrium states (or stable states, which commute with the Hamiltonian of the
quantum system) and all pure states exhibit zero complexity in this setting. As
illustrations, we evaluate the complexity for various prototypical states in
both discrete and continuous-variable quantum systems.

</details>


### [22] [A Hybrid Quantum Solver for Gaussian Process Regression](https://arxiv.org/abs/2510.15486)
*Kerem Bükrü,Steffen Leger,M. Lautaro Hickmann,Hans-Martin Rieser,Ralf Sturm,Tjark Siefkes*

Main category: quant-ph

TL;DR: 使用变分量子线性求解器进行高斯过程回归推断，在量子计算机上实现与传统方法相当的回归质量


<details>
  <summary>Details</summary>
Motivation: 高斯过程训练需要立方时间复杂度的矩阵求逆，计算成本高。量子算法如HHL需要容错量子计算机，而变分量子线性求解器适合当前的中等规模含噪声量子计算机

Method: 将高斯过程后验分布计算转化为线性方程组求解问题，使用变分量子线性求解器优化变分量子电路参数

Result: 实验证明该方法在高斯过程回归中能够提供与传统方法相当的回归质量

Conclusion: 变分量子线性求解器为高斯过程回归提供了一种在中等规模量子计算机上可行的替代方案

Abstract: Gaussian processes are widely known for their ability to provide
probabilistic predictions in supervised machine learning models. Their
non-parametric nature and flexibility make them particularly effective for
regression tasks. However, training a Gaussian process model using standard
methods requires matrix inversions with a cubic time complexity, which poses
significant computational challenges for inference on larger datasets. Quantum
algorithms, such as the HHL algorithm, have been proposed as solutions that
overcome the need for classical matrix inversions by efficiently solving linear
systems of equations using quantum computers. However, to gain a computational
advantage over classical algorithms, these algorithms require fault-tolerant
quantum computers with a large number of qubits, which are not yet available.
The Variational Quantum Linear Solver is a hybrid quantum-classical algorithm
that solves linear systems of equations by optimizing the parameters of a
variational quantum circuit using a classical computer. This method is
especially suitable for noisy intermediate-scale quantum computers, as it does
not require many qubits. It can be used to compute the posterior distribution
of a Gaussian process by reformulating the matrix inversion into a set of
linear systems of equations. We empirically demonstrate that using the
Variational Quantum Linear Solver to perform inference for Gaussian process
regression delivers regression quality comparable to that of classical methods.

</details>


### [23] [Adaptive quantum channel discrimination using methods of quantum metrology](https://arxiv.org/abs/2510.15506)
*Stanisław Sieniawski,Rafał Demkowicz-Dobrzański*

Main category: quant-ph

TL;DR: 提出了一种基于张量网络的高效算法，用于寻找最优自适应量子通道区分策略，该方法受量子计量学中最近开发的数值方法启发。


<details>
  <summary>Details</summary>
Motivation: 研究量子通道区分与估计问题之间的联系，特别关注具有海森堡标度估计性能的模型与在有限通道使用次数下允许完美通道区分的模型之间的结构相似性。

Method: 开发基于张量网络的高效算法，灵感来源于量子计量学中用于寻找最优自适应通道估计协议的数值方法。

Result: 建立了通道区分与估计问题之间的理论联系，揭示了两种模型之间的结构相似性。

Conclusion: 该算法为量子通道区分提供了有效工具，同时深化了对通道区分与估计问题之间关系的理解。

Abstract: We present an efficient tensor-network based algorithm for finding the
optimal adaptive quantum channel discrimination strategies inspired by recently
developed numerical methods in quantum metrology to find the optimal adaptive
channel estimation protocols. We examine the connection between channel
discrimination and estimation problems, highlighting in particular an appealing
structural similarity between models that admit Heisenberg scaling estimation
performance, and models that admit perfect channel discrimination in
finite--number of channel uses.

</details>


### [24] [Parameterized quantum algorithms for closest string problems](https://arxiv.org/abs/2510.15529)
*Josh Cudby,Sergii Strelchuk*

Main category: quant-ph

TL;DR: 本文研究了Closest String Problem (CSP)和Closest Substring Problem (CSSP)的量子参数化复杂度，提出了三个CSP量子算法和一个CSSP量子算法，在特定参数范围内相比经典算法有性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着基因组数据的急剧增长和计算需求的增加，作者受到参数化复杂度在解决NP难问题中的实用性启发，特别是字符串问题在该框架下的成功应用，开始研究这些问题的量子参数化复杂度。

Method: 作者开发了三个针对CSP的量子算法和一个针对CSSP的量子算法，利用量子计算的优势在特定参数范围内实现性能改进。

Result: 每个量子算法在特定参数范围内都表现出比经典对应算法更好的性能，证明了量子方法在结构化组合设置中的潜力。作者还为二进制字母表的CSP推导了条件下限，表明第一个算法在其主导缩放因子方面是紧致的。

Conclusion: 量子参数化复杂度方法在解决CSP和CSSP等字符串问题方面具有前景，量子算法在特定参数范围内能够超越经典算法的性能。

Abstract: Parameterized complexity enables the practical solution of generally
intractable NP-hard problems when certain parameters are small, making it
particularly useful in real-world applications. The study of string problems in
this framework has been particularly fruitful, yielding many state-of-the-art
classical algorithms that run efficiently in certain parameter regimes contrary
to their worst- or average-case performance. Motivated by the dramatic increase
in genomic data and its growing computational demands, we initiate the study of
the quantum parameterized complexity of the Closest String Problem (CSP) and
the related Closest Substring Problem (CSSP). We present three quantum
algorithms for the CSP and one for the CSSP. Each algorithm demonstrates
improved performance over classical counterparts in specific parameter regimes,
highlighting the promise of quantum approaches in structured combinatorial
settings. We also derive a conditional lower bound for the CSP with binary
alphabets, showing that our first algorithm is tight in its dominant scaling
factor.

</details>


### [25] [Benchmarking non-Clifford gates using only Pauli twirling group](https://arxiv.org/abs/2510.15554)
*Han Ye,Guoding Liu,Xiongfeng Ma*

Main category: quant-ph

TL;DR: 提出了Pauli转移特征基准测试协议，使用局部Pauli操作估计量子通道的Pauli转移矩阵元素，解决了非Clifford门基准测试的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的随机基准测试方法使用局部旋转门对Clifford门有效，但在基准测试非Clifford门时面临根本性挑战。量子门基准测试不可避免地受到状态准备和测量误差的影响。

Method: 引入Pauli转移特征基准测试协议，仅使用局部Pauli操作估计量子通道的Pauli转移矩阵元素。基于此协议，开发了满足U²=I的非Clifford门的保真度基准测试方法。

Result: 通过数值模拟应用于Toffoli门验证了协议的可行性。

Conclusion: 该协议解决了使用局部旋转门基准测试非Clifford门的基本挑战，为量子门表征提供了新方法。

Abstract: Quantum gate benchmarking is unavoidably influenced by state preparation and
measurement errors. Randomized benchmarking addresses this challenge by
employing group twirling to regularize the noise channel, then provides a
characterization of quantum channels that is robust to these errors through
exponential fittings. In practice, local twirling gates are preferred due to
their high fidelity and experimental feasibility. However, while existing RB
methods leveraging local twirling gates are effective for benchmarking Clifford
gates, they face fundamental challenges in benchmarking non-Clifford gates. In
this work, we solve this problem by introducing the Pauli Transfer Character
Benchmarking. This protocol estimates the Pauli transfer matrix elements for a
quantum channel using only local Pauli operations. Building on this protocol,
we develop a fidelity benchmarking method for non-Clifford gates $U$ satisfying
$U^2=I$. We validate the feasibility of our protocol through numerical
simulations applied to Toffoli gates as a concrete example.

</details>


### [26] [Elastic Quantum Coupling Between Free Electrons and Photons](https://arxiv.org/abs/2510.15584)
*Dingguo Zheng,Ofer Kfir*

Main category: quant-ph

TL;DR: 该论文提出了自由电子与光子之间的弹性量子耦合理论，展示了电子通过光学腔场时会在受限光子模式上诱导相移，这一原理可用于在不改变电子量子态的情况下计数电子束中的电子。


<details>
  <summary>Details</summary>
Motivation: 将量子光学技术应用于电子显微镜，探索自由电子与光子之间的量子耦合及其潜在应用。

Method: 通过理论分析，构建了电子-光子系统的弹性散射算符和电子计数色散哈密顿量，描述了电子在光学腔场中的行为。

Result: 发现电子在光学腔场中会诱导光子模式的相移，这种相移可量化为自由电子的折射率，为电子计数提供了新方法。

Conclusion: 这种弹性电子-光子量子耦合机制有望在电子显微镜中实现量子级和亚散粒噪声水平的传感与成像，达到埃尺度分辨率。

Abstract: The quantum coupling between free-electrons and photons enables applying
quantum optics techniques in electron microscopy. Here, we formulate the
elastic electron-photon quantum coupling and its possible implications. Our
analysis shows that when an electron traverses the field of an optical cavity,
it induces a phase shift onto its confined photonic mode, which can be
quantified as a refractive index of a free electron. This principle can be
applied to counting electrons in a beam without changing its quantum states.
The elastic scattering operator forms an electron-counting dispersive
Hamiltonian for electron-photon systems within electron microscope, and it
could enable quantum- and sub-shot-noise sensing and imaging at the
{\AA}-scale.

</details>


### [27] [A single optically detectable tumbling spin in silicon](https://arxiv.org/abs/2510.15590)
*Félix Cache,Yoann Baron,Baptiste Lefaucher,Jean-Baptiste Jager,Frédéric Mazen,Frédéric Milési,Sébastien Kerdilès,Isabelle Robert-Philip,Jean-Michel Gérard,Guillaume Cassabois,Vincent Jacques,Anaïs Dréau*

Main category: quant-ph

TL;DR: 该论文演示了在硅中对G中心荧光缺陷进行单自旋光谱分析，揭示了自旋主轴在晶体中离散取向跳跃导致的精细磁结构，并展示了自旋翻滚如何影响微波磁场耦合。


<details>
  <summary>Details</summary>
Motivation: 传统电子自旋共振光谱需要探测数十亿个电子自旋，无法检测单个原子的运动。现有单自旋光谱方法仅限于静态系统，无法研究动态重定向过程。

Method: 使用高分辨率自旋光谱分析硅中G中心荧光缺陷，该缺陷在晶体矩阵中随机重定向，表现为伪分子行为。通过建模缺陷的原子重定向过程，分析自旋翻滚对微波磁场耦合的影响。

Result: 揭示了由自旋主轴在晶体中离散取向跳跃产生的精细磁结构，证明了自旋翻滚会引起微波磁场耦合的变化，从而在相干自旋控制实验中检测到位置依赖的Rabi频率。

Conclusion: 硅中的G中心作为伪分子构型，是研究光学、自旋和旋转特性相互作用的独特量子系统，为在高度通用材料中探索这些相互作用提供了新平台。

Abstract: Electron spin resonance spectroscopy is a widely used technique for analyzing
the microscopic structure, local environment and reorientation of atomic and
molecular systems. Conventional inductive detection methods typically require
to probe more than a billion of electron spins such that single atom motion is
hidden through ensemble averaging. While several single spin spectroscopy
methods are currently available, they have been so far limited to static
systems. Here we demonstrate single spin spectroscopy of a fluorescent tumbling
defect in silicon called the G center, behaving as a pseudo-molecule randomly
reorienting itself in the crystalline matrix. Using high-resolution spin
spectroscopy, we reveal a fine magnetic structure resulting from the spin
principal axes jumping between discrete orientations in the crystal. By
modeling the atomic reorientation of the defect, we demonstrate that spin
tumbling induces variations in the coupling to the microwave magnetic field,
enabling position-dependent Rabi frequencies to be detected in coherent spin
control experiments. By virtue of its pseudo-molecule configuration, the G
center in silicon is a unique quantum system to investigate the mutual
interaction between optical, spin and rotation properties in a highly versatile
material.

</details>


### [28] [Time evolution of the Husimi and Glauber-Sudarshan functions in terms of complementary Hamiltonian symbols](https://arxiv.org/abs/2510.15628)
*Mritunjay Tyagi,Simon Friederich*

Main category: quant-ph

TL;DR: 提出了基于互补哈密顿符号的Husimi Q-函数和Glauber-Sudarshan P-函数动力学的紧凑系统表述，揭示了统一的演化方程结构，澄清了之前报道的非经典贡献实际上是量化方案的伪像。


<details>
  <summary>Details</summary>
Motivation: 为了统一表述相空间分布函数的动力学，澄清之前关于非经典贡献的误解，并提供计算和解释量子相空间演化的有效途径。

Method: 使用互补哈密顿符号（Q函数的Anti-Wick符号和P函数的Wick符号）来表述动力学，推导出具有经典Liouville漂移项加高阶导数项的通用演化方程结构。

Result: 对于模量不超过四次方的哈密顿量，高阶项简化为具有无迹扩散矩阵的Fokker-Planck型二阶项；推导了Wick/Anti-Wick符号的透明Ehrenfest定理；证明之前报道的非经典贡献是量化方案的伪像。

Conclusion: 该工作巩固了使用互补符号表述相空间分布函数动力学的方法，为计算和解释量子相空间演化提供了有效途径，澄清了之前关于非经典贡献的误解。

Abstract: We present a compact, systematic formulation of the dynamics of the Husimi Q-
and Glauber-Sudarshan P-phase space distribution functions expressed in terms
of their \emph{complementary} Hamiltonian symbols: Anti-Wick for Q and Wick for
P. The resulting evolution equations have a universal leading structure, the
classical Liouvillian drift plus terms with higher-order derivatives of the
Hamiltonian. For Hamiltonians no higher than quartic in the moduli of the
complex phase space variables $\alpha_i$, the higher-order terms reduce to a
second-order Fokker-Planck type term with a \emph{traceless} diffusion matrix,
thereby clarifying and recovering recent results for such Hamiltonians within a
simple star-product framework. We further derive a transparent Ehrenfest
theorem for Wick/Anti-Wick symbols of the operators representing dynamical
observables. Using these results, we show that a previously reported
nonclassical contribution to the Q-function drift for the anharmonic oscillator
is an artifact of the quantization scheme used. Our paper consolidates the
formulation of the dynamics of the phase space distribution functions using
complementary symbols and provides an efficient route to compute and interpret
quantum phase space evolution.

</details>


### [29] [Distributed Quantum Information Processing: A Review of Recent Progress](https://arxiv.org/abs/2510.15630)
*Johannes Knörzer,Xiaoyu Liu,Benjamin F. Schiffer,Jordi Tura*

Main category: quant-ph

TL;DR: 分布式量子信息处理通过连接多个量子处理节点来克服单设备可扩展性限制，实现更大规模问题求解和新算法能力，包括多副本高维量子态联合测量等新功能。


<details>
  <summary>Details</summary>
Motivation: 解决单量子设备可扩展性瓶颈，通过分布式架构扩展量子计算能力，既能增加量子比特数量，又能实现单设备无法完成的新功能。

Method: 采用分布式量子协议，通过经典和量子通信连接多个量子处理节点，区分单副本和多副本访问模式，分析不同通信模型的权衡。

Result: 分布式方法不仅提升了量子比特规模，还实现了多副本高维量子态联合测量等新能力，揭示了任务复杂度的差异和分布式量子资源的优势。

Conclusion: 分布式量子信息处理是扩展量子计算能力的关键途径，需要平衡经典与量子通信模型，并面临实验实现的实践挑战。

Abstract: Distributed quantum information processing seeks to overcome the scalability
limitations of monolithic quantum devices by interconnecting multiple quantum
processing nodes via classical and quantum communication. This approach extends
the capabilities of individual devices, enabling access to larger problem
instances and novel algorithmic techniques. Beyond increasing qubit counts, it
also enables qualitatively new capabilities, such as joint measurements on
multiple copies of high-dimensional quantum states. The distinction between
single-copy and multi-copy access reveals important differences in task
complexity and helps identify which computational problems stand to benefit
from distributed quantum resources. At the same time, it highlights trade-offs
between classical and quantum communication models and the practical challenges
involved in realizing them experimentally. In this review, we contextualize
recent developments by surveying the theoretical foundations of distributed
quantum protocols and examining the experimental platforms and algorithmic
applications that realize them in practice.

</details>


### [30] [Solving the 3D Heat Equation with VQA via Remeshing-Based Warm Starts](https://arxiv.org/abs/2510.15645)
*Samuel Donachie,Ulysse Remond,Arthur Mathorel,Kyryl Kazymyrenko*

Main category: quant-ph

TL;DR: 该论文研究使用变分量子算法(VQAs)求解稳态热传导偏微分方程，通过有限元离散化构建线性系统，引入网格重划分策略来改善训练性和避免贫瘠高原问题。


<details>
  <summary>Details</summary>
Motivation: 量子计算在解决经典难解问题如线性系统和偏微分方程方面具有巨大潜力。虽然完全容错的量子计算机尚未实现，但当前的NISQ设备使得探索混合量子-经典算法成为可能，其中VQAs是近期应用的主要候选方案。

Method: 使用变分量子算法求解稳态热传导PDEs，通过有限元方法离散化得到线性系统Ku=f。定义编码系统热能的成本函数，使用各种ansatz族进行优化。引入网格重划分策略，通过重用粗网格的优化参数逐步提高分辨率。

Result: 研究结果表明，随着网格细化，标量量实现了收敛。这为将VQAs应用于PDEs提供了实用方法学。

Conclusion: 这项工作为应用VQAs求解PDEs提供了实用方法，揭示了当前量子硬件的能力和局限性。

Abstract: Quantum computing holds great promise for solving classically intractable
problems such as linear systems and partial differential equations (PDEs).
While fully fault-tolerant quantum computers remain out of reach, current noisy
intermediate-scale quantum (NISQ) devices enable the exploration of hybrid
quantum-classical algorithms. Among these, Variational Quantum Algorithms
(VQAs) have emerged as a leading candidate for near-term applications. In this
work, we investigate the use of VQAs to solve PDEs arising in stationary heat
transfer. These problems are discretized via the finite element method (FEM),
yielding linear systems of the form Ku=f, where K is the stiffness matrix. We
define a cost function that encodes the thermal energy of the system, and
optimize it using various ansatz families. To improve trainability and bypass
barren plateaus, we introduce a remeshing strategy which gradually increases
resolution by reusing optimized parameters from coarser discretizations. Our
results demonstrate convergence of scalar quantities with mesh refinement. This
work provides a practical methodology for applying VQAs to PDEs, offering
insight into the capabilities and limitations of current quantum hardware.

</details>


### [31] [High-dimensional Path-Encoded Entanglement Distribution Between Photonic Chips Enabled by Multimode Phase Stabilisation](https://arxiv.org/abs/2510.15675)
*Molly A. Thomas,Daniel Llewellyn,Patrick W. Yard,Benjamin A. Slater,Caterina Vigliar,Stefano Paesani,Massimo Borghi,Döndü Sahin,John G. Rarity,Leif K. Oxenløwe,Mark G. Thompson,Karsten Rottwitt,Yunhong Ding,Jianwei Wang,Davide Bacco,Jorge Barreto*

Main category: quant-ph

TL;DR: 提出了一种新型多模相位稳定算法，能够在仅需两轮测量的情况下稳定高维路径编码纠缠量子态在光子芯片间的传输，将保真度从8.1%提升至86%。


<details>
  <summary>Details</summary>
Motivation: 通过光纤网络可靠分发高维纠缠量子态面临保持多模相干性的挑战，需要解决相位稳定问题。

Method: 利用集成光子电路的可重构性，开发了多模相位稳定算法，仅需两轮测量即可完成任意模式数的相位稳定，无需额外硬件。

Result: 在两芯片间实现了完整的量子态层析，验证了分布式纠缠态的保真度达到86%（无稳定时为8.1%），纠缠熵为0.995±0.002。

Conclusion: 该算法有效解决了高维纠缠态分发中的相位稳定问题，为实现可靠的量子技术应用提供了重要工具。

Abstract: The reliable distribution of high-dimensional entangled quantum states, an
important resource in quantum technologies, through optical fibre networks is
challenging due to the need to maintain coherence across multiple modes. Here
we demonstrate the distribution of four-dimensional path-encoded entangled
quantum states between photonic chips, enabled by a novel multimode phase
stabilisation algorithm. The algorithm utilises the reconfigurability of the
integrated photonic circuits to complete one iteration of phase stabilisation
in just two measurement rounds for an arbitrary number of modes, and requires
no additional hardware to the quantum measurements it enables. As a result, we
are able to perform complete quantum state tomography across two chips using
the minimum number of local projective measurements to verify the fidelity of
the distributed entangled state to be $86\%$ (compared to $8.1\%$ without the
phase stabilisation) with an entanglement entropy of $0.995\pm0.002$.

</details>


### [32] [Fragment, Entangle, and Consolidate: Strong Correlation through Bi-fold Quantum Circuits](https://arxiv.org/abs/2510.15678)
*Arpan Choudhury,Sonaldeep Halder,Rahul Maitra,Debashree Ghosh*

Main category: quant-ph

TL;DR: 提出了一种处理强电子关联的通用可定制方案，通过问题分解、纠缠构建和后续整合，在量子计算机上准确模拟多参考效应。


<details>
  <summary>Details</summary>
Motivation: 准确描述强关联对于探索新兴化学现象至关重要，但现有的变分量子算法在模拟多参考效应方面仍存在困难，阻碍了新型化学空间的设计。

Method: 基于问题启发的分子分解，使用硬件高效ansatz准备纠缠子系统，然后通过幺正耦合簇框架引入动态关联，使用跨片段广义算符参数化静态或动态ansatz。

Result: 在多个强关联系统的数值应用中，该方案显示出高精度、灵活性和鲁棒性，能够释放量子化学中量子优势的潜力。

Conclusion: 该混合架构能够平衡地捕捉不同程度的关联，同时保持可扩展性和灵活性，为量子化学中的量子优势提供了有效途径。

Abstract: An accurate description of strong correlation is quintessential for the
exploration of emerging chemical phenomena. While near-term variational quantum
algorithms provide a theoretically scalable framework for quantum chemical
problems, the accurate simulation of multireference effects remains elusive,
hindering progress toward the rational design of novel chemical space. In this
regard, we introduce a general and customizable scheme to handle strong
electronic correlation, based on problem decomposition, entanglement buildup,
and subsequent consolidation. Based on a problem-inspired molecular
decomposition, the deployment of Hardware Efficient Ansatz to prepare entangled
subsystems ensures efficient construction of a multireference state while
concurrently adhering to the hardware topology. The dynamic correlation is
subsequently introduced through a unitary coupled cluster framework, with
static or dynamic ansatz parametrized by a set of inter-fragment generalized
operators, and with the product state spanning various subsystems taken as the
reference. The hybrid architecture ensures a judicious deployment of separate
ansatze structures for capturing various degrees of correlation in a balanced
manner, while concurrently retaining the scalability and flexibility provided
by them individually. Over a number of numerical applications on a strongly
correlated system, the proposed scheme is shown to be highly accurate,
flexible, and robust in unlocking the potential to harness quantum advantage
for quantum chemistry.

</details>


### [33] [Generation of multipartite photonic entanglement using a trapped-ion quantum processing node](https://arxiv.org/abs/2510.15693)
*Marco Canteri,James Bate,Ida Mishra,Nicolai Friis,Victor Krutyanskiy,Benjamin P. Lanyon*

Main category: quant-ph

TL;DR: 本文演示了使用腔集成俘获离子量子处理器作为工厂节点，生成三光子GHZ纠缠态并验证其纠缠特性，为量子局域网中的多体纠缠分发奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 量子网络中节点间建立纠缠是实现各种量子应用的关键。工厂节点方法通过中心节点确定性制备多体纠缠态并用光子分发到周围节点，是很有前景的方案。

Method: 使用腔集成俘获离子量子处理器作为工厂节点，编程生成三个路径可切换光子的GHZ纠缠态，并通过定制的纠缠见证进行验证。

Result: 成功生成并验证了三光子GHZ纠缠态，证明俘获离子量子比特确定性制备纠缠态的技术可以用于制备飞行光子的相同纠缠态。

Conclusion: 这项工作展示了工厂节点的关键功能，为量子局域网中多体纠缠分发铺平了道路，未来这些光子可用于建立远程物质节点间的存储多体纠缠。

Abstract: The ability to establish entanglement between the nodes of future quantum
networks is essential for enabling a wide range of new applications in science
and technology. A promising approach involves the use of a powerful central
node capable of deterministically preparing arbitrary multipartite entangled
states of its matter-based qubits and efficiently distributing these states to
surrounding end nodes via flying photons. This central node, referred to as a
``factory node", serves as a hub for the production and distribution of
multipartite entanglement. In this work, we demonstrate key functionalities of
a factory node using a cavity-integrated trapped-ion quantum processor.
Specifically, we program the system to generate genuinely multipartite
entangled Greenberger-Horne-Zeilinger (GHZ) states of three path-switchable
photons and verify them using custom-designed entanglement witnesses. These
photons can, in the future, be used to establish stored multipartite
entanglement between remote matter-based nodes. Our results demonstrate that
the well-established techniques for the deterministic preparation of entangled
states of co-trapped ion qubits can be used to prepare the same states of
traveling photons, paving the way for multipartite entanglement distribution in
quantum local area networks.

</details>


### [34] [Quantum Worst-Case to Average-Case Reduction for Matrix-Vector Multiplication](https://arxiv.org/abs/2510.15721)
*Divesh Aggarwal,Dexter Kwan*

Main category: quant-ph

TL;DR: 本文提出了一种新的量子最坏情况到平均情况归约方法，用于矩阵向量乘法问题，比之前的方法更高效且概念更简单。


<details>
  <summary>Details</summary>
Motivation: 现有的最坏情况到平均情况归约方法虽然重要，但通常使用复杂的加法组合工具，导致计算复杂度高且开销不理想。本文旨在为量子设置提供更高效的归约方法。

Method: 通过将硬度自放大技术适配到量子领域，开发了新的量子最坏情况到平均情况归约方法，改进了对成功概率的依赖。

Result: 获得了改进的量子最坏情况到平均情况归约，为量子细粒度复杂性理论中的更广泛应用奠定了基础。

Conclusion: 该方法为矩阵向量乘法问题提供了概念更简单、效率更高的量子最坏情况到平均情况归约，在量子细粒度复杂性理论中具有重要应用前景。

Abstract: Worst-case to average-case reductions are a cornerstone of complexity theory,
providing a bridge between worst-case hardness and average-case computational
difficulty. While recent works have demonstrated such reductions for
fundamental problems using deep tools from ad- ditive combinatorics, these
approaches often suffer from substantial complexity and suboptimal overheads.
In this work, we focus on the quantum setting, and provide a new reduction for
the Matrix-Vector Multiplication problem that is more efficient, and
conceptually simpler than previous constructions. By adapting hardness
self-amplification techniques to the quantum do- main, we obtain a quantum
worst-case to average-case reduction with improved dependence on the success
probability, laying the groundwork for broader applications in quantum
fine-grained complexity.

</details>


### [35] [Optomechanical crystal in light-resilient quantum ground-state](https://arxiv.org/abs/2510.15724)
*Johan Kolvik,Paul Burger,David Hambraeus,Trond H. Haug,Joey Frey,Mads B. Kristensen,Raphaël Van Laer*

Main category: quant-ph

TL;DR: 本文展示了一种芯片级、无释放的硅光机械晶体腔，在低温下运行，对激光具有更好的耐受性。相较于悬浮纳米梁光机械晶体腔，热光效应抑制了18 dB，设备在35 dB更高的腔内光学能量下维持接近单位声子占据。


<details>
  <summary>Details</summary>
Motivation: 传统悬浮光机械结构存在热锚定差的问题，容易受到光学吸收产生的热噪声影响。本文旨在开发一种更稳健的光机械系统。

Method: 采用无释放的芯片级硅光机械晶体腔设计，在低温条件下运行，通过时间分辨测量研究热动力学行为。

Result: 观察到热光效应18 dB的抑制，设备在35 dB更高的腔内光学能量下仍能维持接近单位声子占据，时间分辨测量显示快速初始热化受机械衰减时间控制。

Conclusion: 无释放芯片系统为低噪声、高功率的经典和量子电光机械学提供了可行路径，特别是在微波和光学光子之间的频率转换应用方面。

Abstract: Interaction between light and high-frequency sound is a key area in
integrated photonics, quantum and nonlinear optics, and quantum science.
However, the typical suspended optomechanical structures suffer from poor
thermal anchoring, making them susceptible to thermal noise arising from
optical absorption. Here, we demonstrate a chip-scale, release-free silicon
optomechanical crystal cavity (OMC) operating cryogenically with improved
resilience to laser light. Relative to a suspended nanobeam OMC, we observe an
18 dB suppression of the thermo-optic effect, and the device sustains
near-unity phonon occupation at 35 dB higher intracavity optical energy.
Time-resolved measurements further reveal rapid initial thermalization governed
by the mechanical decay time. With further material and design improvements in
sight, these results bolster release-free systems on a chip as a path for
low-noise and high-power classical and quantum electro-optomechanics, such as
for frequency converters between microwave and optical photons.

</details>


### [36] [The Geometry of Qubit Decoherence: Linear vs. Nonlinear Dynamics in the Bloch Ball](https://arxiv.org/abs/2510.15726)
*Alan C. Maioli,Evaldo M. F. Curado,Jean-Pierre Gazeau,Tomoi Koide*

Main category: quant-ph

TL;DR: 提出了两种互补的方法来研究开放量子比特的GKSL方程：基于线性性的方法和基于SU(2)对称性的非线性方法。


<details>
  <summary>Details</summary>
Motivation: 为开放量子比特系统提供更深入的理论分析框架，特别是通过对称性方法为开放量子dit系统的推广提供途径。

Method: 第一种方法基于线性性，通过混合态轨迹在Bloch球中的演化进行分析；第二种方法利用SU(2)对称性，将角动力学与径向耗散分离，形成非线性动力系统。

Result: 获得了包含非随机渐近固定点和异常点的解，并建立了基于对称性的分析框架。

Conclusion: 基于对称性的视角为开放量子dit系统的推广提供了有前景的研究路径。

Abstract: We present two complementary approaches to the GKSL equation for an open
qubit. The first, based on linearity, yields solutions illustrated by mixed
states trajectories in the Bloch ball, including non-random asymptotic fixed
points, and exceptional points. The second, exploiting the SU(2) symmetry,
leads to a nonlinear dynamical system that separates angular dynamics from
radial dissipation. This symmetry-based perspective offers a promising route
toward generalisation to open qudits.

</details>


### [37] [Emergence of irreversible decoherence from unitary dynamics](https://arxiv.org/abs/2510.15730)
*Ri-Hua Zheng,Jia-Hao Lü,Fan Wu,Yan Xia,Li-Hua Lin,Zhen-Biao Yang,Shi-Biao Zheng*

Main category: quant-ph

TL;DR: 通过实验研究系统-环境相互作用如何导致不可逆的量子退相干，发现随着环境自由度增加，量子相干性逐渐衰减，表明不可逆退相干是系统与环境大量自由度相互作用产生的涌现现象。


<details>
  <summary>Details</summary>
Motivation: 理解幺正系统-环境动力学如何导致不可逆的系统退相干，这一基本问题在实验中尚未得到充分探索。

Method: 使用电路量子电动力学设备，其中存储光子猫态的微波总线谐振器连接到多个非线性电子振荡器，每个振荡器作为环境的一个自由度。

Result: 随着环境自由度数量的逐渐增加，可恢复的量子相干性逐渐衰减，这是由于系统状态信息在环境中编码的不可擦除信息量增加所致。

Conclusion: 量子系统的不可逆退相干是一种涌现现象，源于系统与环境大量自由度的幺正动力学，这对于调和量子力学与经典物理至关重要。

Abstract: The decoherence of superpositions of classically distinguishable states (cat
states) is crucial for understanding quantum-to-classical transitions and
quantum measurements. So far, decoherence processes of mesoscopic cat states
have been demonstrated in several experiments. However, the issue of how the
unitary system-reservoir dynamics can lead to irreversible system decoherence
remains largely unexplored in experiments. Here we experimentally explore this
fundamental issue with a circuit quantum electrodynamics device, where a bus
microwave resonator storing a photonic cat state is connected to many nonlinear
electronic oscillators. Each of these oscillators that are effectively coupled
to the bus resonator serves as one degree of freedom of the reservoir. By
gradually increasing the number of the reservoir's degrees of freedom, we find
that the revivable quantum coherence progressively decays, owing to the growth
in the amount of inerasable information about the system's state encoded in the
reservoir. Our results illustrate that irreversible decoherence of a quantum
system is an emergent phenomenon, arising from the unitary dynamics involving
the system and many of the reservoir's degrees of freedom, which is crucial for
the reconciliation of quantum mechanics and classical physics.

</details>


### [38] [A Gauss-Bonnet Theorem for Quantum States: Gauss Curvature and Topology in the Projective Hilbert Space](https://arxiv.org/abs/2510.15760)
*Shin-Ming Huang*

Main category: quant-ph

TL;DR: 开发了一种计算布洛赫能带量子度量曲率的解析方案，建立了量子态流形的完整黎曼几何，并推导出包含奇异曲率项的广义高斯-博内关系，将总符号高斯曲率与陈数直接联系起来。


<details>
  <summary>Details</summary>
Motivation: 几何和拓扑在现代凝聚态物理中至关重要，但它们在量子系统中的精确联系仍未完全理解。

Method: 使用基于本征投影子的规范不变表述，构建量子态流形的完整黎曼几何，应用于二维两带模型，并引入前沿和符号面积形式的概念。

Result: 发现高斯曲率在规则区域是常数，但流形不可避免地发展出度量张量退化的奇异点闭合曲线，这些奇异性阻碍了传统高斯-博内定理。

Conclusion: 该框架连接了微分几何和拓扑能带理论，揭示了奇异折叠如何调节量子体积和拓扑电荷之间的差异。

Abstract: Geometry and topology are fundamental to modern condensed matter physics, but
their precise connection in quantum systems remains incompletely understood.
Here, we develop an analytical scheme for calculating the curvature of the
quantum metric of Bloch bands. Using a gauge-invariant formulation based on
eigenprojectors, we construct the full Riemannian geometry of the quantum-state
manifold and apply it to a two-dimensional two-band model. We find that the
Gauss curvature is constant over regular regions, but the manifold inevitably
develops a closed curve of singular points where the metric tensor degenerates.
These singularities obstruct the conventional Gauss-Bonnet theorem. By
introducing the notion of a front and a signed area form, we derive a
generalized Gauss-Bonnet relation that includes a singular curvature term
defined along the fold curve. This result establishes a direct, quantized link
between the total signed Gauss curvature and the Chern number, providing a
unified geometric interpretation of Berry curvature and quantum metric. This
framework bridges differential geometry and topological band theory, revealing
how singular folds mediate the discrepancy between quantum volume and
topological charge.

</details>


### [39] [A source of heralded atom-photon entanglement for quantum networking](https://arxiv.org/abs/2510.15765)
*Gianvito Chiarella,Tobias Frank,Leart Zuka,Pau Farrera,Gerhard Rempe*

Main category: quant-ph

TL;DR: 该论文实现了一种在发送节点进行纠缠预兆的方法，通过单原子在两个光学光纤腔中的级联双光子发射来改善量子通信中的光子损失问题。


<details>
  <summary>Details</summary>
Motivation: 量子网络中的通信因光子损失而受到严重影响，传统的在接收节点进行测量预兆的方法速度慢且容易受到错误预兆（如探测器暗计数）的影响。

Method: 利用单原子在两个光学光纤腔中的级联双光子发射：一个光子的偏振与原子自旋纠缠，第二个光子用于预兆纠缠生成。

Result: 预兆方法将原子-光子纠缠在光纤中的效率和保真度分别提高到68(3)%和87(2)%。

Conclusion: 该方法在噪声受限的长距离量子通信中具有潜力，能够扩展恒定保真度的通信距离，或在给定距离下提高保真度。

Abstract: Communication in quantum networks suffers notoriously from photon loss.
Resulting errors can be mitigated with a suitable measurement herald at the
receiving node. However, waiting for a herald and communicating the measurement
result back to the sender in a repeat-until-success strategy makes the protocol
slow and prone to errors from false heralds such as detector dark counts. Here
we implement an entanglement herald at the sending node by employing a cascaded
two-photon emission of a single atom into two optical fiber cavities: The
polarization of one photon is entangled with the spin of the atom, and the
second photon heralds entanglement generation. We show that heralding improves
the atom-photon entanglement in-fiber efficiency and fidelity to 68(3)% and
87(2)%, respectively. We highlight the potential of our source for
noise-limited long-distance quantum communication by extending the range for
constant fidelity or, alternatively, increasing the fidelity for a given
distance.

</details>


### [40] [Hybrid Path-Transverse Electric Mode Qudit Encoding on an Integrated Photonic Chip](https://arxiv.org/abs/2510.15774)
*Imogen Forbes,Patrick Yard,Martin Bielak,Molly A. Thomas,Matthew S. Jones,Stefano Paesani,Massimo Borghi,Anthony Laing*

Main category: quant-ph

TL;DR: 该论文展示了一种可重编程集成光子器件，通过混合编码在路径和横向电模式自由度中生成纠缠量子态，实现了超纠缠态和GHZ₄态，并用于单拷贝纠缠蒸馏协议。


<details>
  <summary>Details</summary>
Motivation: 利用混合编码（多个自由度编码量子信息）可以在不显著增加硬件要求的情况下扩展希尔伯特空间，目标是减小集成量子光子实验的占地面积。

Method: 设计并实现了一个可重编程集成光子器件，包含多模组件以控制横向电模式，用于在路径和横向电模式自由度中生成纠缠量子态。

Result: 生成了保真度为67.3±0.2%的超纠缠态和85.2±0.4%的GHZ₄态，在单拷贝纠缠蒸馏协议中，蒸馏后的贝尔态保真度平均提高9.1%，最高可容忍50%的比特翻转错误概率。

Conclusion: 这项工作展示了利用横向电模式进行混合编码是减小集成量子光子实验占地面积的第一步，突显了与集成光子学兼容的自由度的优势。

Abstract: Hybrid encodings, where multiple degrees of freedom are used to encode
quantum information, can increase the size of the Hilbert space with minimal
increase to hardware requirements. We show a reprogrammable integrated photonic
device, with multimodal components designed to allow for control over the
transverse electric modes. We use this device to generate qudit states
entangled in the path and transverse electric mode degrees of freedom. We
generate and verify a hyperentangled state with a fidelity of
$\mathcal{F}_{\text{HE}} = 67.3 \pm 0.2\%$ and a GHZ$_{4}$-style state with a
fidelity of $\mathcal{F}_{\text{GHZ}_{4}} = 85.2 \pm 0.4 \%$. We use our
hyperentangled state in a single-copy entanglement distillation protocol,
resulting in an average $9.1 \%$ increase in the fidelity of the distilled Bell
state for up to a $50\%$ probability of bit flip error. By utilising degrees of
freedom which are readily compatible with integrated photonics, our work
highlights how this hybrid encoding demonstrates a first step in using the
transverse electric mode to reduce the footprint of integrated quantum photonic
experiments.

</details>


### [41] [Flexible Qubit Allocation of Network Resource States](https://arxiv.org/abs/2510.15776)
*Francesco Mazza,Jorge Miguel-Ramiro,Jessica Illiano,Alexander Pirker,Marcello Caleffi,Angela Sara Cacciapuoti,Wolfgang Dür*

Main category: quant-ph

TL;DR: 探索使用具有灵活量子比特-节点分配能力的图态作为量子网络资源态，特别是簇态，通过优化量子比特分配来创建捷径、提高鲁棒性和内存效率，显著减少远程节点间的平均跳数距离。


<details>
  <summary>Details</summary>
Motivation: 量子互联网仍处于早期阶段，识别可扩展且具有弹性的量子网络资源态对于实现量子互联网至关重要。需要探索能够灵活适应网络拓扑变化的资源态。

Method: 引入一个建模框架，用于在物理网络上叠加纠缠拓扑，重点关注具有任意分配的簇态作为网络资源态，通过优化和随机量子比特分配来创建网络捷径。

Result: 优化的量子比特分配能够创建捷径，提高网络鲁棒性和内存节省，与常规方法相比显著减少了远程网络节点之间的平均跳数距离。

Conclusion: 具有灵活量子比特-节点分配的簇态是量子网络核心级纠缠资源的有前景候选者，因其固有的灵活连接特性和对粒子损失的弹性而具有优势。

Abstract: The Quantum Internet is still in its infancy, yet identifying scalable and
resilient quantum network resource states is an essential task for realizing
it. We explore the use of graph states with flexible, non-trivial qubit-to-node
assignments. This flexibility enables adaptable engineering of the entanglement
topology of an arbitrary quantum network. In particular, we focus on cluster
states with arbitrary allocation as network resource states and as a promising
candidate for a network core-level entangled resource, due to its intrinsic
flexible connectivity properties and resilience to particle losses. We
introduce a modeling framework for overlaying entanglement topologies on
physical networks and demonstrate how optimized and even random qubit
assignment, creates shortcuts and improves robustness and memory savings, while
substantially reducing the average hop distance between remote network nodes,
when compared to conventional approaches.

</details>


### [42] [Adaptive time Compressed QITE (ACQ) and its geometrical interpretation](https://arxiv.org/abs/2510.15781)
*Alberto Acevedo Meléndez,Carmen G. Almudéver,Miguel Angel Garcia-March,Rafael Gómez-Lurbe,Luca Ion,Mohit Lal Bera,Rodrigo M. Sanz,Somayeh Mehrabankar,Tanmoy Pandit,Armando Pérez,Andreu Anglés-Castillo*

Main category: quant-ph

TL;DR: 提出了一种新颖的量子虚时演化算法，通过几何优化方法减少算法运行时间和电路深度，包括线搜索能量最小化和牛顿法优化时间步长。


<details>
  <summary>Details</summary>
Motivation: 为大型强关联系统高效制备基态是经典和量子硬件面临的挑战，量子虚时演化是实现这一目标的重要方法。

Method: 采用迭代线搜索进行能量最小化，使用牛顿法推导最优时间步长，通过将QITE产生的酉算子近似为单参数群元素来减少电路深度。

Result: 进行了数值研究，建立了保真度与不同截断参数的标度关系，并给出了相应的门数估计。

Conclusion: 新算法通过几何优化显著降低了QITE的运行时间和电路深度，为基态制备提供了更高效的量子实现方案。

Abstract: Preparing the ground state of a given Hamiltonian is a computational task of
interest in many fields, such as material science, chemistry and even some
optimization problems, to name a few. Efficiently preparing ground states for
large, strongly correlated systems is a challenging task for both classical and
quantum hardware. Drawing from classical optimization methods, e.g. dynamical
optimization techniques, one may deduce the spectral decomposition in manner
that avoids direct spectral decomposition and is amenable to Trotterization
methods. An instance of the latter is ground state preparation by Imaginary
Time Evolution (ITE), understood in physical terms as a natural cooling
process. Its quantum version QITE (Quantum Imaginary Time Evolution) aims at
implementing ITE in a quantum computer. In this paper we introduce a novel QITE
algorithm, which leverages underlying geometric properties for
algorithm-runtime and circuit depth reduction. This will materialize in the
form of an iterative Line Search approach for minimization of energy as well as
a Newton's method approach for the deduction of the optimal time-steps for each
iteration of QITE. The depth-reduction will be carried out via approximating
the resulting unitary operator estimated from the QITE algorithm with unitary
operator which is an element of a one-parameter group; making expressible as a
single unitary in a quantum circuit. Furthermore, we perform a numerical study
to stablish the scaling of fidelities with the different truncation parameters
and give gate counts estimates for each.

</details>


### [43] [Role of exceptional points in the dynamics of the Lindblad Sachdev-Ye-Kitaev model](https://arxiv.org/abs/2510.15793)
*Jie-ping Zheng,Jorge Dukelsky,Rafael A. Molina,Antonio M. García-García*

Main category: quant-ph

TL;DR: SYK模型与马尔可夫浴耦合的非平衡动力学显示非单调衰减率和一级动力学相变，这些特征源于Liouvillian谱中的异常点。


<details>
  <summary>Details</summary>
Motivation: 研究强相互作用多体量子系统在耗散环境中的非平衡动力学行为，特别是SYK模型与马尔可夫浴耦合时的弛豫特性。

Method: 使用Lindblad形式描述马尔可夫浴，通过解析计算（小N）和数值模拟（大N）分析SYK Liouvillian的实特征值和异常点。

Result: 发现衰减率是浴耦合强度μ的非单调函数，在μ∼0.1处出现异常点对应最长寿命模式，μ∼0.3时Loschmidt回波从相变变为交叉。

Conclusion: 这些特征在量子强相互作用多体Liouvillian的平衡过程中具有普适性，异常点导致弛豫速率随浴耦合增强而减小的反常平衡区域。

Abstract: The out of equilibrium dynamics of the Sachdev-Ye-Kitaev model (SYK),
comprising $N$ Majoranas with random all-to-all four-body interactions,
minimally coupled to a Markovian bath modeled by the Lindblad formalism,
displays intriguing nontrivial features. In particular, the decay rate towards
the steady state is a non-monotonic function of the bath coupling $\mu$, and an
analogue of the Loschmidt echo for dissipative quantum systems undergoes a
first order dynamical phase transitions that eventually becomes a crossover for
sufficiently large $\mu$. We provide evidence that these features have their
origin in the presence of exceptional points in the purely real eigenvalues of
the SYK Liouvillian closest to the zero eigenvalue associated with the steady
state. An analytic calculation at small $N$, supported by numerical results for
larger $N$, reveals that the value of $\mu \sim 0.1$ at which the exceptional
point corresponding to the longest living modes occurs is close to a local
maximum of the decay rate. This value marks the start of a region of anomalous
equilibration where the relaxation rate diminishes as the coupling to the bath
becomes stronger. Moreover, the mentioned change from transition to crossover
in the Loschmidt echo occurs at a larger $\mu \sim 0.3$ corresponding with a
proliferation of exceptional points in the low energy limit of the Liouvillian
spectrum. We expect these features to be generic in the approach to equilibrium
in quantum strongly interacting many-body Liouvillians.

</details>


### [44] [Operator Commutativity Screening and Progressive Operator Block Reordering toward Many-body Inspired Quantum State Preparation](https://arxiv.org/abs/2510.15806)
*Dibyendu Mondal,Debaarjun Mukherjee,Rahul Maitra*

Main category: quant-ph

TL;DR: 提出了一种动态变分量子本征求解器(VQE)的构造策略，通过交换性筛选和能量排序识别主导算符块，逐步扩展并采用降阶张量分解来减少参数数量，有效避免局部陷阱。


<details>
  <summary>Details</summary>
Motivation: 解决VQE方法中表达性ansatz设计与数值稳定性之间的平衡问题，在NISQ时代硬件限制下构建紧凑且能捕捉关键关联效应的ansatz。

Method: 通过交换性筛选和能量排序识别主导算符块，逐步扩展ansatz，采用降阶张量分解减少高阶关联项参数，自适应构造策略优化轨迹。

Result: 在多种分子系统中实现精确能量计算，参数数量显著减少，能有效绕过局部陷阱，在强关联区域如键解离中成功重现基态。

Conclusion: 该渐进算符块添加策略在保持精度的同时大幅减少参数，在强关联区域表现优于现有方法，为NISQ时代的量子化学计算提供了有效解决方案。

Abstract: In the field of quantum chemistry, the variational quantum eigensolver (VQE)
has emerged as a highly promising approach to determine molecular energies and
properties within the noisy intermediate-scale quantum (NISQ) era. The central
challenges of this approach lie in the design of an expressive ansatz capable
of representing the exact ground state wavefunction while concurrently being
efficient to avoid numerical instabilities during the classical optimization.
Owing to the constraints of current quantum hardware, the ansatz must remain
sufficiently compact while retaining the flexibility to capture essential
correlation effects. To address these challenges, we propose a systematic
dynamic ansatz construction strategy in which the dominant operator blocks are
initially identified through commutativity screening, combined with an energy
sorting criteria. Subsequently, the ansatz is progressively expanded in a
stepwise manner via iterative operator block reordering. To minimize the
overhead, the higher order correlation terms are incorporated via reduced
lower-body tensor factorization in each operator block, while the adaptive
construction strategy ensures that the optimization is guided along the optimal
trajectory to mitigate potential numerical instabilities due to the presence of
local traps. Benchmark applications to various molecular systems demonstrate
that this strategy of progressive operator-block addition achieves accurate
energetics with significantly fewer parameters while efficiently bypassing
local traps. Moreover, in strongly correlated regions, such as bond
dissociation, the method successfully reproduces the ground state, where other
contemporary approaches often fail.

</details>


### [45] [Spectral statistics and energy gap-scaling in $k-$local spin Hamiltonians](https://arxiv.org/abs/2510.15829)
*Sasanka Dowarah*

Main category: quant-ph

TL;DR: 研究了全连接相互作用自旋哈密顿量的谱性质，发现无序情况下能级统计的普适类仅取决于系统尺寸L和局域性k的奇偶性，分类为GOE、GUE或GSE系综。对于非零平均耦合，分析了能隙存在条件，发现了小局域性和大局域性两种不同机制。


<details>
  <summary>Details</summary>
Motivation: 为理解随机矩阵普适性、普适能隙标度行为提供一个半可解的玩具模型，并为通过系统修改和耦合探索更一般性质奠定基础。

Method: 研究全连接相互作用自旋哈密顿量，其中耦合系数服从正态分布。分析无序情况下的能级统计普适类，以及非零平均耦合时的能隙存在条件。

Result: 无序情况下能级统计普适类仅取决于L和k的奇偶性；非零平均耦合时，小局域性(k≪√L)下能隙关闭阈值σ∼μ，大局域性(k≫√L)下只要μ>σ就存在能隙。

Conclusion: 该工作提供了一个理解随机矩阵普适性和能隙标度行为的半可解模型，为探索更一般性质奠定了基础。

Abstract: We investigate the spectral properties of all-to-all interacting spin
Hamiltonians acting on exactly $k$ spins whose coupling coefficients are drawn
from a normal distribution with mean $\mu$ and variance $\sigma^2$. For the
completely disordered case $\mu = 0$, we show that the universality class of
level statistics depends solely on the parity of system size $L$ and locality
$k$, classifying these Hamiltonians into the Gaussian Orthogonal (GOE), Unitary
(GUE), or Symplectic (GSE) ensembles. For couplings with a non-zero mean, we
map the Hamiltonians to deformed random matrix ensembles and analyze conditions
for a energy gap between the ground state and the first excited state. We find
two distinct regimes: for small locality ($k \ll \sqrt{L}$), we show that the
gap closing threshold scales proportionally with the mean coupling $\sigma \sim
\mu$, and for large locality ($k \gg \sqrt{L}$) a spectral gap exists as long
as $\mu > \sigma$. We analytically derive the expression for this energy gap in
the $k \gg \sqrt{L}$ limit. Our work provides a semi-solvable toy model for
understanding random matrix universality, universal energy gap scaling, and a
foundation for exploring more general properties through systematic
modifications and couplings.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [46] [The Missing Multipole Problem: Investigating biases from model starting frequency in gravitational-wave analyses](https://arxiv.org/abs/2510.15048)
*Ryan Ursell,Charlie Hoy,Ian Harry,Laura K. Nuttall*

Main category: gr-qc

TL;DR: 论文研究了引力波观测中黑洞碰撞源属性推断的准确性，发现当前探测器对缺失的低频功率敏感，当总质量≥200M⊙时，使用20Hz起始的时域波形模板会导致有偏的源属性估计。


<details>
  <summary>Details</summary>
Motivation: 准确推断碰撞黑洞的真实源属性不仅需要精确的波形模型，还需要正确使用这些模型。选择波形起始时间过晚会遗漏高阶多极子的低频功率，导致源属性估计偏差。

Method: 通过分析总质量≥200M⊙的双星系统，研究当前探测器对缺失低频功率的敏感性，比较不同起始频率（20Hz和13Hz）模板在不同质量比和信噪比条件下的表现。

Result: 对于总质量≲300M⊙、质量比≳0.33、信噪比ρ≳20的系统，20Hz起始模板恢复的源属性存在偏差。随着总质量增加和组分质量不对称性增强，13Hz起始模板也会产生偏差。当信噪比ρ<20时，统计不确定性占主导，20Hz起始模型仍可使用。

Conclusion: 在引力波数据分析中，需要根据系统总质量、质量比和信噪比选择合适的波形起始频率，以避免低频功率缺失导致的源属性估计偏差。

Abstract: Our ability to infer the true source properties of colliding black holes from
gravitational wave observations requires not only accurate waveform models but
also their correct use. A key property when evaluating time-domain models is
when to start the waveform: choosing a time that is too late can omit
low-frequency power from higher order multipoles. By focusing on binary systems
with total mass $\ge 200 \, M_{\odot}$, we show that current detectors are
sensitive to this missing power and biased source properties can be obtained.
We show that for systems with total mass $\lesssim 300 \, M_{\odot}$, mass
ratio $\gtrsim 0.33$, and signal-to-noise ratio $\rho \gtrsim 20$, templates
starting at $20 \, \mathrm{Hz}$ recover biased source properties. As the total
mass increases, and the component masses become more asymmetric, templates
starting from $13 \, \mathrm{Hz}$ recover biased properties. If the
gravitational-wave signal is observed at signal-to-noise ratio $\rho < 20$,
time-domain models can start from $20\, \mathrm{Hz}$ as statistical
uncertainties dominate.

</details>


### [47] [Hints for dynamical dark energy from warm inflation](https://arxiv.org/abs/2510.15051)
*Anupama B*

Main category: gr-qc

TL;DR: 研究热浴耗散对最小暖暴胀模型中宇宙微波背景温度各向异性的影响，发现相移和峰值变化暗示暗能量的动力学性质，需要修改标准宇宙学模型参数。


<details>
  <summary>Details</summary>
Motivation: 研究最小暖暴胀中热浴耗散对CMB温度各向异性的影响，探索解决宇宙学中持续存在的张力问题，特别是哈勃张力。

Method: 分析最小暖暴胀模型中热浴耗散对CMB理论TT模式角功率谱的影响，观察随着耗散强度增加时相移和峰值的变化。

Result: 发现CMB角功率谱的相和峰值随耗散强度增加而发生偏移，表明暗能量具有动力学性质，需要修改标准ΛCDM模型的宇宙学参数。

Conclusion: 研究结果为解释早期宇宙加速膨胀提供了新方向，可能缓解宇宙学中的持续张力，特别是哈勃张力，指向超越标准宇宙学模型的物理机制。

Abstract: The effect of dissipation from the thermal bath of minimal warm inflation
(MWI) on the temperature anisotropies of the cosmic microwave background (CMB)
is investigated. A shift in the phase and peaks of the theoretical TT mode
angular power spectrum of CMB for MWI with increasing dissipation strength
hints at the dynamical nature of dark energy from warm inflation. Outcome of
the study highlights the necessity of modifying the cosmological parameters,
signalling a physics beyond the standard $\Lambda$CDM model of cosmology. The
current findings may open a new direction for explaining the increased
expansion rate in the early universe, thus alleviating some of the persistent
tensions in cosmology, especially the Hubble tension.

</details>


### [48] [The exponential metric: traversable wormhole and possible identification of scalar background](https://arxiv.org/abs/2510.15391)
*Eduard Mychelkin,Gulnara Suliyeva,Maxim Makukov*

Main category: gr-qc

TL;DR: 该论文重新分析了爱因斯坦-克莱因-戈登方程的静态反标量解（Papapetrou指数度量），发现真正的物理效应发生在r=M/2处而非之前认为的虫洞喉部r=M处，并将标量背景解释为由环境准静态电场中性叠加形成的稳定介质。


<details>
  <summary>Details</summary>
Motivation: 重新审视Papapetrou指数度量作为可穿越虫洞的解释，寻找与该尺度相关的物理效应，并探索标量背景的物理本质。

Method: 分析爱因斯坦-克莱因-戈登方程的静态反标量解，研究几何不变量、拓扑不变量和测地线性质，结合爱因斯坦-麦克斯韦方程的反标量静态极限进行对比分析。

Result: 发现拓扑高斯-博内不变量在r=M处改变符号，开普勒频率在该处发散；但真正的物理效应发生在r=M/2处，该处曲率张量的几何不变量取极值，标量背景的热力学特性也在此处达到极值。

Conclusion: 标量背景应被解释为由环境准静态电场中性叠加形成的具有刚性状态方程的稳定介质，而非传统虫洞结构，真正的物理尺度是r=M/2而非r=M。

Abstract: The static antiscalar solution of the Einstein-Klein-Gordon equations in the
form of the Papapetrou exponential metric had been interpreted as a traversable
wormhole with a throat at \textit{r=M}. We aim to search for the effects which
could be associated with this scale and only find that the topological
Gauss-Bonnet invariant swaps sign, and the value of the Keplerian frequency for
circular geodesics becomes singular. At the same time, the geometric invariants
of the curvature tensor have extremal values at the scale twice less than that
of the throat, revealing new physical effects. In particular, the Ricci scalar
at \textit{r=M/2} (rather than \textit{r=M}) is associated with the extremal
values of thermodynamic characteristics of the scalar background. This approach
in combination with the antiscalar static limit of the Einstein-Maxwell
equations suggests the interpretation of the scalar background as a stable
medium with a stiff equation of state, formed by the neutral superposition of
ambient quasistatic electric fields.

</details>


### [49] [Aiming for Proxima Centauri b: Gravitational effects on relativistic spacecraft trajectories](https://arxiv.org/abs/2510.15827)
*Mark C. Baumann,Justin C. Feng,Nicky Ishaak*

Main category: gr-qc

TL;DR: 该论文分析了引力效应和相对论效应对星际旅行的重要性，特别关注以相对论速度飞行的激光推进航天器任务。研究发现，太阳对航天器轨迹影响最大，如果希望以优于约690,000公里的精度击中目标，必须考虑相对论效应。


<details>
  <summary>Details</summary>
Motivation: 研究引力效应和相对论效应对星际旅行的重要性，特别是针对以相对论速度飞行的激光推进航天器任务，如前往比邻星b的任务。

Method: 使用Julia重新实现的PoMiN代码，这是一个在广义相对论的第一后闵可夫斯基近似下建模相对论引力动力学的N体代码。计算了七个不同天体的引力影响，并开发了数值微调方法来求解精确的初始数据。

Result: 太阳对航天器轨迹影响最大；如果希望以优于约690,000公里的精度击中比邻星b，必须考虑相对论效应；数值微调方法在约4.25光年的旅行距离内可达到飞米级精度；高阶广义相对论效应可将航天器最终位置偏移数十公里。

Conclusion: 对于以相对论速度飞行的星际航天器任务，引力效应和相对论效应至关重要，特别是太阳的引力影响。即使正确考虑了相对论效应，发射和助推阶段的初始速度误差仍可能主导脱靶距离。

Abstract: How important are gravitational and relativistic effects for interstellar
travel? We consider this question in the context of proposed laser-propelled
spacecraft missions to neighboring stellar destinations. Our analysis applies
to any spacecraft traveling at relativistic speeds. As a concrete example, we
focus on a mission to Proxima Centauri b -- a terrestrial-sized planet in the
habitable zone around our nearest stellar neighbor, Proxima Centauri. We employ
a Julia reimplementation of the PoMiN code, an N-body code modeling
relativistic gravitational dynamics in the first post-Minkowskian (PM)
approximation to general relativity (valid to linear order in Newton's constant
$G$). We compute the gravitational influence of seven different celestial
bodies and find that the Sun has the greatest influence on the trajectory of
the interstellar spacecraft. We also study the differences between Newtonian
and PM gravity, and find that if mission planners wish to hit Proxima Centauri
b with an accuracy of better than about 690,000 kilometers, relativistic
effects must be taken into account. To solve for the precise initial data
needed to hit an intended target, we develop numerical fine-tuning methods and
demonstrate that these methods can (within a given model) be precise to about a
femtometer over a travel distance of $\sim4.25$ light years. However, we find
that for the spacecraft trajectories we consider, higher order general
relativistic effects (beyond the first PM approximation) from the Sun can
displace the final position of the spacecraft by tens of kilometers. We also
consider the variation in the initial direction of the spacecraft velocity and
find that, even with relativistic effects properly taken into account, the miss
distances can be dominated by the variation in the initial velocity that could
arise from errors during the launch and boost phase of the spacecraft mission.

</details>


### [50] [Anisotropic Dark Matter Bosonic Stars in regularized 4D Einstein$-$Gauss$-$Bonnet gravity](https://arxiv.org/abs/2510.15549)
*Mohamamd Mazhari*

Main category: gr-qc

TL;DR: 在正则化4D爱因斯坦-高斯-博内引力理论中构建了各向异性玻色子暗物质星解，通过数值分析研究了质量-半径关系和稳定性指标，发现正的高斯-博内耦合增强最大质量和致密度，而负的各向异性降低这些参数。


<details>
  <summary>Details</summary>
Motivation: 扩展先前各向同性玻色子星分析，在正则化4D EGB框架中系统性地纳入各向异性，为修正引力下的致密暗物质天体提供观测相关预测。

Method: 使用维度正则化求解修正的Tolman-Oppenheimer-Volkoff方程，针对自相互作用复标量场在稀薄多方状态(p_r = K ρ^2)下，参数化各向异性为σ = β p_r (1 - e^{-2λ})，在(α,β)参数域进行综合数值分析。

Result: 正高斯-博内耦合增强最大质量和致密度(从α=0时的1.62 M⊙增加到α=8 km²时的2.09 M⊙)，负各向异性降低这些参数(从β=0时的2.21 M⊙降到β=-2时的1.73 M⊙)，构型在质量峰值前保持静态稳定并满足物理判据。

Conclusion: 这项工作在正则化4D EGB框架中系统性地结合了各向异性，扩展了玻色子星研究，为修正引力下的致密暗物质天体提供了观测相关预测。

Abstract: In this work, we have constructed anisotropic bosonic dark-matter star (DMS)
solutions in the context of a regularized four-dimensional
Einstein$-$Gauss$-$Bonnet (4D EGB) gravity theory. Using dimensional
regularization, we solve modified Tolman$-$Oppenheimer$-$Volkoff equations for
a self-interacting complex scalar field in the dilute polytropic regime, $p_r =
K \rho^2$, with anisotropy parameterized as $\sigma = \beta\, p_r \left( 1 -
e^{-2\lambda} \right)$. We perform a comprehensive numerical analysis across
the \((\alpha,\beta)\) parameter domain, where \(\alpha \in
[0,8]~\mathrm{km}^2\) and \(\beta \in [-2,0]\), to examine mass$-$radius
relations and evaluate multiple stability indicators including static
equilibrium \(dM/dp_c\), sound-speed causality, the radial adiabatic index
\(\Gamma_r\), and energy conditions. Positive Gauss$-$Bonnet coupling enhances
both the maximum mass and compactness (e.g., \(M_{\rm max} \approx 1.62\,
M_\odot\) at \(\alpha=0\) rising to \(\approx 2.09\, M_\odot\) at \(\alpha =
8~\mathrm{km}^2\)), while negative anisotropy reduces them (e.g., from
\(\approx 2.21\, M_\odot\) at \(\beta=0\) to \(\approx 1.73\, M_\odot\) at
\(\beta = -2\)). The resulting configurations remain statically stable up to
the mass peak and satisfy physical criteria. This work extends previous
isotropic boson-star analyses by systematically incorporating anisotropy within
a regularized 4D EGB framework. These findings provide observationally relevant
predictions for compact dark-matter objects under modified gravity.

</details>


### [51] [Signals from Fermionic inflationary cosmology with Yukawa interaction](https://arxiv.org/abs/2510.15609)
*Lin-Hong Sui,Dan Li,Jia-Ze Sun,Xi-Bin Li*

Main category: gr-qc

TL;DR: 该论文研究了狄拉克场与暴胀子通过Yukawa相互作用耦合的暴胀模型，发现当有效质量较大时，张量-标量比r会被抑制，而当有效质量较小时，r与标准冷暴胀一致。


<details>
  <summary>Details</summary>
Motivation: 研究狄拉克场与暴胀子的Yukawa相互作用对暴胀观测结果的影响，特别是在高能标下的非绝热效应。

Method: 在慢滚近似下推导狄拉克方程的解析解，分析费米子对密度，研究Yukawa相互作用强度g对非绝热程度的影响。

Result: 当有效质量m̃≳1时，张量-标量比r被抑制约1/(1+2.95π²g²)倍；当m̃≪1时，r与标准冷暴胀一致。大有效质量条件伴随显著的反作用效应。

Conclusion: Yukawa相互作用强度g表征非绝热程度，在高能标暴胀中，有效质量大小决定张量-标量比的抑制程度，这对暴胀模型的观测约束有重要意义。

Abstract: We investigate an inflationary model wherein the Dirac field $\psi$ is
directly coupled to a scalar inflaton $\phi$ via a Yukawa interaction
$g\phi\bar\psi\psi$ and examine the resulting observational implications.
Within the slow-roll approximation, we derive analytical solutions of the Dirac
equations during inflation. The analytical result on the fermion pair density
$\langle n\rangle$ indicates that the Yukawa interaction strength $g$ is to
characterize the degree of non-adiabaticity. For large value of the
dimensionless effective mass $\tilde m=(m+g\phi)/H$, i.e. $\tilde m\gtrsim 1$,
the tensor-to-scalar ratio $r$ is suppressed by a factor of approximately
$1/(1+2.95\pi^2g^2)$. This condition is also characterized by a significant
backreaction. Conversely, if $\tilde m \ll 1$, the value of $r$ remains
consistent with that observed in standard cold inflation. Our analysis is
performed under the assumption of the highest inflationary energy scales
compatible with current observational constraints.

</details>


### [52] [Absorption and scattering of massless scalar waves by black holes in quasi-topological gravity](https://arxiv.org/abs/2510.15636)
*SiHao Fan,Chen Wu,WenJun Guo*

Main category: gr-qc

TL;DR: 研究了准拓扑引力中规则黑洞的无质量标量场扰动，计算了吸收截面，发现参数α越大总吸收截面越小，维度D的影响与先前研究不同。


<details>
  <summary>Details</summary>
Motivation: 分析准拓扑引力中规则黑洞的无质量标量场扰动特性，特别是吸收和散射行为，以理解这类黑洞的物理性质。

Method: 使用分波法计算了不同参数下的总吸收截面和部分吸收截面，回顾了多种规则黑洞解。

Result: 参数α越大，总吸收截面越小；维度D对吸收和散射的影响与早期计算结果存在差异。

Conclusion: 准拓扑引力中的规则黑洞在标量场扰动下表现出独特的吸收特性，参数α和维度D对物理过程有显著影响。

Abstract: We consider massless scalar field perturbations of regular black holes in
quasi-topological gravity. After reviewing various regular black hole
solutions, we compute the total absorption cross-section and the partial
absorption cross-section for different choices of the parameters using the
partial wave method. The results show that the larger the parameter ${\alpha}$,
introduced in the context of quasi-topological gravity, the smaller the total
absorption cross-section. In addition, we find that the influence of the
space-time dimension ${D}$ on the absorption and scattering of massless scalar
waves in this set of regular black holes differs from earlier computations.

</details>


### [53] [Geodesic structure of a noncommutative black hole](https://arxiv.org/abs/2510.15702)
*Zihan Xi,Chen Wu,Wenjun Guo*

Main category: gr-qc

TL;DR: 本文研究了Piero Nicolini非对易黑洞时空的度规，计算了有效势并绘制了势能曲线，分析了该时空中测试粒子和光子的轨道类型，以及质量和角动量对类时测地线的影响。


<details>
  <summary>Details</summary>
Motivation: 探索非对易黑洞时空中的轨道动力学特性，特别是研究总质量和角动量对粒子轨道行为的影响。

Method: 计算Piero黑洞时空的有效势能，分析势能曲线，求解粒子和光子的动力学方程，绘制类时和零测地线结构，并研究不同质量和角动量参数的影响。

Result: 在Piero黑洞时空中，总质量和角动量的增加会降低轨道的近日点进动率，其中质量的影响是非线性的，而角动量的影响是线性的。

Conclusion: 非对易黑洞时空中的轨道动力学表现出独特的特性，质量和角动量对轨道进动有显著影响，这为理解量子引力效应提供了新的视角。

Abstract: This paper explores the metric of Piero Nicolini's noncommutative black hole
spacetime, calculates its effective potential, and presents the corresponding
potential curve. By analyzing this curve, we identify various orbit types for
test particles and photons in this spacetime. Using the dynamical equations for
particles and photons near the black hole, we plot the specific time-like and
null geodesic structures. We analyze the impact of different values of the
total mass of the source $M$ and angular momentum $L$ on time-like geodesics.
Our results indicate that in the Piero black hole spacetime, increases in total
mass and angular momentum reduce the perihelion precession rate of the orbit.
Notably, the effect of total mass is nonlinear, while the effect of angular
momentum is linear.

</details>


### [54] [Relativistic tidal divergences in circular orbits and the dynamics of light rings](https://arxiv.org/abs/2510.15705)
*Victor F. C. Vieira,Rafael P. Bernar,Caio F. B. Macedo*

Main category: gr-qc

TL;DR: 本文研究了超致密天体在圆形测地线运动中的相对论潮汐力，发现当轨道接近零圆测地线时潮汐力会发散，这可能与无视界超致密天体的非线性稳定性有关。


<details>
  <summary>Details</summary>
Motivation: 研究引力场不均匀性产生的潮汐力对轨道天体的影响，特别是在超致密天体背景下，分析相对论潮汐力的行为特征。

Method: 分析静态球对称时空中的超致密天体，关注圆形测地线运动的观测者，研究两种均匀密度恒星模型：各向同性模型和纯切向应力支撑模型。

Result: 与径向测地线情况不同，当轨道接近零圆测地线时，潮汐力会发散。

Conclusion: 光环附近的潮汐力发散可能在无视界超致密天体的非线性稳定性中发挥作用。

Abstract: Tidal forces acting on orbiting bodies arise from inhomogeneities in the
gravitational field, generating stresses that can deform or even disrupt these
objects. In this work, we analyze relativistic tidal forces associated with
ultracompact objects described by static and spherically symmetric spacetimes,
focusing on observers in circular geodesic motion. We show that, in contrast to
the case of radial geodesics, tidal forces diverge as the orbit approaches null
circular geodesics. As illustrative examples, we study two uniform-density
stellar models: one isotropic and another supported purely by tangential
stresses. We conjecture that the divergence of tidal forces near light rings
may play a role in the nonlinear stability of ultracompact, horizonless
objects.

</details>


### [55] [Scalar fields with power-law potentials in quantum cosmology](https://arxiv.org/abs/2510.15735)
*V. E. Kuzmichev,V. V. Kuzmichev*

Main category: gr-qc

TL;DR: 该论文研究了具有幂律势标量场的量子宇宙学系统，该标量场可产生各种形式的物质（如刚体物质、辐射、尘埃等），并基于量子几何动力学方法获得了相应的量子哈密顿约束方程及其解析解。


<details>
  <summary>Details</summary>
Motivation: 研究具有幂律势标量场的量子宇宙学系统，探索该标量场在不同时期如何产生各种形式的物质，并建立相应的量子理论框架。

Method: 基于量子几何动力学方法，针对最大对称空间，推导了标量场势能各幂律项的微分方程和相应的量子哈密顿约束方程。

Result: 获得了描述宇宙被不同形式物质主导的量子哈密顿约束方程，并找到了这些方程的解析解。

Conclusion: 成功建立了具有幂律势标量场的量子宇宙学理论框架，获得了描述各种物质形式主导宇宙的量子方程及其解析解。

Abstract: A homogeneous and isotropic quantum cosmological system (universe) initially
filled with a uniform scalar field that has a potential in the power law
representation is considered. Depending on the epoch, this scalar field yields
barotropic matter in the form of stiff matter, perfect gas, radiation, dust,
cosmic strings, domain walls, de Sitter vacuum, or phantom matter. The proposed
approach is based on quantum geometrodynamics for the maximally symmetric
space. The relevant differential equations for the separate power-law summands
of the scalar field potential and the corresponding quantum Hamiltonian
constraint equations, which describe the universe that can be viewed as
dominated by one form or another of barotropic matter, were obtained. The
solutions to these equations have been found in analytical form.

</details>


### [56] [Unified field theory from Hamilton cotangent bundle geometry - The Einstein-Maxwell system](https://arxiv.org/abs/2510.15812)
*Christian Pfeifer,José Javier Relancio*

Main category: gr-qc

TL;DR: 该论文提出了一种通过汉密尔顿几何在相空间中统一物理场的新方法，将物理场编码到时空1粒子相空间的几何结构中，从而统一引力和电磁场。


<details>
  <summary>Details</summary>
Motivation: 统一所有物理场到一个数学对象中，并从一个框架推导出所有物理场方程是基础物理学中长期追求的目标。

Method: 通过汉密尔顿几何将物理场编码到相空间几何中，构建相空间中标量场的作用量原理，并推导相应的标量场方程。

Result: 通过选择特定标量场（描述带电粒子在弯曲时空中的哈密顿量），证明相空间标量场方程等价于时空中的耦合爱因斯坦-麦克斯韦方程，实现了引力和电磁场的几何统一。

Conclusion: 该方法为在相空间几何框架下统一描述更多物理场及其动力学提供了新的可能性，与之前的统一尝试有所不同。

Abstract: The unification of all physical fields into one mathematical object and the
derivation of all physical field equations from that object in one framework is
a long-lasting endeavor in fundamental physics. We suggest a new approach to
achieve this goal by encoding physical fields into the geometry of the
1-particle phase space on spacetime (the cotangent bundle) through Hamilton
geometry. The fundamental field, which contains information about all physical
fields in spacetime and defines the phase space geometry, is a scalar field in
phase space that is interpreted as a point-particle Hamiltonian. We construct
an action principle for scalar fields in phase space and derive the
corresponding scalar field equation. By choosing a specific scalar field,
namely the Hamiltonian describing a charged particle in curved spacetime with
an electromagnetic field, we show that this phase-space scalar field equation
is equivalent to the coupled Einstein-Maxwell equations in spacetime, thus
providing a geometric unification of gravity and electromagnetism. We further
discuss how this approach differs from previous unification attempts and its
potential for describing further physical fields and their dynamics in a
unified manner in terms of phase-space geometry.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [57] [Random walk models of anisotropic diffusion on rectangular and hexagonal lattices](https://arxiv.org/abs/2510.15291)
*Luke P. Filippini,Adrianne L. Jenner,Elliot J. Carr*

Main category: physics.comp-ph

TL;DR: 本文提出了几种等效的随机游走模型来模拟二维各向异性介质中的扩散过程，使用矩形和六边形网格，通过有限体积法和时间离散化得到转移概率的解析表达式。


<details>
  <summary>Details</summary>
Motivation: 确定性模型无法捕捉扩散过程的随机性，且假设粒子数量众多，限制了其适用性。需要开发能够处理各向异性介质中随机扩散的模型。

Method: 使用顶点中心有限体积法在矩形和六边形网格上进行空间离散化，结合前向欧拉时间离散化，构建齐次马尔可夫链模型，得到最近邻随机游走的转移概率解析表达式。

Result: 矩形网格需要对扩散张量施加约束，而六边形网格无此限制。模型与确定性模型在视觉和均方误差方面都表现出良好的一致性。

Conclusion: 提出的随机游走模型能够有效模拟各向异性介质中的扩散过程，六边形网格相比矩形网格具有更好的灵活性，所有结果可通过GitHub上的MATLAB代码复现。

Abstract: The diffusive transport of particles in anisotropic media is a fundamental
phenomenon in computational, medical and biological disciplines. While
deterministic models (partial differential equations) of such processes are
well established, their inability to capture inherent randomness, and the
assumption of a large number of particles, hinders their applicability. To
address these issues, we present several equivalent (discrete-space
discrete-time) random walk models of diffusion described by a
spatially-invariant tensor on a two-dimensional domain with no-flux boundary
conditions. Our approach involves discretising the deterministic model in space
and time to give a homogeneous Markov chain governing particle movement between
(spatial) lattice sites over time. The spatial discretisation is carried out
using a vertex-centred element-based finite volume method on rectangular and
hexagonal lattices, and a forward Euler discretisation in time yields a
nearest-neighbour random walk model with simple analytical expressions for the
transition probabilities. For each lattice configuration, analysis of these
expressions yields constraints on the time step duration, spatial steps and
diffusion tensor to ensure the probabilities are between zero and one. We find
that model implementation on a rectangular lattice can be achieved with a
constraint on the diffusion tensor, whereas a hexagonal lattice overcomes this
limitation (no restrictions on the diffusion tensor). Overall, the results
demonstrate good visual and quantitative (mean-squared error) agreement between
the deterministic model and random walk simulations for several test cases. All
results are obtained using MATLAB code available on GitHub
(https://github.com/lukefilippini/Filippini2025).

</details>


### [58] [Constrained bilinear optimal control of reactive evolution equations](https://arxiv.org/abs/2510.15293)
*Zhexian Li,Felipe de Barros,Ketan Savla*

Main category: physics.comp-ph

TL;DR: 提出了一种新的优化-离散化框架，用于计算带状态和控制约束的双线性最优控制问题，通过积分表示代替PDE约束，避免了微分算子，能够高效求解。


<details>
  <summary>Details</summary>
Motivation: 现有计算方法只考虑控制变量约束，而实际问题中状态变量也需要约束。本文旨在开发能同时处理状态和控制约束的框架。

Method: 采用优化-离散化方法，首先用统一变换方法推导的积分表示代替PDE约束，然后利用KKT条件推导最优性必要条件，最后离散化得到光滑非线性方程组。

Result: 计算结果表明该框架在核反应性控制和反应器水质处理两个应用中有效，能够高效求解带约束的双线性最优控制问题。

Conclusion: 所提出的优化-离散化框架成功解决了带状态和控制约束的双线性最优控制问题，相比离散化-优化方法具有计算效率优势。

Abstract: We consider constrained bilinear optimal control of second-order linear
evolution partial differential equations (PDEs) with a reaction term on the
half line, where control arises as a time-dependent reaction coefficient and
constraints are imposed on the state and control variables. These PDEs
represent a wide range of physical phenomena in fluid flow, heat, and mass
transfer. Existing computational methods for this type of control problems only
consider constraints on control variables. In this paper, we propose a novel
optimize-then-discretize framework for computing constrained bilinear optimal
control with both state and control constraints. Unlike existing methods that
derive optimality conditions directly from the PDE constraint, this framework
first replaces the PDE constraint with an equivalent integral representation of
the PDE solution. The integral representation, derived from the unified
transform method, does not involve differential operators, and thus explicit
expressions for necessary conditions of optimality can be derived using the
Karush-Kuhn-Tucker conditions for infinite-dimensional optimization.
Discretizing the optimality conditions results in a system of
finite-dimensional smooth nonlinear equations, which can be efficiently solved
using existing solvers without the need for specialized algorithms. This is in
contrast with discretize-then-optimize methods that discretize the PDE first
and then solve the optimality conditions of the approximated finite-dimensional
problem. Computational results for two applications, namely nuclear reactivity
control and water quality treatment in a reactor, are presented to illustrate
the effectiveness of the proposed framework.

</details>


### [59] [Towards In-Situ Failure Assessment: Deep Learning on DIC Results for Laminated Composites](https://arxiv.org/abs/2510.15424)
*Amir Mohammad Mirzaei*

Main category: physics.comp-ph

TL;DR: 开发了一种基于DIC应变场数据的深度学习框架，使用MLP和CNN两种架构来预测带应力集中器的层合复合材料断裂载荷，无需有限元模拟或经验校准。


<details>
  <summary>Details</summary>
Motivation: 预测带应力集中器的层合复合材料断裂载荷具有挑战性，因为复杂的失效机制（如分层、纤维断裂和基体开裂）受纤维取向、铺层顺序和缺口几何形状的强烈影响。

Method: 开发了两种互补的深度学习架构：1）多层感知器（MLP）处理缺口前方目标矩形区域的最大主应变数值，通过互信息、Lasso和SHAP等先进特征选择方法聚焦关键数据点；2）卷积神经网络（CNN）在全场应变图像上训练，通过数据增强处理变异性并防止过拟合。

Result: 在116个准静态试验（涵盖31种不同配置）上验证，MLP和CNN的决定系数（R²）分别达到0.86和0.82，能够捕捉从脆性纤维主导断裂到韧性分层驱动失效的广泛损伤模式和响应。

Conclusion: 该框架计算效率高且仅依赖DIC测量，能够实现实用的原位断裂载荷估计，为复合材料断裂预测提供了新的有效方法。

Abstract: Predicting fracture load in laminated composites with stress raisers is
challenging due to complex failure mechanisms such as delamination, fibre
breakage, and matrix cracking, which are heavily influenced by fibre
orientation, layup sequence, and notch geometry. This study aims to address
this by developing a novel deep learning framework that leverages solely
experimental strain field data from Digital Image Correlation (DIC) for
accurate, in-situ predictions--bypassing the need for finite element
simulations or empirical calibrations. Two complementary architectures are
explored: a multi-layer perceptron (MLP) that processes numerical values of
maximum principal strain from a targeted rectangular region ahead of the notch,
enhanced by advanced feature selection (mutual information, Lasso, and SHAP) to
focus on critical data points; and a convolutional neural network (CNN) trained
on full-field strain images, bolstered by data augmentation to handle
variability and prevent overfitting. Validated across 116 quasi-static tests
encompassing 31 distinct configurations--including six layups (quasi-isotropic
to highly anisotropic) with four off-axis angles for open-hole specimens, and
one cross-ply layup with four off-axis and four on-axis notch orientations for
U-notched specimens--the MLP and CNN achieve coefficients of determination
(R^2) of 0.86 and 0.82, respectively. This framework captures a broad spectrum
of damage modes and responses, from brittle fibre-dominated fracture to ductile
delamination-driven failure, and due to its computational efficiency and
reliance only on DIC measurements, the approach enables practical in-situ
fracture load estimation.

</details>


### [60] [Boundary-Informed Method of Lines for Physics Informed Neural Networks](https://arxiv.org/abs/2510.15852)
*Maximilian Cederholm,Siyao Wang,Haochun Wang,Ruichen Xu,Yuefan Deng*

Main category: physics.comp-ph

TL;DR: 提出了一种融合方法线(MOL)降维优势和物理信息神经网络(PINNs)灵活性的混合求解器，通过神经网络表示初始空间剖面，使用自动微分获得高精度梯度，用时间PINN替代时间积分，在保持空间精度的同时大幅减少计算点数量。


<details>
  <summary>Details</summary>
Motivation: 传统方法线(MOL)使用固定有限差分格式近似空间导数，截断误差迫使需要极细网格，导致计算成本高昂。需要一种既能保持高精度又能减少计算资源依赖的方法。

Method: 训练神经网络表示初始空间剖面，通过自动微分获得谱精度梯度，这些高保真导数定义MOL生成的常微分系统右侧，用时间PINN替代时间积分，同时保持空间精度无需网格细化。

Result: 边界信息MOL-PINN在精度上匹配或超越传统MOL，使用少一个数量级的配置点，显著减少内存占用，降低对大数据的依赖，提高复杂度鲁棒性。

Conclusion: 该框架仅依赖自动微分和标准优化器，可自然扩展到任意空间维度的线性和非线性偏微分方程，提供了一种高效且通用的PDE求解方法。

Abstract: We propose a hybrid solver that fuses the dimensionality-reduction strengths
of the Method of Lines (MOL) with the flexibility of Physics-Informed Neural
Networks (PINNs). Instead of approximating spatial derivatives with fixed
finite-difference stencils - whose truncation errors force extremely fine
meshes - our method trains a neural network to represent the initial spatial
profile and then employs automatic differentiation to obtain spectrally
accurate gradients at arbitrary nodes. These high-fidelity derivatives define
the right-hand side of the MOL-generated ordinary-differential system, and time
integration is replaced with a secondary temporal PINN while spatial accuracy
is retained without mesh refinement. The resulting "boundary-informed MOL-PINN"
matches or surpasses conventional MOL in accuracy using an order of magnitude
fewer collocation points, thereby shrinking memory footprints, lessening
dependence on large data sets, and increasing complexity robustness. Because it
relies only on automatic differentiation and standard optimizers, the framework
extends naturally to linear and nonlinear PDEs in any spatial dimension.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [61] [Extending Load Forecasting from Zonal Aggregates to Individual Nodes for Transmission System Operators](https://arxiv.org/abs/2510.14983)
*Oskar Triebe,Fletcher Passow,Simon Wittner,Leonie Wagner,Julio Arend,Tao Sun,Chad Zanocco,Marek Miltner,Arezou Ghesmati,Chen-Hao Tsai,Christoph Bergmeir,Ram Rajagopal*

Main category: cs.LG

TL;DR: 设计了一个多层级电力负荷预测系统，将区域预测扩展到节点级预测，提高预测精度和可解释性，帮助电网运营商更好地管理电力负荷不确定性。


<details>
  <summary>Details</summary>
Motivation: 可持续能源发展增加了电力负荷的不确定性，电网运营商需要更高空间分辨率的负荷预测，从区域级扩展到节点级，但节点负荷预测精度较低且管理困难。

Method: 开发了可解释且可扩展的预测模型，采用完全并行化的单模型预测工作流，解决了节点负荷的异质性和波动性问题。

Result: 区域预测的准确性和可解释性得到改善，节点预测有显著提升，运营商能够以前所未有的信心和准确性调整预测，并精确诊断错误。

Conclusion: 多层级预测系统使电网运营商能够有效管理节点级负荷预测，提高了电网运行的可靠性和效率。

Abstract: The reliability of local power grid infrastructure is challenged by
sustainable energy developments increasing electric load uncertainty.
Transmission System Operators (TSOs) need load forecasts of higher spatial
resolution, extending current forecasting operations from zonal aggregates to
individual nodes. However, nodal loads are less accurate to forecast and
require a large number of individual forecasts, which are hard to manage for
the human experts assessing risks in the control room's daily operations
(operator). In collaboration with a TSO, we design a multi-level system that
meets the needs of operators for hourly day-ahead load forecasting. Utilizing a
uniquely extensive dataset of zonal and nodal net loads, we experimentally
evaluate our system components. First, we develop an interpretable and scalable
forecasting model that allows for TSOs to gradually extend zonal operations to
include nodal forecasts. Second, we evaluate solutions to address the
heterogeneity and volatility of nodal load, subject to a trade-off. Third, our
system is manageable with a fully parallelized single-model forecasting
workflow. Our results show accuracy and interpretability improvements for zonal
forecasts, and substantial improvements for nodal forecasts. In practice, our
multi-level forecasting system allows operators to adjust forecasts with
unprecedented confidence and accuracy, and to diagnose otherwise opaque errors
precisely.

</details>


### [62] [Automotive Crash Dynamics Modeling Accelerated with Machine Learning](https://arxiv.org/abs/2510.15201)
*Mohammad Amin Nabian,Sudeep Chavare,Deepak Akhare,Rishikesh Ranade,Ram Cherukuri,Srinivas Tadepalli*

Main category: cs.LG

TL;DR: 本研究探索了基于机器学习的代理模型在汽车碰撞结构变形预测中的应用，使用NVIDIA PhysicsNeMo框架比较了MeshGraphNet和Transolver两种神经网络架构，以及三种瞬态动力学建模策略。


<details>
  <summary>Details</summary>
Motivation: 传统的高保真有限元仿真计算成本高、耗时，需要开发更高效的碰撞安全性评估方法。

Method: 使用MeshGraphNet和Transolver神经网络架构，结合时间条件、标准自回归和稳定性增强自回归三种策略，基于150个LS-DYNA详细FE仿真数据集进行建模。

Result: 模型能够合理捕捉整体变形趋势，计算成本降低数个数量级，但尚未达到完整FE仿真的精度。

Conclusion: 证明了机器学习在结构碰撞动力学中应用的可行性，为快速设计探索和早期优化提供了有效工具。

Abstract: Crashworthiness assessment is a critical aspect of automotive design,
traditionally relying on high-fidelity finite element (FE) simulations that are
computationally expensive and time-consuming. This work presents an exploratory
comparative study on developing machine learning-based surrogate models for
efficient prediction of structural deformation in crash scenarios using the
NVIDIA PhysicsNeMo framework. Given the limited prior work applying machine
learning to structural crash dynamics, the primary contribution lies in
demonstrating the feasibility and engineering utility of the various modeling
approaches explored in this work. We investigate two state-of-the-art neural
network architectures for modeling crash dynamics: MeshGraphNet, and
Transolver. Additionally, we examine three strategies for modeling transient
dynamics: time-conditional, the standard Autoregressive approach, and a
stability-enhanced Autoregressive scheme incorporating rollout-based training.
The models are evaluated on a comprehensive Body-in-White (BIW) crash dataset
comprising 150 detailed FE simulations using LS-DYNA. The dataset represents a
structurally rich vehicle assembly with over 200 components, including 38 key
components featuring variable thickness distributions to capture realistic
manufacturing variability. Each model utilizes the undeformed mesh geometry and
component characteristics as inputs to predict the spatiotemporal evolution of
the deformed mesh during the crash sequence. Evaluation results show that the
models capture the overall deformation trends with reasonable fidelity,
demonstrating the feasibility of applying machine learning to structural crash
dynamics. Although not yet matching full FE accuracy, the models achieve
orders-of-magnitude reductions in computational cost, enabling rapid design
exploration and early-stage optimization in crashworthiness evaluation.

</details>


### [63] [TangledFeatures: Robust Feature Selection in Highly Correlated Spaces](https://arxiv.org/abs/2510.15005)
*Allen Daniel Sunny*

Main category: cs.LG

TL;DR: 提出了TangledFeatures框架，用于在相关特征空间中进行特征选择，通过识别纠缠预测因子组中的代表性特征来减少冗余并保留解释力。


<details>
  <summary>Details</summary>
Motivation: 传统特征选择方法主要关注预测准确性，但在存在相关预测因子的情况下性能会下降，需要一种能够处理相关特征空间的方法。

Method: 引入TangledFeatures框架，识别相关特征组中的代表性特征，减少冗余同时保持解释能力。

Result: 在Alanine Dipeptide数据集上验证了有效性，所选特征对应于结构上有意义的原子间距离，能够解释骨架扭转角的变化。

Conclusion: TangledFeatures提供比传统选择技术更可解释和稳定的分析基础，可直接应用于下游模型。

Abstract: Feature selection is a fundamental step in model development, shaping both
predictive performance and interpretability. Yet, most widely used methods
focus on predictive accuracy, and their performance degrades in the presence of
correlated predictors. To address this gap, we introduce TangledFeatures, a
framework for feature selection in correlated feature spaces. It identifies
representative features from groups of entangled predictors, reducing
redundancy while retaining explanatory power. The resulting feature subset can
be directly applied in downstream models, offering a more interpretable and
stable basis for analysis compared to traditional selection techniques. We
demonstrate the effectiveness of TangledFeatures on Alanine Dipeptide, applying
it to the prediction of backbone torsional angles and show that the selected
features correspond to structurally meaningful intra-atomic distances that
explain variation in these angles.

</details>


### [64] [ES-C51: Expected Sarsa Based C51 Distributional Reinforcement Learning Algorithm](https://arxiv.org/abs/2510.15006)
*Rijul Tandon,Peter Vamplew,Cameron Foale*

Main category: cs.LG

TL;DR: 提出了ES-C51算法，通过将C51中的贪婪Q学习更新替换为Expected Sarsa更新，使用softmax结合所有可能动作的信息，解决了当多个动作具有相似期望奖励但不同分布时的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于价值的RL算法只估计期望奖励，而分布强化学习估计完整的奖励概率分布。但C51算法在多个动作具有相似期望奖励但不同分布时，由于贪婪更新可能导致分布学习不稳定。

Method: 修改C51算法，用Expected Sarsa更新替换贪婪Q学习更新，使用softmax计算结合所有动作信息，并将探索策略从ε-greedy改为softmax进行公平比较。

Result: 在Gym经典控制环境和Atari-10游戏中评估，ES-C51在多数环境中优于QL-C51（修改探索策略的C51）。

Conclusion: 使用Expected Sarsa更新的ES-C51能够减少动作相似期望奖励时的不稳定性，学习到性能更高的策略，在多个环境中表现优于基于Q学习的C51变体。

Abstract: In most value-based reinforcement learning (RL) algorithms, the agent
estimates only the expected reward for each action and selects the action with
the highest reward. In contrast, Distributional Reinforcement Learning (DRL)
estimates the entire probability distribution of possible rewards, providing
richer information about uncertainty and variability. C51 is a popular DRL
algorithm for discrete action spaces. It uses a Q-learning approach, where the
distribution is learned using a greedy Bellman update. However, this can cause
problems if multiple actions at a state have similar expected reward but with
different distributions, as the algorithm may not learn a stable distribution.
This study presents a modified version of C51 (ES-C51) that replaces the greedy
Q-learning update with an Expected Sarsa update, which uses a softmax
calculation to combine information from all possible actions at a state rather
than relying on a single best action. This reduces instability when actions
have similar expected rewards and allows the agent to learn higher-performing
policies. This approach is evaluated on classic control environments from Gym,
and Atari-10 games. For a fair comparison, we modify the standard C51's
exploration strategy from e-greedy to softmax, which we refer to as QL-C51 (Q-
Learning based C51). The results demonstrate that ES-C51 outperforms QL-C51
across many environments.

</details>


### [65] [Hybrid Autoencoder-Based Framework for Early Fault Detection in Wind Turbines](https://arxiv.org/abs/2510.15010)
*Rekha R Nair,Tina Babu,Alavikunhu Panthakkan,Balamurugan Balusamy,Wathiq Mansoor*

Main category: cs.LG

TL;DR: 提出一种基于集成深度学习的无监督异常检测框架，用于风力涡轮机早期故障检测，结合VAE、LSTM自编码器和Transformer架构，在真实SCADA数据上实现高精度异常检测。


<details>
  <summary>Details</summary>
Motivation: 风力涡轮机可靠性对可再生能源行业至关重要，早期故障检测能显著减少停机时间和维护成本。

Method: 集成变分自编码器、LSTM自编码器和Transformer架构，通过特征工程提取时域、统计和频域指标，采用集成评分和自适应阈值进行无监督异常检测。

Result: 在包含89年真实涡轮机数据的CARE数据集上测试，AUC-ROC达到0.947，能够提前48小时检测到故障。

Conclusion: 该方法通过预测性维护减少涡轮机故障，提高大规模风能部署的运营效率，具有重要社会价值。

Abstract: Wind turbine reliability is critical to the growing renewable energy sector,
where early fault detection significantly reduces downtime and maintenance
costs. This paper introduces a novel ensemble-based deep learning framework for
unsupervised anomaly detection in wind turbines. The method integrates
Variational Autoencoders (VAE), LSTM Autoencoders, and Transformer
architectures, each capturing different temporal and contextual patterns from
high-dimensional SCADA data. A unique feature engineering pipeline extracts
temporal, statistical, and frequency-domain indicators, which are then
processed by the deep models. Ensemble scoring combines model predictions,
followed by adaptive thresholding to detect operational anomalies without
requiring labeled fault data. Evaluated on the CARE dataset containing 89 years
of real-world turbine data across three wind farms, the proposed method
achieves an AUC-ROC of 0.947 and early fault detection up to 48 hours prior to
failure. This approach offers significant societal value by enabling predictive
maintenance, reducing turbine failures, and enhancing operational efficiency in
large-scale wind energy deployments.

</details>


### [66] [AlignFlow: Improving Flow-based Generative Models with Semi-Discrete Optimal Transport](https://arxiv.org/abs/2510.15038)
*Lingkai Kong,Molei Tao,Yang Liu,Bryan Wang,Jinmiao Fu,Chien-Chih Wang,Huidong Liu*

Main category: cs.LG

TL;DR: 提出了AlignFlow方法，利用半离散最优传输(SDOT)来改进流式生成模型的训练，通过将噪声空间划分为Laguerre单元并与数据点建立显式最优对齐，解决了现有方法在大规模高维数据集上的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于最优传输的流式生成模型方法使用小批量采样来估计传输计划，这限制了其在大规模和高维数据集上的可扩展性。

Method: AlignFlow利用半离散最优传输(SDOT)，将噪声空间划分为Laguerre单元，每个单元映射到对应的数据点，在训练过程中通过SDOT映射将独立同分布的噪声样本与数据点配对。

Result: 实验结果表明AlignFlow能够显著提升各种最先进流式生成模型算法的性能，且具有良好的可扩展性，计算开销可忽略不计。

Conclusion: AlignFlow作为一种即插即用组件，能够有效改进流式生成模型的训练，特别是在大规模数据集和复杂模型架构下表现出色。

Abstract: Flow-based Generative Models (FGMs) effectively transform noise into complex
data distributions. Incorporating Optimal Transport (OT) to couple noise and
data during FGM training has been shown to improve the straightness of flow
trajectories, enabling more effective inference. However, existing OT-based
methods estimate the OT plan using (mini-)batches of sampled noise and data
points, which limits their scalability to large and high-dimensional datasets
in FGMs. This paper introduces AlignFlow, a novel approach that leverages
Semi-Discrete Optimal Transport (SDOT) to enhance the training of FGMs by
establishing an explicit, optimal alignment between noise distribution and data
points with guaranteed convergence. SDOT computes a transport map by
partitioning the noise space into Laguerre cells, each mapped to a
corresponding data point. During FGM training, i.i.d. noise samples are paired
with data points via the SDOT map. AlignFlow scales well to large datasets and
model architectures with negligible computational overhead. Experimental
results show that AlignFlow improves the performance of a wide range of
state-of-the-art FGM algorithms and can be integrated as a plug-and-play
component. Code is available at: https://github.com/konglk1203/AlignFlow.

</details>


### [67] [IQNN-CS: Interpretable Quantum Neural Network for Credit Scoring](https://arxiv.org/abs/2510.15044)
*Abdul Samad Khan,Nouhaila Innan,Aeysha Khalique,Muhammad Shafique*

Main category: cs.LG

TL;DR: 提出了IQNN-CS，一种用于多类别信用风险分类的可解释量子神经网络框架，结合变分QNN和专门为结构化数据设计的解释技术，通过新的ICAA指标增强量子机器学习的可解释性。


<details>
  <summary>Details</summary>
Motivation: 信用评分是金融服务中的高风险任务，量子机器学习虽然提供新的计算能力，但其黑盒特性在需要透明度和信任的领域面临采用挑战。

Method: 开发了IQNN-CS框架，结合变分量子神经网络和针对结构化数据的后验解释技术，引入了Inter-Class Attribution Alignment (ICAA)指标来量化不同预测类别间的归因差异。

Result: 在两个真实信用数据集上的评估显示，IQNN-CS具有稳定的训练动态、有竞争力的预测性能和增强的可解释性。

Conclusion: 为金融决策提供了实现透明和负责任量子机器学习模型的实用路径。

Abstract: Credit scoring is a high-stakes task in financial services, where model
decisions directly impact individuals' access to credit and are subject to
strict regulatory scrutiny. While Quantum Machine Learning (QML) offers new
computational capabilities, its black-box nature poses challenges for adoption
in domains that demand transparency and trust. In this work, we present
IQNN-CS, an interpretable quantum neural network framework designed for
multiclass credit risk classification. The architecture combines a variational
QNN with a suite of post-hoc explanation techniques tailored for structured
data. To address the lack of structured interpretability in QML, we introduce
Inter-Class Attribution Alignment (ICAA), a novel metric that quantifies
attribution divergence across predicted classes, revealing how the model
distinguishes between credit risk categories. Evaluated on two real-world
credit datasets, IQNN-CS demonstrates stable training dynamics, competitive
predictive performance, and enhanced interpretability. Our results highlight a
practical path toward transparent and accountable QML models for financial
decision-making.

</details>


### [68] [Internalizing World Models via Self-Play Finetuning for Agentic RL](https://arxiv.org/abs/2510.15047)
*Shiqi Chen,Tongyao Zhu,Zian Wang,Jinghan Zhang,Kangrui Wang,Siyang Gao,Teng Xiao,Yee Whye Teh,Junxian He,Manling Li*

Main category: cs.LG

TL;DR: SPA框架通过自监督微调学习世界模型，然后用于策略优化，显著提升LLM智能体在分布外场景中的性能


<details>
  <summary>Details</summary>
Motivation: 大语言模型智能体在分布外场景中表现不佳，难以将内部知识与复杂动态环境对齐，传统强化学习训练效果有限

Method: 将世界模型分解为状态表示和转移建模，通过自监督微调阶段学习世界模型，然后在策略优化前使用该模型模拟未来状态

Result: 在Sokoban、FrozenLake和Sudoku等环境中显著提升性能，如Sokoban成功率从25.6%提升至59.8%，FrozenLake得分从22.1%提升至70.9%

Conclusion: 为LLM智能体配备内部世界模型能更好地将推理与环境动态对齐，改善决策能力，SPA框架有效提升了强化学习训练效果

Abstract: Large Language Models (LLMs) as agents often struggle in out-of-distribution
(OOD) scenarios. Real-world environments are complex and dynamic, governed by
task-specific rules and stochasticity, which makes it difficult for LLMs to
ground their internal knowledge in those dynamics. Under such OOD conditions,
vanilla RL training often fails to scale; we observe Pass@k--the probability
that at least one of (k) sampled trajectories succeeds--drops markedly across
training steps, indicating brittle exploration and limited generalization.
Inspired by model-based reinforcement learning, we hypothesize that equipping
LLM agents with an internal world model can better align reasoning with
environmental dynamics and improve decision-making. We show how to encode this
world model by decomposing it into two components: state representation and
transition modeling. Building on this, we introduce SPA, a simple reinforcement
learning framework that cold-starts the policy via a Self-Play supervised
finetuning (SFT) stage to learn the world model by interacting with the
environment, then uses it to simulate future states prior to policy
optimization. This simple initialization outperforms the online world-modeling
baseline and greatly boosts the RL-based agent training performance.
Experiments across diverse environments like Sokoban, FrozenLake, and Sudoku
show that our approach significantly improves performance. For example, SPA
boosts the Sokoban success rate from 25.6% to 59.8% and raises the FrozenLake
score from 22.1% to 70.9% for the Qwen2.5-1.5B-Instruct model.

</details>


### [69] [Learn to Change the World: Multi-level Reinforcement Learning with Model-Changing Actions](https://arxiv.org/abs/2510.15056)
*Ziqing Lu,Babak Hassibi,Lifeng Lai,Weiyu Xu*

Main category: cs.LG

TL;DR: 论文提出了一种多层级可配置时变马尔可夫决策过程（MCTVMDP），其中智能体不仅能通过传统动作优化策略，还能通过上层模型改变动作主动修改环境动态模型本身。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习假设环境是给定或固定的，但现实中智能体可能具有主动修改环境动态的能力。通过重新配置底层转移过程，智能体有可能获得更高的奖励。

Method: 引入MCTVMDP框架，包含上层MDP（配置模型改变动作）和下层MDP（具有非平稳转移函数）。智能体需要联合优化上层配置策略和下层原始动作策略。

Result: 提出了一个能够处理智能体主动修改环境动态的理论框架，将传统的被动适应扩展到主动环境重构。

Conclusion: MCTVMDP为研究智能体主动修改环境动态的问题提供了理论基础，扩展了传统强化学习的边界，使智能体能够通过配置环境来优化长期奖励。

Abstract: Reinforcement learning usually assumes a given or sometimes even fixed
environment in which an agent seeks an optimal policy to maximize its long-term
discounted reward. In contrast, we consider agents that are not limited to
passive adaptations: they instead have model-changing actions that actively
modify the RL model of world dynamics itself. Reconfiguring the underlying
transition processes can potentially increase the agents' rewards. Motivated by
this setting, we introduce the multi-layer configurable time-varying Markov
decision process (MCTVMDP). In an MCTVMDP, the lower-level MDP has a
non-stationary transition function that is configurable through upper-level
model-changing actions. The agent's objective consists of two parts: Optimize
the configuration policies in the upper-level MDP and optimize the primitive
action policies in the lower-level MDP to jointly improve its expected
long-term reward.

</details>


### [70] [Antislop: A Comprehensive Framework for Identifying and Eliminating Repetitive Patterns in Language Models](https://arxiv.org/abs/2510.15061)
*Samuel Paech,Allen Roush,Judah Goldfeder,Ravid Shwartz-Ziv*

Main category: cs.LG

TL;DR: Antislop框架通过采样器、自动化管道和FTPO微调方法，有效检测并消除LLM输出中的重复短语模式，在保持性能的同时显著减少AI文本特征。


<details>
  <summary>Details</summary>
Motivation: LLM广泛采用导致输出中出现特征性的重复短语模式（称为"slop"），这会降低输出质量并使AI生成文本容易被识别。

Method: 结合三种创新方法：Antislop采样器（使用回溯抑制不需要的字符串）、自动化管道（分析模型特定slop并生成训练数据）、FTPO微调方法（在单个token级别调整logits）。

Result: 成功抑制8000+个模式，而token banning在2000个模式时就无法使用；FTPO实现90%的slop减少，同时在GSM8K、MMLU和创意写作任务中保持或提升性能。

Conclusion: Antislop框架能有效减少LLM输出中的重复模式，显著改善文本质量，而DPO方法虽然能实现较弱的抑制但会导致写作质量和词汇多样性显著下降。

Abstract: Widespread LLM adoption has introduced characteristic repetitive phraseology,
termed ``slop,'' which degrades output quality and makes AI-generated text
immediately recognizable. We present Antislop, a comprehensive framework
providing tools to both detect and eliminate these overused patterns. Our
approach combines three innovations: (1) The Antislop Sampler, which uses
backtracking to suppress unwanted strings at inference time without destroying
vocabulary; (2) An automated pipeline that profiles model-specific slop against
human baselines and generates training data; (3) Final Token Preference
Optimization (FTPO), a novel fine-tuning method that operates on individual
tokens, surgically adjusting logits wherever a banned pattern has appeared in
an inference trace. We demonstrate that some slop patterns appear over
1,000$\times$ more frequently in LLM output than human text. The Antislop
Sampler successfully suppresses 8,000+ patterns while maintaining quality,
whereas token banning becomes unusable at just 2,000. Most importantly, FTPO
achieves 90\% slop reduction while maintaining or improving performance in
cross-domain evals including GSM8K, MMLU, and creative writing tasks. In
contrast, DPO suffers significant degradation in writing quality and lexical
diversity despite achieving weaker suppression. We release all code and results
under MIT license: https://github.com/sam-paech/auto-antislop.

</details>


### [71] [Physics-informed data-driven machine health monitoring for two-photon lithography](https://arxiv.org/abs/2510.15075)
*Sixian Jia,Zhiqiao Dong,Chenhui Shao*

Main category: cs.LG

TL;DR: 本文提出了三种基于物理信息数据驱动预测模型和统计方法的双光子光刻系统健康监测方法，能够准确及时地监控机器健康状况，提高维护效率。


<details>
  <summary>Details</summary>
Motivation: 当前双光子光刻系统的维护主要依赖经验而非基于机器健康状态的监控，导致维护不及时造成机器停机和制造质量差，或不必要的维护导致效率低下和可避免的停机时间。

Method: 通过整合物理信息数据驱动的结构尺寸预测模型与统计方法，提出了三种能够处理不同泛化水平复杂场景的健康监测方法。

Result: 在包含六种工艺参数组合和六种结构尺寸的全面实验数据集上评估，所有测试场景中方法都实现了高精度，表现出优秀的有效性、鲁棒性和泛化能力。

Conclusion: 这些结果代表了向双光子光刻系统基于状态的维护迈出的重要一步。

Abstract: Two-photon lithography (TPL) is a sophisticated additive manufacturing
technology for creating three-dimensional (3D) micro- and nano-structures.
Maintaining the health of TPL systems is critical for ensuring consistent
fabrication quality. Current maintenance practices often rely on experience
rather than informed monitoring of machine health, resulting in either untimely
maintenance that causes machine downtime and poor-quality fabrication, or
unnecessary maintenance that leads to inefficiencies and avoidable downtime. To
address this gap, this paper presents three methods for accurate and timely
monitoring of TPL machine health. Through integrating physics-informed
data-driven predictive models for structure dimensions with statistical
approaches, the proposed methods are able to handle increasingly complex
scenarios featuring different levels of generalizability. A comprehensive
experimental dataset that encompasses six process parameter combinations and
six structure dimensions under two machine health conditions was collected to
evaluate the effectiveness of the proposed approaches. Across all test
scenarios, the approaches are shown to achieve high accuracies, demonstrating
excellent effectiveness, robustness, and generalizability. These results
represent a significant step toward condition-based maintenance for TPL
systems.

</details>


### [72] [Online Correlation Clustering: Simultaneously Optimizing All $\ell_p$-norms](https://arxiv.org/abs/2510.15076)
*Sami Davies,Benjamin Moseley,Heather Newman*

Main category: cs.LG

TL;DR: 该论文提出了首个在线相关聚类算法，能够在AOS模型中同时近似所有ℓ_p-范数目标，填补了离线与在线设置之间的差距。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于离线设置中可以实现同时近似所有ℓ_p-范数的聚类，但标准在线模型存在根本性分离：ℓ₁-范数可轻松近似，而ℓ_∞-范数需要Ω(n^{1/3})竞争比，因此需要超越最坏情况的模型。

Method: 提出了一种在线带样本(AOS)模型的单一算法，使用输入的一小部分作为样本，产生一个同时适用于所有ℓ_p-范数的聚类。

Result: 算法在AOS模型中：对所有ℓ_p-范数高概率O(log⁴n)-竞争；对ℓ_∞-范数高概率O(logn)-竞争；对ℓ₁-范数期望O(1)-竞争。同时证明了标准RO模型中ℓ_∞-范数需要Ω(n^{1/3})竞争比。

Conclusion: 成功将离线"全范数"保证迁移到在线世界，证明了AOS模型的有效性，并通过下界分析表明竞争比在AOS模型中接近最优。

Abstract: The $\ell_p$-norm objectives for correlation clustering present a fundamental
trade-off between minimizing total disagreements (the $\ell_1$-norm) and
ensuring fairness to individual nodes (the $\ell_\infty$-norm). Surprisingly,
in the offline setting it is possible to simultaneously approximate all
$\ell_p$-norms with a single clustering. Can this powerful guarantee be
achieved in an online setting? This paper provides the first affirmative
answer. We present a single algorithm for the online-with-a-sample (AOS) model
that, given a small constant fraction of the input as a sample, produces one
clustering that is simultaneously $O(\log^4 n)$-competitive for all
$\ell_p$-norms with high probability, $O(\log n)$-competitive for the
$\ell_\infty$-norm with high probability, and $O(1)$-competitive for the
$\ell_1$-norm in expectation. This work successfully translates the offline
"all-norms" guarantee to the online world.
  Our setting is motivated by a new hardness result that demonstrates a
fundamental separation between these objectives in the standard random-order
(RO) online model. Namely, while the $\ell_1$-norm is trivially
$O(1)$-approximable in the RO model, we prove that any algorithm in the RO
model for the fairness-promoting $\ell_\infty$-norm must have a competitive
ratio of at least $\Omega(n^{1/3})$. This highlights the necessity of a
different beyond-worst-case model. We complement our algorithm with lower
bounds, showing our competitive ratios for the $\ell_1$- and $\ell_\infty$-
norms are nearly tight in the AOS model.

</details>


### [73] [Operator Flow Matching for Timeseries Forecasting](https://arxiv.org/abs/2510.15101)
*Yolanne Yi Ran Lee,Kyriakos Flouris*

Main category: cs.LG

TL;DR: 提出了TempO模型，一种基于流匹配的生成模型，用于高维PDE控制动力学的预测，通过稀疏条件化和通道折叠处理3D时空场，在多个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自回归和基于扩散的方法在预测高维PDE控制动力学时存在累积误差和离散化伪影问题，限制了长期物理一致性预测。流匹配提供了一种有效的确定性采样替代方案。

Method: 提出TempO模型，利用稀疏条件化和通道折叠高效处理3D时空场，使用时序条件傅里叶层捕捉多尺度模式，并证明了FNO近似误差的上界。

Result: TempO在三个基准PDE数据集上优于最先进的基线方法，谱分析显示其能更好地恢复多尺度动力学，效率研究表明其参数和内存需求低于基于注意力或卷积的回归器。

Conclusion: TempO通过流匹配和创新的架构设计，为高维PDE动力学预测提供了高效、准确的解决方案，在保持物理一致性的同时实现了长期预测能力。

Abstract: Forecasting high-dimensional, PDE-governed dynamics remains a core challenge
for generative modeling. Existing autoregressive and diffusion-based approaches
often suffer cumulative errors and discretisation artifacts that limit long,
physically consistent forecasts. Flow matching offers a natural alternative,
enabling efficient, deterministic sampling. We prove an upper bound on FNO
approximation error and propose TempO, a latent flow matching model leveraging
sparse conditioning with channel folding to efficiently process 3D
spatiotemporal fields using time-conditioned Fourier layers to capture
multi-scale modes with high fidelity. TempO outperforms state-of-the-art
baselines across three benchmark PDE datasets, and spectral analysis further
demonstrates superior recovery of multi-scale dynamics, while efficiency
studies highlight its parameter- and memory-light design compared to
attention-based or convolutional regressors.

</details>


### [74] [DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning](https://arxiv.org/abs/2510.15110)
*Shih-Yang Liu,Xin Dong,Ximing Lu,Shizhe Diao,Mingjie Liu,Min-Hung Chen,Hongxu Yin,Yu-Chiang Frank Wang,Kwang-Ting Cheng,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.LG

TL;DR: 本文提出DLER方法，通过改进RL优化解决推理语言模型输出过长问题，在保持准确率的同时大幅缩短输出长度。


<details>
  <summary>Details</summary>
Motivation: 现有推理语言模型虽然性能强大，但生成了不必要的冗长输出。最大化每token的智能度（准确率相对于响应长度）仍是一个开放问题。

Method: 提出DLER训练方法，结合批处理奖励归一化、更高裁剪、动态采样和简单的截断长度惩罚，解决RL优化中的三个关键挑战：优势估计偏差大、熵崩溃和稀疏奖励信号。

Result: DLER将输出长度减少70%以上，同时超越所有先前基线准确率。DLER-7B相比DeepSeek-R1-7B，并行生成多个简洁响应时准确率提高28%且延迟更低。

Conclusion: DLER实现了最先进的准确率-效率权衡，并提出了难度感知DLER和更新选择性合并方法，在保持基准准确率的同时保留DLER模型的简洁推理能力。

Abstract: Reasoning language models such as OpenAI-o1, DeepSeek-R1, and Qwen achieve
strong performance via extended chains of thought but often generate
unnecessarily long outputs. Maximizing intelligence per token--accuracy
relative to response length--remains an open problem. We revisit reinforcement
learning (RL) with the simplest length penalty--truncation--and show that
accuracy degradation arises not from the lack of sophisticated penalties but
from inadequate RL optimization. We identify three key challenges: (i) large
bias in advantage estimation, (ii) entropy collapse, and (iii) sparse reward
signal. We address them with Doing Length pEnalty Right (DLER), a training
recipe combining batch-wise reward normalization, higher clipping, dynamic
sampling, and a simple truncation length penalty. DLER achieves
state-of-the-art accuracy--efficiency trade-offs, cutting output length by over
70 percent while surpassing all previous baseline accuracy. It also improves
test-time scaling: compared to DeepSeek-R1-7B, DLER-7B generates multiple
concise responses in parallel with 28 percent higher accuracy and lower
latency. We further introduce Difficulty-Aware DLER, which adaptively tightens
truncation on easier questions for additional efficiency gains. We also propose
an update-selective merging method that preserves baseline accuracy while
retaining the concise reasoning ability of the DLER model, which is useful for
scenarios where RL training data is scarce.

</details>


### [75] [Navigating the consequences of mechanical ventilation in clinical intensive care settings through an evolutionary game-theoretic framework](https://arxiv.org/abs/2510.15127)
*David J. Albers,Tell D. Bennett,Jana de Wiljes,Bradford J. Smith,Peter D. Sottile,J. N. Stroh*

Main category: cs.LG

TL;DR: 开发了一个基于进化博弈论的框架来分析机械通气患者-呼吸机-护理系统的复杂动态，为优化个性化机械通气治疗提供定量分析基础


<details>
  <summary>Details</summary>
Motivation: 需要从异质性患者-呼吸机系统的临床数据中理解机械通气策略对患者预后的影响，改进重症监护呼吸管理

Method: 采用进化博弈论分析呼吸行为，生成定量分析前体，结合概率和随机方法如强化学习进行深度分析

Result: 在合成数据上验证了EGT方法的有效性，揭示了数据生成过程中的复杂性，为构建状态转移模型奠定了基础

Conclusion: 该框架是迈向机械通气优化和个性化治疗的重要一步，展示了基于实证和博弈论元素模拟MV决策效果的潜力

Abstract: Identifying the effects of mechanical ventilation strategies and protocols in
critical care requires analyzing data from heterogeneous patient-ventilator
systems within the context of the clinical decision-making environment. This
research develops a framework to help understand the consequences of mechanical
ventilation (MV) and adjunct care decisions on patient outcome from
observations of critical care patients receiving MV. Developing an
understanding of and improving critical care respiratory management requires
the analysis of existing secondary-use clinical data to generate hypotheses
about advantageous variations and adaptations of current care. This work
introduces a perspective of the joint patient-ventilator-care systems
(so-called J6) to develop a scalable method for analyzing data and trajectories
of these complex systems. To that end, breath behaviors are analyzed using
evolutionary game theory (EGT), which generates the necessary quantitative
precursors for deeper analysis through probabilistic and stochastic machinery
such as reinforcement learning. This result is one step along the pathway
toward MV optimization and personalization. The EGT-based process is
analytically validated on synthetic data to reveal potential caveats before
proceeding to real-world ICU data applications that expose complexities of the
data-generating process J6. The discussion includes potential developments
toward a state transition model for the simulating effects of MV decision using
empirical and game-theoretic elements.

</details>


### [76] [A Simple Method for PMF Estimation on Large Supports](https://arxiv.org/abs/2510.15132)
*Alex Shtoff*

Main category: cs.LG

TL;DR: 提出一种基于图拉普拉斯算子的非参数概率质量函数估计方法，通过数据依赖的低通滤波处理多模态和重尾分布，计算高效且无需过多调参。


<details>
  <summary>Details</summary>
Motivation: 针对大离散支撑集上多模态、重尾概率质量函数的非参数估计问题，传统方法在处理噪声和保持粗粒度结构方面存在挑战。

Method: 将经验PMF视为线图上的信号，构建对称三对角算子（路径图拉普拉斯算子扰动），计算最小特征值对应的特征向量，将经验PMF投影到低维子空间进行平滑估计。

Result: 在合成和真实重尾数据上，该方法能有效保持粗粒度结构并抑制采样噪声，在目标场景下优于logspline和高斯KDE基线方法。

Conclusion: 该方法实现简洁、计算可靠、内存效率高，适用于自动化流水线和规模化探索性分析，但存在已知的失败模式（如突然的不连续性）。

Abstract: We study nonparametric estimation of a probability mass function (PMF) on a
large discrete support, where the PMF is multi-modal and heavy-tailed. The core
idea is to treat the empirical PMF as a signal on a line graph and apply a
data-dependent low-pass filter. Concretely, we form a symmetric tri-diagonal
operator, the path graph Laplacian perturbed with a diagonal matrix built from
the empirical PMF, then compute the eigenvectors, corresponding to the smallest
feq eigenvalues. Projecting the empirical PMF onto this low dimensional
subspace produces a smooth, multi-modal estimate that preserves coarse
structure while suppressing noise. A light post-processing step of clipping and
re-normalizing yields a valid PMF.
  Because we compute the eigenpairs of a symmetric tridiagonal matrix, the
computation is reliable and runs time and memory proportional to the support
times the dimension of the desired low-dimensional supspace. We also provide a
practical, data-driven rule for selecting the dimension based on an
orthogonal-series risk estimate, so the method "just works" with minimal
tuning. On synthetic and real heavy-tailed examples, the approach preserves
coarse structure while suppressing sampling noise, compares favorably to
logspline and Gaussian-KDE baselines in the intended regimes. However, it has
known failure modes (e.g., abrupt discontinuities). The method is short to
implement, robust across sample sizes, and suitable for automated pipelines and
exploratory analysis at scale because of its reliability and speed.

</details>


### [77] [Predicting the Unpredictable: Reproducible BiLSTM Forecasting of Incident Counts in the Global Terrorism Database (GTD)](https://arxiv.org/abs/2510.15136)
*Oluwasegun Adegoke*

Main category: cs.LG

TL;DR: 使用双向LSTM模型进行恐怖主义事件周计数短期预测，在GTD数据集上优于传统方法和LSTM-Attention基线，RMSE达到6.38，提升超过30%。


<details>
  <summary>Details</summary>
Motivation: 研究恐怖主义事件短期预测，建立可复现的预测流程，评估双向LSTM在恐怖主义事件计数预测中的表现。

Method: 构建可复现的预测流程，使用固定时间分割，比较双向LSTM与季节性朴素、线性/ARIMA模型以及LSTM-Attention基线模型。

Result: 双向LSTM在测试集上RMSE为6.38，优于LSTM-Attention的9.19（提升30.6%）和线性滞后回归基线（RMSE提升35.4%），在MAE和MAPE指标上也有并行改进。

Conclusion: 双向LSTM在恐怖主义事件短期预测中表现优异，长期历史数据训练、适度回看窗口（20-30周）和双向编码对捕捉事件模式至关重要，为GTD事件预测提供了透明且超越基线的参考。

Abstract: We study short-horizon forecasting of weekly terrorism incident counts using
the Global Terrorism Database (GTD, 1970--2016). We build a reproducible
pipeline with fixed time-based splits and evaluate a Bidirectional LSTM
(BiLSTM) against strong classical anchors (seasonal-naive, linear/ARIMA) and a
deep LSTM-Attention baseline. On the held-out test set, the BiLSTM attains RMSE
6.38, outperforming LSTM-Attention (9.19; +30.6\%) and a linear lag-regression
baseline (+35.4\% RMSE gain), with parallel improvements in MAE and MAPE.
Ablations varying temporal memory, training-history length, spatial grain,
lookback size, and feature groups show that models trained on long historical
data generalize best; a moderate lookback (20--30 weeks) provides strong
context; and bidirectional encoding is critical for capturing both build-up and
aftermath patterns within the window. Feature-group analysis indicates that
short-horizon structure (lagged counts and rolling statistics) contributes
most, with geographic and casualty features adding incremental lift. We release
code, configs, and compact result tables, and provide a data/ethics statement
documenting GTD licensing and research-only use. Overall, the study offers a
transparent, baseline-beating reference for GTD incident forecasting.

</details>


### [78] [Policy Transfer Ensures Fast Learning for Continuous-Time LQR with Entropy Regularization](https://arxiv.org/abs/2510.15165)
*Xin Guo,Zijiu Lyu*

Main category: cs.LG

TL;DR: 该论文首次为连续时间强化学习提供了策略迁移的理论证明，证明了在一个LQR任务中最优的策略可以作为相关LQR任务的近最优初始化，同时保持原始算法的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 强化学习在复杂任务上从头训练效率低下，而迁移学习在大语言模型中已证明有效。本文旨在探索如何通过预训练模型提升强化学习效率，填补连续时间强化学习中策略迁移理论研究的空白。

Method: 研究在熵正则化的连续时间线性二次调节器(LQRs)中的策略迁移方法，提出了一种新的连续时间LQR策略学习算法，该算法实现了全局线性和局部超线性收敛。

Result: 证明了策略迁移在连续时间RL中的有效性，一个LQR任务的最优策略可以作为相关LQR任务的近最优初始化，同时保持收敛速率。算法实现了全局线性和局部超线性收敛。

Conclusion: 研究为连续时间强化学习中的策略迁移提供了理论保证和算法优势，填补了现有文献的空白，并将先前工作从离散时间扩展到连续时间设置。

Abstract: Reinforcement Learning (RL) enables agents to learn optimal decision-making
strategies through interaction with an environment, yet training from scratch
on complex tasks can be highly inefficient. Transfer learning (TL), widely
successful in large language models (LLMs), offers a promising direction for
enhancing RL efficiency by leveraging pre-trained models.
  This paper investigates policy transfer, a TL approach that initializes
learning in a target RL task using a policy from a related source task, in the
context of continuous-time linear quadratic regulators (LQRs) with entropy
regularization. We provide the first theoretical proof of policy transfer for
continuous-time RL, proving that a policy optimal for one LQR serves as a
near-optimal initialization for closely related LQRs, while preserving the
original algorithm's convergence rate. Furthermore, we introduce a novel policy
learning algorithm for continuous-time LQRs that achieves global linear and
local super-linear convergence. Our results demonstrate both theoretical
guarantees and algorithmic benefits of transfer learning in continuous-time RL,
addressing a gap in existing literature and extending prior work from discrete
to continuous time settings.
  As a byproduct of our analysis, we derive the stability of a class of
continuous-time score-based diffusion models via their connection with LQRs.

</details>


### [79] [A simple mean field model of feature learning](https://arxiv.org/abs/2510.15174)
*Niclas Göring,Chris Mingard,Yoonsoo Nam,Ard Louis*

Main category: cs.LG

TL;DR: 该论文通过统计物理学方法推导了一个可处理的平均场理论，用于描述使用随机梯度朗之万动力学训练的两层非线性网络的贝叶斯后验分布。该理论揭示了有限宽度下的对称性破缺相变，并识别了自增强输入特征选择这一关键机制。


<details>
  <summary>Details</summary>
Motivation: 特征学习（神经网络在训练过程中调整内部表示）仍然缺乏深入理解。论文旨在通过统计物理学方法建立理论框架来理解特征学习的机制。

Method: 使用统计物理学方法推导自洽的平均场理论，描述两层非线性网络在随机梯度朗之万动力学训练下的贝叶斯后验分布。在基本平均场理论基础上，进一步引入了自增强输入特征选择机制。

Result: 在无限宽度下，理论简化为核岭回归；在有限宽度下，预测了网络与目标函数突然对齐的对称性破缺相变。基本平均场理论能够半定量预测特征学习的出现，但低估了相变后泛化能力的提升。加入自增强输入特征选择机制后，理论能够定量匹配SGLD训练网络的学习曲线。

Conclusion: 该研究提供了一个理解特征学习机制的理论框架，识别了自增强输入特征选择作为关键机制，能够定量解释神经网络训练过程中的学习行为。

Abstract: Feature learning (FL), where neural networks adapt their internal
representations during training, remains poorly understood. Using methods from
statistical physics, we derive a tractable, self-consistent mean-field (MF)
theory for the Bayesian posterior of two-layer non-linear networks trained with
stochastic gradient Langevin dynamics (SGLD). At infinite width, this theory
reduces to kernel ridge regression, but at finite width it predicts a symmetry
breaking phase transition where networks abruptly align with target functions.
While the basic MF theory provides theoretical insight into the emergence of FL
in the finite-width regime, semi-quantitatively predicting the onset of FL with
noise or sample size, it substantially underestimates the improvements in
generalisation after the transition. We trace this discrepancy to a key
mechanism absent from the plain MF description: \textit{self-reinforcing input
feature selection}. Incorporating this mechanism into the MF theory allows us
to quantitatively match the learning curves of SGLD-trained networks and
provides mechanistic insight into FL.

</details>


### [80] [Finding geodesics with the Deep Ritz method](https://arxiv.org/abs/2510.15177)
*Conor Rowan*

Main category: cs.LG

TL;DR: 该论文提出测地线问题是深度Ritz方法的理想应用场景，并通过三个数值实验验证了这一观点。


<details>
  <summary>Details</summary>
Motivation: 测地线问题在物理和工程中普遍存在，但科学机器学习社区对此关注较少。作者认为测地线问题具有简单几何结构、变分结构和自然非线性，特别适合深度Ritz方法。

Method: 使用深度Ritz方法解决测地线问题，通过三个数值例子验证：路径规划、光学和固体力学。

Result: 深度Ritz方法在测地线问题上表现良好，验证了该方法在此类问题上的适用性。

Conclusion: 测地线问题是深度Ritz方法的一个有前景的应用方向，为未来科学机器学习研究提供了富有成果的研究方向。

Abstract: Geodesic problems involve computing trajectories between prescribed initial
and final states to minimize a user-defined measure of distance, cost, or
energy. They arise throughout physics and engineering -- for instance, in
determining optimal paths through complex environments, modeling light
propagation in refractive media, and the study of spacetime trajectories in
control theory and general relativity. Despite their ubiquity, the scientific
machine learning (SciML) community has given relatively little attention to
investigating its methods in the context of these problems. In this work, we
argue that given their simple geometry, variational structure, and natural
nonlinearity, geodesic problems are particularly well-suited for the Deep Ritz
method. We substantiate this claim with three numerical examples drawn from
path planning, optics, and solid mechanics. Our goal is not to provide an
exhaustive study of geodesic problems, but rather to identify a promising
application of the Deep Ritz method and a fruitful direction for future SciML
research.

</details>


### [81] [An Advanced Two-Stage Model with High Sensitivity and Generalizability for Prediction of Hip Fracture Risk Using Multiple Datasets](https://arxiv.org/abs/2510.15179)
*Shuo Sun,Meiling Zhou,Chen Zhao,Joyce H. Keyak,Nancy E. Lane,Jeffrey D. Deng,Kuan-Jui Su,Hui Shen,Hong-Wen Deng,Kui Zhang,Weihua Zhou*

Main category: cs.LG

TL;DR: 提出一个两阶段模型，整合临床和影像信息来提高髋部骨折风险预测的准确性，相比传统方法具有更高敏感性和更少的漏诊。


<details>
  <summary>Details</summary>
Motivation: 传统工具如DXA T-score和FRAX在髋部骨折风险预测中缺乏敏感性，特别是对于无既往骨折或骨量减少的高风险人群。

Method: 采用顺序两阶段模型：第一阶段使用临床、人口统计学和功能变量进行筛查，第二阶段整合DXA衍生特征进行精炼。基于MrOS、SOF和UK Biobank数据进行验证。

Result: 经过内外验证，模型在不同队列中表现一致且适应性强。相比T-score和FRAX，该框架实现了更高敏感性和更少的漏诊病例。

Conclusion: 两阶段框架为早期髋部骨折风险评估提供了一种成本效益高且个性化的方法。

Abstract: Hip fractures are a major cause of disability, mortality, and healthcare
burden in older adults, underscoring the need for early risk assessment.
However, commonly used tools such as the DXA T-score and FRAX often lack
sensitivity and miss individuals at high risk, particularly those without prior
fractures or with osteopenia. To address this limitation, we propose a
sequential two-stage model that integrates clinical and imaging information to
improve prediction accuracy. Using data from the Osteoporotic Fractures in Men
Study (MrOS), the Study of Osteoporotic Fractures (SOF), and the UK Biobank,
Stage 1 (Screening) employs clinical, demographic, and functional variables to
estimate baseline risk, while Stage 2 (Imaging) incorporates DXA-derived
features for refinement. The model was rigorously validated through internal
and external testing, showing consistent performance and adaptability across
cohorts. Compared to T-score and FRAX, the two-stage framework achieved higher
sensitivity and reduced missed cases, offering a cost-effective and
personalized approach for early hip fracture risk assessment.
  Keywords: Hip Fracture, Two-Stage Model, Risk Prediction, Sensitivity, DXA,
FRAX

</details>


### [82] [Dissecting Mahalanobis: How Feature Geometry and Normalization Shape OOD Detection](https://arxiv.org/abs/2510.15202)
*Denis Janiak,Jakub Binkowski,Tomasz Kajdanowicz*

Main category: cs.LG

TL;DR: 该论文通过实证研究发现Mahalanobis距离方法在OOD检测中并非普遍可靠，定义了理想的数据表示几何结构，并提出径向缩放ℓ2归一化方法，通过调节特征空间的径向几何显著提升OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: Mahalanobis距离方法在OOD检测中广泛应用，但其性能受表示几何和归一化的影响尚未被充分理解，这限制了其下游应用。

Method: 在多种图像基础模型、数据集和距离归一化方案上进行综合实证研究，分析表示几何与OOD性能的关系，并提出径向缩放ℓ2归一化方法。

Result: 研究发现谱和内在维度指标能准确预测模型的OOD性能，提出的径向缩放归一化方法通过控制特征空间的径向几何，显著改善了OOD检测性能。

Conclusion: 通过连接表示几何、归一化和OOD性能之间的关联，为设计更有效可靠的深度学习模型提供了新见解。

Abstract: Out-of-distribution (OOD) detection is critical for the reliable deployment
of deep learning models. hile Mahalanobis distance methods are widely used, the
impact of representation geometry and normalization on their performance is not
fully understood, which may limit their downstream application. To address this
gap, we conducted a comprehensive empirical study across diverse image
foundation models, datasets, and distance normalization schemes. First, our
analysis shows that Mahalanobis-based methods aren't universally reliable.
Second, we define the ideal geometry for data representations and demonstrate
that spectral and intrinsic-dimensionality metrics can accurately predict a
model's OOD performance. Finally, we analyze how normalization impacts OOD
performance. Building upon these studies, we propose radially scaled $\ell_2$
normalization, a method that generalizes the standard $\ell_2$ normalization
recently applied to Mahalanobis-based OOD detection. Our approach introduces a
tunable parameter to directly control the radial geometry of the feature space,
systematically contracting or expanding representations to significantly
improve OOD detection performance. By bridging the gap between representation
geometry, normalization, and OOD performance, our findings offer new insights
into the design of more effective and reliable deep learning models.

</details>


### [83] [ReasonIF: Large Reasoning Models Fail to Follow Instructions During Reasoning](https://arxiv.org/abs/2510.15211)
*Yongchan Kwon,Shang Zhu,Federico Bianchi,Kaitlyn Zhou,James Zou*

Main category: cs.LG

TL;DR: ReasonIF基准测试评估大型推理模型在推理过程中遵循用户指令的能力，发现现有模型存在严重不足，最高指令遵循分数低于0.25。提出推理指令微调方法可提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要评估模型主要响应的指令遵循能力，但推理过程中的指令遵循对模型可控性、透明性和安全性同样关键，可减少不良捷径、幻觉或奖励攻击风险。

Method: 引入ReasonIF基准测试，包含多语言推理、格式和长度控制等六类指令提示。评估多个开源大型推理模型，并探索多轮推理和推理指令微调两种增强策略。

Result: 现有模型在推理指令遵循方面表现严重不足，最高指令遵循分数仅为0.25。任务难度增加时性能进一步下降。推理指令微调可将GPT-OSS-20B的指令遵循分数从0.11提升至0.27。

Conclusion: 大型推理模型在推理过程中遵循用户指令的能力存在显著缺陷，推理指令微调方法可带来可衡量的改进，但仍有很大提升空间。

Abstract: The ability of large language models (LLMs) to follow user instructions is
central to their reliability, safety, and usefulness. While prior studies
assess instruction adherence in the model's main responses, we argue that it is
also critical for large reasoning models (LRMs) to follow user instructions
throughout their reasoning process. Reasoning instruction following makes LRMs
more controllable and transparent, while reducing risks of undesirable
shortcuts, hallucinations, or reward hacking within reasoning traces. To
evaluate this dimension, we introduce ReasonIF, a systematic benchmark for
assessing reasoning instruction following. ReasonIF includes six categories of
instruction prompts, spanning multilingual reasoning, formatting and length
control. Across many open-source LRMs including GPT-OSS, Qwen3, and
DeepSeek-R1, we find substantial failures in reasoning instruction adherence:
the highest instruction following score (IFS) remains below 0.25, meaning that
fewer than $25\%$ of reasoning traces comply with the given instructions.
Notably, as task difficulty increases, reasoning instruction following degrades
further. We also explore two strategies to enhance reasoning instruction
fidelity. (1) multi-turn reasoning and (2) Reasoning Instruction Finetuning
(RIF) using synthetic data. RIF improves the IFS of $GPT-OSS-20B$ from 0.11 to
0.27, indicating measurable progress but leaving ample room for improvement.

</details>


### [84] [Soundness-Aware Level: A Microscopic Signature that Predicts LLM Reasoning Potential](https://arxiv.org/abs/2510.15216)
*Xuansheng Wu,Xiaoman Pan,Wenlin Yao,Jianshu Chen*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reinforcement learning with verifiable rewards (RLVR) can elicit strong
reasoning in large language models (LLMs), while their performance after RLVR
varies dramatically across different base models. This raises a fundamental
question: what microscopic property of pre-trained models leads to this
variation? To investigate, we formalize reasoning as chains of Horn clauses
("if-then" rules) built from features extracted from the LLM's latent space via
cross-layer sparse autoencoders (SAEs). We estimate the transition
probabilities between its features, and further categorize each rule by its
semantic soundness level (e.g., strict, plausible, noisy) with an LLM. Our key
discovery is that high-potential models are inherently soundness-aware: their
internal probability distributions systematically shift across rules' soundness
levels, becoming highly distinct for "strict" versus "noisy" rules. In
contrast, weaker models are soundness-agnostic, collapsing to one distribution
regardless of soundness levels. To quantify this, we introduce the
Soundness-Aware Level (SAL), a microscopic metric using the Jensen-Shannon
Divergence to measure the separation between these distributions. We show that
SAL's predictions of post-RLVR reasoning performance follow a precise empirical
law (R^2=0.87) across diverse model families (Qwen, Mistral, Llama, DeepSeek)
and scales (0.5B-14B). This reveals that a model's reasoning potential is tied
to its intrinsic, pre-trained ability to distinguish sound knowledge from
unsound ones. These findings underscore the critical role of model pre-training
in shaping reasoning and offer a practical metric grounded in the model's
internal mechanisms for selecting/designing stronger base models.

</details>


### [85] [Reflections from Research Roundtables at the Conference on Health, Inference, and Learning (CHIL) 2025](https://arxiv.org/abs/2510.15217)
*Emily Alsentzer,Marie-Laure Charpignon,Bill Chen,Niharika D'Souza,Jason Fries,Yixing Jiang,Aparajita Kashyap,Chanwoo Kim,Simon Lee,Aishwarya Mandyam,Ashery Christopher Mbilinyi,Nikita Mehandru,Nitish Nagesh,Brighton Nuwagira,Emma Pierson,Arvind Pillai,Akane Sano,Tanveer Syeda-Mahmood,Shashank Yadav,Elias Adhanom,Muhammad Umar Afza,Amelia Archer,Suhana Bedi,Vasiliki Bikia,Trenton Chang,George H. Chen,Winston Chen,Erica Chiang,Edward Choi,Octavia Ciora,Paz Dozie-Nnamah,Shaza Elsharief,Matthew Engelhard,Ali Eshragh,Jean Feng,Josh Fessel,Scott Fleming,Kei Sen Fong,Thomas Frost,Soham Gadgil,Judy Gichoya,Leeor Hershkovich,Sujeong Im,Bhavya Jain,Vincent Jeanselme,Furong Jia,Qixuan,Jin,Yuxuan Jin,Daniel Kapash,Geetika Kapoor,Behdokht Kiafar,Matthias Kleiner,Stefan Kraft,Annika Kumar,Daeun Kyung,Zhongyuan Liang,Joanna Lin,Qianchu,Liu,Chang Liu,Hongzhou Luan,Chris Lunt,Leopoldo Julían Lechuga López,Matthew B. A. McDermott,Shahriar Noroozizadeh,Connor O'Brien,YongKyung Oh,Mixail Ota,Stephen Pfohl,Meagan Pi,Tanmoy Sarkar Pias,Emma Rocheteau,Avishaan Sethi,Toru Shirakawa,Anita Silver,Neha Simha,Kamile Stankeviciute,Max Sunog,Peter Szolovits,Shengpu Tang,Jialu Tang,Aaron Tierney,John Valdovinos,Byron Wallace,Will Ke Wang,Peter Washington,Jeremy Weiss,Daniel Wolfe,Emily Wong,Hye Sun Yun,Xiaoman Zhang,Xiao Yu Cindy Zhang,Hayoung Jeong,Kaveri A. Thakoor*

Main category: cs.LG

TL;DR: CHIL 2025会议举办了8个研究圆桌会议，聚焦机器学习与医疗交叉领域的关键话题，包括可解释性、公平性、因果推断等主题。


<details>
  <summary>Details</summary>
Motivation: 促进机器学习与医疗领域专家之间的协作对话，探讨该交叉领域面临的紧迫挑战和新兴机遇。

Method: 通过小型圆桌会议形式，由资深和初级主席共同主持，鼓励开放式交流和集体构思。

Result: 成功举办了8个圆桌会议，涉及19位主席，覆盖了从可解释性到可扩展医疗解决方案等多个关键主题。

Conclusion: 研究圆桌会议为医疗机器学习社区提供了重要的协作平台，推动了该领域的知识交流和行动方向制定。

Abstract: The 6th Annual Conference on Health, Inference, and Learning (CHIL 2025),
hosted by the Association for Health Learning and Inference (AHLI), was held in
person on June 25-27, 2025, at the University of California, Berkeley, in
Berkeley, California, USA. As part of this year's program, we hosted Research
Roundtables to catalyze collaborative, small-group dialogue around critical,
timely topics at the intersection of machine learning and healthcare. Each
roundtable was moderated by a team of senior and junior chairs who fostered
open exchange, intellectual curiosity, and inclusive engagement. The sessions
emphasized rigorous discussion of key challenges, exploration of emerging
opportunities, and collective ideation toward actionable directions in the
field. In total, eight roundtables were held by 19 roundtable chairs on topics
of "Explainability, Interpretability, and Transparency," "Uncertainty, Bias,
and Fairness," "Causality," "Domain Adaptation," "Foundation Models," "Learning
from Small Medical Data," "Multimodal Methods," and "Scalable, Translational
Healthcare Solutions."

</details>


### [86] [Machine Learning for Early Detection of Meningitis: Stacked Ensemble Learning with EHR data](https://arxiv.org/abs/2510.15218)
*Han Ouyang,Jesse Hamilton,Saeed Amal*

Main category: cs.LG

TL;DR: 使用MIMIC-III数据库中的214名脑膜炎患者和46,303名非脑膜炎患者数据，通过集成学习方法构建脑膜炎诊断模型，在模拟急诊室场景下取得了优异的性能表现。


<details>
  <summary>Details</summary>
Motivation: 开发能够在真实急诊室场景下辅助诊断脑膜炎的AI工具，提高临床应用的实用性。

Method: 采用两阶段特征选择方法筛选临床相关特征，使用随机森林、LightGBM和深度神经网络作为基础模型，通过集成学习将基础模型输出作为元模型（逻辑回归）的输入进行训练。

Result: 集成学习模型在两个测试集上均表现出色，测试集1的AUC为0.9637，测试集2的AUC为0.9472。

Conclusion: 虽然直接将诊断工具部署给临床医生使用仍具挑战性，但本研究为未来使用集成学习的AI驱动脑膜炎诊断方法奠定了基础。

Abstract: We utilized a cohort of 214 meningitis patients and 46,303 non-meningitis
patients from the MIMIC-III database. After extensive data preprocessing, which
included ICD-based cohort selection, one-hot encoding of coding, and a
two-stage feature selection process (for both the training set and the testing
sets), clinically relevant features such as gender and high-risk ICD codes
(including subarachnoid hemorrhage, secondary malignant neoplasm of the brain,
and generalized epilepsy) are selected. Overall, these clinically reasonable
and temporally adherent features provided excellent modeling performance. Three
models (Random Forest, LightGBM, and Deep Neural Networks (DNN) are trained as
base models for Ensemble Learning. Base model outputs are aggregated and
stacked into a meta model (Logistic Regression) that uses the base model
outputs as input values in training. Ultimately, soldier outputs (AUC of
Testing Set 1: 0.9637, AUC of Testing Set 2: 0.9472) are obtained through
ensemble learning.
  We created a challenging condition for diagnosing meningitis, simulating a
real-world ER (Emergency Room) scenario to enhance clinical use in real-world
applications. While directly deploying a diagnostic tool that clinicians can
use is challenging, this paper paves the way for a potential future AI-driven
diagnostic approach for meningitis using Ensemble Learning.

</details>


### [87] [Integrating Product Coefficients for Improved 3D LiDAR Data Classification (Part II)](https://arxiv.org/abs/2510.15219)
*Patricia Medina,Rasika Karkare*

Main category: cs.LG

TL;DR: 该研究通过将乘积系数与自编码器表示和KNN分类器结合，在3D LiDAR点云分类中实现了比PCA基线和先前框架更一致的性能提升。


<details>
  <summary>Details</summary>
Motivation: 扩展先前关于使用乘积系数增强3D LiDAR点云分类的研究，探索如何通过结合自编码器表示进一步提升分类性能。

Method: 将乘积系数与自编码器表示相结合，并使用KNN分类器；逐级添加乘积系数以分析其对分类性能的影响。

Result: 更丰富的乘积系数集合系统性地提高了类别可分性和整体准确率；该方法在LiDAR分类性能上优于PCA基线和先前框架。

Conclusion: 结合分层乘积系数特征与自编码器能够显著提升LiDAR分类性能，乘积系数的丰富程度与分类效果呈正相关。

Abstract: This work extends our previous study on enhancing 3D LiDAR point-cloud
classification with product coefficients
\cite{medina2025integratingproductcoefficientsimproved}, measure-theoretic
descriptors that complement the original spatial Lidar features. Here, we show
that combining product coefficients with an autoencoder representation and a
KNN classifier delivers consistent performance gains over both PCA-based
baselines and our earlier framework. We also investigate the effect of adding
product coefficients level by level, revealing a clear trend: richer sets of
coefficients systematically improve class separability and overall accuracy.
The results highlight the value of combining hierarchical product-coefficient
features with autoencoders to push LiDAR classification performance further.

</details>


### [88] [Stress-Aware Learning under KL Drift via Trust-Decayed Mirror Descent](https://arxiv.org/abs/2510.15222)
*Gabriel Nixon Raj*

Main category: cs.LG

TL;DR: 本文提出了一种在分布漂移下的序列决策方法——熵正则化信任衰减，通过应力感知指数倾斜来更新信念和决策，实现了在KL漂移路径长度下的动态遗憾保证。


<details>
  <summary>Details</summary>
Motivation: 研究在分布漂移环境下的序列决策问题，传统方法在分布变化时表现不佳，需要开发能够适应分布变化的鲁棒决策框架。

Method: 提出熵正则化信任衰减方法，将应力感知指数倾斜注入信念更新和镜像下降决策中，在单纯形上通过Fenchel对偶等价性实现信念倾斜和决策倾斜的一致。

Result: 证明了高概率敏感性边界，在KL漂移路径长度S_T下实现了Õ(√T)的动态遗憾保证，信任衰减实现了O(1)的每切换遗憾，而无应力更新会产生Ω(1)的尾部。

Conclusion: 该框架统一了动态遗憾分析、分布鲁棒目标和KL正则化控制，为分布漂移环境下的序列决策提供了有效的应力自适应更新方法。

Abstract: We study sequential decision-making under distribution drift. We propose
entropy-regularized trust-decay, which injects stress-aware exponential tilting
into both belief updates and mirror-descent decisions. On the simplex, a
Fenchel-dual equivalence shows that belief tilt and decision tilt coincide. We
formalize robustness via fragility (worst-case excess risk in a KL ball),
belief bandwidth (radius sustaining a target excess), and a decision-space
Fragility Index (drift tolerated at $O(\sqrt{T})$ regret). We prove
high-probability sensitivity bounds and establish dynamic-regret guarantees of
$\tilde{O}(\sqrt{T})$ under KL-drift path length $S_T = \sum_{t\ge2}\sqrt{{\rm
KL}(D_t|D_{t-1})/2}$. In particular, trust-decay achieves $O(1)$ per-switch
regret, while stress-free updates incur $\Omega(1)$ tails. A parameter-free
hedge adapts the tilt to unknown drift, whereas persistent over-tilting yields
an $\Omega(\lambda^2 T)$ stationary penalty. We further obtain
calibrated-stress bounds and extensions to second-order updates, bandit
feedback, outliers, stress variation, distributed optimization, and plug-in
KL-drift estimation. The framework unifies dynamic-regret analysis,
distributionally robust objectives, and KL-regularized control within a single
stress-adaptive update.

</details>


### [89] [FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in Finance Domain](https://arxiv.org/abs/2510.15232)
*Tiansheng Hu,Tongyan Hu,Liuyang Bai,Yilun Zhao,Arman Cohan,Chen Zhao*

Main category: cs.LG

TL;DR: FinTrust是一个专门评估金融领域LLM可信赖性的基准测试，涵盖多个维度的对齐问题，发现专有模型在安全性方面表现更好，开源模型在特定公平性方面有优势，但所有模型在法律意识方面都存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 由于金融领域的高风险和高利害关系特性，在真实金融应用中部署LLM具有挑战性，需要评估其可信赖性。

Method: 开发了FinTrust基准测试，基于实际情境设计广泛的对齐问题，并为每个可信赖性评估维度提供细粒度任务。

Result: 评估了11个LLM，发现专有模型如o4-mini在安全性等大多数任务中表现更好，开源模型如DeepSeek-V3在行业级公平性等特定领域有优势，但所有模型在受托责任对齐和披露等挑战性任务中都表现不佳。

Conclusion: FinTrust可以作为金融领域LLM可信赖性评估的有价值基准，当前LLM在法律意识方面存在显著差距。

Abstract: Recent LLMs have demonstrated promising ability in solving finance related
problems. However, applying LLMs in real-world finance application remains
challenging due to its high risk and high stakes property. This paper
introduces FinTrust, a comprehensive benchmark specifically designed for
evaluating the trustworthiness of LLMs in finance applications. Our benchmark
focuses on a wide range of alignment issues based on practical context and
features fine-grained tasks for each dimension of trustworthiness evaluation.
We assess eleven LLMs on FinTrust and find that proprietary models like o4-mini
outperforms in most tasks such as safety while open-source models like
DeepSeek-V3 have advantage in specific areas like industry-level fairness. For
challenging task like fiduciary alignment and disclosure, all LLMs fall short,
showing a significant gap in legal awareness. We believe that FinTrust can be a
valuable benchmark for LLMs' trustworthiness evaluation in finance domain.

</details>


### [90] [Adaptive Individual Uncertainty under Out-Of-Distribution Shift with Expert-Routed Conformal Prediction](https://arxiv.org/abs/2510.15233)
*Amitesh Badkul,Lei Xie*

Main category: cs.LG

TL;DR: 提出了TESSERA方法，通过结合专家混合模型和保形校准，为蛋白质-配体亲和力预测提供可靠、信息丰富且自适应的不确定性量化，在分布偏移下仍能保持接近名义覆盖率和最佳覆盖-宽度权衡。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习社区缺乏可靠、信息丰富且个体化的不确定性量化方法，这阻碍了AI/ML在风险敏感领域的有效应用。特别是在药物发现中的蛋白质-配体亲和力预测面临异质性噪声、化学空间不平衡和分布偏移等挑战。

Method: TESSERA方法结合专家混合模型的多样性和保形校准，提供每个样本的不确定性，具有可靠的覆盖保证、信息丰富且自适应的预测区间宽度，能够跟踪绝对误差。

Result: 在独立同分布和基于支架的分布外分割下，TESSERA实现了接近名义覆盖率和最佳覆盖-宽度权衡（CWC），同时保持竞争性的适应性（最低AUSE）。大小分层覆盖进一步确认区间大小合适。

Conclusion: 通过统一专家混合模型多样性和保形校准，TESSERA提供了可信赖、紧凑且自适应的不确定性，适用于药物发现管道和其他应用中的选择性预测和下游决策。

Abstract: Reliable, informative, and individual uncertainty quantification (UQ) remains
missing in current ML community. This hinders the effective application of
AI/ML to risk-sensitive domains. Most methods either fail to provide coverage
on new data, inflate intervals so broadly that they are not actionable, or
assign uncertainties that do not track actual error, especially under a
distribution shift. In high-stakes drug discovery, protein-ligand affinity
(PLI) prediction is especially challenging as assay noise is heterogeneous,
chemical space is imbalanced and large, and practical evaluations routinely
involve distribution shift. In this work, we introduce a novel uncertainty
quantification method, Trustworthy Expert Split-conformal with Scaled
Estimation for Efficient Reliable Adaptive intervals (TESSERA), that provides
per-sample uncertainty with reliable coverage guarantee, informative and
adaptive prediction interval widths that track the absolute error. We evaluate
on protein-ligand binding affinity prediction under both independent and
identically distributed (i.i.d.) and scaffold-based out-of-distribution (OOD)
splits, comparing against strong UQ baselines. TESSERA attains near-nominal
coverage and the best coverage-width trade-off as measured by the
Coverage-Width Criterion (CWC), while maintaining competitive adaptivity
(lowest Area Under the Sparsification Error (AUSE)). Size-Stratified Coverage
(SSC) further confirms that intervals are right-sized, indicating width
increases when data are scarce or noisy, and remain tight when predictions are
reliable. By unifying Mixture of Expert (MoE) diversity with conformal
calibration, TESSERA delivers trustworthy, tight, and adaptive uncertainties
that are well-suited to selective prediction and downstream decision-making in
the drug-discovery pipeline and other applications.

</details>


### [91] [Dual-Weighted Reinforcement Learning for Generative Preference Modeling](https://arxiv.org/abs/2510.15242)
*Shengyu Feng,Yun He,Shuang Ma,Beibin Li,Yuanhao Xiong,Vincent Li,Karishma Mandyam,Julian Katz-Samuels,Shengjie Bi,Licheng Yu,Hejia Zhang,Karthik Abinav Sankararaman,Han Fang,Riham Mansour,Yiming Yang,Manaal Faruqui*

Main category: cs.LG

TL;DR: DWRL是一个新的偏好建模框架，通过双权重强化学习目标整合思维链推理和Bradley-Terry模型，在非可验证任务上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 强化学习在可验证答案任务上有效扩展思维链推理，但在更普遍的非可验证偏好任务上仍具挑战性且研究不足。

Method: 提出双权重强化学习框架，包含实例级错位权重和组级条件偏好分数，训练生成式偏好模型先产生思维再预测人类偏好分数。

Result: 在多个基准测试和模型规模上，DWRL始终优于GPM基线和标量模型，同时产生连贯可解释的思维。

Conclusion: DWRL作为推理增强偏好学习的通用框架，可扩展到可验证任务之外。

Abstract: Reinforcement learning (RL) has recently proven effective at scaling
chain-of-thought (CoT) reasoning in large language models on tasks with
verifiable answers. However, extending RL to more general non-verifiable tasks,
typically in the format of human preference pairs, remains both challenging and
underexplored. In this work, we propose Dual-Weighted Reinforcement Learning
(DWRL), a new framework for preference modeling that integrates CoT reasoning
with the Bradley-Terry (BT) model via a dual-weighted RL objective that
preserves preference-modeling inductive bias. DWRL approximates the
maximum-likelihood objective of the BT model with two complementary weights: an
instance-wise misalignment weight, which emphasizes under-trained pairs
misaligned with human preference, and a group-wise (self-normalized)
conditional preference score, which promotes promising thoughts. In this paper,
we apply DWRL to preference modeling by training generative preference models
(GPMs) to first generate a thought and then predict the human preference score.
Across multiple benchmarks and model scales (Llama3 and Qwen2.5), DWRL
consistently outperforms both GPM baselines and scalar models, while producing
coherent, interpretable thoughts. In summary, our results position DWRL as a
general framework for reasoning-enhanced preference learning beyond verifiable
tasks.

</details>


### [92] [Spatiotemporal Transformers for Predicting Avian Disease Risk from Migration Trajectories](https://arxiv.org/abs/2510.15254)
*Dingya Feng,Dingyuan Xue*

Main category: cs.LG

TL;DR: 该研究提出了基于Transformer的框架，用于预测候鸟迁徙轨迹终点位置的疾病风险，整合GPS追踪数据、疫情记录和地理空间信息，在测试集上表现出优异的预测性能。


<details>
  <summary>Details</summary>
Motivation: 准确预测禽类疾病暴发对野生动物保护和公共卫生至关重要，需要开发能够预测候鸟迁徙终点疾病风险的方法来支持早期预警系统。

Method: 整合多源数据集（Movebank的GPS追踪数据、WOAH疫情记录、GADM和Natural Earth地理空间数据），使用H3分层地理空间编码处理原始坐标，通过Transformer模型学习鸟类移动序列的时空依赖性来估计终点疾病风险。

Result: 在测试集上表现出色：准确率0.9821，ROC曲线下面积0.9803，平均精度0.9299，F1分数0.8836（在最优阈值下）。

Conclusion: Transformer架构在支持禽类疾病监测的早期预警系统方面具有巨大潜力，能够实现及时的干预和预防策略。

Abstract: Accurate forecasting of avian disease outbreaks is critical for wildlife
conservation and public health. This study presents a Transformer-based
framework for predicting the disease risk at the terminal locations of
migratory bird trajectories. We integrate multi-source datasets, including GPS
tracking data from Movebank, outbreak records from the World Organisation for
Animal Health (WOAH), and geospatial context from GADM and Natural Earth. The
raw coordinates are processed using H3 hierarchical geospatial encoding to
capture spatial patterns. The model learns spatiotemporal dependencies from
bird movement sequences to estimate endpoint disease risk. Evaluation on a
held-out test set demonstrates strong predictive performance, achieving an
accuracy of 0.9821, area under the ROC curve (AUC) of 0.9803, average precision
(AP) of 0.9299, and an F1-score of 0.8836 at the optimal threshold. These
results highlight the potential of Transformer architectures to support
early-warning systems for avian disease surveillance, enabling timely
intervention and prevention strategies.

</details>


### [93] [DRO-InstructZero: Distributionally Robust Prompt Optimization for Large Language Models](https://arxiv.org/abs/2510.15260)
*Yangyang Li*

Main category: cs.LG

TL;DR: DRO-InstructZero通过将零样本提示优化构建为鲁棒贝叶斯优化，使用f-散度球定义评估分布的模糊集，在保持贝叶斯搜索查询效率的同时最大化最坏情况期望效用，从而提升提示在分布偏移下的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示搜索方法（如InstructZero）在分布偏移和对抗性评估下性能下降，因为它们仅优化单一评估分布下的期望性能，导致提示在不同设置间难以迁移。

Method: 将零样本提示优化构建为鲁棒贝叶斯优化，使用f-散度球定义评估分布的模糊集，采用鲁棒采集规则最大化最坏情况期望效用，同时保持贝叶斯搜索的查询效率。

Result: 在形式化重写任务中准确率从61.3%提升至85-90%，绝对增益约25-30点；自动调试在域偏移下提升约25点；稳定任务如因果关系保持96%以上准确率；改进在不同散度选择和解码温度下一致。

Conclusion: DRO-InstructZero将分布鲁棒优化与提示学习相结合，为现实世界不确定性下的可靠、可迁移提示对齐提供了即插即用的通用方法。

Abstract: Large language models are highly sensitive to prompt wording. However,
popular automatic prompt search methods, including InstructZero, often degrade
under distribution shift and adversarial evaluation because they optimize
expected performance under a single evaluation distribution. Consequently,
prompts that work in one setting frequently fail to transfer. To address this,
DRO-InstructZero formulates zero-shot prompt optimization as robust Bayesian
optimization. Specifically, an f-divergence ball defines an ambiguity set
around the evaluation distribution, and a robust acquisition rule maximizes
worst-case expected utility while retaining the query efficiency of Bayesian
search. Therefore, the search explicitly targets reliability under distribution
shift rather than average behavior alone. Experiments follow the
instruction-induction protocol with matched query budgets across formality
rewriting, code debugging, and translation. For example, on BIG-Bench
informative-to-formal rewriting, accuracy improves from 61.3 +/- 0.7% to
approximately 85-90%, yielding an absolute gain of about 25-30 points.
Moreover, auto-debugging shows about +25-point gains under domain shift.
Meanwhile, stable tasks such as cause-and-effect remain above 96%, indicating
no loss on in-distribution cases. Furthermore, improvements are consistent
across divergence choices and decoding temperatures. Overall, DRO-InstructZero
connects distributionally robust optimization with prompt learning, offering a
plug-and-play and general approach for reliable, transferable prompt alignment
under real-world uncertainty.

</details>


### [94] [Robust Layerwise Scaling Rules by Proper Weight Decay Tuning](https://arxiv.org/abs/2510.15262)
*Zhiyuan Fan,Yifeng Liu,Qingyue Zhao,Angela Yuan,Quanquan Gu*

Main category: cs.LG

TL;DR: 该论文提出了AdamW优化器的权重衰减缩放规则，解决了μP在稳态训练中因归一化层导致的宽度依赖性问题，实现了学习率和权重衰减的零样本跨宽度迁移。


<details>
  <summary>Details</summary>
Motivation: 现代尺度不变架构中，训练快速进入优化器主导的稳态，归一化层造成反向尺度敏感性，使有效学习率变得宽度依赖，破坏了μP的迁移能力。

Method: 引入AdamW的权重衰减缩放规则，通过观察奇异值谱的缩放行为，提出λ₂∝√d的权重衰减缩放规则，结合向量参数在η₁=Θ_d(1)和λ₁=0的训练，实现子层增益的宽度不变性。

Result: 在LLaMA风格Transformer和最小合成设置中验证了该规则，提供了匹配顶部奇异值的简单诊断方法，成功实现了学习率和权重衰减的零样本跨宽度迁移。

Conclusion: 通过显式控制优化器设定的稳态尺度，将μP扩展到近初始化阶段之外，为AdamW下的宽度鲁棒超参数迁移提供了实用方案。

Abstract: Empirical scaling laws prescribe how to allocate parameters, data, and
compute, while maximal-update parameterization ($\mu$P) enables learning-rate
transfer across widths by equalizing early-time update magnitudes. However, in
modern scale-invariant architectures, training quickly enters an
optimizer-governed steady state where normalization layers create backward
scale sensitivity and the effective learning rate becomes width dependent,
degrading $\mu$P transfer. We address this by introducing a weight-decay
scaling rule for AdamW that preserves sublayer gain across widths. Empirically,
the singular-value spectrum of each matrix parameter scales in norm as
$\sqrt{\eta/\lambda}$ with an approximately invariant shape; under width
scaling $d$, we observe that the top singular value scales approximately as
$\sqrt{\eta/\lambda}\cdot d^{0.75}$. Combining this observation with the $\mu$P
learning-rate rule $\eta_2\propto d^{-1}$ for matrix-like parameters implies an
empirical weight-decay scaling rule $\lambda_2\propto \sqrt{d}$ that
approximately keeps sublayer gains width invariant. Together with vector-like
parameters trained at $\eta_1=\Theta_d(1)$ and $\lambda_1=0$, this yields
\emph{zero-shot} transfer of both learning rate and weight decay from proxy to
target widths, removing per-width sweeps. We validate the rule on LLaMA-style
Transformers and in a minimal synthetic setting, and we provide a simple
diagnostic, matching top singular values, to check sublayer-gain invariance.
Our results extend $\mu$P beyond the near-init regime by explicitly controlling
steady-state scales set by the optimizer, offering a practical recipe for
width-robust hyperparameter transfer under AdamW.

</details>


### [95] [Causal Time Series Modeling of Supraglacial Lake Evolution in Greenland under Distribution Shift](https://arxiv.org/abs/2510.15265)
*Emam Hossain,Muhammad Hasan Ferdous,Devon Dunmire,Aneesh Subramanian,Md Osman Gani*

Main category: cs.LG

TL;DR: RIC-TSC框架将因果发现融入时间序列分类，通过区域特定的因果图识别冰川湖演化的稳定预测因子，在分布外评估中比基于相关性的方法准确率提升12.59%。


<details>
  <summary>Details</summary>
Motivation: 当前时空地球观测模型依赖纯相关性特征，在异质域间迁移性差。因果建模能发现稳定不变关系，提高分布偏移下的鲁棒性和泛化能力。

Method: 使用J-PCMCI+进行区域特定和不变预测因子识别，将验证的预测因子及其时间滞后输入轻量级分类器。基于多模态卫星和再分析数据。

Result: 在1000个标记冰川湖的平衡基准上，因果模型在分布外评估中比基于相关性的基线准确率最高提升12.59%。

Conclusion: 因果发现不仅是特征选择手段，更是建立可泛化和机制基础的地球表面过程动态模型的途径。

Abstract: Causal modeling offers a principled foundation for uncovering stable,
invariant relationships in time-series data, thereby improving robustness and
generalization under distribution shifts. Yet its potential is underutilized in
spatiotemporal Earth observation, where models often depend on purely
correlational features that fail to transfer across heterogeneous domains. We
propose RIC-TSC, a regionally-informed causal time-series classification
framework that embeds lag-aware causal discovery directly into sequence
modeling, enabling both predictive accuracy and scientific interpretability.
Using multi-modal satellite and reanalysis data-including Sentinel-1 microwave
backscatter, Sentinel-2 and Landsat-8 optical reflectance, and CARRA
meteorological variables-we leverage Joint PCMCI+ (J-PCMCI+) to identify
region-specific and invariant predictors of supraglacial lake evolution in
Greenland. Causal graphs are estimated globally and per basin, with validated
predictors and their time lags supplied to lightweight classifiers. On a
balanced benchmark of 1000 manually labeled lakes from two contrasting melt
seasons (2018-2019), causal models achieve up to 12.59% higher accuracy than
correlation-based baselines under out-of-distribution evaluation. These results
show that causal discovery is not only a means of feature selection but also a
pathway to generalizable and mechanistically grounded models of dynamic Earth
surface processes.

</details>


### [96] [Semi-Supervised Regression with Heteroscedastic Pseudo-Labels](https://arxiv.org/abs/2510.15266)
*Xueqing Sun,Renzhen Wang,Quanziang Wang,Yichen Wu,Xixi Jia,Deyu Meng*

Main category: cs.LG

TL;DR: 提出了一种不确定性感知的伪标签框架，用于半监督回归任务，通过双层优化动态调整伪标签的影响，有效减轻不可靠伪标签的影响。


<details>
  <summary>Details</summary>
Motivation: 半监督回归中的伪标签方法研究较少，由于连续输出和异方差噪声的存在，难以评估伪标签的可靠性，导致错误累积和过拟合问题。

Method: 采用不确定性感知的伪标签框架，通过双层优化联合最小化所有数据的经验风险和优化不确定性估计，以增强在标记数据上的泛化能力。

Result: 在多个基准半监督回归数据集上的实验表明，该方法相比现有方法具有更优越的鲁棒性和性能表现。

Conclusion: 所提出的不确定性感知伪标签框架有效解决了半监督回归中伪标签可靠性评估的挑战，为半监督回归任务提供了有效的解决方案。

Abstract: Pseudo-labeling is a commonly used paradigm in semi-supervised learning, yet
its application to semi-supervised regression (SSR) remains relatively
under-explored. Unlike classification, where pseudo-labels are discrete and
confidence-based filtering is effective, SSR involves continuous outputs with
heteroscedastic noise, making it challenging to assess pseudo-label
reliability. As a result, naive pseudo-labeling can lead to error accumulation
and overfitting to incorrect labels. To address this, we propose an
uncertainty-aware pseudo-labeling framework that dynamically adjusts
pseudo-label influence from a bi-level optimization perspective. By jointly
minimizing empirical risk over all data and optimizing uncertainty estimates to
enhance generalization on labeled data, our method effectively mitigates the
impact of unreliable pseudo-labels. We provide theoretical insights and
extensive experiments to validate our approach across various benchmark SSR
datasets, and the results demonstrate superior robustness and performance
compared to existing methods. Our code is available at
https://github.com/sxq/Heteroscedastic-Pseudo-Labels.

</details>


### [97] [Foundation Models for Scientific Discovery: From Paradigm Enhancement to Paradigm Transition](https://arxiv.org/abs/2510.15280)
*Fan Liu,Jindong Han,Tengfei Lyu,Weijia Zhang,Zhe-Rui Yang,Lu Dai,Cancheng Liu,Hao Liu*

Main category: cs.LG

TL;DR: 该论文提出了一个三阶段框架来描述基础模型如何推动科学范式的转变：从增强传统工作流到人机协同创造，再到自主科学发现。


<details>
  <summary>Details</summary>
Motivation: 探讨基础模型（如GPT-4、AlphaFold）不仅仅是增强现有科学方法，而是在重新定义科学实践方式，催化新的科学范式。

Method: 引入三阶段演化框架：1）元科学整合；2）人机协同创造；3）自主科学发现。通过这一视角回顾当前应用和新兴能力。

Result: 识别了基础模型在科学发现中的风险和未来方向，支持科学界理解其变革性作用。

Conclusion: 基础模型正在催化科学范式的转变，从工具性增强发展到主动协作，最终可能实现自主科学发现。

Abstract: Foundation models (FMs), such as GPT-4 and AlphaFold, are reshaping the
landscape of scientific research. Beyond accelerating tasks such as hypothesis
generation, experimental design, and result interpretation, they prompt a more
fundamental question: Are FMs merely enhancing existing scientific
methodologies, or are they redefining the way science is conducted? In this
paper, we argue that FMs are catalyzing a transition toward a new scientific
paradigm. We introduce a three-stage framework to describe this evolution: (1)
Meta-Scientific Integration, where FMs enhance workflows within traditional
paradigms; (2) Hybrid Human-AI Co-Creation, where FMs become active
collaborators in problem formulation, reasoning, and discovery; and (3)
Autonomous Scientific Discovery, where FMs operate as independent agents
capable of generating new scientific knowledge with minimal human intervention.
Through this lens, we review current applications and emerging capabilities of
FMs across existing scientific paradigms. We further identify risks and future
directions for FM-enabled scientific discovery. This position paper aims to
support the scientific community in understanding the transformative role of
FMs and to foster reflection on the future of scientific discovery. Our project
is available at
https://github.com/usail-hkust/Awesome-Foundation-Models-for-Scientific-Discovery.

</details>


### [98] [Small Ensemble-based Data Assimilation: A Machine Learning-Enhanced Data Assimilation Method with Limited Ensemble Size](https://arxiv.org/abs/2510.15284)
*Zhilin Li,Zhou Yao,Xianglong Li,Zeng Liu,Zhaokuan Lu,Shanlin Xu,Seungnam Kim,Guangyao Wang*

Main category: cs.LG

TL;DR: 提出了一种结合集成卡尔曼滤波和全连接神经网络的机器学习数据同化方法，用小规模集成提高分析精度且计算成本可忽略


<details>
  <summary>Details</summary>
Motivation: 传统集成数据同化方法在分析精度和计算效率之间存在权衡，大集成规模提高精度但计算成本高

Method: 先用小规模集成生成初步分析状态，然后用全连接神经网络学习并预测校正项来弥补小集成规模带来的性能下降

Result: 在Lorenz系统和非线性海洋波场模拟实验中，新方法比传统集成卡尔曼滤波在相同集成规模下精度更高，且额外计算成本可忽略

Conclusion: EnKF-FCNN方法通过机器学习校正提高了小集成规模下的数据同化精度，具有适应不同应用的灵活性

Abstract: Ensemble-based data assimilation (DA) methods have become increasingly
popular due to their inherent ability to address nonlinear dynamic problems.
However, these methods often face a trade-off between analysis accuracy and
computational efficiency, as larger ensemble sizes required for higher accuracy
also lead to greater computational cost. In this study, we propose a novel
machine learning-based data assimilation approach that combines the traditional
ensemble Kalman filter (EnKF) with a fully connected neural network (FCNN).
Specifically, our method uses a relatively small ensemble size to generate
preliminary yet suboptimal analysis states via EnKF. A FCNN is then employed to
learn and predict correction terms for these states, thereby mitigating the
performance degradation induced by the limited ensemble size. We evaluate the
performance of our proposed EnKF-FCNN method through numerical experiments
involving Lorenz systems and nonlinear ocean wave field simulations. The
results consistently demonstrate that the new method achieves higher accuracy
than traditional EnKF with the same ensemble size, while incurring negligible
additional computational cost. Moreover, the EnKF-FCNN method is adaptable to
diverse applications through coupling with different models and the use of
alternative ensemble-based DA methods.

</details>


### [99] [Identifying internal patterns in (1+1)-dimensional directed percolation using neural networks](https://arxiv.org/abs/2510.15294)
*Danil Parkhomenko,Pavel Ovchinnikov,Konstantin Soldatov,Vitalii Kapitan,Gennady Y. Chitov*

Main category: cs.LG

TL;DR: 提出基于CNN、TCN和GRU的神经网络方法，自动检测(1+1)维复制过程中的相变并分类隐藏渗透模式


<details>
  <summary>Details</summary>
Motivation: 开发能够直接从原始配置数据中自动检测相变和分类隐藏渗透模式的神经网络方法，无需手动特征提取

Method: 结合CNN、TCN和GRU网络，直接在原始配置数据上训练，无需手动特征提取

Result: 网络成功重现了相图并对配置分配了相标签，证明了深度架构能从数值实验原始数据中提取层次结构

Conclusion: 深度神经网络架构能够从原始数值实验数据中有效提取层次结构，实现相变的自动检测和隐藏渗透模式的分类

Abstract: In this paper we present a neural network-based method for the automatic
detection of phase transitions and classification of hidden percolation
patterns in a (1+1)-dimensional replication process. The proposed network model
is based on the combination of CNN, TCN and GRU networks, which are trained
directly on raw configurations without any manual feature extraction. The
network reproduces the phase diagram and assigns phase labels to
configurations. It shows that deep architectures are capable of extracting
hierarchical structures from the raw data of numerical experiments.

</details>


### [100] [DFCA: Decentralized Federated Clustering Algorithm](https://arxiv.org/abs/2510.15300)
*Jonas Kirch,Sebastian Becker,Tiago Koketsu Rodrigues,Stefan Harmeling*

Main category: cs.LG

TL;DR: DFCA是一种完全去中心化的聚类联邦学习算法，无需中央服务器协调，通过顺序运行平均聚合邻居模型，在稀疏连接下仍能保持与集中式IFCA相当的聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现有的聚类联邦学习方法依赖中央服务器协调模型更新，存在瓶颈和单点故障问题，限制了在现实去中心化学习场景中的应用。

Method: DFCA使用顺序运行平均来聚合邻居模型作为更新到达，提供通信高效的批量聚合替代方案，同时保持聚类性能。

Result: 在各种数据集上的实验表明，DFCA优于其他去中心化算法，性能与集中式IFCA相当，即使在稀疏连接下也表现出鲁棒性。

Conclusion: DFCA展示了在动态现实世界去中心化网络中的实用性和鲁棒性，为处理异构数据的聚类联邦学习提供了有效的去中心化解决方案。

Abstract: Clustered Federated Learning has emerged as an effective approach for
handling heterogeneous data across clients by partitioning them into clusters
with similar or identical data distributions. However, most existing methods,
including the Iterative Federated Clustering Algorithm (IFCA), rely on a
central server to coordinate model updates, which creates a bottleneck and a
single point of failure, limiting their applicability in more realistic
decentralized learning settings. In this work, we introduce DFCA, a fully
decentralized clustered FL algorithm that enables clients to collaboratively
train cluster-specific models without central coordination. DFCA uses a
sequential running average to aggregate models from neighbors as updates
arrive, providing a communication-efficient alternative to batch aggregation
while maintaining clustering performance. Our experiments on various datasets
demonstrate that DFCA outperforms other decentralized algorithms and performs
comparably to centralized IFCA, even under sparse connectivity, highlighting
its robustness and practicality for dynamic real-world decentralized networks.

</details>


### [101] [On the Generalization Properties of Learning the Random Feature Models with Learnable Activation Functions](https://arxiv.org/abs/2510.15327)
*Zailin Ma,Jiansheng Yang,Yaodong Yang*

Main category: cs.LG

TL;DR: 本文研究了可学习激活函数的随机特征模型(RFLAF)的泛化性质，通过数据依赖的采样方案显著提高了特征数量的理论边界，并提出了加权采样算法来减少所需特征数量。


<details>
  <summary>Details</summary>
Motivation: 传统的随机特征模型需要大量特征才能达到良好性能，本文旨在通过数据依赖的采样方案减少所需特征数量，提高模型效率。

Method: 采用数据依赖的杠杆加权采样方案，提出了近似核学习算法来构建加权RFLAF模型，分别在回归和分类任务中应用。

Result: 通过加权采样，MSE损失情况下的特征数量边界从Ω(1/ε²)改进到Ω̃((1/ε)^{1/t})，当Gram矩阵有限秩时甚至可达到Ω(1)；Lipschitz损失边界从Ω(1/ε²)改进到Ω̃((1/ε²)^{1/t})。实验验证了加权RFLAF能用更少特征达到相同性能。

Conclusion: 数据依赖的加权采样方案能显著减少随机特征模型所需特征数量，提供更紧的理论边界，并在实践中验证了有效性。

Abstract: This paper studies the generalization properties of a recently proposed
kernel method, the Random Feature models with Learnable Activation Functions
(RFLAF). By applying a data-dependent sampling scheme for generating features,
we provide by far the sharpest bounds on the required number of features for
learning RFLAF in both the regression and classification tasks. We provide a
unified theorem that describes the complexity of the feature number $s$, and
discuss the results for the plain sampling scheme and the data-dependent
leverage weighted scheme. Through weighted sampling, the bound on $s$ in the
MSE loss case is improved from $\Omega(1/\epsilon^2)$ to
$\tilde{\Omega}((1/\epsilon)^{1/t})$ in general $(t\geq 1)$, and even to
$\Omega(1)$ when the Gram matrix has a finite rank. For the Lipschitz loss
case, the bound is improved from $\Omega(1/\epsilon^2)$ to
$\tilde{\Omega}((1/\epsilon^2)^{1/t})$. To learn the weighted RFLAF, we also
propose an algorithm to find an approximate kernel and then apply the leverage
weighted sampling. Empirical results show that the weighted RFLAF achieves the
same performances with a significantly fewer number of features compared to the
plainly sampled RFLAF, validating our theories and the effectiveness of this
method.

</details>


### [102] [Backdoor or Manipulation? Graph Mixture of Experts Can Defend Against Various Graph Adversarial Attacks](https://arxiv.org/abs/2510.15333)
*Yuyuan Feng,Bin Ma,Enyan Dai*

Main category: cs.LG

TL;DR: 提出基于Mixture of Experts架构的统一防御框架，同时抵御图神经网络的后门攻击、边操作攻击和节点注入攻击。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法通常只针对单一类型的攻击，缺乏同时防御多种威胁的统一方法。图神经网络面临多种对抗攻击威胁，需要更全面的防御方案。

Method: 1) 使用基于互信息的逻辑多样性损失，鼓励不同专家关注不同的邻域结构；2) 设计鲁棒性感知路由器，识别扰动模式并自适应地将受扰节点路由到相应鲁棒专家。

Result: 在各种对抗设置下的广泛实验表明，该方法在抵御多种图对抗攻击方面始终表现出优越的鲁棒性。

Conclusion: MoE架构提供了一个可扩展的统一框架，能够有效同时防御多种图对抗攻击，确保在局部结构扰动下仍有足够数量的专家保持不受影响。

Abstract: Extensive research has highlighted the vulnerability of graph neural networks
(GNNs) to adversarial attacks, including manipulation, node injection, and the
recently emerging threat of backdoor attacks. However, existing defenses
typically focus on a single type of attack, lacking a unified approach to
simultaneously defend against multiple threats. In this work, we leverage the
flexibility of the Mixture of Experts (MoE) architecture to design a scalable
and unified framework for defending against backdoor, edge manipulation, and
node injection attacks. Specifically, we propose an MI-based logic diversity
loss to encourage individual experts to focus on distinct neighborhood
structures in their decision processes, thus ensuring a sufficient subset of
experts remains unaffected under perturbations in local structures. Moreover,
we introduce a robustness-aware router that identifies perturbation patterns
and adaptively routes perturbed nodes to corresponding robust experts.
Extensive experiments conducted under various adversarial settings demonstrate
that our method consistently achieves superior robustness against multiple
graph adversarial attacks.

</details>


### [103] [Sequence Modeling with Spectral Mean Flows](https://arxiv.org/abs/2510.15366)
*Jinwoo Kim,Max Beier,Petar Bevanda,Nayun Kim,Seunghoon Hong*

Main category: cs.LG

TL;DR: 提出了一种基于算子理论的序列建模新方法，通过谱分解和MMD梯度流在希尔伯特空间中嵌入序列分布，避免了传统的随机递归，实现了可扩展的序列生成。


<details>
  <summary>Details</summary>
Motivation: 传统序列建模方法在处理高度非线性和概率性状态动态时面临挑战，而算子理论提供了将动态视为希尔伯特空间线性映射的视角，但目前被忽视。

Method: 采用算子理论视角，将完整序列分布嵌入到乘积希尔伯特空间中；提出谱均值流算法，结合谱分解构建可扩展的张量网络，并将MMD梯度流扩展到时间依赖的希尔伯特空间。

Result: 在多个时间序列建模数据集上展示了有竞争力的结果，代码已开源。

Conclusion: 该方法为序列建模提供了新的算子理论视角，通过谱分解和MMD梯度流实现了高效的学习和采样。

Abstract: A key question in sequence modeling with neural networks is how to represent
and learn highly nonlinear and probabilistic state dynamics. Operator theory
views such dynamics as linear maps on Hilbert spaces containing mean embedding
vectors of distributions, offering an appealing but currently overlooked
perspective. We propose a new approach to sequence modeling based on an
operator-theoretic view of a hidden Markov model (HMM). Instead of
materializing stochastic recurrence, we embed the full sequence distribution as
a tensor in the product Hilbert space. A generative process is then defined as
maximum mean discrepancy (MMD) gradient flow in the space of sequences. To
overcome challenges with large tensors and slow sampling convergence, we
introduce spectral mean flows, a novel tractable algorithm integrating two core
concepts. First, we propose a new neural architecture by leveraging spectral
decomposition of linear operators to derive a scalable tensor network
decomposition of sequence mean embeddings. Second, we extend MMD gradient flows
to time-dependent Hilbert spaces and connect them to flow matching via the
continuity equation, enabling simulation-free learning and faster sampling. We
demonstrate competitive results on a range of time-series modeling datasets.
Code is available at https://github.com/jw9730/spectral-mean-flow.

</details>


### [104] [Towards Robust Zero-Shot Reinforcement Learning](https://arxiv.org/abs/2510.15382)
*Kexin Zheng,Lauriane Teyssier,Yinan Zheng,Yu Luo,Xiayuan Zhan*

Main category: cs.LG

TL;DR: BREEZE是一个改进的零样本强化学习框架，通过行为正则化、扩散模型策略提取和注意力架构，解决了现有方法在表达性和外推误差方面的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的前向-后向表示方法在零样本RL中存在表达性不足和离线学习中的外推误差问题，导致表示偏差和次优性能。

Method: 引入行为正则化将策略优化转化为稳定的样本内学习；使用任务条件扩散模型提取策略以生成高质量多模态动作分布；采用基于注意力的表达性架构进行表示建模。

Result: 在ExORL和D4RL Kitchen数据集上的广泛实验表明，BREEZE实现了最佳或接近最佳的性能，并展现出比现有离线零样本RL方法更优越的鲁棒性。

Conclusion: BREEZE通过同时增强学习稳定性、策略提取能力和表示学习质量，显著提升了零样本强化学习的性能。

Abstract: The recent development of zero-shot reinforcement learning (RL) has opened a
new avenue for learning pre-trained generalist policies that can adapt to
arbitrary new tasks in a zero-shot manner. While the popular Forward-Backward
representations (FB) and related methods have shown promise in zero-shot RL, we
empirically found that their modeling lacks expressivity and that extrapolation
errors caused by out-of-distribution (OOD) actions during offline learning
sometimes lead to biased representations, ultimately resulting in suboptimal
performance. To address these issues, we propose Behavior-REgularizEd Zero-shot
RL with Expressivity enhancement (BREEZE), an upgraded FB-based framework that
simultaneously enhances learning stability, policy extraction capability, and
representation learning quality. BREEZE introduces behavioral regularization in
zero-shot RL policy learning, transforming policy optimization into a stable
in-sample learning paradigm. Additionally, BREEZE extracts the policy using a
task-conditioned diffusion model, enabling the generation of high-quality and
multimodal action distributions in zero-shot RL settings. Moreover, BREEZE
employs expressive attention-based architectures for representation modeling to
capture the complex relationships between environmental dynamics. Extensive
experiments on ExORL and D4RL Kitchen demonstrate that BREEZE achieves the best
or near-the-best performance while exhibiting superior robustness compared to
prior offline zero-shot RL methods. The official implementation is available
at: https://github.com/Whiterrrrr/BREEZE.

</details>


### [105] [Iterative Refinement of Flow Policies in Probability Space for Online Reinforcement Learning](https://arxiv.org/abs/2510.15388)
*Mingyang Sun,Pengxiang Ding,Weinan Zhang,Donglin Wang*

Main category: cs.LG

TL;DR: SWFP框架通过离散化流匹配推理过程，将全局流分解为一系列小增量变换，每个步骤对应JKO更新，实现稳定的在线适应和高效微调预训练流模型。


<details>
  <summary>Details</summary>
Motivation: 行为克隆中的流/扩散策略虽然擅长从演示中学习复杂技能，但容易受到分布偏移的影响，标准RL方法由于迭代推理过程和现有解决方案的局限性而难以微调这些模型。

Method: 引入Stepwise Flow Policy框架，基于固定步长欧拉方案离散化流匹配推理过程，将其与最优传输的变分JKO原理对齐，将全局流分解为邻近分布间的小增量变换序列。

Result: 实验证明SWFP在多种机器人控制基准测试中展现出增强的稳定性、效率和优越的适应性能。

Conclusion: SWFP通过将流分解为小流块级联，提供了更简单/快速的子模型训练、降低的计算/内存成本，以及基于Wasserstein信任区域的可证明稳定性。

Abstract: While behavior cloning with flow/diffusion policies excels at learning
complex skills from demonstrations, it remains vulnerable to distributional
shift, and standard RL methods struggle to fine-tune these models due to their
iterative inference process and the limitations of existing workarounds. In
this work, we introduce the Stepwise Flow Policy (SWFP) framework, founded on
the key insight that discretizing the flow matching inference process via a
fixed-step Euler scheme inherently aligns it with the variational
Jordan-Kinderlehrer-Otto (JKO) principle from optimal transport. SWFP
decomposes the global flow into a sequence of small, incremental
transformations between proximate distributions. Each step corresponds to a JKO
update, regularizing policy changes to stay near the previous iterate and
ensuring stable online adaptation with entropic regularization. This
decomposition yields an efficient algorithm that fine-tunes pre-trained flows
via a cascade of small flow blocks, offering significant advantages:
simpler/faster training of sub-models, reduced computational/memory costs, and
provable stability grounded in Wasserstein trust regions. Comprehensive
experiments demonstrate SWFP's enhanced stability, efficiency, and superior
adaptation performance across diverse robotic control benchmarks.

</details>


### [106] [Geometric Mixture Models for Electrolyte Conductivity Prediction](https://arxiv.org/abs/2510.15403)
*Anyi Li,Jiacheng Cen,Songyou Li,Mingze Li,Yang Yu,Wenbing Huang*

Main category: cs.LG

TL;DR: 提出了GeoMix框架，一种几何感知的电解质系统离子电导率预测方法，通过保持Set-SE(3)等变性来准确建模混合物系统中的几何结构和分子间相互作用。


<details>
  <summary>Details</summary>
Motivation: 当前电解质研究面临两个基本挑战：缺乏高质量标准化基准，以及对混合物系统中几何结构和分子间相互作用的建模不足。

Method: 首先重组并增强CALiSol和DiffMix电解质数据集，加入分子几何图表示；然后提出GeoMix框架，包含专门设计的等变模块GIN，用于分子间几何消息传递。

Result: GeoMix在两个数据集上持续优于多种基线方法（包括MLPs、GNNs和几何GNNs），验证了跨分子几何相互作用和等变消息传递对于准确性质预测的重要性。

Conclusion: 这项工作不仅为电解质研究建立了新基准，还提供了一个通用的几何学习框架，可推进能源材料、药物开发等领域中混合物系统的建模。

Abstract: Accurate prediction of ionic conductivity in electrolyte systems is crucial
for advancing numerous scientific and technological applications. While
significant progress has been made, current research faces two fundamental
challenges: (1) the lack of high-quality standardized benchmarks, and (2)
inadequate modeling of geometric structure and intermolecular interactions in
mixture systems. To address these limitations, we first reorganize and enhance
the CALiSol and DiffMix electrolyte datasets by incorporating geometric graph
representations of molecules. We then propose GeoMix, a novel geometry-aware
framework that preserves Set-SE(3) equivariance-an essential but challenging
property for mixture systems. At the heart of GeoMix lies the Geometric
Interaction Network (GIN), an equivariant module specifically designed for
intermolecular geometric message passing. Comprehensive experiments demonstrate
that GeoMix consistently outperforms diverse baselines (including MLPs, GNNs,
and geometric GNNs) across both datasets, validating the importance of
cross-molecular geometric interactions and equivariant message passing for
accurate property prediction. This work not only establishes new benchmarks for
electrolyte research but also provides a general geometric learning framework
that advances modeling of mixture systems in energy materials, pharmaceutical
development, and beyond.

</details>


### [107] [Online Kernel Dynamic Mode Decomposition for Streaming Time Series Forecasting with Adaptive Windowing](https://arxiv.org/abs/2510.15404)
*Christopher Salazar,Krithika Manohar,Ashis G. Banerjee*

Main category: cs.LG

TL;DR: 提出了WORK-DMD方法，结合随机傅里叶特征和在线动态模态分解，用于实时流数据预测，在计算受限环境下实现高精度、自适应和高效的非线性动态捕捉。


<details>
  <summary>Details</summary>
Motivation: 解决实时流数据预测中的关键挑战：处理非平稳动态、在严格计算限制下运行、快速适应而不发生灾难性遗忘。现有方法在准确性、适应性和效率之间存在权衡。

Method: 使用随机傅里叶特征进行显式特征映射，结合在线动态模态分解，采用Sherman-Morrison更新和滚动窗口机制，实现固定计算成本和连续适应。

Result: 在多个领域的基准数据集上，WORK-DMD比几种最先进的在线预测方法获得更高精度，仅需单次数据遍历，在短期预测中表现尤其出色。

Conclusion: 结合核评估和自适应矩阵更新，以最小数据需求实现强预测性能，为流预测应用提供了深度学习之外的实用替代方案。

Abstract: Real-time forecasting from streaming data poses critical challenges: handling
non-stationary dynamics, operating under strict computational limits, and
adapting rapidly without catastrophic forgetting. However, many existing
approaches face trade-offs between accuracy, adaptability, and efficiency,
particularly when deployed in constrained computing environments. We introduce
WORK-DMD (Windowed Online Random Kernel Dynamic Mode Decomposition), a method
that combines Random Fourier Features with online Dynamic Mode Decomposition to
capture nonlinear dynamics through explicit feature mapping, while preserving
fixed computational cost and competitive predictive accuracy across evolving
data. WORK-DMD employs Sherman-Morrison updates within rolling windows,
enabling continuous adaptation to evolving dynamics from only current data,
eliminating the need for lengthy training or large storage requirements for
historical data. Experiments on benchmark datasets across several domains show
that WORK-DMD achieves higher accuracy than several state-of-the-art online
forecasting methods, while requiring only a single pass through the data and
demonstrating particularly strong performance in short-term forecasting. Our
results show that combining kernel evaluations with adaptive matrix updates
achieves strong predictive performance with minimal data requirements. This
sample efficiency offers a practical alternative to deep learning for streaming
forecasting applications.

</details>


### [108] [ParaFormer: Shallow Parallel Transformers with Progressive Approximation](https://arxiv.org/abs/2510.15425)
*Wei Wang,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: ParaFormer是一种浅层Transformer架构，通过并行分支结构实现真正的结构和计算并行，解决了深度模型训练时间长、推理延迟高的问题。


<details>
  <summary>Details</summary>
Motivation: 解决深度Transformer模型带来的训练时间长、推理延迟高以及在资源受限设备上不实用的问题。

Method: 将标准Transformer建模为闭式函数逼近器，通过并行分支结构实现层间协作，每个新分支进一步减少前序分支的损失，实现渐进逼近。

Result: ParaFormer在性能上优于标准Transformer如ViT，支持高达15.07倍的模型压缩，在多GPU部署中比FairScale等并行解决方案快3.30倍。

Conclusion: 基于通用逼近定理的闭式Transformer公式不仅解释了"深度信念"，还为设计高效Transformer架构开辟了新途径。

Abstract: The widespread 'deeper is better' philosophy has driven the creation of
architectures like ResNet and Transformer, which achieve high performance by
stacking numerous layers. However, increasing model depth comes with challenges
such as longer training times, higher inference latency, and impracticality on
resource-constrained devices. To address these issues, we propose ParaFormer, a
shallow Transformer architecture designed for true parallelism in both
structure and computation. By formulating standard Transformers as function
approximators in closed-form, our theoretical analysis shows that their
performance relies on inter-layer collaboration for progressive approximation,
rather than depth itself. While deep Transformers enforce this collaboration
through sequential designs, we demonstrate that such collaboration is not
inherently tied to sequential structures. ParaFormer removes the sequential
constraint by organizing layers into parallel branches, enforcing inter-layer
collaboration algorithmically. Specifically, we implement progressive
approximation, ensuring that each new branch further reduces the loss from
preceding branches, enabling faster convergence. Extensive experiments validate
ParaFormer's effectiveness, outperforming standard Transformers like ViT.
Moreover, ParaFormer supports up to 15.07x model compression and facilitates
model expansion for adaptive continuous learning. Experimental results on
multi-GPU deployment demonstrate that ParaFormer is 3.30x faster than widely
used parallelism solutions such as FairScale. These advancements stem from our
closed-form formulation of Transformers based on the Universal Approximation
Theorem, which not only explains the ``depth belief'' but also opens new
avenues for designing efficient Transformer architectures. Source code:
https://(open-upon-acceptance)

</details>


### [109] [Safe, Efficient, and Robust Reinforcement Learning for Ranking and Diffusion Models](https://arxiv.org/abs/2510.15429)
*Shashank Gupta*

Main category: cs.LG

TL;DR: 该论文研究如何设计安全、样本高效且鲁棒的强化学习方法，涵盖排序推荐系统和文本到图像扩散模型两大应用领域，提出了多种理论保证和算法改进。


<details>
  <summary>Details</summary>
Motivation: 为了解决强化学习在真实世界应用中的安全性、样本效率和鲁棒性问题，特别是在排序推荐和生成模型等关键领域。

Method: 采用上下文多臂老虎机框架，开发了曝光泛化边界理论、双重鲁棒估计器、基线校正框架以及LOOP算法（结合PPO和REINFORCE）。

Result: 提出的方法能够保证不劣于记录策略的性能，即使在稀疏反馈下也能安全部署；LOOP算法在保持PPO样本效率的同时，生成更符合文本属性的图像。

Conclusion: 通过理论分析和算法创新，该研究为强化学习的安全部署和高效学习提供了系统性的解决方案，在排序推荐和生成模型领域取得了显著进展。

Abstract: This dissertation investigates how reinforcement learning (RL) methods can be
designed to be safe, sample-efficient, and robust. Framed through the unifying
perspective of contextual-bandit RL, the work addresses two major application
domains - ranking and recommendation, and text-to-image diffusion models. The
first part of the thesis develops theory and algorithms for safe deployment in
ranking systems. An exposure-based generalisation bound is derived, leading to
a counterfactual risk-minimisation objective whose solution is guaranteed not
to underperform the logging policy, even with sparse feedback. This guarantee
is extended to doubly robust estimators, enabling safety even under adversarial
or misspecified user models and offering practitioners explicit control over
permissible utility loss. The second part turns to single-action bandits, where
various off-policy estimators are unified within a baseline-correction
framework. A closed-form optimal baseline is proposed and shown to minimise
both evaluation and policy-gradient variance, thereby improving off-policy
learning reliability. The final part examines the trade-offs between efficiency
and effectiveness in generative RL. A systematic study of PPO and REINFORCE
motivates the Leave-One-Out PPO (LOOP) algorithm, which combines multiple
diffusion trajectories with a REINFORCE-style baseline inside PPO's clipped
objective. LOOP achieves PPO-level sample efficiency while producing
generations that align more faithfully with textual attributes.

</details>


### [110] [A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning](https://arxiv.org/abs/2510.15444)
*Zhi Zhou,Yuhao Tan,Zenan Li,Yuan Yao,Lan-Zhe Guo,Yu-Feng Li,Xiaoxing Ma*

Main category: cs.LG

TL;DR: 本文提出了第一个基于置信度估计的理论框架来分析基于采样的测试时扩展方法，揭示了自一致性和困惑度方法的局限性，并提出了结合两者优点的RPC混合方法。


<details>
  <summary>Details</summary>
Motivation: 尽管基于采样的测试时扩展方法在实践中取得了成功，但其理论基础尚未得到充分探索。本文旨在填补这一空白，为这类方法提供理论分析框架。

Method: 提出了RPC方法，包含两个关键组件：困惑度一致性（结合自一致性和困惑度的优势）和推理剪枝（消除低概率推理路径）。

Result: 在七个基准数据集上的实验结果表明，RPC在保持与自一致性相当推理性能的同时，不仅提高了置信度可靠性，还将采样成本降低了50%。

Conclusion: RPC方法通过理论指导的混合策略，有效解决了现有方法的局限性，在减少推理错误方面展现出强大潜力。

Abstract: Test-time scaling seeks to improve the reasoning performance of large
language models (LLMs) by adding computational resources. A prevalent approach
within the field is sampling-based test-time scaling methods, which enhance
reasoning by generating multiple reasoning paths for a given input during
inference. However, despite its practical success, the theoretical foundations
remain underexplored. In this paper, we provide the first theoretical framework
for analyzing sampling-based test-time scaling methods, grounded in the
perspective of confidence estimation. Based on the framework, we analyze two
dominant paradigms: self-consistency and perplexity, and reveal key
limitations: self-consistency suffers from high estimation error while
perplexity exhibits substantial modeling error and possible degradation of the
estimation error convergence. To address these limitations, we introduce RPC, a
hybrid method that leverages our theoretical insights through two key
components: Perplexity Consistency and Reasoning Pruning. Perplexity
Consistency combines the strengths of self-consistency and perplexity, boosting
the convergence rate of estimation error from linear to exponential while
preserving model error. Reasoning Pruning prevents degradation by eliminating
low-probability reasoning paths. Both theoretical analysis and empirical
results across seven benchmark datasets demonstrate that RPC has a strong
potential for reducing reasoning error. Notably, RPC achieves reasoning
performance comparable to self-consistency while not only enhancing confidence
reliability but also reducing sampling costs by 50%. The code and resources are
available at https://wnjxyk.github.io/RPC.

</details>


### [111] [Particle Dynamics for Latent-Variable Energy-Based Models](https://arxiv.org/abs/2510.15447)
*Shiqin Tang,Shuxin Zhuang,Rong Feng,Runsheng Yu,Hongzong Li,Youzhi Zhang*

Main category: cs.LG

TL;DR: 提出一种基于鞍点问题的隐变量能量模型训练方法，将最大似然训练转化为潜变量和联合流形上的分布优化问题，无需判别器或辅助网络。


<details>
  <summary>Details</summary>
Motivation: 隐变量能量模型能够表达生成建模并捕捉隐藏结构，但传统训练方法存在局限。本文旨在开发无需判别器的有效训练算法。

Method: 将最大似然训练重新表述为鞍点问题，使用耦合Wasserstein梯度流进行内层更新，交替进行过阻尼Langevin更新和随机参数上升。

Result: 在标准平滑性和耗散性假设下证明了存在性和收敛性，在KL散度和Wasserstein-2距离上获得衰减率，性能在物理系统数值近似上具有竞争力。

Conclusion: 提出的鞍点视角提供了比受限摊销后验更紧的ELBO边界，为隐变量能量模型训练提供了理论保证和实用算法。

Abstract: Latent-variable energy-based models (LVEBMs) assign a single normalized
energy to joint pairs of observed data and latent variables, offering
expressive generative modeling while capturing hidden structure. We recast
maximum-likelihood training as a saddle problem over distributions on the
latent and joint manifolds and view the inner updates as coupled Wasserstein
gradient flows. The resulting algorithm alternates overdamped Langevin updates
for a joint negative pool and for conditional latent particles with stochastic
parameter ascent, requiring no discriminator or auxiliary networks. We prove
existence and convergence under standard smoothness and dissipativity
assumptions, with decay rates in KL divergence and Wasserstein-2 distance. The
saddle-point view further yields an ELBO strictly tighter than bounds obtained
with restricted amortized posteriors. Our method is evaluated on numerical
approximations of physical systems and performs competitively against
comparable approaches.

</details>


### [112] [Expediting Reinforcement Learning by Incorporating Knowledge About Temporal Causality in the Environment](https://arxiv.org/abs/2510.15456)
*Jan Corazza,Hadi Partovi Aria,Daniel Neider,Zhe Xu*

Main category: cs.LG

TL;DR: 提出了一种将时序逻辑因果图融入概率奖励机的方法，以解决强化学习中稀疏奖励和复杂时序依赖问题，加速策略学习并促进任务规范迁移。


<details>
  <summary>Details</summary>
Motivation: 强化学习在稀疏奖励和复杂时序依赖任务中表现不佳，概率奖励机虽然能捕捉这些依赖关系但难以手动设计和修改，阻碍了利用高层因果知识和跨领域迁移。

Method: 将时序逻辑因果图整合到概率奖励机中，利用因果信息来构建奖励形式化，并提供理论收敛保证。

Result: 理论证明了方法能收敛到最优策略，并通过实验验证了其在加速策略学习和任务规范迁移方面的优势。

Conclusion: 提出的方法成功地将因果信息融入奖励形式化，有效解决了稀疏奖励环境下的学习挑战，并支持跨环境的任务规范迁移。

Abstract: Reinforcement learning (RL) algorithms struggle with learning optimal
policies for tasks where reward feedback is sparse and depends on a complex
sequence of events in the environment. Probabilistic reward machines (PRMs) are
finite-state formalisms that can capture temporal dependencies in the reward
signal, along with nondeterministic task outcomes. While special RL algorithms
can exploit this finite-state structure to expedite learning, PRMs remain
difficult to modify and design by hand. This hinders the already difficult
tasks of utilizing high-level causal knowledge about the environment, and
transferring the reward formalism into a new domain with a different causal
structure. This paper proposes a novel method to incorporate causal information
in the form of Temporal Logic-based Causal Diagrams into the reward formalism,
thereby expediting policy learning and aiding the transfer of task
specifications to new environments. Furthermore, we provide a theoretical
result about convergence to optimal policy for our method, and demonstrate its
strengths empirically.

</details>


### [113] [Learning to Answer from Correct Demonstrations](https://arxiv.org/abs/2510.15464)
*Nirmit Joshi,Gene Li,Siddharth Bhandari,Shiva Prasad Kasiviswanathan,Cong Ma,Nathan Srebro*

Main category: cs.LG

TL;DR: 本文研究从正确演示中学习生成答案的问题，提出了一种在奖励模型类别基数较小时的替代方法，避免了最大似然估计的失败情况。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设演示者属于低复杂度策略类别，但在奖励模型类别基数较小时，最大似然估计可能失败，需要寻找更弱的假设条件。

Method: 将问题形式化为上下文多臂老虎机中的离线模仿学习，提出基于奖励模型类别基数的新方法，学习复杂度与奖励类别基数呈对数关系。

Result: 证明了最大似然方法在奖励模型类别基数较小时可能失败，而新方法能够以对数复杂度学习。

Conclusion: 在从正确演示中学习时，需要超越最大似然估计，考虑奖励模型类别的基数特性。

Abstract: We study the problem of learning to generate an answer (or completion) to a
question (or prompt), where there could be multiple correct answers, any one of
which is acceptable at test time. Learning is based on demonstrations of some
correct answer to each training question, as in Supervised Fine Tuning (SFT).
We formalize the problem as offline imitation learning in contextual bandits,
with demonstrations from some optimal policy, without explicitly observed
rewards. Prior work assumes that the demonstrator belongs to a low-complexity
policy class, which motivates maximum likelihood estimation (i.e., log-loss
minimization). In contrast, we propose relying only on the reward model
(specifying which answers are correct) being in a low-cardinality class, which
we argue is a weaker assumption. We show that likelihood maximization methods
can fail in this case, and instead devise an alternative novel approach that
learns with sample complexity logarithmic in the cardinality of the reward
class. Our work motivates looking beyond likelihood maximization when learning
from correct demonstrations.

</details>


### [114] [Adversary-Free Counterfactual Prediction via Information-Regularized Representations](https://arxiv.org/abs/2510.15479)
*Shiqin Tang,Rong Feng,Shuxin Zhuang,Hongzong Li,Youzhi Zhang*

Main category: cs.LG

TL;DR: 提出一种基于信息论的因果推断方法，通过最小化表示与治疗变量之间的互信息来消除分配偏差，无需对抗训练。


<details>
  <summary>Details</summary>
Motivation: 解决在存在分配偏差的情况下进行反事实预测的问题，避免对抗训练的不稳定性和调参负担。

Method: 学习一个随机表示Z，在预测结果的同时最小化I(Z;T)，推导出可处理的变分目标来上界信息项并与监督解码器耦合。

Result: 在数值模拟和真实临床数据集上的评估表明，该方法在似然度、反事实误差和政策评估等指标上表现优于现有方法。

Conclusion: 该方法提供了一个数学基础扎实、训练稳定的框架，能够有效处理分配偏差问题，特别适用于动态设置。

Abstract: We study counterfactual prediction under assignment bias and propose a
mathematically grounded, information-theoretic approach that removes
treatment-covariate dependence without adversarial training. Starting from a
bound that links the counterfactual-factual risk gap to mutual information, we
learn a stochastic representation Z that is predictive of outcomes while
minimizing I(Z; T). We derive a tractable variational objective that
upper-bounds the information term and couples it with a supervised decoder,
yielding a stable, provably motivated training criterion. The framework extends
naturally to dynamic settings by applying the information penalty to sequential
representations at each decision time. We evaluate the method on controlled
numerical simulations and a real-world clinical dataset, comparing against
recent state-of-the-art balancing, reweighting, and adversarial baselines.
Across metrics of likelihood, counterfactual error, and policy evaluation, our
approach performs favorably while avoiding the training instabilities and
tuning burden of adversarial schemes.

</details>


### [115] [OffSim: Offline Simulator for Model-based Offline Inverse Reinforcement Learning](https://arxiv.org/abs/2510.15495)
*Woo-Jin Ahn,Sang-Ryul Baek,Yong-Jun Lee,Hyun-Duck Choi,Myo-Taeg Lim*

Main category: cs.LG

TL;DR: OffSim是一个基于模型的离线逆强化学习框架，直接从专家轨迹中学习环境动态和奖励函数，无需真实环境交互即可训练策略。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习需要开发模拟器和手动定义奖励函数，这通常耗时耗力。

Method: 联合优化高熵转移模型和IRL奖励函数，增强探索性和奖励泛化能力；OffSim+扩展支持多数据集设置。

Result: 在MuJoCo实验中，OffSim相比现有离线IRL方法取得了显著性能提升。

Conclusion: OffSim框架在无需环境交互的情况下有效学习奖励函数和策略，具有高效性和鲁棒性。

Abstract: Reinforcement learning algorithms typically utilize an interactive simulator
(i.e., environment) with a predefined reward function for policy training.
Developing such simulators and manually defining reward functions, however, is
often time-consuming and labor-intensive. To address this, we propose an
Offline Simulator (OffSim), a novel model-based offline inverse reinforcement
learning (IRL) framework, to emulate environmental dynamics and reward
structure directly from expert-generated state-action trajectories. OffSim
jointly optimizes a high-entropy transition model and an IRL-based reward
function to enhance exploration and improve the generalizability of the learned
reward. Leveraging these learned components, OffSim can subsequently train a
policy offline without further interaction with the real environment.
Additionally, we introduce OffSim$^+$, an extension that incorporates a
marginal reward for multi-dataset settings to enhance exploration. Extensive
MuJoCo experiments demonstrate that OffSim achieves substantial performance
gains over existing offline IRL methods, confirming its efficacy and
robustness.

</details>


### [116] [The Road Less Traveled: Enhancing Exploration in LLMs via Sequential Sampling](https://arxiv.org/abs/2510.15502)
*Shijia Kang,Muhan Zhang*

Main category: cs.LG

TL;DR: 提出SESA框架，通过顺序采样生成多样化解题思路，解决强化学习中探索不足和熵崩溃问题，显著提升LLM推理多样性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在训练大语言模型时存在探索不足和熵崩溃问题，模型容易收敛到狭窄的解决方案集，导致采样多样性丧失，阻碍性能进一步提升。

Method: SESA框架采用顺序采样方法，首先生成多样化的解题思路草图，然后将其扩展为完整的推理路径。通过让每个新输出基于前一个输出，确保整个过程中的多样性探索。

Result: 在合成任务中，顺序采样在路径多样性和从崩溃中恢复方面持续优于传统RL方法。在三个智能体基准测试中，SESA将成功率分别提升了+0.25、+0.42和+0.07（相比基线RL最高达211%的相对提升）。

Conclusion: SESA为探索提供了一种结构化方法，为RL训练的大语言模型实现更有效和多样化的推理铺平了道路。

Abstract: Reinforcement learning (RL) has been pivotal in enhancing the reasoning
capabilities of large language models (LLMs), but it often suffers from limited
exploration and entropy collapse, where models exploit a narrow set of
solutions, leading to a loss of sampling diversity and subsequently preventing
RL from further improving performance. This issue is exacerbated in parallel
sampling methods, where multiple outputs are drawn from the same distribution,
potentially causing the model to converge to similar solutions. We propose
SESA, a novel SEquential SAmpling framework that mitigates this challenge by
generating diverse solution sketches sequentially before expanding them into
full reasoning paths. This approach ensures broader exploration by conditioning
each new output on previous ones, promoting diversity throughout the process
and preventing policy collapse. Our experiments on a synthetic task show that
sequential sampling consistently outperforms traditional RL methods in terms of
path diversity and recovery from collapse. Further evaluations on real-world
tasks demonstrate that SESA improves both the exploration of valid strategies
and the overall performance of LLMs. On three agent benchmarks, SESA lifts
success rates by $+0.25$, $+0.42$, and $+0.07$ absolute over the base model (up
to an additional $211\%$ relative improvement over baseline RL), underscoring
its exploration advantage. This work introduces a structured approach to
exploration, paving the way for more effective and diverse reasoning in
RL-trained LLMs. Our code is released at https://github.com/MuLabPKU/sesa.

</details>


### [117] [Theoretical Refinement of CLIP by Utilizing Linear Structure of Optimal Similarity](https://arxiv.org/abs/2510.15508)
*Naoki Yoshida,Satoshi Hayakawa,Yuhta Takida,Toshimitsu Uesaka,Hiromi Wakaki,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 提出KME-CLIP方法，通过再生核希尔伯特空间中的内积来改进CLIP中的相似度计算机制，更好地利用点互信息的线性结构。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP及其变体未能充分利用点互信息(PMI)的线性结构，而理论研究表明跨模态最优相似度度量应对应PMI。

Method: 在再生核希尔伯特空间中使用内积来近似PMI，理论上可达到任意精度。

Result: 在多个检索和分类任务中，KME-CLIP整体表现优于标准CLIP。

Conclusion: 通过利用PMI的线性结构，KME-CLIP能够更有效地计算跨模态相似度，提升多模态对比预训练性能。

Abstract: In this study, we propose an enhancement to the similarity computation
mechanism in multi-modal contrastive pretraining frameworks such as CLIP. Prior
theoretical research has demonstrated that the optimal similarity metrics
between paired modalities should correspond to the pointwise mutual information
(PMI) between the two modalities. However, the current implementations of CLIP
and its variants fail to fully utilize the underlying linear structure of PMI.
We therefore propose KME-CLIP, which leverages this structure through the inner
product in a reproducing kernel Hilbert space. We theoretically prove that our
method can approximate PMI with arbitrary accuracy and empirically demonstrate
that our approach overall outperforms the standard CLIP formulation across
several retrieval and classification tasks.

</details>


### [118] [Language Models are Injective and Hence Invertible](https://arxiv.org/abs/2510.15511)
*Giorgos Nikolaou,Tommaso Mencattini,Donato Crisostomi,Andrea Santilli,Yannis Panagakis,Emanuele Rodola'*

Main category: cs.LG

TL;DR: 本文证明了Transformer语言模型在将离散输入序列映射到连续表示时是单射的，这意味着可以从模型的隐藏激活中精确恢复原始输入文本。


<details>
  <summary>Details</summary>
Motivation: 挑战传统观点，即Transformer组件（如非线性激活和归一化）的非单射性会阻止从模型表示中精确恢复输入，探索语言模型是否实际上具有单射性。

Method: 1. 数学证明Transformer语言模型在初始化和训练过程中保持单射性；2. 在六个最先进语言模型上进行数十亿次碰撞测试；3. 提出SipIt算法，可证明且高效地从隐藏激活中重建原始输入文本。

Result: 数学证明和实证测试均表明Transformer语言模型是单射的，未观察到任何碰撞。SipIt算法在实践中实现了精确可逆性，具有线性时间保证。

Conclusion: 单射性是语言模型的一个基本且可利用的特性，对模型的透明度、可解释性和安全部署具有直接意义。

Abstract: Transformer components such as non-linear activations and normalization are
inherently non-injective, suggesting that different inputs could map to the
same output and prevent exact recovery of the input from a model's
representations. In this paper, we challenge this view. First, we prove
mathematically that transformer language models mapping discrete input
sequences to their corresponding sequence of continuous representations are
injective and therefore lossless, a property established at initialization and
preserved during training. Second, we confirm this result empirically through
billions of collision tests on six state-of-the-art language models, and
observe no collisions. Third, we operationalize injectivity: we introduce
SipIt, the first algorithm that provably and efficiently reconstructs the exact
input text from hidden activations, establishing linear-time guarantees and
demonstrating exact invertibility in practice. Overall, our work establishes
injectivity as a fundamental and exploitable property of language models, with
direct implications for transparency, interpretability, and safe deployment.

</details>


### [119] [Revisiting Knowledge Distillation: The Hidden Role of Dataset Size](https://arxiv.org/abs/2510.15516)
*Giulia Lanzillotta,Felix Sarnthein,Gil Kur,Thomas Hofmann,Bobby He*

Main category: cs.LG

TL;DR: 该研究发现知识蒸馏在低数据量情况下效果更佳，称为蒸馏的数据效率，并验证了现有蒸馏理论在不同数据集规模下的预测能力。


<details>
  <summary>Details</summary>
Motivation: 虽然知识蒸馏被广泛使用，但其工作机制仍不明确。现有研究主要关注模型大小和泛化能力，本研究从第三个维度——数据集大小——来探索蒸馏机制。

Method: 通过在不同数据集、任务和神经网络架构上进行广泛实验，测试现有蒸馏理论在不同数据集规模下的预测能力，并分析建模因素的影响。

Result: 实验表明蒸馏效果在低数据量情况下不仅保持而且增强，推翻了蒸馏可理解为标签平滑的假设，为暗知识假设提供了进一步证据。

Conclusion: 数据集大小可能是理解蒸馏机制的一个基本但被忽视的变量，蒸馏的数据效率特性为理解其工作机制提供了新视角。

Abstract: The concept of knowledge distillation (KD) describes the training of a
student model from a teacher model and is a widely adopted technique in deep
learning. However, it is still not clear how and why distillation works.
Previous studies focus on two central aspects of distillation: model size, and
generalisation. In this work we study distillation in a third dimension:
dataset size. We present a suite of experiments across a wide range of
datasets, tasks and neural architectures, demonstrating that the effect of
distillation is not only preserved but amplified in low-data regimes. We call
this newly discovered property the data efficiency of distillation. Equipped
with this new perspective, we test the predictive power of existing theories of
KD as we vary the dataset size. Our results disprove the hypothesis that
distillation can be understood as label smoothing, and provide further evidence
in support of the dark knowledge hypothesis. Finally, we analyse the impact of
modelling factors such as the objective, scale and relative number of samples
on the observed phenomenon. Ultimately, this work reveals that the dataset size
may be a fundamental but overlooked variable in the mechanisms underpinning
distillation.

</details>


### [120] [Compressive Modeling and Visualization of Multivariate Scientific Data using Implicit Neural Representation](https://arxiv.org/abs/2510.15535)
*Abhay Kumar Dwivedi,Shanu Saklani,Soumya Dutta*

Main category: cs.LG

TL;DR: 该论文开发了一种用于多变量数据集的压缩神经表示方法，通过参数共享的单一网络同时学习所有数据变量的表示，实现了最先进的数据压缩效果。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在科学可视化任务中的广泛应用，以及隐式神经表示在构建压缩数据模型方面的成功，激发了开发多变量数据集压缩表示的需求。

Method: 使用单一网络通过参数共享同时学习所有数据变量的表示，构建压缩的神经表示。

Result: 在重建数据质量、渲染和可视化质量、变量间依赖关系保持以及存储效率方面都表现出优越性能。

Conclusion: 该方法为多变量数据集提供了高效的压缩表示方案，在多个评估指标上都达到了最先进水平。

Abstract: The extensive adoption of Deep Neural Networks has led to their increased
utilization in challenging scientific visualization tasks. Recent advancements
in building compressed data models using implicit neural representations have
shown promising results for tasks like spatiotemporal volume visualization and
super-resolution. Inspired by these successes, we develop compressed neural
representations for multivariate datasets containing tens to hundreds of
variables. Our approach utilizes a single network to learn representations for
all data variables simultaneously through parameter sharing. This allows us to
achieve state-of-the-art data compression. Through comprehensive evaluations,
we demonstrate superior performance in terms of reconstructed data quality,
rendering and visualization quality, preservation of dependency information
among variables, and storage efficiency.

</details>


### [121] [An Empirical Study on MC Dropout--Based Uncertainty--Error Correlation in 2D Brain Tumor Segmentation](https://arxiv.org/abs/2510.15541)
*Saumya B*

Main category: cs.LG

TL;DR: 研究发现MC Dropout在脑肿瘤MRI分割中提供的边界误差定位信息有限，全局相关性弱，边界相关性几乎为零，需要开发替代或混合的不确定性估计方法。


<details>
  <summary>Details</summary>
Motivation: 尽管MC Dropout被广泛用于估计模型不确定性，但其在识别分割错误（特别是肿瘤边界附近）方面的有效性仍不明确，这对于医学图像分割的诊断和治疗规划至关重要。

Method: 使用U-Net在四种数据增强设置（无增强、水平翻转、旋转、缩放）下进行2D脑肿瘤MRI分割，通过50次随机前向传播计算不确定性，并使用Pearson和Spearman系数将不确定性与像素级误差进行相关性分析。

Result: 结果显示全局相关性较弱（r≈0.30-0.38），边界相关性可忽略不计（|r|<0.05）。虽然不同增强设置间的差异具有统计显著性（p<0.001），但缺乏实际意义。

Conclusion: MC Dropout不确定性在边界误差定位方面提供的线索有限，强调了在医学图像分割中需要替代或混合不确定性估计方法的必要性。

Abstract: Accurate brain tumor segmentation from MRI is vital for diagnosis and
treatment planning. Although Monte Carlo (MC) Dropout is widely used to
estimate model uncertainty, its effectiveness in identifying segmentation
errors -- especially near tumor boundaries -- remains unclear. This study
empirically examines the relationship between MC Dropout--based uncertainty and
segmentation error in 2D brain tumor MRI segmentation using a U-Net trained
under four augmentation settings: none, horizontal flip, rotation, and scaling.
Uncertainty was computed from 50 stochastic forward passes and correlated with
pixel-wise errors using Pearson and Spearman coefficients. Results show weak
global correlations ($r \approx 0.30$--$0.38$) and negligible boundary
correlations ($|r| < 0.05$). Although differences across augmentations were
statistically significant ($p < 0.001$), they lacked practical relevance. These
findings suggest that MC Dropout uncertainty provides limited cues for boundary
error localization, underscoring the need for alternative or hybrid uncertainty
estimation methods in medical image segmentation.

</details>


### [122] [Doubly Robust Estimation of Causal Effects in Strategic Equilibrium Systems](https://arxiv.org/abs/2510.15555)
*Sibo Xiao*

Main category: cs.LG

TL;DR: 提出战略双重稳健(SDR)估计器，将战略均衡建模与双重稳健估计相结合，用于战略环境中的因果推断。


<details>
  <summary>Details</summary>
Motivation: 解决因战略代理行为导致的内生性处理分配问题，在保持双重稳健性的同时纳入战略考量。

Method: 整合战略均衡建模与双重稳健估计框架，在战略无混淆性假设下进行理论分析。

Result: 实证评估显示SDR优于基线方法，在变化战略强度下实现7.6%-29.3%的偏差减少，并保持与代理群体规模的稳健可扩展性。

Conclusion: 该框架为代理对干预做出战略响应时的可靠因果推断提供了原则性方法。

Abstract: We introduce the Strategic Doubly Robust (SDR) estimator, a novel framework
that integrates strategic equilibrium modeling with doubly robust estimation
for causal inference in strategic environments. SDR addresses endogenous
treatment assignment arising from strategic agent behavior, maintaining double
robustness while incorporating strategic considerations. Theoretical analysis
confirms SDR's consistency and asymptotic normality under strategic
unconfoundedness. Empirical evaluations demonstrate SDR's superior performance
over baseline methods, achieving 7.6\%-29.3\% bias reduction across varying
strategic strengths and maintaining robust scalability with agent populations.
The framework provides a principled approach for reliable causal inference when
agents respond strategically to interventions.

</details>


### [123] [On the Neural Feature Ansatz for Deep Neural Networks](https://arxiv.org/abs/2510.15563)
*Edward Tansley,Estelle Massart,Coralia Cartis*

Main category: cs.LG

TL;DR: 本文扩展了神经特征假设(NFA)的研究，证明了在多层线性网络中NFA具有深度依赖性(α=1/L)，并在不平衡初始化下证明了NFA的渐近成立条件，同时提供了非线性网络的反例。


<details>
  <summary>Details</summary>
Motivation: 理解特征学习是建立深度神经网络数学基础的重要开放问题。神经特征假设(NFA)描述了训练后网络第一层权重Gram矩阵与平均梯度外积(AGOP)的关系，需要进一步研究其在不同网络结构和训练条件下的有效性。

Method: 使用梯度流动力学和平衡权重初始化假设，理论分析多层线性网络的NFA性质；通过数值实验验证理论结果，涵盖多种优化算法、权重衰减率和初始化方案。

Result: 证明对于L≥2层线性网络，NFA以指数α=1/L成立；对于不平衡初始化，在应用权重衰减时NFA渐近成立；提供了非线性网络的反例，即使网络能完美拟合训练数据，NFA也可能不成立。

Conclusion: NFA具有深度依赖性，在多层线性网络中成立，但在非线性网络中可能失效，这为理解深度神经网络的特征学习机制提供了重要理论洞见。

Abstract: Understanding feature learning is an important open question in establishing
a mathematical foundation for deep neural networks. The Neural Feature Ansatz
(NFA) states that after training, the Gram matrix of the first-layer weights of
a deep neural network is proportional to some power $\alpha>0$ of the average
gradient outer product (AGOP) of this network with respect to its inputs.
Assuming gradient flow dynamics with balanced weight initialization, the NFA
was proven to hold throughout training for two-layer linear networks with
exponent $\alpha = 1/2$ (Radhakrishnan et al., 2024). We extend this result to
networks with $L \geq 2$ layers, showing that the NFA holds with exponent
$\alpha = 1/L$, thus demonstrating a depth dependency of the NFA. Furthermore,
we prove that for unbalanced initialization, the NFA holds asymptotically
through training if weight decay is applied. We also provide counterexamples
showing that the NFA does not hold for some network architectures with
nonlinear activations, even when these networks fit arbitrarily well the
training data. We thoroughly validate our theoretical results through numerical
experiments across a variety of optimization algorithms, weight decay rates and
initialization schemes.

</details>


### [124] [Attn-JGNN: Attention Enhanced Join-Graph Neural Networks](https://arxiv.org/abs/2510.15583)
*Jixin Zhang,Yong Lai*

Main category: cs.LG

TL;DR: 提出了Attention Enhanced Join-Graph Neural Networks(Attn-JGNN)模型用于解决#SAT问题，通过注意力机制提升求解精度。


<details>
  <summary>Details</summary>
Motivation: 改进#SAT问题的求解精度，受Iterative Join Graph Propagation(IJGP)算法启发，结合神经网络和注意力机制来优化概率推理。

Method: 使用树分解将CNF公式编码为join-graph，在join-graph上执行迭代消息传递，通过注意力机制在簇内和簇间关注关键变量和簇，减少冗余计算。

Result: 实验表明Attn-JGNN模型比其他神经网络方法取得了更好的结果。

Conclusion: Attn-JGNN通过注意力机制增强了join-graph神经网络在#SAT问题上的求解能力，提高了推理精度。

Abstract: We propose an Attention Enhanced Join-Graph Neural Networks(Attn-JGNN) model
for solving #SAT problems, which significantly improves the solving accuracy.
Inspired by the Iterative Join Graph Propagation (IJGP) algorithm, Attn-JGNN
uses tree decomposition to encode the CNF formula into a join-graph, then
performs iterative message passing on the join-graph, and finally approximates
the model number by learning partition functions. In order to further improve
the accuracy of the solution, we apply the attention mechanism in and between
clusters of the join-graphs, which makes Attn-JGNN pay more attention to the
key variables and clusters in probabilistic inference, and reduces the
redundant calculation. Finally, our experiments show that our Attn-JGNN model
achieves better results than other neural network methods.

</details>


### [125] [GRATING: Low-Latency and Memory-Efficient Semantic Selection on Device](https://arxiv.org/abs/2510.15620)
*Jiahao Zhou,Chengliang Lin,Dingji Li,Mingkai Dong,Haibo Chen*

Main category: cs.LG

TL;DR: GRATING是一个无需训练的推理系统，通过渐进式聚类剪枝和内存优化技术，在保持精度的同时显著降低语义Top-K选择的延迟和内存使用。


<details>
  <summary>Details</summary>
Motivation: 语义Top-K选择在边缘设备上的延迟和内存需求占主导地位，而实际上只需要相对排名而非精确分数，且中间层存在序列级稀疏性，为剪枝提供了机会。

Method: 提出整体前向传播和GRATING系统，通过全局候选视图进行渐进式聚类剪枝，采用双层滑动窗口和分块执行策略重叠I/O与计算以限制峰值内存。

Result: 在0.6B到8B参数的reranker上，GRATING在微基准测试中延迟降低达89.0%，峰值内存降低达94.9%，在三个真实应用中延迟降低11.6%-51.0%，内存降低18.6%-77.8%。

Conclusion: GRATING通过利用相对排名和序列级稀疏性，显著提升了边缘设备上语义Top-K选择的效率和可部署性，且无需重新训练。

Abstract: Semantic top-K selection with cross-encoder rerankers underpins of on-device
AI services, such as retrieval-augmented generation, agent memory, and
personalized recommendation. However, its latency and memory demands dominate
end-to-end budgets on edge hardware. Revisiting the objective of top-K
selection, we reveal that only relative rankings matter, not exact
per-candidate scores. We further observe sequence-level sparsity: relative
rankings stabilize early in intermediate layers, allowing pruning opportunities
prior to completing full inference.
  Building on this insight, we propose monolithic forwarding and develop a
training-free inference system, GRATING. By maintaining a global view of all
candidates, it reduces latency through progressive cluster pruning. It also
bounds peak memory usage by strategically overlapping I/O with computation via
dual-layer sliding window and chunked execution. We evaluate GRATING against
state-of-the-art baselines on rerankers from 0.6B to 8B parameters across Apple
M2 and RTX 5070. GRATING consistently reduces latency by up to 89.0% and peak
memory by up to 94.9% in microbenchmarks, without any loss in precision. Across
three real-world on-device AI applications, GRATING lowers latency by
11.6%-51.0% and peak memory by 18.6%-77.8%, demonstrating substantial
improvements in efficiency and deployability.

</details>


### [126] [CQD-SHAP: Explainable Complex Query Answering via Shapley Values](https://arxiv.org/abs/2510.15623)
*Parsa Abbasi,Stefan Heindorf*

Main category: cs.LG

TL;DR: 提出了CQD-SHAP框架，使用Shapley值计算复杂查询中每个查询部分对特定答案排名的贡献，提高神经符号复杂查询回答的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的神经和神经符号复杂查询回答方法大多是黑盒模型，缺乏用户信任。虽然CQD等神经符号方法允许追踪中间结果，但无法解释查询不同部分的重要性。

Method: 基于合作博弈论中的Shapley值构建CQD-SHAP框架，计算每个查询部分对答案排名的贡献，满足所有基本Shapley公理。

Result: 在必要性和充分性解释方面的自动化评估显示，该方法对大多数查询类型都有效，优于各种基线方法。

Conclusion: CQD-SHAP能够有效解释神经预测器从不完整知识图谱中推断新知识的价值，相比仅依赖知识图谱中现有事实的符号方法具有更好的可解释性。

Abstract: Complex query answering (CQA) goes beyond the well-studied link prediction
task by addressing more sophisticated queries that require multi-hop reasoning
over incomplete knowledge graphs (KGs). Research on neural and neurosymbolic
CQA methods is still an emerging field. Almost all of these methods can be
regarded as black-box models, which may raise concerns about user trust.
Although neurosymbolic approaches like CQD are slightly more interpretable,
allowing intermediate results to be tracked, the importance of different parts
of the query remains unexplained. In this paper, we propose CQD-SHAP, a novel
framework that computes the contribution of each query part to the ranking of a
specific answer. This contribution explains the value of leveraging a neural
predictor that can infer new knowledge from an incomplete KG, rather than a
symbolic approach relying solely on existing facts in the KG. CQD-SHAP is
formulated based on Shapley values from cooperative game theory and satisfies
all the fundamental Shapley axioms. Automated evaluation of these explanations
in terms of necessary and sufficient explanations, and comparisons with various
baselines, shows the effectiveness of this approach for most query types.

</details>


### [127] [Decentralized Parameter-Free Online Learning](https://arxiv.org/abs/2510.15644)
*Tomas Ortega,Hamid Jafarkhani*

Main category: cs.LG

TL;DR: 提出了首个无需超参数调优的分布式在线学习算法，通过多智能体投币博弈和gossip步骤实现次线性网络遗憾。


<details>
  <summary>Details</summary>
Motivation: 解决分布式在线学习中需要手动调参的问题，为分布式感知、去中心化优化和协作机器学习等应用提供参数自由的解决方案。

Method: 结合多智能体投币博弈和gossip步骤，引入新的"投注函数"公式来简化多智能体遗憾分析。

Result: 实现了次线性网络遗憾边界，在合成和真实数据集上的实验验证了算法性能。

Conclusion: 该算法家族适用于分布式感知、去中心化优化和协作机器学习等应用场景。

Abstract: We propose the first parameter-free decentralized online learning algorithms
with network regret guarantees, which achieve sublinear regret without
requiring hyperparameter tuning. This family of algorithms connects multi-agent
coin-betting and decentralized online learning via gossip steps. To enable our
decentralized analysis, we introduce a novel "betting function" formulation for
coin-betting that simplifies the multi-agent regret analysis. Our analysis
shows sublinear network regret bounds and is validated through experiments on
synthetic and real datasets. This family of algorithms is applicable to
distributed sensing, decentralized optimization, and collaborative ML
applications.

</details>


### [128] [Deep Neural ODE Operator Networks for PDEs](https://arxiv.org/abs/2510.15651)
*Ziqian Li,Kang Liu,Yongcun Song,Hangrui Yue,Enrique Zuazua*

Main category: cs.LG

TL;DR: 提出NODE-ONet框架，结合神经ODE和算子学习，通过物理编码的神经ODE来提升PDE求解的泛化能力和数值效率


<details>
  <summary>Details</summary>
Motivation: 现有算子学习方法忽视PDE的领域知识，难以捕捉时间动态且在训练时间范围外泛化能力差

Method: 采用编码器-解码器架构：编码器空间离散化输入函数，神经ODE捕捉潜在时间动态，解码器重构物理空间解

Result: 在非线性扩散-反应和Navier-Stokes方程上展示了高精度、计算效率和在训练时间范围外的预测能力

Conclusion: 该框架具有适应不同编码器/解码器的灵活性，能跨相关PDE族泛化，是科学机器学习中可扩展的物理编码工具

Abstract: Operator learning has emerged as a promising paradigm for developing
efficient surrogate models to solve partial differential equations (PDEs).
However, existing approaches often overlook the domain knowledge inherent in
the underlying PDEs and hence suffer from challenges in capturing temporal
dynamics and generalization issues beyond training time frames. This paper
introduces a deep neural ordinary differential equation (ODE) operator network
framework, termed NODE-ONet, to alleviate these limitations. The framework
adopts an encoder-decoder architecture comprising three core components: an
encoder that spatially discretizes input functions, a neural ODE capturing
latent temporal dynamics, and a decoder reconstructing solutions in physical
spaces. Theoretically, error analysis for the encoder-decoder architecture is
investigated. Computationally, we propose novel physics-encoded neural ODEs to
incorporate PDE-specific physical properties. Such well-designed neural ODEs
significantly reduce the framework's complexity while enhancing numerical
efficiency, robustness, applicability, and generalization capacity. Numerical
experiments on nonlinear diffusion-reaction and Navier-Stokes equations
demonstrate high accuracy, computational efficiency, and prediction
capabilities beyond training time frames. Additionally, the framework's
flexibility to accommodate diverse encoders/decoders and its ability to
generalize across related PDE families further underscore its potential as a
scalable, physics-encoded tool for scientific machine learning.

</details>


### [129] [Fast and Compact Tsetlin Machine Inference on CPUs Using Instruction-Level Optimization](https://arxiv.org/abs/2510.15653)
*Yefan Zeng,Shengyu Duan,Rishad Shafik,Alex Yakovlev*

Main category: cs.LG

TL;DR: 提出了一种基于指令级位运算的Tsetlin机器高效软件实现，通过早期退出机制和文字重排序策略，在ARM处理器上实现了高达96.71%的推理时间减少。


<details>
  <summary>Details</summary>
Motivation: Tsetlin机器在资源受限设备上具有高速推理潜力，其逻辑驱动操作适合在现代CPU架构上并行执行，因此需要优化实现来充分利用这一特性。

Method: 采用指令级位运算进行紧凑模型表示和加速处理，引入基于AND子句评估的早期退出机制避免不必要计算，并通过统计分析的文字重排序策略最大化早期退出概率。

Result: 在ARM处理器上的实验结果显示，优化实现相比传统基于整数的TM实现减少了高达96.71%的推理时间，同时保持了相当的代码密度。

Conclusion: 提出的优化方法显著提升了Tsetlin机器的推理效率，使其更适合在资源受限的嵌入式设备上部署。

Abstract: The Tsetlin Machine (TM) offers high-speed inference on resource-constrained
devices such as CPUs. Its logic-driven operations naturally lend themselves to
parallel execution on modern CPU architectures. Motivated by this, we propose
an efficient software implementation of the TM by leveraging instruction-level
bitwise operations for compact model representation and accelerated processing.
To further improve inference speed, we introduce an early exit mechanism, which
exploits the TM's AND-based clause evaluation to avoid unnecessary
computations. Building upon this, we propose a literal Reorder strategy
designed to maximize the likelihood of early exits. This strategy is applied
during a post-training, pre-inference stage through statistical analysis of all
literals and the corresponding actions of their associated Tsetlin Automata
(TA), introducing negligible runtime overhead. Experimental results using the
gem5 simulator with an ARM processor show that our optimized implementation
reduces inference time by up to 96.71% compared to the conventional
integer-based TM implementations while maintaining comparable code density.

</details>


### [130] [WARP-LUTs - Walsh-Assisted Relaxation for Probabilistic Look Up Tables](https://arxiv.org/abs/2510.15655)
*Lino Gerlach,Liv Våge,Thore Gerlach,Elliott Kauffman*

Main category: cs.LG

TL;DR: 提出WARP-LUTs方法，通过Walsh变换辅助的概率查找表，显著减少可训练参数数量，在CIFAR-10上比DLGNs收敛更快且保持相近精度。


<details>
  <summary>Details</summary>
Motivation: 现有乘法自由模型如DLGNs在训练时计算成本高，且对多输入逻辑块泛化能力差，需要更高效的梯度学习方法。

Method: 使用Walsh变换辅助的概率查找表方法，通过梯度学习逻辑门组合，大幅减少可训练参数。

Result: 在CIFAR-10上实现比DLGNs显著更快的收敛速度，同时保持可比较的准确率。

Conclusion: WARP-LUTs为现代FPGA上的高效部署和实时科学应用提供了潜力，可扩展到更高输入逻辑块。

Abstract: Fast and efficient machine learning is of growing interest to the scientific
community and has spurred significant research into novel model architectures
and hardware-aware design. Recent hard? and software co-design approaches have
demonstrated impressive results with entirely multiplication-free models.
Differentiable Logic Gate Networks (DLGNs), for instance, provide a
gradient-based framework for learning optimal combinations of low-level logic
gates, setting state-of-the-art trade-offs between accuracy, resource usage,
and latency. However, these models suffer from high computational cost during
training and do not generalize well to logic blocks with more inputs. In this
work, we introduce Walsh-Assisted Relaxation for Probabilistic Look-Up Tables
(WARP-LUTs) - a novel gradient-based method that efficiently learns
combinations of logic gates with substantially fewer trainable parameters. We
demonstrate that WARP-LUTs achieve significantly faster convergence on CIFAR-10
compared to DLGNs, while maintaining comparable accuracy. Furthermore, our
approach suggests potential for extension to higher-input logic blocks,
motivating future research on extremely efficient deployment on modern FPGAs
and its real-time science applications.

</details>


### [131] [CarBoN: Calibrated Best-of-N Sampling Improves Test-time Reasoning](https://arxiv.org/abs/2510.15674)
*Yung-Chen Tang,Pin-Yu Chen,Andrea Cavallaro*

Main category: cs.LG

TL;DR: CarBoN是一种测试时校准框架，通过自适应调整温度参数和偏移向量来引导语言模型生成更可靠的推理路径，在减少采样次数的同时提高推理任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时扩展方法（如Best-of-N采样）在N增加时存在收益递减问题，效率较低。需要一种无需重新训练大语言模型就能提高推理性能的方法。

Method: 提出两阶段方法CarBoN：先探索解空间，然后学习输入特定的温度T和加性偏移向量δ来校准logits，引导生成更可靠的推理路径。

Result: 在MATH-500和AIME-2024上的实验表明，CarBoN效率显著提升，达到相同准确率所需的采样次数减少4倍，且在固定预算下通常能获得更高准确率。

Conclusion: CarBoN框架有效提高了测试时推理效率，温度T和偏移向量δ在平衡输出多样性和正确性方面发挥互补作用，该框架也可推广到束搜索等步骤级采样策略。

Abstract: Allocating more computation during inference time (test-time scaling)
improves language model performance, especially for reasoning tasks. However,
popular methods like Best-of-$N$ sampling often show diminishing returns as $N$
increases. To address this inefficiency, we introduce a general test-time
calibration framework that adaptively modifies the model toward high-reward
reasoning paths, with theoretical guarantees of improving the lower bound of
expected reward under finite sampling, all without large language model (LLM)
retraining. Within this framework, we propose CarBoN (Calibrated Best-of-$N$),
a two-phase method that first explores the solution space and then learns a
calibration of the logits via an input-specific temperature $T$ and additive
shift vector $\delta$, guiding generation toward more reliable reasoning.
Experiments on MATH-500 and AIME-2024 show that CarBoN improves efficiency,
with up to $4\times$ fewer rollouts to reach the same accuracy, while often
achieving higher accuracy under fixed budgets. We also analyze the
complementary roles of $T$ and $\delta$ in balancing output diversity and
correctness, and demonstrate that the framework also generalizes to step-level
sampling strategies such as beam search. For more information, please refer to
our project page at huggingface.co/spaces/TrustSafeAI/Test-Time-Calibration.

</details>


### [132] [KS-Net: Multi-layer network model for determining the rotor type from motor parameters in interior PMSMs](https://arxiv.org/abs/2510.15688)
*Kivanc Dogan,Ahmet Orhan*

Main category: cs.LG

TL;DR: 本研究使用机器学习方法对IPMSM转子形状进行分类，开发了KS-Net深度学习模型，并与传统算法比较，实现了接近100%的分类准确率，为电机设计提供了快速替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统有限元方法分析IPMSM转子形状计算成本高，需要开发更高效的数据驱动方法作为替代方案。

Method: 使用自定义深度学习模型KS-Net，并与Cubic SVM、Quadratic SVM、Fine KNN、Cosine KNN、Fine Tree等算法对比，采用9000样本的平衡数据集和10折交叉验证。

Result: Cubic SVM和Quadratic SVM实现100%准确率，KS-Net达到99.98%准确率（仅2个误分类），所有模型性能指标均表现优异。

Conclusion: 数据驱动方法能够高精度预测IPMSM转子形状，为电机设计过程加速、自动化转子识别系统和数据驱动故障诊断提供了可行方案。

Abstract: The demand for high efficiency and precise control in electric drive systems
has led to the widespread adoption of Interior Permanent Magnet Synchronous
Motors (IPMSMs). The performance of these motors is significantly influenced by
rotor geometry. Traditionally, rotor shape analysis has been conducted using
the finite element method (FEM), which involves high computational costs. This
study aims to classify the rotor shape (2D type, V type, Nabla type) of IPMSMs
using electromagnetic parameters through machine learning-based methods and to
demonstrate the applicability of this approach as an alternative to classical
methods. In this context, a custom deep learning model, KS-Net, developed by
the user, was comparatively evaluated against Cubic SVM, Quadratic SVM, Fine
KNN, Cosine KNN, and Fine Tree algorithms. The balanced dataset, consisting of
9,000 samples, was tested using 10-fold cross-validation, and performance
metrics such as accuracy, precision, recall, and F1-score were employed. The
results indicate that the Cubic SVM and Quadratic SVM algorithms classified all
samples flawlessly, achieving 100% accuracy, while the KS-Net model achieved
99.98% accuracy with only two misclassifications, demonstrating competitiveness
with classical methods. This study shows that the rotor shape of IPMSMs can be
predicted with high accuracy using data-driven approaches, offering a fast and
cost-effective alternative to FEM-based analyses. The findings provide a solid
foundation for accelerating motor design processes, developing automated rotor
identification systems, and enabling data-driven fault diagnosis in engineering
applications.

</details>


### [133] [Constrained Adversarial Perturbation](https://arxiv.org/abs/2510.15699)
*Virendra Nishad,Bhaskar Mukhoty,Hilal AlQuabeh,Sandeep K. Shukla,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 提出了CAP方法，通过增强拉格朗日优化在约束特征空间中生成通用对抗扰动，在保持领域特定约束的同时提高攻击成功率


<details>
  <summary>Details</summary>
Motivation: 现有通用对抗扰动方法忽视了领域特定的特征关系约束，导致生成的对抗样本不切实际或容易被检测，限制了实际应用

Method: 基于增强拉格朗日的min-max优化问题，使用梯度交替优化策略的CAP算法，可处理多个复杂约束

Result: 在金融、IT网络和网络物理系统等领域的评估显示，CAP实现了更高的攻击成功率，同时显著减少运行时间

Conclusion: CAP方法有效解决了约束特征空间中的对抗攻击问题，并提出了从数据中学习特征约束的原则性方法

Abstract: Deep neural networks have achieved remarkable success in a wide range of
classification tasks. However, they remain highly susceptible to adversarial
examples - inputs that are subtly perturbed to induce misclassification while
appearing unchanged to humans. Among various attack strategies, Universal
Adversarial Perturbations (UAPs) have emerged as a powerful tool for both
stress testing model robustness and facilitating scalable adversarial training.
Despite their effectiveness, most existing UAP methods neglect domain specific
constraints that govern feature relationships. Violating such constraints, such
as debt to income ratios in credit scoring or packet flow invariants in network
communication, can render adversarial examples implausible or easily
detectable, thereby limiting their real world applicability.
  In this work, we advance universal adversarial attacks to constrained feature
spaces by formulating an augmented Lagrangian based min max optimization
problem that enforces multiple, potentially complex constraints of varying
importance. We propose Constrained Adversarial Perturbation (CAP), an efficient
algorithm that solves this problem using a gradient based alternating
optimization strategy. We evaluate CAP across diverse domains including
finance, IT networks, and cyber physical systems, and demonstrate that it
achieves higher attack success rates while significantly reducing runtime
compared to existing baselines. Our approach also generalizes seamlessly to
individual adversarial perturbations, where we observe similar strong
performance gains. Finally, we introduce a principled procedure for learning
feature constraints directly from data, enabling broad applicability across
domains with structured input spaces.

</details>


### [134] [ProofOptimizer: Training Language Models to Simplify Proofs without Human Demonstrations](https://arxiv.org/abs/2510.15700)
*Alex Gu,Bartosz Piotrowski,Fabian Gloeckle,Kaiyu Yang,Aram H. Markosyan*

Main category: cs.LG

TL;DR: ProofOptimizer是一个通过专家迭代和强化学习训练的语言模型，专门用于简化Lean证明，无需人工监督，能够显著压缩证明长度并提高验证效率。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络定理证明生成的证明过于冗长，难以被人类理解，限制了数学洞察力。证明简化成为关键瓶颈，但训练数据稀缺，现有方法难以处理RL训练证明器生成的极长证明。

Method: 通过专家迭代和强化学习训练ProofOptimizer语言模型，使用Lean验证简化并提供训练信号。在推理时采用迭代证明缩短工作流程，逐步减少证明长度。

Result: 在标准基准测试中，ProofOptimizer显著压缩了最先进RL训练证明器生成的证明：miniF2F减少87%，PutnamBench减少57%，Seed-Prover的IMO 2025证明减少49%。简化后的证明在Lean中验证更快，作为训练数据还能提升下游证明器性能。

Conclusion: ProofOptimizer是首个无需人工监督就能简化Lean证明的语言模型，通过强化学习训练，能有效解决证明冗长问题，提高证明的可理解性和实用性。

Abstract: Neural theorem proving has advanced rapidly in the past year, reaching IMO
gold-medalist capabilities and producing formal proofs that span thousands of
lines. Although such proofs are mechanically verified by formal systems like
Lean, their excessive length renders them difficult for humans to comprehend
and limits their usefulness for mathematical insight. Proof simplification is
therefore a critical bottleneck. Yet, training data for this task is scarce,
and existing methods -- mainly agentic scaffolding with off-the-shelf LLMs --
struggle with the extremely long proofs generated by RL-trained provers. We
introduce ProofOptimizer, the first language model trained to simplify Lean
proofs without requiring additional human supervision. ProofOptimizer is
trained via expert iteration and reinforcement learning, using Lean to verify
simplifications and provide training signal. At inference time, it operates
within an iterative proof-shortening workflow, progressively reducing proof
length. Experiments show that ProofOptimizer substantially compresses proofs
generated by state-of-the-art RL-trained provers on standard benchmarks,
reducing proof length by 87% on miniF2F, 57% on PutnamBench, and 49% on
Seed-Prover's IMO 2025 proofs. Beyond conciseness, the simplified proofs check
faster in Lean and further improve downstream prover performance when reused as
training data for supervised finetuning.

</details>


### [135] [ProSh: Probabilistic Shielding for Model-free Reinforcement Learning](https://arxiv.org/abs/2510.15720)
*Edwin Hamel-De le Court,Gaspard Ohlmann,Francesco Belardinelli*

Main category: cs.LG

TL;DR: ProSh是一种模型无关的安全强化学习算法，通过风险预算增强状态空间，并使用学习到的成本评论家对策略分布应用屏蔽，确保所有采样动作在期望上保持安全。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中的安全问题，开发不仅性能最优且部署安全的RL系统，提供关于安全性的形式化保证。

Method: 将约束MDP状态空间通过风险预算进行增强，使用学习到的成本评论家对智能体策略分布应用屏蔽机制，确保采样动作的安全性。

Result: 在确定性环境中保持最优性，提供仅依赖于备份评论家精度的成本期望上界，实验证明在训练期间也能保证安全性。

Conclusion: ProSh是一种有效的模型无关安全强化学习方法，能够在训练和部署阶段提供形式化的安全保障。

Abstract: Safety is a major concern in reinforcement learning (RL): we aim at
developing RL systems that not only perform optimally, but are also safe to
deploy by providing formal guarantees about their safety. To this end, we
introduce Probabilistic Shielding via Risk Augmentation (ProSh), a model-free
algorithm for safe reinforcement learning under cost constraints. ProSh
augments the Constrained MDP state space with a risk budget and enforces safety
by applying a shield to the agent's policy distribution using a learned cost
critic. The shield ensures that all sampled actions remain safe in expectation.
We also show that optimality is preserved when the environment is
deterministic. Since ProSh is model-free, safety during training depends on the
knowledge we have acquired about the environment. We provide a tight
upper-bound on the cost in expectation, depending only on the backup-critic
accuracy, that is always satisfied during training. Under mild, practically
achievable assumptions, ProSh guarantees safety even at training time, as shown
in the experiments.

</details>


### [136] [RLAF: Reinforcement Learning from Automaton Feedback](https://arxiv.org/abs/2510.15728)
*Mahyar Alinejad,Alvaro Velasquez,Yue Wang,George Atia*

Main category: cs.LG

TL;DR: 提出了一种基于自动机反馈的强化学习方法，用DFA生成的轨迹偏好替代显式奖励函数，无需手动设计奖励，在具有时间依赖性的任务中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在处理具有复杂历史依赖奖励结构的环境时面临挑战，需要手动设计奖励函数，这既困难又低效。

Method: 使用确定性有限自动机(DFA)生成轨迹偏好来学习奖励函数，包含静态方法（直接使用学习到的奖励函数进行策略优化）和动态方法（通过迭代更新持续优化奖励函数和策略）。

Result: 在离散和连续环境中的实验表明，该方法能够学习处理时间依赖性任务的有效策略，优于传统奖励工程和基于自动机的基线方法。

Conclusion: 基于自动机的偏好方法在处理非马尔可夫奖励方面具有优势，提供了可扩展、高效且无需人工干预的奖励建模替代方案，并具有收敛保证。

Abstract: Reinforcement Learning (RL) in environments with complex, history-dependent
reward structures poses significant challenges for traditional methods. In this
work, we introduce a novel approach that leverages automaton-based feedback to
guide the learning process, replacing explicit reward functions with
preferences derived from a deterministic finite automaton (DFA). Unlike
conventional approaches that use automata for direct reward specification, our
method employs the structure of the DFA to generate preferences over
trajectories that are used to learn a reward function, eliminating the need for
manual reward engineering. Our framework introduces a static approach that uses
the learned reward function directly for policy optimization and a dynamic
approach that involves continuous refining of the reward function and policy
through iterative updates until convergence.
  Our experiments in both discrete and continuous environments demonstrate that
our approach enables the RL agent to learn effective policies for tasks with
temporal dependencies, outperforming traditional reward engineering and
automaton-based baselines such as reward machines and LTL-guided methods. Our
results highlight the advantages of automaton-based preferences in handling
non-Markovian rewards, offering a scalable, efficient, and human-independent
alternative to traditional reward modeling. We also provide a convergence
guarantee showing that under standard assumptions our automaton-guided
preference-based framework learns a policy that is near-optimal with respect to
the true non-Markovian objective.

</details>


### [137] [A Comprehensive Evaluation of Graph Neural Networks and Physics Informed Learning for Surrogate Modelling of Finite Element Analysis](https://arxiv.org/abs/2510.15750)
*Nayan Kumar Singh*

Main category: cs.LG

TL;DR: 本文评估了图神经网络和3D U-Net作为有限元分析替代模型的效果，发现GNN显著优于U-Net，其中MPNN和Graph Transformer表现最佳。物理信息神经网络框架和课程学习策略能显著提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 有限元分析计算成本高，不适合设计优化问题，需要开发准确高效的深度学习替代模型。

Method: 使用图神经网络和3D U-Net作为FEA替代模型，引入基于Navier-Cauchy方程的物理信息神经网络框架，采用课程学习策略（数据预训练+物理信息微调）。

Result: GNN显著优于U-Net，MPNN和Graph Transformer分别达到3.5%和2.6%的相对L2误差。PINN框架将误差降低达11.3%，MPNN PINN在性能和推理速度间达到最佳平衡。

Conclusion: 图神经网络是有效的FEA替代方案，物理信息神经网络能显著提升泛化能力，MPNN PINN提供了最实用的解决方案。

Abstract: Although Finite Element Analysis (FEA) is an integral part of the product
design lifecycle, the analysis is computationally expensive, making it
unsuitable for many design optimization problems. The deep learning models can
be a great solution. However, selecting the architecture that emulates the FEA
with great accuracy is a challenge. This paper presents a comprehensive
evaluation of graph neural networks (GNNs) and 3D U-Nets as surrogates for FEA
of parametric I-beams. We introduce a Physics-Informed Neural Network (PINN)
framework, governed by the Navier Cauchy equations, to enforce physical laws.
Crucially, we demonstrate that a curriculum learning strategy, pretraining on
data followed by physics informed fine tuning, is essential for stabilizing
training. Our results show that GNNs fundamentally outperform the U-Net. Even
the worst performer among GNNs, the GCN framework, achieved a relative L2 error
of 8.7% while the best framework among U Net, U Net with attention mechanism
trained on high resolution data, achieved 13.0% score. Among the graph-based
architectures, the Message Passing Neural Networks (MPNN) and Graph
Transformers achieved the highest accuracy, achieving a relative L2 score of
3.5% and 2.6% respectively. The inclusion of physics fundamental laws (PINN)
significantly improved the generalization, reducing error by up to 11.3% on
high-signal tasks. While the Graph Transformer is the most accurate model, it
is more 37.5% slower during inference when compared to second best model, MPNN
PINN. The PINN enhanced MPNN (MPNN PINN) provides the most practical solution.
It offers a good compromise between predictive performance, model size, and
inference speed.

</details>


### [138] [SAMix: Calibrated and Accurate Continual Learning via Sphere-Adaptive Mixup and Neural Collapse](https://arxiv.org/abs/2510.15751)
*Trung-Anh Dang,Vincent Nguyen,Ngoc-Son Vu,Christel Vrain*

Main category: cs.LG

TL;DR: 提出Sphere-Adaptive Mixup (SAMix)方法，通过自适应混合策略增强持续学习中的网络校准和性能，减少过自信和遗忘，提高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法主要关注减轻遗忘和提高准确率，但忽视了网络校准的重要性。神经崩溃现象在持续学习中具有优势，但很少有工作致力于改进持续模型的校准以实现更可靠的预测。

Method: 提出Sphere-Adaptive Mixup (SAMix)，一种针对基于神经崩溃方法的自适应混合策略。该方法根据神经崩溃下特征空间的几何特性调整混合过程，确保更稳健的正则化和对齐。

Result: 实验表明，SAMix显著提升了性能，在持续学习中超越了最先进方法，同时改进了模型校准。该方法提高了跨任务准确率和预测的可靠性。

Conclusion: SAMix是持续学习系统稳健性方面的一个有前景的进展，能够同时增强校准和性能。

Abstract: While most continual learning methods focus on mitigating forgetting and
improving accuracy, they often overlook the critical aspect of network
calibration, despite its importance. Neural collapse, a phenomenon where
last-layer features collapse to their class means, has demonstrated advantages
in continual learning by reducing feature-classifier misalignment. Few works
aim to improve the calibration of continual models for more reliable
predictions. Our work goes a step further by proposing a novel method that not
only enhances calibration but also improves performance by reducing
overconfidence, mitigating forgetting, and increasing accuracy. We introduce
Sphere-Adaptive Mixup (SAMix), an adaptive mixup strategy tailored for neural
collapse-based methods. SAMix adapts the mixing process to the geometric
properties of feature spaces under neural collapse, ensuring more robust
regularization and alignment. Experiments show that SAMix significantly boosts
performance, surpassing SOTA methods in continual learning while also improving
model calibration. SAMix enhances both across-task accuracy and the broader
reliability of predictions, making it a promising advancement for robust
continual learning systems.

</details>


### [139] [Poultry Farm Intelligence: An Integrated Multi-Sensor AI Platform for Enhanced Welfare and Productivity](https://arxiv.org/abs/2510.15757)
*Pieris Panagi,Savvas Karatsiolis,Kyriacos Mosphilis,Nicholas Hadjisavvas,Andreas Kamilaris,Nicolas Nicolaou,Efstathios Stavrakis,Vassilis Vassiliades*

Main category: cs.LG

TL;DR: PoultryFI是一个模块化、经济高效的AI平台，集成了六个AI模块，通过优化摄像头布局、音视频监控、实时鸡蛋计数和预测分析，为中小型家禽养殖场提供全面的智能监控和决策支持。


<details>
  <summary>Details</summary>
Motivation: 中小型家禽养殖场缺乏经济实惠的集成工具进行持续监控和决策，主要依赖手动、反应式检查，难以同时满足生产力目标和动物福利要求。

Method: 使用进化算法优化摄像头布局实现全覆盖，集成音视频监控提取福利指标，边缘视觉模型实现实时鸡蛋计数，预测模型预测产量和饲料消耗，推荐模块结合天气预报指导环境调整。

Result: 现场试验显示在树莓派5上实现100%鸡蛋计数准确率，强大的异常检测能力和可靠的短期预测性能。

Conclusion: PoultryFI填补了孤立试点工具与可扩展农场智能之间的差距，使生产者能够主动保障动物福利和盈利能力。

Abstract: Poultry farming faces increasing pressure to meet productivity targets while
ensuring animal welfare and environmental compliance. Yet many small and
medium-sized farms lack affordable, integrated tools for continuous monitoring
and decision-making, relying instead on manual, reactive inspections. This
paper presents Poultry Farm Intelligence (PoultryFI) - a modular,
cost-effective platform that integrates six AI-powered modules: Camera
Placement Optimizer, Audio-Visual Monitoring, Analytics & Alerting, Real-Time
Egg Counting, Production & Profitability Forecasting, and a Recommendation
Module.
  Camera layouts are first optimized offline using evolutionary algorithms for
full poultry house coverage with minimal hardware. The Audio-Visual Monitoring
module extracts welfare indicators from synchronized video, audio, and feeding
data. Analytics & Alerting produces daily summaries and real-time
notifications, while Real-Time Egg Counting uses an edge vision model to
automate production tracking. Forecasting models predict egg yield and feed
consumption up to 10 days in advance, and the Recommendation Module integrates
forecasts with weather data to guide environmental and operational adjustments.
  This is among the first systems to combine low-cost sensing, edge analytics,
and prescriptive AI to continuously monitor flocks, predict production, and
optimize performance. Field trials demonstrate 100% egg-count accuracy on
Raspberry Pi 5, robust anomaly detection, and reliable short-term forecasting.
PoultryFI bridges the gap between isolated pilot tools and scalable, farm-wide
intelligence, empowering producers to proactively safeguard welfare and
profitability.

</details>


### [140] [Cavity Duplexer Tuning with 1d Resnet-like Neural Networks](https://arxiv.org/abs/2510.15796)
*Anton Raskovalov*

Main category: cs.LG

TL;DR: 提出一种用于调谐具有大量调节螺钉的腔体双工器的机器学习方法，使用监督学习替代强化学习，通过1D ResNet架构处理S参数信息，能在4-5次旋转内接近调谐状态


<details>
  <summary>Details</summary>
Motivation: 针对具有大量调节螺钉的腔体双工器调谐问题，传统强化学习方法效果不佳，需要更高效的自动化调谐方案

Method: 采用监督学习框架，构建包含1D ResNet骨干网络和S参数曲线形状、峰值位置与幅度等附加信息处理的神经网络架构，配合外部控制算法

Result: 该神经网络系统能够在每个螺钉4-5次旋转内使双工器达到接近调谐状态

Conclusion: 所提出的监督学习方法相比传统强化学习更有效，能够快速实现腔体双工器的自动化调谐

Abstract: This paper presents machine learning method for tuning of cavity duplexer
with a large amount of adjustment screws. After testing we declined
conventional reinforcement learning approach and reformulated our task in the
supervised learning setup. The suggested neural network architecture includes
1d ResNet-like backbone and processing of some additional information about
S-parameters, like the shape of curve and peaks positions and amplitudes. This
neural network with external control algorithm is capable to reach almost the
tuned state of the duplexer within 4-5 rotations per screw.

</details>


### [141] [AB-UPT for Automotive and Aerospace Applications](https://arxiv.org/abs/2510.15808)
*Benedikt Alkin,Richard Kurle,Louis Serrano,Dennis Just,Johannes Brandstetter*

Main category: cs.LG

TL;DR: AB-UPT模型在汽车和飞机CFD模拟中表现优异，比传统数值求解器计算量小几个数量级，能在单GPU上一天内完成训练，实现工业级应用。


<details>
  <summary>Details</summary>
Motivation: 扩展AB-UPT模型的应用范围，验证其在更多实际工程场景中的有效性，特别是汽车和飞机空气动力学模拟。

Method: 使用Luminary Cloud平台生成高质量数据集（SHIFT-SUV和SHIFT-Wing），在AB-UPT模型上进行训练和评估，并与现有transformer基线模型比较。

Result: AB-UPT在两个数据集上都优于之前的state-of-the-art transformer基线模型，能够秒级准确预测空气动力，训练效率高。

Conclusion: AB-UPT展示了强大的工业应用潜力，为大规模CFD模拟提供了高效准确的神经代理解决方案。

Abstract: The recently proposed Anchored-Branched Universal Physics Transformers
(AB-UPT) shows strong capabilities to replicate automotive computational fluid
dynamics simulations requiring orders of magnitudes less compute than
traditional numerical solvers. In this technical report, we add two new
datasets to the body of empirically evaluated use-cases of AB-UPT, combining
high-quality data generation with state-of-the-art neural surrogates. Both
datasets were generated with the Luminary Cloud platform containing automotives
(SHIFT-SUV) and aircrafts (SHIFT-Wing). We start by detailing the data
generation. Next, we show favorable performances of AB-UPT against previous
state-of-the-art transformer-based baselines on both datasets, followed by
extensive qualitative and quantitative evaluations of our best AB-UPT model.
AB-UPT shows strong performances across the board. Notably, it obtains near
perfect prediction of integrated aerodynamic forces within seconds from a
simple isotopically tesselate geometry representation and is trainable within a
day on a single GPU, paving the way for industry-scale applications.

</details>


### [142] [Chronos-2: From Univariate to Universal Forecasting](https://arxiv.org/abs/2510.15821)
*Abdul Fatir Ansari,Oleksandr Shchur,Jaris Küken,Andreas Auer,Boran Han,Pedro Mercado,Syama Sundar Rangapuram,Huibin Shen,Lorenzo Stella,Xiyuan Zhang,Mononito Goswami,Shubham Kapoor,Danielle C. Maddix,Pablo Guerron,Tony Hu,Junming Yin,Nick Erickson,Prateek Mutalik Desai,Hao Wang,Huzefa Rangwala,George Karypis,Yuyang Wang,Michael Bohlke-Schneider*

Main category: cs.LG

TL;DR: Chronos-2是一个预训练的时间序列模型，能够以零样本方式处理单变量、多变量和协变量预测任务，通过组注意力机制实现上下文学习，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有预训练时间序列模型主要关注单变量预测，限制了在现实世界多变量数据和协变量场景中的适用性。

Method: 使用组注意力机制促进跨多个时间序列的上下文学习，通过在合成数据集上训练来学习多样的多变量结构。

Result: 在fev-bench、GIFT-Eval和Chronos Benchmark II三个基准测试中达到最先进性能，在涉及协变量的任务中显著优于基线模型。

Conclusion: Chronos-2的上下文学习能力使其成为可在现实世界预测管道中直接使用的通用预测模型。

Abstract: Pretrained time series models have enabled inference-only forecasting systems
that produce accurate predictions without task-specific training. However,
existing approaches largely focus on univariate forecasting, limiting their
applicability in real-world scenarios where multivariate data and covariates
play a crucial role. We present Chronos-2, a pretrained model capable of
handling univariate, multivariate, and covariate-informed forecasting tasks in
a zero-shot manner. Chronos-2 employs a group attention mechanism that
facilitates in-context learning (ICL) through efficient information sharing
across multiple time series within a group, which may represent sets of related
series, variates of a multivariate series, or targets and covariates in a
forecasting task. These general capabilities are achieved through training on
synthetic datasets that impose diverse multivariate structures on univariate
series. Chronos-2 delivers state-of-the-art performance across three
comprehensive benchmarks: fev-bench, GIFT-Eval, and Chronos Benchmark II. On
fev-bench, which emphasizes multivariate and covariate-informed forecasting,
Chronos-2's universal ICL capabilities lead to substantial improvements over
existing models. On tasks involving covariates, it consistently outperforms
baselines by a wide margin. Case studies in the energy and retail domains
further highlight its practical advantages. The in-context learning
capabilities of Chronos-2 establish it as a general-purpose forecasting model
that can be used "as is" in real-world forecasting pipelines.

</details>


### [143] [SNOO: Step-K Nesterov Outer Optimizer - The Surprising Effectiveness of Nesterov Momentum Applied to Pseudo-Gradients](https://arxiv.org/abs/2510.15830)
*Dominik Kallusky,Vinay Rao,Vishal Nandavanam,Hao-Jun Michael Shi*

Main category: cs.LG

TL;DR: 论文提出SNOO优化器，通过在Lookahead框架中对伪梯度应用Nesterov动量，在非分布式设置下实现1.5-2.5倍的计算效率提升。


<details>
  <summary>Details</summary>
Motivation: DiLoCo优化器在非分布式设置中表现优异，研究发现其有效性主要源于对伪梯度应用Nesterov动量，这启发了SNOO的开发。

Method: 在Lookahead双循环框架中，对伪梯度应用Nesterov动量，形成Step-K Nesterov Outer Optimizer (SNOO)，兼容AdamW、Muon等内部优化器。

Result: 在高达1e23训练FLOPs的规模下，SNOO在非分布式设置中实现1.5-2.5倍计算效率提升，且模型越大提升越明显。

Conclusion: SNOO因其最小的计算和内存开销以及与模型分片的兼容性，成为各种内部优化器的实用增强方案。

Abstract: The rapid development of large language models (LLMs) has driven the demand
for more efficient optimization techniques. Among these, the Lookahead family
of optimizers employs a two-loop framework, maintaining fast and slow sets of
model weights. Multiple inner optimizer steps on the fast weights produce a
trajectory - the pseudo-gradient - that is used to update the slow weights.
DiLoCo, a notable example originally designed for distributed training, applies
Nesterov momentum to the averaged pseudo-gradient from multiple workers,
claiming to even outperform AdamW in a non-distributed setup. In this paper, we
empirically show that DiLoCo's surprising effectiveness stems primarily from
applying Nesterov momentum to the pseudo-gradient, which improves training in a
non-distributed setting. We call this Lookahead variant the Step-$K$ Nesterov
Outer Optimizer (SNOO). We demonstrate that SNOO achieves compute factor gains
of 1.5 - 2.5$\times$ in a non-distributed setting up to a scale of 1e23
training FLOPs, with improvements that increase with model size. Because of its
minimal compute and memory overhead and compatibility with model sharding, SNOO
is a practical enhancement for a variety of inner optimizers, including AdamW
and Muon.

</details>


### [144] [FIDDLE: Reinforcement Learning for Quantum Fidelity Enhancement](https://arxiv.org/abs/2510.15833)
*Hoang M. Ngo,Tamer Kahveci,My T. Thai*

Main category: cs.LG

TL;DR: 提出了FIDDLE框架，首个直接最大化量子电路过程保真度的学习框架，包含高斯过程代理模型和强化学习路由优化模块，显著提升量子电路可靠性


<details>
  <summary>Details</summary>
Motivation: 当前量子设备受噪声影响，可靠性降低。在门基量子计算中，提高量子电路在编译过程中（特别是路由阶段）的过程保真度是关键挑战

Method: FIDDLE框架包含两个模块：基于高斯过程的代理模型（用有限训练样本估计过程保真度）和强化学习模块（优化路由），是首个直接最大化过程保真度的方法

Result: 代理模型比现有学习技术提供更好的过程保真度估计，端到端框架在各种噪声模型下显著提高量子电路的过程保真度，优于依赖电路深度或门数等间接指标的传统方法

Conclusion: FIDDLE框架有效解决了路由阶段保真度最大化问题，为噪声量子设备上的可靠量子计算提供了新途径

Abstract: Quantum computing has the potential to revolutionize fields like quantum
optimization and quantum machine learning. However, current quantum devices are
hindered by noise, reducing their reliability. A key challenge in gate-based
quantum computing is improving the reliability of quantum circuits, measured by
process fidelity, during the transpilation process, particularly in the routing
stage. In this paper, we address the Fidelity Maximization in Routing Stage
(FMRS) problem by introducing FIDDLE, a novel learning framework comprising two
modules: a Gaussian Process-based surrogate model to estimate process fidelity
with limited training samples and a reinforcement learning module to optimize
routing. Our approach is the first to directly maximize process fidelity,
outperforming traditional methods that rely on indirect metrics such as circuit
depth or gate count. We rigorously evaluate FIDDLE by comparing it with
state-of-the-art fidelity estimation techniques and routing optimization
methods. The results demonstrate that our proposed surrogate model is able to
provide a better estimation on the process fidelity compared to existing
learning techniques, and our end-to-end framework significantly improves the
process fidelity of quantum circuits across various noise models.

</details>


### [145] [Transfer Orthology Networks](https://arxiv.org/abs/2510.15837)
*Vikash Singh*

Main category: cs.LG

TL;DR: 提出TRON（转移同源网络），一种利用同源关系进行跨物种转移学习的神经网络架构，通过物种转换层将源物种的基因表达映射到目标物种基因空间。


<details>
  <summary>Details</summary>
Motivation: 解决跨物种转录组数据利用不足的问题，通过生物基础的同源关系指导知识转移，提高跨物种预测效果。

Method: 构建物种间的二分图表示同源关系，在预训练前馈神经网络前添加物种转换层，其权重由二分图的邻接矩阵掩码，学习源物种到目标物种基因表达的线性变换。

Result: TRON提供了一种生物学基础且可解释的跨物种转移学习方法，转换层的权重可用于解释功能同源性。

Conclusion: TRON为更有效利用转录组数据铺平了道路，目前正在收集跨物种数据以进行实验验证。

Abstract: We present Transfer Orthology Networks (TRON), a novel neural network
architecture designed for cross-species transfer learning. TRON leverages
orthologous relationships, represented as a bipartite graph between species, to
guide knowledge transfer. Specifically, we prepend a learned species conversion
layer, whose weights are masked by the biadjacency matrix of this bipartite
graph, to a pre-trained feedforward neural network that predicts a phenotype
from gene expression data in a source species. This allows for efficient
transfer of knowledge to a target species by learning a linear transformation
that maps gene expression from the source to the target species' gene space.
The learned weights of this conversion layer offer a potential avenue for
interpreting functional orthology, providing insights into how genes across
species contribute to the phenotype of interest. TRON offers a biologically
grounded and interpretable approach to cross-species transfer learning, paving
the way for more effective utilization of available transcriptomic data. We are
in the process of collecting cross-species transcriptomic/phenotypic data to
gain experimental validation of the TRON architecture.

</details>


### [146] [Learning Correlated Reward Models: Statistical Barriers and Opportunities](https://arxiv.org/abs/2510.15839)
*Yeshwanth Cherapanamjeri,Constantinos Daskalakis,Gabriele Farina,Sobhan Mohammadpour*

Main category: cs.LG

TL;DR: 本文研究了学习相关probit模型的统计和计算挑战，发现传统的成对偏好数据不足以学习相关性信息，而三选一偏好数据能够克服这一限制，并提出了一种高效估计器。


<details>
  <summary>Details</summary>
Motivation: 随机效用模型在人类反馈强化学习中很重要，但许多技术依赖无关选项独立性假设，这限制了人类偏好的精细建模。避免该假设的模型缺乏统计和计算保证。

Method: 研究了相关probit模型的学习，比较了成对偏好数据和三选一偏好数据的有效性，开发了统计和计算高效的估计器。

Result: 证明成对偏好数据无法学习相关性信息，而三选一偏好数据能够实现这一目标，提出的估计器具有接近最优的性能。

Conclusion: 高阶偏好数据在学习相关效用方面具有优势，能够实现更精细的人类偏好建模，并在真实数据集上验证了改进的个性化效果。

Abstract: Random Utility Models (RUMs) are a classical framework for modeling user
preferences and play a key role in reward modeling for Reinforcement Learning
from Human Feedback (RLHF). However, a crucial shortcoming of many of these
techniques is the Independence of Irrelevant Alternatives (IIA) assumption,
which collapses \emph{all} human preferences to a universal underlying utility
function, yielding a coarse approximation of the range of human preferences. On
the other hand, statistical and computational guarantees for models avoiding
this assumption are scarce. In this paper, we investigate the statistical and
computational challenges of learning a \emph{correlated} probit model, a
fundamental RUM that avoids the IIA assumption. First, we establish that the
classical data collection paradigm of pairwise preference data is
\emph{fundamentally insufficient} to learn correlational information,
explaining the lack of statistical and computational guarantees in this
setting. Next, we demonstrate that \emph{best-of-three} preference data
provably overcomes these shortcomings, and devise a statistically and
computationally efficient estimator with near-optimal performance. These
results highlight the benefits of higher-order preference data in learning
correlated utilities, allowing for more fine-grained modeling of human
preferences. Finally, we validate these theoretical guarantees on several
real-world datasets, demonstrating improved personalization of human
preferences.

</details>


### [147] [Self-Certifying Primal-Dual Optimization Proxies for Large-Scale Batch Economic Dispatch](https://arxiv.org/abs/2510.15850)
*Michael Klamkin,Mathieu Tanneau,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: 提出一种混合求解器，结合优化代理和经典求解器，通过对偶理论保证预测的最优性差距，在无法认证最优性时回退到经典求解器，实现可解释的速度-最优性权衡。


<details>
  <summary>Details</summary>
Motivation: 虽然优化代理在平均情况下能达到高精度，但最坏情况下的最优性差距可能非常大，难以在实际中信任预测结果。需要在经典求解器和优化代理之间找到平衡，实现可信部署。

Method: 提出混合求解器，利用对偶理论高效地约束预测的最优性差距；提出结合原始和对偶代理训练的替代训练过程来提高混合求解器的加速效果。

Result: 在大规模传输系统上的实验表明，混合求解器具有高度可扩展性，相比并行化单纯形求解器实现了超过1000倍的加速，同时保证最大最优性差距不超过2%。

Conclusion: 该混合求解器成功地在优化代理和经典求解器之间取得了平衡，实现了可解释的速度-最优性权衡，为优化代理的可信部署提供了有效解决方案。

Abstract: Recent research has shown that optimization proxies can be trained to high
fidelity, achieving average optimality gaps under 1% for large-scale problems.
However, worst-case analyses show that there exist in-distribution queries that
result in orders of magnitude higher optimality gap, making it difficult to
trust the predictions in practice. This paper aims at striking a balance
between classical solvers and optimization proxies in order to enable
trustworthy deployments with interpretable speed-optimality tradeoffs based on
a user-defined optimality threshold. To this end, the paper proposes a hybrid
solver that leverages duality theory to efficiently bound the optimality gap of
predictions, falling back to a classical solver for queries where optimality
cannot be certified. To improve the achieved speedup of the hybrid solver, the
paper proposes an alternative training procedure that combines the primal and
dual proxy training. Experiments on large-scale transmission systems show that
the hybrid solver is highly scalable. The proposed hybrid solver achieves
speedups of over 1000x compared to a parallelized simplex-based solver while
guaranteeing a maximum optimality gap of 2%.

</details>
