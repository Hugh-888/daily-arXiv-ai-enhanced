{"id": "2510.09742", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.09742", "abs": "https://arxiv.org/abs/2510.09742", "authors": ["Jamie Bamber", "Antonios Tsokaros", "Milton Ruiz", "Stuart L. Shapiro", "Marc Favata", "Matthew Karlson", "Fabrizio Venturi Piñas"], "title": "The Gravitational Wave Memory from Binary Neutron Star Mergers", "comment": "8 pages, 5 figures. Movies and additional visualizations available at\n  https://tinyurl.com/shapiromovies", "summary": "The gravitational wave signal produced by the merger of two compact objects\nincludes both an oscillatory transient and a non-oscillatory part, the\nso-called memory effect. This produces a permanent displacement of test masses\nand has not yet been measured. We use general relativistic magnetohydrodynamic\nsimulations, including neutrinos, with several representative viable equations\nof state, to quantify--for the first time--the effects of the neutron star\nmagnetic field, neutrino emission, and the ejected mass on the linear and\nnonlinear displacement memory in binary neutron star mergers. We find that the\nadditional contributions due to the emission of electromagnetic radiation,\nneutrinos and baryonic ejecta can be ~15% of the total memory for moderate\nmagnetic fields and up to ~50% for extreme magnetic fields. The memory is most\naffected by changes in the equation of state, the binary mass, and the magnetic\nfield. In particular, for moderate premerger field strengths, the dominant\nimpact of the electromagnetic field is the change in the gravitational wave\nluminosity, and the associated gravitational wave null memory, due to the\nunstable growth of the magnetic field and the resulting redistribution of\nangular momentum it induces in the remnant. While the direct electromagnetic\ncontribution to the null memory is additive, the change in the gravitational\nwave null memory can--in some cases--result in the total memory being smaller\nthan that from the corresponding nonmagnetized binary. Furthermore, in contrast\nto binary black hole mergers, the growth of the memory in binary neutron star\nmergers is extended due to the long emission timescale of electromagnetic\nfields, neutrinos, and ejecta. These results necessitate the consideration of\nthe magnetic field, as well as the equation of state, for accurate parameter\nestimation in future analyses of gravitational wave memory data."}
{"id": "2510.09803", "categories": ["physics.comp-ph", "physics.atom-ph"], "pdf": "https://arxiv.org/pdf/2510.09803", "abs": "https://arxiv.org/abs/2510.09803", "authors": ["Md Tusher Ahmed", "Farid Ahmed", "Jianzhi Li"], "title": "Enhancement of diffusivity and plastic deformation in ultrasound-assisted cold spray of tungsten: a molecular dynamics study", "comment": null, "summary": "Tungsten ($W$) is widely valued for its exceptional thermal stability,\nmechanical strength, and corrosion resistance, making it an ideal candidate for\nhigh-performance military and aerospace applications. However, its high melting\npoint and inherent brittleness pose significant challenges for processing $W$\nusing additive manufacturing (AM). Cold spray (CS), a solid-state AM process\nthat relies on high-velocity particle impact and plastic deformation, offers a\npromising alternative. In this study, we employ atomistic simulations to\ninvestigate the feasibility of CS for tungsten. We show that ultrasound\nperturbation can significantly enhance the self-diffusivity and plastic\ndeformation of $W$ compared to the negligible diffusion and plastic deformation\nobserved in non-ultrasound-assisted CS of $W$. For different impact velocities,\nparticle sizes, and ultrasound parameters, we demonstrate that\nultrasound-assisted viscoplasticity enhances self-diffusivity by inhibiting\ngrain boundaries and incorporating softening in $W$. Moreover, we found that\nthis enhanced diffusion in ultrasound-assisted $W$ can be exploited to promote\ninterdiffusion at the particle-substrate interface, enabling in situ alloy\nformation. Through the formation of an equimolar $V$-$W$ alloy on a $W$\nsubstrate using ultrasound-assisted CS simulations, we observed distinct\nmechanical properties and a reduced dislocation density in the deposited\ncoating compared to a pure tungsten substrate. These results highlight the\npotential of ultrasound-assisted CS as a viable approach for fabricating\nuniform coatings and engineered alloys, addressing key limitations in the AM of\nrefractory metals."}
{"id": "2510.09956", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.09956", "abs": "https://arxiv.org/abs/2510.09956", "authors": ["Shilong Huang", "Jiawei Chen", "Jinsong Yang"], "title": "Image of a quantum-corrected black hole without Cauchy horizons illuminated by a static thin accretion disk", "comment": null, "summary": "Latest advances in effective quantum gravity propose a quantum-corrected\nblack hole solution that avoids Cauchy horizons. In this paper, we study the\nimage of the black hole and explore the influence of the quantum parameter\n$\\zeta$ on its image. First, we investigate the influence of $\\zeta$ on the\nevent horizon, photon sphere, critical impact parameter, and innermost stable\ncircular orbit associated with the black hole. We find that all these\nquantities exhibit an increase with increasing $\\zeta$. Meanwhile, we analyze\nthe allowed range of $\\zeta$ from both theoretical and observational\nperspectives. We then derive the photon trajectory equation and analyze briefly\nthe behavior of the trajectories. A detailed analysis shows that as $\\zeta$\nincreases, the photon trajectories near the event horizon undergo\nmodifications. Finally, by plotting the optical appearance of the black hole\nunder three emission models, we find that as $\\zeta$ increases, the\nquantum-corrected black hole exhibits a larger shadow, along with a brightening\nof the bright rings and a reduction in the spacing between them near the\ncritical impact parameter. Therefore, we can distinguish the quantum-corrected\nblack hole from the Schwarzschild one by its unique optical appearance."}
{"id": "2510.09812", "categories": ["physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.09812", "abs": "https://arxiv.org/abs/2510.09812", "authors": ["Hongyi Guan", "Ananya Renuka Balakrishna"], "title": "CuPyMag: GPU-Accelerated Finite-Element Micromagnetics with Magnetostriction", "comment": null, "summary": "We introduce CuPyMag, an open-source, Python-based framework for large-scale\nmicromagnetic simulations with magnetostriction. CuPyMag solves micromagnetics\nwith finite elements in a GPU-resident workflow in which key operations, such\nas right-hand-side assembly, spatial derivatives, and volume averages, are\ntensorized using CuPy's BLAS-accelerated backend. Benchmark tests show that the\nGPU solvers in CuPyMag achieve a speedup of up to two orders of magnitude\ncompared to the CPU codes. Its runtime grows linearly/sublinearly with problem\nsize, demonstrating high efficiency. Additionally, CuPyMag uses the\nGauss-Seidel projection method for time integration, which not only allows\nstable time steps (up to 11 ps) but also solves each governing equation with\nonly 1-3 conjugate-gradient iterations without preconditioning. CuPyMag\naccounts for magnetoelastic coupling and far-field effects arising from the\nboundary of the magnetic body, both of which play an important role in\nmagnetization reversal in the presence of local defects. CuPyMag solves these\ncomputationally intensive multiphysics simulations with a high-resolution mesh\n(up to 3M nodes) in under three hours on an NVIDIA H200 GPU. This acceleration\nenables micromagnetic simulations with non-trivial defect geometries and\nresolves nanoscale magnetic structures. It expands the scope of micromagnetic\nsimulations towards realistic, large-scale problems that can guide experiments.\nMore broadly, CuPyMag is developed using widely adopted Python libraries, which\nprovide cross-platform compatibility, ease of installation, and accessibility\nfor adaptations to diverse applications."}
{"id": "2510.09957", "categories": ["gr-qc", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2510.09957", "abs": "https://arxiv.org/abs/2510.09957", "authors": ["Israel Quiros"], "title": "Weyl symmetry without the traceless condition", "comment": "15 pages (one column), no figures. We welcome feedback from\n  colleagues", "summary": "We show that the requirement that the trace of the stress-energy tensor of\nmatter must vanish if invariance under Weyl rescalings is a symmetry of a given\ngravitational theory, is not a general result but depends on the gravitational\ntheory and on whether the matter action is form-invariant under Weyl rescalings\nor not. We focus on conformal general relativity with a Weyl form-invariant\nmatter action. In this case, a linear dependence of the variations of the\nmetric and of the Brans-Dicke scalar field arises, which is a direct\nconsequence of invariance under infinitesimal Weyl rescalings. We demonstrate\nhow to apply the variational procedure properly and obtain the correct\nequations of motion. It results that the Klein-Gordon type equation for the\nBrans-Dicke scalar coincides with the trace of the Einstein-type equation of\nmotion. In consequence, any matter fields, no matter whether the trace of their\nstress-energy tensor vanishes or not, can be coupled to gravity. The\nphenomenological consequences of the modified result are drawn."}
{"id": "2510.09999", "categories": ["physics.comp-ph", "cond-mat.mtrl-sci", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.09999", "abs": "https://arxiv.org/abs/2510.09999", "authors": ["Yiran Bai", "Feng Xiong", "Xueheng Kuang"], "title": "Random State Approach to Quantum Computation of Electronic-Structure Properties", "comment": null, "summary": "Classical computation of electronic properties in large-scale materials\nremains challenging. Quantum computation has the potential to offer advantages\nin memory footprint and computational scaling. However, general and practical\nquantum algorithms for simulating large-scale materials are still lacking. We\npropose and implement random-state quantum algorithms to calculate\nelectronic-structure properties of real materials. Using a random state circuit\nwith only a few qubits, we employ real-time evolution with first-order Trotter\ndecomposition and Hadamard test to obtain electronic density of states, and we\ndevelop a modified quantum phase estimation algorithm to calculate real-space\nlocal density of states via direct quantum measurements. Furthermore, we\nvalidate these algorithms by numerically computing the density of states and\nspatial distributions of electronic states in graphene, twisted bilayer\ngraphene quasicrystals, and fractal lattices, covering system sizes from\nhundreds to thousands of atoms. Our results manifest that the random-state\nquantum algorithms provide a general and qubit-efficient route to simulating\nelectronic properties of large-scale periodic and aperiodic materials on\nquantum computers."}
{"id": "2510.10036", "categories": ["gr-qc", "hep-th", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2510.10036", "abs": "https://arxiv.org/abs/2510.10036", "authors": ["Xiankai Pang", "Yu Tian", "Hongbao Zhang", "Qingquan Jiang"], "title": "Fermionic Love number of Reissner-Nordström black holes", "comment": "14 pages", "summary": "The tidal deformation of compact objects, characterised by their Love\nnumbers, provides insights into the internal structure of neutron stars and\nblack holes. While bosonic tidal Love numbers vanish for black holes in general\nrelativity, it has been recently revealed that fermionic tidal perturbations\ncan induce non-zero Love numbers for Kerr black holes. In this paper, we\ninvestigate the response of the Reissner-Nordstr\\\"om black hole to the\nfermionic Weyl field. As a result, we find that the corresponding fermionic\ntidal Love numbers are also non-vanishing for the Reissner-Nordstr\\\"om black\nholes except for the extremal ones, which highlights the universal distinct\nbehavior of the fermionic tidal Love numbers compared to the bosonic\ncounterparts."}
{"id": "2510.10005", "categories": ["physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.10005", "abs": "https://arxiv.org/abs/2510.10005", "authors": ["Ryan F. Johnson", "Eric J. Ching", "Ethan S. Genter", "Joshua E. Lipman", "Andrew D. Kercher", "Jay Arcities", "Hai Wang"], "title": "ChemGen: Code Generation for Multispecies Chemical Kinetics in Computational Physics Simulations", "comment": "under review", "summary": "This paper introduces ChemGen, a software package that uses code generation\nto integrate multispecies thermodynamics and chemical kinetics into C+-based\ncomputational physics codes. ChemGen aims to make chemical kinetics more\naccessible in existing simulation frameworks and help bridge the gap between\ncombustion modeling and computational physics. The package employs the concept\nof decorators which enable flexible C++ code generation to target established\nsoftware ecosystems. ChemGen generates code to evaluate thermodynamic\nproperties, chemical source terms, and their analytical derivatives for\nJacobian calculations. Also included are a variety of implicit time integration\nschemes, linear solvers, and preconditioners. The various components of Chemgen\nare verified by demonstrating agreement with Cantera and/or theoretical\nconvergence rates. Finally, we integrate ChemGen into OpenFOAM and achieve a\nspeedup over its native chemistry solver by approximately four times. ChemGen\nis an ongoing project released under the NRL Open License, a source-available\nlicense provided by the U.S. Naval Research Laboratory."}
{"id": "2510.09747", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.09747", "abs": "https://arxiv.org/abs/2510.09747", "authors": ["Aaron Z. Goldberg", "Anaelle Hertz"], "title": "Spin coherence scale: operator-ordering sensitivity beyond Heisenberg-Weyl", "comment": "12+3 pages; comments always welcome", "summary": "We introduce the spin coherence scale as a measure of quantum coherence for\nspin systems, generalizing the quadrature coherence scale (QCS) previously\ndefined for quadrature observables. This SU($2$)-invariant measure quantifies\nthe off-diagonal coherences of a quantum state in angular momentum bases,\nweighted by the classical distinguishability of the superposed states. It\nserves as a witness of nonclassicality and provides both upper and lower bounds\non the Hilbert-Schmidt distance to the set of classical (spin coherent) states.\nWe demonstrate that many hallmark properties of the QCS carry over to the spin\nsetting, including its links to noise susceptibility of a state and moments of\nquasiprobability distributions. The spin coherence scale has direct\nimplications for quantum metrology in the guise of rotation sensing. We also\ngeneralize the framework to SU($n$) systems, identifying the unique\nSU($n$)-invariant depolarization channel and outlining a broad, Lie-algebraic\napproach to defining and characterizing the properties of coherence scale\nbeyond harmonic oscillators."}
{"id": "2510.09643", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09643", "abs": "https://arxiv.org/abs/2510.09643", "authors": ["Yuguang Liu", "Yiyun Miao", "Luyao Xia"], "title": "Direct Routing Gradient (DRGrad): A Personalized Information Surgery for Multi-Task Learning (MTL) Recommendations", "comment": null, "summary": "Multi-task learning (MTL) has emerged as a successful strategy in\nindustrial-scale recommender systems, offering significant advantages such as\ncapturing diverse users' interests and accurately detecting different behaviors\nlike ``click\" or ``dwell time\". However, negative transfer and the seesaw\nphenomenon pose challenges to MTL models due to the complex and often\ncontradictory task correlations in real-world recommendations. To address the\nproblem while making better use of personalized information, we propose a\npersonalized Direct Routing Gradient framework (DRGrad), which consists of\nthree key components: router, updater and personalized gate network. DRGrad\njudges the stakes between tasks in the training process, which can leverage all\nvalid gradients for the respective task to reduce conflicts. We evaluate the\nefficiency of DRGrad on complex MTL using a real-world recommendation dataset\nwith 15 billion samples. The results show that DRGrad's superior performance\nover competing state-of-the-art MTL models, especially in terms of AUC (Area\nUnder the Curve) metrics, indicating that it effectively manages task conflicts\nin multi-task learning environments without increasing model complexity, while\nalso addressing the deficiencies in noise processing. Moreover, experiments on\nthe public Census-income dataset and Synthetic dataset, have demonstrated the\ncapability of DRGrad in judging and routing the stakes between tasks with\nvarying degrees of correlation and personalization."}
{"id": "2510.10220", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.10220", "abs": "https://arxiv.org/abs/2510.10220", "authors": ["Ali I. Keskin", "Mehmet Yaşar", "K. Kurt"], "title": "Constant Roll Inflationary Dynamics with Generalized Potentials in $f(R,φ,X)$ Gravity", "comment": "PLB accepted", "summary": "In this work, we study early-time inflation within a class of $f(R, \\phi, X)$\ngravity models under a constant-roll condition. Employing a generalized\npotential of the form $V(\\phi)^\\sigma$, we derive expressions for the spectral\nindex $n_s$ and tensor-to-scalar ratio $r$, demonstrating that the inflationary\ndynamics are primarily governed by the parameter $\\sigma$ and the shape of the\npotential. At the upper limit of $n_s$, we obtain a de Sitter-like phase, while\nat the lower limit, the model transitions to a quintessence-like phase through\nan effective oscillating equation of state parameter (EoS). Therefore, under\nthe tuning parameter $\\sigma < 1$, the model exhibits a smooth transition from\na de Sitter-like phase to a quintessence-like phase via the oscillating EoS\nparameter. The resulting predictions are consistent with recent observations\nfrom the Atacama Cosmology Telescope (ACT), combined with CMB lensing and DESI\nBAO data."}
{"id": "2510.10529", "categories": ["physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.10529", "abs": "https://arxiv.org/abs/2510.10529", "authors": ["Vishal Subramanian", "Bikash Kanungo", "Vikram Gavini"], "title": "invDFT: A CPU-GPU massively parallel tool to find exact exchange-correlation potentials from groundstate densities", "comment": "41 pages, 14 figures", "summary": "Density functional theory (DFT) remains the most widely used electronic\nstructure method. Although exact in principle, in practice, it relies on\napproximations to the exchange-correlation (XC) functional, which is known to\nbe a unique functional of the electron density. Despite 50 years of active\nresearch, existing XC approximations remain far from general purpose chemical\naccuracy of various thermochemical and materials properties. In that light, the\ninverse DFT problem, of finding the exact XC potential corresponding to an\naccurate groundstate density, offers an insightful tool to understand the\nnature of the XC functional as well as aid in the development of more accurate\nfunctionals. However, solving the inverse DFT problem is fraught with several\nnumerical challenges, such as non-uniqueness or spurious oscillations in the\nsolution and non-convergence. We present invDFT as an open-source framework to\naddress the outstanding challenges in inverse DFT and computed XC potentials\nsolely from a target density. We do so by use of a systematically convergent\nfinite-element basis and asymptotic corrections to the target density. We also\nemploy several numerical and high-performance computing (HPC) advances that\naffords both efficiency and parallel scalability, on CPU-GPU hybrid\narchitectures. We demonstrate the accuracy and scalability of invDFT using\naccurate full-configuration interaction (FCI) densities as well as model\ndensities, ranging up to 100 electrons and spanning both weakly and strongly\ncorrelated molecules."}
{"id": "2510.09749", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2510.09749", "abs": "https://arxiv.org/abs/2510.09749", "authors": ["Jeffrey Z. Song", "Gilad Kishony", "Erez Berg", "Mark S. Rudner"], "title": "Vari-Cool: a non-unitary quantum variational protocol for simulated cooling", "comment": null, "summary": "We introduce a variational approach for preparing low energy states of\narbitrary target Hamiltonians. The protocol is defined in terms of a repeated\ncycle consisting of p layers of unitary gates applied to the system and ancilla\n\"bath\" qubits, followed by reset of the bath qubits. The gate parameters within\neach cycle are optimized such that the steady state achieved after many cycles\nhas a low energy expectation value with respect to the target Hamiltonian, and\nthat the energy converges toward the steady state value in as few cycles as\npossible. We illustrate the protocol for the transverse field Ising model, and\nstudy its systematic behaviors with respect to system size, model parameters,\nand noise using tensor network based classical simulations. We then\nexperimentally demonstrate its operation on IBM's ibm_kingston quantum\nprocessor for up to 28 system qubits coupled to 14 bath sites. Classical\ntraining on small system sizes and with few unitary layers per cycle gives\nrobust results that transfer well to larger system sizes and to noisy hardware."}
{"id": "2510.09644", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09644", "abs": "https://arxiv.org/abs/2510.09644", "authors": ["Shaharyar Alam Ansari", "Mohammad Luqman", "Aasim Zafar", "Savir Ali"], "title": "Enhanced Urban Traffic Management Using CCTV Surveillance Videos and Multi-Source Data Current State Prediction and Frequent Episode Mining", "comment": "24 pages, 9 figures", "summary": "Rapid urbanization has intensified traffic congestion, environmental strain,\nand inefficiencies in transportation systems, creating an urgent need for\nintelligent and adaptive traffic management solutions. Conventional systems\nrelying on static signals and manual monitoring are inadequate for the dynamic\nnature of modern traffic. This research aims to develop a unified framework\nthat integrates CCTV surveillance videos with multi-source data descriptors to\nenhance real-time urban traffic prediction. The proposed methodology\nincorporates spatio-temporal feature fusion, Frequent Episode Mining for\nsequential traffic pattern discovery, and a hybrid LSTM-Transformer model for\nrobust traffic state forecasting. The framework was evaluated on the CityFlowV2\ndataset comprising 313,931 annotated bounding boxes across 46 cameras. It\nachieved a high prediction accuracy of 98.46 percent, with a macro precision of\n0.9800, macro recall of 0.9839, and macro F1-score of 0.9819. FEM analysis\nrevealed significant sequential patterns such as moderate-congested transitions\nwith confidence levels exceeding 55 percent. The 46 sustained congestion alerts\nare system-generated, which shows practical value for proactive congestion\nmanagement. This emphasizes the need for the incorporation of video stream\nanalytics with data from multiple sources for the design of real-time,\nresponsive, adaptable multi-level intelligent transportation systems, which\nmakes urban mobility smarter and safer."}
{"id": "2510.10240", "categories": ["gr-qc", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2510.10240", "abs": "https://arxiv.org/abs/2510.10240", "authors": ["Boyan Wang"], "title": "Adiabatic Inspiral Transition and Induction to Plunge of a Compact Body in Equatorial Plane Around a Massive Kerr Black Hole", "comment": null, "summary": "This paper reconstructs the derivation process from the Kerr metric to the\nadiabatic inspiral, transition, and plunge regimes, aiming to highlight the\ndetails and logical connections often overlooked in previous derivations. The\nfirst half provides a comprehensive roadmap for readers familiar with advanced\ngeneral relativity to follow the entire logic of the inspiral-transition-plunge\nregime from this paper alone. The second half addresses the discontinuity\nbetween the adiabatic inspiral and the plunge, including analyses and\nreinterpretations of the Ori-Thorne and Ori-Thorne-Kesden transition\nprocedures, and proposes two new interpretations: a variant of the Kesden\nY-correction and the Adiabatic Inspiral Perturbation-Induced Plunge. The paper\nalso introduces the concept of the Most Stable Circular Orbit (MSCO) and\nanalyzes its properties as a characteristic radius of Kerr black holes."}
{"id": "2510.11180", "categories": ["physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.11180", "abs": "https://arxiv.org/abs/2510.11180", "authors": ["Shilei Ji", "Jianping Yang", "Li Gao", "Xing'ao Li"], "title": "Tuning Layer Orbital Hall Effect via Spin Rotation in Ferromagnetic Transition Metal Dichalcogenides", "comment": null, "summary": "Orbitronics, which leverages the angular momentum of atomic orbitals for\ninformation transmission, provides a novel strategy to overcome the limitations\nof electronic devices. Unlike electron spin, orbital angular momentum (OAM) is\nstrongly influenced by crystal field effects and band topology, making its\norientation difficult to manipulate with external fields. In this work, by\nusing first principle calculations, we investigate quantum anomalous Hall\ninsulators (QAHIs) as a model system to study the layer orbital Hall effect\n(OHE). Due to band inversion, only one valley remains orbital polarization, and\nthus the OHE originates from a single valley. Based on stacking symmetry\nanalysis, we investigated both AA and AB stacking configurations, which possess\nmirror and inversion symmetries, respectively. The excitation of OAM exhibits\nvalley selectivity, determined jointly by valley polarization and orbital\npolarization. In AA stacking, the absence of inversion center gives rise to\nintrinsic orbital polarization, leading to OAM excitations from different\nvalleys in the two layers. In contrast, AB stacking is protected by inversion\nsymmetry, which enforces valley polarization and"}
{"id": "2510.09765", "categories": ["quant-ph", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.09765", "abs": "https://arxiv.org/abs/2510.09765", "authors": ["Bhanu Pratap Yadav", "Mahdi Bayanifar", "Olav Tirkkonen"], "title": "Bounds in the Projective Unitary Group with Respect to Global Phase Invariant Metric", "comment": null, "summary": "We consider a global phase-invariant metric in the projective unitary group\nPUn, relevant for universal quantum computing. We obtain the volume and measure\nof small metric ball in PUn and derive the Gilbert-Varshamov and Hamming bounds\nin PUn. In addition, we provide upper and lower bounds for the kissing radius\nof the codebooks in PUn as a function of the minimum distance. Using the lower\nbound of the kissing radius, we find a tight Hamming bound. Also, we establish\nbounds on the distortion-rate function for quantizing a source uniformly\ndistributed over PUn. As example codebooks in PUn, we consider the projective\nPauli and Clifford groups, as well as the projective group of diagonal gates in\nthe Clifford hierarchy, and find their minimum distances. For any code in PUn\nwith given cardinality we provide a lower bound of covering radius. Also, we\nprovide expected value of the covering radius of randomly distributed points on\nPUn, when cardinality of code is sufficiently large. We discuss codebooks at\nvarious stages of the projective Clifford + T and projective Clifford + S\nconstructions in PU2, and obtain their minimum distance, distortion, and\ncovering radius. Finally, we verify the analytical results by simulation."}
{"id": "2510.09657", "categories": ["cs.LG", "cs.AI", "cs.NA", "eess.SP", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.09657", "abs": "https://arxiv.org/abs/2510.09657", "authors": ["Riccardo Fosco Gramaccioni", "Christian Marinoni", "Fabrizio Frezza", "Aurelio Uncini", "Danilo Comminiello"], "title": "Generative Models for Helmholtz Equation Solutions: A Dataset of Acoustic Materials", "comment": "Accepted at EUSIPCO 2025", "summary": "Accurate simulation of wave propagation in complex acoustic materials is\ncrucial for applications in sound design, noise control, and material\nengineering. Traditional numerical solvers, such as finite element methods, are\ncomputationally expensive, especially when dealing with large-scale or\nreal-time scenarios. In this work, we introduce a dataset of 31,000 acoustic\nmaterials, named HA30K, designed and simulated solving the Helmholtz equations.\nFor each material, we provide the geometric configuration and the corresponding\npressure field solution, enabling data-driven approaches to learn Helmholtz\nequation solutions. As a baseline, we explore a deep learning approach based on\nStable Diffusion with ControlNet, a state-of-the-art model for image\ngeneration. Unlike classical solvers, our approach leverages GPU\nparallelization to process multiple simulations simultaneously, drastically\nreducing computation time. By representing solutions as images, we bypass the\nneed for complex simulation software and explicit equation-solving.\nAdditionally, the number of diffusion steps can be adjusted at inference time,\nbalancing speed and quality. We aim to demonstrate that deep learning-based\nmethods are particularly useful in early-stage research, where rapid\nexploration is more critical than absolute accuracy."}
{"id": "2510.10282", "categories": ["gr-qc", "astro-ph.CO", "astro-ph.GA"], "pdf": "https://arxiv.org/pdf/2510.10282", "abs": "https://arxiv.org/abs/2510.10282", "authors": ["Jens Boos", "Hao Hu"], "title": "Microlensing of non-singular black holes at finite size: a ray tracing approach", "comment": "21 pages, 14 figures, comments welcome", "summary": "We study the gravitational microlensing of various static and spherically\nsymmetric non-singular black holes (and horizonless, non-singular compact\nobjects of similar size). For pointlike sources we extend the parametrized\npost-Newtonian lensing framework to fourth order, whereas for extended sources\nwe develop a ray tracing approach via a simple radiative transfer model.\nModelling non-relativistic proper motion of the lens in front of a background\nstar we record the apparent brightness as a function of time, resulting in a\nphotometric lightcurve. Taking the star radius to smaller values, our numerical\nresults approach the theoretical predictions for point-like sources. Compared\nto the Schwarzschild metric in an otherwise unmodified lensing geometry, we\nfind that non-singular black hole models (and their horizonless, non-singular\ncounterparts) at finite size tend to feature larger magnifications in\nmicrolensing lightcurves, contrary to the point-source prediction."}
{"id": "2510.11187", "categories": ["physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.11187", "abs": "https://arxiv.org/abs/2510.11187", "authors": ["Stephan Wulfinghoff"], "title": "Computational Crystal Plasticity Homogenization using Empirically Corrected Cluster Cubature (E3C) Hyper-Reduction", "comment": null, "summary": "The computational homogenization of elastoplastic polycrystals is a\nchallenging task due to the huge number of grains required, their complicated\ninteractions and due to the complexity of crystal plasticity models per se.\nDespite a few successes of reduced order models, mean field and simplified\nhomogenization approaches often remain the preferred choice. In this work, a\nrecently proposed hyper-reduction method (called E3C) for projection-based\nReduced Order Models (pROMs) is applied to the problem of computational\nhomogenization of geometrically linearly deforming elastoplastic polycrystals.\nThe main novelty lies in the identification of reduced modes (the 'E3C-modes'),\nwhich replace the strain modes of the reduced-order model, leading to a\nsignificantly smaller number of integration points. The peculiarity, which\ndistinguishes the method from more conventional hyper-reduction techniques, is\nthat the E3C integration points are not taken from the set of FE integration\npoints. Instead, they can be interpreted as generalized integration points in\nstrain space which are trained such as to satisfy an orthogonality condition,\nwhich ensures that the hyper-reduced model matches the equilibrium states and\nmacroscopic stresses of full-field model data as accurately as possible. In\naddition, the number of grains is reduced, preserving the main features of the\noriginal texture of the finite element model. Two macroscopic engineering parts\n(untextured and textured) are simulated, illustrating the performance of the\nmethod in three-dimensional two-scale applications involving hundreds of\nthousands macroscopic degrees of freedom and millions of grains with computing\ntimes in the order of hours (cumulated online and offline effort) on standard\nlaptop hardware."}
{"id": "2510.09813", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.09813", "abs": "https://arxiv.org/abs/2510.09813", "authors": ["Kemal Bidzhiev", "Stefano Grava", "Pablo le Henaff", "Mauro Mendizabal", "Elie Merhej", "Anton Quelle"], "title": "Efficient Emulation of Neutral Atom Quantum Hardware", "comment": null, "summary": "Simulating the dynamics of neutral atom arrays is a challenging problem. To\naddress this, we introduce two emulators, emu-sv and emu-mps, as computational\nbackends for Pasqal's pulser package. Emu-sv is designed for high-precision\nstate-vector simulations, giving the possibility to emulate systems of up to\n$\\thicksim 27$ qubits on an A100 40GB GPU, making it perfect for cases where\nnumerically exact results are needed. In contrast, emu-mps uses a Matrix\nProduct State representation and other controlled approximations to efficiently\nsimulate much larger arrays of atoms with manageable errors. We show through\nbenchmark comparisons that both emulators provide significant speed-ups over\ngeneric solvers such as QuTiP. In addition, we provide practical guidance on\nchoosing between the two emulators. These quantum software tools are designed\nto support researchers and developers aiming to simulate quantum systems either\nas a precursor to full hardware implementation or as a means of benchmarking\nhardware performance."}
{"id": "2510.09658", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09658", "abs": "https://arxiv.org/abs/2510.09658", "authors": ["Filippo Rinaldi", "Aniello Panariello", "Giacomo Salici", "Fengyuan Liu", "Marco Ciccone", "Angelo Porrello", "Simone Calderara"], "title": "Gradient-Sign Masking for Task Vector Transport Across Pre-Trained Models", "comment": null, "summary": "When a new release of a foundation model is published, practitioners\ntypically need to repeat full fine-tuning, even if the same task has already\nbeen solved in the previous version. A promising alternative is to reuse the\nparameter changes (i.e., task vectors) that capture how a model adapts to a\nspecific task. However, they often fail to transfer across different\npre-trained models due to their misaligned parameter space. In this work, we\nshow that the key to successful transfer lies in the sign structure of the\ngradients of the new model. Based on this insight, we propose GradFix, a novel\nmethod that approximates the ideal gradient sign structure and leverages it to\ntransfer knowledge using only a handful of labeled samples. Notably, this\nrequires no additional fine-tuning: the adaptation is achieved by computing a\nfew gradients at the target model and masking the source task vector\naccordingly. This yields an update that is locally aligned with the target loss\nlandscape, effectively rebasing the task vector onto the new pre-training. We\nprovide a theoretical guarantee that our method ensures first-order descent.\nEmpirically, we demonstrate significant performance gains on vision and\nlanguage benchmarks, consistently outperforming naive task vector addition and\nfew-shot fine-tuning."}
{"id": "2510.10306", "categories": ["gr-qc", "math.DG"], "pdf": "https://arxiv.org/pdf/2510.10306", "abs": "https://arxiv.org/abs/2510.10306", "authors": ["Sven Hirsch", "Lan-Hsuan Huang"], "title": "Monotonicity of Causal Killing Vectors and Geometry of ADM Mass Minimizers", "comment": null, "summary": "We address two problems concerning the ADM mass-minimizing initial data sets.\nFirst, we show that the equality case of the positive mass theorem embeds into\na pp-wave spacetime. Second, we show that positive Bartnik mass minimizers\nembed into strongly stationary vacuum spacetimes, thereby confirming the\nBartnik stationary vacuum conjecture. A key ingredient is a new monotonicity\nformula for the Lorentzian length of a causal Killing vector field, which,\namong other applications, yields a strong maximum principle for the length."}
{"id": "2510.11562", "categories": ["physics.comp-ph", "cond-mat.mtrl-sci", "cond-mat.stat-mech", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2510.11562", "abs": "https://arxiv.org/abs/2510.11562", "authors": ["Hubert J. Naguszewski", "Christopher D. Woodgate", "David Quigley"], "title": "Optimal parallelisation strategies for flat histogram Monte Carlo sampling", "comment": null, "summary": "Flat histogram methods, such as Wang-Landau sampling, provide a means for\nhigh throughput calculation of phase diagrams of atomistic/lattice model\nsystems. Many parallelisation schemes with varying degrees of complexity have\nbeen proposed to accelerate such sampling simulations. In this study, several\nwidely used schemes are benchmarked - both in isolation and in combination - to\nestablish best practice. The schemes studied include energy domain\ndecomposition with both static sizing of energy sub-domains, as well as a\ndynamic sub-domain sizing scheme which we propose. We also assess the benefits\nboth of replica exchange and of including multiple random walkers per\nsub-domain, to determine which factors have the largest impact on parallel\nefficiency. Additionally, the influence of the choice of size of energy\nsub-domain overlap regions is discussed. As an illustrative test case, we\nimplement and apply the aforementioned strategies to a lattice-based model\ndescribing the internal energies of the AlTiCrMo refractory high-entropy\nsuperalloy, which is understood to crystallographically order into a B2 (CsCl)\nstructure with decreasing temperature. We find that - while all of the proposed\nstrategies confer a non-negligible speedup - parallelisation across energy\ndomains which are non-uniform in size offers the most appreciable performance\nimprovements. This work offers concrete recommendations for which\nparallelisation strategies should be prioritised to optimally accelerate\nflat-histogram Monte Carlo simulations."}
{"id": "2510.09824", "categories": ["quant-ph", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.09824", "abs": "https://arxiv.org/abs/2510.09824", "authors": ["Kamil Khadiev", "Aliya Khadieva", "Vadim Sagitov", "Kamil Khasanov"], "title": "Quantum Circuit for Quantum Fourier Transform for Arbitrary Qubit Connectivity Graphs", "comment": null, "summary": "In the paper, we consider quantum circuits for the Quantum Fourier Transform\n(QFT) algorithm. The QFT algorithm is a very popular technique used in many\nquantum algorithms. We present a generic method for constructing quantum\ncircuits for this algorithm implementing on quantum devices with restrictions.\nMany quantum devices (for example, based on superconductors) have restrictions\non applying two-qubit gates. These restrictions are presented by a qubit\nconnectivity graph. Typically, researchers consider only the linear nearest\nneighbor (LNN) architecture of the qubit connection, but current devices have\nmore complex graphs. We present a method for arbitrary connected graphs that\nminimizes the number of CNOT gates in the circuit for implementing on such\narchitecture.\n  We compare quantum circuits built by our algorithm with existing quantum\ncircuits optimized for specific graphs that are Linear-nearest-neighbor (LNN)\narchitecture, ``sun'' (a cycle with tails, presented by the 16-qubit IBMQ\ndevice) and ``two joint suns'' (two joint cycles with tails, presented by the\n27-qubit IBMQ device). Our generic method gives similar results with existing\noptimized circuits for ``sun'' and ``two joint suns'' architectures, and a\ncircuit with slightly more CNOT gates for the LNN architecture. At the same\ntime, our method allows us to construct a circuit for arbitrary connected\ngraphs."}
{"id": "2510.09659", "categories": ["cs.LG", "hep-ex"], "pdf": "https://arxiv.org/pdf/2510.09659", "abs": "https://arxiv.org/abs/2510.09659", "authors": ["Edgar E. Robles", "Dikshant Sagar", "Alejandro Yankelevich", "Jianming Bian", "Pierre Baldi", "NOvA Collaboration"], "title": "Heterogeneous Point Set Transformers for Segmentation of Multiple View Particle Detectors", "comment": "Submitted to Machine Learning and the Physical Sciences Workshop\n  (ML4PS) at NeurIPS 2025", "summary": "NOvA is a long-baseline neutrino oscillation experiment that detects neutrino\nparticles from the NuMI beam at Fermilab. Before data from this experiment can\nbe used in analyses, raw hits in the detector must be matched to their source\nparticles, and the type of each particle must be identified. This task has\ncommonly been done using a mix of traditional clustering approaches and\nconvolutional neural networks (CNNs). Due to the construction of the detector,\nthe data is presented as two sparse 2D images: an XZ and a YZ view of the\ndetector, rather than a 3D representation. We propose a point set neural\nnetwork that operates on the sparse matrices with an operation that mixes\ninformation from both views. Our model uses less than 10% of the memory\nrequired using previous methods while achieving a 96.8% AUC score, a higher\nscore than obtained when both views are processed independently (85.4%)."}
{"id": "2510.10428", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.10428", "abs": "https://arxiv.org/abs/2510.10428", "authors": ["Shivangi Rathore", "S. Surendra Singh"], "title": "Cosmological implications of LRS Bianchi type-I cosmological model in $f(T)$ gravity", "comment": "12 pages, 1 figures", "summary": "We perform the dynamical system analysis of the Locally Rotationally\nSymmetric (LRS) Bianchi type-I cosmological model in f(T) gravity in the\npresence of energy interaction . A cosmologically viable form of $f(T)$ is\nchosen (where $T$ is the torsion scalar in teleparallelism) in the background\nof homogenous and anisotropic. For our model, we take $f(T) = T+\\zeta T^{2}$\nwhere $\\zeta$ is a constant. The evolution equations are reduced to the\nautonomous system of differential equations by suitable transformation of\nvariables. The behaviour of the equilibrium points is examined by calculating\nthe eigenvalues corresponding to these equilibrium points. We get four\nequilibrium points for our cosmological model out of which one equilibrium\npoint is stable, two are saddle points and one is an unstable equilibrium\npoint. Corresponding to equilibrium point $T_{2}$, our model is consistent with\nthe quintessence dark energy cosmological model. Along the equilibrium points\n$T_{1},T_{3}$ and $T_{4}$, our model is consistent to phantom dark energy\nmodel. After that, we utilize the autonomous equations to analyse the\ncosmographic parameters along with the state-finder parameter. We demonstrate\nthe phase plot analysis for our model."}
{"id": "2510.09670", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.09670", "abs": "https://arxiv.org/abs/2510.09670", "authors": ["Xinlun Cheng", "Bingzhe Chen", "Joseph Choi", "Yen T. Nguyen", "Pradeep Seshadri", "Mayank Verma", "H. S. Udaykumar", "Stephen Baek"], "title": "A physics-aware deep learning model for shear band formation around collapsing pores in shocked reactive materials", "comment": null, "summary": "Modeling shock-to-detonation phenomena in energetic materials (EMs) requires\ncapturing complex physical processes such as strong shocks, rapid changes in\nmicrostructural morphology, and nonlinear dynamics of chemical reaction fronts.\nThese processes participate in energy localization at hotspots, which initiate\nchemical energy release leading to detonation. This study addresses the\nformation of hotspots in crystalline EMs subjected to weak-to-moderate shock\nloading, which, despite its critical relevance to the safe storage and handling\nof EMs, remains underexplored compared to the well-studied strong shock\nconditions. To overcome the computational challenges associated with direct\nnumerical simulations, we advance the Physics-Aware Recurrent Convolutional\nNeural Network (PARCv2), which has been shown to be capable of predicting\nstrong shock responses in EMs. We improved the architecture of PARCv2 to\nrapidly predict shear localizations and plastic heating, which play important\nroles in the weak-to-moderate shock regime. PARCv2 is benchmarked against two\nwidely used physics-informed models, namely, Fourier neural operator and neural\nordinary differential equation; we demonstrate its superior performance in\ncapturing the spatiotemporal dynamics of shear band formation. While all models\nexhibit certain failure modes, our findings underscore the importance of\ndomain-specific considerations in developing robust AI-accelerated simulation\ntools for reactive materials."}
{"id": "2510.09834", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.09834", "abs": "https://arxiv.org/abs/2510.09834", "authors": ["Michael Korenberg", "Uzi Pereg"], "title": "Quantum Action-Dependent Channels", "comment": null, "summary": "We study the quantum action-dependent channel. The model can be viewed as a\nquantum analog of the classical action-dependent channel model. In this\nsetting, the communication channel has two inputs: Alice's transmission and the\ninput environment. The action-dependent mechanism enables the transmitter to\ninfluence the channel's environment through an action channel. Specifically,\nAlice encodes her message into a quantum action, which subsequently affects the\nenvironment state. For example, a quantum measurement at the encoder can induce\na state collapse of the environment. In addition, Alice has access to side\ninformation. Unlike the classical model, she cannot have a copy of the\nenvironment state due to the no-cloning theorem. Instead, she shares\nentanglement with this environment. We establish an achievable communication\nrate for reliable message transmission via the quantum action-dependent\nchannel, thereby extending the classical action-dependent framework to the\nquantum domain."}
{"id": "2510.09660", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09660", "abs": "https://arxiv.org/abs/2510.09660", "authors": ["Luca Scimeca", "Thomas Jiralerspong", "Berton Earnshaw", "Jason Hartford", "Yoshua Bengio"], "title": "Learning What Matters: Steering Diffusion via Spectrally Anisotropic Forward Noise", "comment": null, "summary": "Diffusion Probabilistic Models (DPMs) have achieved strong generative\nperformance, yet their inductive biases remain largely implicit. In this work,\nwe aim to build inductive biases into the training and sampling of diffusion\nmodels to better accommodate the target distribution of the data to model. We\nintroduce an anisotropic noise operator that shapes these biases by replacing\nthe isotropic forward covariance with a structured, frequency-diagonal\ncovariance. This operator unifies band-pass masks and power-law weightings,\nallowing us to emphasize or suppress designated frequency bands, while keeping\nthe forward process Gaussian. We refer to this as spectrally anisotropic\nGaussian diffusion (SAGD). In this work, we derive the score relation for\nanisotropic covariances and show that, under full support, the learned score\nconverges to the true data score as $t\\!\\to\\!0$, while anisotropy reshapes the\nprobability-flow path from noise to data. Empirically, we show the induced\nanisotropy outperforms standard diffusion across several vision datasets, and\nenables selective omission: learning while ignoring known corruptions confined\nto specific bands. Together, these results demonstrate that carefully designed\nanisotropic forward noise provides a simple, yet principled, handle to tailor\ninductive bias in DPMs."}
{"id": "2510.10441", "categories": ["gr-qc", "astro-ph.CO", "hep-th"], "pdf": "https://arxiv.org/pdf/2510.10441", "abs": "https://arxiv.org/abs/2510.10441", "authors": ["Oem Trivedi"], "title": "Cosmological Implications of Thermodynamic Split Conjecture", "comment": "21 pages with no figures, comments are very welcome !", "summary": "Building on initial work on the Thermodynamic Split Conjecture (TSC), which\nposits that black hole and cosmological horizon thermodynamics are generically\ninequivalent, we examine the consequences of that split for the Gibbons Hawking\ntemperature and its role across cosmology. We consider many key results in both\nearly and late universe cosmology and show that many important results such as\nthose governing eternal inflation, vacuum tunneling, quantum breaking and\nprimordial black holes can change. The analysis further reveals that small, TSC\nmotivated corrections to horizon thermodynamics can subtly modify Friedmann\ndynamics, potentially helping to address the $H_0$ and $S_8$ tensions. The work\nthus provides a unified route from quantum gravity motivated thermodynamics to\nobservational cosmology and motivates dedicated tests of the thermal laws\ngoverning the Universe itself."}
{"id": "2510.09768", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.09768", "abs": "https://arxiv.org/abs/2510.09768", "authors": ["Khang Ngo", "Siamak Ravanbakhsh"], "title": "Scaling Laws and Symmetry, Evidence from Neural Force Fields", "comment": "22 pages, 10 figures", "summary": "We present an empirical study in the geometric task of learning interatomic\npotentials, which shows equivariance matters even more at larger scales; we\nshow a clear power-law scaling behaviour with respect to data, parameters and\ncompute with ``architecture-dependent exponents''. In particular, we observe\nthat equivariant architectures, which leverage task symmetry, scale better than\nnon-equivariant models. Moreover, among equivariant architectures, higher-order\nrepresentations translate to better scaling exponents. Our analysis also\nsuggests that for compute-optimal training, the data and model sizes should\nscale in tandem regardless of the architecture. At a high level, these results\nsuggest that, contrary to common belief, we should not leave it to the model to\ndiscover fundamental inductive biases such as symmetry, especially as we scale,\nbecause they change the inherent difficulty of the task and its scaling laws."}
{"id": "2510.09911", "categories": ["quant-ph", "81P68 (Primary), 68Q12 (Secondary)", "F.2.2"], "pdf": "https://arxiv.org/pdf/2510.09911", "abs": "https://arxiv.org/abs/2510.09911", "authors": ["Lingfa Meng", "David Salvador Novo", "Albert H. Werner", "Samir Bhatt"], "title": "Quantum Algorithms for the Minimum Steiner Tree problem with application to Binary Near-Perfect Phylogenies", "comment": null, "summary": "We present a quantum algorithm in bioinformatics for solving the Binary\nNear-Perfect Phylogeny Problem (BNPP) with a complexity bound of $O(8.926^q +\n8^q nm2)$, where n is the number of input taxa and m is the sequence length for\neach taxon with each character in the sequence being a binary bit using the\nQRAM model. We give another polynomial space exact algorithm for the Minimum\nSteiner Tree (MST) problem with complexity $O^*(e^{(1+g(k,l))k})$ in the\ncircuit model."}
{"id": "2510.09662", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.09662", "abs": "https://arxiv.org/abs/2510.09662", "authors": ["Ali Jaberi", "Amin Sadeghi", "Runze Zhang", "Zhaoyang Zhao", "Qiuyu Shi", "Robert Black", "Zoya Sadighi", "Jason Hattrick-Simpers"], "title": "Assessment of different loss functions for fitting equivalent circuit models to electrochemical impedance spectroscopy data", "comment": null, "summary": "Electrochemical impedance spectroscopy (EIS) data is typically modeled using\nan equivalent circuit model (ECM), with parameters obtained by minimizing a\nloss function via nonlinear least squares fitting. This paper introduces two\nnew loss functions, log-B and log-BW, derived from the Bode representation of\nEIS. Using a large dataset of generated EIS data, the performance of proposed\nloss functions was evaluated alongside existing ones in terms of R2 scores,\nchi-squared, computational efficiency, and the mean absolute percentage error\n(MAPE) between the predicted component values and the original values.\nStatistical comparisons revealed that the choice of loss function impacts\nconvergence, computational efficiency, quality of fit, and MAPE. Our analysis\nshowed that X2 loss function (squared sum of residuals with proportional\nweighting) achieved the highest performance across multiple quality of fit\nmetrics, making it the preferred choice when the quality of fit is the primary\ngoal. On the other hand, log-B offered a slightly lower quality of fit while\nbeing approximately 1.4 times faster and producing lower MAPE for most circuit\ncomponents, making log-B as a strong alternative. This is a critical factor for\nlarge-scale least squares fitting in data-driven applications, such as training\nmachine learning models on extensive datasets or iterations."}
{"id": "2510.10579", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.10579", "abs": "https://arxiv.org/abs/2510.10579", "authors": ["Bekir Can Lütfüoğlu"], "title": "Grey-body factors and absorption cross-sections of scalar and Dirac fields in the vicinity of dilaton-de Sitter black hole", "comment": "10 pages, 8 figures", "summary": "We investigate the propagation of a massive scalar field and a massless Dirac\nfield in the geometry of a dilaton--de Sitter black hole. Starting from the\ncovariant perturbation equations, we derive the corresponding effective\npotentials and analyze their dependence on the dilaton charge, field mass, and\ncosmological constant. Using the WKB approximation, we compute the grey-body\nfactors and study the associated absorption cross-sections. The results show\nthat increasing the field mass or dilaton charge raises the effective potential\nbarrier, leading to a suppression of transmission at low frequencies, while a\nlarger cosmological constant lowers the barrier and enhances transmission. The\npartial absorption cross-sections for different multipole numbers display the\nexpected oscillatory structure, with the lowest multipoles dominating at small\nfrequencies. After summation over multipoles, the oscillations average out and\nthe total cross-section interpolates between strong suppression in the infrared\nregime and the geometric capture limit at high frequencies. These findings\nprovide a systematic description of scattering and absorption properties of\ndilaton--de Sitter black holes for both scalar and fermionic perturbations."}
{"id": "2510.10345", "categories": ["quant-ph", "cond-mat.stat-mech", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.10345", "abs": "https://arxiv.org/abs/2510.10345", "authors": ["Elijah Pelofske"], "title": "Depth One Quantum Alternating Operator Ansatz as an Approximate Gibbs Distribution Sampler", "comment": null, "summary": "This study numerically investigates the thermal sampling properties of QAOA,\nthe Quantum Alternating Operator Ansatz which was generalized from the original\nQuantum Approximate Optimization Algorithm. Specifically, the ability of QAOA\nto sample from the Gibbs distribution, equivalently the Boltzmann distribution,\ndefined by a classical Ising model, specifically a fully connected disordered\nspin glass (Sherrington-Kirkpatrick) model. We focus on two different QAOA\nmixers; the standard transverse field X mixer, and the Grover mixer. At a QAOA\ndepth of one we examine, for a single full QAOA parameter search space period,\nthe energy landscape, the Shannon entropy landscape of the QAOA probability\ndistribution, and the tradeoff between Boltzmann distribution sampling\ntemperature and error rate (how close to the true Boltzmann distribution is the\nQAOA distribution). We find that at very high temperatures one-round Grover\nmixer QAOA can sample from the Boltzmann distribution more accurately than the\nstandard X mixer QAOA at one round. Both X mixer and Grover mixer depth one\nQAOA can serve as approximate Boltzmann distribution samplers, and how good\nthis approximation is depends heavily on the QAOA angle choice."}
{"id": "2510.09919", "categories": ["quant-ph", "math.ST", "stat.AP", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.09919", "abs": "https://arxiv.org/abs/2510.09919", "authors": ["Tudor Manole", "Daniel K. Mark", "Wenjie Gong", "Bingtian Ye", "Yury Polyanskiy", "Soonwon Choi"], "title": "How much can we learn from quantum random circuit sampling?", "comment": "The first two authors contributed equally", "summary": "Benchmarking quantum devices is a foundational task for the sustained\ndevelopment of quantum technologies. However, accurate in situ characterization\nof large-scale quantum devices remains a formidable challenge: such systems\nexperience many different sources of errors, and cannot be simulated on\nclassical computers. Here, we introduce new benchmarking methods based on\nrandom circuit sampling (RCS), that substantially extend the scope of\nconventional approaches. Unlike existing benchmarks that report only a single\nquantity--the circuit fidelity--our framework extracts rich diagnostic\ninformation, including spatiotemporal error profiles, correlated and contextual\nerrors, and biased readout errors, without requiring any modifications of the\nexperiment. Furthermore, we develop techniques that achieve this task without\nclassically intractable simulations of the quantum circuit, by leveraging side\ninformation, in the form of bitstring samples obtained from reference quantum\ndevices. Our approach is based on advanced high-dimensional statistical\nmodeling of RCS data. We sharply characterize the information-theoretic limits\nof error estimation, deriving matching upper and lower bounds on the sample\ncomplexity across all regimes of side information. We identify surprising phase\ntransitions in learnability as the amount of side information varies. We\ndemonstrate our methods using publicly available RCS data from a\nstate-of-the-art superconducting processor, obtaining in situ characterizations\nthat are qualitatively consistent yet quantitatively distinct from\ncomponent-level calibrations. Our results establish both practical benchmarking\nprotocols for current and future quantum computers and fundamental\ninformation-theoretic limits on how much can be learned from RCS data."}
{"id": "2510.09664", "categories": ["cs.LG", "cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.09664", "abs": "https://arxiv.org/abs/2510.09664", "authors": ["Changchang Sun", "Vickie Chen", "Yan Yan"], "title": "Semantic-Cohesive Knowledge Distillation for Deep Cross-modal Hashing", "comment": null, "summary": "Recently, deep supervised cross-modal hashing methods have achieve compelling\nsuccess by learning semantic information in a self-supervised way. However,\nthey still suffer from the key limitation that the multi-label semantic\nextraction process fail to explicitly interact with raw multimodal data, making\nthe learned representation-level semantic information not compatible with the\nheterogeneous multimodal data and hindering the performance of bridging\nmodality gap. To address this limitation, in this paper, we propose a novel\nsemantic cohesive knowledge distillation scheme for deep cross-modal hashing,\ndubbed as SODA. Specifically, the multi-label information is introduced as a\nnew textual modality and reformulated as a set of ground-truth label prompt,\ndepicting the semantics presented in the image like the text modality. Then, a\ncross-modal teacher network is devised to effectively distill cross-modal\nsemantic characteristics between image and label modalities and thus learn a\nwell-mapped Hamming space for image modality. In a sense, such Hamming space\ncan be regarded as a kind of prior knowledge to guide the learning of\ncross-modal student network and comprehensively preserve the semantic\nsimilarities between image and text modality. Extensive experiments on two\nbenchmark datasets demonstrate the superiority of our model over the\nstate-of-the-art methods."}
{"id": "2510.10685", "categories": ["gr-qc", "astro-ph.CO", "hep-th"], "pdf": "https://arxiv.org/pdf/2510.10685", "abs": "https://arxiv.org/abs/2510.10685", "authors": ["Teodora Maria Matei", "Cristian Croitoru", "Tiberiu Harko"], "title": "Big Bang Nucleosynthesis constraints on space-time noncommutativity", "comment": "19 pages, 6 figures, accepted for publication in EPJC", "summary": "We consider the implications of the modified dispersion relations, due to the\nnoncommutativity of the spacetime, for a photon gas filling the early Universe\nin the framework of the Big Bang Nucleosynthesis (BBN) processes, during the\nperiod of light elements formation. We consider three types of deformations\npresent in the dispersion relations for the radiation gas, from which we obtain\nthe low temperature corrections to the energy density and pressure. The\ncosmological implications of the modified equations of state in the BBN era are\nexplored in detail for all radiation models. The effects induced on the\nnucleosynthesis process by spacetime noncommutativity are investigated by\nevaluating the abundances of relic nuclei (Hydrogen, Deuterium, Helium-3,\nHelium-4, and Lithium-7). The primordial mass fraction estimates and their\ndeviations due to changes in the freezing temperature impose an upper limit on\nthe energy density of the deformed photon gas, which follows from the modified\nFriedmann equations. The deviations from the standard energy density of the\nradiative plasma are therefore constrained by the abundances of the Helium-4\nnuclei. Upper limits on the free parameters of the spacetime noncommutativity\nare obtained via a numerical analysis performed using the \\texttt{PRyMordial}\nsoftware package. The primordial abundances of the light elements are obtained\nby evaluating the thermonuclear reaction rates for the considered\nnoncommutative spacetime models. An MCMC (Markov Chain Monte Carlo) analysis\nallows to obtain restrictions on the free parameters of the modified dispersion\nrelations. The numerical and statistical approach is implemented in the python\ncode \\texttt{PRyNCe}, available on GitHub."}
{"id": "2510.10483", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.10483", "abs": "https://arxiv.org/abs/2510.10483", "authors": ["Narayan S Iyer", "Bivas Bhaumik", "Ram S Iyer", "Satyasaran Changdar"], "title": "Gradient Enhanced Self-Training Physics-Informed Neural Network (gST-PINN) for Solving Nonlinear Partial Differential Equations", "comment": null, "summary": "Partial differential equations (PDEs) provide a mathematical foundation for\nsimulating and understanding intricate behaviors in both physical sciences and\nengineering. With the growing capabilities of deep learning, data$-$driven\napproaches like Physics$-$Informed Neural Networks (PINNs) have been developed,\noffering a mesh$-$free, analytic type framework for efficiently solving PDEs\nacross a wide range of applications. However, traditional PINNs often struggle\nwith challenges such as limited precision, slow training dynamics, lack of\nlabeled data availability, and inadequate handling of multi$-$physics\ninteractions. To overcome these challenging issues of PINNs, we proposed a\nGradient Enhanced Self$-$Training PINN (gST$-$PINN) method that specifically\nintroduces a gradient based pseudo point self$-$learning algorithm for solving\nPDEs. We tested the proposed method on three different types of PDE problems\nfrom various fields, each representing distinct scenarios. The effectiveness of\nthe proposed method is evident, as the PINN approach for solving the Burgers$'$\nequation attains a mean square error (MSE) on the order of $10^{-3}$, while the\ndiffusion$-$sorption equation achieves an MSE on the order of $10^{-4}$ after\n12,500 iterations, with no further improvement as the iterations increase. In\ncontrast, the MSE for both PDEs in the gST$-$PINN model continues to decrease,\ndemonstrating better generalization and reaching an MSE on the order of\n$10^{-5}$ after 18,500 iterations. Furthermore, the results show that the\nproposed purely semi$-$supervised gST$-$PINN consistently outperforms the\nstandard PINN method in all cases, even when solution of the PDEs are\nunavailable. It generalizes both PINN and Gradient$-$enhanced PINN (gPINN), and\ncan be effectively applied in scenarios prone to low accuracy and convergence\nissues, particularly in the absence of labeled data."}
{"id": "2510.09931", "categories": ["quant-ph", "cs.CC"], "pdf": "https://arxiv.org/pdf/2510.09931", "abs": "https://arxiv.org/abs/2510.09931", "authors": ["Chaitanya Karamchedu", "Matthew Fox", "Daniel Gottesman"], "title": "Bounds on Eventually Universal Quantum Gate Sets", "comment": "11 pages, submitted to QIP 2026", "summary": "Say a collection of $n$-qu$d$it gates $\\Gamma$ is eventually universal if and\nonly if there exists $N_0 \\geq n$ such that for all $N \\geq N_0$, one can\napproximate any $N$-qu$d$it unitary to arbitrary precision by a circuit over\n$\\Gamma$. In this work, we improve the best known upper bound on the smallest\n$N_0$ with the above property. Our new bound is roughly $d^4n$, where $d$ is\nthe local dimension (the `$d$' in qu$d$it), whereas the previous bound was\nroughly $d^8n$. For qubits ($d = 2$), our result implies that if an $n$-qubit\ngate set is eventually universal, then it will exhibit universality when acting\non a $16n$ qubit system, as opposed to the previous bound of a $256n$ qubit\nsystem. In other words, if adding just $15n$ ancillary qubits to a quantum\nsystem (as opposed to the previous bound of $255 n$ ancillary qubits) does not\nboost a gate set to universality, then no number of ancillary qubits ever will.\nOur proof relies on the invariants of finite linear groups as well as a\nclassification result for all finite groups that are unitary $2$-designs."}
{"id": "2510.09665", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09665", "abs": "https://arxiv.org/abs/2510.09665", "authors": ["Yihua Cheng", "Yuhan Liu", "Jiayi Yao", "Yuwei An", "Xiaokun Chen", "Shaoting Feng", "Yuyang Huang", "Samuel Shen", "Kuntai Du", "Junchen Jiang"], "title": "LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference", "comment": null, "summary": "Today's LLM inference systems treat individual engines and queries\nindependently for simplicity, but this causes significant resource\ninefficiencies. While there are proposals to avoid redundant computation by\nreusing KV caches across queries and to increase GPU utilization by\ndisaggregating a single query to different engines, their promises cannot be\nrealized without efficiently offloading and communicating KV cache across LLM\ninference engines and queries.\n  We present LMCache, the first and so far the most efficient open-source KV\ncaching solution, which extracts and stores KV caches generated by modern LLM\nengines (vLLM and SGLang) and shares the KV caches across engines and queries.\nLMCache exposes KV caches in the LLM engine interface, effectively transforming\nLLM engines from individual token processors to a collection of engines with KV\ncache as the storage and communication medium. In particular, it supports both\ncache offloading (prefix reuse across queries) and prefill-decode\ndisaggregation (cross-engine cache transfer). LMCache's high performance and\nwide adoption stem from the following contributions: highly optimized KV cache\ndata movement with performance optimizations including batched data movement\noperations, compute and I/O pipelining; a modular KV cache connector component,\ndecoupling LMCache from the rapid evolution of inference engines; a first-class\ncontrol API, such as pinning, lookup, cleanup, movement, and compression, for\nflexible cache orchestration across GPU, CPU, storage, and network layers.\nEvaluation shows that combining LMCache with vLLM achieves up to 15x\nimprovement in throughput across diverse workloads. With a growing community,\nLMCache has seen dramatic growth in adoption by enterprise inference systems,\nwhich provides valuable lessons for future KV caching solutions. The source\ncode of LMCache is at: https://github.com/LMCache/LMCache."}
{"id": "2510.10727", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.10727", "abs": "https://arxiv.org/abs/2510.10727", "authors": ["Anupama B"], "title": "Thermal gravitational waves from warm inflation", "comment": "8 pages, 2 figures", "summary": "For the first time, the possibility of generation of thermal gravitational\nwaves from warm inflation is investigated with cosmic microwave background.\nGravitons produced from the quantum fluctuations during warm inflation are\nfound to carry the thermal features if they exist in a thermal squeezed vacuum\nstate. Thermal squeezing reduces the amplitude of the BB mode angular power\nspectrum of CMB when compared with the conventional cold inflation. This work\ninspects the thermal nature of tensor power spectrum from warm inflation and\nthe relevance of quantum fluctuations over the thermal fluctuations. The dust\ncorrected lensed BB modes from joint analysis of $Planck$ and BICEP2/$Keck$ for\nwarm inflation provide compelling evidence for the existence of thermal\ngravitons."}
{"id": "2510.11209", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.11209", "abs": "https://arxiv.org/abs/2510.11209", "authors": ["Nicola Alboré", "Gabriele Di Antonio", "Fabrizio Coccetti", "Andrea Gabrielli"], "title": "Cross-Scale Reservoir Computing for large spatio-temporal forecasting and modeling", "comment": null, "summary": "We propose a new reservoir computing method for forecasting high-resolution\nspatiotemporal datasets. By combining multi-resolution inputs from coarser to\nfiner layers, our architecture better captures both local and global dynamics.\nApplied to Sea Surface Temperature data, it outperforms standard parallel\nreservoir models in long-term forecasting, demonstrating the effectiveness of\ncross-layers coupling in improving predictive accuracy. Finally, we show that\nthe optimal network dynamics in each layer become increasingly linear,\nrevealing the slow modes propagated to subsequent layers."}
{"id": "2510.10058", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10058", "abs": "https://arxiv.org/abs/2510.10058", "authors": ["Leonardo Rossetti", "Stefano Mancini", "Andreas Winter", "Joseph Schindler"], "title": "Observational entropy of quantum correlations and entanglement", "comment": "26 pages, 3 figures", "summary": "The use of coarse graining to connect physical and information theoretic\nentropies has recently been given a precise formulation in terms of\n``observational entropy'', describing entropy for observers with respect to a\nmeasurement. Here we consider observers with various locality restrictions,\nincluding local measurements (LO), measurements based on local operations with\nclassical communication (LOCC), and separable measurements (SEP), with the idea\nthat the ``entropy gap'' between the minimum locally measured observational\nentropy and the von Neumann entropy quantifies quantum correlations in a given\nstate. After introducing entropy gaps for general classes of measurements and\nderiving their general properties, we specialize to LO, LOCC, SEP and other\nmeasurement classes related to the locality of subsystems. For those, we show\nthat the entropy gap can be related to well-known measures of entanglement or\nnon-classicality of the state (even though we point out that they are not\nentanglement monotones themselves). In particular, for bipartite pure states,\nall of the ``local'' entropy gaps reproduce the entanglement entropy, and for\ngeneral multipartite states they are lower-bounded by the relative entropy of\nentanglement. The entropy gaps of the different measurement classes are\nordered, and we show that in general (mixed and multipartite states) they are\nall different."}
{"id": "2510.09666", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09666", "abs": "https://arxiv.org/abs/2510.09666", "authors": ["Aditya Chakravarty"], "title": "Spatial Uncertainty Quantification in Wildfire Forecasting for Climate-Resilient Emergency Planning", "comment": null, "summary": "Climate change is intensifying wildfire risks globally, making reliable\nforecasting critical for adaptation strategies. While machine learning shows\npromise for wildfire prediction from Earth observation data, current approaches\nlack uncertainty quantification essential for risk-aware decision making. We\npresent the first systematic analysis of spatial uncertainty in wildfire spread\nforecasting using multimodal Earth observation inputs. We demonstrate that\npredictive uncertainty exhibits coherent spatial structure concentrated near\nfire perimeters. Our novel distance metric reveals high-uncertainty regions\nform consistent 20-60 meter buffer zones around predicted firelines - directly\napplicable for emergency planning. Feature attribution identifies vegetation\nhealth and fire activity as primary uncertainty drivers. This work enables more\nrobust wildfire management systems supporting communities adapting to\nincreasing fire risk under climate change."}
{"id": "2510.10811", "categories": ["gr-qc", "math.AP"], "pdf": "https://arxiv.org/pdf/2510.10811", "abs": "https://arxiv.org/abs/2510.10811", "authors": ["Allen Juntao Fang", "Elena Giorgi", "Jingbo Wan"], "title": "Mass-Centered GCM Framework in Perturbations of Kerr(-Newman)", "comment": null, "summary": "The nonlinear stability problem for black hole solutions of the Einstein\nequations critically depends on choosing an appropriate geometric gauge. In the\nvacuum setting, the use of Generally Covariant Modulated (GCM) spheres and\nhypersurfaces has played a central role in the proof of stability for slowly\nrotating Kerr spacetime. In this work, we develop an alternative GCM framework,\nthat we call mass-centered, designed to overcome the breakdown of the standard\nGCM construction in the charged case, where electromagnetic-gravitational\ncoupling destroys the exceptional behavior of the $\\ell=1$ mode of the\ncenter-of-mass quantity used in the vacuum analysis. This construction is aimed\nat the nonlinear stability of Reissner-Nordstr\\\"om and Kerr-Newman spacetimes.\nOur approach replaces transport-based control of the center-of-mass quantity\nwith a sphere-wise vanishing condition on a renormalized $\\ell=1$ mode,\nyielding mass-centered GCM hypersurfaces with modified gauge constraints. The\nresulting elliptic-transport system remains determined once an $\\ell=1$ basis\nis fixed via effective uniformization and provides an alternative construction\nin vacuum in the uncharged limit."}
{"id": "2510.10064", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10064", "abs": "https://arxiv.org/abs/2510.10064", "authors": ["Guoan Li", "Xiaofan Shi", "Ruixuan Zhang", "Yuxiao Song", "Marco Rossi", "Ghada Badawy", "Zhiyuan Zhang", "Anqi Wang", "Xingchen Guo", "Xiao Deng", "Xiao Chen", "Liangqian Xu", "Bingbing Tong", "Peiling Li", "Xiaohui Song", "Zhaozheng Lyu", "Guangtong Liu", "Fanming Qu", "Michał P. Nowak", "Paweł Wójcik", "Ziwei Dou", "Erik P. A. M. Bakkers", "Li Lu", "Jie Shen"], "title": "Broad nonlocal spectrum in the Pb-InSb hybrid three terminals for potential realization of Kitaev chains", "comment": null, "summary": "Hybrid superconductor-semiconductor(SC-SM) nanowires remain one of the\nforemost platforms for engineering topological superconductivity and Majorana\nzero modes(MZMs) towards fault-tolerant topological qubits, especially with the\nrapid development of artificial Kitaev chains. In contrast to the widely used\naluminum(Al)-based hybrids, lead(Pb) offers a bulk superconducting gap of\n~1.4meV and a critical temperature of ~7.2K, giving rise to a proximity-induced\ngap that is roughly five times larger than that obtained with Al. Here we\npresent the first three-terminal Pb-hybrid devices and perform nonlocal\ndifferential-conductance spectroscopy on this platform. The nonlocal\nmeasurement simultaneously resolves a dual-gap feature of the parent Pb gap and\nthe large, hard, gate-tunable induced superconducting gap, distinguished by a\nswitch between electron- and hole-like dissipation processes. Within the\ninduced gap we observe several types of Andreev bound states(ABSs) that undergo\nsinglet-doublet transitions. Moreover, by tuning gate voltages we achieve\ngate-controlled resonating sign reversals of the nonlocal conductance,\nidentifying three distinct regimes that correspond to different configurations\nof quantum-dot(QD) resonances(single-resonance, double-resonance, and\nseries-resonance). Finally, the coupling between ABSs and QDs also present and\ncan be modulated from the weak- to strong-coupling limit, indicating the\nfeasibility of realizing the artificial Kitaev chains. Crucially, the robust\nnonlocal signatures persist up to temperatures(~1K) far above the operating\ntemperature of Al-based devices thanks to the unusually large induced gap,\nthereby widening the accessible parameter space greatly and underscoring the\nsuitability of Pb-based hybrids for implementing warm temperature artificial\nKitaev chains and the topological quantum devices protected by a substantially\nlarger topological gap."}
{"id": "2510.09668", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2510.09668", "abs": "https://arxiv.org/abs/2510.09668", "authors": ["Maryam Abdollahi Shamami", "Babak Teimourpour", "Farshad Sharifi"], "title": "A Hybrid Computational Intelligence Framework with Metaheuristic Optimization for Drug-Drug Interaction Prediction", "comment": null, "summary": "Drug-drug interactions (DDIs) are a leading cause of preventable adverse\nevents, often complicating treatment and increasing healthcare costs. At the\nsame time, knowing which drugs do not interact is equally important, as such\nknowledge supports safer prescriptions and better patient outcomes. In this\nstudy, we propose an interpretable and efficient framework that blends modern\nmachine learning with domain knowledge to improve DDI prediction. Our approach\ncombines two complementary molecular embeddings - Mol2Vec, which captures\nfragment-level structural patterns, and SMILES-BERT, which learns contextual\nchemical features - together with a leakage-free, rule-based clinical score\n(RBScore) that injects pharmacological knowledge without relying on interaction\nlabels. A lightweight neural classifier is then optimized using a novel\nthree-stage metaheuristic strategy (RSmpl-ACO-PSO), which balances global\nexploration and local refinement for stable performance. Experiments on\nreal-world datasets demonstrate that the model achieves high predictive\naccuracy (ROC-AUC 0.911, PR-AUC 0.867 on DrugBank) and generalizes well to a\nclinically relevant Type 2 Diabetes Mellitus cohort. Beyond raw performance,\nstudies show how embedding fusion, RBScore, and the optimizer each contribute\nto precision and robustness. Together, these results highlight a practical\npathway for building reliable, interpretable, and computationally efficient\nmodels that can support safer drug therapies and clinical decision-making."}
{"id": "2510.10814", "categories": ["gr-qc", "math.AP"], "pdf": "https://arxiv.org/pdf/2510.10814", "abs": "https://arxiv.org/abs/2510.10814", "authors": ["Allen Juntao Fang", "Elena Giorgi", "Jingbo Wan"], "title": "Einstein-Maxwell Equations on Mass-Centered GCM Hypersurfaces", "comment": null, "summary": "The resolution of the nonlinear stability of black holes as solutions to the\nEinstein equations relies crucially on imposing the right geometric gauge\nconditions. In the vacuum case, the use of Generally Covariant Modulated (GCM)\nspheres and hypersurfaces has been successful in the proof of stability for\nslowly rotating Kerr spacetime. For the charged setting, our companion paper\nintroduced an alternative mass-centered GCM framework, adapted to the\nadditional difficulties of the Einstein-Maxwell system.\n  In this work, we solve the Einstein-Maxwell equations on such a mass-centered\nspacelike GCM hypersurface, which is equivalent to solving the constraint\nequations there. We control all geometric quantities of the solution in terms\nof some seed data, corresponding to the gauge-invariant fields describing\ncoupled gravitational-electromagnetic radiation in perturbations of\nReissner-Nordstr\\\"om or Kerr-Newman, first identified by the second author and\nexpected to be governed by favorable hyperbolic equations. This provides the\nfirst step toward controlling gauge-dependent quantities in the nonlinear\nstability analysis of the Reissner-Nordstr\\\"om and Kerr-Newman families."}
{"id": "2510.10091", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10091", "abs": "https://arxiv.org/abs/2510.10091", "authors": ["Pan Zeming", "Tan Naiming", "Gao Chao", "Yao Zhihai", "Wang Xiaoqian"], "title": "Spin exchange of two spin-1/2 atoms", "comment": null, "summary": "The quantum Cheshire cat effect is an important phenomenon in quantum\nmechanics that reveals the separability of physical properties from their\ncarriers. This effect transcends the classical framework whose attributes must\nbe inherently attached to objects, providing new perspectives for quantum\ninformation and precision measurement. According to the quantum Cheshire cat\neffect, we prepare a pre-selected state of a spin1/2 atomic system composed of\ntwo particles through a pre-selection process. We conduct quantum weak\nmeasurements on the spins and positions of these two atoms and extract weak\nvalues by using the method of imaginary time evolution(ITE). Subsequently, we\nperform post-selection on these two atoms and design two distinct post-selected\nstates. Initially, we calculate analytical solutions when both atoms encounter\nthese two different post-selected states separately. We also compare the\nanalytical and numerical solutions. Our research theoretically confirms the\nfeasibility of fermionic systems within bipartite quantum Cheshire cat effects\nand illustrates how delayed-choice influences quantum Cheshire cat effects in\nspin-1/2 atomic systems."}
{"id": "2510.09669", "categories": ["cs.LG", "cs.CY", "cs.SI", "physics.soc-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09669", "abs": "https://arxiv.org/abs/2510.09669", "authors": ["Jacopo Lenti", "Lorenzo Costantini", "Ariadna Fosch", "Anna Monticelli", "David Scala", "Marco Pangallo"], "title": "Population synthesis with geographic coordinates", "comment": null, "summary": "It is increasingly important to generate synthetic populations with explicit\ncoordinates rather than coarse geographic areas, yet no established methods\nexist to achieve this. One reason is that latitude and longitude differ from\nother continuous variables, exhibiting large empty spaces and highly uneven\ndensities. To address this, we propose a population synthesis algorithm that\nfirst maps spatial coordinates into a more regular latent space using\nNormalizing Flows (NF), and then combines them with other features in a\nVariational Autoencoder (VAE) to generate synthetic populations. This approach\nalso learns the joint distribution between spatial and non-spatial features,\nexploiting spatial autocorrelations. We demonstrate the method by generating\nsynthetic homes with the same statistical properties of real homes in 121\ndatasets, corresponding to diverse geographies. We further propose an\nevaluation framework that measures both spatial accuracy and practical utility,\nwhile ensuring privacy preservation. Our results show that the NF+VAE\narchitecture outperforms popular benchmarks, including copula-based methods and\nuniform allocation within geographic areas. The ability to generate geolocated\nsynthetic populations at fine spatial resolution opens the door to applications\nrequiring detailed geography, from household responses to floods, to epidemic\nspread, evacuation planning, and transport modeling."}
{"id": "2510.10836", "categories": ["gr-qc", "hep-th", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10836", "abs": "https://arxiv.org/abs/2510.10836", "authors": ["Partha Nandi", "Partha Ghose", "Francesco Petruccione"], "title": "Spinning into Quantum Geometry: Dirac and Wheeler-DeWitt Dynamics from Stochastic Helicity", "comment": "This manuscript is 13 pages long and contains 1 figure", "summary": "Spin networks in loop quantum gravity provide a kinematical picture of\nquantum geometry but lack a natural mechanism for dynamical Dirac-type\nevolution, while the Wheeler--DeWitt equation typically enters only as an\nimposed constraint. We propose a stochastic framework in which each\nspin-network edge carries helicity-resolved amplitudes -- two-state internal\nlabels that undergo Poisson-driven flips. The resulting coupled master\nequations, after analytic continuation and the introduction of a fundamental\nlength scale, generate Dirac-type dynamics on discrete geometry. At long times,\nthe same process relaxes to helicity-symmetric equilibrium states, which are\nshown to satisfy a Wheeler--DeWitt-type condition. In this way, both quantum\nevolution and the gravitational constraint emerge within a single probabilistic\nframework. Our approach thus provides a background-independent and stochastic\nroute to quantum geometry, offering an alternative to canonical quantization\nand a fresh perspective on the problem of time."}
{"id": "2510.10137", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10137", "abs": "https://arxiv.org/abs/2510.10137", "authors": ["Pietro De Checchi", "Federico Gallina", "Barbara Fresch", "Giulio G. Giusteri"], "title": "On the Noisy Road to Open Quantum Dynamics: The Place of Stochastic Hamiltonians", "comment": null, "summary": "Stochastic evolution underpins several approaches to the dynamics of open\nquantum systems, such as random modulation of Hamiltonian parameters, the\nstochastic Schr\\\"odinger equation (SSE), and the stochastic Liouville equation\n(SLE). In a stochastic formulation, the open-system problem is reduced from a\ncoupled system-environment dynamics to an effective system-only description,\nwith dissipative evolution recovered by ensemble averaging. In this work, we\naim at a self-contained and accessible presentation of these approaches to\nfurther elaborate on their common roots in essential concepts of stochastic\ncalculus and to delineate the conditions under which they are equivalent. We\nalso discuss how different formulations naturally lead to different numerical\ntime-integration schemes, better suited for either classical simulation\nplatforms, based on finite-difference approximations, or quantum algorithms,\nthat employ random unitary maps. Our analysis supplies a unified perspective\nand actionable recipes for classical and quantum implementations of stochastic\nevolution in the simulation of open quantum systems."}
{"id": "2510.09670", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.09670", "abs": "https://arxiv.org/abs/2510.09670", "authors": ["Xinlun Cheng", "Bingzhe Chen", "Joseph Choi", "Yen T. Nguyen", "Pradeep Seshadri", "Mayank Verma", "H. S. Udaykumar", "Stephen Baek"], "title": "A physics-aware deep learning model for shear band formation around collapsing pores in shocked reactive materials", "comment": null, "summary": "Modeling shock-to-detonation phenomena in energetic materials (EMs) requires\ncapturing complex physical processes such as strong shocks, rapid changes in\nmicrostructural morphology, and nonlinear dynamics of chemical reaction fronts.\nThese processes participate in energy localization at hotspots, which initiate\nchemical energy release leading to detonation. This study addresses the\nformation of hotspots in crystalline EMs subjected to weak-to-moderate shock\nloading, which, despite its critical relevance to the safe storage and handling\nof EMs, remains underexplored compared to the well-studied strong shock\nconditions. To overcome the computational challenges associated with direct\nnumerical simulations, we advance the Physics-Aware Recurrent Convolutional\nNeural Network (PARCv2), which has been shown to be capable of predicting\nstrong shock responses in EMs. We improved the architecture of PARCv2 to\nrapidly predict shear localizations and plastic heating, which play important\nroles in the weak-to-moderate shock regime. PARCv2 is benchmarked against two\nwidely used physics-informed models, namely, Fourier neural operator and neural\nordinary differential equation; we demonstrate its superior performance in\ncapturing the spatiotemporal dynamics of shear band formation. While all models\nexhibit certain failure modes, our findings underscore the importance of\ndomain-specific considerations in developing robust AI-accelerated simulation\ntools for reactive materials."}
{"id": "2510.11009", "categories": ["gr-qc", "astro-ph.CO", "hep-ph", "physics.atom-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.11009", "abs": "https://arxiv.org/abs/2510.11009", "authors": ["Jiamin Liang", "Mingqiu Li", "Yu Gao", "Wei Ji", "Sichun Sun", "Qi-Shu Yan"], "title": "Detecting gravitational waves with spin systems", "comment": "8 pages, 3 figures", "summary": "The observation of gravitational waves has opened a new window into the\nUniverse through gravitational-wave astronomy. However, high-frequency\ngravitational waves remain undetected. In this work, we propose that spin\nsystems can be employed to detect gravitational waves in this unexplored\nfrequency regime. We derive the spin's response to gravitational waves and\nidentify three distinct effects: the well-known Gertsenshtein effect, a\nmetric-induced interaction, and the gravitational spin Hall effect. We focus on\nnuclear spins and utilize nuclear magnetic resonance to enhance the\ngravitational response, leveraging the advantages of long coherence time, high\npolarization, and a small gyromagnetic ratio. The proposed experimental scheme\nis capable of probing gravitational waves in the kilohertz to gigahertz range,\nwith projected sensitivities reaching\n$\\sqrt{S_h}\\approx10^{-20}~\\mathrm{Hz}^{-1/2}$."}
{"id": "2510.10167", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10167", "abs": "https://arxiv.org/abs/2510.10167", "authors": ["Shuanping Du", "Zhaofang Bai"], "title": "Features of preparable entangled states in Gaussian quantum networks", "comment": "8 pages, 3 figures, 63 references", "summary": "Large-scale quantum networks have been employed to overcome practical\nconstraints on transmission and storage for single entangled systems. The\ndeterministic preparation of entangled states is one of the key factors for\nrealization of quantum networks. There is no efficient method to verify whether\nsingle multipartite entanglement can be prepared by multisource quantum\nnetworks. Here, we theoretically analysize under what conditions entangled\nstates can be prepared in three kinds of basic Gaussian quantum networks, named\ntriangle networks, star-shaped networks and chain-type networks. Some necessity\ncriteria are derived for all preparable entangled Gaussian states in such\nnetworks. It shows that the network structure imposes strong constraints on the\nset of preparable entangled Gaussian states, which is fundamentally different\nwith the standard single multipartite entanglement. This takes the first step\ntowards understanding network mechanism for preparing entangled Gaussian\nstates."}
{"id": "2510.09676", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09676", "abs": "https://arxiv.org/abs/2510.09676", "authors": ["Shayan Mohajer Hamidi", "En-Hui Yang", "Ben Liang"], "title": "Coupled Data and Measurement Space Dynamics for Enhanced Diffusion Posterior Sampling", "comment": null, "summary": "Inverse problems, where the goal is to recover an unknown signal from noisy\nor incomplete measurements, are central to applications in medical imaging,\nremote sensing, and computational biology. Diffusion models have recently\nemerged as powerful priors for solving such problems. However, existing methods\neither rely on projection-based techniques that enforce measurement consistency\nthrough heuristic updates, or they approximate the likelihood $p(\\boldsymbol{y}\n\\mid \\boldsymbol{x})$, often resulting in artifacts and instability under\ncomplex or high-noise conditions. To address these limitations, we propose a\nnovel framework called \\emph{coupled data and measurement space diffusion\nposterior sampling} (C-DPS), which eliminates the need for constraint tuning or\nlikelihood approximation. C-DPS introduces a forward stochastic process in the\nmeasurement space $\\{\\boldsymbol{y}_t\\}$, evolving in parallel with the\ndata-space diffusion $\\{\\boldsymbol{x}_t\\}$, which enables the derivation of a\nclosed-form posterior $p(\\boldsymbol{x}_{t-1} \\mid \\boldsymbol{x}_t,\n\\boldsymbol{y}_{t-1})$. This coupling allows for accurate and recursive\nsampling based on a well-defined posterior distribution. Empirical results\ndemonstrate that C-DPS consistently outperforms existing baselines, both\nqualitatively and quantitatively, across multiple inverse problem benchmarks."}
{"id": "2510.11075", "categories": ["gr-qc", "hep-th", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.11075", "abs": "https://arxiv.org/abs/2510.11075", "authors": ["Mainak Dutta", "Partha Nandi", "Bibhas Ranjan Majhi"], "title": "A novel quantum memory effect and thermal modulation in graviton-mediated entanglement", "comment": "Latex, 35 pages, 3 figures, 1 table", "summary": "A central challenge in probing the quantum nature of gravity is to\ndistinguish effects that are genuinely quantum from those that can be explained\nclassically. In this work, we study how quantized gravitational waves interact\nwith thermal quantum systems, modeled as harmonic oscillators. We show that,\nunlike classical waves, quantized gravitons generate entanglement and leave\nbehind a persistent ``graviton-induced quantum memory'' even after the wave has\npassed. This effect is further shaped by the presence of thermal noise, which\ndoes not simply wash out quantum correlations but can in fact amplify them in\ndistinctive ways. Our analysis reveals clear signatures - such as nonlinear\nthermal corrections and a prethermal time-crystal-like phase-that cannot arise\nfrom any classical treatment. These results identify experimentally relevant\nmarkers of gravitons and provide a framework for exploring how\nfinite-temperature environments may help uncover the quantum nature of gravity."}
{"id": "2510.10187", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10187", "abs": "https://arxiv.org/abs/2510.10187", "authors": ["Shuo Dai", "Zeqing Wang", "Liang-Liang Wan", "Weidong Li", "Augusto Smerzi", "Ran Qi", "Jianwen Jie"], "title": "Universal Manipulation of Quantum Synchronization in Spin Oscillator Networks", "comment": "12 pages, 5 figures", "summary": "Quantum synchronization (QS) in open many-body systems offers a promising\nroute for controlling collective quantum dynamics, yet existing manipulation\nschemes often rely on dissipation engineering, which distorts limit cycles,\nlacks scalability, and is strongly system-dependent. Here, we propose a\nuniversal and scalable method for continuously tuning QS from maximal\nsynchronization under isotropic interactions to complete synchronization\nblockade (QSB) under fully anisotropic coupling in spin oscillator networks.\nOur approach preserves intrinsic limit cycles and applies to both few-body and\nmacroscopic systems. We analytically show that QS arises solely from spin\nflip-flop processes and their higher-order correlations, while anisotropic\ninteractions induce non-synchronizing coherence. A geometric QS measure reveals\na macroscopic QSB effect in the thermodynamic limit. The proposed mechanism is\nexperimentally feasible using XYZ interactions and optical pumping, and\nprovides a general framework for programmable synchronization control in\ncomplex quantum networks and dynamical phases of matter."}
{"id": "2510.09684", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.09684", "abs": "https://arxiv.org/abs/2510.09684", "authors": ["Chris Engh", "P. M. Aronow"], "title": "Using LLMs to Directly Guess Conditional Expectations Can Improve Efficiency in Causal Estimation", "comment": null, "summary": "We propose a simple yet effective use of LLM-powered AI tools to improve\ncausal estimation. In double machine learning, the accuracy of causal estimates\nof the effect of a treatment on an outcome in the presence of a\nhigh-dimensional confounder depends on the performance of estimators of\nconditional expectation functions. We show that predictions made by generative\nmodels trained on historical data can be used to improve the performance of\nthese estimators relative to approaches that solely rely on adjusting for\nembeddings extracted from these models. We argue that the historical knowledge\nand reasoning capacities associated with these generative models can help\novercome curse-of-dimensionality problems in causal inference problems. We\nconsider a case study using a small dataset of online jewelry auctions, and\ndemonstrate that inclusion of LLM-generated guesses as predictors can improve\nefficiency in estimation."}
{"id": "2510.11197", "categories": ["gr-qc", "astro-ph.HE", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2510.11197", "abs": "https://arxiv.org/abs/2510.11197", "authors": ["Gregory Ashton"], "title": "Reconstructing and resampling: a guide to utilising posterior samples from gravitational wave observations", "comment": "Submitted to RASTI", "summary": "The LIGO, Virgo, and KAGRA (LVK) gravitational-wave observatories have opened\nnew scientific research in astrophysics, fundamental physics, and cosmology.\nThe collaborations that build and operate these observatories release the\ninterferometric strain data as well as a catalogue of observed signals with\naccompanying Bayesian posterior distributions. These posteriors, in the form of\nequally-weighted samples, form a dataset that is used by a multitude of further\nanalyses seeking to constrain the population of merging black holes, identify\nlensed pairs of signals, and much more. However, many of these analyses rely,\noften implicitly, on the ability to reconstruct the likelihood and prior from\nthe inputs to the analysis and apply resampling (a statistical technique to\ngenerate new samples varying the underlying analysis assumptions). In this\nwork, we first provide a guide on how to reconstruct and modify the posterior\ndensity accurately from the inputs for analyses performed with the Bilby\ninference library. We then demonstrate and compare resampling techniques to\nproduce new posterior sample sets and discuss Pareto-smoothing to improve the\nefficiency. Finally, we provide examples of how to use resampling to study\nobserved gravitational-wave signals. We hope this guide provides a useful\nresource for those wishing to use open data products from the LVK for\ngravitational-wave astronomy."}
{"id": "2510.10319", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10319", "abs": "https://arxiv.org/abs/2510.10319", "authors": ["Reuven Ianconescu", "Bin Zhang", "Aharon Friedman", "Jacob Scheuer", "Avraham Gover"], "title": "On the validity of intermediate tracing in multiple quantum interactions", "comment": "13 pages, 2 figures", "summary": "Interactions between many (initially separate) quantum systems raise the\nquestion on how to prepare and how to compute the measurable results of their\ninteraction. When one prepares each system individually and let them interact,\none has to tensor multiply their density matrices and apply Hamiltonians on the\ncomposite system (i.e. the system which includes all the interacting systems)\nfor definite time intervals. Evaluating the final state of one of the systems\nafter multiple consecutive interactions, requires tracing all other systems out\nof the composite system, which may grow up to immense dimensions. For\ncomputation efficiency during the interaction(s) one may consider only the\ncontemporary interacting partial systems, while tracing out the other non\ninteracting systems. In concrete terms, the type of problems to which we direct\nthis formulation is a ``target'' system interacting {\\bf succesively} with\n``incident'' systems, where the ``incident'' systems do not mutually interact.\nFor example a two-level atom, interacting succesively with free electrons, or a\nresonant cavity interacting with radiatively free electrons, or a quantum dot\ninteracting succesively with photons. We refer to a ``system'' as one of the\ncomponents before interaction, while each interaction creates a ``composite\nsystem''. A new interaction of the ``composite system'' with another ``system''\ncreates a ``larger composite system'', unless we trace out one of the systems\nbefore this interaction. The scope of this work is to show that under proper\nconditions one may add a system to the composite system just before it\ninteracts, and one may trace out this very system after it finishes to\ninteract. We show in this work a mathematical proof of the above property and\ngive a computational example."}
{"id": "2510.09685", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NA", "math.NA", "A.1; I.2; I.4"], "pdf": "https://arxiv.org/pdf/2510.09685", "abs": "https://arxiv.org/abs/2510.09685", "authors": ["Yongshuai Liu", "Lianfang Wang", "Kuilin Qin", "Qinghua Zhang", "Faqiang Wang", "Li Cui", "Jun Liu", "Yuping Duan", "Tieyong Zeng"], "title": "Deep Neural Networks Inspired by Differential Equations", "comment": "35 Pages, 3 figures", "summary": "Deep learning has become a pivotal technology in fields such as computer\nvision, scientific computing, and dynamical systems, significantly advancing\nthese disciplines. However, neural Networks persistently face challenges\nrelated to theoretical understanding, interpretability, and generalization. To\naddress these issues, researchers are increasingly adopting a differential\nequations perspective to propose a unified theoretical framework and systematic\ndesign methodologies for neural networks. In this paper, we provide an\nextensive review of deep neural network architectures and dynamic modeling\nmethods inspired by differential equations. We specifically examine deep neural\nnetwork models and deterministic dynamical network constructs based on ordinary\ndifferential equations (ODEs), as well as regularization techniques and\nstochastic dynamical network models informed by stochastic differential\nequations (SDEs). We present numerical comparisons of these models to\nillustrate their characteristics and performance. Finally, we explore promising\nresearch directions in integrating differential equations with deep learning to\noffer new insights for developing intelligent computational methods that boast\nenhanced interpretability and generalization capabilities."}
{"id": "2510.11230", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.11230", "abs": "https://arxiv.org/abs/2510.11230", "authors": ["Bikash Chandra Paul", "Sahil Saini"], "title": "Cyclic universe and uniform rate inflation in loop quantum cosmology", "comment": null, "summary": "We investigate uniform rate inflationary universe in the framework of Loop\nquantum cosmology. The potential for uniform rate inflation in a loop quantized\nFRW spacetime turns out to be a polymerized version of the corresponding\npotential in general relativity, which mimics the potentials for a polymerized\nscalar field and that for hybrid natural inflation (HNI). The potential leads\nto a cyclic universe with identical epochs separated by quantum bounces which\nreplace the classical singularity. The energy density and Hubble rate are\nbounded. The predictions for cosmological perturbations depend on the value of\nthe field at the end of inflation. The parameter space is explored to compare\nthe results for spectral index and tensor-to-scalar ratio with observational\nconstraints."}
{"id": "2510.10333", "categories": ["quant-ph", "hep-ph"], "pdf": "https://arxiv.org/pdf/2510.10333", "abs": "https://arxiv.org/abs/2510.10333", "authors": ["J. D. Franson"], "title": "Fractional Aharonov-Bohm effect for retarded potentials", "comment": "4 pages, 2 figures", "summary": "It has been suggested that the magnetic Aharonov-Bohm effect can be\ninterpreted equally well as being due to a phase shift associated with an\nelectron in an interferometer enclosing a magnetic flux, or as a phase shift\nassociated with the electrons in the solenoid that generates the field. Here\nthe Aharonov-Bohm effect is derived using second-quantized field theory to\ndescribe all the electrons as well as the electromagnetic field in a consistent\nway. The results are in agreement with the usual expression for the\nAharonov-Bohm effect when the retardation of the electromagnetic field is\nnegligible, but they predict the possibility of a fractional phase shift when\nretardation effects are significant."}
{"id": "2510.09687", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09687", "abs": "https://arxiv.org/abs/2510.09687", "authors": ["Stanisław Pawlak"], "title": "On the Occurence of Critical Learning Periods in Neural Networks", "comment": "8 pages, 8 figures", "summary": "This study delves into the plasticity of neural networks, offering empirical\nsupport for the notion that critical learning periods and warm-starting\nperformance loss can be avoided through simple adjustments to learning\nhyperparameters. The critical learning phenomenon emerges when training is\ninitiated with deficit data. Subsequently, after numerous deficit epochs, the\nnetwork's plasticity wanes, impeding its capacity to achieve parity in accuracy\nwith models trained from scratch, even when extensive clean data training\nfollows deficit epochs. Building upon seminal research introducing critical\nlearning periods, we replicate key findings and broaden the experimental scope\nof the main experiment from the original work. In addition, we consider a\nwarm-starting approach and show that it can be seen as a form of deficit\npretraining. In particular, we demonstrate that these problems can be averted\nby employing a cyclic learning rate schedule. Our findings not only impact\nneural network training practices but also establish a vital link between\ncritical learning periods and ongoing research on warm-starting neural network\ntraining."}
{"id": "2510.11319", "categories": ["gr-qc", "hep-th"], "pdf": "https://arxiv.org/pdf/2510.11319", "abs": "https://arxiv.org/abs/2510.11319", "authors": ["Haximjan Abdusattar"], "title": "P-V Criticality and Microstructure of Hayward Black Holes in Anti-de Sitter Space", "comment": "14 pages, 10 figures. Comments are welcome!", "summary": "In this study, we investigate the $P$-$V$ criticality and Ruppeiner geometry\nin the extended phase space of Hayward anti-de Sitter (AdS) black holes.\nThrough thermodynamic analysis, we confirm that Hayward-AdS black holes undergo\ndistinct $P$-$V$ phase transitions and exhibit well-defined critical phenomena\nin the vicinity of their critical points. These behaviors are characterized by\nfour critical exponents that typically obey the scaling laws predicted by\nmean-field theory--indicating a consistent thermodynamic framework with\nclassical phase transition systems ($e.g.,$ van der Waals fluids). Furthermore,\nwe employ Ruppeiner geometry to probe the thermodynamic fluctuations of\nHayward-AdS black holes, and by calculating the corresponding curvature scalar,\nwe gain direct insights into the interaction nature of the black hole's\nmicroscopic constituents."}
{"id": "2510.10334", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10334", "abs": "https://arxiv.org/abs/2510.10334", "authors": ["Hamza Harraf", "Mohamed Amazioug", "Amjad Sohail", "Rachid Ahl Laamara"], "title": "Monogamy of Gaussian quantum steering and entanglement in a hybrid qubit-cavity optomagnonic system with coherent feedback loop", "comment": "13 pages, 7 figures", "summary": "The monogamy of quantum correlations is a fundamental principle in quantum\ninformation processing, limiting how quantum correlations can be shared among\nmultiple subsystems. Here we propose a theoretical scheme to investigate the\nmonogamy of quantum steering and genuine tripartite entanglement in a hybrid\nqubit-cavity optomagnonic system with a coherent feedback loop. Using\nlogarithmic negativity and Gaussian quantum steering, we quantify entanglement\nand steerability, respectively. We verify the CKW-type monogamy inequalities\nwhich leads to steering monogamous through adjustments of the reflective\nparameter among three tripartite modes versus temperature. Our results show\nthat a coherent feedback loop can enhance entanglement and quantum steering\nunder thermal effects."}
{"id": "2510.09691", "categories": ["cs.LG", "cs.AI", "68T07, 68M14", "I.2.6; I.2.11; K.6.5"], "pdf": "https://arxiv.org/pdf/2510.09691", "abs": "https://arxiv.org/abs/2510.09691", "authors": ["Tejash Varsani"], "title": "Evaluation of Differential Privacy Mechanisms on Federated Learning", "comment": "Supervised by Prof. Dr.-Ing. habil. Alois C. Knoll; Advisor:\n  Nagacharan Teja Tangirala, M.Sc", "summary": "Federated learning is distributed model training across several clients\nwithout disclosing raw data. Despite advancements in data privacy, risks still\nremain. Differential Privacy (DP) is a technique to protect sensitive data by\nadding noise to model updates, usually controlled by a fixed privacy budget.\nHowever, this approach can introduce excessive noise, particularly when the\nmodel converges, which compromises performance. To address this problem,\nadaptive privacy budgets have been investigated as a potential solution. This\nwork implements DP methods using Laplace and Gaussian mechanisms with an\nadaptive privacy budget, extending the SelecEval simulator. We introduce an\nadaptive clipping approach in the Gaussian mechanism, ensuring that gradients\nof the model are dynamically updated rather than using a fixed sensitivity. We\nconduct extensive experiments with various privacy budgets, IID and non-IID\ndatasets, and different numbers of selected clients per round. While our\nexperiments were limited to 200 training rounds, the results suggest that\nadaptive privacy budgets and adaptive clipping can help maintain model accuracy\nwhile preserving privacy."}
{"id": "2510.11364", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.11364", "abs": "https://arxiv.org/abs/2510.11364", "authors": ["Anjan Kar", "Ayan Dey", "Sayan Kar"], "title": "A note on the Penrose process in rotating regular black holes", "comment": "18 pages, 9 figures", "summary": "We investigate the Penrose process of energy extraction in the context of\nrotating regular black holes. For the Neves-Saa class of regular black hole\nsolutions, which includes the Bardeen, Hayward and Fan-Wang spacetimes as\nspecial cases, the extraction efficiency is bounded above by the known value\nfor Kerr spacetime. However, in the case of a new rotating regular black hole\nwhich does not have a Schwarzschild singular limit for zero rotation, the\nextraction efficiency can indeed become very large, as shown in our analysis\nhere."}
{"id": "2510.10345", "categories": ["quant-ph", "cond-mat.stat-mech", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.10345", "abs": "https://arxiv.org/abs/2510.10345", "authors": ["Elijah Pelofske"], "title": "Depth One Quantum Alternating Operator Ansatz as an Approximate Gibbs Distribution Sampler", "comment": null, "summary": "This study numerically investigates the thermal sampling properties of QAOA,\nthe Quantum Alternating Operator Ansatz which was generalized from the original\nQuantum Approximate Optimization Algorithm. Specifically, the ability of QAOA\nto sample from the Gibbs distribution, equivalently the Boltzmann distribution,\ndefined by a classical Ising model, specifically a fully connected disordered\nspin glass (Sherrington-Kirkpatrick) model. We focus on two different QAOA\nmixers; the standard transverse field X mixer, and the Grover mixer. At a QAOA\ndepth of one we examine, for a single full QAOA parameter search space period,\nthe energy landscape, the Shannon entropy landscape of the QAOA probability\ndistribution, and the tradeoff between Boltzmann distribution sampling\ntemperature and error rate (how close to the true Boltzmann distribution is the\nQAOA distribution). We find that at very high temperatures one-round Grover\nmixer QAOA can sample from the Boltzmann distribution more accurately than the\nstandard X mixer QAOA at one round. Both X mixer and Grover mixer depth one\nQAOA can serve as approximate Boltzmann distribution samplers, and how good\nthis approximation is depends heavily on the QAOA angle choice."}
{"id": "2510.09693", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.09693", "abs": "https://arxiv.org/abs/2510.09693", "authors": ["Jiakang Chen"], "title": "Neural PDE Solvers with Physics Constraints: A Comparative Study of PINNs, DRM, and WANs", "comment": "50 pages, 13 figures", "summary": "Partial differential equations (PDEs) underpin models across science and\nengineering, yet analytical solutions are atypical and classical mesh-based\nsolvers can be costly in high dimensions. This dissertation presents a unified\ncomparison of three mesh-free neural PDE solvers, physics-informed neural\nnetworks (PINNs), the deep Ritz method (DRM), and weak adversarial networks\n(WANs), on Poisson problems (up to 5D) and the time-independent Schr\\\"odinger\nequation in 1D/2D (infinite well and harmonic oscillator), and extends the\nstudy to a laser-driven case of Schr\\\"odinger's equation via the\nKramers-Henneberger (KH) transformation.\n  Under a common protocol, all methods achieve low $L_2$ errors\n($10^{-6}$-$10^{-9}$) when paired with forced boundary conditions (FBCs),\nforced nodes (FNs), and orthogonality regularization (OG). Across tasks, PINNs\nare the most reliable for accuracy and recovery of excited spectra; DRM offers\nthe best accuracy-runtime trade-off on stationary problems; WAN is more\nsensitive but competitive when weak-form constraints and FN/OG are used\neffectively. Sensitivity analyses show that FBC removes boundary-loss tuning,\nnetwork width matters more than depth for single-network solvers, and most\ngains occur within 5000-10,000 epochs. The same toolkit solves the KH case,\nindicating transfer beyond canonical benchmarks.\n  We provide practical guidelines for method selection and outline the\nfollowing extensions: time-dependent formulations for DRM and WAN, adaptive\nresidual-driven sampling, parallel multi-state training, and neural domain\ndecomposition. These results support physics-guided neural solvers as credible,\nscalable tools for solving complex PDEs."}
{"id": "2510.11406", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.11406", "abs": "https://arxiv.org/abs/2510.11406", "authors": ["Jose Luis Blázquez-Salcedo", "Luis Manuel González-Romero", "Fech Scen Khoo", "Jutta Kunz", "Pablo Navarro Moreno"], "title": "Charged wormholes can be long-lived", "comment": "5 pages, 3 figures", "summary": "Ellis-Bronnikov wormholes suffer from an unstable radial mode. Here we\ninvestigate the evolution of the unstable mode(s) for charged wormholes. We\nshow that the instability remains in the presence of charge, but exhibits a\nvery fast decrease to zero, suggesting that wormholes that approach a\nnear-extremal metric could be long-lived. For so-called supercritical\nwormholes, two purely imaginary unstable modes merge and continue with\ndegenerate imaginary parts and opposite real parts. By analogy, we conjecture\nan analogous behavior for rotating wormholes."}
{"id": "2510.10351", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10351", "abs": "https://arxiv.org/abs/2510.10351", "authors": ["Toshiaki Kanai", "Chuanwei Zhang"], "title": "Electron Lateral Trapping Induced by Non-Uniform Thickness in Solid Neon Layers", "comment": "7 pages, 4 figures", "summary": "Recent experimental advances highlight electron charge qubits floating above\nsolid neon as an emerging promising platform for quantum computing, but the\nphysical origin of single-electron lateral trapping is still not fully\nunderstood. While prior theoretical work has mainly examined electrons above\nbulk solid neon, experimental systems usually feature neon layers of only\n$\\lesssim 10$ nm thickness and non-uniformity, highlighting unresolved\nquestions about how thickness influences electron trapping. Here we\ntheoretically investigate the effect of finite thickness and non-uniformity of\nsolid neon layers on electron trapping. For a 10 nm layer, the electron binding\nenergy is enhanced threefold compared to bulk. Exploiting this thickness\ndependence, we propose a nanopatterned-substrate mechanism in which engineered\nthickness variations generate lateral trapping potentials for electrons. The\nlateral trapping potential can be finely tuned by a perpendicular electric\nfield. Such non-uniform-thickness induced electron charge qubits open a viable\npathway toward building multi-qubit systems for quantum computation."}
{"id": "2510.09694", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09694", "abs": "https://arxiv.org/abs/2510.09694", "authors": ["Xiaodan Li", "Mengjie Wu", "Yao Zhu", "Yunna Lv", "YueFeng Chen", "Cen Chen", "Jianmei Guo", "Hui Xue"], "title": "Kelp: A Streaming Safeguard for Large Models via Latent Dynamics-Guided Risk Detection", "comment": null, "summary": "Large models (LMs) are powerful content generators, yet their open-ended\nnature can also introduce potential risks, such as generating harmful or biased\ncontent. Existing guardrails mostly perform post-hoc detection that may expose\nunsafe content before it is caught, and the latency constraints further push\nthem toward lightweight models, limiting detection accuracy. In this work, we\npropose Kelp, a novel plug-in framework that enables streaming risk detection\nwithin the LM generation pipeline. Kelp leverages intermediate LM hidden states\nthrough a Streaming Latent Dynamics Head (SLD), which models the temporal\nevolution of risk across the generated sequence for more accurate real-time\nrisk detection. To ensure reliable streaming moderation in real applications,\nwe introduce an Anchored Temporal Consistency (ATC) loss to enforce monotonic\nharm predictions by embedding a benign-then-harmful temporal prior. Besides,\nfor a rigorous evaluation of streaming guardrails, we also present\nStreamGuardBench-a model-grounded benchmark featuring on-the-fly responses from\neach protected model, reflecting real-world streaming scenarios in both text\nand vision-language tasks. Across diverse models and datasets, Kelp\nconsistently outperforms state-of-the-art post-hoc guardrails and prior plug-in\nprobes (15.61% higher average F1), while using only 20M parameters and adding\nless than 0.5 ms of per-token latency."}
{"id": "2510.11460", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.11460", "abs": "https://arxiv.org/abs/2510.11460", "authors": ["Vladimir Toussaint"], "title": "Exponential Suppression of the Unruh Effect and Geometric Enhancement in a Fermionic Cavity QED Setup", "comment": "38 pages", "summary": "The Unruh effect -- the prediction that an accelerated observer perceives the\nvacuum as a thermal bath -- remains one of the most profound yet experimentally\nunverified consequences of quantum field theory. This work analyzes the decay\nof an excited state within a uniformly accelerated cavity to explain the\nhistorical null results and identify an alternative measurable signature. We\nmodel a massless Dirac field confined to a uniformly accelerated cavity,\ncoupled to an external massive Dirac field of mass $M$. Our analysis reveals\nthat for fundamental particles with mass $M$, the condition $Mc^2 \\gg \\hbar\na/c$ is satisfied for all achievable accelerations, placing the system in an\nexponentially suppressed decay regime ($\\Gamma_{\\text{acc}} \\sim\ne^{-2Mc^2/(\\hbar a/c)}$) that holds universally across all cavity sizes,\nexplaining why direct observation of Unruh effects has remained elusive.\nHowever, for intermediate-sized cavities ($a l\\sim c^2$) with light external\nfields ($Mc^2 \\ll \\hbar a/c$), we identify a geometric enhancement of the decay\nrate, scaling as $\\Gamma_{\\text{acc}}\\sim \\Gamma_{\\text{in}} \\frac{a\nl/c^2}{\\ln(1+a l/c^2)}$ (with $\\Gamma_{\\text{in}}$ the inertial decay rate),\nwhich arises from non-inertial acceleration effects rather than thermal\nstimulation. This geometric enhancement, reaching up to 26\\% for realistic\nparameters ($a\\sim 10^{20}~\\text{m/s}^2$, $l\\sim 100-500~\\mu\\text{m}$),\nprovides a measurable signature accessible through quantum simulation\nplatforms. Our results provide a unified theoretical framework that explains\nhistorical null results while offering a viable path toward detecting\nnon-inertial quantum effects in accelerated systems."}
{"id": "2510.10394", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10394", "abs": "https://arxiv.org/abs/2510.10394", "authors": ["Man Yin Cheung", "Mona Berciu", "Kyle Monkman"], "title": "Spectrally controlled dissipation in a target subsystem", "comment": null, "summary": "We present a microscopic Hamiltonian time-evolution on an ancilla+target\nsystem that evolves the target to a steady state. Using this description, we\ndemonstrate the potential of a spectrally controllable dissipator: Depending on\nthe energy scale of the target, the subsystem reaches the intended steady state\nor remains partially trapped in the initial state. For a steady state which is\npure, this protocol can function as an autonomous qubit reset. We can also\nchoose a mixed steady state so that this functions as a tunable mixed-state\npreparation. With a particular dissipative condition, we guarantee the\nequilibration to the steady-state with spectral measure theory. This type of\nspectral control over dissipation is not present in the common Lindblad\ndescription of open systems. Our construction establishes a new design\nprinciple for engineered dissipation and opens a pathway toward tunable\nautonomous quantum control."}
{"id": "2510.09696", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09696", "abs": "https://arxiv.org/abs/2510.09696", "authors": ["Lorenzo Nikiforos", "Charalampos Antoniadis", "Luciano Prono", "Fabio Pareschi", "Riccardo Rovatti", "Gianluca Setti"], "title": "Vanishing Contributions: A Unified Approach to Smoothly Transition Neural Models into Compressed Form", "comment": "Code available at https://github.com/foros15/vanishing-contributions", "summary": "The increasing scale of deep neural networks has led to a growing need for\ncompression techniques such as pruning, quantization, and low-rank\ndecomposition. While these methods are very effective in reducing memory,\ncomputation and energy consumption, they often introduce severe accuracy\ndegradation when applied directly. We introduce Vanishing Contributions (VCON),\na general approach for smoothly transitioning neural models into compressed\nform. Rather than replacing the original network directly with its compressed\nversion, VCON executes the two in parallel during fine-tuning. The contribution\nof the original (uncompressed) model is progressively reduced, while that of\nthe compressed model is gradually increased. This smooth transition allows the\nnetwork to adapt over time, improving stability and mitigating accuracy\ndegradation. We evaluate VCON across computer vision and natural language\nprocessing benchmarks, in combination with multiple compression strategies.\nAcross all scenarios, VCON leads to consistent improvements: typical gains\nexceed 3%, while some configuration exhibits accuracy boosts of 20%. VCON thus\nprovides a generalizable method that can be applied to the existing compression\ntechniques, with evidence of consistent gains across multiple benchmarks."}
{"id": "2510.11487", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.11487", "abs": "https://arxiv.org/abs/2510.11487", "authors": ["Sara Rastgoo", "Foad Parsaei"], "title": "Non-exotic wormholes in $f(R,L_m)$ gravity", "comment": null, "summary": "In the present analysis, we examine the potential existence of generalized\nwormhole models within the framework of newly developed extended $f(R,L_m)$\ngravity. We investigate both a linear model, $f(R,L_m)=\\alpha R+\\beta L_m$, and\na non-linear model, $f(R,L_m)=\\frac{R}{2}+ L^\\alpha_m$, to analyze traversable\nwormholes. By employing the variational approach, we derive modified versions\nof the field equations under the influence of an anisotropic matter source. A\npower-law shape function is applied, resulting in a linear equation of state\nfor both radial and lateral pressures. Furthermore, we explore solutions\ncharacterized by a variable equation of state parameter. It was observed that\nthe violation of energy conditions is influenced by the parameters $\\alpha$ and\n$\\beta$. A wide range of non-exotic wormhole solutions was discovered,\ndependent on the specific parameters of the model. We demonstrate that\nwormholes with power-law shape functions yield solutions that comply with the\nenergy conditions in both linear and non-linear forms of $f(R, L_m)$. It is\nshown that the non-exotic wormhole solutions obtained within this framework are\nnot isotropic."}
{"id": "2510.10495", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10495", "abs": "https://arxiv.org/abs/2510.10495", "authors": ["Jungsoo Hong", "Seong Ho Kim", "Seung Kyu Min", "Joonsuk Huh"], "title": "Oscillator-qubit generalized quantum signal processing for vibronic models: a case study of uracil cation", "comment": "12 pages, 9 figures", "summary": "Hybrid oscillator-qubit processors have recently demonstrated high-fidelity\ncontrol of both continuous- and discrete-variable information processing.\nHowever, most of the quantum algorithms remain limited to homogeneous quantum\narchitectures. Here, we present a compiler for hybrid oscillator-qubit\nprocessors, implementing state preparation and time evolution. In hybrid\noscillator-qubit processors, this compiler invokes generalized quantum signal\nprocessing (GQSP) to constructively synthesize arbitrary bosonic phase gates\nwith moderate circuit depth O(log(1/{\\varepsilon})). The approximation cost is\nscaled by the Fourier bandwidth of the target bosonic phase, rather than by the\ndegree of nonlinearity. Armed with GQSP, nonadiabatic molecular dynamics can be\ndecomposed with arbitrary-phase potential propagators. Compared to fully\ndiscrete encodings, our approach avoids the overhead of truncating continuous\nvariables, showing linear dependence on the number of vibration modes while\ntrading success probability for circuit depth. We validate our method on the\nuracil cation, a canonical system whose accurate modeling requires anharmonic\nvibronic models, estimating the cost for state preparation and time evolution."}
{"id": "2510.09704", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09704", "abs": "https://arxiv.org/abs/2510.09704", "authors": ["Matthew Schlegel", "Matthew E. Taylor", "Mostafa Farrokhabadi"], "title": "Operator Learning for Power Systems Simulation", "comment": null, "summary": "Time domain simulation, i.e., modeling the system's evolution over time, is a\ncrucial tool for studying and enhancing power system stability and dynamic\nperformance. However, these simulations become computationally intractable for\nrenewable-penetrated grids, due to the small simulation time step required to\ncapture renewable energy resources' ultra-fast dynamic phenomena in the range\nof 1-50 microseconds. This creates a critical need for solutions that are both\nfast and scalable, posing a major barrier for the stable integration of\nrenewable energy resources and thus climate change mitigation. This paper\nexplores operator learning, a family of machine learning methods that learn\nmappings between functions, as a surrogate model for these costly simulations.\nThe paper investigates, for the first time, the fundamental concept of\nsimulation time step-invariance, which enables models trained on coarse time\nsteps to generalize to fine-resolution dynamics. Three operator learning\nmethods are benchmarked on a simple test system that, while not incorporating\npractical complexities of renewable-penetrated grids, serves as a first\nproof-of-concept to demonstrate the viability of time step-invariance. Models\nare evaluated on (i) zero-shot super-resolution, where training is performed on\na coarse simulation time step and inference is performed at super-resolution,\nand (ii) generalization between stable and unstable dynamic regimes. This work\naddresses a key challenge in the integration of renewable energy for the\nmitigation of climate change by benchmarking operator learning methods to model\nphysical systems."}
{"id": "2510.11518", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.11518", "abs": "https://arxiv.org/abs/2510.11518", "authors": ["Yu-Song Cao", "YanXia Liu", "Ding-Fang Zeng"], "title": "Gravitational wave echos from physical black holes", "comment": "17 pages, 4 figures. Comments are welcome!", "summary": "Gravitational wave echos from the coalescence of black hole binaries are\noften viewed as signals beyond general relativity or standard model. In this\nwork, we show that these echos are inevitable in the black holes coalescence\ndescribed by standard general relativity. This is because it is the physical\nblack holes formed through gravitational collapse serve as the true description\nof astronomical black holes. For physical black holes, only their asymptotic\nstructure before the horizon forms are detectible to the outside probes. Here,\nby investigating the scattering of a gravitational wave burst on a physical\nblack hole and pay special attention to the echos in the waveform, we uncover\ndistinct features of the echos both in the time and frequency domains."}
{"id": "2510.10501", "categories": ["quant-ph", "hep-ph"], "pdf": "https://arxiv.org/pdf/2510.10501", "abs": "https://arxiv.org/abs/2510.10501", "authors": ["Heechan Yi", "Kayoung Ban", "Myeonghun Park", "Kyoungchul Kong"], "title": "Quantum Integration Networks for Efficient Monte Carlo in High-Energy Physics", "comment": null, "summary": "Monte Carlo methods play a central role in particle physics, where they are\nindispensable for simulating scattering processes, modeling detector responses,\nand performing multi-dimensional integrals. However, traditional Monte Carlo\nmethods often suffer from slow convergence and insufficient precision,\nparticularly for functions with singular features such as rapidly varying\nregions or narrow peaks. Quantum circuits provide a promising alternative:\ncompared to conventional neural networks, they can achieve rich expressivity\nwith fewer parameters, and the parameter-shift rule provides an exact analytic\nform for circuit gradients, ensuring precise optimization. Motivated by these\nadvantages, we investigate how sampling strategies and loss functions affect\nintegration efficiency within the \\textbf{Quantum Integration Network}\n(QuInt-Net). We compare adaptive and non-adaptive sampling approaches and\nexamine the impact of different loss functions on accuracy and convergence.\nFurthermore, we explore three quantum circuit architectures for numerical\nintegration: the data re-uploading model, the quantum signal processing\nprotocol, and deterministic quantum computation with one qubit. The results\nprovide new insights into optimizing QuInt-Nets for applications in high energy\nphysics."}
{"id": "2510.09705", "categories": ["cs.LG", "cs.CY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09705", "abs": "https://arxiv.org/abs/2510.09705", "authors": ["Sudip Khadka", "L. S. Paudel"], "title": "A Multi-Component Reward Function with Policy Gradient for Automated Feature Selection with Dynamic Regularization and Bias Mitigation", "comment": null, "summary": "Static feature exclusion strategies often fail to prevent bias when hidden\ndependencies influence the model predictions. To address this issue, we explore\na reinforcement learning (RL) framework that integrates bias mitigation and\nautomated feature selection within a single learning process. Unlike\ntraditional heuristic-driven filter or wrapper approaches, our RL agent\nadaptively selects features using a reward signal that explicitly integrates\npredictive performance with fairness considerations. This dynamic formulation\nallows the model to balance generalization, accuracy, and equity throughout the\ntraining process, rather than rely exclusively on pre-processing adjustments or\npost hoc correction mechanisms. In this paper, we describe the construction of\na multi-component reward function, the specification of the agents action space\nover feature subsets, and the integration of this system with ensemble\nlearning. We aim to provide a flexible and generalizable way to select features\nin environments where predictors are correlated and biases can inadvertently\nre-emerge."}
{"id": "2510.11569", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.11569", "abs": "https://arxiv.org/abs/2510.11569", "authors": ["A. Sheykhi", "A. Asvar", "E. Ebrahimi"], "title": "Note on Kaniadakis Holographic Dark Energy", "comment": "21 pages, 11 figures", "summary": "It is well-known that any modification to the entropy expression not only\nchange the energy density of the holographic dark energy, but also modifies the\ncosmological field equations through thermodynamics-gravity correspondence.\nHere we revisit the Kaniadakis holographic dark energy (KHDE) in the background\nof the modified Kaniadakis cosmology by incorporating the effects of Kaniadakis\nentropy into the Friedmann equations. We choose the Hubble radius, $L=H^{-1}$,\nas system's IR cutoff and determine the cosmological implications of this\nmodel. We first consider a dark energy (DE) dominated universe and reveal that\nthis model mimics the cosmological constant with $w_{DE}=-1$. This implies that\nthe theoretical origin of the cosmological constant, $\\Lambda$, may be\nunderstood through KHDE in the context of Kaniadakis cosmology. Remarkably, we\nobserve that in the absence of interaction between DE and dark matter (DM), and\nin contrast to HDE in standard cosmology, our model can explain the current\nacceleration of the cosmic expansion for the Hubble radius as IR cutoff. When\nthe interaction between DE and DM is taken into account, we see that the total\nequation of state parameter (EoS), $w_{tot}=p_{tot}/\\rho_{tot}$ can cross the\nphantom line at the present time. We also analyze the squared speed of sound,\n$v_s^2$, for this model and find out that $(v_s^2<0)$ for interacting KHDE.\nInvestigating the statefinder, confirms the distinction between KHDE and\n$\\Lambda$CDM model. It is seen that the statefinder diagram move away from the\npoint of $\\left\\lbrace r,s\\right\\rbrace= \\left\\lbrace 1,0\\right\\rbrace$ with\nincreasing the interaction parameter."}
{"id": "2510.10536", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10536", "abs": "https://arxiv.org/abs/2510.10536", "authors": ["V. V. Nesvizhevsky", "J. A. Pioquinto", "K. Schreiner", "S. Baessler", "P. Crivelli", "E. Widmann"], "title": "Gravitational and other shifts of neutron, hydrogen, antihydrogen, muonium, and positronium whispering gallery and gravitational state interference patterns", "comment": null, "summary": "Recently, a shift of a neutron whispering-gallery interference pattern due to\nan external magnetic field gradient was measured. By analogy, a similar\nphenomenon can be observed with other particles and forces. In particular, a\ngravitational shift of the neutron whispering gallery can be easily observed\nwith cold or very cold or ultracold neutrons, and the developed methods can be\nused for observing/searching for other shifts in fundamental neutron physics\nexperiments, for instance, for measuring the gravitational constant or\nconstraining the neutron electric charge. A peculiar feature of analogous\natomic (anti-atomic) experiments is the much smaller effective critical\nenergies of the materials of atomic (anti-atomic) mirrors. We evaluated\nparameters that make a measurement of the hydrogen and antihydrogen whispering\ngallery and their gravitational shifts feasible. A series of such measurements\nwill be made with neutrons at the PF1B/PF2/D17 facilities at the ILL, as well\nas with hydrogen or/and deuterium atoms by the GRASIAN collaboration in Vienna\nand Turku. Such a measurement with antihydrogen atoms may be of interest for\nthe GBAR experiment, the ASACUSA experiment which is producing a beam of slow\nantihydrogen atoms, or other experiments at CERN, which study the gravitational\nproperties of antimatter. Quantum reflection of muonium and positronium from\nmaterial surfaces opens the possibility of observing whispering-galley states,\nalthough such measurements remain experimentally challenging. The observation\nof gravitational shifts is particularly demanding because of the extremely\nshort lifetimes of these systems. Measurements of whispering gallery with all\nthese atoms and particles yield unique information on the quantum reflection\nproperties at surfaces, providing valuable input for both fundamental and\nsurface studies."}
{"id": "2510.09712", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09712", "abs": "https://arxiv.org/abs/2510.09712", "authors": ["Zhao Tong", "Chunlin Gong", "Yimeng Gu", "Haichao Shi", "Qiang Liu", "Shu Wu", "Xiao-Yu Zhang"], "title": "Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments", "comment": "10 pages, 12 figures", "summary": "The spread of fake news online distorts public judgment and erodes trust in\nsocial media platforms. Although recent fake news detection (FND) models\nperform well in standard settings, they remain vulnerable to adversarial\ncomments-authored by real users or by large language models (LLMs)-that subtly\nshift model decisions. In view of this, we first present a comprehensive\nevaluation of comment attacks to existing fake news detectors and then\nintroduce a group-adaptive adversarial training strategy to improve the\nrobustness of FND models. To be specific, our approach comprises three steps:\n(1) dividing adversarial comments into three psychologically grounded\ncategories: perceptual, cognitive, and societal; (2) generating diverse,\ncategory-specific attacks via LLMs to enhance adversarial training; and (3)\napplying a Dirichlet-based adaptive sampling mechanism (InfoDirichlet Adjusting\nMechanism) that dynamically adjusts the learning focus across different comment\ncategories during training. Experiments on benchmark datasets show that our\nmethod maintains strong detection accuracy while substantially increasing\nrobustness to a wide range of adversarial comment perturbations."}
{"id": "2510.11643", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.11643", "abs": "https://arxiv.org/abs/2510.11643", "authors": ["Alexey Dubinsky"], "title": "Scattering and Absorption of Standard Model Fields by Brane-Localized Schwarzschild--de Sitter Black Holes", "comment": null, "summary": "We investigate the propagation and absorption of Standard Model fields --\nscalar, electromagnetic, and Dirac -- on a 3+1-dimensional brane embedded in a\nhigher-dimensional Schwarzschild--de Sitter (SdS) spacetime. Using the\neffective four-dimensional projection of the Tangherlini metric, we compute\ngrey-body factors (GBFs) and absorption cross-sections for each spin sector by\nmeans of the sixth-order WKB method and, independently, via the recently\nproposed correspondence between quasinormal modes (QNMs) and transmission\ncoefficients. The results demonstrate that the cosmological constant and field\nmass crucially affect the transmission probabilities: increasing $\\Lambda$\nlowers the potential barrier and enhances the transparency of the geometry,\nwhile a larger field mass $\\mu$ suppresses low-frequency emission and shifts\nthe absorption spectrum to higher energies. For all fields, the QNM--GBF\ncorrespondence proves reliable to within about one percent for multipoles $\\ell\n\\ge 2$, while the correspondence remains less accurate for the lowest\nmultipoles. The total absorption cross-sections exhibit the expected transition\nfrom the low-frequency suppression to the geometric-optics regime. Overall,\nthese findings provide quantitative insight into the interplay between\ndimensionality, cosmological expansion, and black-hole radiation on the brane."}
{"id": "2510.10538", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10538", "abs": "https://arxiv.org/abs/2510.10538", "authors": ["Amit Te'eni", "Yaron Oz", "Eliahu Cohen"], "title": "One-Query Quantum Algorithms for the Index-$q$ Hidden Subgroup Problem", "comment": "13 pages", "summary": "The quantum Fourier transform (QFT) is central to many quantum algorithms,\nyet its necessity is not always well understood. We re-examine its role in\ncanonical query problems. The Deutsch-Jozsa algorithm requires neither a QFT\nnor a domain group structure. In contrast, the Bernstein-Vazirani problem is an\ninstance of the hidden subgroup problem (HSP), where the hidden subgroup has\neither index $1$ or $2$; and the Bernstein-Vazirani algorithm exploits this\npromise to solve the problem with a single query. Motivated by these insights,\nwe introduce the index-$q$ HSP: determine whether a hidden subgroup $H \\le G$\nhas index $1$ or $q$, and, when possible, identify $H$. We present a\nsingle-query algorithm that always distinguishes index $1$ from $q$, for any\nchoice of abelian structure on the oracle's codomain. Moreover, with suitable\npre- and post-oracle unitaries (inverse-QFT/QFT over $G$), the same query\nexactly identifies $H$ under explicit minimal conditions: $G/H$ is cyclic of\norder $q$, and the output alphabet admits a faithful, compatible group\nstructure. These conditions hold automatically for $q \\in \\left\\{ 2,3\n\\right\\}$, giving unconditional single-query identification in these cases. In\ncontrast, the Shor-Kitaev sampling approach cannot guarantee exact recovery\nfrom a single sample. Our results sharpen the landscape of one-query quantum\nsolvability for abelian HSPs."}
{"id": "2510.09717", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09717", "abs": "https://arxiv.org/abs/2510.09717", "authors": ["Zhenlong Liu", "Hao Zeng", "Weiran Huang", "Hongxin Wei"], "title": "High-Power Training Data Identification with Provable Statistical Guarantees", "comment": null, "summary": "Identifying training data within large-scale models is critical for copyright\nlitigation, privacy auditing, and ensuring fair evaluation. The conventional\napproaches treat it as a simple binary classification task without statistical\nguarantees. A recent approach is designed to control the false discovery rate\n(FDR), but its guarantees rely on strong, easily violated assumptions. In this\npaper, we introduce Provable Training Data Identification (PTDI), a rigorous\nmethod that identifies a set of training data with strict false discovery rate\n(FDR) control. Specifically, our method computes p-values for each data point\nusing a set of known unseen data, and then constructs a conservative estimator\nfor the data usage proportion of the test set, which allows us to scale these\np-values. Our approach then selects the final set of training data by\nidentifying all points whose scaled p-values fall below a data-dependent\nthreshold. This entire procedure enables the discovery of training data with\nprovable, strict FDR control and significantly boosted power. Extensive\nexperiments across a wide range of models (LLMs and VLMs), and datasets\ndemonstrate that PTDI strictly controls the FDR and achieves higher power."}
{"id": "2510.11685", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2510.11685", "abs": "https://arxiv.org/abs/2510.11685", "authors": ["Hector Iglesias", "Leanne Durkan", "Deirdre Shoemaker"], "title": "Hybridization of second-order gravitational self-force and numerical relativity waveforms for quasi-circular and non-spinning black hole binaries", "comment": null, "summary": "In the past few decades, the waveform community has made advances in\nproducing waveforms that span the inspiral-merger-ringdown of\ncomparable-mass-ratio black hole binaries using advances in post-Newtonian and\nnumerical relativity (NR) theory along with state-of-the-art gravitational wave\nmodels. Current methods in NR have shown progress towards producing stable\nsimulations reaching mass ratios of 1:100; however, the computational cost\nbecomes prohibitively expensive as the mass ratio and the length of the\nsimulation increases. Meanwhile, the gravitational self-force (GSF) community\nhas developed waveform models that not only generate extreme mass ratio\ninspiral waveforms, but also generate near-equal-mass-ratio waveforms with high\nfidelity. To assess the limits of both the GSF and NR waveforms and alleviate\nthe computational costs of NR, we present hybridized GSF-NR waveforms for\nnon-spinning binary black hole systems in which GSF provides the inspiral, and\nNR the merger and ringdown. The hybrid waveforms are generated from a set of 68\nnon-spinning NR waveforms from the SXS catalogue with mass ratios spanning 1:1\nto 1:20 and include the (2,2), (2,1), (3,3), (3,2), (4,4), (4,3), and (5,5)\nspin-weighted spherical harmonic modes. In this paper, we will highlight a\nselection of these hybrid waveforms and examine the error in the hybridization\nprocedure. We will investigate the impact of subdominant modes on the accuracy\nof the hybrid waveforms by performing mismatch comparisons with surrogate\nmodels. To address the feasibility of hybridizing GSF inspirals with short,\nhigh-mass ratio NR waveforms, thereby alleviating computational costs, we will\ndiscuss the relationship between mass ratio and the placement of the matching\nwindow, which can be used to predict the necessary and optimal number of NR\ncycles that contribute to the hybrid waveform."}
{"id": "2510.10565", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10565", "abs": "https://arxiv.org/abs/2510.10565", "authors": ["Pankaj K. Jha", "Lakshya Nagpal", "Amir Targholizadeh", "Utkarsh Mishra", "Konstantin E. Dorfman"], "title": "Beating the standard quantum limit with single-photon-added coherent states", "comment": null, "summary": "The standard quantum limit (SQL), also known as the shot-noise limit, defines\nhow quantum fluctuations of light constrain measurement precision. In a\nbenchmark experiment using the Mach-Zehnder interferometer (MZI), where a\ncoherent state with the average photon number $\\langle n\\rangle$ is combined\nwith an ordinary vacuum input, the SQL for the phase uncertainty is given by\nthe well-known relation $\\Delta\\varphi_{\\text{SQL}} = 1/\\langle n\\rangle$.\nUsing a single photon-added coherent state and a weak coherent state as inputs,\nwe report an enhanced phase sensitivity in MZI surpassing the SQL. In stark\ncontrast to other approaches, we focus on the low-photon-number regime,\n$\\langle n\\rangle < 10$, and demonstrate that our scheme offers better phase\nsensitivity compared to the SQL. Beating the SQL at low photon numbers paves\nthe way for the new generation of devices employed in \\textquotedblleft\nphoton-starved\\textquotedblright quantum sensing, spectroscopy, and metrology."}
{"id": "2510.09718", "categories": ["cs.LG", "68T05, 68T10", "I.5.3; I.2.11"], "pdf": "https://arxiv.org/pdf/2510.09718", "abs": "https://arxiv.org/abs/2510.09718", "authors": ["A. Jung"], "title": "Federated k-Means via Generalized Total Variation Minimization", "comment": null, "summary": "We consider the problem of federated clustering, where interconnected devices\nhave access to private local datasets and need to jointly cluster the overall\ndataset without sharing their local dataset. Our focus is on hard clustering\nbased on the k-means principle. We formulate federated k-means clustering as an\ninstance of GTVMin. This formulation naturally lends to a federated k-means\nalgorithm where each device updates local cluster centroids by solving a\nmodified local k-means problem. The modification involves adding a penalty term\nto measure the discrepancy between the cluster centroid of neighbouring\ndevices. Our federated k-means algorithm is privacy-friendly as it only\nrequires sharing aggregated information among interconnected devices."}
{"id": "2510.10566", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2510.10566", "abs": "https://arxiv.org/abs/2510.10566", "authors": ["Naoki Maruyama", "Masayuki Ohzeki", "Kazuyuki Tanaka"], "title": "Uniformity Bias in Ground-State Sampling Induced by Replica Alignment in Quantum Monte Carlo for Quantum Annealing", "comment": null, "summary": "Quantum annealing (QA) with a transverse field often fails to sample\ndegenerate ground states fairly, limiting applicability to problems requiring\ndiverse optimal solutions. Although Quantum Monte Carlo (QMC) is widely used to\nsimulate QA, its ability to reproduce such unfair ground-state sampling remains\nunclear because stochastic and coherent quantum dynamics differ fundamentally.\nWe quantitatively evaluate how accurately QMC reproduces the sampling bias in\nQA by comparing the final ground-state distributions from the QMC master\nequation and the Schr\\\"odinger equation. We find QMC tends to produce uniform\nground-state probabilities, unlike QA's biased distribution, and that this\nuniformity bias strengthens as annealing proceeds. Our analysis reveals that\nthis bias originates from replica alignment -- the dominance of configurations\nin which all Trotter replicas coincide -- caused by the energetic suppression\nand entropic reduction of kink configurations (replica mismatches). These\nfindings clarify a fundamental limitation of discrete-time QMC in faithfully\nsimulating QA dynamics, highlighting the importance of replica correlations and\ntransition rules in achieving realistic ground-state sampling."}
{"id": "2510.09719", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09719", "abs": "https://arxiv.org/abs/2510.09719", "authors": ["Chenxu Wang", "Hao Li", "Yiqun Zhang", "Linyao Chen", "Jianhao Chen", "Ping Jian", "Peng Ye", "Qiaosheng Zhang", "Shuyue Hu"], "title": "ICL-Router: In-Context Learned Model Representations for LLM Routing", "comment": null, "summary": "Large language models (LLMs) often exhibit complementary strengths. Model\nrouting harnesses these strengths by dynamically directing each query to the\nmost suitable model, given a candidate model pool. However, routing performance\nrelies on accurate model representations, and adding new models typically\nrequires retraining, limiting scalability. To address these challenges, we\npropose a novel routing method using in-context vectors to represent model\ncapabilities. The method proceeds in two stages. First, queries are embedded\nand projected into vectors, with a projector and LLM-based router trained to\nreconstruct the original queries, aligning vector representations with the\nrouter's semantic space. Second, each candidate model is profiled on a query\nset, and the router learns -- based on in-context vectors of query and model\nperformance -- to predict whether each model can correctly answer new queries.\nExtensive experiments demonstrate that our method achieves state-of-the-art\nrouting performance in both in-distribution and out-of-distribution tasks.\nMoreover, our method allows for seamless integration of new models without\nretraining the router. The code is available at\nhttps://github.com/lalalamdbf/ICL-Router."}
{"id": "2510.10629", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10629", "abs": "https://arxiv.org/abs/2510.10629", "authors": ["Vladislav Popkov", "Mario Salerno"], "title": "Liouvillian Exceptional Points in Quantum Brickwork Circuits", "comment": "5 pages, 5 figures + Supplemental Material", "summary": "We provide the first systematic demonstration that Liouvillian exceptional\npoints (LEPs)and their associated sensing properties, previously studied only\nin continuous Lindbladian dynamics, also emerge in discrete brickwork CPTP\ncircuits, the natural stroboscopic framework of present day quantum devices."}
{"id": "2510.09723", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T05 (Primary), 68T50", "I.2.6; I.2.7; I.5.4"], "pdf": "https://arxiv.org/pdf/2510.09723", "abs": "https://arxiv.org/abs/2510.09723", "authors": ["Gregory D. Baker"], "title": "It's 2025 -- Narrative Learning is the new baseline to beat for explainable machine learning", "comment": "18 pages, 5 figures", "summary": "In this paper, we introduce Narrative Learning, a methodology where models\nare defined entirely in natural language and iteratively refine their\nclassification criteria using explanatory prompts rather than traditional\nnumerical optimisation. We report on experiments to evaluate the accuracy and\npotential of this approach using 3 synthetic and 3 natural datasets and compare\nthem against 7 baseline explainable machine learning models. We demonstrate\nthat on 5 out of 6 of these datasets, Narrative Learning became more accurate\nthan the baseline explainable models in 2025 or earlier because of improvements\nin language models. We also report on trends in the lexicostatistics of these\nmodels' outputs as a proxy for the comprehensibility of the explanations."}
{"id": "2510.10632", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2510.10632", "abs": "https://arxiv.org/abs/2510.10632", "authors": ["Zhao-Fan Cai", "Tao Liu"], "title": "Quantum-Squeezing-Induced Algebraic Non-Hermitian Skin Effects and Ultra Spectral Sensitivity", "comment": "16 pages, 6 figures", "summary": "The well-established non-Bloch band theory predicts exponential localization\nof skin-mode eigenstates in one-dimensional (1D) non-Hermitian systems. Recent\nstudies, however, have uncovered anomalous algebraic localization in higher\ndimensions. Here, we extend these ideas to Hermitian bosonic quadratic\nHamiltonians incorporating quantum squeezing, offering a genuine quantum\nframework to explore non-Hermitian phenomena without external reservoirs. We\nconstruct a two-dimensional (2D) bosonic lattice model with two-mode squeezing\nand study its spectral properties of bosonic excitation within the\nBogoliubov-de Gennes (BdG) formalism. We demonstrate an algebraic non-Hermitian\nskin effect (NHSE), characterized by quasi-long-range power-law localization of\ncomplex eigenstates. The system shows ultra spectral sensitivity to double\ninfinitesimal on-site and long-range hopping impurities, while remaining\ninsensitive to single impurities. Analytical treatment via the Green's function\nreveals that this sensitivity originates from the divergence of the nonlocal\nGreen's function associated with the formation of nonlocal bound states between\nimpurities. Our study establishes a framework for realizing novel\nhigher-dimensional non-Hermitian physics in Hermitian bosonic platforms such as\nsuperconducting circuits, photonic lattices, and optomechanical arrays, with\nthe demonstrated ultraspectral sensitivity enabling quantum sensing and\namplification via bosonic squeezing."}
{"id": "2510.09732", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09732", "abs": "https://arxiv.org/abs/2510.09732", "authors": ["P. van Oerle", "R. H. Bemthuis", "F. A. Bukhsh"], "title": "Evaluating LLM-Based Process Explanations under Progressive Behavioral-Input Reduction", "comment": "12 pages, 2 figures, 3 tables; to appear in Enterprise Design,\n  Operations, and Computing. EDOC 2025 Workshops, Lecture Notes in Business\n  Information Processing (LNBIP), Springer, 2025. Part of 29th International\n  Conference on Enterprise Design, Operations, and Computing (EDOC)", "summary": "Large Language Models (LLMs) are increasingly used to generate textual\nexplanations of process models discovered from event logs. Producing\nexplanations from large behavioral abstractions (e.g., directly-follows graphs\nor Petri nets) can be computationally expensive. This paper reports an\nexploratory evaluation of explanation quality under progressive\nbehavioral-input reduction, where models are discovered from progressively\nsmaller prefixes of a fixed log. Our pipeline (i) discovers models at multiple\ninput sizes, (ii) prompts an LLM to generate explanations, and (iii) uses a\nsecond LLM to assess completeness, bottleneck identification, and suggested\nimprovements. On synthetic logs, explanation quality is largely preserved under\nmoderate reduction, indicating a practical cost-quality trade-off. The study is\nexploratory, as the scores are LLM-based (comparative signals rather than\nground truth) and the data are synthetic. The results suggest a path toward\nmore computationally efficient, LLM-assisted process analysis in\nresource-constrained settings."}
{"id": "2510.10699", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10699", "abs": "https://arxiv.org/abs/2510.10699", "authors": ["Murat Can Karakoc", "Ozgun Ersoy", "Ahmad Salmanoghli Khiavi", "Asaf Behzat Sahin"], "title": "Quantum Radar: An Engineering Perspective", "comment": "29 pages, 17 figures", "summary": "Quantum radar has emerged as a promising paradigm that utilizes entanglement\nand quantum correlations to overcome the limitations of classical detection in\nnoisy and lossy environments. By exploiting microwave entanglement generated\nfrom superconducting devices such as Josephson parametric amplifiers,\nconverters, and traveling-wave parametric amplifiers, quantum radar systems can\nachieve enhanced detection sensitivity, lower error probabilities, and greater\nrobustness against thermal noise and jamming. This review provides a\ncomprehensive overview of the field, beginning with the theoretical foundations\nof quantum illumination and extending to the generation of entanglement in the\nmicrowave regime. We then examine key quantum radar subsystems, including\nquantum transducers, amplification chains, and receiver architectures, which\nform the backbone of practical designs. Recent experimental systems are\nsurveyed in the microwave domain, highlighting proof-of-principle\ndemonstrations and their transition from conceptual frameworks to laboratory\nrealizations. Collectively, the progress reviewed here demonstrates that\nquantum radar is evolving from a theoretical construct to a practical quantum\ntechnology capable of extending the performance boundaries of classical radar."}
{"id": "2510.09734", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09734", "abs": "https://arxiv.org/abs/2510.09734", "authors": ["Jindong Tian", "Yifei Ding", "Ronghui Xu", "Hao Miao", "Chenjuan Guo", "Bin Yang"], "title": "ARROW: An Adaptive Rollout and Routing Method for Global Weather Forecasting", "comment": "16 pages, 6 figures, conference", "summary": "Weather forecasting is a fundamental task in spatiotemporal data analysis,\nwith broad applications across a wide range of domains. Existing data-driven\nforecasting methods typically model atmospheric dynamics over a fixed short\ntime interval (e.g., 6 hours) and rely on naive autoregression-based rollout\nfor long-term forecasting (e.g., 138 hours). However, this paradigm suffers\nfrom two key limitations: (1) it often inadequately models the spatial and\nmulti-scale temporal dependencies inherent in global weather systems, and (2)\nthe rollout strategy struggles to balance error accumulation with the capture\nof fine-grained atmospheric variations. In this study, we propose ARROW, an\nAdaptive-Rollout Multi-scale temporal Routing method for Global Weather\nForecasting. To contend with the first limitation, we construct a\nmulti-interval forecasting model that forecasts weather across different time\nintervals. Within the model, the Shared-Private Mixture-of-Experts captures\nboth shared patterns and specific characteristics of atmospheric dynamics\nacross different time scales, while Ring Positional Encoding accurately encodes\nthe circular latitude structure of the Earth when representing spatial\ninformation. For the second limitation, we develop an adaptive rollout\nscheduler based on reinforcement learning, which selects the most suitable time\ninterval to forecast according to the current weather state. Experimental\nresults demonstrate that ARROW achieves state-of-the-art performance in global\nweather forecasting, establishing a promising paradigm in this field."}
{"id": "2510.10707", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10707", "abs": "https://arxiv.org/abs/2510.10707", "authors": ["Sören Arlt", "Mario Krenn", "Xuemei Gu"], "title": "Automated discovery of high-dimensional multipartite entanglement with photons that never interacted", "comment": "11 pages, 5 figures; SI: 4 pages,5 figures", "summary": "Quantum entanglement across spatially separated network nodes is\nconventionally established through the distribution of photons from a common\nsource or via entanglement swapping that relies on Bell-state measurements and\npre-shared entanglement. Path identity, where the emission origins of photons\nfrom different sources are made indistinguishable, offers an alternative route.\nWe show that this mechanism enables complex multipartite, high-dimensional, and\neven logical entanglement between remote nodes whose photons never interacted.\nOur schemes require neither direct photon interaction, pre-shared entanglement,\nnor Bell-state measurements, highlighting a distinct resource for distributed\nquantum communication and computation. All of the solutions were discovered\nautomatically using highly efficient computational design tools, indicating the\npotential for scientific inspiration from computational algorithms."}
{"id": "2510.09735", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09735", "abs": "https://arxiv.org/abs/2510.09735", "authors": ["Qianyou Sun", "Jiexin Zheng", "Bohan Jin", "Lihua Chen", "Yijie Peng"], "title": "InterCorpRel-LLM: Enhancing Financial Relational Understanding with Graph-Language Models", "comment": null, "summary": "Identifying inter-firm relationships such as supply and competitive ties is\ncritical for financial analysis and corporate governance, yet remains\nchallenging due to the scale, sparsity, and contextual dependence of corporate\ndata. Graph-based methods capture structure but miss semantic depth, while\nlarge language models (LLMs) excel at text but remain limited in their ability\nto represent relational dependencies. To address this, we propose\nInterCorpRel-LLM, a cross-modal framework that integrates GNNs with LLMs,\nsupported by a proprietary dataset derived from FactSet supply chain records\nand three tailored training tasks: company graph matching, industry\nclassification, and supply relation prediction. This design enables effective\njoint modeling of structure and semantics. Experiments show that\nInterCorpRel-LLM substantially outperforms strong baselines, including GPT-5,\non a supply relation identification task, achieving an F-score of 0.8543 vs.\n0.2287 with only a 7B-parameter backbone and lightweight training. The model\nalso generalizes to zero-shot competitor identification, underscoring its\nability to capture nuanced inter-firm dynamics. Our framework thus provides\nanalysts and strategists with a robust tool for mapping and reasoning about\ncomplex corporate networks, enhancing decision-making and risk management in\ndynamic markets."}
{"id": "2510.10711", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10711", "abs": "https://arxiv.org/abs/2510.10711", "authors": ["Zhen Wu", "Si-Qi Zhou"], "title": "Extreme Capacities in Generalized Direct Sum Channels", "comment": "7 pages, 2 figures for main text, 20 pages for supplementary material", "summary": "Quantum channel capacities play a central role in quantum Shannon theory, a\nformalism built upon rigorous coding theorems for noisy channels. Evaluating\nexact capacity values for general quantum channels remains intractable due to\nsuperadditivity. As a step toward understanding this phenomenon, we construct\nthe generalized direct sum (GDS) channel, extending conventional direct sum\nchannels through a direct sum structure in their Kraus operators. This\nconstruction forms the basis of the GDS framework, encompassing classes of\nchannels with single-letter formula for quantum capacities and others\nexhibiting striking capacity features. The quantum capacity can approach zero\nyet display unbounded superadditivity combined with erasure channels. Private\nand classical capacities coincide and can become arbitrarily large, resulting\nin an unbounded gap with the quantum capacity. Providing a simpler and more\nintuitive approach, the framework deepens our understanding of quantum channel\ncapacities."}
{"id": "2510.09739", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.09739", "abs": "https://arxiv.org/abs/2510.09739", "authors": ["Ayoub Bouguettaya", "Elizabeth M. Stuart"], "title": "Machine learning methods fail to provide cohesive atheoretical construction of personality traits from semantic embeddings", "comment": "1 figure, 12 pages", "summary": "The lexical hypothesis posits that personality traits are encoded in language\nand is foundational to models like the Big Five. We created a bottom-up\npersonality model from a classic adjective list using machine learning and\ncompared its descriptive utility against the Big Five by analyzing one million\nReddit comments. The Big Five, particularly Agreeableness, Conscientiousness,\nand Neuroticism, provided a far more powerful and interpretable description of\nthese online communities. In contrast, our machine-learning clusters provided\nno meaningful distinctions, failed to recover the Extraversion trait, and\nlacked the psychometric coherence of the Big Five. These results affirm the\nrobustness of the Big Five and suggest personality's semantic structure is\ncontext-dependent. Our findings show that while machine learning can help check\nthe ecological validity of established psychological theories, it may not be\nable to replace them."}
{"id": "2510.10800", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10800", "abs": "https://arxiv.org/abs/2510.10800", "authors": ["Davide Rolino", "Paolo Perinotti", "Alessandro Tosini"], "title": "Quantum complementarity", "comment": "4 + 6 pages", "summary": "We propose an operational definition of complementarity, pinning down the\nconcept originally introduced by Bohr. Two properties of a system are\nconsidered complementary if they cannot be simultaneously well defined. We\nfurther show that, within quantum theory, this notion is equivalent to the\nincompatibility of operations -- that is, their inability to be performed\nsimultaneously."}
{"id": "2510.09740", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09740", "abs": "https://arxiv.org/abs/2510.09740", "authors": ["Atharv Goel", "Sharat Agarwal", "Saket Anand", "Chetan Arora"], "title": "Reliable Active Learning from Unreliable Labels via Neural Collapse Geometry", "comment": "Accepted to NeurIPS 2025 Workshop on Reliable ML from Unreliable Data", "summary": "Active Learning (AL) promises to reduce annotation cost by prioritizing\ninformative samples, yet its reliability is undermined when labels are noisy or\nwhen the data distribution shifts. In practice, annotators make mistakes, rare\ncategories are ambiguous, and conventional AL heuristics (uncertainty,\ndiversity) often amplify such errors by repeatedly selecting mislabeled or\nredundant samples. We propose Reliable Active Learning via Neural Collapse\nGeometry (NCAL-R), a framework that leverages the emergent geometric\nregularities of deep networks to counteract unreliable supervision. Our method\nintroduces two complementary signals: (i) a Class-Mean Alignment Perturbation\nscore, which quantifies how candidate samples structurally stabilize or distort\ninter-class geometry, and (ii) a Feature Fluctuation score, which captures\ntemporal instability of representations across training checkpoints. By\ncombining these signals, NCAL-R prioritizes samples that both preserve class\nseparation and highlight ambiguous regions, mitigating the effect of noisy or\nredundant labels. Experiments on ImageNet-100 and CIFAR100 show that NCAL-R\nconsistently outperforms standard AL baselines, achieving higher accuracy with\nfewer labels, improved robustness under synthetic label noise, and stronger\ngeneralization to out-of-distribution data. These results suggest that\nincorporating geometric reliability criteria into acquisition decisions can\nmake Active Learning less brittle to annotation errors and distribution shifts,\na key step toward trustworthy deployment in real-world labeling pipelines. Our\ncode is available at https://github.com/Vision-IIITD/NCAL."}
{"id": "2510.10808", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10808", "abs": "https://arxiv.org/abs/2510.10808", "authors": ["Francisco M. Fernández"], "title": "Sheared potentials and travelling nodes", "comment": null, "summary": "When a sheared potential is deformed in such a way that the distance between\nthe classical turning points remains constant the eigenvalues of the\nSchr\\\"{o}dinger equation oscillate with respect to the potential parameter\nresponsible for the deformation. We show that such an oscillation is intimately\nrelated to the passing of the nodes of the corresponding eigenfunctions through\nthe origin. We illustrate this effect by means of the split harmonic oscillator\nand the split linear potential."}
{"id": "2510.09752", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.09752", "abs": "https://arxiv.org/abs/2510.09752", "authors": ["Sai Krishna Reddy Mudhiganti", "Juanyan Wang", "Ruo Yang", "Manali Sharma"], "title": "Patentformer: A demonstration of AI-assisted automated patent drafting", "comment": null, "summary": "Patent drafting presents significant challenges due to its reliance on the\nextensive experience and specialized expertise of patent attorneys, who must\npossess both legal acumen and technical understanding of an invention to craft\npatent applications in a formal legal writing style. This paper presents a\ndemonstration of Patentformer, an AI-powered automated patent drafting platform\ndesigned to support patent attorneys by rapidly producing high-quality patent\napplications adhering to legal writing standards."}
{"id": "2510.10835", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10835", "abs": "https://arxiv.org/abs/2510.10835", "authors": ["Yichen Xu", "Yiqing Zhou", "James P. Sethna", "Eun-Ah Kim"], "title": "Error thresholds of toric codes with transversal logical gates", "comment": "14 pages, 10 figures. Online talk available at\n  https://online.kitp.ucsb.edu/online/stablephases25/yxu/", "summary": "The threshold theorem promises a path to fault-tolerant quantum computation\nby suppressing logical errors, provided the physical error rate is below a\ncritical threshold. While transversal gates offer an efficient method for\nimplementing logical operations, they risk spreading errors and potentially\nlowering this threshold compared to a static quantum memory. Available\nthreshold estimates for transversal circuits are empirically obtained and\nlimited to specific, sub-optimal decoders. To establish rigorous bounds on the\nnegative impact of error spreading by the transversal gates, we generalize the\nstatistical mechanical (stat-mech) mapping from quantum memories to logical\ncircuits. We establish a mapping for two toric code blocks that undergo a\ntransversal CNOT (tCNOT) gate. Using this mapping, we quantify the impact of\ntwo independent error-spreading mechanisms: the spread of physical bit-flip\nerrors and the spread of syndrome errors. In the former case, the stat-mech\nmodel is a 2D random Ashkin-Teller model. We use numerical simulation to show\nthat the tCNOT gate reduces the optimal bit-flip error threshold to $p=0.080$,\na $26\\%$ decrease from the toric code memory threshold $p=0.109$. The case of\nsyndrome error coexisting with bit-flip errors is mapped to a 3D random 4-body\nIsing model with a plane defect. There, we obtain a conservative estimate error\nthreshold of $p=0.028$, implying an even more modest reduction due to the\nspread of the syndrome error compared to the memory threshold $p=0.033$. Our\nwork establishes that an arbitrary transversal Clifford logical circuit can be\nmapped to a stat-mech model, and a rigorous threshold can be obtained\ncorrespondingly."}
{"id": "2510.09762", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09762", "abs": "https://arxiv.org/abs/2510.09762", "authors": ["Ruo Yang", "Sai Krishna Reddy Mudhiganti", "Manali Sharma"], "title": "PatentVision: A multimodal method for drafting patent applications", "comment": null, "summary": "Patent drafting is complex due to its need for detailed technical\ndescriptions, legal compliance, and visual elements. Although Large Vision\nLanguage Models (LVLMs) show promise across various tasks, their application in\nautomating patent writing remains underexplored. In this paper, we present\nPatentVision, a multimodal framework that integrates textual and visual inputs\nsuch as patent claims and drawings to generate complete patent specifications.\nBuilt on advanced LVLMs, PatentVision enhances accuracy by combining fine tuned\nvision language models with domain specific training tailored to patents.\nExperiments reveal it surpasses text only methods, producing outputs with\ngreater fidelity and alignment with human written standards. Its incorporation\nof visual data allows it to better represent intricate design features and\nfunctional connections, leading to richer and more precise results. This study\nunderscores the value of multimodal techniques in patent automation, providing\na scalable tool to reduce manual workloads and improve consistency.\nPatentVision not only advances patent drafting but also lays the groundwork for\nbroader use of LVLMs in specialized areas, potentially transforming\nintellectual property management and innovation processes."}
{"id": "2510.10839", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10839", "abs": "https://arxiv.org/abs/2510.10839", "authors": ["Hamza Harraf", "Noura Chabar", "Mohamed Amazioug", "Rachid Ahl Laamara", "Mojtaba Mazaheri"], "title": "Magnon-rotation enhanced nonreciprocity of multipartite entanglement in a magnomechanical system", "comment": "12 pages, 7 figures", "summary": "Nonreciprocal physics is attracting significant interest in quantum\ninformation processing. In this work, we propose a scheme to investigate the\nnonreciprocity of bi- and tripartite entanglement and generate squeezed states\nin a magnomechanical system. This is achieved through the Barnett effect, which\noriginates from the rotation of the first magnon mode. The system consists of\ntwo YIG spheres, each supporting a magnon mode that represents collective spin\nmotion, positioned inside a microwave cavity (MC). We show that the Barnett\neffect enhances entanglement under thermal effects and generates squeezed\nstates for the two magnon modes and the photon mode. Moreover, we show that\nmagnon-magnon coupling enhances entanglement between different two modes."}
{"id": "2510.09764", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09764", "abs": "https://arxiv.org/abs/2510.09764", "authors": ["Wanting Mao", "Maxwell A Xu", "Harish Haresamudram", "Mithun Saha", "Santosh Kumar", "James Matthew Rehg"], "title": "Leveraging Shared Prototypes for a Multimodal Pulse Motion Foundation Model", "comment": null, "summary": "Modeling multi-modal time-series data is critical for capturing system-level\ndynamics, particularly in biosignals where modalities such as ECG, PPG, EDA,\nand accelerometry provide complementary perspectives on interconnected\nphysiological processes. While recent self-supervised learning (SSL) advances\nhave improved unimodal representation learning, existing multi-modal approaches\noften rely on CLIP-style contrastive objectives that overfit to easily aligned\nfeatures and misclassify valid cross-modal relationships as negatives,\nresulting in fragmented and non-generalizable embeddings. To overcome these\nlimitations, we propose ProtoMM, a novel SSL framework that introduces a shared\nprototype dictionary to anchor heterogeneous modalities in a common embedding\nspace. By clustering representations around shared prototypes rather than\nexplicit negative sampling, our method captures complementary information\nacross modalities and provides a coherent \"common language\" for physiological\nsignals. In this work, we focus on developing a Pulse Motion foundation model\nwith ProtoMM and demonstrate that our approach outperforms contrastive-only and\nprior multimodal SSL methods, achieving state-of-the-art performance while\noffering improved interpretability of learned features."}
{"id": "2510.10852", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10852", "abs": "https://arxiv.org/abs/2510.10852", "authors": ["Tanay Saha", "Shiroman Prakash"], "title": "Sublogarithmic Distillation in all Prime Dimensions using Punctured Reed-Muller Codes", "comment": "30 pages, 7 figures, 3 tables", "summary": "Magic state distillation is a leading but costly approach to fault-tolerant\nquantum computation, and it is important to explore all possible ways of\nminimizing its overhead cost. The number of ancillae required to produce a\nmagic state within a target error rate $\\epsilon$ is $O(\\log^{\\gamma}\n(\\epsilon^{-1}))$ where $\\gamma$ is known as the yield parameter. Hastings and\nHaah derived a family of distillation protocols with sublogarithmic overhead\n(i.e., $\\gamma < 1$) based on punctured Reed-Muller codes. Building on work by\nCampbell \\textit{et al.} and Krishna-Tillich, which suggests that qudits of\ndimension $p>2$ can significantly reduce overhead, we generalize their\nconstruction to qudits of arbitrary prime dimension $p$. We find that, in an\nanalytically tractable puncturing scheme, the number of qudits required to\nachieve sublogarithmic overhead decreases drastically as $p$ increases, and the\nasymptotic yield parameter approaches $\\frac{1}{\\ln p}$ as $p \\to \\infty$. We\nalso perform a small computational search for optimal puncture locations, which\nresults in several interesting triorthogonal codes, including a\n$[[519,106,5]]_5$ code with $\\gamma=0.99$."}
{"id": "2510.09767", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09767", "abs": "https://arxiv.org/abs/2510.09767", "authors": ["Yifan Lu", "Ziyun Zou", "Belal Alsinglawi", "Islam Al-Qudah", "Izzat Alsmadi", "Feilong Tang", "Pengfei Jiao", "Shoaib Jameel"], "title": "HeSRN: Representation Learning On Heterogeneous Graphs via Slot-Aware Retentive Network", "comment": null, "summary": "Graph Transformers have recently achieved remarkable progress in graph\nrepresentation learning by capturing long-range dependencies through\nself-attention. However, their quadratic computational complexity and inability\nto effectively model heterogeneous semantics severely limit their scalability\nand generalization on real-world heterogeneous graphs. To address these issues,\nwe propose HeSRN, a novel Heterogeneous Slot-aware Retentive Network for\nefficient and expressive heterogeneous graph representation learning. HeSRN\nintroduces a slot-aware structure encoder that explicitly disentangles\nnode-type semantics by projecting heterogeneous features into independent slots\nand aligning their distributions through slot normalization and retention-based\nfusion, effectively mitigating the semantic entanglement caused by forced\nfeature-space unification in previous Transformer-based models. Furthermore, we\nreplace the self-attention mechanism with a retention-based encoder, which\nmodels structural and contextual dependencies in linear time complexity while\nmaintaining strong expressive power. A heterogeneous retentive encoder is\nfurther employed to jointly capture both local structural signals and global\nheterogeneous semantics through multi-scale retention layers. Extensive\nexperiments on four real-world heterogeneous graph datasets demonstrate that\nHeSRN consistently outperforms state-of-the-art heterogeneous graph neural\nnetworks and Graph Transformer baselines on node classification tasks,\nachieving superior accuracy with significantly lower computational complexity."}
{"id": "2510.10888", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10888", "abs": "https://arxiv.org/abs/2510.10888", "authors": ["IlKwon Sohn", "Changyeol Lee", "Wooyeong Song", "Kwangil Bae", "Wonhyuk Lee"], "title": "Structural encoding with classical codes for computational-basis bit-flip correction in the early fault-tolerant regime", "comment": "23 pages, 6 figures", "summary": "Achieving reliable performance on early fault-tolerant quantum hardware will\ndepend on protocols that manage noise without incurring prohibitive overhead.\nWe propose a novel framework that integrates quantum computation with the\nfunctionality of classical error correction. In this approach, quantum\ncomputation is performed within the codeword subspace defined by a classical\nerror correction code. The correction of various types of errors that manifest\nas bit flips is carried out based on the final measurement outcomes. The\napproach leverages the asymmetric structure of many key algorithms, where\nproblem-defining diagonal operators (e.g., oracles) are paired with fixed\nnon-diagonal operators (e.g., diffusion operators). The proposed encoding maps\ncomputational basis states to classical codewords. This approach commutes with\ndiagonal operators, obviating their overhead and confining the main\ncomputational cost to simpler non-diagonal components. Noisy simulations\ncorroborate this analysis, demonstrating that the proposed scheme serves as a\nviable protocol-level layer for enhancing performance in the early\nfault-tolerant regime."}
{"id": "2510.09768", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.09768", "abs": "https://arxiv.org/abs/2510.09768", "authors": ["Khang Ngo", "Siamak Ravanbakhsh"], "title": "Scaling Laws and Symmetry, Evidence from Neural Force Fields", "comment": "22 pages, 10 figures", "summary": "We present an empirical study in the geometric task of learning interatomic\npotentials, which shows equivariance matters even more at larger scales; we\nshow a clear power-law scaling behaviour with respect to data, parameters and\ncompute with ``architecture-dependent exponents''. In particular, we observe\nthat equivariant architectures, which leverage task symmetry, scale better than\nnon-equivariant models. Moreover, among equivariant architectures, higher-order\nrepresentations translate to better scaling exponents. Our analysis also\nsuggests that for compute-optimal training, the data and model sizes should\nscale in tandem regardless of the architecture. At a high level, these results\nsuggest that, contrary to common belief, we should not leave it to the model to\ndiscover fundamental inductive biases such as symmetry, especially as we scale,\nbecause they change the inherent difficulty of the task and its scaling laws."}
{"id": "2510.10899", "categories": ["quant-ph", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.10899", "abs": "https://arxiv.org/abs/2510.10899", "authors": ["Andrew Huang", "Vinod Vaikuntanathan"], "title": "A Simple and Efficient One-Shot Signature Scheme", "comment": null, "summary": "One-shot signatures (OSS) are a powerful and uniquely quantum cryptographic\nprimitive which allows anyone, given common reference string, to come up with a\npublic verification key $\\mathsf{pk}$ and a secret signing state\n$|\\mathsf{sk}\\rangle$. With the secret signing state, one can produce the\nsignature of any one message, but no more. In a recent breakthrough work,\nShmueli and Zhandry (CRYPTO 2025) constructed one-shot signatures, either\nunconditionally in a classical oracle model or assuming post-quantum\nindistinguishability obfuscation and the hardness of Learning with Errors (LWE)\nin the plain model.\n  In this work, we address the inefficiency of the Shmueli-Zhandry construction\nwhich signs messages bit-by-bit, resulting in signing keys of\n$\\Theta(\\lambda^4)$ qubits and signatures of size $\\Theta(\\lambda^3)$ bits for\npolynomially long messages, where $\\lambda$ is the security parameter. We\nconstruct a new, simple, direct, and efficient one-shot signature scheme which\ncan sign messages of any polynomial length using signing keys of\n$\\Theta(\\lambda^2)$ qubits and signatures of size $\\Theta(\\lambda^2)$ bits. We\nachieve corresponding savings in runtimes, in both the oracle model and the\nplain model. In addition, unlike the Shmueli-Zhandry construction, our scheme\nachieves perfect correctness.\n  Our scheme also achieves strong signature incompressibility, which implies a\npublic-key quantum fire scheme with perfect correctness among other\napplications, correcting an error in a recent work of \\c{C}akan, Goyal and\nShmueli (QCrypt 2025) and recovering their applications."}
{"id": "2510.09775", "categories": ["cs.LG", "cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09775", "abs": "https://arxiv.org/abs/2510.09775", "authors": ["Alex Hiles", "Bashar I. Ahmad"], "title": "A Generic Machine Learning Framework for Radio Frequency Fingerprinting", "comment": null, "summary": "Fingerprinting Radio Frequency (RF) emitters typically involves finding\nunique emitter characteristics that are featured in their transmitted signals.\nThese fingerprints are nuanced but sufficiently detailed, motivating the\npursuit of methods that can successfully extract them. The most granular\ndownstream task is known as Specific Emitter Identification (SEI), which\nrequires a well informed RF fingerprinting (RFF) approach for it to be\nsuccessful. RFF and SEI have a long history, with numerous application areas in\ndefence and civilian contexts such as signal intelligence, electronic\nsurveillance, physical-layer authentication of wireless communication devices,\nto name a few. RFF methods also support many other downstream tasks such as\nEmitter Data Association (EDA) and RF Emitter Clustering (RFEC) and are\napplicable to a range of transmission types. In recent years, data-driven\napproaches have become popular in the RFF domain due to their ability to\nautomatically learn intricate fingerprints from raw data. These methods\ngenerally deliver superior performance when compared to traditional techniques.\nThe more traditional approaches are often labour-intensive, inflexible and only\napplicable to a particular emitter type or transmission scheme. Therefore, we\nconsider data-driven Machine Learning (ML)-enabled RFF. In particular, we\npropose a generic framework for ML-enabled RFF which is inclusive of several\npopular downstream tasks such as SEI, EDA and RFEC. Each task is formulated as\na RF fingerprint-dependent task. A variety of use cases using real RF datasets\nare presented here to demonstrate the framework for a range of tasks and\napplication areas, such as spaceborne surveillance, signal intelligence and\ncountering drones."}
{"id": "2510.10905", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10905", "abs": "https://arxiv.org/abs/2510.10905", "authors": ["Ben DalFavero", "Ryan LaRose"], "title": "Error mitigation for partially error-corrected quantum computers", "comment": null, "summary": "We present a method for quantum error mitigation on partially error-corrected\nquantum computers - i.e., computers with some logical qubits and some noisy\nqubits. Our method is inspired by the error cancellation method and is\nimplemented via a circuit for convex combinations of channels which we\nintroduce in this work. We show how logical ancilla qubits can arbitrarily\nreduce the sampling complexity of error cancellation in a continuous space-time\ntradeoff, in the limiting case achieving $O(1)$ sample complexity which\ncircumvents lower bounds for sample complexity with all known error mitigation\ntechniques. This comes at the cost of exponential circuit depth, however, and\nleads us to conjecture that any error mitigation protocol with (sub-)polynomial\nsample complexity requires exponential time and/or space, even when logical\nqubits are utilized as a resource. We anticipate additional applications for\nour quantum circuits to implement convex combinations of channels, and to this\nend we discuss one application in simulating open quantum systems, showing an\norder of magnitude reduction in gate counts relative to current\nstate-of-the-art methods for a canonical problem."}
{"id": "2510.09776", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09776", "abs": "https://arxiv.org/abs/2510.09776", "authors": ["Yufa Zhou", "Yixiao Wang", "Surbhi Goel", "Anru R. Zhang"], "title": "Why Do Transformers Fail to Forecast Time Series In-Context?", "comment": "Code: https://github.com/MasterZhou1/ICL-Time-Series", "summary": "Time series forecasting (TSF) remains a challenging and largely unsolved\nproblem in machine learning, despite significant recent efforts leveraging\nLarge Language Models (LLMs), which predominantly rely on Transformer\narchitectures. Empirical evidence consistently shows that even powerful\nTransformers often fail to outperform much simpler models, e.g., linear models,\non TSF tasks; however, a rigorous theoretical understanding of this phenomenon\nremains limited. In this paper, we provide a theoretical analysis of\nTransformers' limitations for TSF through the lens of In-Context Learning (ICL)\ntheory. Specifically, under AR($p$) data, we establish that: (1) Linear\nSelf-Attention (LSA) models $\\textit{cannot}$ achieve lower expected MSE than\nclassical linear models for in-context forecasting; (2) as the context length\napproaches to infinity, LSA asymptotically recovers the optimal linear\npredictor; and (3) under Chain-of-Thought (CoT) style inference, predictions\ncollapse to the mean exponentially. We empirically validate these findings\nthrough carefully designed experiments. Our theory not only sheds light on\nseveral previously underexplored phenomena but also offers practical insights\nfor designing more effective forecasting architectures. We hope our work\nencourages the broader research community to revisit the fundamental\ntheoretical limitations of TSF and to critically evaluate the direct\napplication of increasingly sophisticated architectures without deeper\nscrutiny."}
{"id": "2510.10917", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10917", "abs": "https://arxiv.org/abs/2510.10917", "authors": ["Xiao Chen", "Haechan Park", "Silas Hoffman", "Shuanglong Liu", "Hai-Ping Cheng"], "title": "Dominant spin-spin relaxation mechanism at clock transition of the $Ho_{x}Y_{1-x}W_{10}$ complex at different concentrations", "comment": null, "summary": "Spin decoherence poses a significant challenge in molecular magnets, with the\nnuclear spin bath serving as a prominent source. Intriguingly, spin qubits at\nthe clock transition exhibit remarkable insensitivity to the surrounding\nnuclear spins. Recent experimental studies have unveiled a correlation between\nthe decoherence time and the density of spin qubits, prompting our\ninvestigation into the contribution of the qubit bath to spin decoherence. In\nthis paper, we present a comprehensive theoretical analysis of a few S=1 spin\nqubits, focusing on their interaction at the clock transition. Employing the\nexact diagonalization and the cluster correlation expansion (CCE) method, we\nsimulate the dynamics of spin decoherence while varying the density of the\nqubit bath. To ensure the realism of our simulations, we incorporate structural\nand energetic parameters derived from previous studies on the HoW10 crystal.\nOur findings indicate that when the energy mismatch between the energy\nsplittings of two qubits exceeds their interaction strength, they can become\neffectively insensitive to each other, offering an explanation for the absence\nof observed changes in the T2 time during experiments with lower qubit\ndensities. Understanding the role of qubit bath density in spin decoherence at\nthe clock transition not only advances our knowledge of decoherence mechanisms\nbut also provides insights for the development of strategies to protect\ncoherence in molecular magnets and other quantum systems. By optimizing the\ndensity of spin qubits, we can enhance the coherence properties and pave the\nway for improved performance of quantum devices. Overall, this study offers\nvaluable insights into the relationship between qubit bath density and spin\ndecoherence at the clock transition, contributing to the broader understanding\nand control of quantum systems in molecular magnets."}
{"id": "2510.09780", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09780", "abs": "https://arxiv.org/abs/2510.09780", "authors": ["ChengAo Shen", "Ziming Zhao", "Hanghang Tong", "Dongjin Song", "Dongsheng Luo", "Qingsong Wen", "Jingchao Ni"], "title": "SVTime: Small Time Series Forecasting Models Informed by \"Physics\" of Large Vision Model Forecasters", "comment": null, "summary": "Time series AI is crucial for analyzing dynamic web content, driving a surge\nof pre-trained large models known for their strong knowledge encoding and\ntransfer capabilities across diverse tasks. However, given their\nenergy-intensive training, inference, and hardware demands, using large models\nas a one-fits-all solution raises serious concerns about carbon footprint and\nsustainability. For a specific task, a compact yet specialized, high-performing\nmodel may be more practical and affordable, especially for resource-constrained\nusers such as small businesses. This motivates the question: Can we build\ncost-effective lightweight models with large-model-like performance on core\ntasks such as forecasting? This paper addresses this question by introducing\nSVTime, a novel Small model inspired by large Vision model (LVM) forecasters\nfor long-term Time series forecasting (LTSF). Recently, LVMs have been shown as\npowerful tools for LTSF. We identify a set of key inductive biases of LVM\nforecasters -- analogous to the \"physics\" governing their behaviors in LTSF --\nand design small models that encode these biases through meticulously crafted\nlinear layers and constraint functions. Across 21 baselines spanning\nlightweight, complex, and pre-trained large models on 8 benchmark datasets,\nSVTime outperforms state-of-the-art (SOTA) lightweight models and rivals large\nmodels with 10^3 fewer parameters than LVMs, while enabling efficient training\nand inference in low-resource settings."}
{"id": "2510.10957", "categories": ["quant-ph", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2510.10957", "abs": "https://arxiv.org/abs/2510.10957", "authors": ["Praveen Jayakumar", "Tao Zeng", "Artur F. Izmaylov"], "title": "On the Feasibility of Exact Unitary Transformations for Many-body Hamiltonians", "comment": null, "summary": "Exact unitary transformations play a central role in the analysis and\nsimulation of many-body quantum systems, yet the conditions under which they\ncan be carried out exactly and efficiently remain incompletely understood. We\nshow that exact transformations arise whenever the adjoint action of a\nunitary's generator defines a linear map within a finite-dimensional operator\nspace. In this regime, the Cayley-Hamilton theorem ensures the existence of a\nfinite-degree polynomial that annihilates the adjoint map, rendering the\nBaker-Campbell-Hausdorff expansion finite. This perspective brings together\npreviously disparate examples of exact transformations under a single unifying\nprinciple and clarifies how algebraic relations between generators and\ntransformed operators determine the polynomial degree of the transformation. We\nillustrate this framework for unitary coupled-cluster and involutory\ngenerators, identifying cases in which a single commutator suffices. The result\nestablishes clear algebraic criteria for when exact unitary transformations are\npossible and provides new strategies for reducing their computational cost in\nquantum simulation."}
{"id": "2510.09781", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09781", "abs": "https://arxiv.org/abs/2510.09781", "authors": ["Yue Huang", "Hang Hua", "Yujun Zhou", "Pengcheng Jing", "Manish Nagireddy", "Inkit Padhi", "Greta Dolcetti", "Zhangchen Xu", "Subhajit Chaudhury", "Ambrish Rawat", "Liubov Nedoshivina", "Pin-Yu Chen", "Prasanna Sattigeri", "Xiangliang Zhang"], "title": "Building a Foundational Guardrail for General Agentic Systems via Synthetic Data", "comment": null, "summary": "While LLM agents can plan multi-step tasks, intervening at the planning\nstage-before any action is executed-is often the safest way to prevent harm,\nsince certain risks can lead to severe consequences once carried out. However,\nexisting guardrails mostly operate post-execution, which is difficult to scale\nand leaves little room for controllable supervision at the plan level. To\naddress this challenge, we highlight three critical gaps in current research:\ndata gap, model gap, and evaluation gap. To close the data gap, we introduce\nAuraGen, a controllable engine that (i) synthesizes benign trajectories, (ii)\ninjects category-labeled risks with calibrated difficulty, and (iii) filters\noutputs via an automated reward model, producing large and reliable corpora for\npre-execution safety. To close the guardian model gap, we propose a\nfoundational guardrail Safiron, combining a cross-planner adapter with a\ncompact guardian model. The adapter unifies different input formats, while\nSafiron flags risky cases, assigns risk types, and generates rationales;\ntrained in two stages with a broadly explored data recipe, Safiron achieves\nrobust transfer across settings. To close the evaluation gap, we release\nPre-Exec Bench, a realistic benchmark covering diverse tools and branching\ntrajectories, which measures detection, fine-grained categorization,\nexplanation, and cross-planner generalization in human-verified scenarios.\nExtensive experiments demonstrate consistent gains of the proposed guardrail\nover strong baselines on Pre-Exec Bench, and ablations further distill\nactionable practices, providing a practical template for safer agentic systems."}
{"id": "2510.10967", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10967", "abs": "https://arxiv.org/abs/2510.10967", "authors": ["Tanuj Khattar", "Noah Shutty", "Craig Gidney", "Adam Zalcman", "Noureldin Yosri", "Dmitri Maslov", "Ryan Babbush", "Stephen P. Jordan"], "title": "Verifiable Quantum Advantage via Optimized DQI Circuits", "comment": "52 pages", "summary": "Decoded Quantum Interferometry (DQI) provides a framework for superpolynomial\nquantum speedups by reducing certain optimization problems to reversible\ndecoding tasks. We apply DQI to the Optimal Polynomial Intersection (OPI)\nproblem, whose dual code is Reed-Solomon (RS). We establish that DQI for OPI is\nthe first known candidate for verifiable quantum advantage with optimal\nasymptotic speedup: solving instances with classical hardness $O(2^N)$ requires\nonly $\\widetilde{O}(N)$ quantum gates, matching the theoretical lower bound.\nRealizing this speedup requires highly efficient reversible RS decoders. We\nintroduce novel quantum circuits for the Extended Euclidean Algorithm, the\ndecoder's bottleneck. Our techniques, including a new representation for\nimplicit B\\'ezout coefficient access, and optimized in-place architectures,\nreduce the leading-order space complexity to the theoretical minimum of $2nb$\nqubits while significantly lowering gate counts. These improvements are broadly\napplicable, including to Shor's algorithm for the discrete logarithm. We\nanalyze OPI over binary extension fields $GF(2^b)$, assess hardness against new\nclassical attacks, and identify resilient instances. Our resource estimates\nshow that classically intractable OPI instances (requiring $>10^{23}$ classical\ntrials) can be solved with approximately 5.72 million Toffoli gates. This is\nsubstantially less than the count required for breaking RSA-2048, positioning\nDQI as a compelling candidate for practical, verifiable quantum advantage."}
{"id": "2510.09783", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09783", "abs": "https://arxiv.org/abs/2510.09783", "authors": ["Dang Nguyen", "Sunil Gupta", "Kien Do", "Thin Nguyen", "Taylor Braund", "Alexis Whitton", "Svetha Venkatesh"], "title": "Large Language Models for Imbalanced Classification: Diversity makes the difference", "comment": null, "summary": "Oversampling is one of the most widely used approaches for addressing\nimbalanced classification. The core idea is to generate additional minority\nsamples to rebalance the dataset. Most existing methods, such as SMOTE, require\nconverting categorical variables into numerical vectors, which often leads to\ninformation loss. Recently, large language model (LLM)-based methods have been\nintroduced to overcome this limitation. However, current LLM-based approaches\ntypically generate minority samples with limited diversity, reducing robustness\nand generalizability in downstream classification tasks. To address this gap,\nwe propose a novel LLM-based oversampling method designed to enhance diversity.\nFirst, we introduce a sampling strategy that conditions synthetic sample\ngeneration on both minority labels and features. Second, we develop a new\npermutation strategy for fine-tuning pre-trained LLMs. Third, we fine-tune the\nLLM not only on minority samples but also on interpolated samples to further\nenrich variability. Extensive experiments on 10 tabular datasets demonstrate\nthat our method significantly outperforms eight SOTA baselines. The generated\nsynthetic samples are both realistic and diverse. Moreover, we provide\ntheoretical analysis through an entropy-based perspective, proving that our\nmethod encourages diversity in the generated samples."}
{"id": "2510.10996", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10996", "abs": "https://arxiv.org/abs/2510.10996", "authors": ["Ethan Abraham", "Junghyun Yoon", "Troy Van Voorhis", "Martin Z. Bazant"], "title": "Reduced Effective Reorganization Energy for Adiabatic Electron Transfer", "comment": null, "summary": "We predict that in the adiabatic limit of the Marcus normal regime, Marcus\nkinetics will be observed with a reduced effective reorganization energy that\nis a function of the standard reorganization energy and the coupling strength.\nThis result enables the derivation of a closed-form Marcus-Hush-Chidsey type\nrate expression for heterogeneous electron transfer in the adiabatic limit,\nwhich also involves a different prefactor than in the non-adiabatic case."}
{"id": "2510.09784", "categories": ["cs.LG", "cond-mat.stat-mech", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.09784", "abs": "https://arxiv.org/abs/2510.09784", "authors": ["Richard John", "Yunrui Qiu", "Lukas Herron", "Pratyush Tiwary"], "title": "Combined Representation and Generation with Diffusive State Predictive Information Bottleneck", "comment": null, "summary": "Generative modeling becomes increasingly data-intensive in high-dimensional\nspaces. In molecular science, where data collection is expensive and important\nevents are rare, compression to lower-dimensional manifolds is especially\nimportant for various downstream tasks, including generation. We combine a\ntime-lagged information bottleneck designed to characterize molecular important\nrepresentations and a diffusion model in one joint training objective. The\nresulting protocol, which we term Diffusive State Predictive Information\nBottleneck (D-SPIB), enables the balancing of representation learning and\ngeneration aims in one flexible architecture. Additionally, the model is\ncapable of combining temperature information from different molecular\nsimulation trajectories to learn a coherent and useful internal representation\nof thermodynamics. We benchmark D-SPIB on multiple molecular tasks and showcase\nits potential for exploring physical conditions outside the training set."}
{"id": "2510.11002", "categories": ["quant-ph", "hep-ph"], "pdf": "https://arxiv.org/pdf/2510.11002", "abs": "https://arxiv.org/abs/2510.11002", "authors": ["Z. L. Li", "A. R. Sun", "J. H. Xia", "J. X. Wu", "Y. J. Li"], "title": "Electron-positron pair creation in a supercritical static asymmetric potential well", "comment": "11 pages, 7 figures", "summary": "The electron-positron pair creation in a supercritical static asymmetric\npotential well, which is composed of a subcritical and a supercritical\npotential separated by a fixed distance, is investigated using computational\nquantum field theory. To explain the discrete peaks in the positron energy\nspectrum, an analytical formula for determining the positions of bound states\nin a subcritical asymmetric potential well is derived and extended to the\nsupercritical asymmetric potential well in two ways. One of the two methods can\nnot only predict the positions of bound states, but also offer the pair\ncreation rate. This study also reveals that the subcritical potential height\ncan optimize the energy spread of created electrons, providing a new way to\nproduce high-energy electron beams with concentrated energy in experiments.\nMoreover, it is found that the pair creation rate in a supercritical asymmetric\npotential well, composed of a subcritical symmetric potential well and a\nsupercritical Sauter potential, exceeds the sum of the pair creation rates\nproduced by each potential individually. This finding suggests a potential\nmethod for enhancing pair yield."}
{"id": "2510.09792", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2510.09792", "abs": "https://arxiv.org/abs/2510.09792", "authors": ["Vahidreza Jahanmard", "Ali Ramezani-Kebrya", "Robinson Hordoir"], "title": "Principled Operator Learning in Ocean Dynamics: The Role of Temporal Structure", "comment": "Accepted at NeurIPS ML4PS 2025", "summary": "Neural operators are becoming the default tools to learn solutions to\ngoverning partial differential equations (PDEs) in weather and ocean\nforecasting applications. Despite early promising achievements, significant\nchallenges remain, including long-term prediction stability and adherence to\nphysical laws, particularly for high-frequency processes. In this paper, we\ntake a step toward addressing these challenges in high-resolution ocean\nprediction by incorporating temporal Fourier modes, demonstrating how this\nmodification enhances physical fidelity. This study compares the standard\nFourier Neural Operator (FNO) with its variant, FNOtD, which has been modified\nto internalize the dispersion relation while learning the solution operator for\nocean PDEs. The results demonstrate that entangling space and time in the\ntraining of integral kernels enables the model to capture multiscale wave\npropagation and effectively learn ocean dynamics. FNOtD substantially improves\nlong-term prediction stability and consistency with underlying physical\ndynamics in challenging high-frequency settings compared to the standard FNO.\nIt also provides competitive predictive skill relative to a state-of-the-art\nnumerical ocean model, while requiring significantly lower computational cost."}
{"id": "2510.11037", "categories": ["quant-ph", "hep-ph"], "pdf": "https://arxiv.org/pdf/2510.11037", "abs": "https://arxiv.org/abs/2510.11037", "authors": ["Sabine Hossenfelder"], "title": "How Gravity Can Explain the Collapse of the Wavefunction", "comment": null, "summary": "I present a simple argument for why a fundamental theory that unifies matter\nand gravity gives rise to what seems to be a collapse of the wavefunction. The\nresulting model is local, parameter-free and makes testable predictions."}
{"id": "2510.09794", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09794", "abs": "https://arxiv.org/abs/2510.09794", "authors": ["Lianghuan Huang", "Yingshan Chang"], "title": "Causality $\\neq$ Decodability, and Vice Versa: Lessons from Interpreting Counting ViTs", "comment": null, "summary": "Mechanistic interpretability seeks to uncover how internal components of\nneural networks give rise to predictions. A persistent challenge, however, is\ndisentangling two often conflated notions: decodability--the recoverability of\ninformation from hidden states--and causality--the extent to which those states\nfunctionally influence outputs. In this work, we investigate their relationship\nin vision transformers (ViTs) fine-tuned for object counting. Using activation\npatching, we test the causal role of spatial and CLS tokens by transplanting\nactivations across clean-corrupted image pairs. In parallel, we train linear\nprobes to assess the decodability of count information at different depths. Our\nresults reveal systematic mismatches: middle-layer object tokens exert strong\ncausal influence despite being weakly decodable, whereas final-layer object\ntokens support accurate decoding yet are functionally inert. Similarly, the CLS\ntoken becomes decodable in mid-layers but only acquires causal power in the\nfinal layers. These findings highlight that decodability and causality reflect\ncomplementary dimensions of representation--what information is present versus\nwhat is used--and that their divergence can expose hidden computational\ncircuits."}
{"id": "2510.11045", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.11045", "abs": "https://arxiv.org/abs/2510.11045", "authors": ["Yicheng Guang", "Pietro Zanotta", "Kai Zhou", "Yueqi Chen", "Ramin Ayanzadeh"], "title": "On the Potential of Quantum Computing in Classical Program Analysis", "comment": null, "summary": "Classical program analysis techniques, such as abstract interpretation and\nsymbolic execution, are essential for ensuring software correctness, optimizing\nperformance, and enabling compiler optimizations. However, these techniques\nface computational limitations when analyzing programs with large or\nexponential state spaces, limiting their effectiveness in ensuring system\nreliability. Quantum computing, with its parallelism and ability to process\nsuperposed states, offers a promising solution to these challenges. In this\nwork, we present QEX, a design that uses quantum computing to analyze classical\nprograms. By synthesizing quantum circuits that encode program states in\nsuperposition and trace data dependency between program variables through\nentanglement, QEX enables the simultaneous exploration of program behaviors,\nsignificantly improving scalability and precision. This advancement has broad\napplications, from debugging and security verification to optimizing compilers\nfor next-generation hardware. As a proof-of-concept, we evaluated QEX on 22\nbenchmark programs, demonstrating its effectiveness in analyzing program\nstates. To support more language features and make QEX realized sooner in\nFault-Tolerant Quantum Computing (FTQC), we propose QEX-H which hybridizes QEX\nwith classical analysis techniques. To our knowledge, this work is the first\nproposal to use quantum computing for classical program analysis."}
{"id": "2510.09796", "categories": ["cs.LG", "cs.NA", "math.NA", "math.OC", "stat.ML", "47A52, 47J30, 65J22, 65K10, 68T01, 68T07, 68W15, 94A08"], "pdf": "https://arxiv.org/pdf/2510.09796", "abs": "https://arxiv.org/abs/2510.09796", "authors": ["Xiaoyu Wang", "Alexandra Valavanis", "Azhir Mahmood", "Andreas Mang", "Martin Benning", "Audrey Repetti"], "title": "A Unified Framework for Lifted Training and Inversion Approaches", "comment": null, "summary": "The training of deep neural networks predominantly relies on a combination of\ngradient-based optimisation and back-propagation for the computation of the\ngradient. While incredibly successful, this approach faces challenges such as\nvanishing or exploding gradients, difficulties with non-smooth activations, and\nan inherently sequential structure that limits parallelisation. Lifted training\nmethods offer an alternative by reformulating the nested optimisation problem\ninto a higher-dimensional, constrained optimisation problem where the\nconstraints are no longer enforced directly but penalised with penalty terms.\nThis chapter introduces a unified framework that encapsulates various lifted\ntraining strategies, including the Method of Auxiliary Coordinates, Fenchel\nLifted Networks, and Lifted Bregman Training, and demonstrates how diverse\narchitectures, such as Multi-Layer Perceptrons, Residual Neural Networks, and\nProximal Neural Networks fit within this structure. By leveraging tools from\nconvex optimisation, particularly Bregman distances, the framework facilitates\ndistributed optimisation, accommodates non-differentiable proximal activations,\nand can improve the conditioning of the training landscape. We discuss the\nimplementation of these methods using block-coordinate descent strategies,\nincluding deterministic implementations enhanced by accelerated and adaptive\noptimisation techniques, as well as implicit stochastic gradient methods.\nFurthermore, we explore the application of this framework to inverse problems,\ndetailing methodologies for both the training of specialised networks (e.g.,\nunrolled architectures) and the stable inversion of pre-trained networks.\nNumerical results on standard imaging tasks validate the effectiveness and\nstability of the lifted Bregman approach compared to conventional training,\nparticularly for architectures employing proximal activations."}
{"id": "2510.11053", "categories": ["quant-ph", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.11053", "abs": "https://arxiv.org/abs/2510.11053", "authors": ["Maurizio Palesi", "Enrico Russo", "Giuseppe Ascia", "Hamaad Rafique", "Davide Patti", "Vincenzo Catania", "Sergi Abadal", "Abhijit Das", "Pau Escofet", "Eduard Alarcon", "Carmen G. Almudéver"], "title": "Assessing the Role of Communication in Modular Multi-Core Quantum Systems", "comment": null, "summary": "The scalability of quantum computing is constrained by the physical and\narchitectural limitations of monolithic quantum processors. Modular multi-core\nquantum architectures, which interconnect multiple quantum cores (QCs) via\nclassical and quantum-coherent links, offer a promising alternative to address\nthese challenges. However, transitioning to a modular architecture introduces\ncommunication overhead, where classical communication plays a crucial role in\nexecuting quantum algorithms by transmitting measurement outcomes and\nsynchronizing operations across QCs. Understanding the impact of classical\ncommunication on execution time is therefore essential for optimizing system\nperformance.\n  In this work, we introduce \\qcomm, an open-source simulator designed to\nevaluate the role of classical communication in modular quantum computing\narchitectures. \\qcomm{} provides a high-level execution and timing model that\ncaptures the interplay between quantum gate execution, entanglement\ndistribution, teleportation protocols, and classical communication latency. We\nconduct an extensive experimental analysis to quantify the impact of classical\ncommunication bandwidth, interconnect types, and quantum circuit mapping\nstrategies on overall execution time. Furthermore, we assess classical\ncommunication overhead when executing real quantum benchmarks mapped onto a\ncryogenically-controlled multi-core quantum system. Our results show that,\nwhile classical communication is generally not the dominant contributor to\nexecution time, its impact becomes increasingly relevant in optimized scenarios\n-- such as improved quantum technology, large-scale interconnects, or\ncommunication-aware circuit mappings. These findings provide useful insights\nfor the design of scalable modular quantum architectures and highlight the\nimportance of evaluating classical communication as a performance-limiting\nfactor in future systems."}
{"id": "2510.09805", "categories": ["cs.LG", "cs.AI", "35Q30, 76D05, 65M70, 68T07, 68T27, 03D45", "I.2.0"], "pdf": "https://arxiv.org/pdf/2510.09805", "abs": "https://arxiv.org/abs/2510.09805", "authors": ["Jeffrey Camlin"], "title": "Temporal Lifting as Latent-Space Regularization for Continuous-Time Flow Models in AI Systems", "comment": "6 pages, 1 figure, 1 table, 1 algorithm", "summary": "We present a latent-space formulation of adaptive temporal reparametrization\nfor continuous-time dynamical systems. The method, called *temporal lifting*,\nintroduces a smooth monotone mapping $t \\mapsto \\tau(t)$ that regularizes\nnear-singular behavior of the underlying flow while preserving its conservation\nlaws. In the lifted coordinate, trajectories such as those of the\nincompressible Navier-Stokes equations on the torus $\\mathbb{T}^3$ become\nglobally smooth. From the standpoint of machine-learning dynamics, temporal\nlifting acts as a continuous-time normalization or time-warping operator that\ncan stabilize physics-informed neural networks and other latent-flow\narchitectures used in AI systems. The framework links analytic regularity\ntheory with representation-learning methods for stiff or turbulent processes."}
{"id": "2510.11055", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.11055", "abs": "https://arxiv.org/abs/2510.11055", "authors": ["Na-Na Zhang", "Chao-Yi Wu", "Ming Li", "Wei-Xuan Cao", "Jun-Hao Zhang", "Yong-Rui Guo", "Ren-Pu Li"], "title": "Analytical Control of Quantum Coherence: Markovian Revival via Basis Engineering and Exact Non-Markovian Criteria", "comment": null, "summary": "The preservation of quantum coherence is besieged by a fundamental dogma: its\nrevival necessitates non-Markovian memory effects from structured environments.\nThis paradigm has constrained quantum control strategies and obscured simpler\npaths to coherence protection. Here, we shatter this belief by demonstrating\nunambiguous coherence revival even in strictly Markovian regimes, achieved\nsolely through basis engineering in the $\\sigma_x/\\sigma_y$ bases. We establish\na comprehensive analytical framework for predictive coherence control,\ndelivering three universal design principles. First, we derive a minimum\ncritical noise based frequency, $\\omega_{0}^{c} = 1.57/(0.4996 \\cdot\nt_{\\max})$, serving as a universal criterion for engineering non-Markovian\ndynamics over any interval $[0, t_{\\max}]$. Crucially, we show that Markovian\nenvironments ($\\omega_0 < \\omega_0^c$) can exhibit coherence revival when the\nZeeman energy satisfies $\\omega_k > \\pi/(2t_{\\max})$, decoupling revival from\nenvironmental memory. Furthermore, for non-Markovian environments, we provide\nexact conditions for periodic and complete revival: setting $\\omega_0 = n \\cdot\n6.285/t_{\\max}$ guarantees revival in the $\\sigma_z$ basis, while combining it\nwith $\\omega_k = \\pi \\omega_0 / 6.285$ ensures perfect revival in the\n$\\sigma_x/\\sigma_y$ bases. Our results, validated by rigorous quantum\nsimulations, provide a predictive toolkit for coherence control, offering\nimmediate strategies for enhancing quantum memory, sensing, and error\nmitigation."}
{"id": "2510.09825", "categories": ["cs.LG", "cs.CV", "cs.IT", "cs.NE", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.09825", "abs": "https://arxiv.org/abs/2510.09825", "authors": ["Mohsen Joneidi"], "title": "Decomposer Networks: Deep Component Analysis and Synthesis", "comment": "13 Pages, 4 figures", "summary": "We propose the Decomposer Networks (DecompNet), a semantic autoencoder that\nfactorizes an input into multiple interpretable components. Unlike classical\nautoencoders that compress an input into a single latent representation, the\nDecomposer Network maintains N parallel branches, each assigned a residual\ninput defined as the original signal minus the reconstructions of all other\nbranches. By unrolling a Gauss--Seidel style block-coordinate descent into a\ndifferentiable network, DecompNet enforce explicit competition among\ncomponents, yielding parsimonious, semantically meaningful representations. We\nsituate our model relative to linear decomposition methods (PCA, NMF), deep\nunrolled optimization, and object-centric architectures (MONet, IODINE, Slot\nAttention), and highlight its novelty as the first semantic autoencoder to\nimplement an all-but-one residual update rule."}
{"id": "2510.11086", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2510.11086", "abs": "https://arxiv.org/abs/2510.11086", "authors": ["Ya Li", "WanRu Wang", "Weizhe Qiao", "Qizhou Wu", "Changqing Niu", "Xiaolong Zou", "Youxing Chen", "Xin Guo"], "title": "Efficient and Robust Spatial-to-Fiber Coupling forMultimode Quantum Networks via CascadedAdaptive Feedback Control", "comment": null, "summary": "Duan-Lukin-Cirac-Zoller (DLCZ)-based multimodequantum networks rely on\nefficient spatial-to-fiber coupling, yetenvironmental perturbations compromise\nthis performance. Wedevelop a cascaded adaptive feedback control system\nintegratedinto the quantum entanglement source preparation path.Leveraging a\npower-feedback hillclimbing algorithm, itdynamically regulates\npiezoelectric-actuated mirrors to achieveautonomous multi-dimensional beam\nalignment, Experimentsshow it rapidly boosts single-mode fiber (SMF) coupling\nefficieneyto over 70% within 20 seconds and entering the most efficient\nandstable transmission state after 75 seconds.Importantly, it enhancesthe\nstability of the atom-photon interfacecritical for quantumlight-matter\ninteractionsproviding a practical framework forefficient, robust spatial light\ntransmission in scalable quantumnetworks."}
{"id": "2510.09827", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09827", "abs": "https://arxiv.org/abs/2510.09827", "authors": ["Michael Crawshaw", "Chirag Modi", "Mingrui Liu", "Robert M. Gower"], "title": "An Exploration of Non-Euclidean Gradient Descent: Muon and its Many Variants", "comment": null, "summary": "To define a steepest descent method over a neural network, we need to choose\na norm for each layer, a way to aggregate these norms across layers, and\nwhether to use normalization. We systematically explore different alternatives\nfor aggregating norms across layers, both formalizing existing combinations of\nAdam and the recently proposed Muon as a type of non-Euclidean gradient\ndescent, and deriving new variants of the Muon optimizer. Through a\ncomprehensive experimental evaluation of the optimizers within our framework,\nwe find that Muon is sensitive to the choice of learning rate, whereas a new\nvariant we call MuonMax is significantly more robust. We then show how to\ncombine any non-Euclidean gradient method with model based momentum (known as\nMomo). The new Momo variants of Muon are significantly more robust to\nhyperparameter tuning, and often achieve a better validation score. Thus for\nnew tasks, where the optimal hyperparameters are not known, we advocate for\nusing Momo in combination with MuonMax to save on costly hyperparameter tuning."}
{"id": "2510.11130", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.11130", "abs": "https://arxiv.org/abs/2510.11130", "authors": ["Justin Tan", "Nengji Zhou", "Yang Zhao"], "title": "Quantum phase transition of sub-Ohmic spin-boson models: An approach by the multiple Davydov D2 Ansatz", "comment": null, "summary": "The ground state properties and quantum phase transitions of sub-Ohmic\nspin-boson models are investigated using the multiple Davydov D2 Ansatz in\nconjunction with the variational principle. Three variants of the model are\nstudied: (i) a single bath with diagonal coupling, (ii) two independent baths\nwith diagonal and off-diagonal couplings, and (iii) a single bath with\nsimultaneous diagonal and off-diagonal couplings. For the purely diagonal\nmodel, the multiple Davydov D2 Ansatz yields critical coupling strengths that\nare consistent with other methodologies, validating its accuracy and\nefficiency. In the two-bath model, the competition between diagonal and\noff-diagonal couplings drives a first-order transition for both symmetric and\nasymmetric spectral exponents, with von-Neumann entropy showing a continuous\npeak only under exact symmetry. Finally, for a single bath with simultaneous\ndiagonal and off-diagonal couplings, we demonstrate that a rotational\ntransformation maps the system to an equivalent purely diagonal model, enabling\nsimpler and intuitive physical interpretation and reduced computational\ncomplexity."}
{"id": "2510.09845", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09845", "abs": "https://arxiv.org/abs/2510.09845", "authors": ["Nicholas LaHaye", "Thilanka Munashinge", "Hugo Lee", "Xiaohua Pan", "Gonzalo Gonzalez Abad", "Hazem Mahmoud", "Jennifer Wei"], "title": "Harnessing Self-Supervised Deep Learning and Geostationary Remote Sensing for Advancing Wildfire and Associated Air Quality Monitoring: Improved Smoke and Fire Front Masking using GOES and TEMPO Radiance Data", "comment": "https://2025.ieeeigarss.org/view_paper.php?PaperNum=6389&SessionID=1611", "summary": "This work demonstrates the possibilities for improving wildfire and air\nquality management in the western United States by leveraging the unprecedented\nhourly data from NASA's TEMPO satellite mission and advances in self-supervised\ndeep learning. Here we demonstrate the efficacy of deep learning for mapping\nthe near real-time hourly spread of wildfire fronts and smoke plumes using an\ninnovative self-supervised deep learning-system: successfully distinguishing\nsmoke plumes from clouds using GOES-18 and TEMPO data, strong agreement across\nthe smoke and fire masks generated from different sensing modalities as well as\nsignificant improvement over operational products for the same cases."}
{"id": "2510.11153", "categories": ["quant-ph", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.11153", "abs": "https://arxiv.org/abs/2510.11153", "authors": ["Sebastian Schlütter", "Tomislav Maras", "Alexander Dotterweich", "Nico Piatkowski"], "title": "Hot-Starting Quantum Portfolio Optimization", "comment": null, "summary": "Combinatorial optimization with a smooth and convex objective function arises\nnaturally in applications such as discrete mean-variance portfolio\noptimization, where assets must be traded in integer quantities. Although\noptimal solutions to the associated smooth problem can be computed efficiently,\nexisting adiabatic quantum optimization methods cannot leverage this\ninformation. Moreover, while various warm-starting strategies have been\nproposed for gate-based quantum optimization, none of them explicitly integrate\ninsights from the relaxed continuous solution into the QUBO formulation. In\nthis work, a novel approach is introduced that restricts the search space to\ndiscrete solutions in the vicinity of the continuous optimum by constructing a\ncompact Hilbert space, thereby reducing the number of required qubits.\nExperiments on software solvers and a D-Wave Advantage quantum annealer\ndemonstrate that our method outperforms state-of-the-art techniques."}
{"id": "2510.09846", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09846", "abs": "https://arxiv.org/abs/2510.09846", "authors": ["Zhenjiang Fan", "Zengyi Qin", "Yuanning Zheng", "Bo Xiong", "Summer Han"], "title": "CALM: A Causal Analysis Language Model for Tabular Data in Complex Systems with Local Scores, Conditional Independence Tests, and Relation Attributes", "comment": null, "summary": "Causal discovery from observational data is fundamental to scientific fields\nlike biology, where controlled experiments are often impractical. However,\nexisting methods, including constraint-based (e.g., PC, causalMGM) and\nscore-based approaches (e.g., NOTEARS), face significant limitations. These\ninclude an inability to resolve causal direction, restrictions to linear\nassociations, sensitivity to violations of the faithfulness assumption, and\ninefficiency in searching vast hypothesis spaces. While large language models\n(LLMs) offer powerful reasoning capabilities, their application is hindered by\na fundamental discrepancy: they are designed for text, while most causal data\nis tabular. To address these challenges, we introduce CALM, a novel causal\nanalysis language model specifically designed for tabular data in complex\nsystems. CALM leverages a Mamba-based architecture to classify causal patterns\nfrom pairwise variable relationships. It integrates a comprehensive suite of\nevidence, including local causal scores, conditional independence tests, and\nrelational attributes, to capture a wide spectrum of linear, nonlinear, and\nconditional causal mechanisms. Trained on a diverse corpus of synthetic data\n(from linear, mixed, and nonlinear models) and 10 real-world biological\ndatasets with rigorously validated causal relationships, our model ensures\nrobustness and generalizability. Empirical evaluation demonstrates that CALM\nsignificantly outperforms existing methods in both simulation studies,\nachieving over 91% accuracy, and in a real-world application identifying causal\nfactors in Hepatitis C virus progression. This work represents a significant\nstep towards accurate and generalizable causal discovery by successfully\nadapting the pattern recognition capabilities of language models to the\nintricacies of tabular data."}
{"id": "2510.11154", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2510.11154", "abs": "https://arxiv.org/abs/2510.11154", "authors": ["Suraj Goel", "Bohnishikha Ghosh", "Mehul Malik"], "title": "Quantum Information Processing with Spatially Structured Light", "comment": null, "summary": "Qudits have proven to be a powerful resource for quantum information\nprocessing, offering enhanced channel capacities, improved robustness to noise,\nand highly efficient implementations of quantum algorithms. The encoding of\nphotonic qudits in transverse-spatial degrees of freedom has emerged as a\nversatile tool for quantum information processing, allowing access to a vast\ninformation capacity within a single photon. In this review, we examine recent\nadvances in quantum optical circuits with spatially structured light, focusing\nparticularly on top-down approaches that employ complex mode-mixing\ntransformations in free-space and fibers. In this context, we highlight\ncircuits based on platforms such as multi-plane light conversion, complex\nscattering media, multimode and multi-core fibers. We discuss their\napplications for the manipulation and measurement of multi-dimensional and\nmulti-mode quantum states. Furthermore, we discuss how these circuits have been\nemployed to perform multi-party operations and multi-outcome measurements,\nthereby opening new avenues for scalable photonic quantum information\nprocessing."}
{"id": "2510.09852", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09852", "abs": "https://arxiv.org/abs/2510.09852", "authors": ["Shivam Patel", "Neharika Jali", "Ankur Mallick", "Gauri Joshi"], "title": "ProxRouter: Proximity-Weighted LLM Query Routing for Improved Robustness to Outliers", "comment": null, "summary": "Large language model (LLM) query routers are critical to modern AI platforms\nas they seek to improve efficiency by assigning inference queries to accurate,\nyet low-cost models. Parametric routers typically use trained neural networks\nfor LLM selection but suffer from retraining and maintenance overheads.\nNonparametric routers are training-free, instead estimating LLM accuracy and\ncost via similarity between encodings of the input query and training set\nqueries. However, like their parametric counterparts, nonparametric routers\nstruggle to generalize to outlier queries, an issue exacerbated by limited\ndiversity in training sets which are costly to expand and difficult to keep\ncurrent with ever-evolving use cases. We propose ProxRouter, which applies an\nexponentially tilted aggregation mechanism to balance bias and variance in\nnonparametric routers, improving their robustness to outliers. Experiments show\nProxRouter enhances outlier routing while preserving inlier performance with\nminimal overhead."}
{"id": "2510.11159", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2510.11159", "abs": "https://arxiv.org/abs/2510.11159", "authors": ["Thomas K. Bracht", "Rachel N. Clark", "Petros Androvitsaneas", "Matthew Jordan", "Samuel G. Bishop", "Harry E. Dyte", "Moritz Cygorek", "Ian A. Farrer", "Doris E. Reiter", "Anthony J. Bennett"], "title": "Tunable multi-photon correlations from a coherently driven quantum dot", "comment": "8 pages, 6 figures", "summary": "Mixing the fields generated by different light sources has emerged as a\npowerful approach for engineering non-Gaussian quantum states. Understanding\nand controlling the resulting photon statistics is useful for emerging quantum\ntechnologies that are underpinned by interference. In this work, we investigate\nintensity correlation functions arising from the interference of resonance\nfluorescence from a quantum emitter with a coherent laser field. We show that\nthe observed bunching behavior results from a subtle interplay between quantum\ninterference and the normalization of the correlation functions. We show that\nby adjusting the mixing ratio and phase one can achieve full tunability of the\nsecond-order correlation, ranging from anti-bunching to bunching. We further\nextend our analysis to third-order correlation functions, both experimentally\nand theoretically, to provide new insights into the interpretation of\nhigher-order correlations and offer practical tools for shaping quantum optical\nfields."}
{"id": "2510.09872", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09872", "abs": "https://arxiv.org/abs/2510.09872", "authors": ["Sanjari Srivastava", "Gang Li", "Cheng Chang", "Rishu Garg", "Manpreet Kaur", "Charlene Y. Lee", "Yuezhang Li", "Yining Mao", "Ignacio Cases", "Yanan Xie", "Peng Qi"], "title": "WARC-Bench: Web Archive Based Benchmark for GUI Subtask Executions", "comment": null, "summary": "Training web agents to navigate complex, real-world websites requires them to\nmaster $\\textit{subtasks}$ - short-horizon interactions on multiple UI\ncomponents (e.g., choosing the correct date in a date picker, or scrolling in a\ncontainer to extract information). We introduce WARC-Bench (Web Archive\nBenchmark), a novel web navigation benchmark featuring 438 tasks designed to\nevaluate multimodal AI agents on subtasks. WARC-Bench enables sandboxed\ninteractions with dynamic and realistic webpages using Web ARChive files. We\nshow that WARC-Bench is challenging for leading computer-use models, with the\nhighest observed success rate being 64.8%. To improve open source models on\nsubtask, we explore two common training techniques: supervised fine-tuning\n(SFT) and reinforcement learning with verifiable rewards (RLVR). Experiments\nshow that SFT models obtain a 48.8% success rate on the benchmark. Training\nwith RLVR over SFT checkpoints, even in data-scarce settings, improves the\nscore to 52.8% on WARC-Bench, outperforming many frontier models. Our analysis\nconcludes that mastering these subtasks is essential for robust web planning\nand navigation, and is a capability not extensively evaluated by existing\nbenchmarks."}
{"id": "2510.11200", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.11200", "abs": "https://arxiv.org/abs/2510.11200", "authors": ["Sujay Mondal", "Siddhartha Dutta", "Abhijit Bandyopadhyay"], "title": "Tensor-Network-Based Unraveling of Non-Markovian Dynamics in Large Spin Chains via the Influence Martingale Approach", "comment": "30 pages, 8 figures", "summary": "Classical simulation of open quantum system dynamics remains challenging due\nto the exponential growth of the Hilbert space, the need to accurately capture\ndissipation and decoherence, and the added complexity of memory effects in the\nnon-Markovian regime. We develop an efficient algorithm for simulating both\nMarkovian and non-Markovian dynamics in large one-dimensional quantum systems.\nExtending the Tensor Jump Method, which combines TDVP-based tensor-network\nevolution with a Suzuki-Trotter decomposition of stochastic trajectories, our\napproach incorporates time-dependent decay rates-treating positive rates as\ntime-inhomogeneous Markovian processes and negative rates via the Influence\nMartingale formalism to unravel time-local non-Markovian dynamics. This\nresource-efficient framework enables scalable simulations of open-system\ndynamics in the non-Markovian regime, as demonstrated for a one-dimensional\ntransverse-field Ising chain comprising up to 100 spin qubits."}
{"id": "2510.09877", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09877", "abs": "https://arxiv.org/abs/2510.09877", "authors": ["Kangping Hu", "Stephen Mussmann"], "title": "Myopic Bayesian Decision Theory for Batch Active Learning with Partial Batch Label Sampling", "comment": null, "summary": "Over the past couple of decades, many active learning acquisition functions\nhave been proposed, leaving practitioners with an unclear choice of which to\nuse. Bayesian Decision Theory (BDT) offers a universal principle to guide\ndecision-making. In this work, we derive BDT for (Bayesian) active learning in\nthe myopic framework, where we imagine we only have one more point to label.\nThis derivation leads to effective algorithms such as Expected Error Reduction\n(EER), Expected Predictive Information Gain (EPIG), and other algorithms that\nappear in the literature. Furthermore, we show that BAIT (active learning based\non V-optimal experimental design) can be derived from BDT and asymptotic\napproximations. A key challenge of such methods is the difficult scaling to\nlarge batch sizes, leading to either computational challenges (BatchBALD) or\ndramatic performance drops (top-$B$ selection). Here, using a particular\nformulation of the decision process, we derive Partial Batch Label Sampling\n(ParBaLS) for the EPIG algorithm. We show experimentally for several datasets\nthat ParBaLS EPIG gives superior performance for a fixed budget and Bayesian\nLogistic Regression on Neural Embeddings. Our code is available at\nhttps://github.com/ADDAPT-ML/ParBaLS."}
{"id": "2510.11201", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.11201", "abs": "https://arxiv.org/abs/2510.11201", "authors": ["Benoit Kaczmarczuk", "Yannick Bidel", "Alexandre Bresson", "Nassim Zahzam", "Alexis Bonnin", "Malo Cadoret", "Tim Enzlberger Jensen", "Quentin Beaufils", "Franck Pereira Dos Santos"], "title": "Comparison and optimisation of hybridization algorithms for onboard classical and quantum accelerometers", "comment": "18 pages, 7 figures. Invited article submitted to AVS Quantum\n  Science, special topic \"Advances in Matter Wave Optics\". Funded by ANR\n  (ANR-22-PETQ-0005) and European Defence Fund (101103417\n  EDF-2021-DIS-RDIS-ADEQUADE)", "summary": "We study two hybridization algorithms used for the combination of a quantum\ninertial sensor based on atom interferometry with a classical inertial sensor\nfor onboard acceleration measurements. The first is based on the direct\nextraction of the interferometer phase, and was previously used in seaborne and\nairborne gravity measurement campaigns. The second is based on the combination\nof three consecutive measurements and was originally developed to increase the\nmeasurement range of the quantum sensor beyond its linear range. After\ncomparing their performances using synthetic data, we implement them on\nacceleration data collected in a recent airborne campaign and evaluate the bias\nand the scale factor error of the classical sensor. We then extend their scope\nto the dynamical evaluation of other key measurement parameters (e.g. alignment\nerrors). We demonstrate an improvement in the correlation between the two\naccelerometers' measurements and a significant reduction of the error in the\nestimation of the bias of the classical sensor."}
{"id": "2510.09884", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09884", "abs": "https://arxiv.org/abs/2510.09884", "authors": ["Soheila Farokhi", "Xiaojun Qi", "Hamid Karimi"], "title": "TAWRMAC: A Novel Dynamic Graph Representation Learning Method", "comment": null, "summary": "Dynamic graph representation learning has become essential for analyzing\nevolving networks in domains such as social network analysis, recommendation\nsystems, and traffic analysis. However, existing continuous-time methods face\nthree key challenges: (1) some methods depend solely on node-specific memory\nwithout effectively incorporating information from neighboring nodes, resulting\nin embedding staleness; (2) most fail to explicitly capture correlations\nbetween node neighborhoods, limiting contextual awareness; and (3) many fail to\nfully capture the structural dynamics of evolving graphs, especially in absence\nof rich link attributes. To address these limitations, we introduce TAWRMAC-a\nnovel framework that integrates Temporal Anonymous Walks with Restart, Memory\nAugmentation, and Neighbor Co-occurrence embedding. TAWRMAC enhances embedding\nstability through a memory-augmented GNN with fixedtime encoding and improves\ncontextual representation by explicitly capturing neighbor correlations.\nAdditionally, its Temporal Anonymous Walks with Restart mechanism distinguishes\nbetween nodes exhibiting repetitive interactions and those forming new\nconnections beyond their immediate neighborhood. This approach captures\nstructural dynamics better and supports strong inductive learning. Extensive\nexperiments on multiple benchmark datasets demonstrate that TAWRMAC\nconsistently outperforms state-of-the-art methods in dynamic link prediction\nand node classification under both transductive and inductive settings across\nthree different negative sampling strategies. By providing stable,\ngeneralizable, and context-aware embeddings, TAWRMAC advances the state of the\nart in continuous-time dynamic graph learning. The code is available at\nhttps://anonymous.4open.science/r/tawrmac-A253 ."}
{"id": "2510.11213", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.11213", "abs": "https://arxiv.org/abs/2510.11213", "authors": ["Songqinghao Yang", "Haomu Yuan", "Crispin H. W. Barnes"], "title": "Experimental Demonstration of the PBR Test on a Superconducting Processor", "comment": "7 pages, 4 figures", "summary": "We present an experimental implementation of the Pusey-Barrett-Rudolph (PBR)\nno-go theorem on IBM's 156-qubit Heron2 Marrakesh superconducting quantum\nprocessor. By preparing qubits in a set of non-orthogonal states and evolving\nthem under carefully compiled unitary circuits, we test whether one can\ninterpret the hidden variable model for quantum states as merely epistemic --\nreflecting ignorance about some underlying physical reality. To account for\nrealistic hardware imperfections, we derive noise-aware error tolerance based\non decoherence models calibrated to the device's performance. Our results show\nthat a significant majority of adjacent qubit pairs and adjacent five-qubit\nconfigurations yield outcome statistics that violate the epistemic bound, thus\nruling out the epistemic interpretation of quantum mechanics. Furthermore, we\nobserve a clear trend: the probability of passing the PBR test decreases as the\nspatial separation within the quantum processor between qubits increases,\nhighlighting the sensitivity of this protocol to connectivity and coherence in\nNoisy Intermediate-Scale Quantum (NISQ) systems. These results demonstrate the\nPBR test as a promising device-level benchmark for quantumness in the presence\nof realistic noise."}
{"id": "2510.09888", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09888", "abs": "https://arxiv.org/abs/2510.09888", "authors": ["Yunlong Feng", "Qiang Wu"], "title": "Understanding Robust Machine Learning for Nonparametric Regression with Heavy-Tailed Noise", "comment": null, "summary": "We investigate robust nonparametric regression in the presence of\nheavy-tailed noise, where the hypothesis class may contain unbounded functions\nand robustness is ensured via a robust loss function $\\ell_\\sigma$. Using Huber\nregression as a close-up example within Tikhonov-regularized risk minimization\nin reproducing kernel Hilbert spaces (RKHS), we address two central challenges:\n(i) the breakdown of standard concentration tools under weak moment\nassumptions, and (ii) the analytical difficulties introduced by unbounded\nhypothesis spaces. Our first message is conceptual: conventional\ngeneralization-error bounds for robust losses do not faithfully capture\nout-of-sample performance. We argue that learnability should instead be\nquantified through prediction error, namely the $L_2$-distance to the truth\n$f^\\star$, which is $\\sigma$-independent and directly reflects the target of\nrobust estimation. To make this workable under unboundedness, we introduce a\n\\emph{probabilistic effective hypothesis space} that confines the estimator\nwith high probability and enables a meaningful bias--variance decomposition\nunder weak $(1+\\epsilon)$-moment conditions. Technically, we establish new\ncomparison theorems linking the excess robust risk to the $L_2$ prediction\nerror up to a residual of order $\\mathcal{O}(\\sigma^{-2\\epsilon})$, clarifying\nthe robustness--bias trade-off induced by the scale parameter $\\sigma$.\nBuilding on this, we derive explicit finite-sample error bounds and convergence\nrates for Huber regression in RKHS that hold without uniform boundedness and\nunder heavy-tailed noise. Our study delivers principled tuning rules, extends\nbeyond Huber to other robust losses, and highlights prediction error, not\nexcess generalization risk, as the fundamental lens for analyzing robust\nlearning."}
{"id": "2510.11285", "categories": ["quant-ph", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.11285", "abs": "https://arxiv.org/abs/2510.11285", "authors": ["Pablo Tieben", "Jan Rhensius", "Takuya F. Segawa", "Risei Abe", "Konosuke Shimazaki", "Shigeki Takeuchi", "Andeas W. Schell", "Hideaki Takashima"], "title": "Bright Single-Photon Emission from Individual Tin-Vacancy Centers in Multi-Cone Diamond Waveguides", "comment": null, "summary": "Diamonds containing color centers have recently gathered significant\nattention for photonic quantum technologies, including quantum sensing,\nphotonic quantum computers, and quantum networks. Among the various color\ncenters, tin-vacancy (SnV) centers are particularly promising due to the high\nemission efficiency from the zero-phonon line and due to their long spin\ncoherence times. However, the extraction of photons from diamond remains a key\nchallenge. Here we demonstrate high photon extraction from a single SnV center\nincorporated in a diamond nanopillar with tapered sidewalls and a multi-cone\nstructure. A sharp emission peak with a full width at half maximum (FWHM) of\n$6\\,$nm was observed at a wavelength of $619\\,$nm. Furthermore, the\nsecond-order correlation function exhibited an antibunching dip well below\n$g^{(2)}(0) = 0.5$, indicating single-photon emission. Remarkably, the emitter\nachieved a high saturation count rate of approximately $9\\,$Mcps. These results\nestablish our nanopillar platform as a promising candidate for bright and\nstable quantum sources and sensors based on SnV centers in diamond."}
{"id": "2510.09891", "categories": ["cs.LG", "cs.AI", "physics.ao-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09891", "abs": "https://arxiv.org/abs/2510.09891", "authors": ["Parsa Gooya", "Reinel Sospedra-Alfonso"], "title": "Probabilistic bias adjustment of seasonal predictions of Arctic Sea Ice Concentration", "comment": null, "summary": "Seasonal forecast of Arctic sea ice concentration is key to mitigate the\nnegative impact and assess potential opportunities posed by the rapid decline\nof sea ice coverage. Seasonal prediction systems based on climate models often\nshow systematic biases and complex spatio-temporal errors that grow with the\nforecasts. Consequently, operational predictions are routinely bias corrected\nand calibrated using retrospective forecasts. For predictions of Arctic sea ice\nconcentration, error corrections are mainly based on one-to-one post-processing\nmethods including climatological mean or linear regression correction and, more\nrecently, machine learning. Such deterministic adjustments are confined at best\nto the limited number of costly-to-run ensemble members of the raw forecast.\nHowever, decision-making requires proper quantification of uncertainty and\nlikelihood of events, particularly of extremes. We introduce a probabilistic\nerror correction framework based on a conditional Variational Autoencoder model\nto map the conditional distribution of observations given the biased model\nprediction. This method naturally allows for generating large ensembles of\nadjusted forecasts. We evaluate our model using deterministic and probabilistic\nmetrics and show that the adjusted forecasts are better calibrated, closer to\nthe observational distribution, and have smaller errors than climatological\nmean adjusted forecasts."}
{"id": "2510.11329", "categories": ["quant-ph", "hep-th"], "pdf": "https://arxiv.org/pdf/2510.11329", "abs": "https://arxiv.org/abs/2510.11329", "authors": ["Ming-Ming Du Yi-Hao Fan", "Hong-Wei Li", "Shu-Ting Shen", "Xiao-Jing Yan", "Xi-Yun Li", "Wei Zhong", "Yu-Bo Sheng", "Lan Zhou"], "title": "Basis-independent Coherence in Noninertial Frames", "comment": "4 pages, 1 figures", "summary": "We investigate the behavior of basis-independent quantum coherence between\ntwo modes of a free Dirac field as observed by relatively accelerated\nobservers. Our findings reveal three key results: (i) the basis-independent\ncoherence between modes A and BI decreases with increasing acceleration but\nremains finite even in the limit of infinite acceleration; (ii) at zero\nacceleration, the coherence between modes $A$ and $B_II$ is nonzero contrasting\nwith the behavior of basis-dependent coherence, which typically vanishes in\nthis case; and (iii) the basis-independent coherence between modes BI and BII\nremains constant regardless of acceleration, exhibiting a freezing phenomenon.\nThese results demonstrate the intrinsic robustness of basis-independent\ncoherence under Unruh effects."}
{"id": "2510.09895", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09895", "abs": "https://arxiv.org/abs/2510.09895", "authors": ["Yubo Li", "Rema Padman"], "title": "Chain-of-Influence: Tracing Interdependencies Across Time and Features in Clinical Predictive Modelings", "comment": null, "summary": "Modeling clinical time-series data is hampered by the challenge of capturing\nlatent, time-varying dependencies among features. State-of-the-art approaches\noften rely on black-box mechanisms or simple aggregation, failing to explicitly\nmodel how the influence of one clinical variable propagates through others over\ntime. We propose $\\textbf{Chain-of-Influence (CoI)}$, an interpretable deep\nlearning framework that constructs an explicit, time-unfolded graph of feature\ninteractions. CoI leverages a multi-level attention architecture: first, a\ntemporal attention layer identifies critical time points in a patient's record;\nsecond, a cross-feature attention layer models the directed influence from\nfeatures at these time points to subsequent features. This design enables the\ntracing of influence pathways, providing a granular audit trail that shows how\nany feature at any time contributes to the final prediction, both directly and\nthrough its influence on other variables. We evaluate CoI on mortality and\ndisease progression tasks using the MIMIC-IV dataset and a private chronic\nkidney disease cohort. Our framework significantly outperforms existing methods\nin predictive accuracy. More importantly, through case studies, we show that\nCoI can uncover clinically meaningful, patient-specific patterns of disease\nprogression that are opaque to other models, offering unprecedented\ntransparency into the temporal and cross-feature dependencies that inform\nclinical decision-making."}
{"id": "2510.11349", "categories": ["quant-ph", "physics.hist-ph"], "pdf": "https://arxiv.org/pdf/2510.11349", "abs": "https://arxiv.org/abs/2510.11349", "authors": ["Andrea Di Biagio", "Carlo Rovelli"], "title": "Relative Information, Relative Facts", "comment": "7+3 pages", "summary": "We offer a fresh perspective on the relational interpretation of quantum\nmechanics as a way of thinking about the world described by quantum theory\nbased on quantifiable notions of information. This allows us to provide a\ndefinition of a relative fact, with no addition to orthodox quantum theory and\nno fundamentally special role for observers. By associating perspectives with\ncommutative observables rather than entire quantum systems, several previous\nproblems with the interpretation are dissolved. As a side result, we show how a\nquantum measurement, properly described, is a continuous process."}
{"id": "2510.09898", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09898", "abs": "https://arxiv.org/abs/2510.09898", "authors": ["Hung Phan", "Son Le Vu", "Ali Jannesari"], "title": "Learning Bug Context for PyTorch-to-JAX Translation with LLMs", "comment": null, "summary": "Despite recent progress of large language models (LLMs) on code translation\namong mainstream languages, translating PyTorch to JAX remains nontrivial. The\ntwo libraries, though both embedded in Python, differ in core design, execution\nsemantics, and ecosystem maturity; JAX is newer and comparatively\nunderrepresented in public code, and parallel PyTorch--JAX corpora are limited.\nWeaknesses in existing evaluation further complicate cross-framework\nbenchmarking. We present T2J, a prompt-augmentation framework that strengthens\nLLM-based PyTorch to JAX translation. Our pipeline (i) assembles two PyTorch\nsources -- the problem-solving set from TorchLeet (Aroori & Chien, 2025) and a\nGitHub-derived set from CodeParrot (Wolf et al., 2022) -- and uses GPT-4o-mini\nto produce initial JAX drafts; (ii) engages two professional developers to\niteratively repair those drafts until functional equivalence, yielding a\ncurated fixed-bug dataset of common errors and patches; and (iii) constructs\naugmented prompts that inject structured guidance from these fixes to steer\nlightweight LLMs (e.g., GPT-4o-mini). We also introduce three metrics tailored\nto PyTorch to JAX: T2J CodeTrans Score, T2J FixCost Score (an LLM-based\nestimate of bug-fix effort), and T2J Comparison Score (LLM-as-judge).\nEmpirically, T2J raises GPT-4o-mini performance by up to 10% on CodeBLEU, 50%\non T2J FixCost Score, 1.33 points on T2J CodeTrans Score (0--4 scale), and 100%\non T2J Comparison Score; moreover, the generated code runs up to 2.5x faster\nthan the baseline."}
{"id": "2510.11376", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.11376", "abs": "https://arxiv.org/abs/2510.11376", "authors": ["Guoqing Tian", "Li-Li Zheng", "Zhi-Ming Zhan", "Franco Nori", "Xin-You Lü"], "title": "Disorder-Induced Strongly Correlated Photons in Waveguide QED", "comment": "7 pages, 5 figures + Supplemental Material", "summary": "Strongly correlated photons play a crucial role in modern quantum\ntechnologies. Here, we investigate the probability of generating strongly\ncorrelated photons in a chain of N qubits coupled to a one-dimensional (1D)\nwaveguide. We found that disorder in the transition frequencies can induce\nphoton antibunching, and especially nearly perfect photon blockade events in\nthe transmission and reflection outputs. As a comparison, in ordered chains,\nstrongly correlated photons cannot be generated in the transmission output, and\nonly weakly antibunched photons are found in the reflection output. The\noccurrence of nearly perfect photon blockade events stems from the\ndisorder-induced near completely destructive interference of photon scattering\npaths. Our work highlights the impact of disorder on photon correlation\ngeneration and suggests that disorder can enhance the potential for achieving\nstrongly correlated photon."}
{"id": "2510.09904", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.09904", "abs": "https://arxiv.org/abs/2510.09904", "authors": ["Kelvin Kan", "Xingjian Li", "Benjamin J. Zhang", "Tuhin Sahai", "Stanley Osher", "Krishna Kumar", "Markos A. Katsoulakis"], "title": "Stability of Transformers under Layer Normalization", "comment": null, "summary": "Despite their widespread use, training deep Transformers can be unstable.\nLayer normalization, a standard component, improves training stability, but its\nplacement has often been ad-hoc. In this paper, we conduct a principled study\non the forward (hidden states) and backward (gradient) stability of\nTransformers under different layer normalization placements. Our theory\nprovides key insights into the training dynamics: whether training drives\nTransformers toward regular solutions or pathological behaviors. For forward\nstability, we derive explicit bounds on the growth of hidden states in trained\nTransformers. For backward stability, we analyze how layer normalization\naffects the backpropagation of gradients, thereby explaining the training\ndynamics of each layer normalization placement. Our analysis also guides the\nscaling of residual steps in Transformer blocks, where appropriate choices can\nfurther improve stability and performance. Our numerical results corroborate\nour theoretical findings. Beyond these results, our framework provides a\nprincipled way to sanity-check the stability of Transformers under new\narchitectural modifications, offering guidance for future designs."}
{"id": "2510.11435", "categories": ["quant-ph", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2510.11435", "abs": "https://arxiv.org/abs/2510.11435", "authors": ["Calum Robson"], "title": "The Dirac equation and the Quantum Potential", "comment": "15 Pages", "summary": "One key theme of Basil Hiley's work was the development of David Bohm's\napproach to Quantum Mechanics; in particular the concept of the quantum\npotential. Another theme was the importance of Clifford Algebras in fundamental\nphysics. In this paper I will combine these approaches by looking at how the\nquantum potential can be extended to the Dirac equation. I will begin by\ndiscussing the geometry of the Dirac equation, and how this is made clearer by\nthe use of Clifford algebras .Next, I will rewrite the Cl(2) Dirac wavefunction\nin Polar form, and show that new behaviour arises due to topological\nnonlocality. Finally, I discuss the relationship between the Dirac and\nSchroedinger equations."}
{"id": "2510.09914", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.09914", "abs": "https://arxiv.org/abs/2510.09914", "authors": ["Aditya Malusare", "Vineet Punyamoorty", "Vaneet Aggarwal"], "title": "Augmenting generative models with biomedical knowledge graphs improves targeted drug discovery", "comment": "This paper has been accepted for publication in the IEEE Transactions\n  on Artificial Intelligence, October 2025", "summary": "Recent breakthroughs in generative modeling have demonstrated remarkable\ncapabilities in molecular generation, yet the integration of comprehensive\nbiomedical knowledge into these models has remained an untapped frontier. In\nthis study, we introduce K-DREAM (Knowledge-Driven Embedding-Augmented Model),\na novel framework that leverages knowledge graphs to augment diffusion-based\ngenerative models for drug discovery. By embedding structured information from\nlarge-scale knowledge graphs, K-DREAM directs molecular generation toward\ncandidates with higher biological relevance and therapeutic suitability. This\nintegration ensures that the generated molecules are aligned with specific\ntherapeutic targets, moving beyond traditional heuristic-driven approaches. In\ntargeted drug design tasks, K-DREAM generates drug candidates with improved\nbinding affinities and predicted efficacy, surpassing current state-of-the-art\ngenerative models. It also demonstrates flexibility by producing molecules\ndesigned for multiple targets, enabling applications to complex disease\nmechanisms. These results highlight the utility of knowledge-enhanced\ngenerative models in rational drug design and their relevance to practical\ntherapeutic development."}
{"id": "2510.11488", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.11488", "abs": "https://arxiv.org/abs/2510.11488", "authors": ["Walter O. Krawec"], "title": "Finite Key Rates for QKD Protocols with Data Filtering", "comment": null, "summary": "In this paper, we derive a new proof of security for a general class of\nquantum cryptographic protocol involving filtering and discarded data. We\nderive a novel bound on the quantum min entropy of such a system, based in\nlarge part on properties of a certain classical sampling strategy. Finally, we\nshow how our methods can be used to readily prove security of the Extended B92\nprotocol, providing the first finite key proof of security for this protocol\nagainst general, coherent, attacks."}
{"id": "2510.09916", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09916", "abs": "https://arxiv.org/abs/2510.09916", "authors": ["Manuel Segura", "Pere Vergés", "Richard Ky", "Ramesh Arangott", "Angela Kristine Garcia", "Thang Dihn Trong", "Makoto Hyodo", "Alexandru Nicolau", "Tony Givargis", "Sergio Gago-Masague"], "title": "Advancing Intoxication Detection: A Smartwatch-Based Approach", "comment": null, "summary": "Excess alcohol consumption leads to serious health risks and severe\nconsequences for both individuals and their communities. To advocate for\nhealthier drinking habits, we introduce a groundbreaking mobile smartwatch\napplication approach to just-in-time interventions for intoxication warnings.\nIn this work, we have created a dataset gathering TAC, accelerometer,\ngyroscope, and heart rate data from the participants during a period of three\nweeks. This is the first study to combine accelerometer, gyroscope, and heart\nrate smartwatch data collected over an extended monitoring period to classify\nintoxication levels. Previous research had used limited smartphone motion data\nand conventional machine learning (ML) algorithms to classify heavy drinking\nepisodes; in this work, we use smartwatch data and perform a thorough\nevaluation of different state-of-the-art classifiers such as the Transformer,\nBidirectional Long Short-Term Memory (bi-LSTM), Gated Recurrent Unit (GRU),\nOne-Dimensional Convolutional Neural Networks (1D-CNN), and Hyperdimensional\nComputing (HDC). We have compared performance metrics for the algorithms and\nassessed their efficiency on resource-constrained environments like mobile\nhardware. The HDC model achieved the best balance between accuracy and\nefficiency, demonstrating its practicality for smartwatch-based applications."}
{"id": "2510.11526", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.11526", "abs": "https://arxiv.org/abs/2510.11526", "authors": ["Mark Deaconu", "Nihar Gargava", "Amolak Ratan Kalra", "Michele Mosca", "Jon Yard"], "title": "Buildings for Synthesis with Clifford+R", "comment": "26 pages, 5 figures", "summary": "We study the problem of exact synthesis for the Clifford+R gate set and give\nthe explicit structure of the underlying Bruhat-Tits building for this group.\nIn this process, we also give an alternative proof of the arithmetic nature of\nthe Clifford+R gate set."}
{"id": "2510.09923", "categories": ["cs.LG", "math.OC", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09923", "abs": "https://arxiv.org/abs/2510.09923", "authors": ["Nikola Surjanovic", "Alexandre Bouchard-Côté", "Trevor Campbell"], "title": "AutoGD: Automatic Learning Rate Selection for Gradient Descent", "comment": null, "summary": "The performance of gradient-based optimization methods, such as standard\ngradient descent (GD), greatly depends on the choice of learning rate. However,\nit can require a non-trivial amount of user tuning effort to select an\nappropriate learning rate schedule. When such methods appear as inner loops of\nother algorithms, expecting the user to tune the learning rates may be\nimpractical. To address this, we introduce AutoGD: a gradient descent method\nthat automatically determines whether to increase or decrease the learning rate\nat a given iteration. We establish the convergence of AutoGD, and show that we\ncan recover the optimal rate of GD (up to a constant) for a broad class of\nfunctions without knowledge of smoothness constants. Experiments on a variety\nof traditional problems and variational inference optimization tasks\ndemonstrate strong performance of the method, along with its extensions to\nAutoBFGS and AutoLBFGS."}
{"id": "2510.11544", "categories": ["quant-ph", "physics.hist-ph"], "pdf": "https://arxiv.org/pdf/2510.11544", "abs": "https://arxiv.org/abs/2510.11544", "authors": ["Jacques L. Pienaar"], "title": "French on London and Bauer, and QBism", "comment": "23 pages excluding references. To appear in: Studies in History and\n  Philosophy of Science: Special Issue on The London-Bauer-French\n  Interpretation", "summary": "In this article I compare two interpretations of quantum mechanics (QM) that\ndraw inspiration from phenomenology: the London-Bauer-French interpretation\n(hereafter LBF) as articulated by Steven French, and QBism. I give special\nattention to certain disagreements between QBism and LBF identified French's\nwork, as well as French's related claims that QBism may be at odds with key\nideas in phenomenology. My main finding is that QBism does not fare so badly\nwith phenomenology as French makes out; in particular it can be made compatible\nwith Zahavi's correlationism and Husserl's notion of intersubjectivity, both of\nwhich strongly inform LBF. Nevertheless, I concur with French's argument that\nQBism is incompatible with the conception of quantum measurement in LBF, hence\nalso with that of Merleau-Ponty, as the latter based his own analysis on that\nof London and Bauer. I explain why I find QBism's account preferable in this\ncase."}
{"id": "2510.09926", "categories": ["cs.LG", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.09926", "abs": "https://arxiv.org/abs/2510.09926", "authors": ["Naman Agrawal"], "title": "Phase-Aware Deep Learning with Complex-Valued CNNs for Audio Signal Applications", "comment": null, "summary": "This study explores the design and application of Complex-Valued\nConvolutional Neural Networks (CVCNNs) in audio signal processing, with a focus\non preserving and utilizing phase information often neglected in real-valued\nnetworks. We begin by presenting the foundational theoretical concepts of\nCVCNNs, including complex convolutions, pooling layers, Wirtinger-based\ndifferentiation, and various complex-valued activation functions. These are\ncomplemented by critical adaptations of training techniques, including complex\nbatch normalization and weight initialization schemes, to ensure stability in\ntraining dynamics. Empirical evaluations are conducted across three stages.\nFirst, CVCNNs are benchmarked on standard image datasets, where they\ndemonstrate competitive performance with real-valued CNNs, even under synthetic\ncomplex perturbations. Although our focus is audio signal processing, we first\nevaluate CVCNNs on image datasets to establish baseline performance and\nvalidate training stability before applying them to audio tasks. In the second\nexperiment, we focus on audio classification using Mel-Frequency Cepstral\nCoefficients (MFCCs). CVCNNs trained on real-valued MFCCs slightly outperform\nreal CNNs, while preserving phase in input workflows highlights challenges in\nexploiting phase without architectural modifications. Finally, a third\nexperiment introduces GNNs to model phase information via edge weighting, where\nthe inclusion of phase yields measurable gains in both binary and multi-class\ngenre classification. These results underscore the expressive capacity of\ncomplex-valued architectures and confirm phase as a meaningful and exploitable\nfeature in audio processing applications. While current methods show promise,\nespecially with activations like cardioid, future advances in phase-aware\ndesign will be essential to leverage the potential of complex representations\nin neural networks."}
{"id": "2510.11585", "categories": ["quant-ph", "physics.atom-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2510.11585", "abs": "https://arxiv.org/abs/2510.11585", "authors": ["Dounan Du", "Eden Figueroa"], "title": "Telecom-compatible cross-band quantum memory via dual photon modes dark-state polaritons", "comment": "6 pages main article, 1 page supplemental material", "summary": "Quantum memories are essential components of quantum networks, enabling\nsynchronization, quantum repeaters, and long-distance entanglement\ndistribution. Most ensemble-based realizations rely on dark-state polaritons\n(DSPs) in $\\Lambda$-type systems that operate at near-infrared wavelengths,\nsuch as 795 nm in $^{87}$Rb, far from the telecom band where long fiber\ntransmission is optimal. Here we identify a DSP in $^{87}$Rb that coherently\ncouples two photonic modes at 795 nm and 1324 nm through a shared spin-wave\ncoherence. We derive its field operator and group velocity, extending the\nFleischhauer-Lukin model to a dual-wavelength regime, and formulate a memory\nprotocol enabling bidirectional storage and retrieval between the two modes.\nNumerical simulations of the full six-level dynamics confirm two-way storage\nand retrieval for both same-mode and cross-mode operation between the two\nwavelengths. The results demonstrate a dual-wavelength memory that unifies\nnode-band and telecom-band operation within a single ensemble, providing a\npotential route toward frequency-conversion-free quantum-network interfaces."}
{"id": "2510.09930", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09930", "abs": "https://arxiv.org/abs/2510.09930", "authors": ["Ching Chang", "Ming-Chih Lo", "Chiao-Tung Chan", "Wen-Chih Peng", "Tien-Fu Chen"], "title": "MemPromptTSS: Persistent Prompt Memory for Iterative Multi-Granularity Time Series State Segmentation", "comment": "This paper is currently under review. The code will be made available\n  upon acceptance", "summary": "Web platforms, mobile applications, and connected sensing systems generate\nmultivariate time series with states at multiple levels of granularity, from\ncoarse regimes to fine-grained events. Effective segmentation in these settings\nrequires integrating across granularities while supporting iterative refinement\nthrough sparse prompt signals, which provide a compact mechanism for injecting\ndomain knowledge. Yet existing prompting approaches for time series\nsegmentation operate only within local contexts, so the effect of a prompt\nquickly fades and cannot guide predictions across the entire sequence. To\novercome this limitation, we propose MemPromptTSS, a framework for iterative\nmulti-granularity segmentation that introduces persistent prompt memory. A\nmemory encoder transforms prompts and their surrounding subsequences into\nmemory tokens stored in a bank. This persistent memory enables each new\nprediction to condition not only on local cues but also on all prompts\naccumulated across iterations, ensuring their influence persists across the\nentire sequence. Experiments on six datasets covering wearable sensing and\nindustrial monitoring show that MemPromptTSS achieves 23% and 85% accuracy\nimprovements over the best baseline in single- and multi-granularity\nsegmentation under single iteration inference, and provides stronger refinement\nin iterative inference with average per-iteration gains of 2.66 percentage\npoints compared to 1.19 for PromptTSS. These results highlight the importance\nof persistent memory for prompt-guided segmentation, establishing MemPromptTSS\nas a practical and effective framework for real-world applications."}
{"id": "2510.11593", "categories": ["quant-ph", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11593", "abs": "https://arxiv.org/abs/2510.11593", "authors": ["Seong-Joon Park", "Hee-Youl Kwak", "Yongjune Kim"], "title": "Hierarchical Qubit-Merging Transformer for Quantum Error Correction", "comment": "6 pages, 5 figures", "summary": "For reliable large-scale quantum computation, a quantum error correction\n(QEC) scheme must effectively resolve physical errors to protect logical\ninformation. Leveraging recent advances in deep learning, neural network-based\ndecoders have emerged as a promising approach to enhance the reliability of\nQEC. We propose the Hierarchical Qubit-Merging Transformer (HQMT), a novel and\ngeneral decoding framework that explicitly leverages the structural graph of\nstabilizer codes to learn error correlations across multiple scales. Our\narchitecture first computes attention locally on structurally related groups of\nstabilizers and then systematically merges these qubit-centric representations\nto build a global view of the error syndrome. The proposed HQMT achieves\nsubstantially lower logical error rates for surface codes by integrating a\ndedicated qubit-merging layer within the transformer architecture. Across\nvarious code distances, HQMT significantly outperforms previous neural\nnetwork-based QEC decoders as well as a powerful belief propagation with\nordered statistics decoding (BP+OSD) baseline. This hierarchical approach\nprovides a scalable and effective framework for surface code decoding,\nadvancing the realization of reliable quantum computing."}
{"id": "2510.09942", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.09942", "abs": "https://arxiv.org/abs/2510.09942", "authors": ["Payel Bhattacharjee", "Fengwei Tian", "Meiyu Zhong", "Guangyi Zhang", "Osvaldo Simeone", "Ravi Tandon"], "title": "Conformal Sparsification for Bandwidth-Efficient Edge-Cloud Speculative Decoding", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: AI and ML for Next-Generation Wireless Communications and\n  Networking (AI4NextG)", "summary": "Edge-cloud speculative decoding (SD) accelerates inference by having a\ncloud-based large language model (LLM) that verifies draft tokens generated by\na resource-constrained small language model (SLM) at the edge. A central\nbottleneck is the limited bandwidth of the edge-cloud link, which necessitates\nefficient compression of draft token distributions. We first derive an\ninformation-theoretic bound that decomposes the token rejection rate into\ncontributions from SLM-LLM distribution mismatch and from quantization\ndistortion. Guided by this analysis, we propose the Sparse Quantize-and-Sample\nSD (SQS-SD) framework, which exploits distributional sparsity through\nstructured sparsification and lattice-based quantization. Within this\nframework, K-SQS applies fixed top-K truncation, while C-SQS adaptively adjusts\nthe retained token set via online conformal prediction to ensure bounded\ndeviation from the dense distribution. Empirical results confirm that both\napproaches improve end-to-end latency and rejection rates in complimentary\noperating regimes."}
{"id": "2510.11601", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2510.11601", "abs": "https://arxiv.org/abs/2510.11601", "authors": ["Yi J. Zhao", "Joel E. Moore", "Juzar Thingna", "Christopher W. Wächtler"], "title": "Quantum Synchronization of Perturbed Oscillating Coherences", "comment": null, "summary": "Synchronization in quantum systems has been recently studied through\npersistent oscillations of local observables, which stem from undamped modes of\nthe dissipative dynamics. However, the existence of such modes requires\nfine-tuning the system to satisfy specific symmetry constraints. We investigate\nthe response of spin systems that possess such oscillating modes to generic,\nweak perturbations. We show that even when these perturbations break the\nsymmetry and lead to a single steady state, the phase correlations in the\nresulting state exhibit signatures of synchronization. Our results therefore\nconnect the persistent oscillation notion (dynamical) and the notion based on\nphase correlations (steady-state) of synchronization, which so far have been\nregarded as distinct phenomena. Furthermore, we demonstrate that steady-state\nsynchronization in these systems can exhibit features that are absent in the\ndynamical synchronization. Our work suggests robustness of synchronization and\npoints toward a potential unifying framework of quantum synchronization."}
{"id": "2510.09959", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09959", "abs": "https://arxiv.org/abs/2510.09959", "authors": ["Jun Yin", "Runcheng Cai", "Shiliang Sun"], "title": "Clustering Result Re-guided Incomplete Multi-view Spectral Clustering", "comment": null, "summary": "Incomplete multi-view spectral clustering generalizes spectral clustering to\nmulti-view data and simultaneously realizes the partition of multi-view data\nwith missing views. For this category of method, K-means algorithm needs to be\nperformed to generate the clustering result after the procedure of feature\nextraction. More importantly, the connectivity of samples reflected by the\nclustering result is not utilized effectively. To overcome these defects, we\npropose Clustering Result re-Guided Incomplete Multi-view Spectral Clustering\n(CRG_IMSC). CRG_IMSC obtains the clustering result directly by imposing\nnonnegative constraint to the extracted feature. Furthermore, it constructs the\nconnectivity matrix according to the result of spectral clustering, and\nminimizes the residual of self-representation based on the connectivity matrix.\nA novel iterative algorithm using multiplicative update is developed to solve\nthe optimization problem of CRG_IMSC, and its convergence is proved rigorously.\nOn benchmark datasets, for multi-view data, CRG_IMSC performs better than\nstate-of-the-art clustering methods, and the experimental results also\ndemonstrate the convergence of CRG_IMSC algorithm."}
{"id": "2510.11621", "categories": ["quant-ph", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2510.11621", "abs": "https://arxiv.org/abs/2510.11621", "authors": ["Nick S. Blunt", "Aleksei V. Ivanov", "Andreas Juul Bay-Smidt"], "title": "A Monte Carlo approach to bound Trotter error", "comment": "20 pages, 7 figures", "summary": "Trotter product formulas are a natural and powerful approach to perform\nquantum simulation. However, the error analysis of product formulas is\nchallenging, and their cost is often overestimated. It is established that\nTrotter error can be bounded in terms of spectral norms of nested commutators\nof the Hamiltonian partitions [Childs et al., Phys. Rev. X 11, 011020], but\nevaluating these expressions is challenging, often achieved by repeated\napplication of the triangle inequality, significantly loosening the bound.\nHere, we show that the spectral norm of an operator can be upper bounded by the\nspectral norm of an equivalent sign-problem-free operator, which can be\ncalculated efficiently to large system sizes using projector Monte Carlo\nsimulation. For a range of Hamiltonians and considering second-order formulas,\nwe demonstrate that this Monte Carlo-based bound is often extremely tight, and\neven exact in some instances. For the uniform electron gas we reduce the cost\nof performing Trotterization from the literature by an order of magnitude. For\nthe Pariser-Parr-Pople model for linear acene molecules, which has\n$\\mathcal{O}(N^2)$ long-range interaction terms, we show that it suffices to\nuse $\\mathcal{O}(N^{0.57})$ Trotter steps and circuit depth\n$\\mathcal{O}(N^{1.57})$ to implement Hamiltonian simulation. We hope that this\napproach will lead to a better understanding of the potential accuracy of\nTrotterization in a range of important applications."}
{"id": "2510.09965", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09965", "abs": "https://arxiv.org/abs/2510.09965", "authors": ["Shuo Zhao", "Yongqiang Li", "Yu Feng", "Zhongsheng Hou", "Yuanjing Feng"], "title": "Homomorphic Mappings for Value-Preserving State Aggregation in Markov Decision Processes", "comment": null, "summary": "State aggregation aims to reduce the computational complexity of solving\nMarkov Decision Processes (MDPs) while preserving the performance of the\noriginal system. A fundamental challenge lies in optimizing policies within the\naggregated, or abstract, space such that the performance remains optimal in the\nground MDP-a property referred to as {\"}optimal policy equivalence {\"}.\n  This paper presents an abstraction framework based on the notion of\nhomomorphism, in which two Markov chains are deemed homomorphic if their value\nfunctions exhibit a linear relationship. Within this theoretical framework, we\nestablish a sufficient condition for the equivalence of optimal policy.\n  We further examine scenarios where the sufficient condition is not met and\nderive an upper bound on the approximation error and a performance lower bound\nfor the objective function under the ground MDP. We propose Homomorphic Policy\nGradient (HPG), which guarantees optimal policy equivalence under sufficient\nconditions, and its extension, Error-Bounded HPG (EBHPG), which balances\ncomputational efficiency and the performance loss induced by aggregation. In\nthe experiments, we validated the theoretical results and conducted comparative\nevaluations against seven algorithms."}
{"id": "2510.11679", "categories": ["quant-ph", "cond-mat.quant-gas", "hep-lat", "hep-ph"], "pdf": "https://arxiv.org/pdf/2510.11679", "abs": "https://arxiv.org/abs/2510.11679", "authors": ["Daniel K. Mark", "Federica M. Surace", "Thomas Schuster", "Adam L. Shaw", "Wenjie Gong", "Soonwon Choi", "Manuel Endres"], "title": "Observation of ballistic plasma and memory in high-energy gauge theory dynamics", "comment": null, "summary": "Gauge theories describe the fundamental forces of nature. However,\nhigh-energy dynamics, such as the formation of quark-gluon plasmas, is\nnotoriously difficult to model with classical methods. Quantum simulation\noffers a promising alternative in this regime, yet experiments have mainly\nprobed low energies. Here, we observe the formation of a ballistic plasma and\nlong-time memory effects in high-energy gauge theory dynamics on a\nhigh-precision quantum simulator. Both observations are unexpected, as the\ninitial state - fully filled with particle-antiparticle pairs - was thought to\nrapidly thermalize. Instead, we find correlations spreading ballistically to\nlong distances and a memory of charge clusters. Our observations cannot be\nexplained by many-body scars, but are captured by a new theory of plasma\noscillations between electric field and current operators, persisting all the\nway to the continuum limit of the (1+1)D Schwinger model, of which we simulate\na lattice version. Adapting techniques from quantum optics, we visualize plasma\noscillations as rotations of Wigner distributions, leading to a novel set of\npredictions which we test in experiment and numerics. The new framework\nencompasses both our scenario and scars, which show up as coherent states of\nthe plasma. The experimental surprises we observe in the high-energy dynamics\nof a simple gauge theory point to the potential of high-precision quantum\nsimulations of gauge theories for general scientific discovery."}
{"id": "2510.09976", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09976", "abs": "https://arxiv.org/abs/2510.09976", "authors": ["Mingyang Lyu", "Yinqian Sun", "Erliang Lin", "Huangrui Li", "Ruolin Chen", "Feifei Zhao", "Yi Zeng"], "title": "Reinforcement Fine-Tuning of Flow-Matching Policies for Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models such as OpenVLA, Octo, and $\\pi_0$ have\nshown strong generalization by leveraging large-scale demonstrations, yet their\nperformance is still fundamentally constrained by the quality and coverage of\nsupervised data. Reinforcement learning (RL) provides a promising path for\nimproving and fine-tuning VLAs through online interaction. However,\nconventional policy gradient methods are computationally infeasible in the\ncontext of flow-matching based models due to the intractability of the\nimportance sampling process, which requires explicit computation of policy\nratios. To overcome this limitation, we propose Flow Policy Optimization (FPO)\nalgorithm, which reformulates importance sampling by leveraging per-sample\nchanges in the conditional flow-matching objective. Furthermore, FPO achieves\nstable and scalable online reinforcement fine-tuning of the $\\pi_0$ model by\nintegrating structure-aware credit assignment to enhance gradient efficiency,\nclipped surrogate objectives to stabilize optimization, multi-step latent\nexploration to encourage diverse policy updates, and a Q-ensemble mechanism to\nprovide robust value estimation. We evaluate FPO on the LIBERO benchmark and\nthe ALOHA simulation task against supervised, preference-aligned,\ndiffusion-based, autoregressive online RL, and $\\pi_0$-FAST baselines,\nobserving consistent improvements over the imitation prior and strong\nalternatives with stable learning under sparse rewards. In addition, ablation\nstudies and analyses of the latent space dynamics further highlight the\ncontributions of individual components within FPO, validating the effectiveness\nof the proposed computational modules and the stable convergence of the\nconditional flow-matching objective during online RL."}
{"id": "2510.11681", "categories": ["quant-ph", "cond-mat.stat-mech", "hep-lat", "hep-ph", "hep-th"], "pdf": "https://arxiv.org/pdf/2510.11681", "abs": "https://arxiv.org/abs/2510.11681", "authors": ["Lukas Ebner", "Berndt Müller", "Andreas Schäfer", "Leonhard Schmotzer", "Clemens Seidl", "Xiaojun Yao"], "title": "The Magic Barrier before Thermalization", "comment": "6 pages, 3 figures", "summary": "We investigate the time dependence of anti-flatness in the entanglement\nspectrum, a measure for non-stabilizerness and lower bound for non-local\nquantum magic, on a subsystem of a linear SU(2) plaquette chain during\nthermalization. Tracing the time evolution of a large number of initial states,\nwe find that the anti-flatness exhibits a barrier-like maximum during the time\nperiod when the entanglement entropy of the subsystem grows rapidly from the\ninitial value to the microcanonical entropy. The location of the peak is\nstrongly correlated with the time when the entanglement exhibits the strongest\ngrowth. This behavior is found for generic highly excited initial computational\nbasis states and persists for coupling constants across the ergodic regime,\nrevealing a universal structure of the entanglement spectrum during\nthermalization. We conclude that quantitative simulations of thermalization for\nnonabelian gauge theories require quantum computing. We speculate that this\nproperty generalizes to other quantum chaotic systems."}
{"id": "2510.09977", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09977", "abs": "https://arxiv.org/abs/2510.09977", "authors": ["Frida Cantu", "Salomon Ibarra", "Arturo Gonzales", "Jesus Barreda", "Chenang Liu", "Li Zhang"], "title": "An Unsupervised Time Series Anomaly Detection Approach for Efficient Online Process Monitoring of Additive Manufacturing", "comment": "2025 IEEE 21st International Conference on Automation Science and\n  Engineering", "summary": "Online sensing plays an important role in advancing modern manufacturing. The\nreal-time sensor signals, which can be stored as high-resolution time series\ndata, contain rich information about the operation status. One of its popular\nusages is online process monitoring, which can be achieved by effective anomaly\ndetection from the sensor signals. However, most existing approaches either\nheavily rely on labeled data for training supervised models, or are designed to\ndetect only extreme outliers, thus are ineffective at identifying subtle\nsemantic off-track anomalies to capture where new regimes or unexpected\nroutines start. To address this challenge, we propose an matrix profile-based\nunsupervised anomaly detection algorithm that captures fabrication cycle\nsimilarity and performs semantic segmentation to precisely identify the onset\nof defect anomalies in additive manufacturing. The effectiveness of the\nproposed method is demonstrated by the experiments on real-world sensor data."}
{"id": "2510.11706", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2510.11706", "abs": "https://arxiv.org/abs/2510.11706", "authors": ["Siva Darbha", "Alexey Khudorozhkov", "Pedro L. S. Lopes", "Fangli Liu", "Ermal Rrapaj", "Jan Balewski", "Majd Hamdan", "Pavel E. Dolgirev", "Alexander Schuckert", "Katherine Klymko", "Sheng-Tao Wang", "Mikhail D. Lukin", "Daan Camps", "Milan Kornjača"], "title": "Probing emergent prethermal dynamics and resonant melting on a programmable quantum simulator", "comment": "10+15 pages, 4+12 figures", "summary": "The dynamics of isolated quantum systems following a sudden quench plays a\ncentral role in many areas of material science, high-energy physics, and\nquantum chemistry. Featuring complex phenomena with implications for\nthermalization, non-equilibrium phase transitions, and Floquet phase\nengineering, such far-from-equilibrium quantum dynamics is challenging to study\nnumerically, in particular, in high-dimensional systems. Here, we use a\nprogrammable neutral atom quantum simulator to systematically explore quench\ndynamics in spin models with up to 180 qubits. By initializing the system in a\nproduct state and performing quenches across a broad parameter space, we\ndiscover several stable, qualitatively distinct dynamical regimes. We trace\ntheir robustness to Floquet-like prethermal steady states that are stabilized\nover long emergent timescales by strong dynamical constraints. In addition, we\nobserve sharp peaks in the dynamical response that are quantitatively explained\nby the structured melting of prethermalization through resonances. In two\ndimensions, we uncover a sharp dynamical response change that converges with\nincreased system size, that is linked to the proliferation of N\\'{e}el-order\ndefects and indicative of a dynamical phase transition with no equilibrium\nanalogs. Uncovering an intricate interplay between quantum prethermalization\nand emergent dynamical phases, our results demonstrate the use of quantum\nsimulators for revealing complex non-equilibrium quantum many-body phenomena."}
{"id": "2510.09984", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.09984", "abs": "https://arxiv.org/abs/2510.09984", "authors": ["Kartikeya Aneja", "Nagender Aneja", "Murat Kantarcioglu"], "title": "Learning Joint Embeddings of Function and Process Call Graphs for Malware Detection", "comment": null, "summary": "Software systems can be represented as graphs, capturing dependencies among\nfunctions and processes. An interesting aspect of software systems is that they\ncan be represented as different types of graphs, depending on the extraction\ngoals and priorities. For example, function calls within the software can be\ncaptured to create function call graphs, which highlight the relationships\nbetween functions and their dependencies. Alternatively, the processes spawned\nby the software can be modeled to generate process interaction graphs, which\nfocus on runtime behavior and inter-process communication. While these graph\nrepresentations are related, each captures a distinct perspective of the\nsystem, providing complementary insights into its structure and operation.\nWhile previous studies have leveraged graph neural networks (GNNs) to analyze\nsoftware behaviors, most of this work has focused on a single type of graph\nrepresentation. The joint modeling of both function call graphs and process\ninteraction graphs remains largely underexplored, leaving opportunities for\ndeeper, multi-perspective analysis of software systems. This paper presents a\npipeline for constructing and training Function Call Graphs (FCGs) and Process\nCall Graphs (PCGs) and learning joint embeddings. We demonstrate that joint\nembeddings outperform a single-graph model. In this paper, we propose\nGeminiNet, a unified neural network approach that learns joint embeddings from\nboth FCGs and PCGs. We construct a new dataset of 635 Windows executables (318\nmalicious and 317 benign), extracting FCGs via Ghidra and PCGs via Any.Run\nsandbox. GeminiNet employs dual graph convolutional branches with an adaptive\ngating mechanism that balances contributions from static and dynamic views."}
{"id": "2510.09693", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.09693", "abs": "https://arxiv.org/abs/2510.09693", "authors": ["Jiakang Chen"], "title": "Neural PDE Solvers with Physics Constraints: A Comparative Study of PINNs, DRM, and WANs", "comment": "50 pages, 13 figures", "summary": "Partial differential equations (PDEs) underpin models across science and\nengineering, yet analytical solutions are atypical and classical mesh-based\nsolvers can be costly in high dimensions. This dissertation presents a unified\ncomparison of three mesh-free neural PDE solvers, physics-informed neural\nnetworks (PINNs), the deep Ritz method (DRM), and weak adversarial networks\n(WANs), on Poisson problems (up to 5D) and the time-independent Schr\\\"odinger\nequation in 1D/2D (infinite well and harmonic oscillator), and extends the\nstudy to a laser-driven case of Schr\\\"odinger's equation via the\nKramers-Henneberger (KH) transformation.\n  Under a common protocol, all methods achieve low $L_2$ errors\n($10^{-6}$-$10^{-9}$) when paired with forced boundary conditions (FBCs),\nforced nodes (FNs), and orthogonality regularization (OG). Across tasks, PINNs\nare the most reliable for accuracy and recovery of excited spectra; DRM offers\nthe best accuracy-runtime trade-off on stationary problems; WAN is more\nsensitive but competitive when weak-form constraints and FN/OG are used\neffectively. Sensitivity analyses show that FBC removes boundary-loss tuning,\nnetwork width matters more than depth for single-network solvers, and most\ngains occur within 5000-10,000 epochs. The same toolkit solves the KH case,\nindicating transfer beyond canonical benchmarks.\n  We provide practical guidelines for method selection and outline the\nfollowing extensions: time-dependent formulations for DRM and WAN, adaptive\nresidual-driven sampling, parallel multi-state training, and neural domain\ndecomposition. These results support physics-guided neural solvers as credible,\nscalable tools for solving complex PDEs."}
{"id": "2510.10000", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10000", "abs": "https://arxiv.org/abs/2510.10000", "authors": ["Bach C. Le", "Tung V. Dao", "Binh T. Nguyen", "Hong T. M. Chu"], "title": "Tight Robustness Certificates and Wasserstein Distributional Attacks for Deep Neural Networks", "comment": null, "summary": "Wasserstein distributionally robust optimization (WDRO) provides a framework\nfor adversarial robustness, yet existing methods based on global Lipschitz\ncontinuity or strong duality often yield loose upper bounds or require\nprohibitive computation. In this work, we address these limitations by\nintroducing a primal approach and adopting a notion of exact Lipschitz\ncertificate to tighten this upper bound of WDRO. In addition, we propose a\nnovel Wasserstein distributional attack (WDA) that directly constructs a\ncandidate for the worst-case distribution. Compared to existing point-wise\nattack and its variants, our WDA offers greater flexibility in the number and\nlocation of attack points. In particular, by leveraging the piecewise-affine\nstructure of ReLU networks on their activation cells, our approach results in\nan exact tractable characterization of the corresponding WDRO problem.\nExtensive evaluations demonstrate that our method achieves competitive robust\naccuracy against state-of-the-art baselines while offering tighter certificates\nthan existing methods. Our code is available at\nhttps://github.com/OLab-Repo/WDA"}
{"id": "2510.09999", "categories": ["physics.comp-ph", "cond-mat.mtrl-sci", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.09999", "abs": "https://arxiv.org/abs/2510.09999", "authors": ["Yiran Bai", "Feng Xiong", "Xueheng Kuang"], "title": "Random State Approach to Quantum Computation of Electronic-Structure Properties", "comment": null, "summary": "Classical computation of electronic properties in large-scale materials\nremains challenging. Quantum computation has the potential to offer advantages\nin memory footprint and computational scaling. However, general and practical\nquantum algorithms for simulating large-scale materials are still lacking. We\npropose and implement random-state quantum algorithms to calculate\nelectronic-structure properties of real materials. Using a random state circuit\nwith only a few qubits, we employ real-time evolution with first-order Trotter\ndecomposition and Hadamard test to obtain electronic density of states, and we\ndevelop a modified quantum phase estimation algorithm to calculate real-space\nlocal density of states via direct quantum measurements. Furthermore, we\nvalidate these algorithms by numerically computing the density of states and\nspatial distributions of electronic states in graphene, twisted bilayer\ngraphene quasicrystals, and fractal lattices, covering system sizes from\nhundreds to thousands of atoms. Our results manifest that the random-state\nquantum algorithms provide a general and qubit-efficient route to simulating\nelectronic properties of large-scale periodic and aperiodic materials on\nquantum computers."}
{"id": "2510.10004", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10004", "abs": "https://arxiv.org/abs/2510.10004", "authors": ["Jiahui Hong", "Siqing Li", "Muqing Jian", "Luming Yang"], "title": "Bidirectional Time-Frequency Pyramid Network for Enhanced Robust EEG Classification", "comment": "Accepted to IEEE BIBM 2025", "summary": "Existing EEG recognition models suffer from poor cross-paradigm\ngeneralization due to dataset-specific constraints and individual variability.\nTo overcome these limitations, we propose BITE (Bidirectional Time-Freq Pyramid\nNetwork), an end-to-end unified architecture featuring robust multistream\nsynergy, pyramid time-frequency attention (PTFA), and bidirectional adaptive\nconvolutions. The framework uniquely integrates: 1) Aligned time-frequency\nstreams maintaining temporal synchronization with STFT for bidirectional\nmodeling, 2) PTFA-based multi-scale feature enhancement amplifying critical\nneural patterns, 3) BiTCN with learnable fusion capturing forward/backward\nneural dynamics. Demonstrating enhanced robustness, BITE achieves\nstate-of-the-art performance across four divergent paradigms (BCICIV-2A/2B,\nHGD, SD-SSVEP), excelling in both within-subject accuracy and cross-subject\ngeneralization. As a unified architecture, it combines robust performance\nacross both MI and SSVEP tasks with exceptional computational efficiency. Our\nwork validates that paradigm-aligned spectral-temporal processing is essential\nfor reliable BCI systems. Just as its name suggests, BITE \"takes a bite out of\nEEG.\" The source code is available at https://github.com/cindy-hong/BiteEEG."}
{"id": "2510.10836", "categories": ["gr-qc", "hep-th", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.10836", "abs": "https://arxiv.org/abs/2510.10836", "authors": ["Partha Nandi", "Partha Ghose", "Francesco Petruccione"], "title": "Spinning into Quantum Geometry: Dirac and Wheeler-DeWitt Dynamics from Stochastic Helicity", "comment": "This manuscript is 13 pages long and contains 1 figure", "summary": "Spin networks in loop quantum gravity provide a kinematical picture of\nquantum geometry but lack a natural mechanism for dynamical Dirac-type\nevolution, while the Wheeler--DeWitt equation typically enters only as an\nimposed constraint. We propose a stochastic framework in which each\nspin-network edge carries helicity-resolved amplitudes -- two-state internal\nlabels that undergo Poisson-driven flips. The resulting coupled master\nequations, after analytic continuation and the introduction of a fundamental\nlength scale, generate Dirac-type dynamics on discrete geometry. At long times,\nthe same process relaxes to helicity-symmetric equilibrium states, which are\nshown to satisfy a Wheeler--DeWitt-type condition. In this way, both quantum\nevolution and the gravitational constraint emerge within a single probabilistic\nframework. Our approach thus provides a background-independent and stochastic\nroute to quantum geometry, offering an alternative to canonical quantization\nand a fresh perspective on the problem of time."}
{"id": "2510.10023", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10023", "abs": "https://arxiv.org/abs/2510.10023", "authors": ["Yinghui He", "Abhishek Panigrahi", "Yong Lin", "Sanjeev Arora"], "title": "Skill-Targeted Adaptive Training", "comment": null, "summary": "Language models often show little to no improvement (i.e., \"saturation\") when\ntrained via vanilla supervised fine-tuning (SFT) on data similar to what they\nsaw in their training set (e.g., MATH). We introduce a new fine-tuning\nstrategy, STAT, to train such a student model by using the metacognition\nability of a stronger large language model (LLM) as the teacher. The teacher\nuses the task dataset to create a list of skills needed for the task, and then\nlabels each data point with its required skills (Didolkar et al., 2024). By\nmonitoring the student's answers, the teacher creates a Missing-Skill-Profile\nfor the student, tracking how often they failed to apply each skill in their\nresponses. We use this idea to build a modified training set in one of two\nways. In STAT-Sel, the teacher uses an existing set of training examples but\nadaptively reweights them according to the Missing-Skill-Profile. In STAT-Syn,\nthe teacher synthesizes additional examples involving missing skills. Across\nextensive experiments on Llama and Qwen models, our methods yield improvements\nof up to 7.5% on MATH, whereas SFT provides only limited gains. Furthermore,\nSTAT enhances performance on out-of-distribution benchmarks (e.g., AIME24/25,\nAMC23, etc.) by an average of 4.6%. Crucially, we find that STAT is\ncomplementary to RL via GRPO (Shao et al., 2024): after the model is improved\nusing STAT to address skill gaps, GRPO continues to add further gains. We\nconclude that skill-targeted adaptive training should broadly improve current\ntraining pipelines. Our code is available at:\nhttps://github.com/princeton-pli/STAT."}
{"id": "2510.11009", "categories": ["gr-qc", "astro-ph.CO", "hep-ph", "physics.atom-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.11009", "abs": "https://arxiv.org/abs/2510.11009", "authors": ["Jiamin Liang", "Mingqiu Li", "Yu Gao", "Wei Ji", "Sichun Sun", "Qi-Shu Yan"], "title": "Detecting gravitational waves with spin systems", "comment": "8 pages, 3 figures", "summary": "The observation of gravitational waves has opened a new window into the\nUniverse through gravitational-wave astronomy. However, high-frequency\ngravitational waves remain undetected. In this work, we propose that spin\nsystems can be employed to detect gravitational waves in this unexplored\nfrequency regime. We derive the spin's response to gravitational waves and\nidentify three distinct effects: the well-known Gertsenshtein effect, a\nmetric-induced interaction, and the gravitational spin Hall effect. We focus on\nnuclear spins and utilize nuclear magnetic resonance to enhance the\ngravitational response, leveraging the advantages of long coherence time, high\npolarization, and a small gyromagnetic ratio. The proposed experimental scheme\nis capable of probing gravitational waves in the kilohertz to gigahertz range,\nwith projected sensitivities reaching\n$\\sqrt{S_h}\\approx10^{-20}~\\mathrm{Hz}^{-1/2}$."}
{"id": "2510.10028", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.10028", "abs": "https://arxiv.org/abs/2510.10028", "authors": ["Yang Li", "Ruichen Zhang", "Yinqiu Liu", "Guangyuan Liu", "Dusit Niyato", "Abbas Jamalipour", "Xianbin Wang", "Dong In Kim"], "title": "Efficient Onboard Vision-Language Inference in UAV-Enabled Low-Altitude Economy Networks via LLM-Enhanced Optimization", "comment": null, "summary": "The rapid advancement of Low-Altitude Economy Networks (LAENets) has enabled\na variety of applications, including aerial surveillance, environmental\nsensing, and semantic data collection. To support these scenarios, unmanned\naerial vehicles (UAVs) equipped with onboard vision-language models (VLMs)\noffer a promising solution for real-time multimodal inference. However,\nensuring both inference accuracy and communication efficiency remains a\nsignificant challenge due to limited onboard resources and dynamic network\nconditions. In this paper, we first propose a UAV-enabled LAENet system model\nthat jointly captures UAV mobility, user-UAV communication, and the onboard\nvisual question answering (VQA) pipeline. Based on this model, we formulate a\nmixed-integer non-convex optimization problem to minimize task latency and\npower consumption under user-specific accuracy constraints. To solve the\nproblem, we design a hierarchical optimization framework composed of two parts:\n(i) an Alternating Resolution and Power Optimization (ARPO) algorithm for\nresource allocation under accuracy constraints, and (ii) a Large Language\nModel-augmented Reinforcement Learning Approach (LLaRA) for adaptive UAV\ntrajectory optimization. The large language model (LLM) serves as an expert in\nrefining reward design of reinforcement learning in an offline fashion,\nintroducing no additional latency in real-time decision-making. Numerical\nresults demonstrate the efficacy of our proposed framework in improving\ninference performance and communication efficiency under dynamic LAENet\nconditions."}
{"id": "2510.11075", "categories": ["gr-qc", "hep-th", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.11075", "abs": "https://arxiv.org/abs/2510.11075", "authors": ["Mainak Dutta", "Partha Nandi", "Bibhas Ranjan Majhi"], "title": "A novel quantum memory effect and thermal modulation in graviton-mediated entanglement", "comment": "Latex, 35 pages, 3 figures, 1 table", "summary": "A central challenge in probing the quantum nature of gravity is to\ndistinguish effects that are genuinely quantum from those that can be explained\nclassically. In this work, we study how quantized gravitational waves interact\nwith thermal quantum systems, modeled as harmonic oscillators. We show that,\nunlike classical waves, quantized gravitons generate entanglement and leave\nbehind a persistent ``graviton-induced quantum memory'' even after the wave has\npassed. This effect is further shaped by the presence of thermal noise, which\ndoes not simply wash out quantum correlations but can in fact amplify them in\ndistinctive ways. Our analysis reveals clear signatures - such as nonlinear\nthermal corrections and a prethermal time-crystal-like phase-that cannot arise\nfrom any classical treatment. These results identify experimentally relevant\nmarkers of gravitons and provide a framework for exploring how\nfinite-temperature environments may help uncover the quantum nature of gravity."}
{"id": "2510.10029", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10029", "abs": "https://arxiv.org/abs/2510.10029", "authors": ["Ruoxing Yang"], "title": "Experience-Efficient Model-Free Deep Reinforcement Learning Using Pre-Training", "comment": null, "summary": "We introduce PPOPT - Proximal Policy Optimization using Pretraining, a novel,\nmodel-free deep-reinforcement-learning algorithm that leverages pretraining to\nachieve high training efficiency and stability on very small training samples\nin physics-based environments. Reinforcement learning agents typically rely on\nlarge samples of environment interactions to learn a policy. However, frequent\ninteractions with a (computer-simulated) environment may incur high\ncomputational costs, especially when the environment is complex. Our main\ninnovation is a new policy neural network architecture that consists of a\npretrained neural network middle section sandwiched between two fully-connected\nnetworks. Pretraining part of the network on a different environment with\nsimilar physics will help the agent learn the target environment with high\nefficiency because it will leverage a general understanding of the\ntransferrable physics characteristics from the pretraining environment. We\ndemonstrate that PPOPT outperforms baseline classic PPO on small training\nsamples both in terms of rewards gained and general training stability. While\nPPOPT underperforms against classic model-based methods such as DYNA DDPG, the\nmodel-free nature of PPOPT allows it to train in significantly less time than\nits model-based counterparts. Finally, we present our implementation of PPOPT\nas open-source software, available at github.com/Davidrxyang/PPOPT."}
{"id": "2510.10041", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10041", "abs": "https://arxiv.org/abs/2510.10041", "authors": ["Sahng-Min Han", "Minjae Kim", "Jinho Cha", "Se-woon Choe", "Eunchan Daniel Cha", "Jungwon Choi", "Kyudong Jung"], "title": "FOSSIL: Regret-Minimizing Curriculum Learning for Metadata-Free and Low-Data Mpox Diagnosis", "comment": "35 pages, 11 figures, submitted to Computers in Biology and Medicine\n  (Elsevier, under review)", "summary": "Deep learning in small and imbalanced biomedical datasets remains\nfundamentally constrained by unstable optimization and poor generalization. We\npresent the first biomedical implementation of FOSSIL (Flexible Optimization\nvia Sample-Sensitive Importance Learning), a regret-minimizing weighting\nframework that adaptively balances training emphasis according to sample\ndifficulty. Using softmax-based uncertainty as a continuous measure of\ndifficulty, we construct a four-stage curriculum (Easy-Very Hard) and integrate\nFOSSIL into both convolutional and transformer-based architectures for Mpox\nskin lesion diagnosis. Across all settings, FOSSIL substantially improves\ndiscrimination (AUC = 0.9573), calibration (ECE = 0.053), and robustness under\nreal-world perturbations, outperforming conventional baselines without\nmetadata, manual curation, or synthetic augmentation. The results position\nFOSSIL as a generalizable, data-efficient, and interpretable framework for\ndifficulty-aware learning in medical imaging under data scarcity."}
{"id": "2510.10057", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10057", "abs": "https://arxiv.org/abs/2510.10057", "authors": ["Lei Gao", "Shihong Huang", "Shengjie Wang", "Hong Ma", "Feng Zhang", "Hengda Bao", "Qichang Chen", "Weihua Zhou"], "title": "One4Many-StablePacker: An Efficient Deep Reinforcement Learning Framework for the 3D Bin Packing Problem", "comment": null, "summary": "The three-dimensional bin packing problem (3D-BPP) is widely applied in\nlogistics and warehousing. Existing learning-based approaches often neglect\npractical stability-related constraints and exhibit limitations in generalizing\nacross diverse bin dimensions. To address these limitations, we propose a novel\ndeep reinforcement learning framework, One4Many-StablePacker (O4M-SP). The\nprimary advantage of O4M-SP is its ability to handle various bin dimensions in\na single training process while incorporating support and weight constraints\ncommon in practice. Our training method introduces two innovative mechanisms.\nFirst, it employs a weighted reward function that integrates loading rate and a\nnew height difference metric for packing layouts, promoting improved bin\nutilization through flatter packing configurations. Second, it combines clipped\npolicy gradient optimization with a tailored policy drifting method to mitigate\npolicy entropy collapse, encouraging exploration at critical decision nodes\nduring packing to avoid suboptimal solutions. Extensive experiments demonstrate\nthat O4M-SP generalizes successfully across diverse bin dimensions and\nsignificantly outperforms baseline methods. Furthermore, O4M-SP exhibits strong\npractical applicability by effectively addressing packing scenarios with\nstability constraints."}
{"id": "2510.10060", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10060", "abs": "https://arxiv.org/abs/2510.10060", "authors": ["Hehe Fan", "Yi Yang", "Mohan Kankanhalli", "Fei Wu"], "title": "Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling", "comment": "technical report", "summary": "When modeling a given type of data, we consider it to involve two key\naspects: 1) identifying relevant elements (e.g., image pixels or textual words)\nto a central element, as in a convolutional receptive field, or to a query\nelement, as in self-attention, and 2) encoding these tokens effectively.\nSelf-attention can adaptively identify these elements but relies on absolute\npositional embedding for structural representation learning. In contrast,\nconvolution encodes elements in a relative manner, yet their fixed kernel size\nlimits their ability to adaptively select the relevant elements. In this paper,\nwe introduce Translution, an operation that unifies the adaptive identification\ncapability of self-attention and the relative encoding advantage of\nconvolution. However, this integration leads to a substantial increase in the\nnumber of parameters, exceeding most currently available computational\nresources. Therefore, we propose a lightweight variant of Translution, named\n{\\alpha}-Translution. Experiments on computer vision and natural language\nprocessing tasks show that Translution (including {\\alpha}-Translution)\nachieves superior accuracy compared to self-attention. The code is available at\nhttps://github.com/hehefan/Translution."}
{"id": "2510.10071", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10071", "abs": "https://arxiv.org/abs/2510.10071", "authors": ["Jinyang Zhang", "Yue Fang", "Hongxin Ding", "Weibin Liao", "Muyang Ye", "Xu Chu", "Junfeng Zhao", "Yasha Wang"], "title": "ADEPT: Continual Pretraining via Adaptive Expansion and Dynamic Decoupled Tuning", "comment": null, "summary": "Conventional continual pretraining (CPT) for large language model (LLM)\ndomain adaptation often suffers from catastrophic forgetting and limited domain\ncapacity. Existing strategies adopt layer expansion, introducing additional\ntrainable parameters to accommodate new knowledge. However, the uniform\nexpansion and updates still entangle general and domain learning, undermining\nits effectiveness. Our pilot studies reveal that LLMs exhibit functional\nspecialization, where layers and units differentially encode general-critical\ncapabilities, suggesting that parameter expansion and optimization should be\nfunction-aware. We then propose ADEPT, Adaptive Expansion and Dynamic Decoupled\nTuning for continual pretraining, a two-stage framework for domain-adaptive\nCPT. ADEPT first performs General-Competence Guided Selective Layer Expansion,\nduplicating layers least critical for the general domain to increase\nrepresentational capacity while minimizing interference with general knowledge.\nIt then applies Adaptive Unit-Wise Decoupled Tuning, disentangling parameter\nunits within expanded layers according to their general-domain importance and\nassigning asymmetric learning rates to balance knowledge injection and\nretention. Experiments on mathematical and medical benchmarks show that ADEPT\noutperforms full-parameter CPT by up to 5.76% on the general domain and 5.58%\non the target domain with only 15% of parameters tuned and less than 50%\ntraining time. Ablation studies, theoretical analysis, and extended\ninvestigations further demonstrate the necessity of targeted expansion and\ndecoupled optimization, providing new principles for efficient and robust\ndomain-adaptive CPT. Our code is open-sourced at\nhttps://github.com/PuppyKnightUniversity/ADEPT"}
{"id": "2510.10075", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10075", "abs": "https://arxiv.org/abs/2510.10075", "authors": ["Salomon Ibarra", "Frida Cantu", "Kaixiong Zhou", "Li Zhang"], "title": "Gradient-based Model Shortcut Detection for Time Series Classification", "comment": "Code available at: https://github.com/IvorySnake02/SAG.git", "summary": "Deep learning models have attracted lots of research attention in time series\nclassification (TSC) task in the past two decades. Recently, deep neural\nnetworks (DNN) have surpassed classical distance-based methods and achieved\nstate-of-the-art performance. Despite their promising performance, deep neural\nnetworks (DNNs) have been shown to rely on spurious correlations present in the\ntraining data, which can hinder generalization. For instance, a model might\nincorrectly associate the presence of grass with the label ``cat\" if the\ntraining set have majority of cats lying in grassy backgrounds. However, the\nshortcut behavior of DNNs in time series remain under-explored. Most existing\nshortcut work are relying on external attributes such as gender, patients\ngroup, instead of focus on the internal bias behavior in time series models.\n  In this paper, we take the first step to investigate and establish\npoint-based shortcut learning behavior in deep learning time series\nclassification. We further propose a simple detection method based on other\nclass to detect shortcut occurs without relying on test data or clean training\nclasses. We test our proposed method in UCR time series datasets."}
{"id": "2510.10089", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10089", "abs": "https://arxiv.org/abs/2510.10089", "authors": ["Zixuan Gong", "Jiaye Teng", "Yong Liu"], "title": "What Makes Looped Transformers Perform Better Than Non-Recursive Ones (Provably)", "comment": null, "summary": "While looped transformers (termed as Looped-Attn) often outperform standard\ntransformers (termed as Single-Attn) on complex reasoning tasks, the\ntheoretical basis for this advantage remains underexplored. In this paper, we\nexplain this phenomenon through the lens of loss landscape geometry, inspired\nby empirical observations of their distinct dynamics at both sample and Hessian\nlevels. To formalize this, we extend the River-Valley landscape model by\ndistinguishing between U-shaped valleys (flat) and V-shaped valleys (steep).\nBased on empirical observations, we conjecture that the recursive architecture\nof Looped-Attn induces a landscape-level inductive bias towards River-V-Valley.\nTheoretical derivations based on this inductive bias guarantee a better loss\nconvergence along the river due to valley hopping, and further encourage\nlearning about complex patterns compared to the River-U-Valley induced by\nSingle-Attn. Building on this insight, we propose SHIFT (Staged HIerarchical\nFramework for Progressive Training), a staged training framework that\naccelerates the training process of Looped-Attn while achieving comparable\nperformances."}
{"id": "2510.10101", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10101", "abs": "https://arxiv.org/abs/2510.10101", "authors": ["Martin Carrasco", "Caio Deberaldini Netto", "Vahan A. Martirosyan", "Aneeqa Mehrab", "Ehimare Okoyomon", "Caterina Graziani"], "title": "Rademacher Meets Colors: More Expressivity, but at What Cost ?", "comment": null, "summary": "The expressive power of graph neural networks (GNNs) is typically understood\nthrough their correspondence with graph isomorphism tests such as the\nWeisfeiler-Leman (WL) hierarchy. While more expressive GNNs can distinguish a\nricher set of graphs, they are also observed to suffer from higher\ngeneralization error. This work provides a theoretical explanation for this\ntrade-off by linking expressivity and generalization through the lens of\ncoloring algorithms. Specifically, we show that the number of equivalence\nclasses induced by WL colorings directly bounds the GNNs Rademacher complexity\n-- a key data-dependent measure of generalization. Our analysis reveals that\ngreater expressivity leads to higher complexity and thus weaker generalization\nguarantees. Furthermore, we prove that the Rademacher complexity is stable\nunder perturbations in the color counts across different samples, ensuring\nrobustness to sampling variability across datasets. Importantly, our framework\nis not restricted to message-passing GNNs or 1-WL, but extends to arbitrary GNN\narchitectures and expressivity measures that partition graphs into equivalence\nclasses. These results unify the study of expressivity and generalization in\nGNNs, providing a principled understanding of why increasing expressive power\noften comes at the cost of generalization."}
{"id": "2510.10102", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10102", "abs": "https://arxiv.org/abs/2510.10102", "authors": ["Guilin Li", "Yun Zhang", "Xiuyuan Chen", "Chengqi Li", "Bo Wang", "Linghe Kong", "Wenjia Wang", "Weiran Huang", "Matthias Hwai Yong Tan"], "title": "PANTHER: Generative Pretraining Beyond Language for Sequential User Behavior Modeling", "comment": null, "summary": "Large language models (LLMs) have shown that generative pretraining can\ndistill vast world knowledge into compact token representations. While LLMs\nencapsulate extensive world knowledge, they remain limited in modeling the\nbehavioral knowledge contained within user interaction histories. User behavior\nforms a distinct modality, where each action, defined by multi-dimensional\nattributes such as time, context, and transaction type, constitutes a\nbehavioral token. Modeling these high-cardinality sequences is challenging, and\ndiscriminative models often falter under limited supervision. To bridge this\ngap, we extend generative pretraining to user behavior, learning transferable\nrepresentations from unlabeled behavioral data analogous to how LLMs learn from\ntext. We present PANTHER, a hybrid generative-discriminative framework that\nunifies user behavior pretraining and downstream adaptation, enabling\nlarge-scale sequential user representation learning and real-time inference.\nPANTHER introduces: (1) Structured Tokenization to compress multi-dimensional\ntransaction attributes into an interpretable vocabulary; (2) Sequence Pattern\nRecognition Module (SPRM) for modeling periodic transaction motifs; (3) a\nUnified User-Profile Embedding that fuses static demographics with dynamic\ntransaction histories; and (4) Real-time scalability enabled by offline caching\nof pretrained embeddings for millisecond-level inference. Fully deployed and\noperational online at WeChat Pay, PANTHER delivers a 25.6 percent boost in\nnext-transaction prediction HitRate@1 and a 38.6 percent relative improvement\nin fraud detection recall over baselines. Cross-domain evaluations on public\nbenchmarks show strong generalization, achieving up to 21 percent HitRate@1\ngains over transformer baselines, establishing PANTHER as a scalable,\nhigh-performance framework for industrial sequential user behavior modeling."}
{"id": "2510.10105", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10105", "abs": "https://arxiv.org/abs/2510.10105", "authors": ["Yanping Zheng", "Zhewei Wei", "Frank de Hoog", "Xu Chen", "Hongteng Xu", "Yuhang Ye", "Jiadeng Huang"], "title": "Lighter-X: An Efficient and Plug-and-play Strategy for Graph-based Recommendation through Decoupled Propagation", "comment": null, "summary": "Graph Neural Networks (GNNs) have demonstrated remarkable effectiveness in\nrecommendation systems. However, conventional graph-based recommenders, such as\nLightGCN, require maintaining embeddings of size $d$ for each node, resulting\nin a parameter complexity of $\\mathcal{O}(n \\times d)$, where $n$ represents\nthe total number of users and items. This scaling pattern poses significant\nchallenges for deployment on large-scale graphs encountered in real-world\napplications. To address this scalability limitation, we propose\n\\textbf{Lighter-X}, an efficient and modular framework that can be seamlessly\nintegrated with existing GNN-based recommender architectures. Our approach\nsubstantially reduces both parameter size and computational complexity while\npreserving the theoretical guarantees and empirical performance of the base\nmodels, thereby enabling practical deployment at scale. Specifically, we\nanalyze the original structure and inherent redundancy in their parameters,\nidentifying opportunities for optimization. Based on this insight, we propose\nan efficient compression scheme for the sparse adjacency structure and\nhigh-dimensional embedding matrices, achieving a parameter complexity of\n$\\mathcal{O}(h \\times d)$, where $h \\ll n$. Furthermore, the model is optimized\nthrough a decoupled framework, reducing computational complexity during the\ntraining process and enhancing scalability. Extensive experiments demonstrate\nthat Lighter-X achieves comparable performance to baseline models with\nsignificantly fewer parameters. In particular, on large-scale interaction\ngraphs with millions of edges, we are able to attain even better results with\nonly 1\\% of the parameter over LightGCN."}
{"id": "2510.10116", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.10116", "abs": "https://arxiv.org/abs/2510.10116", "authors": ["Xing Wei", "Chunchun Chen", "Rui Fan", "Xiaofeng Cao", "Sourav Medya", "Wei Ye"], "title": "Preference-driven Knowledge Distillation for Few-shot Node Classification", "comment": "Accepted at NeurIPS 2025", "summary": "Graph neural networks (GNNs) can efficiently process text-attributed graphs\n(TAGs) due to their message-passing mechanisms, but their training heavily\nrelies on the human-annotated labels. Moreover, the complex and diverse local\ntopologies of nodes of real-world TAGs make it challenging for a single\nmechanism to handle. Large language models (LLMs) perform well in\nzero-/few-shot learning on TAGs but suffer from a scalability challenge.\nTherefore, we propose a preference-driven knowledge distillation (PKD)\nframework to synergize the complementary strengths of LLMs and various GNNs for\nfew-shot node classification. Specifically, we develop a GNN-preference-driven\nnode selector that effectively promotes prediction distillation from LLMs to\nteacher GNNs. To further tackle nodes' intricate local topologies, we develop a\nnode-preference-driven GNN selector that identifies the most suitable teacher\nGNN for each node, thereby facilitating tailored knowledge distillation from\nteacher GNNs to the student GNN. Extensive experiments validate the efficacy of\nour proposed framework in few-shot node classification on real-world TAGs."}
{"id": "2510.10129", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10129", "abs": "https://arxiv.org/abs/2510.10129", "authors": ["Bin Yang", "Qiuyu Leng", "Jun Zeng", "Zhenhua Wu"], "title": "CacheClip: Accelerating RAG with Effective KV Cache Reuse", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems suffer from severe\ntime-to-first-token (TTFT) bottlenecks due to long input sequences. Existing KV\ncache reuse methods face a fundamental trade-off: prefix caching requires\nidentical prefixes that rarely occur in RAG scenarios, while direct\nprecomputation sacrifices quality due to missing inter-chunk attention and\nrepeated attention sinks. Recent methods like APE and CacheBlend partially\naddress these issues but remain inadequate for robust RAG applications. This\npaper presents CacheClip, a novel framework that achieves both fast TTFT and\nhigh generation quality. Our key insight is that small auxiliary LLMs exhibit\nsimilar last-layer attention distributions to primary LLMs (the target model\nfor generation), enabling efficient identification of tokens critical for\nrestoring inter-chunk attention, thereby significantly improving response\nquality on cross-chunk reasoning tasks. CacheClip integrates three techniques:\n(1) auxiliary-model-guided token selection for selective KV cache\nrecomputation, where the auxiliary model is finetuned to improve selection\naccuracy, (2) shared prefixes to eliminate redundant attention sinks, and (3)\ngrouping strategy to maintain local coherence during partial KV cache updates.\nExperiments show CacheClip retains up to 94.8% and 85.0% of full-attention\nperformance on NIAH and LongBench, outperforming APE and CacheBlend by 25.2%\nand 35.1% on NIAH (with reomp% = 20%). Meanwhile, CacheClip accelerates LLM\ninference by up to 1.92x in prefill time, providing a practical solution to the\nefficiency-quality trade-off in RAG systems."}
{"id": "2510.10136", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10136", "abs": "https://arxiv.org/abs/2510.10136", "authors": ["Lancheng Zou", "Shuo Yin", "Zehua Pei", "Tsung-Yi Ho", "Farzan Farnia", "Bei Yu"], "title": "PermLLM: Learnable Channel Permutation for N:M Sparse Large Language Models", "comment": "Accepted by NeurIPS 2025", "summary": "Channel permutation is a powerful technique for enhancing the accuracy of N:M\nsparse models by reordering the channels of weight matrices to prioritize the\nretention of important weights. However, traditional channel permutation\nmethods rely on handcrafted quality metrics, which often fail to accurately\ncapture the true impact of pruning on model performance. To address this\nlimitation, we propose PermLLM, a novel post-training pruning framework that\nintroduces learnable channel permutation (LCP) for N:M sparsity. LCP leverages\nSinkhorn normalization to transform discrete permutation matrices into\ndifferentiable soft permutation matrices, enabling end-to-end optimization.\nAdditionally, PermLLM incorporates an efficient block-wise channel permutation\nstrategy, which significantly reduces the number of learnable parameters and\ncomputational complexity. PermLLM seamlessly integrates with existing one-shot\npruning methods to adaptively optimize channel permutations, effectively\nmitigating pruning-induced errors. Extensive experiments on the LLaMA series,\nQwen, and OPT models demonstrate that PermLLM achieves superior performance in\noptimizing N:M sparse models. The code is available at\nhttps://github.com/lanchengzou/PermLLM."}
{"id": "2510.10140", "categories": ["cs.LG", "cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10140", "abs": "https://arxiv.org/abs/2510.10140", "authors": ["Yue Deng", "Francisco Santos", "Pang-Ning Tan", "Lifeng Luo"], "title": "Adversarial Attacks on Downstream Weather Forecasting Models: Application to Tropical Cyclone Trajectory Prediction", "comment": null, "summary": "Deep learning based weather forecasting (DLWF) models leverage past weather\nobservations to generate future forecasts, supporting a wide range of\ndownstream tasks, including tropical cyclone (TC) trajectory prediction. In\nthis paper, we investigate their vulnerability to adversarial attacks, where\nsubtle perturbations to the upstream weather forecasts can alter the downstream\nTC trajectory predictions. Although research on adversarial attacks in DLWF\nmodels has grown recently, generating perturbed upstream forecasts that\nreliably steer downstream output toward attacker-specified trajectories remains\na challenge. First, conventional TC detection systems are opaque,\nnon-differentiable black boxes, making standard gradient-based attacks\ninfeasible. Second, the extreme rarity of TC events leads to severe class\nimbalance problem, making it difficult to develop efficient attack methods that\nwill produce the attacker's target trajectories. Furthermore, maintaining\nphysical consistency in adversarially generated forecasts presents another\nsignificant challenge. To overcome these limitations, we propose Cyc-Attack, a\nnovel method that perturbs the upstream forecasts of DLWF models to generate\nadversarial trajectories. First, we pre-train a differentiable surrogate model\nto approximate the TC detector's output, enabling the construction of\ngradient-based attacks. Cyc-Attack also employs skewness-aware loss function\nwith kernel dilation strategy to address the imbalance problem. Finally, a\ndistance-based gradient weighting scheme and regularization are used to\nconstrain the perturbations and eliminate spurious trajectories to ensure the\nadversarial forecasts are realistic and not easily detectable."}
{"id": "2510.10145", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10145", "abs": "https://arxiv.org/abs/2510.10145", "authors": ["Cheng He", "Xijie Liang", "Zengrong Zheng", "Patrick P. C. Lee", "Xu Huang", "Zhaoyi Li", "Hong Xie", "Defu Lian", "Enhong Chen"], "title": "A Unified Frequency Domain Decomposition Framework for Interpretable and Robust Time Series Forecasting", "comment": null, "summary": "Current approaches for time series forecasting, whether in the time or\nfrequency domain, predominantly use deep learning models based on linear layers\nor transformers. They often encode time series data in a black-box manner and\nrely on trial-and-error optimization solely based on forecasting performance,\nleading to limited interpretability and theoretical understanding. Furthermore,\nthe dynamics in data distribution over time and frequency domains pose a\ncritical challenge to accurate forecasting. We propose FIRE, a unified\nfrequency domain decomposition framework that provides a mathematical\nabstraction for diverse types of time series, so as to achieve interpretable\nand robust time series forecasting. FIRE introduces several key innovations:\n(i) independent modeling of amplitude and phase components, (ii) adaptive\nlearning of weights of frequency basis components, (iii) a targeted loss\nfunction, and (iv) a novel training paradigm for sparse data. Extensive\nexperiments demonstrate that FIRE consistently outperforms state-of-the-art\nmodels on long-term forecasting benchmarks, achieving superior predictive\nperformance and significantly enhancing interpretability of time series"}
{"id": "2510.10149", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10149", "abs": "https://arxiv.org/abs/2510.10149", "authors": ["Xin Chen", "Gillian Dobbie", "Xinyu Wang", "Feng Liu", "Di Wang", "Jingfeng Zhang"], "title": "Robust Learning of Diffusion Models with Extremely Noisy Conditions", "comment": null, "summary": "Conditional diffusion models have the generative controllability by\nincorporating external conditions. However, their performance significantly\ndegrades with noisy conditions, such as corrupted labels in the image\ngeneration or unreliable observations or states in the control policy\ngeneration. This paper introduces a robust learning framework to address\nextremely noisy conditions in conditional diffusion models. We empirically\ndemonstrate that existing noise-robust methods fail when the noise level is\nhigh. To overcome this, we propose learning pseudo conditions as surrogates for\nclean conditions and refining pseudo ones progressively via the technique of\ntemporal ensembling. Additionally, we develop a Reverse-time Diffusion\nCondition (RDC) technique, which diffuses pseudo conditions to reinforce the\nmemorization effect and further facilitate the refinement of the pseudo\nconditions. Experimentally, our approach achieves state-of-the-art performance\nacross a range of noise levels on both class-conditional image generation and\nvisuomotor policy generation tasks.The code can be accessible via the project\npage https://robustdiffusionpolicy.github.io"}
{"id": "2510.10150", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10150", "abs": "https://arxiv.org/abs/2510.10150", "authors": ["Zhezheng Hao", "Hong Wang", "Haoyang Liu", "Jian Luo", "Jiarui Yu", "Hande Dong", "Qiang Lin", "Can Wang", "Jiawei Chen"], "title": "Rethinking Entropy Interventions in RLVR: An Entropy Change Perspective", "comment": null, "summary": "While Reinforcement Learning with Verifiable Rewards (RLVR) can enhance LLM\nreasoning, its training process poses a critical risk: entropy collapse. This\nphenomenon is a rapid loss of policy diversity, stemming from the\nexploration-exploitation imbalance and leading to a lack of generalization.\nRecent entropy-intervention methods aim to prevent \\coloredtext{entropy\ncollapse}, yet their underlying mechanisms remain unclear. In this paper, we\nconduct a quantitative analysis to reveal token-level entropy changes and how\nexisting entropy intervention methods help avoid entropy collapse. Our findings\npoint out a fundamental limitation of existing methods: they attempt to control\nentropy dynamics indirectly. By only affecting related factors, such as the\nadvantage signal and generation probability, their effectiveness is inherently\nlimited and could potentially fail. To address this limitation, we introduce an\nentropy-change-aware reweighting scheme, namely Stabilizing Token-level\nEntropy-changE via Reweighting (STEER), that adaptively stabilizes entropy\ndynamics through fine-grained token-level adjustments. Our approach mitigates\nover-exploitation while fostering robust exploration. Extensive experiments\ndemonstrate that STEER significantly mitigates entropy collapse, stabilizes\nentropy dynamics, and achieves stronger downstream performance across various\nmathematical reasoning benchmarks \\footnote{Our code is available at\nhttps://github.com/zz-haooo/STEER."}
{"id": "2510.10188", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10188", "abs": "https://arxiv.org/abs/2510.10188", "authors": ["Linfei Li", "Fengyi Zhang", "Zhong Wang", "Lin Zhang", "Ying Shen"], "title": "INR-Bench: A Unified Benchmark for Implicit Neural Representations in Multi-Domain Regression and Reconstruction", "comment": null, "summary": "Implicit Neural Representations (INRs) have gained success in various signal\nprocessing tasks due to their advantages of continuity and infinite resolution.\nHowever, the factors influencing their effectiveness and limitations remain\nunderexplored. To better understand these factors, we leverage insights from\nNeural Tangent Kernel (NTK) theory to analyze how model architectures (classic\nMLP and emerging KAN), positional encoding, and nonlinear primitives affect the\nresponse to signals of varying frequencies. Building on this analysis, we\nintroduce INR-Bench, the first comprehensive benchmark specifically designed\nfor multimodal INR tasks. It includes 56 variants of Coordinate-MLP models\n(featuring 4 types of positional encoding and 14 activation functions) and 22\nCoordinate-KAN models with distinct basis functions, evaluated across 9\nimplicit multimodal tasks. These tasks cover both forward and inverse problems,\noffering a robust platform to highlight the strengths and limitations of\ndifferent neural models, thereby establishing a solid foundation for future\nresearch. The code and dataset are available at\nhttps://github.com/lif314/INR-Bench."}
{"id": "2510.10195", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10195", "abs": "https://arxiv.org/abs/2510.10195", "authors": ["Hong-Kun Zhang", "Xin Li", "Sikun Yang", "Zhihong Xia"], "title": "CauchyNet: Compact and Data-Efficient Learning using Holomorphic Activation Functions", "comment": null, "summary": "A novel neural network inspired by Cauchy's integral formula, is proposed for\nfunction approximation tasks that include time series forecasting, missing data\nimputation, etc. Hence, the novel neural network is named CauchyNet. By\nembedding real-valued data into the complex plane, CauchyNet efficiently\ncaptures complex temporal dependencies, surpassing traditional real-valued\nmodels in both predictive performance and computational efficiency. Grounded in\nCauchy's integral formula and supported by the universal approximation theorem,\nCauchyNet offers strong theoretical guarantees for function approximation. The\narchitecture incorporates complex-valued activation functions, enabling robust\nlearning from incomplete data while maintaining a compact parameter footprint\nand reducing computational overhead. Through extensive experiments in diverse\ndomains, including transportation, energy consumption, and epidemiological\ndata, CauchyNet consistently outperforms state-of-the-art models in predictive\naccuracy, often achieving a 50% lower mean absolute error with fewer\nparameters. These findings highlight CauchyNet's potential as an effective and\nefficient tool for data-driven predictive modeling, particularly in\nresource-constrained and data-scarce environments."}
{"id": "2510.10201", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10201", "abs": "https://arxiv.org/abs/2510.10201", "authors": ["Jinghao Zhang", "Naishan Zheng", "Ruilin Li", "Dongzhou Cheng", "Zheming Liang", "Feng Zhao", "Jiaqi Wang"], "title": "RLFR: Extending Reinforcement Learning for LLMs with Flow Environment", "comment": "Project Website: https://jinghaoleven.github.io/RLFR/", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na promising framework for improving reasoning abilities in Large Language\nModels (LLMs). However, policy optimized with binary verification prone to\noverlook potential valuable exploration in reasoning trajectory. In view of\nheavy annotation cost of golden Process Reward Models (PRMs), recent works\nattempt using auxiliary signals for reward shaping of process tokens, involving\nentropy and likelihood collected from logit space. In this work, we offer a\nnovel perspective on shaping RLVR with flow rewards derived from latent space,\nand propose RLFR, where the flow fields of model latents are constructed from\neither off-policy high-quality data and on-policy rejection sampling data, and\nthe velocity deviations of policy latents within it are quantified to serve as\na reward signal. RLFR first demonstrates that a well-established flow field can\nbe a sound environment for reward signal collection, highlighting the\nexpressive latent space is much underexplored. Moreover, RLFR is able to\ncompress any off-policy expert data as reference for constituting reward\nsignals, and we show that the efficient context dependence compressed within\nthe hidden states are utilized, rather than individual token-level denotation\nfor context comprehending. Experiments on both language and multimodal\nreasoning benchmarks demonstrate the reliability of flow rewards, and\nsuggesting a promising paradigm for reward shaping with auxiliary signals."}
{"id": "2510.10211", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10211", "abs": "https://arxiv.org/abs/2510.10211", "authors": ["Yida Xiong", "Jiameng Chen", "Kun Li", "Hongzhi Zhang", "Xiantao Cai", "Wenbin Hu"], "title": "Hierarchical Bayesian Flow Networks for Molecular Graph Generation", "comment": null, "summary": "Molecular graph generation is essentially a classification generation\nproblem, aimed at predicting categories of atoms and bonds. Currently,\nprevailing paradigms such as continuous diffusion models are trained to predict\ncontinuous numerical values, treating the training process as a regression\ntask. However, the final generation necessitates a rounding step to convert\nthese predictions back into discrete classification categories, which is\nintrinsically a classification operation. Given that the rounding operation is\nnot incorporated during training, there exists a significant discrepancy\nbetween the model's training objective and its inference procedure. As a\nconsequence, an excessive emphasis on point-wise precision can lead to\noverfitting and inefficient learning. This occurs because considerable efforts\nare devoted to capturing intra-bin variations that are ultimately irrelevant to\nthe discrete nature of the task at hand. Such a flaw results in diminished\nmolecular diversity and constrains the model's generalization capabilities. To\naddress this fundamental limitation, we propose GraphBFN, a novel hierarchical\ncoarse-to-fine framework based on Bayesian Flow Networks that operates on the\nparameters of distributions. By innovatively introducing Cumulative\nDistribution Function, GraphBFN is capable of calculating the probability of\nselecting the correct category, thereby unifying the training objective with\nthe sampling rounding operation. We demonstrate that our method achieves\nsuperior performance and faster generation, setting new state-of-the-art\nresults on the QM9 and ZINC250k molecular graph generation benchmarks."}
{"id": "2510.10232", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10232", "abs": "https://arxiv.org/abs/2510.10232", "authors": ["Xuening Wu", "Shenqin Yin", "Yanlan Kang", "Xinhang Zhang", "Qianya Xu", "Zeping Chen", "Wenqiang Zhang"], "title": "SGM: A Statistical Godel Machine for Risk-Controlled Recursive Self-Modification", "comment": null, "summary": "Recursive self-modification is increasingly central in AutoML, neural\narchitecture search, and adaptive optimization, yet no existing framework\nensures that such changes are made safely. Godel machines offer a principled\nsafeguard by requiring formal proofs of improvement before rewriting code;\nhowever, such proofs are unattainable in stochastic, high-dimensional settings.\nWe introduce the Statistical Godel Machine (SGM), the first statistical safety\nlayer for recursive edits. SGM replaces proof-based requirements with\nstatistical confidence tests (e-values, Hoeffding bounds), admitting a\nmodification only when superiority is certified at a chosen confidence level,\nwhile allocating a global error budget to bound cumulative risk across\nrounds.We also propose Confirm-Triggered Harmonic Spending (CTHS), which\nindexes spending by confirmation events rather than rounds, concentrating the\nerror budget on promising edits while preserving familywise\nvalidity.Experiments across supervised learning, reinforcement learning, and\nblack-box optimization validate this role: SGM certifies genuine gains on\nCIFAR-100, rejects spurious improvement on ImageNet-100, and demonstrates\nrobustness on RL and optimization benchmarks.Together, these results position\nSGM as foundational infrastructure for continual, risk-aware self-modification\nin learning systems.Code is available at:\nhttps://github.com/gravitywavelet/sgm-anon."}
{"id": "2510.10244", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10244", "abs": "https://arxiv.org/abs/2510.10244", "authors": ["Ziyu Zhou", "Keyan Hu", "Ling Zhang", "Zhaohui Xue", "Yutian Fang", "Yusha Zheng"], "title": "Progressive Scale Convolutional Network for Spatio-Temporal Downscaling of Soil Moisture: A Case Study Over the Tibetan Plateau", "comment": null, "summary": "Soil moisture (SM) plays a critical role in hydrological and meteorological\nprocesses. High-resolution SM can be obtained by combining coarse passive\nmicrowave data with fine-scale auxiliary variables. However, the inversion of\nSM at the temporal scale is hindered by the incompleteness of surface auxiliary\nfactors. To address this issue, first, we introduce validated high temporal\nresolution ERA5-Land variables into the downscaling process of the\nlow-resolution SMAP SM product. Subsequently, we design a progressive scale\nconvolutional network (PSCNet), at the core of which are two innovative\ncomponents: a multi-frequency temporal fusion module (MFTF) for capturing\ntemporal dynamics, and a bespoke squeeze-and-excitation (SE) block designed to\npreserve fine-grained spatial details. Using this approach, we obtained\nseamless SM products for the Tibetan Plateau (TP) from 2016 to 2018 at 10-km\nspatial and 3-hour temporal resolution. The experimental results on the TP\ndemonstrated the following: 1) In the satellite product validation, the PSCNet\nexhibited comparable accuracy and lower error, with a mean R value of 0.881,\noutperforming other methods. 2) In the in-situ site validation, PSCNet\nconsistently ranked among the top three models for the R metric across all\nsites, while also showing superior performance in overall error reduction. 3)\nIn the temporal generalization validation, the feasibility of using\nhigh-temporal resolution ERA5-Land variables for downscaling was confirmed, as\nall methods maintained an average relative error within 6\\% for the R metric\nand 2\\% for the ubRMSE metric. 4) In the temporal dynamics and visualization\nvalidation, PSCNet demonstrated excellent temporal sensitivity and vivid\nspatial details. Overall, PSCNet provides a promising solution for\nspatio-temporal downscaling by effectively modeling the intricate\nspatio-temporal relationships in SM data."}
{"id": "2510.10248", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10248", "abs": "https://arxiv.org/abs/2510.10248", "authors": ["Jiaxi Zhuang", "Yaorui Shi", "Jue Hou", "Yunong He", "Mingwei Ye", "Mingjun Xu", "Yuming Su", "Linfeng Zhang", "Linfeng Zhang", "Guolin Ke", "Hengxing Cai"], "title": "Reasoning-Enhanced Large Language Models for Molecular Property Prediction", "comment": null, "summary": "Molecular property prediction is crucial for drug discovery and materials\nscience, yet existing approaches suffer from limited interpretability, poor\ncross-task generalization, and lack of chemical reasoning capabilities.\nTraditional machine learning models struggle with task transferability, while\nspecialized molecular language models provide little insight into their\ndecision-making processes. To address these limitations, we propose\n\\textbf{MPPReasoner}, a multimodal large language model that incorporates\nchemical reasoning for molecular property prediction. Our approach, built upon\nQwen2.5-VL-7B-Instruct, integrates molecular images with SMILES strings to\nenable comprehensive molecular understanding. We develop a two-stage training\nstrategy: supervised fine-tuning (SFT) using 16,000 high-quality reasoning\ntrajectories generated through expert knowledge and multiple teacher models,\nfollowed by Reinforcement Learning from Principle-Guided Rewards (RLPGR). RLPGR\nemploys verifiable, rule-based rewards that systematically evaluate chemical\nprinciple application, molecular structure analysis, and logical consistency\nthrough computational verification. Extensive experiments across 8 datasets\ndemonstrate significant performance improvements, with MPPReasoner\noutperforming the best baselines by 7.91\\% and 4.53\\% on in-distribution and\nout-of-distribution tasks respectively. MPPReasoner exhibits exceptional\ncross-task generalization and generates chemically sound reasoning paths that\nprovide valuable insights into molecular property analysis, substantially\nenhancing both interpretability and practical utility for chemists. Code is\navailable at https://anonymous.4open.science/r/MPPReasoner-12687."}
{"id": "2510.10262", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10262", "abs": "https://arxiv.org/abs/2510.10262", "authors": ["Jingwen Li", "Zhiguang Cao", "Yaoxin Wu", "Tang Liu"], "title": "Enhancing the Cross-Size Generalization for Solving Vehicle Routing Problems via Continual Learning", "comment": null, "summary": "Exploring machine learning techniques for addressing vehicle routing problems\nhas attracted considerable research attention. To achieve decent and efficient\nsolutions, existing deep models for vehicle routing problems are typically\ntrained and evaluated using instances of a single size. This substantially\nlimits their ability to generalize across different problem sizes and thus\nhampers their practical applicability. To address the issue, we propose a\ncontinual learning based framework that sequentially trains a deep model with\ninstances of ascending problem sizes. Specifically, on the one hand, we design\nan inter-task regularization scheme to retain the knowledge acquired from\nsmaller problem sizes in the model training on a larger size. On the other\nhand, we introduce an intra-task regularization scheme to consolidate the model\nby imitating the latest desirable behaviors during training on each size.\nAdditionally, we exploit the experience replay to revisit instances of formerly\ntrained sizes for mitigating the catastrophic forgetting. Experimental results\nshow that our approach achieves predominantly superior performance across\nvarious problem sizes (either seen or unseen in the training), as compared to\nstate-of-the-art deep models including the ones specialized for\ngeneralizability enhancement. Meanwhile, the ablation studies on the key\ndesigns manifest their synergistic effect in the proposed framework."}
{"id": "2510.10276", "categories": ["cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.10276", "abs": "https://arxiv.org/abs/2510.10276", "authors": ["Nikolaus Salvatore", "Hao Wang", "Qiong Zhang"], "title": "Lost in the Middle: An Emergent Property from Information Retrieval Demands in LLMs", "comment": null, "summary": "The performance of Large Language Models (LLMs) often degrades when crucial\ninformation is in the middle of a long context, a \"lost-in-the-middle\"\nphenomenon that mirrors the primacy and recency effects in human memory. We\npropose that this behavior is not simply a flaw indicative of information loss\nbut an adaptation to different information retrieval demands during\npre-training: some tasks require uniform recall across the entire input (a\nlong-term memory demand), while others prioritize the most recent information\n(a short-term memory demand). Consistent with this view, we show that this\nU-shaped performance curve emerges when LLMs (GPT-2 and Llama variants) are\ntrained from scratch on two simple human memory paradigms simulating long-term\nand short-term memory demands. Our analysis reveals that while the recency\neffect directly aligns with short-term memory demand in the training data, the\nprimacy effect is induced by the uniform long-term memory demand and is\nadditionally influenced by the model's autoregressive properties and the\nformation of attention sinks. Our main findings from simple human memory\nparadigms also generalize to a sequence completion task, which more closely\nresembles the next-token prediction process in LLM pre-training. Together, our\nfindings reveal how information retrieval demands, model architecture, and\nstructural attention dynamics during model training can jointly produce\npositional bias observed in LLMs."}
{"id": "2510.10278", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10278", "abs": "https://arxiv.org/abs/2510.10278", "authors": ["Christopher Chiu", "Silviu Pitis", "Mihaela van der Schaar"], "title": "Simulating Viva Voce Examinations to Evaluate Clinical Reasoning in Large Language Models", "comment": null, "summary": "Clinical reasoning in medicine is a hypothesis-driven process where\nphysicians refine diagnoses from limited information through targeted history,\nphysical examination, and diagnostic investigations. In contrast, current\nmedical benchmarks for large language models (LLMs) primarily assess knowledge\nrecall through single-turn questions, where complete clinical information is\nprovided upfront. To address this gap, we introduce VivaBench, a multi-turn\nbenchmark that evaluates sequential clinical reasoning in LLM agents. Our\ndataset consists of 1762 physician-curated clinical vignettes structured as\ninteractive scenarios that simulate a (oral) examination in medical training,\nrequiring agents to actively probe for relevant findings, select appropriate\ninvestigations, and synthesize information across multiple steps to reach a\ndiagnosis. While current LLMs demonstrate competence in diagnosing conditions\nfrom well-described clinical presentations, their performance degrades\nsignificantly when required to navigate iterative diagnostic reasoning under\nuncertainty in our evaluation. Our analysis identified several failure modes\nthat mirror common cognitive errors in clinical practice, including: (1)\nfixation on initial hypotheses, (2) inappropriate investigation ordering, (3)\npremature diagnostic closure, and (4) failing to screen for critical\nconditions. These patterns reveal fundamental limitations in how current LLMs\nreason and make decisions under uncertainty. Through VivaBench, we provide a\nstandardized benchmark for evaluating conversational medical AI systems for\nreal-world clinical decision support. Beyond medical applications, we\ncontribute to the larger corpus of research on agentic AI by demonstrating how\nsequential reasoning trajectories can diverge in complex decision-making\nenvironments."}
{"id": "2510.10304", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10304", "abs": "https://arxiv.org/abs/2510.10304", "authors": ["Michael Y. Hu", "Benjamin Van Durme", "Jacob Andreas", "Harsh Jhamtani"], "title": "Sample-Efficient Online Learning in LM Agents via Hindsight Trajectory Rewriting", "comment": null, "summary": "Language model (LM) agents deployed in novel environments often exhibit poor\nsample efficiency when learning from sequential interactions. This\nsignificantly hinders the usefulness of such agents in environments where\ninteraction is costly (for example, when they interact with humans or reset\nphysical systems). While a number of existing LM agent architectures\nincorporate various mechanisms for experience storage and reflection, they make\nlimited use of LMs' abilities to directly generate or reason about full\ncounterfactual trajectories. We introduce ECHO (Experience Consolidation via\nHindsight Optimization), a prompting framework that adapts hindsight experience\nreplay from reinforcement learning for language model agents. ECHO generates\noptimized trajectories for alternative goals that could have been achieved\nduring failed attempts, effectively creating synthetic positive examples from\nunsuccessful interactions. Our approach consists of two components: a hindsight\nrule that uses the language model itself to identify relevant subgoals and\ngenerate optimized trajectories, and an update rule that maintains compressed\ntrajectory representations in memory. We evaluate ECHO on stateful versions of\nXMiniGrid, a text-based navigation and planning benchmark, and PeopleJoinQA, a\ncollaborative information-gathering enterprise simulation. Across both domains,\nECHO outperforms vanilla language agent baselines by up to 80%; in XMiniGrid,\nit also outperforms a number of sophisticated agent architectures including\nReflexion and AWM, demonstrating faster adaptation to novel environments\nthrough more effective utilization of past experiences."}
{"id": "2510.10341", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10341", "abs": "https://arxiv.org/abs/2510.10341", "authors": ["Shiyu Chen", "Ningyuan", "Huang", "Soledad Villar"], "title": "Multi-View Graph Learning with Graph-Tuple", "comment": "Submitted to TAG workshop", "summary": "Graph Neural Networks (GNNs) typically scale with the number of graph edges,\nmaking them well suited for sparse graphs but less efficient on dense graphs,\nsuch as point clouds or molecular interactions. A common remedy is to sparsify\nthe graph via similarity thresholding or distance pruning, but this forces an\narbitrary choice of a single interaction scale and discards crucial information\nfrom other scales. To overcome this limitation, we introduce a multi-view\ngraph-tuple framework. Instead of a single graph, our graph-tuple framework\npartitions the graph into disjoint subgraphs, capturing primary local\ninteractions and weaker, long-range connections. We then learn multi-view\nrepresentations from the graph-tuple via a heterogeneous message-passing\narchitecture inspired by the theory of non-commuting operators, which we\nformally prove is strictly more expressive and guarantees a lower oracle risk\ncompared to single-graph message-passing models. We instantiate our framework\non two scientific domains: molecular property prediction from feature-scarce\nCoulomb matrices and cosmological parameter inference from geometric point\nclouds. On both applications, our multi-view graph-tuple models demonstrate\nbetter performance than single-graph baselines, highlighting the power and\nversatility of our multi-view approach."}
{"id": "2510.10364", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10364", "abs": "https://arxiv.org/abs/2510.10364", "authors": ["Ali Mirzazadeh", "Simon Cadavid", "Kaiwen Zha", "Chao Li", "Sultan Alzahrani", "Manar Alawajy", "Joshua Korzenik", "Kreshnik Hoti", "Charles Reynolds", "David Mischoulon", "John Winkelman", "Maurizio Fava", "Dina Katabi"], "title": "Transformer Model Detects Antidepressant Use From a Single Night of Sleep, Unlocking an Adherence Biomarker", "comment": null, "summary": "Antidepressant nonadherence is pervasive, driving relapse, hospitalization,\nsuicide risk, and billions in avoidable costs. Clinicians need tools that\ndetect adherence lapses promptly, yet current methods are either invasive\n(serum assays, neuroimaging) or proxy-based and inaccurate (pill counts,\npharmacy refills). We present the first noninvasive biomarker that detects\nantidepressant intake from a single night of sleep. A transformer-based model\nanalyzes sleep data from a consumer wearable or contactless wireless sensor to\ninfer antidepressant intake, enabling remote, effortless, daily adherence\nassessment at home. Across six datasets comprising 62,000 nights from >20,000\nparticipants (1,800 antidepressant users), the biomarker achieved AUROC = 0.84,\ngeneralized across drug classes, scaled with dose, and remained robust to\nconcomitant psychotropics. Longitudinal monitoring captured real-world\ninitiation, tapering, and lapses. This approach offers objective, scalable\nadherence surveillance with potential to improve depression care and outcomes."}
{"id": "2510.10374", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10374", "abs": "https://arxiv.org/abs/2510.10374", "authors": ["Ziyi Wei", "Huaiyang Zhong", "Xiaocheng Li"], "title": "Exploration-free Algorithms for Multi-group Mean Estimation", "comment": null, "summary": "We address the problem of multi-group mean estimation, which seeks to\nallocate a finite sampling budget across multiple groups to obtain uniformly\naccurate estimates of their means. Unlike classical multi-armed bandits, whose\nobjective is to minimize regret by identifying and exploiting the best arm, the\noptimal allocation in this setting requires sampling every group on the order\nof $\\Theta(T)$ times. This fundamental distinction makes exploration-free\nalgorithms both natural and effective. Our work makes three contributions.\nFirst, we strengthen the existing results on subgaussian variance concentration\nusing the Hanson-Wright inequality and identify a class of strictly subgaussian\ndistributions that yield sharper guarantees. Second, we design exploration-free\nnon-adaptive and adaptive algorithms, and we establish tighter regret bounds\nthan the existing results. Third, we extend the framework to contextual bandit\nsettings, an underexplored direction, and propose algorithms that leverage side\ninformation with provable guarantees. Overall, these results position\nexploration-free allocation as a principled and efficient approach to\nmulti-group mean estimation, with potential applications in experimental\ndesign, personalization, and other domains requiring accurate multi-group\ninference."}
{"id": "2510.10375", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.10375", "abs": "https://arxiv.org/abs/2510.10375", "authors": ["Kenichi Satoh"], "title": "Applying non-negative matrix factorization with covariates to label matrix for classification", "comment": "2 figures, R package: nmfkc published in GitHub,\n  https://github.com/ksatohds/nmfkc", "summary": "Non-negative matrix factorization (NMF) is widely used for dimensionality\nreduction and interpretable analysis, but standard formulations are\nunsupervised and cannot directly exploit class labels. Existing supervised or\nsemi-supervised extensions usually incorporate labels only via penalties or\ngraph constraints, still requiring an external classifier. We propose\n\\textit{NMF-LAB} (Non-negative Matrix Factorization for Label Matrix), which\nredefines classification as the inverse problem of non-negative matrix\ntri-factorization (tri-NMF). Unlike joint NMF methods, which reconstruct both\nfeatures and labels, NMF-LAB directly factorizes the label matrix $Y$ as the\nobservation, while covariates $A$ are treated as given explanatory variables.\nThis yields a direct probabilistic mapping from covariates to labels,\ndistinguishing our method from label-matrix factorization approaches that\nmainly model label correlations or impute missing labels. Our inversion offers\ntwo key advantages: (i) class-membership probabilities are obtained directly\nfrom the factorization without a separate classifier, and (ii) covariates,\nincluding kernel-based similarities, can be seamlessly integrated to generalize\npredictions to unseen samples. In addition, unlabeled data can be encoded as\nuniform distributions, supporting semi-supervised learning. Experiments on\ndiverse datasets, from small-scale benchmarks to the large-scale MNIST dataset,\ndemonstrate that NMF-LAB achieves competitive predictive accuracy, robustness\nto noisy or incomplete labels, and scalability to high-dimensional problems,\nwhile preserving interpretability. By unifying regression and classification\nwithin the tri-NMF framework, NMF-LAB provides a novel, probabilistic, and\nscalable approach to modern classification tasks."}
{"id": "2510.10402", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.10402", "abs": "https://arxiv.org/abs/2510.10402", "authors": ["Jiachi Zhao", "Zehong Wang", "Yamei Liao", "Chuxu Zhang", "Yanfang Ye"], "title": "Controllable Graph Generation with Diffusion Models via Inference-Time Tree Search Guidance", "comment": null, "summary": "Graph generation is a fundamental problem in graph learning with broad\napplications across Web-scale systems, knowledge graphs, and scientific domains\nsuch as drug and material discovery. Recent approaches leverage diffusion\nmodels for step-by-step generation, yet unconditional diffusion offers little\ncontrol over desired properties, often leading to unstable quality and\ndifficulty in incorporating new objectives. Inference-time guidance methods\nmitigate these issues by adjusting the sampling process without retraining, but\nthey remain inherently local, heuristic, and limited in controllability. To\novercome these limitations, we propose TreeDiff, a Monte Carlo Tree Search\n(MCTS) guided dual-space diffusion framework for controllable graph generation.\nTreeDiff is a plug-and-play inference-time method that expands the search space\nwhile keeping computation tractable. Specifically, TreeDiff introduces three\nkey designs to make it practical and scalable: (1) a macro-step expansion\nstrategy that groups multiple denoising updates into a single transition,\nreducing tree depth and enabling long-horizon exploration; (2) a dual-space\ndenoising mechanism that couples efficient latent-space denoising with\nlightweight discrete correction in graph space, ensuring both scalability and\nstructural fidelity; and (3) a dual-space verifier that predicts long-term\nrewards from partially denoised graphs, enabling early value estimation and\nremoving the need for full rollouts. Extensive experiments on 2D and 3D\nmolecular generation benchmarks, under both unconditional and conditional\nsettings, demonstrate that TreeDiff achieves state-of-the-art performance.\nNotably, TreeDiff exhibits favorable inference-time scaling: it continues to\nimprove with additional computation, while existing inference-time methods\nplateau early under limited resources."}
{"id": "2510.10425", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10425", "abs": "https://arxiv.org/abs/2510.10425", "authors": ["Sara Dragutinović", "Andrew M. Saxe", "Aaditya K. Singh"], "title": "Softmax $\\geq$ Linear: Transformers may learn to classify in-context by kernel gradient descent", "comment": null, "summary": "The remarkable ability of transformers to learn new concepts solely by\nreading examples within the input prompt, termed in-context learning (ICL), is\na crucial aspect of intelligent behavior. Here, we focus on understanding the\nlearning algorithm transformers use to learn from context. Existing theoretical\nwork, often based on simplifying assumptions, has primarily focused on linear\nself-attention and continuous regression tasks, finding transformers can learn\nin-context by gradient descent. Given that transformers are typically trained\non discrete and complex tasks, we bridge the gap from this existing work to the\nsetting of classification, with non-linear (importantly, softmax) activation.\nWe find that transformers still learn to do gradient descent in-context, though\non functionals in the kernel feature space and with a context-adaptive learning\nrate in the case of softmax transformer. These theoretical findings suggest a\ngreater adaptability to context for softmax attention, which we empirically\nverify and study through ablations. Overall, we hope this enhances theoretical\nunderstanding of in-context learning algorithms in more realistic settings,\npushes forward our intuitions and enables further theory bridging to larger\nmodels."}
{"id": "2510.10432", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10432", "abs": "https://arxiv.org/abs/2510.10432", "authors": ["Zhichen Zeng", "Mengyue Hang", "Xiaolong Liu", "Xiaoyi Liu", "Xiao Lin", "Ruizhong Qiu", "Tianxin Wei", "Zhining Liu", "Siyang Yuan", "Chaofei Yang", "Yiqun Liu", "Hang Yin", "Jiyan Yang", "Hanghang Tong"], "title": "Hierarchical LoRA MoE for Efficient CTR Model Scaling", "comment": "13 pages, 9 figures", "summary": "Deep models have driven significant advances in click-through rate (CTR)\nprediction. While vertical scaling via layer stacking improves model\nexpressiveness, the layer-by-layer sequential computation poses challenges to\nefficient scaling. Conversely, horizontal scaling through Mixture of Experts\n(MoE) achieves efficient scaling by activating a small subset of experts in\nparallel, but flat MoE layers may struggle to capture the hierarchical\nstructure inherent in recommendation tasks. To push the Return-On-Investment\n(ROI) boundary, we explore the complementary strengths of both directions and\npropose HiLoMoE, a hierarchical LoRA MoE framework that enables holistic\nscaling in a parameter-efficient manner. Specifically, HiLoMoE employs\nlightweight rank-1 experts for parameter-efficient horizontal scaling, and\nstacks multiple MoE layers with hierarchical routing to enable combinatorially\ndiverse expert compositions. Unlike conventional stacking, HiLoMoE routes based\non prior layer scores rather than outputs, allowing all layers to execute in\nparallel. A principled three-stage training framework ensures stable\noptimization and expert diversity. Experiments on four public datasets show\nthat HiLoMoE achieving better performance-efficiency tradeoff, achieving an\naverage AUC improvement of 0.20\\% in AUC and 18.5\\% reduction in FLOPs compared\nto the non-MoE baseline."}
{"id": "2510.10433", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10433", "abs": "https://arxiv.org/abs/2510.10433", "authors": ["Zixiang Xu", "Menghui Zhou", "Jun Qi", "Xuanhan Fan", "Yun Yang", "Po Yang"], "title": "Multi-Task Learning with Feature-Similarity Laplacian Graphs for Predicting Alzheimer's Disease Progression", "comment": null, "summary": "Alzheimer's Disease (AD) is the most prevalent neurodegenerative disorder in\naging populations, posing a significant and escalating burden on global\nhealthcare systems. While Multi-Tusk Learning (MTL) has emerged as a powerful\ncomputational paradigm for modeling longitudinal AD data, existing frameworks\ndo not account for the time-varying nature of feature correlations. To address\nthis limitation, we propose a novel MTL framework, named Feature Similarity\nLaplacian graph Multi-Task Learning (MTL-FSL). Our framework introduces a novel\nFeature Similarity Laplacian (FSL) penalty that explicitly models the\ntime-varying relationships between features. By simultaneously considering\ntemporal smoothness among tasks and the dynamic correlations among features,\nour model enhances both predictive accuracy and biological interpretability. To\nsolve the non-smooth optimization problem arising from our proposed penalty\nterms, we adopt the Alternating Direction Method of Multipliers (ADMM)\nalgorithm. Experiments conducted on the Alzheimer's Disease Neuroimaging\nInitiative (ADNI) dataset demonstrate that our proposed MTL-FSL framework\nachieves state-of-the-art performance, outperforming various baseline methods.\nThe implementation source can be found at https://github.com/huatxxx/MTL-FSL."}
{"id": "2510.10446", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10446", "abs": "https://arxiv.org/abs/2510.10446", "authors": ["Masoud Makrehchi"], "title": "Reverse Supervision at Scale: Exponential Search Meets the Economics of Annotation", "comment": "10 pages", "summary": "We analyze a reversed-supervision strategy that searches over labelings of a\nlarge unlabeled set \\(B\\) to minimize error on a small labeled set \\(A\\). The\nsearch space is \\(2^n\\), and the resulting complexity remains exponential even\nunder large constant-factor speedups (e.g., quantum or massively parallel\nhardware). Consequently, arbitrarily fast -- but not exponentially faster --\ncomputation does not obviate the need for informative labels or priors. In\npractice, the machine learning pipeline still requires an initial human\ncontribution: specifying the objective, defining classes, and providing a seed\nset of representative annotations that inject inductive bias and align models\nwith task semantics. Synthetic labels from generative AI can partially\nsubstitute provided their quality is human-grade and anchored by a\nhuman-specified objective, seed supervision, and validation. In this view,\ngenerative models function as \\emph{label amplifiers}, leveraging small\nhuman-curated cores via active, semi-supervised, and self-training loops, while\nhumans retain oversight for calibration, drift detection, and failure auditing.\nThus, extreme computational speed reduces wall-clock time but not the\nfundamental supervision needs of learning; initial human (or human-grade) input\nremains necessary to ground the system in the intended task."}
{"id": "2510.10451", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10451", "abs": "https://arxiv.org/abs/2510.10451", "authors": ["Keisuke Fujii", "Kazushi Tsutsui", "Yu Teshima", "Makoto Itoh", "Naoya Takeishi", "Nozomi Nishiumi", "Ryoya Tanaka", "Shunsuke Shigaki", "Yoshinobu Kawahara"], "title": "Data-driven simulator of multi-animal behavior with unknown dynamics via offline and online reinforcement learning", "comment": "21 pages, 7 figures", "summary": "Simulators of animal movements play a valuable role in studying behavior.\nAdvances in imitation learning for robotics have expanded possibilities for\nreproducing human and animal movements. A key challenge for realistic\nmulti-animal simulation in biology is bridging the gap between unknown\nreal-world transition models and their simulated counterparts. Because\nlocomotion dynamics are seldom known, relying solely on mathematical models is\ninsufficient; constructing a simulator that both reproduces real trajectories\nand supports reward-driven optimization remains an open problem. We introduce a\ndata-driven simulator for multi-animal behavior based on deep reinforcement\nlearning and counterfactual simulation. We address the ill-posed nature of the\nproblem caused by high degrees of freedom in locomotion by estimating movement\nvariables of an incomplete transition model as actions within an RL framework.\nWe also employ a distance-based pseudo-reward to align and compare states\nbetween cyber and physical spaces. Validated on artificial agents, flies,\nnewts, and silkmoth, our approach achieves higher reproducibility of\nspecies-specific behaviors and improved reward acquisition compared with\nstandard imitation and RL methods. Moreover, it enables counterfactual behavior\nprediction in novel experimental settings and supports multi-individual\nmodeling for flexible what-if trajectory generation, suggesting its potential\nto simulate and elucidate complex multi-animal behaviors."}
{"id": "2510.10465", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10465", "abs": "https://arxiv.org/abs/2510.10465", "authors": ["Yi Ren", "Xinjie Yu"], "title": "LightSAE: Parameter-Efficient and Heterogeneity-Aware Embedding for IoT Multivariate Time Series Forecasting", "comment": "Submitted to IEEE IoT-J", "summary": "Modern Internet of Things (IoT) systems generate massive, heterogeneous\nmultivariate time series data. Accurate Multivariate Time Series Forecasting\n(MTSF) of such data is critical for numerous applications. However, existing\nmethods almost universally employ a shared embedding layer that processes all\nchannels identically, creating a representational bottleneck that obscures\nvaluable channel-specific information. To address this challenge, we introduce\na Shared-Auxiliary Embedding (SAE) framework that decomposes the embedding into\na shared base component capturing common patterns and channel-specific\nauxiliary components modeling unique deviations. Within this decomposition, we\n\\rev{empirically observe} that the auxiliary components tend to exhibit\nlow-rank and clustering characteristics, a structural pattern that is\nsignificantly less apparent when using purely independent embeddings.\nConsequently, we design LightSAE, a parameter-efficient embedding module that\noperationalizes these observed characteristics through low-rank factorization\nand a shared, gated component pool. Extensive experiments across 9 IoT-related\ndatasets and 4 backbone architectures demonstrate LightSAE's effectiveness,\nachieving MSE improvements of up to 22.8\\% with only 4.0\\% parameter increase."}
{"id": "2510.10467", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10467", "abs": "https://arxiv.org/abs/2510.10467", "authors": ["Gunho Park", "Jeongin Bae", "Beomseok Kwon", "Byeongwook Kim", "Se Jung Kwon", "Dongsoo Lee"], "title": "AnyBCQ: Hardware Efficient Flexible Binary-Coded Quantization for Multi-Precision LLMs", "comment": null, "summary": "The deployment of large language models (LLMs) is increasingly constrained by\nmemory and latency bottlenecks, motivating the need for quantization techniques\nthat flexibly balance accuracy and efficiency. Recent work has introduced\nmulti-precision models, which enable inference at multiple precisions within a\nsingle model depending on runtime constraints. To support such flexibility,\nquantized weights are often stored as bit-planes, where hardware efficiency\nimproves when the compute operates directly at the bit-plane level and\nactivates only the precision required by each request. In this work, we present\nAnyBCQ, a hardware-friendly multi-precision extension of Binary-Coded\nQuantization (BCQ) that supports direct bit-plane operations. By representing\nweights as binary bit-planes with corresponding scale factors, AnyBCQ enables\nbit-plane-level computation and maps naturally to accelerator-friendly,\nbit-parallel arithmetic. Our progressive precision expansion mechanism\nincrementally refines scaling factors while reusing previously assigned binary\ncodes, yielding monotonic improvements in accuracy as additional bits are\nenabled. We further co-design a specialized kernel that exploits the BCQ\nstructure to support dynamic per-request precision selection with negligible\noverhead. Experiments on recent LLMs demonstrate that AnyBCQ significantly\nnarrows the accuracy drop in the low-bit regime (e.g. 2-bit), remains\ncompetitive at higher precision, and achieves throughput gains of up to 3.0x\nover half precision and 1.2x over state-of-the-art multi-precision methods. By\naligning algorithmic flexibility with hardware efficiency, AnyBCQ provides a\npractical foundation for multi-precision LLM deployment across diverse\nservice-level objectives."}
{"id": "2510.10477", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10477", "abs": "https://arxiv.org/abs/2510.10477", "authors": ["Zhijian Zhou", "Liuhua Peng", "Xunye Tian", "Feng Liu"], "title": "Anchor-based Maximum Discrepancy for Relative Similarity Testing", "comment": null, "summary": "The relative similarity testing aims to determine which of the distributions,\nP or Q, is closer to an anchor distribution U. Existing kernel-based approaches\noften test the relative similarity with a fixed kernel in a manually specified\nalternative hypothesis, e.g., Q is closer to U than P. Although kernel\nselection is known to be important to kernel-based testing methods, the\nmanually specified hypothesis poses a significant challenge for kernel\nselection in relative similarity testing: Once the hypothesis is specified\nfirst, we can always find a kernel such that the hypothesis is rejected. This\nchallenge makes relative similarity testing ill-defined when we want to select\na good kernel after the hypothesis is specified. In this paper, we cope with\nthis challenge via learning a proper hypothesis and a kernel simultaneously,\ninstead of learning a kernel after manually specifying the hypothesis. We\npropose an anchor-based maximum discrepancy (AMD), which defines the relative\nsimilarity as the maximum discrepancy between the distances of (U, P) and (U,\nQ) in a space of deep kernels. Based on AMD, our testing incorporates two\nphases. In Phase I, we estimate the AMD over the deep kernel space and infer\nthe potential hypothesis. In Phase II, we assess the statistical significance\nof the potential hypothesis, where we propose a unified testing framework to\nderive thresholds for tests over different possible hypotheses from Phase I.\nLastly, we validate our method theoretically and demonstrate its effectiveness\nvia extensive experiments on benchmark datasets. Codes are publicly available\nat: https://github.com/zhijianzhouml/AMD."}
{"id": "2510.10480", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10480", "abs": "https://arxiv.org/abs/2510.10480", "authors": ["Zishen Zhang", "Xiangzhe Kong", "Wenbing Huang", "Yang Liu"], "title": "Latent Retrieval Augmented Generation of Cross-Domain Protein Binders", "comment": null, "summary": "Designing protein binders targeting specific sites, which requires to\ngenerate realistic and functional interaction patterns, is a fundamental\nchallenge in drug discovery. Current structure-based generative models are\nlimited in generating nterfaces with sufficient rationality and\ninterpretability. In this paper, we propose Retrieval-Augmented Diffusion for\nAligned interface (RADiAnce), a new framework that leverages known interfaces\nto guide the design of novel binders. By unifying retrieval and generation in a\nshared contrastive latent space, our model efficiently identifies relevant\ninterfaces for a given binding site and seamlessly integrates them through a\nconditional latent diffusion generator, enabling cross-domain interface\ntransfer. Extensive exeriments show that RADiAnce significantly outperforms\nbaseline models across multiple metrics, including binding affinity and\nrecovery of geometries and interactions. Additional experimental results\nvalidate cross-domain generalization, demonstrating that retrieving interfaces\nfrom diverse domains, such as peptides, antibodies, and protein fragments,\nenhances the generation performance of binders for other domains. Our work\nestablishes a new paradigm for protein binder design that successfully bridges\nretrieval-based knowledge and generative AI, opening new possibilities for drug\ndiscovery."}
{"id": "2510.10483", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.10483", "abs": "https://arxiv.org/abs/2510.10483", "authors": ["Narayan S Iyer", "Bivas Bhaumik", "Ram S Iyer", "Satyasaran Changdar"], "title": "Gradient Enhanced Self-Training Physics-Informed Neural Network (gST-PINN) for Solving Nonlinear Partial Differential Equations", "comment": null, "summary": "Partial differential equations (PDEs) provide a mathematical foundation for\nsimulating and understanding intricate behaviors in both physical sciences and\nengineering. With the growing capabilities of deep learning, data$-$driven\napproaches like Physics$-$Informed Neural Networks (PINNs) have been developed,\noffering a mesh$-$free, analytic type framework for efficiently solving PDEs\nacross a wide range of applications. However, traditional PINNs often struggle\nwith challenges such as limited precision, slow training dynamics, lack of\nlabeled data availability, and inadequate handling of multi$-$physics\ninteractions. To overcome these challenging issues of PINNs, we proposed a\nGradient Enhanced Self$-$Training PINN (gST$-$PINN) method that specifically\nintroduces a gradient based pseudo point self$-$learning algorithm for solving\nPDEs. We tested the proposed method on three different types of PDE problems\nfrom various fields, each representing distinct scenarios. The effectiveness of\nthe proposed method is evident, as the PINN approach for solving the Burgers$'$\nequation attains a mean square error (MSE) on the order of $10^{-3}$, while the\ndiffusion$-$sorption equation achieves an MSE on the order of $10^{-4}$ after\n12,500 iterations, with no further improvement as the iterations increase. In\ncontrast, the MSE for both PDEs in the gST$-$PINN model continues to decrease,\ndemonstrating better generalization and reaching an MSE on the order of\n$10^{-5}$ after 18,500 iterations. Furthermore, the results show that the\nproposed purely semi$-$supervised gST$-$PINN consistently outperforms the\nstandard PINN method in all cases, even when solution of the PDEs are\nunavailable. It generalizes both PINN and Gradient$-$enhanced PINN (gPINN), and\ncan be effectively applied in scenarios prone to low accuracy and convergence\nissues, particularly in the absence of labeled data."}
{"id": "2510.10503", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10503", "abs": "https://arxiv.org/abs/2510.10503", "authors": ["Kanishkha Jaisankar", "Sunidhi Tandel"], "title": "Align2Act: Instruction-Tuned Models for Human-Aligned Autonomous Driving", "comment": null, "summary": "Motion planning in complex scenarios is a core challenge in autonomous\ndriving. Conventional methods apply predefined rules or learn from driving data\nto generate trajectories, while recent approaches leverage large language\nmodels (LLMs) for decision-making. However, it remains unclear whether LLMs\ntruly capture human driving logic. We propose Align2Act, a motion planning\nframework that transforms instruction-tuned LLMs into interpretable planners\naligned with human behavior. We derive structured driving instructions based on\nhuman reasoning patterns (e.g., anticipate hazards, yield at intersections) and\ntraffic rules (e.g., stop at red lights, maintain lane boundaries). Our\nAlign2ActChain module guides step-by-step reasoning to produce both an\ninterpretable rationale and a safe trajectory. By fine-tuning LLaMA-2-7B with\nLoRA on one million scenarios from the nuPlan dataset, our method achieves an\nopen-loop score of 85.17 and closed-loop scores of 70.31 (non-reactive) and\n66.96 (reactive) on Test14-random. Unlike prior work focused on synthetic or\nopen-loop settings, we demonstrate improved planning quality and human-likeness\non the real-world nuPlan closed-loop benchmark. Ablation studies confirm that\nstructured reasoning significantly improves performance over baseline LLM\nplanners."}
{"id": "2510.10510", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10510", "abs": "https://arxiv.org/abs/2510.10510", "authors": ["Subhodip Panda", "Dhruv Tarsadiya", "Shashwat Sourav", "Prathosh A. P", "Sai Praneeth Karimireddy"], "title": "f-INE: A Hypothesis Testing Framework for Estimating Influence under Training Randomness", "comment": null, "summary": "Influence estimation methods promise to explain and debug machine learning by\nestimating the impact of individual samples on the final model. Yet, existing\nmethods collapse under training randomness: the same example may appear\ncritical in one run and irrelevant in the next. Such instability undermines\ntheir use in data curation or cleanup since it is unclear if we indeed\ndeleted/kept the correct datapoints. To overcome this, we introduce\n*f-influence* -- a new influence estimation framework grounded in hypothesis\ntesting that explicitly accounts for training randomness, and establish\ndesirable properties that make it suitable for reliable influence estimation.\nWe also design a highly efficient algorithm **f**-**IN**fluence **E**stimation\n(**f-INE**) that computes f-influence **in a single training run**. Finally, we\nscale up f-INE to estimate influence of instruction tuning data on Llama-3.1-8B\nand show it can reliably detect poisoned samples that steer model opinions,\ndemonstrating its utility for data cleanup and attributing model behavior."}
{"id": "2510.10513", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10513", "abs": "https://arxiv.org/abs/2510.10513", "authors": ["Md Ibrahim Shikder Mahin", "Md Shamsul Arefin", "Md Tanvir Hasan"], "title": "A Hybrid Machine Learning Approach for Synthetic Data Generation with Post Hoc Calibration for Clinical Tabular Datasets", "comment": null, "summary": "Healthcare research and development face significant obstacles due to data\nscarcity and stringent privacy regulations, such as HIPAA and the GDPR,\nrestricting access to essential real-world medical data. These limitations\nimpede innovation, delay robust AI model creation, and hinder advancements in\npatient-centered care. Synthetic data generation offers a transformative\nsolution by producing artificial datasets that emulate real data statistics\nwhile safeguarding patient privacy. We introduce a novel hybrid framework for\nhigh-fidelity healthcare data synthesis integrating five augmentation methods:\nnoise injection, interpolation, Gaussian Mixture Model (GMM) sampling,\nConditional Variational Autoencoder (CVAE) sampling, and SMOTE, combined via a\nreinforcement learning-based dynamic weight selection mechanism. Its key\ninnovations include advanced calibration techniques -- moment matching, full\nhistogram matching, soft and adaptive soft histogram matching, and iterative\nrefinement -- that align marginal distributions and preserve joint feature\ndependencies. Evaluated on the Breast Cancer Wisconsin (UCI Repository) and\nKhulna Medical College cardiology datasets, our calibrated hybrid achieves\nWasserstein distances as low as 0.001 and Kolmogorov-Smirnov statistics around\n0.01, demonstrating near-zero marginal discrepancy. Pairwise trend scores\nsurpass 90%, and Nearest Neighbor Adversarial Accuracy approaches 50%,\nconfirming robust privacy protection. Downstream classifiers trained on\nsynthetic data achieve up to 94% accuracy and F1 scores above 93%, comparable\nto models trained on real data. This scalable, privacy-preserving approach\nmatches state-of-the-art methods, sets new benchmarks for joint-distribution\nfidelity in healthcare, and supports sensitive AI applications."}
{"id": "2510.10530", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10530", "abs": "https://arxiv.org/abs/2510.10530", "authors": ["Hanbing Liu", "Huaze Tang", "Yanru Wu", "Yang Li", "Xiao-Ping Zhang"], "title": "Reinforced Domain Selection for Continuous Domain Adaptation", "comment": null, "summary": "Continuous Domain Adaptation (CDA) effectively bridges significant domain\nshifts by progressively adapting from the source domain through intermediate\ndomains to the target domain. However, selecting intermediate domains without\nexplicit metadata remains a substantial challenge that has not been extensively\nexplored in existing studies. To tackle this issue, we propose a novel\nframework that combines reinforcement learning with feature disentanglement to\nconduct domain path selection in an unsupervised CDA setting. Our approach\nintroduces an innovative unsupervised reward mechanism that leverages the\ndistances between latent domain embeddings to facilitate the identification of\noptimal transfer paths. Furthermore, by disentangling features, our method\nfacilitates the calculation of unsupervised rewards using domain-specific\nfeatures and promotes domain adaptation by aligning domain-invariant features.\nThis integrated strategy is designed to simultaneously optimize transfer paths\nand target task performance, enhancing the effectiveness of domain adaptation\nprocesses. Extensive empirical evaluations on datasets such as Rotated MNIST\nand ADNI demonstrate substantial improvements in prediction accuracy and domain\nselection efficiency, establishing our method's superiority over traditional\nCDA approaches."}
{"id": "2510.10541", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10541", "abs": "https://arxiv.org/abs/2510.10541", "authors": ["Zihan Chen", "Yiming Zhang", "Hengguang Zhou", "Zenghui Ding", "Yining Sun", "Cho-Jui Hsieh"], "title": "Rethinking RL Evaluation: Can Benchmarks Truly Reveal Failures of RL Methods?", "comment": null, "summary": "Current benchmarks are inadequate for evaluating progress in reinforcement\nlearning (RL) for large language models (LLMs).Despite recent benchmark gains\nreported for RL, we find that training on these benchmarks' training sets\nachieves nearly the same performance as training directly on the test sets,\nsuggesting that the benchmarks cannot reliably separate further progress.To\nstudy this phenomenon, we introduce a diagnostic suite and the Oracle\nPerformance Gap (OPG) metric that quantifies the performance difference between\ntraining on the train split versus the test split of a benchmark. We further\nanalyze this phenomenon with stress tests and find that, despite strong\nbenchmark scores, existing RL methods struggle to generalize across\ndistribution shifts, varying levels of difficulty, and counterfactual\nscenarios: shortcomings that current benchmarks fail to reveal.We conclude that\ncurrent benchmarks are insufficient for evaluating generalization and propose\nthree core principles for designing more faithful benchmarks: sufficient\ndifficulty, balanced evaluation, and distributional robustness."}
{"id": "2510.10544", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10544", "abs": "https://arxiv.org/abs/2510.10544", "authors": ["Abdelkrim Zitouni", "Mehdi Hennequin", "Juba Agoun", "Ryan Horache", "Nadia Kabachi", "Omar Rivasplata"], "title": "PAC-Bayesian Reinforcement Learning Trains Generalizable Policies", "comment": null, "summary": "We derive a novel PAC-Bayesian generalization bound for reinforcement\nlearning that explicitly accounts for Markov dependencies in the data, through\nthe chain's mixing time. This contributes to overcoming challenges in obtaining\ngeneralization guarantees for reinforcement learning, where the sequential\nnature of data breaks the independence assumptions underlying classical bounds.\nOur bound provides non-vacuous certificates for modern off-policy algorithms\nlike Soft Actor-Critic. We demonstrate the bound's practical utility through\nPB-SAC, a novel algorithm that optimizes the bound during training to guide\nexploration. Experiments across continuous control tasks show that our approach\nprovides meaningful confidence certificates while maintaining competitive\nperformance."}
{"id": "2510.10558", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10558", "abs": "https://arxiv.org/abs/2510.10558", "authors": ["Weiming Zhao", "Xulong Wang", "Jun Qi", "Yun Yang", "Po Yang"], "title": "Multi-scale Frequency-Aware Adversarial Network for Parkinson's Disease Assessment Using Wearable Sensors", "comment": null, "summary": "Severity assessment of Parkinson's disease (PD) using wearable sensors offers\nan effective, objective basis for clinical management. However, general-purpose\ntime series models often lack pathological specificity in feature extraction,\nmaking it difficult to capture subtle signals highly correlated with\nPD.Furthermore, the temporal sparsity of PD symptoms causes key diagnostic\nfeatures to be easily \"diluted\" by traditional aggregation methods, further\ncomplicating assessment. To address these issues, we propose the Multi-scale\nFrequency-Aware Adversarial Multi-Instance Network (MFAM). This model enhances\nfeature specificity through a frequency decomposition module guided by medical\nprior knowledge. Furthermore, by introducing an attention-based multi-instance\nlearning (MIL) framework, the model can adaptively focus on the most\ndiagnostically valuable sparse segments.We comprehensively validated MFAM on\nboth the public PADS dataset for PD versus differential diagnosis (DD) binary\nclassification and a private dataset for four-class severity assessment.\nExperimental results demonstrate that MFAM outperforms general-purpose time\nseries models in handling complex clinical time series with specificity,\nproviding a promising solution for automated assessment of PD severity."}
{"id": "2510.10570", "categories": ["cs.LG", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.10570", "abs": "https://arxiv.org/abs/2510.10570", "authors": ["Zirui Wan", "Stefan Vlaski"], "title": "Multitask Learning with Learned Task Relationships", "comment": null, "summary": "Classical consensus-based strategies for federated and decentralized learning\nare statistically suboptimal in the presence of heterogeneous local data or\ntask distributions. As a result, in recent years, there has been growing\ninterest in multitask or personalized strategies, which allow individual agents\nto benefit from one another in pursuing locally optimal models without\nenforcing consensus. Existing strategies require either precise prior knowledge\nof the underlying task relationships or are fully non-parametric and instead\nrely on meta-learning or proximal constructions. In this work, we introduce an\nalgorithmic framework that strikes a balance between these extremes. By\nmodeling task relationships through a Gaussian Markov Random Field with an\nunknown precision matrix, we develop a strategy that jointly learns both the\ntask relationships and the local models, allowing agents to self-organize in a\nway consistent with their individual data distributions. Our theoretical\nanalysis quantifies the quality of the learned relationship, and our numerical\nexperiments demonstrate its practical effectiveness."}
{"id": "2510.10572", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10572", "abs": "https://arxiv.org/abs/2510.10572", "authors": ["Byeongchan Lee"], "title": "Understanding Self-supervised Contrastive Learning through Supervised Objectives", "comment": "Accepted at TMLR 2025", "summary": "Self-supervised representation learning has achieved impressive empirical\nsuccess, yet its theoretical understanding remains limited. In this work, we\nprovide a theoretical perspective by formulating self-supervised representation\nlearning as an approximation to supervised representation learning objectives.\nBased on this formulation, we derive a loss function closely related to popular\ncontrastive losses such as InfoNCE, offering insight into their underlying\nprinciples. Our derivation naturally introduces the concepts of prototype\nrepresentation bias and a balanced contrastive loss, which help explain and\nimprove the behavior of self-supervised learning algorithms. We further show\nhow components of our theoretical framework correspond to established practices\nin contrastive learning. Finally, we empirically validate the effect of\nbalancing positive and negative pair interactions. All theoretical proofs are\nprovided in the appendix, and our code is included in the supplementary\nmaterial."}
{"id": "2510.10586", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.10586", "abs": "https://arxiv.org/abs/2510.10586", "authors": ["Giulio Ruffini"], "title": "Compositional Symmetry as Compression: Lie Pseudogroup Structure in Algorithmic Agents", "comment": "Submitted to NeurReps 2025 (https://www.neurreps.org)", "summary": "In the algorithmic (Kolmogorov) view, agents are programs that track and\ncompress sensory streams using generative programs. We propose a framework\nwhere the relevant structural prior is simplicity (Solomonoff) understood as\n\\emph{compositional symmetry}: natural streams are well described by (local)\nactions of finite-parameter Lie pseudogroups on geometrically and topologically\ncomplex low-dimensional configuration manifolds (latent spaces). Modeling the\nagent as a generic neural dynamical system coupled to such streams, we show\nthat accurate world-tracking imposes (i) \\emph{structural constraints} --\nequivariance of the agent's constitutive equations and readouts -- and (ii)\n\\emph{dynamical constraints}: under static inputs, symmetry induces conserved\nquantities (Noether-style labels) in the agent dynamics and confines\ntrajectories to reduced invariant manifolds; under slow drift, these manifolds\nmove but remain low-dimensional. This yields a hierarchy of reduced manifolds\naligned with the compositional factorization of the pseudogroup, providing a\ngeometric account of the ``blessing of compositionality'' in deep models. We\nconnect these ideas to the Spencer formalism for Lie pseudogroups and formulate\na symmetry-based, self-contained version of predictive coding in which higher\nlayers receive only \\emph{coarse-grained residual transformations}\n(prediction-error coordinates) along symmetry directions unresolved at lower\nlayers."}
{"id": "2510.10604", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10604", "abs": "https://arxiv.org/abs/2510.10604", "authors": ["Yuheng Chen", "Dingkun Liu", "Xinyao Yang", "Xinping Xu", "Baicheng Chen", "Dongrui Wu"], "title": "FusionGen: Feature Fusion-Based Few-Shot EEG Data Generation", "comment": null, "summary": "Brain-computer interfaces (BCIs) provide potential for applications ranging\nfrom medical rehabilitation to cognitive state assessment by establishing\ndirect communication pathways between the brain and external devices via\nelectroencephalography (EEG). However, EEG-based BCIs are severely constrained\nby data scarcity and significant inter-subject variability, which hinder the\ngeneralization and applicability of EEG decoding models in practical settings.\nTo address these challenges, we propose FusionGen, a novel EEG data generation\nframework based on disentangled representation learning and feature fusion. By\nintegrating features across trials through a feature matching fusion module and\ncombining them with a lightweight feature extraction and reconstruction\npipeline, FusionGen ensures both data diversity and trainability under limited\ndata constraints. Extensive experiments on multiple publicly available EEG\ndatasets demonstrate that FusionGen significantly outperforms existing\naugmentation techniques, yielding notable improvements in classification\naccuracy."}
{"id": "2510.10605", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10605", "abs": "https://arxiv.org/abs/2510.10605", "authors": ["MohammadHossein Bateni", "Hossein Esfandiari", "Samira HosseinGhorban", "Alireza Mirrokni", "Radin Shahdaei"], "title": "Budget Allocation for Unknown Value Functions in a Lipschitz Space", "comment": null, "summary": "Building learning models frequently requires evaluating numerous intermediate\nmodels. Examples include models considered during feature selection, model\nstructure search, and parameter tunings. The evaluation of an intermediate\nmodel influences subsequent model exploration decisions. Although prior\nknowledge can provide initial quality estimates, true performance is only\nrevealed after evaluation. In this work, we address the challenge of optimally\nallocating a bounded budget to explore the space of intermediate models. We\nformalize this as a general budget allocation problem over unknown-value\nfunctions within a Lipschitz space."}
{"id": "2510.10617", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.10617", "abs": "https://arxiv.org/abs/2510.10617", "authors": ["Bahadur Yadav", "Sanjay Kumar Mohanty"], "title": "Encoder Decoder Generative Adversarial Network Model for Stock Market Prediction", "comment": null, "summary": "Forecasting stock prices remains challenging due to the volatile and\nnon-linear nature of financial markets. Despite the promise of deep learning,\nissues such as mode collapse, unstable training, and difficulty in capturing\ntemporal and feature level correlations have limited the applications of GANs\nin this domain. We propose a GRU-based Encoder-Decoder GAN (EDGAN) model that\nstrikes a balance between expressive power and simplicity. The model introduces\nkey innovations such as a temporal decoder with residual connections for\nprecise reconstruction, conditioning on static and dynamic covariates for\ncontextual learning, and a windowing mechanism to capture temporal dynamics.\nHere, the generator uses a dense encoder-decoder framework with residual GRU\nblocks. Extensive experiments on diverse stock datasets demonstrate that EDGAN\nachieves superior forecasting accuracy and training stability, even in volatile\nmarkets. It consistently outperforms traditional GAN variants in forecasting\naccuracy and convergence stability under market conditions."}
{"id": "2510.10621", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10621", "abs": "https://arxiv.org/abs/2510.10621", "authors": ["Hanbing Liu", "Yanru Wu", "Yang Li", "Ercan E. Kuruoglu", "Xuan Zhang"], "title": "SDG-L: A Semiparametric Deep Gaussian Process based Framework for Battery Capacity Prediction", "comment": null, "summary": "Lithium-ion batteries are becoming increasingly omnipresent in energy supply.\nHowever, the durability of energy storage using lithium-ion batteries is\nthreatened by their dropping capacity with the growing number of\ncharging/discharging cycles. An accurate capacity prediction is the key to\nensure system efficiency and reliability, where the exploitation of battery\nstate information in each cycle has been largely undervalued. In this paper, we\npropose a semiparametric deep Gaussian process regression framework named SDG-L\nto give predictions based on the modeling of time series battery state data. By\nintroducing an LSTM feature extractor, the SDG-L is specially designed to\nbetter utilize the auxiliary profiling information during charging/discharging\nprocess. In experimental studies based on NASA dataset, our proposed method\nobtains an average test MSE error of 1.2%. We also show that SDG-L achieves\nbetter performance compared to existing works and validate the framework using\nablation studies."}
{"id": "2510.10625", "categories": ["cs.LG", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10625", "abs": "https://arxiv.org/abs/2510.10625", "authors": ["Yuval Golbari", "Navve Wasserman", "Gal Vardi", "Michal Irani"], "title": "ImpMIA: Leveraging Implicit Bias for Membership Inference Attack under Realistic Scenarios", "comment": null, "summary": "Determining which data samples were used to train a model-known as Membership\nInference Attack (MIA)-is a well-studied and important problem with\nimplications for data privacy. Black-box methods presume access only to the\nmodel's outputs and often rely on training auxiliary reference models. While\nthey have shown strong empirical performance, they rely on assumptions that\nrarely hold in real-world settings: (i) the attacker knows the training\nhyperparameters; (ii) all available non-training samples come from the same\ndistribution as the training data; and (iii) the fraction of training data in\nthe evaluation set is known. In this paper, we demonstrate that removing these\nassumptions leads to a significant drop in the performance of black-box\nattacks. We introduce ImpMIA, a Membership Inference Attack that exploits the\nImplicit Bias of neural networks, hence removes the need to rely on any\nreference models and their assumptions. ImpMIA is a white-box attack -- a\nsetting which assumes access to model weights and is becoming increasingly\nrealistic given that many models are publicly available (e.g., via Hugging\nFace). Building on maximum-margin implicit bias theory, ImpMIA uses the\nKarush-Kuhn-Tucker (KKT) optimality conditions to identify training samples.\nThis is done by finding the samples whose gradients most strongly reconstruct\nthe trained model's parameters. As a result, ImpMIA achieves state-of-the-art\nperformance compared to both black and white box attacks in realistic settings\nwhere only the model weights and a superset of the training data are available."}
{"id": "2510.10634", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10634", "abs": "https://arxiv.org/abs/2510.10634", "authors": ["Shaoning Li", "Le Zhuo", "Yusong Wang", "Mingyu Li", "Xinheng He", "Fandi Wu", "Hongsheng Li", "Pheng-Ann Heng"], "title": "ProteinAE: Protein Diffusion Autoencoders for Structure Encoding", "comment": null, "summary": "Developing effective representations of protein structures is essential for\nadvancing protein science, particularly for protein generative modeling.\nCurrent approaches often grapple with the complexities of the SE(3) manifold,\nrely on discrete tokenization, or the need for multiple training objectives,\nall of which can hinder the model optimization and generalization. We introduce\nProteinAE, a novel and streamlined protein diffusion autoencoder designed to\novercome these challenges by directly mapping protein backbone coordinates from\nE(3) into a continuous, compact latent space. ProteinAE employs a\nnon-equivariant Diffusion Transformer with a bottleneck design for efficient\ncompression and is trained end-to-end with a single flow matching objective,\nsubstantially simplifying the optimization pipeline. We demonstrate that\nProteinAE achieves state-of-the-art reconstruction quality, outperforming\nexisting autoencoders. The resulting latent space serves as a powerful\nfoundation for a latent diffusion model that bypasses the need for explicit\nequivariance. This enables efficient, high-quality structure generation that is\ncompetitive with leading structure-based approaches and significantly\noutperforms prior latent-based methods. Code is available at\nhttps://github.com/OnlyLoveKFC/ProteinAE_v1."}
{"id": "2510.10645", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10645", "abs": "https://arxiv.org/abs/2510.10645", "authors": ["Michal Sadowski", "Maria Wyrzykowska", "Lukasz Sztukiewicz", "Tadija Radusinović", "Jan Rzymkowski", "Paweł Włodarczyk-Pruszyński", "Mikołaj Sacha", "Piotr Kozakowski", "Ruard van Workum", "Stanislaw Kamil Jastrzebski"], "title": "Trustworthy Retrosynthesis: Eliminating Hallucinations with a Diverse Ensemble of Reaction Scorers", "comment": null, "summary": "Retrosynthesis is one of the domains transformed by the rise of generative\nmodels, and it is one where the problem of nonsensical or erroneous outputs\n(hallucinations) is particularly insidious: reliable assessment of synthetic\nplans is time-consuming, with automatic methods lacking. In this work, we\npresent RetroTrim, a retrosynthesis system that successfully avoids nonsensical\nplans on a set of challenging drug-like targets. Compared to common baselines\nin the field, our system is not only the sole method that succeeds in filtering\nout hallucinated reactions, but it also results in the highest number of\nhigh-quality paths overall. The key insight behind RetroTrim is the combination\nof diverse reaction scoring strategies, based on machine learning models and\nexisting chemical databases. We show that our scoring strategies capture\ndifferent classes of hallucinations by analyzing them on a dataset of labeled\nretrosynthetic intermediates. To measure the performance of retrosynthesis\nsystems, we propose a novel evaluation protocol for reactions and synthetic\npaths based on a structured review by expert chemists. Using this protocol, we\ncompare systems on a set of 32 novel targets, curated to reflect recent trends\nin drug structures. While the insights behind our methodology are broadly\napplicable to retrosynthesis, our focus is on targets in the drug-like domain.\nBy releasing our benchmark targets and the details of our evaluation protocol,\nwe hope to inspire further research into reliable retrosynthesis."}
{"id": "2510.10694", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10694", "abs": "https://arxiv.org/abs/2510.10694", "authors": ["Ying-Kuan Tsai", "Vispi Karkaria", "Yi-Ping Chen", "Wei Chen"], "title": "Digital Twin-enabled Multi-generation Control Co-Design with Deep Reinforcement Learning", "comment": "to be published in Journal of Mechanical Design", "summary": "Control Co-Design (CCD) integrates physical and control system design to\nimprove the performance of dynamic and autonomous systems. Despite advances in\nuncertainty-aware CCD methods, real-world uncertainties remain highly\nunpredictable. Multi-generation design addresses this challenge by considering\nthe full lifecycle of a product: data collected from each generation informs\nthe design of subsequent generations, enabling progressive improvements in\nrobustness and efficiency. Digital Twin (DT) technology further strengthens\nthis paradigm by creating virtual representations that evolve over the\nlifecycle through real-time sensing, model updating, and adaptive\nre-optimization. This paper presents a DT-enabled CCD framework that integrates\nDeep Reinforcement Learning (DRL) to jointly optimize physical design and\ncontroller. DRL accelerates real-time decision-making by allowing controllers\nto continuously learn from data and adapt to uncertain environments. Extending\nthis approach, the framework employs a multi-generation paradigm, where each\ncycle of deployment, operation, and redesign uses collected data to refine DT\nmodels, improve uncertainty quantification through quantile regression, and\ninform next-generation designs of both physical components and controllers. The\nframework is demonstrated on an active suspension system, where DT-enabled\nlearning from road conditions and driving behaviors yields smoother and more\nstable control trajectories. Results show that the method significantly\nenhances dynamic performance, robustness, and efficiency. Contributions of this\nwork include: (1) extending CCD into a lifecycle-oriented multi-generation\nframework, (2) leveraging DTs for continuous model updating and informed\ndesign, and (3) employing DRL to accelerate adaptive real-time decision-making."}
{"id": "2510.10695", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10695", "abs": "https://arxiv.org/abs/2510.10695", "authors": ["Long Chen", "Huixin Bai", "Mingxin Wang", "Xiaohua Huang", "Ying Liu", "Jie Zhao", "Ziyu Guan"], "title": "Stock Prediction via a Dual Relation Fusion Network incorporating Static and Dynamic Relations", "comment": "11 pages", "summary": "Accurate modeling of inter-stock relationships is critical for stock price\nforecasting. However, existing methods predominantly focus on single-state\nrelationships, neglecting the essential complementarity between dynamic and\nstatic inter-stock relations. To solve this problem, we propose a Dual Relation\nFusion Network (DRFN) to capture the long-term relative stability of stock\nrelation structures while retaining the flexibility to respond to sudden market\nshifts. Our approach features a novel relative static relation component that\nmodels time-varying long-term patterns and incorporates overnight informational\ninfluences. We capture dynamic inter-stock relationships through distance-aware\nmechanisms, while evolving long-term structures via recurrent fusion of dynamic\nrelations from the prior day with the pre-defined static relations. Experiments\ndemonstrate that our method significantly outperforms the baselines across\ndifferent markets, with high sensitivity to the co-movement of relational\nstrength and stock price."}
{"id": "2510.10702", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10702", "abs": "https://arxiv.org/abs/2510.10702", "authors": ["Usman Gani Joy", "Shahadat kabir", "Tasnim Niger"], "title": "Attention-Enhanced LSTM Modeling for Improved Temperature and Rainfall Forecasting in Bangladesh", "comment": null, "summary": "Accurate climate forecasting is vital for Bangladesh, a region highly\nsusceptible to climate change impacts on temperature and rainfall. Existing\nmodels often struggle to capture long-range dependencies and complex temporal\npatterns in climate data. This study introduces an advanced Long Short-Term\nMemory (LSTM) model integrated with an attention mechanism to enhance the\nprediction of temperature and rainfall dynamics. Utilizing comprehensive\ndatasets from 1901-2023, sourced from NASA's POWER Project for temperature and\nthe Humanitarian Data Exchange for rainfall, the model effectively captures\nseasonal and long-term trends. It outperforms baseline models, including\nXGBoost, Simple LSTM, and GRU, achieving a test MSE of 0.2411 (normalized\nunits), MAE of 0.3860 degrees C, R^2 of 0.9834, and NRMSE of 0.0370 for\ntemperature, and MSE of 1283.67 mm^2, MAE of 22.91 mm, R^2 of 0.9639, and NRMSE\nof 0.0354 for rainfall on monthly forecasts. The model demonstrates improved\nrobustness with only a 20 percent increase in MSE under simulated climate\ntrends (compared to an approximately 2.2-fold increase in baseline models\nwithout trend features) and a 50 percent degradation under regional variations\n(compared to an approximately 4.8-fold increase in baseline models without\nenhancements). These results highlight the model's ability to improve\nforecasting precision and offer potential insights into the physical processes\ngoverning climate variability in Bangladesh, supporting applications in\nclimate-sensitive sectors."}
{"id": "2510.10706", "categories": ["cs.LG", "cs.DM"], "pdf": "https://arxiv.org/pdf/2510.10706", "abs": "https://arxiv.org/abs/2510.10706", "authors": ["Mamoona Ghafoor", "Tatsuya Akutsu"], "title": "Designing ReLU Generative Networks to Enumerate Trees with a Given Tree Edit Distance", "comment": null, "summary": "The generation of trees with a specified tree edit distance has significant\napplications across various fields, including computational biology, structured\ndata analysis, and image processing. Recently, generative networks have been\nincreasingly employed to synthesize new data that closely resembles the\noriginal datasets. However, the appropriate size and depth of generative\nnetworks required to generate data with a specified tree edit distance remain\nunclear. In this paper, we theoretically establish the existence and\nconstruction of generative networks capable of producing trees similar to a\ngiven tree with respect to the tree edit distance. Specifically, for a given\nrooted, ordered, and vertex-labeled tree T of size n + 1 with labels from an\nalphabet \\Sigma, and a non-negative integer d, we prove that all rooted,\nordered, and vertex-labeled trees over \\Sigma with tree edit distance at most d\nfrom T can be generated using a ReLU-based generative network with size O(n^3 )\nand constant depth. The proposed networks were implemented and evaluated for\ngenerating trees with up to 21 nodes. Due to their deterministic architecture,\nthe networks successfully generated all valid trees within the specified tree\nedit distance. In contrast, state-of-the-art graph generative models GraphRNN\nand GraphGDP, which rely on non-deterministic mechanisms, produced\nsignificantly fewer valid trees, achieving validation rates of only up to 35%\nand 48%, respectively. These findings provide a theoretical foundation towards\nconstruction of compact generative models and open new directions for exact and\nvalid tree-structured data generation. An implementation of the proposed\nnetworks is available at https://github.com/MGANN-KU/TreeGen_ReLUNetworks."}
{"id": "2510.10730", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10730", "abs": "https://arxiv.org/abs/2510.10730", "authors": ["Jiazheng Sun", "Weixin Wang", "Pan Xu"], "title": "Provable Anytime Ensemble Sampling Algorithms in Nonlinear Contextual Bandits", "comment": "40 pages, 1 figure", "summary": "We provide a unified algorithmic framework for ensemble sampling in nonlinear\ncontextual bandits and develop corresponding regret bounds for two most common\nnonlinear contextual bandit settings: Generalized Linear Ensemble Sampling\n(\\texttt{GLM-ES}) for generalized linear bandits and Neural Ensemble Sampling\n(\\texttt{Neural-ES}) for neural contextual bandits. Both methods maintain\nmultiple estimators for the reward model parameters via maximum likelihood\nestimation on randomly perturbed data. We prove high-probability frequentist\nregret bounds of $\\mathcal{O}(d^{3/2} \\sqrt{T} + d^{9/2})$ for \\texttt{GLM-ES}\nand $\\mathcal{O}(\\widetilde{d} \\sqrt{T})$ for \\texttt{Neural-ES}, where $d$ is\nthe dimension of feature vectors, $\\widetilde{d}$ is the effective dimension of\na neural tangent kernel matrix, and $T$ is the number of rounds. These regret\nbounds match the state-of-the-art results of randomized exploration algorithms\nin nonlinear contextual bandit settings. In the theoretical analysis, we\nintroduce techniques that address challenges specific to nonlinear models.\nPractically, we remove fixed-time horizon assumptions by developing anytime\nversions of our algorithms, suitable when $T$ is unknown. Finally, we\nempirically evaluate \\texttt{GLM-ES}, \\texttt{Neural-ES}, and their anytime\nvariants, demonstrating strong performance. Overall, our results establish\nensemble sampling as a provable and practical randomized exploration approach\nfor nonlinear contextual bandits."}
{"id": "2510.10739", "categories": ["cs.LG", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.10739", "abs": "https://arxiv.org/abs/2510.10739", "authors": ["Shivani Shukla", "Himanshu Joshi"], "title": "A Stochastic Differential Equation Framework for Multi-Objective LLM Interactions: Dynamical Systems Analysis with Code Generation Applications", "comment": "Peer-reviewed and accepted to the 39th Conference on Neural\n  Information Processing Systems (NeurIPS 2025) DynaFront 2025 Workshop\n  (https://sites.google.com/view/dynafrontneurips25)", "summary": "We introduce a general stochastic differential equation framework for\nmodelling multiobjective optimization dynamics in iterative Large Language\nModel (LLM) interactions. Our framework captures the inherent stochasticity of\nLLM responses through explicit diffusion terms and reveals systematic\ninterference patterns between competing objectives via an interference matrix\nformulation. We validate our theoretical framework using iterative code\ngeneration as a proof-of-concept application, analyzing 400 sessions across\nsecurity, efficiency, and functionality objectives. Our results demonstrate\nstrategy-dependent convergence behaviors with rates ranging from 0.33 to 1.29,\nand predictive accuracy achieving R2 = 0.74 for balanced approaches. This work\nproposes the feasibility of dynamical systems analysis for multi-objective LLM\ninteractions, with code generation serving as an initial validation domain."}
{"id": "2510.10764", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10764", "abs": "https://arxiv.org/abs/2510.10764", "authors": ["Shaharyar Ahmed Khan Tareen", "Filza Khan Tareen"], "title": "Optimally Deep Networks -- Adapting Model Depth to Datasets for Superior Efficiency", "comment": "6 pages, 3 figures, 1 table", "summary": "Deep neural networks (DNNs) have provided brilliant performance across\nvarious tasks. However, this success often comes at the cost of unnecessarily\nlarge model sizes, high computational demands, and substantial memory\nfootprints. Typically, powerful architectures are trained at full depths but\nnot all datasets or tasks require such high model capacity. Training very deep\narchitectures on relatively low-complexity datasets frequently leads to wasted\ncomputation, unnecessary energy consumption, and excessive memory usage, which\nin turn makes deployment of models on resource-constrained devices impractical.\nTo address this problem, we introduce Optimally Deep Networks (ODNs), which\nprovide a balance between model depth and task complexity. Specifically, we\npropose a NAS like training strategy called progressive depth expansion, which\nbegins by training deep networks at shallower depths and incrementally\nincreases their depth as the earlier blocks converge, continuing this process\nuntil the target accuracy is reached. ODNs use only the optimal depth for the\ngiven datasets, removing redundant layers. This cuts down future training and\ninference costs, lowers the memory footprint, enhances computational\nefficiency, and facilitates deployment on edge devices. Empirical results show\nthat the optimal depths of ResNet-18 and ResNet-34 for MNIST and SVHN, achieve\nup to 98.64 % and 96.44 % reduction in memory footprint, while maintaining a\ncompetitive accuracy of 99.31 % and 96.08 %, respectively."}
{"id": "2510.10767", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.10767", "abs": "https://arxiv.org/abs/2510.10767", "authors": ["Jiayuan Sheng", "Hanyang Zhao", "Haoxian Chen", "David D. Yao", "Wenpin Tang"], "title": "Understanding Sampler Stochasticity in Training Diffusion Models for RLHF", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) is increasingly used to\nfine-tune diffusion models, but a key challenge arises from the mismatch\nbetween stochastic samplers used during training and deterministic samplers\nused during inference. In practice, models are fine-tuned using stochastic SDE\nsamplers to encourage exploration, while inference typically relies on\ndeterministic ODE samplers for efficiency and stability. This discrepancy\ninduces a reward gap, raising concerns about whether high-quality outputs can\nbe expected during inference. In this paper, we theoretically characterize this\nreward gap and provide non-vacuous bounds for general diffusion models, along\nwith sharper convergence rates for Variance Exploding (VE) and Variance\nPreserving (VP) Gaussian models. Methodologically, we adopt the generalized\ndenoising diffusion implicit models (gDDIM) framework to support arbitrarily\nhigh levels of stochasticity, preserving data marginals throughout.\nEmpirically, our findings through large-scale experiments on text-to-image\nmodels using denoising diffusion policy optimization (DDPO) and mixed group\nrelative policy optimization (MixGRPO) validate that reward gaps consistently\nnarrow over training, and ODE sampling quality improves when models are updated\nusing higher-stochasticity SDE training."}
{"id": "2510.10775", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10775", "abs": "https://arxiv.org/abs/2510.10775", "authors": ["Amber Li", "Aruzhan Abil", "Juno Marques Oda"], "title": "Structure Over Signal: A Globalized Approach to Multi-relational GNNs for Stock Prediction", "comment": null, "summary": "In financial markets, Graph Neural Networks have been successfully applied to\nmodeling relational data, effectively capturing nonlinear inter-stock\ndependencies. Yet, existing models often fail to efficiently propagate messages\nduring macroeconomic shocks. In this paper, we propose OmniGNN, an\nattention-based multi-relational dynamic GNN that integrates macroeconomic\ncontext via heterogeneous node and edge types for robust message passing.\nCentral to OmniGNN is a sector node acting as a global intermediary, enabling\nrapid shock propagation across the graph without relying on long-range\nmulti-hop diffusion. The model leverages Graph Attention Networks (GAT) to\nweigh neighbor contributions and employs Transformers to capture temporal\ndynamics across multiplex relations. Experiments show that OmniGNN outperforms\nexisting stock prediction models on public datasets, particularly demonstrating\nstrong robustness during the COVID-19 period."}
{"id": "2510.10777", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.10777", "abs": "https://arxiv.org/abs/2510.10777", "authors": ["Andrey Veprikov", "Arman Bolatov", "Samuel Horváth", "Aleksandr Beznosikov", "Martin Takáč", "Slavomir Hanzely"], "title": "Preconditioned Norms: A Unified Framework for Steepest Descent, Quasi-Newton and Adaptive Methods", "comment": "22 pages, 2 figures, 8 tables", "summary": "Optimization lies at the core of modern deep learning, yet existing methods\noften face a fundamental trade-off between adapting to problem geometry and\nleveraging curvature utilization. Steepest descent algorithms adapt to\ndifferent geometries through norm choices but remain strictly first-order,\nwhereas quasi-Newton and adaptive optimizers incorporate curvature information\nbut are restricted to Frobenius geometry, limiting their applicability across\ndiverse architectures. In this work, we propose a unified framework\ngeneralizing steepest descent, quasi-Newton methods, and adaptive methods\nthrough the novel notion of preconditioned matrix norms. This abstraction\nreveals that widely used optimizers such as SGD and Adam, as well as more\nadvanced approaches like Muon and KL-Shampoo, and recent hybrids including SOAP\nand SPlus, all emerge as special cases of the same principle. Within this\nframework, we provide the first systematic treatment of affine and scale\ninvariance in the matrix-parameterized setting, establishing necessary and\nsufficient conditions under generalized norms. Building on this foundation, we\nintroduce two new methods, $\\texttt{MuAdam}$ and $\\texttt{MuAdam-SANIA}$, which\ncombine the spectral geometry of Muon with Adam-style preconditioning. Our\nexperiments demonstrate that these optimizers are competitive with, and in some\ncases outperform, existing state-of-the-art methods. Our code is available at\nhttps://github.com/brain-lab-research/LIB/tree/quasi_descent"}
{"id": "2510.10790", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10790", "abs": "https://arxiv.org/abs/2510.10790", "authors": ["Zhongju Yuan", "Geraint Wiggins", "Dick Botteldooren"], "title": "BioOSS: A Bio-Inspired Oscillatory State System with Spatio-Temporal Dynamics", "comment": null, "summary": "Today's deep learning architectures are primarily based on perceptron models,\nwhich do not capture the oscillatory dynamics characteristic of biological\nneurons. Although oscillatory systems have recently gained attention for their\ncloser resemblance to neural behavior, they still fall short of modeling the\nintricate spatio-temporal interactions observed in natural neural circuits. In\nthis paper, we propose a bio-inspired oscillatory state system (BioOSS)\ndesigned to emulate the wave-like propagation dynamics critical to neural\nprocessing, particularly in the prefrontal cortex (PFC), where complex activity\npatterns emerge. BioOSS comprises two interacting populations of neurons: p\nneurons, which represent simplified membrane-potential-like units inspired by\npyramidal cells in cortical columns, and o neurons, which govern propagation\nvelocities and modulate the lateral spread of activity. Through local\ninteractions, these neurons produce wave-like propagation patterns. The model\nincorporates trainable parameters for damping and propagation speed, enabling\nflexible adaptation to task-specific spatio-temporal structures. We evaluate\nBioOSS on both synthetic and real-world tasks, demonstrating superior\nperformance and enhanced interpretability compared to alternative\narchitectures."}
{"id": "2510.10799", "categories": ["cs.LG", "physics.ao-ph", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2510.10799", "abs": "https://arxiv.org/abs/2510.10799", "authors": ["Wanshu Nie", "Sujay V. Kumar", "Junyu Chen", "Long Zhao", "Olya Skulovich", "Jinwoong Yoo", "Justin Pflug", "Shahryar Khalique Ahmad", "Goutam Konapala"], "title": "Rethinking deep learning: linear regression remains a key benchmark in predicting terrestrial water storage", "comment": null, "summary": "Recent advances in machine learning such as Long Short-Term Memory (LSTM)\nmodels and Transformers have been widely adopted in hydrological applications,\ndemonstrating impressive performance amongst deep learning models and\noutperforming physical models in various tasks. However, their superiority in\npredicting land surface states such as terrestrial water storage (TWS) that are\ndominated by many factors such as natural variability and human driven\nmodifications remains unclear. Here, using the open-access, globally\nrepresentative HydroGlobe dataset - comprising a baseline version derived\nsolely from a land surface model simulation and an advanced version\nincorporating multi-source remote sensing data assimilation - we show that\nlinear regression is a robust benchmark, outperforming the more complex LSTM\nand Temporal Fusion Transformer for TWS prediction. Our findings highlight the\nimportance of including traditional statistical models as benchmarks when\ndeveloping and evaluating deep learning models. Additionally, we emphasize the\ncritical need to establish globally representative benchmark datasets that\ncapture the combined impact of natural variability and human interventions."}
{"id": "2510.10803", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10803", "abs": "https://arxiv.org/abs/2510.10803", "authors": ["Javier García-Sigüenza", "Mirco Nanni", "Faraón Llorens-Largo", "José F. Vicent"], "title": "PruneGCRN: Minimizing and explaining spatio-temporal problems through node pruning", "comment": null, "summary": "This work addresses the challenge of using a deep learning model to prune\ngraphs and the ability of this method to integrate explainability into\nspatio-temporal problems through a new approach. Instead of applying\nexplainability to the model's behavior, we seek to gain a better understanding\nof the problem itself. To this end, we propose a novel model that integrates an\noptimized pruning mechanism capable of removing nodes from the graph during the\ntraining process, rather than doing so as a separate procedure. This\nintegration allows the architecture to learn how to minimize prediction error\nwhile selecting the most relevant nodes. Thus, during training, the model\nsearches for the most relevant subset of nodes, obtaining the most important\nelements of the problem, facilitating its analysis. To evaluate the proposed\napproach, we used several widely used traffic datasets, comparing the accuracy\nobtained by pruning with the model and with other methods. The experiments\ndemonstrate that our method is capable of retaining a greater amount of\ninformation as the graph reduces in size compared to the other methods used.\nThese results highlight the potential of pruning as a tool for developing\nmodels capable of simplifying spatio-temporal problems, thereby obtaining their\nmost important elements."}
{"id": "2510.10807", "categories": ["cs.LG", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2510.10807", "abs": "https://arxiv.org/abs/2510.10807", "authors": ["Ali Atiah Alzahrani"], "title": "Crisis-Aware Regime-Conditioned Diffusion with CVaR Allocation", "comment": "Code available at: https://github.com/AliAtiah/MARCD", "summary": "We study whether regime-conditioned generative scenarios, coupled with a\nconvex CVaR allocator, improve portfolio decisions under regime shifts. We\nintroduce Multi-Agent Regime-Conditioned Diffusion (MARCD), which (i) infers\nlatent regimes via a Gaussian HMM, (ii) trains a diffusion model with a\ntail-weighted objective and a regime-specialized mixture-of-experts (MoE)\ndenoiser to enrich crisis co-movements, and (iii) feeds the generated scenarios\ninto a turnover-aware CVaR epigraph quadratic program with explicit governance.\nIn strict walk-forward tests on liquid multi-asset ETFs (2005-2025), MARCD\noutperforms standard allocators and improves calibration relative to popular\ngenerators. Over 2020-2025 out-of-sample (monthly; 10 bps), MARCD attains\nSharpe 1.23 (BL 1.02) and MaxDD 9.3 percent (BL 14.1 percent), a 34 percent\nreduction, at comparable turnover; stationary block-bootstrap intervals\nindicate the Sharpe uplift is significant at 5 percent. We provide theory\nlinking tail-weighted diffusion to spectral-risk control of the\ndecision-relevant CVaR gap, oracle/consistency results for the regime-MoE\ndenoiser, and Lipschitz/regret guarantees for the allocator. Together, MARCD\noffers a reproducible bridge from tail-faithful scenario modeling to governed\nportfolio decisions with materially improved drawdown control."}
{"id": "2510.10810", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.10810", "abs": "https://arxiv.org/abs/2510.10810", "authors": ["Omar Islam Laskar", "Fatemeh Ramezani Khozestani", "Ishika Nankani", "Sohrab Namazi Nia", "Senjuti Basu Roy", "Kaustubh Beedkar"], "title": "Aegis: A Correlation-Based Data Masking Advisor for Data Sharing Ecosystems", "comment": "Accepted at SIGMOD 2026", "summary": "Data-sharing ecosystems enable entities -- such as providers, consumers, and\nintermediaries -- to access, exchange, and utilize data for various downstream\ntasks and applications. Due to privacy concerns, data providers typically\nanonymize datasets before sharing them; however, the existence of multiple\nmasking configurations results in masked datasets with varying utility.\nConsequently, a key challenge lies in efficiently determining the optimal\nmasking configuration that maximizes a dataset's utility. This paper presents\nAEGIS, a middleware framework for identifying the optimal masking configuration\nfor machine learning datasets that consist of features and a class label. We\nintroduce a utility optimizer that minimizes predictive utility deviation -- a\nmetric based on the changes in feature-label correlations before and after\nmasking. Our framework leverages limited data summaries (such as 1D histograms)\nor none to estimate the feature-label joint distribution, making it suitable\nfor scenarios where raw data is inaccessible due to privacy restrictions. To\nachieve this, we propose a joint distribution estimator based on iterative\nproportional fitting, which allows supporting various feature-label correlation\nquantification methods such as g3, mutual information, or chi-square. Our\nexperimental evaluation on real-world datasets shows that AEGIS identifies\noptimal masking configurations over an order of magnitude faster, while the\nresulting masked datasets achieve predictive performance on downstream ML tasks\nthat is on par with baseline approaches."}
{"id": "2510.10849", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10849", "abs": "https://arxiv.org/abs/2510.10849", "authors": ["Donald Loveland", "Yao-An Yang", "Danai Koutra"], "title": "Glance for Context: Learning When to Leverage LLMs for Node-Aware GNN-LLM Fusion", "comment": null, "summary": "Learning on text-attributed graphs has motivated the use of Large Language\nModels (LLMs) for graph learning. However, most fusion strategies are applied\nuniformly across all nodes and attain only small overall performance gains. We\nargue this result stems from aggregate metrics that obscure when LLMs provide\nbenefit, inhibiting actionable signals for new strategies. In this work, we\nreframe LLM-GNN fusion around nodes where GNNs typically falter. We first show\nthat performance can significantly differ between GNNs and LLMs, with each\nexcelling on distinct structural patterns, such as local homophily. To leverage\nthis finding, we propose GLANCE (GNN with LLM Assistance for Neighbor- and\nContext-aware Embeddings), a framework that invokes an LLM to refine a GNN's\nprediction. GLANCE employs a lightweight router that, given inexpensive\nper-node signals, decides whether to query the LLM. Since the LLM calls are\nnon-differentiable, the router is trained with an advantage-based objective\nthat compares the utility of querying the LLM against relying solely on the\nGNN. Across multiple benchmarks, GLANCE achieves the best performance balance\nacross node subgroups, achieving significant gains on heterophilous nodes (up\nto $+13\\%$) while simultaneously achieving top overall performance. Our\nfindings highlight the value of adaptive, node-aware GNN-LLM architectures,\nwhere selectively invoking the LLM enables scalable deployment on large graphs\nwithout incurring high computational costs."}
{"id": "2510.10854", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10854", "abs": "https://arxiv.org/abs/2510.10854", "authors": ["Aadithya Srikanth", "Mudit Gaur", "Vaneet Aggarwal"], "title": "Discrete State Diffusion Models: A Sample Complexity Perspective", "comment": null, "summary": "Diffusion models have demonstrated remarkable performance in generating\nhigh-dimensional samples across domains such as vision, language, and the\nsciences. Although continuous-state diffusion models have been extensively\nstudied both empirically and theoretically, discrete-state diffusion models,\nessential for applications involving text, sequences, and combinatorial\nstructures, remain significantly less understood from a theoretical standpoint.\nIn particular, all existing analyses of discrete-state models assume score\nestimation error bounds without studying sample complexity results. In this\nwork, we present a principled theoretical framework for discrete-state\ndiffusion, providing the first sample complexity bound of\n$\\widetilde{\\mathcal{O}}(\\epsilon^{-2})$. Our structured decomposition of the\nscore estimation error into statistical, approximation, optimization, and\nclipping components offers critical insights into how discrete-state models can\nbe trained efficiently. This analysis addresses a fundamental gap in the\nliterature and establishes the theoretical tractability and practical relevance\nof discrete-state diffusion models."}
{"id": "2510.10862", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.10862", "abs": "https://arxiv.org/abs/2510.10862", "authors": ["Samuel Yuan", "Divyanshu Saxena", "Jiayi Chen", "Nihal Sharma", "Aditya Akella"], "title": "A Joint Learning Approach to Hardware Caching and Prefetching", "comment": "Accepted at ML for Systems Workshop at the 39th Conference on Neural\n  Information Processing Systems (NeurIPS 2025)", "summary": "Several learned policies have been proposed to replace heuristics for\nscheduling, caching, and other system components in modern systems. By\nleveraging diverse features, learning from historical trends, and predicting\nfuture behaviors, such models promise to keep pace with ever-increasing\nworkload dynamism and continuous hardware evolution. However, policies trained\nin isolation may still achieve suboptimal performance when placed together. In\nthis paper, we inspect one such instance in the domain of hardware caching --\nfor the policies of cache replacement and prefetching. We argue that these two\npolicies are bidirectionally interdependent and make the case for training the\ntwo jointly. We propose a joint learning approach based on developing shared\nrepresentations for the features used by the two policies. We present two\napproaches to develop these shared representations, one based on a joint\nencoder and another based on contrastive learning of the embeddings, and\ndemonstrate promising preliminary results for both of these. Finally, we lay\ndown an agenda for future research in this direction."}
{"id": "2510.10864", "categories": ["cs.LG", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.10864", "abs": "https://arxiv.org/abs/2510.10864", "authors": ["Shuaicheng Zhang", "Haohui Wang", "Junhong Lin", "Xiaojie Guo", "Yada Zhu", "Si Zhang", "Dongqi Fu", "Dawei Zhou"], "title": "HeroFilter: Adaptive Spectral Graph Filter for Varying Heterophilic Relations", "comment": null, "summary": "Graph heterophily, where connected nodes have different labels, has attracted\nsignificant interest recently. Most existing works adopt a simplified approach\n- using low-pass filters for homophilic graphs and high-pass filters for\nheterophilic graphs. However, we discover that the relationship between graph\nheterophily and spectral filters is more complex - the optimal filter response\nvaries across frequency components and does not follow a strict monotonic\ncorrelation with heterophily degree. This finding challenges conventional fixed\nfilter designs and suggests the need for adaptive filtering to preserve\nexpressiveness in graph embeddings. Formally, natural questions arise: Given a\nheterophilic graph G, how and to what extent will the varying heterophily\ndegree of G affect the performance of GNNs? How can we design adaptive filters\nto fit those varying heterophilic connections? Our theoretical analysis reveals\nthat the average frequency response of GNNs and graph heterophily degree do not\nfollow a strict monotonic correlation, necessitating adaptive graph filters to\nguarantee good generalization performance. Hence, we propose [METHOD NAME], a\nsimple yet powerful GNN, which extracts information across the heterophily\nspectrum and combines salient representations through adaptive mixing. [METHOD\nNAME]'s superior performance achieves up to 9.2% accuracy improvement over\nleading baselines across homophilic and heterophilic graphs."}
{"id": "2510.10902", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10902", "abs": "https://arxiv.org/abs/2510.10902", "authors": ["Mahmoud Abdelghafar", "Maryam Aliakbarpour", "Chris Jermaine"], "title": "Quantifying Information Disclosure During Gradient Descent Using Gradient Uniqueness", "comment": null, "summary": "Disclosing private information via publication of a machine learning model is\noften a concern. Intuitively, publishing a learned model should be less risky\nthan publishing a dataset. But how much risk is there? In this paper, we\npresent a principled disclosure metric called \\emph{gradient uniqueness} that\nis derived from an upper bound on the amount of information disclosure from\npublishing a learned model. Gradient uniqueness provides an intuitive way to\nperform privacy auditing. The mathematical derivation of gradient uniqueness is\ngeneral, and does not make any assumption on the model architecture, dataset\ntype, or the strategy of an attacker. We examine a simple defense based on\nmonitoring gradient uniqueness, and find that it achieves privacy comparable to\nclassical methods such as DP-SGD, while being substantially better in terms of\n(utility) testing accuracy."}
{"id": "2510.10915", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10915", "abs": "https://arxiv.org/abs/2510.10915", "authors": ["Hanchang Cheng", "Weimin Mu", "Fan Liu", "Weilin Zhu", "Can Ma"], "title": "LPCVAE: A Conditional VAE with Long-Term Dependency and Probabilistic Time-Frequency Fusion for Time Series Anomaly Detection", "comment": null, "summary": "Time series anomaly detection(TSAD) is a critical task in signal processing\nfield, ensuring the reliability of complex systems. Reconstruction-based\nmethods dominate in TSAD. Among these methods, VAE-based methods have achieved\npromising results. Existing VAE-based methods suffer from the limitation of\nsingle-window feature and insufficient leveraging of long-term time and\nfrequency information. We propose a Conditional Variational AutoEncoder with\nLong-term dependency and Probabilistic time-frequency fusion, named LPCVAE.\nLPCVAE introduces LSTM to capture long-term dependencies beyond windows. It\nfurther incorporates a Product-of-Experts (PoE) mechanism for adaptive and\ndistribution-level probabilistic fusion. This design effectively mitigates\ntime-frequency information loss. Extensive experiments on four public datasets\ndemonstrate it outperforms state-of-the-art methods. The results confirm that\nintegrating long-term time and frequency representations with adaptive fusion\nyields a robust and efficient solution for TSAD."}
{"id": "2510.10925", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10925", "abs": "https://arxiv.org/abs/2510.10925", "authors": ["Hengyuan Zhang", "Shiping Yang", "Xiao Liang", "Chenming Shang", "Yuxuan Jiang", "Chaofan Tao", "Jing Xiong", "Hayden Kwok-Hay So", "Ruobing Xie", "Angel X. Chang", "Ngai Wong"], "title": "Find Your Optimal Teacher: Personalized Data Synthesis via Router-Guided Multi-Teacher Distillation", "comment": "19 pages, 10 figures", "summary": "Training student models on synthetic data generated by strong teacher models\nis a promising way to distilling the capabilities of teachers. However, recent\nstudies show that stronger models are not always optimal teachers, revealing a\nmismatch between teacher outputs and student learnability. To address this\nissue, we propose PerSyn (Personalized data Synthesis), a novel synthesis\nstrategy that operates under a new ``Route then Generate'' paradigm to create\ndata tailored to each student model, enabling it to learn more effectively.\nSpecifically, PerSyn first assigns each prompt to its optimal teacher via a\nquery-level router that jointly considers student learnability and teacher\nresponse quality. Each teacher then synthesizes data only for its assigned\nprompts, making the process more efficient than the conventional ``Generate\nthen Select'' paradigm, where all teachers must generate parallel responses for\nthe entire prompt set before constructing the final dataset. Extensive\nexperiments across different model families and scales demonstrate that PerSyn\nconsistently achieves superior or comparable performance to all baselines in\ninstruct tuning and math reasoning settings. Further analysis verifies the\neffectiveness of PerSyn and offers extra insights to propel future research."}
{"id": "2510.10937", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.10937", "abs": "https://arxiv.org/abs/2510.10937", "authors": ["Qizhou Peng", "Yang Zheng", "Yu Wen", "Yanna Wu", "Yingying Du"], "title": "Neutral Agent-based Adversarial Policy Learning against Deep Reinforcement Learning in Multi-party Open Systems", "comment": null, "summary": "Reinforcement learning (RL) has been an important machine learning paradigm\nfor solving long-horizon sequential decision-making problems under uncertainty.\nBy integrating deep neural networks (DNNs) into the RL framework, deep\nreinforcement learning (DRL) has emerged, which achieved significant success in\nvarious domains. However, the integration of DNNs also makes it vulnerable to\nadversarial attacks. Existing adversarial attack techniques mainly focus on\neither directly manipulating the environment with which a victim agent\ninteracts or deploying an adversarial agent that interacts with the victim\nagent to induce abnormal behaviors. While these techniques achieve promising\nresults, their adoption in multi-party open systems remains limited due to two\nmajor reasons: impractical assumption of full control over the environment and\ndependent on interactions with victim agents.\n  To enable adversarial attacks in multi-party open systems, in this paper, we\nredesigned an adversarial policy learning approach that can mislead\nwell-trained victim agents without requiring direct interactions with these\nagents or full control over their environments. Particularly, we propose a\nneutral agent-based approach across various task scenarios in multi-party open\nsystems. While the neutral agents seemingly are detached from the victim\nagents, indirectly influence them through the shared environment. We evaluate\nour proposed method on the SMAC platform based on Starcraft II and the\nautonomous driving simulation platform Highway-env. The experimental results\ndemonstrate that our method can launch general and effective adversarial\nattacks in multi-party open systems."}
{"id": "2510.10938", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10938", "abs": "https://arxiv.org/abs/2510.10938", "authors": ["Yuda Bi", "Ying Zhu", "Vince D Calhoun"], "title": "Redundancy as a Structural Information Principle for Learning and Generalization", "comment": null, "summary": "We present a theoretical framework that extends classical information theory\nto finite and structured systems by redefining redundancy as a fundamental\nproperty of information organization rather than inefficiency. In this\nframework, redundancy is expressed as a general family of informational\ndivergences that unifies multiple classical measures, such as mutual\ninformation, chi-squared dependence, and spectral redundancy, under a single\ngeometric principle. This reveals that these traditional quantities are not\nisolated heuristics but projections of a shared redundancy geometry. The theory\nfurther predicts that redundancy is bounded both above and below, giving rise\nto an optimal equilibrium that balances over-compression (loss of structure)\nand over-coupling (collapse). While classical communication theory favors\nminimal redundancy for transmission efficiency, finite and structured systems,\nsuch as those underlying real-world learning, achieve maximal stability and\ngeneralization near this equilibrium. Experiments with masked autoencoders are\nused to illustrate and verify this principle: the model exhibits a stable\nredundancy level where generalization peaks. Together, these results establish\nredundancy as a measurable and tunable quantity that bridges the asymptotic\nworld of communication and the finite world of learning."}
{"id": "2510.10952", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.10952", "abs": "https://arxiv.org/abs/2510.10952", "authors": ["Xi Mao", "Zhendong Wang", "Jingyu Li", "Lingchao Mao", "Utibe Essien", "Hairong Wang", "Xuelei Sherry Ni"], "title": "Interpretable Machine Learning for Cognitive Aging: Handling Missing Data and Uncovering Social Determinant", "comment": null, "summary": "Early detection of Alzheimer's disease (AD) is crucial because its\nneurodegenerative effects are irreversible, and neuropathologic and\nsocial-behavioral risk factors accumulate years before diagnosis. Identifying\nhigher-risk individuals earlier enables prevention, timely care, and equitable\nresource allocation. We predict cognitive performance from social determinants\nof health (SDOH) using the NIH NIA-supported PREPARE Challenge Phase 2 dataset\nderived from the nationally representative Mex-Cog cohort of the 2003 and 2012\nMexican Health and Aging Study (MHAS).\n  Data: The target is a validated composite cognitive score across seven\ndomains-orientation, memory, attention, language, constructional praxis, and\nexecutive function-derived from the 2016 and 2021 MHAS waves. Predictors span\ndemographic, socioeconomic, health, lifestyle, psychosocial, and healthcare\naccess factors.\n  Methodology: Missingness was addressed with a singular value decomposition\n(SVD)-based imputation pipeline treating continuous and categorical variables\nseparately. This approach leverages latent feature correlations to recover\nmissing values while balancing reliability and scalability. After evaluating\nmultiple methods, XGBoost was chosen for its superior predictive performance.\n  Results and Discussion: The framework outperformed existing methods and the\ndata challenge leaderboard, demonstrating high accuracy, robustness, and\ninterpretability. SHAP-based post hoc analysis identified top contributing SDOH\nfactors and age-specific feature patterns. Notably, flooring material emerged\nas a strong predictor, reflecting socioeconomic and environmental disparities.\nOther influential factors, age, SES, lifestyle, social interaction, sleep,\nstress, and BMI, underscore the multifactorial nature of cognitive aging and\nthe value of interpretable, data-driven SDOH modeling."}
{"id": "2510.10959", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10959", "abs": "https://arxiv.org/abs/2510.10959", "authors": ["Xiaoyun Zhang", "Xiaojian Yuan", "Di Huang", "Wang You", "Chen Hu", "Jingqing Ruan", "Kejiang Chen", "Xing Hu"], "title": "Rediscovering Entropy Regularization: Adaptive Coefficient Unlocks Its Potential for LLM Reinforcement Learning", "comment": "16 pages, 4 figures", "summary": "Reasoning ability has become a defining capability of Large Language Models\n(LLMs), with Reinforcement Learning with Verifiable Rewards (RLVR) emerging as\na key paradigm to enhance it. However, RLVR training often suffers from policy\nentropy collapse, where the policy becomes overly deterministic, hindering\nexploration and limiting reasoning performance. While entropy regularization is\na common remedy, its effectiveness is highly sensitive to the fixed\ncoefficient, making it unstable across tasks and models. In this work, we\nrevisit entropy regularization in RLVR and argue that its potential has been\nlargely underestimated. Our analysis shows that (i) tasks of varying difficulty\ndemand distinct exploration intensities, and (ii) balanced exploration may\nrequire the policy entropy to be maintained within a moderate range below its\ninitial level. Therefore, we propose Adaptive Entropy Regularization (AER)--a\nframework that dynamically balances exploration and exploitation via three\ncomponents: difficulty-aware coefficient allocation, initial-anchored target\nentropy, and dynamic global coefficient adjustment. Experiments on multiple\nmathematical reasoning benchmarks show that AER consistently outperforms\nbaselines, improving both reasoning accuracy and exploration capability."}
{"id": "2510.10962", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10962", "abs": "https://arxiv.org/abs/2510.10962", "authors": ["Wei Huang", "Yue Liao", "Yukang Chen", "Jianhui Liu", "Haoru Tan", "Si Liu", "Shiming Zhang", "Shuicheng Yan", "Xiaojuan Qi"], "title": "MC#: Mixture Compressor for Mixture-of-Experts Large Models", "comment": "15 pages, 13 figures", "summary": "Mixture-of-Experts (MoE) effectively scales large language models (LLMs) and\nvision-language models (VLMs) by increasing capacity through sparse activation.\nHowever, preloading all experts into memory and activating multiple experts per\ninput introduces significant computational and memory overhead, making the\nexpert module a major contributor to model size and inference cost. To address\nthis, we propose MC# (Mixture-Compressor-sharp), a framework that combines\nstatic quantization and dynamic expert pruning by leveraging the significance\nof experts and tokens for aggressive compression of MoE-LLMs/VLMs. To reduce\nstorage and loading costs, we introduce Pre-Loading Mixed-Precision\nQuantization (PMQ), which optimizes bit allocation via linear programming,\nbalancing expert importance and quantization error for a Pareto-optimal\ntrade-off between size and performance. To reduce runtime computation, Online\nTop-any Pruning (OTP) uses Gumbel-Softmax sampling to dynamically select a\nsubset of experts per token, enabling fine-grained control over activation. By\ncombining PMQ's static bit-width optimization with OTP's dynamic routing, MC#\nachieves extreme compression with minimal accuracy loss. On DeepSeek-VL2, MC#\nachieves a 6.2 times weight reduction at 2.57 average bits with only a 1.7%\naccuracy drop across five multimodal benchmarks. Additionally, OTP reduces\nexpert activation over 20% with less than 1% performance degradation,\ndemonstrating strong potential for efficient MoE-based model deployment."}
{"id": "2510.10963", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10963", "abs": "https://arxiv.org/abs/2510.10963", "authors": ["Zhuo Li", "Yuege Feng", "Dandan Guo", "Jinpeng Hu", "Anningzhe Gao", "Xiang Wan"], "title": "APLOT: Robust Reward Modeling via Adaptive Preference Learning with Optimal Transport", "comment": "EMNLP2025", "summary": "The reward model (RM) plays a crucial role in aligning Large Language Models\n(LLMs) with human preferences through Reinforcement Learning, where the\nBradley-Terry (BT) objective has been recognized as simple yet powerful,\nspecifically for pairwise preference learning. However, BT-based RMs often\nstruggle to effectively distinguish between similar preference responses,\nleading to insufficient separation between preferred and non-preferred outputs.\nConsequently, they may easily overfit easy samples and cannot generalize well\nto Out-Of-Distribution (OOD) samples, resulting in suboptimal performance. To\naddress these challenges, this paper introduces an effective enhancement to\nBT-based RMs through an adaptive margin mechanism. Specifically, we design to\ndynamically adjust the RM focus on more challenging samples through margins,\nbased on both semantic similarity and model-predicted reward differences, which\nis approached from a distributional perspective solvable with Optimal Transport\n(OT). By incorporating these factors into a principled OT cost matrix design,\nour adaptive margin enables the RM to better capture distributional differences\nbetween chosen and rejected responses, yielding significant improvements in\nperformance, convergence speed, and generalization capabilities. Experimental\nresults across multiple benchmarks demonstrate that our method outperforms\nseveral existing RM techniques, showcasing enhanced performance in both\nIn-Distribution (ID) and OOD settings. Moreover, RLHF experiments support our\npractical effectiveness in better aligning LLMs with human preferences. Our\ncode is available at https://github.com/BIRlz/APLOT"}
{"id": "2510.10964", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10964", "abs": "https://arxiv.org/abs/2510.10964", "authors": ["Junhyuck Kim", "Ethan Ewer", "Taehong Moon", "Jongho Park", "Dimitris Papailiopoulos"], "title": "Not All Bits Are Equal: Scale-Dependent Memory Optimization Strategies for Reasoning Models", "comment": "20 pages, 12 figures", "summary": "While 4-bit quantization has emerged as a memory-optimal choice for\nnon-reasoning models and zero-shot tasks across scales, we show that this\nuniversal prescription fails for reasoning models, where the KV cache rather\nthan model size can dominate memory. Through systematic experiments across\n1,700 inference scenarios on AIME25 and GPQA-Diamond, we find a scale-dependent\ntrade-off: models with an effective size below 8-bit 4B parameters achieve\nbetter accuracy by allocating memory to more weights rather than longer\ngeneration, while larger models achieve better accuracy by allocating memory to\nlonger generations. This scale threshold also determines when parallel scaling\nbecomes memory-efficient and whether KV cache eviction outperforms KV\nquantization. Our findings show that memory optimization for LLMs cannot be\nscale-agnostic, while providing principled guidelines: for small reasoning\nmodels, prioritize model capacity over test-time compute, while for larger\nones, maximize test-time compute. Our results suggest that optimizing reasoning\nmodels for deployment requires fundamentally different strategies from those\nestablished for non-reasoning models."}
{"id": "2510.10968", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10968", "abs": "https://arxiv.org/abs/2510.10968", "authors": ["Hongkai Zheng", "Austin Wang", "Zihui Wu", "Zhengyu Huang", "Ricardo Baptista", "Yisong Yue"], "title": "Blade: A Derivative-free Bayesian Inversion Method using Diffusion Priors", "comment": null, "summary": "Derivative-free Bayesian inversion is an important task in many science and\nengineering applications, particularly when computing the forward model\nderivative is computationally and practically challenging. In this paper, we\nintroduce Blade, which can produce accurate and well-calibrated posteriors for\nBayesian inversion using an ensemble of interacting particles. Blade leverages\npowerful data-driven priors based on diffusion models, and can handle nonlinear\nforward models that permit only black-box access (i.e., derivative-free).\nTheoretically, we establish a non-asymptotic convergence analysis to\ncharacterize the effects of forward model and prior estimation errors.\nEmpirically, Blade achieves superior performance compared to existing\nderivative-free Bayesian inversion methods on various inverse problems,\nincluding challenging highly nonlinear fluid dynamics."}
{"id": "2510.10980", "categories": ["cs.LG", "cs.CV", "cs.IT", "math.IT", "math.ST", "stat.ML", "stat.TH", "68T07, 62B11, 94A17, 53B12", "I.2.6; I.5.1; G.3; H.1.1"], "pdf": "https://arxiv.org/pdf/2510.10980", "abs": "https://arxiv.org/abs/2510.10980", "authors": ["Di Zhang"], "title": "On the Optimal Representation Efficiency of Barlow Twins: An Information-Geometric Interpretation", "comment": "7 pages", "summary": "Self-supervised learning (SSL) has achieved remarkable success by learning\nmeaningful representations without labeled data. However, a unified theoretical\nframework for understanding and comparing the efficiency of different SSL\nparadigms remains elusive. In this paper, we introduce a novel\ninformation-geometric framework to quantify representation efficiency. We\ndefine representation efficiency $\\eta$ as the ratio between the effective\nintrinsic dimension of the learned representation space and its ambient\ndimension, where the effective dimension is derived from the spectral\nproperties of the Fisher Information Matrix (FIM) on the statistical manifold\ninduced by the encoder. Within this framework, we present a theoretical\nanalysis of the Barlow Twins method. Under specific but natural assumptions, we\nprove that Barlow Twins achieves optimal representation efficiency ($\\eta = 1$)\nby driving the cross-correlation matrix of representations towards the identity\nmatrix, which in turn induces an isotropic FIM. This work provides a rigorous\ntheoretical foundation for understanding the effectiveness of Barlow Twins and\noffers a new geometric perspective for analyzing SSL algorithms."}
{"id": "2510.10982", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10982", "abs": "https://arxiv.org/abs/2510.10982", "authors": ["Zihan Wang", "Zhiyong Ma", "Zhongkui Ma", "Shuofeng Liu", "Akide Liu", "Derui Wang", "Minhui Xue", "Guangdong Bai"], "title": "Catch-Only-One: Non-Transferable Examples for Model-Specific Authorization", "comment": null, "summary": "Recent AI regulations call for data that remain useful for innovation while\nresistant to misuse, balancing utility with protection at the model level.\nExisting approaches either perturb data to make it unlearnable or retrain\nmodels to suppress transfer, but neither governs inference by unknown models,\nand both typically require control over training. We propose non-transferable\nexamples (NEs), a training-free and data-agnostic input-side usage-control\nmechanism. We recode inputs within a model-specific low-sensitivity subspace,\npreserving outputs for the authorized model while reducing performance on\nunauthorized models through subspace misalignment. We establish formal bounds\nthat guarantee utility for the authorized model and quantify deviation for\nunauthorized ones, with the Hoffman-Wielandt inequality linking degradation to\nspectral differences. Empirically, NEs retain performance on diverse vision\nbackbones and state-of-the-art vision-language models under common\npreprocessing, whereas non-target models collapse even with reconstruction\nattempts. These results establish NEs as a practical means to preserve intended\ndata utility while preventing unauthorized exploitation. Our project is\navailable at https://trusted-system-lab.github.io/model-specificity"}
{"id": "2510.11016", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11016", "abs": "https://arxiv.org/abs/2510.11016", "authors": ["Ziyi Gao", "Yike Xu", "Jiahao Yuan", "Baokun Wang", "Jinyong Wen", "Xiaotong Lin", "Yun Liu", "Xing Fu", "Yu Cheng", "Yongchao Liu", "Weiqiang Wang", "Zhongle Xie"], "title": "Instruction-aware User Embedding via Synergistic Language and Representation Modeling", "comment": null, "summary": "User representation modeling has become increasingly crucial for personalized\napplications, yet existing approaches struggle with generalizability across\ndomains and sensitivity to noisy behavioral signals. We present InstructUE, an\ninstruction-aware user embedding foundation model that leverages large language\nmodels (LLMs) to generate general and instruction-aware user representations.\nInstructUE introduces a multi-encoder architecture with a lightweight adapter\nthat efficiently processes heterogeneous data from six different sources while\npreserving their structural characteristics. Additionally, it proposes a novel\ncontrastive-autoregressive training framework that bridges language and\nrepresentation spaces through a curated UserQA dataset. The\ncontrastive-autoregressive training framework simultaneously leverages\nautoregressive learning to capture domain knowledge in language space and\ncontrastive learning to align user-text embeddings in representation space,\nthereby enhancing the instruction-awareness and noise-robustness of user\nembeddings. Through extensive experiments on real-world applications, we\ndemonstrate that InstructUE significantly outperforms existing methods across\nmultiple domains including user prediction, marketing, and recommendation\nscenarios. Our results show that instruction-aware user modeling can\neffectively achieve instruction-guided denoising of user information in\nspecific scenarios, paving the way for more generalizable and robust user\nrepresentation learning."}
{"id": "2510.11018", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11018", "abs": "https://arxiv.org/abs/2510.11018", "authors": ["Pranav Ramesh", "Arjun Roy", "Deepak Ravikumar", "Kaushik Roy", "Gopalakrishnan Srinivasan"], "title": "The Easy Path to Robustness: Coreset Selection using Sample Hardness", "comment": null, "summary": "Designing adversarially robust models from a data-centric perspective\nrequires understanding which input samples are most crucial for learning\nresilient features. While coreset selection provides a mechanism for efficient\ntraining on data subsets, current algorithms are designed for clean accuracy\nand fall short in preserving robustness. To address this, we propose a\nframework linking a sample's adversarial vulnerability to its\n\\textit{hardness}, which we quantify using the average input gradient norm\n(AIGN) over training. We demonstrate that \\textit{easy} samples (with low AIGN)\nare less vulnerable and occupy regions further from the decision boundary.\nLeveraging this insight, we present EasyCore, a coreset selection algorithm\nthat retains only the samples with low AIGN for training. We empirically show\nthat models trained on EasyCore-selected data achieve significantly higher\nadversarial accuracy than those trained with competing coreset methods under\nboth standard and adversarial training. As AIGN is a model-agnostic dataset\nproperty, EasyCore is an efficient and widely applicable data-centric method\nfor improving adversarial robustness. We show that EasyCore achieves up to 7\\%\nand 5\\% improvement in adversarial accuracy under standard training and TRADES\nadversarial training, respectively, compared to existing coreset methods."}
{"id": "2510.11049", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11049", "abs": "https://arxiv.org/abs/2510.11049", "authors": ["Sonakshi Dua", "Gonzalo Mateos", "Sundeep Prabhakar Chepuri"], "title": "Conformal Inference for Time Series over Graphs", "comment": null, "summary": "Trustworthy decision making in networked, dynamic environments calls for\ninnovative uncertainty quantification substrates in predictive models for graph\ntime series. Existing conformal prediction (CP) methods have been applied\nseparately to multivariate time series and static graphs, but they either\nignore the underlying graph topology or neglect temporal dynamics. To bridge\nthis gap, here we develop a CP-based sequential prediction region framework\ntailored for graph time series. A key technical innovation is to leverage the\ngraph structure and thus capture pairwise dependencies across nodes, while\nproviding user-specified coverage guarantees on the predictive outcomes. We\nformally establish that our scheme yields an exponential shrinkage in the\nvolume of the ellipsoidal prediction set relative to its graph-agnostic\ncounterpart. Using real-world datasets, we demonstrate that the novel\nuncertainty quantification framework maintains desired empirical coverage while\nachieving markedly smaller (up to 80% reduction) prediction regions than\nexisting approaches."}
{"id": "2510.11057", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11057", "abs": "https://arxiv.org/abs/2510.11057", "authors": ["Youngrok Park", "Hojung Jung", "Sangmin Bae", "Se-Young Yun"], "title": "Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models", "comment": "54 pages, 17 figures, 18 tables", "summary": "Diffusion models have achieved remarkable success as generative models.\nHowever, even a well-trained model can accumulate errors throughout the\ngeneration process. These errors become particularly problematic when arbitrary\nguidance is applied to steer samples toward desired properties, which often\nbreaks sample fidelity. In this paper, we propose a general solution to address\nthe off-manifold phenomenon observed in diffusion models. Our approach\nleverages a time predictor to estimate deviations from the desired data\nmanifold at each timestep, identifying that a larger time gap is associated\nwith reduced generation quality. We then design a novel guidance mechanism,\n`Temporal Alignment Guidance' (TAG), attracting the samples back to the desired\nmanifold at every timestep during generation. Through extensive experiments, we\ndemonstrate that TAG consistently produces samples closely aligned with the\ndesired manifold at each timestep, leading to significant improvements in\ngeneration quality across various downstream tasks."}
{"id": "2510.11058", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11058", "abs": "https://arxiv.org/abs/2510.11058", "authors": ["I Chiu", "Yu-Tung Liu", "Kuan-Chen Wang", "Hung-Yu Wei", "Yu Tsao"], "title": "Robust Photoplethysmography Signal Denoising via Mamba Networks", "comment": "5 pages, 2 figures", "summary": "Photoplethysmography (PPG) is widely used in wearable health monitoring, but\nits reliability is often degraded by noise and motion artifacts, limiting\ndownstream applications such as heart rate (HR) estimation. This paper presents\na deep learning framework for PPG denoising with an emphasis on preserving\nphysiological information. In this framework, we propose DPNet, a Mamba-based\ndenoising backbone designed for effective temporal modeling. To further enhance\ndenoising performance, the framework also incorporates a scale-invariant\nsignal-to-distortion ratio (SI-SDR) loss to promote waveform fidelity and an\nauxiliary HR predictor (HRP) that provides physiological consistency through\nHR-based supervision. Experiments on the BIDMC dataset show that our method\nachieves strong robustness against both synthetic noise and real-world motion\nartifacts, outperforming conventional filtering and existing neural models. Our\nmethod can effectively restore PPG signals while maintaining HR accuracy,\nhighlighting the complementary roles of SI-SDR loss and HR-guided supervision.\nThese results demonstrate the potential of our approach for practical\ndeployment in wearable healthcare systems."}
{"id": "2510.11062", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.11062", "abs": "https://arxiv.org/abs/2510.11062", "authors": ["Yujie Zhao", "Lanxiang Hu", "Yang Wang", "Minmin Hou", "Hao Zhang", "Ke Ding", "Jishen Zhao"], "title": "Stronger Together: On-Policy Reinforcement Learning for Collaborative LLMs", "comment": null, "summary": "Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to\nenhance the agentic capabilities of large language models (LLMs). MAS improves\ntask performance through role-based orchestration, while RL uses environmental\nrewards to learn stronger policies, such as GRPO-style optimization. However,\napplying on-policy RL to MAS remains underexplored and presents unique\nchallenges. Algorithmically, standard GRPO grouping assumptions break down\nbecause prompts vary by role and by turn. System-wise, the training stack must\nsupport MAS-workflow rollouts and on-policy updates for both single-policy and\nmulti-policy models.\n  We propose AT-GRPO, which includes (i) an agent- and turn-wise grouped RL\nalgorithm tailored to MAS and (ii) a training system that supports both single-\nand multi-policy regimes. Across game, planning, coding, and math tasks,\nAT-GRPO delivers substantial gains. On long-horizon planning, it increases\naccuracy from a 14.0 to 47.0 percent single-agent RL baseline to 96.0 to 99.5\npercent. It also improves reasoning performance, with average gains of 3.87 to\n7.62 percent on coding tasks and 9.0 to 17.93 percent on math. Code and\nenvironments are available at: https://github.com/pettingllms-ai/PettingLLMs."}
{"id": "2510.11068", "categories": ["cs.LG", "eess.AS", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.11068", "abs": "https://arxiv.org/abs/2510.11068", "authors": ["Xinyu Luo", "Jie Liu", "Kecheng Chen", "Junyi Yang", "Bo Ding", "Arindam Basu", "Haoliang Li"], "title": "Efficient Edge Test-Time Adaptation via Latent Feature Coordinate Correction", "comment": "Under review", "summary": "Edge devices face significant challenges due to limited computational\nresources and distribution shifts, making efficient and adaptable machine\nlearning essential. Existing test-time adaptation (TTA) methods often rely on\ngradient-based optimization or batch processing, which are inherently\nunsuitable for resource-constrained edge scenarios due to their reliance on\nbackpropagation and high computational demands. Gradient-free alternatives\naddress these issues but often suffer from limited learning capacity, lack\nflexibility, or impose architectural constraints. To overcome these\nlimitations, we propose a novel single-instance TTA method tailored for edge\ndevices (TED), which employs forward-only coordinate optimization in the\nprincipal subspace of latent using the covariance matrix adaptation evolution\nstrategy (CMA-ES). By updating a compact low-dimensional vector, TED not only\nenhances output confidence but also aligns the latent representation closer to\nthe source latent distribution within the latent principal subspace. This is\nachieved without backpropagation, keeping the model parameters frozen, and\nenabling efficient, forgetting-free adaptation with minimal memory and\ncomputational overhead. Experiments on image classification and keyword\nspotting tasks across the ImageNet and Google Speech Commands series datasets\ndemonstrate that TED achieves state-of-the-art performance while\n$\\textit{reducing computational complexity by up to 63 times}$, offering a\npractical and scalable solution for real-world edge applications. Furthermore,\nwe successfully $\\textit{deployed TED on the ZYNQ-7020 platform}$,\ndemonstrating its feasibility and effectiveness for resource-constrained edge\ndevices in real-world deployments."}
{"id": "2510.11084", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11084", "abs": "https://arxiv.org/abs/2510.11084", "authors": ["Wonah Kim", "Jeonghyeon Park", "Dongsan Jun", "Jungkyu Han", "Sejin Chun"], "title": "Causal Disentanglement Learning for Accurate Anomaly Detection in Multivariate Time Series", "comment": "20 pages, 4 Figures,", "summary": "Disentangling complex causal relationships is important for accurate\ndetection of anomalies. In multivariate time series analysis, dynamic\ninteractions among data variables over time complicate the interpretation of\ncausal relationships. Traditional approaches assume statistical independence\nbetween variables in unsupervised settings, whereas recent methods capture\nfeature correlations through graph representation learning. However, their\nrepresentations fail to explicitly infer the causal relationships over\ndifferent time periods. To solve the problem, we propose Causally Disentangled\nRepresentation Learning for Anomaly Detection (CDRL4AD) to detect anomalies and\nidentify their causal relationships in multivariate time series. First, we\ndesign the causal process as model input, the temporal heterogeneous graph, and\ncausal relationships. Second, our representation identifies causal\nrelationships over different time periods and disentangles latent variables to\ninfer the corresponding causal factors. Third, our experiments on real-world\ndatasets demonstrate that CDRL4AD outperforms state-of-the-art methods in terms\nof accuracy and root cause analysis. Fourth, our model analysis validates\nhyperparameter sensitivity and the time complexity of CDRL4AD. Last, we conduct\na case study to show how our approach assists human experts in diagnosing the\nroot causes of anomalies."}
{"id": "2510.11110", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11110", "abs": "https://arxiv.org/abs/2510.11110", "authors": ["Cheol-Hui Lee", "Hwa-Yeon Lee", "Min-Kyung Jung", "Dong-Joo Kim"], "title": "PhysioME: A Robust Multimodal Self-Supervised Framework for Physiological Signals with Missing Modalities", "comment": "9 pages, 2 figures", "summary": "Missing or corrupted modalities are common in physiological signal-based\nmedical applications owing to hardware constraints or motion artifacts.\nHowever, most existing methods assume the availability of all modalities,\nresulting in substantial performance degradation in the absence of any\nmodality. To overcome this limitation, this study proposes PhysioME, a robust\nframework designed to ensure reliable performance under missing modality\nconditions. PhysioME adopts: (1) a multimodal self-supervised learning approach\nthat combines contrastive learning with masked prediction; (2) a\nDual-PathNeuroNet backbone tailored to capture the temporal dynamics of each\nphysiological signal modality; and (3) a restoration decoder that reconstructs\nmissing modality tokens, enabling flexible processing of incomplete inputs. The\nexperimental results show that PhysioME achieves high consistency and\ngeneralization performance across various missing modality scenarios. These\nfindings highlight the potential of PhysioME as a reliable tool for supporting\nclinical decision-making in real-world settings with imperfect data\navailability."}
{"id": "2510.11121", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11121", "abs": "https://arxiv.org/abs/2510.11121", "authors": ["Rongjie Zhu", "Cong Zhang", "Zhiguang Cao"], "title": "Refining Hybrid Genetic Search for CVRP via Reinforcement Learning-Finetuned LLM", "comment": null, "summary": "While large language models (LLMs) are increasingly used as automated\nheuristic designers for vehicle routing problems (VRPs), current\nstate-of-the-art methods predominantly rely on prompting massive,\ngeneral-purpose models like GPT-4. This work challenges that paradigm by\ndemonstrating that a smaller, specialized LLM, when meticulously fine-tuned,\ncan generate components that surpass expert-crafted heuristics within advanced\nsolvers. We propose RFTHGS, a novel Reinforcement learning (RL) framework for\nFine-Tuning a small LLM to generate high-performance crossover operators for\nthe Hybrid Genetic Search (HGS) solver, applied to the Capacitated VRP (CVRP).\nOur method employs a multi-tiered, curriculum-based reward function that\nprogressively guides the LLM to master generating first compilable, then\nexecutable, and finally, superior-performing operators that exceed human expert\ndesigns. This is coupled with an operator caching mechanism that discourages\nplagiarism and promotes diversity during training. Comprehensive experiments\nshow that our fine-tuned LLM produces crossover operators which significantly\noutperform the expert-designed ones in HGS. The performance advantage remains\nconsistent, generalizing from small-scale instances to large-scale problems\nwith up to 1000 nodes. Furthermore, RFTHGS exceeds the performance of leading\nneuro-combinatorial baselines, prompt-based methods, and commercial LLMs such\nas GPT-4o and GPT-4o-mini."}
{"id": "2510.11128", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11128", "abs": "https://arxiv.org/abs/2510.11128", "authors": ["Qiyi Tong", "Olivia Nocentini", "Marta Lagomarsino", "Kuanqi Cai", "Marta Lorenzini", "Arash Ajoudani"], "title": "Lightweight Facial Landmark Detection in Thermal Images via Multi-Level Cross-Modal Knowledge Transfer", "comment": null, "summary": "Facial Landmark Detection (FLD) in thermal imagery is critical for\napplications in challenging lighting conditions, but it is hampered by the lack\nof rich visual cues. Conventional cross-modal solutions, like feature fusion or\nimage translation from RGB data, are often computationally expensive or\nintroduce structural artifacts, limiting their practical deployment. To address\nthis, we propose Multi-Level Cross-Modal Knowledge Distillation (MLCM-KD), a\nnovel framework that decouples high-fidelity RGB-to-thermal knowledge transfer\nfrom model compression to create both accurate and efficient thermal FLD\nmodels. A central challenge during knowledge transfer is the profound modality\ngap between RGB and thermal data, where traditional unidirectional distillation\nfails to enforce semantic consistency across disparate feature spaces. To\novercome this, we introduce Dual-Injected Knowledge Distillation (DIKD), a\nbidirectional mechanism designed specifically for this task. DIKD establishes a\nconnection between modalities: it not only guides the thermal student with rich\nRGB features but also validates the student's learned representations by\nfeeding them back into the frozen teacher's prediction head. This closed-loop\nsupervision forces the student to learn modality-invariant features that are\nsemantically aligned with the teacher, ensuring a robust and profound knowledge\ntransfer. Experiments show that our approach sets a new state-of-the-art on\npublic thermal FLD benchmarks, notably outperforming previous methods while\ndrastically reducing computational overhead."}
{"id": "2510.11133", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11133", "abs": "https://arxiv.org/abs/2510.11133", "authors": ["Yingnan Liu", "Rui Qiao", "Mong Li Lee", "Wynne Hsu"], "title": "Test-Time Adaptation by Causal Trimming", "comment": "Accepted to the Thirty-Ninth Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2025); Code is available at\n  https://github.com/NancyQuris/TACT", "summary": "Test-time adaptation aims to improve model robustness under distribution\nshifts by adapting models with access to unlabeled target samples. A primary\ncause of performance degradation under such shifts is the model's reliance on\nfeatures that lack a direct causal relationship with the prediction target. We\nintroduce Test-time Adaptation by Causal Trimming (TACT), a method that\nidentifies and removes non-causal components from representations for test\ndistributions. TACT applies data augmentations that preserve causal features\nwhile varying non-causal ones. By analyzing the changes in the representations\nusing Principal Component Analysis, TACT identifies the highest variance\ndirections associated with non-causal features. It trims the representations by\nremoving their projections on the identified directions, and uses the trimmed\nrepresentations for the predictions. During adaptation, TACT continuously\ntracks and refines these directions to get a better estimate of non-causal\nfeatures. We theoretically analyze the effectiveness of this approach and\nempirically validate TACT on real-world out-of-distribution benchmarks. TACT\nconsistently outperforms state-of-the-art methods by a significant margin."}
{"id": "2510.11140", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11140", "abs": "https://arxiv.org/abs/2510.11140", "authors": ["Zhijian Zhou", "Xunye Tian", "Liuhua Peng", "Chao Lei", "Antonin Schrab", "Danica J. Sutherland", "Feng Liu"], "title": "DUAL: Learning Diverse Kernels for Aggregated Two-sample and Independence Testing", "comment": null, "summary": "To adapt kernel two-sample and independence testing to complex structured\ndata, aggregation of multiple kernels is frequently employed to boost testing\npower compared to single-kernel tests. However, we observe a phenomenon that\ndirectly maximizing multiple kernel-based statistics may result in highly\nsimilar kernels that capture highly overlapping information, limiting the\neffectiveness of aggregation. To address this, we propose an aggregated\nstatistic that explicitly incorporates kernel diversity based on the covariance\nbetween different kernels. Moreover, we identify a fundamental challenge: a\ntrade-off between the diversity among kernels and the test power of individual\nkernels, i.e., the selected kernels should be both effective and diverse. This\nmotivates a testing framework with selection inference, which leverages\ninformation from the training phase to select kernels with strong individual\nperformance from the learned diverse kernel pool. We provide rigorous\ntheoretical statements and proofs to show the consistency on the test power and\ncontrol of Type-I error, along with asymptotic analysis of the proposed\nstatistics. Lastly, we conducted extensive empirical experiments demonstrating\nthe superior performance of our proposed approach across various benchmarks for\nboth two-sample and independence testing."}
{"id": "2510.11141", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11141", "abs": "https://arxiv.org/abs/2510.11141", "authors": ["Mohammad Karami", "Mostafa Jalali", "Fatemeh Ghassemi"], "title": "A Comprehensive Forecasting-Based Framework for Time Series Anomaly Detection: Benchmarking on the Numenta Anomaly Benchmark (NAB)", "comment": null, "summary": "Time series anomaly detection is critical for modern digital infrastructures,\nyet existing methods lack systematic cross-domain evaluation. We present a\ncomprehensive forecasting-based framework unifying classical methods\n(Holt-Winters, SARIMA) with deep learning architectures (LSTM, Informer) under\na common residual-based detection interface. Our modular pipeline integrates\npreprocessing (normalization, STL decomposition), four forecasting models, four\ndetection methods, and dual evaluation through forecasting metrics (MAE, RMSE,\nPCC) and detection metrics (Precision, Recall, F1, AUC). We conduct the first\ncomplete evaluation on the Numenta Anomaly Benchmark (58 datasets, 7\ncategories) with 232 model training runs and 464 detection evaluations\nachieving 100\\% success rate. LSTM achieves best performance (F1: 0.688,\nranking first or second on 81\\% of datasets) with exceptional correlation on\ncomplex patterns (PCC: 0.999). Informer provides competitive accuracy (F1:\n0.683) with 30\\% faster training. Classical methods achieve perfect predictions\non simple synthetic data with 60 lower cost but show 2-3 worse F1-scores on\nreal-world datasets. Forecasting quality dominates detection performance:\ndifferences between detection methods (F1: 0.621-0.688) are smaller than\nbetween forecasting models (F1: 0.344-0.688). Our findings provide\nevidence-based guidance: use LSTM for complex patterns, Informer for\nefficiency-critical deployments, and classical methods for simple periodic data\nwith resource constraints. The complete implementation and results establish\nbaselines for future forecasting-based anomaly detection research."}
{"id": "2510.11162", "categories": ["cs.LG", "cs.NE", "nlin.AO", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.11162", "abs": "https://arxiv.org/abs/2510.11162", "authors": ["Roman A. Kononov", "Nikita A. Pospelov", "Konstantin V. Anokhin", "Vladimir V. Nekorkin", "Oleg V. Maslennikov"], "title": "Emergence of hybrid computational dynamics through reinforcement learning", "comment": "22 pages, 11 figures", "summary": "Understanding how learning algorithms shape the computational strategies that\nemerge in neural networks remains a fundamental challenge in machine\nintelligence. While network architectures receive extensive attention, the role\nof the learning paradigm itself in determining emergent dynamics remains\nlargely unexplored. Here we demonstrate that reinforcement learning (RL) and\nsupervised learning (SL) drive recurrent neural networks (RNNs) toward\nfundamentally different computational solutions when trained on identical\ndecision-making tasks. Through systematic dynamical systems analysis, we reveal\nthat RL spontaneously discovers hybrid attractor architectures, combining\nstable fixed-point attractors for decision maintenance with quasi-periodic\nattractors for flexible evidence integration. This contrasts sharply with SL,\nwhich converges almost exclusively to simpler fixed-point-only solutions. We\nfurther show that RL sculpts functionally balanced neural populations through a\npowerful form of implicit regularization -- a structural signature that\nenhances robustness and is conspicuously absent in the more heterogeneous\nsolutions found by SL-trained networks. The prevalence of these complex\ndynamics in RL is controllably modulated by weight initialization and\ncorrelates strongly with performance gains, particularly as task complexity\nincreases. Our results establish the learning algorithm as a primary\ndeterminant of emergent computation, revealing how reward-based optimization\nautonomously discovers sophisticated dynamical mechanisms that are less\naccessible to direct gradient-based optimization. These findings provide both\nmechanistic insights into neural computation and actionable principles for\ndesigning adaptive AI systems."}
{"id": "2510.11164", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11164", "abs": "https://arxiv.org/abs/2510.11164", "authors": ["Ilaria Vascotto", "Alex Rodriguez", "Alessandro Bonaita", "Luca Bortolussi"], "title": "Beyond single-model XAI: aggregating multi-model explanations for enhanced trustworthiness", "comment": "Accepted at the European Workshop on Trustworthy Artificial\n  Intelligence (TRUST-AI), co-located within ECAI 2025", "summary": "The use of Artificial Intelligence (AI) models in real-world and high-risk\napplications has intensified the discussion about their trustworthiness and\nethical usage, from both a technical and a legislative perspective. The field\nof eXplainable Artificial Intelligence (XAI) addresses this challenge by\nproposing explanations that bring to light the decision-making processes of\ncomplex black-box models. Despite being an essential property, the robustness\nof explanations is often an overlooked aspect during development: only robust\nexplanation methods can increase the trust in the system as a whole. This paper\ninvestigates the role of robustness through the usage of a feature importance\naggregation derived from multiple models ($k$-nearest neighbours, random forest\nand neural networks). Preliminary results showcase the potential in increasing\nthe trustworthiness of the application, while leveraging multiple model's\npredictive power."}
{"id": "2510.11168", "categories": ["cs.LG", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11168", "abs": "https://arxiv.org/abs/2510.11168", "authors": ["Jinbin Zhang", "Nasib Ullah", "Erik Schultheis", "Rohit Babbar"], "title": "ELMO: Efficiency via Low-precision and Peak Memory Optimization in Large Output Spaces", "comment": "Accepted to ICML 2025", "summary": "Large output spaces, also referred to as Extreme multilabel classification\n(XMC), is a setting that arises, e.g., in large-scale tagging and\nproduct-to-product recommendation, and is characterized by the number of labels\nranging from hundreds of thousands to millions. This means that the linear\nclassification head, usually only a tiny fraction of the overall model, turns\ninto the main driver for compute and memory demand. Current state-of-the-art\nXMC methods predominantly rely on FP16-FP32 mixed-precision training, which we\nshow can be unstable, and inefficient in terms of memory usage and\ncomputational overhead. Meanwhile, existing low-precision methods typically\nretain higher precision for the classification layer. In this work, we propose\nELMO, a pure low-precision training framework for XMC models using BFloat16 and\nFloat8 data types. By leveraging Kahan summation and stochastic rounding, we\ndemonstrate that XMC models can be effectively trained entirely in Float8,\nwithout relying on single-precision master weights or tensor scaling.\nLow-precision training, combined with our proposed memory optimizations --\ngradient fusion and chunking -- enables significant reductions in GPU memory\nusage. For example, we train a 3-million-label XMC model with only 6.6 GiB of\nGPU memory, compared to the 39.7 GiB required by the optimized SOTA method,\nRenee without compromising accuracy."}
{"id": "2510.11170", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11170", "abs": "https://arxiv.org/abs/2510.11170", "authors": ["Daniel Scalena", "Leonidas Zotos", "Elisabetta Fersini", "Malvina Nissim", "Ahmet Üstün"], "title": "EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling", "comment": null, "summary": "With the rise of reasoning language models and test-time scaling methods as a\nparadigm for improving model performance, substantial computation is often\nrequired to generate multiple candidate sequences from the same prompt. This\nenables exploration of different reasoning paths toward the correct solution,\nhowever, allocates the same compute budget for each prompt. Grounded on the\nassumption that different prompts carry different degrees of complexity, and\nthus different computation needs, we propose EAGer, a training-free generation\nmethod that leverages model uncertainty through token-wise entropy distribution\nto reduce redundant computation and concurrently improve overall performance.\nEAGer allows branching to multiple reasoning paths only in the presence of\nhigh-entropy tokens, and then reallocates the saved compute budget to the\ninstances where exploration of alternative paths is most needed. We find that\nacross multiple open-source models on complex reasoning benchmarks such as AIME\n2025, EAGer can reallocate the budget without accessing target labels,\nachieving the best efficiency-performance trade-off in terms of reasoning\nlength and Pass@k. When target labels are accessible, EAGer generates up to 65%\nfewer tokens (hence saving compute) and achieves up to 37% improvement in\nPass@k compared to the Full Parallel Sampling."}
{"id": "2510.11184", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11184", "abs": "https://arxiv.org/abs/2510.11184", "authors": ["Zhengyu Chen", "Jinluan Yang", "Teng Xiao", "Ruochen Zhou", "Luan Zhang", "Xiangyu Xi", "Xiaowei Shi", "Wei Wang", "Jinggang Wang"], "title": "Can Tool-Integrated Reinforcement Learning Generalize Across Diverse Domains?", "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities in reasoning and tool utilization. However, the generalization of\ntool-augmented reinforcement learning (RL) across diverse domains remains\nunderexplored. In this work, we investigate the cross-domain generalization of\nan LLM agent equipped with a code interpreter tool, which is exclusively\ntrained on mathematical problem-solving tasks. Despite the restricted training\ndomain, we evaluate the agent's performance across several distinct reasoning\ndomains. The results reveal that RL-based tool usage learned from mathematical\ntasks can be effectively transferred to complex tasks in other domains,\nenabling great task performance and high token efficiency. To facilitate this\ncross-domain transfer, we propose a Tool Generalization Reinforcement Learning\n(TGRL) framework designed to promote domain-agnostic learning and skill\nmigration, encompassing: (i) a standardized tool interface that abstracts\ndomain-specific nuances through consistent formatting and explicit termination,\nfostering transferable invocation patterns; (ii) a dual-component reward system\nthat decomposes rewards to incentivize generalizable behaviors like tool\nefficiency and reasoning abstraction, ensuring alignment and robustness across\ndomain shifts; and (iii) an XML-based prompt template that separates thinking,\ntool calls, and responses to encourage modular, domain-invariant planning and\ncoherent multi-turn interactions. Extensive experiments across diverse\nbenchmarks validate our approach, achieving state-of-the-art performance and\nhighlighting the cross-domain potential of Tool RL for LLM reasoning."}
{"id": "2510.11188", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2510.11188", "abs": "https://arxiv.org/abs/2510.11188", "authors": ["Xinhui Chen", "Zuchao Li", "Mengqi Gao", "Yufeng Zhang", "Chak Tou Leong", "Haoyang Li", "Jiaqi Chen"], "title": "Protein as a Second Language for LLMs", "comment": "Main paper: 9 pages, 6 figures. With references and appendix: 18\n  pages, 9 figures total. Submitted to ICLR 2026 (under review)", "summary": "Deciphering the function of unseen protein sequences is a fundamental\nchallenge with broad scientific impact, yet most existing methods depend on\ntask-specific adapters or large-scale supervised fine-tuning. We introduce the\n\"Protein-as-Second-Language\" framework, which reformulates amino-acid sequences\nas sentences in a novel symbolic language that large language models can\ninterpret through contextual exemplars. Our approach adaptively constructs\nsequence-question-answer triples that reveal functional cues in a zero-shot\nsetting, without any further training. To support this process, we curate a\nbilingual corpus of 79,926 protein-QA instances spanning attribute prediction,\ndescriptive understanding, and extended reasoning. Empirically, our method\ndelivers consistent gains across diverse open-source LLMs and GPT-4, achieving\nup to 17.2% ROUGE-L improvement (average +7%) and even surpassing fine-tuned\nprotein-specific language models. These results highlight that generic LLMs,\nwhen guided with protein-as-language cues, can outperform domain-specialized\nmodels, offering a scalable pathway for protein understanding in foundation\nmodels."}
{"id": "2510.11202", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.11202", "abs": "https://arxiv.org/abs/2510.11202", "authors": ["Marco Pintore", "Giorgio Piras", "Angelo Sotgiu", "Maura Pintor", "Battista Biggio"], "title": "Evaluating Line-level Localization Ability of Learning-based Code Vulnerability Detection Models", "comment": "Preprint", "summary": "To address the extremely concerning problem of software vulnerability, system\nsecurity is often entrusted to Machine Learning (ML) algorithms. Despite their\nnow established detection capabilities, such models are limited by design to\nflagging the entire input source code function as vulnerable, rather than\nprecisely localizing the concerned code lines. However, the detection\ngranularity is crucial to support human operators during software development,\nensuring that such predictions reflect the true code semantics to help debug,\nevaluate, and fix the detected vulnerabilities. To address this issue, recent\nwork made progress toward improving the detector's localization ability, thus\nnarrowing down the vulnerability detection \"window\" and providing more\nfine-grained predictions. Such approaches, however, implicitly disregard the\npresence of spurious correlations and biases in the data, which often\npredominantly influence the performance of ML algorithms. In this work, we\ninvestigate how detectors comply with this requirement by proposing an\nexplainability-based evaluation procedure. Our approach, defined as Detection\nAlignment (DA), quantifies the agreement between the input source code lines\nthat most influence the prediction and the actual localization of the\nvulnerability as per the ground truth. Through DA, which is model-agnostic and\nadaptable to different detection tasks, not limited to our use case, we analyze\nmultiple learning-based vulnerability detectors and datasets. As a result, we\nshow how the predictions of such models are consistently biased by\nnon-vulnerable lines, ultimately highlighting the high impact of biases and\nspurious correlations. The code is available at\nhttps://github.com/pralab/vuln-localization-eval."}
{"id": "2510.11209", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.11209", "abs": "https://arxiv.org/abs/2510.11209", "authors": ["Nicola Alboré", "Gabriele Di Antonio", "Fabrizio Coccetti", "Andrea Gabrielli"], "title": "Cross-Scale Reservoir Computing for large spatio-temporal forecasting and modeling", "comment": null, "summary": "We propose a new reservoir computing method for forecasting high-resolution\nspatiotemporal datasets. By combining multi-resolution inputs from coarser to\nfiner layers, our architecture better captures both local and global dynamics.\nApplied to Sea Surface Temperature data, it outperforms standard parallel\nreservoir models in long-term forecasting, demonstrating the effectiveness of\ncross-layers coupling in improving predictive accuracy. Finally, we show that\nthe optimal network dynamics in each layer become increasingly linear,\nrevealing the slow modes propagated to subsequent layers."}
{"id": "2510.11227", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11227", "abs": "https://arxiv.org/abs/2510.11227", "authors": ["Ahmed Rashwan", "Keith Briggs", "Chris Budd", "Lisa Kreusser"], "title": "Enforcing convex constraints in Graph Neural Networks", "comment": null, "summary": "Many machine learning applications require outputs that satisfy complex,\ndynamic constraints. This task is particularly challenging in Graph Neural\nNetwork models due to the variable output sizes of graph-structured data. In\nthis paper, we introduce ProjNet, a Graph Neural Network framework which\nsatisfies input-dependant constraints. ProjNet combines a sparse vector\nclipping method with the Component-Averaged Dykstra (CAD) algorithm, an\niterative scheme for solving the best-approximation problem. We establish a\nconvergence result for CAD and develop a GPU-accelerated implementation capable\nof handling large-scale inputs efficiently. To enable end-to-end training, we\nintroduce a surrogate gradient for CAD that is both computationally efficient\nand better suited for optimization than the exact gradient. We validate ProjNet\non four classes of constrained optimisation problems: linear programming, two\nclasses of non-convex quadratic programs, and radio transmit power\noptimization, demonstrating its effectiveness across diverse problem settings."}
{"id": "2510.11234", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11234", "abs": "https://arxiv.org/abs/2510.11234", "authors": ["Jegwang Ryu", "Minkyu Kim", "Seungjun Shin", "Hee Min Choi", "Dokwan Oh", "Jaeho Lee"], "title": "Neural Weight Compression for Language Models", "comment": null, "summary": "The efficient storage and transmission of language model weights is becoming\nincreasingly important, as their scale and adoption continue to grow. However,\nas our understanding of this new data modality is limited, designing a good\ncompression algorithm for language model weights heavily relies on manual,\ntrial-and-error approaches. In this paper, we propose a learned compression\nframework that trains neural codecs directly from pretrained language model\nweights. Unlike conventional data (e.g., images), language model weights pose\nunique challenges: the sizes and shapes of weight tensors vary significantly,\nand the reconstruction quality must be judged by downstream model predictions\nrather than na\\\"ive MSE loss. To address this, we introduce Neural Weight\nCompression (NWC), a novel autoencoder-based neural codec tailored to model\nweight compression. The proposed method inherits the advantages of\nautoencoder-based codecs while incorporating three technical components: (1)\ncolumn-wise tensor chunking and normalization; (2) an importance-aware training\nloss; (3) an inference-time error compensation mechanism guided by model\noutputs. Experiments on open-weight language models show that NWC achieves\ncompetitive or state-of-the-art accuracy-compression tradeoffs, with\nparticularly strong results at 4-6 bit precisions where accuracy remains nearly\non par with FP16 models."}
{"id": "2510.11245", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11245", "abs": "https://arxiv.org/abs/2510.11245", "authors": ["Leonardo Di Nino", "Gabriele D'Acunto", "Sergio Barbarossa", "Paolo Di Lorenzo"], "title": "Learning the Structure of Connection Graphs", "comment": null, "summary": "Connection graphs (CGs) extend traditional graph models by coupling network\ntopology with orthogonal transformations, enabling the representation of global\ngeometric consistency. They play a key role in applications such as\nsynchronization, Riemannian signal processing, and neural sheaf diffusion. In\nthis work, we address the inverse problem of learning CGs directly from\nobserved signals. We propose a principled framework based on maximum\npseudo-likelihood under a consistency assumption, which enforces spectral\nproperties linking the connection Laplacian to the underlying combinatorial\nLaplacian. Based on this formulation, we introduce the Structured Connection\nGraph Learning (SCGL) algorithm, a block-optimization procedure over Riemannian\nmanifolds that jointly infers network topology, edge weights, and geometric\nstructure. Our experiments show that SCGL consistently outperforms existing\nbaselines in both topological recovery and geometric fidelity, while remaining\ncomputationally efficient."}
{"id": "2510.11250", "categories": ["cs.LG", "I.5.2"], "pdf": "https://arxiv.org/pdf/2510.11250", "abs": "https://arxiv.org/abs/2510.11250", "authors": ["Sujan Chakraborty", "Rahul Bordoloi", "Anindya Sengupta", "Olaf Wolkenhauer", "Saptarshi Bej"], "title": "FUSE: Fast Semi-Supervised Node Embedding Learning via Structural and Label-Aware Optimization", "comment": null, "summary": "Graph-based learning is a cornerstone for analyzing structured data, with\nnode classification as a central task. However, in many real-world graphs,\nnodes lack informative feature vectors, leaving only neighborhood connectivity\nand class labels as available signals. In such cases, effective classification\nhinges on learning node embeddings that capture structural roles and\ntopological context. We introduce a fast semi-supervised embedding framework\nthat jointly optimizes three complementary objectives: (i) unsupervised\nstructure preservation via scalable modularity approximation, (ii) supervised\nregularization to minimize intra-class variance among labeled nodes, and (iii)\nsemi-supervised propagation that refines unlabeled nodes through\nrandom-walk-based label spreading with attention-weighted similarity. These\ncomponents are unified into a single iterative optimization scheme, yielding\nhigh-quality node embeddings. On standard benchmarks, our method consistently\nachieves classification accuracy at par with or superior to state-of-the-art\napproaches, while requiring significantly less computational cost."}
{"id": "2510.11257", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.11257", "abs": "https://arxiv.org/abs/2510.11257", "authors": ["Davide Borghini", "Davide Marchi", "Angelo Nardone", "Giordano Scerra", "Silvia Giulia Galfrè", "Alessandro Pingitore", "Giuseppe Prencipe", "Corrado Priami", "Alina Sîrbu"], "title": "MIEO: encoding clinical data to enhance cardiovascular event prediction", "comment": "Presented in the Poster Session of Computational Intelligence methods\n  for Bioinformatics and Biostatistics (CIBB) 2025", "summary": "As clinical data are becoming increasingly available, machine learning\nmethods have been employed to extract knowledge from them and predict clinical\nevents. While promising, approaches suffer from at least two main issues: low\navailability of labelled data and data heterogeneity leading to missing values.\nThis work proposes the use of self-supervised auto-encoders to efficiently\naddress these challenges. We apply our methodology to a clinical dataset from\npatients with ischaemic heart disease. Patient data is embedded in a latent\nspace, built using unlabelled data, which is then used to train a neural\nnetwork classifier to predict cardiovascular death. Results show improved\nbalanced accuracy compared to applying the classifier directly to the raw data,\ndemonstrating that this solution is promising, especially in conditions where\navailability of unlabelled data could increase."}
{"id": "2510.11274", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11274", "abs": "https://arxiv.org/abs/2510.11274", "authors": ["Jianzhe Zhao", "Hailin Zhu", "Yu Zhang", "Ziqi Chen", "Guibing Guo"], "title": "FedLoRA-Optimizer: Federated LoRA Fine-Tuning with Global and Local Optimization in Heterogeneous Data Scenarios", "comment": null, "summary": "Federated efficient fine-tuning has emerged as an approach that leverages\ndistributed data and computational resources across nodes to address the\nchallenges of large-scale fine-tuning and privacy preservation. The Low-Rank\nAdaptation (LoRA) enables efficient fine-tuning of large-scale pre-trained\nmodels by introducing trainable low-rank matrices into weight updates.However,\nin heterogeneous data scenarios, client drift weakens the generalization of the\nglobal model, and local models often fail to meet the personalized needs of\nindividual clients.Moreover, existing federated LoRA efficient fine-tuning\ntechniques overlook fine-grained analysis of the tuning matrices. To address\nthis, we conducted preliminary experiments and found that different LoRA\nmatrices exhibit different sensitivity to changes in the direction and\nmagnitude of their vectors.We thus propose a fine-grained federated LoRA tuning\nmethod. By fine-tuning the more sensitive directional vectors in the A matrix,\nwhich encode shared knowledge, our method learns shared features more\neffectively across clients and enhances global generalization. Simultaneously,\nby fine-tuning the more sensitive magnitude vectors in the B matrix, which\nencode personalized knowledge, our method better captures personalized\nknowledge, enabling detailed adaptation to local data. The method uses a\npipeline combining global and local optimizers. Global optimization further\nimproves local models, achieving collaborative optimization between global and\nlocal levels. This improves both the generalization ability of the global model\nand the personalized adaptation of local models under heterogeneous data\nscenarios. Experiments on Databricks-Dolly-15k and Natural Instructions with\nLLaMA2-7B and Deepseek-7B confirm that our method improves global performance\nby 0.39% and local performance by 0.59%."}
{"id": "2510.11278", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.11278", "abs": "https://arxiv.org/abs/2510.11278", "authors": ["Gareth Seneque", "Lap-Hang Ho", "Nafise Erfanian Saeedi", "Jeffrey Molendijk", "Ariel Kupermann", "Tim Elson"], "title": "ENIGMA: The Geometry of Reasoning and Alignment in Large-Language Models", "comment": "52 pages, 10 figures", "summary": "We present Entropic Mutual-Information Geometry Large-Language Model\nAlignment (ENIGMA), a novel approach to Large-Language Model (LLM) training\nthat jointly improves reasoning, alignment and robustness by treating an\norganisation's policies/principles as directions to move on a model's\ninformation manifold. Our single-loop trainer combines Group-Relative Policy\nOptimisation (GRPO), an on-policy, critic-free RL method with Chain-of-Thought\n(CoT)-format only rewards; a Self-Supervised Alignment with Mutual Information\n(SAMI)-style symmetric InfoNCE auxiliary; and an entropic Sinkhorn\noptimal-transport regulariser on hidden-state distributions to bound geometry\ndrift. We also introduce infoNCE metrics that specialise to a standard MI lower\nbound under matched negatives to measure how strongly a model's CoT encodes\nthese policies. These metrics include a Sufficiency Index (SI) that enables the\nselection and creation of principles that maximise downstream performance prior\nto training. In our experiments using small (1B) LLMs, high-SI principles\npredict steadier training dynamics and improved benchmark performance over GRPO\nablations. Our information-geometry analysis of trained models validates\ndesirable structural change in the manifold. These results support our\nhypothesis that reasoning, alignment, and robustness are projections of a\nsingle informationgeometric objective, and that models trained using ENIGMA\ndemonstrate principled reasoning without the use of a reward model, offering a\npath to trusted capability"}
{"id": "2510.11282", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11282", "abs": "https://arxiv.org/abs/2510.11282", "authors": ["Ning Yang", "Hengyu Zhong", "Haijun Zhang", "Randall Berry"], "title": "Vision-LLMs for Spatiotemporal Traffic Forecasting", "comment": null, "summary": "Accurate spatiotemporal traffic forecasting is a critical prerequisite for\nproactive resource management in dense urban mobile networks. While Large\nLanguage Models (LLMs) have shown promise in time series analysis, they\ninherently struggle to model the complex spatial dependencies of grid-based\ntraffic data. Effectively extending LLMs to this domain is challenging, as\nrepresenting the vast amount of information from dense geographical grids can\nbe inefficient and overwhelm the model's context. To address these challenges,\nwe propose ST-Vision-LLM, a novel framework that reframes spatiotemporal\nforecasting as a vision-language fusion problem. Our approach leverages a\nVision-LLM visual encoder to process historical global traffic matrices as\nimage sequences, providing the model with a comprehensive global view to inform\ncell-level predictions. To overcome the inefficiency of LLMs in handling\nnumerical data, we introduce an efficient encoding scheme that represents\nfloating-point values as single tokens via a specialized vocabulary, coupled\nwith a two-stage numerical alignment fine-tuning process. The model is first\ntrained with Supervised Fine-Tuning (SFT) and then further optimized for\npredictive accuracy using Group Relative Policy Optimization (GRPO), a\nmemory-efficient reinforcement learning method. Evaluations on real-world\nmobile traffic datasets demonstrate that ST-Vision-LLM outperforms existing\nmethods by 15.6% in long-term prediction accuracy and exceeds the second-best\nbaseline by over 30.04% in cross-domain few-shot scenarios. Our extensive\nexperiments validate the model's strong generalization capabilities across\nvarious data-scarce environments."}
{"id": "2510.11283", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11283", "abs": "https://arxiv.org/abs/2510.11283", "authors": ["Antoine Mouchamps", "Arthur Malherbe", "Adrien Bolland", "Damien Ernst"], "title": "Gym-TORAX: Open-source software for integrating RL with plasma control simulators", "comment": null, "summary": "This paper presents Gym-TORAX, a Python package enabling the implementation\nof Reinforcement Learning (RL) environments for simulating plasma dynamics and\ncontrol in tokamaks. Users define succinctly a set of control actions and\nobservations, and a control objective from which Gym-TORAX creates a Gymnasium\nenvironment that wraps TORAX for simulating the plasma dynamics. The objective\nis formulated through rewards depending on the simulated state of the plasma\nand control action to optimize specific characteristics of the plasma, such as\nperformance and stability. The resulting environment instance is then\ncompatible with a wide range of RL algorithms and libraries and will facilitate\nRL research in plasma control. In its current version, one environment is\nreadily available, based on a ramp-up scenario of the International\nThermonuclear Experimental Reactor (ITER)."}
{"id": "2510.11292", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11292", "abs": "https://arxiv.org/abs/2510.11292", "authors": ["Wenbo Wu", "Qingyi Si", "Xiurui Pan", "Ye Wang", "Jie Zhang"], "title": "LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences", "comment": null, "summary": "While Key-Value (KV) cache succeeds in reducing redundant computations in\nauto-regressive models, it introduces significant memory overhead, limiting its\npractical deployment in long-sequence scenarios. Existing KV retrieval methods\nmitigate this by dynamically retaining only a subset of KV entries on the GPU.\nHowever, they still suffer from notable efficiency and accuracy bottlenecks due\nto per-token retrieval and coarse-grained page-level KV management, especially\nin long-output reasoning scenarios. With the emergence of large reasoning\nmodels, efficiently handling such scenarios has become increasingly important.\nTo address this issue, we present two key observations: (1) critical KVs\nexhibit strong temporal locality during decoding, and (2) these KVs exhibit\ndistinct distribution patterns across the input prompt and generated output.\nBuilding on these observations, we propose LouisKV, an efficient KV cache\nretrieval framework designed for various long-sequence scenarios. Specifically,\nLouisKV introduces a semantic-aware retrieval strategy leveraging temporal\nlocality to trigger retrieval only at semantic boundaries, drastically reducing\ncomputation and data transfer overhead. LouisKV also designs a decoupled,\nfine-grained management scheme that tailors differentiated strategies for input\nand output sequences to create retrieval units that better match the model's\nattention patterns, enabling precise identification of critical KVs.\nFurthermore, to boost efficiency, LouisKV incorporates several kernel-level\noptimizations, including custom Triton and CUDA kernels to accelerate the KV\nclustering and retrieval. Evaluations show that LouisKV achieves up to\n4.7$\\times$ speedup over state-of-the-art KV retrieval methods while\nmaintaining near-lossless accuracy across diverse long-sequence tasks,\nincluding long-input short-output, short-input long-output, and long-input\nlong-output scenarios."}
{"id": "2510.11335", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11335", "abs": "https://arxiv.org/abs/2510.11335", "authors": ["Mayank Nagda", "Phil Ostheimer", "Justus Arweiler", "Indra Jungjohann", "Jennifer Werner", "Dennis Wagner", "Aparna Muraleedharan", "Pouya Jafari", "Jochen Schmid", "Fabian Jirasek", "Jakob Burger", "Michael Bortz", "Hans Hasse", "Stephan Mandt", "Marius Kloft", "Sophie Fellenz"], "title": "DiffStyleTS: Diffusion Model for Style Transfer in Time Series", "comment": null, "summary": "Style transfer combines the content of one signal with the style of another.\nIt supports applications such as data augmentation and scenario simulation,\nhelping machine learning models generalize in data-scarce domains. While well\ndeveloped in vision and language, style transfer methods for time series data\nremain limited. We introduce DiffTSST, a diffusion-based framework that\ndisentangles a time series into content and style representations via\nconvolutional encoders and recombines them through a self-supervised\nattention-based diffusion process. At inference, encoders extract content and\nstyle from two distinct series, enabling conditional generation of novel\nsamples to achieve style transfer. We demonstrate both qualitatively and\nquantitatively that DiffTSST achieves effective style transfer. We further\nvalidate its real-world utility by showing that data augmentation with DiffTSST\nimproves anomaly detection in data-scarce regimes."}
{"id": "2510.11339", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11339", "abs": "https://arxiv.org/abs/2510.11339", "authors": ["Xingtong Yu", "Ruijuan Liang", "Xinming Zhang", "Yuan Fang"], "title": "Event-Aware Prompt Learning for Dynamic Graphs", "comment": "Under review", "summary": "Real-world graph typically evolve via a series of events, modeling dynamic\ninteractions between objects across various domains. For dynamic graph\nlearning, dynamic graph neural networks (DGNNs) have emerged as popular\nsolutions. Recently, prompt learning methods have been explored on dynamic\ngraphs. However, existing methods generally focus on capturing the relationship\nbetween nodes and time, while overlooking the impact of historical events. In\nthis paper, we propose EVP, an event-aware dynamic graph prompt learning\nframework that can serve as a plug-in to existing methods, enhancing their\nability to leverage historical events knowledge. First, we extract a series of\nhistorical events for each node and introduce an event adaptation mechanism to\nalign the fine-grained characteristics of these events with downstream tasks.\nSecond, we propose an event aggregation mechanism to effectively integrate\nhistorical knowledge into node representations. Finally, we conduct extensive\nexperiments on four public datasets to evaluate and analyze EVP."}
{"id": "2510.11345", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11345", "abs": "https://arxiv.org/abs/2510.11345", "authors": ["Han Lu", "Zichen Liu", "Shaopan Xiong", "Yancheng He", "Wei Gao", "Yanan Wu", "Weixun Wang", "Jiashun Liu", "Yang Li", "Haizhou Zhao", "Ju Huang", "Siran Yang", "Xiaoyang Li", "Yijia Luo", "Zihe Liu", "Ling Pan", "Junchi Yan", "Wei Wang", "Wenbo Su", "Jiamang Wang", "Lin Qu", "Bo Zheng"], "title": "Part II: ROLL Flash -- Accelerating RLVR and Agentic Training with Asynchrony", "comment": null, "summary": "Synchronous Reinforcement Learning (RL) post-training has emerged as a\ncrucial step for enhancing Large Language Models (LLMs) with diverse\ncapabilities. However, many systems designed to accelerate RL post-training\nstill suffer from low resource utilization and limited scalability. We present\nROLL Flash, a system that extends ROLL with native support for asynchronous RL\npost-training. ROLL Flash is built upon two core design principles:\nfine-grained parallelism and rollout-train decoupling. Guided by these\nprinciples, ROLL Flash provides flexible programming interfaces that enable a\nfully asynchronous training architecture and support efficient rollout\nmechanisms, including queue scheduling and environment-level asynchronous\nexecution. Through comprehensive theoretical analysis and extensive\nexperiments, we demonstrate that ROLL Flash significantly improves resource\nutilization and scalability over synchronous RL post-training. ROLL Flash\nachieves up to 2.24x speedup on RLVR tasks and 2.72x on agentic tasks, using\nthe same GPU budget as synchronous baselines. Furthermore, we implement several\npopular off-policy algorithms and verify that asynchronous training can achieve\nperformance on par with synchronous training."}
{"id": "2510.11347", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11347", "abs": "https://arxiv.org/abs/2510.11347", "authors": ["Etzion Harari", "Moshe Unger"], "title": "Multi-View Graph Feature Propagation for Privacy Preservation and Feature Sparsity", "comment": null, "summary": "Graph Neural Networks (GNNs) have demonstrated remarkable success in node\nclassification tasks over relational data, yet their effectiveness often\ndepends on the availability of complete node features. In many real-world\nscenarios, however, feature matrices are highly sparse or contain sensitive\ninformation, leading to degraded performance and increased privacy risks.\nFurthermore, direct exposure of information can result in unintended data\nleakage, enabling adversaries to infer sensitive information. To address these\nchallenges, we propose a novel Multi-view Feature Propagation (MFP) framework\nthat enhances node classification under feature sparsity while promoting\nprivacy preservation. MFP extends traditional Feature Propagation (FP) by\ndividing the available features into multiple Gaussian-noised views, each\npropagating information independently through the graph topology. The\naggregated representations yield expressive and robust node embeddings. This\nframework is novel in two respects: it introduces a mechanism that improves\nrobustness under extreme sparsity, and it provides a principled way to balance\nutility with privacy. Extensive experiments conducted on graph datasets\ndemonstrate that MFP outperforms state-of-the-art baselines in node\nclassification while substantially reducing privacy leakage. Moreover, our\nanalysis demonstrates that propagated outputs serve as alternative imputations\nrather than reconstructions of the original features, preserving utility\nwithout compromising privacy. A comprehensive sensitivity analysis further\nconfirms the stability and practical applicability of MFP across diverse\nscenarios. Overall, MFP provides an effective and privacy-aware framework for\ngraph learning in domains characterized by missing or sensitive features."}
{"id": "2510.11354", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.11354", "abs": "https://arxiv.org/abs/2510.11354", "authors": ["Xuan Tang", "Han Zhang", "Yuan Cao", "Difan Zou"], "title": "Understanding the Generalization of Stochastic Gradient Adam in Learning Neural Networks", "comment": "71 pages, 12 figures, NeurIPS 2025", "summary": "Adam is a popular and widely used adaptive gradient method in deep learning,\nwhich has also received tremendous focus in theoretical research. However, most\nexisting theoretical work primarily analyzes its full-batch version, which\ndiffers fundamentally from the stochastic variant used in practice. Unlike SGD,\nstochastic Adam does not converge to its full-batch counterpart even with\ninfinitesimal learning rates. We present the first theoretical characterization\nof how batch size affects Adam's generalization, analyzing two-layer\nover-parameterized CNNs on image data. Our results reveal that while both Adam\nand AdamW with proper weight decay $\\lambda$ converge to poor test error\nsolutions, their mini-batch variants can achieve near-zero test error. We\nfurther prove Adam has a strictly smaller effective weight decay bound than\nAdamW, theoretically explaining why Adam requires more sensitive $\\lambda$\ntuning. Extensive experiments validate our findings, demonstrating the critical\nrole of batch size and weight decay in Adam's generalization performance."}
{"id": "2510.11390", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11390", "abs": "https://arxiv.org/abs/2510.11390", "authors": ["Razvan Marinescu", "Victoria-Elisabeth Gruber", "Diego Fajardo"], "title": "Medical Interpretability and Knowledge Maps of Large Language Models", "comment": "29 pages, 34 figures, 5 tables", "summary": "We present a systematic study of medical-domain interpretability in Large\nLanguage Models (LLMs). We study how the LLMs both represent and process\nmedical knowledge through four different interpretability techniques: (1) UMAP\nprojections of intermediate activations, (2) gradient-based saliency with\nrespect to the model weights, (3) layer lesioning/removal and (4) activation\npatching. We present knowledge maps of five LLMs which show, at a\ncoarse-resolution, where knowledge about patient's ages, medical symptoms,\ndiseases and drugs is stored in the models. In particular for Llama3.3-70B, we\nfind that most medical knowledge is processed in the first half of the model's\nlayers. In addition, we find several interesting phenomena: (i) age is often\nencoded in a non-linear and sometimes discontinuous manner at intermediate\nlayers in the models, (ii) the disease progression representation is\nnon-monotonic and circular at certain layers of the model, (iii) in\nLlama3.3-70B, drugs cluster better by medical specialty rather than mechanism\nof action, especially for Llama3.3-70B and (iv) Gemma3-27B and MedGemma-27B\nhave activations that collapse at intermediate layers but recover by the final\nlayers. These results can guide future research on fine-tuning, un-learning or\nde-biasing LLMs for medical tasks by suggesting at which layers in the model\nthese techniques should be applied."}
{"id": "2510.11400", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11400", "abs": "https://arxiv.org/abs/2510.11400", "authors": ["Kahou Tam", "Chunlin Tian", "Li Li", "Haikai Zhao", "ChengZhong Xu"], "title": "FedHybrid: Breaking the Memory Wall of Federated Learning via Hybrid Tensor Management", "comment": "Sensys 2024", "summary": "Federated Learning (FL) emerges as a new learning paradigm that enables\nmultiple devices to collaboratively train a shared model while preserving data\nprivacy. However, one fundamental and prevailing challenge that hinders the\ndeployment of FL on mobile devices is the memory limitation. This paper\nproposes \\textit{FedHybrid}, a novel framework that effectively reduces the\nmemory footprint during the training process while guaranteeing the model\naccuracy and the overall training progress. Specifically, \\textit{FedHybrid}\nfirst selects the participating devices for each training round by jointly\nevaluating their memory budget, computing capability, and data diversity. After\nthat, it judiciously analyzes the computational graph and generates an\nexecution plan for each selected client in order to meet the corresponding\nmemory budget while minimizing the training delay through employing a hybrid of\nrecomputation and compression techniques according to the characteristic of\neach tensor. During the local training process, \\textit{FedHybrid} carries out\nthe execution plan with a well-designed activation compression technique to\neffectively achieve memory reduction with minimum accuracy loss. We conduct\nextensive experiments to evaluate \\textit{FedHybrid} on both simulation and\noff-the-shelf mobile devices. The experiment results demonstrate that\n\\textit{FedHybrid} achieves up to a 39.1\\% increase in model accuracy and a\n15.5$\\times$ reduction in wall clock time under various memory budgets compared\nwith the baselines."}
{"id": "2510.11409", "categories": ["cs.LG", "cs.DL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.11409", "abs": "https://arxiv.org/abs/2510.11409", "authors": ["Lucas Joos", "Daniel A. Keim", "Maximilian T. Fischer"], "title": "Leveraging LLMs for Semi-Automatic Corpus Filtration in Systematic Literature Reviews", "comment": null, "summary": "The creation of systematic literature reviews (SLR) is critical for analyzing\nthe landscape of a research field and guiding future research directions.\nHowever, retrieving and filtering the literature corpus for an SLR is highly\ntime-consuming and requires extensive manual effort, as keyword-based searches\nin digital libraries often return numerous irrelevant publications. In this\nwork, we propose a pipeline leveraging multiple large language models (LLMs),\nclassifying papers based on descriptive prompts and deciding jointly using a\nconsensus scheme. The entire process is human-supervised and interactively\ncontrolled via our open-source visual analytics web interface, LLMSurver, which\nenables real-time inspection and modification of model outputs. We evaluate our\napproach using ground-truth data from a recent SLR comprising over 8,000\ncandidate papers, benchmarking both open and commercial state-of-the-art LLMs\nfrom mid-2024 and fall 2025. Results demonstrate that our pipeline\nsignificantly reduces manual effort while achieving lower error rates than\nsingle human annotators. Furthermore, modern open-source models prove\nsufficient for this task, making the method accessible and cost-effective.\nOverall, our work demonstrates how responsible human-AI collaboration can\naccelerate and enhance systematic literature reviews within academic workflows."}
{"id": "2510.11442", "categories": ["cs.LG", "cs.AI", "68T05", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.11442", "abs": "https://arxiv.org/abs/2510.11442", "authors": ["Xinyan Guan", "Yongfan Lai", "Jiarui Jin", "Jun Li", "Haoyu Wang", "Qinghao Zhao", "Deyun Zhang", "Shijia Geng", "Shenda Hong"], "title": "Reconstructing 12-Lead ECG from 3-Lead ECG using Variational Autoencoder to Improve Cardiac Disease Detection of Wearable ECG Devices", "comment": "24 pages, 5 figures, submitted to Nature Communications", "summary": "Twelve-lead electrocardiograms (ECGs) are the clinical gold standard for\ncardiac diagnosis, providing comprehensive spatial coverage of the heart\nnecessary to detect conditions such as myocardial infarction (MI). However,\ntheir lack of portability limits continuous and large-scale use. Three-lead ECG\nsystems are widely used in wearable devices due to their simplicity and\nmobility, but they often fail to capture pathologies in unmeasured regions. To\naddress this, we propose WearECG, a Variational Autoencoder (VAE) method that\nreconstructs twelve-lead ECGs from three leads: II, V1, and V5. Our model\nincludes architectural improvements to better capture temporal and spatial\ndependencies in ECG signals. We evaluate generation quality using MSE, MAE, and\nFrechet Inception Distance (FID), and assess clinical validity via a Turing\ntest with expert cardiologists. To further validate diagnostic utility, we\nfine-tune ECGFounder, a large-scale pretrained ECG model, on a multi-label\nclassification task involving over 40 cardiac conditions, including six\ndifferent myocardial infarction locations, using both real and generated\nsignals. Experiments on the MIMIC dataset show that our method produces\nphysiologically realistic and diagnostically informative signals, with robust\nperformance in downstream tasks. This work demonstrates the potential of\ngenerative modeling for ECG reconstruction and its implications for scalable,\nlow-cost cardiac screening."}
{"id": "2510.11471", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11471", "abs": "https://arxiv.org/abs/2510.11471", "authors": ["Sarthak Mittal", "Divyat Mahajan", "Guillaume Lajoie", "Mohammad Pezeshki"], "title": "Iterative Amortized Inference: Unifying In-Context Learning and Learned Optimizers", "comment": null, "summary": "Modern learning systems increasingly rely on amortized learning - the idea of\nreusing computation or inductive biases shared across tasks to enable rapid\ngeneralization to novel problems. This principle spans a range of approaches,\nincluding meta-learning, in-context learning, prompt tuning, learned optimizers\nand more. While motivated by similar goals, these approaches differ in how they\nencode and leverage task-specific information, often provided as in-context\nexamples. In this work, we propose a unified framework which describes how such\nmethods differ primarily in the aspects of learning they amortize - such as\ninitializations, learned updates, or predictive mappings - and how they\nincorporate task data at inference. We introduce a taxonomy that categorizes\namortized models into parametric, implicit, and explicit regimes, based on\nwhether task adaptation is externalized, internalized, or jointly modeled.\nBuilding on this view, we identify a key limitation in current approaches: most\nmethods struggle to scale to large datasets because their capacity to process\ntask data at inference (e.g., context length) is often limited. To address\nthis, we propose iterative amortized inference, a class of models that refine\nsolutions step-by-step over mini-batches, drawing inspiration from stochastic\noptimization. Our formulation bridges optimization-based meta-learning with\nforward-pass amortization in models like LLMs, offering a scalable and\nextensible foundation for general-purpose task adaptation."}
{"id": "2510.11472", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11472", "abs": "https://arxiv.org/abs/2510.11472", "authors": ["Yanjie Zhu", "Zhen Zhang", "Yunli Wang", "Zhiqiang Wang", "Yu Li", "Rufan Zhou", "Shiyang Wen", "Peng Jiang", "Chenhao Lin", "Jian Yang"], "title": "Differentiable Fast Top-K Selection for Large-Scale Recommendation", "comment": "12 pages, 5 figures", "summary": "Cascade ranking is a widely adopted paradigm in large-scale information\nretrieval systems for Top-K item selection. However, the Top-K operator is\nnon-differentiable, hindering end-to-end training. Existing methods include\nLearning-to-Rank approaches (e.g., LambdaLoss), which optimize ranking metrics\nlike NDCG and suffer from objective misalignment, and differentiable\nsorting-based methods (e.g., ARF, LCRON), which relax permutation matrices for\ndirect Top-K optimization but introduce gradient conflicts through matrix\naggregation. A promising alternative is to directly construct a differentiable\napproximation of the Top-K selection operator, bypassing the use of soft\npermutation matrices. However, even state-of-the-art differentiable Top-K\noperator (e.g., LapSum) require $O(n \\log n)$ complexity due to their\ndependence on sorting for solving the threshold. Thus, we propose DFTopK, a\nnovel differentiable Top-K operator achieving optimal $O(n)$ time complexity.\nBy relaxing normalization constraints, DFTopK admits a closed-form solution and\navoids sorting. DFTopK also avoids the gradient conflicts inherent in\ndifferentiable sorting-based methods. We evaluate DFTopK on both the public\nbenchmark RecFLow and an industrial system. Experimental results show that\nDFTopK significantly improves training efficiency while achieving superior\nperformance, which enables us to scale up training samples more efficiently. In\nthe online A/B test, DFTopK yielded a +1.77\\% revenue lift with the same\ncomputational budget compared to the baseline. To the best of our knowledge,\nthis work is the first to introduce differentiable Top-K operators into\nrecommendation systems and the first to achieve theoretically optimal\nlinear-time complexity for Top-K selection. We have open-sourced our\nimplementation to facilitate future research in both academia and industry."}
{"id": "2510.11484", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.11484", "abs": "https://arxiv.org/abs/2510.11484", "authors": ["Lion Mueller", "Alberto Garcia-Ortiz", "Ardalan Najafi", "Adam Fuks", "Lennart Bamberg"], "title": "Rescaling-Aware Training for Efficient Deployment of Deep Learning Models on Full-Integer Hardware", "comment": "Submitted to IEEE Embedded Systems Letters", "summary": "Integer AI inference significantly reduces computational complexity in\nembedded systems. Quantization-aware training (QAT) helps mitigate accuracy\ndegradation associated with post-training quantization but still overlooks the\nimpact of integer rescaling during inference, which is a hardware costly\noperation in integer-only AI inference. This work shows that rescaling cost can\nbe dramatically reduced post-training, by applying a stronger quantization to\nthe rescale multiplicands at no model-quality loss. Furthermore, we introduce\nRescale-Aware Training, a fine tuning method for ultra-low bit-width rescaling\nmultiplicands. Experiments show that even with 8x reduced rescaler widths, the\nfull accuracy is preserved through minimal incremental retraining. This enables\nmore energy-efficient and cost-efficient AI inference for resource-constrained\nembedded systems."}
{"id": "2510.11495", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.11495", "abs": "https://arxiv.org/abs/2510.11495", "authors": ["Nikolaos Tsilivis", "Eran Malach", "Karen Ullrich", "Julia Kempe"], "title": "How Reinforcement Learning After Next-Token Prediction Facilitates Learning", "comment": null, "summary": "Recent advances in reasoning domains with neural networks have primarily been\nenabled by a training recipe that optimizes Large Language Models, previously\ntrained to predict the next-token in a sequence, with reinforcement learning\nalgorithms. We introduce a framework to study the success of this paradigm, and\nwe theoretically expose the optimization mechanisms by which reinforcement\nlearning improves over next-token prediction in this setting. We study learning\nfrom mixture distributions of short and long ``chain-of-thought'' sequences\nencoding a single task. In particular, when the task consists of predicting the\nparity of $d$ bits and long sequences are rare, we show how reinforcement\nlearning after next-token prediction enables autoregressive transformers to\ngeneralize, whereas mere next-token prediction requires extreme statistical or\ncomputational resources to do so. We further explain how reinforcement learning\nleverages increased test-time computation, manifested in longer responses, to\nfacilitate this learning process. In a simplified setting, we theoretically\nprove that autoregressive linear models following this training recipe can\nefficiently learn to predict the parity of $d$ bits as long as the proportion\nof long demonstrations in the data mix is not exponentially small in the input\ndimension $d$. Finally, we demonstrate these same phenomena in other settings,\nincluding the post-training of Llama-series models on mixture variations of\ncommon mathematical reasoning benchmarks."}
{"id": "2510.11498", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11498", "abs": "https://arxiv.org/abs/2510.11498", "authors": ["Yuhang Li", "Chenchen Zhang", "Ruilin Lv", "Ao Liu", "Ken Deng", "Yuanxing Zhang", "Jiaheng Liu", "Wiggin Zhou", "Bo Zhou"], "title": "ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding", "comment": null, "summary": "While Large Language Models (LLMs) excel at algorithmic code generation, they\nstruggle with front-end development, where correctness is judged on rendered\npixels and interaction. We present ReLook, an agentic, vision-grounded\nreinforcement learning framework that empowers an agent to close a robust\ngenerate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool.\nDuring training, the agent uses the MLLM-in-the-loop both as a visual\ncritic--scoring code with screenshots--and as a source of actionable,\nvision-grounded feedback; a strict zero-reward rule for invalid renders anchors\nrenderability and prevents reward hacking. To prevent behavioral collapse, we\nintroduce Forced Optimization, a strict acceptance rule that admits only\nimproving revisions, yielding monotonically better trajectories. At inference,\nwe decouple the critic and run a lightweight, critic-free self-edit cycle,\nkeeping latency comparable to base decoding while retaining most of the gains.\nAcross three widely used benchmarks, ReLook consistently outperforms strong\nbaselines in vision-grounded front-end code generation, highlighting the\nbenefits of agentic perception, visual rewards, and training-inference\ndecoupling."}
{"id": "2510.11499", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11499", "abs": "https://arxiv.org/abs/2510.11499", "authors": ["Xinsong Feng", "Leshu Tang", "Chenan Wang", "Haipeng Chen"], "title": "Offline Reinforcement Learning with Generative Trajectory Policies", "comment": "Preprint. Under review at ICLR 2026", "summary": "Generative models have emerged as a powerful class of policies for offline\nreinforcement learning (RL) due to their ability to capture complex,\nmulti-modal behaviors. However, existing methods face a stark trade-off: slow,\niterative models like diffusion policies are computationally expensive, while\nfast, single-step models like consistency policies often suffer from degraded\nperformance. In this paper, we demonstrate that it is possible to bridge this\ngap. The key to moving beyond the limitations of individual methods, we argue,\nlies in a unifying perspective that views modern generative models, including\ndiffusion, flow matching, and consistency models, as specific instances of\nlearning a continuous-time generative trajectory governed by an Ordinary\nDifferential Equation (ODE). This principled foundation provides a clearer\ndesign space for generative policies in RL and allows us to propose Generative\nTrajectory Policies (GTPs), a new and more general policy paradigm that learns\nthe entire solution map of the underlying ODE. To make this paradigm practical\nfor offline RL, we further introduce two key theoretically principled\nadaptations. Empirical results demonstrate that GTP achieves state-of-the-art\nperformance on D4RL benchmarks - it significantly outperforms prior generative\npolicies, achieving perfect scores on several notoriously hard AntMaze tasks."}
{"id": "2510.11501", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11501", "abs": "https://arxiv.org/abs/2510.11501", "authors": ["Emran Yasser Moustafa", "Ivana Dusparic"], "title": "Context-Aware Model-Based Reinforcement Learning for Autonomous Racing", "comment": "Accepted to IEEE ICAR 2025", "summary": "Autonomous vehicles have shown promising potential to be a groundbreaking\ntechnology for improving the safety of road users. For these vehicles, as well\nas many other safety-critical robotic technologies, to be deployed in\nreal-world applications, we require algorithms that can generalize well to\nunseen scenarios and data. Model-based reinforcement learning algorithms (MBRL)\nhave demonstrated state-of-the-art performance and data efficiency across a\ndiverse set of domains. However, these algorithms have also shown\nsusceptibility to changes in the environment and its transition dynamics.\n  In this work, we explore the performance and generalization capabilities of\nMBRL algorithms for autonomous driving, specifically in the simulated\nautonomous racing environment, Roboracer (formerly F1Tenth). We frame the\nhead-to-head racing task as a learning problem using contextual Markov decision\nprocesses and parameterize the driving behavior of the adversaries using the\ncontext of the episode, thereby also parameterizing the transition and reward\ndynamics. We benchmark the behavior of MBRL algorithms in this environment and\npropose a novel context-aware extension of the existing literature, cMask. We\ndemonstrate that context-aware MBRL algorithms generalize better to\nout-of-distribution adversary behaviors relative to context-free approaches. We\nalso demonstrate that cMask displays strong generalization capabilities, as\nwell as further performance improvement relative to other context-aware MBRL\napproaches when racing against adversaries with in-distribution behaviors."}
{"id": "2510.11502", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11502", "abs": "https://arxiv.org/abs/2510.11502", "authors": ["Alexis Ross", "Jacob Andreas"], "title": "Learning to Make MISTAKEs: Modeling Incorrect Student Thinking And Key Errors", "comment": null, "summary": "Research on reasoning in language models (LMs) predominantly focuses on\nimproving the correctness of their outputs. But some important applications\nrequire modeling reasoning patterns that are incorrect. For example, automated\nsystems that can reason about and simulate student errors are useful for\nproviding real-time feedback in the classroom or offline practice for\neducators-in-training. This paper presents a new method, MISTAKE, that (1)\nconstructs high-quality synthetic examples of reasoning errors by leveraging\ncycle consistency between incorrect answers and latent misconceptions; and (2)\nuses the generated data to learn models for student simulation, misconception\nclassification, and answer generation. We evaluate MISTAKE on three educational\ntasks and find that it results in (1) higher accuracy when simulating incorrect\nstudent answers based on specific misconceptions, (2) increased performance\ninferring latent misconceptions from observed incorrect answers, and (3) higher\nalignment with expert-written distractor answers when generating incorrect\nanswers (e.g., for multiple-choice tests)."}
{"id": "2510.11505", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11505", "abs": "https://arxiv.org/abs/2510.11505", "authors": ["Aleksei Rozanov", "Samikshya Subedi", "Vasudha Sharma", "Bryan C. Runck"], "title": "Knowledge-Guided Machine Learning Models to Upscale Evapotranspiration in the U.S. Midwest", "comment": null, "summary": "Evapotranspiration (ET) plays a critical role in the land-atmosphere\ninteractions, yet its accurate quantification across various spatiotemporal\nscales remains a challenge. In situ measurement approaches, like eddy\ncovariance (EC) or weather station-based ET estimation, allow for measuring ET\nat a single location. Agricultural uses of ET require estimates for each field\nover broad areas, making it infeasible to deploy sensing systems at each\nlocation. This study integrates tree-based and knowledge-guided machine\nlearning (ML) techniques with multispectral remote sensing data, griddled\nmeteorology and EC data to upscale ET across the Midwest United States. We\ncompare four tree-based models - Random Forest, CatBoost, XGBoost, LightGBM -\nand a simple feed-forward artificial neural network in combination with\nfeatures engineered using knowledge-guided ML principles. Models were trained\nand tested on EC towers located in the Midwest of the United States using\nk-fold cross validation with k=5 and site-year, biome stratified train-test\nsplit to avoid data leakage. Results show that LightGBM with knowledge-guided\nfeatures outperformed other methods with an R2=0.86, MSE=14.99 W m^-2 and MAE =\n8.82 W m^-2 according to grouped k-fold validation (k=5). Feature importance\nanalysis shows that knowledge-guided features were most important for\npredicting evapotranspiration. Using the best performing model, we provide a\ndata product at 500 m spatial and one-day temporal resolution for gridded ET\nfor the period of 2019-2024. Intercomparison between the new gridded product\nand state-level weather station-based ET estimates show best-in-class\ncorrespondence."}
{"id": "2510.11541", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11541", "abs": "https://arxiv.org/abs/2510.11541", "authors": ["Yuchen Yan", "Zhihua Liu", "Hao Wang", "Weiming Li", "Xiaoshuai Hao"], "title": "Query-Specific GNN: A Comprehensive Graph Representation Learning Method for Retrieval Augmented Generation", "comment": null, "summary": "Retrieval-augmented generation (RAG) has demonstrated its ability to enhance\nLarge Language Models (LLMs) by integrating external knowledge sources.\nHowever, multi-hop questions, which require the identification of multiple\nknowledge targets to form a synthesized answer, raise new challenges for RAG\nsystems. Under the multi-hop settings, existing methods often struggle to fully\nunderstand the questions with complex semantic structures and are susceptible\nto irrelevant noise during the retrieval of multiple information targets. To\naddress these limitations, we propose a novel graph representation learning\nframework for multi-hop question retrieval. We first introduce a\nMulti-information Level Knowledge Graph (Multi-L KG) to model various\ninformation levels for a more comprehensive understanding of multi-hop\nquestions. Based on this, we design a Query-Specific Graph Neural Network\n(QSGNN) for representation learning on the Multi-L KG. QSGNN employs\nintra/inter-level message passing mechanisms, and in each message passing the\ninformation aggregation is guided by the query, which not only facilitates\nmulti-granular information aggregation but also significantly reduces the\nimpact of noise. To enhance its ability to learn robust representations, we\nfurther propose two synthesized data generation strategies for pre-training the\nQSGNN. Extensive experimental results demonstrate the effectiveness of our\nframework in multi-hop scenarios, especially in high-hop questions the\nimprovement can reach 33.8\\%. The code is available at:\nhttps://github.com/Jerry2398/QSGNN."}
{"id": "2510.11561", "categories": ["cs.LG", "cs.SC"], "pdf": "https://arxiv.org/pdf/2510.11561", "abs": "https://arxiv.org/abs/2510.11561", "authors": ["Caglar Demir", "Alkid Baci", "N'Dah Jean Kouagou", "Leonie Nora Sieger", "Stefan Heindorf", "Simon Bin", "Lukas Blübaum", "Alexander Bigerl", "Axel-Cyrille Ngonga Ngomo"], "title": "Ontolearn-A Framework for Large-scale OWL Class Expression Learning in Python", "comment": null, "summary": "In this paper, we present Ontolearn-a framework for learning OWL class\nexpressions over large knowledge graphs. Ontolearn contains efficient\nimplementations of recent stateof-the-art symbolic and neuro-symbolic class\nexpression learners including EvoLearner and DRILL. A learned OWL class\nexpression can be used to classify instances in the knowledge graph.\nFurthermore, Ontolearn integrates a verbalization module based on an LLM to\ntranslate complex OWL class expressions into natural language sentences. By\nmapping OWL class expressions into respective SPARQL queries, Ontolearn can be\neasily used to operate over a remote triplestore. The source code of Ontolearn\nis available at https://github.com/dice-group/Ontolearn."}
{"id": "2510.11590", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.11590", "abs": "https://arxiv.org/abs/2510.11590", "authors": ["Zihao Zhao", "Christopher Yeh", "Lingkai Kong", "Kai Wang"], "title": "Diffusion-DFL: Decision-focused Diffusion Models for Stochastic Optimization", "comment": null, "summary": "Decision-focused learning (DFL) integrates predictive modeling and\noptimization by training predictors to optimize the downstream decision target\nrather than merely minimizing prediction error. To date, existing DFL methods\ntypically rely on deterministic point predictions, which are often insufficient\nto capture the intrinsic stochasticity of real-world environments. To address\nthis challenge, we propose the first diffusion-based DFL approach, which trains\na diffusion model to represent the distribution of uncertain parameters and\noptimizes the decision by solving a stochastic optimization with samples drawn\nfrom the diffusion model. Our contributions are twofold. First, we formulate\ndiffusion DFL using the reparameterization trick, enabling end-to-end training\nthrough diffusion. While effective, it is memory and compute-intensive due to\nthe need to differentiate through the diffusion sampling process. Second, we\npropose a lightweight score function estimator that uses only several forward\ndiffusion passes and avoids backpropagation through the sampling. This follows\nfrom our results that backpropagating through stochastic optimization can be\napproximated by a weighted score function formulation. We empirically show that\nour diffusion DFL approach consistently outperforms strong baselines in\ndecision quality. The source code for all experiments is available at the\nproject repository: https://github.com/GT-KOALA/Diffusion_DFL."}
{"id": "2510.11616", "categories": ["cs.LG", "cs.AI", "q-fin.CP", "I.2.0"], "pdf": "https://arxiv.org/pdf/2510.11616", "abs": "https://arxiv.org/abs/2510.11616", "authors": ["Elliot L. Epstein", "Rose Wang", "Jaewon Choi", "Markus Pelger"], "title": "Attention Factors for Statistical Arbitrage", "comment": "Accepted to the 6th ACM International Conference on AI in Finance", "summary": "Statistical arbitrage exploits temporal price differences between similar\nassets. We develop a framework to jointly identify similar assets through\nfactors, identify mispricing and form a trading policy that maximizes\nrisk-adjusted performance after trading costs. Our Attention Factors are\nconditional latent factors that are the most useful for arbitrage trading. They\nare learned from firm characteristic embeddings that allow for complex\ninteractions. We identify time-series signals from the residual portfolios of\nour factors with a general sequence model. Estimating factors and the arbitrage\ntrading strategy jointly is crucial to maximize profitability after trading\ncosts. In a comprehensive empirical study we show that our Attention Factor\nmodel achieves an out-of-sample Sharpe ratio above 4 on the largest U.S.\nequities over a 24-year period. Our one-step solution yields an unprecedented\nSharpe ratio of 2.3 net of transaction costs. We show that weak factors are\nimportant for arbitrage trading."}
{"id": "2510.11653", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11653", "abs": "https://arxiv.org/abs/2510.11653", "authors": ["Prasanna Mayilvahanan", "Ricardo Dominguez-Olmedo", "Thaddäus Wiedemer", "Wieland Brendel"], "title": "MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model", "comment": null, "summary": "With the advent of DeepSeek-R1, a new wave of reinforcement learning (RL)\nmethods has emerged that seem to unlock stronger mathematical reasoning.\nHowever, a closer look at the open-source ecosystem reveals a critical\nlimitation: with sufficiently many draws (e.g., $\\texttt{pass@1024}$), many\nexisting base models already solve nearly all questions on widely used math\nbenchmarks such as MATH-500 and AIME 2024. This suggests that the RL\nfine-tuning methods prevalent in the LLM reasoning literature largely sharpen\nexisting solution modes rather than discovering entirely new ones. Such\nsharpening stands in contrast to the broader promise of RL: to foster\nexploration and to acquire new skills. To move beyond this plateau, we\nintroduce MATH-Beyond (MATH-B), a benchmark deliberately constructed to defeat\ncommon open-source models of up to 8B parameters even under large sampling\nbudgets. Improving performance on our benchmark via RL requires methods that\nlearn to reason in ways that go beyond base model capabilities in repeated\nsampling. Since the problems are drawn from subsets of DAPO-Math-17K and\nDeepScaleR datasets, they remain topically equivalent to standard high-school\nmath. Validating our premise, RL fine-tuned models such as\nNemotron-Research-Reasoning-Qwen-1.5B and DeepScaleR-1.5B-Preview perform\npoorly on MATH-B at $\\texttt{pass@1024}$, showing how existing approaches fall\nshort on tackling harder instances. We hope MATH-B will catalyze\nexploration-driven RL approaches that elicit deeper reasoning capabilities. We\nrelease MATH-B at https://huggingface.co/datasets/brendel-group/MATH-Beyond."}
{"id": "2510.11657", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.11657", "abs": "https://arxiv.org/abs/2510.11657", "authors": ["Panos Tsimpos", "Youssef Marzouk"], "title": "An Eulerian Perspective on Straight-Line Sampling", "comment": null, "summary": "We study dynamic measure transport for generative modeling: specifically,\nflows induced by stochastic processes that bridge a specified source and target\ndistribution. The conditional expectation of the process' velocity defines an\nODE whose flow map achieves the desired transport. We ask \\emph{which processes\nproduce straight-line flows} -- i.e., flows whose pointwise acceleration\nvanishes and thus are exactly integrable with a first-order method? We provide\na concise PDE characterization of straightness as a balance between conditional\nacceleration and the divergence of a weighted covariance (Reynolds) tensor.\nUsing this lens, we fully characterize affine-in-time interpolants and show\nthat straightness occurs exactly under deterministic endpoint couplings. We\nalso derive necessary conditions that constrain flow geometry for general\nprocesses, offering broad guidance for designing transports that are easier to\nintegrate."}
{"id": "2510.11677", "categories": ["cs.LG", "q-fin.GN"], "pdf": "https://arxiv.org/pdf/2510.11677", "abs": "https://arxiv.org/abs/2510.11677", "authors": ["Songrun He", "Linying Lv", "Asaf Manela", "Jimmy Wu"], "title": "Chronologically Consistent Generative AI", "comment": null, "summary": "We introduce a family of chronologically consistent, instruction-following\nlarge language models to eliminate lookahead bias. Each model is trained only\non data available before a clearly defined knowledge-cutoff date, ensuring\nstrict temporal separation from any post-cutoff data. The resulting framework\noffers (i) a simple, conversational chat interface, (ii) fully open, fixed\nmodel weights that guarantee replicability, and (iii) a conservative lower\nbound on forecast accuracy, isolating the share of predictability that survives\nonce training leakage is removed. Together, these features provide researchers\nwith an easy-to-use generative AI tool useful for a wide range of prediction\ntasks that is free of lookahead bias."}
{"id": "2510.11683", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11683", "abs": "https://arxiv.org/abs/2510.11683", "authors": ["Nianyi Lin", "Jiajie Zhang", "Lei Hou", "Juanzi Li"], "title": "Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models", "comment": null, "summary": "A key challenge in applying reinforcement learning (RL) to diffusion large\nlanguage models (dLLMs) lies in the intractability of their likelihood\nfunctions, which are essential for the RL objective, necessitating\ncorresponding approximation in each training step. While existing methods\napproximate the log-likelihoods by their evidence lower bounds (ELBOs) via\ncustomized Monte Carlo (MC) sampling, the forward computational graphs of all\nMC samples need to be retained for the gradient computation of non-linear terms\nin the RL objective, resulting in significant memory overhead. This constraint\nrestricts feasible sample sizes, leading to imprecise likelihood approximations\nand ultimately distorting the RL objective. To overcome this limitation, we\npropose \\emph{Boundary-Guided Policy Optimization} (BGPO), a memory-efficient\nRL algorithm that maximizes a specially constructed lower bound of the\nELBO-based objective. This lower bound is carefully designed to satisfy two key\nproperties: (1) Linearity: it is formulated in a linear sum where each term\ndepends only on a single MC sample, thereby enabling gradient accumulation\nacross samples and ensuring constant memory usage; (2) Equivalence: Both the\nvalue and gradient of this lower bound are equal to those of the ELBO-based\nobjective in on-policy training, making it also an effective approximation for\nthe original RL objective. These properties allow BGPO to adopt a large MC\nsample size, resulting in more accurate likelihood approximations and improved\nRL objective estimation, which in turn leads to enhanced performance.\nExperiments show that BGPO significantly outperforms previous RL algorithms for\ndLLMs in math problem solving, code generation, and planning tasks."}
{"id": "2510.11686", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11686", "abs": "https://arxiv.org/abs/2510.11686", "authors": ["Jens Tuyls", "Dylan J. Foster", "Akshay Krishnamurthy", "Jordan T. Ash"], "title": "Representation-Based Exploration for Language Models: From Test-Time to Post-Training", "comment": "Website and code: https://rep-exp.github.io", "summary": "Reinforcement learning (RL) promises to expand the capabilities of language\nmodels, but it is unclear if current RL techniques promote the discovery of\nnovel behaviors, or simply sharpen those already present in the base model. In\nthis paper, we investigate the value of deliberate exploration -- explicitly\nincentivizing the model to discover novel and diverse behaviors -- and aim to\nunderstand how the knowledge in pre-trained models can guide this search. Our\nmain finding is that exploration with a simple, principled,\nrepresentation-based bonus derived from the pre-trained language model's hidden\nstates significantly improves diversity and pass@k rates -- both for\npost-training, and in a novel inference-time scaling setting we introduce. For\ninference-time, exploration with representation-based diversity improves\nefficiency, consistently improving pass@k rates across a variety of models and\nreasoning tasks. For example, for Qwen-2.5-14b-Instruct we obtain over 50%\nimprovement in verifier efficiency on almost all tasks. For post-training, we\nshow that integrating this exploration strategy into an RL pipeline improves\nreasoning performance over that of the initial model and over standard RL\npost-training. For example, on AIME 2024, our post-trained\nQwen-2.5-7b-Instruct's pass@80 matches the pass@256 of GRPO on the same model,\ndemonstrating a 3x improvement in test-time sample efficiency. Overall, our\nfindings suggest that deliberate exploration -- with the right notion of\ndiversity -- is a practical path toward discovery of new behaviors beyond\nsharpening."}
{"id": "2510.11691", "categories": ["cs.LG", "cs.GT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.11691", "abs": "https://arxiv.org/abs/2510.11691", "authors": ["Taira Tsuchiya"], "title": "Tight Regret Upper and Lower Bounds for Optimistic Hedge in Two-Player Zero-Sum Games", "comment": "29 pages, 2 figures", "summary": "In two-player zero-sum games, the learning dynamic based on optimistic Hedge\nachieves one of the best-known regret upper bounds among strongly-uncoupled\nlearning dynamics. With an appropriately chosen learning rate, the social and\nindividual regrets can be bounded by $O(\\log(mn))$ in terms of the numbers of\nactions $m$ and $n$ of the two players. This study investigates the optimality\nof the dependence on $m$ and $n$ in the regret of optimistic Hedge. To this\nend, we begin by refining existing regret analysis and show that, in the\nstrongly-uncoupled setting where the opponent's number of actions is known,\nboth the social and individual regret bounds can be improved to $O(\\sqrt{\\log m\n\\log n})$. In this analysis, we express the regret upper bound as an\noptimization problem with respect to the learning rates and the coefficients of\ncertain negative terms, enabling refined analysis of the leading constants. We\nthen show that the existing social regret bound as well as these new social and\nindividual regret upper bounds cannot be further improved for optimistic Hedge\nby providing algorithm-dependent individual regret lower bounds. Importantly,\nthese social regret upper and lower bounds match exactly including the constant\nfactor in the leading term. Finally, building on these results, we improve the\nlast-iterate convergence rate and the dynamic regret of a learning dynamic\nbased on optimistic Hedge, and complement these bounds with algorithm-dependent\ndynamic regret lower bounds that match the improved bounds."}
{"id": "2510.11696", "categories": ["cs.LG", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11696", "abs": "https://arxiv.org/abs/2510.11696", "authors": ["Wei Huang", "Yi Ge", "Shuai Yang", "Yicheng Xiao", "Huizi Mao", "Yujun Lin", "Hanrong Ye", "Sifei Liu", "Ka Chun Cheung", "Hongxu Yin", "Yao Lu", "Xiaojuan Qi", "Song Han", "Yukang Chen"], "title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs", "comment": "Code is available at https://github.com/NVlabs/QeRL", "summary": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for\nlarge language models (LLMs). While RL is essential for LLMs' reasoning\ncapabilities, it is resource-intensive, requiring substantial GPU memory and\nlong rollout durations. QeRL addresses these issues by combining NVFP4\nquantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL\nwhile reducing memory overhead. Beyond efficiency, our findings show that\nquantization noise increases policy entropy, enhancing exploration, and\nenabling the discovery of better strategies during RL. To further optimize\nexploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,\nwhich dynamically adjusts noise during training. Experiments demonstrate that\nQeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is\nthe first framework to enable RL training of a 32B LLM on a single H100 80GB\nGPU, while delivering overall speedups for RL training. It also achieves faster\nreward growth and higher final accuracy than 16-bit LoRA and QLoRA, while\nmatching the performance of full-parameter fine-tuning on mathematical\nbenchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These\nresults establish QeRL as an efficient and effective framework for RL training\nin LLMs."}
{"id": "2510.11709", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11709", "abs": "https://arxiv.org/abs/2510.11709", "authors": ["Edward Stevinson", "Lucas Prieto", "Melih Barsbey", "Tolga Birdal"], "title": "Adversarial Attacks Leverage Interference Between Features in Superposition", "comment": null, "summary": "Fundamental questions remain about when and why adversarial examples arise in\nneural networks, with competing views characterising them either as artifacts\nof the irregularities in the decision landscape or as products of sensitivity\nto non-robust input features. In this paper, we instead argue that adversarial\nvulnerability can stem from efficient information encoding in neural networks.\nSpecifically, we show how superposition - where networks represent more\nfeatures than they have dimensions - creates arrangements of latent\nrepresentations that adversaries can exploit. We demonstrate that adversarial\nperturbations leverage interference between superposed features, making attack\npatterns predictable from feature arrangements. Our framework provides a\nmechanistic explanation for two known phenomena: adversarial attack\ntransferability between models with similar training regimes and class-specific\nvulnerability patterns. In synthetic settings with precisely controlled\nsuperposition, we establish that superposition suffices to create adversarial\nvulnerability. We then demonstrate that these findings persist in a ViT trained\non CIFAR-10. These findings reveal adversarial vulnerability can be a byproduct\nof networks' representational compression, rather than flaws in the learning\nprocess or non-robust inputs."}
{"id": "2510.11711", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.11711", "abs": "https://arxiv.org/abs/2510.11711", "authors": ["Sanghyeok Choi", "Sarthak Mittal", "Víctor Elvira", "Jinkyoo Park", "Nikolay Malkin"], "title": "Reinforced sequential Monte Carlo for amortised sampling", "comment": "code: https://github.com/hyeok9855/gfn-smc-jax", "summary": "This paper proposes a synergy of amortised and particle-based methods for\nsampling from distributions defined by unnormalised density functions. We state\na connection between sequential Monte Carlo (SMC) and neural sequential\nsamplers trained by maximum-entropy reinforcement learning (MaxEnt RL), wherein\nlearnt sampling policies and value functions define proposal kernels and twist\nfunctions. Exploiting this connection, we introduce an off-policy RL training\nprocedure for the sampler that uses samples from SMC -- using the learnt\nsampler as a proposal -- as a behaviour policy that better explores the target\ndistribution. We describe techniques for stable joint training of proposals and\ntwist functions and an adaptive weight tempering scheme to reduce training\nsignal variance. Furthermore, building upon past attempts to use experience\nreplay to guide the training of neural samplers, we derive a way to combine\nhistorical samples with annealed importance sampling weights within a replay\nbuffer. On synthetic multi-modal targets (in both continuous and discrete\nspaces) and the Boltzmann distribution of alanine dipeptide conformations, we\ndemonstrate improvements in approximating the true distribution as well as\ntraining stability compared to both amortised and Monte Carlo methods."}
{"id": "2510.11593", "categories": ["quant-ph", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11593", "abs": "https://arxiv.org/abs/2510.11593", "authors": ["Seong-Joon Park", "Hee-Youl Kwak", "Yongjune Kim"], "title": "Hierarchical Qubit-Merging Transformer for Quantum Error Correction", "comment": "6 pages, 5 figures", "summary": "For reliable large-scale quantum computation, a quantum error correction\n(QEC) scheme must effectively resolve physical errors to protect logical\ninformation. Leveraging recent advances in deep learning, neural network-based\ndecoders have emerged as a promising approach to enhance the reliability of\nQEC. We propose the Hierarchical Qubit-Merging Transformer (HQMT), a novel and\ngeneral decoding framework that explicitly leverages the structural graph of\nstabilizer codes to learn error correlations across multiple scales. Our\narchitecture first computes attention locally on structurally related groups of\nstabilizers and then systematically merges these qubit-centric representations\nto build a global view of the error syndrome. The proposed HQMT achieves\nsubstantially lower logical error rates for surface codes by integrating a\ndedicated qubit-merging layer within the transformer architecture. Across\nvarious code distances, HQMT significantly outperforms previous neural\nnetwork-based QEC decoders as well as a powerful belief propagation with\nordered statistics decoding (BP+OSD) baseline. This hierarchical approach\nprovides a scalable and effective framework for surface code decoding,\nadvancing the realization of reliable quantum computing."}
