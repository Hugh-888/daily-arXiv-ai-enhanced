<div id=toc></div>

# Table of Contents

- [quant-ph](#quant-ph) [Total: 37]
- [cs.LG](#cs.LG) [Total: 78]
- [gr-qc](#gr-qc) [Total: 15]
- [physics.comp-ph](#physics.comp-ph) [Total: 5]


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [1] [Neutrino thermalization via randomization on a quantum processor](https://arxiv.org/abs/2510.24841)
*Oriel Kiss,Ivano Tavernelli,Francesco Tacchino,Denis Lacroix,Alessandro Roggero*

Main category: quant-ph

TL;DR: 使用随机量子电路模拟超新星中微子味热化，发现热化时间随系统尺寸平方根增长，验证了半经典方法预测。


<details>
  <summary>Details</summary>
Motivation: 模拟超新星中微子味演化的全连接自旋哈密顿量动力学是重大挑战，传统方法需要电路深度随系统尺寸线性增长，超出当前量子设备能力。

Method: 使用随机量子电路作为经验工具来模拟非局域动力学，通过模拟超过100个量子比特的动力学来研究味热化行为。

Result: 发现热化行为可以用与系统尺寸无关的电路深度重现，热化时间随系统尺寸平方根增长，与半经典方法预测一致。

Conclusion: 近期量子设备是验证经验经典方法的有用工具，随机电路在物理中提供了新的应用，为经典难解复杂多体动力学提供洞见。

Abstract: The dynamical evolution of neutrino flavor in supernovae can be modeled by an
all-to-all spin Hamiltonian with random couplings. Simulating such two-local
Hamiltonian dynamics remains a major challenge, as methods with controllable
accuracy require circuit depths that increase at least linearly with system
size, exceeding the capabilities of current quantum devices. The eigenstate
thermalization hypothesis predicts that these systems should thermalize, a
behavior confirmed in small-scale classical simulations. In this work, we
investigate flavor thermalization in much larger systems using random quantum
circuits as an empirical tool to emulate the non-local dynamics, and
demonstrate that the thermal behavior can be reproduced using a depth
independent of the system size. By simulating dynamics of over one hundred
qubits, we find that the thermalization time grows approximately as the square
root of the system size, consistent with predictions from semi-classical
methods. Beyond this specific result, our study illustrates that near-term
quantum devices are useful tools to test and validate empirical classical
methods. It also highlights a new application of random circuits in physics,
providing insight into complex many-body dynamics that are classically
intractable.

</details>


### [2] [Frustration-Free Control and Absorbing-State Transport in Entangled State Preparation](https://arxiv.org/abs/2510.24845)
*T. Dörstel,T. Iadecola,J. H. Wilson,M. Buchhold*

Main category: quant-ph

TL;DR: 该论文提出了一种无挫败控制协议，将无挫败哈密顿量的概念扩展到随机动力学，通过测量反馈驱动多体系统进入高度纠缠的目标态，无需后选择。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过测量反馈协议高效制备量子纠缠态，将无挫败哈密顿量的概念扩展到随机动力学领域。

Method: 使用测量反馈协议，结合局部幺正修正，实现吸收态动力学。通过SU(N) SWAP测量和局部修正的基线模型，映射到可解的吸收随机游走。

Result: 弛豫到目标态由非局域荷的涌现输运主导，SU(2)对称动力学中单重态激发的输运。SU(N) SWAP模型显示运行时间标度t∼L^z，输运指数z=2。Motzkin和Fredkin链显示亚扩散标度z≥8/3。

Conclusion: 测量和混洗幺正操作诱导荷输运，决定了收敛时间。这为受监控量子动力学中的可控纠缠态制备和荷输运探测提供了策略。

Abstract: We study frustration-free control, a measurement-feedback protocol for
quantum state preparation that extends the concept of frustration-free
Hamiltonians to stochastic dynamics. The protocol drives many-body systems into
highly entangled target states, common dark states of all measurement
projectors, through minimal local unitary corrections that realize an
absorbing-state dynamics without post-selection. We show that relaxation to the
target state is governed by emergent transport of nonlocal charges, such as
singlet excitations in SU$(2)$-symmetric dynamics. While measurement-feedback
annihilates compatible charge configurations, both measurement and scrambling
unitaries induce charge transport and thus determine the convergence time.
Mapping a baseline model of SU$(N)$ SWAP measurements with local corrections to
a solvable absorbing random walk yields a runtime scaling $t \sim L^z$ with
transport exponent $z=2$. Simulations of Motzkin and Fredkin chains reveal
subdiffusive scaling $z \ge \tfrac{8}{3}$, confirming the transport picture and
suggesting strategies for controlled entangled-state preparation and
charge-transport probing in monitored quantum dynamics.

</details>


### [3] [Classically Prepared, Quantumly Evolved: Hybrid Algorithm for Molecular Spectra](https://arxiv.org/abs/2510.24911)
*Alessandro Santini,Stefano Barison,Filippo Vicentini*

Main category: quant-ph

TL;DR: 提出一种混合经典-量子算法，用于计算多体量子系统中的动力学关联函数和激发谱，特别适用于分子系统。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理多体量子系统的长期动力学时面临计算复杂度挑战，需要开发能在近期量子硬件上高效运行的算法。

Method: 结合经典方法准备扰动基态，通过短时间量子演化对从中采样的乘积态进行演化，在希尔伯特空间中构建有效子空间，然后进行经典模拟。

Result: 在分子系统基准测试中与精确对角化结果高度一致，能够访问超出纯经典方法范围的动力学时间尺度。

Conclusion: 该方法使用浅层电路和少量样本即可实现高分辨率谱重构，适用于近期和早期容错量子硬件。

Abstract: We introduce a hybrid classical-quantum algorithm to compute dynamical
correlation functions and excitation spectra in many-body quantum systems, with
a focus on molecular systems. The method combines classical preparation of a
perturbed ground state with short-time quantum evolution of product states
sampled from it. The resulting quantum samples define an effective subspace of
the Hilbert space, onto which the Hamiltonian is projected to enable efficient
classical simulation of long-time dynamics. This subspace-based approach
achieves high-resolution spectral reconstruction using shallow circuits and few
samples. Benchmarks on molecular systems show excellent agreement with exact
diagonalization and demonstrate access to dynamical timescales beyond the reach
of purely classical methods, highlighting its suitability for near-term and
early fault-tolerant quantum hardware.

</details>


### [4] [Pairing-induced phase transition in the non-reciprocal Kitaev chain](https://arxiv.org/abs/2510.24851)
*Pietro Brighi,Andreas Nunnenkamp*

Main category: quant-ph

TL;DR: 研究了非互易性在竞争相互作用下的鲁棒性，揭示了配对诱导的相变，发现了非互易相和密度波相，以及配对非互易性对关联传播的显著影响。


<details>
  <summary>Details</summary>
Motivation: 研究非互易量子物质在竞争相互作用下的鲁棒性，理解非互易性如何受到超导配对的影响。

Method: 使用储层工程在费米子Kitaev链中诱导非互易跳跃和配对，通过非厄米Kitaev哈密顿量描述关联动力学。

Result: 发现了由异常点分隔的两个相：非互易相具有方向性和慢弛豫特性，稳态支持非互易密度和空间关联；强配对时出现密度波相，具有短弛豫时间和粒子占据调制。

Conclusion: 工作突出了超导配对导致非互易性的非平凡破坏，鼓励对非互易费米子系统的实验研究。

Abstract: Investigating the robustness of non-reciprocity in the presence of competing
interactions is central to understanding non-reciprocal quantum matter. In this
work, we use reservoir engineering to induce non-reciprocal hopping and pairing
in the fermionic Kitaev chain, and reveal the emergence of a pairing-induced
phase transition. The two phases appear in the spectrum of the non-Hermitian
Kitaev Hamiltonian describing the dynamics of correlations, separated by an
exceptional point. In the non-reciprocal phase, dynamics are characterized by
directionality and slow relaxation, and the steady state supports
non-reciprocal density and spatial correlations. At strong pairing, we uncover
an unexpected density wave phase, featuring short relaxation times, a
modulation in particle occupation and strikingly different correlation
spreading depending on pairing non-reciprocity. Our work highlights the
non-trivial breakdown of non-reciprocity due to superconducting pairing and
invites experimental investigation of non-reciprocal fermionic systems.

</details>


### [5] [Variational quantum computing for quantum simulation: principles, implementations, and challenges](https://arxiv.org/abs/2510.25449)
*Lucas Q. Galvão,Anna Beatriz M. de Souza,Marcelo A. Moret,Clebson Cruz*

Main category: quant-ph

TL;DR: 本文全面综述了变分量子计算及其在量子模拟中的关键作用，重点探讨了量子数据在变分量子算法和量子机器学习中的重要性，分析了NISQ时代下的挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 在嘈杂中等规模量子（NISQ）时代，变分量子算法提供了一种有前景的混合量子-经典计算框架，但实际应用仍受限于可训练性和可扩展性挑战。

Method: 采用系统性的综述方法，阐述变分量子计算的基本原理，在混合量子-经典框架下分析各类典型量子模拟问题的应用。

Result: 该综述综合了该领域最新进展，强调了变分量子算法在量子模拟中的潜力，同时指出了噪声和贫瘠高原等约束下的实际限制。

Conclusion: 变分量子计算为量子模拟提供了有前景但问题依赖的路径，其实际可行性取决于在噪声和贫瘠高原约束下的可训练性和可扩展性。

Abstract: This work presents a comprehensive overview of variational quantum computing
and their key role in advancing quantum simulation. This work explores the
simulation of quantum systems and sets itself apart from approaches centered on
classical data processing, by focusing on the critical role of quantum data in
Variational Quantum Algorithms (VQA) and Quantum Machine Learning (QML). We
systematically delineate the foundational principles of variational quantum
computing, establish their motivational and challenges context within the noisy
intermediate-scale quantum (NISQ) era, and critically examine their application
across a range of prototypical quantum simulation problems. Operating within a
hybrid quantum-classical framework, these algorithms represent a promising yet
problem-dependent pathway whose practicality remains contingent on trainability
and scalability under noise and barren-plateau constraints.This review serves
to complement and extend existing literature by synthesizing the most recent
advancements in the field and providing a focused perspective on the persistent
challenges and emerging opportunities that define the current landscape of
variational quantum computing for quantum simulation.

</details>


### [6] [Structure, Optimality, and Symmetry in Shadow Unitary Inversion](https://arxiv.org/abs/2510.24880)
*Guocheng Zhen,Yu-Ao Chen,Mingrui Jing,Jingu Xie,Ranyiliu Chen,Xin Wang*

Main category: quant-ph

TL;DR: 本文系统研究了量子阴影酉反演问题，提出了确定性协议和优化方法，包括三查询最优的量子比特酉反演协议、量子比特可观测量的完整特征化，以及高维情况的半定规划框架。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注未知酉操作的逆映射实现，但如何针对给定可观测量的酉反演（阴影酉反演）这一基本问题尚未充分发展，需要系统研究。

Method: 提出了确定性协议和优化问题简化方法：1）三查询的量子比特酉反演确定性协议；2）量子比特阴影反演可行操作的完整特征化；3）利用表示论工具的高维情况半定规划优化框架。

Result: 开发了量子比特酉反演的最优三查询协议，建立了量子比特可观测量的完整理论特征，并为高维情况提供了有效的优化计算框架。

Conclusion: 系统建立了阴影酉反演的理论框架和实用协议，解决了量子计算中针对可观测量的酉反演这一基本问题，为量子信息处理提供了重要工具。

Abstract: The ability to reverse any unknown unitary operation plays a fundamental role
in quantum computing. While existing studies mostly focus on realizing the
inversion map of the unknown unitary, how to reverse a unitary with respect to
a given observable, which we call shadow unitary inversion, has remained a
natural basic question that is less developed. In this work, we systematically
investigate shadow unitary inversion by providing explicit protocols and
optimization problem simplification. First, we present a deterministic protocol
for shadow inversion of qubit-unitaries. Such construction sequentially queries
the unitary 3 times, which is suggested to be optimal by our numerical
experiments. Second, we provide a complete characterization of feasible quantum
operations for qubit shadow inversion under any fixed qubit observable. Third,
for the qudit case, we give a framework of semidefinite programming for
optimizing the shadow unitary inversion sequential protocol for tackling
high-dimensional cases, utilizing tools from representation theory.

</details>


### [7] [Quantifying Unxtendibility via Virtual State Extension](https://arxiv.org/abs/2510.24895)
*Hongshun Yao,Jingu Xie,Xuanqiang Zhao,Chengkai Zhu,Ranyiliu Chen,Xin Wang*

Main category: quant-ph

TL;DR: 本文提出了虚拟扩展成本的概念，通过虚拟状态扩展任务来量化量子态的可扩展性程度，建立了纠缠理论与量子通信理论之间的深刻联系。


<details>
  <summary>Details</summary>
Motivation: 传统纠缠单调性限制了纠缠在多方之间的共享方式，本文旨在通过操作化视角来量化态的非可扩展性程度。

Method: 引入虚拟状态扩展任务，定义虚拟扩展成本，利用部分转置置换矩阵代数的代数工具进行分析，构造最优广播协议的量子电路。

Result: 得到了各向同性态虚拟扩展成本的精确闭式表达式，证明了最大纠缠态的虚拟扩展成本等于通用虚拟量子广播的最优模拟成本。

Conclusion: 研究结果统一了不同的概念，为纠缠量化提供了新的视角，赋予了不可扩展性的绝对鲁棒性以清晰的操作意义。

Abstract: The monogamy of entanglement, which restricts how entanglement can be shared
among multiple parties, provides a powerful lens through which to characterize
its structure, often formalized through the concept of symmetric extendibility.
In this work, we propose a novel, operational viewpoint to quantify the degree
to which a state is not extendible by introducing a virtual state extension
task. We define the virtual extension cost as the minimum simulation cost of a
non-physical protocol that satisfies the marginal conditions of a
$k$-extension. We derive an exact, closed-form expression for this cost for the
important family of isotropic states. Our central result establishes a profound
connection between entanglement theory and quantum communication theory: the
virtual extension cost of a maximally entangled state is precisely equal to the
optimal simulation cost of universal virtual quantum broadcasting. Leveraging
the algebraic machinery of the partially transposed permutations matrix
algebra, we find an analytical formula for this cost and construct an explicit
quantum circuit for the optimal broadcasting protocol, thereby resolving an
open question. Furthermore, we demonstrate the connection between the virtual
extension cost and the absolute robustness of unextendibility, thus endowing
the latter with a clear operational meaning. We also show the natural
properties of virtual extension cost as an entanglement measure, including its
role as a bound for entanglement distillation and its relationship with
logarithmic negativity. Our findings unify disparate concepts and offer a new
perspective on entanglement quantification.

</details>


### [8] [Universal Limits on Quantum Correlations](https://arxiv.org/abs/2510.24950)
*Samuel Alperin*

Main category: quant-ph

TL;DR: 提出了一个基于量子态空间正性的统一几何框架，从单一几何原理推导出所有已知和新的量子关联极限，包括Cramer-Rao不等式、海森堡极限和Lieb-Robinson界。


<details>
  <summary>Details</summary>
Motivation: 现有量子关联极限（如Cramer-Rao不等式、海森堡极限、Lieb-Robinson界）各自适用于狭窄的系统或可观测量类别，缺乏统一的数学基础。

Method: 利用量子态空间的正性几何，定义了一个独特的行列式比不变量χ，量化任意量子系统中关联的组合结构。通过李群对称性获得紧凑的闭式边界。

Result: 恢复了海森堡和Cramer-Rao极限，发现了新的约束条件，包括多模压缩网络中的精确纠缠下限和全连接自旋系综中的通用Fisher信息上限。

Conclusion: 正性几何为量子极限提供了一个统一的、基于第一性原理的理论框架，所有关联边界都表现出局部突变理论结构。

Abstract: The fundamental limits of quantum correlations set the foundation of quantum
mechanics and quantum information science. Exact bounds-the Cramer-Rao
inequality, the Heisenberg limit, and the Lieb-Robinson bound-have anchored
entire fields, yet each applies only to a narrow class of systems or
observables. Here we introduce a general framework from which all known
correlation limits, as well as new ones, can be derived from a single geometric
principle: the positivity of quantum state space. This intrinsic positive
geometry defines a unique determinant-ratio invariant, denoted chi, which
quantifies the combinatorial structure of correlations in any quantum system.
Every measure of nonclassical correlation is bounded by a simple function of
chi, yielding universal, model-independent floors and ceilings valid for
arbitrary architectures. For systems with Lie-group symmetries, the bounds
acquire compact closed forms. We recover the Heisenberg and Cramer-Rao limits
and uncover previously unknown constraints, including an exact entanglement
floor in multimode squeezing networks and a universal Fisher-information
ceiling in fully connected spin ensembles-demonstrating that even all-to-all
connectivity cannot exceed the positivity-imposed light cone in state space.
Finally, we show that every correlation bound, old or new, exhibits local
catastrophe-theoretic structure, with universal critical exponents classifying
its approach to saturation. Positivity geometry thus provides a unified,
first-principles theory of quantum limits.

</details>


### [9] [Engineering chlorine-based emitters in silicon carbide for telecom-band quantum technologies](https://arxiv.org/abs/2510.25008)
*A. N. Anisimov,A. V. Mathews,K. Mavridou,U. Kentsch,M. Helm,G. V. Astakhov*

Main category: quant-ph

TL;DR: 在4H-SiC中通过氯离子注入和高温退火成功制备了氯空位(ClV)色心，这些色心在光纤通信波段发光，具有四种不同构型，分布在O、S、C波段。


<details>
  <summary>Details</summary>
Motivation: 开发与CMOS工艺兼容的电信波段色心，用于可扩展量子网络应用。

Method: 采用氯离子注入和高温退火工艺在4H-SiC中创建ClV色心，通过光致发光光谱进行表征。

Result: 成功制备出四种不同构型的ClV色心，其零声子线分别位于O、S、C波段，在30K温度下仍保持稳定的发光强度。

Conclusion: ClV色心作为一种新型电信波段色心，在CMOS兼容平台上展现出构建可扩展量子网络的巨大潜力。

Abstract: We report the experimental realization and optical characterization of
chlorine-vacancy (ClV) color centers in 4H-SiC emitting in the fiber-optic
telecom bands. These defects are created via chlorine ion implantation followed
by high-temperature annealing. Photoluminescence spectroscopy reveals four
distinct ClV configurations with zero-phonon lines (ZPLs) located in the O-band
(1260 - 1360 nm), S-band (1460 - 1530 nm) and C-band (1530 - 1565 nm).
Controlled implantation and annealing experiments confirm that the ClV centers
originate specifically from chlorine incorporation into SiC and are not
intrinsic to this material. We optimize the creation conditions for ClV
ensembles and demonstrate negligible reduction of the ZPL intensity up to a
temperature of 30 K. These results establish ClV defects as a new class of
telecom-band color centers in a CMOS-compatible platform, offering strong
potential for scalable quantum networks.

</details>


### [10] [Properties and Applications of Partially Deterministic Polytopes](https://arxiv.org/abs/2510.25127)
*Marwan Haddara,Howard M. Wiseman,Eric G. Cavalcanti*

Main category: quant-ph

TL;DR: 该论文系统研究了部分确定性概念，构建了新的凸多面体类别，推广了Fine定理，并讨论了在量子基础、设备无关量子态不可分性见证等场景中的应用。


<details>
  <summary>Details</summary>
Motivation: 探索当每个位点只有部分测量的输出由模型预先确定时的统计约束，这比传统的贝尔局域性假设更一般化。

Method: 通过引入部分确定性概念，构建了新的凸多面体类别，分类了非平凡等价类，并推广了Fine定理。

Result: 发现了贝尔多面体可以用局部部分确定性模型的多种不同方式表达，建立了部分确定性多面体与顺序扩展维格纳朋友场景的一一对应关系。

Conclusion: 部分确定性框架捕捉了广泛的非经典性概念，识别了更广泛的'可组合集合'概念，其中部分确定性多面体是特例。

Abstract: The assumption of a deterministic local hidden variable model constrains the
experimentally accessible statistics in a Bell experiment to be contained in
the Bell-local polytope. But what if the outputs for only a subset of the
measurements at each site are predetermined by the model? In this work, we
thoroughly explore this concept of `partial determinism', allowing for
arbitrary numbers of parties, inputs and outputs per site. The resulting
objects form new classes of convex polytopes which recover the Bell and the
no-signalling polytopes as special cases. Nontrivial equivalence classes of
partially deterministic models arise, which we classify completely. In
particular, the Bell polytope for any scenario can be expressed in multiple
different ways in terms of local partially deterministic models. This allows us
to generalise Fine's theorem, recovering the original formulation as a special
case, but finding new constraints otherwise. We discuss scenarios with
different physical motivations, which do not require the causal structure of
the Bell scenario, and where classes of partially deterministic polytopes are
relevant. Our example applications include device-independent quantum state
inseparability witnesses, classes of broadcast-local polytopes, and Local
Friendliness scenarios in quantum foundations. We also point out instances in
previous literature where classes of related objects have been studied. In the
case of correlations compatible with the Local Friendliness assumptions, we
find a one-to-one correspondence between partially deterministic polytopes and
sequential extended Wigner's friend scenarios so that every partially
deterministic polytope has physical relevance. We discuss how the framework
captures a broad class of non-classicality notions, and identify an even
broader notion of `composable sets', of which partially deterministic polytopes
are special cases.

</details>


### [11] [The Phase-Coupled Caldeira-Leggett Model: Non-Markovian Open Quantum Dynamics beyond Linear Dissipation](https://arxiv.org/abs/2510.25133)
*Ao-Xiang Chang,Yu Su,Zi-Fan Zhu,Yao Wang,Rui-Xue Xu,YiJing Yan*

Main category: quant-ph

TL;DR: 提出了相位耦合Caldeira-Leggett模型，这是一种具有指数型系统-浴耦合的量子耗散模型，为研究超出线性响应范围的相位介导耗散和退相干提供了通用平台。


<details>
  <summary>Details</summary>
Motivation: 传统Caldeira-Leggett模型采用线性系统-浴耦合，限制了其在非线性响应范围的应用。PCL模型通过指数耦合统一了量子布朗运动和极化子物理的概念。

Method: 利用环境的高斯性质，在耗散准粒子代数框架内对PCL模型进行非微扰和非马尔可夫处理，得到了约化密度算符的精确封闭形式运动方程。

Result: 数值模拟显示PCL模型展现出与传统Caldeira-Leggett模型显著不同的独特动力学行为。

Conclusion: PCL模型为研究相位介导的耗散和退相干提供了超越线性响应范围的理论框架，其精确解揭示了非线性系统-浴耦合带来的新物理现象。

Abstract: We introduce the \textit{Phase-Coupled Caldeira-Leggett} (PCL) model of
quantum dissipation and develop an exact framework for its dynamics. Unlike the
conventional Caldeira-Leggett model with linear system-bath coupling
$H_{\mathrm{SB}}\propto\hat F$, the PCL model features an exponential
interaction $H_{\mathrm{SB}}\propto e^{i\lambda \hat F}$, where $\hat F$
denotes the collective bath coordinate. This model unifies concepts from
quantum Brownian motion and polaron physics, providing a general platform to
study phase-mediated dissipation and decoherence beyond the linear-response
regime. Despite its nonlinear system-bath coupling, the Gaussian nature of the
environment allows a nonperturbative and non-Markovian treatment of PCL model
within the algebra of dissipative quasiparticles. We obtain an exact
closed-form equation of motion for the reduced density operator, and numerical
simulations reveal distinctive dynamical behaviors that deviate markedly from
those predicted by the conventional Caldeira-Leggett model.

</details>


### [12] [Sustainable NARMA-10 Benchmarking for Quantum Reservoir Computing](https://arxiv.org/abs/2510.25183)
*Avyay Kodali,Priyanshi Singh,Pranay Pandey,Krishna Bhatia,Shalini Devendrababu,Srinjoy Ganguly*

Main category: quant-ph

TL;DR: 比较量子储层计算(QRC)与经典模型(ESN、LSTM)及混合量子-经典架构(QLSTM)在NARMA-10任务上的性能，评估预测精度、计算成本和评估时间。


<details>
  <summary>Details</summary>
Motivation: 探索量子计算在时间序列预测中的潜力，特别是量子储层计算在资源受限环境下的可持续性优势。

Method: 使用量子储层计算与经典回声状态网络、长短期记忆网络以及混合量子-经典架构进行对比实验，在非线性自回归移动平均任务(NARMA-10)上评估性能指标。

Result: 量子储层计算在保持竞争性精度的同时，在资源受限环境中展现出可持续性优势。

Conclusion: 量子储层计算在可持续时间序列AI应用中具有发展前景，特别是在计算资源有限的环境中。

Abstract: This study compares Quantum Reservoir Computing (QRC) with classical models
such as Echo State Networks (ESNs) and Long Short-Term Memory networks (LSTMs),
as well as hybrid quantum-classical architectures (QLSTM), for the nonlinear
autoregressive moving average task (NARMA-10). We evaluate forecasting accuracy
(NRMSE), computational cost, and evaluation time. Results show that QRC
achieves competitive accuracy while offering potential sustainability
advantages, particularly in resource-constrained settings, highlighting its
promise for sustainable time-series AI applications.

</details>


### [13] [Platform Architecture for Tight Coupling of High-Performance Computing with Quantum Processors](https://arxiv.org/abs/2510.25213)
*Shane A. Caldwell,Moein Khazraee,Elena Agostini,Tom Lassiter,Corey Simpson,Omri Kahalon,Mrudula Kanuri,Jin-Sung Kim,Sam Stanwyck,Muyuan Li,Jan Olle,Christopher Chamberland,Ben Howe,Bruno Schmitt,Justin G. Lietz,Alex McCaskey,Jun Ye,Ang Li,Alicia B. Magann,Corey I. Ostrove,Kenneth Rudinger,Robin Blume-Kohout,Kevin Young,Nathan E. Miller,Yilun Xu,Gang Huang,Irfan Siddiqi,John Lange,Christopher Zimmer,Travis Humble*

Main category: quant-ph

TL;DR: NVQLink是一个连接高性能计算资源与量子处理单元控制系统的架构，支持所有物理模态的QPU和QSC类型，通过以太网实现微秒级延迟的实时处理，并扩展CUDA-Q编程模型以支持HPC与QSC间的实时回调。


<details>
  <summary>Details</summary>
Motivation: 为量子处理单元的控制系统提供高性能计算加速支持，解决传统HTTP接口性能限制问题，实现CPU、GPU和FPGA子系统的统一编程。

Method: 基于商用以太网构建HPC与QSC间的网络连接，扩展CUDA-Q编程模型支持实时回调，采用多级中间表示方言和渐进式降低来封装QSC代码。

Result: 实现了最大3.96微秒的往返延迟测量，并有望进一步优化，成功扩展了异构核编程到QSC。

Conclusion: NVQLink架构为QPU控制系统提供了高效的HPC加速解决方案，简化了QSC构建者的集成过程，提升了量子计算系统的整体性能。

Abstract: We propose an architecture, called NVQLink, for connecting high-performance
computing (HPC) resources to the control system of a quantum processing unit
(QPU) to accelerate workloads necessary to the operation of the QPU. We aim to
support every physical modality of QPU and every type of QPU system controller
(QSC). The HPC resource is optimized for real-time (latency-bounded) processing
on tasks with latency tolerances of tens of microseconds. The network
connecting the HPC and QSC is implemented on commercially available Ethernet
and can be adopted relatively easily by QPU and QSC builders, and we report a
round-trip latency measurement of 3.96 microseconds (max) with prospects of
further optimization. We describe an extension to the CUDA-Q programming model
and runtime architecture to support real-time callbacks and data marshaling
between the HPC and QSC. By doing so, NVQLink extends heterogeneous,
kernel-based programming to the QSC, allowing the programmer to address CPU,
GPU, and FPGA subsystems in the QSC, all in the same C++ program, avoiding the
use of a performance-limiting HTTP interface. We provide a pattern for QSC
builders to integrate with this architecture by making use of multi-level
intermediate representation dialects and progressive lowering to encapsulate
QSC code.

</details>


### [14] [Decoder Switching: Breaking the Speed-Accuracy Tradeoff in Real-Time Quantum Error Correction](https://arxiv.org/abs/2510.25222)
*Riki Toshio,Kaito Kishi,Jun Fujisaki,Hirotaka Oshima,Shintaro Sato,Keisuke Fujii*

Main category: quant-ph

TL;DR: 提出解码器切换框架，结合快速弱解码器和慢速强解码器，通过软信息评估可靠性，仅在低可靠性时切换至强解码器，实现高精度与高速的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决容错量子计算机中解码器速度与精度之间的根本性权衡问题，传统方法难以同时满足高速和高精度的需求。

Method: 采用解码器切换框架，包含快速软输出弱解码器和慢速高精度强解码器，通过软信息评估解码可靠性，仅在必要时切换至强解码器。

Result: 数值模拟显示该框架能达到与强解码器相当甚至更高的精度，同时平均解码时间与弱解码器相当。

Conclusion: 该框架打破了长期存在的速度-精度权衡，为可扩展的实时解码设备铺平了道路。

Abstract: The realization of fault-tolerant quantum computers hinges on the
construction of high-speed, high-accuracy, real-time decoding systems. The
persistent challenge lies in the fundamental trade-off between speed and
accuracy: efforts to improve the decoder's accuracy often lead to unacceptable
increases in decoding time and hardware complexity, while attempts to
accelerate decoding result in a significant degradation in logical error rate.
To overcome this challenge, we propose a novel framework, decoder switching,
which balances these competing demands by combining a faster, soft-output
decoder ("weak decoder") with a slower, high-accuracy decoder ("strong
decoder"). In usual rounds, the weak decoder processes error syndromes and
simultaneously evaluates its reliability via soft information. Only when
encountering a decoding window with low reliability do we switch to the strong
decoder to achieve more accurate decoding. Numerical simulations suggest that
this framework can achieve accuracy comparable to, or even surpassing, that of
the strong decoder, while maintaining an average decoding time on par with the
weak decoder. We also develop an online decoding scheme tailored to our
framework, named double window decoding, and elucidate the criteria for
preventing an exponential slowdown of quantum computation. These findings break
the long-standing speed-accuracy trade-off, paving the way for scalable
real-time decoding devices.

</details>


### [15] [Encoding computationally hard problems in triangular Rydberg atom arrays](https://arxiv.org/abs/2510.25249)
*Xi-Wei Pan,Huan-Hai Zhou,Yi-Ming Lu,Jin-Guo Liu*

Main category: quant-ph

TL;DR: 本文提出了一种在三角晶格上编码计算难题的新方案，相比King's子图，能显著减少独立约束违反，降低实验后处理需求。


<details>
  <summary>Details</summary>
Motivation: Rydberg原子阵列是量子优化的有前景平台，但King's子图在二维中不是最优选择，且Rydberg相互作用的幂律衰减导致单元盘图近似较差，需要缺乏物理解释的后处理。

Method: 开发了基于创新自动化小工具搜索策略的编码方案，可在三角晶格上普遍编码计算难题。

Result: 数值模拟显示，三角晶格上的量子优化相比King's子图将独立约束违反减少了约两个数量级。

Conclusion: 该编码方案显著减轻了实验中的后处理需求，为Rydberg原子阵列的量子优化提供了更有效的实现途径。

Abstract: Rydberg atom arrays are a promising platform for quantum optimization,
encoding computationally hard problems by reducing them to independent set
problems with unit-disk graph topology. In Nguyen et al., PRX Quantum 4, 010316
(2023), a systematic and efficient strategy was introduced to encode multiple
problems into a special unit-disk graph: the King's subgraph. However, King's
subgraphs are not the optimal choice in two dimensions. Due to the power-law
decay of Rydberg interaction strengths, the approximation to unit-disk graphs
in real devices is poor, necessitating post-processing that lacks physical
interpretability. In this work, we develop an encoding scheme that can
universally encode computationally hard problems on triangular lattices, based
on our innovative automated gadget search strategy. Numerical simulations
demonstrate that quantum optimization on triangular lattices reduces
independence-constraint violations by approximately two orders of magnitude
compared to King's subgraphs, substantially alleviating the need for
post-processing in experiments.

</details>


### [16] [Statistical Physics from Quantum Envariance Principles](https://arxiv.org/abs/2510.25253)
*Amul Ojha,Shubhit Sardana,Arnab Ghosh*

Main category: quant-ph

TL;DR: 该论文基于Deffner和Zurek的工作，展示了如何通过环境辅助不变性(envariance)概念从量子力学推导出统计力学原理，包括二项分布、泊松分布和高斯分布的自然出现，并用量子纠缠熵解决吉布斯悖论。


<details>
  <summary>Details</summary>
Motivation: 旨在证明统计力学可以从量子力学基本原理推导出来，而不是基于现象学假设，强化统计力学源于量子信息动力学的观点。

Method: 使用环境辅助不变性(envariance)概念，分析纠缠系统-环境态，通过量子对称性推导统计分布和热力学关系。

Result: 成功推导出二项分布、泊松分布、高斯分布，解决了吉布斯悖论得到Sackur-Tetrode方程，推导了修正的Saha电离平衡方程，并恢复了玻色-爱因斯坦和费米-狄拉克统计。

Conclusion: 统计力学确实可以作为量子信息动力学的直接结果出现，而非基于独立的现象学假设，这为量子力学与统计力学之间的深刻联系提供了有力证据。

Abstract: We build on the foundational work of Deffner and Zurek [S.~Deffner and
W.~H.~Zurek, {New J.~Phys.18, 063013 (2016)}] to demonstrate how the principles
of statistical mechanics can be derived from quantum mechanics using the
concept of envariance (environment-assisted invariance). In particular, we show
how the Binomial, Poisson, and Gaussian distributions naturally emerge from
entangled system--environment states. Furthermore, we resolve the Gibbs paradox
using entanglement entropy, obtaining the Sackur--Tetrode equation with quantum
corrections. Extending this framework, we derive a modified Saha equation for
ionization equilibrium and recover Bose--Einstein and Fermi--Dirac statistics
from quantum symmetries. Our results reinforce and extend the view that
statistical mechanics arises as a direct consequence of quantum information
dynamics, rather than being founded on phenomenological postulates.

</details>


### [17] [Design and Fabrication of Metal-Shielded Fiber-Cavity Mirrors for Ion-Trap Systems](https://arxiv.org/abs/2510.25294)
*Wei-Bin Chen,Ding Fang,Cheng-Hao Zhang,Jin-Ming Cui,Yun-Feng Huang,Chuan-Feng Li,Guang-Can Guo*

Main category: quant-ph

TL;DR: 本文提出了一种金属屏蔽微腔镜的设计和制造方法，成功实现了与微腔集成的离子阱系统，显著降低了离子加热速率，为可扩展量子网络提供了关键技术。


<details>
  <summary>Details</summary>
Motivation: 微腔中的囚禁离子是量子信息处理和量子网络的关键平台，但表面充电效应会破坏离子囚禁稳定性，附近介电材料会使离子加热速率急剧增加，阻碍了离子阱系统与微腔集成的实际应用。

Method: 设计和制造了金属屏蔽微腔镜，构建了与光纤法布里-珀罗腔集成的针形离子阱系统。

Result: 成功在腔内稳定囚禁单个离子，测量的离子加热速率比未屏蔽配置降低了一个数量级以上。

Conclusion: 这项工作为可扩展量子网络的全集成离子-光子接口建立了关键技术。

Abstract: Trapped ions in micro-cavities constitute a key platform for advancing
quantum information processing and quantum networking. By providing an
efficient light-matter interface within a compact architecture, they serve as
highly efficient quantum nodes with strong potential for scalable quantum
network. However, in such systems, ion trapping stability is often compromised
by surface charging effects, and nearby dielectric materials are known to cause
a dramatic increase in the ion heating rate by several orders of magnitude.
These challenges significantly hinder the practical implementation of ion trap
systems integrated with micro-cavities. To overcome these limitations, we
present the design and fabrication of metal-shielded micro-cavity mirrors,
enabling the stable realization of ion trap systems integrated with micro
cavities. Using this method, we constructed a needle ion trap integrated with
fiber Fabry-Perot cavity and successfully achieved stable trapping of a single
ion within the cavity. The measured ion heating rate was reduced by more than
an order of magnitude compared with unshielded configurations. This work
establishes a key technique toward fully integrated ion-photon interfaces for
scalable quantum network.

</details>


### [18] [Imaginarity measures induced by real part states and the complementarity relations](https://arxiv.org/abs/2510.25313)
*Jingyan Liu,Yue Sun,Jianwei Xu,Ming-Jing Zhao*

Main category: quant-ph

TL;DR: 提出了一种通过实部态构造虚数度量的方法，基于保真度定义虚数度量并分析其性质，在量子比特系统中给出了解析表达式，建立了与其他虚数度量的关系，并研究了在相互无偏基下的互补关系。


<details>
  <summary>Details</summary>
Motivation: 复数是量子力学中不可或缺的，虚数资源理论最近得到发展。本文旨在探索实部态在构造虚数度量中的作用，揭示虚数在相互无偏基下的物理约束。

Method: 通过实部态构造虚数度量，提出基于保真度的虚数度量方法，在量子比特系统中推导解析表达式，建立与其他虚数度量的关系，研究在相互无偏基下的互补关系。

Result: 获得了基于保真度的虚数度量的解析表达式，建立了与几何虚数、Tsallis相对熵虚数和迹范数虚数的关系，在低维系统中给出了相互无偏基下的互补关系。

Conclusion: 这项工作不仅突出了实部态在虚数资源理论中的重要作用，而且从物理上揭示了虚数在相互无偏基集合上的约束关系。

Abstract: Complex numbers are indispensable in quantum mechanics and the resource
theory of imaginarity has been developed recently. In this paper, we propose a
method to construct imaginary measures by real part states. Specifically, we
propose an imaginarity measure in terms of fidelity and explore its properties.
The analytical expression of the imaginarity measure is presented in qubit
systems. The relations between the proposed imaginarity measure and some other
imaginarity measures (such as geometric imaginarity, Tsallis relative entropy
imaginarity and trace norm imaginarity) are derived. The complementarity
relations of the imaginarity measure under a complete set of mutually unbiased
bases are provided in low-dimensional systems. This work not only highlights
the prominent role of the real part state in the imaginarity resource theory,
but also reveals the constraint of imaginarity on a complete set of mutually
unbiased bases physically.

</details>


### [19] [Quantum-correlated photons from spectrally-separated modes of a cavity coupled to a strongly-driven two-level atom](https://arxiv.org/abs/2510.25331)
*Alex Elliott,Jacob Ngaha,Scott Parkins,Takao Aoki*

Main category: quant-ph

TL;DR: 研究强驱动二能级原子中两个腔模的光子计数统计特性，发现腔模获得非经典光子统计，表现为反聚束和交叉相关值大于1的特性。


<details>
  <summary>Details</summary>
Motivation: 探索强驱动二能级原子系统中腔模的光子统计特性，揭示其非经典行为与原子跃迁的缀饰态图像之间的关系。

Method: 理论分析强驱动二能级原子与两个腔模耦合的系统，研究光子计数统计特性，并提出基于纳米光纤腔QED系统和铯原子的实验实现方案。

Result: 腔模获得非经典光子统计特性，表现为反聚束行为，同时交叉相关值大于1，这反映了缀饰态图像中的原子跃迁特性。

Conclusion: 强驱动二能级原子系统能够产生具有非经典统计特性的腔模光子，为量子光学应用提供了新的可能性。

Abstract: Photon counting statistics are explored, theoretically, from a pair of cavity
modes coupled to the fluorescent transitions in a strongly-driven two-level
atom. We show that the cavity modes acquire nonclassical photon statistics that
are representative of dressed-state picture atomic transitions. In particular,
the modes are shown to be antibunched, while simultaneously having a
cross-correlation value greater than unity. Furthermore, we propose an
implementation of the system with a nanofiber cavity QED system, based on a
strongly-driven cesium atom.

</details>


### [20] [Quantum Fisher Information With General Quantum Coherence in multi-dimensional quantum systems](https://arxiv.org/abs/2510.25457)
*Jun-Long Zhao,Li Yu,Ming Yang,Chui-Ping Yang*

Main category: quant-ph

TL;DR: 本文在多维量子系统中探索了量子Fisher信息(QFI)与量子相干性之间的关系，提出了广义量子相干(GQC)概念，并发现了GQC与QFI之间的严格平方关系。


<details>
  <summary>Details</summary>
Motivation: 量子计量学在量子参数估计精度中起关键作用，量子相干性是重要的量子特性，QFI是量子参数估计精度的重要指标。本文旨在探索多维量子系统中QFI与量子相干性的关系。

Method: 引入了广义量子相干(GQC)概念，该概念在相互作用过程中表征量子相干性和哈密顿量的本征能，捕捉高维量子态的量子性质并解决相干性测量的不足。

Result: 观察到GQC与QFI之间存在严格的平方关系，这一发现为改善参数估计精度提供了重要指导。

Conclusion: 提出的GQC概念有效表征了高维量子系统的量子特性，GQC与QFI的平方关系为量子参数估计精度的提升提供了理论基础和实用指南。

Abstract: Quantum metrology is a science about quantum measurements and it plays a key
role in precision of quantum parameter estimation. Meanwhile, quantum coherence
is an important quantum feature and quantum Fisher information (QFI) is an
important indicator for precision of quantum parameter estimation. In this
paper, we explore the relationship between QFI and quantum coherence in
multi-dimensional quantum systems. We introduce a new concept referred to as
General Quantum Coherence (GQC), which characterizes the quantum coherence and
the eigenenergies of the Hamiltonian in the interaction processes. GQC captures
quantum nature of high-dimensional quantum states and addresses shortcomings in
coherence measurement. Additionally, we observe a stringent square relationship
between GQC and QFI. This finding provides a crucial guideline for improving
the precision of parameter estimation.

</details>


### [21] [Generalized collective quantum tomography: algorithm design, optimization, and validation](https://arxiv.org/abs/2510.25466)
*Shuixin Xiao,Yuanlong Wang,Zhibo Hou,Aritra Das,Ian R. Petersen,Farhad Farokhi,Guo-Yong Xiang,Jie Zhao,Daoyi Dong*

Main category: quant-ph

TL;DR: 本文扩展了集体量子层析框架，提出了针对量子态、探测器和过程的集体层析优化算法，通过SOS技术获得最优解，实验证明相比现有方法具有更低的MSE。


<details>
  <summary>Details</summary>
Motivation: 集体量子层析能够通过联合测量多个副本更高效地提取信息，但现有框架局限于相同量子态的估计。本文旨在将其扩展到包含不同量子态、探测器或过程的更一般化场景。

Method: 将集体层析任务建模为优化问题，开发了三种分别针对量子态、探测器和过程的算法，使用SOS技术和半代数约束获得最优解，并通过双副本纠缠测量实验验证。

Result: 数值模拟和实验结果表明，所提算法相比现有方法实现了更低的均方误差，接近集体MSE界限，有效利用了纯度信息。

Conclusion: 本文提出的广义集体量子层析框架和优化算法在信息提取效率和估计精度方面优于传统方法，为量子技术表征提供了更强大的工具。

Abstract: Quantum tomography is a fundamental technique for characterizing,
benchmarking, and verifying quantum states and devices. It plays a crucial role
in advancing quantum technologies and deepening our understanding of quantum
mechanics. Collective quantum state tomography, which estimates an unknown
state \r{ho} through joint measurements on multiple copies
$\rho\otimes\cdots\otimes\rho$ of the unknown state, offers superior
information extraction efficiency. Here we extend this framework to a
generalized setting where the target becomes $S_1\otimes\cdots\otimes S_n$,
with each $S_i$ representing identical or distinct quantum states, detectors,
or processes from the same category. We formulate these tasks as optimization
problems and develop three algorithms for collective quantum state, detector
and process tomography, respectively, each accompanied by an analytical
characterization of the computational complexity and mean squared error (MSE)
scaling. Furthermore, we develop optimal solutions of these optimization
problems using sum of squares (SOS) techniques with semi-algebraic constraints.
The effectiveness of our proposed methods is demonstrated through numerical
examples. Additionally, we experimentally demonstrate the algorithms using
two-copy collective measurements, where entangled measurements directly provide
information about the state purity. Compared to existing methods, our
algorithms achieve lower MSEs and approach the collective MSE bound by
effectively leveraging purity information.

</details>


### [22] [Detuning Choice for solving MIS and MWIS](https://arxiv.org/abs/2510.25473)
*Sem Saada Khelkhal,Louis Barcikowsky*

Main category: quant-ph

TL;DR: 提出了在Pasqal中性原子处理器约束下实现最大加权独立集及其量子模拟的三种方法，考虑了硬件限制如有限量子比特数、参数边界和寄生相互作用。


<details>
  <summary>Details</summary>
Motivation: 研究如何在当前中性原子量子处理器的实际约束下实现MWIS问题，包括有限的量子比特数量、参数边界、序列时长等硬件限制，目标是获得与现有硬件直接兼容的结果。

Method: 引入了新的失谐计算方法，提出了三种实现方案：(I)理论性的局部失谐方法；(II)用于未来QPU集成的失谐映射调制方法；(III)当前实验可行的全局脉冲和频率偏移方法。

Result: 在多达30个量子比特的图上使用Pasqal模拟器进行测试，确认了这些方法在所有QPU约束下的实用性。

Conclusion: 提出的方法能够在中性原子量子处理器的实际约束下有效实现MWIS问题，为当前和未来的量子硬件提供了可行的解决方案。

Abstract: We study the realization of a Maximum Weighted Independent Set (MWIS) and its
quantum analogue under the constraints of Pasqal's neutral-atom processor:
limited qubit number, bounds on $\Omega$ and $\Delta$, sequence duration,
confinement space, minimum interatomic distance, and parasitic interactions.
Our goal is to obtain results directly compatible with current hardware, on
asymmetric graphs whose size is limited only by the QPU's topology. We
introduce a novel detuning computation method that departs from conventional
bounds, as parasitic interactions between nearby but unconnected atoms can
significantly bias results. Three implementations are proposed, suited to
different hardware maturity levels: (I) a speculative local-detuning approach
demonstrating the pure theory; (II) a \emph{Detuning Map Modulation} (DMM)
method approximating this theory for future QPU integration; and (III) a
global-pulse and frequency-shift approach, diverging from theory but
experimentally viable today. Tests using Pasqal's emulators on graphs up to
$30$ qubits confirm the practicality of our methods within all QPU constraints.

</details>


### [23] [Decoherence Estimation of Superconducting Qubit](https://arxiv.org/abs/2510.25491)
*Yoav Koral,Shilo Avraham,Manimuthu Peryasamy,Shmuel E. Schacham,Eliyahu Farber*

Main category: quant-ph

TL;DR: 本文分析了量子比特退相干过程，使用Caldeira-Leggett电路模型研究量子比特光子与寄生电阻原子间的物理相互作用，无需外部哈密顿量。


<details>
  <summary>Details</summary>
Motivation: 量子比特退相干主要由寄生电阻引起，需要理解其物理机制以改善量子计算性能。

Method: 使用Caldeira-Leggett电路模型分析量子比特光子与寄生电阻原子的相互作用，推导Lindblad主方程所需的发射和吸收速率。

Result: 分析显示电路噪声模型与Johnson-Nyquist噪声高度一致，数值计算结果与先前测量数据强相关。

Conclusion: 该分析为未来模拟提供了适当的电路特性推导方法，有助于理解和控制量子比特退相干。

Abstract: Decoherence of quantum bits arises primarily from the parasitic resistance
within the qubit. This study presents the analysis of the decoherence process
due to physical interactions between the qubit photons and parasitic resistance
atoms, utilizing exclusively the Caldeira-Leggett electrical model, without
relying on external Hamiltonians. The analysis shows a good agreement between
the model of the electrical noise and the Johnson-Nyquist noise. The emission
and absorption rates of the qubit's coherent loss, required for the Lindblad
master equation that approximates the decoherence, are obtained. A numerical
substitution in the analysis result yields a strong correlation with previous
measurements. The present analysis enables also the derivation of the
appropriate circuit characteristics for future simulations.

</details>


### [24] [Second-order Stark shifts exceeding 10$\,$GHz in electrically contacted SiV$^-$ centers in diamond](https://arxiv.org/abs/2510.25543)
*Manuel Rieger,Nori N. Chavira Leal,Rubek Poudel,Tobias Waldmann,Lina M. Todenhagen,Stefan Kresta,Viviana Villafane,Martin S. Brandt,Kai Müller,Jonathan J. Finley*

Main category: quant-ph

TL;DR: 本文展示了通过电场调控金刚石中硅空位中心(SiV⁻)的零声子线，利用面内电极施加高达45 MV/m的电场，实现了超过10 GHz的斯塔克位移，这足以克服SiV⁻在光学纳米结构中观察到的15 GHz不均匀分布。


<details>
  <summary>Details</summary>
Motivation: 金刚石中的带负电硅空位中心(SiV⁻)具有优异的自旋相干性和光学特性，是量子技术的有前景候选者。然而，应变引起的光学跃迁频率不均匀分布对可扩展性构成了挑战。

Method: 使用面内电极施加高达45 MV/m的电场来调控SiV⁻中心的零声子线，通过光致发光激发测量分析单个SiV⁻中心的极化率变化。

Result: 实现了超过10 GHz的二次斯塔克位移，与SiV⁻在光学纳米结构中观察到的15 GHz不均匀分布相当。单个SiV⁻中心的极化率存在显著变化，比锡空位中心大3-25倍。光学线宽随电场强度适度增加。

Conclusion: 大的电斯塔克位移可以克服跃迁频率的不均匀分布，这是实现基于SiV⁻的可扩展量子技术（如量子中继器）的重要一步。

Abstract: Negatively charged silicon vacancy centers (SiV$^-$) in diamond exhibit
excellent spin coherence and optical properties, making them promising
candidates for quantum technologies. However, the strain-induced inhomogeneous
distribution of optical transition frequencies poses a challenge for
scalability. We demonstrate electrical tuning of the SiV$^-$ center zero-phonon
lines using in-plane contacts to apply moderate electric fields up to
45$\,$MV/m. The second-order Stark shift exceeds 10$\,$GHz, which is of the
same order of magnitude as the 15$\,$GHz inhomogeneous distribution of SiV$^-$
observed in emitters embedded in optical nanostructures such as photonic
crystal nanocavities. Analysis of individual SiV$^-$ centers shows significant
variation in polarizabilities between defects indicating that the
polarizability strongly depends on local parameters like strain. The observed
polarizabilities are 3-25 times larger than those of tin vacancy centers, which
we attribute to valence band resonances that delocalize the $e_u$
wavefunctions. Photoluminescence excitation measurements reveal that optical
linewidths increase moderately with applied electric field strength. Our
results demonstrate that large electrical Stark shifts can overcome the
inhomogeneous distribution of transition frequencies, representing a
significant step toward scalable SiV$^-$-based quantum technologies such as
quantum repeaters.

</details>


### [25] [Super-Moiré Spin Textures in Twisted Antiferromagnets](https://arxiv.org/abs/2510.25545)
*King Cho Wong,Ruoming Peng,Eric Anderson,Jackson Ross,Bowen Yang,Meixin Cheng,Sreehari Jayaram,Malik Lenger,Xuankai Zhou,Yan Tung Kong,Takashi Taniguchi,Kenji Watanabe,Michael A. McGuire,Rainer Stöhr,Adam Wei Tsen,Elton J. G. Santos,Xiaodong Xu,Jörg Wrachtrup*

Main category: quant-ph

TL;DR: 在扭曲双层CrI3中发现了一种新型超莫尔磁态，其特征是跨越多个莫尔晶胞的长程磁性纹理，形成反铁磁奈尔型斯格明子。


<details>
  <summary>Details</summary>
Motivation: 研究二维层状材料堆叠中的电子和磁态工程，探索超越单个莫尔晶胞尺度的新型磁性状态。

Method: 使用扫描量子自旋磁强计测量扭曲双层CrI3的矢量场图，结合大尺度原子模拟分析磁相互作用。

Result: 在小扭转角下，自旋纹理尺寸随扭转角增大而增大，在1.1度时达到约300nm，比基础莫尔波长大一个数量级，在2度以上消失。

Conclusion: 层间相对旋转控制的Dzyaloshinskii-Moriya相互作用、磁各向异性和交换相互作用之间的复杂磁竞争产生了超莫尔自旋序中的拓扑纹理。

Abstract: Stacking two-dimensional (2D) layered materials offers a powerful platform to
engineer electronic and magnetic states. In general, the resulting states, such
as Moir\'e magnetism, have a periodicity at the length scale of the Moir\'e
unit cell. Here, we report a new type of magnetism -- dubbed a super-Moir\'e
magnetic state -- which is characterized by long-range magnetic textures
extending beyond the single Moir\'e unit cell -- in twisted double bilayer
chromium triiodide (tDB CrI$_3$). We found that at small twist angles, the size
of the spontaneous magnetic texture increases with twist angle, opposite to the
underlying Moir\'e periodicity. The spin-texture size reaches a maximum of
about 300 nm in 1.1${\deg}$ twisted devices, an order of magnitude larger than
the underlying Moir\'e wavelength, and vanishes at twist angles above
2${\deg}$. Employing scanning quantum spin magnetometry, the obtained vector
field maps suggest the formation of antiferromagnetic N\'eel-type skyrmions
spanning multiple Moir\'e cells. The twist-angle-dependent study combined with
large-scale atomistic simulations suggests that complex magnetic competition
between the Dzyaloshinskii--Moriya interaction, magnetic anisotropy, and
exchange interactions controlled by the relative rotation of the layers
produces the topological textures which arise in the super-Moir\'e spin orders.

</details>


### [26] [Model Reduction for Controlled Quantum Markov Dynamics](https://arxiv.org/abs/2510.25546)
*Tommaso Grigoletto,Lorenza Viola,Francesco Ticozzi*

Main category: quant-ph

TL;DR: 该论文提出了一种基于Krylov算子子空间和算子代数技术的模型降阶方法，用于处理具有时间相关Lindblad生成器的马尔可夫量子系统，确保降阶模型保持Lindblad形式并能精确再现感兴趣观测量的演化。


<details>
  <summary>Details</summary>
Motivation: 处理具有外部控制的时间相关Lindblad生成器的马尔可夫量子系统的模型降阶问题，确保降阶模型保持物理一致性。

Method: 基于Krylov算子子空间和算子代数技术，扩展了时间无关生成器的方法来处理时间相关情况。

Result: 开发出的降阶模型能够精确再现感兴趣观测量的演化，并保证处于Lindblad形式。

Conclusion: 该方法为具有时间相关Lindblad生成器的量子系统提供了一种有效的模型降阶技术，保持了系统的物理特性。

Abstract: We consider the problem of model reduction for Markovian quantum systems
whose dynamics are described by a time-dependent Lindblad generator -- notably,
as arising in the presence of external control. Our approach, which builds upon
Krylov operator subspaces and operator-algebraic techniques introduced for
time-independent generators, returns a reduced model that reproduces exactly
the evolution of observables of interest and is guaranteed to be in Lindblad
form.

</details>


### [27] [Charge-Preserving Operations in Quantum Batteries](https://arxiv.org/abs/2510.25549)
*André H. A. Malavazi,Borhan Ahmadi,Paweł Horodecki,Pedro R. Dieguez*

Main category: quant-ph

TL;DR: 该论文引入了isoergotropic态和ergotropy保持操作的概念，这些操作在保持总ergotropy不变的情况下重新组织其内部结构，并展示了如何在离散和连续变量系统中实现这些操作。


<details>
  <summary>Details</summary>
Motivation: 理解如何有效操纵和转换ergotropy对于推进量子能量管理技术至关重要，这有助于优化充电协议和减少开放量子电池中的电荷损失。

Method: 通过引入isoergotropic态和ergotropy保持操作，研究了这些操作如何重新分配相干-非相干和位移-压缩分量，并探讨了伴随的热力学交换。

Result: 展示了ergotropy保持操作可以在离散和连续变量系统中实现，并通过标准的分束器型相互作用动态实施，同时分析了能量和熵的变化。

Conclusion: isoergotropic态和操作在优化充电协议和减轻开放量子电池中的电荷损失方面具有实际应用价值。

Abstract: Ergotropy provides a fundamental measure of the extractable work from a
quantum system and, consequently, of the maximal useful energy, or charge,
stored within it. Understanding how this quantity can be manipulated and
transformed efficiently is crucial for advancing quantum energy management
technologies. Here, we introduce and formalize the concepts of isoergotropic
states and ergotropy-preserving operations, which reorganize the internal
structure of ergotropy while keeping its total value unchanged. These ideas are
illustrated for both discrete (two-level systems) and continuous-variable
systems (single-mode Gaussian states). In each case, we show how
ergotropy-preserving operations redistribute the respective coherent-incoherent
and displacement-squeezing components. We further examine the thermodynamic
exchanges accompanying ergotropy-preserving operations, including variations in
energy and entropy, and demonstrate that these transformations can be
dynamically implemented through standard beam-splitter-type interactions with
an auxiliary system. Finally, we discuss the practical implications of
isoergotropic states and operations in optimizing charging protocols and
mitigating charge loss in open quantum batteries.

</details>


### [28] [Transition-Aware Decomposition of Single-Qudit Gates](https://arxiv.org/abs/2510.25561)
*Denis A. Drozhzhin,Evgeniy O. Kiktenko,Aleksey K. Fedorov,Anastasiia S. Nikolaeva*

Main category: quant-ph

TL;DR: 提出了一种资源高效的单qudit操作分解算法，可将任意单qudit操作分解为最多d(d-1)/2个脉冲序列，适用于受选择规则限制的qudit硬件平台。


<details>
  <summary>Details</summary>
Motivation: qudit量子计算比qubit具有更丰富的计算空间，但通用qudit操作需要分解为符合选择规则的本机脉冲序列，且脉冲数量会影响执行时间和误差。

Method: 开发了资源高效的分解算法，将单qudit操作分解为允许的脉冲序列，考虑了不同量子系统（如囚禁离子和超导qudit）的选择规则。

Result: 算法对任意单qudit操作最多需要d(d-1)/2个脉冲，对特定操作可产生更少脉冲。比较了不同囚禁离子和超导qudit的分解效果。

Conclusion: 该算法为qudit量子计算提供了高效的脉冲分解方法，减少了脉冲数量，有助于提高计算效率和降低误差。

Abstract: Quantum computation with $d$-level quantum systems, also known as qudits,
benefits from the possibility to use a richer computational space compared to
qubits. However, for arbitrary qudit-based hardware platform the issue is that
a generic qudit operation has to be decomposed into the sequence of native
operations $-$ pulses that are adjusted to the transitions between two levels
in a qudit. Typically, not all levels in a qudit are simply connected to each
other due to specific selection rules. Moreover, the number of pulses plays a
significant role, since each pulse takes a certain execution time and may
introduce error. In this paper, we propose a resource-efficient algorithm to
decompose single-qudit operations into the sequence of pulses that are allowed
by qudit selection rules. Using the developed algorithm, the number of pulses
is at most $d(d{-}1)/2$ for an arbitrary single-qudit operation. For specific
operations the algorithm could produce even fewer pulses. We provide a
comparison of qudit decompositions for several types of trapped ions,
specifically $^{171}\text{Yb}^+$, $^{137}\text{Ba}^+$, $^{40}\text{Ca}^+$,
$^{86}\text{Rb}^+$ with different selection rules, and also decomposition for
superconducting qudits.

</details>


### [29] [Systematic Non-Binary Extension of LDPC-CSS Codes Preserving Orthogonality](https://arxiv.org/abs/2510.25583)
*Kenta Kasai*

Main category: quant-ph

TL;DR: 本文研究了保持二进制CSS码奇偶校验矩阵相同支撑的有限域扩展，提出了系统化构造方法，适用于任意CSS码的有限域推广。


<details>
  <summary>Details</summary>
Motivation: 研究如何将二进制CSS码扩展到有限域，同时保持原始码的支撑结构和正交性条件，这对于量子纠错码的构造具有重要意义。

Method: 提出系统化构造方法，通过有限域扩展来保持二进制奇偶校验矩阵的支撑结构，同时确保正交性条件在扩展后仍然成立。

Result: 成功构建了保持二进制支撑和正交性条件的有限域CSS码扩展，该方法适用于稀疏LDPC-CSS码和任意CSS码。

Conclusion: 所提出的系统化构造方法能够有效实现二进制CSS码到有限域的推广，为量子纠错码设计提供了新的工具。

Abstract: We study finite-field extensions that preserve the same support as the
parity-check matrices defining a given binary CSS code. Here, an LDPC-CSS code
refers to a CSS code whose parity-check matrices are orthogonal in the sense
that each pair of corresponding rows overlaps in an even (possibly zero) number
of positions, typically at most twice in sparse constructions. Beyond the
low-density setting, we further propose a systematic construction method that
extends to arbitrary CSS codes, providing feasible finite-field generalizations
that maintain both the binary support and the orthogonality condition.

</details>


### [30] [Controlling the Dynamical Evolution of Quantum Coherence and Quantum Correlations in $ e^{+}e^{-} \to Λ\barΛ$ Processes at BESIII](https://arxiv.org/abs/2510.25615)
*Elhabib Jaloum,Mohamed Amazioug*

Main category: quant-ph

TL;DR: 研究BESIII实验中e+e-→ΛΛ̄过程的量子相干性和量子关联，发现这些资源在散射角φ=π/2时达到最大值，经典关联能显著延缓量子关联和相干性的衰减。


<details>
  <summary>Details</summary>
Motivation: 量子相干性是量子信息协议的基础，但在基本粒子系统中维持相干性面临挑战，需要研究高能粒子物理中的量子相干性和关联。

Method: 使用BESIII实验的可行参数分析e+e-→ΛΛ̄过程，研究量子相干性和量子关联对散射角φ的依赖性。

Result: 量子相干性和量子关联在φ=π/2时达到最大值，经典关联能显著延缓量子关联和相干性的衰减。

Conclusion: 这项研究强调了理解高能粒子物理中经典和量子关联相互作用的重要性，特别是在BESIII实验探索的超子-反超子相互作用中，结果可能在量子信息处理和高能物理中有潜在应用。

Abstract: Quantum coherence, a cornerstone of quantum mechanics, is of paramount
importance for quantum information protocols. However, maintaining coherence in
elementary particle systems presents significant challenges. In this work, we
investigate quantum coherence and quantum correlations in the $e^{+}e^{-} \to
\Lambda\bar{\Lambda}$ processes at BESIII using experimentally feasible
parameters, where $\Lambda$ and $\bar{\Lambda}$ denote the spin-$1/2$ hyperon
and its antihyperon, respectively. We analyze the dependence of quantum
coherence and quantum correlations on the scattering angle $\varphi$. Notably,
these resources reach their maximum at $\varphi=\pi/2$. We demonstrate that
classical correlations can significantly delay the decay of quantum
correlations and coherence. This study underscores the importance of
understanding the interplay between classical and quantum correlations in
high-energy particle physics, particularly in the context of
hyperon-antihyperon interactions explored in the BESIII experiment. This result
could have potential applications in quantum information processing and
high-energy physics.

</details>


### [31] [Engineering Atom-Photon Hybridization with Density-Modulated Atomic Ensembles in Coupled Cavities](https://arxiv.org/abs/2510.25617)
*Carlos E. Máximo,Romain Bachelard,Tobias Donner*

Main category: quant-ph

TL;DR: 利用原子系综的空间结构控制不同腔模之间的耦合，通过空间工程化的原子系综实现选择性光子传输和复杂多体系统的精确控制。


<details>
  <summary>Details</summary>
Motivation: 辐射-物质杂化使原子能够作为光模之间有效相互作用的媒介，反之亦然。研究如何利用原子系综的空间结构来控制不同腔模之间的耦合，从而重塑原子-光子光谱。

Method: 通过分析扩展均匀云和光栅结构云对腔模耦合的影响，研究在特定布拉格条件下如何保持模式间耦合，导致光谱子分裂。

Result: 扩展均匀云通过相消干涉抑制模式间耦合，而光栅结构云在特定布拉格条件下可以保持模式间耦合，产生模式间光谱子分裂。

Conclusion: 空间工程化的原子系综是实现选择性光子传输和精确控制多体复杂性的有效途径。

Abstract: Radiation-matter hybridization allows atoms to serve as mediators of
effective interactions between light modes and, conversely, to interact among
themselves via light. Here we exploit the spatial structure of atomic ensembles
to control the coupling between modes of distinct cavities, thereby reshaping
the resulting atom-photon spectra. We show that extended homogeneous clouds
suppress mode-mode couplings through destructive interference, whereas grated
clouds can preserve them under specific Bragg conditions. This leads to
mode-mode spectral subsplittings, where collectivity arises not only from the
atom number but also from the ability to tune modes of different cavities
independently. Our results establish spatially engineered atomic ensembles as a
pathway to selective photon transfer between modes and precise control of
many-body complexity.

</details>


### [32] [Photoelectric detection of single spins in diamond by optically controlled discharge of long-lived trap states](https://arxiv.org/abs/2510.25619)
*A. C. Ulibarri,D. J. McCloskey,D. Wang,N. Dontschuk,A. M. Martin,A. A. Wood*

Main category: quant-ph

TL;DR: 提出了一种基于电荷捕获检测磁共振(CCDMR)的光电自旋读取方案，通过检测存储在长寿命电荷陷阱中的自旋信息来实现固态自旋量子比特的电气读取。


<details>
  <summary>Details</summary>
Motivation: 在宽禁带材料中实现与光学方法类似保真度和带宽的电气自旋读取仍然具有挑战性，需要开发新的读取方案。

Method: 使用金刚石中的氮空位中心作为模型系统，通过自旋依赖的光电离产生电荷载流子，这些载流子被存储在金刚石-金属肖特基结的长寿命陷阱态中。在电偏压下按需照射结区释放存储的电荷，产生与自旋态成正比的瞬态光电流。

Result: 演示了单氮空位中心相干控制后的自旋读取，使用电荷读取协议实现了电荷捕获检测磁共振，并通过电荷成像识别了电荷载流子生成和捕获过程。

Conclusion: CCDMR作为固态自旋量子比特读取的新技术，结合了电气检测的优势与宽禁带材料中长寿命电荷陷阱的稳定性。

Abstract: Electrical detection methods for solid-state spins are attractive for quantum
technologies, being readily chip-scalable and not subject to the small photon
budgets of single emitters. However, realising electrical spin readout in
wide-bandgap materials with similar fidelity and bandwidth to optical
approaches remains challenging. Here, we introduce a photoelectrical spin
readout scheme that detects spin information stored long-term as trapped
electrical charges. Using nitrogen-vacancy (NV) centres in diamond as a model
system, spin-dependent photoionisation generates charge carriers that are
stored in long-lived trap states at a diamond-metal Schottky junction.
On-demand illumination of the junction under electrical bias releases stored
charge, yielding a photocurrent transient proportional to the amount of trapped
charge and hence spin state. Spin readout after coherent control of single NVs
is demonstrated using charge readout in a protocol we call charge-capture
detected magnetic resonance (CCDMR), and we use charge-based imaging to
identify charge carrier generation and trapping processes. Our results
establish CCDMR as a new technique for solid-state spin qubit readout,
combining attaractive features of electrical detection with the stability of
long-lived charge traps in wide-bandgap materials.

</details>


### [33] [Large-scale implementation of quantum subspace expansion with classical shadows](https://arxiv.org/abs/2510.25640)
*Laurin E. Fischer,Daniel Bultrini,Ivano Tavernelli,Francesco Tacchino*

Main category: quant-ph

TL;DR: 该论文首次大规模实现了基于信息完备测量的量子子空间展开方法，在80量子比特系统上准确恢复了基态能量，并观测到局域序参量的缓解。


<details>
  <summary>Details</summary>
Motivation: 量子子空间展开方法在量子处理器上进行谱计算很有前景，但存在大量测量开销的问题。信息完备测量（如经典阴影）被提出用于克服这一瓶颈。

Method: 将量子子空间展开重新表述为约束优化问题，使用信息完备测量（经典阴影）方法，通过超过3万次测量基随机化和评估约10^14个泡利迹来进行实验。

Result: 在包含三体相互作用的自旋模型的量子相变研究中，准确恢复了基态能量，并在80量子比特系统上观测到局域序参量的缓解。

Conclusion: 这是迄今为止经典阴影方法最重要的实验实现之一，通过约束优化公式获得了严格的统计误差估计并避免了数值病态问题。

Abstract: Quantum subspace expansion (QSE) offers promising avenues to perform spectral
calculations on quantum processors but comes with a large measurement overhead.
Informationally complete (IC) measurements, such as classical shadows, were
recently proposed to overcome this bottleneck. Here, we report the first
large-scale implementation of QSE with IC measurements. In particular, we probe
the quantum phase transition of a spin model with three-body interactions, for
which we observe accurate ground state energy recovery and mitigation of local
order parameters across system sizes of up to 80 qubits. We achieve this by
reformulating QSE as a constrained optimization problem, obtaining rigorous
statistical error estimates and avoiding numerical ill-conditioning. With over
$3 \times 10^4$ measurement basis randomizations per circuit and the evaluation
of $O(10^{14})$ Pauli traces, this represents one of the most significant
experimental realizations of classical shadows to date.

</details>


### [34] [Accurate Leakage Speculation for Quantum Error Correction](https://arxiv.org/abs/2510.25661)
*Chaithanya Naik Mude,Swamit Tannu*

Main category: quant-ph

TL;DR: gladiator是一个通用的量子泄漏推测框架，通过构建代码感知的错误传播图来精确检测泄漏，减少不必要的泄漏减少电路(LRCs)，从而提升量子纠错效率。


<details>
  <summary>Details</summary>
Motivation: 量子比特泄漏到高能级会破坏后续的综合征测量并扩散到邻近量子比特，而现有方法使用固定启发式规则容易误判良性综合征，导致过多的LRCs触发，延长QEC周期并增加逻辑错误率。

Method: 离线构建代码感知的错误传播图，在线快速分类每个综合征，仅在可证明泄漏主导的模式下调度LRC。

Result: 消除了最多3倍（平均2倍）不必要的LRCs，缩短QEC周期，在标准容错基准测试中实现1.7x-3.9x加速和16%逻辑错误率降低。

Conclusion: gladiator框架显著提升了容错量子计算的效率，为量子纠错提供了更精确的泄漏检测方案。

Abstract: Quantum Error Correction (QEC) protects qubits against bit- and phase-flip
errors in the |0> or |1> subspace, but physical qubits can also leak into
higher energy levels (e.g., |2>). Leakage is especially harmful, as it corrupts
all subsequent syndrome measurements and can spread to neighboring qubits.
Detecting leakage on data qubits is particularly challenging, since they are
never measured directly during QEC cycles. Prior work, such as eraser,
addresses this by inferring leakage from syndrome patterns using a fixed
heuristic. However, this approach often misclassifies benign syndromes,
triggering excessive leakage-reduction circuits (LRCs). Because LRCs are
themselves noisy and slow, these false triggers lengthen QEC cycles and inflate
logical error rates.
  We propose gladiator, a general and adaptable leakage speculation framework
that works across surface code, color code, and qLDPC codes. Offline, gladiator
builds a code-aware error-propagation graph calibrated to device data. Online,
it classifies each syndrome in a few nanoseconds and schedules LRC only when
the observed pattern is provably leakage-dominated. This precise speculation
eliminates up to 3x (and on average 2x) unnecessary LRCs, shortens QEC cycles,
and suppresses false positives at their source. Evaluated on standard
fault-tolerant benchmarks, gladiator delivers 1.7x-3.9x speedups and 16%
reduction in logical error rate, advancing the efficiency of fault-tolerant
quantum computing.

</details>


### [35] [Quantum simulation of actinide chemistry: towards scalable algorithms on trapped ion quantum computers](https://arxiv.org/abs/2510.25675)
*Kesha Sorathia,Cono Di Paola,Gabriel Greene-Diniz,Carlo A. Gaggioli,David Zsolt Manrique,Joe Gibbs,Sean Harding,Thomas M. Soini,Neil Gaspar,Robert Harker,Mark Storr,David Munoz Ramo*

Main category: quant-ph

TL;DR: 本文比较了量子计算矩方法和量子相位估计算法在研究钚氧化物和氢化物反应能量学中的应用，通过减少资源需求在量子硬件上成功实验，结果与经典计算高度一致。


<details>
  <summary>Details</summary>
Motivation: 由于锕系元素在技术应用中的广泛性，理解其电子结构对技术改进至关重要。量子计算可能提供指数级加速，帮助理解锕系化合物的复杂电子结构。

Method: 使用量子计算矩方法作为含噪声中等规模量子算法，与单辅助量子比特的量子相位估计算法进行比较。采用筛选哈密顿量泡利项减少测量需求，以及变分编译减少电路深度。

Result: 在Quantinuum的H系列离子阱设备上使用最多19个量子比特进行实验，实验结果与经典电子结构计算和状态向量模拟结果高度一致。

Conclusion: 量子计算在锕系元素电子结构研究中具有可行性，通过资源优化技术可以在当前量子硬件上成功进行实验，为未来量子计算化学应用奠定了基础。

Abstract: Due to the wide range of technical applications of actinide elements, a
thorough understanding of their electronic structure could complement
technological improvements in many different areas. Quantum computing could
greatly aid in this understanding, as it can potentially provide exponential
speedups over classical approaches, thereby offering insights into the complex
electronic structure of actinide compounds. As a first foray into quantum
computational chemistry of actinides, this paper compares the method of quantum
computed moments (QCM) as a noisy intermediate-scale quantum algorithm with a
single-ancilla version of quantum phase estimation (QPE), a quantum algorithm
expected to run on fault-tolerant quantum computers. We employ these algorithms
to study the reaction energetics of plutonium oxides and hydrides. In order to
enable quantum hardware experiments, we use several techniques to reduce
resource requirements: screening individual Hamiltonian Pauli terms to reduce
the measurement requirements of QCM and variational compilation to reduce the
depth of QPE circuits. Finally, we derive electronic structure descriptions
from a series of representative chemical models and compute the energetics from
quantum experiments on Quantinuum's H-series ion trap devices using up to 19
qubits. We find our experiments to be in excellent agreement with results from
classical electronic structure calculations and state vector simulations.

</details>


### [36] [Symmetry and Asymmetry in Bosonic Gaussian Systems: A Resource-Theoretic Framework](https://arxiv.org/abs/2510.25719)
*Nikolaos Koukoulekidis,Iman Marvian*

Main category: quant-ph

TL;DR: 该论文开发了玻色子系统中高斯不对称性的资源理论，研究了对称性和高斯性在封闭和开放动力学中的相互作用。


<details>
  <summary>Details</summary>
Motivation: 研究玻色子系统中对称性和高斯性之间的相互作用，为理解对称性约束下的高斯操作提供理论框架。

Method: 使用高斯对称性保持（协变）操作作为自由操作，证明这些操作可以通过高斯哈密顿量实现，并识别了一族可处理的单调函数。

Result: 证明了高斯对称性保持操作可以通过对称性保持的高斯哈密顿量实现，并发现这些单调函数在非高斯对称性保持动力学中通常不被保持。

Conclusion: 建立了高斯不对称性的资源理论，为量子信息和光学社区提供了新的技术结果，包括Stinespring扩张定理的新方法和Williamson定理的扩展。

Abstract: We study the interplay of symmetries and Gaussianity in bosonic systems,
under closed and open dynamics, and develop a resource theory of Gaussian
asymmetry. Specifically, we focus on Gaussian symmetry-respecting (covariant)
operations, which serve as the free operations in this framework. We prove that
any such operation can be realized via Gaussian Hamiltonians that respect the
symmetry under consideration, coupled to an environment prepared in a
symmetry-respecting pure Gaussian state. We further identify a family of
tractable monotone functions of states that remain non-increasing under
Gaussian symmetry-respecting dynamics, and are exactly conserved in closed
systems. We demonstrate that these monotones are not generally respected under
non-Gaussian symmetry-respecting dynamics. Along the way, we provide several
technical results of independent interest to the quantum information and optics
communities, including a new approach to the Stinespring dilation theorem, and
an extension of Williamson's theorem for the simultaneous normal mode
decomposition of Gaussian systems and conserved charges.

</details>


### [37] [Inverse-free quantum state estimation with Heisenberg scaling](https://arxiv.org/abs/2510.25750)
*Kean Chen*

Main category: quant-ph

TL;DR: 提出了一种无逆查询的纯量子态估计协议，实现了海森堡标度，在仅使用前向查询的情况下估计未知酉变换后的量子态。


<details>
  <summary>Details</summary>
Motivation: 现有量子态估计方法需要同时使用前向和逆查询，限制了实际应用。本文旨在开发仅需前向查询的高效估计协议。

Method: 设计了一种无逆查询的纯量子态估计协议，通过优化查询策略来估计未知酉变换U作用在基态|d⟩上的结果U|d⟩。

Result: 协议在仅使用前向查询的情况下，以O(min{d^{3/2}/ε,d/ε^2})的查询复杂度达到迹距离误差ε，改进了现有结果。

Conclusion: 该工作证明了无逆查询也能实现高效量子态估计，推翻了之前关于无逆查询效率的猜想，为量子算法设计提供了新思路。

Abstract: In this paper, we present an inverse-free pure quantum state estimation
protocol that achieves Heisenberg scaling. Specifically, let $\mathcal{H}\cong
\mathbb{C}^d$ be a $d$-dimensional Hilbert space with an orthonormal basis
$\{|1\rangle,\ldots,|d\rangle\}$ and $U$ be an unknown unitary on
$\mathcal{H}$. Our protocol estimates $U|d\rangle$ to within trace distance
error $\varepsilon$ using $O(\min\{d^{3/2}/\varepsilon,d/\varepsilon^2\})$
forward queries to $U$. This complements the previous result
$O(d\log(d)/\varepsilon)$ by van Apeldoorn, Cornelissen, Gily\'en, and
Nannicini (SODA 2023), which requires both forward and inverse queries.
Moreover, our result implies a query upper bound
$O(\min\{d^{3/2}/\varepsilon,1/\varepsilon^2\})$ for inverse-free amplitude
estimation, improving the previous best upper bound
$O(\min\{d^{2}/\varepsilon,1/\varepsilon^2\})$ based on optimal unitary
estimation by Haah, Kothari, O'Donnell, and Tang (FOCS 2023), and disproving a
conjecture posed in Tang and Wright (2025).

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [38] [Fortytwo: Swarm Inference with Peer-Ranked Consensus](https://arxiv.org/abs/2510.24801)
*Vladyslav Larin,Ihor Naumenko,Aleksei Ivashov,Ivan Nikitin,Alexander Firsov*

Main category: cs.LG

TL;DR: Fortytwo协议利用群体智能和分布式成对排名共识，在AI推理中实现优于多数投票的性能，在GPQA Diamond上达到85.90%准确率，比多数投票高17.21个百分点。


<details>
  <summary>Details</summary>
Motivation: 随着集中式AI面临计算瓶颈和训练收益递减，需要能够水平扩展容量和能力的推理层。

Method: 采用群体推理方法，通过同行排名、声誉加权的异构模型共识，使用成对排名和Bradley-Terry风格聚合模型，结合链上声誉和能力证明机制。

Result: 在六个基准测试中表现优异，GPQA Diamond准确率85.90%，比多数投票高+17.21个百分点，对对抗性提示具有强韧性（仅0.12%退化）。

Conclusion: 为去中心化AI系统奠定基础，通过集体智能实现高质量推理的民主化访问，同时保持可靠性和安全性。

Abstract: As centralized AI hits compute ceilings and diminishing returns from
ever-larger training runs, meeting demand requires an inference layer that
scales horizontally in both capacity and capability. We present Fortytwo, a
novel protocol that leverages swarm intelligence principles and distributed
pairwise ranking consensus to achieve superior performance in AI inference. Our
approach reimagines collaboration among AI nodes using swarm inference: a
peer-ranked, reputation-weighted consensus across heterogeneous models that
surfaces the highest-quality responses. Using pairwise ranking with a custom
Bradley-Terry-style aggregation model, we demonstrate that swarm inference
substantially outperforms majority voting, achieving 85.90% on GPQA Diamond
versus 68.69% for majority voting with the same model set - an improvement of
+17.21 percentage points (approximately +25.1% relative). The protocol
incorporates on-chain reputation so node influence adapts to demonstrated
accuracy over time, yielding a meritocratic consensus that filters low-quality
or malicious participants. To resist Sybil attacks, Fortytwo employs
proof-of-capability in its consensus: nodes must successfully complete
calibration/test requests and stake reputation to enter ranking rounds, making
multi-identity attacks economically unattractive while preserving openness.
Across six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and
AIME, our evaluation indicates higher accuracy and strong resilience to
adversarial and noisy free-form prompting (e.g., prompt-injection degradation
of only 0.12% versus 6.20% for a monolithic single-model baseline), while
retaining practical deployability. Together, these results establish a
foundation for decentralized AI systems - democratizing access to high-quality
inference through collective intelligence without sacrificing reliability or
security.

</details>


### [39] [From Linear to Nonlinear: Provable Weak-to-Strong Generalization through Feature Learning](https://arxiv.org/abs/2510.24812)
*Junsoo Oh,Jerry Song,Chulhee Yun*

Main category: cs.LG

TL;DR: 本文分析了从弱线性CNN到强两层ReLU CNN的弱到强泛化现象，识别了基于数据集信噪比特征的数据稀缺和数据丰富两种机制，揭示了泛化通过良性过拟合或标签校正实现，但过度训练会降低性能。


<details>
  <summary>Details</summary>
Motivation: 先前关于弱到强泛化的理论研究大多局限于抽象框架或线性/随机特征模型，本文旨在在更具体的CNN架构中提供形式化分析。

Method: 考虑包含不同难度标签相关信号和标签无关噪声的结构化数据，分析强模型在预训练弱模型标注数据上使用梯度下降训练时的动态过程。

Result: 识别了数据稀缺和数据丰富两种机制：数据稀缺时泛化通过良性过拟合实现或失败于有害过拟合；数据丰富时泛化在早期通过标签校正出现，但过度训练会降低性能。

Conclusion: 弱到强泛化的机制取决于数据集特征，数据稀缺和数据丰富两种机制下泛化以不同方式实现，过度训练对性能有负面影响。

Abstract: Weak-to-strong generalization refers to the phenomenon where a stronger model
trained under supervision from a weaker one can outperform its teacher. While
prior studies aim to explain this effect, most theoretical insights are limited
to abstract frameworks or linear/random feature models. In this paper, we
provide a formal analysis of weak-to-strong generalization from a linear CNN
(weak) to a two-layer ReLU CNN (strong). We consider structured data composed
of label-dependent signals of varying difficulty and label-independent noise,
and analyze gradient descent dynamics when the strong model is trained on data
labeled by the pretrained weak model. Our analysis identifies two regimes --
data-scarce and data-abundant -- based on the signal-to-noise characteristics
of the dataset, and reveals distinct mechanisms of weak-to-strong
generalization. In the data-scarce regime, generalization occurs via benign
overfitting or fails via harmful overfitting, depending on the amount of data,
and we characterize the transition boundary. In the data-abundant regime,
generalization emerges in the early phase through label correction, but we
observe that overtraining can subsequently degrade performance.

</details>


### [40] [Augmenting Biological Fitness Prediction Benchmarks with Landscapes Features from GraphFLA](https://arxiv.org/abs/2510.24826)
*Mingyu Huang,Shasha Zhou,Ke Li*

Main category: cs.LG

TL;DR: GraphFLA是一个Python框架，用于从突变数据构建和分析适应度景观，计算20个生物学相关特征来表征景观地形，帮助解释和比较多种适应度预测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习模型基准缺乏关于底层适应度景观的地形信息，这限制了模型性能的深入解释和比较。

Method: 开发GraphFLA框架，从DNA、RNA、蛋白质等不同模态的突变数据构建适应度景观，计算20个特征来表征4个基本地形方面。

Result: 应用于5,300多个景观，成功解释了数十种适应度预测模型的性能差异，识别了影响模型准确性的因素和不同模型的优势。

Conclusion: GraphFLA提供了分析适应度景观地形的有效工具，有助于更深入地理解和比较生物序列适应度预测模型。

Abstract: Machine learning models increasingly map biological sequence-fitness
landscapes to predict mutational effects. Effective evaluation of these models
requires benchmarks curated from empirical data. Despite their impressive
scales, existing benchmarks lack topographical information regarding the
underlying fitness landscapes, which hampers interpretation and comparison of
model performance beyond averaged scores. Here, we introduce GraphFLA, a Python
framework that constructs and analyzes fitness landscapes from mutagensis data
in diverse modalities (e.g., DNA, RNA, protein, and beyond) with up to millions
of mutants. GraphFLA calculates 20 biologically relevant features that
characterize 4 fundamental aspects of landscape topography. By applying
GraphFLA to over 5,300 landscapes from ProteinGym, RNAGym, and CIS-BP, we
demonstrate its utility in interpreting and comparing the performance of dozens
of fitness prediction models, highlighting factors influencing model accuracy
and respective advantages of different models. In addition, we release 155
combinatorially complete empirical fitness landscapes, encompassing over 2.2
million sequences across various modalities. All the codes and datasets are
available at https://github.com/COLA-Laboratory/GraphFLA.

</details>


### [41] [Send Less, Save More: Energy-Efficiency Benchmark of Embedded CNN Inference vs. Data Transmission in IoT](https://arxiv.org/abs/2510.24829)
*Benjamin Karic,Nina Herrmann,Jan Stenkamp,Paula Scharf,Fabian Gieseke,Angela Schwering*

Main category: cs.LG

TL;DR: 该论文研究了在ESP32-S3微控制器上使用压缩CNN模型和低功耗广域网进行环境监测，通过设备端推理仅传输结果而非原始图像数据，可将能耗降低高达5倍。


<details>
  <summary>Details</summary>
Motivation: 环境监测物联网设备需要在偏远地区长期运行，但数据传输能耗高。需要设计能在资源受限微控制器上运行的节能解决方案。

Method: 在ESP32-S3微控制器上评估常用低功耗广域网和针对特定领域数据集训练的压缩CNN模型，通过设备端推理减少数据传输量。

Result: 设备端执行CNN推理并仅传输结果，相比发送原始图像数据可将整体能耗降低高达5倍。模型量化仅导致精度轻微下降。

Conclusion: 嵌入式机器学习可开发碳足迹更小、能在环境监测场景中自主运行的物联网应用。

Abstract: The integration of the Internet of Things (IoT) and Artificial Intelligence
offers significant opportunities to enhance our ability to monitor and address
ecological changes. As environmental challenges become increasingly pressing,
the need for effective remote monitoring solutions is more critical than ever.
A major challenge in designing IoT applications for environmental monitoring -
particularly those involving image data - is to create energy-efficient IoT
devices capable of long-term operation in remote areas with limited power
availability. Advancements in the field of Tiny Machine Learning allow the use
of Convolutional Neural Networks (CNNs) on resource-constrained,
battery-operated microcontrollers. Since data transfer is energy-intensive,
performing inference directly on microcontrollers to reduce the message size
can extend the operational lifespan of IoT nodes. This work evaluates the use
of common Low Power Wide Area Networks and compressed CNNs trained on domain
specific datasets on an ESP32-S3. Our experiments demonstrate, among other
things, that executing CNN inference on-device and transmitting only the
results reduces the overall energy consumption by a factor of up to five
compared to sending raw image data. %The compression of the model using Post
Training Quantization is accompanied by an acceptable reduction in accuracy of
only a few percentage points compared to a non-quantized model. These findings
advocate the development of IoT applications with reduced carbon footprint and
capable of operating autonomously in environmental monitoring scenarios by
incorporating Embedded Machine Learning.

</details>


### [42] [Aggregation Hides Out-of-Distribution Generalization Failures from Spurious Correlations](https://arxiv.org/abs/2510.24884)
*Olawale Salaudeen,Haoran Zhang,Kumail Alhamoud,Sara Beery,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: 研究发现OOD泛化基准中的"准确率线性关系"常是异质OOD样本聚合的假象，通过OODSelect方法识别出语义一致的子集，在这些子集中更高的ID准确率反而预测更低的OOD准确率。


<details>
  <summary>Details</summary>
Motivation: 挑战现有OOD泛化基准中观察到的ID与OOD准确率强正相关现象，认为这种模式可能掩盖了实际存在的伪相关问题。

Method: 提出基于梯度的OODSelect方法，用于识别语义一致的OOD子集，在这些子集中检验ID与OOD准确率的关系。

Result: 在广泛使用的分布偏移基准中，OODSelect识别出的子集（有时超过标准OOD集的一半）显示更高的ID准确率预测更低的OOD准确率。

Conclusion: 聚合指标可能掩盖OOD鲁棒性的重要失效模式，需要更细粒度的分析来理解模型在OOD场景下的真实表现。

Abstract: Benchmarks for out-of-distribution (OOD) generalization frequently show a
strong positive correlation between in-distribution (ID) and OOD accuracy
across models, termed "accuracy-on-the-line." This pattern is often taken to
imply that spurious correlations - correlations that improve ID but reduce OOD
performance - are rare in practice. We find that this positive correlation is
often an artifact of aggregating heterogeneous OOD examples. Using a simple
gradient-based method, OODSelect, we identify semantically coherent OOD subsets
where accuracy on the line does not hold. Across widely used distribution shift
benchmarks, the OODSelect uncovers subsets, sometimes over half of the standard
OOD set, where higher ID accuracy predicts lower OOD accuracy. Our findings
indicate that aggregate metrics can obscure important failure modes of OOD
robustness. We release code and the identified subsets to facilitate further
research.

</details>


### [43] [Graph Network-based Structural Simulator: Graph Neural Networks for Structural Dynamics](https://arxiv.org/abs/2510.25683)
*Alessandro Lucchetti,Francesco Cadini,Marco Giglio,Luca Lomazzi*

Main category: cs.LG

TL;DR: 提出了GNSS（基于图网络的结构模拟器），一种用于动态结构问题代理建模的GNN框架，解决了现有GNN在结构动力学模拟中的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络在结构动力学问题中应用不足的问题，特别是动态结构模拟领域缺乏有效的GNN代理模型。

Method: 采用编码-处理-解码范式，包含三个关键特性：节点固定局部坐标系表达运动学、符号感知回归损失函数、波长感知连接半径优化图构建。

Result: 在50kHz汉宁调制脉冲激励的梁案例中，GNSS能够准确再现数百个时间步的物理特性，并能泛化到未见过的加载条件，而现有GNN无法收敛或提供有意义的预测。

Conclusion: 具有物理一致性更新规则的局部保持GNN是动态、波主导结构模拟的竞争性替代方案，相比显式有限元基准实现了显著的推理加速，同时保持时空保真度。

Abstract: Graph Neural Networks (GNNs) have recently been explored as surrogate models
for numerical simulations. While their applications in computational fluid
dynamics have been investigated, little attention has been given to structural
problems, especially for dynamic cases. To address this gap, we introduce the
Graph Network-based Structural Simulator (GNSS), a GNN framework for surrogate
modeling of dynamic structural problems.
  GNSS follows the encode-process-decode paradigm typical of GNN-based machine
learning models, and its design makes it particularly suited for dynamic
simulations thanks to three key features: (i) expressing node kinematics in
node-fixed local frames, which avoids catastrophic cancellation in
finite-difference velocities; (ii) employing a sign-aware regression loss,
which reduces phase errors in long rollouts; and (iii) using a
wavelength-informed connectivity radius, which optimizes graph construction.
  We evaluate GNSS on a case study involving a beam excited by a 50kHz
Hanning-modulated pulse. The results show that GNSS accurately reproduces the
physics of the problem over hundreds of timesteps and generalizes to unseen
loading conditions, where existing GNNs fail to converge or deliver meaningful
predictions.
  Compared with explicit finite element baselines, GNSS achieves substantial
inference speedups while preserving spatial and temporal fidelity. These
findings demonstrate that locality-preserving GNNs with physics-consistent
update rules are a competitive alternative for dynamic, wave-dominated
structural simulations.

</details>


### [44] [Adaptive EEG-based stroke diagnosis with a GRU-TCN classifier and deep Q-learning thresholding](https://arxiv.org/abs/2510.24889)
*Shakeel Abdulkareem,Bora Yimenicioglu,Andrea Yang,Khartik Uppalapati,Aneesh Gudipati,Zhaoyang Fan*

Main category: cs.LG

TL;DR: 开发了一种自适应多任务EEG分类器，使用GRU-TCN网络结合DQN阈值调整，用于卒中快速分诊，在卒中类型分类上达到98%准确率。


<details>
  <summary>Details</summary>
Motivation: 卒中快速分诊需要准确、床旁可用的工具，EEG有潜力但在初次接触时使用不足。

Method: 将32通道EEG信号转换为功率谱密度特征，使用GRU-TCN网络预测卒中类型、半球偏侧化和严重程度，并应用DQN实时调整决策阈值。

Result: 基线GRU-TCN在卒中类型分类上达到89.3%准确率，严重程度96.9%，偏侧化96.7%；加入DQN阈值适应后卒中类型准确率提升至98.0%。

Conclusion: 自适应阈值调整可将操作点转移到临床偏好的敏感度-特异度权衡，同时集成的头皮图和频谱可视化支持可解释性。

Abstract: Rapid triage of suspected stroke needs accurate, bedside-deployable tools;
EEG is promising but underused at first contact. We present an adaptive
multitask EEG classifier that converts 32-channel signals to power spectral
density features (Welch), uses a recurrent-convolutional network (GRU-TCN) to
predict stroke type (healthy, ischemic, hemorrhagic), hemispheric
lateralization, and severity, and applies a deep Q-network (DQN) to tune
decision thresholds in real time. Using a patient-wise split of the UCLH Stroke
EIT/EEG data set (44 recordings; about 26 acute stroke, 10 controls), the
primary outcome was stroke-type performance; secondary outcomes were severity
and lateralization. The baseline GRU-TCN reached 89.3% accuracy (F1 92.8%) for
stroke type, about 96.9% (F1 95.9%) for severity, and about 96.7% (F1 97.4%)
for lateralization. With DQN threshold adaptation, stroke-type accuracy
increased to about 98.0% (F1 97.7%). We also tested robustness on an
independent, low-density EEG cohort (ZJU4H) and report paired patient-level
statistics. Analyses follow STARD 2015 guidance for diagnostic accuracy studies
(index test: GRU-TCN+DQN; reference standard: radiology/clinical diagnosis;
patient-wise evaluation). Adaptive thresholding shifts the operating point to
clinically preferred sensitivity-specificity trade-offs, while integrated
scalp-map and spectral visualizations support interpretability.

</details>


### [45] [LieSolver: A PDE-constrained solver for IBVPs using Lie symmetries](https://arxiv.org/abs/2510.25731)
*René P. Klausen,Ivan Timofeev,Johannes Frank,Jonas Naujoks,Thomas Wiegand,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: 提出LieSolver方法，利用李对称性精确求解初边值问题，比PINNs更快更准确


<details>
  <summary>Details</summary>
Motivation: 传统方法如PINNs在求解PDE约束问题时存在收敛慢和精度不足的问题，需要更高效可靠的方法

Method: 利用李对称变换构建模型，通过构造精确满足PDE，从初始和边界数据学习解，损失函数直接衡量模型精度

Result: LieSolver在线性齐次PDE上表现优于PINNs，计算效率更高，预测更可靠

Conclusion: 该方法提高了PDE约束问题的计算效率和预测可靠性，支持严格的误差估计

Abstract: We introduce a method for efficiently solving initial-boundary value problems
(IBVPs) that uses Lie symmetries to enforce the associated partial differential
equation (PDE) exactly by construction. By leveraging symmetry transformations,
the model inherently incorporates the physical laws and learns solutions from
initial and boundary data. As a result, the loss directly measures the model's
accuracy, leading to improved convergence. Moreover, for well-posed IBVPs, our
method enables rigorous error estimation. The approach yields compact models,
facilitating an efficient optimization. We implement LieSolver and demonstrate
its application to linear homogeneous PDEs with a range of initial conditions,
showing that it is faster and more accurate than physics-informed neural
networks (PINNs). Overall, our method improves both computational efficiency
and the reliability of predictions for PDE-constrained problems.

</details>


### [46] [Topic Analysis with Side Information: A Neural-Augmented LDA Approach](https://arxiv.org/abs/2510.24918)
*Biyi Fang,Kripa Rajshekhar,Truong Vo,Diego Klabjan*

Main category: cs.LG

TL;DR: 提出了nnLDA，一种神经增强的概率主题模型，通过神经先验机制动态整合辅助信息，在主题一致性、困惑度和下游分类任务上优于传统LDA和Dirichlet-Multinomial回归。


<details>
  <summary>Details</summary>
Motivation: 传统主题模型如LDA难以整合元数据、用户属性或文档标签等辅助信息，限制了其表达能力、个性化和可解释性。

Method: nnLDA将每个文档建模为潜在主题的混合，其中主题比例的先验由基于辅助特征的神经网络生成，并开发了随机变分EM算法来联合优化神经和概率组件。

Result: 在多个基准数据集上，nnLDA在主题一致性、困惑度和下游分类任务上始终优于LDA和Dirichlet-Multinomial回归。

Conclusion: 结果表明，在辅助信息可用的场景下，将神经表示学习与概率主题建模相结合具有显著优势。

Abstract: Traditional topic models such as Latent Dirichlet Allocation (LDA) have been
widely used to uncover latent structures in text corpora, but they often
struggle to integrate auxiliary information such as metadata, user attributes,
or document labels. These limitations restrict their expressiveness,
personalization, and interpretability. To address this, we propose nnLDA, a
neural-augmented probabilistic topic model that dynamically incorporates side
information through a neural prior mechanism. nnLDA models each document as a
mixture of latent topics, where the prior over topic proportions is generated
by a neural network conditioned on auxiliary features. This design allows the
model to capture complex nonlinear interactions between side information and
topic distributions that static Dirichlet priors cannot represent. We develop a
stochastic variational Expectation-Maximization algorithm to jointly optimize
the neural and probabilistic components. Across multiple benchmark datasets,
nnLDA consistently outperforms LDA and Dirichlet-Multinomial Regression in
topic coherence, perplexity, and downstream classification. These results
highlight the benefits of combining neural representation learning with
probabilistic topic modeling in settings where side information is available.

</details>


### [47] [KAN-GCN: Combining Kolmogorov-Arnold Network with Graph Convolution Network for an Accurate Ice Sheet Emulator](https://arxiv.org/abs/2510.24926)
*Zesheng Liu,YoungHyun Koo,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: KAN-GCN是一种快速准确的冰盖建模模拟器，将Kolmogorov-Arnold网络作为特征校准器置于图卷积网络之前，通过可学习的一维扭曲和线性混合步骤改善特征条件化和非线性编码，在不增加消息传递深度的情况下提升性能。


<details>
  <summary>Details</summary>
Motivation: 提高冰盖数值模型模拟器的性能，为大规模瞬态情景扫描提供更优的精度与效率权衡。

Method: 在GCN前放置KAN作为特征校准器，使用可学习的一维扭曲和线性混合步骤，用节点级变换替代边级消息传递层，在36个融化率模拟和3种网格尺寸设置下训练测试。

Result: 在2-5层架构中，KAN-GCN匹配或超越了纯GCN和MLP-GCN基线，在较粗网格上提高了推理吞吐量，仅在最细网格上有适度成本增加。

Conclusion: KAN优先设计为大规模瞬态情景扫描提供了有利的精度与效率权衡。

Abstract: We introduce KAN-GCN, a fast and accurate emulator for ice sheet modeling
that places a Kolmogorov-Arnold Network (KAN) as a feature-wise calibrator
before graph convolution networks (GCNs). The KAN front end applies learnable
one-dimensional warps and a linear mixing step, improving feature conditioning
and nonlinear encoding without increasing message-passing depth. We employ this
architecture to improve the performance of emulators for numerical ice sheet
models. Our emulator is trained and tested using 36 melting-rate simulations
with 3 mesh-size settings for Pine Island Glacier, Antarctica. Across 2- to
5-layer architectures, KAN-GCN matches or exceeds the accuracy of pure GCN and
MLP-GCN baselines. Despite a small parameter overhead, KAN-GCN improves
inference throughput on coarser meshes by replacing one edge-wise
message-passing layer with a node-wise transform; only the finest mesh shows a
modest cost. Overall, KAN-first designs offer a favorable accuracy vs.
efficiency trade-off for large transient scenario sweeps.

</details>


### [48] [WBT-BGRL: A Non-Contrastive Weighted Bipartite Link Prediction Model for Inductive Learning](https://arxiv.org/abs/2510.24927)
*Joel Frank Huarayo Quispe,Lilian Berton,Didier Vega-Oliveros*

Main category: cs.LG

TL;DR: 提出了WBT-BGRL框架，一种用于二分图链接预测的非对比学习方法，通过三重损失中的加权机制增强自举学习，在归纳、加权和二分图场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 二分图链接预测在推荐系统和故障检测中很重要，但研究较少。对比方法存在负采样效率低和偏差问题，而非对比方法仅依赖正样本。现有模型在转导设置中表现良好，但在归纳、加权和二分图场景中的有效性尚未验证。

Method: WBT-BGRL使用具有双GCN编码器的二分图架构，通过三重损失中的新颖加权机制增强自举学习。这是一种非对比框架，专门针对加权二分图设计。

Result: 在真实世界数据集（工业和电子商务）上的评估显示，WBT-BGRL与适应后的最先进模型（T-BGRL、BGRL、GBT、CCA-SSG）相比具有竞争力，特别是在预训练期间应用加权时表现突出。

Conclusion: 加权非对比学习对于二分图中的归纳链接预测具有重要价值，WBT-BGRL框架在加权二分图场景中表现出色。

Abstract: Link prediction in bipartite graphs is crucial for applications like
recommendation systems and failure detection, yet it is less studied than in
monopartite graphs. Contrastive methods struggle with inefficient and biased
negative sampling, while non-contrastive approaches rely solely on positive
samples. Existing models perform well in transductive settings, but their
effectiveness in inductive, weighted, and bipartite scenarios remains untested.
To address this, we propose Weighted Bipartite Triplet-Bootstrapped Graph
Latents (WBT-BGRL), a non-contrastive framework that enhances bootstrapped
learning with a novel weighting mechanism in the triplet loss. Using a
bipartite architecture with dual GCN encoders, WBT-BGRL is evaluated against
adapted state-of-the-art models (T-BGRL, BGRL, GBT, CCA-SSG). Results on
real-world datasets (Industry and E-commerce) show competitive performance,
especially when weighting is applied during pretraining-highlighting the value
of weighted, non-contrastive learning for inductive link prediction in
bipartite graphs.

</details>


### [49] [Can Aha Moments Be Fake? Identifying True and Decorative Thinking Steps in Chain-of-Thought](https://arxiv.org/abs/2510.24941)
*Jiachen Zhao,Yiyou Sun,Weiyan Shi,Dawn Song*

Main category: cs.LG

TL;DR: 研究发现LLMs生成的思维链中许多推理步骤并不真正影响最终预测，提出了真实思维评分(TTS)来衡量各步骤的因果影响，发现只有少量步骤真正驱动模型预测。


<details>
  <summary>Details</summary>
Motivation: 当前假设思维链忠实反映模型内部思考过程，但作者发现许多推理步骤只是装饰性的，并不真正贡献于预测，这影响LLM推理效率和思维链可信度。

Method: 提出真实思维评分(TTS)来衡量每个推理步骤对最终预测的因果影响，并在LLMs的潜在空间中识别真实思维方向，通过沿该方向引导来强制模型执行或忽略特定推理步骤。

Result: 在AIME数据集上，Qwen-2.5模型中平均只有2.3%的推理步骤TTS≥0.7；自我验证步骤也可能是装饰性的，沿真实思维方向引导可以改变最终结果。

Conclusion: LLMs经常口头表达推理步骤但内部并未真正执行，这削弱了LLM推理效率和思维链的可信度。

Abstract: Recent large language models (LLMs) can generate long Chain-of-Thought (CoT)
at test time, enabling them to solve complex tasks. These reasoning steps in
CoT are often assumed as a faithful reflection of the model's internal thinking
process, and used to monitor unsafe intentions. However, we find many reasoning
steps don't truly contribute to LLMs' prediction. We measure the step-wise
causal influence of each reasoning step on the model's final prediction with a
proposed True Thinking Score (TTS). We reveal that LLMs often interleave
between true-thinking steps (which are genuinely used to produce the final
output) and decorative-thinking steps (which only give the appearance of
reasoning but have minimal causal impact). Notably, only a small subset of the
total reasoning steps have a high TTS that causally drive the model's
prediction: e.g., for the AIME dataset, only an average of 2.3% of reasoning
steps in CoT have a TTS >= 0.7 (range: 0-1) under the Qwen-2.5 model.
Furthermore, we identify a TrueThinking direction in the latent space of LLMs.
By steering along or against this direction, we can force the model to perform
or disregard certain CoT steps when computing the final result. Finally, we
highlight that self-verification steps in CoT (i.e., aha moments) can also be
decorative, where LLMs do not truly verify their solution. Steering along the
TrueThinking direction can force internal reasoning over these steps, resulting
in a change in the final results. Overall, our work reveals that LLMs often
verbalize reasoning steps without actually performing them internally, which
undermines both the efficiency of LLM reasoning and the trustworthiness of CoT.

</details>


### [50] [Finding Culture-Sensitive Neurons in Vision-Language Models](https://arxiv.org/abs/2510.24942)
*Xiutian Zhao,Rochelle Choenni,Rohit Saxena,Ivan Titov*

Main category: cs.LG

TL;DR: 研究发现视觉语言模型存在文化敏感神经元，这些神经元对特定文化背景的输入表现出选择性敏感，其消融会显著影响对应文化的视觉问答性能。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型性能优异，但在处理文化相关输入时仍存在困难，需要理解模型如何处理文化背景信息。

Method: 使用CVQA基准识别文化选择性神经元，通过消融实验进行因果测试，并提出新的基于边界的对比激活选择方法。

Result: 在三个VLMs和25个文化群体上的实验证明，存在文化敏感神经元，其消融会不成比例地损害对应文化的性能，而对其他文化影响最小。

Conclusion: 文化敏感神经元倾向于聚集在某些解码器层中，这为多模态表征的内部组织提供了新的见解。

Abstract: Despite their impressive performance, vision-language models (VLMs) still
struggle on culturally situated inputs. To understand how VLMs process
culturally grounded information, we study the presence of culture-sensitive
neurons, i.e. neurons whose activations show preferential sensitivity to inputs
associated with particular cultural contexts. We examine whether such neurons
are important for culturally diverse visual question answering and where they
are located. Using the CVQA benchmark, we identify neurons of culture
selectivity and perform causal tests by deactivating the neurons flagged by
different identification methods. Experiments on three VLMs across 25 cultural
groups demonstrate the existence of neurons whose ablation disproportionately
harms performance on questions about the corresponding cultures, while having
minimal effects on others. Moreover, we propose a new margin-based selector -
Contrastive Activation Selection (CAS), and show that it outperforms existing
probability- and entropy-based methods in identifying culture-sensitive
neurons. Finally, our layer-wise analyses reveals that such neurons tend to
cluster in certain decoder layers. Overall, our findings shed new light on the
internal organization of multimodal representations.

</details>


### [51] [Resource-Efficient and Robust Inference of Deep and Bayesian Neural Networks on Embedded and Analog Computing Platforms](https://arxiv.org/abs/2510.24951)
*Bernhard Klein*

Main category: cs.LG

TL;DR: 该论文提出通过算法-硬件协同设计的方法，同时提升机器学习的效率和可靠性，包括模型压缩、近似贝叶斯推理、数字/模拟加速器优化等创新技术。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习计算需求不断增长，在嵌入式等资源受限平台上受到限制，同时需要处理分布偏移和未见数据的可靠预测。贝叶斯神经网络能量化不确定性但计算开销更大。

Method: 采用算法和硬件效率的联合优化：1) Galen系统通过敏感度分析和硬件反馈进行自动层特定压缩；2) 模拟加速器建模设备缺陷并扩展噪声训练；3) 开发分析性和集成近似方法替代昂贵采样；4) 概率光子计算利用模拟噪声作为熵源。

Result: 实现了资源高效且鲁棒的推理，通过算法-硬件协同设计在效率和可靠性方面取得显著进展。

Conclusion: 研究表明效率和可靠性可以通过算法-硬件协同设计共同推进，为下一代可信赖、高能效机器学习系统奠定基础。

Abstract: While modern machine learning has transformed numerous application domains,
its growing computational demands increasingly constrain scalability and
efficiency, particularly on embedded and resource-limited platforms. In
practice, neural networks must not only operate efficiently but also provide
reliable predictions under distributional shifts or unseen data. Bayesian
neural networks offer a principled framework for quantifying uncertainty, yet
their computational overhead further compounds these challenges.
  This work advances resource-efficient and robust inference for both
conventional and Bayesian neural networks through the joint pursuit of
algorithmic and hardware efficiency. The former reduces computation through
model compression and approximate Bayesian inference, while the latter
optimizes deployment on digital accelerators and explores analog hardware,
bridging algorithmic design and physical realization. The first contribution,
Galen, performs automatic layer-specific compression guided by sensitivity
analysis and hardware-in-the-loop feedback. Analog accelerators offer
efficiency gains at the cost of noise; this work models device imperfections
and extends noisy training to nonstationary conditions, improving robustness
and stability. A second line of work advances probabilistic inference,
developing analytic and ensemble approximations that replace costly sampling,
integrate into a compiler stack, and optimize embedded inference. Finally,
probabilistic photonic computing introduces a paradigm where controlled analog
noise acts as an intrinsic entropy source, enabling fast, energy-efficient
probabilistic inference directly in hardware.
  Together, these studies demonstrate how efficiency and reliability can be
advanced jointly through algorithm-hardware co-design, laying the foundation
for the next generation of trustworthy, energy-efficient machine-learning
systems.

</details>


### [52] [Sequences of Logits Reveal the Low Rank Structure of Language Models](https://arxiv.org/abs/2510.24966)
*Noah Golowich,Allen Liu,Abhishek Shetty*

Main category: cs.LG

TL;DR: 该论文发现大型语言模型存在低维结构，可以通过不相关甚至无意义的提示生成响应，并提出了理论框架来分析和利用这种结构。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型的固有低维结构，理解其内在工作机制。

Method: 将语言模型视为序列概率模型，分析其logits矩阵的近似秩，并利用低秩结构进行生成。

Result: 实证表明多种现代语言模型具有低秩结构，可以利用线性组合从无关提示生成目标响应。

Conclusion: 语言模型的低维结构可被理论抽象和实际利用，为模型理解和生成提供了新视角。

Abstract: A major problem in the study of large language models is to understand their
inherent low-dimensional structure. We introduce an approach to study the
low-dimensional structure of language models at a model-agnostic level: as
sequential probabilistic models. We first empirically demonstrate that a wide
range of modern language models exhibit low-rank structure: in particular,
matrices built from the model's logits for varying sets of prompts and
responses have low approximate rank. We then show that this low-rank structure
can be leveraged for generation -- in particular, we can generate a response to
a target prompt using a linear combination of the model's outputs on unrelated,
or even nonsensical prompts.
  On the theoretical front, we observe that studying the approximate rank of
language models in the sense discussed above yields a simple universal
abstraction whose theoretical predictions parallel our experiments. We then
analyze the representation power of the abstraction and give provable learning
guarantees.

</details>


### [53] [Conformational Rank Conditioned Committees for Machine Learning-Assisted Directed Evolution](https://arxiv.org/abs/2510.24974)
*Mia Adler,Carrie Liang,Brian Peng,Oleg Presnyakov,Justin M. Baker,Jannelle Lauffer,Himani Sharma,Barry Merriman*

Main category: cs.LG

TL;DR: 提出了一种基于排名条件委员会（RCC）的机器学习辅助定向进化框架，通过为每个构象排名分配深度神经网络委员会，实现了构象不确定性与认知不确定性的分离。


<details>
  <summary>Details</summary>
Motivation: 现有的结构感知MLDE方法通常依赖单一构象或单一委员会，无法有效区分构象不确定性和认知不确定性，限制了抗体适应性景观的探索效率。

Method: 引入排名条件委员会框架，利用排名构象为每个排名分配深度神经网络委员会，从而在原理上分离认知不确定性和构象不确定性。

Result: 在SARS-CoV-2抗体对接任务上验证了该方法，相比基线策略显示出显著改进。

Conclusion: 该方法为治疗性抗体发现提供了可扩展的途径，同时直接解决了构象不确定性建模的挑战。

Abstract: Machine Learning-assisted directed evolution (MLDE) is a powerful tool for
efficiently navigating antibody fitness landscapes. Many structure-aware MLDE
pipelines rely on a single conformation or a single committee across all
conformations, limiting their ability to separate conformational uncertainty
from epistemic uncertainty. Here, we introduce a rank -conditioned committee
(RCC) framework that leverages ranked conformations to assign a deep neural
network committee per rank. This design enables a principled separation between
epistemic uncertainty and conformational uncertainty. We validate our approach
on SARS-CoV-2 antibody docking, demonstrating significant improvements over
baseline strategies. Our results offer a scalable route for therapeutic
antibody discovery while directly addressing the challenge of modeling
conformational uncertainty.

</details>


### [54] [Strategic inputs: feature selection from game-theoretic perspective](https://arxiv.org/abs/2510.24982)
*Chi Zhao,Jing Liu,Elena Parilina*

Main category: cs.LG

TL;DR: 提出基于博弈论的端到端特征选择框架，通过评估特征的协同作用和边际贡献来确定重要性，显著降低计算成本同时保持预测性能。


<details>
  <summary>Details</summary>
Motivation: 数据量指数级增长导致机器学习模型训练计算成本急剧上升，许多特征对模型性能无正面贡献却消耗大量计算资源。

Method: 基于合作博弈论的特征选择框架，将特征建模为玩家，包含四个核心组件：样本选择、博弈论特征重要性评估、冗余特征消除和优化模型训练。

Result: 实验结果表明该方法在保持预测性能的同时实现了显著的计算减少，为大规模机器学习计算挑战提供了高效解决方案。

Conclusion: 该博弈论特征选择框架能够有效解决大规模机器学习中的计算效率问题，在减少计算成本的同时维持模型性能，具有实际应用价值。

Abstract: The exponential growth of data volumes has led to escalating computational
costs in machine learning model training. However, many features fail to
contribute positively to model performance while consuming substantial
computational resources. This paper presents an end-to-end feature selection
framework for tabular data based on game theory. We formulate feature selection
procedure based on a cooperative game where features are modeled as players,
and their importance is determined through the evaluation of synergistic
interactions and marginal contributions. The proposed framework comprises four
core components: sample selection, game-theoretic feature importance
evaluation, redundant feature elimination, and optimized model training.
Experimental results demonstrate that the proposed method achieves substantial
computation reduction while preserving predictive performance, thereby offering
an efficient solution of the computational challenges of large-scale machine
learning. The source code is available at
https://github.com/vectorsss/strategy_inputs.

</details>


### [55] [LRT-Diffusion: Calibrated Risk-Aware Guidance for Diffusion Policies](https://arxiv.org/abs/2510.24983)
*Ximan Sun,Xiang Cheng*

Main category: cs.LG

TL;DR: LRT-Diffusion是一种风险感知的采样方法，通过将每个去噪步骤视为无条件先验和状态条件策略头之间的顺序假设检验，为扩散策略添加了统计风险控制。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散策略在离线强化学习中具有竞争力，但在采样时通常使用缺乏统计风险概念的启发式方法进行引导。需要一种具有用户可解释风险预算的风险感知采样规则。

Method: 累积对数似然比，并使用逻辑控制器对条件均值进行门控，该控制器的阈值τ在H0下校准一次以满足用户指定的Type-I水平α。训练保持标准DDPM结构，LRT引导与Q梯度自然组合。

Result: 在D4RL MuJoCo任务上，LRT-Diffusion在实现中优于强Q引导基线，改善了回报-OOD权衡，同时满足期望的α水平。

Conclusion: LRT-Diffusion是一种即插即用的推理时方法，为离线RL的扩散策略添加了原则性、校准的风险控制，特别在离支撑误差占主导时优于Q引导。

Abstract: Diffusion policies are competitive for offline reinforcement learning (RL)
but are typically guided at sampling time by heuristics that lack a statistical
notion of risk. We introduce LRT-Diffusion, a risk-aware sampling rule that
treats each denoising step as a sequential hypothesis test between the
unconditional prior and the state-conditional policy head. Concretely, we
accumulate a log-likelihood ratio and gate the conditional mean with a logistic
controller whose threshold tau is calibrated once under H0 to meet a
user-specified Type-I level alpha. This turns guidance from a fixed push into
an evidence-driven adjustment with a user-interpretable risk budget.
Importantly, we deliberately leave training vanilla (two heads with standard
epsilon-prediction) under the structure of DDPM. LRT guidance composes
naturally with Q-gradients: critic-gradient updates can be taken at the
unconditional mean, at the LRT-gated mean, or a blend, exposing a continuum
from exploitation to conservatism. We standardize states and actions
consistently at train and test time and report a state-conditional
out-of-distribution (OOD) metric alongside return. On D4RL MuJoCo tasks,
LRT-Diffusion improves the return-OOD trade-off over strong Q-guided baselines
in our implementation while honoring the desired alpha. Theoretically, we
establish level-alpha calibration, concise stability bounds, and a return
comparison showing when LRT surpasses Q-guidance-especially when off-support
errors dominate. Overall, LRT-Diffusion is a drop-in, inference-time method
that adds principled, calibrated risk control to diffusion policies for offline
RL.

</details>


### [56] [Epileptic Seizure Detection and Prediction from EEG Data: A Machine Learning Approach with Clinical Validation](https://arxiv.org/abs/2510.24986)
*Ria Jayanti,Tanish Jain*

Main category: cs.LG

TL;DR: 该研究提出了一种结合癫痫实时检测和预测的新方法，使用多种机器学习算法在CHB-MIT头皮EEG数据库上进行评估，展示了从被动监测向主动预测的转变潜力。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅在癫痫发作后检测，限制了早期干预机会。本研究旨在开发能够预测癫痫发作的系统，实现更主动的治疗策略。

Method: 使用K近邻、逻辑回归、随机森林和支持向量机进行癫痫检测，采用LSTM网络进行癫痫预测，在CHB-MIT头皮EEG数据库（969小时记录，173次发作）上进行评估。

Result: 逻辑回归检测准确率90.9%，召回率89.6%；随机森林和SVM准确率94.0%但召回率为0%；LSTM预测准确率达到89.26%。

Conclusion: 研究证明了开发可访问的实时监测工具的潜力，能够预测癫痫发作，标志着从被动管理向主动预防的重要转变。

Abstract: In recent years, machine learning has become an increasingly powerful tool
for supporting seizure detection and monitoring in epilepsy care. Traditional
approaches focus on identifying seizures only after they begin, which limits
the opportunity for early intervention and proactive treatment. In this study,
we propose a novel approach that integrates both real-time seizure detection
and prediction, aiming to capture subtle temporal patterns in EEG data that may
indicate an upcoming seizure. Our approach was evaluated using the CHB-MIT
Scalp EEG Database, which includes 969 hours of recordings and 173 seizures
collected from 23 pediatric and young adult patients with drug-resistant
epilepsy. To support seizure detection, we implemented a range of supervised
machine learning algorithms, including K-Nearest Neighbors, Logistic
Regression, Random Forest, and Support Vector Machine. The Logistic Regression
achieved 90.9% detection accuracy with 89.6% recall, demonstrating balanced
performance suitable for clinical screening. Random Forest and Support Vector
Machine models achieved higher accuracy (94.0%) but with 0% recall, failing to
detect any seizures, illustrating that accuracy alone is insufficient for
evaluating medical ML models with class imbalance. For seizure prediction, we
employed Long Short-Term Memory (LSTM) networks, which use deep learning to
model temporal dependencies in EEG data. The LSTM model achieved 89.26%
prediction accuracy. These results highlight the potential of developing
accessible, real-time monitoring tools that not only detect seizures as
traditionally done, but also predict them before they occur. This ability to
predict seizures marks a significant shift from reactive seizure management to
a more proactive approach, allowing patients to anticipate seizures and take
precautionary measures to reduce the risk of injury or other complications.

</details>


### [57] [Enhancing Hierarchical Reinforcement Learning through Change Point Detection in Time Series](https://arxiv.org/abs/2510.24988)
*Hemanath Arumugam,Falong Fan,Bo Liu*

Main category: cs.LG

TL;DR: 提出了一种将自监督Transformer变化点检测模块集成到Option-Critic框架中的新架构，通过自适应轨迹分割实现选项发现，在Four-Rooms和Pinball任务中表现出加速收敛、更高累积回报和更好的选项专业化。


<details>
  <summary>Details</summary>
Motivation: 解决分层强化学习中自主发现语义上有意义的子目标和学习最优选项终止边界的实际挑战。

Method: 集成自监督Transformer变化点检测模块到Option-Critic框架，利用启发式伪标签推断环境动态的潜在变化，并通过变化点稳定终止函数梯度、预训练内部选项策略、强制功能专业化。

Result: 在Four-Rooms和Pinball任务中，CPD引导的智能体表现出加速收敛、更高累积回报和显著改进的选项专业化。

Conclusion: 通过变化点分割整合结构先验可以在复杂环境中产生更可解释、样本高效和鲁棒的分层策略。

Abstract: Hierarchical Reinforcement Learning (HRL) enhances the scalability of
decision-making in long-horizon tasks by introducing temporal abstraction
through options-policies that span multiple timesteps. Despite its theoretical
appeal, the practical implementation of HRL suffers from the challenge of
autonomously discovering semantically meaningful subgoals and learning optimal
option termination boundaries. This paper introduces a novel architecture that
integrates a self-supervised, Transformer-based Change Point Detection (CPD)
module into the Option-Critic framework, enabling adaptive segmentation of
state trajectories and the discovery of options. The CPD module is trained
using heuristic pseudo-labels derived from intrinsic signals to infer latent
shifts in environment dynamics without external supervision. These inferred
change-points are leveraged in three critical ways: (i) to serve as supervisory
signals for stabilizing termination function gradients, (ii) to pretrain
intra-option policies via segment-wise behavioral cloning, and (iii) to enforce
functional specialization through inter-option divergence penalties over
CPD-defined state partitions. The overall optimization objective enhances the
standard actor-critic loss using structure-aware auxiliary losses. In our
framework, option discovery arises naturally as CPD-defined trajectory segments
are mapped to distinct intra-option policies, enabling the agent to
autonomously partition its behavior into reusable, semantically meaningful
skills. Experiments on the Four-Rooms and Pinball tasks demonstrate that
CPD-guided agents exhibit accelerated convergence, higher cumulative returns,
and significantly improved option specialization. These findings confirm that
integrating structural priors via change-point segmentation leads to more
interpretable, sample-efficient, and robust hierarchical policies in complex
environments.

</details>


### [58] [What Really Matters in Matrix-Whitening Optimizers?](https://arxiv.org/abs/2510.25000)
*Kevin Frans,Pieter Abbeel,Sergey Levine*

Main category: cs.LG

TL;DR: 本文系统分析矩阵白化优化器的性能来源，发现方差自适应是解释性能差距的关键因素，而非仅靠准确的光谱归一化。


<details>
  <summary>Details</summary>
Motivation: 最近出现了一系列近似矩阵白化变换的优化器，需要系统解构这些优化器以理解其性能来源。

Method: 通过实验比较各种矩阵白化方法的变体，包括SOAP和Muon等优化器，分析其光谱归一化和方差自适应组件。

Result: 所有矩阵白化方法都可靠地优于元素级优化器如Adam；方差自适应版本始终优于符号下降对应版本；低秩方差估计器能有效降低内存成本且无性能损失。

Conclusion: 矩阵白化优化器的性能优势主要来自方差自适应组件，而非仅靠光谱归一化；低秩方差估计是实现高效矩阵白化优化的可行方案。

Abstract: A range of recent optimizers have emerged that approximate the same
"matrix-whitening" transformation in various ways. In this work, we
systematically deconstruct such optimizers, aiming to disentangle the key
components that explain performance. Across tuned hyperparameters across the
board, all flavors of matrix-whitening methods reliably outperform elementwise
counterparts, such as Adam. Matrix-whitening is often related to spectral
descent -- however, experiments reveal that performance gains are *not
explained solely by accurate spectral normalization* -- particularly, SOAP
displays the largest per-step gain, even though Muon more accurately descends
along the steepest spectral descent direction. Instead, we argue that
matrix-whitening serves two purposes, and the variance adaptation component of
matrix-whitening is the overlooked ingredient explaining this performance gap.
Experiments show that variance-adapted versions of optimizers consistently
outperform their sign-descent counterparts, including an adaptive version of
Muon. We further ablate variance adaptation strategies, finding that while
lookahead style approximations are not as effective, low-rank variance
estimators can effectively reduce memory costs without a performance loss.

</details>


### [59] [Disentangling Shared and Private Neural Dynamics with SPIRE: A Latent Modeling Framework for Deep Brain Stimulation](https://arxiv.org/abs/2510.25023)
*Rahil Soroushmojdehi,Sina Javadzadeh,Mehrnaz Asadi,Terence D. Sanger*

Main category: cs.LG

TL;DR: SPIRE是一种深度多编码器自编码器，能够将多区域神经记录分解为共享和私有潜在子空间，通过新颖的对齐和解缠损失来分离网络级动态与区域特定活动。


<details>
  <summary>Details</summary>
Motivation: 在多区域神经数据建模中，分离共享网络级动态与区域特定活动是一个核心挑战。

Method: 引入SPIRE（共享-私有区域间编码器），使用深度多编码器自编码器，通过新颖的对齐和解缠损失将记录分解为共享和私有潜在子空间。仅使用基线数据训练。

Result: 在具有真实潜在变量的合成基准测试中，SPIRE在非线性扭曲和时间错位下优于经典概率模型。应用于颅内深部脑刺激记录时，SPIRE显示共享潜在变量可靠地编码了刺激特异性特征，这些特征在不同部位和频率间具有泛化性。

Conclusion: SPIRE为分析刺激下多区域神经动态提供了一个实用、可复现的工具。

Abstract: Disentangling shared network-level dynamics from region-specific activity is
a central challenge in modeling multi-region neural data. We introduce SPIRE
(Shared-Private Inter-Regional Encoder), a deep multi-encoder autoencoder that
factorizes recordings into shared and private latent subspaces with novel
alignment and disentanglement losses. Trained solely on baseline data, SPIRE
robustly recovers cross-regional structure and reveals how external
perturbations reorganize it. On synthetic benchmarks with ground-truth latents,
SPIRE outperforms classical probabilistic models under nonlinear distortions
and temporal misalignments. Applied to intracranial deep brain stimulation
(DBS) recordings, SPIRE shows that shared latents reliably encode
stimulation-specific signatures that generalize across sites and frequencies.
These results establish SPIRE as a practical, reproducible tool for analyzing
multi-region neural dynamics under stimulation.

</details>


### [60] [Machine Learning based Analysis for Radiomics Features Robustness in Real-World Deployment Scenarios](https://arxiv.org/abs/2510.25026)
*Sarmad Ahmad Khan,Simon Bernatz,Zahra Moslehi,Florian Buettner*

Main category: cs.LG

TL;DR: 基于放射组学的机器学习模型在临床决策支持中具有潜力，但对成像协议、定位和分割变化引起的分布偏移很脆弱。本研究系统评估了五个MRI序列下放射组学模型的鲁棒性，发现使用协议不变特征训练的模型在分布偏移下保持高精度，而使用所有特征的模型性能下降40%。


<details>
  <summary>Details</summary>
Motivation: 放射组学模型在实际临床应用中面临成像协议、分割策略等变化导致的分布偏移问题，这会影响模型的可靠性和临床适用性。需要系统评估模型在不同分布偏移条件下的表现。

Method: 使用16个水果的体模，通过：(1) 五个MRI序列的协议变化；(2) 分割变化（完整、部分、旋转）；(3) 观察者间变异性来评估分布偏移。训练XGBoost分类器，比较协议不变特征与序列特定特征的表现。

Result: 使用协议不变特征训练的模型在分布偏移下F1分数>0.85，而使用所有特征的模型在协议变化下性能下降40%。数据增强显著改善了不确定性估计质量，将预期校准误差降低35%且不牺牲准确性。

Conclusion: 协议感知的特征选择和受控体模研究能有效预测模型在分布偏移下的行为，为开发对真实世界协议变化具有鲁棒性的放射组学模型提供了框架。

Abstract: Radiomics-based machine learning models show promise for clinical decision
support but are vulnerable to distribution shifts caused by variations in
imaging protocols, positioning, and segmentation. This study systematically
investigates the robustness of radiomics-based machine learning models under
distribution shifts across five MRI sequences. We evaluated how different
acquisition protocols and segmentation strategies affect model reliability in
terms of predictive power and uncertainty-awareness. Using a phantom of 16
fruits, we evaluated distribution shifts through: (1) protocol variations
across T2-HASTE, T2-TSE, T2-MAP, T1-TSE, and T2-FLAIR sequences; (2)
segmentation variations (full, partial, rotated); and (3) inter-observer
variability. We trained XGBoost classifiers on 8 consistent robust features
versus sequence-specific features, testing model performance under in-domain
and out-of-domain conditions. Results demonstrate that models trained on
protocol-invariant features maintain F1-scores >0.85 across distribution
shifts, while models using all features showed 40% performance degradation
under protocol changes. Dataset augmentation substantially improved the quality
of uncertainty estimates and reduced the expected calibration error (ECE) by
35% without sacrificing accuracy. Temperature scaling provided minimal
calibration benefits, confirming XGBoost's inherent reliability. Our findings
reveal that protocol-aware feature selection and controlled phantom studies
effectively predict model behavior under distribution shifts, providing a
framework for developing robust radiomics models resilient to real-world
protocol variations.

</details>


### [61] [Graph Distance Based on Cause-Effect Estimands with Latents](https://arxiv.org/abs/2510.25037)
*Zhufeng Li,Niki Kilbertus*

Main category: cs.LG

TL;DR: 提出一种基于因果效应估计任务的有向无环混合图距离度量方法，用于评估因果发现方法在潜在混杂下的性能


<details>
  <summary>Details</summary>
Motivation: 当前因果发现领域缺乏在潜在混杂情况下有效评估发现图质量的方法，难以衡量方法进展

Method: 使用基于固定识别的符号验证器，量化图差异对不同处理-结果对因果效应估计量的扭曲程度

Result: 分析了该度量在不同图扰动下的行为，并与现有距离度量进行了比较

Conclusion: 该距离度量能够更好地评估因果发现在潜在混杂下的性能，为方法比较提供更可靠的基准

Abstract: Causal discovery aims to recover graphs that represent causal relations among
given variables from observations, and new methods are constantly being
proposed. Increasingly, the community raises questions about how much progress
is made, because properly evaluating discovered graphs remains notoriously
difficult, particularly under latent confounding. We propose a graph distance
measure for acyclic directed mixed graphs (ADMGs) based on the downstream task
of cause-effect estimation under unobserved confounding. Our approach uses
identification via fixing and a symbolic verifier to quantify how graph
differences distort cause-effect estimands for different treatment-outcome
pairs. We analyze the behavior of the measure under different graph
perturbations and compare it against existing distance metrics.

</details>


### [62] [Dynamically Weighted Momentum with Adaptive Step Sizes for Efficient Deep Network Training](https://arxiv.org/abs/2510.25042)
*Zhifeng Wang,Longlong Li,Chunyan Zeng*

Main category: cs.LG

TL;DR: 提出DWMGrad优化算法，通过动态引导机制自适应调整动量和学习率，解决传统优化算法在处理复杂模型和非凸优化问题时的不足。


<details>
  <summary>Details</summary>
Motivation: 现有优化算法如SGD和Adam在处理学习效率波动、复杂模型需求和非凸优化问题时存在明显不足，特别是在处理复杂数据结构和模型时面临学习率选择困难、局部最优和高维空间导航等挑战。

Method: 基于传统方法，引入依赖历史数据的动态引导机制，动态更新动量和学习率，使优化器能灵活调整对历史信息的依赖，适应不同训练场景。

Result: 经过大量实验验证，DWMGrad在多种场景下能够实现更快的收敛速度和更高的准确率。

Conclusion: DWMGrad算法通过动态调整机制，显著提升了优化器对变化环境和任务复杂度的适应能力，在收敛性能和精度方面表现优异。

Abstract: Within the current sphere of deep learning research, despite the extensive
application of optimization algorithms such as Stochastic Gradient Descent
(SGD) and Adaptive Moment Estimation (Adam), there remains a pronounced
inadequacy in their capability to address fluctuations in learning efficiency,
meet the demands of complex models, and tackle non-convex optimization issues.
These challenges primarily arise from the algorithms' limitations in handling
complex data structures and models, for instance, difficulties in selecting an
appropriate learning rate, avoiding local optima, and navigating through
high-dimensional spaces. To address these issues, this paper introduces a novel
optimization algorithm named DWMGrad. This algorithm, building on the
foundations of traditional methods, incorporates a dynamic guidance mechanism
reliant on historical data to dynamically update momentum and learning rates.
This allows the optimizer to flexibly adjust its reliance on historical
information, adapting to various training scenarios. This strategy not only
enables the optimizer to better adapt to changing environments and task
complexities but also, as validated through extensive experimentation,
demonstrates DWMGrad's ability to achieve faster convergence rates and higher
accuracies under a multitude of scenarios.

</details>


### [63] [Training Across Reservoirs: Using Numerical Differentiation To Couple Trainable Networks With Black-Box Reservoirs](https://arxiv.org/abs/2510.25074)
*Andrew Clark,Jack Moursounidis,Osmaan Rasouli,William Gan,Cooper Doyle,Anna Leontjeva*

Main category: cs.LG

TL;DR: BOND是一种扰动方法，用于估计无法访问计算图的网络结构中的偏导数，相比现有方法具有更好的准确性和可扩展性，使集成黑盒函数的新架构探索成为可能。


<details>
  <summary>Details</summary>
Motivation: 探索如何在不增加可训练参数的情况下通过集成黑盒函数来提升模型性能，为结合模拟和数字设备扩展网络规模提供路径。

Method: 提出Bounded Numerical Differentiation (BOND)方法，这是一种扰动性方法，用于估计无法访问计算图的网络结构中的偏导数。

Result: 实验表明，将黑盒函数（实验中表现为固定的未训练网络）集成到模型中可以提高性能，而无需增加可训练参数数量，也无需对黑盒函数本身进行大量优化。

Conclusion: 利用固定的不可训练模块扩展模型容量具有潜力，为结合模拟和数字设备作为网络扩展机制指明了方向。

Abstract: We introduce Bounded Numerical Differentiation (BOND), a perturbative method
for estimating partial derivatives across network structures with inaccessible
computational graphs. BOND demonstrates improved accuracy and scalability from
existing perturbative methods, enabling new explorations of trainable
architectures that integrate black-box functions. We observe that these
black-box functions, realized in our experiments as fixed, untrained networks,
can enhance model performance without increasing the number of trainable
parameters. This improvement is achieved without extensive optimization of the
architecture or properties of the black-box function itself. Our findings
highlight the potential of leveraging fixed, non-trainable modules to expand
model capacity, suggesting a path toward combining analogue and digital devices
as a mechanism for scaling networks.

</details>


### [64] [Continual Low-Rank Adapters for LLM-based Generative Recommender Systems](https://arxiv.org/abs/2510.25093)
*Hyunsik Yoo,Ting-Wei Li,SeongKu Kang,Zhining Liu,Charlie Xu,Qilin Qi,Hanghang Tong*

Main category: cs.LG

TL;DR: PESO是一种用于推荐系统中LoRA持续学习的新方法，通过近端正则化平衡适应与保留，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA持续学习方法主要关注保留过去任务性能，但推荐系统的目标是预测当前偏好而非过去偏好，过时的偏好甚至可能损害性能。

Method: 提出PESO方法，引入近端正则化将当前适配器锚定到最近的冻结状态，在LoRA子空间中提供数据感知、方向性指导。

Result: 实证研究表明，PESO在持续学习性能上始终优于现有的基于LoRA的持续学习方法。

Conclusion: PESO通过近端正则化设计，能够灵活平衡适应与保留，更好地捕捉近期用户行为，是推荐系统持续学习的有效解决方案。

Abstract: While large language models (LLMs) achieve strong performance in
recommendation, they face challenges in continual learning as users, items, and
user preferences evolve over time. Existing LoRA-based continual methods
primarily focus on preserving performance on previous tasks, but this overlooks
the unique nature of recommendation: the goal is not to predict past
preferences, and outdated preferences can even harm performance when current
interests shift significantly. To address this, we propose PESO (Proximally
rEgularized Single evolving lOra, a continual adaptation method for LoRA in
recommendation. PESO introduces a proximal regularizer that anchors the current
adapter to its most recent frozen state, enabling the model to flexibly balance
adaptation and preservation, and to better capture recent user behaviors.
Theoretically, we show that this proximal design provides data-aware,
direction-wise guidance in the LoRA subspace. Empirically, PESO consistently
outperforms existing LoRA-based continual learning methods.

</details>


### [65] [Learning Fair Graph Representations with Multi-view Information Bottleneck](https://arxiv.org/abs/2510.25096)
*Chuxun Liu,Debo Cheng,Qingfeng Chen,Jiangzhang Gan,Jiuyong Li,Lin Liu*

Main category: cs.LG

TL;DR: FairMIB是一个多视图信息瓶颈框架，通过分解图的特征、结构和扩散视图来缓解GNN中的复杂性偏见，使用对比学习和条件信息瓶颈来平衡任务效用和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有的公平性方法通常将偏见视为单一来源，忽略了属性和结构效应的差异，导致公平性和效用之间的权衡不理想。

Method: 提出FairMIB框架，将图分解为特征、结构和扩散三个视图，使用对比学习最大化跨视图互信息，集成多视角条件信息瓶颈目标，并在扩散视图中引入逆概率加权邻接校正。

Result: 在五个真实世界基准数据集上的实验表明，FairMIB在效用和公平性指标上都达到了最先进的性能。

Conclusion: FairMIB通过多视图分解和信息瓶颈方法有效解决了GNN中的公平性问题，在保持任务效用的同时显著提升了公平性。

Abstract: Graph neural networks (GNNs) excel on relational data by passing messages
over node features and structure, but they can amplify training data biases,
propagating discriminatory attributes and structural imbalances into unfair
outcomes. Many fairness methods treat bias as a single source, ignoring
distinct attribute and structure effects and leading to suboptimal fairness and
utility trade-offs. To overcome this challenge, we propose FairMIB, a
multi-view information bottleneck framework designed to decompose graphs into
feature, structural, and diffusion views for mitigating complexity biases in
GNNs. Especially, the proposed FairMIB employs contrastive learning to maximize
cross-view mutual information for bias-free representation learning. It further
integrates multi-perspective conditional information bottleneck objectives to
balance task utility and fairness by minimizing mutual information with
sensitive attributes. Additionally, FairMIB introduces an inverse
probability-weighted (IPW) adjacency correction in the diffusion view, which
reduces the spread of bias propagation during message passing. Experiments on
five real-world benchmark datasets demonstrate that FairMIB achieves
state-of-the-art performance across both utility and fairness metrics.

</details>


### [66] [Shift is Good: Mismatched Data Mixing Improves Test Performance](https://arxiv.org/abs/2510.25108)
*Marko Medvedev,Kaifeng Lyu,Zhiyuan Li,Nathan Srebro*

Main category: cs.LG

TL;DR: 该论文研究发现，在某些情况下，训练和测试分布的不匹配（分布偏移）实际上可能有益，能够提升测试性能，即使各组成成分之间没有关联或迁移学习。


<details>
  <summary>Details</summary>
Motivation: 探讨训练和测试数据分布比例不匹配时的影响，挑战传统认为分布偏移总是有害的观点。

Method: 通过理论分析和多种场景下的实验，识别最优训练比例，并分析分布偏移的益处程度。

Result: 研究表明，在某些设置中，分布偏移可以是有益的，测试性能可能因训练比例不匹配而提高。

Conclusion: 分布偏移在某些情况下具有积极影响，可以优化测试性能，这一发现也适用于成分技能分布不同的组合场景。

Abstract: We consider training and testing on mixture distributions with different
training and test proportions. We show that in many settings, and in some sense
generically, distribution shift can be beneficial, and test performance can
improve due to mismatched training proportions, even if the components are
unrelated and with no transfer between components. In a variety of scenarios,
we identify the optimal training proportions and the extent to which such
distribution shift can be beneficial. We show how the same analysis applies
also to a compositional setting with differing distribution of component
"skills'' at training and test.

</details>


### [67] [The Neural Differential Manifold: An Architecture with Explicit Geometric Structure](https://arxiv.org/abs/2510.25113)
*Di Zhang*

Main category: cs.LG

TL;DR: 提出神经微分流形(NDM)新架构，将神经网络重新概念化为微分流形，通过几何结构增强泛化能力和可解释性


<details>
  <summary>Details</summary>
Motivation: 传统神经网络使用欧几里得参数空间，缺乏几何结构。NDM旨在将几何结构显式融入网络设计，提供内在正则化并增强可解释性

Method: 三层架构：坐标层实现平滑图表转换，几何层通过辅助子网络动态生成流形度量，演化层通过双目标损失函数优化任务性能和几何简洁性

Result: NDM框架支持与学习流形几何对齐的自然梯度下降优化，为内部表示提供清晰的几何意义，增强泛化能力和鲁棒性

Conclusion: 神经微分流形代表了向几何结构化、可解释且高效深度学习系统的根本性转变，尽管存在计算挑战，但具有优化效率、持续学习和科学发现应用的潜力

Abstract: This paper introduces the Neural Differential Manifold (NDM), a novel neural
network architecture that explicitly incorporates geometric structure into its
fundamental design. Departing from conventional Euclidean parameter spaces, the
NDM re-conceptualizes a neural network as a differentiable manifold where each
layer functions as a local coordinate chart, and the network parameters
directly parameterize a Riemannian metric tensor at every point. The
architecture is organized into three synergistic layers: a Coordinate Layer
implementing smooth chart transitions via invertible transformations inspired
by normalizing flows, a Geometric Layer that dynamically generates the
manifold's metric through auxiliary sub-networks, and an Evolution Layer that
optimizes both task performance and geometric simplicity through a
dual-objective loss function. This geometric regularization penalizes excessive
curvature and volume distortion, providing intrinsic regularization that
enhances generalization and robustness. The framework enables natural gradient
descent optimization aligned with the learned manifold geometry and offers
unprecedented interpretability by endowing internal representations with clear
geometric meaning. We analyze the theoretical advantages of this approach,
including its potential for more efficient optimization, enhanced continual
learning, and applications in scientific discovery and controllable generative
modeling. While significant computational challenges remain, the Neural
Differential Manifold represents a fundamental shift towards geometrically
structured, interpretable, and efficient deep learning systems.

</details>


### [68] [A Unified Bilevel Model for Adversarial Learning and A Case Study](https://arxiv.org/abs/2510.25121)
*Yutong Zheng,Qingna Li*

Main category: cs.LG

TL;DR: 本文提出了一个统一的双层对抗学习模型，从数据扰动角度解释聚类模型中的对抗攻击机制，并分析了δ-度量在衡量攻击效果时的良好定义性。


<details>
  <summary>Details</summary>
Motivation: 由于大多数机器学习模型结构复杂，对抗攻击的机制尚未得到很好解释，如何衡量攻击效果仍不清楚。

Method: 提出统一的双层对抗学习模型，从数据扰动角度研究聚类模型中的对抗攻击，分析δ-度量的良好定义性。

Result: 当数据扰动较小时，聚类模型具有鲁棒性；当扰动较大时，聚类结果会发生变化，从而导致攻击。

Conclusion: δ-度量可以用于所提出的聚类模型对抗学习双层模型中衡量攻击效果。

Abstract: Adversarial learning has been attracting more and more attention thanks to
the fast development of machine learning and artificial intelligence. However,
due to the complicated structure of most machine learning models, the mechanism
of adversarial attacks is not well interpreted. How to measure the effect of
attack is still not quite clear. In this paper, we propose a unified bilevel
model for adversarial learning. We further investigate the adversarial attack
in clustering models and interpret it from data perturbation point of view. We
reveal that when the data perturbation is relatively small, the clustering
model is robust, whereas if it is relatively large, the clustering result
changes, which leads to an attack. To measure the effect of attacks for
clustering models, we analyse the well-definedness of the so-called
$\delta$-measure, which can be used in the proposed bilevel model for
adversarial learning of clustering models.

</details>


### [69] [Learning Low Rank Neural Representations of Hyperbolic Wave Dynamics from Data](https://arxiv.org/abs/2510.25123)
*Woojin Cho,Kookjin Lee,Noseong Park,Donsub Rim,Gerrit Welper*

Main category: cs.LG

TL;DR: 提出了一种基于低秩神经表示（LRNR）的数据驱动降维方法，专门用于处理双曲波传播的物理数据，通过深度学习技术学习波传播的高效低维表示。


<details>
  <summary>Details</summary>
Motivation: 针对物理基础的双曲波传播数据，寻找高效的降维表示方法，基于理论证明这类波存在高效表示的可能性。

Method: 在超网络框架中使用专门设计的低秩神经表示（LRNR）架构，结合深度学习技术直接从数据中学习波传播的低维表示。

Result: 训练后的LRNR自然产生低秩张量表示，揭示了波传播的新分解方式，每个分解模式对应可解释的物理特征，并支持通过压缩方案进行高效推理。

Conclusion: LRNR架构能够有效学习波传播的低维表示，不仅提供物理可解释性，还具备高效推理能力，在性能要求高的场景中具有重要应用价值。

Abstract: We present a data-driven dimensionality reduction method that is well-suited
for physics-based data representing hyperbolic wave propagation. The method
utilizes a specialized neural network architecture called low rank neural
representation (LRNR) inside a hypernetwork framework. The architecture is
motivated by theoretical results that rigorously prove the existence of
efficient representations for this wave class. We illustrate through archetypal
examples that such an efficient low-dimensional representation of propagating
waves can be learned directly from data through a combination of deep learning
techniques. We observe that a low rank tensor representation arises naturally
in the trained LRNRs, and that this reveals a new decomposition of wave
propagation where each decomposed mode corresponds to interpretable physical
features. Furthermore, we demonstrate that the LRNR architecture enables
efficient inference via a compression scheme, which is a potentially important
feature when deploying LRNRs in demanding performance regimes.

</details>


### [70] [Bridging the Divide: End-to-End Sequence-Graph Learning](https://arxiv.org/abs/2510.25126)
*Yuen Chen,Yulun Wu,Samuel Sharpe,Igor Melnyk,Nam H. Nguyen,Furong Huang,C. Bayan Bruss,Rizal Fathony*

Main category: cs.LG

TL;DR: BRIDGE是一个统一的端到端架构，将序列编码器与图神经网络耦合在单一目标下，通过token级交叉注意力层实现邻居间细粒度消息传递，在友谊预测和欺诈检测任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集通常同时具有序列性和关系性，但现有方法往往忽视其中一种模态。作者认为序列和图不是分离的问题，而是同一数据集的互补方面，应该联合学习。

Method: 提出BRIDGE架构，将序列编码器与GNN在单一目标下耦合，允许梯度在两者间流动。引入TOKENXATTN层实现邻居序列间事件级别的细粒度消息传递。

Result: 在友谊预测（Brightkite）和欺诈检测（Amazon）两个场景中，BRIDGE在排序和分类指标上持续优于静态GNN、时序图方法和纯序列基线。

Conclusion: 序列和图应该联合建模，BRIDGE通过耦合序列编码器和GNN实现了更好的表示学习，证明了多模态联合学习的有效性。

Abstract: Many real-world datasets are both sequential and relational: each node
carries an event sequence while edges encode interactions. Existing methods in
sequence modeling and graph modeling often neglect one modality or the other.
We argue that sequences and graphs are not separate problems but complementary
facets of the same dataset, and should be learned jointly. We introduce BRIDGE,
a unified end-to-end architecture that couples a sequence encoder with a GNN
under a single objective, allowing gradients to flow across both modules and
learning task-aligned representations. To enable fine-grained token-level
message passing among neighbors, we add TOKENXATTN, a token-level
cross-attention layer that passes messages between events in neighboring
sequences. Across two settings, friendship prediction (Brightkite) and fraud
detection (Amazon), BRIDGE consistently outperforms static GNNs, temporal graph
methods, and sequence-only baselines on ranking and classification metrics.

</details>


### [71] [An Analysis of Causal Effect Estimation using Outcome Invariant Data Augmentation](https://arxiv.org/abs/2510.25128)
*Uzair Akbar,Niki Kilbertus,Hao Shen,Krikamol Muandet,Bo Dai*

Main category: cs.LG

TL;DR: 论文提出了一个统一框架，将数据增强与因果推断结合，证明当结果生成机制对数据增强选择不变时，数据增强可被视为对治疗生成机制的干预，有助于减少隐藏混杂因素带来的因果效应估计偏差。


<details>
  <summary>Details</summary>
Motivation: 传统数据增强主要用于i.i.d.设置下的正则化，但工具变量在许多应用中不易获得。本文旨在探索数据增强在跨干预泛化中的应用，特别是在存在未观测混杂的情况下。

Method: 通过适当正则化基于工具变量的估计器，引入IV-like回归概念来缓解混杂偏差；将参数化数据增强建模为IV-like回归问题，并通过组合模拟最坏情况的数据增强应用。

Result: 理论和模拟实验表明，该方法在因果估计和泛化任务上优于简单数据增强，在线性示例的有限样本情况下得到验证，并通过真实数据实验支持。

Conclusion: 数据增强可超越传统i.i.d.设置，作为工具变量的替代方案，在存在未观测混杂时改善因果效应估计和跨干预泛化性能。

Abstract: The technique of data augmentation (DA) is often used in machine learning for
regularization purposes to better generalize under i.i.d. settings. In this
work, we present a unifying framework with topics in causal inference to make a
case for the use of DA beyond just the i.i.d. setting, but for generalization
across interventions as well. Specifically, we argue that when the outcome
generating mechanism is invariant to our choice of DA, then such augmentations
can effectively be thought of as interventions on the treatment generating
mechanism itself. This can potentially help to reduce bias in causal effect
estimation arising from hidden confounders. In the presence of such unobserved
confounding we typically make use of instrumental variables (IVs) -- sources of
treatment randomization that are conditionally independent of the outcome.
However, IVs may not be as readily available as DA for many applications, which
is the main motivation behind this work. By appropriately regularizing IV based
estimators, we introduce the concept of IV-like (IVL) regression for mitigating
confounding bias and improving predictive performance across interventions even
when certain IV properties are relaxed. Finally, we cast parameterized DA as an
IVL regression problem and show that when used in composition can simulate a
worst-case application of such DA, further improving performance on causal
estimation and generalization tasks beyond what simple DA may offer. This is
shown both theoretically for the population case and via simulation experiments
for the finite sample case using a simple linear example. We also present real
data experiments to support our case.

</details>


### [72] [Lipschitz-aware Linearity Grafting for Certified Robustness](https://arxiv.org/abs/2510.25130)
*Yongjin Han,Suhyun Kim*

Main category: cs.LG

TL;DR: 本文提出了一种基于线性嫁接的Lipschitz常数优化方法，通过将非线性激活函数替换为线性函数来消除近似误差，从而获得更紧的局部Lipschitz常数，提高认证鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在计算Lipschitz常数时存在近似误差问题，这些误差阻碍了获得紧的局部Lipschitz常数，而紧的Lipschitz常数对于认证鲁棒性至关重要。线性嫁接可以消除近似误差，但之前缺乏理论分析。

Method: 提出Lipschitz感知的线性嫁接方法，将非线性激活函数替换为线性函数，消除主导的近似误差源。通过理论分析证明线性嫁接如何改善认证鲁棒性。

Result: 实验表明，在线性嫁接后，l∞局部Lipschitz常数变得更紧，认证鲁棒性得到提升，即使没有进行认证训练。

Conclusion: 线性嫁接通过消除近似误差，能够有效收紧局部Lipschitz常数，从而改善神经网络的认证鲁棒性，为认证鲁棒性研究提供了新的理论视角和方法。

Abstract: Lipschitz constant is a fundamental property in certified robustness, as
smaller values imply robustness to adversarial examples when a model is
confident in its prediction. However, identifying the worst-case adversarial
examples is known to be an NP-complete problem. Although over-approximation
methods have shown success in neural network verification to address this
challenge, reducing approximation errors remains a significant obstacle.
Furthermore, these approximation errors hinder the ability to obtain tight
local Lipschitz constants, which are crucial for certified robustness.
Originally, grafting linearity into non-linear activation functions was
proposed to reduce the number of unstable neurons, enabling scalable and
complete verification. However, no prior theoretical analysis has explained how
linearity grafting improves certified robustness. We instead consider linearity
grafting primarily as a means of eliminating approximation errors rather than
reducing the number of unstable neurons, since linear functions do not require
relaxation. In this paper, we provide two theoretical contributions: 1) why
linearity grafting improves certified robustness through the lens of the
$l_\infty$ local Lipschitz constant, and 2) grafting linearity into non-linear
activation functions, the dominant source of approximation errors, yields a
tighter local Lipschitz constant. Based on these theoretical contributions, we
propose a Lipschitz-aware linearity grafting method that removes dominant
approximation errors, which are crucial for tightening the local Lipschitz
constant, thereby improving certified robustness, even without certified
training. Our extensive experiments demonstrate that grafting linearity into
these influential activations tightens the $l_\infty$ local Lipschitz constant
and enhances certified robustness.

</details>


### [73] [Machine Learning Guided Optimal Transmission Switching to Mitigate Wildfire Ignition Risk](https://arxiv.org/abs/2510.25147)
*Weimin Huang,Ryan Piansky,Bistra Dilkina,Daniel K. Molzahn*

Main category: cs.LG

TL;DR: 本文提出了一种机器学习引导的框架，用于快速解决优化电力停运问题，以管理野火点燃风险，同时减少负荷削减。


<details>
  <summary>Details</summary>
Motivation: 为了缓解野火点燃风险，电力公司需要在高风险区域停运电力线路。优化电力停运问题需要快速频繁地解决，而传统方法计算成本高。

Method: 开发了机器学习引导的框架，扩展现有ML引导的混合整数线性规划求解方法，并整合了关于通电和断电线路数量的领域知识。

Result: 在加州大型合成测试系统上的结果显示，所提出的ML引导方法比传统优化方法更快地产生高质量解决方案。

Conclusion: 机器学习引导的方法能够有效解决优化电力停运问题，在保证解决方案质量的同时显著提高求解速度。

Abstract: To mitigate acute wildfire ignition risks, utilities de-energize power lines
in high-risk areas. The Optimal Power Shutoff (OPS) problem optimizes line
energization statuses to manage wildfire ignition risks through
de-energizations while reducing load shedding. OPS problems are computationally
challenging Mixed-Integer Linear Programs (MILPs) that must be solved rapidly
and frequently in operational settings. For a particular power system, OPS
instances share a common structure with varying parameters related to wildfire
risks, loads, and renewable generation. This motivates the use of Machine
Learning (ML) for solving OPS problems by exploiting shared patterns across
instances. In this paper, we develop an ML-guided framework that quickly
produces high-quality de-energization decisions by extending existing ML-guided
MILP solution methods while integrating domain knowledge on the number of
energized and de-energized lines. Results on a large-scale realistic
California-based synthetic test system show that the proposed ML-guided method
produces high-quality solutions faster than traditional optimization methods.

</details>


### [74] [Machine Learning and CPU (Central Processing Unit) Scheduling Co-Optimization over a Network of Computing Centers](https://arxiv.org/abs/2510.25176)
*Mohammadreza Doostmohammadian,Zulfiya R. Gabidullina,Hamid R. Rabiee*

Main category: cs.LG

TL;DR: 提出了一种分布式机器学习的计算资源优化算法，通过协同优化数据分配和CPU资源分配，在时变网络中实现高效训练。


<details>
  <summary>Details</summary>
Motivation: 随着AI研究快速发展，对快速、计算高效且可扩展的分布式机器学习解决方案的需求日益增长。

Method: 设计协同优化框架，同时优化数据处理和计算资源分配，支持时变网络和量化通信，使用Lyapunov稳定性分析证明收敛性。

Result: 与现有CPU调度方案相比，该算法将成本最优性差距降低了50%以上。

Conclusion: 提出的算法在分布式机器学习环境中有效优化了计算资源分配，显著提升了性能。

Abstract: In the rapidly evolving research on artificial intelligence (AI) the demand
for fast, computationally efficient, and scalable solutions has increased in
recent years. The problem of optimizing the computing resources for distributed
machine learning (ML) and optimization is considered in this paper. Given a set
of data distributed over a network of computing-nodes/servers, the idea is to
optimally assign the CPU (central processing unit) usage while simultaneously
training each computing node locally via its own share of data. This formulates
the problem as a co-optimization setup to (i) optimize the data processing and
(ii) optimally allocate the computing resources. The information-sharing
network among the nodes might be time-varying, but with balanced weights to
ensure consensus-type convergence of the algorithm. The algorithm is all-time
feasible, which implies that the computing resource-demand balance constraint
holds at all iterations of the proposed solution. Moreover, the solution allows
addressing possible log-scale quantization over the information-sharing
channels to exchange log-quantized data. For some example applications,
distributed support-vector-machine (SVM) and regression are considered as the
ML training models. Results from perturbation theory, along with Lyapunov
stability and eigen-spectrum analysis, are used to prove the convergence
towards the optimal case. As compared to existing CPU scheduling solutions, the
proposed algorithm improves the cost optimality gap by more than $50\%$.

</details>


### [75] [Selective Learning for Deep Time Series Forecasting](https://arxiv.org/abs/2510.25207)
*Yisong Fu,Zezhi Shao,Chengqing Yu,Yujie Li,Zhulin An,Qi Wang,Yongjun Xu,Fei Wang*

Main category: cs.LG

TL;DR: 提出了一种用于深度时间序列预测的选择性学习策略，通过双掩码机制筛选可泛化的时间步来避免过拟合


<details>
  <summary>Details</summary>
Motivation: 深度模型在时间序列预测中容易因噪声和异常值而严重过拟合，传统方法对所有时间步进行统一优化导致模型学习不确定和异常时间步

Method: 选择性学习策略，使用双掩码机制：不确定性掩码利用残差熵过滤不确定时间步，异常掩码使用残差下界估计排除异常时间步

Result: 在8个真实世界数据集上的实验表明，该方法显著提升了主流深度模型的预测性能，包括Informer的MSE降低37.4%，TimesNet降低8.4%，iTransformer降低6.5%

Conclusion: 选择性学习能够有效指导模型关注可泛化的时间步，显著改善深度时间序列预测模型的性能

Abstract: Benefiting from high capacity for capturing complex temporal patterns, deep
learning (DL) has significantly advanced time series forecasting (TSF).
However, deep models tend to suffer from severe overfitting due to the inherent
vulnerability of time series to noise and anomalies. The prevailing DL paradigm
uniformly optimizes all timesteps through the MSE loss and learns those
uncertain and anomalous timesteps without difference, ultimately resulting in
overfitting. To address this, we propose a novel selective learning strategy
for deep TSF. Specifically, selective learning screens a subset of the whole
timesteps to calculate the MSE loss in optimization, guiding the model to focus
on generalizable timesteps while disregarding non-generalizable ones. Our
framework introduces a dual-mask mechanism to target timesteps: (1) an
uncertainty mask leveraging residual entropy to filter uncertain timesteps, and
(2) an anomaly mask employing residual lower bound estimation to exclude
anomalous timesteps. Extensive experiments across eight real-world datasets
demonstrate that selective learning can significantly improve the predictive
performance for typical state-of-the-art deep models, including 37.4% MSE
reduction for Informer, 8.4% for TimesNet, and 6.5% for iTransformer.

</details>


### [76] [Cost-Sensitive Unbiased Risk Estimation for Multi-Class Positive-Unlabeled Learning](https://arxiv.org/abs/2510.25226)
*Miao Zhang,Junpeng Li,Changchun Hua,Yana Yang*

Main category: cs.LG

TL;DR: 提出了一种基于自适应损失加权的成本敏感多类正-未标记学习方法，通过为正向和推断负向损失分配数据依赖的权重，实现无偏风险估计。


<details>
  <summary>Details</summary>
Motivation: 在多类正-未标记学习中，许多现有方法无法确保无偏风险估计，这限制了性能和稳定性。

Method: 在经验风险最小化框架下，为正向和从未标记混合数据中推断出的负向损失分配不同的数据依赖权重，使经验目标成为目标风险的无偏估计器。

Result: 在八个公共数据集上的广泛实验表明，在不同类别先验和类别数量的情况下，相对于强基线方法在准确性和稳定性方面都取得了持续提升。

Conclusion: 提出的自适应损失加权方法在多类正-未标记学习场景中实现了更好的性能和稳定性。

Abstract: Positive--Unlabeled (PU) learning considers settings in which only positive
and unlabeled data are available, while negatives are missing or left
unlabeled. This situation is common in real applications where annotating
reliable negatives is difficult or costly. Despite substantial progress in PU
learning, the multi-class case (MPU) remains challenging: many existing
approaches do not ensure \emph{unbiased risk estimation}, which limits
performance and stability. We propose a cost-sensitive multi-class PU method
based on \emph{adaptive loss weighting}. Within the empirical risk minimization
framework, we assign distinct, data-dependent weights to the positive and
\emph{inferred-negative} (from the unlabeled mixture) loss components so that
the resulting empirical objective is an unbiased estimator of the target risk.
We formalize the MPU data-generating process and establish a generalization
error bound for the proposed estimator. Extensive experiments on \textbf{eight}
public datasets, spanning varying class priors and numbers of classes, show
consistent gains over strong baselines in both accuracy and stability.

</details>


### [77] [BSFA: Leveraging the Subspace Dichotomy to Accelerate Neural Network Training](https://arxiv.org/abs/2510.25244)
*Wenjie Zhou,Bohan Wang,Wei Chen,Xueqi Cheng*

Main category: cs.LG

TL;DR: BSFA框架通过区分处理损失Hessian矩阵的特征子空间来加速深度学习训练：在主导特征方向（Dom-space）抑制更新以增强稳定性，在其余特征方向（Bulk-space）放大更新以加速收敛。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明深度学习优化中存在一个根本性二分法：沿损失Hessian矩阵顶部特征方向的参数更新虽然幅度大但对损失减少贡献小，而正交方向（Bulk-space）的更新幅度小但驱动大部分学习进展。

Method: 提出Bulk-Space-Filtration-Accelerator (BSFA)框架，通过PCA对历史更新进行高效子空间估计，并采用分块策略在每个参数块上应用估计，对不同子空间的更新分量进行差异化缩放。

Result: 在各种任务中展示了BSFA的加速效果，特别是在WikiText-103上预训练LLaMA-72M和在OpenWebText上预训练LLaMA-134M时，相比标准AdamW实现了约2倍的加速。

Conclusion: BSFA是一个实用且可扩展的即插即用框架，通过智能地调节不同特征子空间的更新分量，同时增强了训练稳定性和收敛速度。

Abstract: Recent studies \citep{gur2018gradient,song2024does, wen2024understanding}
highlight a fundamental dichotomy in deep learning optimization: Although
parameter updates along the top eigendirections of the loss Hessian (Dom-space)
capture most of the update magnitude, they often contribute minimally to loss
reduction. In contrast, updates in the orthogonal component (Bulk-space) have
smaller magnitudes but drive most learning progress. In this work, we further
advance the understanding of this phenomenon and introduce the
\textbf{Bulk-Space-Filtration-Accelerator (BSFA)}, a novel plug-and-play
framework. BSFA accelerates training by differentially scaling update
components projected onto these distinct subspaces, simultaneously enhancing
stability by moderating updates in the dominant subspace and boosting
convergence speed by amplifying those in the bulk-space. To ensure BSFA is both
practical and scalable for contemporary large models, we introduce two key
innovations: an efficient estimator using Principal Component Analysis (PCA) on
historical updates for fast subspace estimation, and a block-wise strategy that
applies this estimation on a per-parameter-block basis. These designs make BSFA
computationally tractable and highly effective. We demonstrate BSFA's
acceleration across various tasks, notably achieving approximately 2$\times$
speedup when pre-training LLaMA-72M on WikiText-103 and LLaMA-134M on
OpenWebText compared to vanilla AdamW.

</details>


### [78] [Near-Equilibrium Propagation training in nonlinear wave systems](https://arxiv.org/abs/2510.16084)
*Karol Sajnok,Michał Matuszewski*

Main category: cs.LG

TL;DR: 将平衡传播学习算法扩展到离散和连续复值波系统，在弱耗散状态下有效，适用于多种物理系统，通过局部可训练参数实现原位学习。


<details>
  <summary>Details</summary>
Motivation: 反向传播算法在物理神经网络中难以实现，平衡传播(EP)作为替代方案具有相似效率且适合原位训练，但需要扩展到更广泛的物理系统。

Method: 扩展EP学习到离散和连续复值波系统，在弱耗散状态下有效，用可训练的局部势能替代节点间连接，在激子-极化激子凝聚体中测试。

Result: 在标准基准测试（包括逻辑任务和手写数字识别）中表现出稳定收敛，验证了方法的有效性。

Conclusion: 为系统控制仅限于局部参数的物理系统中实现原位学习提供了实用途径。

Abstract: Backpropagation learning algorithm, the workhorse of modern artificial
intelligence, is notoriously difficult to implement in physical neural
networks. Equilibrium Propagation (EP) is an alternative with comparable
efficiency and strong potential for in-situ training. We extend EP learning to
both discrete and continuous complex-valued wave systems. In contrast to
previous EP implementations, our scheme is valid in the weakly dissipative
regime, and readily applicable to a wide range of physical settings, even
without well defined nodes, where trainable inter-node connections can be
replaced by trainable local potential. We test the method in driven-dissipative
exciton-polariton condensates governed by generalized Gross-Pitaevskii
dynamics. Numerical studies on standard benchmarks, including a simple logical
task and handwritten-digit recognition, demonstrate stable convergence,
establishing a practical route to in-situ learning in physical systems in which
system control is restricted to local parameters.

</details>


### [79] [Scaling Up Bayesian DAG Sampling](https://arxiv.org/abs/2510.25254)
*Daniele Nikzad,Alexander Zhilkin,Juha Harviainen,Jack Kuipers,Giusi Moffa,Mikko Koivisto*

Main category: cs.LG

TL;DR: 提出了两种改进贝叶斯网络结构采样的技术：高效实现基本移动操作和通过预处理修剪可能父集来加速父集求和


<details>
  <summary>Details</summary>
Motivation: 贝叶斯网络结构推断通常通过马尔可夫链采样进行，但现有方法效率有待提升

Method: 1. 高效实现添加、删除或反转单条边的基本移动操作；2. 设计预处理方法修剪可能父集，近似保持求和结果

Result: 实证研究表明，相比之前的方法，这些技术能带来显著的效率提升

Conclusion: 提出的两种技术有效提高了贝叶斯网络结构采样的效率

Abstract: Bayesian inference of Bayesian network structures is often performed by
sampling directed acyclic graphs along an appropriately constructed Markov
chain. We present two techniques to improve sampling. First, we give an
efficient implementation of basic moves, which add, delete, or reverse a single
arc. Second, we expedite summing over parent sets, an expensive task required
for more sophisticated moves: we devise a preprocessing method to prune
possible parent sets so as to approximately preserve the sums. Our empirical
study shows that our techniques can yield substantial efficiency gains compared
to previous methods.

</details>


### [80] [Hybrid Quantum-Classical Recurrent Neural Networks](https://arxiv.org/abs/2510.25557)
*Wenduan Xu*

Main category: cs.LG

TL;DR: 提出了一种混合量子-经典循环神经网络架构，其中整个循环核心由参数化量子电路实现，并由经典前馈网络控制。该模型在多个序列学习任务中展示了与强经典基线相竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 构建一个物理一致的量子循环神经网络，将量子系统的指数大状态空间与经典非线性控制相结合，实现高容量记忆和输入条件参数化。

Method: 使用n量子比特参数化量子电路作为隐藏状态，通过中间电路测量获得读出，结合输入嵌入和经典前馈网络提供非线性控制，通过酉动力学更新隐藏状态。

Result: 在情感分析、MNIST、置换MNIST、复制记忆和语言建模等任务中，使用最多14个量子比特进行模拟评估，模型表现与强经典基线相竞争。

Conclusion: 这是第一个基于量子操作并在广泛序列学习任务中实现竞争性能的模型，成功统一了酉循环、部分观测和非线性经典控制。

Abstract: We present a hybrid quantum-classical recurrent neural network (QRNN)
architecture in which the entire recurrent core is realized as a parametrized
quantum circuit (PQC) controlled by a classical feedforward network. The hidden
state is the quantum state of an $n$-qubit PQC, residing in an exponentially
large Hilbert space $\mathbb{C}^{2^n}$. The PQC is unitary by construction,
making the hidden-state evolution norm-preserving without external constraints.
At each timestep, mid-circuit readouts are combined with the input embedding
and processed by the feedforward network, which provides explicit classical
nonlinearity. The outputs parametrize the PQC, which updates the hidden state
via unitary dynamics. The QRNN is compact and physically consistent, and it
unifies (i) unitary recurrence as a high-capacity memory, (ii) partial
observation via mid-circuit measurements, and (iii) nonlinear classical control
for input-conditioned parametrization. We evaluate the model in simulation with
up to 14 qubits on sentiment analysis, MNIST, permuted MNIST, copying memory,
and language modeling, adopting projective measurements as a limiting case to
obtain mid-circuit readouts while maintaining a coherent recurrent quantum
memory. We further devise a soft attention mechanism over the mid-circuit
readouts in a sequence-to-sequence model and show its effectiveness for machine
translation. To our knowledge, this is the first model (RNN or otherwise)
grounded in quantum operations to achieve competitive performance against
strong classical baselines across a broad class of sequence-learning tasks.

</details>


### [81] [IBNorm: Information-Bottleneck Inspired Normalization for Representation Learning](https://arxiv.org/abs/2510.25262)
*Xiandong Zou,Pan Zhou*

Main category: cs.LG

TL;DR: 提出基于信息瓶颈原理的IB-Inspired Normalization (IBNorm)，通过有界压缩操作鼓励嵌入保留预测信息同时抑制无关变异性，在理论和实验上均优于传统的BatchNorm、LayerNorm和RMSNorm。


<details>
  <summary>Details</summary>
Motivation: 现有归一化方法如BatchNorm、LayerNorm和RMSNorm都是方差中心的，通过强制零均值和单位方差来稳定训练，但没有控制表示如何捕获任务相关信息。

Method: 基于信息瓶颈原理，引入有界压缩操作，鼓励嵌入保留预测信息同时抑制无关变异性，同时保持标准归一化的稳定性和兼容性。

Result: 在大型语言模型(LLaMA、GPT-2)和视觉模型(ResNet、ViT)上持续优于BatchNorm、LayerNorm和RMSNorm，互信息分析确认了更优的信息瓶颈行为。

Conclusion: IBNorm在理论和实证上都证明比方差中心方法具有更高的IB值和更紧的泛化边界，能产生更具信息量的表示。

Abstract: Normalization is fundamental to deep learning, but existing approaches such
as BatchNorm, LayerNorm, and RMSNorm are variance-centric by enforcing zero
mean and unit variance, stabilizing training without controlling how
representations capture task-relevant information. We propose IB-Inspired
Normalization (IBNorm), a simple yet powerful family of methods grounded in the
Information Bottleneck principle. IBNorm introduces bounded compression
operations that encourage embeddings to preserve predictive information while
suppressing nuisance variability, yielding more informative representations
while retaining the stability and compatibility of standard normalization.
Theoretically, we prove that IBNorm achieves a higher IB value and tighter
generalization bounds than variance-centric methods. Empirically, IBNorm
consistently outperforms BatchNorm, LayerNorm, and RMSNorm across large-scale
language models (LLaMA, GPT-2) and vision models (ResNet, ViT), with mutual
information analysis confirming superior information bottleneck behavior. Code
will be released publicly.

</details>


### [82] [On the Stability of Neural Networks in Deep Learning](https://arxiv.org/abs/2510.25282)
*Blaise Delattre*

Main category: cs.LG

TL;DR: 该论文通过敏感性分析的统一视角，研究神经网络对输入和参数扰动的响应，提出结合Lipschitz网络、随机平滑和曲率正则化的框架来提升模型稳定性、鲁棒性和训练效果。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型存在不稳定性和脆弱性问题：输入微小变化可能严重影响预测结果，优化过程受限于尖锐的损失函数景观。需要系统性解决这些敏感性相关问题。

Method: 采用Lipschitz网络约束输入敏感性；引入基于损失函数曲率的正则化技术；探索随机平滑方法增强决策边界鲁棒性；结合这三种视角构建统一框架。

Result: 开发了高效谱范数计算、新型Lipschitz约束层和改进的认证程序等实用方法，在理论分析和实际应用层面均有贡献。

Conclusion: 通过统一整合Lipschitz连续性、随机平滑和曲率正则化，有效解决了深度学习中的稳定性基本挑战，为构建更鲁棒的神经网络提供了系统性解决方案。

Abstract: Deep learning has achieved remarkable success across a wide range of tasks,
but its models often suffer from instability and vulnerability: small changes
to the input may drastically affect predictions, while optimization can be
hindered by sharp loss landscapes. This thesis addresses these issues through
the unifying perspective of sensitivity analysis, which examines how neural
networks respond to perturbations at both the input and parameter levels.
  We study Lipschitz networks as a principled way to constrain sensitivity to
input perturbations, thereby improving generalization, adversarial robustness,
and training stability. To complement this architectural approach, we introduce
regularization techniques based on the curvature of the loss function,
promoting smoother optimization landscapes and reducing sensitivity to
parameter variations. Randomized smoothing is also explored as a probabilistic
method for enhancing robustness at decision boundaries.
  By combining these perspectives, we develop a unified framework where
Lipschitz continuity, randomized smoothing, and curvature regularization
interact to address fundamental challenges in stability. The thesis contributes
both theoretical analysis and practical methodologies, including efficient
spectral norm computation, novel Lipschitz-constrained layers, and improved
certification procedures.

</details>


### [83] [Hierarchical Physics-Embedded Learning for Spatiotemporal Dynamical Systems](https://arxiv.org/abs/2510.25306)
*Xizhe Wang,Xiaobin Song,Qingshan Jia,Hongbo Zhao,Benben Jiang*

Main category: cs.LG

TL;DR: 提出了一种分层物理嵌入学习框架，用于从稀疏噪声数据中进行时空预测和物理定律反演发现。该框架采用两级架构，第一级学习PDE的基本符号组件，第二级学习其控制组合，实现先验知识的结构化集成。


<details>
  <summary>Details</summary>
Motivation: 建模复杂时空动力学，特别是远离平衡态系统，是科学中的重大挑战。传统方法存在局限性：纯数据驱动模型物理不一致且数据密集，现有物理信息方法缺乏表示复杂算子的结构能力。

Method: 使用分层物理嵌入学习框架，基于自适应傅里叶神经算子构建，采用两级架构分别学习PDE的基本符号组件和控制组合，通过计算图直接嵌入已知物理定律。

Result: 该框架能够保证物理一致性，提高数据效率，有效捕捉动力学系统的非局部依赖和高阶算子特征，实现底层控制方程的可解释发现。

Conclusion: 分层物理嵌入学习框架在时空预测和物理定律发现方面取得根本性进展，通过结构化分解和先验知识集成，解决了复杂系统建模的关键挑战。

Abstract: Modeling complex spatiotemporal dynamics, particularly in
far-from-equilibrium systems, remains a grand challenge in science. The
governing partial differential equations (PDEs) for these systems are often
intractable to derive from first principles, due to their inherent complexity,
characterized by high-order derivatives and strong nonlinearities, coupled with
incomplete physical knowledge. This has spurred the development of data-driven
methods, yet these approaches face limitations: Purely data-driven models are
often physically inconsistent and data-intensive, while existing
physics-informed methods lack the structural capacity to represent complex
operators or systematically integrate partial physical knowledge. Here, we
propose a hierarchical physics-embedded learning framework that fundamentally
advances both the forward spatiotemporal prediction and inverse discovery of
physical laws from sparse and noisy data. The key innovation is a two-level
architecture that mirrors the process of scientific discovery: the first level
learns fundamental symbolic components of a PDE, while the second learns their
governing combinations. This hierarchical decomposition not only reduces
learning complexity but, more importantly, enables a structural integration of
prior knowledge. Known physical laws are directly embedded into the models
computational graph, guaranteeing physical consistency and improving data
efficiency. By building the framework upon adaptive Fourier Neural Operators,
we can effectively capture the non-local dependencies and high-order operators
characteristic of dynamical systems. Additionally, by structurally decoupling
known and unknown terms, the framework further enables interpretable discovery
of underlying governing equations through symbolic regression, without
presupposing functional forms.

</details>


### [84] [Dense and Diverse Goal Coverage in Multi Goal Reinforcement Learning](https://arxiv.org/abs/2510.25311)
*Sagalpreet Singh,Rishi Saket,Aravindan Raghuveer*

Main category: cs.LG

TL;DR: 提出了一种多目标强化学习算法，在最大化期望回报的同时，确保策略在目标状态上具有分散的边际状态分布。


<details>
  <summary>Details</summary>
Motivation: 传统RL算法主要关注最大化期望回报，可能导致策略过度利用少数奖励源。但在许多自然场景中，需要在保持高回报的同时均匀访问多个目标状态，而现有方法无法有效实现这一目标。

Method: 提出基于策略混合的算法，通过优化自定义RL奖励函数，结合离线RL方法更新策略混合，确保边际状态分布在目标状态上分散。

Result: 算法在合成MDP和标准RL环境中得到验证，能够有效平衡期望回报和状态分布分散性。

Conclusion: 该算法成功解决了多目标强化学习问题，在保持高回报的同时实现了目标状态的均匀访问，具有理论性能保证。

Abstract: Reinforcement Learning algorithms are primarily focused on learning a policy
that maximizes expected return. As a result, the learned policy can exploit one
or few reward sources. However, in many natural situations, it is desirable to
learn a policy that induces a dispersed marginal state distribution over
rewarding states, while maximizing the expected return which is typically tied
to reaching a goal state. This aspect remains relatively unexplored. Existing
techniques based on entropy regularization and intrinsic rewards use
stochasticity for encouraging exploration to find an optimal policy which may
not necessarily lead to dispersed marginal state distribution over rewarding
states. Other RL algorithms which match a target distribution assume the latter
to be available apriori. This may be infeasible in large scale systems where
enumeration of all states is not possible and a state is determined to be a
goal state only upon reaching it. We formalize the problem of maximizing the
expected return while uniformly visiting the goal states as Multi Goal RL in
which an oracle classifier over the state space determines the goal states. We
propose a novel algorithm that learns a high-return policy mixture with
marginal state distribution dispersed over the set of goal states. Our
algorithm is based on optimizing a custom RL reward which is computed - based
on the current policy mixture - at each iteration for a set of sampled
trajectories. The latter are used via an offline RL algorithm to update the
policy mixture. We prove performance guarantees for our algorithm, showing
efficient convergence bounds for optimizing a natural objective which captures
the expected return as well as the dispersion of the marginal state
distribution over the goal states. We design and perform experiments on
synthetic MDPs and standard RL environments to evaluate the effectiveness of
our algorithm.

</details>


### [85] [CDFlow: Building Invertible Layers with Circulant and Diagonal Matrices](https://arxiv.org/abs/2510.25323)
*Xuchen Feng,Siyu Liao*

Main category: cs.LG

TL;DR: 提出基于循环矩阵和对角矩阵乘积的新型可逆线性层，显著降低参数复杂度和计算复杂度，构建了CDFlow模型在图像密度估计中表现优异


<details>
  <summary>Details</summary>
Motivation: 设计既增强表达能力又保持雅可比行列式和逆矩阵高效计算的可逆线性层，解决归一化流模型中的关键挑战

Method: 使用m个对角矩阵和m-1个循环矩阵的乘积来分解线性变换，利用快速傅里叶变换实现高效计算

Result: 参数复杂度从O(n²)降至O(mn)，矩阵求逆时间复杂度从O(n³)降至O(mn log n)，对数行列式计算从O(n³)降至O(mn)，在自然图像数据集上实现强大的密度估计

Conclusion: CDFlow模型在保持表达力的同时显著加速了归一化流的关键操作，为可扩展生成建模提供了实用优势

Abstract: Normalizing flows are deep generative models that enable efficient likelihood
estimation and sampling through invertible transformations. A key challenge is
to design linear layers that enhance expressiveness while maintaining efficient
computation of the Jacobian determinant and inverse. We introduce a novel
invertible linear layer based on the product of circulant and diagonal
matrices. This decomposition reduces parameter complexity from
$\mathcal{O}(n^2)$ to $\mathcal{O}(mn)$ using $m$ diagonal matrices and $m-1$
circulant matrices while still approximating general linear transformations. By
leveraging the Fast Fourier Transform, our approach reduces the time complexity
of matrix inversion from $\mathcal{O}(n^3)$ to $\mathcal{O}(mn\log n)$ and that
of computing the log-determinant from $\mathcal{O}(n^3)$ to $\mathcal{O}(mn)$,
where $n$ is the input dimension. We build upon this layer to develop
Circulant-Diagonal Flow (CDFlow), which achieves strong density estimation on
natural image datasets and effectively models data with inherent periodic
structure. Furthermore, CDFlow significantly accelerates key operations in
normalizing flows, providing practical benefits for scalable generative
modeling.

</details>


### [86] [Beyond Leakage and Complexity: Towards Realistic and Efficient Information Cascade Prediction](https://arxiv.org/abs/2510.25348)
*Jie Peng,Rui Wang,Qiang Wang,Zhewei Wei,Bin Tong,Guan Wang*

Main category: cs.LG

TL;DR: 该论文提出了解决信息级联流行度预测中三个关键问题的方法：时间泄漏、数据特征不足和计算效率低。通过时间有序分割策略、Taoke电商数据集和CasTemp轻量级框架，实现了无泄漏评估下的最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前信息级联流行度预测存在三个主要问题：评估中存在时间泄漏导致模型能访问未来信息；数据集缺乏下游转化信号；图计算方法训练效率低下。这些问题限制了实际应用效果。

Method: 1) 提出时间有序分割策略，按时间窗口划分数据避免未来信息泄漏；2) 构建Taoke电商级联数据集，包含丰富的推广者/产品属性和真实购买转化数据；3) 开发CasTemp轻量级框架，通过时间游走、Jaccard邻居选择和GRU编码建模级联动态。

Result: 在无泄漏评估下，CasTemp在四个数据集上达到最先进性能，训练速度提升数个数量级。特别擅长预测第二阶段流行度转化，这对实际应用至关重要。

Conclusion: 通过系统解决任务设置、数据集构建和模型设计三个维度的挑战，该研究为信息级联预测提供了更实用和高效的解决方案，特别是在电商转化预测等实际场景中表现出色。

Abstract: Information cascade popularity prediction is a key problem in analyzing
content diffusion in social networks. However, current related works suffer
from three critical limitations: (1) temporal leakage in current
evaluation--random cascade-based splits allow models to access future
information, yielding unrealistic results; (2) feature-poor datasets that lack
downstream conversion signals (e.g., likes, comments, or purchases), which
limits more practical applications; (3) computational inefficiency of complex
graph-based methods that require days of training for marginal gains. We
systematically address these challenges from three perspectives: task setup,
dataset construction, and model design. First, we propose a time-ordered
splitting strategy that chronologically partitions data into consecutive
windows, ensuring models are evaluated on genuine forecasting tasks without
future information leakage. Second, we introduce Taoke, a large-scale
e-commerce cascade dataset featuring rich promoter/product attributes and
ground-truth purchase conversions--capturing the complete diffusion lifecycle
from promotion to monetization. Third, we develop CasTemp, a lightweight
framework that efficiently models cascade dynamics through temporal walks,
Jaccard-based neighbor selection for inter-cascade dependencies, and GRU-based
encoding with time-aware attention. Under leak-free evaluation, CasTemp
achieves state-of-the-art performance across four datasets with
orders-of-magnitude speedup. Notably, it excels at predicting second-stage
popularity conversions--a practical task critical for real-world applications.

</details>


### [87] [Analysis of Semi-Supervised Learning on Hypergraphs](https://arxiv.org/abs/2510.25354)
*Adrien Weihs,Andrea Bertozzi,Matthew Thorpe*

Main category: cs.LG

TL;DR: 该论文提出了高阶超图学习（HOHL）方法，通过骨架图的拉普拉斯算子幂进行正则化，实现多尺度平滑性，并在标准基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 超图为建模高阶交互提供了自然框架，但在半监督学习中的理论基础仍然有限，需要研究超图学习的渐近一致性和正则化方法。

Method: 提出高阶超图学习（HOHL），通过骨架图的拉普拉斯算子幂进行正则化，实现多尺度平滑性，并证明其收敛到高阶Sobolev半范数。

Result: HOHL在标准基准测试中表现优异，理论分析表明超图学习收敛到加权p-拉普拉斯方程。

Conclusion: HOHL方法有效解决了超图学习的正则化问题，提供了理论保证并在实践中表现出色。

Abstract: Hypergraphs provide a natural framework for modeling higher-order
interactions, yet their theoretical underpinnings in semi-supervised learning
remain limited. We provide an asymptotic consistency analysis of variational
learning on random geometric hypergraphs, precisely characterizing the
conditions ensuring the well-posedness of hypergraph learning as well as
showing convergence to a weighted $p$-Laplacian equation. Motivated by this, we
propose Higher-Order Hypergraph Learning (HOHL), which regularizes via powers
of Laplacians from skeleton graphs for multiscale smoothness. HOHL converges to
a higher-order Sobolev seminorm. Empirically, it performs strongly on standard
baselines.

</details>


### [88] [Parameter Averaging in Link Prediction](https://arxiv.org/abs/2510.25361)
*Rupesh Sapkota,Caglar Demir,Arnab Sharma,Axel-Cyrille Ngonga Ngomo*

Main category: cs.LG

TL;DR: 该论文提出在知识图谱嵌入模型中使用模型合并（特别是加权平均）作为集成学习的替代方法，通过维护训练过程中的参数运行平均值来提升链接预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统集成学习方法需要训练多个模型，导致计算开销大、延迟高和内存负担重。模型合并方法提供了一种无需训练多个模型的替代方案。

Method: 提出两种加权平均方法：1）从训练周期开始维护模型参数的运行平均值；2）仅在验证集上泛化性能提升时选择性更新集成模型参数。

Result: 在链接预测任务中，提出的加权平均方法相比最先进的基准集成方法持续提升了性能，并在字面增强知识图谱嵌入模型和多跳查询回答任务中也表现良好。

Conclusion: 加权平均方法在各种评估设置中都能一致地提升性能，为知识图谱嵌入模型的集成学习提供了更高效的替代方案。

Abstract: Ensemble methods are widely employed to improve generalization in machine
learning. This has also prompted the adoption of ensemble learning for the
knowledge graph embedding (KGE) models in performing link prediction. Typical
approaches to this end train multiple models as part of the ensemble, and the
diverse predictions are then averaged. However, this approach has some
significant drawbacks. For instance, the computational overhead of training
multiple models increases latency and memory overhead. In contrast, model
merging approaches offer a promising alternative that does not require training
multiple models. In this work, we introduce model merging, specifically
weighted averaging, in KGE models. Herein, a running average of model
parameters from a training epoch onward is maintained and used for predictions.
To address this, we additionally propose an approach that selectively updates
the running average of the ensemble model parameters only when the
generalization performance improves on a validation dataset. We evaluate these
two different weighted averaging approaches on link prediction tasks, comparing
the state-of-the-art benchmark ensemble approach. Additionally, we evaluate the
weighted averaging approach considering literal-augmented KGE models and
multi-hop query answering tasks as well. The results demonstrate that the
proposed weighted averaging approach consistently improves performance across
diverse evaluation settings.

</details>


### [89] [A Convexity-dependent Two-Phase Training Algorithm for Deep Neural Networks](https://arxiv.org/abs/2510.25366)
*Tomas Hrycej,Bernhard Bermeitinger,Massimo Pavone,Götz-Henrik Wiegand,Siegfried Handschuh*

Main category: cs.LG

TL;DR: 提出一种基于损失函数从初始非凸性向最优解附近凸性转换假设的两阶段优化算法，通过检测转换点分别使用Adam和共轭梯度法，显著提升收敛性和精度。


<details>
  <summary>Details</summary>
Motivation: 现实任务中的损失函数通常具有从初始非凸区域向最优解附近凸区域转换的特性，这一特性可被利用来设计更高效的优化算法。

Method: 提出两阶段优化框架：通过观测梯度范数与损失的关系检测凸性转换点，在非凸区域使用Adam算法，在凸区域使用共轭梯度法。

Result: 计算实验证实该简单凸性结构足够频繁出现，可被实际利用来显著改善收敛性和准确性。

Conclusion: 利用损失函数从非凸到凸的转换特性设计的混合优化算法能有效提升机器学习模型的优化性能。

Abstract: The key task of machine learning is to minimize the loss function that
measures the model fit to the training data. The numerical methods to do this
efficiently depend on the properties of the loss function. The most decisive
among these properties is the convexity or non-convexity of the loss function.
The fact that the loss function can have, and frequently has, non-convex
regions has led to a widespread commitment to non-convex methods such as Adam.
However, a local minimum implies that, in some environment around it, the
function is convex. In this environment, second-order minimizing methods such
as the Conjugate Gradient (CG) give a guaranteed superlinear convergence. We
propose a novel framework grounded in the hypothesis that loss functions in
real-world tasks swap from initial non-convexity to convexity towards the
optimum. This is a property we leverage to design an innovative two-phase
optimization algorithm. The presented algorithm detects the swap point by
observing the gradient norm dependence on the loss. In these regions,
non-convex (Adam) and convex (CG) algorithms are used, respectively. Computing
experiments confirm the hypothesis that this simple convexity structure is
frequent enough to be practically exploited to substantially improve
convergence and accuracy.

</details>


### [90] [Position: Biology is the Challenge Physics-Informed ML Needs to Evolve](https://arxiv.org/abs/2510.25368)
*Julien Martinelli*

Main category: cs.LG

TL;DR: 提出生物学信息机器学习(BIML)作为物理信息机器学习(PIML)的扩展，适应生物学的独特挑战，包括不确定性先验知识、异质数据、部分可观测性和复杂网络。


<details>
  <summary>Details</summary>
Motivation: 将PIML成功应用于物理领域的经验推广到生物学，但生物学建模面临多面性先验知识、异质噪声数据、部分可观测性和高维网络等独特挑战。

Method: 提出BIML框架，基于四个基础支柱：不确定性量化、情境化、约束潜在结构推断和可扩展性，利用基础模型和大型语言模型作为关键赋能工具。

Result: BIML将PIML方法重新工具化，在更软的概率先验知识下运作，为生物学建模提供结构化基础。

Conclusion: BIML是PIML的演化方向，需要建立相应生态系统，将PIML启发式创新导向具有高科学和社会相关性的挑战。

Abstract: Physics-Informed Machine Learning (PIML) has successfully integrated
mechanistic understanding into machine learning, particularly in domains
governed by well-known physical laws. This success has motivated efforts to
apply PIML to biology, a field rich in dynamical systems but shaped by
different constraints. Biological modeling, however, presents unique
challenges: multi-faceted and uncertain prior knowledge, heterogeneous and
noisy data, partial observability, and complex, high-dimensional networks. In
this position paper, we argue that these challenges should not be seen as
obstacles to PIML, but as catalysts for its evolution. We propose
Biology-Informed Machine Learning (BIML): a principled extension of PIML that
retains its structural grounding while adapting to the practical realities of
biology. Rather than replacing PIML, BIML retools its methods to operate under
softer, probabilistic forms of prior knowledge. We outline four foundational
pillars as a roadmap for this transition: uncertainty quantification,
contextualization, constrained latent structure inference, and scalability.
Foundation Models and Large Language Models will be key enablers, bridging
human expertise with computational modeling. We conclude with concrete
recommendations to build the BIML ecosystem and channel PIML-inspired
innovation toward challenges of high scientific and societal relevance.

</details>


### [91] [A Deep Learning Framework for Multi-Operator Learning: Architectures and Approximation Theory](https://arxiv.org/abs/2510.25379)
*Adrien Weihs,Jingmin Sun,Zecheng Zhang,Hayden Schaeffer*

Main category: cs.LG

TL;DR: 该论文研究了学习函数空间之间映射（算子）的集合问题，提出了两种新架构MNO和MONet用于多算子学习，并建立了通用逼近理论。同时为学习多个独立算子提供了平衡架构复杂度的框架，在参数PDE基准测试中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 科学应用需要近似函数空间之间的映射（算子），而传统机器学习主要关注有限维空间之间的映射学习。需要开发能够有效学习算子集合的理论框架和架构。

Method: 提出两种新架构MNO和MONet用于多算子学习，建立了连续、可积和Lipschitz算子三种设置下的通用逼近理论。对于学习多个独立算子，开发了平衡子网络架构复杂度的框架。

Result: 理论分析表明所提架构具有通用逼近能力，并推导了网络规模与逼近精度的显式缩放规律。在参数PDE基准测试中验证了架构的强大表达能力和效率。

Conclusion: 这项工作为跨多个算子的可扩展神经算子学习建立了统一的理论和实践基础，为科学计算中的算子学习问题提供了有效的解决方案。

Abstract: While many problems in machine learning focus on learning mappings between
finite-dimensional spaces, scientific applications require approximating
mappings between function spaces, i.e., operators. We study the problem of
learning collections of operators and provide both theoretical and empirical
advances. We distinguish between two regimes: (i) multiple operator learning,
where a single network represents a continuum of operators parameterized by a
parametric function, and (ii) learning several distinct single operators, where
each operator is learned independently. For the multiple operator case, we
introduce two new architectures, $\mathrm{MNO}$ and $\mathrm{MONet}$, and
establish universal approximation results in three settings: continuous,
integrable, or Lipschitz operators. For the latter, we further derive explicit
scaling laws that quantify how the network size must grow to achieve a target
approximation accuracy. For learning several single operators, we develop a
framework for balancing architectural complexity across subnetworks and show
how approximation order determines computational efficiency. Empirical
experiments on parametric PDE benchmarks confirm the strong expressive power
and efficiency of the proposed architectures. Overall, this work establishes a
unified theoretical and practical foundation for scalable neural operator
learning across multiple operators.

</details>


### [92] [GPTOpt: Towards Efficient LLM-Based Black-Box Optimization](https://arxiv.org/abs/2510.25404)
*Jamison Meindl,Yunsheng Tian,Tony Cui,Veronika Thost,Zhang-Wei Hong,Jie Chen,Wojciech Matusik,Mina Konaković Luković*

Main category: cs.LG

TL;DR: GPTOpt是一种基于大语言模型的优化方法，通过在多样化的贝叶斯优化参数化合成数据集上微调，使LLM具备连续黑盒优化能力，无需参数调优即可超越传统优化器。


<details>
  <summary>Details</summary>
Motivation: 解决昂贵、无导数黑盒函数全局优化需要极高样本效率的问题。传统贝叶斯优化需要针对每个应用领域进行参数调优，而现有LLM在连续黑盒优化任务上能力有限。

Method: 在来自多样化BO参数化的合成数据集上微调大语言模型，利用LLM预训练实现跨优化任务的泛化能力。

Result: 在各种黑盒优化基准测试中，GPTOpt超越了传统优化器。

Conclusion: LLM具备高级数值推理能力，GPTOpt为无需参数调优的全局优化提供了一个灵活框架。

Abstract: Global optimization of expensive, derivative-free black-box functions demands
extreme sample efficiency. Classical methods such as Bayesian Optimization (BO)
can be effective, but they often require careful parameter tuning to each
application domain. At the same time, Large Language Models (LLMs) have shown
broad capabilities, yet state-of-the-art models remain limited in solving
continuous black-box optimization tasks. We introduce GPTOpt, an LLM-based
optimization method that equips LLMs with continuous black-box optimization
capabilities. By fine-tuning large language models on extensive synthetic
datasets derived from diverse BO parameterizations, GPTOpt leverages LLM
pre-training to generalize across optimization tasks. On a variety of black-box
optimization benchmarks, GPTOpt surpasses traditional optimizers, highlighting
the capacity of LLMs for advanced numerical reasoning and introducing a
flexible framework for global optimization without parameter tuning.

</details>


### [93] [Scalable Utility-Aware Multiclass Calibration](https://arxiv.org/abs/2510.25458)
*Mahmoud Hegazy,Michael I. Jordan,Aymeric Dieuleveut*

Main category: cs.LG

TL;DR: 提出了效用校准框架，用于评估多分类器的校准误差，该框架通过特定效用函数来反映最终用户的目标或决策标准。


<details>
  <summary>Details</summary>
Motivation: 现有的多分类校准评估方法要么关注预测的特定方面（如top-class置信度、类级校准），要么使用计算复杂的变分公式，需要一种可扩展的评估方法。

Method: 提出了效用校准框架，通过特定效用函数来衡量校准误差，该框架可以统一和重新解释现有的校准指标，并支持更丰富的下游效用类。

Result: 该框架能够统一和重新解释现有的校准指标，特别是允许更鲁棒的top-class和类级校准指标，并扩展到评估更丰富的下游效用类。

Conclusion: 效用校准提供了一个通用且可扩展的框架，用于评估多分类器的校准性能，能够更好地反映最终用户的实际需求。

Abstract: Ensuring that classifiers are well-calibrated, i.e., their predictions align
with observed frequencies, is a minimal and fundamental requirement for
classifiers to be viewed as trustworthy. Existing methods for assessing
multiclass calibration often focus on specific aspects associated with
prediction (e.g., top-class confidence, class-wise calibration) or utilize
computationally challenging variational formulations. In this work, we study
scalable \emph{evaluation} of multiclass calibration. To this end, we propose
utility calibration, a general framework that measures the calibration error
relative to a specific utility function that encapsulates the goals or decision
criteria relevant to the end user. We demonstrate how this framework can unify
and re-interpret several existing calibration metrics, particularly allowing
for more robust versions of the top-class and class-wise calibration metrics,
and, going beyond such binarized approaches, toward assessing calibration for
richer classes of downstream utilities.

</details>


### [94] [Gradient-Weight Alignment as a Train-Time Proxy for Generalization in Classification Tasks](https://arxiv.org/abs/2510.25480)
*Florian A. Hölzl,Daniel Rueckert,Georgios Kaissis*

Main category: cs.LG

TL;DR: 提出了梯度权重对齐（GWA）指标，通过量化每个样本梯度与模型权重之间的一致性来跟踪泛化性能，无需验证集即可预测最优早停点、比较模型和识别有影响力的训练样本。


<details>
  <summary>Details</summary>
Motivation: 在深度学习领域，需要稳健的验证指标来检测过拟合、监控训练动态。研究者希望找到一种能够从训练数据本身获取、既能跟踪泛化性能又能归因到单个训练样本的指标。

Method: 引入梯度权重对齐（GWA）方法，计算每个样本梯度与模型权重之间的一致性。有效学习对应一致的对齐，而错位则表示泛化性能下降。该方法可在训练过程中高效计算。

Result: 大量实验表明，GWA能准确预测最优早停点，支持基于原则的模型比较，并能识别有影响力的训练样本，提供了一种无需验证集的模型分析方法。

Conclusion: GWA提供了一种直接从训练数据中分析模型的验证集无关方法，能够有效监控训练动态并评估泛化性能。

Abstract: Robust validation metrics remain essential in contemporary deep learning, not
only to detect overfitting and poor generalization, but also to monitor
training dynamics. In the supervised classification setting, we investigate
whether interactions between training data and model weights can yield such a
metric that both tracks generalization during training and attributes
performance to individual training samples. We introduce Gradient-Weight
Alignment (GWA), quantifying the coherence between per-sample gradients and
model weights. We show that effective learning corresponds to coherent
alignment, while misalignment indicates deteriorating generalization. GWA is
efficiently computable during training and reflects both sample-specific
contributions and dataset-wide learning dynamics. Extensive experiments show
that GWA accurately predicts optimal early stopping, enables principled model
comparisons, and identifies influential training samples, providing a
validation-set-free approach for model analysis directly from the training
data.

</details>


### [95] [Right for the Right Reasons: Avoiding Reasoning Shortcuts via Prototypical Neurosymbolic AI](https://arxiv.org/abs/2510.25497)
*Luca Andolfi,Eleonora Giunchiglia*

Main category: cs.LG

TL;DR: 本文提出原型神经符号架构来解决神经符号AI中的推理捷径问题，通过在极低数据量下学习正确的概念而非利用伪相关来满足符号约束。


<details>
  <summary>Details</summary>
Motivation: 神经符号AI模型容易学习到推理捷径，即利用伪相关性来满足符号约束而非学习正确的概念，这影响了模型的可靠性和安全性。

Method: 利用原型学习理论，训练模型在考虑输入与少量标记数据点相似度的同时满足背景知识，从而有效避免推理捷径。

Result: 在rsbench基准测试中，无论是合成任务（MNIST-EvenOdd和Kand-Logic）还是真实高风险任务（BDD-OIA），都显著改善了学习正确概念的能力。

Conclusion: 原型基础化是神经符号学习的一种有效且标注效率高的策略，为实现安全可靠的神经符号AI铺平了道路。

Abstract: Neurosymbolic AI is growing in popularity thanks to its ability to combine
neural perception and symbolic reasoning in end-to-end trainable models.
However, recent findings reveal these are prone to shortcut reasoning, i.e., to
learning unindented concepts--or neural predicates--which exploit spurious
correlations to satisfy the symbolic constraints. In this paper, we address
reasoning shortcuts at their root cause and we introduce prototypical
neurosymbolic architectures. These models are able to satisfy the symbolic
constraints (be right) because they have learnt the correct basic concepts (for
the right reasons) and not because of spurious correlations, even in extremely
low data regimes. Leveraging the theory of prototypical learning, we
demonstrate that we can effectively avoid reasoning shortcuts by training the
models to satisfy the background knowledge while taking into account the
similarity of the input with respect to the handful of labelled datapoints. We
extensively validate our approach on the recently proposed rsbench benchmark
suite in a variety of settings and tasks with very scarce supervision: we show
significant improvements in learning the right concepts both in synthetic tasks
(MNIST-EvenOdd and Kand-Logic) and real-world, high-stake ones (BDD-OIA). Our
findings pave the way to prototype grounding as an effective,
annotation-efficient strategy for safe and reliable neurosymbolic learning.

</details>


### [96] [TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting](https://arxiv.org/abs/2510.25502)
*Vladyslav Moroshan,Julien Siems,Arber Zela,Timur Carstensen,Frank Hutter*

Main category: cs.LG

TL;DR: TempoPFN是一个基于线性RNN的单变量时间序列基础模型，仅使用合成数据预训练，在零样本时间序列预测中实现了高效的长时程预测和顶级性能。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本时间序列预测基础模型在高效长时程预测和可复现性方面面临挑战，且仅基于合成数据的方法在挑战性基准测试中表现不佳。

Method: 使用带有状态编织的GatedDeltaProduct架构的线性RNN，通过完全并行化训练跨序列长度，无需窗口化或汇总技术，同时保持稳健的时间状态跟踪。构建了统一多种生成器（包括随机微分方程、高斯过程和音频合成）的综合合成数据流水线。

Result: 在Gift-Eval基准测试的零样本评估中，TempoPFN实现了顶级竞争性能，超越了所有现有的仅基于合成数据的方法，并超过了大多数在真实世界数据上训练的模型，同时通过完全并行化训练和推理比现有基线更高效。

Conclusion: TempoPFN为时间序列预测提供了一个可复现的基础，开源了完整的数据生成流水线和训练代码，为未来研究奠定了基础。

Abstract: Foundation models for zero-shot time series forecasting face challenges in
efficient long-horizon prediction and reproducibility, with existing
synthetic-only approaches underperforming on challenging benchmarks. This paper
presents TempoPFN, a univariate time series foundation model based on linear
Recurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The
model uses a GatedDeltaProduct architecture with state-weaving for fully
parallelizable training across sequence lengths, eliminating the need for
windowing or summarization techniques while maintaining robust temporal
state-tracking. Our comprehensive synthetic data pipeline unifies diverse
generators, including stochastic differential equations, Gaussian processes,
and audio synthesis, with novel augmentations. In zero-shot evaluations on the
Gift-Eval benchmark, TempoPFN achieves top-tier competitive performance,
outperforming all existing synthetic-only approaches and surpassing the vast
majority of models trained on real-world data, while being more efficient than
existing baselines by leveraging fully parallelizable training and inference.
We open-source our complete data generation pipeline and training code,
providing a reproducible foundation for future research.

</details>


### [97] [Support Vector Machine-Based Burnout Risk Prediction with an Interactive Interface for Organizational Use](https://arxiv.org/abs/2510.25509)
*Bruno W. G. Teodosio,Mário J. O. T. Lira,Pedro H. M. Araújo,Lucas R. C. Farias*

Main category: cs.LG

TL;DR: 该研究使用机器学习方法预测员工倦怠风险，在三种算法中SVM表现最佳，并开发了交互界面供非技术人员使用。


<details>
  <summary>Details</summary>
Motivation: 员工倦怠对个人福祉和组织绩效有显著影响，需要早期检测和预防。

Method: 使用HackerEarth员工倦怠挑战数据集，评估KNN、随机森林和SVM三种监督学习算法，通过30折交叉验证和R2系数评估性能。

Result: SVM模型表现最佳（R2=0.84），统计上显著优于其他两种模型，并开发了基于Streamlit的交互预测界面。

Conclusion: 机器学习方法在员工倦怠早期检测方面具有潜力，可支持组织制定数据驱动的心理健康策略。

Abstract: Burnout is a psychological syndrome marked by emotional exhaustion,
depersonalization, and reduced personal accomplishment, with a significant
impact on individual well-being and organizational performance. This study
proposes a machine learning approach to predict burnout risk using the
HackerEarth Employee Burnout Challenge dataset. Three supervised algorithms
were evaluated: nearest neighbors (KNN), random forest, and support vector
machine (SVM), with model performance evaluated through 30-fold
cross-validation using the determination coefficient (R2). Among the models
tested, SVM achieved the highest predictive performance (R2 = 0.84) and was
statistically superior to KNN and Random Forest based on paired $t$-tests. To
ensure practical applicability, an interactive interface was developed using
Streamlit, allowing non-technical users to input data and receive burnout risk
predictions. The results highlight the potential of machine learning to support
early detection of burnout and promote data-driven mental health strategies in
organizational settings.

</details>


### [98] [FaCT: Faithful Concept Traces for Explaining Neural Network Decisions](https://arxiv.org/abs/2510.25512)
*Amin Parchami-Araghi,Sukrut Rao,Jonas Fischer,Bernt Schiele*

Main category: cs.LG

TL;DR: 提出了一种具有模型内在机制概念解释的新模型，强调概念解释的忠实性，概念跨类别共享，可以从任何层追踪其对logit的贡献和输入可视化，并利用基础模型提出了概念一致性度量C²-Score。


<details>
  <summary>Details</summary>
Motivation: 现有后验概念解释方法往往不忠实于模型，且对模型学习的概念做出限制性假设（如类别特异性、小空间范围或与人类期望对齐）。本工作强调概念解释的忠实性。

Method: 提出具有模型内在机制概念解释的新模型，概念跨类别共享，可以从任何层追踪其对logit的贡献和输入可视化，并利用基础模型提出概念一致性度量C²-Score。

Result: 相比先前工作，提出的概念在数量上更一致，用户认为这些概念更具可解释性，同时在ImageNet上保持有竞争力的性能。

Conclusion: 提出的方法能够提供更忠实、一致和可解释的概念解释，同时保持模型性能，为深度网络的概念级理解提供了新途径。

Abstract: Deep networks have shown remarkable performance across a wide range of tasks,
yet getting a global concept-level understanding of how they function remains a
key challenge. Many post-hoc concept-based approaches have been introduced to
understand their workings, yet they are not always faithful to the model.
Further, they make restrictive assumptions on the concepts a model learns, such
as class-specificity, small spatial extent, or alignment to human expectations.
In this work, we put emphasis on the faithfulness of such concept-based
explanations and propose a new model with model-inherent mechanistic
concept-explanations. Our concepts are shared across classes and, from any
layer, their contribution to the logit and their input-visualization can be
faithfully traced. We also leverage foundation models to propose a new
concept-consistency metric, C$^2$-Score, that can be used to evaluate
concept-based methods. We show that, compared to prior work, our concepts are
quantitatively more consistent and users find our concepts to be more
interpretable, all while retaining competitive ImageNet performance.

</details>


### [99] [Transformers Provably Learn Directed Acyclic Graphs via Kernel-Guided Mutual Information](https://arxiv.org/abs/2510.25542)
*Yuan Cheng,Yu Huang,Zhe Xiong,Yingbin Liang,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 本文提出了一种基于核引导互信息(KG-MI)的新目标函数，结合多头注意力机制，能够从K-父DAG生成的数据中可证明地恢复底层图结构。


<details>
  <summary>Details</summary>
Motivation: 现有基于transformer的模型在图结构学习方面仅限于树状图的理论保证，难以扩展到具有多个父节点的通用有向无环图(DAG)。

Method: 引入基于f-散度的核引导互信息(KG-MI)度量，结合多头注意力框架，每个头关联不同的边际转移核来建模不同的父子依赖关系。

Result: 理论证明单层多头transformer通过梯度上升能在多项式时间内收敛到全局最优，且当f-散度为KL散度时，学习到的注意力分数能准确反映真实邻接矩阵。

Conclusion: 该方法为从复杂数据中可证明地恢复底层图结构提供了理论保证，实验验证了理论结果。

Abstract: Uncovering hidden graph structures underlying real-world data is a critical
challenge with broad applications across scientific domains. Recently,
transformer-based models leveraging the attention mechanism have demonstrated
strong empirical success in capturing complex dependencies within graphs.
However, the theoretical understanding of their training dynamics has been
limited to tree-like graphs, where each node depends on a single parent.
Extending provable guarantees to more general directed acyclic graphs (DAGs) --
which involve multiple parents per node -- remains challenging, primarily due
to the difficulty in designing training objectives that enable different
attention heads to separately learn multiple different parent relationships.
  In this work, we address this problem by introducing a novel
information-theoretic metric: the kernel-guided mutual information (KG-MI),
based on the $f$-divergence. Our objective combines KG-MI with a multi-head
attention framework, where each head is associated with a distinct marginal
transition kernel to model diverse parent-child dependencies effectively. We
prove that, given sequences generated by a $K$-parent DAG, training a
single-layer, multi-head transformer via gradient ascent converges to the
global optimum in polynomial time. Furthermore, we characterize the attention
score patterns at convergence. In addition, when particularizing the
$f$-divergence to the KL divergence, the learned attention scores accurately
reflect the ground-truth adjacency matrix, thereby provably recovering the
underlying graph structure. Experimental results validate our theoretical
findings.

</details>


### [100] [Leveraging an Atmospheric Foundational Model for Subregional Sea Surface Temperature Forecasting](https://arxiv.org/abs/2510.25563)
*Víctor Medina,Giovanny A. Cuervo-Londoño,Javier Sánchez*

Main category: cs.LG

TL;DR: 将大气预报深度学习模型Aurora迁移到海洋领域，用于预测加那利上升流系统的海表温度，通过微调实现低计算成本的高精度预测。


<details>
  <summary>Details</summary>
Motivation: 传统数值海洋预报模型计算成本高、可扩展性有限，需要探索更高效的预测方法。深度学习模型在气象领域已取得成功，但将其应用于海洋预测仍具挑战性。

Method: 采用分阶段微调策略，使用高分辨率海洋再分析数据对Aurora模型进行微调，结合纬度加权误差指标和超参数优化。

Result: 模型在加那利上升流系统取得了0.119K的低RMSE和约0.997的高异常相关系数，能成功再现大尺度海温结构，但在沿海区域细节捕捉方面存在挑战。

Conclusion: 证明了将不同领域预训练的深度学习模型应用于海洋预测的可行性，为数据驱动的海洋预报开辟了新途径，未来可通过增加变量、提高分辨率和引入物理约束来进一步提升性能。

Abstract: The accurate prediction of oceanographic variables is crucial for
understanding climate change, managing marine resources, and optimizing
maritime activities. Traditional ocean forecasting relies on numerical models;
however, these approaches face limitations in terms of computational cost and
scalability. In this study, we adapt Aurora, a foundational deep learning model
originally designed for atmospheric forecasting, to predict sea surface
temperature (SST) in the Canary Upwelling System. By fine-tuning this model
with high-resolution oceanographic reanalysis data, we demonstrate its ability
to capture complex spatiotemporal patterns while reducing computational
demands. Our methodology involves a staged fine-tuning process, incorporating
latitude-weighted error metrics and optimizing hyperparameters for efficient
learning. The experimental results show that the model achieves a low RMSE of
0.119K, maintaining high anomaly correlation coefficients (ACC $\approx
0.997$). The model successfully reproduces large-scale SST structures but faces
challenges in capturing finer details in coastal regions. This work contributes
to the field of data-driven ocean forecasting by demonstrating the feasibility
of using deep learning models pre-trained in different domains for oceanic
applications. Future improvements include integrating additional oceanographic
variables, increasing spatial resolution, and exploring physics-informed neural
networks to enhance interpretability and understanding. These advancements can
improve climate modeling and ocean prediction accuracy, supporting
decision-making in environmental and economic sectors.

</details>


### [101] [A Framework for Bounding Deterministic Risk with PAC-Bayes: Applications to Majority Votes](https://arxiv.org/abs/2510.25569)
*Benjamin Leblanc,Pascal Germain*

Main category: cs.LG

TL;DR: 提出一个统一框架，从随机PAC-Bayesian保证中提取单个假设的保证，解决了经典PAC-Bayes只能提供随机采样假设期望风险保证的问题。


<details>
  <summary>Details</summary>
Motivation: 经典PAC-Bayes框架只提供随机采样假设的期望风险保证，需要随机预测，这在实际部署单个确定性假设时不可用。

Method: 提出统一框架，包括一般oracle边界、数值边界和多数投票特化方法，从随机PAC-Bayesian保证中提取单个确定性假设的保证。

Result: 经验表明该方法在确定性分类器的泛化边界方面持续优于流行基线（最高可达2倍）。

Conclusion: 该框架成功解决了PAC-Bayes在实际部署中的局限性，为单个确定性假设提供了有效的泛化保证。

Abstract: PAC-Bayes is a popular and efficient framework for obtaining generalization
guarantees in situations involving uncountable hypothesis spaces.
Unfortunately, in its classical formulation, it only provides guarantees on the
expected risk of a randomly sampled hypothesis. This requires stochastic
predictions at test time, making PAC-Bayes unusable in many practical
situations where a single deterministic hypothesis must be deployed. We propose
a unified framework to extract guarantees holding for a single hypothesis from
stochastic PAC-Bayesian guarantees. We present a general oracle bound and
derive from it a numerical bound and a specialization to majority vote. We
empirically show that our approach consistently outperforms popular baselines
(by up to a factor of 2) when it comes to generalization bounds on
deterministic classifiers.

</details>


### [102] [Perturbation Bounds for Low-Rank Inverse Approximations under Noise](https://arxiv.org/abs/2510.25571)
*Phuc Tran,Nisheeth K. Vishnoi*

Main category: cs.LG

TL;DR: 本文系统研究了低秩伪逆在噪声环境下的谱范数鲁棒性，提出了改进的扰动界限，相比传统方法可减少高达√n倍的误差估计。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的矩阵常受到采样、草图化和量化等噪声影响，但低秩逆近似的谱范数鲁棒性尚未得到充分理解。

Method: 在温和的噪声假设下，使用轮廓积分技术分析非全纯函数f(z)=1/z，推导出尖锐的非渐近扰动界限。

Result: 新界限能紧密跟踪真实扰动误差，而基于经典结果的估计往往显著高估误差，改进因子可达√n。

Conclusion: 研究结果为噪声计算环境中低秩逆近似提供了实用的、频谱感知的保证。

Abstract: Low-rank pseudoinverses are widely used to approximate matrix inverses in
scalable machine learning, optimization, and scientific computing. However,
real-world matrices are often observed with noise, arising from sampling,
sketching, and quantization. The spectral-norm robustness of low-rank inverse
approximations remains poorly understood. We systematically study the
spectral-norm error $\| (\tilde{A}^{-1})_p - A_p^{-1} \|$ for an $n\times n$
symmetric matrix $A$, where $A_p^{-1}$ denotes the best rank-\(p\)
approximation of $A^{-1}$, and $\tilde{A} = A + E$ is a noisy observation.
Under mild assumptions on the noise, we derive sharp non-asymptotic
perturbation bounds that reveal how the error scales with the eigengap,
spectral decay, and noise alignment with low-curvature directions of $A$. Our
analysis introduces a novel application of contour integral techniques to the
\emph{non-entire} function $f(z) = 1/z$, yielding bounds that improve over
naive adaptations of classical full-inverse bounds by up to a factor of
$\sqrt{n}$. Empirically, our bounds closely track the true perturbation error
across a variety of real-world and synthetic matrices, while estimates based on
classical results tend to significantly overpredict. These findings offer
practical, spectrum-aware guarantees for low-rank inverse approximations in
noisy computational environments.

</details>


### [103] [Generalized Sobolev IPM for Graph-Based Measures](https://arxiv.org/abs/2510.25591)
*Tam Le,Truyen Nguyen,Hideitsu Hino,Kenji Fukumizu*

Main category: cs.LG

TL;DR: 本文提出了一种基于Orlicz几何结构的广义Sobolev IPM方法，通过Musielak正则化将复杂优化问题简化为单变量优化，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有Sobolev IPM方法受限于L^p几何结构，无法融入其他结构先验，需要更灵活的几何框架来捕捉复杂关系。

Method: 通过Orlicz几何结构推广Sobolev IPM，建立Orlicz-Sobolev范数与Musielak范数的理论联系，并利用图结构将优化问题简化为单变量优化。

Result: GSI-M方法比流行的Orlicz-Wasserstein方法快几个数量级，在文档分类和拓扑数据分析任务中表现出优越性能。

Conclusion: 提出的广义Sobolev IPM框架不仅包含经典方法作为特例，还能融入多样化几何先验，同时通过理论创新解决了计算效率问题。

Abstract: We study the Sobolev IPM problem for measures supported on a graph metric
space, where critic function is constrained to lie within the unit ball defined
by Sobolev norm. While Le et al. (2025) achieved scalable computation by
relating Sobolev norm to weighted $L^p$-norm, the resulting framework remains
intrinsically bound to $L^p$ geometric structure, limiting its ability to
incorporate alternative structural priors beyond the $L^p$ geometry paradigm.
To overcome this limitation, we propose to generalize Sobolev IPM through the
lens of \emph{Orlicz geometric structure}, which employs convex functions to
capture nuanced geometric relationships, building upon recent advances in
optimal transport theory -- particularly Orlicz-Wasserstein (OW) and
generalized Sobolev transport -- that have proven instrumental in advancing
machine learning methodologies. This generalization encompasses classical
Sobolev IPM as a special case while accommodating diverse geometric priors
beyond traditional $L^p$ structure. It however brings up significant
computational hurdles that compound those already inherent in Sobolev IPM. To
address these challenges, we establish a novel theoretical connection between
Orlicz-Sobolev norm and Musielak norm which facilitates a novel regularization
for the generalized Sobolev IPM (GSI). By further exploiting the underlying
graph structure, we show that GSI with Musielak regularization (GSI-M) reduces
to a simple \emph{univariate optimization} problem, achieving remarkably
computational efficiency. Empirically, GSI-M is several-order faster than the
popular OW in computation, and demonstrates its practical advantages in
comparing probability measures on a given graph for document classification and
several tasks in topological data analysis.

</details>


### [104] [Feedback Alignment Meets Low-Rank Manifolds: A Structured Recipe for Local Learning](https://arxiv.org/abs/2510.25594)
*Arani Roy,Marco P. Apolinario,Shristi Das Biswas,Kaushik Roy*

Main category: cs.LG

TL;DR: 提出了一种基于SVD分解的结构化局部学习框架，在低秩流形上训练深度神经网络，减少可训练参数数量，同时保持与反向传播相当的准确率。


<details>
  <summary>Details</summary>
Motivation: 反向传播需要全局误差传播和全参数化，内存和计算开销大；直接反馈对齐虽然局部可并行但反馈结构非结构化，在深层架构中扩展性差。

Method: 在SVD分解的权重矩阵上训练每层，使用包含交叉熵、子空间对齐和正交正则化的复合损失函数更新SVD分量，构建匹配SVD结构的反馈矩阵。

Result: 在CIFAR-10、CIFAR-100和ImageNet上达到与反向传播相当的准确率，减少可训练参数数量，无需剪枝或后处理压缩。

Conclusion: 低秩流形上的局部学习是完整梯度训练的一个有原则且可扩展的替代方案。

Abstract: Training deep neural networks (DNNs) with backpropagation (BP) achieves
state-of-the-art accuracy but requires global error propagation and full
parameterization, leading to substantial memory and computational overhead.
Direct Feedback Alignment (DFA) enables local, parallelizable updates with
lower memory requirements but is limited by unstructured feedback and poor
scalability in deeper architectures, specially convolutional neural networks.
To address these limitations, we propose a structured local learning framework
that operates directly on low-rank manifolds defined by the Singular Value
Decomposition (SVD) of weight matrices. Each layer is trained in its decomposed
form, with updates applied to the SVD components using a composite loss that
integrates cross-entropy, subspace alignment, and orthogonality regularization.
Feedback matrices are constructed to match the SVD structure, ensuring
consistent alignment between forward and feedback pathways. Our method reduces
the number of trainable parameters relative to the original DFA model, without
relying on pruning or post hoc compression. Experiments on CIFAR-10, CIFAR-100,
and ImageNet show that our method achieves accuracy comparable to that of BP.
Ablation studies confirm the importance of each loss term in the low-rank
setting. These results establish local learning on low-rank manifolds as a
principled and scalable alternative to full-rank gradient-based training.

</details>


### [105] [Uncertainty Quantification for Regression: A Unified Framework based on kernel scores](https://arxiv.org/abs/2510.25599)
*Christopher Bülte,Yusuf Sale,Gitta Kutyniok,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 提出基于核评分规则的不确定性度量框架，统一了多种现有方法，为回归任务中的不确定性量化提供原则性设计指南。


<details>
  <summary>Details</summary>
Motivation: 回归任务在安全关键领域需要准确的不确定性量化，但现有研究主要关注分类任务，缺乏针对回归问题的系统化不确定性度量方法。

Method: 基于适当评分规则构建总不确定性、偶然不确定性和认知不确定性的度量家族，特别强调核评分方法，通过核函数选择控制度量的尾部敏感性、鲁棒性和分布外响应特性。

Result: 实验证明这些度量在下游任务中有效，揭示了不同实例化方法在鲁棒性和分布外检测性能之间的权衡关系。

Conclusion: 该框架为回归任务的不确定性量化提供了统一的理论基础和实践指导，核函数的选择直接影响度量在具体应用场景中的表现。

Abstract: Regression tasks, notably in safety-critical domains, require proper
uncertainty quantification, yet the literature remains largely
classification-focused. In this light, we introduce a family of measures for
total, aleatoric, and epistemic uncertainty based on proper scoring rules, with
a particular emphasis on kernel scores. The framework unifies several
well-known measures and provides a principled recipe for designing new ones
whose behavior, such as tail sensitivity, robustness, and out-of-distribution
responsiveness, is governed by the choice of kernel. We prove explicit
correspondences between kernel-score characteristics and downstream behavior,
yielding concrete design guidelines for task-specific measures. Extensive
experiments demonstrate that these measures are effective in downstream tasks
and reveal clear trade-offs among instantiations, including robustness and
out-of-distribution detection performance.

</details>


### [106] [INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats](https://arxiv.org/abs/2510.25602)
*Mengzhao Chen,Meng Wu,Hui Jin,Zhihang Yuan,Jing Liu,Chaoyi Zhang,Yunshui Li,Jie Huang,Jin Ma,Zeyue Xue,Zhiheng Liu,Xingyan Bin,Ping Luo*

Main category: cs.LG

TL;DR: 本文系统比较了FP和INT量化格式在不同粒度下的性能，发现FP在粗粒度量化中表现优异，但在细粒度（块级）量化中，MXINT8在算法精度和硬件效率上均优于FP格式。对于4位格式，FP通常具有精度优势，但通过异常值缓解技术，NVINT4可以超越NVFP4。


<details>
  <summary>Details</summary>
Motivation: 现代AI硬件越来越倾向于使用低精度FP格式来处理LLM中的激活异常值，但缺乏FP和INT量化在不同粒度下的统一比较，导致算法和硬件协同设计缺乏明确指导。

Method: 系统研究FP和INT格式之间的权衡，包括不同粒度（粗粒度和细粒度）和位宽（8位和4位）的比较，引入对称裁剪方法解决细粒度低比特INT训练中的梯度偏差问题。

Result: 发现性能关键交叉点：FP在粗粒度量化中表现优异，但在细粒度（块大小为32）的8位格式中，MXINT8在算法精度和硬件效率上均优于FP对应格式。对于4位格式，FP通常具有精度优势，但应用Hadamard旋转等异常值缓解技术后，NVINT4可以超越NVFP4。

Conclusion: 挑战当前硬件发展趋势，证明一刀切的FP方法不是最优选择，倡导细粒度INT格式（特别是MXINT8）为未来AI加速器提供了更好的精度、功耗和效率平衡。

Abstract: Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly
embracing low-precision floating-point (FP) formats to handle the pervasive
activation outliers in Large Language Models (LLMs). Despite this industry
trend, a unified comparison of FP and integer (INT) quantization across varying
granularities has been missing, leaving algorithm and hardware co-design
without clear guidance. This paper fills that gap by systematically
investigating the trade-offs between FP and INT formats. We reveal a critical
performance crossover: while FP excels in coarse-grained quantization, the
comparison at fine-grained (block-wise) levels is more nuanced. Our
comprehensive comparison demonstrates that for popular 8-bit fine-grained
formats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart
in both algorithmic accuracy and hardware efficiency. However, for 4-bit
formats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we
show that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like
Hadamard rotation are applied. We also introduce a symmetric clipping method
that resolves gradient bias in fine-grained low-bit INT training, enabling
nearly lossless performance for MXINT8 training. These findings challenge the
current hardware trajectory, demonstrating that a one-size-fits-all FP approach
is suboptimal and advocating that fine-grained INT formats, particularly
MXINT8, offer a better balance of accuracy, power, and efficiency for future AI
accelerators.

</details>


### [107] [BOLT-GAN: Bayes-Optimal Loss for Stable GAN Training](https://arxiv.org/abs/2510.25609)
*Mohammadreza Tavasoli Naeini,Ali Bereyhi,Morteza Noshad,Ben Liang,Alfred O. Hero III*

Main category: cs.LG

TL;DR: BOLT-GAN是基于贝叶斯最优学习阈值(BOLT)原理改进的WGAN框架，通过使用Lipschitz连续判别器，在四个标准图像生成基准上比WGAN获得10-60%更低的FID分数，训练稳定性更好。


<details>
  <summary>Details</summary>
Motivation: 改进WGAN框架的训练稳定性，通过引入贝叶斯最优学习阈值原理来提升生成对抗网络的性能。

Method: 在WGAN框架基础上引入BOLT原理，使用Lipschitz连续判别器，隐式最小化不同于Wasserstein距离的度量距离。

Result: 在CIFAR-10、CelebA-64、LSUN Bedroom-64和LSUN Church-64四个基准数据集上，BOLT-GAN比WGAN获得10-60%更低的FID分数，训练稳定性显著提升。

Conclusion: BOLT是一个广泛适用的原理，可以有效增强GAN的训练效果和稳定性。

Abstract: We introduce BOLT-GAN, a simple yet effective modification of the WGAN
framework inspired by the Bayes Optimal Learning Threshold (BOLT). We show that
with a Lipschitz continuous discriminator, BOLT-GAN implicitly minimizes a
different metric distance than the Earth Mover (Wasserstein) distance and
achieves better training stability. Empirical evaluations on four standard
image generation benchmarks (CIFAR-10, CelebA-64, LSUN Bedroom-64, and LSUN
Church-64) show that BOLT-GAN consistently outperforms WGAN, achieving 10-60%
lower Frechet Inception Distance (FID). Our results suggest that BOLT is a
broadly applicable principle for enhancing GAN training.

</details>


### [108] [Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization](https://arxiv.org/abs/2510.25616)
*Nikita Kachaev,Mikhail Kolosov,Daniil Zelezetsky,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 该论文系统研究了VLA模型在动作微调过程中的表示保留问题，发现简单的动作微调会导致视觉表示退化，并提出了一种简单有效的方法来缓解这种退化，提高对分布外场景的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究VLA模型在动作微调过程中，预训练的视觉语言模型的表示和知识保留程度，因为不清楚这些原始VL表示和知识在适应动作模态时是否被保留。

Method: 通过探测VLA的隐藏表示和分析注意力图来表征和测量表示退化效应；设计针对性任务和方法对比VLA模型与对应VLM，分离动作微调引起的VL能力变化；评估多种视觉表示对齐策略，并提出简单有效的方法来缓解退化。

Result: 发现简单的动作微调会导致视觉表示退化；提出的方法能够有效缓解这种退化，并在分布外场景中表现出更好的泛化能力。

Conclusion: 阐明了动作微调与VL表示退化之间的权衡关系，并强调了恢复继承VL能力的实用方法。

Abstract: The growing success of Vision-Language-Action (VLA) models stems from the
promise that pretrained Vision-Language Models (VLMs) can endow agents with
transferable world knowledge and vision-language (VL) grounding, laying a
foundation for action models with broader generalization. Yet when these VLMs
are adapted to the action modality, it remains unclear to what extent their
original VL representations and knowledge are preserved. In this work, we
conduct a systematic study of representation retention during VLA fine-tuning,
showing that naive action fine-tuning leads to degradation of visual
representations. To characterize and measure these effects, we probe VLA's
hidden representations and analyze attention maps, further, we design a set of
targeted tasks and methods that contrast VLA models with their counterpart
VLMs, isolating changes in VL capabilities induced by action fine-tuning. We
further evaluate a range of strategies for aligning visual representations and
introduce a simple yet effective method that mitigates degradation and yields
improved generalization to out-of-distribution (OOD) scenarios. Taken together,
our analysis clarifies the trade-off between action fine-tuning and the
degradation of VL representations and highlights practical approaches to
recover inherited VL capabilities. Code is publicly available:
https://blind-vla-paper.github.io

</details>


### [109] [Subgraph Federated Learning via Spectral Methods](https://arxiv.org/abs/2510.25657)
*Javad Aliakbari,Johan Östman,Ashkan Panahi,Alexandre Graell i Amat*

Main category: cs.LG

TL;DR: FedLap是一个用于图结构数据联邦学习的新框架，通过拉普拉斯平滑在谱域中利用全局结构信息，有效捕捉节点间依赖关系，同时确保隐私和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在隐私风险（需要交换敏感节点嵌入）或计算密集型步骤（影响可扩展性），无法有效处理多客户端间互联子图的联邦学习问题。

Method: 提出FedLap框架，在谱域中使用拉普拉斯平滑来捕获全局结构信息，避免直接交换节点嵌入，提供形式化的隐私分析。

Result: 在基准数据集上的广泛实验表明，FedLap相比现有技术实现了竞争性或更优的效用，是首个具有强隐私保证的子图联邦学习方案。

Conclusion: FedLap成功解决了图结构数据联邦学习中的隐私和可扩展性挑战，为互联子图场景提供了有效的解决方案。

Abstract: We consider the problem of federated learning (FL) with graph-structured data
distributed across multiple clients. In particular, we address the prevalent
scenario of interconnected subgraphs, where interconnections between clients
significantly influence the learning process. Existing approaches suffer from
critical limitations, either requiring the exchange of sensitive node
embeddings, thereby posing privacy risks, or relying on
computationally-intensive steps, which hinders scalability. To tackle these
challenges, we propose FedLap, a novel framework that leverages global
structure information via Laplacian smoothing in the spectral domain to
effectively capture inter-node dependencies while ensuring privacy and
scalability. We provide a formal analysis of the privacy of FedLap,
demonstrating that it preserves privacy. Notably, FedLap is the first subgraph
FL scheme with strong privacy guarantees. Extensive experiments on benchmark
datasets demonstrate that FedLap achieves competitive or superior utility
compared to existing techniques.

</details>


### [110] [Spectral Perturbation Bounds for Low-Rank Approximation with Applications to Privacy](https://arxiv.org/abs/2510.25670)
*Phuc Tran,Nisheeth K. Vishnoi,Van H. Vu*

Main category: cs.LG

TL;DR: 本文建立了对称矩阵谱范数扰动的新界限，改进了经典的Eckart-Young-Mirsky定理，并在差分隐私PCA中提供了改进的效用保证。


<details>
  <summary>Details</summary>
Motivation: 机器学习中需要理解噪声或测量误差如何影响低秩近似，特别是在谱范数下。现有工作通常分析Frobenius范数误差，但该度量可能高估或低估真实的子空间失真。谱范数能捕捉最坏情况的方向误差并提供最强的效用保证。

Method: 使用来自复分析的新颖轮廓自举方法，并将其扩展到包括多项式和矩阵指数在内的广泛谱函数类。在温和的特征值间隙和范数条件下，建立了对称矩阵的高概率谱范数扰动界限。

Result: 新界限对∥(A+E)_p - A_p∥提供了锐利估计，改进因子可达√n。在差分隐私PCA中获得了改进的效用保证，解决了文献中的一个开放问题。实证结果证实新界限在不同扰动机制下紧密跟踪实际谱误差。

Conclusion: 本文提出的谱范数扰动界限改进了经典理论，为差分隐私低秩近似提供了更强的效用保证，并通过实证验证了其有效性。

Abstract: A central challenge in machine learning is to understand how noise or
measurement errors affect low-rank approximations, particularly in the spectral
norm. This question is especially important in differentially private low-rank
approximation, where one aims to preserve the top-$p$ structure of a
data-derived matrix while ensuring privacy. Prior work often analyzes Frobenius
norm error or changes in reconstruction quality, but these metrics can over- or
under-estimate true subspace distortion. The spectral norm, by contrast,
captures worst-case directional error and provides the strongest utility
guarantees. We establish new high-probability spectral-norm perturbation bounds
for symmetric matrices that refine the classical Eckart--Young--Mirsky theorem
and explicitly capture interactions between a matrix $A \in \mathbb{R}^{n
\times n}$ and an arbitrary symmetric perturbation $E$. Under mild eigengap and
norm conditions, our bounds yield sharp estimates for $\|(A + E)_p - A_p\|$,
where $A_p$ is the best rank-$p$ approximation of $A$, with improvements of up
to a factor of $\sqrt{n}$. As an application, we derive improved utility
guarantees for differentially private PCA, resolving an open problem in the
literature. Our analysis relies on a novel contour bootstrapping method from
complex analysis and extends it to a broad class of spectral functionals,
including polynomials and matrix exponentials. Empirical results on real-world
datasets confirm that our bounds closely track the actual spectral error under
diverse perturbation regimes.

</details>


### [111] [Mechanistic Interpretability of RNNs emulating Hidden Markov Models](https://arxiv.org/abs/2510.25674)
*Elia Torre,Michele Viscione,Lucas Pompe,Benjamin F Grewe,Valerio Mante*

Main category: cs.LG

TL;DR: 本文展示了循环神经网络（RNNs）如何通过噪声维持的轨道动力学来模拟隐马尔可夫模型（HMMs）的离散状态转换，揭示了RNNs实现概率计算的新机制。


<details>
  <summary>Details</summary>
Motivation: 过去研究主要关注简单、确定性行为，但自然环境中观察到的行为往往更丰富、自发且具有随机性。需要理解RNNs如何生成这类复杂行为，特别是如何模拟HMMs揭示的离散状态转换动态。

Method: 首先训练RNNs复制HMMs的发射统计特性，然后逆向工程分析训练后的网络机制。在无输入时观察固定点动态，在有随机输入时分析噪声维持的轨道动力学。

Result: 训练后的RNNs活动在无输入时坍缩到单一固定点，但在随机输入驱动下沿闭合轨道表现出噪声维持的动态。网络发展出高度结构化的连接，少量"踢神经元"控制区域间转换，网络进入随机共振状态以实现概率计算。

Conclusion: RNNs通过模块化重用相同的动态模式来模拟复杂离散潜在动态，这为理解神经网络如何实现概率计算提供了新的组成性原则。

Abstract: Recurrent neural networks (RNNs) provide a powerful approach in neuroscience
to infer latent dynamics in neural populations and to generate hypotheses about
the neural computations underlying behavior. However, past work has focused on
relatively simple, input-driven, and largely deterministic behaviors - little
is known about the mechanisms that would allow RNNs to generate the richer,
spontaneous, and potentially stochastic behaviors observed in natural settings.
Modeling with Hidden Markov Models (HMMs) has revealed a segmentation of
natural behaviors into discrete latent states with stochastic transitions
between them, a type of dynamics that may appear at odds with the continuous
state spaces implemented by RNNs. Here we first show that RNNs can replicate
HMM emission statistics and then reverse-engineer the trained networks to
uncover the mechanisms they implement. In the absence of inputs, the activity
of trained RNNs collapses towards a single fixed point. When driven by
stochastic input, trajectories instead exhibit noise-sustained dynamics along
closed orbits. Rotation along these orbits modulates the emission probabilities
and is governed by transitions between regions of slow, noise-driven dynamics
connected by fast, deterministic transitions. The trained RNNs develop highly
structured connectivity, with a small set of "kick neurons" initiating
transitions between these regions. This mechanism emerges during training as
the network shifts into a regime of stochastic resonance, enabling it to
perform probabilistic computations. Analyses across multiple HMM architectures
- fully connected, cyclic, and linear-chain - reveal that this solution
generalizes through the modular reuse of the same dynamical motif, suggesting a
compositional principle by which RNNs can emulate complex discrete latent
dynamics.

</details>


### [112] [Convolutional Spiking-based GRU Cell for Spatio-temporal Data](https://arxiv.org/abs/2510.25696)
*Yesmine Abdennadher,Eleonora Cicciarella,Michele Rossi*

Main category: cs.LG

TL;DR: 提出卷积脉冲GRU（CS-GRU）单元，通过卷积操作保留局部结构，结合脉冲神经元的时间精度和GRU的门控机制，在时序和时空数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统RNN在处理长序列时容易丢失局部细节，现有方法如SpikGRU无法捕捉事件型时空数据中的细粒度局部依赖关系。

Method: 设计CS-GRU单元，集成卷积操作以保持局部结构依赖，同时结合脉冲神经元的时间精度和GRU的高效门控机制。

Result: 在时序数据集（NTIDIGITS、SHD）和时空基准测试（MNIST、DVSGesture、CIFAR10DVS）上表现优异，平均比最先进GRU变体提升4.35%，在MNIST上达到99.31%准确率，效率比SpikGRU高69%。

Conclusion: CS-GRU是一个多功能架构，在保持局部结构的同时有效处理时序和时空数据，在准确性和效率方面均优于现有方法。

Abstract: Spike-based temporal messaging enables SNNs to efficiently process both
purely temporal and spatio-temporal time-series or event-driven data. Combining
SNNs with Gated Recurrent Units (GRUs), a variant of recurrent neural networks,
gives rise to a robust framework for sequential data processing; however,
traditional RNNs often lose local details when handling long sequences.
Previous approaches, such as SpikGRU, fail to capture fine-grained local
dependencies in event-based spatio-temporal data. In this paper, we introduce
the Convolutional Spiking GRU (CS-GRU) cell, which leverages convolutional
operations to preserve local structure and dependencies while integrating the
temporal precision of spiking neurons with the efficient gating mechanisms of
GRUs. This versatile architecture excels on both temporal datasets (NTIDIGITS,
SHD) and spatio-temporal benchmarks (MNIST, DVSGesture, CIFAR10DVS). Our
experiments show that CS-GRU outperforms state-of-the-art GRU variants by an
average of 4.35%, achieving over 90% accuracy on sequential tasks and up to
99.31% on MNIST. It is worth noting that our solution achieves 69% higher
efficiency compared to SpikGRU. The code is available at:
https://github.com/YesmineAbdennadher/CS-GRU.

</details>


### [113] [MLPrE -- A tool for preprocessing and exploratory data analysis prior to machine learning model construction](https://arxiv.org/abs/2510.25755)
*David S Maxwell,Michael Darkoh,Sidharth R Samudrala,Caroline Chung,Stephanie T Schmidt,Bissan Al-Lazikani*

Main category: cs.LG

TL;DR: MLPrE是一个基于SparkDataFrames的通用、可扩展的机器学习预处理和探索性数据分析工具，通过JSON配置文件描述数据处理步骤，包含69个处理阶段，支持多种数据格式和规模。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习需求的增长，现有数据处理工具在可扩展性和集成性方面存在限制，无法满足大规模机器学习流水线的需求，需要开发一个轻量级、可扩展的预处理工具。

Method: 使用SparkDataFrames存储数据以确保可扩展性，采用通用JSON输入文件格式描述对DataFrame的逐步更改，实现了输入输出、过滤、基础统计、特征工程和探索性数据分析等69个处理阶段。

Result: 在六个不同数据集上验证了关键阶段的功能，展示了MLPrE能够独立处理平面文件中的多个字段并重新组合，使用UniProt术语数据集和葡萄酒质量数据进行了聚类分析，使用磷酸化位点激酶数据为图数据库准备数据。

Conclusion: MLPrE提供了一个通用且可扩展的预处理和早期数据分析工具，填补了机器学习应用中此类工具的关键空白，能够加速和简化大型工作流中的早期开发阶段。

Abstract: With the recent growth of Deep Learning for AI, there is a need for tools to
meet the demand of data flowing into those models. In some cases, source data
may exist in multiple formats, and therefore the source data must be
investigated and properly engineered for a Machine Learning model or graph
database. Overhead and lack of scalability with existing workflows limit
integration within a larger processing pipeline such as Apache Airflow, driving
the need for a robust, extensible, and lightweight tool to preprocess arbitrary
datasets that scales with data type and size. To address this, we present
Machine Learning Preprocessing and Exploratory Data Analysis, MLPrE, in which
SparkDataFrames were utilized to hold data during processing and ensure
scalability. A generalizable JSON input file format was utilized to describe
stepwise changes to that DataFrame. Stages were implemented for input and
output, filtering, basic statistics, feature engineering, and exploratory data
analysis. A total of 69 stages were implemented into MLPrE, of which we
highlight and demonstrate key stages using six diverse datasets. We further
highlight MLPrE's ability to independently process multiple fields in flat
files and recombine them, otherwise requiring an additional pipeline, using a
UniProt glossary term dataset. Building on this advantage, we demonstrated the
clustering stage with available wine quality data. Lastly, we demonstrate the
preparation of data for a graph database in the final stages of MLPrE using
phosphosite kinase data. Overall, our MLPrE tool offers a generalizable and
scalable tool for preprocessing and early data analysis, filling a critical
need for such a tool given the ever expanding use of machine learning. This
tool serves to accelerate and simplify early stage development in larger
workflows.

</details>


### [114] [Synthetic Data Reveals Generalization Gaps in Correlated Multiple Instance Learning](https://arxiv.org/abs/2510.25759)
*Ethan Harvey,Dennis Johan Loevlie,Michael C. Hughes*

Main category: cs.LG

TL;DR: 论文分析了传统多示例学习在医学影像中的局限性，指出其忽略实例间上下文关系的问题，并通过合成分类任务验证了现有方法的性能限制。


<details>
  <summary>Details</summary>
Motivation: 传统MIL方法在处理医学影像时，将实例（如切片或补丁）独立处理，忽略了相邻实例间的上下文关系，这在真实应用中可能影响分类准确性。

Method: 设计了一个合成分类任务，其中相邻实例的特征对准确预测至关重要，并将现成MIL方法与最优贝叶斯估计器进行性能比较。

Result: 实证表明，即使是最新的相关MIL方法，在从数万个实例从头训练时，仍难以达到最优泛化性能。

Conclusion: 当前MIL方法在处理实例间相关性方面仍有不足，需要进一步改进以充分利用上下文信息。

Abstract: Multiple instance learning (MIL) is often used in medical imaging to classify
high-resolution 2D images by processing patches or classify 3D volumes by
processing slices. However, conventional MIL approaches treat instances
separately, ignoring contextual relationships such as the appearance of nearby
patches or slices that can be essential in real applications. We design a
synthetic classification task where accounting for adjacent instance features
is crucial for accurate prediction. We demonstrate the limitations of
off-the-shelf MIL approaches by quantifying their performance compared to the
optimal Bayes estimator for this task, which is available in closed-form. We
empirically show that newer correlated MIL methods still struggle to generalize
as well as possible when trained from scratch on tens of thousands of
instances.

</details>


### [115] [Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE Solutions](https://arxiv.org/abs/2510.25769)
*Naoki Kiyohara,Edward Johns,Yingzhen Li*

Main category: cs.LG

TL;DR: 提出神经随机流（NSFs）及其潜在变体，通过条件归一化流直接学习SDE转移规律，实现任意时间点的一步采样，相比传统数值方法获得高达两个数量级的加速。


<details>
  <summary>Details</summary>
Motivation: 传统随机微分方程（SDEs）建模需要昂贵的数值求解器在任意时间点之间进行采样，这限制了其在金融、物理和机器学习中噪声和不规则采样时间序列的应用。

Method: 使用具有架构约束的条件归一化流直接学习（潜在）SDE转移规律，这些约束保留了从随机流继承的属性。

Result: 在大时间间隔下实现高达两个数量级的加速，在合成SDE模拟和真实世界跟踪及视频数据上保持与数值方法相当的分布准确性。

Conclusion: NSFs在保持分布准确性的同时，显著减少了任意时间点采样的计算成本，为SDE建模提供了高效的替代方案。

Abstract: Stochastic differential equations (SDEs) are well suited to modelling noisy
and irregularly sampled time series found in finance, physics, and machine
learning. Traditional approaches require costly numerical solvers to sample
between arbitrary time points. We introduce Neural Stochastic Flows (NSFs) and
their latent variants, which directly learn (latent) SDE transition laws using
conditional normalising flows with architectural constraints that preserve
properties inherited from stochastic flows. This enables one-shot sampling
between arbitrary states and yields up to two orders of magnitude speed-ups at
large time gaps. Experiments on synthetic SDE simulations and on real-world
tracking and video data show that NSFs maintain distributional accuracy
comparable to numerical approaches while dramatically reducing computation for
arbitrary time-point sampling.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [116] [Accelerating behavior from dynamical system analysis parameters](https://arxiv.org/abs/2510.24808)
*Rahul Bhagat,B. Mishra*

Main category: gr-qc

TL;DR: 该论文通过动力系统分析研究了f(Q)引力模型，使用CC和Pantheon+超新星数据约束参数，发现模型在当前表现为类quintessence行为，在晚期趋于ΛCDM模型，并确认了稳定的de Sitter吸引子存在。


<details>
  <summary>Details</summary>
Motivation: 研究f(Q)引力模型在宇宙学中的应用，特别是验证其是否能解释宇宙的晚期加速膨胀行为，并寻找与观测数据一致的参数空间。

Method: 将修正的Friedmann方程重构为耦合微分方程组，使用数值方法求解并结合MCMC技术约束参数，利用CC和Pantheon+超新星数据集进行参数拟合。

Result: 模型在当前表现出类quintessence行为，但在晚期趋于ΛCDM模型；动力系统分析找到了对应不同宇宙演化阶段的临界点；确认了稳定的de Sitter吸引子存在，支持宇宙加速膨胀。

Conclusion: f(Q)引力模型能够解释宇宙的加速膨胀现象，通过动力系统分析找到了稳定的de Sitter解，模型在晚期趋于标准ΛCDM模型，与观测数据一致。

Abstract: We have performed the dynamical system analysis to obtain the critical point
in which, the value of the geometric and dynamical parameters satisfy the
late-time cosmic behavior of the Universe. At the outset, the modified
Friedmann equations have been reformulated into a system of coupled
differential equations to ensure that the minimal set of equations required for
a second-order $f(Q)$ gravity. Then these equations are solved numerically to
constrain the parameters with Markov Chain Monte Carlo (MCMC) techniques.
Cosmic Chronometers (CC) and high-precision Pantheon$^+$ Type Ia Supernovae
datasets are used to constrain the parameters. The evolution of key
cosmological parameters indicates that the model exhibits quintessence-like
behavior at present, with a tendency to converge towards the $\Lambda$CDM model
at late-times. The dynamic system analysis provided the critical points that
correspond to different phases of the Universe, which are analyzed in detail.
The existence of a stable de Sitter attractor confirms the accelerating
behavior of the model.

</details>


### [117] [A Cosmological Bragg Law: Interpreting the CMB as a Diffractogram of Foliated Spacetime](https://arxiv.org/abs/2510.24828)
*David Izabel*

Main category: gr-qc

TL;DR: 该论文将宇宙微波背景辐射的角功率谱解释为早期时空的反转X射线衍射图，提出了宇宙学版的布拉格定律，准确预测了普朗克卫星观测到的前四个声学峰位置。


<details>
  <summary>Details</summary>
Motivation: 基于Ringermacher和Mead发现的宇宙尺度因子离散振荡现象，探索时空可能具有类似晶体的振动特性，为理解CMB的谐波结构提供新的几何框架。

Method: 将CMB角功率谱视为早期时空的反转X射线衍射图，建立宇宙学版的布拉格定律，将角多极子与声学壳层间距和共动距离联系起来。

Result: 该模型以惊人的准确性预测了普朗克卫星观测到的前四个声学峰位置，需要一个接近0.9的全局校正因子来使预测与观测对齐。

Conclusion: 虽然不能替代完整的辐射传输建模，但这种方法为理解CMB的谐波结构和原始时空可能的晶体性质提供了一个补充的几何框架，校正因子可能反映了爱因斯坦-嘉当理论中几何扭转的影响。

Abstract: Recent work by Ringermacher and Mead has revealed discrete oscillations in
the cosmological scale factor, suggesting that spacetime may exhibit
vibrational properties akin to those of a crystal. Building on this idea, we
propose a novel geometric interpretation of the Cosmic Microwave Background CMB
angular power spectrum by treating it as an inverted X-ray diffractogram of
early spacetime. In this framework,the acoustic peaks correspond to
constructive interference from a stratified, resonant structure of the universe
at the time of recombination. We formulate a cosmological analogue of Bragg
law, linking angular multipoles to acoustic shell spacing and comoving
distance. This model predicts the positions of the first four acoustic peaks
observed by the Planck satellite with remarkable accuracy.A global correction
factor of near 0.9 needed to align predictions with observations, may reflect
the influence of geometric torsion as described in EinsteinCartan theory. While
not a replacement for full radiative transfer modeling,this approach offers a
complementary geometric framework for understanding the harmonic structure of
the CMB and the possible crystalline nature of primordial spacetime.

</details>


### [118] [Observable signature of magnetic tidal coupling in hierarchical triple systems](https://arxiv.org/abs/2510.24897)
*Marta Cocco,Gianluca Grignani,Troels Harmark,Marta Orselli,Davide Panella,Daniele Pica*

Main category: gr-qc

TL;DR: 研究围绕超大质量黑洞的紧凑双星系统中的相对论磁潮汐相互作用，发现在0.5后牛顿阶下磁潮汐会引入新的共振，显著改变双星长期演化并加速合并过程。


<details>
  <summary>Details</summary>
Motivation: 探索紧凑双星系统在超大质量黑洞附近的动力学行为，特别关注相对论磁潮汐这种强引力效应在共振动力学中的作用。

Method: 将进动共振分析扩展到0.5后牛顿阶，引入四极磁潮汐矩，通过对内轨道哈密顿量平均化，并用拉格朗日行星方程进行数值求解。

Result: 磁潮汐引入新的共振机制，产生额外的偏心率激发，显著改变双星长期演化，共振强度依赖于轨道偏心率和倾角，加速双星合并过程。

Conclusion: 磁潮汐耦合是一种新颖的强引力效应，对超大质量黑洞附近紧凑双星系统的共振动力学具有重要影响，其产生的引力波特征可能被LISA观测到。

Abstract: We study hierarchical triple systems formed by a compact binary orbiting a
supermassive black hole (SMBH), focusing on the role of relativistic magnetic
tidal interactions. Extending previous analyses of precession resonances to 0.5
post-Newtonian order, we incorporate quadrupolar magnetic tidal moments, which
have no Newtonian counterpart. Averaging the Hamiltonian over the inner orbit,
we find that magnetic tides introduce new resonances absent at lower order,
leading to additional eccentricity excitations and significantly modifying the
binary's long-term evolution. Numerical solutions of the Lagrange Planetary
Equations confirm these analytical predictions and reveal how resonance
strength depends on orbital eccentricity and inclination. The resulting
dynamics accelerates the binary merger and imprint distinctive signatures on
gravitational waves, potentially observable by LISA. Our findings identify
magnetic tidal coupling as a novel strong-gravity effect and establish its
importance for the resonant dynamics of compact-object binaries near SMBHs.

</details>


### [119] [Midisuperspacetime foam and the cosmological constant](https://arxiv.org/abs/2510.24953)
*Steven Carlip*

Main category: gr-qc

TL;DR: 该论文探讨了在普朗克尺度不均匀的宇宙中，巨大的宇宙常数可能被有效隐藏，使得平均膨胀率保持接近零。


<details>
  <summary>Details</summary>
Motivation: 标准量子场论预测巨大的宇宙常数，但在不均匀宇宙中这一预测的观测意义需要重新审视。

Method: 使用球对称中超空间模型，研究量子波函数在平均膨胀率小的区域中被长期困住的现象。

Result: 波函数可以在平均膨胀率保持较小的区域中被长期困住，从而有效隐藏大的宇宙常数。

Conclusion: 在普朗克尺度不均匀的宇宙中，大的宇宙常数可能通过量子效应被隐藏，不表现为观测到的宇宙膨胀。

Abstract: Standard quantum field theory arguments predict an enormous cosmological
constant. But what would this mean observationally? For a homogeneous universe
the answer is clear, but if the universe is inhomogeneous at the Planck scale,
the question becomes more subtle: for a large class of initial data, rapidly
expanding and contracting regions coexist and give an average expansion near
zero. Classically, such data develop singularities, and we need a quantum
description of their evolution. I describe results from a spherically symmetric
midisuperspace model, in which the wave function can become trapped for long
periods in regions in which the average expansion remains small, effectively
hiding a large cosmological constant.

</details>


### [120] [Characteristic Critical Collapse of a Yang-Mills Field With Null Infinity](https://arxiv.org/abs/2510.25534)
*Rita P. Santos,Krinio Marouda,David Hilditch*

Main category: gr-qc

TL;DR: 研究了爱因斯坦方程在黑洞形成阈值附近的临界现象，发现Yang-Mills场坍缩表现出离散自相似性，黑洞质量按临界指数γ≈0.1977标度。


<details>
  <summary>Details</summary>
Motivation: 研究引力坍缩中的临界现象，特别是Yang-Mills场在黑洞形成阈值附近的行为，以了解全局量如Bondi质量和news函数的特性。

Method: 使用紧致化Bondi坐标进行特征演化数值模拟，采用四阶精度数值方法研究Yang-Mills场的临界坍缩。

Result: 坍缩场表现出局部DSS行为，回响周期Δ≈0.7388；全局量如Bondi质量和news函数也显示相同DSS行为；黑洞质量标度临界指数γ≈0.1977±0.0009。

Conclusion: 这些结果是普适的，与初始数据无关，证实了引力坍缩临界现象的普遍性。

Abstract: Solutions to the Einstein equations near the threshold of black hole
formation exhibit remarkable behavior known as critical phenomena gravitational
collapse. In this work we perform characteristic evolution in compactified
Bondi coordinates in order to study the critical collapse of a Yang-Mills
field, allowing for the extraction of global quantities such as the Bondi mass
and news function. Our numerical approach is fourth-order accurate. First, we
demonstrate that the collapsing field exhibits local DSS behavior,
characterized by an echoing period of~$\Delta \simeq 0.7388$, agreeing with
previous works up to the second decimal place. We find that global quantities
such as the Bondi mass and news function display the same DSS behavior. We
furthermore show that the mass of the black holes formed during near-threshold
evolutions scales as a function of the distance to the critical parameter, with
a critical exponent of approximately~$\gamma=0.1977\pm0.0009$. Finally, our
findings indicate that these results are universal, irrespective of the initial
data.

</details>


### [121] [Periodic orbits and their gravitational waves in EMRIs: supermassive black hole affected by galactic dark matter halos](https://arxiv.org/abs/2510.24989)
*Guo-He Li,Chen-Kai Qiao,Jun Tao*

Main category: gr-qc

TL;DR: 该研究系统分析了三种暗物质晕模型（NFW、Beta和Moore）对黑洞周围周期性轨道及其引力波辐射的影响，发现暗物质质量和特征半径会显著改变轨道形状和波形特征。


<details>
  <summary>Details</summary>
Motivation: 研究极端质量比旋进（EMRIs）中周期性轨道的动力学和引力波辐射，特别是在不同暗物质晕环境下的表现，以理解暗物质对引力波特征的影响。

Method: 使用两个参数（暗物质特征质量和晕特征半径）来有效描述暗物质分布，系统研究三种暗物质晕模型（NFW、Beta和Moore）对黑洞周围周期性轨道及其引力波辐射的影响。

Result: 较大的暗物质质量和较小的特征半径会导致轨道形状和引力波波形与史瓦西黑洞情况产生更显著偏差；随着晕特征半径增加，结果逐渐收敛于史瓦西黑洞；NFW和Beta模型结果几乎无法区分，而Moore模型显示出明显不同的效应。

Conclusion: 这些发现增强了我们对暗物质晕影响引力波特征的理解，可能为未来空间探测器提供约束条件。

Abstract: Periodic orbits exhibiting zoom-whirl behavior have become attractive topics
for studying particle dynamics and gravitational wave emission in
extreme-mass-ratio inspirals (EMRIs). This study systematically investigates
periodic orbits around black holes and their gravitational wave radiation in
three dark matter halo environments: NFW, Beta, and Moore models. The dark
matter distribution in these models can be effectively incorporated using two
parameters -- the dark matter characteristic mass and halo characteristic
radius. Our results reveal that for a larger dark matter mass and a smaller
characteristic radius, the shapes of the periodic orbits and the corresponding
gravitational waveforms show more significant deviations from the Schwarzschild
case. As the halo characteristic radius increases, the orbital shapes and
waveform characteristics gradually converge with the Schwarzschild black hole
results. For the different dark matter halo models, the NFW and Beta models
produce nearly indistinguishable results, while the Moore model shows
distinctly different effects. These findings enhance our understanding of dark
matter halo effects on gravitational wave signatures and may provide
constraints for future space-based detectors.

</details>


### [122] [BOB the (Waveform) Builder: Optimizing Analytical Black-Hole Binary Merger Waveforms](https://arxiv.org/abs/2510.25012)
*Anuj Kankani,Sean T. McWilliams*

Main category: gr-qc

TL;DR: BOB模型为黑洞合并的引力波辐射提供了完全解析的物理描述，在合并-衰减阶段表现出与最先进波形模型相当的精度，且参数空间覆盖更广。


<details>
  <summary>Details</summary>
Motivation: 开发一个完全解析的物理模型来描述黑洞合并产生的引力波辐射，特别是合并-衰减阶段，以提供对NR校准模型的独立测试并更好地理解合并物理。

Method: 使用BOB模型对主导(2,2)模式进行综合验证，与数值相对论模拟、最先进波形模型和准正规模求和进行比较分析。

Result: BOB在描述引力波新闻方面最准确，精度与高度校准的EOB和NR代理模型相当，且在NR目录稀疏覆盖的参数区域仍保持高精度。

Conclusion: BOB成为引力波分析的强大工具，可用于提供NR校准模型的独立测试，并更好地理解合并的底层物理机制。

Abstract: The Backwards-One-Body (BOB) model provides a fully analytical and physically
motivated description of the merger-ringdown gravitational radiation emanating
from a black hole binary merger. We perform a comprehensive validation of BOB
for the dominant $(2,2)$ mode of quasi-circular and non-precessing systems,
assessing its accuracy against numerical relativity (NR) simulations,
state-of-the-art waveform models, and a sum of quasinormal modes. We
demonstrate that BOB most accurately describes the gravitational wave news,
achieving accuracy comparable to highly-calibrated Effective-One-Body and NR
surrogate models. Because BOB is minimally tuned to NR catalogs, it retains a
high level of accuracy in regions of the parameter space sparsely covered by
current NR catalogs. BOB yields an analytic link between the amplitude of the
fundamental quasinormal mode and the peak amplitude of the News, which we
verify to within the errors of a surrogate ringdown model. We identify a flavor
of BOB that requires only the remnant mass and spin, yet matches the accuracy
of models that fit a sum of many overtones. Lastly, we show that BOB accurately
models both the mass and current quadrupole waves for superkick configurations,
contrary to a claim in the literature, and explain why that study was not
actually implementing BOB as it has been defined. Our findings establish BOB as
a powerful tool for gravitational wave analysis, for providing independent
tests of NR-calibrated models, and for better understanding the underlying
physics of the merger. We provide a companion python package, gwBOB, allowing
for the easy construction of various flavors of BOB and comparison to NR
waveforms.

</details>


### [123] [Ringdown in Vaidya spacetimes: time-dependent frequencies, Penrose limit and time-domain analyses](https://arxiv.org/abs/2510.25062)
*Chul-Moon Yoo,Masashi Kimura,Akihiro Ishibashi,Rikuto Ohashi*

Main category: gr-qc

TL;DR: 本文研究了在动态Vaidya时空中使用Penrose极限几何来表征环降波的可能性，特别是围绕动态光子球的分析。


<details>
  <summary>Details</summary>
Motivation: 在静态球对称黑洞时空中，准正规模频率可以通过不稳定圆形零测地线轨道上的角速度和Lyapunov指数来表征。本文试图将这种分析扩展到动态Vaidya时空中的动态光子球。

Method: 使用Penrose极限几何分析动态Vaidya时空中的动态光子球，并将结果与Vaidya时空中数值计算的波形进行比较。

Result: 讨论了Penrose极限几何在多大程度上与Vaidya时空中的环降波相关，并与数值计算结果进行了对比分析。

Conclusion: 探索了Penrose极限几何在动态时空背景下表征环降波的适用性和局限性。

Abstract: We examine the possible characterization of ringdown waves in a dynamical
Vaidya spacetime using the Penrose limit geometry around the dynamical photon
sphere. In the case of a static spherically symmetric black hole spacetime, it
is known that the quasinormal frequency in the eikonal limit can be
characterized by the angular velocity and the Lyapunov exponent for the null
geodesic congruence on the orbit of the unstable circular null geodesic. This
correspondence can be further backed up by the analysis of the Penrose limit
geometry around the unstable circular null geodesic orbit. We try to extend
this analysis to a Vaidya spacetime, focusing on the dynamical photon sphere in
it. Then we discuss to what extent the Penrose limit geometry can be relevant
to the ringdown waves in the Vaidya spacetime, comparing the results with the
numerically calculated waveform in the Vaidya spacetime.

</details>


### [124] [Inferring the Stochastic Gravitational-Wave Background from Eccentric Stellar-mass Binary Black Holes with Spaceborne Detectors](https://arxiv.org/abs/2510.25353)
*Zheng-Cheng Liang,Zhi-Yuan Li,Yi-Ming Hu*

Main category: gr-qc

TL;DR: 使用贝叶斯框架首次评估偏心恒星质量双黑洞产生的随机引力波背景在空间探测器中的可探测性和区分特征，考虑了银河系前景污染。


<details>
  <summary>Details</summary>
Motivation: 偏心恒星质量双黑洞产生的随机引力波背景包含其起源的重要线索，需要评估其在空间探测器中的可探测性并区分不同形成通道的特征。

Method: 采用贝叶斯框架分析三种形成通道（孤立双星演化、球状星团动力学组装、活动星系核）的偏心双黑洞产生的随机引力波背景，考虑银河系前景污染。

Result: 天琴、LISA和太极探测器在运行4年后能探测到孤立和球状星团形成的双黑洞背景，信噪比分别约为10、60和170。活动星系核形成的高度偏心双黑洞产生具有频谱转折特征的背景，虽然信噪比降低约一个量级，但能被LISA和太极清晰区分。

Conclusion: 空间引力波探测器能有效探测偏心双黑洞产生的随机引力波背景，其中活动星系核形成的双黑洞背景具有独特的频谱特征，可用于区分不同形成通道。

Abstract: The stochastic gravitational-wave background (SGWB) from eccentric
stellar-mass binary black holes (SBBHs) holds crucial clues to their origins.
For the first time, we employ a Bayesian framework to assess the detectability
and distinguishing features of such an SGWB with spaceborne detectors, while
accounting for contamination from the Galactic foreground. Our analysis covers
eccentric SBBHs from three formation channels: isolated binary evolution,
dynamical assembly in globular clusters (GCs), and in active galactic nuclei
(AGNs). We find that TianQin, LISA, and Taiji can detect the SGWBs from both
isolated and GC-formed SBBHs after 4 years of operation, with the corresponding
signal-to-noise ratios of around 10, 60, and 170. However, these backgrounds
are spectrally degenerate with a strictly power-law SGWB. Furthermore, highly
eccentric SBBHs formed in AGNs yield an SGWB marked by a spectral turnover and
sharp decline. While this feature lowers the signal-to-noise ratio by
approximately an order of magnitude, it can enable a clear distinction from the
strictly power-law background using LISA and Taiji.

</details>


### [125] [Highly damped Quasi-Normal Modes of a Loop Quantum Black Hole](https://arxiv.org/abs/2510.25397)
*Clara Montagnon*

Main category: gr-qc

TL;DR: 计算环量子引力启发的黑洞的渐近准正则模频率，研究聚合物变形参数P对高阻尼QNM谱的影响，发现QNM的实部和虚部都出现有趣的振荡行为。


<details>
  <summary>Details</summary>
Motivation: 研究环量子引力启发的黑洞模型中的准正则模，特别关注聚合物变形参数P对高阻尼QNM谱的影响，以理解量子引力效应在黑洞动力学中的表现。

Method: 使用单值性技术计算QNM的渐近行为，考虑自旋0和自旋2的测试场扰动，并与连分数法获得的数值结果进行比较。

Result: 发现QNM的实部和虚部都出现振荡行为，振荡周期随聚合物变形参数P变化，数值结果与单值性预测相当吻合。

Conclusion: 环量子引力启发的黑洞模型在高阻尼QNM谱中展现出独特的振荡特性，这为理解量子引力对黑洞动力学的影响提供了重要见解。

Abstract: We compute asymptotic Quasi-Normal Mode (QNM) frequencies -- i.e. frequencies
with a very large Imaginary part -- of a Loop Quantum Gravity inspired Black
Hole. The deformations from the Schwarzschild Black Hole are encoded via two
parameters: the minimal area gap $a_0$ and the polymeric deformation parameter
$P$. In this study, we focus on the effect of the latter one, $P$, on the
highly-damped part of QNM spectra. We consider both spin 0 and spin 2
test-field perturbations on the Black Hole as proper gravitational
perturbations cannot be performed on an effective model. We use an analytical
method of computation of QNMs referred to as the monodromy technique, which
allows us to compute the asymptotic behaviour of QNMs. We found interesting
oscillating behaviour in both the Real part and the Imaginary part of the QNMs,
where the oscillation period varies with the polymeric deformation parameter
$P$. We compare these analytical predictions to numerical results obtained
thanks to the Continued fraction method. Even though the latter does not
converge for QNMs with a very large Imaginary part, the numerical results are
in rather good agreement with the monodromy prediction.

</details>


### [126] [Gauge Boundary conditions to mitigate CoM drift in BBH simulations](https://arxiv.org/abs/2510.25465)
*Dongze Sun,Sizheng Ma,Mark A. Scheel,Saul A. Teukolsky*

Main category: gr-qc

TL;DR: 本文分析了数值相对论模拟中黑洞双星系统质心漂移的问题，发现这是由规范边界条件不准确导致的规范伪影，并提出了一种包含质心校正源项的改进边界条件来有效抑制这种漂移。


<details>
  <summary>Details</summary>
Motivation: 在SpEC代码中进行黑洞双星系统的长期数值相对论模拟时，观察到质心会意外地呈指数漂移远离模拟原点，这影响了模拟的准确性，需要分析其原因并找到解决方案。

Method: 通过分析发现质心漂移是由规范波在计算域外边界反射引起的规范伪影，提出了一种改进的边界条件，该条件包含一个显式的质心校正源项来抵消质心运动。

Result: 数值实验表明，与标准实现相比，新的边界处理方法将质心漂移减少了几个数量级，且没有引入任何不希望的物理伪影。

Conclusion: 质心漂移问题可以通过改进规范边界条件得到有效解决，新方法显著提高了数值相对论模拟的准确性。

Abstract: Long-term numerical relativity (NR) simulations of binary black hole (BBH)
systems in the Spectral Einstein Code (SpEC) code exhibit an unexpected
exponential drift of the center-of-mass (CoM) away from the simulation's
origin. In our work, we analyze this phenomenon and demonstrate that it is not
a physical effect but rather a manifestation of a gauge artifact. The origin of
this drift is the reflection of the gauge waves off the outer boundary of the
computational domain. These reflections are introduced by inaccuracies in the
gauge boundary condition, specifically, the application of the Sommerfeld
condition to the time derivative of the gauge fields. Such an approach fails to
completely suppress or correctly absorb the outgoing modes, thereby generating
artificial feedback into the simulation. To mitigate this problem, we introduce
a modified boundary condition that incorporates an explicit CoM correction
source term designed to counteract the CoM motion. Our numerical experiments,
performed with the SpEC code, reveal that this new boundary treatment reduces
the CoM drift by several orders of magnitude compared to the standard
implementation, and does not introduce any unwanted physical artifacts.

</details>


### [127] [Observational appearance and additional photon rings of the asymmetric thin-shell wormhole in the Kalb-Ramond background](https://arxiv.org/abs/2510.25485)
*K. Tan,X. G. Lan*

Main category: gr-qc

TL;DR: 使用光线追踪方法研究Kalb-Ramond场中非对称薄壳虫洞的观测图像，发现其具有额外的透镜环和光子环簇等独特观测特征。


<details>
  <summary>Details</summary>
Motivation: 研究非对称薄壳虫洞在Kalb-Ramond场中的观测特性，探索其与黑洞的区别，特别是电荷Q和洛伦兹破坏参数l对观测特征的影响。

Method: 采用光线追踪方法计算零测地线和有效势能，分析光子球半径和临界撞击参数的变化，确定光子偏转角和轨迹，使用薄吸积盘作为背景光源并结合两种经典观测辐射模型。

Result: 发现非对称薄壳虫洞具有额外的透镜环和光子环簇等独特特征，随着电荷Q和洛伦兹破坏参数l的增加，特定附加光晕的覆盖区域相应扩大。

Conclusion: 非对称薄壳虫洞在Kalb-Ramond场中展现出与黑洞不同的独特观测特征，这些特征随电荷和洛伦兹破坏参数的变化而变化。

Abstract: In this paper, we utilize the ray-tracing method to conduct an in-depth study
of the observational images of asymmetric thin-shell wormholes in the
Kalb-Ramond field. Initially, we calculate the null geodesics and effective
potential energy of ATSW, and investigate the variations in the photon sphere
radius and critical impact parameter under different values of charge Q and
Lorentz-violating parameter l. Based on these calculations, we determine the
photon deflection angles and trajectories within this space-time structure.
Specifically, depending on the photon impact parameters, the photon
trajectories can be categorized into three types. By using a thin accretion
disk as the sole background light source and incorporating two classical
observational radiation models, we find that under conditions of equal mass M,
charge quantity Q, and Lorentz-violation parameter l, the asymmetric thin-shell
wormhole exhibits unique observational features such as additional lensing
rings and photon ring clusters. Furthermore, distinct from black holes, as the
charge quantity Q and the Lorentz-violation parameter l increase, the coverage
area of the specific additional halo also expands correspondingly.

</details>


### [128] [Parameter matching between horizon quasi-local and point-particle definitions at 1PN for quasi-circular and non spinning BBH systems in harmonic gauge](https://arxiv.org/abs/2510.25618)
*Dongze Sun,Leo C. Stein*

Main category: gr-qc

TL;DR: 比较了后牛顿理论和数值相对论中黑洞系统参数定义的差异，发现在谐和规范下，视界准局域质量与后牛顿点粒子质量在1PN阶一致，为PN-NR波形混合提供了关键参数匹配框架。


<details>
  <summary>Details</summary>
Motivation: 后牛顿理论和数值相对论对黑洞质量和自旋的定义不同：NR基于视界几何准局域测量，PN基于点粒子渐近匹配。这些定义在有限分离时可能存在差异，影响精确建模。

Method: 在谐和规范下，通过渐近匹配将黑洞扰动理论的内区度规与PN双体度规连接，构造保持强场区规范的坐标变换，在谐和惯性时间片上求解视界并计算准局域质量。

Result: 在穿透视界的谐和切片上，视界准局域质量与PN点粒子质量在1PN阶一致；对于偏离穿透条件的切片，两者差异为1PN阶修正。

Conclusion: 建立了准局域和PN参数定义之间的匹配关系，为PN-NR波形混合和数值模拟初始条件提供了重要桥梁，可扩展到自旋、偏心等更一般情况。

Abstract: We investigate how commonly used parameter definitions in Post-Newtonian (PN)
theory compare with those from Numerical Relativity (NR) for binary black hole
(BBH) systems. In NR, masses and spins of each companion are measured
quasi-locally from apparent horizon geometry, whereas in PN they are attributes
of point particles defined via asymptotic matching in body zones. Although
these definitions coincide in the infinite-separation limit, they could differ
by finite-separation corrections that matter for precision modeling. Working
entirely in harmonic gauge, we perform asymptotic matching between each
companion's inner zone metric -- obtained from black hole perturbation theory
-- and the PN two-body metric, and construct the coordinate transformation that
preserves the gauge in the strong field region. We solve perturbatively for the
apparent horizon (AH) on a group of harmonic inertial time slice and compute
its quasi-local areal mass from the horizon geometry. Then we establish the
leading order matching between quasi-local (AH based) and PN (point-particle)
parameter definitions in harmonic gauge. We find that on a horizon penetrating
harmonic slicing, the AH quasi-local mass agrees with the PN point-particle
mass at 1PN order. For generic harmonic slicings that deviate from the horizon
penetrating condition by a 1PN order perturbation, the AH mass differs from the
PN mass also by a 1PN correction. This parameter matching is crucial for
hybridizing PN and NR waveforms and for providing better initial conditions in
NR and Cauchy-Characteristic Evolution (CCE) simulations. The framework
provides a bridge between different descriptions of BBH systems, and it can be
extended to spinning and eccentric cases and more general NR gauges.

</details>


### [129] [Cross-correlating Astrometric and Timing Residuals to Constrain Stochastic Gravitational-Wave Backgrounds](https://arxiv.org/abs/2510.25646)
*Elias Fink,Carlo Contaldi,Giorgio Mentasti*

Main category: gr-qc

TL;DR: 该论文研究了脉冲星计时阵列与太阳系天体天体测量观测之间的交叉相关，用于探测随机引力波背景，提出了广义Hellings-Downs曲线并分析了信噪比和灵敏度。


<details>
  <summary>Details</summary>
Motivation: 探索不同观测域（脉冲星计时和太阳系天体天体测量）之间的交叉相关，以提供对随机引力波背景的互补探测方法，减少系统误差影响。

Method: 使用统一的旋量加权形式体系推导角相关函数（广义Hellings-Downs曲线），计算信噪比和灵敏度，重点关注PTA红移信号与太阳系天体天体测量效应的交叉相关。

Result: 当前小行星跟踪的天体测量精度尚无法与纯PTA测量竞争，但该方法在高频段具有增强灵敏度，未来亚毫角秒精度的广域巡天可能使其成为可行工具。

Conclusion: 该技术提供了对PTA探测的独立交叉检验，减少了不同测量域相关系统误差的影响，并可能成为连接PTA和LISA频段的观测桥梁。

Abstract: We investigate the cross-correlation between astrometric and timing-residual
observables for distant sources, such as pulsars and galaxies, and equivalent
observables for nearby solar system bodies. Using the unified spin-weighted
formalism introduced in [1], we derive the angular correlation
functions-generalised Hellings-Downs curves-that describe the response of these
mixed observables to a stochastic, unpolarised gravitational-wave background
(SGWB). We compute the expected signal-to-noise ratio (SNR) and sensitivity for
such measurements, focusing on cross-correlations between pulsar timing array
(PTA) redshift signals and astrometric or distortion (shimmering) effects
induced in solar system objects such as asteroids. Although the current
astrometric precision of asteroid tracking does not yet provide competitive
constraints relative to PTA-only surveys, the method offers a complementary
probe with enhanced sensitivity at higher frequencies. Future wide-field
surveys capable of sub-milliarcsecond precision could make this approach a
viable tool for detecting or constraining the SGWB. A key advantage of the
technique is its reduced susceptibility to correlated systematics across
different measurement domains, providing an independent cross-check of PTA
detections and a potential observational bridge between PTA and LISA frequency
bands.

</details>


### [130] [Action-Angle Variables in Quantum Gravity](https://arxiv.org/abs/2510.25717)
*Jarmo Mäkelä*

Main category: gr-qc

TL;DR: 基于作用角变量和玻尔-索末菲量子化规则，论证如果存在最小非零面积，则对于有质量粒子也存在最大可观测速度，略小于光速。


<details>
  <summary>Details</summary>
Motivation: 探索量子引力理论中最小面积假设的物理后果，特别是对有质量粒子速度限制的影响。

Method: 使用作用角变量和玻尔-索末菲量子化规则进行理论推导，从最小面积假设出发分析粒子运动学。

Result: 推导出如果存在最小非零面积，则存在最大可观测速度，该速度略小于光速。

Conclusion: 最小面积假设自然地导致对有质量粒子的最大速度限制，为量子引力理论提供了新的物理洞察。

Abstract: We formulate an argument, based on the use of the action-angle variables and
the Bohr-Sommerfeld quantization rule, to the effect that if there exists the
smallest possible non-zero area, there also exists, for massive particles, the
largest possible observable speed, which is a bit less than the speed of light.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [131] [Discovery of Hyperelastic Constitutive Laws from Experimental Data with EUCLID](https://arxiv.org/abs/2510.24747)
*Arefeh Abbasi,Maurizio Ricci,Pietro Carrara,Moritz Flaschel,Siddhant Kumar,Sonia Marfia,Laura De Lorenzis*

Main category: physics.comp-ph

TL;DR: EUCLID框架在实验数据上的性能评估，比较传统材料模型参数识别与自动化模型发现方法在橡胶材料本构关系识别中的表现。


<details>
  <summary>Details</summary>
Motivation: 评估EUCLID自动化本构关系发现框架在真实实验数据上的有效性，比较其与传统参数识别方法在预测精度和泛化能力方面的差异。

Method: 对天然橡胶试样进行机械测试，收集全局（力-伸长）和局部（全场位移）测量数据，分别使用传统参数识别方法和EUCLID自动化模型发现流程进行本构关系识别。

Result: 比较了两种方法使用不同数据集（全局vs局部）的预测准确性，分析了实验噪声影响、材料状态空间覆盖度，以及不同先验模型与EUCLID的相对性能。

Conclusion: EUCLID在自动化模型发现和参数识别方面表现出色，能够有效处理实验数据并具有良好的泛化能力到未见几何形状。

Abstract: We assess the performance of EUCLID, Efficient Unsupervised Constitutive Law
Identification and Discovery, a recently proposed framework for automated
discovery of constitutive laws, on experimental data. Mechanical tests are
performed on natural rubber specimens spanning simple to complex geometries,
from which we collect both global, force elongation, and local, full-field
displacement, measurements. Using these data, we obtain constitutive laws via
two routes, the conventional identification of unknown parameters in a priori
selected material models, and EUCLID, which automates model selection and
parameter identification within a unified model-discovery pipeline. We compare
the two methodologies using global versus local data, analyze predictive
accuracy, and examine generalization to unseen geometries. Moreover, we
quantify the experimental noise, investigate the coverage of the material state
space achieved by each approach and discuss the relative performance of
different datasets and different a priori chosen models versus EUCLID.

</details>


### [132] [Breaking the Timescale Barrier: Generative Discovery of Conformational Free-Energy Landscapes and Transition Pathways](https://arxiv.org/abs/2510.24979)
*Chenyu Tang,Mayank Prakash Pandey,Cheng Giuseppe Chen,Alberto Megías,François Dehez,Christophe Chipot*

Main category: physics.comp-ph

TL;DR: Gen-COMPAS是一个生成式承诺概率引导的路径采样框架，无需预定义变量即可重建分子转变路径，成本仅为传统方法的一小部分。


<details>
  <summary>Details</summary>
Motivation: 分子转变（如蛋白质折叠、变构和膜转运）在生物学中至关重要，但难以模拟。其内在稀有性使其超出标准分子动力学范围，而增强采样方法成本高且通常依赖于偏置结果的任意变量。

Method: Gen-COMPAS将生成扩散模型与基于承诺概率的过滤相结合。扩散模型产生物理上真实的中间态，承诺概率过滤精确定位过渡态。从这些中间态进行短的无偏模拟快速获得完整的转变路径集合。

Result: 该方法在纳秒级时间内收敛，而传统方法需要数量级更多的采样。应用于从迷你蛋白到核糖结合蛋白再到线粒体载体的系统，Gen-COMPAS能有效获取承诺概率、过渡态和自由能景观。

Conclusion: Gen-COMPAS将机器学习与分子动力学相结合，为广泛的机制和实际洞察提供了统一框架。

Abstract: Molecular transitions -- such as protein folding, allostery, and membrane
transport -- are central to biology yet remain notoriously difficult to
simulate. Their intrinsic rarity pushes them beyond reach of standard molecular
dynamics, while enhanced-sampling methods are costly and often depend on
arbitrary variables that bias outcomes. We introduce Gen-COMPAS, a generative
committor-guided path sampling framework that reconstructs transition pathways
without predefined variables and at a fraction of the cost. Gen-COMPAS couples
a generative diffusion model, which produces physically realistic
intermediates, with committor-based filtering to pinpoint transition states.
Short unbiased simulations from these intermediates rapidly yield full
transition-path ensembles that converge within nanoseconds, where conventional
methods require orders of magnitude more sampling. Applied to systems from a
miniprotein to a ribose-binding protein to a mitochondrial carrier, Gen-COMPAS
retrieves committors, transition states, and free-energy landscapes
efficiently, uniting machine learning and molecular dynamics for broad
mechanistic and practical insight.

</details>


### [133] [Six-Dimensional Movable Antenna Enabled Wideband THz Communications](https://arxiv.org/abs/2510.25088)
*Wencai Yan,Wanming Hao,Yajun Fan,Yabo Guo,Qingqing Wu,Xingwang Li*

Main category: physics.comp-ph

TL;DR: 提出了一种六维可移动天线(6DMA)支持的宽带太赫兹通信系统，通过优化天线位置和旋转来减轻波束倾斜效应，无需昂贵的真时延设备。


<details>
  <summary>Details</summary>
Motivation: 在宽带太赫兹通信系统中，波束倾斜效应会显著降低系统性能。传统方法使用真时延设备成本高昂，因此需要一种更经济的解决方案。

Method: 采用交替优化算法，将问题分解为三个子问题：混合模拟/数字波束成形、6DMA表面位置优化和旋转优化。使用半定松弛交替最小化和可行梯度下降法求解。

Result: 数值结果表明，所提方案在性能上优于传统的固定位置天线架构。

Conclusion: 6DMA技术通过灵活配置天线位置和旋转，能有效缓解宽带太赫兹系统中的波束倾斜效应，提供了一种成本效益高的解决方案。

Abstract: In this paper, we investigate a six-dimensional movable antenna
(6DMA)-enabled wideband terahertz (THz) communication system with sub-connected
hybrid beamforming architecture at the base station (BS). In particular, the
three-dimensional (3D) position and 3D rotation of each 6DMA surface can be
flexibly reconfigured to mitigate the beam squint effects instead of
introducing costly true-time-delay devices. We first analyze the normalized
array gain in the 6DMA-enabled wideband THz systems based on the beam squint
effects. Then, we formulate a sum-rate maximization problem via jointly
optimizing 3D positions, 3D rotations, and hybrid analog/digital beamforming.
To solve the non-convex problem, an alternating optimization algorithm is
developed that decomposes the original problem into three subproblems, which
are solved alternately. Specifically, given the positions and rotations of 6DMA
surfaces, we first reformulate the objective function and design a semidefinite
relaxation-based alternating minimization scheme to obtain the hybrid
analog/digital beamforming. Then, the positions and rotations of the 6DMA
surfaces are further optimized through a feasible gradient descent procedure.
The final solutions are obtained by repeating the above procedure until
convergence. Numerical results demonstrate the superior performance of the
proposed scheme compared with conventional fixed-position antenna
architectures.

</details>


### [134] [Bayesian MINFLUX localization microscopy](https://arxiv.org/abs/2510.25654)
*Steffen Schultze,Helmut Grubmüller*

Main category: physics.comp-ph

TL;DR: 提出了一种基于贝叶斯方法的MINFLUX显微镜优化方案，相比启发式扫描模式，能在更少光子或曝光下实现纳米级分辨率


<details>
  <summary>Details</summary>
Motivation: 当前MINFLUX显微镜的扫描模式和流程基于启发式方法，可能不是最优的

Method: 开发了严格的贝叶斯方法，通过模拟定位运行进行估计

Result: 该方法可将实现1纳米分辨率所需的光子数减少约四倍

Conclusion: 贝叶斯方法显著提升了MINFLUX显微镜的效率

Abstract: MINFLUX microscopy allows for localization of fluorophores with nanometer
precision using targeted scanning with an illumination profile with a minimum.
However, current scanning patterns and the overall procedure are based on
heuristics, and may therefore be suboptimal. Here we present a rigorous
Bayesian that offers maximal resolutions from either minimal detected photons
or minimal exposures. We estimate using simulated localization runs that this
approach should reduce the number of photons required for 1 nm resolution by a
factor of about four.

</details>


### [135] [Optical excitations in nanographenes from the Bethe-Salpeter equation and time-dependent density functional theory: absorption spectra and spatial descriptors](https://arxiv.org/abs/2510.25658)
*Maximilian Graml,Jan Wilhelm*

Main category: physics.comp-ph

TL;DR: 在CP2K代码中实现了GW-BSE方法，验证了其在有机分子测试集上的准确性，并应用于纳米石墨烯的光学性质研究，展示了该方法在描述纳米结构电子激发方面的优势。


<details>
  <summary>Details</summary>
Motivation: GW-BSE方法是计算分子、纳米结构和晶体材料激发能和光学光谱的成熟方法，但需要在实际代码中实现并验证其准确性，特别是在纳米结构中的应用。

Method: 在CP2K代码中实现GW-BSE形式，使用标准有机分子测试集进行验证，然后研究不同长度纳米石墨烯的光学光谱，并与含不同精确交换分数的TDDFT方法进行比较。

Result: 在有机分子测试集上获得与参考数据极好的一致性，激发能的平均绝对误差低于3 meV；纳米石墨烯的光学光谱与实验吻合良好，最低光学活性激发的大小收敛到约7.6 Å；TDDFT无法同时复现激发大小和光学光谱。

Conclusion: GW-BSE方法能够准确描述纳米结构中的电子激发，而TDDFT方法存在局限性，强调了多体方法在纳米结构电子激发准确描述中的必要性。

Abstract: The GW plus Bethe-Salpeter equation (GW-BSE) formalism is a well-established
approach for calculating excitation energies and optical spectra of molecules,
nanostructures, and crystalline materials. We implement GW-BSE in the CP2K code
and validate the implementation for a standard organic molecular test set,
obtaining excellent agreement with reference data, with a mean absolute error
in excitation energies below 3 meV. We then study optical spectra of
nanographenes of increasing length, showing excellent agreement with
experiment. We further compute the size of the excitation of the lowest
optically active excitation which converges to about 7.6 $\r{A}$ with
increasing length. Comparison with time-dependent density functional theory
using functionals of varying exact-exchange fraction shows that none reproduce
both the size of the excitation and optical spectra of GW-BSE, underscoring the
need for many-body methods for accurate description of electronic excitations
in nanostructures.

</details>
