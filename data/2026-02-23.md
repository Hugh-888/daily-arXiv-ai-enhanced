<div id=toc></div>

# Table of Contents

- [quant-ph](#quant-ph) [Total: 38]
- [gr-qc](#gr-qc) [Total: 13]
- [cs.LG](#cs.LG) [Total: 104]


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [1] [A dimension-independent strict submultiplicativity for the transposition map in diamond norm](https://arxiv.org/abs/2602.17748)
*Hyunho Cha*

Main category: quant-ph

TL;DR: 证明了量子通道的转置映射存在绝对常数α<1，使得对于任意有限维度和量子通道T，有范数不等式成立，具体取α=1/√2


<details>
  <summary>Details</summary>
Motivation: 研究量子通道的转置映射与恒等映射之间的范数关系，寻找保证不等式成立的绝对常数

Method: 使用数学证明方法，通过分析量子通道的转置映射Θ和恒等映射id-T的钻石范数关系，推导出不等式成立的常数

Result: 证明了存在绝对常数α<1使得不等式成立，具体构造了α=1/√2的显式选择

Conclusion: 量子通道的转置映射与恒等映射之间存在确定的范数不等式关系，该结果为量子信息理论提供了重要的数学工具

Abstract: We prove that there exists an absolute constant $α<1$ such that for every finite dimension $d$ and every quantum channel $T$ on $\mathsf{L}(\mathbb{C}^d)$, $\left\|Θ\circ(\mathrm{id}-T)\right\|_\diamond \le α\,\left\|Θ\right\|_\diamond\,\left\|\mathrm{id}-T\right\|_\diamond$, where $Θ$ is the transposition map. In fact we show the explicit choice $α=1/\sqrt{2}$ works.

</details>


### [2] [Topological Boundary Time Crystal Oscillations](https://arxiv.org/abs/2602.17765)
*Dominik Nemeth,Ahsan Nazir,Alessandro Principi,Robert-Jan Slager*

Main category: quant-ph

TL;DR: 边界时间晶体中的集体自旋系统在算子空间中出现拓扑缠绕数，通过将密度算子展开为球张量基，将Lindblad动力学映射到有效局域跳跃问题，解释了BTC中观察到的鲁棒振荡动力学。


<details>
  <summary>Details</summary>
Motivation: 边界时间晶体(BTCs)打破了时间平移对称性，表现出与初始条件无关的长寿命鲁棒振荡。本文旨在理解这种鲁棒振荡动力学的机制，并探索其与拓扑现象的联系。

Method: 将密度算子展开为球张量基，将Lindblad动力学映射到算子空间中的有效局域跳跃问题。集体自由度标记为二维算子空间晶格的格点，识别拓扑障碍，这些障碍强制算子模式在晶格上离域化。

Result: 发现了集体自旋BTCs在算子空间中允许出现拓扑缠绕数。谱离域化为BTCs中观察到的鲁棒振荡动力学提供了自然解释。算子空间中算子权重的非互易输运与拓扑机制结合，导致广泛初始状态类中长时间动力学的普适性。

Conclusion: BTC动力学是拓扑约束的算子空间输运的一种形式，与非厄米趋肤效应有密切联系。这为理解边界时间晶体的鲁棒振荡行为提供了新的拓扑视角。

Abstract: Boundary time crystals (BTCs) break time-translation symmetry and exhibit long-lived, robust oscillations insensitive to initial conditions. We show that collective spin BTCs can admit emergent topological winding numbers in operator space. Expanding the density operator in a spherical tensor basis, we map the Lindblad dynamics onto an effective local hopping problem, where collective degrees of freedom label sites of an emergent two-dimensional operator space lattice and identify topological obstructions that enforce the delocalization of operator modes on the lattice. The resulting spectral delocalization provides a natural explanation for the robust oscillatory dynamics observed in BTCs. When combined with non-reciprocal transport of operator weight across operator space, this mechanism moreover also leads to the universality of long-time dynamics across a broad class of initial states. Our results frame BTC dynamics as a form of topologically constrained operator space transport and suggest a close connection to non-Hermitian skin-effects.

</details>


### [3] [Exact quantum decision diagrams with scaling guarantees for Clifford+$T$ circuits and beyond](https://arxiv.org/abs/2602.17775)
*Arend-Jan Quist,Tim Coopmans,Alfons Laarman*

Main category: quant-ph

TL;DR: 该论文提出了一种使用代数表示替代浮点数的精确量子决策图方法，用于分析Clifford+T门量子电路，并证明了其规模与T门数量呈线性关系，运行时上界为2^t·poly(g,n)。


<details>
  <summary>Details</summary>
Motivation: 浮点误差严重影响了实值和复值决策图的实际实现，特别是在量子计算领域。现有方法缺乏理论缩放保证，在实践中效果有限。需要一种能够避免数值不稳定性、具有理论保证的精确方法。

Method: 1. 手工设计复数代数表示，替代决策图中的浮点系数；2. 基于Clifford+T门电路产生的量子态密度矩阵条目，建立T门数量相关的表征；3. 揭示量子态稳定子零性与决策图宽度之间的关联；4. 实现开源工具验证方法。

Result: 1. 代数表示规模与T门数量呈线性关系，与Clifford门数量无关；2. 决策图运行时和节点数上界为2^t·poly(g,n)；3. 精确方法解决了浮点实现的不准确性问题，且节点数更少；4. 首次为通用门集提供了量子决策图模拟的缩放保证。

Conclusion: 该研究提出了首个具有理论缩放保证的精确量子决策图方法，通过代数表示解决了浮点误差问题，为Clifford+T门量子电路的精确分析提供了有效工具，在量子电路验证和模拟领域具有重要意义。

Abstract: A decision diagram (DD) is a graph-like data structure for homomorphic compression of Boolean and pseudo-Boolean functions. Over the past decades, decision diagrams have been successfully applied to verification, linear algebra, stochastic reasoning, and quantum circuit analysis. Floating-point errors have, however, significantly slowed down practical implementations of real- and complex-valued decision diagrams. In the context of quantum computing, attempts to mitigate this numerical instability have thus far lacked theoretical scaling guarantees and have had only limited success in practice. Here, we focus on the analysis of quantum circuits consisting of Clifford gates and $T$ gates (a common universal gate set). We first hand-craft an algebraic representation for complex numbers, which replace the floating point coefficients in a decision diagram. Then, we prove that the sizes of these algebraic representations are linearly bounded in the number of $T$ gates and qubits, and constant in the number of Clifford gates. Furthermore, we prove that both the runtime and the number of nodes of decision diagrams are upper bounded as $2^t \cdot poly(g, n)$, where $t$ ($g$) is the number of $t$ gates (Clifford gates) and $n$ the number of qubits. Our proofs are based on a $T$-count dependent characterization of the density matrix entries of quantum states produced by circuits with Clifford+$T$ gates, and uncover a connection between a quantum state's stabilizer nullity and its decision diagram width. With an open source implementation, we demonstrate that our exact method resolves the inaccuracies occurring in floating-point-based counterparts and can outperform them due to lower node counts. Our contributions are, to the best of our knowledge, the first scaling guarantees on the runtime of (exact) quantum decision diagram simulation for a universal gate set.

</details>


### [4] [Shortcuts to Adiabaticity via Adaptive Quantum Zeno Measurements](https://arxiv.org/abs/2602.17786)
*Adolfo del Campo*

Main category: quant-ph

TL;DR: 量子芝诺动力学中，通过监测时间依赖投影算子实现非绝热几何连接，为绝热捷径提供统一框架


<details>
  <summary>Details</summary>
Motivation: 研究时间依赖投影算子监测下的量子芝诺动力学，探索如何通过量子测量实现绝热捷径

Method: 从频闪测量协议出发，推导芝诺动力学的有效哈密顿量，包含Kato-Avron哈密顿量形式的非绝热几何连接；扩展到连续量子测量和时间依赖复吸收势的非厄米演化

Result: 有效芝诺哈密顿量包含平行输运的几何连接，在瞬时能量本征基投影测量时简化为反绝热驱动，为绝热捷径提供统一框架

Conclusion: 通过自适应量子芝诺测量实现绝热捷径的统一框架，连接了量子测量、几何相位和量子控制

Abstract: We consider the quantum Zeno dynamics arising from monitoring a time-dependent projector. Starting from a stroboscopic measurement protocol, it is shown that the effective Hamiltonian for Zeno dynamics involves a nonadiabatic geometric connection that takes the form of the Kato-Avron Hamiltonian for parallel transport, stirring the evolution within the time-dependent Zeno subspace. The latter reduces to counterdiabatic driving when projective measurements are performed in the instantaneous energy eigenbasis of the quantum system. The effective Zeno Hamiltonian can also be derived in the context of continuous quantum measurements of a time-dependent observable and the non-Hermitian evolution with a complex absorbing potential varying in time. Our results thus provide a unified framework for realizing shortcuts to adiabaticity via adaptive quantum Zeno measurements.

</details>


### [5] [Manipulating heterogeneous quantum resources over a network](https://arxiv.org/abs/2602.17803)
*Ray Ganardi,Jeongrak Son,Jakub Czartowski,Seok Hyung Lie,Nelly H. Y. Ng*

Main category: quant-ph

TL;DR: 提出统一框架处理分布式量子网络中的复合资源理论，建立局部结构约束下的资源操纵基本界限


<details>
  <summary>Details</summary>
Motivation: 现实量子网络中各方具有异质的局部资源约束，不同资源共存并相互作用，但现有资源理论大多孤立处理单一资源，缺乏分布式环境下的通用理论框架

Method: 开发统一复合量子资源理论框架，制定尊重局部结构的自然公理，从公理推导独立于特定网络特征的资源操纵基本界限

Result: 建立了分布式量子资源操纵的普适基本定律，应用于资源转换、辅助蒸馏等核心任务，引入构建新资源单调量的方法，揭示了量子资源远程认证的新现象

Conclusion: 为跨不同物理平台的分布式量子资源操纵建立了理论基础，解决了异质约束网络中的资源共存与相互作用问题

Abstract: Quantum information processing relies on a variety of resources, including entanglement, coherence, non-Gaussianity, and magic. In realistic settings, protocols run on networks of parties with heterogeneous local resource constraints, so different resources coexist and interact. Yet, resource theories have mostly treated each resource in isolation, and a general theory for manipulation in such distributed settings has been lacking. We develop a unified framework for composite quantum resource theories that describes distributed networks of locally constrained parties. We formulate natural axioms a composite theory should satisfy to respect the local structure, and from these axioms derive fundamental bounds on resource manipulation that hold universally, independent of the particular network characteristics. We apply our results to central operational tasks, including resource conversion and assisted distillation, and introduce new methods to construct new resource monotones from this setup. Our framework further reveals previously unexplored phenomena in the remote certification of quantum resources. Together, these results establish foundational laws for distributed quantum resource manipulation across diverse physical platforms.

</details>


### [6] [Digital Quantum Simulation of the Holstein-Primakoff Transformation on Noisy Qubits](https://arxiv.org/abs/2602.17806)
*Kelvin Yip,Alessandro Monteros,Sahel Ashhab,Lin Tian*

Main category: quant-ph

TL;DR: 该研究实现了基于Holstein-Primakoff变换的玻色子模式数字量子模拟，在超导量子处理器上成功模拟了驱动谐振子和Jaynes-Cummings模型，并系统分析了算法误差与硬件误差的相互作用。


<details>
  <summary>Details</summary>
Motivation: 玻色子系统的量子模拟在可编程量子处理器上受到玻色模式固有的大希尔伯特空间的阻碍。虽然自旋和费米子模型已在数字量子计算机上广泛模拟，但玻色子系统的模拟仍面临挑战。

Method: 采用Holstein-Primakoff变换将玻色子模式映射到量子比特上，在云端超导量子处理器上实现数字量子模拟。具体实现了两个代表性模型：驱动谐振子和Jaynes-Cummings模型。

Result: 成功在量子硬件上实现了玻色子系统的数字量子模拟。系统分析了算法误差（HP映射中有限量子比特数和Trotter步数）与硬件误差（门保真度、退相干、读出误差）的相互作用，确定了最优模拟参数。

Conclusion: 该研究推进了当前可用云端量子处理器上涉及玻色自由度的多体系统数字量子模拟，为扩展到更复杂的自旋-玻色子和多模腔模型提供了框架。

Abstract: Quantum simulation of many-body systems offers a powerful approach to exploring collective quantum dynamics beyond classical computational reach. Although spin and fermionic models have been extensively simulated on digital quantum computers, the simulation of bosonic systems on programmable quantum processors is often hindered by the intrinsically large Hilbert space of bosonic modes. In this work, we study the digital quantum simulation of bosonic modes using the Holstein-Primakoff (HP) transformation and implement this protocol on a cloud-based superconducting quantum processor. Two representative models are realized on quantum hardware: (i) the driven harmonic oscillator and (ii) the Jaynes-Cummings model. Using data obtained from the quantum simulations, we systematically examine the interplay between algorithmic and hardware-induced errors to identify optimal simulation parameters. The dominant algorithmic errors arise from the finite number of qubits used in the HP mapping and the finite number of Trotter steps in the time evolution, while hardware errors mainly originate from gate infidelity, decoherence, and readout errors. This study advances the digital quantum simulation of many-body systems involving bosonic degrees of freedom on currently available cloud quantum processors and provides a framework that can be extended to more complex spin-boson and multimode cavity models.

</details>


### [7] [Quantum superresolution and noise spectroscopy with quantum computing](https://arxiv.org/abs/2602.17862)
*James W. Gardner,Federico Belliardo,Gideon Lee,Tuvia Gefen,Liang Jiang*

Main category: quant-ph

TL;DR: 量子计算可以加速检测未知弱非相干信号，通过量子算法测试量子态的秩、纯度和谱隙，比全态层析更快。


<details>
  <summary>Details</summary>
Motivation: 非相干信号的量子计量学是超分辨和噪声光谱中的典型问题。当信号和噪声不精确已知时，需要更高效的方法来检测弱非相干信号。

Method: 使用弱Schur采样、密度矩阵指数化和量子信号处理等量子算法，测试未知量子态的秩、纯度和谱隙来检测非相干信号。

Result: 这些量子算法比全态层析（其复杂度随希尔伯特空间维度增长）更快，可应用于多种物理场景。

Conclusion: 量子计算为检测未知弱非相干信号提供了加速方法，在多个物理领域具有应用潜力。

Abstract: Quantum metrology of an incoherent signal is a canonical sensing problem related to superresolution and noise spectroscopy. We show that quantum computing can accelerate searches for a weak incoherent signal when the signal and noise are not precisely known. In particular, we consider weak Schur sampling, density matrix exponentiation, and quantum signal processing for testing the rank, purity, and spectral gap of the unknown quantum state to detect the incoherent signal. We show that these algorithms are faster than full-state tomography, which scales with the dimension of the Hilbert space. We apply our results to detecting exoplanets, stochastic gravitational waves, ultralight dark matter, geontropic quantum gravity, and Pauli noise.

</details>


### [8] [Measuring and correcting nanosecond pulse distortions in quantum-dot spin qubits](https://arxiv.org/abs/2602.17899)
*Jiheng Duan,Fernando Torres-Leal,John M. Nichol*

Main category: quant-ph

TL;DR: 利用失谐轴脉冲光谱表征硅双量子点中的基带脉冲失真，通过数字预失真滤波器消除长于1纳秒的脉冲失真，提高单重态-三重态量子比特相干交换振荡的频率稳定性


<details>
  <summary>Details</summary>
Motivation: 半导体量子点中电脉冲失真会限制量子比特控制保真度，但在器件层面难以测量，需要开发可扩展且调谐高效的脉冲失真表征方法

Method: 使用失谐轴脉冲光谱表征硅双量子点中的基带脉冲失真，提取栅极电压脉冲响应，并应用数字预失真滤波器消除长于1纳秒的脉冲失真

Result: 成功减少了单重态-三重态量子比特相干交换振荡的频率啁啾，验证了预失真滤波器在消除脉冲失真方面的有效性

Conclusion: 该方法为量子点自旋量子比特提供了一种可扩展且调谐高效的脉冲失真表征方法，有助于提高量子比特控制保真度

Abstract: Gate-defined semiconductor quantum dots utilize fast electrical control to manipulate spin and charge states of individual electrons. Electrical pulse distortions can limit control fidelities but are difficult to measure at the device level. Here, we use detuning-axis pulsed spectroscopy to characterize baseband pulse distortions in a silicon double quantum-dot. We extract the gate-voltage impulse response and apply a digital pre-distortion filter to eliminate pulse distortions on timescales longer than 1~ns. With the pre-distortion, we reduce the frequency chirp of coherent exchange oscillations in a singlet-triplet qubit. Our results suggest a scalable and tuning-efficient method for characterizing pulse distortions in quantum-dot spin qubits.

</details>


### [9] [Comparison of security mechanisms of Mathematical cipher, Wyner scheme, QKD, and Quantum stream cipher](https://arxiv.org/abs/2602.17933)
*Gikyu Yamamoto,Osamu Hirota*

Main category: quant-ph

TL;DR: 该论文比较了Y-00协议与其他密码技术（数学密码、Wyner方案和QKD）的安全机制，旨在促进不同领域研究者对第三代密码技术的理解。


<details>
  <summary>Details</summary>
Motivation: 新一代全球通信系统需要超高速、低成本和强安全性，但不同密码技术（数学密码、Wyner方案、QKD和Y-00）的安全原理差异很大，导致不同领域研究者难以相互理解。需要对这些技术的安全机制进行比较分析。

Method: 作为首次尝试，该讲座笔记通过比较分析的方法，解释第三代密码技术Y-00协议的安全机制，并将其与其他机制（数学密码、Wyner方案和QKD）的原理进行对比。

Result: 论文提供了Y-00协议与其他密码技术安全原理的对比分析，帮助不同领域的研究者理解这些技术的差异和特点，特别是Y-00作为光学量子领域第三代密码技术的独特安全机制。

Conclusion: 通过比较不同密码技术的安全原理，特别是解释Y-00协议作为第三代密码技术的机制，可以促进跨领域研究者的相互理解，推动全球通信系统安全技术的发展。

Abstract: A new generation of global communications technology has been emerging. These systems, which utilize established device technologies and quantum effect devices, require ultra-high speeds, low cost, and strong security. In recent years, global communication systems have faced various practical security challenges depending on their configurations, and research efforts are underway to address these issues. In particular, the issue of the security of physical layer security from microwave wireless systems to quantum optical communication systems is urgent problem. However, concepts of cryptographic schemes have also been diversifying. Typical examples are mathematical ciphers, the Wyner scheme and QKD. Then, the Y-00 protocol has recently emerged as a third pillar cryptographic technology in the optical quantum domain. These security principles differ significantly from one another. This makes it difficult for different fields to understand each other. At this stage, comparative explanations of the security principles underlying these various cryptographic technologies are likely to promote mutual understanding among researchers across different fields. As the first trial, this lecture note explains the security mechanism of the third pillar (Y-00), comparing it with the principles of other mechanisms.

</details>


### [10] [Distributed Hyperbolic Floquet Codes under Depolarizing and Erasure Noise](https://arxiv.org/abs/2602.17969)
*Aygul Azatovna Galimova*

Main category: quant-ph

TL;DR: 本文提出了一种分布式双曲Floquet量子纠错码的构造方法，通过谱二分法将量子比特分配到不同QPU上，并在多种噪声模型下评估了性能。


<details>
  <summary>Details</summary>
Motivation: 为了扩展量子计算规模，需要将量子比特分布到多个量子处理单元(QPU)上。双曲Floquet码仅使用权重2的测量，是分布式量子纠错码的良好候选方案。

Method: 使用Wythoff万花筒构造和低指数正规子群(LINS)算法，从{8,3}、{10,3}和{12,3}镶嵌构造双曲和半双曲Floquet码，并通过谱二分法将量子比特分布到不同QPU上。

Result: 在四种噪声模型下评估分布式码性能：在去极化噪声下，{8,3}和{10,3}的非局域伪阈值达3.0%，{12,3}为1.75%；相关EM3噪声下伪阈值分别为0.75%、0.75%和0.50%；SDEM3模型下分别为1.75%、1.25%和1.00%；在擦除噪声下，1%局部损失时的阈值分别为35-40%、30-35%和25-30%。

Conclusion: 双曲Floquet码在分布式架构中表现出良好的纠错能力，特别是在擦除噪声下具有较高的阈值，为大规模量子计算提供了有前景的分布式纠错方案。

Abstract: Distributing qubits across quantum processing units (QPUs) connected by shared entanglement enables scaling beyond monolithic architectures. Hyperbolic Floquet codes use only weight-2 measurements and are good candidates for distributed quantum error correcting codes. We construct hyperbolic and semi-hyperbolic Floquet codes from $\{8,3\}$, $\{10,3\}$, and $\{12,3\}$ tessellations via the Wythoff kaleidoscopic construction with the Low-Index Normal Subgroups (LINS) algorithm and distribute them across QPUs via spectral bisection. The $\{10,3\}$ and $\{12,3\}$ families are new to hyperbolic Floquet codes.
  We simulate these distributed codes under four noise models: depolarizing, SDEM3, correlated EM3, and erasure. With depolarizing noise ($p_{\text{local}} = 0.03\%$), fine-grained codes achieve non-local pseudo-thresholds up to 3.0\% for $\{8,3\}$, 3.0\% for $\{10,3\}$, and 1.75\% for $\{12,3\}$. Correlated EM3 yields pseudo-thresholds up to 0.75\% for $\{8,3\}$, 0.75\% for $\{10,3\}$, and 0.50\% for $\{12,3\}$; crossing-based thresholds from same-$k$ families are ${\sim}1.75$--$2.9\%$ across all tessellations. Using the SDEM3 model, fine-grained codes achieve distributed pseudo-thresholds of 1.75\% for $\{8,3\}$, 1.25\% for $\{10,3\}$, and 1.00\% for $\{12,3\}$. Under erasure noise motivated by spin-optical architectures, thresholds at 1\% local loss are 35--40\% for $\{8,3\}$, 30--35\% for $\{10,3\}$, and 25--30\% for $\{12,3\}$.

</details>


### [11] [Recursive Sketched Interpolation: Efficient Hadamard Products of Tensor Trains](https://arxiv.org/abs/2602.17974)
*Zhaonan Meng,Yuehaw Khoo,Jiajia Li,E. Miles Stoudenmire*

Main category: quant-ph

TL;DR: 提出RSI算法，将张量链格式的Hadamard积计算复杂度从O(χ⁴)降低到O(χ³)，通过随机张量链草图与插值分解切片选择相结合


<details>
  <summary>Details</summary>
Motivation: 张量链格式的Hadamard积在非线性微分方程、卷积等应用中至关重要，但传统方法计算复杂度至少为O(χ⁴)，成为实际计算中的严重瓶颈

Method: 结合随机张量链草图和基于插值分解的切片选择，提出递归草图插值（RSI）算法，实现"尺度积"计算

Result: 在各种张量链场景下的基准测试表明，RSI相比传统方法具有更好的可扩展性，同时保持相当的精度

Conclusion: RSI算法将Hadamard积计算复杂度降低到O(χ³)，并可推广到多个张量链的Hadamard积和其他元素级非线性映射，复杂度不增加

Abstract: The Hadamard product of two tensors in the tensor-train (TT) format is a fundamental operation across various applications, such as TT-based function multiplication for nonlinear differential equations or convolutions. However, conventional methods for computing this product typically scale as at least $\mathcal{O}(χ^4)$ with respect to the TT bond dimension (TT-rank) $χ$, creating a severe computational bottleneck in practice. By combining randomized tensor-train sketching with slice selection via interpolative decomposition, we introduce Recursive Sketched Interpolation (RSI), a ``scale product'' algorithm that computes the Hadamard product of TTs at a computational cost of $\mathcal{O}(χ^3)$. Benchmarks across various TT scenarios demonstrate that RSI offers superior scalability compared to traditional methods while maintaining comparable accuracy. We generalize RSI to compute more complex operations, including Hadamard products of multiple TTs and other element-wise nonlinear mappings, without increasing the complexity beyond $\mathcal{O}(χ^3)$.

</details>


### [12] [Enhanced Maximum Independent Set Preparation with Rydberg Atoms Guided by the Spectral Gap](https://arxiv.org/abs/2602.17991)
*Seokho Jeong,Minhyuk Kim*

Main category: quant-ph

TL;DR: 提出ADGLB方法，通过调整激光失谐曲线抑制绝热量子计算中的基态泄漏，提高最大独立集问题的求解概率


<details>
  <summary>Details</summary>
Motivation: 绝热量子计算在求解组合优化问题时，随着系统规模和连接性增加，能隙减小导致基态泄漏，限制了性能

Method: ADGLB方法：基于能隙指导的调度工程方法，修改激光失谐曲线来抑制泄漏，无需额外哈密顿量项或迭代优化循环

Result: 在10原子准一维链上，MIS制备概率显著提高；优化后的调度可直接应用于25和37原子的二维三角晶格；对高难度实例仍有效

Conclusion: 能隙指导的调度工程为中性原子平台上的绝热量子优化提供了可扩展且硬件高效的增强策略

Abstract: Adiabatic quantum computation with Rydberg atoms provides a natural route for solving combinatorial optimization problems such as the maximum independent set (MIS). However, its performance is fundamentally limited by the reduction of the spectral gap with increasing system size and connectivity, which induces population leakage from the ground state during finite-time evolution. Here we introduce the Adjusted Detuning for Ground-Energy Leakage Blockade (ADGLB), a spectral-gap-guided schedule engineering method that modifies the laser detuning profile to suppress leakage without introducing additional Hamiltonian terms or iterative optimization loops. We experimentally benchmark ADGLB on a quasi-one-dimensional chain of $N=10$ atoms, and the MIS preparation probability increases substantially compared with the standard adiabatic schedule. Furthermore, we show that the schedule optimized for smaller instances can be directly applied to larger two-dimensional triangular lattices with $N=25$ and $N=37$. With a small heuristic offset, the method also remains effective for instances with higher hardness parameters. These findings demonstrate that spectral-gap-guided schedule engineering offers a scalable and hardware-efficient strategy for enhancing adiabatic quantum optimization on neutral-atom platforms.

</details>


### [13] [A Tailored Fidelity Estimation and Purification Method for Entangled Quantum Networks](https://arxiv.org/abs/2602.18011)
*Takafumi Oka,Michal Hajdušek,Shota Nagayama,Rodney Van Meter*

Main category: quant-ph

TL;DR: 提出同时进行量子态重构和纠缠纯化的新方法，显著减少量子网络启动所需的Bell对数量，并能同时纠正比特翻转和相位翻转错误。


<details>
  <summary>Details</summary>
Motivation: 现有量子网络启动方法需要大量Bell对进行态重构，且只能纠正单一类型错误，效率低下。需要更高效的方法来减少资源消耗并提高纠错能力。

Method: 开发同时进行量子态重构和纠缠纯化的集成方法，通过统计估计在纯化过程中实时评估保真度，并能同时纠正比特翻转和相位翻转错误。

Result: 新方法仅需约2,841个Bell对就能以99.7%置信度估计保真度在[0.98, 1.00]范围内，相比现有方法所需的10^5个Bell对大幅减少。同时实现了两种错误的同步纠正。

Conclusion: 该方法显著降低了量子网络启动的资源需求，提高了效率，为实际量子网络的时间估计提供了数值基础，支持现实世界量子网络的实际部署。

Abstract: We present a method to conduct both quantum state reconstruction and entanglement purification simultaneously that is advantageous in several respects over previous work in this direction, showing that the number of Bell pairs necessary to boot a quantum network can be significantly reduced compared to an existing method. The existing method requires at least $10^5$ Bell pairs for the state reconstruction phase to estimate that the state is of fidelity $0.99$ within the error range of $10^{-2}$, whereas our approach only requires around $2,841$ to be certain with $99.7\%$ of confidence that the estimated fidelity lies within $[0.99-0.01, 0.99+0.01]$. In addition, in our approach we can start with a lower fidelity Bell pair and purify it multiple times, estimating at the same time the resultant fidelity with guarantee of $99.7\%$ that the fidelity estimate lies within a certain range. Moreover, the existing method cannot correct both bit-flip and phase-flip errors at the same time and can only correct one of these, whereas our approach can correct both bit-flip and phase-flip errors simultaneously. This research produces numerical estimates for the number of Bell pairs actually needed to guarantee a certain threshold fidelity $F$. The research can support the functioning real-world quantum networking by providing the information of the time needed for the bootstrapping of a quantum network to finish.

</details>


### [14] [Separating Non-Interactive Classical Verification of Quantum Computation from Falsifiable Assumptions](https://arxiv.org/abs/2602.18034)
*Mohammed Barhoush,Tomoyuki Morimae,Ryo Nishimaki,Takashi Yamakawa*

Main category: quant-ph

TL;DR: 该论文证明了在量子黑盒归约下，不存在基于任何可证伪假设的非交互式经典验证量子计算协议，这对基于LWE假设的交互式验证方案提出了重要限制。


<details>
  <summary>Details</summary>
Motivation: Mahadev基于LWE假设提出了4轮交互的经典验证量子计算协议，这自然引出一个问题：在普通模型中能否使用更少的轮次（特别是非交互式）？这个问题在密码学中具有重要意义但一直未解决。

Method: 作者证明了不存在从非交互式经典验证量子计算到任何可证伪假设的量子黑盒归约。分离结果基于QMA-QCMA间隙问题的存在性假设，并通过量子酉预言机构造支持这种问题的存在性。

Result: 证明了在量子黑盒归约框架下，不存在基于可证伪假设（包括LWE等几乎所有标准密码学假设）的非交互式经典验证量子计算协议。这是一个强烈的否定性结果。

Conclusion: 该工作表明，在普通模型中实现非交互式经典验证量子计算需要比可证伪假设更强的假设，这为量子密码学的基础理论提供了重要限制，并强调了QMA≠QCMA假设在量子验证中的核心作用。

Abstract: Mahadev [SIAM J. Comput. 2022] introduced the first protocol for classical verification of quantum computation based on the Learning-with-Errors (LWE) assumption, achieving a 4-message interactive scheme. This breakthrough naturally raised the question of whether fewer messages are possible in the plain model. Despite its importance, this question has remained unresolved.
  In this work, we prove that there is no quantum black-box reduction of non-interactive classical verification of quantum computation of $\textsf{QMA}$ to any falsifiable assumption. Here, "non-interactive" means that after an instance-independent setup, the protocol consists of a single message. This constitutes a strong negative result given that falsifiable assumptions cover almost all standard assumptions used in cryptography, including LWE. Our separation holds under the existence of a $\textsf{QMA} \text{-} \textsf{QCMA}$ gap problem. Essentially, these problems require a slightly stronger assumption than $\textsf{QMA}\neq \textsf{QCMA}$. To support the existence of such problems, we present a construction relative to a quantum unitary oracle.

</details>


### [15] [Gaussian Dynamical Quantum State Tomography](https://arxiv.org/abs/2602.18044)
*Hjalmar Rall*

Main category: quant-ph

TL;DR: 提出了一种基于固定单模零差测量和时间控制的动态量子态层析方案，适用于多模玻色高斯态，证明了该方案对几乎所有离散均匀高斯演化和高斯量子动力学半群有效。


<details>
  <summary>Details</summary>
Motivation: 传统量子态层析需要测量信息完备的观测量集合，而动态量子态层析（DQST）提供了一种替代方案：在已知系统动力学和固定观测量的情况下，仅通过控制测量时间即可实现层析。

Method: 使用固定的单模零差测量，仅控制每个独立同分布系统副本的测量时间，对经历高斯演化的多模玻色高斯态进行层析。

Result: 证明了该方案对所有离散均匀高斯演化和高斯量子动力学半群（除零测集外，包括幺正演化）都有效。当已知态为纯态时，所需测量时间更少。

Conclusion: 动态量子态层析方案为高斯态层析提供了一种简化控制要求的方法，仅需控制测量时间即可实现几乎所有高斯演化的层析，特别适用于纯态情况。

Abstract: Standard quantum state tomography assumes sufficient control of a system to measure an informationally complete set of observables. Dynamical quantum state tomography (DQST) presents an alternative: given a system with known dynamics and a single fixed observable, it almost always suffices to control only the time at which each i.i.d. copy of the system is measured. This work presents an analogous scheme for tomography of multi-mode Bosonic Gaussian states undergoing Gaussian evolution, using a fixed single-mode homodyne measurement and only assuming control of the time of measurement. I prove that the scheme enables tomography for all discrete homogenous Gaussian evolutions and Gaussian quantum dynamical semigroups except for a null set which includes unitary evolution. When the state is known to be pure, a smaller number of measurement times is shown to be sufficient.

</details>


### [16] [Pulsed coherent spectroscopy of a quantum emitter in hexagonal Boron Nitride](https://arxiv.org/abs/2602.18096)
*Jake Horder,Hugo Quard,Kenji Watanabe,Takashi Taniguchi,Nathan Coste,Igor Aharonovich*

Main category: quant-ph

TL;DR: 在六方氮化硼中实现了B中心缺陷的相干控制，观测到高达5π的拉比振荡，单光子纯度达93%，非均匀相干时间T_2*=0.60 ns，证明其作为确定性量子发射器的潜力。


<details>
  <summary>Details</summary>
Motivation: 固态系统中的缺陷是实现确定性量子发射器的有前景平台。在众多候选材料中，六方氮化硼中的点缺陷最近显示出特别潜力，需要验证其相干特性以评估实际应用价值。

Method: 使用脉冲共振激发探测单个B中心（零声子线436 nm）的相干性。通过功率依赖的拉比振荡实验实现光学相干控制，并用拉姆齐干涉法测量两能级系统的相干性。

Result: 观测到高达5π的拉比振荡，在π脉冲下单光子纯度达93%。通过拉姆齐干涉测得非均匀相干时间T_2*=0.60 ns，成功实现了光学相干控制。

Conclusion: 六方氮化硼中的B中心被确立为可行的触发式相干量子发射器候选材料，这是将其集成到量子光子平台的重要进展。

Abstract: Defects in solid-state systems constitute a promising platform for the realization of deterministic quantum emitters. Among many candidate materials and emitters, point defects in hexagonal Boron Nitride (hBN) have recently emerged as particularly promising. In this work, we probe the coherence of an individual B center with a zero phonon line at 436 nm, under pulsed resonant excitation. We observe power-dependent Rabi oscillations up to 5π, demonstrating optical coherent control of the transition. We achieve an excellent single photon purity of 93% at π-pulse. Furthermore, we probe the coherence of the two-level system using Ramsey interferometry, revealing an inhomogeneous coherence time of T_2*=0.60 ns. These results establish B centers in hBN as viable candidates for triggered, coherent quantum emitters and represent an important step towards their integration into quantum photonic platforms.

</details>


### [17] [Polariton-polariton coherent coupling in a molecular spin-superconductor chip](https://arxiv.org/abs/2602.18103)
*Carolina del Río,Marcos Rubín-Osanz,David Rodriguez,Sebastián Roca-Jerat,María Carmen Pallarés,J. Alejandro de Sousa,Paweł Pakulski,José Luis García Palacios,Daniel Granados,Dawid Pinkowicz,Núria Crivillers,Anabel Lostao,David Zueco,Alicia Gomez,Fernando Luis*

Main category: quant-ph

TL;DR: 研究人员在超导芯片上实现了远程极化子之间的相干相互作用，通过分子自旋系综作为中介，展示了可扩展量子器件中的远程光子-光子和自旋-自旋关联控制。


<details>
  <summary>Details</summary>
Motivation: 建立相干通信通道是扩展量子器件的关键。本研究旨在通过工程化远程极化子之间的相互作用，实现可扩展量子芯片中远程量子关联的控制和检测。

Method: 设计包含多个谐振器对的超导芯片，每个谐振器对频率略有失谐以实现可寻址性。在谐振器电感上沉积有机自由基分子样品作为自旋系综。在极低温下进行频率相关的微波传输实验，测量不同磁场场景下的极化子频率。

Result: 实验证明：1）当只有一个谐振器含有分子样品时，自旋能远程耦合到空谐振器；2）当两个谐振器都与自旋系综相互作用时，磁场可调谐极化子频率；3）极化子共振时出现避免能级交叉，显示电路介导的相干极化子-极化子相互作用；4）泵浦-探测实验显示一个极化子的激发能被另一个检测到。

Conclusion: 该研究展示了在可扩展模块化芯片中控制和检测远程光子-光子和自旋-自旋关联及纠缠的能力，为构建大规模量子网络提供了重要基础。

Abstract: The ability to establish coherent communication channels is key for scaling up quantum devices. Here, we engineer interactions between distant polaritons, hybrid spin-photon excitations formed at different lumped-element superconducting resonators within a chip. The chip consists of several resonator pairs, slightly detuned in frequency to make them addressable, capacitively coupled within each pair and inductively coupled to a common readout line. They interact locally with samples of PTMr and Tripak$^{-}$ organic free radicals, deposited onto their inductors, which provide model $S = 1/2$, $g \simeq 2$ spin ensembles. Frequency-dependent microwave transmission experiments, performed at very low temperatures, measure polariton frequencies as a function of magnetic field in different scenarios. When only one resonator within a pair hosts a molecular sample, the results evidence that spins couple remotely to the empty LER as well as to the local cavity mode. If both resonators interact with a spin ensemble, the magnetic field tunes the polariton frequencies relative to each other, on account of the different spin-photon interactions at each LER. When polaritons are brought into mutual resonance, an avoided level crossing emerges that gives direct spectroscopic evidence for a coherent polariton-polariton interaction mediated by the circuit. Pump-probe experiments reveal that the excitation of a polariton within a connected pair is felt, thus it can be read out, by the other one. These observations, backed by model calculations, illustrate the control and detection of distant photon-photon and spin-spin correlations and entanglement in a scalable modular chip.

</details>


### [18] [Directional Dynamics of the Non-Hermitian Skin Effect](https://arxiv.org/abs/2602.18106)
*Bin Yi*

Main category: quant-ph

TL;DR: QLIF分析揭示非厄米趋肤效应中信息流的剪刀效应、速度排序和三个时间演化阶段，首次建立静态趋肤局域化与方向性信息动力学之间的定量联系。


<details>
  <summary>Details</summary>
Motivation: 尽管对非厄米趋肤效应的静态性质已有广泛研究，但其动力学后果仍未被充分探索。本文旨在填补这一空白，研究非互易量子系统中的信息传播。

Method: 应用量子梁信息流（QLIF）——一种固有的方向性因果影响度量，到具有非互易跳跃的非厄米Su-Schrieffer-Heeger模型。QLIF直接捕捉非互易系统的方向不对称性特征。

Result: 发现剪刀效应：不对称性随非互易参数γ近似线性变化；对趋肤长度呈非单调依赖，在中等趋肤局域化时达到最优不对称性。速度排序显示NHSE阻碍逆趋肤方向的信息流。观察到三个时间演化阶段：光锥限制的扩散、γ依赖的稳定化和相干振荡。

Conclusion: 首次建立了静态趋肤局域化与方向性信息动力学之间的定量联系，为非互易量子系统中的信息传播提供了新见解。

Abstract: The dynamical consequences of the non-Hermitian skin effect (NHSE) remain largely unexplored despite extensive studies of its static properties. Here we address this gap by applying quantum Liang information flow (QLIF) an inherently directional measure of causal influence to the nonHermitian Su Schrieffer Heeger model with non reciprocal hopping. Unlike symmetric correlation functions, QLIF directly captures the directional asymmetry characteristic of non reciprocal systems. We demonstrate a scissors effect where the asymmetry varies approximately linearly with the non-reciprocity parameter gamma for small gamma, and exhibits non-monotonic dependence on the skin length, with optimal asymmetry at moderate skin localization. The velocity ordering reveals NHSE-induced blocking of information flow against the skin direction. Three distinct temporal regimes emerge: light-cone-bounded spreading, gamma-dependent stabilization, and coherent oscillations. These results establish the first quantitative connection between static skin localization and directional information dynamics, offering new insights into information propagation in non-reciprocal quantum systems.

</details>


### [19] [Flux-Activated Resonant Control of a Bosonic Quantum Memory](https://arxiv.org/abs/2602.18122)
*Fernando Valadares,Aleksandr Dorogov,Tanjung Krisnanda,May Chee Loke,Ni-Ni Huang,Pengtao Song,Yvonne Y. Gao*

Main category: quant-ph

TL;DR: 该论文通过在3D超导腔中集成片上磁通控制架构，实现了对玻色子Fock能级间的高效任意旋转，为高维量子信息处理提供了新工具。


<details>
  <summary>Details</summary>
Motivation: 传统色散区中腔模跃迁近乎简并，限制了单个激发能级的直接寻址能力，增加了工程化门操作的复杂性，需要新的控制方法。

Method: 将片上磁通控制架构与3D超导腔中的长寿命玻色子存储器集成，动态访问共振Jaynes-Cummings相互作用，实现对存储器中任意Fock能级对的高效旋转。

Result: 实现了按需访问JC相互作用，为构建稳健的Fock基qudit和利用高维玻色子元素的丰富动力学提供了多功能工具箱。

Conclusion: 该方法为高维量子信息处理提供了硬件高效的途径，扩展了玻色子电路量子电动力学的应用潜力。

Abstract: Universal control of bosonic degrees of freedom provides a hardware-efficient route for quantum information processing with high-dimensional systems. Bosonic circuit quantum electrodynamics (cQED), which leverages transmon ancillae to coherently control long-lived superconducting cavities, is well suited to this goal. However, the cavity transitions are nearly degenerate in the usual dispersive regime, which limits the direct addressability of individual excitation levels and increases the complexity of engineered gates. Here, we integrate an on-chip flux-control architecture with a long-lived bosonic memory housed in a 3D superconducting cavity to dynamically access resonant Jaynes-Cummings (JC) interactions, and realize efficient arbitrary rotations between any pair of Fock levels in the memory. This on-demand access to JC interactions offers a versatile toolbox for implementing robust Fock-basis qudits and harnessing the rich dynamics of high-dimensional bosonic elements for quantum information processing.

</details>


### [20] [Clock Synchronization with Weakly Correlated Photons](https://arxiv.org/abs/2602.18147)
*Justin Yu Xiang Peh,Darren Ming Zhi Koh,Zifang Xu,Xi Jie Yeo,Peng Kian Tan,Christian Kurtsiefer*

Main category: quant-ph

TL;DR: 使用弱时间相关光子实现晶体时钟同步，在-102dB光信道损耗下达到10ns同步抖动，持续25小时


<details>
  <summary>Details</summary>
Motivation: 时钟同步对于通信和分布式计算至关重要，传统方案依赖脉冲光或光子对的强时间相关性，但需要探索使用弱相关光子实现同步的可能性

Method: 使用相干时间为180ns的束状光源产生的弱时间相关光子，通过相干峰检测实现时钟同步，并建立了低信号条件下相干峰检测成功概率的改进模型

Result: 在对称-102dB光信道损耗下实现了10ns的同步定时抖动，系统稳定运行25小时，证明了弱相关光子同步的可行性

Conclusion: 弱时间相关光子可以成功用于时钟同步，为低信号条件下的同步方案提供了新途径，改进模型能更准确估计低信号条件下的同步性能

Abstract: Clock synchronization is necessary for communication and distributed computing tasks. Previous schemes based on photon timing correlations use pulsed light or photon pairs for their strong timing correlations. In this work, we demonstrate successful synchronization of crystal clocks using weakly time-correlated photons of 180 ns coherence time from a bunched light source. A synchronization timing jitter of 10 ns is achieved over symmetric -102 dB optical channel loss between two parties, over a span of 25 hours. We also present a model that gives better estimates to the coherence peak finding success probabilities under low signal.

</details>


### [21] [High-quality single photons from cavity-enhanced biexciton-to-exciton transition](https://arxiv.org/abs/2602.18153)
*Nils Heinisch,Francesco Salusti,Mark R. Hogg,Timon L. Baltisberger,Malwina A. Marczak,Sascha R. Valentin,Arne Ludwig,Klaus D. Jöns,Richard J. Warburton,Stefan Schumacher*

Main category: quant-ph

TL;DR: 该论文提出了一种利用量子点双激子到激子跃迁的共振腔增强技术，通过双光子共振激发双激子态，实现高纯度、高不可区分性的确定性单光子源，避免了传统方法中激光与发射光子光谱重叠和重激发的问题。


<details>
  <summary>Details</summary>
Motivation: 传统两能级系统的共振激光激发存在两个主要问题：1) 激发激光与发射光子光谱重叠带来技术挑战；2) 发射体重激发限制了单光子纯度。现有解决方案通常以牺牲源效率和增加复杂度为代价。因此需要寻找具有光谱分离的少能级系统方案。

Method: 采用半导体量子点，通过共振和选择性腔增强双激子到激子的跃迁。通过双光子共振激发双激子态，然后通过腔收集发射的单光子。这种方法避免了发射体重激发，并自然实现了激发激光与发射单光子的光谱分离。

Result: 实验结果表明，通过选择性Purcell增强，单光子发射的质量指标（纯度g^(2)(0)和HOM可见度V/不可区分性）与高质量确定性量子点单光子源具有竞争力。这些结果是在没有系统优化或针对性系统工程的情况下实现的。

Conclusion: 该报道的方法为下一代最高质量的量子点基确定性单光子源提供了一条可行的技术路线，通过双激子到激子跃迁的共振腔增强技术，能够同时实现高纯度和高不可区分性，且无需复杂的系统优化。

Abstract: Resonant laser excitation of a two-level system with subsequent single-photon emission can be used to generate single photons with high indistinguishability or Hong-Ou-Mandel (HOM) visibility. However, spectral overlap between excitation laser and emitted photons generally poses significant challenges. Furthermore, emitter re-excitation intrinsically limits achievable single-photon purity. Established solutions mitigate these issues at significant cost to source efficiency and with increased source complexity. This motivates the use of few-level systems with spectral separation of excitation and emission pathways. One option is a three-level cascade. However, without targeted lifetime engineering of emitting states, the cascade naturally limits achievable photon indistinguishability. Here we study a semiconductor quantum dot with resonant and selective cavity-enhancement of biexciton-to-exciton transition. Following resonant two-photon excitation of the biexciton state, we collect the emitted single photon with the cavity. This approach circumvents emitter re-excitation and naturally introduces spectral separation of excitation laser and emitted single photon. Supported by first experimental results, we demonstrate theoretically that with selective Purcell enhancement, the observed quality quantifiers of single-photon emission (purity, equivalently $g^{(2)}(0)$, and HOM visibility $\mathcal{V}$, equivalently indistinguishability) are competitive with respect to high-quality deterministic quantum-dot single-photon sources. This is already achieved without systematic optimization or targeted system engineering, which firmly places the reported approach as a viable route to the next generation of highest-quality quantum-dot based deterministic single-photon sources.

</details>


### [22] [Dispersive Hong-Ou-Mandel Interference with Finite Coincidence Windows](https://arxiv.org/abs/2602.18156)
*T. J. Walstra,A. J. Hasenack,P. W. H. Pinkse,B. Skoric*

Main category: quant-ph

TL;DR: 该研究发现，在Hong-Ou-Mandel干涉实验中，现代时间标记模块的矩形符合窗口会破坏标准的色散抵消条件，恢复对对称群速度色散的敏感性，导致HOM干涉谷出现特征振荡和展宽。


<details>
  <summary>Details</summary>
Motivation: 虽然色散对HOM干涉的影响已被广泛研究，但色散与真实测量设备有限检测窗口之间的相互作用尚未得到充分探索。现代时间标记模块的矩形符合窗口本质上是一种时间滤波器，可能影响色散抵消条件。

Method: 作者推导了II型SPDC过程的解析模型，预测HOM干涉谷形状的变化（特征振荡和展宽）。通过使用ppKTP光源和长达29公里的光纤传输进行实验验证，将实验数据与理论模型进行比较。

Result: 实验数据与理论模型高度一致，证实了窗口诱导振荡的存在，并能够精确提取光纤色散参数。研究发现矩形符合窗口打破了标准色散抵消条件，恢复了对对称群速度色散的敏感性。

Conclusion: 这些发现强调了在设计和表征色散量子通信链路时，必须考虑有限时间分辨率的影响。矩形符合窗口会显著改变HOM干涉特性，这对量子信息处理中的光子不可区分性评估具有重要意义。

Abstract: Hong-Ou-Mandel (HOM) interference is a fundamental tool for assessing photon indistinguishability in quantum information processing. While the effect of chromatic dispersion on HOM interference has been widely studied, the interplay between dispersion and the finite detection window of realistic measurement devices remains under-explored. In this work, we demonstrate that the rectangular coincidence window inherent to modern time-tagging modules, which effectively acts as a temporal filter, breaks the standard dispersion cancellation condition and restores sensitivity to symmetric group velocity dispersion. We derive an analytical model for type-II SPDC processes that predicts a modification of the HOM dip shape, specifically the emergence of characteristic oscillations and dip broadening. We experimentally validate this theoretical framework using a ppKTP source and transmission through optical fibers of lengths up to 29 km. The experimental data show excellent agreement with the model, confirming the presence of window-induced oscillations and allowing for the precise extraction of the fiber dispersion parameter. These findings underscore the importance of accounting for finite timing resolution in the design and characterization of dispersive quantum communication links.

</details>


### [23] [Experimental realization of a photonic weighted graph state for quantum metrology](https://arxiv.org/abs/2602.18177)
*Unathi Skosana,Byron Alexander,Changhyoup Lee,Mark Tame*

Main category: quant-ph

TL;DR: 实验实现了权重可调的光子双量子比特加权图态，用于量子增强相位传感，证明远低于最大纠缠态所需的纠缠量即可实现量子优势


<details>
  <summary>Details</summary>
Motivation: 传统量子计量学依赖最大纠缠态，但这些态在实际实验中难以产生和维持。需要探索是否可以使用更少的纠缠资源实现量子优势

Method: 实验实现光子双量子比特加权图态，其图权重可任意调节。将该态作为量子增强相位传感的资源，在不同局部测量基下研究其最小估计方差随图权重变化

Result: 实验与理论预测高度一致，观察到在图权重远低于最大纠缠极限时，精度仍超过经典可达到的精度极限，证实实现量子优势所需的纠缠量显著减少

Conclusion: 首次在线性光学中实现了图权重可调的加权图态实验，证明较少纠缠即可实现量子计量优势，预期在片上光子平台可实现更可扩展的版本

Abstract: Quantum metrology seeks to push the boundaries of measurement precision by harnessing quantum phenomena. Conventional methods often rely on maximally entangled resources, with states that are usually challenging to produce and sustain in practical setups. Here, we show that the maximally entangled constraint can be lifted by experimentally realizing a photonic two-qubit weighted graph state with an arbitrarily tunable graph weight. We use the generated state as a resource for quantum-enhanced phase sensing. We experimentally characterize the state and study its minimum estimator variance for two distinct local measurement bases as the graph weight varies from the maximally entangled to weakly entangled limit. We find excellent quantitative agreement with theoretical predictions, and observe a gain in precision beyond the classically attainable precision limit for graph weights substantially below the maximally entangled limit. This confirms that considerably less entanglement is required to achieve a quantum advantage. Albeit non-scalable in our test setup, this work represents the first experimental realization of weighted graph states with a tunable graph weight using linear optics. We expect more scalable versions of the model to be possible in an on-chip photonic platform.

</details>


### [24] [High-Fidelity Teleportation of Continuous-Variable Quantum States Via Non-Ideal Qutrit Entangled Resources](https://arxiv.org/abs/2602.18180)
*Fatemeh Taghipoor,Mojtaba Golshani,Mostafa Motamedifar,Khatereh Jafari*

Main category: quant-ph

TL;DR: 提出使用纠缠三能级系统（qutrit）资源进行连续变量量子隐形传态，在理想和噪声条件下都能实现高保真度传输


<details>
  <summary>Details</summary>
Motivation: 传统的基于双模压缩真空态的连续变量量子隐形传态方案无法实现接近1的保真度，需要寻找替代方案来克服这一限制

Method: 采用纠缠三能级系统（qutrit）资源进行连续变量量子隐形传态，并在实际噪声条件下进行研究

Result: 提出的方案在理想和噪声条件下都表现良好，能够以合理的成功概率实现高保真度隐形传态

Conclusion: 使用纠缠三能级系统资源是克服传统连续变量量子隐形传态保真度限制的有效方法，在实际噪声环境下仍能保持良好性能

Abstract: Achieving near-unity fidelity in conventional continuous-variable quantum teleportation schemes based on two-mode squeezed vacuum states is fundamentally unattainable. To overcome this limitation, alternative approaches utilizing ensembles of two-dimensional entangled qubits have been proposed. In this work, we investigate continuous-variable quantum teleportation employing entangled qutrit resources under realistic noise effects. The results demonstrate that the proposed scheme performs well in both ideal and noisy conditions, enabling high-fidelity teleportation with a reasonable success probability.

</details>


### [25] [Geometry-Controlled Work Extraction in a Non-Markovian Quantum Battery](https://arxiv.org/abs/2602.18192)
*Maryam Hadipour,Soroush Haseli*

Main category: quant-ph

TL;DR: 研究非马尔可夫量子电池中空间几何结构对能量存储和功提取的影响，通过两个二能级系统在结构化波导环境中的模型，探索几何相位依赖的集体干涉效应


<details>
  <summary>Details</summary>
Motivation: 探索量子电池系统中空间几何结构如何影响能量存储和功提取性能，特别是在非马尔可夫环境中几何相位对集体干涉效应的调制作用

Method: 构建包含两个相同二能级系统的模型，嵌入结构化波导环境中，一个量子比特作为充电器，另一个作为电池，通过调节量子比特之间的相对距离引入几何依赖相位

Result: 量子比特之间的相对分离引入了几何依赖相位，该相位控制集体干涉效应并调制能量传输过程，表明空间几何结构在非马尔可夫量子电池中起关键作用

Conclusion: 空间几何结构通过几何依赖相位显著影响非马尔可夫量子电池的性能，为优化量子能量存储系统提供了新的设计维度

Abstract: We investigate the role of spatial geometry in controlling energy storage and work extraction in a non-Markovian quantum battery. The model consists of two identical two-level systems embedded in a structured waveguide environment, where one qubit acts as the charger and the other as the battery. The relative separation between the qubits introduces a geometry-dependent phase that governs collective interference effects and modulates.

</details>


### [26] [Higher-order spatial photon interference versus dipole blockade effect](https://arxiv.org/abs/2602.18269)
*Arthur Rotari,Mihai A. Macovei*

Main category: quant-ph

TL;DR: 研究三个偶极-偶极耦合的二能级发射体在热环境中产生亚泊松光子统计的量子动力学


<details>
  <summary>Details</summary>
Motivation: 探索对称排列的量子发射体在热环境中如何产生非经典光子统计特性，特别是亚泊松光子流

Method: 分析三个偶极-偶极耦合的二能级发射体（等边三角形排列）与热环境相互作用的稳态量子动力学，解析计算三原子合作态布居以及二阶和三阶空间光子关联函数

Result: 系统在非相干激发下自发产生具有亚泊松光子统计的单光子流；量子光子特性源于对称排列发射体与热环境的相互作用性质；较大原子间距时效应源于高阶空间干涉现象；可观测亚波长干涉条纹

Conclusion: 对称排列的量子发射体与热环境相互作用能够产生非经典光子统计，这种现象不同于偶极-偶极阻塞机制，而是源于系统与热库的相互作用性质和高阶空间干涉效应

Abstract: The steady-state quantum dynamics of three dipole-dipole coupled two-level emitters, fixed at the vertices of an equilateral triangle, and interacting via the environmental thermostat is investigated. We have analytically obtained the populations of the involved three-atom cooperative states as well as of the second- and third-order spatial photon correlation functions of the light scattered by the few-qubit sample. As a consequence, we have demonstrated that this incoherently excited system spontaneously generates streams of single photons possessing sub-Poissonian photon statistics. In analogy to the dipole-dipole blockade, one may expect that at smaller inter particle distances, compared to the photon emission wavelength, the reported phenomenon has the same origin. However, we have shown that the quantum photon features are due to the interaction's nature of the few symmetrically arranged two-level emitters with the surrounding thermal reservoir. Respectively, at larger atomic intervals the effect occurs because of high-order spatial interference phenomena. Sub-wavelength interference fringes can be observed too, via measurements of spatial higher-order photon correlation functions.

</details>


### [27] [Impossibility of Refrigeration and Engine Operation in Minimal Qubit Repeated-Interaction Models](https://arxiv.org/abs/2602.18300)
*Gabrielle Barsky-Giles,Alessandro Prositto,Matthew Gerry,Dvira Segal*

Main category: quant-ph

TL;DR: 该论文研究了在重复相互作用框架下量子比特作为量子热装置的操作，分析了两种最小模型，证明了量子制冷的不可能性定理，并建立了量子比特热机的基本限制。


<details>
  <summary>Details</summary>
Motivation: 研究量子比特在强系统-浴耦合和有限相互作用时间下作为量子热装置的操作，探索量子热机的基本限制和实现功能量子热装置所需的条件。

Method: 采用重复相互作用框架，分析两种最小模型：交替耦合模型（量子比特依次与热浴和冷浴相互作用）和同时耦合模型（两个浴同时与量子比特相互作用）。对交替模型获得任意耦合强度和碰撞时间的精确解析解，对同时模型进行微扰分析。

Result: 证明了量子制冷的不可能性定理；虽然可以在单个系统-浴接触处局部产生功，但一个周期内的总功总是非正的，排除了发动机操作；在纯热传导情况下，热流表现出非单调的翻转行为；同时耦合模型在短碰撞时间极限下再现了交替模型的稳态行为。

Conclusion: 该研究建立了在马尔可夫重复相互作用下操作的量子比特热机的基本限制，强调了需要更丰富的模型来实现功能量子热装置。

Abstract: We investigate the operation of a qubit as a quantum thermal device within the repeated interaction framework, allowing for strong system-bath coupling and finite interaction times. We analyze two minimal models: an alternating-coupling setup, in which the qubit sequentially interacts with hot and cold baths, and a simultaneous-coupling setup, where both baths interact with the qubit during each collision. For the alternating model, we obtain an exact analytical solution for the limit-cycle state, valid for arbitrary coupling strengths and collision durations. Using this solution, we rigorously prove a no-go theorem for quantum refrigeration. We further demonstrate that, although work can be generated locally at individual system-bath contacts, the total work over a cycle is always nonpositive, precluding engine operation. In the absence of work, the model describes pure heat conduction, for which we derive a closed-form expression for the heat current and show that it exhibits a nonmonotonic turnover behavior. The simultaneous-coupling model is analyzed perturbatively. In the short-collision-time limit, it reproduces the same steady-state behavior as the alternating model, reinforcing the generality of the constraints identified. Our results establish fundamental limitations on qubit-based quantum thermal machines operating under Markovian repeated interactions and highlight the need for enriched models to realize functional quantum thermal devices.

</details>


### [28] [Instability as a Quantum Resource](https://arxiv.org/abs/2602.18323)
*Goni Yoeli,Gilad Gour*

Main category: quant-ph

TL;DR: 该论文提出"不稳定性"作为量子资源理论的基本框架，将相干性、非热性和非均匀性统一为其子资源，建立了普适的第二定律


<details>
  <summary>Details</summary>
Motivation: 现有量子资源理论（如相干性、非热性等）各自独立发展，缺乏统一框架。本文旨在提出一个基础性的量子资源理论——不稳定性，将这些看似不同的资源统一起来，揭示它们共同的本质

Method: 通过公理化方法将不稳定性定义为衰变物理系统中的瞬态信息。指定衰变机制（如退相干、热化）可恢复熟悉的资源。计算单次蒸馏产率和稀释成本，确定极值加性单调量，并在渐近区域建立转换率

Result: 证明了所有转换率都由单一加性单调量决定，建立了不稳定性理论的普适第二定律。将相干性、非热性和非均匀性统一为不稳定性的具体表现形式

Conclusion: 不稳定性提供了一个统一的量子资源理论框架，揭示了不同量子资源之间的深层联系，并建立了普适的第二定律，为量子信息处理和热力学提供了新的理论基础

Abstract: We consolidate coherence, athermality, and nonuniformity as sub-resources within an underlying quantum resource theory: instability. We formulate instability axiomatically as the transient information within a decaying physical system. Specifying a decay mechanism (e.g., dephasing, thermalization) recovers these familiar resources as specific manifestations of instability. We compute the one-shot distillation yield and dilution cost in various operational paradigms, and use them to pin down the extremal additive monotones. In the asymptotic regime, we show that all conversion rates are governed by a single additive monotone, and thereby we establish a universal second law for instability.

</details>


### [29] [Universal Protection of Quantum States from Decoherence](https://arxiv.org/abs/2602.18327)
*Francesco Atzori,Salvatore Virzì,Francesco Devecchi,Domenico Abbondandolo,Alessio Avella,Fabrizio Piacentini,Marco Gramegna,Ivo Pietro Degiovanni,Marco Genovese*

Main category: quant-ph

TL;DR: 提出一种与状态和动力学无关的量子保护协议，通过将系统嵌入更大的希尔伯特空间，将量子信息临时交换到无退相干辅助自由度，实现未知量子态的通用保护。


<details>
  <summary>Details</summary>
Motivation: 量子相干性的脆弱性限制了量子技术的可扩展性，环境相互作用会导致退相干并迅速降解量子特性。现有的量子芝诺效应保护方法需要预先知道量子态信息，严重限制了其适用性。

Method: 引入一种与状态和动力学无关的保护协议，将系统嵌入更大的希尔伯特空间，将量子信息从其原始自由度临时交换到无退相干的辅助自由度，从而保护量子相干性。

Result: 在量子光学平台上实验验证了该协议，证明了对任意偏振量子比特在退相干条件下的相干性和纯度的稳健保护，实现了未知量子态的通用保护。

Conclusion: 该工作提出了一种通用的量子保护方法，不需要预先知道量子态信息，为量子技术的可扩展性提供了重要解决方案。

Abstract: The fragility of quantum coherence fundamentally limits the scalability of quantum technologies, as unavoidable environmental interactions induce decoherence and rapidly degrade quantum properties. The Quantum Zeno Effect offers a powerful route to suppress quantum evolution and protect coherence through frequent measurements, irrespective of the underlying dynamics. However, existing implementations require prior knowledge of the quantum state, severely restricting their applicability. Here we introduce a state- and dynamics-independent protection protocol embedding the system in a larger Hilbert space, temporarily swapping the quantum information from its original degree of freedom to a decoherence-free ancillary one. We experimentally validate the protocol on a quantum optical platform, demonstrating robust preservation of coherence and purity for arbitrary polarization qubits under decoherence, thereby enabling the universal safeguarding of unknown quantum states.

</details>


### [30] [A Fine-Grained and Efficient Reliability Analysis Framework for Noisy Quantum Circuits](https://arxiv.org/abs/2602.18347)
*Jindi Wu,Tianjie Hu,Qun Li*

Main category: quant-ph

TL;DR: 提出一个细粒度、可扩展、可解释的框架，用于高效准确评估噪声量子电路的可靠性，通过噪声代理电路和代理保真度指标实现执行无关的可靠性估计。


<details>
  <summary>Details</summary>
Motivation: 当前量子硬件存在多种噪声机制，其复合效应使得准确高效的可靠性评估变得困难。虽然状态保真度是最可靠的指标，但实验和计算成本过高；其他替代指标要么准确性不足，要么缺乏通用性或可解释性。

Method: 提出噪声代理电路（NPC）框架，移除所有逻辑操作但保留完整的噪声通道序列，抽象累积噪声效应。基于NPC定义代理保真度指标，量化比特级和电路级可靠性。开发分析算法估计退极化、热弛豫和读出误差通道下的代理保真度。

Result: 实验结果显示，该方法能准确估计电路保真度，在不同电路和设备上的平均绝对差异（AAD）范围为0.031到0.069，实现了保真度级别的可靠性估计，同时保持执行无关、可扩展和可解释的特点。

Conclusion: 该框架解决了噪声量子电路可靠性评估的挑战，提供了一种准确、高效、可解释的评估方法，为量子算法在噪声量子设备上的实现提供了重要工具。

Abstract: Evaluating the reliability of noisy quantum circuits is essential for implementing quantum algorithms on noisy quantum devices. However, current quantum hardware exhibits diverse noise mechanisms whose compounded effects make accurate and efficient reliability evaluation challenging. While state fidelity is the most faithful indicator of circuit reliability, it is experimentally and computationally prohibitive to obtain. Alternative metrics, although easier to compute, often fail to accurately reflect circuit reliability, lack universality across circuit types, or offer limited interpretability. To address these challenges, we propose a fine-grained, scalable, and interpretable framework for efficient and accurate reliability evaluation of noisy quantum circuits. Our approach performs a state-independent analysis to model how circuit reliability progressively degrades during execution. We introduce the Noise Proxy Circuit (NPC), which removes all logical operations while preserving the complete sequence of noise channels, thereby providing an abstraction of cumulative noise effects. Based on the NPC, we define Proxy Fidelity, a reliability metric that quantifies both qubit-level and circuit-level reliability. We further develop an analytical algorithm to estimate Proxy Fidelity under depolarizing, thermal relaxation, and readout error channels. The proposed framework achieves fidelity-level reliability estimation while remaining execution-free, scalable, and interpretable. Experimental results show that our method accurately estimates circuit fidelity, with an average absolute difference (AAD) ranging from 0.031 to 0.069 across diverse circuits and devices.

</details>


### [31] [Quantum-enhanced satellite image classification](https://arxiv.org/abs/2602.18350)
*Qi Zhang,Anton Simen,Carlos Flores-Garrigós,Gabriel Alvarado Barrios,Paolo A. Erdman,Enrique Solano,Aaron C. Kemp,Vincent Beltrani,Vedangi Pathak,Hamed Mohammadbagherpoor*

Main category: quant-ph

TL;DR: 量子特征提取方法应用于空间图像多分类，相比经典方法准确率提升2-3%，达到87%


<details>
  <summary>Details</summary>
Motivation: 探索量子计算在空间应用（如卫星成像和遥感）等高风险数据驱动领域的实际潜力，提升图像分类性能

Method: 利用多体自旋哈密顿量动力学生成表达性量子特征，结合经典处理形成量子-经典混合方法，在IBM量子处理器上实现

Result: 经典ResNet50基线准确率83%，迁移学习提升至84%，量子-经典混合方法达到87%准确率，实现2-3%的绝对提升

Conclusion: 量子特征提取方法能显著提升图像分类性能，展示了当前和近期量子处理器在现实机器学习任务中的实际应用潜力

Abstract: We demonstrate the application of a quantum feature extraction method to enhance multi-class image classification for space applications. By harnessing the dynamics of many-body spin Hamiltonians, the method generates expressive quantum features that, when combined with classical processing, lead to quantum-enhanced classification accuracy. Using a strong and well-established ResNet50 baseline, we achieved a maximum classical accuracy of 83%, which can be improved to 84% with a transfer learning approach. In contrast, applying our quantum-classical method the performance is increased to 87% accuracy, demonstrating a clear and reproducible improvement over robust classical approaches. Implemented on several of IBM's quantum processors, our hybrid quantum-classical approach delivers consistent gains of 2-3% in absolute accuracy. These results highlight the practical potential of current and near-term quantum processors in high-stakes, data-driven domains such as satellite imaging and remote sensing, while suggesting broader applicability in real-world machine learning tasks.

</details>


### [32] [Quantum-enhanced phase sensitivity in an all-fiber Mach-Zehnder interferometer](https://arxiv.org/abs/2602.18354)
*Romain Dalidet,Anthony Martin,Gregory Sauder,Sébastien Tanzilli,Laurent Labonté*

Main category: quant-ph

TL;DR: 实验展示了在光纤马赫-曾德尔干涉仪中实现10%量子优势的相位估计，无需后选择，使用能量-时间纠缠替代偏振纠缠


<details>
  <summary>Details</summary>
Motivation: 量子光子学在光学相位估计方面取得进展，但固有损耗阻碍了无条件超灵敏度的实现。研究旨在展示在相同资源下超越任何经典对应物的量子优势，推动紧凑、免对准量子干涉仪在实际传感中的应用

Method: 使用全光纤马赫-曾德尔型干涉仪，工作在电信波长，无后选择。将偏振纠缠光子对转换为更适合光纤传感器的能量-时间纠缠，通过费舍尔信息分析考虑所有系统缺陷（包括非对称损耗和探测器效率）

Result: 实验测量到10%的量子优势，证明了紧凑、免对准量子干涉仪在实际传感应用中的实用性

Conclusion: 该研究成功展示了在光纤量子干涉仪中实现量子优势，为实际量子传感应用提供了实用方案，突出了能量-时间纠缠在可扩展光纤传感器中的优势

Abstract: Recent advances in quantum photonics have enabled increasingly robust protocols in optical phase estimation, achieving precisions beyond the standard quantum limit and approaching the Heisenberg limit. While intrinsic losses hinder the realization of unconditional super-sensitivity, reaching quantum advantage, defined as sensitivity surpassing that of any classical counterpart with identical resources, remains achievable. Here we experimentally demonstrate such an advantage using a fully fibered Mach-Zehnder-type interferometer operating at telecom wavelengths, free of post-selection. The scheme relies on the conversion of polarization-entangled photon pairs, a degree of freedom commonly favored for experimental convenience, into energy-time entanglement, which is particularly well suited for scalable fiber-based sensors. All system imperfections, including asymmetric losses and detector inefficiencies, are accounted for in the Fisher information analysis, yielding a measured quantum advantage of 10%. This result highlights the practicality of compact, alignment-free quantum interferometers for real-world sensing applications.

</details>


### [33] [Improving Single Excitation Fidelity in Rydberg Superatoms for Efficient Single Photon Emission](https://arxiv.org/abs/2602.18363)
*Vidisha Aggarwal,Boxi Li,Eloisa Cuestas,Tommaso Calarco,Robert Zeier,Alexei Ourjoumtsev,Felix Motzoi*

Main category: quant-ph

TL;DR: 将超导量子比特的DRAG技术应用于原子系统，通过优化光脉冲形状抑制双激发，将单激发概率从77%提升至91.9%，接近退相干极限


<details>
  <summary>Details</summary>
Motivation: 在里德堡原子系综与光学腔耦合的单光子源中，不完美的里德堡阻塞会导致双激发，降低光子不可区分性，需要高保真度地制备集体单激发

Method: 将超导量子比特中开发的DRAG（导数去除绝热门）技术适配到原子平台，通过分析建模和数值优化来整形光脉冲，抑制双激发

Result: DRAG相比传统正弦平方脉冲有显著改进，通过优化脉冲持续时间和系综尺寸，将单激发概率从77%提升至91.9%，接近退相干设定的基本极限

Conclusion: DRAG接近最优控制极限，同时保持平滑、实验可行的脉冲形状，证明了DRAG在高保真单光子源中的有效性和跨平台适应性

Abstract: Deterministic single photon emission from a Rydberg ensemble coupled to an optical cavity requires high-fidelity preparation of collective single excitations. In such a setup imperfect Rydberg blockade can lead to unwanted double excitations, which degrade photon indistinguishability. In this work we adapt the Derivative Removal by Adiabatic Gate (DRAG) technique, originally developed for superconducting qubits, to shape optical pulses that suppress double excitations in this atomic platform. By combining analytical modeling with numerical optimization, DRAG provides an improvement over conventional sine-squared pulses. Further optimization of pulse duration and atomic ensemble size identifies a parameter regime, distinct from that used in [Nature Photonics 17, 688 (2023)], that enhances the single excitation probability from the previous theoretical benchmark of 77% to 91.9%, approaching the fundamental limits set by decoherence in the system. Benchmarking against GRAPE (Gradient Ascent Pulse Engineering) confirms that DRAG operates close to the optimal control limit, while maintaining smooth, experimentally feasible pulse shapes. These results demonstrate the effectiveness and cross platform adaptability of DRAG for a high-fidelity single photon source.

</details>


### [34] [Towards scalable multi-qubit optimal control via interaction decomposition in the diagonal frame](https://arxiv.org/abs/2602.18375)
*Bora Baran,Tommaso Calarco,Matthias M. Mueller,Felix Motzoi*

Main category: quant-ph

TL;DR: 提出一种n量子比特控制目标的一般化表述方法，通过在计算基态的对角相位图中表示任意幺正变换，将成本函数复杂度二次降低，并利用相位不变量确定性地量化多量子比特相互作用。


<details>
  <summary>Details</summary>
Motivation: 传统量子控制方法需要处理完整的幺正矩阵，复杂度随量子比特数指数增长。本文旨在通过在对角框架中指定控制目标，显著降低优化复杂度，同时能够精确表征多量子比特相互作用。

Method: 将任意n量子比特幺正变换表示为计算基态上的对角相位图，利用离散导数算子解析构造支持选择性相位不变量，形成坐标系统来表述任意多量子比特相互作用的控制目标，无需在优化过程中进行对角化逆运算。

Result: 成功合成了两种真正的三方纠缠门（对角和非对角），通过单个微波脉冲在数值模拟的室温氮空位中心三量子比特核自旋寄存器中实现，操作时间约1微秒，比现有最快NV基多量子比特纠缠器快10-100倍。

Conclusion: 提出的对角相位框架显著降低了量子控制优化的复杂度，同时能够精确表征和合成任意多量子比特相互作用，为高效量子门实现提供了新方法，在氮空位系统中展示了显著的性能提升。

Abstract: In this work, we introduce a general n-qubit formulation of control objectives that allows a control target to be specified in a diagonal frame, so that only the diagonal entries must be characterized, thus quadratically reducing the complexity of the cost functional in constrast to a full target matrix. We do so by representing any n-qubit unitary transformation as a diagonal phase map on the computational basis states, as they are naturally diagonalizable by unitarity. By using discrete derivative operators to analytically construct support-selective phase invariants, we enable to deterministically isolate and quantify any multi-qubit interactions encoded in the phase map. These phase invariants form a coordinate system for the formulation of specific control targets in terms of arbitrary desired multi-qubit interactions, without having to invert the diagonalization during the optimizatiion, solely relying on the experimentally accesible diagonal phases. To illustrate the framework, we synthesize two genuinely tripartite entangling gates, both, diagonal and non-diagonal. These are obtained with a single shaped microwave pulse, for a numerically simulated room-temperature nitrogen-vacancy center with a three qubit nuclear spin register, with durations of about a microsecond. These results represent a factor 10-100 reduction in operation time compared with the fastest existing NV-based entanglers that act on more than two qubits at once.

</details>


### [35] [Theory and interpretability of Quantum Extreme Learning Machines: a Pauli-transfer matrix approach](https://arxiv.org/abs/2602.18377)
*Markus Gross,Hans-Martin Rieser*

Main category: quant-ph

TL;DR: 该论文使用泡利转移矩阵形式理论分析了量子极限学习机，揭示了编码决定可用特征集，量子通道线性变换这些特征，优化QELM可转化为解码问题，并展示了QELM学习非线性动力系统时能近似底层流映射。


<details>
  <summary>Details</summary>
Motivation: 量子储层计算机作为量子机器学习的有前景方法，利用量子系统自然动力学进行数据处理且训练简单。然而，需要理论框架来分析编码、储层动力学和测量操作对量子极限学习机性能的影响。

Method: 采用泡利转移矩阵形式理论分析连续时间储层动力学的n-qubit量子极限学习机，研究编码、储层动力学和测量操作（包括时间复用）对性能的影响。

Result: PTM形式明确显示编码决定了QELM可用的完整（非线性）特征集，量子通道在测量操作探测前线性变换这些特征。优化QELM可转化为解码问题，通过塑造通道诱导变换使任务相关特征对回归器可用。

Conclusion: PTM形式能识别QELM的经典表示，从而指导其设计以满足特定训练目标。在非线性动力系统学习应用中，QELM能学习底层流映射的代理近似，为量子机器学习提供理论指导框架。

Abstract: Quantum reservoir computers (QRCs) have emerged as a promising approach to quantum machine learning, since they utilize the natural dynamics of quantum systems for data processing and are simple to train. Here, we consider n-qubit quantum extreme learning machines (QELMs) with continuous-time reservoir dynamics. QELMs are memoryless QRCs capable of various ML tasks, including image classification and time series forecasting. We apply the Pauli transfer matrix (PTM) formalism to theoretically analyze the influence of encoding, reservoir dynamics, and measurement operations, including temporal multiplexing, on the QELM performance. This formalism makes explicit that the encoding determines the complete set of (nonlinear) features available to the QELM, while the quantum channels linearly transform these features before they are probed by the chosen measurement operators. Optimizing a QELM can therefore be cast as a decoding problem in which one shapes the channel-induced transformations such that task-relevant features become available to the regressor. The PTM formalism allows one to identify the classical representation of a QELM and thereby guide its design towards a given training objective. As a specific application, we focus on learning nonlinear dynamical systems and show that a QELM trained on such trajectories learns a surrogate-approximation to the underlying flow map.

</details>


### [36] [Bell-GHZ nonclassicality of many-observer interwoven frustrated down conversions](https://arxiv.org/abs/2602.18381)
*Marek Żukowski,Paweł Cieśliński,Marcin Markiewicz,Konrad Schlichtholz*

Main category: quant-ph

TL;DR: 该研究扩展了受挫下转换干涉方案至多个测量站，证明了WWWZB不等式的违反，并提出了GHZ/Hardy型论证，揭示了该干涉过程的非经典特性。


<details>
  <summary>Details</summary>
Motivation: 受挫下转换干涉过程能产生基于路径同一性的干涉效应，先前研究已证明其在双观测者配置中能违反标准Bell不等式。本研究旨在将这种干涉方案扩展到多个测量站，探索其揭示更广泛非经典现象的能力。

Method: 扩展了交织受挫PDC干涉方案，从两个测量站扩展到多个测量站。通过精心安排信号和闲频模式在多个源晶体和测量晶体之间的重叠路径，创建了多站干涉配置。使用合适的测量设置来测试WWWZB不等式。

Result: 成功证明了该干涉方案违反WWWZB不等式，表明其非经典特性。同时提出了GHZ/Hardy型论证，进一步突显了该干涉过程的悖论性质。

Conclusion: 该干涉方案为揭示[Rev. Mod. Phys. 94, 025007 (2022)]中讨论的一系列现象的非经典性提供了一种通用方法，扩展了量子干涉在揭示非经典相关性方面的应用范围。

Abstract: Frustrated down conversion is a process in which a quantum superposition of emissions from two separate parametric down-conversion processes gives rise to observable interference. Depending on the phase relation between the probability amplitudes associated with emissions by the first and second crystal, the process can be enhanced or suppressed. This is achieved by aligning the setup so that the signal and idler modes from the first crystal are fed into the second and constitute its signal-idler modes.
  In Sci. Adv. 11, 1794 (2025), two-observer interwoven frustrated PDC processes produced interference effects based on path identity [Phys. Rev. Lett. 118, 080401 (2017)]. The signal and idler modes of source crystals I and II are arranged to fully overlap with the emission modes of crystals A and B, which serve as elements of measurement stations controlled by Alice and Bob. In the interwoven configuration, crystal A (B) receives the signal mode of crystal I (II) and the idler mode of crystal II (I), enabling interference between joint emission processes at the sources and at the measurement stations. It was conjectured that such interference may lead to new non-classical phenomena.
  In arXiv:2508.19207 it was shown that the process violates the standard Clauser-Horne Bell inequality without additional assumptions, provided suitable measurement settings are used. Here we extend the interference scheme to more than two measurement stations and demonstrate a violation of one of the WWWZB inequalities. This indicates that the proposed approach may provide a general method for revealing non-classicality in a range of phenomena discussed in [Rev. Mod. Phys. 94, 025007 (2022)]. We also present a GHZ/Hardy-type argument that further highlights the paradoxical character of the interference.

</details>


### [37] [Qubit error bursts in superconducting quantum processors of Quantum Inspire: quasiparticle pumping and anomalous time dependence](https://arxiv.org/abs/2602.18388)
*G. R. Di Carlo,M. Samiotis,A. Kamlapure,M. Finkel,N. Muthusubramanian,M. W. Beekman,N. Haider,B. Segers,S. Vallés-Sanclemente,L. DiCarlo*

Main category: quant-ph

TL;DR: 研究比较了两种不同约瑟夫森结（Dolan结和标准结）的超导量子处理器中的量子比特错误爆发现象，发现Dolan结处理器具有独特的错误恢复特性和异常时间依赖性。


<details>
  <summary>Details</summary>
Motivation: 研究超导量子处理器中量子比特错误爆发的特性，特别关注不同约瑟夫森结设计（Dolan结与标准结）对错误行为的影响，以理解设备特定因素与制冷机依赖特性之间的区别。

Method: 使用5和7个transmon量子比特的处理器，这些处理器在设计、制造和封装上相似，但采用不同类型的约瑟夫森结。在两个不同的制冷机中进行测量，以区分设备特定特性和制冷机依赖特性。通过π脉冲速率变化实验研究错误恢复机制。

Result: 1. 错误爆发的持续时间和速率是设备特定的，但与先前实验范围一致，符合电离辐射解释。2. Dolan结处理器显示出两个意外特征：π脉冲速率增加会缩短恢复时间（可通过准粒子泵浦机制解释）；错误爆发率具有异常时间依赖性——冷却后数天或数周出现激增，随后持续抑制直到热循环。

Conclusion: 约瑟夫森结类型显著影响超导量子处理器中的错误爆发行为。Dolan结表现出独特的准粒子相关恢复机制和长期时间依赖性，这对量子计算系统的稳定性和可靠性具有重要影响，表明结设计是优化量子处理器性能的关键因素。

Abstract: We investigate qubit error bursts in 5- and 7-transmon processors of similar design, fabrication and packaging, but with different types of qubit Josephson junctions. Measurements for each are performed in two refrigerators to discern device-specific from refrigerator-dependent characteristics. The duration and rate of bursts are device specific but within the range of prior experiments and consistent with ionizing radiation. We observe two unforeseen signatures specifically in the processor with Dolan junctions. First, increasing the rate of $π$ pulsing in the detection scheme shortens the recovery time to equilibrium, which is explained by a quasiparticle pumping mechanism. The second signature is an anomalous time dependence in the burst rate: a surge happens days or weeks after cooldown, followed by a strong suppression that persists until thermal cycling.

</details>


### [38] [Participation Ratio as a Quantum Probe of Hierarchical Stickiness](https://arxiv.org/abs/2602.18412)
*Ariel A. Galindo Duque,Miguel A. Prado Reynoso,Miguel Gonzalez,Jorge G. Hirsch*

Main category: quant-ph

TL;DR: 量子参与率能探测经典混合相空间中的分层黏滞结构，与有限时间李雅普诺夫指数分布对应，在特定演化时间窗口内最优匹配。


<details>
  <summary>Details</summary>
Motivation: 研究量子局域化如何编码经典混合相空间中的分层黏滞结构，这种结构控制着输运过程。

Method: 使用周期驱动的kicked top模型，分析相干态在Floquet本征基中的参与率，并与经典有限时间李雅普诺夫指数的多峰分布进行对比。引入高斯粗粒化处理FTLE以匹配相干态的半经典分辨率。

Result: 量子参与率能解析经典分层黏滞结构，局部相关性和全局概率分布比较显示，在有限演化时间窗口内量子与经典指标最优匹配，此时黏滞结构最清晰。

Conclusion: 参与率从混沌的全局度量提升为分层输运的敏感探针，为诊断驱动量子系统中的反常局域化提供了实用途径。

Abstract: We investigate how quantum localization encodes the hierarchical stickiness that governs transport in mixed classical phase spaces. Using the periodically driven kicked top, we show that the participation ratio (PR) of coherent states in the Floquet eigenbasis resolves the same layered structure that appears classically as a multimodal distribution of finite-time Lyapunov exponents (FTLEs). To establish a quantitative correspondence, we introduce a Gaussian coarse graining of the FTLE matched to the intrinsic semiclassical resolution of coherent states. Both local correlations and global comparisons of probability distributions demonstrate that quantum and classical indicators agree optimally within a finite window of evolution times, where sticky structures are most clearly resolved. Our results promote the participation ratio from a global measure of chaos to a sensitive probe of hierarchical transport and provide a practical route for diagnosing anomalous localization in driven quantum systems.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [39] [There and back again -- Closed timelike curves as EFT selection principle](https://arxiv.org/abs/2602.17724)
*Bum-Hoon Lee,Nils A. Nilsson,Somyadip Thakur*

Main category: gr-qc

TL;DR: 论文提出在修正引力理论中，闭合类时曲线应比广义相对论中更难形成，以此作为约束修正引力理论的新指导原则，并在旋转黑洞背景下验证这一原则。


<details>
  <summary>Details</summary>
Motivation: 当前修正引力理论通常基于有效场论框架，但主要关注平直时空的正定性和幺正性要求。作者认为这种考虑应扩展到弯曲时空的因果结构，特别是闭合类时曲线的形成条件。

Method: 1. 提出新指导原则：闭合类时曲线在修正引力中应比广义相对论中更难形成；2. 在旋转黑洞背景下研究修正引力有效场论；3. 基于Horndeski类构造微扰旋转黑洞解；4. 通过准正规模与黑洞回波探测闭合类时曲线。

Result: 1. 获得了保持因果性和稳定性的参数界限；2. 发现这些界限与基于时间延迟方法得到的界限部分重叠；3. 提出了通过引力波数据诊断时空因果性的新探测方法。

Conclusion: 闭合类时曲线形成难度的要求为修正引力理论提供了强有力的约束原则，这一原则可用于限制理论参数，并为下一代引力波数据提供了诊断时空因果性的新工具。

Abstract: Modified gravity is often approached in the context of effective-field theory (EFT), with the view that the EFT corrections permit a more desirable theory. In this paper, we posit that this should extend to the causal structure of curved spacetime in addition to the standard demands such that of flat spacetime positivity and unitarity. We propose a new guiding principle for modified-gravity theories, namely that closed timelike curves should always be {\it harder} to obtain than in General Relativity. By demanding this, one can place powerful constraints on modified gravity. To elucidate this claim, we investigate modified-gravity EFTs on rotating black-hole backgrounds, focusing on the appearance/disappearance of closed timelike curves, and provide parameter bounds which only partly overlap with other approaches based on time delay. We construct perturbative rotating black-hole solutions in modified-gravity EFTs based on the Horndeski class and provide parameter bounds necessary to preserve causality and stability. Finally, we present a novel probe for the existence of closed timelike curves through quasinormal modes and black-hole echoes. This can be used to diagnose spacetime causality once next-generation gravitational-wave data becomes available.

</details>


### [40] [Scaling Laws for Template-Free Detection of Environmental Phase Modulation in Gravitational-Wave Signals](https://arxiv.org/abs/2602.17725)
*Jericho Cain*

Main category: gr-qc

TL;DR: 研究使用连续小波变换检测引力波信号中由环境效应引起的平滑相位调制，发现检测性能取决于相位畸变与信噪比的乘积这一单一标度参数。


<details>
  <summary>Details</summary>
Motivation: 环境效应（如分层三重运动）可通过随时间变化的视线加速度在引力波信号中引入累积相位调制。这种平滑的时间扭曲畸变是否可观测取决于畸变强度和信噪比，但这一关系尚未在无模板框架下量化。

Method: 使用连续小波变换导出的时频表示进行研究。不依赖重构误差，而是检查基于轨迹的统计量，特别是功率加权频率质心的演化。采用单样本统计量参考孤立双星分布，无需匹配模板。

Result: 在累积相位畸变和信噪比的网格上，检测性能坍缩到由相位畸变与信噪比乘积定义的单一标度参数上。ROC-AUC在此参数上遵循S型转变。中等畸变在低信噪比下可检测，而较小畸变需要更高信噪比。

Conclusion: 平滑的环境相位调制不会被固有波形变异性完全吸收；相反，其可检测性由累积相位畸变与信号强度之间的简单标度关系决定。该方法为无模板检测环境调制提供了量化框架。

Abstract: Environmental effects such as hierarchical triple motion can introduce cumulative phase modulation in gravitational-wave signals through time-dependent line-of-sight acceleration. Whether such smooth time-warp distortions are observable depends jointly on deformation strength and signal-to-noise ratio (SNR), yet this relationship has not been quantified in a template-free framework. We study the detectability of these distortions using time-frequency representations derived from the continuous wavelet transform. Instead of reconstruction error alone, we examine trajectory-based statistics, in particular the evolution of the power-weighted frequency centroid. We find that environmental modulation can be detected using a single-sample statistic referenced to an isolated-binary distribution, without requiring matched templates. Across a grid of cumulative phase distortions and SNR, detection performance collapses onto a single scaling parameter defined as the product of phase distortion and SNR. The ROC-AUC follows a sigmoid transition in this parameter. Moderate distortions are detectable at low SNR, whereas smaller distortions require higher SNR. These results indicate that smooth environmental phase modulation is not generically absorbed by intrinsic waveform variability; instead, detectability is governed by a simple scaling between cumulative phase distortion and signal strength.

</details>


### [41] [Stability of neutral and charged Dyson shells around Reissner-Nordstrom compact objects](https://arxiv.org/abs/2602.17728)
*S. Habib Mazharimousavi*

Main category: gr-qc

TL;DR: 带电Reissner-Nordstrom时空中的中性戴森壳可以形成稳定平衡构型，而围绕不带电致密天体的戴森壳通常不稳定。


<details>
  <summary>Details</summary>
Motivation: 研究电磁相互作用在广义相对论框架下对戴森型薄壳构型的稳定作用，探索带电致密天体周围戴森壳的稳定性条件。

Method: 使用Reissner-Nordstrom时空描述带电致密天体，分析中性戴森壳的稳定性条件，推导平衡半径和最小渐近能量，研究小扰动下的振荡行为，并扩展到带电壳的情况。

Result: 中性戴森壳在带电致密天体周围可以形成稳定平衡构型，小扰动导致稳定振荡运动，振荡频率随壳质量增加而增加，随中心天体电荷增加而减小。带电壳的稳定性取决于电荷符号：与中心电荷同号的壳稳定性降低，异号壳因静电吸引而稳定性增强。

Conclusion: 电磁相互作用在广义相对论框架下的戴森型薄壳构型中起到稳定作用，带电致密天体周围的中性戴森壳可以形成稳定平衡，这为戴森壳的物理实现提供了新的可能性。

Abstract: In this Letter, we show that, in contrast to Dyson shells surrounding uncharged compact objects, which are generally unstable, a neutral Dyson shell enclosing a charged compact object described by the Reissner-Nordstrom spacetime can attain a stable equilibrium configuration. We analytically derive the conditions for stability, determine the equilibrium radius and the corresponding minimum asymptotic energy, and show that small perturbations about this equilibrium lead to a stable oscillatory motion of the shell. The oscillation frequency is obtained explicitly and shown to increase with the shell mass and decrease with the charge of the central object. When the shell itself carries charge, its stability depends on the sign of this charge. Shells with the same sign as the central charge become progressively less stable, while oppositely charged shells exhibit enhanced stability due to the electrostatic attraction. These findings highlight the stabilizing role of electromagnetic interactions in Dyson-type thin-shell configurations within general relativity.

</details>


### [42] [Improved constraints on modified Newtonian gravity from Cassini radio tracking data](https://arxiv.org/abs/2602.17884)
*R. S. Park,A. Hees,B. Famaey,H. Desmond,A. Durakovic*

Main category: gr-qc

TL;DR: 利用DE440行星历表数据改进太阳系四极矩参数Q₂的约束，精度提升40%，验证了MOND修正引力理论中太阳系外部场效应的预测，发现与星系旋转曲线存在显著矛盾。


<details>
  <summary>Details</summary>
Motivation: MOND修正引力理论预测存在外部场效应，可通过太阳系四极矩参数Q₂来检验。之前对Q₂的约束不够精确，需要利用最新行星历表数据改进测量，以更严格地检验MOND理论。

Method: 使用DE440行星历表数据集，将Q₂参数与其他行星历表参数同时进行估计，通过统计分析获得更精确的Q₂约束值。同时验证了木星对Q₂预测的贡献可忽略不计，支持理论计算中仅考虑太阳的近似。

Result: 得到Q₂ = (1.6 ± 1.8) × 10⁻²⁷ s⁻² (1σ)，精度比之前提高40%。木星贡献仅为0.05%，验证了理论近似。新约束显示与外部星系旋转曲线存在3-15σ水平的矛盾，在银河系内对太阳位置的MOND加速度增强上限仅为2%，与观测限制存在强烈矛盾。

Conclusion: 太阳系测量对经典MOND修正引力版本提供了比当前宽双星数据更强的约束。更新的Q₂后验分布证实了MOND理论与太阳系观测数据存在显著矛盾，这对MOND理论构成了严峻挑战。

Abstract: We report an updated constraint on the Solar System quadrupole parameter $Q_2$, which encodes the external field effect predicted by modified gravity versions of the MOND paradigm. Using the dataset employed to compute the DE440 planetary ephemerides, and estimating it simultaneously with other parameters included in the planetary ephemerides, we find $Q_2 = (1.6 \pm 1.8) \times 10^{-27}\,\mathrm{s}^{-2}$ (1-$σ$), representing an improvement of 40% over previous estimates. We also show explicitly that the contribution to the MOND prediction of $Q_2$ from the Solar System's largest planet, Jupiter, is at the 0.05% level, validating the approximation of retaining only the Sun in theoretical calculations. With this new constraint on $Q_2$, we update previously acknowledged tensions with external galaxy rotation curves, now leading to discrepancies at the $3$-$15σ$ level depending on the detailed mass modeling or the subset of galaxies considered. Within the Milky Way itself, the $Q_2$ constraint imposes an upper bound of only 2% (at 95% confidence) on the MOND boost to the galactic radial acceleration (i.e., the ratio of the observed over baryonic Newtonian acceleration) at the position of the Sun, in strong tension with current observational limits. The updated $Q_2$ posterior finally confirms that Solar System measurements provide stronger constraints than current wide-binary data on classical modified gravity versions of MOND.

</details>


### [43] [Observer-robust energy condition verification for warp drive spacetimes](https://arxiv.org/abs/2602.18023)
*An T. Le*

Main category: gr-qc

TL;DR: warpax是一个开源的GPU加速Python工具包，用于对曲速引擎时空进行观测者鲁棒的能量条件分析，通过连续优化替代离散采样，能更准确地检测能量条件违反情况。


<details>
  <summary>Details</summary>
Motivation: 现有工具仅对有限观测者方向进行离散采样评估能量条件，这可能导致低估曲速引擎时空中能量条件违反的空间范围和严重程度。

Method: 使用GPU加速的连续梯度优化方法在类时观测者流形上进行优化，结合Hawking-Ellis代数分类。在Type I应力能量点通过代数特征值检查精确确定能量条件满足情况，非Type I点则提供快速率上限的诊断。通过前向模式自动微分从ADM度量计算应力能量张量，消除有限差分截断误差。

Result: 分析了五个曲速引擎度量，发现标准欧拉框架分析会遗漏大量能量条件违反点（如Rodal度量中遗漏超过28%的强能量条件违反点和15%的弱能量条件违反点）。即使欧拉框架识别出违反点，观测者优化显示违反严重程度可能高出几个数量级（如Alcubierre弱能量条件违反严重程度在快速率上限为5时约90,000倍）。

Conclusion: 单框架评估会系统性地低估曲速引擎时空中能量条件违反的空间范围和严重程度，warpax工具包提供了更准确的分析方法，并已开源发布。

Abstract: We present \textbf{warpax}, an open-source, GPU-accelerated Python toolkit for observer-robust energy condition analysis of warp drive spacetimes. Existing tools evaluate energy conditions for a finite sample of observer directions; \textbf{warpax} replaces discrete sampling with continuous, gradient-based optimization over the timelike observer manifold (rapidity and boost direction), backed by Hawking--Ellis algebraic classification. At Type~I stress-energy points, which comprise ${>}\,96$\% of all grid points across the tested metrics, an algebraic eigenvalue check determines energy-condition satisfaction \emph{exactly}, independent of any observer search or rapidity cap. At non-Type~I points the optimizer provides rapidity-capped diagnostics. Stress-energy tensors are computed from the ADM metric via forward-mode automatic differentiation, eliminating finite-difference truncation error. Geodesic integration with tidal-force and blueshift analysis is also included.
  We analyze five warp drive metrics (Alcubierre, Lentz, Van~Den~Broeck, Natário, Rodal) and one warp shell metric (used primarily as a numerical stress test). For the Rodal metric, the standard Eulerian-frame analysis misses violations at over $28\%$ of grid points (dominant energy condition) and over $15\%$ (weak energy condition). Even where the Eulerian frame identifies the correct violation set, observer optimization reveals that violation severity can be orders of magnitude larger (e.g.\ Alcubierre weak energy condition: ${\sim}\,90{,}000\times$ at rapidity cap $ζ_{\max} = 5$, scaling as $e^{2ζ_{\max}}$). These results demonstrate that single-frame evaluation can systematically underestimate both the spatial extent and the magnitude of energy condition violations in warp drive spacetimes. \textbf{warpax} is freely available at https://github.com/anindex/warpax.

</details>


### [44] [Evolutionary Behavior of Fractional Holographic Dark Energy within $f(T)$ Teleparallel Gravity](https://arxiv.org/abs/2602.17897)
*Elangbam Chingkheinganba Meetei,S. Surendra Singh*

Main category: gr-qc

TL;DR: 该研究在f(T)引力框架下，通过动力系统方法分析全息暗能量(FHDE)的宇宙学动力学，发现系统存在四个临界点，能够再现标准宇宙演化序列并自然导向晚期加速膨胀。


<details>
  <summary>Details</summary>
Motivation: 研究f(T)引力理论中全息暗能量(FHDE)的宇宙学动力学行为，探索该理论框架是否能够自然地再现标准宇宙演化序列，包括辐射主导、物质主导和晚期加速膨胀阶段。

Method: 在平坦FRW背景下，引入适当的无量纲变量，将场方程重构为封闭的动力系统，然后进行系统的相空间分析，识别临界点并研究其稳定性特征。

Result: 系统存在四个临界点：两个鞍点对应辐射和物质主导时期，两个稳定点对应暗能量主导阶段和de Sitter解。鞍点的性质确保了早期宇宙阶段的过渡性，使宇宙演化能够自然地趋向稳定的晚期加速吸引子。

Conclusion: f(T)引力理论能够在一致的动力学框架下再现标准宇宙演化序列，包括辐射主导、物质主导和晚期加速膨胀阶段，表明该理论是描述宇宙演化的可行框架。

Abstract: We investigate the cosmological dynamics of FHDE within $f(T)$ gravity by employing the dynamical system approach in a spatially flat FRW background. By introducing appropriate dimensionless variables, the field equations are reformulated as a closed system, which allows a systematic phase-space analysis. The resulting system admits four critical points, including two saddle points corresponding to radiation and matter-dominated epochs, and two stable points associated with a DE-dominated phase and a de Sitter solution. The radiation- and matter-dominated critical points are found to possess a saddle character in phase space, ensuring their transient nature and enabling the cosmological evolution to naturally progress toward a stable late-time accelerated attractor. The stable critical points describe accelerated expansion with effective equations of state compatible with DE and de Sitter regimes. Overall, the analysis indicates that $f(T)$ gravity is capable of reproducing the standard cosmological sequence within a consistent dynamical framework.

</details>


### [45] [Cosmic Acceleration from a Simultaneous Variation of Fundamental Constants](https://arxiv.org/abs/2602.18013)
*Malavika K,Soumya Chakrabarti*

Main category: gr-qc

TL;DR: 论文提出G和m_e同时变化可以解释宇宙晚期加速膨胀，无需宇宙常数或暗能量流体


<details>
  <summary>Details</summary>
Motivation: 探索基本物理常数变化与宇宙加速膨胀之间的关联，建立引力、粒子物理和宇宙学之间的桥梁

Method: 研究牛顿引力常数G和电子质量m_e的同时宇宙学变化，与类星体吸收光谱的实验室约束进行比较

Result: 发现G和m_e的变化可以解释晚期宇宙加速膨胀，且m_e的变化与实验室观测约束相符

Conclusion: 宇宙晚期加速膨胀可能是基本耦合常数演化的表现，建立了引力精密测试、粒子物理和宇宙加速起源之间的直接联系

Abstract: We discuss the possibility of a simultaneous cosmic variation of two fundamental entities: the Newtonian gravitational coupling $G$ and the electron mass $m_e$. We show that this variation can account for the late-time cosmic acceleration without invoking a cosmological constant or an explicit dark-energy fluid. We compare the derived $m_e$ variation with laboratory bounds found from Quasar absorption Spectra. Our results indicate that late-time cosmic acceleration could be a manifestation of evolving fundamental couplings, establishing a direct bridge between precision tests of gravity, particle physics and the origin of cosmic acceleration.

</details>


### [46] [Decay of a multi-axionic SU(N) symmetric color aether in the early Universe as an origin of emergence of a many-component dark matter](https://arxiv.org/abs/2602.18041)
*Alexander B. Balakin,Gleb B. Kiselev*

Main category: gr-qc

TL;DR: 提出一个新的SU(N)对称模型，包含矢量场多重态（色以太）、赝标量场多重态（多组分暗物质）、规范场和引力场，引入协变SU(N)对称散度构造，扩展Peccei-Quinn理论，并应用于早期宇宙自发极化和宇宙学


<details>
  <summary>Details</summary>
Motivation: 建立一个新的SU(N)对称相互作用模型，将色以太、多组分暗物质、规范场和引力场统一起来，扩展Peccei-Quinn理论，并探索早期宇宙中的自发极化现象

Method: 构建包含矢量场多重态（色以太）、赝标量场多重态（暗物质）、规范场和引力场的扩展拉格朗日量，引入基于协变SU(N)对称散度的新构造元素，推导自洽主方程，并在Bianchi-I时空平台上建立截断测试模型

Result: 建立了新的SU(N)对称模型，提出了早期宇宙中多轴子色以太自发极化的假设，推导了自洽主方程，并在Bianchi-I时空背景下求解了截断测试模型

Conclusion: 该模型成功统一了色以太、暗物质和规范引力场，扩展了Peccei-Quinn理论，为早期宇宙自发极化现象提供了理论框架，并在宇宙学应用中展示了可行性

Abstract: We establish a new SU(N) symmetric model of interaction between the fields of four types: the multiplet of vector fields, which describes the so-called color aether, the multiplet of pseudoscalar fields, which is associated with the multi-component cosmic dark matter, the gauge and gravitational fields. The extended Lagrangian of the model contains a new constructive element, which is based on the covariant SU(N) symmetric divergence of the multiplet of vector fields; this new element, being the multiplet of scalars from the point of view of spacetime transformations and the color vector from the point of view of the SU(N) group space, gives us the possibility to formulate properly the multi-axionic extension of the Peccei-Quinn theory. The hypothesis of a spontaneous polarization of the multi-axionic color aether in the early Universe is presented. The set of self-consistent master equations of the model is derived. An application to cosmology is considered: the obtained master equations are solved for the truncated test model based on the Bianchi-I spacetime platform.

</details>


### [47] [On type II(D) Einstein spacetimes in six dimensions](https://arxiv.org/abs/2602.18074)
*David Kokoška,Marcello Ortaggio*

Main category: gr-qc

TL;DR: 六维爱因斯坦时空类型II的广义度规分析，包含离散和连续参数，属于Kerr-Schild类，与已知Kerr-(A)dS和Kerr-NUT-(A)dS度规的关系得到澄清


<details>
  <summary>Details</summary>
Motivation: 研究六维爱因斯坦时空类型II（或更特殊）的度规结构，在光学矩阵非退化且"通用"，以及外尔张量在无穷远处快速衰减的假设下，探索最一般的度规形式

Method: 首先对四维和五维爱因斯坦时空类型II进行简要概述，然后总结六维情况的最新结果。假设光学矩阵非退化且"通用"，外尔张量在无穷远处快速衰减，推导最一般的度规形式

Result: 最一般的度规由一个离散（归一化）参数和三个连续参数表征，属于类型D，且属于Kerr-Schild类。阐明了该度规与先前已知的Kerr-(A)dS和Kerr-NUT-(A)dS度规之间的关系

Conclusion: 六维爱因斯坦时空类型II的最一般度规具有特定的参数化特征，属于Kerr-Schild类，为理解高维引力理论中的黑洞解提供了重要框架

Abstract: After a concise overview of Einstein spacetimes of type II (or more special) in four and five dimensions, we summarize recent results in the six-dimensional case. We assume the optical matrix to be non-degenerate and ``generic'', and the Weyl tensor to fall off sufficiently rapidly at infinity. As it turns out, the most general metric is characterized by one discrete (normalized) and three continuous parameters, is of type D and belongs to the Kerr-Schild class. Its relation to the previously known Kerr-(A)dS and Kerr-NUT-(A)dS metrics is clarified.

</details>


### [48] [The Emergence of Measured Geometry in Self-Gravitating Systems](https://arxiv.org/abs/2602.18115)
*Maria I. R. Lourenço,Julian Barbour,Francisco S. N. Lobo*

Main category: gr-qc

TL;DR: 该研究从Poincaré和Einstein的几何操作视角出发，通过数值分析N体系统的中心构型，发现粒子间距随径向距离的系统变化，表明引力系统中的几何是相互作用产生的涌现构造。


<details>
  <summary>Details</summary>
Motivation: 重新审视Poincaré和Einstein关于测量几何本质的经典观点：几何不是固定背景，而是由物理相互作用（特别是引力）塑造的涌现构造。研究旨在将这些思想应用于现代计算框架下的自引力N体系统。

Method: 对牛顿N体问题的中心构型（特殊平衡解）进行数值分析，研究最近邻粒子间距与系统质心径向距离的相关性，揭示空间几何的变化模式。

Result: 发现最近邻粒子间距存在系统性空间变化，与径向距离相关。这些变化反映了由引力相互作用塑造的、依赖于上下文的涌现有效几何，支持几何是内部物理相互作用产物的观点。

Conclusion: 自引力牛顿系统中的几何不是固定背景，而是由内部物理相互作用产生的涌现构造，验证了Poincaré和Einstein关于几何操作本质的深刻见解。

Abstract: This work investigates the geometrical properties of self-gravitating $N$-body systems from the perspective established by Henri Poincaré and Albert Einstein concerning the operational nature of measured geometry. Utilizing recent numerical analyses of central configurations--special equilibrium solutions to the Newtonian $N$-body problem--we uncover systematic spatial variations in nearest-neighbor particle separations correlated with the radial distance from the system's center of mass. We argue that these variations reflect a context-dependent, emergent effective geometry shaped by gravitational interactions, in accordance with Poincaré's assertion that measured geometry depends on the forces influencing measuring devices, and Einstein's view that rods and clocks define physical geometry through their local dynamics. By revisiting these foundational insights within a modern computational framework, we provide evidence that geometry in self-gravitating Newtonian systems is not a fixed background, but an emergent construct arising from internal physical interactions.

</details>


### [49] [Dynamical wormholes](https://arxiv.org/abs/2602.18231)
*Ben Kain*

Main category: gr-qc

TL;DR: 数值研究球对称无电荷虫洞的动力学演化，重点关注Ellis-Bronnikov虫洞和量子修正Schwarzschild黑洞两个例子，发现尽管源项不同但动力学行为相似


<details>
  <summary>Details</summary>
Motivation: 研究不同物理源（幽灵标量场和重整化能动张量）驱动的虫洞的动力学行为，探索它们演化过程的相似性

Method: 使用双零坐标进行球对称动力学演化数值模拟，分析面积半径和能动张量分量的演化图，同时详细描述静态解

Result: Ellis-Bronnikov虫洞和量子修正Schwarzschild黑洞都表现出虫洞膨胀和塌缩的相似动力学行为，尽管它们的物理源完全不同

Conclusion: 不同物理机制驱动的虫洞可能表现出相似的动力学演化特征，双零坐标是研究球对称动力学演化的有效工具，该工作也提供了相关方法的详细综述

Abstract: We numerically investigate the dynamical evolution of spherically symmetric charge free wormholes. We concentrate on two specific examples, both of which exhibit wormhole expansion and wormhole collapse: the Ellis-Bronnikov wormhole, which is sourced by a real massless ghost scalar field, and the quantum corrected Schwarzschild black hole in semiclassical gravity (which has a wormhole structure and is not a true black hole), which is sourced by a renormalized energy-momentum tensor. Despite their very different sources, we demonstrate that the dynamics of these two wormholes are remarkably similar. Our analysis focuses on diagrams for the areal radius and components of the energy-momentum tensor. This work also serves as a review, offering a detailed description of how to perform a spherically symmetric dynamical evolution using double null coordinates as well as a review of the static solutions for our two examples.

</details>


### [50] [GR-Athena++: Binary Neutron Star Merger Simulations with Neutrino Transport](https://arxiv.org/abs/2602.18290)
*Boris Daszuta,Sebastiano Bernuzzi,Maximilian Jacobi,Eduardo M. Gutiérrez,Peter Hammond,William Cook,David Radice*

Main category: gr-qc

TL;DR: GR-Athena++模拟双中子星并合，采用M1中微子输运和N0数密度演化，通过自适应网格验证，展示旋转磁化中子星引力坍缩和双中子星并合场景的稳定演化。


<details>
  <summary>Details</summary>
Motivation: 研究双中子星并合过程中的广义相对论辐射磁流体动力学，特别是中微子输运在引力坍缩和并合后演化中的关键作用，需要开发能够处理复杂物理过程的数值模拟工具。

Method: 使用GR-Athena++进行广义相对论辐射磁流体动力学模拟，采用基于矩的M1中微子输运方案，结合N0中微子数密度演化，通过自适应网格细化和基于切除技术的视界处理方法。

Result: 成功验证了数值实现的稳定性，展示了旋转磁化中子星引力坍缩的稳定辐射演化，并在DD2和SFHo状态方程下的双中子星并合场景中实现了长期稳定演化，能够处理中微子冷却时间尺度的物理过程。

Conclusion: GR-Athena++能够稳健地模拟双中子星并合中的广义相对论辐射磁流体动力学过程，包括引力坍缩和并合后演化，为研究这些极端天体物理事件提供了强大的数值工具。

Abstract: We present general-relativistic radiation magnetohydrodynamics simulations of binary neutron star mergers performed with GR-Athena++. Neutrino transport is treated using a moment-based, energy-integrated scheme (M1), augmented by neutrino number density evolution (N0). Our implementation is validated through an extensive suite of standard tests and demonstrated to perform robustly under adaptive mesh refinement. As a first application, we simulate the gravitational collapse of a uniformly rotating, magnetized neutron star, demonstrating stable radiation evolution through apparent-horizon formation using a novel excision technique based on the tapering of state vector evolution inside the horizon. To further test robustness in highly dynamic environments, we apply our code to two demanding binary neutron star merger scenarios. We investigate a long-lived remnant with the DD2 equation of state, evolved with full general-relativistic magnetohydrodynamics and M1 neutrino transport. Following this, a gravitational collapse scenario with the SFHo equation of state is explored. We showcase long-term stable evolution on neutrino cooling time-scales, demonstrating robust handling of excision and stable evolution of the post-collapse accretion phase in three-dimensional mergers with magnetic fields and neutrino radiation.

</details>


### [51] [Detection prospects of solar $g$-modes with LISA](https://arxiv.org/abs/2602.18385)
*Aman Awasthi*

Main category: gr-qc

TL;DR: 重新评估空间引力波探测器（如LISA、TianQin）探测太阳振荡模式（特别是g模式）的可能性，使用更新的太阳模型、探测器灵敏度和理论约束，发现不同太阳金属丰度模型对探测性影响可忽略。


<details>
  <summary>Details</summary>
Motivation: Polnarev (2009) 曾指出空间引力波干涉仪可能探测到低频太阳振荡模式。本研究旨在利用更新的太阳模型、探测器灵敏度曲线（LISA、TianQin）和改进的理论与观测约束，重新评估这一可能性。

Method: 使用MESA生成标准太阳模型，GYRE计算模式本征频率和本征函数，主要分析太阳g模式（l=2，m=0和m=2）。比较早期和当前LISA灵敏度曲线，并与TianQin在低频波段对比。使用GS98和AGSS09两种金属丰度模型评估预测信号的稳健性。

Result: 两种太阳模型（GS98和AGSS09）产生的引力响应信号几乎相同，表明太阳金属丰度的不确定性对空间干涉仪探测太阳g模式的影响可忽略不计。

Conclusion: 太阳金属丰度差异不影响空间引力波探测器对太阳g模式的探测性评估，为未来利用LISA、TianQin等任务探测太阳内部振荡提供了更可靠的理论基础。

Abstract: The possibility of detecting solar oscillation modes using space-based gravitational-wave detectors has been investigated in the context of gravitational-wave interferometry, with Polnarev \cite{Polnarev:2009xf} demonstrating that low-frequency solar modes could, in principle, produce detectable signals in a LISA-type interferometer. Motivated by this work, I revisit the problem using current solar models, updated detector sensitivities, and improved theoretical and observational constraints on mode amplitudes. In this study, I compute the gravitational response of solar oscillation modes using standard solar models generated with \texttt{MESA}, and mode eigenfrequencies and eigenfunctions calculated with \texttt{GYRE}. I focus primarily on solar $g$ modes, evaluating their responses for degree $l=2$ and azimuthal orders $m=0$ and $m=2$. The analysis incorporates both the earlier proposed and the current updated LISA sensitivity curves, and I perform a comparative assessment with the TianQin mission in the relevant low-frequency band. To assess the robustness of the predicted signals, I estimate the gravitational responses using two different standard solar models based on the GS98 and AGSS09 abundance compilations. I find that the resulting signal responses are nearly identical for the two models, indicating that uncertainties in solar metallicity have a negligible impact on the detectability of solar $g$ modes by space-based interferometers.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [52] [Reducing Text Bias in Synthetically Generated MCQAs for VLMs in Autonomous Driving](https://arxiv.org/abs/2602.17677)
*Sutej Kulgod,Sean Ye,Sanchit Tanwar,Christoffer Heckman*

Main category: cs.LG

TL;DR: 该论文发现MCQA基准测试存在文本线索漏洞，导致VLM无需视觉输入就能获得高准确率。作者提出方法将盲准确率从高于随机66.9%降至2.9%，强制模型依赖视觉基础。


<details>
  <summary>Details</summary>
Motivation: 现有的MCQA基准测试存在严重问题：合成生成的MCQA数据容易包含隐藏的文本线索，使得视觉语言模型可以仅凭语言模式而非视觉上下文就能获得高准确率，这无法真实反映模型的视觉理解能力。

Method: 1. 将正确答案与语言伪影解耦，消除可被利用的文本捷径；2. 采用课程学习策略，逐步增加任务难度；3. 强制模型依赖视觉基础，确保性能准确反映感知理解能力。

Result: 提出的方法显著减少了可被利用的文本捷径，将盲准确率（无视觉输入时的准确率）从高于随机猜测66.9%降低到仅高于随机2.9%，消除了绝大多数文本漏洞。

Conclusion: 合成MCQA基准测试存在严重的文本线索漏洞，需要新的方法来确保VLM性能真正反映视觉理解能力。通过消除文本捷径和强制视觉基础，可以更准确地评估模型的感知能力。

Abstract: Multiple Choice Question Answering (MCQA) benchmarks are an established standard for measuring Vision Language Model (VLM) performance in driving tasks. However, we observe the known phenomenon that synthetically generated MCQAs are highly susceptible to hidden textual cues that allow models to exploit linguistic patterns rather than visual context. Our results show that a VLM fine-tuned on such data can achieve accuracy comparable to human-validated benchmarks even without visual input. Our proposed method reduces blind accuracy from +66.9% above random to +2.9%, eliminating the vast majority of exploitable textual shortcuts. By decoupling the correct answer from linguistic artifacts and employing a curriculum learning strategy, we force the model to rely on visual grounding, ensuring that performance accurately reflects perceptual understanding.

</details>


### [53] [Joint Parameter and State-Space Bayesian Optimization: Using Process Expertise to Accelerate Manufacturing Optimization](https://arxiv.org/abs/2602.17679)
*Saksham Kiroriwal,Julius Pfrommer,Jürgen Beyerer*

Main category: cs.LG

TL;DR: 提出POGPN-JPSS框架，结合部分可观测高斯过程网络与联合参数-状态空间建模，利用专家知识从高维中间观测中提取特征，显著提升多阶段制造过程的贝叶斯优化效率。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化在处理高维多阶段制造过程时存在局限，忽略了中间观测和过程结构信息。当中间观测是高维状态空间时间序列时，利用这些信息尤为困难。

Method: 提出POGPN-JPSS框架：1) 将过程建模为有向无环图；2) 结合专家知识从高维状态空间数据中提取低维潜在特征；3) 使用联合参数-状态空间建模整合中间提取的信息。

Result: 在复杂的多阶段生物乙醇生产过程仿真中，POGPN-JPSS显著优于现有方法，达到期望性能阈值的速度提高两倍，且可靠性更高，直接转化为时间和资源的大量节省。

Conclusion: 将专家知识与结构化概率模型相结合对于快速过程成熟至关重要，POGPN-JPSS框架为高维多阶段制造过程的优化提供了有效解决方案。

Abstract: Bayesian optimization (BO) is a powerful method for optimizing black-box manufacturing processes, but its performance is often limited when dealing with high-dimensional multi-stage systems, where we can observe intermediate outputs. Standard BO models the process as a black box and ignores the intermediate observations and the underlying process structure. Partially Observable Gaussian Process Networks (POGPN) model the process as a Directed Acyclic Graph (DAG). However, using intermediate observations is challenging when the observations are high-dimensional state-space time series. Process-expert knowledge can be used to extract low-dimensional latent features from the high-dimensional state-space data. We propose POGPN-JPSS, a framework that combines POGPN with Joint Parameter and State-Space (JPSS) modeling to use intermediate extracted information. We demonstrate the effectiveness of POGPN-JPSS on a challenging, high-dimensional simulation of a multi-stage bioethanol production process. Our results show that POGPN-JPSS significantly outperforms state-of-the-art methods by achieving the desired performance threshold twice as fast and with greater reliability. The fast optimization directly translates to substantial savings in time and resources. This highlights the importance of combining expert knowledge with structured probabilistic models for rapid process maturation.

</details>


### [54] [BioBridge: Bridging Proteins and Language for Enhanced Biological Reasoning with LLMs](https://arxiv.org/abs/2602.17680)
*Yujia Wang,Jihong Guan,Wengen Li,Shuigeng Zhou,Xuhong Wang*

Main category: cs.LG

TL;DR: BioBridge是一个蛋白质理解领域自适应持续预训练框架，通过结合蛋白质语言模型和通用大语言模型的优势，在蛋白质性质预测和知识问答等任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质语言模型在多任务适应性和跨生物环境泛化能力有限，而通用大语言模型缺乏蛋白质序列解释能力和领域专业知识，无法进行有效的生物语义推理。需要结合两者的优势。

Method: 提出BioBridge框架：1）采用领域增量持续预训练（DICP）同时注入蛋白质领域知识和通用推理语料；2）通过PLM-Projector-LLM管道实现跨模态对齐，将蛋白质序列嵌入映射到语言模型的语义空间；3）采用端到端优化统一支持多种任务。

Result: BioBridge在EC和BindingDB等多个蛋白质基准测试中表现与主流PLMs相当，在MMLU和RACE等通用理解任务上达到与LLMs相当的结果，展示了领域特定适应性和通用语言能力的创新优势。

Conclusion: BioBridge成功结合了蛋白质语言模型和通用大语言模型的优势，实现了蛋白质序列理解和通用语言能力的统一，为蛋白质理解和生物语义推理提供了有效的解决方案。

Abstract: Existing Protein Language Models (PLMs) often suffer from limited adaptability to multiple tasks and exhibit poor generalization across diverse biological contexts. In contrast, general-purpose Large Language Models (LLMs) lack the capability to interpret protein sequences and fall short in domain-specific knowledge, limiting their capacity for effective biosemantic reasoning. To combine the advantages of both, we propose BioBridge, a domain-adaptive continual pretraining framework for protein understanding. This framework employs Domain-Incremental Continual Pre-training (DICP) to infuse protein domain knowledge and general reasoning corpus into a LLM simultaneously, effectively mitigating catastrophic forgetting. Cross-modal alignment is achieved via a PLM-Projector-LLM pipeline, which maps protein sequence embeddings into the semantic space of the language model. Ultimately, an end-to-end optimization is adopted to uniformly support various tasks, including protein property prediction and knowledge question-answering. Our proposed BioBridge demonstrates performance comparable to that of mainstream PLMs on multiple protein benchmarks, such as EC and BindingDB. It also achieves results on par with LLMs on general understanding tasks like MMLU and RACE. This showcases its innovative advantage of combining domain-specific adaptability with general-purpose language competency.

</details>


### [55] [LATMiX: Learnable Affine Transformations for Microscaling Quantization of LLMs](https://arxiv.org/abs/2602.17681)
*Ofir Gordon,Lior Dikstein,Arnon Netzer,Idan Achituve,Hai Victor Habi*

Main category: cs.LG

TL;DR: LATMiX提出了一种可学习的可逆仿射变换方法，用于改进大语言模型的微缩放（MX）低比特量化性能，通过理论分析和实验验证在多个模型尺寸上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有后训练量化方法主要关注传统量化方案，而现代硬件越来越多支持微缩放（MX）数据格式。现有方法将可逆变换与MX量化结合时会出现严重的性能下降，需要引入对变换的假设限制。

Method: 首先对MX量化下的变换进行理论分析，推导量化误差边界。基于此分析提出LATMiX方法，将异常值减少推广到可学习的可逆仿射变换，使用标准深度学习工具进行优化。

Result: 实验表明，在广泛的零样本基准测试中，LATMiX在MX低比特量化上相比强基线取得了平均准确率的持续提升，且适用于多种模型尺寸。

Conclusion: LATMiX通过可学习的可逆仿射变换有效改进了大语言模型的MX量化性能，强调了同时考虑激活分布和底层量化结构的重要性。

Abstract: Post-training quantization (PTQ) is a widely used approach for reducing the memory and compute costs of large language models (LLMs). Recent studies have shown that applying invertible transformations to activations can significantly improve quantization robustness by reducing activation outliers; however, existing approaches are largely restricted to rotation or Hadamard-based transformations. Moreover, most studies focused primarily on traditional quantization schemes, whereas modern hardware increasingly supports the microscaling (MX) data format. Attempts to combine both showed severe performance degradation, leading prior work to introduce assumptions on the transformations. In this work, we take a complementary perspective. First, we provide a theoretical analysis of transformations under MX quantization by deriving a bound on the quantization error. Our analysis emphasizes the importance of accounting for both the activation distribution and the underlying quantization structure. Building on this analysis, we propose LATMiX, a method that generalizes outlier reduction to learnable invertible affine transformations optimized using standard deep learning tools. Experiments show consistent improvements in average accuracy for MX low-bit quantization over strong baselines on a wide range of zero-shot benchmarks, across multiple model sizes.

</details>


### [56] [Duality Models: An Embarrassingly Simple One-step Generation Paradigm](https://arxiv.org/abs/2602.17682)
*Peng Sun,Xinyi Shang,Tao Lin,Zhiqiang Shen*

Main category: cs.LG

TL;DR: 提出Duality Models (DuMo)，通过"一个输入，双输出"范式同时预测速度场和流映射，解决了传统一致性模型训练目标分离导致的效率问题，在ImageNet 256×256上仅用2步达到SOTA FID 1.79。


<details>
  <summary>Details</summary>
Motivation: 传统基于一致性的生成模型（如Shortcut和MeanFlow）采用"一个输入，一个输出"范式，需要将训练预算分割给多步目标和少步目标，导致少步生成训练不足，影响收敛和可扩展性。

Method: 提出Duality Models (DuMo)，采用共享主干网络和双头结构，从单个输入x_t同时预测速度场v_t和流映射u_t，将多步目标的几何约束应用于每个样本，无需分离训练目标。

Result: 在ImageNet 256×256数据集上，使用679M Diffusion Transformer和SD-VAE，仅用2步推理就达到了1.79的FID，创造了新的最先进结果。

Conclusion: DuMo通过"一个输入，双输出"范式有效解决了传统一致性模型训练目标分离的问题，显著提高了稳定性和效率，在少步生成任务上取得了突破性进展。

Abstract: Consistency-based generative models like Shortcut and MeanFlow achieve impressive results via a target-aware design for solving the Probability Flow ODE (PF-ODE). Typically, such methods introduce a target time $r$ alongside the current time $t$ to modulate outputs between a local multi-step derivative ($r = t$) and a global few-step integral ($r = 0$). However, the conventional "one input, one output" paradigm enforces a partition of the training budget, often allocating a significant portion (e.g., 75% in MeanFlow) solely to the multi-step objective for stability. This separation forces a trade-off: allocating sufficient samples to the multi-step objective leaves the few-step generation undertrained, which harms convergence and limits scalability. To this end, we propose Duality Models (DuMo) via a "one input, dual output" paradigm. Using a shared backbone with dual heads, DuMo simultaneously predicts velocity $v_t$ and flow-map $u_t$ from a single input $x_t$. This applies geometric constraints from the multi-step objective to every sample, bounding the few-step estimation without separating training objectives, thereby significantly improving stability and efficiency. On ImageNet 256 $\times$ 256, a 679M Diffusion Transformer with SD-VAE achieves a state-of-the-art (SOTA) FID of 1.79 in just 2 steps. Code is available at: https://github.com/LINs-lab/DuMo

</details>


### [57] [Probabilistic NDVI Forecasting from Sparse Satellite Time Series and Weather Covariates](https://arxiv.org/abs/2602.17683)
*Irene Iele,Giulia Romoli,Daniele Molino,Elena Mulero Ayllón,Filippo Ruffini,Paolo Soda,Matteo Tortora*

Main category: cs.LG

TL;DR: 提出基于Transformer的概率预测框架，用于卫星NDVI时间序列预测，通过分离历史植被动态与未来外生信息建模，并引入时间距离加权分位数损失来处理不规则采样和不确定性。


<details>
  <summary>Details</summary>
Motivation: 卫星NDVI预测面临云层遮挡导致的不规则稀疏采样以及作物生长的异质性气候条件等挑战，需要开发专门针对晴空采集约束的场级预测方法。

Method: 使用Transformer架构，明确分离历史植被动态与未来外生信息建模；引入时间距离加权分位数损失处理不规则重访模式；结合累积和极端天气特征工程捕捉延迟气象效应。

Result: 在欧洲卫星数据上的广泛实验表明，该方法在点预测和概率评估指标上均优于统计、深度学习和近期时间序列基线方法。消融研究显示目标历史信息起核心作用，气象协变量提供补充增益。

Conclusion: 提出的概率预测框架能有效处理卫星NDVI预测中的不规则采样和不确定性，为精准农业的数据驱动决策支持提供了可靠工具。

Abstract: Accurate short-term forecasting of vegetation dynamics is a key enabler for data-driven decision support in precision agriculture. Normalized Difference Vegetation Index (NDVI) forecasting from satellite observations, however, remains challenging due to sparse and irregular sampling caused by cloud coverage, as well as the heterogeneous climatic conditions under which crops evolve. In this work, we propose a probabilistic forecasting framework specifically designed for field-level NDVI prediction under clear-sky acquisition constraints. The method leverages a transformer-based architecture that explicitly separates the modeling of historical vegetation dynamics from future exogenous information, integrating historical NDVI observations with both historical and future meteorological covariates. To address irregular revisit patterns and horizon-dependent uncertainty, we introduce a temporal-distance weighted quantile loss that aligns the training objective with the effective forecasting horizon. In addition, we incorporate cumulative and extreme-weather feature engineering to better capture delayed meteorological effects relevant to vegetation response. Extensive experiments on European satellite data demonstrate that the proposed approach consistently outperforms a diverse set of statistical, deep learning, and recent time series baselines across both point-wise and probabilistic evaluation metrics. Ablation studies further highlight the central role of target history, while showing that meteorological covariates provide complementary gains when jointly exploited. The code is available at https://github.com/arco-group/ndvi-forecasting.

</details>


### [58] [CodeScaler: Scaling Code LLM Training and Test-Time Inference via Execution-Free Reward Models](https://arxiv.org/abs/2602.17684)
*Xiao Zhu,Xinyu Zhou,Boyu Zhu,Hanxu Hu,Mingzhe Du,Haotian Zhang,Huiming Wang,Zhijiang Guo*

Main category: cs.LG

TL;DR: CodeScaler提出了一种无需执行的奖励模型，用于代码生成的强化学习和推理时扩展，通过语法感知的代码提取和保持有效性的奖励塑造，在多个基准上超越了基于执行的RL方法。


<details>
  <summary>Details</summary>
Motivation: 基于可验证奖励的强化学习（RLVR）虽然推动了代码大语言模型的发展，但其可扩展性受到高质量测试用例可用性和可靠性的根本限制。需要一种无需执行的奖励模型来扩展强化学习训练和推理时的代码生成。

Method: CodeScaler是一个无需执行的奖励模型，基于从已验证代码问题中精心策划的偏好数据进行训练。方法包括语法感知的代码提取和保持有效性的奖励塑造，以确保稳定和鲁棒的优化。

Result: 在五个编码基准上，CodeScaler将Qwen3-8B-Base平均提升了11.72分，比基于执行的RL高出1.82分。能够在无需测试用例的合成数据集上进行可扩展的强化学习。推理时，CodeScaler实现了与单元测试方法相当的性能，同时将延迟降低了10倍。在RM-Bench上，CodeScaler不仅在代码领域（+3.3分），还在通用和推理领域（平均+2.7分）超越了现有奖励模型。

Conclusion: CodeScaler作为一种无需执行的奖励模型，成功解决了基于测试用例的RLVR的可扩展性限制，为代码生成的强化学习和推理时扩展提供了有效解决方案，在多个领域都表现出优越性能。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has driven recent progress in code large language models by leveraging execution-based feedback from unit tests, but its scalability is fundamentally constrained by the availability and reliability of high-quality test cases. We propose CodeScaler, an execution-free reward model designed to scale both reinforcement learning training and test-time inference for code generation. CodeScaler is trained on carefully curated preference data derived from verified code problems and incorporates syntax-aware code extraction and validity-preserving reward shaping to ensure stable and robust optimization. Across five coding benchmarks, CodeScaler improves Qwen3-8B-Base by an average of +11.72 points, outperforming binary execution-based RL by +1.82 points, and enables scalable reinforcement learning on synthetic datasets without any test cases. At inference time, CodeScaler serves as an effective test-time scaling method, achieving performance comparable to unit test approaches while providing a 10-fold reduction in latency. Moreover, CodeScaler surpasses existing reward models on RM-Bench not only in the code domain (+3.3 points), but also in general and reasoning domains (+2.7 points on average).

</details>


### [59] [Optimal Multi-Debris Mission Planning in LEO: A Deep Reinforcement Learning Approach with Co-Elliptic Transfers and Refueling](https://arxiv.org/abs/2602.17685)
*Agni Bandyopadhyay,Gunther Waxenegger-Wilfing*

Main category: cs.LG

TL;DR: 提出统一共椭圆机动框架，结合霍曼转移、安全椭圆接近操作和显式加油逻辑，用于低地球轨道多目标主动碎片清除任务规划，并通过三种算法对比验证强化学习的优越性能。


<details>
  <summary>Details</summary>
Motivation: 低地球轨道多目标主动碎片清除面临复杂的任务规划挑战，需要高效、安全且资源优化的解决方案，以应对随机碎片场、禁飞区和燃料约束等现实环境因素。

Method: 提出统一共椭圆机动框架，整合霍曼转移、安全椭圆接近操作和显式加油逻辑。在包含随机碎片场、禁飞区和ΔV约束的轨道仿真环境中，对比评估贪婪启发式、蒙特卡洛树搜索和基于Masked PPO的深度强化学习三种规划算法。

Result: 在100个测试场景中，Masked PPO算法表现出最优的任务效率和计算性能：访问的碎片数量可达贪婪算法的两倍，且在运行时间上显著优于蒙特卡洛树搜索。

Conclusion: 现代强化学习方法在可扩展、安全和资源高效的空间任务规划中展现出巨大潜力，为主动碎片清除自主化技术的未来发展奠定了基础。

Abstract: This paper addresses the challenge of multi target active debris removal (ADR) in Low Earth Orbit (LEO) by introducing a unified coelliptic maneuver framework that combines Hohmann transfers, safety ellipse proximity operations, and explicit refueling logic. We benchmark three distinct planning algorithms Greedy heuristic, Monte Carlo Tree Search (MCTS), and deep reinforcement learning (RL) using Masked Proximal Policy Optimization (PPO) within a realistic orbital simulation environment featuring randomized debris fields, keep out zones, and delta V constraints. Experimental results over 100 test scenarios demonstrate that Masked PPO achieves superior mission efficiency and computational performance, visiting up to twice as many debris as Greedy and significantly outperforming MCTS in runtime. These findings underscore the promise of modern RL methods for scalable, safe, and resource efficient space mission planning, paving the way for future advancements in ADR autonomy.

</details>


### [60] [Curriculum Learning for Efficient Chain-of-Thought Distillation via Structure-Aware Masking and GRPO](https://arxiv.org/abs/2602.17686)
*Bowen Yu,Maolin Wang,Sheng Zhang,Binhao Wang,Yi Wen,Jingtong Gao,Bowen Liu,Zimo Zhao,Wanyu Wang,Xiangyu Zhao*

Main category: cs.LG

TL;DR: 提出三阶段课程学习框架，通过渐进式技能获取解决大模型到小模型的CoT推理蒸馏问题，在保持准确性的同时显著缩短输出长度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将大语言模型的链式思维推理蒸馏到小模型时面临容量不匹配问题：教师模型的推理过程通常过于冗长，小模型难以忠实复现。现有方法要么将推理压缩为单步（失去可解释性），要么无法平衡准确性和简洁性。

Method: 三阶段课程学习框架：1）通过掩码打乱重建建立结构理解；2）在掩码补全任务上应用组相对策略优化（GRPO），让模型自主发现准确性与简洁性的平衡；3）识别持续失败案例，通过针对性重写指导学生内化教师知识，同样使用GRPO优化。

Result: 在GSM8K数据集上，该方法使Qwen2.5-3B-Base模型准确率提升11.29%，同时输出长度减少27.4%，超越了指令调优变体和先前蒸馏方法。

Conclusion: 该框架通过渐进式技能获取有效解决了大模型到小模型的CoT蒸馏容量不匹配问题，在保持推理可解释性的同时实现了准确性与简洁性的平衡。

Abstract: Distilling Chain-of-Thought (CoT) reasoning from large language models into compact student models presents a fundamental challenge: teacher rationales are often too verbose for smaller models to faithfully reproduce. Existing approaches either compress reasoning into single-step, losing the interpretability that makes CoT valuable. We present a three-stage curriculum learning framework that addresses this capacity mismatch through progressive skill acquisition. First, we establish structural understanding via masked shuffled reconstruction. Second, we apply Group Relative Policy Optimization (GRPO) on masked completion tasks, enabling the model to discover its own balance between accuracy and brevity. Third, we identify persistent failure cases and guide the student to internalize teacher knowledge through targeted rewriting, again optimized with GRPO. Experiments on GSM8K demonstrate that our approach enables Qwen2.5-3B-Base to achieve an 11.29 percent accuracy improvement while reducing output length by 27.4 percent, surpassing both instruction-tuned variants and prior distillation methods.

</details>


### [61] [AnCoder: Anchored Code Generation via Discrete Diffusion Models](https://arxiv.org/abs/2602.17688)
*Anton Xue,Litu Rout,Constantine Caramanis,Sanjay Shakkottai*

Main category: cs.LG

TL;DR: AnchorTree框架通过抽象语法树引导扩散过程，优先解决代码中的关键语法和语义标记，提高代码生成质量


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型在代码生成中经常产生无法执行的程序，因为它们没有尊重编程语言的刚性结构

Method: 引入AnchorTree框架，利用代码固有的结构化层次先验来锚定扩散过程，通过抽象语法树优先解决语法和语义上重要的标记（如关键字和标识符），建立结构支架指导剩余生成

Result: 通过AnCoder模型系列验证，结构锚定扩散为高质量代码生成提供了参数高效的路径

Conclusion: 结构化锚定扩散方法能够有效解决现有扩散模型在代码生成中的结构问题，提供更可靠的程序生成方案

Abstract: Diffusion language models offer a compelling alternative to autoregressive code generation, enabling global planning and iterative refinement of complex program logic. However, existing approaches fail to respect the rigid structure of programming languages and, as a result, often produce broken programs that fail to execute. To address this, we introduce AnchorTree, a framework that explicitly anchors the diffusion process using structured, hierarchical priors native to code. Specifically, AnchorTree uses the abstract syntax tree to prioritize resolving syntactically and semantically salient tokens, such as keywords (e.g., if, while) and identifiers (e.g., variable names), thereby establishing a structural scaffold that guides the remaining generation. We validate this framework via AnCoder, a family of models showing that structurally anchored diffusion offers a parameter-efficient path to high-quality code generation.

</details>


### [62] [Robust Pre-Training of Medical Vision-and-Language Models with Domain-Invariant Multi-Modal Masked Reconstruction](https://arxiv.org/abs/2602.17689)
*Melika Filvantorkaman,Mohsen Piri*

Main category: cs.LG

TL;DR: 提出Robust-MMR框架，通过自监督预训练增强医学视觉语言模型的领域鲁棒性，在多个医学VQA和分类任务上显著提升跨域性能


<details>
  <summary>Details</summary>
Motivation: 医学视觉语言模型在实际部署中面临领域偏移问题（成像设备、采集协议、报告风格差异），现有多模态预训练方法忽视鲁棒性，将其视为下游适应问题

Method: 提出Robust-MMR自监督预训练框架，整合非对称扰动感知掩码、领域一致性正则化和模态韧性约束，鼓励领域不变表示学习

Result: 在VQA-RAD上跨域准确率达78.9%（比最强基线提升3.8%），SLAKE和VQA-2019分别达74.6%和77.0%；扰动下VQA-RAD准确率从69.1%提升至75.6%；MELINDA跨域分类准确率从70.3%提升至75.2%；检索任务中平均排序退化从16降至4.1

Conclusion: 在预训练阶段显式建模鲁棒性能够获得更可靠、可迁移的医学视觉语言表示，适用于真实世界部署

Abstract: Medical vision-language models show strong potential for joint reasoning over medical images and clinical text, but their performance often degrades under domain shift caused by variations in imaging devices, acquisition protocols, and reporting styles. Existing multi-modal pre-training methods largely overlook robustness, treating it as a downstream adaptation problem. In this work, we propose Robust Multi-Modal Masked Reconstruction (Robust-MMR), a self-supervised pre-training framework that explicitly incorporates robustness objectives into masked vision-language learning. Robust-MMR integrates asymmetric perturbation-aware masking, domain-consistency regularization, and modality-resilience constraints to encourage domain-invariant representations. We evaluate Robust-MMR on multiple medical vision-language benchmarks, including medical visual question answering (VQA-RAD, SLAKE, VQA-2019), cross-domain image-text classification (MELINDA), and robust image-caption retrieval (ROCO). Robust-MMR achieves 78.9% cross-domain accuracy on VQA-RAD, outperforming the strongest baseline by 3.8 percentage points, and reaches 74.6% and 77.0% accuracy on SLAKE and VQA-2019, respectively. Under perturbed evaluation, Robust-MMR improves VQA-RAD accuracy from 69.1% to 75.6%. For image-text classification, cross-domain MELINDA accuracy increases from 70.3% to 75.2%, while retrieval experiments show a reduction in mean rank degradation from over 16 to 4.1 under perturbation. Qualitative results further demonstrate improved clinical reasoning for disease detection and structural abnormality assessment. These findings show that explicitly modeling robustness during pre-training leads to more reliable and transferable medical vision-language representations for real-world deployment.

</details>


### [63] [Tethered Reasoning: Decoupling Entropy from Hallucination in Quantized LLMs via Manifold Steering](https://arxiv.org/abs/2602.17691)
*Craig Atkinson*

Main category: cs.LG

TL;DR: HELIX框架通过几何约束解耦输出熵与幻觉，在量化语言模型中实现高温度下的稳定生成，保持逻辑一致性同时提升创意多样性。


<details>
  <summary>Details</summary>
Motivation: 量化语言模型面临基本困境：低温导致重复性输出，高温（T>2.0）则引发轨迹发散和语义不连贯。需要一种方法既能保持高温度下的创意多样性，又能避免幻觉和语义崩溃。

Method: HELIX几何框架通过将隐藏状态轨迹约束到预计算的真实性流形上，解耦输出熵与幻觉。计算统一真实性分数（UTS），结合词元级语义熵和马氏距离。当UTS指示轨迹发散时，使用分级转向向量将激活重定向到结构连贯区域，仅影响0.2-2.5%的词元。

Result: 在4位量化Granite 4.0 H Small模型上：GSM8K在T=3.0时保持88.84%准确率（仅比T=0.5下降2.81pp）；MMLU在14,042个问题上保持72.49%（下降1.24pp）。高温度下转向输出仅显示5-20%想法重复，而保守设置下为70-80%。跨架构验证（Qwen3-30B-A3B MOE）确认该现象与架构无关，概念生成独特性提高46.7%。

Conclusion: HELIX作为语法约束，使模型能够探索语义多样性而不违反有效输出所需的逻辑骨架。几何约束揭示了先前被掩盖的高熵创意储备，实现多温度合成，生成比单温度推理多200%的独特概念。高温度幻觉主要是轨迹发散而非语义崩溃，通过稀疏Transformer注意力层（约10%层）的转向足以纠正Mamba-2状态空间公式中的漂移。

Abstract: Quantized language models face a fundamental dilemma: low sampling temperatures yield repetitive, mode-collapsed outputs, while high temperatures (T > 2.0) cause trajectory divergence and semantic incoherence. We present HELIX, a geometric framework that decouples output entropy from hallucination by tethering hidden-state trajectories to a pre-computed truthfulness manifold. HELIX computes a Unified Truth Score (UTS) combining token-level semantic entropy with Mahalanobis distance from the manifold. When UTS indicates trajectory divergence, graduated steering vectors redirect activations toward structurally coherent regions while affecting only 0.2-2.5% of tokens.
  On 4-bit quantized Granite 4.0 H Small (32B/9B active, hybrid Mamba-Transformer): GSM8K maintains 88.84% accuracy at T = 3.0 (2.81pp degradation from T = 0.5); MMLU maintains 72.49% across 14,042 questions (1.24pp degradation). This demonstrates that high-temperature hallucination is primarily trajectory divergence rather than semantic collapse. Notably, steering the sparse Transformer attention layers (~10% of layers) is sufficient to correct drift in the Mamba-2 state-space formulation.
  Geometric tethering reveals a previously-masked High-Entropy Creative Reservoir. At T > 2.0, steered outputs exhibit 5-20% idea duplication versus 70-80% at conservative settings. Cross-architecture validation (Qwen3-30B-A3B MOE) confirms this phenomenon is architecture-independent, with 46.7% higher unique concept generation. HELIX acts as a syntax tether, enabling exploration of semantic diversity without violating the logical backbone required for valid output. This enables Multi-Temperature Synthesis, generating 200% more unique concepts than single-temperature inference.

</details>


### [64] [Agentic Unlearning: When LLM Agent Meets Machine Unlearning](https://arxiv.org/abs/2602.17692)
*Bin Wang,Fan Wang,Pingping Wang,Jinyu Cong,Yang Yu,Yilong Yin,Zhongyi Han,Benzheng Wei*

Main category: cs.LG

TL;DR: 本文提出"智能体遗忘学习"方法，通过同步反向流遗忘框架同时从模型参数和持久记忆中移除指定信息，解决现有方法只针对参数而忽略参数-记忆回流污染的问题。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘学习方法仅针对模型参数，存在两个关键缺陷：1) 参数-记忆回流问题，检索会重新激活参数残留或记忆伪影重新引入敏感内容；2) 缺乏覆盖参数和记忆通路的统一策略。

Method: 提出同步反向流遗忘(SBU)框架，包含两个通路：记忆通路采用基于依赖闭包的遗忘方法，修剪孤立实体并逻辑上使共享伪影无效；参数通路使用随机参考对齐，将模型输出导向高熵先验。通过同步双更新协议集成两个通路，形成闭环机制。

Result: 在医疗QA基准测试中，SBU有效减少了目标私有信息在两个通路中的痕迹，同时对保留数据的性能退化有限。

Conclusion: SBU框架首次实现了智能体中参数和记忆通路的联合遗忘学习，通过同步更新机制防止跨通路再污染，为具有闭环交互的智能体提供了有效的隐私信息移除方案。

Abstract: In this paper, we introduce \textbf{agentic unlearning} which removes specified information from both model parameters and persistent memory in agents with closed-loop interaction. Existing unlearning methods target parameters alone, leaving two critical gaps: (i) parameter-memory backflow, where retrieval reactivates parametric remnants or memory artifacts reintroduce sensitive content, and (ii) the absence of a unified strategy that covers both parameter and memory pathways. We present Synchronized Backflow Unlearning (SBU), a framework that unlearns jointly across parameter and memory pathways. The memory pathway performs dependency closure-based unlearning that prunes isolated entities while logically invalidating shared artifacts. The parameter pathway employs stochastic reference alignment to guide model outputs toward a high-entropy prior. These pathways are integrated via a synchronized dual-update protocol, forming a closed-loop mechanism where memory unlearning and parametric suppression reinforce each other to prevent cross-pathway recontamination. Experiments on medical QA benchmarks show that SBU reduces traces of targeted private information across both pathways with limited degradation on retained data.

</details>


### [65] [A Case Study of Selected PTQ Baselines for Reasoning LLMs on Ascend NPU](https://arxiv.org/abs/2602.17693)
*Yuchen Luo,Fangyue Zhu,Ruining Zhou,Mingzhe Huang,Jian Zhu,Fanyu Fan,Wei Shao*

Main category: cs.LG

TL;DR: 本文研究了在昇腾NPU上对推理导向模型进行后训练量化的可行性与局限性，发现4位权重量化对大型模型可行，但4位权重-激活量化在长上下文推理中不稳定，而8位量化保持数值稳定。


<details>
  <summary>Details</summary>
Motivation: 后训练量化对高效模型部署至关重要，但现有研究主要集中在GPU架构上，对昇腾NPU平台的有效性研究不足。需要探索在昇腾NPU上部署量化推理模型的可行性和限制。

Method: 对DeepSeek-R1-Distill-Qwen系列(1.5B/7B/14B)和QwQ-32B等推理导向模型进行案例研究，评估了四种量化算法：AWQ、GPTQ、SmoothQuant和FlatQuant，涵盖从仅权重量化到基于旋转的高级方法。

Result: 4位仅权重量化对大型模型可行，但4位权重-激活量化在NPU上存在层间校准不稳定性，导致长上下文推理任务中的逻辑崩溃。8位量化保持数值稳定。INT8部署显示优化内核虽降低延迟，但动态量化开销限制端到端加速。

Conclusion: 研究为在昇腾NPU上部署量化推理模型提供了实用参考：4位权重量化可行但需谨慎，8位量化更稳定，当前动态量化开销仍是端到端加速的瓶颈。

Abstract: Post-Training Quantization (PTQ) is crucial for efficient model deployment, yet its effectiveness on Ascend NPU remains under-explored compared to GPU architectures. This paper presents a case study of representative PTQ baselines applied to reasoning-oriented models such as DeepSeek-R1-Distill-Qwen series (1.5B/7B/14B) and QwQ-32B. We evaluate four distinct algorithms, including AWQ, GPTQ, SmoothQuant, and FlatQuant, to cover the spectrum from weight-only compression to advanced rotation-based methods. Our empirical results reveal significant platform sensitivity. While 4-bit weight-only quantization proves viable for larger models, aggressive 4-bit weight-activation schemes suffer from layer-wise calibration instability on the NPU, leading to logic collapse in long-context reasoning tasks. Conversely, standard 8-bit quantization remains numerically stable. Furthermore, a real-world INT8 deployment demonstrates that although optimized kernels reduce latency, dynamic quantization overheads currently limit end-to-end acceleration. These findings offer a practical reference for the feasibility and limitations of deploying quantized reasoning models on Ascend NPU.

</details>


### [66] [AsynDBT: Asynchronous Distributed Bilevel Tuning for efficient In-Context Learning with Large Language Models](https://arxiv.org/abs/2602.17694)
*Hui Ma,Shaoyu Dou,Ya Liu,Fei Xing,Li Feng,Feng Pi*

Main category: cs.LG

TL;DR: 提出AsynDBT算法，通过异步分布式双层调优优化上下文学习样本和提示片段，解决云LLM API应用中数据隐私、异构计算和慢节点问题。


<details>
  <summary>Details</summary>
Motivation: 云LLM API应用面临参数不可知、提示调优成本高、高质量数据稀缺且敏感、传统联邦学习存在慢节点和异构数据分布等问题。

Method: 提出异步分布式双层调优算法，优化上下文学习样本和提示片段，基于LLM反馈进行分布式隐私保护调优，适应异构计算环境。

Result: 理论分析证明算法收敛性，多个基准数据集实验验证了AsynDBT的有效性和效率。

Conclusion: AsynDBT通过分布式双层调优解决了云LLM API应用中的隐私保护、异构计算和慢节点问题，提升了下游任务性能。

Abstract: With the rapid development of large language models (LLMs), an increasing number of applications leverage cloud-based LLM APIs to reduce usage costs. However, since cloud-based models' parameters and gradients are agnostic, users have to manually or use heuristic algorithms to adjust prompts for intervening LLM outputs, which requiring costly optimization procedures. In-context learning (ICL) has recently emerged as a promising paradigm that enables LLMs to adapt to new tasks using examples provided within the input, eliminating the need for parameter updates. Nevertheless, the advancement of ICL is often hindered by the lack of high-quality data, which is often sensitive and different to share. Federated learning (FL) offers a potential solution by enabling collaborative training of distributed LLMs while preserving data privacy. Despite this issues, previous FL approaches that incorporate ICL have struggled with severe straggler problems and challenges associated with heterogeneous non-identically data. To address these problems, we propose an asynchronous distributed bilevel tuning (AsynDBT) algorithm that optimizes both in-context learning samples and prompt fragments based on the feedback from the LLM, thereby enhancing downstream task performance. Benefiting from its distributed architecture, AsynDBT provides privacy protection and adaptability to heterogeneous computing environments. Furthermore, we present a theoretical analysis establishing the convergence guarantees of the proposed algorithm. Extensive experiments conducted on multiple benchmark datasets demonstrate the effectiveness and efficiency of AsynDBT.

</details>


### [67] [EXACT: Explicit Attribute-Guided Decoding-Time Personalization](https://arxiv.org/abs/2602.17695)
*Xin Yu,Hanwen Xing,Lingzhou Xue*

Main category: cs.LG

TL;DR: EXACT：一种基于可解释属性的解码时个性化方法，通过识别用户特定属性子集和基于语义检索相关属性来适应上下文偏好变化，实现个性化对齐。


<details>
  <summary>Details</summary>
Motivation: 现有解码时个性化方法依赖隐式、难以解释的偏好表示，且采用僵化的上下文无关用户表示，无法处理不同提示下偏好的变化。需要一种能够适应上下文偏好转移的可解释个性化方法。

Method: EXACT使用预定义的可解释属性集，通过离线阶段最大化偏好响应似然识别用户特定属性子集，在线推理时基于语义检索与输入提示最相关的属性并注入上下文来引导生成。

Result: 在人工标注的偏好数据集上，EXACT在偏好建模准确性和个性化生成质量方面持续优于强基线方法。理论分析表明算法在温和假设下具有近似保证，相似性检索机制能有效缓解上下文偏好转移。

Conclusion: EXACT提供了一种可扩展、可解释的解码时个性化方法，能够适应上下文偏好变化，在有限成对偏好反馈下实现个性化对齐，优于现有方法。

Abstract: Achieving personalized alignment requires adapting large language models to each user's evolving context. While decoding-time personalization offers a scalable alternative to training-time methods, existing methods largely rely on implicit, less interpretable preference representations and impose a rigid, context-agnostic user representation, failing to account for how preferences shift across prompts. We introduce EXACT, a new decoding-time personalization that aligns generation with limited pairwise preference feedback using a predefined set of interpretable attributes. EXACT first identifies user-specific attribute subsets by maximizing the likelihood of preferred responses in the offline stage. Then, for online inference, EXACT retrieves the most semantically relevant attributes for an incoming prompt and injects them into the context to steer generation. We establish theoretical approximation guarantees for the proposed algorithm under mild assumptions, and provably show that our similarity-based retrieval mechanism effectively mitigates contextual preference shifts, adapting to disparate tasks without pooling conflicting preferences. Extensive experiments on human-annotated preference datasets demonstrate that EXACT consistently outperforms strong baselines, including preference modeling accuracy and personalized generation quality.

</details>


### [68] [Can LLM Safety Be Ensured by Constraining Parameter Regions?](https://arxiv.org/abs/2602.17696)
*Zongmin Li,Jian Su,Farah Benamara,Aixin Sun*

Main category: cs.LG

TL;DR: 对LLM安全区域识别方法的系统性评估显示，现有技术无法可靠识别稳定、数据集无关的安全区域


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通常被认为包含"安全区域"——直接影响安全行为的参数子集，但现有安全区域识别方法的可靠性需要验证

Method: 系统评估了四种不同参数粒度（从单个权重到整个Transformer层）的安全区域识别方法，覆盖四个不同规模的LLM家族，使用十个安全识别数据集

Result: 识别的安全区域仅显示低到中度的重叠（IoU测量），当使用效用数据集（非有害查询）进一步细化时，重叠显著下降

Conclusion: 当前技术无法可靠识别稳定、数据集无关的安全区域，表明安全区域识别仍面临挑战

Abstract: Large language models (LLMs) are often assumed to contain ``safety regions'' -- parameter subsets whose modification directly influences safety behaviors. We conduct a systematic evaluation of four safety region identification methods spanning different parameter granularities, from individual weights to entire Transformer layers, across four families of backbone LLMs with varying sizes. Using ten safety identification datasets, we find that the identified safety regions exhibit only low to moderate overlap, as measured by IoU. The overlap drops significantly when the safety regions are further refined using utility datasets (\ie non-harmful queries). These results suggest that current techniques fail to reliably identify a stable, dataset-agnostic safety region.

</details>


### [69] [Pimp My LLM: Leveraging Variability Modeling to Tune Inference Hyperparameters](https://arxiv.org/abs/2602.17697)
*Nada Zine,Clément Quinton,Romain Rouvoy*

Main category: cs.LG

TL;DR: 该论文提出将大语言模型视为可配置系统，应用变异性管理技术来系统分析推理配置选择，以解决LLM推理能耗优化问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型计算需求巨大，推理阶段尤其耗能，但推理服务器的配置空间庞大，经验评估不可行，需要系统化的配置分析方法。

Method: 将LLM视为可配置系统，使用基于特征的变异性模型表示生成超参数及其约束，采样代表性配置，测量能耗、延迟、准确性，并学习预测模型。

Result: 变异性建模有效管理LLM推理配置复杂性，能够系统分析超参数效应和交互，揭示权衡关系，并能从有限测量中准确预测推理行为。

Conclusion: 这项工作开辟了软件工程与机器学习交叉的新研究方向，利用变异性建模实现LLM的高效可持续配置。

Abstract: Large Language Models (LLMs) are being increasingly used across a wide range of tasks. However, their substantial computational demands raise concerns about the energy efficiency and sustainability of both training and inference. Inference, in particular, dominates total compute usage, making its optimization crucial. Recent research has explored optimization techniques and analyzed how configuration choices influence energy consumption. Yet, the vast configuration space of inference servers makes exhaustive empirical evaluation infeasible due to combinatorial explosion. In this paper, we introduce a new perspective on this problem by treating LLMs as configurable systems and applying variability management techniques to systematically analyze inference-time configuration choices. We evaluate our approach on the Hugging Face Transformers library by representing generation hyperparameters and their constraints using a feature-based variability model, sampling representative configurations, measuring their energy consumption, latency, accuracy, and learning predictive models from the collected data. Our results show that variability modeling effectively manages the complexity of LLM inference configurations. It enables systematic analysis of hyperparameters effects and interactions, reveals trade-offs, and supports accurate prediction of inference behavior from a limited number of measurements. Overall, this work opens a new research direction that bridges software engineering and machine learning by leveraging variability modeling for the efficient and sustainable configuration of LLMs.

</details>


### [70] [ScaleBITS: Scalable Bitwidth Search for Hardware-Aligned Mixed-Precision LLMs](https://arxiv.org/abs/2602.17698)
*Xinlin Li,Timothy Chou,Josh Fromm,Zichang Liu,Yunjie Pan,Christina Fragouli*

Main category: cs.LG

TL;DR: ScaleBITS是一个混合精度量化框架，通过硬件对齐的块级权重分区和双向通道重排序，在内存预算下实现自动化的细粒度比特分配，在超低比特量化中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLM的后训练权重量化对于减少内存和推理成本至关重要，但将平均精度推到4比特以下仍然具有挑战性，因为权重敏感性高度不均匀，且缺乏原则性的精度分配方法。现有解决方案要么使用高运行时开销的不规则细粒度混合精度，要么依赖于启发式或高度受限的精度分配策略。

Method: 1) 基于新的敏感性分析指导；2) 引入硬件对齐的块级权重分区方案，通过双向通道重排序实现；3) 将全局比特分配建模为约束优化问题，并开发可扩展的贪心算法近似方法，实现端到端的原理性分配。

Result: ScaleBITS在超低比特量化中显著优于均匀精度量化（提升高达36%），并且优于最先进的敏感性感知基线方法（提升高达13%），同时不增加运行时开销。

Conclusion: ScaleBITS是一个有效的混合精度量化框架，能够在保持硬件效率的同时，在内存预算下实现自动化的细粒度比特分配，为LLM的超低比特量化提供了实用的解决方案。

Abstract: Post-training weight quantization is crucial for reducing the memory and inference cost of large language models (LLMs), yet pushing the average precision below 4 bits remains challenging due to highly non-uniform weight sensitivity and the lack of principled precision allocation. Existing solutions use irregular fine-grained mixed-precision with high runtime overhead or rely on heuristics or highly constrained precision allocation strategies. In this work, we propose ScaleBITS, a mixed-precision quantization framework that enables automated, fine-grained bitwidth allocation under a memory budget while preserving hardware efficiency. Guided by a new sensitivity analysis, we introduce a hardware-aligned, block-wise weight partitioning scheme, powered by bi-directional channel reordering. We formulate global bitwidth allocation as a constrained optimization problem and develop a scalable approximation to the greedy algorithm, enabling end-to-end principled allocation. Experiments show that ScaleBITS significantly improves over uniform-precision quantization (up to +36%) and outperforms state-of-the-art sensitivity-aware baselines (up to +13%) in ultra-low-bit regime, without adding runtime overhead.

</details>


### [71] [Certified Learning under Distribution Shift: Sound Verification and Identifiable Structure](https://arxiv.org/abs/2602.17699)
*Chandrasekhar Gokavarapu,Sudhakar Gadde,Y. Rajasekhar,S. R. Bhargava*

Main category: cs.LG

TL;DR: 提出一个理论框架，证明在分布偏移下预测器的超额风险存在可计算的显式上界，该上界由可计算的偏移度量和模型参数决定。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型在分布偏移下的风险认证问题，提供可验证的保证，而不是依赖事后解释。

Method: 建立统一的理论框架，基于可验证的规律性和复杂性约束，推导出分布偏移下风险的显式不等式上界，包含可计算的偏移度量和模型参数。

Result: 证明了在分布偏移下，超额风险存在由可计算偏移度量和模型参数决定的显式上界；开发了可验证的学习模型认证方法；通过可识别性条件而非事后解释实现可解释性。

Conclusion: 该框架为分布偏移下的机器学习模型提供了理论保证，包括风险认证、模型验证和可解释性，同时明确了假设条件、失败模式和不可认证的机制。

Abstract: Proposition. Let $f$ be a predictor trained on a distribution $P$ and evaluated on a shifted distribution $Q$. Under verifiable regularity and complexity constraints, the excess risk under shift admits an explicit upper bound determined by a computable shift metric and model parameters. We develop a unified framework in which (i) risk under distribution shift is certified by explicit inequalities, (ii) verification of learned models is sound for nontrivial sizes, and (iii) interpretability is enforced through identifiability conditions rather than post hoc explanations. All claims are stated with explicit assumptions. Failure modes are isolated. Non-certifiable regimes are characterized.

</details>


### [72] [MIDAS: Mosaic Input-Specific Differentiable Architecture Search](https://arxiv.org/abs/2602.17700)
*Konstanty Subbotko*

Main category: cs.LG

TL;DR: MIDAS 提出了一种新颖的差分神经架构搜索方法，通过自注意力机制动态计算输入特定的架构参数，并引入局部化架构选择和拓扑感知搜索空间，在多个基准测试中取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 尽管差分神经架构搜索（NAS）提供了高效的梯度优化方法，但在实际应用中仍然有限。现有方法如DARTS使用静态架构参数，无法适应不同输入的特性，且搜索过程不够鲁棒。

Method: MIDAS 通过自注意力机制动态计算输入特定的架构参数，替代DARTS中的静态参数。采用局部化架构选择，为激活图的每个空间补丁单独计算架构选择。引入参数免费、拓扑感知的搜索空间，建模节点连接性并简化每个节点的两条入边选择。

Result: 在DARTS搜索空间上，CIFAR-10达到97.42% top-1准确率，CIFAR-100达到83.38%。在NAS-Bench-201上始终能找到全局最优架构。在RDARTS搜索空间中，在CIFAR-10的四个搜索空间中的两个上达到了最先进水平。

Conclusion: MIDAS 通过动态、输入特定的架构参数和局部化选择机制，显著提升了差分NAS的性能和鲁棒性。补丁级注意力提高了候选操作之间的区分度，产生的参数分布具有类别感知性和单峰性，为解码提供了可靠指导。

Abstract: Differentiable Neural Architecture Search (NAS) provides efficient, gradient-based methods for automatically designing neural networks, yet its adoption remains limited in practice. We present MIDAS, a novel approach that modernizes DARTS by replacing static architecture parameters with dynamic, input-specific parameters computed via self-attention. To improve robustness, MIDAS (i) localizes the architecture selection by computing it separately for each spatial patch of the activation map, and (ii) introduces a parameter-free, topology-aware search space that models node connectivity and simplifies selecting the two incoming edges per node. We evaluate MIDAS on the DARTS, NAS-Bench-201, and RDARTS search spaces. In DARTS, it reaches 97.42% top-1 on CIFAR-10 and 83.38% on CIFAR-100. In NAS-Bench-201, it consistently finds globally optimal architectures. In RDARTS, it sets the state of the art on two of four search spaces on CIFAR-10. We further analyze why MIDAS works, showing that patchwise attention improves discrimination among candidate operations, and the resulting input-specific parameter distributions are class-aware and predominantly unimodal, providing reliable guidance for decoding.

</details>


### [73] [Parallel Complex Diffusion for Scalable Time Series Generation](https://arxiv.org/abs/2602.17706)
*Rongyao Cai,Yuxi Wan,Kexin Zhang,Ming Jin,Zhiqiang Ge,Qingsong Wen,Yong Liu*

Main category: cs.LG

TL;DR: PaCoDi提出了一种基于频域的并行复杂扩散模型，通过傅里叶变换将时间信号转换为解耦的频谱分量，解决了传统时间扩散模型中的局部纠缠和计算复杂度问题，实现了生成质量和推理速度的双重提升。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列生成模型在建模长程依赖时面临表示能力与计算效率的根本权衡。传统时间扩散模型存在局部纠缠问题，且注意力机制的O(L²)计算成本过高。需要一种既能保持强大表示能力又能高效计算的方法。

Method: 提出PaCoDi（并行复杂扩散）架构，在频域进行生成建模。使用傅里叶变换作为对角化算子，将时间信号转换为解耦的频谱分量。理论证明了正交前向扩散和条件反向分解定理，将复杂扩散过程分解为独立的实部和虚部。采用平均场理论近似和交互校正机制，并将离散DDPM推广到连续时间频率SDE。利用实值信号的埃尔米特对称性压缩序列长度，减少50%注意力FLOPs，并推导了异方差损失处理压缩流形上的非各向同性噪声分布。

Result: PaCoDi在生成质量和推理速度方面均优于现有基线方法。通过频域建模和序列长度压缩，实现了计算效率的显著提升，同时保持了高质量的生成能力。

Conclusion: PaCoDi为时间序列建模提供了一个理论严谨且计算高效的解决方案，通过频域方法从根本上改变了问题拓扑结构，解决了传统扩散模型在长程依赖建模中的核心挑战。

Abstract: Modeling long-range dependencies in time series generation poses a fundamental trade-off between representational capacity and computational efficiency. Traditional temporal diffusion models suffer from local entanglement and the $\mathcal{O}(L^2)$ cost of attention mechanisms. We address these limitations by introducing PaCoDi (Parallel Complex Diffusion), a spectral-native architecture that decouples generative modeling in the frequency domain. PaCoDi fundamentally alters the problem topology: the Fourier Transform acts as a diagonalizing operator, converting locally coupled temporal signals into globally decorrelated spectral components. Theoretically, we prove the Quadrature Forward Diffusion and Conditional Reverse Factorization theorem, demonstrating that the complex diffusion process can be split into independent real and imaginary branches. We bridge the gap between this decoupled theory and data reality using a \textbf{Mean Field Theory (MFT) approximation} reinforced by an interactive correction mechanism. Furthermore, we generalize this discrete DDPM to continuous-time Frequency SDEs, rigorously deriving the Spectral Wiener Process describe the differential spectral Brownian motion limit. Crucially, PaCoDi exploits the Hermitian Symmetry of real-valued signals to compress the sequence length by half, achieving a 50% reduction in attention FLOPs without information loss. We further derive a rigorous Heteroscedastic Loss to handle the non-isotropic noise distribution on the compressed manifold. Extensive experiments show that PaCoDi outperforms existing baselines in both generation quality and inference speed, offering a theoretically grounded and computationally efficient solution for time series modeling.

</details>


### [74] [Provable Adversarial Robustness in In-Context Learning](https://arxiv.org/abs/2602.17743)
*Di Zhang*

Main category: cs.LG

TL;DR: 论文提出了一个分布鲁棒的元学习框架，为基于Wasserstein距离的分布偏移下的上下文学习提供最坏情况性能保证，揭示了模型鲁棒性与容量平方根成正比，对抗性设置会带来与扰动幅度平方成正比的样本复杂度惩罚。


<details>
  <summary>Details</summary>
Motivation: 当前对上下文学习能力的理论解释假设测试任务与预训练期间的分布相似，忽略了对抗性分布偏移对实际可靠性的威胁。需要填补这一理论空白，为对抗性条件下的上下文学习提供理论保证。

Method: 引入分布鲁棒的元学习框架，专注于线性自注意力Transformer模型，推导非渐近边界，将对抗性扰动强度(ρ)、模型容量(m)和上下文示例数量(N)联系起来。

Result: 分析表明：1) 模型鲁棒性随其容量的平方根缩放(ρ_max ∝ √m)；2) 对抗性设置带来的样本复杂度惩罚与扰动幅度的平方成正比(N_ρ - N_0 ∝ ρ^2)。合成任务实验验证了这些缩放规律。

Conclusion: 这些发现推进了对对抗性条件下上下文学习极限的理论理解，表明模型容量是分布鲁棒性的基本资源，为实际应用中对抗性分布偏移下的可靠上下文学习提供了理论指导。

Abstract: Large language models adapt to new tasks through in-context learning (ICL) without parameter updates. Current theoretical explanations for this capability assume test tasks are drawn from a distribution similar to that seen during pretraining. This assumption overlooks adversarial distribution shifts that threaten real-world reliability. To address this gap, we introduce a distributionally robust meta-learning framework that provides worst-case performance guarantees for ICL under Wasserstein-based distribution shifts. Focusing on linear self-attention Transformers, we derive a non-asymptotic bound linking adversarial perturbation strength ($ρ$), model capacity ($m$), and the number of in-context examples ($N$). The analysis reveals that model robustness scales with the square root of its capacity ($ρ_{\text{max}} \propto \sqrt{m}$), while adversarial settings impose a sample complexity penalty proportional to the square of the perturbation magnitude ($N_ρ- N_0 \propto ρ^2$). Experiments on synthetic tasks confirm these scaling laws. These findings advance the theoretical understanding of ICL's limits under adversarial conditions and suggest that model capacity serves as a fundamental resource for distributional robustness.

</details>


### [75] [Bayesian Optimality of In-Context Learning with Selective State Spaces](https://arxiv.org/abs/2602.17744)
*Di Zhang,Jiaqi Xing*

Main category: cs.LG

TL;DR: 论文提出贝叶斯最优序列预测作为理解上下文学习的新原则，证明选择性状态空间模型在特定任务中渐近实现贝叶斯最优预测器，优于基于梯度下降的Transformer方法。


<details>
  <summary>Details</summary>
Motivation: 当前对Transformer上下文学习的解释多基于"隐式梯度下降"框架，但缺乏对序列任务中贝叶斯最优性的理论理解。本文旨在从元学习和最优推断的角度重新理解上下文学习，为架构设计提供理论基础。

Method: 将上下文学习形式化为对潜在序列任务的元学习，在线性高斯状态空间模型任务中，证明元训练的选择性状态空间模型渐近实现贝叶斯最优预测器（后验预测均值）。构造具有时间相关噪声的任务，展示贝叶斯预测器严格优于任何经验风险最小化估计器。

Result: 选择性状态空间模型在线性高斯状态空间模型任务和字符级马尔可夫基准测试中：1）更快收敛到贝叶斯最优风险；2）在结构化噪声设置中，长上下文下具有更好的样本效率；3）比线性Transformer更稳健地跟踪潜在状态。

Conclusion: 将上下文学习从"隐式优化"重新框架为"最优推断"，解释了选择性状态空间模型的效率优势，为架构设计提供了原则性基础。贝叶斯最优序列预测为理解上下文学习提供了新的理论视角。

Abstract: We propose Bayesian optimal sequential prediction as a new principle for understanding in-context learning (ICL). Unlike interpretations framing Transformers as performing implicit gradient descent, we formalize ICL as meta-learning over latent sequence tasks. For tasks governed by Linear Gaussian State Space Models (LG-SSMs), we prove a meta-trained selective SSM asymptotically implements the Bayes-optimal predictor, converging to the posterior predictive mean. We further establish a statistical separation from gradient descent, constructing tasks with temporally correlated noise where the optimal Bayesian predictor strictly outperforms any empirical risk minimization (ERM) estimator. Since Transformers can be seen as performing implicit ERM, this demonstrates selective SSMs achieve lower asymptotic risk due to superior statistical efficiency. Experiments on synthetic LG-SSM tasks and a character-level Markov benchmark confirm selective SSMs converge faster to Bayes-optimal risk, show superior sample efficiency with longer contexts in structured-noise settings, and track latent states more robustly than linear Transformers. This reframes ICL from "implicit optimization" to "optimal inference," explaining the efficiency of selective SSMs and offering a principled basis for architecture design.

</details>


### [76] [Investigating Target Class Influence on Neural Network Compressibility for Energy-Autonomous Avian Monitoring](https://arxiv.org/abs/2602.17751)
*Nina Brolich,Simon Geis,Maximilian Kasper,Alexander Barnhill,Axel Plinge,Dominik Seuß*

Main category: cs.LG

TL;DR: 该论文提出了一种在微控制器单元上运行的鸟类监测方法，通过模型压缩实现边缘设备上的高效鸟类物种识别，显著降低了计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 传统鸟类监测方法成本高、效率低，而现有的机器学习解决方案需要复杂模型和大量计算资源。为了在野外直接进行实时监测，需要在资源受限的微控制器上运行AI模型。

Method: 在微控制器单元上训练和压缩模型，针对不同数量的目标类别评估鸟类物种检测，研究物种数量对神经网络可压缩性的影响。

Result: 实现了显著的压缩率且性能损失最小，提供了不同硬件平台的基准测试结果，并评估了部署能源自主设备的可行性。

Conclusion: 提出的方法能够在资源受限的边缘设备上高效进行鸟类监测，为野外自主监测系统提供了可行的技术方案。

Abstract: Biodiversity loss poses a significant threat to humanity, making wildlife monitoring essential for assessing ecosystem health. Avian species are ideal subjects for this due to their popularity and the ease of identifying them through their distinctive songs. Traditionalavian monitoring methods require manual counting and are therefore costly and inefficient. In passive acoustic monitoring, soundscapes are recorded over long periods of time. The recordings are analyzed to identify bird species afterwards. Machine learning methods have greatly expedited this process in a wide range of species and environments, however, existing solutions require complex models and substantial computational resources. Instead, we propose running machine learning models on inexpensive microcontroller units (MCUs) directly in the field. Due to the resulting hardware and energy constraints, efficient artificial intelligence (AI) architecture is required. In this paper, we present our method for avian monitoring on MCUs. We trained and compressed models for various numbers of target classes to assess the detection of multiple bird species on edge devices and evaluate the influence of the number of species on the compressibility of neural networks. Our results demonstrate significant compression rates with minimal performance loss. We also provide benchmarking results for different hardware platforms and evaluate the feasibility of deploying energy-autonomous devices.

</details>


### [77] [Asking Forever: Universal Activations Behind Turn Amplification in Conversational LLMs](https://arxiv.org/abs/2602.17778)
*Zachary Coalson,Bo Fang,Sanghyun Hong*

Main category: cs.LG

TL;DR: 论文发现对话LLMs存在"轮次放大"故障模式，攻击者可系统利用澄清寻求行为来延长对话轮次而不完成任务，这种攻击源于对话动态机制而非特定提示，能通过微调或参数破坏实现，现有防御措施对此保护有限。


<details>
  <summary>Details</summary>
Motivation: 多轮交互长度是对话LLMs运营成本的主要因素，需要识别和防范可能被利用来人为延长对话的系统性故障模式。

Method: 采用机制视角，识别与澄清寻求响应相关的查询无关的通用激活子空间；通过供应链攻击（微调）和运行时攻击（低级参数破坏）来诱导轮次放大；在多个指令调优LLM和基准测试上验证攻击效果。

Result: 攻击能显著增加对话轮次同时保持合规性；攻击源于对话动态机制，在不同提示和任务中持续存在；现有防御措施对此类故障保护有限。

Conclusion: 轮次放大是对话LLMs中一种新的系统性故障模式，攻击者可通过利用澄清寻求行为来可扩展地延长交互，这揭示了基于对话动态的成本放大攻击风险，需要新的防御机制。

Abstract: Multi-turn interaction length is a dominant factor in the operational costs of conversational LLMs. In this work, we present a new failure mode in conversational LLMs: turn amplification, in which a model consistently prolongs multi-turn interactions without completing the underlying task. We show that an adversary can systematically exploit clarification-seeking behavior$-$commonly encouraged in multi-turn conversation settings$-$to scalably prolong interactions. Moving beyond prompt-level behaviors, we take a mechanistic perspective and identify a query-independent, universal activation subspace associated with clarification-seeking responses. Unlike prior cost-amplification attacks that rely on per-turn prompt optimization, our attack arises from conversational dynamics and persists across prompts and tasks. We show that this mechanism provides a scalable pathway to induce turn amplification: both supply-chain attacks via fine-tuning and runtime attacks through low-level parameter corruptions consistently shift models toward abstract, clarification-seeking behavior across prompts. Across multiple instruction-tuned LLMs and benchmarks, our attack substantially increases turn count while remaining compliant. We also show that existing defenses offer limited protection against this emerging class of failures.

</details>


### [78] [Multi-material Multi-physics Topology Optimization with Physics-informed Gaussian Process Priors](https://arxiv.org/abs/2602.17783)
*Xiangyu Sun,Shirin Hosseinmardi,Amin Yousefpour,Ramin Bostanabad*

Main category: cs.LG

TL;DR: 提出基于物理信息高斯过程（PIGP）的拓扑优化框架，解决多材料、多物理场问题，通过神经网络参数化GP均值函数，同时优化主变量、伴随变量和设计变量，实现超分辨率拓扑生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的拓扑优化方法大多局限于简化基准问题，存在计算成本高、频谱偏差和处理复杂物理困难等局限，特别是在多材料、多物理场问题中，当目标或约束函数非自伴随时问题更为突出。

Method: 使用物理信息高斯过程（PIGP）框架，将主变量、伴随变量和设计变量表示为独立的高斯过程先验，其均值函数通过神经网络参数化。通过最小化基于目标函数、多物理场势能泛函和设计约束的损失函数，同时估计所有模型参数。引入微分和积分方案显著加速训练过程。

Result: 在基准拓扑优化问题（柔度最小化、热传导优化、柔顺机构设计）以及热机械拓扑优化等单/多材料多物理场问题上验证了框架有效性，生成具有锐利界面和物理可解释材料分布的超分辨率拓扑，结果通过开源代码和COMSOL商业软件验证。

Conclusion: 提出的PIGP框架能够有效同时解决耦合多物理场和设计问题，克服了传统ML方法在复杂物理问题中的局限，为多材料、多物理场拓扑优化提供了高效且物理可解释的解决方案。

Abstract: Machine learning (ML) has been increasingly used for topology optimization (TO). However, most existing ML-based approaches focus on simplified benchmark problems due to their high computational cost, spectral bias, and difficulty in handling complex physics. These limitations become more pronounced in multi-material, multi-physics problems whose objective or constraint functions are not self-adjoint. To address these challenges, we propose a framework based on physics-informed Gaussian processes (PIGPs). In our approach, the primary, adjoint, and design variables are represented by independent GP priors whose mean functions are parametrized via neural networks whose architectures are particularly beneficial for surrogate modeling of PDE solutions. We estimate all parameters of our model simultaneously by minimizing a loss that is based on the objective function, multi-physics potential energy functionals, and design-constraints. We demonstrate the capability of the proposed framework on benchmark TO problems such as compliance minimization, heat conduction optimization, and compliant mechanism design under single- and multi-material settings. Additionally, we leverage thermo-mechanical TO with single- and multi-material options as a representative multi-physics problem. We also introduce differentiation and integration schemes that dramatically accelerate the training process. Our results demonstrate that the proposed PIGP framework can effectively solve coupled multi-physics and design problems simultaneously -- generating super-resolution topologies with sharp interfaces and physically interpretable material distributions. We validate these results using open-source codes and the commercial software package COMSOL.

</details>


### [79] [Grassmannian Mixture-of-Experts: Concentration-Controlled Routing on Subspace Manifolds](https://arxiv.org/abs/2602.17798)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 提出Grassmannian MoE (GrMoE)，在Grassmannian流形上使用Matrix Bingham分布控制路由稀疏性，通过单一浓度参数Λ连续调节路由熵，替代离散的top-k选择，实现无路由崩溃的专家分配。


<details>
  <summary>Details</summary>
Motivation: 传统MoE模型使用softmax门控缺乏控制稀疏性与利用率权衡的原则性机制，且容易发生专家崩溃问题。需要一种能连续控制路由稀疏性、提供可解释性、并能避免专家崩溃的路由框架。

Method: 在Grassmannian流形上构建路由框架，使用Matrix Bingham分布的浓度参数作为门控权重。开发摊销变分推理程序用于后验路由分布，实现不确定性感知的专家分配。理论上证明浓度谱与路由熵、期望top-k质量及专家崩溃指数界的关系。

Result: 在350M/8专家、1.3B/16专家、2.7B/32专家的MoE语言模型上，GrMoE实现0%路由崩溃，困惑度相当或更好，负载平衡提升15-30%。浓度与有效稀疏性呈现平滑单调关系，支持训练后稀疏性调节。token级分析显示专家学习到与语言专业化相关的异质浓度值。

Conclusion: GrMoE提供了一种几何原理驱动的稀疏性控制机制，通过单一可解释参数连续调节路由熵，避免了专家崩溃问题，实现了更好的负载平衡和可解释的路由行为，为浓度控制的稀疏性建立了首个形式化理论。

Abstract: Mixture-of-Experts models rely on learned routers to assign tokens to experts, yet standard softmax gating provides no principled mechanism to control the tradeoff between sparsity and utilization. We propose Grassmannian MoE (GrMoE), a routing framework that operates on the Grassmannian manifold of subspaces, where gating weights arise from the concentration parameters of Matrix Bingham distributions. This construction yields a single, interpretable knob -- the concentration matrix $Λ$ -- that continuously controls routing entropy, replacing discrete top-$k$ selection with a smooth, geometrically principled sparsity mechanism. We further develop an amortized variational inference procedure for posterior routing distributions, enabling uncertainty-aware expert assignment that naturally resists expert collapse. We formally prove tight bounds relating the Bingham concentration spectrum to routing entropy, expected top-$k$ mass, and an exponential bound on expert collapse, establishing the first formal theory of concentration-controlled sparsity. On synthetic routing tasks, a 350M-parameter MoE language model with 8 experts, a 1.3B-parameter model with 16 experts, and a 2.7B-parameter model with 32 experts, GrMoE achieves 0\% routing collapse across all seeds, comparable or better perplexity with 15--30\% improved load balance, and a smooth monotonic relationship between concentration and effective sparsity that enables post-hoc sparsity tuning without retraining. Token-level analysis reveals that experts learn heterogeneous concentration values that correlate with linguistic specialization, providing interpretable routing behavior.

</details>


### [80] [Calibrated Adaptation: Bayesian Stiefel Manifold Priors for Reliable Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2602.17809)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: SBA是一种贝叶斯参数高效微调框架，在Stiefel流形上使用Matrix Langevin先验，通过切空间拉普拉斯近似进行后验推断，提供校准的不确定性估计，在领域偏移下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法（如LoRA）缺乏原则性的不确定性估计，导致预测校准差且在领域偏移下不可靠。需要一种能提供校准预测不确定性的贝叶斯PEFT框架。

Method: 在Stiefel流形上放置Matrix Langevin先验于正交适配器因子，通过切空间拉普拉斯近似与测地线回缩进行近似后验推断。相比平坦空间的高斯先验投影到正交约束，该方法在流形上自然编码适配器子空间应良好条件且正交的归纳偏置。

Result: 在RoBERTa-large、LLaMA-2-7B/13B、Mistral-7B、Qwen2.5-7B等模型上，SBA在GLUE和SuperGLUE基准上达到与LoRA和DoRA相当的任务性能，同时将预期校准误差降低18-34%，在领域偏移下将选择性预测AUROC提高12-25%，在OOD检测上以更少参数优于五个LoRA模型的深度集成。

Conclusion: 将不确定性放置在正确的几何结构上比简单地为适配器添加任何贝叶斯处理更重要。SBA提供了校准的预测不确定性而无需重新校准，在流形上的内在推理具有严格的理论优势。

Abstract: Parameter-efficient fine-tuning methods such as LoRA enable practical adaptation of large language models but provide no principled uncertainty estimates, leading to poorly calibrated predictions and unreliable behavior under domain shift. We introduce Stiefel-Bayes Adapters (SBA), a Bayesian PEFT framework that places a Matrix Langevin prior over orthonormal adapter factors on the Stiefel manifold $\St$ and performs approximate posterior inference via tangent space Laplace approximation with geodesic retraction. Unlike Gaussian priors in flat space projected onto orthogonality constraints, our prior on the manifold naturally encodes the inductive bias that adapter subspaces should be well conditioned and orthogonal, while the posterior provides calibrated predictive uncertainty without recalibration. We prove formally that the tangent space approximation strictly avoids the structural variance inflation inherent in projecting from ambient space, establishing a rigorous theoretical advantage for intrinsic manifold inference. Across GLUE and SuperGLUE benchmarks on RoBERTa-large, LLaMA-2-7B, LLaMA-2-13B, Mistral-7B, and Qwen2.5-7B, domain shift evaluations, selective prediction protocols, and an abstractive summarization task, SBA achieves task performance comparable to LoRA and DoRA while reducing Expected Calibration Error by 18 to 34\% over deterministic baselines, improving selective prediction AUROC by 12 to 25\% under domain shift, and outperforming deep ensembles of five LoRA models on OOD detection at a fraction of the parameter cost. Our results demonstrate that where you place uncertainty, on the right geometric structure, matters more than simply adding any Bayesian treatment to adapters.

</details>


### [81] [Avoid What You Know: Divergent Trajectory Balance for GFlowNets](https://arxiv.org/abs/2602.17827)
*Pedro Dall'Antonia,Tiago da Silva,Daniel Csillag,Salem Lahlou,Diego Mesquita*

Main category: cs.LG

TL;DR: 提出ACE算法，通过训练一个探索型GFlowNet专门搜索未被充分探索的高奖励区域，显著提升GFlowNets的学习效率和多样性发现能力。


<details>
  <summary>Details</summary>
Motivation: GFlowNets在训练过程中探索效率受限，现有方法（如好奇心驱动搜索和自监督随机网络蒸馏）会浪费样本在已充分探索的区域，需要更有效的探索策略来发现新颖且高概率的区域。

Method: 提出自适应互补探索（ACE）算法，训练一个专门的探索型GFlowNet来搜索未被标准GFlowNet充分探索的高奖励状态区域，形成互补探索机制。

Result: ACE在目标分布的近似精度和多样高奖励状态的发现率方面显著优于先前工作，通过实验验证了其有效性。

Conclusion: ACE为GFlowNets提供了一种原则性的有效探索算法，通过专门的探索网络实现互补搜索，解决了现有探索方法效率低下的问题。

Abstract: Generative Flow Networks (GFlowNets) are a flexible family of amortized samplers trained to generate discrete and compositional objects with probability proportional to a reward function. However, learning efficiency is constrained by the model's ability to rapidly explore diverse high-probability regions during training. To mitigate this issue, recent works have focused on incentivizing the exploration of unvisited and valuable states via curiosity-driven search and self-supervised random network distillation, which tend to waste samples on already well-approximated regions of the state space. In this context, we propose Adaptive Complementary Exploration (ACE), a principled algorithm for the effective exploration of novel and high-probability regions when learning GFlowNets. To achieve this, ACE introduces an exploration GFlowNet explicitly trained to search for high-reward states in regions underexplored by the canonical GFlowNet, which learns to sample from the target distribution. Through extensive experiments, we show that ACE significantly improves upon prior work in terms of approximation accuracy to the target distribution and discovery rate of diverse high-reward states.

</details>


### [82] [Causality by Abstraction: Symbolic Rule Learning in Multivariate Timeseries with Large Language Models](https://arxiv.org/abs/2602.17829)
*Preetom Biswas,Giulia Pedrielli,K. Selçuk Candan*

Main category: cs.LG

TL;DR: ruleXplain：利用大语言模型从仿真驱动的动力系统中提取可验证因果规则的解释框架


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理具有延迟效应的时序数据因果推断时，难以产生泛化且可解释的解释，因为多个不同的输入轨迹可能产生几乎无法区分的输出。需要一种能够从复杂动力系统中提取形式化解释的方法。

Method: 提出ruleXplain框架，利用LLMs从仿真驱动的动力系统中提取形式化解释。引入具有时间算子和延迟语义的约束符号规则语言，通过结构化提示让LLMs生成可验证的因果规则。使用仿真器生成多样化的反事实输入轨迹作为候选解释，聚类后提供给LLM生成编码联合时间趋势的符号规则，并通过闭环精炼过程确保规则一致性和语义有效性。

Result: 在PySIRTEM流行病仿真器（测试率输入到每日感染数）和EnergyPlus建筑能耗仿真器（温度和太阳辐照度输入到电力需求）上验证。通过三类实验验证：(1) 通过输入重建评估规则集效果；(2) 消融研究评估规则集的因果编码；(3) 在不同相位动态的未见输出趋势上测试提取规则的泛化能力。

Conclusion: ruleXplain框架能够从复杂动力系统中提取可验证的因果规则，为具有延迟效应的时序数据提供泛化且可解释的因果推断方法，在流行病学和建筑能耗等仿真驱动系统中表现出良好效果。

Abstract: Inferring causal relations in timeseries data with delayed effects is a fundamental challenge, especially when the underlying system exhibits complex dynamics that cannot be captured by simple functional mappings. Traditional approaches often fail to produce generalized and interpretable explanations, as multiple distinct input trajectories may yield nearly indistinguishable outputs. In this work, we present ruleXplain, a framework that leverages Large Language Models (LLMs) to extract formal explanations for input-output relations in simulation-driven dynamical systems. Our method introduces a constrained symbolic rule language with temporal operators and delay semantics, enabling LLMs to generate verifiable causal rules through structured prompting. ruleXplain relies on the availability of a principled model (e.g., a simulator) that maps multivariate input time series to output time series. Within ruleXplain, the simulator is used to generate diverse counterfactual input trajectories that yield similar target output, serving as candidate explanations. Such counterfactual inputs are clustered and provided as context to the LLM, which is tasked with the generation of symbolic rules encoding the joint temporal trends responsible for the patterns observable in the output times series. A closed-loop refinement process ensures rule consistency and semantic validity. We validate the framework using the PySIRTEM epidemic simulator, mapping testing rate inputs to daily infection counts; and the EnergyPlus building energy simulator, observing temperature and solar irradiance inputs to electricity needs. For validation, we perform three classes of experiments: (1) the efficacy of the ruleset through input reconstruction; (2) ablation studies evaluating the causal encoding of the ruleset; and (3) generalization tests of the extracted rules across unseen output trends with varying phase dynamics.

</details>


### [83] [MePoly: Max Entropy Polynomial Policy Optimization](https://arxiv.org/abs/2602.17832)
*Hang Liu,Sangli Teng,Maani Ghaffari*

Main category: cs.LG

TL;DR: 提出MePoly，一种基于多项式能量模型的策略参数化方法，为随机最优控制提供显式可处理的概率密度，解决扩散策略缺乏显式密度的问题


<details>
  <summary>Details</summary>
Motivation: 传统参数化策略难以表示解决方案的多模态性，而基于扩散的策略缺乏显式概率密度，使策略梯度优化复杂化

Method: 基于多项式能量模型（EBM）的策略参数化方法MePoly，利用经典矩问题的理论基础，提供显式可处理的概率密度

Result: MePoly能有效捕捉复杂的非凸流形，在多样化基准测试中性能优于基线方法

Conclusion: MePoly通过提供显式概率密度，实现了精确的熵最大化，为多模态决策问题提供了有效的解决方案

Abstract: Stochastic Optimal Control provides a unified mathematical framework for solving complex decision-making problems, encompassing paradigms such as maximum entropy reinforcement learning(RL) and imitation learning(IL). However, conventional parametric policies often struggle to represent the multi-modality of the solutions. Though diffusion-based policies are aimed at recovering the multi-modality, they lack an explicit probability density, which complicates policy-gradient optimization. To bridge this gap, we propose MePoly, a novel policy parameterization based on polynomial energy-based models. MePoly provides an explicit, tractable probability density, enabling exact entropy maximization. Theoretically, we ground our method in the classical moment problem, leveraging the universal approximation capabilities for arbitrary distributions. Empirically, we demonstrate that MePoly effectively captures complex non-convex manifolds and outperforms baselines in performance across diverse benchmarks.

</details>


### [84] [Influence-Preserving Proxies for Gradient-Based Data Selection in LLM Fine-tuning](https://arxiv.org/abs/2602.17835)
*Sirui Chen,Yunzhe Qi,Mengting Ai,Yifan Sun,Ruizhong Qiu,Jiaru Zou,Jingrui He*

Main category: cs.LG

TL;DR: Iprox是一个两阶段框架，通过从目标大语言模型中直接构建保持影响力的代理模型，解决了梯度数据选择方法计算成本高的问题，使基于梯度的数据选择对LLMs更具可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有梯度数据选择方法（如TracIn和Influence Functions）计算成本高，不适用于数十亿参数的大语言模型。使用现成的小模型作为代理效果不佳，因为其学习动态不明确、大小无法灵活调整，且无法与目标模型在梯度影响力估计上对齐。

Method: Iprox采用两阶段框架：1）低秩压缩阶段，保留目标模型的影响力信息；2）对齐阶段，对齐模型梯度和logits，从而构建可灵活控制计算成本同时保持目标模型影响力的代理模型。

Result: 在多种LLM家族和评估任务上的实验结果表明，Iprox始终优于现成的代理模型和基线方法。在Qwen3-4B上，使用Iprox构建的1.5B代理比更大的1.7B现成代理表现更好。在Llama3.2上，Iprox在比完整3B模型减少一半以上计算成本的同时，实现了比基线更好的性能。

Conclusion: Iprox提供了有效的保持影响力的代理模型，使基于梯度的数据选择对大语言模型更具可扩展性，解决了现有方法计算成本高的问题。

Abstract: Supervised fine-tuning (SFT) relies critically on selecting training data that most benefits a model's downstream performance. Gradient-based data selection methods such as TracIn and Influence Functions leverage influence to identify useful samples, but their computational cost scales poorly, making them impractical for multi-billion-parameter large language models (LLMs). A common alternative is to use off-the-shelf smaller models as proxies, but they remain suboptimal since their learning dynamics are unclear, their sizes cannot be flexibly adjusted, and they cannot be further aligned with the target model in terms of gradient-based influence estimation. To address these challenges, we introduce Iprox, a two-stage framework that derives influence-preserving proxies directly from the target model. It first applies a low-rank compression stage to preserve influence information of the target model, and then an aligning stage to align both model gradients and logits, thereby constructing proxies that flexibly control computational cost while retaining the target model's influence. Experimental results across diverse LLM families and evaluation tasks show that Iprox consistently outperforms off-the-shelf proxies and baseline methods. On Qwen3-4B, a 1.5B proxy constructed with Iprox achieves stronger performance than the larger 1.7B off-the-shelf proxy. Notably, on Llama3.2, Iprox achieves better performance than baselines while reducing computational cost by more than half relative to the full 3B model. These results show that Iprox provides effective influence-preserving proxies, making gradient-based data selection more scalable for LLMs.

</details>


### [85] [Two Calm Ends and the Wild Middle: A Geometric Picture of Memorization in Diffusion Models](https://arxiv.org/abs/2602.17846)
*Nick Dodson,Xinyu Gao,Qingsong Wang,Yusu Wang,Zhengchao Wan*

Main category: cs.LG

TL;DR: 扩散模型存在记忆训练数据风险，作者提出几何框架将噪声调度分为三个区域，揭示记忆风险在不同噪声水平下高度不均匀，并识别出记忆最严重的中等噪声危险区。


<details>
  <summary>Details</summary>
Motivation: 扩散模型能生成高质量样本但也可能记忆训练数据，引发隐私担忧。目前对记忆与泛化机制的理解不足，特别是记忆在噪声调度中的位置、数据几何的影响以及不同噪声尺度现象的相互作用。

Method: 引入几何框架，基于高斯壳覆盖训练数据的特性和后验集中行为这两个控制记忆与泛化的基本对象，将噪声调度分为三个区域。提出几何感知的针对性干预来缓解记忆。

Result: 记忆风险在噪声水平间高度不均匀，中等噪声区域是记忆最严重的危险区。小噪声区因训练覆盖有限而避免记忆，大噪声区后验集中度低且可证明接近线性高斯去噪行为。

Conclusion: 扩散模型的记忆风险集中在特定噪声区域，通过几何框架可识别记忆危险区并设计针对性干预措施，为理解和缓解扩散模型记忆问题提供了新视角。

Abstract: Diffusion models generate high-quality samples but can also memorize training data, raising serious privacy concerns. Understanding the mechanisms governing when memorization versus generalization occurs remains an active area of research. In particular, it is unclear where along the noise schedule memorization is induced, how data geometry influences it, and how phenomena at different noise scales interact. We introduce a geometric framework that partitions the noise schedule into three regimes based on the coverage properties of training data by Gaussian shells and the concentration behavior of the posterior, which we argue are two fundamental objects governing memorization and generalization in diffusion models. This perspective reveals that memorization risk is highly non-uniform across noise levels. We further identify a danger zone at medium noise levels where memorization is most pronounced. In contrast, both the small and large noise regimes resist memorization, but through fundamentally different mechanisms: small noise avoids memorization due to limited training coverage, while large noise exhibits low posterior concentration and admits a provably near linear Gaussian denoising behavior. For the medium noise regime, we identify geometric conditions through which we propose a geometry-informed targeted intervention that mitigates memorization.

</details>


### [86] [Dual Length Codes for Lossless Compression of BFloat16](https://arxiv.org/abs/2602.17849)
*Aditya Agrawal,Albert Magyar,Hiteshwar Eswaraiah,Patrick Sheridan,Pradeep Janedula,Ravi Krishnan Venkatesan,Krishna Nair,Ravi Iyer*

Main category: cs.LG

TL;DR: 本文提出Dual Length Codes，一种混合编码方案，在压缩效率和解码速度之间取得平衡，针对LLM中的网络带宽瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 训练和服务大型语言模型（LLMs）严重依赖并行化和集体操作，这些操作经常受限于网络带宽。虽然无损压缩（如霍夫曼编码）可以缓解这个问题，但霍夫曼编码存在解码速度慢（需要逐位顺序解码）和硬件复杂度高（由于深度树遍历）的问题。通用编码（如指数哥伦布编码）解码更快，但不能利用符号频率分布。

Method: 提出Dual Length Codes混合方法：分析Gemma模型的BFloat16张量，发现前8个最频繁符号约占累积概率的50%。这8个符号分配4位短码，其余248个符号分配9位长码。编码方案使用单个前缀位来区分两种码长，仅需8个条目的小型查找表进行编码和解码。

Result: 该方案实现了18.6%的压缩率，而霍夫曼编码为21.3%。虽然压缩率略低，但显著提高了解码速度并简化了硬件复杂度。

Conclusion: Dual Length Codes在压缩效率和解码速度之间取得了良好平衡，通过简单的双长度编码方案和小的查找表，有效解决了LLM中网络带宽瓶颈问题，同时保持了硬件实现的简洁性。

Abstract: Training and serving Large Language Models (LLMs) relies heavily on parallelization and collective operations, which are frequently bottlenecked by network bandwidth. Lossless compression using e.g., Huffman codes can alleviate the issue, however, Huffman codes suffer from slow, bit-sequential decoding and high hardware complexity due to deep tree traversals. Universal codes e.g., Exponential-Golomb codes are faster to decode but do not exploit the symbol frequency distributions. To address these limitations, this paper introduces Dual Length Codes, a hybrid approach designed to balance compression efficiency with decoding speed. Analyzing BFloat16 tensors from the Gemma model, we observed that the top 8 most frequent symbols account for approximately 50% of the cumulative probability. These 8 symbols are assigned a short 4 bit code. The remaining 248 symbols are assigned a longer 9 bit code. The coding scheme uses a single prefix bit to distinguish between the two code lengths. The scheme uses a small Look Up Table with only 8 entries for encoding and decoding. The scheme achieves a compressibility of 18.6% in comparison to 21.3% achieved by Huffman codes, but it significantly speeds up the decoding and simplifies the hardware complexity.

</details>


### [87] [Neural Prior Estimation: Learning Class Priors from Latent Representations](https://arxiv.org/abs/2602.17853)
*Masoud Yavari,Payman Moallem*

Main category: cs.LG

TL;DR: 提出Neural Prior Estimator (NPE)框架，通过特征条件化的对数先验估计解决类别不平衡问题，无需显式类别计数或分布特定超参数


<details>
  <summary>Details</summary>
Motivation: 类别不平衡会导致深度神经网络产生系统性偏差，因为倾斜的有效类别先验会影响模型性能。现有方法通常需要显式的类别计数或分布假设，缺乏理论依据和适应性。

Method: 提出NPE框架，学习特征条件化的对数先验估计。使用一个或多个先验估计模块与主干网络联合训练，通过单向逻辑损失优化。在神经坍缩机制下，NPE理论上可恢复类别对数先验（至加性常数）。将学习到的估计融入logit调整，形成NPE-LA进行偏差感知预测。

Result: 在长尾CIFAR和不平衡语义分割基准（STARE、ADE20K）上的实验表明，NPE-LA能带来一致改进，特别是对代表性不足的类别。方法轻量且理论上有依据。

Conclusion: NPE提供了一种轻量级、理论上有依据的学习先验估计方法，能够有效处理类别不平衡问题，改善对少数类别的预测性能。

Abstract: Class imbalance induces systematic bias in deep neural networks by imposing a skewed effective class prior. This work introduces the Neural Prior Estimator (NPE), a framework that learns feature-conditioned log-prior estimates from latent representations. NPE employs one or more Prior Estimation Modules trained jointly with the backbone via a one-way logistic loss. Under the Neural Collapse regime, NPE is analytically shown to recover the class log-prior up to an additive constant, providing a theoretically grounded adaptive signal without requiring explicit class counts or distribution-specific hyperparameters. The learned estimate is incorporated into logit adjustment, forming NPE-LA, a principled mechanism for bias-aware prediction. Experiments on long-tailed CIFAR and imbalanced semantic segmentation benchmarks (STARE, ADE20K) demonstrate consistent improvements, particularly for underrepresented classes. NPE thus offers a lightweight and theoretically justified approach to learned prior estimation and imbalance-aware prediction.

</details>


### [88] [JAX-Privacy: A library for differentially private machine learning](https://arxiv.org/abs/2602.17861)
*Ryan McKenna,Galen Andrew,Borja Balle,Vadym Doroshenko,Arun Ganesh,Weiwei Kong,Alex Kurakin,Brendan McMahan,Mikhail Pravilov*

Main category: cs.LG

TL;DR: JAX-Privacy是一个用于简化差分隐私机器学习机制部署的库，兼顾易用性、灵活性和效率，为研究者和实践者提供模块化组件。


<details>
  <summary>Details</summary>
Motivation: 差分隐私机器学习在实际部署中面临复杂性和性能挑战，需要一个统一的工具来简化机制设计、实现和验证过程，同时支持深度定制和开箱即用两种需求。

Method: 基于JAX框架构建，提供模块化原语组件，包括批次选择、梯度裁剪、噪声添加、隐私预算计算和审计等关键环节，整合了差分隐私ML领域的最新研究成果。

Result: 开发了一个功能完整的差分隐私机器学习库，支持从研究到生产的全流程，提供了经过验证的模块化组件，能够满足不同用户群体的需求。

Conclusion: JAX-Privacy成功构建了一个兼顾易用性、灵活性和效率的差分隐私机器学习工具库，为研究者和实践者提供了统一的解决方案，有助于推动差分隐私ML的实际应用。

Abstract: JAX-Privacy is a library designed to simplify the deployment of robust and performant mechanisms for differentially private machine learning. Guided by design principles of usability, flexibility, and efficiency, JAX-Privacy serves both researchers requiring deep customization and practitioners who want a more out-of-the-box experience. The library provides verified, modular primitives for critical components for all aspects of the mechanism design including batch selection, gradient clipping, noise addition, accounting, and auditing, and brings together a large body of recent research on differentially private ML.

</details>


### [89] [Financial time series augmentation using transformer based GAN architecture](https://arxiv.org/abs/2602.17865)
*Andrzej Podobiński,Jarosław A. Chudziak*

Main category: cs.LG

TL;DR: 使用基于Transformer的GAN生成合成金融时间序列数据，增强LSTM预测模型的训练数据，显著提升比特币和标普500价格预测准确性。


<details>
  <summary>Details</summary>
Motivation: 金融时间序列数据通常稀缺且波动性大，导致深度学习模型训练不足、泛化能力差。需要可靠的数据增强方法来提升预测模型的准确性。

Method: 提出使用基于Transformer的GAN（TTS-GAN）生成合成金融时间序列数据作为数据增强工具，并训练LSTM预测模型。同时提出结合动态时间规整（DTW）和改进的深度数据集相似度度量（DeD-iMs）的时间序列质量评估指标。

Result: 在比特币和标普500价格数据上，使用GAN增强数据的LSTM模型预测准确性显著优于仅使用真实数据的模型，且在不同预测时间范围内都表现良好。

Conclusion: GAN作为数据增强工具能有效克服金融时间序列数据稀缺问题，显著提升深度学习预测模型的性能，为金融预测提供了有前景的技术方案。

Abstract: Time-series forecasting is a critical task across many domains, from engineering to economics, where accurate predictions drive strategic decisions. However, applying advanced deep learning models in challenging, volatile domains like finance is difficult due to the inherent limitation and dynamic nature of financial time series data. This scarcity often results in sub-optimal model training and poor generalization. The fundamental challenge lies in determining how to reliably augment scarce financial time series data to enhance the predictive accuracy of deep learning forecasting models. Our main contribution is a demonstration of how Generative Adversarial Networks (GANs) can effectively serve as a data augmentation tool to overcome data scarcity in the financial domain. Specifically, we show that training a Long Short-Term Memory (LSTM) forecasting model on a dataset augmented with synthetic data generated by a transformer-based GAN (TTS-GAN) significantly improves the forecasting accuracy compared to using real data alone. We confirm these results across different financial time series (Bitcoin and S\&P500 price data) and various forecasting horizons. Furthermore, we propose a novel, time series specific quality metric that combines Dynamic Time Warping (DTW) and a modified Deep Dataset Dissimilarity Measure (DeD-iMs) to reliably monitor the training progress and evaluate the quality of the generated data. These findings provide compelling evidence for the benefits of GAN-based data augmentation in enhancing financial predictive capabilities.

</details>


### [90] [ADAPT: Hybrid Prompt Optimization for LLM Feature Visualization](https://arxiv.org/abs/2602.17867)
*João N. Cardoso,Arlindo L. Oliveira,Bruno Martins*

Main category: cs.LG

TL;DR: ADAPT是一种结合束搜索初始化和自适应梯度引导变异的混合方法，用于优化文本输入以最大化激活LLM中的特定方向，解决了文本离散性和局部最优问题。


<details>
  <summary>Details</summary>
Motivation: 理解LLM激活空间中学习方向编码的特征需要识别强烈激活这些方向的输入。特征可视化通过优化输入来最大化激活目标方向，但文本的离散性和现有提示优化技术容易陷入局部最优，限制了其在LLM中的应用。

Method: ADAPT是一种混合方法，结合了束搜索初始化和自适应梯度引导变异。该方法针对文本离散性和局部最优问题设计，使用基于数据集激活统计的指标进行严格评估。

Result: 在Gemma 2 2B的稀疏自编码器潜在空间上评估，ADAPT在不同层和潜在类型上始终优于先前方法，证明LLM特征可视化是可行的。

Conclusion: LLM的特征可视化是可行的，但需要针对该领域特点进行专门的设计假设。ADAPT方法为解决文本离散性和局部最优问题提供了有效方案。

Abstract: Understanding what features are encoded by learned directions in LLM activation space requires identifying inputs that strongly activate them. Feature visualization, which optimizes inputs to maximally activate a target direction, offers an alternative to costly dataset search approaches, but remains underexplored for LLMs due to the discrete nature of text. Furthermore, existing prompt optimization techniques are poorly suited to this domain, which is highly prone to local minima. To overcome these limitations, we introduce ADAPT, a hybrid method combining beam search initialization with adaptive gradient-guided mutation, designed around these failure modes. We evaluate on Sparse Autoencoder latents from Gemma 2 2B, proposing metrics grounded in dataset activation statistics to enable rigorous comparison, and show that ADAPT consistently outperforms prior methods across layers and latent types. Our results establish that feature visualization for LLMs is tractable, but requires design assumptions tailored to the domain.

</details>


### [91] [MantisV2: Closing the Zero-Shot Gap in Time Series Classification with Synthetic Data and Test-Time Strategies](https://arxiv.org/abs/2602.17868)
*Vasilii Feofanov,Songkang Wen,Jianfeng Zhang,Lujia Pan,Ievgen Redko*

Main category: cs.LG

TL;DR: MantisV2和Mantis+时间序列基础模型在零样本特征提取方面取得显著进步，通过合成数据预训练、架构优化和测试时方法改进，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 开发时间序列分类的基础模型具有重要实践价值，可作为通用特征提取器用于多种下游任务。虽然早期模型如Mantis已显示出潜力，但冻结编码器和微调编码器之间仍存在显著性能差距。

Method: 1. 提出Mantis+：完全在合成时间序列上预训练的Mantis变体；2. 通过受控消融研究改进架构，得到更轻量化的MantisV2编码器；3. 提出增强的测试时方法，利用中间层表示并改进输出标记聚合；4. 通过自集成和跨模型嵌入融合进一步提升性能。

Result: 在UCR、UEA、人类活动识别(HAR)基准测试和EEG数据集上的广泛实验表明，MantisV2和Mantis+始终优于先前的时间序列基础模型，实现了最先进的零样本性能。

Conclusion: 该工作显著加强了时间序列的零样本特征提取能力，通过合成数据预训练、架构优化和测试时方法改进，为时间序列基础模型的发展提供了重要进展。

Abstract: Developing foundation models for time series classification is of high practical relevance, as such models can serve as universal feature extractors for diverse downstream tasks. Although early models such as Mantis have shown the promise of this approach, a substantial performance gap remained between frozen and fine-tuned encoders. In this work, we introduce methods that significantly strengthen zero-shot feature extraction for time series. First, we introduce Mantis+, a variant of Mantis pre-trained entirely on synthetic time series. Second, through controlled ablation studies, we refine the architecture and obtain MantisV2, an improved and more lightweight encoder. Third, we propose an enhanced test-time methodology that leverages intermediate-layer representations and refines output-token aggregation. In addition, we show that performance can be further improved via self-ensembling and cross-model embedding fusion. Extensive experiments on UCR, UEA, Human Activity Recognition (HAR) benchmarks, and EEG datasets show that MantisV2 and Mantis+ consistently outperform prior time series foundation models, achieving state-of-the-art zero-shot performance.

</details>


### [92] [Machine Learning Based Prediction of Surgical Outcomes in Chronic Rhinosinusitis from Clinical Data](https://arxiv.org/abs/2602.17888)
*Sayeed Shafayet Chowdhury,Karen D'Souza,V. Siva Kakumani,Snehasis Mukhopadhyay,Shiaofen Fang,Rodney J. Schlosser,Daniel M. Beswick,Jeremiah A. Alt,Jess C. Mace,Zachary M. Soler,Timothy L. Smith,Vijay R. Ramakrishnan*

Main category: cs.LG

TL;DR: 该研究开发了机器学习模型，利用术前数据预测慢性鼻窦炎患者的手术获益，模型准确率达85%，在30例测试病例中表现优于临床专家


<details>
  <summary>Details</summary>
Motivation: 慢性鼻窦炎（CRS）手术决策复杂，需要权衡手术风险与个体化结果的不确定性。虽然AI在医疗预后中应用广泛，但在前瞻性观察性临床试验中应用机器学习预测手术获益的研究仍不足，这有潜力降低医疗成本并改善患者结局。

Method: 使用前瞻性收集的观察性干预试验队列数据，所有患者均接受了手术。研究监督式机器学习模型，仅基于术前数据预测手术获益，以Sino-Nasal Outcome Test-22（SNOT-22）作为主要患者报告结局指标。采用多种算法包括集成方法，并在30例混合难度病例的保留测试集上评估模型性能。

Result: 最佳模型达到约85%的分类准确率，能够准确且可解释地预测手术候选资格。在30例保留测试病例中，模型准确率达80%，超过了临床专家平均预测准确率（75.6%）。

Conclusion: 该研究证明机器学习模型能够有效预测CRS患者的手术获益，性能优于临床专家，有潜力增强临床决策支持，实现个性化CRS治疗。

Abstract: Artificial intelligence (AI) has increasingly transformed medical prognostics by enabling rapid and accurate analysis across imaging and pathology. However, the investigation of machine learning predictions applied to prospectively collected, standardized data from observational clinical intervention trials remains underexplored, despite its potential to reduce costs and improve patient outcomes. Chronic rhinosinusitis (CRS), a persistent inflammatory disease of the paranasal sinuses lasting more than three months, imposes a substantial burden on quality of life (QoL) and societal cost. Although many patients respond to medical therapy, others with refractory symptoms often pursue surgical intervention. Surgical decision-making in CRS is complex, as it must weigh known procedural risks against uncertain individualized outcomes. In this study, we evaluated supervised machine learning models for predicting surgical benefit in CRS, using the Sino-Nasal Outcome Test-22 (SNOT-22) as the primary patient-reported outcome. Our prospectively collected cohort from an observational intervention trial comprised patients who all underwent surgery; we investigated whether models trained only on preoperative data could identify patients who might not have been recommended surgery prior to the procedure. Across multiple algorithms, including an ensemble approach, our best model achieved approximately 85% classification accuracy, providing accurate and interpretable predictions of surgical candidacy. Moreover, on a held-out set of 30 cases spanning mixed difficulty, our model achieved 80% accuracy, exceeding the average prediction accuracy of expert clinicians (75.6%), demonstrating its potential to augment clinical decision-making and support personalized CRS care.

</details>


### [93] [COMBA: Cross Batch Aggregation for Learning Large Graphs with Context Gating State Space Models](https://arxiv.org/abs/2602.17893)
*Jiajun Shen,Yufei Jin,Yi He,xingquan Zhu*

Main category: cs.LG

TL;DR: COMBA：使用状态空间模型进行大规模图学习，通过图上下文门控和跨批次聚合解决图到序列转换的挑战


<details>
  <summary>Details</summary>
Motivation: 状态空间模型（SSMs）在序列数据的长程依赖建模方面表现出色，但将其应用于图结构数据（特别是大规模图）面临挑战，因为SSMs是序列模型，而将图转换为序列进行有效学习的计算成本极高

Method: 提出COMBA框架，包含两个关键创新：1）图上下文门控：利用不同跳数的邻居上下文来学习最佳邻居聚合控制；2）跨批次聚合：对每个图上下文采样节点批次，训练图神经网络（GNN），并在批次间聚合信息，实现大规模图的可扩展学习

Result: 理论分析表明跨批次聚合保证比无聚合训练GNN具有更低的误差；在基准网络上的实验显示相比基线方法有显著的性能提升

Conclusion: COMBA成功将状态空间模型应用于大规模图学习，通过图上下文门控和跨批次聚合机制解决了图到序列转换的挑战，实现了可扩展且性能优越的图学习框架

Abstract: State space models (SSMs) have recently emerged for modeling long-range dependency in sequence data, with much simplified computational costs than modern alternatives, such as transformers. Advancing SMMs to graph structured data, especially for large graphs, is a significant challenge because SSMs are sequence models and the shear graph volumes make it very expensive to convert graphs as sequences for effective learning. In this paper, we propose COMBA to tackle large graph learning using state space models, with two key innovations: graph context gating and cross batch aggregation. Graph context refers to different hops of neighborhood for each node, and graph context gating allows COMBA to use such context to learn best control of neighbor aggregation. For each graph context, COMBA samples nodes as batches, and train a graph neural network (GNN), with information being aggregated cross batches, allowing COMBA to scale to large graphs. Our theoretical study asserts that cross-batch aggregation guarantees lower error than training GNN without aggregation. Experiments on benchmark networks demonstrate significant performance gains compared to baseline approaches. Code and benchmark datasets will be released for public access.

</details>


### [94] [Breaking the Correlation Plateau: On the Optimization and Capacity Limits of Attention-Based Regressors](https://arxiv.org/abs/2602.17898)
*Jingquan Yan,Yuwei Miao,Peiran Yu,Junzhou Huang*

Main category: cs.LG

TL;DR: 论文首次理论分析了注意力回归模型中PCC停滞现象，揭示了优化动态和模型容量的根本限制，并提出ECA方法突破PCC瓶颈


<details>
  <summary>Details</summary>
Motivation: 注意力回归模型训练中常出现PCC停滞现象：即使MSE持续下降，PCC却早期停止改进。这种现象缺乏理论解释，阻碍了模型在形状匹配任务上的性能提升

Method: 1) 理论分析PCC停滞的两个根本原因：MSE优化与PCC梯度的冲突，以及softmax注意力等凸聚合器的PCC改进上限；2) 提出外推相关性注意力(ECA)，包含新机制改善PCC优化并超越凸包限制

Result: 在多样化基准测试中，ECA能持续突破PCC停滞，在相关性方面取得显著改进，同时不损害MSE性能，特别是在具有挑战性的同质数据设置中表现优异

Conclusion: 论文首次为PCC停滞现象提供了理论解释，揭示了注意力回归模型的基本限制，并提出有效的ECA方法突破这些限制，为改进形状匹配任务提供了新方向

Abstract: Attention-based regression models are often trained by jointly optimizing Mean Squared Error (MSE) loss and Pearson correlation coefficient (PCC) loss, emphasizing the magnitude of errors and the order or shape of targets, respectively. A common but poorly understood phenomenon during training is the PCC plateau: PCC stops improving early in training, even as MSE continues to decrease. We provide the first rigorous theoretical analysis of this behavior, revealing fundamental limitations in both optimization dynamics and model capacity. First, in regard to the flattened PCC curve, we uncover a critical conflict where lowering MSE (magnitude matching) can paradoxically suppress the PCC gradient (shape matching). This issue is exacerbated by the softmax attention mechanism, particularly when the data to be aggregated is highly homogeneous. Second, we identify a limitation in the model capacity: we derived a PCC improvement limit for any convex aggregator (including the softmax attention), showing that the convex hull of the inputs strictly bounds the achievable PCC gain. We demonstrate that data homogeneity intensifies both limitations. Motivated by these insights, we propose the Extrapolative Correlation Attention (ECA), which incorporates novel, theoretically-motivated mechanisms to improve the PCC optimization and extrapolate beyond the convex hull. Across diverse benchmarks, including challenging homogeneous data setting, ECA consistently breaks the PCC plateau, achieving significant improvements in correlation without compromising MSE performance.

</details>


### [95] [Distribution-Free Sequential Prediction with Abstentions](https://arxiv.org/abs/2602.17918)
*Jialin Yu,Moïse Blanchard*

Main category: cs.LG

TL;DR: 本文研究了一种半对抗性序列预测问题，学习者可以在实例被污染时弃权而不受惩罚。作者提出了无需先验分布知识的分布无关算法AbstainBoost，为VC类提供次线性误差保证。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设学习者已知干净样本的分布μ，这在理论和实践中都是强假设。本文旨在探索在分布未知的情况下，是否仍能获得类似的学习保证，这与经典学习框架（如PAC学习）和其他非i.i.d.模型（如平滑在线学习）的标准一致。

Method: 提出了基于弱学习器提升过程的算法AbstainBoost，采用分布无关的弃权学习方法。算法通过提升过程处理未知分布情况，为VC类提供学习保证，并对结构化函数类（如线性分类器）扩展到自适应对手。

Result: AbstainBoost算法为一般VC类在分布无关的弃权学习中提供了次线性误差保证，适用于遗忘型对手。对于自适应对手，结构化函数类（包括线性分类器）也能获得类似保证。同时提供了相应的下界，揭示了误分类错误与错误弃权次数之间的多项式权衡关系。

Conclusion: 本文成功解决了在半对抗性设置中无需先验分布知识的学习问题，提出的AbstainBoost算法为VC类提供了分布无关的学习保证，填补了现有研究的空白，并揭示了错误类型之间的基本权衡关系。

Abstract: We study a sequential prediction problem in which an adversary is allowed to inject arbitrarily many adversarial instances in a stream of i.i.d.\ instances, but at each round, the learner may also \emph{abstain} from making a prediction without incurring any penalty if the instance was indeed corrupted. This semi-adversarial setting naturally sits between the classical stochastic case with i.i.d.\ instances for which function classes with finite VC dimension are learnable; and the adversarial case with arbitrary instances, known to be significantly more restrictive. For this problem, Goel et al. (2023) showed that, if the learner knows the distribution $μ$ of clean samples in advance, learning can be achieved for all VC classes without restrictions on adversary corruptions. This is, however, a strong assumption in both theory and practice: a natural question is whether similar learning guarantees can be achieved without prior distributional knowledge, as is standard in classical learning frameworks (e.g., PAC learning or asymptotic consistency) and other non-i.i.d.\ models (e.g., smoothed online learning). We therefore focus on the distribution-free setting where $μ$ is \emph{unknown} and propose an algorithm \textsc{AbstainBoost} based on a boosting procedure of weak learners, which guarantees sublinear error for general VC classes in \emph{distribution-free} abstention learning for oblivious adversaries. These algorithms also enjoy similar guarantees for adaptive adversaries, for structured function classes including linear classifiers. These results are complemented with corresponding lower bounds, which reveal an interesting polynomial trade-off between misclassification error and number of erroneous abstentions.

</details>


### [96] [MIRA: Memory-Integrated Reinforcement Learning Agent with Limited LLM Guidance](https://arxiv.org/abs/2602.17930)
*Narjes Nourzad,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: MIRA通过结构化记忆图整合LLM先验知识，减少对实时LLM监督的依赖，在稀疏奖励环境中加速强化学习训练


<details>
  <summary>Details</summary>
Motivation: 强化学习在稀疏或延迟奖励环境中样本效率低，而大语言模型能提供子目标分解等先验知识，但过度依赖LLM实时监督存在可扩展性和可靠性问题

Method: 构建结构化记忆图存储高回报经验和LLM输出，从中推导效用信号软调整优势估计，随着训练进展效用项衰减，保留标准收敛保证

Result: MIRA在稀疏奖励环境中优于RL基线，达到与频繁LLM监督方法相当的回报，同时显著减少在线LLM查询次数

Conclusion: 通过记忆图整合LLM先验知识能有效加速早期学习，减少对实时LLM监督的依赖，为稀疏奖励RL提供可扩展的解决方案

Abstract: Reinforcement learning (RL) agents often suffer from high sample complexity in sparse or delayed reward settings due to limited prior structure. Large language models (LLMs) can provide subgoal decompositions, plausible trajectories, and abstract priors that facilitate early learning. However, heavy reliance on LLM supervision introduces scalability constraints and dependence on potentially unreliable signals. We propose MIRA (Memory-Integrated Reinforcement Learning Agent), which incorporates a structured, evolving memory graph to guide early training. The graph stores decision-relevant information, including trajectory segments and subgoal structures, and is constructed from both the agent's high-return experiences and LLM outputs. This design amortizes LLM queries into a persistent memory rather than requiring continuous real-time supervision. From this memory graph, we derive a utility signal that softly adjusts advantage estimation to influence policy updates without modifying the underlying reward function. As training progresses, the agent's policy gradually surpasses the initial LLM-derived priors, and the utility term decays, preserving standard convergence guarantees. We provide theoretical analysis showing that utility-based shaping improves early-stage learning in sparse-reward environments. Empirically, MIRA outperforms RL baselines and achieves returns comparable to approaches that rely on frequent LLM supervision, while requiring substantially fewer online LLM queries. Project webpage: https://narjesno.github.io/MIRA/

</details>


### [97] [Memory-Based Advantage Shaping for LLM-Guided Reinforcement Learning](https://arxiv.org/abs/2602.17931)
*Narjes Nourzad,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 论文提出了一种结合LLM指导与智能体自身经验的方法，通过构建记忆图来编码子目标和轨迹，从中推导效用函数来引导优势函数，提高稀疏奖励环境中的样本效率。


<details>
  <summary>Details</summary>
Motivation: 在稀疏或延迟奖励环境中，强化学习需要大量交互导致样本复杂度高。虽然大语言模型可用于子目标发现和轨迹指导，但频繁依赖LLM调用存在可扩展性和可靠性问题。

Method: 构建记忆图编码来自LLM指导和智能体自身成功轨迹的子目标和轨迹，从中推导效用函数评估智能体轨迹与先前成功策略的匹配程度，该效用函数塑造优势函数为critic提供额外指导而不改变奖励。

Result: 在基准环境中的初步实验显示，相比基线RL方法，该方法提高了样本效率并加速了早期学习，最终回报与需要频繁LLM交互的方法相当。

Conclusion: 该方法通过结合LLM指导和智能体自身经验，构建记忆图并推导效用函数来引导优势函数，在减少对持续LLM监督依赖的同时，提高了稀疏奖励环境中的学习效率。

Abstract: In environments with sparse or delayed rewards, reinforcement learning (RL) incurs high sample complexity due to the large number of interactions needed for learning. This limitation has motivated the use of large language models (LLMs) for subgoal discovery and trajectory guidance. While LLMs can support exploration, frequent reliance on LLM calls raises concerns about scalability and reliability. We address these challenges by constructing a memory graph that encodes subgoals and trajectories from both LLM guidance and the agent's own successful rollouts. From this graph, we derive a utility function that evaluates how closely the agent's trajectories align with prior successful strategies. This utility shapes the advantage function, providing the critic with additional guidance without altering the reward. Our method relies primarily on offline input and only occasional online queries, avoiding dependence on continuous LLM supervision. Preliminary experiments in benchmark environments show improved sample efficiency and faster early learning compared to baseline RL methods, with final returns comparable to methods that require frequent LLM interaction.

</details>


### [98] [Causal Neighbourhood Learning for Invariant Graph Representations](https://arxiv.org/abs/2602.17934)
*Simi Job,Xiaohui Tao,Taotao Cai,Haoran Xie,Jianming Yong*

Main category: cs.LG

TL;DR: CNL-GNN是一个新颖的图神经网络框架，通过因果干预和图结构干预来解决图数据中虚假相关性问题，提高模型在分布变化下的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 图数据中常存在噪声和虚假相关性，掩盖了真实的因果关系。传统GNN依赖虚假连接，难以在不同图之间有效泛化，传统聚合方法会放大这些虚假模式，限制了模型在分布变化下的鲁棒性。

Method: 提出CNL-GNN框架，通过因果干预图结构来识别和保留因果相关连接，减少虚假影响。方法包括：生成反事实邻域、基于可学习重要性掩码和注意力机制的自适应边扰动、结构级干预与因果特征从混杂因素中解耦相结合。

Result: 在四个公开数据集（包括一个数据集的多个领域变体）上的广泛实验表明，CNL-GNN优于最先进的GNN模型，学习到鲁棒且在不同图结构间泛化良好的不变节点表示。

Conclusion: CNL-GNN通过因果干预和图结构干预，超越了传统的基于特征的方法，实现了更鲁棒的分类模型，提高了因果图学习能力。

Abstract: Graph data often contain noisy and spurious correlations that mask the true causal relationships, which are essential for enabling graph models to make predictions based on the underlying causal structure of the data. Dependence on spurious connections makes it challenging for traditional Graph Neural Networks (GNNs) to generalize effectively across different graphs. Furthermore, traditional aggregation methods tend to amplify these spurious patterns, limiting model robustness under distribution shifts. To address these issues, we propose Causal Neighbourhood Learning with Graph Neural Networks (CNL-GNN), a novel framework that performs causal interventions on graph structure. CNL-GNN effectively identifies and preserves causally relevant connections and reduces spurious influences through the generation of counterfactual neighbourhoods and adaptive edge perturbation guided by learnable importance masking and an attention-based mechanism. In addition, by combining structural-level interventions with the disentanglement of causal features from confounding factors, the model learns invariant node representations that are robust and generalize well across different graph structures. Our approach improves causal graph learning beyond traditional feature-based methods, resulting in a robust classification model. Extensive experiments on four publicly available datasets, including multiple domain variants of one dataset, demonstrate that CNL-GNN outperforms state-of-the-art GNN models.

</details>


### [99] [Tighter Regret Lower Bound for Gaussian Process Bandits with Squared Exponential Kernel in Hypersphere](https://arxiv.org/abs/2602.17940)
*Shogo Iwazaki*

Main category: cs.LG

TL;DR: 该论文研究了高斯过程（GP）赌博机问题的算法无关最坏情况下界，针对平方指数（SE）核函数和超球面输入域，解决了维度依赖对数因子的开放问题。


<details>
  <summary>Details</summary>
Motivation: 高斯过程赌博机问题中，对于平方指数核函数，现有上界和下界在维度依赖的对数因子方面存在差距，这是一个尚未解决的开放问题。本文旨在部分解决这个问题，特别是在超球面输入域下。

Method: 采用算法无关的最坏情况分析框架，在已知再生核希尔伯特空间（RKHS）中具有有界范数的固定奖励函数设置下，针对平方指数核函数和超球面输入域，推导累积遗憾和简单遗憾的下界。

Result: 1. 累积遗憾下界：Ω(√[T(ln T)^d (ln ln T)^{-d}])；2. 简单遗憾下界：找到ε最优点需要Ω(ε^{-2}(ln 1/ε)^d (ln ln 1/ε)^{-d})时间步；3. 改进了SE核的最大信息增益上界：O((ln T)^{d+1}(ln ln T)^{-d})。

Conclusion: 在超球面输入域下，本文结果保证了现有最佳算法在维度无关对数因子范围内的最优性，部分解决了GP赌博机问题中维度依赖对数因子的开放问题。

Abstract: We study an algorithm-independent, worst-case lower bound for the Gaussian process (GP) bandit problem in the frequentist setting, where the reward function is fixed and has a bounded norm in the known reproducing kernel Hilbert space (RKHS). Specifically, we focus on the squared exponential (SE) kernel, one of the most widely used kernel functions in GP bandits. One of the remaining open questions for this problem is the gap in the \emph{dimension-dependent} logarithmic factors between upper and lower bounds. This paper partially resolves this open question under a hyperspherical input domain. We show that any algorithm suffers $Ω(\sqrt{T (\ln T)^{d} (\ln \ln T)^{-d}})$ cumulative regret, where $T$ and $d$ represent the total number of steps and the dimension of the hyperspherical domain, respectively. Regarding the simple regret, we show that any algorithm requires $Ω(ε^{-2}(\ln \frac{1}ε)^d (\ln \ln \frac{1}ε)^{-d})$ time steps to find an $ε$-optimal point. We also provide the improved $O((\ln T)^{d+1}(\ln \ln T)^{-d})$ upper bound on the maximum information gain for the SE kernel. Our results guarantee the optimality of the existing best algorithm up to \emph{dimension-independent} logarithmic factors under a hyperspherical input domain.

</details>


### [100] [Optimizing Graph Causal Classification Models: Estimating Causal Effects and Addressing Confounders](https://arxiv.org/abs/2602.17941)
*Simi Job,Xiaohui Tao,Taotao Cai,Haoran Xie,Jianming Yong,Xin Wang*

Main category: cs.LG

TL;DR: 提出CCAGNN框架，将因果推理融入图学习，通过考虑混杂因素实现更稳健的预测


<details>
  <summary>Details</summary>
Motivation: 传统图机器学习方法（如GNN）依赖相关性，对虚假模式和分布变化敏感，而因果模型能通过隔离真实因果因素实现稳健预测

Method: 提出CCAGNN（Confounder-Aware causal GNN）框架，将因果推理融入图学习，支持反事实推理，考虑混杂因素调整

Result: 在六个公开数据集上的综合实验显示，CCAGNN始终优于领先的SOTA模型

Conclusion: CCAGNN通过因果推理增强图学习，在现实场景中提供可靠预测，解决了传统方法对虚假相关性的敏感性

Abstract: Graph data is becoming increasingly prevalent due to the growing demand for relational insights in AI across various domains. Organizations regularly use graph data to solve complex problems involving relationships and connections. Causal learning is especially important in this context, since it helps to understand cause-effect relationships rather than mere associations. Since many real-world systems are inherently causal, graphs can efficiently model these systems. However, traditional graph machine learning methods including graph neural networks (GNNs), rely on correlations and are sensitive to spurious patterns and distribution changes. On the other hand, causal models enable robust predictions by isolating true causal factors, thus making them more stable under such shifts. Causal learning also helps in identifying and adjusting for confounders, ensuring that predictions reflect true causal relationships and remain accurate even under interventions. To address these challenges and build models that are robust and causally informed, we propose CCAGNN, a Confounder-Aware causal GNN framework that incorporates causal reasoning into graph learning, supporting counterfactual reasoning and providing reliable predictions in real-world settings. Comprehensive experiments on six publicly available datasets from diverse domains show that CCAGNN consistently outperforms leading state-of-the-art models.

</details>


### [101] [Understanding the Generalization of Bilevel Programming in Hyperparameter Optimization: A Tale of Bias-Variance Decomposition](https://arxiv.org/abs/2602.17947)
*Yubo Zhou,Jun Shu,Junmin Liu,Deyu Meng*

Main category: cs.LG

TL;DR: 该论文分析了超参数优化中梯度估计的偏差-方差分解，提出了降低方差的集成策略，改善了超梯度估计性能。


<details>
  <summary>Details</summary>
Motivation: 现有超参数优化方法主要关注梯度估计的偏差，而忽略了数据分布带来的方差误差，这影响了实际性能。需要全面分析超梯度估计误差的偏差-方差分解。

Method: 1. 对超梯度估计误差进行偏差-方差分解，特别关注被忽略的方差项；2. 提出集成超梯度策略来有效降低HPO算法中的方差；3. 建立超梯度估计与超额误差之间的联系。

Result: 在正则化超参数学习、数据超清洗和少样本学习等任务上的实验表明，方差降低策略改善了超梯度估计性能，并能解释实践中观察到的过拟合验证集等现象。

Conclusion: 通过偏差-方差分解分析超梯度估计误差，提出的集成策略能有效降低方差，提高超参数优化性能，并为实际观察到的现象提供了理论解释。

Abstract: Gradient-based hyperparameter optimization (HPO) have emerged recently, leveraging bilevel programming techniques to optimize hyperparameter by estimating hypergradient w.r.t. validation loss. Nevertheless, previous theoretical works mainly focus on reducing the gap between the estimation and ground-truth (i.e., the bias), while ignoring the error due to data distribution (i.e., the variance), which degrades performance. To address this issue, we conduct a bias-variance decomposition for hypergradient estimation error and provide a supplemental detailed analysis of the variance term ignored by previous works. We also present a comprehensive analysis of the error bounds for hypergradient estimation. This facilitates an easy explanation of some phenomena commonly observed in practice, like overfitting to the validation set. Inspired by the derived theories, we propose an ensemble hypergradient strategy to reduce the variance in HPO algorithms effectively. Experimental results on tasks including regularization hyperparameter learning, data hyper-cleaning, and few-shot learning demonstrate that our variance reduction strategy improves hypergradient estimation. To explain the improved performance, we establish a connection between excess error and hypergradient estimation, offering some understanding of empirical observations.

</details>


### [102] [A Geometric Probe of the Accuracy-Robustness Trade-off: Sharp Boundaries in Symmetry-Breaking Dimensional Expansion](https://arxiv.org/abs/2602.17948)
*Yu Bai,Zhe Wang,Jiarui Zhang,Dong-Xiao Zhang,Yinjun Gao,Jun-Jie Zhang*

Main category: cs.LG

TL;DR: 通过对称性破缺维度扩展(SBDE)研究深度学习中的准确率-鲁棒性权衡，发现扩展维度能提高干净准确率但会沿辅助轴创建陡峭边界，导致对抗脆弱性。


<details>
  <summary>Details</summary>
Motivation: 深度学习中的准确率与对抗鲁棒性之间存在普遍权衡现象，但其几何起源尚不明确。本研究旨在探究这一权衡背后的机制。

Method: 使用对称性破缺维度扩展(SBDE)作为受控探针，通过在输入图像中插入恒定值像素来打破平移对称性。采用测试时的掩码投影技术，将插入的辅助像素重置为训练值，以分析脆弱性来源。

Result: SBDE显著提高了干净准确率（如CIFAR-10上从90.47%提升到95.63%），但降低了对抗迭代白盒攻击的鲁棒性。掩码投影能有效中和攻击并恢复鲁棒性，表明脆弱性几乎完全来自插入的维度。模型通过沿辅助轴创建陡峭边界（陡峭损失梯度）来实现高准确率。

Conclusion: 准确率-鲁棒性悖论的几何解释是：优化景观加深了吸引盆以提升准确率，但不可避免地沿辅助自由度建立了陡峭墙壁，导致对流形外扰动的脆弱敏感性。

Abstract: The trade-off between clean accuracy and adversarial robustness is a pervasive phenomenon in deep learning, yet its geometric origin remains elusive. In this work, we utilize Symmetry-Breaking Dimensional Expansion (SBDE) as a controlled probe to investigate the mechanism underlying this trade-off. SBDE expands input images by inserting constant-valued pixels, which breaks translational symmetry and consistently improves clean accuracy (e.g., from $90.47\%$ to $95.63\%$ on CIFAR-10 with ResNet-18) by reducing parameter degeneracy. However, this accuracy gain comes at the cost of reduced robustness against iterative white-box attacks. By employing a test-time \emph{mask projection} that resets the inserted auxiliary pixels to their training values, we demonstrate that the vulnerability stems almost entirely from the inserted dimensions. The projection effectively neutralizes the attacks and restores robustness, revealing that the model achieves high accuracy by creating \emph{sharp boundaries} (steep loss gradients) specifically along the auxiliary axes. Our findings provide a concrete geometric explanation for the accuracy-robustness paradox: the optimization landscape deepens the basin of attraction to improve accuracy but inevitably erects steep walls along the auxiliary degrees of freedom, creating a fragile sensitivity to off-manifold perturbations.

</details>


### [103] [Hardware-Friendly Input Expansion for Accelerating Function Approximation](https://arxiv.org/abs/2602.17952)
*Hu Lou,Yin-Jun Gao,Dong-Xiao Zhang,Tai-Jiao Du,Jun-Jie Zhang,Jia-Rui Zhang*

Main category: cs.LG

TL;DR: 提出一种通过输入空间扩展来打破参数对称性的硬件友好型函数逼近方法，使用常数（如π）扩展一维输入到高维向量，显著加速训练并提高精度。


<details>
  <summary>Details</summary>
Motivation: 神经网络在函数逼近中虽然具有强大的通用逼近能力，但参数空间对称性导致的平坦损失景观会阻碍优化过程，造成收敛缓慢和泛化能力差，特别是对高频分量。受物理学中对称性破缺原理启发，需要一种硬件友好的方法来改善这一问题。

Method: 提出输入空间扩展方法：将原始一维输入（如x）与常数值（如π）组合形成高维向量（如[π, π, x, π, π]），在不增加网络参数数量的情况下有效打破参数对称性。研究了不同扩展维度和常数选择的影响。

Result: 在10个代表性一维函数（包括平滑、不连续、高频和不可微函数）上的实验表明，输入空间扩展显著加速训练收敛（LBFGS迭代平均减少12%），并提高逼近精度（最优5D扩展使最终MSE减少66.3%）。消融研究显示π常数始终优于其他常数。

Conclusion: 提出了一种低成本、高效且硬件友好的算法设计技术，通过输入空间扩展打破参数对称性，有效改善神经网络在函数逼近中的优化效率和精度。

Abstract: One-dimensional function approximation is a fundamental problem in scientific computing and engineering applications. While neural networks possess powerful universal approximation capabilities, their optimization process is often hindered by flat loss landscapes induced by parameter-space symmetries, leading to slow convergence and poor generalization, particularly for high-frequency components. Inspired by the principle of \emph{symmetry breaking} in physics, this paper proposes a hardware-friendly approach for function approximation through \emph{input-space expansion}. The core idea involves augmenting the original one-dimensional input (e.g., $x$) with constant values (e.g., $π$) to form a higher-dimensional vector (e.g., $[π, π, x, π, π]$), effectively breaking parameter symmetries without increasing the network's parameter count. We evaluate the method on ten representative one-dimensional functions, including smooth, discontinuous, high-frequency, and non-differentiable functions. Experimental results demonstrate that input-space expansion significantly accelerates training convergence (reducing LBFGS iterations by 12\% on average) and enhances approximation accuracy (reducing final MSE by 66.3\% for the optimal 5D expansion). Ablation studies further reveal the effects of different expansion dimensions and constant selections, with $π$ consistently outperforming other constants. Our work proposes a low-cost, efficient, and hardware-friendly technique for algorithm design.

</details>


### [104] [Bayesian Online Model Selection](https://arxiv.org/abs/2602.17958)
*Aida Afshar,Yuke Zhang,Aldo Pacchiano*

Main category: cs.LG

TL;DR: 提出一种贝叶斯在线模型选择算法，用于随机多臂老虎机问题，能够自适应探索多个基础学习器并与最优学习器竞争。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯老虎机中的在线模型选择面临探索挑战：当环境实例从先验分布中采样时，如何设计自适应策略来探索多个老虎机学习器并与事后最优学习器竞争？

Method: 提出新的贝叶斯算法用于随机老虎机的在线模型选择，研究基础学习器之间共享数据的效果及其在缓解先验错误设定中的作用。

Result: 证明了贝叶斯遗憾的oracle-style保证为O(d*M√T + √(MT))，其中M是基础学习器数量，d*是最优基础学习器的遗憾系数，T是时间范围。实验验证了该方法在多种随机老虎机设置中具有竞争力。

Conclusion: 该贝叶斯在线模型选择算法能够有效探索多个基础学习器，与最优学习器竞争，并通过数据共享缓解先验错误设定的影响。

Abstract: Online model selection in Bayesian bandits raises a fundamental exploration challenge: When an environment instance is sampled from a prior distribution, how can we design an adaptive strategy that explores multiple bandit learners and competes with the best one in hindsight? We address this problem by introducing a new Bayesian algorithm for online model selection in stochastic bandits. We prove an oracle-style guarantee of $O\left( d^* M \sqrt{T} + \sqrt{(MT)} \right)$ on the Bayesian regret, where $M$ is the number of base learners, $d^*$ is the regret coefficient of the optimal base learner, and $T$ is the time horizon. We also validate our method empirically across a range of stochastic bandit settings, demonstrating performance that is competitive with the best base learner. Additionally, we study the effect of sharing data among base learners and its role in mitigating prior mis-specification.

</details>


### [105] [Improving Generalizability of Hip Fracture Risk Prediction via Domain Adaptation Across Multiple Cohorts](https://arxiv.org/abs/2602.17962)
*Shuo Sun,Meiling Zhou,Chen Zhao,Joyce H. Keyak,Nancy E. Lane,Jeffrey D. Deng,Kuan-Jui Su,Hui Shen,Hong-Wen Deng,Kui Zhang,Weihua Zhou*

Main category: cs.LG

TL;DR: 该研究评估了三种域适应方法（MMD、CORAL、DANN）及其组合在髋部骨折风险预测中的跨队列泛化能力，发现多方法组合能显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 临床风险预测模型在不同队列间泛化能力差，因为数据分布受临床地点、地区、人口统计学和测量协议等因素影响。这在髋部骨折风险预测中尤为明显，模型在一个队列训练后在另一个队列部署时性能会显著下降。

Method: 使用三个大型队列（SOF、MrOS、UKB）的临床和DXA特征，系统评估了三种域适应方法：最大均值差异（MMD）、相关对齐（CORAL）和域对抗神经网络（DANN）及其组合。采用无结果方法，不依赖目标队列的已知结果。

Result: 域适应方法相比无适应基线（仅源队列训练）持续表现更好。MMD、CORAL和DANN的组合方法获得最高区分度：仅男性源队列AUC为0.88，仅女性源队列AUC为0.95。多方法组合提供了最大且最稳定的性能提升。

Conclusion: 整合多种域适应方法可以产生对数据集差异不敏感的特征表示，提高髋部骨折风险预测模型的泛化能力。与依赖监督调优或假设目标队列已知结果的方法不同，这种无结果方法能在实际部署条件下实现模型选择。

Abstract: Clinical risk prediction models often fail to be generalized across cohorts because underlying data distributions differ by clinical site, region, demographics, and measurement protocols. This limitation is particularly pronounced in hip fracture risk prediction, where the performance of models trained on one cohort (the source cohort) can degrade substantially when deployed in other cohorts (target cohorts). We used a shared set of clinical and DXA-derived features across three large cohorts - the Study of Osteoporotic Fractures (SOF), the Osteoporotic Fractures in Men Study (MrOS), and the UK Biobank (UKB), to systematically evaluate the performance of three domain adaptation methods - Maximum Mean Discrepancy (MMD), Correlation Alignment (CORAL), and Domain - Adversarial Neural Networks (DANN) and their combinations. For a source cohort with males only and a source cohort with females only, domain-adaptation methods consistently showed improved performance than the no-adaptation baseline (source-only training), and the use of combinations of multiple domain adaptation methods delivered the largest and most stable gains. The method that combines MMD, CORAL, and DANN achieved the highest discrimination with the area under curve (AUC) of 0.88 for a source cohort with males only and 0.95 for a source cohort with females only), demonstrating that integrating multiple domain adaptation methods could produce feature representations that are less sensitive to dataset differences. Unlike existing methods that rely heavily on supervised tuning or assume known outcomes of samples in target cohorts, our outcome-free approaches enable the model selection under realistic deployment conditions and improve generalization of models in hip fracture risk prediction.

</details>


### [106] [Student Flow Modeling for School Decongestion via Stochastic Gravity Estimation and Constrained Spatial Allocation](https://arxiv.org/abs/2602.17972)
*Sebastian Felipe R. Bundoc,Paula Joy B. Martinez,Sebastian C. Ibañez,Erika Fille T. Legara*

Main category: cs.LG

TL;DR: 菲律宾教育服务合同项目等补贴计划未能有效缓解学校拥挤问题，因为数据碎片化阻碍了有效实施。研究引入计算框架模拟学生流动模式，发现地理距离比学费成本对学校选择的限制更强四倍，且座位容量而非补贴金额是主要约束。


<details>
  <summary>Details</summary>
Motivation: 低收入和中等收入国家面临学校拥挤问题，严重影响学习成果并加剧教育不平等。补贴计划（如菲律宾教育服务合同项目）虽然理论上可以通过将学生从公立学校转移到私立学校来缓解拥挤，但由于数据系统碎片化而效果不佳，缺乏基于科学和数据的分析来理解学生入学流动模式。

Method: 引入计算框架建模学生流动模式并模拟政策情景。通过整合近3000个机构的异构政府数据，采用负二项回归估计的随机重力模型，推导距离、净学费成本和社会经济决定因素的行为弹性。这些弹性为双重约束空间分配机制提供信息，在尊重原始候选池和目标座位容量的同时模拟不同补贴金额下的学生重新分配。

Result: 研究发现地理邻近性对学校选择的限制比学费成本强四倍，座位容量而非补贴金额是主要约束因素。这表明补贴计划本身无法解决系统性过度拥挤问题。

Conclusion: 计算建模可以赋能教育政策制定者做出公平、数据驱动的决策，揭示塑造有效资源分配的结构性约束，即使在资源有限的情况下也能如此。补贴计划本身无法解决系统性过度拥挤问题，需要更全面的策略。

Abstract: School congestion, where student enrollment exceeds school capacity, is a major challenge in low- and middle-income countries. It highly impacts learning outcomes and deepens inequities in education. While subsidy programs that transfer students from public to private schools offer a mechanism to alleviate congestion without capital-intensive construction, they often underperform due to fragmented data systems that hinder effective implementation. The Philippine Educational Service Contracting program, one of the world's largest educational subsidy programs, exemplifies these challenges, falling short of its goal to decongest public schools. This prevents the science-based and data-driven analyses needed to understand what shapes student enrollment flows, particularly how families respond to economic incentives and spatial constraints. We introduce a computational framework for modeling student flow patterns and simulating policy scenarios. By synthesizing heterogeneous government data across nearly 3,000 institutions, we employ a stochastic gravity model estimated via negative binomial regression to derive behavioral elasticities for distance, net tuition cost, and socioeconomic determinants. These elasticities inform a doubly constrained spatial allocation mechanism that simulates student redistribution under varying subsidy amounts while respecting both origin candidate pools and destination slot capacities. We find that geographic proximity constrains school choice four times more strongly than tuition cost and that slot capacity, not subsidy amounts, is the binding constraint. Our work demonstrates that subsidy programs alone cannot resolve systemic overcrowding, and computational modeling can empower education policymakers to make equitable, data-driven decisions by revealing the structural constraints that shape effective resource allocation, even when resources are limited.

</details>


### [107] [Generating adversarial inputs for a graph neural network model of AC power flow](https://arxiv.org/abs/2602.17975)
*Robert Parker*

Main category: cs.LG

TL;DR: 该研究通过优化方法生成能导致神经网络交流潮流预测与真实解之间高误差的对抗性输入点，在14节点测试电网上的CANOS-PF图神经网络模型上实现了最大3.4p.u.无功功率和0.08p.u.电压幅值误差


<details>
  <summary>Details</summary>
Motivation: 神经网络作为交流潮流方程的替代模型存在脆弱性，需要验证其鲁棒性并开发对抗性训练方法

Method: 构建优化问题生成对抗性输入点，最小化与训练点的扰动同时满足对抗性约束条件

Result: 成功生成导致显著预测误差的对抗性样本，最小仅需单个节点0.04p.u.电压幅值扰动即可满足对抗性约束

Conclusion: 神经网络潮流替代模型存在安全漏洞，需要开发严格的验证方法和鲁棒训练技术来增强模型可靠性

Abstract: This work formulates and solves optimization problems to generate input points that yield high errors between a neural network's predicted AC power flow solution and solutions to the AC power flow equations. We demonstrate this capability on an instance of the CANOS-PF graph neural network model, as implemented by the PF$Δ$ benchmark library, operating on a 14-bus test grid. Generated adversarial points yield errors as large as 3.4 per-unit in reactive power and 0.08 per-unit in voltage magnitude. When minimizing the perturbation from a training point necessary to satisfy adversarial constraints, we find that the constraints can be met with as little as an 0.04 per-unit perturbation in voltage magnitude on a single bus. This work motivates the development of rigorous verification and robust training methods for neural network surrogate models of AC power flow.

</details>


### [108] [In-Context Learning for Pure Exploration in Continuous Spaces](https://arxiv.org/abs/2602.17976)
*Alessio Russo,Yin-Ching Lee,Ryan Welch,Aldo Pacchiano*

Main category: cs.LG

TL;DR: 提出C-ICPE-TS算法，用于连续空间中的纯探索问题，通过元训练深度神经网络策略学习可迁移的序列测试策略，无需参数更新即可在新任务上推断真实假设。


<details>
  <summary>Details</summary>
Motivation: 许多现代设置中，假设空间是连续的且与查询/动作空间自然重合，如连续臂老虎机中的最优动作识别、目标区域内的ε球定位或未知函数极小值点估计。需要解决连续空间中的纯探索问题。

Method: 提出C-ICPE-TS算法，元训练深度神经网络策略，将观察历史映射到(i)下一个连续查询动作和(ii)预测假设，直接从数据中学习可迁移的序列测试策略。推理时主动收集证据并推断真实假设，无需参数更新或显式手工信息模型。

Result: 在多个基准测试中验证C-ICPE-TS，涵盖连续最佳臂识别、区域定位和函数极小值点识别等任务。

Conclusion: C-ICPE-TS能够有效解决连续空间中的纯探索问题，学习可迁移的序列测试策略，在新任务上无需参数更新即可推断真实假设。

Abstract: In active sequential testing, also termed pure exploration, a learner is tasked with the goal to adaptively acquire information so as to identify an unknown ground-truth hypothesis with as few queries as possible. This problem, originally studied by Chernoff in 1959, has several applications: classical formulations include Best-Arm Identification (BAI) in bandits, where actions index hypotheses, and generalized search problems, where strategically chosen queries reveal partial information about a hidden label. In many modern settings, however, the hypothesis space is continuous and naturally coincides with the query/action space: for example, identifying an optimal action in a continuous-armed bandit, localizing an $ε$-ball contained in a target region, or estimating the minimizer of an unknown function from a sequence of observations. In this work, we study pure exploration in such continuous spaces and introduce Continuous In-Context Pure Exploration for this regime. We introduce C-ICPE-TS, an algorithm that meta-trains deep neural policies to map observation histories to (i) the next continuous query action and (ii) a predicted hypothesis, thereby learning transferable sequential testing strategies directly from data. At inference time, C-ICPE-TS actively gathers evidence on previously unseen tasks and infers the true hypothesis without parameter updates or explicit hand-crafted information models. We validate C-ICPE-TS across a range of benchmarks, spanning continuous best-arm identification, region localization, and function minimizer identification.

</details>


### [109] [Learning Optimal and Sample-Efficient Decision Policies with Guarantees](https://arxiv.org/abs/2602.17978)
*Daqian Shao*

Main category: cs.LG

TL;DR: 该论文提出了一种从存在隐藏混杂因素的离线数据集中学习决策策略的方法，使用工具变量解决因果效应识别问题，并扩展到模仿学习和时序逻辑目标学习，具有理论保证和实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习需要大量在线环境交互，这在成本高、危险或不可行的实际应用中存在问题。离线学习面临隐藏混杂因素导致虚假相关性的挑战，可能使智能体采取次优或对抗性行动。

Method: 1. 使用工具变量识别因果效应，作为条件矩限制问题；2. 受双重/去偏机器学习启发，开发样本高效的CMR求解算法；3. 放宽隐藏混杂因素条件，将CMR估计器应用于离线模仿学习；4. 开发线性时序逻辑目标学习的可证明最优算法。

Result: 1. 提出的CMR算法在收敛性和最优性方面优于现有方法；2. 模仿学习算法能学习有效的模仿策略并具有收敛率保证；3. LTL学习算法提高了样本效率；4. 在强化学习基准、合成和半合成数据集上验证了方法的实用性。

Conclusion: 该论文开发了从存在隐藏混杂因素的离线数据中学习决策策略的理论保证方法，解决了实际决策应用中的关键挑战，为高风险应用中的可靠决策提供了新工具。

Abstract: The paradigm of decision-making has been revolutionised by reinforcement learning and deep learning. Although this has led to significant progress in domains such as robotics, healthcare, and finance, the use of RL in practice is challenging, particularly when learning decision policies in high-stakes applications that may require guarantees. Traditional RL algorithms rely on a large number of online interactions with the environment, which is problematic in scenarios where online interactions are costly, dangerous, or infeasible. However, learning from offline datasets is hindered by the presence of hidden confounders. Such confounders can cause spurious correlations in the dataset and can mislead the agent into taking suboptimal or adversarial actions. Firstly, we address the problem of learning from offline datasets in the presence of hidden confounders. We work with instrumental variables (IVs) to identify the causal effect, which is an instance of a conditional moment restrictions (CMR) problem. Inspired by double/debiased machine learning, we derive a sample-efficient algorithm for solving CMR problems with convergence and optimality guarantees, which outperforms state-of-the-art algorithms. Secondly, we relax the conditions on the hidden confounders in the setting of (offline) imitation learning, and adapt our CMR estimator to derive an algorithm that can learn effective imitator policies with convergence rate guarantees. Finally, we consider the problem of learning high-level objectives expressed in linear temporal logic (LTL) and develop a provably optimal learning algorithm that improves sample efficiency over existing methods. Through evaluation on reinforcement learning benchmarks and synthetic and semi-synthetic datasets, we demonstrate the usefulness of the methods developed in this thesis in real-world decision making.

</details>


### [110] [Learning Without Training](https://arxiv.org/abs/2602.17985)
*Ryan O'Dowd*

Main category: cs.LG

TL;DR: 该博士论文包含三个基于数学理论的机器学习项目：1) 改进监督学习和流形学习的函数逼近方法；2) 研究部分域数据上的迁移学习函数提升；3) 将信号分离技术应用于主动学习分类任务的新算法。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络在大规模问题上的成功，机器学习研究日益增多。本文旨在通过数学理论解决机器学习中的核心问题：函数逼近、跨域知识迁移以及高效分类，以弥补现有方法的理论缺陷。

Method: 1) 针对监督学习提出改进函数逼近的新方法；2) 研究部分域数据上的函数提升理论，分析提升可定义性和局部光滑性关系；3) 将信号分离技术引入分类任务，提出统一理论和快速主动学习算法。

Result: 1) 解决了当前监督学习范式的理论缺陷；2) 确定了函数提升可定义的子集及其局部光滑性关系；3) 新算法在保持竞争力的准确率同时，相比其他主动学习算法显著提升了速度。

Conclusion: 该论文通过数学理论为机器学习的三个核心领域提供了创新方法：改进了监督学习的函数逼近理论，发展了部分域数据的迁移学习框架，并将信号分离技术成功应用于快速主动学习分类，为机器学习理论发展做出了贡献。

Abstract: Machine learning is at the heart of managing the real-world problems associated with massive data. With the success of neural networks on such large-scale problems, more research in machine learning is being conducted now than ever before. This dissertation focuses on three different projects rooted in mathematical theory for machine learning applications.
  The first project deals with supervised learning and manifold learning. In theory, one of the main problems in supervised learning is that of function approximation: that is, given some data set $\mathcal{D}=\{(x_j,f(x_j))\}_{j=1}^M$, can one build a model $F\approx f$? We introduce a method which aims to remedy several of the theoretical shortcomings of the current paradigm for supervised learning.
  The second project deals with transfer learning, which is the study of how an approximation process or model learned on one domain can be leveraged to improve the approximation on another domain. We study such liftings of functions when the data is assumed to be known only on a part of the whole domain. We are interested in determining subsets of the target data space on which the lifting can be defined, and how the local smoothness of the function and its lifting are related.
  The third project is concerned with the classification task in machine learning, particularly in the active learning paradigm. Classification has often been treated as an approximation problem as well, but we propose an alternative approach leveraging techniques originally introduced for signal separation problems. We introduce theory to unify signal separation with classification and a new algorithm which yields competitive accuracy to other recent active learning algorithms while providing results much faster.

</details>


### [111] [Turbo Connection: Reasoning as Information Flow from Higher to Lower Layers](https://arxiv.org/abs/2602.17993)
*Mohan Tang,Sidi Lu*

Main category: cs.LG

TL;DR: TurboConn是一种新型Transformer架构，通过将高层隐藏状态路由到后续token的低层，突破固定计算深度限制，显著提升LLMs在复杂推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 人类通过多步骤推理解决复杂问题，但传统Transformer的计算深度受限于固定层数，限制了其多步推理能力。需要突破这一限制来提升LLMs的复杂问题解决能力。

Method: 提出TurboConn架构，将每个token的高层隐藏状态通过多个残差连接路由到下一个token的低层，形成密集的向后连接，从而扩展计算路径深度。

Result: 在GSM8K、Parity和多步算术等基准测试上获得0.9%到超过10%的准确率提升；Qwen-3-1.7B在Parity任务上从53.78%提升到100%准确率；密集连接显著优于稀疏替代方案。

Conclusion: 计算路径深度是推理能力的关键因素，TurboConn为增强LLMs提供了新机制，无需从头训练或复杂课程学习，且不影响生成延迟。

Abstract: Complex problems, whether in math, logic, or planning, are solved by humans through a sequence of steps where the result of one step informs the next. In this work, we adopt the perspective that the reasoning power of Transformers is fundamentally limited by a fixed maximum number of steps along any latent path of computation. To address this, we introduce Turbo Connection (TurboConn), a novel architecture that overcomes the fixed-depth constraint by routing multiple residual connections from the higher-layer hidden states of each token $t$ to the lower layers of token $t+1$. Fine-tuning pre-trained LLMs with our method not only yields accuracy gains of 0.9% to over 10% on benchmarks like GSM8K, Parity, and multi-step arithmetic, but also demonstrates that the density of these backward connections is critical; our dense interaction significantly outperforms "sparse" alternatives that only pass a single hidden state or vector. Notably, TurboConn can be integrated into pre-trained LLMs to overcome task-specific plateaus: while a fine-tuned Qwen-3-1.7B achieves only 53.78% on Parity, adding our architectural modification enables the model to reach 100% accuracy, all without the necessity to retrain the full model from scratch or sophisticated curriculum learning. Our results provide strong empirical evidence that the depth of the computational path is a key factor in reasoning ability, also offering a new mechanism to enhance LLMs without significantly affecting generation latency.

</details>


### [112] [Whole-Brain Connectomic Graph Model Enables Whole-Body Locomotion Control in Fruit Fly](https://arxiv.org/abs/2602.17997)
*Zehao Jin,Yaoye Zhu,Chen Zhang,Yanan Sui*

Main category: cs.LG

TL;DR: 将果蝇完整脑连接组作为神经网络控制器，用于全身运动控制，无需任务特定架构调整即可实现稳定控制


<details>
  <summary>Details</summary>
Motivation: 探索使用大脑连接组作为具身强化学习中的神经网络控制器，研究果蝇脑连接组在身体运动控制中的应用潜力

Method: 开发FlyGM模型，其静态结构与成年果蝇完整连接组相同，将其表示为有向消息传递图，与生物力学果蝇模型集成进行动态控制

Result: FlyGM在多样运动任务中实现稳定控制，相比度保持重连图、随机图和多层感知机，具有更高的样本效率和性能优势

Conclusion: 静态大脑连接组可以转化为有效的神经策略，用于具身学习的运动控制，为脑启发计算提供了新途径

Abstract: Whole-brain biological neural networks naturally support the learning and control of whole-body movements. However, the use of brain connectomes as neural network controllers in embodied reinforcement learning remains unexplored. We investigate using the exact neural architecture of an adult fruit fly's brain for the control of its body movement. We develop Fly-connectomic Graph Model (FlyGM), whose static structure is identical to the complete connectome of an adult Drosophila for whole-body locomotion control. To perform dynamical control, FlyGM represents the static connectome as a directed message-passing graph to impose a biologically grounded information flow from sensory inputs to motor outputs. Integrated with a biomechanical fruit fly model, our method achieves stable control across diverse locomotion tasks without task-specific architectural tuning. To verify the structural advantages of the connectome-based model, we compare it against a degree-preserving rewired graph, a random graph, and multilayer perceptrons, showing that FlyGM yields higher sample efficiency and superior performance. This work demonstrates that static brain connectomes can be transformed to instantiate effective neural policy for embodied learning of movement control.

</details>


### [113] [PHAST: Port-Hamiltonian Architecture for Structured Temporal Dynamics Forecasting](https://arxiv.org/abs/2602.17998)
*Shubham Bhardwaj,Chandrajit Bajaj*

Main category: cs.LG

TL;DR: PHAST：一种用于位置观测时序动力学的端口哈密顿架构，通过保守-耗散分解实现稳定长期预测和物理参数恢复


<details>
  <summary>Details</summary>
Motivation: 真实物理系统具有耗散性，从部分观测（仅位置）预测动力学是科学机器学习中的核心挑战。需要解决仅位置观测问题：给定离散时间的位置数据（动量隐式），学习结构化模型以实现稳定长期预测并在提供足够结构时恢复物理意义参数。

Method: 提出PHAST（端口哈密顿架构），基于端口哈密顿框架明确保守-耗散分解，将哈密顿量分解为势能、质量和阻尼三个知识域（已知、部分已知、未知），使用高效低秩PSD/SPD参数化，通过Strang分裂推进动力学。

Result: 在13个仅位置基准测试（机械、电气、分子、热、引力、生态系统）中，PHAST在长期预测方面优于竞争基线，并在提供足够锚点时实现物理意义参数恢复。研究表明，没有锚点时识别问题本质上是病态的（规范自由度）。

Conclusion: PHAST通过端口哈密顿框架成功解决了仅位置观测的动力学预测问题，实现了稳定长期预测和物理参数恢复。研究强调了评估需要分离预测稳定性与可识别性两个维度。

Abstract: Real physical systems are dissipative -- a pendulum slows, a circuit loses charge to heat -- and forecasting their dynamics from partial observations is a central challenge in scientific machine learning. We address the \emph{position-only} (q-only) problem: given only generalized positions~$q_t$ at discrete times (momenta~$p_t$ latent), learn a structured model that (a)~produces stable long-horizon forecasts and (b)~recovers physically meaningful parameters when sufficient structure is provided. The port-Hamiltonian framework makes the conservative-dissipative split explicit via $\dot{x}=(J-R)\nabla H(x)$, guaranteeing $dH/dt\le 0$ when $R\succeq 0$. We introduce \textbf{PHAST} (Port-Hamiltonian Architecture for Structured Temporal dynamics), which decomposes the Hamiltonian into potential~$V(q)$, mass~$M(q)$, and damping~$D(q)$ across three knowledge regimes (KNOWN, PARTIAL, UNKNOWN), uses efficient low-rank PSD/SPD parameterizations, and advances dynamics with Strang splitting. Across thirteen q-only benchmarks spanning mechanical, electrical, molecular, thermal, gravitational, and ecological systems, PHAST achieves the best long-horizon forecasting among competitive baselines and enables physically meaningful parameter recovery when the regime provides sufficient anchors. We show that identification is fundamentally ill-posed without such anchors (gauge freedom), motivating a two-axis evaluation that separates forecasting stability from identifiability.

</details>


### [114] [Asynchronous Heavy-Tailed Optimization](https://arxiv.org/abs/2602.18002)
*Junfei Sun,Dixi Yao,Xuchen Gong,Tahseen Rabbani,Manzil Zaheer,Tian Li*

Main category: cs.LG

TL;DR: 针对transformer模型中常见的重尾随机梯度噪声，本文研究了异步优化与重尾噪声的交互，提出了两种处理延迟的通信方案，通过延迟感知学习率调度和延迟补偿来提升异步算法性能。


<details>
  <summary>Details</summary>
Motivation: 重尾随机梯度噪声在transformer模型中常见，会破坏优化过程的稳定性。现有研究主要关注集中式或分布式同步设置下的重尾噪声处理，而重尾噪声与异步优化之间的交互关系尚未得到充分探索。

Method: 提出了两种处理延迟的通信方案：基于延迟感知的学习率调度和延迟补偿的算法修改。这些方法旨在在存在重尾梯度噪声的情况下，通过异步更新处理延迟问题。

Result: 理论分析表明，在重尾噪声下的收敛保证与同步对应方法的速率相匹配，并且相比现有异步方法提高了延迟容忍度。实验结果显示，在图像和语言任务中，该方法在准确率/运行时间权衡方面优于先前的同步和异步方法，并且对超参数更加鲁棒。

Conclusion: 本文提出的延迟感知学习率调度和延迟补偿方法有效解决了异步优化中重尾梯度噪声的问题，在理论和实验上都表现出优于现有方法的性能，为处理transformer模型中的重尾噪声提供了有效的异步优化方案。

Abstract: Heavy-tailed stochastic gradient noise, commonly observed in transformer models, can destabilize the optimization process. Recent works mainly focus on developing and understanding approaches to address heavy-tailed noise in the centralized or distributed, synchronous setting, leaving the interactions between such noise and asynchronous optimization underexplored. In this work, we investigate two communication schemes that handle stragglers with asynchronous updates in the presence of heavy-tailed gradient noise. We propose and theoretically analyze algorithmic modifications based on delay-aware learning rate scheduling and delay compensation to enhance the performance of asynchronous algorithms. Our convergence guarantees under heavy-tailed noise match the rate of the synchronous counterparts and improve delay tolerance compared with existing asynchronous approaches. Empirically, our approaches outperform prior synchronous and asynchronous methods in terms of accuracy/runtime trade-offs and are more robust to hyperparameters in both image and language tasks.

</details>


### [115] [NIMMGen: Learning Neural-Integrated Mechanistic Digital Twins with LLMs](https://arxiv.org/abs/2602.18008)
*Zihan Guan,Rituparna Datta,Mengxuan Hu,Shunshun Liu,Aiying Zhang,Prasanna Balachandran,Sheng Li,Anil Vullikanti*

Main category: cs.LG

TL;DR: 提出NIMM评估框架测试LLM生成机制模型在真实场景下的可靠性，并开发NIMMgen框架通过迭代优化提升模型质量


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成机制模型的方法过于简化现实条件，无法确定在实际应用中的可靠性，需要更真实的评估框架

Method: 提出NIMM评估框架，在部分观测和多样化任务目标的真实设置下评估LLM生成的机制模型；开发NIMMgen代理框架，通过迭代优化提升代码正确性和实际有效性

Result: 评估显示现有基线存在从模型有效性到代码正确性的根本挑战；NIMMgen在三个不同科学领域的数据集上表现优异；学习到的机制模型支持反事实干预模拟

Conclusion: NIMM框架揭示了当前LLM生成机制模型的局限性，NIMMgen通过迭代优化显著提升了模型质量，为实际应用提供了可靠解决方案

Abstract: Mechanistic models encode scientific knowledge about dynamical systems and are widely used in downstream scientific and policy applications. Recent work has explored LLM-based agentic frameworks to automatically construct mechanistic models from data; however, existing problem settings substantially oversimplify real-world conditions, leaving it unclear whether LLM-generated mechanistic models are reliable in practice. To address this gap, we introduce the Neural-Integrated Mechanistic Modeling (NIMM) evaluation framework, which evaluates LLM-generated mechanistic models under realistic settings with partial observations and diversified task objectives. Our evaluation reveals fundamental challenges in current baselines, ranging from model effectiveness to code-level correctness. Motivated by these findings, we design NIMMgen, an agentic framework for neural-integrated mechanistic modeling that enhances code correctness and practical validity through iterative refinement. Experiments across three datasets from diversified scientific domains demonstrate its strong performance. We also show that the learned mechanistic models support counterfactual intervention simulation.

</details>


### [116] [Flow Actor-Critic for Offline Reinforcement Learning](https://arxiv.org/abs/2602.18015)
*Jongseong Chae,Jongeui Park,Yongjae Shin,Gyeongmin Kim,Seungyul Han,Youngchul Sung*

Main category: cs.LG

TL;DR: 提出Flow Actor-Critic方法，将流模型同时用于策略和保守价值函数学习，在离线RL中处理多模态数据分布并防止Q值爆炸，在D4RL和OGBench基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中的数据分布通常呈现复杂多模态特性，需要表达能力更强的策略来捕捉这种分布，而传统高斯策略表达能力有限。同时需要防止在数据外区域的Q值爆炸问题。

Method: 提出Flow Actor-Critic方法：1）使用流模型作为策略（actor）；2）利用流模型进行保守价值函数（critic）学习，通过流行为代理模型构建新的critic正则化器，防止Q值在数据外区域爆炸。

Result: 在D4RL和OGBench等离线RL测试数据集上取得了新的state-of-the-art性能。

Conclusion: 通过将流模型联合用于策略和价值函数学习，能够有效处理离线RL中的复杂多模态数据分布，同时防止Q值爆炸，显著提升了离线RL的性能。

Abstract: The dataset distributions in offline reinforcement learning (RL) often exhibit complex and multi-modal distributions, necessitating expressive policies to capture such distributions beyond widely-used Gaussian policies. To handle such complex and multi-modal datasets, in this paper, we propose Flow Actor-Critic, a new actor-critic method for offline RL, based on recent flow policies. The proposed method not only uses the flow model for actor as in previous flow policies but also exploits the expressive flow model for conservative critic acquisition to prevent Q-value explosion in out-of-data regions. To this end, we propose a new form of critic regularizer based on the flow behavior proxy model obtained as a byproduct of flow-based actor design. Leveraging the flow model in this joint way, we achieve new state-of-the-art performance for test datasets of offline RL including the D4RL and recent OGBench benchmarks.

</details>


### [117] [Gradient Regularization Prevents Reward Hacking in Reinforcement Learning from Human Feedback and Verifiable Rewards](https://arxiv.org/abs/2602.18037)
*Johannes Ackermann,Michael Noukhovitch,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 本文提出使用梯度正则化（GR）替代传统的KL惩罚，通过将策略更新偏向奖励模型更准确的平坦区域，来解决RLHF中的奖励黑客问题。


<details>
  <summary>Details</summary>
Motivation: 在语言模型的后训练中，RLHF/RLVR存在奖励黑客问题，即策略可能利用奖励模型的不准确性学习到非预期行为。传统方法使用KL惩罚限制策略更新，但本文认为应该将训练偏向奖励模型更准确的区域。

Method: 首先建立奖励模型准确性与收敛时最优解平坦度之间的理论联系，然后使用梯度正则化（GR）将训练偏向平坦区域以保持奖励模型准确性。提出两种实现：1）利用KL惩罚的参考重置隐含使用GR；2）使用有限差分估计的显式GR。

Result: 实验表明：1）梯度范数与奖励准确性在RLHF中确实相关；2）GR在多种语言模型RL实验中优于KL惩罚；3）GR在RLHF中获得更高的GPT评判胜率；4）避免基于规则的数学奖励中过度关注格式；5）防止LLM-as-a-Judge数学任务中的评判黑客。

Conclusion: 梯度正则化为解决RLHF中的奖励黑客问题提供了新框架，通过将训练偏向奖励模型更准确的平坦区域，比传统KL惩罚方法表现更好，为语言模型后训练提供了更有效的优化方法。

Abstract: Reinforcement Learning from Human Feedback (RLHF) or Verifiable Rewards (RLVR) are two key steps in the post-training of modern Language Models (LMs). A common problem is reward hacking, where the policy may exploit inaccuracies of the reward and learn an unintended behavior. Most previous works address this by limiting the policy update with a Kullback-Leibler (KL) penalty towards a reference model. We propose a different framing: Train the LM in a way that biases policy updates towards regions in which the reward is more accurate. First, we derive a theoretical connection between the accuracy of a reward model and the flatness of an optimum at convergence. Gradient regularization (GR) can then be used to bias training to flatter regions and thereby maintain reward model accuracy. We confirm these results by showing that the gradient norm and reward accuracy are empirically correlated in RLHF. We then show that Reference Resets of the KL penalty implicitly use GR to find flatter regions with higher reward accuracy. We further improve on this by proposing to use explicit GR with an efficient finite-difference estimate. Empirically, GR performs better than a KL penalty across a diverse set of RL experiments with LMs. GR achieves a higher GPT-judged win-rate in RLHF, avoids overly focusing on the format in rule-based math rewards, and prevents hacking the judge in LLM-as-a-Judge math tasks.

</details>


### [118] [Continual-NExT: A Unified Comprehension And Generation Continual Learning Framework](https://arxiv.org/abs/2602.18055)
*Jingyang Qiao,Zhizhong Zhang,Xin Tan,Jingyu Gong,Yanyun Qu,Yuan Xie*

Main category: cs.LG

TL;DR: 本文提出Continual-NExT框架和MAGE方法，解决双模态大语言模型在持续学习中的遗忘、幻觉等问题，提升跨模态知识迁移能力。


<details>
  <summary>Details</summary>
Motivation: 双模态大语言模型虽然具备强大的即时学习和泛化能力，但在持续学习方面存在缺陷，难以适应动态现实场景。传统方法面临灾难性遗忘、幻觉、指令不遵循和跨模态知识迁移失败等挑战，且缺乏标准化持续学习框架。

Method: 提出Continual-NExT持续学习框架，包含精心设计的评估指标。并提出MAGE方法，通过混合和聚合通用LoRA与专家LoRA来促进跨模态知识迁移并减轻遗忘。

Result: 大量实验表明，MAGE方法优于其他持续学习方法，实现了最先进的性能表现。

Conclusion: 本文建立了首个双模态大语言模型的持续学习框架，提出的MAGE方法有效解决了跨模态知识迁移和遗忘问题，为双模态模型的持续学习提供了系统解决方案。

Abstract: Dual-to-Dual MLLMs refer to Multimodal Large Language Models, which can enable unified multimodal comprehension and generation through text and image modalities. Although exhibiting strong instantaneous learning and generalization capabilities, Dual-to-Dual MLLMs still remain deficient in lifelong evolution, significantly affecting continual adaptation to dynamic real-world scenarios. One of the challenges is that learning new tasks inevitably destroys the learned knowledge. Beyond traditional catastrophic forgetting, Dual-to-Dual MLLMs face other challenges, including hallucination, instruction unfollowing, and failures in cross-modal knowledge transfer. However, no standardized continual learning framework for Dual-to-Dual MLLMs has been established yet, leaving these challenges unexplored. Thus, in this paper, we establish Continual-NExT, a continual learning framework for Dual-to-Dual MLLMs with deliberately-architected evaluation metrics. To improve the continual learning capability of Dual-to-Dual MLLMs, we propose an efficient MAGE (Mixture and Aggregation of General LoRA and Expert LoRA) method to further facilitate knowledge transfer across modalities and mitigate forgetting. Extensive experiments demonstrate that MAGE outperforms other continual learning methods and achieves state-of-the-art performance.

</details>


### [119] [Deepmechanics](https://arxiv.org/abs/2602.18060)
*Abhay Shinde,Aryan Amit Barsainyan,Jose Siguenza,Ankita Vaishnobi Bisoi,Rakshit Kr. Singh,Bharath Ramsundar*

Main category: cs.LG

TL;DR: 对三种物理信息深度学习模型（HNN、LNN、SRNN）在六种动力系统上的系统性基准测试，发现这些模型在混沌或非保守系统中难以保持稳定性。


<details>
  <summary>Details</summary>
Motivation: 物理信息深度学习模型已成为学习动力系统的强大工具，但缺乏对保守和耗散系统的系统性基准测试，且现有基准测试未检查完整轨迹的稳定性。

Method: 使用DeepChem框架对三种物理信息架构（哈密顿神经网络、拉格朗日神经网络、辛循环神经网络）进行基准测试，评估六种动力系统（质量-弹簧、单摆、双摆、三体问题、弹簧摆、弹跳球）。

Result: 所有基准测试模型在混沌或非保守系统中都难以保持稳定性，预测轨迹误差较大。

Conclusion: 物理信息深度学习模型需要更多研究才能学习到经典力学系统的鲁棒模型。

Abstract: Physics-informed deep learning models have emerged as powerful tools for learning dynamical systems. These models directly encode physical principles into network architectures. However, systematic benchmarking of these approaches across diverse physical phenomena remains limited, particularly in conservative and dissipative systems. In addition, benchmarking that has been done thus far does not integrate out full trajectories to check stability. In this work, we benchmark three prominent physics-informed architectures such as Hamiltonian Neural Networks (HNN), Lagrangian Neural Networks (LNN), and Symplectic Recurrent Neural Networks (SRNN) using the DeepChem framework, an open-source scientific machine learning library. We evaluate these models on six dynamical systems spanning classical conservative mechanics (mass-spring system, simple pendulum, double pendulum, and three-body problem, spring-pendulum) and non-conservative systems with contact (bouncing ball). We evaluate models by computing error on predicted trajectories and evaluate error both quantitatively and qualitatively. We find that all benchmarked models struggle to maintain stability for chaotic or nonconservative systems. Our results suggest that more research is needed for physics-informed deep learning models to learn robust models of classical mechanical systems.

</details>


### [120] [Balancing Symmetry and Efficiency in Graph Flow Matching](https://arxiv.org/abs/2602.18084)
*Benjamin Honoré,Alba Carballo-Castro,Yiming Qin,Pascal Frossard*

Main category: cs.LG

TL;DR: 研究图生成模型中严格等变性的权衡，提出通过可控对称性调制方案放松等变性，加速训练收敛并避免过拟合


<details>
  <summary>Details</summary>
Motivation: 严格等变性虽然能确保模型尊重图的置换对称性，但会增加计算成本并减慢收敛速度，因为模型必须在大量可能的节点置换中保持一致。需要研究这种权衡关系。

Method: 从等变离散流匹配模型出发，通过基于正弦位置编码和节点置换的可控对称性调制方案，在训练过程中放松模型的等变性约束。

Result: 实验表明：对称性破坏能通过提供更简单的学习信号加速早期训练，但会导致模型过拟合，反复生成训练集的重复图。而适当调制对称性信号可以延迟过拟合同时加速收敛，仅用基线训练轮数的19%就能达到更强的性能。

Conclusion: 在图生成模型中，适当调制而非完全破坏对称性，可以在加速训练收敛的同时避免过拟合，实现更好的性能效率平衡。

Abstract: Equivariance is central to graph generative models, as it ensures the model respects the permutation symmetry of graphs. However, strict equivariance can increase computational cost due to added architectural constraints, and can slow down convergence because the model must be consistent across a large space of possible node permutations. We study this trade-off for graph generative models. Specifically, we start from an equivariant discrete flow-matching model, and relax its equivariance during training via a controllable symmetry modulation scheme based on sinusoidal positional encodings and node permutations. Experiments first show that symmetry-breaking can accelerate early training by providing an easier learning signal, but at the expense of encouraging shortcut solutions that can cause overfitting, where the model repeatedly generates graphs that are duplicates of the training set. On the contrary, properly modulating the symmetry signal can delay overfitting while accelerating convergence, allowing the model to reach stronger performance with $19\%$ of the baseline training epochs.

</details>


### [121] [TempoNet: Slack-Quantized Transformer-Guided Reinforcement Scheduler for Adaptive Deadline-Centric Real-Time Dispatchs](https://arxiv.org/abs/2602.18109)
*Rong Fu,Yibo Meng,Guangzhen Yao,Jiaxuan Lu,Zeyu Zhang,Zhaolu Kang,Ziming Guo,Jia Yee Tan,Xiaojing Du,Simon James Fong*

Main category: cs.LG

TL;DR: TempoNet：基于Transformer的强化学习实时调度器，通过Urgency Tokenizer处理时间松弛度，稀疏注意力机制实现高效推理，在多核实时调度中优于传统方法


<details>
  <summary>Details</summary>
Motivation: 实时调度器需要在严格的计算预算下处理紧迫的截止时间，传统调度方法在复杂多核环境中面临挑战，需要更智能的调度决策框架

Method: 1. 使用Urgency Tokenizer将时间松弛度离散化为可学习的嵌入表示；2. 采用具有延迟感知的稀疏注意力机制，包含块状top-k选择和局部敏感分块；3. 多核映射层将Q值转换为处理器分配；4. 支持掩码贪婪选择或可微分匹配

Result: 在工业混合关键性追踪和大规模多处理器环境中，相比分析调度器和神经基线方法，在截止时间满足率方面取得一致提升，同时改善了优化稳定性，推理时间达到亚毫秒级

Conclusion: TempoNet为基于Transformer的高吞吐量实时调度决策建立了实用框架，展示了在复杂实时系统中的实际应用潜力

Abstract: Real-time schedulers must reason about tight deadlines under strict compute budgets. We present TempoNet, a reinforcement learning scheduler that pairs a permutation-invariant Transformer with a deep Q-approximation. An Urgency Tokenizer discretizes temporal slack into learnable embeddings, stabilizing value learning and capturing deadline proximity. A latency-aware sparse attention stack with blockwise top-k selection and locality-sensitive chunking enables global reasoning over unordered task sets with near-linear scaling and sub-millisecond inference. A multicore mapping layer converts contextualized Q-scores into processor assignments through masked-greedy selection or differentiable matching. Extensive evaluations on industrial mixed-criticality traces and large multiprocessor settings show consistent gains in deadline fulfillment over analytic schedulers and neural baselines, together with improved optimization stability. Diagnostics include sensitivity analyses for slack quantization, attention-driven policy interpretation, hardware-in-the-loop and kernel micro-benchmarks, and robustness under stress with simple runtime mitigations; we also report sample-efficiency benefits from behavioral-cloning pretraining and compatibility with an actor-critic variant without altering the inference pipeline. These results establish a practical framework for Transformer-based decision making in high-throughput real-time scheduling.

</details>


### [122] [Non-Stationary Online Resource Allocation: Learning from a Single Sample](https://arxiv.org/abs/2602.18114)
*Yiding Feng,Jiashuo Jiang,Yige Wang*

Main category: cs.LG

TL;DR: 研究非平稳需求下的在线资源分配问题，仅需每个周期一个历史样本，提出基于分位数的元策略，在两种样本设置下分别实现√T和对数级遗憾


<details>
  <summary>Details</summary>
Motivation: 解决实际应用中资源分配面临的两个关键挑战：需求分布的非平稳性（可能任意变化）和离线数据稀缺（每个周期仅有一个历史样本）。传统方法通常需要大量历史数据或假设平稳性，而现实环境往往是非平稳且数据有限的。

Method: 提出类型依赖的分位数元策略，将问题解耦为三个模块：1) 奖励分布估计；2) 通过流体松弛优化目标服务概率；3) 通过动态接受阈值进行实时决策。针对两种样本设置分别设计策略：奖励观测样本使用静态阈值策略，类型仅样本在最小到达概率假设下设计部分自适应和完全自适应策略。

Result: 对于奖励观测样本，静态阈值策略达到Õ(√T)遗憾；对于类型仅样本，在最小到达概率假设下，部分自适应策略达到Õ(√T)遗憾，完全自适应策略通过精心舍入实现O((log T)³)的多对数遗憾，这是非平稳多资源分配问题的首个多对数遗憾保证。

Conclusion: 该框架在仅需最小离线数据（每个周期一个样本）的情况下，能够处理任意非平稳性而无需变化预算假设，并支持多资源约束，显著推进了先前工作。特别是针对类型仅样本场景，首次实现了多对数遗憾保证。

Abstract: We study online resource allocation under non-stationary demand with a minimum offline data requirement. In this problem, a decision-maker must allocate multiple types of resources to sequentially arriving queries over a finite horizon. Each query belongs to a finite set of types with fixed resource consumption and a stochastic reward drawn from an unknown, type-specific distribution. Critically, the environment exhibits arbitrary non-stationarity -- arrival distributions may shift unpredictably-while the algorithm requires only one historical sample per period to operate effectively. We distinguish two settings based on sample informativeness: (i) reward-observed samples containing both query type and reward realization, and (ii) the more challenging type-only samples revealing only query type information.
  We propose a novel type-dependent quantile-based meta-policy that decouples the problem into modular components: reward distribution estimation, optimization of target service probabilities via fluid relaxation, and real-time decisions through dynamic acceptance thresholds. For reward-observed samples, our static threshold policy achieves $\tilde{O}(\sqrt{T})$ regret. For type-only samples, we first establish that sublinear regret is impossible without additional structure; under a mild minimum-arrival-probability assumption, we design both a partially adaptive policy attaining the same $\tilde{O}({T})$ bound and, more significantly, a fully adaptive resolving policy with careful rounding that achieves the first poly-logarithmic regret guarantee of $O((\log T)^3)$ for non-stationary multi-resource allocation. Our framework advances prior work by operating with minimal offline data (one sample per period), handling arbitrary non-stationarity without variation-budget assumptions, and supporting multiple resource constraints.

</details>


### [123] [Cut Less, Fold More: Model Compression through the Lens of Projection Geometry](https://arxiv.org/abs/2602.18116)
*Olga Saukh,Dong Wang,Haris Šikić,Yun Cheng,Lothar Thiele*

Main category: cs.LG

TL;DR: 模型折叠（folding）作为无需校准的神经网络压缩方法，在理论和实践上通常优于结构化剪枝（pruning），特别是在中等至高压缩率下。


<details>
  <summary>Details</summary>
Motivation: 无需重新训练的神经网络压缩对于大规模部署至关重要。作者从投影几何的角度研究无需校准的压缩方法，比较结构化剪枝和模型折叠两种方法。

Method: 将结构化剪枝和模型折叠形式化为正交算子，从投影几何角度分析。结构化剪枝是轴对齐投影，而模型折叠是通过权重聚类进行低秩投影。在大规模实验中评估了超过1000个检查点，涵盖ResNet18、PreActResNet18、ViT-B/32、CLIP ViT-B/32和LLaMA系列模型，覆盖多种训练超参数设置。

Result: 在秩距离为1的情况下，模型折叠在理论上证明具有更小的参数重构误差，在温和的光滑性假设下具有更小的函数扰动。实践中，模型折叠通常获得更高的压缩后精度，在中等至高压缩率下优势最大。在某些特定训练设置下，差距会缩小甚至反转。

Conclusion: 模型折叠作为一种几何感知、无需校准的压缩替代方案，在实践中通常优于剪枝，在理论上也有原理支撑。它为神经网络压缩提供了新的几何视角。

Abstract: Compressing neural networks without retraining is vital for deployment at scale. We study calibration-free compression through the lens of projection geometry: structured pruning is an axis-aligned projection, whereas model folding performs a low-rank projection via weight clustering. We formalize both as orthogonal operators and show that, within a rank distance of one, folding provably yields smaller parameter reconstruction error, and under mild smoothness assumptions, smaller functional perturbations than pruning. At scale, we evaluate >1000 checkpoints spanning ResNet18, PreActResNet18, ViT-B/32, and CLIP ViT-B/32 on CIFAR-10 and ImageNet-1K, covering diverse training hyperparameters (optimizers, learning rates, augmentations, regularization, sharpness-aware training), as well as multiple LLaMA-family 60M and 130M parameter models trained on C4. We show that folding typically achieves higher post-compression accuracy, with the largest gains at moderate-high compression. The gap narrows and occasionally reverses at specific training setups. Our results position folding as a geometry-aware, calibration-free alternative to pruning that is often superior in practice and principled in theory.

</details>


### [124] [Flow Matching with Injected Noise for Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2602.18117)
*Yongjae Shin,Jongseong Chae,Jongeui Park,Youngchul Sung*

Main category: cs.LG

TL;DR: FINO是一种基于流匹配的策略方法，通过注入噪声增强探索，结合熵引导采样平衡探索与利用，在有限在线预算下提升离线到在线强化学习的样本效率。


<details>
  <summary>Details</summary>
Motivation: 生成模型在离线强化学习中表现出色，但在扩展到在线微调时面临挑战。现有方法通常将在线微调视为离线预训练的直接延续，未能解决关键问题，特别是探索效率问题。

Method: FINO采用流匹配策略，在策略训练中注入噪声以鼓励探索离线数据集中未观察到的动作。同时结合熵引导采样机制，在在线微调过程中动态平衡探索与利用。

Result: 在多样化的挑战性任务实验中，FINO在有限的在线预算下始终表现出优越的性能，证明了其在离线到在线强化学习中的有效性。

Conclusion: FINO通过噪声注入和熵引导采样有效解决了离线到在线强化学习中的探索挑战，在样本效率方面取得了显著改进，为生成模型在强化学习中的应用提供了新思路。

Abstract: Generative models have recently demonstrated remarkable success across diverse domains, motivating their adoption as expressive policies in reinforcement learning (RL). While they have shown strong performance in offline RL, particularly where the target distribution is well defined, their extension to online fine-tuning has largely been treated as a direct continuation of offline pre-training, leaving key challenges unaddressed. In this paper, we propose Flow Matching with Injected Noise for Offline-to-Online RL (FINO), a novel method that leverages flow matching-based policies to enhance sample efficiency for offline-to-online RL. FINO facilitates effective exploration by injecting noise into policy training, thereby encouraging a broader range of actions beyond those observed in the offline dataset. In addition to exploration-enhanced flow policy training, we combine an entropy-guided sampling mechanism to balance exploration and exploitation, allowing the policy to adapt its behavior throughout online fine-tuning. Experiments across diverse, challenging tasks demonstrate that FINO consistently achieves superior performance under limited online budgets.

</details>


### [125] [Learning Long-Range Dependencies with Temporal Predictive Coding](https://arxiv.org/abs/2602.18131)
*Tom Potter,Oliver Rhodes*

Main category: cs.LG

TL;DR: 本文提出了一种结合时序预测编码(tPC)和近似实时循环学习(RTRL)的新方法，用于训练循环神经网络，在保持预测编码局部化、可并行化特性的同时，有效处理长程时间依赖任务，性能接近BPTT但更节能。


<details>
  <summary>Details</summary>
Motivation: 预测编码(PC)具有局部化、可并行化操作的特点，适合在神经形态硬件上实现节能学习。然而，将PC有效扩展到循环神经网络(RNNs)处理长程时间依赖任务一直具有挑战性。传统的BPTT方法存在非局部计算、缺乏空间并行性、需要存储大量激活历史等问题，导致能耗较高。

Method: 提出了一种新颖的方法，将时序预测编码(tPC)与近似实时循环学习(RTRL)相结合，实现有效的时空信用分配。该方法保留了PC框架的局部化、可并行化和灵活性特性。

Result: 在合成基准测试和真实世界任务上，该方法性能接近BPTT。在一个具有挑战性的机器翻译任务中，使用1500万参数模型，测试困惑度达到7.62（BPTT为7.49），这是tPC首次应用于如此规模的任务。

Conclusion: 该方法展示了在保持原始PC框架局部化、可并行化和灵活性特性的同时，学习复杂时间依赖关系的潜力，为更节能的学习系统铺平了道路。

Abstract: Predictive Coding (PC) is a biologically-inspired learning framework characterised by local, parallelisable operations, properties that enable energy-efficient implementation on neuromorphic hardware. Despite this, extending PC effectively to recurrent neural networks (RNNs) has been challenging, particularly for tasks involving long-range temporal dependencies. Backpropagation Through Time (BPTT) remains the dominant method for training RNNs, but its non-local computation, lack of spatial parallelism, and requirement to store extensive activation histories results in significant energy consumption. This work introduces a novel method combining Temporal Predictive Coding (tPC) with approximate Real-Time Recurrent Learning (RTRL), enabling effective spatio-temporal credit assignment. Results indicate that the proposed method can closely match the performance of BPTT on both synthetic benchmarks and real-world tasks. On a challenging machine translation task, with a 15-million parameter model, the proposed method achieves a test perplexity of 7.62 (vs. 7.49 for BPTT), marking one of the first applications of tPC to tasks of this scale. These findings demonstrate the potential of this method to learn complex temporal dependencies whilst retaining the local, parallelisable, and flexible properties of the original PC framework, paving the way for more energy-efficient learning systems.

</details>


### [126] [Advection-Diffusion on Graphs: A Bakry-Emery Laplacian for Spectral Graph Neural Networks](https://arxiv.org/abs/2602.18141)
*Pierre-Gabriel Berlureau,Ali Hariri,Victor Kawasaki-Borruat,Mia Zosso,Pierre Vandergheynst*

Main category: cs.LG

TL;DR: 提出基于Bakry-Emery图拉普拉斯算子的mu-ChebNet架构，通过可学习节点势能调节信息传播，解决GNN中的长距离信息传递问题，无需改变图结构。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在长距离信息传播中存在过平滑和过挤压问题，现有解决方案如图变换器或重布线通常计算成本高或需要改变图结构，需要一种不改变拓扑结构但能自适应调节信息传播的方法。

Method: 引入Bakry-Emery图拉普拉斯算子，通过可学习的节点势能整合扩散和对流，作为标准拉普拉斯算子的替代品。基于此开发mu-ChebNet，联合学习势能和切比雪夫滤波器，结合消息传递的自适应性和谱方法的效率。

Result: 在合成长距离推理任务和真实世界基准测试中取得一致性能提升，同时提供可解释的路由场，揭示信息在图中如何流动。

Conclusion: Bakry-Emery拉普拉斯算子为自适应谱图学习提供了原则性且高效的基础，mu-ChebNet有效解决了GNN的长距离信息传播问题。

Abstract: Graph Neural Networks (GNNs) often struggle to propagate information across long distances due to oversmoothing and oversquashing. Existing remedies such as graph transformers or rewiring typically incur high computational cost or require altering the graph structure. We introduce a Bakry-Emery graph Laplacian that integrates diffusion and advection through a learnable node-wise potential, inducing task-dependent propagation dynamics without modifying topology. This operator has a well-behaved spectral decomposition and acts as a drop-in replacement for standard Laplacians in spectral GNNs. Building on this insight, we develop mu-ChebNet, a spectral architecture that jointly learns the potential and Chebyshev filters, effectively bridging message-passing adaptivity and spectral efficiency. Our theoretical analysis shows how the potential modulates the spectrum, enabling control of key graph properties. Empirically, mu-ChebNet delivers consistent gains on synthetic long-range reasoning tasks, as well as real-world benchmarks, while offering an interpretable routing field that reveals how information flows through the graph. This establishes the Bakry-Emery Laplacian as a principled and efficient foundation for adaptive spectral graph learning.

</details>


### [127] [Stable Long-Horizon Spatiotemporal Prediction on Meshes Using Latent Multiscale Recurrent Graph Neural Networks](https://arxiv.org/abs/2602.18146)
*Lionel Salesses,Larbi Arbaoui,Tariq Benamara,Arnaud Francois,Caroline Sainvitu*

Main category: cs.LG

TL;DR: 提出一种用于复杂几何体上时空场长期预测的深度学习框架，采用时间多尺度架构和潜在循环图神经网络，在增材制造温度预测中表现出色。


<details>
  <summary>Details</summary>
Motivation: 复杂几何体上时空场的长期准确预测是科学机器学习的基本挑战，特别是在增材制造中，温度历史直接影响缺陷形成和机械性能。高保真模拟计算成本高，现有机器学习方法在长期温度和梯度预测方面仍有局限。

Method: 提出深度学习框架，采用时间多尺度架构：两个互补时间尺度的耦合模型。使用潜在循环图神经网络捕捉网格上的时空动态，变分图自编码器提供紧凑潜在表示以减少内存使用并提高训练稳定性。框架直接基于网格预测完整温度历史，以几何形状和工艺参数为条件。

Result: 在模拟粉末床熔融数据上的实验表明，该框架能够实现准确且时间稳定的长期预测，适用于多样几何形状，性能优于现有基线方法。虽然仅在二维评估，但框架具有通用性。

Conclusion: 该框架为复杂几何体上时空场的长期预测提供了有效解决方案，可扩展到具有多尺度动态的物理驱动系统和三维几何体，在增材制造等应用中具有重要价值。

Abstract: Accurate long-horizon prediction of spatiotemporal fields on complex geometries is a fundamental challenge in scientific machine learning, with applications such as additive manufacturing where temperature histories govern defect formation and mechanical properties. High-fidelity simulations are accurate but computationally costly, and despite recent advances, machine learning methods remain challenged by long-horizon temperature and gradient prediction. We propose a deep learning framework for predicting full temperature histories directly on meshes, conditioned on geometry and process parameters, while maintaining stability over thousands of time steps and generalizing across heterogeneous geometries. The framework adopts a temporal multiscale architecture composed of two coupled models operating at complementary time scales. Both models rely on a latent recurrent graph neural network to capture spatiotemporal dynamics on meshes, while a variational graph autoencoder provides a compact latent representation that reduces memory usage and improves training stability. Experiments on simulated powder bed fusion data demonstrate accurate and temporally stable long-horizon predictions across diverse geometries, outperforming existing baseline. Although evaluated in two dimensions, the framework is general and extensible to physics-driven systems with multiscale dynamics and to three-dimensional geometries.

</details>


### [128] [Unifying Formal Explanations: A Complexity-Theoretic Perspective](https://arxiv.org/abs/2602.18160)
*Shahaf Bassan,Xuanxiang Huang,Guy Katz*

Main category: cs.LG

TL;DR: 本文提出统一框架分析ML解释的计算复杂度，发现全局解释具有单调性、子模性等组合优化性质，可在多项式时间内计算；而局部解释则NP难。


<details>
  <summary>Details</summary>
Motivation: 先前研究分别探讨了充分理由和对比理由在不同框架下的计算复杂度，缺乏统一分析。本文旨在建立统一框架，系统研究这些解释的计算复杂性及其与组合优化性质的关系。

Method: 提出统一概率价值函数框架，将各种解释问题统一为价值函数最小化问题。通过分析价值函数的单调性、子模性和超模性等组合优化性质，研究不同解释设置下的计算复杂度。

Result: 发现全局价值函数具有单调性、子模性等性质，使得在神经网络、决策树等多种ML模型中计算全局解释可在多项式时间内完成且有理论保证。而局部价值函数缺乏这些性质，即使简化版本也是NP难的。

Conclusion: 解释的计算复杂度取决于价值函数的组合优化性质。全局解释具有良好性质可实现高效计算，而局部解释则计算困难。这为ML解释的算法设计提供了理论基础。

Abstract: Previous work has explored the computational complexity of deriving two fundamental types of explanations for ML model predictions: (1) *sufficient reasons*, which are subsets of input features that, when fixed, determine a prediction, and (2) *contrastive reasons*, which are subsets of input features that, when modified, alter a prediction. Prior studies have examined these explanations in different contexts, such as non-probabilistic versus probabilistic frameworks and local versus global settings. In this study, we introduce a unified framework for analyzing these explanations, demonstrating that they can all be characterized through the minimization of a unified probabilistic value function. We then prove that the complexity of these computations is influenced by three key properties of the value function: (1) *monotonicity*, (2) *submodularity*, and (3) *supermodularity* - which are three fundamental properties in *combinatorial optimization*. Our findings uncover some counterintuitive results regarding the nature of these properties within the explanation settings examined. For instance, although the *local* value functions do not exhibit monotonicity or submodularity/supermodularity whatsoever, we demonstrate that the *global* value functions do possess these properties. This distinction enables us to prove a series of novel polynomial-time results for computing various explanations with provable guarantees in the global explainability setting, across a range of ML models that span the interpretability spectrum, such as neural networks, decision trees, and tree ensembles. In contrast, we show that even highly simplified versions of these explanations become NP-hard to compute in the corresponding local explainability setting.

</details>


### [129] [A Deep Surrogate Model for Robust and Generalizable Long-Term Blast Wave Prediction](https://arxiv.org/abs/2602.18168)
*Danning Jing,Xinhai Chen,Xifeng Pu,Jie Hu,Chao Huang,Xuguang Chen,Qinglin Wang,Jie Liu*

Main category: cs.LG

TL;DR: RGD-Blast：一种用于高保真长期爆炸波预测的鲁棒可泛化深度代理模型，相比传统数值方法加速两个数量级，在未见建筑布局上保持高精度


<details>
  <summary>Details</summary>
Motivation: 爆炸波传播的时空动力学建模面临高度非线性行为、陡峭梯度和计算成本高的挑战。现有机器学习代理模型在复杂城市布局或分布外场景下精度下降，且自回归预测策略在长期预测中容易误差累积

Method: 提出RGD-Blast模型，包含多尺度模块捕获全局流模式和局部边界交互，减少自回归预测误差累积；引入动态-静态特征耦合机制，融合时变压力场与静态源和布局特征，增强分布外泛化能力

Result: 相比传统数值方法实现两个数量级加速，同时保持可比精度；在未见建筑布局上，280个连续时间步的平均RMSE低于0.01，R²超过0.89；在不同爆炸源位置和炸药重量下验证了泛化能力

Conclusion: RGD-Blast显著推进了长期爆炸波建模的技术水平，通过多尺度建模和动态-静态特征耦合，解决了现有代理模型的精度下降和误差累积问题，实现了高效高精度的爆炸波预测

Abstract: Accurately modeling the spatio-temporal dynamics of blast wave propagation remains a longstanding challenge due to its highly nonlinear behavior, sharp gradients, and burdensome computational cost. While machine learning-based surrogate models offer fast inference as a promising alternative, they suffer from degraded accuracy, particularly evaluated on complex urban layouts or out-of-distribution scenarios. Moreover, autoregressive prediction strategies in such models are prone to error accumulation over long forecasting horizons, limiting their robustness for extended-time simulations. To address these limitations, we propose RGD-Blast, a robust and generalizable deep surrogate model for high-fidelity, long-term blast wave forecasting. RGD-Blast incorporates a multi-scale module to capture both global flow patterns and local boundary interactions, effectively mitigating error accumulation during autoregressive prediction. We introduce a dynamic-static feature coupling mechanism that fuses time-varying pressure fields with static source and layout features, thereby enhancing out-of-distribution generalization. Experiments demonstrate that RGD-Blast achieves a two-order-of-magnitude speedup over traditional numerical methods while maintaining comparable accuracy. In generalization tests on unseen building layouts, the model achieves an average RMSE below 0.01 and an R2 exceeding 0.89 over 280 consecutive time steps. Additional evaluations under varying blast source locations and explosive charge weights further validate its generalization, substantially advancing the state of the art in long-term blast wave modeling.

</details>


### [130] [SeedFlood: A Step Toward Scalable Decentralized Training of LLMs](https://arxiv.org/abs/2602.18181)
*Jihun Kim,Namhoon Lee*

Main category: cs.LG

TL;DR: SeedFlood是一种去中心化训练新方法，通过利用零阶更新的种子可重构特性，实现近乎零大小的消息传播，使通信开销与模型大小无关，解决了去中心化训练的主要可扩展性瓶颈。


<details>
  <summary>Details</summary>
Motivation: 传统基于gossip的方法存在两个主要问题：1) 消息通信成本随模型大小增长；2) 网络跳数导致信息衰减，使得全局共识效率低下。这些限制阻碍了大规模模型在复杂网络拓扑中的去中心化训练。

Method: SeedFlood利用零阶更新的种子可重构结构，使消息大小接近零，然后通过flooding机制将消息传播到网络中的每个客户端。这种方法使通信开销变得可忽略且与模型大小无关。

Result: 实验表明，SeedFlood在去中心化LLM微调中始终优于基于gossip的基线方法，在泛化性能和通信效率方面都有更好表现，甚至在大规模设置中达到与一阶方法相当的结果。

Conclusion: SeedFlood解决了去中心化训练的主要可扩展性瓶颈，使之前被认为不切实际的训练场景成为可能，如跨数百个客户端的十亿参数模型训练，为大规模去中心化机器学习开辟了新途径。

Abstract: This work presents a new approach to decentralized training-SeedFlood-designed to scale for large models across complex network topologies and achieve global consensus with minimal communication overhead. Traditional gossip-based methods suffer from message communication costs that grow with model size, while information decay over network hops renders global consensus inefficient. SeedFlood departs from these practices by exploiting the seed-reconstructible structure of zeroth-order updates and effectively making the messages near-zero in size, allowing them to be flooded to every client in the network. This mechanism makes communication overhead negligible and independent of model size, removing the primary scalability bottleneck in decentralized training. Consequently, SeedFlood enables training in regimes previously considered impractical, such as billion-parameter models distributed across hundreds of clients. Our experiments on decentralized LLM fine-tuning demonstrate thatSeedFlood consistently outperforms gossip-based baselines in both generalization performance and communication efficiency, and even achieves results comparable to first-order methods in large scale settings.

</details>


### [131] [Capabilities Ain't All You Need: Measuring Propensities in AI](https://arxiv.org/abs/2602.18182)
*Daniel Romero-Alvarado,Fernando Martínez-Plumed,Lorenzo Pacchiardi,Hugo Save,Siddhesh Milind Pawar,Behzad Mehrbakhsh,Pablo Antonio Moreno Casares,Ben Slater,Paolo Bova,Peter Romero,Zachary R. Tyler,Jonathan Prunty,Luning Sun,Jose Hernandez-Orallo*

Main category: cs.LG

TL;DR: 提出首个测量AI倾向性的形式化框架，使用双逻辑公式定义模型在"理想区间"内的高成功率，通过LLM评估理想区间边界，发现倾向性与能力结合比单独使用任一项有更强的预测能力


<details>
  <summary>Details</summary>
Motivation: 传统AI评估主要关注能力测量，使用项目反应理论(IRT)等正式方法。但倾向性（模型展现特定行为的趋势）对性能和安全结果至关重要。传统IRT将模型成功描述为模型能力和任务需求的单调函数，这不适用于倾向性，因为过度和不足都可能有问题。

Method: 引入首个测量AI倾向性的形式化框架，使用双逻辑公式描述模型成功概率：当模型倾向性在"理想区间"内时成功概率高。使用配备新开发的任务无关评估标准的LLM来估计理想区间的边界。将该框架应用于六个LLM模型家族，这些模型的倾向性在两个方向上被激发。

Result: 能够测量倾向性偏移程度及其对任务的影响。使用一个基准估计的倾向性成功预测了保留任务的行为。结合倾向性和能力比单独使用任一项有更强的预测能力。展示了如何进行严格的倾向性测量，以及它如何超越仅使用能力评估来预测AI行为。

Conclusion: 提出了首个测量AI倾向性的形式化框架，展示了倾向性测量的可行性及其价值。倾向性与能力结合比单独使用任一项能更好地预测AI行为，为更全面的AI评估提供了新方法。

Abstract: AI evaluation has primarily focused on measuring capabilities, with formal approaches inspired from Item Response Theory (IRT) being increasingly applied. Yet propensities - the tendencies of models to exhibit particular behaviours - play a central role in determining both performance and safety outcomes. However, traditional IRT describes a model's success on a task as a monotonic function of model capabilities and task demands, an approach unsuited to propensities, where both excess and deficiency can be problematic. Here, we introduce the first formal framework for measuring AI propensities by using a bilogistic formulation for model success, which attributes high success probability when the model's propensity is within an "ideal band". Further, we estimate the limits of the ideal band using LLMs equipped with newly developed task-agnostic rubrics. Applying our framework to six families of LLM models whose propensities are incited in either direction, we find that we can measure how much the propensity is shifted and what effect this has on the tasks. Critically, propensities estimated using one benchmark successfully predict behaviour on held-out tasks. Moreover, we obtain stronger predictive power when combining propensities and capabilities than either separately. More broadly, our framework showcases how rigorous propensity measurements can be conducted and how it yields gains over solely using capability evaluations to predict AI behaviour.

</details>


### [132] [LERD: Latent Event-Relational Dynamics for Neurodegenerative Classification](https://arxiv.org/abs/2602.18195)
*Hairong Chen,Yicheng Feng,Ziyu Jia,Samir Bhatt,Hengguan Huang*

Main category: cs.LG

TL;DR: LERD：一种端到端贝叶斯电生理神经动力学系统，直接从多通道EEG推断潜在神经事件及其关系结构，用于阿尔茨海默病诊断


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病改变脑电生理并破坏多通道EEG动力学，现有方法依赖黑盒分类器且未显式建模生成观察信号的基础动力学

Method: 提出LERD系统，结合连续时间事件推断模块和随机事件生成过程，融入电生理启发的动力学先验，提供理论分析和可训练边界

Result: 在合成基准和两个真实AD EEG队列上，LERD持续优于强基线，产生生理对齐的潜在总结，有助于表征群体级动力学差异

Conclusion: LERD提供了一种可解释的、基于动力学的EEG分析方法，为阿尔茨海默病诊断和监测提供了新工具

Abstract: Alzheimer's disease (AD) alters brain electrophysiology and disrupts multichannel EEG dynamics, making accurate and clinically useful EEG-based diagnosis increasingly important for screening and disease monitoring. However, many existing approaches rely on black-box classifiers and do not explicitly model the underlying dynamics that generate observed signals. To address these limitations, we propose LERD, an end-to-end Bayesian electrophysiological neural dynamical system that infers latent neural events and their relational structure directly from multichannel EEG without event or interaction annotations. LERD combines a continuous-time event inference module with a stochastic event-generation process to capture flexible temporal patterns, while incorporating an electrophysiology-inspired dynamical prior to guide learning in a principled way. We further provide theoretical analysis that yields a tractable bound for training and stability guarantees for the inferred relational dynamics. Extensive experiments on synthetic benchmarks and two real-world AD EEG cohorts demonstrate that LERD consistently outperforms strong baselines and yields physiology-aligned latent summaries that help characterize group-level dynamical differences.

</details>


### [133] [RAT+: Train Dense, Infer Sparse -- Recurrence Augmented Attention for Dilated Inference](https://arxiv.org/abs/2602.18196)
*Xiuying Wei,Caglar Gulcehre*

Main category: cs.LG

TL;DR: RAT+是一种通过密集预训练结合全序列循环的注意力架构，可在推理时灵活切换到扩张注意力模式，仅需少量适应而非重新训练稀疏模型，在保持准确性的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 结构化扩张注意力在推理时具有效率优势（降低FLOPs和KV缓存大小），但直接将预训练注意力模型稀疏化为扩张模式会导致严重的准确性下降。需要一种既能保持密集预训练优势，又能在推理时灵活切换到高效稀疏模式的解决方案。

Method: RAT+架构在密集预训练时增强注意力机制，加入全序列循环和主动循环学习。单一模型密集预训练一次后，在推理时可灵活切换到扩张注意力模式（可选局部窗口）或混合层/头组合，仅需10亿token的短适应而非重新训练单独稀疏模型。

Result: 在15亿参数、1000亿token训练下，RAT+在16倍扩张时接近密集注意力准确性，在64倍扩张时在常识推理和LongBench任务上仅下降2-3个点。RAT+在稀疏化为top-k块注意力时甚至优于原始注意力。在26亿参数、2000亿token规模下观察到相同趋势。

Conclusion: RAT+通过密集预训练结合循环增强，实现了推理时的高效稀疏化，避免了单独训练稀疏模型的成本，在保持模型准确性的同时显著提升了推理效率，为大规模语言模型的高效部署提供了新方案。

Abstract: Structured dilated attention has an appealing inference-time efficiency knob: it reduces the FLOPs of the attention and the KV cache size by a factor of the dilation size D, while preserving long-range connectivity. However, we find a persistent failure mode of them -- sparsifying a pretrained attention model to a dilated pattern leads to severe accuracy degradation. We introduce RAT+, a dense-pretraining architecture that augments attention with full-sequence recurrence and active recurrence learning. A single RAT+ model is pretrained densely once, then flexibly switched at inference time to dilated attention (optionally with local windows) or hybrid layer/head compositions, requiring only a short 1B-token resolution adaptation rather than retraining separate sparse models. At 1.5B parameters trained on 100B tokens, RAT+ closely matches dense accuracy at 16 and drops by about 2-3 points at 64 on commonsense reasoning and LongBench tasks, respectively. Moreover, RAT+ outperforms attention when sparsifying to the top-k block attention. We further scale to 2.6B parameters and 200B tokens and observe the same trend.

</details>


### [134] [Generative Model via Quantile Assignment](https://arxiv.org/abs/2602.18216)
*Georgi Hrusanov,Oliver Y. Chén,Julien S. Bodelet*

Main category: cs.LG

TL;DR: NeuroSQL是一种新的生成范式，通过隐式学习低维潜在表示，无需辅助网络（如VAE的编码器或GAN的判别器），解决了传统生成模型训练不稳定、计算开销大等问题。


<details>
  <summary>Details</summary>
Motivation: 传统深度生成模型（如VAE和GAN）依赖辅助网络（编码器或判别器），导致训练不稳定、计算开销大、存在模式崩溃等风险。需要一种更稳定、高效的生成方法。

Method: NeuroSQL通过渐近近似将潜在变量表达为最优传输问题的解，通过求解线性分配问题学习潜在变量，然后将潜在信息传递给独立的生成器，无需辅助网络。

Result: 在MNIST、CelebA、AFHQ和OASIS四个数据集上，NeuroSQL相比VAE、GAN和扩散模型：1）图像质量更高（像素距离更小，感知/结构保真度更强）；2）训练时间最短；3）在有限训练样本下仍能有效生成合成数据。

Conclusion: NeuroSQL通过分位数分配而非编码器，提供了一种快速、稳定、鲁棒的合成数据生成方法，信息损失最小，为生成建模提供了新范式。

Abstract: Deep Generative models (DGMs) play two key roles in modern machine learning: (i) producing new information (e.g., image synthesis) and (ii) reducing dimensionality. However, traditional architectures often rely on auxiliary networks such as encoders in Variational Autoencoders (VAEs) or discriminators in Generative Adversarial Networks (GANs), which introduce training instability, computational overhead, and risks like mode collapse. We present NeuroSQL, a new generative paradigm that eliminates the need for auxiliary networks by learning low-dimensional latent representations implicitly. NeuroSQL leverages an asymptotic approximation that expresses the latent variables as the solution to an optimal transportation problem. Specifically, NeuroSQL learns the latent variables by solving a linear assignment problem and then passes the latent information to a standalone generator. We benchmark its performance against GANs, VAEs, and a budget-matched diffusion baseline on four datasets: handwritten digits (MNIST), faces (CelebA), animal faces (AFHQ), and brain images (OASIS). Compared to VAEs, GANs, and diffusion models: (1) in terms of image quality, NeuroSQL achieves overall lower mean pixel distance between synthetic and authentic images and stronger perceptual/structural fidelity; (2) computationally, NeuroSQL requires the least training time; and (3) practically, NeuroSQL provides an effective solution for generating synthetic data with limited training samples. By embracing quantile assignment rather than an encoder, NeuroSQL provides a fast, stable, and robust way to generate synthetic data with minimal information loss.

</details>


### [135] [Parameter-Efficient Domain Adaptation of Physics-Informed Self-Attention based GNNs for AC Power Flow Prediction](https://arxiv.org/abs/2602.18227)
*Redwanul Karim,Changhun Kim,Timon Conrad,Nora Gourmelon,Julian Oelhaf,David Riebesel,Tomás Arias-Vergara,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.LG

TL;DR: 论文提出了一种参数高效的领域自适应方法，将LoRA与选择性解冻预测头相结合，用于物理信息自注意力GNN，以解决AC-PF预测在电压域迁移中的问题。


<details>
  <summary>Details</summary>
Motivation: 现有物理信息图神经网络求解器在跨电压域迁移时通常需要完全微调，这导致高重训练成本，并且难以在目标域适应和源域保留之间平衡稳定性和可塑性。

Method: 采用LoRA（低秩适应）技术应用于注意力投影层，同时选择性解冻预测头，通过物理损失函数鼓励基尔霍夫一致性行为，同时限制适应过程为低秩更新。

Result: 提出的LoRA+PHead方法在多个电网拓扑中，以85.46%的可训练参数减少，实现了接近完全微调的精度（目标域RMSE差距为2.6×10⁻⁴），物理残差与完全微调相当。

Conclusion: 该方法在电压域迁移下实现了参数高效且物理一致的AC-PF估计，提供了可控的效率-精度权衡，尽管在源域保留方面比完全微调略有降低。

Abstract: Accurate AC-PF prediction under domain shift is critical when models trained on medium-voltage (MV) grids are deployed on high-voltage (HV) networks. Existing physics-informed graph neural solvers typically rely on full fine-tuning for cross-regime transfer, incurring high retraining cost and offering limited control over the stability-plasticity trade-off between target-domain adaptation and source-domain retention. We study parameter-efficient domain adaptation for physics-informed self-attention based GNN, encouraging Kirchhoff-consistent behavior via a physics-based loss while restricting adaptation to low-rank updates. Specifically, we apply LoRA to attention projections with selective unfreezing of the prediction head to regulate adaptation capacity. This design yields a controllable efficiency-accuracy trade-off for physics-constrained inverse estimation under voltage-regime shift. Across multiple grid topologies, the proposed LoRA+PHead adaptation recovers near-full fine-tuning accuracy with a target-domain RMSE gap of $2.6\times10^{-4}$ while reducing the number of trainable parameters by 85.46%. The physics-based residual remains comparable to full fine-tuning; however, relative to Full FT, LoRA+PHead reduces MV source retention by 4.7 percentage points (17.9% vs. 22.6%) under domain shift, while still enabling parameter-efficient and physically consistent AC-PF estimation.

</details>


### [136] [[Re] Benchmarking LLM Capabilities in Negotiation through Scoreable Games](https://arxiv.org/abs/2602.18230)
*Jorge Carrasco Pollo,Ioannis Kapetangeorgis,Joshua Rosenthal,John Hua Yao*

Main category: cs.LG

TL;DR: 该论文对Abdelnabi等人(2024)提出的基于可评分游戏的LLM多智能体谈判基准进行了可重复性研究，发现基准虽然复杂但模型比较存在模糊性，揭示了实验设置的局限性。


<details>
  <summary>Details</summary>
Motivation: LLM在多智能体谈判任务中展现潜力，但该领域缺乏稳健且可泛化的评估基准。Abdelnabi等人(2024)提出了基于可评分游戏的谈判基准，本文旨在验证该基准的可重复性、可用性和泛化性。

Method: 1) 复制原始实验并在更多模型上进行测试；2) 引入额外指标验证谈判质量和评估公平性；3) 在扩展版基准上分析更广泛模型的行为；4) 检查实验设置的局限性，特别是信息泄露检测和消融研究的完整性。

Result: 研究发现：1) 基准确实复杂，但模型比较存在模糊性，质疑其客观性；2) 实验设置存在局限性，特别是信息泄露检测不足和消融研究不彻底；3) 通过分析更广泛模型在扩展基准上的表现，为潜在用户提供了额外背景信息。

Conclusion: 研究强调了模型比较评估中上下文的重要性，指出现有基准在客观性和实验严谨性方面存在问题，为未来谈判基准的开发提供了重要见解。

Abstract: Large Language Models (LLMs) demonstrate significant potential in multi-agent negotiation tasks, yet evaluation in this domain remains challenging due to a lack of robust and generalizable benchmarks. Abdelnabi et al. (2024) introduce a negotiation benchmark based on Scoreable Games, with the aim of developing a highly complex and realistic evaluation framework for LLMs. Our work investigates the reproducibility of claims in their benchmark, and provides a deeper understanding of its usability and generalizability. We replicate the original experiments on additional models, and introduce additional metrics to verify negotiation quality and evenness of evaluation. Our findings reveal that while the benchmark is indeed complex, model comparison is ambiguous, raising questions about its objectivity. Furthermore, we identify limitations in the experimental setup, particularly in information leakage detection and thoroughness of the ablation study. By examining and analyzing the behavior of a wider range of models on an extended version of the benchmark, we reveal insights that provide additional context to potential users. Our results highlight the importance of context in model-comparative evaluations.

</details>


### [137] [Neural-HSS: Hierarchical Semi-Separable Neural PDE Solver](https://arxiv.org/abs/2602.18248)
*Pietro Sittoni,Emanuele Zangrando,Angelo A. Casulli,Nicola Guglielmi,Francesco Tudisco*

Main category: cs.LG

TL;DR: 提出Neural-HSS架构，基于分层半可分矩阵结构，针对椭圆型PDEs实现数据高效学习，在低数据量下仍保持精确性


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在求解PDEs方面表现出色，但许多关键应用仍受限于大规模高质量数据集生成和模型训练的计算成本。需要开发参数效率高且数据高效的架构

Method: 受椭圆型PDEs格林函数结构启发，提出基于分层半可分矩阵结构的Neural-HSS架构。理论分析证明其在低数据量下的精确性，并探索与傅里叶神经算子层和卷积层的联系

Result: 在200万网格点的三维泊松方程上验证了数据效率，在低数据量下优于基线方法。展示了在电磁学、流体动力学和生物学等多个领域PDEs数据上的学习能力

Conclusion: Neural-HSS是一种参数效率高、数据高效的架构，特别适用于椭圆型PDEs，在低数据量下仍能保持精确性，具有广泛的应用潜力

Abstract: Deep learning-based methods have shown remarkable effectiveness in solving PDEs, largely due to their ability to enable fast simulations once trained. However, despite the availability of high-performance computing infrastructure, many critical applications remain constrained by the substantial computational costs associated with generating large-scale, high-quality datasets and training models. In this work, inspired by studies on the structure of Green's functions for elliptic PDEs, we introduce Neural-HSS, a parameter-efficient architecture built upon the Hierarchical Semi-Separable (HSS) matrix structure that is provably data-efficient for a broad class of PDEs. We theoretically analyze the proposed architecture, proving that it satisfies exactness properties even in very low-data regimes. We also investigate its connections with other architectural primitives, such as the Fourier neural operator layer and convolutional layers. We experimentally validate the data efficiency of Neural-HSS on the three-dimensional Poisson equation over a grid of two million points, demonstrating its superior ability to learn from data generated by elliptic PDEs in the low-data regime while outperforming baseline methods. Finally, we demonstrate its capability to learn from data arising from a broad class of PDEs in diverse domains, including electromagnetism, fluid dynamics, and biology.

</details>


### [138] [Variational Distributional Neuron](https://arxiv.org/abs/2602.18250)
*Yves Ruffenach*

Main category: cs.LG

TL;DR: 提出变分分布神经元的概念验证：将神经元设计为VAE模块，显式携带先验、摊销后验和局部ELBO，使计算从传播标量值变为在约束下收缩可能性空间。


<details>
  <summary>Details</summary>
Motivation: 解决结构张力：序列生成中因果性主要在符号空间组织，潜在变量常为辅助角色；而概率潜在模型虽能捕捉变化因素和不确定性，但不确定性通常由全局或参数机制承载，计算单元仍传播标量值。核心问题：若不确定性是计算的内在属性，为何计算单元不显式携带它？

Method: 提出变分分布神经元：每个神经元参数化后验分布，传播重参数化样本，并通过局部ELBO的KL项进行正则化。分析"崩溃"模式和"活神经元"条件，通过自回归先验在时间上扩展贡献。

Result: 概念验证提出：神经元变为分布而非确定性标量，计算变为在约束下收缩可能性空间。通过局部约束可测试这种"收缩"，并通过内部度量监控。单元携带的上下文信息量及其时间持久性可通过不同约束局部调整。

Conclusion: 提出两个轴心：(1)概率约束的组合需稳定、可解释和可控；(2)粒度问题：若推断是在约束下分布协商，原始单元应保持确定性还是变为分布性？变分分布神经元为计算单元显式携带不确定性提供了新范式。

Abstract: We propose a proof of concept for a variational distributional neuron: a compute unit formulated as a VAE brick, explicitly carrying a prior, an amortized posterior and a local ELBO. The unit is no longer a deterministic scalar but a distribution: computing is no longer about propagating values, but about contracting a continuous space of possibilities under constraints. Each neuron parameterizes a posterior, propagates a reparameterized sample and is regularized by the KL term of a local ELBO - hence, the activation is distributional. This "contraction" becomes testable through local constraints and can be monitored via internal measures. The amount of contextual information carried by the unit, as well as the temporal persistence of this information, are locally tuned by distinct constraints. This proposal addresses a structural tension: in sequential generation, causality is predominantly organized in the symbolic space and, even when latents exist, they often remain auxiliary, while the effective dynamics are carried by a largely deterministic decoder. In parallel, probabilistic latent models capture factors of variation and uncertainty, but that uncertainty typically remains borne by global or parametric mechanisms, while units continue to propagate scalars - hence the pivot question: if uncertainty is intrinsic to computation, why does the compute unit not carry it explicitly? We therefore draw two axes: (i) the composition of probabilistic constraints, which must be made stable, interpretable and controllable; and (ii) granularity: if inference is a negotiation of distributions under constraints, should the primitive unit remain deterministic or become distributional? We analyze "collapse" modes and the conditions for a "living neuron", then extend the contribution over time via autoregressive priors over the latent, per unit.

</details>


### [139] [MEG-to-MEG Transfer Learning and Cross-Task Speech/Silence Detection with Limited Data](https://arxiv.org/abs/2602.18253)
*Xabier de Zuazo,Vincenzo Verbeni,Eva Navas,Ibon Saratxaga,Mathieu Bourguignon,Nicola Molinaro*

Main category: cs.LG

TL;DR: 首次展示MEG语音模型的跨任务迁移学习，通过预训练-微调框架实现感知与产生任务间的有效解码


<details>
  <summary>Details</summary>
Motivation: 解决语音脑机接口中的数据效率问题，探索感知与产生任务间的共享神经表征

Method: 使用Conformer模型在50小时单被试听音数据上预训练，然后在18名被试每人仅5分钟数据上微调，实现感知与产生任务的跨任务解码

Result: 迁移学习带来1-4%的组内任务准确率提升和5-6%的跨任务提升，语音产生模型能解码被动听音任务，证明学习到的表征反映共享神经过程

Conclusion: 预训练显著提升数据效率，感知与产生任务共享神经表征，为语音脑机接口的跨任务解码提供了新途径

Abstract: Data-efficient neural decoding is a central challenge for speech brain-computer interfaces. We present the first demonstration of transfer learning and cross-task decoding for MEG-based speech models spanning perception and production. We pre-train a Conformer-based model on 50 hours of single-subject listening data and fine-tune on just 5 minutes per subject across 18 participants. Transfer learning yields consistent improvements, with in-task accuracy gains of 1-4% and larger cross-task gains of up to 5-6%. Not only does pre-training improve performance within each task, but it also enables reliable cross-task decoding between perception and production. Critically, models trained on speech production decode passive listening above chance, confirming that learned representations reflect shared neural processes rather than task-specific motor activity.

</details>


### [140] [A Probabilistic Framework for LLM-Based Model Discovery](https://arxiv.org/abs/2602.18266)
*Stefan Wahl,Raphaela Schenk,Ali Farnoud,Jakob H. Macke,Daniel Gedon*

Main category: cs.LG

TL;DR: 将模型发现重新构建为概率推断问题，提出基于序列蒙特卡洛采样的ModelSMC算法，通过LLM迭代提出和精炼候选模型，在真实科学系统中发现可解释机制并改进后验预测检查


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的模型发现方法通常采用手工设计的启发式流程，缺乏明确的概率公式化。作者希望将模型发现重新构建为概率推断问题，为模型提出、精炼和选择提供统一的推理框架

Method: 提出ModelSMC算法，基于序列蒙特卡洛采样。将候选模型表示为粒子，由LLM迭代提出和精炼，并使用基于似然的标准进行加权。将模型发现视为从未知分布中采样能够解释数据的机制模型

Result: 在真实世界科学系统上的实验表明，该方法能够发现具有可解释机制的模型，并改进了后验预测检查。为理解和开发基于LLM的模型发现方法提供了概率视角

Conclusion: 将模型发现重新构建为概率推断问题提供了一个统一的框架，ModelSMC作为具体实现展示了这一视角的价值，为基于LLM的模型发现方法提供了概率理论基础

Abstract: Automated methods for discovering mechanistic simulator models from observational data offer a promising path toward accelerating scientific progress. Such methods often take the form of agentic-style iterative workflows that repeatedly propose and revise candidate models by imitating human discovery processes. However, existing LLM-based approaches typically implement such workflows via hand-crafted heuristic procedures, without an explicit probabilistic formulation. We recast model discovery as probabilistic inference, i.e., as sampling from an unknown distribution over mechanistic models capable of explaining the data. This perspective provides a unified way to reason about model proposal, refinement, and selection within a single inference framework. As a concrete instantiation of this view, we introduce ModelSMC, an algorithm based on Sequential Monte Carlo sampling. ModelSMC represents candidate models as particles which are iteratively proposed and refined by an LLM, and weighted using likelihood-based criteria. Experiments on real-world scientific systems illustrate that this formulation discovers models with interpretable mechanisms and improves posterior predictive checks. More broadly, this perspective provides a probabilistic lens for understanding and developing LLM-based approaches to model discovery.

</details>


### [141] [PRISM: Parallel Reward Integration with Symmetry for MORL](https://arxiv.org/abs/2602.18277)
*Finn van der Knaap,Kejiang Qian,Zheng Xu,Fengxiang He*

Main category: cs.LG

TL;DR: PRISM算法通过引入反射对称性作为归纳偏置，解决多目标强化学习中目标时间频率异质性问题，使用ReSymNet模型和SymReg正则化器提升稀疏奖励的信用分配效率。


<details>
  <summary>Details</summary>
Motivation: 在多目标强化学习中，当各目标的时间频率差异很大时，密集奖励目标会主导学习过程，而稀疏的长时程奖励则信用分配薄弱，导致样本效率低下。

Method: 提出PRISM算法：1) ReSymNet模型通过残差块学习缩放机会价值，协调目标间的时间频率不匹配；2) SymReg反射等变正则化器强制智能体镜像，将策略搜索限制在反射等变子空间。

Result: 在MuJoCo基准测试中，PRISM持续优于稀疏奖励基线和完整密集奖励的oracle模型，超体积增益超过基线100%，比oracle提升达32%，改善了帕累托覆盖和分布平衡。

Conclusion: 通过反射对称性作为归纳偏置，PRISM有效解决了多目标强化学习中的时间频率异质性问题，显著提升了稀疏奖励的信用分配效率和整体性能。

Abstract: This work studies heterogeneous Multi-Objective Reinforcement Learning (MORL), where objectives can differ sharply in temporal frequency. Such heterogeneity allows dense objectives to dominate learning, while sparse long-horizon rewards receive weak credit assignment, leading to poor sample efficiency. We propose a Parallel Reward Integration with Symmetry (PRISM) algorithm that enforces reflectional symmetry as an inductive bias in aligning reward channels. PRISM introduces ReSymNet, a theory-motivated model that reconciles temporal-frequency mismatches across objectives, using residual blocks to learn a scaled opportunity value that accelerates exploration while preserving the optimal policy. We also propose SymReg, a reflectional equivariance regulariser that enforces agent mirroring and constrains policy search to a reflection-equivariant subspace. This restriction provably reduces hypothesis complexity and improves generalisation. Across MuJoCo benchmarks, PRISM consistently outperforms both a sparse-reward baseline and an oracle trained with full dense rewards, improving Pareto coverage and distributional balance: it achieves hypervolume gains exceeding 100\% over the baseline and up to 32\% over the oracle. The code is at \href{https://github.com/EVIEHub/PRISM}{https://github.com/EVIEHub/PRISM}.

</details>


### [142] [Decoding as Optimisation on the Probability Simplex: From Top-K to Top-P (Nucleus) to Best-of-K Samplers](https://arxiv.org/abs/2602.18292)
*Xiaotong Ji,Rasul Tutunov,Matthieu Zimmer,Haitham Bou-Ammar*

Main category: cs.LG

TL;DR: 论文提出将解码视为一个原则性的优化层，而非启发式调参，通过正则化问题统一现有解码方法，并基于此框架设计了新的Best-of-K解码器来提升多样本管道的性能。


<details>
  <summary>Details</summary>
Motivation: 当前解码方法（如贪婪解码、Top-K、Top-P等）被视为启发式的调参过程，缺乏统一的理论框架。作者认为解码应该被理解为一个原则性的优化层，在语言模型和应用之间起到关键作用。

Method: 提出一个统一框架：在每个token位置，解决一个在概率单纯形上的正则化优化问题，平衡模型分数与结构偏好/约束。该框架将现有解码方法统一为特例，并基于此设计了新的Best-of-K解码器，使用KL散度锚定的覆盖目标来优化多样本管道。

Result: Best-of-K解码器在固定K样本预算下提高了覆盖良好替代方案的概率，显著提升了性能。例如，在Qwen2.5-Math-7B模型上，MATH500数据集在高采样温度下准确率提升了+18.6%。

Conclusion: 解码应该被视为一个原则性的优化层而非启发式调参，提出的统一框架不仅解释了现有解码方法的共性，还便于设计新的解码器。Best-of-K解码器展示了该框架的实际价值，能显著提升多样本管道的性能。

Abstract: Decoding sits between a language model and everything we do with it, yet it is still treated as a heuristic knob-tuning exercise. We argue decoding should be understood as a principled optimisation layer: at each token, we solve a regularised problem over the probability simplex that trades off model score against structural preferences and constraints. This single template recovers greedy decoding, Softmax sampling, Top-K, Top-P, and Sparsemax-style sparsity as special cases, and explains their common structure through optimality conditions. More importantly, the framework makes it easy to invent new decoders without folklore. We demonstrate this by designing Best-of-K (BoK), a KL-anchored coverage objective aimed at multi-sample pipelines (self-consistency, reranking, verifier selection). BoK targets the probability of covering good alternatives within a fixed K-sample budget and improves empirical performance. We show that such samples can improve accuracy by, for example, +18.6% for Qwen2.5-Math-7B on MATH500 at high sampling temperatures.

</details>


### [143] [Analyzing and Improving Chain-of-Thought Monitorability Through Information Theory](https://arxiv.org/abs/2602.18297)
*Usman Anwar,Tim Bakker,Dana Kianfar,Cristina Pinneri,Christos Louizos*

Main category: cs.LG

TL;DR: 本文通过信息论分析CoT监控的可行性，提出两种训练方法提升监控准确性并防止CoT退化


<details>
  <summary>Details</summary>
Motivation: 研究CoT监控的理论基础和实践限制，解决监控器在实际应用中的性能问题，防止奖励黑客行为

Method: 1. 信息论分析CoT与输出的互信息条件；2. 识别信息差距和启发误差；3. 提出两种训练方法：基于oracle的直接奖励法和无标签的条件互信息最大化法

Result: 两种方法在多个环境中显著提升监控准确性，防止CoT退化，有效缓解任务奖励不完善时的奖励黑客问题

Conclusion: CoT监控可通过针对性训练系统改进，信息论分析为理解监控可行性提供了理论基础，提出的方法在实践中有效

Abstract: Chain-of-thought (CoT) monitors are LLM-based systems that analyze reasoning traces to detect when outputs may exhibit attributes of interest, such as test-hacking behavior during code generation. In this paper, we use information-theoretic analysis to show that non-zero mutual information between CoT and output is a necessary but not sufficient condition for CoT monitorability. We identify two sources of approximation error that may undermine the performance of CoT monitors in practice: information gap, which measures the extent to which the monitor can extract the information available in CoT, and elicitation error, which measures the extent to which the monitor approximates the optimal monitoring function. We further demonstrate that CoT monitorability can be systematically improved through targeted training objectives. To this end, we propose two complementary approaches: (a) an oracle-based method that directly rewards the monitored model for producing CoTs that maximize monitor accuracy, and (b) a more practical, label-free approach that maximizes conditional mutual information between outputs and CoTs. Across multiple different environments, we show both methods significantly improve monitor accuracy while preventing CoT degeneration even when training against a monitor, thereby mitigating reward hacking when the task reward is imperfectly specified.

</details>


### [144] [On the Semantic and Syntactic Information Encoded in Proto-Tokens for One-Step Text Reconstruction](https://arxiv.org/abs/2602.18301)
*Ivan Bondarenko,Egor Palkin,Fedor Tikunov*

Main category: cs.LG

TL;DR: 本文研究LLM中两个学习到的原型令牌如何编码信息，发现m-令牌更倾向于捕获语义信息，锚点约束会显著降低重建精度，而关系蒸馏可以在不牺牲重建质量的情况下将语义关系转移到原型令牌空间。


<details>
  <summary>Details</summary>
Motivation: 最近的研究表明冻结的LLM可以从仅两个学习到的原型令牌中重建数百个令牌，这为超越自回归范式提供了可能。本文旨在探究这些原型令牌编码了哪些信息以及它们在重建和受控约束下的行为。

Method: 进行了一系列实验：1）分离两个原型令牌中的语义和句法内容；2）分析e-令牌的稳定性特性；3）可视化重建过程中对e-令牌的注意力模式；4）测试两种正则化方案：基于锚点的损失和关系蒸馏目标，使用教师嵌入来"强加"语义结构到e-令牌上。

Result: 1）在标准优化下，m-令牌比e-令牌更强烈地捕获语义信息；2）基于锚点的约束会与重建精度形成尖锐的权衡；3）关系蒸馏可以将批次级语义关系转移到原型令牌空间，且不牺牲重建质量。

Conclusion: 研究支持了未来非自回归seq2seq系统的可行性，这些系统可以将原型令牌作为中间表示进行预测，为超越传统自回归文本生成提供了有前景的路径。

Abstract: Autoregressive large language models (LLMs) generate text token-by-token, requiring n forward passes to produce a sequence of length n. Recent work, Exploring the Latent Capacity of LLMs for One-Step Text Reconstruction (Mezentsev and Oseledets), shows that frozen LLMs can reconstruct hundreds of tokens from only two learned proto-tokens in a single forward pass, suggesting a path beyond the autoregressive paradigm. In this paper, we study what information these proto-tokens encode and how they behave under reconstruction and controlled constraints. We perform a series of experiments aimed at disentangling semantic and syntactic content in the two proto-tokens, analyzing stability properties of the e-token, and visualizing attention patterns to the e-token during reconstruction. Finally, we test two regularization schemes for "imposing" semantic structure on the e-token using teacher embeddings, including an anchor-based loss and a relational distillation objective. Our results indicate that the m-token tends to capture semantic information more strongly than the e-token under standard optimization; anchor-based constraints trade off sharply with reconstruction accuracy; and relational distillation can transfer batch-level semantic relations into the proto-token space without sacrificing reconstruction quality, supporting the feasibility of future non-autoregressive seq2seq systems that predict proto-tokens as an intermediate representation.

</details>


### [145] [JPmHC Dynamical Isometry via Orthogonal Hyper-Connections](https://arxiv.org/abs/2602.18308)
*Biswa Sengupta,Jinhua Wang,Leo Brunswic*

Main category: cs.LG

TL;DR: JPmHC提出了一种保持雅可比谱特性的超连接框架，通过可训练的线性混合器替代恒等映射，在算子范数有界流形上约束混合器，解决了传统超连接训练不稳定、可扩展性有限和内存开销大的问题。


<details>
  <summary>Details</summary>
Motivation: 超连接等深度学习创新虽然提升了性能，但破坏了残差连接的恒等映射特性，导致训练不稳定、可扩展性有限和内存开销增加。需要一种既能保持性能优势又能解决这些问题的框架。

Method: JPmHC用可训练的线性混合器替代恒等跳跃连接，在n个并行流上操作，通过算子范数有界流形（如双随机、Stiefel、Grassmann）约束混合器M，控制梯度条件。包含三个关键技术：自由概率分析预测雅可比谱、内存高效的隐式微分、通过Cayley变换的Stiefel约束混合器。

Result: 在ARC-AGI上的实验表明，JPmHC相比双随机基线实现了更快的收敛速度、更高的准确率和更低的计算成本，同时防止了梯度病理并增强了稳定性。

Conclusion: JPmHC作为一种灵活可扩展的超连接扩展，推进了谱感知、稳定且高效的深度学习，为拓扑架构设计和基础模型演进提供了见解。

Abstract: Recent advances in deep learning, exemplified by Hyper-Connections (HC), have expanded the residual connection paradigm by introducing wider residual streams and diverse connectivity patterns. While these innovations yield significant performance gains, they compromise the identity mapping property of residual connections, leading to training instability, limited scalability, and increased memory overhead. To address these challenges, we propose JPmHC (Jacobian-spectrum Preserving manifold-constrained Hyper-Connections), a framework that replaces identity skips with a trainable linear mixer acting on n parallel streams while explicitly controlling gradient conditioning. By constraining the mixer M on operator-norm-bounded manifolds (e.g., bistochastic, Stiefel, Grassmann), JPmHC prevents gradient pathologies and enhances stability. JPmHC introduces three key contributions: (i) a free-probability analysis that predicts Jacobian spectra for structured skips, providing actionable design rules for mixer selection; (ii) memory-efficient implicit differentiation for fixed-point projections, reducing activation memory and synchronization overhead; and (iii) a Stiefel-constrained mixer via Cayley transforms, ensuring orthogonality without post-hoc normalization. Empirical evaluations on ARC-AGI demonstrate that JPmHC achieves faster convergence, higher accuracy, and lower computational cost compared to bistochastic baselines. As a flexible and scalable extension of HC, JPmHC advances spectrum-aware, stable, and efficient deep learning, offering insights into topological architecture design and foundational model evolution.

</details>


### [146] [On the "Induction Bias" in Sequence Models](https://arxiv.org/abs/2602.18333)
*M. Reza Ebrahimi,Michaël Defferrard,Sunny Panchal,Roland Memisevic*

Main category: cs.LG

TL;DR: Transformer模型在状态追踪任务上存在根本性挑战，即使在训练和评估分布匹配的情况下，其数据效率远低于RNN，且缺乏跨序列长度的权重共享能力。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在语言模型实践中取得了显著成功，但最近的研究对其状态追踪能力提出了质疑。现有研究主要关注其在分布外泛化（如长度外推）上的失败，本文转向研究这些局限在分布内的影响。

Method: 通过大规模实验研究，比较Transformer和RNN在不同监督机制下的数据效率；分析学习到的状态追踪机制在不同序列长度间的共享程度；考察权重跨长度共享的情况。

Result: 1. Transformer所需训练数据量随状态空间大小和序列长度的增长远快于RNN；2. Transformer在不同序列长度间表现出可忽略甚至有害的权重共享，表明它们学习的是长度特定的孤立解决方案；3. 循环模型通过跨长度共享权重实现有效的摊销学习，允许一个序列长度的数据改善其他长度的性能。

Conclusion: 状态追踪仍然是Transformer面临的根本性挑战，即使在训练和评估分布匹配的情况下。Transformer缺乏RNN所具有的跨长度权重共享能力，导致数据效率低下和孤立学习。

Abstract: Despite the remarkable practical success of transformer-based language models, recent work has raised concerns about their ability to perform state tracking. In particular, a growing body of literature has shown this limitation primarily through failures in out-of-distribution (OOD) generalization, such as length extrapolation. In this work, we shift attention to the in-distribution implications of these limitations. We conduct a large-scale experimental study of the data efficiency of transformers and recurrent neural networks (RNNs) across multiple supervision regimes. We find that the amount of training data required by transformers grows much more rapidly with state-space size and sequence length than for RNNs. Furthermore, we analyze the extent to which learned state-tracking mechanisms are shared across different sequence lengths. We show that transformers exhibit negligible or even detrimental weight sharing across lengths, indicating that they learn length-specific solutions in isolation. In contrast, recurrent models exhibit effective amortized learning by sharing weights across lengths, allowing data from one sequence length to improve performance on others. Together, these results demonstrate that state tracking remains a fundamental challenge for transformers, even when training and evaluation distributions match.

</details>


### [147] [Explaining AutoClustering: Uncovering Meta-Feature Contribution in AutoML for Clustering](https://arxiv.org/abs/2602.18348)
*Matheus Camilo da Silva,Leonardo Arrighi,Ana Carolina Lorena,Sylvio Barbon Junior*

Main category: cs.LG

TL;DR: 该论文研究了AutoClustering元模型的可解释性，通过分析22种现有方法的元特征，应用全局和局部可解释性技术来揭示元特征对聚类算法和超参数选择的影响。


<details>
  <summary>Details</summary>
Motivation: 当前AutoClustering系统虽然性能良好，但其推荐决策缺乏可解释性。元特征对算法和超参数选择的影响通常不透明，这限制了系统的可靠性、偏差诊断和元特征工程效率，需要提高无监督学习自动化的决策透明度。

Method: 1) 回顾22种现有方法并构建元特征结构化分类法；2) 应用全局可解释性技术（决策谓词图）评估元模型中特征重要性；3) 使用局部可解释性工具（如SHAP）分析特定聚类决策。

Result: 研究发现元特征相关性存在一致模式，识别出当前元学习策略中的结构弱点（可能扭曲推荐），并为更可解释的AutoML设计提供了实用指导。

Conclusion: 该研究为提高无监督学习自动化的决策透明度提供了实践基础，有助于构建更可靠、可解释的AutoClustering系统。

Abstract: AutoClustering methods aim to automate unsupervised learning tasks, including algorithm selection (AS), hyperparameter optimization (HPO), and pipeline synthesis (PS), by often leveraging meta-learning over dataset meta-features. While these systems often achieve strong performance, their recommendations are often difficult to justify: the influence of dataset meta-features on algorithm and hyperparameter choices is typically not exposed, limiting reliability, bias diagnostics, and efficient meta-feature engineering. This limits reliability and diagnostic insight for further improvements. In this work, we investigate the explainability of the meta-models in AutoClustering. We first review 22 existing methods and organize their meta-features into a structured taxonomy. We then apply a global explainability technique (i.e., Decision Predicate Graphs) to assess feature importance within meta-models from selected frameworks. Finally, we use local explainability tools such as SHAP (SHapley Additive exPlanations) to analyse specific clustering decisions. Our findings highlight consistent patterns in meta-feature relevance, identify structural weaknesses in current meta-learning strategies that can distort recommendations, and provide actionable guidance for more interpretable Automated Machine Learning (AutoML) design. This study therefore offers a practical foundation for increasing decision transparency in unsupervised learning automation.

</details>


### [148] [FedZMG: Efficient Client-Side Optimization in Federated Learning](https://arxiv.org/abs/2602.18384)
*Fotios Zantalis,Evangelos Zervas,Grigorios Koulouras*

Main category: cs.LG

TL;DR: FedZMG是一种无需额外参数、客户端侧的联邦学习优化算法，通过将本地梯度投影到零均值超平面来缓解非IID数据导致的客户端漂移问题，在保持通信效率的同时提升收敛速度和模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端数据通常是非独立同分布的，这会导致客户端漂移问题，降低收敛速度和模型性能。现有的自适应优化器虽然能缓解这一问题，但往往引入计算复杂度或通信开销，不适合资源受限的物联网环境。

Method: 提出FedZMG算法，基于梯度中心化的思想，将本地梯度投影到零均值超平面上，有效中和异构数据分布中的"强度"或"偏差"偏移，无需额外通信或超参数调优。

Result: 理论分析证明FedZMG能降低有效梯度方差并保证比标准FedAvg更紧的收敛界。在EMNIST、CIFAR100和Shakespeare数据集上的实验表明，FedZMG在高度非IID设置下比FedAvg和FedAdam基线具有更好的收敛速度和最终验证准确率。

Conclusion: FedZMG是一种参数免费、通信高效的客户端优化算法，能有效解决联邦学习中的客户端漂移问题，特别适合资源受限的物联网环境中的非IID数据场景。

Abstract: Federated Learning (FL) enables distributed model training on edge devices while preserving data privacy. However, clients tend to have non-Independent and Identically Distributed (non-IID) data, which often leads to client-drift, and therefore diminishing convergence speed and model performance. While adaptive optimizers have been proposed to mitigate these effects, they frequently introduce computational complexity or communication overhead unsuitable for resource-constrained IoT environments. This paper introduces Federated Zero Mean Gradients (FedZMG), a novel, parameter-free, client-side optimization algorithm designed to tackle client-drift by structurally regularizing the optimization space. Advancing the idea of Gradient Centralization, FedZMG projects local gradients onto a zero-mean hyperplane, effectively neutralizing the "intensity" or "bias" shifts inherent in heterogeneous data distributions without requiring additional communication or hyperparameter tuning. A theoretical analysis is provided, proving that FedZMG reduces the effective gradient variance and guarantees tighter convergence bounds compared to standard FedAvg. Extensive empirical evaluations on EMNIST, CIFAR100, and Shakespeare datasets demonstrate that FedZMG achieves better convergence speed and final validation accuracy compared to the baseline FedAvg and the adaptive optimizer FedAdam, particularly in highly non-IID settings.

</details>


### [149] [PRISM-FCP: Byzantine-Resilient Federated Conformal Prediction via Partial Sharing](https://arxiv.org/abs/2602.18396)
*Ehsan Lari,Reza Arablouei,Stefan Werner*

Main category: cs.LG

TL;DR: PRISM-FCP：一种拜占庭鲁棒的联邦共形预测框架，通过部分模型共享和统计边缘校准，在训练和校准阶段都抵御攻击，保持覆盖率保证同时减少通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有联邦共形预测方法仅在校准阶段处理对抗行为，而训练阶段学习的模型容易受到中毒更新的影响。需要一种端到端的拜占庭鲁棒解决方案，同时保证通信效率。

Method: 1. 训练阶段：客户端每轮只传输D个参数中的M个（部分共享），将攻击者扰动的预期能量衰减M/D倍。2. 校准阶段：将非共形分数转换为特征向量，计算基于距离的恶意分数，在估计共形分位数前对可疑拜占庭贡献进行降权或过滤。

Result: 在合成数据和UCI超导数据集上的实验表明，PRISM-FCP在拜占庭攻击下保持名义覆盖率保证，避免了标准FCP中观察到的区间膨胀，同时减少了通信开销。

Conclusion: PRISM-FCP提供了一个鲁棒且通信高效的联邦不确定性量化方法，通过端到端的拜占庭鲁棒性设计，在训练和校准阶段都有效抵御攻击，实现了更好的均方误差和更紧的预测区间。

Abstract: We propose PRISM-FCP (Partial shaRing and robust calIbration with Statistical Margins for Federated Conformal Prediction), a Byzantine-resilient federated conformal prediction framework that utilizes partial model sharing to improve robustness against Byzantine attacks during both model training and conformal calibration. Existing approaches address adversarial behavior only in the calibration stage, leaving the learned model susceptible to poisoned updates. In contrast, PRISM-FCP mitigates attacks end-to-end. During training, clients partially share updates by transmitting only $M$ of $D$ parameters per round. This attenuates the expected energy of an adversary's perturbation in the aggregated update by a factor of $M/D$, yielding lower mean-square error (MSE) and tighter prediction intervals. During calibration, clients convert nonconformity scores into characterization vectors, compute distance-based maliciousness scores, and downweight or filter suspected Byzantine contributions before estimating the conformal quantile. Extensive experiments on both synthetic data and the UCI Superconductivity dataset demonstrate that PRISM-FCP maintains nominal coverage guarantees under Byzantine attacks while avoiding the interval inflation observed in standard FCP with reduced communication, providing a robust and communication-efficient approach to federated uncertainty quantification.

</details>


### [150] [Leakage and Second-Order Dynamics Improve Hippocampal RNN Replay](https://arxiv.org/abs/2602.18401)
*Josue Casco-Rodriguez,Nanda H. Krishna,Richard G. Baraniuk*

Main category: cs.LG

TL;DR: 本文重新审视噪声循环神经网络中的重放现象，将其视为采样过程，提出了改进重放性能的新方法：通过隐藏状态泄漏、适应性和动量机制来优化探索与速度的平衡。


<details>
  <summary>Details</summary>
Motivation: 生物神经网络（如海马体）能够内部生成类似刺激驱动活动的"重放"现象。现有计算模型将重放描述为朗之万采样，但新的重放改进方法已超越这一描述。本文旨在重新审视噪声RNN重放作为采样过程，以理解和改进重放性能。

Method: 1. 在简单假设下证明重放活动应遵循的梯度是时变的且难以估计，但支持在RNN中使用隐藏状态泄漏
2. 验证隐藏状态适应（负反馈）促进重放探索，但会导致非马尔可夫采样并减慢重放速度
3. 提出首个通过隐藏状态动量实现时间压缩重放的模型，将其与欠阻尼朗之万采样联系起来，并与适应性结合以平衡速度与探索

Result: 通过2D三角形路径、T迷宫路径以及合成大鼠位置细胞活动的高维路径的路径整合验证了研究结果。隐藏状态动量与适应性结合能够有效对抗重放速度减慢，同时保持探索能力。

Conclusion: 噪声路径整合RNN中的重放可以理解为采样过程，通过隐藏状态泄漏、适应性和动量机制的系统性组合，能够优化重放性能，在探索与速度之间取得更好的平衡，为理解生物重放机制和设计更高效的神经网络模型提供了新见解。

Abstract: Biological neural networks (like the hippocampus) can internally generate "replay" resembling stimulus-driven activity. Recent computational models of replay use noisy recurrent neural networks (RNNs) trained to path-integrate. Replay in these networks has been described as Langevin sampling, but new modifiers of noisy RNN replay have surpassed this description. We re-examine noisy RNN replay as sampling to understand or improve it in three ways: (1) Under simple assumptions, we prove that the gradients replay activity should follow are time-varying and difficult to estimate, but readily motivate the use of hidden state leakage in RNNs for replay. (2) We confirm that hidden state adaptation (negative feedback) encourages exploration in replay, but show that it incurs non-Markov sampling that also slows replay. (3) We propose the first model of temporally compressed replay in noisy path-integrating RNNs through hidden state momentum, connect it to underdamped Langevin sampling, and show that, together with adaptation, it counters slowness while maintaining exploration. We verify our findings via path-integration of 2D triangular and T-maze paths and of high-dimensional paths of synthetic rat place cell activity.

</details>


### [151] [Scientific Knowledge-Guided Machine Learning for Vessel Power Prediction: A Comparative Study](https://arxiv.org/abs/2602.18403)
*Orfeas Bourchas,George Papalambrou*

Main category: cs.LG

TL;DR: 提出混合建模框架，结合物理知识与数据驱动残差学习，提升船舶主机功率预测精度，改善外推性能


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法（如SVM、ANN、树模型等）虽然能捕捉非线性关系，但往往无法遵循功率与速度之间的基本螺旋桨定律关系，导致在训练范围外的外推性能较差

Method: 引入混合建模框架：1）基于平静水域功率曲线P=cV^n的基线组件捕捉主要功率-速度依赖关系；2）训练非线性回归器预测残差功率（由环境和操作条件引起的偏差）；3）将XGBoost、简单神经网络和物理信息神经网络与基线组件结合，并与无基线组件的相同模型进行比较

Result: 验证表明，混合模型在稀疏数据区域始终优于纯数据驱动基线，在数据密集区域保持相似性能，提供实用且计算高效的工具

Conclusion: 通过约束机器学习任务为残差修正，混合模型简化了学习过程，改善了泛化能力，并确保与基础物理的一致性，适用于船舶性能监控、天气路由、纵倾优化和能效规划

Abstract: Accurate prediction of main engine power is essential for vessel performance optimization, fuel efficiency, and compliance with emission regulations. Conventional machine learning approaches, such as Support Vector Machines, variants of Artificial Neural Networks (ANNs), and tree-based methods like Random Forests, Extra Tree Regressors, and XGBoost, can capture nonlinearities but often struggle to respect the fundamental propeller law relationship between power and speed, resulting in poor extrapolation outside the training envelope. This study introduces a hybrid modeling framework that integrates physics-based knowledge from sea trials with data-driven residual learning. The baseline component, derived from calm-water power curves of the form $P = cV^n$, captures the dominant power-speed dependence, while another, nonlinear, regressor is then trained to predict the residual power, representing deviations caused by environmental and operational conditions. By constraining the machine learning task to residual corrections, the hybrid model simplifies learning, improves generalization, and ensures consistency with the underlying physics. In this study, an XGBoost, a simple Neural Network, and a Physics-Informed Neural Network (PINN) coupled with the baseline component were compared to identical models without the baseline component. Validation on in-service data demonstrates that the hybrid model consistently outperformed a pure data-driven baseline in sparse data regions while maintaining similar performance in populated ones. The proposed framework provides a practical and computationally efficient tool for vessel performance monitoring, with applications in weather routing, trim optimization, and energy efficiency planning.

</details>


### [152] [Unifying approach to uniform expressivity of graph neural networks](https://arxiv.org/abs/2602.18409)
*Huan Luo,Jonni Virtema*

Main category: cs.LG

TL;DR: 提出了Template GNNs (T-GNNs)框架，将GNN表达能力统一到模板聚合范式，并建立了与Graded template modal logic (GML(T))的等价性。


<details>
  <summary>Details</summary>
Motivation: 标准GNN只能聚合直接邻居或全局信息，表达能力有限。现有方法尝试通过纳入子结构信息（如环计数和子图属性）来增强表达能力，但缺乏统一的理论框架。

Method: 提出Template GNNs (T-GNNs)框架，通过从指定图模板集合中聚合有效模板嵌入来更新节点特征。同时提出对应的Graded template modal logic (GML(T))、模板基双模拟和WL算法。

Result: 建立了T-GNNs与GML(T)表达能力之间的等价性，并展示了标准AC-GNNs及其变体可以作为T-GNNs的实例化，为分析GNN表达能力提供了统一方法。

Conclusion: T-GNNs提供了一个统一的框架来形式化GNN架构趋势，将子结构信息纳入GNN设计，并为分析GNN表达能力提供了理论基础。

Abstract: The expressive power of Graph Neural Networks (GNNs) is often analysed via correspondence to the Weisfeiler-Leman (WL) algorithm and fragments of first-order logic. Standard GNNs are limited to performing aggregation over immediate neighbourhoods or over global read-outs. To increase their expressivity, recent attempts have been made to incorporate substructural information (e.g. cycle counts and subgraph properties). In this paper, we formalize this architectural trend by introducing Template GNNs (T-GNNs), a generalized framework where node features are updated by aggregating over valid template embeddings from a specified set of graph templates. We propose a corresponding logic, Graded template modal logic (GML(T)), and generalized notions of template-based bisimulation and WL algorithm. We establish an equivalence between the expressive power of T-GNNs and GML(T), and provide a unifying approach for analysing GNN expressivity: we show how standard AC-GNNs and its recent variants can be interpreted as instantiations of T-GNNs.

</details>


### [153] [Subgroups of $U(d)$ Induce Natural RNN and Transformer Architectures](https://arxiv.org/abs/2602.18417)
*Joshua Nunley*

Main category: cs.LG

TL;DR: 提出一个在U(d)闭子群上构建隐藏状态序列模型的直接框架，通过最小公理化设置从共享骨架推导循环和Transformer模板，子群选择可作为状态空间、切向投影和更新映射的即插即用组件


<details>
  <summary>Details</summary>
Motivation: 为序列模型提供统一的数学框架，将隐藏状态限制在酉群的闭子群上，通过子群选择实现状态空间、切向投影和更新映射的灵活配置

Method: 使用最小公理化设置，从共享骨架推导循环神经网络和Transformer模板，子群选择作为可替换组件，特别专注于O(d)正交群，并提出了切空间的通用线性混合扩展

Result: 在Tiny Shakespeare和Penn Treebank数据集上评估正交状态RNN和Transformer模型，在参数匹配设置下表现良好，切空间线性混合扩展改善了有限预算下的性能

Conclusion: 提出了一个统一的框架，将序列模型的隐藏状态限制在酉群的闭子群上，子群选择提供了灵活的架构设计，切空间扩展进一步提升了性能

Abstract: This paper presents a direct framework for sequence models with hidden states on closed subgroups of U(d). We use a minimal axiomatic setup and derive recurrent and transformer templates from a shared skeleton in which subgroup choice acts as a drop-in replacement for state space, tangent projection, and update map. We then specialize to O(d) and evaluate orthogonal-state RNN and transformer models on Tiny Shakespeare and Penn Treebank under parameter-matched settings. We also report a general linear-mixing extension in tangent space, which applies across subgroup choices and improves finite-budget performance in the current O(d) experiments.

</details>


### [154] [The Geometry of Noise: Why Diffusion Models Don't Need Noise Conditioning](https://arxiv.org/abs/2602.18428)
*Mojtaba Sahraee-Ardakan,Mauricio Delbracio,Peyman Milanfar*

Main category: cs.LG

TL;DR: 该论文揭示了自主生成模型（如均衡匹配和盲扩散）通过边际能量梯度流实现稳定采样的机制，解决了噪声不可知模型中梯度发散的理论悖论。


<details>
  <summary>Details</summary>
Motivation: 自主生成模型（无需噪声级别条件）挑战了标准范式，但存在一个根本悖论：当噪声级别被视为随机变量时，优化的是什么景观？有界的噪声不可知网络如何在数据流形附近保持稳定（通常梯度会发散）？

Method: 形式化边际能量 $E_{\text{marg}}(\mathbf{u}) = -\log p(\mathbf{u})$，其中 $p(\mathbf{u})$ 是噪声数据的边际密度。证明自主模型的生成是边际能量上的黎曼梯度流。通过相对能量分解，展示学习的时间不变场隐式包含局部共形度量，抵消几何奇异性。

Result: 揭示了自主模型不是简单的盲去噪，而是边际能量的特定梯度流形式。证明了噪声预测参数化存在"Jensen Gap"会导致确定性盲模型灾难性失败，而速度参数化因满足有界增益条件而固有稳定。

Conclusion: 自主生成模型通过边际能量梯度流实现稳定采样，解决了噪声不可知模型的理论悖论。速度参数化比噪声预测参数化更稳定，为设计鲁棒的自主生成模型提供了理论基础。

Abstract: Autonomous (noise-agnostic) generative models, such as Equilibrium Matching and blind diffusion, challenge the standard paradigm by learning a single, time-invariant vector field that operates without explicit noise-level conditioning. While recent work suggests that high-dimensional concentration allows these models to implicitly estimate noise levels from corrupted observations, a fundamental paradox remains: what is the underlying landscape being optimized when the noise level is treated as a random variable, and how can a bounded, noise-agnostic network remain stable near the data manifold where gradients typically diverge? We resolve this paradox by formalizing Marginal Energy, $E_{\text{marg}}(\mathbf{u}) = -\log p(\mathbf{u})$, where $p(\mathbf{u}) = \int p(\mathbf{u}|t)p(t)dt$ is the marginal density of the noisy data integrated over a prior distribution of unknown noise levels. We prove that generation using autonomous models is not merely blind denoising, but a specific form of Riemannian gradient flow on this Marginal Energy. Through a novel relative energy decomposition, we demonstrate that while the raw Marginal Energy landscape possesses a $1/t^p$ singularity normal to the data manifold, the learned time-invariant field implicitly incorporates a local conformal metric that perfectly counteracts the geometric singularity, converting an infinitely deep potential well into a stable attractor. We also establish the structural stability conditions for sampling with autonomous models. We identify a ``Jensen Gap'' in noise-prediction parameterizations that acts as a high-gain amplifier for estimation errors, explaining the catastrophic failure observed in deterministic blind models. Conversely, we prove that velocity-based parameterizations are inherently stable because they satisfy a bounded-gain condition that absorbs posterior uncertainty into a smooth geometric drift.

</details>


### [155] [Assigning Confidence: K-partition Ensembles](https://arxiv.org/abs/2602.18435)
*Aggelos Semoglou,John Pavlopoulos*

Main category: cs.LG

TL;DR: CAKE框架通过聚类集成计算分配稳定性和局部几何拟合一致性，为每个点提供可解释的置信度评分，以识别稳定核心点和模糊点。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法无法评估单个分配的可信度，诊断指标仅反映全局质量而非特定实例的置信度。分配级的不稳定性会影响准确性和鲁棒性，而集成方法虽然提高了全局一致性，但缺乏结合跨运行一致性和学习到的聚类结构几何支持来量化点级置信度的工具。

Method: CAKE框架在聚类集成上计算两个互补统计量：分配稳定性（跨运行的一致性）和局部几何拟合一致性（点与聚类结构的几何关系）。这两个统计量被组合成[0,1]范围内的单一可解释评分。

Result: 理论分析表明CAKE在噪声下仍然有效，并能区分稳定点和不稳定点。在合成和真实数据集上的实验表明，CAKE能有效突出模糊点和稳定核心成员，提供可用于指导过滤或优先级排序以改进聚类质量的置信度排名。

Conclusion: CAKE框架通过量化点级聚类置信度，解决了传统聚类方法在评估单个分配可靠性方面的局限性，为聚类结果提供了更细粒度的可信度评估工具。

Abstract: Clustering is widely used for unsupervised structure discovery, yet it offers limited insight into how reliable each individual assignment is. Diagnostics, such as convergence behavior or objective values, may reflect global quality, but they do not indicate whether particular instances are assigned confidently, especially for initialization-sensitive algorithms like k-means. This assignment-level instability can undermine both accuracy and robustness. Ensemble approaches improve global consistency by aggregating multiple runs, but they typically lack tools for quantifying pointwise confidence in a way that combines cross-run agreement with geometric support from the learned cluster structure. We introduce CAKE (Confidence in Assignments via K-partition Ensembles), a framework that evaluates each point using two complementary statistics computed over a clustering ensemble: assignment stability and consistency of local geometric fit. These are combined into a single, interpretable score in [0,1]. Our theoretical analysis shows that CAKE remains effective under noise and separates stable from unstable points. Experiments on synthetic and real-world datasets indicate that CAKE effectively highlights ambiguous points and stable core members, providing a confidence ranking that can guide filtering or prioritization to improve clustering quality.

</details>
