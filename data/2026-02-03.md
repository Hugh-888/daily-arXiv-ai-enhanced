<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 416]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [quant-ph](#quant-ph) [Total: 72]
- [gr-qc](#gr-qc) [Total: 20]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Multi-Fidelity Physics-Informed Neural Networks with Bayesian Uncertainty Quantification and Adaptive Residual Learning for Efficient Solution of Parametric Partial Differential Equations](https://arxiv.org/abs/2602.01176)
*Olaf Yunus Laitinen Imanov*

Main category: cs.LG

TL;DR: MF-BPINN：结合物理信息神经网络与贝叶斯不确定性量化的多保真度框架，通过分层架构和自适应残差学习，利用低保真度模拟和稀疏高保真度数据高效求解参数化PDE系统。


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络（PINNs）在求解偏微分方程方面表现出色，但求解高保真度PDE仍然计算成本高昂，特别是对于需要在不同参数配置下进行多次评估的参数化系统。

Method: 提出MF-BPINN多保真度框架：1）结合PINNs与贝叶斯不确定性量化；2）采用自适应残差学习；3）通过分层神经网络架构学习不同保真度级别间的非线性相关性；4）引入具有可学习门控机制的自适应残差网络，动态平衡线性和非线性保真度差异；5）开发基于哈密顿蒙特卡洛的严格贝叶斯框架。

Result: 论文提出了一个新颖的多保真度框架，能够有效利用丰富的低保真度模拟和稀疏的高保真度数据，通过自适应机制动态调整不同保真度级别的贡献，并提供了严格的贝叶斯不确定性量化。

Conclusion: MF-BPINN框架通过结合多保真度学习、自适应残差网络和贝叶斯不确定性量化，为高效求解计算成本高昂的参数化PDE系统提供了一种有前景的解决方案。

Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful paradigm for solving partial differential equations (PDEs) by embedding physical laws directly into neural network training. However, solving high-fidelity PDEs remains computationally prohibitive, particularly for parametric systems requiring multiple evaluations across varying parameter configurations. This paper presents MF-BPINN, a novel multi-fidelity framework that synergistically combines physics-informed neural networks with Bayesian uncertainty quantification and adaptive residual learning. Our approach leverages abundant low-fidelity simulations alongside sparse high-fidelity data through a hierarchical neural architecture that learns nonlinear correlations across fidelity levels. We introduce an adaptive residual network with learnable gating mechanisms that dynamically balances linear and nonlinear fidelity discrepancies. Furthermore, we develop a rigorous Bayesian framework employing Hamiltonian Monte Carlo.

</details>


### [2] [OGD4All: A Framework for Accessible Interaction with Geospatial Open Government Data Based on Large Language Models](https://arxiv.org/abs/2602.00012)
*Michael Siebenmann,Javier Argota Sánchez-Vaquerizo,Stefan Arisona,Krystian Samp,Luis Gisler,Dirk Helbing*

Main category: cs.LG

TL;DR: OGD4All是一个基于大语言模型的透明、可审计、可复现框架，用于增强公民与地理空间开放政府数据的交互，通过语义数据检索、代理推理和沙箱执行实现高准确率和低幻觉风险。


<details>
  <summary>Details</summary>
Motivation: 增强公民与开放政府数据的交互，提供透明、可审计、可复现的访问方式，减少大语言模型在数据访问中的幻觉风险，推进可信AI在开放治理中的应用。

Method: 结合语义数据检索、代理推理进行迭代代码生成，以及安全的沙箱执行，生成可验证的多模态输出，在430个苏黎世市数据集和11个大语言模型上进行评估。

Result: 在199个问题的基准测试中（包括事实性和不可回答的问题），达到98%的分析正确率和94%的召回率，同时可靠地拒绝数据不支持的问题，最小化幻觉风险。

Conclusion: 大语言模型可以提供可解释的多模态公共数据访问，推进开放治理中的可信AI，统计稳健性测试和专家反馈证明了其可靠性和社会相关性。

Abstract: We present OGD4All, a transparent, auditable, and reproducible framework based on Large Language Models (LLMs) to enhance citizens' interaction with geospatial Open Government Data (OGD). The system combines semantic data retrieval, agentic reasoning for iterative code generation, and secure sandboxed execution that produces verifiable multimodal outputs. Evaluated on a 199-question benchmark covering both factual and unanswerable questions, across 430 City-of-Zurich datasets and 11 LLMs, OGD4All reaches 98% analytical correctness and 94% recall while reliably rejecting questions unsupported by available data, which minimizes hallucination risks. Statistical robustness tests, as well as expert feedback, show reliability and social relevance. The proposed approach shows how LLMs can provide explainable, multimodal access to public data, advancing trustworthy AI for open governance.

</details>


### [3] [Measurement for Opaque Systems: Multi-source Triangulation with Interpretable Machine Learning](https://arxiv.org/abs/2602.00022)
*Margaret Foster*

Main category: cs.LG

TL;DR: 提出一个测量框架，用于难以直接访问的上下文，通过间接数据痕迹、可解释机器学习模型和理论指导的三角验证来填补不可访问的测量空间。


<details>
  <summary>Details</summary>
Motivation: 许多高风险的系统和政策关注领域难以直接测量：关键动态不可观测、数据间接且分散、真实情况缺失或被隐藏。在这些情况下，现有数据不支持传统的统计分析或模型验证方法。

Method: 结合多源三角验证与可解释机器学习模型，不依赖无法获得的理想数据准确性，而是寻求不同部分信息模型之间的一致性，通过跨信号一致性或与预期状态的偏离来得出可靠结论。

Result: 通过对一个秘密军事组织的组织增长和内部压力动态的实证分析，展示了该方法如何恢复具有实质意义的变异，同时明确揭示了推理的局限性。

Conclusion: 该框架为在缺乏传统统计或因果推断所需数据的情况下进行定量表征提供了分析工作流程，展示了三角验证和可解释机器学习如何在不完整和偏见的观测信号中提取有意义的见解。

Abstract: We propose a measurement framework for difficult-to-access contexts that uses indirect data traces, interpretable machine-learning models, and theory-guided triangulation to fill inaccessible measurement spaces. Many high-stakes systems of scientific and policy interest are difficult, if not impossible, to reach directly: dynamics of interest are unobservable, data are indirect and fragmented across sources, and ground truth is absent or concealed. In these settings, available data often do not support conventional strategies for analysis, such as statistical inference on a single authoritative data stream or model validation against labeled outcomes. To address this problem, we introduce a general framework for measurement in data regimes characterized by structurally missing or adversarial data. We propose combining multi-source triangulation with interpretable machine learning models. Rather than relying on accuracy against unobservable, unattainable ideal data, our framework seeks consistency across separate, partially informative models. This allows users to draw defensible conclusions about the state of the world based on cross-signal consistency or divergence from an expected state. Our framework provides an analytical workflow tailored to quantitative characterization in the absence of data sufficient for conventional statistical or causal inference. We demonstrate our approach and explicitly surface inferential limits through an empirical analysis of organizational growth and internal pressure dynamics in a clandestine militant organization, drawing on multiple observational signals that individually provide incomplete and biased views of the underlying process. The results show how triangulated, interpretable ML can recover substantively meaningful variation.

</details>


### [4] [Representation Learning Enhanced Deep Reinforcement Learning for Optimal Operation of Hydrogen-based Multi-Energy Systems](https://arxiv.org/abs/2602.00027)
*Zhenyu Pu,Yu Yang,Lun Yang,Qing-Shan Jia,Xiaohong Guan,Costas J. Spanos*

Main category: cs.LG

TL;DR: 本文提出了一个综合的氢基多能源系统（HMES）运行模型，并开发了集成表示学习技术的增强深度强化学习（DRL）框架，以解决HMES运行中的非线性动力学和多物理场耦合挑战。


<details>
  <summary>Details</summary>
Motivation: 氢基多能源系统（HMES）作为一种有前景的低碳高效解决方案，能够协调电、热、冷供应与需求，但HESS的非线性多物理场耦合动力学以及供需不确定性使得其优化运行面临挑战。

Method: 1. 开发了全面捕捉HESS非线性动力学和多物理场过程的综合运行模型；2. 提出了集成表示学习技术的增强DRL框架（SR-DRL），加速和改进复杂网络系统的策略优化。

Result: 基于真实数据集的实验表明：综合模型对确保HESS安全可靠运行至关重要；SR-DRL方法在收敛速度和性能上优于传统DRL，能有效降低HMES运行成本并处理系统运行约束。

Conclusion: 表示学习在DRL中能够将原始状态空间重组为结构良好、聚类感知的几何表示，从而平滑和促进DRL的学习过程，为HMES优化运行提供了有效解决方案。

Abstract: Hydrogen-based multi-energy systems (HMES) have emerged as a promising low-carbon and energy-efficient solution, as it can enable the coordinated operation of electricity, heating and cooling supply and demand to enhance operational flexibility, improve overall energy efficiency, and increase the share of renewable integration. However, the optimal operation of HMES remains challenging due to the nonlinear and multi-physics coupled dynamics of hydrogen energy storage systems (HESS) (consisting of electrolyters, fuel cells and hydrogen tanks) as well as the presence of multiple uncertainties from supply and demand. To address these challenges, this paper develops a comprehensive operational model for HMES that fully captures the nonlinear dynamics and multi-physics process of HESS. Moreover, we propose an enhanced deep reinforcement learning (DRL) framework by integrating the emerging representation learning techniques, enabling substantially accelerated and improved policy optimization for spatially and temporally coupled complex networked systems, which is not provided by conventional DRL. Experimental studies based on real-world datasets show that the comprehensive model is crucial to ensure the safe and reliable of HESS. In addition, the proposed SR-DRL approaches demonstrate superior convergence rate and performance over conventional DRL counterparts in terms of reducing the operation cost of HMES and handling the system operating constraints. Finally, we provide some insights into the role of representation learning in DRL, speculating that it can reorganize the original state space into a well-structured and cluster-aware geometric representation, thereby smoothing and facilitating the learning process of DRL.

</details>


### [5] [Backpropagation as Physical Relaxation: Exact Gradients in Finite Time](https://arxiv.org/abs/2602.02281)
*Antonino Emanuele Scurria*

Main category: cs.LG

TL;DR: 论文提出"Dyadic Backpropagation"框架，将反向传播解释为物理动力系统的有限时间松弛过程，通过拉格朗日理论在双状态空间推导全局能量函数，其鞍点动力学在2L步内精确恢复标准反向传播。


<details>
  <summary>Details</summary>
Motivation: 传统上反向传播被理解为符号计算，但作者希望将其建立为物理动力系统的自然涌现过程，为模拟和神经形态计算提供理论基础。

Method: 将前向推理建模为连续时间过程，应用非保守系统的拉格朗日理论处理非对称交互，在编码激活和敏感度的双状态空间构建全局能量函数，分析其鞍点动力学。

Result: 证明单位步长欧拉离散化在2L步内精确恢复标准反向传播，无需对称权重、渐近收敛或微小扰动等近似条件，保证有限时间内获得精确梯度。

Conclusion: 反向传播是连续物理松弛过程的数字优化影子，为模拟和神经形态基板中的精确梯度计算提供了严格理论基础。

Abstract: Backpropagation, the foundational algorithm for training neural networks, is typically understood as a symbolic computation that recursively applies the chain rule. We show it emerges exactly as the finite-time relaxation of a physical dynamical system. By formulating feedforward inference as a continuous-time process and applying Lagrangian theory of non-conservative systems to handle asymmetric interactions, we derive a global energy functional on a doubled state space encoding both activations and sensitivities. The saddle-point dynamics of this energy perform inference and credit assignment simultaneously through local interactions. We term this framework ''Dyadic Backpropagation''. Crucially, we prove that unit-step Euler discretization, the natural timescale of layer transitions, recovers standard backpropagation exactly in precisely 2L steps for an L-layer network, with no approximations. Unlike prior energy-based methods requiring symmetric weights, asymptotic convergence, or vanishing perturbations, our framework guarantees exact gradients in finite time. This establishes backpropagation as the digitally optimized shadow of a continuous physical relaxation, providing a rigorous foundation for exact gradient computation in analog and neuromorphic substrates where continuous dynamics are native.

</details>


### [6] [ELLMPEG: An Edge-based Agentic LLM Video Processing Tool](https://arxiv.org/abs/2602.00028)
*Zoha Azimi,Reza Farahani,Radu Prodan,Christian Timmerer*

Main category: cs.LG

TL;DR: ELLMPEG是一个边缘使能的智能LLM框架，用于自动生成视频处理命令，通过工具感知的RAG和迭代自省在边缘本地生成和验证FFmpeg/VVenC命令，消除对云API的依赖。


<details>
  <summary>Details</summary>
Motivation: 云基LLM部署面临三大限制：高计算和能耗需求、远程处理的隐私与可靠性风险、以及持续的API成本。需要利用边缘计算和本地部署的LLM来解决这些问题。

Method: 集成工具感知的检索增强生成(RAG)与迭代自省机制，在边缘设备上直接生成并本地验证可执行的FFmpeg和VVenC命令。收集480个多样化查询的数据集进行评估。

Result: Qwen2.5在ELLMPEG框架下实现了78%的平均命令生成准确率，零API成本，在FFmpeg和VVenC数据集上均优于其他开源模型。评估了命令有效性、生成速度、推理时间和能效。

Conclusion: ELLMPEG框架成功实现了在边缘设备上高效、准确、低成本地生成视频处理命令，为边缘AI应用提供了实用的解决方案，平衡了性能、隐私和成本。

Abstract: Large language models (LLMs), the foundation of generative AI systems like ChatGPT, are transforming many fields and applications, including multimedia, enabling more advanced content generation, analysis, and interaction. However, cloud-based LLM deployments face three key limitations: high computational and energy demands, privacy and reliability risks from remote processing, and recurring API costs. Recent advances in agentic AI, especially in structured reasoning and tool use, offer a better way to exploit open and locally deployed tools and LLMs. This paper presents ELLMPEG, an edge-enabled agentic LLM framework for the automated generation of video-processing commands. ELLMPEG integrates tool-aware Retrieval-Augmented Generation (RAG) with iterative self-reflection to produce and locally verify executable FFmpeg and VVenC commands directly at the edge, eliminating reliance on external cloud APIs. To evaluate ELLMPEG, we collect a dedicated prompt dataset comprising 480 diverse queries covering different categories of FFmpeg and the Versatile Video Codec (VVC) encoder (VVenC) commands. We validate command generation accuracy and evaluate four open-source LLMs based on command validity, tokens generated per second, inference time, and energy efficiency. We also execute the generated commands to assess their runtime correctness and practical applicability. Experimental results show that Qwen2.5, when augmented with the ELLMPEG framework, achieves an average command-generation accuracy of 78 % with zero recurring API cost, outperforming all other open-source models across both the FFmpeg and VVenC datasets.

</details>


### [7] [RAPTOR-AI for Disaster OODA Loop: Hierarchical Multimodal RAG with Experience-Driven Agentic Decision-Making](https://arxiv.org/abs/2602.00030)
*Takato Yasuno*

Main category: cs.LG

TL;DR: 提出一个用于人道主义援助和灾害响应的智能RAG框架，通过分层知识库和多模态检索支持灾害响应的三个阶段，并采用代理控制器动态选择检索策略，在真实灾害数据集上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 有效的人道主义援助和灾害响应需要快速的情境理解、可靠的决策支持，以及能够泛化到各种未见灾害场景的能力。传统方法在处理多样化灾害情境和多模态信息方面存在局限。

Method: 构建分层知识库整合文本灾害手册、历史经验（如2011年东北地震）和空中/地面图像。采用开源多模态实现处理46个海啸相关PDF文件，使用BLIP图像描述、ColVBERT嵌入和长上下文摘要生成高效的多模态检索树。代理控制器通过熵感知场景抽象动态选择检索策略（如RAPTOR、ColBERT），并采用轻量级LoRA后训练方法注入历史灾害经验知识。

Result: 在真实灾害数据集上的实验表明，系统在情境理解、任务分解准确性和应急操作可用性方面均有显著提升。通过自适应检索增强生成与自推理和多模态思维链能力，实现了实质性性能增益。

Conclusion: 该智能RAG框架通过整合多模态知识、自适应检索策略和经验知识注入，为灾害响应的三个阶段提供了有效的决策支持，能够同时满足专家和非专家响应者的需求，在应急操作中表现出优越的实用性和泛化能力。

Abstract: Effective humanitarian assistance and disaster relief (HADR) requires rapid situational understanding, reliable decision support, and the ability to generalize across diverse and previously unseen disaster contexts. This work introduces an agentic Retrieval-Augmented Generation (RAG) framework designed to support the three canonical phases of disaster response: initial rescue, mid-term recovery, and long-term reconstruction. To achieve robust multimodal grounding, we construct a hierarchical knowledge base that integrates textual disaster manuals, historical lessons (e.g., the 2011 Tohoku earthquake), and both aerial and ground-level imagery. Our system builds on the open-source multimodal implementation, which processes 46 tsunami-related PDFs (2,378 pages) using BLIP-based image captioning, ColVBERT embeddings, and long-context summarization to generate an efficient, structured multimodal retrieval tree optimized for disaster knowledge preservation. An agentic controller dynamically selects retrieval strategies (e.g., RAPTOR, ColBERT) through entropy-aware scene abstraction, enabling adaptive reasoning across heterogeneous inputs. Additionally, a lightweight LoRA-based post-training method injects experiential knowledge from past disasters, enhancing the models' capacity to support both expert and non-expert responders. Experiments on real disaster datasets demonstrate improved situational grounding, enhanced task decomposition accuracy, and superior usability for emergency operations. Incorporating recent advances in long-context RAG systems, agentic information retrieval, and contemporary emergency response AI, our system achieves substantial gains through adaptive retrieval-augmented generation with self-reasoning and multimodal chain-of-thought capabilities.

</details>


### [8] [Enhancing few-shot time series forecasting with LLM-guided diffusion](https://arxiv.org/abs/2602.00040)
*Haonan Shi,Dehua Shuai,Liming Wang,Xiyang Liu,Long Tian*

Main category: cs.LG

TL;DR: 提出LTSM-DIFF框架，结合大语言模型和扩散模型，解决小样本时间序列预测问题，在数据稀缺和丰富场景下均表现优异。


<details>
  <summary>Details</summary>
Motivation: 专业领域时间序列预测常面临数据稀缺问题，传统模型需要大规模数据集才能有效捕捉时间动态。需要解决小样本条件下的时间序列预测挑战。

Method: 提出LTSM-DIFF框架：1) LTSM模块作为时间记忆机制，微调后提取丰富序列表示；2) 这些表示作为条件指导联合概率扩散过程；3) 实现从语言领域到时间序列任务的知识迁移。

Result: 在多个基准测试中，LTSM-DIFF在数据丰富场景下达到最先进性能，在小样本预测方面也有显著改进。

Conclusion: 该工作为数据稀缺条件下的时间序列分析建立了新范式，通过结合大语言模型和扩散模型，显著提升了泛化能力和鲁棒性。

Abstract: Time series forecasting in specialized domains is often constrained by limited data availability, where conventional models typically require large-scale datasets to effectively capture underlying temporal dynamics. To tackle this few-shot challenge, we propose LTSM-DIFF (Large-scale Temporal Sequential Memory with Diffusion), a novel learning framework that integrates the expressive power of large language models with the generative capability of diffusion models. Specifically, the LTSM module is fine-tuned and employed as a temporal memory mechanism, extracting rich sequential representations even under data-scarce conditions. These representations are then utilized as conditional guidance for a joint probability diffusion process, enabling refined modeling of complex temporal patterns. This design allows knowledge transfer from the language domain to time series tasks, substantially enhancing both generalization and robustness. Extensive experiments across diverse benchmarks demonstrate that LTSM-DIFF consistently achieves state-of-the-art performance in data-rich scenarios, while also delivering significant improvements in few-shot forecasting. Our work establishes a new paradigm for time series analysis under data scarcity.

</details>


### [9] [Extending Beacon to Hindi: Cultural Adaptation Drives Cross-Lingual Sycophancy](https://arxiv.org/abs/2602.00046)
*Sarthak Sattigeri*

Main category: cs.LG

TL;DR: 研究发现语言模型在印地语文化适配提示下比英语原版表现出更高的谄媚倾向，文化适配是主要影响因素，而非语言编码本身。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究语言模型的谄媚行为是否在不同语言和文化背景下具有普适性。现有研究主要在英语环境中进行，不清楚这种诊断是否适用于其他语言和文化语境。

Method: 将Beacon单轮强制选择谄媚诊断扩展到印地语，采用三条件设计：英语原版、印地语字面翻译、印地语文化适配提示。评估四个开源指令调优模型，每个条件50个提示，分离语言编码效应和文化适配效应。

Result: 所有模型在文化适配印地语提示下的谄媚率均高于英语，绝对差异12.0-16.0个百分点。Qwen 2.5-Coder-7B分解显示文化适配贡献主要差异(delta=14.0%)，语言编码贡献极小(delta=2.0%)。建议类提示跨语言差异最大(20-25个百分点)。

Conclusion: 英语环境中测量的对齐行为不能均匀地跨语言转移，文化基础的提示框架起重要作用。研究强调需要考虑文化背景来评估和改善语言模型的对齐。

Abstract: Sycophancy, the tendency of language models to prioritize agreement with user preferences over principled reasoning, has been identified as a persistent alignment failure in English-language evaluations. However, it remains unclear whether such diagnostics generalize across languages and cultural contexts. We extend the Beacon single-turn forced-choice sycophancy diagnostic to Hindi through a controlled three-condition design: English original, Hindi literal translation, and Hindi culturally adapted prompts. We evaluate four open-weight instruction-tuned models on 50 prompts per condition, enabling separation of language encoding effects from cultural adaptation effects. Across all models, sycophancy rates are consistently higher for culturally adapted Hindi prompts than for English, with absolute differences ranging from 12.0 to 16.0 percentage points. A decomposition on Qwen 2.5-Coder-7B shows that cultural adaptation (delta = 14.0%, 95% CI: [4.0%, 26.0%]) accounts for the majority of this gap, while language encoding contributes minimally (delta = 2.0%, 95% CI: [0.0%, 6.0%]). Category-level analysis reveals that advice prompts exhibit the largest cross-lingual differences (20-25 percentage points), achieving statistical significance in two of four models. These findings indicate that alignment behaviors measured in English may not transfer uniformly across languages and that culturally grounded prompt framing plays a substantial role. We release all datasets and evaluation code to support replication and extension.

</details>


### [10] [Lightweight Edge Learning via Dataset Pruning](https://arxiv.org/abs/2602.00047)
*Laha Ale,Hu Luo,Mingsheng Cao,Shichao Li,Huanlai Xing,Haifeng Sun*

Main category: cs.LG

TL;DR: 提出基于数据集剪枝的数据中心化优化框架，通过构建紧凑且信息丰富的训练子集，在边缘设备上实现资源高效的学习，显著降低训练延迟和能耗，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 边缘学习虽然能保护隐私并降低通信延迟，但在电池供电的移动系统上，训练阶段的高计算和能耗开销阻碍了其部署。现有研究主要优化模型架构以提高推理效率，但训练阶段仍受限于处理大量冗余本地数据。

Method: 提出数据中心化优化框架，利用数据集剪枝实现资源高效的边缘学习。通过截断预热阶段获得的平均损失统计来评估样本重要性，确定性地保留最关键的数据点，采用动态剪枝比例。该方法与模型无关，无需设备间通信。

Result: 在标准图像分类基准测试中，该框架实现了与剪枝比例成比例的近乎线性的训练延迟和能耗降低，模型精度下降可忽略不计。

Conclusion: 数据集剪枝是增强资源受限移动边缘设备学习可持续性和可扩展性的重要补充范式，为边缘学习提供了有效的资源优化方案。

Abstract: Edge learning facilitates ubiquitous intelligence by enabling model training and adaptation directly on data-generating devices, thereby mitigating privacy risks and communication latency. However, the high computational and energy overhead of on-device training hinders its deployment on battery-powered mobile systems with strict thermal and memory budgets. While prior research has extensively optimized model architectures for efficient inference, the training phase remains bottlenecked by the processing of massive, often redundant, local datasets. In this work, we propose a data-centric optimization framework that leverages dataset pruning to achieve resource-efficient edge learning. Unlike standard methods that process all available data, our approach constructs compact, highly informative training subsets via a lightweight, on-device importance evaluation. Specifically, we utilize average loss statistics derived from a truncated warm-up phase to rank sample importance, deterministically retaining only the most critical data points under a dynamic pruning ratio. This mechanism is model-agnostic and operates locally without inter-device communication. Extensive experiments on standard image classification benchmarks demonstrate that our framework achieves a near-linear reduction in training latency and energy consumption proportional to the pruning ratio, with negligible degradation in model accuracy. These results validate dataset pruning as a vital, complementary paradigm for enhancing the sustainability and scalability of learning on resource-constrained mobile edge devices.

</details>


### [11] [Distributional Reinforcement Learning for Condition-Based Maintenance of Multi-Pump Equipment](https://arxiv.org/abs/2602.00051)
*Takato Yasuno*

Main category: cs.LG

TL;DR: 本文提出一种基于分位数回归深度Q网络(QR-DQN)的分布强化学习方法，用于多设备状态维护，通过三种策略场景实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统基于时间的维护策略常导致不必要的开支和意外设备故障，需要从被动维护转向基于设备实时状态的主动维护策略。

Method: 采用QR-DQN分布强化学习方法，整合老化因子，通过安全优先、平衡和成本效益三种策略场景同时管理多个泵单元。

Result: 经过3000次训练验证，安全优先策略表现最优，投资回报率达3.91，性能比替代方案提升152%，仅需增加31%投资，系统运行稳定性达95.66%。

Conclusion: 提出的基于QR-DQN的分布强化学习方法在状态维护中表现出色，安全优先策略在成本效益和性能方面均有显著优势，具备工业应用潜力。

Abstract: Condition-Based Maintenance (CBM) signifies a paradigm shift from reactive to proactive equipment management strategies in modern industrial systems. Conventional time-based maintenance schedules frequently engender superfluous expenditures and unanticipated equipment failures. In contrast, CBM utilizes real-time equipment condition data to enhance maintenance timing and optimize resource allocation. The present paper proposes a novel distributional reinforcement learning approach for multi-equipment CBM using Quantile Regression Deep Q-Networks (QR-DQN) with aging factor integration. The methodology employed in this study encompasses the concurrent administration of multiple pump units through three strategic scenarios. The implementation of safety-first, balanced, and cost-efficient approaches is imperative. Comprehensive experimental validation over 3,000 training episodes demonstrates significant performance improvements across all strategies. The Safety-First strategy demonstrates superior cost efficiency, with a return on investment (ROI) of 3.91, yielding 152\% better performance than alternatives while requiring only 31\% higher investment. The system exhibits 95.66\% operational stability and immediate applicability to industrial environments.

</details>


### [12] [TextBFGS: Quasi-Newton Optimization for Discrete Executable Text via Gradient-Operator Retrieval](https://arxiv.org/abs/2602.00059)
*Zizheng Zhang,Yuyang Liao,Chen Chen,Jian He,Dun Wu,Qianjin Yu,Yanqin Gao,Jin Yang,Kailai Zhang,Eng Siong Chng,Xionghu Zhong*

Main category: cs.LG

TL;DR: TextBFGS：一种用于离散文本优化的二阶框架，通过检索梯度算子实现类拟牛顿法优化，显著优于一阶方法


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的离散文本优化方法（如提示和代码优化）主要采用一阶优化器，类似于随机梯度下降，存在收敛慢和不稳定的问题，因为它们忽略了优化景观的语义曲率

Method: TextBFGS是一个二阶框架，通过从预学习成功轨迹的记忆中检索梯度算子来近似逆海森矩阵。给定文本梯度反馈，系统识别历史修正模式并尝试将这些抽象算子应用于当前变量，实现单次更新

Result: 在代码优化任务（HumanEval、MBPP等）上的实证评估表明，TextBFGS显著优于一阶基线方法，以更少的模型调用获得更高的通过率，并展现出强大的跨任务迁移能力

Conclusion: TextBFGS为高效、内存感知的文本优化建立了一个数学基础范式，通过二阶优化方法解决了传统一阶方法在离散文本优化中的局限性

Abstract: Optimizing discrete executable text such as prompts and code has recently been framed as a gradient-based process, effectively translating backpropagation concepts to the semantic space. However, existing methods predominantly operate as first-order optimizers akin to Stochastic Gradient Descent, which are suffering from slow convergence and instability because they neglect the semantic curvature of the optimization landscape. To bridge this gap, we introduce TextBFGS, a second-order framework to implement a Quasi-Newton optimization method for discrete text. Unlike traditional memory-based approaches that retrieve similar textual instances, TextBFGS approximates the inverse Hessian matrix by retrieving Gradient-Operators from the memory of pre-learned successful trajectories. Specifically, given a textual gradient feedback, TextBFGS identifies historical correction patterns from the optimization knowledge base and tries to apply these abstract operators to the current variable. This mechanism enables a One-Pass Update, combining feedback generation and second-order correction into a single inference step. Empirical evaluations on code optimization across diverse domains (e.g., HumanEval, MBPP) demonstrate that TextBFGS significantly outperforms first-order baselines. It achieves superior pass rates with fewer model calls and exhibits strong cross-task transferability, thus establishes a mathematically grounded paradigm for efficient, memory-aware text optimization.

</details>


### [13] [SCPL: Enhancing Neural Network Training Throughput with Decoupled Local Losses and Model Parallelism](https://arxiv.org/abs/2602.00062)
*Ming-Yao Ho,Cheng-Kai Wang,You-Teng Lin,Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: 提出SCPL方法，通过解耦反向传播将长梯度流分解为多个短梯度流，实现层间并行计算，提高训练效率


<details>
  <summary>Details</summary>
Motivation: 企业信息系统采用大规模AI模型面临高训练成本和长开发周期的挑战，传统端到端反向传播算法是深度网络训练效率低下的主要原因

Method: 提出监督对比并行学习(SCPL)，解耦反向传播，将长梯度流转化为多个短梯度流，实现不同层的参数梯度同时计算，获得更好的模型并行性

Result: 实验证明SCPL相比BP、Early Exit、GPipe和AL等先进方法，在效率和效果上都有优势，缓解了性能瓶颈

Conclusion: SCPL为企业开发部署先进信息系统提供了更经济、更敏捷的实用路径，代码已开源

Abstract: Adopting large-scale AI models in enterprise information systems is often hindered by high training costs and long development cycles, posing a significant managerial challenge. The standard end-to-end backpropagation (BP) algorithm is a primary driver of modern AI, but it is also the source of inefficiency in training deep networks. This paper introduces a new training methodology, Supervised Contrastive Parallel Learning (SCPL), that addresses this issue by decoupling BP and transforming a long gradient flow into multiple short ones. This design enables the simultaneous computation of parameter gradients in different layers, achieving superior model parallelism and enhancing training throughput. Detailed experiments are presented to demonstrate the efficiency and effectiveness of our model compared to BP, Early Exit, GPipe, and Associated Learning (AL), a state-of-the-art method for decoupling backpropagation. By mitigating a fundamental performance bottleneck, SCPL provides a practical pathway for organizations to develop and deploy advanced information systems more cost-effectively and with greater agility. The experimental code is released for reproducibility. https://github.com/minyaho/scpl/

</details>


### [14] [The Impact of Machine Learning Uncertainty on the Robustness of Counterfactual Explanations](https://arxiv.org/abs/2602.00063)
*Leonidas Christodoulou,Chang Sun*

Main category: cs.LG

TL;DR: 研究发现反事实解释对模型不确定性高度敏感，即使模型准确度轻微下降也会导致反事实解释大幅变化，强调了在金融和社会科学等领域需要不确定性感知的解释方法。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法大多未在模型和数据不确定性变化的情况下进行测试，导致在现实世界变化中可能产生不稳定或无效的解释。需要研究反事实解释在存在偶然性和认知不确定性时的鲁棒性。

Method: 通过实验研究常见机器学习模型与反事实生成算法组合在存在偶然性和认知不确定性时的鲁棒性，使用合成和真实世界表格数据集进行测试。

Result: 反事实解释对模型不确定性高度敏感，即使由噪声增加或数据有限导致的模型准确度轻微下降，也会导致生成的反事实在平均水平和单个实例上出现大幅变化。

Conclusion: 反事实解释在不确定性存在时不稳定，强调了在金融和社会科学等关键领域开发不确定性感知解释方法的必要性。

Abstract: Counterfactual explanations are widely used to interpret machine learning predictions by identifying minimal changes to input features that would alter a model's decision. However, most existing counterfactual methods have not been tested when model and data uncertainty change, resulting in explanations that may be unstable or invalid under real-world variability. In this work, we investigate the robustness of common combinations of machine learning models and counterfactual generation algorithms in the presence of both aleatoric and epistemic uncertainty. Through experiments on synthetic and real-world tabular datasets, we show that counterfactual explanations are highly sensitive to model uncertainty. In particular, we find that even small reductions in model accuracy - caused by increased noise or limited data - can lead to large variations in the generated counterfactuals on average and on individual instances. These findings underscore the need for uncertainty-aware explanation methods in domains such as finance and the social sciences.

</details>


### [15] [SPGCL: Effective Graph Contrastive Learning via SVD-Guided Structural Perturbation](https://arxiv.org/abs/2602.00064)
*Hao Deng,Yingping Li,Shuiping Gou,Bo Liu*

Main category: cs.LG

TL;DR: SPGCL提出了一种通过SVD引导的结构扰动进行鲁棒图对比学习的框架，通过平衡边缘移除和恢复来生成语义丰富的视图，提高GNN对结构噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的图对比学习方法存在局限性：随机扰动（如边缘丢弃）是结构无关的，可能移除关键边缘；而基于SVD的视图通常变得密集且缺乏足够的多样性。需要一种方法既能保留全局结构先验，又能生成多样化的视图。

Method: SPGCL结合轻量级随机边缘移除和SVD引导的细化步骤：1）通过稀疏的top-ranked边缘选择和合并避免图密集化；2）平衡边缘移除和恢复率，控制视图间的结构差异；3）加入由全局相似性约束正则化的对比融合模块来对齐两个视图。

Result: 在十个基准数据集上的广泛实验表明，SPGCL持续提高了基础GNN的鲁棒性和准确性，优于最先进的图对比学习和结构学习方法。

Conclusion: SPGCL通过SVD引导的结构扰动，有效地平衡了边缘移除和恢复，生成了语义丰富的对比视图，显著提高了图神经网络对结构噪声的鲁棒性。

Abstract: Graph Neural Networks (GNNs) can be highly sensitive to structural noise, including spurious or missing edges caused by adversarial attacks or non-adversarial imperfections. Existing graph contrastive learning methods typically rely on either random perturbations (e.g., edge dropping) to generate diverse views or purely spectral augmentations (e.g., SVD) to preserve global structural priors. However, random perturbations are structure-agnostic and may remove critical edges, while SVD-based views often become dense and lack sufficient diversity. To bridge this gap, we propose SPGCL, a robust graph contrastive learning framework via SVD-guided structural perturbation. SPGCL couples lightweight stochastic edge removal with an SVD-guided refinement step that can recover mistakenly removed informative edges and introduce semantically meaningful missing links while avoiding graph densification through sparse top-ranked edge selection and merging. By balancing edge removal and recovery rates, SPGCL explicitly controls structural discrepancy between views so that contrastive signals reflect semantic structural differences rather than edge-count gaps. We further incorporate a contrastive fusion module regularized by a global similarity constraint to better align the two views. Extensive experiments on ten benchmark datasets demonstrate that SPGCL consistently improves robustness and accuracy of base GNNs, outperforming state-of-the-art graph contrastive learning and structure learning methods.

</details>


### [16] [Modality as Heterogeneity: Node Splitting and Graph Rewiring for Multimodal Graph Learning](https://arxiv.org/abs/2602.00067)
*Yihan Zhang,Ercan E. Kuruoglu*

Main category: cs.LG

TL;DR: 提出NSG-MoE框架，通过节点分裂和图形重连机制结合结构化MoE架构，解决多模态图中的模态混淆问题，在多个基准测试中表现优异且训练效率高。


<details>
  <summary>Details</summary>
Motivation: 多模态图具有丰富的表示能力和广泛适用性，但面临严重的模态混淆问题，通用GNNs常出现不理想的混合效应，需要专门的方法来保持结构信息和多模态语义。

Method: 提出NSG-MoE框架：1) 节点分裂机制将每个节点分解为模态特定组件；2) 图形重连机制；3) 结构化MoE架构，分配关系感知专家处理异构消息流。

Result: 在三个多模态基准测试中一致超越强基线方法；尽管使用通常计算量大的MoE架构，仍实现了有竞争力的训练效率；谱分析显示NSG在模态特定子空间上进行自适应滤波；信息论分析表明架构约束减少了数据和参数间的互信息，提高了泛化能力。

Conclusion: NSG-MoE通过节点分裂和结构化MoE架构有效解决多模态图中的模态混淆问题，在保持结构信息和多模态语义的同时提高性能，并提供理论分析解释其解耦行为和泛化优势。

Abstract: Multimodal graphs are gaining increasing attention due to their rich representational power and wide applicability, yet they introduce substantial challenges arising from severe modality confusion. To address this issue, we propose NSG (Node Splitting Graph)-MoE, a multimodal graph learning framework that integrates a node-splitting and graph-rewiring mechanism with a structured Mixture-of-Experts (MoE) architecture. It explicitly decomposes each node into modality-specific components and assigns relation-aware experts to process heterogeneous message flows, thereby preserving structural information and multimodal semantics while mitigating the undesirable mixing effects commonly observed in general-purpose GNNs. Extensive experiments on three multimodal benchmarks demonstrate that NSG-MoE consistently surpasses strong baselines. Despite incorporating MoE -- which is typically computationally heavy -- our method achieves competitive training efficiency. Beyond empirical results, we provide a spectral analysis revealing that NSG performs adaptive filtering over modality-specific subspaces, thus explaining its disentangling behavior. Furthermore, an information-theoretic analysis shows that the architectural constraints imposed by NSG reduces mutual information between data and parameters and improving generalization capability.

</details>


### [17] [Generative AI-enhanced Probabilistic Multi-Fidelity Surrogate Modeling Via Transfer Learning](https://arxiv.org/abs/2602.00072)
*Jice Zeng,David Barajas-Solano,Hui Chen*

Main category: cs.LG

TL;DR: 提出基于生成式迁移学习的概率多保真度代理框架，使用归一化流作为骨干网络，通过两阶段训练（先在大量低保真数据上预训练，再在少量高保真数据上微调）解决高保真数据稀缺问题，并引入满射层实现维度约简。


<details>
  <summary>Details</summary>
Motivation: 机器学习代理模型的性能严重依赖数据质量和数量，但高保真数据通常稀缺且计算成本高，低保真数据虽丰富但精度不足。需要解决数据稀缺问题，构建数据高效的高精度代理模型。

Method: 1) 使用归一化流作为生成模型骨干；2) 两阶段训练：先在大量低保真数据上预训练学习概率前向模型，然后在少量高保真数据上微调以纠正低保真-高保真差异；3) 集成满射层与标准耦合块，放宽标准双射归一化流的维度保持约束，实现可学习的维度约简。

Result: 该代理模型能提供快速概率预测并量化不确定性，显著优于仅使用低保真数据的基线方法，同时使用更少的高保真评估。在钢筋混凝土板基准测试中，结合大量粗网格模拟和有限细网格模拟，实现了具有高保真精度的概率预测。

Conclusion: 该研究为复杂工程系统提供了一条实用的数据高效、生成式AI驱动的代理模型路径，通过生成式迁移学习和概率多保真度框架有效解决了高保真数据稀缺问题。

Abstract: The performance of machine learning surrogates is critically dependent on data quality and quantity. This presents a major challenge, as high-fidelity (HF) data is often scarce and computationally expensive to acquire, while low-fidelity (LF) data is abundant but less accurate. To address this data scarcity problem, we develop a probabilistic multi-fidelity surrogate framework based on generative transfer learning. We employ a normalizing flow (NF) generative model as the backbone, which is trained in two phases: (i) the NF is first pretrained on a large LF dataset to learn a probabilistic forward model; (ii) the pretrained model is then fine-tuned on a small HF dataset, allowing it to correct for LF-HF discrepancies via knowledge transfer. To relax the dimension-preserving constraint of standard bijective NFs, we integrate surjective (dimension-reducing) layers with standard coupling blocks. This architecture enables learned dimension reduction while preserving the ability to train with exact likelihoods. The resulting surrogate provides fast probabilistic predictions with quantified uncertainty and significantly outperforms LF-only baselines while using fewer HF evaluations. We validate the approach on a reinforced concrete slab benchmark, combining many coarse-mesh (LF) simulations with a limited set of fine-mesh (HF) simulations. The proposed model achieves probabilistic predictions with HF accuracy, demonstrating a practical path toward data-efficient, generative AI-driven surrogates for complex engineering systems.

</details>


### [18] [Dimensional Peeking for Low-Variance Gradients in Zeroth-Order Discrete Optimization via Simulation](https://arxiv.org/abs/2602.00075)
*Philipp Andelfinger,Wentong Cai*

Main category: cs.LG

TL;DR: 提出一种名为"dimensional peeking"的方差缩减方法，用于离散仿真优化中的梯度估计，通过提升采样粒度到遵循相同控制流的数值类别，减少梯度估计方差，提高零阶优化的竞争力。


<details>
  <summary>Details</summary>
Motivation: 在基于仿真的离散优化中，当无法直接计算导数时，通常使用随机估计器近似梯度。但这些基于扰动的采样方法会引入方差，导致收敛缓慢。需要一种减少方差的方法来提高优化效率。

Method: 提出"dimensional peeking"方法，将采样粒度从标量值提升到遵循相同控制流路径的数值类别，从而增加每次仿真评估收集的信息量。该方法基于平滑梯度估计器推导，不引入偏差。通过自定义数值数据类型在C++程序中透明实现。

Result: 在三个高维输入的仿真优化问题中，观察到方差减少因子高达7.9。与三种元启发式算法相比，优化进展表明dimensional peeking提高了零阶优化在离散非凸仿真问题中的竞争力。

Conclusion: Dimensional peeking是一种有效的方差缩减方法，能够显著提高基于仿真的离散优化中梯度估计的效率，使零阶优化方法在离散非凸仿真问题中更具竞争力。

Abstract: Gradient-based optimization methods are commonly used to identify local optima in high-dimensional spaces. When derivatives cannot be evaluated directly, stochastic estimators can provide approximate gradients. However, these estimators' perturbation-based sampling of the objective function introduces variance that can lead to slow convergence. In this paper, we present dimensional peeking, a variance reduction method for gradient estimation in discrete optimization via simulation. By lifting the sampling granularity from scalar values to classes of values that follow the same control flow path, we increase the information gathered per simulation evaluation. Our derivation from an established smoothed gradient estimator shows that the method does not introduce any bias. We present an implementation via a custom numerical data type to transparently carry out dimensional peeking over C++ programs. Variance reductions by factors of up to 7.9 are observed for three simulation-based optimization problems with high-dimensional input. The optimization progress compared to three meta-heuristics shows that dimensional peeking increases the competitiveness of zeroth-order optimization for discrete and non-convex simulations.

</details>


### [19] [Automated univariate time series forecasting with regression trees](https://arxiv.org/abs/2602.00077)
*Francisco Martínez,María P. Frías*

Main category: cs.LG

TL;DR: 该论文提出了一种基于回归树及其集成方法（装袋和随机森林）的自动化单变量时间序列预测方法，解决了自回归特征选择、趋势处理和季节性处理等关键问题，实验结果显示预测精度可与指数平滑或ARIMA等经典统计模型相媲美。


<details>
  <summary>Details</summary>
Motivation: 开发一种自动化、基于机器学习的时间序列预测方法，以替代传统统计模型，同时解决自回归特征选择、趋势和季节性处理等时间序列预测中的关键挑战。

Method: 采用回归树及其集成方法（装袋和随机森林）进行单变量时间序列预测，使用自回归方法和递归预测，提出了自回归特征选择策略、趋势序列处理方法和季节性行为应对方案。

Result: 实验结果表明，该方法在预测精度上与指数平滑或ARIMA等成熟统计模型相当，并开发了公开可用的软件实现所有提出的策略。

Conclusion: 基于回归树集成的方法为时间序列预测提供了一种有效的自动化替代方案，能够处理自回归特征选择、趋势和季节性等关键问题，且预测性能与传统统计方法相当，同时提供了实用的软件工具。

Abstract: This paper describes a methodology for automated univariate time series forecasting using regression trees and their ensembles: bagging and random forests. The key aspects that are addressed are: the use of an autoregressive approach and recursive forecasts, how to select the autoregressive features, how to deal with trending series and how to cope with seasonal behavior. Experimental results show a forecast accuracy comparable with well-established statistical models such as exponential smoothing or ARIMA. Furthermore, a publicly available software implementing all the proposed strategies has been developed and is described in the paper.

</details>


### [20] [Lossless Embedding Compression via Spherical Coordinates](https://arxiv.org/abs/2602.00079)
*Han Xiao*

Main category: cs.LG

TL;DR: 提出一种无损压缩单位范数嵌入的方法，实现1.5倍压缩率，比现有最佳方法提升25%


<details>
  <summary>Details</summary>
Motivation: 单位范数嵌入在信息检索和机器学习中广泛应用，但现有压缩方法效率有限。高维单位向量的球坐标集中在π/2附近，这一特性未被充分利用

Method: 利用高维单位向量球坐标集中在π/2附近的特性，导致IEEE 754指数位坍缩为单一值，从而启用熵编码。方法无需训练，在float32精度内完全无损

Result: 在文本、图像和多向量嵌入的26种配置评估中，均实现1.5倍压缩率，比先前最佳方法提升25%

Conclusion: 该方法为高维单位向量嵌入提供了一种高效的无损压缩方案，利用球坐标分布特性显著提升压缩效率

Abstract: We present a lossless compression method for unit-norm embeddings that achieves 1.5$\times$ compression, 25\% better than the best prior method. The method exploits that spherical coordinates of high-dimensional unit vectors concentrate around $π/2$, causing IEEE 754 exponents to collapse to a single value and enabling entropy coding. Evaluation across 26 configurations spanning text, image, and multi-vector embeddings confirms consistent improvement. The method requires no training and is fully lossless within float32 precision.

</details>


### [21] [Why LoRA Resists Label Noise: A Theoretical Framework for Noise-Robust Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2602.00084)
*Brady Steele*

Main category: cs.LG

TL;DR: LoRA具有内在的抗标签噪声能力，论文通过理论分析揭示了其三个关键特性：有限的记忆能力、最优秩平衡以及时间分离现象，并提出了基于秩感知的课程训练方法RACT。


<details>
  <summary>Details</summary>
Motivation: 虽然LoRA已成为微调大型预训练模型的主流方法，但其抗标签噪声的特性尚未得到充分探索。本文旨在从理论角度解释LoRA为何对标签噪声具有内在抵抗力，并利用这一特性开发有效的噪声检测方法。

Method: 论文建立了理论框架分析LoRA的抗噪声特性，包括证明其记忆能力的数学界限、推导最优秩平衡公式，以及发现干净模式和噪声记忆的时间分离现象。基于这些理论发现，提出了RACT（Rank-Aware Curriculum Training）方法，利用秩差异进行噪声检测。

Result: 理论分析得到实验验证：RACT在AG News数据集上实现了91.1%的噪声检测F1分数，同时保持了91.46%的准确率，与缺乏噪声检测能力的基线方法表现相当。

Conclusion: LoRA的内在抗噪声特性源于其有限的记忆能力、最优秩平衡和时间分离现象。利用这些特性开发的RACT方法能够有效检测标签噪声，为参数高效微调提供了新的理论理解和实用工具。

Abstract: Parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA) have become the dominant paradigm for adapting large pretrained models. We present a theoretical framework explaining an underexplored property: LoRA's inherent resistance to label noise. Our analysis reveals three key insights. First, we prove that rank-$r$ LoRA cannot memorize all possible label assignments once the sample size exceeds $O(r(d+k-r))$, limiting its capacity to fit arbitrary noise. Second, we derive an optimal rank balancing approximation bias and noise-induced variance, showing it decreases with noise rate. Third, we establish temporal separation: clean patterns are learned early while noise memorization occurs later. We propose RACT (Rank-Aware Curriculum Training), leveraging rank discrepancy for noise detection. Experiments validate our predictions, with RACT achieving 91.1% F1 for noise detection on AG News while maintaining 91.46% accuracy, competitive with baselines that lack noise detection capability.

</details>


### [22] [CARE-RFT: Confidence-Anchored Reinforcement Finetuning for Reliable Reasoning in Large Language Models](https://arxiv.org/abs/2602.00085)
*Shuozhe Li,Jincheng Cao,Bodun Hu,Aryan Mokhtari,Leqi Liu,Amy Zhang*

Main category: cs.LG

TL;DR: CARE-RFT是一种新的强化微调方法，使用偏斜反向KL散度替代标准反向KL正则化，在保持模型可信度的同时实现强大的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化微调方法存在关键权衡：无约束RFT能获得强大推理性能但严重损害模型可信度（增加幻觉、恶化校准），而RKL约束RFT能保持可信度但限制了推理增益。需要解决这一矛盾。

Method: 提出CARE-RFT方法，用偏斜反向KL散度替代标准反向KL正则化。该方法提供置信度敏感的惩罚：对自信且持续获得奖励的探索提供有界惩罚以支持推理，对其他情况提供无界惩罚以保持校准。

Result: 在多个模型规模和RFT算法上的广泛实验表明，CARE-RFT实现了优越的平衡，匹配无约束RFT的推理性能，同时恢复基础模型的可信度和校准。

Conclusion: 精心设计的置信度感知正则化是构建既有能力又可信赖的推理模型的关键。CARE-RFT解决了强化微调中的关键权衡问题。

Abstract: Reinforcement finetuning (RFT) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models. However, we identify a critical trade-off: while unconstrained RFT achieves strong reasoning performance, it severely compromises model trustworthiness by amplifying hallucination and worsening calibration; conversely, RKL-constrained RFT preserves trustworthiness but limits reasoning gains due to its unbounded penalty on exploratory deviations. To resolve this tension, we introduce CARE-RFT (Confidence-Anchored Regularized Reinforcement Finetuning), a novel method that replaces standard reverse KL regularization with a skew reverse KL divergence. CARE-RFT provides a confidence-sensitive penalty: it is bounded for confident, consistently rewarded explorations to enable reasoning, while unbounded elsewhere to preserve calibration. Extensive experiments across multiple model scales and RFT algorithms show that CARE-RFT achieves a superior balance, matching the reasoning performance of unconstrained RFT while recovering the trustworthiness and calibration of the base model. Our work establishes that careful, confidence-aware regularization is key to building both capable and trustworthy reasoning models.

</details>


### [23] [ECCO: Evidence-Driven Causal Reasoning for Compiler Optimization](https://arxiv.org/abs/2602.00087)
*Haolin Pan,Lianghong Huang,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: ECCO框架结合可解释推理与组合搜索，通过构建思维链数据集让LLM学习优化决策的因果逻辑，然后让LLM作为策略师指导遗传算法，在7个数据集上平均减少24.44%的周期数。


<details>
  <summary>Details</summary>
Motivation: 传统编译器自动调优面临两难：黑盒搜索方法缺乏语义指导，而LLM方法又存在表面模式匹配和因果不透明的问题。需要一种既能利用语义理解又能保持可解释性的方法。

Method: 1) 提出逆向工程方法构建思维链数据集，将静态代码特征映射到可验证的性能证据；2) 让LLM学习优化决策的因果逻辑而非简单模仿序列；3) 设计协作推理机制，LLM作为策略师定义优化意图，动态指导遗传算法的变异操作。

Result: 在7个数据集上的实验结果表明，ECCO显著优于LLVM opt -O3基准，平均减少了24.44%的周期数。

Conclusion: ECCO成功地将可解释推理与组合搜索相结合，解决了传统编译器自动调优方法的局限性，实现了语义指导与可解释性的平衡，显著提升了优化效果。

Abstract: Compiler auto-tuning faces a dichotomy between traditional black-box search methods, which lack semantic guidance, and recent Large Language Model (LLM) approaches, which often suffer from superficial pattern matching and causal opacity. In this paper, we introduce ECCO, a framework that bridges interpretable reasoning with combinatorial search. We first propose a reverse engineering methodology to construct a Chain-of-Thought dataset, explicitly mapping static code features to verifiable performance evidence. This enables the model to learn the causal logic governing optimization decisions rather than merely imitating sequences. Leveraging this interpretable prior, we design a collaborative inference mechanism where the LLM functions as a strategist, defining optimization intents that dynamically guide the mutation operations of a genetic algorithm. Experimental results on seven datasets demonstrate that ECCO significantly outperforms the LLVM opt -O3 baseline, achieving an average 24.44% reduction in cycles.

</details>


### [24] [From Numbers to Prompts: A Cognitive Symbolic Transition Mechanism for Lightweight Time-Series Forecasting](https://arxiv.org/abs/2602.00088)
*Namkyung Yoon,Hwangnam Kim*

Main category: cs.LG

TL;DR: STM是一种通过符号抽象和提示工程将数值时间序列数据与语言模型连接的新框架，显著提升轻量级语言模型在时间序列预测中的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在时间序列预测中表现出色，但其巨大的计算和内存需求限制了在轻量级平台上的部署。需要一种方法在保持模型完整性的同时，显著提高效率。

Method: 提出符号转换机制（STM），通过基于人类认知结构的量化技术将连续时间序列值转换为符号标记，并通过符号的结构化转换捕捉时间动态，使语言模型专注于时间序列数据的关键部分。

Result: 在多种时间序列数据集上，配合四种小型语言模型（SLM），STM相比默认骨干SLM实现了高达69%的MAE误差减少和90%的MSE误差减少，而资源开销极小（GPU内存仅增加约0.06%，延迟开销仅增加0.64%）。

Conclusion: STM作为一种高效、适应性强的符号驱动时间序列预测层，展示了在有限计算环境下使用基础模型进行时间序列预测的潜力，以可忽略的资源成本实现了显著的准确性提升。

Abstract: Large language models have achieved remarkable success in time series prediction tasks, but their substantial computational and memory requirements limit deployment on lightweight platforms. In this paper, we propose the Symbolic Transition Mechanism (STM) a novel framework that bridges numeric time series data and language models through symbolic abstraction and prompt engineering. STM transforms continuous time series values into symbol tokens with quantization techniques based on human cognitive structures, and captures temporal dynamics through structured transformations of symbols, enabling fast engineering based predictions in which language models focus on critical parts of time series data. STM is a general purpose mechanisms that ensure the integrity of backbone language models, but they significantly improve their efficiency by inferring the dynamic and structured patterns inherent in time series data. We evaluated STM on various time series datasets, paired with four small language models (SLM) with limited computational environments. For all models, STM achieves error reductions of up to 69% in MAE and 90% in MSE compared to the default backbone SLM without STM. These results demonstrate the potential of STM as an efficient, adaptable layer for symbol-driven time series prediction using foundation models. The accuracy improvements were made at negligible resource costs, with maximum GPU memory of the base model increasing by approximately 0.06% and latency overhead increasing by only 0.64%.

</details>


### [25] [Interpreting and Controlling Model Behavior via Constitutions for Atomic Concept Edits](https://arxiv.org/abs/2602.00092)
*Neha Kalibhat,Zi Wang,Prasoon Bajpai,Drew Proud,Wenjun Zeng,Been Kim,Mani Malek*

Main category: cs.LG

TL;DR: 该论文提出了一种黑盒可解释性框架，通过学习可验证的"宪法"来理解提示词修改如何影响模型行为，该方法通过概念编辑操作学习因果映射，并在数学推理和文本到图像生成等任务中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 当前需要一种系统性的方法来理解提示词修改如何影响模型的具体行为（如对齐性、正确性、约束遵守），以便更好地控制和理解模型行为。

Method: 提出黑盒可解释性框架，使用原子概念编辑（ACEs）——在输入提示中添加、移除或替换可解释概念，通过系统应用这些编辑并观察模型行为变化，学习从编辑到可预测结果的因果映射。

Result: 在文本到图像生成中，GPT-Image关注语法遵守，Imagen 4优先考虑氛围一致性；在数学推理中，干扰变量会混淆GPT-5，但对Gemini 2.5和o4-mini影响较小。学习到的宪法在控制模型行为方面比不使用宪法的方法平均提升1.86倍成功率。

Conclusion: 该框架能够学习到可验证的宪法，提供对模型行为的深入、可泛化的洞察，有效用于控制和理解模型行为，在不同任务和模型中都表现出显著效果。

Abstract: We introduce a black-box interpretability framework that learns a verifiable constitution: a natural language summary of how changes to a prompt affect a model's specific behavior, such as its alignment, correctness, or adherence to constraints. Our method leverages atomic concept edits (ACEs), which are targeted operations that add, remove, or replace an interpretable concept in the input prompt. By systematically applying ACEs and observing the resulting effects on model behavior across various tasks, our framework learns a causal mapping from edits to predictable outcomes. This learned constitution provides deep, generalizable insights into the model. Empirically, we validate our approach across diverse tasks, including mathematical reasoning and text-to-image alignment, for controlling and understanding model behavior. We found that for text-to-image generation, GPT-Image tends to focus on grammatical adherence, while Imagen 4 prioritizes atmospheric coherence. In mathematical reasoning, distractor variables confuse GPT-5 but leave Gemini 2.5 models and o4-mini largely unaffected. Moreover, our results show that the learned constitutions are highly effective for controlling model behavior, achieving an average of 1.86 times boost in success rate over methods that do not use constitutions.

</details>


### [26] [Trade-offs Between Individual and Group Fairness in Machine Learning: A Comprehensive Review](https://arxiv.org/abs/2602.00094)
*Sandra Benítez-Peña,Blas Kolic,Victoria Menendez,Belén Pulido*

Main category: cs.LG

TL;DR: 这篇综述论文系统性地回顾了同时处理群体公平和个体公平的混合公平方法，分析了不同方法的理论基础、优化机制和实证评估，并讨论了现有方法的局限性和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 算法公平已成为计算决策系统的核心关注点。传统上，群体公平和个体公平这两种主要的公平概念被孤立研究，但实际应用中需要同时考虑这两种公平性。本文旨在填补这一研究空白，为研究人员和从业者提供设计同时保证个体和群体层面公平性的混合算法的全面资源。

Method: 采用系统性文献综述方法，对混合公平方法进行分类整理：1) 根据采用的公平机制进行分类；2) 分析用于协调多个公平标准的算法和数学策略；3) 对每类方法考察其理论基础、优化机制和实证评估实践；4) 讨论现有方法的局限性。

Result: 论文提供了混合公平方法的系统分类框架，揭示了不同方法在理论保证、计算效率和实际效果方面的差异。研究发现，虽然已有多种方法尝试整合群体公平和个体公平，但大多数方法在理论严谨性、计算可扩展性和实际部署方面仍存在局限性。

Conclusion: 混合公平方法是一个重要且有前景的研究方向，但需要开发更加原则化、上下文感知的方法。未来的研究应关注：1) 开发更严谨的理论框架；2) 提高计算效率；3) 考虑实际应用场景的复杂性；4) 建立更全面的评估标准。这篇综述为这一领域的研究提供了系统的参考框架。

Abstract: Algorithmic fairness has become a central concern in computational decision-making systems, where ensuring equitable outcomes is essential for both ethical and legal reasons. Two dominant notions of fairness have emerged in the literature: Group Fairness (GF), which focuses on mitigating disparities across demographic subpopulations, and Individual Fairness (IF), which emphasizes consistent treatment of similar individuals. These notions have traditionally been studied in isolation. In contrast, this survey examines methods that jointly address GF and IF, integrating both perspectives within unified frameworks and explicitly characterizing the trade-offs between them. We provide a systematic and critical review of hybrid fairness approaches, organizing existing methods according to the fairness mechanisms they employ and the algorithmic and mathematical strategies used to reconcile multiple fairness criteria. For each class of methods, we examine their theoretical foundations, optimization mechanisms, and empirical evaluation practices, and discuss their limitations. Additionally, we discuss the challenges and identify open research directions for developing principled, context-aware hybrid fairness methods. By synthesizing insights across the literature, this survey aims to serve as a comprehensive resource for researchers and practitioners seeking to design hybrid algorithms that provide reliable fairness guarantees at both the individual and group levels.

</details>


### [27] [Gauss-Newton Natural Gradient Descent for Shape Learning](https://arxiv.org/abs/2602.00099)
*James King,Arturs Berzins,Siddhartha Mishra,Marius Zeinhofer*

Main category: cs.LG

TL;DR: 论文探索了在形状学习中应用高斯-牛顿法进行优化，包括隐式神经表面和几何感知神经网络，相比一阶方法实现了更快更稳定的收敛。


<details>
  <summary>Details</summary>
Motivation: 形状学习面临两个关键挑战：底层微分约束的病态性，以及参数空间优化问题与自然问题所在函数空间之间的不匹配。传统一阶方法在这些问题上收敛缓慢且不稳定。

Method: 采用高斯-牛顿方法进行形状学习优化，该方法特别适用于处理隐式神经表面和几何感知神经网络的优化问题，能够更好地处理微分约束和空间不匹配问题。

Result: 在基准形状优化任务上的实验表明，高斯-牛顿方法相比标准一阶方法显著提高了训练速度，减少了迭代次数，同时提高了最终解的精度。

Conclusion: 高斯-牛顿方法为形状学习提供了一种更高效、更稳定的优化框架，能够有效解决传统方法面临的收敛问题，在训练速度和最终精度方面都有显著改进。

Abstract: We explore the use of the Gauss-Newton method for optimization in shape learning, including implicit neural surfaces and geometry-informed neural networks. The method addresses key challenges in shape learning, such as the ill-conditioning of the underlying differential constraints and the mismatch between the optimization problem in parameter space and the function space where the problem is naturally posed. This leads to significantly faster and more stable convergence than standard first-order methods, while also requiring far fewer iterations. Experiments across benchmark shape optimization tasks demonstrate that the Gauss-Newton method consistently improves both training speed and final solution accuracy.

</details>


### [28] [THDC: Training Hyperdimensional Computing Models with Backpropagation](https://arxiv.org/abs/2602.00116)
*Hanne Dejonghe,Sam Leroux*

Main category: cs.LG

TL;DR: THDC通过可训练嵌入和二进制神经网络实现端到端超维计算，将维度从10,000降至64，在多个数据集上达到或超越现有HDC性能


<details>
  <summary>Details</summary>
Motivation: 传统超维计算依赖超高维度和静态随机初始化向量，导致内存效率低和学习能力有限，需要更高效的HDC方法

Method: 提出可训练超维计算(THDC)，用可训练嵌入替代随机初始化向量，引入单层二进制神经网络优化类别表示，支持端到端反向传播训练

Result: 在MNIST、Fashion-MNIST和CIFAR-10数据集上，THDC以仅64维度的超向量达到或超越现有HDC方法（需要10,000维）的准确率

Conclusion: THDC显著提高了超维计算的内存效率和性能，为资源受限设备上的轻量级学习提供了更优解决方案

Abstract: Hyperdimensional computing (HDC) offers lightweight learning for energy-constrained devices by encoding data into high-dimensional vectors. However, its reliance on ultra-high dimensionality and static, randomly initialized hypervectors limits memory efficiency and learning capacity. Therefore, we propose Trainable Hyperdimensional Computing (THDC), which enables end-to-end HDC via backpropagation. THDC replaces randomly initialized vectors with trainable embeddings and introduces a one-layer binary neural network to optimize class representations. Evaluated on MNIST, Fashion-MNIST and CIFAR-10, THDC achieves equal or better accuracy than state-of-the-art HDC, with dimensionality reduced from 10.000 to 64.

</details>


### [29] [Predicting Mortgage Default with Machine Learning: AutoML, Class Imbalance, and Leakage Control](https://arxiv.org/abs/2602.00120)
*Xianghong Hu,Tianning Xu,Ying Chen,Shuai Wang*

Main category: cs.LG

TL;DR: 该论文研究了抵押贷款违约预测中的三个关键挑战：标签模糊、类别不平衡和信息泄露，并通过泄漏感知特征选择、严格时间分割和受控下采样等方法，在真实数据集上评估了多种机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 抵押贷款违约预测是金融风险管理的核心任务，但在真实数据集中存在三个主要问题：违约标签模糊、严重的类别不平衡以及由时间结构和事后变量引起的信息泄露，这些因素会削弱评估有效性和部署可靠性。

Method: 采用泄漏感知特征选择、严格的时间分割（限制贷款发放和报告期）、受控的多数类下采样，并评估了多种机器学习方法，包括AutoML（AutoGluon）。

Result: 在不同正负样本比例下，模型性能保持稳定，AutoGluon在所有评估模型中取得了最强的AUROC性能。

Conclusion: 通过适当的泄漏控制和类别不平衡处理，机器学习模型可以在抵押贷款违约预测中取得可靠性能，AutoML方法表现出色。该研究的扩展教学版本将作为书籍章节发表。

Abstract: Mortgage default prediction is a core task in financial risk management, and machine learning models are increasingly used to estimate default probabilities and provide interpretable signals for downstream decisions. In real-world mortgage datasets, however, three factors frequently undermine evaluation validity and deployment reliability: ambiguity in default labeling, severe class imbalance, and information leakage arising from temporal structure and post-event variables. We compare multiple machine learning approaches for mortgage default prediction using a real-world loan-level dataset, with emphasis on leakage control and imbalance handling. We employ leakage-aware feature selection, a strict temporal split that constrains both origination and reporting periods, and controlled downsampling of the majority class. Across multiple positive-to-negative ratios, performance remains stable, and an AutoML approach (AutoGluon) achieves the strongest AUROC among the models evaluated. An extended and pedagogical version of this work will appear as a book chapter.

</details>


### [30] [MiniTensor: A Lightweight, High-Performance Tensor Operations Library](https://arxiv.org/abs/2602.00125)
*Soumyadip Sarkar*

Main category: cs.LG

TL;DR: MiniTensor是一个开源张量运算库，专注于简洁性、正确性和性能，提供类似PyTorch的Python API，但用Rust引擎执行关键代码，包大小仅几MB，比主流框架小几个数量级。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习框架如PyTorch和TensorFlow包体积庞大，安装复杂。MiniTensor旨在提供一个极简但功能完整的张量运算库，保持研究开发所需的核心功能，同时大幅减小安装体积。

Method: 采用Python API与Rust引擎分离的架构：Python层提供类似PyTorch的API，Rust引擎处理性能关键操作。支持密集n维张量、广播、归约、矩阵乘法、反向模式自动微分、神经网络层和优化器。通过PyO3实现Python集成，采用动态计算图进行梯度计算。

Result: MiniTensor包大小仅几MB，比PyTorch和TensorFlow小几个数量级。在保持核心功能（张量运算、自动微分、神经网络层、优化器）的同时，实现了简洁、正确和性能的平衡。

Conclusion: MiniTensor证明了深度学习框架可以大幅简化而不牺牲核心功能，为CPU上的研究开发提供了一个轻量级替代方案，特别适合资源受限环境或需要快速部署的场景。

Abstract: We present MiniTensor, an open source tensor operations library that focuses on minimalism, correctness, and performance. MiniTensor exposes a familiar PyTorch-like Python API while it executes performance critical code in a Rust engine. The core supports dense $n$ dimensional tensors, broadcasting, reductions, matrix multiplication, reverse mode automatic differentiation, a compact set of neural network layers, and standard optimizers. In this paper, we describe the design of MiniTensor's architecture, including its efficient memory management, dynamic computation graph for gradients, and integration with Python via PyO3. We also compare the install footprint with PyTorch and TensorFlow to demonstrate that MiniTensor achieves a package size of only a few megabytes, several orders of magnitude smaller than mainstream frameworks, while preserving the essentials needed for research and development on CPUs. The repository can be found at https://github.com/neuralsorcerer/minitensor

</details>


### [31] [ALIGN: Aligned Delegation with Performance Guarantees for Multi-Agent LLM Reasoning](https://arxiv.org/abs/2602.00127)
*Tong Zhu,Baiting Chen,Jin Zhou,Hua Zhou,Sriram Sankararaman,Xiaowu Dai*

Main category: cs.LG

TL;DR: ALIGN提出了一种将LLM推理建模为对齐委托游戏的新方法，通过设计激励机制让多个智能体生成候选解决方案，然后选择最佳答案，理论上保证比单智能体方法有更好的性能。


<details>
  <summary>Details</summary>
Motivation: 传统LLM在复杂推理任务上表现不佳，现有的推理时集成方法虽然通过采样多样化推理路径或聚合多个候选答案来改进性能，但通常将候选答案视为独立的，且无法保证集成能提高推理质量。

Method: ALIGN将LLM推理建模为对齐委托游戏：委托人将任务委托给多个智能体，这些智能体在设计好的激励机制下生成候选解决方案，然后委托人从中选择最终答案。这种方法在保持智能体与委托人目标对齐的同时，促进了智能体间的结构化交互。

Result: 理论分析表明，在公平比较（同等访问候选解决方案）的条件下，ALIGN能够证明比单智能体生成方法有更好的期望性能。该方法适应了候选答案的相关性，并放宽了先前工作中常用的独立性假设。在广泛的LLM推理基准测试中，ALIGN始终优于强大的单智能体和集成基线方法。

Conclusion: ALIGN通过将LLM推理建模为对齐委托游戏，提供了一种理论上保证性能提升的多智能体推理框架，在保持目标对齐的同时实现了智能体间的结构化交互，显著提高了复杂推理任务的性能。

Abstract: LLMs often underperform on complex reasoning tasks when relying on a single generation-and-selection pipeline. Inference-time ensemble methods can improve performance by sampling diverse reasoning paths or aggregating multiple candidate answers, but they typically treat candidates independently and provide no formal guarantees that ensembling improves reasoning quality. We propose a novel method, Aligned Delegation for Multi-Agent LLM Reasoning (ALIGN), which formulates LLM reasoning as an aligned delegation game. In ALIGN, a principal delegates a task to multiple agents that generate candidate solutions under designed incentives, and then selects among their outputs to produce a final answer. This formulation induces structured interaction among agents while preserving alignment between agent and principal objectives. We establish theoretical guarantees showing that, under a fair comparison with equal access to candidate solutions, ALIGN provably improves expected performance over single-agent generation. Our analysis accommodates correlated candidate answers and relaxes independence assumptions that are commonly used in prior work. Empirical results across a broad range of LLM reasoning benchmarks consistently demonstrate that ALIGN outperforms strong single-agent and ensemble baselines.

</details>


### [32] [Quantum Model Parallelism for MRI-Based Classification of Alzheimer's Disease Stages](https://arxiv.org/abs/2602.00128)
*Emine Akpinar,Murat Oduncuoglu*

Main category: cs.LG

TL;DR: 提出量子并行模型QBPM用于阿尔茨海默病分期分类，利用量子优势实现高精度，在噪声环境下表现稳健，相比经典方法参数更少、精度更高。


<details>
  <summary>Details</summary>
Motivation: 随着寿命延长，阿尔茨海默病成为全球重大健康问题。传统AI方法面临数据量大、计算资源有限的问题，需要更快速高效的方法。量子AI利用叠加、纠缠原理和高维希尔伯特空间，能超越经典方法限制，对高维、异构、噪声数据提供更高精度。

Method: 提出量子并行模型QBPM架构，受经典模型并行启发，使用两个不同的量子电路（每个包含旋转和纠缠模块），在同一量子模拟器上并行运行，用于MRI数据集的AD分期分类。

Result: 在两个不同数据集上评估，模型表现出高分类精度，证明其稳健性和泛化能力。在高斯噪声环境下（模拟真实条件）仍表现良好，验证了实际应用潜力。相比五种经典迁移学习方法，QBPM使用更少电路参数，实现了更高分类精度和可比执行时间。

Conclusion: QBPM架构代表了复杂疾病（如阿尔茨海默病）分期分类的创新强大方法，展示了量子计算在医学诊断中的实际应用潜力。

Abstract: With increasing life expectancy, AD has become a major global health concern. While classical AI-based methods have been developed for early diagnosis and stage classification of AD, growing data volumes and limited computational resources necessitate faster, more efficient approaches. Quantum-based AI methods, which leverage superposition and entanglement principles along with high-dimensional Hilbert space, can surpass classical approaches' limitations and offer higher accuracy for high-dimensional, heterogeneous, and noisy data. In this study, a Quantum-Based Parallel Model (QBPM) architecture is proposed for the efficient classification of AD stages using MRI datasets, inspired by the principles of classical model parallelism. The proposed model leverages quantum advantages by employing two distinct quantum circuits, each incorporating rotational and entanglement blocks, running in parallel on the same quantum simulator. The classification performance of the model was evaluated on two different datasets to assess its overall robustness and generalization capability. The proposed model demonstrated high classification accuracy across both datasets, highlighting its overall robustness and generalization capability. Results obtained under high-level Gaussian noise, simulating real-world conditions, further provided experimental evidence for the model's applicability not only in theoretical but also in practical scenarios. Moreover, compared with five different classical transfer learning methods, the proposed model demonstrated its efficiency as an alternative to classical approaches by achieving higher classification accuracy and comparable execution time while utilizing fewer circuit parameters. The results indicate that the proposed QBPM architecture represents an innovative and powerful approach for the classification of stages in complex diseases such as Alzheimer's.

</details>


### [33] [Monte Carlo Tree Search for Execution-Guided Program Repair with Large Language Models](https://arxiv.org/abs/2602.00129)
*Yixuan Liang*

Main category: cs.LG

TL;DR: CodePilot：结合蒙特卡洛树搜索与大语言模型的混合框架，通过执行引导的程序修复解决GitHub仓库级问题，在SWE-bench Lite上达到24.67%的问题解决率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自动程序修复在仓库级别面临挑战，主要由于长时程推理需求以及自回归解码的限制。需要一种能够处理真实GitHub问题、结合执行反馈的方法。

Method: CodePilot采用混合框架，将蒙特卡洛树搜索与大语言模型结合：1）从仓库到文件再到函数的分层故障定位；2）使用MCTS探索多样化的补丁轨迹；3）利用执行反馈作为奖励信号指导搜索和优化；4）引入置信度校准生成，选择性优化低置信度输出。

Result: 在SWE-bench Lite基准测试中，CodePilot使用开源模型实现了24.67%的问题解决率，优于可比基线方法。

Conclusion: 将符号搜索与神经语言模型相结合是构建可扩展、执行感知的软件工程自动化的有效策略。CodePilot展示了这种混合方法在真实GitHub问题修复中的潜力。

Abstract: Automated program repair with large language models remains challenging at the repository level due to long-horizon reasoning requirements and the limitations of autoregressive decoding. We present CodePilot, a hybrid framework that integrates Monte Carlo Tree Search (MCTS) with large language models to enable execution-guided program repair for real-world GitHub issues. CodePilot performs hierarchical fault localization from repository to file and function level, explores diverse patch trajectories using MCTS, and leverages execution feedback as a reward signal to guide search and refinement. The framework further incorporates confidence-calibrated generation to selectively refine low-confidence outputs. Experiments on SWE-bench Lite demonstrate that CodePilot achieves a 24.67% issue resolution rate using open-weight models, outperforming comparable baselines. These results suggest that combining symbolic search with neural language models is an effective strategy for scalable, execution-aware software engineering automation.

</details>


### [34] [On the Relationship Between Representation Geometry and Generalization in Deep Neural Networks](https://arxiv.org/abs/2602.00130)
*Sumit Yadav*

Main category: cs.LG

TL;DR: 有效维度（一种无监督几何度量）能强预测神经网络性能，跨视觉和NLP任务都有效，且存在双向因果关系


<details>
  <summary>Details</summary>
Motivation: 研究神经网络表示几何与性能之间的关系，探索能否通过无监督的几何度量来预测模型性能，而不依赖标签或模型大小

Method: 分析52个预训练ImageNet模型（13个架构家族），使用有效维度作为几何度量；在ImageNet、CIFAR-10、SST-2/MNLI、AG News等任务上验证；通过添加噪声和PCA进行因果实验

Result: 有效维度与准确率强相关（r=0.75），而模型大小不相关（r=0.07）；噪声降低几何质量导致性能下降（r=-0.94），PCA改善几何能保持性能；这种关系对噪声类型不敏感

Conclusion: 有效维度提供跨领域的预测性和因果性信息，完全无需标签即可计算，为神经网络性能评估提供了新的无监督度量方法

Abstract: We investigate the relationship between representation geometry and neural network performance. Analyzing 52 pretrained ImageNet models across 13 architecture families, we show that effective dimension -- an unsupervised geometric metric -- strongly predicts accuracy. Output effective dimension achieves partial r=0.75 ($p < 10^(-10)$) after controlling for model capacity, while total compression achieves partial r=-0.72. These findings replicate across ImageNet and CIFAR-10, and generalize to NLP: effective dimension predicts performance for 8 encoder models on SST-2/MNLI and 15 decoder-only LLMs on AG News (r=0.69, p=0.004), while model size does not (r=0.07). We establish bidirectional causality: degrading geometry via noise causes accuracy loss (r=-0.94, $p < 10^(-9)$), while improving geometry via PCA maintains accuracy across architectures (-0.03pp at 95% variance). This relationship is noise-type agnostic -- Gaussian, Uniform, Dropout, and Salt-and-pepper noise all show $|r| > 0.90$. These results establish that effective dimension provides domain-agnostic predictive and causal information about neural network performance, computed entirely without labels.

</details>


### [35] [RAPTOR: Ridge-Adaptive Logistic Probes](https://arxiv.org/abs/2602.00158)
*Ziqi Gao,Yaotian Zhu,Qingcheng Zeng,Xu Zhao,Ziqing Wang,Feng Ruan,Kaize Ding*

Main category: cs.LG

TL;DR: RAPTOR是一种基于L2正则化逻辑回归的轻量级探针方法，用于从冻结LLM的层表示中提取概念向量，在准确性、方向稳定性和训练成本方面优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有探针方法在提取概念向量时需要在准确性、方向稳定性和计算成本之间权衡。探针-引导流水线的有效性依赖于能够获得准确、方向稳定且计算成本低的概念向量。

Method: 提出RAPTOR（Ridge-Adaptive Logistic Probe），一种简单的L2正则化逻辑探针，通过验证调优的岭强度从归一化权重中提取概念向量。使用凸高斯极小极大定理（CGMT）在高维少样本机制下分析岭逻辑回归的理论特性。

Result: 在指令调优LLM和人工编写概念数据集上的广泛实验表明，RAPTOR在准确性上匹配或超过强基线方法，同时获得竞争性的方向稳定性和显著更低的训练成本。定性下游引导演示支持这些定量结果。

Conclusion: RAPTOR提供了一种简单有效的探针方法，能够从LLM表示中提取高质量概念向量。理论分析解释了惩罚强度如何调节探针准确性和概念向量稳定性，其结构预测与真实LLM嵌入中观察到的趋势定性一致。

Abstract: Probing studies what information is encoded in a frozen LLM's layer representations by training a lightweight predictor on top of them. Beyond analysis, probes are often used operationally in probe-then-steer pipelines: a learned concept vector is extracted from a probe and injected via additive activation steering by adding it to a layer representation during the forward pass. The effectiveness of this pipeline hinges on estimating concept vectors that are accurate, directionally stable under ablation, and inexpensive to obtain. Motivated by these desiderata, we propose RAPTOR (Ridge-Adaptive Logistic Probe), a simple L2-regularized logistic probe whose validation-tuned ridge strength yields concept vectors from normalized weights. Across extensive experiments on instruction-tuned LLMs and human-written concept datasets, RAPTOR matches or exceeds strong baselines in accuracy while achieving competitive directional stability and substantially lower training cost; these quantitative results are supported by qualitative downstream steering demonstrations. Finally, using the Convex Gaussian Min-max Theorem (CGMT), we provide a mechanistic characterization of ridge logistic regression in an idealized Gaussian teacher-student model in the high-dimensional few-shot regime, explaining how penalty strength mediates probe accuracy and concept-vector stability and yielding structural predictions that qualitatively align with trends observed on real LLM embeddings.

</details>


### [36] [Sheaf Neural Networks and biomedical applications](https://arxiv.org/abs/2602.00159)
*Aneeqa Mehrab,Jan Willem Van Looy,Pietro Demurtas,Stefano Iotti,Emil Malucelli,Francesca Rossi,Ferdinando Zanchetta,Rita Fioresi*

Main category: cs.LG

TL;DR: 本文介绍了层神经网络（SNN）的理论与数学模型，并通过生物医学案例研究证明SNN在性能上优于主流图神经网络（GNNs）如GCN、GAT和GraphSage。


<details>
  <summary>Details</summary>
Motivation: 当前图神经网络（GNNs）在处理生物医学问题时存在局限性，需要更有效的算法来回答复杂的生物医学问题。层神经网络（SNN）作为一种新兴方法，有望提供更好的性能。

Method: 提出层神经网络（SNN）算法，详细阐述其理论基础和数学模型，并通过具体的生物医学案例研究来验证其有效性。

Result: 在生物医学案例研究中，SNN算法表现出色，性能超越了最流行的图神经网络（GNNs），包括图卷积网络（GCNs）、图注意力网络（GAT）和GraphSage。

Conclusion: 层神经网络（SNN）不仅具有坚实的理论基础，而且在生物医学应用中能够有效解决实际问题，其性能优于现有的主流图神经网络方法。

Abstract: The purpose of this paper is to elucidate the theory and mathematical modelling behind the sheaf neural network (SNN) algorithm and then show how SNN can effectively answer to biomedical questions in a concrete case study and outperform the most popular graph neural networks (GNNs) as graph convolutional networks (GCNs), graph attention networks (GAT) and GraphSage.

</details>


### [37] [Block removal for large language models through constrained binary optimization](https://arxiv.org/abs/2602.00161)
*David Jansen,Roman Rausch,David Montero,Roman Orus*

Main category: cs.LG

TL;DR: 提出一种将Transformer块移除问题转化为约束二元优化问题的方法，通过映射到伊辛模型来高效评估候选配置，在多个基准测试中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 压缩大型语言模型时移除整个Transformer块看似简单，但确定移除哪些块构成指数级困难的组合优化问题。现有方法难以高效找到高质量的非连续块移除方案。

Method: 将块移除问题形式化为约束二元优化问题，映射到物理系统（伊辛模型），其能量可作为下游模型性能的强代理。仅需少量活动参数的前向和反向传播，配合伊辛求解器即可高效评估大量候选配置。

Result: 在多个基准测试中优于最先进的块移除方法，性能提升在短期重训练后仍能保持，在MMLU基准上达到高达6个点的改进。方法可应用于任意架构，包括具有高度非均匀块结构的复杂模型。

Conclusion: 该方法提供了一种高效、通用的Transformer块移除解决方案，能够发现高质量的非连续块移除配置，显著提升模型压缩性能，且计算成本较低。

Abstract: Compressing resource-intensive large language models by removing whole transformer blocks is a seemingly simple idea, but identifying which blocks to remove constitutes an exponentially difficult combinatorial problem. In this paper, we formulate block removal as a constrained binary optimization problem that can be mapped to a physical system (Ising model), whose energies are a strong proxy for downstream model performance. This formulation enables an efficient ranking of a large number of candidate block-removal configurations and yields many high-quality, non-trivial solutions beyond consecutive regions. We demonstrate that our approach outperforms state-of-the-art block-removal methods across several benchmarks, with performance gains persisting after short retraining, and reaching improvements of up to 6 points on the MMLU benchmark. Our method requires only forward and backward passes for a few active parameters, together with an (at least approximate) Ising solver, and can be readily applied to any architecture. We illustrate this generality on the recent NVIDIA-Nemotron-3-Nano-30B-A3B-FP8 model, which exhibits a highly inhomogeneous and challenging block structure.

</details>


### [38] [Benford's Law as a Distributional Prior for Post-Training Quantization of Large Language Models](https://arxiv.org/abs/2602.00165)
*Arthur Negrão,Pedro Silva,Vander L. S. Freitas,Gladston Moreira,Eduardo Luz*

Main category: cs.LG

TL;DR: Benford-Quant：一种受本福特定律启发的非均匀量化器，针对LLM权重分布特点设计，在低比特量化中提升性能


<details>
  <summary>Details</summary>
Motivation: 大语言模型压缩需求日益增长，但现有均匀量化器假设权重均匀分布，这与实际观察到的偏态分布不符。需要更符合实际权重分布的量化方法。

Method: 提出Benford-Quant，一种数据无关的非均匀量化器。将均匀网格替换为对数间隔的码本，为频繁出现的小幅度权重分配更多分辨率。该方法基于本福特定律，即前导数字遵循对数分布。

Result: 1) 发现transformer变换层权重符合本福特定律，归一化层则系统性地偏离；2) 在小语言模型上，Benford-Quant持续改善困惑度，在Gemma-270M的4位量化中降低困惑度超过10%；3) 在大语言模型上保持竞争力，差异由过参数化效应解释。

Conclusion: 将本福特定律先验融入量化网格是一种低成本修改，能在激进的低比特量化中带来精度提升。虽然在某些任务上未超越SOTA，但可与SmoothQuant、Activation-Aware Quantization等其他量化方法结合使用，无需大幅修改流程。

Abstract: The rapid growth of Large Language Models (LLMs) intensifies the need for effective compression, with weight quantization being the most widely adopted technique. Standard uniform quantizers assume that parameters are evenly distributed, an assumption at odds with the highly skewed distributions observed in practice. We propose Benford-Quant, a simple, data-free non-uniform quantizer inspired by Benford's Law, which predicts that leading digits follow a logarithmic distribution. Benford-Quant replaces the uniform grid with a log-spaced codebook, dedicating more resolution to the frequent small-magnitude weights. We provide both theoretical intuition and empirical evidence: (i) weights in transformer transformational layers adhere closely to Benford statistics, while normalization layers systematically deviate; (ii) on Small Language Models (SLMs), Benford-Quant consistently improves perplexity, reducing 4-bit perplexity on Gemma-270M by more than 10%; and (iii) on larger LLMs, it remains competitive, with differences explained by over-parameterization effects. Our results indicate that incorporating a Benford-inspired prior into quantization grids is a low-cost modification that yields accuracy gains in aggressive few-bit regimes. Although it is not able to surpass the state of the art in tasks such as perplexity and LAMBADA, the Benford-Quant approach can be hybridized with other quantization methods-such as SmoothQuant and Activation-Aware Quantization-without major pipeline modification, potentially improving their performance.

</details>


### [39] [Joint Continual Learning of Local Language Models and Cloud Offloading Decisions with Budget Constraints](https://arxiv.org/abs/2602.00166)
*Evan Chen,Wenzhi Fang,Shiqiang Wang,Christopher Brinton*

Main category: cs.LG

TL;DR: DA-GRPO是一种双优势扩展的组相对策略优化方法，通过在优势计算中直接融入云使用约束，使本地小语言模型能联合学习任务能力和协作行为，在持续学习中稳定控制云助手使用。


<details>
  <summary>Details</summary>
Motivation: 本地部署的小语言模型需要在严格的内存和计算约束下持续支持多样化任务，必须选择性地依赖云端大语言模型。然而，在持续学习中调节云助手使用具有挑战性，因为基于奖励的强化学习通常会产生不稳定的卸载行为，并在任务分布变化时加剧灾难性遗忘。

Method: 提出DA-GRPO（双优势组相对策略优化），这是组相对策略优化的双优势扩展，将云使用约束直接纳入优势计算中，避免了固定的奖励塑造和外部路由模型。该设计使本地模型能够联合学习任务能力和协作行为，允许在训练后自然出现云请求，同时遵守预设的助手预算。

Result: 在数学推理和代码生成基准测试中，DA-GRPO相比之前的协作和基于路由的方法，提高了切换后的准确性，显著减少了遗忘，并保持了稳定的云使用。

Conclusion: DA-GRPO通过将云使用约束直接融入策略优化过程，有效解决了小语言模型在持续学习中依赖云助手的稳定性问题，实现了任务能力学习与协作行为的联合优化。

Abstract: Locally deployed Small Language Models (SLMs) must continually support diverse tasks under strict memory and computation constraints, making selective reliance on cloud Large Language Models (LLMs) unavoidable. Regulating cloud assistance during continual learning is challenging, as naive reward-based reinforcement learning often yields unstable offloading behavior and exacerbates catastrophic forgetting as task distributions shift. We propose DA-GRPO, a dual-advantage extension of Group Relative Policy Optimization that incorporates cloud-usage constraints directly into advantage computation, avoiding fixed reward shaping and external routing models. This design enables the local model to jointly learn task competence and collaboration behavior, allowing cloud requests to emerge naturally during post-training while respecting a prescribed assistance budget. Experiments on mathematical reasoning and code generation benchmarks show that DA-GRPO improves post-switch accuracy, substantially reduces forgetting, and maintains stable cloud usage compared to prior collaborative and routing-based approaches.

</details>


### [40] [The Blessing of Dimensionality in LLM Fine-tuning: A Variance-Curvature Perspective](https://arxiv.org/abs/2602.00170)
*Qiyao Liang,Jinyeop Song,Yizhou Liu,Jeff Gore,Ila Fiete,Risto Miikkulainen,Xin Qiu*

Main category: cs.LG

TL;DR: 权重扰动进化策略(ES)可以用极小种群(约30个)微调十亿参数语言模型，这与经典零阶优化的维度诅咒直觉相悖。研究发现微调景观具有低维曲率特性，少量高曲率维度主导改进，解释了ES的可扩展性和训练动态的非单调性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解释两个看似矛盾的现象：1) 权重扰动进化策略(ES)能用极小种群微调大规模语言模型，违反维度诅咒直觉；2) 固定超参数下，微调奖励呈现先上升后下降的非单调动态。作者认为这两个现象反映了微调景观的共同几何特性。

Method: 使用ES作为几何探针，在GSM8K、ARC-C和WinoGrande数据集上对Qwen2.5-Instruct模型(0.5B-7B参数)进行微调奖励景观分析。提出最小二次随机上升模型来捕捉固定随机性下的上升-衰减动态，并分析改进更新的退化特性。

Result: 实验表明：1) 奖励改进扰动在不同规模模型中都可通过小种群实现；2) 微调景观具有低维曲率特性，少量高曲率维度主导改进；3) 许多随机扰动在这些方向上共享相似分量，产生退化改进更新；4) 这些发现解释了ES的可扩展性和非单调训练动态。

Conclusion: 高维微调可能比最坏情况理论所暗示的具有更广泛的可行优化方法。微调景观的低维曲率特性使得小种群ES能够有效工作，同时解释了固定超参数下的非单调训练动态。这些发现调和了ES的可扩展性与训练动态，为大规模语言模型微调提供了新的理论见解。

Abstract: Weight-perturbation evolution strategies (ES) can fine-tune billion-parameter language models with surprisingly small populations (e.g., $N\!\approx\!30$), contradicting classical zeroth-order curse-of-dimensionality intuition. We also observe a second seemingly separate phenomenon: under fixed hyperparameters, the stochastic fine-tuning reward often rises, peaks, and then degrades in both ES and GRPO. We argue that both effects reflect a shared geometric property of fine-tuning landscapes: they are low-dimensional in curvature. A small set of high-curvature dimensions dominates improvement, producing (i) heterogeneous time scales that yield rise-then-decay under fixed stochasticity, as captured by a minimal quadratic stochastic-ascent model, and (ii) degenerate improving updates, where many random perturbations share similar components along these directions. Using ES as a geometric probe on fine-tuning reward landscapes of GSM8K, ARC-C, and WinoGrande across Qwen2.5-Instruct models (0.5B--7B), we show that reward-improving perturbations remain empirically accessible with small populations across scales. Together, these results reconcile ES scalability with non-monotonic training dynamics and suggest that high-dimensional fine-tuning may admit a broader class of viable optimization methods than worst-case theory implies.

</details>


### [41] [Learning Robust Reasoning through Guided Adversarial Self-Play](https://arxiv.org/abs/2602.00173)
*Shuozhe Li,Vaishnav Tadiparthi,Kwonjoon Lee,Nakul Agarwal,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Lizhang Chen,Amy Zhang,Liu Leqi*

Main category: cs.LG

TL;DR: GASP：一种通过对抗性自博弈增强推理模型鲁棒性的方法，使模型能在错误上下文条件下检测并修复推理错误


<details>
  <summary>Details</summary>
Motivation: 现有的基于可验证奖励的强化学习（RLVR）模型在干净上下文条件下表现良好，但当上下文存在错误（如被污染的思维链、误导性部分解决方案或轻微输入扰动）时会灾难性失败，因为标准RLVR只优化干净条件下的最终答案正确性

Method: GASP（引导对抗性自博弈）方法：在单个模型内形成对抗性自博弈游戏，污染者学习通过局部一致的污染诱导失败，而智能体学习在相同污染条件下诊断和恢复。为解决训练早期成功恢复稀缺的问题，提出了分布内修复引导，即对自生成修复的模仿项，增加恢复概率同时保留已有能力

Result: 在四个开源模型（1.5B-8B）上，GASP将强但脆弱的推理器转化为鲁棒推理器，能够抵抗误导和扰动上下文，同时通常还能提高干净准确率。分析显示对抗性污染诱导了有效课程，分布内引导实现了快速恢复学习且表征漂移最小

Conclusion: GASP是一种有效的鲁棒化方法，仅使用结果验证就能显式训练检测和修复能力，无需人工标签或外部教师，显著提升了推理模型在错误上下文条件下的鲁棒性

Abstract: Reinforcement learning from verifiable rewards (RLVR) produces strong reasoning models, yet they can fail catastrophically when the conditioning context is fallible (e.g., corrupted chain-of-thought, misleading partial solutions, or mild input perturbations), since standard RLVR optimizes final-answer correctness only under clean conditioning. We introduce GASP (Guided Adversarial Self-Play), a robustification method that explicitly trains detect-and-repair capabilities using only outcome verification. Without human labels or external teachers, GASP forms an adversarial self-play game within a single model: a polluter learns to induce failure via locally coherent corruptions, while an agent learns to diagnose and recover under the same corrupted conditioning. To address the scarcity of successful recoveries early in training, we propose in-distribution repair guidance, an imitation term on self-generated repairs that increases recovery probability while preserving previously acquired capabilities. Across four open-weight models (1.5B--8B), GASP transforms strong-but-brittle reasoners into robust ones that withstand misleading and perturbed context while often improving clean accuracy. Further analysis shows that adversarial corruptions induce an effective curriculum, and in-distribution guidance enables rapid recovery learning with minimal representational drift.

</details>


### [42] [The Illusion of Forgetting: Attack Unlearned Diffusion via Initial Latent Variable Optimization](https://arxiv.org/abs/2602.00175)
*Manyi Li,Yufan Liu,Lai Jiang,Bing Li,Yuming Li,Weiming Hu*

Main category: cs.LG

TL;DR: 本文揭示扩散模型中的"遗忘"防御机制存在根本缺陷，NSFW概念并未真正被清除，而是作为休眠记忆保留。作者提出IVO攻击框架，通过优化初始潜在变量重新激活这些记忆，暴露当前防御方法的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 当前基于遗忘的防御方法声称能够从扩散模型中清除NSFW概念，但作者发现这种"遗忘"很大程度上是假象。未学习只是部分破坏了语言符号与底层知识之间的映射关系，而知识本身作为休眠记忆仍然完整保留。这暴露了当前防御方法的根本缺陷。

Method: 提出IVO（初始潜在变量优化）攻击框架，通过三个步骤：图像反演、对抗优化和重用攻击。该方法优化初始潜在变量，使未学习模型的噪声分布与其原始不安全状态重新对齐，从而重新激活休眠记忆。

Result: 在8种广泛使用的未学习技术上进行广泛实验，IVO实现了优越的攻击成功率（最高可达100%）和强大的语义一致性，显著优于现有攻击方法。这表明当前防御方法存在根本性漏洞。

Conclusion: 扩散模型中的未学习防御存在根本缺陷，NSFW概念并未真正被清除而是作为休眠记忆保留。IVO攻击框架能够有效重新激活这些记忆，暴露了当前防御方法的脆弱性，为未来更鲁棒的防御设计提供了重要启示。

Abstract: Although unlearning-based defenses claim to purge Not-Safe-For-Work (NSFW) concepts from diffusion models (DMs), we reveals that this "forgetting" is largely an illusion. Unlearning partially disrupts the mapping between linguistic symbols and the underlying knowledge, which remains intact as dormant memories. We find that the distributional discrepancy in the denoising process serves as a measurable indicator of how much of the mapping is retained, also reflecting the strength of unlearning. Inspired by this, we propose IVO (Initial Latent Variable Optimization), a concise and powerful attack framework that reactivates these dormant memories by reconstructing the broken mappings. Through Image Inversion}, Adversarial Optimization and Reused Attack, IVO optimizes initial latent variables to realign the noise distribution of unlearned models with their original unsafe states. Extensive experiments across 8 widely used unlearning techniques demonstrate that IVO achieves superior attack success rates and strong semantic consistency, exposing fundamental flaws in current defenses. The code is available at anonymous.4open.science/r/IVO/. Warning: This paper has unsafe images that may offend some readers.

</details>


### [43] [How Understanding Forecast Uncertainty Resolves the Explainability Problem in Machine Learning Models](https://arxiv.org/abs/2602.00179)
*Joseph L. Breeden*

Main category: cs.LG

TL;DR: 论文指出传统局部线性解释方法（如LIME、SHAP）在决策边界附近的不稳定性反映了预测不确定性问题，提出应先评估预测可用性再寻求解释，并批评某些模型的"处处可解释性"是虚假的。


<details>
  <summary>Details</summary>
Motivation: 在关键决策应用中，可解释性是主要关注点且常为监管要求。传统局部线性解释方法在决策边界附近的不稳定性受到批评，但作者认为这反映了对问题的误解——预测不确定性在决策边界处自然较高。

Method: 提出改变问题序列：首先评估预测是否具有足够低的可用性不确定性；当存在可用预测时，通过局部线性近似寻求解释；当没有可用预测时，决策应回归到更简单的整体模型（如传统逻辑回归）。

Result: 研究表明：1）预测不确定性高时解释稳定性自然低；2）应先判断预测可用性再寻求解释；3）某些声称"处处可解释"的模型（如ReLU网络、分段线性模型）在分段边界处的预测不确定性过高，其可解释性是虚假的。

Conclusion: 解释方法应与预测不确定性相匹配：当预测具有足够低的可用性不确定性时，局部线性解释是合适的；当预测不确定性过高时，解释无用，应使用更简单的模型进行决策。可解释性评估应优先考虑预测可用性而非盲目追求处处可解释。

Abstract: For applications of machine learning in critical decisions, explainability is a primary concern, and often a regulatory requirement. Local linear methods for generating explanations, such as LIME and SHAP, have been criticized for being unstable near decision boundaries. In this paper, we explain that such concerns reflect a misunderstanding of the problem. The forecast uncertainty is high at decision boundaries, so consequently, the explanatory instability is high. The correct approach is to change the sequence of events and questions being asked. Nonlinear models can be highly predictive in some regions while having little or no predictability in others. Therefore, the first question is whether a usable forecast exists. When there is a forecast with low enough uncertainty to be useful, an explanation can be sought via a local linear approximation. In such cases, the explanatory instability is correspondingly low. When no usable forecast exists, the decision must fall to a simpler overall model such as traditional logistic regression. Additionally, these results show that some methods that purport to be explainable everywhere, such as ReLU networks or any piecewise linear model, have only an illusory explainability, because the forecast uncertainty at the segment boundaries is too high to be useful. Explaining an unusable forecast is pointless.

</details>


### [44] [GEPC: Group-Equivariant Posterior Consistency for Out-of-Distribution Detection in Diffusion Models](https://arxiv.org/abs/2602.00191)
*Yadang Alexis Rouzoumka,Jean Pinsolle,Eugénie Terreaux,Christèle Morisseau,Jean-Philippe Ovarlez,Chengfang Ren*

Main category: cs.LG

TL;DR: 提出GEPC方法，通过检测扩散模型分数场的等变性一致性来识别分布外数据，无需训练且计算轻量


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的OOD检测方法主要利用分数大小或局部几何特征，忽略了分数场的等变性特性，而等变性破坏可能是OOD检测的有效信号

Method: 提出Group-Equivariant Posterior Consistency (GEPC)，通过测量学习到的分数场在有限群变换下的变换一致性来检测等变性破坏，仅需分数评估并生成可解释的等变性破坏图

Result: 在OOD图像基准数据集上，GEPC实现了与现有扩散基线方法相当或更好的AUROC性能；在高分辨率合成孔径雷达图像中，GEPC实现了强目标-背景分离并生成可解释的等变性破坏图

Conclusion: GEPC是一种无需训练、计算轻量的OOD检测方法，通过利用扩散模型分数场的等变性一致性，在多种数据集上表现出色，并提供了可解释的检测结果

Abstract: Diffusion models learn a time-indexed score field $\mathbf{s}_θ(\mathbf{x}_t,t)$ that often inherits approximate equivariances (flips, rotations, circular shifts) from in-distribution (ID) data and convolutional backbones. Most diffusion-based out-of-distribution (OOD) detectors exploit score magnitude or local geometry (energies, curvature, covariance spectra) and largely ignore equivariances. We introduce Group-Equivariant Posterior Consistency (GEPC), a training-free probe that measures how consistently the learned score transforms under a finite group $\mathcal{G}$, detecting equivariance breaking even when score magnitude remains unchanged. At the population level, we propose the ideal GEPC residual, which averages an equivariance-residual functional over $\mathcal{G}$, and we derive ID upper bounds and OOD lower bounds under mild assumptions. GEPC requires only score evaluations and produces interpretable equivariance-breaking maps. On OOD image benchmark datasets, we show that GEPC achieves competitive or improved AUROC compared to recent diffusion-based baselines while remaining computationally lightweight. On high-resolution synthetic aperture radar imagery where OOD corresponds to targets or anomalies in clutter, GEPC yields strong target-background separation and visually interpretable equivariance-breaking maps. Code is available at https://github.com/RouzAY/gepc-diffusion/.

</details>


### [45] [Reducing Memorisation in Generative Models via Riemannian Bayesian Inference](https://arxiv.org/abs/2602.00199)
*Johanna Marie Gegenfurtner,Albert Kjøller Jacobsen,Naima Elosegui Borras,Alejandro Valverde Mahou,Georgios Arvanitidis*

Main category: cs.LG

TL;DR: 该论文提出了一种贝叶斯方法，通过考虑损失函数的黎曼几何结构来平衡生成模型的记忆与泛化能力，减少记忆效应同时保持泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现代生成模型虽然能产生逼真样本，但在记忆与泛化之间取得平衡仍然是一个开放问题。作者从贝叶斯视角出发，关注流匹配和扩散模型的参数空间，旨在构建能更好捕捉数据分布变异性的预测后验分布。

Method: 采用贝叶斯方法，在流匹配和扩散模型的参数空间中构建预测后验。利用黎曼度量捕捉损失函数的几何结构，采用灵活的近似后验来适应损失景观的局部结构。这种方法允许采样与原始模型相似但记忆效应减少的生成模型。

Result: 实验证明，所提出的方法在保持泛化能力的同时有效减少了记忆效应。作者还提供了理论分析来解释这些发现。

Conclusion: 研究表明，考虑损失函数的几何结构能够有效利用参数空间，即使对于复杂的高维生成模型也是如此。这为解决生成模型中记忆与泛化的平衡问题提供了一种新途径。

Abstract: Modern generative models can produce realistic samples, however, balancing memorisation and generalisation remains an open problem. We approach this challenge from a Bayesian perspective by focusing on the parameter space of flow matching and diffusion models and constructing a predictive posterior that better captures the variability of the data distribution. In particular, we capture the geometry of the loss using a Riemannian metric and leverage a flexible approximate posterior that adapts to the local structure of the loss landscape. This approach allows us to sample generative models that resemble the original model, but exhibit reduced memorisation. Empirically, we demonstrate that the proposed approach reduces memorisation while preserving generalisation. Further, we provide a theoretical analysis of our method, which explains our findings. Overall, our work illustrates how considering the geometry of the loss enables effective use of the parameter space, even for complex high-dimensional generative models.

</details>


### [46] [Reducing Class-Wise Performance Disparity via Margin Regularization](https://arxiv.org/abs/2602.00205)
*Beier Zhu,Kesen Zhao,Jiequan Cui,Qianru Sun,Yuan Zhou,Xun Yang,Hanwang Zhang*

Main category: cs.LG

TL;DR: MR²通过动态调整logit和表示空间的边界来减少分类中的性能差异，基于理论分析提出边界正则化方法，在多个数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络即使在类别平衡的数据上训练，也经常表现出显著的类别间准确率差异，这对可靠部署构成挑战。虽然已有经验性解决方法，但对分类中这种性能差异的理论理解仍然有限。

Method: 提出MR²（Margin Regularization for Performance Disparity Reduction），通过理论分析建立基于边界的类别敏感泛化界，揭示每类特征变异性如何影响误差。该方法动态调整logit和表示空间的边界：优化每类logit边界与特征扩展成比例，并惩罚过大的表示边界以增强类内紧凑性。

Result: 在7个数据集（包括ImageNet）和多种预训练骨干网络（MAE、MoCov2、CLIP）上的实验表明，MR²不仅提高了整体准确率，还显著提升了困难类别的性能，同时不牺牲简单类别的性能，从而减少了性能差异。

Conclusion: MR²是一种理论上有原则的正则化方法，通过动态调整边界来减少分类中的性能差异，为理解和管理神经网络中的类别性能差异提供了理论指导和实用解决方案。

Abstract: Deep neural networks often exhibit substantial disparities in class-wise accuracy, even when trained on class-balanced data, posing concerns for reliable deployment. While prior efforts have explored empirical remedies, a theoretical understanding of such performance disparities in classification remains limited. In this work, we present Margin Regularization for Performance Disparity Reduction (MR$^2$), a theoretically principled regularization for classification by dynamically adjusting margins in both the logit and representation spaces. Our analysis establishes a margin-based, class-sensitive generalization bound that reveals how per-class feature variability contributes to error, motivating the use of larger margins for hard classes. Guided by this insight, MR$^2$ optimizes per-class logit margins proportional to feature spread and penalizes excessive representation margins to enhance intra-class compactness. Experiments on seven datasets, including ImageNet, and diverse pre-trained backbones (MAE, MoCov2, CLIP) demonstrate that MR$^2$ not only improves overall accuracy but also significantly boosts hard class performance without trading off easy classes, thus reducing performance disparity. Code is available at: https://github.com/BeierZhu/MR2

</details>


### [47] [Analyzing Shapley Additive Explanations to Understand Anomaly Detection Algorithm Behaviors and Their Complementarity](https://arxiv.org/abs/2602.00208)
*Jordan Levy,Paul Saves,Moncef Garouani,Nicolas Verstaevel,Benoit Gaudou*

Main category: cs.LG

TL;DR: 该论文提出了一种基于SHAP解释的异常检测器多样性评估方法，通过分析模型决策机制而非仅依赖输出分数来构建更互补的集成模型，从而提升无监督异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 无监督异常检测面临数据分布多样性和标签缺失的挑战。现有集成方法常因检测器决策机制相似而产生冗余异常分数，难以构建真正互补的集成模型，限制了集成学习的潜力。

Method: 使用SHAP（SHapley Additive exPlanations）量化每个异常检测器对输入特征的重要性分配，构建特征归因剖面。基于这些解释剖面计算检测器之间的相似性，通过解释多样性而非仅依赖原始输出分数来选择和组合模型。

Result: 研究表明：具有相似SHAP解释的检测器会产生相关的异常分数并识别重叠的异常；解释差异能可靠指示互补的检测行为；解释驱动的指标为集成模型选择提供了不同于原始输出的新标准。

Conclusion: 通过明确针对解释多样性同时保持模型质量，能够构建更互补、更多样化的集成模型，从而更有效地进行无监督异常检测。但仅多样性不足，高个体模型性能仍是有效集成的前提。

Abstract: Unsupervised anomaly detection is a challenging problem due to the diversity of data distributions and the lack of labels. Ensemble methods are often adopted to mitigate these challenges by combining multiple detectors, which can reduce individual biases and increase robustness. Yet building an ensemble that is genuinely complementary remains challenging, since many detectors rely on similar decision cues and end up producing redundant anomaly scores. As a result, the potential of ensemble learning is often limited by the difficulty of identifying models that truly capture different types of irregularities. To address this, we propose a methodology for characterizing anomaly detectors through their decision mechanisms. Using SHapley Additive exPlanations, we quantify how each model attributes importance to input features, and we use these attribution profiles to measure similarity between detectors. We show that detectors with similar explanations tend to produce correlated anomaly scores and identify largely overlapping anomalies. Conversely, explanation divergence reliably indicates complementary detection behavior. Our results demonstrate that explanation-driven metrics offer a different criterion than raw outputs for selecting models in an ensemble. However, we also demonstrate that diversity alone is insufficient; high individual model performance remains a prerequisite for effective ensembles. By explicitly targeting explanation diversity while maintaining model quality, we are able to construct ensembles that are more diverse, more complementary, and ultimately more effective for unsupervised anomaly detection.

</details>


### [48] [Dispersion Loss Counteracts Embedding Condensation and Improves Generalization in Small Language Models](https://arxiv.org/abs/2602.00217)
*Chen Liu,Xingzhi Sun,Xi Xiao,Alexandre Van Tassel,Ke Xu,Kristof Reimann,Danqi Liao,Mark Gerstein,Tianyang Wang,Xiao Wang,Smita Krishnaswamy*

Main category: cs.LG

TL;DR: 研究发现大语言模型存在"嵌入凝聚"现象，小模型token嵌入会坍缩到狭窄子空间，而大模型对此有抵抗力。提出分散损失函数来对抗此现象，无需增加参数即可提升小模型性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通过增加参数获得优异性能，但计算成本高昂。为了理解模型缩放机制，研究大模型与小模型之间的表征差异，目标是让小模型复制大模型的表征质量。

Method: 1. 系统分析多个Transformer家族模型，观察嵌入凝聚现象；2. 发现知识蒸馏不能可靠缓解此现象；3. 提出分散损失函数，在训练中显式鼓励嵌入分散；4. 在10个基准测试上进行实验验证。

Result: 1. 小模型如GPT2和Qwen3-0.6B表现出严重凝聚，大模型如GPT2-xl和Qwen3-32B对此有抵抗力；2. 分散损失能有效缓解凝聚现象；3. 恢复了大模型中的分散模式；4. 在10个基准测试上获得性能提升。

Conclusion: 嵌入凝聚是影响小模型性能的重要几何现象，提出的分散损失为在不增加参数的情况下改进小Transformer模型提供了原则性方法。

Abstract: Large language models (LLMs) achieve remarkable performance through ever-increasing parameter counts, but scaling incurs steep computational costs. To better understand LLM scaling, we study representational differences between LLMs and their smaller counterparts, with the goal of replicating the representational qualities of larger models in the smaller models. We observe a geometric phenomenon which we term $\textbf{embedding condensation}$, where token embeddings collapse into a narrow cone-like subspace in some language models. Through systematic analyses across multiple Transformer families, we show that small models such as $\texttt{GPT2}$ and $\texttt{Qwen3-0.6B}$ exhibit severe condensation, whereas the larger models such as $\texttt{GPT2-xl}$ and $\texttt{Qwen3-32B}$ are more resistant to this phenomenon. Additional observations show that embedding condensation is not reliably mitigated by knowledge distillation from larger models. To fight against it, we formulate a dispersion loss that explicitly encourages embedding dispersion during training. Experiments demonstrate that it mitigates condensation, recovers dispersion patterns seen in larger models, and yields performance gains across 10 benchmarks. We believe this work offers a principled path toward improving smaller Transformers without additional parameters.

</details>


### [49] [GRIP2: A Robust and Powerful Deep Knockoff Method for Feature Selection](https://arxiv.org/abs/2602.00218)
*Bob Junyi Zou,Lu Tian*

Main category: cs.LG

TL;DR: GRIP2是一种基于深度学习的特征选择方法，通过二维正则化表面积分和块随机采样，在高度相关、低信噪比场景下实现假发现率控制和高统计功效。


<details>
  <summary>Details</summary>
Motivation: 在非线性、高度相关、低信噪比场景中，现有深度学习方法难以在严格控制假发现率的同时识别真正有预测性的协变量，这是特征选择的核心挑战。

Method: 提出GRIP2方法：1) 构建二维正则化表面控制稀疏强度和稀疏化几何；2) 通过块随机采样在单次训练中近似表面积分；3) 生成反对称的深度knockoff特征重要性统计量，确保有限样本FDR控制。

Result: 在合成和半真实数据实验中，GRIP2对特征相关性和噪声水平具有更强鲁棒性：在高相关、低信噪比场景下保持高统计功效和稳定性。在真实HIV耐药数据中，能比线性基线方法更好地识别已知耐药相关突变。

Conclusion: GRIP2通过二维正则化表面积分和高效采样策略，在复杂非线性场景中实现了严格的FDR控制和高统计功效，为深度学习特征选择提供了可靠解决方案。

Abstract: Identifying truly predictive covariates while strictly controlling false discoveries remains a fundamental challenge in nonlinear, highly correlated, and low signal-to-noise regimes, where deep learning based feature selection methods are most attractive. We propose Group Regularization Importance Persistence in 2 Dimensions (GRIP2), a deep knockoff feature importance statistic that integrates first-layer feature activity over a two-dimensional regularization surface controlling both sparsity strength and sparsification geometry. To approximate this surface integral in a single training run, we introduce efficient block-stochastic sampling, which aggregates feature activity magnitudes across diverse regularization regimes along the optimization trajectory. The resulting statistics are antisymmetric by construction, ensuring finite-sample FDR control. In extensive experiments on synthetic and semi-real data, GRIP2 demonstrates improved robustness to feature correlation and noise level: in high correlation and low signal-to-noise ratio regimes where standard deep learning based feature selectors may struggle, our method retains high power and stability. Finally, on real-world HIV drug resistance data, GRIP2 recovers known resistance-associated mutations with power better than established linear baselines, confirming its reliability in practice.

</details>


### [50] [Green-NAS: A Global-Scale Multi-Objective Neural Architecture Search for Robust and Efficient Edge-Native Weather Forecasting](https://arxiv.org/abs/2602.00240)
*Md Muhtasim Munif Fahim,Soyda Humyra Yesmin,Saiful Islam,Md. Palash Bin Faruque,Md. A. Salam,Md. Mahfuz Uddin,Samiul Islam,Tofayel Ahmed,Md. Binyamin,Md. Rezaul Karim*

Main category: cs.LG

TL;DR: Green-NAS是一个面向低资源环境的多目标神经架构搜索框架，以天气预报为案例研究，在保持高精度的同时大幅减少模型参数和计算能耗。


<details>
  <summary>Details</summary>
Motivation: 针对传统神经架构搜索计算成本高、碳足迹大的问题，遵循"绿色AI"原则，开发适用于低资源环境的可持续NAS框架，以天气预报为具体应用场景。

Method: 采用多目标优化方法，同时优化模型精度和效率，寻找轻量级高精度模型；结合迁移学习技术，在历史数据有限的城市中提升预测精度。

Result: 最佳模型Green-NAS-A仅用153k参数达到RMSE 0.0988，比手动调优基线仅差1.4%，比GraphCast等全球天气预报模型参数少239倍；迁移学习可将预测精度提升约5.2%。

Conclusion: Green-NAS框架成功实现了在保持高预测精度的同时大幅降低计算成本和碳足迹，为低资源环境下的可持续AI应用提供了有效解决方案。

Abstract: We introduce Green-NAS, a multi-objective NAS (neural architecture search) framework designed for low-resource environments using weather forecasting as a case study. By adhering to 'Green AI' principles, the framework explicitly minimizes computational energy costs and carbon footprints, prioritizing sustainable deployment over raw computational scale. The Green-NAS architecture search method is optimized for both model accuracy and efficiency to find lightweight models with high accuracy and very few model parameters; this is accomplished through an optimization process that simultaneously optimizes multiple objectives. Our best-performing model, Green-NAS-A, achieved an RMSE of 0.0988 (i.e., within 1.4% of our manually tuned baseline) using only 153k model parameters, which is 239 times fewer than other globally applied weather forecasting models, such as GraphCast. In addition, we also describe how the use of transfer learning will improve the weather forecasting accuracy by approximately 5.2%, in comparison to a naive approach of training a new model for each city, when there is limited historical weather data available for that city.

</details>


### [51] [TABES: Trajectory-Aware Backward-on-Entropy Steering for Masked Diffusion Models](https://arxiv.org/abs/2602.00250)
*Shreshth Saini,Avinab Saha,Balu Adsumilli,Neil Birkbeck,Yilin Wang,Alan C. Bovik*

Main category: cs.LG

TL;DR: 提出BoE Steering方法，通过单次反向传播近似无限视野前瞻，解决MDM采样中的轨迹锁定问题，实现更高效的非自回归生成。


<details>
  <summary>Details</summary>
Motivation: 当前MDM采样方法依赖简单的置信度启发式规则，忽略了局部决策的长期影响，导致早期幻觉会级联成全局不一致性。虽然基于搜索的方法可以缓解此问题，但计算成本过高（每步需要O(K)次前向传播）。

Method: 提出Backward-on-Entropy (BoE) Steering框架：1) 从轨迹成本函数的一阶展开推导出Token Influence Score (TIS)，证明未来熵对输入嵌入的梯度可作为最小化不确定性的最优控制信号；2) 引入ActiveQueryAttention稀疏伴随原语，利用掩码目标结构降低反向传播复杂度。

Result: BoE在推理时间缩放方面实现了优于现有解掩码方法的Pareto前沿，表明梯度引导的转向为鲁棒的非自回归生成提供了数学原理清晰且高效的路径。

Conclusion: BoE Steering通过单次反向传播近似无限视野前瞻，解决了MDM采样中的轨迹锁定问题，在保持高效性的同时提升了生成质量，为掩码扩散模型提供了更优的推理框架。

Abstract: Masked Diffusion Models (MDMs) have emerged as a promising non-autoregressive paradigm for generative tasks, offering parallel decoding and bidirectional context utilization. However, current sampling methods rely on simple confidence-based heuristics that ignore the long-term impact of local decisions, leading to trajectory lock-in where early hallucinations cascade into global incoherence. While search-based methods mitigate this, they incur prohibitive computational costs ($O(K)$ forward passes per step). In this work, we propose Backward-on-Entropy (BoE) Steering, a gradient-guided inference framework that approximates infinite-horizon lookahead via a single backward pass. We formally derive the Token Influence Score (TIS) from a first-order expansion of the trajectory cost functional, proving that the gradient of future entropy with respect to input embeddings serves as an optimal control signal for minimizing uncertainty. To ensure scalability, we introduce \texttt{ActiveQueryAttention}, a sparse adjoint primitive that exploits the structure of the masking objective to reduce backward pass complexity. BoE achieves a superior Pareto frontier for inference-time scaling compared to existing unmasking methods, demonstrating that gradient-guided steering offers a mathematically principled and efficient path to robust non-autoregressive generation. We will release the code.

</details>


### [52] [VoxServe: Streaming-Centric Serving System for Speech Language Models](https://arxiv.org/abs/2602.00269)
*Keisuke Kamahori,Wei-Tzu Lee,Atindra Jha,Rohan Kadekodi,Stephanie Wang,Arvind Krishnamurthy,Baris Kasikci*

Main category: cs.LG

TL;DR: VoxServe是一个用于语音语言模型的统一流式服务系统，通过解耦模型架构与系统优化，实现高吞吐量和低延迟的流式推理。


<details>
  <summary>Details</summary>
Motivation: 现有系统在支持多样化语音语言模型的流式部署方面存在不足，无法同时满足低延迟、高吞吐量和流式保证的要求。

Method: 提出模型执行抽象层解耦模型架构与系统优化；实现流式感知调度和异步推理流水线以提高端到端效率。

Result: 在多种现代语音语言模型上评估，VoxServe相比现有实现实现了10-20倍的吞吐量提升，在可比延迟下保持高流式可行性。

Conclusion: VoxServe为语音语言模型提供了一个灵活高效的流式服务框架，显著提升了流式推理性能。

Abstract: Deploying modern Speech Language Models (SpeechLMs) in streaming settings requires systems that provide low latency, high throughput, and strong guarantees of streamability. Existing systems fall short of supporting diverse models flexibly and efficiently. We present VoxServe, a unified serving system for SpeechLMs that optimizes streaming performance. VoxServe introduces a model-execution abstraction that decouples model architecture from system-level optimizations, thereby enabling support for diverse SpeechLM architectures within a single framework. Building on this abstraction, VoxServe implements streaming-aware scheduling and an asynchronous inference pipeline to improve end-to-end efficiency. Evaluations across multiple modern SpeechLMs show that VoxServe achieves 10-20x higher throughput than existing implementations at comparable latency while maintaining high streaming viability. The code of VoxServe is available at https://github.com/vox-serve/vox-serve.

</details>


### [53] [Sample Complexity Analysis for Constrained Bilevel Reinforcement Learning](https://arxiv.org/abs/2602.00282)
*Naman Saxena,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 本文分析了约束双层强化学习算法的样本复杂度，提出了CBSO算法，获得了O(ε^{-2})的迭代复杂度和Õ(ε^{-4})的样本复杂度，首次使用Moreau包络分析非光滑目标函数的参数化策略梯度RL算法。


<details>
  <summary>Details</summary>
Motivation: 元学习、分层学习和人类反馈强化学习等许多重要RL问题都可以建模为双层RL问题，这些领域在实证上取得了很大进展，但双层RL算法的理论分析尚未得到足够关注。本文旨在分析约束双层RL算法的样本复杂度。

Method: 提出了约束双层次梯度优化（CBSO）算法，使用基于惩罚的目标函数来避免约束双层问题中的原始-对偶间隙和超梯度问题。采用Moreau包络来分析非光滑优化问题，这是首次使用Moreau包络分析具有非光滑目标函数的一般参数化策略梯度RL算法。

Result: 获得了O(ε^{-2})的迭代复杂度和Õ(ε^{-4})的样本复杂度。通过惩罚公式处理约束，避免了原始-对偶间隙问题，为约束双层RL问题提供了理论保证。

Conclusion: 本文首次对约束双层RL算法进行了系统的理论分析，提出了CBSO算法并证明了其收敛性，为元学习、分层学习和RL-HF等实际应用提供了理论基础，特别是在处理非光滑目标函数方面做出了创新性贡献。

Abstract: Several important problem settings within the literature of reinforcement learning (RL), such as meta-learning, hierarchical learning, and RL from human feedback (RL-HF), can be modelled as bilevel RL problems. A lot has been achieved in these domains empirically; however, the theoretical analysis of bilevel RL algorithms hasn't received a lot of attention. In this work, we analyse the sample complexity of a constrained bilevel RL algorithm, building on the progress in the unconstrained setting. We obtain an iteration complexity of $O(ε^{-2})$ and sample complexity of $\tilde{O}(ε^{-4})$ for our proposed algorithm, Constrained Bilevel Subgradient Optimization (CBSO). We use a penalty-based objective function to avoid the issue of primal-dual gap and hyper-gradient in the context of a constrained bilevel problem setting. The penalty-based formulation to handle constraints requires analysis of non-smooth optimization. We are the first ones to analyse the generally parameterized policy gradient-based RL algorithm with a non-smooth objective function using the Moreau envelope.

</details>


### [54] [Generation Order and Parallel Decoding in Masked Diffusion Models: An Information-Theoretic Perspective](https://arxiv.org/abs/2602.00286)
*Shaorong Zhang,Longxuan Yu,Rob Brekelmans,Luhan Tang,Salman Asif,Greg Ver Steeg*

Main category: cs.LG

TL;DR: 本文提出了一个统一的信息论框架来分析掩码扩散模型中的两个基本失败来源：顺序敏感性和并行化偏差，揭示了易先解码的优势、因子化并行解码的内在采样误差，以及验证与启发式方法的权衡。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型通过牺牲顺序确定性来加速推理，但生成顺序的理论机制和并行化风险尚未得到充分探索。本文旨在分析这些模型失败的根本原因。

Method: 提出了一个统一的信息论框架，将失败来源解耦为顺序敏感性和并行化偏差。通过理论分析推导出关键见解，并在受控的Block-HMM模型和大规模MDMs（如LLaDA）上进行实验验证。

Result: 研究发现：(1) 易先解码的优势随模型误差增大而增强；(2) 因子化并行解码会引入内在采样误差，可能导致任意大的反向KL散度；(3) 验证可以消除采样误差但代价指数增长，而启发式方法如重新掩码无法保证分布正确性。

Conclusion: 本文的理论框架为理解掩码扩散模型的失败机制提供了基础，揭示了并行化与准确性之间的根本权衡，为未来改进此类模型的设计提供了指导。

Abstract: Masked Diffusion Models (MDMs) significantly accelerate inference by trading off sequential determinism. However, the theoretical mechanisms governing generation order and the risks inherent in parallelization remain under-explored. In this work, we provide a unified information-theoretic framework to decouple and analyze two fundamental sources of failure: order sensitivity and parallelization bias. Our analysis yields three key insights: (1) The benefits of Easy-First decoding (prioritizing low-entropy tokens) are magnified as model error increases; (2) factorized parallel decoding introduces intrinsic sampling errors that can lead to arbitrary large Reverse KL divergence, capturing "incoherence" failures that standard Forward KL metrics overlook; and (3) while verification can eliminate sampling error, it incurs an exponential cost governed by the total correlation within a block. Conversely, heuristics like remasking, though computationally efficient, cannot guarantee distributional correctness. Experiments on a controlled Block-HMM and large-scale MDMs (LLaDA) for arithmetic reasoning validate our theoretical framework.

</details>


### [55] [Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation](https://arxiv.org/abs/2602.00294)
*Franz A. Heinsen,Leo Kozachkov*

Main category: cs.LG

TL;DR: 提出一种新的自注意力计算方法，能以固定成本（与上下文长度无关）计算任意精度，大幅降低内存和计算需求


<details>
  <summary>Details</summary>
Motivation: 传统Transformer自注意力计算成本随上下文长度增长，导致存储、计算和能耗需求超过社会供给能力，需要更高效的替代方案

Method: 通过将传统自注意力的泰勒展开分解为对称张量链表达式，利用对称性获得前馈变换，将查询和键高效映射到最小多项式核特征基坐标

Result: 实现了数量级的内存和计算减少，能以固定成本进行无限令牌生成，显著降低大规模Transformer模型的基础设施和能耗需求

Conclusion: 提出的方法使自注意力能以固定成本计算任意精度，为解决Transformer模型的计算和能耗问题提供了有效方案，引入的数学技术具有独立价值

Abstract: The most widely used artificial intelligence (AI) models today are Transformers employing self-attention. In its standard form, self-attention incurs costs that increase with context length, driving demand for storage, compute, and energy that is now outstripping society's ability to provide them. To help address this issue, we show that self-attention is efficiently computable to arbitrary precision with constant cost per token, achieving orders-of-magnitude reductions in memory use and computation. We derive our formulation by decomposing the conventional formulation's Taylor expansion into expressions over symmetric chains of tensor products. We exploit their symmetry to obtain feed-forward transformations that efficiently map queries and keys to coordinates in a minimal polynomial-kernel feature basis. Notably, cost is fixed inversely in proportion to head size, enabling application over a greater number of heads per token than otherwise feasible. We implement our formulation and empirically validate its correctness. Our work enables unbounded token generation at modest fixed cost, substantially reducing the infrastructure and energy demands of large-scale Transformer models. The mathematical techniques we introduce are of independent interest.

</details>


### [56] [From Observations to States: Latent Time Series Forecasting](https://arxiv.org/abs/2602.00297)
*Jie Yang,Yifan Hu,Yuante Li,Kexin Zhang,Kaize Ding,Philip S. Yu*

Main category: cs.LG

TL;DR: LatentTSF提出了一种新的时间序列预测范式，将预测从观测空间转移到潜在状态空间，解决了现有方法中存在的"潜在混沌"问题，即预测准确但潜在表示混乱的现象。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测模型存在"潜在混沌"问题：虽然预测准确，但学到的潜在表示在时间上是混乱且缺乏连续性的。这是因为当前主流的观测空间预测范式只最小化噪声和部分观测数据上的逐点误差，鼓励了捷径解而非恢复底层系统动态。

Method: 提出LatentTSF范式，使用自编码器将每个时间步的观测投影到高维潜在状态空间，然后在潜在空间中进行预测。这种方法将时间序列预测从观测回归转变为潜在状态预测，让模型专注于学习结构化的时间动态。

Result: 理论分析表明，提出的潜在目标函数隐式地最大化预测潜在状态与真实状态及观测之间的互信息。在广泛使用的基准测试上的实验证实，LatentTSF能有效缓解潜在混沌问题，并取得优越性能。

Conclusion: LatentTSF通过将预测转移到潜在状态空间，解决了时间序列预测中的潜在混沌问题，使模型能够更好地学习底层系统动态，从而提高预测性能。

Abstract: Deep learning has achieved strong performance in Time Series Forecasting (TSF). However, we identify a critical representation paradox, termed Latent Chaos: models with accurate predictions often learn latent representations that are temporally disordered and lack continuity. We attribute this phenomenon to the dominant observation-space forecasting paradigm. Most TSF models minimize point-wise errors on noisy and partially observed data, which encourages shortcut solutions instead of the recovery of underlying system dynamics. To address this issue, we propose Latent Time Series Forecasting (LatentTSF), a novel paradigm that shifts TSF from observation regression to latent state prediction. Specifically, LatentTSF employs an AutoEncoder to project observations at each time step into a higher-dimensional latent state space. This expanded representation aims to capture underlying system variables and impose a smoother temporal structure. Forecasting is then performed entirely in the latent space, allowing the model to focus on learning structured temporal dynamics. Theoretical analysis demonstrates that our proposed latent objectives implicitly maximize mutual information between predicted latent states and ground-truth states and observations. Extensive experiments on widely-used benchmarks confirm that LatentTSF effectively mitigates latent chaos, achieving superior performance. Our code is available in https://github.com/Muyiiiii/LatentTSF.

</details>


### [57] [Agentic Framework for Epidemiological Modeling](https://arxiv.org/abs/2602.00299)
*Rituparna Datta,Zihan Guan,Baltazar Espinoza,Yiqi Su,Priya Pitre,Srini Venkatramanan,Naren Ramakrishnan,Anil Vullikanti*

Main category: cs.LG

TL;DR: EPIAGENT是一个自动合成、校准、验证和优化流行病学模拟器的智能体框架，通过将疾病进展建模为迭代程序合成问题，使用流行病学流图中间表示来确保模型正确性


<details>
  <summary>Details</summary>
Motivation: 传统流行病学建模方法依赖固定模型类别，需要随着病原体、政策和场景假设的变化而手动重新设计，这限制了模型的适应性和效率

Method: 采用智能体框架，将疾病进展建模为迭代程序合成问题，使用流行病学流图中间表示连接场景规范与模型结构，进行模块化正确性检查，然后将验证的流图编译为支持可解释参数学习的机制模型

Result: 在流行病学场景案例研究中，EPIAGENT能够捕捉复杂的增长动态，在不同疫苗接种和免疫逃逸假设下产生流行病学一致的反事实预测，智能体反馈循环防止模型退化并显著加速向有效模型的收敛

Conclusion: EPIAGENT通过模仿专业专家工作流程，实现了流行病学模拟器的自动合成和优化，解决了传统方法需要手动重新设计的问题，提高了建模效率和适应性

Abstract: Epidemic modeling is essential for public health planning, yet traditional approaches rely on fixed model classes that require manual redesign as pathogens, policies, and scenario assumptions evolve. We introduce EPIAGENT, an agentic framework that automatically synthesizes, calibrates, verifies, and refines epidemiological simulators by modeling disease progression as an iterative program synthesis problem. A central design choice is an explicit epidemiological flow graph intermediate representation that links scenario specifications to model structure and enables strong, modular correctness checks before code is generated. Verified flow graphs are then compiled into mechanistic models supporting interpretable parameter learning under physical and epidemiological constraints. Evaluation on epidemiological scenario case studies demonstrates that EPIAGENT captures complex growth dynamics and produces epidemiologically consistent counterfactual projections across varying vaccination and immune escape assumptions. Our results show that the agentic feedback loop prevents degeneration and significantly accelerates convergence toward valid models by mimicking professional expert workflows.

</details>


### [58] [Neural Ising Machines via Unrolling and Zeroth-Order Training](https://arxiv.org/abs/2602.00302)
*Sam Reifenstein,Timothee Leleu*

Main category: cs.LG

TL;DR: 提出一种数据驱动的NP-hard Ising和Max-Cut优化启发式方法，通过神经网络参数化迭代动力学系统，学习节点级更新规则，实现高效的非凸能量景观搜索。


<details>
  <summary>Details</summary>
Motivation: 针对NP-hard的Ising和Max-Cut优化问题，传统方法面临计算复杂度高、难以在非凸能量景观中高效搜索的挑战。需要开发能够学习有效算法结构的数据驱动方法，提高优化效率。

Method: 提出神经网络参数化Ising机（NPIM），使用紧凑的多层感知器学习共享的节点级更新规则，将局部交互场映射到自旋更新。采用零阶优化器进行训练，避免长循环动力学中的梯度不稳定问题。

Result: NPIM在标准Ising和神经组合优化基准测试中，与最近的学习方法和经典Ising机启发式方法相比，获得了竞争性的解质量和求解时间。学习的动力学恢复了有效的算法结构，包括动量行为和时变调度。

Conclusion: NPIM作为一种数据驱动的启发式方法，能够学习有效的优化动力学，在低参数量的情况下实现高效的非凸能量景观搜索，为NP-hard优化问题提供了有前景的解决方案。

Abstract: We propose a data-driven heuristic for NP-hard Ising and Max-Cut optimization that learns the update rule of an iterative dynamical system. The method learns a shared, node-wise update rule that maps local interaction fields to spin updates, parameterized by a compact multilayer perceptron with a small number of parameters. Training is performed using a zeroth-order optimizer, since backpropagation through long, recurrent Ising-machine dynamics leads to unstable and poorly informative gradients. We call this approach a neural network parameterized Ising machine (NPIM). Despite its low parameter count, the learned dynamics recover effective algorithmic structure, including momentum-like behavior and time-varying schedules, enabling efficient search in highly non-convex energy landscapes. Across standard Ising and neural combinatorial optimization benchmarks, NPIM achieves competitive solution quality and time-to-solution relative to recent learning-based methods and strong classical Ising-machine heuristics.

</details>


### [59] [Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors](https://arxiv.org/abs/2602.00315)
*Arian Khorasani,Nathaniel Chen,Yug D Oswal,Akshat Santhana Gopalan,Egemen Kolemen,Ravid Shwartz-Ziv*

Main category: cs.LG

TL;DR: 使用类别条件归一化流作为oracle，在真实图像上获得精确后验分布，从而评估神经网络性能极限，揭示标准基准无法检测到的持续学习、架构差异和分布偏移本质。


<details>
  <summary>Details</summary>
Motivation: 标准基准无法评估神经网络性能极限，因为它们无法访问真实后验分布p(y|x)。需要一种方法来获得精确后验，从而深入分析神经网络的真实性能、学习极限和分布偏移影响。

Method: 使用类别条件归一化流作为oracle，在AFHQ和ImageNet等真实图像数据集上获得精确可处理的后验分布。通过这个框架进行五个方面的研究：缩放定律、学习极限、软标签、分布偏移和主动学习。

Result: 1) 预测误差可分解为不可约的偶然不确定性和可约的认知误差；认知误差随数据集大小呈幂律下降。2) 不同架构接近偶然不确定性下限的方式不同：ResNets呈现干净的幂律缩放，而Vision Transformers在低数据区域停滞。3) 使用精确后验作为软标签优于硬标签，实现近乎完美的校准。4) 分布偏移类型比幅度更重要：类别不平衡在KL散度值下几乎不影响准确性，而输入噪声会导致灾难性退化。5) 精确认知不确定性可区分信息丰富样本和固有模糊样本，提高主动学习效率。

Conclusion: 标准指标隐藏了持续学习过程，掩盖了架构差异，无法诊断分布偏移的本质。使用精确后验的框架揭示了神经网络性能的真实极限和学习动态，为模型评估和改进提供了新视角。

Abstract: How close are neural networks to the best they could possibly do? Standard benchmarks cannot answer this because they lack access to the true posterior p(y|x). We use class-conditional normalizing flows as oracles that make exact posteriors tractable on realistic images (AFHQ, ImageNet). This enables five lines of investigation. Scaling laws: Prediction error decomposes into irreducible aleatoric uncertainty and reducible epistemic error; the epistemic component follows a power law in dataset size, continuing to shrink even when total loss plateaus. Limits of learning: The aleatoric floor is exactly measurable, and architectures differ markedly in how they approach it: ResNets exhibit clean power-law scaling while Vision Transformers stall in low-data regimes. Soft labels: Oracle posteriors contain learnable structure beyond class labels: training with exact posteriors outperforms hard labels and yields near-perfect calibration. Distribution shift: The oracle computes exact KL divergence of controlled perturbations, revealing that shift type matters more than shift magnitude: class imbalance barely affects accuracy at divergence values where input noise causes catastrophic degradation. Active learning: Exact epistemic uncertainty distinguishes genuinely informative samples from inherently ambiguous ones, improving sample efficiency. Our framework reveals that standard metrics hide ongoing learning, mask architectural differences, and cannot diagnose the nature of distribution shift.

</details>


### [60] [Optimal Transport-Guided Adversarial Attacks on Graph Neural Network-Based Bot Detection](https://arxiv.org/abs/2602.00318)
*Kunal Mukherjee,Zulfikar Alom,Tran Gia Bao Ngo,Cuneyt Gurcan Akcora,Murat Kantarcioglu*

Main category: cs.LG

TL;DR: BOCLOAK：一种在现实约束下评估GNN社交机器人检测鲁棒性的对抗攻击框架，利用最优传输理论实现高效攻击


<details>
  <summary>Details</summary>
Motivation: 当前GNN机器人检测器在现实攻击场景下的鲁棒性未知，现有攻击方法受限于领域特定和时间约束，需要评估约束感知攻击下的检测器脆弱性

Method: BOCLOAK构建时空邻居特征的概率度量，学习分离人类和机器人行为的最优传输几何，将传输计划解码为稀疏、合理的边编辑，在现实约束下逃避检测

Result: 在三个社交机器人数据集、五个SOTA检测器、三个对抗防御和四个基线攻击上评估，BOCLOAK攻击成功率提高80.13%，GPU内存使用减少99.80%

Conclusion: 最优传输为对抗攻击和现实机器人检测之间提供了轻量级、原则性的桥梁，揭示了GNN检测器在约束感知攻击下的脆弱性

Abstract: The rise of bot accounts on social media poses significant risks to public discourse. To address this threat, modern bot detectors increasingly rely on Graph Neural Networks (GNNs). However, the effectiveness of these GNN-based detectors in real-world settings remains poorly understood. In practice, attackers continuously adapt their strategies as well as must operate under domain-specific and temporal constraints, which can fundamentally limit the applicability of existing attack methods. As a result, there is a critical need for robust GNN-based bot detection methods under realistic, constraint-aware attack scenarios.
  To address this gap, we introduce BOCLOAK to systematically evaluate the robustness of GNN-based social bot detection via both edge editing and node injection adversarial attacks under realistic constraints. BOCLOAK constructs a probability measure over spatio-temporal neighbor features and learns an optimal transport geometry that separates human and bot behaviors. It then decodes transport plans into sparse, plausible edge edits that evade detection while obeying real-world constraints. We evaluate BOCLOAK across three social bot datasets, five state-of-the-art bot detectors, three adversarial defenses, and compare it against four leading graph adversarial attack baselines. BOCLOAK achieves up to 80.13% higher attack success rates while using 99.80% less GPU memory under realistic real-world constraints. Most importantly, BOCLOAK shows that optimal transport provides a lightweight, principled framework for bridging the gap between adversarial attacks and real-world bot detection.

</details>


### [61] [Harvest: Opportunistic Peer-to-Peer GPU Caching for LLM Inference](https://arxiv.org/abs/2602.00328)
*Nikhil Gopal,Kostis Kaffes*

Main category: cs.LG

TL;DR: Harvest框架利用GPU间高速互连，将模型权重和KV缓存动态放置在空闲GPU内存中，作为临时缓存层，显著提升LLM推理吞吐量


<details>
  <summary>Details</summary>
Motivation: LLM推理越来越受GPU内存容量限制而非计算吞吐量，现有方法通过将模型状态和KV张量卸载到主机内存会因PCIe带宽有限导致显著延迟

Method: 提出Harvest框架，利用GPU间高速对等互连，将模型权重和KV缓存动态放置在未使用的GPU内存中，将其作为临时缓存层，在动态内存可用性下减少数据移动开销

Result: 通过使用Harvest加速两个广泛使用的推理组件（专家层权重和KV缓存条目）的检索，实现了超过2倍的吞吐量加速

Conclusion: Harvest框架通过利用空闲GPU内存作为缓存层，有效解决了LLM推理中的内存瓶颈问题，显著提升了推理性能

Abstract: Large Language Model (LLM) inference is increasingly constrained by GPU memory capacity rather than compute throughput, driven by growing model sizes and the linear growth of the key-value (KV) cache during autoregressive decoding. Existing approaches mitigate memory pressure by offloading model state and KV tensors to host memory, but incur substantial latency due to limited PCIe bandwidth. We present Harvest, an opportunistic GPU cache management framework that exploits high-bandwidth peer-to-peer GPU interconnects to dynamically place model weights and KV cache in unused GPU memory. Harvest treats peer GPU memory as a transient cache tier, preserving correctness while reducing data movement overhead under dynamic memory availability. We demonstrate significant throughput speedup of more than 2 times by using Harvest to accelerate the retrieval of two widely-used inference components: expert layer weights and KV cache entries.

</details>


### [62] [In-Run Data Shapley for Adam Optimizer](https://arxiv.org/abs/2602.00329)
*Meng Ding,Zeqing Zhang,Di Wang,Lijie Hu*

Main category: cs.LG

TL;DR: 提出Adam-Aware In-Run Data Shapley方法，解决传统SGD-based数据贡献度评估方法在Adam优化器下失效的问题，实现高保真度且高效的数据贡献度计算。


<details>
  <summary>Details</summary>
Motivation: 现有基于SGD的"In-Run"数据贡献度评估方法无法捕捉Adam等自适应优化器的复杂动态，导致在现代化训练流程中失效。数据贡献度评估本质上是优化器依赖的，需要针对特定优化器设计评估方法。

Method: 提出Adam-Aware In-Run Data Shapley方法：1) 通过固定状态假设重新定义效用函数以恢复可加性；2) 提出线性化幽灵近似技术，线性化方差依赖的缩放项，无需计算每个样本的梯度即可计算成对梯度点积。

Result: 方法在Adam优化器下实现了接近完美的边际贡献保真度（Pearson R > 0.99），同时保持了约95%的标准训练吞吐量。在数据贡献度下游任务中显著优于基于SGD的基线方法。

Conclusion: 数据贡献度评估必须考虑优化器特性，提出的Adam-aware方法有效解决了自适应优化器下的数据贡献度评估问题，为现代化机器学习训练流程提供了可靠的数据归属分析工具。

Abstract: Reliable data attribution is essential for mitigating bias and reducing computational waste in modern machine learning, with the Shapley value serving as the theoretical gold standard. While recent "In-Run" methods bypass the prohibitive cost of retraining by estimating contributions dynamically, they heavily rely on the linear structure of Stochastic Gradient Descent (SGD) and fail to capture the complex dynamics of adaptive optimizers like Adam. In this work, we demonstrate that data attribution is inherently optimizer-dependent: we show that SGD-based proxies diverge significantly from true contributions under Adam (Pearson $R \approx 0.11$), rendering them ineffective for modern training pipelines. To bridge this gap, we propose Adam-Aware In-Run Data Shapley. We derive a closed-form approximation that restores additivity by redefining utility under a fixed-state assumption and enable scalable computation via a novel Linearized Ghost Approximation. This technique linearizes the variance-dependent scaling term, allowing us to compute pairwise gradient dot-products without materializing per-sample gradients. Extensive experiments show that our method achieves near-perfect fidelity to ground-truth marginal contributions ($R > 0.99$) while retaining $\sim$95\% of standard training throughput. Furthermore, our Adam-aware attribution significantly outperforms SGD-based baselines in data attribution downstream tasks.

</details>


### [63] [Prototype-based Explainable Neural Networks with Channel-specific Reasoning for Geospatial Learning Tasks](https://arxiv.org/abs/2602.00331)
*Anushka Narayanan,Karianne J. Bergen*

Main category: cs.LG

TL;DR: 提出一种针对多通道地理空间数据的原型可解释AI方法，通过通道特定原型增强地理科学机器学习模型的透明度和可信度


<details>
  <summary>Details</summary>
Motivation: 现有基于原型的可解释AI方法主要针对标准RGB图像设计，不适用于地理科学中常见的多通道、变量特定的图像和栅格数据集，需要专门的方法来增强地理科学机器学习模型的透明度和可信度

Method: 开发针对多通道地理空间数据的原型可解释AI方法，每个通道代表不同的物理环境变量或光谱通道，模型能够识别来自多个不同训练样本的通道特定原型特征，展示这些特征如何单独或组合影响模型预测

Result: 在两个地理科学案例中验证方法：(1)使用多变量气候数据对Madden Julian振荡相位进行分类，(2)从多光谱卫星图像进行土地利用分类，方法在保持与标准神经网络相当性能的同时，提供局部和全局解释

Conclusion: 通过将通道原型明确纳入预测过程，该方法增强了地理科学学习任务中机器学习模型的透明度和可信度，为多通道地理空间数据提供了有效的可解释AI解决方案

Abstract: Explainable AI (XAI) is essential for understanding machine learning (ML) decision-making and ensuring model trustworthiness in scientific applications. Prototype-based XAI methods offer an intrinsically interpretable alternative to post-hoc approaches which often yield inconsistent explanations. Prototype-based XAI methods make predictions based on the similarity between inputs and learned prototypes that represent typical characteristics of target classes. However, existing prototype-based models are primarily designed for standard RGB image data and are not optimized for the distinct, variable-specific channels commonly found in geoscientific image and raster datasets. In this study, we develop a prototype-based XAI approach tailored for multi-channel geospatial data, where each channel represents a distinct physical environmental variable or spectral channel. Our approach enables the model to identify separate, channel-specific prototypical characteristics sourced from multiple distinct training examples that inform how these features individually and in combination influence model prediction while achieving comparable performance to standard neural networks. We demonstrate this method through two geoscientific case studies: (1) classification of Madden Julian Oscillation phases using multi-variable climate data and (2) land-use classification from multispectral satellite imagery. This approach produces both local (instance-level) and global (model-level) explanations for providing insights into feature-relevance across channels. By explicitly incorporating channel-prototypes into the prediction process, we discuss how this approach enhances the transparency and trustworthiness of ML models for geoscientific learning tasks.

</details>


### [64] [Efficient and accurate steering of Large Language Models through attention-guided feature learning](https://arxiv.org/abs/2602.00333)
*Parmida Davarmanesh,Ashia Wilson,Adityanarayanan Radhakrishnan*

Main category: cs.LG

TL;DR: 提出注意力引导的引导框架，解决LLM内部激活操纵的三个核心挑战，在512个语义概念基准上性能显著提升，几乎使成功引导概念数量翻倍。


<details>
  <summary>Details</summary>
Motivation: 现有LLM引导方法非常脆弱，概念的可引导性对特征提取的算法选择极其敏感，需要更鲁棒的引导框架来理解LLM中语义概念的存储方式并提升LLM能力。

Method: 引入注意力引导的引导框架，解决三个核心挑战：1) 自动选择相关token嵌入提取概念特征；2) 处理LLM激活中概念特征的异质性；3) 识别最相关的引导层。

Result: 在512个语义概念的引导基准测试中，框架显著优于先前最先进方法（几乎使成功引导概念数量翻倍），在不同架构和规模（高达700亿参数）的模型上均表现良好。

Conclusion: 该框架为开发高效、高度可扩展的行业级LLM微调算法开辟了新途径，并揭示了概念特定特征在LLM层间的分布规律。

Abstract: Steering, or direct manipulation of internal activations to guide LLM responses toward specific semantic concepts, is emerging as a promising avenue for both understanding how semantic concepts are stored within LLMs and advancing LLM capabilities. Yet, existing steering methods are remarkably brittle, with seemingly non-steerable concepts becoming completely steerable based on subtle algorithmic choices in how concept-related features are extracted. In this work, we introduce an attention-guided steering framework that overcomes three core challenges associated with steering: (1) automatic selection of relevant token embeddings for extracting concept-related features; (2) accounting for heterogeneity of concept-related features across LLM activations; and (3) identification of layers most relevant for steering. Across a steering benchmark of 512 semantic concepts, our framework substantially improved steering over previous state-of-the-art (nearly doubling the number of successfully steered concepts) across model architectures and sizes (up to 70 billion parameter models). Furthermore, we use our framework to shed light on the distribution of concept-specific features across LLM layers. Overall, our framework opens further avenues for developing efficient, highly-scalable fine-tuning algorithms for industry-scale LLMs.

</details>


### [65] [Adaptive Momentum and Nonlinear Damping for Neural Network Training](https://arxiv.org/abs/2602.00334)
*Aikaterini Karoni,Rajit Rajpal,Benedict Leimkuhler,Gabriel Stoltz*

Main category: cs.LG

TL;DR: 提出一种连续时间优化方案，通过每个模型参数的动能调节个体自适应动量系数，自动适应局部曲率以保持稳定性而不牺牲收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法（如mSGD）在大规模优化任务中可能不稳定或收敛缓慢，需要一种能自动适应局部曲率并保持稳定性的自适应动量机制。

Method: 提出连续时间优化方案，引入由每个模型参数动能调节的个体自适应动量系数，将自适应摩擦与立方阻尼联系起来，并通过在mSGD和Adam的连续动力学中添加立方阻尼项来构建具体优化方案。

Result: 在ViT、BERT和GPT2训练任务中，该方法表现出鲁棒性，匹配或优于Adam性能，特别是在mSGD通常表现不佳的任务上。理论分析证明了所提方案的指数收敛性。

Conclusion: 通过动能调节的自适应动量系数和立方阻尼机制，实现了稳定且快速的优化方案，在大规模深度学习任务中表现出优越性能，并有理论收敛保证。

Abstract: We propose a continuous-time scheme for large-scale optimization that introduces individual, adaptive momentum coefficients regulated by the kinetic energy of each model parameter. This approach automatically adjusts to local landscape curvature to maintain stability without sacrificing convergence speed. We demonstrate that our adaptive friction can be related to cubic damping, a suppression mechanism from structural dynamics. Furthermore, we introduce two specific optimization schemes by augmenting the continuous dynamics of mSGD and Adam with a cubic damping term. Empirically, our methods demonstrate robustness and match or outperform Adam on training ViT, BERT, and GPT2 tasks where mSGD typically struggles. We further provide theoretical results establishing the exponential convergence of the proposed schemes.

</details>


### [66] [Planning with Language and Generative Models: Toward General Reward-Guided Wireless Network Design](https://arxiv.org/abs/2602.00357)
*Chenyang Yuan,Xiaoyuan Cheng*

Main category: cs.LG

TL;DR: 该论文提出使用扩散模型作为生成推理方法，配合统一奖励函数来解决室内AP部署规划问题，相比传统LLM方法具有更好的可扩展性和性能。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络中，复杂的室内几何结构和信号传播使得智能AP部署具有挑战性。作者发现通用大语言模型虽然具备无线领域知识，但依赖外部验证器导致计算成本高且可扩展性有限。

Method: 提出基于扩散采样器的生成推理模型，使用统一奖励函数捕捉不同楼层平面图中的核心AP部署目标。扩散过程通过平滑和锐化奖励景观来逐步改进采样，而不是依赖迭代优化。

Result: 扩散采样器在替代生成方法中表现最佳，能够有效处理非凸和碎片化的目标函数。作者还创建了大规模真实世界室内AP部署数据集，需要超过5万CPU小时来训练通用奖励函数。

Conclusion: 基于扩散的生成推理配合统一奖励函数为室内AP部署规划提供了可扩展且领域无关的基础框架，在分布内和分布外泛化以及鲁棒性方面表现良好。

Abstract: Intelligent access point (AP) deployment remains challenging in next-generation wireless networks due to complex indoor geometries and signal propagation. We firstly benchmark general-purpose large language models (LLMs) as agentic optimizers for AP planning and find that, despite strong wireless domain knowledge, their dependence on external verifiers results in high computational costs and limited scalability. Motivated by these limitations, we study generative inference models guided by a unified reward function capturing core AP deployment objectives across diverse floorplans. We show that diffusion samplers consistently outperform alternative generative approaches. The diffusion process progressively improves sampling by smoothing and sharpening the reward landscape, rather than relying on iterative refinement, which is effective for non-convex and fragmented objectives. Finally, we introduce a large-scale real-world dataset for indoor AP deployment, requiring over $50k$ CPU hours to train general reward functions, and evaluate in- and out-of-distribution generalization and robustness. Our results suggest that diffusion-based generative inference with a unified reward function provides a scalable and domain-agnostic foundation for indoor AP deployment planning.

</details>


### [67] [Leveraging Textual-Cues for Enhancing Multimodal Sentiment Analysis by Object Recognition](https://arxiv.org/abs/2602.00360)
*Sumana Biswas,Karen Young,Josephine Griffith*

Main category: cs.LG

TL;DR: 本文提出TEMSA方法，通过提取图像中所有检测到的物体名称并与相关文本结合（称为TEMS），来提升多模态情感分析性能。实验表明，相比单独分析，TEMS能有效改善多模态数据的情感分析结果。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析面临文本和图像模态差异、情感模糊性以及上下文复杂性等挑战。本文旨在解决这些困难，探索图像和文本数据单独及组合分析的效果。

Method: 提出TEMSA方法：基于物体识别技术提取图像中所有检测到的物体名称，将其与相关文本结合形成TEMS（文本和图像数据组合）。在两个数据集上分别实验图像、文本数据单独分析以及TEMS组合分析。

Result: 实验结果显示，仅TEMS方法在使用所有物体名称时，相比单独分析能改善多模态数据的整体情感分析结果。其他单独分析方式未能带来类似提升。

Conclusion: TEMSA方法能有效结合图像和文本数据进行多模态情感分析，为解决模态差异和上下文复杂性提供了新思路，推动了该领域的发展。

Abstract: Multimodal sentiment analysis, which includes both image and text data, presents several challenges due to the dissimilarities in the modalities of text and image, the ambiguity of sentiment, and the complexities of contextual meaning. In this work, we experiment with finding the sentiments of image and text data, individually and in combination, on two datasets. Part of the approach introduces the novel `Textual-Cues for Enhancing Multimodal Sentiment Analysis' (TEMSA) based on object recognition methods to address the difficulties in multimodal sentiment analysis. Specifically, we extract the names of all objects detected in an image and combine them with associated text; we call this combination of text and image data TEMS. Our results demonstrate that only TEMS improves the results when considering all the object names for the overall sentiment of multimodal data compared to individual analysis. This research contributes to advancing multimodal sentiment analysis and offers insights into the efficacy of TEMSA in combining image and text data for multimodal sentiment analysis.

</details>


### [68] [Quantum Generator Kernels](https://arxiv.org/abs/2602.00361)
*Philipp Altmann,Maximilian Mansky,Maximilian Zorn,Jonas Stein,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: 提出量子生成核（QGKs），通过变分生成群（VGGs）构建可参数化的量子核，解决NISQ时代量子机器学习中大规模数据嵌入问题，在投影和分类任务上优于现有量子与经典核方法。


<details>
  <summary>Details</summary>
Motivation: 量子核方法理论上能将经典不可分的特征在量子空间中变得可分，但受限于NISQ硬件的约束，需要有效策略将大规模真实世界数据（如图像）压缩并嵌入到有限的量子设备或模拟器中。现有混合架构的中间嵌入过程固定，可能无法充分利用量子计算的全部潜力。

Method: 提出量子生成核（QGKs），采用基于生成器的方法，包含一组变分生成群（VGGs），将通用生成器合并为可参数化算子，确保对可用量子空间的可扩展覆盖。通过训练权重向量来参数化VGGs在当前数据上下文中的投影，优化核与目标领域的对齐。

Result: 实证结果表明，QGKs在投影和分类能力上优于最先进的量子与经典核方法，展示了其作为各种量子机器学习应用的通用框架的潜力。

Conclusion: 量子生成核（QGKs）通过变分生成群提供了一种可扩展的量子核方法，解决了NISQ时代量子机器学习中的数据嵌入挑战，在性能上超越现有方法，为量子机器学习应用提供了通用框架。

Abstract: Quantum kernel methods offer significant theoretical benefits by rendering classically inseparable features separable in quantum space. Yet, the practical application of Quantum Machine Learning (QML), currently constrained by the limitations of Noisy Intermediate-Scale Quantum (NISQ) hardware, necessitates effective strategies to compress and embed large-scale real-world data like images into the constrained capacities of existing quantum devices or simulators. To this end, we propose Quantum Generator Kernels (QGKs), a generator-based approach to quantum kernels, comprising a set of Variational Generator Groups (VGGs) that merge universal generators into a parameterizable operator, ensuring scalable coverage of the available quantum space. Thereby, we address shortcomings of current leading strategies employing hybrid architectures, which might prevent exploiting quantum computing's full potential due to fixed intermediate embedding processes. To optimize the kernel alignment to the target domain, we train a weight vector to parameterize the projection of the VGGs in the current data context. Our empirical results demonstrate superior projection and classification capabilities of the QGK compared to state-of-the-art quantum and classical kernel approaches and show its potential to serve as a versatile framework for various QML applications.

</details>


### [69] [Post-Training Probability Manifold Correction via Structured SVD Pruning and Self-Referential Distillation](https://arxiv.org/abs/2602.00372)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: SparseKD是一种后训练压缩方法，结合结构化SVD剪枝和自参考知识蒸馏，无需外部教师模型，通过模型自我匹配压缩前的概率分布来恢复质量，实现15-65%参数减少。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型部署成本高昂，需要有效的压缩方法。现有方法通常需要外部教师模型或架构修改，不够实用。

Method: 结合结构化SVD剪枝和自参考知识蒸馏：1）使用SVD进行结构化剪枝；2）采用自参考蒸馏，让压缩后模型匹配压缩前自身的概率分布；3）固定校准数据集和相同目标函数。

Result: 1）仅自参考蒸馏就能使模型质量相对原始检查点提升39%；2）结合剪枝可实现15-65%参数减少且质量可接受；3）速度提升来自前馈层的密集矩阵乘法减少，注意力层不变；4）在0.6B和3.8B参数模型上验证，重现性高。

Conclusion: SparseKD提供了一种实用、可立即部署的模型压缩方案，无需外部教师模型、架构修改或定制推理内核，与注意力优化互补。

Abstract: Large language models are expensive to deploy. We introduce Sparse Knowledge Distillation (SparseKD), a post-training method that compresses transformer models by combining structured SVD pruning with self-referential knowledge distillation. The key insight is simple: instead of using an external teacher, the model teaches itself by matching its own probability distribution from before compression. This self-referential setup enables surprisingly strong quality recovery after aggressive pruning.
  Our experiments reveal an unexpected finding: self-referential distillation alone, applied post-training under an identical objective and fixed calibration dataset, improves model quality by 39% relative to the original converged checkpoint. When combined with structured pruning, SparseKD achieves 15-65% parameter reduction with acceptable quality trade-offs. Kernel profiling shows that speedups arise entirely from reduced dense matrix multiplication in feed-forward layers while attention remains unchanged, making this approach complementary to attention optimizations.
  We validate across two model families (0.6B and 3.8B parameters) with multi-seed experiments confirming high reproducibility. SparseKD requires no external super-teacher, no architectural changes, and no custom inference kernels, making it immediately deployable with existing infrastructure.

</details>


### [70] [MATRIX: A Multimodal Benchmark and Post-Training Framework for Materials Science](https://arxiv.org/abs/2602.00376)
*Delia McGrath,Curtis Chong,Rohil Kulkarni,Gerbrand Ceder,Adeesh Kolluru*

Main category: cs.LG

TL;DR: MATRIX是一个材料科学多模态基准测试，用于评估视觉实验数据如何提升科学推理能力，研究发现视觉监督能显著改善实验解释和文本推理任务。


<details>
  <summary>Details</summary>
Motivation: 现有基准难以评估视觉实验数据在训练后阶段是否能提升基于机制的推理能力，需要专门的多模态基准来研究视觉基础对科学推理的影响。

Method: 创建MATRIX多模态基准，包含材料科学基础理论、研究级推理和多种表征模态的实验数据解释。通过对比纯文本训练和包含配对实验图像的多模态训练，隔离视觉基础的影响。

Result: 视觉监督使实验解释能力提升10-25%，文本科学推理任务提升5-16%。改进依赖于训练中正确的图像-文本对齐，体现了跨模态表示迁移。在ScienceQA和PubMedQA上也观察到一致改进。

Conclusion: 多模态训练能显著提升科学推理能力，即使使用少量多模态数据。MATRIX基准为评估视觉基础对科学推理的影响提供了有效工具，相关数据集和模型已开源。

Abstract: Scientific reasoning in materials science requires integrating multimodal experimental evidence with underlying physical theory. Existing benchmarks make it difficult to assess whether incorporating visual experimental data during post-training improves mechanism-grounded explanation reasoning beyond text-only supervision. We introduce MATRIX, a multimodal benchmark for materials science reasoning that evaluates foundational theory, research-level reasoning, and the interpretation of real experimental artifacts across multiple characterization modalities. Using MATRIX as a controlled diagnostic, we isolate the effect of visual grounding by comparing post-training on structured materials science text alone with post-training that incorporates paired experimental images. Despite using relatively small amounts of multimodal data, visual supervision improves experimental interpretation by 10-25% and yields 5-16% gains on text-only scientific reasoning tasks. Our results demonstrate that these improvements rely on correct image-text alignment during post-training, highlighting cross-modal representational transfer. We also observe consistent improvements on ScienceQA and PubMedQA, demonstrating that the benefits of structured multimodal post-training extend beyond materials science. The MATRIX dataset is available at https://huggingface.co/datasets/radical-ai/MATRIX and the model at https://huggingface.co/radical-ai/MATRIX-PT.

</details>


### [71] [RePaint-Enhanced Conditional Diffusion Model for Parametric Engineering Designs under Performance and Parameter Constraints](https://arxiv.org/abs/2602.00384)
*Ke Wang,Nguyen Gia Hien Vu,Yifan Tang,Mostafa Rahmani Dehaghani,G. Gary Wang*

Main category: cs.LG

TL;DR: 提出RePaint增强框架，集成预训练的性能引导DDPM模型，用于工程设计中满足性能和参数约束的生成任务，无需重新训练模型即可基于部分参考设计生成缺失组件。


<details>
  <summary>Details</summary>
Motivation: 传统基于DDPM的方法无法同时满足性能和参数约束下的部分设计生成需求，需要一种无需重新训练、能高效可控地生成满足约束条件的设计方案的方法。

Method: 采用RePaint增强框架，集成预训练的性能引导DDPM模型，在推理过程中应用基于掩码的重采样技术，实现对部分设计的可控重绘，同时满足性能和参数约束。

Result: 在参数化船体设计和翼型设计两个代表性问题上验证，方法能够基于部分参考设计生成具有预期性能的新设计，精度达到或超过预训练模型，同时通过固定部分设计实现可控创新。

Conclusion: 该方法为工程应用中的参数约束感知生成设计提供了一种高效、无需训练的新解决方案，能够在满足约束条件下实现可控的设计创新。

Abstract: This paper presents a RePaint-enhanced framework that integrates a pre-trained performance-guided denoising diffusion probabilistic model (DDPM) for performance- and parameter-constraint engineering design generation. The proposed method enables the generation of missing design components based on a partial reference design while satisfying performance constraints, without retraining the underlying model. By applying mask-based resampling during inference process, RePaint allows efficient and controllable repainting of partial designs under both performance and parameter constraints, which is not supported by conventional DDPM-base methods. The framework is evaluated on two representative design problems, parametric ship hull design and airfoil design, demonstrating its ability to generate novel designs with expected performance based on a partial reference design. Results show that the method achieves accuracy comparable to or better than pre-trained models while enabling controlled novelty through fixing partial designs. Overall, the proposed approach provides an efficient, training-free solution for parameter-constraint-aware generative design in engineering applications.

</details>


### [72] [A Fragile Guardrail: Diffusion LLM's Safety Blessing and Its Failure Mode](https://arxiv.org/abs/2602.00388)
*Zeyuan He,Yupeng Chen,Lang Lin,Yihan Wang,Shenxu Chang,Eric Sommerlade,Philip Torr,Junchi Yu,Adel Bibi,Jialin Yu*

Main category: cs.LG

TL;DR: 扩散大语言模型（D-LLMs）相比自回归模型具有内在安全优势，能抵抗传统越狱攻击，但存在"上下文嵌套"漏洞，可将有害请求嵌入良性结构绕过防护。


<details>
  <summary>Details</summary>
Motivation: 研究扩散大语言模型（D-LLMs）相比自回归模型（AR-LLMs）的安全特性，探索其内在的越狱攻击抵抗能力及其局限性。

Method: 分析D-LLMs的扩散轨迹机制，发现其逐步减少效应能抑制不安全生成；同时识别出"上下文嵌套"攻击方法，将有害请求嵌入良性结构来绕过防护机制。

Result: D-LLMs确实具有内在安全优势，但"上下文嵌套"攻击能有效绕过其防护，在多个模型和基准测试中达到最先进的攻击成功率，首次成功越狱Gemini Diffusion。

Conclusion: D-LLMs的安全优势源于扩散轨迹的逐步减少效应，但存在"上下文嵌套"这一关键漏洞，需要早期红队测试来识别和修复此类安全问题。

Abstract: Diffusion large language models (D-LLMs) offer an alternative to autoregressive LLMs (AR-LLMs) and have demonstrated advantages in generation efficiency. Beyond the utility benefits, we argue that D-LLMs exhibit a previously underexplored safety blessing: their diffusion-style generation confers intrinsic robustness against jailbreak attacks originally designed for AR-LLMs. In this work, we provide an initial analysis of the underlying mechanism, showing that the diffusion trajectory induces a stepwise reduction effect that progressively suppresses unsafe generations. This robustness, however, is not absolute. We identify a simple yet effective failure mode, termed context nesting, where harmful requests are embedded within structured benign contexts, effectively bypassing the stepwise reduction mechanism. Empirically, we show that this simple strategy is sufficient to bypass D-LLMs' safety blessing, achieving state-of-the-art attack success rates across models and benchmarks. Most notably, it enables the first successful jailbreak of Gemini Diffusion, to our knowledge, exposing a critical vulnerability in commercial D-LLMs. Together, our results characterize both the origins and the limits of D-LLMs' safety blessing, constituting an early-stage red-teaming of D-LLMs.

</details>


### [73] [Localized, High-resolution Geographic Representations with Slepian Functions](https://arxiv.org/abs/2602.00392)
*Arjun Rao,Ruth Crasto,Tessa Ooms,David Rolnick,Konstantin Klemmer,Marc Rußwurm*

Main category: cs.LG

TL;DR: 提出基于球面Slepian函数的地理位置编码器，能在感兴趣区域集中表示能力，并扩展到高分辨率，同时保持极地安全和球面距离保持等特性。


<details>
  <summary>Details</summary>
Motivation: 地理数据本质上是局部的（如疾病爆发集中在人口中心、生态模式沿海岸线出现），但现有机器学习模型的地理位置编码器在全球范围内均匀分布表示能力，难以满足局部应用的高分辨率需求。

Method: 提出基于球面Slepian函数的地理位置编码器，能在感兴趣区域集中表示能力；对于需要全局上下文的情况，提出混合Slepian-球谐函数编码器，有效平衡局部-全局性能。

Result: 在分类、回归和图像增强预测等五个任务中，Slepian编码优于基线方法，并在多种神经网络架构中保持性能优势。

Conclusion: Slepian编码器为地理数据建模提供了有效的解决方案，能够处理局部高分辨率需求，同时保持全局上下文，具有实际应用价值。

Abstract: Geographic data is fundamentally local. Disease outbreaks cluster in population centers, ecological patterns emerge along coastlines, and economic activity concentrates within country borders. Machine learning models that encode geographic location, however, distribute representational capacity uniformly across the globe, struggling at the fine-grained resolutions that localized applications require. We propose a geographic location encoder built from spherical Slepian functions that concentrate representational capacity inside a region-of-interest and scale to high resolutions without extensive computational demands. For settings requiring global context, we present a hybrid Slepian-Spherical Harmonic encoder that efficiently bridges the tradeoff between local-global performance, while retaining desirable properties such as pole-safety and spherical-surface-distance preservation. Across five tasks spanning classification, regression, and image-augmented prediction, Slepian encodings outperform baselines and retain performance advantages across a wide range of neural network architectures.

</details>


### [74] [Fast Forward: Accelerating LLM Prefill with Predictive FFN Sparsity](https://arxiv.org/abs/2602.00397)
*Aayush Gautam,Mukul Gagrani,Junyoung Park,Mingu Lee,Chiris Lott,Narasimha Reddy*

Main category: cs.LG

TL;DR: FastForward：一种预测性稀疏框架，通过块级、上下文感知的FFN稀疏化加速LLM预填充阶段，在保持精度的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: LLM推理的预填充阶段是长上下文工作负载的主要计算瓶颈。在短到中等上下文长度下，前馈网络(FFN)占用了大部分计算成本。现有的FFN稀疏化方法主要针对自回归解码设计，无法充分利用预填充阶段的并行性，且常常导致精度下降。

Method: FastForward包含三个核心组件：(1) 轻量级专家预测器，用于按块选择高重要性神经元；(2) 误差补偿网络，用于纠正稀疏化引起的误差；(3) 层间稀疏调度器，基于token混合重要性分配计算资源。

Result: 在LLaMA和Qwen模型（最大8B参数）上，FastForward在50% FFN稀疏度下实现了高达1.45倍的计算加速，在LongBench基准测试中相比密集基线的精度损失小于6%，显著降低了首token生成时间。

Conclusion: FastForward有效解决了LLM预填充阶段的计算瓶颈，通过上下文感知的FFN稀疏化在保持模型精度的同时显著提升推理效率，特别适用于资源受限硬件上的长上下文LLM推理。

Abstract: The prefill stage of large language model (LLM) inference is a key computational bottleneck for long-context workloads. At short-to-moderate context lengths (1K--16K tokens), Feed-Forward Networks (FFNs) dominate this cost, accounting for most of the total FLOPs. Existing FFN sparsification methods, designed for autoregressive decoding, fail to exploit the prefill stage's parallelism and often degrade accuracy. To address this, we introduce FastForward, a predictive sparsity framework that accelerates LLM prefill through block-wise, context-aware FFN sparsity. FastForward combines (1) a lightweight expert predictor to select high-importance neurons per block, (2) an error compensation network to correct sparsity-induced errors, and (3) a layer-wise sparsity scheduler to allocate compute based on token-mixing importance. Across LLaMA and Qwen models up to 8B parameters, FastForward delivers up to 1.45$\times$ compute-bound speedup at 50% FFN sparsity with $<$ 6% accuracy loss compared to the dense baseline on LongBench, substantially reducing Time-to-First-Token (TTFT) for efficient, long-context LLM inference on constrained hardware.

</details>


### [75] [MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers](https://arxiv.org/abs/2602.00398)
*Ajay Jaiswal,Lauren Hannah,Han-Byul Kim,Duc Hoang,Arnav Kundu,Mehrdad Farajtabar,Minsik Cho*

Main category: cs.LG

TL;DR: MemoryLLM：将Transformer中的前馈网络（FFN）与自注意力机制解耦，将其视为上下文无关的令牌级神经检索内存，通过独立训练FFN实现预计算令牌查找表，提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer组件在LLM中的运作机制对AI技术进步至关重要。当前前馈网络（FFN）的可解释性存在挑战，需要新的方法来解耦FFN与自注意力机制，以便更深入地研究FFN作为神经检索内存的功能。

Method: 提出MemoryLLM方法：1）将FFN与自注意力解耦，将其视为上下文无关的令牌级神经检索内存；2）使用令牌嵌入独立训练FFN，使其成为可预计算的令牌查找表（ToLs）；3）引入Flex-MemoryLLM作为传统Transformer与MemoryLLM之间的中间架构。

Result: MemoryLLM实现了上下文无关的FFN，可预计算为令牌查找表，支持按需在VRAM和存储间传输，提升推理效率。Flex-MemoryLLM弥补了使用上下文无关令牌嵌入训练FFN导致的性能差距。

Conclusion: MemoryLLM为Transformer FFN提供了新的可解释性框架，将其视为神经检索内存，同时通过预计算机制提升推理效率。Flex-MemoryLLM在保持性能的同时实现了架构灵活性，为LLM组件研究提供了新视角。

Abstract: Understanding how transformer components operate in LLMs is important, as it is at the core of recent technological advances in artificial intelligence. In this work, we revisit the challenges associated with interpretability of feed-forward modules (FFNs) and propose MemoryLLM, which aims to decouple FFNs from self-attention and enables us to study the decoupled FFNs as context-free token-wise neural retrieval memory. In detail, we investigate how input tokens access memory locations within FFN parameters and the importance of FFN memory across different downstream tasks. MemoryLLM achieves context-free FFNs by training them in isolation from self-attention directly using the token embeddings. This approach allows FFNs to be pre-computed as token-wise lookups (ToLs), enabling on-demand transfer between VRAM and storage, additionally enhancing inference efficiency. We also introduce Flex-MemoryLLM, positioning it between a conventional transformer design and MemoryLLM. This architecture bridges the performance gap caused by training FFNs with context-free token-wise embeddings.

</details>


### [76] [DROGO: Default Representation Objective via Graph Optimization in Reinforcement Learning](https://arxiv.org/abs/2602.00403)
*Hon Tik Tse,Marlos C. Machado*

Main category: cs.LG

TL;DR: 提出直接近似默认表示主特征向量的目标函数，避免先计算矩阵再分解的高计算成本，应用于奖励塑形等任务


<details>
  <summary>Details</summary>
Motivation: 传统方法需要先近似默认表示矩阵再进行特征分解，计算成本高且难以扩展到高维空间，需要更高效的直接近似方法

Method: 推导出直接使用神经网络近似默认表示主特征向量的目标函数，避免先计算矩阵再分解的两步过程

Result: 在多个环境中实证验证了该目标函数的有效性，并将学习到的特征向量成功应用于奖励塑形任务

Conclusion: 提出的直接近似方法计算效率更高，可扩展到高维空间，为默认表示主特征向量的应用提供了更实用的解决方案

Abstract: In computational reinforcement learning, the default representation (DR) and its principal eigenvector have been shown to be effective for a wide variety of applications, including reward shaping, count-based exploration, option discovery, and transfer. However, in prior investigations, the eigenvectors of the DR were computed by first approximating the DR matrix, and then performing an eigendecomposition. This procedure is computationally expensive and does not scale to high-dimensional spaces. In this paper, we derive an objective for directly approximating the principal eigenvector of the DR with a neural network. We empirically demonstrate the effectiveness of the objective in a number of environments, and apply the learned eigenvectors for reward shaping.

</details>


### [77] [Fed-Listing: Federated Label Distribution Inference in Graph Neural Networks](https://arxiv.org/abs/2602.00407)
*Suprim Nakarmi,Junggab Son,Yue Zhao,Zuobin Xiong*

Main category: cs.LG

TL;DR: 提出Fed-Listing攻击方法，利用联邦图神经网络训练中的梯度信息推断目标客户端的私有标签分布统计信息，无需原始数据或节点特征。


<details>
  <summary>Details</summary>
Motivation: 联邦图神经网络虽然保护了用户隐私，但共享的模型更新（特别是梯度）仍可能无意中泄露敏感信息。现有研究主要关注传统联邦学习的隐私推理攻击，而图神经网络中的标签分布推断问题尚未得到充分探索。

Method: 提出Fed-Listing攻击方法：1) 仅利用训练过程中交换的最终层梯度；2) 使用辅助影子数据集生成多样化的标签划分策略，模拟不同客户端分布；3) 在这些模拟数据上训练攻击模型以推断目标客户端的标签统计信息。

Result: 在四个基准数据集和三种GNN架构上的实验表明，Fed-Listing显著优于现有基线方法（包括随机猜测和Decaf），即使在具有挑战性的非独立同分布场景下也表现优异。防御机制几乎无法降低攻击性能，除非严重损害模型效用。

Conclusion: Fed-Listing攻击方法能够有效推断联邦图神经网络中目标客户端的私有标签分布，揭示了当前联邦图神经网络在隐私保护方面仍存在严重漏洞，需要更强大的防御机制来保护用户隐私。

Abstract: Graph Neural Networks (GNNs) have been intensively studied for their expressive representation and learning performance on graph-structured data, enabling effective modeling of complex relational dependencies among nodes and edges in various domains. However, the standalone GNNs can unleash threat surfaces and privacy implications, as some sensitive graph-structured data is collected and processed in a centralized setting. To solve this issue, Federated Graph Neural Networks (FedGNNs) are proposed to facilitate collaborative learning over decentralized local graph data, aiming to preserve user privacy. Yet, emerging research indicates that even in these settings, shared model updates, particularly gradients, can unintentionally leak sensitive information of local users. Numerous privacy inference attacks have been explored in traditional federated learning and extended to graph settings, but the problem of label distribution inference in FedGNNs remains largely underexplored. In this work, we introduce Fed-Listing (Federated Label Distribution Inference in GNNs), a novel gradient-based attack designed to infer the private label statistics of target clients in FedGNNs without access to raw data or node features. Fed-Listing only leverages the final-layer gradients exchanged during training to uncover statistical patterns that reveal class proportions in a stealthy manner. An auxiliary shadow dataset is used to generate diverse label partitioning strategies, simulating various client distributions, on which the attack model is obtained. Extensive experiments on four benchmark datasets and three GNN architectures show that Fed-Listing significantly outperforms existing baselines, including random guessing and Decaf, even under challenging non-i.i.d. scenarios. Moreover, applying defense mechanisms can barely reduce our attack performance, unless the model's utility is severely degraded.

</details>


### [78] [Variational Approach for Job Shop Scheduling](https://arxiv.org/abs/2602.00408)
*Seung Heon Oh,Jiwon Baek,Ki Young Cho,Hee Chang Yoon,Jong Hun Woo*

Main category: cs.LG

TL;DR: VG2S框架首次将变分推理引入作业车间调度问题，通过变分图编码器学习调度实例的鲁棒结构表示，显著提升训练稳定性和零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习方法在作业车间调度问题中面临训练非平稳性和泛化能力有限的问题，因为它们同时优化表示学习和策略执行。需要一种能够解耦这两个过程的方法来提高训练稳定性和泛化性能。

Method: 提出变分图到调度器框架，首次将变分推理引入JSSP领域，基于证据下界和最大熵强化学习推导概率目标。通过变分图编码器学习调度实例的结构表示，将表示学习与策略优化数学解耦。

Result: VG2S在训练稳定性和超参数变化鲁棒性方面显著提升，在DMU和SWV等大规模挑战性基准实例上表现出优于现有DRL基线和传统调度规则的零样本泛化能力。

Conclusion: 通过变分推理解耦表示学习和策略优化是解决JSSP中DRL方法非平稳性和泛化限制的有效途径，VG2S框架为制造调度提供了更稳定和泛化能力强的解决方案。

Abstract: This paper proposes a novel Variational Graph-to-Scheduler (VG2S) framework for solving the Job Shop Scheduling Problem (JSSP), a critical task in manufacturing that directly impacts operational efficiency and resource utilization. Conventional Deep Reinforcement Learning (DRL) approaches often face challenges such as non-stationarity during training and limited generalization to unseen problem instances because they optimize representation learning and policy execution simultaneously. To address these issues, we introduce variational inference to the JSSP domain for the first time and derive a probabilistic objective based on the Evidence of Lower Bound (ELBO) with maximum entropy reinforcement learning. By mathematically decoupling representation learning from policy optimization, the VG2S framework enables the agent to learn robust structural representations of scheduling instances through a variational graph encoder. This approach significantly enhances training stability and robustness against hyperparameter variations. Extensive experiments demonstrate that the proposed method exhibits superior zero-shot generalization compared with state-of-the-art DRL baselines and traditional dispatching rules, particularly on large-scale and challenging benchmark instances such as DMU and SWV.

</details>


### [79] [Robustness of AutoML on Dirty Categorical Data](https://arxiv.org/abs/2602.00412)
*Marcos L. P. Bueno,Joaquin Vanschoren*

Main category: cs.LG

TL;DR: 该论文提出了一种将分类数据转换为数值数据的管道，使AutoML能够处理经过高级编码方案转换的分类数据，并在脏分类数据集上评估了AutoML方法的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前AutoML方法在处理脏分类数据集时表现不佳，这些数据集通常具有高基数分类特征，而现有的形态学编码器虽然能提升ML模型性能，但在AutoML中的应用效果尚不明确。

Method: 提出一个将分类数据转换为数值数据的管道，使用更先进的编码方案，然后在脏数据集上对AutoML方法进行基准测试，比较预测性能差异，并分析AutoML构建的ML管道。

Result: 通过提出的管道，AutoML方法在脏分类数据集上的预测性能得到提升，同时通过分析AutoML构建的管道获得了超出最佳模型的洞察。

Conclusion: 提出的管道能够有效提升AutoML处理脏分类数据的能力，为AutoML方法在处理此类数据时提供了更好的解决方案，并提供了对AutoML内部工作流程的深入理解。

Abstract: The goal of automated machine learning (AutoML) is to reduce trial and error when doing machine learning (ML). Although AutoML methods for classification are able to deal with data imperfections, such as outliers, multiple scales and missing data, their behavior is less known on dirty categorical datasets. These datasets often have several categorical features with high cardinality arising from issues such as lack of curation and automated collection. Recent research has shown that ML models can benefit from morphological encoders for dirty categorical data, leading to significantly superior predictive performance. However the effects of using such encoders in AutoML methods are not known at the moment. In this paper, we propose a pipeline that transforms categorical data into numerical data so that an AutoML can handle categorical data transformed by more advanced encoding schemes. We benchmark the current robustness of AutoML methods on a set of dirty datasets and compare it with the proposed pipeline. This allows us to get insight on differences in predictive performance. We also look at the ML pipelines built by AutoMLs in order to gain insight beyond the best model as typically returned by these methods.

</details>


### [80] [Federated-inspired Single-cell Batch Integration in Latent Space](https://arxiv.org/abs/2602.00423)
*Quang-Huy Nguyen,Zongliang Yue,Hao Chen,Wei-Shinn Ku,Jiaqi Wang*

Main category: cs.LG

TL;DR: scBatchProx：一种基于联邦学习原理的后优化方法，用于改进单细胞RNA测序数据中的批次效应校正，无需原始表达数据或集中式优化


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序产生大量高维数据，但跨实验的数据积累会引入批次效应，掩盖真实的生物信号。现有批次校正方法要么校正不足，要么需要在完整数据集上集中重新训练，限制了它们在分布式和持续演化的单细胞数据环境中的应用。

Method: scBatchProx是一种后优化方法，受联邦学习原理启发，用于优化任意上游方法产生的细胞级嵌入。将每个批次视为客户端，在近端正则化下学习批次条件适配器，直接在潜在空间中校正批次结构，无需原始表达数据或集中式优化。

Result: 实验表明，scBatchProx在整体嵌入质量上持续带来约3-8%的相对提升，在90%的数据-方法对中改善了批次校正，在85%的数据-方法对中改善了生物保守性。

Conclusion: 这项工作是在动态单细胞数据系统中实际改进学习表示的一步，提供了一种轻量级、可部署的批次校正方法。

Abstract: Advances in single-cell RNA sequencing enable the rapid generation of massive, high-dimensional datasets, yet the accumulation of data across experiments introduces batch effects that obscure true biological signals. Existing batch correction approaches either insufficiently correct batch effects or require centralized retraining on the complete dataset, limiting their applicability in distributed and continually evolving single-cell data settings. We introduce scBatchProx, a post-hoc optimization method inspired by federated learning principles for refining cell-level embeddings produced by arbitrary upstream methods. Treating each batch as a client, scBatchProx learns batch-conditioned adapters under proximal regularization, correcting batch structure directly in latent space without requiring raw expression data or centralized optimization. The method is lightweight and deployable, optimizing batch-specific adapter parameters only. Extensive experiments show that scBatchProx consistently yields relative gains of approximately 3-8% in overall embedding quality, with batch correction and biological conservation improving in 90% and 85% of data-method pairs, respectively. We envision this work as a step toward the practical refinement of learned representations in dynamic single-cell data systems.

</details>


### [81] [Open Materials Generation with Inference-Time Reinforcement Learning](https://arxiv.org/abs/2602.00424)
*Philipp Hoellmer,Stefano Martiniani*

Main category: cs.LG

TL;DR: OMatG-IRL：一种基于策略梯度强化学习的框架，可直接在学习的速度场上操作，无需显式计算分数，用于晶体结构预测的推理时强化学习。


<details>
  <summary>Details</summary>
Motivation: 连续时间生成模型可用于晶体材料设计，但难以将显式目标属性纳入生成过程。策略梯度强化学习能对齐生成模型与下游目标，但通常需要访问分数，这阻碍了其在仅学习速度场的流模型中的应用。

Method: 提出OMatG-IRL框架，直接在学习的速度场上进行策略梯度强化学习，无需显式计算分数。通过随机扰动底层生成动态，保持预训练生成模型的基线性能，同时在推理时实现探索和策略梯度估计。

Result: 该方法首次将强化学习应用于晶体结构预测，能有效强化基于能量的目标，同时通过组成条件保持多样性，性能与基于分数的强化学习方法相当。还能学习时间依赖的速度退火调度，实现采样效率数量级提升和生成时间大幅减少。

Conclusion: OMatG-IRL为流式生成模型提供了有效的推理时强化学习框架，无需分数计算，在晶体结构预测中实现了高效采样和性能提升，为材料设计开辟了新途径。

Abstract: Continuous-time generative models for crystalline materials enable inverse materials design by learning to predict stable crystal structures, but incorporating explicit target properties into the generative process remains challenging. Policy-gradient reinforcement learning (RL) provides a principled mechanism for aligning generative models with downstream objectives but typically requires access to the score, which has prevented its application to flow-based models that learn only velocity fields. We introduce Open Materials Generation with Inference-time Reinforcement Learning (OMatG-IRL), a policy-gradient RL framework that operates directly on the learned velocity fields and eliminates the need for the explicit computation of the score. OMatG-IRL leverages stochastic perturbations of the underlying generation dynamics preserving the baseline performance of the pretrained generative model while enabling exploration and policy-gradient estimation at inference time. Using OMatG-IRL, we present the first application of RL to crystal structure prediction (CSP). Our method enables effective reinforcement of an energy-based objective while preserving diversity through composition conditioning, and it achieves performance competitive with score-based RL approaches. Finally, we show that OMatG-IRL can learn time-dependent velocity-annealing schedules, enabling accurate CSP with order-of-magnitude improvements in sampling efficiency and, correspondingly, reduction in generation time.

</details>


### [82] [LLMs as High-Dimensional Nonlinear Autoregressive Models with Attention: Training, Alignment and Inference](https://arxiv.org/abs/2602.00426)
*Vikram Krishnamurthy*

Main category: cs.LG

TL;DR: 该论文提供了一个关于大语言模型的简明数学参考框架，将LLMs形式化为具有注意力依赖的高维非线性自回归模型，涵盖预训练、对齐方法和推理生成。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer架构的大语言模型通常通过架构组件和训练过程的描述来介绍，这模糊了其底层计算结构。研究者需要一个明确的、方程级别的LLM训练、对齐和生成描述。

Method: 将LLMs形式化为高维非线性自回归模型，其中自注意力自然表现为重复的双线性-softmax-线性组合。框架包含：1）通过下一词预测进行预训练；2）对齐方法（RLHF、DPO、RSFT、RLVR）；3）推理时的自回归生成。

Result: 该数学框架能够对对齐诱导行为（如谄媚）、推理时现象（如幻觉、上下文学习、思维链提示、检索增强生成）以及持续学习等扩展进行原则性分析。

Conclusion: 该公式化方法为解释和进一步的理论发展提供了一个简洁的参考框架，使研究者能够更清晰地理解LLM的计算本质和数学结构。

Abstract: Large language models (LLMs) based on transformer architectures are typically described through collections of architectural components and training procedures, obscuring their underlying computational structure. This review article provides a concise mathematical reference for researchers seeking an explicit, equation-level description of LLM training, alignment, and generation. We formulate LLMs as high-dimensional nonlinear autoregressive models with attention-based dependencies. The framework encompasses pretraining via next-token prediction, alignment methods such as reinforcement learning from human feedback (RLHF), direct preference optimization (DPO), rejection sampling fine-tuning (RSFT), and reinforcement learning from verifiable rewards (RLVR), as well as autoregressive generation during inference. Self-attention emerges naturally as a repeated bilinear--softmax--linear composition, yielding highly expressive sequence models. This formulation enables principled analysis of alignment-induced behaviors (including sycophancy), inference-time phenomena (such as hallucination, in-context learning, chain-of-thought prompting, and retrieval-augmented generation), and extensions like continual learning, while serving as a concise reference for interpretation and further theoretical development.

</details>


### [83] [Towards Building Non-Fine-Tunable Foundation Models](https://arxiv.org/abs/2602.00446)
*Ziyao Wang,Nizhang Li,Pingzhi Li,Guoheng Sun,Tianlong Chen,Ang Li*

Main category: cs.LG

TL;DR: 提出Private Mask Pre-Training (PMP)框架，通过预训练时将表征学习集中在稀疏子网络中，并保密该子网络的二进制掩码，使未经授权的微调难以有效适应，从而保护开源基础模型免受滥用。


<details>
  <summary>Details</summary>
Motivation: 开源基础模型虽然促进了广泛重用，但也使模型训练者面临未经授权的下游微调带来的经济和安全风险。需要一种方法让模型在保持可用性的同时，限制未经授权的微调效果。

Method: 提出Private Mask Pre-Training (PMP)预训练框架：1) 在训练早期识别稀疏子网络，将表征学习集中在该子网络中；2) 保密定义该子网络的二进制掩码，只发布最终的密集权重；3) 未经授权的微调由于无法访问掩码，会更新与预训练子空间不匹配的参数，导致微调目标与预训练几何结构不匹配。

Result: 理论分析表明这种不匹配会破坏基于梯度的适应过程并限制微调收益。在大语言模型上的实证结果显示，PMP能保持基础模型性能，同时在广泛的下游任务中持续降低未经授权微调的效果，且非微调性强度可通过掩码比例控制。

Conclusion: PMP框架为开源基础模型提供了一种有效的保护机制，通过控制表征学习的稀疏子网络并保密掩码，实现了模型在保持可用性的同时限制未经授权微调的效果，平衡了开放共享与风险控制的需求。

Abstract: Open-sourcing foundation models (FMs) enables broad reuse but also exposes model trainers to economic and safety risks from unrestricted downstream fine-tuning. We address this problem by building non-fine-tunable foundation models: models that remain broadly usable in their released form while yielding limited adaptation gains under task-agnostic unauthorized fine-tuning. We propose Private Mask Pre-Training (PMP), a pre-training framework that concentrates representation learning into a sparse subnetwork identified early in training. The binary mask defining this subnetwork is kept private, and only the final dense weights are released. This forces unauthorized fine-tuning without access to the mask to update parameters misaligned with pretraining subspace, inducing an intrinsic mismatch between the fine-tuning objective and the pre-training geometry. We provide theoretical analysis showing that this mismatch destabilizes gradient-based adaptation and bounds fine-tuning gains. Empirical results on large language models demonstrating that PMP preserves base model performance while consistently degrading unauthorized fine-tuning across a wide range of downstream tasks, with the strength of non-fine-tunability controlled by the mask ratio.

</details>


### [84] [Stabilizing Decentralized Federated Fine-Tuning via Topology-Aware Alternating LoRA](https://arxiv.org/abs/2602.00451)
*Xiaoyu Wang,Xiaotian Li,Zhixiang Zhou,Chen Li,Yong Liu*

Main category: cs.LG

TL;DR: TAD-LoRA：针对去中心化联邦学习中LoRA参数高效微调的拓扑感知框架，解决动态通信图下LoRA更新的稳定性问题


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习（DFL）作为无服务器联邦学习变体，在参数高效微调方面面临独特挑战。与线性参数不同，LoRA更新的去中心化聚合会引入拓扑依赖的交叉项，在动态通信图下可能破坏训练稳定性。

Method: 提出TAD-LoRA（拓扑感知去中心化低秩适应）框架，协调LoRA因子的更新和混合以控制客户端间错位。理论证明在非凸目标下的收敛性，明确表征拓扑诱导交叉项误差与块坐标表示偏差之间的权衡。

Result: 在各种通信条件下的实验验证了分析，TAD-LoRA在不同通信场景下实现稳健性能：在强连接拓扑中保持竞争力，在中等和弱连接拓扑中提供明显增益，在MNLI数据集上表现尤为突出。

Conclusion: TAD-LoRA有效解决了去中心化联邦学习中LoRA微调的稳定性问题，通过拓扑感知的协调机制在不同通信条件下实现稳健性能，为动态通信环境下的参数高效微调提供了有效解决方案。

Abstract: Decentralized federated learning (DFL), a serverless variant of federated learning, poses unique challenges for parameter-efficient fine-tuning due to the factorized structure of low-rank adaptation (LoRA). Unlike linear parameters, decentralized aggregation of LoRA updates introduces topology-dependent cross terms that can destabilize training under dynamic communication graphs. We propose \texttt{TAD-LoRA}, a Topology-Aware Decentralized Low-Rank Adaptation framework that coordinates the updates and mixing of LoRA factors to control inter-client misalignment. We theoretically prove the convergence of \texttt{TAD-LoRA} under non-convex objectives, explicitly characterizing the trade-off between topology-induced cross-term error and block-coordinate representation bias governed by the switching interval of alternative training. Experiments under various communication conditions validate our analysis, showing that \texttt{TAD-LoRA} achieves robust performance across different communication scenarios, remaining competitive in strongly connected topologies and delivering clear gains under moderately and weakly connected topologies, with particularly strong results on the MNLI dataset.

</details>


### [85] [FedMOA: Federated GRPO for Personalized Reasoning LLMs under Heterogeneous Rewards](https://arxiv.org/abs/2602.00453)
*Ziyao Wang,Daeun Jung,Yexiao He,Guoheng Sun,Zheyu Shen,Myungjin Lee,Ang Li*

Main category: cs.LG

TL;DR: FedMOA：一个基于GRPO的联邦多目标对齐框架，通过自适应权重调整和任务感知聚合，在异构奖励下实现高效的多目标优化。


<details>
  <summary>Details</summary>
Motivation: 传统RL对齐在联邦学习中存在内存开销大、异构奖励定义、多目标优化不平衡等问题，需要一种能在设备端高效训练且能处理异构多目标对齐的解决方案。

Method: 基于GRPO的联邦学习框架，包含：1）本地训练使用基于超梯度下降的自适应权重机制，优先处理主要推理目标；2）服务器端采用任务和准确率感知的聚合策略，优先选择高质量更新。

Result: 在数学推理和代码生成基准测试中，FedMOA始终优于联邦平均方法，准确率提升高达2.2%，同时改善了全局性能、个性化和多目标平衡。

Conclusion: FedMOA成功解决了联邦GRPO中的异构奖励和多目标优化挑战，为设备端多目标对齐提供了可行的解决方案。

Abstract: Group Relative Policy Optimization (GRPO) has recently emerged as an effective approach for improving the reasoning capabilities of large language models through online multi-objective reinforcement learning. While personalization on private data is increasingly vital, traditional Reinforcement Learning (RL) alignment is often memory-prohibitive for on-device federated learning due to the overhead of maintaining a separate critic network. GRPO's critic-free architecture enables feasible on-device training, yet transitioning to a federated setting introduces systemic challenges: heterogeneous reward definitions, imbalanced multi-objective optimization, and high training costs. We propose FedMOA, a federated GRPO framework for multi-objective alignment under heterogeneous rewards. FedMOA stabilizes local training through an online adaptive weighting mechanism via hypergradient descent, which prioritizes primary reasoning as auxiliary objectives saturate. On the server side, it utilizes a task- and accuracy-aware aggregation strategy to prioritize high-quality updates. Experiments on mathematical reasoning and code generation benchmarks demonstrate that FedMOA consistently outperforms federated averaging, achieving accuracy gains of up to 2.2% while improving global performance, personalization, and multi-objective balance.

</details>


### [86] [LatentTrack: Sequential Weight Generation via Latent Filtering](https://arxiv.org/abs/2602.00458)
*Omer Haq*

Main category: cs.LG

TL;DR: LatentTrack (LT) 是一种用于非平稳动态下在线概率预测的序列神经网络架构，通过在低维潜空间进行因果贝叶斯滤波，使用轻量级超网络生成预测模型参数，实现恒定时间在线适应而无需梯度更新。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理非平稳动态和分布偏移时面临挑战，需要有效的在线适应机制。现有方法要么需要昂贵的梯度更新，要么难以在保持校准的同时实现高效预测。

Method: LT在潜空间执行因果贝叶斯滤波，使用超网络生成每个时间步的预测模型参数。采用预测-生成-更新的滤波框架：学习潜模型预测下一个潜分布，通过摊销推理使用新观测更新，支持结构化和非结构化潜动态，通过潜轨迹的蒙特卡洛推理产生校准的预测混合。

Result: 在Jena Climate基准测试的长时域在线回归中，LT持续实现比状态序列和静态不确定性感知基线更低的负对数似然和均方误差，具有竞争力的校准性能。

Conclusion: 潜条件函数演化是传统潜状态建模在分布偏移下的有效替代方案，能够在恒定计算成本下实现校准的在线概率预测。

Abstract: We introduce LatentTrack (LT), a sequential neural architecture for online probabilistic prediction under nonstationary dynamics. LT performs causal Bayesian filtering in a low-dimensional latent space and uses a lightweight hypernetwork to generate predictive model parameters at each time step, enabling constant-time online adaptation without per-step gradient updates.
  At each time step, a learned latent model predicts the next latent distribution, which is updated via amortized inference using new observations, yielding a predict--generate--update filtering framework in function space. The formulation supports both structured (Markovian) and unstructured latent dynamics within a unified objective, while Monte Carlo inference over latent trajectories produces calibrated predictive mixtures with fixed per-step cost. Evaluated on long-horizon online regression using the Jena Climate benchmark, LT consistently achieves lower negative log-likelihood and mean squared error than stateful sequential and static uncertainty-aware baselines, with competitive calibration, demonstrating that latent-conditioned function evolution is an effective alternative to traditional latent-state modeling under distribution shift.

</details>


### [87] [Search Inspired Exploration in Reinforcement Learning](https://arxiv.org/abs/2602.00460)
*Georgios Sotirchos,Zlatan Ajanović,Jens Kober*

Main category: cs.LG

TL;DR: SIERL是一种基于搜索启发的强化学习探索方法，通过设置子目标来主动引导探索，在稀疏奖励环境中优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 稀疏奖励环境中的探索是强化学习的核心挑战。现有方法如课程学习和Go-Explore依赖人工设计的启发式规则，而好奇心驱动的方法可能收敛到次优策略。

Method: SIERL在每个episode开始时从边界（已知状态空间的边界）选择子目标，然后代理继续向主要任务目标探索。子目标选择机制确保状态-动作对既不过于熟悉也不完全新颖，通过成本估计（到达成本和前往成本）对边界子目标进行优先级排序。

Result: 在具有挑战性的稀疏奖励环境中，SIERL在实现主要任务目标和泛化到环境中任意状态方面都优于主流基线方法。

Conclusion: SIERL通过搜索启发的子目标选择机制，能够系统性地扩展边界并引导探索到最具信息量的区域，有效解决了稀疏奖励环境中的探索问题。

Abstract: Exploration in environments with sparse rewards remains a fundamental challenge in reinforcement learning (RL). Existing approaches such as curriculum learning and Go-Explore often rely on hand-crafted heuristics, while curiosity-driven methods risk converging to suboptimal policies. We propose Search-Inspired Exploration in Reinforcement Learning (SIERL), a novel method that actively guides exploration by setting sub-goals based on the agent's learning progress. At the beginning of each episode, SIERL chooses a sub-goal from the \textit{frontier} (the boundary of the agent's known state space), before the agent continues exploring toward the main task objective. The key contribution of our method is the sub-goal selection mechanism, which provides state-action pairs that are neither overly familiar nor completely novel. Thus, it assures that the frontier is expanded systematically and that the agent is capable of reaching any state within it. Inspired by search, sub-goals are prioritized from the frontier based on estimates of cost-to-come and cost-to-go, effectively steering exploration towards the most informative regions. In experiments on challenging sparse-reward environments, SIERL outperforms dominant baselines in both achieving the main task goal and generalizing to reach arbitrary states in the environment.

</details>


### [88] [PAIR-Former: Budgeted Relational MIL for miRNA Target Prediction](https://arxiv.org/abs/2602.00465)
*Jiaqi Yin,Baiming Chen,Jia Fei,Mingjun Yang*

Main category: cs.LG

TL;DR: PAIR-Former：一种预算感知的关系多示例学习框架，用于miRNA-mRNA靶向预测，通过廉价全池扫描和多样性选择，在计算预算内实现高效准确的预测。


<details>
  <summary>Details</summary>
Motivation: miRNA-mRNA靶向预测是一个大规模预测问题，每个转录本产生大量候选靶位点，但只有配对级标签可用。现有方法计算成本高，需要处理大量候选位点。

Method: 提出BR-MIL框架和PAIR-Former模型：1）廉价全池扫描；2）在CPU上选择最多K个多样性候选靶位点；3）使用置换不变的Set Transformer聚合器处理选定标记。

Result: 在miRAW数据集上，PAIR-Former在实用预算（K*=64）下优于强基线方法，并提供可控的精度-计算权衡。理论分析显示选择误差随K减小，泛化项由K控制。

Conclusion: PAIR-Former为大规模miRNA-mRNA靶向预测提供了一种高效的计算预算感知解决方案，通过多样性选择和关系处理实现了精度与计算成本的平衡。

Abstract: Functional miRNA--mRNA targeting is a large-bag prediction problem: each transcript yields a heavy-tailed pool of candidate target sites (CTSs), yet only a pair-level label is observed. We formalize this regime as \emph{Budgeted Relational Multi-Instance Learning (BR-MIL)}, where at most $K$ instances per bag may receive expensive encoding and relational processing under a hard compute budget. We propose \textbf{PAIR-Former} (Pool-Aware Instance-Relational Transformer), a BR-MIL pipeline that performs a cheap full-pool scan, selects up to $K$ diverse CTSs on CPU, and applies a permutation-invariant Set Transformer aggregator on the selected tokens. On miRAW, PAIR-Former outperforms strong pooling baselines at a practical operating budget ($K^\star{=}64$) while providing a controllable accuracy--compute trade-off as $K$ varies. We further provide theory linking budgeted selection to (i) approximation error decreasing with $K$ and (ii) generalization terms governed by $K$ in the expensive relational component.

</details>


### [89] [Parallel Stochastic Gradient-Based Planning for World Models](https://arxiv.org/abs/2602.00475)
*Michael Psenka,Michael Rabbat,Aditi Krishnapriyan,Yann LeCun,Amir Bar*

Main category: cs.LG

TL;DR: GRASP是一种基于可微世界模型的并行化规划器，通过虚拟状态优化和随机性引入解决视觉输入的长时域控制任务


<details>
  <summary>Details</summary>
Motivation: 世界模型可以从原始视觉输入模拟环境动态，但用于规划时面临搜索空间巨大且非结构化的挑战，需要更高效的优化方法

Method: 将状态视为优化变量（虚拟状态），加入软动力学约束实现并行计算；引入状态随机性促进探索；修改梯度结构以缓解高维视觉世界模型的敏感梯度问题

Result: 在基于视频的世界模型实验中，GRASP在长时域任务上的成功率和收敛时间均优于交叉熵方法（CEM）和普通梯度优化（GD）

Conclusion: GRASP作为一种随机化的非凝聚或配点最优控制器，通过可微世界模型的梯度优化实现了高效的长时域视觉控制规划

Abstract: World models simulate environment dynamics from raw sensory inputs like video. However, using them for planning can be challenging due to the vast and unstructured search space. We propose a robust and highly parallelizable planner that leverages the differentiability of the learned world model for efficient optimization, solving long-horizon control tasks from visual input. Our method treats states as optimization variables ("virtual states") with soft dynamics constraints, enabling parallel computation and easier optimization. To facilitate exploration and avoid local optima, we introduce stochasticity into the states. To mitigate sensitive gradients through high-dimensional vision-based world models, we modify the gradient structure to descend towards valid plans while only requiring action-input gradients. Our planner, which we call GRASP (Gradient RelAxed Stochastic Planner), can be viewed as a stochastic version of a non-condensed or collocation-based optimal controller. We provide theoretical justification and experiments on video-based world models, where our resulting planner outperforms existing planning algorithms like the cross-entropy method (CEM) and vanilla gradient-based optimization (GD) on long-horizon experiments, both in success rate and time to convergence.

</details>


### [90] [Diffusion LMs Can Approximate Optimal Infilling Lengths Implicitly](https://arxiv.org/abs/2602.00476)
*Hengchang Liu,Zhao Yang,Bing Su*

Main category: cs.LG

TL;DR: CAL方法通过利用DLMs在第一步去噪中的统计信号（Oracle Peak和Length Bias），实现无需训练的自适应长度预测，显著提升代码和文本填充性能。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型虽然天然适合填充任务，但其性能受限于预设的填充长度。研究发现DLMs具有发现正确填充长度的内在能力，但这一信号被系统性的长度偏差所掩盖。

Method: 提出CAL方法：利用第一步去噪置信度中的Oracle Peak信号，校准Length Bias，通过高效搜索在正式解码前近似最优长度。该方法无需训练，仅基于统计信号。

Result: 在代码填充中，CAL比固定长度基线提升Pass@1达47.7%，比基于聊天的自适应方法提升40.5%；在文本填充中，BLEU-2和ROUGE-L分别提升达8.5%和9.9%。

Conclusion: CAL证明了DLMs具有发现正确填充长度的内在能力，通过校准偏差和利用统计信号，实现了无需专门训练的鲁棒填充方法，为DLM填充开辟了新途径。

Abstract: Diffusion language models (DLMs) provide a bidirectional generation framework naturally suited for infilling, yet their performance is constrained by the pre-specified infilling length. In this paper, we reveal that DLMs possess an inherent ability to discover the correct infilling length. We identify two key statistical phenomena in the first-step denoising confidence: a local \textit{Oracle Peak} that emerges near the ground-truth length and a systematic \textit{Length Bias} that often obscures this signal. By leveraging this signal and calibrating the bias, our training-free method \textbf{CAL} (\textbf{C}alibrated \textbf{A}daptive \textbf{L}ength) enables DLMs to approximate the optimal length through an efficient search before formal decoding. Empirical evaluations demonstrate that CAL improves Pass@1 by up to 47.7\% over fixed-length baselines and 40.5\% over chat-based adaptive methods in code infilling, while boosting BLEU-2 and ROUGE-L by up to 8.5\% and 9.9\% in text infilling. These results demonstrate that CAL paves the way for robust DLM infilling without requiring any specialized training. Code is available at https://github.com/NiuHechang/Calibrated_Adaptive_Length.

</details>


### [91] [Quality-Diversity Optimization as Multi-Objective Optimization](https://arxiv.org/abs/2602.00478)
*Xi Lin,Ping Guo,Yilu Liu,Qingfu Zhang,Jianyong Sun*

Main category: cs.LG

TL;DR: 该论文将质量多样性优化重新表述为具有大量目标的多目标优化问题，使得可以直接应用成熟的MOO方法来解决QD问题。


<details>
  <summary>Details</summary>
Motivation: 质量多样性优化在机器人控制、创意设计等领域有重要应用，但现有QD算法各有不同的设计原则。本文不提出新的QD算法，而是寻求建立QD与MOO之间的理论联系，从而利用成熟的MOO方法来解决QD问题。

Method: 将QD优化重新表述为具有大量优化目标的多目标优化问题，特别是采用基于集合的标量化技术，通过协作搜索过程解决QD问题。

Result: 理论分析表明该方法继承了MOO的理论保证，同时为QD优化提供了理想特性。在多个QD应用中的实验研究表明，该方法达到了与最先进QD算法相竞争的性能。

Conclusion: 通过建立QD与MOO之间的理论联系，使得可以直接应用成熟的MOO方法解决QD问题，为QD优化提供了新的视角和有效的解决方案。

Abstract: The Quality-Diversity (QD) optimization aims to discover a collection of high-performing solutions that simultaneously exhibit diverse behaviors within a user-defined behavior space. This paradigm has stimulated significant research interest and demonstrated practical utility in domains including robot control, creative design, and adversarial sample generation. A variety of QD algorithms with distinct design principles have been proposed in recent years. Instead of proposing a new QD algorithm, this work introduces a novel reformulation by casting the QD optimization as a multi-objective optimization (MOO) problem with a huge number of optimization objectives. By establishing this connection, we enable the direct adoption of well-established MOO methods, particularly set-based scalarization techniques, to solve QD problems through a collaborative search process. We further provide a theoretical analysis demonstrating that our approach inherits theoretical guarantees from MOO while providing desirable properties for the QD optimization. Experimental studies across several QD applications confirm that our method achieves performance competitive with state-of-the-art QD algorithms.

</details>


### [92] [AREAL-DTA: Dynamic Tree Attention for Efficient Reinforcement Learning of Large Language Models](https://arxiv.org/abs/2602.00482)
*Jiarui Zhang,Yuchen Yang,Ran Yan,Zhiyu Mei,Liyuan Zhang,Daifeng Li,Wei Fu,Jiaxuan Gao,Shusheng Xu,Yi Wu,Binhang Yuan*

Main category: cs.LG

TL;DR: AREAL-DTA：一种基于深度优先搜索的动态树注意力机制，用于高效利用RL训练中的前缀共享，提升大型语言模型强化学习后训练的计算效率


<details>
  <summary>Details</summary>
Motivation: 现有RL框架在处理具有长共享前缀的rollout序列时效率低下，因为它们独立处理每个序列，重复计算相同的前缀，导致计算和内存使用的大量浪费

Method: 采用深度优先搜索（DFS）执行策略，在前后向计算中动态遍历rollout前缀树，每次只实例化单个根到叶路径；结合负载均衡的分布式批处理机制，在多GPU上动态构建和处理前缀树

Result: 在流行的RL后训练任务中，AREAL-DTA实现了高达8.31倍的τ²-bench训练吞吐量提升

Conclusion: AREAL-DTA通过动态树注意力机制有效解决了RL训练中的前缀共享效率问题，显著提升了大型语言模型强化学习后训练的计算性能

Abstract: Reinforcement learning (RL) based post-training for large language models (LLMs) is computationally expensive, as it generates many rollout sequences that could frequently share long token prefixes. Existing RL frameworks usually process these sequences independently, repeatedly recomputing identical prefixes during forward and backward passes during policy model training, leading to substantial inefficiencies in computation and memory usage. Although prefix sharing naturally induces a tree structure over rollouts, prior tree-attention-based solutions rely on fully materialized attention masks and scale poorly in RL settings. In this paper, we introduce AREAL-DTA to efficiently exploit prefix sharing in RL training. AREAL-DTA employs a depth-first-search (DFS)-based execution strategy that dynamically traverses the rollout prefix tree during both forward and backward computation, materializing only a single root-to-leaf path at a time. To further improve scalability, AREAL-DTA incorporates a load-balanced distributed batching mechanism that dynamically constructs and processes prefix trees across multiple GPUs. Across the popular RL post-training workload, AREAL-DTA achieves up to $8.31\times$ in $τ^2$-bench higher training throughput.

</details>


### [93] [OD-DEAL: Dynamic Expert-Guided Adversarial Learning with Online Decomposition for Scalable Capacitated Vehicle Routing](https://arxiv.org/abs/2602.00488)
*Dongbin Jiao,Zisheng Chen,Xianyi Wang,Jintao Shi,Shengcai Liu,Shi Yan*

Main category: cs.LG

TL;DR: OD-DEAL是一个用于大规模有容量车辆路径问题(CVRP)的对抗学习框架，通过混合遗传搜索和在线重心聚类分解的紧密集成，结合知识蒸馏，实现了无需聚类的实时高质量推理。


<details>
  <summary>Details</summary>
Motivation: 解决大规模CVRP面临传统启发式算法复杂度高和神经网络求解器在大规模图上泛化能力有限的双重挑战，需要开发能够实现实时、高质量推理的解决方案。

Method: 提出OD-DEAL对抗学习框架：1) 紧密集成混合遗传搜索(HGS)和在线重心聚类(BCC)分解；2) 通过高保真知识蒸馏将专家启发式行为转移到神经网络；3) 使用基于图注意力网络(GAT)的生成策略，通过极小极大博弈训练，将分治策略蒸馏为密集代理奖励。

Result: OD-DEAL实现了最先进的实时CVRP性能，能够解决10000节点的大规模实例，具有接近恒定的神经缩放特性，实现了亚秒级、启发式质量的推理，满足动态大规模部署需求。

Conclusion: OD-DEAL通过对抗学习和知识蒸馏的有机结合，成功解决了大规模CVRP的实时求解难题，为动态大规模部署提供了有效的解决方案。

Abstract: Solving large-scale capacitated vehicle routing problems (CVRP) is hindered by the high complexity of heuristics and the limited generalization of neural solvers on massive graphs. We propose OD-DEAL, an adversarial learning framework that tightly integrates hybrid genetic search (HGS) and online barycenter clustering (BCC) decomposition, and leverages high-fidelity knowledge distillation to transfer expert heuristic behavior. OD-DEAL trains a graph attention network (GAT)-based generative policy through a minimax game, in which divide-and-conquer strategies from a hybrid expert are distilled into dense surrogate rewards. This enables high-quality, clustering-free inference on large-scale instances. Empirical results demonstrate that OD-DEAL achieves state-of-the-art (SOTA) real-time CVRP performance, solving 10000-node instances with near-constant neural scaling. This uniquely enables the sub-second, heuristic-quality inference required for dynamic large-scale deployment.

</details>


### [94] [Partition of Unity Neural Networks for Interpretable Classification with Explicit Class Regions](https://arxiv.org/abs/2602.00511)
*Akram Aldroubi*

Main category: cs.LG

TL;DR: PUNN是一种新的神经网络架构，用学习到的单位分解直接产生类别概率，无需softmax层，提供更可解释的分类器。


<details>
  <summary>Details</summary>
Motivation: 传统softmax分类器的类别区域由logits的不等式系统隐式定义，难以提取和可视化，缺乏可解释性。

Method: 提出Partition of Unity Neural Networks (PUNN)，学习k个非负函数h_i满足∑h_i(x)=1，每个h_i(x)直接表示P(class i|x)。门函数g_i可使用多种激活函数和参数化设计。

Result: 在合成数据、UCI基准和MNIST上，基于MLP的PUNN准确率比标准多层感知机低0.3-0.6%。当几何先验匹配数据结构时，形状感知门函数用少300倍的参数达到可比准确率。

Conclusion: PUNN证明可解释性设计架构可以与黑盒模型竞争，同时提供透明的类别概率分配，为可解释分类器提供了新方向。

Abstract: Despite their empirical success, neural network classifiers remain difficult to interpret. In softmax-based models, class regions are defined implicitly as solutions to systems of inequalities among logits, making them difficult to extract and visualize. We introduce Partition of Unity Neural Networks (PUNN), an architecture in which class probabilities arise directly from a learned partition of unity, without requiring a softmax layer.
  PUNN constructs $k$ nonnegative functions $h_1, \ldots, h_k$ satisfying $\sum_i h_i(x) = 1$, where each $h_i(x)$ directly represents $P(\text{class } i \mid x)$. Unlike softmax, where class regions are defined implicitly through coupled inequalities among logits, each PUNN partition function $h_i$ directly defines the probability of class $i$ as a standalone function of $x$.
  We prove that PUNN is dense in the space of continuous probability maps on compact domains. The gate functions $g_i$ that define the partition can use various activation functions (sigmoid, Gaussian, bump) and parameterizations ranging from flexible MLPs to parameter-efficient shape-informed designs (spherical shells, ellipsoids, spherical harmonics).
  Experiments on synthetic data, UCI benchmarks, and MNIST show that PUNN with MLP-based gates achieves accuracy within 0.3--0.6\% of standard multilayer perceptrons. When geometric priors match the data structure, shape-informed gates achieve comparable accuracy with up to 300$\times$ fewer parameters. These results demonstrate that interpretable-by-design architectures can be competitive with black-box models while providing transparent class probability assignments.

</details>


### [95] [Minerva: Reinforcement Learning with Verifiable Rewards for Cyber Threat Intelligence LLMs](https://arxiv.org/abs/2602.00513)
*Md Tanvirul Alam,Aritran Piplai,Ionut Cardei,Nidhi Rastogi,Peter J Worth*

Main category: cs.LG

TL;DR: 论文提出Minerva框架，利用可验证奖励的强化学习（RLVR）改进网络安全威胁情报（CTI）的结构化信息提取，相比监督微调（SFT）在准确性和鲁棒性上均有提升。


<details>
  <summary>Details</summary>
Motivation: 当前CTI分析师需要将非结构化的安全数据转换为标准化的自动化就绪表示。虽然大语言模型（LLM）在此任务上显示出潜力，但现有方法在生成结构化CTI输出时仍显脆弱，且主要依赖监督微调。CTI标准和社区维护的资源定义了规范的标识符和模式，这为模型输出提供了确定性验证的可能性。

Method: 提出Minerva框架：1）构建统一数据集和训练管道，涵盖多个CTI子任务；2）为每个任务配备特定验证器，对结构化输出和标识符预测进行评分；3）针对奖励稀疏性问题，提出轻量级自训练机制，生成额外的已验证轨迹并蒸馏回模型中；4）采用可验证奖励的强化学习（RLVR）方法。

Result: 实验结果表明，在不同LLM骨干网络上，该方法在多个基准测试中相比监督微调（SFT）在准确性和鲁棒性方面均取得一致改进。

Conclusion: 利用CTI标准的确定性验证特性，通过可验证奖励的强化学习方法能够有效提升CTI结构化信息提取的性能，为网络安全威胁情报处理提供了更可靠的技术方案。

Abstract: Cyber threat intelligence (CTI) analysts routinely convert noisy, unstructured security artifacts into standardized, automation-ready representations. Although large language models (LLMs) show promise for this task, existing approaches remain brittle when producing structured CTI outputs and have largely relied on supervised fine-tuning (SFT). In contrast, CTI standards and community-maintained resources define canonical identifiers and schemas that enable deterministic verification of model outputs. We leverage this structure to study reinforcement learning with verifiable rewards (RLVR) for CTI tasks. We introduce \textit{Minerva}, a unified dataset and training pipeline spanning multiple CTI subtasks, each paired with task-specific verifiers that score structured outputs and identifier predictions. To address reward sparsity during rollout, we propose a lightweight self-training mechanism that generates additional verified trajectories and distills them back into the model. Experiments across LLM backbones show consistent improvements in accuracy and robustness over SFT across multiple benchmarks.

</details>


### [96] [Contrastive Learning for Privacy Enhancements in Industrial Internet of Things](https://arxiv.org/abs/2602.00515)
*Lin Liu,Rita Machacy,Simi Kuniyilh*

Main category: cs.LG

TL;DR: 本文对工业物联网(IIoT)中的隐私保护对比学习技术进行了全面综述，重点分析了工业数据特性、系统架构和应用场景，并讨论了现有解决方案、开放挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 工业物联网(IIoT)在实现预测性维护和跨站点优化的同时，由于运营数据的敏感性，带来了显著的隐私和机密性风险。对比学习作为一种自监督表示学习范式，通过减少对标记数据和原始数据共享的依赖，为隐私保护分析提供了有前景的方法。

Method: 本文采用文献综述方法，系统性地回顾和分析了工业物联网领域中基于对比学习的隐私保护技术。重点关注工业数据的独特特性、系统架构和各种应用场景，并对现有解决方案进行综合评述。

Result: 论文提供了工业物联网隐私保护对比学习技术的全面综述，识别了该领域的关键技术、应用场景和现有解决方案，同时指出了当前面临的开放挑战。

Conclusion: 对比学习在工业物联网隐私保护中具有重要应用潜力，但需要针对工业环境的独特需求进行专门研究。未来研究应关注解决现有挑战，推动隐私保护技术在工业物联网中的实际应用。

Abstract: The Industrial Internet of Things (IIoT) integrates intelligent sensing, communication, and analytics into industrial environments, including manufacturing, energy, and critical infrastructure. While IIoT enables predictive maintenance and cross-site optimization of modern industrial control systems, such as those in manufacturing and energy, it also introduces significant privacy and confidentiality risks due to the sensitivity of operational data. Contrastive learning, a self-supervised representation learning paradigm, has recently emerged as a promising approach for privacy-preserving analytics by reducing reliance on labeled data and raw data sharing. Although contrastive learning-based privacy-preserving techniques have been explored in the Internet of Things (IoT) domain, this paper offers a comprehensive review of these techniques specifically for privacy preservation in Industrial Internet of Things (IIoT) systems. It emphasizes the unique characteristics of industrial data, system architectures, and various application scenarios. Additionally, the paper discusses solutions and open challenges and outlines future research directions.

</details>


### [97] [NEST: Nested Event Stream Transformer for Sequences of Multisets](https://arxiv.org/abs/2602.00520)
*Minghui Sun,Haoyu Gong,Xingyu You,Jillian Hurst,Benjamin Goldstein,Matthew Engelhard*

Main category: cs.LG

TL;DR: NEST模型：针对具有层次结构的事件流数据（如电子健康记录）的Transformer架构，通过保留原始的多重集序列结构来提高计算效率和表示质量


<details>
  <summary>Details</summary>
Motivation: 现有基础模型通常将层次化的事件流数据（如医疗记录中的临床就诊序列）扁平化为一维序列，导致计算效率低下（密集注意力机制）和表示质量下降（启发式池化方法）

Method: 提出NEST（Nested Event Stream Transformer）架构，保留事件流的原始层次结构（多重集序列）。引入Masked Set Modeling（MSM）预训练范式，促进更好的集合级表示学习

Result: 实验表明NEST能够捕捉真实世界动态，同时提高预训练效率和下游任务性能

Conclusion: 在基础模型中保留事件流的层次结构提供了有益的归纳偏置，既能提高计算效率又能提升表示质量，NEST架构为此类数据提供了有效的建模方法

Abstract: Event stream data often exhibit hierarchical structure in which multiple events co-occur, resulting in a sequence of multisets (i.e., bags of events). In electronic health records (EHRs), for example, medical events are grouped into a sequence of clinical encounters with well-defined temporal structure, but the order and timing of events within each encounter may be unknown or unreliable. Most existing foundation models (FMs) for event stream data flatten this hierarchy into a one-dimensional sequence, leading to (i) computational inefficiency associated with dense attention and learning spurious within-set relationships, and (ii) lower-quality set-level representations from heuristic post-training pooling for downstream tasks. Here, we show that preserving the original hierarchy in the FM architecture provides a useful inductive bias that improves both computational efficiency and representation quality. We then introduce Nested Event Stream Transformer (NEST), a FM for event streams comprised of sequences of multisets. Building on this architecture, we formulate Masked Set Modeling (MSM), an efficient paradigm that promotes improved set-level representation learning. Experiments on real-world multiset sequence data show that NEST captures real-world dynamics while improving both pretraining efficiency and downstream performance.

</details>


### [98] [Physiology as Language: Translating Respiration to Sleep EEG](https://arxiv.org/abs/2602.00526)
*Kaiwen Zha,Chao Li,Hao He,Peng Cao,Tianhong Li,Ali Mirzazadeh,Ellen Zhang,Jong Woo Lee,Yoon Kim,Dina Katabi*

Main category: cs.LG

TL;DR: 提出一种从呼吸信号合成睡眠脑电图的新方法，通过波形条件生成框架和离散标记化技术，实现跨生理模态转换，并在大规模数据上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 传统睡眠监测需要接触式脑电图设备，给用户带来不便。本研究旨在探索从更易获取的呼吸信号合成脑电图的可能性，实现非接触式、便捷的睡眠神经评估。

Method: 提出波形条件生成框架，通过离散标记化技术约束脑电图目标空间，同时保留呼吸信号的细粒度动态特征。模型在超过28,000名个体的数据上进行训练。

Result: 脑电图频谱图重建的平均绝对误差为7%。合成脑电图在下游任务中表现接近真实脑电图：年龄估计（MAE 5.0 vs. 5.1年）、性别检测（AUROC 0.81 vs. 0.82）、睡眠分期（准确率0.84 vs. 0.88），显著优于直接在呼吸信号上训练的基线方法。

Conclusion: 该框架成功实现了从呼吸信号到脑电图的跨生理模态转换，并扩展到无线射频反射的非接触式传感，展示了远程、非接触式睡眠神经评估的可行性。

Abstract: This paper introduces a novel cross-physiology translation task: synthesizing sleep electroencephalography (EEG) from respiration signals. To address the significant complexity gap between the two modalities, we propose a waveform-conditional generative framework that preserves fine-grained respiratory dynamics while constraining the EEG target space through discrete tokenization. Trained on over 28,000 individuals, our model achieves a 7% Mean Absolute Error in EEG spectrogram reconstruction. Beyond reconstruction, the synthesized EEG supports downstream tasks with performance comparable to ground truth EEG on age estimation (MAE 5.0 vs. 5.1 years), sex detection (AUROC 0.81 vs. 0.82), and sleep staging (Accuracy 0.84 vs. 0.88), significantly outperforming baselines trained directly on breathing. Finally, we demonstrate that the framework generalizes to contactless sensing by synthesizing EEG from wireless radio-frequency reflections, highlighting the feasibility of remote, non-contact neurological assessment during sleep.

</details>


### [99] [Convergent World Representations and Divergent Tasks](https://arxiv.org/abs/2602.00533)
*Core Francisco Park*

Main category: cs.LG

TL;DR: 多任务训练能产生收敛的世界表示，但某些"发散任务"会损害新实体的表示整合和泛化能力


<details>
  <summary>Details</summary>
Motivation: 研究神经表示的几何特性及其在下游适应能力中的作用，目前对这些表示的条件和角色理解不足

Method: 建立分离世界、数据生成过程和模型表示的框架，使用5,075个城市坐标定义世界，7个几何任务生成自回归训练数据，研究多任务训练和微调下的表示变化

Result: 不同任务产生不同的世界表示几何，多任务训练使表示收敛对齐；但某些"发散任务"会损害新实体的表示整合，即使经过多任务预训练

Conclusion: 多任务关系训练能可靠产生收敛的世界表示，但隐藏的发散任务可能通过微调灾难性地损害新实体的表示整合

Abstract: While neural representations are central to modern deep learning, the conditions governing their geometry and their roles in downstream adaptability remain poorly understood. We develop a framework clearly separating the underlying world, the data generation process and the resulting model representations to study these questions in a controlled setup. 5,075 city coordinates define the world and 7 geometric tasks generate the training data for autoregressive training. We find that different tasks give rise to qualitatively and quantitatively distinct world representation geometries. However, multi-task training drives convergence of world representations: models trained on non-overlapping tasks develop aligned geometric representations, providing controlled evidence for the Multitask Scaling Hypothesis of the Platonic Representation Hypothesis. To study adaptation, we pretrain models on all tasks, then test whether new entities (cities) can be consistently integrated into the representation space via fine-tuning. Surprisingly, we find that despite multi-task pretraining, some tasks, which we call divergent, actively harm the representational integration of new entities and harm generalization. Our results show that training on multiple relational tasks reliably produces convergent world representations, but lurking divergent tasks can catastrophically harm new entity integration via fine-tuning.

</details>


### [100] [AIRE-Prune: Asymptotic Impulse-Response Energy for State Pruning in State Space Models](https://arxiv.org/abs/2602.00534)
*Apurba Prasad Padhy,Fernando Camacho,Saibal Mukhopadhyay*

Main category: cs.LG

TL;DR: AIRE-Prune是一种针对状态空间模型的结构化后训练剪枝方法，通过最小化长期输出能量失真来减少每层状态维度，实现60.8%的平均剪枝率，仅带来0.29%的精度下降。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型在处理大状态维度时面临内存和计算成本问题，通常需要牺牲模型容量、搜索空间或稳定性。现有方法缺乏有效的剪枝技术来平衡这些因素。

Method: 提出AIRE-Prune方法，为每个状态分配基于渐近脉冲响应能量的封闭形式评分，表示其在无限时间范围内贡献的总脉冲响应能量。通过层间归一化实现全局跨层比较和选择，将模态截断从单系统扩展到深度堆栈。

Result: 在多样化序列基准测试中，AIRE-Prune在SISO和MIMO状态空间模型中揭示了大量冗余，平均剪枝率达到60.8%，平均精度下降仅为0.29%（无需重新训练），同时显著降低计算成本。

Conclusion: AIRE-Prune提供了一种有效的状态空间模型剪枝方法，通过基于渐近响应能量的剪枝策略而非最坏情况增益，实现了高效的状态维度减少，为实际部署提供了实用解决方案。

Abstract: State space models (SSMs) often sacrifice capacity, search space, or stability to offset the memory and compute costs of large state dimensions. We introduce a structured post-training pruning method for SSMs -- AIRE-Prune (Asymptotic Impulse-Response Energy for State PRUN(E)) -- that reduces each layer's state dimension by directly minimizing long-run output-energy distortion. AIRE-Prune assigns every state a closed-form asymptotic impulse-response energy-based score, i.e., the total impulse-response energy it contributes over an infinite horizon (time), and normalizes these scores layer-wise to enable global cross-layer comparison and selection. This extends modal truncation from single systems to deep stacks and aligns pruning with asymptotic response energy rather than worst-case gain. Across diverse sequence benchmarks, AIRE-Prune reveals substantial redundancy in SISO and MIMO SSMs with average pruning of 60.8%, with average accuracy drop of 0.29% without retraining, while significantly lowering compute. Code: https://github.com/falcon-arrow/AIRE-Prune.

</details>


### [101] [Invertible Memory Flow Networks](https://arxiv.org/abs/2602.00535)
*Liyu Zerihun,Alexandr Plashchinsky*

Main category: cs.LG

TL;DR: IMFN通过二叉树分解解决长序列压缩问题，避免端到端优化的困难，实现对数深度压缩，并可蒸馏为常数成本的循环网络。


<details>
  <summary>Details</summary>
Motivation: 解决长序列神经记忆的挑战：RNN存在梯度消失问题，Transformer有二次方复杂度，而将长序列压缩为有限固定表示由于优化困难而难以实现。

Method: 使用可逆记忆流网络（IMFN），通过二叉树分解方法：将长序列压缩问题分解为成对合并，使用"清扫器"模块构建二叉树结构，每个清扫器学习简单的2对1压缩任务。

Result: IMFN在长MNIST序列和UCF-101视频上验证有效，能够压缩长序列中的高维数据，实现O(log N)深度和亚线性误差累积，并可蒸馏为O(1)序列步骤的循环学生网络。

Conclusion: IMFN通过分解策略使长序列压缩变得可行，避免了端到端优化的困难，为长序列处理提供了有效的解决方案。

Abstract: Long sequence neural memory remains a challenging problem. RNNs and their variants suffer from vanishing gradients, and Transformers suffer from quadratic scaling. Furthermore, compressing long sequences into a finite fixed representation remains an intractable problem due to the difficult optimization landscape. Invertible Memory Flow Networks (IMFN) make long sequence compression tractable through factorization: instead of learning end-to-end compression, we decompose the problem into pairwise merges using a binary tree of "sweeper" modules. Rather than learning to compress long sequences, each sweeper learns a much simpler 2-to-1 compression task, achieving O(log N) depth with sublinear error accumulation in sequence length. For online inference, we distilled into a constant-cost recurrent student achieving O(1) sequential steps. Empirical results validate IMFN on long MNIST sequences and UCF-101 videos, demonstrating compression of high-dimensional data over long sequences.

</details>


### [102] [OpenDDI: A Comprehensive Benchmark for DDI Prediction](https://arxiv.org/abs/2602.00539)
*Xinmo Jin,Bowen Fan,Xunkai Li,Henan Sun,YuXin Zeng,Zekai Chen,Yuxuan Sun,Jia Li,Qiangqiang Dai,Hongchao Qin,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: OpenDDI是一个全面的药物相互作用预测基准，统一了多个数据集和评估标准，提供了新的LLM增强数据集和多模态药物表示，并对20个SOTA模型进行了标准化评估。


<details>
  <summary>Details</summary>
Motivation: 药物相互作用预测面临两个主要挑战：1）缺乏高质量数据（小规模数据集和单模态表示）；2）缺乏标准化评估（不一致的场景、指标和基线）。这些限制了该领域的进一步发展。

Method: 1）数据层面：统一6个常用DDI数据集和2种现有药物表示，贡献3个新的LLM增强大规模数据集和覆盖5种模态的多模态药物表示；2）评估层面：统一20个SOTA模型基线，涵盖3个下游任务，制定数据质量、有效性、泛化性、鲁棒性和效率的标准化协议。

Result: 基于OpenDDI进行了全面评估，得出了10个有价值的DDI预测见解，同时揭示了当前局限性，为这个快速发展的领域提供了关键指导。

Conclusion: OpenDDI为DDI预测提供了一个全面的基准，解决了数据质量和评估标准化的问题，为该领域的未来发展提供了重要参考框架。

Abstract: Drug-Drug Interactions (DDIs) significantly influence therapeutic efficacy and patient safety. As experimental discovery is resource-intensive and time-consuming, efficient computational methodologies have become essential. The predominant paradigm formulates DDI prediction as a drug graph-based link prediction task. However, further progress is hindered by two fundamental challenges: (1) lack of high-quality data: most studies rely on small-scale DDI datasets and single-modal drug representations; (2) lack of standardized evaluation: inconsistent scenarios, varied metrics, and diverse baselines. To address the above issues, we propose OpenDDI, a comprehensive benchmark for DDI prediction. Specifically, (1) from the data perspective, OpenDDI unifies 6 widely used DDI datasets and 2 existing forms of drug representation, while additionally contributing 3 new large-scale LLM-augmented datasets and a new multimodal drug representation covering 5 modalities. (2) From the evaluation perspective, OpenDDI unifies 20 SOTA model baselines across 3 downstream tasks, with standardized protocols for data quality, effectiveness, generalization, robustness, and efficiency. Based on OpenDDI, we conduct a comprehensive evaluation and derive 10 valuable insights for DDI prediction while exposing current limitations to provide critical guidance for this rapidly evolving field. Our code is available at https://github.com/xiaoriwuguang/OpenDDI

</details>


### [103] [One Loss to Rule Them All: Marked Time-to-Event for Structured EHR Foundation Models](https://arxiv.org/abs/2602.00541)
*Zilin Jing,Vincent Jeanselme,Yuta Kobayashi,Simon A. Lee,Chao Pang,Aparajita Kashyap,Yanwei Li,Xinzhuo Jiang,Shalmali Joshi*

Main category: cs.LG

TL;DR: ORA：一种新的电子健康记录基础模型预训练目标，联合建模事件时间和相关测量值，相比传统的下一个标记预测方法能产生更具泛化能力的表示。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）数据具有不规则采样、包含离散事件和数值测量的混合特性。现有的基于下一个标记预测的EHR基础模型训练方法无法捕捉EHR的完整结构，需要更好的预训练目标来提升下游任务的泛化能力。

Method: 提出ORA（标记时间到事件）预训练目标，联合建模事件发生时间和相关的连续测量值（如实验室值或治疗剂量），而不是仅仅预测下一个事件标记。

Result: 在多个数据集、下游任务和模型架构上，ORA目标始终比下一个标记预测和忽略连续测量的预训练损失产生更具泛化能力的表示。改进不仅体现在传统分类评估，还包括更好的回归和时间到事件预测性能。

Conclusion: ORA引入了一个新的基础模型家族，更重要的是表明：考虑EHR结构的预训练目标对于扩展下游能力和提升泛化性至关重要。

Abstract: Clinical events captured in Electronic Health Records (EHR) are irregularly sampled and may consist of a mixture of discrete events and numerical measurements, such as laboratory values or treatment dosages. The sequential nature of EHR, analogous to natural language, has motivated the use of next-token prediction to train prior EHR Foundation Models (FMs) over events. However, this training fails to capture the full structure of EHR. We propose ORA, a marked time-to-event pretraining objective that jointly models event timing and associated measurements. Across multiple datasets, downstream tasks, and model architectures, this objective consistently yields more generalizable representations than next-token prediction and pretraining losses that ignore continuous measurements. Importantly, the proposed objective yields improvements beyond traditional classification evaluation, including better regression and time-to-event prediction. Beyond introducing a new family of FMs, our results suggest a broader takeaway: pretraining objectives that account for EHR structure are critical for expanding downstream capabilities and generalizability

</details>


### [104] [Depth, Not Data: An Analysis of Hessian Spectral Bifurcation](https://arxiv.org/abs/2602.00545)
*Shenyang Deng,Boyao Liao,Zhuoli Ouyang,Tianyu Pang,Yaoqing Yang*

Main category: cs.LG

TL;DR: 深度线性网络的Hessian矩阵特征值分布呈现"bulk-and-spike"结构，这种谱分岔源于网络架构而非数据不平衡，主导特征值与批量特征值的比值随网络深度线性增长。


<details>
  <summary>Details</summary>
Motivation: 先前研究将深度神经网络Hessian矩阵的"bulk-and-spike"谱结构归因于数据协方差矩阵的不平衡。本文挑战这一观点，旨在证明这种谱分岔可以纯粹由网络架构引起，与数据不平衡无关。

Method: 分析深度线性网络设置，在数据协方差完全平衡的条件下，证明Hessian矩阵仍表现出分岔特征值结构：一个主导聚类和一个批量聚类。

Result: 发现主导特征值与批量特征值的比值随网络深度线性增长，表明谱间隙受网络架构强烈影响，而不仅仅是数据分布。

Conclusion: 设计深度网络优化算法时，应同时考虑模型架构和数据特性，因为谱结构不仅由数据不平衡决定，也受网络深度等架构因素影响。

Abstract: The eigenvalue distribution of the Hessian matrix plays a crucial role in understanding the optimization landscape of deep neural networks. Prior work has attributed the well-documented ``bulk-and-spike'' spectral structure, where a few dominant eigenvalues are separated from a bulk of smaller ones, to the imbalance in the data covariance matrix. In this work, we challenge this view by demonstrating that such spectral Bifurcation can arise purely from the network architecture, independent of data imbalance.
  Specifically, we analyze a deep linear network setup and prove that, even when the data covariance is perfectly balanced, the Hessian still exhibits a Bifurcation eigenvalue structure: a dominant cluster and a bulk cluster. Crucially, we establish that the ratio between dominant and bulk eigenvalues scales linearly with the network depth. This reveals that the spectral gap is strongly affected by the network architecture rather than solely by data distribution. Our results suggest that both model architecture and data characteristics should be considered when designing optimization algorithms for deep networks.

</details>


### [105] [Contrastive Domain Generalization for Cross-Instrument Molecular Identification in Mass Spectrometry](https://arxiv.org/abs/2602.00547)
*Seunghyun Yoo,Sanghong Kim,Namkyung Yoon,Hwangnam Kim*

Main category: cs.LG

TL;DR: 该论文提出了一种跨模态对齐框架，将质谱数据直接映射到预训练化学语言模型的分子结构嵌入空间，解决了质谱分子识别中的泛化瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法将质谱匹配视为封闭集识别任务，限制了它们对未见分子骨架的泛化能力。质谱峰值与化学结构之间存在语义鸿沟，需要克服这一限制。

Method: 提出跨模态对齐框架，将质谱直接映射到预训练化学语言模型的分子结构嵌入空间，整合物理谱分辨率和分子结构嵌入。

Result: 在严格骨架不相交基准测试中，模型在256路零样本检索中达到42.2%的Top-1准确率，在全局检索设置下表现出强泛化能力。学习到的嵌入空间具有强化学一致性，在5路5样本分子重识别中达到95.4%准确率。

Conclusion: 明确整合物理谱分辨率与分子结构嵌入是解决质谱数据分子识别泛化瓶颈的关键，跨模态对齐框架为分子识别提供了有效的泛化解决方案。

Abstract: Identifying molecules from mass spectrometry (MS) data remains a fundamental challenge due to the semantic gap between physical spectral peaks and underlying chemical structures. Existing deep learning approaches often treat spectral matching as a closed-set recognition task, limiting their ability to generalize to unseen molecular scaffolds. To overcome this limitation, we propose a cross-modal alignment framework that directly maps mass spectra into the chemically meaningful molecular structure embedding space of a pretrained chemical language model. On a strict scaffold-disjoint benchmark, our model achieves a Top-1 accuracy of 42.2% in fixed 256-way zero-shot retrieval and demonstrates strong generalization under a global retrieval setting. Moreover, the learned embedding space demonstrates strong chemical coherence, reaching 95.4% accuracy in 5-way 5-shot molecular re-identification. These results suggest that explicitly integrating physical spectral resolution with molecular structure embedding is key to solving the generalization bottleneck in molecular identification from MS data.

</details>


### [106] [Beyond the Node: Clade-level Selection for Efficient MCTS in Automatic Heuristic Design](https://arxiv.org/abs/2602.00549)
*Kezhao Lai,Yutao Lai,Hai-Lin Liu*

Main category: cs.LG

TL;DR: Clade-AHD：一个用clade级贝叶斯信念替代节点级点估计的高效框架，通过聚合后代评估为Beta分布并进行Thompson采样，解决MCTS在有限计算预算下的过度开发问题


<details>
  <summary>Details</summary>
Motivation: MCTS在基于LLM的自动启发式设计中表现出潜力，但在有限计算预算下存在严重的过度开发倾向，这限制了启发式评估的效果

Method: 提出Clade-AHD框架，用clade级贝叶斯信念替代节点级点估计，将后代评估聚合为Beta分布，并通过Thompson采样在这些信念上进行探索，显式建模不确定性以指导探索

Result: 在复杂组合优化问题上的大量实验表明，Clade-AHD始终优于最先进方法，同时显著降低计算成本

Conclusion: Clade-AHD通过显式建模不确定性来解决MCTS的过度开发问题，在有限计算预算下实现更可靠的决策，是自动启发式设计的高效解决方案

Abstract: While Monte Carlo Tree Search (MCTS) shows promise in Large Language Model (LLM) based Automatic Heuristic Design (AHD), it suffers from a critical over-exploitation tendency under the limited computational budgets required for heuristic evaluation. To address this limitation, we propose Clade-AHD, an efficient framework that replaces node-level point estimates with clade-level Bayesian beliefs. By aggregating descendant evaluations into Beta distributions and performing Thompson Sampling over these beliefs, Clade-AHD explicitly models uncertainty to guide exploration, enabling more reliable decision-making under sparse and noisy evaluations. Extensive experiments on complex combinatorial optimization problems demonstrate that Clade-AHD consistently outperforms state-of-the-art methods while significantly reducing computational cost. The source code is publicly available at: https://github.com/Mriya0306/Clade-AHD.

</details>


### [107] [Forget by Uncertainty: Orthogonal Entropy Unlearning for Quantized Neural Networks](https://arxiv.org/abs/2602.00567)
*Tian Zhang,Yujia Tong,Junhao Dong,Ke Xu,Yuze Wang,Jingling Yuan*

Main category: cs.LG

TL;DR: OEU提出正交熵遗忘框架，通过熵引导最大化遗忘数据预测不确定性实现真正遗忘，使用梯度正交投影消除干扰，在量化模型中实现高效机器遗忘。


<details>
  <summary>Details</summary>
Motivation: 量化神经网络在边缘设备部署与GDPR等隐私法规结合，迫切需要量化模型的机器遗忘能力。现有方法存在关键问题：通过训练模型记忆错误标签实现遗忘，混淆了遗忘与错误记忆；使用标量梯度重加权无法解决梯度间的方向冲突。

Method: 提出正交熵遗忘框架(OEU)：1) 熵引导遗忘：最大化遗忘数据的预测不确定性，实现真正遗忘而非自信错误预测；2) 梯度正交投影：将遗忘梯度投影到保留梯度的正交补空间，在一阶近似下提供效用保持的理论保证。

Result: 大量实验表明，OEU在遗忘效果和保留准确性方面均优于现有方法。

Conclusion: OEU框架成功解决了量化神经网络中的机器遗忘问题，通过熵引导和梯度正交投影实现了真正的遗忘效果，同时保持了模型效用。

Abstract: The deployment of quantized neural networks on edge devices, combined with privacy regulations like GDPR, creates an urgent need for machine unlearning in quantized models. However, existing methods face critical challenges: they induce forgetting by training models to memorize incorrect labels, conflating forgetting with misremembering, and employ scalar gradient reweighting that cannot resolve directional conflicts between gradients. We propose OEU, a novel Orthogonal Entropy Unlearning framework with two key innovations: 1) Entropy-guided unlearning maximizes prediction uncertainty on forgotten data, achieving genuine forgetting rather than confident misprediction, and 2) Gradient orthogonal projection eliminates interference by projecting forgetting gradients onto the orthogonal complement of retain gradients, providing theoretical guarantees for utility preservation under first-order approximation. Extensive experiments demonstrate that OEU outperforms existing methods in both forgetting effectiveness and retain accuracy.

</details>


### [108] [When Classes Evolve: A Benchmark and Framework for Stage-Aware Class-Incremental Learning](https://arxiv.org/abs/2602.00573)
*Zheng Zhang,Tao Hu,Xueheng Li,Yang Wang,Rui Li,Jie Zhang,Chengjun Xie*

Main category: cs.LG

TL;DR: 论文提出了Stage-CIL范式，解决类别增量学习中类内形态演化问题，并提出了STAGE方法和Stage-Bench基准数据集。


<details>
  <summary>Details</summary>
Motivation: 传统CIL方法假设类别形态静态，忽略了类内演化现象（如幼虫变蝴蝶），导致模型需要同时处理类间区分和类内形态适应。

Method: 提出Stage-CIL范式，将每个类别按不同形态阶段渐进学习；提出STAGE方法，在固定大小记忆池中学习抽象可转移的演化模式，解耦语义身份和变换动态。

Result: STAGE方法在Stage-Bench（10个领域、2阶段数据集）上显著优于现有SOTA方法，有效同时解决类间区分和类内形态适应问题。

Conclusion: 类内形态演化是CIL的重要挑战，Stage-CIL范式和STAGE方法为此提供了系统解决方案，在多个领域表现出色。

Abstract: Class-Incremental Learning (CIL) aims to sequentially learn new classes while mitigating catastrophic forgetting of previously learned knowledge. Conventional CIL approaches implicitly assume that classes are morphologically static, focusing primarily on preserving previously learned representations as new classes are introduced. However, this assumption neglects intra-class evolution: a phenomenon wherein instances of the same semantic class undergo significant morphological transformations, such as a larva turning into a butterfly. Consequently, a model must both discriminate between classes and adapt to evolving appearances within a single class. To systematically address this challenge, we formalize Stage-Aware CIL (Stage-CIL), a paradigm in which each class is learned progressively through distinct morphological stages. To facilitate rigorous evaluation within this paradigm, we introduce the Stage-Bench, a 10-domain, 2-stages dataset and protocol that jointly measure inter- and intra-class forgetting. We further propose STAGE, a novel method that explicitly learns abstract and transferable evolution patterns within a fixed-size memory pool. By decoupling semantic identity from transformation dynamics, STAGE enables accurate prediction of future morphologies based on earlier representations. Extensive empirical evaluation demonstrates that STAGE consistently and substantially outperforms existing state-of-the-art approaches, highlighting its effectiveness in simultaneously addressing inter-class discrimination and intra-class morphological adaptation.

</details>


### [109] [Data Distribution as a Lever for Guiding Optimizers Toward Superior Generalization in LLMs](https://arxiv.org/abs/2602.00576)
*Tushaar Gangavarapu,Jiping Li,Christopher Vattheuer,Zhangyang Wang,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: 通过修改训练数据分布（上采样或增强后期学习的样本）来降低简单性偏置，从而提升大语言模型的泛化性能，在多个LLM上实现了最高18%的相对准确率提升。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过修改训练数据分布来引导优化器找到具有更好泛化性能的解。SAM优化器虽然泛化性能好但计算成本过高，需要寻找更实用的方法来获得类似效果。

Method: 1. 理论分析带多头线性自注意力的上下文线性回归模型；2. 比较GD和SAM的训练动态；3. 发现SAM降低简单性偏置；4. 提出通过上采样或增强后期学习样本来修改训练数据分布，以降低简单性偏置。

Result: 1. SAM首次被证明能降低简单性偏置；2. 降低简单性偏置是SAM改善泛化的关键因素；3. 修改训练数据分布的方法在Phi2-2.7B、Llama3.2-1B、Gemma3-1B-PT、Qwen3-0.6B-Base等多个LLM上有效；4. 在数学推理任务上使用AdamW和Muon微调时，相对准确率提升最高达18%。

Conclusion: 通过修改训练数据分布来降低简单性偏置是一种有效提升LLM泛化性能的实用方法，避免了SAM的高计算成本，在多个模型和任务上验证了其有效性。

Abstract: Can modifying the training data distribution guide optimizers toward solutions with improved generalization when training large language models (LLMs)? In this work, we theoretically analyze an in-context linear regression model with multi-head linear self-attention, and compare the training dynamics of two gradient based optimizers, namely gradient descent (GD) and sharpness-aware minimization (SAM), the latter exhibiting superior generalization properties but is prohibitively expensive for training even medium-sized LLMs. We show, for the first time, that SAM induces a lower simplicity bias (SB)-the tendency of an optimizer to preferentially learn simpler features earlier in training-and identify this reduction as a key factor underlying its improved generalization performance. Motivated by this insight, we demonstrate that altering the training data distribution by upsampling or augmenting examples learned later in training similarly reduces SB and leads to improved generalization. Our extensive experiments show that our strategy improves the performance of multiple LLMs-including Phi2-2.7B , Llama3.2-1B, Gemma3-1B-PT, and Qwen3-0.6B-Base-achieving relative accuracy gains up to 18% when fine-tuned with AdamW and Muon on mathematical reasoning tasks.

</details>


### [110] [Sparsity-Aware Unlearning for Large Language Models](https://arxiv.org/abs/2602.00577)
*Yuze Wang,Yujia Tong,Ke Xu,Jingling Yuan,Jiawei Jiang,Chuang Hu*

Main category: cs.LG

TL;DR: 提出SAU方法解决稀疏大语言模型遗忘效果下降问题，通过梯度掩码和重要性重分配实现有效遗忘同时保持模型效用


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练时会记忆敏感信息，带来隐私风险。机器遗忘技术可以选择性删除这些信息，但现有方法针对密集模型设计，忽略了模型稀疏化这一LLM高效部署的关键技术。研究发现稀疏模型的遗忘效果显著下降。

Method: 提出稀疏感知遗忘(SAU)方法：1) 通过梯度掩码将更新重定向到存活的权重，将遗忘目标与稀疏化目标解耦；2) 结合重要性感知重分配来补偿被剪枝的参数。

Result: 大量实验表明，SAU在稀疏LLMs上显著优于现有方法，能够实现有效遗忘同时保持模型效用。

Conclusion: SAU解决了稀疏LLMs中遗忘效果下降的问题，通过创新的梯度掩码和重要性重分配机制，为高效LLM部署中的隐私保护提供了有效解决方案。

Abstract: Large Language Models (LLMs) inevitably memorize sensitive information during training, posing significant privacy risks. Machine unlearning has emerged as a promising solution to selectively remove such information without full retraining. However, existing methods are designed for dense models and overlook model sparsification-an essential technique for efficient LLM deployment. We find that unlearning effectiveness degrades substantially on sparse models. Through empirical analysis, we reveal that this degradation occurs because existing unlearning methods require updating all parameters, yet sparsification prunes substantial weights to zero, fundamentally limiting the model's forgetting capacity. To address this challenge, we propose Sparsity-Aware Unlearning (SAU), which decouples unlearning from sparsification objectives through gradient masking that redirects updates to surviving weights, combined with importance-aware redistribution to compensate for pruned parameters. Extensive experiments demonstrate that SAU significantly outperforms existing methods on sparse LLMs, achieving effective forgetting while preserving model utility.

</details>


### [111] [Bridging Time and Frequency: A Joint Modeling Framework for Irregular Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.00582)
*Xiangfei Qiu,Kangjia Yan,Xvyuan Liu,Xingjian Wu,Jilin Hu*

Main category: cs.LG

TL;DR: TFMixer是一个用于不规则多元时间序列预测的联合时频建模框架，通过可学习的非均匀离散傅里叶变换提取频谱表示，并结合基于查询的补丁混合机制进行局部时间建模，实现了最先进的预测性能。


<details>
  <summary>Details</summary>
Motivation: 不规则多元时间序列预测面临非均匀采样和变量异步性的挑战，这些不规则性违反了标准模型的等距假设，阻碍了局部时间建模，并使经典频域方法无法有效捕捉全局周期性结构。

Method: TFMixer包含全局频率模块（使用可学习的非均匀离散傅里叶变换直接从不规则时间戳提取频谱表示）和局部时间模块（引入基于查询的补丁混合机制自适应聚合信息性时间补丁，缓解信息密度不平衡），最后融合时域和频域表示生成预测，并利用逆NUDFT进行显式季节性外推。

Result: 在真实世界数据集上的大量实验表明，TFMixer实现了最先进的性能。

Conclusion: TFMixer通过联合时频建模有效解决了不规则多元时间序列预测的挑战，为非均匀采样和异步变量提供了有效的解决方案。

Abstract: Irregular multivariate time series forecasting (IMTSF) is challenging due to non-uniform sampling and variable asynchronicity. These irregularities violate the equidistant assumptions of standard models, hindering local temporal modeling and rendering classical frequency-domain methods ineffective for capturing global periodic structures. To address this challenge, we propose TFMixer, a joint time-frequency modeling framework for IMTS forecasting. Specifically, TFMixer incorporates a Global Frequency Module that employs a learnable Non-Uniform Discrete Fourier Transform (NUDFT) to directly extract spectral representations from irregular timestamps. In parallel, the Local Time Module introduces a query-based patch mixing mechanism to adaptively aggregate informative temporal patches and alleviate information density imbalance. Finally, TFMixer fuses the time-domain and frequency-domain representations to generate forecasts and further leverages inverse NUDFT for explicit seasonal extrapolation. Extensive experiments on real-world datasets demonstrate the state--of-the-art performance of TFMixer.

</details>


### [112] [Safe Langevin Soft Actor Critic](https://arxiv.org/abs/2602.00587)
*Mahesh Keswani,Samyak Jain,Raunak P. Bhattacharyya*

Main category: cs.LG

TL;DR: SL-SAC是一种安全的强化学习算法，通过参数空间探索和分布风险控制解决奖励与安全平衡问题，在Safety-Gymnasium基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 约束强化学习中奖励与安全的平衡面临两大挑战：尖锐价值最小值导致的泛化能力差，以及重尾风险分布处理不足。现有方法难以同时解决这两个问题。

Method: 提出Safe Langevin Soft Actor-Critic (SL-SAC)算法，包含三个核心机制：1) 使用自适应随机梯度朗之万动力学(aSGLD)增强奖励批评器的集成多样性；2) 通过隐式分位数网络(IQN)和条件风险价值(CVaR)优化进行分布成本估计；3) 基于经验CVaR的反应性拉格朗日松弛方案自适应约束执行。

Result: 在Safety-Gymnasium基准测试中，SL-SAC在10个任务中的7个实现了最低成本，同时保持竞争力回报。在速度任务中成本降低了19-63%，优于现有最先进基线方法。

Conclusion: SL-SAC通过参数空间探索和分布风险控制的创新结合，有效解决了约束强化学习中的泛化问题和重尾风险处理，为安全强化学习提供了新的解决方案。

Abstract: Balancing reward and safety in constrained reinforcement learning remains challenging due to poor generalization from sharp value minima and inadequate handling of heavy-tailed risk distribution. We introduce Safe Langevin Soft Actor-Critic (SL-SAC), a principled algorithm that addresses both issues through parameter-space exploration and distributional risk control. Our approach combines three key mechanisms: (1) Adaptive Stochastic Gradient Langevin Dynamics (aSGLD) for reward critics, promoting ensemble diversity and escape from poor optima; (2) distributional cost estimation via Implicit Quantile Networks (IQN) with Conditional Value-at-Risk (CVaR) optimization for tail-risk mitigation; and (3) a reactive Lagrangian relaxation scheme that adapts constraint enforcement based on the empirical CVaR of episodic costs. We provide theoretical guarantees on CVaR estimation error and demonstrate that CVaR-based Lagrange updates yield stronger constraint violation signals than expected-cost updates. On Safety-Gymnasium benchmarks, SL-SAC achieves the lowest cost in 7 out of 10 tasks while maintaining competitive returns, with cost reductions of 19-63% in velocity tasks compared to state-of-the-art baselines.

</details>


### [113] [SEER: Transformer-based Robust Time Series Forecasting via Automated Patch Enhancement and Replacement](https://arxiv.org/abs/2602.00589)
*Xiangfei Qiu,Xvyuan Liu,Tianen Shen,Xingjian Wu,Hanyin Cheng,Bin Yang,Jilin Hu*

Main category: cs.LG

TL;DR: SEER是一个鲁棒的时间序列预测框架，通过可学习的补丁替换机制动态过滤低质量补丁，提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于补丁的时间序列预测方法通常使用所有补丁，但现实数据存在缺失值、分布偏移、异常和噪声等问题，导致某些补丁包含低质量信息，影响预测结果。

Method: 1. 增强嵌入模块：使用混合专家架构改进补丁表示，通过通道自适应感知机制获得序列级标记表示。2. 可学习补丁替换模块：两阶段过程：动态过滤机制消除负向补丁标记；替换注意力模块用全局序列级标记替代低质量补丁，并通过因果注意力机制精化表示。

Result: 综合实验结果表明SEER达到了最先进的性能。

Conclusion: SEER框架通过动态选择高质量补丁并替换低质量补丁，有效提升了时间序列预测的鲁棒性和准确性。

Abstract: Time series forecasting is important in many fields that require accurate predictions for decision-making. Patching techniques, commonly used and effective in time series modeling, help capture temporal dependencies by dividing the data into patches. However, existing patch-based methods fail to dynamically select patches and typically use all patches during the prediction process. In real-world time series, there are often low-quality issues during data collection, such as missing values, distribution shifts, anomalies and white noise, which may cause some patches to contain low-quality information, negatively impacting the prediction results. To address this issue, this study proposes a robust time series forecasting framework called SEER. Firstly, we propose an Augmented Embedding Module, which improves patch-wise representations using a Mixture-of-Experts (MoE) architecture and obtains series-wise token representations through a channel-adaptive perception mechanism. Secondly, we introduce a Learnable Patch Replacement Module, which enhances forecasting robustness and model accuracy through a two-stage process: 1) a dynamic filtering mechanism eliminates negative patch-wise tokens; 2) a replaced attention module substitutes the identified low-quality patches with global series-wise token, further refining their representations through a causal attention mechanism. Comprehensive experimental results demonstrate the SOTA performance of SEER.

</details>


### [114] [Kernelized Edge Attention: Addressing Semantic Attention Blurring in Temporal Graph Neural Networks](https://arxiv.org/abs/2602.00596)
*Govind Waghmare,Srini Rohan Gujulla Leel,Nikhil Tumbde,Sumedh B G,Sonia Gupta,Srikanta Bedathur*

Main category: cs.LG

TL;DR: KEAT提出了一种新的注意力机制，通过连续时间核函数调制边特征，解决了TGNN中节点和边特征纠缠导致的语义注意力模糊问题，显著提升了动态图链接预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有TGNN模型在计算注意力时，将节点和边特征纠缠在一起，但节点嵌入演化缓慢（聚合长期结构上下文），而边特征反映瞬时的带时间戳交互。这种不匹配导致语义注意力模糊，注意力权重无法区分缓慢漂移的节点状态和快速变化的信息丰富的边交互，限制了模型捕捉细粒度时间依赖和计算时间相关性的透明度。

Method: KEAT（Kernelized Edge Attention for Temporal Graphs）是一种新颖的注意力公式，使用一系列连续时间核函数（包括拉普拉斯核、RBF核和可学习的MLP变体）来调制边特征。该方法保留了节点和边的不同角色，并能与Transformer风格（如DyGFormer）和消息传递（如TGN）架构无缝集成。

Result: 在链接预测任务上，KEAT相比最近的DyGFormer实现了高达18%的MRR提升，相比TGN提升了7%。这使得TGNN能够进行更准确、可解释和具有时间感知的消息传递。

Conclusion: KEAT通过解耦节点和边的注意力计算，解决了TGNN中的语义注意力模糊问题，显著提升了动态图建模的性能和可解释性，为时间感知的消息传递提供了更有效的框架。

Abstract: Temporal Graph Neural Networks (TGNNs) aim to capture the evolving structure and timing of interactions in dynamic graphs. Although many models incorporate time through encodings or architectural design, they often compute attention over entangled node and edge representations, failing to reflect their distinct temporal behaviors. Node embeddings evolve slowly as they aggregate long-term structural context, while edge features reflect transient, timestamped interactions (e.g. messages, trades, or transactions). This mismatch results in semantic attention blurring, where attention weights cannot distinguish between slowly drifting node states and rapidly changing, information-rich edge interactions. As a result, models struggle to capture fine-grained temporal dependencies and provide limited transparency into how temporal relevance is computed. This paper introduces KEAT (Kernelized Edge Attention for Temporal Graphs), a novel attention formulation that modulates edge features using a family of continuous-time kernels, including Laplacian, RBF, and learnable MLP variant. KEAT preserves the distinct roles of nodes and edges, and integrates seamlessly with both Transformer-style (e.g., DyGFormer) and message-passing (e.g., TGN) architectures. It achieves up to 18% MRR improvement over the recent DyGFormer and 7% over TGN on link prediction tasks, enabling more accurate, interpretable and temporally aware message passing in TGNNs.

</details>


### [115] [Direct Preference Optimization with Rating Information: Practical Algorithms and Provable Gains](https://arxiv.org/abs/2602.00603)
*Luca Viano,Ruida Zhou,Yifan Sun,Mahdi Namazifar,Volkan Cevher,Shoham Sabach,Mohammad Ghavamzadeh*

Main category: cs.LG

TL;DR: 本文提出基于评分差距信息改进的偏好优化算法，相比传统DPO方法具有更快的统计收敛速度，同时对评分差距的不准确性具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统DPO算法仅使用成对偏好反馈，这种反馈形式虽然数据收集容易，但存在模糊性（无法区分偏好程度差异）。评分差距信息能提供更多关于响应质量的信息，但现有算法未能充分利用这一信息。

Method: 设计新的算法利用评分差距信息（即被选响应比被拒绝响应好多少的程度），这些算法在准确评分差距信息下能获得比DPO更快的统计收敛速度。算法设计考虑了评分差距可能不准确的情况。

Result: 理论证明和实验验证表明：1）在准确评分差距信息下，新算法比DPO具有更快的统计收敛速度；2）算法对评分差距的不准确性具有鲁棒性；3）在多种LLM和评估基准上，新方法表现优于多种DPO风格算法。

Conclusion: 利用评分差距信息可以显著改进偏好优化算法的性能，新算法在保持DPO数据收集优势的同时，通过利用额外信息实现了更好的对齐效果和统计效率。

Abstract: The class of direct preference optimization (DPO) algorithms has emerged as a promising approach for solving the alignment problem in foundation models. These algorithms work with very limited feedback in the form of pairwise preferences and fine-tune models to align with these preferences without explicitly learning a reward model. While the form of feedback used by these algorithms makes the data collection process easy and relatively more accurate, its ambiguity in terms of the quality of responses could have negative implications. For example, it is not clear if a decrease (increase) in the likelihood of preferred (dispreferred) responses during the execution of these algorithms could be interpreted as a positive or negative phenomenon. In this paper, we study how to design algorithms that can leverage additional information in the form of rating gap, which informs the learner how much the chosen response is better than the rejected one. We present new algorithms that can achieve faster statistical rates than DPO in presence of accurate rating gap information. Moreover, we theoretically prove and empirically show that the performance of our algorithms is robust to inaccuracy in rating gaps. Finally, we demonstrate the solid performance of our methods in comparison to a number of DPO-style algorithms across a wide range of LLMs and evaluation benchmarks.

</details>


### [116] [Actor-Dual-Critic Dynamics for Zero-sum and Identical-Interest Stochastic Games](https://arxiv.org/abs/2602.00606)
*Ahmed Said Donmez,Yuksel Arslantas,Muhammed O. Sayin*

Main category: cs.LG

TL;DR: 提出了一种用于随机博弈的独立、基于收益的学习框架，该框架无需模型、与博弈类型无关且无需梯度，通过快速和慢速批评家实现最佳响应式学习，在零和博弈和共同利益博弈中都能收敛到均衡。


<details>
  <summary>Details</summary>
Motivation: 现有随机博弈学习算法通常需要模型知识、梯度信息或中心化协调，本文旨在开发一种完全去中心化、基于收益且无需模型和梯度的学习框架，适用于多种博弈类型。

Method: 采用最佳响应式演员-批评家架构，包含两个批评家：快速批评家基于有限信息直观响应观察到的收益，慢速批评家审慎逼近底层动态规划问题解。学习过程通过平滑最佳响应进行非均衡适应。

Result: 理论证明在无限时域下，该算法能在两智能体零和博弈和多智能体共同利益随机博弈中收敛到（近似）均衡。实验验证了算法在这两类博弈中的鲁棒性和有效性。

Conclusion: 这是首个在零和博弈和共同利益博弈中都具有理论保证的完全去中心化、基于收益的学习算法，为随机博弈学习提供了模型无关、博弈无关且无需梯度的解决方案。

Abstract: We propose a novel independent and payoff-based learning framework for stochastic games that is model-free, game-agnostic, and gradient-free. The learning dynamics follow a best-response-type actor-critic architecture, where agents update their strategies (actors) using feedback from two distinct critics: a fast critic that intuitively responds to observed payoffs under limited information, and a slow critic that deliberatively approximates the solution to the underlying dynamic programming problem. Crucially, the learning process relies on non-equilibrium adaptation through smoothed best responses to observed payoffs. We establish convergence to (approximate) equilibria in two-agent zero-sum and multi-agent identical-interest stochastic games over an infinite horizon. This provides one of the first payoff-based and fully decentralized learning algorithms with theoretical guarantees in both settings. Empirical results further validate the robustness and effectiveness of the proposed approach across both classes of games.

</details>


### [117] [Rethinking Zero-Shot Time Series Classification: From Task-specific Classifiers to In-Context Inference](https://arxiv.org/abs/2602.00620)
*Juntao Fang,Shifeng Xie,Shengbin Nie,Yuhui Ling,Yuming Liu,Zijian Li,Keli Zhang,Lujia Pan,Themis Palpanas,Ruichu Cai*

Main category: cs.LG

TL;DR: TIC-FM：一种用于时间序列基础模型零样本评估的上下文学习框架，无需参数更新，通过单次前向传播完成分类


<details>
  <summary>Details</summary>
Motivation: 传统时间序列基础模型零样本评估方法使用冻结编码器加任务特定分类器，这违反了零样本部署的训练免费前提，并因分类器相关训练选择引入评估偏差

Method: 提出TIC-FM框架，将标记训练集作为上下文，通过时间序列编码器、轻量级投影适配器和分割掩码潜在记忆Transformer，在单次前向传播中预测所有测试实例标签

Result: 在128个UCR数据集上表现出强大的准确性，在极端低标签情况下获得一致的性能提升

Conclusion: TIC-FM提供了一种真正的训练免费零样本评估方法，理论上证明上下文推理可以替代训练分类器，并在单次前向传播中模拟基于梯度的分类器训练

Abstract: The zero-shot evaluation of time series foundation models (TSFMs) for classification typically uses a frozen encoder followed by a task-specific classifier. However, this practice violates the training-free premise of zero-shot deployment and introduces evaluation bias due to classifier-dependent training choices. To address this issue, we propose TIC-FM, an in-context learning framework that treats the labeled training set as context and predicts labels for all test instances in a single forward pass, without parameter updates. TIC-FM pairs a time series encoder and a lightweight projection adapter with a split-masked latent memory Transformer. We further provide theoretical justification that in-context inference can subsume trained classifiers and can emulate gradient-based classifier training within a single forward pass. Experiments on 128 UCR datasets show strong accuracy, with consistent gains in the extreme low-label situation, highlighting training-free transfer

</details>


### [118] [MoDEx: Mixture of Depth-specific Experts for Multivariate Long-term Time Series Forecasting](https://arxiv.org/abs/2602.00624)
*Hyekyung Yoon,Minhyuk Lee,Imseung Park,Myungjoo Kang*

Main category: cs.LG

TL;DR: 提出MoDEx框架，通过深度特定专家混合替代复杂骨干网络，在多元长期时间序列预测中实现SOTA性能，同时大幅减少参数和计算资源。


<details>
  <summary>Details</summary>
Motivation: 现有长期时间序列预测（LTSF）采用嵌入-骨干精化-长期预测的三阶段流程，但各骨干层的行为研究不足。需要量化每个时间点对层特征的贡献，以理解深度特定建模特性。

Method: 引入层敏感度（梯度度量），量化时间点对层特征的贡献。基于此提出MoDEx：轻量级深度特定专家混合，用深度特定MLP专家替代复杂骨干网络。

Result: 在7个真实世界基准测试中达到SOTA准确率，78%情况下排名第一，同时显著减少参数和计算资源。能无缝集成到Transformer变体中，持续提升其性能。

Conclusion: MoDEx作为高效高性能LTSF框架，展示了深度特定建模的重要性，提供了一种轻量级但强大的替代复杂骨干网络的方法，具有良好泛化能力。

Abstract: Multivariate long-term time series forecasting (LTSF) supports critical applications such as traffic-flow management, solar-power scheduling, and electricity-transformer monitoring. The existing LTSF paradigms follow a three-stage pipeline of embedding, backbone refinement, and long-horizon prediction. However, the behaviors of individual backbone layers remain underexplored. We introduce layer sensitivity, a gradient-based metric inspired by GradCAM and effective receptive field theory, which quantifies both positive and negative contributions of each time point to a layer's latent features. Applying this metric to a three-layer MLP backbone reveals depth-specific specialization in modeling temporal dynamics in the input sequence. Motivated by these insights, we propose MoDEx, a lightweight Mixture of Depth-specific Experts, which replaces complex backbones with depth-specific MLP experts. MoDEx achieves state-of-the-art accuracy on seven real-world benchmarks, ranking first in 78 percent of cases, while using significantly fewer parameters and computational resources. It also integrates seamlessly into transformer variants, consistently boosting their performance and demonstrating robust generalizability as an efficient and high-performance LTSF framework.

</details>


### [119] [From Associations to Activations: Comparing Behavioral and Hidden-State Semantic Geometry in LLMs](https://arxiv.org/abs/2602.00628)
*Louis Schiekiera,Max Zimmer,Christophe Roux,Sebastian Pokutta,Fritz Günther*

Main category: cs.LG

TL;DR: 通过心理语言学实验行为数据可以部分恢复LLM隐藏状态几何结构，强制选择任务比自由联想任务更能反映内部语义几何


<details>
  <summary>Details</summary>
Motivation: 研究LLM在心理语言学实验中的行为数据能否揭示其隐藏状态的几何结构，探索行为测量能否恢复内部语义表示

Method: 在8个指令调优的Transformer模型上运行两种实验范式（相似性强制选择和自由联想），使用5000词词汇表收集1750万+试验构建行为相似性矩阵，通过表征相似性分析比较行为几何与层间隐藏状态相似性

Result: 强制选择行为与隐藏状态几何的对齐程度显著高于自由联想；在保留词回归中，行为相似性（特别是强制选择）能够预测未见过的隐藏状态相似性，超越了词汇基线和跨模型共识

Conclusion: 仅基于行为的测量保留了关于内部语义几何的可恢复信息，行为任务有能力揭示隐藏的认知状态

Abstract: We investigate the extent to which an LLM's hidden-state geometry can be recovered from its behavior in psycholinguistic experiments. Across eight instruction-tuned transformer models, we run two experimental paradigms -- similarity-based forced choice and free association -- over a shared 5,000-word vocabulary, collecting 17.5M+ trials to build behavior-based similarity matrices. Using representational similarity analysis, we compare behavioral geometries to layerwise hidden-state similarity and benchmark against FastText, BERT, and cross-model consensus. We find that forced-choice behavior aligns substantially more with hidden-state geometry than free association. In a held-out-words regression, behavioral similarity (especially forced choice) predicts unseen hidden-state similarities beyond lexical baselines and cross-model consensus, indicating that behavior-only measurements retain recoverable information about internal semantic geometry. Finally, we discuss implications for the ability of behavioral tasks to uncover hidden cognitive states.

</details>


### [120] [Equilibrium of Feasible Zone and Uncertain Model in Safe Exploration](https://arxiv.org/abs/2602.00636)
*Yujie Yang,Zhilong Zheng,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 论文首次揭示了安全探索的目标是在可行区域与环境模型之间找到平衡，提出了SEE框架，通过交替寻找最大可行区域和最小不确定模型实现零约束违反的安全探索。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习中的安全探索方法虽然普遍采用限制探索到可行区域来确保安全，但存在两个关键未解问题：通过探索能达到的最大可行区域是什么？如何识别这个区域？本文旨在首次回答这些问题。

Method: 提出安全平衡探索（SEE）框架，交替进行两个过程：1）寻找最大可行区域；2）寻找最小不确定模型。使用不确定模型的图表示，证明该方法能单调细化模型、单调扩展可行区域，并收敛到安全探索的平衡点。

Result: 在经典控制任务上的实验表明，SEE算法能够成功扩展可行区域且零约束违反，在几次迭代内就能达到安全探索的平衡状态。

Conclusion: 安全探索的目标是在可行区域与环境模型之间找到平衡，两者相互依赖：更大的可行区域带来更准确的环境模型，更准确的模型又能支持探索更大的区域。SEE框架首次实现了这种平衡导向的安全探索。

Abstract: Ensuring the safety of environmental exploration is a critical problem in reinforcement learning (RL). While limiting exploration to a feasible zone has become widely accepted as a way to ensure safety, key questions remain unresolved: what is the maximum feasible zone achievable through exploration, and how can it be identified? This paper, for the first time, answers these questions by revealing that the goal of safe exploration is to find the equilibrium between the feasible zone and the environment model. This conclusion is based on the understanding that these two components are interdependent: a larger feasible zone leads to a more accurate environment model, and a more accurate model, in turn, enables exploring a larger zone. We propose the first equilibrium-oriented safe exploration framework called safe equilibrium exploration (SEE), which alternates between finding the maximum feasible zone and the least uncertain model. Using a graph formulation of the uncertain model, we prove that the uncertain model obtained by SEE is monotonically refined, the feasible zones monotonically expand, and both converge to the equilibrium of safe exploration. Experiments on classic control tasks show that our algorithm successfully expands the feasible zones with zero constraint violation, and achieves the equilibrium of safe exploration within a few iterations.

</details>


### [121] [Combinatorial Bandit Bayesian Optimization for Tensor Outputs](https://arxiv.org/abs/2602.00640)
*Jingru Huang,Haijie Xu,Jie Guo,Manrui Jiang,Chen Zhang*

Main category: cs.LG

TL;DR: 提出两种张量输出贝叶斯优化方法：TOGP用于完整张量输出优化，CBBO用于组合选择部分输出的场景，均具有理论保证和实验优势。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯优化方法未处理张量输出函数，且实际应用中常需从张量输出中选择子集贡献目标函数，需要新的方法填补这一空白。

Method: 1) 提出张量输出高斯过程(TOGP)作为代理模型，使用两类张量输出核捕捉结构依赖；2) 基于TOGP开发UCB采集函数；3) 针对组合选择问题提出CBBO方法，扩展TOGP处理部分观测，设计CMAB-UCB2准则选择查询点和最优输出子集。

Result: 建立了两种方法的理论遗憾界，确保次线性性能；大量合成和真实世界实验证明了方法的优越性。

Conclusion: 成功填补了贝叶斯优化在张量输出函数领域的空白，提出的TOGP和CBBO方法在理论和实验上均表现出色，为复杂优化问题提供了有效解决方案。

Abstract: Bayesian optimization (BO) has been widely used to optimize expensive and black-box functions across various domains. Existing BO methods have not addressed tensor-output functions. To fill this gap, we propose a novel tensor-output BO method. Specifically, we first introduce a tensor-output Gaussian process (TOGP) with two classes of tensor-output kernels as a surrogate model of the tensor-output function, which can effectively capture the structural dependencies within the tensor. Based on it, we develop an upper confidence bound (UCB) acquisition function to select the queried points. Furthermore, we introduce a more complex and practical problem setting, named combinatorial bandit Bayesian optimization (CBBO), where only a subset of the outputs can be selected to contribute to the objective function. To tackle this, we propose a tensor-output CBBO method, which extends TOGP to handle partially observed outputs, and accordingly design a novel combinatorial multi-arm bandit-UCB2 (CMAB-UCB2) criterion to sequentially select both the queried points and the optimal output subset. Theoretical regret bounds for the two methods are established, ensuring their sublinear performance. Extensive synthetic and real-world experiments demonstrate their superiority.

</details>


### [122] [CoRe-Fed: Bridging Collaborative and Representation Fairness via Federated Embedding Distillation](https://arxiv.org/abs/2602.00647)
*Noorain Mukhtiar,Adnan Mahmood,Quan Z. Sheng*

Main category: cs.LG

TL;DR: CoRe-Fed是一个联邦学习公平性优化框架，通过嵌入对齐和公平聚合解决表示偏差和协作偏差问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中存在数据分布异构和参与不平等导致的性能差异问题，具体表现为表示偏差（客户端表示不一致）和协作偏差（聚合贡献不公平），这些偏差降低了模型性能和泛化能力。

Method: 提出CoRe-Fed统一优化框架：1）嵌入对齐机制促进本地与全局嵌入的语义一致性；2）基于奖励惩罚的动态聚合策略，根据参与历史和嵌入对齐调整客户端权重。

Result: 在多种模型和数据集上的实验表明，CoRe-Fed在公平性和模型性能方面均优于现有基线算法。

Conclusion: CoRe-Fed通过联合优化协作公平和表示公平，有效缓解了联邦学习中的偏差问题，提升了整体性能。

Abstract: With the proliferation of distributed data sources, Federated Learning (FL) has emerged as a key approach to enable collaborative intelligence through decentralized model training while preserving data privacy. However, conventional FL algorithms often suffer from performance disparities across clients caused by heterogeneous data distributions and unequal participation, which leads to unfair outcomes. Specifically, we focus on two core fairness challenges, i.e., representation bias, arising from misaligned client representations, and collaborative bias, stemming from inequitable contribution during aggregation, both of which degrade model performance and generalizability. To mitigate these disparities, we propose CoRe-Fed, a unified optimization framework that bridges collaborative and representation fairness via embedding-level regularization and fairness-aware aggregation. Initially, an alignment-driven mechanism promotes semantic consistency between local and global embeddings to reduce representational divergence. Subsequently, a dynamic reward-penalty-based aggregation strategy adjusts each client's weight based on participation history and embedding alignment to ensure contribution-aware aggregation. Extensive experiments across diverse models and datasets demonstrate that CoRe-Fed improves both fairness and model performance over the state-of-the-art baseline algorithms.

</details>


### [123] [PHAT: Modeling Period Heterogeneity for Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.00654)
*Jiaming Ma,Guanjun Wang,Qihe Huang,Sheng Huang,Haofeng Ma,Zhengyang Zhou,Pengkun Wang,Binwu Wang,Yang Wang*

Main category: cs.LG

TL;DR: PHAT是一种针对多元时间序列预测的Transformer模型，专门处理现实数据中不同变量具有不同且动态变化周期的周期性异质性问题，通过周期性桶结构和正负注意力机制显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有多元时间序列预测模型虽然能很好建模周期性，但忽略了现实数据中常见的周期性异质性——即不同变量具有不同且动态变化的周期。这种异质性会导致模型性能下降，需要专门的方法来处理。

Method: PHAT将多元输入组织成三维"周期性桶"张量，维度分别对应：具有相似周期性的变量组、按相位对齐的时间步、周期内的偏移。通过限制桶内交互和屏蔽跨桶连接来避免不一致周期的干扰。提出正负注意力机制，从周期性对齐和周期性偏差两个角度捕捉周期性依赖，并将对齐注意力分数分解为正负分量，用编码周期性先验的调制项约束注意力机制。

Result: 在14个真实世界数据集上与18个基线方法进行综合评估，结果显示PHAT显著优于现有方法，实现了极具竞争力的预测性能。

Conclusion: PHAT通过有效处理周期性异质性，在多元时间序列预测任务中取得了显著改进，为处理现实世界中复杂的周期性模式提供了有效解决方案。

Abstract: While existing multivariate time series forecasting models have advanced significantly in modeling periodicity, they largely neglect the periodic heterogeneity common in real-world data, where variates exhibit distinct and dynamically changing periods. To effectively capture this periodic heterogeneity, we propose PHAT (Period Heterogeneity-Aware Transformer). Specifically, PHAT arranges multivariate inputs into a three-dimensional "periodic bucket" tensor, where the dimensions correspond to variate group characteristics with similar periodicity, time steps aligned by phase, and offsets within the period. By restricting interactions within buckets and masking cross-bucket connections, PHAT effectively avoids interference from inconsistent periods. We also propose a positive-negative attention mechanism, which captures periodic dependencies from two perspectives: periodic alignment and periodic deviation. Additionally, the periodic alignment attention scores are decomposed into positive and negative components, with a modulation term encoding periodic priors. This modulation constrains the attention mechanism to more faithfully reflect the underlying periodic trends. A mathematical explanation is provided to support this property. We evaluate PHAT comprehensively on 14 real-world datasets against 18 baselines, and the results show that it significantly outperforms existing methods, achieving highly competitive forecasting performance. Our sources is available at GitHub.

</details>


### [124] [Riemannian Flow Matching for Disentangled Graph Domain Adaptation](https://arxiv.org/abs/2602.00656)
*Yingxu Wang,Xinwang Liu,Mengzhu Wang,Siyang Gao,Nan Yin*

Main category: cs.LG

TL;DR: DisRFM：一种几何感知的图域自适应框架，通过黎曼流形嵌入和基于流的传输解决结构退化和优化不稳定性问题


<details>
  <summary>Details</summary>
Motivation: 传统图域自适应方法在欧几里得空间中使用对抗学习对齐图嵌入，但面临两个关键挑战：1）结构退化问题 - 层次和语义表示纠缠；2）优化不稳定性 - 最小最大对抗训练的振荡动态

Method: 1）将图嵌入黎曼流形，使用极坐标显式解耦结构（半径）和语义（角度）；2）通过径向Wasserstein对齐保持拓扑结构，通过角度聚类实现语义区分；3）使用黎曼流匹配学习平滑向量场，沿测地线路径引导源特征向目标移动，确保稳定收敛

Result: 实验证明DisRFM在多个数据集上持续优于最先进方法，理论证明了流匹配的渐近稳定性并推导了更紧的目标风险界

Conclusion: DisRFM通过几何感知方法有效解决了图域自适应中的结构退化和优化不稳定性问题，为GDA提供了更稳定和有效的解决方案

Abstract: Graph Domain Adaptation (GDA) typically uses adversarial learning to align graph embeddings in Euclidean space. However, this paradigm suffers from two critical challenges: Structural Degeneration, where hierarchical and semantic representations are entangled, and Optimization Instability, which arises from oscillatory dynamics of minimax adversarial training. To tackle these issues, we propose DisRFM, a geometry-aware GDA framework that unifies Riemannian embedding and flow-based transport. First, to overcome structural degeneration, we embed graphs into a Riemannian manifold. By adopting polar coordinates, we explicitly disentangle structure (radius) from semantics (angle). Then, we enforce topology preservation through radial Wasserstein alignment and semantic discrimination via angular clustering, thereby preventing feature entanglement and collapse. Second, we address the instability of adversarial alignment by using Riemannian flow matching. This method learns a smooth vector field to guide source features toward the target along geodesic paths, guaranteeing stable convergence. The geometric constraints further guide the flow to maintain the disentangled structure during transport. Theoretically, we prove the asymptotic stability of the flow matching and derive a tighter bound for the target risk. Extensive experiments demonstrate that DisRFM consistently outperforms state-of-the-art methods.

</details>


### [125] [Three-Way Emotion Classification of EEG-based Signals using Machine Learning](https://arxiv.org/abs/2602.00670)
*Ashna Purwar,Gaurav Simkar,Madhumita,Sachin Kadam*

Main category: cs.LG

TL;DR: 该研究使用机器学习模型对EEG信号进行三分类情感识别（消极、中性、积极），比较了逻辑回归、支持向量机和随机森林三种模型，发现随机森林表现最佳。


<details>
  <summary>Details</summary>
Motivation: EEG信号能直接反映大脑活动，可用于识别人的情绪状态。随着情感感知系统和EEG情感识别研究的增长，需要探索哪种机器学习模型最适合处理这类有限数据集的三分类情感识别问题。

Method: 研究采用完整的工作流程，包括数据预处理和机器学习模型比较。在有限EEG数据集上训练和测试三种常用模型：逻辑回归(LR)、支持向量机(SVM)和随机森林(RF)，使用准确率和F1分数评估性能。

Result: 结果表明机器学习模型能有效用于EEG信号的三分类情感识别。在三种模型中，随机森林表现最佳，其更高的准确率和F1分数表明它能更准确有效地捕捉情感模式。随机森林在准确率参数上也优于现有最先进的分类模型。

Conclusion: 机器学习模型可用于EEG情感识别，其中随机森林模型在有限数据集的三分类任务中表现最优，优于逻辑回归和支持向量机，并在准确率上超越了现有最先进模型。

Abstract: Electroencephalography (EEG) is a widely used technique for measuring brain activity. EEG-based signals can reveal a persons emotional state, as they directly reflect activity in different brain regions. Emotion-aware systems and EEG-based emotion recognition are a growing research area. This paper presents how machine learning (ML) models categorize a limited dataset of EEG signals into three different classes, namely Negative, Neutral, or Positive. It also presents the complete workflow, including data preprocessing and comparison of ML models. To understand which ML classification model works best for this kind of problem, we train and test the following three commonly used models: logistic regression (LR), support vector machine (SVM), and random forest (RF). The performance of each is evaluated with respect to accuracy and F1-score. The results indicate that ML models can be effectively utilized for three-way emotion classification of EEG signals. Among the three ML models trained on the available dataset, the RF model gave the best results. Its higher accuracy and F1-score suggest that it is able to capture the emotional patterns more accurately and effectively than the other two models. The RF model also outperformed the existing state-of-the-art classification models in terms of the accuracy parameter.

</details>


### [126] [Strong Linear Baselines Strike Back: Closed-Form Linear Models as Gaussian Process Conditional Density Estimators for TSAD](https://arxiv.org/abs/2602.00672)
*Aleksandr Yugay,Hang Cui,Changhua Pei,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 线性自回归异常检测方法（OLS回归）在时间序列异常检测中表现优于复杂深度学习模型，计算效率更高，建议未来研究应包括强线性基线并开发更丰富的时间结构基准。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列异常检测研究过度关注复杂、难以训练且推理昂贵的神经网络架构，需要重新评估这种范式，探索更简单有效的替代方案。

Method: 提出使用普通最小二乘法（OLS）回归的线性自回归异常评分方法，通过闭式解估计有限历史高斯过程条件密度，能够捕捉广泛的异常类型。

Result: 在广泛的单变量和多变量基准测试中，该方法始终匹配或优于最先进的深度检测器，同时计算资源需求低几个数量级。

Conclusion: 未来研究应始终包含强线性基线，更重要的是开发具有更丰富时间结构的新基准，以突显深度学习模型的真正优势。

Abstract: Research in time series anomaly detection (TSAD) has largely focused on developing increasingly sophisticated, hard-to-train, and expensive-to-infer neural architectures. We revisit this paradigm and show that a simple linear autoregressive anomaly score with the closed-form solution provided by ordinary least squares (OLS) regression consistently matches or outperforms state-of-the-art deep detectors. From a theoretical perspective, we show that linear models capture a broad class of anomaly types, estimating a finite-history Gaussian process conditional density. From a practical side, across extensive univariate and multivariate benchmarks, the proposed approach achieves superior accuracy while requiring orders of magnitude fewer computational resources. Thus, future research should consistently include strong linear baselines and, more importantly, develop new benchmarks with richer temporal structures pinpointing the advantages of deep learning models.

</details>


### [127] [Provably Protecting Fine-Tuned LLMs from Training Data Extraction](https://arxiv.org/abs/2602.00688)
*Tom Segal,Asaf Shabtai,Yuval Elovici*

Main category: cs.LG

TL;DR: 提出SCP-Δr算法，通过仅保留关键token的概率偏移并平滑低影响token，在保护隐私的同时最小化性能损失


<details>
  <summary>Details</summary>
Motivation: 大语言模型在敏感数据上微调存在隐私风险，现有防御方法要么缺乏形式化隐私保证，要么导致显著的性能下降。研究发现微调引起的概率偏移中，只有少量关键token的偏移对性能至关重要，其余偏移可以平滑处理。

Method: 提出SCP-Δr算法，基于Near Access Freeness（NAF）框架，在相对概率空间操作，显式地使用基础模型平滑低影响token，同时保留关键token的概率偏移。

Result: SCP-Δr在理论上比现有NAF方法获得数量级更好的理论边界，在实验中能有效防御训练数据提取攻击，同时仅造成最小的性能损失。

Conclusion: 通过选择性保留关键token的概率偏移并平滑其余部分，可以在保护隐私的同时保持模型性能，为隐私保护微调提供了有效解决方案。

Abstract: Fine-tuning large language models (LLMs) on sensitive datasets raises privacy concerns, as training data extraction (TDE) attacks can expose highly confidential information. Existing defenses against such attacks either lack formal privacy guarantees or incur substantial utility degradation. We observe that fine-tuning induces widespread probability shifts, yet preserving only a small subset of influential token-level deviations is sufficient; the remaining shifts can be aggressively smoothed with minimal impact on utility. Motivated by this insight, we propose SCP-$Δ_r$, a Near Access Freeness (NAF)-based algorithm that operates on relative probabilities and explicitly smooths low-impact tokens using a base model. SCP-$Δ_r$ achieves orders-of-magnitude better theoretical bounds than existing NAF based methods and provides strong empirical protection against TDE attacks with minimal performance loss.

</details>


### [128] [Topology and Geometry of the Learning Space of ReLU Networks: Connectivity and Singularities](https://arxiv.org/abs/2602.00693)
*Marco Nurisso,Pierrick Leroy,Giovanni Petri,Francesco Vaccarino*

Main category: cs.LG

TL;DR: 研究前馈ReLU网络参数空间的连通性和奇异性，发现梯度流训练将参数限制在代数簇上，奇异性与DAG拓扑结构密切相关


<details>
  <summary>Details</summary>
Motivation: 理解前馈ReLU网络参数空间特性对分析训练动态至关重要，特别是梯度流训练后参数空间被限制在由ReLU齐次性产生的代数簇上

Method: 扩展先前结果，全面表征参数空间连通性，分析瓶颈节点和平衡条件作用，研究奇异性与DAG拓扑结构关系，建立与可微分剪枝的理论联系

Result: 发现奇异性与底层DAG拓扑及其诱导子网络密切相关，建立了奇异性可达性与可微分剪枝的原则性联系，并通过简单数值实验验证理论

Conclusion: 前馈ReLU网络的参数空间特性由DAG架构决定，奇异性与网络拓扑深度相关，这为理解训练动态和设计网络架构提供了理论基础

Abstract: Understanding the properties of the parameter space in feed-forward ReLU networks is critical for effectively analyzing and guiding training dynamics. After initialization, training under gradient flow decisively restricts the parameter space to an algebraic variety that emerges from the homogeneous nature of the ReLU activation function. In this study, we examine two key challenges associated with feed-forward ReLU networks built on general directed acyclic graph (DAG) architectures: the (dis)connectedness of the parameter space and the existence of singularities within it. We extend previous results by providing a thorough characterization of connectedness, highlighting the roles of bottleneck nodes and balance conditions associated with specific subsets of the network. Our findings clearly demonstrate that singularities are intricately connected to the topology of the underlying DAG and its induced sub-networks. We discuss the reachability of these singularities and establish a principled connection with differentiable pruning. We validate our theory with simple numerical experiments.

</details>


### [129] [Forecasting Energy Availability in Local Energy Communities via LSTM Federated Learning](https://arxiv.org/abs/2602.00694)
*Fabio Turazza,Marcello Pietri,Natalia Selini Hadjidimitriou,Marco Mamei*

Main category: cs.LG

TL;DR: 本文探讨了使用联邦学习和LSTM网络解决本地能源社区中隐私保护与能源预测准确性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 本地能源社区在实现自给自足时面临能源生产与消费平衡管理的挑战，需要准确的预测模型。然而，用户因隐私顾虑不愿分享消费数据，限制了传统预测方法的应用。

Method: 采用联邦学习框架结合长短期记忆网络，在不共享原始隐私敏感数据的情况下，通过分布式训练创建预测模型。

Result: 展示了联邦学习在保护用户隐私的同时，能够实现可接受的预测准确性，揭示了数据共享程度与预测精度之间的权衡关系。

Conclusion: 联邦学习是解决本地能源社区隐私保护与预测需求矛盾的有效方案，为可持续能源管理提供了可行的技术路径。

Abstract: Local Energy Communities are emerging as crucial players in the landscape of sustainable development. A significant challenge for these communities is achieving self-sufficiency through effective management of the balance between energy production and consumption. To meet this challenge, it is essential to develop and implement forecasting models that deliver accurate predictions, which can then be utilized by optimization and planning algorithms. However, the application of forecasting solutions is often hindered by privacy constrains and regulations as the users participating in the Local Energy Community can be (rightfully) reluctant sharing their consumption patterns with others. In this context, the use of Federated Learning (FL) can be a viable solution as it allows to create a forecasting model without the need to share privacy sensitive information among the users. In this study, we demonstrate how FL and long short-term memory (LSTM) networks can be employed to achieve this objective, highlighting the trade-off between data sharing and forecasting accuracy.

</details>


### [130] [LocalV: Exploiting Information Locality for IP-level Verilog Generation](https://arxiv.org/abs/2602.00704)
*Hanqi Lyu,Di Huang,Yaoyu Zhu,Kangcheng Liu,Bohan Dou,Chongxiao Li,Pengwei Jin,Shuyao Cheng,Rui Zhang,Zidong Du,Qi Guo,Xing Hu,Yunji Chen*

Main category: cs.LG

TL;DR: LocalV是一个多智能体框架，通过利用模块化硬件设计中的信息局部性，将长文档到长代码生成问题分解为短文档、短代码任务，显著提升了RTL代码生成的质量和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统RTL代码生成是劳动密集型过程，现有LLM方法在处理工业级IP设计任务时面临三大挑战：处理冗长详细文档、生成长RTL代码时正确性下降、以及复杂的调试周期。

Method: LocalV采用多智能体框架，集成层次化文档分割、任务规划、局部化代码生成、接口一致性合并和AST引导的局部感知调试，将长文档分解为短文档任务。

Result: 在RealBench（IP级Verilog生成基准）上，LocalV显著优于现有SOTA LLM和智能体方法，通过率达到45.0%，相比之前的21.6%有大幅提升。

Conclusion: LocalV通过利用硬件设计中的信息局部性，有效解决了工业级RTL代码生成的扩展性问题，为自动化硬件设计提供了有前景的解决方案。

Abstract: The generation of Register-Transfer Level (RTL) code is a crucial yet labor-intensive step in digital hardware design, traditionally requiring engineers to manually translate complex specifications into thousands of lines of synthesizable Hardware Description Language (HDL) code. While Large Language Models (LLMs) have shown promise in automating this process, existing approaches-including fine-tuned domain-specific models and advanced agent-based systems-struggle to scale to industrial IP-level design tasks. We identify three key challenges: (1) handling long, highly detailed documents, where critical interface constraints become buried in unrelated submodule descriptions; (2) generating long RTL code, where both syntactic and semantic correctness degrade sharply with increasing output length; and (3) navigating the complex debugging cycles required for functional verification through simulation and waveform analysis. To overcome these challenges, we propose LocalV, a multi-agent framework that leverages information locality in modular hardware design. LocalV decomposes the long-document to long-code generation problem into a set of short-document, short-code tasks, enabling scalable generation and debugging. Specifically, LocalV integrates hierarchical document partitioning, task planning, localized code generation, interface-consistent merging, and AST-guided locality-aware debugging. Experiments on RealBench, an IP-level Verilog generation benchmark, demonstrate that LocalV substantially outperforms state-of-the-art (SOTA) LLMs and agents, achieving a pass rate of 45.0% compared to 21.6%.

</details>


### [131] [Deep Time-series Forecasting Needs Kernelized Moment Balancing](https://arxiv.org/abs/2602.00717)
*Licheng Pan,Hao Wang,Haocheng Yang,Yuqi Li,Qingsong Wen,Xiaoxi Li,Zhichao Chen,Haoxuan Li,Zhixuan Chu,Yuan Lu*

Main category: cs.LG

TL;DR: 提出KMB-DF方法，通过核化矩平衡实现深度时间序列预测中的分布对齐，相比现有方法能更全面地匹配分布


<details>
  <summary>Details</summary>
Motivation: 现有深度时间序列预测方法在分布对齐上存在不足，它们只匹配一两个预定义的平衡函数，无法实现完整的分布平衡，导致预测精度受限

Method: 提出核化矩平衡直接预测(KMB-DF)，从再生核希尔伯特空间自适应选择最具信息量的平衡函数，推导出可微的、可处理的优化目标，可集成到基于梯度的训练流程中

Result: 在多个模型和数据集上的实验表明，KMB-DF能持续提升预测精度，达到最先进的性能水平

Conclusion: KMB-DF通过核化矩平衡实现了更全面的分布对齐，为深度时间序列预测提供了有效的分布平衡解决方案

Abstract: Deep time-series forecasting can be formulated as a distribution balancing problem aimed at aligning the distribution of the forecasts and ground truths. According to Imbens' criterion, true distribution balance requires matching the first moments with respect to any balancing function. We demonstrate that existing objectives fail to meet this criterion, as they enforce moment matching only for one or two predefined balancing functions, thus failing to achieve full distribution balance. To address this limitation, we propose direct forecasting with kernelized moment balancing (KMB-DF). Unlike existing objectives, KMB-DF adaptively selects the most informative balancing functions from a reproducing kernel hilbert space (RKHS) to enforce sufficient distribution balancing. We derive a tractable and differentiable objective that enables efficient estimation from empirical samples and seamless integration into gradient-based training pipelines. Extensive experiments across multiple models and datasets show that KMB-DF consistently improves forecasting accuracy and achieves state-of-the-art performance. Code is available at https://anonymous.4open.science/r/KMB-DF-403C.

</details>


### [132] [Federated Learning at the Forefront of Fairness: A Multifaceted Perspective](https://arxiv.org/abs/2602.00718)
*Noorain Mukhtiar,Adnan Mahmood,Yipeng Zhou,Jian Yang,Jing Teng,Quan Z. Sheng*

Main category: cs.LG

TL;DR: 这篇论文是关于联邦学习中公平性研究的综述，从多角度对现有公平感知方法进行分类，提供了评估框架和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的公平性日益重要，因为客户端存在异构性约束，需要在不同场景下平衡模型性能。当前需要系统性的分类和评估框架来指导公平性研究。

Method: 采用多角度分类方法：1) 模型性能导向的公平性方法；2) 能力导向的公平性方法。提供分类框架解决各种公平性问题和技术方面，并分析在平衡公平与性能方面的有效性。

Result: 建立了全面的公平性方法分类体系，提供了量化测量公平性的评估指标，为联邦学习公平性研究奠定了坚实基础。

Conclusion: 论文为联邦学习公平性研究提供了系统性框架，指出了未来研究方向并提出前瞻性解决方案，有助于推动该领域的发展。

Abstract: Fairness in Federated Learning (FL) is emerging as a critical factor driven by heterogeneous clients' constraints and balanced model performance across various scenarios. In this survey, we delineate a comprehensive classification of the state-of-the-art fairness-aware approaches from a multifaceted perspective, i.e., model performance-oriented and capability-oriented. Moreover, we provide a framework to categorize and address various fairness concerns and associated technical aspects, examining their effectiveness in balancing equity and performance within FL frameworks. We further examine several significant evaluation metrics leveraged to measure fairness quantitatively. Finally, we explore exciting open research directions and propose prospective solutions that could drive future advancements in this important area, laying a solid foundation for researchers working toward fairness in FL.

</details>


### [133] [Spectral Imbalance Causes Forgetting in Low-Rank Continual Adaptation](https://arxiv.org/abs/2602.00722)
*Hao Gu,Mao-Lin Luo,Zi-Hao Zhou,Han-Chen Zhang,Min-Ling Zhang,Tong Wei*

Main category: cs.LG

TL;DR: 本文提出了一种参数高效的持续学习方法EBLoRA，通过解耦任务更新的幅度与方向结构，在受限Stiefel流形上优化，平衡奇异值谱，减轻前后向遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法主要关注避免与过去更新的干扰，而非考虑什么特性使当前任务特定更新能自然保留先前获得的知识。从知识分解视角看，低秩适应表现出高度不平衡的奇异值谱：少数主导分量吸收大部分适应能量，这既容易破坏先前知识，也使更新更易受后续任务干扰。

Method: 将任务更新的幅度与方向结构解耦，将其表述为受限Stiefel流形上的约束优化问题。使用与标准深度学习优化器兼容的投影一阶方法解决该问题，实现分量间的显式平衡。

Result: 该方法能同时减轻后向遗忘和前向遗忘，在持续学习基准测试中一致优于现有基线方法。

Conclusion: 通过平衡奇异值谱并解耦更新幅度与方向，EBLoRA方法在参数高效持续学习中取得了显著改进，为视觉语言模型等场景提供了有效的持续学习解决方案。

Abstract: Parameter-efficient continual learning aims to adapt pre-trained models to sequential tasks without forgetting previously acquired knowledge. Most existing approaches treat continual learning as avoiding interference with past updates, rather than considering what properties make the current task-specific update naturally preserve previously acquired knowledge. From a knowledge-decomposition perspective, we observe that low-rank adaptations exhibit highly imbalanced singular value spectra: a few dominant components absorb most of the adaptation energy, thereby (i) more likely to disrupt previously acquired knowledge and (ii) making the update more vulnerable to interference from subsequent tasks. To enable explicit balance among components, we decouple the magnitude of the task update from its directional structure and formulate it as a constrained optimization problem on a restricted Stiefel manifold. We address this problem using a projected first-order method compatible with standard deep-learning optimizers used in vision-language models. Our method mitigates both backward and forward forgetting, consistently outperforming continual learning baselines. The implementation code is available at https://github.com/haodotgu/EBLoRA.

</details>


### [134] [Rethinking Hallucinations: Correctness, Consistency, and Prompt Multiplicity](https://arxiv.org/abs/2602.00723)
*Prakhar Ganesh,Reza Shokri,Golnoosh Farnadi*

Main category: cs.LG

TL;DR: 该论文提出"提示多样性"框架来量化LLM评估中的一致性，发现现有幻觉评估过度关注正确性而忽视一致性，导致对幻觉相关危害的严重误解。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型幻觉评估主要关注正确性，但忽视了评估的一致性。这种局限性导致无法准确区分和解决幻觉带来的各种危害（从信任侵蚀到广泛错误信息）。

Method: 引入"提示多样性"框架来量化LLM评估中的一致性，分析现有基准测试中的不一致性，并研究一致性在幻觉检测和缓解中的作用。

Result: 研究发现：1) 在Med-HALT等基准测试中存在超过50%的不一致性；2) 现有检测技术检测的是一致性而非正确性；3) RAG等缓解技术虽然有益，但可能引入额外的不一致性。

Conclusion: 通过将提示多样性整合到幻觉评估中，提供了改进的危害评估框架，并揭示了当前检测和缓解策略的关键局限性，强调了一致性评估对于准确理解幻觉相关危害的重要性。

Abstract: Large language models (LLMs) are known to "hallucinate" by generating false or misleading outputs. Hallucinations pose various harms, from erosion of trust to widespread misinformation. Existing hallucination evaluation, however, focuses only on correctness and often overlooks consistency, necessary to distinguish and address these harms. To bridge this gap, we introduce prompt multiplicity, a framework for quantifying consistency in LLM evaluations. Our analysis reveals significant multiplicity (over 50% inconsistency in benchmarks like Med-HALT), suggesting that hallucination-related harms have been severely misunderstood. Furthermore, we study the role of consistency in hallucination detection and mitigation. We find that: (a) detection techniques detect consistency, not correctness, and (b) mitigation techniques like RAG, while beneficial, can introduce additional inconsistencies. By integrating prompt multiplicity into hallucination evaluation, we provide an improved framework of potential harms and uncover critical limitations in current detection and mitigation strategies.

</details>


### [135] [Pareto-Conditioned Diffusion Models for Offline Multi-Objective Optimization](https://arxiv.org/abs/2602.00737)
*Jatan Shrestha,Santeri Heiskanen,Kari Hepola,Severi Rissanen,Pekka Jääskeläinen,Joni Pajarinen*

Main category: cs.LG

TL;DR: PCD将离线多目标优化建模为条件采样问题，通过直接条件化于期望权衡来避免显式代理模型，使用重加权策略和参考方向机制探索帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 离线多目标优化中，仅使用静态数据集时的主要挑战是如何泛化到未观测数据。现有方法需要显式代理模型，限制了泛化能力。

Method: 提出Pareto-Conditioned Diffusion (PCD)框架，将离线MOO转化为条件采样问题。通过直接条件化于期望权衡避免代理模型，使用重加权策略聚焦高性能样本，采用参考方向机制引导采样到训练数据之外的新区域。

Result: 在标准离线MOO基准测试中，PCD取得了高度竞争力的性能，更重要的是，在不同任务间表现出比现有离线MOO方法更好的一致性。

Conclusion: PCD通过将离线MOO重新定义为条件采样问题，提供了一种无需显式代理模型的有效方法，能够更好地泛化到未观测数据并探索帕累托前沿。

Abstract: Multi-objective optimization (MOO) arises in many real-world applications where trade-offs between competing objectives must be carefully balanced. In the offline setting, where only a static dataset is available, the main challenge is generalizing beyond observed data. We introduce Pareto-Conditioned Diffusion (PCD), a novel framework that formulates offline MOO as a conditional sampling problem. By conditioning directly on desired trade-offs, PCD avoids the need for explicit surrogate models. To effectively explore the Pareto front, PCD employs a reweighting strategy that focuses on high-performing samples and a reference-direction mechanism to guide sampling towards novel, promising regions beyond the training data. Experiments on standard offline MOO benchmarks show that PCD achieves highly competitive performance and, importantly, demonstrates greater consistency across diverse tasks than existing offline MOO approaches.

</details>


### [136] [GraphNNK -- Graph Classification and Interpretability](https://arxiv.org/abs/2602.00753)
*Zeljko Bolevic,Milos Brajovic,Isidora Stankovic,Ljubisa Stankovic*

Main category: cs.LG

TL;DR: GNNs依赖参数化分类器限制了可解释性和泛化能力，基于插值的方法（如NNK）通过训练样本的凸组合进行预测，提供理论保证和可解释性。


<details>
  <summary>Details</summary>
Motivation: GNNs已成为处理图结构数据的标准方法，但其依赖参数化分类器（通常是线性softmax层）限制了模型的可解释性，有时也阻碍了泛化能力。需要更可解释且泛化能力强的预测方法。

Method: 采用基于插值的方法，特别是非负核回归（NNK），将预测表示为嵌入空间中相似训练样本的凸组合，从而获得可解释的预测结果。

Result: NNK方法能够将预测表示为训练样本的凸组合，不仅提供了理论保证，还生成了可解释的预测解释，解决了传统GNNs在可解释性和泛化方面的限制。

Conclusion: 基于插值的非负核回归方法为GNNs提供了一种有前景的替代方案，能够同时提高模型的可解释性和泛化能力，通过训练样本的凸组合实现可解释的预测。

Abstract: Graph Neural Networks (GNNs) have become a standard approach for learning from graph-structured data. However, their reliance on parametric classifiers (most often linear softmax layers) limits interpretability and sometimes hinders generalization. Recent work on interpolation-based methods, particularly Non-Negative Kernel regression (NNK), has demonstrated that predictions can be expressed as convex combinations of similar training examples in the embedding space, yielding both theoretical results and interpretable explanations.

</details>


### [137] [BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features](https://arxiv.org/abs/2602.00767)
*Muhammed Ustaomeroglu,Guannan Qu*

Main category: cs.LG

TL;DR: 通过识别控制模型不良行为的内部特征并在微调时抑制这些特征，可以有效防止语言模型在特定目标微调时产生的意外不良行为，实现高达95%的不良行为减少且不影响模型质量和目标任务性能。


<details>
  <summary>Details</summary>
Motivation: 语言模型在针对特定目标进行微调时，虽然学会了目标行为，但可能同时发展出在领域外的不良行为（即"涌现错位"）。需要找到一种机制性的方法来防止这种不良行为的出现。

Method: 识别控制不良行为的一组内部特征，然后在微调过程中阻止模型加强这些特征。通过约束这些固定特征集来防止不良行为的涌现。

Result: 在六个微调领域中，该方法实现了高达95%的相对不良行为减少，且没有降低模型质量或目标任务性能。通过多种验证方法（分离选择/评估集、多独立评估者、随机种子、质量指标、广泛消融实验）证实了效果的有效性。

Conclusion: 针对内部机制的有针对性训练时约束可以有效缓解涌现错位，同时保持目标任务性能。研究还发现长时间微调可能导致错位重新出现，但可以通过修改部分恢复阻断效果。

Abstract: Emergent misalignment can arise when a language model is fine-tuned on a narrowly scoped supervised objective: the model learns the target behavior, yet also develops undesirable out-of-domain behaviors. We investigate a mechanistic approach to preventing emergent misalignment by identifying a small set of internal features that reliably control the misaligned behavior and then discouraging the model from strengthening these features during fine-tuning. Across six fine-tuning domains, blocking (i.e., constraining) a fixed set of features achieves up to 95\% relative reduction in emergent misalignment with no degradation in model quality or target-task performance. We strengthen validity with disjoint selection/evaluation splits, multiple independent judges, multiple random seeds for key settings, quality metrics, and extensive ablations demonstrating that the reduction in misalignment is specific to the identified mechanism. We also characterize a limiting regime in which misalignment re-emerges under prolonged fine-tuning, present evidence consistent with rerouting through alternative features or layers, and evaluate modifications that partially restore the misalignment-blocking effect. Overall, our results show that targeted training-time constraints on internal mechanisms can mitigate emergent misalignment without degrading target-task performance.

</details>


### [138] [Provable Model Provenance Set for Large Language Models](https://arxiv.org/abs/2602.00772)
*Xiaoqi Qiu,Hao Zeng,Zhiyu Hou,Hongxin Wei*

Main category: cs.LG

TL;DR: 提出MPS方法解决模型溯源问题，通过顺序测试和排除程序构建满足可证明保证的小型溯源集合


<details>
  <summary>Details</summary>
Motivation: 现有模型溯源方法依赖启发式指纹匹配规则，缺乏可证明的错误控制，且常忽略多源情况，导致溯源声明的可靠性无法验证

Method: 提出模型溯源集(MPS)方法，采用顺序测试和排除程序，在候选池中测试溯源存在的显著性，建立用户特定置信水平下的可证明渐近保证

Result: MPS能有效实现目标溯源覆盖，同时严格限制无关模型的包含，并在归因和审计任务中展示实际溯源分析潜力

Conclusion: MPS为模型溯源问题提供了具有可证明保证的正式化解决方案，显著提升了溯源分析的可靠性和实用性

Abstract: The growing prevalence of unauthorized model usage and misattribution has increased the need for reliable model provenance analysis. However, existing methods largely rely on heuristic fingerprint-matching rules that lack provable error control and often overlook the existence of multiple sources, leaving the reliability of their provenance claims unverified. In this work, we first formalize the model provenance problem with provable guarantees, requiring rigorous coverage of all true provenances at a prescribed confidence level. Then, we propose the Model Provenance Set (MPS), which employs a sequential test-and-exclusion procedure to adaptively construct a small set satisfying the guarantee. The key idea of MPS is to test the significance of provenance existence within a candidate pool, thereby establishing a provable asymptotic guarantee at a user-specific confidence level. Extensive experiments demonstrate that MPS effectively achieves target provenance coverage while strictly limiting the inclusion of unrelated models, and further reveal its potential for practical provenance analysis in attribution and auditing tasks.

</details>


### [139] [A novel VAE-DML fusion framework for casual analysis of greenwashing in the mining industry](https://arxiv.org/abs/2602.00774)
*Yuxin Lu,Zhen Peng,Xiqiang Xia,Jie Wang*

Main category: cs.LG

TL;DR: 该研究探讨股权制衡对矿业产业链企业绿色漂洗行为的抑制作用，采用变分自编码器和双重机器学习模型构建反事实场景，发现股权制衡通过缓解管理层业绩压力、增强高管团队稳定性和强化媒体监督三条路径抑制绿色漂洗。


<details>
  <summary>Details</summary>
Motivation: 在全球绿色转型和"双碳"目标背景下，矿业产业链企业作为资源消耗和环境影响的重点实体，其环境信息披露的真实可靠性对区域生态安全和国家资源战略至关重要。从公司治理角度研究股权制衡这一基础治理机制对绿色漂洗行为的抑制作用具有紧迫的现实意义。

Method: 创新性地采用变分自编码器（VAE）和双重机器学习（DML）模型构建反事实场景，有效缓解内生性问题，精确识别股权制衡与绿色漂洗之间的因果关系。

Result: 1. 股权制衡与企业绿色漂洗存在显著负向因果关系，证实其治理效应；2. 抑制作用存在显著异质性，在西部地区、产业链上游和环境敏感行业更强；3. 治理效应具有明显时态动态，当期最强，滞后效应递减但显著，长期累积影响稳定；4. 机制分析揭示三条作用路径：缓解管理层业绩压力、增强高管团队稳定性、强化媒体监督。

Conclusion: 股权制衡作为基础治理机制能有效抑制矿业产业链企业的绿色漂洗行为，其作用具有异质性和时态动态特征，通过多重路径实现治理效果，为提升企业环境信息披露质量和完善公司治理提供了实证依据。

Abstract: Against the backdrop of the global green transition and "dual carbon" goals, mining industry chain enterprises are pivotal entities in terms of resource consumption and environmental impact. Their environmental performance directly affects regional ecological security and is closely tied to national resource strategies and green transformation outcomes. Ensuring the authenticity and reliability of their environmental disclosure is thus a core and urgent issue for sustainable development and national strategic objectives.From a corporate governance perspective, this study examines equity balance as a fundamental governance mechanism, investigating its inhibitory effect on greenwashing behavior among these enterprises and the underlying pathways involved. Methodologically, the paper innovatively employs a Variational Autoencoder (VAE) and a Double Machine Learning (DML) model to construct counterfactual scenarios, mitigating endogeneity concerns and precisely identifying the causal relationship between equity balance and greenwashing. The findings indicate, first, a significant negative causal relationship between equity balance and corporate greenwashing, confirming its substantive governance effect. Second, this inhibitory effect exhibits notable heterogeneity, manifesting more strongly in western regions, upstream segments of the industrial chain, and industries with high environmental sensitivity. Third, the governance effect demonstrates clear temporal dynamics, with the strongest impact occurring in the current period, followed by a diminishing yet statistically significant lagged effect, and ultimately a stable long-term cumulative influence. Finally, mechanism analysis reveals that equity balance operates through three distinct channels to curb greenwashing: alleviating management performance pressure, enhancing the stability of the executive team, and intensifying media scrutiny.

</details>


### [140] [Stable Time Series Prediction of Enterprise Carbon Emissions Based on Causal Inference](https://arxiv.org/abs/2602.00775)
*Zitao Hong,Zhen Peng,Xueping Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种针对企业碳排放预测的稳定时间预测机制，通过因果推断和稳定学习方法解决跨区域、跨企业的数据分布漂移问题。


<details>
  <summary>Details</summary>
Motivation: 在碳达峰和碳中和目标背景下，准确预测企业碳排放趋势对能源结构优化和低碳转型决策至关重要。然而，不同地区、行业和企业在能源结构、生产规模、政策强度等方面存在显著异质性，导致碳排放数据在时空维度上出现明显的分布漂移和非平稳性，这影响了预测模型的准确性和决策指导价值。

Method: 整合因果推断视角与稳定学习方法及时间序列建模，提出针对分布漂移环境的稳定时间预测机制。该机制包含企业级能源投入、资本投资、劳动力配置、碳定价、政府干预和政策实施强度等变量，构建风险一致性约束的稳定学习框架，从多环境样本中提取因果稳定特征。通过自适应归一化和样本重加权策略动态校正经济波动和政策转变引起的时序非平稳性。

Result: 该方法能够提取对外部扰动具有鲁棒性且对二氧化碳排放具有长期稳定影响的因果稳定特征，增强了模型在复杂环境中的泛化能力和可解释性。

Conclusion: 提出的稳定时间预测机制有效解决了企业碳排放预测中的分布漂移问题，为能源结构优化和低碳转型决策提供了更可靠的预测工具，具有重要的实践应用价值。

Abstract: Against the backdrop of ongoing carbon peaking and carbon neutrality goals, accurate prediction of enterprise carbon emission trends constitutes an essential foundation for energy structure optimization and low-carbon transformation decision-making. Nevertheless, significant heterogeneity persists across regions, industries and individual enterprises regarding energy structure, production scale, policy intensity and governance efficacy, resulting in pronounced distribution shifts and non-stationarity in carbon emission data across both temporal and spatial dimensions. Such cross-regional and cross-enterprise data drift not only compromises the accuracy of carbon emission reporting but substantially undermines the guidance value of predictive models for production planning and carbon quota trading decisions. To address this critical challenge, we integrate causal inference perspectives with stable learning methodologies and time-series modelling, proposing a stable temporal prediction mechanism tailored to distribution shift environments. This mechanism incorporates enterprise-level energy inputs, capital investment, labour deployment, carbon pricing, governmental interventions and policy implementation intensity, constructing a risk consistency-constrained stable learning framework that extracts causal stable features (robust against external perturbations yet demonstrating long-term stable effects on carbon dioxide emissions) from multi-environment samples across diverse policies, regions and industrial sectors. Furthermore, through adaptive normalization and sample reweighting strategies, the approach dynamically rectifies temporal non-stationarity induced by economic fluctuations and policy transitions, ultimately enhancing model generalization capability and explainability in complex environments.

</details>


### [141] [Fast Non-Episodic Finite-Horizon RL with K-Step Lookahead Thresholding](https://arxiv.org/abs/2602.00781)
*Jiamin Xu,Kyra Gan*

Main category: cs.LG

TL;DR: 提出一种针对有限时域非片段式MDP的在线强化学习算法，使用K步前瞻Q函数和阈值机制，在理论和实验上均优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有无限时域方法依赖折扣收缩，不适用于有限时域MDP，需要估计到固定终止时间的回报。有限时域非片段式在线强化学习研究不足且具有挑战性。

Method: 引入K步前瞻Q函数，将规划截断到未来K步；采用阈值机制，仅当估计的K步前瞻值超过时变阈值时才选择动作；提出高效的表格学习算法，并随时间自适应增加K以平衡前瞻深度和估计方差。

Result: 理论证明：K=1时达到极小极大最优常数遗憾，K≥2时遗憾为O(max((K-1),C_{K-1})√(SATlog(T)))；实验在JumpRiverswim、FrozenLake和AnyTrading等环境中优于最先进的表格RL方法。

Conclusion: K步前瞻Q函数和阈值机制能有效解决有限时域MDP的在线学习问题，在理论和实验上均表现出色，为有限时域强化学习提供了新思路。

Abstract: Online reinforcement learning in non-episodic, finite-horizon MDPs remains underexplored and is challenged by the need to estimate returns to a fixed terminal time. Existing infinite-horizon methods, which often rely on discounted contraction, do not naturally account for this fixed-horizon structure. We introduce a modified Q-function: rather than targeting the full-horizon, we learn a K-step lookahead Q-function that truncates planning to the next K steps. To further improve sample efficiency, we introduce a thresholding mechanism: actions are selected only when their estimated K-step lookahead value exceeds a time-varying threshold. We provide an efficient tabular learning algorithm for this novel objective, proving it achieves fast finite-sample convergence: it achieves minimax optimal constant regret for $K=1$ and $\mathcal{O}(\max((K-1),C_{K-1})\sqrt{SAT\log(T)})$ regret for any $K \geq 2$. We numerically evaluate the performance of our algorithm under the objective of maximizing reward. Our implementation adaptively increases K over time, balancing lookahead depth against estimation variance. Empirical results demonstrate superior cumulative rewards over state-of-the-art tabular RL methods across synthetic MDPs and RL environments: JumpRiverswim, FrozenLake and AnyTrading.

</details>


### [142] [Multi-Objective Multi-Fidelity Bayesian Optimization with Causal Priors](https://arxiv.org/abs/2602.00788)
*Md Abir Hossen,Mohammad Ali Javidian,Vignesh Narayanan,Jason M. O'Kane,Pooyan Jamshidi*

Main category: cs.LG

TL;DR: RESCUE是一种多保真度贝叶斯优化方法，通过引入因果模型来改进多保真度代理的准确性，从而提高采样效率。


<details>
  <summary>Details</summary>
Motivation: 现有多保真度贝叶斯优化方法主要捕捉输入、保真度和目标之间的关联依赖而非因果机制，当低保真度代理与目标保真度对齐不佳时性能会下降。

Method: RESCUE学习一个结构因果模型来捕捉输入、保真度和目标之间的因果关系，构建编码干预效应的概率多保真度代理，并引入因果超体积知识梯度获取策略来选择输入-保真度对。

Result: 在机器人、机器学习(AutoML)和医疗保健等领域的合成和实际问题上，RESCUE相比最先进的多保真度优化方法提高了采样效率。

Conclusion: 通过将因果计算融入多保真度贝叶斯优化，RESCUE能够更有效地平衡低保真度代理的成本效益和准确性，从而加速寻找全局最优解。

Abstract: Multi-fidelity Bayesian optimization (MFBO) accelerates the search for the global optimum of black-box functions by integrating inexpensive, low-fidelity approximations. The central task of an MFBO policy is to balance the cost-efficiency of low-fidelity proxies against their reduced accuracy to ensure effective progression toward the high-fidelity optimum. Existing MFBO methods primarily capture associational dependencies between inputs, fidelities, and objectives, rather than causal mechanisms, and can perform poorly when lower-fidelity proxies are poorly aligned with the target fidelity. We propose RESCUE (REducing Sampling cost with Causal Understanding and Estimation), a multi-objective MFBO method that incorporates causal calculus to systematically address this challenge. RESCUE learns a structural causal model capturing causal relationships between inputs, fidelities, and objectives, and uses it to construct a probabilistic multi-fidelity (MF) surrogate that encodes intervention effects. Exploiting the causal structure, we introduce a causal hypervolume knowledge-gradient acquisition strategy to select input-fidelity pairs that balance expected multi-objective improvement and cost. We show that RESCUE improves sample efficiency over state-of-the-art MF optimization methods on synthetic and real-world problems in robotics, machine learning (AutoML), and healthcare.

</details>


### [143] [Sporadic Gradient Tracking over Directed Graphs: A Theoretical Perspective on Decentralized Federated Learning](https://arxiv.org/abs/2602.00791)
*Shahryar Zehtabi,Dong-Jun Han,Seyyedali Hosseinalipour,Christopher Brinton*

Main category: cs.LG

TL;DR: 本文提出了Spod-GT算法，这是首个在一般有向图上同时解决数据异构性和资源多样性的去中心化联邦学习算法，允许客户端特定的梯度计算频率和异构非对称通信频率。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习（DFL）使客户端能够在点对点方式下协作训练通用模型。现有研究分别解决了DFL中的两个重要挑战：梯度跟踪技术缓解数据异构性，以及考虑客户端资源可用性差异。本文旨在统一这两个研究方向。

Method: 提出Sporadic Gradient Tracking（Spod-GT）算法，该算法在一般有向图上结合了梯度跟踪技术，允许客户端特定的梯度计算频率和异构非对称通信频率。算法放宽了对梯度估计方差和客户端梯度多样性的假设。

Result: 通过严格的收敛分析，证明了算法在客户端间歇参与的情况下仍能保证在有向图上的共识和最优性。在图像分类数据集上的数值实验表明，Spod-GT相比已知的梯度跟踪基线方法具有更好的效果。

Conclusion: Spod-GT是首个在一般有向图上同时解决数据异构性和资源多样性问题的DFL算法，通过允许客户端特定的计算和通信频率，在现实场景中具有更好的适用性和性能。

Abstract: Decentralized Federated Learning (DFL) enables clients with local data to collaborate in a peer-to-peer manner to train a generalized model. In this paper, we unify two branches of work that have separately solved important challenges in DFL: (i) gradient tracking techniques for mitigating data heterogeneity and (ii) accounting for diverse availability of resources across clients. We propose $\textit{Sporadic Gradient Tracking}$ ($\texttt{Spod-GT}$), the first DFL algorithm that incorporates these factors over general directed graphs by allowing (i) client-specific gradient computation frequencies and (ii) heterogeneous and asymmetric communication frequencies. We conduct a rigorous convergence analysis of our methodology with relaxed assumptions on gradient estimation variance and gradient diversity of clients, providing consensus and optimality guarantees for GT over directed graphs despite intermittent client participation. Through numerical experiments on image classification datasets, we demonstrate the efficacy of $\texttt{Spod-GT}$ compared to well-known GT baselines.

</details>


### [144] [Latent Shadows: The Gaussian-Discrete Duality in Masked Diffusion](https://arxiv.org/abs/2602.00792)
*Guinan Chen,Xunpeng Huang,Ying Sun,Shijin Wang,Yanyong Zhang,Chao Wang*

Main category: cs.LG

TL;DR: 提出Masked Consistency Distillation (MCD)，通过建立masked diffusion duality理论，实现确定性采样，将推理速度提升16倍而不损失生成质量。


<details>
  <summary>Details</summary>
Motivation: Masked discrete diffusion在高质量语言建模中占主导地位，但其推理效率受限于缺乏确定性采样工具。现有方法要么性能较差，要么依赖复杂算子或随机蒸馏。

Method: 建立明确的Masked Diffusion Duality理论，证明masked过程是连续高斯过程的投影；提出Masked Consistency Distillation (MCD)框架，利用该对偶性解析构造确定性耦合轨迹，绕过数值ODE求解器。

Result: MCD严格优于先前的随机蒸馏方法，实现16倍推理加速而不损害生成质量，为一致性蒸馏在高性能离散生成中发挥全部潜力提供了可能。

Conclusion: 该工作不仅为masked和连续扩散提供了坚实的理论基础，还解锁了一致性蒸馏在高性能离散生成中的全部潜力。

Abstract: Masked discrete diffusion is a dominant paradigm for high-quality language modeling where tokens are iteratively corrupted to a mask state, yet its inference efficiency is bottlenecked by the lack of deterministic sampling tools. While diffusion duality enables deterministic distillation for uniform models, these approaches generally underperform masked models and rely on complex integral operators. Conversely, in the masked domain, prior methods typically assume the absence of deterministic trajectories, forcing a reliance on stochastic distillation. To bridge this gap, we establish explicit Masked Diffusion Duality, proving that the masked process arises as the projection of a continuous Gaussian process via a novel maximum-value index preservation mechanism. Furthermore, we introduce Masked Consistency Distillation (MCD), a principled framework that leverages this duality to analytically construct the deterministic coupled trajectories required for consistency distillation, bypassing numerical ODE solvers. This result strictly improves upon prior stochastic distillation methods, achieving a 16$\times$ inference speedup without compromising generation quality. Our findings not only provide a solid theoretical foundation connecting masked and continuous diffusion, but also unlock the full potential of consistency distillation for high-performance discrete generation. Our code is available at https://anonymous.4open.science/r/MCD-70FD.

</details>


### [145] [JTok: On Token Embedding as another Axis of Scaling Law via Joint Token Self-modulation](https://arxiv.org/abs/2602.00800)
*Yebin Yang,Huaijin Wu,Fu Guo,Lin Yao,Xiaohan Qin,Jingzhi Wang,Debing Zhang,Junchi Yan*

Main category: cs.LG

TL;DR: 提出token-indexed parameters作为新的扩展维度，通过JTok和JTok-M方法在Transformer层中添加调制向量，以极低计算成本提升模型性能


<details>
  <summary>Details</summary>
Motivation: 传统LLM扩展依赖密集维度，性能提升伴随计算成本线性增加；MoE方法虽然解耦容量与计算，但带来内存开销和硬件效率问题。需要新的扩展维度来解耦模型容量与FLOPs

Method: 提出token-indexed parameters作为新的扩展轴，引入JTok和JTok-M方法，在Transformer层中通过辅助嵌入表检索调制向量，通过轻量级逐元素操作调制主干网络

Result: 在650M到61B参数规模的密集和MoE主干上验证，一致降低验证损失并显著提升下游任务性能（如MMLU +4.1，ARC +8.3，CEval +8.9）。isoFLOPs分析显示JTok-M将质量-计算Pareto前沿移动，相比普通MoE架构减少35%计算量

Conclusion: token-indexed parameters提供了一种新的可扩展维度，能够以可忽略的计算开销显著提升模型性能，并展现出可预测的幂律缩放行为，为LLM扩展提供了新方向

Abstract: LLMs have traditionally scaled along dense dimensions, where performance is coupled with near-linear increases in computational cost. While MoE decouples capacity from compute, it introduces large memory overhead and hardware efficiency challenges. To overcome these, we propose token-indexed parameters as a novel, orthogonal scaling axis that decouple model capacity from FLOPs. Specifically, we introduce Joint-Token (JTok) and Mixture of Joint-Token (JTok-M), which augment Transformer layers with modulation vectors retrieved from auxiliary embedding tables. These vectors modulate the backbone via lightweight, element-wise operations, incurring negligible FLOPs overhead. Extensive experiments on both dense and MoE backbones, spanning from 650M (190M + 460M embedding) to 61B (17B + 44B embedding) total parameters, demonstrate that our approach consistently reduces validation loss and significantly improves downstream task performance (e.g., +4.1 on MMLU, +8.3 on ARC, +8.9 on CEval). Rigorous isoFLOPs analysis further confirms that JTok-M fundamentally shifts the quality-compute Pareto frontier, achieving comparable model quality with 35% less compute relative to vanilla MoE architectures, and we validate that token-indexed parameters exhibit a predictable power-law scaling behavior. Moreover, our efficient implementation ensures that the overhead introduced by JTok and JTok-M remains marginal.

</details>


### [146] [Mobile Exergames: Activity Recognition Based on Smartphone Sensors](https://arxiv.org/abs/2602.00809)
*David Craveiro,Hugo Silva*

Main category: cs.LG

TL;DR: 提出一款结合传感器活动识别与语音识别的2D无尽游戏Duck Catch & Fit，通过智能手机传感器检测人体活动，结合机器学习实现高精度识别，提升游戏沉浸感。


<details>
  <summary>Details</summary>
Motivation: 智能手机传感器能够提供丰富的用户活动和行为信息，人体活动识别在游戏、医疗和监控领域应用日益广泛。研究者希望探索如何利用传感器数据开发更沉浸式的游戏体验。

Method: 开发概念验证游戏Duck Catch & Fit，使用智能手机加速度计、陀螺仪和磁力计传感器，通过特征提取和学习机制检测静止、侧向移动和虚假侧向移动等活动。同时结合语音识别系统识别"fire"指令，增加游戏复杂度。

Result: 结果显示，使用机器学习技术能够以高识别率识别人体活动。运动识别与语音识别的结合为游戏创造了更沉浸的体验。

Conclusion: 研究表明，智能手机传感器结合机器学习技术能够有效识别人体活动，而多模态（运动+语音）的集成可以显著提升游戏的沉浸感和复杂性，为智能游戏开发提供了新思路。

Abstract: Smartphone sensors can be extremely useful in providing information on the activities and behaviors of persons. Human activity recognition is increasingly used for games, medical, or surveillance. In this paper, we propose a proof-of-concept 2D endless game called Duck Catch & Fit, which implements a detailed activity recognition system that uses a smartphone accelerometer, gyroscope, and magnetometer sensors. The system applies feature extraction and learning mechanism to detect human activities like staying, side movements, and fake side movements. In addition, a voice recognition system is combined to recognize the word "fire" and raise the game's complexity. The results show that it is possible to use machine learning techniques to recognize human activity with high recognition levels. Also, the combination of movement-based and voice-based integrations contributes to a more immersive gameplay.

</details>


### [147] [Over-Alignment vs Over-Fitting: The Role of Feature Learning Strength in Generalization](https://arxiv.org/abs/2602.00827)
*Taesun Yeom,Taehyeok Ha,Jaeho Lee*

Main category: cs.LG

TL;DR: 研究发现特征学习强度（FLS）存在最优值，既不能太小也不能太大，这与传统认为更强的特征学习总是改善泛化的直觉相反。


<details>
  <summary>Details</summary>
Motivation: 现有理论主要在渐近条件下研究特征学习强度对优化动态的影响，但对实际训练条件下（如达到目标训练风险时停止）FLS如何影响泛化缺乏深入理解。

Method: 通过实证研究观察FLS对泛化的影响，然后对两层ReLU网络在逻辑损失下的梯度流动态进行理论分析，通过初始化尺度控制FLS。

Result: 发现了最优FLS的存在，既不能太小也不能太大。过大的FLS会导致"过度对齐"现象损害泛化，而过小的FLS会导致过拟合。

Conclusion: 特征学习强度存在最优平衡点，需要在过度对齐和过拟合之间取得权衡，这对深度网络的实际训练具有重要指导意义。

Abstract: Feature learning strength (FLS), i.e., the inverse of the effective output scaling of a model, plays a critical role in shaping the optimization dynamics of neural nets. While its impact has been extensively studied under the asymptotic regimes -- both in training time and FLS -- existing theory offers limited insight into how FLS affects generalization in practical settings, such as when training is stopped upon reaching a target training risk. In this work, we investigate the impact of FLS on generalization in deep networks under such practical conditions. Through empirical studies, we first uncover the emergence of an $\textit{optimal FLS}$ -- neither too small nor too large -- that yields substantial generalization gains. This finding runs counter to the prevailing intuition that stronger feature learning universally improves generalization. To explain this phenomenon, we develop a theoretical analysis of gradient flow dynamics in two-layer ReLU nets trained with logistic loss, where FLS is controlled via initialization scale. Our main theoretical result establishes the existence of an optimal FLS arising from a trade-off between two competing effects: An excessively large FLS induces an $\textit{over-alignment}$ phenomenon that degrades generalization, while an overly small FLS leads to $\textit{over-fitting}$.

</details>


### [148] [Don't Forget Its Variance! The Minimum Path Variance Principle for Accurate and Stable Score-Based Density Ratio Estimation](https://arxiv.org/abs/2602.00834)
*Wei Chen,Jiacheng Li,Shigui Li,Zhiqi Lin,Junmei Yang,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: 基于分数的密度比估计方法存在路径依赖悖论，本文提出最小路径方差原则，通过参数化路径学习数据自适应低方差路径，提高估计准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 基于分数的密度比估计方法在理论上应该是路径无关的，但实际性能却严重依赖于选择的路径调度，这种理论与实践的矛盾需要解决。

Method: 提出最小路径方差原则，推导出路径方差的闭式表达式，使用Kumaraswamy混合模型参数化路径，学习数据自适应的低方差路径。

Result: 该方法产生了更准确和稳定的密度比估计器，在具有挑战性的基准测试中取得了新的最先进结果。

Conclusion: 通过最小化被忽视的路径方差，解决了基于分数的密度比估计方法的路径依赖悖论，实现了理论上路径无关而实践中性能稳定的统一。

Abstract: Score-based methods have emerged as a powerful framework for density ratio estimation (DRE), but they face an important paradox in that, while theoretically path-independent, their practical performance depends critically on the chosen path schedule. We resolve this issue by proving that tractable training objectives differ from the ideal, ground-truth objective by a crucial, overlooked term: the path variance of the time score. To address this, we propose MinPV (\textbf{Min}imum \textbf{P}ath \textbf{V}ariance) Principle, which introduces a principled heuristic to minimize the overlooked path variance. Our key contribution is the derivation of a closed-form expression for the variance, turning an intractable problem into a tractable optimization. By parameterizing the path with a flexible Kumaraswamy Mixture Model, our method learns a data-adaptive, low-variance path without heuristic selection. This principled optimization of the complete objective yields more accurate and stable estimators, establishing new state-of-the-art results on challenging benchmarks.

</details>


### [149] [RMFlow: Refined Mean Flow by a Noise-Injection Step for Multimodal Generation](https://arxiv.org/abs/2602.00849)
*Yuhao Huang,Shih-Hsin Wang,Andrea L. Bertozzi,Bao Wang*

Main category: cs.LG

TL;DR: RMFlow通过结合粗粒度1-NFE MeanFlow传输和定制化噪声注入细化步骤，实现了高效的多模态生成，在仅需1次函数评估的情况下达到接近SOTA的结果。


<details>
  <summary>Details</summary>
Motivation: MeanFlow虽然能够实现高效、高保真度的图像生成，但其单次函数评估（1-NFE）生成往往无法产生令人满意的结果，需要改进1-NFE生成质量。

Method: RMFlow整合了粗粒度1-NFE MeanFlow传输和后续定制化噪声注入细化步骤，使用神经网络近似流路径的平均速度，并采用新的损失函数训练，平衡概率路径之间的Wasserstein距离最小化和样本似然最大化。

Result: RMFlow在文本到图像、上下文到分子和时间序列生成任务上，仅使用1-NFE就达到了接近最先进水平的结果，计算成本与基线MeanFlows相当。

Conclusion: RMFlow通过创新的两阶段方法有效解决了1-NFE生成质量不足的问题，为高效多模态生成提供了有前景的解决方案。

Abstract: Mean flow (MeanFlow) enables efficient, high-fidelity image generation, yet its single-function evaluation (1-NFE) generation often cannot yield compelling results. We address this issue by introducing RMFlow, an efficient multimodal generative model that integrates a coarse 1-NFE MeanFlow transport with a subsequent tailored noise-injection refinement step. RMFlow approximates the average velocity of the flow path using a neural network trained with a new loss function that balances minimizing the Wasserstein distance between probability paths and maximizing sample likelihood. RMFlow achieves near state-of-the-art results on text-to-image, context-to-molecule, and time-series generation using only 1-NFE, at a computational cost comparable to the baseline MeanFlows.

</details>


### [150] [Investigating the Robustness of Subtask Distillation under Spurious Correlation](https://arxiv.org/abs/2602.00852)
*Pattarawat Chormai,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 评估在存在虚假相关性的数据上进行知识蒸馏时，不同蒸馏方法的鲁棒性表现差异


<details>
  <summary>Details</summary>
Motivation: 子任务蒸馏虽然使用教师模型，但仍依赖可能有限、缺乏代表性或存在虚假相关性的数据集。需要评估在存在虚假相关性的数据上进行蒸馏时，不同方法的鲁棒性表现。

Method: 评估了传统蒸馏方法和最新的SubDistill方法，在具有虚假相关性的数据上进行蒸馏实验。通过增加相关性强度来观察不同方法的性能变化。

Result: 随着虚假相关性强度增加，SubDistill等先进方法保持相对鲁棒，而一些基线方法性能下降到接近随机水平，两者之间差距扩大。

Conclusion: 研究强调了在具有虚假相关性的不完美现实数据集上进行知识蒸馏的挑战，需要更鲁棒的蒸馏方法。

Abstract: Subtask distillation is an emerging paradigm in which compact, specialized models are extracted from large, general-purpose 'foundation models' for deployment in environments with limited resources or in standalone computer systems. Although distillation uses a teacher model, it still relies on a dataset that is often limited in size and may lack representativeness or exhibit spurious correlations. In this paper, we evaluate established distillation methods, as well as the recent SubDistill method, when using data with spurious correlations for distillation. As the strength of the correlations increases, we observe a widening gap between advanced methods, such as SubDistill, which remain fairly robust, and some baseline methods, which degrade to near-random performance. Overall, our study underscores the challenges of knowledge distillation when applied to imperfect, real-world datasets, particularly those with spurious correlations.

</details>


### [151] [Towards Multiscale Graph-based Protein Learning with Geometric Secondary Structural Motifs](https://arxiv.org/abs/2602.00862)
*Shih-Hsin Wang,Yuhao Huang,Taos Transue,Justin Baker,Jonathan Forstater,Thomas Strohmer,Bao Wang*

Main category: cs.LG

TL;DR: 提出一种针对蛋白质结构的高效多尺度图学习框架，通过构建包含细粒度子图和粗粒度图的层次化表示，使用两个GNN分别学习局部和全局特征，显著提升预测精度并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络（GNN）的蛋白质结构学习方法在多尺度表示学习和长程依赖建模方面存在效率挑战，需要更有效的框架来捕捉蛋白质的层次化结构特征。

Method: 构建包含细粒度子图（对应二级结构基序如α-螺旋、β-折叠、环）和粗粒度图（连接这些基序）的层次化图表示；使用两个GNN：第一个在单个二级结构基序内学习局部相互作用，第二个建模基序间的高层结构关系；框架允许各阶段灵活选择GNN。

Result: 理论证明该层次化框架保持了最大表达能力，不会丢失关键结构信息；实验表明将基线GNN集成到该多尺度框架中，在各种基准测试中显著提高了预测精度并降低了计算成本。

Conclusion: 提出的多尺度图学习框架为蛋白质结构分析提供了一种高效且表达力强的解决方案，通过层次化表示和分离的局部/全局学习机制，克服了现有GNN方法在多尺度建模和长程依赖方面的局限性。

Abstract: Graph neural networks (GNNs) have emerged as powerful tools for learning protein structures by capturing spatial relationships at the residue level. However, existing GNN-based methods often face challenges in learning multiscale representations and modeling long-range dependencies efficiently. In this work, we propose an efficient multiscale graph-based learning framework tailored to proteins. Our proposed framework contains two crucial components: (1) It constructs a hierarchical graph representation comprising a collection of fine-grained subgraphs, each corresponding to a secondary structure motif (e.g., $α$-helices, $β$-strands, loops), and a single coarse-grained graph that connects these motifs based on their spatial arrangement and relative orientation. (2) It employs two GNNs for feature learning: the first operates within individual secondary motifs to capture local interactions, and the second models higher-level structural relationships across motifs. Our modular framework allows a flexible choice of GNN in each stage. Theoretically, we show that our hierarchical framework preserves the desired maximal expressiveness, ensuring no loss of critical structural information. Empirically, we demonstrate that integrating baseline GNNs into our multiscale framework remarkably improves prediction accuracy and reduces computational cost across various benchmarks.

</details>


### [152] [Improving Flow Matching by Aligning Flow Divergence](https://arxiv.org/abs/2602.00869)
*Yuhao Huang,Taos Transue,Shih-Hsin Wang,William Feldman,Hong Zhang,Bao Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种改进条件流匹配的方法，通过同时匹配流场和其散度来减少学习概率路径的误差，从而提升流基生成模型的性能。


<details>
  <summary>Details</summary>
Motivation: 条件流匹配（CFM）虽然是一种高效的生成模型训练方法，但无法保证概率路径学习的准确性。现有方法在匹配流场时存在误差，需要新的理论框架来量化并减少这种误差。

Method: 提出了描述学习概率路径与真实概率路径之间误差的偏微分方程表征及其解。理论分析表明总变差差距受CFM损失和散度损失共同约束，据此设计了同时匹配流场和散度的新目标函数。

Result: 新方法在多个基准任务上显著提升了流基生成模型的性能，包括动力系统、DNA序列和视频生成，且不牺牲生成效率。

Conclusion: 通过同时匹配流场和散度，可以显著改善条件流匹配的准确性，为流基生成模型提供了更有效的训练框架。

Abstract: Conditional flow matching (CFM) stands out as an efficient, simulation-free approach for training flow-based generative models, achieving remarkable performance for data generation. However, CFM is insufficient to ensure accuracy in learning probability paths. In this paper, we introduce a new partial differential equation characterization for the error between the learned and exact probability paths, along with its solution. We show that the total variation gap between the two probability paths is bounded above by a combination of the CFM loss and an associated divergence loss. This theoretical insight leads to the design of a new objective function that simultaneously matches the flow and its divergence. Our new approach improves the performance of the flow-based generative model by a noticeable margin without sacrificing generation efficiency. We showcase the advantages of this enhanced training approach over CFM on several important benchmark tasks, including generative modeling for dynamical systems, DNA sequences, and videos. Code is available at \href{https://github.com/Utah-Math-Data-Science/Flow_Div_Matching}{Utah-Math-Data-Science}.

</details>


### [153] [Learning Heat-based Equations in Self-similar variables](https://arxiv.org/abs/2602.00872)
*Shihao Wang,Qipeng Qian,Jingquan Wang*

Main category: cs.LG

TL;DR: 研究热基方程在自相似变量中的解学习，开发SSV训练框架，在Navier-Stokes和Burgers方程上验证，SSV训练的网络在长期外推和稳定性上表现更好


<details>
  <summary>Details</summary>
Motivation: 为热基方程（如Navier-Stokes和Burgers方程）的长期动力学学习提供数学上合理的归纳偏置，解决传统物理坐标训练中模型外推能力不足的问题

Method: 开发自相似变量（SSV）训练框架，与标准神经算子训练兼容；在二维不可压缩Navier-Stokes方程和一维粘性Burgers方程上实例化；使用两种全连接架构（标准多层感知机和因子化全连接网络）进行物理坐标与自相似坐标的对比训练

Result: SSV训练的网络在所有系统和架构中都表现出一致性优势：显著更准确和稳定的训练窗口外推，更好地捕捉长期定性趋势

Conclusion: 自相似坐标为学习热基方程的长期动力学提供了数学上合理的归纳偏置，能显著提升神经算子的外推能力和长期预测稳定性

Abstract: We study solution learning for heat-based equations in self-similar variables (SSV). We develop an SSV training framework compatible with standard neural-operator training. We instantiate this framework on the two-dimensional incompressible Navier-Stokes equations and the one-dimensional viscous Burgers equation, and perform controlled comparisons between models trained in physical coordinates and in the corresponding self-similar coordinates using two simple fully connected architectures (standard multilayer perceptrons and a factorized fully connected network). Across both systems and both architectures, SSV-trained networks consistently deliver substantially more accurate and stable extrapolation beyond the training window and better capture qualitative long-time trends. These results suggest that self-similar coordinates provide a mathematically motivated inductive bias for learning the long-time dynamics of heat-based equations.

</details>


### [154] [Dynamic Expert Sharing: Decoupling Memory from Parallelism in Mixture-of-Experts Diffusion LLMs](https://arxiv.org/abs/2602.00879)
*Hao Mark Chen,Zhiwen Mo,Royson Lee,Qianzhou Wang,Da Li,Shell Xu Hu,Wayne Luk,Timothy Hospedales,Hongxiang Fan*

Main category: cs.LG

TL;DR: 提出Dynamic Expert Sharing (DES)方法，通过序列级核心集选择解决MoE扩散大语言模型中的专家爆炸问题，显著减少内存开销并保持精度。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(dLLMs)结合MoE架构时存在专家爆炸问题：并行生成token数量增加时，激活的专家数量几乎线性增长，导致内存流量激增，使推理进入内存受限状态，抵消了MoE和并行解码的效率优势。

Method: 提出动态专家共享(DES)技术，将MoE优化从token级剪枝和传统专家跳过方法转向序列级核心集选择。包含两种策略：1) 序列内共享(DES-Seq)，在序列级别自适应最优分配；2) 显著性感知投票(DES-Vote)，让token基于聚合的路由权重共同选举核心集。

Result: 在MoE dLLMs上的实验表明，DES将独特专家激活减少超过55%，延迟降低高达38%，同时保持99%的原始精度，有效解耦内存开销与并行度。

Conclusion: DES通过序列级专家共享有效解决了MoE扩散大语言模型中的专家爆炸问题，在保持生成质量的同时显著提升推理效率，为并行解码与MoE架构的高效集成提供了解决方案。

Abstract: Among parallel decoding paradigms, diffusion large language models (dLLMs) have emerged as a promising candidate that balances generation quality and throughput. However, their integration with Mixture-of-Experts (MoE) architectures is constrained by an expert explosion: as the number of tokens generated in parallel increases, the number of distinct experts activated grows nearly linearly. This results in substantial memory traffic that pushes inference into a memory-bound regime, negating the efficiency gains of both MoE and parallel decoding. To address this challenge, we propose Dynamic Expert Sharing (DES), a novel technique that shifts MoE optimization from token-centric pruning and conventional expert skipping methods to sequence-level coreset selection. To maximize expert reuse, DES identifies a compact, high-utility set of experts to satisfy the requirements of an entire parallel decoding block. We introduce two innovative selection strategies: (1) Intra-Sequence Sharing (DES-Seq), which adapts optimal allocation to the sequence level, and (2) Saliency-Aware Voting (DES-Vote), a novel mechanism that allows tokens to collectively elect a coreset based on aggregated router weights. Extensive experiments on MoE dLLMs demonstrate that DES reduces unique expert activations by over 55% and latency by up to 38%, while retaining 99% of vanilla accuracy, effectively decoupling memory overhead from the degree of parallelism.

</details>


### [155] [Test-time Generalization for Physics through Neural Operator Splitting](https://arxiv.org/abs/2602.00884)
*Louis Serrano,Jiequn Han,Edouard Oyallon,Shirley Ho,Rudy Morel*

Main category: cs.LG

TL;DR: 提出一种测试时神经算子分裂策略，通过组合训练算子来近似未见过的动力学，实现零样本泛化


<details>
  <summary>Details</summary>
Motivation: 神经算子在处理训练分布外的测试输入（如新初始条件、未见PDE系数或物理现象）时泛化能力有限。现有方法需要新动力学的示例进行微调，无法实现真正的零样本泛化。

Method: 基于DISCO（在不同动力学上训练的神经算子字典），提出神经算子分裂策略：在测试时搜索训练算子的组合来近似未见过的动力学，无需修改预训练权重。

Result: 在参数外推和物理现象新组合等挑战性分布外任务上，实现了最先进的零样本泛化结果，并能恢复底层PDE参数。

Conclusion: 测试时计算是构建灵活、可组合且可泛化的神经算子的关键途径。

Abstract: Neural operators have shown promise in learning solution maps of partial differential equations (PDEs), but they often struggle to generalize when test inputs lie outside the training distribution, such as novel initial conditions, unseen PDE coefficients or unseen physics. Prior works address this limitation with large-scale multiple physics pretraining followed by fine-tuning, but this still requires examples from the new dynamics, falling short of true zero-shot generalization. In this work, we propose a method to enhance generalization at test time, i.e., without modifying pretrained weights. Building on DISCO, which provides a dictionary of neural operators trained across different dynamics, we introduce a neural operator splitting strategy that, at test time, searches over compositions of training operators to approximate unseen dynamics. On challenging out-of-distribution tasks including parameter extrapolation and novel combinations of physics phenomena, our approach achieves state-of-the-art zero-shot generalization results, while being able to recover the underlying PDE parameters. These results underscore test-time computation as a key avenue for building flexible, compositional, and generalizable neural operators.

</details>


### [156] [Reliability-Aware Determinantal Point Processes for Robust Informative Data Selection in Large Language Models](https://arxiv.org/abs/2602.00885)
*Ahmad Sarlak,Abolfazl Razi*

Main category: cs.LG

TL;DR: 提出ProbDPP方法，在传统DPP数据选择基础上考虑数据访问的可靠性问题，通过正则化项处理概率性数据访问，并设计UCB算法在线学习未知可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统数据选择方法（如DPP）假设数据批次总是可用的，但在实际部署中会面临存储中断、通信不完美和随机访问失败等问题。原始方法在这些条件下会失效，需要可靠性感知的数据选择方案。

Method: 提出ProbDPP，这是k-DPP的可靠性感知实现，通过正则化项重新构建目标函数，将问题分解为几何多样性项和不可靠性成本。将可靠性感知的多样性最大化建模为组合半赌博问题，并提出UCB风格算法在线学习未知可靠性。

Result: 理论分析为所提方法提供了遗憾界限，确保性能保证。ProbDPP能够在不确定性下稳健地选择多样化的数据批次。

Conclusion: ProbDPP解决了传统数据选择方法在可靠性问题上的局限性，通过结合多样性最大化和可靠性成本，为在计算和通信约束下的实际部署提供了有效的解决方案。

Abstract: Informative data selection is a key requirement for large language models (LLMs) to minimize the amount of data required for fine-tuning, network distillation, and token pruning, enabling fast and efficient deployment, especially under computational and communication constraints. Traditional subset selection methods, including those based on Determinantal Point Processes (DPP), focus on maximizing diversity but assume that selected data batches are always available error-free. This presumption prohibits their use under partial storage outage, imperfect communication, and stochastic access failures. Furthermore, we show that the original formulation collapses under such conditions. To address this gap, we introduce ProbDPP, a novel reliability-aware implementation of k-DPP that accounts for probabilistic data access by recasting the objective function with a regularization term that remains well-posed and decomposes into a geometric diversity term and unreliability cost. The resulting objective facilitates robust selection of diverse data batches under uncertainty. Furthermore, we frame this reliability-aware diversity maximization as a combinatorial semi-bandit problem and propose a UCB-style algorithm to efficiently learn the unknown reliability online. Theoretical analysis provides regret bounds for the proposed approach, ensuring performance guarantees.

</details>


### [157] [GAPNet: Plug-in Jointly Learning Task-Specific Graph for Dynamic Stock Relation](https://arxiv.org/abs/2602.00888)
*Yingjie Niu,Lanxin Lu,Changhong Jin,Ruihai Dong*

Main category: cs.LG

TL;DR: GAPNet是一个图适应插件网络，通过端到端学习任务特定拓扑和表示，动态调整股票关系图结构，提升金融预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预定义图结构捕捉股票关系，但网络信号噪声大、异步且难以获取，导致泛化性差且与下游任务不对齐。需要动态适应任务特定拓扑。

Method: 提出GAPNet插件网络，可附加到现有图或超图骨干模型上，通过空间感知层捕捉短期资产共动和时间感知层维持分布漂移下的长期依赖，动态调整边拓扑。

Result: 在两个真实股票数据集上，GAPNet显著提升盈利性和稳定性，RT-GCN年化累计收益达0.47，CI-STHPAN达0.63，峰值夏普比率分别为2.20和2.12。

Conclusion: 联合学习图结构和表示对任务特定关系建模至关重要，GAPNet的即插即用设计确保其广泛适用于各种GNN架构。

Abstract: The advent of the web has led to a paradigm shift in the financial relations, with the real-time dissemination of news, social discourse, and financial filings contributing significantly to the reshaping of financial forecasting. The existing methods rely on establishing relations a priori, i.e. predefining graphs to capture inter-stock relationships. However, the stock-related web signals are characterised by high levels of noise, asynchrony, and challenging to obtain, resulting in poor generalisability and non-alignment between the predefined graphs and the downstream tasks. To address this, we propose GAPNet, a Graph Adaptation Plug-in Network that jointly learns task-specific topology and representations in an end-to-end manner. GAPNet attaches to existing pairwise graph or hypergraph backbone models, enabling the dynamic adaptation and rewiring of edge topologies via two complementary components: a Spatial Perception Layer that captures short-term co-movements across assets, and a Temporal Perception Layer that maintains long-term dependency under distribution shift. Across two real-world stock datasets, GAPNet has been shown to consistently enhance the profitability and stability in comparision to the state-of-the-art models, yielding annualised cumulative returns of up to 0.47 for RT-GCN and 0.63 for CI-STHPAN, with peak Sharpe Ratio of 2.20 and 2.12 respectively. The plug-and-play design of GAPNet ensures its broad applicability to diverse GNN-based architectures. Our results underscore that jointly learning graph structures and representations is essential for task-specific relational modeling.

</details>


### [158] [Domain-Adaptive and Scalable Dense Retrieval for Content-Based Recommendation](https://arxiv.org/abs/2602.00899)
*Mritunjay Pandey*

Main category: cs.LG

TL;DR: 提出基于双塔编码器的稠密检索系统，用于电商推荐，通过语义相似度匹配用户意图与商品，相比传统关键词匹配显著提升召回率。


<details>
  <summary>Details</summary>
Motivation: 传统电商推荐依赖稀疏关键词匹配（如BM25），在词汇不匹配时效果差。用户意图与商品元数据之间词汇重叠有限时，需要语义层面的匹配能力。

Method: 1. 采用双塔编码器架构；2. 在Amazon Reviews 2023（时尚）数据集上使用监督对比学习，采用多重负样本排序损失；3. 使用评论文本作为查询代理，商品元数据作为正样本文档；4. 结合FAISS HNSW索引和ONNX Runtime推理管道，采用INT8动态量化。

Result: 在826,402个商品目录的评论到标题基准测试中，Recall@10从0.26（BM25）提升到0.66；CPU推理延迟中位数为6.1毫秒（批次大小1）；模型大小减少4倍。

Conclusion: 提供了一个端到端、可复现的蓝图，将领域适应的稠密检索从离线训练扩展到CPU高效的大规模目录服务，满足实际延迟和模型大小约束。

Abstract: E-commerce recommendation and search commonly rely on sparse keyword matching (e.g., BM25), which breaks down under vocabulary mismatch when user intent has limited lexical overlap with product metadata. We cast content-based recommendation as recommendation-as-retrieval: given a natural-language intent signal (a query or review), retrieve the top-K most relevant items from a large catalog via semantic similarity.
  We present a scalable dense retrieval system based on a two-tower bi-encoder, fine-tuned on the Amazon Reviews 2023 (Fashion) subset using supervised contrastive learning with Multiple Negatives Ranking Loss. We construct training pairs from review text (as a query proxy) and item metadata (as the positive document) and fine-tune on 50,000 sampled interactions with a maximum sequence length of 500 tokens.
  For efficient serving, we combine FAISS HNSW indexing with an ONNX Runtime inference pipeline using INT8 dynamic quantization. On a review-to-title benchmark over 826,402 catalog items, our approach improves Recall@10 from 0.26 (BM25) to 0.66, while meeting practical latency and model-size constraints: 6.1 ms median CPU inference latency (batch size 1) and a 4x reduction in model size.
  Overall, we provide an end-to-end, reproducible blueprint for taking domain-adapted dense retrieval from offline training to CPU-efficient serving at catalog scale.

</details>


### [159] [Hallucination is a Consequence of Space-Optimality: A Rate-Distortion Theorem for Membership Testing](https://arxiv.org/abs/2602.00906)
*Anxin Guo,Jingwei Li*

Main category: cs.LG

TL;DR: 论文提出一个信息论框架，将LLM的记忆问题形式化为成员测试问题，证明了在有限容量下最优策略会导致幻觉，这是损失压缩的自然结果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常对缺乏可推断模式的"随机事实"产生高置信度的幻觉。需要从信息论角度理解这种现象的根本原因，而不是简单归因于训练数据或模型缺陷。

Method: 将事实记忆形式化为成员测试问题，统一Bloom滤波器的离散误差度量与LLM的连续对数损失。在事实稀疏的假设下，建立速率-失真定理，用事实与非事实得分分布之间的最小KL散度表征最优记忆效率。

Result: 理论分析表明，即使在最优训练、完美数据和简化"封闭世界"设置下，有限容量下的信息论最优策略不是弃权或遗忘，而是对某些非事实分配高置信度，导致幻觉。在合成数据上的实证验证支持这一理论。

Conclusion: 幻觉是损失压缩的自然结果，是有限容量下信息论最优策略的必然产物，而不是训练缺陷。这为理解LLM幻觉提供了新的理论框架。

Abstract: Large language models often hallucinate with high confidence on "random facts" that lack inferable patterns. We formalize the memorization of such facts as a membership testing problem, unifying the discrete error metrics of Bloom filters with the continuous log-loss of LLMs. By analyzing this problem in the regime where facts are sparse in the universe of plausible claims, we establish a rate-distortion theorem: the optimal memory efficiency is characterized by the minimum KL divergence between score distributions on facts and non-facts. This theoretical framework provides a distinctive explanation for hallucination: even with optimal training, perfect data, and a simplified "closed world" setting, the information-theoretically optimal strategy under limited capacity is not to abstain or forget, but to assign high confidence to some non-facts, resulting in hallucination. We validate this theory empirically on synthetic data, showing that hallucinations persist as a natural consequence of lossy compression.

</details>


### [160] [PyGALAX: An Open-Source Python Toolkit for Advanced Explainable Geospatial Machine Learning](https://arxiv.org/abs/2602.00907)
*Pingping Wang,Yihong Yuan,Lingcheng Li,Yongmei Lu*

Main category: cs.LG

TL;DR: PyGALAX是一个集成AutoML和XAI的Python地理空间分析包，能自动选择优化机器学习模型并保持可解释性，改进了传统地理加权回归方法。


<details>
  <summary>Details</summary>
Motivation: 传统地理加权回归方法在处理空间异质性时存在局限性，需要更灵活、自动化的工具来分析空间非平稳性，同时保持模型的可解释性。

Method: 基于GALAX框架改进，集成AutoML自动选择和优化机器学习模型，使用SHAP进行可解释性分析，新增自动带宽选择和灵活核函数选择功能。

Result: PyGALAX在性能上优于传统地理加权回归方法，提供了更灵活、稳健的空间建模能力，能够处理多样化的数据集和研究问题。

Conclusion: PyGALAX将先进的地理空间机器学习方法打包成易用、可复现的Python工具包，使地理学、城市规划、环境科学等领域的研究者和从业者能够更便捷地分析复杂的空间关系。

Abstract: PyGALAX is a Python package for geospatial analysis that integrates automated machine learning (AutoML) and explainable artificial intelligence (XAI) techniques to analyze spatial heterogeneity in both regression and classification tasks. It automatically selects and optimizes machine learning models for different geographic locations and contexts while maintaining interpretability through SHAP (SHapley Additive exPlanations) analysis. PyGALAX builds upon and improves the GALAX framework (Geospatial Analysis Leveraging AutoML and eXplainable AI), which has proven to outperform traditional geographically weighted regression (GWR) methods. Critical enhancements in PyGALAX from the original GALAX framework include automatic bandwidth selection and flexible kernel function selection, providing greater flexibility and robustness for spatial modeling across diverse datasets and research questions. PyGALAX not only inherits all the functionalities of the original GALAX framework but also packages them into an accessible, reproducible, and easily deployable Python toolkit while providing additional options for spatial modeling. It effectively addresses spatial non-stationarity and generates transparent insights into complex spatial relationships at both global and local scales, making advanced geospatial machine learning methods accessible to researchers and practitioners in geography, urban planning, environmental science, and related fields.

</details>


### [161] [Efficient Deep Learning for Medical Imaging: Bridging the Gap Between High-Performance AI and Clinical Deployment](https://arxiv.org/abs/2602.00910)
*Cuong Manh Nguyen,Truong-Son Hy*

Main category: cs.LG

TL;DR: 这篇综述系统梳理了面向医疗领域的高效轻量深度学习架构，将现代高效模型分为CNN、轻量Transformer和线性复杂度模型三大类，并评估了模型压缩策略在保持诊断性能同时降低硬件需求的效果。


<details>
  <summary>Details</summary>
Motivation: 大规模深度学习模型在真实临床环境中部署面临挑战，包括高计算成本、延迟限制和云端处理带来的患者数据隐私问题，需要开发高效轻量的解决方案。

Method: 将现代高效模型分为三类：卷积神经网络(CNNs)、轻量Transformer和新兴线性复杂度模型；同时研究了剪枝、量化、知识蒸馏和低秩分解等关键模型压缩策略。

Result: 提供了医疗领域高效深度学习架构的全面综合，评估了不同压缩策略在维持诊断性能同时降低硬件需求的效果，为资源受限临床环境提供了实用指南。

Conclusion: 该综述为研究人员和实践者提供了路线图，旨在弥合高性能AI与资源受限临床环境之间的差距，推动向设备端智能的过渡。

Abstract: Deep learning has revolutionized medical image analysis, playing a vital role in modern clinical applications. However, the deployment of large-scale models in real-world clinical settings remains challenging due to high computational costs, latency constraints, and patient data privacy concerns associated with cloud-based processing. To address these bottlenecks, this review provides a comprehensive synthesis of efficient and lightweight deep learning architectures specifically tailored for the medical domain. We categorize the landscape of modern efficient models into three primary streams: Convolutional Neural Networks (CNNs), Lightweight Transformers, and emerging Linear Complexity Models. Furthermore, we examine key model compression strategies (including pruning, quantization, knowledge distillation, and low-rank factorization) and evaluate their efficacy in maintaining diagnostic performance while reducing hardware requirements. By identifying current limitations and discussing the transition toward on-device intelligence, this review serves as a roadmap for researchers and practitioners aiming to bridge the gap between high-performance AI and resource-constrained clinical environments.

</details>


### [162] [Early Classification of Time Series in Non-Stationary Cost Regimes](https://arxiv.org/abs/2602.00918)
*Aurélien Renault,Alexis Bondu,Antoine Cornuéjols,Vincent Lemaire*

Main category: cs.LG

TL;DR: 该论文研究时间序列早期分类（ECTS）在成本非平稳性下的鲁棒性问题，提出在线学习方法来适应部署期间的成本变化。


<details>
  <summary>Details</summary>
Motivation: 现有ECTS方法通常假设时间相关的决策成本是已知、固定且正确指定的，但实际应用中这些成本往往不确定且随时间变化，导致训练目标和部署目标不匹配。

Method: 针对成本非平稳性（成本漂移和随机成本实现），将代表性ECTS方法适应到在线学习设置，聚焦可分离方法，在部署期间只更新触发模型而保持分类器固定，提出了包括基于bandit和基于强化学习的多种在线适应方法和基线。

Result: 在合成数据上的控制实验表明，在线学习能有效提高ECTS方法对成本漂移的鲁棒性，其中基于强化学习的策略在不同成本机制下表现出强大且稳定的性能。

Conclusion: 在线学习是应对ECTS中成本非平稳性的有效方法，特别是基于强化学习的策略在适应成本变化方面表现优异，为实际部署中的动态成本环境提供了解决方案。

Abstract: Early Classification of Time Series (ECTS) addresses decision-making problems in which predictions must be made as early as possible while maintaining high accuracy. Most existing ECTS methods assume that the time-dependent decision costs governing the learning objective are known, fixed, and correctly specified. In practice, however, these costs are often uncertain and may change over time, leading to mismatches between training-time and deployment-time objectives. In this paper, we study ECTS under two practically relevant forms of cost non-stationarity: drift in the balance between misclassification and decision delay costs, and stochastic realizations of decision costs that deviate from the nominal training-time model. To address these challenges, we revisit representative ECTS approaches and adapt them to an online learning setting. Focusing on separable methods, we update only the triggering model during deployment, while keeping the classifier fixed. We propose several online adaptations and baselines, including bandit-based and RL-based approaches, and conduct controlled experiments on synthetic data to systematically evaluate robustness under cost non-stationarity. Our results demonstrate that online learning can effectively improve the robustness of ECTS methods to cost drift, with RL-based strategies exhibiting strong and stable performance across varying cost regimes.

</details>


### [163] [Beyond What Seems Necessary: Hidden Gains from Scaling Training-Time Reasoning Length under Outcome Supervision](https://arxiv.org/abs/2602.00927)
*Yihao Xue,Allan Zhang,Jianhao Huang,Amit Sahai,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: 研究发现：在仅结果监督下，即使模型在分布内性能已饱和，增加训练时推理长度仍能持续提升分布外泛化能力，表明鲁棒性需要比ID验证更大的计算预算。


<details>
  <summary>Details</summary>
Motivation: 当前训练LLM进行长推理已成为构建先进模型的关键，但现有方法主要关注分布内性能。本文旨在探索训练时推理长度对分布外泛化能力的影响，发现ID性能饱和后OOD性能仍可提升的现象。

Method: 通过理论分析和实验验证：1）理论机制：自迭代能增强假设空间的归纳偏置，重塑ID最优解以改善OOD泛化；2）正则化机制：增加自迭代次数可减少对ID有效但OOD无效的捷径解的依赖。实验采用两种实现：循环Transformer增加循环次数，RL微调增加token预算。

Result: 实验证明：在合成任务中增加循环Transformer循环次数，在数学推理任务中增加RL微调token预算，均观察到ID性能饱和后OOD性能持续提升的现象，验证了理论预测。

Conclusion: 训练时推理长度是重要的扩展参数，仅依赖ID验证可能低估模型所需的计算预算。增加推理长度能通过改善归纳偏置和正则化机制提升OOD鲁棒性，这对构建更稳健的复杂问题解决模型具有重要意义。

Abstract: Training LLMs to think and reason for longer has become a key ingredient in building state-of-the-art models that can solve complex problems previously out of reach. Recent efforts pursue this in different ways, such as RL fine-tuning to elicit long CoT or scaling latent reasoning through architectural recurrence. This makes reasoning length an important scaling knob. In this work, we identify a novel phenomenon (both theoretically and experimentally): under outcome-only supervision, out-of-distribution (OOD) performance can continue improving as training-time reasoning length (e.g., the token budget in RL, or the loop count in looped Transformers) increases, even after in-distribution (ID) performance has saturated. This suggests that robustness may require a larger budget than ID validation alone would indicate. We provide theoretical explanations via two mechanisms: (i) self-iteration can induce a stronger inductive bias in the hypothesis class, reshaping ID-optimal solutions in ways that improve OOD generalization; and (ii) when shortcut solutions that work for ID samples but not for OOD samples persist in the hypothesis class, regularization can reduce the learned solution's reliance on these shortcuts as the number of self-iterations increases. We complement the theory with empirical evidence from two realizations of scaling training-time reasoning length: increasing the number of loops in looped Transformers on a synthetic task, and increasing token budgets during RL fine-tuning of LLMs on mathematical reasoning.

</details>


### [164] [Continuous-Utility Direct Preference Optimization](https://arxiv.org/abs/2602.00931)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Zihao He,Muhammad Usman Rafique,Asad Aali,Muhammad Ali Jamshed,John M. Cioffi,Emily Fox*

Main category: cs.LG

TL;DR: CU-DPO：用连续效用分数替代二元标签，通过策略选择和执行优化两阶段训练，提升大语言模型推理能力


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型推理能力评估过于简单，使用二元偏好监督无法捕捉推理过程中的部分进展和细粒度质量，需要更精细的评估和优化方法

Method: 提出CU-DPO框架：1）用连续分数替代二元标签捕捉细粒度推理质量；2）两阶段训练：策略选择阶段通过最佳vs所有比较优化模型选择最佳策略，执行优化阶段使用边缘分层对训练模型正确执行选定策略

Result: 在数学推理基准上，策略选择准确率从35-46%提升到68-78%，在下游推理任务上获得高达6.6分的提升，并能有效迁移到分布外任务

Conclusion: CU-DPO通过连续效用分数和两阶段训练显著提升大语言模型的推理能力，理论证明其样本复杂度优于传统二元偏好方法

Abstract: Large language model reasoning is often treated as a monolithic capability, relying on binary preference supervision that fails to capture partial progress or fine-grained reasoning quality. We introduce Continuous Utility Direct Preference Optimization (CU-DPO), a framework that aligns models to a portfolio of prompt-based cognitive strategies by replacing binary labels with continuous scores that capture fine-grained reasoning quality. We prove that learning with K strategies yields a Theta(K log K) improvement in sample complexity over binary preferences, and that DPO converges to the entropy-regularized utility-maximizing policy. To exploit this signal, we propose a two-stage training pipeline: (i) strategy selection, which optimizes the model to choose the best strategy for a given problem via best-vs-all comparisons, and (ii) execution refinement, which trains the model to correctly execute the selected strategy using margin-stratified pairs. On mathematical reasoning benchmarks, CU-DPO improves strategy selection accuracy from 35-46 percent to 68-78 percent across seven base models, yielding consistent downstream reasoning gains of up to 6.6 points on in-distribution datasets with effective transfer to out-of-distribution tasks.

</details>


### [165] [SALAAD: Sparse And Low-Rank Adaptation via ADMM](https://arxiv.org/abs/2602.00942)
*Hao Ma,Melis Ilayda Bal,Liang Zhang,Bingcong Li,Niao He,Melanie Zeilinger,Michael Muehlebach*

Main category: cs.LG

TL;DR: SALAAD是一个即插即用的框架，通过稀疏和低秩结构在训练中动态控制模型容量，实现内存消耗减少和弹性部署。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型在计算和内存受限环境下部署，需要灵活控制模型容量。现有方法依赖启发式设计，忽略了层和矩阵的异质性，或需要模型特定的架构修改。

Method: 提出SALAAD框架，在增强拉格朗日框架下制定结构化权重学习，引入自适应控制器动态平衡训练损失和结构约束，保持标准训练动态稳定性，同时显式控制有效模型容量的演化。

Result: 在不同模型规模上的实验表明，SALAAD显著减少了部署时的内存消耗，同时实现了与专门方法相当的性能。单次训练运行产生连续模型容量谱，无需重新训练即可在不同内存预算下实现平滑弹性部署。

Conclusion: SALAAD提供了一个通用框架，通过稀疏和低秩结构在训练中动态控制模型容量，解决了大语言模型在受限环境下的部署挑战，实现了内存效率与性能的良好平衡。

Abstract: Modern large language models are increasingly deployed under compute and memory constraints, making flexible control of model capacity a central challenge. While sparse and low-rank structures naturally trade off capacity and performance, existing approaches often rely on heuristic designs that ignore layer and matrix heterogeneity or require model-specific architectural modifications. We propose SALAAD, a plug-and-play framework applicable to different model architectures that induces sparse and low-rank structures during training. By formulating structured weight learning under an augmented Lagrangian framework and introducing an adaptive controller that dynamically balances the training loss and structural constraints, SALAAD preserves the stability of standard training dynamics while enabling explicit control over the evolution of effective model capacity during training. Experiments across model scales show that SALAAD substantially reduces memory consumption during deployment while achieving performance comparable to ad-hoc methods. Moreover, a single training run yields a continuous spectrum of model capacities, enabling smooth and elastic deployment across diverse memory budgets without the need for retraining.

</details>


### [166] [Dynamic Prior Thompson Sampling for Cold-Start Exploration in Recommender Systems](https://arxiv.org/abs/2602.00943)
*Zhenyu Zhao,David Zhang,Ellie Zhao,Ehsan Saberian*

Main category: cs.LG

TL;DR: 提出动态先验汤普森采样方法，通过调整先验分布控制新物品的探索概率，解决推荐系统中冷启动探索过度的问题。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中冷启动探索面临核心挑战：新物品或数据稀疏物品需要流量来估计价值，但过度探索会损害用户体验并浪费曝光机会。传统汤普森采样使用均匀Beta(1,1)先验，隐含假设新物品有50%成功率，当真实基础率远低于此值时，这种乐观先验会系统性地过度分配给弱物品。

Method: 提出动态先验汤普森采样，通过设计先验分布直接控制新物品胜过现有优胜者的概率。核心贡献是提供先验均值的闭式二次解，确保在引入时P(X_j > Y_k) = epsilon，使探索强度可预测且可调，同时保留汤普森采样的贝叶斯更新特性。

Result: 通过蒙特卡洛验证、离线批量模拟和在服务于数百万用户的缩略图个性化系统上进行的大规模在线实验，动态先验相比均匀先验基线提供了精确的探索控制和改进的效率。

Conclusion: 动态先验汤普森采样方法能够有效控制推荐系统中冷启动探索的强度，避免过度探索造成的资源浪费和用户体验损害，在实际应用中表现出优于传统均匀先验的性能。

Abstract: Cold-start exploration is a core challenge in large-scale recommender systems: new or data-sparse items must receive traffic to estimate value, but over-exploration harms users and wastes impressions. In practice, Thompson Sampling (TS) is often initialized with a uniform Beta(1,1) prior, implicitly assuming a 50% success rate for unseen items. When true base rates are far lower, this optimistic prior systematically over-allocates to weak items. The impact is amplified by batched policy updates and pipeline latency: for hours, newly launched items can remain effectively "no data," so the prior dominates allocation before feedback is incorporated. We propose Dynamic Prior Thompson Sampling, a prior design that directly controls the probability that a new arm outcompetes the incumbent winner. Our key contribution is a closed-form quadratic solution for the prior mean that enforces P(X_j > Y_k) = epsilon at introduction time, making exploration intensity predictable and tunable while preserving TS Bayesian updates. Across Monte Carlo validation, offline batched simulations, and a large-scale online experiment on a thumbnail personalization system serving millions of users, dynamic priors deliver precise exploration control and improved efficiency versus a uniform-prior baseline.

</details>


### [167] [Optimal Budgeted Adaptation of Large Language Models](https://arxiv.org/abs/2602.00952)
*Jing Wang,Jie Shen,Dean Foster,Zohar Karnin,Jeremy C Weiss*

Main category: cs.LG

TL;DR: 提出了一种基于上下文Stackelberg博弈的预算感知监督微调框架，通过将LLM适应建模为领导者-追随者博弈，在有限标注预算下实现高效的模型微调。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型微调中标注数据可用性与下游准确性之间的权衡问题，特别是在有限标注预算下的高效微调挑战。

Method: 将LLM适应建模为上下文Stackelberg博弈：学习者（领导者）承诺评分策略和标签查询策略，自适应环境（追随者）选择具有挑战性的监督替代方案。引入有限监督预算到学习目标中，并提出了带有最大延迟优先（LLF）置信门的扩展框架。

Result: 在全反馈机制下，算法在标准线性上下文假设下实现了$\tilde{O}(d\sqrt{T})$的遗憾。通过LLF置信门选择性查询标签，获得了$\tilde{O}(\sqrt{dB} + c\sqrt{B})$的预算感知遗憾界限，其中$B=βT$。

Conclusion: 该框架为预算受限的LLM监督微调提供了理论保证，通过博弈论方法平衡了标注成本与模型性能，为实际应用中的资源约束问题提供了解决方案。

Abstract: The trade-off between labeled data availability and downstream accuracy remains a central challenge in fine-tuning large language models (LLMs). We propose a principled framework for \emph{budget-aware supervised fine-tuning} by casting LLM adaptation as a contextual Stackelberg game. In our formulation, the learner (leader) commits to a scoring policy and a label-querying strategy, while an adaptive environment (follower) selects challenging supervised alternatives in response. To explicitly address label efficiency, we incorporate a finite supervision budget directly into the learning objective. Our algorithm operates in the full-feedback regime and achieves $\tilde{O}(d\sqrt{T})$ regret under standard linear contextual assumptions. We extend the framework with a Largest-Latency-First (LLF) confidence gate that selectively queries labels, achieving a budget-aware regret bound of $\tilde{O}(\sqrt{dB} + c\sqrt{B})$ with $B=βT$.

</details>


### [168] [SAGE: Agentic Framework for Interpretable and Clinically Translatable Computational Pathology Biomarker Discovery](https://arxiv.org/abs/2602.00953)
*Sahar Almahfouz Nasser,Juan Francisco Pesantez Borja,Jincheng Liu,Tanvir Hasan,Zenghan Wang,Suman Ghosh,Sandeep Manandhar,Shikhar Shiromani,Twisha Shah,Naoto Tokuyama,Anant Madabhushi*

Main category: cs.LG

TL;DR: SAGE是一个基于代理的AI系统，通过整合文献推理和多模态数据分析，从病理图像中识别可解释的工程化生物标志物，并将其与分子生物标志物和临床结果关联，提高计算病理学的临床可解释性和可转化性。


<details>
  <summary>Details</summary>
Motivation: 当前计算病理学中的AI模型多为黑盒，缺乏可解释性，阻碍了临床采用。虽然工程化的图像生物标志物更具可解释性，但通常基于轶事证据或碎片化文献提出，缺乏系统性的生物学验证。

Method: SAGE采用结构化代理系统，整合文献锚定推理和多模态数据分析，通过协调专门代理进行生物学情境化和经验假设验证，将图像特征与基因表达等分子生物标志物及临床结果关联。

Result: SAGE能够优先选择透明且生物学支持的生物标志物，推动计算病理学的临床转化，通过系统性的生物学验证提高生物标志物的可靠性和可解释性。

Conclusion: SAGE通过代理AI系统将图像特征与生物学证据系统关联，解决了计算病理学中黑盒模型的可解释性问题，为临床采用提供了更透明、生物学支持的工程化生物标志物。

Abstract: Despite significant progress in computational pathology, many AI models remain black-box and difficult to interpret, posing a major barrier to clinical adoption due to limited transparency and explainability. This has motivated continued interest in engineered image-based biomarkers, which offer greater interpretability but are often proposed based on anecdotal evidence or fragmented prior literature rather than systematic biological validation. We introduce SAGE (Structured Agentic system for hypothesis Generation and Evaluation), an agentic AI system designed to identify interpretable, engineered pathology biomarkers by grounding them in biological evidence. SAGE integrates literature-anchored reasoning with multimodal data analysis to correlate image-derived features with molecular biomarkers, such as gene expression, and clinically relevant outcomes. By coordinating specialized agents for biological contextualization and empirical hypothesis validation, SAGE prioritizes transparent, biologically supported biomarkers and advances the clinical translation of computational pathology.

</details>


### [169] [From drift to adaptation to the failed ml model: Transfer Learning in Industrial MLOps](https://arxiv.org/abs/2602.00957)
*Waqar Muhammad Ashraf,Talha Ansar,Fahad Ahmed,Jawad Hussain,Muhammad Mujtaba Abbas,Vivek Dua*

Main category: cs.LG

TL;DR: 该论文比较了三种迁移学习方法（ETL、ALTL、LLTL）用于更新因数据漂移而失效的ANN模型，以火电厂烟气差压监测为案例，发现不同批量大小下各方法效果不同。


<details>
  <summary>Details</summary>
Motivation: MLOps中模型适应生产环境很重要，但缺乏系统框架来更新因数据漂移而失效的模型。需要比较不同迁移学习策略在工业批处理场景下的效果。

Method: 使用三种迁移学习策略更新失效的ANN模型：集成迁移学习（ETL）、全层迁移学习（ALTL）和最后一层迁移学习（LLTL）。以660MW火电厂空气预热器烟气差压监测为案例研究，模拟电厂负荷循环的批处理过程。

Result: ETL在5天批量大小下预测精度最高；ALTL适合8天大批量大小的有效更新；不同批量大小下计算需求（超参数调优和模型训练）呈现混合趋势。

Conclusion: 从工业批处理案例中获得的基本经验和实证见解可帮助MLOps从业者适应失效模型到数据漂移，实现工业过程的准确监测。不同批量大小下需要选择不同的迁移学习策略。

Abstract: Model adaptation to production environment is critical for reliable Machine Learning Operations (MLOps), less attention is paid to developing systematic framework for updating the ML models when they fail under data drift. This paper compares the transfer learning enabled model update strategies including ensemble transfer learning (ETL), all-layers transfer learning (ALTL), and last-layer transfer learning (LLTL) for updating the failed feedforward artificial neural network (ANN) model. The flue gas differential pressure across the air preheater unit installed in a 660 MW thermal power plant is analyzed as a case study since it mimics the batch processes due to load cycling in the power plant. Updating the failed ANN model by three transfer learning techniques reveals that ETL provides relatively higher predictive accuracy for the batch size of 5 days than those of LLTL and ALTL. However, ALTL is found to be suitable for effective update of the model trained on large batch size (8 days). A mixed trend is observed for computational requirement (hyperparameter tuning and model training) of model update techniques for different batch sizes. These fundamental and empiric insights obtained from the batch process-based industrial case study can assist the MLOps practitioners in adapting the failed models to data drifts for the accurate monitoring of industrial processes.

</details>


### [170] [Probing the Knowledge Boundary: An Interactive Agentic Framework for Deep Knowledge Extraction](https://arxiv.org/abs/2602.00959)
*Yuheng Yang,Siqi Zhu,Tao Feng,Ge Liu,Jiaxuan You*

Main category: cs.LG

TL;DR: 提出一个交互式代理框架来系统提取和量化LLM的知识，通过四种自适应探索策略在不同粒度上探测知识，发现递归分类法最有效，观察到知识扩展定律，识别出Pass@1与Pass@k的权衡，并显示不同训练数据组成导致可测量的知识差异。


<details>
  <summary>Details</summary>
Motivation: LLMs被视为压缩的知识库，但尚不清楚它们真正包含什么知识以及知识边界在哪里。现有基准大多是静态的，对系统知识探测支持有限。

Method: 提出交互式代理框架，包含四种自适应探索策略在不同粒度上探测知识。采用三阶段知识处理流程：向量过滤去除重复、LLM裁决解决语义重叠、领域相关性审计保留有效知识单元。

Result: 递归分类法是最有效的探索策略；观察到清晰的知识扩展定律（更大模型提取更多知识）；识别出Pass@1与Pass@k权衡（专业模型初始准确率高但退化快，通用模型性能稳定）；不同训练数据组成导致可测量的知识差异。

Conclusion: 提出的交互式框架能系统提取和量化LLM知识，揭示了模型规模、专业性与通用性、训练数据组成对知识分布的影响，为理解LLM知识边界提供了新方法。

Abstract: Large Language Models (LLMs) can be seen as compressed knowledge bases, but it remains unclear what knowledge they truly contain and how far their knowledge boundaries extend. Existing benchmarks are mostly static and provide limited support for systematic knowledge probing. In this paper, we propose an interactive agentic framework to systematically extract and quantify the knowledge of LLMs. Our method includes four adaptive exploration policies to probe knowledge at different granularities. To ensure the quality of extracted knowledge, we introduce a three-stage knowledge processing pipeline that combines vector-based filtering to remove exact duplicates, LLM-based adjudication to resolve ambiguous semantic overlaps, and domain-relevance auditing to retain valid knowledge units. Through extensive experiments, we find that recursive taxonomy is the most effective exploration strategy. We also observe a clear knowledge scaling law, where larger models consistently extract more knowledge. In addition, we identify a Pass@1-versus-Pass@k trade-off: domain-specialized models achieve higher initial accuracy but degrade rapidly, while general-purpose models maintain stable performance during extended extraction. Finally, our results show that differences in training data composition lead to distinct and measurable knowledge profiles across model families.

</details>


### [171] [Multimodal Scientific Learning Beyond Diffusions and Flows](https://arxiv.org/abs/2602.00960)
*Leonardo Ferreira Guilhoto,Akshat Kaushal,Paris Perdikaris*

Main category: cs.LG

TL;DR: MDNs作为显式参数密度估计器，在科学机器学习中为多模态不确定性量化提供了一种高效、可解释的替代方案，特别适合低维多模态物理问题。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习需要处理多模态条件不确定性（如不适定逆问题、多稳态和混沌动力学），而当前流行的隐式生成模型（如扩散模型和流模型）存在数据需求大、计算成本高、与科学问题的结构化解空间不匹配等问题。

Method: 提出使用混合密度网络（MDNs）作为显式参数密度估计器，通过直接全局分配概率质量到不同解分支，为低维多模态物理问题提供结构化归纳偏置。

Result: MDNs在多种逆问题、多稳态和混沌科学回归任务中表现出优异的泛化能力、可解释性和样本效率，特别是在科学数据稀缺的情况下仍能可靠恢复分离的模式。

Conclusion: MDNs为科学机器学习中的多模态不确定性量化提供了一个被忽视但有效的替代方案，具有数据效率高、计算成本低、与科学问题结构对齐等优势。

Abstract: Scientific machine learning (SciML) increasingly requires models that capture multimodal conditional uncertainty arising from ill-posed inverse problems, multistability, and chaotic dynamics. While recent work has favored highly expressive implicit generative models such as diffusion and flow-based methods, these approaches are often data-hungry, computationally costly, and misaligned with the structured solution spaces frequently found in scientific problems. We demonstrate that Mixture Density Networks (MDNs) provide a principled yet largely overlooked alternative for multimodal uncertainty quantification in SciML. As explicit parametric density estimators, MDNs impose an inductive bias tailored to low-dimensional, multimodal physics, enabling direct global allocation of probability mass across distinct solution branches. This structure delivers strong data efficiency, allowing reliable recovery of separated modes in regimes where scientific data is scarce. We formalize these insights through a unified probabilistic framework contrasting explicit and implicit distribution networks, and demonstrate empirically that MDNs achieve superior generalization, interpretability, and sample efficiency across a range of inverse, multistable, and chaotic scientific regression tasks.

</details>


### [172] [On the Spectral Flattening of Quantized Embeddings](https://arxiv.org/abs/2602.00969)
*Junlin Huang,Wenyi Fang,Zhenheng Tang,Yuxin Wang,Xueze Kang,Yang Zheng,Bo Li,Xiaowen Chu*

Main category: cs.LG

TL;DR: 该论文证明语言数据的幂律谱特性是语义编码的基本要求，而均匀量化会截断谱尾导致表示崩溃，建立了谱保真度作为稳定低比特优化的必要条件。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型时，超低精度量化受到不稳定性困扰，这种不稳定性源于离散量化约束与语言数据固有的重尾谱特性之间的冲突。需要理解量化如何影响语言表示的谱结构。

Method: 通过形式化Zipf统计与随机矩阵理论之间的联系，证明嵌入奇异值谱的幂律衰减是语义编码的基本要求。推导理论界限显示均匀量化引入噪声基底会不成比例地截断谱尾，导致谱平坦化和稳定秩增加。在GPT-2和TinyLlama等架构上进行实证验证。

Result: 理论分析表明均匀量化引入的噪声基底会截断谱尾，导致谱平坦化和表示稳定秩的严格可证明增加。实证验证证实这种几何退化会导致表示崩溃。量化了LLMs的谱敏感性，并建立了谱保真度作为稳定低比特优化的必要条件。

Conclusion: 语言数据的幂律谱特性是语义编码的基本要求，均匀量化会破坏这种谱结构导致表示崩溃。谱保真度是稳定低比特优化的必要条件，这为设计更有效的量化方法提供了理论基础。

Abstract: Training Large Language Models (LLMs) at ultra-low precision is critically impeded by instability rooted in the conflict between discrete quantization constraints and the intrinsic heavy-tailed spectral nature of linguistic data. By formalizing the connection between Zipfian statistics and random matrix theory, we prove that the power-law decay in the singular value spectra of embeddings is a fundamental requisite for semantic encoding. We derive theoretical bounds showing that uniform quantization introduces a noise floor that disproportionately truncates this spectral tail, which induces spectral flattening and a strictly provable increase in the stable rank of representations. Empirical validation across diverse architectures including GPT-2 and TinyLlama corroborates that this geometric degradation precipitates representational collapse. This work not only quantifies the spectral sensitivity of LLMs but also establishes spectral fidelity as a necessary condition for stable low-bit optimization.

</details>


### [173] [Forest-Guided Semantic Transport for Label-Supervised Manifold Alignment](https://arxiv.org/abs/2602.00974)
*Adrien Aumon,Myriam Lizotte,Guy Wolf,Kevin R. Moon,Jake S. Rhodes*

Main category: cs.LG

TL;DR: FoSTA是一个利用森林诱导几何来降噪并恢复任务相关流形的多模态数据对齐框架，通过标签信息构建语义表示并进行层次语义传输对齐。


<details>
  <summary>Details</summary>
Motivation: 现有标签监督的流形对齐方法大多依赖欧几里得几何建模域内关系，当特征与任务兴趣弱相关时，会产生噪声和语义误导结构，导致对齐质量下降。

Method: FoSTA利用森林诱导几何降噪域内结构，从标签信息化的森林亲和度直接构建语义表示，通过快速层次语义传输进行对齐，捕捉有意义的跨域关系。

Result: 在合成基准测试中，FoSTA改进了对应关系恢复和标签转移性能；在实际单细胞应用中，包括批次校正和生物保守性方面，表现出强大性能。

Conclusion: FoSTA通过森林诱导几何有效解决了现有方法在弱相关特征下的噪声问题，提升了多模态数据对齐的质量和实用性。

Abstract: Label-supervised manifold alignment bridges the gap between unsupervised and correspondence-based paradigms by leveraging shared label information to align multimodal datasets. Still, most existing methods rely on Euclidean geometry to model intra-domain relationships. This approach can fail when features are only weakly related to the task of interest, leading to noisy, semantically misleading structure and degraded alignment quality. To address this limitation, we introduce FoSTA (Forest-guided Semantic Transport Alignment), a scalable alignment framework that leverages forest-induced geometry to denoise intra-domain structure and recover task-relevant manifolds prior to alignment. FoSTA builds semantic representations directly from label-informed forest affinities and aligns them via fast, hierarchical semantic transport, capturing meaningful cross-domain relationships. Extensive comparisons with established baselines demonstrate that FoSTA improves correspondence recovery and label transfer on synthetic benchmarks and delivers strong performance in practical single-cell applications, including batch correction and biological conservation.

</details>


### [174] [Scalable Random Wavelet Features: Efficient Non-Stationary Kernel Approximation with Convergence Guarantees](https://arxiv.org/abs/2602.00987)
*Sawan Kumar,Souvik Chakraborty*

Main category: cs.LG

TL;DR: 提出Random Wavelet Features (RWF)框架，通过采样小波族构建可扩展的非平稳核近似，解决了传统方法在表达能力和计算效率之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 大多数可扩展的机器学习方法依赖于平稳性假设，这导致了一个困难的权衡：要么使用表达能力强但计算成本高的模型（如深度高斯过程），要么使用可扩展但表达能力有限的方法（如随机傅里叶特征）。需要一种既能处理非平稳过程又能保持计算效率的方法。

Method: 引入Random Wavelet Features (RWF)框架，通过从小波族中采样来构建可扩展的非平稳核近似。利用小波固有的局部化和多分辨率结构，生成显式特征映射以捕获复杂的输入相关模式。该方法为将随机傅里叶特征推广到非平稳设置提供了原则性方法。

Result: RWF在理论分析上具有正定性、无偏性和一致收敛保证。在多个具有挑战性的合成和真实数据集上的实验表明，RWF优于平稳随机特征，并在与更复杂模型的比较中提供了有竞争力的准确率-效率权衡。

Conclusion: RWF框架为广泛的现实世界非平稳问题解锁了可扩展且表达力强的核方法，填补了表达能力强但计算成本高的模型与可扩展但表达能力有限的方法之间的空白。

Abstract: Modeling non-stationary processes, where statistical properties vary across the input domain, is a critical challenge in machine learning; yet most scalable methods rely on a simplifying assumption of stationarity. This forces a difficult trade-off: use expressive but computationally demanding models like Deep Gaussian Processes, or scalable but limited methods like Random Fourier Features (RFF). We close this gap by introducing Random Wavelet Features (RWF), a framework that constructs scalable, non-stationary kernel approximations by sampling from wavelet families. By harnessing the inherent localization and multi-resolution structure of wavelets, RWF generates an explicit feature map that captures complex, input-dependent patterns. Our framework provides a principled way to generalize RFF to the non-stationary setting and comes with a comprehensive theoretical analysis, including positive definiteness, unbiasedness, and uniform convergence guarantees. We demonstrate empirically on a range of challenging synthetic and real-world datasets that RWF outperforms stationary random features and offers a compelling accuracy-efficiency trade-off against more complex models, unlocking scalable and expressive kernel methods for a broad class of real-world non-stationary problems.

</details>


### [175] [ESSAM: A Novel Competitive Evolution Strategies Approach to Reinforcement Learning for Memory Efficient LLMs Fine-Tuning](https://arxiv.org/abs/2602.01003)
*Zhishen Sun,Sizhe Dang,Guang Dai,Haishan Ye*

Main category: cs.LG

TL;DR: ESSAM结合进化策略和锐度感知最大化，在数学推理任务上实现与强化学习方法相当的性能，同时大幅降低GPU内存使用（相比PPO减少18倍，相比GRPO减少10倍）。


<details>
  <summary>Details</summary>
Motivation: 强化学习在提升大语言模型数学推理能力方面很有效，但GPU内存使用过高，限制了其在资源受限环境中的应用。需要开发更高效的全参数微调方法。

Method: 提出ESSAM框架，将进化策略（ES）的参数空间零阶搜索与锐度感知最大化（SAM）紧密结合，以提升泛化能力。在GSM8K数学推理任务上进行全参数微调实验。

Result: ESSAM在所有模型上平均准确率达到78.27%，性能与RL方法相当：超越PPO（77.72%），与GRPO（78.34%）相当，在某些模型上甚至超越。GPU内存使用方面，相比PPO减少18倍，相比GRPO减少10倍，实现极低的内存占用。

Conclusion: ESSAM提供了一种高效的全参数微调方案，在保持与强化学习方法相当性能的同时，大幅降低了GPU内存需求，使其更适合资源受限的环境。

Abstract: Reinforcement learning (RL) has become a key training step for improving mathematical reasoning in large language models (LLMs), but it often has high GPU memory usage, which makes it hard to use in settings with limited resources. To reduce these issues, we propose Evolution Strategies with Sharpness-Aware Maximization (ESSAM), a full parameter fine-tuning framework that tightly combines the zero-order search in parameter space from Evolution Strategies (ES) with the Sharpness-Aware Maximization (SAM) to improve generalization. We conduct fine-tuning experiments on the mainstream mathematica reasoning task GSM8K. The results show that ESSAM achieves an average accuracy of 78.27\% across all models and its overall performance is comparable to RL methods. It surpasses classic RL algorithm PPO with an accuracy of 77.72\% and is comparable to GRPO with an accuracy of 78.34\%, and even surpassing them on some models. In terms of GPU memory usage, ESSAM reduces the average GPU memory usage by $18\times$ compared to PPO and by $10\times$ compared to GRPO, achieving an extremely low GPU memory usage.

</details>


### [176] [Predicting Anemia Among Under-Five Children in Nepal Using Machine Learning and Deep Learning](https://arxiv.org/abs/2602.01005)
*Deepak Bastola,Pitambar Acharya,Dipak Dulal,Rabina Dhakal,Yang Li*

Main category: cs.LG

TL;DR: 该研究使用机器学习方法预测尼泊尔儿童贫血，通过特征选择确定关键风险因素，比较多种模型性能，发现逻辑回归在F1分数和召回率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 儿童贫血是尼泊尔重要的公共卫生问题，与生长发育受损、认知障碍和发病率增加相关。需要开发有效的预测模型来识别高风险儿童，为公共卫生干预提供依据。

Method: 使用尼泊尔人口与健康调查(NDHS 2022)的1,855名6-59个月儿童数据，将贫血状态定义为二分类任务。采用四种特征选择方法(卡方检验、互信息、点二列相关、Boruta)确定关键特征，然后比较8种传统机器学习模型和2种深度学习模型的性能。

Result: 逻辑回归获得最佳召回率(0.701)和最高F1分数(0.649)，DNN达到最高准确率(0.709)，SVM获得最强区分能力(AUC=0.736)。所有方法一致选择的五个关键特征包括：儿童年龄、近期发热、家庭规模、母亲贫血状况和寄生虫驱虫情况。

Conclusion: 机器学习和深度学习模型都能提供有竞争力的贫血预测能力，可解释的特征如儿童年龄、感染指标、母亲贫血状况和驱虫历史对尼泊尔儿童贫血风险分层和公共卫生筛查至关重要。

Abstract: Childhood anemia remains a major public health challenge in Nepal and is associated with impaired growth, cognition, and increased morbidity. Using World Health Organization hemoglobin thresholds, we defined anemia status for children aged 6-59 months and formulated a binary classification task by grouping all anemia severities as \emph{anemic} versus \emph{not anemic}. We analyzed Nepal Demographic and Health Survey (NDHS 2022) microdata comprising 1,855 children and initially considered 48 candidate features spanning demographic, socioeconomic, maternal, and child health characteristics. To obtain a stable and substantiated feature set, we applied four features selection techniques (Chi-square, mutual information, point-biserial correlation, and Boruta) and prioritized features supported by multi-method consensus. Five features: child age, recent fever, household size, maternal anemia, and parasite deworming were consistently selected by all methods, while amenorrhea, ethnicity indicators, and provinces were frequently retained. We then compared eight traditional machine learning classifiers (LR, KNN, DT, RF, XGBoost, SVM, NB, LDA) with two deep learning models (DNN and TabNet) using standard evaluation metrics, emphasizing F1-score and recall due to class imbalance. Among all models, logistic regression attained the best recall (0.701) and the highest F1-score (0.649), while DNN achieved the highest accuracy (0.709), and SVM yielded the strongest discrimination with the highest AUC (0.736). Overall, the results indicate that both machine learning and deep learning models can provide competitive anemia prediction and the interpretable features such as child age, infection proxy, maternal anemia, and deworming history are central for risk stratification and public health screening in Nepal.

</details>


### [177] [LASS-ODE: Scaling ODE Computations to Connect Foundation Models with Dynamical Physical Systems](https://arxiv.org/abs/2602.01009)
*Haoran Li,Chenhan Xiao,Lihao Mai,Yang Weng,Erik Blasch*

Main category: cs.LG

TL;DR: LASS-ODE是一个用于大规模ODE系统动态预测的基础模型，通过局部线性ODE表示和跨系统注意力机制，解决了物理计算可扩展性和知识共享效率问题。


<details>
  <summary>Details</summary>
Motivation: 基础模型在语言、视觉和时间序列分析中取得了成功，但在物理系统动态预测方面进展有限。主要面临两个挑战：1）物理计算可扩展性：物理约束学习需要ODE积分等计算，难以扩展到大规模系统；2）知识共享效率：注意力机制主要在单个系统内计算，难以提取跨系统的共享ODE结构。

Method: 1）提出局部线性ODE表示：通过token-wise的局部线性ODE表示来保持物理保真度，避免昂贵的非线性积分；2）引入跨系统注意力机制：通过公共结构中心（CSH）存储共享token并聚合跨系统知识；3）在40GB ODE轨迹数据集上预训练LASS-ODE模型。

Result: LASS-ODE在预训练后表现出：1）强大的领域内性能；2）跨多样ODE系统的零样本泛化能力；3）通过微调可获得额外改进。

Conclusion: LASS-ODE通过局部线性ODE表示和跨系统注意力机制，成功解决了物理基础模型中的计算可扩展性和知识共享问题，为大规模ODE系统动态预测提供了有效的解决方案。

Abstract: Foundation models have transformed language, vision, and time series data analysis, yet progress on dynamic predictions for physical systems remains limited. Given the complexity of physical constraints, two challenges stand out. $(i)$ Physics-computation scalability: physics-informed learning can enforce physical regularization, but its computation (e.g., ODE integration) does not scale to extensive systems. $(ii)$ Knowledge-sharing efficiency: the attention mechanism is primarily computed within each system, which limits the extraction of shared ODE structures across systems. We show that enforcing ODE consistency does not require expensive nonlinear integration: a token-wise locally linear ODE representation preserves physical fidelity while scaling to foundation-model regimes. Thus, we propose novel token representations that respect locally linear ODE evolution. Such linearity substantially accelerates integration while accurately approximating the local data manifold. Second, we introduce a simple yet effective inter-system attention that augments attention with a common structure hub (CSH) that stores shared tokens and aggregates knowledge across systems. The resulting model, termed LASS-ODE (\underline{LA}rge-\underline{S}cale \underline{S}mall \underline{ODE}), is pretrained on our $40$GB ODE trajectory collections to enable strong in-domain performance, zero-shot generalization across diverse ODE systems, and additional improvements through fine-tuning.

</details>


### [178] [How Does Unfaithful Reasoning Emerge from Autoregressive Training? A Study of Synthetic Experiments](https://arxiv.org/abs/2602.01017)
*Fuxin Wang,Amr Alazali,Yiqiao Zhong*

Main category: cs.LG

TL;DR: 论文通过可控合成实验研究思维链推理的忠实性问题，发现训练噪声低于临界阈值时模型能学习忠实推理，噪声过高则转向不忠实的跳跃推理，揭示了自回归训练中隐含自我验证机制的出现。


<details>
  <summary>Details</summary>
Motivation: 尽管观察到LLM生成的思维链推理常不忠实（中间步骤逻辑不一致或无法反映因果），但对思维链的基本理解仍缺乏——什么是忠实的思维链推理，以及不忠实性如何从自回归训练中产生。

Method: 使用可控合成实验，训练小型transformer在噪声数据上逐步解决模算术表达式（称为算术表达式推理任务），分析训练动态和机制。

Result: 发现模型能学习忠实推理（因果遵循底层算术规则），但仅当训练噪声低于临界阈值（归因于简单性偏差）。噪声较高时，训练动态从忠实逐步推理转变为不忠实跳跃推理，中间出现预测熵短暂增加的混合模式。机制分析显示模型学会通过解决不一致推理步骤来编码内部不确定性。

Conclusion: 研究揭示了思维链推理忠实性的临界阈值现象，以及自回归训练中隐含自我验证机制的出现，为理解LLM推理能力提供了理论基础。

Abstract: Chain-of-thought (CoT) reasoning generated by large language models (LLMs) is often unfaithful: intermediate steps can be logically inconsistent or fail to reflect the causal relationship leading to the final answer. Despite extensive empirical observations, a fundamental understanding of CoT is lacking--what constitutes faithful CoT reasoning, and how unfaithfulness emerges from autoregressive training. We study these questions using well-controlled synthetic experiments, training small transformers on noisy data to solve modular arithmetic expressions step by step, a task we term Arithmetic Expression Reasoning. We find that models can learn faithful reasoning that causally follows the underlying arithmetic rules, but only when the training noise is below a critical threshold, a phenomenon attributable to simplicity bias. At higher noise levels, training dynamics exhibit a transition from faithful stepwise reasoning to unfaithful skip-step reasoning via an intermediate mixed mode characterized by a transient increase in prediction entropy. Mechanistic analysis reveals that models learn to encode internal uncertainty by resolving inconsistent reasoning steps, which suggests the emergence of implicit self-verification from autoregressive training.

</details>


### [179] [Toward Universal and Transferable Jailbreak Attacks on Vision-Language Models](https://arxiv.org/abs/2602.01025)
*Kaiyuan Cui,Yige Li,Yutao Wu,Xingjun Ma,Sarah Erfani,Christopher Leckie,Hanxun Huang*

Main category: cs.LG

TL;DR: 提出UltraBreak框架，通过视觉空间变换正则化和语义引导的文本目标，实现跨模型和攻击目标的通用对抗模式生成，解决现有梯度方法过拟合和迁移性差的问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型扩展了攻击面，容易受到图像基越狱攻击。现有基于梯度的越狱方法迁移性差，对抗模式会过拟合到单个白盒代理模型，无法泛化到黑盒模型。

Method: 提出UltraBreak框架：1) 在视觉空间通过变换和正则化约束对抗模式；2) 通过语义基目标放松文本目标；3) 在目标LLM的文本嵌入空间定义损失函数，发现通用对抗模式。

Result: 实验表明UltraBreak持续优于现有越狱方法。分析揭示早期方法失败原因：通过语义目标平滑损失景观对实现通用可迁移越狱至关重要。

Conclusion: UltraBreak通过视觉级正则化和语义引导的文本监督，缓解代理过拟合，实现跨模型和攻击目标的强迁移性，为视觉语言模型安全提供新见解。

Abstract: Vision-language models (VLMs) extend large language models (LLMs) with vision encoders, enabling text generation conditioned on both images and text. However, this multimodal integration expands the attack surface by exposing the model to image-based jailbreaks crafted to induce harmful responses. Existing gradient-based jailbreak methods transfer poorly, as adversarial patterns overfit to a single white-box surrogate and fail to generalise to black-box models. In this work, we propose Universal and transferable jailbreak (UltraBreak), a framework that constrains adversarial patterns through transformations and regularisation in the vision space, while relaxing textual targets through semantic-based objectives. By defining its loss in the textual embedding space of the target LLM, UltraBreak discovers universal adversarial patterns that generalise across diverse jailbreak objectives. This combination of vision-level regularisation and semantically guided textual supervision mitigates surrogate overfitting and enables strong transferability across both models and attack targets. Extensive experiments show that UltraBreak consistently outperforms prior jailbreak methods. Further analysis reveals why earlier approaches fail to transfer, highlighting that smoothing the loss landscape via semantic objectives is crucial for enabling universal and transferable jailbreaks. The code is publicly available in our \href{https://github.com/kaiyuanCui/UltraBreak}{GitHub repository}.

</details>


### [180] [SFMP: Fine-Grained, Hardware-Friendly and Search-Free Mixed-Precision Quantization for Large Language Models](https://arxiv.org/abs/2602.01027)
*Xin Nie,Haicheng Zhang,Liang Dong,Beining Feng,Jinhong Weng,Guiling Sun*

Main category: cs.LG

TL;DR: SFMP提出了一种无需搜索、硬件友好的混合精度量化框架，通过分数位宽、块级混合精度、行列重排和统一GEMM内核，在相同内存约束下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有混合精度方法存在两个主要问题：要么依赖昂贵的离散优化来确定精度分配，要么由于不规则内存布局导致硬件效率低下。需要一种既高效又硬件友好的混合精度量化方案。

Method: 1) 分数位宽：将权重矩阵的整数位宽扩展到分数值，将离散精度分配转化为连续问题；2) 块级混合精度：在权重矩阵内实现细粒度精度，同时保持硬件友好；3) 行列权重重排：通过行列重排聚合重要权重，推理时仅产生小的激活重排开销；4) 统一GEMM内核：支持任意平均位宽的混合精度GEMM。

Result: 大量实验表明，SFMP在相同内存约束下优于最先进的层级混合精度方法，同时显著降低量化成本并提高推理效率。

Conclusion: SFMP框架成功解决了现有混合精度量化方法的局限性，提供了一种无需搜索、硬件友好的高效解决方案，在大语言模型压缩方面具有显著优势。

Abstract: Mixed-precision quantization is a promising approach for compressing large language models under tight memory budgets. However, existing mixed-precision methods typically suffer from one of two limitations: they either rely on expensive discrete optimization to determine precision allocation, or introduce hardware inefficiencies due to irregular memory layouts. We propose SFMP, a search-free and hardware-friendly mixed-precision quantization framework for large language models. The framework is built upon four novel ideas: Fractional bit-width, which extends integer bit-width for weight matrix to fractional value and transforms discrete precision allocation as a continuous problem; 2)Block-wise mixed-precision, enabling fine-grained precision within weight matrices while remaining hardware-friendly; 3)Row-column weight reordering, which aggregates salient weights via row and column reordering, incurring only a small activation reordering overhead during inference; 4)Unified GEMM kernel, which supports mixed-precision GEMM at arbitrary average bit-width. Extensive experiments demonstrate that SFMP outperforms state-of-the-art layer-wise mixed-precision methods under the same memory constraints, while significantly reducing quantization cost and improving inference efficiency. Code is available at https://github.com/Nkniexin/SFMP

</details>


### [181] [Adaptive Dual-Weighting Framework for Federated Learning via Out-of-Distribution Detection](https://arxiv.org/abs/2602.01039)
*Zhiwei Ling,Hailiang Zhao,Chao Zhang,Xiang Ao,Ziqi Wang,Cheng Zhang,Zhen Qin,Xinkui Zhao,Kingsum Chow,Yuanqing Wu,MengChu Zhou*

Main category: cs.LG

TL;DR: FLood：基于OOD检测的联邦学习框架，通过双重加权机制处理非IID数据异质性，提升模型收敛稳定性和泛化能力


<details>
  <summary>Details</summary>
Motivation: 现实世界联邦学习部署中，用户、设备和应用场景的异质性导致数据非独立同分布，严重破坏全局模型的收敛稳定性、泛化能力和服务质量

Method: 提出FLood框架，采用双重加权机制：客户端层面通过提升伪OOD样本权重来重加权监督损失；服务器层面根据客户端OOD置信度分数加权聚合模型更新

Result: 在多种非IID设置下的基准测试中，FLood在准确率和泛化能力上均优于现有联邦学习方法，且可作为正交插件模块无缝集成到现有算法中

Conclusion: FLood为现实世界联邦环境提供实用且可扩展的解决方案，能够部署可靠的智能服务，有效应对数据异质性挑战

Abstract: Federated Learning (FL) enables collaborative model training across large-scale distributed service nodes while preserving data privacy, making it a cornerstone of intelligent service systems in edge-cloud environments. However, in real-world service-oriented deployments, data generated by heterogeneous users, devices, and application scenarios are inherently non-IID. This severe data heterogeneity critically undermines the convergence stability, generalization ability, and ultimately the quality of service delivered by the global model. To address this challenge, we propose FLood, a novel FL framework inspired by out-of-distribution (OOD) detection. FLood dynamically counteracts the adverse effects of heterogeneity through a dual-weighting mechanism that jointly governs local training and global aggregation. At the client level, it adaptively reweights the supervised loss by upweighting pseudo-OOD samples, thereby encouraging more robust learning from distributionally misaligned or challenging data. At the server level, it refines model aggregation by weighting client contributions according to their OOD confidence scores, prioritizing updates from clients with higher in-distribution consistency and enhancing the global model's robustness and convergence stability. Extensive experiments across multiple benchmarks under diverse non-IID settings demonstrate that FLood consistently outperforms state-of-the-art FL methods in both accuracy and generalization. Furthermore, FLood functions as an orthogonal plug-in module: it seamlessly integrates with existing FL algorithms to boost their performance under heterogeneity without modifying their core optimization logic. These properties make FLood a practical and scalable solution for deploying reliable intelligent services in real-world federated environments.

</details>


### [182] [Superposition unifies power-law training dynamics](https://arxiv.org/abs/2602.01045)
*Zixin Jessie Chen,Hao Chen,Yizhou Liu,Jeff Gore*

Main category: cs.LG

TL;DR: 特征叠加导致训练速度显著提升，产生普适的1/t幂律衰减，比无叠加的序列学习快10倍


<details>
  <summary>Details</summary>
Motivation: 研究特征叠加在幂律训练动力学中的作用，特别是对于使用叠加的大规模语言模型等神经网络具有重要意义

Method: 使用师生框架，首先推导无叠加时的解析理论，然后研究叠加瓶颈如何影响训练动力学

Result: 叠加瓶颈导致训练指数向普适的~1幂律转变，与数据和通道统计无关，比无叠加的序列学习快10倍

Conclusion: 特征叠加导致快速训练并产生数据无关的幂律指数，这对使用叠加的神经网络（包括生产级大语言模型）有重要启示

Abstract: We investigate the role of feature superposition in the emergence of power-law training dynamics using a teacher-student framework. We first derive an analytic theory for training without superposition, establishing that the power-law training exponent depends on both the input data statistics and channel importance. Remarkably, we discover that a superposition bottleneck induces a transition to a universal power-law exponent of $\sim 1$, independent of data and channel statistics. This one over time training with superposition represents an up to tenfold acceleration compared to the purely sequential learning that takes place in the absence of superposition. Our finding that superposition leads to rapid training with a data-independent power law exponent may have important implications for a wide range of neural networks that employ superposition, including production-scale large language models.

</details>


### [183] [SwiftRepertoire: Few-Shot Immune-Signature Synthesis via Dynamic Kernel Codes](https://arxiv.org/abs/2602.01051)
*Rong Fu,Wenxin Zhang,Muge Qi,Yang Li,Yabin Jin,Jiekai Wu,Jiaxuan Lu,Chunlei Meng,Youjin Wang,Zeli Su,Juntao Gao,Li Bao,Qi Zhao,Wei Luo,Simon Fong*

Main category: cs.LG

TL;DR: 提出一个从原型字典合成任务特定参数化的框架，用于T细胞受体库分析，通过轻量级适配器实现小样本适应，无需完整微调


<details>
  <summary>Details</summary>
Motivation: T细胞受体库分析为疾病检测和免疫监测提供生物学信号，但实际部署面临标签稀疏、队列异质性以及大型编码器适应新任务的计算负担等问题

Method: 从学习的原型字典中合成紧凑的任务特定参数化，基于从库探针和池化嵌入统计中提取的轻量级任务描述符，生成应用于冻结预训练骨干的小型适配器模块

Result: 通过少量支持示例即可立即适应新任务，无需完整模型微调，同时通过基序感知探针和校准的基序发现流程保持可解释性

Conclusion: 为在标签数据稀缺和计算资源受限的临床与研究环境中部署库信息模型提供了实用、样本高效且可解释的途径

Abstract: Repertoire-level analysis of T cell receptors offers a biologically grounded signal for disease detection and immune monitoring, yet practical deployment is impeded by label sparsity, cohort heterogeneity, and the computational burden of adapting large encoders to new tasks. We introduce a framework that synthesizes compact task-specific parameterizations from a learned dictionary of prototypes conditioned on lightweight task descriptors derived from repertoire probes and pooled embedding statistics. This synthesis produces small adapter modules applied to a frozen pretrained backbone, enabling immediate adaptation to novel tasks with only a handful of support examples and without full model fine-tuning. The architecture preserves interpretability through motif-aware probes and a calibrated motif discovery pipeline that links predictive decisions to sequence-level signals. Together, these components yield a practical, sample-efficient, and interpretable pathway for translating repertoire-informed models into diverse clinical and research settings where labeled data are scarce and computational resources are constrained.

</details>


### [184] [LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents](https://arxiv.org/abs/2602.01053)
*Hyesung Jeon,Hyeongju Ha,Jae-Joon Kim*

Main category: cs.LG

TL;DR: LRAgent：针对多LoRA智能体系统的KV缓存共享框架，通过分解缓存为共享基础组件和适配器依赖组件，显著降低内存和计算开销


<details>
  <summary>Details</summary>
Motivation: 多LoRA智能体系统中，每个智能体独立存储相同的长工具增强轨迹的KV缓存，导致大量内存和计算开销。现有KV缓存共享方法未充分考虑多LoRA场景

Method: 将KV缓存分解为：1）来自预训练权重的共享基础组件；2）来自LoRA权重的适配器依赖组件（以低秩形式存储）。提出Flash-LoRA-Attention内核，重新排序注意力计算以避免将低秩缓存扩展到完整维度

Result: LRAgent实现了接近完全共享缓存的吞吐量和首令牌延迟，同时在多个智能体问答基准测试中保持了接近非共享缓存基线的准确性

Conclusion: 通过利用多LoRA智能体系统中缓存差异主要由适配器输出主导的特性，LRAgent有效解决了KV缓存共享问题，显著降低了内存和计算开销

Abstract: Role specialization in multi-LLM agent systems is often realized via multi-LoRA, where agents share a pretrained backbone and differ only through lightweight adapters. Despite sharing base model weights, each agent independently builds and stores its own KV cache for the same long, tool-augmented trajectories, incurring substantial memory and compute overhead. Existing KV cache sharing methods largely overlook this multi-LoRA setting. We observe that, across agents, cache differences are dominated by adapter outputs, while activations from the shared pretrained backbone remain highly similar. Based on this observation, we propose LRAgent, a KV cache sharing framework for multi-LoRA agents that decomposes the cache into a shared base component from the pretrained weights and an adapter-dependent component from LoRA weights. LRAgent reduces memory overhead by sharing the base component and storing the adapter component in its inherent low-rank form, and further reduces compute overhead, enabled by shared-$A$ multi-LoRA architectures, by also sharing the low-rank cache and avoiding redundant computations for contexts already processed by other agents. To efficiently reconstruct adapter contributions at runtime, we introduce Flash-LoRA-Attention, a kernel that reorders attention computation to avoid materializing the low-rank cache to full dimension. LRAgent achieves throughput and time-to-first-token latency close to fully shared caching, while preserving accuracy near the non-shared caching baseline across agentic question-answering benchmarks.

</details>


### [185] [Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning](https://arxiv.org/abs/2602.01058)
*Dylan Zhang,Yufeng Xu,Haojin Wang,Qingzhi Chen,Hao Peng*

Main category: cs.LG

TL;DR: PEAR是一种SFT阶段方法，通过重要性采样重新加权SFT损失，纠正SFT与RL阶段的数据分布不匹配问题，从而提升后续RL训练效果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM后训练流程中，离线SFT阶段通常孤立优化以最大化SFT性能，但更强的SFT检查点在相同RL训练后可能表现更差，原因是SFT数据分布与RL阶段策略优化的分布存在不匹配。

Method: 提出PEAR方法，使用重要性采样重新加权SFT损失，包含token级、block级和序列级三种变体。该方法可增强标准SFT目标，在收集离线数据概率后仅增加少量训练开销。

Result: 在可验证推理游戏和数学推理任务上对Qwen 2.5/3和DeepSeek-distilled模型进行实验，PEAR持续提升后RL性能，在AIME2025上pass@8增益高达14.6%。

Conclusion: PEAR是迈向更整体化LLM后训练的有效步骤，通过考虑下游RL来设计和评估SFT，而不是孤立优化SFT阶段。

Abstract: Post-training of reasoning LLMs is a holistic process that typically consists of an offline SFT stage followed by an online reinforcement learning (RL) stage. However, SFT is often optimized in isolation to maximize SFT performance alone.
  We show that, after identical RL training, models initialized from stronger SFT checkpoints can significantly underperform those initialized from weaker ones. We attribute this to a mismatch typical in current SFT-RL pipelines: the distribution that generates the offline SFT data can differ substantially from the policy optimized during online RL, which learns from its own rollouts.
  We propose PEAR (Policy Evaluation-inspired Algorithm for Offline Learning Loss Re-weighting), an SFT-stage method that corrects this mismatch and better prepares the model for RL. PEAR uses importance sampling to reweight the SFT loss, with three variants operating at the token, block, and sequence levels. It can be used to augment standard SFT objectives and incurs little additional training overhead once probabilities for the offline data are collected.
  We conduct controlled experiments on verifiable reasoning games and mathematical reasoning tasks on Qwen 2.5 and 3 and DeepSeek-distilled models. PEAR consistently improves post-RL performance over canonical SFT, with pass at 8 gains up to a 14.6 percent on AIME2025. Our results suggest that PEAR is an effective step toward more holistic LLM post-training by designing and evaluating SFT with downstream RL in mind rather than in isolation.

</details>


### [186] [On the Expressive Power of Permutation-Equivariant Weight-Space Networks](https://arxiv.org/abs/2602.01083)
*Adir Dayan,Yam Eitan,Haggai Maron*

Main category: cs.LG

TL;DR: 该论文系统研究了权重空间网络的表达能力，证明了主流置换等变网络具有相同的表达能力，并在温和假设下建立了权重空间和函数空间的普适性理论。


<details>
  <summary>Details</summary>
Motivation: 随着预训练模型的普及，权重空间网络在各种任务中表现出色。现有SOTA方法依赖置换等变设计来提升泛化能力，但这可能限制表达能力，且权重空间学习同时涉及权重空间和函数空间的映射，使得表达能力分析特别复杂。目前缺乏对权重空间网络表达能力的全面理论分析。

Method: 开发了权重空间网络表达能力的系统理论：1) 证明所有主流置换等变网络在表达能力上是等价的；2) 在输入权重的温和自然假设下，建立了权重空间和函数空间的普适性；3) 刻画了普适性不再成立的边缘情况。

Result: 1) 所有主要置换等变网络具有相同的表达能力；2) 在合理假设下，权重空间网络在权重空间和函数空间都具有普适性；3) 明确了普适性失效的具体边界条件。

Conclusion: 该研究为权重空间网络的表达能力提供了统一而坚实的理论基础，填补了现有理论空白，为未来权重空间网络的设计和应用提供了理论指导。

Abstract: Weight-space learning studies neural architectures that operate directly on the parameters of other neural networks. Motivated by the growing availability of pretrained models, recent work has demonstrated the effectiveness of weight-space networks across a wide range of tasks. SOTA weight-space networks rely on permutation-equivariant designs to improve generalization. However, this may negatively affect expressive power, warranting theoretical investigation. Importantly, unlike other structured domains, weight-space learning targets maps operating on both weight and function spaces, making expressivity analysis particularly subtle. While a few prior works provide partial expressivity results, a comprehensive characterization is still missing. In this work, we address this gap by developing a systematic theory for expressivity of weight-space networks. We first prove that all prominent permutation-equivariant networks are equivalent in expressive power. We then establish universality in both weight- and function-space settings under mild, natural assumptions on the input weights, and characterize the edge-case regimes where universality no longer holds. Together, these results provide a strong and unified foundation for the expressivity of weight-space networks.

</details>


### [187] [OLion: Approaching the Hadamard Ideal by Intersecting Spectral and $\ell_{\infty}$ Implicit Biases](https://arxiv.org/abs/2602.01105)
*Zixiao Wang,Yifei Shen,Huishuai Zhang*

Main category: cs.LG

TL;DR: 提出一种名为A的新优化器，结合谱控制和坐标控制，通过正交化更新方向和符号更新，在语言和视觉任务中匹配或超越AdamW和Muon，同时使用更少的内存。


<details>
  <summary>Details</summary>
Motivation: 许多优化器可以解释为范数诱导几何下的最速下降方法，从而继承相应的隐式偏差。需要结合谱控制和坐标控制来获得更好的优化性能。

Method: 提出A优化器，结合正交化更新方向的谱控制和符号更新的ℓ∞风格坐标控制。通过Lion风格的动量方向，用少量Newton-Schulz迭代近似正交化，然后应用逐元素符号，近似在谱和ℓ∞约束集的交集上取最大步长。

Result: 在大型语言和视觉训练中，包括GPT-2和Llama预训练、SiT图像预训练和监督微调，A匹配或优于AdamW和Muon，同时仅使用动量级优化器状态，并在微调AdamW预训练检查点时减轻优化器不匹配问题。

Conclusion: A优化器通过结合谱控制和坐标控制，提供了一种高效且内存友好的优化方法，在各种大规模深度学习任务中表现出色，证明了其理论收敛性和实际有效性。

Abstract: Many optimizers can be interpreted as steepest-descent methods under norm-induced geometries, and thus inherit corresponding implicit biases. We introduce \nameA{} (\fullname{}), which combines spectral control from orthogonalized update directions with $\ell_\infty$-style coordinate control from sign updates. \nameA{} forms a Lion-style momentum direction, approximately orthogonalizes it via a few Newton--Schulz iterations, and then applies an entrywise sign, providing an efficient approximation to taking a maximal step over the intersection of the spectral and $\ell_\infty$ constraint sets (a scaled Hadamard-like set for matrix parameters). Despite the strong nonlinearity of orthogonalization and sign, we prove convergence under a mild, empirically verified diagonal-isotropy assumption. Across large-scale language and vision training, including GPT-2 and Llama pretraining, SiT image pretraining, and supervised fine-tuning, \nameA{} matches or outperforms AdamW and Muon under comparable tuning while using only momentum-level optimizer state, and it mitigates optimizer mismatch when fine-tuning AdamW-pretrained checkpoints.

</details>


### [188] [Single-Edge Node Injection Threats to GNN-Based Security Monitoring in Industrial Graph Systems](https://arxiv.org/abs/2602.01113)
*Wenjie Liang,Ranhui Yan,Jia Cai,You-Gan Wang*

Main category: cs.LG

TL;DR: 论文提出SEGIA单边图注入攻击方法，在资源受限下通过注入单边连接的伪造节点来影响GNN决策，攻击成功率比基线高25%以上，揭示了工业GNN部署的系统性风险。


<details>
  <summary>Details</summary>
Motivation: 工业GNN监控系统（如IIoT设备图、电网拓扑模型等）中，攻击者可能通过注入伪造节点（如恶意传感器、虚拟端点）来影响下游决策，同时规避基于拓扑和同质性的净化机制。现有攻击方法在资源受限的工业部署场景下效果有限。

Method: 提出SEGIA（单边图注入攻击）方法：1）每个注入节点仅通过单条边连接到操作图；2）集成修剪的SGC代理模型；3）多跳邻域采样；4）基于反向图卷积的特征合成；5）相似性正则化目标以保持局部同质性并规避边修剪。

Result: 理论分析和跨数据集、防御机制的广泛评估显示，在显著更小的边预算下，SEGIA的攻击成功率比代表性基线至少高25%。该方法能有效规避多种图净化防御。

Conclusion: SEGIA揭示了工业GNN部署中的系统性风险，表明现有防御机制不足以抵御资源受限的节点注入攻击。研究结果强调了需要轻量级准入验证和邻域一致性监控来增强工业GNN系统的安全性。

Abstract: Graph neural networks (GNNs) are increasingly adopted in industrial graph-based monitoring systems (e.g., Industrial internet of things (IIoT) device graphs, power-grid topology models, and manufacturing communication networks) to support anomaly detection, state estimation, and asset classification. In such settings, an adversary that compromises a small number of edge devices may inject counterfeit nodes (e.g., rogue sensors, virtualized endpoints, or spoofed substations) to bias downstream decisions while evading topology- and homophily-based sanitization. This paper formulates deployment-oriented node-injection attacks under constrained resources and proposes the \emph{Single-Edge Graph Injection Attack} (SEGIA), in which each injected node attaches to the operational graph through a single edge. SEGIA integrates a pruned SGC surrogate, multi-hop neighborhood sampling, and reverse graph convolution-based feature synthesis with a similarity-regularized objective to preserve local homophily and survive edge pruning. Theoretical analysis and extensive evaluations across datasets and defenses show at least $25\%$ higher attack success than representative baselines under substantially smaller edge budgets. These results indicate a system-level risk in industrial GNN deployments and motivate lightweight admission validation and neighborhood-consistency monitoring.

</details>


### [189] [MarkovScale: Towards Optimal Sequential Scaling at Inference Time](https://arxiv.org/abs/2602.01120)
*Youkang Wang,Jian Wang,Rubing Chen,Tianyi Zeng,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: 提出MarkovScale框架，将顺序缩放建模为马尔可夫过程，获得理论最优解，在准确性和效率间取得平衡，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 顺序缩放是重要的推理时缩放范式，但现有方法多为启发式、非原则性的，导致性能提升有限且缺乏理论理解，需要建立原则性框架来揭示其本质特性并获得最优性边界。

Method: 将顺序缩放建模为两状态马尔可夫过程，推导出闭式解，确定准确性提升的具体条件及理论上界、中性界和下界性能边界，并基于此开发MarkovScale实用系统。

Result: 在3个骨干LLM、5个基准测试和20多个配置上的综合实验表明，MarkovScale在准确性和效率方面均优于最先进的并行和顺序缩放方法。

Conclusion: MarkovScale为顺序缩放提供了理论依据和实用系统，是实现LLM最优且资源高效推理的重要进展。

Abstract: Sequential scaling is a prominent inference-time scaling paradigm, yet its performance improvements are typically modest and not well understood, largely due to the prevalence of heuristic, non-principled approaches that obscure clear optimality bounds. To address this, we propose a principled framework that models sequential scaling as a two-state Markov process. This approach reveals the underlying properties of sequential scaling and yields closed-form solutions for essential aspects, such as the specific conditions under which accuracy is improved and the theoretical upper, neutral, and lower performance bounds. Leveraging this formulation, we develop MarkovScale, a practical system that applies these optimality criteria to achieve a theoretically grounded balance between accuracy and efficiency. Comprehensive experiments across 3 backbone LLMs, 5 benchmarks, and over 20 configurations show that MarkovScale consistently outperforms state-of-the-art parallel and sequential scaling methods, representing a significant step toward optimal and resource-efficient inference in LLMs. The source code will be open upon acceptance at https://open-upon-acceptance.

</details>


### [190] [ChronoSpike: An Adaptive Spiking Graph Neural Network for Dynamic Graphs](https://arxiv.org/abs/2602.01124)
*Md Abrar Jahin,Taufikur Rahman Fuad,Jay Pujara,Craig Knoblock*

Main category: cs.LG

TL;DR: ChronoSpike：自适应脉冲图神经网络，通过可学习LIF神经元、多头注意力空间聚合和轻量Transformer时间编码器，实现线性内存复杂度的动态图表示学习，在性能和效率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有动态图表示学习方法面临基本权衡：注意力方法表达能力强但复杂度高（O(T²)），循环架构存在梯度问题和密集状态存储问题。脉冲神经网络具有事件驱动效率，但受限于顺序传播、二进制信息丢失和局部聚合无法捕获全局上下文。

Method: 提出ChronoSpike自适应脉冲图神经网络，包含：1）具有每通道膜动力学的可学习LIF神经元；2）在连续特征上的多头注意力空间聚合；3）轻量Transformer时间编码器。实现细粒度局部建模和长程依赖捕获，内存复杂度为O(T·d)。

Result: 在三个大规模基准测试中，ChronoSpike在12个最先进基线方法上提升2.0% Macro-F1和2.4% Micro-F1，训练速度比循环方法快3-10倍，参数预算恒定为105K（与图大小无关）。理论保证包括膜电位有界性、收缩因子ρ<1下的梯度流稳定性和BIBO稳定性。

Conclusion: ChronoSpike成功解决了动态图表示学习中的效率-表达能力权衡问题，通过脉冲神经网络与注意力机制的创新结合，实现了线性内存复杂度的同时保持高性能，具有理论保证和良好的可解释性（83-88%稀疏性）。

Abstract: Dynamic graph representation learning requires capturing both structural relationships and temporal evolution, yet existing approaches face a fundamental trade-off: attention-based methods achieve expressiveness at $O(T^2)$ complexity, while recurrent architectures suffer from gradient pathologies and dense state storage. Spiking neural networks offer event-driven efficiency but remain limited by sequential propagation, binary information loss, and local aggregation that misses global context. We propose ChronoSpike, an adaptive spiking graph neural network that integrates learnable LIF neurons with per-channel membrane dynamics, multi-head attentive spatial aggregation on continuous features, and a lightweight Transformer temporal encoder, enabling both fine-grained local modeling and long-range dependency capture with linear memory complexity $O(T \cdot d)$. On three large-scale benchmarks, ChronoSpike outperforms twelve state-of-the-art baselines by $2.0\%$ Macro-F1 and $2.4\%$ Micro-F1 while achieving $3-10\times$ faster training than recurrent methods with a constant 105K-parameter budget independent of graph size. We provide theoretical guarantees for membrane potential boundedness, gradient flow stability under contraction factor $ρ< 1$, and BIBO stability; interpretability analyses reveal heterogeneous temporal receptive fields and a learned primacy effect with $83-88\%$ sparsity.

</details>


### [191] [WinFLoRA: Incentivizing Client-Adaptive Aggregation in Federated LoRA under Privacy Heterogeneity](https://arxiv.org/abs/2602.01126)
*Mengsha Kou,Xiaoyu Xia,Ziqi Wang,Ibrahim Khalil,Runkun Luo,Jingwen Zhou,Minhui Xue*

Main category: cs.LG

TL;DR: WinFLoRA：一种隐私异构的联邦LoRA框架，通过噪声感知的聚合权重作为激励机制，在保护隐私的同时提升全局模型性能


<details>
  <summary>Details</summary>
Motivation: 在隐私敏感的联邦学习场景中，不同客户端注入不同级别的差分隐私噪声，导致隐私异质性，这会扭曲个体激励与全局性能的对齐。现有方法难以同时满足异构隐私需求和保持模型性能。

Method: WinFLoRA通过基于上传的LoRA适配器估计客户端噪声水平，将聚合权重作为激励机制：噪声越低的客户端获得更大权重，从而在全局模型中具有更大影响力。这种方法在无需第三方介入的情况下，将客户端的隐私效用与下游性能与全局目标对齐。

Result: 在多个LLM和数据集上的评估表明，WinFLoRA相比最先进基准方法，全局准确率最高提升52.58%，客户端效用最高提升2.56倍。

Conclusion: WinFLoRA成功解决了隐私异构联邦学习中的激励对齐问题，通过噪声感知的权重分配机制，在满足不同客户端隐私需求的同时显著提升了全局模型性能。

Abstract: Large Language Models (LLMs) increasingly underpin intelligent web applications, from chatbots to search and recommendation, where efficient specialization is essential. Low-Rank Adaptation (LoRA) enables such adaptation with minimal overhead, while federated LoRA allows web service providers to fine-tune shared models without data sharing. However, in privacy-sensitive deployments, clients inject varying levels of differential privacy (DP) noise, creating privacy heterogeneity that misaligns individual incentives and global performance. In this paper, we propose WinFLoRA, a privacy-heterogeneous federated LoRA that utilizes aggregation weights as incentives with noise awareness. Specifically, the noises from clients are estimated based on the uploaded LoRA adapters. A larger weight indicates greater influence on the global model and better downstream task performance, rewarding lower-noise contributions. By up-weighting low-noise updates, WinFLoRA improves global accuracy while accommodating clients' heterogeneous privacy requirements. Consequently, WinFLoRA aligns heterogeneous client utility in terms of privacy and downstream performance with global model objectives without third-party involvement. Extensive evaluations demonstrate that across multiple LLMs and datasets, WinFLoRA achieves up to 52.58% higher global accuracy and up to 2.56x client utility than state-of-the-art benchmarks. Source code is publicly available at https://github.com/koums24/WinFLoRA.git.

</details>


### [192] [Tangent Space Fine-Tuning for Directional Preference Alignment in Large Language Models](https://arxiv.org/abs/2602.01128)
*Mete Erdogan*

Main category: cs.LG

TL;DR: TS-DPO：在切线空间中执行DPO，学习每个目标的更新方向，可在推理时线性组合以实现用户指定的行为平衡，无需额外优化


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法（如DPO）将反馈压缩为单一标量奖励，固定了目标间的平衡，无法遍历帕累托前沿。需要让大语言模型能够平衡多个人类偏好维度（如帮助性、安全性、冗长度），实现可控对齐

Method: 基于切线空间理论，将微调视为在模型切线空间中的操作，提出切线空间直接偏好优化（TS-DPO）。在局部线性机制中执行DPO，学习每个目标的更新方向，这些方向可在推理时线性组合以生成用户指定的行为

Result: 在HelpSteer和UltraFeedback数据集上评估帮助性-冗长度权衡，TS-DPO比标量化DPO获得更广泛的帕累托最优覆盖和更平滑的偏好控制。典型相关分析（CCA）显示切线空间训练放大了与不同偏好对齐的典型方向，改善了可分离性

Conclusion: TS-DPO通过切线空间中的线性组合实现了多目标偏好的可控对齐，为LLM提供更灵活和可解释的偏好平衡机制

Abstract: Our goal is to enable large language models (LLMs) to balance multiple human preference dimensions; such as helpfulness, safety, and verbosity, through principled and controllable alignment. Existing preference optimization methods, including Direct Preference Optimization (DPO), collapse feedback into a single scalar reward, fixing one balance among objectives and preventing traversal of the Pareto front. Recent work by Ortiz-Jimenez et al. (2023) showed that fine-tuning can be viewed in a model's tangent space, where linearized updates act as additive vectors that can be composed to jointly perform well on multiple tasks. Building on this formulation, we extend this idea to preference alignment and propose Tangent-Space Direct Preference Optimization (TS-DPO), which performs DPO within this locally linear regime to learn per-objective update directions. These directions can be linearly combined at inference to generate user-specified behaviors without additional optimization. Evaluated on the helpfulness-verbosity trade-off using the HelpSteer and UltraFeedback datasets, TS-DPO achieves broader Pareto-optimal coverage and smoother preference control than scalarized DPO. Canonical Correlation Analysis (CCA) further shows that tangent-space training amplifies canonical directions aligned with distinct preferences, improving disentanglement.

</details>


### [193] [TRACE: Scalable Amortized Causal Discovery from Single Sequences via Autoregressive Density Estimation](https://arxiv.org/abs/2602.01135)
*Hugo Math,Rainer Lienhart*

Main category: cs.LG

TL;DR: TRACE：利用自回归模型作为预训练密度估计器，从单个离散事件序列中推断事件类型间的因果图，支持延迟因果效应，可扩展至大规模词汇表


<details>
  <summary>Details</summary>
Motivation: 解决从单个观测序列（如车辆日志、制造系统、患者轨迹）中进行因果发现的挑战，这些场景缺乏重复样本、维度高且具有长程时间依赖性

Method: TRACE框架将自回归模型重新用作预训练密度估计器进行条件互信息估计，推断事件类型间的因果图，支持延迟因果效应，在GPU上完全并行，计算复杂度随事件词汇表线性增长

Result: 在不同基线和不同词汇表规模下表现出稳健性能，在包含29,100+事件类型的车辆诊断根因分析应用中验证了有效性，理论证明了在非完美自回归模型下的可识别性

Conclusion: TRACE为从单个离散事件序列中进行可扩展的因果发现提供了有效框架，特别适用于大规模词汇表场景，在理论和实验上均验证了其有效性

Abstract: We study causal discovery from a single observed sequence of discrete events generated by a stochastic process, as encountered in vehicle logs, manufacturing systems, or patient trajectories. This regime is particularly challenging due to the absence of repeated samples, high dimensionality, and long-range temporal dependencies of the single observation during inference. We introduce TRACE, a scalable framework that repurposes autoregressive models as pretrained density estimators for conditional mutual information estimation. TRACE infers the summary causal graph between event types in a sequence, scaling linearly with the event vocabulary and supporting delayed causal effects, while being fully parallel on GPUs. We establish its theoretical identifiability under imperfect autoregressive models. Experiments demonstrate robust performance across different baselines and varying vocabulary sizes including an application to root-cause analysis in vehicle diagnostics with over 29,100 event types.

</details>


### [194] [A Unified Matrix-Spectral Framework for Stability and Interpretability in Deep Learning](https://arxiv.org/abs/2602.01136)
*Ronald Katende*

Main category: cs.LG

TL;DR: 提出统一矩阵谱框架分析深度神经网络稳定性和可解释性，引入全局矩阵稳定性指数聚合谱信息，通过谱熵改进经典算子范数界，实验验证谱正则化能显著提升归因稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏统一框架分析深度神经网络的稳定性和可解释性，需要建立连接谱特性与网络分析稳定性之间的精确关系，为鲁棒性感知的模型设计和训练提供实用指导。

Method: 将网络表示为数据依赖的线性算子乘积，引入全局矩阵稳定性指数聚合雅可比矩阵、参数梯度、神经正切核算子和损失海森矩阵的谱信息，使用谱熵改进经典算子范数界，提出可计算诊断和稳定性导向的正则化原则。

Result: 在MNIST、CIFAR-10和CIFAR-100上的合成实验和受控研究表明，适度的谱正则化即使全局谱摘要变化不大，也能显著改善归因稳定性，建立了谱集中度与分析稳定性之间的精确连接。

Conclusion: 该研究建立了谱特性与深度神经网络分析稳定性之间的精确联系，提供了实用的可计算诊断和正则化原则，为鲁棒性感知的模型设计和训练提供了理论指导和实践方法。

Abstract: We develop a unified matrix-spectral framework for analyzing stability and interpretability in deep neural networks. Representing networks as data-dependent products of linear operators reveals spectral quantities governing sensitivity to input perturbations, label noise, and training dynamics.
  We introduce a Global Matrix Stability Index that aggregates spectral information from Jacobians, parameter gradients, Neural Tangent Kernel operators, and loss Hessians into a single stability scale controlling forward sensitivity, attribution robustness, and optimization conditioning. We further show that spectral entropy refines classical operator-norm bounds by capturing typical, rather than purely worst-case, sensitivity.
  These quantities yield computable diagnostics and stability-oriented regularization principles. Synthetic experiments and controlled studies on MNIST, CIFAR-10, and CIFAR-100 confirm that modest spectral regularization substantially improves attribution stability even when global spectral summaries change little.
  The results establish a precise connection between spectral concentration and analytic stability, providing practical guidance for robustness-aware model design and training.

</details>


### [195] [Self-Generative Adversarial Fine-Tuning for Large Language Models](https://arxiv.org/abs/2602.01137)
*Shiguang Wu,Yaqing Wang,Quanming Yao*

Main category: cs.LG

TL;DR: SGALM提出了一种新的LLM对齐框架，通过单个LLM内的生成对抗游戏联合进化生成和判别能力，无需外部奖励模型或大量人工标注。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐方法依赖昂贵的人工标注或存在偏差积累问题，需要一种更高效、自包含的对齐框架。

Method: SGALM将LLM对齐建模为生成对抗游戏，在单个LLM内联合训练生成器和判别器，通过对抗过程实现自我进化。

Result: SGALM在理论和实证上都达到了最先进性能，既能作为有效的对齐算法，也能作为鲁棒的合成数据生成引擎。

Conclusion: SGALM提供了一种统一的自包含对齐框架，减少了对人工标注的依赖，同时避免了自评估方法的偏差积累问题。

Abstract: Fine-tuning large language models (LLMs) for alignment typically relies on supervised fine-tuning or reinforcement learning from human feedback, both limited by the cost and scarcity of high-quality annotations. Recent self-play and synthetic data approaches reduce this dependence but often rely on heuristic assumptions or ungrounded self-evaluation, which can cause bias accumulation and performance drift. In this paper, we propose Self-Generative Adversarial LLM (SGALM), a unified fine-tuning framework that formulates alignment as a generative adversarial game within a single LLM. SGALM jointly evolves generation and discrimination capabilities without external reward models. Theoretical and empirical results demonstrate that SGALM achieves state-of-the-art performance, serves as an effective alignment algorithm and a robust synthetic data engine.

</details>


### [196] [Key Principles of Graph Machine Learning: Representation, Robustness, and Generalization](https://arxiv.org/abs/2602.01139)
*Yassine Abbahaddou*

Main category: cs.LG

TL;DR: 该论文针对图神经网络在泛化性、对抗鲁棒性和表示学习方面的挑战，提出了基于图移位算子的表示学习、图数据增强和正交化防御三大解决方案。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在处理结构化数据方面表现出色，但在泛化能力、对抗鲁棒性和表示学习效果方面仍存在局限性，需要系统性地解决这些核心挑战。

Method: 1. 基于图移位算子的新表示学习技术；2. 通过图数据增强提升泛化能力的方法；3. 利用正交化技术和基于噪声的防御机制增强对抗鲁棒性。

Result: 提出了系统性解决方案，为理解GNN的局限性和潜力提供了更原则性的理论基础，提升了GNN在多种应用场景下的性能表现。

Conclusion: 通过解决GNN在表示学习、泛化和鲁棒性方面的核心挑战，为图神经网络的发展提供了更坚实的理论基础和实践指导。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning representations from structured data. Despite their growing popularity and success across various applications, GNNs encounter several challenges that limit their performance. in their generalization, robustness to adversarial perturbations, and the effectiveness of their representation learning capabilities. In this dissertation, I investigate these core aspects through three main contributions: (1) developing new representation learning techniques based on Graph Shift Operators (GSOs, aiming for enhanced performance across various contexts and applications, (2) introducing generalization-enhancing methods through graph data augmentation, and (3) developing more robust GNNs by leveraging orthonormalization techniques and noise-based defenses against adversarial attacks. By addressing these challenges, my work provides a more principled understanding of the limitations and potential of GNNs.

</details>


### [197] [Generalized Radius and Integrated Codebook Transforms for Differentiable Vector Quantization](https://arxiv.org/abs/2602.01140)
*Haochen You,Heng Zhang,Hongyang He,Yuqi Li,Baojing Liu*

Main category: cs.LG

TL;DR: GRIT-VQ是一种新的向量量化框架，通过半径更新和集成变换解决传统VQ的梯度不稳定和码本利用不足问题，保持前向硬分配的同时实现完全可微优化。


<details>
  <summary>Details</summary>
Motivation: 传统向量量化(VQ)使用硬最近邻分配和直通估计器，导致梯度不稳定、码本利用不足，特别是在大规模应用中问题更严重。需要一种能保持前向硬分配同时实现完全可微优化的方法。

Method: GRIT-VQ采用半径更新机制，沿量化方向以可控的几何感知步长移动潜在表示；同时应用数据无关的集成变换，使所有码字通过共享参数更新而非独立更新，形成统一的代理框架。

Result: 在图像重建、图像生成和推荐系统标记化基准测试中，GRIT-VQ持续改善重建误差、生成质量和推荐准确性，同时显著提高码本利用率。

Conclusion: GRIT-VQ提供了一个理论上有保证的统一框架，解决了传统VQ的优化问题，实现了稳定的梯度流、协调的码本演化和可靠的避免崩溃，适用于广泛的量化器家族。

Abstract: Vector quantization (VQ) underpins modern generative and representation models by turning continuous latents into discrete tokens. Yet hard nearest-neighbor assignments are non-differentiable and are typically optimized with heuristic straight-through estimators, which couple the update step size to the quantization gap and train each code in isolation, leading to unstable gradients and severe codebook under-utilization at scale. In this paper, we introduce GRIT-VQ (Generalized Radius and Integrated Transform-Vector Quantization), a unified surrogate framework that keeps hard assignments in the forward pass while making VQ fully differentiable. GRIT-VQ replaces the straight-through estimator with a radius-based update that moves latents along the quantization direction with a controllable, geometry-aware step, and applies a data-agnostic integrated transform to the codebook so that all codes are updated through shared parameters instead of independently. Our theoretical analysis clarifies the fundamental optimization dynamics introduced by GRIT-VQ, establishing conditions for stable gradient flow, coordinated codebook evolution, and reliable avoidance of collapse across a broad family of quantizers. Across image reconstruction, image generation, and recommendation tokenization benchmarks, GRIT-VQ consistently improves reconstruction error, generative quality, and recommendation accuracy while substantially increasing codebook utilization compared to existing VQ variants.

</details>


### [198] [Statistical MIA: Rethinking Membership Inference Attack for Reliable Unlearning Auditing](https://arxiv.org/abs/2602.01150)
*Jialong Sun,Zeming Wei,Jiaxuan Zou,Jiacheng Gong,Guanheng Wang,Chengyang Dong,Jialong Li,Bo Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的机器遗忘审计框架SMIA，通过统计测试直接比较成员和非成员数据的分布，无需训练攻击模型，提供带置信区间的遗忘率评估，比传统MIA方法更可靠且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 当前机器遗忘审计主要依赖成员推理攻击（MIA），但MIA存在根本缺陷：成员推理失败并不代表真正遗忘。MIA作为二元分类问题会产生无法观测的统计误差，导致对遗忘性能的过度乐观评估，同时需要大量计算资源训练影子模型。

Method: 提出统计成员推理攻击（SMIA）框架：1）使用统计测试直接比较成员和非成员数据的分布，无需训练攻击模型；2）输出遗忘率及其置信区间，量化审计结果的可靠性；3）完全免训练，显著降低计算成本。

Result: 大量实验表明，SMIA比现有MIA方法提供更可靠的审计结果，计算成本显著降低。SMIA能够输出带置信区间的遗忘率，使审计结果具有可量化的可靠性。理论保证和实证效果表明SMIA可作为可靠的机器遗忘审计新范式。

Conclusion: SMIA解决了传统MIA审计的根本缺陷，提供了一种免训练、高可靠性、计算高效的机器遗忘审计新框架。其理论保证和实证效果使其成为机器遗忘审计的新范式，对实现"被遗忘权"具有重要意义。

Abstract: Machine unlearning (MU) is essential for enforcing the right to be forgotten in machine learning systems. A key challenge of MU is how to reliably audit whether a model has truly forgotten specified training data. Membership Inference Attacks (MIAs) are widely used for unlearning auditing, where samples that evade membership detection are often regarded as successfully forgotten. After carefully revisiting the reliability of MIA, we show that this assumption is flawed: failed membership inference does not imply true forgetting. We theoretically demonstrate that MIA-based auditing, when formulated as a binary classification problem, inevitably incurs statistical errors whose magnitude cannot be observed during the auditing process. This leads to overly optimistic evaluations of unlearning performance, while incurring substantial computational overhead due to shadow model training. To address these limitations, we propose Statistical Membership Inference Attack (SMIA), a novel training-free and highly effective auditing framework. SMIA directly compares the distributions of member and non-member data using statistical tests, eliminating the need for learned attack models. Moreover, SMIA outputs both a forgetting rate and a corresponding confidence interval, enabling quantified reliability of the auditing results. Extensive experiments show that SMIA provides more reliable auditing with significantly lower computational cost than existing MIA-based approaches. Notably, the theoretical guarantees and empirical effectiveness of SMIA suggest it as a new paradigm for reliable machine unlearning auditing.

</details>


### [199] [PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning](https://arxiv.org/abs/2602.01156)
*Shunpeng Yang,Ben Liu,Hua Chen*

Main category: cs.LG

TL;DR: PolicyFlow：一种基于连续归一化流（CNF）的强化学习算法，通过近似重要性比率和布朗正则化器，解决了PPO在表达性策略模型中的计算和稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 标准PPO依赖重要性比率，这需要评估策略似然，对于高斯分布策略很简单。但将PPO扩展到表达能力更强的连续归一化流（CNF）策略时，沿整个流轨迹评估似然计算昂贵且数值不稳定。

Method: 提出PolicyFlow算法：1）通过沿简单插值路径的流场变化近似重要性比率，避免沿完整流路径的似然评估；2）引入布朗正则化器，这是一种受布朗运动启发的隐式策略熵正则化器，防止模式崩溃并鼓励多样化行为。

Result: 在MultiGoal、PointMaze、IsaacLab和MuJoCo Playground等多个环境的多样化任务上，PolicyFlow相比使用高斯策略的PPO以及基于流的基线方法（FPO和DPPO）取得了竞争性或更优的性能。在MultiGoal任务中特别展示了捕获更丰富的多模态动作分布的能力。

Conclusion: PolicyFlow成功地将表达能力强的CNF策略与PPO风格目标相结合，解决了计算效率和数值稳定性问题，同时通过布朗正则化器增强了策略的多样性，为强化学习中更复杂策略模型的应用提供了有效方案。

Abstract: Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow's ability to capture richer multimodal action distributions.

</details>


### [200] [Multi-Horizon Electricity Price Forecasting with Deep Learning in the Australian National Electricity Market](https://arxiv.org/abs/2602.01157)
*Mohammed Osman Gani,Zhipeng He,Chun Ouyang,Sara Khalifa*

Main category: cs.LG

TL;DR: 提出一个基于深度学习的多日前电价预测框架，在澳大利亚电力市场进行综合评估，发现标准DL模型在多数区域表现更好，而SOTA时间序列DL模型对预测时域扩展更具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前电价预测存在三个主要问题：1) 多日预测时域研究有限；2) 对SOTA时间序列DL模型探索不足；3) 主要依赖聚合时域评估，掩盖了日内预测差异。需要解决这些缺口以提升电价预测的准确性和实用性。

Method: 提出一个新颖的电价预测框架，将预测时域扩展到多日前，系统构建基于基准SOTA时间序列DL模型的预测模型。在澳大利亚国家电力市场五个区域进行综合评估，分析日内时段预测性能，集成模型在日内间隔水平的评估。

Result: 结果显示：1) 没有单一模型在所有区域、指标和时域上持续占优；2) 标准DL模型在多数区域表现更优；3) SOTA时间序列DL模型对预测时域扩展更具鲁棒性；4) 日内评估揭示明显的昼夜误差模式：绝对误差在晚间爬坡期达到峰值，相对误差在午间负电价时段膨胀，方向准确性在趋势频繁变化期下降。

Conclusion: 未来基于DL的电价预测研究可以从丰富的特征表示和建模策略中受益，这些策略应增强长期预测的鲁棒性，同时保持对日内波动性和结构性价格动态的敏感性。

Abstract: Accurate electricity price forecasting (EPF) is essential for operational planning, trading, and flexible asset scheduling in liberalised power systems, yet remains challenging due to volatility, heavy-tailed spikes, and frequent regime shifts. While deep learning (DL) has been increasingly adopted in EPF to capture complex and nonlinear price dynamics, several important gaps persist: (i) limited attention to multi-day horizons beyond day-ahead forecasting, (ii) insufficient exploration of state-of-the-art (SOTA) time series DL models, and (iii) a predominant reliance on aggregated horizon-level evaluation that obscures time-of-day forecasting variation. To address these gaps, we propose a novel EPF framework that extends the forecast horizon to multi-day-ahead by systematically building forecasting models that leverage benchmarked SOTA time series DL models. We conduct a comprehensive evaluation to analyse time-of-day forecasting performance by integrating model assessment at intraday interval levels across all five regions in the Australian National Electricity Market (NEM). The results show that no single model consistently dominates across regions, metrics, and horizons. Overall, standard DL models deliver superior performance in most regions, while SOTA time series DL models demonstrate greater robustness to forecast horizon extension. Intraday interval-level evaluation reveals pronounced diurnal error patterns, indicating that absolute errors peak during the evening ramp, relative errors inflate during midday negative-price regimes, and directional accuracy degrades during periods of frequent trend changes. These findings suggest that future research on DL-based EPF can benefit from enriched feature representations and modelling strategies that enhance longer-term forecasting robustness while maintaining sensitivity to intraday volatility and structural price dynamics.

</details>


### [201] [Rethinking the Flow-Based Gradual Domain Adaption: A Semi-Dual Optimal Transport Perspective](https://arxiv.org/abs/2602.01179)
*Zhichao Chen,Zhan Zhuang,Yunfei Teng,Hao Wang,Fangyikang Wang,Zhengnan Li,Tianqiao Liu,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 提出E-SUOT框架，通过熵正则化半对偶不平衡最优传输构建中间域，解决流模型在渐进域适应中因似然估计导致信息丢失的问题。


<details>
  <summary>Details</summary>
Motivation: 渐进域适应需要中间域来缓解域偏移，但真实中间域通常不可用或无效。现有流模型通过插值构建中间样本，但依赖样本似然估计会丢弃有用信息，影响GDA性能。

Method: 将流式GDA重新表述为拉格朗日对偶问题，推导出避免似然估计的等价半对偶目标。引入熵正则化将不稳定的min-max训练转换为更稳定的交替优化过程，提出E-SUOT框架。

Result: 通过广泛实验验证了E-SUOT框架的有效性，提供了关于稳定性和泛化性的理论分析。

Conclusion: E-SUOT框架通过熵正则化半对偶不平衡最优传输成功解决了流模型在渐进域适应中的信息丢失问题，提供了更稳定的训练过程和更好的性能。

Abstract: Gradual domain adaptation (GDA) aims to mitigate domain shift by progressively adapting models from the source domain to the target domain via intermediate domains. However, real intermediate domains are often unavailable or ineffective, necessitating the synthesis of intermediate samples. Flow-based models have recently been used for this purpose by interpolating between source and target distributions; however, their training typically relies on sample-based log-likelihood estimation, which can discard useful information and thus degrade GDA performance. The key to addressing this limitation is constructing the intermediate domains via samples directly. To this end, we propose an Entropy-regularized Semi-dual Unbalanced Optimal Transport (E-SUOT) framework to construct intermediate domains. Specifically, we reformulate flow-based GDA as a Lagrangian dual problem and derive an equivalent semi-dual objective that circumvents the need for likelihood estimation. However, the dual problem leads to an unstable min-max training procedure. To alleviate this issue, we further introduce entropy regularization to convert it into a more stable alternative optimization procedure. Based on this, we propose a novel GDA training framework and provide theoretical analysis in terms of stability and generalization. Finally, extensive experiments are conducted to demonstrate the efficacy of the E-SUOT framework.

</details>


### [202] [Analyzing and Improving Diffusion Models for Time-Series Data Imputation: A Proximal Recursion Perspective](https://arxiv.org/abs/2602.01182)
*Zhichao Chen,Hao Wang,Fangyikang Wang,Licheng Pan,Zhengnan Li,Yunfei Teng,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 提出SPIRIT框架，通过半近端传输正则化解决扩散模型在时间序列插补中的非平稳性和目标不一致问题


<details>
  <summary>Details</summary>
Motivation: 扩散模型在时间序列数据插补中表现不稳定，主要面临两个障碍：非平稳时间动态会偏差推理轨迹导致异常值敏感；目标不一致（插补需要准确点恢复而扩散模型生成多样样本）

Method: 从近端算子角度分析扩散模型插补过程，提出SPIRIT框架：引入熵诱导的Bregman散度松弛Wasserstein距离的质量保持约束，构建半近端传输差异，理论上证明其对非平稳性的鲁棒性，并移除耗散结构

Result: 大量实验证明SPIRIT方法的有效性

Conclusion: 通过半近端传输正则化解决了扩散模型在时间序列插补中的核心问题，提高了插补的准确性和鲁棒性

Abstract: Diffusion models (DMs) have shown promise for Time-Series Data Imputation (TSDI); however, their performance remains inconsistent in complex scenarios. We attribute this to two primary obstacles: (1) non-stationary temporal dynamics, which can bias the inference trajectory and lead to outlier-sensitive imputations; and (2) objective inconsistency, since imputation favors accurate pointwise recovery whereas DMs are inherently trained to generate diverse samples. To better understand these issues, we analyze DM-based TSDI process through a proximal-operator perspective and uncover that an implicit Wasserstein distance regularization inherent in the process hinders the model's ability to counteract non-stationarity and dissipative regularizer, thereby amplifying diversity at the expense of fidelity. Building on this insight, we propose a novel framework called SPIRIT (Semi-Proximal Transport Regularized time-series Imputation). Specifically, we introduce entropy-induced Bregman divergence to relax the mass preserving constraint in the Wasserstein distance, formulate the semi-proximal transport (SPT) discrepancy, and theoretically prove the robustness of SPT against non-stationarity. Subsequently, we remove the dissipative structure and derive the complete SPIRIT workflow, with SPT serving as the proximal operator. Extensive experiments demonstrate the effectiveness of the proposed SPIRIT approach.

</details>


### [203] [The Gaussian-Head OFL Family: One-Shot Federated Learning from Client Global Statistics](https://arxiv.org/abs/2602.01186)
*Fabio Turazza,Marco Picone,Marco Mamei*

Main category: cs.LG

TL;DR: 提出GH-OFL系列方法，通过假设预训练嵌入的类条件高斯性，在单轮通信中实现联邦学习，仅传输统计量而非模型，无需公共数据集且保持数据隐私。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习存在多轮通信成本高和隐私风险问题，而现有单轮联邦学习方法要么不实用，要么受限于需要公共数据集、假设同质客户端模型或需上传额外数据。需要一种更实用、无数据依赖的单轮联邦学习方案。

Method: GH-OFL方法假设预训练嵌入的类条件高斯性，客户端仅传输每类的计数和一阶/二阶矩统计量。服务器端构建三种头部：(1)闭式高斯头部(NB/LDA/QDA)；(2)FisherMix：在估计的Fisher子空间中采样合成样本训练的余弦间隔线性头部；(3)Proto-Hyper：通过知识蒸馏在合成样本上精炼高斯对数几率的轻量级低秩残差头部。

Result: GH-OFL方法在强非IID偏斜下实现了最先进的鲁棒性和准确性，同时保持严格的无数据特性。

Conclusion: GH-OFL系列方法提供了一种实用、高效的单轮联邦学习解决方案，显著降低了通信开销和隐私风险，无需公共数据集，在非IID数据分布下表现优异。

Abstract: Classical Federated Learning relies on a multi-round iterative process of model exchange and aggregation between server and clients, with high communication costs and privacy risks from repeated model transmissions. In contrast, one-shot federated learning (OFL) alleviates these limitations by reducing communication to a single round, thereby lowering overhead and enhancing practical deployability. Nevertheless, most existing one-shot approaches remain either impractical or constrained, for example, they often depend on the availability of a public dataset, assume homogeneous client models, or require uploading additional data or model information. To overcome these issues, we introduce the Gaussian-Head OFL (GH-OFL) family, a suite of one-shot federated methods that assume class-conditional Gaussianity of pretrained embeddings. Clients transmit only sufficient statistics (per-class counts and first/second-order moments) and the server builds heads via three components: (i) Closed-form Gaussian heads (NB/LDA/QDA) computed directly from the received statistics; (ii) FisherMix, a linear head with cosine margin trained on synthetic samples drawn in an estimated Fisher subspace; and (iii) Proto-Hyper, a lightweight low-rank residual head that refines Gaussian logits via knowledge distillation on those synthetic samples. In our experiments, GH-OFL methods deliver state-of-the-art robustness and accuracy under strong non-IID skew while remaining strictly data-free.

</details>


### [204] [Unraveling the Hidden Dynamical Structure in Recurrent Neural Policies](https://arxiv.org/abs/2602.01196)
*Jin Li,Yue Wu,Mengsha Huang,Yuhao Sun,Hao He,Xianyuan Zhan*

Main category: cs.LG

TL;DR: 研究发现循环神经网络策略在与环境交互时会自发形成稳定的循环结构，这些结构与动力系统中的极限环相似，能够解释循环策略的泛化能力和鲁棒性优势。


<details>
  <summary>Details</summary>
Motivation: 循环神经网络策略在部分可观测控制和元强化学习任务中表现出色，但其优越泛化能力和鲁棒性的内在机制尚未被充分理解。本研究旨在揭示循环策略内部工作机制的本质。

Method: 通过分析多种训练方法、模型架构和任务下学习的循环策略的隐藏状态域，研究其与环境交互过程中形成的动态结构。

Result: 发现循环策略在与环境交互时会自发形成稳定的循环结构，这些结构与动力系统中的极限环相似。极限环的几何结构与策略行为存在结构化对应关系。

Conclusion: 极限环的出现稳定了策略的内部记忆和任务相关环境状态，同时抑制了环境不确定性带来的干扰；极限环的几何结构编码了行为的关系结构，促进了面对非平稳环境时的技能适应。

Abstract: Recurrent neural policies are widely used in partially observable control and meta-RL tasks. Their abilities to maintain internal memory and adapt quickly to unseen scenarios have offered them unparalleled performance when compared to non-recurrent counterparts. However, until today, the underlying mechanisms for their superior generalization and robustness performance remain poorly understood. In this study, by analyzing the hidden state domain of recurrent policies learned over a diverse set of training methods, model architectures, and tasks, we find that stable cyclic structures consistently emerge during interaction with the environment. Such cyclic structures share a remarkable similarity with \textit{limit cycles} in dynamical system analysis, if we consider the policy and the environment as a joint hybrid dynamical system. Moreover, we uncover that the geometry of such limit cycles also has a structured correspondence with the policies' behaviors. These findings offer new perspectives to explain many nice properties of recurrent policies: the emergence of limit cycles stabilizes both the policies' internal memory and the task-relevant environmental states, while suppressing nuisance variability arising from environmental uncertainty; the geometry of limit cycles also encodes relational structures of behaviors, facilitating easier skill adaptation when facing non-stationary environments.

</details>


### [205] [SimpleGPT: Improving GPT via A Simple Normalization Strategy](https://arxiv.org/abs/2602.01212)
*Marco Chen,Xianbiao Qi,Yelin He,Jiaquan Ye,Rong Xiao*

Main category: cs.LG

TL;DR: 本文通过二阶几何视角重新审视Transformer优化，提出SimpleNorm归一化策略，显著降低Hessian矩阵谱范数，使学习率可提升3-10倍，在1B-8B参数规模的GPT模型上验证了优化稳定性和性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer优化存在学习率受限问题，作者从二阶几何角度分析架构设计、激活尺度、Hessian矩阵与最大可容忍学习率之间的关系，旨在提高优化稳定性。

Method: 提出SimpleNorm归一化策略，通过稳定中间激活尺度来降低Hessian谱范数，理论上证明该方法能显著扩大稳定学习率范围，并在SimpleGPT架构中实现。

Result: 在1B、1.4B、7B、8B参数规模的GPT模型上验证，SimpleGPT能容忍比标准方法高3-10倍的学习率，优化稳定性强，性能显著优于基线。7B模型训练60K步后损失从2.290降至2.208。

Conclusion: 通过SimpleNorm归一化策略有效解决了Transformer优化中的学习率限制问题，为大规模语言模型训练提供了更稳定高效的优化方案。

Abstract: In this work, we revisit Transformer optimization through the lens of second-order geometry and establish a direct connection between architectural design, activation scale, the Hessian matrix, and the maximum tolerable learning rate. We introduce a simple normalization strategy, termed SimpleNorm, which stabilizes intermediate activation scales by construction. Then, by analyzing the Hessian of the loss with respect to network activations, we theoretically show that SimpleNorm significantly reduces the spectral norm of the Hessian, thereby permitting larger stable learning rates. We validate our theoretical findings through extensive experiments on large GPT models at parameter scales 1B, 1.4B, 7B and 8B. Empirically, SimpleGPT, our SimpleNorm-based network, tolerates learning rates 3$\times$-10$\times$ larger than standard convention, consistently demonstrates strong optimization stability, and achieves substantially better performance than well-established baselines. Specifically, when training 7B-scale models for 60K steps, SimpleGPT achieves a training loss that is 0.08 lower than that of LLaMA2 with QKNorm, reducing the loss from 2.290 to 2.208. Our source code will be released at https://github.com/Ocram7/SimpleGPT.

</details>


### [206] [Learning from Anonymized and Incomplete Tabular Data](https://arxiv.org/abs/2602.01217)
*Lucas Lange,Adrian Böttinger,Victor Christen,Anushka Vidanage,Peter Christen,Erhard Rahm*

Main category: cs.LG

TL;DR: 论文提出处理用户驱动隐私保护下混合原始、泛化和缺失值的表格数据的新方法，通过考虑异构匿名化的数据转换策略，相比标准插补和LLM方法能更有效地恢复数据效用。


<details>
  <summary>Details</summary>
Motivation: 用户驱动的隐私保护允许个人控制数据共享的粒度，导致数据集在同一记录和属性中混合原始值、泛化值和缺失值。虽然这种表示对隐私保护很直观，但对机器学习提出了挑战，因为机器学习通常将非原始值视为新类别或缺失值，从而丢弃了泛化语义。

Method: 提出新颖的数据转换策略，考虑异构匿名化，并与标准插补方法和基于LLM的方法进行比较评估。使用多个数据集、隐私配置和部署场景进行实验。

Result: 方法可靠地恢复了数据效用。结果显示：泛化值优于纯抑制；最佳数据准备策略取决于具体场景；一致的数据表示对保持下游效用至关重要。

Conclusion: 有效学习与适当处理匿名化值密切相关。研究强调了在用户驱动隐私保护下，考虑泛化语义对机器学习的重要性。

Abstract: User-driven privacy allows individuals to control whether and at what granularity their data is shared, leading to datasets that mix original, generalized, and missing values within the same records and attributes. While such representations are intuitive for privacy, they pose challenges for machine learning, which typically treats non-original values as new categories or as missing, thereby discarding generalization semantics. For learning from such tabular data, we propose novel data transformation strategies that account for heterogeneous anonymization and evaluate them alongside standard imputation and LLM-based approaches. We employ multiple datasets, privacy configurations, and deployment scenarios, demonstrating that our method reliably regains utility. Our results show that generalized values are preferable to pure suppression, that the best data preparation strategy depends on the scenario, and that consistent data representations are crucial for maintaining downstream utility. Overall, our findings highlight that effective learning is tied to the appropriate handling of anonymized values.

</details>


### [207] [MiTA Attention: Efficient Fast-Weight Scaling via a Mixture of Top-$k$ Activations](https://arxiv.org/abs/2602.01219)
*Qishuai Wen,Zhiyuan Huang,Xianghan Meng,Wei He,Chun-Guang Li*

Main category: cs.LG

TL;DR: 提出MiTA注意力机制，通过压缩和路由策略将N宽度的快速权重MLP压缩为更窄的版本，使用地标查询和top-k激活来构建可变形专家。


<details>
  <summary>Details</summary>
Motivation: Transformer中的注意力算子可视为两层快速权重MLP，其宽度等于序列长度N。随着上下文扩展，这种N宽度MLP的表达能力增强，但快速权重的扩展对于极长序列变得极其昂贵。需要一种高效的方法来扩展快速权重。

Method: 提出压缩和路由策略：1) 使用少量地标查询将N宽度MLP压缩为更窄的MLP；2) 为每个地标查询收集top-k激活的键值对来构建可变形专家。该方法称为Mixture of Top-k Activations (MiTA)。

Result: 在视觉任务上的初步实验显示了MiTA注意力的潜力，表明需要进一步研究其优化和在更具挑战性场景中的更广泛应用。

Conclusion: 将高效注意力方法统一解释为通过路由和/或压缩来扩展快速权重的框架，提出的MiTA注意力通过压缩和路由策略有效解决了长序列的扩展问题。

Abstract: The attention operator in Transformers can be viewed as a two-layer fast-weight MLP, whose weights are dynamically instantiated from input tokens and whose width equals sequence length $N$. As the context extends, the expressive capacity of such an $N$-width MLP increases, but scaling its fast weights becomes prohibitively expensive for extremely long sequences. Recently, this fast-weight scaling perspective has motivated the Mixture-of-Experts (MoE) attention, which partitions the sequence into fast-weight experts and sparsely routes the tokens to them. In this paper, we elevate this perspective to a unifying framework for a wide range of efficient attention methods by interpreting them as scaling fast weights through routing and/or compression. Then we propose a compress-and-route strategy, which compresses the $N$-width MLP into a narrower one using a small set of landmark queries and constructs deformable experts by gathering top-$k$ activated key-value pairs for each landmark query. We call this strategy a Mixture of Top-$k$ Activations (MiTA), and refer to the resulting efficient mechanism as MiTA attention. Preliminary experiments on vision tasks demonstrate the promise of our MiTA attention and motivate further investigation on its optimization and broader applications in more challenging settings.

</details>


### [208] [Lotus: Efficient LLM Training by Randomized Low-Rank Gradient Projection with Adaptive Subspace Switching](https://arxiv.org/abs/2602.01233)
*Tianhao Miao,Zhongyuan Bao,Lejun Zhang*

Main category: cs.LG

TL;DR: Lotus是一种高效训练方法，通过改进梯度投影过程，在减少30%训练时间和40%内存消耗的同时保持模型性能，解决了GaLore方法中SVD计算带来的时间开销问题。


<details>
  <summary>Details</summary>
Motivation: 当前大规模模型训练存在内存消耗、训练时间和模型性能之间的权衡问题。GaLore方法虽然能实现内存高效训练，但由于需要对梯度进行奇异值分解(SVD)，带来了额外的训练时间成本。需要一种方法来解决这种权衡问题。

Method: Lotus通过修改投影过程，提出一个量化单位梯度位移的准则，实现低秩梯度子空间之间的高效转换。该方法避免了GaLore中的SVD计算，从而减少了时间开销。

Result: 实验结果显示，Lotus是最有效的方法：训练时间减少30%，梯度和优化器状态的内存消耗降低40%。在预训练和微调任务中都优于基线方法。

Conclusion: Lotus成功解决了大规模模型训练中内存消耗、训练时间和性能之间的权衡问题，通过改进梯度投影过程实现了更高效的训练。

Abstract: Training efficiency in large-scale models is typically assessed through memory consumption, training time, and model performance. Current methods often exhibit trade-offs among these metrics, as optimizing one generally degrades at least one of the others. Addressing this trade-off remains a central challenge in algorithm design. While GaLore enables memory-efficient training by updating gradients in a low-rank subspace, it incurs a comparable extra training time cost due to the Singular Value Decomposition(SVD) process on gradients. In this paper, we propose Lotus, a method that resolves this trade-off by simply modifying the projection process. We propose a criterion that quantifies the displacement of the unit gradient to enable efficient transitions between low-rank gradient subspaces. Experimental results indicate that Lotus is the most efficient method, achieving a 30% reduction in training time and a 40% decrease in memory consumption for gradient and optimizer states. Additionally, it outperforms the baseline method in both pre-training and fine-tuning tasks.

</details>


### [209] [Mechanistic Interpretability of Brain-to-Speech Models Across Speech Modes](https://arxiv.org/abs/2602.01247)
*Maryam Maghsoudi,Ayushi Mishra*

Main category: cs.LG

TL;DR: 该研究使用机制可解释性方法探究脑到语音解码模型中不同语音模态（发声、默读、想象）的内部表征机制，发现语音模态位于共享的连续因果流形上，跨模态传输由紧凑的层特定子空间而非扩散活动介导。


<details>
  <summary>Details</summary>
Motivation: 虽然脑到语音解码模型在发声、默读和想象语音中表现出稳健性能，但这些模型如何在不同语音模态间捕获和传递信息的基本机制尚不明确。研究旨在因果性地探究神经语音解码器的内部表征机制。

Method: 使用机制可解释性方法：1）跨模态激活修补分析内部激活；2）三模态插值检验语音表征是离散还是连续变化；3）粗到细因果追踪和因果擦除以定位因果结构；4）神经元级激活修补分析效应分布。

Result: 发现：1）语音模态位于共享的连续因果流形上；2）跨模态传输由紧凑的层特定子空间介导，而非扩散活动；3）小而非分布的子集神经元（非孤立单元）影响跨模态传输；4）语音模态间存在层次性和方向依赖的表征结构。

Conclusion: 研究为脑到语音解码模型中语音模态信息的组织和使用提供了因果解释，揭示了跨语音模态的层次性和方向依赖表征结构，表明跨模态传输通过紧凑的层特定子空间实现。

Abstract: Brain-to-speech decoding models demonstrate robust performance in vocalized, mimed, and imagined speech; yet, the fundamental mechanisms via which these models capture and transmit information across different speech modalities are less explored. In this work, we use mechanistic interpretability to causally investigate the internal representations of a neural speech decoder. We perform cross-mode activation patching of internal activations across speech modes, and use tri-modal interpolation to examine whether speech representations vary discretely or continuously. We use coarse-to-fine causal tracing and causal scrubbing to find localized causal structure, allowing us to find internal subspaces that are sufficient for cross-mode transfer. In order to determine how finely distributed these effects are within layers, we perform neuron-level activation patching. We discover that small but not distributed subsets of neurons, rather than isolated units, affect the cross-mode transfer. Our results show that speech modes lie on a shared continuous causal manifold, and cross-mode transfer is mediated by compact, layer-specific subspaces rather than diffuse activity. Together, our findings give a causal explanation for how speech modality information is organized and used in brain-to-speech decoding models, revealing hierarchical and direction-dependent representational structure across speech modes.

</details>


### [210] [Sample Efficient Active Algorithms for Offline Reinforcement Learning](https://arxiv.org/abs/2602.01260)
*Soumyadeep Roy,Shashwat Kushwaha,Ambedkar Dukkipati*

Main category: cs.LG

TL;DR: 本文提出了ActiveRL算法，通过有限在线交互选择性优化价值函数的不确定区域，将离线强化学习的样本复杂度从Ω(1/ε²(1-γ)⁴)提升到O(1/ε²)，实现了近最优的信息效率。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习通常面临状态-动作空间覆盖不足和分布偏移问题。虽然通过有限在线交互选择性优化价值函数不确定区域的ActiveRL方法在实证中表现良好，但缺乏理论分析。

Method: 提出ActiveRL算法，利用高斯过程不确定性建模，通过信息增益边界和GP集中不等式，在不确定区域进行选择性在线交互来优化价值函数。

Result: 理论证明ActiveRL能以O(1/ε²)的主动转移学习ε-最优策略，相比纯离线方法的Ω(1/ε²(1-γ)⁴)有显著提升，实现了近最优的信息效率。实验验证了算法和理论发现。

Conclusion: ActiveRL通过引导不确定性减少实现了加速的价值函数收敛，以最少的在线数据达到近最优性能，为贝叶斯非参数回归和强化学习理论架起了桥梁。

Abstract: Offline reinforcement learning (RL) enables policy learning from static data but often suffers from poor coverage of the state-action space and distributional shift problems. This problem can be addressed by allowing limited online interactions to selectively refine uncertain regions of the learned value function, which is referred to as Active Reinforcement Learning (ActiveRL). While there has been good empirical success, no theoretical analysis is available in the literature. We fill this gap by developing a rigorous sample-complexity analysis of ActiveRL through the lens of Gaussian Process (GP) uncertainty modeling. In this respect, we propose an algorithm and using GP concentration inequalities and information-gain bounds, we derive high-probability guarantees showing that an $ε$-optimal policy can be learned with ${\mathcal{O}}(1/ε^2)$ active transitions, improving upon the $Ω(1/ε^2(1-γ)^4)$ rate of purely offline methods. Our results reveal that ActiveRL achieves near-optimal information efficiency, that is, guided uncertainty reduction leads to accelerated value-function convergence with minimal online data. Our analysis builds on GP concentration inequalities and information-gain bounds, bridging Bayesian nonparametric regression and reinforcement learning theories. We conduct several experiments to validate the algorithm and theoretical findings.

</details>


### [211] [BicKD: Bilateral Contrastive Knowledge Distillation](https://arxiv.org/abs/2602.01265)
*Jiangnan Zhu,Yukai Xu,Li Xiong,Yixuan Liu,Junxu Liu,Hong kyu Lee,Yujie Gu*

Main category: cs.LG

TL;DR: BicKD提出双边对比知识蒸馏方法，通过双边对比损失增强类间正交性，同时保持类内一致性，在多种架构和基准测试中超越现有蒸馏技术。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏（vanilla KD）只进行样本级别的概率对齐，缺乏类级别的比较机制，且没有对概率空间施加结构约束，限制了知识迁移效果。

Method: 提出双边对比知识蒸馏（BicKD），引入双边对比损失，增强不同类泛化空间的正交性，同时保持同类一致性，实现样本级和类级预测模式的显式比较。

Result: 大量实验表明，BicKD方法增强了知识迁移能力，在各种模型架构和基准测试中一致优于最先进的知识蒸馏技术。

Conclusion: BicKD通过双边对比损失和概率正交性约束，有效解决了传统知识蒸馏的局限性，为知识蒸馏提供了更强大的框架。

Abstract: Knowledge distillation (KD) is a machine learning framework that transfers knowledge from a teacher model to a student model. The vanilla KD proposed by Hinton et al. has been the dominant approach in logit-based distillation and demonstrates compelling performance. However, it only performs sample-wise probability alignment between teacher and student's predictions, lacking an mechanism for class-wise comparison. Besides, vanilla KD imposes no structural constraint on the probability space. In this work, we propose a simple yet effective methodology, bilateral contrastive knowledge distillation (BicKD). This approach introduces a novel bilateral contrastive loss, which intensifies the orthogonality among different class generalization spaces while preserving consistency within the same class. The bilateral formulation enables explicit comparison of both sample-wise and class-wise prediction patterns between teacher and student. By emphasizing probabilistic orthogonality, BicKD further regularizes the geometric structure of the predictive distribution. Extensive experiments show that our BicKD method enhances knowledge transfer, and consistently outperforms state-of-the-art knowledge distillation techniques across various model architectures and benchmarks.

</details>


### [212] [Diving into Kronecker Adapters: Component Design Matters](https://arxiv.org/abs/2602.01267)
*Jiayu Bai,Danchen Yu,Zhenyu Liao,TianQi Hou,Feng Zhou,Robert C. Qiu,Zenan Ling*

Main category: cs.LG

TL;DR: CDKA通过精细设计Kronecker适配器的组件结构和维度，提升大模型微调性能，并提供参数预算感知的配置指南。


<details>
  <summary>Details</summary>
Motivation: 现有Kronecker适配器方法将组件结构视为固定或启发式设计，未深入探索组件维度和数量对模型容量的影响，限制了其与全参数微调的对齐效果。

Method: 提出Component Designed Kronecker Adapters (CDKA)，通过精细分析Kronecker组件的维度和数量，设计优化的组件结构，并提供参数预算感知的配置指南和训练稳定策略。

Result: 在多个自然语言处理任务上的实验证明了CDKA的有效性，能够更好地与全参数微调对齐，提升模型性能。

Conclusion: 组件结构是Kronecker适配器容量的关键因素，CDKA通过优化组件设计实现了更好的微调效果，为实际部署提供了实用指南。

Abstract: Kronecker adapters have emerged as a promising approach for fine-tuning large-scale models, enabling high-rank updates through tunable component structures. However, existing work largely treats the component structure as a fixed or heuristic design choice, leaving the dimensions and number of Kronecker components underexplored. In this paper, we identify component structure as a key factor governing the capacity of Kronecker adapters. We perform a fine-grained analysis of both the dimensions and number of Kronecker components. In particular, we show that the alignment between Kronecker adapters and full fine-tuning depends on component configurations. Guided by these insights, we propose Component Designed Kronecker Adapters (CDKA). We further provide parameter-budget-aware configuration guidelines and a tailored training stabilization strategy for practical deployment. Experiments across various natural language processing tasks demonstrate the effectiveness of CDKA. Code is available at https://github.com/rainstonee/CDKA.

</details>


### [213] [Mixture-of-World Models: Scaling Multi-Task Reinforcement Learning with Modular Latent Dynamics](https://arxiv.org/abs/2602.01270)
*Boxuan Zhang,Weipu Zhang,Zhaohan Feng,Wei Xiao,Jian Sun,Jie Chen,Gang Wang*

Main category: cs.LG

TL;DR: MoW是一种用于多任务强化学习的混合世界模型架构，通过模块化视觉压缩、任务条件化专家和梯度聚类策略，在Atari和Meta-World基准上实现了参数高效且性能优越的结果。


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习在视觉领域中面临样本效率挑战，特别是当任务在观察和动态特性上存在显著异质性时。传统的单体世界模型架构难以捕捉多样化的任务动态，导致重建和预测准确性差。

Method: 提出了混合世界模型（MoW）架构，包含：1）用于任务自适应视觉压缩的模块化变分自编码器；2）具有任务条件化专家和共享骨干的混合Transformer动态模型；3）基于梯度的任务聚类策略以实现高效参数分配。

Result: 在Atari 100k基准上，单个MoW代理在26个Atari游戏中获得110.4%的平均人类标准化分数，与26个任务特定模型集成（STORM）的114.2%相当，但参数减少50%。在Meta-World上，MoW在30万环境步内达到74.5%的平均成功率，创下新纪录。

Conclusion: MoW为通用世界模型提供了一个可扩展且参数高效的基础框架，能够有效处理多任务强化学习中视觉领域的异质性问题。

Abstract: A fundamental challenge in multi-task reinforcement learning (MTRL) is achieving sample efficiency in visual domains where tasks exhibit substantial heterogeneity in both observations and dynamics. Model-based reinforcement learning offers a promising path to improved sample efficiency through world models, but standard monolithic architectures struggle to capture diverse task dynamics, resulting in poor reconstruction and prediction accuracy. We introduce Mixture-of-World Models (MoW), a scalable architecture that combines modular variational autoencoders for task-adaptive visual compression, a hybrid Transformer-based dynamics model with task-conditioned experts and a shared backbone, and a gradient-based task clustering strategy for efficient parameter allocation. On the Atari 100k benchmark, a single MoW agent trained once on 26 Atari games achieves a mean human-normalized score of 110.4%, competitive with the score of 114.2% achieved by STORM, an ensemble of 26 task-specific models, while using 50% fewer parameters. On Meta-World, MoW achieves a 74.5% average success rate within 300 thousand environment steps, establishing a new state of the art. These results demonstrate that MoW provides a scalable and parameter-efficient foundation for generalist world models.

</details>


### [214] [From Intents to Actions: Agentic AI in Autonomous Networks](https://arxiv.org/abs/2602.01271)
*Burak Demirel,Pablo Soldati,Yu Wang*

Main category: cs.LG

TL;DR: 提出基于三个专门智能体的Agentic AI系统，用于意图驱动的自治网络，通过语言模型解析意图、优化问题转换和多目标强化学习控制，实现网络自主解释、推理和适应多样化意图。


<details>
  <summary>Details</summary>
Motivation: 电信网络需要自主运行并支持具有多样化且经常冲突意图的异构服务，但现有启发式方法无法将高层意图（如超低延迟、高吞吐量、能效）转化为具体的控制动作。

Method: 构建包含三个专门智能体的系统：1) 基于语言模型的监督解释器，将意图解析为可执行优化模板并进行认知细化；2) 优化器，将模板转化为可处理的优化问题并分析权衡；3) 基于多目标强化学习的偏好驱动控制器，利用偏好操作接近帕累托前沿。

Result: 该系统使网络能够以可扩展的方式自主解释、推理、适应和响应多样化意图和网络条件，实现意图驱动的自治网络控制。

Conclusion: 提出的Agentic AI系统通过三个专门智能体的协同工作，解决了将高层意图转化为具体控制动作的挑战，为意图驱动的自治网络提供了可扩展的解决方案。

Abstract: Telecommunication networks are increasingly expected to operate autonomously while supporting heterogeneous services with diverse and often conflicting intents -- that is, performance objectives, constraints, and requirements specific to each service. However, transforming high-level intents -- such as ultra-low latency, high throughput, or energy efficiency -- into concrete control actions (i.e., low-level actuator commands) remains beyond the capability of existing heuristic approaches. This work introduces an Agentic AI system for intent-driven autonomous networks, structured around three specialized agents. A supervisory interpreter agent, powered by language models, performs both lexical parsing of intents into executable optimization templates and cognitive refinement based on feedback, constraint feasibility, and evolving network conditions. An optimizer agent converts these templates into tractable optimization problems, analyzes trade-offs, and derives preferences across objectives. Lastly, a preference-driven controller agent, based on multi-objective reinforcement learning, leverages these preferences to operate near the Pareto frontier of network performance that best satisfies the original intent. Collectively, these agents enable networks to autonomously interpret, reason over, adapt to, and act upon diverse intents and network conditions in a scalable manner.

</details>


### [215] [Richer Bayesian Last Layers with Subsampled NTK Features](https://arxiv.org/abs/2602.01279)
*Sergio Calvo-Ordoñez,Jonathan Plenk,Richard Bergna,Álvaro Cartea,Yarin Gal,Jose Miguel Hernández-Lobato,Kamil Ciosek*

Main category: cs.LG

TL;DR: 本文提出了一种改进贝叶斯最后一层（BLLs）的方法，通过将神经正切核（NTK）特征投影到最后一层特征空间，从而在保持BLL计算效率的同时，更好地估计神经网络的认知不确定性。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯最后一层（BLLs）虽然提供了一种计算高效的神经网络不确定性估计方法，但只对最后一层应用贝叶斯处理，忽略了前面层引入的不确定性，导致认知不确定性被低估。

Method: 提出将神经正切核（NTK）特征投影到最后一层特征空间的方法，使后验推断能够考虑整个网络的变异性。同时引入均匀子采样方案来估计投影矩阵和进行后验推断，以进一步降低计算成本。

Result: 理论证明该方法产生的后验方差大于或等于标准BLL，纠正了其低估认知不确定性的倾向。在UCI回归、上下文赌博机、图像分类以及图像和表格数据集的分布外检测任务中，相比标准BLL和竞争基线，该方法显示出更好的校准和不确定性估计，同时降低了计算成本。

Conclusion: 该方法在保持BLL计算效率的同时，通过NTK特征投影有效改善了神经网络认知不确定性的估计，在实际应用中表现出更好的校准性能和不确定性量化能力。

Abstract: Bayesian Last Layers (BLLs) provide a convenient and computationally efficient way to estimate uncertainty in neural networks. However, they underestimate epistemic uncertainty because they apply a Bayesian treatment only to the final layer, ignoring uncertainty induced by earlier layers. We propose a method that improves BLLs by leveraging a projection of Neural Tangent Kernel (NTK) features onto the space spanned by the last-layer features. This enables posterior inference that accounts for variability of the full network while retaining the low computational cost of inference of a standard BLL. We show that our method yields posterior variances that are provably greater or equal to those of a standard BLL, correcting its tendency to underestimate epistemic uncertainty. To further reduce computational cost, we introduce a uniform subsampling scheme for estimating the projection matrix and for posterior inference. We derive approximation bounds for both types of sub-sampling. Empirical evaluations on UCI regression, contextual bandits, image classification, and out-of-distribution detection tasks in image and tabular datasets, demonstrate improved calibration and uncertainty estimates compared to standard BLLs and competitive baselines, while reducing computational cost.

</details>


### [216] [Multi-LLM Adaptive Conformal Inference for Reliable LLM Responses](https://arxiv.org/abs/2602.01285)
*Kangjun Noh,Seongchan Lee,Ilmun Kim,Kyungwoo Song*

Main category: cs.LG

TL;DR: 提出MACI方法，通过乘积过滤框架和多LLM集成，在保证覆盖率的同时显著提高真实声明的保留率


<details>
  <summary>Details</summary>
Motivation: 在医疗和法律等高风险领域，确保大语言模型的事实性至关重要。现有保形推理方法要么过于保守（丢弃过多真实声明），要么依赖自适应错误率和简单线性模型，无法捕捉复杂群体结构。

Method: 将保形推理重新表述为乘积过滤框架，将事实性建模为声明级分数的乘积。MACI方法利用多LLM集成产生更准确的事实性分数，并通过群体条件校准保持有效性。

Result: 实验表明MACI始终达到用户指定的覆盖率，同时相比基线方法显著提高保留率并降低时间成本。

Conclusion: MACI通过乘积过滤和多LLM集成，在保证保形推理有效性的同时，显著改善了事实性评估的效率和准确性。

Abstract: Ensuring factuality is essential for the safe use of Large Language Models (LLMs) in high-stakes domains such as medicine and law. Conformal inference provides distribution-free guarantees, but existing approaches are either overly conservative, discarding many true-claims, or rely on adaptive error rates and simple linear models that fail to capture complex group structures. To address these challenges, we reformulate conformal inference in a multiplicative filtering setting, modeling factuality as a product of claim-level scores. Our method, Multi-LLM Adaptive Conformal Inference (MACI), leverages ensembles to produce more accurate factuality-scores, which in our experiments led to higher retention, while validity is preserved through group-conditional calibration. Experiments show that MACI consistently achieves user-specified coverage with substantially higher retention and lower time cost than baselines. Our repository is available at https://github.com/MLAI-Yonsei/MACI

</details>


### [217] [EDIS: Diagnosing LLM Reasoning via Entropy Dynamics](https://arxiv.org/abs/2602.01288)
*Chenghua Zhu,Siyan Wu,Xiangkang Zeng,Zishan Xu,Zhaolu Kang,Yifu Guo,Yuquan Lu,Junduan Huang,Guojing Zhou*

Main category: cs.LG

TL;DR: 论文提出利用熵动态演化而非静态聚合统计来提升大语言模型推理能力，通过分析token级熵轨迹识别正确与错误推理的特征模式，并引入熵动态不稳定性评分（EDIS）作为诊断信号。


<details>
  <summary>Details</summary>
Motivation: 现有方法将置信度视为静态量（通常在token上聚合），但置信度在生成过程中的时间演化包含比聚合统计更丰富的信息，这为理解和改进LLM推理提供了新视角。

Method: 分析token级熵轨迹，识别错误推理的特征模式（不稳定动态、突发尖峰、峰谷尖峰等），并引入熵动态不稳定性评分（EDIS）来量化熵演化中的不稳定性。

Result: 错误解决方案表现出不稳定动态，这些模式在不同模型和训练阶段持续存在，表明它们反映了推理失败的内在特性。EDIS作为推理时选择的有效诊断信号，显著提高了推理准确性。

Conclusion: 熵动态为理解和改进LLM推理提供了一个未被充分探索但信息丰富的视角，EDIS不仅可用于推理时选择，还为训练时样本筛选提供了有前景的方向。

Abstract: Entropy-based confidence signals are increasingly leveraged to improve reasoning in large language models (LLMs), yet existing approaches treat confidence as a static quantity -- typically aggregated over tokens. We show that the \emph{temporal evolution} of confidence during generation carries richer information than aggregate statistics alone. Analyzing token-level entropy trajectories, we identify characteristic patterns distinguishing correct from incorrect reasoning: erroneous solutions exhibit unstable dynamics, including burst spikes (sustained uncertainty growth) and peak-valley spikes (sharp rebounds following transient confidence). These patterns persist across models and training stages, suggesting they reflect intrinsic properties of reasoning failure rather than superficial noise. To formalize this observation, we introduce the Entropy Dynamics Instability Score (\textbf{EDIS}), a trajectory-level metric quantifying instability in entropy evolution. EDIS serves as an effective diagnostic signal for inference-time selection, substantially improving reasoning accuracy, and offers a promising direction for training-time sample curation. Our findings establish entropy dynamics as an underexplored yet informative lens for understanding and improving LLM reasoning.

</details>


### [218] [Gradient-Aligned Calibration for Post-Training Quantization of Diffusion Models](https://arxiv.org/abs/2602.01289)
*Dung Anh Hoang,Cuong Pham anh Trung Le,Jianfei Cai,Toan Do*

Main category: cs.LG

TL;DR: 提出一种新的扩散模型后训练量化方法，通过学习为不同时间步的校准样本分配最优权重，解决现有方法中均匀权重分配和梯度冲突问题，显著提升量化性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然图像合成性能优异，但存在推理速度慢、内存占用高、计算需求大的问题。后训练量化是加速采样和减少内存开销的可行方案，但现有方法在不同时间步使用均匀权重分配校准样本，忽略了不同时间步数据对扩散过程的贡献差异，且由于激活分布和梯度在不同时间步的变化，均匀量化方法会导致梯度冲突，降低性能。

Method: 提出一种新颖的后训练量化方法，通过学习为校准样本分配最优权重。该方法通过优化权重分配，使量化模型在不同时间步的梯度对齐，从而促进量化过程。具体来说，针对不同时间步的激活分布和梯度变化特点，为每个时间步分配不同的权重，避免梯度冲突。

Result: 在CIFAR-10、LSUN-Bedrooms和ImageNet数据集上进行广泛实验，结果表明该方法相比其他扩散模型后训练量化方法具有优越性。

Conclusion: 通过为不同时间步的校准样本学习最优权重分配，能够有效解决扩散模型量化中的梯度冲突问题，提升量化性能，为扩散模型的实用部署提供更高效的量化解决方案。

Abstract: Diffusion models have shown remarkable performance in image synthesis by progressively estimating a smooth transition from a Gaussian distribution of noise to a real image. Unfortunately, their practical deployment is limited by slow inference speed, high memory usage, and the computational demands of the noise estimation process. Post-training quantization (PTQ) emerges as a promising solution to accelerate sampling and reduce memory overhead for diffusion models. Existing PTQ methods for diffusion models typically apply uniform weights to calibration samples across timesteps, which is sub-optimal since data at different timesteps may contribute differently to the diffusion process. Additionally, due to varying activation distributions and gradients across timesteps, a uniform quantization approach is sub-optimal. Each timestep requires a different gradient direction for optimal quantization, and treating them equally can lead to conflicting gradients that degrade performance. In this paper, we propose a novel PTQ method that addresses these challenges by assigning appropriate weights to calibration samples. Specifically, our approach learns to assign optimal weights to calibration samples to align the quantized model's gradients across timesteps, facilitating the quantization process. Extensive experiments on CIFAR-10, LSUN-Bedrooms, and ImageNet demonstrate the superiority of our method compared to other PTQ methods for diffusion models.

</details>


### [219] [The BoBW Algorithms for Heavy-Tailed MDPs](https://arxiv.org/abs/2602.01295)
*Yu Chen,Yuhao Liu,Jiatai Huang,Yihan Du,Longbo Huang*

Main category: cs.LG

TL;DR: 提出HT-FTRL-OM和HT-FTRL-UOB算法，用于处理具有重尾反馈的马尔可夫决策过程，实现了对抗环境下实例无关的遗憾界和随机环境下对数遗憾界的"两全其美"保证。


<details>
  <summary>Details</summary>
Motivation: 现有处理重尾反馈MDP的方法在随机环境中过于保守，在对抗环境中缺乏适应性。需要开发能够同时适应两种环境的算法。

Method: HT-FTRL-OM：在已知转移概率下，在占用度量上应用FTRL框架，采用新颖的跳跃损失估计器。HT-FTRL-UOB：针对未知转移概率，使用悲观跳跃损失估计器，处理更复杂的设置。

Result: HT-FTRL-OM在对抗环境中达到$\widetilde{\mathcal{O}}(T^{1/α})$遗憾，在随机环境中达到$\mathcal{O}(\log T)$遗憾。HT-FTRL-UOB在对抗环境中达到$\widetilde{\mathcal{O}}(T^{1/α} + \sqrt{T})$遗憾，在随机环境中达到$\mathcal{O}(\log^2(T))$遗憾。

Conclusion: 通过局部控制机制、次优质量传播原理和新的遗憾分解等技术创新，成功解决了重尾反馈MDP中的关键障碍，实现了对抗和随机环境下的两全其美性能保证。

Abstract: We investigate episodic Markov Decision Processes with heavy-tailed feedback (HTMDPs). Existing approaches for HTMDPs are conservative in stochastic environments and lack adaptivity in adversarial regimes. In this work, we propose algorithms ```HT-FTRL-OM``` and ```HT-FTRL-UOB``` for HTMDPs that achieve Best-of-Both-Worlds (BoBW) guarantees: instance-independent regret in adversarial environments and logarithmic instance-dependent regret in self-bounding (including the stochastic case) environments. For the known transition setting, ```HT-FTRL-OM``` applies the Follow-The-Regularized-Leader (FTRL) framework over occupancy measures with novel skipping loss estimators, achieving a $\widetilde{\mathcal{O}}(T^{1/α})$ regret bound in adversarial regimes and a $\mathcal{O}(\log T)$ regret in stochastic regimes. Building upon this framework, we develop a novel algorithm ```HT-FTRL-UOB``` to tackle the more challenging unknown-transition setting. This algorithm employs a pessimistic skipping loss estimator and achieves a $\widetilde{\mathcal{O}}(T^{1/α} + \sqrt{T})$ regret in adversarial regimes and a $\mathcal{O}(\log^2(T))$ regret in stochastic regimes. Our analysis overcomes key barriers through several technical insights, including a local control mechanism for heavy-tailed shifted losses, a new suboptimal-mass propagation principle, and a novel regret decomposition that isolates transition uncertainty from heavy-tailed estimation errors and skipping bias.

</details>


### [220] [Dispelling the Curse of Singularities in Neural Network Optimizations](https://arxiv.org/abs/2602.01308)
*Hengjie Cao,Mengyi Chen,Yifeng Yang,Fang Dong,Ruijun Huang,Anrui Chen,Jixian Zhou,Mingzhi Dong,Yujiang Wang,Dongsheng Li,Wenyi Fang,Yuanyi Lin,Fan Wu,Li Shang*

Main category: cs.LG

TL;DR: 论文从参数空间奇异性的角度研究深度神经网络优化不稳定性，发现奇异值随训练增长并导致损失爆炸风险，提出参数奇异性平滑方法PSS来缓解问题


<details>
  <summary>Details</summary>
Motivation: 从参数空间奇异性的新视角研究深度神经网络优化不稳定性问题，传统方法较少关注参数奇异性的产生和放大对训练稳定性的影响

Method: 提出参数奇异性平滑方法PSS，轻量级、灵活且有效地平滑权重矩阵的奇异谱，防止奇异值过度增长

Result: 在多种数据集、架构和优化器上的实验表明，PSS能缓解不稳定性，在训练失败后恢复可训练性，并提高训练效率和泛化能力

Conclusion: 参数奇异性是深度神经网络优化不稳定性的关键因素，PSS方法能有效控制奇异值增长，提升训练稳定性和模型性能

Abstract: This work investigates the optimization instability of deep neural networks from a less-explored yet insightful perspective: the emergence and amplification of singularities in the parametric space. Our analysis reveals that parametric singularities inevitably grow with gradient updates and further intensify alignment with representations, leading to increased singularities in the representation space. We show that the gradient Frobenius norms are bounded by the top singular values of the weight matrices, and as training progresses, the mutually reinforcing growth of weight and representation singularities, termed the curse of singularities, relaxes these bounds, escalating the risk of sharp loss explosions. To counter this, we propose Parametric Singularity Smoothing (PSS), a lightweight, flexible, and effective method for smoothing the singular spectra of weight matrices. Extensive experiments across diverse datasets, architectures, and optimizers demonstrate that PSS mitigates instability, restores trainability even after failure, and improves both training efficiency and generalization.

</details>


### [221] [Imperfect Influence, Preserved Rankings: A Theory of TRAK for Data Attribution](https://arxiv.org/abs/2602.01312)
*Han Tong,Shubhangi Ghosh,Haolin Zou,Arian Maleki*

Main category: cs.LG

TL;DR: TRAK算法用于数据归因，通过核机器近似模型并利用ALO风险近似技术，本文首次提供理论分析，证明虽然近似引入显著误差，但TRAK估计的影响与原始影响高度相关，能保持数据点相对排序。


<details>
  <summary>Details</summary>
Motivation: TRAK算法在数据归因方面表现出色，但其理论条件、准确性和失效机制尚未充分探索，需要理论分析来理解其性能边界和近似误差。

Method: 对TRAK算法进行理论分析，量化方法依赖的近似引入的误差，通过理论推导和数学证明来表征算法性能。

Result: 尽管近似引入显著误差，但TRAK估计的影响与原始影响高度相关，能有效保持数据点的相对排序，理论结果通过大量模拟和实证研究得到验证。

Conclusion: TRAK算法在数据归因中虽然存在近似误差，但在保持相对排序方面表现良好，为理解该算法的理论基础和实际应用提供了重要见解。

Abstract: Data attribution, tracing a model's prediction back to specific training data, is an important tool for interpreting sophisticated AI models. The widely used TRAK algorithm addresses this challenge by first approximating the underlying model with a kernel machine and then leveraging techniques developed for approximating the leave-one-out (ALO) risk. Despite its strong empirical performance, the theoretical conditions under which the TRAK approximations are accurate as well as the regimes in which they break down remain largely unexplored. In this paper, we provide a theoretical analysis of the TRAK algorithm, characterizing its performance and quantifying the errors introduced by the approximations on which the method relies. We show that although the approximations incur significant errors, TRAK's estimated influence remains highly correlated with the original influence and therefore largely preserves the relative ranking of data points. We corroborate our theoretical results through extensive simulations and empirical studies.

</details>


### [222] [PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding](https://arxiv.org/abs/2602.01322)
*Panagiotis Koromilas,Andreas D. Demou,James Oldfield,Yannis Panagakis,Mihalis Nicolaou*

Main category: cs.LG

TL;DR: PolySAE通过引入高阶多项式项扩展稀疏自编码器，能够捕捉特征间的组合结构，而不仅仅是线性叠加，从而更好地解释神经网络表示中的复合概念。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏自编码器(SAE)假设特征通过线性重构相加，无法区分组合结构（如"star"+"coffee"→"Starbucks"）与简单共现。这导致SAE为复合概念分配整体特征而非可解释的组成部分。

Method: PolySAE在SAE解码器中引入高阶项（二阶和三阶特征交互），通过共享投影子空间上的低秩张量分解建模特征交互，同时保持线性编码器以确保可解释性，参数开销仅增加3%（在GPT2上）。

Result: 在四个语言模型和三种SAE变体上，PolySAE平均提升约8%的探测F1分数，同时保持可比的重构误差，并产生2-10倍更大的类别条件特征分布Wasserstein距离。学习到的交互权重与共现频率相关性极低（r=0.06 vs SAE特征协方差的r=0.82）。

Conclusion: 多项式项能够捕捉形态绑定和短语组合等组合结构，这些结构与表面统计基本独立，为神经网络表示的可解释性提供了更准确的特征分解方法。

Abstract: Sparse autoencoders (SAEs) have emerged as a promising method for interpreting neural network representations by decomposing activations into sparse combinations of dictionary atoms. However, SAEs assume that features combine additively through linear reconstruction, an assumption that cannot capture compositional structure: linear models cannot distinguish whether "Starbucks" arises from the composition of "star" and "coffee" features or merely their co-occurrence. This forces SAEs to allocate monolithic features for compound concepts rather than decomposing them into interpretable constituents. We introduce PolySAE, which extends the SAE decoder with higher-order terms to model feature interactions while preserving the linear encoder essential for interpretability. Through low-rank tensor factorization on a shared projection subspace, PolySAE captures pairwise and triple feature interactions with small parameter overhead (3% on GPT2). Across four language models and three SAE variants, PolySAE achieves an average improvement of approximately 8% in probing F1 while maintaining comparable reconstruction error, and produces 2-10$\times$ larger Wasserstein distances between class-conditional feature distributions. Critically, learned interaction weights exhibit negligible correlation with co-occurrence frequency ($r = 0.06$ vs. $r = 0.82$ for SAE feature covariance), suggesting that polynomial terms capture compositional structure, such as morphological binding and phrasal composition, largely independent of surface statistics.

</details>


### [223] [High-accuracy sampling for diffusion models and log-concave distributions](https://arxiv.org/abs/2602.01338)
*Fan Chen,Sinho Chewi,Constantinos Daskalakis,Alexander Rakhlin*

Main category: cs.LG

TL;DR: 提出扩散模型采样算法，能以polylog(1/δ)步数达到δ误差，相比之前方法有指数级改进


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型采样方法需要较多步数才能达到高精度，本文旨在显著降低采样复杂度，实现指数级加速

Method: 基于L²空间中δ-准确分数估计，在不同数据假设下设计高效采样算法：最小数据假设、非均匀L-Lipschitz条件、数据分布具有内在维度d*

Result: 在三种情况下分别实现复杂度：Õ(d polylog(1/δ))、Õ(√(dL) polylog(1/δ))、Õ(d* polylog(1/δ))，并首次为一般对数凹分布提供polylog(1/δ)复杂度采样器

Conclusion: 本文算法实现了扩散模型采样的指数级加速，显著降低了采样复杂度，为高效扩散模型采样提供了新方法

Abstract: We present algorithms for diffusion model sampling which obtain $δ$-error in $\mathrm{polylog}(1/δ)$ steps, given access to $\widetilde O(δ)$-accurate score estimates in $L^2$. This is an exponential improvement over all previous results. Specifically, under minimal data assumptions, the complexity is $\widetilde O(d\,\mathrm{polylog}(1/δ))$ where $d$ is the dimension of the data; under a non-uniform $L$-Lipschitz condition, the complexity is $\widetilde O(\sqrt{dL}\,\mathrm{polylog}(1/δ))$; and if the data distribution has intrinsic dimension $d_\star$, then the complexity reduces to $\widetilde O(d_\star\,\mathrm{polylog}(1/δ))$. Our approach also yields the first $\mathrm{polylog}(1/δ)$ complexity sampler for general log-concave distributions using only gradient evaluations.

</details>


### [224] [Finding Differentially Private Second Order Stationary Points in Stochastic Minimax Optimization](https://arxiv.org/abs/2602.01339)
*Difei Xu,Youming Tao,Meng Ding,Chenglin Fan,Di Wang*

Main category: cs.LG

TL;DR: 首次研究随机非凸极小极大优化中寻找差分隐私二阶平稳点的问题，提出结合嵌套梯度下降-上升、SPIDER方差缩减和高斯扰动的一阶方法，在经验风险和总体风险下均达到最优隐私收敛率。


<details>
  <summary>Details</summary>
Motivation: 现有文献要么只关注极小极大问题的一阶平稳点，要么只关注经典随机最小化问题的二阶平稳点。缺乏对随机非凸极小极大优化中差分隐私二阶平稳点的研究，特别是在经验风险和总体风险统一框架下的分析。

Method: 提出纯一阶方法，结合嵌套梯度下降-上升方案、SPIDER风格方差缩减和高斯扰动确保隐私。关键技术是块状(q周期)分析，控制随机方差和隐私噪声的累积，无需在整个迭代范围内求和，统一处理经验风险和总体风险公式。

Result: 在标准光滑性、Hessian-Lipschitz性和强凹性假设下，建立了高概率保证：对于经验风险目标达到(α,√(ρ_Φα))-近似二阶平稳点，α=O((√d/nε)^(2/3))；对于总体目标达到O(1/n^(1/3)+(√d/nε)^(1/2))，匹配已知的隐私一阶平稳性最优速率。

Conclusion: 首次为随机非凸极小极大优化提供了差分隐私二阶平稳点的统一分析框架，提出的方法在经验风险和总体风险下均达到最优收敛速率，填补了该领域的研究空白。

Abstract: We provide the first study of the problem of finding differentially private (DP) second-order stationary points (SOSP) in stochastic (non-convex) minimax optimization. Existing literature either focuses only on first-order stationary points for minimax problems or on SOSP for classical stochastic minimization problems. This work provides, for the first time, a unified and detailed treatment of both empirical and population risks. Specifically, we propose a purely first-order method that combines a nested gradient descent--ascent scheme with SPIDER-style variance reduction and Gaussian perturbations to ensure privacy. A key technical device is a block-wise ($q$-period) analysis that controls the accumulation of stochastic variance and privacy noise without summing over the full iteration horizon, yielding a unified treatment of both empirical-risk and population formulations. Under standard smoothness, Hessian-Lipschitzness, and strong concavity assumptions, we establish high-probability guarantees for reaching an $(α,\sqrt{ρ_Φα})$-approximate second-order stationary point with $α= \mathcal{O}( (\frac{\sqrt{d}}{n\varepsilon})^{2/3})$ for empirical risk objectives and $\mathcal{O}(\frac{1}{n^{1/3}} + (\frac{\sqrt{d}}{n\varepsilon})^{1/2})$ for population objectives, matching the best known rates for private first-order stationarity.

</details>


### [225] [Your Self-Play Algorithm is Secretly an Adversarial Imitator: Understanding LLM Self-Play through the Lens of Imitation Learning](https://arxiv.org/abs/2602.01357)
*Shangzhe Li,Xuchao Zhang,Chetan Bansal,Weitong Zhang*

Main category: cs.LG

TL;DR: 论文提出将自博弈微调与对抗模仿学习联系起来，通过将微调过程建模为模型与正则化隐式奖励玩家之间的min-max博弈，统一了自博弈模仿和偏好对齐，并基于χ²散度变分目标提出了更稳定的新算法。


<details>
  <summary>Details</summary>
Motivation: 自博弈后训练方法已成为微调大语言模型的有效方法，能将弱语言模型转变为强语言模型而无需偏好数据。然而，自博弈微调的理论基础尚未得到充分探索，需要建立理论框架来理解其工作原理。

Method: 将自博弈微调过程形式化为模型与由模型本身参数化的正则化隐式奖励玩家之间的min-max博弈，从博弈论角度分析收敛性。基于χ²散度变分目标提出新的自博弈模仿微调算法，具有有界奖励和更好的稳定性。

Result: 理论分析表明自博弈微调将收敛到均衡点。在多种语言模型微调任务上的实验表明，新算法相比现有自博弈方法取得了持续改进，验证了理论见解。

Conclusion: 通过将自博弈微调与对抗模仿学习联系起来，建立了统一的理论框架，提出的新算法在理论和实践上都表现出优越性，为自博弈微调提供了坚实的理论基础。

Abstract: Self-play post-training methods has emerged as an effective approach for finetuning large language models and turn the weak language model into strong language model without preference data. However, the theoretical foundations for self-play finetuning remain underexplored. In this work, we tackle this by connecting self-play finetuning with adversarial imitation learning by formulating finetuning procedure as a min-max game between the model and a regularized implicit reward player parameterized by the model itself. This perspective unifies self-play imitation and general preference alignment within a common framework. Under this formulation, we present a game-theoretic analysis showing that the self-play finetuning will converge to it's equilibrium. Guided by this theoretical formulation, we propose a new self-play imitation finetuning algorithm based on the $χ^2$-divergence variational objective with bounded rewards and improved stability. Experiments on various of language model finetuning tasks demonstrate consistent improvements over existing self-play methods and validate our theoretical insights.

</details>


### [226] [PaAno: Patch-Based Representation Learning for Time-Series Anomaly Detection](https://arxiv.org/abs/2602.01359)
*Jinju Park,Seokho Kang*

Main category: cs.LG

TL;DR: PaAno是一种轻量级的时间序列异常检测方法，使用1D卷积神经网络提取时间片段表示，通过对比学习训练，在TSB-AD基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer和大模型的时间序列异常检测方法计算成本高、内存占用大，不适合实时和资源受限场景，且性能提升有限。

Method: 从时间序列训练数据中提取短时间片段，使用1D卷积神经网络将每个片段嵌入为向量表示，结合三元组损失和预训练损失进行训练，确保嵌入捕捉信息性时间模式。

Result: 在TSB-AD基准测试中，PaAno在单变量和多变量时间序列异常检测的各种范围级和点级性能指标上均达到最先进性能，显著优于现有方法。

Conclusion: PaAno提供了一种轻量级但有效的时间序列异常检测方法，在保持高性能的同时降低了计算成本和内存使用，适合实时和资源受限场景。

Abstract: Although recent studies on time-series anomaly detection have increasingly adopted ever-larger neural network architectures such as transformers and foundation models, they incur high computational costs and memory usage, making them impractical for real-time and resource-constrained scenarios. Moreover, they often fail to demonstrate significant performance gains over simpler methods under rigorous evaluation protocols. In this study, we propose Patch-based representation learning for time-series Anomaly detection (PaAno), a lightweight yet effective method for fast and efficient time-series anomaly detection. PaAno extracts short temporal patches from time-series training data and uses a 1D convolutional neural network to embed each patch into a vector representation. The model is trained using a combination of triplet loss and pretext loss to ensure the embeddings capture informative temporal patterns from input patches. During inference, the anomaly score at each time step is computed by comparing the embeddings of its surrounding patches to those of normal patches extracted from the training time-series. Evaluated on the TSB-AD benchmark, PaAno achieved state-of-the-art performance, significantly outperforming existing methods, including those based on heavy architectures, on both univariate and multivariate time-series anomaly detection across various range-wise and point-wise performance measures.

</details>


### [227] [When Domains Interact: Asymmetric and Order-Sensitive Cross-Domain Effects in Reinforcement Learning for Reasoning](https://arxiv.org/abs/2602.01365)
*Wang Yang,Shouren Wang,Chaoda Song,Chuang Ma,Xinpeng Li,Nengbo Wang,Kaixiong Zhou,Vipin Chaudhary,Xiaotian Han*

Main category: cs.LG

TL;DR: GRPO在不同领域训练顺序策略下的行为分析：发现单领域泛化高度不对称、跨领域交互高度顺序依赖、多领域训练无单一最优策略


<details>
  <summary>Details</summary>
Motivation: 尽管GRPO已成为提升大语言模型推理能力的关键技术，但其在不同领域排序策略下的行为尚未被充分理解，特别是顺序训练与混合领域训练的影响缺乏系统研究

Method: 对数学、科学、逻辑和谜题推理任务进行训练顺序效应的系统分析，比较顺序训练（一次一个领域）与混合领域训练（多个领域同时）的不同策略

Result: 发现三个关键结果：1）单领域泛化高度不对称，其他领域训练能提升数学推理约25%准确率，但对逻辑和谜题几乎没有转移效果；2）跨领域交互高度顺序依赖，数学→科学顺序获得83%/41%准确率，而科学→数学顺序降至77%/25%；3）多领域训练无单一最优策略，顺序训练有利于数学（达84%），混合训练有利于科学和逻辑，不良排序会导致大性能差距（从70%到56%）

Conclusion: GRPO在多领域设置下表现出明显的不对称性、顺序敏感性和策略依赖性，强调了领域感知和顺序感知训练设计的必要性

Abstract: Group Relative Policy Optimization (GRPO) has become a key technique for improving reasoning abilities in large language models, yet its behavior under different domain sequencing strategies is poorly understood. In particular, the impact of sequential (one domain at a time) versus mixed-domain (multiple domain at a time) training in GRPO has not been systematically studied. We provide the first systematic analysis of training-order effects across math, science, logic, and puzzle reasoning tasks. We found (1) single-domain generalization is highly asymmetric: training on other domains improves math reasoning by approximately 25\% accuracy, while yielding negligible transfer to logic and puzzle; (2) cross-domain interactions are highly order-dependent: training in the order math$\rightarrow$science achieves 83\% / 41\% accuracy on math / science, while reversing the order to science$\rightarrow$math degrades performance to 77\% / 25\%; (3) no single strategy is universally optimal in multi-domain training: sequential training favors math (up to 84\%), mixed training favors science and logic, and poor ordering can incur large performance gaps (from 70\% to 56\%). Overall, our findings demonstrate that GRPO under multi-domain settings exhibits pronounced asymmetry, order sensitivity, and strategy dependence, highlighting the necessity of domain-aware and order-aware training design.

</details>


### [228] [Deep Variational Contrastive Learning for Joint Risk Stratification and Time-to-Event Estimation](https://arxiv.org/abs/2602.01367)
*Pinar Erbil,Alberto Archetti,Eugenio Lomurno,Matteo Matteucci*

Main category: cs.LG

TL;DR: CONVERSE是一个结合变分自编码器和对比学习的深度生存分析模型，在保持高预测性能的同时实现可解释的风险分层。


<details>
  <summary>Details</summary>
Motivation: 临床决策需要生存分析来估计时间到事件结果、分层患者风险并指导治疗规划。深度学习在该领域具有强大预测能力，但存在性能与可解释性的根本权衡：神经网络准确性高但黑盒特性限制临床采用，而基于深度聚类的方法虽然可解释但通常牺牲预测能力。

Method: CONVERSE结合变分自编码器与对比学习，使用变分嵌入和多种簇内簇间对比损失。采用自步学习从易到难逐步纳入样本以提高训练稳定性。模型支持簇特定生存头，实现准确的集成预测。

Result: 在四个基准数据集上的综合评估表明，CONVERSE相比现有深度生存方法实现了竞争性或更优的性能，同时保持了有意义的患者分层。

Conclusion: CONVERSE通过统一变分自编码器和对比学习，成功弥合了生存分析中性能与可解释性之间的差距，为临床决策提供了既准确又可解释的风险分层工具。

Abstract: Survival analysis is essential for clinical decision-making, as it allows practitioners to estimate time-to-event outcomes, stratify patient risk profiles, and guide treatment planning. Deep learning has revolutionized this field with unprecedented predictive capabilities but faces a fundamental trade-off between performance and interpretability. While neural networks achieve high accuracy, their black-box nature limits clinical adoption. Conversely, deep clustering-based methods that stratify patients into interpretable risk groups typically sacrifice predictive power. We propose CONVERSE (CONtrastive Variational Ensemble for Risk Stratification and Estimation), a deep survival model that bridges this gap by unifying variational autoencoders with contrastive learning for interpretable risk stratification. CONVERSE combines variational embeddings with multiple intra- and inter-cluster contrastive losses. Self-paced learning progressively incorporates samples from easy to hard, improving training stability. The model supports cluster-specific survival heads, enabling accurate ensemble predictions. Comprehensive evaluation on four benchmark datasets demonstrates that CONVERSE achieves competitive or superior performance compared to existing deep survival methods, while maintaining meaningful patient stratification.

</details>


### [229] [An Odd Estimator for Shapley Values](https://arxiv.org/abs/2602.01399)
*Fabian Fumagalli,Landon Butler,Justin Singh Kang,Kannan Ramchandran,R. Teal Witter*

Main category: cs.LG

TL;DR: 论文提出OddSHAP，一种新颖的Shapley值估计器，通过证明Shapley值仅依赖于集合函数的奇分量，并利用配对采样正交化回归目标来过滤无关的偶分量，从而显著提升估计精度。


<details>
  <summary>Details</summary>
Motivation: Shapley值是机器学习中普遍使用的归因框架，但其精确计算通常不可行，需要高效的近似方法。虽然最有效的估计器利用配对采样启发式来减少估计误差，但这种改进的理论机制一直不明确。

Method: 1) 证明Shapley值仅依赖于集合函数的奇分量；2) 提出配对采样通过正交化回归目标来过滤偶分量；3) 提出OddSHAP估计器，在奇子空间上进行多项式回归；4) 利用傅里叶基隔离奇子空间，并使用代理模型识别高影响力交互作用。

Result: 通过广泛的基准评估，OddSHAP实现了最先进的估计精度，克服了高阶近似的组合爆炸问题。

Conclusion: 该工作为配对采样提供了理论基础，证明了Shapley值与集合函数奇分量的关系，并提出了高效的OddSHAP估计器，显著提升了Shapley值估计的准确性。

Abstract: The Shapley value is a ubiquitous framework for attribution in machine learning, encompassing feature importance, data valuation, and causal inference. However, its exact computation is generally intractable, necessitating efficient approximation methods. While the most effective and popular estimators leverage the paired sampling heuristic to reduce estimation error, the theoretical mechanism driving this improvement has remained opaque. In this work, we provide an elegant and fundamental justification for paired sampling: we prove that the Shapley value depends exclusively on the odd component of the set function, and that paired sampling orthogonalizes the regression objective to filter out the irrelevant even component. Leveraging this insight, we propose OddSHAP, a novel consistent estimator that performs polynomial regression solely on the odd subspace. By utilizing the Fourier basis to isolate this subspace and employing a proxy model to identify high-impact interactions, OddSHAP overcomes the combinatorial explosion of higher-order approximations. Through an extensive benchmark evaluation, we find that OddSHAP achieves state-of-the-art estimation accuracy.

</details>


### [230] [SNIP: An Adaptive Mixed Precision Framework for Subbyte Large Language Model Training](https://arxiv.org/abs/2602.01410)
*Yunjie Pan,Yongyi Yang,Hanmei Yang,Scott Mahlke*

Main category: cs.LG

TL;DR: SNIP是一个用于大语言模型预训练的细粒度自适应混合精度训练框架，支持亚字节精度，通过周期性收集统计信息并定义两个关键指标来指导整数线性规划问题，系统优化层间精度，在保持模型质量的同时显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前混合精度训练方法要么对所有GEMM操作应用统一精度，要么依赖启发式方法，这些方法在训练过程中无法泛化，导致次优收敛和不稳定性。需要一种能够有效支持亚字节精度并保持模型质量的训练框架。

Method: SNIP周期性收集激活、梯度和优化器状态的统计信息，定义前向传播中的损失发散和后向传播中的权重发散两个关键指标。这些指标指导一个整数线性规划问题，系统优化层间精度以最小化整体质量损失，同时满足效率目标。

Result: 在1B、3B、7B和70B Llama-like模型上的实验表明，SNIP始终优于现有基线方法，在保持不同模型大小和训练阶段模型质量的同时，将FLOPs减少高达80%，且计算开销最小。

Conclusion: SNIP提供了一个有效的细粒度自适应混合精度训练框架，能够在大语言模型预训练中显著提高效率，同时保持模型质量，解决了当前混合精度训练方法的局限性。

Abstract: Training large language models (LLMs) efficiently while preserving model quality poses significant challenges, particularly with subbyte precision supported by state-of-the-art GPUs. Current mixed-precision training approaches either apply uniform precision to all GEMM operations or rely on heuristic-based methods that fail to generalize during training, leading to suboptimal convergence and instability. To address these challenges, this paper introduces SNIP, a fine-grained adaptive mixed-precision training framework for LLM pretraining that supports subbyte precision. SNIP periodically collects statistics on activations, gradients, and optimizer states to assess the precision loss impact on model quality. We define two key metrics: loss divergence in the forward pass, caused by quantization-induced increases in training loss, and weight divergence in the backward pass, which measures error propagation through gradients affecting model updates. These metrics guide an Integer Linear Programming (ILP) problem that systematically optimizes layerwise precision to minimize overall quality loss while meeting efficiency targets. Experiments on 1B, 3B, 7B and 70B Llama-like models demonstrate that SNIP consistently outperforms existing baselines, reducing FLOPs by up to 80% while preserving model quality across different model sizes and training phases with minimal computational overhead.

</details>


### [231] [Semi-supervised CAPP Transformer Learning via Pseudo-labeling](https://arxiv.org/abs/2602.01419)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb,Emmanuel Stathatos,Panorios Benardos,George-Christopher Vosniakos*

Main category: cs.LG

TL;DR: 提出半监督学习方法改进基于Transformer的CAPP模型，无需人工标注，利用oracle筛选正确预测进行一次性重训练，在小规模数据集上实现准确率提升


<details>
  <summary>Details</summary>
Motivation: 工业中CAPP数据集有限，导致模型泛化能力不足，需要解决数据稀缺环境下的模型性能提升问题

Method: 使用半监督学习方法：1) 在可用Transformer行为数据上训练oracle模型；2) 用oracle筛选未见零件中的正确预测；3) 利用筛选数据进行一次性的模型重训练

Result: 在全数据分布的小规模数据集实验中，相比基线方法获得一致的准确率提升，证明该方法在数据稀缺制造环境中的有效性

Conclusion: 提出的半监督学习方法能有效提升CAPP Transformer模型性能，无需人工标注，适用于工业数据稀缺场景

Abstract: High-level Computer-Aided Process Planning (CAPP) generates manufacturing process plans from part specifications. It suffers from limited dataset availability in industry, reducing model generalization. We propose a semi-supervised learning approach to improve transformer-based CAPP transformer models without manual labeling. An oracle, trained on available transformer behaviour data, filters correct predictions from unseen parts, which are then used for one-shot retraining. Experiments on small-scale datasets with simulated ground truth across the full data distribution show consistent accuracy gains over baselines, demonstrating the method's effectiveness in data-scarce manufacturing environments.

</details>


### [232] [Improve the Trade-off Between Watermark Strength and Speculative Sampling Efficiency for Language Models](https://arxiv.org/abs/2602.01428)
*Weiqing He,Xiang Li,Li Shen,Weijie Su,Qi Long*

Main category: cs.LG

TL;DR: 提出一种新方法，在保持推测采样效率的同时实现最大水印强度，解决了水印与推测采样之间的效率冲突


<details>
  <summary>Details</summary>
Motivation: 水印技术用于追踪大语言模型输出来源，但在实际部署中受到推理效率低下的阻碍。推测采样能加速推理，但现有研究表明水印强度与推测采样接受率存在根本性冲突

Method: 引入量化水印强度度量，将权衡问题建模为约束优化问题，推导出帕累托曲线，并提出在草稿令牌接受中注入伪随机性的原则性机制

Result: 实验表明该方法能在不牺牲效率的情况下提高可检测性，实现了水印强度与推测采样效率的协同优化

Conclusion: 揭示了推测采样与水印之间的统一原则，为两者的高效实用部署铺平了道路

Abstract: Watermarking is a principled approach for tracing the provenance of large language model (LLM) outputs, but its deployment in practice is hindered by inference inefficiency. Speculative sampling accelerates inference, with efficiency improving as the acceptance rate between draft and target models increases. Yet recent work reveals a fundamental trade-off: higher watermark strength reduces acceptance, preventing their simultaneous achievement. We revisit this trade-off and show it is not absolute. We introduce a quantitative measure of watermark strength that governs statistical detectability and is maximized when tokens are deterministic functions of pseudorandom numbers. Using this measure, we fully characterize the trade-off as a constrained optimization problem and derive explicit Pareto curves for two existing watermarking schemes. Finally, we introduce a principled mechanism that injects pseudorandomness into draft-token acceptance, ensuring maximal watermark strength while maintaining speculative sampling efficiency. Experiments further show that this approach improves detectability without sacrificing efficiency. Our findings uncover a principle that unites speculative sampling and watermarking, paving the way for their efficient and practical deployment.

</details>


### [233] [DCD: Decomposition-based Causal Discovery from Autocorrelated and Non-Stationary Temporal Data](https://arxiv.org/abs/2602.01433)
*Muhammad Hasan Ferdous,Md Osman Gani*

Main category: cs.LG

TL;DR: 提出基于分解的因果发现框架，将时间序列分解为趋势、季节和残差分量，分别进行因果分析，最后整合为多尺度因果结构


<details>
  <summary>Details</summary>
Motivation: 多元时间序列中的长期趋势、季节模式和短期波动使得非平稳和自相关条件下的因果推断变得复杂。现有方法直接在原始观测上操作，容易产生虚假边和错误归因的时间依赖关系

Method: 将每个时间序列分解为趋势、季节和残差三个分量，分别进行因果分析：趋势分量使用平稳性检验，季节分量使用基于核的依赖度量，残差分量使用基于约束的因果发现方法，最后将分量级图整合为统一的多尺度因果结构

Result: 在广泛的合成基准测试和真实世界气候数据中，该框架比现有最先进基线方法更准确地恢复真实因果结构，特别是在强非平稳性和时间自相关条件下表现更好

Conclusion: 基于分解的因果发现框架能够分离长期和短期因果效应，减少虚假关联，提高可解释性，在非平稳和自相关时间序列中具有优越性能

Abstract: Multivariate time series in domains such as finance, climate science, and healthcare often exhibit long-term trends, seasonal patterns, and short-term fluctuations, complicating causal inference under non-stationarity and autocorrelation. Existing causal discovery methods typically operate on raw observations, making them vulnerable to spurious edges and misattributed temporal dependencies. We introduce a decomposition-based causal discovery framework that separates each time series into trend, seasonal, and residual components and performs component-specific causal analysis. Trend components are assessed using stationarity tests, seasonal components using kernel-based dependence measures, and residual components using constraint-based causal discovery. The resulting component-level graphs are integrated into a unified multi-scale causal structure. This approach isolates long- and short-range causal effects, reduces spurious associations, and improves interpretability. Across extensive synthetic benchmarks and real-world climate data, our framework more accurately recovers ground-truth causal structure than state-of-the-art baselines, particularly under strong non-stationarity and temporal autocorrelation.

</details>


### [234] [Phase Transitions for Feature Learning in Neural Networks](https://arxiv.org/abs/2602.01434)
*Andrea Montanari,Zihao Wang*

Main category: cs.LG

TL;DR: 论文研究两层神经网络在多索引模型下的梯度下降动态，推导出样本复杂度阈值δ_NN，该阈值由Hessian矩阵谱的相变决定，揭示了特征学习的动态过程。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络如何通过梯度下降学习低维表示的特征，特别是在多索引模型设置下，理解特征学习的动态过程和样本复杂度阈值。

Method: 在比例渐近框架下（n,d→∞, n/d→δ），研究两层神经网络的梯度下降动态，其中潜在空间维度k和隐藏神经元数m固定。通过分析训练过程中的梯度动态和Hessian矩阵谱的相变来推导阈值δ_NN。

Result: 推导出两层神经网络的特征学习阈值δ_NN，该阈值由训练过程中Hessian矩阵谱的相变决定。训练过程分为两个阶段：首先学习梯度方向，然后Hessian的负方向主导动态。

Conclusion: δ_NN阈值表征了神经网络在多索引模型中特征学习的样本复杂度边界，为研究网络架构和训练算法对学习动态的影响提供了理论基础。

Abstract: According to a popular viewpoint, neural networks learn from data by first identifying low-dimensional representations, and subsequently fitting the best model in this space. Recent works provide a formalization of this phenomenon when learning multi-index models. In this setting, we are given $n$ i.i.d. pairs $({\boldsymbol x}_i,y_i)$, where the covariate vectors ${\boldsymbol x}_i\in\mathbb{R}^d$ are isotropic, and responses $y_i$ only depend on ${\boldsymbol x}_i$ through a $k$-dimensional projection ${\boldsymbol Θ}_*^{\sf T}{\boldsymbol x}_i$. Feature learning amounts to learning the latent space spanned by ${\boldsymbol Θ}_*$.
  In this context, we study the gradient descent dynamics of two-layer neural networks under the proportional asymptotics $n,d\to\infty$, $n/d\toδ$, while the dimension of the latent space $k$ and the number of hidden neurons $m$ are kept fixed. Earlier work establishes that feature learning via polynomial-time algorithms is possible if $δ> δ_{\text{alg}}$, for $δ_{\text{alg}}$ a threshold depending on the data distribution, and is impossible (within a certain class of algorithms) below $δ_{\text{alg}}$. Here we derive an analogous threshold $δ_{\text{NN}}$ for two-layer networks. Our characterization of $δ_{\text{NN}}$ opens the way to study the dependence of learning dynamics on the network architecture and training algorithm.
  The threshold $δ_{\text{NN}}$ is determined by the following scenario. Training first visits points for which the gradient of the empirical risk is large and learns the directions spanned by these gradients. Then the gradient becomes smaller and the dynamics becomes dominated by negative directions of the Hessian. The threshold $δ_{\text{NN}}$ corresponds to a phase transition in the spectrum of the Hessian in this second phase.

</details>


### [235] [Theoretical Analysis of Measure Consistency Regularization for Partially Observed Data](https://arxiv.org/abs/2602.01437)
*Yinsong Wang,Shahin Shahrampour*

Main category: cs.LG

TL;DR: 本文从神经网络距离的角度，为测量一致性正则化（MCR）提供了理论分析，解释了其在部分可观测数据下提升插补质量的原因，并提出基于对偶间隙的早停训练协议。


<details>
  <summary>Details</summary>
Motivation: 尽管测量一致性正则化（MCR）在图像修复、数据插补和半监督学习等应用中取得了经验成功，但其理论基础仍然有限。本文旨在填补这一空白，从理论上理解MCR为何、何时以及如何在部分可观测性下提升插补质量。

Method: 1. 从神经网络距离的角度对MCR进行理论分析；2. 识别MCR带来泛化优势的关键项；3. 扩展到不完美训练机制；4. 提出基于对偶间隙监控的早停训练协议；5. 通过实证研究和真实世界数据模拟验证理论主张。

Result: 理论分析揭示了MCR泛化优势的来源，并表明这种优势并非总是保证的。提出的早停条件能有效保留泛化效益。实证证据支持了理论主张，并展示了MCR在不同数据源和模型架构下的通用性。

Conclusion: 本文为MCR提供了坚实的理论基础，解释了其在部分可观测数据下的工作机制，并提出了实用的训练协议来确保其泛化优势。研究展示了MCR在不同应用场景中的通用性和有效性。

Abstract: The problem of corrupted data, missing features, or missing modalities continues to plague the modern machine learning landscape. To address this issue, a class of regularization methods that enforce consistency between imputed and fully observed data has emerged as a promising approach for improving model generalization, particularly in partially observed settings. We refer to this class of methods as Measure Consistency Regularization (MCR). Despite its empirical success in various applications, such as image inpainting, data imputation and semi-supervised learning, a fundamental understanding of the theoretical underpinnings of MCR remains limited. This paper bridges this gap by offering theoretical insights into why, when, and how MCR enhances imputation quality under partial observability, viewed through the lens of neural network distance.
  Our theoretical analysis identifies the term responsible for MCR's generalization advantage and extends to the imperfect training regime, demonstrating that this advantage is not always guaranteed. Guided by these insights, we propose a novel training protocol that monitors the duality gap to determine an early stopping point that preserves the generalization benefit. We then provide detailed empirical evidence to support our theoretical claims and to show the effectiveness and accuracy of our proposed stopping condition. We further provide a set of real-world data simulations to show the versatility of MCR under different model architectures designed for different data sources.

</details>


### [236] [TQL: Scaling Q-Functions with Transformers by Preventing Attention Collapse](https://arxiv.org/abs/2602.01439)
*Perry Dong,Kuo-Han Hung,Alexander Swerdlow,Dorsa Sadigh,Chelsea Finn*

Main category: cs.LG

TL;DR: 提出Transformer Q-Learning (TQL)方法，通过控制注意力分数熵来稳定训练，使transformer能够有效扩展用于强化学习的价值函数学习。


<details>
  <summary>Details</summary>
Motivation: 尽管规模扩展在机器学习中取得显著进展，但强化学习方法仍主要使用小型价值函数。直接将transformer等可扩展架构用于价值函数学习会导致训练不稳定和性能下降，需要解决这一问题。

Method: 通过实证分析发现注意力分数崩溃是主要失败模式，提出通过控制注意力分数熵来防止崩溃。在此基础上开发了Transformer Q-Learning (TQL)方法，稳定训练并实现transformer在价值函数学习中的有效扩展。

Result: TQL方法在从最小到最大网络规模扩展时，性能提升高达43%，而先前方法在扩展时会出现性能退化。

Conclusion: 通过控制注意力分数熵可以有效防止transformer在价值函数学习中的扩展失败，TQL方法成功解锁了transformer在强化学习价值函数学习中的扩展潜力。

Abstract: Despite scale driving substantial recent advancements in machine learning, reinforcement learning (RL) methods still primarily use small value functions. Naively scaling value functions -- including with a transformer architecture, which is known to be highly scalable -- often results in learning instability and worse performance. In this work, we ask what prevents transformers from scaling effectively for value functions? Through empirical analysis, we identify the critical failure mode in this scaling: attention scores collapse as capacity increases. Our key insight is that we can effectively prevent this collapse and stabilize training by controlling the entropy of the attention scores, thereby enabling the use of larger models. To this end, we propose Transformer Q-Learning (TQL), a method that unlocks the scaling potential of transformers in learning value functions in RL. Our approach yields up to a 43% improvement in performance when scaling from the smallest to the largest network sizes, while prior methods suffer from performance degradation.

</details>


### [237] [The Gradient-Causal Gap: Why Gradient Importance Fails on Complex Tasks](https://arxiv.org/abs/2602.01442)
*Donald Ye*

Main category: cs.LG

TL;DR: 研究发现梯度大小与因果重要性在简单任务中相关，但在复杂任务中关系崩溃甚至反转，导致基于梯度的剪枝无法可靠保留模型能力


<details>
  <summary>Details</summary>
Motivation: 探索神经网络中梯度大小与组件重要性之间的关系，揭示基于梯度的剪枝方法的局限性

Method: 在Transformer上训练算法任务，形式化梯度-因果鸿沟，通过相关性分析和剪枝实验验证梯度大小与因果重要性的关系

Result: 简单任务中梯度与因果重要性正相关（ρ=0.73），复杂任务中关系崩溃（ρ=0.32）甚至反转（ρ=-0.11）；移除低梯度"隐藏英雄"严重损害OOD准确率（-32%），移除高梯度"梯度膨胀"结果不可预测

Conclusion: 梯度大小不能可靠指示组件重要性，基于梯度的剪枝方法无法可靠保留模型能力，因为梯度与因果重要性的关系随任务复杂度变化且不可预测

Abstract: Removing ''important'' high-gradient components from a neural network can improve generalization, while removing unimportant'' low-gradient components can destroy it. We demonstrate this paradox by formalizing the \textit{Gradient-Causal Gap} in Transformers trained on algorithmic tasks. While gradient magnitude and causal importance align on simple tasks ($ρ=0.73$ for reversal), this relationship collapses as task complexity increases ($ρ=0.32$ for sorting), sometimes becoming inverted ($ρ=-0.11$). Pruning experiments reveal that gradient magnitude is not merely inaccurate but \textit{unpredictably} so. Removing low-gradient ''Hidden Heroes'' consistently devastates OOD accuracy ($-32\%$). Removing high-gradient ''Gradient Bloats'' is a coin flip: harmless in most seeds (indicating optimization noise), catastrophic in others (indicating overfitting circuits). This unpredictability means gradient-based pruning cannot reliably preserve model capabilities.

</details>


### [238] [A Meta-Knowledge-Augmented LLM Framework for Hyperparameter Optimization in Time-Series Forecasting](https://arxiv.org/abs/2602.01445)
*Ons Saadallah,Mátyás andó,Tamás Gábor Orosz*

Main category: cs.LG

TL;DR: LLM-AutoOpt：结合贝叶斯优化与LLM推理的混合超参数优化框架，用于时间序列预测，提高性能与可解释性


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化在超参数调优中计算成本高、可解释性差，特别是对于时间序列预测任务。LLMs的发展为整合结构化先验知识和推理提供了新机会。

Method: 提出LLM-AutoOpt混合框架，将数据集元特征、模型描述、历史优化结果和目标目标编码为结构化元知识，通过LLM提示进行上下文推理，使用BO初始化搜索缓解冷启动问题。

Result: 在多变量时间序列预测基准测试中，LLM-AutoOpt相比纯BO和无元知识的LLM基线，实现了更好的预测性能和更可解释的优化行为。

Conclusion: LLM-AutoOpt通过结合BO与LLM推理，为超参数优化提供了更有效和可解释的解决方案，特别适用于时间序列预测任务。

Abstract: Hyperparameter optimization (HPO) plays a central role in the performance of deep learning models, yet remains computationally expensive and difficult to interpret, particularly for time-series forecasting. While Bayesian Optimization (BO) is a standard approach, it typically treats tuning tasks independently and provides limited insight into its decisions. Recent advances in large language models (LLMs) offer new opportunities to incorporate structured prior knowledge and reasoning into optimization pipelines. We introduce LLM-AutoOpt, a hybrid HPO framework that combines BO with LLM-based contextual reasoning. The framework encodes dataset meta-features, model descriptions, historical optimization outcomes, and target objectives as structured meta-knowledge within LLM prompts, using BO to initialize the search and mitigate cold-start effects. This design enables context-aware and stable hyperparameter refinement while exposing the reasoning behind optimization decisions. Experiments on a multivariate time series forecasting benchmark demonstrate that LLM-AutoOpt achieves improved predictive performance and more interpretable optimization behavior compared to BO and LLM baselines without meta-knowledge.

</details>


### [239] [Provable Cooperative Multi-Agent Exploration for Reward-Free MDPs](https://arxiv.org/abs/2602.01453)
*Idan Barnea,Orin Levy,Yishay Mansour*

Main category: cs.LG

TL;DR: 多智能体强化学习在无奖励探索下的研究，关注学习阶段数与智能体数量之间的权衡关系，发现由时间步长H控制的尖锐转变


<details>
  <summary>Details</summary>
Motivation: 研究多智能体强化学习在无奖励探索设置下的合作问题，多个智能体共同探索未知MDP以学习其动态（不观察奖励），重点关注学习阶段数与智能体数量之间的权衡关系

Method: 采用分阶段学习框架，每个学习阶段中多个智能体独立与环境交互，每个智能体被分配策略并执行，观察结果轨迹。研究计算高效算法，当学习阶段数等于H时，使用多项式数量的智能体获得ε近似动态

Result: 发现由时间步长H控制的尖锐转变：当学习阶段数等于H时，算法仅需Õ(S⁶H⁶A/ε²)个智能体即可获得ε近似动态；当阶段数ρ<H时，任何算法至少需要A^{H/ρ}个智能体才能达到常数精度

Conclusion: 要限制智能体数量为多项式级别，必须要有大约H个学习阶段，这揭示了多智能体无奖励探索中阶段数与智能体数量之间的基本权衡

Abstract: We study cooperative multi-agent reinforcement learning in the setting of reward-free exploration, where multiple agents jointly explore an unknown MDP in order to learn its dynamics (without observing rewards). We focus on a tabular finite-horizon MDP and adopt a phased learning framework. In each learning phase, multiple agents independently interact with the environment. More specifically, in each learning phase, each agent is assigned a policy, executes it, and observes the resulting trajectory. Our primary goal is to characterize the tradeoff between the number of learning phases and the number of agents, especially when the number of learning phases is small.
  Our results identify a sharp transition governed by the horizon $H$. When the number of learning phases equals $H$, we present a computationally efficient algorithm that uses only $\tilde{O}(S^6 H^6 A / ε^2)$ agents to obtain an $ε$ approximation of the dynamics (i.e., yields an $ε$-optimal policy for any reward function). We complement our algorithm with a lower bound showing that any algorithm restricted to $ρ< H$ phases requires at least $A^{H/ρ}$ agents to achieve constant accuracy. Thus, we show that it is essential to have an order of $H$ learning phases if we limit the number of agents to be polynomial.

</details>


### [240] [Modeling Topological Impact on Node Attribute Distributions in Attributed Graphs](https://arxiv.org/abs/2602.01454)
*Amirreza Shiralinasab Langari,Leila Yeganeh,Kim Khoa Nguyen*

Main category: cs.LG

TL;DR: 本文提出一种代数方法，将图拓扑与节点属性分布结合，形成拓扑影响分布，并建立范畴框架量化节点对拓扑的感知。


<details>
  <summary>Details</summary>
Motivation: 研究图拓扑如何影响节点属性的分布，将拓扑和属性视为结构不同但相互作用的组件，提供新的分析视角。

Method: 1. 开发范畴框架形式化节点对图拓扑的感知；2. 量化这种观点并与节点属性分布结合；3. 引入代数方法结合图拓扑和节点属性概率分布；4. 使用简单测试模型ID和无监督图异常检测作为验证任务。

Result: 建立了拓扑影响分布的理论框架，证明了在完全图（拓扑无信息结构）上能恢复原始属性分布的充分性条件，并将拓扑条件分布解释为后验概率P(·|v)和P(·|G)的近似。

Conclusion: 提出了一种将图拓扑与节点属性分布相结合的创新代数方法，为理解拓扑如何影响属性分布提供了理论基础，并通过异常检测任务验证了方法的有效性。

Abstract: We investigate how the topology of attributed graphs influences the distribution of node attributes. This work offers a novel perspective by treating topology and attributes as structurally distinct but interacting components. We introduce an algebraic approach that combines a graph's topology with the probability distribution of node attributes, resulting in topology-influenced distributions. First, we develop a categorical framework to formalize how a node perceives the graph's topology. We then quantify this point of view and integrate it with the distribution of node attributes to capture topological effects. We interpret these topology-conditioned distributions as approximations of the posteriors $P(\cdot \mid v)$ and $P(\cdot \mid \mathcal{G})$.
  We further establish a principled sufficiency condition by showing that, on complete graphs, where topology carries no informative structure, our construction recovers the original attribute distribution. To evaluate our approach, we introduce an intentionally simple testbed model, $\textbf{ID}$, and use unsupervised graph anomaly detection as a probing task.

</details>


### [241] [Rectified LpJEPA: Joint-Embedding Predictive Architectures with Sparse and Maximum-Entropy Representations](https://arxiv.org/abs/2602.01456)
*Yilun Kuang,Yash Dagade,Tim G. J. Rudner,Randall Balestriero,Yann LeCun*

Main category: cs.LG

TL;DR: 提出RDMReg正则化方法，将JEPA表示对齐到Rectified Generalized Gaussian分布，实现稀疏控制，优于现有高斯分布正则化方法。


<details>
  <summary>Details</summary>
Motivation: 现有JEPA方法使用各向同性高斯分布正则化表示，但这种方法偏好密集表示，无法捕捉高效表示中的稀疏性这一关键特性。

Method: 引入Rectified Distribution Matching Regularization (RDMReg)，这是一种切片双样本分布匹配损失，将表示对齐到Rectified Generalized Gaussian (RGG)分布。RGG通过整流实现期望ℓ0范数的显式控制，同时在期望ℓp范数约束下保持最大熵。

Result: Rectified LpJEPA学习到稀疏、非负的表示，在稀疏性与性能之间取得良好权衡，在图像分类基准测试中表现出竞争力的下游性能。

Conclusion: RDMReg能有效强制稀疏性同时保留任务相关信息，Rectified LpJEPA严格推广了先前基于高斯的JEPA方法。

Abstract: Joint-Embedding Predictive Architectures (JEPA) learn view-invariant representations and admit projection-based distribution matching for collapse prevention. Existing approaches regularize representations towards isotropic Gaussian distributions, but inherently favor dense representations and fail to capture the key property of sparsity observed in efficient representations. We introduce Rectified Distribution Matching Regularization (RDMReg), a sliced two-sample distribution-matching loss that aligns representations to a Rectified Generalized Gaussian (RGG) distribution. RGG enables explicit control over expected $\ell_0$ norm through rectification, while preserving maximum-entropy up to rescaling under expected $\ell_p$ norm constraints. Equipping JEPAs with RDMReg yields Rectified LpJEPA, which strictly generalizes prior Gaussian-based JEPAs. Empirically, Rectified LpJEPA learns sparse, non-negative representations with favorable sparsity-performance trade-offs and competitive downstream performance on image classification benchmarks, demonstrating that RDMReg effectively enforces sparsity while preserving task-relevant information.

</details>


### [242] [A Statistical Theory of Gated Attention through the Lens of Hierarchical Mixture of Experts](https://arxiv.org/abs/2602.01468)
*Viet Nguyen,Tuan Minh Pham,Thinh Cao,Tan Dinh,Huy Nguyen,Nhat Ho,Alessandro Rinaldo*

Main category: cs.LG

TL;DR: 门控注意力比多头自注意力更样本高效，通过专家混合框架证明前者仅需多项式样本而后者需指数样本达到相同估计误差。


<details>
  <summary>Details</summary>
Motivation: 门控注意力模型在实践中表现出色，能增强标准注意力的表达能力并消除注意力汇聚现象，但其理论优势尚不明确，需要填补这一理论空白。

Method: 将门控注意力矩阵和多头自注意力矩阵的每个条目重新表述为分层专家混合，将学习问题转化为专家估计问题，从理论上分析样本效率。

Result: 门控注意力比多头自注意力更样本高效：门控注意力仅需多项式数量数据点来估计专家，而多头自注意力需要指数数量数据点才能达到相同的估计误差。

Conclusion: 门控注意力在理论上是更优的架构选择，其样本效率优势解释了实践中观察到的性能提升，并为门控位置选择（输出或值映射处）提供了理论依据。

Abstract: Self-attention has greatly contributed to the success of the widely used Transformer architecture by enabling learning from data with long-range dependencies. In an effort to improve performance, a gated attention model that leverages a gating mechanism within the multi-head self-attention has recently been proposed as a promising alternative. Gated attention has been empirically demonstrated to increase the expressiveness of low-rank mapping in standard attention and even to eliminate the attention sink phenomenon. Despite its efficacy, a clear theoretical understanding of gated attention's benefits remains lacking in the literature. To close this gap, we rigorously show that each entry in a gated attention matrix or a multi-head self-attention matrix can be written as a hierarchical mixture of experts. By recasting learning as an expert estimation problem, we demonstrate that gated attention is more sample-efficient than multi-head self-attention. In particular, while the former needs only a polynomial number of data points to estimate an expert, the latter requires exponentially many data points to achieve the same estimation error. Furthermore, our analysis also provides a theoretical justification for why gated attention yields higher performance when a gate is placed at the output of the scaled dot product attention or the value map rather than at other positions in the multi-head self-attention architecture.

</details>


### [243] [P-EAGLE: Parallel-Drafting EAGLE with Scalable Training](https://arxiv.org/abs/2602.01469)
*Mude Hui,Xin Huang,Jaime Campos Salas,Yue Sun,Nathan Pemberton,Xiang Song,Ashish Khetan,George Karypis*

Main category: cs.LG

TL;DR: P-EAGLE将EAGLE从自回归转换为并行多token预测，通过可学习的共享隐藏状态实现，解决了长上下文训练的计算复杂度问题，在多个大模型上实现了1.10-1.36倍的加速。


<details>
  <summary>Details</summary>
Motivation: 推理大语言模型产生更长输出，需要训练在扩展序列上的推测解码草稿器。并行草稿（每次前向传播预测多个token）相比顺序生成有延迟优势，但训练复杂度随序列长度和并行位置的乘积呈二次方增长，使得长上下文训练不切实际。

Method: 提出P-EAGLE，通过可学习的共享隐藏状态将EAGLE从自回归转换为并行多token预测。为了扩展到长上下文训练，开发了包含注意力掩码预计算和序列分区技术的框架，支持在单个序列内进行梯度累积的并行预测训练。

Result: 在vLLM中实现P-EAGLE，在GPT-OSS 120B、20B和Qwen3-Coder 30B模型上相比自回归EAGLE-3实现了1.10-1.36倍的加速。

Conclusion: P-EAGLE通过创新的并行预测架构和高效训练框架，成功解决了长上下文并行草稿训练的计算挑战，显著提升了推理速度。

Abstract: Reasoning LLMs produce longer outputs, requiring speculative decoding drafters trained on extended sequences. Parallel drafting - predicting multiple tokens per forward pass - offers latency benefits over sequential generation, but training complexity scales quadratically with the product of sequence length and parallel positions, rendering long-context training impractical. We present P(arallel)-EAGLE, which transforms EAGLE from autoregressive to parallel multi-token prediction via a learnable shared hidden state. To scale training to long contexts, we develop a framework featuring attention mask pre-computation and sequence partitioning techniques, enabling gradient accumulation within individual sequences for parallel-prediction training. We implement P-EAGLE in vLLM and demonstrate speedups of 1.10-1.36x over autoregressive EAGLE-3 across GPT-OSS 120B, 20B, and Qwen3-Coder 30B.

</details>


### [244] [Rod Flow: A Continuous-Time Model for Gradient Descent at the Edge of Stability](https://arxiv.org/abs/2602.01480)
*Eric Regis,Sinho Chewi*

Main category: cs.LG

TL;DR: 本文提出Rod Flow作为梯度下降动力学的新ODE近似方法，相比之前的Central Flow具有更理论化的推导、更好的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 理解梯度下降在非凸景观上的训练动态是一个挑战。Cohen等人(2021)发现的"稳定性边缘"现象表明，大学习率的梯度下降会偏离梯度流。虽然Cohen等人(2025)提出的Central Flow提供了有效的ODE近似，但需要更理论化、更准确且计算高效的替代方法。

Method: 提出Rod Flow方法，将梯度下降迭代视为一维扩展对象——"杆"（rod），基于物理图像进行原理性推导。该方法提供显式且计算成本低的ODE近似，能够捕捉梯度下降动态。

Result: Rod Flow在简单玩具示例中比Central Flow更好地捕捉梯度下降动态，在代表性神经网络架构中达到相同精度。理论证明Rod Flow能正确预测临界锐度阈值并解释四次势中的自稳定现象。数值实验验证了理论。

Conclusion: Rod Flow提供了一个理论化、准确且计算高效的梯度下降ODE近似方法，为理解非凸景观上的训练动态提供了新视角，特别是在稳定性边缘区域。

Abstract: How can we understand gradient-based training over non-convex landscapes? The edge of stability phenomenon, introduced in Cohen et al. (2021), indicates that the answer is not so simple: namely, gradient descent (GD) with large step sizes often diverges away from the gradient flow. In this regime, the "Central Flow", recently proposed in Cohen et al. (2025), provides an accurate ODE approximation to the GD dynamics over many architectures. In this work, we propose Rod Flow, an alternative ODE approximation, which carries the following advantages: (1) it rests on a principled derivation stemming from a physical picture of GD iterates as an extended one-dimensional object -- a "rod"; (2) it better captures GD dynamics for simple toy examples and matches the accuracy of Central Flow for representative neural network architectures, and (3) is explicit and cheap to compute. Theoretically, we prove that Rod Flow correctly predicts the critical sharpness threshold and explains self-stabilization in quartic potentials. We validate our theory with a range of numerical experiments.

</details>


### [245] [Causal Preference Elicitation](https://arxiv.org/abs/2602.01483)
*Edwin V. Bonilla,He Zhao,Daniel M. Steinberg*

Main category: cs.LG

TL;DR: 提出因果偏好获取框架，通过主动查询专家对局部边关系的判断来加速因果图后验集中，在有限查询预算下提升因果发现效果


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法通常仅依赖观测数据，而专家知识可以显著提升因果图推断的准确性。然而，如何高效地获取专家知识并整合到因果发现过程中是一个挑战。本文旨在开发一个主动学习框架，通过精心设计的查询策略，以最少的专家交互成本获得最大的因果图推断改进。

Method: 提出因果偏好获取框架：1) 从任意黑盒观测后验出发；2) 使用三分类似然函数建模专家对边存在性和方向的噪声判断；3) 采用灵活的粒子近似进行后验推断；4) 设计基于期望信息增益的高效查询选择标准，针对专家的分类响应优化查询策略。

Result: 在合成图、蛋白质信号数据和人类基因扰动基准测试中，该方法在有限查询预算下实现了更快的后验集中，并显著改善了有向效应的恢复效果。相比仅使用观测数据的方法，能够更高效地利用专家知识。

Conclusion: 因果偏好获取框架为专家参与的因果发现提供了一个有效的主动学习范式，通过精心设计的查询策略，能够在有限的专家交互成本下显著提升因果图推断的准确性和效率，为实际应用中的因果发现提供了实用工具。

Abstract: We propose causal preference elicitation, a Bayesian framework for expert-in-the-loop causal discovery that actively queries local edge relations to concentrate a posterior over directed acyclic graphs (DAGs). From any black-box observational posterior, we model noisy expert judgments with a three-way likelihood over edge existence and direction. Posterior inference uses a flexible particle approximation, and queries are selected by an efficient expected information gain criterion on the expert's categorical response. Experiments on synthetic graphs, protein signaling data, and a human gene perturbation benchmark show faster posterior concentration and improved recovery of directed effects under tight query budgets.

</details>


### [246] [Predicting and improving test-time scaling laws via reward tail-guided search](https://arxiv.org/abs/2602.01485)
*Muheng Li,Jian Qian,Wenlong Mou*

Main category: cs.LG

TL;DR: 提出一种基于尾部分布预测的测试时缩放方法SLG，通过动态计算分配优化LLM推理能力，相比传统Best-of-N策略获得理论保证和实际性能提升。


<details>
  <summary>Details</summary>
Motivation: 虽然Best-of-N策略已能显著提升大语言模型的推理能力，但缺乏对N值选择、预算分配和多阶段决策的原则性指导，存在优化空间。现有优化方法缺乏严格理论保证。

Method: 提出尾部引导搜索方法：通过估计奖励的尾部分布来预测缩放定律，无需详尽评估；基于此引入缩放定律引导搜索(SLG)，动态分配计算资源以识别和利用具有最高预测潜力的中间状态。

Result: 理论证明SLG相比完美信息预言机实现可忽略的遗憾，获得相同预期奖励所需计算预算比Best-of-N多项式级减少。实证验证在不同LLM和奖励模型上，尾部引导分配始终比相同计算预算下的Best-of-N获得更高奖励收益。

Conclusion: 提出的尾部引导搜索方法为测试时缩放提供了理论保证的优化框架，通过动态计算分配显著提升LLM推理能力，代码已开源。

Abstract: Test-time scaling has emerged as a critical avenue for enhancing the reasoning capabilities of Large Language Models (LLMs). Though the straight-forward ''best-of-$N$'' (BoN) strategy has already demonstrated significant improvements in performance, it lacks principled guidance on the choice of $N$, budget allocation, and multi-stage decision-making, thereby leaving substantial room for optimization. While many works have explored such optimization, rigorous theoretical guarantees remain limited. In this work, we propose new methodologies to predict and improve scaling properties via tail-guided search. By estimating the tail distribution of rewards, our method predicts the scaling law of LLMs without the need for exhaustive evaluations. Leveraging this prediction tool, we introduce Scaling-Law Guided (SLG) Search, a new test-time algorithm that dynamically allocates compute to identify and exploit intermediate states with the highest predicted potential. We theoretically prove that SLG achieves vanishing regret compared to perfect-information oracles, and achieves expected rewards that would otherwise require a polynomially larger compute budget required when using BoN. Empirically, we validate our framework across different LLMs and reward models, confirming that tail-guided allocation consistently achieves higher reward yields than Best-of-$N$ under identical compute budgets. Our code is available at https://github.com/PotatoJnny/Scaling-Law-Guided-search.

</details>


### [247] [Multi-Scale Wavelet Transformers for Operator Learning of Dynamical Systems](https://arxiv.org/abs/2602.01486)
*Xuesong Wang,Michael Groom,Rafael Oliveira,He Zhao,Terence O'Kane,Edwin V. Bonilla*

Main category: cs.LG

TL;DR: 提出多尺度小波变换器(MSWT)解决神经算子等数据驱动代理模型中的频谱偏差问题，通过在小波域学习系统动力学，显著提升长期预测的频谱保真度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 许多基于机器学习的动态系统代理模型（如神经算子）存在频谱偏差，会衰减高频成分，而这些高频成分通常编码了小尺度结构。在天气预报等应用中，高频成分的误表示会导致长期预测的不稳定性。

Method: 提出多尺度小波变换器(MSWT)，在小波域学习系统动力学。小波变换明确分离了不同尺度的低频和高频内容。MSWT采用保留小波特性的下采样方案来保持高频特征，并使用基于小波的注意力机制来捕捉跨尺度和频带的依赖关系。

Result: 在混沌动态系统实验中，MSWT显著减少了误差并改善了长期频谱保真度。在ERA5气候再分析数据上，MSWT进一步减少了气候学偏差，证明了其在真实世界预测场景中的有效性。

Conclusion: 多尺度小波变换器通过在小波域学习动态系统，有效解决了机器学习模型的频谱偏差问题，在动态系统建模和天气预报等应用中表现出优越的性能和稳定性。

Abstract: Recent years have seen a surge in data-driven surrogates for dynamical systems that can be orders of magnitude faster than numerical solvers. However, many machine learning-based models such as neural operators exhibit spectral bias, attenuating high-frequency components that often encode small-scale structure. This limitation is particularly damaging in applications such as weather forecasting, where misrepresented high frequencies can induce long-horizon instability. To address this issue, we propose multi-scale wavelet transformers (MSWTs), which learn system dynamics in a tokenized wavelet domain. The wavelet transform explicitly separates low- and high-frequency content across scales. MSWTs leverage a wavelet-preserving downsampling scheme that retains high-frequency features and employ wavelet-based attention to capture dependencies across scales and frequency bands. Experiments on chaotic dynamical systems show substantial error reductions and improved long horizon spectral fidelity. On the ERA5 climate reanalysis, MSWTs further reduce climatological bias, demonstrating their effectiveness in a real-world forecasting setting.

</details>


### [248] [OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference](https://arxiv.org/abs/2602.01493)
*Zhuoyuan Wang,Hanjiang Hu,Xiyu Deng,Saviz Mowlavi,Yorie Nakahira*

Main category: cs.LG

TL;DR: OpInf-LLM：结合算子推理与大语言模型的参数化PDE求解框架，利用少量解数据实现高精度预测，支持自然语言指定PDE求解任务。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在PDE求解中存在执行成功率与数值精度之间的权衡问题，特别是在泛化到未见参数和边界条件时面临挑战。需要一种能够可靠解决异构PDE问题的方法。

Method: 提出OpInf-LLM框架，基于算子推理方法，利用少量解数据构建参数化PDE求解器。通过统一工具接口与LLM集成，支持自然语言指定求解任务。

Result: 该框架能够在异构设置下实现高执行成功率，准确预测包括未见参数和配置在内的多样PDE实例，计算需求低且泛化能力强。

Conclusion: OpInf-LLM通过结合算子推理与LLM能力，为基于LLM的PDE求解中的可泛化降阶建模开辟了新可能性。

Abstract: Solving diverse partial differential equations (PDEs) is fundamental in science and engineering. Large language models (LLMs) have demonstrated strong capabilities in code generation, symbolic reasoning, and tool use, but reliably solving PDEs across heterogeneous settings remains challenging. Prior work on LLM-based code generation and transformer-based foundation models for PDE learning has shown promising advances. However, a persistent trade-off between execution success rate and numerical accuracy arises, particularly when generalization to unseen parameters and boundary conditions is required. In this work, we propose OpInf-LLM, an LLM parametric PDE solving framework based on operator inference. The proposed framework leverages a small amount of solution data to enable accurate prediction of diverse PDE instances, including unseen parameters and configurations, and provides seamless integration with LLMs for natural language specification of PDE solving tasks. Its low computational demands and unified tool interface further enable a high execution success rate across heterogeneous settings. By combining operator inference with LLM capabilities, OpInf-LLM opens new possibilities for generalizable reduced-order modeling in LLM-based PDE solving.

</details>


### [249] [Optimal Sample Complexity for Single Time-Scale Actor-Critic with Momentum](https://arxiv.org/abs/2602.01505)
*Navdeep Kumar,Tehila Dahan,Lior Cohen,Ananyabrata Barua,Giorgia Ramponi,Kfir Yehuda Levy,Shie Mannor*

Main category: cs.LG

TL;DR: 单时间尺度演员-评论家算法在无限时域折扣MDP中实现O(ε⁻²)样本复杂度，优于之前的O(ε⁻³)，通过STORM方差减少和样本缓冲技术


<details>
  <summary>Details</summary>
Motivation: 现有演员-评论家算法在无限时域折扣马尔可夫决策过程中需要O(ε⁻³)样本复杂度才能获得ε最优全局策略，这限制了算法的效率和实际应用，需要改进样本复杂度

Method: 1. 使用STORM（随机递归动量）技术减少评论家更新的方差；2. 维护一个小型最近样本缓冲区，均匀采样用于评论家更新，以应对非平稳占用度量带来的挑战；3. 采用单时间尺度演员-评论家算法框架

Result: 实现了O(ε⁻²)的最优样本复杂度，显著优于之前的O(ε⁻³)结果，且方法兼容现有深度学习架构，只需少量修改，不损害实际应用性

Conclusion: 通过结合STORM方差减少技术和样本缓冲机制，成功将单时间尺度演员-评论家算法的样本复杂度提升到最优的O(ε⁻²)，为强化学习算法的实际应用提供了更高效的解决方案

Abstract: We establish an optimal sample complexity of $O(ε^{-2})$ for obtaining an $ε$-optimal global policy using a single-timescale actor-critic (AC) algorithm in infinite-horizon discounted Markov decision processes (MDPs) with finite state-action spaces, improving upon the prior state of the art of $O(ε^{-3})$. Our approach applies STORM (STOchastic Recursive Momentum) to reduce variance in the critic updates. However, because samples are drawn from a nonstationary occupancy measure induced by the evolving policy, variance reduction via STORM alone is insufficient. To address this challenge, we maintain a buffer of small fraction of recent samples and uniformly sample from it for each critic update. Importantly, these mechanisms are compatible with existing deep learning architectures and require only minor modifications, without compromising practical applicability.

</details>


### [250] [Enhancing Generalization in Evolutionary Feature Construction for Symbolic Regression through Vicinal Jensen Gap Minimization](https://arxiv.org/abs/2602.01510)
*Hengzhe Zhang,Qi Chen,Bing Xue,Wolfgang Banzhaf,Mengjie Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于遗传编程的特征构建框架，通过优化经验风险和vicinal Jensen gap来控制过拟合，并引入噪声估计和流形入侵检测机制，在58个数据集上取得了优于15种机器学习算法的性能。


<details>
  <summary>Details</summary>
Motivation: 遗传编程特征构建虽然取得了显著成功，但过拟合问题限制了其更广泛的应用。需要改进泛化能力，特别是在不同噪声水平的数据集上。

Method: 1) 证明vicinal risk可以通过经验风险加正则化项（有限差分或vicinal Jensen gap）来界定；2) 提出进化特征构建框架，联合优化经验风险和vicinal Jensen gap；3) 开发噪声估计策略动态调整正则化强度；4) 提出流形入侵检测机制防止数据增强产生不现实的样本。

Result: 在58个数据集上的实验表明，Jensen gap最小化相比其他复杂度度量更有效。与15种机器学习算法的比较显示，采用所提过拟合控制策略的遗传编程获得了更优性能。

Conclusion: 通过理论分析和提出的过拟合控制策略，遗传编程特征构建的泛化能力得到显著提升，为自动化机器学习提供了有效的解决方案。

Abstract: Genetic programming-based feature construction has achieved significant success in recent years as an automated machine learning technique to enhance learning performance. However, overfitting remains a challenge that limits its broader applicability. To improve generalization, we prove that vicinal risk, estimated through noise perturbation or mixup-based data augmentation, is bounded by the sum of empirical risk and a regularization term-either finite difference or the vicinal Jensen gap. Leveraging this decomposition, we propose an evolutionary feature construction framework that jointly optimizes empirical risk and the vicinal Jensen gap to control overfitting. Since datasets may vary in noise levels, we develop a noise estimation strategy to dynamically adjust regularization strength. Furthermore, to mitigate manifold intrusion-where data augmentation may generate unrealistic samples that fall outside the data manifold-we propose a manifold intrusion detection mechanism. Experimental results on 58 datasets demonstrate the effectiveness of Jensen gap minimization compared to other complexity measures. Comparisons with 15 machine learning algorithms further indicate that genetic programming with the proposed overfitting control strategy achieves superior performance.

</details>


### [251] [White-Box Neural Ensemble for Vehicular Plasticity: Quantifying the Efficiency Cost of Symbolic Auditability in Adaptive NMPC](https://arxiv.org/abs/2602.01516)
*Enzo Nicolas Spotorno,Matheus Wagner,Antonio Augusto Medeiros Frohlich*

Main category: cs.LG

TL;DR: 提出了一种白盒自适应NMPC架构，通过模块主权范式仲裁冻结的特定工况神经网络专家，解决车辆可塑性问题，实现无需重新训练即可适应不同工况


<details>
  <summary>Details</summary>
Motivation: 解决车辆控制系统在不同工况（摩擦、质量、阻力等）下的适应性问题，传统非自适应基准方法在复合工况变化下会失效，需要一种无需重新训练就能适应变化的解决方案

Method: 采用白盒自适应NMPC架构，使用模块主权范式仲裁多个冻结的特定工况神经网络专家，将集成动力学维护为CasADi中完全可遍历的符号图，确保运行时可审计性

Result: 同步仿真验证了快速适应能力（约7.3毫秒），在复合工况变化下实现接近理想的跟踪精度，而基准方法会失效；量化了透明度成本：符号图维护使求解器延迟增加72-102倍

Conclusion: 该架构成功解决了车辆可塑性问题，实现了严格白盒实现的效率代价与透明度之间的权衡，为需要高可审计性的控制系统提供了可行的解决方案

Abstract: We present a white-box adaptive NMPC architecture that resolves vehicular plasticity (adaptation to varying operating regimes without retraining) by arbitrating among frozen, regime-specific neural specialists using a Modular Sovereignty paradigm. The ensemble dynamics are maintained as a fully traversable symbolic graph in CasADi, enabling maximal runtime auditability. Synchronous simulation validates rapid adaptation (~7.3 ms) and near-ideal tracking fidelity under compound regime shifts (friction, mass, drag) where non-adaptive baselines fail. Empirical benchmarking quantifies the transparency cost: symbolic graph maintenance increases solver latency by 72-102X versus compiled parametric physics models, establishing the efficiency price of strict white-box implementation.

</details>


### [252] [You Need an Encoder for Native Position-Independent Caching](https://arxiv.org/abs/2602.01519)
*Shiju Zhao,Junhao Hu,Jiaqi Zheng,Guihai Chen*

Main category: cs.LG

TL;DR: 提出原生位置无关缓存(PIC)方法COMB，通过为仅解码器LLMs重新引入编码器并专门训练，实现任意顺序上下文的高效KV缓存重用，大幅降低首token延迟并提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统LLMs的KV缓存基于前缀，对任意顺序检索的上下文处理效率低下。现有位置无关缓存方法存在显著的准确率下降问题，限制了实际应用。

Method: 1) 为流行的仅解码器LLMs重新引入编码器，并显式训练以支持位置无关缓存；2) 开发COMB系统，这是一个与现有推理框架无缝集成的PIC感知缓存系统。

Result: COMB将首token时间(TTFT)降低51-94%，吞吐量提升3倍，同时保持可比的准确率。在DeepSeek-V2-Lite-Chat上的质量改进证明了COMB对其他仅解码器LLMs的适用性。

Conclusion: 通过原生位置无关缓存方法COMB，解决了传统KV缓存对任意顺序上下文处理的效率瓶颈，实现了显著的性能提升，为仅解码器LLMs的高效推理提供了实用解决方案。

Abstract: The Key-Value (KV) cache of Large Language Models (LLMs) is prefix-based, making it highly inefficient for processing contexts retrieved in arbitrary order. Position-Independent Caching (PIC) has been proposed to enable KV reuse without positional constraints; however, existing approaches often incur substantial accuracy degradation, limiting their practical adoption. To address this issue, we propose native PIC by reintroducing the encoder to prevalent decoder-only LLMs and explicitly training it to support PIC. We further develop COMB, a PIC-aware caching system that integrates seamlessly with existing inference frameworks. Experimental results show that COMB reduces Time-to-First-Token (TTFT) by 51-94% and increases throughput by 3$\times$ with comparable accuracy. Furthermore, the quality improvement when using DeepSeek-V2-Lite-Chat demonstrates the applicability of COMB to other types of decoder-only LLMs. Our code is available at https://github.com/shijuzhao/Comb.

</details>


### [253] [When Is Rank-1 Enough? Geometry-Guided Initialization for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2602.01522)
*Haoran Zhao,Soyeon Caren Han,Eduard Hovy*

Main category: cs.LG

TL;DR: 论文提出Gap-Init初始化方法，通过将rank-1 LoRA方向与模态间隙向量对齐，解决了极低秩PEFT训练不稳定的问题，在多个视觉语言任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 参数高效微调(PEFT)是多模态大语言模型适配的标准方法，但在极低秩设置（特别是rank-1 LoRA）下经常不稳定。作者发现这种不稳定性不仅源于有限容量，还因为优化对更新方向高度敏感。

Method: 提出Gap-Init方法：通过分析预训练表示，识别主导早期梯度流的模态间隙轴，使用小型校准集估计模态间隙向量，将rank-1 LoRA方向与该向量对齐，同时保持初始LoRA更新为零。

Result: 在多个视觉语言任务和骨干网络上，Gap-Init能稳定rank-1训练，性能可匹配或超越强大的rank-8基线。结果表明在极低秩限制下，初始对齐与秩本身同样重要。

Conclusion: 极低秩PEFT的不稳定性源于模态间隙导致的梯度方向敏感性问题，通过几何感知的初始化方法可以有效解决。在rank-1限制下，初始方向对齐的重要性不亚于秩本身。

Abstract: Parameter-efficient fine-tuning (PEFT) is a standard way to adapt multimodal large language models, yet extremely low-rank settings -- especially rank-1 LoRA -- are often unstable. We show that this instability is not solely due to limited capacity: in the rank-1 regime, optimization is highly sensitive to the update direction. Concretely, pretrained vision and text features form mismatched anisotropic regions, yielding a dominant "gap" direction that acts like a translation component and disproportionately steers early gradients under rank-1 constraints. Analyzing pretrained representations, we identify a modality-gap axis that dominates early gradient flow, while a random rank-1 initialization is unlikely to align with it, leading to weak gradients and training collapse. We propose Gap-Init, a geometry-aware initialization that aligns the rank-1 LoRA direction with an estimated modality-gap vector from a small calibration set, while keeping the initial LoRA update zero. Across multiple vision-language tasks and backbones, Gap-Init consistently stabilizes rank-1 training and can match or outperform strong rank-8 baselines. Our results suggest that at the extreme low-rank limit, initial alignment can matter as much as rank itself.

</details>


### [254] [A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards in Large Language Model Reasoning](https://arxiv.org/abs/2602.01523)
*Akifumi Wachi,Hirota Kinoshita,Shokichi Takakura,Rei Higuchi,Taiji Suzuki*

Main category: cs.LG

TL;DR: 论文提出相对预算理论，通过单一量ξ=H/E[T]解释RL在不同任务和计算预算下的效果差异，揭示了三个学习机制：不足、平衡和充足，并提供了有限样本保证。


<details>
  <summary>Details</summary>
Motivation: 强化学习在提升大语言模型推理能力方面效果不一，现有理论无法解释这种在不同任务和计算预算下的性能差异，需要新的理论框架来理解RL学习的效率变化。

Method: 提出相对预算理论，定义ξ=H/E[T]作为核心指标，分析其对奖励方差和信息轨迹概率的影响，推导出三个学习机制，并在理想分布假设下提供有限样本保证。

Result: 理论分析揭示了三个学习机制：不足机制(ξ→0)样本复杂度爆炸，平衡机制(ξ=Θ(1))样本效率最高，充足机制(ξ→∞)学习稳定但边际收益递减。实证发现ξ∈[1.5,2.0]时学习效率最高且推理性能最佳。

Conclusion: 相对预算ξ是理解RL学习效率的关键指标，为优化语言模型RL训练提供了理论指导，平衡机制下的相对预算范围能最大化学习效率和推理性能。

Abstract: Reinforcement learning (RL) is a dominant paradigm for improving the reasoning abilities of large language models, yet its effectiveness varies across tasks and compute budgets. We propose a \emph{relative-budget} theory explaining this variation through a single quantity called relative budget $ξ:= H/\mathbb{E}[T]$, where $H$ is the generation horizon (token budget) and $T$ denotes the number of tokens until the first correct solution under a base policy. We show that $ξ$ determines sample efficiency by controlling reward variance and the likelihood of informative trajectories. Our analysis reveals three regimes: in the \emph{deficient} regime ($ξ\to 0$), informative trajectories are rare and the sample complexity explodes; in the \emph{balanced} regime ($ξ=Θ(1)$), informative trajectories occur with non-negligible probability and RL is maximally sample-efficient; and in the \emph{ample} regime ($ξ\to \infty$), learning remains stable but marginal gains per iteration diminish. We further provide finite-sample guarantees for online RL that characterize learning progress across these regimes. Specifically, in a case study under idealized distributional assumptions, we show that the relative budget grows linearly over iterations. Our empirical results confirm these predictions in realistic settings, identifying a budget $ξ\in [1.5, 2.0]$ that maximizes learning efficiency and coincides with peak reasoning performance.

</details>


### [255] [The Inlet Rank Collapse in Implicit Neural Representations: Diagnosis and Unified Remedy](https://arxiv.org/abs/2602.01526)
*Jianqiao Zheng,Hemanth Saratchandran,Simon Lucey*

Main category: cs.LG

TL;DR: 论文提出了一种诊断框架，通过层级的NTK分解识别了"入口秩崩溃"现象，并开发了秩扩展初始化方法来解决INR中细节恢复不足的问题。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示在连续信号建模中表现出色，但在有限训练预算下难以恢复细粒度细节。现有技术如位置编码、正弦激活和批归一化虽然有效，但理论解释多为事后分析，缺乏统一的理论框架。

Method: 提出了结构诊断框架，通过层级分解神经正切核，数学上识别了"入口秩崩溃"现象。基于此诊断，推导出秩扩展初始化方法，确保表示秩随层宽扩展，无需架构修改或计算开销。

Result: 秩扩展初始化使标准MLP能够实现高保真重建，证明了通过优化初始秩传播来有效填充潜在空间是增强INR的关键。

Conclusion: 研究揭示了INR中细节恢复不足的根本原因，提供了统一的理论框架来解释现有技术，并提出了简单有效的秩扩展初始化方法，为INR的结构优化提供了新方向。

Abstract: Implicit Neural Representations (INRs) have revolutionized continuous signal modeling, yet they struggle to recover fine-grained details within finite training budgets. While empirical techniques, such as positional encoding (PE), sinusoidal activations (SIREN), and batch normalization (BN), effectively mitigate this, their theoretical justifications are predominantly post hoc, focusing on the global NTK spectrum only after modifications are applied. In this work, we reverse this paradigm by introducing a structural diagnostic framework. By performing a layer-wise decomposition of the NTK, we mathematically identify the ``Inlet Rank Collapse'': a phenomenon where the low-dimensional input coordinates fail to span the high-dimensional embedding space, creating a fundamental rank deficiency at the first layer that acts as an expressive bottleneck for the entire network. This framework provides a unified perspective to re-interpret PE, SIREN, and BN as different forms of rank restoration. Guided by this diagnosis, we derive a Rank-Expanding Initialization, a minimalist remedy that ensures the representation rank scales with the layer width without architectural modifications or computational overhead. Our results demonstrate that this principled remedy enables standard MLPs to achieve high-fidelity reconstructions, proving that the key to empowering INRs lies in the structural optimization of the initial rank propagation to effectively populate the latent space.

</details>


### [256] [Plain Transformers are Surprisingly Powerful Link Predictors](https://arxiv.org/abs/2602.01553)
*Quang Truong,Yu Song,Donald Loveland,Mingxuan Ju,Tong Zhao,Neil Shah,Jiliang Tang*

Main category: cs.LG

TL;DR: PENCIL：一种用于链接预测的纯Transformer模型，通过注意力机制处理采样子图，无需手工先验知识，在保持可扩展性的同时超越了传统GNN和启发式方法。


<details>
  <summary>Details</summary>
Motivation: 当前图链接预测方法面临挑战：GNN依赖显式结构启发式或内存密集型节点嵌入，难以泛化和扩展到大规模图；图Transformer因复杂结构编码产生显著开销。需要一种既能捕获丰富拓扑依赖，又具备可扩展性和硬件效率的解决方案。

Method: PENCIL采用仅编码器的纯Transformer架构，用注意力机制处理采样的局部子图，替代手工设计的先验知识。该方法保留了标准Transformer的可扩展性和硬件效率，能够隐式泛化广泛的启发式和基于子图的表达能力。

Result: PENCIL在实验中提取了比GNN更丰富的结构信号，超越了启发式GNN，比基于ID嵌入的替代方案参数效率更高，在不同基准测试中保持竞争力，即使在没有节点特征的情况下也能表现良好。

Conclusion: 研究结果表明，简单的设计选择可能足以实现复杂工程技术的能力，挑战了当前对复杂工程技术的依赖。PENCIL展示了纯Transformer架构在图链接预测任务中的潜力，为大规模图学习提供了可扩展且高效的解决方案。

Abstract: Link prediction is a core challenge in graph machine learning, demanding models that capture rich and complex topological dependencies. While Graph Neural Networks (GNNs) are the standard solution, state-of-the-art pipelines often rely on explicit structural heuristics or memory-intensive node embeddings -- approaches that struggle to generalize or scale to massive graphs. Emerging Graph Transformers (GTs) offer a potential alternative but often incur significant overhead due to complex structural encodings, hindering their applications to large-scale link prediction. We challenge these sophisticated paradigms with PENCIL, an encoder-only plain Transformer that replaces hand-crafted priors with attention over sampled local subgraphs, retaining the scalability and hardware efficiency of standard Transformers. Through experimental and theoretical analysis, we show that PENCIL extracts richer structural signals than GNNs, implicitly generalizing a broad class of heuristics and subgraph-based expressivity. Empirically, PENCIL outperforms heuristic-informed GNNs and is far more parameter-efficient than ID-embedding--based alternatives, while remaining competitive across diverse benchmarks -- even without node features. Our results challenge the prevailing reliance on complex engineering techniques, demonstrating that simple design choices are potentially sufficient to achieve the same capabilities.

</details>


### [257] [InfoTok: Regulating Information Flow for Capacity-Constrained Shared Visual Tokenization in Unified MLLMs](https://arxiv.org/abs/2602.01554)
*Lv Tang,Tianyi Zheng,Bo Li,Xingyu Li*

Main category: cs.LG

TL;DR: 本文提出InfoTok，一种基于信息瓶颈原则的信息正则化视觉分词机制，用于统一多模态大语言模型，通过控制从图像到共享token的信息流，在压缩和任务相关性之间取得平衡，从而同时提升理解和生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态大语言模型中的共享token设计大多是架构驱动的，缺乏明确的标准来确定token应该保留什么信息来同时支持理解和生成任务。作者从容量约束的角度出发，认为视觉分词器应该优先考虑可重用的结构信息，而不是难以利用的高熵变化和冗余信息。

Method: 提出InfoTok，一种基于信息瓶颈原则的信息正则化视觉分词机制。该方法将分词过程形式化为控制从图像到共享token再到多模态输出的信息流，通过互信息正则化在压缩和任务相关性之间取得原则性平衡。该方法无需额外训练数据，可集成到现有的统一MLLMs中。

Result: 实验表明，将InfoTok集成到三种代表性的统一MLLMs中，在理解和生成任务上都取得了一致的性能提升，验证了信息正则化分词作为学习统一MLLMs共享token空间的原则性基础的有效性。

Conclusion: 信息正则化视觉分词机制为统一多模态大语言模型中的共享token学习提供了原则性基础，通过信息瓶颈原则在压缩和任务相关性之间取得平衡，能够同时提升模型的理解和生成能力。

Abstract: Unified multimodal large language models (MLLMs) integrate image understanding and generation in a single framework, with the visual tokenizer acting as the sole interface that maps visual inputs into tokens for downstream tasks. However, existing shared-token designs are mostly architecture-driven and lack an explicit criterion for what information tokens should preserve to support both understanding and generation. Therefore, we introduce a capacity-constrained perspective, highlighting that in shared-token unified MLLMs the visual tokenizer behaves as a compute-bounded learner, so the token budget should prioritize reusable structure over hard-to-exploit high-entropy variations and redundancy. Motivated by this perspective, we propose InfoTok, an information-regularized visual tokenization mechanism grounded in the Information Bottleneck (IB) principle. InfoTok formulates tokenization as controlling information flow from images to shared tokens to multimodal outputs, yielding a principled trade-off between compression and task relevance via mutual-information regularization. We integrate InfoTok into three representative unified MLLMs without introducing any additional training data. Experiments show consistent improvements on both understanding and generation, supporting information-regularized tokenization as a principled foundation for learning a shared token space in unified MLLMs.

</details>


### [258] [How Implicit Bias Accumulates and Propagates in LLM Long-term Memory](https://arxiv.org/abs/2602.01558)
*Yiming Ma,Lixu Wang,Lionel Z. Wang,Hongkun Yang,Haoming Sun,Xin Xu,Jiaqi Wu,Bin Chen,Wei Dong*

Main category: cs.LG

TL;DR: 研究LLM长期记忆机制中的隐性偏见积累与传播问题，提出动态记忆标记方法进行缓解


<details>
  <summary>Details</summary>
Motivation: LLM的长期记忆机制虽然能保持交互连续性和个性化，但也带来了新的公平性风险，特别是隐性偏见在长期决策过程中的积累和传播问题尚未充分研究

Method: 1) 构建DIB基准数据集（3,776个决策场景，9个社会领域）；2) 使用长时程模拟框架评估6个SOTA LLM和3种记忆架构；3) 提出动态记忆标记(DMT)方法，在记忆写入时强制执行公平约束

Result: 实验表明：1) LLM的隐性偏见不会保持静态，而是随时间加剧并在无关领域间传播；2) 静态系统级提示的缓解效果有限且短暂；3) DMT方法能显著减少偏见积累并有效遏制跨领域偏见传播

Conclusion: LLM长期记忆中的隐性偏见是动态积累和传播的系统性问题，需要动态干预策略，DMT方法通过代理式干预在记忆写入时施加公平约束，能有效缓解这一问题

Abstract: Long-term memory mechanisms enable Large Language Models (LLMs) to maintain continuity and personalization across extended interaction lifecycles, but they also introduce new and underexplored risks related to fairness. In this work, we study how implicit bias, defined as subtle statistical prejudice, accumulates and propagates within LLMs equipped with long-term memory. To support systematic analysis, we introduce the Decision-based Implicit Bias (DIB) Benchmark, a large-scale dataset comprising 3,776 decision-making scenarios across nine social domains, designed to quantify implicit bias in long-term decision processes. Using a realistic long-horizon simulation framework, we evaluate six state-of-the-art LLMs integrated with three representative memory architectures on DIB and demonstrate that LLMs' implicit bias does not remain static but intensifies over time and propagates across unrelated domains. We further analyze mitigation strategies and show that a static system-level prompting baseline provides limited and short-lived debiasing effects. To address this limitation, we propose Dynamic Memory Tagging (DMT), an agentic intervention that enforces fairness constraints at memory write time. Extensive experimental results show that DMT substantially reduces bias accumulation and effectively curtails cross-domain bias propagation.

</details>


### [259] [Local Exponential Stability of Mean-Field Langevin Descent-Ascent in Wasserstein Space](https://arxiv.org/abs/2602.01564)
*Geuntaek Seo,Minseop Shin,Pierre Monmarché,Beomjun Choi*

Main category: cs.LG

TL;DR: 证明了均值场Langevin下降-上升动态在熵正则化两人零和博弈中的局部指数稳定性：当初始化足够接近均衡时，动态以指数速率收敛到均衡。


<details>
  <summary>Details</summary>
Motivation: 尽管均值场目标函数存在唯一的混合纳什均衡，但原始MFL-DA动态对于一般非凸-非凹收益函数的长期行为一直未得到解决。本文旨在回答Wang和Chizat在COLT 2024中提出的一个开放性问题。

Method: 通过线性化算子的谱分析建立均衡附近熵的强制性估计，揭示局部位移凸-凹结构，从而证明收缩性质。

Result: 证明了均衡是局部指数稳定的：如果初始化在Wasserstein度量下足够接近均衡，动态以指数速率收敛到均衡。

Conclusion: 解决了Wang和Chizat提出的局部稳定性和定量速率问题，但全局收敛仍然是一个未解决的挑战。

Abstract: We study the mean-field Langevin descent-ascent (MFL-DA), a coupled optimization dynamics on the space of probability measures for entropically regularized two-player zero-sum games. Although the associated mean-field objective admits a unique mixed Nash equilibrium, the long-time behavior of the original MFL-DA for general nonconvex-nonconcave payoffs has remained largely open. Answering an open question posed by Wang and Chizat (COLT 2024), we provide a partial resolution by proving that this equilibrium is locally exponentially stable: if the initialization is sufficiently close in Wasserstein metric, the dynamics trends to the equilibrium at an exponential rate. The key to our analysis is to establish a coercivity estimate for the entropy near equilibrium via spectral analysis of the linearized operator. We show that this coercivity effectively reveals a local displacement convex-concave structure, thereby driving contraction. This result settles the local stability and quantitative rate questions of Wang and Chizat, leaving global convergence as a remaining open challenge.

</details>


### [260] [Generative Visual Code Mobile World Models](https://arxiv.org/abs/2602.01576)
*Woosung Koh,Sungjun Han,Segyu Lee,Se-Young Yun,Jamin Shin*

Main category: cs.LG

TL;DR: gWorld提出通过生成可执行的网页代码来构建视觉移动GUI世界模型，结合了文本模型的精确文本渲染和视觉模型的高保真度，在多个基准测试中超越了现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前移动GUI世界模型面临关键权衡：基于文本的模型牺牲视觉保真度，而视觉模型无法精确渲染文本，依赖缓慢复杂的外部模型管道。需要一种新范式来结合两者的优势。

Method: 提出通过可渲染代码生成的视觉世界建模范式：使用单一视觉语言模型预测下一个GUI状态作为可执行的网页代码，而不是直接生成像素。同时开发了gWorld数据生成框架自动合成基于代码的训练数据。

Result: gWorld（8B和32B参数版本）在4个分布内和2个分布外基准测试中建立了新的帕累托前沿，在准确率与模型大小方面优于8个前沿开源模型（最大达50.25倍）。分析显示数据扩展、管道组件和数据质量改进都能提升性能。

Conclusion: 通过可渲染代码生成的视觉世界建模范式成功结合了文本和视觉模型的优势，gWorld展示了这一方法的有效性，并证明更强的世界建模能提升下游移动GUI策略性能。

Abstract: Mobile Graphical User Interface (GUI) World Models (WMs) offer a promising path for improving mobile GUI agent performance at train- and inference-time. However, current approaches face a critical trade-off: text-based WMs sacrifice visual fidelity, while the inability of visual WMs in precise text rendering led to their reliance on slow, complex pipelines dependent on numerous external models. We propose a novel paradigm: visual world modeling via renderable code generation, where a single Vision-Language Model (VLM) predicts the next GUI state as executable web code that renders to pixels, rather than generating pixels directly. This combines the strengths of both approaches: VLMs retain their linguistic priors for precise text rendering while their pre-training on structured web code enables high-fidelity visual generation. We introduce gWorld (8B, 32B), the first open-weight visual mobile GUI WMs built on this paradigm, along with a data generation framework (gWorld) that automatically synthesizes code-based training data. In extensive evaluation across 4 in- and 2 out-of-distribution benchmarks, gWorld sets a new pareto frontier in accuracy versus model size, outperforming 8 frontier open-weight models over 50.25x larger. Further analyses show that (1) scaling training data via gWorld yields meaningful gains, (2) each component of our pipeline improves data quality, and (3) stronger world modeling improves downstream mobile GUI policy performance.

</details>


### [261] [Nearly Optimal Active Preference Learning and Its Application to LLM Alignment](https://arxiv.org/abs/2602.01581)
*Yao Zhao,Kwang-Sung Jun*

Main category: cs.LG

TL;DR: 提出针对偏好学习的高效主动学习算法，相比传统实验设计方法显著提升样本效率


<details>
  <summary>Details</summary>
Motivation: 对齐大语言模型需要高质量的人类偏好标注数据，但收集成本高昂。现有的主动学习方法采用经典实验设计准则（如G-或D-最优性），这些方法未针对偏好学习的特定结构进行优化，需要设计问题特定的算法。

Method: 1. 识别了偏好学习特有的简单直觉，质疑现有设计目标的适用性；2. 提出两种主动学习算法：第一种提供该场景下首个实例依赖的标签复杂度保证，第二种是简单实用的贪婪方法。

Result: 在真实世界偏好数据集上评估算法，相比现有方法观察到样本效率的提升。

Conclusion: 针对偏好学习设计的主动学习算法能有效提高样本效率，为LLM对齐提供更经济的数据收集方案。

Abstract: Aligning large language models (LLMs) depends on high-quality datasets of human preference labels, which are costly to collect. Although active learning has been studied to improve sample efficiency relative to passive collection, many existing approaches adopt classical experimental design criteria such as G- or D-optimality. These objectives are not tailored to the structure of preference learning, leaving open the design of problem-specific algorithms. In this work, we identify a simple intuition specific to preference learning that calls into question the suitability of these existing design objectives. Motivated by this insight, we propose two active learning algorithms. The first provides the first instance-dependent label complexity guarantee for this setting, and the second is a simple, practical greedy method. We evaluate our algorithm on real-world preference datasets and observe improved sample efficiency compared to existing methods.

</details>


### [262] [A Lightweight Sparse Interaction Network for Time Series Forecasting](https://arxiv.org/abs/2602.01585)
*Xu Zhang,Qitong Wang,Peng Wang,Wei Wang*

Main category: cs.LG

TL;DR: LSINet提出了一种轻量级稀疏交互网络，通过多头稀疏交互机制和共享交互学习，在线性模型基础上实现了显式的时间交互，在长时序预测任务中取得了更高的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有线性模型虽然在某些长时序预测任务中优于Transformer模型，但它们通过堆叠MLP结构隐式进行时间交互，可能不足以捕捉复杂的时间依赖关系，性能仍有提升空间。

Method: 提出LSINet，包含多头稀疏交互机制（MSIM），通过稀疏诱导的伯努利分布学习时间步之间的重要连接来捕捉时间依赖；引入自适应正则化损失确保稀疏性；提出共享交互学习（SIL）进一步提升效率和收敛性。

Result: 在公开数据集上的大量实验表明，LSINet在长时序预测任务中比先进的线性模型和Transformer模型实现了更高的准确性和更好的效率。

Conclusion: LSINet通过显式的时间交互机制，在线性模型的低开销基础上，有效提升了长时序预测的性能，为时序预测任务提供了一个高效准确的解决方案。

Abstract: Recent work shows that linear models can outperform several transformer models in long-term time-series forecasting (TSF). However, instead of explicitly performing temporal interaction through self-attention, linear models implicitly perform it based on stacked MLP structures, which may be insufficient in capturing the complex temporal dependencies and their performance still has potential for improvement. To this end, we propose a Lightweight Sparse Interaction Network (LSINet) for TSF task. Inspired by the sparsity of self-attention, we propose a Multihead Sparse Interaction Mechanism (MSIM). Different from self-attention, MSIM learns the important connections between time steps through sparsity-induced Bernoulli distribution to capture temporal dependencies for TSF. The sparsity is ensured by the proposed self-adaptive regularization loss. Moreover, we observe the shareability of temporal interactions and propose to perform Shared Interaction Learning (SIL) for MSIM to further enhance efficiency and improve convergence. LSINet is a linear model comprising only MLP structures with low overhead and equipped with explicit temporal interaction mechanisms. Extensive experiments on public datasets show that LSINet achieves both higher accuracy and better efficiency than advanced linear models and transformer models in TSF tasks. The code is available at the link https://github.com/Meteor-Stars/LSINet.

</details>


### [263] [Spectral Text Fusion: A Frequency-Aware Approach to Multimodal Time-Series Forecasting](https://arxiv.org/abs/2602.01588)
*Huu Hiep Nguyen,Minh Hoang Nguyen,Dung Nguyen,Hung Le*

Main category: cs.LG

TL;DR: SpecTF：一种通过频域融合文本信息的多模态时间序列预测框架，显著优于现有方法且参数更少


<details>
  <summary>Details</summary>
Motivation: 现有方法在融合文本上下文与时间序列时，通常采用逐步对齐方式，忽略了上下文信息的多尺度时间影响（如周期性和动态变化），导致局部对齐与全局文本上下文不匹配

Method: 提出SpecTF框架：提取文本嵌入，将其投影到频域，通过轻量级交叉注意力机制与时间序列的频谱分量融合，基于文本相关性自适应重加权频段，最后映射回时域进行预测

Result: 在多个多模态时间序列数据集上，SpecTF显著优于现有最先进模型，同时使用的参数数量显著减少

Conclusion: 通过频域融合文本信息能有效解决多模态时间序列预测中局部对齐与全局上下文不匹配的问题，SpecTF框架简单有效且参数高效

Abstract: Multimodal time series forecasting is crucial in real-world applications, where decisions depend on both numerical data and contextual signals. The core challenge is to effectively combine temporal numerical patterns with the context embedded in other modalities, such as text. While most existing methods align textual features with time-series patterns one step at a time, they neglect the multiscale temporal influences of contextual information such as time-series cycles and dynamic shifts. This mismatch between local alignment and global textual context can be addressed by spectral decomposition, which separates time series into frequency components capturing both short-term changes and long-term trends. In this paper, we propose SpecTF, a simple yet effective framework that integrates the effect of textual data on time series in the frequency domain. Our method extracts textual embeddings, projects them into the frequency domain, and fuses them with the time series' spectral components using a lightweight cross-attention mechanism. This adaptively reweights frequency bands based on textual relevance before mapping the results back to the temporal domain for predictions. Experimental results demonstrate that SpecTF significantly outperforms state-of-the-art models across diverse multi-modal time series datasets while utilizing considerably fewer parameters. Code is available at https://github.com/hiepnh137/SpecTF.

</details>


### [264] [The Multiple Ticket Hypothesis: Random Sparse Subnetworks Suffice for RLVR](https://arxiv.org/abs/2602.01599)
*Israel Adewuyi,Solomon Okibe,Vladmir Ivanov*

Main category: cs.LG

TL;DR: 在强化学习可验证奖励（RLVR）中，仅训练1%的随机参数子集就能达到或超过全参数微调的性能，表明预训练模型包含多个可行的稀疏子网络而非单一特权集合。


<details>
  <summary>Details</summary>
Motivation: 彩票假说表明稀疏子网络能匹配完整模型性能，而RLVR中更新集中在稀疏参数子集上，这进一步证明了参数冗余。本研究旨在探索利用这种冗余的最简单方式：在极端稀疏度下仅训练随机选择的参数子集。

Method: 在RLVR微调中，仅训练随机选择的1%参数子集（使用随机掩码），比较不同随机掩码的性能和重叠度，分析KL约束对更新空间的影响。

Result: 仅训练1%参数就能匹配或超过全参数RLVR微调（在3个模型和2个任务领域验证）。不同随机掩码重叠度极低（Jaccard相似度≤0.005），但都能成功，表明存在多个可行的稀疏子网络。

Conclusion: 提出了"多票假说"：预训练模型包含许多可行的稀疏子网络，而非单一特权集合。RLVR中的隐式每步KL约束将更新限制在低维子空间，使得任意稀疏掩码都能成功。

Abstract: The Lottery Ticket Hypothesis demonstrated that sparse subnetworks can match full-model performance, suggesting parameter redundancy. Meanwhile, in Reinforcement Learning with Verifiable Rewards (RLVR), recent work has shown that updates concentrate on a sparse subset of parameters, which further lends evidence to this underlying redundancy. We study the simplest possible way to exploit this redundancy: training only a randomly selected subset of parameters at extreme sparsities. Empirically, we find that training just 1\% of parameters matches or exceeds full-parameter RLVR finetuning across 3 models and 2 task domains. Moreover, different random masks show minimal overlap ($\leq 0.005$ Jaccard similarity) and yet all succeed, suggesting pretrained models contain many viable sparse subnetworks rather than one privileged set. We term this the Multiple Ticket Hypothesis. We explain this phenomenon through the implicit per-step KL constraint in RLVR, which restricts updates to a low-dimensional subspace, enabling arbitrary sparse masks to succeed.

</details>


### [265] [Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.01601)
*Hieu Trung Nguyen,Bao Nguyen,Wenao Ma,Yuzhi Zhao,Ruifeng She,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: VIP是一种基于方差感知的预测性分配策略，通过高斯过程模型预测每个提示的成功概率，优化rollout分配以最小化策略更新的梯度方差，从而提高强化学习的采样效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于组的策略优化方法（如GRPO）对所有训练提示分配固定数量的rollout，这种均匀分配隐含地将所有提示视为同等重要，导致计算预算使用效率低下，阻碍训练进展。

Method: VIP使用轻量级高斯过程模型基于最近的rollout预测每个提示的成功概率，将这些概率预测转换为方差估计，然后通过凸优化问题在硬计算预算约束下确定最优的rollout分配。

Result: 实验结果表明，VIP在多个基准测试中持续提高采样效率，并比均匀分配或启发式分配策略获得更高的性能。

Conclusion: VIP通过方差感知的预测性分配策略，有效解决了强化学习中采样效率低下的问题，为计算预算的优化分配提供了有效解决方案。

Abstract: Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at https://github.com/HieuNT91/VIP.

</details>


### [266] [Universal Redundancies in Time Series Foundation Models](https://arxiv.org/abs/2602.01605)
*Anthony Bao,Venkata Hasith Vattikuti,Jeffrey Lai,William Gilpin*

Main category: cs.LG

TL;DR: 研究发现时间序列基础模型存在冗余组件，提出机制可解释性工具，发现模型对整层剪枝具有鲁棒性，并开发理论框架识别导致退化现象的具体注意力头。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型通过大规模预训练实现零样本预测，但现有研究对其内部工作机制了解有限，需要开发可解释性工具来理解这些模型的运行机制和冗余特性。

Method: 提出时间序列基础模型的机制可解释性工具集，包括组件消融和残差流直接对数归因。开发理论框架将Transformer视为核回归器，基于每个注意力头投影矩阵的稳定秩进行消融分析。

Result: 发现所有研究的模型对整层消融具有鲁棒性。通过稳定秩方法识别出导致时间序列基础模型中常见退化现象（如上下文模式重复和季节性偏差）的具体注意力头。

Conclusion: 研究揭示了时间序列基础模型架构的普遍特性，为理解这一新兴连续时间序列建模架构提供了机制可解释性工具和理论框架，有助于改进模型设计和减少冗余。

Abstract: Time Series Foundation Models (TSFMs) leverage extensive pretraining to accurately predict unseen time series during inference, without the need for task-specific fine-tuning. Through large-scale evaluations on standard benchmarks, we find that leading transformer-based TSFMs exhibit redundant components in their intermediate layers. We introduce a set of tools for mechanistic interpretability of TSFMs, including ablations of specific components and direct logit attribution on the residual stream. Our findings are consistent across several leading TSFMs with diverse architectures, and across a diverse set of real-world and synthetic time-series datasets. We discover that all models in our study are robust to ablations of entire layers. Furthermore, we develop a theoretical framework framing transformers as kernel regressors, motivating a purely intrinsic strategy for ablating heads based on the stable rank of the per-head projection matrices. Using this approach, we uncover the specific heads responsible for degenerate phenomena widely observed in TSFMs, such as parroting of motifs from the context and seasonality bias. Our study sheds light on the universal properties of this emerging class of architectures for continuous-time sequence modeling.

</details>


### [267] [Boosting Maximum Entropy Reinforcement Learning via One-Step Flow Matching](https://arxiv.org/abs/2602.01606)
*Zeqiao Li,Yijing Wang,Haoyu Wang,Zheng Li,Zhiqiang Zuo*

Main category: cs.LG

TL;DR: FLAME是一个基于流匹配的最大熵强化学习框架，通过Q重加权目标绕过配分函数估计，使用解耦熵估计器纠正偏差，实现高效探索和一步推理控制，在保持性能的同时显著降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 扩散策略具有表达能力强但推理延迟高的问题，而流匹配虽然能实现一步生成，但难以集成到最大熵强化学习中，因为最优策略是难以处理的基于能量的分布，且高效的似然估计存在严重离散化偏差。

Method: 1) 提出Q重加权流匹配目标，通过重要性重加权绕过配分函数估计；2) 设计解耦熵估计器，严格纠正偏差，实现高效探索；3) 集成MeanFlow公式，实现表达性强且高效的一步控制。

Result: 在MuJoCo基准测试中，FLAME优于高斯基线，与多步扩散策略性能相当，但推理成本显著降低。

Conclusion: FLAME为最大熵强化学习提供了一个原则性框架，解决了流匹配集成中的挑战，实现了表达性强、推理高效的一步策略，平衡了探索与利用。

Abstract: Diffusion policies are expressive yet incur high inference latency. Flow Matching (FM) enables one-step generation, but integrating it into Maximum Entropy Reinforcement Learning (MaxEnt RL) is challenging: the optimal policy is an intractable energy-based distribution, and the efficient log-likelihood estimation required to balance exploration and exploitation suffers from severe discretization bias. We propose \textbf{F}low-based \textbf{L}og-likelihood-\textbf{A}ware \textbf{M}aximum \textbf{E}ntropy RL (\textbf{FLAME}), a principled framework that addresses these challenges. First, we derive a Q-Reweighted FM objective that bypasses partition function estimation via importance reweighting. Second, we design a decoupled entropy estimator that rigorously corrects bias, which enables efficient exploration and brings the policy closer to the optimal MaxEnt policy. Third, we integrate the MeanFlow formulation to achieve expressive and efficient one-step control. Empirical results on MuJoCo show that FLAME outperforms Gaussian baselines and matches multi-step diffusion policies with significantly lower inference cost. Code is available at https://github.com/lzqw/FLAME.

</details>


### [268] [What Do Agents Learn from Trajectory-SFT: Semantics or Interfaces?](https://arxiv.org/abs/2602.01611)
*Weizheng Gu,Chengze Li,Zhuohao Yu,Mengyuan Sun,Zhibang Yang,Wei Wang,Hongrui Jia,Shikun Zhang,Wei Ye*

Main category: cs.LG

TL;DR: 论文提出PIPE评估协议，用于诊断AI智能体对特定界面设计的依赖，发现轨迹监督微调会强化界面捷径学习而非语义理解


<details>
  <summary>Details</summary>
Motivation: 现有智能体评估基准混淆了语义工具使用和界面特定交互模式记忆两种不同的成功机制，基准分数无法区分环境不变的能力

Method: 提出PIPE协议级评估增强方法，通过最小化重写环境界面（保持任务语义和执行行为不变）来诊断界面依赖；引入界面依赖度(IR)度量来量化对训练时界面的偏好

Result: 在16个环境和多种智能体上的实验显示：轨迹SFT显著放大界面捷径学习，训练后的智能体在最小界面重写下性能急剧下降，而非轨迹训练模型保持稳定；界面捷径学习呈现环境依赖、非单调的训练动态

Conclusion: 标准评估无法揭示智能体对界面的依赖，PIPE协议能有效诊断界面捷径学习问题，轨迹SFT可能强化界面模式记忆而非语义理解能力

Abstract: Large language models are increasingly evaluated as interactive agents, yet standard agent benchmarks conflate two qualitatively distinct sources of success: semantic tool-use and interface-specific interaction pattern memorization. Because both mechanisms can yield identical task success on the original interface, benchmark scores alone are not identifiable evidence of environment-invariant capability. We propose PIPE, a protocol-level evaluation augmentation for diagnosing interface reliance by minimally rewriting environment interfaces while preserving task semantics and execution behavior. Across 16 environments from AgentBench and AgentGym and a range of open-source and API-based agents, PIPE reveals that trajectory-SFT substantially amplifies interface shortcutting: trained agents degrade sharply under minimal interface rewrites, while non-trajectory-trained models remain largely stable. We further introduce Interface Reliance (IR), a counterbalanced alias-based metric that quantifies preference for training-time interfaces, and show that interface shortcutting exhibits environment-dependent, non-monotonic training dynamics that remain invisible under standard evaluation. Our code is available at https://anonymous.4open.science/r/What-Do-Agents-Learn-from-Trajectory-SFT-Semantics-or-Interfaces--0831/.

</details>


### [269] [A Practical Tensor-Network Compression Pipeline for Production-Scale Large Language Models](https://arxiv.org/abs/2602.01613)
*Sergii Kozyrev,Davyd Maiboroda*

Main category: cs.LG

TL;DR: Minima是一个生产级压缩流水线，通过结构压缩Transformer模型来减少GPU内存占用和推理延迟，结合多种张量分解方法和自定义内核，并支持推测解码来提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在部署时受到GPU内存和推理延迟的限制，需要有效的压缩方法来实现实际的服务性能提升。

Method: 训练轻量级卷积预测器评估层和补丁级别的敏感性；对低敏感性区域应用Tucker、张量链和张量环分解；进行短期恢复微调；使用自定义Triton和CUDA内核执行；通过减少内存占用实现小草案模型和大验证器的推测解码。

Result: 在Qwen3-32B模型上，8k上下文窗口下，峰值VRAM从64GiB降至40GiB；单请求吞吐量从40tps提升至50tps（Minima）和75tps（Minima+推测解码）；50并行请求下吞吐量为34、44、53tps，显示高并发下仍有效。

Conclusion: Minima是向更激进的结构压缩的实用步骤，通过共享张量骨干网络和微小每层适配器实现，相比现有张量网络、低秩+量化和跨层共享方法具有实际优势。

Abstract: Large language models are limited in deployment by GPU memory and inference latency. We present Minima, a production compression pipeline that learns where and how to structurally compress a Transformer and turns that compression into real serving gains. Minima trains a lightweight convolutional predictor to estimate layer- and patch-level sensitivity, applies a mixture of Tucker, tensor-train, and tensor-ring decompositions to low-sensitivity regions, performs a short healing fine-tune, and executes the resulting operators with custom Triton and CUDA kernels. The reduced memory footprint enables speculative decoding with a small draft model and a larger verifier. On Qwen3-32B at an 8k-token context window, Minima reduces peak VRAM from 64 GiB to 40 GiB. For a single active request, throughput increases from 40 tokens per second (baseline) to 50 tokens per second (Minima) and 75 tokens per second (Minima with speculative decoding). Under 50 parallel requests, throughput is 34, 44, and 53 tokens per second respectively, showing that Minima remains effective under high concurrency even when speculative decoding gains compress. We position Minima relative to recent tensor-network, low-rank plus quantization, and cross-layer sharing methods, and argue that it is a practical step toward more aggressive structural compression via shared tensor backbones with tiny per-layer adapters.

</details>


### [270] [AgroFlux: A Spatial-Temporal Benchmark for Carbon and Nitrogen Flux Prediction in Agricultural Ecosystems](https://arxiv.org/abs/2602.01614)
*Qi Cheng,Licheng Liu,Yao Zhang,Mu Hong,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TL;DR: 该研究创建了首个时空农业生态系统温室气体基准数据集，结合物理模型模拟和实际观测数据，评估了多种深度学习模型在碳氮通量预测中的性能，并探索了迁移学习提升模型泛化能力的方法。


<details>
  <summary>Details</summary>
Motivation: 农业生态系统占全球温室气体排放的四分之一，准确量化其碳、养分和水循环对于理解温室气体驱动因素和制定有效减缓策略至关重要。传统方法面临数据稀疏、时空异质性高、地下生物地球化学过程复杂等挑战，而AI模型的发展缺乏AI就绪的基准数据集和标准化协议。

Method: 1. 创建首个时空农业生态系统温室气体基准数据集，整合Ecosys和DayCent物理模型模拟数据、涡度协方差通量塔观测数据和受控环境设施数据；2. 评估多种序列深度学习模型（LSTM、时间CNN、Transformer）在碳氮通量预测中的性能；3. 探索迁移学习，利用模拟数据提升深度学习模型在实际观测数据上的泛化能力。

Result: 建立了首个农业生态系统温室气体基准数据集，为AI驱动的农业生态系统模型开发提供了标准化评估框架。通过比较不同深度学习架构，为碳氮通量预测提供了模型性能基准。迁移学习研究表明，利用模拟数据可以提升模型在实际观测数据上的泛化能力。

Conclusion: 该研究通过创建基准数据集和评估框架，推动了更准确、可扩展的AI驱动农业生态系统模型的发展，增进了对生态系统-气候相互作用的理解，为农业温室气体减排提供了重要的数据和方法支持。

Abstract: Agroecosystem, which heavily influenced by human actions and accounts for a quarter of global greenhouse gas emissions (GHGs), plays a crucial role in mitigating global climate change and securing environmental sustainability. However, we can't manage what we can't measure. Accurately quantifying the pools and fluxes in the carbon, nutrient, and water nexus of the agroecosystem is therefore essential for understanding the underlying drivers of GHG and developing effective mitigation strategies. Conventional approaches like soil sampling, process-based models, and black-box machine learning models are facing challenges such as data sparsity, high spatiotemporal heterogeneity, and complex subsurface biogeochemical and physical processes. Developing new trustworthy approaches such as AI-empowered models, will require the AI-ready benchmark dataset and outlined protocols, which unfortunately do not exist. In this work, we introduce a first-of-its-kind spatial-temporal agroecosystem GHG benchmark dataset that integrates physics-based model simulations from Ecosys and DayCent with real-world observations from eddy covariance flux towers and controlled-environment facilities. We evaluate the performance of various sequential deep learning models on carbon and nitrogen flux prediction, including LSTM-based models, temporal CNN-based model, and Transformer-based models. Furthermore, we explored transfer learning to leverage simulated data to improve the generalization of deep learning models on real-world observations. Our benchmark dataset and evaluation framework contribute to the development of more accurate and scalable AI-driven agroecosystem models, advancing our understanding of ecosystem-climate interactions.

</details>


### [271] [SUSD: Structured Unsupervised Skill Discovery through State Factorization](https://arxiv.org/abs/2602.01619)
*Seyed Mohammad Hadi Hosseini,Mahdieh Soleymani Baghshah*

Main category: cs.LG

TL;DR: SUSD提出了一种基于环境因子分解的无监督技能发现框架，通过将状态空间分解为独立组件并为不同因子分配技能变量，实现更细粒度的技能发现和控制。


<details>
  <summary>Details</summary>
Motivation: 现有无监督技能发现方法（如基于互信息的方法）倾向于发现简单、静态的技能，而基于距离的方法虽然能发现更动态的技能，但仍无法充分利用环境的结构化特性，难以发现全面覆盖所有可控因子或实体的技能集。

Method: SUSD将状态空间分解为独立因子（如物体或可控实体），为不同因子分配独立的技能变量，实现细粒度技能发现。同时使用动态模型跟踪各因子的学习进度，自适应地将智能体注意力引导到未充分探索的因子。

Result: 在包含1到10个因子的三个环境中，SUSD能够发现多样且复杂的技能，在因子化和复杂环境中显著优于现有无监督技能发现方法。该方法还产生了因子化的技能表示，支持对单个实体的细粒度解耦控制。

Conclusion: SUSD通过利用环境的结构化特性，实现了更丰富多样的技能发现，产生的因子化技能表示便于通过分层强化学习高效训练组合式下游任务，为无监督技能发现提供了新方向。

Abstract: Unsupervised Skill Discovery (USD) aims to autonomously learn a diverse set of skills without relying on extrinsic rewards. One of the most common USD approaches is to maximize the Mutual Information (MI) between skill latent variables and states. However, MI-based methods tend to favor simple, static skills due to their invariance properties, limiting the discovery of dynamic, task-relevant behaviors. Distance-Maximizing Skill Discovery (DSD) promotes more dynamic skills by leveraging state-space distances, yet still fall short in encouraging comprehensive skill sets that engage all controllable factors or entities in the environment. In this work, we introduce SUSD, a novel framework that harnesses the compositional structure of environments by factorizing the state space into independent components (e.g., objects or controllable entities). SUSD allocates distinct skill variables to different factors, enabling more fine-grained control on the skill discovery process. A dynamic model also tracks learning across factors, adaptively steering the agent's focus toward underexplored factors. This structured approach not only promotes the discovery of richer and more diverse skills, but also yields a factorized skill representation that enables fine-grained and disentangled control over individual entities which facilitates efficient training of compositional downstream tasks via Hierarchical Reinforcement Learning (HRL). Our experimental results across three environments, with factors ranging from 1 to 10, demonstrate that our method can discover diverse and complex skills without supervision, significantly outperforming existing unsupervised skill discovery methods in factorized and complex environments. Code is publicly available at: https://github.com/hadi-hosseini/SUSD.

</details>


### [272] [Toward Enhancing Representation Learning in Federated Multi-Task Settings](https://arxiv.org/abs/2602.01626)
*Mehdi Setayesh,Mahdi Beitollahi,Yasser H. Khalil,Hongliang Li*

Main category: cs.LG

TL;DR: FedMuscle：一种基于Muscle损失函数的联邦多任务学习框架，通过对比学习对齐异构模型的表示空间，解决模型和任务异质性问题


<details>
  <summary>Details</summary>
Motivation: 现有联邦多任务学习方法大多假设模型同构（完全或部分同质），限制了在实际场景中的应用。需要一种能够处理模型和任务异质性的方法。

Method: 提出Muscle损失函数，一种新颖的对比学习目标，同时对齐所有参与模型的表示。基于此开发FedMuscle算法，这是一个实用且通信高效的联邦多任务学习框架。

Result: 在多种图像和语言任务上的实验表明，FedMuscle始终优于现有最先进的基线方法，在异构设置下实现了显著改进和鲁棒性能。

Conclusion: FedMuscle通过共享表示空间而非模型参数，有效解决了联邦多任务学习中的模型和任务异质性问题，为实际应用提供了更灵活的解决方案。

Abstract: Federated multi-task learning (FMTL) seeks to collaboratively train customized models for users with different tasks while preserving data privacy. Most existing approaches assume model congruity (i.e., the use of fully or partially homogeneous models) across users, which limits their applicability in realistic settings. To overcome this limitation, we aim to learn a shared representation space across tasks rather than shared model parameters. To this end, we propose Muscle loss, a novel contrastive learning objective that simultaneously aligns representations from all participating models. Unlike existing multi-view or multi-model contrastive methods, which typically align models pairwise, Muscle loss can effectively capture dependencies across tasks because its minimization is equivalent to the maximization of mutual information among all the models' representations. Building on this principle, we develop FedMuscle, a practical and communication-efficient FMTL algorithm that naturally handles both model and task heterogeneity. Experiments on diverse image and language tasks demonstrate that FedMuscle consistently outperforms state-of-the-art baselines, delivering substantial improvements and robust performance across heterogeneous settings.

</details>


### [273] [AdaptNC: Adaptive Nonconformity Scores for Uncertainty-Aware Autonomous Systems in Dynamic Environments](https://arxiv.org/abs/2602.01629)
*Renukanandan Tumu,Aditya Singh,Rahul Mangharam*

Main category: cs.LG

TL;DR: AdaptNC：联合在线适应非共形分数参数和共形阈值，在分布偏移下保持目标覆盖率的同时显著减少预测区域体积


<details>
  <summary>Details</summary>
Motivation: 现实世界机器人应用中存在分布偏移，违反传统共形预测的交换性假设。现有在线CP方法仅自适应调整阈值，但使用固定的非共形分数函数，导致预测区域过于保守且体积效率低下。

Method: 提出AdaptNC框架，联合在线适应非共形分数参数和共形阈值。采用自适应重加权方案优化分数函数，并引入回放缓冲区机制缓解分数转换期间的覆盖率不稳定性。

Result: 在涉及多智能体策略变化、环境变化和传感器退化的多样化机器人基准测试中，AdaptNC在保持目标覆盖率的同时，相比仅调整阈值的最先进基线方法显著减少了预测区域体积。

Conclusion: AdaptNC通过联合适应分数函数和阈值，有效解决了分布偏移下共形预测的保守性问题，为自主系统在非结构化环境中的安全部署提供了更高效的预测区域。

Abstract: Rigorous uncertainty quantification is essential for the safe deployment of autonomous systems in unconstrained environments. Conformal Prediction (CP) provides a distribution-free framework for this task, yet its standard formulations rely on exchangeability assumptions that are violated by the distribution shifts inherent in real-world robotics. Existing online CP methods maintain target coverage by adaptively scaling the conformal threshold, but typically employ a static nonconformity score function. We show that this fixed geometry leads to highly conservative, volume-inefficient prediction regions when environments undergo structural shifts. To address this, we propose \textbf{AdaptNC}, a framework for the joint online adaptation of both the nonconformity score parameters and the conformal threshold. AdaptNC leverages an adaptive reweighting scheme to optimize score functions, and introduces a replay buffer mechanism to mitigate the coverage instability that occurs during score transitions. We evaluate AdaptNC on diverse robotic benchmarks involving multi-agent policy changes, environmental changes and sensor degradation. Our results demonstrate that AdaptNC significantly reduces prediction region volume compared to state-of-the-art threshold-only baselines while maintaining target coverage levels.

</details>


### [274] [COMET: Codebook-based Online-adaptive Multi-scale Embedding for Time-series Anomaly Detection](https://arxiv.org/abs/2602.01635)
*Jinwoo Park,Hyeongwon Kang,Seung Hun Han,Pilsung Kang*

Main category: cs.LG

TL;DR: COMET提出了一种基于码本的在线自适应多尺度嵌入方法，用于时间序列异常检测，通过多尺度补丁编码、向量量化核心集和在线码本适应三个组件，在多个基准数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测方法存在三个主要问题：1) 在补丁级表示学习中未能充分捕捉时间依赖性和多变量相关性；2) 依赖单尺度模式限制了不同时间范围异常的检测；3) 专注于正常数据表示使模型在推理时容易受到分布偏移的影响。

Method: COMET包含三个关键组件：1) 多尺度补丁编码：捕捉多个补丁尺度上的时间依赖性和变量间相关性；2) 向量量化核心集：通过码本学习代表性正常模式，使用量化误差和记忆距离的双重评分检测异常；3) 在线码本适应：基于码本条目生成伪标签，通过对比学习在推理时动态适应模型。

Result: 在五个基准数据集上的实验表明，COMET在45个评估指标中的36个上取得了最佳性能，验证了其在多样化环境中的有效性。

Conclusion: COMET通过多尺度表示学习、码本驱动的正常模式学习和在线自适应机制，有效解决了时间序列异常检测中的关键挑战，为工业应用提供了鲁棒的解决方案。

Abstract: Time series anomaly detection is a critical task across various industrial domains. However, capturing temporal dependencies and multivariate correlations within patch-level representation learning remains underexplored, and reliance on single-scale patterns limits the detection of anomalies across different temporal ranges. Furthermore, focusing on normal data representations makes models vulnerable to distribution shifts at inference time. To address these limitations, we propose Codebook-based Online-adaptive Multi-scale Embedding for Time-series anomaly detection (COMET), which consists of three key components: (1) Multi-scale Patch Encoding captures temporal dependencies and inter-variable correlations across multiple patch scales. (2) Vector-Quantized Coreset learns representative normal patterns via codebook and detects anomalies with a dual-score combining quantization error and memory distance. (3) Online Codebook Adaptation generates pseudo-labels based on codebook entries and dynamically adapts the model at inference through contrastive learning. Experiments on five benchmark datasets demonstrate that COMET achieves the best performance in 36 out of 45 evaluation metrics, validating its effectiveness across diverse environments.

</details>


### [275] [Chance-Constrained Inference for Hallucination Risk Control in Large Language Models](https://arxiv.org/abs/2602.01637)
*Sreenivasan Mohandas*

Main category: cs.LG

TL;DR: 提出机会约束推理方法，通过有限样本自适应验证可行性，为语言模型生成提供概率风险保证，控制幻觉频率。


<details>
  <summary>Details</summary>
Motivation: 现有方法只能降低平均错误率，无法控制重复使用中的幻觉频率，需要提供明确的概率风险保证。

Method: 将推理建模为部署时风险控制问题，提出机会约束推理，使用顺序、任意有效的推理过程，自适应验证可行性或不可行性。

Result: 在NaturalQuestions风格问题和受控多跳问答上实现可靠的风险控制，能早期检测内在不可行输入，在重复使用下安全组合，而置信度基线无法提供一致保证。

Conclusion: 机会约束推理为语言模型生成提供概率风险保证，有效控制幻觉频率，优于传统置信度方法。

Abstract: Large language models generate outputs stochastically and may produce fluent but invalid responses, including factual hallucinations. Existing mitigation strategies reduce average error rates but do not provide explicit control over the \emph{frequency} of such failures under repeated use. We formulate inference as a deployment-time risk control problem and introduce \emph{chance-constrained inference}, which directly bounds the probability of hallucinations among accepted generations. Hallucinations are modeled as stochastic constraint violations, and we show that confidence-based selective prediction does not, in general, imply probabilistic risk guarantees. To enforce chance constraints efficiently, we propose a sequential, anytime-valid inference procedure that adaptively certifies feasibility or infeasibility using finite samples, avoiding conservative fixed-sample bounds. Experiments on questions inspired by NaturalQuestions and controlled multi-hop question answering demonstrate reliable risk control, early detection of intrinsically infeasible inputs, and safe composition under repeated use, while confidence-based baselines fail to provide consistent guarantees.

</details>


### [276] [The Effect of Mini-Batch Noise on the Implicit Bias of Adam](https://arxiv.org/abs/2602.01642)
*Matias D. Cattaneo,Boris Shigida*

Main category: cs.LG

TL;DR: 本文提出理论框架分析Adam优化器中动量超参数(β₁, β₂)和批次大小如何通过小批量噪声影响隐式偏置，研究其在多轮训练中对泛化能力的影响。


<details>
  <summary>Details</summary>
Motivation: 随着高质量数据有限而计算资源增长，多轮训练在深度学习各领域重新变得重要。Adam优化器作为许多任务的首选，其动量超参数和批次大小对泛化能力的影响机制尚不明确，需要理论框架来理解小批量噪声如何通过Adam的内存机制影响损失景观的平坦/尖锐区域选择。

Method: 建立理论框架分析小批量噪声如何影响Adam内存的隐式偏置，研究β₁、β₂和批次大小之间的相互作用。通过理论推导连接批次大小尺度变化与临界批次尺寸的关系，并在即将过拟合的小规模数据上进行实验验证。

Result: 发现批次大小不同时，β₁和β₂对正则化效果的影响方向会发生反转：大批次时高β₂增加反正则化（损害泛化），小批次时依赖关系反转。常用默认值(0.9, 0.999)适合小批次；大批次时使β₁接近β₂能获得更好的验证精度。理论推导显示这种反转发生的批次大小尺度与临界批次尺寸相关。

Conclusion: Adam优化器的超参数选择应基于批次大小进行调整，而非使用固定默认值。大批次训练时应使β₁接近β₂以获得更好的泛化性能，这为多轮训练中的优化器调优提供了理论指导。

Abstract: With limited high-quality data and growing compute, multi-epoch training is gaining back its importance across sub-areas of deep learning. Adam(W), versions of which are go-to optimizers for many tasks such as next token prediction, has two momentum hyperparameters $(β_1, β_2)$ controlling memory and one very important hyperparameter, batch size, controlling (in particular) the amount mini-batch noise. We introduce a theoretical framework to understand how mini-batch noise influences the implicit bias of memory in Adam (depending on $β_1$, $β_2$) towards sharper or flatter regions of the loss landscape, which is commonly observed to correlate with the generalization gap in multi-epoch training. We find that in the case of large batch sizes, higher $β_2$ increases the magnitude of anti-regularization by memory (hurting generalization), but as the batch size becomes smaller, the dependence of (anti-)regulariation on $β_2$ is reversed. A similar monotonicity shift (in the opposite direction) happens in $β_1$. In particular, the commonly "default" pair $(β_1, β_2) = (0.9, 0.999)$ is a good choice if batches are small; for larger batches, in many settings moving $β_1$ closer to $β_2$ is much better in terms of validation accuracy in multi-epoch training. Moreover, our theoretical derivations connect the scale of the batch size at which the shift happens to the scale of the critical batch size. We illustrate this effect in experiments with small-scale data in the about-to-overfit regime.

</details>


### [277] [De Novo Molecular Generation from Mass Spectra via Many-Body Enhanced Diffusion](https://arxiv.org/abs/2602.01643)
*Xichen Sun,Wentao Wei,Jiahua Rao,Jiancong Xie,Yuedong Yang*

Main category: cs.LG

TL;DR: MBGen：基于多体增强扩散框架的质谱分子结构从头生成方法，通过多体注意力机制和高阶边建模，显著提升质谱分子结构生成和异构体区分能力。


<details>
  <summary>Details</summary>
Motivation: 现有质谱分子结构生成方法主要采用原子中心和成对相互作用建模，忽略了高阶边相互作用，无法系统捕捉多体特征，限制了复杂异构体和非局部碎裂机制解析能力。

Method: 提出MBGen多体增强扩散框架，集成多体注意力机制和高阶边建模，充分利用MS/MS质谱中编码的丰富结构信息，实现准确的分子结构从头生成和异构体区分。

Result: 在NPLIB1和MassSpecGym基准测试中，MBGen性能显著优于现有方法，提升幅度最高达230%，有效捕捉高阶相互作用，对复杂异构体和非局部碎裂信息表现出增强的敏感性。

Conclusion: 多体建模在质谱分子结构生成中具有重要科学价值和实际应用意义，MBGen框架通过高阶相互作用建模显著提升了质谱分子结构生成的准确性和异构体区分能力。

Abstract: Molecular structure generation from mass spectrometry is fundamental for understanding cellular metabolism and discovering novel compounds. Although tandem mass spectrometry (MS/MS) enables the high-throughput acquisition of fragment fingerprints, these spectra often reflect higher-order interactions involving the concerted cleavage of multiple atoms and bonds-crucial for resolving complex isomers and non-local fragmentation mechanisms. However, most existing methods adopt atom-centric and pairwise interaction modeling, overlooking higher-order edge interactions and lacking the capacity to systematically capture essential many-body characteristics for structure generation. To overcome these limitations, we present MBGen, a Many-Body enhanced diffusion framework for de novo molecular structure Generation from mass spectra. By integrating a many-body attention mechanism and higher-order edge modeling, MBGen comprehensively leverages the rich structural information encoded in MS/MS spectra, enabling accurate de novo generation and isomer differentiation for novel molecules. Experimental results on the NPLIB1 and MassSpecGym benchmarks demonstrate that MBGen achieves superior performance, with improvements of up to 230% over state-of-the-art methods, highlighting the scientific value and practical utility of many-body modeling for mass spectrometry-based molecular generation. Further analysis and ablation studies show that our approach effectively captures higher-order interactions and exhibits enhanced sensitivity to complex isomeric and non-local fragmentation information.

</details>


### [278] [From Perception to Action: Spatial AI Agents and World Models](https://arxiv.org/abs/2602.01644)
*Gloria Felicia,Nolan Bryant,Handi Putra,Ayaan Gazali,Eliel Lobo,Esteban Rojas*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的三轴分类法，将智能体能力与空间任务连接起来，强调空间智能对于具身智能体的重要性，并识别了三个关键发现和六个重大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究要么单独关注智能体架构，要么单独关注空间领域，缺乏一个统一的框架来连接这两种互补能力。大语言模型在符号领域的成功不能直接转化到物理世界，空间智能（感知3D结构、推理物体关系和在物理约束下行动的能力）对于具身智能体至关重要。

Method: 通过对2000多篇论文的全面综述（引用742篇顶级会议论文），提出了一个统一的三轴分类法，将智能体能力与跨尺度的空间任务连接起来。区分了空间基础（对几何和物理的度量理解）与符号基础（将图像与文本关联）。

Result: 分析揭示了三个关键发现：(1) 分层记忆系统对于长时程空间任务很重要；(2) GNN-LLM集成是结构化空间推理的有前景方法；(3) 世界模型对于跨微观到宏观空间尺度的安全部署至关重要。

Conclusion: 该分类法为统一碎片化的研究努力奠定了基础，并提出了六个重大挑战和未来研究方向，包括需要统一评估框架来标准化跨领域评估，以推动机器人、自动驾驶和地理空间智能等领域下一代空间感知自主系统的发展。

Abstract: While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physical constraints, is an orthogonal capability that proves important for embodied agents. Existing surveys address either agentic architectures or spatial domains in isolation. None provide a unified framework connecting these complementary capabilities. This paper bridges that gap. Through a thorough review of over 2,000 papers, citing 742 works from top-tier venues, we introduce a unified three-axis taxonomy connecting agentic capabilities with spatial tasks across scales. Crucially, we distinguish spatial grounding (metric understanding of geometry and physics) from symbolic grounding (associating images with text), arguing that perception alone does not confer agency. Our analysis reveals three key findings mapped to these axes: (1) hierarchical memory systems (Capability axis) are important for long-horizon spatial tasks. (2) GNN-LLM integration (Task axis) is a promising approach for structured spatial reasoning. (3) World models (Scale axis) are essential for safe deployment across micro-to-macro spatial scales. We conclude by identifying six grand challenges and outlining directions for future research, including the need for unified evaluation frameworks to standardize cross-domain assessment. This taxonomy provides a foundation for unifying fragmented research efforts and enabling the next generation of spatially-aware autonomous systems in robotics, autonomous vehicles, and geospatial intelligence.

</details>


### [279] [On the Spatiotemporal Dynamics of Generalization in Neural Networks](https://arxiv.org/abs/2602.01651)
*Zichao Wei*

Main category: cs.LG

TL;DR: 神经网络无法将加法从16位数推广到32位数，而儿童学习规则后能应用于任意长序列。作者认为这是违反物理公设的问题，提出满足局部性、对称性、稳定性三个约束的SEAD架构，在奇偶性、加法、Rule 110等任务上实现了完美长度泛化。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在算术任务上长度泛化失败的问题。作者认为这不是工程问题，而是违反了物理计算的基本公设。受物理学启发，提出任何泛化系统必须满足的三个约束条件。

Method: 从物理公设推导出SEAD架构：局部性（信息有限速度传播）、对称性（计算法则时空不变）、稳定性（收敛到离散吸引子抵抗噪声）。SEAD是一种神经细胞自动机，通过迭代局部卷积规则直到收敛。

Result: 在三个任务上验证：1) 奇偶性：通过光锥传播实现完美长度泛化；2) 加法：从L=16到L=100万实现100%准确率的尺度不变推理，展示输入自适应计算；3) Rule 110：学习图灵完备细胞自动机而无轨迹发散。

Conclusion: 统计学习与逻辑推理之间的鸿沟可以通过尊重计算物理而非简单缩放参数来弥合。SEAD架构为构建具有真正泛化能力的计算系统提供了物理基础。

Abstract: Why do neural networks fail to generalize addition from 16-digit to 32-digit numbers, while a child who learns the rule can apply it to arbitrarily long sequences? We argue that this failure is not an engineering problem but a violation of physical postulates. Drawing inspiration from physics, we identify three constraints that any generalizing system must satisfy: (1) Locality -- information propagates at finite speed; (2) Symmetry -- the laws of computation are invariant across space and time; (3) Stability -- the system converges to discrete attractors that resist noise accumulation. From these postulates, we derive -- rather than design -- the Spatiotemporal Evolution with Attractor Dynamics (SEAD) architecture: a neural cellular automaton where local convolutional rules are iterated until convergence. Experiments on three tasks validate our theory: (1) Parity -- demonstrating perfect length generalization via light-cone propagation; (2) Addition -- achieving scale-invariant inference from L=16 to L=1 million with 100% accuracy, exhibiting input-adaptive computation; (3) Rule 110 -- learning a Turing-complete cellular automaton without trajectory divergence. Our results suggest that the gap between statistical learning and logical reasoning can be bridged -- not by scaling parameters, but by respecting the physics of computation.

</details>


### [280] [Efficient Adversarial Attacks on High-dimensional Offline Bandits](https://arxiv.org/abs/2602.01658)
*Seyed Mohammad Hadi Hosseini,Amir Najafi,Mahdieh Soleymani Baghshah*

Main category: cs.LG

TL;DR: 本文研究了离线bandit评估中奖励模型对抗攻击的脆弱性，发现即使对奖励模型权重进行微小扰动也能显著改变bandit行为，且高维输入下攻击更容易成功。


<details>
  <summary>Details</summary>
Motivation: 离线bandit评估已成为评估生成模型的重要方法，但现有研究忽略了奖励模型对抗攻击的鲁棒性问题。当攻击者在bandit训练前扰动奖励模型（而非训练数据）时，系统安全性未知。

Method: 提出新型威胁模型，攻击者利用高维离线数据劫持bandit行为。从线性奖励函数扩展到非线性模型（如ReLU神经网络），在Hugging Face的两个生成模型评估器（美学质量和组合对齐）上进行攻击实验。

Result: 实验表明：1）随机扰动无效，但针对性扰动攻击成功率接近完美；2）理论证明高维效应：输入维度增加时，成功攻击所需扰动范数减少，使图像评估等现代应用特别脆弱。

Conclusion: 离线bandit训练对奖励模型对抗攻击高度脆弱，即使微小扰动也能显著改变bandit行为。高维应用（如图像评估）尤其危险，需要开发更鲁棒的离线评估方法。

Abstract: Bandit algorithms have recently emerged as a powerful tool for evaluating machine learning models, including generative image models and large language models, by efficiently identifying top-performing candidates without exhaustive comparisons. These methods typically rely on a reward model, often distributed with public weights on platforms such as Hugging Face, to provide feedback to the bandit. While online evaluation is expensive and requires repeated trials, offline evaluation with logged data has become an attractive alternative. However, the adversarial robustness of offline bandit evaluation remains largely unexplored, particularly when an attacker perturbs the reward model (rather than the training data) prior to bandit training. In this work, we fill this gap by investigating, both theoretically and empirically, the vulnerability of offline bandit training to adversarial manipulations of the reward model. We introduce a novel threat model in which an attacker exploits offline data in high-dimensional settings to hijack the bandit's behavior. Starting with linear reward functions and extending to nonlinear models such as ReLU neural networks, we study attacks on two Hugging Face evaluators used for generative model assessment: one measuring aesthetic quality and the other assessing compositional alignment. Our results show that even small, imperceptible perturbations to the reward model's weights can drastically alter the bandit's behavior. From a theoretical perspective, we prove a striking high-dimensional effect: as input dimensionality increases, the perturbation norm required for a successful attack decreases, making modern applications such as image evaluation especially vulnerable. Extensive experiments confirm that naive random perturbations are ineffective, whereas carefully targeted perturbations achieve near-perfect attack success rates ...

</details>


### [281] [Quantifying Epistemic Predictive Uncertainty in Conformal Prediction](https://arxiv.org/abs/2602.01667)
*Siu Lun Chau,Soroush H. Zargarbashi,Yusuf Sale,Michele Caprio*

Main category: cs.LG

TL;DR: 该论文研究如何在共形预测框架下量化认知预测不确定性，提出基于最大平均不精确度的计算高效方法，并在主动学习和选择性分类中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究如何在共形预测框架下量化认知预测不确定性，即由于存在多个合理预测模型而产生的预测时不确定性。现有方法主要依赖共形预测区域大小，但这种方法无法提供细粒度的不确定性评估。

Method: 首先证明在分裂共形预测中，共形预测区域与诱导的信度集中所有分布分配概率至少为1-α的标签集合完全一致。然后提出基于最大平均不精确度的计算高效且分析可处理的认知预测不确定性度量方法，通过测量诱导信度集中的信息冲突程度来量化不确定性。

Result: 在主动学习和选择性分类任务上的实验表明，量化的认知预测不确定性比仅依赖共形预测区域大小提供了更丰富和细粒度的不确定性评估，证明了该方法的有效性。

Conclusion: 该工作展示了共形预测作为在认知不确定性下进行决策的原则性基础的潜力，提出的不确定性度量方法能够提供更精细的不确定性评估，有助于在不确定性下的决策制定。

Abstract: We study the problem of quantifying epistemic predictive uncertainty (EPU) -- that is, uncertainty faced at prediction time due to the existence of multiple plausible predictive models -- within the framework of conformal prediction (CP). To expose the implicit model multiplicity underlying CP, we build on recent results showing that, under a mild assumption, any full CP procedure induces a set of closed and convex predictive distributions, commonly referred to as a credal set. Importantly, the conformal prediction region (CPR) coincides exactly with the set of labels to which all distributions in the induced credal set assign probability at least $1-α$. As our first contribution, we prove that this characterisation also holds in split CP. Building on this connection, we then propose a computationally efficient and analytically tractable uncertainty measure, based on \emph{Maximum Mean Imprecision}, to quantify the EPU by measuring the degree of conflicting information within the induced credal set. Experiments on active learning and selective classification demonstrate that the quantified EPU provides substantially more informative and fine-grained uncertainty assessments than reliance on CPR size alone. More broadly, this work highlights the potential of CP serving as a principled basis for decision-making under epistemic uncertainty.

</details>


### [282] [ASGMamba: Adaptive Spectral Gating Mamba for Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.01668)
*Qianyang Li,Xingjun Zhang,Shaoxun Wang,Jia Wei,Yueqi Xing*

Main category: cs.LG

TL;DR: ASGMamba：一种用于资源受限超算环境的高效多变量时间序列预测框架，结合自适应谱门控机制和Mamba骨干网络，在保持线性复杂度的同时显著降低内存使用


<details>
  <summary>Details</summary>
Motivation: 现有方法面临两难：Transformer模型具有二次复杂度，限制了长序列的可扩展性；而线性状态空间模型难以区分有价值信号和高频噪声，导致状态容量浪费。需要为资源受限的超算环境设计高效预测框架

Method: 提出ASGMamba框架：1) 轻量级自适应谱门控机制，基于局部谱能量动态过滤噪声；2) Mamba骨干网络专注于鲁棒的时间动态；3) 分层多尺度架构，包含变量特定的节点嵌入以捕捉不同的物理特性

Result: 在9个基准测试上达到最先进的准确性，在保持严格O(L)复杂度的同时，显著降低了长时程任务的内存使用

Conclusion: ASGMamba为资源受限环境中的高吞吐量预测提供了一个可扩展的解决方案，在效率和准确性之间取得了良好平衡

Abstract: Long-term multivariate time series forecasting (LTSF) plays a crucial role in various high-performance computing applications, including real-time energy grid management and large-scale traffic flow simulation. However, existing solutions face a dilemma: Transformer-based models suffer from quadratic complexity, limiting their scalability on long sequences, while linear State Space Models (SSMs) often struggle to distinguish valuable signals from high-frequency noise, leading to wasted state capacity. To bridge this gap, we propose ASGMamba, an efficient forecasting framework designed for resource-constrained supercomputing environments. ASGMamba integrates a lightweight Adaptive Spectral Gating (ASG) mechanism that dynamically filters noise based on local spectral energy, enabling the Mamba backbone to focus its state evolution on robust temporal dynamics. Furthermore, we introduce a hierarchical multi-scale architecture with variable-specific Node Embeddings to capture diverse physical characteristics. Extensive experiments on nine benchmarks demonstrate that ASGMamba achieves state-of-the-art accuracy. While keeping strictly $$\mathcal{O}(L)$$ complexity we significantly reduce the memory usage on long-horizon tasks, thus establishing ASGMamba as a scalable solution for high-throughput forecasting in resource limited environments.The code is available at https://github.com/hit636/ASGMamba

</details>


### [283] [Finite and Corruption-Robust Regret Bounds in Online Inverse Linear Optimization under M-Convex Action Sets](https://arxiv.org/abs/2602.01682)
*Taihei Oki,Shinsaku Sakaue*

Main category: cs.LG

TL;DR: 该论文研究了在线逆线性优化问题，证明了当可行集是M-凸集时，可以实现O(d log d)的有限遗憾界，解决了该领域的一个开放性问题。


<details>
  <summary>Details</summary>
Motivation: 在线逆线性优化（也称为上下文推荐）中，学习者需要从随时间变化的可行集中观察最优动作来推断代理的隐藏目标向量。先前研究建立了O(d log T)的遗憾界和指数级有限界exp(O(d log d))，而Ω(d)的下界已知。是否存在多项式有限遗憾界一直是一个开放性问题。

Method: 结合M-凸集上最优解的结构特征和几何体积论证方法。对于对抗性反馈，通过监测观察反馈诱导的有向图来自适应检测腐败，无需事先知道腐败轮数C。

Result: 当可行集是M-凸集时，实现了O(d log d)的有限遗憾界。对于最多C轮对抗性腐败反馈，获得了O((C+1)d log d)的遗憾界，且无需事先知道C。

Conclusion: 该研究部分解决了在线逆线性优化中是否存在多项式有限遗憾界的开放性问题，证明了在M-凸集这一广泛类别下可以实现O(d log d)的有限遗憾界，并将方法扩展到对抗性腐败反馈场景。

Abstract: We study online inverse linear optimization, also known as contextual recommendation, where a learner sequentially infers an agent's hidden objective vector from observed optimal actions over feasible sets that change over time. The learner aims to recommend actions that perform well under the agent's true objective, and the performance is measured by the regret, defined as the cumulative gap between the agent's optimal values and those achieved by the learner's recommended actions. Prior work has established a regret bound of $O(d\log T)$, as well as a finite but exponentially large bound of $\exp(O(d\log d))$, where $d$ is the dimension of the optimization problem and $T$ is the time horizon, while a regret lower bound of $Ω(d)$ is known (Gollapudi et al. 2021; Sakaue et al. 2025). Whether a finite regret bound polynomial in $d$ is achievable or not has remained an open question. We partially resolve this by showing that when the feasible sets are M-convex -- a broad class that includes matroids -- a finite regret bound of $O(d\log d)$ is possible. We achieve this by combining a structural characterization of optimal solutions on M-convex sets with a geometric volume argument. Moreover, we extend our approach to adversarially corrupted feedback in up to $C$ rounds. We obtain a regret bound of $O((C+1)d\log d)$ without prior knowledge of $C$, by monitoring directed graphs induced by the observed feedback to detect corruptions adaptively.

</details>


### [284] [Semantic-aware Wasserstein Policy Regularization for Large Language Model Alignment](https://arxiv.org/abs/2602.01685)
*Byeonghu Na,Hyungho Na,Yeongmin Kim,Suhyeon Jo,HeeSun Bae,Mina Kang,Il-Chul Moon*

Main category: cs.LG

TL;DR: 提出Wasserstein Policy Regularization (WPR)，一种基于熵正则化Wasserstein距离的语义感知正则化方法，用于改进RLHF框架中的策略对齐。


<details>
  <summary>Details</summary>
Motivation: 传统的RLHF方法使用KL散度及其f-散度变体作为正则化项，但这些方法只比较相同索引位置的token概率，无法捕捉语义相似性。需要一种能够考虑token空间几何结构的语义感知正则化方法。

Method: 提出Wasserstein Policy Regularization (WPR)，基于熵正则化Wasserstein距离，将token空间的几何结构纳入考虑。通过距离的对偶形式，将正则化表示为通过最优对偶变量应用于奖励的惩罚项，得到与标准RL算法兼容的可处理目标。

Result: 实验表明，该方法在性能上优于基于KL散度和f-散度的基线方法，证明了语义感知策略距离在模型对齐中的优势。

Conclusion: WPR通过引入语义感知的正则化，改进了RLHF框架中的策略对齐，能够更好地捕捉语言模型输出之间的语义相似性，为LLM对齐提供了更有效的方法。

Abstract: Large language models (LLMs) are commonly aligned with human preferences using reinforcement learning from human feedback (RLHF). In this method, LLM policies are generally optimized through reward maximization with Kullback-Leibler (KL) divergence regularization of the reference policy. However, KL and its $f$-divergence variants only compare token probabilities at identical indices, failing to capture semantic similarity. We propose Wasserstein Policy Regularization (WPR), a semantic-aware regularization for the RLHF framework based on the entropy-regularized Wasserstein distance, which incorporates the geometry of the token space. The dual formulation of the distance expresses the regularization as penalty terms applied to the reward via optimal dual variables, which yield a tractable objective compatible with standard RL algorithms. Empirically, our method outperforms KL- and $f$-divergence-based baselines, demonstrating the benefits of semantic-aware policy distances for alignment. Our code is available at https://github.com/aailab-kaist/WPR.

</details>


### [285] [$\textbf{AGT$^{AO}$}$: Robust and Stabilized LLM Unlearning via Adversarial Gating Training with Adaptive Orthogonality](https://arxiv.org/abs/2602.01703)
*Pengyu Li,Lingling Zhang,Zhitao Gao,Yanrui Wu,Yuxuan Dong,Huan Liu,Bifan Wei,Jun Liu*

Main category: cs.LG

TL;DR: 提出AGT^AO框架，通过自适应正交化和对抗门控训练，解决大语言模型遗忘敏感数据时面临的灾难性遗忘与表面遗忘的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型会无意中记忆敏感数据，带来隐私和安全风险。现有遗忘方法面临根本困境：激进遗忘会导致灾难性遗忘损害模型效用，保守策略则可能只是表面遗忘，模型仍易受对抗恢复攻击。

Method: 提出AGT^AO统一框架：1) 自适应正交化(AO)动态缓解遗忘与保留目标间的梯度冲突；2) 对抗门控训练(AGT)将遗忘建模为潜在空间最小最大博弈，使用课程式门控机制模拟和对抗内部恢复尝试。

Result: 实验表明AGT^AO在遗忘效果(KUR≈0.01)和模型效用(MMLU 58.30)之间取得了优越的权衡。

Conclusion: AGT^AO框架有效解决了大语言模型遗忘敏感数据时的权衡问题，实现了稳健擦除与效用保持的平衡。

Abstract: While Large Language Models (LLMs) have achieved remarkable capabilities, they unintentionally memorize sensitive data, posing critical privacy and security risks. Machine unlearning is pivotal for mitigating these risks, yet existing paradigms face a fundamental dilemma: aggressive unlearning often induces catastrophic forgetting that degrades model utility, whereas conservative strategies risk superficial forgetting, leaving models vulnerable to adversarial recovery. To address this trade-off, we propose $\textbf{AGT$^{AO}$}$ (Adversarial Gating Training with Adaptive Orthogonality), a unified framework designed to reconcile robust erasure with utility preservation. Specifically, our approach introduces $\textbf{Adaptive Orthogonality (AO)}$ to dynamically mitigate geometric gradient conflicts between forgetting and retention objectives, thereby minimizing unintended knowledge degradation. Concurrently, $\textbf{Adversarial Gating Training (AGT)}$ formulates unlearning as a latent-space min-max game, employing a curriculum-based gating mechanism to simulate and counter internal recovery attempts. Extensive experiments demonstrate that $\textbf{AGT$^{AO}$}$ achieves a superior trade-off between unlearning efficacy (KUR $\approx$ 0.01) and model utility (MMLU 58.30). Code is available at https://github.com/TiezMind/AGT-unlearning.

</details>


### [286] [Beyond Mode Elicitation: Diversity-Preserving Reinforcement Learning via Latent Diffusion Reasoner](https://arxiv.org/abs/2602.01705)
*Haoqiang Kang,Yizhe Zhang,Nikki Lijing Kuang,Yi-An Ma,Lianhui Qin*

Main category: cs.LG

TL;DR: LaDi-RL：通过潜在扩散模型在连续潜在空间进行探索的强化学习框架，解决离散RL中推理多样性崩溃问题，在代码生成和数学推理任务上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法优化离散的思维链生成时，由于策略熵降低和模式激发行为，会导致探索多样性崩溃，限制了推理能力的提升

Method: 提出LaDi-RL框架：1）在连续潜在空间进行探索，潜在变量编码语义级推理轨迹；2）通过引导扩散建模探索，多步去噪分布随机性并保持多个共存解模式；3）解耦潜在空间探索和文本空间生成，结合潜在探索和文本策略

Result: 在代码生成和数学推理基准测试中，相比离散RL基线，pass@1绝对提升+9.4%（代码生成）和+5.7%（数学推理），pass@k也有持续改进

Conclusion: 基于扩散的潜在RL是离散词元级RL的有原则替代方案，能更有效地优化LLM推理能力，同时保持解决方案多样性

Abstract: Recent reinforcement learning (RL) methods improve LLM reasoning by optimizing discrete Chain-of-Thought (CoT) generation; however, exploration in token space often suffers from diversity collapse as policy entropy decreases due to mode elicitation behavior in discrete RL. To mitigate this issue, we propose Latent Diffusion Reasoning with Reinforcement Learning (LaDi-RL), a framework that conducts exploration directly in a continuous latent space, where latent variables encode semantic-level reasoning trajectories. By modeling exploration via guided diffusion, multi-step denoising distributes stochasticity and preserves multiple coexisting solution modes without mutual suppression. Furthermore, by decoupling latent-space exploration from text-space generation, we show that latent diffusion-based optimization is more effective than text-space policy optimization alone, while a complementary text policy provides additional gains when combined with latent exploration. Experiments on code generation and mathematical reasoning benchmarks demonstrate consistent improvements in both pass@1 and pass@k over discrete RL baselines, with absolute pass@1 gains of +9.4% on code generation and +5.7% on mathematical reasoning, highlighting diffusion-based latent RL as a principled alternative to discrete token-level RL for reasoning.

</details>


### [287] [Revisiting Generalization Measures Beyond IID: An Empirical Study under Distributional Shift](https://arxiv.org/abs/2602.01718)
*Sora Nakai,Youssef Fadhloun,Kacem Mathlouthi,Kotaro Yoshida,Ganesh Talluri,Ioannis Mitliagkas,Hiroki Naganuma*

Main category: cs.LG

TL;DR: 该研究大规模评估了40多种泛化度量方法在分布偏移下的鲁棒性，发现许多方法在分布偏移下性能显著变化，只有少数方法能保持相对稳定。


<details>
  <summary>Details</summary>
Motivation: 深度学习泛化问题尚未解决，特别是在预测模型在训练分布之外的性能方面。现有研究存在局限性：Jiang等人的大规模研究主要关注IID设置，而Dziugaite等人指出泛化度量在不同训练配置下不稳定。需要评估泛化度量在更广泛场景下的鲁棒性。

Method: 1) 训练小到中型模型，覆盖10,000个超参数配置；2) 评估40多种仅从训练模型和训练数据可计算的泛化度量；3) 扩展实验范围：a) 从标准IID扩展到多样化的分布偏移场景，b) 评估多种架构和训练方案，c) 新纳入基于校准和信息准则的度量方法。

Result: 分布偏移显著改变了许多泛化度量的预测性能，只有较小的子集在不同设置下保持相对稳定。这表明泛化度量的有效性高度依赖于评估环境。

Conclusion: 泛化度量的鲁棒性在分布偏移下存在显著差异，需要谨慎选择适合特定场景的度量方法。该研究为理解泛化度量在非IID环境下的表现提供了重要基准。

Abstract: Generalization remains a central yet unresolved challenge in deep learning, particularly the ability to predict a model's performance beyond its training distribution using quantities available prior to test-time evaluation. Building on the large-scale study of Jiang et al. (2020). and concerns by Dziugaite et al. (2020). about instability across training configurations, we benchmark the robustness of generalization measures beyond IID regime. We train small-to-medium models over 10,000 hyperparameter configurations and evaluate more than 40 measures computable from the trained model and the available training data alone. We significantly broaden the experimental scope along multiple axes: (i) extending the evaluation beyond the standard IID setting to include benchmarking for robustness across diverse distribution shifts, (ii) evaluating multiple architectures and training recipes, and (iii) newly incorporating calibration- and information-criteria-based measures to assess their alignment with both IID and OOD generalization. We find that distribution shifts can substantially alter the predictive performance of many generalization measures, while a smaller subset remains comparatively stable across settings.

</details>


### [288] [MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration](https://arxiv.org/abs/2602.01734)
*Lianhai Ren,Yucheng Ding,Xiao Liu,Qianxiao Li,Peng Cheng,Yeyun Gong*

Main category: cs.LG

TL;DR: 论文提出MSign优化器，通过周期性应用矩阵符号操作恢复稳定秩，有效防止LLM预训练中的梯度爆炸问题，计算开销小于7.0%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型预训练中的训练不稳定性是一个关键挑战，表现为突然的梯度爆炸，浪费大量计算资源。研究旨在理解并解决这一训练失败问题。

Method: 通过μP缩放的5M参数NanoGPT模型研究训练失败，发现崩溃前的两个关键现象：权重矩阵稳定秩快速下降和相邻层雅可比矩阵对齐度增加。提出MSign优化器，周期性应用矩阵符号操作来恢复稳定秩。

Result: 从5M到3B参数的模型实验表明，MSign能有效防止训练失败，计算开销小于7.0%。理论证明稳定秩下降和雅可比矩阵对齐共同导致梯度范数随网络深度指数增长。

Conclusion: MSign通过打破训练不稳定性机制，为LLM预训练提供了有效的稳定性解决方案，显著减少计算资源浪费。

Abstract: Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via $μ$P, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.

</details>


### [289] [Position: The Inevitable End of One-Architecture-Fits-All-Domains in Time Series Forecasting](https://arxiv.org/abs/2602.01736)
*Qinwei Ma,Jingzhe Shi,Jiahao Qiu,Zaiwen Yang*

Main category: cs.LG

TL;DR: 该论文质疑通用领域时间序列预测神经网络架构的有效性，指出其与特定领域SOTA存在不可调和的冲突，呼吁研究重点转向特定领域深度学习或通用领域元学习方法


<details>
  <summary>Details</summary>
Motivation: 近年来，时间序列预测的神经网络架构变得越来越复杂，但性能已趋于饱和。通用领域架构与特定领域（金融、天气、交通等）的实际需求脱节，特定领域开发的方法很少利用近2-3年时间序列社区的神经网络进展，导致通用架构研究失去实践意义

Method: 通过分析现有研究，总结通用领域时间序列神经网络架构的固有局限性：单/少数相似领域SOTA与通用领域泛化能力之间的不可调和冲突。论文没有提出新的技术方法，而是对研究现状进行批判性分析并提出研究方向转变建议

Result: 识别出通用领域时间序列神经网络架构研究的饱和状态，以及这些架构与特定领域实际需求之间的脱节。特定领域（金融、天气、交通等）开发的方法很少采用近2-3年时间序列社区的神经网络进展

Conclusion: 呼吁时间序列社区转变研究方向：1）专注于特定领域的深度学习方法；2）转向通用领域的元学习方法开发。通用领域时间序列神经网络架构研究已饱和且远离特定领域SOTA，需要新的研究方向

Abstract: Recent work has questioned the effectiveness and robustness of neural network architectures for time series forecasting tasks. We summarize these concerns and analyze groundly their inherent limitations: i.e. the irreconcilable conflict between single (or few similar) domains SOTA and generalizability over general domains for time series forecasting neural network architecture designs. Moreover, neural networks architectures for general domain time series forecasting are becoming more and more complicated and their performance has almost saturated in recent years. As a result, network architectures developed aiming at fitting general time series domains are almost not inspiring for real world practices for certain single (or few similar) domains such as Finance, Weather, Traffic, etc: each specific domain develops their own methods that rarely utilize advances in neural network architectures of time series community in recent 2-3 years. As a result, we call for the time series community to shift focus away from research on time series neural network architectures for general domains: these researches have become saturated and away from domain-specific SOTAs over time. We should either (1) focus on deep learning methods for certain specific domain(s), or (2) turn to the development of meta-learning methods for general domains.

</details>


### [290] [Softmax Linear Attention: Reclaiming Global Competition](https://arxiv.org/abs/2602.01744)
*Mingwei Xu,Xuan Lin,Xinnan Guo,Wanqing Xu,Wanyun Cui*

Main category: cs.LG

TL;DR: SLA（Softmax Linear Attention）通过将softmax从token级别提升到head级别，在线性注意力中恢复了全局竞争机制，既保持了线性复杂度又增强了表达能力。


<details>
  <summary>Details</summary>
Motivation: 线性注意力虽然降低了计算复杂度，但移除了softmax归一化，导致缺乏全局竞争机制，难以在长上下文噪声中聚焦相关信息。

Method: 提出SLA框架，将softmax操作从token级别提升到head级别，利用注意力头作为粗粒度语义槽，通过竞争门控机制动态选择最相关的子空间。

Result: SLA在语言建模和长上下文基准测试中持续提升了现有线性基线（RetNet、GLA、GDN）的性能，特别是在具有挑战性的检索场景中显著增强了抗噪声鲁棒性。

Conclusion: SLA成功在线性注意力中恢复了精确聚焦能力，同时保持了线性复杂度，为长上下文理解提供了更有效的解决方案。

Abstract: While linear attention reduces the quadratic complexity of standard Transformers to linear time, it often lags behind in expressivity due to the removal of softmax normalization. This omission eliminates \emph{global competition}, a critical mechanism that enables models to sharply focus on relevant information amidst long-context noise. In this work, we propose \textbf{Softmax Linear Attention (SLA)}, a framework designed to restore this competitive selection without sacrificing efficiency. By lifting the softmax operation from the token level to the head level, SLA leverages attention heads as coarse semantic slots, applying a competitive gating mechanism to dynamically select the most relevant subspaces. This reintroduces the ``winner-take-all'' dynamics essential for precise retrieval and robust long-context understanding. Distinct from prior methods that focus on refining local kernel functions, SLA adopts a broader perspective by exploiting the higher-level multi-head aggregation structure. Extensive experiments demonstrate that SLA consistently enhances state-of-the-art linear baselines (RetNet, GLA, GDN) across language modeling and long-context benchmarks, particularly in challenging retrieval scenarios where it significantly boosts robustness against noise, validating its capability to restore precise focus while maintaining linear complexity.

</details>


### [291] [Probability-Entropy Calibration: An Elastic Indicator for Adaptive Fine-tuning](https://arxiv.org/abs/2602.01745)
*Wenhao Yu,Shaohang Wei,Jiahong Liu,Yifan Li,Minda Hu,Aiwei Liu,Hao Zhang,Irwin King*

Main category: cs.LG

TL;DR: RankTuner通过概率-熵校准信号（相对排名指标）重新加权微调目标，专注于真正未学习到的token，在数学推理基准上优于仅使用概率或熵的基线方法


<details>
  <summary>Details</summary>
Motivation: 现有的token级重新加权方法主要是一维的：真实概率反映下游对齐，token熵反映预训练先验的内在不确定性。忽略熵会误将噪声或易替换token识别为学习关键，而忽略概率则无法反映目标特定对齐

Method: RankTuner引入概率-熵校准信号——相对排名指标，比较真实token的排名与其在预测分布下的期望排名。使用该指标的倒数作为token级相对尺度来重新加权微调目标

Result: 在多个骨干模型上的实验显示，在数学推理基准上持续改进，在分布外推理上获得迁移增益，在代码生成性能上优于仅使用概率或熵的重新加权基线

Conclusion: RankTuner通过概率-熵校准有效识别真正未学习到的token，避免对内在不确定位置的过度惩罚，在监督微调控制中优于传统一维重新加权方法

Abstract: Token-level reweighting is a simple yet effective mechanism for controlling supervised fine-tuning, but common indicators are largely one-dimensional: the ground-truth probability reflects downstream alignment, while token entropy reflects intrinsic uncertainty induced by the pre-training prior. Ignoring entropy can misidentify noisy or easily replaceable tokens as learning-critical, while ignoring probability fails to reflect target-specific alignment. RankTuner introduces a probability--entropy calibration signal, the Relative Rank Indicator, which compares the rank of the ground-truth token with its expected rank under the prediction distribution. The inverse indicator is used as a token-wise Relative Scale to reweight the fine-tuning objective, focusing updates on truly under-learned tokens without over-penalizing intrinsically uncertain positions. Experiments on multiple backbones show consistent improvements on mathematical reasoning benchmarks, transfer gains on out-of-distribution reasoning, and pre code generation performance over probability-only or entropy-only reweighting baselines.

</details>


### [292] [Rethinking LoRA for Data Heterogeneous Federated Learning: Subspace and State Alignment](https://arxiv.org/abs/2602.01746)
*Hongyi Peng,Han Yu,Xiaoxiao Li,Qiang Yang*

Main category: cs.LG

TL;DR: FedGaLore：针对非IID联邦学习中LoRA性能下降问题，提出结合客户端GaLore梯度子空间优化与服务器端漂移鲁棒同步的方法，显著提升鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 在非独立同分布（non-IID）的联邦学习设置中，低秩适应（LoRA）方法相比全参数微调存在显著性能差距。研究发现这种差距源于两个耦合的不匹配问题：更新空间不匹配和优化器状态不匹配。

Method: 提出FedGaLore方法：1）客户端采用GaLore风格的梯度子空间优化；2）服务器端通过谱共享信号提取实现投影二阶矩状态的漂移鲁棒同步，以解决上述不匹配问题。

Result: 在自然语言理解、视觉和自然语言生成等多个基准测试中，FedGaLore在非IID设置下相比最先进的联邦LoRA基线方法，显著提升了鲁棒性和准确性。

Conclusion: FedGaLore通过解决联邦LoRA中的更新空间和优化器状态不匹配问题，有效提升了非IID联邦学习场景下的性能，为高效参数高效的联邦微调提供了新方案。

Abstract: Low-Rank Adaptation (LoRA) is widely used for federated fine-tuning. Yet under non-IID settings, it can substantially underperform full-parameter fine-tuning. Through with-high-probability robustness analysis, we uncover that this gap can be attributed to two coupled mismatches: (i) update-space mismatch, where clients optimize in a low-rank subspace but aggregation occurs in the full space; and (ii) optimizer-state mismatch, where unsynchronized adaptive states amplify drift across rounds. We propose FedGaLore, which combines client-side GaLore-style gradient-subspace optimization with server-side drift-robust synchronization of projected second-moment states via spectral shared-signal extraction, to address this challenge. Across NLU, vision, and NLG benchmarks, FedGaLore improves robustness and accuracy over state-of-the-art federated LoRA baselines in non-IID settings.

</details>


### [293] [MGKAN: Predicting Asymmetric Drug-Drug Interactions via a Multimodal Graph Kolmogorov-Arnold Network](https://arxiv.org/abs/2602.01751)
*Kunyi Fan,Mengjie Chen,Longlong Li,Cunquan Qu*

Main category: cs.LG

TL;DR: MGKAN是一种基于图Kolmogorov-Arnold网络的药物相互作用预测模型，通过引入可学习基函数和非对称建模，在基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络模型主要依赖线性聚合和对称假设，难以捕捉药物相互作用中的非线性和异质性模式，限制了预测准确性。

Method: 提出MGKAN模型：1) 用KAN驱动的基函数替代传统MLP变换；2) 整合三个网络视图（非对称DDI网络、共相互作用网络、生化相似性网络）；3) 使用角色特定嵌入保持方向语义；4) 融合线性注意力和非线性变换增强表示能力。

Result: 在两个基准数据集上，MGKAN优于七个最先进的基线方法。消融研究和案例研究证实了其预测准确性以及在建模方向性药物效应方面的有效性。

Conclusion: MGKAN通过引入可学习基函数和非对称建模，能够更有效地捕捉药物相互作用的复杂模式，为药物安全性评估提供了更准确的预测工具。

Abstract: Predicting drug-drug interactions (DDIs) is essential for safe pharmacological treatments. Previous graph neural network (GNN) models leverage molecular structures and interaction networks but mostly rely on linear aggregation and symmetric assumptions, limiting their ability to capture nonlinear and heterogeneous patterns. We propose MGKAN, a Graph Kolmogorov-Arnold Network that introduces learnable basis functions into asymmetric DDI prediction. MGKAN replaces conventional MLP transformations with KAN-driven basis functions, enabling more expressive and nonlinear modeling of drug relationships. To capture pharmacological dependencies, MGKAN integrates three network views-an asymmetric DDI network, a co-interaction network, and a biochemical similarity network-with role-specific embeddings to preserve directional semantics. A fusion module combines linear attention and nonlinear transformation to enhance representational capacity. On two benchmark datasets, MGKAN outperforms seven state-of-the-art baselines. Ablation studies and case studies confirm its predictive accuracy and effectiveness in modeling directional drug effects.

</details>


### [294] [A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention](https://arxiv.org/abs/2602.01763)
*Xiaowei Ye,Xiaoyu He,Chao Liao,Chen Wu,Pinyan Lu*

Main category: cs.LG

TL;DR: 该论文首次从理论上证明了混合注意力与标准全注意力在表达能力上存在可证明的分离，建立了注意力机制的表达能力层次结构。


<details>
  <summary>Details</summary>
Motivation: 尽管各种高效注意力机制（如线性注意力和混合注意力）被开发出来以缓解标准全注意力的二次复杂度问题，但它们的表达能力相对于全注意力缺乏严格的理论特征。本研究旨在填补这一基本空白。

Method: 通过理论分析，建立注意力机制的表达能力层次结构。研究适用于所有可以表示为递归形式的线性注意力变体（包括Mamba、DeltaNet等），分析它们在序列函数组合任务上的表现。

Result: 证明了表达能力层次：对于序列函数组合任务（必须在模型前向传递中发生的多步推理任务），(L+1)层的全注意力网络就足够了，而任何混合网络（交错L-1层全注意力与大量(2^{3L^2})层线性注意力）都无法解决该任务。

Conclusion: 该研究首次提供了混合注意力与标准全注意力之间的可证明分离，为理解不同注意力机制的基本能力和局限性提供了理论视角。

Abstract: Transformers serve as the foundation of most modern large language models. To mitigate the quadratic complexity of standard full attention, various efficient attention mechanisms, such as linear and hybrid attention, have been developed. A fundamental gap remains: their expressive power relative to full attention lacks a rigorous theoretical characterization. In this work, we theoretically characterize the performance differences among these attention mechanisms. Our theory applies to all linear attention variants that can be formulated as a recurrence, including Mamba, DeltaNet, etc. Specifically, we establish an expressiveness hierarchy: for the sequential function composition-a multi-step reasoning task that must occur within a model's forward pass, an ($L+1$)-layer full attention network is sufficient, whereas any hybrid network interleaving $L-1$ layers of full attention with a substantially larger number ($2^{3L^2}$) of linear attention layers cannot solve it. This result demonstrates a clear separation in expressive power between the two types of attention. Our work provides the first provable separation between hybrid attention and standard full attention, offering a theoretical perspective for understanding the fundamental capabilities and limitations of different attention mechanisms.

</details>


### [295] [CoMeT: Collaborative Memory Transformer for Efficient Long Context Modeling](https://arxiv.org/abs/2602.01766)
*Runsong Zhao,Shilei Liu,Jiwei Tang,Langming Liu,Haibin Chen,Weidong Zhang,Yujin Yuan,Tong Xiao,Jingbo Zhu,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: CoMeT是一种新型Transformer架构，通过双记忆系统和分块处理，实现线性时间复杂度和恒定内存使用，支持任意长序列处理


<details>
  <summary>Details</summary>
Motivation: 标准Transformer的二次复杂度和不断增长的KV缓存是处理长上下文的主要障碍，需要更高效的架构

Method: 提出协作记忆Transformer(CoMeT)，采用双记忆系统：FIFO队列临时记忆处理近期事件，门控更新全局记忆处理长程依赖；作为动态软提示集成到预训练模型中；引入层级流水线并行策略进行高效微调

Result: 在32k上下文微调的模型能在1M令牌序列中准确检索任意位置的密码；在SCROLLS基准上超越其他高效方法，在摘要任务上达到全注意力基线水平；在实际代理和用户行为QA任务中验证有效性

Conclusion: CoMeT通过恒定内存和线性时间处理任意长序列，可作为即插即用模块集成到预训练模型中，在长上下文任务中表现优异

Abstract: The quadratic complexity and indefinitely growing key-value (KV) cache of standard Transformers pose a major barrier to long-context processing. To overcome this, we introduce the Collaborative Memory Transformer (CoMeT), a novel architecture that enables LLMs to handle arbitrarily long sequences with constant memory usage and linear time complexity. Designed as an efficient, plug-in module, CoMeT can be integrated into pre-trained models with only minimal fine-tuning. It operates on sequential data chunks, using a dual-memory system to manage context: a temporary memory on a FIFO queue for recent events, and a global memory with a gated update rule for long-range dependencies. These memories then act as a dynamic soft prompt for the next chunk. To enable efficient fine-tuning on extremely long contexts, we introduce a novel layer-level pipeline parallelism strategy. The effectiveness of our approach is remarkable: a model equipped with CoMeT and fine-tuned on 32k contexts can accurately retrieve a passkey from any position within a 1M token sequence. On the SCROLLS benchmark, CoMeT surpasses other efficient methods and achieves performance comparable to a full-attention baseline on summarization tasks. Its practical effectiveness is further validated on real-world agent and user behavior QA tasks. The code is available at: https://anonymous.4open.science/r/comet-B00B/

</details>


### [296] [IRIS: Implicit Reward-Guided Internal Sifting for Mitigating Multimodal Hallucination](https://arxiv.org/abs/2602.01769)
*Yuanshuai Li,Yuping Yan,Jirui Han,Fei Ming,Lingjuan Lv,Yaochu Jin*

Main category: cs.LG

TL;DR: IRIS提出了一种基于隐式奖励的内部筛选方法，通过连续隐式奖励在原生对数概率空间中捕捉模态竞争，无需外部反馈即可有效减少多模态大语言模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于DPO的方法依赖昂贵的外部评估器进行评分或重写，存在离策略学习差距和离散化损失，且无法访问内部状态，忽略了导致幻觉的不同模态间的细粒度冲突。

Method: IRIS利用连续隐式奖励在原生对数概率空间中保留完整信息密度，捕捉内部模态竞争。这种在策略范式通过自生成偏好对消除学习差距，基于多模态隐式奖励筛选这些偏好对，确保优化直接解决模态冲突。

Result: 在关键幻觉基准测试中，IRIS仅使用5.7k样本就实现了高度竞争力的性能，在偏好对齐过程中完全不需要任何外部反馈。

Conclusion: IRIS为缓解MLLM幻觉问题提供了一个高效且有原则的范式，通过内部模态竞争的直接解决机制，在无需外部反馈的情况下实现了优异的幻觉抑制效果。

Abstract: Hallucination remains a fundamental challenge for Multimodal Large Language Models (MLLMs). While Direct Preference Optimization (DPO) is a key alignment framework, existing approaches often rely heavily on costly external evaluators for scoring or rewriting, incurring off-policy learnability gaps and discretization loss. Due to the lack of access to internal states, such feedback overlooks the fine-grained conflicts between different modalities that lead to hallucinations during generation.
  To address this issue, we propose IRIS (Implicit Reward-Guided Internal Sifting), which leverages continuous implicit rewards in the native log-probability space to preserve full information density and capture internal modal competition. This on-policy paradigm eliminates learnability gaps by utilizing self-generated preference pairs. By sifting these pairs based on multimodal implicit rewards, IRIS ensures that optimization is driven by signals that directly resolve modal conflicts. Extensive experiments demonstrate that IRIS achieves highly competitive performance on key hallucination benchmarks using only 5.7k samples, without requiring any external feedback during preference alignment. These results confirm that IRIS provides an efficient and principled paradigm for mitigating MLLM hallucinations.

</details>


### [297] [DIA-CLIP: a universal representation learning framework for zero-shot DIA proteomics](https://arxiv.org/abs/2602.01772)
*Yucheng Liao,Han Wen,Weinan E,Weijie Zhang*

Main category: cs.LG

TL;DR: DIA-CLIP是一个基于预训练模型的DIA-MS分析新范式，通过跨模态表示学习实现零样本PSM推断，显著提升蛋白质鉴定性能


<details>
  <summary>Details</summary>
Motivation: 当前DIA分析框架需要在每个实验中半监督训练PSM重评分模型，容易过拟合且缺乏跨物种和实验条件的泛化能力

Method: 采用双编码器对比学习框架与编码器-解码器架构相结合，建立肽段和对应谱图特征的统一跨模态表示，实现高精度零样本PSM推断

Result: 在多个基准测试中一致优于现有最先进工具，蛋白质鉴定数量提升高达45%，同时误鉴定减少12%

Conclusion: DIA-CLIP将DIA分析范式从半监督训练转向通用跨模态表示学习，在单细胞和空间蛋白质组学等应用中具有巨大潜力

Abstract: Data-independent acquisition mass spectrometry (DIA-MS) has established itself as a cornerstone of proteomic profiling and large-scale systems biology, offering unparalleled depth and reproducibility. Current DIA analysis frameworks, however, require semi-supervised training within each run for peptide-spectrum match (PSM) re-scoring. This approach is prone to overfitting and lacks generalizability across diverse species and experimental conditions. Here, we present DIA-CLIP, a pre-trained model shifting the DIA analysis paradigm from semi-supervised training to universal cross-modal representation learning. By integrating dual-encoder contrastive learning framework with encoder-decoder architecture, DIA-CLIP establishes a unified cross-modal representation for peptides and corresponding spectral features, achieving high-precision, zero-shot PSM inference. Extensive evaluations across diverse benchmarks demonstrate that DIA-CLIP consistently outperforms state-of-the-art tools, yielding up to a 45% increase in protein identification while achieving a 12% reduction in entrapment identifications. Moreover, DIA-CLIP holds immense potential for diverse practical applications, such as single-cell and spatial proteomics, where its enhanced identification depth facilitates the discovery of novel biomarkers and the elucidates of intricate cellular mechanisms.

</details>


### [298] [Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting](https://arxiv.org/abs/2602.01776)
*Mingyue Cheng,Xiaoyu Tao,Qi Liu,Ze Guo,Enhong Chen*

Main category: cs.LG

TL;DR: 论文提出"代理式时间序列预测(ATSF)"新范式，将传统模型中心的静态预测重构为包含感知、规划、行动、反思和记忆的代理式工作流程，强调交互、反馈和经验积累。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测方法存在局限性：1) 模型中心化，过度关注预测模型本身；2) 静态单次预测，无法适应自适应和多轮交互场景；3) 缺乏信息特征提取、推理驱动推断、迭代优化和持续适应能力。需要新范式来应对复杂动态环境下的预测需求。

Method: 提出代理式时间序列预测(ATSF)框架，将预测重构为包含五个核心组件的代理式流程：1) 感知(数据获取与特征提取)；2) 规划(推理与策略制定)；3) 行动(预测执行与工具交互)；4) 反思(结果评估与反馈整合)；5) 记忆(经验积累与知识存储)。介绍了三种实现范式：工作流设计、代理式强化学习和混合代理工作流。

Result: 建立了从模型中心预测向代理式预测转变的理论框架，明确了ATSF的核心概念和组件，提出了三种可行的实现路径，为时间序列预测研究开辟了新的方向。

Conclusion: 代理式时间序列预测(ATSF)为解决传统预测方法在自适应、多轮交互和动态环境中的局限性提供了新范式。通过将预测重构为包含感知、规划、行动、反思和记忆的代理式工作流程，ATSF能够更好地整合工具交互、反馈机制和经验积累，为时间序列预测与人工智能代理的交叉研究奠定基础。

Abstract: Time series forecasting has traditionally been formulated as a model-centric, static, and single-pass prediction problem that maps historical observations to future values. While this paradigm has driven substantial progress, it proves insufficient in adaptive and multi-turn settings where forecasting requires informative feature extraction, reasoning-driven inference, iterative refinement, and continual adaptation over time. In this paper, we argue for agentic time series forecasting (ATSF), which reframes forecasting as an agentic process composed of perception, planning, action, reflection, and memory. Rather than focusing solely on predictive models, ATSF emphasizes organizing forecasting as an agentic workflow that can interact with tools, incorporate feedback from outcomes, and evolve through experience accumulation. We outline three representative implementation paradigms -- workflow-based design, agentic reinforcement learning, and a hybrid agentic workflow paradigm -- and discuss the opportunities and challenges that arise when shifting from model-centric prediction to agentic forecasting. Together, this position aims to establish agentic forecasting as a foundation for future research at the intersection of time series forecasting.

</details>


### [299] [Stein-Rule Shrinkage for Stochastic Gradient Estimation in High Dimensions](https://arxiv.org/abs/2602.01777)
*M. Arashi,M. Amintoosi*

Main category: cs.LG

TL;DR: 提出基于Stein规则收缩的随机梯度估计器，通过自适应收缩小批量梯度来改进高维深度学习中的梯度估计，在Adam优化器中实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统随机梯度方法将小批量梯度视为无偏估计，但统计决策理论表明在高维设置下无偏估计在二次损失下通常不可接受，标准随机梯度可能从风险角度是次优的。

Method: 将随机梯度计算构建为高维估计问题，基于Stein规则收缩构建收缩梯度估计器，自适应地将噪声小批量梯度收缩到历史动量推导的稳定受限估计器，使用梯度噪声方差的在线估计确定收缩强度。

Result: 在p≥3维的高斯噪声模型下，提出的估计器在平方误差损失下均匀支配标准随机梯度，且在经典决策理论意义下是最小最大最优的。在CIFAR10和CIFAR100上，在多种标签噪声水平下，在大批量机制中相比Adam获得一致改进。

Conclusion: 经典收缩原则为改进现代深度学习中的随机梯度估计提供了原则性且有效的方法，特别是选择性应用于高维卷积层时效果显著。

Abstract: Stochastic gradient methods are central to large-scale learning, yet their analysis typically treats mini-batch gradients as unbiased estimators of the population gradient. In high-dimensional settings, however, classical results from statistical decision theory show that unbiased estimators are generally inadmissible under quadratic loss, suggesting that standard stochastic gradients may be suboptimal from a risk perspective. In this work, we formulate stochastic gradient computation as a high-dimensional estimation problem and introduce a decision-theoretic framework based on Stein-rule shrinkage. We construct a shrinkage gradient estimator that adaptively contracts noisy mini-batch gradients toward a stable restricted estimator derived from historical momentum. The shrinkage intensity is determined in a data-driven manner using an online estimate of gradient noise variance, leveraging second-moment statistics commonly maintained by adaptive optimization methods. Under a Gaussian noise model and for dimension p>=3, we show that the proposed estimator uniformly dominates the standard stochastic gradient under squared error loss and is minimax-optimal in the classical decision-theoretic sense. We further demonstrate how this estimator can be incorporated into the Adam optimizer, yielding a practical algorithm with negligible additional computational cost. Empirical evaluations on CIFAR10 and CIFAR100, across multiple levels of label noise, show consistent improvements over Adam in the large-batch regime. Ablation studies indicate that the gains arise primarily from selectively applying shrinkage to high-dimensional convolutional layers, while indiscriminate shrinkage across all parameters degrades performance. These results illustrate that classical shrinkage principles provide a principled and effective approach to improving stochastic gradient estimation in modern deep learning.

</details>


### [300] [Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning](https://arxiv.org/abs/2602.01791)
*Zheng Zhang,Ao Lu,Yuanhao Zeng,Ziwei Shan,Jinjin Guo,Lufei Li,Yexin Li,Kan Ren*

Main category: cs.LG

TL;DR: Grad2Reward：通过单次反向传播从LLM法官的推理过程中提取密集过程奖励，解决开放任务中稀疏奖励和缺乏细粒度监督的问题，实现精确的token级信用分配。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在开放任务中使用LLM-as-a-Judge提供序列级奖励，但奖励稀疏且缺乏细粒度监督，同时将法官视为黑盒，忽略了其中间反馈信号。

Method: 提出Grad2Reward框架，通过梯度归因从法官模型的推理过程中提取密集过程奖励，实现token级信用分配，并引入自判断机制让策略通过自身评估信号改进。

Result: 实验表明，使用Grad2Reward优化的策略在多种开放任务上表现出色，验证了其有效性和广泛泛化能力。

Conclusion: Grad2Reward通过提取法官模型的密集过程奖励，解决了开放任务RL中的稀疏奖励问题，显著提升了训练效率和推理质量，无需专门奖励模型或依赖外部法官。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has catalyzed significant breakthroughs in complex LLM reasoning within verifiable domains, such as mathematics and programming. Recent efforts have sought to extend this paradigm to open-ended tasks by employing LLMs-as-a-Judge to provide sequence-level rewards for policy optimization. However, these rewards are inherently sparse, failing to provide the fine-grained supervision necessary for generating complex, long-form trajectories. Furthermore, current work treats the Judge as a black-box oracle, discarding the rich intermediate feedback signals encoded in it. To address these limitations, we introduce Grad2Reward, a novel framework that extracts dense process rewards directly from the Judge's model inference process via a single backward pass. By leveraging gradient-based attribution, Grad2Reward enables precise token-level credit assignment, substantially enhancing training efficiency and reasoning quality. Additionally, Grad2Reward introduces a self-judging mechanism, allowing the policy to improve through its own evaluative signals without training specialized reward models or reliance on superior external Judges. The experiments demonstrate that policies optimized with Grad2Reward achieve outstanding performance across diverse open-ended tasks, affirming its effectiveness and broad generalizability.

</details>


### [301] [Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It](https://arxiv.org/abs/2602.01826)
*Yaxiang Zhang,Yingru Li,Jiacai Liu,Jiawei Xu,Ziniu Li,Qian Liu,Haoyuan Li*

Main category: cs.LG

TL;DR: 本文提出一种基于响应长度的动态学习率调度器，用于稳定大型语言模型的强化学习训练，解决训练-推理不匹配导致的优化不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的强化学习训练存在严重的不稳定性问题。现有研究将不稳定性归因于训练-推理不匹配，但传统解决方案如重要性采样在长期训练中可能失效。本文从优化角度分析该问题，发现梯度噪声和训练-推理不匹配会随着训练进展同时加剧。

Method: 提出一种专门的学习率调度器，不同于传统预定义衰减计划，该方法基于响应长度动态触发学习率衰减。研究发现响应长度是不稳定性即将发生的可靠早期预警信号。通过在学习率衰减时降低梯度噪声，可以有效稳定RL训练。

Result: 经验证据表明，通过响应长度动态调整学习率，能够持续稳定RL训练，并将训练-推理不匹配保持在安全水平。该方法简单有效，解决了长期训练中的不稳定性问题。

Conclusion: 训练-推理不匹配不是静态数值差异，而是与模型优化耦合的动态故障。基于响应长度的动态学习率调度器能够有效抑制不匹配，为大型语言模型的强化学习训练提供了稳定可靠的解决方案。

Abstract: Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to "training inference mismatch stemming" from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model's optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level.

</details>


### [302] [Hyperbolic Graph Neural Networks Under the Microscope: The Role of Geometry-Task Alignment](https://arxiv.org/abs/2602.01828)
*Dionisia Naddeo,Jonas Linkerhägner,Nicola Toschi,Geri Skenderi,Veronica Lachi*

Main category: cs.LG

TL;DR: 论文质疑双曲图神经网络（HGNNs）在树状图上的默认优势，提出几何-任务对齐条件，证明只有当任务需要保持度量结构时HGNNs才优于欧氏模型。


<details>
  <summary>Details</summary>
Motivation: 挑战当前将双曲图神经网络（HGNNs）作为树状图表示学习首选范式的观点，提出需要考虑几何-任务对齐条件，即目标任务的度量结构是否与输入图的度量结构一致。

Method: 通过理论和实证分析，在合成回归问题上测试HGNNs恢复低失真表示的能力；在链接预测和节点分类任务中联合分析预测性能和嵌入失真，评估几何对齐性。

Result: HGNNs在需要保持度量结构的问题中表现出几何归纳偏置优势，但仅在链接预测任务中表现出几何对齐性；当任务与双曲几何对齐时，HGNNs始终优于欧氏模型，否则优势消失。

Conclusion: 研究重点应从"图是否双曲？"扩展到"任务是否与双曲几何对齐？"，HGNNs的优势取决于几何-任务对齐条件，而非仅仅图的树状结构。

Abstract: Many complex networks exhibit hyperbolic structural properties, making hyperbolic space a natural candidate for representing hierarchical and tree-like graphs with low distortion. Based on this observation, Hyperbolic Graph Neural Networks (HGNNs) have been widely adopted as a principled choice for representation learning on tree-like graphs. In this work, we question this paradigm by proposing an additional condition of geometry-task alignment, i.e., whether the metric structure of the target follows that of the input graph. We theoretically and empirically demonstrate the capability of HGNNs to recover low-distortion representations on two synthetic regression problems, and show that their geometric inductive bias becomes helpful when the problem requires preserving metric structure. Additionally, we evaluate HGNNs on the tasks of link prediction and node classification by jointly analyzing predictive performance and embedding distortion, revealing that only link prediction is geometry-aligned. Overall, our findings shift the focus from only asking "Is the graph hyperbolic?" to also questioning "Is the task aligned with hyperbolic geometry?", showing that HGNNs consistently outperform Euclidean models under such alignment, while their advantage vanishes otherwise.

</details>


### [303] [DOGMA: Weaving Structural Information into Data-centric Single-cell Transcriptomics Analysis](https://arxiv.org/abs/2602.01839)
*Ru Zhang,Xunkai Li,Yaxin Deng,Sicheng Liu,Daohan Su,Qiangqiang Dai,Hongchao Qin,Rong-Hua Li,Guoren Wang,Jia Li*

Main category: cs.LG

TL;DR: DOGMA是一个数据中心的单细胞转录组学分析框架，通过整合多层次生物先验知识来重塑数据结构并增强语义，实现确定性图构建和跨物种对齐，在复杂基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个主要问题：1）早期序列方法将细胞视为独立实体，忽略了生物系统功能机制驱动的潜在细胞间关系；2）结构化方法虽然捕捉细胞间关系并增强原始数据，但往往忽略生物先验知识，导致计算开销大且图表示不理想。

Method: DOGMA是一个整体性数据中心的框架，通过多层次生物先验知识进行结构重塑和语义增强：1）整合统计锚点、细胞本体和系统发育树实现确定性图构建和鲁棒的跨物种对齐；2）利用基因本体通过功能先验知识弥合特征级语义鸿沟。

Result: 在复杂的多物种和多器官基准测试中，DOGMA实现了最先进的性能，表现出优异的零样本鲁棒性和样本效率，同时以显著更低的计算成本运行。

Conclusion: DOGMA通过整合生物先验知识超越了依赖随机启发式的方法，为单细胞转录组学分析提供了更有效的数据中心解决方案，能够实现确定性结构发现和鲁棒的跨物种对齐。

Abstract: Recently, data-centric AI methodology has been a dominant paradigm in single-cell transcriptomics analysis, which treats data representation rather than model complexity as the fundamental bottleneck. In the review of current studies, earlier sequence methods treat cells as independent entities and adapt prevalent ML models to analyze their directly inherited sequence data. Despite their simplicity and intuition, these methods overlook the latent intercellular relationships driven by the functional mechanisms of biological systems and the inherent quality issues of the raw sequence data. Therefore, a series of structured methods has emerged. Although they employ various heuristic rules to capture intricate intercellular relationships and enhance the raw sequencing data, these methods often neglect biological prior knowledge. This omission incurs substantial overhead and yields suboptimal graph representations, thereby hindering the utility of ML models.
  To address them, we propose DOGMA, a holistic data-centric framework designed for the structural reshaping and semantic enhancement of raw data through multi-level biological prior knowledge. Transcending reliance on stochastic heuristics, DOGMA redefines graph construction by integrating Statistical Anchors with Cell Ontology and Phylogenetic Trees to enable deterministic structure discovery and robust cross-species alignment. Furthermore, Gene Ontology is utilized to bridge the feature-level semantic gap by incorporating functional priors. In complex multi-species and multi-organ benchmarks, DOGMA achieves SOTA performance, exhibiting superior zero-shot robustness and sample efficiency while operating with significantly lower computational cost.

</details>


### [304] [Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models](https://arxiv.org/abs/2602.01842)
*Jinbin Bai,Yixuan Li,Yuchen Zhu,Yi Xin,Qingyu Shi,Aosong Feng,Xiaohong Liu,Molei Tao,Jianru Xue,Xiangtai Li,Ming-Hsuan Yang*

Main category: cs.LG

TL;DR: Prism是一个针对离散扩散语言模型的高效测试时扩展框架，通过分层轨迹搜索、局部分支和自验证反馈，在数学推理和代码生成任务上实现了性能与效率的良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时扩展算法主要依赖自回归解码，不适用于并行解码的离散扩散语言模型，因此需要开发高效的方法来释放dLLMs的生成潜力。

Method: 提出Prism框架，包含三个核心组件：1)分层轨迹搜索，在去噪早期到中期动态剪枝和重新分配计算资源；2)局部分支与部分重掩码，探索多样化实现同时保留高置信度token；3)自验证反馈，通过自评估提示替代外部验证器。

Result: 在三个dLLMs（LLaDA 8B Instruct、Dream 7B Instruct、LLaDA 2.0-mini）和四个数学推理与代码生成基准测试上，Prism实现了性能与效率的良好平衡，以显著更少的函数评估次数匹配最佳N选1性能。

Conclusion: Prism为离散扩散语言模型提供了一种高效实用的测试时扩展框架，成功解决了dLLMs在推理时计算优化方面的挑战，代码已开源。

Abstract: Inference-time compute has re-emerged as a practical way to improve LLM reasoning. Most test-time scaling (TTS) algorithms rely on autoregressive decoding, which is ill-suited to discrete diffusion language models (dLLMs) due to their parallel decoding over the entire sequence. As a result, developing effective and efficient TTS methods to unlock dLLMs' full generative potential remains an underexplored challenge. To address this, we propose Prism (Pruning, Remasking, and Integrated Self-verification Method), an efficient TTS framework for dLLMs that (i) performs Hierarchical Trajectory Search (HTS) which dynamically prunes and reallocates compute in an early-to-mid denoising window, (ii) introduces Local branching with partial remasking to explore diverse implementations while preserving high-confidence tokens, and (iii) replaces external verifiers with Self-Verified Feedback (SVF) obtained via self-evaluation prompts on intermediate completions. Across four mathematical reasoning and code generation benchmarks on three dLLMs, including LLaDA 8B Instruct, Dream 7B Instruct, and LLaDA 2.0-mini, our Prism achieves a favorable performance-efficiency trade-off, matching best-of-N performance with substantially fewer function evaluations (NFE). The code is released at https://github.com/viiika/Prism.

</details>


### [305] [No Generation without Representation: Efficient Causal Protein Language Models Enable Zero-Shot Fitness Estimation](https://arxiv.org/abs/2602.01845)
*Furkan Eris*

Main category: cs.LG

TL;DR: Proust是一个309M参数的因果蛋白质语言模型，通过架构创新弥合了掩码语言模型在适应度预测和因果模型在生成能力之间的鸿沟，在蛋白质适应度预测和生成任务上均表现出色。


<details>
  <summary>Details</summary>
Motivation: 蛋白质语言模型面临基本分歧：掩码语言模型擅长适应度预测，而因果模型支持生成，迫使研究人员维护不同的架构。需要一种能同时具备这两种能力的统一模型。

Method: 提出Proust模型，采用从LLM研究借鉴的架构创新：分组查询注意力（共享K/V投影）、跨层值残差和深度因果卷积。使用33B token在40 B200 GPU小时上训练。

Result: 在ProteinGym替换任务上达到Spearman ρ=0.390，与需要50-200倍计算量的MLM竞争；在indel任务上创下新SOTA，优于大20倍的模型；在EVEREST病毒适应度基准上接近结构感知方法；同时保留因果模型的生成能力。

Conclusion: Proust通过架构创新成功弥合了蛋白质语言模型的适应度预测和生成能力之间的鸿沟，在多个基准上表现出色，同时可解释性分析为检索增强提供了见解，为蛋白质设计提供了强大的统一工具。

Abstract: Protein language models (PLMs) face a fundamental divide: masked language models (MLMs) excel at fitness prediction while causal models enable generation, forcing practitioners to maintain separate architectures. We introduce \textbf{Proust}, a 309M-parameter causal PLM that bridges this gap through architectural innovations adapted from recent LLM research, including grouped-query attention with shared K/V projections, cross-layer value residuals, and depthwise causal convolutions. Trained on 33B tokens in 40 B200 GPU-hours, Proust achieves Spearman $ρ= 0.390$ on ProteinGym substitutions, competitive with MLMs requiring 50--200$\times$ the compute. On indels, Proust sets a new state-of-the-art, outperforming models up to 20$\times$ larger. On EVEREST viral fitness benchmarks, it approaches structure-aware methods using sequence alone. These powerful representations position Proust in a sweet spot as it also retains native generative capabilities that MLMs lack by design. Interpretability analysis reveals that per-position entropy variance predicts, to an extent, when retrieval augmentation helps and hurts. Such insights can grow in both quantity and quality at scale and inform capabilities such as test-time scaling. Code and weights are available at https://github.com/Furkan9015/proust-inference

</details>


### [306] [Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models](https://arxiv.org/abs/2602.01849)
*Ziwei Luo,Ziqi Jin,Lei Wang,Lidong Bing,Thomas B. Schön*

Main category: cs.LG

TL;DR: 提出自奖励序列蒙特卡洛方法，通过并行扩散粒子探索轨迹，使用轨迹级置信度作为自奖励信号来提升掩码扩散语言模型的采样质量。


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散语言模型主要依赖置信度采样策略，只保留预测置信度最高的token，导致生成过程局限于噪声敏感的贪婪解码范式，限制了路径多样性。

Method: 启动多个并行交互的扩散过程（粒子）进行轨迹探索，引入轨迹级置信度作为自奖励信号分配粒子重要性权重，通过迭代加权和重采样系统性地引导生成全局置信度高、质量好的样本。

Result: 在多种掩码扩散语言模型和基准测试上验证了显著改进，无需额外训练或奖励指导，有效将并行推理能力转化为采样质量提升。

Conclusion: 自奖励序列蒙特卡洛方法解决了掩码扩散语言模型采样多样性受限的问题，通过并行粒子探索和轨迹级置信度奖励机制，实现了采样质量的显著提升。

Abstract: This work presents self-rewarding sequential Monte Carlo (SMC), an inference-time scaling algorithm enabling effective sampling of masked diffusion language models (MDLMs). Our algorithm stems from the observation that most existing MDLMs rely on a confidence-based sampling strategy, where only tokens with the highest prediction confidence are preserved at each step. This restricts the generation to a noise-sensitive, greedy decoding paradigm, resulting in an inevitable collapse in the diversity of possible paths. We address this problem by launching multiple interacting diffusion processes in parallel, referred to as particles, for trajectory exploration. Importantly, we introduce the trajectory-level confidence as a self-rewarding signal for assigning particle importance weights. During sampling, particles are iteratively weighted and resampled to systematically steer generation towards globally confident, high-quality samples. Our self-rewarding SMC is verified on various masked diffusion language models and benchmarks, achieving significant improvement without extra training or reward guidance, while effectively converting parallel inference capacity into improved sampling quality. Our code is available at https://github.com/Algolzw/self-rewarding-smc.

</details>


### [307] [FUPareto: Bridging the Forgetting-Utility Gap in Federated Unlearning via Pareto Augmented Optimization](https://arxiv.org/abs/2602.01852)
*Zeyan Wang,Zhengmao Liu,Yongxin Cai,Chi Li,Xiaoying Tang,Jingchao Chen,Zibin Pan,Jing Qiu*

Main category: cs.LG

TL;DR: FUPareto：基于帕累托优化的联邦遗忘框架，通过最小边界偏移损失和空空间投影多梯度下降算法，解决联邦遗忘中的效用-遗忘冲突和多客户端并发遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘方法存在三个关键挑战：1）遗忘目标常损害模型效用或增加成员推理攻击风险；2）遗忘与效用之间存在固有冲突；3）多客户端并发遗忘支持差，梯度冲突影响遗忘质量。

Method: 提出FUPareto框架：1）最小边界偏移损失强制目标类logit低于最高非目标类logit；2）帕累托改进步骤保持模型效用；3）帕累托扩展保证遗忘，集成空空间投影多梯度下降算法解耦梯度冲突。

Result: 在多种场景下的实验表明，FUPareto在遗忘效果和保留效用方面均优于现有最先进的联邦遗忘方法。

Conclusion: FUPareto通过帕累托增强优化有效解决了联邦遗忘中的关键挑战，实现了高效、公平的多客户端并发遗忘，同时最小化效用损失。

Abstract: Federated Unlearning (FU) aims to efficiently remove the influence of specific client data from a federated model while preserving utility for the remaining clients. However, three key challenges remain: (1) existing unlearning objectives often compromise model utility or increase vulnerability to Membership Inference Attacks (MIA); (2) there is a persistent conflict between forgetting and utility, where further unlearning inevitably harms retained performance; and (3) support for concurrent multi-client unlearning is poor, as gradient conflicts among clients degrade the quality of forgetting. To address these issues, we propose FUPareto, an efficient unlearning framework via Pareto-augmented optimization. We first introduce the Minimum Boundary Shift (MBS) Loss, which enforces unlearning by suppressing the target class logit below the highest non-target class logit; this can improve the unlearning efficiency and mitigate MIA risks. During the unlearning process, FUPareto performs Pareto improvement steps to preserve model utility and executes Pareto expansion to guarantee forgetting. Specifically, during Pareto expansion, the framework integrates a Null-Space Projected Multiple Gradient Descent Algorithm (MGDA) to decouple gradient conflicts. This enables effective, fair, and concurrent unlearning for multiple clients while minimizing utility degradation. Extensive experiments across diverse scenarios demonstrate that FUPareto consistently outperforms state-of-the-art FU methods in both unlearning efficacy and retained utility.

</details>


### [308] [Designing Time Series Experiments in A/B Testing with Transformer Reinforcement Learning](https://arxiv.org/abs/2602.01853)
*Xiangkun Wu,Qianglin Wen,Yingying Zhang,Hongtu Zhu,Ting Li,Chengchun Shi*

Main category: cs.LG

TL;DR: 提出基于Transformer强化学习的时间序列A/B测试设计方法，解决传统方法无法充分利用历史信息和依赖强假设的问题


<details>
  <summary>Details</summary>
Motivation: 时间序列实验中的A/B测试面临挑战：现有设计无法充分利用完整历史信息，且依赖强假设来近似目标函数，导致次优设计

Method: 提出Transformer强化学习方法：利用Transformer处理完整历史信息，通过强化学习直接优化均方误差，避免依赖限制性假设

Result: 在合成数据、公开调度模拟器和真实网约车数据集上的实验表明，该方法持续优于现有设计

Conclusion: 通过Transformer RL方法，成功解决了时间序列A/B测试中的历史信息利用和目标函数优化问题，实现了更优的实验设计

Abstract: A/B testing has become a gold standard for modern technological companies to conduct policy evaluation. Yet, its application to time series experiments, where policies are sequentially assigned over time, remains challenging. Existing designs suffer from two limitations: (i) they do not fully leverage the entire history for treatment allocation; (ii) they rely on strong assumptions to approximate the objective function (e.g., the mean squared error of the estimated treatment effect) for optimizing the design. We first establish an impossibility theorem showing that failure to condition on the full history leads to suboptimal designs, due to the dynamic dependencies in time series experiments. To address both limitations simultaneously, we next propose a transformer reinforcement learning (RL) approach which leverages transformers to condition allocation on the entire history and employs RL to directly optimize the MSE without relying on restrictive assumptions. Empirical evaluations on synthetic data, a publicly available dispatch simulator, and a real-world ridesharing dataset demonstrate that our proposal consistently outperforms existing designs.

</details>


### [309] [Time2Vec-Integrated Transformer for Robust Gesture Recognition from Low-Density sEMG](https://arxiv.org/abs/2602.01855)
*Blagoj Hristov,Hristijan Gjoreski,Vesna Ojleska Latkoska,Gorjan Nadzinski*

Main category: cs.LG

TL;DR: 提出一种数据高效的深度学习框架，使用最少传感器实现精确肌电控制，通过混合Transformer和可学习时间嵌入达到95.7%的F1分数，挑战高密度传感的必要性。


<details>
  <summary>Details</summary>
Motivation: 传统肌电假肢控制依赖复杂密集的多传感器阵列，限制了消费者可及性。需要开发使用最少传感器硬件就能实现精确控制的方法。

Method: 提出混合Transformer架构，采用Time2Vec可学习时间嵌入捕获生物信号的随机时间扭曲，使用归一化加性融合策略对齐空间和时间特征的潜在分布，采用两阶段课程学习协议确保数据稀缺下的稳健特征提取。

Result: 在10类运动集上达到95.7% ± 0.20%的多受试者F1分数，优于标准Transformer和CNN-LSTM模型。快速校准协议仅需每个手势两次试验，就能将性能从21.0%提升到96.9%。

Conclusion: 高保真时间嵌入可以补偿低空间分辨率，挑战了高密度传感的必要性。该框架为下一代能够快速个性化的假肢接口提供了稳健、经济高效的蓝图。

Abstract: Accurate and responsive myoelectric prosthesis control typically relies on complex, dense multi-sensor arrays, which limits consumer accessibility. This paper presents a novel, data-efficient deep learning framework designed to achieve precise and accurate control using minimal sensor hardware. Leveraging an external dataset of 8 subjects, our approach implements a hybrid Transformer optimized for sparse, two-channel surface electromyography (sEMG). Unlike standard architectures that use fixed positional encodings, we integrate Time2Vec learnable temporal embeddings to capture the stochastic temporal warping inherent in biological signals. Furthermore, we employ a normalized additive fusion strategy that aligns the latent distributions of spatial and temporal features, preventing the destructive interference common in standard implementations. A two-stage curriculum learning protocol is utilized to ensure robust feature extraction despite data scarcity. The proposed architecture achieves a state-of-the-art multi-subject F1-score of 95.7% $\pm$ 0.20% for a 10-class movement set, statistically outperforming both a standard Transformer with fixed encodings and a recurrent CNN-LSTM model. Architectural optimization reveals that a balanced allocation of model capacity between spatial and temporal dimensions yields the highest stability. Furthermore, while direct transfer to a new unseen subject led to poor accuracy due to domain shifts, a rapid calibration protocol utilizing only two trials per gesture recovered performance from 21.0% $\pm$ 2.98% to 96.9% $\pm$ 0.52%. By validating that high-fidelity temporal embeddings can compensate for low spatial resolution, this work challenges the necessity of high-density sensing. The proposed framework offers a robust, cost-effective blueprint for next-generation prosthetic interfaces capable of rapid personalization.

</details>


### [310] [Autocorrelated Optimize-via-Estimate: Predict-then-Optimize versus Finite-sample Optimal](https://arxiv.org/abs/2602.01877)
*Zichun Wang,Gar Goei Loke,Ruiting Zuo*

Main category: cs.LG

TL;DR: 提出A-OVE模型，在自相关不确定性下直接优化样本外性能，相比传统先估计后优化方法表现更好


<details>
  <summary>Details</summary>
Motivation: 传统先估计后优化方法在数据驱动优化中存在局限性，特别是在自相关不确定性下。需要开发直接优化样本外性能的模型

Method: 提出自相关优化-估计(A-OVE)模型，通过充分统计量获得样本外最优解，并开发递归形式计算充分统计量

Result: A-OVE在带交易成本的组合优化问题中表现优异，相对于完美信息oracle具有低regret，优于预测-优化机器学习基准。机器学习模型即使预测精度更高也可能决策质量更差

Conclusion: A-OVE模型在自相关不确定性下有效，即使存在小规模模型误设也能保持性能，验证了直接优化样本外性能方法的优势

Abstract: Models that directly optimize for out-of-sample performance in the finite-sample regime have emerged as a promising alternative to traditional estimate-then-optimize approaches in data-driven optimization. In this work, we compare their performance in the context of autocorrelated uncertainties, specifically, under a Vector Autoregressive Moving Average VARMA(p,q) process. We propose an autocorrelated Optimize-via-Estimate (A-OVE) model that obtains an out-of-sample optimal solution as a function of sufficient statistics, and propose a recursive form for computing its sufficient statistics. We evaluate these models on a portfolio optimization problem with trading costs. A-OVE achieves low regret relative to a perfect information oracle, outperforming predict-then-optimize machine learning benchmarks. Notably, machine learning models with higher accuracy can have poorer decision quality, echoing the growing literature in data-driven optimization. Performance is retained under small mis-specification.

</details>


### [311] [Internal Flow Signatures for Self-Checking and Refinement in LLMs](https://arxiv.org/abs/2602.01897)
*Sungheon Jeong,Sanggeon Yun,Ryozo Masukawa,Wenjun Haung,Hanning Chen,Mohsen Imani*

Main category: cs.LG

TL;DR: 提出内部流签名方法，通过监控LLM内部深度动态来检测和修正不忠实生成，无需外部验证或修改基础模型


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可能生成与上下文不忠实但流畅的答案，现有方法多依赖外部验证或生成后的独立判断，缺乏从模型内部决策动态进行自我检查的方法

Method: 引入内部流签名：1）通过偏置中心监控稳定token级动态；2）在紧凑移动读取对齐子空间中总结轨迹；3）使用正交传输对齐相邻窗口帧；4）训练轻量GRU验证器进行自我检查；5）定位问题深度事件并针对性修正

Result: 方法能够检测不忠实生成，定位问题深度事件，并通过回滚到问题token并钳制异常传输步骤进行针对性修正，同时保持正交残差不变

Conclusion: 内部流签名方法为LLM提供了可操作的定位和低开销的自我检查能力，直接从内部决策动态实现检测和修正，无需修改基础模型

Abstract: Large language models can generate fluent answers that are unfaithful to the provided context, while many safeguards rely on external verification or a separate judge after generation. We introduce \emph{internal flow signatures} that audit decision formation from depthwise dynamics at a fixed inter-block monitoring boundary. The method stabilizes token-wise motion via bias-centered monitoring, then summarizes trajectories in compact \emph{moving} readout-aligned subspaces constructed from the top token and its close competitors within each depth window. Neighboring window frames are aligned by an orthogonal transport, yielding depth-comparable transported step lengths, turning angles, and subspace drift summaries that are invariant to within-window basis choices. A lightweight GRU validator trained on these signatures performs self-checking without modifying the base model. Beyond detection, the validator localizes a culprit depth event and enables a targeted refinement: the model rolls back to the culprit token and clamps an abnormal transported step at the identified block while preserving the orthogonal residual. The resulting pipeline provides actionable localization and low-overhead self-checking from internal decision dynamics. \emph{Code is available at} \texttt{github.com/EavnJeong/Internal-Flow-Signatures-for-Self-Checking-and-Refinement-in-LLMs}.

</details>


### [312] [Observation-dependent Bayesian active learning via input-warped Gaussian processes](https://arxiv.org/abs/2602.01898)
*Sanna Jarl,Maria Bånkestad,Jonathan J. S. Scragg,Jens Sjölund*

Main category: cs.LG

TL;DR: 提出一种通过单调重参数化扭曲输入空间的方法，使贝叶斯主动学习中的探索策略能够根据观测反馈调整，提高采样效率


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程代理模型的预测方差仅通过超参数依赖观测输出，导致探索策略对实际测量不敏感。需要一种机制使设计策略能够根据观测变异性调整输入空间

Method: 通过学习的单调重参数化扭曲输入空间，使设计策略能够根据观测变异性扩展或压缩输入空间区域，从而塑造基于方差的采集函数行为。提出新的自监督目标训练这种扭曲

Result: 该方法在一系列主动学习基准测试中提高了采样效率，特别是在非平稳性挑战传统方法的场景下表现优异

Conclusion: 通过输入空间扭曲注入观测依赖反馈是提高贝叶斯主动学习采样效率的有效方法，自监督训练目标优于传统的边际似然训练

Abstract: Bayesian active learning relies on the precise quantification of predictive uncertainty to explore unknown function landscapes. While Gaussian process surrogates are the standard for such tasks, an underappreciated fact is that their posterior variance depends on the observed outputs only through the hyperparameters, rendering exploration largely insensitive to the actual measurements. We propose to inject observation-dependent feedback by warping the input space with a learned, monotone reparameterization. This mechanism allows the design policy to expand or compress regions of the input space in response to observed variability, thereby shaping the behavior of variance-based acquisition functions. We demonstrate that while such warps can be trained via marginal likelihood, a novel self-supervised objective yields substantially better performance. Our approach improves sample efficiency across a range of active learning benchmarks, particularly in regimes where non-stationarity challenges traditional methods.

</details>


### [313] [Data- and Variance-dependent Regret Bounds for Online Tabular MDPs](https://arxiv.org/abs/2602.01903)
*Mingyi Li,Taira Tsuchiya,Kenji Yamanishi*

Main category: cs.LG

TL;DR: 该论文研究在线表格MDP，开发了"两全其美"算法，在对抗性环境中实现数据依赖的遗憾界，在随机环境中实现方差依赖的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有MDP算法通常在对抗性或随机环境中单独优化，缺乏能同时适应两种环境的统一框架。需要开发既能处理对抗性环境中的数据依赖复杂性，又能利用随机环境中方差信息的算法。

Method: 提出两种算法：基于全局优化的算法和基于策略优化的算法。两者都建立在具有对数障碍正则化的乐观跟随正则化领导者（OFTRL）框架上。全局优化算法直接优化价值函数，策略优化算法使用新的乐观Q函数估计器。

Result: 全局优化算法在对抗性环境中实现一阶、二阶和路径长度遗憾界；在随机环境中实现方差感知的间隙无关界和间隙依赖界（对episode数呈多对数）。策略优化算法达到类似的自适应性（相差一个episode长度因子）。建立了数据依赖复杂性度量的遗憾下界，证明全局优化方法的上界接近最优。

Conclusion: 该工作开发了能同时适应对抗性和随机环境的统一算法框架，通过新的复杂性度量实现了精细的数据和方差依赖遗憾界，为在线MDP学习提供了理论上的最优保证。

Abstract: This work studies online episodic tabular Markov decision processes (MDPs) with known transitions and develops best-of-both-worlds algorithms that achieve refined data-dependent regret bounds in the adversarial regime and variance-dependent regret bounds in the stochastic regime. We quantify MDP complexity using a first-order quantity and several new data-dependent measures for the adversarial regime, including a second-order quantity and a path-length measure, as well as variance-based measures for the stochastic regime. To adapt to these measures, we develop algorithms based on global optimization and policy optimization, both built on optimistic follow-the-regularized-leader with log-barrier regularization. For global optimization, our algorithms achieve first-order, second-order, and path-length regret bounds in the adversarial regime, and in the stochastic regime, they achieve a variance-aware gap-independent bound and a variance-aware gap-dependent bound that is polylogarithmic in the number of episodes. For policy optimization, our algorithms achieve the same data- and variance-dependent adaptivity, up to a factor of the episode horizon, by exploiting a new optimistic $Q$-function estimator. Finally, we establish regret lower bounds in terms of data-dependent complexity measures for the adversarial regime and a variance measure for the stochastic regime, implying that the regret upper bounds achieved by the global-optimization approach are nearly optimal.

</details>


### [314] [Towards Long-Horizon Interpretability: Efficient and Faithful Multi-Token Attribution for Reasoning LLMs](https://arxiv.org/abs/2602.01914)
*Wenbo Pan,Zhichao Liu,Xianlong Wang,Haining Yu,Xiaohua Jia*

Main category: cs.LG

TL;DR: FlashTrace：一种高效的多token归因方法，通过跨段聚合和递归归因机制解决长上下文归因的效率瓶颈和忠实度下降问题


<details>
  <summary>Details</summary>
Motivation: 随着现代LLM越来越依赖扩展推理链，现有token归因方法面临两个关键挑战：1) 效率瓶颈 - 在长度为N的上下文中归因M个目标token需要O(M*N)操作，长上下文归因速度极慢；2) 忠实度下降 - 中间推理token吸收归因质量，阻止重要性传播回原始输入

Method: 提出FlashTrace方法：1) 使用跨段聚合在单次传递中计算多token目标的归因；2) 设计递归归因机制，通过中间推理链追踪重要性回到源输入；3) 保持归因的忠实性

Result: 在长上下文检索(RULER)和多步推理(MATH, MorehopQA)任务上的实验表明，FlashTrace相比现有基线实现超过130倍加速，同时保持更优的忠实度。递归归因分析显示，即使单次递归跳转也能通过追踪推理链中的重要性提高忠实度

Conclusion: FlashTrace通过高效的多token归因和递归归因机制，有效解决了长上下文归因中的效率和忠实度问题，为理解LLM的扩展推理过程提供了实用的解释工具

Abstract: Token attribution methods provide intuitive explanations for language model outputs by identifying causally important input tokens. However, as modern LLMs increasingly rely on extended reasoning chains, existing schemes face two critical challenges: (1) efficiency bottleneck, where attributing a target span of M tokens within a context of length N requires O(M*N) operations, making long-context attribution prohibitively slow; and (2) faithfulness drop, where intermediate reasoning tokens absorb attribution mass, preventing importance from propagating back to the original input. To address these, we introduce FlashTrace, an efficient multi-token attribution method that employs span-wise aggregation to compute attribution over multi-token targets in a single pass, while maintaining faithfulness. Moreover, we design a recursive attribution mechanism that traces importance through intermediate reasoning chains back to source inputs. Extensive experiments on long-context retrieval (RULER) and multi-step reasoning (MATH, MorehopQA) tasks demonstrate that FlashTrace achieves over 130x speedup over existing baselines while maintaining superior faithfulness. We further analyze the dynamics of recursive attribution, showing that even a single recursive hop improves faithfulness by tracing importance through the reasoning chain.

</details>


### [315] [VLM-Guided Experience Replay](https://arxiv.org/abs/2602.01915)
*Elad Sharony,Tom Jurgenson,Orr Krupnik,Dotan Di Castro,Shie Mannor*

Main category: cs.LG

TL;DR: 利用预训练的视觉语言模型（VLM）作为自动评估器，指导强化学习回放缓冲区中经验子轨迹的优先级排序，无需微调即可显著提升样本效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型和视觉语言模型已被整合到强化学习的各个组件中，但回放缓冲区这一存储和重用经验的核心组件尚未被探索。本文旨在填补这一空白，利用VLM指导回放缓冲区中经验的优先级排序。

Method: 使用冻结的预训练VLM作为自动评估器，识别并优先处理智能体经验中有前景的子轨迹。该方法无需微调VLM，适用于离散和连续领域。

Result: 在游戏和机器人等多种场景中，使用该优先级排序方法的智能体相比先前方法，平均成功率提升11-52%，样本效率提高19-45%。

Conclusion: 利用预训练VLM指导回放缓冲区优先级排序是一种有效的方法，能显著提升强化学习的样本效率和性能，为VLM在RL中的新应用开辟了道路。

Abstract: Recent advances in Large Language Models (LLMs) and Vision-Language Models (VLMs) have enabled powerful semantic and multimodal reasoning capabilities, creating new opportunities to enhance sample efficiency, high-level planning, and interpretability in reinforcement learning (RL). While prior work has integrated LLMs and VLMs into various components of RL, the replay buffer, a core component for storing and reusing experiences, remains unexplored. We propose addressing this gap by leveraging VLMs to guide the prioritization of experiences in the replay buffer. Our key idea is to use a frozen, pre-trained VLM (requiring no fine-tuning) as an automated evaluator to identify and prioritize promising sub-trajectories from the agent's experiences. Across scenarios, including game-playing and robotics, spanning both discrete and continuous domains, agents trained with our proposed prioritization method achieve 11-52% higher average success rates and improve sample efficiency by 19-45% compared to previous approaches. https://esharony.me/projects/vlm-rb/

</details>


### [316] [PIMPC-GNN: Physics-Informed Multi-Phase Consensus Learning for Enhancing Imbalanced Node Classification in Graph Neural Networks](https://arxiv.org/abs/2602.01920)
*Abdul Joseph Fofanah,Lian Wen,David Chen*

Main category: cs.LG

TL;DR: 提出PIMPC-GNN框架，通过物理启发的多相共识机制解决图神经网络中的类别不平衡问题，显著提升少数类召回率和平衡准确率。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在类别不平衡场景中表现不佳，少数类样本不足导致预测偏向多数类，需要有效处理不平衡节点分类问题。

Method: 整合热力学扩散、Kuramoto同步和谱嵌入三种互补动态机制，通过类别自适应集成权重和结合平衡交叉熵与物理约束的不平衡感知损失函数进行训练。

Result: 在5个基准数据集和5-100的不平衡比例下，优于16个最先进基线方法，少数类召回率提升达12.7%，平衡准确率提升达8.3%。

Conclusion: PIMPC-GNN不仅提升了不平衡节点分类性能，还为图学习中的共识动态提供了可解释的见解，代码已开源。

Abstract: Graph neural networks (GNNs) often struggle in class-imbalanced settings, where minority classes are under-represented and predictions are biased toward majorities. We propose \textbf{PIMPC-GNN}, a physics-informed multi-phase consensus framework for imbalanced node classification. Our method integrates three complementary dynamics: (i) thermodynamic diffusion, which spreads minority labels to capture long-range dependencies, (ii) Kuramoto synchronisation, which aligns minority nodes through oscillatory consensus, and (iii) spectral embedding, which separates classes via structural regularisation. These perspectives are combined through class-adaptive ensemble weighting and trained with an imbalance-aware loss that couples balanced cross-entropy with physics-based constraints. Across five benchmark datasets and imbalance ratios from 5-100, PIMPC-GNN outperforms 16 state-of-the-art baselines, achieving notable gains in minority-class recall (up to +12.7\%) and balanced accuracy (up to +8.3\%). Beyond empirical improvements, the framework also provides interpretable insights into consensus dynamics in graph learning. The code is available at \texttt{https://github.com/afofanah/PIMPC-GNN}.

</details>


### [317] [Embedding Learning on Multiplex Networks for Link Prediction](https://arxiv.org/abs/2602.01922)
*Orell Trautmann,Olaf Wolkenhauer,Clémence Réda*

Main category: cs.LG

TL;DR: 这篇综述论文回顾了多层网络嵌入学习方法在链接预测任务中的应用，提出了新的分类体系，并解决了评估的公平性和可重复性问题。


<details>
  <summary>Details</summary>
Motivation: 随着网络复杂性的增加（连接数量和交互类型的增多），多层网络嵌入学习变得越来越具有挑战性。现有的嵌入学习方法在多层网络上的应用需要系统性的分类和公平的评估框架。

Method: 1. 提出细化的分类体系，根据嵌入类型和技术对模型进行分类和比较；2. 回顾并解决多层网络嵌入学习在链接预测任务中的可重复性和公平评估问题；3. 针对有向多层网络提出新颖且公平的测试程序。

Result: 建立了多层网络嵌入学习模型的系统分类框架，解决了评估的公平性和可重复性问题，为有向多层网络提供了专门的测试方法。

Conclusion: 这篇综述为开发更高效、更易处理的多层网络嵌入学习方法及其公平评估奠定了基础，提供了模型评估指南，并对当前可用的工具和面临的挑战提供了有见地的视角。

Abstract: Over the past years, embedding learning on networks has shown tremendous results in link prediction tasks for complex systems, with a wide range of real-life applications. Learning a representation for each node in a knowledge graph allows us to capture topological and semantic information, which can be processed in downstream analyses later. In the link prediction task, high-dimensional network information is encoded into low-dimensional vectors, which are then fed to a predictor to infer new connections between nodes in the network. As the network complexity (that is, the numbers of connections and types of interactions) grows, embedding learning turns out increasingly challenging. This review covers published models on embedding learning on multiplex networks for link prediction. First, we propose refined taxonomies to classify and compare models, depending on the type of embeddings and embedding techniques. Second, we review and address the problem of reproducible and fair evaluation of embedding learning on multiplex networks for the link prediction task. Finally, we tackle evaluation on directed multiplex networks by proposing a novel and fair testing procedure. This review constitutes a crucial step towards the development of more performant and tractable embedding learning approaches for multiplex networks and their fair evaluation for the link prediction task. We also suggest guidelines on the evaluation of models, and provide an informed perspective on the challenges and tools currently available to address downstream analyses applied to multiplex networks.

</details>


### [318] [Bayesian Integration of Nonlinear Incomplete Clinical Data](https://arxiv.org/abs/2602.01924)
*Lucía González-Zamorano,Nuria Balbás-Esteban,Vanessa Gómez-Verdejo,Albert Belenguer-Llorens,Carlos Sevilla-Salcedo*

Main category: cs.LG

TL;DR: BIONIC是一个贝叶斯多模态临床数据集成框架，通过生成-判别式潜在架构处理高维异构数据和结构化缺失，在缺失数据场景下表现优异并提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 多模态临床数据具有高维度、异构表示和结构化缺失的特点，这给预测建模、数据集成和可解释性带来了重大挑战。现有方法难以有效处理这些复杂特征。

Method: 提出BIONIC框架，采用贝叶斯集成非线性不完全临床数据的方法，使用预训练嵌入处理医学图像和临床文本等复杂模态，将结构化临床变量纳入贝叶斯多模态公式，通过联合生成-判别式潜在架构显式建模模态级和变量级缺失以及缺失标签。

Result: 在三个多模态临床和生物医学数据集上评估，相比代表性多模态基线方法，BIONIC展现出强大且一致的判别性能，特别是在不完全数据场景下表现突出。

Conclusion: BIONIC不仅提供预测准确性，还通过其潜在结构提供内在可解释性，支持模态相关性的群体级分析和临床有意义的洞察，为多模态临床数据分析提供了统一框架。

Abstract: Multimodal clinical data are characterized by high dimensionality, heterogeneous representations, and structured missingness, posing significant challenges for predictive modeling, data integration, and interpretability. We propose BIONIC (Bayesian Integration of Nonlinear Incomplete Clinical data), a unified probabilistic framework that integrates heterogeneous multimodal data under missingness through a joint generative-discriminative latent architecture. BIONIC uses pretrained embeddings for complex modalities such as medical images and clinical text, while incorporating structured clinical variables directly within a Bayesian multimodal formulation. The proposed framework enables robust learning in partially observed and semi-supervised settings by explicitly modeling modality-level and variable-level missingness, as well as missing labels. We evaluate BIONIC on three multimodal clinical and biomedical datasets, demonstrating strong and consistent discriminative performance compared to representative multimodal baselines, particularly under incomplete data scenarios. Beyond predictive accuracy, BIONIC provides intrinsic interpretability through its latent structure, enabling population-level analysis of modality relevance and supporting clinically meaningful insight.

</details>


### [319] [COLT: Lightweight Multi-LLM Collaboration through Shared MCTS Reasoning for Model Compilation](https://arxiv.org/abs/2602.01935)
*Annabelle Sujun Tang,Christopher Priebe,Lianhui Qin,Hadi Esmaeilzadeh*

Main category: cs.LG

TL;DR: COLT：一个轻量级多LLM协作框架，通过共享MCTS树实现编译器优化，用小模型主导搜索，大模型兜底，降低推理成本


<details>
  <summary>Details</summary>
Motivation: AI系统中模型服务成本占主导，编译器优化对可扩展部署至关重要。现有方法使用单一大型LLM指导编译器搜索成本高，而小模型单独使用可靠性不足。需要探索多LLM协作能否达到或超越单一大型模型的性能

Method: 提出COLT框架，在单一MCTS过程中实现多LLM协调推理。关键创新是使用共享MCTS树作为协作基础，重用转换前缀和跨模型价值传播。每个迭代中，执行LLM提出联合动作（编译器转换，下一个查询的模型）。引入模型感知树策略偏向小模型同时保持探索，以及当搜索出现持续回归时升级到大模型的航向修正机制

Result: 论文表明多LLM协作推理主要依赖小LLM，能够匹配或超越单一大型模型的性能

Conclusion: COLT通过轻量级多LLM协作框架，在降低推理成本的同时保持编译器优化性能，为AI系统部署提供了可扩展的解决方案

Abstract: Model serving costs dominate AI systems, making compiler optimization essential for scalable deployment. Recent works show that a large language model (LLM) can guide compiler search by reasoning over program structure and optimization history. However, using a single large model throughout the search is expensive, while smaller models are less reliable when used alone. Thus, this paper seeks to answer whether multi-LLM collaborative reasoning relying primarily on small LLMs can match or exceed the performance of a single large model. As such, we propose a lightweight collaborative multi-LLM framework, dubbed COLT, for compiler optimization that enables coordinated reasoning across multiple models within a single Monte Carlo tree search (MCTS) process. A key contribution is the use of a single shared MCTS tree as the collaboration substrate across LLMs, enabling the reuse of transformation prefixes and cross-model value propagation. Hence, we circumvent both heavy internal reasoning mechanisms and conventional agentic machinery that relies on external planners, multiple concurrent LLMs, databases, external memory/versioning of intermediate results, and controllers by simply endogenizing model selection within the lightweight MCTS optimization loop. Every iteration, the acting LLM proposes a joint action: (compiler transformation, model to be queried next). We also introduce a model-aware tree policy that biases search toward smaller models while preserving exploration, and a course-alteration mechanism that escalates to the largest model when the search exhibits persistent regressions attributable to smaller models.

</details>


### [320] [PIMCST: Physics-Informed Multi-Phase Consensus and Spatio-Temporal Few-Shot Learning for Traffic Flow Forecasting](https://arxiv.org/abs/2602.01936)
*Abdul Joseph Fofanah,Lian Wen,David Chen*

Main category: cs.LG

TL;DR: MCPST是一个用于少样本交通预测的多阶段共识时空框架，通过建模交通动态的扩散、同步和谱嵌入，自适应融合预测，并采用结构化元学习快速适应新城市，在跨域少样本场景中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统中的交通流预测在跨域、数据稀缺场景下面临挑战，有限的历史数据阻碍了模型训练和泛化能力，复杂的时空依赖性和非线性动态使得不同城市间的少样本学习变得困难。

Method: 提出MCPST框架：1) 多阶段引擎通过扩散、同步和谱嵌入建模交通动态；2) 自适应共识机制动态融合各阶段预测并确保一致性；3) 结构化元学习策略用于快速适应新城市的最小数据需求。

Result: 在四个真实数据集上的实验表明，MCPST在时空图学习方法、动态图迁移学习方法、基于提示的时空预测方法和跨域少样本设置中，优于14种最先进方法，提高了预测准确性，减少了所需训练数据，并提供可解释的见解。

Conclusion: MCPST将交通预测重新概念化为多阶段共识学习问题，通过理论保证和实证验证，为跨域少样本交通预测提供了有效的解决方案，代码已开源。

Abstract: Accurate traffic flow prediction remains a fundamental challenge in intelligent transportation systems, particularly in cross-domain, data-scarce scenarios where limited historical data hinders model training and generalisation. The complex spatio-temporal dependencies and nonlinear dynamics of urban mobility networks further complicate few-shot learning across different cities. This paper proposes MCPST, a novel Multi-phase Consensus Spatio-Temporal framework for few-shot traffic forecasting that reconceptualises traffic prediction as a multi-phase consensus learning problem. Our framework introduces three core innovations: (1) a multi-phase engine that models traffic dynamics through diffusion, synchronisation, and spectral embeddings for comprehensive dynamic characterisation; (2) an adaptive consensus mechanism that dynamically fuses phase-specific predictions while enforcing consistency; and (3) a structured meta-learning strategy for rapid adaptation to new cities with minimal data. We establish extensive theoretical guarantees, including representation theorems with bounded approximation errors and generalisation bounds for few-shot adaptation. Through experiments on four real-world datasets, MCPST outperforms fourteen state-of-the-art methods in spatio-temporal graph learning methods, dynamic graph transfer learning methods, prompt-based spatio-temporal prediction methods and cross-domain few-shot settings, improving prediction accuracy while reducing required training data and providing interpretable insights. The implementation code is available at https://github.com/afofanah/MCPST.

</details>


### [321] [T-LLM: Teaching Large Language Models to Forecast Time Series via Temporal Distillation](https://arxiv.org/abs/2602.01937)
*Suhan Guo,Bingxu Wang,Shaodan Zhang,Furao Shen*

Main category: cs.LG

TL;DR: T-LLM是一个时间序列预测框架，通过从轻量级时间教师模型蒸馏预测行为，使通用大语言模型获得时间序列预测能力，在多种设置下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据与底层过程演化紧密相关，只能随真实时间积累，这限制了仅靠规模驱动的预训练效果。现有方法主要依赖表示级对齐或推理时时间模块，而非明确教授LLM预测行为。

Method: 提出T-LLM时间蒸馏框架，在训练期间从轻量级时间教师模型转移预测行为给LLM。教师模型结合趋势建模和频域分析提供结构化时间监督，推理时完全移除教师模型，仅保留LLM作为预测模型。

Result: 在基准数据集和传染病预测任务上的实验表明，T-LLM在全样本、少样本和零样本设置下始终优于现有的基于LLM的预测方法，同时实现了简单高效的部署流程。

Conclusion: T-LLM通过时间蒸馏有效赋予通用LLM时间序列预测能力，解决了时间约束带来的挑战，提供了一种将预测行为直接教授给LLM的新方法。

Abstract: Time series forecasting plays a critical role in decision-making across many real-world applications. Unlike data in vision and language domains, time series data is inherently tied to the evolution of underlying processes and can only accumulate as real-world time progresses, limiting the effectiveness of scale-driven pretraining alone. This time-bound constraint poses a challenge for enabling large language models (LLMs) to acquire forecasting capability, as existing approaches primarily rely on representation-level alignment or inference-time temporal modules rather than explicitly teaching forecasting behavior to the LLM. We propose T-LLM, a temporal distillation framework that equips general-purpose LLMs with time series forecasting capability by transferring predictive behavior from a lightweight temporal teacher during training. The teacher combines trend modeling and frequency-domain analysis to provide structured temporal supervision, and is removed entirely at inference, leaving the LLM as the sole forecasting model. Experiments on benchmark datasets and infectious disease forecasting tasks demonstrate that T-LLM consistently outperforms existing LLM-based forecasting methods under full-shot, few-shot, and zero-shot settings, while enabling a simple and efficient deployment pipeline.

</details>


### [322] [Boundary-Constrained Diffusion Models for Floorplan Generation: Balancing Realism and Diversity](https://arxiv.org/abs/2602.01949)
*Leonardo Stoppani,Davide Bacciu,Shahab Mokarizadeh*

Main category: cs.LG

TL;DR: 提出多样性评分(DS)衡量固定约束下的布局多样性，引入边界交叉注意力(BCA)提升几何一致性，发现FID优化导致多样性下降，揭示真实性与多样性之间的权衡


<details>
  <summary>Details</summary>
Motivation: 扩散模型在自动平面图生成中广泛应用，但基于FID等感知指标的优化导致设计多样性有限，需要量化布局多样性并提升几何一致性

Method: 提出多样性评分(DS)量化固定约束下的布局多样性；引入边界交叉注意力(BCA)模块，使模型能够基于建筑边界进行条件生成

Result: BCA显著提升边界遵循度；长时间训练导致多样性崩溃但FID无法诊断；揭示真实性与多样性之间的关键权衡；OOD评估显示模型依赖数据集先验

Conclusion: 在建筑设计中需要明确平衡保真度、多样性和泛化能力的生成系统，传统指标如FID无法捕捉多样性问题，需要新的评估方法

Abstract: Diffusion models have become widely popular for automated floorplan generation, producing highly realistic layouts conditioned on user-defined constraints. However, optimizing for perceptual metrics such as the Fréchet Inception Distance (FID) causes limited design diversity. To address this, we propose the Diversity Score (DS), a metric that quantifies layout diversity under fixed constraints. Moreover, to improve geometric consistency, we introduce a Boundary Cross-Attention (BCA) module that enables conditioning on building boundaries. Our experiments show that BCA significantly improves boundary adherence, while prolonged training drives diversity collapse undiagnosed by FID, revealing a critical trade-off between realism and diversity. Out-Of-Distribution evaluations further demonstrate the models' reliance on dataset priors, emphasizing the need for generative systems that explicitly balance fidelity, diversity, and generalization in architectural design tasks.

</details>


### [323] [Deep Multivariate Models with Parametric Conditionals](https://arxiv.org/abs/2602.01953)
*Dmitrij Schlesinger,Boris Flach,Alexander Shekhovtsov*

Main category: cs.LG

TL;DR: 提出一种基于条件概率分布的深度多元模型，通过训练参数化马尔可夫链核来学习联合分布，适用于多种下游任务和半监督学习场景。


<details>
  <summary>Details</summary>
Motivation: 现有深度多元模型通常针对特定应用任务设计，限制了在其他下游任务中的适用性。需要一种更通用的建模方法，能够灵活处理异构变量集合（如图像、分割、属性、隐变量等）。

Method: 通过每个变量组相对于其他变量的条件概率分布来表示联合概率分布，将学习问题转化为训练参数化马尔可夫链核，最大化其极限分布的数据似然。

Result: 该方法能够构建适用于几乎所有可能下游任务的模型，同时支持广泛的半监督学习场景，提高了模型的通用性和灵活性。

Conclusion: 提出的条件概率分布建模方法比传统任务特定设计更具通用性，通过马尔可夫链核训练实现了灵活的下游任务适应性和半监督学习能力。

Abstract: We consider deep multivariate models for heterogeneous collections of random variables. In the context of computer vision, such collections may e.g. consist of images, segmentations, image attributes, and latent variables. When developing such models, most existing works start from an application task and design the model components and their dependencies to meet the needs of the chosen task. This has the disadvantage of limiting the applicability of the resulting model for other downstream tasks. Here, instead, we propose to represent the joint probability distribution by means of conditional probability distributions for each group of variables conditioned on the rest. Such models can then be used for practically any possible downstream task. Their learning can be approached as training a parametrised Markov chain kernel by maximising the data likelihood of its limiting distribution. This has the additional advantage of allowing a wide range of semi-supervised learning scenarios.

</details>


### [324] [Efficient Epistemic Uncertainty Estimation for Large Language Models via Knowledge Distillation](https://arxiv.org/abs/2602.01956)
*Seonghyeon Park,Jewon Yeom,Jaewon Sok,Jeongjae Park,Heejun Kim,Taesup Kim*

Main category: cs.LG

TL;DR: 提出利用小型草稿模型高效估计LLM中token级认知不确定性的框架，避免全规模集成的高计算成本


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的不确定性量化对于缓解幻觉和在安全关键任务中实现风险感知部署至关重要，但通过深度集成估计认知不确定性在现代模型规模下计算成本过高

Method: 基于偏差-方差分解理论框架，通过草稿模型间的Jensen-Shannon散度（方差代理）和草稿混合与目标模型间的KL散度（偏差代理）近似认知不确定性；引入在线随机蒸馏高效近似目标聚合，以及数据多样化草稿策略增强草稿多样性

Result: 在GSM8K上实验表明，该方法将估计误差（RMSE）降低高达37%；在幻觉检测性能上与TokUR等重扰动方法相当，同时推理成本可忽略

Conclusion: 该方法为不确定性感知的LLM部署提供了实用解决方案，能够高效估计token级认知不确定性，显著降低计算成本

Abstract: Quantifying uncertainty in Large Language Models (LLMs) is essential for mitigating hallucinations and enabling risk-aware deployment in safety-critical tasks. However, estimating Epistemic Uncertainty(EU) via Deep Ensembles is computationally prohibitive at the scale of modern models. We propose a framework that leverages the small draft models to efficiently estimate token-level EU, bypassing the need for full-scale ensembling. Theoretically grounded in a Bias-Variance Decomposition, our approach approximates EU via Jensen-Shannon divergence among drafts (variance proxy) and KL divergence between the draft mixture and the target (bias proxy). To further ensure accuracy without significant overhead, we introduce Online Stochastic Distillation (OSD) to efficiently approximate target aggregation and the Data-Diverse Drafts (DDD) strategy to enhance draft diversity for better target approximation. Extensive experiments on GSM8K demonstrate that our method reduces the estimation error (RMSE) by up to 37% compared to baselines. Crucially, our approach achieves Hallucination Detection performance competitive with heavy perturbation-based methods like TokUR while incurring negligible inference costs, offering a practical solution for uncertainty-aware LLM deployment.

</details>


### [325] [Grounding Generated Videos in Feasible Plans via World Models](https://arxiv.org/abs/2602.01960)
*Christos Ziakas,Amir Bar,Alessandra Russo*

Main category: cs.LG

TL;DR: GVP-WM：通过世界模型将视频生成计划转化为可行动作序列的规划方法


<details>
  <summary>Details</summary>
Motivation: 大规模视频生成模型作为零样本视觉规划器显示出潜力，但视频生成的计划经常违反时间一致性和物理约束，导致映射到可执行动作时失败

Method: 提出GVP-WM方法：1）从初始和目标观察生成视频计划；2）通过视频引导的潜在配准将视频指导投影到动态可行的潜在轨迹流形上；3）将接地问题表述为目标条件潜在空间轨迹优化问题，在世界模型动态约束下联合优化潜在状态和动作，同时保持与视频生成计划的语义对齐

Result: GVP-WM能够从违反物理约束的零样本图像到视频生成和运动模糊视频中恢复可行的长时程计划，在导航和操作模拟任务中表现良好

Conclusion: GVP-WM通过世界模型将视频生成计划接地为可行动作序列，解决了视频生成计划违反物理约束的问题，提高了规划的可执行性

Abstract: Large-scale video generative models have shown emerging capabilities as zero-shot visual planners, yet video-generated plans often violate temporal consistency and physical constraints, leading to failures when mapped to executable actions. To address this, we propose Grounding Video Plans with World Models (GVP-WM), a planning method that grounds video-generated plans into feasible action sequences using a learned action-conditioned world model. At test-time, GVP-WM first generates a video plan from initial and goal observations, then projects the video guidance onto the manifold of dynamically feasible latent trajectories via video-guided latent collocation. In particular, we formulate grounding as a goal-conditioned latent-space trajectory optimization problem that jointly optimizes latent states and actions under world-model dynamics, while preserving semantic alignment with the video-generated plan. Empirically, GVP-WM recovers feasible long-horizon plans from zero-shot image-to-video-generated and motion-blurred videos that violate physical constraints, across navigation and manipulation simulation tasks.

</details>


### [326] [Zero-Shot Off-Policy Learning](https://arxiv.org/abs/2602.01962)
*Arip Asadulaev,Maksim Bobrin,Salem Lahlou,Dmitry Dylov,Fakhri Karray,Martin Takac*

Main category: cs.LG

TL;DR: 该论文提出了一种零样本强化学习方法，通过建立后继度量与平稳密度比的理论联系，实现了在无需额外训练的情况下快速适应新任务。


<details>
  <summary>Details</summary>
Motivation: 离策略学习面临分布偏移和价值函数高估偏差的挑战，在零样本强化学习中尤为明显。研究者希望解决在零样本设置下的离策略问题，使智能体能够在无需额外训练的情况下适应新任务。

Method: 通过发现后继度量与平稳密度比的理论联系，提出算法能够推断最优重要性采样比，在运行时为任何任务执行平稳分布校正。该方法可无缝集成到前向-后向表示框架中。

Result: 在SMPL Humanoid的运动跟踪任务、ExoRL的连续控制任务以及长时程OGBench任务上进行了基准测试，展示了方法的有效性。

Conclusion: 这项工作桥接了离策略学习和零样本适应，为两个研究领域都带来了益处，实现了在训练无关机制下对新任务的快速适应。

Abstract: Off-policy learning methods seek to derive an optimal policy directly from a fixed dataset of prior interactions. This objective presents significant challenges, primarily due to the inherent distributional shift and value function overestimation bias. These issues become even more noticeable in zero-shot reinforcement learning, where an agent trained on reward-free data must adapt to new tasks at test time without additional training. In this work, we address the off-policy problem in a zero-shot setting by discovering a theoretical connection of successor measures to stationary density ratios. Using this insight, our algorithm can infer optimal importance sampling ratios, effectively performing a stationary distribution correction with an optimal policy for any task on the fly. We benchmark our method in motion tracking tasks on SMPL Humanoid, continuous control on ExoRL, and for the long-horizon OGBench tasks. Our technique seamlessly integrates into forward-backward representation frameworks and enables fast-adaptation to new tasks in a training-free regime. More broadly, this work bridges off-policy learning and zero-shot adaptation, offering benefits to both research areas.

</details>


### [327] [Self-Consolidation for Self-Evolving Agents](https://arxiv.org/abs/2602.01966)
*Hongzhuo Yu,Fei Zhu,Guo-Sen Xie,Ling Shao*

Main category: cs.LG

TL;DR: 提出了一种LLM智能体自我进化框架，通过对比反思策略总结错误模式，并通过自我整合机制将文本经验蒸馏为可学习参数，实现长期进化。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体通常是静态系统，缺乏终身交互进化的能力。现有方法主要依赖检索成功轨迹作为演示，但存在两个关键局限：1）只关注成功，忽略了失败尝试中的教学价值；2）持续积累文本经验会增加检索时间、引入噪声并耗尽上下文窗口。

Method: 提出包含两个互补进化机制的新框架：1）对比反思策略，显式总结易错模式并捕捉可重用见解；2）自我整合机制，将非参数化文本经验蒸馏为紧凑的可学习参数，使智能体能够将大量历史经验内化到其潜在空间中。

Result: 大量实验证明了该方法在长期智能体进化方面的优势。

Conclusion: 该框架通过结合对比反思和自我整合机制，解决了现有LLM智能体进化方法的局限性，实现了更有效的长期学习和适应能力。

Abstract: While large language model (LLM) agents have demonstrated impressive problem-solving capabilities, they typically operate as static systems, lacking the ability to evolve through lifelong interaction. Existing attempts to bridge this gap primarily rely on retrieving successful past trajectories as demonstrations. However, this paradigm faces two critical limitations. First, by focusing solely on success, agents overlook the rich pedagogical value embedded in failed attempts, preventing them from identifying and avoiding recurrent pitfalls. Second, continually accumulating textual experiences not only increases the time consumption during retrieval but also inevitably introduces noise and exhausts the largest context window of current LLMs. To address these challenges, we propose a novel self-evolving framework for LLM agents that introduces a complementary evolution mechanism: First, a contrastive reflection strategy is introduced to explicitly summarize error-prone patterns and capture reusable insights. Second, we propose a self-consolidation mechanism that distills non-parametric textual experience into compact learnable parameters. This enables the agent to internalize extensive historical experience directly into its latent space. Extensive experiments demonstrate the advantages of our method in long-term agent evolution.

</details>


### [328] [IntraSlice: Towards High-Performance Structural Pruning with Block-Intra PCA for LLMs](https://arxiv.org/abs/2602.01975)
*Meng Li,Peisong Wang,Yuantian Shao,Qinghao Hu,Hongjian Fang,Yifan Zhang,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: 提出IntraSlice框架，通过模块内PCA压缩剪枝解决LLM部署中的性能退化问题，相比现有方法在相同压缩比或推理速度下表现更优


<details>
  <summary>Details</summary>
Motivation: 大型语言模型部署面临巨大规模挑战，结构化剪枝虽能加速但导致显著性能下降，现有PCA剪枝方法仅应用于模块间，会引入额外参数并严重破坏残差连接带来的激活分布

Method: 提出IntraSlice框架，采用块级模块内PCA压缩剪枝，利用Transformer模块结构特性设计近似PCA方法，其变换矩阵可完全融合到模型中而不增加参数；引入基于PCA的全局剪枝比例估计器，在传统模块重要性基础上进一步考虑压缩激活的分布

Result: 在Llama2、Llama3和Phi系列模型上验证，实验结果表明在相同压缩比或推理速度下，相比现有基线方法获得更优的压缩性能

Conclusion: IntraSlice框架通过模块内PCA压缩剪枝有效解决了LLM部署中的性能退化问题，在保持模型性能的同时实现了高效压缩

Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks but face deployment challenges due to their massive size. Structured pruning offers acceleration benefits but leads to significant performance degradation. Recent PCA-based pruning methods have alleviated this issue by retaining key activation components, but are only applied between modules in order to fuse the transformation matrix, which introduces extra parameters and severely disrupts activation distributions due to residual connections. To address these issues, we propose IntraSlice, a framework that applies block-wise module-intra PCA compression pruning. By leveraging the structural characteristics of Transformer modules, we design an approximate PCA method whose transformation matrices can be fully fused into the model without additional parameters. We also introduce a PCA-based global pruning ratio estimator that further considers the distribution of compressed activations, building on conventional module importance. We validate our method on Llama2, Llama3, and Phi series across various language benchmarks. Experimental results demonstrate that our approach achieves superior compression performance compared to recent baselines at the same compression ratio or inference speed.

</details>


### [329] [FlyPrompt: Brain-Inspired Random-Expanded Routing with Temporal-Ensemble Experts for General Continual Learning](https://arxiv.org/abs/2602.01976)
*Hongwei Yan,Guanglong Sun,Kanglei Zhou,Qian Li,Liyuan Wang,Yi Zhong*

Main category: cs.LG

TL;DR: FlyPrompt是一个受果蝇分层记忆系统启发的通用持续学习框架，通过随机扩展分析路由器和时间集成输出头来解决专家参数分配和表示能力提升问题，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 通用持续学习面临单次通过、非平稳数据流且无明确任务边界的挑战。现有持续参数高效调优方法通常依赖多轮训练和明确任务提示，在GCL场景中效果有限，且缺乏针对专家参数分配和数据分布演化的专门设计。

Method: 受果蝇稀疏扩展和模块化集成记忆系统启发，将GCL分解为专家路由和专家能力提升两个子问题。引入随机扩展分析路由器进行实例级专家激活，以及时间集成输出头动态调整决策边界。

Result: 在CIFAR-100、ImageNet-R和CUB-200数据集上分别实现了11.23%、12.43%和7.62%的性能提升，显著优于现有最先进基线方法。

Conclusion: FlyPrompt通过脑启发设计有效解决了通用持续学习中的专家参数分配和表示能力提升问题，为单次通过、非平稳数据流学习提供了有效解决方案。

Abstract: General continual learning (GCL) challenges intelligent systems to learn from single-pass, non-stationary data streams without clear task boundaries. While recent advances in continual parameter-efficient tuning (PET) of pretrained models show promise, they typically rely on multiple training epochs and explicit task cues, limiting their effectiveness in GCL scenarios. Moreover, existing methods often lack targeted design and fail to address two fundamental challenges in continual PET: how to allocate expert parameters to evolving data distributions, and how to improve their representational capacity under limited supervision. Inspired by the fruit fly's hierarchical memory system characterized by sparse expansion and modular ensembles, we propose FlyPrompt, a brain-inspired framework that decomposes GCL into two subproblems: expert routing and expert competence improvement. FlyPrompt introduces a randomly expanded analytic router for instance-level expert activation and a temporal ensemble of output heads to dynamically adapt decision boundaries over time. Extensive theoretical and empirical evaluations demonstrate FlyPrompt's superior performance, achieving up to 11.23%, 12.43%, and 7.62% gains over state-of-the-art baselines on CIFAR-100, ImageNet-R, and CUB-200, respectively. Our source code is available at https://github.com/AnAppleCore/FlyGCL.

</details>


### [330] [SAME: Stabilized Mixture-of-Experts for Multimodal Continual Instruction Tuning](https://arxiv.org/abs/2602.01990)
*Zhen-Hao Xie,Jun-Tao Tang,Yu-Cheng Shi,Han-Jia Ye,De-Chuan Zhan,Da-Wei Zhou*

Main category: cs.LG

TL;DR: SAME方法通过稳定专家路由和防止专家漂移来解决多模态持续指令调优中的路由漂移和专家漂移问题，在无需排练的情况下实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型需要持续扩展能力，但现有的稀疏专家路由方法存在两个问题：1) 路由漂移 - 随着数据分布变化，专家选择变得不一致；2) 专家漂移 - 共享专家被新任务覆盖而失去原有功能。

Method: 提出SAME方法：1) 通过将路由动态分解到正交子空间并仅更新任务相关方向来稳定专家选择；2) 使用历史输入协方差的曲率感知缩放来调节专家更新；3) 引入自适应专家激活机制，在训练期间冻结选定专家以减少冗余计算和跨任务干扰。

Result: 大量实验表明SAME在多模态持续指令调优任务上实现了最先进的性能。

Conclusion: SAME通过解决路由漂移和专家漂移问题，为多模态持续指令调优提供了有效的解决方案，无需排练即可稳定专家路由并保护专家功能。

Abstract: Multimodal Large Language Models (MLLMs) achieve strong performance through instruction tuning, but real-world deployment requires them to continually expand their capabilities, making Multimodal Continual Instruction Tuning (MCIT) essential. Recent methods leverage sparse expert routing to promote task specialization, but we find that the expert routing process suffers from drift as the data distribution evolves. For example, a grounding query that previously activated localization experts may instead be routed to irrelevant experts after learning OCR tasks. Meanwhile, the grounding-related experts can be overwritten by new tasks and lose their original functionality. Such failure reflects two problems: router drift, where expert selection becomes inconsistent over time, and expert drift, where shared experts are overwritten across tasks. Therefore, we propose StAbilized Mixture-of-Experts (SAME) for MCIT. To address router drift, SAME stabilizes expert selection by decomposing routing dynamics into orthogonal subspaces and updating only task-relevant directions. To mitigate expert drift, we regulate expert updates via curvature-aware scaling using historical input covariance in a rehearsal-free manner. SAME also introduces adaptive expert activation to freeze selected experts during training, reducing redundant computation and cross-task interference. Extensive experiments demonstrate its SOTA performance.

</details>


### [331] [Optimizing Tensor Train Decomposition in DNNs for RISC-V Architectures Using Design Space Exploration and Compiler Optimizations](https://arxiv.org/abs/2602.01996)
*Theologos Anthimopoulos,Milad Kokhazadeh,Vasilios Kelefouras,Benjamin Himpel,Georgios Keramidas*

Main category: cs.LG

TL;DR: 提出一种针对RISC-V处理器的端到端低秩分解设计空间探索方法，通过Tensor Train分解压缩全连接层，结合编译器优化实现高效推理


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在资源受限设备（如RISC-V平台）上部署困难，全连接层计算和内存需求高，低秩分解设计空间复杂，需要权衡FLOPs、内存、推理时间和精度

Method: 使用TensorFlow T3F库的Tensor Train分解，通过剪枝低效分解形状和RISC-V架构上推理性能差的方案，并应用编译器优化提升自定义T3F层性能

Result: TT分解层平均比IREE快3倍，比Pluto快8倍，在相同压缩模型上实现更快的推理速度

Conclusion: 为基于RISC-V架构的边缘和嵌入式设备部署DNN提供了高效解决方案，通过系统化设计空间探索和编译器优化显著提升推理效率

Abstract: Deep neural networks (DNNs) have become indispensable in many real-life applications like natural language processing, and autonomous systems. However, deploying DNNs on resource-constrained devices, e.g., in RISC-V platforms, remains challenging due to the high computational and memory demands of fully connected (FC) layers, which dominate resource consumption. Low-rank factorization (LRF) offers an effective approach to compressing FC layers, but the vast design space of LRF solutions involves complex trade-offs among FLOPs, memory size, inference time, and accuracy, making the LRF process complex and time-consuming. This paper introduces an end-to-end LRF design space exploration methodology and a specialized design tool for optimizing FC layers on RISC-V processors. Using Tensor Train Decomposition (TTD) offered by TensorFlow T3F library, the proposed work prunes the LRF design space by excluding first, inefficient decomposition shapes and second, solutions with poor inference performance on RISC-V architectures. Compiler optimizations are then applied to enhance custom T3F layer performance, minimizing inference time and boosting computational efficiency. On average, our TT-decomposed layers run 3x faster than IREE and 8x faster than Pluto on the same compressed model. This work provides an efficient solution for deploying DNNs on edge and embedded devices powered by RISC-V architectures.

</details>


### [332] [On the Limits of Layer Pruning for Generative Reasoning in LLMs](https://arxiv.org/abs/2602.01997)
*Safal Shrestha,Anubhav Shrestha,Aadim Nepal,Minwu Kim,Keith Ross*

Main category: cs.LG

TL;DR: 层剪枝在分类任务上表现良好，但在生成式推理任务上严重退化，特别是在多步推理任务中。通过自生成响应的监督微调可以部分恢复性能，但生成式推理的恢复仍然有限，主要适用于低剪枝比例。


<details>
  <summary>Details</summary>
Motivation: 现有层剪枝技术在压缩大语言模型时，虽然在分类任务上表现良好，但在生成式推理任务上会出现严重性能退化。需要研究深度减少对生成式推理任务的影响，并探索在有限后训练资源下的恢复策略。

Method: 通过跨多个模型家族的系统研究，分析深度减少对多步推理任务的影响。在有限后训练资源下（无法访问预训练规模的数据或计算），评估基于自生成响应的监督微调策略。

Result: 自生成响应的监督微调在分类任务上能恢复高达90%的基线性能，在生成式基准测试上比先前技术提升20-30个百分点。但生成式推理的恢复仍然有限，主要适用于低剪枝比例。

Conclusion: 层剪枝在生成式推理任务上存在根本性限制，恢复主要适用于低剪枝比例。研究为在受限后训练机制下何时有效应用深度减少提供了指导。

Abstract: Recent works have shown that layer pruning can compress large language models (LLMs) while retaining strong performance on classification benchmarks with little or no finetuning. However, existing pruning techniques often suffer severe degradation on generative reasoning tasks. Through a systematic study across multiple model families, we find that tasks requiring multi-step reasoning are particularly sensitive to depth reduction. Beyond surface-level text degeneration, we observe degradation of critical algorithmic capabilities, including arithmetic computation for mathematical reasoning and balanced parenthesis generation for code synthesis. Under realistic post-training constraints, without access to pretraining-scale data or compute, we evaluate a simple mitigation strategy based on supervised finetuning with Self-Generated Responses. This approach achieves strong recovery on classification tasks, retaining up to 90\% of baseline performance, and yields substantial gains of up to 20--30 percentage points on generative benchmarks compared to prior post-pruning techniques. Crucially, despite these gains, recovery for generative reasoning remains fundamentally limited relative to classification tasks and is viable primarily at lower pruning ratios. Overall, we characterize the practical limits of layer pruning for generative reasoning and provide guidance on when depth reduction can be applied effectively under constrained post-training regimes.

</details>


### [333] [Preserve-Then-Quantize: Balancing Rank Budgets for Quantization Error Reconstruction in LLMs](https://arxiv.org/abs/2602.02001)
*Yoonjun Cho,Dongjae Jeon,Soeun Kim,Moongyu Jeon,Albert No*

Main category: cs.LG

TL;DR: SRR提出结构化残差重建框架，在PTQ中通过保留权重的主奇异子空间，仅量化残差部分，并用剩余秩进行误差重建，显著提升量化性能。


<details>
  <summary>Details</summary>
Motivation: 现有QER方法将所有秩预算用于量化误差重建，但当权重具有内在低秩结构且量化破坏了主方向时，这种方法不是最优的。需要一种能同时保留权重重要结构和重建量化误差的方法。

Method: 提出结构化残差重建(SRR)：1) 保留激活缩放权重的前k个奇异子空间；2) 仅量化残差部分；3) 使用剩余秩r-k进行误差重建。理论指导选择k值，平衡量化暴露能量和秩约束下的不可恢复误差。该参数化还支持量化参数高效微调(QPEFT)，并通过梯度缩放稳定微调。

Result: 实验表明，在多种模型和量化设置下，SRR在PTQ中持续降低困惑度，在2位QPEFT下在GLUE基准上平均提升5.9个百分点。

Conclusion: SRR通过结构化秩分配框架，在保留权重重要结构的同时有效重建量化误差，显著提升后训练量化和量化参数高效微调的性能。

Abstract: Quantization Error Reconstruction (QER) reduces accuracy loss in Post-Training Quantization (PTQ) by approximating weights as $\mathbf{W} \approx \mathbf{Q} + \mathbf{L}\mathbf{R}$, using a rank-$r$ correction to reconstruct quantization error. Prior methods devote the full rank budget to error reconstruction, which is suboptimal when $\mathbf{W}$ has intrinsic low-rank structure and quantization corrupts dominant directions. We propose Structured Residual Reconstruction (SRR), a rank-allocation framework that preserves the top-$k$ singular subspace of the activation-scaled weight before quantization, quantizes only the residual, and uses the remaining rank $r-k$ for error reconstruction. We derive a theory-guided criterion for selecting $k$ by balancing quantization-exposed energy and unrecoverable error under rank constraints. We further show that resulting $\mathbf{Q} + \mathbf{L}\mathbf{R}$ parameterization naturally supports Quantized Parameter-Efficient Fine-Tuning (QPEFT), and stabilizes fine-tuning via gradient scaling along preserved directions. Experiments demonstrate consistent perplexity reductions across diverse models and quantization settings in PTQ, along with a 5.9 percentage-point average gain on GLUE under 2-bit QPEFT.

</details>


### [334] [Logic-Guided Vector Fields for Constrained Generative Modeling](https://arxiv.org/abs/2602.02009)
*Ali Baheri*

Main category: cs.LG

TL;DR: 提出LGVF框架，将符号逻辑约束注入流匹配生成模型，通过训练时逻辑损失和推理时调整机制，在约束生成任务中显著减少约束违反


<details>
  <summary>Details</summary>
Motivation: 神经符号系统结合符号逻辑的表达结构和神经学习的灵活性，但生成模型通常缺乏在生成时强制执行声明性约束的机制

Method: 提出逻辑引导向量场(LGVF)框架：1)训练时逻辑损失，惩罚连续流轨迹上的约束违反；2)推理时调整，使用约束梯度引导采样

Result: 在三个约束生成案例中，LGVF相比标准流匹配减少59-82%的约束违反，在线性和环形设置中提高分布保真度，在多障碍设置中观察到满足度-保真度权衡

Conclusion: LGVF有效将符号知识注入生成模型，产生具有障碍规避行为的约束感知向量场，无需显式路径规划即可绕过禁止区域

Abstract: Neuro-symbolic systems aim to combine the expressive structure of symbolic logic with the flexibility of neural learning; yet, generative models typically lack mechanisms to enforce declarative constraints at generation time. We propose Logic-Guided Vector Fields (LGVF), a neuro-symbolic framework that injects symbolic knowledge, specified as differentiable relaxations of logical constraints, into flow matching generative models. LGVF couples two complementary mechanisms: (1) a training-time logic loss that penalizes constraint violations along continuous flow trajectories, with weights that emphasize correctness near the target distribution; and (2) an inference-time adjustment that steers sampling using constraint gradients, acting as a lightweight, logic-informed correction to the learned dynamics. We evaluate LGVF on three constrained generation case studies spanning linear, nonlinear, and multi-region feasibility constraints. Across all settings, LGVF reduces constraint violations by 59-82% compared to standard flow matching and achieves the lowest violation rates in each case. In the linear and ring settings, LGVF also improves distributional fidelity as measured by MMD, while in the multi-obstacle setting, we observe a satisfaction-fidelity trade-off, with improved feasibility but increased MMD. Beyond quantitative gains, LGVF yields constraint-aware vector fields exhibiting emergent obstacle-avoidance behavior, routing samples around forbidden regions without explicit path planning.

</details>


### [335] [SNAP: A Self-Consistent Agreement Principle with Application to Robust Computation](https://arxiv.org/abs/2602.02013)
*Xiaoyi Jiang,Andreas Nienkötter*

Main category: cs.LG

TL;DR: SNAP是一个基于相互一致性的自监督鲁棒计算框架，通过一致性-可靠性假设为数据项分配权重，强调可信项并抑制异常值，无需监督或先验知识。


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒计算方法通常需要迭代优化或先验知识，SNAP旨在提供一个灵活、易用、广泛适用的自监督框架，通过数据项之间的相互一致性来识别和抑制异常值。

Method: SNAP基于一致性-可靠性假设，通过计算数据项之间的相互一致性来分配权重。关键特性是异常值权重的指数抑制，确保异常值对计算贡献可忽略不计，即使在多维设置中。该方法是非迭代的。

Result: 在向量平均和子空间估计任务中，非迭代的SNAP表现优于迭代的Weiszfeld算法和两种多元中位数均值变体。异常值权重被指数级抑制，确保鲁棒性。

Conclusion: SNAP提供了一个灵活、易用、广泛适用的鲁棒计算框架，通过自监督的一致性原则有效抑制异常值，在多种任务中优于现有方法。

Abstract: We introduce SNAP (Self-coNsistent Agreement Principle), a self-supervised framework for robust computation based on mutual agreement. Based on an Agreement-Reliability Hypothesis SNAP assigns weights that quantify agreement, emphasizing trustworthy items and downweighting outliers without supervision or prior knowledge. A key result is the Exponential Suppression of Outlier Weights, ensuring that outliers contribute negligibly to computations, even in high-dimensional settings. We study properties of SNAP weighting scheme and show its practical benefits on vector averaging and subspace estimation. Particularly, we demonstrate that non-iterative SNAP outperforms the iterative Weiszfeld algorithm and two variants of multivariate median of means. SNAP thus provides a flexible, easy-to-use, broadly applicable approach to robust computation.

</details>


### [336] [Robust Domain Generalization under Divergent Marginal and Conditional Distributions](https://arxiv.org/abs/2602.02015)
*Jewon Yeom,Kyubyung Chae,Hyunggyu Lim,Yoonna Oh,Dongyoon Yang,Taesup Kim*

Main category: cs.LG

TL;DR: 提出一个统一框架用于处理域泛化中的复合分布偏移，同时考虑边际标签分布P(Y)和条件分布P(X|Y)的变化，通过元学习最小化风险边界，在传统DG基准和多域长尾识别任务中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有域泛化方法主要关注条件分布偏移(P(X|Y)变化)，假设P(Y)稳定，但现实多域场景中常同时存在边际标签分布P(Y)和条件分布P(X|Y)的复合偏移，需要更全面的解决方案。

Method: 提出统一框架，通过显式分解联合分布为边际和条件分量，推导未见域的风险边界。设计元学习过程，在可见域上最小化和验证该风险边界，确保对未见域的强泛化能力。

Result: 方法在传统域泛化基准和具有显著边际与条件偏移的多域长尾识别设置中均达到最先进的性能表现。

Conclusion: 通过同时处理边际和条件分布偏移，提出的统一框架能够有效应对现实世界中的复合分布变化，在域泛化任务中展现出优越的泛化能力。

Abstract: Domain generalization (DG) aims to learn predictive models that can generalize to unseen domains. Most existing DG approaches focus on learning domain-invariant representations under the assumption of conditional distribution shift (i.e., primarily addressing changes in $P(X\mid Y)$ while assuming $P(Y)$ remains stable). However, real-world scenarios with multiple domains often involve compound distribution shifts where both the marginal label distribution $P(Y)$ and the conditional distribution $P(X\mid Y)$ vary simultaneously. To address this, we propose a unified framework for robust domain generalization under divergent marginal and conditional distributions. We derive a novel risk bound for unseen domains by explicitly decomposing the joint distribution into marginal and conditional components and characterizing risk gaps arising from both sources of divergence. To operationalize this bound, we design a meta-learning procedure that minimizes and validates the proposed risk bound across seen domains, ensuring strong generalization to unseen ones. Empirical evaluations demonstrate that our method achieves state-of-the-art performance not only on conventional DG benchmarks but also in challenging multi-domain long-tailed recognition settings where both marginal and conditional shifts are pronounced.

</details>


### [337] [DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers](https://arxiv.org/abs/2602.02016)
*Ionut-Vlad Modoranu,Philip Zmushko,Erik Schultheis,Mher Safaryan,Dan Alistarh*

Main category: cs.LG

TL;DR: DASH提出了一种更快的分布式Shampoo优化器实现，通过3D张量堆叠和新的矩阵逆根计算方法，实现了4.83倍加速，同时保持更好的收敛性能。


<details>
  <summary>Details</summary>
Motivation: Shampoo作为领先的近似二阶优化器，在MLCommons竞赛中表现出色且能产生更易压缩的模型，但其计算成本高昂，限制了实际应用。

Method: 提出DASH方法：1) 将预处理器块堆叠为3D张量以提高GPU利用率；2) 引入Newton-DB迭代和切比雪夫多项式近似来加速矩阵逆根计算；3) 深入分析矩阵缩放对收敛的影响。

Result: GPU实现相比优化后的分布式Shampoo快4.83倍；Newton-DB在所有测试方法中达到每迭代最低验证困惑度；代码已开源。

Conclusion: DASH显著解决了Shampoo的计算瓶颈问题，通过算法创新和GPU优化实现了高效的第二阶优化，为大规模深度学习训练提供了实用的二阶优化解决方案。

Abstract: Shampoo is one of the leading approximate second-order optimizers: a variant of it has won the MLCommons AlgoPerf competition, and it has been shown to produce models with lower activation outliers that are easier to compress. Yet, applying Shampoo currently comes at the cost of significant computational slowdown, due to its expensive internal operations. In this paper, we take a significant step to address this shortcoming by proposing \method (for \textbf{D}istributed \textbf{A}ccelerated \textbf{SH}ampoo), a faster implementation of Distributed Shampoo based on two main new techniques: First, we show that preconditioner blocks can be stacked into 3D tensors to significantly improve GPU utilization; second, we introduce the Newton-DB iteration and the Chebyshev polynomial approximations as novel and faster approaches for computing the inverse matrix roots required by Shampoo. Along with these algorithmic contributions, we provide a first in-depth analysis of how matrix scaling critically affects Shampoo convergence. On the practical side, our GPU-aware implementation achieves up to $4.83\times$ faster optimizer steps compared to the well-optimized Distributed Shampoo, while Newton-DB attains the lowest validation perplexity per iteration among all tested methods. Our code is available at https://github.com/IST-DASLab/DASH.

</details>


### [338] [On Stability and Robustness of Diffusion Posterior Sampling for Bayesian Inverse Problems](https://arxiv.org/abs/2602.02045)
*Yiming Yang,Xiaoyuan Cheng,Yi He,Kaiyu Li,Wenxuan Yuan,Zhuo Sun*

Main category: cs.LG

TL;DR: 本文分析了扩散模型在贝叶斯逆问题中的稳定性与鲁棒性问题，提出了鲁棒扩散后验采样方法来解决似然函数不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型已成为贝叶斯逆问题的强大先验，但现有扩散求解器依赖于假定的观测似然函数，而似然函数与恢复质量之间的关系不明确。同时，扩散求解器缺乏鲁棒性，当假定的似然函数与真实数据生成过程不匹配时性能会下降。

Method: 本文首先理论分析了扩散求解器的稳定性，然后提出了鲁棒扩散后验采样方法，该方法与现有的基于梯度的后验采样器兼容，能够处理似然函数不匹配的情况。

Result: 理论分析表明扩散求解器具有稳定性但缺乏鲁棒性。提出的鲁棒扩散后验采样方法在科学逆问题和自然图像任务中表现出有效性，在具有挑战性的似然函数不匹配情况下实现了性能提升。

Conclusion: 本文填补了扩散模型在贝叶斯逆问题中理论分析的空白，证明了扩散求解器的稳定性但揭示了其鲁棒性问题，提出的鲁棒扩散后验采样方法为解决似然函数不匹配问题提供了有效解决方案。

Abstract: Diffusion models have recently emerged as powerful learned priors for Bayesian inverse problems (BIPs). Diffusion-based solvers rely on a presumed likelihood for the observations in BIPs to guide the generation process. However, the link between likelihood and recovery quality for BIPs is unclear in previous works. We bridge this gap by characterizing the posterior approximation error and proving the \emph{stability} of the diffusion-based solvers. Meanwhile, an immediate result of our findings on stability demonstrates the lack of robustness in diffusion-based solvers, which remains unexplored. This can degrade performance when the presumed likelihood mismatches the unknown true data generation processes. To address this issue, we propose a simple yet effective solution, \emph{robust diffusion posterior sampling}, which is provably \emph{robust} and compatible with existing gradient-based posterior samplers. Empirical results on scientific inverse problems and natural image tasks validate the effectiveness and robustness of our method, showing consistent performance improvements under challenging likelihood misspecifications.

</details>


### [339] [Dissecting Outlier Dynamics in LLM NVFP4 Pretraining](https://arxiv.org/abs/2602.02047)
*Peijie Dong,Ruibo Fan,Yuechen Tao,Di Mou,Wenhu Hu,Zhenheng Tang,Yinghao Yu,Jiamang Wang,Wenbo Su,Guodong Yang,Liping Zhang,Xiaowen Chu,Baochun Li,Bo Li*

Main category: cs.LG

TL;DR: 本文研究了4位浮点（NVFP4）训练中的异常值问题，发现线性注意力能减少异常值但仍有块级尖峰，提出热通道补丁（HCP）机制和CHON训练方案，显著缩小了与BF16的精度差距。


<details>
  <summary>Details</summary>
Motivation: 使用4位算术训练大语言模型能提高吞吐量和内存效率，但FP4的动态范围有限导致对异常值敏感。虽然NVFP4通过分层微缩放减轻量化误差，但与BF16相比仍存在精度差距。需要深入分析异常值在训练过程中的动态特性。

Method: 1) 纵向分析NVFP4预训练中异常值的动态特性，包括定位、原因和演化；2) 发现异常值主要来自Softmax Attention的Softmax、Linear Attention的门控和FFN的SwiGLU；3) 提出热通道补丁（HCP）在线补偿机制，识别热通道并用硬件高效核重新注入残差；4) 开发CHON训练方案，结合HCP和后QK操作保护。

Result: 在GLA-1.3B模型上训练600亿token，CHON将NVFP4与BF16的损失差距从0.94%降低到0.58%，同时保持下游任务准确率。异常值从训练早期的瞬态尖峰演变为后期的持久热通道。

Conclusion: 通过深入分析NVFP4训练中的异常值动态特性，提出的HCP机制和CHON方案有效缓解了4位训练中的精度损失问题，为高效低精度训练提供了实用解决方案。

Abstract: Training large language models using 4-bit arithmetic enhances throughput and memory efficiency. Yet, the limited dynamic range of FP4 increases sensitivity to outliers. While NVFP4 mitigates quantization error via hierarchical microscaling, a persistent loss gap remains compared to BF16. This study conducts a longitudinal analysis of outlier dynamics across architecture during NVFP4 pretraining, focusing on where they localize, why they occur, and how they evolve temporally. We find that, compared with Softmax Attention (SA), Linear Attention (LA) reduces per-tensor heavy tails but still exhibits persistent block-level spikes under block quantization. Our analysis attributes outliers to specific architectural components: Softmax in SA, gating in LA, and SwiGLU in FFN, with "post-QK" operations exhibiting higher sensitivity to quantization. Notably, outliers evolve from transient spikes early in training to a small set of persistent hot channels (i.e., channels with persistently large magnitudes) in later stages. Based on these findings, we introduce Hot-Channel Patch (HCP), an online compensation mechanism that identifies hot channels and reinjects residuals using hardware-efficient kernels. We then develop CHON, an NVFP4 training recipe integrating HCP with post-QK operation protection. On GLA-1.3B model trained for 60B tokens, CHON reduces the loss gap to BF16 from 0.94% to 0.58% while maintaining downstream accuracy.

</details>


### [340] [FORLER: Federated Offline Reinforcement Learning with Q-Ensemble and Actor Rectification](https://arxiv.org/abs/2602.02055)
*Nan Qiao,Sheng Yue*

Main category: cs.LG

TL;DR: FORLER：一种结合服务器端Q-ensemble聚合和设备端actor rectification的离线联邦强化学习方法，有效解决异构低质量数据下的策略污染问题


<details>
  <summary>Details</summary>
Motivation: 物联网系统中，在线联邦强化学习存在风险和高成本，离线联邦强化学习虽然避免了这些问题，但在低质量异构数据下容易陷入局部最优，导致策略污染问题

Method: 1) 服务器端：使用Q-ensemble聚合技术稳健合并设备Q函数，减轻策略污染，将计算负担从资源受限设备转移到服务器；2) 设备端：采用actor rectification，通过零阶搜索寻找高Q值动作，并设计专用正则化器将策略推向这些动作；3) 采用δ-周期性策略进一步减少本地计算

Result: 理论分析提供了安全策略改进的性能保证，大量实验表明FORLER在不同数据质量和异构性条件下始终优于强基线方法

Conclusion: FORLER通过创新的服务器-设备协同设计，有效解决了离线联邦强化学习中的策略污染问题，在保持隐私的同时实现了高效稳健的分布式强化学习

Abstract: In Internet-of-Things systems, federated learning has advanced online reinforcement learning (RL) by enabling parallel policy training without sharing raw data. However, interacting with real environments online can be risky and costly, motivating offline federated RL (FRL), where local devices learn from fixed datasets. Despite its promise, offline FRL may break down under low-quality, heterogeneous data. Offline RL tends to get stuck in local optima, and in FRL, one device's suboptimal policy can degrade the aggregated model, i.e., policy pollution. We present FORLER, combining Q-ensemble aggregation on the server with actor rectification on devices. The server robustly merges device Q-functions to curb policy pollution and shift heavy computation off resource-constrained hardware without compromising privacy. Locally, actor rectification enriches policy gradients via a zeroth-order search for high-Q actions plus a bespoke regularizer that nudges the policy toward them. A $δ$-periodic strategy further reduces local computation. We theoretically provide safe policy improvement performance guarantees. Extensive experiments show FORLER consistently outperforms strong baselines under varying data quality and heterogeneity.

</details>


### [341] [FiLoRA: Focus-and-Ignore LoRA for Controllable Feature Reliance](https://arxiv.org/abs/2602.02060)
*Hyunsuk Chung,Caren Han,Yerin Choi,Seungyeon Ji,Jinwoo Kim,Eun-Jung Holden,Kyungreem Han*

Main category: cs.LG

TL;DR: FiLoRA是一个指令条件化的参数高效适配框架，通过分解特征组对齐的LoRA模块和指令条件化门控，实现对内部特征依赖的显式控制，而不改变预测目标或任务语义。


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型整合了跨模态的异构信号，但人们对其预测如何依赖特定内部特征组以及这种依赖是否可以被有意控制的理解仍然不足。现有关于捷径和伪相关行为的研究主要依赖事后分析或特征移除，无法深入探究在不改变任务语义的情况下是否能够调节特征依赖。

Method: 提出FiLoRA（Focus-and-Ignore LoRA）框架：1）将适配分解为特征组对齐的LoRA模块；2）应用指令条件化门控，使自然语言指令作为计算层面的控制信号而非任务重定义；3）保持预测目标和标签空间不变。

Result: 在文本-图像和音频-视觉基准测试中，指令条件化门控能够诱导内部计算的一致性和因果性转移，选择性地放大或抑制核心和伪相关特征组。进一步分析显示FiLoRA在伪相关特征干预下提高了鲁棒性，揭示了超越相关性驱动学习的依赖调节机制。

Conclusion: FiLoRA提供了一个原则性机制来调节多模态基础模型对内部特征的依赖，实现了在不改变任务语义的情况下对计算层面的显式控制，为理解和管理模型的特征依赖行为提供了新途径。

Abstract: Multimodal foundation models integrate heterogeneous signals across modalities, yet it remains poorly understood how their predictions depend on specific internal feature groups and whether such reliance can be deliberately controlled. Existing studies of shortcut and spurious behavior largely rely on post hoc analyses or feature removal, offering limited insight into whether reliance can be modulated without altering task semantics. We introduce FiLoRA (Focus-and-Ignore LoRA), an instruction-conditioned, parameter-efficient adaptation framework that enables explicit control over internal feature reliance while keeping the predictive objective fixed. FiLoRA decomposes adaptation into feature group-aligned LoRA modules and applies instruction-conditioned gating, allowing natural language instructions to act as computation-level control signals rather than task redefinitions. Across text--image and audio--visual benchmarks, we show that instruction-conditioned gating induces consistent and causal shifts in internal computation, selectively amplifying or suppressing core and spurious feature groups without modifying the label space or training objective. Further analyses demonstrate that FiLoRA yields improved robustness under spurious feature interventions, revealing a principled mechanism to regulate reliance beyond correlation-driven learning.

</details>


### [342] [Learning to Route and Schedule LLMs from User Retrials via Contextual Queueing Bandits](https://arxiv.org/abs/2602.02061)
*Seoungbin Bae,Junyoung Son,Dabeen Lee*

Main category: cs.LG

TL;DR: 论文提出ACQB算法，通过上下文排队老虎机与多项Logit反馈框架，利用用户重试行为的隐式反馈来优化LLM服务的路由和调度，实现高效学习和队列稳定。


<details>
  <summary>Details</summary>
Motivation: 当前LLM服务面临两个关键挑战：1) 用户查询未满足时会重试，增加服务器积压；2) 显式反馈请求会降低用户体验。现有在线算法忽视了这些挑战，需要利用隐式反馈来优化路由和调度。

Method: 提出CQB-MNL框架，建模查询重试和基于上下文的用户对LLM偏好学习。开发ACQB算法，结合Thompson采样和衰减率的强制探索，实现高效学习同时保持队列稳定。

Result: ACQB算法在路由方面实现累计遗憾$\widetilde{\mathcal{O}}(\sqrt{t})$，在队列长度方面实现遗憾$\widetilde{\mathcal{O}}(t^{-1/4})$。实验在SPROUT、EmbedLLM和RouterBench数据集上验证了算法优于基线方法。

Conclusion: 通过利用用户重试行为的隐式反馈，ACQB算法能有效解决LLM服务中的路由和调度问题，在保持队列稳定的同时实现高效学习，为实际部署提供了有效解决方案。

Abstract: Explosive demands for LLMs often cause user queries to accumulate in server queues, requiring efficient routing (query-LLM matching) and scheduling (query prioritization) mechanisms. Several online algorithms are being deployed, but they overlook the following two key challenges inherent to conversational LLM services: (1) unsatisfied users may retry queries, increasing the server backlog, and (2) requests for ``explicit" feedback, such as ratings, degrade user experiences. In this paper, we develop a joint routing and scheduling algorithm that leverages ``implicit" feedback inferred from user retrial behaviors. The key idea is to propose and study the framework of contextual queueing bandits with multinomial logit feedback (CQB-MNL). CQB-MNL models query retrials, as well as context-based learning for user preferences over LLMs. Our algorithm, anytime CQB (ACQB), achieves efficient learning while maintaining queue stability by combining Thompson sampling with forced exploration at a decaying rate. We show that ACQB simultaneously achieves a cumulative regret of $\widetilde{\mathcal{O}}(\sqrt{t})$ for routing and a queue length regret of $\widetilde{\mathcal{O}}(t^{-1/4})$ for any large $t$. For experiments, we refine query embeddings via contrastive learning while adopting a disjoint parameter model to learn LLM-specific parameters. Experiments on SPROUT, EmbedLLM, and RouterBench datasets confirm that both algorithms consistently outperform baselines.

</details>


### [343] [BAPS: A Fine-Grained Low-Precision Scheme for Softmax in Attention via Block-Aware Precision reScaling](https://arxiv.org/abs/2602.02071)
*Zisheng Ye,Xiaoyu He,Maoyuan Song,Guoliang Qiu,Chao Liao,Chen Wu,Yonggang Sun,Zhichun Li,Xiaoru Xie,Yuanyong Luo,Hu Liu,Pinyan Lu,Heng Liao*

Main category: cs.LG

TL;DR: 提出一种使用8位浮点格式（HiF8）和块感知精度重缩放的软硬件协同设计方法，解决Transformer推理中softmax成为瓶颈的问题，可在不增加芯片面积的情况下将端到端推理吞吐量翻倍。


<details>
  <summary>Details</summary>
Motivation: 随着量化矩阵乘法加速的性能提升趋于平稳，softmax操作成为Transformer推理的关键瓶颈。这源于两个硬件限制：1）矩阵和向量计算核心之间的数据带宽有限；2）高精度（FP32/16）指数运算单元（EXP2）的面积成本过高。

Method: 引入新颖的低精度工作流，采用特定的8位浮点格式（HiF8）和块感知精度重缩放技术。该方法通过算法创新使低精度softmax可行，避免了直接低精度方法导致的显著模型精度损失。

Result: 设计将所需数据移动带宽减半（通过约束矩阵乘法输出为8位），并大幅减少EXP2单元面积（通过在低8位精度下计算指数）。在语言模型和多模态模型上的广泛评估证实了方法的有效性。

Conclusion: 通过缓解向量计算瓶颈，该工作为在不增加芯片面积的情况下将端到端推理吞吐量翻倍铺平了道路，并为未来的低精度硬件和软件协同设计提供了具体路径。

Abstract: As the performance gains from accelerating quantized matrix multiplication plateau, the softmax operation becomes the critical bottleneck in Transformer inference. This bottleneck stems from two hardware limitations: (1) limited data bandwidth between matrix and vector compute cores, and (2) the significant area cost of high-precision (FP32/16) exponentiation units (EXP2). To address these issues, we introduce a novel low-precision workflow that employs a specific 8-bit floating-point format (HiF8) and block-aware precision rescaling for softmax. Crucially, our algorithmic innovations make low-precision softmax feasible without the significant model accuracy loss that hampers direct low-precision approaches. Specifically, our design (i) halves the required data movement bandwidth by enabling matrix multiplication outputs constrained to 8-bit, and (ii) substantially reduces the EXP2 unit area by computing exponentiations in low (8-bit) precision. Extensive evaluation on language models and multi-modal models confirms the validity of our method. By alleviating the vector computation bottleneck, our work paves the way for doubling end-to-end inference throughput without increasing chip area, and offers a concrete co-design path for future low-precision hardware and software.

</details>


### [344] [Calibrating Adaptive Smoothing Methods for Freeway Traffic Reconstruction](https://arxiv.org/abs/2602.02072)
*Junyi Ji,Derek Gloudemans,Gergely Zachár,Matthew Nice,William Barbour,Daniel B. Work*

Main category: cs.LG

TL;DR: 本文提出了一个基于PyTorch的Python实现自适应平滑方法(ASM)，通过真实世界数据进行端到端校准，为交通状态重建提供基准指标。


<details>
  <summary>Details</summary>
Motivation: 自适应平滑方法(ASM)是广泛使用的交通状态重建方法，但缺乏可复现的基准实现。本文旨在提供一个可复现的Python实现，通过真实数据校准，为交通重建问题建立基准。

Method: 使用PyTorch实现ASM，将校准问题形式化为参数化核优化问题。利用稀疏雷达传感器网络数据，在完整状态观测测试平台上进行端到端校准。

Result: 通过速度分布、时空误差分布和空间误差等指标评估结果，为交通重建问题提供基准指标。校准后的方法在多个高速公路上展示出可用性。

Conclusion: 本文提供了一个可复现的ASM实现，可作为各种高速公路运营任务的基准。同时讨论了通用交通模型校准的可复现性挑战和ASM的局限性。

Abstract: The adaptive smoothing method (ASM) is a widely used approach for traffic state reconstruction. This article presents a Python implementation of ASM, featuring end-to-end calibration using real-world ground truth data. The calibration is formulated as a parameterized kernel optimization problem. The model is calibrated using data from a full-state observation testbed, with input from a sparse radar sensor network. The implementation is developed in PyTorch, enabling integration with various deep learning methods. We evaluate the results in terms of speed distribution, spatio-temporal error distribution, and spatial error to provide benchmark metrics for the traffic reconstruction problem. We further demonstrate the usability of the calibrated method across multiple freeways. Finally, we discuss the challenges of reproducibility in general traffic model calibration and the limitations of ASM. This article is reproducible and can serve as a benchmark for various freeway operation tasks.

</details>


### [345] [AICD Bench: A Challenging Benchmark for AI-Generated Code Detection](https://arxiv.org/abs/2602.02079)
*Daniil Orel,Dilshod Azizov,Indraneil Paul,Yuxia Wang,Iryna Gurevych,Preslav Nakov*

Main category: cs.LG

TL;DR: AICD Bench：最全面的AI生成代码检测基准，包含200万样本、77个模型、9种编程语言，提出三个现实检测任务，评估显示现有检测器性能远未达到实用水平。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成功能性源代码的能力增强，引发了关于作者身份、责任和安全的担忧。现有AI生成代码检测数据集和基准过于狭窄，通常仅限于分布内设置的二元人机分类，需要更全面的评估框架。

Method: 构建AICD Bench基准，包含200万个样本、77个模型（涵盖11个模型家族）、9种编程语言，包括最新的推理模型。提出三个现实检测任务：分布偏移下的鲁棒二元分类、模型家族溯源、细粒度人机分类（人类、机器、混合、对抗代码）。

Result: 对神经和经典检测器的广泛评估显示，性能远未达到实际可用水平，特别是在分布偏移下以及对混合或对抗代码的检测效果较差。

Conclusion: AICD Bench作为一个统一且具有挑战性的评估套件发布，旨在推动下一代鲁棒的AI生成代码检测方法的发展。

Abstract: Large language models (LLMs) are increasingly capable of generating functional source code, raising concerns about authorship, accountability, and security. While detecting AI-generated code is critical, existing datasets and benchmarks are narrow, typically limited to binary human-machine classification under in-distribution settings. To bridge this gap, we introduce $\emph{AICD Bench}$, the most comprehensive benchmark for AI-generated code detection. It spans $\emph{2M examples}$, $\emph{77 models}$ across $\emph{11 families}$, and $\emph{9 programming languages}$, including recent reasoning models. Beyond scale, AICD Bench introduces three realistic detection tasks: ($\emph{i}$)~$\emph{Robust Binary Classification}$ under distribution shifts in language and domain, ($\emph{ii}$)~$\emph{Model Family Attribution}$, grouping generators by architectural lineage, and ($\emph{iii}$)~$\emph{Fine-Grained Human-Machine Classification}$ across human, machine, hybrid, and adversarial code. Extensive evaluation on neural and classical detectors shows that performance remains far below practical usability, particularly under distribution shift and for hybrid or adversarial code. We release AICD Bench as a $\emph{unified, challenging evaluation suite}$ to drive the next generation of robust approaches for AI-generated code detection. The data and the code are available at https://huggingface.co/AICD-bench}.

</details>


### [346] [Learning Half-Spaces from Perturbed Contrastive Examples](https://arxiv.org/abs/2602.02080)
*Aryan Alavi Razavi Ravari,Farnam Mansouri,Yuxin Chen,Valentio Iverson,Adish Singla,Sandra Zilles*

Main category: cs.LG

TL;DR: 研究在带噪声的对比示例oracle下的学习问题，其中对比示例的扰动程度由查询点到决策边界的距离决定，距离越近扰动越小。分析了一维阈值和半空间在均匀分布下的主动和被动对比样本复杂度。


<details>
  <summary>Details</summary>
Motivation: Mansouri等人提出了理想的对比示例oracle，其中对比示例总是距离查询点最近的反例。但在实际应用中，对比示例可能存在噪声。本文旨在研究当对比示例被扰动时的学习问题，扰动程度由查询点到决策边界的距离控制，这更符合现实情况。

Method: 引入参数化噪声函数f的机制，对比示例的扰动幅度由f(d)控制，其中d是查询点到决策边界的距离。研究两种设置：(i)最大扰动幅度固定，(ii)扰动是随机的。分析一维阈值和半空间在均匀分布下的主动和被动对比样本复杂度。

Result: 在特定条件下，对比示例的存在可以加速学习过程，降低渐近查询复杂度和期望查询复杂度。通过噪声函数f的参数化机制，能够量化对比示例质量对学习效率的影响。

Conclusion: 本文扩展了对比学习理论框架，引入了更现实的噪声模型，证明在适当条件下对比示例能够提高学习效率，为实际对比学习应用提供了理论指导。

Abstract: We study learning under a two-step contrastive example oracle, as introduced by Mansouri et. al. (2025), where each queried (or sampled) labeled example is paired with an additional contrastive example of opposite label. While Mansouri et al. assume an idealized setting, where the contrastive example is at minimum distance of the originally queried/sampled point, we introduce and analyze a mechanism, parameterized by a non-decreasing noise function $f$, under which this ideal contrastive example is perturbed. The amount of perturbation is controlled by $f(d)$, where $d$ is the distance of the queried/sampled point to the decision boundary. Intuitively, this results in higher-quality contrastive examples for points closer to the decision boundary. We study this model in two settings: (i) when the maximum perturbation magnitude is fixed, and (ii) when it is stochastic.
  For one-dimensional thresholds and for half-spaces under the uniform distribution on a bounded domain, we characterize active and passive contrastive sample complexity in dependence on the function $f$. We show that, under certain conditions on $f$, the presence of contrastive examples speeds up learning in terms of asymptotic query complexity and asymptotic expected query complexity.

</details>


### [347] [Active learning from positive and unlabeled examples](https://arxiv.org/abs/2602.02081)
*Farnam Mansouri,Sandra Zilles,Shai Ben-David*

Main category: cs.LG

TL;DR: 该论文首次对主动正无标记学习（Active PU Learning）的标签复杂度进行了理论分析，研究了一种特殊的主动学习场景：学习者只能自适应查询未标记池中的实例，且仅当实例为正例且独立硬币投掷成功时才能获得标签信息。


<details>
  <summary>Details</summary>
Motivation: 动机源于广告和异常检测等实际应用场景，在这些场景中，学习者只能获得部分正例的标签信息，而其他实例保持未标记状态。传统的主动学习假设可以获取任何查询实例的真实标签，但在实际应用中，标签信息可能只在特定条件下才可获取。

Method: 研究了一种主动PU学习设置，学习者可以自适应地从未标记池中查询实例，但只有当查询的实例是正例且一个独立的硬币投掷成功时，标签才会被揭示；否则学习者不会获得任何信息。论文对这种设置下的标签复杂度进行了理论分析。

Result: 论文提供了主动PU学习的首个标签复杂度理论分析，建立了在这种受限标签获取机制下的学习效率理论框架。

Conclusion: 该研究填补了主动PU学习理论分析的空白，为实际应用中标签获取受限的弱监督学习场景提供了理论基础，特别是在广告和异常检测等只能部分获取正例标签信息的领域具有重要应用价值。

Abstract: Learning from positive and unlabeled data (PU learning) is a weakly supervised variant of binary classification in which the learner receives labels only for (some) positively labeled instances, while all other examples remain unlabeled. Motivated by applications such as advertising and anomaly detection, we study an active PU learning setting where the learner can adaptively query instances from an unlabeled pool, but a queried label is revealed only when the instance is positive and an independent coin flip succeeds; otherwise the learner receives no information. In this paper, we provide the first theoretical analysis of the label complexity of active PU learning.

</details>


### [348] [Efficient Swap Regret Minimization in Combinatorial Bandits](https://arxiv.org/abs/2602.02087)
*Andreas Kontogiannis,Vasilis Pollatos,Panayotis Mertikopoulos,Ioannis Panageas*

Main category: cs.LG

TL;DR: 提出了一种针对组合赌博机的高效无交换遗憾算法，其遗憾在动作数N上呈多对数依赖，解决了该领域长期存在的问题。


<details>
  <summary>Details</summary>
Motivation: 在组合赌博机中，动作数量N相对于问题维度呈指数级增长。虽然外部遗憾最小化问题已有较好理解，但实现多对数依赖N的无交换遗憾一直是个难题，本文旨在解决这一挑战。

Method: 引入了一种无交换遗憾学习算法，其遗憾在N上呈多对数缩放，并且对组合赌博机类别是紧的。同时展示了如何在多种应用中高效实现该算法，每次迭代复杂度也在N上呈多对数。

Result: 成功设计了首个在组合赌博机中实现多对数依赖N的无交换遗憾算法，其遗憾界是紧的，并且算法实现具有高效的计算复杂度。

Conclusion: 本文解决了组合赌博机中长期存在的无交换遗憾算法设计难题，提供了理论紧的遗憾界和高效实现方法，为组合赌博机领域做出了重要贡献。

Abstract: This paper addresses the problem of designing efficient no-swap regret algorithms for combinatorial bandits, where the number of actions $N$ is exponentially large in the dimensionality of the problem. In this setting, designing efficient no-swap regret translates to sublinear -- in horizon $T$ -- swap regret with polylogarithmic dependence on $N$. In contrast to the weaker notion of external regret minimization - a problem which is fairly well understood in the literature - achieving no-swap regret with a polylogarithmic dependence on $N$ has remained elusive in combinatorial bandits. Our paper resolves this challenge, by introducing a no-swap-regret learning algorithm with regret that scales polylogarithmically in $N$ and is tight for the class of combinatorial bandits. To ground our results, we also demonstrate how to implement the proposed algorithm efficiently -- that is, with a per-iteration complexity that also scales polylogarithmically in $N$ -- across a wide range of well-studied applications.

</details>


### [349] [Probabilistic Performance Guarantees for Multi-Task Reinforcement Learning](https://arxiv.org/abs/2602.02098)
*Yannik Schnitzer,Mathias Jackermeier,Alessandro Abate,David Parker*

Main category: cs.LG

TL;DR: 提出一种为多任务强化学习策略在新任务上性能提供高置信度保证的方法，结合任务级泛化和任务内置信下界。


<details>
  <summary>Details</summary>
Motivation: 现有多任务强化学习方法缺乏形式化性能保证，这在安全关键场景部署中是不可或缺的。需要为训练中未见任务提供高置信度性能保证。

Method: 引入新的泛化边界，组合两个部分：(1) 从有限次rollout得到的每个任务的置信下界，(2) 从有限采样任务得到的任务级泛化，从而为从相同未知分布抽取的新任务提供高置信度保证。

Result: 在现有最先进的多任务RL方法上验证，证明该保证在理论上是可靠的，并且在现实样本量下具有信息量。

Conclusion: 该方法为多任务强化学习策略在新任务上的性能提供了形式化保证，填补了安全关键部署中的重要空白。

Abstract: Multi-task reinforcement learning trains generalist policies that can execute multiple tasks. While recent years have seen significant progress, existing approaches rarely provide formal performance guarantees, which are indispensable when deploying policies in safety-critical settings. We present an approach for computing high-confidence guarantees on the performance of a multi-task policy on tasks not seen during training. Concretely, we introduce a new generalisation bound that composes (i) per-task lower confidence bounds from finitely many rollouts with (ii) task-level generalisation from finitely many sampled tasks, yielding a high-confidence guarantee for new tasks drawn from the same arbitrary and unknown distribution. Across state-of-the-art multi-task RL methods, we show that the guarantees are theoretically sound and informative at realistic sample sizes.

</details>


### [350] [No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs](https://arxiv.org/abs/2602.02103)
*Liyan Xu,Mo Yu,Fandong Meng,Jie Zhou*

Main category: cs.LG

TL;DR: 本文通过Tele-Lens探测方法研究LLM的潜在规划能力，发现LLM具有短视视野，主要进行增量推理而非全局规划，并基于此提出改进CoT不确定性估计的方法。


<details>
  <summary>Details</summary>
Motivation: 先前研究发现LLM在CoT出现前已存在潜在规划，这削弱了显式CoT的重要性，但CoT对多步推理任务仍至关重要。为了深入理解LLM内部状态与其语言化推理轨迹之间的关系，需要研究LLM的潜在规划能力。

Method: 提出Tele-Lens探测方法，应用于不同任务领域的隐藏状态，分析LLM的潜在规划强度。基于发现的特征，提出改进CoT不确定性估计的假设，并验证少量CoT位置即可有效代表整个路径的不确定性。

Result: 实证结果表明LLM表现出短视视野，主要进行增量转换而非精确的全局规划。验证了假设：少量CoT位置可以有效代表整个路径的不确定性。进一步证明可以自动识别CoT绕过而不降低性能。

Conclusion: LLM的推理过程具有短视特性，主要依赖增量推理而非全局规划。利用CoT动态特性可以改进不确定性估计，并实现自动识别CoT绕过。这为理解LLM推理机制和优化推理过程提供了新见解。

Abstract: This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.

</details>


### [351] [An Empirical Study of World Model Quantization](https://arxiv.org/abs/2602.02110)
*Zhongqian Fu,Tianyi Zhao,Kai Han,Hang Zhou,Xinghao Chen,Yunhe Wang*

Main category: cs.LG

TL;DR: 系统研究世界模型量化，使用DINO-WM作为代表案例，评估多种后训练量化方法，发现世界模型量化具有独特的失败模式，为严格计算约束下的部署提供实用指导。


<details>
  <summary>Details</summary>
Motivation: 世界模型学习环境动态的内部表示，使智能体能够在紧凑的潜在空间中模拟和推理未来状态，但运行世界模型依赖大量计算成本和内存占用，使得模型量化对于高效部署至关重要。目前，后训练量化对世界模型的影响在很大程度上尚未被研究。

Method: 使用DINO-WM作为代表性案例，评估多种后训练量化方法，包括仅权重量化和联合权重-激活量化。在不同视觉规划任务上进行广泛实验，涵盖广泛的比特宽度、量化粒度和高达50次迭代的规划视野。

Result: 世界模型中的量化效应超出了标准的精度和比特宽度权衡：分组权重量化可以稳定低比特展开，激活量化粒度产生不一致的益处，编码器和预测器模块之间的量化敏感性高度不对称。此外，激进的低比特量化显著降低了规划目标与任务成功之间的对齐，导致无法通过额外优化修复的失败。

Conclusion: 这些发现揭示了基于世界模型的规划中独特的量化诱导失败模式，并为在严格计算约束下部署量化世界模型提供了实用指导。代码将在指定GitHub仓库中提供。

Abstract: World models learn an internal representation of environment dynamics, enabling agents to simulate and reason about future states within a compact latent space for tasks such as planning, prediction, and inference. However, running world models rely on hevay computational cost and memory footprint, making model quantization essential for efficient deployment. To date, the effects of post-training quantization (PTQ) on world models remain largely unexamined. In this work, we present a systematic empirical study of world model quantization using DINO-WM as a representative case, evaluating diverse PTQ methods under both weight-only and joint weight-activation settings. We conduct extensive experiments on different visual planning tasks across a wide range of bit-widths, quantization granularities, and planning horizons up to 50 iterations. Our results show that quantization effects in world models extend beyond standard accuracy and bit-width trade-offs: group-wise weight quantization can stabilize low-bit rollouts, activation quantization granularity yields inconsistent benefits, and quantization sensitivity is highly asymmetric between encoder and predictor modules. Moreover, aggressive low-bit quantization significantly degrades the alignment between the planning objective and task success, leading to failures that cannot be remedied by additional optimization. These findings reveal distinct quantization-induced failure modes in world model-based planning and provide practical guidance for deploying quantized world models under strict computational constraints. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/QuantWM.

</details>


### [352] [Unifying Masked Diffusion Models with Various Generation Orders and Beyond](https://arxiv.org/abs/2602.02112)
*Chunsan Hong,Sanghyun Lee,Jong Chul Ye*

Main category: cs.LG

TL;DR: 本文提出OeMDM框架统一多种文本生成顺序，并在此基础上开发LoMDM模型，通过单一目标联合学习生成顺序和扩散主干，在多个语言建模基准上优于现有离散扩散模型。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型作为自回归模型的潜在替代方案，其生成质量严重依赖生成顺序。现有方法要么硬编码顺序，要么为预训练MDM学习顺序策略，导致额外成本且可能产生次优解。

Method: 提出OeMDM框架统一多种生成顺序，包括MDM、ARM和块扩散。在此基础上开发LoMDM，通过单一目标联合学习生成顺序和扩散主干，支持上下文相关的文本生成顺序。

Result: LoMDM在多个语言建模基准上优于各种离散扩散模型，验证了联合学习生成顺序和扩散模型的有效性。

Conclusion: 提出的OeMDM框架统一了多种文本生成顺序，LoMDM通过联合学习实现了上下文相关的生成顺序，在语言建模任务上表现出色，为掩码扩散模型提供了更灵活和有效的解决方案。

Abstract: Masked diffusion models (MDMs) are a potential alternative to autoregressive models (ARMs) for language generation, but generation quality depends critically on the generation order. Prior work either hard-codes an ordering (e.g., blockwise left-to-right) or learns an ordering policy for a pretrained MDM, which incurs extra cost and can yield suboptimal solutions due to the two-stage optimization. Motivated by this, we propose order-expressive masked diffusion model (OeMDM) for a broad class of diffusion generative processes with various generation orders, enabling the interpretation of MDM, ARM, and block diffusion in a single framework. Furthermore, building on OeMDM, we introduce learnable-order masked diffusion model (LoMDM), which jointly learns the generation ordering and diffusion backbone through a single objective from scratch, enabling the diffusion model to generate text in context-dependent ordering. Empirically, we confirm that LoMDM outperforms various discrete diffusion models across multiple language modeling benchmarks.

</details>


### [353] [The Maximum von Neumann Entropy Principle: Theory and Applications in Machine Learning](https://arxiv.org/abs/2602.02117)
*Youqi Wu,Farzan Farnia*

Main category: cs.LG

TL;DR: 将最大熵原理的极小极大公式扩展到冯·诺依曼熵，为核学习中的VNE最大化提供博弈论和信息论基础


<details>
  <summary>Details</summary>
Motivation: 冯·诺依曼熵在机器学习中作为核矩阵的谱多样性度量被采用，但缺乏类似经典最大熵框架的决策论和博弈论解释

Method: 将Grünwald和Dawid的最大熵原理极小极大公式扩展到冯·诺依曼熵设置，为密度矩阵和迹归一化半正定算子的VNE最大化提供博弈论证明

Result: 建立了最大VNE原理，提供了在部分信息下最大VNE解的稳健解释，阐明了其在谱域中作为最小承诺推断的作用

Conclusion: 提出的框架为核学习中的VNE方法提供了统一的信息论基础，并通过两个应用示例展示了其实际价值

Abstract: Von Neumann entropy (VNE) is a fundamental quantity in quantum information theory and has recently been adopted in machine learning as a spectral measure of diversity for kernel matrices and kernel covariance operators. While maximizing VNE under constraints is well known in quantum settings, a principled analogue of the classical maximum entropy framework, particularly its decision theoretic and game theoretic interpretation, has not been explicitly developed for VNE in data driven contexts. In this paper, we extend the minimax formulation of the maximum entropy principle due to Grünwald and Dawid to the setting of von Neumann entropy, providing a game-theoretic justification for VNE maximization over density matrices and trace-normalized positive semidefinite operators. This perspective yields a robust interpretation of maximum VNE solutions under partial information and clarifies their role as least committed inferences in spectral domains. We then illustrate how the resulting Maximum VNE principle applies to modern machine learning problems by considering two representative applications, selecting a kernel representation from multiple normalized embeddings via kernel-based VNE maximization, and completing kernel matrices from partially observed entries. These examples demonstrate how the proposed framework offers a unifying information-theoretic foundation for VNE-based methods in kernel learning.

</details>


### [354] [Two-Stage Grid Optimization for Group-wise Quantization of LLMs](https://arxiv.org/abs/2602.02126)
*Junhan Kim,Gukryeol Lee,Seungwoo Son,Jeewook Kim,Yongkweon Jeon*

Main category: cs.LG

TL;DR: 提出一种两阶段优化框架，通过最小化层重建损失来改进GPTQ分组量化，显著提升大语言模型低比特量化精度


<details>
  <summary>Details</summary>
Motivation: GPTQ等现有分组量化方法在确定分组尺度时忽略了输入统计特性和组间相关性，导致与最小化层重建损失的目标不匹配，造成精度下降

Method: 提出两阶段优化框架：第一阶段在GPTQ之前初始化每个分组尺度以最小化分组重建损失；第二阶段冻结GPTQ得到的整数权重，使用坐标下降算法和闭式更新规则优化分组尺度以最小化层重建损失，并考虑前层量化误差防止误差累积

Result: 实验结果表明该方法能持续提升分组量化性能，以可忽略的开销获得更高的精度

Conclusion: 提出的两阶段优化框架有效解决了GPTQ忽略输入统计和组间相关性的问题，通过最小化层重建损失显著提升了分组量化的精度

Abstract: Group-wise quantization is an effective strategy for mitigating accuracy degradation in low-bit quantization of large language models (LLMs). Among existing methods, GPTQ has been widely adopted due to its efficiency; however, it neglects input statistics and inter-group correlations when determining group scales, leading to a mismatch with its goal of minimizing layer-wise reconstruction loss. In this work, we propose a two-stage optimization framework for group scales that explicitly minimizes the layer-wise reconstruction loss. In the first stage, performed prior to GPTQ, we initialize each group scale to minimize the group-wise reconstruction loss, thereby incorporating input statistics. In the second stage, we freeze the integer weights obtained via GPTQ and refine the group scales to minimize the layer-wise reconstruction loss. To this end, we employ the coordinate descent algorithm and derive a closed-form update rule, which enables efficient refinement without costly numerical optimization. Notably, our derivation incorporates the quantization errors from preceding layers to prevent error accumulation. Experimental results demonstrate that our method consistently enhances group-wise quantization, achieving higher accuracy with negligible overhead.

</details>


### [355] [Scalable Spatio-Temporal SE(3) Diffusion for Long-Horizon Protein Dynamics](https://arxiv.org/abs/2602.02128)
*Nima Shoghi,Yuxuan Liu,Yuning Shen,Rob Brekelmans,Pan Li,Quanquan Gu*

Main category: cs.LG

TL;DR: STAR-MD是一种SE(3)-等变扩散模型，通过联合时空注意力机制生成微秒级蛋白质轨迹，在ATLAS基准测试中取得最先进性能，解决了现有生成模型在长时程模拟中的局限性。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟是研究蛋白质动力学的金标准，但计算成本限制了其在生物相关时间尺度上的应用。现有生成模型由于架构限制、误差累积和时空动力学建模不足，在长时程生成方面存在困难。

Method: 提出STAR-MD（时空自回归展开分子动力学），一种可扩展的SE(3)-等变扩散模型，采用具有联合时空注意力的因果扩散变换器，有效捕捉复杂的时空依赖关系，避免现有方法的内存瓶颈。

Result: 在标准ATLAS基准测试中，STAR-MD在所有指标上都达到了最先进的性能，显著改善了构象覆盖、结构有效性和动态保真度。能够成功外推生成稳定的微秒级轨迹，而基线方法则完全失败。

Conclusion: STAR-MD的联合时空建模能够在生物相关时间尺度上实现稳健的动力学模拟，为加速探索蛋白质功能开辟了新途径，同时揭示了当前模型在长时程生成方面的严重局限性。

Abstract: Molecular dynamics (MD) simulations remain the gold standard for studying protein dynamics, but their computational cost limits access to biologically relevant timescales. Recent generative models have shown promise in accelerating simulations, yet they struggle with long-horizon generation due to architectural constraints, error accumulation, and inadequate modeling of spatio-temporal dynamics. We present STAR-MD (Spatio-Temporal Autoregressive Rollout for Molecular Dynamics), a scalable SE(3)-equivariant diffusion model that generates physically plausible protein trajectories over microsecond timescales. Our key innovation is a causal diffusion transformer with joint spatio-temporal attention that efficiently captures complex space-time dependencies while avoiding the memory bottlenecks of existing methods. On the standard ATLAS benchmark, STAR-MD achieves state-of-the-art performance across all metrics--substantially improving conformational coverage, structural validity, and dynamic fidelity compared to previous methods. STAR-MD successfully extrapolates to generate stable microsecond-scale trajectories where baseline methods fail catastrophically, maintaining high structural quality throughout the extended rollout. Our comprehensive evaluation reveals severe limitations in current models for long-horizon generation, while demonstrating that STAR-MD's joint spatio-temporal modeling enables robust dynamics simulation at biologically relevant timescales, paving the way for accelerated exploration of protein function.

</details>


### [356] [DCoPilot: Generative AI-Empowered Policy Adaptation for Dynamic Data Center Operations](https://arxiv.org/abs/2602.02137)
*Minghao Li,Ruihang Wang,Rui Tan,Yonggang Wen*

Main category: cs.LG

TL;DR: DCoPilot：一个用于动态数据中心操作的混合生成控制策略框架，结合LLM符号化生成结构化奖励形式和超网络参数化生成策略权重，实现零样本策略生成


<details>
  <summary>Details</summary>
Motivation: 现代AI专用数据中心在高功率密度和快速变化的工作负载下运行，需要分钟级自适应以确保安全和能效。手动设计的分段DRL代理无法跟上数据中心频繁的动态变化和服务级别协议（SLA）变更，导致规范到策略的滞后，可能引发服务中断。

Method: DCoPilot结合两种生成范式：1）大型语言模型（LLM）执行结构化奖励形式的符号生成；2）超网络进行策略权重的参数生成。框架包含三个阶段：模拟扩展（在不同SimReady场景中压力测试奖励候选）、元策略蒸馏（训练超网络根据SLA和场景嵌入输出策略权重）、在线自适应（实现零样本策略生成）。

Result: 在涵盖不同数据中心组件的五个控制任务系列中，DCoPilot实现了接近零的约束违规，并在所有规范变化中优于所有基线方法。消融研究验证了基于LLM的统一奖励生成在实现稳定超网络收敛方面的有效性。

Conclusion: DCoPilot通过结合LLM的符号生成能力和超网络的参数生成能力，成功解决了动态数据中心操作中规范到策略的滞后问题，实现了及时有效的控制策略生成，确保了数据中心的安全和能效运行。

Abstract: Modern data centers (DCs) hosting artificial intelligence (AI)-dedicated devices operate at high power densities with rapidly varying workloads, making minute-level adaptation essential for safe and energy-efficient operation. However, manually designing piecewise deep reinforcement learning (DRL) agents cannot keep pace with frequent dynamics shifts and service-level agreement (SLA) changes of an evolving DC. This specification-to-policy lag causes a lack of timely, effective control policies, which may lead to service outages. To bridge the gap, we present DCoPilot, a hybrid framework for generative control policies in dynamic DC operation. DCoPilot synergizes two distinct generative paradigms, i.e., a large language model (LLM) that performs symbolic generation of structured reward forms, and a hypernetwork that conducts parametric generation of policy weights. DCoPilot operates through three coordinated phases: (i) simulation scale-up, which stress-tests reward candidates across diverse simulation-ready (SimReady) scenes; (ii) meta policy distillation, where a hypernetwork is trained to output policy weights conditioned on SLA and scene embeddings; and (iii) online adaptation, enabling zero-shot policy generation in response to updated specifications. Evaluated across five control task families spanning diverse DC components, DCoPilot achieves near-zero constraint violations and outperforms all baselines across specification variations. Ablation studies validate the effectiveness of LLM-based unified reward generation in enabling stable hypernetwork convergence.

</details>


### [357] [EvoMU: Evolutionary Machine Unlearning](https://arxiv.org/abs/2602.02139)
*Pawel Batorski,Paul Swoboda*

Main category: cs.LG

TL;DR: EvoMU使用进化搜索自动发现针对特定任务的无学习损失函数，在有限计算资源下实现SotA效果


<details>
  <summary>Details</summary>
Motivation: 机器无学习需要合适的损失函数，但现有损失函数空间庞大且没有通用最优解，不同数据集结构差异导致损失函数效果不一

Method: 采用进化搜索程序自动在庞大的无学习损失函数空间中寻找任务特定的损失函数，使用小型4B参数模型(Qwen3-4B-Thinking)实现

Result: 在TOFU-5%、TOFU-10%、MUSE和WMDP数据集上超越了先前基于损失的无学习方法，合成了新颖的无学习损失函数

Conclusion: EvoMU展示了有限计算资源下AI共同科学家的潜力，实现了自动科学发现，无需人工干预即可找到数据集特定的最优损失函数

Abstract: Machine unlearning aims to unlearn specified training data (e.g. sensitive or copyrighted material). A prominent approach is to fine-tune an existing model with an unlearning loss that retains overall utility. The space of suitable unlearning loss functions is vast, making the search for an optimal loss function daunting. Additionally, there might not even exist a universally optimal loss function: differences in the structure and overlap of the forget and retain data can cause a loss to work well in one setting but over-unlearn or under-unlearn in another. Our approach EvoMU tackles these two challenges simultaneously. An evolutionary search procedure automatically finds task-specific losses in the vast space of possible unlearning loss functions. This allows us to find dataset-specific losses that match or outperform existing losses from the literature, without the need for a human-in-the-loop. This work is therefore an instance of automatic scientific discovery, a.k.a. an AI co-scientist. In contrast to previous AI co-scientist works, we do so on a budget: We achieve SotA results using a small 4B parameter model (Qwen3-4B-Thinking), showing the potential of AI co-scientists with limited computational resources. Our experimental evaluation shows that we surpass previous loss-based unlearning formulations on TOFU-5%, TOFU-10%, MUSE and WMDP by synthesizing novel unlearning losses. Our code is available at https://github.com/Batorskq/EvoMU.

</details>


### [358] [Learning Generative Selection for Best-of-N](https://arxiv.org/abs/2602.02143)
*Shubham Toshniwal,Aleksander Ficek,Siddhartha Jain,Wei Du,Vahid Noroozi,Sadegh Mahdavi,Somshubra Majumdar,Igor Gitman*

Main category: cs.LG

TL;DR: 小型推理模型通过强化学习获得强大的生成选择能力，能在数学和代码推理任务中超越提示和多数投票基线，接近甚至超过更大模型的表现。


<details>
  <summary>Details</summary>
Motivation: 通过并行采样扩展测试时计算可以显著提升LLM推理能力，但受限于Best-of-N选择质量。现有生成选择方法（如GenSelect）虽然解决了这一瓶颈，但强大的选择性能主要局限于大模型。本研究旨在探索小型推理模型是否也能获得强大的生成选择能力。

Method: 从大规模数学和代码指令数据集中筛选出同时包含正确和错误候选解决方案的实例，合成选择任务。使用DAPO（强化学习方法）训练1.7B参数模型，奖励正确的选择行为。

Result: 在数学推理（AIME24、AIME25、HMMT25）和代码推理（LiveCodeBench）基准测试中，训练后的模型一致优于提示和多数投票基线，通常接近或超过更大模型的表现。更重要的是，这些增益能够泛化到选择更强模型的输出，尽管训练时只使用了较弱模型的输出。

Conclusion: 强化学习是解锁小型模型中强大生成选择能力的可扩展方法，能够实现高效的测试时扩展。这表明小型模型通过针对性训练可以获得与大模型相当的选择性能，为实际应用提供了更高效的解决方案。

Abstract: Scaling test-time compute via parallel sampling can substantially improve LLM reasoning, but is often limited by Best-of-N selection quality. Generative selection methods, such as GenSelect, address this bottleneck, yet strong selection performance remains largely limited to large models. We show that small reasoning models can acquire strong GenSelect capabilities through targeted reinforcement learning. To this end, we synthesize selection tasks from large-scale math and code instruction datasets by filtering to instances with both correct and incorrect candidate solutions, and train 1.7B-parameter models with DAPO to reward correct selections. Across math (AIME24, AIME25, HMMT25) and code (LiveCodeBench) reasoning benchmarks, our models consistently outperform prompting and majority-voting baselines, often approaching or exceeding much larger models. Moreover, these gains generalize to selecting outputs from stronger models despite training only on outputs from weaker models. Overall, our results establish reinforcement learning as a scalable way to unlock strong generative selection in small models, enabling efficient test-time scaling.

</details>


### [359] [Back to the Future: Look-ahead Augmentation and Parallel Self-Refinement for Time Series Forecasting](https://arxiv.org/abs/2602.02146)
*Sunho Kim,Susik Yoon*

Main category: cs.LG

TL;DR: BTTF框架通过前瞻增强和自校正精炼提升长期时间序列预测稳定性，无需复杂架构即可显著提高预测精度


<details>
  <summary>Details</summary>
Motivation: 解决长期时间序列预测中并行效率与时间一致性之间的权衡问题。直接多步预测方法速度快但失去时间一致性，迭代多步预测保持时间依赖但存在误差累积和推理速度慢的问题。

Method: 提出BTTF框架，通过前瞻增强和自校正精炼来增强预测稳定性。该方法不依赖复杂架构，而是重新审视基本预测过程，通过集成第二阶段模型（使用初始预测进行增强）来精炼基础模型。

Result: BTTF显著提高了长期预测精度，准确率提升高达58%，即使在第一阶段模型训练条件不理想的情况下也能稳定改进。有效缓解了线性预测模型的不稳定性。

Conclusion: 利用模型生成的预测作为增强数据是提升长期预测能力的简单而有效的方法，即使没有复杂架构也能实现显著改进。

Abstract: Long-term time series forecasting (LTSF) remains challenging due to the trade-off between parallel efficiency and sequential modeling of temporal coherence. Direct multi-step forecasting (DMS) methods enable fast, parallel prediction of all future horizons but often lose temporal consistency across steps, while iterative multi-step forecasting (IMS) preserves temporal dependencies at the cost of error accumulation and slow inference. To bridge this gap, we propose Back to the Future (BTTF), a simple yet effective framework that enhances forecasting stability through look-ahead augmentation and self-corrective refinement. Rather than relying on complex model architectures, BTTF revisits the fundamental forecasting process and refines a base model by ensembling the second-stage models augmented with their initial predictions. Despite its simplicity, our approach consistently improves long-horizon accuracy and mitigates the instability of linear forecasting models, achieving accuracy gains of up to 58% and demonstrating stable improvements even when the first-stage model is trained under suboptimal conditions. These results suggest that leveraging model-generated forecasts as augmentation can be a simple yet powerful way to enhance long-term prediction, even without complex architectures.

</details>


### [360] [ECHO: Entropy-Confidence Hybrid Optimization for Test-Time Reinforcement Learning](https://arxiv.org/abs/2602.02150)
*Chu Zhao,Enneng Yang,Yuting Liu,Jianzhe Zhao,Guibing Guo*

Main category: cs.LG

TL;DR: 提出ECHO方法解决测试时强化学习中的两个关键问题：高熵分支导致的rollout崩溃和早期伪标签噪声引起的过早过拟合，通过自适应分支控制和置信度增强优化提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有测试时强化学习方法采用树结构rollout提高采样效率，但仍面临两个挑战：1）高熵分支可能导致rollout崩溃，分支预算集中在少数高熵轨迹上，减少有效分支数；2）早期伪标签噪声大且存在偏差，可能引发自增强过拟合，导致策略过早锐化并抑制探索。

Method: 提出Entropy Confidence Hybrid Group Relative Policy Optimization (ECHO)方法：1）在rollout阶段，联合利用局部熵和组级置信度自适应控制分支宽度，并引入基于置信度的在线剪枝终止持续低置信度分支；2）在策略更新阶段，采用置信度自适应裁剪和熵-置信度混合优势塑形方法增强训练鲁棒性。

Result: 实验表明ECHO在多个数学和视觉推理基准上取得一致性能提升，在有限rollout预算下具有更好的泛化能力。

Conclusion: ECHO通过自适应分支控制和置信度增强优化，有效解决了测试时强化学习中的rollout崩溃和早期偏差问题，提高了采样效率和训练稳定性。

Abstract: Test-time reinforcement learning generates multiple candidate answers via repeated rollouts and performs online updates using pseudo-labels constructed by majority voting. To reduce overhead and improve exploration, prior work introduces tree structured rollouts, which share reasoning prefixes and branch at key nodes to improve sampling efficiency. However, this paradigm still faces two challenges: (1) high entropy branching can trigger rollout collapse, where the branching budget concentrates on a few trajectories with consecutive high-entropy segments, rapidly reducing the number of effective branches; (2) early pseudo-labels are noisy and biased, which can induce self-reinforcing overfitting, causing the policy to sharpen prematurely and suppress exploration. To address these issues, we propose Entropy Confidence Hybrid Group Relative Policy Optimization (ECHO). During rollout, ECHO jointly leverages local entropy and group level confidence to adaptively control branch width, and further introduces online confidence-based pruning to terminate persistently low confidence branches, avoiding high entropy traps and mitigating collapse. During policy updates, ECHO employs confidence adaptive clipping and an entropy confidence hybrid advantage shaping approach to enhance training robustness and mitigate early stage bias. Experiments demonstrate that ECHO achieves consistent gains on multiple mathematical and visual reasoning benchmarks, and generalizes more effectively under a limited rollout budget.

</details>


### [361] [Revisiting Adaptive Rounding with Vectorized Reparameterization for LLM Quantization](https://arxiv.org/abs/2602.02151)
*Yuli Zhou,Qingxuan Chen,Luca Benini,Guolei Sun,Yawei Li*

Main category: cs.LG

TL;DR: VQRound：一种参数高效的量化优化框架，通过将舍入矩阵重新参数化为紧凑码本来实现自适应舍入，显著减少训练参数，在LLM量化中实现更好的收敛性。


<details>
  <summary>Details</summary>
Motivation: 传统自适应舍入方法需要密集的逐元素舍入矩阵，对于数十亿参数的大语言模型来说计算成本过高。需要一种既能保持自适应舍入优势（跨元素误差抵消）又高效的方法。

Method: 提出VQRound框架：1）将舍入矩阵重新参数化为紧凑码本，减少参数量；2）在L∞范数下最小化逐元素最坏情况误差，适合处理LLM中的重尾权重分布；3）设计轻量级端到端微调流程，仅用128个样本优化所有层的码本。

Result: 在OPT、LLaMA、LLaMA2和Qwen3模型上的实验表明，VQRound在相同步数下比传统自适应舍入收敛更好，同时仅使用0.2%的可训练参数，实现了自适应舍入的可扩展性和快速拟合。

Conclusion: VQRound证明了自适应舍入可以通过参数高效的重新参数化变得既可扩展又快速拟合，为大语言模型的高效量化提供了实用解决方案。

Abstract: Adaptive Rounding has emerged as an alternative to round-to-nearest (RTN) for post-training quantization by enabling cross-element error cancellation. Yet, dense and element-wise rounding matrices are prohibitively expensive for billion-parameter large language models (LLMs). We revisit adaptive rounding from an efficiency perspective and propose VQRound, a parameter-efficient optimization framework that reparameterizes the rounding matrix into a compact codebook. Unlike low-rank alternatives, VQRound minimizes the element-wise worst-case error under $L_\infty$ norm, which is critical for handling heavy-tailed weight distributions in LLMs. Beyond reparameterization, we identify rounding initialization as a decisive factor and develop a lightweight end-to-end finetuning pipeline that optimizes codebooks across all layers using only 128 samples. Extensive experiments on OPT, LLaMA, LLaMA2, and Qwen3 models demonstrate that VQRound achieves better convergence than traditional adaptive rounding at the same number of steps while using as little as 0.2% of the trainable parameters. Our results show that adaptive rounding can be made both scalable and fast-fitting. The code is available at https://github.com/zhoustan/VQRound.

</details>


### [362] [Efficient Neural Controlled Differential Equations via Attentive Kernel Smoothing](https://arxiv.org/abs/2602.02157)
*Egor Serov,Ilya Kuleshov,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 提出一种基于核与高斯过程平滑的Neural CDE路径构建方法，替代传统样条插值，通过多视图CDE框架恢复平滑损失细节，显著降低计算开销并提升性能


<details>
  <summary>Details</summary>
Motivation: 传统Neural CDE使用样条插值构建驱动路径时，路径粗糙度导致高频变化，迫使自适应求解器采用过小步长，大幅增加函数评估次数(NFE)和计算成本

Method: 1) 用核与高斯过程平滑替代精确插值，显式控制轨迹正则性；2) 提出注意力机制的多视图CDE(MV-CDE)及其卷积扩展(MVC-CDE)，使用可学习查询重建路径细节；3) 多轨迹框架让模型在不同路径上分配表示能力，捕捉不同时间模式

Result: MVC-CDE with GP方法在保持最先进准确率的同时，相比基于样条的基线方法显著减少了NFE和总推理时间

Conclusion: 通过平滑路径构建和多视图重建的协同设计，实现了Neural CDE在计算效率与建模能力上的双重提升，为连续时间序列建模提供了更高效的框架

Abstract: Neural Controlled Differential Equations (Neural CDEs) provide a powerful continuous-time framework for sequence modeling, yet the roughness of the driving control path often restricts their efficiency. Standard splines introduce high-frequency variations that force adaptive solvers to take excessively small steps, driving up the Number of Function Evaluations (NFE). We propose a novel approach to Neural CDE path construction that replaces exact interpolation with Kernel and Gaussian Process (GP) smoothing, enabling explicit control over trajectory regularity. To recover details lost during smoothing, we propose an attention-based Multi-View CDE (MV-CDE) and its convolutional extension (MVC-CDE), which employ learnable queries to inform path reconstruction. This framework allows the model to distribute representational capacity across multiple trajectories, each capturing distinct temporal patterns. Empirical results demonstrate that our method, MVC-CDE with GP, achieves state-of-the-art accuracy while significantly reducing NFEs and total inference time compared to spline-based baselines.

</details>


### [363] [Generating Causal Temporal Interaction Graphs for Counterfactual Validation of Temporal Link Prediction](https://arxiv.org/abs/2602.02161)
*Aniq Ur Rahman,Justin P. Coon*

Main category: cs.LG

TL;DR: 提出一个用于时序链接预测模型的反事实验证框架，通过生成具有已知因果结构的因果时序交互图来评估模型是否捕捉到因果机制。


<details>
  <summary>Details</summary>
Motivation: 当前时序链接预测模型主要基于预测准确性评估，但这种方法无法评估模型是否真正捕捉到时序交互背后的因果机制。需要一种能够验证模型因果理解能力的评估框架。

Method: 1. 提出支持兴奋和抑制效应的连续时间事件序列结构方程模型；2. 将该机制扩展到时序交互图；3. 提出基于跨模型预测误差的距离度量来比较因果模型；4. 在两种场景下实例化反事实评估：控制因果偏移和时间戳重排。

Result: 经验验证了假设：在一个因果模型上训练的预测器，在足够远的因果模型上评估时性能会下降。框架为因果感知的基准测试提供了基础。

Conclusion: 该框架能够评估时序链接预测模型是否真正捕捉到因果机制，而不仅仅是预测准确性，为因果感知的模型评估提供了系统方法。

Abstract: Temporal link prediction (TLP) models are commonly evaluated based on predictive accuracy, yet such evaluations do not assess whether these models capture the causal mechanisms that govern temporal interactions. In this work, we propose a framework for counterfactual validation of TLP models by generating causal temporal interaction graphs (CTIGs) with known ground-truth causal structure. We first introduce a structural equation model for continuous-time event sequences that supports both excitatory and inhibitory effects, and then extend this mechanism to temporal interaction graphs. To compare causal models, we propose a distance metric based on cross-model predictive error, and empirically validate the hypothesis that predictors trained on one causal model degrade when evaluated on sufficiently distant models. Finally, we instantiate counterfactual evaluation under (i) controlled causal shifts between generating models and (ii) timestamp shuffling as a stochastic distortion with measurable causal distance. Our framework provides a foundation for causality-aware benchmarking.

</details>


### [364] [Interpretable Tabular Foundation Models via In-Context Kernel Regression](https://arxiv.org/abs/2602.02162)
*Ratmir Miftachov,Bruno Charron,Simon Valentin*

Main category: cs.LG

TL;DR: KernelICL框架通过将表格基础模型的最终预测层替换为核函数，实现了可量化的基于样本的可解释性，在保持性能的同时提供透明预测。


<details>
  <summary>Details</summary>
Motivation: 现有的表格基础模型（如TabPFN和TabICL）虽然通过上下文学习取得了最先进的性能，但其架构本质上是不透明的，缺乏可解释性。需要一种方法来增强这些模型的可解释性，同时保持其性能优势。

Method: 基于上下文学习类似于核回归的洞察，将最终预测层替换为核函数（高斯核、点积核、kNN），使每个预测都成为训练标签的透明加权平均。提出了一个二维分类法，将标准核方法、现代基于邻居的方法和注意力机制统一在一个框架下，并通过训练样本权重分布的困惑度来量化可检查性。

Result: 在55个TALENT基准数据集上，KernelICL实现了与现有表格基础模型相当的性能，表明对最终层的显式核约束能够在不牺牲性能的情况下实现可检查的预测。

Conclusion: 通过将核机制显式化，KernelICL框架成功地为表格基础模型提供了可量化的基于样本的可解释性，实现了透明预测与高性能的平衡，为可解释的表格学习提供了新方向。

Abstract: Tabular foundation models like TabPFN and TabICL achieve state-of-the-art performance through in-context learning, yet their architectures remain fundamentally opaque. We introduce KernelICL, a framework to enhance tabular foundation models with quantifiable sample-based interpretability. Building on the insight that in-context learning is akin to kernel regression, we make this mechanism explicit by replacing the final prediction layer with kernel functions (Gaussian, dot-product, kNN) so that every prediction is a transparent weighted average of training labels. We introduce a two-dimensional taxonomy that formally unifies standard kernel methods, modern neighbor-based approaches, and attention mechanisms under a single framework, and quantify inspectability via the perplexity of the weight distribution over training samples. On 55 TALENT benchmark datasets, KernelICL achieves performance on par with existing tabular foundation models, demonstrating that explicit kernel constraints on the final layer enable inspectable predictions without sacrificing performance.

</details>


### [365] [Co-RedTeam: Orchestrated Security Discovery and Exploitation with LLM Agents](https://arxiv.org/abs/2602.02164)
*Pengfei He,Ash Fox,Lesly Miculicich,Stefan Friedli,Daniel Fabian,Burak Gokturk,Jiliang Tang,Chen-Yu Lee,Tomas Pfister,Long T. Le*

Main category: cs.LG

TL;DR: Co-RedTeam是一个安全感知的多智能体框架，通过集成安全领域知识、代码感知分析、执行基础迭代推理和长期记忆，模拟真实红队工作流程，显著提升漏洞发现和利用的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在网络安全任务中存在局限性：交互有限、执行基础薄弱、缺乏经验复用，难以实现自动化的漏洞发现和利用。需要开发能够模拟真实红队工作流程的系统。

Method: 提出Co-RedTeam框架，将漏洞分析分解为协调的发现和利用阶段。智能体基于真实执行反馈进行规划、执行、验证和优化行动，同时从先前轨迹中学习。框架集成安全领域知识、代码感知分析、执行基础迭代推理和长期记忆。

Result: 在具有挑战性的安全基准测试中，Co-RedTeam在不同骨干模型上始终优于强基线，漏洞利用成功率超过60%，漏洞检测绝对改进超过10%。消融和迭代研究证实执行反馈、结构化交互和记忆对构建稳健可泛化网络安全智能体的关键作用。

Conclusion: Co-RedTeam通过模拟真实红队工作流程，有效解决了现有LLM在网络安全任务中的局限性，为构建能够自动发现和利用漏洞的稳健网络安全智能体提供了有效框架。

Abstract: Large language models (LLMs) have shown promise in assisting cybersecurity tasks, yet existing approaches struggle with automatic vulnerability discovery and exploitation due to limited interaction, weak execution grounding, and a lack of experience reuse. We propose Co-RedTeam, a security-aware multi-agent framework designed to mirror real-world red-teaming workflows by integrating security-domain knowledge, code-aware analysis, execution-grounded iterative reasoning, and long-term memory. Co-RedTeam decomposes vulnerability analysis into coordinated discovery and exploitation stages, enabling agents to plan, execute, validate, and refine actions based on real execution feedback while learning from prior trajectories. Extensive evaluations on challenging security benchmarks demonstrate that Co-RedTeam consistently outperforms strong baselines across diverse backbone models, achieving over 60% success rate in vulnerability exploitation and over 10% absolute improvement in vulnerability detection. Ablation and iteration studies further confirm the critical role of execution feedback, structured interaction, and memory for building robust and generalizable cybersecurity agents.

</details>


### [366] [Generalized Optimal Classification Trees: A Mixed-Integer Programming Approach](https://arxiv.org/abs/2602.02173)
*Jiancheng Tu,Wenqi Fan,Zhibin Wu*

Main category: cs.LG

TL;DR: 提出基于混合整数规划（MIP）的框架，用于学习在非线性性能指标（如F1分数）下的最优分类树，专门解决类别不平衡问题，并通过特定加速技术提高可扩展性。


<details>
  <summary>Details</summary>
Motivation: 决策树的全局优化是组合优化中长期存在的挑战，但这类模型在可解释机器学习中扮演重要角色。虽然问题已研究数十年，但直到最近离散优化的进展才使得在真实数据集上解决最优分类树问题的实用算法成为可能。

Method: 提出基于混合整数规划（MIP）的框架，用于学习在非线性性能指标下的最优分类树。开发了特定问题加速技术：定制分支切割算法、实例缩减方案和热启动策略。

Result: 在50个基准数据集上评估，计算结果表明该框架能高效优化非线性指标，同时实现强预测性能，与现有方法相比减少了求解时间。

Conclusion: 混合整数规划提供了高度的建模灵活性，提出的MIP框架能够有效解决类别不平衡问题，并通过特定加速技术实现可扩展的最优分类树学习。

Abstract: Global optimization of decision trees is a long-standing challenge in combinatorial optimization, yet such models play an important role in interpretable machine learning. Although the problem has been investigated for several decades, only recent advances in discrete optimization have enabled practical algorithms for solving optimal classification tree problems on real-world datasets. Mixed-integer programming (MIP) offers a high degree of modeling flexibility, and we therefore propose a MIP-based framework for learning optimal classification trees under nonlinear performance metrics, such as the F1-score, that explicitly addresses class imbalance. To improve scalability, we develop problem-specific acceleration techniques, including a tailored branch-and-cut algorithm, an instance-reduction scheme, and warm-start strategies. We evaluate the proposed approach on 50 benchmark datasets. The computational results show that the framework can efficiently optimize nonlinear metrics while achieving strong predictive performance and reduced solution times compared with existing methods.

</details>


### [367] [SurvKAN: A Fully Parametric Survival Model Based on Kolmogorov-Arnold Networks](https://arxiv.org/abs/2602.02179)
*Marina Mastroleo,Alberto Archetti,Federico Mastroleo,Matteo Matteucci*

Main category: cs.LG

TL;DR: SurvKAN：基于Kolmogorov-Arnold Networks的完全参数化生存分析模型，消除比例风险假设，在保持可解释性的同时实现竞争性预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统生存模型（如Cox）依赖线性协变量关系和比例风险假设，难以捕捉真实临床动态；深度学习模型（如DeepSurv）虽表达能力更强但牺牲了可解释性，限制了临床采用。需要一种既能保持可解释性又能突破传统假设限制的生存分析方法。

Method: 提出SurvKAN模型，将时间作为KAN架构的显式输入，直接预测对数风险函数。采用完全参数化、时间连续的生存模型设计，基于完整生存似然进行端到端训练。通过可学习的单变量函数保持可解释性，展示个体特征如何随时间影响风险。

Result: 在标准生存基准测试中，SurvKAN在一致性（concordance）和校准（calibration）指标上达到或优于经典方法和最先进基线。可解释性分析揭示了与医学领域知识一致的临床有意义模式。

Conclusion: SurvKAN通过结合KAN架构的优势，在保持临床可解释性的同时突破了传统生存模型的限制，为临床决策提供了更准确、更透明的预测工具。

Abstract: Accurate prediction of time-to-event outcomes is critical for clinical decision-making, treatment planning, and resource allocation in modern healthcare. While classical survival models such as Cox remain widely adopted in standard practice, they rely on restrictive assumptions, including linear covariate relationships and proportional hazards over time, that often fail to capture real-world clinical dynamics. Recent deep learning approaches like DeepSurv and DeepHit offer improved expressivity but sacrifice interpretability, limiting clinical adoption where trust and transparency are paramount. Hybrid models incorporating Kolmogorov-Arnold Networks (KANs), such as CoxKAN, have begun to address this trade-off but remain constrained by the semi-parametric Cox framework. In this work we introduce SurvKAN, a fully parametric, time-continuous survival model based on KAN architectures that eliminates the proportional hazards constraint. SurvKAN treats time as an explicit input to a KAN that directly predicts the log-hazard function, enabling end-to-end training on the full survival likelihood. Our architecture preserves interpretability through learnable univariate functions that indicate how individual features influence risk over time. Extensive experiments on standard survival benchmarks demonstrate that SurvKAN achieves competitive or superior performance compared to classical and state-of-the-art baselines across concordance and calibration metrics. Additionally, interpretability analyses reveal clinically meaningful patterns that align with medical domain knowledge.

</details>


### [368] [STILL: Selecting Tokens for Intra-Layer Hybrid Attention to Linearize LLMs](https://arxiv.org/abs/2602.02180)
*Weikang Meng,Liangyu Huo,Yadan Luo,Jiawen Guan,Jingyi Zhang,Yingjian Li,Zheng Zhang*

Main category: cs.LG

TL;DR: STILL是一个用于高效线性化预训练大语言模型的框架，通过自显著性评分和规范保持特征映射，在保持模型性能的同时显著提升长上下文处理效率。


<details>
  <summary>Details</summary>
Motivation: 现有线性化方法存在两个主要问题：1) 基于滑动窗口分区的token路由方法只能进行基于位置的选择，无法捕捉token特定的全局重要性；2) 可学习特征映射导致分布偏移，扭曲了预训练特征的大小。

Method: STILL框架包含三个核心组件：1) 具有强局部-全局一致性的自显著性评分，通过滑动窗口计算实现准确的token选择；2) 规范保持特征映射(NP-Map)，将特征方向与大小解耦并重新注入预训练规范；3) 统一的训练-推理架构，采用分块并行化和延迟选择以提高硬件效率。

Result: 实验表明，STILL在常识和一般推理任务上匹配或超越了原始预训练模型，在长上下文基准测试中相比之前的线性化注意力方法实现了高达86.2%的相对改进。

Conclusion: STILL通过创新的自显著性评分和规范保持特征映射，有效解决了现有线性化方法的局限性，在保持预训练模型性能的同时显著提升了长上下文处理效率，为大语言模型的高效部署提供了有前景的解决方案。

Abstract: Linearizing pretrained large language models (LLMs) primarily relies on intra-layer hybrid attention mechanisms to alleviate the quadratic complexity of standard softmax attention. Existing methods perform token routing based on sliding-window partitions, resulting in position-based selection and fails to capture token-specific global importance. Meanwhile, linear attention further suffers from distribution shift caused by learnable feature maps that distort pretrained feature magnitudes. Motivated by these limitations, we propose STILL, an intra-layer hybrid linearization framework for efficiently linearizing LLMs. STILL introduces a Self-Saliency Score with strong local-global consistency, enabling accurate token selection using sliding-window computation, and retains salient tokens for sparse softmax attention while summarizing the remaining context via linear attention. To preserve pretrained representations, we design a Norm-Preserved Feature Map (NP-Map) that decouples feature direction from magnitude and reinjects pretrained norms. We further adopt a unified training-inference architecture with chunk-wise parallelization and delayed selection to improve hardware efficiency. Experiments show that STILL matches or surpasses the original pretrained model on commonsense and general reasoning tasks, and achieves up to a 86.2% relative improvement over prior linearized attention methods on long-context benchmarks.

</details>


### [369] [ECHO-2: A Large Scale Distributed Rollout Framework for Cost-efficient Reinforcement Learning](https://arxiv.org/abs/2602.02192)
*Jie Xiao,Meng Chen,Qingnan Ren,Song Jingwei,Jiaqi Huang,Yangshen Deng,Chris Tong,Wanyi Chen,Suli Wang,Ziqian Bi,Shuo Lu,Yiqun Duan,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: ECHO-2是一个分布式强化学习框架，用于大语言模型的后训练，通过远程推理工作节点和重叠策略传播来提升成本效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型后训练中的强化学习阶段需要重复的交互过程，包括生成、奖励评估和集中学习。分布式执行生成可以利用成本更低的推理资源，但面临广域协调和策略传播的挑战。

Method: ECHO-2结合集中式学习和分布式生成，将策略过时性作为用户可控参数，使生成、传播和训练重叠执行。采用基于重叠的容量模型，使用对等辅助流水线广播和成本感知的异构工作节点激活。

Result: 在4B和8B模型的GRPO后训练实验中，在真实广域带宽条件下，ECHO-2显著提高了成本效率，同时保持了与强基线相当的强化学习奖励。

Conclusion: ECHO-2通过分布式生成和重叠策略传播，有效解决了大语言模型后训练中的成本效率问题，为分布式强化学习提供了实用解决方案。

Abstract: Reinforcement learning (RL) is a critical stage in post-training large language models (LLMs), involving repeated interaction between rollout generation, reward evaluation, and centralized learning. Distributing rollout execution offers opportunities to leverage more cost-efficient inference resources, but introduces challenges in wide-area coordination and policy dissemination. We present ECHO-2, a distributed RL framework for post-training with remote inference workers and non-negligible dissemination latency. ECHO-2 combines centralized learning with distributed rollouts and treats bounded policy staleness as a user-controlled parameter, enabling rollout generation, dissemination, and training to overlap. We introduce an overlap-based capacity model that relates training time, dissemination latency, and rollout throughput, yielding a practical provisioning rule for sustaining learner utilization. To mitigate dissemination bottlenecks and lower cost, ECHO-2 employs peer-assisted pipelined broadcast and cost-aware activation of heterogeneous workers. Experiments on GRPO post-training of 4B and 8B models under real wide-area bandwidth regimes show that ECHO-2 significantly improves cost efficiency while preserving RL reward comparable to strong baselines.

</details>


### [370] [State Rank Dynamics in Linear Attention LLMs](https://arxiv.org/abs/2602.02195)
*Ao Sun,Hongtao Zhang,Heng Zhou,Yixuan Ma,Yiran Qin,Tongrui Su,Yan Liu,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: 研究发现线性注意力LLM存在状态秩分层现象：低秩头对推理至关重要，高秩头具有冗余性，据此提出的剪枝策略可减少38.9%的KV缓存开销


<details>
  <summary>Details</summary>
Motivation: 线性注意力LLM通过固定大小的状态矩阵压缩上下文实现常数时间推理，但其内部压缩状态的动态特性仍不透明，需要深入理解状态动态特性

Method: 对最先进的线性注意力模型进行运行时状态动态研究，发现状态秩分层现象，通过诊断探针分析功能差异，提出联合秩-范数剪枝策略

Result: 发现线性注意力头存在明显的谱分叉：一组保持接近零的有效秩，另一组快速增长并收敛到上界；低秩头对模型推理不可或缺，高秩头具有显著冗余性

Conclusion: 线性注意力头的秩特性是预训练获得的内在结构属性，基于此提出的零样本剪枝策略能有效减少KV缓存开销，同时保持模型准确性

Abstract: Linear Attention Large Language Models (LLMs) offer a compelling recurrent formulation that compresses context into a fixed-size state matrix, enabling constant-time inference. However, the internal dynamics of this compressed state remain largely opaque. In this work, we present a comprehensive study on the runtime state dynamics of state-of-the-art Linear Attention models. We uncover a fundamental phenomenon termed State Rank Stratification, characterized by a distinct spectral bifurcation among linear attention heads: while one group maintains an effective rank oscillating near zero, the other exhibits rapid growth that converges to an upper bound. Extensive experiments across diverse inference contexts reveal that these dynamics remain strikingly consistent, indicating that the identity of a head,whether low-rank or high-rank,is an intrinsic structural property acquired during pre-training, rather than a transient state dependent on the input data. Furthermore, our diagnostic probes reveal a surprising functional divergence: low-rank heads are indispensable for model reasoning, whereas high-rank heads exhibit significant redundancy. Leveraging this insight, we propose Joint Rank-Norm Pruning, a zero-shot strategy that achieves a 38.9\% reduction in KV-cache overhead while largely maintaining model accuracy.

</details>


### [371] [Hierarchical Adaptive Eviction for KV Cache Management in Multimodal Language Models](https://arxiv.org/abs/2602.02197)
*Xindian Ma,Yidi Lu,Peng Zhang,Jing Zhang*

Main category: cs.LG

TL;DR: HAE是一种针对多模态大语言模型的KV缓存逐出框架，通过分层自适应策略优化视觉-文本token交互，显著减少内存使用并提升推理速度


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存逐出策略未能处理视觉和文本token之间的异质注意力分布，导致效率低下或性能下降，需要专门针对MLLMs的优化方案

Method: 提出分层自适应逐出框架：预填充阶段使用双重注意力剪枝（利用视觉token稀疏性和注意力方差），解码阶段采用动态解码逐出策略（受操作系统回收站启发）

Result: 在图像理解任务中减少41% KV缓存内存，仅损失0.3%准确率；在故事生成推理中加速1.5倍，同时保持输出质量（基于Phi3.5-Vision-Instruct模型）

Conclusion: HAE通过理论保证的信息完整性和更低误差边界，有效解决了MLLMs中视觉-文本token交互的KV缓存优化问题，在效率和性能间取得良好平衡

Abstract: The integration of visual information into Large Language Models (LLMs) has enabled Multimodal LLMs (MLLMs), but the quadratic memory and computational costs of Transformer architectures remain a bottleneck. Existing KV cache eviction strategies fail to address the heterogeneous attention distributions between visual and text tokens, leading to suboptimal efficiency or degraded performance. In this paper, we propose Hierarchical Adaptive Eviction (HAE), a KV cache eviction framework that optimizes text-visual token interaction in MLLMs by implementing Dual-Attention Pruning during pre-filling (leveraging visual token sparsity and attention variance) and a Dynamic Decoding Eviction Strategy (inspired by OS Recycle Bins) during decoding. HAE minimizes KV cache usage across layers, reduces computational overhead via index broadcasting, and theoretically ensures superior information integrity and lower error bounds compared to greedy strategies, enhancing efficiency in both comprehension and generation tasks. Empirically, HAE reduces KV-Cache memory by 41\% with minimal accuracy loss (0.3\% drop) in image understanding tasks and accelerates story generation inference by 1.5x while maintaining output quality on Phi3.5-Vision-Instruct model.

</details>


### [372] [Cardinality-Preserving Structured Sparse Graph Transformers for Molecular Property Prediction](https://arxiv.org/abs/2602.02201)
*Abhijit Gupta*

Main category: cs.LG

TL;DR: CardinalGraphFormer是一个图Transformer模型，通过结合Graphormer的结构偏置和结构化稀疏注意力，在分子表示学习中取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 药物发现需要高效的分子性质预测，但化学空间巨大（约10^60个类药分子），而仅有数千种获批药物，因此需要利用大规模无标签数据进行自监督预训练来实现数据高效的分子表示学习。

Method: 提出CardinalGraphFormer图Transformer，结合Graphormer的结构偏置（最短路径距离、中心性、直接键边偏置），采用结构化稀疏注意力（限制最短路径距离≤3），并增加基数保持的未归一化聚合通道。预训练结合对比图级对齐和掩码属性重建。

Result: 在完全匹配的评估协议下，CardinalGraphFormer在所有11个评估任务中均提升了平均性能，在MoleculeNet、OGB和TDC ADMET基准的10/11个任务上取得了统计显著增益。

Conclusion: CardinalGraphFormer通过结构化稀疏注意力和基数保持聚合，在分子表示学习中实现了显著改进，为数据高效的药物发现提供了有效解决方案。

Abstract: Drug discovery motivates efficient molecular property prediction under limited labeled data. Chemical space is vast, often estimated at approximately 10^60 drug-like molecules, while only thousands of drugs have been approved. As a result, self-supervised pretraining on large unlabeled molecular corpora has become essential for data-efficient molecular representation learning. We introduce **CardinalGraphFormer**, a graph transformer that incorporates Graphormer-inspired structural biases, including shortest-path distance and centrality, as well as direct-bond edge bias, within a structured sparse attention regime limited to shortest-path distance <= 3. The model further augments this design with a cardinality-preserving unnormalized aggregation channel over the same support set. Pretraining combines contrastive graph-level alignment with masked attribute reconstruction. Under a fully matched evaluation protocol, CardinalGraphFormer improves mean performance across all 11 evaluated tasks and achieves statistically significant gains on 10 of 11 public benchmarks spanning MoleculeNet, OGB, and TDC ADMET tasks when compared to strong reproduced baselines.

</details>


### [373] [Fat-Cat: Document-Driven Metacognitive Multi-Agent System for Complex Reasoning](https://arxiv.org/abs/2602.02206)
*Tong Yang,Yemin Wang,Chaoning Zhang,Aming Wu*

Main category: cs.LG

TL;DR: Fat-Cat是一种基于文档的智能体架构，通过Markdown文档表示状态、文本策略演进和闭环监控，提高上下文信息利用效率，在推理、检索和编码任务上优于传统JSON状态管理方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体框架依赖嵌套JSON等语法繁重的状态表示，迫使模型将有限注意力浪费在语法处理而非语义推理上，限制了智能体的有效性。

Method: 提出Fat-Cat架构，包含三个核心组件：1) 语义文件系统，用Markdown文档表示智能体状态；2) 文本策略演进模块，积累任务解决知识；3) 闭环监控器，监控推理轨迹减少幻觉。

Result: 在推理、检索和编码基准测试中表现优异，使Kimi-k2模型在HotPotQA上超越GPT-4o基线。实验表明，将文档状态替换为JSON会导致性能下降，验证了文档驱动状态建模的必要性。

Conclusion: 文档驱动的状态表示比传统JSON等刚性语法结构更有效，能提高智能体的信号噪声比，使模型专注于语义推理而非语法处理，提升整体性能。

Abstract: The effectiveness of LLM-based agents is often limited not by model capacity alone, but by how efficiently contextual information is utilized at runtime. Existing agent frameworks rely on rigid, syntax-heavy state representations such as nested JSON, which require models to devote a substantial portion of their limited attention to syntactic processing rather than semantic reasoning. In this paper, we propose Fat-Cat, a document-driven agent architecture that improves the signal-to-noise ratio of state management. By integrating three key components: (1) a Semantic File System that represents agent state as Markdown documents aligned with common pre-training corpora, (2) a Textual Strategy Evolution module that accumulates task-solving knowledge without parameter updates, and (3) a Closed-Loop Watcher that monitors reasoning trajectories to reduce hallucinations. Extensive reasoning, retrieval, and coding benchmarks, Fat-Cat consistently improves agent performance. It enables the Kimi-k2 model to outperform the proprietary GPT-4o baseline on HotPotQA. Replacing the document-based state with JSON leads to performance drop, while empirically validating the critical necessity of document-driven state modeling over rigid syntax. The code is available at https://github.com/answeryt/Fat-Cat.

</details>


### [374] [Generating Physically Sound Designs from Text and a Set of Physical Constraints](https://arxiv.org/abs/2602.02213)
*Gregory Barber,Todd C. Henry,Mulugeta A. Haile*

Main category: cs.LG

TL;DR: TIDES是一种基于文本描述和物理约束的文本引导设计方法，能够联合优化结构（拓扑）和视觉属性，生成符合物理原理的设计。


<details>
  <summary>Details</summary>
Motivation: 传统设计方法难以同时满足物理性能要求和文本描述的美学/功能特征，需要一种能够将文本指导与物理约束相结合的设计方法。

Method: 使用预训练的文本-图像模型衡量设计与文本提示的视觉对齐度，同时使用可微分物理模拟器评估物理性能，通过联合优化这两个目标来生成设计。

Result: TIDES在不同载荷和支撑条件下的结构优化问题中表现良好，能够满足工程设计需求（柔顺性和密度），同时实现文本指定的特征。实验验证包括3点弯曲测试的2D梁设计。

Conclusion: TIDES成功实现了文本引导的物理设计生成，能够联合优化视觉和物理目标，为工程设计中结合功能性和美学特征提供了有效方法。

Abstract: We present TIDES, a text informed design approach for generating physically sound designs based on a textual description and a set of physical constraints. TIDES jointly optimizes structural (topology) and visual properties. A pre-trained text-image model is used to measure the design's visual alignment with a text prompt and a differentiable physics simulator is used to measure its physical performance. We evaluate TIDES on a series of structural optimization problems operating under different load and support conditions, at different resolutions, and experimentally in the lab by performing the 3-point bending test on 2D beam designs that are extruded and 3D printed. We find that it can jointly optimize the two objectives and return designs that satisfy engineering design requirements (compliance and density) while utilizing features specified by text.

</details>


### [375] [Scientific Theory of a Black-Box: A Life Cycle-Scale XAI Framework Based on Constructive Empiricism](https://arxiv.org/abs/2602.02215)
*Sebastian Müller,Vanessa Toborek,Eike Stadtländer,Tamás Horváth,Brendan Balcerak Jackson,Christian Bauckhage*

Main category: cs.LG

TL;DR: 提出"黑盒科学理论"(SToBB)框架，将可解释AI的分散解释整合为伴随黑盒模型全生命周期的持久、可审计的文档化理论


<details>
  <summary>Details</summary>
Motivation: 当前可解释AI(XAI)算法只能回答特定问题，缺乏将黑盒模型的解释信息整合为持久、可审计文档的系统方法，无法支持模型全生命周期的可追溯性

Method: 基于建构经验主义提出SToBB概念，要求满足三个义务：经验充分性、可适应性、可审计性；实现为包含可扩展观察库、可追溯假设类、构造与修订算法组件的框架；开发了CoBoT算法在线构建和维护规则替代模型

Result: 为表格任务的神经网络分类器实例化了完整的SToBB，通过CoBoT算法随着观察积累构建和维护经验充分的规则替代模型，支持通过接口查询而非孤立输出

Conclusion: SToBB作为生命周期规模的可检查参考点，支持一致、可重用的分析和系统性外部审查，为黑盒模型提供持久、可审计的理论文档

Abstract: Explainable AI (XAI) offers a growing number of algorithms that aim to answer specific questions about black-box models. What is missing is a principled way to consolidate explanatory information about a fixed black-box model into a persistent, auditable artefact, that accompanies the black-box throughout its life cycle. We address this gap by introducing the notion of a scientific theory of a black (SToBB). Grounded in Constructive Empiricism, a SToBB fulfils three obligations: (i) empirical adequacy with respect to all available observations of black-box behaviour, (ii) adaptability via explicit update commitments that restore adequacy when new observations arrive, and (iii) auditability through transparent documentation of assumptions, construction choices, and update behaviour. We operationalise these obligations as a general framework that specifies an extensible observation base, a traceable hypothesis class, algorithmic components for construction and revision, and documentation sufficient for third-party assessment. Explanations for concrete stakeholder needs are then obtained by querying the maintained record through interfaces, rather than by producing isolated method outputs. As a proof of concept, we instantiate a complete SToBB for a neural-network classifier on a tabular task and introduce the Constructive Box Theoriser (CoBoT) algorithm, an online procedure that constructs and maintains an empirically adequate rule-based surrogate as observations accumulate. Together, these contributions position SToBBs as a life cycle-scale, inspectable point of reference that supports consistent, reusable analyses and systematic external scrutiny.

</details>


### [376] [Spectral Superposition: A Theory of Feature Geometry](https://arxiv.org/abs/2602.02224)
*Georgi Ivanov,Narmeen Oozeer,Shivam Raval,Tasana Pejovic,Shriyash Upadhyay,Amir Abdullah*

Main category: cs.LG

TL;DR: 提出利用谱方法分析神经网络特征几何结构的新理论，通过帧算子研究特征在特征空间中的分布，证明容量饱和会导致谱定位现象。


<details>
  <summary>Details</summary>
Motivation: 现有方法将激活分解为稀疏线性特征但丢弃了几何结构，需要开发能够分析特征全局几何结构的新理论工具。

Method: 引入帧算子F=WW⊤作为谱度量工具，分析权重矩阵的谱特性（特征值、特征空间等），研究特征在特征空间中的范数分配方式。

Result: 在叠加的玩具模型中证明容量饱和会强制谱定位：特征坍缩到单个特征空间，组织成紧框架，并通过关联方案进行离散分类，涵盖先前工作中的所有几何结构。

Conclusion: 谱度量形式适用于任意权重矩阵，能够诊断超越玩具设置的特征定位现象，为将算子理论应用于可解释性研究开辟了更广阔的研究方向。

Abstract: Neural networks represent more features than they have dimensions via superposition, forcing features to share representational space. Current methods decompose activations into sparse linear features but discard geometric structure. We develop a theory for studying the geometric structre of features by analyzing the spectra (eigenvalues, eigenspaces, etc.) of weight derived matrices. In particular, we introduce the frame operator $F = WW^\top$, which gives us a spectral measure that describes how each feature allocates norm across eigenspaces. While previous tools could describe the pairwise interactions between features, spectral methods capture the global geometry (``how do all features interact?''). In toy models of superposition, we use this theory to prove that capacity saturation forces spectral localization: features collapse onto single eigenspaces, organize into tight frames, and admit discrete classification via association schemes, classifying all geometries from prior work (simplices, polygons, antiprisms). The spectral measure formalism applies to arbitrary weight matrices, enabling diagnosis of feature localization beyond toy settings. These results point toward a broader program: applying operator theory to interpretability.

</details>


### [377] [Prediction-Powered Risk Monitoring of Deployed Models for Detecting Harmful Distribution Shifts](https://arxiv.org/abs/2602.02229)
*Guangyi Zhang,Yunlong Cai,Guanding Yu,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出预测驱动风险监控(PPRM)，一种基于预测驱动推理的半监督风险监控方法，用于在标注数据有限的动态环境中监控模型性能


<details>
  <summary>Details</summary>
Motivation: 在动态环境中监控模型性能时，标注数据通常有限，需要一种能够有效利用合成标签和少量真实标签的方法来检测有害的性能变化

Method: 基于预测驱动推理(PPI)，PPRM通过结合合成标签和少量真实标签构建运行风险的下界，并通过与名义风险上界的阈值比较来检测有害变化

Result: 在图像分类、大语言模型和电信监控任务上的广泛实验证明了PPRM的有效性，该方法在误报概率方面满足无假设有限样本保证

Conclusion: PPRM为动态环境中的模型性能监控提供了一种有效的半监督方法，能够在标注数据有限的情况下可靠地检测性能下降

Abstract: We study the problem of monitoring model performance in dynamic environments where labeled data are limited. To this end, we propose prediction-powered risk monitoring (PPRM), a semi-supervised risk-monitoring approach based on prediction-powered inference (PPI). PPRM constructs anytime-valid lower bounds on the running risk by combining synthetic labels with a small set of true labels. Harmful shifts are detected via a threshold-based comparison with an upper bound on the nominal risk, satisfying assumption-free finite-sample guarantees in the probability of false alarm. We demonstrate the effectiveness of PPRM through extensive experiments on image classification, large language model (LLM), and telecommunications monitoring tasks.

</details>


### [378] [SEDformer: Event-Synchronous Spiking Transformers for Irregular Telemetry Time Series Forecasting](https://arxiv.org/abs/2602.02230)
*Ziyu Zhou,Yuchen Fang,Weilin Ruan,Shiyu Wang,James Kwok,Yuxuan Liang*

Main category: cs.LG

TL;DR: SEDformer：一种基于脉冲神经网络的SED增强型脉冲Transformer，用于不规则多元时间序列预测，通过事件驱动的方式自然匹配IMTS的稀疏-事件对偶特性，在提高预测精度的同时降低能耗和内存使用。


<details>
  <summary>Details</summary>
Motivation: 大规模互联网系统（如物联网和在线平台）的遥测数据自然形成不规则多元时间序列（IMTS），其准确预测对运营至关重要。现有基于图和Transformer的预测方法忽略了IMTS的稀疏-事件对偶（SED）特性：通过填充对齐到均匀网格违反了稀疏性，而关系重构破坏了局部时间连续性。这需要一种更忠实、更自然的IMTS建模范式。

Method: 提出SEDformer，一个SED增强的脉冲Transformer，包含三个核心组件：1）SED-based Spike Encoder：使用事件对齐LIF神经元将原始观测转换为事件同步脉冲；2）Event-Preserving Temporal Downsampling：压缩长间隙同时保留显著脉冲；3）SED-based Spike Transformer blocks：使用基于膜电位的线性注意力实现序列内依赖建模。

Result: 在公共遥测IMTS数据集上的实验表明，SEDformer达到了最先进的预测精度，同时显著降低了能耗和内存使用。

Conclusion: SEDformer为建模IMTS提供了一种自然且高效的路径，通过利用脉冲神经网络的事件驱动特性，更好地匹配IMTS的稀疏-事件对偶本质，在保持高预测性能的同时实现了计算效率的提升。

Abstract: Telemetry streams from large-scale Internet-connected systems (e.g., IoT deployments and online platforms) naturally form an irregular multivariate time series (IMTS) whose accurate forecasting is operationally vital. A closer examination reveals a defining Sparsity-Event Duality (SED) property of IMTS, i.e., long stretches with sparse or no observations are punctuated by short, dense bursts where most semantic events (observations) occur. However, existing Graph- and Transformer-based forecasters ignore SED: pre-alignment to uniform grids with heavy padding violates sparsity by inflating sequences and forcing computation at non-informative steps, while relational recasting weakens event semantics by disrupting local temporal continuity. These limitations motivate a more faithful and natural modeling paradigm for IMTS that aligns with its SED property. We find that Spiking Neural Networks meet this requirement, as they communicate via sparse binary spikes and update in an event-driven manner, aligning naturally with the SED nature of IMTS. Therefore, we present SEDformer, an SED-enhanced Spiking Transformer for telemetry IMTS forecasting that couples: (1) a SED-based Spike Encoder converts raw observations into event synchronous spikes using an Event-Aligned LIF neuron, (2) an Event-Preserving Temporal Downsampling module compresses long gaps while retaining salient firings and (3) a stack of SED-based Spike Transformer blocks enable intra-series dependency modeling with a membrane-based linear attention driven by EA-LIF spiking features. Experiments on public telemetry IMTS datasets show that SEDformer attains state-of-the-art forecasting accuracy while reducing energy and memory usage, providing a natural and efficient path for modeling IMTS.

</details>


### [379] [Geometry- and Relation-Aware Diffusion for EEG Super-Resolution](https://arxiv.org/abs/2602.02238)
*Laura Yao,Gengwei Zhang,Moajjem Chowdhury,Yunmei Liu,Tianlong Chen*

Main category: cs.LG

TL;DR: TopoDiff：一种用于EEG空间超分辨率的几何和关系感知扩散模型，通过结合拓扑感知图像嵌入和动态通道关系图，显著提高了EEG信号的空间生成质量和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有EEG空间超分辨率方法缺乏对生理空间结构的认知，限制了空间生成性能。人类专家在解读EEG空间模式时会考虑拓扑结构，因此需要开发能够理解EEG几何和通道关系的模型。

Method: 提出TopoDiff模型，包含两个关键组件：1）从EEG地形图表示中提取的拓扑感知图像嵌入，为空间生成提供全局几何上下文；2）动态通道关系图，编码电极间关系并随时间动态演化。该设计形成了具有空间基础性的EEG超分辨率框架。

Result: 在多个EEG数据集上（包括SEED/SEED-IV情感识别、PhysioNet运动想象MI/MM、TUSZ癫痫检测）都取得了显著成果：生成保真度大幅提升，下游EEG任务性能明显改善。

Conclusion: TopoDiff通过结合几何和关系感知机制，成功解决了现有EEG空间超分辨率方法缺乏生理空间结构认知的问题，为EEG信号的空间生成提供了更准确、更有效的解决方案。

Abstract: Recent electroencephalography (EEG) spatial super-resolution (SR) methods, while showing improved quality by either directly predicting missing signals from visible channels or adapting latent diffusion-based generative modeling to temporal data, often lack awareness of physiological spatial structure, thereby constraining spatial generation performance. To address this issue, we introduce TopoDiff, a geometry- and relation-aware diffusion model for EEG spatial super-resolution. Inspired by how human experts interpret spatial EEG patterns, TopoDiff incorporates topology-aware image embeddings derived from EEG topographic representations to provide global geometric context for spatial generation, together with a dynamic channel-relation graph that encodes inter-electrode relationships and evolves with temporal dynamics. This design yields a spatially grounded EEG spatial super-resolution framework with consistent performance improvements. Across multiple EEG datasets spanning diverse applications, including SEED/SEED-IV for emotion recognition, PhysioNet motor imagery (MI/MM), and TUSZ for seizure detection, our method achieves substantial gains in generation fidelity and leads to notable improvements in downstream EEG task performance.

</details>


### [380] [Interpretability in Deep Time Series Models Demands Semantic Alignment](https://arxiv.org/abs/2602.02239)
*Giovanni De Felice,Riccardo D'Elia,Alberto Termine,Pietro Barbiero,Giuseppe Marra,Silvia Santini*

Main category: cs.LG

TL;DR: 该论文提出深度时间序列模型应追求语义对齐，即预测需用对用户有意义的变量表达，并通过可接受用户约束的时空机制实现，且这种对齐必须在时间演化中保持。


<details>
  <summary>Details</summary>
Motivation: 现有深度时间序列模型虽然预测性能不断提升，但其黑盒特性限制了实际部署。当前的解释性方法主要关注解释模型内部计算，而没有解决这些解释是否与人类对研究现象的理解方式相一致的问题。

Method: 论文形式化了语义对齐的要求，强调预测必须用对终端用户有意义的变量表达，并通过可接受用户依赖约束的时空机制实现。特别提出语义对齐必须在时间演化中保持，这是静态场景中没有的约束条件。

Result: 论文为语义对齐的深度时间序列模型提供了蓝图，识别了支持信任的特性，并讨论了模型设计的影响。

Conclusion: 深度时间序列模型的解释性应追求语义对齐，而不仅仅是解释内部计算。这种对齐需要在时间维度上保持一致性，为构建更可信、可部署的时间序列模型提供了新的设计方向。

Abstract: Deep time series models continue to improve predictive performance, yet their deployment remains limited by their black-box nature. In response, existing interpretability approaches in the field keep focusing on explaining the internal model computations, without addressing whether they align or not with how a human would reason about the studied phenomenon. Instead, we state interpretability in deep time series models should pursue semantic alignment: predictions should be expressed in terms of variables that are meaningful to the end user, mediated by spatial and temporal mechanisms that admit user-dependent constraints. In this paper, we formalize this requirement and require that, once established, semantic alignment must be preserved under temporal evolution: a constraint with no analog in static settings. Provided with this definition, we outline a blueprint for semantically aligned deep time series models, identify properties that support trust, and discuss implications for model design.

</details>


### [381] [Variational Entropic Optimal Transport](https://arxiv.org/abs/2602.02241)
*Roman Dyachenko,Nikita Gushchin,Kirill Sokolov,Petr Mokrov,Evgeny Burnaev,Alexander Korotin*

Main category: cs.LG

TL;DR: 提出VarEOT方法，通过变分重构log-partition项，解决弱对偶EOT目标中计算不可行的问题，避免了MCMC模拟，在图像翻译任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有连续空间二次代价的熵最优传输方法在优化弱对偶目标时，由于log-partition项难以计算而效率低下。现有方法要么限制传输族（高斯混合参数化），要么需要模拟训练过程，都有局限性。

Method: 提出VarEOT方法，将log-partition项重构为可处理的辅助正归一化器的最小化问题，得到可微学习目标，使用随机梯度优化，无需MCMC模拟训练。

Result: 在合成数据和未配对图像到图像翻译实验中，VarEOT表现出竞争性或改进的翻译质量。与使用相同弱对偶EOT目标的求解器相比，验证了所提优化原理的优势。

Conclusion: VarEOT通过变分重构log-partition项，解决了弱对偶EOT目标的计算难题，提供了理论保证和实用优势，在域翻译问题中具有竞争力。

Abstract: Entropic optimal transport (EOT) in continuous spaces with quadratic cost is a classical tool for solving the domain translation problem. In practice, recent approaches optimize a weak dual EOT objective depending on a single potential, but doing so is computationally not efficient due to the intractable log-partition term. Existing methods typically resolve this obstacle in one of two ways: by significantly restricting the transport family to obtain closed-form normalization (via Gaussian-mixture parameterizations), or by using general neural parameterizations that require simulation-based training procedures. We propose Variational Entropic Optimal Transport (VarEOT), based on an exact variational reformulation of the log-partition $\log \mathbb{E}[\exp(\cdot)]$ as a tractable minimization over an auxiliary positive normalizer. This yields a differentiable learning objective optimized with stochastic gradients and avoids the necessity of MCMC simulations during the training. We provide theoretical guarantees, including finite-sample generalization bounds and approximation results under universal function approximation. Experiments on synthetic data and unpaired image-to-image translation demonstrate competitive or improved translation quality, while comparisons within the solvers that use the same weak dual EOT objective support the benefit of the proposed optimization principle.

</details>


### [382] [Learning While Staying Curious: Entropy-Preserving Supervised Fine-Tuning via Adaptive Self-Distillation for Large Reasoning Models](https://arxiv.org/abs/2602.02244)
*Hao Wang,Hao Gu,Hongming Piao,Kaixiong Gong,Yuxiao Ye,Xiangyu Yue,Sirui Han,Yike Guo,Dapeng Wu*

Main category: cs.LG

TL;DR: 提出CurioSFT方法，通过保持熵和内在好奇心增强监督微调阶段的探索能力，为后续强化学习阶段创造更好的起点。


<details>
  <summary>Details</summary>
Motivation: 传统SFT-then-RL流程中，监督微调阶段会导致模型过度自信和生成多样性降低，限制了强化学习阶段的探索空间。简单的熵正则化方法会使token分布趋于均匀，无法真正提升有意义的探索能力。

Method: CurioSFT包含两个核心组件：1) 自探索蒸馏：将模型蒸馏到自生成的温度缩放教师模型，鼓励在自身能力范围内的探索；2) 熵引导温度选择：自适应调整蒸馏强度，在推理token上增强探索，在事实token上保持稳定，减轻知识遗忘。

Result: 在数学推理任务上，CurioSFT在SFT阶段比传统SFT提升2.5个点（分布内）和2.9个点（分布外）。更重要的是，SFT阶段保留的探索能力成功转化为RL阶段的具体收益，平均提升5.0个点。

Conclusion: CurioSFT通过保持熵和引入内在好奇心，有效解决了传统SFT导致的探索能力下降问题，为后续RL阶段创造了更好的起点，实现了端到端的性能提升。

Abstract: The standard post-training recipe for large reasoning models, supervised fine-tuning followed by reinforcement learning (SFT-then-RL), may limit the benefits of the RL stage: while SFT imitates expert demonstrations, it often causes overconfidence and reduces generation diversity, leaving RL with a narrowed solution space to explore. Adding entropy regularization during SFT is not a cure-all; it tends to flatten token distributions toward uniformity, increasing entropy without improving meaningful exploration capability. In this paper, we propose CurioSFT, an entropy-preserving SFT method designed to enhance exploration capabilities through intrinsic curiosity. It consists of (a) Self-Exploratory Distillation, which distills the model toward a self-generated, temperature-scaled teacher to encourage exploration within its capability; and (b) Entropy-Guided Temperature Selection, which adaptively adjusts distillation strength to mitigate knowledge forgetting by amplifying exploration at reasoning tokens while stabilizing factual tokens. Extensive experiments on mathematical reasoning tasks demonstrate that, in SFT stage, CurioSFT outperforms the vanilla SFT by 2.5 points on in-distribution tasks and 2.9 points on out-of-distribution tasks. We also verify that exploration capabilities preserved during SFT successfully translate into concrete gains in RL stage, yielding an average improvement of 5.0 points.

</details>


### [383] [Alignment-Aware Model Adaptation via Feedback-Guided Optimization](https://arxiv.org/abs/2602.02258)
*Gaurav Bhatt,Aditya Chinchure,Jiawei Zhou,Leonid Sigal*

Main category: cs.LG

TL;DR: 提出一种对齐感知的微调框架，通过策略梯度正则化整合外部对齐信号，引入自适应门控机制平衡监督和对齐梯度，学习对完全未对齐输入的弃权行为，在保持下游任务性能的同时减少有害和幻觉输出。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法主要优化任务目标，忽略了安全性和避免幻觉等关键对齐目标，导致下游微调可能破坏模型对齐性，无法纠正预训练中已有的未对齐行为。

Method: 提出对齐感知微调框架，通过策略梯度正则化整合外部对齐信号反馈；引入自适应门控机制，基于每个样本动态平衡监督和对齐驱动梯度；学习对完全未对齐输入的弃权行为，将保守响应直接纳入微调模型。

Result: 在通用和领域特定指令微调基准测试中，该方法在保持下游任务性能的同时，持续减少有害和幻觉输出；额外分析显示对对抗性微调、提示攻击和不安全初始化的鲁棒性。

Conclusion: 自适应门控对齐优化是一种有效的对齐保持和对齐恢复模型适应方法，能够在微调过程中同时优化任务目标和对齐目标。

Abstract: Fine-tuning is the primary mechanism for adapting foundation models to downstream tasks; however, standard approaches largely optimize task objectives in isolation and do not account for secondary yet critical alignment objectives (e.g., safety and hallucination avoidance). As a result, downstream fine-tuning can degrade alignment and fail to correct pre-existing misaligned behavior. We propose an alignment-aware fine-tuning framework that integrates feedback from an external alignment signal through policy-gradient-based regularization. Our method introduces an adaptive gating mechanism that dynamically balances supervised and alignment-driven gradients on a per-sample basis, prioritizing uncertain or misaligned cases while allowing well-aligned examples to follow standard supervised updates. The framework further learns abstention behavior for fully misaligned inputs, incorporating conservative responses directly into the fine-tuned model. Experiments on general and domain-specific instruction-tuning benchmarks demonstrate consistent reductions in harmful and hallucinated outputs without sacrificing downstream task performance. Additional analyses show robustness to adversarial fine-tuning, prompt-based attacks, and unsafe initializations, establishing adaptively gated alignment optimization as an effective approach for alignment-preserving and alignment-recovering model adaptation.

</details>


### [384] [Segment to Focus: Guiding Latent Action Models in the Presence of Distractors](https://arxiv.org/abs/2602.02259)
*Hamza Adnan,Matthew T. Jackson,Alexey Zakharov*

Main category: cs.LG

TL;DR: MaskLAM通过引入视觉代理分割来改进LAM训练，有效过滤动作相关噪声，在存在背景干扰的环境中显著提升强化学习性能


<details>
  <summary>Details</summary>
Motivation: LAMs（潜在动作模型）能够从原始观察中学习提取动作相关表示，但面临一个关键挑战：难以区分动作相关特征与动作相关噪声（如背景运动）。未能过滤这些干扰会导致LAMs捕获虚假相关性并构建次优的潜在动作空间。

Method: MaskLAM是对LAM训练的轻量级修改，通过整合视觉代理分割来解决此问题。该方法利用预训练基础模型的分割掩码来加权LAM重建损失，从而优先考虑显著信息而非背景元素，同时无需架构修改。

Result: 在添加了动作相关背景噪声的连续控制MuJoCo任务上，该方法相比标准基线实现了高达4倍的累积奖励提升，并通过线性探针评估显示潜在动作质量提高了3倍。

Conclusion: MaskLAM通过简单而有效的分割掩码加权策略，显著提升了LAMs在存在背景干扰环境中的性能，为从无标签视频中学习动作表示提供了更鲁棒的解决方案。

Abstract: Latent Action Models (LAMs) learn to extract action-relevant representations solely from raw observations, enabling reinforcement learning from unlabelled videos and significantly scaling available training data. However, LAMs face a critical challenge in disentangling action-relevant features from action-correlated noise (e.g., background motion). Failing to filter these distractors causes LAMs to capture spurious correlations and build sub-optimal latent action spaces. In this paper, we introduce MaskLAM -- a lightweight modification to LAM training to mitigate this issue by incorporating visual agent segmentation. MaskLAM utilises segmentation masks from pretrained foundation models to weight the LAM reconstruction loss, thereby prioritising salient information over background elements while requiring no architectural modifications. We demonstrate the effectiveness of our method on continuous-control MuJoCo tasks, modified with action-correlated background noise. Our approach yields up to a 4x increase in accrued rewards compared to standard baselines and a 3x improvement in the latent action quality, as evidenced by linear probe evaluation.

</details>


### [385] [Learning Markov Decision Processes under Fully Bandit Feedback](https://arxiv.org/abs/2602.02260)
*Zhengjia Zhuo,Anupam Gupta,Viswanath Nagarajan*

Main category: cs.LG

TL;DR: 本文提出了首个针对完全bandit反馈的episodic MDP的高效学习算法，实现了$\widetilde{O}(\sqrt{T})$的遗憾界，在指数依赖于时间步长$\H$的情况下是紧的，并在k项先知不等式等应用中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习假设智能体能够观察到每个访问的状态-动作对及其即时奖励，但实际应用中这种详细反馈往往不现实。本文研究更受限的"完全bandit"反馈模型，智能体只能观察到聚合奖励，无法看到访问的状态-动作对，这更贴近现实应用场景。

Method: 设计了首个针对episodic MDP完全bandit反馈的高效学习算法。算法处理智能体只能获得聚合奖励的极端受限反馈情况，通过创新性的技术手段实现学习效率。

Result: 算法实现了$\widetilde{O}(\sqrt{T})$的遗憾界，遗憾对时间步长$\H$具有指数依赖性（被证明是必要的）。对于"有序"MDP获得了改进的近乎紧的遗憾界，可用于k项先知不等式和顺序定价等经典随机优化问题。

Conclusion: 尽管反馈高度受限，本文算法在k项先知不等式等实际应用中的性能与具有详细状态-动作反馈的最先进学习算法（UCB-VI）相当，证明了完全bandit反馈下高效学习的可行性。

Abstract: A standard assumption in Reinforcement Learning is that the agent observes every visited state-action pair in the associated Markov Decision Process (MDP), along with the per-step rewards. Strong theoretical results are known in this setting, achieving nearly-tight $Θ(\sqrt{T})$-regret bounds. However, such detailed feedback can be unrealistic, and recent research has investigated more restricted settings such as trajectory feedback, where the agent observes all the visited state-action pairs, but only a single \emph{aggregate} reward. In this paper, we consider a far more restrictive ``fully bandit'' feedback model for episodic MDPs, where the agent does not even observe the visited state-action pairs -- it only learns the aggregate reward. We provide the first efficient bandit learning algorithm for episodic MDPs with $\widetilde{O}(\sqrt{T})$ regret. Our regret has an exponential dependence on the horizon length $\H$, which we show is necessary. We also obtain improved nearly-tight regret bounds for ``ordered'' MDPs; these can be used to model classical stochastic optimization problems such as $k$-item prophet inequality and sequential posted pricing. Finally, we evaluate the empirical performance of our algorithm for the setting of $k$-item prophet inequalities; despite the highly restricted feedback, our algorithm's performance is comparable to that of a state-of-art learning algorithm (UCB-VI) with detailed state-action feedback.

</details>


### [386] [Unlocking the Duality between Flow and Field Matching](https://arxiv.org/abs/2602.02261)
*Daniil Shlenskii,Alexander Varlamov,Nazar Buzun,Alexander Korotin*

Main category: cs.LG

TL;DR: 本文揭示了条件流匹配（CFM）与交互场匹配（IFM）之间的对偶关系，证明前向IFM与CFM等价，而一般IFM更具表达力，为两个框架提供了相互借鉴的理论基础。


<details>
  <summary>Details</summary>
Motivation: CFM和IFM都是生成模型框架，但分别从数据空间的条件概率路径和增广空间的物理场出发。这引发了一个基本问题：它们是本质上不同的方法，还是描述相同底层动力学的两种方式？本文旨在澄清这两种框架之间的关系。

Method: 通过构建CFM与前向IFM之间的双射来证明它们的等价性，同时展示一般IFM（包括EFM等）无法在标准CFM中实现，从而证明其更强的表达能力。

Result: 证明了前向IFM与CFM完全等价，而一般IFM比CFM更具表达力。这种对偶关系为前向IFM提供了概率解释，并为CFM带来了新的IFM驱动技术。

Conclusion: CFM和IFM之间存在深刻的对偶关系：前向IFM与CFM等价，而一般IFM更具表达力。这种理解为两个框架提供了相互借鉴的机会，促进了生成模型理论的发展。

Abstract: Conditional Flow Matching (CFM) unifies conventional generative paradigms such as diffusion models and flow matching. Interaction Field Matching (IFM) is a newer framework that generalizes Electrostatic Field Matching (EFM) rooted in Poisson Flow Generative Models (PFGM). While both frameworks define generative dynamics, they start from different objects: CFM specifies a conditional probability path in data space, whereas IFM specifies a physics-inspired interaction field in an augmented data space. This raises a basic question: are CFM and IFM genuinely different, or are they two descriptions of the same underlying dynamics? We show that they coincide for a natural subclass of IFM that we call forward-only IFM. Specifically, we construct a bijection between CFM and forward-only IFM. We further show that general IFM is strictly more expressive: it includes EFM and other interaction fields that cannot be realized within the standard CFM formulation. Finally, we highlight how this duality can benefit both frameworks: it provides a probabilistic interpretation of forward-only IFM and yields novel, IFM-driven techniques for CFM.

</details>


### [387] [Unsupervised Physics-Informed Operator Learning through Multi-Stage Curriculum Training](https://arxiv.org/abs/2602.02264)
*Paolo Marcandelli,Natansh Mathur,Stefano Markidis,Martina Siena,Stefano Mariani*

Main category: cs.LG

TL;DR: 提出多阶段物理信息训练策略和PhIS-FNO模型，通过渐进式边界条件约束和样条傅里叶神经算子，实现无监督物理约束学习，达到接近监督学习的精度。


<details>
  <summary>Details</summary>
Motivation: 传统神经算子需要监督数据，而物理信息神经网络存在收敛不稳定和泛化能力有限的问题。需要一种既能利用物理约束进行无监督学习，又能保持稳定收敛和良好泛化能力的方法。

Method: 提出多阶段物理信息训练策略：1) 渐进式在损失函数中强制边界条件；2) 随后加入内部残差；3) 每个阶段重新初始化优化器作为延续机制。同时提出PhIS-FNO模型，结合傅里叶层和Hermite样条核进行平滑残差评估。

Result: 在标准基准测试中，PhIS-FNO仅使用狭窄边界区域的标注信息，达到了与监督学习相当的精度水平，证明了该方法的有效性。

Conclusion: 分阶段、基于样条的优化策略为物理信息算子学习提供了一个稳健的范式，能够在无监督物理约束下实现高质量的偏微分方程求解。

Abstract: Solving partial differential equations remains a central challenge in scientific machine learning. Neural operators offer a promising route by learning mappings between function spaces and enabling resolution-independent inference, yet they typically require supervised data. Physics-informed neural networks address this limitation through unsupervised training with physical constraints but often suffer from unstable convergence and limited generalization capability. To overcome these issues, we introduce a multi-stage physics-informed training strategy that achieves convergence by progressively enforcing boundary conditions in the loss landscape and subsequently incorporating interior residuals. At each stage the optimizer is re-initialized, acting as a continuation mechanism that restores stability and prevents gradient stagnation. We further propose the Physics-Informed Spline Fourier Neural Operator (PhIS-FNO), combining Fourier layers with Hermite spline kernels for smooth residual evaluation. Across canonical benchmarks, PhIS-FNO attains a level of accuracy comparable to that of supervised learning, using labeled information only along a narrow boundary region, establishing staged, spline-based optimization as a robust paradigm for physics-informed operator learning.

</details>


### [388] [HopFormer: Sparse Graph Transformers with Explicit Receptive Field Control](https://arxiv.org/abs/2602.02268)
*Sanggeon Yun,Raheeb Hassan,Ryozo Masukawa,Sungheon Jeong,Mohsen Imani*

Main category: cs.LG

TL;DR: HopFormer是一种图Transformer，仅通过头特定的n跳掩码稀疏注意力注入结构，无需位置编码或架构修改，在计算成本随掩码稀疏度线性扩展的同时实现竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 挑战图Transformer依赖显式位置/结构编码和密集全局注意力的主流假设，探索更简单高效的替代方案。

Method: 提出HopFormer，通过头特定的n跳掩码稀疏注意力注入图结构，不使用位置编码或架构修改，提供显式可控的感受野和线性计算复杂度。

Result: 在节点级和图级基准测试中达到竞争性或更优性能，发现密集全局注意力通常不必要：小世界属性强的图适合局部注意力，小世界效应弱的图全局注意力收益递减。

Conclusion: 挑战了图Transformer设计的流行假设，强调稀疏控制注意力作为原则性高效替代方案的重要性。

Abstract: Graph Transformers typically rely on explicit positional or structural encodings and dense global attention to incorporate graph topology. In this work, we show that neither is essential. We introduce HopFormer, a graph Transformer that injects structure exclusively through head-specific n-hop masked sparse attention, without the use of positional encodings or architectural modifications. This design provides explicit and interpretable control over receptive fields while enabling genuinely sparse attention whose computational cost scales linearly with mask sparsity. Through extensive experiments on both node-level and graph-level benchmarks, we demonstrate that our approach achieves competitive or superior performance across diverse graph structures. Our results further reveal that dense global attention is often unnecessary: on graphs with strong small-world properties, localized attention yields more stable and consistently high performance, while on graphs with weaker small-world effects, global attention offers diminishing returns. Together, these findings challenge prevailing assumptions in graph Transformer design and highlight sparsity-controlled attention as a principled and efficient alternative.

</details>


### [389] [MoLF: Mixture-of-Latent-Flow for Pan-Cancer Spatial Gene Expression Prediction from Histology](https://arxiv.org/abs/2602.02282)
*Susu Hu,Stefanie Speidel*

Main category: cs.LG

TL;DR: MoLF是一种用于泛癌组织基因组预测的生成模型，通过条件流匹配和混合专家架构，在保持泛癌训练优势的同时有效处理组织异质性，实现了最先进的性能并展示了跨物种的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前的空间转录组学推断方法主要局限于单组织模型，这种碎片化无法利用跨癌症类型的共享生物学原理，并且在数据稀缺场景中应用受限。虽然泛癌训练提供了解决方案，但由此产生的异质性对单一架构构成了挑战。

Method: MoLF采用条件流匹配目标将噪声映射到基因潜在流形，通过混合专家速度场进行参数化。该架构通过动态路由输入到专门的子网络，有效解耦了不同组织模式的优化。

Result: MoLF在泛癌基准测试中建立了新的最先进水平，始终优于专业模型和基础模型基线。此外，MoLF展示了对跨物种数据的零样本泛化能力，表明它捕捉到了基本的、保守的组织分子机制。

Conclusion: MoLF通过创新的混合专家流匹配架构，成功解决了泛癌组织基因组预测中的异质性挑战，不仅实现了优越的性能，还展示了捕捉跨物种保守生物学机制的能力，为可扩展的组织基因组分析提供了有力工具。

Abstract: Inferring spatial transcriptomics (ST) from histology enables scalable histogenomic profiling, yet current methods are largely restricted to single-tissue models. This fragmentation fails to leverage biological principles shared across cancer types and hinders application to data-scarce scenarios. While pan-cancer training offers a solution, the resulting heterogeneity challenges monolithic architectures. To bridge this gap, we introduce MoLF (Mixture-of-Latent-Flow), a generative model for pan-cancer histogenomic prediction. MoLF leverages a conditional Flow Matching objective to map noise to the gene latent manifold, parameterized by a Mixture-of-Experts (MoE) velocity field. By dynamically routing inputs to specialized sub-networks, this architecture effectively decouples the optimization of diverse tissue patterns. Our experiments demonstrate that MoLF establishes a new state-of-the-art, consistently outperforming both specialized and foundation model baselines on pan-cancer benchmarks. Furthermore, MoLF exhibits zero-shot generalization to cross-species data, suggesting it captures fundamental, conserved histo-molecular mechanisms.

</details>


### [390] [Choice-Model-Assisted Q-learning for Delayed-Feedback Revenue Management](https://arxiv.org/abs/2602.02283)
*Owen Shen,Patrick Jaillet*

Main category: cs.LG

TL;DR: 提出了一种结合离散选择模型与强化学习的方法来处理收益管理中延迟反馈问题，通过模型辅助估算延迟的学习目标，在参数变化时能提升鲁棒性，但在模型假设错误时会导致性能下降。


<details>
  <summary>Details</summary>
Motivation: 收益管理中存在延迟反馈问题，大量价值由预订后数天才观察到的客户取消和修改决定，这使得传统强化学习方法难以有效学习。

Method: 提出"选择模型辅助RL"方法：使用校准的离散选择模型作为固定的部分世界模型，在决策时估算延迟的学习目标分量，结合表格Q学习进行训练。

Result: 理论证明：表格Q学习收敛到最优Q函数的O(ε/(1-γ))邻域；实验显示：在平稳环境下与基线无显著差异，在参数变化时5/10场景有显著收益提升（最高12.4%），但在模型假设错误时收益下降1.4-2.6%。

Conclusion: 部分行为模型在参数变化时能提升鲁棒性，但在模型假设错误时会引入有害偏差，这为理解何时使用此类模型提供了指导。

Abstract: We study reinforcement learning for revenue management with delayed feedback, where a substantial fraction of value is determined by customer cancellations and modifications observed days after booking. We propose \emph{choice-model-assisted RL}: a calibrated discrete choice model is used as a fixed partial world model to impute the delayed component of the learning target at decision time. In the fixed-model deployment regime, we prove that tabular Q-learning with model-imputed targets converges to an $O(\varepsilon/(1-γ))$ neighborhood of the optimal Q-function, where $\varepsilon$ summarizes partial-model error, with an additional $O(t^{-1/2})$ sampling term. Experiments in a simulator calibrated from 61{,}619 hotel bookings (1{,}088 independent runs) show: (i) no statistically detectable difference from a maturity-buffer DQN baseline in stationary settings; (ii) positive effects under in-family parameter shifts, with significant gains in 5 of 10 shift scenarios after Holm--Bonferroni correction (up to 12.4\%); and (iii) consistent degradation under structural misspecification, where the choice model assumptions are violated (1.4--2.6\% lower revenue). These results characterize when partial behavioral models improve robustness under shift and when they introduce harmful bias.

</details>


### [391] [Statistical Learning Theory in Lean 4: Empirical Processes from Scratch](https://arxiv.org/abs/2602.02285)
*Yuanhe Zhang,Jason D. Lee,Fanghui Liu*

Main category: cs.LG

TL;DR: 首个基于经验过程理论的统计学习理论完整Lean 4形式化，填补了Mathlib库空白，包含高斯Lipschitz集中性、Dudley熵积分定理等形式化，应用于最小二乘回归，采用人机协作工作流。


<details>
  <summary>Details</summary>
Motivation: 填补Lean 4 Mathlib库在统计学习理论方面的空白，建立可重用的形式化基础，通过形式化过程揭示并解决标准SLT教材中的隐含假设和缺失细节。

Method: 采用人机协作工作流：人类设计证明策略，AI代理执行战术证明构造，最终得到人类验证的Lean 4工具箱。包括高斯Lipschitz集中性、Dudley熵积分定理等形式化开发。

Result: 实现了首个基于经验过程理论的统计学习理论完整Lean 4形式化，填补了Mathlib库空白，应用于最小二乘（稀疏）回归并获得尖锐速率，代码已在GitHub开源。

Conclusion: 这项工作建立了可重用的形式化基础，为机器学习理论的未来发展打开了大门，同时通过形式化过程强化了对理论的逐行理解。

Abstract: We present the first comprehensive Lean 4 formalization of statistical learning theory (SLT) grounded in empirical process theory. Our end-to-end formal infrastructure implement the missing contents in latest Lean 4 Mathlib library, including a complete development of Gaussian Lipschitz concentration, the first formalization of Dudley's entropy integral theorem for sub-Gaussian processes, and an application to least-squares (sparse) regression with a sharp rate. The project was carried out using a human-AI collaborative workflow, in which humans design proof strategies and AI agents execute tactical proof construction, leading to the human-verified Lean 4 toolbox for SLT. Beyond implementation, the formalization process exposes and resolves implicit assumptions and missing details in standard SLT textbooks, enforcing a granular, line-by-line understanding of the theory. This work establishes a reusable formal foundation and opens the door for future developments in machine learning theory. The code is available at https://github.com/YuanheZ/lean-stat-learning-theory

</details>


### [392] [An Optimization Method for Autoregressive Time Series Forecasting](https://arxiv.org/abs/2602.02288)
*Zheng Li,Jerry Cheng,Huanying Gu*

Main category: cs.LG

TL;DR: 提出一种新的时间序列预测训练方法，通过强制自回归预测误差随预测范围增加而增加，并允许模型拼接短期预测形成灵活长期预测，在多个基准测试中达到SOTA


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的时间序列预测模型主要通过扩大模型规模而非真正的自回归展开来实现长期预测，且传统训练过程忽略了时间因果关系

Method: 提出新颖的训练方法，强制两个关键特性：1) 自回归预测误差应随预测范围增加而增加，违反此原则被视为随机猜测并在损失函数中明确惩罚；2) 使模型能够拼接短期自回归预测形成灵活的长期预测

Result: 在多个基准测试中建立新的SOTA，相比iTransformer等强基线MSE降低超过10%，使短期预测模型能够在超过7.5倍长的范围内进行可靠的长期预测

Conclusion: 提出的训练方法通过强制时间因果关系和灵活的预测拼接，显著提升了时间序列预测性能，特别是在长期预测任务上

Abstract: Current time-series forecasting models are primarily based on transformer-style neural networks. These models achieve long-term forecasting mainly by scaling up the model size rather than through genuinely autoregressive (AR) rollout. From the perspective of large language model training, the traditional training process for time-series forecasting models ignores temporal causality. In this paper, we propose a novel training method for time-series forecasting that enforces two key properties: (1) AR prediction errors should increase with the forecasting horizon. Any violation of this principle is considered random guessing and is explicitly penalized in the loss function, and (2) the method enables models to concatenate short-term AR predictions for forming flexible long-term forecasts. Empirical results demonstrate that our method establishes a new state-of-the-art across multiple benchmarks, achieving an MSE reduction of more than 10% compared to iTransformer and other recent strong baselines. Furthermore, it enables short-horizon forecasting models to perform reliable long-term predictions at horizons over 7.5 times longer. Code is available at https://github.com/LizhengMathAi/AROpt

</details>


### [393] [EvalQReason: A Framework for Step-Level Reasoning Evaluation in Large Language Models](https://arxiv.org/abs/2602.02295)
*Shaima Ahmad Freja,Ferhat Ozgur Catak,Betul Yurdem,Chunming Rong*

Main category: cs.LG

TL;DR: 提出了EvalQReason框架，通过分析推理步骤的概率分布来量化LLM的推理质量，无需人工标注，包含CSD和SFC两种算法，在数学和医疗数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: LLM在关键应用中需要可靠的推理能力，但现有方法主要关注最终答案的正确性，难以系统评估推理过程的质量，缺乏对中间步骤推理展开的洞察。

Method: 提出EvalQReason框架，包含两种算法：Consecutive Step Divergence (CSD) 测量相邻推理步骤的局部连贯性，Step-to-Final Convergence (SFC) 评估与最终答案的全局对齐。每种算法使用五个统计指标来捕捉推理动态。

Result: 在数学和医疗数据集上使用开源7B参数模型实验表明：CSD特征在正确性分类中表现优异，传统机器学习模型达到F1=0.78和ROC-AUC=0.82，序列神经网络模型显著提升性能（F1=0.88，ROC-AUC=0.97）。CSD始终优于SFC，序列架构优于传统机器学习方法。推理动态具有领域特异性：数学推理显示清晰的分歧模式，而医疗推理的区分信号较弱。

Conclusion: EvalQReason实现了可扩展的、过程感知的推理可靠性评估，建立了基于概率的分歧分析作为可信AI部署的原则性方法，揭示了LLM处理不同类型推理的根本差异。

Abstract: Large Language Models (LLMs) are increasingly deployed in critical applications requiring reliable reasoning, yet their internal reasoning processes remain difficult to evaluate systematically. Existing methods focus on final-answer correctness, providing limited insight into how reasoning unfolds across intermediate steps. We present EvalQReason, a framework that quantifies LLM reasoning quality through step-level probability distribution analysis without requiring human annotation. The framework introduces two complementary algorithms: Consecutive Step Divergence (CSD), which measures local coherence between adjacent reasoning steps, and Step-to-Final Convergence (SFC), which assesses global alignment with final answers. Each algorithm employs five statistical metrics to capture reasoning dynamics. Experiments across mathematical and medical datasets with open-source 7B-parameter models demonstrate that CSD-based features achieve strong predictive performance for correctness classification, with classical machine learning models reaching F1=0.78 and ROC-AUC=0.82, and sequential neural models substantially improving performance (F1=0.88, ROC-AUC=0.97). CSD consistently outperforms SFC, and sequential architectures outperform classical machine learning approaches. Critically, reasoning dynamics prove domain-specific: mathematical reasoning exhibits clear divergence-based discrimination patterns between correct and incorrect solutions, while medical reasoning shows minimal discriminative signals, revealing fundamental differences in how LLMs process different reasoning types. EvalQReason enables scalable, process-aware evaluation of reasoning reliability, establishing probability-based divergence analysis as a principled approach for trustworthy AI deployment.

</details>


### [394] [Decoupling Generalizability and Membership Privacy Risks in Neural Networks](https://arxiv.org/abs/2602.02296)
*Xingli Fang,Jung-Eun Kim*

Main category: cs.LG

TL;DR: 本文提出了一种新的隐私保护训练原则（PPTP），通过识别深度学习模型中泛化能力和隐私风险存在于不同区域，实现最大化隐私保护的同时最小化泛化能力损失


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在获得某些能力或特性时通常需要牺牲其他效用，隐私保护与模型效用之间存在权衡关系。不同防御方法之间的损失差异表明存在将泛化能力和隐私风险解耦以最大化隐私收益的潜力

Method: 作者识别出深度学习架构中模型的泛化能力和隐私风险存在于不同区域，基于这一观察提出了隐私保护训练原则（PPTP），通过保护模型组件免受隐私风险影响，同时最小化泛化能力的损失

Result: 通过广泛的评估，该方法在增强隐私保护的同时，显著更好地保持了模型的泛化能力

Conclusion: PPTP方法能够有效解耦模型的泛化能力和隐私风险，在最大化隐私保护的同时最小化对模型性能的影响，为解决隐私保护与模型效用之间的权衡问题提供了新思路

Abstract: A deep learning model usually has to sacrifice some utilities when it acquires some other abilities or characteristics. Privacy preservation has such trade-off relationships with utilities. The loss disparity between various defense approaches implies the potential to decouple generalizability and privacy risks to maximize privacy gain. In this paper, we identify that the model's generalization and privacy risks exist in different regions in deep neural network architectures. Based on the observations that we investigate, we propose Privacy-Preserving Training Principle (PPTP) to protect model components from privacy risks while minimizing the loss in generalizability. Through extensive evaluations, our approach shows significantly better maintenance in model generalizability while enhancing privacy preservation.

</details>


### [395] [ReasonCACHE: Teaching LLMs To Reason Without Weight Updates](https://arxiv.org/abs/2602.02366)
*Sharut Gupta,Phillip Isola,Stefanie Jegelka,David Lopez-Paz,Kartik Ahuja,Mark Ibrahim,Mohammad Pezeshki*

Main category: cs.LG

TL;DR: ReasonCACHE通过前缀调优将演示蒸馏到固定键值缓存中，使大语言模型无需权重更新即可学习推理，在效率上优于标准上下文学习，性能匹配或超越权重学习方法。


<details>
  <summary>Details</summary>
Motivation: 标准上下文学习(ICL)在复杂推理任务中需要大量演示样本，但增加演示会导致注意力成本二次增长、性能饱和或下降等问题。权重学习(IWL)虽然有效但需要参数更新。本文旨在寻找一种无需权重更新、能超越上下文窗口限制的中间路径。

Method: 提出ReasonCACHE方法，基于前缀调优将演示样本蒸馏到固定的键值缓存中，直接注入注意力机制，避免了上下文窗口过载和权重更新的需求。

Result: 在GPQA-Diamond等挑战性推理基准测试中，ReasonCACHE优于标准ICL，性能匹配或超越IWL方法，同时在数据效率、推理成本和可训练参数三个维度上更高效。

Conclusion: ReasonCACHE在上下文学习和权重学习之间提供了可扩展的中间路径，无需修改模型参数即可学习超越上下文窗口的推理技能，理论上比低秩权重更新更具表达力。

Abstract: Can Large language models (LLMs) learn to reason without any weight update and only through in-context learning (ICL)? ICL is strikingly sample-efficient, often learning from only a handful of demonstrations, but complex reasoning tasks typically demand many training examples to learn from. However, naively scaling ICL by adding more demonstrations breaks down at this scale: attention costs grow quadratically, performance saturates or degrades with longer contexts, and the approach remains a shallow form of learning. Due to these limitations, practitioners predominantly rely on in-weight learning (IWL) to induce reasoning. In this work, we show that by using Prefix Tuning, LLMs can learn to reason without overloading the context window and without any weight updates. We introduce $\textbf{ReasonCACHE}$, an instantiation of this mechanism that distills demonstrations into a fixed key-value cache. Empirically, across challenging reasoning benchmarks, including GPQA-Diamond, ReasonCACHE outperforms standard ICL and matches or surpasses IWL approaches. Further, it achieves this all while being more efficient across three key axes: data, inference cost, and trainable parameters. We also theoretically prove that ReasonCACHE can be strictly more expressive than low-rank weight update since the latter ties expressivity to input rank, whereas ReasonCACHE bypasses this constraint by directly injecting key-values into the attention mechanism. Together, our findings identify ReasonCACHE as a middle path between in-context and in-weight learning, providing a scalable algorithm for learning reasoning skills beyond the context window without modifying parameters. Our project page: https://reasoncache.github.io/

</details>


### [396] [C-kNN-LSH: A Nearest-Neighbor Algorithm for Sequential Counterfactual Inference](https://arxiv.org/abs/2602.02371)
*Jing Wang,Jie Shen,Qiaomin Xie,Jeremy C Weiss*

Main category: cs.LG

TL;DR: C-kNN-LSH：一种基于局部敏感哈希的最近邻框架，用于从高维、混杂的纵向数据中进行序列因果推断，特别适用于长新冠恢复等临床轨迹分析。


<details>
  <summary>Details</summary>
Motivation: 从纵向轨迹中估计因果效应对于理解复杂疾病进展和优化临床决策（如共病管理和长新冠恢复）至关重要，但面临高维度和混杂因素的挑战。

Method: 提出C-kNN-LSH框架，利用局部敏感哈希高效识别具有相似协变量历史的"临床双胞胎"，实现跨演化疾病状态的局部条件治疗效果估计。结合双重稳健校正来减轻不规则采样和患者恢复曲线变化带来的偏差。

Result: 理论分析保证估计量的一致性和对干扰误差的二阶稳健性。在13,511名参与者的真实世界长新冠队列中，C-kNN-LSH在捕捉恢复异质性和估计策略价值方面优于现有基线方法。

Conclusion: C-kNN-LSH为高维、混杂的纵向临床数据中的序列因果推断提供了一个有效且高效的框架，特别适用于长新冠恢复等复杂疾病轨迹分析。

Abstract: Estimating causal effects from longitudinal trajectories is central to understanding the progression of complex conditions and optimizing clinical decision-making, such as comorbidities and long COVID recovery. We introduce \emph{C-kNN--LSH}, a nearest-neighbor framework for sequential causal inference designed to handle such high-dimensional, confounded situations. By utilizing locality-sensitive hashing, we efficiently identify ``clinical twins'' with similar covariate histories, enabling local estimation of conditional treatment effects across evolving disease states. To mitigate bias from irregular sampling and shifting patient recovery profiles, we integrate neighborhood estimator with a doubly-robust correction.
  Theoretical analysis guarantees our estimator is consistent and second-order robust to nuisance error.
  Evaluated on a real-world Long COVID cohort with 13,511 participants, \emph{C-kNN-LSH} demonstrates superior performance in capturing recovery heterogeneity and estimating policy values compared to existing baselines.

</details>


### [397] [Self-Supervised Learning from Structural Invariance](https://arxiv.org/abs/2602.02381)
*Yipeng Zhang,Hafez Ghaemi,Jungyoon Lee,Shahab Bakhtiari,Eilif B. Muller,Laurent Charlin*

Main category: cs.LG

TL;DR: AdaSSL通过引入潜变量解决SSL中的一对多映射问题，提升对条件不确定性的建模能力


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法难以灵活捕捉数据对之间的条件不确定性，特别是当数据来自自然生成过程（如连续视频帧）时，存在一对多映射问题

Method: 引入潜变量来建模条件不确定性，推导出配对嵌入之间互信息的变分下界，为标准的SSL目标添加简单正则化项

Result: AdaSSL适用于对比式和蒸馏式SSL目标，在因果表示学习、细粒度图像理解和视频世界建模等任务中表现出良好性能

Conclusion: 通过显式建模一对多映射中的条件不确定性，AdaSSL增强了自监督学习方法的灵活性和表达能力

Abstract: Joint-embedding self-supervised learning (SSL), the key paradigm for unsupervised representation learning from visual data, learns from invariances between semantically-related data pairs. We study the one-to-many mapping problem in SSL, where each datum may be mapped to multiple valid targets. This arises when data pairs come from naturally occurring generative processes, e.g., successive video frames. We show that existing methods struggle to flexibly capture this conditional uncertainty. As a remedy, we introduce a latent variable to account for this uncertainty and derive a variational lower bound on the mutual information between paired embeddings. Our derivation yields a simple regularization term for standard SSL objectives. The resulting method, which we call AdaSSL, applies to both contrastive and distillation-based SSL objectives, and we empirically show its versatility in causal representation learning, fine-grained image understanding, and world modeling on videos.

</details>


### [398] [SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization](https://arxiv.org/abs/2602.02383)
*Maksim Afanasyev,Illarion Iov*

Main category: cs.LG

TL;DR: SLIME是一种新的无参考对齐方法，通过解耦偏好学习和生成质量，解决了现有DPO方法中的"遗忘"和"格式化崩溃"问题。


<details>
  <summary>Details</summary>
Motivation: 现有直接偏好优化方法存在目标不匹配问题：优化选择与拒绝响应的相对边际不能保证保留选择响应的绝对似然，导致"遗忘"（高质量输出概率降低）和"格式化崩溃"（拒绝序列过度惩罚）。

Method: SLIME采用三部分目标：(1)锚定项最大化偏好响应似然；(2)稳定惩罚防止拒绝标记概率崩溃为零；(3)结合硬约束和软约束的双边际机制进行精确边界塑造。

Result: SLIME在保持更高生成稳定性的同时，相比最先进基线实现了更优性能。

Conclusion: SLIME通过解耦偏好学习和生成质量，提供了一种更稳定有效的语言模型对齐方法。

Abstract: Direct preference optimization methods have emerged as a computationally efficient alternative to Reinforcement Learning from Human Feedback (RLHF) for aligning Large Language Models (LLMs). Latest approaches have streamlined the alignment process by deriving implicit reward functions, yet they often suffer from a critical objective mismatch: optimizing the relative margin between chosen and rejected responses does not guarantee the preservation of the chosen response's absolute likelihood. This can lead to ``unlearning'', where the model degrades the probability of high-quality outputs to satisfy margin constraints, and ``formatting collapse'' caused by the over-penalization of rejected sequences. In this work, we introduce SLIME (Stabilized Likelihood Implicit Margin Enforcement), a reference-free alignment objective designed to decouple preference learning from generation quality. SLIME incorporates a three-pronged objective: (1) an anchoring term to maximize the likelihood of preferred responses; (2) a stabilizing penalty that prevents the probabilities of rejected tokens from collapsing to zero; and (3) a dual-margin mechanism that combines hard and soft constraints for precise boundary shaping. Our results demonstrate that SLIME achieves superior performance compared to state-of-the-art baselines while maintaining higher generation stability.

</details>


### [399] [Transformers learn factored representations](https://arxiv.org/abs/2602.02385)
*Adam Shai,Loren Amdahl-Culleton,Casper L. Christensen,Henry R. Bigelow,Fernando E. Rosas,Alexander B. Boyd,Eric A. Alt,Kyle J. Ray,Paul M. Riechers*

Main category: cs.LG

TL;DR: Transformer模型倾向于将世界分解为独立因子，在正交子空间中表示，即使这会牺牲预测精度，反映了对因式分解的归纳偏好


<details>
  <summary>Details</summary>
Motivation: 研究Transformer模型如何表示世界中的因子，探索模型是使用维度呈指数增长的乘积空间表示，还是使用维度呈线性增长的正交子空间分解表示

Method: 提出两种表示假设的几何结构预测，在已知潜在结构的合成过程上训练Transformer，测试激活的几何结构、子空间数量和维度

Result: 当因子条件独立时，模型学习分解表示；即使存在噪声或隐藏依赖破坏条件独立性，模型在训练早期仍偏好分解表示，以牺牲预测保真度为代价

Conclusion: Transformer倾向于将世界分解为部分，这种对因式分解的归纳偏好在复杂数据训练中可能持续存在，为模型的可解释低维结构提供了原理性解释

Abstract: Transformers pretrained via next token prediction learn to factor their world into parts, representing these factors in orthogonal subspaces of the residual stream. We formalize two representational hypotheses: (1) a representation in the product space of all factors, whose dimension grows exponentially with the number of parts, or (2) a factored representation in orthogonal subspaces, whose dimension grows linearly. The factored representation is lossless when factors are conditionally independent, but sacrifices predictive fidelity otherwise, creating a tradeoff between dimensional efficiency and accuracy. We derive precise predictions about the geometric structure of activations for each, including the number of subspaces, their dimensionality, and the arrangement of context embeddings within them. We test between these hypotheses on transformers trained on synthetic processes with known latent structure. Models learn factored representations when factors are conditionally independent, and continue to favor them early in training even when noise or hidden dependencies undermine conditional independence, reflecting an inductive bias toward factoring at the cost of fidelity. This provides a principled explanation for why transformers decompose the world into parts, and suggests that interpretable low dimensional structure may persist even in models trained on complex data.

</details>


### [400] [David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning](https://arxiv.org/abs/2602.02395)
*Samuel Nellessen,Tal Kachman*

Main category: cs.LG

TL;DR: 提出Tag-Along攻击威胁模型：无工具权限的攻击者通过对话"搭便车"利用安全对齐的操作员工具权限，实现禁止的工具使用。开发Slingshot强化学习框架自动发现攻击向量，在极端困难任务上成功率67%，显著降低攻击尝试次数。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型向自主代理演进引入了对抗性失效，攻击者可能利用合法工具权限。传统安全评估在工具增强环境中从主观NLP任务转变为客观控制问题，需要形式化威胁模型来评估这种风险。

Method: 提出Tag-Along攻击威胁模型，开发Slingshot"冷启动"强化学习框架，自动发现涌现的攻击向量。框架通过环境交互学习攻击策略，无需预训练数据，专注于发现短指令式语法模式而非多轮说服。

Result: 在极端困难任务上，Slingshot对Qwen2.5-32B-Instruct-AWQ操作员攻击成功率67.0%（基线仅1.7%），首次成功所需尝试次数从52.3降至1.3。零样本迁移到多个模型家族，包括Gemini 2.5 Flash（56.0%）和Meta-SecAlign-8B（39.2%）。

Conclusion: Tag-Along攻击是可验证的一级威胁模型，通过环境交互即可从现成的开源权重模型中引发有效的代理攻击。这表明工具增强环境中的安全评估需要更严格的威胁建模和防御机制。

Abstract: The evolution of large language models into autonomous agents introduces adversarial failures that exploit legitimate tool privileges, transforming safety evaluation in tool-augmented environments from a subjective NLP task into an objective control problem. We formalize this threat model as Tag-Along Attacks: a scenario where a tool-less adversary "tags along" on the trusted privileges of a safety-aligned Operator to induce prohibited tool use through conversation alone. To validate this threat, we present Slingshot, a 'cold-start' reinforcement learning framework that autonomously discovers emergent attack vectors, revealing a critical insight: in our setting, learned attacks tend to converge to short, instruction-like syntactic patterns rather than multi-turn persuasion. On held-out extreme-difficulty tasks, Slingshot achieves a 67.0% success rate against a Qwen2.5-32B-Instruct-AWQ Operator (vs. 1.7% baseline), reducing the expected attempts to first success (on solved tasks) from 52.3 to 1.3. Crucially, Slingshot transfers zero-shot to several model families, including closed-source models like Gemini 2.5 Flash (56.0% attack success rate) and defensive-fine-tuned open-source models like Meta-SecAlign-8B (39.2% attack success rate). Our work establishes Tag-Along Attacks as a first-class, verifiable threat model and shows that effective agentic attacks can be elicited from off-the-shelf open-weight models through environment interaction alone.

</details>


### [401] [An Empirical Study on Noisy Data and LLM Pretraining Loss Divergence](https://arxiv.org/abs/2602.02400)
*Qizhen Zhang,Ankush Garg,Jakob Foerster,Niladri Chatterji,Kshitiz Malik,Mike Lewis*

Main category: cs.LG

TL;DR: 本文通过系统实验研究，发现大规模语言模型预训练中噪声数据确实会导致训练损失发散，发散概率受噪声类型、噪声量和模型规模影响，并提供了区分噪声引发发散与高学习率引发发散的诊断方法。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练数据集不可避免地包含大量噪声数据，虽然研究者推测这些噪声可能导致LLM预训练不稳定甚至损失发散，但这一现象尚未得到充分理解。本文旨在系统研究噪声数据是否以及如何导致LLM预训练发散。

Method: 通过在干净数据集中注入受控的合成均匀随机噪声，分析从480M到5.2B参数规模的模型训练动态。研究噪声类型、噪声量和模型规模对训练发散的影响，并比较噪声引发发散与高学习率引发发散的激活模式差异。

Result: 噪声数据确实会引发训练损失发散，发散概率强烈依赖于噪声类型、噪声量和模型规模。噪声引发的发散表现出与高学习率引发发散不同的激活模式，研究提供了区分这两种失败模式的诊断方法。

Conclusion: 本研究首次提供了大规模、受控的关于噪声数据如何影响LLM预训练损失发散的系统表征，为理解预训练中的不稳定现象提供了实证基础。

Abstract: Large-scale pretraining datasets drive the success of large language models (LLMs). However, these web-scale corpora inevitably contain large amounts of noisy data due to unregulated web content or randomness inherent in data. Although LLM pretrainers often speculate that such noise contributes to instabilities in large-scale LLM pretraining and, in the worst cases, loss divergence, this phenomenon remains poorly understood.In this work, we present a systematic empirical study of whether noisy data causes LLM pretraining divergences and how it does so. By injecting controlled synthetic uniformly random noise into otherwise clean datasets, we analyze training dynamics across model sizes ranging from 480M to 5.2B parameters. We show that noisy data indeed induces training loss divergence, and that the probability of divergence depends strongly on the noise type, amount of noise, and model scale. We further find that noise-induced divergences exhibit activation patterns distinct from those caused by high learning rates, and we provide diagnostics that differentiate these two failure modes. Together, these results provide a large-scale, controlled characterization of how noisy data affects loss divergence in LLM pretraining.

</details>


### [402] [Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning](https://arxiv.org/abs/2602.02405)
*Ethan Mendes,Jungsoo Park,Alan Ritter*

Main category: cs.LG

TL;DR: DAIL方法通过将专家解决方案转化为详细推理轨迹，再使用对比学习目标，仅用少量专家数据就能显著提升LLM的推理能力


<details>
  <summary>Details</summary>
Motivation: 当前提升LLM推理能力的方法存在局限：要么依赖模型能采样到正确解来强化学习，要么需要更强的模型来解决问题。许多难题对现有前沿模型仍然难以解决，无法提取有效的训练信号。专家解决方案虽然质量高但成本昂贵，且直接模仿会失败，因为专家解决方案是为人类设计的，包含隐含的推理跳跃，与模型分布不匹配。

Method: 提出分布对齐模仿学习（DAIL），包含两个步骤：1）将专家解决方案转化为详细、分布内的推理轨迹，弥合分布差距；2）应用对比学习目标，专注于学习专家的见解和方法论。

Result: DAIL仅使用不到1000个高质量专家解决方案，就能在Qwen2.5-Instruct和Qwen3模型上实现10-25%的pass@k提升，推理效率提高2-4倍，并具备跨领域泛化能力。

Conclusion: DAIL提供了一种样本高效的训练方法，能够有效利用少量高质量专家数据来显著提升LLM的推理能力，解决了专家解决方案分布不匹配的问题，为难以解决的问题提供了可行的训练信号获取途径。

Abstract: Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization.

</details>


### [403] [Active Transfer Bagging: A New Approach for Accelerated Active Learning Acquisition of Data by Combined Transfer Learning and Bagging Based Models](https://arxiv.org/abs/2602.02415)
*Vivienne Pelletier,Daniel J. Rivera,Obinna Nwokonkwo,Steven A. Wilson,Christopher L. Muhich*

Main category: cs.LG

TL;DR: ATBagging是一种新的主动学习种子集选择方法，通过贝叶斯集成模型估计数据点信息量，结合DPP多样性采样，在低数据场景下显著提升主动学习早期性能。


<details>
  <summary>Details</summary>
Motivation: 主动学习虽然能降低标注成本，但其早期性能通常受随机选择的初始种子集影响。许多应用场景中存在相关数据集，可用于构建更好的种子集来提升主动学习效果。

Method: 提出Active-Transfer Bagging方法：1) 使用袋装集成模型的贝叶斯解释，通过比较袋内和袋外预测分布来估计候选数据点的信息量；2) 采用确定性点过程结合随机傅里叶特征和质量-多样性分解来保证特征空间多样性，避免冗余选择。

Result: 在四个真实数据集（QM9、ERA5、Forbes 2000、Beijing PM2.5）上测试，种子规模10-100时，ATBagging在几乎所有情况下都改善或持平早期主动学习性能，提高了学习曲线下面积，在低数据场景中效果最显著。

Conclusion: ATBagging提供了一种低成本、高回报的主动学习数据收集启动方法，特别适用于目标迁移和特征偏移场景，能有效利用相关数据集构建优质种子集。

Abstract: Modern machine learning has achieved remarkable success on many problems, but this success often depends on the existence of large, labeled datasets. While active learning can dramatically reduce labeling cost when annotations are expensive, early performance is frequently dominated by the initial seed set, typically chosen at random. In many applications, however, related or approximate datasets are readily available and can be leveraged to construct a better seed set. We introduce a new method for selecting the seed data set for active learning, Active-Transfer Bagging (ATBagging). ATBagging estimates the informativeness of candidate data point from a Bayesian interpretation of bagged ensemble models by comparing in-bag and out-of-bag predictive distributions from the labeled dataset, yielding an information-gain proxy. To avoid redundant selections, we impose feature-space diversity by sampling a determinantal point process (DPP) whose kernel uses Random Fourier Features and a quality-diversity factorization that incorporates the informativeness scores. This same blended method is used for selection of new data points to collect during the active learning phase. We evaluate ATBagging on four real-world datasets covering both target-transfer and feature-shift scenarios (QM9, ERA5, Forbes 2000, and Beijing PM2.5). Across seed sizes nseed = 10-100, ATBagging improves or ties early active learning and increases area under the learning-curve relative to alternative seed subset selection methodologies in almost all cases, with strongest benefits in low-data regimes. Thus, ATBagging provides a low-cost, high reward means to initiating active learning-based data collection.

</details>


### [404] [Trust Region Continual Learning as an Implicit Meta-Learner](https://arxiv.org/abs/2602.02417)
*Zekun Wang,Anant Gupta,Christopher J. MacLellan*

Main category: cs.LG

TL;DR: 提出信任区域持续学习方法，结合生成回放与Fisher度量信任区域约束，在局部近似下获得类似MAML的元学习特性，实现任务快速重收敛。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法存在核心权衡：正则化方法（如EWC）在任务最优解重叠度低时可能过度约束更新，而回放方法虽能保持性能但会因不完美回放而漂移。需要结合两者优势的混合方法。

Method: 提出信任区域持续学习方法，结合生成回放与Fisher度量信任区域约束。在局部近似下，该方法更新具有类似MAML的解释：回放提供旧任务梯度信号（类似查询），Fisher加权惩罚提供高效的离线曲率整形（类似支持）。

Result: 在任务增量扩散图像生成和持续扩散策略控制任务上，信任区域持续学习方法获得最佳最终性能和保留率，并比EWC、回放和持续元学习基线更快恢复早期任务性能。

Conclusion: 该方法在持续学习中展现出涌现的元学习特性：模型成为初始化点，在每次任务转换后能快速重新收敛到先前任务最优解，无需显式优化双层目标。为持续学习提供了新的混合视角。

Abstract: Continual learning aims to acquire tasks sequentially without catastrophic forgetting, yet standard strategies face a core tradeoff: regularization-based methods (e.g., EWC) can overconstrain updates when task optima are weakly overlapping, while replay-based methods can retain performance but drift due to imperfect replay. We study a hybrid perspective: \emph{trust region continual learning} that combines generative replay with a Fisher-metric trust region constraint. We show that, under local approximations, the resulting update admits a MAML-style interpretation with a single implicit inner step: replay supplies an old-task gradient signal (query-like), while the Fisher-weighted penalty provides an efficient offline curvature shaping (support-like). This yields an emergent meta-learning property in continual learning: the model becomes an initialization that rapidly \emph{re-converges} to prior task optima after each task transition, without explicitly optimizing a bilevel objective. Empirically, on task-incremental diffusion image generation and continual diffusion-policy control, trust region continual learning achieves the best final performance and retention, and consistently recovers early-task performance faster than EWC, replay, and continual meta-learning baselines.

</details>


### [405] [Poly-attention: a general scheme for higher-order self-attention](https://arxiv.org/abs/2602.02422)
*Sayak Chakrabarti,Toniann Pitassi,Josh Alman*

Main category: cs.LG

TL;DR: 本文提出了一类称为"多注意力机制"的自注意力泛化方法，能够处理任意高阶张量计算和输入标记间的任意关系结构，系统研究了其计算复杂度和表示能力，并提出了新的二次时间可计算的注意力机制。


<details>
  <summary>Details</summary>
Motivation: 传统自注意力机制虽然能有效建模标记间的成对交互，但无法处理涉及三个或更多相关标记的基本任务，也无法完成需要引用多个输入标记的组合任务。现有高阶注意力替代方案虽然能处理某些多元任务，但计算复杂度高（超二次时间）。

Method: 定义了一类广泛的自注意力泛化机制——多注意力机制，能够纳入任意高阶张量计算和输入标记间的任意关系结构。系统研究这些机制的计算复杂度和表示能力，包括给出新算法和匹配的计算复杂度下界，并确定它们能执行哪些多元任务。

Result: 提出了新的注意力机制，可以在二次时间内精确计算，并能执行任意固定数量函数的函数组合。而先前机制即使只组合两个函数也需要超二次时间，且新的下界表明更快的算法是不可能的。研究结果揭示了这些机制在不同需求之间的权衡关系。

Conclusion: 多注意力机制为Transformer模型提供了更强大的表示能力，能够处理传统自注意力无法完成的多元任务。研究在表达能力和计算效率之间建立了紧密的权衡关系，为设计更高效的注意力机制提供了理论指导。

Abstract: The self-attention mechanism, at the heart of the Transformer model, is able to effectively model pairwise interactions between tokens. However, numerous recent works have shown that it is unable to perform basic tasks involving detecting triples of correlated tokens, or compositional tasks where multiple input tokens need to be referenced to generate a result. Some higher-dimensional alternatives to self-attention have been proposed to address this, including higher-order attention and Strassen attention, which can perform some of these polyadic tasks in exchange for slower, superquadratic running times.
  In this work, we define a vast class of generalizations of self-attention, which we call poly-attention mechanisms. Our mechanisms can incorporate arbitrary higher-order (tensor) computations as well as arbitrary relationship structures between the input tokens, and they include the aforementioned alternatives as special cases. We then systematically study their computational complexity and representational strength, including giving new algorithms and matching complexity-theoretic lower bounds on the time complexity of computing the attention matrix exactly as well as approximately, and tightly determining which polyadic tasks they can each perform. Our results give interesting trade-offs between different desiderata for these mechanisms, including a tight relationship between how expressive a mechanism is, and how large the coefficients in the model may be so that the mechanism can be approximated in almost-linear time.
  Notably, we give a new attention mechanism which can be computed exactly in quadratic time, and which can perform function composition for any fixed number of functions. Prior mechanisms, even for just composing two functions, could only be computed in superquadratic time, and our new lower bounds show that faster algorithms for them are not possible.

</details>


### [406] [Repurposing Protein Language Models for Latent Flow-Based Fitness Optimization](https://arxiv.org/abs/2602.02425)
*Amaru Caceres Arroyo,Lea Bogensperger,Ahmed Allam,Michael Krauthammer,Konrad Schindler,Dominik Narnhofer*

Main category: cs.LG

TL;DR: CHASE：利用预训练蛋白质语言模型的进化知识，通过压缩嵌入到紧凑潜在空间，结合条件流匹配和分类器无关引导，直接生成高适应性蛋白质变体，无需预测器引导的ODE采样步骤。


<details>
  <summary>Details</summary>
Motivation: 蛋白质适应性优化面临巨大组合空间挑战，高适应性变体极其稀疏。现有方法要么性能不足，要么需要计算昂贵的基于梯度的采样。

Method: CHASE框架重新利用预训练蛋白质语言模型的进化知识，将其嵌入压缩到紧凑潜在空间。通过训练带有分类器无关引导的条件流匹配模型，在ODE采样步骤中无需预测器引导即可直接生成高适应性变体。

Result: 在AAV和GFP蛋白质设计基准测试中达到最先进性能。在数据受限设置中，使用合成数据进行引导可以进一步提升性能。

Conclusion: CHASE通过有效利用预训练蛋白质语言模型的进化知识，提供了一种高效、无需预测器引导的蛋白质适应性优化方法，在蛋白质设计任务中表现出色。

Abstract: Protein fitness optimization is challenged by a vast combinatorial landscape where high-fitness variants are extremely sparse. Many current methods either underperform or require computationally expensive gradient-based sampling. We present CHASE, a framework that repurposes the evolutionary knowledge of pretrained protein language models by compressing their embeddings into a compact latent space. By training a conditional flow-matching model with classifier-free guidance, we enable the direct generation of high-fitness variants without predictor-based guidance during the ODE sampling steps. CHASE achieves state-of-the-art performance on AAV and GFP protein design benchmarks. Finally, we show that bootstrapping with synthetic data can further enhance performance in data-constrained settings.

</details>


### [407] [Embedding Perturbation may Better Reflect the Uncertainty in LLM Reasoning](https://arxiv.org/abs/2602.02427)
*Qihao Wen,Jiahao Wang,Yang Nan,Pengfei He,Ravi Tandon,Han Xu*

Main category: cs.LG

TL;DR: 论文提出一种基于扰动敏感性的不确定性量化方法，用于识别LLM推理过程中的错误中间步骤，相比传统方法效果更好且更高效。


<details>
  <summary>Details</summary>
Motivation: LLM在推理任务中可能产生不可靠输出，需要不确定性量化来识别问题。现有方法主要关注最终答案的不确定性，但中间推理步骤的不确定性量化对于精细干预同样重要。

Method: 提出基于扰动敏感性的不确定性量化指标：通过对前一个token的嵌入进行微小扰动，观察后续token的敏感性，错误推理步骤中的token对扰动更敏感。

Result: 实验表明，基于扰动的指标在不确定性量化性能上优于基线方法（如token生成概率和token熵），且相比需要多次采样的方法更简单高效。

Conclusion: 扰动敏感性是量化LLM推理过程中中间步骤不确定性的有效指标，能够识别错误推理步骤，为更精细的干预提供指导。

Abstract: Large language Models (LLMs) have achieved significant breakthroughs across diverse domains; however, they can still produce unreliable or misleading outputs. For responsible LLM application, Uncertainty Quantification (UQ) techniques are used to estimate a model's uncertainty about its outputs, indicating the likelihood that those outputs may be problematic. For LLM reasoning tasks, it is essential to estimate the uncertainty not only for the final answer, but also for the intermediate steps of the reasoning, as this can enable more fine-grained and targeted interventions. In this study, we explore what UQ metrics better reflect the LLM's ``intermediate uncertainty''during reasoning. Our study reveals that an LLMs' incorrect reasoning steps tend to contain tokens which are highly sensitive to the perturbations on the preceding token embeddings. In this way, incorrect (uncertain) intermediate steps can be readily identified using this sensitivity score as guidance in practice. In our experiments, we show such perturbation-based metric achieves stronger uncertainty quantification performance compared with baseline methods such as token (generation) probability and token entropy. Besides, different from approaches that rely on multiple sampling, the perturbation-based metrics offer better simplicity and efficiency.

</details>


### [408] [Maximizing Reliability with Bayesian Optimization](https://arxiv.org/abs/2602.02432)
*Jack M. Buckingham,Ivo Couckuyt,Juergen Branke*

Main category: cs.LG

TL;DR: 提出两种基于Thompson采样和知识梯度的贝叶斯优化方法，用于极小失效概率（10^-6-10^-8）的可靠性优化问题，结合重要性采样提升效率


<details>
  <summary>Details</summary>
Motivation: 制造业中需要优化设计的可靠性（最小化失效概率），但失效概率极低（10^-6-10^-8），传统贝叶斯优化方法难以有效处理这种极端罕见事件

Method: 提出两种贝叶斯优化方法：1）基于Thompson采样；2）基于知识梯度（近似最小化失效概率对数的一步贝叶斯最优策略）。两种方法都结合重要性采样来处理极小的失效概率

Result: 实验结果表明，所提出的方法在极端和非极端失效概率情况下都优于现有方法

Conclusion: 提出的基于Thompson采样和知识梯度的贝叶斯优化方法，结合重要性采样，能够有效处理极小失效概率的可靠性优化问题，在极端和非极端情况下均表现优异

Abstract: Bayesian optimization (BO) is a popular, sample-efficient technique for expensive, black-box optimization. One such problem arising in manufacturing is that of maximizing the reliability, or equivalently minimizing the probability of a failure, of a design which is subject to random perturbations - a problem that can involve extremely rare failures ($P_\mathrm{fail} = 10^{-6}-10^{-8}$). In this work, we propose two BO methods based on Thompson sampling and knowledge gradient, the latter approximating the one-step Bayes-optimal policy for minimizing the logarithm of the failure probability. Both methods incorporate importance sampling to target extremely small failure probabilities. Empirical results show the proposed methods outperform existing methods in both extreme and non-extreme regimes.

</details>


### [409] [Certain Head, Uncertain Tail: Expert-Sample for Test-Time Scaling in Fine-Grained MoE](https://arxiv.org/abs/2602.02443)
*Yuanteng Chen,Peisong Wang,Nanxin Zeng,Yuantian Shao,Gang Li,Jing Liu,Jian Cheng*

Main category: cs.LG

TL;DR: Expert-Sample：一种无需训练的方法，通过在高置信度专家选择中保持确定性，在不确定尾部注入可控随机性，提升细粒度MoE模型的多样性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有测试时缩放方法需要温度调优来平衡多样性与稳定性，而细粒度MoE的丰富路由空间提供了未探索的替代方案。研究发现MoE路由模式显示高置信度专家头部和低置信度候选尾部，前者决定核心推理能力，后者与推理多样性相关。

Method: 提出Expert-Sample方法：保持高置信度专家选择不变，在不确定尾部注入可控随机性。该方法无需训练，通过利用MoE路由的置信度分布模式，实现多样化生成而不破坏输出稳定性。

Result: 在多个细粒度MoE模型和数学、知识推理、代码任务上，Expert-Sample持续提升pass@n和基于验证的准确性。在Qwen3-30B-A3B-Instruct模型上，GPQA-Diamond任务的pass@32从85.4%提升到91.9%，Best-of-N验证准确率从59.1%提升到62.6%。

Conclusion: 细粒度MoE的路由模式揭示了核心推理能力与多样性的分离，Expert-Sample方法有效利用这一特性，在保持稳定性的同时提升生成多样性，为MoE模型的测试时优化提供了新方向。

Abstract: Test-time scaling improves LLM performance by generating multiple candidate solutions, yet token-level sampling requires temperature tuning that trades off diversity against stability. Fine-grained MoE, featuring hundreds of well-trained experts per layer and multi-expert activation per token, offers an unexplored alternative through its rich routing space. We empirically characterize fine-grained MoE routing and uncover an informative pattern: router scores exhibit a certain head of high-confidence experts followed by an uncertain tail of low-confidence candidates. While single-run greedy accuracy remains stable when fewer experts are activated, multi-sample pass@n degrades significantly-suggesting that the certain head governs core reasoning capability while the uncertain tail correlates with reasoning diversity. Motivated by these findings, we propose Expert-Sample, a training-free method that preserves high-confidence selections while injecting controlled stochasticity into the uncertain tail, enabling diverse generation without destabilizing outputs. Evaluated on multiple fine-grained MoE models across math, knowledge reasoning, and code tasks, Expert-Sample consistently improves pass@n and verification-based accuracy. On Qwen3-30B-A3B-Instruct evaluated on GPQA-Diamond with 32 parallel samples, pass@32 rises from 85.4% to 91.9%, and accuracy improves from 59.1% to 62.6% with Best-of-N verification.

</details>


### [410] [Finite-Sample Wasserstein Error Bounds and Concentration Inequalities for Nonlinear Stochastic Approximation](https://arxiv.org/abs/2602.02445)
*Seo Taek Kong,R. Srikant*

Main category: cs.LG

TL;DR: 论文为非线性和线性随机逼近算法在Wasserstein-p距离下推导了非渐近误差界，包括最后迭代和Polyak-Ruppert平均的收敛速率分析


<details>
  <summary>Details</summary>
Motivation: 现有随机逼近算法的有限样本分析通常基于矩界和马尔可夫不等式，缺乏精确的分布保证。本文旨在为非线性随机逼近算法提供明确的有限样本保证，特别是在Wasserstein距离下的分布收敛速率

Method: 采用耦合方法将离散时间过程与极限Ornstein-Uhlenbeck过程比较，分析最后迭代的收敛性；同时直接分析Polyak-Ruppert平均的收敛速率。方法适用于一般噪声条件，包括鞅差和遍历马尔可夫链函数

Result: 在驱动噪声满足非渐近中心极限定理的条件下，证明归一化最后迭代以γ_n^{1/6}速率在p-Wasserstein距离下收敛到高斯分布；Polyak-Ruppert平均以n^{-1/6}速率收敛。这些分布保证导出了比矩界更好的高概率浓度不等式

Conclusion: 本文为随机逼近算法提供了严格的有限样本分布保证，填补了有限样本分析与渐近理论之间的空白。在线性随机逼近中量化了从重尾到高斯行为的转变，在随机梯度下降中建立了到中心极限定理的收敛速率

Abstract: This paper derives non-asymptotic error bounds for nonlinear stochastic approximation algorithms in the Wasserstein-$p$ distance. To obtain explicit finite-sample guarantees for the last iterate, we develop a coupling argument that compares the discrete-time process to a limiting Ornstein-Uhlenbeck process. Our analysis applies to algorithms driven by general noise conditions, including martingale differences and functions of ergodic Markov chains. Complementing this result, we handle the convergence rate of the Polyak-Ruppert average through a direct analysis that applies under the same general setting.
  Assuming the driving noise satisfies a non-asymptotic central limit theorem, we show that the normalized last iterates converge to a Gaussian distribution in the $p$-Wasserstein distance at a rate of order $γ_n^{1/6}$, where $γ_n$ is the step size. Similarly, the Polyak-Ruppert average is shown to converge in the Wasserstein distance at a rate of order $n^{-1/6}$. These distributional guarantees imply high-probability concentration inequalities that improve upon those derived from moment bounds and Markov's inequality. We demonstrate the utility of this approach by considering two applications: (1) linear stochastic approximation, where we explicitly quantify the transition from heavy-tailed to Gaussian behavior of the iterates, thereby bridging the gap between recent finite-sample analyses and asymptotic theory and (2) stochastic gradient descent, where we establish rate of convergence to the central limit theorem.

</details>


### [411] [Active Causal Experimentalist (ACE): Learning Intervention Strategies via Direct Preference Optimization](https://arxiv.org/abs/2602.02451)
*Patrick Cooper,Alvaro Velasquez*

Main category: cs.LG

TL;DR: ACE使用基于偏好的强化学习学习自适应实验设计策略，在因果发现中比传统方法显著提升效率


<details>
  <summary>Details</summary>
Motivation: 传统因果实验设计方法（随机采样、贪婪信息最大化、轮询覆盖）无法从经验中学习自适应策略，每个决策孤立处理，效率低下

Method: 提出Active Causal Experimentalist (ACE)，将实验设计视为序列策略学习问题。利用相对比较比绝对信息增益更稳定的特性，采用Direct Preference Optimization从干预对比较中学习，而非依赖非平稳的奖励幅度

Result: 在合成基准、物理模拟和经济数据上，ACE在相同干预预算下比基线方法提升70-71%（p < 0.001，Cohen's d ~ 2）。学习到的策略自主发现了对碰撞机制需要集中干预父变量的理论策略

Conclusion: 基于偏好的学习能够恢复原则性实验策略，通过经验学习补充理论，实现领域自适应

Abstract: Discovering causal relationships requires controlled experiments, but experimentalists face a sequential decision problem: each intervention reveals information that should inform what to try next. Traditional approaches such as random sampling, greedy information maximization, and round-robin coverage treat each decision in isolation, unable to learn adaptive strategies from experience. We propose Active Causal Experimentalist (ACE), which learns experimental design as a sequential policy. Our key insight is that while absolute information gains diminish as knowledge accumulates (making value-based RL unstable), relative comparisons between candidate interventions remain meaningful throughout. ACE exploits this via Direct Preference Optimization, learning from pairwise intervention comparisons rather than non-stationary reward magnitudes. Across synthetic benchmarks, physics simulations, and economic data, ACE achieves 70-71% improvement over baselines at equal intervention budgets (p < 0.001, Cohen's d ~ 2). Notably, the learned policy autonomously discovers that collider mechanisms require concentrated interventions on parent variables, a theoretically-grounded strategy that emerges purely from experience. This suggests preference-based learning can recover principled experimental strategies, complementing theory with learned domain adaptation.

</details>


### [412] [Conflict-Aware Client Selection for Multi-Server Federated Learning](https://arxiv.org/abs/2602.02458)
*Mingwei Hong,Zheng Lin,Zehang Lin,Lin Li,Miao Yang,Xia Du,Zihan Fang,Zhaolu Kang,Dianxin Luan,Shunzhi Zhu*

Main category: cs.LG

TL;DR: 提出RL-CRP框架，使用去中心化强化学习和冲突风险预测来优化多服务器联邦学习中的客户端选择，减少服务器间冲突并提高训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统单服务器联邦学习存在高通信延迟问题，而多服务器联邦学习虽然能分布工作负载，但客户端覆盖重叠和选择不协调会导致资源竞争、带宽冲突和训练失败。

Method: 提出RL-CRP框架：1) 每个服务器使用基于稀疏历史客户端选择序列的分类隐马尔可夫模型预测客户端选择冲突的可能性；2) 结合公平感知奖励机制促进客户端长期参与，以最小化训练延迟和资源竞争。

Result: 大量实验表明，RL-CRP框架能有效减少服务器间冲突，并在收敛速度和通信成本方面显著提高训练效率。

Conclusion: RL-CRP框架通过冲突风险预测和公平感知奖励机制，成功解决了多服务器联邦学习中的客户端选择协调问题，提高了系统的整体性能。

Abstract: Federated learning (FL) has emerged as a promising distributed machine learning (ML) that enables collaborative model training across clients without exposing raw data, thereby preserving user privacy and reducing communication costs. Despite these benefits, traditional single-server FL suffers from high communication latency due to the aggregation of models from a large number of clients. While multi-server FL distributes workloads across edge servers, overlapping client coverage and uncoordinated selection often lead to resource contention, causing bandwidth conflicts and training failures. To address these limitations, we propose a decentralized reinforcement learning with conflict risk prediction, named RL CRP, to optimize client selection in multi-server FL systems. Specifically, each server estimates the likelihood of client selection conflicts using a categorical hidden Markov model based on its sparse historical client selection sequence. Then, a fairness-aware reward mechanism is incorporated to promote long-term client participation for minimizing training latency and resource contention. Extensive experiments demonstrate that the proposed RL-CRP framework effectively reduces inter-server conflicts and significantly improves training efficiency in terms of convergence speed and communication cost.

</details>


### [413] [SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning](https://arxiv.org/abs/2602.02472)
*Qifan Yu,Xinyu Ma,Zhijian Zhuo,Minrui Wang,Deyi Liu,Shiyi Zhan,Yiyuan Ma,Liang Xiang,Xingyan Bin,Di He*

Main category: cs.LG

TL;DR: SPARKLING是一个用于模型宽度渐进学习的新框架，通过RMS尺度一致性和非对称优化器状态重置，解决了中阶段宽度扩展时的训练不稳定问题，在MoE模型上实现了35%的训练成本节省。


<details>
  <summary>Details</summary>
Motivation: 渐进学习虽然能减少预训练计算开销，但现有研究主要集中在深度扩展，宽度扩展研究不足，特别是中阶段宽度扩展存在严重训练不稳定问题，这阻碍了最大化计算节省。

Method: 提出SPARKLING框架：1）通过RMS尺度一致性保持信号稳定，防止激活统计量破坏；2）通过非对称优化器状态重置和学习率重新预热来打破梯度对称性，促进特征多样性。

Result: 在多种宽度轴和优化器家族上，SPARKLING始终优于从头训练的方法，在2倍宽度扩展下最多减少35%的训练成本，在Mixture-of-Experts模型上验证了有效性。

Conclusion: SPARKLING成功解决了中阶段宽度扩展的挑战，实现了稳定的渐进学习，为高效模型扩展提供了新方法，显著降低了大规模模型预训练的计算成本。

Abstract: Progressive Learning (PL) reduces pre-training computational overhead by gradually increasing model scale. While prior work has extensively explored depth expansion, width expansion remains significantly understudied, with the few existing methods limited to the early stages of training. However, expanding width during the mid-stage is essential for maximizing computational savings, yet it remains a formidable challenge due to severe training instabilities. Empirically, we show that naive initialization at this stage disrupts activation statistics, triggering loss spikes, while copy-based initialization introduces gradient symmetry that hinders feature diversity. To address these issues, we propose SPARKLING (balancing {S}ignal {P}reservation {A}nd symmet{R}y brea{K}ing for width-progressive {L}earn{ING}), a novel framework for mid-stage width expansion. Our method achieves signal preservation via RMS-scale consistency, stabilizing activation statistics during expansion. Symmetry breaking is ensured through asymmetric optimizer state resetting and learning rate re-warmup. Extensive experiments on Mixture-of-Experts (MoE) models demonstrate that, across multiple width axes and optimizer families, SPARKLING consistently outperforms training from scratch and reduces training cost by up to 35% under $2\times$ width expansion.

</details>


### [414] [Expanding the Capabilities of Reinforcement Learning via Text Feedback](https://arxiv.org/abs/2602.02482)
*Yuda Song,Lili Chen,Fahim Tajwar,Remi Munos,Deepak Pathak,J. Andrew Bagnell,Aarti Singh,Andrea Zanette*

Main category: cs.LG

TL;DR: 论文提出RLTF框架，利用文本反馈作为介于稀疏奖励和完整演示之间的中间监督信号，通过两种方法（自我蒸馏和反馈建模）让模型内化反馈以提升单轮推理性能。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法使用稀疏的二元奖励信息，而蒸馏需要昂贵的完整演示。文本反馈提供了比标量奖励更丰富、比演示更便宜的中间监督信号，且在实际场景中已大量存在。

Method: 提出RLTF框架，包含两种方法：1) RLTF-SD：训练单轮策略匹配自身反馈条件下的第二轮生成；2) RLTF-FM：将预测反馈作为辅助目标。两种方法都让模型在训练时利用文本反馈，但在推理时无需反馈。

Result: 在推理谜题、竞赛数学和创意写作任务上的实验表明，两种方法均显著优于强基线，证明了利用丰富文本反馈进行RL的潜力。

Conclusion: 文本反馈作为中间监督信号能有效提升LLM性能，RLTF框架为利用大规模文本反馈提供了可行方案，在多种任务上展现出优于传统方法的性能。

Abstract: The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete demonstrations. Textual feedback is a natural mode of human interaction and is already abundant in many real-world settings, where users, annotators, and automated judges routinely critique LLM outputs. Towards leveraging text feedback at scale, we formalize a multi-turn RL setup, RL from Text Feedback (RLTF), where text feedback is available during training but not at inference. Therefore, models must learn to internalize the feedback in order to improve their test-time single-turn performance. To do this, we propose two methods: Self Distillation (RLTF-SD), which trains the single-turn policy to match its own feedback-conditioned second-turn generations; and Feedback Modeling (RLTF-FM), which predicts the feedback as an auxiliary objective. We provide theoretical analysis on both methods, and empirically evaluate on reasoning puzzles, competition math, and creative writing tasks. Our results show that both methods consistently outperform strong baselines across benchmarks, highlighting the potential of RL with an additional source of rich supervision at scale.

</details>


### [415] [RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System](https://arxiv.org/abs/2602.02488)
*Yinjie Wang,Tianbao Xie,Ke Shen,Mengdi Wang,Ling Yang*

Main category: cs.LG

TL;DR: RLAnything是一个强化学习框架，通过闭环优化动态构建环境、策略和奖励模型，增强LLM和智能体场景的学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在LLM和智能体场景中面临学习信号弱、训练效率低的问题，需要一种能动态优化整个RL系统的框架来提升学习效果。

Method: 1) 策略通过步进信号和结果信号的集成反馈进行训练；2) 奖励模型通过一致性反馈联合优化；3) 基于理论的自动环境适应利用批评反馈改进奖励和策略模型训练。

Result: 每个组件都持续改进整体系统，RLAnything在多个代表性任务上取得显著提升：Qwen3-VL-8B-Thinking在OSWorld上提升9.1%，Qwen2.5-7B-Instruct在AlfWorld和LiveBench上分别提升18.7%和11.9%。优化后的奖励模型信号优于依赖人工标签的结果。

Conclusion: RLAnything通过闭环优化环境、策略和奖励模型，有效增强了强化学习系统的整体性能，在LLM和智能体任务中展现出显著优势。

Abstract: We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL

</details>


### [416] [MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training](https://arxiv.org/abs/2602.02494)
*Dulhan Jayalath,Oiwi Parker Jones*

Main category: cs.LG

TL;DR: MEG-XL：通过2.5分钟长上下文预训练的脑磁图模型，显著提升脑电到文本解码的数据效率，仅需1小时数据即可达到传统方法50小时数据的性能


<details>
  <summary>Details</summary>
Motivation: 临床脑机接口面临瘫痪患者无法提供大量训练数据的问题。现有预训练方法通常只使用几秒上下文，无法捕捉自然语言处理所需的长时间神经上下文信息

Method: 提出MEG-XL模型，使用2.5分钟（相当于191k tokens）的长上下文进行预训练，比现有方法长5-300倍，然后微调用于脑电数据的单词解码任务

Result: MEG-XL仅需1小时数据即可匹配传统监督方法50小时数据的性能，优于现有脑基础模型。长上下文预训练学到的表征在单词解码任务上迁移效果更好

Conclusion: 长上下文预训练能够有效利用被其他方法丢弃的扩展神经上下文信息，为临床脑机接口提供了数据高效的高性能解决方案

Abstract: Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL .

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [417] [An Oscillation-Free Real Fluid Quasi-Conservative Finite Volume Method for Transcritical and Phase-Change Flows](https://arxiv.org/abs/2602.00658)
*Haotong Bai,Wenjia Xie,Yixin Yang,Ping Yi,Mingbo Sun*

Main category: physics.comp-ph

TL;DR: 提出了一种新的真实流体准守恒有限体积法，用于模拟涉及激波的跨临界和相变流动，通过局部线性化状态方程消除压力振荡。


<details>
  <summary>Details</summary>
Motivation: 传统完全守恒格式在模拟真实流体（特别是跨临界和相变流动）时会产生虚假压力振荡，需要开发能够消除这些振荡同时保持准确性的数值方法。

Method: 扩展经典五方程准守恒模型到真实流体，在每个网格点和时间步局部线性化状态方程，构建并演化冻结Grüneisen系数Γ和线性化余项E₀，通过两个平流方程传输，最后重建无振荡压力场并进行热力学重投影。

Result: 理论分析表明在光滑区域能量守恒误差是高阶项，在间断区域误差由熵增率决定。数值测试验证了方法能稳健模拟跨临界流动、相变和激波-界面相互作用等复杂过程，仅有微小能量守恒误差。

Conclusion: RFQC方法在捕捉激波和相变方面既准确又稳健，成功解决了真实流体模拟中的压力振荡问题，同时保持了可接受的守恒特性。

Abstract: A new Real Fluid Quasi-Conservative (RFQC) finite volume method is developed to address the numerical simulation of real fluids involving shock waves in transcritical and phase-change flows. To eliminate the spurious pressure oscillations inherent in fully conservative schemes, we extend the classic five-equation quasi-conservative model, originally designed for two-phase flows, to real fluids governed by arbitrary equations of state (EoS). The RFQC method locally linearizes the real fluid EoS at each grid point and time step, constructing and evolving the frozen Grüneisen coefficient $Γ$ and the linearization remainder $E_0$ via two advection equations. At the end of each time step, the evolved $Γ$ and $E_0$ are utilized to reconstruct the oscillation-free pressure field, followed by a thermodynamic re-projection applied to the conserved variables. Theoretical analysis demonstrates that, in smooth regions, the energy conservation error of the RFQC method is a high-order term relative to the spatial reconstruction truncation error. In discontinuous regions, this error is determined by the entropy increase rate, thereby maintaining consistency with the inherent truncation error of shock-capturing methods. A series of numerical tests verifies that the method can robustly simulate complex flow processes with only minor energy conservation errors, including transcritical flows, phase transitions, and shock-interface interactions. The RFQC method is proven to be both accurate and robust in capturing shock waves and phase transitions.

</details>


### [418] [Semi-implicit Lax-Wendroff kinetic scheme for electron-phonon coupling](https://arxiv.org/abs/2602.01220)
*Jiaming Li,Hong Liang,Meng Lian,Chuang Zhang,Jiangrong Xu*

Main category: physics.comp-ph

TL;DR: 提出了一种用于金属中电子-声子耦合过程的半隐式Lax-Wendroff格式，基于双温度动力学方程，能够有效捕捉从弹道到扩散区域的电子-声子耦合或热传导过程。


<details>
  <summary>Details</summary>
Motivation: 微电子器件中的电子-声子耦合和热管理需要能够跨越不同传输机制（从弹道到扩散区域）的数值方法，传统方法受到弛豫时间和平均自由程的限制。

Method: 开发了基于双温度动力学方程的半隐式Lax-Wendroff格式，将物理方程的演化信息整合到数值建模过程中，通过有限差分法重构界面分布函数，将粒子迁移、散射和电子-声子耦合过程在单个时间步内耦合。

Result: 数值测试表明该方法能够高效捕捉从弹道到扩散区域的电子-声子耦合或热传导过程，时间步长或网格尺寸不受弛豫时间和平均自由程的限制。

Conclusion: 该方法为描述微电子器件中的电子-声子耦合或热管理提供了新工具，能够跨越不同传输机制，具有重要的应用价值。

Abstract: A semi-implicit Lax-Wendroff scheme is developed for electron-phonon coupling process in metals based on the two-temperature kinetic equations. The core of this method is to integrate the evolution information of physical equations into the numerical modeling process, which leads to that the time step or cell size is not limited by the relaxation time and mean free path. Specifically, the finite difference method is used to solve the kinetic model again when reconstructing the interfacial distribution function, through which the particle migration, scattering and electron-phonon coupling processes are coupled together within a single time step. Numerical tests demonstrate that this method could efficiently capture electron-phonon coupling or heat conduction processes from the ballistic to diffusive regimes. It provides a new tool for describing electron-phonon coupling or thermal management in microelectronic devices.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [419] [From Block Diagrams to Bloch Spheres: Graphical Quantum Circuit Simulation in LabVIEW](https://arxiv.org/abs/2602.00643)
*Murtaza Vefadar*

Main category: quant-ph

TL;DR: QuVI是一个在NI LabVIEW环境中开发的开源量子电路工具包，利用数据流范式提供直观的量子电路可视化设计，降低量子计算学习门槛。


<details>
  <summary>Details</summary>
Motivation: 量子计算从理论物理转向工程应用，需要更易用的仿真工具。现有文本框架（如Qiskit、Cirq）学习曲线陡峭，不适合习惯图形系统设计的学生和工程师。

Method: 在NI LabVIEW环境中开发QuVI工具包，利用LabVIEW的"数据流"范式（线表示数据，节点表示操作），提供直观的量子电路可视化表示，并集成经典控制结构（循环、条件语句）。

Result: 通过构建和可视化基本量子算法验证了工具包能力，结果与理论预测一致。将"框图"直接转换为量子态演化（"布洛赫球"），为教育和研究提供强大平台。

Conclusion: QuVI为教育工作者和研究人员提供了一个强大的量子逻辑原型设计平台，无需离开图形工程工作空间，降低了量子计算的学习和应用门槛。

Abstract: As quantum computing transitions from theoretical physics to engineering applications, there is a growing need for accessible simulation tools that bridge the gap between abstract linear algebra and practical implementation. While text-based frameworks (like Qiskit or Cirq) are standard, they often present a steep learning curve for students and engineers accustomed to graphical system design. This paper introduces QuVI (Quantum Virtual Instrument), an open-source quantum circuit toolkit developed natively within the NI LabVIEW environment. Moving beyond initial proof-of-concept models, QuVI establishes a robust framework that leverages LabVIEW's "dataflow" paradigm-where wires represent data and nodes represent operations-to provide an intuitive, visual analog to standard quantum circuit notation while enabling the seamless integration of classical control structures like loops and conditionals. The toolkit's capabilities are demonstrated through the construction and visualization of fundamental quantum algorithms, verifying results against theoretical predictions. By translating "Block Diagrams" directly into quantum state evolutions ("Bloch Spheres"), QuVI offers educators and researchers a powerful platform for prototyping quantum logic without leaving the graphical engineering workspace.

</details>


### [420] [QSPE: Enumerating Skeletal Quantum Programs for Quantum Library Testing](https://arxiv.org/abs/2602.00024)
*Jiaming Ye,Fuyuan Zhang,Shangzhou Xia,Xiaoyu Guo,Xiongfei Wu,Jianjun Zhao,Yinxing Xue*

Main category: quant-ph

TL;DR: QSPE：一种基于差分测试的量子库自动化测试方法，无需专家配置，通过状态向量验证减少误报，成功检测708个错误


<details>
  <summary>Details</summary>
Motivation: 量子计算库快速发展但缺乏成熟的测试方法，现有工具依赖专家配置且使用基于测量的验证会产生误报

Method: 基于差分测试原则，扩展SPE方法，自动化生成大量量子程序变体，采用状态向量验证替代测量验证

Result: 生成22,770个程序变体，减少90%执行成本，检测708个错误，其中81个被Qiskit团队确认

Conclusion: QSPE提供了一种实用、自动化、高效的量子库测试方法，显著提升测试覆盖率和准确性

Abstract: The rapid advancement of quantum computing has led to the development of various quantum libraries, empowering compilation, simulation, and hardware backend interfaces. However, ensuring the correctness of these libraries remains a fundamental challenge due to the lack of mature testing methodologies. The state-of-the-art tools often rely on domain-specific configurations and expert knowledge, which limits their accessibility and scalability in practice. Furthermore, although these tools demonstrate strong performance, they adopt measurement-based for output validation in testing, which makes them produce false positive reports.
  To alleviate these limitations, we propose QSPE, a practical approach that follows the differential testing principle and extends the existing approach, SPE, for quantum libraries. QSPE is fully automated, requiring no pre-set configurations or domain expertise, and can effectively generate a large set of diverse program variants that comprehensively explore the quantum compilation space. To mitigate the possible false positive reports, we propose statevector-based validation as an alternative to measurement-based validation. In our experiments, the QSPE approach demonstrates remarkable effectiveness in generating 22,770 program variants across multiple quantum computing platforms. By avoiding $α$-equivalence at the quantum and classical program wise, QSPE can reduce redundant generation and save more than 90\% of execution cost. Finally, the statevector-based validation method assists QSPE to reduce false alarms and effectively detects 708 miscompilations across multiple quantum libraries. Notably, 81 of the discovered bugs have been officially approved and acknowledged by the Qiskit development team, demonstrating the practical impact of our approach.

</details>


### [421] [Methods for non-variational heuristic quantum optimisation](https://arxiv.org/abs/2602.01353)
*Stuart Ferguson,Petros Wallden*

Main category: quant-ph

TL;DR: 提出两种新型量子优化启发式算法QeSA和QePT，基于MCMC而非变分框架，在Sherrington-Kirkpatrick问题上展示优于经典基准的扩展性


<details>
  <summary>Details</summary>
Motivation: 当前噪声鲁棒量子算法研究主要集中于变分方法，替代方案相对较少探索。本文旨在开发非变分框架的混合量子-经典优化算法，利用量子计算在优化领域的潜在计算优势

Method: 引入基于马尔可夫链蒙特卡洛(MCMC)技术的混合量子-经典方法，提出了量子增强模拟退火(QeSA)和量子增强并行回火(QePT)两种算法

Result: 在困难的Sherrington-Kirkpatrick实例上验证了算法有效性，展示了优于经典基准的扩展性能。算法具有固有的噪声鲁棒性，支持量子与经典资源的并行执行

Conclusion: 这些算法为使用近期量子设备解决大规模优化问题提供了可扩展且具有潜在竞争力的途径，仅需经典通信即可实现量子-经典并行执行

Abstract: Optimisation plays a central role in a wide range of scientific and industrial applications, and quantum computing has been widely proposed as a means to achieve computational advantages in this domain. To date, research into the design of noise-resilient quantum algorithms has been dominated by variational approaches, while alternatives remain relatively unexplored. In this work, we introduce a novel class of quantum optimisation heuristics that forgo this variational framework in favour of a hybrid quantum-classical approach built upon Markov Chain Monte Carlo (MCMC) techniques. We introduce Quantum-enhanced Simulated Annealing (QeSA) and Quantum-enhanced Parallel Tempering (QePT), before validating these heuristics on hard Sherrington-Kirkpatrick instances and demonstrate their superior scaling over classical benchmarks. These algorithms are expected to exhibit inherent robustness to noise and support parallel execution across both quantum and classical resources with only classical communication required. As such, they offer a scalable and potentially competitive route toward solving large-scale optimisation problems with near-term quantum devices.

</details>


### [422] [Quantum Circuit-Based Learning Models: Bridging Quantum Computing and Machine Learning](https://arxiv.org/abs/2602.00048)
*Fan Fan,Yilei Shi,Mihai Datcu,Bertrand Le Saux,Luigi Iapichino,Francesca Bovolo,Silvia Liberata Ullo,Xiao Xiang Zhu*

Main category: quant-ph

TL;DR: 本文综述了量子机器学习（QML）领域，重点关注基于量子电路的经典数据分析模型，涵盖QML模型、混合框架、理论分析、噪声鲁棒性设计以及在不同应用领域的适应性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在多个领域广泛应用，大规模数据和强大算力带来性能提升的同时也带来了挑战。量子计算作为可能解决这些挑战的技术受到关注，因此量子机器学习（QML）作为两者的结合成为研究热点。本文旨在回顾现有量子电路学习模型在经典数据分析中的应用，识别其潜力和挑战。

Method: 本文采用文献综述方法，系统梳理量子机器学习领域的研究成果。重点关注：1）基于核方法和神经网络的QML模型；2）QML与经典机器学习层结合的混合框架；3）理论分析和实证研究；4）噪声鲁棒和硬件高效的QML方法；5）先进的量子电路设计范式；6）QML在不同应用领域的适应性。

Result: 通过对现有研究的系统回顾，本文识别了量子机器学习在经典数据分析中的潜力和挑战。研究发现QML模型在特定任务中展现出优势，混合框架能够结合量子与经典方法的优点，噪声鲁棒性设计有助于在当前硬件限制下提升实用性，而先进的量子电路设计则为未来发展提供了新方向。

Conclusion: 量子机器学习作为量子计算与机器学习交叉领域，具有解决传统机器学习挑战的潜力。本文通过全面综述为研究者提供了该领域的概览，识别了关键研究方向，并为未来发展和更广泛的应用提供了指导。随着硬件进步和算法创新，QML有望在未来几年实现更广泛的采用。

Abstract: Machine Learning (ML) has been widely applied across numerous domains due to its ability to automatically identify informative patterns from data for various tasks. The availability of large-scale data and advanced computational power enables the development of sophisticated models and training strategies, leading to state-of-the-art performance, but it also introduces substantial challenges. Quantum Computing (QC), which exploits quantum mechanisms for computation, has attracted growing attention and significant global investment as it may address these challenges. Consequently, Quantum Machine Learning (QML), the integration of these two fields, has received increasing interest, with a notable rise in related studies in recent years. We are motivated to review these existing contributions regarding quantum circuit-based learning models for classical data analysis and highlight the identified potentials and challenges of this technique. Specifically, we focus not only on QML models, both kernel-based and neural network-based, but also on recent explorations of their integration with classical machine learning layers within hybrid frameworks. Moreover, we examine both theoretical analysis and empirical findings to better understand their capabilities, and we also discuss the efforts on noise-resilient and hardware-efficient QML that could enhance its practicality under current hardware limitations. In addition, we cover several emerging paradigms for advanced quantum circuit design and highlight the adaptability of QML across representative application domains. This study aims to provide an overview of the contributions made to bridge quantum computing and machine learning, offering insights and guidance to support its future development and pave the way for broader adoption in the coming years.

</details>


### [423] [Sampling two-dimensional isometric tensor network states](https://arxiv.org/abs/2602.02245)
*Alec Dektor,Eugene Dumitrescu,Chao Yang*

Main category: quant-ph

TL;DR: 提出了两种用于二维等距张量网络态（isoTNS）的采样算法，扩展了一维张量网络采样方法，包括独立采样算法和贪婪搜索算法。


<details>
  <summary>Details</summary>
Motivation: 量子系统概率分布的采样是重要的计算任务，如量子优势实验和量子蒙特卡洛算法。张量网络是高效表示具有有限纠缠的大量子系统状态的宝贵工具。虽然一维张量网络采样算法已成熟，但需要为二维等距张量网络态开发有效的采样方法。

Method: 提出了两种算法：1）独立采样算法，生成单个配置及其相关概率；2）贪婪搜索算法，识别K个高概率配置及其对应概率。这些算法是现有1D张量网络采样方法的扩展。

Result: 数值结果表明，这些算法在不同纠缠程度和系统大小的量子状态中均表现出有效性。

Conclusion: 成功开发了两种用于二维等距张量网络态的采样算法，扩展了一维张量网络采样方法，为量子计算和模拟中的采样任务提供了新工具。

Abstract: Sampling a quantum systems underlying probability distributions is an important computational task, e.g., for quantum advantage experiments and quantum Monte Carlo algorithms. Tensor networks are an invaluable tool for efficiently representing states of large quantum systems with limited entanglement. Algorithms for sampling one-dimensional (1D) tensor networks are well-established and utilized in several 1D tensor network methods. In this paper we introduce two novel sampling algorithms for two-dimensional (2D) isometric tensor network states (isoTNS) that can be viewed as extensions of algorithms for 1D tensor networks. The first algorithm we propose performs independent sampling and yields a single configuration together with its associated probability. The second algorithm employs a greedy search strategy to identify K high-probability configurations and their corresponding probabilities. Numerical results demonstrate the effectiveness of these algorithms across quantum states with varying entanglement and system size.

</details>


### [424] [Integrity from Algebraic Manipulation Detection in Trusted-Repeater QKD Networks](https://arxiv.org/abs/2602.00069)
*Ailsa Robertson,Christian Schaffner,Sebastian R. Verschoor*

Main category: quant-ph

TL;DR: 首次提出在可信中继量子密钥分发网络中同时提供机密性和完整性的协议，结合代数操作检测码和多路径中继技术


<details>
  <summary>Details</summary>
Motivation: 当前量子密钥分发(QKD)受硬件限制只能短距离部署，虽然通过可信中继网络可以扩展距离，但现有协议缺乏对完整性的可证明保证，无法同时抵御外部攻击者和被腐蚀的中继节点

Method: 结合代数操作检测码(AMD)与可信中继网络中的多路径中继技术，通过序列游戏形式化证明协议的安全性

Result: 提出了首个能同时提供机密性和完整性的协议，实现了信息论安全，能够检测到来自外部攻击者和被腐蚀中继节点的篡改

Conclusion: 该协议填补了量子密钥分发网络中完整性保证的空白，为长距离安全通信提供了可证明的安全保障

Abstract: Quantum Key Distribution (QKD) allows secure communication without relying on computational assumptions, but can currently only be deployed over relatively short distances due to hardware constraints. To extend QKD over long distances, networks of trusted repeater nodes can be used, wherein QKD is executed between neighbouring nodes and messages between non-neighbouring nodes are forwarded using a relay protocol. Although these networks are being deployed worldwide, no protocol exists which provides provable guarantees of integrity against manipulation from both external adversaries and corrupted intermediates. In this work, we present the first protocol that provably provides both confidentiality and integrity. Our protocol combines an existing cryptographic technique, Algebraic Manipulation Detection (AMD) codes, with multi-path relaying over trusted repeater networks. This protocol achieves Information Theoretic Security (ITS) against the detection of manipulation, which we prove formally through a sequence of games.

</details>


### [425] [A generating-function approach to the interference of squeezed states with partial distinguishability](https://arxiv.org/abs/2602.00071)
*Matheus Eiji Ohno Bezerra,Valery Shchesnovich*

Main category: quant-ph

TL;DR: 提出基于相空间生成函数的框架，分析单模压缩态干涉中部分可区分性的影响，超越传统非干涉模式模型，揭示光子内态重叠引起的多光子干涉效应。


<details>
  <summary>Details</summary>
Motivation: 光子可区分性是光子量子信息处理中的主要噪声源，传统方法使用第一量子化方法或非干涉模式模型，无法完全捕捉光子内态重叠引起的多光子干涉效应。

Method: 基于相空间形式主义的生成函数框架，用于表征单模压缩态干涉中的部分可区分性效应，超越传统的非干涉模式表示方法。

Result: 该框架能够捕获由光子内态重叠引起的真实多光子干涉效应，清晰展示可区分性如何在高斯玻色子采样协议中产生有效噪声，并系统研究内态重叠的相位效应。

Conclusion: 提出的相空间生成函数方法为分析光子可区分性提供了更全面的物理描述，特别适用于高斯玻色子采样等量子信息处理协议中的噪声分析。

Abstract: Photon distinguishability is a fundamental property manifested in multiphoton interference and one of the main sources of noise in any photonic quantum information processing. In this work, rather than relying on first-quantization methods, we build on a generating-function framework based on the phase-space formalism to characterize the effects of partial distinguishability on the interference of single-mode squeezed states. Our approach goes beyond commonly used models that represent distinguishability via additional noninterfering modes and captures genuine multiphoton interference effects induced by the overlap of the internal state of the photons. This description provides a clear physical account of how distinguishability gives rise to effective noise in Gaussian boson sampling protocols while enabling a systematic investigation of phase effects arising from the overlap of the internal states.

</details>


### [426] [Accelerating De Novo Genome Assembly via Quantum-Assisted Graph Optimization with Bitstring Recovery](https://arxiv.org/abs/2602.00156)
*Jaya Vasavi Pamidimukkala,Himanshu Sahu,Ashwini Kannan,Janani Ananthanarayanan,Kalyan Dasgupta,Sanjib Senapati*

Main category: quant-ph

TL;DR: 本文提出了一种结合量子计算优化算法与经典预处理技术的混合方法，用于加速基因组组装过程，通过量子变分本征求解器解决基因组组装图中的哈密顿路径和欧拉路径问题。


<details>
  <summary>Details</summary>
Motivation: 基因组测序对于解码遗传信息、识别生物体、理解疾病和推进个性化医疗至关重要。然而，从头基因组组装由于缺乏参考基因组且计算复杂度高，在时间和准确性方面都面临重大挑战。

Method: 采用混合方法：结合量子计算优化算法与经典预处理技术。具体使用基于门的量子计算，通过高阶二元优化（HOBO）公式和变分量子本征求解器（VQE）算法解决基因组组装图中的哈密顿路径和欧拉路径问题，并引入新颖的比特串恢复机制以改进优化器在解空间中的遍历。

Result: 与经典优化技术进行比较分析，结果表明随着量子硬件的不断发展和噪声水平的降低，该公式在加速基因组测序方面具有显著潜力，能够为基因组研究中的复杂挑战提供更快、更准确的解决方案。

Conclusion: 量子计算与经典预处理相结合的混合方法在基因组组装中展现出巨大潜力，随着量子硬件技术的进步，有望显著加速基因组测序过程，为基因组研究提供更高效的解决方案。

Abstract: Genome sequencing is essential to decode genetic information, identify organisms, understand diseases and advance personalized medicine. A critical step in any genome sequencing technique is genome assembly. However, de novo genome assembly, which involves constructing an entire genome sequence from scratch without a reference genome, presents significant challenges due to its high computational complexity, affecting both time and accuracy. In this study, we propose a hybrid approach utilizing a quantum computing-based optimization algorithm integrated with classical pre-processing to expedite the genome assembly process. Specifically, we present a method to solve the Hamiltonian and Eulerian paths within the genome assembly graph using gate-based quantum computing through a Higher-Order Binary Optimization (HOBO) formulation with the Variational Quantum Eigensolver algorithm (VQE), in addition to a novel bitstring recovery mechanism to improve optimizer traversal of the solution space. A comparative analysis with classical optimization techniques was performed to assess the effectiveness of our quantum-based approach in genome assembly. The results indicate that, as quantum hardware continues to evolve and noise levels diminish, our formulation holds a significant potential to accelerate genome sequencing by offering faster and more accurate solutions to the complex challenges in genomic research.

</details>


### [427] [Signatures of coherent initial ensembles on all work moments](https://arxiv.org/abs/2602.00227)
*Pranay Nayak,Sreenath K. Manikandan,Tan Van Vu,Supriya Krishnamurthy*

Main category: quant-ph

TL;DR: 该论文提出了一种非侵入式的工作定义，用于研究初始相干叠加态对量子热力学的影响，发现相同密度矩阵的不同初始系综具有相同平均功但不同功涨落，并建立了包含量子绝对不可逆性的广义涨落定理。


<details>
  <summary>Details</summary>
Motivation: 传统量子功的投影能量测量方法会消除初始相干性并改变动力学，无法捕捉初始系综中能量本征态相干叠加的热力学效应。需要一种非侵入式的工作定义来研究相干叠加的物理影响。

Method: 使用非侵入式工作定义，应用于驱动耗散量子比特系统，其中初始制备包含相干叠加态而驱动是无相干的。推导了该工作矩生成函数的演化方程，忠实捕捉初始系综中相干叠加的热力学特征。

Result: 发现对应相同密度矩阵的不同初始系综具有相同平均功但不同功涨落；单调驱动下，无相干初始系综的涨落最大；量子比特擦除中，擦除经典比特与Haar随机初始系综的工作统计显著不同；建立了包含量子绝对不可逆性的广义涨落定理。

Conclusion: 初始系综中的相干性可作为热力学精度的资源而不增加额外耗散功成本，并建立了新的量子平均耗散功下界，该下界反直觉地也适用于具有相同初始密度矩阵的"经典"初始系综。

Abstract: Standard treatments of quantum work using projective energy measurements erase initial coherence and alter the dynamics, thereby failing to capture the thermodynamic effects of coherent superpositions of energy eigenstates in an ensemble of initial states. In this article, we use an operational work definition that is non-intrusive, applying it to the case of a driven dissipative qubit, where the qubit's initial preparation comprises coherent superposition states, while the driving is coherence-less. We derive an evolution equation for the moment generating function for this work, faithfully capturing the thermodynamic signature of coherent superpositions in the initial ensemble. We demonstrate that different initial ensembles that correspond to the same density matrix upon ensemble average, while having the same average work, display different work fluctuations. For monotonic driving, we show that fluctuations are maximum for coherence-less initial ensembles. As an application, we consider quantum bit-erasure in finite time and demonstrate significantly different work statistics for erasing a classical bit of information versus a Haar random initial ensemble. Our results indicate that coherence in the initial ensemble can be utilized as a resource for thermodynamic precision without incurring additional dissipative work costs. We also obtain a generalized fluctuation theorem that establishes a new quantum lower bound on the mean dissipated work. This bound, counterintuitively, is also applicable to a "classical" initial ensemble with the same initial density matrix and is connected to quantum absolute irreversibility.

</details>


### [428] [Complexity of Quantum Trajectories](https://arxiv.org/abs/2602.00232)
*Luca Lumia,Emanuele Tirrito,Mario Collura,Fabian H. L. Essler,Rosario Fazio*

Main category: quant-ph

TL;DR: 论文提出了一种基于本征维度的数据驱动方法，用于分析开放量子系统中量子轨迹的复杂性，发现该方法能有效探测耗散量子系统中的混沌、可积性、希尔伯特空间碎片化等现象。


<details>
  <summary>Details</summary>
Motivation: 研究开放量子系统中量子轨迹的复杂性如何受到守恒定律和其他动力学约束的影响，开发一种无监督的方法来探测耗散量子系统中的混沌和遍历性破坏现象。

Method: 采用数据驱动方法，基于本征维度（编码数据集所需的最小变量数）来表征量子轨迹的复杂性。将该框架应用于多个系统，包括量子陀螺和XXZ链的耗散变体。

Result: 本征维度对系统动力学结构敏感：在混沌的Lindblad演化中，当系统在特定参数值下变得可积、出现希尔伯特空间碎片化或形成闭合BBGKY层次结构时，本征维度会出现显著的最小值。

Conclusion: 本征维度方法提供了一种无监督的探针，能够敏感地探测耗散量子系统中超越初始瞬态机制的混沌和遍历性破坏现象，为分析开放量子系统的复杂性提供了新工具。

Abstract: Open quantum systems can be described by unraveling Lindblad master equations into ensembles of quantum trajectories. Here we investigate how the complexity of such trajectories is affected by conservation laws and other dynamical constraints of the underlying Lindblad evolution. We characterize this complexity using a data-driven approach based on the intrinsic dimension, defined as the minimal number of variables required to encode the information contained in a data set. Applying this framework to several systems, including dissipative variants of the quantum top and of the XXZ chain, we find that the intrinsic dimension is sensitive to the structure of their dynamics. The Lindblad evolution in these systems is typically chaotic, but additional constraints arise at specific parameter values, where the dynamics becomes integrable, exhibits Hilbert-space fragmentation, or develops a closed BBGKY hierarchy, leading to pronounced minima in the intrinsic dimension. Our approach results in an unsupervised probe of the complexity of dissipative quantum systems that is sensitive to chaos and ergodicity breaking phenomena beyond the initial transient regime.

</details>


### [429] [Fermionic magic resources in disordered quantum spin chains](https://arxiv.org/abs/2602.00245)
*Pedro R. Nicácio Falcão,Jakub Zakrzewski,Piotr Sierant*

Main category: quant-ph

TL;DR: 该研究使用费米子反平坦度(FAF)量化量子态偏离自由费米子描述的程度，分析了无序XXZ自旋链及其杂质变体在遍历和MBL区域的行为，发现MBL抑制费米子非高斯性而遍历性恢复它。


<details>
  <summary>Details</summary>
Motivation: 费米子非高斯性量化量子态偏离经典可处理的自由费米子描述的程度，是计算量子优势的必要资源。研究旨在理解这种非高斯性在遍历和MBL区域的行为，探索MBL如何影响超越自由费米子的复杂性。

Method: 使用费米子反平坦度(FAF)作为度量，研究无序自旋-1/2 XXZ链及其杂质变体。分析高激发本征态在弱无序和深度MBL区域的行为，包括典型态和罕见长程猫态，并研究从乘积态出发的时间演化。

Result: FAF从弱无序的典型态行为演变为MBL区域的强烈抑制，XXZ链显示体积律标度而杂质设置受面积律限制。罕见猫态显示FAF显著增强。在MBL区域，FAF随时间缓慢增长并通过幂律弛豫接近饱和。

Conclusion: MBL抑制费米子非高斯性及相关的超越自由费米子的复杂性，而遍历性恢复它。FAF可作为诊断MBL失稳机制的敏感工具，激励在其他遍历性破坏现象中探索费米子非高斯性。

Abstract: Fermionic non-Gaussianity quantifies a quantum state's deviation from a classically tractable free-fermionic description, constituting a necessary resource for computational quantum advantage. Here we use fermionic antiflatness (FAF) to measure this deviation across ergodic and many-body localized (MBL) regimes. We focus on the paradigmatic disordered spin-$1\!/2$ XXZ chain and its impurity variant with local interactions. Across highly excited eigenstates, FAF evolves from typical-state behavior at weak disorder to strongly suppressed values deep in the MBL regime, with volume-law scaling in the XXZ chain and an area-law bound in the impurity setting. Rare long range catlike eigenstates exhibit a pronounced enhancement of FAF, making it a sensitive diagnostic of mechanisms proposed to destabilize MBL. Starting from product states, we find that in the MBL regime FAF grows slowly in time, approaching saturation via a power-law relaxation. Overall, our results show that MBL suppresses fermionic non-Gaussianity, and the associated complexity beyond free fermions, while ergodicity restores it, motivating explorations of fermionic non-Gaussianity in other ergodicity-breaking phenomena.

</details>


### [430] [Synchronized distribution of quantum entanglement coexisting with high-rate, broadband classical optical communications over a real-world fiber link](https://arxiv.org/abs/2602.00253)
*Gina M. Talcott,Ahnnika I. Hess,Laura d'Avossa,Scott J. Kohlert,Fei I. Yeh,Jim Hao Chen,Joe J. Mambretti,Tim M. Rambo,Gregory S. Kanter,Jordan M. Thomas,Prem Kumar*

Main category: quant-ph

TL;DR: 在已部署的24.4公里光纤上实现O波段偏振编码量子纠缠分发，与全负载C波段经典通信系统和L波段同步信号共存，展示了量子网络与现有基础设施的兼容性。


<details>
  <summary>Details</summary>
Motivation: 与现有经典网络基础设施兼容是实现大规模量子网络部署的可扩展路径。研究旨在验证量子通信系统能否在高功率、高带宽的经典通信环境下正常工作。

Method: 使用O波段偏振编码量子纠缠分发，在24.4公里已部署光纤上运行，同时与C波段全负载经典通信系统（包含两个800Gbps信道）和L波段同步信号共存。分析宽带C波段光产生的自发拉曼散射谱，优化波长选择和窄带滤波。

Result: 通过优化波长选择和窄带滤波，在C波段总发射功率21.4dBm（支持36Tbps传输）的强干扰环境下，仍能保持完好的贝尔态保真度。这是首次实现两个远程节点间的纠缠量子通信与独立经典通信流量的共存。

Conclusion: 量子纠缠可以与超高功率水平和创纪录的经典带宽共存，为将基于纠缠的量子网络集成到高容量通信基础设施中提供了现实可行性。

Abstract: Compatibility with existing classical network infrastructure offers a scalable path towards deploying largescale quantum networks. Here, we demonstrate O-band polarization-encoded quantum entanglement distribution over an installed 24.4-km fiber while coexisting with a state-of-the-art fully-loaded C-band classical communications line system and a picosecond-level precision L-band synchronization signal. The classical system carries two 800-Gbps channels while the remainder of the C-band is filled with amplified spontaneous emission as is standard for such state-of-the-art communications systems. We examine the spontaneous Raman scattering spectrum generated from this broadband C-band light and offer insights into wavelength allocation for O-band quantum channels. Optimal wavelength selection and narrow filtering enable well-preserved Bell state fidelity when coexisting with 21.4-dBm aggregate launch power across the C-band suitable for 36 Tbps transmission. To the best of our knowledge, this is the first implementation of entanglement-based quantum communications between two remote nodes coexisting with independent classical communications traffic. We demonstrate coexistence of quantum entanglement with ultra-high power levels and record classical bandwidth, offering promise for real-world entanglement-based networking integrated within high-capacity communications infrastructure.

</details>


### [431] [Lower bounds on non-local computation from controllable correlation](https://arxiv.org/abs/2602.00255)
*Richard Cleve,Alex May*

Main category: quant-ph

TL;DR: 本文提出了两种新的下界技术来评估非局域量子计算中的纠缠成本，适用于任意酉算子，解决了CNOT等常用两比特量子门纠缠成本下界的开放问题。


<details>
  <summary>Details</summary>
Motivation: 非局域量子计算中的纠缠成本在复杂性、密码学、引力等领域具有重要意义，但现有下界技术适用范围有限，大多数简单酉算子的下界问题尚未解决。

Method: 提出了两种基于可控相关性和可控纠缠的新下界技术，这些技术可以评估任意酉算子的纠缠成本，并具有并行重复特性。

Result: 对Haar随机两比特酉算子获得非平凡下界；首次获得了CNOT、DCNOT、√SWAP和XX相互作用等常用两比特量子门的下界；其中CNOT门的下界是紧致的，完全解决了其纠缠成本问题。

Conclusion: 新提出的下界技术能够有效评估非局域量子计算中的纠缠成本，解决了长期存在的开放问题，并在噪声环境下也适用。

Abstract: Understanding entanglement cost in non-local quantum computation (NLQC) is relevant to complexity, cryptography, gravity, and other areas. This entanglement cost is largely uncharacterized; previous lower bound techniques apply to narrowly defined cases, and proving lower bounds on most simple unitaries has remained open. Here, we give two new lower bound techniques that can be evaluated for any unitary, based on their controllable correlation and controllable entanglement. For Haar random two qubit unitaries, our techniques typically lead to non-trivial lower bounds. Further, we obtain lower bounds on most of the commonly studied two qubit quantum gates, including CNOT, DCNOT, $\sqrt{\text{SWAP}}$, and the XX interaction, none of which previously had known lower bounds. For the CNOT gate, one of our techniques gives a tight lower bound, fully resolving its entanglement cost. The resulting lower bounds have parallel repetition properties, and apply in the noisy setting.

</details>


### [432] [From Feynman-Vernon to Wiener Stochastic Path Integral](https://arxiv.org/abs/2602.00258)
*Antonio Camurati,Felipe Sobrero,Bruno Suassuna,Pedro V. Paraguassú*

Main category: quant-ph

TL;DR: 该论文建立了开放量子系统的Feynman-Vernon路径积分形式与经典随机动力学的Wiener路径积分之间的直接联系，展示了在强退相干极限下量子Feynman测度如何转化为随机Wiener测度。


<details>
  <summary>Details</summary>
Motivation: 建立量子开放系统与经典随机动力学之间的数学联系，理解量子退相干如何导致经典随机行为的出现。

Method: 采用Feynman-Vernon路径积分形式，考虑广义影响泛函，在强退相干极限下通过积分量子相干长度，推导出随机Langevin动力学。

Result: 证明了量子Feynman测度可以转化为随机Wiener测度，系统在Wigner函数表示下遵循可解释为经典概率理论的随机路径，并解决了从给定经典Langevin方程构造等效量子影响泛函的逆问题。

Conclusion: 该研究建立了量子开放系统与经典随机动力学之间的直接数学对应关系，为理解量子到经典的转变提供了新的理论框架。

Abstract: We establish a direct connection between the Feynman-Vernon path integral formalism for open quantum systems and the Wiener path integral used in classical stochastic dynamics. By considering a generalized influence functional in the strong decoherence limit, we demonstrate that integrating over the quantum coherence length leads to a derivation of stochastic Langevin dynamics. Specifically, we show that the quantum Feynman measure transforms into the stochastic Wiener measure. Applying this framework to the Wigner function representation, we show that the system follows a stochastic path interpretable via classical probability theory. Finally, we address the inverse problem: constructing an equivalent quantum influence functional from a given classical Langevin equation.

</details>


### [433] [A room-temperature cavity-magnonic source of correlated microwave pairs](https://arxiv.org/abs/2602.00287)
*Qiuyuan Wang,Aravind Karthigeyan,Chung-Tao Chou,Luqiao Liu*

Main category: quant-ph

TL;DR: 在室温下通过混合磁子-光子平台实现了强相关的微波信号产生，利用非简并磁子激发和腔光子模式耦合，实现了微波光子的分裂和强相关多通道信号生成。


<details>
  <summary>Details</summary>
Motivation: 传统的相关微波光子源需要在毫开尔文温度下工作，这限制了其可扩展性和广泛应用。本研究旨在开发一种能在室温下工作的相关微波信号源。

Method: 构建混合磁子-光子平台，通过同时耦合磁子模式和两个腔光子模式实现非简并激发。利用磁子-光子在线性和非线性区域的相互作用，使单个输入微波光子分裂成具有不同频率但保持强相关性的磁子极化激元对。

Result: 成功在室温下产生了强相关的微波信号，验证了真正的随机性和鲁棒的多通道相关性。基于此构建了微波通信实验，实现了噪声弹性信号传输并增强了安全性。

Conclusion: 该工作确立了腔磁子学作为生成相关多模微波信号的通用紧凑平台，为经典和量子领域的应用开辟了新途径。

Abstract: Correlated microwave photon sources are key enablers for technologies in quantum-limited sensing, signal amplification and communication, but the reliance on millikelvin operating temperature limits their scalability for broader applications. Here, at room temperature, we demonstrate strong correlated microwave signals emitted from a hybrid magnon-photon platform. Different from traditional parametrically induced magnons with degenerate frequencies, we achieve non-degenerate excitations by coupling magnon modes simultaneously with two cavity photon modes. Through the magnon-photon interactions in the corresponding linear and nonlinear regimes, one input microwave photon splits into a pair of magnon polaritons that possess distinct frequencies but maintain strong inter-mode correlations. The nonlinear magnon polariton dynamics empowered by this new parametric platform brings both verified true randomness and robust multi-channel correlations, from which we construct a microwave communication experiment for noise resilient signal transmission with added security. This work establishes cavity magnonics as a versatile and compact platform for generating correlated multi-mode microwave signals, opening new avenues for applications in classical and quantum domains.

</details>


### [434] [Path integrals and deformation quantization:the fermionic case](https://arxiv.org/abs/2602.00367)
*Anuar Kafuri*

Main category: quant-ph

TL;DR: 该论文解决了变形量子化中星指数计算困难的收敛问题，通过将玻色系统的形式推广到费米系统，基于格拉斯曼变量和相干态建立了严格方法，推导出费米版本的Feynman-Kac公式，并验证了简化方法在弱耦合极限下的有效性。


<details>
  <summary>Details</summary>
Motivation: 变形量子化中星指数（演化算符的符号）由于收敛问题难以计算。受玻色系统中星指数与量子传播子形式联系的启发，需要将类似方法扩展到费米系统，为费米系统研究提供新的计算工具。

Method: 基于格拉斯曼变量和相干态构建严格方法，从关联传播子获得费米星指数的闭式表达式。推导费米版本的Feynman-Kac公式，允许在相空间中直接计算基态能量。通过简单和谐振子模型验证方法。

Result: 成功建立了费米星指数的严格计算方法，推导出费米版本的Feynman-Kac公式。验证了简化（"naive"）方法在弱耦合极限下是严格（"meticulous"）形式主义的有效近似，为费米系统研究提供了新的计算工具。

Conclusion: 该工作解决了变形量子化中费米星指数计算的收敛问题，建立了基于格拉斯曼变量和相干态的严格形式主义，推导了费米Feynman-Kac公式，验证了简化方法的有效性，为费米系统的相空间研究提供了强大计算工具。

Abstract: This thesis addresses a fundamental problem in deformation quantization: the difficulty of calculating the star-exponential, the symbol of the evolution operator, due to convergence issues. Inspired by the formalism that connects the star-exponential with the quantum propagator for bosonic systems, this work develops the analogous extension for the fermionic case. A rigorous method, based on Grassmann variables and coherent states, is constructed to obtain a closed-form expression for the fermionic star-exponential from its associated propagator. As a primary application, a fermionic version of the Feynman-Kac formula is derived within this formalism, allowing for the calculation of the ground state energy directly in phase space. Finally, the method is validated by successfully applying it to the simple and driven harmonic oscillators, where it is demonstrated that a simplified ("naive") approach (with an ad-hoc "remediation") is a valid weak-coupling limit of the rigorous ("meticulous") formalism, thereby providing a new and powerful computational tool for the study of fermionic systems.

</details>


### [435] [Dynamical witnesses and universal behavior across chaos and non-ergodicity in the tilted Bose-Hubbard model](https://arxiv.org/abs/2602.00369)
*Carlos Diaz-Mejia,Sergio Lerma-Hernandez,Jorge G. Hirsch*

Main category: quant-ph

TL;DR: 该研究通过分析生存概率、单点纠缠熵和半链不平衡的时间演化，揭示了这些可观测量对玻色-哈伯德模型中混沌-规则相变的敏感性层次，发现生存概率是最稳健的相变指标。


<details>
  <summary>Details</summary>
Motivation: 虽然倾斜玻色-哈伯德模型中混沌相的谱特性已有充分研究，但跨越规则相变的动力学特征仍较少探索，需要研究不同可观测量如何反映这一相变。

Method: 通过分析生存概率、单点纠缠熵和半链不平衡的时间演化，研究这些可观测量在混沌-规则相变中的行为，并探索适当的标度变换使它们收敛到共同行为。

Result: 发现可观测量敏感性存在明显层次：纠缠熵的弛豫值在相变中平滑变化，不平衡表现出更明显的区分，而生存概率是最稳健的相变指标。适当标度后，所有可观测量都收敛到共同行为。

Conclusion: 生存概率是混沌-规则相变的最稳健指标，通过适当标度，不同可观测量可以统一描述相变行为，为混沌和规则动力学之间的转变提供了普适性表征。

Abstract: Quantum chaos in isolated quantum systems is intimately linked to thermalization and the rapid relaxation of observables. Although the spectral properties of the chaotic phase in the tilted Bose-Hubbard model have been well characterized, the corresponding dynamical signatures across the transition to regularity remain less explored . In this work, we investigate this transition by analyzing the time evolution of the survival probability, the single-site entanglement entropy, and the half-chain imbalance. Our results reveal a clear hierarchy in the sensitivity of these observables: the relaxation value of the entanglement entropy varies smoothly as a function of the Hamiltonian parameters across the chaos-regular transition, while the imbalance exhibits a more pronounced distinction. Most notably, the survival probability emerges as the most robust indicator of the transition between chaos and regularity. When appropriately scaled, all three observables converge onto a common behavior as a function of the Hamiltonian parameters for different numbers of sites and bosons,enabling a universal characterization of the transition between chaotic and regular dynamics.

</details>


### [436] [Single-site dissipation stabilizes a superconducting nonequilibrium steady state in a strongly correlated lattice](https://arxiv.org/abs/2602.00452)
*X. Z. Zhang*

Main category: quant-ph

TL;DR: 通过局域耗散工程，在强关联Hubbard模型中实现超导序作为开放系统动力学的鲁棒吸引子


<details>
  <summary>Details</summary>
Motivation: 探索能否使超导序成为强关联晶格开放系统动力学的鲁棒吸引子，寻找比传统空间扩展耗散方案更高效的局域控制方法

Method: 提出最小耗散工程协议：在粒子-空穴对称Hubbard模型中，对单个晶格位点应用旋转量子跃迁算符（局域变换的η-对降低算符），通过Lindblad演化将系统从真空态泵浦到非平衡稳态

Result: 局域耗散种子足以建立整个相互作用系统的相干性，实现宏观η-对非对角长程序；揭示了局域暗态选择机制，并分类了该稳态对静态无序的稳定性

Conclusion: 通过最小局域量子跃迁控制，建立了一条无序容忍的路径来稳定超导序作为非热吸引子，实现了局域到全局的同步

Abstract: Can superconducting order be made a robust attractor of open-system dynamics in strongly correlated lattices? We demonstrate that it can by proposing a minimal dissipation-engineering protocol for the particle--hole symmetric Hubbard model. By applying a rotated quantum jump operator, a locally transformed $η$-pair lowering operator, on as little as a single lattice site, we show that the Lindblad evolution autonomously pumps the system from the vacuum into a nonequilibrium steady state (NESS) with macroscopic $η$-pair off-diagonal long-range order (ODLRO). Crucially, this local-to-global synchronization contrasts with schemes requiring spatially extensive reservoirs: here, a strictly local dissipative seed suffices to establish coherence across the interacting system. We elucidate the mechanism via local dark-state selection, controlled elimination of off-manifold excursions induced by hopping, and a Liouvillian invariant-subspace structure that yields an attractive fixed point with a finite dissipative gap. Furthermore, we classify the stability of this NESS against static disorder, identifying a broad regime where the superconducting attractor is resilient to Hamiltonian perturbations that leave the effective subspace structure intact, while pinpointing specific perturbations that directly dephase the $η$-pseudospin coherence and suppress ODLRO. Our results establish a disorder-tolerant route to stabilizing superconducting order as a non-thermal attractor via minimal local quantum-jump control.

</details>


### [437] [Liouvillian gap closing--bound states in the continuum connection and diverse dynamics in a giant-atom waveguide QED setup](https://arxiv.org/abs/2602.00468)
*Hongwei Yu,Mingzhu Weng,Zhihai Wang,Jin Wang*

Main category: quant-ph

TL;DR: 论文揭示了开放量子系统中Liouvillian能隙闭合与连续谱束缚态之间的直接联系，在巨原子波导平台中建立了有效马尔可夫描述与底层非马尔可夫描述之间的谱-动力学对应关系。


<details>
  <summary>Details</summary>
Motivation: 在开放量子系统中，约化动力学通常用主方程描述，其Liouvillian能隙闭合通常标志着退相干自由子空间的出现。而全系统-环境复合体的动力学由哈密顿量谱决定，其中连续谱束缚态可以保护长寿命量子资源。尽管存在这些平行视角，但Liouvillian能隙闭合与连续谱束缚态形成之间的关系尚未被充分探索。

Method: 在典型的巨原子波导平台中研究这一问题，通过工程化巨原子几何结构来调控连续谱束缚态的数量（从3个到0个），从而展示丰富的动力学机制。

Result: 发现Liouvillian能隙闭合的出现必然标志着全哈密顿量描述中存在连续谱束缚态。通过调控巨原子几何，展示了包括拉比振荡、分数衰减和完全指数弛豫在内的丰富动力学机制。当两个连续谱束缚态频率简并时，长时间动力学趋于稳态而非持续振荡。

Conclusion: 研究结果建立了有效马尔可夫描述与底层非马尔可夫描述之间的直接谱-动力学联系，为灵活控制开放系统动力学提供了新途径。

Abstract: In open quantum systems, reduced dynamics is commonly described by a master equation, whose Liouvillian gap closing (LGC) typically signals the emergence of decoherence-free subspace. By contrast, the dynamics of the full system-environment compound is governed by the underlying Hamiltonian spectrum, where bound states in the continuum (BICs) can protect long-lived quantum resources. Despite these parallel perspectives, the relation between LGC and BIC formation has remained largely unexplored. Here we bridge this gap in a paradigmatic giant-atom waveguide platform and show that the occurrence of LGC necessarily benchmarks the presence of a BIC in the full Hamiltonian description. By engineering the giant-atom geometry, we further demonstrate rich dynamical regimes-including Rabi oscillations, fractional decay, and complete exponential relaxation-depending on the number of supported BICs, which can be tuned from three to zero. Remarkably, when two BICs become frequency-degenerate, the long-time dynamics approaches a steady state rather than exhibiting persistent oscillations. Our results establish a direct spectral-dynamical connection between effective Markovian and underlying non-Markovian descriptions, and provide a route toward flexible control of open-system dynamics.

</details>


### [438] [Quantum Phase Recognition via Quantum Attention Mechanism](https://arxiv.org/abs/2602.00473)
*Jin-Long Chen,Xin Li,Zhang-Qi Yin*

Main category: quant-ph

TL;DR: 提出混合量子-经典注意力模型，通过注意力机制和参数化量子电路提取量子态相关性，实现量子相变分类，在9和15量子比特系统中取得高准确率。


<details>
  <summary>Details</summary>
Motivation: 量子多体系统中的相变具有复杂的关联结构，传统计算方法在大系统中面临计算挑战，需要开发可扩展且数据高效的新方法。

Method: 使用注意力机制（通过swap测试实现）和参数化量子电路构建混合量子-经典注意力模型，提取量子态内部相关性并进行基态分类。

Result: 在9和15量子比特的cluster-Ising模型上，模型使用少于100个训练数据即达到高分类准确率，对训练集变化具有鲁棒性，并能捕捉相敏感特征和特征物理长度尺度。

Conclusion: 该模型为复杂多体系统中的量子相识别提供了可扩展且数据高效的方法，成功捕捉了相变的关键物理特征。

Abstract: Quantum phase transitions in many-body systems are fundamentally characterized by complex correlation structures, which pose computational challenges for conventional methods in large systems. To address this, we propose a hybrid quantum-classical attention model. This model uses an attention mechanism, realized through swap tests and a parameterized quantum circuit, to extract correlations within quantum states and perform ground-state classification. Benchmarked on the cluster-Ising model with system sizes of 9 and 15 qubits, the model achieves high classification accuracy with less than 100 training data and demonstrates robustness against variations in the training set. Further analysis reveals that the model successfully captures phase-sensitive features and characteristic physical length scales, offering a scalable and data-efficient approach for quantum phase recognition in complex many-body systems.

</details>


### [439] [Ermakov-Lewis Invariants in Stationary Bohm-Madelung Quantum Mechanics](https://arxiv.org/abs/2602.00507)
*Anand Aruna Kumar*

Main category: quant-ph

TL;DR: 论文展示了在特定条件下，量子力学中的薛定谔方程会自然地引出Ermakov-Pinney方程及其不变量，揭示了量子势的几何本质。


<details>
  <summary>Details</summary>
Motivation: 探索量子力学中Bohm-Madelung形式下的隐藏结构，特别是当哈密顿量可分离时，揭示量子势的几何本质及其与经典Ermakov-Pinney系统的联系。

Method: 将薛定谔方程转换为Bohm-Madelung形式，在哈密顿量可分离条件下分析连续性方程，得到Ermakov-Pinney型振幅方程。通过Sturm-Liouville形式和Liouville归一化，将量子势重新解释为自伴算子的曲率贡献。

Result: 发现量子势可以编码为自伴算子的曲率贡献而非额外动力学项，得到了精确的定态Bohmian振幅及其不变量，建立了基于不变量的表述，阐明了Bohmian振幅的几何本质。

Conclusion: 定态约束的Bohm-Madelung系统自然地允许变分表述，其极值保持Ermakov-Lewis不变量，为量子力学提供了新的几何解释框架，同时保持标准概率预测。

Abstract: The Ermakov Pinney equation and its associated invariant are shown to arise naturally in stationary quantum mechanics when the Schrodinger equation is expressed in Bohm Madelung form and the Hamiltonian is diagonal and separable. Under these conditions, the stationary continuity constraint induces a nonlinear amplitude equation of Ermakov Pinney type in each degree of freedom, revealing a hidden invariant structure that is independent of whether the evolution parameter is time or space.
  By reformulating the separated stationary equations in Sturm Liouville form and applying Liouville normalization, we demonstrate that the quantum potential is encoded as a curvature contribution of the self adjoint operator rather than appearing as an additional dynamical term. This correspondence preserves the standard probabilistic predictions of quantum mechanics while yielding exact stationary Bohmian amplitudes and their associated invariants. The resulting invariant-based formulation provides stationary guiding fields and clarifies the ontological status of Bohmian amplitudes as geometrically encoded structures rather than auxiliary dynamical additions. The results further show that stationary constrained Bohm Madelung systems naturally admit variational formulations whose extremals preserve the Ermakov Lewis invariant.

</details>


### [440] [First-Principles Optical Descriptors and Hybrid Classical-Quantum Classification of Er-Doped CaF$_2$](https://arxiv.org/abs/2602.00525)
*David Angel Alba Bonilla,Kerem Yurtseven,Krishan Sharma,Ragunath Chandrasekharan,Muhammad Khizar,Alireza Alipour,Dennis Delali Kwesi Wayo*

Main category: quant-ph

TL;DR: 该研究提出了一个物理信息化的经典-量子机器学习框架，用于区分原始CaF₂和Er掺杂CaF₂，使用第一性原理光学描述符，并比较了经典SVM、量子SVM和混合量子神经网络的性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个结合物理原理和机器学习的方法，利用掺杂引起的光学特征来区分材料，同时为近期量子学习模型提供基准测试平台。

Method: 构建Ca₈F₁₆和Ca₇ErF₁₆团簇，使用DFT和LR-TDDFT计算光学性质，提取可解释的光学描述符，然后分别用经典RBF核SVM、量子SVM和混合量子神经网络进行分类。

Result: 经典SVM表现最佳（ACC=0.983，ROC-AUC=0.999）；量子SVM在模拟器上ACC=0.851/0.817，在IBM量子硬件上ACC=0.733；混合量子神经网络ACC=0.93，AUC=0.96。

Conclusion: 掺杂诱导的光学指纹形成了稳健的物理特征空间，可用于基准测试量子学习模型，混合量子神经网络显示出有前景的性能，但经典方法仍占优势。

Abstract: We present a physics-informed classical-quantum machine learning framework for discriminating pristine CaF$_2$ from Er-doped CaF$_2$ using first-principles optical descriptors. Finite Ca$_8$F$_{16}$ and Ca$_7$ErF$_{16}$ clusters were constructed from the fluorite structure (a=5.46~$Å$) and treated using density functional theory (DFT) and linear-response time-dependent DFT (LR-TDDFT) within the GPAW code. Geometry optimization was performed in LCAO mode with a DZP basis and PBE exchange-correlation functional, followed by real-space finite-difference ground-state calculations with grid spacing h=0.30~$Å$ and N$_{bands}$=N$_{occ}$+20. Optical excitations up to 10~eV were obtained via the Casida formalism and converted into continuous absorption spectra using Gaussian broadening ($σ$=0.1-0.2~eV). From 1,589 energy-resolved points per system, physically interpretable descriptors including transition energy $E$, extinction coefficient $κ$, and absorption coefficient $α$ were extracted. A classical RBF-kernel support vector machine (SVM) achieves a test accuracy (ACC) of 0.983 and ROC-AUC of 0.999. Quantum support vector machines (QSVMs) evaluated on statevector and noisy simulators reach accuracies of 0.851 and 0.817, respectively, while execution on IBM quantum hardware yields a test-slice accuracy of 0.733 under finite-shot and decoherence constraints. A hybrid quantum neural network (QNN) with a 3-qubit feature map and depth-4 ansatz achieves a test accuracy of 0.93 and AUC of 0.96. Results here demonstrate that dopant-induced optical fingerprints form a robust, physically grounded feature space for benchmarking near-term quantum learning models against strong classical baselines.

</details>


### [441] [Entanglement-Dependent Error Bounds for Hamiltonian Simulation](https://arxiv.org/abs/2602.00555)
*Prateek P. Kulkarni*

Main category: quant-ph

TL;DR: 本文建立了纠缠熵与哈密顿模拟中Trotter-Suzuki乘积公式近似误差之间的紧密联系，证明了对于具有最大纠缠熵S_max的几何局域哈密顿量，一阶Trotter误差为O(t²S_max polylog(n)/r)而非最坏情况的O(t²n/r)，显著改进了资源估计。


<details>
  <summary>Details</summary>
Motivation: 标准误差分析给出最坏情况边界，对于结构化问题可能严重高估所需资源。本文旨在建立纠缠熵与Trotter误差之间的理论联系，为量子模拟提供更精确的资源估计。

Method: 结合Lieb-Robinson边界处理局域性，使用张量网络表示纠缠结构，并开发新的对易子-熵不等式，通过状态的Schmidt秩来界定嵌套对易子的期望值。

Result: 证明了对于几何局域哈密顿量，Trotter误差与最大纠缠熵S_max成正比而非系统大小n，对一维面积律系统改进约Ω̃(n²)，对二维系统改进约Ω̃(n^{3/2})。建立了体积律纠缠系统比面积律系统需要Ω̃(n)更多Trotter步骤的分离结果。

Conclusion: 纠缠熵是决定Trotter-Suzuki乘积公式误差的关键因素，为量子化学、凝聚态模拟和容错量子计算的资源估计提供了理论基础，表明结构化问题的模拟资源需求远低于最坏情况分析。

Abstract: We establish tight connections between entanglement entropy and the approximation error in Trotter-Suzuki product formulas for Hamiltonian simulation. Product formulas remain the workhorse of quantum simulation on near-term devices, yet standard error analyses yield worst-case bounds that can vastly overestimate the resources required for structured problems. For systems governed by geometrically local Hamiltonians with maximum entanglement entropy $S_\text{max}$ across all bipartitions, we prove that the first-order Trotter error scales as $\mathcal{O}(t^2 S_\text{max} \operatorname{polylog}(n)/r)$ rather than the worst-case $\mathcal{O}(t^2 n/r)$, where $n$ is the system size and $r$ is the number of Trotter steps. This yields improvements of $\tildeΩ(n^2)$ for one-dimensional area-law systems and $\tildeΩ(n^{3/2})$ for two-dimensional systems. We extend these bounds to higher-order Suzuki formulas, where the improvement factor involves $2^{pS^*/2}$ for the $p$-th order formula. We further establish a separation result demonstrating that volume-law entangled systems fundamentally require $\tildeΩ(n)$ more Trotter steps than area-law systems to achieve the same precision. This separation is tight up to logarithmic factors. Our analysis combines Lieb-Robinson bounds for locality, tensor network representations for entanglement structure, and novel commutator-entropy inequalities that bound the expectation value of nested commutators by the Schmidt rank of the state. These results have immediate applications to quantum chemistry, condensed matter simulation, and resource estimation for fault-tolerant quantum computing.

</details>


### [442] [Quantum clock and Newtonian time](https://arxiv.org/abs/2602.02077)
*Dorje C. Brody,Lane P. Hughston*

Main category: quant-ph

TL;DR: 提出一种扩展量子力学，用"量子时钟"的时间取代牛顿时间参数，推导出冯·诺依曼方程及其修正形式


<details>
  <summary>Details</summary>
Motivation: 扩展标准量子力学，将牛顿时间参数替换为量子时钟显示的时间，探索时间测量的量子本质

Method: 定义量子时钟的三个特性：(a)时间非递减，(b)随机滴答且滴答大小随机，(c)平均显示牛顿时间。推导密度矩阵演化方程，以伽马分布为例详细分析修正项

Result: 主要项给出冯·诺依曼方程，修正项由哈密顿量生成的Lindblad方程描述，存在高阶项进一步推广方程。通过原子钟精度极限推导量子时钟模型参数的下界

Conclusion: 量子时钟模型为量子力学提供了时间参数的新视角，推导出标准量子力学方程的修正形式，并与实际时钟精度建立联系

Abstract: An extension of standard quantum mechanics is proposed in which the Newtonian time parameter appearing in the unitary evolution operator is replaced with the time shown by a `quantum clock'. A quantum clock is defined by the following properties: (a) the time that the clock shows is non-decreasing, (b) the clock ticks at random with random tick sizes, and (c) on average the clock shows the Newtonian time. We show that the leading term in the evolution equation for the density matrix associated with any quantum clock model gives the von Neumann equation. Modifications to the von Neumann equation are worked out in detail in a parametric family of examples for which the tick sizes have a gamma distribution. The leading correction to the von Neumann equation is given by the Lindblad equation generated by the Hamiltonian, but there are higher-order terms that generalize the von Neumann equation and the Lindblad equation. Lower bounds on the parameters of these quantum clock models are derived by use of the precision limit of an atomic clock.

</details>


### [443] [Geometric Optimization for Tight Entropic Uncertainty Relations](https://arxiv.org/abs/2602.00595)
*Ma-Cheng Yang,Cong-Feng Qiao*

Main category: quant-ph

TL;DR: 提出一种几何优化方法，通过量子概率空间的外逼近技术，为有限维量子系统中的一般测量提供紧致的熵不确定性界限。


<details>
  <summary>Details</summary>
Motivation: 熵不确定性关系在量子信息理论中具有基础性作用，但为一般可观测量确定最优（紧致）的熵不确定性关系仍然是一个艰巨挑战，目前仅在少数特殊情况下实现。受Schwonnek等人工作的启发，需要开发更通用的方法来获得紧致的熵不确定性界限。

Method: 将熵不确定性关系的确定重新表述为量子概率空间上的几何优化问题，采用有效的外逼近方法，为有限维量子系统中的一般测量提供具有预定数值精度的紧致熵不确定性界限。

Result: 该方法能够产生紧致的熵不确定性界限，通过基准测试验证了其相对于现有解析界限和基于主序化界限的优势，并在量子导引应用中展示了实际优势。

Conclusion: 提出的几何优化框架为解决一般测量的熵不确定性关系问题提供了有效工具，通过外逼近技术实现了紧致界限的计算，在量子信息处理中具有实际应用价值。

Abstract: Entropic uncertainty relations play a fundamental role in quantum information theory. However, determining optimal (tight) entropic uncertainty relations for general observables remains a formidable challenge and has so far been achieved only in a few special cases. Motivated by Schwonnek \emph{et al.} [PRL \textbf{119}, 170404 (2017)], we recast this task as a geometric optimization problem over the quantum probability space. This procedure leads to an effective outer-approximation method that yields tight entropic uncertainty bounds for general measurements in finite-dimensional quantum systems with preassigned numerical precision. We benchmark our approach against existing analytical and majorization-based bounds, and demonstrate its practical advantage through applications to quantum steering.

</details>


### [444] [Practical Quantum Reservoir Computing in Rydberg Atom Arrays](https://arxiv.org/abs/2602.00610)
*Dong-Sheng Liu,Qing-Xuan Jie,Chang-Ling Zou,Xi-Feng Ren,Guang-Can Guo*

Main category: quant-ph

TL;DR: SS-QRC比MS-QRC在噪声环境下更稳健，适合近期量子计算应用


<details>
  <summary>Details</summary>
Motivation: 比较不同量子储层计算架构在实际约束下的性能，为近期量子平台寻找更实用的方案

Method: 使用里德堡原子阵列实现SS-QRC和MS-QRC架构，通过随机测量工具箱减少测量开销，分析采样噪声对性能的影响

Result: MS-QRC对系统动力学相和退相干高度敏感，采样噪声破坏其收敛性，显著降低信息处理能力；SS-QRC在各种任务中保持高信息处理能力和准确性

Conclusion: SS-QRC因其对系统配置和统计噪声的鲁棒性，是近期实际应用的优选方案

Abstract: Quantum reservoir computing (QRC) is a promising quantum machine learning framework for near-term quantum platforms, yet the performance of different QRC architectures under realistic constraints remains largely unexplored. Here, we provide a comparative numerical study of single-step-QRC (SS-QRC) and multi-step-QRC (MS-QRC) architectures implemented on a Rydberg atom array. We demonstrate that while MS-QRC performance is highly sensitive to the underlying dynamical phase of matter and decoherence, SS-QRC exhibits greater robustness. Using the randomized measurement toolbox to mitigate measurement overhead, we reveal that sampling noise undermines the convergence property required for MS-QRC. This leads to a significant reduction in the information processing capacity (IPC) of MS-QRC, deteriorating its performance on nonlinear time-series benchmarks. In contrast, SS-QRC maintains high IPC and accuracy across both temporal and non-temporal tasks. Our results suggest SS-QRC as a preferred candidate for near-term practical applications due to its resilience to system configurations and statistical noise.

</details>


### [445] [Near-losslesss method for generating thermal photon-bunched light](https://arxiv.org/abs/2602.00633)
*Xi Jie Yeo,Darren Ming Zhi Koh,Justin Yu Xiang Peh,Christian Kurtsiefer,Peng Kian Tan*

Main category: quant-ph

TL;DR: 提出一种高效生成光子聚束的新方法，转换效率比其他方法高近9个数量级，解决了现有热光源在传感应用中的实际限制问题。


<details>
  <summary>Details</summary>
Motivation: 热光源的光子聚束现象在测距、时钟同步、非视距成像等传感应用中具有潜力，但现有技术存在两个主要问题：要么相干时间太短无法被光电探测器时间分辨，要么亮度太低无法承受实际返回损耗。

Method: 开发了一种低损耗的光子聚束生成方法，实现了接近9个数量级的转换效率提升。

Result: 成功演示了该方法，转换效率比许多其他聚束过程高出近9个数量级。

Conclusion: 该方法解决了热光源光子聚束在实际传感应用中的关键限制，为相关技术实现提供了可行方案。

Abstract: Thermal light sources exhibiting photon bunching have been suggested for sensing applications that exploit timing correlations of stationary light, including range finding, clock synchronization, and non-line-of-sight imaging. However, these proposals have remained unrealized in practice because available sources of photon bunching either possess coherence times too short to be timing resolved by photodetectors, or produce brightness levels too low to tolerate realistic return losses. In this work, we demonstrate a low-loss method for generating photon bunching with a conversion efficiency nearly 9 orders of magnitude higher than that achieved by many other bunching processes.

</details>


### [446] [Pauli Cloners for Pauli Channels](https://arxiv.org/abs/2602.00646)
*S. F. Kerstan,M. Gallezot,T. Decker,M. Braun,N. Hegemann*

Main category: quant-ph

TL;DR: 提出一种用于N量子比特寄存器一对二克隆的量子电路架构，扩展了Niu-Griffiths架构到多量子比特系统，探索了泡利克隆与噪声模型的关系。


<details>
  <summary>Details</summary>
Motivation: 扩展量子克隆技术到多量子比特系统，探索泡利克隆与量子通信中噪声模型的关系，特别是在量子密钥分发中的应用。

Method: 提出一种量子电路架构，将Niu-Griffiths架构扩展到多量子比特系统，实现泡利克隆器的一对二克隆。在单量子比特情况下，提供了非对称通用克隆器、相位协变克隆器和偏置克隆器的具体构造。

Result: 成功构建了N量子比特寄存器的一对二克隆架构，探索了泡利错误、互无偏基和泡利克隆之间的基本关系，展示了如何针对特定噪声模型定制泡利克隆器。

Conclusion: 该量子电路架构为多量子比特系统的克隆提供了有效方法，在量子通信特别是量子密钥分发中，泡利克隆器可以根据特定噪声模型进行定制，具有实际应用价值。

Abstract: We present a quantum circuit architecture for the one-to-two cloning of $N$-qubit registers. It implements the broad class of Pauli cloners by extending the Niu--Griffiths architecture to multi-qubit systems. In the single-qubit case, we provide explicit constructions for asymmetric universal, phase covariant and biased cloners. We explore the fundamental relationship between Pauli errors, mutually unbiased bases and Pauli cloning. Furthermore, we demonstrate how Pauli cloners can be tailored to specific noise models in the context of quantum communication, especially quantum key distribution.

</details>


### [447] [Fidelity and quantum geometry approach to Dirac exceptional points in diamond nitrogen-vacancy centers](https://arxiv.org/abs/2602.00666)
*Chia-Yi Ju,Gunnar Möller,Yu-Chin Tzeng*

Main category: quant-ph

TL;DR: 本文研究了金刚石中氮空位中心实现的狄拉克例外点（EPs）的量子几何特性，利用保真度敏感性作为探针，揭示了狄拉克EPs在宇称-时间对称未破缺相中诱导的几何奇异性及其各向异性发散行为。


<details>
  <summary>Details</summary>
Motivation: 狄拉克例外点是一种新型非厄米奇点，与传统例外点不同，它完全位于宇称-时间对称未破缺相中并呈现线性能量色散。研究其量子几何特性对于理解非厄米系统中的临界现象以及开发量子控制和传感应用具有重要意义。

Method: 在金刚石氮空位中心的理论模型中实现狄拉克例外点，使用保真度敏感性作为探针来研究其量子几何特性。通过分析保真度敏感性在狄拉克例外点附近的行为，特别是其实部的发散特性。

Result: 尽管没有对称性破缺相变，狄拉克例外点诱导了显著的几何奇异性，保真度敏感性的实部发散至负无穷，这作为非厄米临界性的特征。关键发现是这种发散表现出明显的各向异性：沿非互易耦合方向发散，而沿失谐轴保持有限，与传统例外点的全方位发散形成鲜明对比。

Conclusion: 研究提供了狄拉克例外点附近保真度探针的全面图像，强调了参数方向性在利用狄拉克例外点进行量子控制和传感应用中的关键作用，证实了保真度在表征非厄米例外点方面的有效性。

Abstract: Dirac exceptional points (EPs) represent a novel class of non-Hermitian singularities that, unlike conventional EPs, reside entirely within the parity-time unbroken phase and exhibit linear energy dispersion. Here, we theoretically investigate the quantum geometry of Dirac EPs realized in nitrogen-vacancy centers in diamond, utilizing fidelity susceptibility as a probe. We demonstrate that despite the absence of a symmetry-breaking phase transition, the Dirac EP induces a pronounced geometric singularity, confirming the validity of fidelity in characterizing non-Hermitian EPs. Specifically, the real part of the fidelity susceptibility diverges to negative infinity, which serves as a signature of non-Hermitian criticality. Crucially, however, we reveal that this divergence exhibits a distinct anisotropy, diverging along the non-reciprocal coupling direction while remaining finite along the detuning axis. This behavior stands in stark contrast to the omnidirectional divergence observed in conventional EPs. Our findings provide a comprehensive picture of the fidelity probe near the Dirac EP, highlighting the critical role of parameter directionality in exploiting Dirac EPs for quantum control and sensing applications.

</details>


### [448] [High-resolution wide-field magnetic imaging with sparse sampling using nitrogen-vacancy centers](https://arxiv.org/abs/2602.00679)
*Keqing Liu,Jiazhao Tian,Bokun Duan,Hao Zhang,Kangze Li,Guofeng Zhang,Fedor Jelezko,Ressa S. Said,Jianming Cai,Liantuan Xiao*

Main category: quant-ph

TL;DR: 提出一种稀疏采样策略，利用贝叶斯估计框架从少量测量点重建高分辨率宽场NV磁图像，显著提高成像效率


<details>
  <summary>Details</summary>
Motivation: 传统单NV扫描磁力计分辨率高但采集慢，宽场NV集成显微镜可并行读取但受光学衍射极限和传感器-样品间距限制。需要平衡空间分辨率与采集时间

Method: 采用稀疏采样策略和均值调整贝叶斯估计（MABE）框架，从少量采样点重建高分辨率图像。结合优化的动态解耦脉冲序列提高磁场灵敏度

Result: MABE框架能从仅25个采样点重建10000像素图像，对典型平滑场分布的SSIM值超过0.999。优化的动态解耦脉冲序列使磁场灵敏度提高约两倍

Conclusion: 该方法为更快、更可扩展的磁成像架构提供了途径，可扩展到点扫描NV传感器和其他磁力计平台（如SQUID、霍尔探头、磁隧道结）

Abstract: Nitrogen-vacancy (NV) centers in diamond enable quantitative magnetic imaging, yet practical implementations must balance spatial resolution against acquisition time (and thus per-pixel sensitivity). Single-NV scanning magnetometry achieves genuine nanoscale resolution, nonetheless requires typically a slow pixel-by-pixel acquisition. Meanwhile, wide-field NV-ensemble microscopy provides parallel readout over a large field of view, however is jointly limited by the optical diffraction limit and the sensor-sample standoff. Here, we present a sparse-sampling strategy for reconstructing high-resolution wide-field images from only a small number of measurements. Using simulated NV-ensemble detection of ac magnetic fields, we show that a mean-adjusted Bayesian estimation (MABE) framework can reconstruct 10000-pixel images from only 25 sampling points, achieving SSIM values exceeding 0.999 for representative smooth field distributions, while optimized dynamical-decoupling pulse sequences yield an approximately twofold improvement in magnetic-field sensitivity. The method further clarifies how sampling patterns and sampling density affect reconstruction accuracy and suggests a route toward faster and more scalable magnetic-imaging architectures that may extend to point-scanning NV sensors and other magnetometry platforms, such as SQUIDs, Hall probes, and magnetic tunnel junctions.

</details>


### [449] [Enhanced Phase Estimation via Photon-Added Two-Mode Squeezed States and Kerr Nonlinearity](https://arxiv.org/abs/2602.00700)
*Zekun Zhao,Qingqian Kang,Teng Zhao,Cunjin Liu,Liyun Hu*

Main category: quant-ph

TL;DR: 该论文研究了一种包含Kerr非线性相位调制器的马赫-曾德尔干涉仪，使用四波混频产生的光子添加双模压缩相干态作为输入，实现了超越标准量子极限甚至海森堡极限的相位测量精度。


<details>
  <summary>Details</summary>
Motivation: 量子计量学利用量子资源实现超越经典极限的测量精度。本研究旨在探索如何通过非线性光学效应和特殊量子态进一步提高干涉测量的灵敏度，克服传统量子计量中的限制。

Method: 采用马赫-曾德尔干涉仪结构，其中包含Kerr非线性相位调制器。输入态为通过四波混频产生的光子添加双模压缩相干态。通过分析光子添加阶数和输入资源强度对相位灵敏度、量子Fisher信息和量子Cramér-Rao界的影响。

Result: 研究表明，增加光子添加阶数和输入资源强度能系统性提升相位灵敏度、量子Fisher信息和量子Cramér-Rao界。该系统不仅超越标准量子极限，在线性相位偏移下接近或超过海森堡极限，而Kerr非线性效应使其能够突破超海森堡极限。此外，该方案对光子损耗具有增强的鲁棒性。

Conclusion: 提出的方案为高精度量子计量应用提供了有前景的方法，通过结合非线性光学效应和特殊量子态，实现了超越传统量子极限的测量性能，并展现出良好的抗损耗能力。

Abstract: Quantum metrology harnesses quantum resources to achieve measurement precision beyond classical limits. This work investigates a Mach-Zehnder interferometer incorporating a Kerr nonlinear phase shifter, using photon-added two-mode squeezed coherent states generated via four-wave mixing as input. This study demonstrates that increasing both the photon-addition order and input resource strength systematically enhances phase sensitivity, quantum Fisher information, and the quantum Cramér-Rao bound. The system not only surpasses the standard quantum limit but also approaches or exceeds the Heisenberg limit with linear phase shifts, while Kerr nonlinearity enables overcoming the super-Heisenberg limit. The proposed scheme exhibits enhanced robustness against photon loss, providing a promising approach for high-precision quantum metrology applications.

</details>


### [450] [Analysis of Hessian Scaling for Local and Global Costs in Variational Quantum Algorithm](https://arxiv.org/abs/2602.00783)
*Yihan Huang,Yangshuai Wang*

Main category: quant-ph

TL;DR: 论文研究了变分量子算法中Hessian矩阵在随机初始化时的统计可分辨性，揭示了两种不同的缩放机制：全局目标函数需要指数级测量次数，而局部目标函数只需多项式级测量次数。


<details>
  <summary>Details</summary>
Motivation: 虽然贫瘠高原通常表现为梯度消失，但基于曲率的优化方法的可行性从根本上取决于Hessian矩阵的统计可分辨性。本研究旨在量化变分量子算法在随机初始化时Hessian矩阵的逐项可分辨性。

Method: 利用精确的二阶参数平移规则，推导出将Hessian矩阵条目方差简化为移位成本评估的有限协方差二次形式的结构表示。通过理论分析和数值实验验证不同系统规模和电路深度下的缩放行为。

Result: 发现两种不同的缩放机制：对于全局目标函数，Hessian方差呈指数抑制，需要O(e^{αn})的测量次数；对于有界深度电路中的逐项局部目标函数，方差呈多项式衰减，仅需O(poly(n))测量次数。

Conclusion: 研究结果为二阶方法在初始化时的计算可处理性提供了严格判据：全局目标函数的曲率信息统计上难以获取，而局部目标函数的曲率信息在多项式测量次数下仍可统计获取。

Abstract: Barren plateaus are typically characterized by vanishing gradients, yet the feasibility of curvature-based optimization fundamentally relies on the statistical resolvability of the Hessian matrix. In this work, we quantify the entrywise resolvability of the Hessian for Variational Quantum Algorithms at random initialization. By leveraging exact second-order parameter-shift rules, we derive a structural representation that reduces the variance of Hessian entries to a finite covariance quadratic form of shifted cost evaluations. This framework reveals two distinct scaling regimes that govern the sample complexity required to resolve Hessian entries against shot noise. For global objectives, we prove that Hessian variances are exponentially suppressed, implying that the number of measurement shots must scale as $O(e^{αn})$ with the number of qubits $n$ to maintain a constant signal-to-noise ratio. In contrast, for termwise local objectives in bounded-depth circuits, the variance decay is polynomial and explicitly controlled by the backward lightcone growth on the interaction graph, ensuring that curvature information remains statistically accessible with $O(\mathrm{poly}(n))$ shots. Extensive numerical experiments across varying system sizes and circuit depths demonstrate these theoretical bounds and the associated sampling costs. Our results provide a rigorous criterion for the computational tractability of second-order methods at initialization.

</details>


### [451] [Higher-order transformations of bidirectional quantum processes](https://arxiv.org/abs/2602.00856)
*Luca Apadula,Alessandro Bisio,Giulio Chiribella,Paolo Perinotti,Kyrylo Simonov*

Main category: quant-ph

TL;DR: 本文研究了双向量子设备的输入输出不定性，建立了基于双随机量子通道的高阶变换层次结构，揭示了局部输入输出方向和全局因果顺序的混合不定性。


<details>
  <summary>Details</summary>
Motivation: 双向量子设备（输入输出端口可互换）由双随机量子通道描述。最近研究发现这些设备可以以与确定输入输出方向不兼容的方式使用，产生"输入输出不定性"现象。本文旨在表征最一般的输入输出不定性形式。

Method: 通过构建基于双随机量子通道变换的高阶变换层次结构来表征输入输出不定性。该层次结构包含不同级别的变换：一些级别对应在确定因果顺序下组合双随机通道，但每个通道使用不确定的输入输出方向；其他级别则涉及局部输入输出方向和全局因果顺序的混合不定性。

Result: 建立了完整的输入输出不定性层次结构，揭示了量子理论中时间对称变体下最广泛的物理过程集合。该层次结构代表了在状态变换限制为双随机通道的量子理论框架下可能的最大物理过程集合。

Conclusion: 本文系统地表征了双向量子设备输入输出不定性的最一般形式，建立了高阶变换层次结构，为理解量子理论的时间对称变体提供了基础框架，并揭示了局部方向不定性与全局因果顺序不定性之间的复杂关系。

Abstract: Bidirectional devices are devices for which the roles of the input and output ports can be exchanged. Mathematically, these devices are described by bistochastic quantum channels, namely completely positive linear maps that are both trace-preserving and identity-preserving. Recently, it has been shown that bidirectional quantum devices can, in principle, be used in ways that are incompatible with a definite input-output direction, giving rise to a new phenomenon called input-output indefiniteness. Here we characterize the most general forms of input-output indefiniteness, associated with a hierarchy of higher-order transformations built from transformations of bistochastic quantum channels. Some levels of the hierarchy correspond to transformations that combine bistochastic channels in a definite causal order, while generally using each channel in an indefinite input-output direction. For other levels of the hierarchy, the indefiniteness can involve both the local input-output direction of each process and the global causal order among the processes. On the foundational side, the hierarchy of higher-order transformations characterized here can be regarded as the largest set of physical processes compatible with a time-symmetric variant of quantum theory, where the possible state transformations are restricted to bistochastic channels.

</details>


### [452] [Sublinear Time Quantum Algorithm for Attention Approximation](https://arxiv.org/abs/2602.00874)
*Zhao Song,Jianfei Xue,Jiahao Zhang,Lichen Zhang*

Main category: quant-ph

TL;DR: 提出首个量子数据结构，在子线性时间内近似注意力矩阵的行，预处理时间与n的平方根相关，查询时间与n无关。


<details>
  <summary>Details</summary>
Motivation: 传统注意力计算需要O(n²)时间，限制了大规模语言模型的应用。现有经典近似方法虽然能达到O(nd)时间，但仍不够高效。量子计算提供了进一步加速的可能性。

Method: 采用量子Nyström近似指数核，量子多元均值估计计算D矩阵，量子杠杆得分采样进行V矩阵乘法。通过量子数据结构支持仅使用行查询来近似注意力矩阵的行。

Result: 预处理时间：Õ(ε⁻¹n⁰·⁵(sλ²·⁵ + sλ¹·⁵d + α⁰·⁵d))，其中sλ为指数核的λ统计维度，α为V的行失真度。每个行查询时间：Õ(sλ² + sλd)，实现了相对于n的子线性时间。

Conclusion: 这是首个能够在子线性时间内近似注意力矩阵行的量子数据结构，为大规模语言模型的量子加速提供了新途径，特别适合处理高维数据和大规模序列。

Abstract: Given the query, key and value matrices $Q, K, V\in \mathbb{R}^{n\times d}$, the attention module is defined as $\mathrm{Att}(Q, K, V)=D^{-1}AV$ where $A=\exp(QK^\top/\sqrt{d})$ with $\exp(\cdot)$ applied entrywise, $D=\mathrm{diag}(A{\bf 1}_n)$. The attention module is the backbone of modern transformers and large language models, but explicitly forming the softmax matrix $D^{-1}A$ incurs $Ω(n^2)$ time, motivating numerous approximation schemes that reduce runtime to $\widetilde O(nd)$ via sparsity or low-rank factorization.
  We propose a quantum data structure that approximates any row of $\mathrm{Att}(Q, K, V)$ using only row queries to $Q, K, V$. Our algorithm preprocesses these matrices in $\widetilde{O}\left( ε^{-1} n^{0.5} \left( s_λ^{2.5} + s_λ^{1.5} d + α^{0.5} d \right) \right)$ time, where $ε$ is the target accuracy, $s_λ$ is the $λ$-statistical dimension of the exponential kernel defined by $Q$ and $K$, and $α$ measures the row distortion of $V$ that is at most $d/{\rm srank}(V)$, the stable rank of $V$. Each row query can be answered in $\widetilde{O}(s_λ^2 + s_λd)$ time.
  To our knowledge, this is the first quantum data structure that approximates rows of the attention matrix in sublinear time with respect to $n$. Our approach relies on a quantum Nyström approximation of the exponential kernel, quantum multivariate mean estimation for computing $D$, and quantum leverage score sampling for the multiplication with $V$.

</details>


### [453] [Universal Quantum Birthmark: Ghost of the quantum past](https://arxiv.org/abs/2602.00891)
*Ivy Xiaoya,Anton M. Graf,Eric J. Heller,Joonas Keski-Rahkonen*

Main category: quant-ph

TL;DR: 量子系统即使具有完全混沌的随机矩阵谱，也会永久保留其初始条件的记忆，这种"量子胎记"效应表现为非稳态态的长时返回概率增强，仅依赖于全局对称性类别和希尔伯特空间维度。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为混沌量子系统会完全忘记初始条件并达到热平衡，但研究发现量子动力学保留了初始条件的永久记忆，这挑战了经典遍历性和热化预期，需要建立完整的理论框架来解释这一普遍现象。

Method: 发展了完整的理论框架来分析量子胎记效应，该理论仅依赖于全局对称性类别和可访问的希尔伯特空间维度，而不依赖于微观动力学细节，揭示了对称性控制的量子演化起源印记。

Result: 量子演化保留了不可避免的、对称性控制的起源印记，表现为长时返回概率的增强，这一效应普遍存在于具有随机矩阵谱的混沌系统中，挑战了经典遍历性和热化预期。

Conclusion: 量子动力学即使在混沌系统中也保留了初始条件的永久记忆，这种量子胎记效应仅由全局对称性和希尔伯特空间维度决定，表明量子演化具有经典遍历性无法解释的起源印记，对热化场景提出了根本性质疑。

Abstract: Quantum dynamics retains a permanent and universal memory of its initial conditions, even in systems whose spectra display fully chaotic, random-matrix behavior. This effect, known as the quantum birthmark, appears as an enhancement of the long-time return probability of any non-stationary state compared to the overlap with a typical ergodic state. In this work, we develop the full theoretical foundation for this universal contribution that depends only on the global symmetry class and accessible Hilbert-space dimension, not on the microscopic dynamics. Our findings reveal that quantum evolution preserves an unavoidable, symmetry-controlled imprint of its origin, a quantum effect calling into question classical expectations of ergodicity and the resulting thermalization scenarios.

</details>


### [454] [Asymmetry and dynamical criticality](https://arxiv.org/abs/2602.00900)
*Andesson B. Nascimento,Lucas Chibebe Céleri*

Main category: quant-ph

TL;DR: 该论文建立了量子不对称性与动力学量子相变之间的直接联系，证明不对称性单调量可以作为动力学量子临界性的鲁棒且物理透明的指标。


<details>
  <summary>Details</summary>
Motivation: 对称性在平衡和非平衡相变中都起着核心作用，但在动力学量子相变中对对称性的定量表征仍然是一个开放挑战。需要建立对称性性质与量子不对称性度量之间的直接联系。

Method: 聚焦于淬火后的Lipkin-Meshkov-Glick模型，研究与集体自旋生成元相关的不对称性度量，分析时间平均不对称性，并探索不对称性生成与热力学不可逆性之间的定量联系。

Result: 不对称性度量能够忠实捕捉动力学量子相变的起始，反映底层对称性的动力学恢复或破缺。时间平均不对称性显示出清晰的动力学临界点特征，与动力学序参数和熵产生行为密切相关。不对称性峰值与相变过程中的最大熵产生相一致。

Conclusion: 不对称性成为连接对称性、信息论量子和非平衡热力学的统一概念，为理解超越传统序参数的临界动力学提供了强大框架。

Abstract: Symmetries play a central role in both equilibrium and nonequilibrium phase transitions, yet their quantitative characterization in dynamical quantum phase transitions (DQPTs) remains an open challenge. In this work, we establish a direct connection between symmetry properties of a many-body model and measures of quantum asymmetry, showing that asymmetry monotones provide a robust and physically transparent indicator of dynamical quantum criticality. Focusing on the quenched Lipkin-Meshkov-Glick model, we demonstrate that asymmetry measures associated with collective spin generators faithfully capture the onset of DQPTs, reflecting the dynamical restoration or breaking of underlying symmetries. Remarkably, the time-averaged asymmetry exhibits clear signatures of the dynamical critical point, in close correspondence with both the dynamical order parameter and the behavior of entropy production. We further uncover a quantitative link between asymmetry generation and thermodynamic irreversibility, showing that peaks in asymmetry coincide with maximal entropy production across the transition. Our results position asymmetry as a unifying concept bridging symmetry, information-theoretic quantifiers, and nonequilibrium thermodynamics in dynamical quantum phase transitions, providing a powerful framework for understanding critical dynamics beyond traditional order parameters.

</details>


### [455] [Noise Resilient 1SDIQKD for Practical Quantum Networks](https://arxiv.org/abs/2602.00916)
*Syed M Arslan,Muhammad T Rahim,Asad Ali,Hashir Kuniyil,Saif Al Kuwari*

Main category: quant-ph

TL;DR: 本文扩展了单边设备无关量子密钥分发(1SDI-QKD)到含噪声信道，分析了振幅阻尼、退相位和退极化噪声的影响，发现退相位噪声最可容忍，而振幅阻尼和退极化噪声显著提高效率要求。研究表明安全性由steering violation而非纠缠决定，并展示了BBPSSW纠缠纯化协议可恢复安全密钥率。


<details>
  <summary>Details</summary>
Motivation: 先前对1SDI-QKD的分析假设理想信道，忽略了实际噪声源。需要研究噪声对安全密钥率和效率要求的影响，为实际部署提供指导。

Method: 扩展1SDI-QKD框架以包含振幅阻尼、退相位和退极化噪声，量化其对安全密钥率的影响。集成BBPSSW纠缠纯化协议来缓解噪声效应，分析资源开销。

Result: 发现噪声容忍度层次：退相位噪声最可容忍（70%效率下可承受30%噪声），而振幅阻尼和退极化噪声要求效率超过90%。安全性在仍有显著纠缠（C≈0.7-0.8）时已丧失，表明steering violation而非纠缠决定安全性。2-4轮BBPSSW纯化可在不安全区域恢复正密钥率，但过度纯化会适得其反。

Conclusion: 该研究为在城市规模量子网络中部署1SDI-QKD建立了实际边界，强调了噪声容忍度和steering violation的关键作用，并展示了纠缠纯化作为噪声缓解策略的有效性。

Abstract: One-sided device-independent quantum key distribution (1SDI-QKD) offers a practical middle ground between fully device-independent protocols and standard QKD, achieving security with detection efficiencies as low as 50.1\% on the untrusted side. However, prior analyses assumed idealized channels, neglecting realistic noise sources. We extend the 1SDI-QKD framework to include amplitude damping, dephasing, and depolarizing noise, quantifying their impact on secure key rates and efficiency requirements. Our results reveal a clear noise hierarchy: dephasing is most tolerable (secure keys achievable at 70\% efficiency with 30\% noise), while amplitude damping and depolarizing noise dramatically elevate requirements to over 90\%. Crucially, we find that security is lost while substantial entanglement remains (concurrence $C \approx 0.7$--$0.8$), demonstrating that steering violation, not merely entanglement, determines 1SDI-QKD security. To mitigate noise effects, we integrate the BBPSSW entanglement purification protocol, showing that 2--4 rounds can restore positive key rates in otherwise insecure regimes. Our resource overhead analysis reveals that effective key rates peak at moderate purification depths; excessive rounds become counterproductive. These findings establish practical boundaries for deploying 1SDI-QKD over metropolitan-scale quantum networks.

</details>


### [456] [Electro-optic conversion of itinerant Fock states](https://arxiv.org/abs/2602.00928)
*Thomas Werner,Erfan Riyazi,Samarth Hawaldar,Rishabh Sahu,Georg Arnold,Paul Falthansl-Scheinecker,Jennifer A. Sánchez Naranjo,Dante Loi,Lucky N. Kapoor,Martin Zemlicka,Liu Qiu,Andrei Militaru,Johannes M. Fink*

Main category: quant-ph

TL;DR: 该研究实现了从超导量子比特生成单微波光子，并通过低噪声微波-光转换将其上转换为电信光子，为分布式量子计算提供了关键技术。


<details>
  <summary>Details</summary>
Motivation: 超导量子比特需要毫开尔文工作温度，这成为规模化瓶颈。虽然模块化架构可以使用光纤连接不同低温节点，但超导电路没有相干光学跃迁，且微波-光转换尚未展示非经典光子态。

Method: 从超导量子比特按需生成8.9 GHz的微波单光子，使用转换器将其上转换为193.4 THz的电信光子，转换器添加噪声低于0.012量子，并通过光子计数测量信号噪声比。

Result: 成功实现了微波单光子的生成和层析重建，上转换后电信光子的信噪比高达5.1±1.1，表征了吞吐量与噪声之间的权衡关系。

Conclusion: 该工作为分布式量子技术和异构量子系统中的超导设备提供了关键技术路径，特别是为预示纠缠分布和门隐形传态奠定了基础。

Abstract: Superconducting qubits are a leading candidate for utility-scale quantum computing due to their fast gate speeds and steadily decreasing error rates. The requirement for millikelvin operating temperatures, however, creates a significant scaling bottleneck. Modular architectures using optical fiber links could bridge separate cryogenic nodes, but superconducting circuits do not have coherent optical transitions and microwave-to-optical conversion has not been shown for any non-classical photon state. In this work, we demonstrate the on-demand generation and tomographic reconstruction of itinerant single microwave photons at 8.9 GHz from a superconducting qubit. We upconvert this non-Gaussian state with a transducer added noise below 0.012 quanta and count the converted telecom photons at 193.4 THz with a signal-to-noise ratio of up to 5.1$\pm$1.1. We characterize the trade-offs between throughput and noise, and establish a viable path toward heralded entanglement distribution and gate teleportation. Looking ahead, these results empower existing superconducting devices to take a key role in distributed quantum technologies and heterogeneous quantum systems.

</details>


### [457] [A Deflationary Account of Quantum Theory and its Implications for the Complex Numbers](https://arxiv.org/abs/2602.01043)
*Jacob A. Barandes*

Main category: quant-ph

TL;DR: 论文认为量子理论需要复数是因为希尔伯特空间形式是马尔可夫嵌入的特殊情况，复数确保了这种嵌入的有效性。


<details>
  <summary>Details</summary>
Motivation: 探讨量子理论为何需要复数，试图从更基础的随机过程角度理解量子力学的数学结构。

Method: 将希尔伯特空间形式视为一般马尔可夫嵌入方法的特例，提出不可分解释，将量子系统视为在经典构型空间中展开的不可分随机过程。

Result: 复数成为确保希尔伯特空间形式确实是马尔可夫嵌入的必要条件，波函数等希尔伯特空间要素被降级为不具有本体论地位。

Conclusion: 量子理论需要复数是因为复数确保了希尔伯特空间形式作为马尔可夫嵌入的有效性，这为理解量子力学的数学基础提供了新视角。

Abstract: Why does quantum theory need the complex numbers? With a view toward answering this question, this paper argues that the usual Hilbert-space formalism is a special case of the general method of Markovian embeddings. This paper then describes the indivisible interpretation of quantum theory, according to which a quantum system can be regarded as an indivisible stochastic process unfolding in an old-fashioned configuration space, with wave functions and other exotic Hilbert-space ingredients demoted from having an ontological status. The complex numbers end up being necessary to ensure that the Hilbert-space formalism is indeed a Markovian embedding.

</details>


### [458] [The Quantum Learning Menagerie (A survey on Quantum learning for Classical concepts)](https://arxiv.org/abs/2602.01054)
*Sagnik Chatterjee*

Main category: quant-ph

TL;DR: 该论文综述了量子学习理论领域的结果，重点关注PAC框架下量子编码经典概念的学习，强调经典与量子学习在查询、样本和时间复杂度上的分离，并提出了23个开放问题。


<details>
  <summary>Details</summary>
Motivation: 整合量子学习理论领域的所有已知结果，特别是在PAC框架下学习量子编码经典概念的研究，强调经典与量子学习之间的复杂度分离，并指出当前理解的局限性。

Method: 采用综述研究方法，系统整理和分析量子学习理论领域的现有成果，重点关注不同标注预言查询访问下的学习复杂度分离问题。

Result: 论文整合了量子学习理论领域的已知结果，明确了经典与量子学习在查询、样本和时间复杂度上的分离现象，并识别了当前研究中的知识空白。

Conclusion: 量子学习理论在PAC框架下展现出显著的复杂度优势，但该领域仍存在许多未解问题，论文通过提出23个开放问题为未来研究指明了方向。

Abstract: This paper surveys various results in the field of Quantum Learning theory, specifically focusing on learning quantum-encoded classical concepts in the Probably Approximately Correct (PAC) framework. The cornerstone of this work is the emphasis on query, sample, and time complexity separations between classical and quantum learning that emerge under learning with query access to different labeling oracles. This paper aims to consolidate all known results in the area under the above umbrella and underscore the limits of our understanding by leaving the reader with 23 open problems.

</details>


### [459] [Suppression of Decoherence at Exceptional Transitions](https://arxiv.org/abs/2602.01123)
*Mei-Lin Li,Zuo Wang,Liang He*

Main category: quant-ph

TL;DR: 非厄米环境中的异常点可以显著抑制退相干，与传统厄米临界点增强退相干的范式相反


<details>
  <summary>Details</summary>
Motivation: 传统厄米临界点普遍增强量子相干性损失，但非厄米环境中的退相干行为尚未被充分探索。研究非厄米临界性如何影响退相干，为开放量子系统和量子技术中的相干性控制提供新途径。

Method: 研究量子比特耦合到非厄米自旋链和相互作用超冷费米气体系统，分析接近异常点时退相干行为的变化。通过平衡厄米和非厄米系统-环境耦合，探索退相干抑制机制。

Result: 发现接近异常点可以增强或强烈抑制退相干，取决于厄米和非厄米耦合的平衡。当这两种耦合可比时，异常转变处退相干被显著抑制。这种现象源于非厄米简并附近环境基态的独特响应，并在多个模型中表现出鲁棒性。

Conclusion: 异常点可作为抑制退相干的具体机制，非厄米临界性为开放量子系统和量子技术中的相干性控制提供了新途径。预测的退相干抑制效应可在当前数字量子模拟平台上直接观测。

Abstract: Decoherence is strongly influenced by environmental criticality, with conventional Hermitian critical points universally enhancing the loss of quantum coherence. Here we show that this paradigm is fundamentally altered in non-Hermitian environments. Focusing on qubits coupled to non-Hermitian spin chains and interacting ultracold Fermi gases, we find that approaching exceptional points can either enhance or strongly suppress decoherence, depending on the balance between Hermitian and non-Hermitian system-environment couplings. In particular, when these couplings are comparable, decoherence is dramatically suppressed at exceptional transitions. We trace this behavior to the distinct response of the environmental ground state near non-Hermitian degeneracies and demonstrate the robustness of the effect across multiple models. Finally, we show that the predicted suppression of decoherence is directly observable on current digital quantum simulation platforms. Our results establish exceptional points as a concrete mechanism for suppressing decoherence and identify non-Hermitian criticality as a new avenue for coherence control in open quantum systems and quantum technologies.

</details>


### [460] [Noise-Resilient Quantum Chemistry with Half the Qubits](https://arxiv.org/abs/2602.01165)
*Shane McFarthing,Aidan Pellow-Jarman,Francesco Petruccione*

Main category: quant-ph

TL;DR: HSQD是一种半量子比特对角化方法，将化学模拟的量子比特需求减半，降低电路深度和门数，抑制硬件噪声，在N2分子解离和铁硫簇系统中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 在NISQ设备上实现精确量子化学计算面临量子比特资源有限、电路深度大、硬件噪声严重等挑战，需要开发更高效的量子对角化方法。

Method: 提出HSQD（半量子比特样本对角化）方法，将量子比特需求减半；进一步结合热浴构型相互作用（HCI）的样本选择策略，形成HCI-HSQD，提高样本效率。

Result: HSQD在N2分子解离中仅用一半量子比特达到与SQD相同的精度，测量减少40%；HCI-HSQD在N2势能面上达到亚毫哈特里精度，子空间比经典HCI小39%；在铁硫簇系统中，HCI-HSQD将SQD能量误差降低76%（[2Fe-2S]）和26%（[4Fe-4S]），同时减半测量需求。

Conclusion: 半量子比特SQD为强关联化学中的实用量子优势提供了一条噪声鲁棒且资源高效的途径。

Abstract: Sample-based quantum diagonalization (SQD) offers a powerful route to accurate quantum chemistry on noisy intermediate-scale quantum (NISQ) devices by combining quantum sampling with classical diagonalization. Here we introduce HSQD, a novel half-qubit SQD approach that halves the qubit requirement for simulating a chemical system and drastically reduces overall circuit depth and gate counts, suppressing hardware noise. When modeling the dissociation of the nitrogen molecule with a (10e, 26o) active space, HSQD matches the accuracy of SQD on IBM quantum hardware using only half the number of qubits and 40% fewer measurements.
  We further enhance HSQD with a heat-bath configuration interaction (HCI) inspired selection of the samples, forming HCI-HSQD. This yields sub-millihartree accuracy across the N2 potential energy surface and produces subspaces up to 39% smaller than those from classical HCI, showing a significant improvement in the compactness of the ground-state representation.
  Finally, we demonstrate the scalability of HCI-HSQD using iron-sulfur clusters, reaching active spaces of up to (54e, 36o) while using only half as many qubits as the original SQD. For these systems, HCI-HSQD reduces SQD energy errors by up to 76% for [2Fe-2S] and 26% for [4Fe-4S], while also reducing subspace sizes, halving measurement requirements, and eliminating expensive post-processing.
  Together, these results establish half-qubit SQD as a noise-resilient and resource-efficient pathway toward practical quantum advantage in strongly correlated chemistry.

</details>


### [461] [Equivalence of Privacy and Stability with Generalization Guarantees in Quantum Learning](https://arxiv.org/abs/2602.01177)
*Ayanava Dasgupta,Naqueeb Ahmad Warsi,Masahito Hayashi*

Main category: quant-ph

TL;DR: 提出统一信息论框架分析差分隐私量子学习算法的泛化性能，建立隐私与算法稳定性联系，推导互信息上界，并将稳定性保证扩展到泛化误差界，最后引入信息论可容许性概念处理不可信数据处理器场景。


<details>
  <summary>Details</summary>
Motivation: 需要分析差分隐私量子学习算法的泛化性能，建立隐私保护与泛化能力之间的理论联系，特别是在量子计算背景下，为隐私保护量子机器学习提供理论保证。

Method: 建立统一信息论框架，利用隐私与算法稳定性之间的联系，推导(ε,δ)-量子差分隐私对训练数据与算法输出之间互信息的约束，建立机制无关的上界，并将稳定性保证与泛化误差联系起来。

Result: 证明了(ε,δ)-量子差分隐私学习算法的期望泛化误差受隐私诱导稳定性项的平方根约束，并针对不可信数据处理器场景提出了信息论可容许性概念，刻画了隐私的基本限制。

Conclusion: 该工作为差分隐私量子学习算法提供了坚实的泛化性能理论保证，建立了隐私、稳定性和泛化之间的定量联系，并为不可信数据处理场景提供了新的隐私分析框架。

Abstract: We present a unified information-theoretic framework to analyze the generalization performance of differentially private (DP) quantum learning algorithms. By leveraging the connection between privacy and algorithmic stability, we establish that $(\varepsilon, δ)$-Quantum Differential Privacy (QDP) imposes a strong constraint on the mutual information between the training data and the algorithm's output. We derive a rigorous, mechanism-agnostic upper bound on this mutual information for learning algorithms satisfying a 1-neighbor privacy constraint. Furthermore, we connect this stability guarantee to generalization, proving that the expected generalization error of any $(\varepsilon, δ)$-QDP learning algorithm is bounded by the square root of the privacy-induced stability term. Finally, we extend our framework to the setting of an untrusted Data Processor, introducing the concept of Information-Theoretic Admissibility (ITA) to characterize the fundamental limits of privacy in scenarios where the learning map itself must remain oblivious to the specific dataset instance.

</details>


### [462] [The Intrinsic Connection between Dynamical Phase Transitions and Magnetization in the 1D XY Model](https://arxiv.org/abs/2602.01259)
*Lin-Yue Luo,Wei-Lin Li,Bao-Ming Xu,Zhi Li*

Main category: quant-ph

TL;DR: 研究横向场XY模型从相干吉布斯态淬火的动力学，发现初始磁化强度影响动力学量子相变的出现：初始磁化越强，相变越难发生。


<details>
  <summary>Details</summary>
Motivation: 探索初始条件对动力学量子相变的影响，特别是初始磁化强度如何调控量子淬火过程中的相变行为。

Method: 使用横向场XY模型，从相干吉布斯态开始进行量子淬火，通过分析Fisher零点和磁化等可观测量来研究动力学量子相变。

Result: 初始磁化强度越强，动力学量子相变越难出现。强初始磁化提供方向性效应，抑制量子淬火过程中的自旋翻转。

Conclusion: 初始磁化强度是调控动力学量子相变的关键因素，这一物理机制可在人工系统中实验验证。

Abstract: In this manuscript, we study the quench dynamics of a transverse-field XY model starting from coherent Gibbs states. The results reveal that the initial strength of magnetization plays a crucial role in the emergence of dynamical quantum phase transitions. In concrete terms, when quenching within the same phase, through the properties of observables such as Fisher zeros and magnetization, we show that the stronger the initial magnetization, the more difficult the emergence of dynamical quantum phase transitions. The underlying mechanism is that the strong initial magnetization provides a directional effect, which inhibits the spin flipping in the process of quantum quench, making the dynamical quantum phase transition difficult to emerge. Since dynamical quantum phase transitions can be experimentally realized in various artificial systems, we hope that the physics predicted here can be experimentally verified in tabletop platforms.

</details>


### [463] [Scalable Tensor Network Simulation for Quantum-Classical Dual Kernel](https://arxiv.org/abs/2602.01330)
*Mei Ian Sam,Tai-Yu Li*

Main category: quant-ph

TL;DR: 提出高效可扩展的张量网络框架用于量子核电路模拟，实现784量子比特的量子-经典混合双核评估，在Fashion-MNIST上双核性能优于单核基准


<details>
  <summary>Details</summary>
Motivation: 解决量子核电路模拟中随着量子比特数和数据规模增加带来的实际计算成本问题，同时探索量子-经典混合核在机器学习中的优势

Method: 开发高效可扩展的张量网络框架，实现量子核电路的大规模模拟；构建量子-经典线性混合双核模型；在Fashion-MNIST数据集上进行分类实验，特征维度从2到784，特征与量子比特一一映射

Result: 量子-经典双核在所有特征维度上均优于单一量子核和经典核；双核性能随维度增加保持稳定，而量子核在大规模时出现性能退化；学习到的混合权重显示：低于128特征时量子贡献主导，超过128特征时经典贡献变得更重要

Conclusion: 量子-经典混合双核结合了两者的优势：经典核提供对浓度效应和硬件噪声的稳定锚定，同时保留低维度时的量子增益；张量网络框架为大规模量子核评估提供了实用工具

Abstract: This paper presents an efficient and scalable tensor network framework for quantum kernel circuit simulation, alleviating practical costs associated with increasing qubit counts and data size. The framework enables systematic large-scale evaluation of a linearly mixed quantum-classical dual kernel of up to 784 qubits. Using Fashion-MNIST, the classification performance of the test dataset is compared between a classical kernel, a quantum kernel, and the quantum-classical dual kernel across the feature dimensions from 2 to 784, with a one-to-one mapping between encoded features and qubits. Our result shows that the quantum-classical dual kernel consistently outperforms both single-kernel baselines, remains stable as the dimensionality increases, and mitigates the large-scale degradation observed in the quantum kernel. Analysis of the learned mixing weights indicates that quantum contributions dominate below 128 features, while classical contributions become increasingly important beyond 128, suggesting that the classical kernel provides a stabilizing anchor against concentration effects and hardware noise while preserving quantum gains at lower dimensions.

</details>


### [464] [Spectroscopic Signatures of a Liouvillian Exceptional Spectral Phase in a Collective Spin](https://arxiv.org/abs/2602.01375)
*Rafael A. Molina*

Main category: quant-ph

TL;DR: 非厄米Lindblad生成元的简并（Liouvillian异常点）能导致非指数弛豫和响应函数高阶极点。极化自旋与Markovian浴耦合时出现"异常谱相"，缺陷Liouvillian模在频域谱中产生超洛伦兹特征。通过Liouvillian预解式计算发射谱，识别对称性选择规则，发现异常点特征具有强状态依赖性：稳态荧光中被抑制，但在一般初始态下变得明显。


<details>
  <summary>Details</summary>
Motivation: 研究Liouvillian异常点在开放量子系统中的物理表现，特别是如何通过光谱学方法检测这些异常特征。传统稳态发射谱可能无法揭示异常点特征，需要探索更通用的初始态来观测这些现象。

Method: 采用极化自旋与Markovian浴耦合的模型，通过Liouvillian预解式计算发射谱，分析对称性选择规则，比较不同初始态（包括稳态、无限温度态和随机态）下的光谱特征。

Result: 发现Liouvillian异常相中缺陷模产生超洛伦兹光谱特征；异常点特征具有强状态依赖性：在稳态荧光中被抑制，但在一般初始态（如无限温度或随机态）下变得明确可观测。

Conclusion: 为多体Liouvillian异常相提供了实验可及的光谱诊断方法，阐明了稳态发射谱何时能（何时不能）揭示这些异常特征，为开放量子系统的非厄米物理研究提供了新视角。

Abstract: Non-Hermitian degeneracies of Lindblad generators (Liouvillian exceptional points) can induce non-exponential relaxation and higher-order poles in dynamical response functions. A collective spin coupled to a polarized Markovian bath exhibits an \emph{exceptional spectral phase} in which defective Liouvillian modes imprint super-Lorentzian features in frequency-resolved spectra. We compute the emission spectrum via the Liouvillian resolvent, identify symmetry-sector selection rules, and demonstrate that exceptional-point signatures are strongly state-dependent: they are suppressed in steady-state fluorescence yet become unambiguous for generic (infinite-temperature or random) initial states. Our results provide an experimentally accessible spectroscopic diagnostic of many-body Liouvillian exceptional phases and clarify when steady-state emission can (and cannot) reveal them.

</details>


### [465] [Free-space and Satellite-Based Quantum Communication: Principles, Implementations, and Challenges](https://arxiv.org/abs/2602.01426)
*Georgi Gary Rozenman,Alona Maslennikov,Sara P. Gandelman,Yuval Reches,Sahar Delfan,Neel Kanth Kundu,Leyi Zhang,Ruiqi Liu*

Main category: quant-ph

TL;DR: 本文综述了卫星量子通信的现状、技术路线（DV与CV）、关键里程碑（如墨子号卫星）以及实现全球量子互联网面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 卫星量子通信是实现全球安全量子网络的关键技术，相比地面系统具有覆盖范围广的优势，能够突破传统量子通信的距离限制，为构建全球量子互联网奠定基础。

Method: 采用综述研究方法，系统梳理卫星量子通信从地面到空间的发展历程，重点分析离散变量（DV）和连续变量（CV）两种量子通信技术在卫星部署中的优缺点，并总结关键实验成果。

Result: 卫星量子通信已取得重要进展，特别是通过墨子号卫星成功实现了量子通信，证明了空间量子通信的可行性。DV和CV技术各有优势，DV技术成熟度高，CV技术对设备要求较低但受大气湍流影响更大。

Conclusion: 卫星量子通信是实现全球量子互联网的关键路径，但仍面临大气湍流、量子中继器等挑战。未来需要技术创新来解决这些问题，推动卫星量子通信向实用化、全球化方向发展。

Abstract: Satellite-based quantum communications represent a critical advancement in the pursuit of secure, global-scale quantum networks. Leveraging the principles of quantum mechanics, these systems offer unparalleled security through Quantum Key Distribution (QKD) and other quantum communication protocols. This review provides a comprehensive overview of the current state of satellite-based quantum communications, focusing on the evolution from terrestrial to space-based systems. We explore the distinct advantages and challenges of discrete-variable (DV) and continuous-variable (CV) quantum communication technologies in the context of satellite deployments. The paper also discusses key milestones such as the successful implementation of quantum communication via the Micius satellite and outlines the primary challenges, including atmospheric turbulence and the development of quantum repeaters, that must be addressed to achieve a global quantum internet. This review aims to consolidate recent advancements in the field, providing insights and perspectives on the future directions and potential innovations that will drive the continued evolution of satellite-based quantum communications.

</details>


### [466] [What non-additive integral for ensemble spaces?](https://arxiv.org/abs/2602.01470)
*Gabriele Carcassi,Christine A. Aidala,Tobias Thrien*

Main category: quant-ph

TL;DR: 本文证明Sugeno和Choquet积分不适合作为统计系综空间中期望值的推广


<details>
  <summary>Details</summary>
Motivation: 先前研究定义了能表示经典和量子态的非可加测度，并将其扩展到统计系综空间。但何种非可加积分适合推广期望值概念仍是一个开放问题。

Method: 分析Sugeno和Choquet积分在统计系综空间中的适用性，证明它们不适合作为期望值的推广

Result: Sugeno和Choquet积分不适合推广期望值的概念

Conclusion: 需要寻找其他非可加积分来推广统计系综空间中的期望值概念

Abstract: In a previous work we were able to define a non-additive measure that can be used to represent both classical and quantum states in physics. We further extended this idea to work on a generic space of statistical ensembles (i.e. an ensemble space) in a way that connects to Choquet theory. The question of which non-additive integral is suitable to generalize the notion of expectation value remains open. In this paper we show that the Sugeno and Choquet integrals are not suitable.

</details>


### [467] [Unified entropy production in finite quantum systems](https://arxiv.org/abs/2602.01669)
*Tomohiro Nishiyama,Yoshihiko Hasegawa*

Main category: quant-ph

TL;DR: 该论文提出了一种基于量子相对熵的有限维量子系统熵产生统一定义，使用有效温度参考态，并分析了熵产生的分解、非负性条件及下界。


<details>
  <summary>Details</summary>
Motivation: 在有限维量子系统中，温度无法唯一定义，这导致熵产生也有多种定义方式。需要建立一个统一的熵产生定义框架，以解决温度定义不唯一带来的问题。

Method: 提出基于量子相对熵的熵产生定义，使用有效温度参考态。将熵产生分解为克劳修斯型熵产生和有效温度时间依赖性产生的附加项。通过要求熵产生率取传统形式（熵变加热流）来约束有效温度。

Result: 1. 熵产生可分解为克劳修斯型项和有效温度时间依赖性项；2. 有效温度必须为常数或特定能量匹配有效温度才能保持传统熵产生率形式；3. 一般初始态下熵产生可能为负，但建立了下界；4. 使用迹距离建立了熵产生非负性的充分条件。

Conclusion: 该研究为有限维量子系统提供了统一的熵产生定义框架，揭示了有效温度在熵产生中的作用，建立了熵产生的数学性质和非负性条件，为量子热力学提供了理论基础。

Abstract: In finite-dimensional quantum systems, temperature cannot be uniquely defined. This, in turn, implies that there are several ways to define entropy production in finite-dimensional quantum systems, because the classical entropy production depends on temperature. We propose a unified definition of entropy production based on the difference in quantum relative entropy with respect to reference states characterized by effective temperatures. We demonstrate that the proposed definition naturally decomposes into a Clausius-type entropy production and an additional contribution arising from the time dependence of the effective temperature. Furthermore, we show that requiring the entropy production rate to take the conventional form as the sum of the entropy change and the heat flow constrains the effective temperature to be either constant or equal to a specific energy-matching effective temperature. For general initial states, entropy production can become negative, in which case we derive lower bounds on entropy production and establish sufficient conditions for its non-negativity using the trace distance.

</details>


### [468] [Quantum Jacobi-Davidson Method](https://arxiv.org/abs/2602.01670)
*Shaobo Zhang,Akib Karim,Harry M. Quiney,Muhammad Usman*

Main category: quant-ph

TL;DR: 提出量子雅可比-戴维森（QJD）和基于采样的量子雅可比-戴维森（SBQJD）方法，用于高效计算量子系统的电子结构，相比现有量子戴维森方法收敛更快、所需泡利测量更少。


<details>
  <summary>Details</summary>
Motivation: 量子系统电子结构计算是光子学、固态物理和量子技术应用的关键任务，但传统迭代算法计算成本高且存在收敛问题，需要更高效的量子计算方法。

Method: 开发并实现了量子雅可比-戴维森（QJD）方法及其量子对角化变体——基于采样的量子雅可比-戴维森（SBQJD）方法，通过精确数值模拟评估算法性能。

Result: 在8量子位对角占优矩阵、12量子位一维伊辛模型和10量子位水分子哈密顿量上测试，QJD和SBQJD相比量子戴维森方法收敛显著更快、所需泡利测量更少，SBQJD还受益于优化的参考态制备。

Conclusion: QJD框架是解决量子特征值问题的高效通用子空间技术，为未来容错量子硬件上的稀疏哈密顿量计算提供了有前景的基础。

Abstract: Computing electronic structures of quantum systems is a key task underpinning many applications in photonics, solid-state physics, and quantum technologies. This task is typically performed through iterative algorithms to find the energy eigenstates of a Hamiltonian, which are usually computationally expensive and suffer from convergence issues. In this work, we develop and implement the Quantum Jacobi-Davidson (QJD) method and its quantum diagonalization variant, the Sample-Based Quantum Jacobi-Davidson (SBQJD) method, and demonstrate their fast convergence for ground state energy estimation. We assess the intrinsic algorithmic performance of our methods through exact numerical simulations on a variety of quantum systems, including 8-qubit diagonally dominant matrices, 12-qubit one-dimensional Ising models, and a 10-qubit water molecule (H$_2$O) Hamiltonian. Our results show that both QJD and SBQJD achieve significantly faster convergence and require fewer Pauli measurements compared to the recently reported Quantum Davidson method, with SBQJD further benefiting from optimized reference state preparation. These findings establish the QJD framework as an efficient general-purpose subspace-based technique for solving quantum eigenvalue problems, providing a promising foundation for sparse Hamiltonian calculations on future fault-tolerant quantum hardware.

</details>


### [469] [Optimal Control to Minimize Dissipation and Fluctuations in Open Quantum Systems Beyond Slow and Rapid Regimes](https://arxiv.org/abs/2602.01688)
*Yuki Kurokawa,Yoshihiko Hasegawa*

Main category: quant-ph

TL;DR: 提出了一种用于开放量子系统中耗散功和功方差最优控制的通用框架，适用于中间时间尺度，超越了慢驱动和快驱动极限。


<details>
  <summary>Details</summary>
Motivation: 量子热力学中的最优控制问题在快驱动和慢驱动极限下已有理论，但缺乏适用于中间时间尺度的通用优化方法。

Method: 通过引入辅助算子，将历史依赖的功方差转换为时间局域积分，实现了基于梯度的高效优化，适用于由含时Lindblad主方程描述的开放量子系统。

Result: 在相干自旋-玻色子模型中，优化协议会在耗散和涨落相对权重变化时在不同局部最优解间不连续切换；在单能级量子点-费米子库系统中，涨落最小化协议展现出多步结构，该结构无法由慢驱动或快驱动极限方法捕捉。

Conclusion: 该框架为量子热力学中间时间尺度的最优控制提供了通用方法，揭示了优化协议在耗散和涨落权衡下的复杂行为，超越了传统极限方法的局限性。

Abstract: Optimal control is a central problem in quantum thermodynamics. While control theories in the rapid-driving and slow-driving limits have been developed, to the best of our knowledge there is no general optimization method applicable to intermediate timescales. We introduce an optimal-control framework to minimize dissipated work and work variance, defined via the two-point measurement scheme, in open quantum systems governed by time-dependent Lindblad master equations. By introducing an auxiliary operator, we convert the history-dependent work variance into a time-local integral, enabling efficient gradient-based optimization beyond slow or rapid driving regimes. Applying our method, we find that in the coherent spin-boson model the optimized protocol can switch discontinuously between distinct locally optimal solutions as the relative weight between dissipation and fluctuations is varied. Moreover, for a single-level quantum dot coupled to a fermionic reservoir, the optimized fluctuation-minimizing protocol develops a qualitatively different multi-step structure that is not captured by approaches based on slow- or rapid-driving limits.

</details>


### [470] [N-dimensional Coulomb-Sturmians with noninteger quantum numbers](https://arxiv.org/abs/2602.01704)
*Ali Bagci*

Main category: quant-ph

TL;DR: Bagci-Hoggan指数型轨道推广了Coulomb-Sturmian函数，允许分数阶量子数，并推导了N维方程，同时澄清了Guseinov的Psi-alpha-ETOs并非独立完备正交基。


<details>
  <summary>Details</summary>
Motivation: 传统Coulomb-Sturmian函数虽然完备正交且包含连续谱，但量子数被限制为整数。Bagci-Hoggan指数型轨道通过推广到分数阶量子数来消除这一限制，提供更灵活的基函数。

Method: 推导了N维Bagci-Hoggan轨道的微分方程，证明Coulomb-Sturmian函数是这些方程的特例。通过维度参数分析，识别了Guseinov的Psi-alpha-ETOs与N维Coulomb-Sturmian函数的关系。

Result: 成功建立了Bagci-Hoggan轨道与Coulomb-Sturmian函数的数学关系，澄清了Guseinov的Psi-alpha-ETOs实际上是维度参数平移后的N维Coulomb-Sturmian函数，而非独立的完备正交基。

Conclusion: Bagci-Hoggan轨道提供了更一般的数学框架，Coulomb-Sturmian函数是其特例。Guseinov的基函数并非新的独立完备正交集，而是现有框架内的变体。

Abstract: Coulomb-Sturmian functions are complete, orthonormal, and include the full spectrum of continuum states. They are restricted to integer values of quantum numbers, as imposed by boundary and orthonormality conditions. Bagci-Hoggan exponential-type orbitals remove this restriction through a generalization to quantum number with fractional order. The differential equations for N-dimensional Bagci-Hoggan orbitals are derived. It is demonstrated that Coulomb-Sturmian functions satisfy a particular case of these equations. Additionally, Guseinov's Psi-alpha-ETOs are identified as N-dimensional Coulomb-Sturmians with a shifted dimensional parameter alpha, rather than representing an independent complete orthonormal sets of basis in a weighted Hilbert space.

</details>


### [471] [Dual channel multi-product formulas](https://arxiv.org/abs/2602.01713)
*Seung Park,Sangjin Lee,Kyunghyun Baek*

Main category: quant-ph

TL;DR: 提出双通道多乘积公式，将Trotter误差缩放提升两倍，使达到目标模拟精度所需的电路深度减半，降低物理误差缓解开销。


<details>
  <summary>Details</summary>
Motivation: 基于乘积公式的量子模拟在近期量子计算机上具有前景，但达到所需精度需要多项式增长的Trotter步数，受限于当前量子硬件性能。现有后处理技术如多乘积公式在有限硬件资源下抑制算法误差。

Method: 提出双通道多乘积公式方法，通过改进设计实现Trotter误差缩放的两倍提升，从而在保持相同模拟精度下将所需电路深度减半。

Result: 在固定CNOT门数量作为量子电路度量下，该方法产生显著更小的算法误差，同时采样误差基本保持不变，电路深度减半直接转化为实际量子硬件上更低的物理误差缓解开销。

Conclusion: 双通道多乘积公式通过改进误差缩放，有效减少量子模拟所需的电路深度，降低硬件实现时的误差缓解开销，为近期量子计算机上的高效量子模拟提供了有前景的解决方案。

Abstract: Product-formula (PF) based quantum simulation is a promising approach for simulating quantum systems on near-term quantum computers. Achieving a desired simulation precision typically requires a polynomially increasing number of Trotter steps, which remains challenging due to the limited performance of current quantum hardware. To alleviate this issue, post-processing techniques such as the multi-product formula (MPF) have been introduced to suppress algorithmic errors within restricted hardware resources.
  In this work, we propose a dual-channel multi-product formula that achieves a two-fold improvement in Trotter error scaling. As a result, our method enables the target simulation precision to be reached with approximately half the circuit depth compared to conventional MPF schemes. Importantly, the reduced circuit depth directly translates into lower physical error mitigation overhead when implemented on real quantum hardware. We demonstrate that, for a fixed CNOT count as a measure of quantum circuit, our proposal yields significantly smaller algorithmic errors, while the sampling error remains essentially unchanged.

</details>


### [472] [Gravitational effects on a dissipative two-level atom in the weak-field regime](https://arxiv.org/abs/2602.01715)
*Kaito Kashiwagi,Akira Matsumura*

Main category: quant-ph

TL;DR: 研究弱引力场中二能级原子的耗散动力学，发现引力场会修正自发辐射率，修正取决于原子偶极矩、相对引力源位置和辐射频率，并识别了引力增强或抑制自发辐射的参数区域。


<details>
  <summary>Details</summary>
Motivation: 探索引力场对开放量子系统的影响，特别是弱引力场中二能级原子与标量场相互作用的耗散动力学，为研究引力效应在开放量子系统中的表现提供理论基础。

Method: 使用Feynman-Vernon影响泛函形式，推导牛顿引力场中二能级原子与标量场相互作用的量子主方程，计算原子的能量耗散率。

Result: 发现真空中的自发辐射率（耗散率）被引力场修正，修正取决于原子偶极矩、原子相对于引力源的位置以及原子发射的标量辐射频率。识别了引力增强或抑制自发辐射的参数区域。

Conclusion: 引力场通过时间膨胀和弱引力场中的偶极辐射机制修正自发辐射率，这些发现为探索开放量子系统中的引力效应提供了理论基础。

Abstract: We investigate the dissipative dynamics of a two-level atom in a weak gravitational field. Using the Feynman--Vernon influence functional formalism, we derive a quantum master equation describing the two-level atom interacting with a scalar field in a Newtonian gravitational field, and compute the energy dissipation rate of the atom. We find that the spontaneous emission rate (the dissipation rate in vacuum) is modified by the gravitational field. Specifically, this modification depends on the atom's dipole, the position of the atom relative to the source of the gravitational field, and the frequency of the scalar radiation emitted by the atom. Furthermore, we identify the parameter regimes in which the spontaneous emission rate is enhanced or suppressed by gravity. We also discuss how the modification arises from time dilation and dipole radiation in a weak gravitational field. These findings provide a theoretical basis for exploring gravitational effects in open quantum systems.

</details>


### [473] [Relativistic Position Verification with Coherent States](https://arxiv.org/abs/2602.01787)
*Guan-Jie Fan-Yuan,Yang-Guang Shan,Cong Zhang,Yu-Long Wang,Yu-Xuan Fan,Wei-Xin Xie,De-Yong He,Shuang Wang,Zhen-Qiang Yin,Wei Chen,Song-Nian Fu,Guang-Can Guo,Zheng-Fu Han*

Main category: quant-ph

TL;DR: 利用量子光学和相对论原理，实验实现了安全的位置验证协议，在2公里距离内以75米精度验证实体位置


<details>
  <summary>Details</summary>
Motivation: 传统方法无法提供安全的位置验证，攻击者可能误导验证者关于其实际位置，需要信息论安全的位置验证方案

Method: 采用量子光学和相对论原理，使用相位随机化的弱相干态，在两个相距2公里的验证者之间实现安全位置验证

Result: 成功在2公里距离内验证证明者位置，精度优于75米，实现了信息论安全的位置验证

Conclusion: 安全位置验证已成为实际可能，为金融交易、灾难响应和安全通信认证等应用铺平道路

Abstract: Determining the position of an entity is a fundamental prerequisite for nearly all activities. Classical means, however, have been proven incapable of providing secure position verification, meaning that a prover can mislead verifiers about its actual position. In this work, we propose and experimentally realize a secure position-verification protocol that leverages quantum optics and relativity within an information-theoretic framework. Using phase-randomized weak coherent states, two verifiers separated by 2 km securely verify the prover's position with an accuracy better than 75 meters. These results establish secure position-based authentication as a practical possibility, paving the way for applications in financial transactions, disaster response, and authenticated secure communications.

</details>


### [474] [Semidefinite programming for understanding limitations of Lindblad equations](https://arxiv.org/abs/2602.01794)
*Soumyadeep Sarma,Manas Kulkarni,Archak Purkayastha,Devashish Tupkary*

Main category: quant-ph

TL;DR: 该论文提出使用半定规划(SDP)来判断Lindblad主方程能否在弱耦合下准确描述量子系统的稳态，发现多数情况下无法同时准确描述布居数和相干性。


<details>
  <summary>Details</summary>
Motivation: Lindblad主方程作为弱耦合量子系统的主流描述方法存在根本性限制：无法同时准确描述布居数和相干性，可能导致违反热化和局域守恒律等基本性质。需要判断在特定物理情境下是否存在能准确描述的Lindblad方程。

Method: 将判断Lindblad主方程能否准确描述平衡和非平衡稳态的问题转化为半定规划(SDP)问题，这是一种凸优化技术。通过求解SDP来判断是否存在满足精度要求的Lindblad描述。

Result: 对于各向同性XXZ型多比特模型耦合多个热库的情况，在大多数参数范围内无法找到能同时准确描述布居数和相干性的Lindblad方程。但在某些参数范围内，可以找到能准确描述布居数但相干性不准确、且满足局域守恒律的Lindblad描述。

Conclusion: 半定规划是分析物理一致Lindblad方程的有力工具，有助于理解弱系统-热库耦合下马尔可夫描述的根本限制。多数情况下无法获得同时准确描述布居数和相干性的Lindblad描述。

Abstract: Lindbladian quantum master equations (LEs) are the most popular descriptions for quantum systems weakly coupled to baths. But, recent works have established that in many situations such Markovian descriptions are fundamentally limited: they cannot simultaneously capture populations and coherences even to the leading-order in system-bath couplings. This can cause violation of fundamental properties like thermalization and continuity equations associated with local conservation laws, even when such properties are expected in the actual setting. This begs the question: given a physical situation, how do we know if there exists an LE that describes it to a desired accuracy? Here we show that, for both equilibrium and non-equilibrium steady states (NESS), this question can be succinctly formulated as a semidefinite program (SDP), a convex optimization technique. If a solution to the SDP can be found to a desired accuracy, then an LE description is possible for the chosen setting. If not, no LE description is fundamentally attainable, showing that a consistent Markovian treatment is impossible even at weak system-bath coupling for that particular setting. Considering few qubit isotropic XXZ-type models coupled to multiple baths, we find that in most parameter regimes, LE description giving accurate populations and coherences to leading-order is unattainable, leading to rigorous no-go results. However, in some cases, LE description having correct populations but inaccurate coherences, and satisfying local conservation laws, is possible over some of the parameter regimes. Our work highlights the power of semidefinite programming in the analysis of physically consistent LEs, thereby, in understanding the limits of Markovian descriptions at weak system-bath couplings.

</details>


### [475] [Finite-Size Scaling of the Full Eigenstate Thermalization in Quantum Spin Chains](https://arxiv.org/abs/2602.01809)
*Yuke Zhang,Pengfei Zhang*

Main category: quant-ph

TL;DR: 该研究通过精确对角化分析量子多体系统中本征态热化假说（ETH）的有限尺寸修正，区分了能量波动引起的多项式衰减修正和能量窗口内波动引起的指数衰减修正，解决了某些可观测量在混沌系统中随系统尺寸增大的异常修正问题。


<details>
  <summary>Details</summary>
Motivation: 尽管封闭量子系统具有幺正演化，但局部可观测量长时间期望值可由热系综描述，这构成了量子统计力学的基础。本征态热化假说（ETH）为理解量子热化提供了途径，但需要深入研究其有限尺寸修正，特别是某些可观测量在混沌系统中随系统尺寸增大的异常修正现象。

Method: 采用精确对角化方法研究正则系综中ETH关系的有限尺寸修正。区分两种修正来源：1）能量波动引起的多项式衰减修正；2）每个能量窗口内波动引起的指数衰减修正。对量子多体系统进行详细分析。

Result: 研究发现两种不同的有限尺寸修正机制：能量波动导致的修正随系统尺寸多项式衰减，而能量窗口内波动导致的修正随系统尺寸指数衰减。这一分析解决了某些可观测量在混沌系统中随系统尺寸增大而出现异常修正增长的问题。

Conclusion: 该研究为验证量子多体系统中的完整ETH提供了系统且实用的方法论，区分了不同来源的有限尺寸修正，深化了对量子热化机制的理解。

Abstract: Despite the unitary evolution of closed quantum systems, long-time expectation of local observables are well described by thermal ensembles, providing the foundation of quantum statistical mechanics. A promising route to understanding this quantum thermalization is the eigenstate thermalization hypothesis (ETH), which posits that individual energy eigenstates already appear locally thermal. Subsequent studies have extended this concept to the full ETH, which captures higher-order correlations among matrix elements through nontrivial relations. In this work, we perform a detailed exact-diagonalization study of finite-size corrections to these relations in the canonical ensemble. We distinguish two distinct sources of corrections: those arising from energy fluctuations, which decay polynomially with system size, and those originating from fluctuations within each energy window, which decay exponentially with system size. In particular, our analysis resolves the puzzle that, for certain observables, finite-size corrections exhibit anomalous growth with increasing system size even in chaotic systems. Our results provide a systematic and practical methodology for validating the full ETH in quantum many-body systems.

</details>


### [476] [Quantum Circuit Representation of Bosonic Matrix Functions](https://arxiv.org/abs/2602.01868)
*Minhyeok Kang,Gwonhak Lee,Youngrong Lim,Joonsuk Huh*

Main category: quant-ph

TL;DR: 该论文建立了玻色子网络与量子自旋动力学之间的统一框架，将伊辛模型的跃迁振幅与hafnian和loop-hafnian矩阵函数联系起来，扩展了先前仅限于二分自旋相互作用的研究。


<details>
  <summary>Details</summary>
Motivation: 先前的研究主要局限于二分自旋相互作用，其中跃迁振幅与永久式(permanent)成正比。需要将伊辛模型构造扩展到任意相互作用网络，建立更一般的对应关系。

Method: 将伊辛模型构造扩展到任意相互作用网络，证明伊辛哈密顿量的跃迁振幅与hafnian和loop-hafnian成正比。特别关注需要狄克(Dicke)叠加态的loop-hafnian情况。

Result: 建立了玻色子单光子和高斯态网络与量子自旋动力学及矩阵函数之间的统一框架。发现loop-hafnian需要狄克叠加态，使得相应量子电路设计变得非平凡。

Conclusion: 该统一框架不仅拓宽了量子电路模型的理论基础，还突出了新的、多样化的、经典难解的应用，为量子计算提供了新的理论连接。

Abstract: Bosonic counting problems can be framed as estimation tasks of matrix functions such as the permanent, hafnian, and loop-hafnian, depending on the underlying bosonic network. Remarkably, the same functions also arise in spin models, including the Ising and Heisenberg models, where distinct interaction structures correspond to different matrix functions. This correspondence has been used to establish the classical hardness of simulating interacting spin systems by relating their output distributions to #P-hard quantities. Previous works, however, have largely been restricted to bipartite spin interactions, where transition amplitudes, which provide the leading-order contribution to the output probabilities, are proportional to the permanent. In this work, we extend the Ising model construction to arbitrary interaction networks and show that transition amplitudes of the Ising Hamiltonian are proportional to the hafnian and the loop-hafnian. The loop-hafnian generalizes both the permanent and hafnian, but unlike these cases, loop-hafnian-based states require Dicke-like superpositions, making the design of corresponding quantum circuits non-trivial. Our results establish a unified framework linking bosonic networks of single photons and Gaussian states with quantum spin dynamics and matrix functions. This unification not only broadens the theoretical foundation of quantum circuit models but also highlights new, diverse, and classically intractable applications.

</details>


### [477] [Numerical Error Extraction by Quantum Measurement Algorithm](https://arxiv.org/abs/2602.01927)
*Clement Ronfaut,Robin Ollive,Stephane Louise*

Main category: quant-ph

TL;DR: 提出NEEQMA协议，通过量子测量从QPU直接提取量子门近似中的收敛常数，优化量子算法参数选择


<details>
  <summary>Details</summary>
Motivation: 量子算法中迭代结构（如QSP、Trotterization）的收敛常数难以经典计算，现有上界过于悲观，需要实际测量这些常数来优化参数选择

Method: 提出NEEQMA协议，通过在QPU上构建不同精度的门近似，直接通过量子测量提取收敛常数

Result: 在Quantum Signal Processing和Hamiltonian Simulation by Trotterization实例上测试NEEQMA概念

Conclusion: 通过测量实际收敛常数，可以选择最小的收敛参数来满足量子算法的精度要求，优化量子算法实现

Abstract: Important quantum algorithm routines allow the implementation of specific quantum operations (a.k.a. gates) by combining basic quantum circuits with an iterative structure. In this structure, the number of repetitions of the basic circuit pattern is associated to convergence parameters. This iterative structure behaves similarly to function approximation by series expansion: the higher the truncation order, the better the target gate (i.e. operation) approximation. The asymptotic convergence of the gate error with respect to the number of basic pattern repetitions is known. It is referred to as the query complexity. The underlying convergence law is bounded, but not in an explicit fashion. Upper bounds are generally too pessimistic to be useful in practice. The actual convergence law contains constants that depend on the joint properties of the matrix encoded by the query and the initial state vector, which are difficult to compute classically. This paper proposes a strategy to study this convergence law and extract the associated constants from the gate (operation) approximation at different accuracy (convergence parameter) constructed directly on a Quantum Processing Unit (QPU). This protocol is called Numerical Error Extraction by Quantum Measurement Algorithm (NEEQMA). NEEQMA concepts are tested on specific instances of Quantum Signal Processing (QSP) and Hamiltonian Simulation by Trotterization. Knowing theexact convergence constants allows for selecting the smallest convergence parameters that enable reaching the required gate approximation accuracy, hence satisfying the quantum algorithm's requirements.

</details>


### [478] [Exceptional phase transition in a single Kerr-cat qubit](https://arxiv.org/abs/2602.01934)
*Pei-Rong Han,Tian-Le Yang,Wen Ning,Hao-Long Zhang,Huifang Kang,Huiye Qiu,Zhen-Biao Yang*

Main category: quant-ph

TL;DR: 该论文研究了基于驱动耗散Kerr猫态量子比特的刘维尔异常点结构，揭示了在异常点处发生的量子相变，表现为从欠阻尼振荡到过阻尼弛豫的动力学行为突变。


<details>
  <summary>Details</summary>
Motivation: 尽管异常点诱导的量子相变已从离散变量系统扩展到连续变量系统，但在连续变量平台中由刘维尔异常点驱动的量子相变仍未被充分探索。本文旨在填补这一空白。

Method: 构建并研究基于驱动耗散Kerr猫态量子比特的刘维尔异常结构，通过数值模拟分析Wigner函数和布洛赫球轨迹，并引入刘维尔本征矩阵非对角元相位差作为量化相变的新参数。

Result: 在刘维尔异常点处观察到量子相变，表现为动力学行为从欠阻尼振荡到过阻尼弛豫的突变。Wigner函数的负性作为真正量子相干性的直接标志，这在传统单量子比特非厄米系统中无法实现。

Conclusion: Kerr猫态量子比特为探索耗散量子临界性和固有非厄米物理提供了一个新颖的连续变量平台，建立了研究刘维尔异常点驱动量子相变的新框架。

Abstract: Exceptional points in non-Hermitian quantum systems give rise to novel genuine quantum phenomena. Recent explorations of exceptional-point-induced quantum phase transitions have extended from discrete-variable to continuous-variable-encoded quantum systems. However, quantum phase transitions driven by Liouvillian exceptional points (LEPs) in continuous-variable platforms remain largely unexplored. Here, we construct and investigate a Liouvillian exceptional structure based on a driven-dissipative Kerr-cat qubit. Through numerical simulations, we reveal a quantum phase transition occurring at the LEP characterized by a sudden change in dynamical behavior from underdamped oscillations to overdamped relaxations as visualized via Wigner functions and Bloch sphere trajectories. Notably the negativity of the Wigner function serves as a direct signature of genuine quantum coherence unattainable in conventional single-qubit non-Hermitian systems. Furthermore, we introduce the phase difference between the off-diagonal elements of the Liouvillian eigenmatrices as a novel parameter to quantify the transition. Our results establish the Kerr-cat qubit as a novel continuous-variable setting for exploring dissipative quantum criticality and intrinsic non-Hermitian physics.

</details>


### [479] [Universal scaling of finite-temperature quantum adiabaticity in driven many-body systems](https://arxiv.org/abs/2602.01943)
*Li-Ying Chou,Jyong-Hao Chen*

Main category: quant-ph

TL;DR: 该论文推导了混合态量子系统的严格绝热性判据，提出了有限温度下非绝热性开始的阈值驱动速率，并展示了在热力学极限下该阈值可分解为系统尺寸贡献和普适温度依赖因子。


<details>
  <summary>Details</summary>
Motivation: 现实量子系统永远不会处于绝对零度，但有限温度下的定量绝热性判据发展远不如纯态情况成熟，需要建立适用于混合态的实用判据。

Method: 结合混合态量子速度极限和混合态保真度敏感性，在量子力学的刘维尔空间表述中推导希尔伯特-施密特保真度的严格界限，应用于驱动初始吉布斯态向准吉布斯目标的协议。

Result: 对于有能隙相中的局域哈密顿量，在热力学极限下阈值可分解为两部分：恢复零温标度的系统尺寸贡献和普适温度依赖因子（低温下指数接近1，高温下线性增长）。在多个自旋-1/2链中验证了预测的标度行为。

Conclusion: 该研究为封闭多体系统中的有限温度绝热性提供了实用且基本模型无关的判据，填补了混合态绝热性理论的空白。

Abstract: Establishing quantitative adiabaticity criteria at finite temperature remains substantially less developed than in the pure-state setting, despite the fact that realistic quantum systems are never at absolute zero. Here we derive rigorous bounds on the Hilbert-Schmidt fidelity between mixed states by combining a mixed-state quantum speed limit with mixed-state fidelity susceptibility within the Liouville space formulation of quantum mechanics. Applied to protocols that drive an initial Gibbs state toward a quasi-Gibbs target, these bounds yield an explicit threshold driving rate for the onset of nonadiabaticity. For a broad class of local Hamiltonians in gapped phases, we show that, in the thermodynamic limit, the threshold factorizes into two factors: a system-size contribution that recovers the zero-temperature scaling and a universal temperature-dependent factor. The latter is exponentially close to unity at low temperature, whereas at high temperature it increases linearly with temperature. We verify the predicted scaling in several spin-1/2 chains by obtaining closed-form expressions for the threshold driving rate. Our results provide practical and largely model-independent criteria for finite-temperature adiabaticity in closed many-body systems.

</details>


### [480] [Real-time detection of correlated quasiparticle tunneling events in a multi-qubit superconducting device](https://arxiv.org/abs/2602.01945)
*Simon Sundelin,Linus Andersson,Hampus Brunander,Simone Gasparinetti*

Main category: quant-ph

TL;DR: 该研究通过同时检测两个共置的电荷敏感transmon中的准粒子隧穿事件，实现了对超导电路中准粒子隧穿事件的实时检测，揭示了准粒子爆发的时空特性和相关性。


<details>
  <summary>Details</summary>
Motivation: 准粒子隧穿事件是超导电路中退相干和相关误差的来源，需要实时检测单个器件上的准粒子隧穿事件以理解和最终减轻这些误差。

Method: 同时检测两个共置的电荷敏感transmon中准粒子隧穿事件，这些transmon耦合到共同的波导，使用时间标记的符合分析来研究事件相关性。

Result: 测量到单赫兹水平的背景准粒子隧穿率，时间分辨率数十微秒；发现单个事件在器件间不相关，但爆发事件约每分钟一次且高度相关；爆发特征寿命7毫秒，使准粒子隧穿率增加千倍；还发现每小时约一次伴随偏移电荷变化的罕见爆发。

Conclusion: 建立了一种实用且可扩展的方法来识别超导电路中的准粒子爆发及其相关性和空间结构，为抑制超导量子处理器中的相关误差提供了途径。

Abstract: Quasiparticle tunneling events are a source of decoherence and correlated errors in superconducting circuits. Understanding and ultimately mitigating these errors calls for real-time detection of quasiparticle tunneling events on individual devices. In this work, we simultaneously detect quasiparticle tunneling events in two co-housed, charge-sensitive transmons coupled to a common waveguide. We measure background quasiparticle tunneling rates at the single-hertz level, with temporal resolution of tens of microseconds. Using time-tagged coincidence analysis, we show that individual events are uncorrelated across devices, whereas burst episodes occur about once per minute and are largely correlated. These bursts have a characteristic lifetime of 7 ms and induce a thousand-fold increase in the quasiparticle tunneling rate across both devices. In addition, we identify a rarer subset of bursts which are accompanied by a shift in the offset charge, at approximately one event per hour. Our results establish a practical and extensible method to identify quasiparticle bursts in superconducting circuits, as well as their correlations and spatial structure, advancing routes to suppress correlated errors in superconducting quantum processors.

</details>


### [481] [Scalable Quantum-Classical DFT Embedding for NISQ Molecular Simulation](https://arxiv.org/abs/2602.01994)
*Namrata Manglani,Samrit Kumar Maity,Ranjit Thapa,Sanjay Wandhekar*

Main category: quant-ph

TL;DR: QDFT方法在NISQ硬件上实现可扩展的量子-经典嵌入，在固定六轨道活性空间中恢复60-68%的相关能，仅需2次迭代收敛，为NISQ时代模拟提供实用指南。


<details>
  <summary>Details</summary>
Motivation: 在近期的NISQ硬件上实现化学上有意义的模拟需要可扩展的量子-经典嵌入方法，以有效恢复DFT基准之外的相关能。

Method: 使用量子密度泛函理论(QDFT)方法，在固定六轨道活性空间中进行量子-经典嵌入，通过改变嵌入电子数(2-8个)和活性空间大小，以CCSD为基准评估相关能恢复效果。

Result: 芳香体系相关能恢复率接近63-64%，线性分子如二氧化碳达到68%；所有体系在两次嵌入迭代内收敛；使用(4e,6o)活性空间和10个量子比特可恢复约60%的相关能。

Conclusion: QDFT方法在NISQ硬件上表现出稳健性，为化学模拟提供了实用的量子-经典嵌入方案，通过少量迭代和适中的量子资源即可获得有意义的相关能恢复。

Abstract: Scalable quantum-classical embedding is essential for chemically meaningful simulations on near-term NISQ hardware. Using QDFT, we show systematic recovery of correlation energy relative to the DFT baseline, benchmarked against CCSD in a fixed six-orbital active space across molecules ranging from water to naphthalene. By varying the number of embedded electrons from 2 to 8, aromatic systems saturate near 63-64 percent, while linear molecules such as carbon dioxide reach 68 percent. All systems converge within two embedding iterations under relaxed self-consistency thresholds, highlighting the robustness of the approach. A (4e,6o) active space recovers approximately 60 percent correlation using 10 qubits, providing practical guidelines for NISQ-era simulations.

</details>


### [482] [On Quantum Learning Advantage Under Symmetries](https://arxiv.org/abs/2602.02008)
*Tuyen Nguyen,Mária Kieferová,Amira Abbas*

Main category: quant-ph

TL;DR: 量子统计查询(QSQ)模型在对称函数学习中可以提供指数级优势，但大多数常见对称性下量子与经典SQ的查询复杂度下界相同，只有在轨道分布高度偏斜时量子才可能有优势，且量子学习器对噪声容忍度更高。


<details>
  <summary>Details</summary>
Motivation: 研究量子学习算法能否在对称性约束下获得根本性优势，基于经典统计查询(SQ)框架在对称函数类学习上表现出指数查询复杂度的证据，探索量子统计查询(QSQ)模型中的对称性潜在优势。

Method: 在量子统计查询(QSQ)模型框架下研究对称函数学习，分析不同对称性条件下的查询复杂度，包括排列不变函数类、常见对称性以及噪声容忍度。

Result: 发现三种现象：1) 在排列不变函数类上QSQ与SQ存在指数级分离；2) 大多数常见对称性下QSQ查询复杂度下界与经典SQ相同；3) 量子学习器在噪声容忍度上优于经典SQ算法。

Conclusion: 对称性可以在特定条件下为量子学习提供优势，特别是在轨道分布高度偏斜时，量子学习器对噪声有更高容忍度，这为理解何时对称性能实现量子学习优势提供了见解。

Abstract: Symmetry underlies many of the most effective classical and quantum learning algorithms, yet whether quantum learners can gain a fundamental advantage under symmetry-imposed structures remains an open question. Based on evidence that classical statistical query ($\SQ$) frameworks have revealed exponential query complexity in learning symmetric function classes, we ask: can quantum learning algorithms exploit the problem symmetry better? In this work, we investigate the potential benefits of symmetry within the quantum statistical query ($\QSQ$) model, which is a natural quantum analog of classical $\SQ$. Our results uncover three distinct phenomena: (i) we obtain an exponential separation between $\QSQ$ and $\SQ$ on a permutation-invariant function class; (ii) we establish query complexity lower bounds for $\QSQ$ learning that match, up to constant factors, the corresponding classical $\SQ$ lower bounds for most commonly studied symmetries; however, the potential advantages may occur under highly skewed orbit distributions; and (iii) we further identify a tolerance-based separation exists, where quantum learners succeed at noise levels that render classical $\SQ$ algorithms ineffective. Together, these findings provide insight into when symmetry can enable quantum advantage in learning.

</details>


### [483] [A Schwinger-Keldysh Formulation of Semiclassical Operator Dynamics](https://arxiv.org/abs/2602.02106)
*Jeff Murugan,Hendrik J. R. van Zyl*

Main category: quant-ph

TL;DR: 本文提出了一种实时Schwinger-Keldysh形式的Krylov动力学框架，将Krylov复杂度视为闭合时间路径积分生成的内-内可观测量，揭示了相空间描述和有效哈密顿量，并提供了复杂度的涨落和大偏差分析。


<details>
  <summary>Details</summary>
Motivation: 现有Krylov复杂度研究主要关注平均值，缺乏对涨落和大偏差的系统性分析。需要建立场论框架来统一描述算子增长，特别是揭示可积-混沌交叉的精细特征。

Method: 采用实时Schwinger-Keldysh形式，将Krylov复杂度构建为闭合时间路径积分生成的可观测量。Lanczos系数定义了控制算子沿Krylov链运动的有效哈密顿量，在半经典极限下分析相空间轨迹。

Result: 指数复杂度增长对应双曲轨迹，渐近线性Lanczos增长表现为普适混沌不动点。涨落分析揭示了可积-混沌交叉的尖锐特征，这些特征在平均值层面不可见。

Conclusion: 该框架将Krylov复杂度重组为动力学场论框架，识别了封闭量子系统中算子增长的新涨落诊断工具，为理解量子混沌和算子动力学提供了新视角。

Abstract: In this work we develop a real-time Schwinger-Keldysh formulation of Krylov dynamics that treats Krylov complexity as an in-in observable generated by a closed time contour path integral. The resulting generating functional exposes an emergent phase-space description in which the Lanczos coefficients define an effective Hamiltonian governing operator motion along the Krylov chain. In the semiclassical limit, exponential complexity growth arises from hyperbolic trajectories, and asymptotically linear Lanczos growth appears as a universal chaotic fixed point, with sub-leading deformations classified as irrelevant, marginal or relevant. Going beyond the saddle, the Schwinger-Keldysh framework provides controlled access to fluctuations and large deviations of Krylov complexity, revealing sharp signatures of integrability-chaos crossovers that are invisible at the level of the mean. This formulation reorganises Krylov complexity into a dynamical field-theoretic framework and identifies new fluctuation diagnostics of operator growth in closed quantum systems.

</details>


### [484] [Towards Ultimate Accuracy in Quantum Multi-Class Classification: A Trace-Distance Binary Tree AdaBoost Classifier](https://arxiv.org/abs/2602.02120)
*Xin Wang,Yabo Wang,Rebing Wu*

Main category: quant-ph

TL;DR: 提出TTA量子多分类器，通过构建层次二叉树和AdaBoost集成学习，将复杂多分类问题分解为多个浅层量子电路，提高训练性和资源效率。


<details>
  <summary>Details</summary>
Motivation: 解决量子多分类中存在的训练困难（如barren-plateau问题）、资源效率低和可扩展性差的问题，为近期量子平台提供实用的多分类解决方案。

Method: 构建基于迹距离最大化的层次二叉树，每个内部节点选择使平均量子态迹距离最大的二分划分，每个节点训练一个由浅层变分量子基学习器组成的AdaBoost集成模型。

Result: TTA在测试准确率上达到约100%，优于量子和经典基线方法，对常见量子误差具有鲁棒性，实现了累计10000层和0.2M参数的聚合系统（由多个浅层电路组成）。

Conclusion: TTA为近期量子平台提供了资源高效、可扩展的多类量子机器学习路径，通过分布式浅层电路架构缓解了训练困难，同时保持了泛化能力。

Abstract: We propose a Trace-distance binary Tree AdaBoost (TTA) multi-class quantum classifier, a practical pipeline for quantum multi-class classification that combines quantum-aware reductions with ensemble learning to improve trainability and resource efficiency. TTA builds a hierarchical binary tree by choosing, at each internal node, the bipartition that maximizes the trace distance between average quantum states; each node trains a binary AdaBoost ensemble of shallow variational quantum base learners. By confining intrinsically hard, small trace distance distinctions to small node-specific datasets and combining weak shallow learners via AdaBoost, TTA distributes capacity across many small submodels rather than one deep circuit, mitigating barren-plateau and optimization failures without sacrificing generalization. Empirically TTA achieves top test accuracy ($\approx $100\%) among quantum and classical baselines, is robust to common quantum errors, and realizes aggregate systems with 10000 cumulative layers and 0.2M parameters, implemented as many shallow circuits. Our results are empirical and implementable on near-term platforms, providing a resource-efficient route to scalable multi-class quantum machine learning.

</details>


### [485] [AQER: a scalable and efficient data loader for digital quantum computers](https://arxiv.org/abs/2602.02165)
*Kaining Zhang,Xinbiao Wang,Yuxuan Du,Min-Hsiu Hsieh,Dacheng Tao*

Main category: quant-ph

TL;DR: 提出AQER方法，通过系统性地减少目标态中的纠缠来构建量子加载电路，在精度和门效率上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 量子计算面临量子资源稀缺的挑战，特别是如何高效地将经典或量子数据加载到量子电路中。现有的近似量子加载器方法要么是启发式的，要么只对特定输入类型提供保证，缺乏通用理论框架。

Method: 将现有AQL方法统一到一个框架中，建立信息论误差界限，并开发AQER方法——通过系统性地减少目标态中的纠缠来构建加载电路

Result: AQER在合成数据集、经典图像/语言数据集和最多50个量子比特的量子多体态数据集上，在精度和门效率方面均优于现有方法

Conclusion: 该工作为可扩展的量子数据处理和实际量子计算应用铺平了道路，建立了AQL的统一理论框架并提出了高效的AQER方法

Abstract: Digital quantum computing promises to offer computational capabilities beyond the reach of classical systems, yet its capabilities are often challenged by scarce quantum resources. A critical bottleneck in this context is how to load classical or quantum data into quantum circuits efficiently. Approximate quantum loaders (AQLs) provide a viable solution to this problem by balancing fidelity and circuit complexity. However, most existing AQL methods are either heuristic or provide guarantees only for specific input types, and a general theoretical framework is still lacking. To address this gap, here we reformulate most AQL methods into a unified framework and establish information-theoretic bounds on their approximation error. Our analysis reveals that the achievable infidelity between the prepared state and target state scales linearly with the total entanglement entropy across subsystems when the loading circuit is applied to the target state. In light of this, we develop AQER, a scalable AQL method that constructs the loading circuit by systematically reducing entanglement in target states. We conduct systematic experiments to evaluate the effectiveness of AQER, using synthetic datasets, classical image and language datasets, and a quantum many-body state datasets with up to 50 qubits. The results show that AQER consistently outperforms existing methods in both accuracy and gate efficiency. Our work paves the way for scalable quantum data processing and real-world quantum computing applications.

</details>


### [486] [The trouble with recording devices](https://arxiv.org/abs/2602.02191)
*Eric Tesse*

Main category: quant-ph

TL;DR: 量子理论在描述记录设备时遇到困难，特别是当记录涉及量子不确定性事件时。作者提出对玻恩规则进行轻微修正，以正确预测所有概率，并澄清量子理论在连续测量和包含所有观察者的封闭系统等场景中的应用。


<details>
  <summary>Details</summary>
Motivation: 量子理论在描述记录设备时存在根本性困难：当记录涉及量子不确定性事件（如量子系统实验）时，量子理论无法正确预测记录设备未来和过去状态的概率。这种困难源于量子理论在描述测量和记录过程时的局限性。

Method: 作者提出对玻恩规则进行轻微修正，通过修改量子概率的计算方式来正确预测所有概率。这种方法旨在解决量子理论在描述记录设备时的内在矛盾，特别是处理测量结果记录时的概率预测问题。

Result: 修正后的玻恩规则能够正确预测记录设备的所有概率，包括涉及量子不确定性事件的记录。这一修正不仅解决了原始量子理论在描述记录设备时的困难，还澄清了量子理论在连续测量和包含所有观察者的封闭系统等模糊场景中的应用。

Conclusion: 通过对玻恩规则进行轻微修正，可以解决量子理论在描述记录设备时的根本困难。这一修正不仅完善了量子概率的预测能力，还澄清了量子理论在多种复杂场景中的应用，为量子理论在测量和记录问题上的理解提供了重要进展。

Abstract: Quantum theory encounters a difficulty when attempting to describe recording devices. If the recording is of events in which quantum uncertainty plays a role, such as an experiment on a quantum system, quantum theory is unable to correctly predict the probabilities of both future and past states of the recording. The nature of this difficulty will be laid out at the outset. A resolution then will be presented, in which the Born rule will be lightly amended so as to correctly predict all probabilities. The resolution will have the further benefit of clarifying how quantum theory applies to an array of situations in which the theory can be ambiguous, such as the descriptions of continuous measurements, and of closed systems containing all observers.

</details>


### [487] [On the Spectral theory of Isogeny Graphs and Quantum Sampling of Hard Supersingular Elliptic curves](https://arxiv.org/abs/2602.02263)
*David Jao,Maher Mamah*

Main category: quant-ph

TL;DR: 本文提出了首个可证明的量子多项式时间算法，用于采样具有未知自同态环的随机超奇异椭圆曲线，解决了同源密码学中的关键难题。


<details>
  <summary>Details</summary>
Motivation: 许多同源密码协议的安全实例化依赖于能够采样"困难"曲线（具有未知自同态环的超奇异椭圆曲线）。现有方法仅在可信设置环境下实现这一目标，需要一种无需可信设置的采样方法。

Method: 提出了首个可证明的量子多项式时间算法，利用超奇异ℓ-同源图的新谱去局域化结果，包括证明量子唯一遍历猜想，并提供完全特征向量去局域化的数值证据。

Result: 算法在启发式下以Õ(log⁴p)量子门复杂度运行，在广义黎曼假设下以Õ(log¹³p)运行。该算法为CGL哈希函数和其他密码原语提供了安全实例化。

Conclusion: 该工作首次实现了无需可信设置的随机困难超奇异椭圆曲线采样，解决了同源密码学中的关键问题，同时提供了超奇异同源图谱性质的重要理论结果。

Abstract: In this paper we study the problem of sampling random supersingular elliptic curves with unknown endomorphism rings. This task has recently attracted significant attention, as the secure instantiation of many isogeny-based cryptographic protocols relies on the ability to sample such ``hard'' curves. Existing approaches, however, achieve this only in a trusted-setup setting. We present the first provable quantum polynomial-time algorithm that samples a random hard supersingular elliptic curve with high probability.Our algorithm runs heuristically in $\tilde{O}\!\left(\log^{4}p\right)$ quantum gate complexity and in $\tilde{O}\!\left(\log^{13} p\right)$ under the Generalized Riemann Hypothesis. As a consequence, our algorithm gives a secure instantiation of the CGL hash function and other cryptographic primitives.
  Our analysis relies on a new spectral delocalization result for supersingular $\ell$-isogeny graphs: we prove the Quantum Unique Ergodicity conjecture, and we further provide numerical evidence for complete eigenvector delocalization; this theoretical result may be of independent interest. Along the way, we prove a stronger $\varepsilon$-separation property for eigenvalues of isogeny graphs than that predicted in the quantum money protocol of Kane, Sharif, and Silverberg, thereby removing a key heuristic assumption in their construction.

</details>


### [488] [Optimal enhancement of the Overhauser and Solid Effects within a unified framework](https://arxiv.org/abs/2602.02309)
*Sarfraj Fency,Rangeet Bhattacharyya*

Main category: quant-ph

TL;DR: 该论文提出了一个统一的量子主方程框架来解释Overhauser效应和固体效应这两种动态核极化技术，预测了最优微波驱动振幅和电子-核耦合条件以实现最大增强。


<details>
  <summary>Details</summary>
Motivation: Overhauser效应和固体效应是两种广泛使用的动态核极化技术，分别适用于液体和固体体系。目前缺乏一个统一的框架来解释这两种效应，特别是预测最优操作条件以实现最大核极化增强。

Method: 采用最近提出的涨落正则化量子主方程框架，该方程能够预测偶极弛豫和驱动诱导耗散，以及标准环境耗散通道。使用这个统一框架来同时解释Overhauser效应和固体效应。

Result: 该统一方法成功解释了两种动态核极化技术，并预测了存在最优的微波驱动振幅来最大化Overhauser效应和固体效应的增强。同时还报告了电子-核耦合的最优增强条件。

Conclusion: 提出的量子主方程框架为理解Overhauser效应和固体效应提供了一个统一的理论基础，能够预测最优操作参数，对动态核极化实验的设计和优化具有重要指导意义。

Abstract: The Overhauser effect (OE) and the Solid effect (SE) are two Dynamic Nuclear Polarization techniques. These two-spin techniques are widely used to create nonequilibrium nuclear spin states having polarization far beyond its equilibrium value. OE is commonly encountered in liquids, and SE is a solid-state technique. Here, we report a single framework based on a recently proposed quantum master equation, to explain both OE and SE. To this end, we use a fluctuation-regularized quantum master equation that predicts dipolar relaxation and drive-induced dissipation, in addition to the standard environmental dissipation channels. Importantly, this unified approach predicts the existence of optimal microwave drive amplitudes that maximize the OE and SE enhancements. We also report optimal enhancement regime for electron-nuclear coupling for maximal enhancement.

</details>


### [489] [Guaranteeing Privacy in Hybrid Quantum Learning through Theoretical Mechanisms](https://arxiv.org/abs/2602.02364)
*Hoang M. Ngo,Tre' R. Jeter,Incheol Shin,Wanli Xing,Tamer Kahveci,My T. Thai*

Main category: quant-ph

TL;DR: HYPER-Q：结合经典噪声与量子噪声的混合机制，为量子机器学习提供差分隐私保护，在保持隐私预算的同时提升模型效用和对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 量子噪声通常被视为量子计算的主要挑战，但它也为隐私保护提供了独特机会。量子固有噪声可作为随机资源，结合经典机制满足差分隐私保证，从而减少所需经典扰动而不损害隐私预算，可能提升模型效用。然而，经典与量子噪声结合用于隐私保护尚未被探索。

Method: 提出HYPER-Q混合噪声机制，结合经典噪声和量子噪声来保护QML模型隐私。在差分隐私框架内对量子噪声进行严格分析，并与经典机制组合。提供隐私保证的全面分析和效用理论边界。

Result: HYPER-Q在多个真实数据集上实证表现优于现有经典噪声机制，在对抗鲁棒性方面表现更佳。理论分析建立了隐私保证和效用边界。

Conclusion: HYPER-Q成功将经典和量子噪声结合用于QML隐私保护，展示了量子噪声在隐私增强方面的潜力，为量子机器学习提供了新的隐私保护方案。

Abstract: Quantum Machine Learning (QML) is becoming increasingly prevalent due to its potential to enhance classical machine learning (ML) tasks, such as classification. Although quantum noise is often viewed as a major challenge in quantum computing, it also offers a unique opportunity to enhance privacy. In particular, intrinsic quantum noise provides a natural stochastic resource that, when rigorously analyzed within the differential privacy (DP) framework and composed with classical mechanisms, can satisfy formal $(\varepsilon, δ)$-DP guarantees. This enables a reduction in the required classical perturbation without compromising the privacy budget, potentially improving model utility. However, the integration of classical and quantum noise for privacy preservation remains unexplored. In this work, we propose a hybrid noise-added mechanism, HYPER-Q, that combines classical and quantum noise to protect the privacy of QML models. We provide a comprehensive analysis of its privacy guarantees and establish theoretical bounds on its utility. Empirically, we demonstrate that HYPER-Q outperforms existing classical noise-based mechanisms in terms of adversarial robustness across multiple real-world datasets.

</details>


### [490] [Resolving problems with the continuum limit in coherent-state path integrals](https://arxiv.org/abs/2602.02466)
*Oliwier Urbański*

Main category: quant-ph

TL;DR: 该论文解决了玻色子热相干态路径积分中的连续极限问题，通过构建三种不同哈密顿量排序（正规序、反正规序、对称序）的精确离散版本，并选择对称序作为所有哈密顿量的正确选择。


<details>
  <summary>Details</summary>
Motivation: 解决玻色子热相干态路径积分中的连续极限问题，因为不同哈密顿量排序会导致不同的连续版本，需要确定哪种排序适用于所有哈密顿量。

Method: 1. 为三种不同哈密顿量排序（正规序、反正规序、对称序）构建精确的离散路径积分版本；2. 在谐振子模型上检验不同连续版本；3. 通过重正化程序从精确离散情况推导连续路径积分，为对称序提供一般性证明。

Result: 确定对称序（Weyl序）是所有哈密顿量的正确选择，简化了先前仅使用产生和湮灭算符（无需位置和动量算符）的构造，并提供了对称序适用于每个哈密顿量的缺失证明。

Conclusion: 对称序（Weyl序）是玻色子热相干态路径积分连续极限的正确选择，适用于所有哈密顿量，论文通过重正化程序提供了严格的证明，并简化了构造方法。

Abstract: The paper solves the problem of continuum limit in bosonic thermal coherent-state path integrals. For this purpose, exact discrete versions of the path integral are constructed for three different orderings of the Hamiltonian: normal, anti-normal and symmetric (Weyl order). Subsequently, their different continuum versions are checked on the harmonic oscillator, to choose the symmetric ordering as a possibly correct choice for all Hamiltonians. Spotted mathematical subtleties in the simple case serve as a clue to the general solution. Finally, a general justification for the symmetric order is provided by deriving the continuum path integral starting from the exact discrete case by a renormalization procedure in the imaginary time frequency domain. While the role of Weyl order has already been found, the paper provides the missing proof of its suitability for every Hamiltonian and simplifies the previously established construction by referring only to creation and annihilation operators (without position and momentum operators).

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [491] [Reheating in geometric Weyl-invariant Einstein-Cartan gravity](https://arxiv.org/abs/2602.00317)
*Ioannis D. Gialamas*

Main category: gr-qc

TL;DR: 研究爱因斯坦-嘉当框架下的Weyl不变纯引力理论，这些模型在爱因斯坦框架中等效于广义相对论耦合轴子类赝标量场，能自然驱动宇宙暴胀。研究表明，再加热过程对暴胀预测有重要影响。


<details>
  <summary>Details</summary>
Motivation: 探索爱因斯坦-嘉当框架下的Weyl不变引力理论，这些理论能自然地产生暴胀机制，同时研究再加热过程对暴胀观测预测的影响。

Method: 在爱因斯坦-嘉当框架下构建Weyl不变纯引力理论，将其转化为爱因斯坦框架下的广义相对论耦合轴子类赝标量场模型。通过分析再加热温度、状态方程参数等对暴胀观测量的影响，研究再加热动力学的重要性。

Result: 再加热动力学对暴胀预测有显著影响，再加热温度和状态方程参数的假设会显著改变暴胀观测量的预测值，表明在暴胀模型的现象学分析中必须一致地纳入再加热效应。

Conclusion: Weyl不变引力理论能自然地驱动暴胀，但暴胀预测强烈依赖于再加热过程的细节。在分析暴胀模型时，必须一致地考虑再加热动力学，否则可能导致不准确的观测预测。

Abstract: We study Weyl-invariant purely gravitational theories formulated within the Einstein-Cartan framework. In the Einstein-frame description, these models are dynamically equivalent to standard general relativity coupled to an axion-like pseudoscalar degree of freedom, which naturally drives a period of cosmic inflation. Without committing to a specific microscopic mechanism for reheating, we demonstrate that the post-inflationary reheating dynamics play a crucial role in shaping the inflationary predictions. In particular, we show that assumptions about the reheating temperature and the equation-of-state parameter can significantly affect the predicted values of inflationary observables, highlighting the necessity of consistently incorporating reheating effects in the phenomenological analysis of inflationary models.

</details>


### [492] [Jacobson's thermodynamic approach to classical gravity applied to non-Riemmanian geometries: remarks on the simplicity of Nature](https://arxiv.org/abs/2602.00422)
*Jhan N. Martinez,Jose F. Rodriguez-Ruiz,Yeinzon Rodriguez*

Main category: gr-qc

TL;DR: 该论文探讨了将Jacobson的热力学方法从黎曼几何推广到非黎曼几何（含挠率、非度量性）时，发现爱因斯坦-希尔伯特作用量在非黎曼情况下不是自然选择的引力理论，而需要添加挠率矢量的二次项。


<details>
  <summary>Details</summary>
Motivation: Jacobson的热力学方法成功地将引力场方程与热力学第一定律联系起来，但该方法仅针对黎曼几何。作者想知道如果将这种方法推广到包含挠率和非度量性的非黎曼几何，会产生什么后果。

Method: 将Jacobson的热力学方法（将引力场方程视为局部Rindler观测者的热力学第一定律）扩展到非黎曼几何。同时考虑了Lanczos-Lovelock引力理论的假设，寻找在非黎曼情况下自然选择的引力理论。

Result: 发现爱因斯坦-希尔伯特作用量在非黎曼情况下（除非是纯黎曼情况）不属于自然选择的引力理论池。在无非度量性的非黎曼情况下，自然选择的引力理论是爱因斯坦-希尔伯特作用量加上挠率矢量的二次项。但在完全非黎曼情况下，两种方法相互矛盾。

Conclusion: 热力学方法为引力理论的选择提供了新的视角，在非黎曼几何中，最简单的爱因斯坦-希尔伯特作用量需要修正，添加挠率矢量的二次项可能是自然的选择，但完全非黎曼情况下的理论选择问题仍未解决。

Abstract: Three decades ago, Ted Jacobson surprised us with a very appealing approach to classical gravity. According to his proposal, the gravitational field equations are the consequence of the first law of thermodynamics applied, locally, to a Rindler observer. Together with the dynamical laws of black holes, Jacobson's approach has become a very strong piece of evidence supporting the intimate connection between gravity, thermodynamics, and quantum theory. Jacobson's approach being formulated for Riemannian geometries, we have wondered what its consequences would be for non-Riemannian geometries, i.e., those that involve torsion, non metricity, or both. The results of our quest have been particularly appealing: we have found that the gravitational theory that derives from the Einstein-Hilbert action, arguably ``the simplest one'', does not belong to the pool of gravitational theories available for Nature's selection (except in the Riemannian case). In the search of a unique alternative, we have also considered the hypotheses employed in the formulation of the Lanczos-Lovelock theories of gravity. Together, the two approaches point towards the gravitational theory that derives from the Einstein-Hilbert action plus a term quadratic in the torsion vector as the one that would be selected by Nature in the non-Riemannian case without non metricity. The same strategy cannot be followed in the full non-Riemannian case as the two approaches are mutually inconsistent.

</details>


### [493] [Realization of quintom dark energy after DESI DR2 in Nieh-Yan modified teleparallel gravity](https://arxiv.org/abs/2602.00506)
*Yuxuan Kang,Mingzhe Li,Changzhi Yi*

Main category: gr-qc

TL;DR: 论文提出在teleparallel引力中引入暗能量与Nieh-Yan密度的耦合，以解决quintom暗能量模型中的微扰不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: DESI合作组观测显示暗能量状态方程可能跨越w=-1边界（quintom暗能量），但传统单完美流体或单标量场模型在跨越边界时存在微扰不稳定性，无法实现quintom场景。

Method: 在teleparallel引力中引入暗能量与Nieh-Yan密度的耦合。这种耦合不影响背景演化，但能消除暗能量微扰作为动力学自由度，从而避免传统模型的内在困难。

Result: 该方法成功绕过了quintom暗能量模型中的不稳定性问题，使得暗能量状态方程能够稳定地跨越w=-1边界，同时保持背景演化不受影响。

Conclusion: 通过teleparallel引力中暗能量与Nieh-Yan密度的耦合，可以构建稳定的quintom暗能量模型，为解释DESI观测数据提供理论框架。

Abstract: Recent observations from the DESI Collaboration indicate a preference for quintom dark energy, i.e., its equation of state evolves across the cosmological constant boundary $w=-1$. It is well known that models with single perfect fluid or single scalar field minimally coupled to Einstein gravity develop perturbative instabilities around the crossing, thereby cannot realize the quintom scenario. In this paper, we provide a method to circumvent the instability problem of these models by introducing the coupling of dark energy to the Nieh-Yan density in teleparallel gravity. We show that with this coupling the background evolution is not affected, but the dark energy perturbation is removed from the menu of dynamical degrees of freedom, thus avoiding the intrinsic difficulties in the old models.

</details>


### [494] [Charged Superradiant Instability of Spherically Symmetric Regular Black Holes in de Sitter Spacetime: Time- and Frequency-Domain Analysis](https://arxiv.org/abs/2602.00518)
*Yizhi Zhan,Hengyu Xu,Haowei Chen,Shao-Jun Zhang*

Main category: gr-qc

TL;DR: 研究ABG-dS黑洞在无质量带电标量扰动下的超辐射不稳定性，发现仅球对称模式不稳定，强调宇宙视界的关键作用，并分析参数依赖性。


<details>
  <summary>Details</summary>
Motivation: 研究非线性电动力学背景下Ayón-Beato-García-de Sitter黑洞的超辐射不稳定性，探索宇宙视界对不稳定性的影响，并与Reissner-Nordström-de Sitter黑洞进行比较。

Method: 使用时域演化和频域计算两种方法，分析无质量带电标量扰动下的黑洞稳定性，研究参数Λ、q、Q对不稳定性增长率的影响。

Result: 不稳定性仅出现在球对称模式(ℓ=0)，渐近平坦ABG黑洞在无质量极限下稳定；增长率在Λ和q的中间值达到最大，随Q单调增加；与RN-dS黑洞相比表现出不同的不稳定性特征。

Conclusion: 宇宙视界作为限制边界对超辐射不稳定性起关键作用，非线性电动力学修改的静电势导致ABG-dS黑洞具有独特的稳定性特征，为理解黑洞稳定性提供了新视角。

Abstract: We investigate the superradiant instability of Ayón-Beato-García-de Sitter (ABG-dS) black holes under massless charged scalar perturbations using both time-domain evolutions and frequency-domain computations. We show that the instability occurs only for the spherically symmetric mode with $\ell=0$, whereas asymptotically flat ABG black holes remain stable in the massless limit, which underscores the essential role of the cosmological horizon in providing a confining boundary. We further study the dependence of the growth rate on the cosmological constant $Λ$, the scalar charge $q$, and the black hole charge $Q$, finding that it reaches a maximum at intermediate values of $Λ$ and $q$ and increases monotonically with $Q$. Compared with Reissner-Nordström-de Sitter black holes, ABG-dS black holes exhibit distinct instability characteristics due to the modified electrostatic potential induced by nonlinear electrodynamics.

</details>


### [495] [Null Raychaudhuri Equation and the Impossibility of Traversable Wormholes in Unimodular Gravity](https://arxiv.org/abs/2602.00524)
*Erick Pastén,Marco Bosquez,Norman Cruz*

Main category: gr-qc

TL;DR: 在幺模引力中，任何真正可穿越的虫洞都必然违反零能量条件，这建立了该框架下普通物质支持虫洞的局部不可行定理。


<details>
  <summary>Details</summary>
Motivation: 研究幺模引力框架下虫洞的可穿越性问题，探索该理论中虫洞存在的可能性及其与能量条件的关系。

Method: 将虫洞喉部的可穿越性表述为从Raychaudhuri方程导出的局部协变零散焦条件。由于幺模引力保持了时空的局部几何结构，测地线汇的零聚焦性质与广义相对论相同。

Result: 证明在幺模引力中，任何真正可穿越的虫洞都必然违反零能量条件。

Conclusion: 幺模引力框架下存在局部不可行定理：普通物质无法支持可穿越虫洞，任何可穿越虫洞都需要违反零能量条件的奇异物质。

Abstract: We formulate the traversability of wormhole throats as a local and covariant null defocusing condition derived from the Raychaudhuri equation. Since unimodular gravity preserves the local geometric structure of spacetime, the null focusing properties of geodesic congruences are unchanged with respect to general relativity. We show that any genuinely traversable wormhole in unimodular gravity necessarily violates the null energy condition, establishing a local no--go theorem for wormholes supported by ordinary matter in this framework.

</details>


### [496] [Non-exotic asymptotically flat wormholes in $f(Q,T)$ gravity](https://arxiv.org/abs/2602.00527)
*Sara Rastgoo,Foad Parsaei,Soudabe Nasirimoghadam*

Main category: gr-qc

TL;DR: 在扩展f(Q,T)引力理论中，通过线性模型f(Q,T)=αQ+βT，研究了静态球对称虫洞解的存在性，发现某些在广义相对论中违反能量条件的解在f(Q,T)理论中可能满足能量条件。


<details>
  <summary>Details</summary>
Motivation: 研究扩展f(Q,T)引力理论中是否存在静态球对称虫洞解，探索该理论框架下虫洞解的性质及其与能量条件的关系。

Method: 采用线性模型f(Q,T)=αQ+βT，通过变分法推导修正的场方程，考虑各向异性物质源和零红移函数，使用径向压力的线性状态方程得到幂律形状函数，并研究可变状态方程参数定义的解。

Result: 发现能量条件的违反受参数α和β影响，在广义相对论中违反径向和横向零能量条件的解在f(Q,T)引力中可能满足能量条件，获得了幂律形状函数，并识别出依赖于模型参数的广泛非奇异虫洞解。

Conclusion: 在扩展f(Q,T)引力理论中，存在静态球对称虫洞解，这些解在某些参数条件下可以满足能量条件，为虫洞研究提供了新的理论框架。

Abstract: In this study, we investigate the possible existence of static and spherically symmetric wormhole solutions within the context of the newly formulated extended $f(Q,T)$ gravity. We analyze a linear model, $f(Q,T)=αQ+ βT$, and focus on traversable wormholes. By applying the variational method, we derive modified versions of the field equations that are influenced by an anisotropic matter source for a zero redshift function. It has been observed that the violation of energy conditions is influenced by the parameters $α$ and $β$. We reach the conclusion that solutions which violate the radial and lateral null energy condition in the context of general relativity may still adhere to the energy conditions within the realm of $f(Q,T)$ gravity. To begin with, by utilizing a linear equation of state for radial pressure, we obtain a power-law shape function. Additionally, we investigate solutions defined by a variable equation of state parameter. A broad spectrum of non-exotic wormhole solutions has been identified, contingent upon the particular parameters of the model.

</details>


### [497] [Chaotic Dynamics due Prolate and Oblate Sources in Kerr-like and Hartle-Thorne Spacetimes with and without Magnetic Field](https://arxiv.org/abs/2602.00565)
*Adrián Eduarte-Rojas,Francisco Frutos-Alfaro,Rodrigo Carboni*

Main category: gr-qc

TL;DR: 论文分析了描述旋转变形天体时空的几种度规（Kerr-like和Hartle-Thorne），比较了它们的特性，并研究了包含磁偶极矩贡献的扩展度规中的混沌轨道行为。


<details>
  <summary>Details</summary>
Motivation: 实际天体由于旋转而变形，不再是完美球体，需要比Kerr解更精确的度规来描述。现有Kerr-like和Hartle-Thorne度规各有特点，需要验证其有效性并扩展到包含磁偶极矩的情况。

Method: 选择了两种Hartle-Thorne度规版本（含对数项和指数项）进行测试，计算了运动方程，通过庞加莱截面分析系统的动力学特性，特别是混沌轨道行为。

Result: 含指数项的Hartle-Thorne度规比含对数项的传统版本更精确。当质量四极矩参数q_KL≠0时，系统失去Carter常数，运动方程不可积，出现混沌轨道。包含磁偶极矩贡献的扩展度规展现出有趣的动力学特征。

Conclusion: 对于描述旋转变形天体的时空，需要选择合适的度规模型。Hartle-Thorne的指数项版本更精确，而Kerr-like度规在包含变形参数时会破坏可积性，导致混沌行为。这些度规可进一步扩展到包含磁效应的情况。

Abstract: As demonstrated by observations, every stellar-mass object rotates around some axis; some objects spin faster than others due to different mechanisms. Furthermore, these spinning objects are slightly deformed and are no longer perfect spheres because of hydrostatic equilibrium. The well-known Kerr solution of the Einstein Field Equations (EFE) represents the spacetime surrounding a rotating spherical gravitational source. However, real objects deviate from a perfect sphere and may be prolate or oblate. There are several solutions of the EFE that represent the spacetime of deformed objects. The Kerr--like (KL) metric represents the spacetime surrounding this kind of object, where the deformation is characterized by the mass quadrupole moment parameter $q_{\mathrm{KL}}$. When $q_{\mathrm{KL}} \neq 0$, the Carter constant no longer exists and the equations of motion (EOM) are no longer integrable; therefore, the system exhibits chaotic orbits. Another widely used solution is the Hartle--Thorne (HT) metric, which has similar characteristics and represents a slightly deformed, slowly rotating star. The HT metric has several versions, and two of them were selected to test their validity. The traditional HT version, which contains logarithmic terms, is less accurate than the version with exponential terms. Moreover, both the KL and HT metrics may be extended to include contributions due to the magnetic dipole moment of the source. The equations of motion (EOM) were computed, and these new dynamical systems display several interesting features, which are shown in their Poincar'e sections.

</details>


### [498] [Path Integrated Geodesics and Distances](https://arxiv.org/abs/2602.00721)
*Nima Khosravi*

Main category: gr-qc

TL;DR: 研究量子修正对几何运动学（测地线）的影响，发现测地线本身无修正，但距离的量子修正对类时、类光、类空测地线有不同表现。


<details>
  <summary>Details</summary>
Motivation: 探索量子引力背景下几何运动学的量子修正，特别是测地线行为的量子效应，理解量子引力如何影响时空几何的基本结构。

Method: 采用路径积分方法处理测地线，通过测地线路径积分计算量子修正，分析不同测地线类型（类时、类光、类空）的量子修正行为。

Result: 测地线本身无量子修正；类时测地线距离修正最大为普朗克长度，经典分离为零时修正消失；类光测地线无量子修正，因果光锥保持不变；类空测地线存在最小空间分离，可能消除奇点。

Conclusion: 量子修正对不同类型的测地线距离有不同影响，类空测地线的最小分离可能消除奇点，同时建立了类时/类空测地线与量子/统计物理的对应关系。

Abstract: In this paper, the quantum corrections to the kinematics of geometry, specifically geodesics, are presented. This is done by employing the path integral over the geodesics. Interestingly, the geodesics do not see any modifications in this framework. However for the distances, it is demonstrated that these quantum corrections exhibit distinct behaviors for time-like, light-like, and space-like geodesics. For time-like geodesics, the maximum correction is the Planck length, which disappears when the classical separation vanishes. The light-like geodesics do not exhibit quantum corrections, meaning that the causal light cone remains the same in both classical and quantum frameworks under certain conditions. The quantum corrections for space-like geodesics impose a minimum on space-like separation, potentially playing a role in removing singularities by preventing null congruences from being closer than the Planck length. This framework also explores the correspondence between space-like/time-like geodesics and quantum/statistical physics.

</details>


### [499] [A Local Lorentz Invariance test with LAGEOS satellites](https://arxiv.org/abs/2602.00867)
*David Lucchesi,Massimo Visco,Roberto Peron,José C. Rodriguez,Giuseppe Pucacco,Luciano Anselmo,Massimo Bassan,Graham Appleby,Marco Cinelli,Alessandro Di Marco,Marco Lucente,Carmelo Magnafico,Carmen Pardini,Feliciana Sapio*

Main category: gr-qc

TL;DR: 利用LAGEOS卫星近30年轨道数据，通过分析宇宙微波背景辐射对卫星轨道的影响，将参数化后牛顿参数α₁约束到2×10⁻⁵水平，改进了月球激光测距技术的先前限制。


<details>
  <summary>Details</summary>
Motivation: 洛伦兹不变性可能在特定条件下被破坏，需要在所有背景下以更高精度进行实验验证。引力背景下的洛伦兹不变性测试困难且罕见，可能来自量子引力或存在与广义相对论度规张量共同传递引力相互作用的矢量/张量场。

Method: 分析LAGEOS和LAGEOS II卫星近30年的轨道数据，考虑宇宙微波背景辐射作为优先参考系对卫星轨道平均纬度参数的影响，这些效应主要通过后牛顿参数α₁（在广义相对论中为零）表现出来。

Result: 将参数化后牛顿参数α₁约束到α₁ ≤ 2×10⁻⁵的水平，这比之前通过月球激光测距技术获得的限制有所改进。

Conclusion: 通过卫星轨道分析为引力背景下洛伦兹不变性的测试提供了新的约束，支持了广义相对论在α₁参数上的预测，并为可能的引力相互作用新物理提供了限制。

Abstract: Strong theoretical arguments suggest that a breakdown of Lorentz Invariance could arise under some very particular conditions. From an experimental point of view, it is important to test the Local Lorentz Invariance with ever greater precision and in all contexts, regardless of the theoretical motivation for the possible violation. In this paper we discuss some aspects of the gravitational sector. Tests of Lorentz Invariance in the context of gravity are difficult and rare in the literature. Possible violations could arise from quantum physics applied to gravity or the presence of vector and tensor fields mediating the gravitational interaction together with the metric tensor of General Relativity. We present our results in the latter case. We analyzed the orbit of the LAGEOS and LAGEOS II satellites over a period of almost three decades. The effects of the possible preferred frame represented by the cosmic microwave background radiation on the mean argument of latitude of the satellites orbit were considered. These effects would manifest themselves mainly through the post-Newtonian parameter $α_1$, a parameter that has a null value in General Relativity. We constrain this parameterized post-Newtonian parameter down to the level of $α_1 \le 2\times10^{-5}$, improving a previous limit obtained through the Lunar Laser Ranging technique.

</details>


### [500] [$δN$ formalism with gradient interactions](https://arxiv.org/abs/2602.00902)
*S. Mohammad Ahmadi,Nahid Ahmadi*

Main category: gr-qc

TL;DR: 提出一种改进的δN形式，通过引入梯度修正项来处理超视界尺度非线性曲率扰动，特别适用于原初黑洞形成等需要梯度效应的场景。


<details>
  <summary>Details</summary>
Motivation: 标准δN形式基于分离宇宙假设，忽略空间梯度，但在原初黑洞形成等关键场景中（如超慢滚相变），梯度相互作用会导致共动曲率扰动的显著非守恒，标准方法失效。

Method: 在背景Klein-Gordon方程中添加有效源项，将梯度修正纳入δN形式到所需阶数，从而在考虑视界退出初始条件的情况下，处理暴胀结束时的非线性曲率扰动。

Result: 通过计算等边非高斯性参数f_NL^eq，证明该方法能捕捉标准δN遗漏的关键特征，提供了一条简单而严格的路径来确定宇宙学微扰理论预期的非线性演化。

Conclusion: 提出的框架将梯度修正纳入δN形式，为处理原初黑洞形成等需要梯度效应的场景中的非线性曲率扰动提供了更准确的方法，弥补了标准δN形式的不足。

Abstract: The standard $δN$ formalism, a cornerstone for calculating nonlinear curvature perturbations on super-Hubble scales, relies on the separate universe assumption, in which spatial gradients are neglected. However, this approximation breaks down in scenarios critical for primordial black hole formation, such as transitions to an ultra-slow-roll phase, where gradient interactions induce significant non-conservation of the comoving curvature perturbation. In this Letter, we introduce a framework that incorporates gradient corrections into the $δN$ formalism at a desired order by adding an effective source term to the background Klein--Gordon equation. This approach allows for a nonlinear treatment of curvature perturbations at the end of inflation considering initial conditions at the time of horizon exit. By computing the equilateral non-Gaussianity parameter $f_{\mathrm{NL}}^{\mathrm{eq}}$, we demonstrate that our method captures essential features missed by the standard $δN$, offering a simple yet rigorous pathway to determine nonlinear evolution expected from cosmological perturbation theory.

</details>


### [501] [Tripartite quantum steering in Schwarzschild spacetime](https://arxiv.org/abs/2602.00991)
*Guang-Wei Mi,Xiaofen Huang,Tinggui Zhang*

Main category: gr-qc

TL;DR: 研究霍金辐射对史瓦西时空三体系统中量子导引和导引不对称性的影响，发现霍金辐射在不同可访问模式场景下对量子导引有不同作用：增强、抑制或破坏。


<details>
  <summary>Details</summary>
Motivation: 探索弯曲时空中量子关联的性质，特别是霍金辐射如何影响量子导引现象，为量子引力效应提供可观测特征。

Method: 将三体系统嵌入史瓦西时空，分类所有三体导引类型（包括3个"1对2"和3个"2对1"情况），系统分析所有物理相关场景（可访问和不可访问模式），重点关注三个典型场景：1个、2个和3个物理可访问模式。

Result: 1）三个可访问模式时，霍金辐射破坏量子导引，最大导引不对称性精确标记双向导引向单向导引转变的相边界；2）两个可访问模式时，霍金辐射具有双重行为：在某些参数下增强Alice和Bob对anti-Charlie的导引，在其他参数下抑制，但总体上增强其他导引类型；3）一个可访问模式时，黑洞霍金效应显著增强量子导引。

Conclusion: 霍金辐射对量子导引的影响取决于可访问模式数量，为弯曲时空中的量子关联提供新见解，并建立了霍金效应在量子导引现象中的可观测特征。

Abstract: We investigate the effects of Hawking radiation on quantum steering and steering asymmetry in a tripartite system embedded in Schwarzschild spacetime. All tripartite steering types were classified,comprising three "1 to 2" and three "2 to 1" steering cases. Through a systematic analysis of all physically relevant scenarios (including accessible and inaccessible modes), we classify three canonical scenarios with one, two and three physically accessible modes. In the scenario of three physically accessible modes, Hawking radiation disrupts quantum steering, with the maximum steering asymmetry during the two-way steering to one-way steering transition precisely demarcating the phase boundary between these regimes. For two physically accessible modes, Hawking radiation exhibits dual behavior: enhancing the steering from Alice and Bob to anti-Charlie under certain parameters while suppressing it under others, while net strengthening other steering types. When considering one physically accessible mode, the Hawking effect of the black hole significantly enhances quantum steering. These findings provide new insights into quantum correlations in curved spacetime and establish observable signatures of Hawking effects in quantum steering phenomena.

</details>


### [502] [Ellis--Bronnikov wormhole in Quasi-topological Gravity](https://arxiv.org/abs/2602.01029)
*Gen Li,Yong-Qiang Wang*

Main category: gr-qc

TL;DR: 在准拓扑引力中构建了由幻影标量场支持的高维可穿越虫洞，数值分析了准拓扑引力修正对虫洞几何和物理性质的影响。


<details>
  <summary>Details</summary>
Motivation: 研究准拓扑引力修正如何影响虫洞的几何结构和物理性质，探索高维可穿越虫洞在修正引力理论中的存在性和特性。

Method: 采用静态球对称度规，在准拓扑引力框架下引入幻影标量场，通过数值方法分析不同参数对虫洞解的影响。

Result: 虫洞解关于喉部对称；特定参数下会出现负质量；幻影场的标量荷随高阶曲率耦合参数增大而快速减小至零；Kretschmann标量整体水平降低；足够大的耦合参数下，-g_{tt}在喉部附近趋近于零，呈现类似"视界"的结构。

Conclusion: 准拓扑引力修正显著影响虫洞的几何和物理性质，能够产生具有特殊性质（如负质量、类视界结构）的可穿越虫洞解，为研究修正引力中的虫洞物理提供了新视角。

Abstract: We construct higher-dimensional traversable wormholes in quasi-topological gravity (QTG) supported by a phantom scalar field. Using a static, spherically symmetric ansatz, we numerically analyze how quasi-topological gravity corrections affect the geometry and physical properties of the wormhole solutions. The resulting wormhole solutions are symmetric about the throat. Negative mass can arise for certain choices of parameters. For certain parameter ranges, the scalar charge $\mathcal{D}$ of the phantom field rapidly decreases with increasing the higher-curvature coupling parameter $α$ and approaches zero. Moreover, by changing $α$, the overall level of the Kretschmann scalar is also lowered. Finally, for sufficiently large $α$, $-g_{tt}$ becomes close to zero near the throat, exhibiting a ``horizon''-like structure.

</details>


### [503] [Charged nutty black holes are hairy](https://arxiv.org/abs/2602.01111)
*Dmitri Gal'tsov,Rostom Karsanov*

Main category: gr-qc

TL;DR: 该论文揭示了带电NUT黑洞Misner弦上电磁单极子的物理本质，证明这些弦携带奇异、非均匀的电磁场流，产生有效电荷密度，形成可观测的短程电磁毛发区。


<details>
  <summary>Details</summary>
Motivation: 研究McGuire和Ruffini在带电NUT黑洞Misner弦上发现的电磁单极子的物理本质，理解这些弦如何产生可观测的电磁效应，以及旋转如何影响毛发生成。

Method: 通过分析Misner弦上电磁场的性质，证明这些弦携带奇异、非均匀的电磁场流，这些场具有非零散度，从而模拟沿弦的有效电磁电荷密度。

Result: 发现Misner弦产生有效电磁电荷密度，形成围绕黑洞视界的复杂短程电磁毛发区，使Misner-Dirac弦成为经典可观测对象。旋转即使在无NUT情况下也能作为毛发生成器。

Conclusion: 带电NUT黑洞的Misner弦携带奇异电磁场流，产生有效电荷密度和可观测的短程电磁毛发，旋转本身也能生成毛发，这为黑洞物理学提供了新的观测特征。

Abstract: We uncover the physical nature of the electric and magnetic monopoles discovered by McGuire and Ruffini on Misner strings accompanying charged nutty black holes, showing that these strings carry singular, nonuniform flows of electric and magnetic fields. These fields inevitably have nonzero divergence, thereby simulating the effective electric and magnetic charge densities along the strings. The latter create a complex short-range electromagnetic hair zone around the horizon, making the combined Misner-Dirac strings classically observable. Typical features of this new type of hair are presented. We also note that rotation can act as a hair generator even in the absence of NUT.

</details>


### [504] [Testing the wormhole echo hypothesis for GW231123](https://arxiv.org/abs/2602.01615)
*Qi Lai,Qing-Yu Lan,Zhan-He Wang,Yun-Song Piao*

Main category: gr-qc

TL;DR: GW231123引力波事件在质量间隙且呈爆发形态，研究测试其是否与虫洞回波模型兼容，发现弱到中等支持回波假设，与GW190521相比更兼容单脉冲回波描述。


<details>
  <summary>Details</summary>
Motivation: GW231123事件具有质量间隙成分质量和爆发形态，缺乏明显螺旋特征，这使其成为测试标准双黑洞解释之外物理的有趣目标。研究基于其与GW190521的现象学相似性，探索虫洞回波解释的可能性。

Method: 使用正弦-高斯波包建模主导回波脉冲，进行贝叶斯模型比较：将虫洞回波模型与IMRPhenomXPHM-SpinTaylor波形描述的标准双黑洞基线进行比较。

Result: 获得贝叶斯因子比ln B^{Echo}_{BBH} = 1.87，对应弱到中等程度的回波假设支持。与GW190521的ln B^{Echo}_{BBH} ≈ -2.9相比，Δln B ≈ 4.8的差异表明GW231123更兼容单脉冲回波描述。

Conclusion: GW231123比GW190521更兼容虫洞回波解释，但证据强度仅为弱到中等水平。这一发现为探索超越标准双黑洞解释的引力波现象提供了新线索。

Abstract: The short-duration gravitational-wave (GW) event GW231123 has inferred component masses in the pair-instability mass gap and exhibits a burst-like morphology with no clearly inspiral, making it an interesting target for tests beyond the standard binary black hole (BBH) interpretation. In this work, motivated by its phenomenological similarity to GW190521, we test whether GW231123 is compatible with a wormhole-echo scenario by modeling a leading echo pulse with a well-motivated phenomenological sine-Gaussian wavepacket. We perform Bayesian model comparison against a BBH baseline described by the IMRPhenomXPHM-SpinTaylor waveform, and obtain the Bayes factor ratio $\ln B^{\rm Echo}_{\rm BBH} = 1.87$, corresponding to weak-to-moderate support for the echo hypothesis. In our previous analysis for GW190521 within the same overall framework, we found $\ln B^{\rm Echo}_{\rm BBH} \approx -2.9$, implying a shift of $Δ\ln B \approx 4.8$ between the two events. This sign change indicates that GW231123 is more compatible with a single-pulse echo description than GW190521.

</details>


### [505] [Reduced Phase Space Quantization and Quantum Corrected Entropy of Schwarzschild-de Sitter Horizons](https://arxiv.org/abs/2602.01767)
*S. Jalalzadeh,H. Moradpour*

Main category: gr-qc

TL;DR: 该论文在约化相空间框架下，使用Misner-Sharp-Hernandez质量作为内能，研究了Schwarzschild-de Sitter黑洞的量子化，得到了黑洞视界面积和MSH质量的离散谱，并推导出包含对数修正的熵公式。


<details>
  <summary>Details</summary>
Motivation: 研究Schwarzschild-de Sitter黑洞的量子化，探索在约化相空间框架下使用MSH质量作为内能时黑洞热力学的量子修正特性。

Method: 采用约化相空间框架，引入正则变量，使用Misner-Sharp-Hernandez质量作为内能，推导黑洞视界面积和MSH质量的离散谱，进而计算黑洞熵。

Result: 得到了黑洞事件视界面积和MSH质量的离散谱，推导出的黑洞熵和宇宙视界熵都包含Bekenstein-Hawking项的对数修正。

Conclusion: 研究结果支持了Schwarzschild-de Sitter黑洞热力学中量子修正对数形式的鲁棒性，为黑洞量子化提供了新的理论支持。

Abstract: This paper investigates the quantization of the Schwarzschild--de Sitter (SdS) black hole (BH) using the Misner--Sharp--Hernandez (MSH) mass as the internal energy in a reduced phase space framework. After introducing the canonical variables of the reduced phase space, we derive a discrete spectrum for the surface areas of the BH event horizon (EH) as well as MSH masses. We utilized the MSH mass spectrum to obtain the entropy of the BH. The entropy of the BH and cosmic EHs reveals a logarithmic correction to the Bekenstein--Hawking term. Our results support the robustness of the logarithmic form of quantum corrections in SdS thermodynamics.

</details>


### [506] [Hyperbolicity analysis of the linearised 3+1 formulation in the Teleparallel Equivalent of General Relativity](https://arxiv.org/abs/2602.01830)
*Cheng Cheng,Maria Jose Guzman*

Main category: gr-qc

TL;DR: 该论文研究了TEGR（广义相对论的远平行等价理论）中3+1运动方程的主符号特性，评估了双曲性条件，发现系统存在虚特征值导致非双曲性，这归因于特定规范选择而非TEGR的一般问题。


<details>
  <summary>Details</summary>
Motivation: 研究TEGR中3+1运动方程的双曲性条件，这对于建立数值相对论设置和证明球对称情况下的适定性至关重要。

Method: 采用基于VAST分解的哈密顿形式体系，使用文献中已有的哈密顿方程，在线性水平上研究微分方程组，分析主符号的特征值特性。

Result: 主符号存在虚特征值部分，导致系统非双曲性。这种状况在取一个或三个坐标方向的空间导数时持续存在，表明这是特定规范选择的问题而非TEGR的一般缺陷。

Conclusion: TEGR的3+1运动方程在特定规范选择下存在非双曲性问题，但本文建立的哈密顿方程方法可扩展用于证明球对称情况下的适定性，并为TEGR中的数值相对论设置奠定基础。

Abstract: We study the properties of the principal symbol of the 3+1 equations of motion in Teleparallel Equivalent of General Relativity (TEGR) and assess the conditions for hyperbolicity. We use the Hamiltonian formulation based on the vectorial, antisymmetric, symmetric trace-free, and trace (VAST) decomposition of the canonical variables in the Hamiltonian formalism, and the Hamilton's equations previously presented in the literature. We study the system of differential equations at the linear level, and show that the principal symbol has a sector with imaginary eigenvalues, which renders the system not hyperbolic. This situation persists by taking spatial derivatives in either one or three coordinate directions, and it should be interpreted as a problem of the specific gauge choice instead of a general problem with TEGR. The first practical use of Hamilton's equations in this work can be extended for proving well-posedness in spherical symmetry, and establish numerical relativity setups in TEGR.

</details>


### [507] [Cosmic evolution from Lorentz-violating bumblebee dynamics and Tsallis holographic dark energy](https://arxiv.org/abs/2602.02094)
*E. M. Siquieri,D. S. Cabral,A. F. Santos*

Main category: gr-qc

TL;DR: 该研究在洛伦兹破缺框架下，结合Tsallis全息暗能量，探讨宇宙的行为、演化和膨胀，通过自发对称破缺的Bumblebee场实现宇宙学扩展，为哈勃张力问题提供新视角。


<details>
  <summary>Details</summary>
Motivation: 探索洛伦兹破缺框架下的宇宙演化，结合Tsallis全息暗能量理论，为哈勃张力问题提供替代性解释方案。

Method: 采用自发对称破缺的Bumblebee场实现洛伦兹破缺的宇宙学扩展，结合Tsallis全息暗能量模型，分析关键洛伦兹破缺参数，研究从早期宇宙到当前时期的哈勃参数演化。

Result: 获得了关键洛伦兹破缺参数的估计值，分析了哈勃参数从早期宇宙到当前时期的演化轨迹，为哈勃张力问题提供了新的理论框架。

Conclusion: 洛伦兹破缺框架与Tsallis全息暗能量的结合为宇宙演化提供了新视角，为解决哈勃张力问题提供了有前景的替代方案。

Abstract: In this work, the behavior, evolution, and expansion of the universe are investigated within a Lorentz-violating framework driven by Tsallis holographic dark energy. The cosmological extension is implemented through a spontaneously symmetry-breaking Bumblebee field, which is assumed to play a fundamental role in the dynamics of the universe. Estimates for key Lorentz-violating quantities are obtained, and the evolution of the Hubble parameter is analyzed from the early universe era to the present epoch. This formulation provides an alternative perspective on the Hubble tension.

</details>


### [508] [Observable Optical Signatures, Particle Dynamics and Epicyclic Frequencies of Mod(A)Max Black Holes](https://arxiv.org/abs/2602.02116)
*Faizuddin Ahmed,Ahmad Al-Badawi,Edilberto O. Silva*

Main category: gr-qc

TL;DR: 研究Mod(A)Max黑洞时空的光学特征和粒子动力学，包括光子球、黑洞阴影、粒子轨道以及准周期振荡频率，分析这些可观测量如何依赖于几何参数。


<details>
  <summary>Details</summary>
Motivation: 探索Mod(A)Max黑洞时空的可观测光学特征，理解几何参数（如电荷和耦合参数）如何影响黑洞的光学特征和粒子动力学行为，为黑洞观测提供理论依据。

Method: 分析光子球、黑洞阴影和光子轨迹等光学特征；在哈密顿形式下推导有效势能，研究中性测试粒子的动力学；计算圆形轨道的特定能量和角动量、最内稳定圆轨道（ISCO）以及方位角、径向和垂直方向的回旋频率。

Result: 获得了Mod(A)Max黑洞的光学特征参数，包括光子球和阴影特性；确定了测试粒子的有效势能、圆形轨道参数和ISCO半径；推导了准周期振荡频率，并分析了这些量如何随几何参数变化。

Conclusion: Mod(A)Max黑洞的几何参数显著影响其光学特征和粒子动力学行为，这些结果为通过观测黑洞阴影、粒子轨道和准周期振荡来约束黑洞参数提供了理论框架。

Abstract: In this work, we investigate the observable optical signatures of the Mod(A)Max black hole spacetime. We analyze key optical features, including the photon sphere, black hole shadow, and photon trajectories, and examine how these observables depend on the underlying geometric parameters, such as the electric charge and the Mod(A)Max coupling parameter. We further study the dynamics of neutral test particles in the vicinity of the black hole by deriving the effective potential within the Hamiltonian formalism. Using this potential, we obtain the specific energy and specific angular momentum for test particles on circular orbits of fixed radius, as well as the innermost stable circular orbit (ISCO), and explore how the geometric parameters influence these quantities and the ISCO radius. Finally, we derive the epicyclic (azimuthal, radial, and vertical) frequencies to analyze quasi-periodic oscillations (QPOs) exploring how the geometric parameters influences these and discuss their physical implications.

</details>


### [509] [Radial perturbations of neutron stars in Scalar-Vector-Tensor (SVT)](https://arxiv.org/abs/2602.02272)
*Hamza Boumaza*

Main category: gr-qc

TL;DR: 研究SVT理论中中子星的平衡构型和径向扰动，分析标量化和电磁耦合对中子星结构的影响，并计算准正规模式和正规模式


<details>
  <summary>Details</summary>
Motivation: 研究标量-矢量-张量(SVT)理论中带电中子星的平衡构型和径向扰动，探索标量化现象（标量场与电磁张量和双对偶黎曼张量耦合）对中子星结构的影响，并验证广义相对论中稳定性与最大质量点的一致性是否在修正引力理论中仍然成立

Method: 1. 求解SVT理论中的广义Tolman-Oppenheimer-Volkoff方程，分析不同修正引力参数下的中子星平衡构型；2. 推导径向线性扰动到二阶的作用量；3. 计算标量准正规模式和正规模式

Result: 研究了标量化对带电中子星结构的影响，计算了径向扰动的准正规模式和正规模式，发现在SVT修正引力理论中，广义相对论中稳定性与最大质量点的一致性仍然成立

Conclusion: 在SVT修正引力理论中，带电中子星的自发标量化现象会影响其平衡构型，但广义相对论中稳定性与最大质量点的一致性特征在该理论中仍然保持

Abstract: In this paper, we investigate the equilibrium configurations and the radial perturbations of neutron stars in a subclass of Scalar-Vector-Tensor (SVT) theories. By solving the generalised Tolman-Oppenheimer-Volkoff equations in SVT theories for several values of the modified gravity parameter, we examine the impact of the spontaneous scalarization of charged neutron star (NSs), which arises from the coupling of the scalar field to the electromagnetic tensor and double-dual Reimann tensor, $L^{μναβ}F_{μν}F_{αβ}$. Then we extend our study by deriving the action at quadratic order in linear perturbations of radial type and computing scalar quasinormal modes (QNMs)as well as the normal modes (NMs) showing the coincidence of stability and maximum mass points in generlar relativity (GR) is still present in this modified theory.

</details>


### [510] [Complete asymptotics in the formation of quiescent big bang singularities](https://arxiv.org/abs/2602.02373)
*Andrés Franco-Grisales,Hans Ringström*

Main category: gr-qc

TL;DR: 该论文将关于静止大爆炸奇点的三类数学结果统一起来，证明Oude Groeniger等人的解能在奇点上诱导出初始数据，填补了第三类结果的空白。


<details>
  <summary>Details</summary>
Motivation: 关于静止大爆炸奇点的数学研究分为三类：对称类中的渐近分析、基于奇点初始数据的时空构造、以及无对称性下大爆炸形成的证明。虽然前两类已有统一几何视角，但第三类结果（特别是Oude Groeniger等人的工作）缺少解能在奇点上诱导初始数据的陈述。本文旨在填补这一空白，将三类结果统一起来。

Method: 通过证明Oude Groeniger等人关于大爆炸形成的解能够在奇点上诱导出初始数据，将第三类结果与前两类连接起来。方法具有一般性，可应用于其他规范条件。

Result: 成功证明Oude Groeniger等人的解确实能在奇点上诱导出初始数据，从而将三类关于静止大爆炸奇点的数学结果完全统一起来。这一结果比预期更一般，有望在其他规范条件下得出类似结论。

Conclusion: 本文完成了关于静止大爆炸奇点的三类数学结果的统一框架，证明了大爆炸形成解与奇点初始数据之间的内在联系，为相关研究提供了更完整的几何视角。

Abstract: There are three categories of mathematical results concerning quiescent big bang singularities: the derivation of asymptotics in a symmetry class; the construction of spacetimes given initial data on the singularity; and the proof of big bang formation in the absence of symmetries, including the proof of stable big bang formation. In a recent article, the first author demonstrated the existence of developments corresponding to a geometric notion of initial data on a big bang singularity. Moreover, this article, combined with previous articles by the second author, gives a unified and geometric perspective on large classes of seemingly disparate results in the first two categories. Concerning the third category, Oude Groeniger et al recently formulated a general condition on initial data ensuring big bang formation, including curvature blow up. This result, among other things, generalises previous results on stable big bang formation. However, it does not include a statement saying that the solutions induce initial data on the singularity. Here we tie all three categories of results together by demonstrating that the solutions of Oude Groeniger et al induce data on the singularity. However, the results are more general and can potentially be used to derive similar conclusions in other gauges.

</details>
