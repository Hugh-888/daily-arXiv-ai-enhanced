<div id=toc></div>

# Table of Contents

- [quant-ph](#quant-ph) [Total: 27]
- [gr-qc](#gr-qc) [Total: 9]
- [cs.LG](#cs.LG) [Total: 51]


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [1] [Investigation of Hardware Architecture Effects on Quantum Algorithm Performance: A Comparative Hardware Study](https://arxiv.org/abs/2601.05286)
*Askar Oralkhan,Temirlan Zhaxalykov*

Main category: quant-ph

TL;DR: 该研究系统性地在亚马逊Braket平台上对五个量子算法（贝尔态制备、GHZ态生成、量子傅里叶变换、Grover搜索、QAOA）在离子阱、超导和模拟器后端进行了基准测试，发现算法性能强烈依赖于硬件拓扑和噪声特性。


<details>
  <summary>Details</summary>
Motivation: 由于量子处理器架构差异（量子比特连接性、门保真度、相干时间等），相同的量子电路在不同设备上可能表现出显著不同的行为，需要系统性评估硬件对算法性能的影响。

Method: 使用亚马逊Braket平台，在离子阱、超导和模拟器三种后端上测试五个代表性量子算法，评估保真度、CHSH违反、成功概率、电路深度和门计数等性能指标。

Result: 算法性能强烈依赖于硬件拓扑和噪声特性：10量子比特GHZ态在离子阱硬件上保真度超过0.8，而在超导平台上由于路由开销和累积的双量子比特门误差降至0.15以下。

Conclusion: 研究强调了硬件感知的算法选择的重要性，为NISQ时代的基准测试提供了实用指导，表明需要根据硬件特性优化量子算法实现。

Abstract: Cloud-accessible quantum processors enable direct execution of quantum algorithms on heterogeneous hardware platforms. Unlike classical systems, however, identical quantum circuits may exhibit substantially different behavior across devices due to architectural variations in qubit connectivity, gate fidelity, and coherence times.
  In this work, we systematically benchmark five representative quantum algorithms - Bell state preparation, GHZ state generation, Quantum Fourier Transform (QFT), Grover's Search, and the Quantum Approximate Optimization Algorithm (QAOA) - across trapped-ion, superconducting, and simulator backends using Amazon Braket. Performance metrics including fidelity, CHSH violation, success probability, circuit depth, and gate counts are evaluated.
  Our results demonstrate a strong dependence of algorithmic performance on hardware topology and noise characteristics. For example, 10-qubit GHZ states achieved fidelities above 0.8 on trapped-ion hardware, while superconducting platforms dropped below 0.15 due to routing overhead and accumulated two-qubit gate errors. These findings highlight the importance of hardware-aware algorithm selection and provide practical guidance for benchmarking in the NISQ era.

</details>


### [2] [Temporal Kirkwood-Dirac Quasiprobability Distribution and Unification of Temporal State Formalisms through Temporal Bloch Tomography](https://arxiv.org/abs/2601.05294)
*Zhian Jia,Kavan Modi,Dagomir Kaszlikowski*

Main category: quant-ph

TL;DR: 将Kirkwood-Dirac准概率分布扩展到多时间量子过程和时空设置，建立统一的时域量子态操作基础


<details>
  <summary>Details</summary>
Motivation: 尽管时域量子态形式主义不断发展，但其精确的操作关系和概念区别仍不清楚，需要建立统一的操作基础

Method: 扩展Kirkwood-Dirac准概率分布到任意多时间量子过程和一般时空设置，定义左、右和双倍时域KD准概率及其实部（时域Margenau-Hill准概率）

Result: 这些量可通过干涉测量方案实验访问，通过表征其非经典特征，广义KD框架为广泛的时域态方法提供了统一的操作基础，可通过时域或时空Bloch层析直接实现

Conclusion: 广义KD框架解决了时域量子态形式主义的操作关系不明确问题，为时域和时空量子系统提供了统一的描述和实验实现方案

Abstract: Temporal quantum states generalize the multipartite density operator formalism to the time domain, enabling a unified treatment of quantum systems with both timelike and spacelike correlations. Despite a growing body of temporal state formalisms, their precise operational relationships and conceptual distinctions remain unclear. In this work, we resolve this issue by extending the Kirkwood-Dirac (KD) quasiprobability distribution to arbitrary multi-time quantum processes and, more broadly, to general spatiotemporal settings. We define left, right, and doubled temporal KD quasiprobabilities, together with their real components, which we identify as temporal Margenau-Hill (MH) quasiprobabilities. All of these quantities are experimentally accessible through interferometric measurement schemes. By characterizing their nonclassical features, we show that the generalized KD framework provides a unified operational foundation for a wide class of temporal state approaches and can be directly implemented via temporal or spatiotemporal Bloch tomography.

</details>


### [3] [Fundamental Limitations on the Reliabilities of Power and Work in Quantum Batteries](https://arxiv.org/abs/2601.05315)
*Brij Mohan,Tanmoy Pandit,Maciej Lewenstein,Manabendra Nath Bera*

Main category: quant-ph

TL;DR: 量子电池的可靠性受限于工作与功率的噪声信号比，存在量子不确定性关系导致工作与功率波动无法同时抑制，需要混合充电方案平衡功率与可靠性。


<details>
  <summary>Details</summary>
Motivation: 量子电池作为量子技术中的微观能量设备，虽然充电放电功率高，但其实际应用价值取决于可靠性。目前缺乏对量子电池可靠性的基本限制研究，特别是工作与功率波动之间的基本权衡关系。

Method: 通过分析噪声信号比（NSR）量化可靠性，建立工作与功率NSR的普遍下界。研究量子不确定性关系对工作与功率波动的限制。分析并行（局域）、集体（完全非局域）和混合（半局域）充电方案下的权衡关系，包括横向Ising类相互作用的充电方案。

Result: 发现量子电池存在固有可靠性限制：工作与功率NSR受充电速度函数下界约束。量子不确定性关系禁止工作与功率波动同时抑制。集体充电虽能提高功率但降低功率可靠性，并行充电可靠性高但功率有限。混合充电方案在功率与可靠性间取得最佳平衡。

Conclusion: 量子电池设计中存在功率与可靠性的基本权衡，既非完全并行也非完全集体充电，而是需要具有中等相互作用范围的混合充电方案，才能实现高性能与高可靠性的量子电池设计。

Abstract: Quantum batteries, microscopic devices designed to address energy demands in quantum technologies, promise high power during charging and discharging processes. Yet their practical usefulness and performance depend critically on reliability, quantified by the noise-to-signal ratios (NSRs), i.e., normalized fluctuations of work and power, where reliability decreases inversely with increasing NSR. We establish fundamental limits to this reliability: both work and power NSRs are universally bounded from below by a function of charging speed, imposing a reliability limit inherent to any quantum battery. More strikingly, we find that a quantum mechanical uncertainty relation forbids the simultaneous suppression of work and power fluctuations, revealing a fundamental trade-off that also limits the reliability of quantum batteries. We analyze the trade-off and limits, as well as their scaling behavior, across parallel (local), collective {(fully non-local)}, and hybrid (semi-local) charging schemes for many-body quantum batteries, finding that increasing power by exploiting stronger entanglement comes at the cost of diminished reliability of power. Similar trends are also observed in the charging of quantum batteries utilizing transverse Ising-like interactions. These suggest that achieving both high power and reliability require neither parallel nor collective charging, but a hybrid charging scheme with an intermediate range of interactions. Therefore, our analysis shapes the practical and efficient design of reliable and high-performance quantum batteries.

</details>


### [4] [From compatibility of measurements to exploring Quantum Darwinism on NISQ](https://arxiv.org/abs/2601.05350)
*Emery Doucet,Sebastian Deffner*

Main category: quant-ph

TL;DR: 研究量子达尔文主义破坏如何导致非经典测量统计，并开发用于NISQ硬件量子特性基准测试的工具


<details>
  <summary>Details</summary>
Motivation: 量子达尔文主义解释了经典现实性如何从量子世界中涌现，但需要研究其破坏机制以及如何转化为非经典测量统计，这有助于开发评估NISQ硬件量子特性的工具

Method: 在特定模型中研究量子达尔文主义的破坏，分析其如何转化为非经典测量统计，并利用IonQ的离子阱和IBM的超导量子计算平台进行实验验证

Result: 开发了有效的工具来基准测试NISQ硬件的真正量子特性，并在两个主要量子计算平台上进行了演示验证

Conclusion: 量子达尔文主义的破坏导致非经典测量统计，这为评估NISQ硬件的量子特性提供了有效的基准测试方法，有助于推进量子计算技术的发展

Abstract: Quantum Darwinism explains how tenets of classical reality, such as objectivity and repeatability, emerge within a quantum universe. As a mathematical framework, Quantum Darwinism also provides guiding principles that determine what physical models support emergent classical behavior, what specific observables obey classical laws, and much more. For instance, in a recent work we elucidated that the limit under which Kirkwood-Dirac quasiprobability distributions become effectively classical coincides with the regime where the underlying physical model obeys the rules of Quantum Darwinism. In the present work, we study the breaking of Quantum Darwinism in a specific model and how that translates to non-classical measurement statistics. Interestingly, this provides effective tools for benchmarking the genuine quantum characteristics of NISQ hardware, which we demonstrate with IonQ's trapped-ion and IBM's superconducting quantum computing platforms.

</details>


### [5] [Analytical Solutions to Asymmetric Two-Photon Rabi Model](https://arxiv.org/abs/2601.05421)
*M. Baradaran,L. M. Nieto,S. Zarrinkamar*

Main category: quant-ph

TL;DR: 在Segal-Bargmann表示中研究包含双光子和非对称项的广义Rabi模型，通过变换和Bethe ansatz方法获得近精确解，给出了四阶问题的精确解析解


<details>
  <summary>Details</summary>
Motivation: 研究广义Rabi模型，该模型包含双光子和非对称项，这些扩展项使得模型更加复杂，需要寻找有效的解析求解方法

Method: 采用Segal-Bargmann表示，通过合适的变换将问题简化，然后应用Bethe ansatz方法，分析所得微分方程的亚纯结构

Result: 获得了广义Rabi模型的近精确解，给出了四阶问题的精确解析形式，包括任意态的解和参数之间的约束关系

Conclusion: 通过Segal-Bargmann表示和Bethe ansatz方法，成功求解了包含双光子和非对称项的广义Rabi模型，为这类复杂量子光学模型提供了有效的解析求解途径

Abstract: Within the Segal-Bargmann representation, a generalized Rabi model is considered that includes both two-photon and asymmetric terms. It is shown that, through a suitable transformation, nearly exact solutions can be obtained using the Bethe ansatz approach. Applying this approach to the meromorphic structure of the resulting differential equation, solutions in exact analytical form of the fourth-order problem are presented for both an arbitrary state and for the restriction between the parameters.

</details>


### [6] [Achieving the Heisenberg limit using fault-tolerant quantum error correction](https://arxiv.org/abs/2601.05457)
*Himanshu Sahu,Qian Xu,Sisi Zhou*

Main category: quant-ph

TL;DR: 该论文研究了在完全噪声环境下（包括量子纠错操作）的容错量子计量学，提出了使用重复码和重复综合征测量的容错计量协议，证明了存在误差阈值，低于该阈值可实现海森堡极限。


<details>
  <summary>Details</summary>
Motivation: 量子计量学中，海森堡极限（HL）是量子力学允许的最终精度极限。虽然在噪声存在下HL通常无法达到，但量子纠错（QEC）可以在各种场景中恢复HL。然而，先前协议通常假设噪声仅影响信号积累步骤，而QEC操作（包括状态制备和测量）是无噪声的。为了克服这一限制，本研究旨在研究所有量子比特操作都受噪声影响的容错量子计量学。

Method: 研究在比特翻转噪声下估计Pauli-Z信号的场景，同时考虑所有QEC操作中的状态制备和测量误差。提出一个容错计量协议：通过重复综合征测量制备重复码，然后进行容错逻辑测量。该方法允许所有量子比特操作都存在噪声。

Result: 证明了存在一个误差阈值，低于该阈值时误差被有效抑制，并且可以实现海森堡极限。这表明即使在所有操作都存在噪声的情况下，通过适当的容错协议仍然可以达到量子计量学的最终精度极限。

Conclusion: 该研究克服了先前量子计量协议中QEC操作无噪声的假设限制，提出了一个完全容错的计量协议，证明了在存在状态制备和测量误差的情况下，通过重复码和适当的容错技术仍然可以实现海森堡极限，为实际量子计量应用提供了重要理论基础。

Abstract: Quantum effect enables enhanced estimation precision in metrology, with the Heisenberg limit (HL) representing the ultimate limit allowed by quantum mechanics. Although the HL is generally unattainable in the presence of noise, quantum error correction (QEC) can recover the HL in various scenarios. A notable example is estimating a Pauli-$Z$ signal under bit-flip noise using the repetition code, which is both optimal for metrology and robust against noise. However, previous protocols often assume noise affects only the signal accumulation step, while the QEC operations -- including state preparation and measurement -- are noiseless. To overcome this limitation, we study fault-tolerant quantum metrology where all qubit operations are subject to noise. We focus on estimating a Pauli-$Z$ signal under bit-flip noise, together with state preparation and measurement errors in all QEC operations. We propose a fault-tolerant metrological protocol where a repetition code is prepared via repeated syndrome measurements, followed by a fault-tolerant logical measurement. We demonstrate the existence of an error threshold, below which errors are effectively suppressed and the HL is attained.

</details>


### [7] [A three-dimensional multimode lumped-element resonator for collective spin manipulation and dispersive readout](https://arxiv.org/abs/2601.05476)
*Zhuo Chen,Wenhua Qin,Hanyu Ren,Ziyi Liu,Kae Nemoto,William John Munro,Yingqiu Mao,Johannes Majer*

Main category: quant-ph

TL;DR: 三维集总元件多模微波谐振器实现宏观自旋系综的集体强耦合和非破坏性色散读出


<details>
  <summary>Details</summary>
Motivation: 开发一种能够对宏观自旋系综进行均匀集体操控和色散读出的多模微波谐振器，用于混合自旋-光子系统和多模固态量子技术

Method: 利用几何对称性设计三维集总元件多模微波谐振器，工程化两个反对称模式，使其空间重叠但频率不同，同时耦合到同一自旋系综

Result: 在28 mK下使用金刚石中的氮空位中心，观察到5.0 MHz的集体强耦合强度，并通过失谐模式实现了非破坏性色散读出

Conclusion: 该谐振器具有紧凑设计、可调耦合和高场均匀性，是混合自旋-光子系统和多模固态量子技术的多功能器件

Abstract: We report a three-dimensional lumped-element multimode microwave resonator that enables homogeneous collective manipulation and dispersive readout of a macroscopic spin ensemble. By exploiting geometric symmetry, two antisymmetric modes with strongly suppressed cross-talk are engineered to spatially overlap and couple to the same ensemble at distinct frequencies. Using negatively charged nitrogen-vacancy centers in diamond at 28 mK, we observe collective strong coupling with a coupling strength of 5.0 MHz and demonstrate non-destructive dispersive readout via a detuned mode. The compact design, tunable coupling, and high field homogeneity make this resonator a versatile device for hybrid spin-photon systems and multimode solid-state quantum technologies.

</details>


### [8] [Emergence of the 2nd Law in an Exactly Solvable Model of a Quantum Wire](https://arxiv.org/abs/2601.05514)
*Marco A. Jimenez-Valencia,Charles A. Stafford*

Main category: quant-ph

TL;DR: 该论文通过可精确求解的量子导线模型研究焦耳加热中的熵产生，发现熵产生并非自动出现，而是需要大量局部测量或非弹性散射来实现。


<details>
  <summary>Details</summary>
Motivation: 研究热力学第二定律在微观量子系统中的具体实现，特别是焦耳加热过程中的熵产生问题。玻尔兹曼曾指出，热力学第二定律在统计论证中容易证明，但在微观描述中验证却越来越困难。

Method: 使用可精确求解的量子导线模型，考虑独立量子粒子系统的熵流精确公式，分析在电偏压下的焦耳加热过程。通过沿导线长度布置的一系列浮动热电探针进行局部测量来研究熵产生。

Result: 在精确的微观量子动力学描述中，焦耳加热的熵产生不会自动出现。只有当存在大量局部测量时，测量获得的信息将熵注入系统，才能实现预期的熵产生。局部测量引入的非弹性过程导致的退相干对熵产生至关重要。

Conclusion: 焦耳加热中的熵产生需要测量过程或非弹性散射来实现，这解释了为什么在真实相互作用粒子系统中，非弹性散射对于熵产生是必要的。研究揭示了测量在热力学第二定律微观实现中的关键作用。

Abstract: As remarked by Boltzmann, the Second Law of Thermodynamics is notable for the fact that it is readily proved using elementary statistical arguments, but becomes harder and harder to verify the more precise the microscopic description of a system. In this article, we investigate one particular realization of the 2nd Law, namely Joule heating in a wire under electrical bias. We analyze the production of entropy in an exactly solvable model of a quantum wire wherein the conserved flow of entropy under unitary quantum evolution is taken into account using an exact formula for the entropy current of a system of independent quantum particles. In this exact microscopic description of the quantum dynamics, the entropy production due to Joule heating does not arise automatically. Instead, we show that the expected entropy production is realized in the limit of a large number of local measurements by a series of floating thermoelectric probes along the length of the wire, which inject entropy into the system as a result of the information obtained via their continuous measurements of the system. The decoherence resulting from inelastic processes introduced by the local measurements is essential to the phenomenon of entropy production due to Joule heating, and would be expected to arise due to inelastic scattering in real systems of interacting particles.

</details>


### [9] [Narrowband four-photon states from spontaneous four-wave mixing](https://arxiv.org/abs/2601.05558)
*Yifan Li,Justin Yu Xiang Peh,Chang Hoong Chow,Boon Long Ng,Vindhiya Prakash,Christian Kurtsiefer*

Main category: quant-ph

TL;DR: 在冷铷-87原子云中通过双Λ方案实现连续波泵浦下的时间相关四光子产生，生成率约2.5×10⁶计数/秒，光子带宽MHz级，兼容原子量子网络应用


<details>
  <summary>Details</summary>
Motivation: 传统晶体中的χ²非线性过程需要高功率脉冲泵浦，而本研究旨在通过原子系统中的自发四波混频实现连续波泵浦下的相关四光子态生成，以产生与原子跃迁共振的窄带光子，便于量子网络应用

Method: 采用双Λ方案在冷铷-87原子云中进行自发四波混频，使用连续波泵浦而非高功率脉冲，通过高阶强度互相关测量和偶然符合扣除来验证真正的四光子关联态

Result: 在20ns相关窗口内观测到时间相关的四光子，推断出接近饱和时的四光子生成率为2.5(4)×10⁶计数/秒，光子与原子跃迁近共振，带宽为MHz量级

Conclusion: 该方案成功实现了连续波泵浦下的相关四光子态生成，产生的窄带、原子共振光子可直接用于基于原子的量子网络应用，为量子信息处理提供了实用光源

Abstract: We observe time-correlated four photons within a correlation window of 20ns from spontaneous four-wave mixing via a double-Lambda scheme in a cold cloud of Rb-87 atoms. In contrast to high-power pulsed pumping of chi^(2) nonlinear processes in crystals, our scheme generates correlated four-photon states by direct continuous-wave pumping at nominal powers. We verify the presence of genuinely correlated four-photon states over accidentals by higher-order intensity cross-correlation measurements and accidental subtraction. We infer a time-correlated four-photon generation rate of 2.5(4)x10^6 counts per second close to saturation. The photons produced are near-resonant with atomic transitions, and have a bandwidth in the order of MHz, making them readily compatible with quantum networking applications involving atoms.

</details>


### [10] [Bath-free squeezed phonon lasing via intrinsic ion-phonon coupling](https://arxiv.org/abs/2601.05575)
*Chen-Yu Lee,Guin-Dar Lin*

Main category: quant-ph

TL;DR: 提出一种在囚禁离子系统中实现压缩激光的理论模型，无需工程化浴或定制耗散储层，直接利用离子-声子相互作用实现压缩态。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要复杂的浴工程或耗散储层来产生压缩态，这增加了实验复杂性。本研究旨在探索一种更简单直接的方法，利用囚禁离子系统的本征相互作用实现压缩激光。

Method: 使用两个囚禁离子与共享振动模式相互作用，同时在红边带和蓝边带跃迁上驱动离子。通过离子内部态与声子模式之间的动态耦合产生压缩运动态。

Result: 模型表明压缩激光可以通过直接操控离子-声子相互作用实现，无需外部储层。分析了系统的稳态行为、激光产生条件、增益-损耗平衡以及压缩参数对声子场统计特性的影响。

Conclusion: 该方法为在声子基系统中实现压缩态提供了新途径，通过外部相干驱动可以稳定相位相干性并实现可控正交压缩，在量子计量学和信息处理中具有应用潜力。

Abstract: We present a theoretical model for realizing squeezed lasing in a trapped-ion system without relying on engineered baths or tailored dissipative reservoirs. Our approach leverages the intrinsic ion-phonon interactions, where two trapped ions, each interacting with a shared vibrational mode, are driven on both red- and blue-sideband transitions. This enables the creation of a squeezed state of motion through the dynamic coupling between the ions' internal states and the phonon mode. Unlike traditional methods that require bath engineering, our model demonstrates that squeezed lasing can be achieved through a direct manipulation of ion-phonon interactions, with no external reservoirs required. We explore the steady-state behavior of the system, analyzing the onset of lasing, gain-loss balance, and the role of the squeezing parameter in shaping the phonon field's statistical properties. Furthermore, we show how external coherent drives can stabilize phase coherence and achieve controlled quadrature squeezing, offering a simple yet effective method for achieving squeezed lasing in quantum mechanical systems. Our findings provide new insights into the realization of squeezed states in phonon-based systems, with potential applications in quantum metrology and information processing.

</details>


### [11] [Squeezing-Enhanced Two-Phase Estimation with N-Particle W-type States](https://arxiv.org/abs/2601.05595)
*Huan Zhang,Ying Xia,Xiuxing Zhang,Shoukang Chang,Wei Ye*

Main category: quant-ph

TL;DR: 本文研究在三模干涉仪中利用光学参量放大（OPA）同时估计两个光学相位，通过分析输出态的光子数矩和量子Fisher信息矩阵，证明OPA能显著提升多参数相位估计精度，并揭示了其物理机制源于模内光子数关联的放大而非模间关联。


<details>
  <summary>Details</summary>
Motivation: 研究光学参量放大（OPA）如何增强多参数量子计量中的相位估计精度，特别是在三模干涉仪中同时估计两个光学相位的场景，旨在理解OPA增强的物理机制并优化实际噪声环境中的相位估计协议。

Method: 采用正规序特征函数形式，解析获得输出量子态的所有光子数矩，从而显式计算多参数相位估计的量子Fisher信息矩阵；分析二阶关联函数；对于有损耗的实际情况，采用基于纯化的变分方法。

Result: 在无损耗情况下，均匀应用的OPA显著提高了可达到的精度，超越了未放大的干涉仪；这种增强源于模内光子数关联的放大而非模间关联；在适度损耗下，OPA辅助方案仍保持明显优势，显示出对耗散的一定鲁棒性。

Conclusion: 研究阐明了OPA增强多参数量子计量的物理机制，为在实际噪声环境中优化相位估计协议提供了指导，表明OPA辅助方案在适度损耗下仍具有优势。

Abstract: We investigate the simultaneous estimation of two optical phases in a three-mode interferometer assisted by optical parametric amplification (OPA). By employing the normally ordered characteristic-function formalism, we analytically obtain all photon-number moments of the output quantum state, enabling an explicit evaluation of the quantum Fisher information matrix for multiparameter phase estimation. In the lossless scenario, we show that uniformly applied OPA significantly enhances the attainable precision beyond that of an unamplified interferometer. By analyzing the second-order correlation functions, we demonstrate that this enhancement originates from the amplification of intra-mode photon-number correlations, rather than from inter-mode correlations. We further extend our analysis to realistic interferometers with photon loss using a purification-based variational approach. Although loss degrades the achievable precision, the OPA-assisted scheme retains a clear advantage for moderate loss, indicating a degree of robustness against dissipation. Our results clarify the physical mechanism underlying OPA-enhanced multiparameter quantum metrology and provide guidelines for optimizing phase estimation protocols in realistic noisy environments.

</details>


### [12] [Chaos, thermalization and breakdown of quantum-classical correspondence in a collective many-body system](https://arxiv.org/abs/2601.05627)
*Ángel L. Corps,Sebastián Gómez,Pavel Stránský,Armando Relaño,Pavel Cejnar*

Main category: quant-ph

TL;DR: 研究全连接Bose-Hubbard模型的热化和量子-经典对应关系，发现即使在相对大粒子数下，量子动力学仍受限于对称破缺区域，与经典相空间连通性不匹配，表明集体多体动力学中存在稳健的有限尺寸效应。


<details>
  <summary>Details</summary>
Motivation: 研究全连接Bose-Hubbard模型中的热化过程和量子-经典对应关系，特别关注四格点情况，探索量子系统如何趋近经典极限以及有限尺寸效应的影响。

Method: 分析经典相空间结构和激发态量子相变，识别三个动力学区域：对称破缺低能态、量子与经典平衡态显著不一致的中间区域、以及对应关系恢复的高能区域。对比经典间歇性和量子动力学行为。

Result: 发现经典相空间在第一个激发态量子相变以上出现间歇性，但量子动力学仍受限于对称破缺区域，即使经典相空间是连通的。这种不匹配源于不平衡本征态的占据，即使在相对大粒子数下也持续存在。

Conclusion: 量子系统向经典极限的收敛速度意外缓慢，表明集体多体动力学中存在稳健的有限尺寸效应，量子与经典对应关系在中间能量区域显著偏离。

Abstract: We investigate thermalization and the quantum-classical correspondence in the fully-connected Bose-Hubbard model, focusing on the four-site case. Our analysis of the classical phase-space structure and its excited-state quantum phase transitions leads us to three dynamical regimes: symmetry-breaking low-energy states, an intermediate region where quantum and classical equilibrium states markedly disagree, and a high-energy regime with restored correspondence. The observed classical intermittency above the first excited-state quantum phase transition contrasts with quantum dynamics, which remains trapped in symmetry-breaking sectors despite the existence of a classically connected phase space. This mismatch originates from the population of imbalance-carrying eigenstates and persists even for relatively large number of particles. Our results reveal unexpectedly slow convergence to the classical limit, signaling robust finite-size effects in collective many-body dynamics.

</details>


### [13] [Improving quantum interference visibility between independent sources by enhancing the purity of correlated photon pairs](https://arxiv.org/abs/2601.05671)
*Hsin-Pin Lo,Kai Asaoka,Hiroki Takesue*

Main category: quant-ph

TL;DR: 通过调节泵浦带宽和干涉滤波带宽两种方法提高PPLN波导产生的光子对纯度，两种方法都能将HOM干涉可见度提升至约80%，但前者还能获得更高的三重符合计数率，有利于多光子GHZ态制备。


<details>
  <summary>Details</summary>
Motivation: 独立光子间的高可见度量子干涉对于多光子量子信息处理至关重要，而干涉可见度与关联光子对的频谱纯度密切相关。本研究旨在提高0型PPLN波导产生的光子对纯度。

Method: 研究两种提高光子对纯度的方法：1）系统改变泵浦带宽；2）系统改变干涉滤波带宽。在相同实验条件下直接比较两种方法的性能，通过测量联合频谱强度并使用施密特分解来评估频谱纯度。

Result: 两种方法都能显著提高Hong-Ou-Mandel干涉可见度至约80%。但第一种方法（调节泵浦带宽）还能获得更高的三重符合计数率。

Conclusion: 调节泵浦带宽的方法在提高HOM干涉可见度的同时，还能获得更高的三重符合计数率，这对于提高多光子时间bin GHZ态的态保真度和生成率更为有利。

Abstract: High-visibility quantum interference between independent photons is essential for demonstrating multi-photon quantum information processing, and it is closely linked to the spectral purity of correlated photon pairs. In this study, we investigate two approaches to enhance the purity of photon pairs generated from a type-0 PPLN waveguide by systematically varying both the pump bandwidth and the interference-filter bandwidth, and we directly compare their performance under identical experimental conditions. The spectral purity is evaluated from measured joint spectral intensities using Schmidt decomposition. Both methods significantly improve the Hong-Ou-Mandel interference visibility to approximately 80%. However, the former approach also yields a higher three-fold coincidence rate, which is advantageous for our ongoing efforts to increase the state fidelity and generation rate of multi-photon time-bin Greenberger-Horne-Zeilinger (GHZ) states.

</details>


### [14] [Block Encoding Linear Combinations of Pauli Strings Using the Stabilizer Formalism](https://arxiv.org/abs/2601.05740)
*Niclas Schillo,Andreas Sturm,Rüdiger Quay*

Main category: quant-ph

TL;DR: 提出一种基于Pauli字符串线性组合的新型块编码方法，通过反交换变换和稳定子校正，实现比LCU方法更高效的量子电路构建。


<details>
  <summary>Details</summary>
Motivation: 量子奇异值变换(QSVT)框架需要高效的块编码实现，而现有线性组合单元(LCU)方法存在电路复杂度较高的问题，需要开发更高效的块编码方法以实现实际量子优势。

Method: 1) 将Pauli字符串转换为成对反交换的形式，使变换后的线性组合成为酉算子；2) 基于稳定子形式使用辅助寄存器进行校正变换，恢复原始Pauli字符串；3) 辅助寄存器大小随系统量子比特数对数增长，也可扩展以降低电路复杂度。

Result: 通过四个具体示例的数值模拟，该方法在电路复杂度上与LCU方法相当或更优，特别是在利用目标算子结构时具有优势，辅助寄存器大小对数缩放。

Conclusion: 该方法为Pauli字符串线性组合提供了高效的块编码方案，有望在QSVT框架中实现更高效的量子算法，超越本文分析的具体示例，适用于更广泛的相关问题。

Abstract: The Quantum Singular Value Transformation (QSVT) provides a powerful framework with the potential for quantum speedups across a wide range of applications. Its core input model is the block encoding framework, in which non-unitary matrices are embedded into larger unitary matrices. Because the gate complexity of the block-encoding subroutine largely determines the overall cost of QSVT-based algorithms, developing new and more efficient block encodings is crucial for achieving practical quantum advantage. In this paper, we introduce a novel method for constructing quantum circuits that block encode linear combinations of Pauli strings. Our approach relies on two key components. First, we apply a transformation that converts the Pauli strings into pairwise anti-commuting ones, making the transformed linear combination unitary and thus directly implementable as a quantum circuit. Second, we employ a correction transformation based on the stabilizer formalism which uses an ancilla register to restore the original Pauli strings. Our method can be implemented with an ancilla register whose size scales logarithmically with the number of system qubits. It can also be extended to larger ancilla registers, which can substantially reduce the overall quantum circuit complexity. We present four concrete examples and use numerical simulations to compare our method's circuit complexity with that of the Linear Combination of Unitaries (LCU) approach. We find that our method achieves circuit complexities comparable to or better than LCU, with possible advantages when the structure of the target operators can be exploited. These results suggest that our approach could enable more efficient block encodings for a range of relevant problems extending beyond the examples analyzed in this work.

</details>


### [15] [Quantum Interference-Induced Bhattacharyya Distance](https://arxiv.org/abs/2601.05749)
*Mostafizur Rahaman Laskar*

Main category: quant-ph

TL;DR: 提出基于量子干涉脆弱性的量子距离度量QIBD，用于测量量子态编码的概率分布之间的距离


<details>
  <summary>Details</summary>
Motivation: 现有基于保真度的度量无法充分捕捉量子态中的关联结构信息，需要开发能反映量子干涉特性的距离度量

Method: 使用单辅助比特干涉电路，通过相互作用哈密顿量产生依赖关联的相位，调制干涉可见度来定义QIBD

Result: QIBD在无相互作用时退化为经典Bhattacharyya距离，但在纠缠相互作用下不能仅用保真度表示；数值模拟显示QIBD对关联结构的响应与基于重叠的度量不同

Conclusion: QIBD能捕捉量子态中与相互作用对齐的关联结构，在物理相关场景中具有潜在应用价值

Abstract: We propose a quantum distance measure between probability distributions encoded in quantum states based on the fragility of quantum interference under entangling evolution. The Quantum Interference-Induced Bhattacharyya Distance (QIBD) is defined through a single-ancilla interferometric circuit in which an interaction Hamiltonian generates correlation-dependent phases that modulate interference visibility. When the interaction vanishes, QIBD reduces to the classical Bhattacharyya distance; however, for entangling interactions, it cannot be expressed as a function of fidelity alone. Numerical simulations demonstrate that QIBD responds to correlation structure in ways that overlap-based measures do not, suggesting potential utility in contexts where interaction-aligned correlations are physically relevant.

</details>


### [16] [Hidden time-nonlocal Floquet symmetries](https://arxiv.org/abs/2601.05783)
*Sigmund Kohler,Jesús Casado-Pascual*

Main category: quant-ph

TL;DR: 驱动二能级系统的Floquet谱在失谐为驱动场能量量子整数倍时出现精确准能交叉，这源于隐藏的时间非定域宇称对称性。


<details>
  <summary>Details</summary>
Motivation: 研究失谐驱动二能级系统的Floquet谱特性，特别是准能交叉现象，探索其背后的对称性原理。

Method: 通过隐藏的时间非定域宇称对称性分析Floquet模式，建立标量递推关系进行构造性证明，并开发适用于更广泛模型的数值计算方案。

Result: 当失谐为驱动场能量量子整数倍时，系统出现精确的准能交叉，不同宇称的Floquet模式之间形成交叉点。

Conclusion: 驱动二能级系统的Floquet谱具有隐藏的时间非定域宇称对称性，这解释了准能交叉现象，且该分析方法可推广到更复杂的模型。

Abstract: We investigate the Floquet spectrum of a detuned, driven two-level system and show that it exhibits exact quasienergy crossings when the detuning is an integer multiple of the energy quantum of the driving field. This behavior can be explained by a hidden time-nonlocal parity, which allows the Floquet modes to be classified as even or odd. Then a generic feature is the emergence of exact crossings between quasienergies of different parity. A constructive proof of the existence of the symmetry is based on a scalar recurrence relation. Moreover, we present a general scheme for its numerical computation, which can be applied to models beyond the two-level system. Analytical results are illustrated with numerical data.

</details>


### [17] [On the robustness of Quantum Phase Estimation to compute ground properties of many-electron systems](https://arxiv.org/abs/2601.05788)
*Wassil Sennane,Jérémie Messud*

Main category: quant-ph

TL;DR: 该论文提出了一种系统分析量子相位估计算法在电子系统应用中的自由参数的方法，并建立了实现化学精度的明确条件。


<details>
  <summary>Details</summary>
Motivation: 量子相位估计算法在计算化学和材料科学中有重要应用，但其多个自由参数（时间步长、相位量子比特数、初始态准备等）的选择缺乏系统性研究，需要开发整体参数选择方法以实现算法自动化。

Method: 首先回顾QPE的关键特性，然后提出构建性方法设置QPE自由参数，推导实现化学精度的明确条件，并通过H2分子的数值模拟验证方法。

Result: 建立了实现化学精度的明确条件，证明使用这些条件时，Trotter化QPE的复杂度主要取决于物理系统特性而非相位量子比特数，H2分子的数值模拟初步验证了方法的有效性。

Conclusion: 该研究为QPE在预测性计算化学和材料科学中的自动化应用铺平了道路，提供了系统化的参数选择框架，有助于推动量子计算在化学领域的实际应用。

Abstract: We propose an analysis of the Quantum Phase Estimation (QPE) algorithm applied to electronic systems by investigating its free parameters such as the time step, number of phase qubits, initial state preparation, number of measurement shots, and parameters related to the unitary operators implementation. A deep understanding of these parameters is crucial to pave the way towards more automation of QPE applied to predictive computational chemistry and material science. To our knowledge, various aspects remain unexplored and a holistic parameter selection method remains to be developed. After reviewing key QPE features, we propose a constructive method to set the QPE free parameters. We derive, among other things, explicit conditions for achieving chemical accuracy in ground energy estimation. We also demonstrate that, using our conditions, the complexity of the Trotterized version of QPE tends to depend only on physical system properties and not on the number of phase qubits. Numerical simulations on the H2 molecule provide a first validation of our approach.

</details>


### [18] [Optimally driving multi-photon transitions in the perturbative single-mode regime](https://arxiv.org/abs/2601.05854)
*Frieder Lindel,Stefan Yoshi Buhmann,Andreas Buchleitner,Edoardo G. Carnio*

Main category: quant-ph

TL;DR: 研究弱光场驱动多光子跃迁的最优光态，发现对于短寿命多能级原子系统，经典相干态混合是最优的，无需利用量子特性。


<details>
  <summary>Details</summary>
Motivation: 多光子跃迁速率取决于光场的高阶相干函数，因此可以通过调控光场的相干特性来提高多光子跃迁效率。研究在固定强度、窄带、光子数受限的弱光场条件下，如何优化光场状态以最大化驱动短寿命多能级原子系统的多光子跃迁。

Method: 分析弱固定强度、窄带入射光场的最优状态，限制最大光子数，针对短寿命原子多能级系统，确定驱动m光子跃迁的最佳光场状态。通过理论分析证明经典相干态混合是最优解。

Result: 研究发现对于短寿命多能级原子系统，无需利用光场的量子特性，经典相干态的混合就是驱动多光子跃迁的最优光场状态。

Conclusion: 在弱光场驱动短寿命多能级原子系统的多光子跃迁场景中，经典光场（相干态混合）已经足够优化，无需使用量子光场资源，这对实验实现具有重要指导意义。

Abstract: The rate of $m$-photon transitions in matter, induced by an incident light field, depends on the field's $m$th order coherence function. Consequently, the coherence properties of the light field may be shaped to increase the rate of multi-photon transitions. Here, we determine the optimal state of a weak fixed-intensity, narrow-band incident light field, with a restricted maximal photon number, that optimally drives $m$-photon transitions in the case of a short-lived atomic multilevel system. We show that, in this case, no quantum properties of the light field need to be exploited, but that classical mixtures of coherent states are optimal.

</details>


### [19] [Breaking the Exponential: Decoherence-Driven Power-Law Spontaneous Emission in Waveguide Quantum Electrodynamics](https://arxiv.org/abs/2601.05884)
*Stefano Longhi*

Main category: quant-ph

TL;DR: 研究发现光子波导中动态退相会显著改变二能级系统的自发辐射衰减规律，导致短时幂律衰减而非传统指数衰减


<details>
  <summary>Details</summary>
Motivation: 探索在光子波导量子电动力学平台中，动态退相对二能级系统自发辐射衰减规律的影响，揭示退相诱导的非指数衰减新机制

Method: 研究二能级系统与光子波导耦合，分析动态退相在光子模式中的作用，比较有无退相情况下的衰减规律

Result: 无退相时呈现传统指数衰减加长时幂律尾迹；引入退相后，短时即出现由光子扩散驱动的稳健幂律衰减，而非谱边缘效应

Conclusion: 动态退相通过光子扩散机制在波导QED平台中诱导出新的非指数自发辐射衰减现象，揭示了退相干诱导的衰减规律转变

Abstract: We investigate the spontaneous emission of a two-level system coupled to a photonic waveguide, showing that dynamical dephasing in the photon modes profoundly alters the decay law. In the absence of dephasing, the emitter displays conventional exponential decay followed by a long-time power-law tail -- observable only at extremely low survival probabilities. Strikingly, when dephasing is introduced, a robust power-law decay emerges already at short times, driven by photon diffusion in the dynamically disordered environment rather than spectral edge effects. These results reveal a novel, decoherence-induced mechanism for non-exponential spontaneous emission in waveguide QED platforms.

</details>


### [20] [Sub-Planck structure quantification in non-Gaussian probability densities](https://arxiv.org/abs/2601.05898)
*Darren W. Moore,Vojtěch Švarc,Kratveer Singh,Artem Kovalenko,Minh Tuan Pham,Ondřej Číp,Lukáš Slodička,Radim Filip*

Main category: quant-ph

TL;DR: 提出一种识别、量化和比较相空间变量概率密度中sub-Planck结构的通用实验方法，并在单原子机械振荡器的高阶Fock态中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 亚普朗克结构在玻色子量子系统的相空间变量概率密度中普遍存在，特别是在非线性动力学或非线性测量演化中。然而，目前对这些结构的识别和比较仍停留在定性层面，缺乏定量方法。

Method: 提出一种通用且实验友好的方法，能够从可直接测量或估计的单相空间变量概率密度中识别、量化和比较亚普朗克结构。该方法适用于各种实验场景。

Result: 在单原子机械振荡器的高阶Fock态实验中成功验证了该方法，证明随着Fock占据数增加，尽管声子、位置和动量基中的不确定性增加，但亚普朗克结构确实变得更精细。

Conclusion: 该方法为定量研究相空间中的亚普朗克结构提供了实用工具，有助于深入理解玻色子量子系统中的非线性效应和量子特性。

Abstract: Sub-Planck structures in non-Gaussian probability densities of phase space variables are pervasive in bosonic quantum systems. They are almost universally present if the bosonic system evolves via nonlinear dynamics or nonlinear measurements. So far, identification and comparison of such structures remains qualitative. Here we provide a universally applicable and experimentally friendly method to identify, quantify and compare sub-Planck structures from directly measurable or estimated probability densities of single phase space variables. We demonstrate the efficacy of this method on experimental high order Fock states of a single-atom mechanical oscillator, showing provably finer sub-Planck structures as the Fock occupation increases despite the accompanying uncertainty increase in the phonon, position, and momentum bases.

</details>


### [21] [Generation of squeezed optical states via stored classical pulses in a Bose gas](https://arxiv.org/abs/2601.05908)
*Sevilay Sevinçli,Dennis Rätzel,Markus Krutzik,Mehmet Özgür Oktel,Mustafa Gündoğan*

Main category: quant-ph

TL;DR: 利用BEC存储光脉冲并通过原子碰撞非线性演化产生压缩光，可实现数dB的压缩光转移


<details>
  <summary>Details</summary>
Motivation: 传统压缩光产生方法存在限制，需要探索利用原子系统产生压缩光的新方案，特别是结合光学存储和原子相互作用

Method: 采用Λ型光学存储接口将经典探测脉冲映射到BEC的集体自旋波上，利用原子碰撞相互作用实现单轴扭曲动力学，产生自旋压缩，再通过单模分束器映射将原子压缩转移到输出光场

Result: 在考虑实际损耗和有限存储/检索效率的情况下，识别出最佳存储时间，预测在现实条件下可将数dB的压缩转移到检索光中

Conclusion: 提出的方案能够有效利用BEC的非线性动力学产生压缩光，为量子信息处理和精密测量提供了有前景的光源

Abstract: We propose and analyze a scheme to generate squeezed light by storing a classical probe pulse in a Bose--Einstein condensate (BEC) and exploiting the nonlinear evolution caused by atom--atom collisions during the storage time. A $Λ$-type optical memory interface maps a chosen temporal probe mode onto a single phase-matched collective spin wave; for a coherent input this prepares a tunable coherent spin state of a two-component BEC, with its initial spin orientation set by the stored mean excitation number and the phase relation between the probe and control fields. Collisional interactions during storage then implement one-axis-twisting dynamics and generate spin squeezing in the atomic ensemble. We account for realistic loss and finite memory and retrieval efficiencies, and model readout as a single-mode beam-splitter mapping that transfers the atomic quadrature squeezing onto a propagating optical mode. We identify optimal storage times and predict that, under realistic conditions, several dB of squeezing can be transferred to the retrieved light.

</details>


### [22] [Dynamical entanglement percolation with spatially correlated disorder](https://arxiv.org/abs/2601.05925)
*Lorenzo Cirigliano,Valentina Brosco,Claudio Castellano,Simone Felicetti,Laura Pilozzi,Bernard van Heck*

Main category: quant-ph

TL;DR: 该研究通过渗流理论分析量子网络中纠缠的动态传播，发现幺正演化与空间相关无序的相互作用导致非标准渗流现象，比均匀键渗流更丰富且具有滞后性。


<details>
  <summary>Details</summary>
Motivation: 量子网络中纠缠分布在量子信息应用中具有基础性作用，需要理解纠缠如何通过网络动态传播。

Method: 应用渗流理论工具，研究由独立两量子比特相互作用边构成的量子比特网络动力学，通过数值模拟和平均场理论分析双色相关键渗流模型。

Result: 发现幺正演化与空间相关无序的相互作用导致非标准渗流现象，比均匀键渗流显著更丰富，并表现出滞后性。

Conclusion: 双色相关键渗流模型完全阐明了这一现象的物理机制，为理解量子网络中纠缠传播提供了新视角。

Abstract: The distribution of entanglement between the nodes of a quantum network plays a fundamental role in quantum information applications. In this work, we investigate the dynamics of a network of qubits where each edge corresponds to an independent two-qubit interaction. By applying tools from percolation theory, we study how entanglement dynamically spreads across the network. We show that the interplay between unitary evolution and spatially correlated disorder leads to a non-standard percolation phenomenology, significantly richer than uniform bond percolation and featuring hysteresis. A two-colour correlated bond percolation model, whose phase diagram is determined via numerical simulations and a mean-field theory, fully elucidates the physics behind this phenomenon.

</details>


### [23] [Universal Dilation of Linear Itô SDEs: Quantum Trajectories and Lindblad Simulation of Second Moments](https://arxiv.org/abs/2601.05928)
*Hsuan-Cheng Wu,Xiantao Li*

Main category: quant-ph

TL;DR: 提出一个通用框架，用于在量子计算机上模拟N维线性伊藤随机微分方程，通过酉扩张技术建立SDE与随机薛定谔方程的精确对应关系。


<details>
  <summary>Details</summary>
Motivation: 传统上模拟随机微分方程需要大量计算资源，特别是在高维情况下。量子计算机具有并行处理能力，可能为SDE模拟提供更高效的解决方案。

Method: 使用酉扩张技术将线性SDE嵌入到扩张的希尔伯特空间，建立与随机薛定谔方程的精确对应。开发两种算法策略：基于轨迹的弱测量方法和基于系综的确定性Lindblad主方程方法。

Result: 建立了SDE与量子系统之间的路径精确对应，证明了随机薛定谔方程可在数字量子处理器上自然实现，并提供了基于随机光锥分析的误差界限。

Conclusion: 该框架为在量子计算机上模拟线性随机微分方程提供了理论基础和实用算法，展示了量子计算在随机过程模拟中的潜力。

Abstract: We present a universal framework for simulating $N$-dimensional linear Itô stochastic differential equations (SDEs) on quantum computers with additive or multiplicative noises. Building on a unitary dilation technique, we establish a rigorous correspondence between the general linear SDE \[ dX_t = A(t) X_t\,dt + \sum_{j=1}^J B_j(t)X_t\,dW_t^j \] and a Stochastic Schrödinger Equation (SSE) on a dilated Hilbert space. Crucially, this embedding is pathwise exact: the classical solution is recovered as a projection of the dilated quantum state for each fixed noise realization. We demonstrate that the resulting SSE is {naturally implementable} on digital quantum processors, where the stochastic Wiener increments correspond directly to measurement outcomes of ancillary qubits. Exploiting this physical mapping, we develop two algorithmic strategies: (1) a trajectory-based approach that uses sequential weak measurements to realize efficient stochastic integrators, including a second-order scheme, and (2) an ensemble-based approach that maps moment evolution to a deterministic Lindblad quantum master equation, enabling simulation without Monte Carlo sampling. We provide error bounds based on a stochastic light-cone analysis and validate the framework with numerical simulations.

</details>


### [24] [Below-threshold error reduction in single photons through photon distillation](https://arxiv.org/abs/2601.05947)
*F. H. B. Somhorst,J. Saied,N. Kannan,B. Kassenberg,J. Marshall,M. de Goede,H. J. Snijders,P. Stremoukhov,A. Lukianenko,P. Venderbosch,T. B. Demille,A. Roos,N. Walk,J. Eisert,E. G. Rieffel,D. H. Smith,J. J. Renema*

Main category: quant-ph

TL;DR: 本文提出并实验验证了光子蒸馏技术，这是一种比量子纠错更高效的相干误差缓解方法，用于减少光子不可区分性误差，适用于容错量子计算。


<details>
  <summary>Details</summary>
Motivation: 光子量子计算机利用光子的玻色统计特性通过量子干涉构建大规模纠缠态。然而，任何路径信息都会降低量子干涉并引入误差。虽然量子纠错理论上可以处理这些误差，但它资源消耗大、容错阈值低，需要大量高质量光学元件。

Method: 采用可扩展的最优光子蒸馏技术，这是一种固有的玻色相干误差缓解方法。通过量子干涉将单光子投影到纯化的内部状态，从而减少不可区分性误差。实验验证了该方法即使在蒸馏门引入噪声的情况下也能实现无条件误差减少。

Result: 实验观察到与理论预测一致的无条件误差减少（低于阈值行为），即使在考虑蒸馏门引入的噪声后，仍能在容错量子计算相关条件下实现实际净增益误差缓解。

Conclusion: 光子蒸馏比量子纠错具有更高效率和更高阈值，有望应用于大规模量子计算机。这项工作将激发寻找更多固有玻色误差减少策略，即使对于容错架构也是如此。

Abstract: Photonic quantum computers use the bosonic statistics of photons to construct, through quantum interference, the large entangled states required for measurement-based quantum computation. Therefore, any which-way information present in the photons will degrade quantum interference and introduce errors. While quantum error correction can address such errors in principle, it is highly resource-intensive and operates with a low error threshold, requiring numerous high-quality optical components. We experimentally demonstrate scalable, optimal photon distillation as a substantially more resource-efficient strategy to reduce indistinguishability errors in a way that is compatible with fault-tolerant operation. Photon distillation is an intrinsically bosonic, coherent error-mitigation technique which exploits quantum interference to project single photons into purified internal states, thereby reducing indistinguishability errors at both a higher efficiency and higher threshold than quantum error correction. We observe unconditional error reduction (i.e., below-threshold behaviour) consistent with theoretical predictions, even when accounting for noise introduced by the distillation gate, thereby achieving actual net-gain error mitigation under conditions relevant for fault-tolerant quantum computing. We anticipate photon distillation will find uses in large-scale quantum computers. We also expect this work to inspire the search for additional intrinsically bosonic error-reduction strategies, even for fault-tolerant architectures.

</details>


### [25] [Continuous-time noise mitigation in analogue quantum simulation](https://arxiv.org/abs/2601.05952)
*Gabriele Bressanini,Yue Ma,Hyukjoon Kwon,M. S. Kim*

Main category: quant-ph

TL;DR: 提出首个完全模拟的精确噪声消除协议，通过少量辅助量子比特与系统的相互作用，结合经典后处理，实现连续时间量子模拟中的噪声消除。


<details>
  <summary>Details</summary>
Motivation: 模拟量子模拟器在探索量子多体动力学方面具有潜力，但噪声限制了模拟精度。现有方法难以在连续时间模拟中实现精确噪声消除。

Method: 设计完全模拟的噪声消除框架：使用少量辅助量子比特与系统相互作用，结合经典后处理联合测量数据，实现精确噪声消除。协议是哈密顿量无关的，保持连续时间特性。

Result: 建立了首个完全模拟的精确噪声消除协议，能够抵抗实际辅助比特噪声，无需离散化，保持系统动力学的连续时间特性。

Conclusion: 该工作为在噪声环境下实现高保真模拟量子模拟开辟了新方向，解决了连续时间量子模拟中的噪声问题。

Abstract: Analogue quantum simulators offer a promising route to explore quantum many-body dynamics beyond classical reach in the near term. However, their vulnerability to noise limits the accuracy of simulations. Here, we establish a new framework for mitigating noise in analogue quantum simulation, operating in a time-continuous manner. To our knowledge, this is the first protocol that is fully analogue and that achieves exact noise cancellation. Our method requires a small number of ancillary qubits, whose interaction with the system$-$combined with classical post-processing of joint measurement data$-$is tailored to cancel the effect of noise. Furthermore, the protocol is Hamiltonian-independent, robust to realistic ancilla noise, and avoids any discretization, preserving the continuous-time nature of the system's dynamics. This work opens a new direction for achieving high-fidelity analogue quantum simulation in the presence of noise.

</details>


### [26] [Counterdiabatic ADAPT-VQE for molecular simulation](https://arxiv.org/abs/2601.05973)
*Diego Tancara,Herbert Díaz-Moraga,Dardo Goyeneche*

Main category: quant-ph

TL;DR: 提出了一种结合ADAPT-VQE框架与反绝热驱动的混合方法，用于分子模拟，在性能和电路深度方面均优于单独使用两种方法。


<details>
  <summary>Details</summary>
Motivation: ADAPT-VQE在NISQ设备上具有抗贫瘠高原的优势，而反绝热算法相比标准绝热方法在性能和电路深度上都有优势。作者希望结合两者的优点，开发更高效的分子模拟方法。

Method: 将分子哈密顿量映射到量子比特表示，构建绝热哈密顿量，使用嵌套对易子计算近似绝热规范势，得到的算子项定义算子池，然后应用ADAPT-VQE算法迭代选择最相关的元素构建ansatz。

Result: 与单独使用反绝热算法或ADAPT-VQE结合费米子激发算子相比，该方法在性能和电路深度方面都有改善，证明了两种范式结合的有效性。

Conclusion: 结合ADAPT-VQE框架与反绝热驱动的混合方法在分子模拟中表现出优越性能，为NISQ设备上的量子计算提供了更有效的策略。

Abstract: Among variational quantum algorithms designed for NISQ devices, ADAPT-VQE stands out for its robustness against barren plateaus, particularly in estimating molecular ground states. On the other hand, counterdiabatic algorithms have shown advantages in both performance and circuit depth when compared to standard adiabatic approaches. In this work, we propose a hybrid method that integrates the ADAPT-VQE framework with counterdiabatic driving within an adiabatic evolution scheme. Specifically, we map the molecular Hamiltonian to a qubit representation and construct an adiabatic Hamiltonian, from which an approximate adiabatic gauge potential is computed using nested commutators. The resulting operator terms define the operator pool, and the ADAPT-VQE algorithm is applied to iteratively select the most relevant elements for the ansatz. Our results demonstrate improvements in performance and reductions in circuit depth compared to using either counterdiabatic algorithms or ADAPT-VQE with fermionic excitation operators, thus supporting the effectiveness of combining both paradigms in molecular simulations.

</details>


### [27] [From Superradiance to Superabsorption: An Exact Treatment of Non-Markovian Cooperative Radiation](https://arxiv.org/abs/2601.05989)
*Ignacio González,Ángel Rivas*

Main category: quant-ph

TL;DR: 研究非马尔可夫和非平均场近似下，耦合到损耗谐振腔的二能级原子系综中合作辐射现象的出现，揭示了三种不同动力学相：马尔可夫超辐射爆发、非马尔可夫超吸收和临界脉冲发射。


<details>
  <summary>Details</summary>
Motivation: 传统研究多基于马尔可夫和平均场近似，但实际系统中环境记忆效应可能显著影响合作辐射现象。本文旨在超越这些近似，探索非马尔可夫环境下原子系综的集体动力学行为。

Method: 针对双发射体情况推导完整解析解，对更大系综（最多10^3个发射体）采用数值精确方法，表征从马尔可夫到非马尔可夫集体动力学的完整转变。

Result: 发现三种不同相：1）马尔可夫相：标准超辐射爆发；2）非马尔可夫相：自发超吸收发射场；3）临界相：脉冲集体发射。临界谱宽随发射体数量单调增加，表明合作性可增强环境记忆效应。峰值强度的超辐射标度随系统尺寸增加逐渐退化，在完美腔极限下接近亚二次方律。

Conclusion: 自发超吸收是非马尔可夫合作性的独特表现，环境记忆效应与合作性之间存在协同增强关系，这为量子光学和量子信息处理中的集体现象提供了新见解。

Abstract: We investigate the emergence of cooperative radiation phenomena in ensembles of two-level atoms coupled to a lossy resonant cavity beyond the Markovian and mean-field approximations. By deriving a complete analytical solution for the two-emitter case and employing a numerically exact method for larger ensembles, we characterize the full transition from Markovian to non-Markovian collective dynamics for systems of up to $10^3$ emitters. Our results reveal three distinct regimes: a Markovian phase exhibiting the standard superradiant burst, a non-Markovian phase featuring spontaneous superabsorption of the emitted field, and a critical regime marked by pulsed collective emission. We show that the critical spectral width separating these behaviors increases monotonically with the number of emitters, demonstrating that environmental memory effects can be enhanced by cooperativity. Finally, we find that the superradiant scaling of the peak intensity progressively degrades with increasing system size, approaching a subquadratic law in the limit of a perfect cavity. In this regime, spontaneous superabsorption emerges as a distinct manifestation of non-Markovian cooperativity.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [28] [More Exact Thermodynamic Analysis of Topological Black Holes in $R^2$ Gravity](https://arxiv.org/abs/2601.05295)
*Sudhaker Upadhyay,Jyotish Kumar,Dharm Veer Singh,Yerlan Myrzakulov,Kairat Myrzakulov,Abhishek Ashish*

Main category: gr-qc

TL;DR: 研究R²引力中拓扑黑洞的热力学，考虑一阶统计涨落修正，发现小黑洞内能发散、吉布斯自由能升高，热涨落引入双相变，显著影响小黑洞稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究R²引力中拓扑黑洞在热涨落影响下的热力学性质，特别是小尺度黑洞的行为，以理解统计涨落对黑洞热力学的修正效应。

Method: 在R²引力框架下计算拓扑黑洞的热力学量（熵、内能、亥姆霍兹自由能、比热、焓、吉布斯自由能），并考虑一阶统计涨落修正，分析稳定性。

Result: 小黑洞内能因涨落而发散；修正的吉布斯自由能在小视界半径时趋近高值，而平衡吉布斯自由能趋近零；热涨落引入双相变，显著影响小黑洞稳定性。

Conclusion: 热涨落对拓扑黑洞热力学有重要影响，特别是小黑洞，导致内能发散、吉布斯自由能升高，并引入双相变，为理解黑洞在统计涨落下的行为提供新见解。

Abstract: This study investigates the thermodynamics of topological black hole solutions in $R^{2}$ gravity, incorporating the effects of small statistical fluctuations up to first-order corrections. We precisely calculate entropy, internal energy, Helmholtz free energy, specific heat, enthalpy, and Gibbs free energy, accounting for perturbative thermal corrections. Our results reveal that the internal energy of small black holes diverges asymptotically due to these fluctuations. The corrected Gibbs free energy attains asymptotically high values for small horizon radii. In contrast, the equilibrium Gibbs free energy approaches zero. Additionally, we assess the stability of the black hole in the presence of these thermal fluctuations. We find that, in contrast to the equilibrium state, the thermal fluctuation introduces a double phase transition to the stability of the black hole. Our analysis reveals that the influence of fluctuations is notably significant, primarily for small black holes. These findings offer new insights into the thermodynamic properties of topological black holes in the presence of thermal fluctuations.

</details>


### [29] [Condensate Dark Stars beyond the Mean-Field Approximation: The Lee-Huang-Yang correction](https://arxiv.org/abs/2601.05506)
*Grigoris Panotopoulos*

Main category: gr-qc

TL;DR: 研究包含Lee-Huang-Yang修正的稀薄、均匀、超冷玻色气体自引力流体球的结构特性，该修正超越了Hartree平均场近似，考虑了量子涨落。


<details>
  <summary>Details</summary>
Motivation: 研究量子涨落（通过Lee-Huang-Yang修正）对凝聚态暗星结构特性的影响，超越传统的Hartree平均场近似。

Method: 在n=1的多方状态方程中加入Lee-Huang-Yang修正，研究自引力流体球的结构特性，包括质量-半径关系、致密因子和潮汐Love数。

Result: LHY修正对凝聚态暗星的M-R关系和其他特性有显著影响，特别是对于支持更大最高恒星质量的状态方程影响更明显。

Conclusion: 量子涨落通过LHY修正对自引力玻色气体球的结构特性产生重要影响，需要在暗星模型中加以考虑。

Abstract: We study structural properties of self-gravitating fluid spheres made of a dilute, homogeneous and ultracold Bose gas assuming repulsive, short-range interactions. For the first time we include the Lee-Huang-Yang correction to the usual polytropic equation-of-state of index $n=1$, which goes beyond the Hartree mean-field approximation taking into account quantum fluctuations. We find that the correction has a considerable impact on the M-R relationships and other properties of condensate dark stars, such as factor of compactness and tidal Love numbers. The impact is more significant for equation-of-states that support larger highest stellar masses.

</details>


### [30] [Radiation properties and images of loop quantum Reissner-Nordström black hole with a thin accretion disk](https://arxiv.org/abs/2601.05608)
*Qian Li,Jia-Hui Huang*

Main category: gr-qc

TL;DR: 该论文研究了环量子Reissner-Nordström黑洞的圆形测地线特性、薄吸积盘辐射性质及观测特征，利用M87*和Sgr A*观测数据约束量子参数ζ和电荷参数Q。


<details>
  <summary>Details</summary>
Motivation: 研究环量子引力效应对黑洞物理性质的影响，特别是通过观测数据约束环量子引力参数，加深对量子引力理论物理后果的理解。

Method: 计算阴影半径并利用M87*和Sgr A*观测数据约束参数；分析LQRNBH的类时圆形测地线；采用薄吸积盘模型研究辐射特性；使用光线追踪方法数值计算等径向曲线、红移分布和观测辐射通量。

Result: 发现量子参数ζ增加会导致最内稳定圆轨道半径增加（与电荷参数Q相反）；获得了量子参数ζ和电荷参数Q的观测约束；定量比较了LQRNBH与Schwarzschild黑洞的辐射特性；计算了不同参数和观测角度下的直接像和次级像特征。

Conclusion: 研究结果有助于理解环量子引力效应的物理后果，为通过黑洞观测检验量子引力理论提供了具体方法和约束条件。

Abstract: We investigate the characteristics of circular geodesics around loop quantum Reissner-Nordström black hole (LQRNBH) and the radiation properties and observational appearance of a thin accretion disk around it. By calculating the shadow radius and utilizing observational data from M87* and Sgr A*, we derive constraints on the quantum parameter $ζ$ and charge parameter $Q$. The timelike circular geodesics around LQRNBH and the influence of the model parameters on the circular motion are also discussed. It is found that contrary to the case of the parameter $Q$, the increase of the quantum parameter $ζ$ leads to the increase of the radius of the innermost stable circular orbit (ISCO). Then, by considering a thin accretion disk model, various radiation properties of the LQRNBH and the effects of the model parameters on them are studied. Concrete examples are provided for quantitative comparison of the radiation properties between LQRNBH and Schwarzschild black hole. With the ray-tracing method, the isoradial curves, redshift distributions, and the observed radiation fluxes of the direct and secondary images of the LQRNBH with the thin accretion disk for various model parameters and observation angles are numerically calculated and discussed. These results are beneficial for us to understand the physical consequences of loop quantum gravity effect.

</details>


### [31] [Tunnelling in Quantum Cosmology: WKB vs SWKB](https://arxiv.org/abs/2601.05687)
*Duarte Guimarães,João Marto,Paulo Vargas Moniz*

Main category: gr-qc

TL;DR: 比较SWKB与WKB方法在量子宇宙学隧穿问题中的应用，发现SWKB在WKB假设失效时提供更准确的隧穿概率


<details>
  <summary>Details</summary>
Motivation: 研究量子宇宙学中的隧穿问题，比较传统WKB近似与超对称WKB方法的差异，探索Chaplygin气体参数对宇宙从尘埃主导到暗能量主导时期转变的影响

Method: 使用超对称WKB方法分析闭合FRW迷你超空间模型，采用幂级数和Picard近似解析超势，推导闭式SWKB隧穿表达式，计算Chaplygin参数A、B和α相关的传输概率，数值寻找经典转折点并与标准WKB结果比较

Result: 当WKB有效性条件满足时，SWKB与WKB结果一致；当WKB假设失效时，SWKB给出系统更大的隧穿概率，可能更准确

Conclusion: SWKB作为量子宇宙学中势垒传输研究的有用补充方法，在WKB假设不成立时提供更可靠的隧穿概率估计

Abstract: The WKB approximation is a standard tool for studying tunnelling problems in quantum cosmology. We compare this method to the Supersymmetric WKB (SWKB) applied to a closed FRW minisuperspace model. We consider the transition from a dust towards a dark energy-dominated epoch can be explained by a generalized Chaplygin gas. Using analytic approximations for the superpotential (power-series and a Picard approximation), we derive closed-form SWKB tunnelling expressions and compute transmission probabilities as functions of the Chaplygin parameters $A$, $B$ and $α$. Numerical root-finding locates classical turning points and numerical integration allows comparison with standard WKB results. We find that SWKB and WKB agree when the WKB validity condition holds, while the SWKB yields systematically larger (and plausibly more accurate) tunnelling probabilities for parameter values where the WKB assumptions break down. The results support the SWKB as a useful complementary approach for barrier-transmission studies in quantum cosmology.

</details>


### [32] [A nonlinear voice from GW250114 ringdown](https://arxiv.org/abs/2601.05734)
*Yi-Fan Wang,Sizheng Ma,Neev Khera,Huan Yang*

Main category: gr-qc

TL;DR: 首次观测到黑洞合并引力波余辉中的二次准正则模，通过分析GW250114信号检测到6个非线性模式，为黑洞非线性扰动提供了直接证据。


<details>
  <summary>Details</summary>
Motivation: 探测二次准正则模可以直接检验黑洞非线性扰动理论，但之前从未在观测中得到证实。

Method: 使用新型数据分析方法，将旋进-合并推断结果作为高度约束的先验用于余辉推断；利用数值相对论替代波形计算波形并减去相应的非线性模式；允许二次模振幅存在唯象偏差进行测试。

Result: 在GW250114信号中检测到6个来自基本(2,2,0)模及其前两个泛音的二次耦合的非线性模；合并后5个最终质量时，这些非线性模的证据达到贝叶斯因子74；零振幅在3.0σ显著性水平上被排除，理论预期与推断一致。

Conclusion: 这是首次观测到黑洞余辉中的二次准正则模，标志着观测表征黑洞非线性扰动的第一步。

Abstract: The detection of quadratic quasi-normal modes would provide a direct probe into black hole nonlinear perturbations. We report the first observational evidence of a set of quadratic quasi-normal modes in the gravitational-wave ringdown of a binary black hole merger. Analyzing the signal from GW250114, we detect six nonlinear modes from the quadratic coupling of the fundamental $(2,2,0)$ mode and its first two overtones. At 5 final mass ($M_\mathrm{f}$) after the merger, the evidence for these nonlinear modes reaches a Bayes factor of 74. To single out these contributions, we employ recent theoretical progress to compute the waveforms and subtract the corresponding nonlinear modes from a numerical relativity surrogate waveform. Our data analysis uses a novel method that incorporates inspiral-merger inference results as a highly constraining prior for the ringdown inference. We further perform a test allowing for phenomenological deviations for the theoretically predicted amplitudes of the quadratic modes. The results show that an amplitude of zero is excluded at $3.0~σ$ significance level, while the theoretical expectation is consistent with the inference. This detection marks a first step towards observationally characterizing nonlinear perturbations in the ringdown of a black hole.

</details>


### [33] [Parameterized Post-Newtonian Analysis of Quadratic Gravity and Solar System Constraints](https://arxiv.org/abs/2601.05750)
*Jie Zhu,Hao Li*

Main category: gr-qc

TL;DR: 该研究系统分析了广义二次引力在弱场区域的后牛顿行为，推导了1.5PN阶度规表达式，计算了PPN参数γ(r)和β(r)，发现与广义相对论的偏差呈指数衰减，并根据太阳系实验给出了参数约束。


<details>
  <summary>Details</summary>
Motivation: 研究广义二次引力理论（在爱因斯坦-希尔伯特作用量中加入二次曲率项）在弱场区域的后牛顿行为，为未来通过脉冲星计时阵列、引力波观测和实验室短程引力实验测试该理论提供理论基础。

Method: 使用后牛顿展开方法，推导了广义源的度规表达式至1.5PN阶，对于点质量源进一步扩展到2PN阶，并计算了参数化后牛顿参数γ(r)和β(r)。

Result: 发现与广义相对论的偏差呈指数衰减；当m_R=m_W时γ(r)≡1；为保证引力保持吸引力，需要m_W>m_R/4；β(r)的主要修正具有O(r ln(r)e^{-mr})特征；基于太阳系实验得到参数约束：m_R,m_W≳23 AU^{-1}，对应λ≲2.1×10^{19} m^2和μ≲7.1×10^{18} m^2。

Conclusion: 该研究为未来通过多种实验手段测试广义二次引力理论提供了理论基础，表明该理论在弱场区域与广义相对论的偏差很小且呈指数衰减，同时给出了理论参数的具体约束条件。

Abstract: This work systematically investigates the post-Newtonian behavior of general quadratic gravity in the weak-field regime. By extending the Einstein-Hilbert action to include quadratic curvature terms as $\mathcal{L}\propto R-λC^2+μR^2$, the theory introduces two massive modes: a scalar mode and a ghost tensor mode. Using the post-Newtonian expansion method, we derive the explicit expressions for the metric for a general source up to 1.5PN order. Furthermore, for a point-mass source, we extend the solution to 2PN order and evaluate the effective Parameterized Post-Newtonian parameters $γ(r)$ and $β(r)$. The results show that deviations from General Relativity are exponentially suppressed. The theory has the feature $γ(r)\equiv 1$ when $m_R=m_W$, and to ensure that gravity remains attractive, we have $m_W>m_R/4$. The leading correction to $β(r)$ exhibiting a characteristic $\mathcal{O}(r \ln (r)e^{-mr})$ dependence. Based on the Solar System experiments, we derive preliminary constraints on the theory's parameters: $m_R,m_W\gtrsim23~\mathrm{AU}^{-1}$, corresponding to $λ\lesssim2.1\times10^{19}~\mathrm{m}^2$ and $μ\lesssim 7.1\times 10^{18}~\mathrm{m}^2$. This study provides a theoretical foundation for future tests of quadratic gravity using pulsar timing arrays, gravitational-wave observations, and laboratory-scale short-range gravity experiments.

</details>


### [34] [Gravitational waves and small-field astrometry](https://arxiv.org/abs/2601.05754)
*Robin Geyer,Sven Zschocke,Michael Soffel,Sergei Klioner,Lennart Lindegren,Uwe Lammers*

Main category: gr-qc

TL;DR: 小视场天体测量任务在探测引力波方面存在显著局限性，因为小视场只能测量微小差分效应，这些效应在标准星图校准过程中几乎完全被吸收。


<details>
  <summary>Details</summary>
Motivation: 天体测量观测原则上可用于探测引力波，但需要具体分析小视场天体测量数据中可预期的引力波效应，特别是有限视场内源对之间的差分效应。

Method: 提供了平面引力波对天体测量效应的详细理论推导，重点分析小视场数据中的引力波效应，特别是源对之间的差分效应，并进行数值模拟验证理论发现。

Result: 小视场任务具有显著的不利特性，主要因为相对较小的视场只能测量微小的差分效应，这些效应预计会被标准星图校准过程几乎完全吸收。

Conclusion: 小视场天体测量任务在探测引力波方面存在根本性限制，需要重新评估这类观测在引力波探测中的实际可行性。

Abstract: Astrometric observations can, in principle, be used to detect gravitational waves. In this paper we give a practical overview of the gravitational wave effects which can be expected specifically in small-field astrometric data. Particular emphasis is placed on the differential effect between pairs of sources within a finite field of view. We also present several general findings that are not restricted to the small-field case. A detailed theoretical derivation of the general astrometric effect of a plane gravitational wave is provided. Numerical simulations, which underline our theoretical findings, are presented.
  We find that small-field missions suffer from significant detrimental properties, largely because their relatively small fields only allow the measurement of small differential effects which can be expected to be almost totally absorbed by standard plate calibrations.

</details>


### [35] [On Coordinate Singularities Induced by Trapping Horizons](https://arxiv.org/abs/2601.05902)
*Jinbo Yang,Hongwei Tan,Hyat Huang,Wen-Cong Gan*

Main category: gr-qc

TL;DR: 研究球对称动态时空中由陷获视界引起的坐标奇点，区别于Killing视界相关的坐标奇点，使用Kodama矢量场阐明其几何结构，并以演化Ellis漏孔为例说明。


<details>
  <summary>Details</summary>
Motivation: 陷获视界是追踪黑洞完整演化的关键工具，但会引发一类特殊的坐标奇点，这些奇点与Killing视界相关的坐标奇点不同，需要澄清其几何结构以避免物理假象。

Method: 使用Kodama矢量场分析球对称动态时空中由陷获视界引起的坐标奇点的几何结构，并采用演化Ellis漏孔作为解析模型来具体说明这一现象。

Result: 阐明了由陷获视界引起的坐标奇点的几何结构，避免了物理假象，并通过演化Ellis漏孔模型具体展示了这一现象的关键细节。

Conclusion: 在球对称动态时空中，陷获视界会引发一类特殊的坐标奇点，这些奇点与Killing视界相关的坐标奇点不同，通过Kodama矢量场可以清晰地描述其几何结构，这对于准确理解黑洞演化至关重要。

Abstract: The trapping (or apparent) horizon serves as a key tool for tracing the complete evolution of black holes. We investigate a class of coordinate singularities induced by such trapping (or apparent) horizons in a spherically symmetric, dynamic spacetime, which are distinct from the well-known coordinate singularities associated with the Killing horizon. In particular, we clarify the geometric structure of this coordinate singularity by means of the Kodama vector field, thereby avoiding unphysical artifacts. We further employ the evolving Ellis drainhole as an analytical model to illustrate key details of this phenomenon.

</details>


### [36] [Interpolated Topology Change in a Spin Cobordism and the Chiral Weyl Curvature Diagnostic](https://arxiv.org/abs/2601.05957)
*Keith Andrew,Eric V. Steinfelds,Kristopher A. Andrew*

Main category: gr-qc

TL;DR: 提出基于光滑洛伦兹旋量配边的受调控拓扑变化框架，引入手征外尔曲率诊断函数，并用辫子理论描述虫洞喉部动力学。


<details>
  <summary>Details</summary>
Motivation: 洛伦兹量子引力中的拓扑变化需要几何调节器来控制曲率、旋量结构和手征性，特别是在非平凡插值过程中。需要建立既能保持光滑性、洛伦兹特征和旋量兼容性，又能允许瞬态全局双曲性失效的框架。

Method: 1. 基于光滑洛伦兹旋量配边建立受调控拓扑变化框架；2. 引入手征外尔曲率诊断函数，包含外尔曲率不变量和宇称奇偶对偶外尔项；3. 通过Stiefel-Whitney约束确保旋量一致性；4. 用辫子理论描述虫洞喉部动力学，将交叉数替换为基于辫子的复杂度。

Result: 1. 构造了Morris-Thorne虫洞与渐近平坦闵可夫斯基时空之间的光滑旋量配边；2. 在喉部区域显示曲率响应以外尔曲率为主；3. 宇称奇偶外尔项能清晰区分手征扭结嵌入，在非手征构型中消失；4. 辫子为喉部动力学提供了自然语言，基本辫子生成元代表配边的基本拓扑操作。

Conclusion: 该框架为洛伦兹量子引力中的拓扑变化提供了受调控的几何描述，手征外尔曲率诊断函数能加权拓扑变化几何，辫子理论将诊断函数扩展到多喉部和网络化构型，为理解量子引力中的拓扑动力学提供了新工具。

Abstract: Topology change in Lorentzian quantum gravity demands geometric regulators that control curvature, spin structure, and chirality during nontrivial interpolations. We develop a framework for regulated topology change based on smooth Lorentzian spin cobordisms with interpolating metrics, allowing a transient failure of global hyperbolicity while preserving smoothness, Lorentz signature, and spin compatibility. Within this framework we introduce the Chiral Weyl Curvature Diagnostic, a curvature-based functional that weights topology-changing geometries by conformal curvature, spin admissibility, and topological complexity. The diagnostic functional is built from Weyl curvature invariants and includes a parity-odd dual Weyl term that is sensitive to geometric chirality. Spin consistency is enforced via a Stiefel-Whitney constraint, ensuring that only physically admissible cobordisms contribute. As an example, we construct a smooth spin cobordism between a Morris-Thorne wormhole and asymptotically flat Minkowski spacetime. In the throat region the curvature response is shown to be Weyl-dominated, and the parity-odd Weyl contribution sharply distinguishes chiral knotted embeddings while vanishing for amphichiral configurations. We then show that braids provide a natural language of throat dynamics: evolving wormhole throats trace time-dependent braid movies, with elementary braid generators representing the fundamental topological operations of the cobordism. Replacing crossing number by a braid-based complexity refines the diagnostic functional to operate at the level of these elementary exchanges and extends it naturally to multithroat and networked configurations.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [37] [GlueNN: gluing patchwise analytic solutions with neural networks](https://arxiv.org/abs/2601.05889)
*Doyoung Kim,Donghee Lee,Hye-Sung Lee,Jiheon Lee,Jaeok Yi*

Main category: cs.LG

TL;DR: 提出一种学习框架，将渐近解析解的积分常数推广为尺度相关函数，通过约束这些系数函数来学习全局有效解，避免传统匹配方法的缺陷。


<details>
  <summary>Details</summary>
Motivation: 物理和工程中的复杂微分方程常具有强尺度依赖性，传统方法将区域划分为多个补丁并在每个区域简化方程，然后通过匹配界面构造全局解。但这种方法可能失败，因为近似形式在匹配边界附近可能失效。

Method: 提出学习框架，将渐近解析解的积分常数推广为尺度相关函数。通过在整个域上约束这些系数函数满足原始微分方程，网络学习到一个全局有效解，平滑地在渐近区域之间插值，无需任意边界匹配。

Result: 在化学动力学和宇宙学的代表性问题上展示了该框架的有效性，能够准确再现全局解，并优于传统的匹配程序。

Conclusion: 该学习框架通过将渐近解的积分常数推广为尺度相关函数，能够学习全局有效解，避免了传统匹配方法的缺陷，在复杂微分方程求解中表现出优越性能。

Abstract: In many problems in physics and engineering, one encounters complicated differential equations with strongly scale-dependent terms for which exact analytical or numerical solutions are not available. A common strategy is to divide the domain into several regions (patches) and simplify the equation in each region. When approximate analytic solutions can be obtained in each patch, they are then matched at the interfaces to construct a global solution. However, this patching procedure can fail to reproduce the correct solution, since the approximate forms may break down near the matching boundaries. In this work, we propose a learning framework in which the integration constants of asymptotic analytic solutions are promoted to scale-dependent functions. By constraining these coefficient functions with the original differential equation over the domain, the network learns a globally valid solution that smoothly interpolates between asymptotic regimes, eliminating the need for arbitrary boundary matching. We demonstrate the effectiveness of this framework in representative problems from chemical kinetics and cosmology, where it accurately reproduces global solutions and outperforms conventional matching procedures.

</details>


### [38] [MoEBlaze: Breaking the Memory Wall for Efficient MoE Training on Modern GPUs](https://arxiv.org/abs/2601.05296)
*Jiyuan Zhang,Yining Liu,Siqi Yan,Lisen Deng,Jennifer Cao,Shuqi Yang,Min Ni,Bi Xue,Shen Li*

Main category: cs.LG

TL;DR: MoEBlaze：一个内存高效的MoE训练框架，通过协同设计的系统方法解决MoE架构中的内存瓶颈问题，实现4倍加速和50%内存节省。


<details>
  <summary>Details</summary>
Motivation: 现代大规模Mixture-of-Experts（MoE）架构中普遍存在的"内存墙"瓶颈被显著放大。MoE固有的架构稀疏性导致稀疏算术计算，同时引入了大量的激活内存开销——由大型令牌路由缓冲区和需要实例化和缓冲中间张量驱动。这种内存压力限制了GPU上能够容纳的最大批处理大小和序列长度，并导致过多的数据移动，阻碍了性能和高效模型扩展。

Method: MoEBlaze采用协同设计的系统方法：1）端到端令牌调度和MoE训练方法，通过优化的数据结构消除中间缓冲区和激活实例化；2）协同设计的内核与智能激活检查点，在减少内存占用的同时实现更好的性能。

Result: MoEBlaze相比现有MoE框架可以实现超过4倍的加速和超过50%的内存节省。

Conclusion: MoEBlaze通过协同设计的系统方法有效解决了MoE训练中的内存瓶颈问题，显著提升了训练效率和可扩展性。

Abstract: The pervasive "memory wall" bottleneck is significantly amplified in modern large-scale Mixture-of-Experts (MoE) architectures. MoE's inherent architectural sparsity leads to sparse arithmetic compute and also introduces substantial activation memory overheads -- driven by large token routing buffers and the need to materialize and buffer intermediate tensors. This memory pressure limits the maximum batch size and sequence length that can fit on GPUs, and also results in excessive data movements that hinders performance and efficient model scaling. We present MoEBlaze, a memory-efficient MoE training framework that addresses these issues through a co-designed system approach: (i) an end-to-end token dispatch and MoE training method with optimized data structures to eliminate intermediate buffers and activation materializing, and (ii) co-designed kernels with smart activation checkpoint to mitigate memory footprint while simultaneously achieving better performance. We demonstrate that MoEBlaze can achieve over 4x speedups and over 50% memory savings compared to existing MoE frameworks.

</details>


### [39] [TIME: Temporally Intelligent Meta-reasoning Engine for Context Triggered Explicit Reasoning](https://arxiv.org/abs/2601.05300)
*Susmit Das*

Main category: cs.LG

TL;DR: TIME框架通过时间感知的元推理引擎，在对话中引入可选的时间标签和简短思考块，大幅减少推理标记的同时提升时间推理能力


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的显式推理设计存在成本高、可审计性差、无法重新触发推理的问题，且对话模型缺乏对时间结构的感知能力

Method: 引入TIME框架，使用ISO 8601时间标签、tick turns表示静默间隔、短思考块；通过四阶段课程训练Qwen3模型，包括小批量多样化对齐步骤

Result: 在4B到32B规模上，TIME在TIMEBench基准上优于基础Qwen3，同时将推理标记减少约一个数量级

Conclusion: TIME框架通过上下文敏感的推理机制和时间感知设计，有效提升了对话模型的时间推理能力并降低了推理成本

Abstract: Reasoning oriented large language models often expose explicit "thinking" as long, turn-global traces at the start of every response, either always on or toggled externally at inference time. While useful for arithmetic, programming, and problem solving, this design is costly, blurs claim level auditability, and cannot re-trigger explicit reasoning once the model begins presenting. Dialogue models are also largely blind to temporal structure, treating replies after seconds and replies after weeks as equivalent unless time is stated in text. We introduce TIME, the Temporally Intelligent Meta-reasoning Engine, a behavioral alignment framework that treats explicit reasoning as a context sensitive resource driven by discourse and temporal cues. TIME augments dialogue with optional ISO 8601 <time> tags, tick turns that represent silent gaps, and short <think> blocks that can appear anywhere in a reply. A four-phase curriculum including a small, maximally diverse full-batch alignment step trains Qwen3 dense models to invoke brief, in-place reasoning bursts and keep user facing text compact. We evaluate with TIMEBench, a temporally grounded dialogue benchmark probing chronology, commonsense under gaps and offsets, anomaly detection, and continuity. Across 4B to 32B scales, TIME improves TIMEBench scores over base Qwen3 in both thinking and no-thinking modes while reducing reasoning tokens by about an order of magnitude. Our training data and code are available at https://github.com/The-Coherence-Initiative/TIME and TIMEBench is available at https://github.com/The-Coherence-Initiative/TIMEBench

</details>


### [40] [Ontology Neural Networks for Topologically Conditioned Constraint Satisfaction](https://arxiv.org/abs/2601.05304)
*Jaehong Oh*

Main category: cs.LG

TL;DR: 本文提出了一种增强的神经符号推理框架，通过拓扑条件化和梯度稳定机制，在保持语义一致性的同时满足物理和逻辑约束，显著提高了约束满足任务的性能。


<details>
  <summary>Details</summary>
Motivation: 神经符号推理系统在保持语义一致性的同时满足物理和逻辑约束方面面临根本性挑战。需要在基于梯度的优化中融入拓扑结构信息，而不牺牲可解释性或计算效率。

Method: 该方法集成了拓扑条件化与梯度稳定机制：1) 使用Forman-Ricci曲率捕捉图拓扑结构；2) 采用Deep Delta Learning在约束投影期间实现稳定的秩一扰动；3) 使用协方差矩阵自适应进化策略进行参数优化。

Result: 实验评估显示，该方法将平均能量降低至1.15（基线值为11.68），在约束满足任务中达到95%的成功率。框架表现出种子独立的收敛性和良好的扩展性，可处理多达20个节点的问题。

Conclusion: 拓扑结构可以为基于梯度的优化提供信息，同时保持可解释性和计算效率。该框架在神经符号推理中实现了语义一致性与约束满足的平衡，具有优雅的扩展行为。

Abstract: Neuro-symbolic reasoning systems face fundamental challenges in maintaining semantic coherence while satisfying physical and logical constraints. Building upon our previous work on Ontology Neural Networks, we present an enhanced framework that integrates topological conditioning with gradient stabilization mechanisms. The approach employs Forman-Ricci curvature to capture graph topology, Deep Delta Learning for stable rank-one perturbations during constraint projection, and Covariance Matrix Adaptation Evolution Strategy for parameter optimization. Experimental evaluation across multiple problem sizes demonstrates that the method achieves mean energy reduction to 1.15 compared to baseline values of 11.68, with 95 percent success rate in constraint satisfaction tasks. The framework exhibits seed-independent convergence and graceful scaling behavior up to twenty-node problems, suggesting that topological structure can inform gradient-based optimization without sacrificing interpretability or computational efficiency.

</details>


### [41] [When the Server Steps In: Calibrated Updates for Fair Federated Learning](https://arxiv.org/abs/2601.05352)
*Tianrun Yu,Kaixiang Zhao,Cheng Zhang,Anjun Gao,Yueyang Quan,Zhuqing Liu,Minghong Fang*

Main category: cs.LG

TL;DR: EquFL是一种新颖的服务器端去偏方法，通过在联邦学习中生成校准更新来减少系统偏见，无需修改客户端训练协议。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然具有分布式学习的优势，但在确保不同人口群体间的公平性方面面临挑战。现有的公平感知去偏方法要么需要修改客户端训练协议，要么缺乏聚合策略的灵活性。

Method: EquFL是一种服务器端去偏方法，服务器在收到客户端模型更新后生成单个校准更新，然后将这个校准更新与聚合的客户端更新结合，生成减少偏见的调整后全局模型。

Result: 理论上证明EquFL能够收敛到FedAvg达到的最优全局模型，并有效减少训练轮次中的公平性损失。实证研究表明EquFL显著减轻了系统中的偏见。

Conclusion: EquFL提供了一种有效的服务器端去偏解决方案，无需修改客户端协议，在联邦学习中实现了更好的公平性。

Abstract: Federated learning (FL) has emerged as a transformative distributed learning paradigm, enabling multiple clients to collaboratively train a global model under the coordination of a central server without sharing their raw training data. While FL offers notable advantages, it faces critical challenges in ensuring fairness across diverse demographic groups. To address these fairness concerns, various fairness-aware debiasing methods have been proposed. However, many of these approaches either require modifications to clients' training protocols or lack flexibility in their aggregation strategies. In this work, we address these limitations by introducing EquFL, a novel server-side debiasing method designed to mitigate bias in FL systems. EquFL operates by allowing the server to generate a single calibrated update after receiving model updates from the clients. This calibrated update is then integrated with the aggregated client updates to produce an adjusted global model that reduces bias. Theoretically, we establish that EquFL converges to the optimal global model achieved by FedAvg and effectively reduces fairness loss over training rounds. Empirically, we demonstrate that EquFL significantly mitigates bias within the system, showcasing its practical effectiveness.

</details>


### [42] [GlyRAG: Context-Aware Retrieval-Augmented Framework for Blood Glucose Forecasting](https://arxiv.org/abs/2601.05353)
*Shovito Barua Soumma,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: GlyRAG是一个基于检索增强的血糖预测框架，使用LLM从CGM数据中提取临床上下文，通过多模态transformer融合文本和生理特征，结合历史相似案例进行预测，显著提升血糖预测准确性和临床可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前血糖预测模型将CGM数据视为纯数值序列，忽略了临床上下文，或者依赖难以大规模收集的额外传感器。LLM在时间序列预测中展现潜力，但在糖尿病护理中作为上下文提取器的角色尚未充分探索。

Method: GlyRAG框架：1) 使用LLM作为上下文提取代理，从CGM轨迹生成临床摘要；2) 语言模型嵌入摘要并与基于patch的血糖表示在多模态transformer中融合，使用交叉翻译损失对齐文本和生理嵌入；3) 检索模块在学习的嵌入空间中识别相似历史事件，通过交叉注意力整合这些案例类比，最后进行预测推断。

Result: 在两个T1D队列上的评估显示：GlyRAG始终优于最先进方法，RMSE降低达39%，比基线进一步降低1.7%。临床评估表明85%的预测位于安全区域，预测血糖异常事件的性能提升51%。

Conclusion: 基于LLM的上下文提取和CGM轨迹检索可以在不需要额外传感器的情况下，显著提高长期血糖预测的准确性和临床可靠性，为未来糖尿病管理的智能决策支持工具提供支持。

Abstract: Accurate forecasting of blood glucose from CGM is essential for preventing dysglycemic events, thus enabling proactive diabetes management. However, current forecasting models treat blood glucose readings captured using CGMs as a numerical sequence, either ignoring context or relying on additional sensors/modalities that are difficult to collect and deploy at scale. Recently, LLMs have shown promise for time-series forecasting tasks, yet their role as agentic context extractors in diabetes care remains largely unexplored. To address these limitations, we propose GlyRAG, a context-aware, retrieval-augmented forecasting framework that derives semantic understanding of blood glucose dynamics directly from CGM traces without requiring additional sensor modalities. GlyRAG employs an LLM as a contextualization agent to generate clinical summaries. These summaries are embedded by a language model and fused with patch-based glucose representations in a multimodal transformer architecture with a cross translation loss aligining textual and physiological embeddings. A retrieval module then identifies similar historical episodes in the learned embedding space and uses cross-attention to integrate these case-based analogues prior to making a forecasting inference. Extensive evaluations on two T1D cohorts show that GlyRAG consistently outperforms state-of-the art methods, achieving up to 39% lower RMSE and a further 1.7% reduction in RMSE over the baseline. Clinical evaluation shows that GlyRAG places 85% predictions in safe zones and achieves 51% improvement in predicting dysglycemic events across both cohorts. These results indicate that LLM-based contextualization and retrieval over CGM traces can enhance the accuracy and clinical reliability of long-horizon glucose forecasting without the need for extra sensors, thus supporting future agentic decision-support tools for diabetes management.

</details>


### [43] [The Kernel Manifold: A Geometric Approach to Gaussian Process Model Selection](https://arxiv.org/abs/2601.05371)
*Md Shafiqul Islam,Shakti Prasad Padhy,Douglas Allaire,Raymundo Arróyave*

Main category: cs.LG

TL;DR: 提出基于核几何的贝叶斯优化框架，通过多维缩放将离散核库嵌入连续欧几里得流形，实现高效核搜索，在合成数据、时间序列和增材制造应用中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 高斯过程回归的性能严重依赖于协方差核的选择，但核选择是概率建模中最具挑战性和计算成本最高的步骤之一。现有方法难以高效探索核空间。

Method: 基于核几何的贝叶斯优化框架：1) 使用基于期望散度的距离度量GP先验之间的差异；2) 通过多维缩放将离散核库嵌入连续欧几里得流形；3) 将核组合作为输入空间，对数边际似然作为目标函数，MDS坐标作为特征化表示。

Result: 在合成基准测试、真实世界时间序列数据集和增材制造熔池几何预测案例研究中，该方法相比基线（包括LLM引导搜索）实现了更优的预测精度和不确定性校准。

Conclusion: 该框架为核搜索建立了可重用的概率几何结构，对高斯过程建模和深度核学习具有直接相关性，提供了一种高效探索核空间的新方法。

Abstract: Gaussian Process (GP) regression is a powerful nonparametric Bayesian framework, but its performance depends critically on the choice of covariance kernel. Selecting an appropriate kernel is therefore central to model quality, yet remains one of the most challenging and computationally expensive steps in probabilistic modeling. We present a Bayesian optimization framework built on kernel-of-kernels geometry, using expected divergence-based distances between GP priors to explore kernel space efficiently. A multidimensional scaling (MDS) embedding of this distance matrix maps a discrete kernel library into a continuous Euclidean manifold, enabling smooth BO. In this formulation, the input space comprises kernel compositions, the objective is the log marginal likelihood, and featurization is given by the MDS coordinates. When the divergence yields a valid metric, the embedding preserves geometry and produces a stable BO landscape. We demonstrate the approach on synthetic benchmarks, real-world time-series datasets, and an additive manufacturing case study predicting melt-pool geometry, achieving superior predictive accuracy and uncertainty calibration relative to baselines including Large Language Model (LLM)-guided search. This framework establishes a reusable probabilistic geometry for kernel search, with direct relevance to GP modeling and deep kernel learning.

</details>


### [44] [Inverting Non-Injective Functions with Twin Neural Network Regression](https://arxiv.org/abs/2601.05378)
*Sebastian J. Wetzel*

Main category: cs.LG

TL;DR: 提出一种使用孪生神经网络回归和k近邻搜索的确定性框架，用于反演非单射函数，即使输入输出维度不匹配也能找到首选解。


<details>
  <summary>Details</summary>
Motivation: 非单射函数不可逆，但在局部子域上可限制为单射且满射从而实现反演。即使维度不匹配，通常也能从多个可能解中选择一个首选解。需要一种系统方法来反演非单射函数。

Method: 使用孪生神经网络回归结合k近邻搜索。孪生网络学习从已知输入变量x^anchor调整到新输入x^new的预测，对应目标变量从y^anchor到y^new的变化。通过k近邻搜索构建确定性框架来寻找非单射函数的输入参数。

Result: 方法在玩具问题和机器人手臂控制问题上得到验证，这些非单射函数包括：a) 数据定义的函数，b) 已知数学公式的函数。展示了框架能够有效反演非单射函数。

Conclusion: 孪生神经网络回归自然具备处理非单射函数反演的特性，结合k近邻搜索形成确定性框架，能够为给定目标变量找到非单射函数的输入参数，适用于数据定义和公式定义的函数。

Abstract: Non-injective functions are not invertible. However, non-injective functions can be restricted to sub-domains on which they are locally injective and surjective and thus invertible if the dimensionality between input and output spaces are the same. Further, even if the dimensionalities do not match it is often possible to choose a preferred solution from many possible solutions. Twin neural network regression is naturally capable of incorporating these properties to invert non-injective functions. Twin neural network regression is trained to predict adjustments to well known input variables $\mathbf{x}^{\text{anchor}}$ to obtain an estimate for an unknown $\mathbf{x}^{\text{new}}$ under a change of the target variable from $\mathbf{y}^{\text{anchor}}$ to $\mathbf{y}^{\text{new}}$. In combination with k-nearest neighbor search, I propose a deterministic framework that finds input parameters to a given target variable of non-injective functions. The method is demonstrated by inverting non-injective functions describing toy problems and robot arm control that are a) defined by data or b) known as mathematical formula.

</details>


### [45] [Imitation Learning for Combinatorial Optimisation under Uncertainty](https://arxiv.org/abs/2601.05383)
*Prakash Gawas,Antoine Legrain,Louis-Martin Rousseau*

Main category: cs.LG

TL;DR: 本文系统分类了模仿学习中专家构建方法，提出支持多专家查询和交互策略的广义DAgger算法，并在动态医生分配问题上验证了随机专家优于确定性专家，交互学习能减少所需演示数量。


<details>
  <summary>Details</summary>
Motivation: 模仿学习为大规模组合优化问题提供了数据驱动框架，但现有研究对专家构建方法缺乏统一框架来系统分析其建模假设、计算特性和对学习性能的影响。

Method: 提出基于三个维度的专家分类框架：不确定性处理方式（短视、确定性、全信息、两阶段随机、多阶段随机）、最优性水平（任务最优vs近似）、与学习器的交互模式（一次性监督到迭代交互）。基于此提出支持多专家查询、专家聚合和灵活交互策略的广义DAgger算法。

Result: 在动态医生-患者分配问题上的实验表明：从随机专家学习的策略始终优于从确定性或全信息专家学习的策略；交互学习能以更少的专家演示获得更高质量的解；当随机优化计算困难时，聚合的确定性专家是有效替代方案。

Conclusion: 系统分类框架为模仿学习中的专家选择提供了理论基础，广义DAgger算法支持灵活专家交互，实验验证了随机专家和交互学习的优势，为实际应用中的专家构建提供了指导。

Abstract: Imitation learning (IL) provides a data-driven framework for approximating policies for large-scale combinatorial optimisation problems formulated as sequential decision problems (SDPs), where exact solution methods are computationally intractable. A central but underexplored aspect of IL in this context is the role of the \emph{expert} that generates training demonstrations. Existing studies employ a wide range of expert constructions, yet lack a unifying framework to characterise their modelling assumptions, computational properties, and impact on learning performance.
  This paper introduces a systematic taxonomy of experts for IL in combinatorial optimisation under uncertainty. Experts are classified along three dimensions: (i) their treatment of uncertainty, including myopic, deterministic, full-information, two-stage stochastic, and multi-stage stochastic formulations; (ii) their level of optimality, distinguishing task-optimal and approximate experts; and (iii) their interaction mode with the learner, ranging from one-shot supervision to iterative, interactive schemes. Building on this taxonomy, we propose a generalised Dataset Aggregation (DAgger) algorithm that supports multiple expert queries, expert aggregation, and flexible interaction strategies.
  The proposed framework is evaluated on a dynamic physician-to-patient assignment problem with stochastic arrivals and capacity constraints. Computational experiments compare learning outcomes across expert types and interaction regimes. The results show that policies learned from stochastic experts consistently outperform those learned from deterministic or full-information experts, while interactive learning improves solution quality using fewer expert demonstrations. Aggregated deterministic experts provide an effective alternative when stochastic optimisation becomes computationally challenging.

</details>


### [46] [DynaSTy: A Framework for SpatioTemporal Node Attribute Prediction in Dynamic Graphs](https://arxiv.org/abs/2601.05391)
*Namrata Banerji,Tanya Berger-Wolf*

Main category: cs.LG

TL;DR: 提出一种动态图上的端到端时空模型，通过可适应的注意力偏置处理动态邻接矩阵，实现多步节点属性预测，适用于多系统场景。


<details>
  <summary>Details</summary>
Motivation: 现有时空图神经网络通常假设静态邻接矩阵，无法有效处理动态图上的多步节点属性预测问题，这在金融信任网络、生物网络等应用中至关重要。

Method: 基于Transformer的端到端模型，将动态邻接矩阵作为可适应的注意力偏置注入；采用掩码节点-时间预训练目标重构缺失特征；使用计划采样和水平加权损失缓解长期预测的累积误差。

Result: 在均方根误差（RMSE）和平均绝对误差（MAE）指标上持续优于强基线方法。

Conclusion: 提出的动态边偏置时空模型能够有效处理跨样本变化的动态图，适用于多系统设置（如不同受试者的大脑网络、不同背景的金融系统等），在动态图上的多步节点属性预测任务中表现优异。

Abstract: Accurate multistep forecasting of node-level attributes on dynamic graphs is critical for applications ranging from financial trust networks to biological networks. Existing spatiotemporal graph neural networks typically assume a static adjacency matrix. In this work, we propose an end-to-end dynamic edge-biased spatiotemporal model that ingests a multi-dimensional timeseries of node attributes and a timeseries of adjacency matrices, to predict multiple future steps of node attributes. At each time step, our transformer-based model injects the given adjacency as an adaptable attention bias, allowing the model to focus on relevant neighbors as the graph evolves. We further deploy a masked node-time pretraining objective that primes the encoder to reconstruct missing features, and train with scheduled sampling and a horizon-weighted loss to mitigate compounding error over long horizons. Unlike prior work, our model accommodates dynamic graphs that vary across input samples, enabling forecasting in multi-system settings such as brain networks across different subjects, financial systems in different contexts, or evolving social systems. Empirical results demonstrate that our method consistently outperforms strong baselines on Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE).

</details>


### [47] [Interactive Distillation for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.05407)
*Minwoo Cho,Batuhan Altundas,Matthew Gombolay*

Main category: cs.LG

TL;DR: HINT是一个用于多智能体强化学习的知识蒸馏框架，通过分层交互式教师机制解决传统方法在复杂领域、OOD状态和观察空间不匹配方面的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法在多智能体强化学习中面临三个主要瓶颈：1) 在复杂领域难以合成高性能教学策略；2) 教师需要在分布外状态进行推理；3) 分散式学生与集中式教师的观察空间不匹配。

Method: 提出HINT框架：1) 利用分层强化学习构建可扩展的高性能教师；2) 提出伪离策略强化学习，使教师策略能够同时使用教师和学生经验进行更新，提高OOD适应能力；3) 应用基于性能的过滤机制，仅保留与结果相关的指导，减少观察空间不匹配。

Result: 在FireCommander（资源分配）和MARINE（战术战斗）等具有挑战性的合作领域进行评估，HINT显著优于基线方法，成功率提高了60%到165%。

Conclusion: HINT通过分层交互式教师机制有效解决了多智能体强化学习中知识蒸馏的关键瓶颈，在复杂合作任务中取得了显著性能提升，为加速多智能体学习提供了有效解决方案。

Abstract: Knowledge distillation (KD) has the potential to accelerate MARL by employing a centralized teacher for decentralized students but faces key bottlenecks. Specifically, there are (1) challenges in synthesizing high-performing teaching policies in complex domains, (2) difficulties when teachers must reason in out-of-distribution (OOD) states, and (3) mismatches between the decentralized students' and the centralized teacher's observation spaces. To address these limitations, we propose HINT (Hierarchical INteractive Teacher-based transfer), a novel KD framework for MARL in a centralized training, decentralized execution setup. By leveraging hierarchical RL, HINT provides a scalable, high-performing teacher. Our key innovation, pseudo off-policy RL, enables the teacher policy to be updated using both teacher and student experience, thereby improving OOD adaptation. HINT also applies performance-based filtering to retain only outcome-relevant guidance, reducing observation mismatches. We evaluate HINT on challenging cooperative domains (e.g., FireCommander for resource allocation, MARINE for tactical combat). Across these benchmarks, HINT outperforms baselines, achieving improvements of 60% to 165% in success rate.

</details>


### [48] [Efficient Inference for Noisy LLM-as-a-Judge Evaluation](https://arxiv.org/abs/2601.05420)
*Yiqun T Chen,Sizhu Lu,Sijia Li,Moran Guo,Shengyi Li*

Main category: cs.LG

TL;DR: 该论文系统研究了两种纠正LLM作为评判者偏差的方法：基于测量误差校正的Rogan-Gladen式估计器和基于预测残差校准的预测驱动推断(PPI)，通过半参数效率理论统一了这两类估计器，并分析了PPI在何种条件下具有更小的渐近方差。


<details>
  <summary>Details</summary>
Motivation: LLM作为评判者存在系统性非随机误差，现有两种主要纠偏方法（测量误差校正和预测驱动推断）需要系统性的理论分析和性能比较，以指导实际应用中的方法选择。

Method: 利用半参数效率理论，推导出基于高效影响函数(EIF)的高效估计器，统一了测量误差校正和PPI两类方法，通过理论分析比较它们的渐近方差，并在模拟和真实数据中进行验证。

Result: 理论分析表明，在某些条件下PPI式估计器比测量误差校正方法具有更小的渐近方差，模拟和真实数据实验验证了这一理论结果。

Conclusion: 该研究为LLM作为评判者的偏差校正提供了统一的理论框架，明确了不同方法的适用条件，有助于在实际应用中更有效地使用小规模人工标注来纠正LLM评判的偏差。

Abstract: Large language models (LLMs) are increasingly used as automatic evaluators of generative AI outputs, a paradigm often referred to as "LLM-as-a-judge." In practice, LLM judges are imperfect predictions for the underlying truth and can exhibit systematic, non-random errors. Two main approaches have recently been proposed to address this issue: (i) direct measurementerror correction based on misclassification models such as Rogan-Gladen-style estimators, and (ii) surrogate-outcome approaches such as prediction-powered inference (PPI), which correct bias by calibrating prediction residuals on a small set of gold-standard human labels. In this paper, we systematically study the performance of these two approaches for estimating mean parameters (e.g., average benchmark scores or pairwise win rates). Leveraging tools from semiparametric efficiency theory, we unify the two classes of estimators by deriving explicit forms of efficient influence function (EIF)-based efficient estimators and characterize conditions under which PPI-style estimators attain strictly smaller asymptotic variance than measurement-error corrections. We verify our theoretical results in simulations and demonstrate the methods on real-data examples. We provide an implementation of the benchmarked methods and comparison utilities at https://github.com/yiqunchen/debias-llm-as-a-judge.

</details>


### [49] [Prediction of Fault Slip Tendency in CO${_2}$ Storage using Data-space Inversion](https://arxiv.org/abs/2601.05431)
*Xiaowen He,Su Jiang,Louis J. Durlofsky*

Main category: cs.LG

TL;DR: 基于变分自编码器的数据空间反演框架，用于预测CO₂封存项目中的压力、应力、应变场及断层滑移倾向，无需生成后验地质模型。


<details>
  <summary>Details</summary>
Motivation: 在CO₂封存等地下作业中，准确评估断层滑移潜力至关重要。传统的基于模型的历史匹配方法在处理含断层的流固耦合问题时面临挑战，需要生成校准到观测数据的后验地质模型，计算成本高。

Method: 采用变分自编码器（VAE）的数据空间反演（DSI）框架。首先生成约1000个先验地质模型，通过GEOS软件进行流固耦合模拟。使用堆叠卷积LSTM层的VAE训练，将压力、应变、有效正应力和剪应力场表示为潜变量。利用监测井的压力和应变观测数据，通过DSI进行后验预测。

Result: DSI-VAE框架能够准确预测压力、应变和应力场以及断层滑移倾向。该框架还能减少关键地质力学和断层参数的不确定性。合成真实模型的后验结果验证了方法的准确性。

Conclusion: VAE-DSI框架为含断层系统的流固耦合问题提供了一种高效的数据空间反演方法，避免了传统后验地质模型生成的复杂性，能够准确预测断层滑移倾向并降低参数不确定性。

Abstract: Accurately assessing the potential for fault slip is essential in many subsurface operations. Conventional model-based history matching methods, which entail the generation of posterior geomodels calibrated to observed data, can be challenging to apply in coupled flow-geomechanics problems with faults. In this work, we implement a variational autoencoder (VAE)-based data-space inversion (DSI) framework to predict pressure, stress and strain fields, and fault slip tendency, in CO${_2}$ storage projects. The main computations required by the DSI workflow entail the simulation of O(1000) prior geomodels. The posterior distributions for quantities of interest are then inferred directly from prior simulation results and observed data, without the need to generate posterior geomodels. The model used here involves a synthetic 3D system with two faults. Realizations of heterogeneous permeability and porosity fields are generated using geostatistical software, and uncertain geomechanical and fault parameters are sampled for each realization from prior distributions. Coupled flow-geomechanics simulations for these geomodels are conducted using GEOS. A VAE with stacked convolutional long short-term memory layers is trained, using the prior simulation results, to represent pressure, strain, effective normal stress and shear stress fields in terms of latent variables. The VAE parameterization is used with DSI for posterior predictions, with monitoring wells providing observed pressure and strain data. Posterior results for synthetic true models demonstrate that the DSI-VAE framework gives accurate predictions for pressure, strain, and stress fields and for fault slip tendency. The framework is also shown to reduce uncertainty in key geomechanical and fault parameters.

</details>


### [50] [RingSQL: Generating Synthetic Data with Schema-Independent Templates for Text-to-SQL Reasoning Models](https://arxiv.org/abs/2601.05451)
*Marko Sterbentz,Kevin Cushing,Cameron Barrie,Kristian J. Hammond*

Main category: cs.LG

TL;DR: RingSQL是一个结合查询模板和LLM转述的混合数据生成框架，用于解决文本到SQL训练数据稀缺问题，在六个基准测试中平均提升2.3%的准确率。


<details>
  <summary>Details</summary>
Motivation: 文本到SQL系统的发展受限于高质量训练数据的稀缺性。手动创建数据成本高，现有合成方法在可靠性和可扩展性之间存在权衡：基于模板的方法能确保SQL正确但需要特定模式模板，而基于LLM的生成方法可扩展但缺乏质量和正确性保证。

Method: RingSQL采用混合数据生成框架，结合模式无关的查询模板和基于LLM的自然语言问题转述。这种方法在保持跨不同模式SQL正确性的同时，提供了广泛的语言多样性。

Result: 使用RingSQL生成的数据训练的模型，在六个文本到SQL基准测试中，相比使用其他合成数据训练的模型，平均准确率提升了2.3%。

Conclusion: RingSQL通过结合模板的可靠性和LLM的可扩展性，有效解决了文本到SQL训练数据生成的质量与规模平衡问题，为模型训练提供了高质量合成数据。

Abstract: Recent advances in text-to-SQL systems have been driven by larger models and improved datasets, yet progress is still limited by the scarcity of high-quality training data. Manual data creation is expensive, and existing synthetic methods trade off reliability and scalability. Template-based approaches ensure correct SQL but require schema-specific templates, while LLM-based generation scales easily but lacks quality and correctness guarantees. We introduce RingSQL, a hybrid data generation framework that combines schema-independent query templates with LLM-based paraphrasing of natural language questions. This approach preserves SQL correctness across diverse schemas while providing broad linguistic variety. In our experiments, we find that models trained using data produced by RingSQL achieve an average gain in accuracy of +2.3% across six text-to-SQL benchmarks when compared to models trained on other synthetic data. We make our code available at https://github.com/nu-c3lab/RingSQL.

</details>


### [51] [Efficient Differentiable Causal Discovery via Reliable Super-Structure Learning](https://arxiv.org/abs/2601.05474)
*Pingchuan Ma,Qixin Zhang,Shuai Wang,Dacheng Tao*

Main category: cs.LG

TL;DR: ALVGL是一种增强可微分因果发现的新方法，通过稀疏低秩分解学习数据的精度矩阵，构建包含真实因果图的超结构，从而缩小搜索空间并提高优化效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的可微分因果发现在处理高维数据或存在潜在混杂因素的数据时面临挑战，包括搜索空间过大、目标函数复杂以及图论约束难以处理。虽然利用超结构指导优化过程受到关注，但如何高效学习适当粒度的超结构仍存在困难。

Method: ALVGL采用稀疏和低秩分解来学习数据的精度矩阵，设计了ADMM优化过程来识别与底层因果结构最相关的精度矩阵成分，然后组合这些成分构建一个可证明包含真实因果图的超结构，最后用这个超结构初始化标准的可微分因果发现方法。

Result: 在合成和真实数据集上的广泛实验表明，ALVGL不仅实现了最先进的准确性，还显著提高了优化效率，使其成为可微分因果发现的可靠有效解决方案。

Conclusion: ALVGL是一种通用且有效的可微分因果发现增强方法，能够处理各种结构因果模型（包括高斯和非高斯设置，有或无未测量混杂因素），通过构建适当的超结构显著提高了因果发现的效率和准确性。

Abstract: Recently, differentiable causal discovery has emerged as a promising approach to improve the accuracy and efficiency of existing methods. However, when applied to high-dimensional data or data with latent confounders, these methods, often based on off-the-shelf continuous optimization algorithms, struggle with the vast search space, the complexity of the objective function, and the nontrivial nature of graph-theoretical constraints. As a result, there has been a surge of interest in leveraging super-structures to guide the optimization process. Nonetheless, learning an appropriate super-structure at the right level of granularity, and doing so efficiently across various settings, presents significant challenges.
  In this paper, we propose ALVGL, a novel and general enhancement to the differentiable causal discovery pipeline. ALVGL employs a sparse and low-rank decomposition to learn the precision matrix of the data. We design an ADMM procedure to optimize this decomposition, identifying components in the precision matrix that are most relevant to the underlying causal structure. These components are then combined to construct a super-structure that is provably a superset of the true causal graph. This super-structure is used to initialize a standard differentiable causal discovery method with a more focused search space, thereby improving both optimization efficiency and accuracy.
  We demonstrate the versatility of ALVGL by instantiating it across a range of structural causal models, including both Gaussian and non-Gaussian settings, with and without unmeasured confounders. Extensive experiments on synthetic and real-world datasets show that ALVGL not only achieves state-of-the-art accuracy but also significantly improves optimization efficiency, making it a reliable and effective solution for differentiable causal discovery.

</details>


### [52] [MaxCode: A Max-Reward Reinforcement Learning Framework for Automated Code Optimization](https://arxiv.org/abs/2601.05475)
*Jiefu Ou,Sapana Chaudhary,Kaj Bostrom,Nathaniel Weir,Shuai Zhang,Huzefa Rangwala,George Karypis*

Main category: cs.LG

TL;DR: MaxCode：基于最大奖励强化学习框架的统一代码优化搜索方法，通过执行反馈和自然语言诊断提升LLM代码优化能力


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在通用编码任务中表现出色，但在代码优化方面面临两大挑战：1）编写优化代码（如高性能CUDA内核和竞赛级CPU代码）需要系统、算法和特定语言的专业知识；2）需要解释性能指标（如计时和设备利用率），而不仅仅是二进制正确性

Method: 提出MaxCode框架，将现有搜索方法统一到最大奖励强化学习框架下，使观察和动作价值函数模块化可修改。增强观察空间：集成自然语言批判模型，将原始执行反馈转换为关于错误和性能瓶颈的诊断见解。改进搜索探索：训练生成式奖励到目标模型，使用来自滚动的动作值对潜在解决方案进行重新排序

Result: 在KernelBench（CUDA）和PIE（C++）优化基准测试中，MaxCode相比基线方法提高了优化代码性能，在绝对加速值和相对加速排名方面分别实现了20.3%和10.1%的相对改进

Conclusion: MaxCode通过统一的强化学习框架和增强的观察空间，有效提升了LLM在代码优化任务中的性能，特别是在需要专业知识和性能解释的复杂优化场景中

Abstract: Large Language Models (LLMs) demonstrate strong capabilities in general coding tasks but encounter two key challenges when optimizing code: (i) the complexity of writing optimized code (such as performant CUDA kernels and competition-level CPU code) requires expertise in systems, algorithms and specific languages and (ii) requires interpretation of performance metrics like timing and device utilization beyond binary correctness. In this work, we explore inference-time search algorithms that guide the LLM to discover better solutions through iterative refinement based on execution feedback. Our approach, called MaxCode unifies existing search methods under a max-reward reinforcement learning framework, making the observation and action-value functions modular for modification. To enhance the observation space, we integrate a natural language critique model that converts raw execution feedback into diagnostic insights about errors and performance bottlenecks, and the best-discounted reward seen so far. Together, these provide richer input to the code proposal function. To improve exploration during search, we train a generative reward-to-go model using action values from rollouts to rerank potential solutions. Testing on the KernelBench (CUDA) and PIE (C++) optimization benchmarks shows that MaxCode improves optimized code performance compared to baselines, achieving 20.3% and 10.1% relative improvements in absolute speedup value and relative speedup ranking, respectively.

</details>


### [53] [Hi-ZFO: Hierarchical Zeroth- and First-Order LLM Fine-Tuning via Importance-Guided Tensor Selection](https://arxiv.org/abs/2601.05501)
*Feihu Jin,Ying Tan*

Main category: cs.LG

TL;DR: Hi-ZFO提出了一种分层混合优化框架，结合一阶方法的精确性和零阶方法的探索能力，通过自适应分层更新策略提升LLM微调效果并减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 标准一阶优化方法容易使LLM微调陷入尖锐、泛化能力差的局部最小值，而零阶方法虽然探索性强但收敛慢且方差大。特别是在生成任务中，巨大的输出和搜索空间进一步放大了估计方差，使得零阶方法既嘈杂又低效。

Method: Hi-ZFO（分层零阶和一阶优化）通过层重要性分析自适应地划分模型：对关键层使用精确的一阶梯度更新，对不敏感层使用零阶优化。零阶优化不仅作为内存节省的替代方案，更被有意引入作为"有益随机性"的来源，帮助模型逃离纯一阶优化容易停滞的局部最小值。

Result: 在多种生成、数学和代码推理任务上的验证表明，Hi-ZFO始终实现更优性能，同时显著减少训练时间。

Conclusion: 分层混合优化是LLM微调的有效方法，能够协同一阶优化的精确性和零阶优化的探索能力，提升模型性能并加速训练。

Abstract: Fine-tuning large language models (LLMs) using standard first-order (FO) optimization often drives training toward sharp, poorly generalizing minima. Conversely, zeroth-order (ZO) methods offer stronger exploratory behavior without relying on explicit gradients, yet suffer from slow convergence. More critically, our analysis reveals that in generative tasks, the vast output and search space significantly amplify estimation variance, rendering ZO methods both noisy and inefficient. To address these challenges, we propose \textbf{Hi-ZFO} (\textbf{Hi}erarchical \textbf{Z}eroth- and \textbf{F}irst-\textbf{O}rder optimization), a hybrid framework designed to synergize the precision of FO gradients with the exploratory capability of ZO estimation. Hi-ZFO adaptively partitions the model through layer-wise importance profiling, applying precise FO updates to critical layers while leveraging ZO optimization for less sensitive ones. Notably, ZO in Hi-ZFO is not merely a memory-saving surrogate; it is intentionally introduced as a source of "beneficial stochasticity" to help the model escape the local minima where pure FO optimization tends to stagnate. Validated across diverse generative, mathematical, and code reasoning tasks, Hi-ZFO consistently achieves superior performance while significantly reducing the training time. These results demonstrate the effectiveness of hierarchical hybrid optimization for LLM fine-tuning.

</details>


### [54] [Over-Searching in Search-Augmented Large Language Models](https://arxiv.org/abs/2601.05503)
*Roy Xie,Deepak Gopinath,David Qiu,Dong Lin,Haitian Sun,Saloni Potdar,Bhuwan Dhingra*

Main category: cs.LG

TL;DR: 论文系统评估了搜索增强LLM中的过度搜索问题，发现过度搜索会降低效率并导致幻觉，提出了TPC指标来衡量性能-成本权衡，并发布了OverSearchQA数据集来促进研究。


<details>
  <summary>Details</summary>
Motivation: 搜索增强的大型语言模型在知识密集型任务中表现出色，但存在过度搜索问题——即使搜索不会改善回答质量也频繁调用搜索工具，这导致计算效率低下和因引入无关上下文而产生的幻觉。

Method: 从多个维度系统评估过度搜索：查询类型、模型类别、检索条件和多轮对话；引入Tokens Per Correctness (TPC)指标来量化性能-成本权衡；研究查询和检索层面的缓解方法；发布OverSearchQA数据集。

Result: (1) 搜索通常能提高可回答查询的准确性，但会损害不可回答查询的弃答能力；(2) 过度搜索在复杂推理模型和深度研究系统中更明显，受噪声检索加剧，并在多轮对话中累积；(3) 检索证据的组成很重要，负面证据的存在能改善弃答。

Conclusion: 过度搜索是搜索增强LLM中的一个重要问题，需要更好的性能-成本权衡指标和缓解策略。TPC指标和OverSearchQA数据集为研究高效搜索增强LLM提供了基础。

Abstract: Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.

</details>


### [55] [Toward an Integrated Cross-Urban Accident Prevention System: A Multi-Task Spatial-Temporal Learning Framework for Urban Safety Management](https://arxiv.org/abs/2601.05521)
*Jiayu Fang,Zhiqi Shao,Haoning Xi,Boris Choy,Junbin Gao*

Main category: cs.LG

TL;DR: MLA-STNet：基于Mamba注意力机制的跨城市事故预防系统，通过多任务学习统一处理异质城市数据，在噪声环境下保持稳定性能


<details>
  <summary>Details</summary>
Motivation: 城市事故数据具有异质性、报告不一致、稀疏、周期性和噪声等固有特性，加上碎片化治理和不兼容的报告标准，长期阻碍了跨城市事故预防框架的建立

Method: 提出MLA-STNet统一系统，将事故风险预测建模为跨城市多任务学习问题。包含两个互补模块：STG-MA（时空地理Mamba注意力）抑制不稳定时空波动并增强长程时间依赖；STS-MA（时空语义Mamba注意力）通过共享参数设计缓解跨城市异质性，同时保持个体语义表示空间

Result: 在纽约和芝加哥真实数据集上进行75次实验，在全天和高频事故时段两种预测场景下，相比SOTA基线：RMSE降低6%，Recall提高8%，MAP提高5%，在50%输入噪声下性能变化小于1%

Conclusion: MLA-STNet有效地将异质城市数据集统一到可扩展、鲁棒且可解释的跨城市事故预防系统中，为协调和数据驱动的城市安全管理铺平了道路

Abstract: The development of a cross-city accident prevention system is particularly challenging due to the heterogeneity, inconsistent reporting, and inherently clustered, sparse, cyclical, and noisy nature of urban accident data. These intrinsic data properties, combined with fragmented governance and incompatible reporting standards, have long hindered the creation of an integrated, cross-city accident prevention framework. To address this gap, we propose the Mamba Local-ttention Spatial-Temporal Network MLA-STNet, a unified system that formulates accident risk prediction as a multi-task learning problem across multiple cities. MLA-STNet integrates two complementary modules: (i)the Spatio-Temporal Geographical Mamba-Attention (STG-MA), which suppresses unstable spatio-temporal fluctuations and strengthens long-range temporal dependencies; and (ii) the Spatio-Temporal Semantic Mamba-Attention (STS-MA), which mitigates cross-city heterogeneity through a shared-parameter design that jointly trains all cities while preserving individual semantic representation spaces. We validate the proposed framework through 75 experiments under two forecasting scenarios, full-day and high-frequency accident periods, using real-world datasets from New York City and Chicago. Compared with the state-of-the-art baselines, MLA-STNet achieves up to 6% lower RMSE, 8% higher Recall, and 5% higher MAP, while maintaining less than 1% performance variation under 50% input noise. These results demonstrate that MLA-STNet effectively unifies heterogeneous urban datasets within a scalable, robust, and interpretable Cross-City Accident Prevention System, paving the way for coordinated and data-driven urban safety management.

</details>


### [56] [DeMa: Dual-Path Delay-Aware Mamba for Efficient Multivariate Time Series Analysis](https://arxiv.org/abs/2601.05527)
*Rui An,Haohao Qu,Wenqi Fan,Xuequn Shang,Qing Li*

Main category: cs.LG

TL;DR: DeMa：一种双路径延迟感知Mamba骨干网络，用于高效的多变量时间序列分析，通过分解时间动态和变量交互，在保持线性复杂度的同时提升性能


<details>
  <summary>Details</summary>
Motivation: Transformer在多变量时间序列分析中存在二次计算复杂度和高内存开销问题，而直接应用Mamba模型存在三个关键限制：缺乏显式跨变量建模、难以解耦时间动态与变量交互、对潜在时滞效应建模不足

Method: 提出DeMa双路径延迟感知Mamba骨干网络：1）将MTS分解为时间动态和变量交互；2）时间路径使用Mamba-SSD模块捕获单变量长程动态；3）变量路径使用Mamba-DALA模块集成延迟感知线性注意力建模跨变量依赖

Result: 在五个代表性任务（长短期预测、数据插补、异常检测、序列分类）上实现最先进性能，同时保持卓越的计算效率

Conclusion: DeMa在保持Mamba线性复杂度优势的同时，显著提升了多变量时间序列建模的适用性，为大规模长期MTS分析提供了高效解决方案

Abstract: Accurate and efficient multivariate time series (MTS) analysis is increasingly critical for a wide range of intelligent applications. Within this realm, Transformers have emerged as the predominant architecture due to their strong ability to capture pairwise dependencies. However, Transformer-based models suffer from quadratic computational complexity and high memory overhead, limiting their scalability and practical deployment in long-term and large-scale MTS modeling. Recently, Mamba has emerged as a promising linear-time alternative with high expressiveness. Nevertheless, directly applying vanilla Mamba to MTS remains suboptimal due to three key limitations: (i) the lack of explicit cross-variate modeling, (ii) difficulty in disentangling the entangled intra-series temporal dynamics and inter-series interactions, and (iii) insufficient modeling of latent time-lag interaction effects. These issues constrain its effectiveness across diverse MTS tasks. To address these challenges, we propose DeMa, a dual-path delay-aware Mamba backbone. DeMa preserves Mamba's linear-complexity advantage while substantially improving its suitability for MTS settings. Specifically, DeMa introduces three key innovations: (i) it decomposes the MTS into intra-series temporal dynamics and inter-series interactions; (ii) it develops a temporal path with a Mamba-SSD module to capture long-range dynamics within each individual series, enabling series-independent, parallel computation; and (iii) it designs a variate path with a Mamba-DALA module that integrates delay-aware linear attention to model cross-variate dependencies. Extensive experiments on five representative tasks, long- and short-term forecasting, data imputation, anomaly detection, and series classification, demonstrate that DeMa achieves state-of-the-art performance while delivering remarkable computational efficiency.

</details>


### [57] [Scalable Heterogeneous Graph Learning via Heterogeneous-aware Orthogonal Prototype Experts](https://arxiv.org/abs/2601.05537)
*Wei Zhou,Hong Huang,Ruize Shi,Bang Liu*

Main category: cs.LG

TL;DR: HOPE框架通过原型路由和专家正交化解决异质图神经网络中的线性投影瓶颈问题，提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有异质图神经网络主要改进编码器，但解码/投影阶段仍使用单一共享线性头，这导致线性投影瓶颈：在异质图中，上下文多样性和长尾分布使得全局头无法捕捉细粒度语义，容易过拟合中心节点而忽略尾部节点

Method: 提出HOPE框架，作为标准预测头的即插即用替代方案。使用基于可学习原型的路由机制，通过相似度将实例分配给专家，让专家使用遵循自然长尾分布；同时加入专家正交化来促进多样性并防止专家崩溃

Result: 在四个真实数据集上的实验表明，HOPE在多个最先进的HGNN骨干网络上都能带来一致的性能提升，且计算开销最小

Conclusion: HOPE框架有效解决了异质图神经网络中的线性投影瓶颈问题，通过原型路由和专家正交化机制，能够更好地处理异质图中的上下文多样性和长尾分布，提升模型性能

Abstract: Heterogeneous Graph Neural Networks(HGNNs) have advanced mainly through better encoders, yet their decoding/projection stage still relies on a single shared linear head, assuming it can map rich node embeddings to labels. We call this the Linear Projection Bottleneck: in heterogeneous graphs, contextual diversity and long-tail shifts make a global head miss fine semantics, overfit hub nodes, and underserve tail nodes. While Mixture-of-Experts(MoE) could help, naively applying it clashes with structural imbalance and risks expert collapse. We propose a Heterogeneous-aware Orthogonal Prototype Experts framework named HOPE, a plug-and-play replacement for the standard prediction head. HOPE uses learnable prototype-based routing to assign instances to experts by similarity, letting expert usage follow the natural long-tail distribution, and adds expert orthogonalization to encourage diversity and prevent collapse. Experiments on four real datasets show consistent gains across SOTA HGNN backbones with minimal overhead.

</details>


### [58] [Buffered AUC maximization for scoring systems via mixed-integer optimization](https://arxiv.org/abs/2601.05544)
*Moe Shiina,Shunnosuke Ikeda,Yuichi Takano*

Main category: cs.LG

TL;DR: 提出一种基于混合整数线性优化的评分系统构建方法，直接最大化缓冲AUC来提升分类性能


<details>
  <summary>Details</summary>
Motivation: 现有评分系统构建方法未直接优化AUC这一重要评估指标，需要开发能直接最大化AUC的优化框架

Method: 建立混合整数线性优化(MILO)框架，最大化缓冲AUC(bAUC)作为AUC的最紧凹下界，并加入组稀疏约束控制问题数量

Result: 在公开真实数据集上的实验表明，该方法构建的评分系统AUC值优于基于正则化和逐步回归的基线方法

Conclusion: 该研究推动了混合整数优化技术在开发高可解释性分类模型方面的进步

Abstract: A scoring system is a linear classifier composed of a small number of explanatory variables, each assigned a small integer coefficient. This system is highly interpretable and allows predictions to be made with simple manual calculations without the need for a calculator. Several previous studies have used mixed-integer optimization (MIO) techniques to develop scoring systems for binary classification; however, they have not focused on directly maximizing AUC (i.e., area under the receiver operating characteristic curve), even though AUC is recognized as an essential evaluation metric for scoring systems. Our goal herein is to establish an effective MIO framework for constructing scoring systems that directly maximize the buffered AUC (bAUC) as the tightest concave lower bound on AUC. Our optimization model is formulated as a mixed-integer linear optimization (MILO) problem that maximizes bAUC subject to a group sparsity constraint for limiting the number of questions in the scoring system. Computational experiments using publicly available real-world datasets demonstrate that our MILO method can build scoring systems with superior AUC values compared to the baseline methods based on regularization and stepwise regression. This research contributes to the advancement of MIO techniques for developing highly interpretable classification models.

</details>


### [59] [Learn to Evolve: Self-supervised Neural JKO Operator for Wasserstein Gradient Flow](https://arxiv.org/abs/2601.05583)
*Xue Feng,Li Wang,Deanna Needell,Rongjie Lai*

Main category: cs.LG

TL;DR: 提出自监督学习方法学习JKO解算子，无需数值求解JKO轨迹，通过Learn-to-Evolve算法联合学习算子和轨迹生成，提高泛化能力


<details>
  <summary>Details</summary>
Motivation: JKO方案为计算Wasserstein梯度流提供了稳定变分框架，但实际应用受限于重复求解JKO子问题的高计算成本。需要一种高效方法避免数值求解JKO轨迹。

Method: 提出自监督方法学习JKO解算子，将输入密度直接映射到对应JKO子问题的最小化解。引入Learn-to-Evolve算法，交替进行轨迹生成和算子更新，通过生成数据逼近真实JKO轨迹，同时作为数据增强提升泛化能力。

Result: 数值实验证明该方法在各种能量函数和初始条件下具有准确性、稳定性和鲁棒性。学习到的算子能够高效生成梯度流演化。

Conclusion: 提出的自监督学习框架成功解决了JKO方案计算成本高的问题，Learn-to-Evolve策略有效提升了算子的泛化能力，为Wasserstein梯度流计算提供了高效实用的解决方案。

Abstract: The Jordan-Kinderlehrer-Otto (JKO) scheme provides a stable variational framework for computing Wasserstein gradient flows, but its practical use is often limited by the high computational cost of repeatedly solving the JKO subproblems. We propose a self-supervised approach for learning a JKO solution operator without requiring numerical solutions of any JKO trajectories. The learned operator maps an input density directly to the minimizer of the corresponding JKO subproblem, and can be iteratively applied to efficiently generate the gradient-flow evolution. A key challenge is that only a number of initial densities are typically available for training. To address this, we introduce a Learn-to-Evolve algorithm that jointly learns the JKO operator and its induced trajectories by alternating between trajectory generation and operator updates. As training progresses, the generated data increasingly approximates true JKO trajectories. Meanwhile, this Learn-to-Evolve strategy serves as a natural form of data augmentation, significantly enhancing the generalization ability of the learned operator. Numerical experiments demonstrate the accuracy, stability, and robustness of the proposed method across various choices of energies and initial conditions.

</details>


### [60] [Poisson Hyperplane Processes with Rectified Linear Units](https://arxiv.org/abs/2601.05586)
*Shufei Ge,Shijia Wang,Lloyd Elliott*

Main category: cs.LG

TL;DR: 论文建立了泊松超平面过程与两层ReLU神经网络之间的理论联系，提出了一种基于PHP的神经网络替代表示方法，并通过退火序列蒙特卡洛算法进行贝叶斯推断，实验表明该方法优于经典的两层ReLU神经网络。


<details>
  <summary>Details</summary>
Motivation: 神经网络在各种分类和回归任务中表现出最先进的性能，ReLU通常用作隐藏层的激活函数。本文旨在建立泊松超平面过程与两层ReLU神经网络之间的理论联系，提供一种替代的概率表示方法。

Method: 1. 建立泊松超平面过程与两层ReLU神经网络的数学联系；2. 证明具有高斯先验的PHP是两层ReLU神经网络的替代概率表示；3. 通过分解命题使基于PHP构建的两层神经网络能够扩展到大规模问题；4. 提出退火序列蒙特卡洛算法进行贝叶斯推断。

Result: 数值实验表明，提出的基于PHP的方法在性能上优于经典的两层ReLU神经网络。该方法已实现并开源在GitHub上。

Conclusion: 本文成功建立了泊松超平面过程与两层ReLU神经网络之间的理论联系，提供了一种可扩展的替代表示方法，并通过贝叶斯推断算法实现了性能提升，为神经网络的理论分析和实际应用提供了新视角。

Abstract: Neural networks have shown state-of-the-art performances in various classification and regression tasks. Rectified linear units (ReLU) are often used as activation functions for the hidden layers in a neural network model. In this article, we establish the connection between the Poisson hyperplane processes (PHP) and two-layer ReLU neural networks. We show that the PHP with a Gaussian prior is an alternative probabilistic representation to a two-layer ReLU neural network. In addition, we show that a two-layer neural network constructed by PHP is scalable to large-scale problems via the decomposition propositions. Finally, we propose an annealed sequential Monte Carlo algorithm for Bayesian inference. Our numerical experiments demonstrate that our proposed method outperforms the classic two-layer ReLU neural network. The implementation of our proposed model is available at https://github.com/ShufeiGe/Pois_Relu.git.

</details>


### [61] [PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning](https://arxiv.org/abs/2601.05593)
*Jingcheng Hu,Yinmin Zhang,Shijie Shang,Xiaobo Yang,Yue Peng,Zhewei Huang,Hebin Zhou,Xin Wu,Jie Cheng,Fanqi Wan,Xiangwen Kong,Chengyuan Yao,Kaiwen Yan,Ailin Huang,Hongyu Zhou,Qi Han,Zheng Ge,Daxin Jiang,Xiangyu Zhang,Heung-Yeung Shum*

Main category: cs.LG

TL;DR: PaCoRe是一个训练-推理框架，通过并行协调推理解决语言模型无法在固定上下文窗口内扩展测试时计算的问题，使用消息传递架构进行多轮并行探索，显著提升数学推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型存在核心限制：无法在固定上下文窗口内扩展测试时计算，只能进行顺序推理。这限制了模型通过增加计算资源来提升性能的能力。

Method: 提出并行协调推理框架，采用消息传递架构进行多轮并行探索。每轮启动多个并行推理轨迹，将结果压缩为上下文有界的消息，合成这些消息指导下一轮并最终生成答案。通过大规模基于结果的强化学习进行端到端训练。

Result: 在多个领域取得显著改进，特别是在数学推理方面表现突出：8B参数模型在HMMT 2025上达到94.5%，超过GPT-5的93.2%，有效测试时计算扩展到约200万token而不超过上下文限制。

Conclusion: PaCoRe成功解决了语言模型测试时计算扩展问题，通过并行协调推理实现了远超上下文限制的有效计算，在数学推理等任务上超越了前沿系统，开源了模型检查点、训练数据和完整推理管道以促进后续研究。

Abstract: We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.

</details>


### [62] [Good Allocations from Bad Estimates](https://arxiv.org/abs/2601.05597)
*Sílvia Casacuberta,Moritz Hardt*

Main category: cs.LG

TL;DR: 本文提出一种用于治疗分配的算法，相比传统的条件平均治疗效果（CATE）估计方法，样本复杂度从O(M/ε²)降低到O(M/ε)，通过粗粒度估计实现近最优的治疗分配。


<details>
  <summary>Details</summary>
Motivation: 传统CATE估计方法需要大量样本来准确估计治疗效果，但治疗分配的目标是找到最优的治疗分配策略，而非精确估计所有治疗效果。本文旨在探索治疗分配所需的样本复杂度是否低于治疗效果估计。

Method: 提出一种新算法，利用治疗效果的天然分布特性，通过粗粒度估计而非精确估计来实现近最优的治疗分配。算法还利用预算灵活性进一步降低样本复杂度。

Result: 在多个真实世界随机对照试验数据集上的评估表明，该算法能够用极少的样本找到近最优的治疗分配方案。样本复杂度从O(M/ε²)降低到O(M/ε)。

Conclusion: 治疗效果估计和治疗分配之间存在根本区别：后者所需的样本数量远少于前者。粗粒度估计足以实现近最优的治疗分配，这为资源受限的实际应用提供了重要启示。

Abstract: Conditional average treatment effect (CATE) estimation is the de facto gold standard for targeting a treatment to a heterogeneous population. The method estimates treatment effects up to an error $ε> 0$ in each of $M$ different strata of the population, targeting individuals in decreasing order of estimated treatment effect until the budget runs out. In general, this method requires $O(M/ε^2)$ samples. This is best possible if the goal is to estimate all treatment effects up to an $ε$ error. In this work, we show how to achieve the same total treatment effect as CATE with only $O(M/ε)$ samples for natural distributions of treatment effects. The key insight is that coarse estimates suffice for near-optimal treatment allocations. In addition, we show that budget flexibility can further reduce the sample complexity of allocation. Finally, we evaluate our algorithm on various real-world RCT datasets. In all cases, it finds nearly optimal treatment allocations with surprisingly few samples. Our work highlights the fundamental distinction between treatment effect estimation and treatment allocation: the latter requires far fewer samples.

</details>


### [63] [Orchestrating Tokens and Sequences: Dynamic Hybrid Policy Optimization for RLVR](https://arxiv.org/abs/2601.05607)
*Zijun Min,Bingshuai Liu,Ante Wang,Long Zhang,Anxiang Zeng,Haibo Zhang,Jinsong Su*

Main category: cs.LG

TL;DR: 提出DHPO方法，结合GRPO的token级重要性比率和GSPO的序列级重要性比率，通过动态混合机制和分支特定裁剪策略，在数学推理任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法在粒度上各有优缺点：GRPO使用token级重要性比率能保留细粒度信用分配但方差高不稳定；GSPO使用序列级重要性比率匹配序列级奖励但牺牲token级信用分配。需要结合两者优势。

Method: 提出动态混合策略优化(DHPO)，在单一裁剪代理目标中结合token级和序列级重要性比率。探索两种混合机制：平均混合和熵引导混合。采用分支特定裁剪策略，在混合前分别约束两个分支的重要性比率，防止异常值主导更新。

Result: 在七个具有挑战性的数学推理基准测试中，对Qwen3系列的密集模型和MoE模型进行实验，DHPO始终优于GRPO和GSPO。

Conclusion: DHPO成功结合了GRPO和GSPO的优势，通过动态混合机制和分支特定裁剪策略，在数学推理任务上取得了更好的性能，代码将在论文接受后发布。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising framework for optimizing large language models in reasoning tasks. However, existing RLVR algorithms focus on different granularities, and each has complementary strengths and limitations. Group Relative Policy Optimization (GRPO) updates the policy with token-level importance ratios, which preserves fine-grained credit assignment but often suffers from high variance and instability. In contrast, Group Sequence Policy Optimization (GSPO) applies single sequence-level importance ratios across all tokens in a response that better matches sequence-level rewards, but sacrifices token-wise credit assignment. In this paper, we propose Dynamic Hybrid Policy Optimization (DHPO) to bridge GRPO and GSPO within a single clipped surrogate objective. DHPO combines token-level and sequence-level importance ratios using weighting mechanisms. We explore two variants of the mixing mechanism, including an averaged mixing and an entropy-guided mixing. To further stabilize training, we employ a branch-specific clipping strategy that constrains token-level and sequence-level ratios within separate trust regions before mixing, preventing outliers in either branch from dominating the update. Across seven challenging mathematical reasoning benchmarks, experiments on both dense and MoE models from the Qwen3 series show that DHPO consistently outperforms GRPO and GSPO. We will release our code upon acceptance of this paper.

</details>


### [64] [PiXTime: A Model for Federated Time Series Forecasting with Heterogeneous Data Structures Across Nodes](https://arxiv.org/abs/2601.05613)
*Yiming Zhou,Mingyue Cheng,Hao Wang,Enhong Chen*

Main category: cs.LG

TL;DR: PiXTime：一种用于联邦学习的时间序列预测模型，可处理多粒度异构变量，通过个性化补丁嵌入和全局变量嵌入表实现跨节点知识共享


<details>
  <summary>Details</summary>
Motivation: 时间序列数据价值高但难以跨节点共享，不同节点的采样标准导致时间粒度和变量集存在差异，阻碍了传统联邦学习的应用

Method: 1) 个性化补丁嵌入将节点特定粒度时间序列映射到统一维度的token序列；2) 全局变量嵌入表对齐跨节点的变量类别语义；3) 基于Transformer的共享模型处理任意数量变量的辅助序列，使用交叉注意力增强目标序列预测

Result: 在联邦学习设置下达到最先进性能，在八个广泛使用的真实世界传统基准测试中表现出优越性能

Conclusion: PiXTime能够有效处理联邦学习中多粒度和异构变量集的时间序列预测问题，实现跨节点的知识迁移和共享

Abstract: Time series are highly valuable and rarely shareable across nodes, making federated learning a promising paradigm to leverage distributed temporal data. However, different sampling standards lead to diverse time granularities and variable sets across nodes, hindering classical federated learning. We propose PiXTime, a novel time series forecasting model designed for federated learning that enables effective prediction across nodes with multi-granularity and heterogeneous variable sets. PiXTime employs a personalized Patch Embedding to map node-specific granularity time series into token sequences of a unified dimension for processing by a subsequent shared model, and uses a global VE Table to align variable category semantics across nodes, thereby enhancing cross-node transferability. With a transformer-based shared model, PiXTime captures representations of auxiliary series with arbitrary numbers of variables and uses cross-attention to enhance the prediction of the target series. Experiments show PiXTime achieves state-of-the-art performance in federated settings and demonstrates superior performance on eight widely used real-world traditional benchmarks.

</details>


### [65] [Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks](https://arxiv.org/abs/2601.05616)
*ShaoZhen Liu,Xinting Huang,Houwen Peng,Xin Chen,Xinyang Song,Qi Li,Zhenan Sun*

Main category: cs.LG

TL;DR: 提出两阶段训练框架，通过自生成长链思维数据增强模型自我修正能力，在数学推理任务上取得显著提升


<details>
  <summary>Details</summary>
Motivation: 现有研究主要依赖强化学习框架，忽视了监督微调方法在复杂推理任务中的潜力。本文旨在探索如何通过监督微调有效激活大语言模型的内在推理能力。

Method: 两阶段训练框架：第一阶段使用多轮对话策略引导模型生成包含验证、回溯、子目标分解和反向推理的思维链数据，并通过预定义规则筛选高质量样本进行监督微调；第二阶段采用难度感知拒绝采样机制动态优化数据分布，增强模型处理复杂问题的能力。

Result: 在GSM8K和MATH500等数学基准测试上表现提升，生成的推理链长度扩展超过4倍，在AIME24等竞赛级问题上取得显著改进，证明了监督微调能有效激活模型内在推理能力。

Conclusion: 监督微调方法能够有效激活大语言模型的内在推理能力，为复杂任务优化提供了资源高效的途径，具有强可扩展性。

Abstract: In recent years, large language models (LLMs) have demonstrated significant potential in complex reasoning tasks like mathematical problem-solving. However, existing research predominantly relies on reinforcement learning (RL) frameworks while overlooking supervised fine-tuning (SFT) methods. This paper proposes a new two-stage training framework that enhances models' self-correction capabilities through self-generated long chain-of-thought (CoT) data. During the first stage, a multi-turn dialogue strategy guides the model to generate CoT data incorporating verification, backtracking, subgoal decomposition, and backward reasoning, with predefined rules filtering high-quality samples for supervised fine-tuning. The second stage employs a difficulty-aware rejection sampling mechanism to dynamically optimize data distribution, strengthening the model's ability to handle complex problems. The approach generates reasoning chains extended over 4 times longer while maintaining strong scalability, proving that SFT effectively activates models' intrinsic reasoning capabilities and provides a resource-efficient pathway for complex task optimization. Experimental results demonstrate performance improvements on mathematical benchmarks including GSM8K and MATH500, with the fine-tuned model achieving a substantial improvement on competition-level problems like AIME24. Code will be open-sourced.

</details>


### [66] [Continual Learning of Achieving Forgetting-free and Positive Knowledge Transfer](https://arxiv.org/abs/2601.05623)
*Zhi Wang,Zhongbin Wu,Yanni Li,Bing Liu,Guangxi Li,Yuping Wang*

Main category: cs.LG

TL;DR: 本文提出ETCL方法，通过任务特定掩码、梯度对齐和双目标优化，实现无遗忘的正向/反向知识迁移，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习研究主要关注克服灾难性遗忘，但理想的持续学习智能体还应促进正向和反向知识迁移，即利用先前任务知识学习新任务（正向迁移），以及利用新任务知识改进先前任务性能（反向迁移）。

Method: 提出增强任务持续学习（ETCL）方法：1）学习任务特定二进制掩码隔离稀疏子网络；2）新任务开始时，将其梯度与最相似先前任务的子网络梯度对齐以确保正向迁移；3）使用双目标优化策略和正交梯度投影方法，仅更新先前相似任务分类层权重以实现反向迁移。

Result: 在不相似、相似和混合任务序列上的广泛评估表明，ETCL方法显著优于强基线方法。

Conclusion: ETCL方法能够实现无遗忘学习和正知识迁移，通过理论分析估计负迁移边界并提出在线任务相似性检测策略，为持续学习提供了更全面的解决方案。

Abstract: Existing research on continual learning (CL) of a sequence of tasks focuses mainly on dealing with catastrophic forgetting (CF) to balance the learning plasticity of new tasks and the memory stability of old tasks. However, an ideal CL agent should not only be able to overcome CF, but also encourage positive forward and backward knowledge transfer (KT), i.e., using the learned knowledge from previous tasks for the new task learning (namely FKT), and improving the previous tasks' performance with the knowledge of the new task (namely BKT). To this end, this paper first models CL as an optimization problem in which each sequential learning task aims to achieve its optimal performance under the constraint that both FKT and BKT should be positive. It then proposes a novel Enhanced Task Continual Learning (ETCL) method, which achieves forgetting-free and positive KT. Furthermore, the bounds that can lead to negative FKT and BKT are estimated theoretically. Based on the bounds, a new strategy for online task similarity detection is also proposed to facilitate positive KT. To overcome CF, ETCL learns a set of task-specific binary masks to isolate a sparse sub-network for each task while preserving the performance of a dense network for the task. At the beginning of a new task learning, ETCL tries to align the new task's gradient with that of the sub-network of the previous most similar task to ensure positive FKT. By using a new bi-objective optimization strategy and an orthogonal gradient projection method, ETCL updates only the weights of previous similar tasks at the classification layer to achieve positive BKT. Extensive evaluations demonstrate that the proposed ETCL markedly outperforms strong baselines on dissimilar, similar, and mixed task sequences.

</details>


### [67] [Transformer Is Inherently a Causal Learner](https://arxiv.org/abs/2601.05647)
*Xinyue Wang,Stephen Wang,Biwei Huang*

Main category: cs.LG

TL;DR: Transformer模型在自回归训练中自然学习到时延因果结构，其梯度敏感性可直接恢复因果图，无需显式因果目标


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法在非线性、长时依赖和非平稳系统等挑战性场景下表现有限，需要探索基于基础模型的新范式

Method: 通过聚合梯度归因提取因果结构，利用transformer输出对过去输入的梯度敏感性来恢复因果图

Result: 该方法在非线性动态、长时依赖和非平稳系统等挑战性案例中显著超越现有最优算法，且随数据量和异质性增加而提升

Conclusion: 为未来因果发现通过基础模型视角、基础模型通过因果视角获得可解释性和增强的统一范式奠定基础

Abstract: We reveal that transformers trained in an autoregressive manner naturally encode time-delayed causal structures in their learned representations. When predicting future values in multivariate time series, the gradient sensitivities of transformer outputs with respect to past inputs directly recover the underlying causal graph, without any explicit causal objectives or structural constraints. We prove this connection theoretically under standard identifiability conditions and develop a practical extraction method using aggregated gradient attributions. On challenging cases such as nonlinear dynamics, long-term dependencies, and non-stationary systems, this approach greatly surpasses the performance of state-of-the-art discovery algorithms, especially as data heterogeneity increases, exhibiting scaling potential where causal accuracy improves with data volume and heterogeneity, a property traditional methods lack. This unifying view lays the groundwork for a future paradigm where causal discovery operates through the lens of foundation models, and foundation models gain interpretability and enhancement through the lens of causality.

</details>


### [68] [From Global to Local: Cluster-Aware Learning for Wi-Fi Fingerprinting Indoor Localisation](https://arxiv.org/abs/2601.05650)
*Miguel Matey-Sanz,Joaquín Torres-Sospedra,Joaquín Huerta,Sergio Trilles*

Main category: cs.LG

TL;DR: 该论文提出一种基于聚类的Wi-Fi指纹室内定位方法，通过聚类预处理指纹数据集，在定位阶段仅对相关聚类进行定位，从而提高定位精度。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi指纹室内定位面临数据集规模小、异构性强、信号强度变化大、多楼层环境模糊性等问题，这些因素导致全局模型定位精度下降，特别是在不考虑结构约束的情况下。

Method: 提出基于聚类的方法，在定位前对指纹数据集进行结构化处理。使用空间或无线电特征对指纹进行分组，可在建筑或楼层级别应用聚类。定位阶段，基于最强接入点的聚类估计程序将未见指纹分配到最相关聚类，然后仅在选定聚类内进行定位，使学习模型能在更小、更一致的数据子集上操作。

Result: 在三个公共数据集和多个机器学习模型上评估，结果显示定位误差持续减少，特别是在建筑级策略下，但代价是降低了楼层检测精度。结果表明通过聚类显式结构化数据集是有效的可扩展室内定位方法。

Conclusion: 通过聚类显式结构化数据集是一种有效且灵活的室内定位方法，能够提高定位精度，尽管在建筑级策略下会牺牲部分楼层检测准确性。

Abstract: Wi-Fi fingerprinting remains one of the most practical solutions for indoor positioning, however, its performance is often limited by the size and heterogeneity of fingerprint datasets, strong Received Signal Strength Indicator variability, and the ambiguity introduced in large and multi-floor environments. These factors significantly degrade localisation accuracy, particularly when global models are applied without considering structural constraints. This paper introduces a clustering-based method that structures the fingerprint dataset prior to localisation. Fingerprints are grouped using either spatial or radio features, and clustering can be applied at the building or floor level. In the localisation phase, a clustering estimation procedure based on the strongest access points assigns unseen fingerprints to the most relevant cluster. Localisation is then performed only within the selected clusters, allowing learning models to operate on reduced and more coherent subsets of data. The effectiveness of the method is evaluated on three public datasets and several machine learning models. Results show a consistent reduction in localisation errors, particularly under building-level strategies, but at the cost of reducing the floor detection accuracy. These results demonstrate that explicitly structuring datasets through clustering is an effective and flexible approach for scalable indoor positioning.

</details>


### [69] [Do Sparse Autoencoders Identify Reasoning Features in Language Models?](https://arxiv.org/abs/2601.05679)
*George Ma,Zhongyuan Liang,Irene Y. Chen,Somayeh Sojoudi*

Main category: cs.LG

TL;DR: 研究发现稀疏自编码器（SAEs）通过对比激活方法识别出的"推理特征"主要捕捉的是推理的语言相关性而非真正的推理计算过程。


<details>
  <summary>Details</summary>
Motivation: 验证稀疏自编码器（SAEs）是否真正识别出大型语言模型（LLMs）中的推理特征，而不是仅仅捕捉到与推理相关的表面语言模式。

Method: 采用证伪导向框架：1）因果标记注入实验，将特征相关标记注入非推理文本；2）LLM引导的证伪，生成激活特征的非推理输入和不激活特征的推理输入。在20个配置（多模型家族、层、推理数据集）上进行测试。

Result: 1）59%-94%的特征对标记级干预高度敏感，注入少量特征相关标记即可在非推理文本中引发强激活，表明依赖词汇伪影；2）剩余特征通过LLM引导证伪均无法满足真正推理行为的标准；3）操纵这些特征对基准性能影响很小或略有下降。

Conclusion: SAE通过对比方法识别的特征主要捕捉的是推理的语言相关性，而非底层的推理计算本身，这对当前特征解释方法提出了挑战。

Abstract: We investigate whether sparse autoencoders (SAEs) identify genuine reasoning features in large language models (LLMs). Starting from features selected using standard contrastive activation methods, we introduce a falsification-oriented framework that combines causal token injection experiments and LLM-guided falsification to test whether feature activation reflects reasoning processes or superficial linguistic correlates. Across 20 configurations spanning multiple model families, layers, and reasoning datasets, we find that identified reasoning features are highly sensitive to token-level interventions. Injecting a small number of feature-associated tokens into non-reasoning text is sufficient to elicit strong activation for 59% to 94% of features, indicating reliance on lexical artifacts. For the remaining features that are not explained by simple token triggers, LLM-guided falsification consistently produces non-reasoning inputs that activate the feature and reasoning inputs that do not, with no analyzed feature satisfying our criteria for genuine reasoning behavior. Steering these features yields minimal changes or slight degradations in benchmark performance. Together, these results suggest that SAE features identified by contrastive approaches primarily capture linguistic correlates of reasoning rather than the underlying reasoning computations themselves.

</details>


### [70] [AGDC: Autoregressive Generation of Variable-Length Sequences with Joint Discrete and Continuous Spaces](https://arxiv.org/abs/2601.05680)
*Yeonsang Shin,Insoo Kim,Bongkeun Kim,Keonwoo Bae,Bohyung Han*

Main category: cs.LG

TL;DR: AGDC提出统一框架联合建模离散和连续值，解决Transformer在生成高精度混合序列时的精度限制问题，特别针对半导体电路设计等需要高精度的领域。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的自回归模型依赖离散化token，无法高精度表示连续值，在半导体电路设计等高精度领域会导致精度损失和功能失效。现有离散化方法在生成混合离散-连续序列时存在可扩展性限制。

Method: AGDC采用混合方法：离散值用分类预测，连续值用扩散模型建模。关键技术包括：1) EOS logit调整机制，使用MLP根据序列上下文动态调整EOS token logits；2) 损失函数中集成长度正则化项。还提出了ContLayNet大规模基准数据集。

Result: 在半导体布局(ContLayNet)、图形布局和SVG上的实验表明，AGDC相比离散化方法和固定模式基线，在生成高保真混合向量表示方面表现更优，实现了跨领域可扩展的高精度生成。

Conclusion: AGDC通过联合建模离散和连续值，解决了Transformer在高精度混合序列生成中的局限性，为半导体设计等高精度领域提供了有效的解决方案，并在多个领域验证了其优越性能。

Abstract: Transformer-based autoregressive models excel in data generation but are inherently constrained by their reliance on discretized tokens, which limits their ability to represent continuous values with high precision. We analyze the scalability limitations of existing discretization-based approaches for generating hybrid discrete-continuous sequences, particularly in high-precision domains such as semiconductor circuit designs, where precision loss can lead to functional failure. To address the challenge, we propose AGDC, a novel unified framework that jointly models discrete and continuous values for variable-length sequences. AGDC employs a hybrid approach that combines categorical prediction for discrete values with diffusion-based modeling for continuous values, incorporating two key technical components: an end-of-sequence (EOS) logit adjustment mechanism that uses an MLP to dynamically adjust EOS token logits based on sequence context, and a length regularization term integrated into the loss function. Additionally, we present ContLayNet, a large-scale benchmark comprising 334K high-precision semiconductor layout samples with specialized evaluation metrics that capture functional correctness where precision errors significantly impact performance. Experiments on semiconductor layouts (ContLayNet), graphic layouts, and SVGs demonstrate AGDC's superior performance in generating high-fidelity hybrid vector representations compared to discretization-based and fixed-schema baselines, achieving scalable high-precision generation across diverse domains.

</details>


### [71] [FLRQ: Faster LLM Quantization with Flexible Low-Rank Matrix Sketching](https://arxiv.org/abs/2601.05684)
*Hongyaoxing Gul,Lijuan Hu,Shuzi Niu,Fangfang Liu*

Main category: cs.LG

TL;DR: FLRQ是一种灵活的低秩量化方法，通过快速识别最优秩并聚合来实现最小存储组合，在量化质量和算法效率上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统低秩PTQ方法需要昂贵的微调来确定不同数据和层的最佳秩，无法充分利用模型潜力，且基于SVD的低秩近似增加了计算开销。

Method: FLRQ包含两个核心组件：1) R1-FLR使用R1-Sketch和高斯投影进行快速低秩近似，实现异常值感知的秩提取；2) BLC通过迭代方法在缩放和裁剪策略下最小化低秩量化误差。

Result: FLRQ在综合实验中表现出强大的有效性和鲁棒性，在量化质量和算法效率方面都达到了最先进的性能。

Conclusion: FLRQ通过灵活的低秩量化方法，解决了传统方法需要昂贵微调和计算开销大的问题，为大规模语言模型的高效量化提供了有效解决方案。

Abstract: Traditional post-training quantization (PTQ) is considered an effective approach to reduce model size and accelerate inference of large-scale language models (LLMs). However, existing low-rank PTQ methods require costly fine-tuning to determine a compromise rank for diverse data and layers in large models, failing to exploit their full potential. Additionally, the current SVD-based low-rank approximation compounds the computational overhead. In this work, we thoroughly analyze the varying effectiveness of low-rank approximation across different layers in representative models. Accordingly, we introduce \underline{F}lexible \underline{L}ow-\underline{R}ank \underline{Q}uantization (FLRQ), a novel solution designed to quickly identify the accuracy-optimal ranks and aggregate them to achieve minimal storage combinations. FLRQ comprises two powerful components, Rank1-Sketch-based Flexible Rank Selection (R1-FLR) and Best Low-rank Approximation under Clipping (BLC). R1-FLR applies the R1-Sketch with Gaussian projection for the fast low-rank approximation, enabling outlier-aware rank extraction for each layer. Meanwhile, BLC aims at minimizing the low-rank quantization error under the scaling and clipping strategy through an iterative method. FLRQ demonstrates strong effectiveness and robustness in comprehensive experiments, achieving state-of-the-art performance in both quantization quality and algorithm efficiency.

</details>


### [72] [mHC-lite: You Don't Need 20 Sinkhorn-Knopp Iterations](https://arxiv.org/abs/2601.05732)
*Yongyi Yang,Jianyang Gao*

Main category: cs.LG

TL;DR: mHC-lite通过将双随机矩阵重构为置换矩阵的凸组合，解决了mHC中Sinkhorn-Knopp归一化的近似误差和工程实现复杂性问题，保证了精确的双随机性，提升了训练稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: Hyper-Connections (HC) 通过动态残差矩阵加速深度神经网络收敛，但无约束的残差矩阵会损害训练稳定性。DeepSeek的mHC使用Sinkhorn-Knopp归一化近似投影到Birkhoff多面体，但存在两个问题：(1) 有限迭代无法保证精确双随机性，近似误差随网络深度累积影响稳定性；(2) SK实现需要专门的CUDA内核，工程门槛高且可移植性差。

Method: 基于Birkhoff-von Neumann定理，提出mHC-lite方法，通过将双随机矩阵显式构造为置换矩阵的凸组合。这种方法通过构造保证精确的双随机性，且仅需原生矩阵操作即可实现，无需专门的CUDA内核。

Result: 大量实验表明，mHC-lite在性能上匹配或超越mHC，同时使用简单实现获得更高的训练吞吐量，并消除了HC和mHC中观察到的残差不稳定性。

Conclusion: mHC-lite通过简单的重参数化方法解决了mHC的近似误差和工程实现问题，保证了精确的双随机性，提高了训练稳定性和效率，同时保持了优异的性能表现。

Abstract: Hyper-Connections (HC) generalizes residual connections by introducing dynamic residual matrices that mix information across multiple residual streams, accelerating convergence in deep neural networks. However, unconstrained residual matrices can compromise training stability. To address this, DeepSeek's Manifold-Constrained Hyper-Connections (mHC) approximately projects these matrices onto the Birkhoff polytope via iterative Sinkhorn--Knopp (SK) normalization. We identify two limitations of this approach: (i) finite SK iterations do not guarantee exact doubly stochasticity, leaving an approximation gap that can accumulate through network depth and undermine stability; (ii) efficient SK implementation requires highly specialized CUDA kernels, raising engineering barriers and reducing portability. Motivated by the Birkhoff--von Neumann theorem, we propose mHC-lite, a simple reparameterization that explicitly constructs doubly stochastic matrices as convex combinations of permutation matrices. This approach guarantees exact doubly stochasticity by construction and can be implemented using only native matrix operations. Extensive experiments demonstrate that mHC-lite matches or exceeds mHC in performance while achieving higher training throughput with a naive implementation and eliminating the residual instabilities observed in both HC and mHC. The code is publicly available at https://github.com/FFTYYY/mhc-lite.

</details>


### [73] [Variational Autoencoders for P-wave Detection on Strong Motion Earthquake Spectrograms](https://arxiv.org/abs/2601.05759)
*Turkan Simge Ispak,Salih Tileylioglu,Erdem Akagunduz*

Main category: cs.LG

TL;DR: 该研究将P波检测重构为自监督异常检测任务，通过492种VAE配置对比发现，注意力机制优于跳跃连接，在0-40公里近源范围达到0.91 AUC，更适合地震预警应用。


<details>
  <summary>Details</summary>
Motivation: 强震记录中的P波检测面临高噪声、标记数据有限和复杂波形特征的挑战，对地震早期预警至关重要。需要探索自监督方法来解决这些限制。

Method: 将P波到达检测重构为自监督异常检测任务，通过492种变分自编码器(VAE)配置的网格搜索，比较跳跃连接和注意力机制等架构变化对重建保真度与异常识别权衡的影响。

Result: 跳跃连接最小化重建误差(MAE约0.0012)但导致"过度泛化"，重建噪声并掩盖检测信号；注意力机制优先全局上下文而非局部细节，获得最高检测性能(AUC 0.875)，在0-40公里近源范围达到0.91 AUC。

Conclusion: 优先全局上下文而非像素级完美重建的架构约束对于稳健的自监督P波检测至关重要，注意力基VAE在近源范围表现优异，非常适合即时早期预警应用。

Abstract: Accurate P-wave detection is critical for earthquake early warning, yet strong-motion records pose challenges due to high noise levels, limited labeled data, and complex waveform characteristics. This study reframes P-wave arrival detection as a self-supervised anomaly detection task to evaluate how architectural variations regulate the trade-off between reconstruction fidelity and anomaly discrimination. Through a comprehensive grid search of 492 Variational Autoencoder configurations, we show that while skip connections minimize reconstruction error (Mean Absolute Error approximately 0.0012), they induce "overgeneralization", allowing the model to reconstruct noise and masking the detection signal. In contrast, attention mechanisms prioritize global context over local detail and yield the highest detection performance with an area-under-the-curve of 0.875. The attention-based Variational Autoencoder achieves an area-under-the-curve of 0.91 in the 0 to 40-kilometer near-source range, demonstrating high suitability for immediate early warning applications. These findings establish that architectural constraints favoring global context over pixel-perfect reconstruction are essential for robust, self-supervised P-wave detection.

</details>


### [74] [Weights to Code: Extracting Interpretable Algorithms from the Discrete Transformer](https://arxiv.org/abs/2601.05770)
*Yifan Zhang,Wei Bi,Kechi Zhang,Dongming Jin,Jie Fu,Zhi Jin*

Main category: cs.LG

TL;DR: 提出Discrete Transformer架构，通过功能解耦和温度退火采样，从连续表示中提取可读程序，实现无演示的算法发现。


<details>
  <summary>Details</summary>
Motivation: Transformer在算法提取中存在特征叠加问题，连续表示与离散符号逻辑之间存在鸿沟，阻碍了可解释程序的提取。

Method: 设计Discrete Transformer，强制功能解耦（数值注意力负责信息路由，数值MLP负责元素级算术），采用温度退火采样，实现从连续表示到离散符号的转换。

Result: 性能与RNN基线相当，但扩展到连续变量域的可解释性；退火过程显示从探索到利用的相变；可通过归纳偏置精细控制合成程序。

Conclusion: Discrete Transformer为无演示算法发现提供了稳健框架，为Transformer可解释性提供了严格途径。

Abstract: Algorithm extraction aims to synthesize executable programs directly from models trained on specific algorithmic tasks, enabling de novo algorithm discovery without relying on human-written code. However, extending this paradigm to Transformer is hindered by superposition, where entangled features encoded in overlapping directions obstruct the extraction of symbolic expressions. In this work, we propose the Discrete Transformer, an architecture explicitly engineered to bridge the gap between continuous representations and discrete symbolic logic. By enforcing a strict functional disentanglement, which constrains Numerical Attention to information routing and Numerical MLP to element-wise arithmetic, and employing temperature-annealed sampling, our method effectively facilitates the extraction of human-readable programs. Empirically, the Discrete Transformer not only achieves performance comparable to RNN-based baselines but crucially extends interpretability to continuous variable domains. Moreover, our analysis of the annealing process shows that the efficient discrete search undergoes a clear phase transition from exploration to exploitation. We further demonstrate that our method enables fine-grained control over synthesized programs by imposing inductive biases. Collectively, these findings establish the Discrete Transformer as a robust framework for demonstration-free algorithm discovery, offering a rigorous pathway toward Transformer interpretability.

</details>


### [75] [Tensor-DTI: Enhancing Biomolecular Interaction Prediction with Contrastive Embedding Learning](https://arxiv.org/abs/2601.05792)
*Manel Gil-Sorribes,Júlia Vilalta-Mor,Isaac Filella-Mercè,Robert Soliva,Álvaro Ciudad,Víctor Guallar,Alexis Molina*

Main category: cs.LG

TL;DR: Tensor-DTI是一个基于对比学习的多模态药物-靶点相互作用预测框架，整合分子图、蛋白质语言模型和结合位点预测，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有DTI预测模型通常依赖单模态预定义分子描述符或序列嵌入，代表性有限，需要更全面的多模态信息整合来提高预测准确性。

Method: 提出Tensor-DTI对比学习框架，采用孪生双编码器架构，整合分子图、蛋白质语言模型和结合位点预测的多模态嵌入，捕捉化学和结构相互作用特征，区分相互作用与非相互作用对。

Result: 在多个DTI基准测试中优于现有序列和基于图的方法；在CDK2的十亿级化学库大规模推理中产生化学合理的命中分布；在严格家族保留分割下，对家族外靶点的高亲和力配体筛选预算有所改善；可扩展到蛋白质-RNA和肽-蛋白质相互作用。

Conclusion: 多模态信息与对比目标的整合能显著提高相互作用预测准确性，为虚拟筛选提供更可解释和可靠性感知的模型。

Abstract: Accurate drug-target interaction (DTI) prediction is essential for computational drug discovery, yet existing models often rely on single-modality predefined molecular descriptors or sequence-based embeddings with limited representativeness. We propose Tensor-DTI, a contrastive learning framework that integrates multimodal embeddings from molecular graphs, protein language models, and binding-site predictions to improve interaction modeling. Tensor-DTI employs a siamese dual-encoder architecture, enabling it to capture both chemical and structural interaction features while distinguishing interacting from non-interacting pairs. Evaluations on multiple DTI benchmarks demonstrate that Tensor-DTI outperforms existing sequence-based and graph-based models. We also conduct large-scale inference experiments on CDK2 across billion-scale chemical libraries, where Tensor-DTI produces chemically plausible hit distributions even when CDK2 is withheld from training. In enrichment studies against Glide docking and Boltz-2 co-folder, Tensor-DTI remains competitive on CDK2 and improves the screening budget required to recover moderate fractions of high-affinity ligands on out-of-family targets under strict family-holdout splits. Additionally, we explore its applicability to protein-RNA and peptide-protein interactions. Our findings highlight the benefits of integrating multimodal information with contrastive objectives to enhance interaction-prediction accuracy and to provide more interpretable and reliability-aware models for virtual screening.

</details>


### [76] [Fusion Matters: Length-Aware Analysis of Positional-Encoding Fusion in Transformers](https://arxiv.org/abs/2601.05807)
*Mohamed Amine Hallam,Kuo-Kun Tseng*

Main category: cs.LG

TL;DR: 研究位置编码与词嵌入的融合机制对Transformer性能的影响，发现融合方式在长序列任务中显著影响性能，而在短序列中影响不大。


<details>
  <summary>Details</summary>
Motivation: 大多数研究关注设计新的位置编码，而忽略了位置信息与词嵌入的融合机制本身可能影响性能，特别是在长序列场景下。

Method: 在相同Transformer架构、数据划分和随机种子下，对比三种经典融合策略：逐元素加法、拼接投影和标量门控融合；在短、中、长序列文本分类数据集上进行实验，并进行配对种子分析和跨数据集比较。

Result: 融合选择对短文本影响可忽略，但在长文档上产生一致的性能提升；可学习融合的益处适用于多种位置编码家族；轻量级卷积门控机制在长文档上引入局部归纳偏置。

Conclusion: 位置编码融合是长序列Transformer的重要设计选择，应被视为明确的建模决策而非固定默认设置。

Abstract: Transformers require positional encodings to represent sequence order, yet most prior work focuses on designing new positional encodings rather than examining how positional information is fused with token embeddings. In this paper, we study whether the fusion mechanism itself affects performance, particularly in long-sequence settings. We conduct a controlled empirical study comparing three canonical fusion strategies--element-wise addition, concatenation with projection, and scalar gated fusion--under identical Transformer architectures, data splits, and random seeds. Experiments on three text classification datasets spanning short (AG News), medium (IMDB), and long (ArXiv) sequences show that fusion choice has negligible impact on short texts but produces consistent gains on long documents. To verify that these gains are structural rather than stochastic, we perform paired-seed analysis and cross-dataset comparison across sequence-length regimes. Additional experiments on the ArXiv dataset indicate that the benefit of learnable fusion generalizes across multiple positional encoding families. Finally, we explore a lightweight convolutional gating mechanism that introduces local inductive bias at the fusion level, evaluated on long documents only. Our results indicate that positional-encoding fusion is a non-trivial design choice for long-sequence Transformers and should be treated as an explicit modeling decision rather than a fixed default.

</details>


### [77] [Learning Reconstructive Embeddings in Reproducing Kernel Hilbert Spaces via the Representer Theorem](https://arxiv.org/abs/2601.05811)
*Enrique Feito-Casares,Francisco M. Melgarejo-Meseguer,José-Luis Rojo-Álvarez*

Main category: cs.LG

TL;DR: 提出基于RKHS的自表示重构流形学习新算法，通过核对齐将高维重构几何传递到低维嵌入空间


<details>
  <summary>Details</summary>
Motivation: 受表示学习方法揭示高维数据潜在结构兴趣增长的驱动，提出在再生核希尔伯特空间中进行重构式流形学习的新算法

Method: 1. 在RKHS中通过优化向量形式的表示定理实现样本自表示重构；2. 使用可分离算子值核扩展到向量值数据；3. 通过核对齐任务将数据投影到低维潜在空间，使其Gram矩阵匹配高维重构核

Result: 在模拟数据集（同心圆、瑞士卷）和真实数据集（癌症分子活动、物联网网络入侵）上的数值实验证明了方法的实际有效性

Conclusion: 该算法通过利用和调整核学习理论的已知结果，扩展了自然数据表现的自表示特性，实现了有效的流形学习

Abstract: Motivated by the growing interest in representation learning approaches that uncover the latent structure of high-dimensional data, this work proposes new algorithms for reconstruction-based manifold learning within Reproducing-Kernel Hilbert Spaces (RKHS). Each observation is first reconstructed as a linear combination of the other samples in the RKHS, by optimizing a vector form of the Representer Theorem for their autorepresentation property. A separable operator-valued kernel extends the formulation to vector-valued data while retaining the simplicity of a single scalar similarity function. A subsequent kernel-alignment task projects the data into a lower-dimensional latent space whose Gram matrix aims to match the high-dimensional reconstruction kernel, thus transferring the auto-reconstruction geometry of the RKHS to the embedding. Therefore, the proposed algorithms represent an extended approach to the autorepresentation property, exhibited by many natural data, by using and adapting well-known results of Kernel Learning Theory. Numerical experiments on both simulated (concentric circles and swiss-roll) and real (cancer molecular activity and IoT network intrusions) datasets provide empirical evidence of the practical effectiveness of the proposed approach.

</details>


### [78] [Detecting Autism Spectrum Disorder with Deep Eye Movement Features](https://arxiv.org/abs/2601.05812)
*Zhanpei Huang,Taochen chen,Fangqing Gu,Yiqun Zhang*

Main category: cs.LG

TL;DR: 提出DSTS框架，利用眼动数据的离散短时序列特性，通过类感知表示和不平衡感知机制，有效区分自闭症谱系障碍与典型发育个体。


<details>
  <summary>Details</summary>
Motivation: 自闭症谱系障碍（ASD）的诊断需要非侵入性工具。眼动数据具有离散性和短时依赖特性，能反映局部注视焦点，但传统Transformer模型的全局注意力机制对此类数据效果有限，需要专门针对眼动数据特性的建模方法。

Method: 设计了离散短时序列（DSTS）建模框架，包含类感知表示机制和不平衡感知机制，专门针对眼动数据的离散性和短时依赖特性进行建模，而非依赖传统的全局注意力机制。

Result: 在多个眼动数据集上的实验表明，DSTS框架优于传统机器学习方法和更复杂的深度学习模型，能有效区分ASD和典型发育个体。

Conclusion: 针对眼动数据的离散短时序列特性设计的DSTS框架，通过专门的类感知和不平衡感知机制，能够有效捕捉ASD相关的微妙眼动模式，为ASD诊断提供了有效的非侵入性工具。

Abstract: Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder characterized by deficits in social communication and behavioral patterns. Eye movement data offers a non-invasive diagnostic tool for ASD detection, as it is inherently discrete and exhibits short-term temporal dependencies, reflecting localized gaze focus between fixation points. These characteristics enable the data to provide deeper insights into subtle behavioral markers, distinguishing ASD-related patterns from typical development. Eye movement signals mainly contain short-term and localized dependencies. However, despite the widespread application of stacked attention layers in Transformer-based models for capturing long-range dependencies, our experimental results indicate that this approach yields only limited benefits when applied to eye movement data. This may be because discrete fixation points and short-term dependencies in gaze focus reduce the utility of global attention mechanisms, making them less efficient than architectures focusing on local temporal patterns. To efficiently capture subtle and complex eye movement patterns, distinguishing ASD from typically developing (TD) individuals, a discrete short-term sequential (DSTS) modeling framework is designed with Class-aware Representation and Imbalance-aware Mechanisms. Through extensive experiments on several eye movement datasets, DSTS outperforms both traditional machine learning techniques and more sophisticated deep learning models.

</details>


### [79] [A Dual Pipeline Machine Learning Framework for Automated Multi Class Sleep Disorder Screening Using Hybrid Resampling and Ensemble Learning](https://arxiv.org/abs/2601.05814)
*Md Sultanul Islam Ovi,Muhsina Tarannum Munfa,Miftahul Alam Adib,Syed Sabbir Hasan*

Main category: cs.LG

TL;DR: 提出双管道机器学习框架用于睡眠障碍筛查，结合统计和包装器方法，在Sleep Health and Lifestyle数据集上达到98.67%准确率，推理延迟低于400毫秒。


<details>
  <summary>Details</summary>
Motivation: 睡眠障碍（特别是失眠和睡眠呼吸暂停）的准确分类对降低长期健康风险和改善患者生活质量很重要，但临床睡眠研究资源密集且难以大规模筛查。

Method: 双管道机器学习框架：统计管道使用互信息和线性判别分析处理线性可分性；包装器管道使用Boruta特征选择和自编码器进行非线性表示学习。采用SMOTETomek混合重采样策略处理类别不平衡。

Result: Extra Trees和K最近邻算法达到98.67%准确率，优于相同数据集上的最新基线。Wilcoxon符号秩检验显示改进显著，推理延迟低于400毫秒。

Conclusion: 提出的双管道设计支持准确高效的自动化睡眠障碍风险分层筛查，为非侵入性大规模筛查提供了可行方案。

Abstract: Accurate classification of sleep disorders, particularly insomnia and sleep apnea, is important for reducing long term health risks and improving patient quality of life. However, clinical sleep studies are resource intensive and are difficult to scale for population level screening. This paper presents a Dual Pipeline Machine Learning Framework for multi class sleep disorder screening using the Sleep Health and Lifestyle dataset. The framework consists of two parallel processing streams: a statistical pipeline that targets linear separability using Mutual Information and Linear Discriminant Analysis, and a wrapper based pipeline that applies Boruta feature selection with an autoencoder for non linear representation learning. To address class imbalance, we use the hybrid SMOTETomek resampling strategy. In experiments, Extra Trees and K Nearest Neighbors achieved an accuracy of 98.67%, outperforming recent baselines on the same dataset. Statistical testing using the Wilcoxon Signed Rank Test indicates that the improvement over baseline configurations is significant, and inference latency remains below 400 milliseconds. These results suggest that the proposed dual pipeline design supports accurate and efficient automated screening for non invasive sleep disorder risk stratification.

</details>


### [80] [A New Family of Poisson Non-negative Matrix Factorization Methods Using the Shifted Log Link](https://arxiv.org/abs/2601.05845)
*Eric Weine,Peter Carbonetto,Rafael A. Irizarry,Matthew Stephens*

Main category: cs.LG

TL;DR: 提出了带移位对数链接函数的泊松非负矩阵分解，通过可调参数从加性组合过渡到乘性组合，改进了传统泊松NMF的加性假设限制。


<details>
  <summary>Details</summary>
Motivation: 传统泊松NMF假设分解的"部分"以加性方式组合，这在某些场景下可能不自然。需要一种更灵活的模型来适应不同数据组合模式。

Method: 引入带移位对数链接函数的泊松NMF，通过单个调优参数控制从加性到乘性组合的过渡。提供最大似然估计算法，以及针对大规模稀疏数据的高效近似计算方法。

Result: 在多个真实数据集上验证了新方法的有效性。结果显示链接函数的选择对分解结果有实质性影响，移位对数链接函数在某些情况下能提高结果的可解释性。

Conclusion: 移位对数链接函数为泊松NMF提供了更灵活的建模框架，能够适应不同数据组合模式，在特定场景下相比标准加性链接能获得更好的可解释性结果。

Abstract: Poisson non-negative matrix factorization (NMF) is a widely used method to find interpretable "parts-based" decompositions of count data. While many variants of Poisson NMF exist, existing methods assume that the "parts" in the decomposition combine additively. This assumption may be natural in some settings, but not in others. Here we introduce Poisson NMF with the shifted-log link function to relax this assumption. The shifted-log link function has a single tuning parameter, and as this parameter varies the model changes from assuming that parts combine additively (i.e., standard Poisson NMF) to assuming that parts combine more multiplicatively. We provide an algorithm to fit this model by maximum likelihood, and also an approximation that substantially reduces computation time for large, sparse datasets (computations scale with the number of non-zero entries in the data matrix). We illustrate these new methods on a variety of real datasets. Our examples show how the choice of link function in Poisson NMF can substantively impact the results, and how in some settings the use of a shifted-log link function may improve interpretability compared with the standard, additive link.

</details>


### [81] [IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck](https://arxiv.org/abs/2601.05870)
*Huilin Deng,Hongchen Luo,Yue Zhu,Long Li,Zhuoyue Chen,Xinghao Zhao,Ming Li,Jihai Zhang,Mengchang Wang,Yang Cao,Yu Kang*

Main category: cs.LG

TL;DR: 提出IIB-LPO方法，通过潜在策略优化和迭代信息瓶颈解决RLVR中的探索崩溃问题，在数学推理基准上取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法面临探索崩溃问题：随机rollout的语义同质性导致模型陷入狭窄、过度优化的行为。全局熵正则化容易导致奖励攻击和冗长输出，而局部token选择性更新难以克服预训练模型的强归纳偏差。

Method: 提出IIB-LPO方法，将探索从token分布的统计扰动转向推理轨迹的拓扑分支。在高熵状态触发潜在分支以多样化推理路径，并利用信息瓶颈原则作为轨迹过滤器和自奖励机制，确保简洁且信息丰富的探索。

Result: 在四个数学推理基准测试中，IIB-LPO实现了最先进的性能，准确率比先前方法提高达5.3%，多样性指标提高达7.4%。

Conclusion: IIB-LPO通过拓扑分支和迭代信息瓶颈有效解决了RLVR中的探索崩溃问题，显著提升了推理任务的性能和多样性。

Abstract: Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.

</details>


### [82] [Auditing Fairness under Model Updates: Fundamental Complexity and Property-Preserving Updates](https://arxiv.org/abs/2601.05909)
*Ayoub Ajarra,Debabrota Basu*

Main category: cs.LG

TL;DR: 提出一个用于机器学习模型在自适应更新下的群体公平性审计框架，通过SP维度量化战略更新的复杂性，并建立样本高效的PAC审计边界。


<details>
  <summary>Details</summary>
Motivation: 现实世界中模型所有者会自适应更新模型，这些更新可能改变模型类别但保持某些审计属性不变，这给可靠审计带来挑战。需要研究在任意更新下的公平性审计问题。

Method: 提出基于经验属性优化（EPO）oracle的通用PAC审计框架。针对统计公平性，引入SP维度这一新的组合度量来刻画允许的战略更新复杂性，并建立分布无关的审计边界。

Result: 建立了群体公平性审计的样本复杂度边界，该边界由SP维度表征。框架可扩展到其他审计目标，如预测误差和鲁棒风险。

Conclusion: 该工作为机器学习模型在自适应更新环境下的可靠审计提供了理论基础和实用框架，通过量化战略更新的复杂性来实现高效的公平性审计。

Abstract: As machine learning models become increasingly embedded in societal infrastructure, auditing them for bias is of growing importance. However, in real-world deployments, auditing is complicated by the fact that model owners may adaptively update their models in response to changing environments, such as financial markets. These updates can alter the underlying model class while preserving certain properties of interest, raising fundamental questions about what can be reliably audited under such shifts.
  In this work, we study group fairness auditing under arbitrary updates. We consider general shifts that modify the pre-audit model class while maintaining invariance of the audited property. Our goals are two-fold: (i) to characterize the information complexity of allowable updates, by identifying which strategic changes preserve the property under audit; and (ii) to efficiently estimate auditing properties, such as group fairness, using a minimal number of labeled samples.
  We propose a generic framework for PAC auditing based on an Empirical Property Optimization (EPO) oracle. For statistical parity, we establish distribution-free auditing bounds characterized by the SP dimension, a novel combinatorial measure that captures the complexity of admissible strategic updates. Finally, we demonstrate that our framework naturally extends to other auditing objectives, including prediction error and robust risk.

</details>


### [83] [Distilling Lightweight Domain Experts from Large ML Models by Identifying Relevant Subspaces](https://arxiv.org/abs/2601.05913)
*Pattarawat Chormai,Ali Hashemi,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: SubDistill是一种新的知识蒸馏算法，专注于只蒸馏教师模型中与特定子任务相关的组件，在计算资源有限的环境中实现更高效的模型压缩。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，通常只有少数类别及其相关中间概念需要蒸馏，但现有蒸馏方法很少明确关注相关子任务。需要一种方法能够只蒸馏教师模型中与特定子任务相关的组件，而不是整个模型。

Method: SubDistill算法改进了数值特性，在每一层只蒸馏教师模型的相关组件。该方法专注于相关子任务，而不是对整个模型进行蒸馏。

Result: 在CIFAR-100和ImageNet数据集上使用卷积和Transformer模型的实验表明，SubDistill在代表性子任务集上优于现有的逐层蒸馏技术。可解释AI分析显示，蒸馏后的学生模型更接近原始教师模型的决策结构。

Conclusion: SubDistill提供了一种有效的知识蒸馏方法，专注于相关子任务，在计算资源受限的环境中实现了更好的性能，并且学生模型能够更好地保持教师模型的决策特性。

Abstract: Knowledge distillation involves transferring the predictive capabilities of large, high-performing AI models (teachers) to smaller models (students) that can operate in environments with limited computing power. In this paper, we address the scenario in which only a few classes and their associated intermediate concepts are relevant to distill. This scenario is common in practice, yet few existing distillation methods explicitly focus on the relevant subtask. To address this gap, we introduce 'SubDistill', a new distillation algorithm with improved numerical properties that only distills the relevant components of the teacher model at each layer. Experiments on CIFAR-100 and ImageNet with Convolutional and Transformer models demonstrate that SubDistill outperforms existing layer-wise distillation techniques on a representative set of subtasks. Our benchmark evaluations are complemented by Explainable AI analyses showing that our distilled student models more closely match the decision structure of the original teacher model.

</details>


### [84] [Prophet as a Repro ducible Forecasting Framework: A Methodological Guide for Business and Financial Analytics](https://arxiv.org/abs/2601.05929)
*Sidney Shapiro,Burhanuddin Panvelwala*

Main category: cs.LG

TL;DR: 该研究评估了Meta开发的Prophet预测框架在可重复性方面的优势，通过多模型比较（ARIMA和随机森林）展示了Prophet在可解释性、标准化工作流程和可复制性方面的平衡表现。


<details>
  <summary>Details</summary>
Motivation: 预测研究中的可重复性一直是个持续挑战，特别是在商业和金融分析领域。传统方法需要大量手动调参且难以在专有环境中复制，而机器学习方法虽然灵活但存在可解释性差和随机训练过程的问题。需要一种既能保持可解释性又能实现标准化工作流程的解决方案。

Method: 使用公开可用的金融和零售数据集，在受控且完全文档化的实验设计中，将Prophet与多种ARIMA规格（自动选择、手动指定和季节性变体）以及随机森林进行性能比较。通过具体的Python示例展示Prophet如何促进高效预测工作流程。

Result: Prophet在可解释性、标准化工作流程和可访问性方面表现出平衡优势。其加法结构、开源实现和标准化工作流程有助于透明和可复制的预测实践，支持验证、可审计性和方法严谨性。

Conclusion: Prophet作为一个可重复性促进解决方案，在预测研究中发挥着重要作用。它为基于Python的研究工作流程提供了实用的可重复预测参考框架，支持验证、可审计性和方法严谨性，是方法论构建的重要工具。

Abstract: Reproducibility remains a persistent challenge in forecasting research and practice, particularly in business and financial analytics where forecasts inform high-stakes decisions. Traditional forecasting methods, while theoretically interpretable, often require extensive manual tuning and are difficult to replicate in proprietary environments. Machine learning approaches offer predictive flexibility but introduce challenges related to interpretability, stochastic training procedures, and cross-environment reproducibility. This paper examines Prophet, an open-source forecasting framework developed by Meta, as a reproducibility-enabling solution that balances interpretability, standardized workflows, and accessibility. Rather than proposing a new algorithm, this study evaluates how Prophet's additive structure, open-source implementation, and standardized workflow contribute to transparent and replicable forecasting practice. Using publicly available financial and retail datasets, we compare Prophet's performance and interpretability with multiple ARIMA specifications (auto-selected, manually specified, and seasonal variants) and Random Forest under a controlled and fully documented experimental design. This multi-model comparison provides a robust assessment of Prophet's relative performance and reproducibility advantages. Through concrete Python examples, we demonstrate how Prophet facilitates efficient forecasting workflows and integration with analytical pipelines. The study positions Prophet within the broader context of reproducible research. It highlights Prophet's role as a methodological building block that supports verification, auditability, and methodological rigor. This work provides researchers and practitioners with a practical reference framework for reproducible forecasting in Python-based research workflows.

</details>


### [85] [On the Robustness of Age for Learning-Based Wireless Scheduling in Unknown Environments](https://arxiv.org/abs/2601.05956)
*Juaren Steiger,Bin Li*

Main category: cs.LG

TL;DR: 提出一种基于队首年龄（而非虚拟队列长度）的学习调度策略，在信道条件突变时仍能保持系统稳定


<details>
  <summary>Details</summary>
Motivation: 现有基于虚拟队列长度的约束组合多臂老虎机算法在信道条件突变时，约束可能变得不可行，导致虚拟队列长度无限增长，系统不稳定

Method: 设计基于队首年龄（虚拟队列中最旧数据包的年龄）的学习调度策略，替代传统的虚拟队列长度

Result: 在i.i.d.网络条件下性能与现有最优方法相当；在信道条件突变时仍能保持系统稳定，并能从约束不可行期快速恢复

Conclusion: 队首年龄比虚拟队列长度更鲁棒，能有效应对网络条件突变，提高调度算法的稳定性和恢复能力

Abstract: The constrained combinatorial multi-armed bandit model has been widely employed to solve problems in wireless networking and related areas, including the problem of wireless scheduling for throughput optimization under unknown channel conditions. Most work in this area uses an algorithm design strategy that combines a bandit learning algorithm with the virtual queue technique to track the throughput constraint violation. These algorithms seek to minimize the virtual queue length in their algorithm design. However, in networks where channel conditions change abruptly, the resulting constraints may become infeasible, leading to unbounded growth in virtual queue lengths. In this paper, we make the key observation that the dynamics of the head-of-line age, i.e. the age of the oldest packet in the virtual queue, make it more robust when used in algorithm design compared to the virtual queue length. We therefore design a learning-based scheduling policy that uses the head-of-line age in place of the virtual queue length. We show that our policy matches state-of-the-art performance under i.i.d. network conditions. Crucially, we also show that the system remains stable even under abrupt changes in channel conditions and can rapidly recover from periods of constraint infeasibility.

</details>


### [86] [Community-Based Model Sharing and Generalisation: Anomaly Detection in IoT Temperature Sensor Networks](https://arxiv.org/abs/2601.05984)
*Sahibzada Saadoon Hammad,Joaquín Huerta Guijarro,Francisco Ramos,Michael Gould Carlson,Sergio Trilles Oliver*

Main category: cs.LG

TL;DR: 基于社区兴趣（CoI）范式的物联网传感器网络异常检测框架，通过融合时间、空间和海拔相似性将传感器分组，使用自编码器检测温度异常，验证社区内模型共享的可行性。


<details>
  <summary>Details</summary>
Motivation: 物联网设备大规模部署形成异构传感器网络，需要有效组织和管理。社区兴趣（CoI）范式通过分组具有相似特性的设备，为异常检测提供结构化方法，同时减少计算开销。

Method: 1. 使用融合相似性矩阵（Spearman时间相关性、高斯距离空间邻近性、海拔相似性）将传感器分组为社区；2. 基于最佳轮廓系数选择代表性站点；3. 使用三种自编码器架构（BiLSTM、LSTM、MLP）进行贝叶斯超参数优化和扩展窗口交叉验证；4. 在正常温度模式上训练模型，通过重构误差分析检测异常。

Result: 实验结果显示：1. 在评估的配置中，社区内性能稳健；2. 不同社区间存在性能差异；3. 社区间模型共享在减少计算开销方面具有可行性；4. 为分析模型在物联网传感器网络间的泛化性提供了框架。

Conclusion: 基于社区兴趣的异常检测框架有效支持物联网传感器网络的社区内模型共享，减少计算开销，同时为分析模型在不同社区间的泛化性提供了实用方法。

Abstract: The rapid deployment of Internet of Things (IoT) devices has led to large-scale sensor networks that monitor environmental and urban phenomena in real time. Communities of Interest (CoIs) provide a promising paradigm for organising heterogeneous IoT sensor networks by grouping devices with similar operational and environmental characteristics. This work presents an anomaly detection framework based on the CoI paradigm by grouping sensors into communities using a fused similarity matrix that incorporates temporal correlations via Spearman coefficients, spatial proximity using Gaussian distance decay, and elevation similarities. For each community, representative stations based on the best silhouette are selected and three autoencoder architectures (BiLSTM, LSTM, and MLP) are trained using Bayesian hyperparameter optimization with expanding window cross-validation and tested on stations from the same cluster and the best representative stations of other clusters. The models are trained on normal temperature patterns of the data and anomalies are detected through reconstruction error analysis. Experimental results show a robust within-community performance across the evaluated configurations, while variations across communities are observed. Overall, the results support the applicability of community-based model sharing in reducing computational overhead and to analyse model generalisability across IoT sensor networks.

</details>


### [87] [LookAroundNet: Extending Temporal Context with Transformers for Clinically Viable EEG Seizure Detection](https://arxiv.org/abs/2601.06016)
*Þór Sverrisson,Steinn Guðmundsson*

Main category: cs.LG

TL;DR: LookAroundNet：基于Transformer的癫痫检测模型，利用更宽的时间窗口分析EEG信号，通过多数据集评估展现良好的泛化能力和临床部署潜力。


<details>
  <summary>Details</summary>
Motivation: 现有自动癫痫检测方法面临挑战，主要因为癫痫动态在不同患者、记录条件和临床环境中的巨大变异性。需要开发能够适应这种多样性的鲁棒检测系统。

Method: 提出LookAroundNet，一种基于Transformer的癫痫检测器，使用更宽的EEG时间窗口（包括感兴趣段前后的信号），模仿临床医生解读EEG时使用的上下文信息。在多数据集上进行评估，包括公开数据集和大型专有家庭EEG记录。

Result: LookAroundNet在多个数据集上表现出色，对未见过的记录条件具有良好的泛化能力，计算成本适合实际临床部署。扩展的时间上下文、增加训练数据多样性和模型集成是提升性能的关键因素。

Conclusion: 这项研究推动了自动癫痫检测模型向临床可行解决方案的发展，表明利用更广泛的时间上下文和多样化训练数据可以显著提高模型在真实世界条件下的性能。

Abstract: Automated seizure detection from electroencephalography (EEG) remains difficult due to the large variability of seizure dynamics across patients, recording conditions, and clinical settings. We introduce LookAroundNet, a transformer-based seizure detector that uses a wider temporal window of EEG data to model seizure activity. The seizure detector incorporates EEG signals before and after the segment of interest, reflecting how clinicians use surrounding context when interpreting EEG recordings. We evaluate the proposed method on multiple EEG datasets spanning diverse clinical environments, patient populations, and recording modalities, including routine clinical EEG and long-term ambulatory recordings, in order to study performance across varying data distributions. The evaluation includes publicly available datasets as well as a large proprietary collection of home EEG recordings, providing complementary views of controlled clinical data and unconstrained home-monitoring conditions. Our results show that LookAroundNet achieves strong performance across datasets, generalizes well to previously unseen recording conditions, and operates with computational costs compatible with real-world clinical deployment. The results indicate that extended temporal context, increased training data diversity, and model ensembling are key factors for improving performance. This work contributes to moving automatic seizure detection models toward clinically viable solutions.

</details>
