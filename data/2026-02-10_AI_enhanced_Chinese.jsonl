{"id": "2602.07004", "categories": ["gr-qc", "astro-ph.HE", "hep-ph", "nucl-th"], "pdf": "https://arxiv.org/pdf/2602.07004", "abs": "https://arxiv.org/abs/2602.07004", "authors": ["Herman J. Mosquera Cuesta", "Fabi\u00e1n H. Zuluaga Giraldo", "Wilmer D. Alfonso Pardo", "Edgardo Marbello Santrich", "Guillermo U. Avenda\u00f1o Franco", "Rafael Fragozo Larrazabal"], "title": "Asymptotic Freedom and Vacuum Polarization Determine the Astrophysical End State of Relativistic Gravitational Collapse: Quark--Gluon Plasma Star Instead of Black Hole", "comment": "25 pages, 4 figues", "summary": "A general relativistic model of an astrophysical hypermassive extremely magnetized ultra-compact self-bound quark--gluon plasma object that is supported against its ultimate gravitational implosion by the simultaneous action of the vacuum polarization driven by nonlinear electrodynamics (NLED: light-by-light scattering) and the quantum chromodynamics (QCD) asymptotic freedom, is presented. These QCD stars can be the final figures of the equilibrium of collapsing stellar cores. Post-supernova fallback material pushes the nascent remnant beyond its stability to collapse into a hybrid hypermassive neutron star (HHMNS). Hypercritical accretion can unbind the whole HHMNS's baryons to spontaneously break away color confinement, powering a first-order hadron-to-quark phase transition to a sea of ever-freer quarks and gluons. This core is hydro-stabilized by the steady, endlessly compression-admitting asymptotic freedom state, possibly via gluon-mediated enduring exchange of color charge among bound states. The nonlinear TOV equation indicates the occurrence of hypermassive QGP/QCD stars with a wide mass spectrum ($0\\lesssim$ M$^{\\rm{QGP}}_{\\rm{Star}}\\lesssim$\\,7\\,M$_\\odot$ and beyond), for star radii ($0\\lesssim R^{\\rm{QGP}}_{\\rm{Star}}\\lesssim 24$\\,km and beyond) with B-fields ($10^{14} \\leq$ B$^{\\rm{QGP}}_{\\rm{Star}} \\leq 10^{16}$\\,G and beyond). Such QCD stars can emulate what the true black holes are supposed to gravitationally do in most astrophysical settings. This color quark star could be found through a search for its eternal ``yo-yo'' state gravitational-wave emission, or via lensing phenomena like gravitational rainbows, as in this scenario it is expected that the light deflection angle, directly influenced by the larger effective mass/radius and magnetic field of the deflecting object, increases as the incidence angle decreases for impact parameter lower values.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5e7f\u4e49\u76f8\u5bf9\u8bba\u6a21\u578b\uff0c\u63cf\u8ff0\u7531\u975e\u7ebf\u6027\u7535\u52a8\u529b\u5b66\u771f\u7a7a\u6781\u5316\u548c\u91cf\u5b50\u8272\u52a8\u529b\u5b66\u6e10\u8fd1\u81ea\u7531\u5171\u540c\u652f\u6491\u7684\u8d85\u5927\u8d28\u91cf\u3001\u6781\u5f3a\u78c1\u573a\u3001\u8d85\u81f4\u5bc6\u7684\u81ea\u675f\u7f1a\u5938\u514b-\u80f6\u5b50\u7b49\u79bb\u5b50\u4f53\u5929\u4f53\uff0c\u53ef\u4f5c\u4e3a\u574d\u7f29\u6052\u661f\u6838\u5fc3\u7684\u6700\u7ec8\u5e73\u8861\u6001\u3002", "motivation": "\u63a2\u7d22\u8d85\u65b0\u661f\u540e\u574d\u7f29\u8fc7\u7a0b\u4e2d\uff0c\u5f53\u7269\u8d28\u56de\u843d\u5230\u65b0\u751f\u9057\u8ff9\u4e0a\u8d85\u8fc7\u5176\u7a33\u5b9a\u6027\u65f6\uff0c\u5982\u4f55\u5f62\u6210\u7531\u5938\u514b-\u80f6\u5b50\u7b49\u79bb\u5b50\u4f53\u6784\u6210\u7684\u65b0\u578b\u5929\u4f53\uff0c\u8fd9\u79cd\u5929\u4f53\u53ef\u80fd\u6a21\u62df\u9ed1\u6d1e\u7684\u5f15\u529b\u6548\u5e94\u4f46\u5177\u6709\u4e0d\u540c\u7684\u7269\u7406\u672c\u8d28\u3002", "method": "\u7ed3\u5408\u5e7f\u4e49\u76f8\u5bf9\u8bba\u3001\u975e\u7ebf\u6027\u7535\u52a8\u529b\u5b66\uff08NLED\uff09\u548c\u91cf\u5b50\u8272\u52a8\u529b\u5b66\uff08QCD\uff09\uff0c\u5efa\u7acb\u975e\u7ebf\u6027TOV\u65b9\u7a0b\u6a21\u578b\uff0c\u5206\u6790\u7531\u6e10\u8fd1\u81ea\u7531\u72b6\u6001\u548c\u771f\u7a7a\u6781\u5316\u5171\u540c\u652f\u6491\u7684\u5938\u514b-\u80f6\u5b50\u7b49\u79bb\u5b50\u4f53\u5929\u4f53\u7684\u5e73\u8861\u6761\u4ef6\u3002", "result": "\u6a21\u578b\u9884\u6d4b\u5b58\u5728\u8d28\u91cf\u8303\u56f4\u5e7f\u6cdb\uff080-7\u592a\u9633\u8d28\u91cf\u53ca\u4ee5\u4e0a\uff09\u3001\u534a\u5f840-24\u516c\u91cc\u53ca\u4ee5\u4e0a\u3001\u78c1\u573a\u5f3a\u5ea610^14-10^16\u9ad8\u65af\u53ca\u4ee5\u4e0a\u7684\u8d85\u5927\u8d28\u91cfQCD\u661f\uff0c\u8fd9\u4e9b\u5929\u4f53\u53ef\u901a\u8fc7\u5f15\u529b\u6ce2\"\u60a0\u60a0\u7403\"\u6001\u53d1\u5c04\u6216\u5f15\u529b\u5f69\u8679\u7b49\u900f\u955c\u73b0\u8c61\u88ab\u63a2\u6d4b\u3002", "conclusion": "QCD\u661f\u53ef\u4f5c\u4e3a\u574d\u7f29\u6052\u661f\u6838\u5fc3\u7684\u6700\u7ec8\u5e73\u8861\u6001\uff0c\u5728\u5927\u591a\u6570\u5929\u4f53\u7269\u7406\u73af\u5883\u4e2d\u6a21\u62df\u9ed1\u6d1e\u7684\u5f15\u529b\u6548\u5e94\uff0c\u4f46\u5177\u6709\u4e0d\u540c\u7684\u5185\u90e8\u7ed3\u6784\u548c\u53ef\u89c2\u6d4b\u7279\u5f81\uff0c\u4e3a\u63a2\u7d22\u6781\u7aef\u6761\u4ef6\u4e0b\u7684\u5f3a\u76f8\u4e92\u4f5c\u7528\u7269\u8d28\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.07043", "categories": ["gr-qc", "hep-th", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.07043", "abs": "https://arxiv.org/abs/2602.07043", "authors": ["Zhilong Liu", "Wentao Liu", "Zehua Tian", "Jieci Wang"], "title": "Escape of quantum information across an analogue black hole horizon", "comment": "7 pages, 5 figures", "summary": "The complete evaporation of black holes, as a natural endpoint of Hawking radiation, gives rise to the black hole information paradox, which fundamentally challenges the principles of unitarity and information conservation in quantum mechanics. Although the AdS/CFT correspondence indicates that information is preserved during black hole evaporation, the precise mechanism by which it is recovered from the Hawking radiation remains an open question. To explore a potential resolution, we investigate information transfer in an analog black hole spacetime realized through position-dependent coupling in an XY spin chain. We derive and demonstrate Page curve-like behavior, and analyze the transmission of quantum resources, such as entanglement and coherence, across the effective horizon. Our results show that quantum resources initially localized within an interior subsystem can be transferred to the exterior via particle radiation through the horizon. This study provides a novel perspective from quantum simulation on how information may escape from black holes, thereby contributing to the further understanding of the black hole information paradox.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7XY\u81ea\u65cb\u94fe\u4e2d\u7684\u4f4d\u7f6e\u4f9d\u8d56\u8026\u5408\u6a21\u62df\u9ed1\u6d1e\u65f6\u7a7a\uff0c\u63a2\u7d22\u4e86\u9ed1\u6d1e\u4fe1\u606f\u6096\u8bba\u7684\u91cf\u5b50\u6a21\u62df\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u7c7bPage\u66f2\u7ebf\u884c\u4e3a\u4ee5\u53ca\u91cf\u5b50\u8d44\u6e90\u901a\u8fc7\u89c6\u754c\u7684\u4f20\u8f93\u673a\u5236\u3002", "motivation": "\u9ed1\u6d1e\u5b8c\u5168\u84b8\u53d1\u4f5c\u4e3a\u970d\u91d1\u8f90\u5c04\u7684\u81ea\u7136\u7ec8\u70b9\uff0c\u5f15\u53d1\u4e86\u9ed1\u6d1e\u4fe1\u606f\u6096\u8bba\uff0c\u8fd9\u4ece\u6839\u672c\u4e0a\u6311\u6218\u4e86\u91cf\u5b50\u529b\u5b66\u4e2d\u7684\u5e7a\u6b63\u6027\u548c\u4fe1\u606f\u5b88\u6052\u539f\u7406\u3002\u867d\u7136AdS/CFT\u5bf9\u5e94\u8868\u660e\u4fe1\u606f\u5728\u9ed1\u6d1e\u84b8\u53d1\u8fc7\u7a0b\u4e2d\u88ab\u4fdd\u5b58\uff0c\u4f46\u4fe1\u606f\u5982\u4f55\u4ece\u970d\u91d1\u8f90\u5c04\u4e2d\u6062\u590d\u7684\u5177\u4f53\u673a\u5236\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5728XY\u81ea\u65cb\u94fe\u4e2d\u5b9e\u73b0\u4f4d\u7f6e\u4f9d\u8d56\u8026\u5408\u6765\u6784\u5efa\u6a21\u62df\u9ed1\u6d1e\u65f6\u7a7a\uff0c\u7814\u7a76\u4fe1\u606f\u4f20\u8f93\u673a\u5236\u3002\u63a8\u5bfc\u5e76\u5c55\u793a\u4e86\u7c7bPage\u66f2\u7ebf\u884c\u4e3a\uff0c\u5206\u6790\u4e86\u7ea0\u7f20\u548c\u76f8\u5e72\u6027\u7b49\u91cf\u5b50\u8d44\u6e90\u5728\u6709\u6548\u89c6\u754c\u4e0a\u7684\u4f20\u8f93\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6700\u521d\u5c40\u57df\u5728\u5185\u90e8\u5b50\u7cfb\u7edf\u4e2d\u7684\u91cf\u5b50\u8d44\u6e90\u53ef\u4ee5\u901a\u8fc7\u7c92\u5b50\u8f90\u5c04\u7a7f\u8fc7\u89c6\u754c\u4f20\u8f93\u5230\u5916\u90e8\u3002\u8fd9\u4e3a\u4fe1\u606f\u5982\u4f55\u4ece\u9ed1\u6d1e\u4e2d\u9003\u9038\u63d0\u4f9b\u4e86\u65b0\u7684\u91cf\u5b50\u6a21\u62df\u89c6\u89d2\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u91cf\u5b50\u6a21\u62df\u65b9\u6cd5\u4e3a\u9ed1\u6d1e\u4fe1\u606f\u6096\u8bba\u7684\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5c55\u793a\u4e86\u91cf\u5b50\u8d44\u6e90\u901a\u8fc7\u6a21\u62df\u9ed1\u6d1e\u89c6\u754c\u7684\u4f20\u8f93\u673a\u5236\uff0c\u6709\u52a9\u4e8e\u8fdb\u4e00\u6b65\u7406\u89e3\u9ed1\u6d1e\u4fe1\u606f\u95ee\u9898\u3002"}}
{"id": "2602.07115", "categories": ["gr-qc", "astro-ph.HE", "hep-th"], "pdf": "https://arxiv.org/pdf/2602.07115", "abs": "https://arxiv.org/abs/2602.07115", "authors": ["Irvin Mart\u00ednez-Rodr\u00edguez"], "title": "Neutron Stars as Perfect Fluids: Extracting the Linearized Response Function", "comment": "7 pages, 1 figure", "summary": "We derive the general relativistic linear tidal response of a neutron star modeled as a barotropic perfect fluid. From the covariant fluid effective action, we linearize about equilibrium and obtain the action for fluid displacements coupled to metric perturbations. Splitting the latter into external and induced parts and integrating out the induced field yields a Hermitian operator and a discrete gapped spectrum of driven modes. Projecting the displacement onto this eigenbasis and integrating out the spatial dependence over the stellar radius reduces the dynamics to tidal-driven oscillators, with couplings set by relativistic inner products and overlap integrals. Matching to the quadrupolar worldline effective action gives a mode-sum response function and analytic dynamical tidal deformabilities from mode frequencies, normalizations, and overlaps.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a8\u5bfc\u4e86\u4e2d\u5b50\u661f\u5728\u5e7f\u4e49\u76f8\u5bf9\u8bba\u6846\u67b6\u4e0b\u7684\u7ebf\u6027\u6f6e\u6c50\u54cd\u5e94\uff0c\u5c06\u4e2d\u5b50\u661f\u5efa\u6a21\u4e3a\u6b63\u538b\u7406\u60f3\u6d41\u4f53\uff0c\u901a\u8fc7\u6709\u6548\u4f5c\u7528\u91cf\u65b9\u6cd5\u5f97\u5230\u79bb\u6563\u7684\u9a71\u52a8\u6a21\u5f0f\u8c31\uff0c\u6700\u7ec8\u7ed9\u51fa\u6a21\u5f0f\u6c42\u548c\u7684\u54cd\u5e94\u51fd\u6570\u548c\u89e3\u6790\u7684\u52a8\u6001\u6f6e\u6c50\u5f62\u53d8\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u4e2d\u5b50\u661f\u5728\u6f6e\u6c50\u529b\u4f5c\u7528\u4e0b\u7684\u54cd\u5e94\u5bf9\u4e8e\u7406\u89e3\u53cc\u661f\u5e76\u5408\u3001\u5f15\u529b\u6ce2\u4fe1\u53f7\u7b49\u5929\u4f53\u7269\u7406\u73b0\u8c61\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u5efa\u7acb\u5e7f\u4e49\u76f8\u5bf9\u8bba\u6846\u67b6\u4e0b\u7684\u6d41\u4f53\u52a8\u529b\u5b66\u6a21\u578b\u6765\u63cf\u8ff0\u4e2d\u5b50\u661f\u7684\u6f6e\u6c50\u5f62\u53d8\u3002", "method": "\u4ece\u534f\u53d8\u7684\u6d41\u4f53\u6709\u6548\u4f5c\u7528\u91cf\u51fa\u53d1\uff0c\u7ebf\u6027\u5316\u5e73\u8861\u6001\uff0c\u5f97\u5230\u6d41\u4f53\u4f4d\u79fb\u4e0e\u5ea6\u89c4\u6270\u52a8\u7684\u8026\u5408\u4f5c\u7528\u91cf\u3002\u5c06\u5ea6\u89c4\u6270\u52a8\u5206\u4e3a\u5916\u90e8\u548c\u8bf1\u5bfc\u90e8\u5206\uff0c\u79ef\u5206\u6389\u8bf1\u5bfc\u573a\u5f97\u5230\u5384\u7c73\u7b97\u7b26\u548c\u79bb\u6563\u7684\u5e26\u9699\u9a71\u52a8\u6a21\u5f0f\u8c31\u3002\u5c06\u4f4d\u79fb\u6295\u5f71\u5230\u672c\u5f81\u57fa\u4e0a\uff0c\u79ef\u5206\u6389\u7a7a\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u7b80\u5316\u4e3a\u6f6e\u6c50\u9a71\u52a8\u7684\u632f\u5b50\u6a21\u578b\u3002", "result": "\u5f97\u5230\u4e86\u6a21\u5f0f\u6c42\u548c\u7684\u54cd\u5e94\u51fd\u6570\uff0c\u4ee5\u53ca\u4ece\u6a21\u5f0f\u9891\u7387\u3001\u5f52\u4e00\u5316\u5e38\u6570\u548c\u91cd\u53e0\u79ef\u5206\u89e3\u6790\u63a8\u5bfc\u51fa\u7684\u52a8\u6001\u6f6e\u6c50\u5f62\u53d8\u80fd\u529b\u3002\u5efa\u7acb\u4e86\u6f6e\u6c50\u54cd\u5e94\u4e0e\u4e2d\u5b50\u661f\u5185\u90e8\u632f\u8361\u6a21\u5f0f\u4e4b\u95f4\u7684\u76f4\u63a5\u8054\u7cfb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7814\u7a76\u4e2d\u5b50\u661f\u6f6e\u6c50\u54cd\u5e94\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u573a\u8bba\u6846\u67b6\uff0c\u80fd\u591f\u89e3\u6790\u5730\u8ba1\u7b97\u52a8\u6001\u6f6e\u6c50\u5f62\u53d8\u80fd\u529b\uff0c\u5bf9\u4e8e\u7406\u89e3\u53cc\u661f\u5e76\u5408\u8fc7\u7a0b\u4e2d\u7684\u6f6e\u6c50\u6548\u5e94\u548c\u5f15\u529b\u6ce2\u4fe1\u53f7\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.07268", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2602.07268", "abs": "https://arxiv.org/abs/2602.07268", "authors": ["S. C. Ulhoa", "F. L. Carneiro", "B. C. C. Carneiro"], "title": "On the Gravitational Energy of Axial Perturbations in Regular Black Holes", "comment": "15 pages, 4 figures", "summary": "The article deals with the gravitational energy associated with axial perturbations of regular black holes. We review the stability of the geometry under odd-parity perturbations and the corresponding quasinormal modes, previously obtained for this class of spacetimes. The perturbative functions describing the metric fluctuations are reconstructed from the master equation. To evaluate the energy content of these perturbations, we employ the Teleparallel Equivalent of General Relativity (TEGR), which provides a well-defined expression for gravitational energy. The gravitational energy is computed up to second order in the perturbation parameter and expressed in terms of the quasinormal mode functions. Our results establish a direct connection between the dynamical response of regular black holes and the energy carried by their gravitational perturbations.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u89c4\u5219\u9ed1\u6d1e\u8f74\u5411\u6270\u52a8\u7684\u5f15\u529b\u80fd\u91cf\uff0c\u5229\u7528TEGR\u7406\u8bba\u8ba1\u7b97\u4e8c\u9636\u6270\u52a8\u80fd\u91cf\uff0c\u5efa\u7acb\u89c4\u5219\u9ed1\u6d1e\u52a8\u529b\u5b66\u54cd\u5e94\u4e0e\u5f15\u529b\u6270\u52a8\u80fd\u91cf\u4e4b\u95f4\u7684\u76f4\u63a5\u8054\u7cfb\u3002", "motivation": "\u7814\u7a76\u89c4\u5219\u9ed1\u6d1e\u8f74\u5411\u6270\u52a8\u76f8\u5173\u7684\u5f15\u529b\u80fd\u91cf\uff0c\u63a2\u7d22\u9ed1\u6d1e\u52a8\u529b\u5b66\u54cd\u5e94\u4e0e\u5f15\u529b\u6270\u52a8\u80fd\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4e3a\u7406\u89e3\u89c4\u5219\u9ed1\u6d1e\u7684\u7a33\u5b9a\u6027\u63d0\u4f9b\u80fd\u91cf\u89c6\u89d2\u3002", "method": "1. \u56de\u987e\u89c4\u5219\u9ed1\u6d1e\u5728\u5947\u5b87\u79f0\u6270\u52a8\u4e0b\u7684\u7a33\u5b9a\u6027\u53ca\u51c6\u6b63\u89c4\u6a21\uff1b2. \u4ece\u4e3b\u65b9\u7a0b\u91cd\u6784\u63cf\u8ff0\u5ea6\u89c4\u6da8\u843d\u7684\u6270\u52a8\u51fd\u6570\uff1b3. \u4f7f\u7528\u5e7f\u4e49\u76f8\u5bf9\u8bba\u7684Teleparallel\u7b49\u6548\u7406\u8bba\uff08TEGR\uff09\u8ba1\u7b97\u5f15\u529b\u80fd\u91cf\uff1b4. \u5c06\u5f15\u529b\u80fd\u91cf\u8ba1\u7b97\u5230\u6270\u52a8\u53c2\u6570\u7684\u4e8c\u9636\uff0c\u5e76\u7528\u51c6\u6b63\u89c4\u6a21\u51fd\u6570\u8868\u793a\u3002", "result": "\u6210\u529f\u8ba1\u7b97\u4e86\u89c4\u5219\u9ed1\u6d1e\u8f74\u5411\u6270\u52a8\u7684\u5f15\u529b\u80fd\u91cf\uff08\u4e8c\u9636\u6270\u52a8\uff09\uff0c\u5efa\u7acb\u4e86\u89c4\u5219\u9ed1\u6d1e\u52a8\u529b\u5b66\u54cd\u5e94\u4e0e\u5f15\u529b\u6270\u52a8\u80fd\u91cf\u4e4b\u95f4\u7684\u76f4\u63a5\u8054\u7cfb\uff0c\u4e3a\u7406\u89e3\u89c4\u5219\u9ed1\u6d1e\u7684\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u80fd\u91cf\u6846\u67b6\u3002", "conclusion": "\u901a\u8fc7TEGR\u7406\u8bba\u6210\u529f\u91cf\u5316\u4e86\u89c4\u5219\u9ed1\u6d1e\u8f74\u5411\u6270\u52a8\u7684\u5f15\u529b\u80fd\u91cf\uff0c\u63ed\u793a\u4e86\u9ed1\u6d1e\u52a8\u529b\u5b66\u54cd\u5e94\u4e0e\u6270\u52a8\u80fd\u91cf\u4e4b\u95f4\u7684\u5185\u5728\u8054\u7cfb\uff0c\u4e3a\u89c4\u5219\u9ed1\u6d1e\u7684\u7a33\u5b9a\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u80fd\u91cf\u5b66\u57fa\u7840\u3002"}}
{"id": "2602.06986", "categories": ["physics.comp-ph", "cond-mat.mtrl-sci", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.06986", "abs": "https://arxiv.org/abs/2602.06986", "authors": ["Udaykumar Gajera", "Mohsen Sotoudeh", "Kanchan Sarkar", "Axel Gro\u00df"], "title": "DISCOVER: A Physics-Informed, GPU-Accelerated Symbolic Regression Framework", "comment": "main paper with only 7 pages including 1 figure along with supplementary materials with mathematics guide", "summary": "Symbolic Regression (SR) enables the discovery of interpretable mathematical relationships from experimental and simulation data. These relationships are often coined descriptors which are defined as a fundamental materials property that is directly correlated to a desired or undesired functional property of the material. Although established approaches such as Sure Independence Screening and Sparsifying Operator (SISSO) have successfully identified low-dimensional descriptors within large feature spaces many existing SR tools integrate poorly with modern Python workflows, offer limited control over the symbolic search space, or struggle with the computational demands of large-scale studies. This paper introduces DISCOVER (Data-Informed Symbolic Combination of Operators for Variable Equation Regression), an open-source symbolic regression package developed to address these challenges through a modular, physics-motivated design. DISCOVER allows users to guide the symbolic search using domain knowledge, constrain the feature space explicitly, and take advantage of optional GPU acceleration to improve computational efficiency in data-intensive workflows, enabling reproducible and scalable SR workflows. The software is intended for applications in computational physics, computational chemistry, and materials science, where interpretability, physical consistency, and execution time are especially important, and it complements general-purpose SR frameworks by emphasizing the discovery of physically meaningful models.", "AI": {"tldr": "DISCOVER\u662f\u4e00\u4e2a\u7528\u4e8e\u6750\u6599\u79d1\u5b66\u7b49\u9886\u57df\u7684\u7b26\u53f7\u56de\u5f52\u5f00\u6e90\u8f6f\u4ef6\u5305\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u3001\u7269\u7406\u9a71\u52a8\u7684\u8bbe\u8ba1\u89e3\u51b3\u73b0\u6709\u5de5\u5177\u4e0ePython\u5de5\u4f5c\u6d41\u96c6\u6210\u5dee\u3001\u7b26\u53f7\u641c\u7d22\u7a7a\u95f4\u63a7\u5236\u6709\u9650\u3001\u8ba1\u7b97\u6548\u7387\u4f4e\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7b26\u53f7\u56de\u5f52\u5de5\u5177\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u4e0e\u73b0\u4ee3Python\u5de5\u4f5c\u6d41\u96c6\u6210\u5dee\uff1b2\uff09\u5bf9\u7b26\u53f7\u641c\u7d22\u7a7a\u95f4\u7684\u63a7\u5236\u6709\u9650\uff1b3\uff09\u5728\u5927\u89c4\u6a21\u7814\u7a76\u4e2d\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3002\u8fd9\u4e9b\u95ee\u9898\u9650\u5236\u4e86\u7b26\u53f7\u56de\u5f52\u5728\u6750\u6599\u79d1\u5b66\u7b49\u9886\u57df\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u53ef\u89e3\u91ca\u6027\u3001\u7269\u7406\u4e00\u81f4\u6027\u548c\u6267\u884c\u6548\u7387\u7684\u573a\u666f\u4e2d\u3002", "method": "DISCOVER\u91c7\u7528\u6a21\u5757\u5316\u3001\u7269\u7406\u9a71\u52a8\u7684\u8bbe\u8ba1\uff0c\u5141\u8bb8\u7528\u6237\uff1a1\uff09\u5229\u7528\u9886\u57df\u77e5\u8bc6\u6307\u5bfc\u7b26\u53f7\u641c\u7d22\uff1b2\uff09\u663e\u5f0f\u7ea6\u675f\u7279\u5f81\u7a7a\u95f4\uff1b3\uff09\u5229\u7528\u53ef\u9009\u7684GPU\u52a0\u901f\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002\u8f6f\u4ef6\u5f3a\u8c03\u53d1\u73b0\u7269\u7406\u4e0a\u6709\u610f\u4e49\u7684\u6a21\u578b\uff0c\u652f\u6301\u53ef\u91cd\u590d\u548c\u53ef\u6269\u5c55\u7684\u7b26\u53f7\u56de\u5f52\u5de5\u4f5c\u6d41\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f00\u6e90\u7b26\u53f7\u56de\u5f52\u8f6f\u4ef6\u5305DISCOVER\uff0c\u4e13\u95e8\u9488\u5bf9\u8ba1\u7b97\u7269\u7406\u3001\u8ba1\u7b97\u5316\u5b66\u548c\u6750\u6599\u79d1\u5b66\u9886\u57df\u7684\u9700\u6c42\uff0c\u5728\u8fd9\u4e9b\u9886\u57df\u4e2d\u53ef\u89e3\u91ca\u6027\u3001\u7269\u7406\u4e00\u81f4\u6027\u548c\u6267\u884c\u65f6\u95f4\u5c24\u4e3a\u91cd\u8981\u3002", "conclusion": "DISCOVER\u901a\u8fc7\u5176\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u7269\u7406\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7b26\u53f7\u56de\u5f52\u5de5\u5177\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6750\u6599\u79d1\u5b66\u7b49\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u7b26\u53f7\u56de\u5f52\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u53d1\u73b0\u7269\u7406\u4e0a\u6709\u610f\u4e49\u7684\u6a21\u578b\u3002"}}
{"id": "2602.07097", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.07097", "abs": "https://arxiv.org/abs/2602.07097", "authors": ["Tayyab Ali"], "title": "Alleviating Post-Linearization Challenges for Solving Nonlinear Systems on a Quantum Computer", "comment": null, "summary": "The linearity inherent in quantum mechanics limits current quantum hardware from directly solving nonlinear systems governed by nonlinear differential equations. One can opt for linearization frameworks such as Carleman linearization, which provides a high dimensional infinite linear system corresponding to a finite nonlinear system, as an indirect way of solving nonlinear systems using current quantum computers. We provide an efficient data access model to load this infinite linear representation of the nonlinear system, upto truncation order $N$, on a quantum computer by decomposing the Hamiltonian into the weighted sum of non-unitary operators, namely the Sigma basis. We have shown that the Sigma basis provides an exponential reduction in the number of decomposition terms compared to the traditional decomposition, which is usually done in a linear combination of Pauli operators. Once the Hamiltonian is decomposed, we then use the concept of unitary completion to construct the circuit for the implementation of each weighted tensor product component $\\mathcal{H}_{j}$ of the decomposition.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u8bbf\u95ee\u6a21\u578b\uff0c\u901a\u8fc7Sigma\u57fa\u5206\u89e3\u5c06\u975e\u7ebf\u6027\u7cfb\u7edf\u7684Carleman\u7ebf\u6027\u5316\u8868\u793a\u52a0\u8f7d\u5230\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\uff0c\u76f8\u6bd4\u4f20\u7edfPauli\u5206\u89e3\u5b9e\u73b0\u4e86\u6307\u6570\u7ea7\u51cf\u5c11\u5206\u89e3\u9879\u6570", "motivation": "\u91cf\u5b50\u529b\u5b66\u7684\u7ebf\u6027\u7279\u6027\u9650\u5236\u4e86\u5f53\u524d\u91cf\u5b50\u786c\u4ef6\u76f4\u63a5\u6c42\u89e3\u975e\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u7cfb\u7edf\u3002\u867d\u7136\u53ef\u4ee5\u901a\u8fc7Carleman\u7ebf\u6027\u5316\u5c06\u6709\u9650\u975e\u7ebf\u6027\u7cfb\u7edf\u8f6c\u5316\u4e3a\u9ad8\u7ef4\u65e0\u9650\u7ebf\u6027\u7cfb\u7edf\uff0c\u4f46\u9700\u8981\u9ad8\u6548\u7684\u65b9\u6cd5\u5728\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u52a0\u8f7d\u8fd9\u79cd\u8868\u793a", "method": "1) \u4f7f\u7528Sigma\u57fa\uff08\u975e\u9149\u7b97\u5b50\u7684\u52a0\u6743\u548c\uff09\u5206\u89e3Carleman\u7ebf\u6027\u5316\u8868\u793a\u7684\u54c8\u5bc6\u987f\u91cf\uff1b2) \u4e0e\u4f20\u7edfPauli\u7ebf\u6027\u7ec4\u5408\u5206\u89e3\u76f8\u6bd4\uff0cSigma\u57fa\u5b9e\u73b0\u4e86\u6307\u6570\u7ea7\u51cf\u5c11\u5206\u89e3\u9879\u6570\uff1b3) \u5229\u7528\u9149\u5b8c\u5907\u5316\u6982\u5ff5\u4e3a\u5206\u89e3\u4e2d\u7684\u6bcf\u4e2a\u52a0\u6743\u5f20\u91cf\u79ef\u5206\u91cf\u6784\u5efa\u7535\u8def\u5b9e\u73b0", "result": "\u63d0\u51fa\u7684Sigma\u57fa\u5206\u89e3\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edfPauli\u5206\u89e3\uff0c\u5728\u5c06\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u65e0\u9650\u7ebf\u6027\u8868\u793a\uff08\u622a\u65ad\u5230N\u9636\uff09\u52a0\u8f7d\u5230\u91cf\u5b50\u8ba1\u7b97\u673a\u65f6\uff0c\u5b9e\u73b0\u4e86\u5206\u89e3\u9879\u6570\u7684\u6307\u6570\u7ea7\u51cf\u5c11", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5728\u73b0\u6709\u91cf\u5b50\u786c\u4ef6\u4e0a\u901a\u8fc7Carleman\u7ebf\u6027\u5316\u95f4\u63a5\u6c42\u89e3\u975e\u7ebf\u6027\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u52a0\u8f7d\u65b9\u6848\uff0c\u901a\u8fc7\u521b\u65b0\u7684Sigma\u57fa\u5206\u89e3\u548c\u9149\u5b8c\u5907\u5316\u6280\u672f\u663e\u8457\u964d\u4f4e\u4e86\u5b9e\u73b0\u590d\u6742\u5ea6"}}
{"id": "2602.06993", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.06993", "abs": "https://arxiv.org/abs/2602.06993", "authors": ["Shashank"], "title": "Attractor Patch Networks: Reducing Catastrophic Forgetting with Routed Low-Rank Patch Experts", "comment": "9 pages. Code (APN implementation in nanoGPT transformer): https://github.com/shankch/nanoGPT-apn (baseline: https://github.com/karpathy/nanoGPT) Data prep: https://github.com/karpathy/nanoGPT/tree/master/data/shakespeare_char and https://github.com/karpathy/nanoGPT/tree/master/data/shakespeare", "summary": "Transformers achieve strong language modeling accuracy, yet their position-wise feed-forward networks (FFNs) are dense, globally shared, and typically updated end to end. These properties create two practical tensions. First, dense FFNs spend the same compute on every token regardless of context, and they allocate capacity uniformly even when language exhibits highly clustered context structure. Second, continual learning, in the sense of updating the model while serving a data stream, often produces interference because a small update touches broadly shared weights.\n  We propose Attractor Patch Networks (APN), a plug-compatible replacement for the Transformer FFN. APN is a bank of patch experts. A similarity router selects a small top-k set of patches for each token by matching the token representation to learned prototypes. Each selected patch emits a low-rank residual update conditioned on a compact code. The architecture yields conditional, context-specialized nonlinear transformations while preserving the standard Transformer interface.\n  This paper focuses on APN as an architectural primitive. We formalize APN, analyze its expressivity as a piecewise low-rank residual function class, and derive simple interference and stability arguments that make APN naturally compatible with continual learning. In experiments on character-level language modeling, APN achieves competitive perplexity (4.57 vs 4.32 PPL) while enabling dramatically better continual adaptation: when adapting to a shifted domain, APN achieves 2.6 times better retention (11.1 vs 29.4 PPL on the original domain) and 2.8 times better adaptation (6.4 vs 17.8 PPL on the new domain) compared to global fine-tuning of a dense FFN baseline.", "AI": {"tldr": "\u63d0\u51faAttractor Patch Networks (APN)\u4f5c\u4e3aTransformer FFN\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u76f8\u4f3c\u6027\u8def\u7531\u9009\u62e9top-k\u4e13\u5bb6\u8865\u4e01\uff0c\u5b9e\u73b0\u6761\u4ef6\u5316\u3001\u4e0a\u4e0b\u6587\u7279\u5316\u7684\u975e\u7ebf\u6027\u53d8\u6362\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u8bed\u8a00\u5efa\u6a21\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfTransformer\u7684FFN\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a1) \u5bf9\u6240\u6709token\u4f7f\u7528\u76f8\u540c\u8ba1\u7b97\u91cf\uff0c\u4e0d\u8003\u8651\u4e0a\u4e0b\u6587\u7ed3\u6784\uff1b2) \u6301\u7eed\u5b66\u4e60\u65f6\u5168\u5c40\u5171\u4eab\u6743\u91cd\u66f4\u65b0\u4f1a\u5bfc\u81f4\u5e72\u6270\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6839\u636e\u4e0a\u4e0b\u6587\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u5e76\u51cf\u5c11\u5e72\u6270\u7684\u67b6\u6784\u3002", "method": "\u63d0\u51faAPN\u67b6\u6784\uff1a\u5305\u542b\u4e13\u5bb6\u8865\u4e01\u5e93\uff0c\u901a\u8fc7\u76f8\u4f3c\u6027\u8def\u7531\u5339\u914dtoken\u8868\u793a\u4e0e\u5b66\u4e60\u539f\u578b\uff0c\u9009\u62e9top-k\u8865\u4e01\uff0c\u6bcf\u4e2a\u8865\u4e01\u57fa\u4e8e\u7d27\u51d1\u4ee3\u7801\u751f\u6210\u4f4e\u79e9\u6b8b\u5dee\u66f4\u65b0\uff0c\u5f62\u6210\u6761\u4ef6\u5316\u3001\u4e0a\u4e0b\u6587\u7279\u5316\u7684\u975e\u7ebf\u6027\u53d8\u6362\u3002", "result": "\u5728\u5b57\u7b26\u7ea7\u8bed\u8a00\u5efa\u6a21\u4e2d\uff0cAPN\u8fbe\u5230\u7ade\u4e89\u6027\u56f0\u60d1\u5ea6(4.57 vs 4.32 PPL)\uff0c\u540c\u65f6\u5728\u6301\u7eed\u9002\u5e94\u65b9\u9762\u8868\u73b0\u663e\u8457\u66f4\u597d\uff1a\u9002\u5e94\u65b0\u9886\u57df\u65f6\uff0cAPN\u5728\u539f\u59cb\u9886\u57df\u4fdd\u63012.6\u500d\u66f4\u597d(11.1 vs 29.4 PPL)\uff0c\u5728\u65b0\u9886\u57df\u9002\u5e942.8\u500d\u66f4\u597d(6.4 vs 17.8 PPL)\u3002", "conclusion": "APN\u4f5c\u4e3a\u67b6\u6784\u539f\u8bed\uff0c\u65e2\u80fd\u4fdd\u6301Transformer\u63a5\u53e3\u517c\u5bb9\u6027\uff0c\u53c8\u80fd\u5b9e\u73b0\u6761\u4ef6\u5316\u8ba1\u7b97\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u6301\u7eed\u5b66\u4e60\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3FFN\u7684\u5bc6\u96c6\u8ba1\u7b97\u548c\u6301\u7eed\u5b66\u4e60\u5e72\u6270\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.07323", "categories": ["gr-qc", "hep-th", "math-ph"], "pdf": "https://arxiv.org/pdf/2602.07323", "abs": "https://arxiv.org/abs/2602.07323", "authors": ["M. A. Est\u00e9vez", "E. Sadurn\u00ed"], "title": "The effects of boundary conditions on Rindler's spectral anomaly", "comment": "21 pages, 9 figures", "summary": "Rindler's metric is an interesting way to incorporate a set of uniformly accelerated observers into space-time coordinates; this is consistent with special and general relativity. It is known that such an acceleration gives rise to the famous Unruh effect. Interestingly, its Galilean limit already shows the appearance of quantized modes for particles in free space, given by Airy functions. This happens when a wall or boundary condition is moving in an accelerated trajectory in free space and in the presence of a field. Here we show that such a boundary, when viewed as a material obstacle in motion, gives rise to quantized modes for the Klein-Gordon and Maxwell fields, as long as the boundary does not touch the singularity at the Rindler wedge. This corresponds to a quantum-mechanical problem with an anomalous fall-to-the-origin potential $-1/x^2$ supplemented with a Dirichlet condition. We provide further mathematical analysis regarding the completeness of the solutions in terms of Hankel functions $H^{(1)}$ of imaginary index and argument, and clarify the nature of the corresponding Sobolev spaces when the boundary condition disappears for the accelerated observer. A detailed interpretation of the transition amplitudes is given in connection with particle production obtained from a Bogoliubov transformation.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86Rindler\u65f6\u7a7a\u4e2d\u52a0\u901f\u8fd0\u52a8\u7684\u8fb9\u754c\u6761\u4ef6\u5982\u4f55\u5bfc\u81f4Klein-Gordon\u573a\u548cMaxwell\u573a\u7684\u91cf\u5b50\u5316\u6a21\u5f0f\uff0c\u8fd9\u4e0eUnruh\u6548\u5e94\u76f8\u5173\uff0c\u5e76\u5206\u6790\u4e86\u76f8\u5e94\u7684\u6570\u5b66\u7ed3\u6784\u548c\u7c92\u5b50\u4ea7\u751f\u673a\u5236\u3002", "motivation": "\u7814\u7a76Rindler\u65f6\u7a7a\u4e2d\u52a0\u901f\u8fd0\u52a8\u7684\u8fb9\u754c\u6761\u4ef6\u5982\u4f55\u5f71\u54cd\u91cf\u5b50\u573a\u7684\u91cf\u5b50\u5316\u6a21\u5f0f\uff0c\u63a2\u7d22Unruh\u6548\u5e94\u5728\u5b58\u5728\u8fd0\u52a8\u8fb9\u754c\u65f6\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u76f8\u5e94\u7684\u6570\u5b66\u7ed3\u6784\u548c\u7269\u7406\u610f\u4e49\u3002", "method": "\u4f7f\u7528Rindler\u5ea6\u89c4\u63cf\u8ff0\u52a0\u901f\u89c2\u6d4b\u8005\uff0c\u5206\u6790\u52a0\u901f\u8fd0\u52a8\u8fb9\u754c\u6761\u4ef6\u4e0b\u7684Klein-Gordon\u573a\u548cMaxwell\u573a\uff0c\u901a\u8fc7Hankel\u51fd\u6570\u7b49\u7279\u6b8a\u51fd\u6570\u6c42\u89e3\u91cf\u5b50\u5316\u6a21\u5f0f\uff0c\u5e76\u5229\u7528Bogoliubov\u53d8\u6362\u7814\u7a76\u7c92\u5b50\u4ea7\u751f\u673a\u5236\u3002", "result": "\u53d1\u73b0\u52a0\u901f\u8fd0\u52a8\u7684\u8fb9\u754c\u6761\u4ef6\u4f1a\u5bfc\u81f4\u91cf\u5b50\u573a\u7684\u91cf\u5b50\u5316\u6a21\u5f0f\uff0c\u53ea\u8981\u8fb9\u754c\u4e0d\u63a5\u89e6Rindler\u6954\u5f62\u4e2d\u7684\u5947\u70b9\u3002\u8fd9\u5bf9\u5e94\u4e8e\u5177\u6709\u53cd\u5e38\u52bf\u80fd-1/x\u00b2\u548cDirichlet\u8fb9\u754c\u6761\u4ef6\u7684\u91cf\u5b50\u529b\u5b66\u95ee\u9898\u3002\u63d0\u4f9b\u4e86\u5173\u4e8eHankel\u51fd\u6570\u89e3\u5b8c\u5907\u6027\u7684\u6570\u5b66\u5206\u6790\uff0c\u5e76\u9610\u660e\u4e86\u76f8\u5e94\u7684Sobolev\u7a7a\u95f4\u6027\u8d28\u3002", "conclusion": "Rindler\u65f6\u7a7a\u4e2d\u52a0\u901f\u8fd0\u52a8\u7684\u8fb9\u754c\u6761\u4ef6\u80fd\u591f\u4ea7\u751f\u91cf\u5b50\u573a\u7684\u91cf\u5b50\u5316\u6a21\u5f0f\uff0c\u8fd9\u4e0eUnruh\u6548\u5e94\u5bc6\u5207\u76f8\u5173\u3002\u901a\u8fc7\u8be6\u7ec6\u7684\u6570\u5b66\u5206\u6790\u548cBogoliubov\u53d8\u6362\uff0c\u9610\u660e\u4e86\u8fd9\u79cd\u8fb9\u754c\u6761\u4ef6\u5bfc\u81f4\u7684\u7c92\u5b50\u4ea7\u751f\u673a\u5236\u548c\u76f8\u5e94\u7684\u7269\u7406\u610f\u4e49\u3002"}}
{"id": "2602.06987", "categories": ["physics.comp-ph", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2602.06987", "abs": "https://arxiv.org/abs/2602.06987", "authors": ["Andrew Yang", "Christopher L. Farrow", "Pavol Juh\u00e1s", "Luis Kitsu Iglesias", "Chia-Hao Liu", "Samuel D. Marks", "Vivian R. K. Wall", "Joshua Safin", "Sean M. Drewry", "Caden Myers", "Dillon F. Hanlon", "Nicholas Leonard", "Cedomir Petrovic", "Ahhyun Jeong", "Dmitri V. Talapin", "Linda F. Nazar", "Haidong Zhou", "Samuel W. Teitelbaum", "Tim B. van Driel", "Soham Banerjee", "Emil S. Bozin", "Michael F. Toney", "Katharine Page", "Naomi S. Ginsberg", "Simon J. L. Billinge"], "title": "diffpy.morph: Python tools for model independent comparisons between sets of 1D functions", "comment": "24 pages, 18 figures", "summary": "diffpy.morph addresses a need to gain scientific insights from 1D scientific spectra in model independent ways. A powerful approach for this is to take differences between pairs of spectra and look for meaningful changes that might indicate underlying chemical, structural, or other modifications. The challenge is that the difference curve may contain uninteresting differences such as experimental inconsistencies and benign physical changes such as the effects of thermal expansion. diffpy.morph allows researchers to apply simple transformations, or \"morphs\", to one of the datasets to remove the unwanted differences revealing, when they are present, non-trivial differences. diffpy.morph is an open-source Python package available on the Python Package Index and conda-forge. Here, we describe its functionality and apply it to solve a range of experimental challenges on diffraction and PDF data from x-rays and neutrons, though we note that it may be applied to any 1D function in principle.", "AI": {"tldr": "diffpy.morph \u662f\u4e00\u4e2a\u7528\u4e8e\u5206\u6790\u4e00\u7ef4\u79d1\u5b66\u8c31\u56fe\u7684Python\u5305\uff0c\u901a\u8fc7\u5e94\u7528\u53d8\u6362\u6765\u6d88\u9664\u4e0d\u76f8\u5173\u7684\u5dee\u5f02\uff0c\u63ed\u793a\u6709\u610f\u4e49\u7684\u79d1\u5b66\u53d8\u5316\u3002", "motivation": "\u5728\u5206\u6790\u4e00\u7ef4\u79d1\u5b66\u8c31\u56fe\u65f6\uff0c\u7814\u7a76\u4eba\u5458\u9700\u8981\u4ece\u8c31\u56fe\u5dee\u5f02\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u7684\u79d1\u5b66\u4fe1\u606f\uff0c\u4f46\u5dee\u5f02\u66f2\u7ebf\u4e2d\u5e38\u5305\u542b\u5b9e\u9a8c\u4e0d\u4e00\u81f4\u6027\u6216\u826f\u6027\u7269\u7406\u53d8\u5316\u7b49\u4e0d\u76f8\u5173\u5dee\u5f02\uff0c\u8fd9\u4e9b\u5e72\u6270\u4f7f\u5f97\u96be\u4ee5\u8bc6\u522b\u771f\u6b63\u7684\u5316\u5b66\u3001\u7ed3\u6784\u6216\u5176\u4ed6\u91cd\u8981\u53d8\u5316\u3002", "method": "\u5f00\u53d1\u4e86diffpy.morph\u5f00\u6e90Python\u5305\uff0c\u5141\u8bb8\u7814\u7a76\u4eba\u5458\u5bf9\u6570\u636e\u96c6\u5e94\u7528\u7b80\u5355\u7684\u53d8\u6362\uff08\u79f0\u4e3a\"morphs\"\uff09\uff0c\u4ee5\u6d88\u9664\u4e0d\u60f3\u8981\u7684\u5dee\u5f02\uff0c\u4ece\u800c\u63ed\u793a\u975e\u5e73\u51e1\u7684\u53d8\u5316\u3002\u8be5\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u4efb\u4f55\u4e00\u7ef4\u51fd\u6570\uff0c\u7279\u522b\u9488\u5bf9\u884d\u5c04\u548cPDF\u6570\u636e\u3002", "result": "diffpy.morph\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u89e3\u51b3X\u5c04\u7ebf\u548c\u4e2d\u5b50\u884d\u5c04\u53caPDF\u6570\u636e\u4e2d\u7684\u4e00\u7cfb\u5217\u5b9e\u9a8c\u6311\u6218\uff0c\u80fd\u591f\u6709\u6548\u5206\u79bb\u4e0d\u76f8\u5173\u5dee\u5f02\u5e76\u63ed\u793a\u6709\u610f\u4e49\u7684\u79d1\u5b66\u53d8\u5316\u3002", "conclusion": "diffpy.morph\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u901a\u8fc7\u6d88\u9664\u4e0d\u76f8\u5173\u5dee\u5f02\u6765\u589e\u5f3a\u4e00\u7ef4\u79d1\u5b66\u8c31\u56fe\u7684\u5206\u6790\u80fd\u529b\uff0c\u6709\u52a9\u4e8e\u4ece\u5b9e\u9a8c\u6570\u636e\u4e2d\u63d0\u53d6\u66f4\u6df1\u523b\u7684\u79d1\u5b66\u89c1\u89e3\u3002"}}
{"id": "2602.07110", "categories": ["quant-ph", "cond-mat.str-el", "hep-ph", "hep-th", "math.QA"], "pdf": "https://arxiv.org/pdf/2602.07110", "abs": "https://arxiv.org/abs/2602.07110", "authors": ["Thomas Bartsch", "Yuhan Gai", "Sakura Schafer-Nameki"], "title": "Beyond Wigner: Non-Invertible Symmetries Preserve Probabilities", "comment": "4 pages + Supplementary Material", "summary": "In recent years, the traditional notion of symmetry in quantum theory was expanded to so-called generalised or categorical symmetries, which, unlike ordinary group symmetries, may be non-invertible. This appears to be at odds with Wigner's theorem, which requires quantum symmetries to be implemented by (anti)unitary -- and hence invertible -- operators in order to preserve probabilities. We resolve this puzzle for (higher) fusion category symmetries $\\mathcal{C}$ by proposing that, instead of acting by unitary operators on a fixed Hilbert space, symmetry defects in $\\mathcal{C}$ act as isometries between distinct Hilbert spaces constructed from twisted sectors. As a result, we find that non-invertible symmetries naturally act as trace-preserving quantum channels. Crucially, our construction relies on the symmetry category $\\mathcal{C}$ being unitary. We illustrate our proposal through several examples that include Tambara-Yamagami, Fibonacci, and Yang-Lee as well as higher categorical symmetries.", "AI": {"tldr": "\u8bba\u6587\u89e3\u51b3\u4e86\u975e\u53ef\u9006\u5bf9\u79f0\u6027\u4e0e\u7ef4\u683c\u7eb3\u5b9a\u7406\u7684\u77db\u76fe\uff0c\u63d0\u51fa\u5bf9\u79f0\u6027\u7f3a\u9677\u4f5c\u4e3a\u4e0d\u540c\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e4b\u95f4\u7684\u7b49\u8ddd\u6620\u5c04\uff0c\u4f7f\u5f97\u975e\u53ef\u9006\u5bf9\u79f0\u6027\u81ea\u7136\u8868\u73b0\u4e3a\u4fdd\u8ff9\u91cf\u5b50\u901a\u9053\u3002", "motivation": "\u4f20\u7edf\u91cf\u5b50\u7406\u8bba\u4e2d\u7684\u5bf9\u79f0\u6027\u88ab\u6269\u5c55\u5230\u975e\u53ef\u9006\u7684\u5e7f\u4e49\u5bf9\u79f0\u6027\uff0c\u8fd9\u4e0e\u7ef4\u683c\u7eb3\u5b9a\u7406\u8981\u6c42\u91cf\u5b50\u5bf9\u79f0\u6027\u5fc5\u987b\u7531\uff08\u53cd\uff09\u5e7a\u6b63\u7b97\u5b50\u5b9e\u73b0\u76f8\u77db\u76fe\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u7406\u8bba\u51b2\u7a81\u3002", "method": "\u63d0\u51fa\u5bf9\u79f0\u6027\u7f3a\u9677\u4e0d\u662f\u4f5c\u7528\u5728\u56fa\u5b9a\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e0a\u7684\u5e7a\u6b63\u7b97\u5b50\uff0c\u800c\u662f\u4f5c\u4e3a\u4e0d\u540c\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff08\u7531\u626d\u66f2\u6247\u533a\u6784\u9020\uff09\u4e4b\u95f4\u7684\u7b49\u8ddd\u6620\u5c04\u3002\u8fd9\u8981\u6c42\u5bf9\u79f0\u6027\u8303\u7574\u5fc5\u987b\u662f\u5e7a\u6b63\u7684\u3002", "result": "\u975e\u53ef\u9006\u5bf9\u79f0\u6027\u81ea\u7136\u5730\u8868\u73b0\u4e3a\u4fdd\u8ff9\u91cf\u5b50\u901a\u9053\uff0c\u89e3\u51b3\u4e86\u4e0e\u7ef4\u683c\u7eb3\u5b9a\u7406\u7684\u77db\u76fe\u3002\u901a\u8fc7Tambara-Yamagami\u3001Fibonacci\u3001Yang-Lee\u4ee5\u53ca\u9ad8\u9636\u8303\u7574\u5bf9\u79f0\u6027\u7b49\u4f8b\u5b50\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5bf9\u79f0\u6027\u7f3a\u9677\u91cd\u65b0\u89e3\u91ca\u4e3a\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e4b\u95f4\u7684\u7b49\u8ddd\u6620\u5c04\uff0c\u6210\u529f\u8c03\u548c\u4e86\u975e\u53ef\u9006\u5bf9\u79f0\u6027\u4e0e\u7ef4\u683c\u7eb3\u5b9a\u7406\u7684\u77db\u76fe\uff0c\u4e3a\u5e7f\u4e49\u5bf9\u79f0\u6027\u7406\u8bba\u63d0\u4f9b\u4e86\u81ea\u6d3d\u7684\u6570\u5b66\u6846\u67b6\u3002"}}
{"id": "2602.07030", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07030", "abs": "https://arxiv.org/abs/2602.07030", "authors": ["Young Jin Ahn", "Yiyang Du", "Zheyuan Zhang", "Haisen Kang"], "title": "Neural Sabermetrics with World Model: Play-by-play Predictive Modeling with Large Language Model", "comment": null, "summary": "Classical sabermetrics has profoundly shaped baseball analytics by summarizing long histories of play into compact statistics. While these metrics are invaluable for valuation and retrospective analysis, they do not define a generative model of how baseball games unfold pitch by pitch, leaving most existing approaches limited to single-step prediction or post-hoc analysis. In this work, we present Neural Sabermetrics with World Model, a Large Language Model (LLM) based play-by-play world model for baseball. We cast baseball games as long auto-regressive sequences of events and continuously pretrain a single LLM on more than ten years of Major League Baseball (MLB) tracking data, comprising over seven million pitch sequences and approximately three billion tokens. The resulting model is capable of predicting multiple aspects of game evolution within a unified framework. We evaluate our model on both in-distribution regular-season data and out-of-distribution postseason games and compare against strong neural baselines from prior work. Despite using a single backbone model, our approach outperforms the performance of existing baselines, (1) correctly predicting approximately 64% of next pitches within a plate appearance and (2) 78% of batter swing decisions, suggesting that LLMs can serve as effective world models for sports.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86Neural Sabermetrics with World Model\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u68d2\u7403\u9010\u7403\u4e16\u754c\u6a21\u578b\uff0c\u80fd\u591f\u9884\u6d4b\u6bd4\u8d5b\u7684\u591a\u65b9\u9762\u6f14\u5316\uff0c\u5728\u9884\u6d4b\u4e0b\u4e00\u7403\u548c\u51fb\u7403\u624b\u6325\u68d2\u51b3\u7b56\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u4f20\u7edf\u68d2\u7403\u7edf\u8ba1\u65b9\u6cd5\u867d\u7136\u80fd\u603b\u7ed3\u5386\u53f2\u6570\u636e\uff0c\u4f46\u65e0\u6cd5\u5efa\u7acb\u9010\u7403\u751f\u6210\u6a21\u578b\uff0c\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u5c40\u9650\u4e8e\u5355\u6b65\u9884\u6d4b\u6216\u4e8b\u540e\u5206\u6790\uff0c\u7f3a\u4e4f\u5bf9\u6bd4\u8d5b\u52a8\u6001\u6f14\u5316\u7684\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u5c06\u68d2\u7403\u6bd4\u8d5b\u5efa\u6a21\u4e3a\u957f\u81ea\u56de\u5f52\u4e8b\u4ef6\u5e8f\u5217\uff0c\u5728\u8d85\u8fc710\u5e74MLB\u8ffd\u8e2a\u6570\u636e\uff08700\u4e07\u6b21\u6295\u7403\u5e8f\u5217\uff0c\u7ea630\u4ebftoken\uff09\u4e0a\u6301\u7eed\u9884\u8bad\u7ec3\u5355\u4e00LLM\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u9010\u7403\u4e16\u754c\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u5206\u5e03\u5185\u5e38\u89c4\u8d5b\u548c\u5206\u5e03\u5916\u5b63\u540e\u8d5b\u6570\u636e\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff1a\u6b63\u786e\u9884\u6d4b\u7ea664%\u7684\u4e0b\u4e00\u6295\u7403\uff0c78%\u7684\u51fb\u7403\u624b\u6325\u68d2\u51b3\u7b56\uff0c\u4f18\u4e8e\u73b0\u6709\u795e\u7ecf\u57fa\u7ebf\u3002", "conclusion": "LLM\u53ef\u4ee5\u4f5c\u4e3a\u6709\u6548\u7684\u4f53\u80b2\u4e16\u754c\u6a21\u578b\uff0c\u7edf\u4e00\u6846\u67b6\u80fd\u591f\u9884\u6d4b\u6bd4\u8d5b\u591a\u65b9\u9762\u6f14\u5316\uff0c\u4e3a\u68d2\u7403\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u751f\u6210\u5efa\u6a21\u65b9\u6cd5\u3002"}}
{"id": "2602.07430", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2602.07430", "abs": "https://arxiv.org/abs/2602.07430", "authors": ["Jia-Xi Feng", "Jia-Yuan Fang", "Xian Gao"], "title": "Circularly polarized gravitational waves from parity-violating scalar-tensor theory", "comment": "34 pages, 6figures", "summary": "We study both primordial GWs and scalar-induced gravitational waves (SIGWs) in a class of the parity-violating scalar-tensor (PVST) theory, of which the Lagrangian is the linear combination of seven ghost-free parity-violating scalar-tensor monomials dubbed the ``Qi-Xiu'' Lagrangians. At linear order, we obtain the quadratic action for tensor perturbations and show that parity-violating terms associated with $\\mathcal{L}_{1,2,5,6,7}$ render the tensor propagation polarization dependent, leading to chiral primordial spectra and a nonvanishing degree of circular polarization. At second order, we derive the EOM for SIGWs and identify the explicit parity-violating source terms. In particular, $\\mathcal{L}_3$ and $\\mathcal{L}_4$ enter exclusively through the source term for SIGWs, allowing parity violation to arise even when the linear GWs propagation remains effectively GR-like. During the radiation-dominated era, we compute the fractional energy density of SIGWs for both monochromatic and lognormal curvature power spectra. We find that, around the peak frequency, SIGWs in PVST gravity exhibit characteristic deviations from those in GR, resulting in a nonzero degree of circular polarization.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5728\u5b87\u79f0\u7834\u7f3a\u6807\u91cf-\u5f20\u91cf\u7406\u8bba\u4e2d\u5206\u6790\u4e86\u539f\u521d\u5f15\u529b\u6ce2\u548c\u6807\u91cf\u8bf1\u5bfc\u5f15\u529b\u6ce2\uff0c\u53d1\u73b0\u5b87\u79f0\u7834\u7f3a\u9879\u5bfc\u81f4\u5f20\u91cf\u4f20\u64ad\u504f\u632f\u4f9d\u8d56\uff0c\u4ea7\u751f\u624b\u6027\u539f\u521d\u8c31\u548c\u975e\u96f6\u5706\u504f\u632f\u5ea6\uff0cSIGWs\u5728\u8f90\u5c04\u4e3b\u5bfc\u65f6\u671f\u8868\u73b0\u51fa\u4e0e\u5e7f\u4e49\u76f8\u5bf9\u8bba\u4e0d\u540c\u7684\u7279\u5f81\u6027\u504f\u5dee\u3002", "motivation": "\u7814\u7a76\u5b87\u79f0\u7834\u7f3a\u6807\u91cf-\u5f20\u91cf\u7406\u8bba\u4e2d\u7684\u5f15\u529b\u6ce2\u7279\u6027\uff0c\u63a2\u7d22\u5b87\u79f0\u7834\u7f3a\u5982\u4f55\u5f71\u54cd\u539f\u521d\u5f15\u529b\u6ce2\u548c\u6807\u91cf\u8bf1\u5bfc\u5f15\u529b\u6ce2\u7684\u4f20\u64ad\u548c\u504f\u632f\u7279\u6027\uff0c\u7279\u522b\u662f\u7406\u89e3\u4e0d\u540c\u5b87\u79f0\u7834\u7f3a\u9879\u5728\u7ebf\u6027\u548c\u4e8c\u9636\u6270\u52a8\u4e2d\u7684\u4f5c\u7528\u673a\u5236\u3002", "method": "\u5728\"Qi-Xiu\"\u62c9\u683c\u6717\u65e5\u63cf\u8ff0\u7684\u5b87\u79f0\u7834\u7f3a\u6807\u91cf-\u5f20\u91cf\u7406\u8bba\u6846\u67b6\u4e0b\uff0c\u63a8\u5bfc\u4e86\u5f20\u91cf\u6270\u52a8\u7684\u4e8c\u6b21\u4f5c\u7528\u91cf\uff0c\u5206\u6790\u4e86\u7ebf\u6027\u9636\u548c\u4e8c\u9636\u6270\u52a8\u65b9\u7a0b\u3002\u5728\u7ebf\u6027\u9636\u7814\u7a76\u5f20\u91cf\u4f20\u64ad\u7684\u504f\u632f\u4f9d\u8d56\u6027\uff0c\u5728\u4e8c\u9636\u63a8\u5bfc\u6807\u91cf\u8bf1\u5bfc\u5f15\u529b\u6ce2\u7684\u8fd0\u52a8\u65b9\u7a0b\u5e76\u8bc6\u522b\u5b87\u79f0\u7834\u7f3a\u6e90\u9879\u3002\u5728\u8f90\u5c04\u4e3b\u5bfc\u65f6\u671f\u8ba1\u7b97\u4e86\u5355\u8272\u548c\u5bf9\u6570\u6b63\u6001\u66f2\u7387\u529f\u7387\u8c31\u4e0bSIGWs\u7684\u5206\u6570\u80fd\u91cf\u5bc6\u5ea6\u3002", "result": "\u53d1\u73b0\u5b87\u79f0\u7834\u7f3a\u9879\u2112\u2081,\u2082,\u2085,\u2086,\u2087\u4f7f\u5f20\u91cf\u4f20\u64ad\u504f\u632f\u4f9d\u8d56\uff0c\u5bfc\u81f4\u624b\u6027\u539f\u521d\u8c31\u548c\u975e\u96f6\u5706\u504f\u632f\u5ea6\u3002\u2112\u2083\u548c\u2112\u2084\u4e13\u95e8\u901a\u8fc7SIGWs\u7684\u6e90\u9879\u8fdb\u5165\uff0c\u5373\u4f7f\u7ebf\u6027\u5f15\u529b\u6ce2\u4f20\u64ad\u4fdd\u6301\u5e7f\u4e49\u76f8\u5bf9\u8bba\u7279\u5f81\u65f6\u4e5f\u80fd\u4ea7\u751f\u5b87\u79f0\u7834\u7f3a\u3002\u5728\u5cf0\u503c\u9891\u7387\u9644\u8fd1\uff0cPVST\u5f15\u529b\u4e2d\u7684SIGWs\u8868\u73b0\u51fa\u4e0e\u5e7f\u4e49\u76f8\u5bf9\u8bba\u4e0d\u540c\u7684\u7279\u5f81\u6027\u504f\u5dee\uff0c\u5e76\u4ea7\u751f\u975e\u96f6\u5706\u504f\u632f\u5ea6\u3002", "conclusion": "\u5b87\u79f0\u7834\u7f3a\u6807\u91cf-\u5f20\u91cf\u7406\u8bba\u4e3a\u5f15\u529b\u6ce2\u7814\u7a76\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u65b0\u7269\u7406\u73b0\u8c61\uff0c\u5305\u62ec\u624b\u6027\u539f\u521d\u8c31\u3001\u975e\u96f6\u5706\u504f\u632f\u5ea6\u4ee5\u53caSIGWs\u7684\u7279\u5f81\u6027\u504f\u5dee\uff0c\u8fd9\u4e9b\u7279\u5f81\u53ef\u4f5c\u4e3a\u533a\u5206PVST\u5f15\u529b\u548c\u5e7f\u4e49\u76f8\u5bf9\u8bba\u7684\u91cd\u8981\u89c2\u6d4b\u4fe1\u53f7\u3002"}}
{"id": "2602.07199", "categories": ["physics.comp-ph", "cond-mat.mtrl-sci", "cond-mat.stat-mech", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2602.07199", "abs": "https://arxiv.org/abs/2602.07199", "authors": ["E. A. J. F. Peters"], "title": "Event-Chain Monte Carlo: The global-balance breakthrough", "comment": "18 pages", "summary": "The seminal 2009 paper by Bernard, Krauth, and Wilson marked a paradigm shift in Monte Carlo sampling. By abandoning the restrictive condition of detailed balance in favor of the more fundamental principle of global balance, they introduced the Event-Chain Monte Carlo (ECMC) algorithm, which achieves rejection-free, deterministic sampling for hard spheres. This breakthrough demonstrated that persistent, directional dynamics could dramatically accelerate equilibration in dense particle systems. In this commentary, we review this foundational work and elucidate its underlying mechanism using the broader Event-Driven Monte Carlo (EDMC) framework developed in subsequent years. We show how the original hard-sphere concept naturally generalizes to continuous potentials and modern lifted Markov chain formalisms, transforming a surprising specific result into a powerful general class of sampling algorithms.", "AI": {"tldr": "2009\u5e74Bernard\u7b49\u4eba\u63d0\u51fa\u7684Event-Chain Monte Carlo\u7b97\u6cd5\u901a\u8fc7\u653e\u5f03\u7ec6\u81f4\u5e73\u8861\u6761\u4ef6\u3001\u91c7\u7528\u5168\u5c40\u5e73\u8861\u539f\u5219\uff0c\u5b9e\u73b0\u4e86\u786c\u7403\u7cfb\u7edf\u7684\u65e0\u62d2\u7edd\u786e\u5b9a\u6027\u91c7\u6837\uff0c\u5f00\u521b\u4e86\u8499\u7279\u5361\u6d1b\u91c7\u6837\u7684\u65b0\u8303\u5f0f\u3002", "motivation": "\u4f20\u7edf\u8499\u7279\u5361\u6d1b\u91c7\u6837\u53d7\u9650\u4e8e\u7ec6\u81f4\u5e73\u8861\u6761\u4ef6\uff0c\u5728\u5bc6\u96c6\u7c92\u5b50\u7cfb\u7edf\u4e2d\u6536\u655b\u7f13\u6162\u3002\u4f5c\u8005\u5bfb\u6c42\u7a81\u7834\u8fd9\u4e00\u9650\u5236\uff0c\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u91c7\u6837\u65b9\u6cd5\u3002", "method": "\u653e\u5f03\u4f20\u7edf\u7684\u7ec6\u81f4\u5e73\u8861\u6761\u4ef6\uff0c\u91c7\u7528\u66f4\u57fa\u672c\u7684\u5168\u5c40\u5e73\u8861\u539f\u5219\uff0c\u63d0\u51faEvent-Chain Monte Carlo\u7b97\u6cd5\uff0c\u5b9e\u73b0\u65e0\u62d2\u7edd\u3001\u786e\u5b9a\u6027\u7684\u6301\u7eed\u5b9a\u5411\u52a8\u529b\u5b66\u91c7\u6837\u3002", "result": "ECMC\u7b97\u6cd5\u663e\u8457\u52a0\u901f\u4e86\u5bc6\u96c6\u7c92\u5b50\u7cfb\u7edf\u7684\u5e73\u8861\u8fc7\u7a0b\uff0c\u540e\u7eed\u53d1\u5c55\u4e3a\u66f4\u5e7f\u6cdb\u7684Event-Driven Monte Carlo\u6846\u67b6\uff0c\u53ef\u63a8\u5e7f\u5230\u8fde\u7eed\u52bf\u80fd\u548c\u73b0\u4ee3\u63d0\u5347\u9a6c\u5c14\u53ef\u592b\u94fe\u5f62\u5f0f\u3002", "conclusion": "ECMC\u7b97\u6cd5\u4ece\u786c\u7403\u7cfb\u7edf\u7684\u7279\u5b9a\u7ed3\u679c\u53d1\u5c55\u4e3a\u5f3a\u5927\u7684\u4e00\u822c\u91c7\u6837\u7b97\u6cd5\u7c7b\u522b\uff0c\u4ee3\u8868\u4e86\u8499\u7279\u5361\u6d1b\u91c7\u6837\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4e3a\u590d\u6742\u7cfb\u7edf\u6a21\u62df\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2602.07112", "categories": ["quant-ph", "hep-th"], "pdf": "https://arxiv.org/pdf/2602.07112", "abs": "https://arxiv.org/abs/2602.07112", "authors": ["Kelly Wurtz", "Caroline Lima", "Robert C. Myers", "Eduardo Mart\u00edn-Mart\u00ednez"], "title": "Entanglement harvesting in conformal field theory", "comment": "59 pages, 17 figures", "summary": "We study entanglement harvesting in general $d$-dimensional conformal field theories using pointlike Unruh-DeWitt detectors coupled to scalar primary operators. This extends standard harvesting protocols beyond free fields to interacting conformal theories and arbitrary spatial dimensions. We find that increasing the operator scaling dimension suppresses both negativity and mutual information, reflecting the faster decay of correlations. For holographic CFTs, we show that bulk effective field theory enables a separation between field-harvested and communication-mediated entanglement. We also derive asymptotic, closed-form approximations that agree well with numerical results.", "AI": {"tldr": "\u7814\u7a76d\u7ef4\u5171\u5f62\u573a\u8bba\u4e2d\u901a\u8fc7\u70b9\u72b6Unruh-DeWitt\u63a2\u6d4b\u5668\u4e0e\u6807\u91cf\u4e3b\u7b97\u7b26\u8026\u5408\u7684\u7ea0\u7f20\u6536\u83b7\uff0c\u6269\u5c55\u4e86\u81ea\u7531\u573a\u7406\u8bba\u5230\u76f8\u4e92\u4f5c\u7528\u5171\u5f62\u7406\u8bba\u548c\u4efb\u610f\u7a7a\u95f4\u7ef4\u5ea6\u3002", "motivation": "\u5c06\u6807\u51c6\u7ea0\u7f20\u6536\u83b7\u534f\u8bae\u6269\u5c55\u5230\u81ea\u7531\u573a\u4e4b\u5916\u7684\u76f8\u4e92\u4f5c\u7528\u5171\u5f62\u573a\u8bba\u548c\u4efb\u610f\u7a7a\u95f4\u7ef4\u5ea6\uff0c\u7814\u7a76\u7b97\u5b50\u6807\u5ea6\u7ef4\u5ea6\u5bf9\u7ea0\u7f20\u6536\u83b7\u7684\u5f71\u54cd\uff0c\u5e76\u5728\u5168\u606fCFT\u4e2d\u533a\u5206\u573a\u6536\u83b7\u7ea0\u7f20\u4e0e\u901a\u4fe1\u4ecb\u5bfc\u7ea0\u7f20\u3002", "method": "\u4f7f\u7528\u70b9\u72b6Unruh-DeWitt\u63a2\u6d4b\u5668\u4e0e\u6807\u91cf\u4e3b\u7b97\u7b26\u8026\u5408\uff0c\u5206\u6790d\u7ef4\u5171\u5f62\u573a\u8bba\u4e2d\u7684\u7ea0\u7f20\u6536\u83b7\u3002\u5bf9\u4e8e\u5168\u606fCFT\uff0c\u5229\u7528\u4f53\u6709\u6548\u573a\u7406\u8bba\u5206\u79bb\u573a\u6536\u83b7\u7ea0\u7f20\u548c\u901a\u4fe1\u4ecb\u5bfc\u7ea0\u7f20\u3002\u63a8\u5bfc\u6e10\u8fd1\u95ed\u5f0f\u8fd1\u4f3c\u5e76\u4e0e\u6570\u503c\u7ed3\u679c\u6bd4\u8f83\u3002", "result": "\u589e\u52a0\u7b97\u5b50\u6807\u5ea6\u7ef4\u5ea6\u4f1a\u6291\u5236\u8d1f\u6027\u548c\u4e92\u4fe1\u606f\uff0c\u53cd\u6620\u5173\u8054\u7684\u66f4\u5feb\u8870\u51cf\u3002\u5728\u5168\u606fCFT\u4e2d\uff0c\u4f53\u6709\u6548\u573a\u7406\u8bba\u80fd\u591f\u5206\u79bb\u573a\u6536\u83b7\u7ea0\u7f20\u548c\u901a\u4fe1\u4ecb\u5bfc\u7ea0\u7f20\u3002\u63a8\u5bfc\u7684\u6e10\u8fd1\u95ed\u5f0f\u8fd1\u4f3c\u4e0e\u6570\u503c\u7ed3\u679c\u543b\u5408\u826f\u597d\u3002", "conclusion": "\u8be5\u7814\u7a76\u6269\u5c55\u4e86\u7ea0\u7f20\u6536\u83b7\u5230\u76f8\u4e92\u4f5c\u7528\u5171\u5f62\u573a\u8bba\uff0c\u63ed\u793a\u4e86\u7b97\u5b50\u6807\u5ea6\u7ef4\u5ea6\u5bf9\u7ea0\u7f20\u6536\u83b7\u7684\u6291\u5236\u4f5c\u7528\uff0c\u5e76\u5728\u5168\u606fCFT\u4e2d\u63d0\u4f9b\u4e86\u533a\u5206\u4e0d\u540c\u7c7b\u578b\u7ea0\u7f20\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4e3a\u7406\u89e3\u91cf\u5b50\u573a\u8bba\u4e2d\u7684\u7ea0\u7f20\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.07031", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.07031", "abs": "https://arxiv.org/abs/2602.07031", "authors": ["Dong Li", "Shuai Huang", "Yapeng Cao", "Yujun Cui", "Xiaobin Wei", "Hongtao Cao"], "title": "Lagged backward-compatible physics-informed neural networks for unsaturated soil consolidation analysis", "comment": null, "summary": "This study develops a Lagged Backward-Compatible Physics-Informed Neural Network (LBC-PINN) for simulating and inverting one-dimensional unsaturated soil consolidation under long-term loading. To address the challenges of coupled air and water pressure dissipation across multi-scale time domains, the framework integrates logarithmic time segmentation, lagged compatibility loss enforcement, and segment-wise transfer learning.\n  In forward analysis, the LBC-PINN with recommended segmentation schemes accurately predicts pore air and pore water pressure evolution. Model predictions are validated against finite element method (FEM) results, with mean absolute errors below 1e-2 for time durations up to 1e10 seconds. A simplified segmentation strategy based on the characteristic air-phase dissipation time improves computational efficiency while preserving predictive accuracy. Sensitivity analyses confirm the robustness of the framework across air-to-water permeability ratios ranging from 1e-3 to 1e3.", "AI": {"tldr": "\u63d0\u51faLBC-PINN\u65b9\u6cd5\uff0c\u7528\u4e8e\u6a21\u62df\u548c\u53cd\u6f14\u957f\u671f\u8377\u8f7d\u4e0b\u4e00\u7ef4\u975e\u9971\u548c\u571f\u56fa\u7ed3\u95ee\u9898\uff0c\u901a\u8fc7\u65f6\u95f4\u5206\u6bb5\u548c\u6ede\u540e\u517c\u5bb9\u6027\u635f\u5931\u89e3\u51b3\u591a\u65f6\u95f4\u5c3a\u5ea6\u8026\u5408\u538b\u529b\u6d88\u6563\u6311\u6218\u3002", "motivation": "\u975e\u9971\u548c\u571f\u56fa\u7ed3\u6d89\u53ca\u7a7a\u6c14\u548c\u6c34\u538b\u529b\u7684\u8026\u5408\u6d88\u6563\uff0c\u5b58\u5728\u591a\u65f6\u95f4\u5c3a\u5ea6\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u957f\u671f\u52a0\u8f7d\u4e0b\u7684\u6a21\u62df\u548c\u53cd\u6f14\u3002", "method": "\u5f00\u53d1Lagged Backward-Compatible Physics-Informed Neural Network (LBC-PINN)\uff0c\u96c6\u6210\u5bf9\u6570\u65f6\u95f4\u5206\u6bb5\u3001\u6ede\u540e\u517c\u5bb9\u6027\u635f\u5931\u5f3a\u5236\u548c\u5206\u6bb5\u8fc1\u79fb\u5b66\u4e60\uff0c\u91c7\u7528\u57fa\u4e8e\u7279\u5f81\u7a7a\u6c14\u76f8\u6d88\u6563\u65f6\u95f4\u7684\u7b80\u5316\u5206\u6bb5\u7b56\u7565\u3002", "result": "LBC-PINN\u80fd\u51c6\u786e\u9884\u6d4b\u5b54\u9699\u7a7a\u6c14\u548c\u5b54\u9699\u6c34\u538b\u529b\u6f14\u5316\uff0c\u4e0e\u6709\u9650\u5143\u7ed3\u679c\u5bf9\u6bd4\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4f4e\u4e8e1e-2\uff08\u65f6\u95f4\u8fbe1e10\u79d2\uff09\uff0c\u5bf9\u6e17\u900f\u7387\u6bd41e-3\u52301e3\u8303\u56f4\u5177\u6709\u9c81\u68d2\u6027\uff0c\u7b80\u5316\u5206\u6bb5\u7b56\u7565\u4fdd\u6301\u7cbe\u5ea6\u540c\u65f6\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "LBC-PINN\u4e3a\u957f\u671f\u8377\u8f7d\u4e0b\u975e\u9971\u548c\u571f\u56fa\u7ed3\u6a21\u62df\u548c\u53cd\u6f14\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u591a\u65f6\u95f4\u5c3a\u5ea6\u8026\u5408\u95ee\u9898\uff0c\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2602.07436", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2602.07436", "abs": "https://arxiv.org/abs/2602.07436", "authors": ["Dan Zhang", "Chao Zhang", "Qiyuan Pan", "Guoyang Fu", "Jian-Pin Wu"], "title": "Probing Quantum Gravity effects with Extreme Mass Ratio Inspirals around Rotating Hayward Black Holes", "comment": null, "summary": "We investigate extreme mass-ratio inspirals (EMRIs) around a rotating Hayward black hole to assess the detectability of signatures arising from quantum gravity.The quantum parameter $\u03b1_0$, which encodes deviations from general relativity (GR), introduces extra correction terms in both the orbital frequency and the fluxes. Our results show that after one year of accumulated observation, these corrections induce a detectable dephasing in the EMRI waveform. Using the modified orbital evolution driven by $\u03b1_0$, we generate waveforms via the augmented analytic kludge (AAK) model implemented in the \\texttt{FastEMRIWaveforms} package. Furthermore, we utilize the time-delay interferometry (TDI) to suppress the laser noise and phase fluctuations induced by spacecraft motion, and then employ the Fisher information matrix (FIM) to test the sensitivity of LISA in detecting deviations from GR. Our results demonstrate the potential of LISA to probe quantum-gravity effects through high-precision observations of EMRIs.", "AI": {"tldr": "\u7814\u7a76\u65cb\u8f6cHayward\u9ed1\u6d1e\u5468\u56f4\u7684\u6781\u7aef\u8d28\u91cf\u6bd4\u65cb\u8fdb\uff0c\u901a\u8fc7LISA\u63a2\u6d4b\u91cf\u5b50\u5f15\u529b\u6548\u5e94\uff0c\u53d1\u73b0\u4e00\u5e74\u89c2\u6d4b\u79ef\u7d2f\u540e\u91cf\u5b50\u53c2\u6570\u03b1\u2080\u5f15\u8d77\u7684\u6ce2\u5f62\u76f8\u79fb\u53ef\u88ab\u63a2\u6d4b", "motivation": "\u8bc4\u4f30\u91cf\u5b50\u5f15\u529b\u6548\u5e94\u5728\u6781\u7aef\u8d28\u91cf\u6bd4\u65cb\u8fdb\u4e2d\u7684\u53ef\u63a2\u6d4b\u6027\uff0c\u5229\u7528LISA\u9ad8\u7cbe\u5ea6\u89c2\u6d4b\u6765\u68c0\u9a8c\u5e7f\u4e49\u76f8\u5bf9\u8bba\u7684\u504f\u79bb", "method": "\u4f7f\u7528\u65cb\u8f6cHayward\u9ed1\u6d1e\u6a21\u578b\uff0c\u5206\u6790\u91cf\u5b50\u53c2\u6570\u03b1\u2080\u5bf9\u8f68\u9053\u9891\u7387\u548c\u901a\u91cf\u7684\u4fee\u6b63\uff1b\u91c7\u7528AAK\u6a21\u578b\u751f\u6210\u6ce2\u5f62\uff0c\u5229\u7528TDI\u6291\u5236\u566a\u58f0\uff0c\u901a\u8fc7Fisher\u4fe1\u606f\u77e9\u9635\u8bc4\u4f30LISA\u63a2\u6d4b\u7075\u654f\u5ea6", "result": "\u91cf\u5b50\u5f15\u529b\u4fee\u6b63\u9879\u5728\u4e00\u5e74\u89c2\u6d4b\u79ef\u7d2f\u540e\u4f1a\u5f15\u8d77\u53ef\u63a2\u6d4b\u7684\u6ce2\u5f62\u76f8\u79fb\uff0cLISA\u5177\u5907\u63a2\u6d4b\u8fd9\u4e9b\u504f\u79bb\u5e7f\u4e49\u76f8\u5bf9\u8bba\u6548\u5e94\u7684\u6f5c\u529b", "conclusion": "LISA\u901a\u8fc7\u9ad8\u7cbe\u5ea6\u89c2\u6d4b\u6781\u7aef\u8d28\u91cf\u6bd4\u65cb\u8fdb\uff0c\u6709\u671b\u63a2\u6d4b\u91cf\u5b50\u5f15\u529b\u6548\u5e94\uff0c\u4e3a\u68c0\u9a8c\u5e7f\u4e49\u76f8\u5bf9\u8bba\u63d0\u4f9b\u65b0\u9014\u5f84"}}
{"id": "2602.07771", "categories": ["physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.07771", "abs": "https://arxiv.org/abs/2602.07771", "authors": ["Ethan Lame", "Camille Palmer", "Todd Palmer", "Ilham Variansyah"], "title": "Compressed Sensing Methods for Memory Reduction in Monte Carlo Simulations", "comment": "15 figures", "summary": "Monte Carlo simulations of neutronic systems are computationally intensive and demand significant memory resources for high-fidelity modeling. Compressed sensing enables accurate reconstruction of signals from significantly fewer samples than traditional methods. The specific implementation of compressed sensing investigated here involves the use of overlapping cells to collect tallies. Increasing the number of samples improves the reconstruction accuracy, although the marginal gains diminish with more samples. Reconstruction quality is strongly influenced by the sparsity parameter used in basis pursuit denoising. Across the three test cases considered, memory reductions of up to 81.25% (96.25%) are demonstrated for 2D (3D) reconstructions, with select scenarios achieving reconstruction errors within 1 standard deviation of the corresponding high-fidelity reference results.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5728\u8499\u7279\u5361\u7f57\u4e2d\u5b50\u8f93\u8fd0\u6a21\u62df\u4e2d\u5e94\u7528\u538b\u7f29\u611f\u77e5\u6280\u672f\uff0c\u901a\u8fc7\u91cd\u53e0\u5355\u5143\u6536\u96c6\u8ba1\u6570\u6570\u636e\uff0c\u5b9e\u73b0\u663e\u8457\u7684\u5185\u5b58\u51cf\u5c11\uff082D\u8fbe81.25%\uff0c3D\u8fbe96.25%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u91cd\u5efa\u8bef\u5dee\u5728\u53c2\u8003\u7ed3\u679c1\u4e2a\u6807\u51c6\u5dee\u5185\u3002", "motivation": "\u8499\u7279\u5361\u7f57\u4e2d\u5b50\u8f93\u8fd0\u6a21\u62df\u8ba1\u7b97\u5bc6\u96c6\u4e14\u5185\u5b58\u9700\u6c42\u9ad8\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6837\u672c\u3002\u538b\u7f29\u611f\u77e5\u6280\u672f\u80fd\u591f\u4ece\u8f83\u5c11\u6837\u672c\u4e2d\u51c6\u786e\u91cd\u5efa\u4fe1\u53f7\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e00\u8ba1\u7b97\u6311\u6218\u63d0\u4f9b\u53ef\u80fd\u3002", "method": "\u91c7\u7528\u538b\u7f29\u611f\u77e5\u6280\u672f\uff0c\u901a\u8fc7\u91cd\u53e0\u5355\u5143\u6536\u96c6\u8ba1\u6570\u6570\u636e\uff0c\u4f7f\u7528\u57fa\u8ffd\u8e2a\u53bb\u566a\u7b97\u6cd5\u8fdb\u884c\u4fe1\u53f7\u91cd\u5efa\u3002\u7814\u7a76\u6837\u672c\u6570\u91cf\u5bf9\u91cd\u5efa\u7cbe\u5ea6\u7684\u5f71\u54cd\uff0c\u5e76\u5206\u6790\u7a00\u758f\u53c2\u6570\u5bf9\u91cd\u5efa\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "result": "\u5728\u4e09\u4e2a\u6d4b\u8bd5\u6848\u4f8b\u4e2d\uff0c\u5b9e\u73b0\u4e862D\u91cd\u5efa81.25%\u548c3D\u91cd\u5efa96.25%\u7684\u5185\u5b58\u51cf\u5c11\u3002\u90e8\u5206\u573a\u666f\u7684\u91cd\u5efa\u8bef\u5dee\u5728\u9ad8\u4fdd\u771f\u53c2\u8003\u7ed3\u679c\u76841\u4e2a\u6807\u51c6\u5dee\u8303\u56f4\u5185\u3002\u589e\u52a0\u6837\u672c\u6570\u91cf\u80fd\u63d0\u9ad8\u91cd\u5efa\u7cbe\u5ea6\uff0c\u4f46\u8fb9\u9645\u6536\u76ca\u9012\u51cf\u3002", "conclusion": "\u538b\u7f29\u611f\u77e5\u6280\u672f\u80fd\u663e\u8457\u51cf\u5c11\u8499\u7279\u5361\u7f57\u4e2d\u5b50\u6a21\u62df\u7684\u5185\u5b58\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u63a5\u53d7\u7684\u7cbe\u5ea6\u3002\u7a00\u758f\u53c2\u6570\u9009\u62e9\u5bf9\u91cd\u5efa\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u4fdd\u771f\u4e2d\u5b50\u8f93\u8fd0\u6a21\u62df\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8ba1\u7b97\u65b9\u6848\u3002"}}
{"id": "2602.07123", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.07123", "abs": "https://arxiv.org/abs/2602.07123", "authors": ["William M. Watkins", "Leigh Norris", "Paraj Titum"], "title": "Performance limits of a quantum receiver for detecting phase-modulated communication signals", "comment": null, "summary": "Quantum sensors are an ideal candidate for detecting weak electromagnetic signals because of their exceptional sensitivity and compact form factor. In this work, we analyze the performance of a quantum-sensor-based receive chain for demodulating information encoded in phase-modulated electromagnetic waves. We introduce a generalized cumulant expansion to model a noisy quantum receiver and use it to compare the performance of various quantum demodulation protocols. Employing bit error probability (BEP) and channel capacity as quantitative performance metrics, we compare the capabilities of ensembles of quantum sensors - both unentangled and entangled - using Binary Phase-Shift Keying (BPSK) as a representative example of phase modulation. We identify conditions when the channel capacity of an ensemble of quantum sensors may surpass the limits of a classical electrically small antenna. Additionally, we discuss modifications to the quantum protocol that enables high-fidelity data recovery even in the presence of sensor noise and channel distortions. Finally, we explore practical performance limits of such a quantum receive chain, with a focus on NV-diamond as the quantum sensor platform.", "AI": {"tldr": "\u91cf\u5b50\u4f20\u611f\u5668\u7528\u4e8e\u89e3\u8c03\u76f8\u4f4d\u8c03\u5236\u7535\u78c1\u6ce2\u4fe1\u606f\uff0c\u901a\u8fc7\u5e7f\u4e49\u7d2f\u79ef\u5c55\u5f00\u5efa\u6a21\u566a\u58f0\u91cf\u5b50\u63a5\u6536\u673a\uff0c\u6bd4\u8f83\u4e0d\u540c\u91cf\u5b50\u89e3\u8c03\u534f\u8bae\u6027\u80fd\uff0c\u53d1\u73b0\u7ea0\u7f20\u91cf\u5b50\u4f20\u611f\u5668\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u53ef\u8d85\u8d8a\u7ecf\u5178\u7535\u5c0f\u5929\u7ebf\u7684\u4fe1\u9053\u5bb9\u91cf\u9650\u5236\u3002", "motivation": "\u91cf\u5b50\u4f20\u611f\u5668\u5177\u6709\u5353\u8d8a\u7684\u7075\u654f\u5ea6\u548c\u7d27\u51d1\u5c3a\u5bf8\uff0c\u662f\u68c0\u6d4b\u5fae\u5f31\u7535\u78c1\u4fe1\u53f7\u7684\u7406\u60f3\u9009\u62e9\u3002\u672c\u6587\u65e8\u5728\u5206\u6790\u57fa\u4e8e\u91cf\u5b50\u4f20\u611f\u5668\u7684\u63a5\u6536\u94fe\u5728\u89e3\u8c03\u76f8\u4f4d\u8c03\u5236\u7535\u78c1\u6ce2\u4fe1\u606f\u65f6\u7684\u6027\u80fd\uff0c\u63a2\u7d22\u91cf\u5b50\u4f20\u611f\u5668\u5728\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u5f15\u5165\u5e7f\u4e49\u7d2f\u79ef\u5c55\u5f00\u6765\u5efa\u6a21\u566a\u58f0\u91cf\u5b50\u63a5\u6536\u673a\uff0c\u4f7f\u7528\u6bd4\u7279\u9519\u8bef\u6982\u7387\uff08BEP\uff09\u548c\u4fe1\u9053\u5bb9\u91cf\u4f5c\u4e3a\u6027\u80fd\u6307\u6807\uff0c\u6bd4\u8f83\u975e\u7ea0\u7f20\u548c\u7ea0\u7f20\u91cf\u5b50\u4f20\u611f\u5668\u96c6\u5408\u7684\u6027\u80fd\uff0c\u4ee5\u4e8c\u8fdb\u5236\u76f8\u79fb\u952e\u63a7\uff08BPSK\uff09\u4f5c\u4e3a\u76f8\u4f4d\u8c03\u5236\u7684\u4ee3\u8868\u6027\u793a\u4f8b\u3002", "result": "\u8bc6\u522b\u51fa\u91cf\u5b50\u4f20\u611f\u5668\u96c6\u5408\u7684\u4fe1\u9053\u5bb9\u91cf\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u53ef\u80fd\u8d85\u8d8a\u7ecf\u5178\u7535\u5c0f\u5929\u7ebf\u7684\u6781\u9650\u3002\u8ba8\u8bba\u4e86\u91cf\u5b50\u534f\u8bae\u7684\u4fee\u6539\u65b9\u6848\uff0c\u4f7f\u5176\u5728\u4f20\u611f\u5668\u566a\u58f0\u548c\u4fe1\u9053\u5931\u771f\u60c5\u51b5\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u9ad8\u4fdd\u771f\u6570\u636e\u6062\u590d\u3002\u63a2\u7d22\u4e86\u4ee5NV-\u91d1\u521a\u77f3\u4e3a\u91cf\u5b50\u4f20\u611f\u5668\u5e73\u53f0\u7684\u5b9e\u7528\u6027\u80fd\u6781\u9650\u3002", "conclusion": "\u91cf\u5b50\u4f20\u611f\u5668\u5728\u89e3\u8c03\u76f8\u4f4d\u8c03\u5236\u7535\u78c1\u6ce2\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u53ef\u8d85\u8d8a\u7ecf\u5178\u7535\u5c0f\u5929\u7ebf\u7684\u6027\u80fd\u9650\u5236\u3002\u901a\u8fc7\u534f\u8bae\u4f18\u5316\uff0c\u91cf\u5b50\u63a5\u6536\u94fe\u80fd\u591f\u5728\u566a\u58f0\u548c\u5931\u771f\u73af\u5883\u4e0b\u5b9e\u73b0\u9ad8\u4fdd\u771f\u6570\u636e\u6062\u590d\uff0c\u4e3a\u91cf\u5b50\u4f20\u611f\u5668\u5728\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.07033", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07033", "abs": "https://arxiv.org/abs/2602.07033", "authors": ["Md Shahriar Kabir", "Sana Alamgeer", "Minakshi Debnath", "Anne H. H. Ngu"], "title": "TransConv-DDPM: Enhanced Diffusion Model for Generating Time-Series Data in Healthcare", "comment": "Previously published at IEEE COMPSAC 2025", "summary": "The lack of real-world data in clinical fields poses a major obstacle in training effective AI models for diagnostic and preventive tools in medicine. Generative AI has shown promise in increasing data volume and enhancing model training, particularly in computer vision and natural language processing (NLP) domains. However, generating physiological time-series data, a common type in medical AI applications, presents unique challenges due to its inherent complexity and variability. This paper introduces TransConv-DDPM, an enhanced generative AI method for biomechanical and physiological time-series data generation. The model employs a denoising diffusion probabilistic model (DDPM) with U-Net, multi-scale convolution modules, and a transformer layer to capture both global and local temporal dependencies. We evaluated TransConv-DDPM on three diverse datasets, generating both long and short-sequence time-series data. Quantitative comparisons against state-of-the-art methods, TimeGAN and Diffusion-TS, using four performance metrics, demonstrated promising results, particularly on the SmartFallMM and EEG datasets, where it effectively captured the more gradual temporal change patterns between data points. Additionally, a utility test on the SmartFallMM dataset revealed that adding synthetic fall data generated by TransConv-DDPM improved predictive model performance, showing a 13.64% improvement in F1-score and a 14.93% increase in overall accuracy compared to the baseline model trained solely on fall data from the SmartFallMM dataset. These findings highlight the potential of TransConv-DDPM to generate high-quality synthetic data for real-world applications.", "AI": {"tldr": "TransConv-DDPM\uff1a\u4e00\u79cd\u7528\u4e8e\u751f\u6210\u751f\u7269\u529b\u5b66\u548c\u751f\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u589e\u5f3a\u578b\u751f\u6210AI\u65b9\u6cd5\uff0c\u7ed3\u5408DDPM\u3001U-Net\u3001\u591a\u5c3a\u5ea6\u5377\u79ef\u548cTransformer\u5c42\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u63d0\u5347\u9884\u6d4b\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4e34\u5e8a\u9886\u57df\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u6570\u636e\u963b\u788d\u4e86AI\u6a21\u578b\u8bad\u7ec3\uff0c\u751f\u6210AI\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548cNLP\u9886\u57df\u5df2\u663e\u793a\u6f5c\u529b\uff0c\u4f46\u751f\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u751f\u6210\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u56e0\u5176\u5177\u6709\u590d\u6742\u6027\u548c\u53d8\u5f02\u6027\u3002", "method": "\u63d0\u51faTransConv-DDPM\u65b9\u6cd5\uff0c\u91c7\u7528\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DDPM\uff09\u7ed3\u5408U-Net\u67b6\u6784\u3001\u591a\u5c3a\u5ea6\u5377\u79ef\u6a21\u5757\u548cTransformer\u5c42\uff0c\u4ee5\u6355\u6349\u5168\u5c40\u548c\u5c40\u90e8\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4e0eTimeGAN\u548cDiffusion-TS\u7b49\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728SmartFallMM\u548cEEG\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u6709\u6548\u6355\u6349\u6570\u636e\u70b9\u95f4\u6e10\u53d8\u7684\u65f6\u95f4\u6a21\u5f0f\u3002\u5728SmartFallMM\u6570\u636e\u96c6\u4e0a\uff0c\u6dfb\u52a0\u5408\u6210\u6570\u636e\u4f7f\u9884\u6d4b\u6a21\u578b\u7684F1\u5206\u6570\u63d0\u534713.64%\uff0c\u6574\u4f53\u51c6\u786e\u7387\u63d0\u9ad814.93%\u3002", "conclusion": "TransConv-DDPM\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u751f\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u53ef\u89e3\u51b3\u4e34\u5e8a\u9886\u57df\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u5e76\u63d0\u5347AI\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2602.07622", "categories": ["gr-qc", "hep-th", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.07622", "abs": "https://arxiv.org/abs/2602.07622", "authors": ["Max Joseph Fahn", "Kristina Giesel", "Roman Kemper"], "title": "A gravitationally induced decoherence model for photons in the context of the relational formalism", "comment": "47 pages + appendix", "summary": "We formulate a model of gravitationally induced decoherence for photons starting from Maxwell theory coupled to linearised gravity, expressed in terms of Ashtekar-Barbero variables and treated as an open quantum field theoretic system. In contrast to quantum mechanical models, the interaction between the system (Maxwell field) and the environment (gravitational field) is not postulated phenomenologically, but is instead dictated by the underlying action in a post-Minkowskian approximation. This framework extends earlier models for a scalar field and enables a more detailed analysis of the role of dynamical reference fields (clocks) within the relational formalism. We show that, for a suitable choice of geometrical clocks together with a U(1)-Gauss clock, and by employing an appropriate combination of the observable map and its dual, the resulting Dirac observables are given directly by the transverse components of the photon field as well as the symmetric-transverse-traceless degrees of freedom of gravitational waves on the linearised phase space of the coupled system. In addition we also compare different choices of Dirac observables and their dynamics. Upon applying a Fock quantisation to the reduced system, we derive the time convolutionless (TCL) master equation, truncated at second order, and analyse its structural properties. These results provide a foundation for further investigations of the decoherence model, including its renormalisation and a detailed study of its one-particle sector, and are found to be structurally consistent with former master equations for photons derived using ADM variables and a specific gauge fixing.", "AI": {"tldr": "\u8bba\u6587\u4eceMaxwell\u7406\u8bba\u4e0e\u7ebf\u6027\u5316\u5f15\u529b\u8026\u5408\u51fa\u53d1\uff0c\u5efa\u7acb\u4e86\u5149\u5b50\u5f15\u529b\u8bf1\u5bfc\u9000\u76f8\u5e72\u6a21\u578b\uff0c\u63a8\u5bfc\u51faTCL\u4e3b\u65b9\u7a0b\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u57fa\u7840\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u91cf\u5b50\u529b\u5b66\u6a21\u578b\u4e2d\u7684\u7cfb\u7edf-\u73af\u5883\u76f8\u4e92\u4f5c\u7528\u662f\u552f\u8c61\u5047\u8bbe\u7684\uff0c\u9700\u8981\u4ece\u57fa\u7840\u4f5c\u7528\u91cf\u51fa\u53d1\u5efa\u7acb\u66f4\u6839\u672c\u7684\u5f15\u529b\u9000\u76f8\u5e72\u7406\u8bba\uff0c\u5e76\u6269\u5c55\u6807\u91cf\u573a\u6a21\u578b\u4ee5\u5206\u6790\u52a8\u529b\u5b66\u53c2\u8003\u573a\uff08\u65f6\u949f\uff09\u5728\u5173\u7cfb\u5f62\u5f0f\u4e3b\u4e49\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u4f7f\u7528Ashtekar-Barbero\u53d8\u91cf\u8868\u8fbeMaxwell\u7406\u8bba\u4e0e\u7ebf\u6027\u5316\u5f15\u529b\u8026\u5408\uff0c\u5728post-Minkowskian\u8fd1\u4f3c\u4e0b\u4f5c\u4e3a\u5f00\u653e\u91cf\u5b50\u573a\u8bba\u7cfb\u7edf\u5904\u7406\uff1b\u9009\u62e9\u5408\u9002\u7684\u51e0\u4f55\u65f6\u949f\u548cU(1)-Gauss\u65f6\u949f\uff0c\u901a\u8fc7\u53ef\u89c2\u6d4b\u91cf\u6620\u5c04\u53ca\u5176\u5bf9\u5076\u5f97\u5230Dirac\u53ef\u89c2\u6d4b\u91cf\uff1b\u5bf9\u7ea6\u5316\u7cfb\u7edf\u5e94\u7528Fock\u91cf\u5b50\u5316\uff0c\u63a8\u5bfc\u4e8c\u9636\u622a\u65ad\u7684\u65f6\u95f4\u5377\u79ef\u65e0\u5173\uff08TCL\uff09\u4e3b\u65b9\u7a0b\u3002", "result": "\u5f97\u5230\u7684Dirac\u53ef\u89c2\u6d4b\u91cf\u76f4\u63a5\u5bf9\u5e94\u4e8e\u5149\u5b50\u573a\u7684\u6a2a\u5411\u5206\u91cf\u4ee5\u53ca\u5f15\u529b\u6ce2\u7684\u5bf9\u79f0-\u6a2a\u5411-\u65e0\u8ff9\u81ea\u7531\u5ea6\uff1b\u63a8\u5bfc\u51fa\u7684TCL\u4e3b\u65b9\u7a0b\u5728\u7ed3\u6784\u4e0a\u4e0e\u4f7f\u7528ADM\u53d8\u91cf\u548c\u7279\u5b9a\u89c4\u8303\u56fa\u5b9a\u5f97\u5230\u7684\u5149\u5b50\u4e3b\u65b9\u7a0b\u4e00\u81f4\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5f15\u529b\u9000\u76f8\u5e72\u6a21\u578b\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\uff08\u5305\u62ec\u91cd\u6574\u5316\u548c\u5355\u7c92\u5b50\u6001\u8be6\u7ec6\u7814\u7a76\uff09\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u8bc1\u660e\u4e86\u4ece\u57fa\u7840\u4f5c\u7528\u91cf\u51fa\u53d1\u6784\u5efa\u5f15\u529b\u9000\u76f8\u5e72\u7406\u8bba\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u4e0d\u540cDirac\u53ef\u89c2\u6d4b\u91cf\u9009\u62e9\u53ca\u5176\u52a8\u529b\u5b66\u7684\u6bd4\u8f83\u7ed3\u679c\u3002"}}
{"id": "2602.07948", "categories": ["physics.comp-ph", "cond-mat.soft", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.07948", "abs": "https://arxiv.org/abs/2602.07948", "authors": ["Sandy H. S. Herho", "Iwan P. Anwar", "Faruq Khadami", "Alfita P. Handayani", "Karina A. Sujatmiko", "Kamaluddin Kasim", "Rusmawan Suwarman", "Dasapta E. Irawan"], "title": "dewi-kadita: A Python Library for Idealized Fish Schooling Simulation with Entropy-Based Diagnostics", "comment": "15 pages, 4 figures", "summary": "Collective motion in fish schools exemplifies emergent self-organization in active matter systems, yet computational tools for simulating and analyzing these dynamics remain fragmented across research groups. We present dewi-kadita, an open-source Python library implementing the three-dimensional Couzin zone-based model with comprehensive entropy diagnostics tailored for marine collective behavior research. The library introduces seven information-theoretic metrics -- school cohesion entropy, polarization entropy, depth stratification entropy, angular momentum entropy, nearest-neighbor entropy, velocity correlation entropy, and school shape entropy -- that characterize distinct organizational features inaccessible to classical order parameters. These metrics combine into an Oceanic Schooling Index (OSI) providing a single scalar measure of collective disorder. Validation across four canonical configurations (swarm, torus, dynamic parallel, highly parallel) confirms correct reproduction of known phase behaviors: the swarm maintains disorder with polarization $P < 0.1$ and OSI $\\approx 0.71$, while the highly parallel state achieves $P = 0.998$ with OSI $= 0.24$ and velocity correlation entropy vanishing to zero. The entropy framework successfully discriminates the torus and dynamic parallel configurations that exhibit comparable order parameter magnitudes through different organizational mechanisms. Numba just-in-time (JIT) compilation accelerates pairwise interaction calculations by $10$--$100\\times$, enabling simulations of $150$--$250$ agents over $1000$--$2000$ time steps within five minutes on standard workstation hardware. NetCDF4 output ensures interoperability with oceanographic analysis tools. The library addresses the need for standardized, reproducible infrastructure in collective behavior modeling analogous to established molecular dynamics codes.", "AI": {"tldr": "\u5f00\u53d1\u4e86dewi-kadita\u5f00\u6e90Python\u5e93\uff0c\u5b9e\u73b0\u4e09\u7ef4Couzin\u533a\u57df\u6a21\u578b\uff0c\u63d0\u4f9b\u4e03\u4e2a\u4fe1\u606f\u71b5\u6307\u6807\u548c\u7efc\u5408\u7684\u6d77\u6d0b\u96c6\u7fa4\u6307\u6570\uff0c\u7528\u4e8e\u91cf\u5316\u9c7c\u7fa4\u96c6\u4f53\u8fd0\u52a8\u7684\u7ec4\u7ec7\u7279\u5f81\u3002", "motivation": "\u9c7c\u7fa4\u96c6\u4f53\u8fd0\u52a8\u662f\u6d3b\u6027\u7269\u8d28\u7cfb\u7edf\u4e2d\u81ea\u7ec4\u7ec7\u73b0\u8c61\u7684\u5178\u578b\u4f8b\u5b50\uff0c\u4f46\u76ee\u524d\u6a21\u62df\u548c\u5206\u6790\u8fd9\u4e9b\u52a8\u6001\u7684\u8ba1\u7b97\u5de5\u5177\u5728\u4e0d\u540c\u7814\u7a76\u7ec4\u4e4b\u95f4\u5206\u6563\u4e14\u4e0d\u7edf\u4e00\u3002\u9700\u8981\u6807\u51c6\u5316\u7684\u3001\u53ef\u91cd\u590d\u7684\u57fa\u7840\u8bbe\u65bd\u6765\u652f\u6301\u96c6\u4f53\u884c\u4e3a\u5efa\u6a21\u7814\u7a76\u3002", "method": "\u5f00\u53d1\u4e86dewi-kadita\u5f00\u6e90Python\u5e93\uff0c\u5b9e\u73b0\u4e09\u7ef4Couzin\u533a\u57df\u6a21\u578b\u3002\u5f15\u5165\u4e86\u4e03\u4e2a\u4fe1\u606f\u8bba\u6307\u6807\uff08\u96c6\u7fa4\u5185\u805a\u71b5\u3001\u6781\u5316\u71b5\u3001\u6df1\u5ea6\u5206\u5c42\u71b5\u3001\u89d2\u52a8\u91cf\u71b5\u3001\u6700\u8fd1\u90bb\u71b5\u3001\u901f\u5ea6\u76f8\u5173\u71b5\u548c\u96c6\u7fa4\u5f62\u72b6\u71b5\uff09\uff0c\u8fd9\u4e9b\u6307\u6807\u7ec4\u5408\u6210\u6d77\u6d0b\u96c6\u7fa4\u6307\u6570\uff08OSI\uff09\u3002\u4f7f\u7528Numba JIT\u7f16\u8bd1\u52a0\u901f\u6210\u5bf9\u76f8\u4e92\u4f5c\u7528\u8ba1\u7b97\uff0c\u652f\u6301NetCDF4\u8f93\u51fa\u683c\u5f0f\u3002", "result": "\u9a8c\u8bc1\u4e86\u56db\u79cd\u5178\u578b\u914d\u7f6e\uff08\u96c6\u7fa4\u3001\u73af\u72b6\u3001\u52a8\u6001\u5e73\u884c\u3001\u9ad8\u5ea6\u5e73\u884c\uff09\u7684\u6b63\u786e\u6027\uff1a\u96c6\u7fa4\u4fdd\u6301\u65e0\u5e8f\u72b6\u6001\uff08P<0.1\uff0cOSI\u22480.71\uff09\uff0c\u9ad8\u5ea6\u5e73\u884c\u72b6\u6001\u8fbe\u5230P=0.998\uff0cOSI=0.24\u3002\u71b5\u6846\u67b6\u6210\u529f\u533a\u5206\u4e86\u5177\u6709\u76f8\u4f3c\u6709\u5e8f\u53c2\u6570\u4f46\u7ec4\u7ec7\u673a\u5236\u4e0d\u540c\u7684\u73af\u72b6\u548c\u52a8\u6001\u5e73\u884c\u914d\u7f6e\u3002Numba JIT\u7f16\u8bd1\u4f7f\u6210\u5bf9\u76f8\u4e92\u4f5c\u7528\u8ba1\u7b97\u52a0\u901f10-100\u500d\uff0c\u80fd\u5728\u6807\u51c6\u5de5\u4f5c\u7ad9\u4e0a5\u5206\u949f\u5185\u6a21\u62df150-250\u4e2a\u4ee3\u7406\u57281000-2000\u4e2a\u65f6\u95f4\u6b65\u957f\u7684\u52a8\u6001\u3002", "conclusion": "dewi-kadita\u5e93\u4e3a\u6d77\u6d0b\u96c6\u4f53\u884c\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u3001\u53ef\u91cd\u590d\u7684\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\uff0c\u7c7b\u4f3c\u4e8e\u6210\u719f\u7684\u5206\u5b50\u52a8\u529b\u5b66\u4ee3\u7801\uff0c\u586b\u8865\u4e86\u96c6\u4f53\u884c\u4e3a\u5efa\u6a21\u4e2d\u6807\u51c6\u5316\u5de5\u5177\u7684\u7a7a\u767d\u3002\u4fe1\u606f\u71b5\u6846\u67b6\u80fd\u591f\u6355\u6349\u7ecf\u5178\u6709\u5e8f\u53c2\u6570\u65e0\u6cd5\u8bbf\u95ee\u7684\u7ec4\u7ec7\u7279\u5f81\u3002"}}
{"id": "2602.07151", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.07151", "abs": "https://arxiv.org/abs/2602.07151", "authors": ["Riley W. Chien", "Mitchell L. Chiew", "Brent Harrison", "Jason Necaise", "Weishi Wang", "Maryam Mudassar", "Campbell McLauchlan", "Thomas M. Henderson", "Gustavo E. Scuseria", "Sergii Strelchuk", "James D. Whitfield"], "title": "Putting fermions onto a digital quantum computer", "comment": "To appear in Nature Physics Reviews", "summary": "Quantum computers are expected to become a powerful tool for studying physical quantum systems. Consequently, a number of quantum algorithms for studying the physical properties of such systems have been developed. While qubit-based quantum computers are naturally suited to the study of spin-1/2 systems, systems containing other degrees of freedom must first be encoded into qubits. Transformations to and from fermionic degrees of freedom have long been an important tool in physics and, now the simulation of fermionic systems on quantum computers based on qubits provides yet another application. In this perspective, we review methods for encoding fermionic degrees of freedom into qubits and attempt to dispel the persistent notion that fermionic systems beyond one dimension are fundamentally more difficult to deal with.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u5c06\u8d39\u7c73\u5b50\u81ea\u7531\u5ea6\u7f16\u7801\u5230\u91cf\u5b50\u6bd4\u7279\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u6d88\u9664\u9ad8\u7ef4\u8d39\u7c73\u5b50\u7cfb\u7edf\u672c\u8d28\u4e0a\u66f4\u96be\u5904\u7406\u7684\u8bef\u89e3\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u673a\u6709\u671b\u6210\u4e3a\u7814\u7a76\u7269\u7406\u91cf\u5b50\u7cfb\u7edf\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u4f46\u57fa\u4e8e\u91cf\u5b50\u6bd4\u7279\u7684\u91cf\u5b50\u8ba1\u7b97\u673a\u5929\u7136\u9002\u5408\u7814\u7a76\u81ea\u65cb-1/2\u7cfb\u7edf\uff0c\u800c\u5305\u542b\u5176\u4ed6\u81ea\u7531\u5ea6\u7684\u7cfb\u7edf\uff08\u5982\u8d39\u7c73\u5b50\u7cfb\u7edf\uff09\u9700\u8981\u5148\u7f16\u7801\u5230\u91cf\u5b50\u6bd4\u7279\u4e2d\u3002\u8d39\u7c73\u5b50\u81ea\u7531\u5ea6\u53d8\u6362\u5728\u7269\u7406\u5b66\u4e2d\u4e00\u76f4\u662f\u91cd\u8981\u5de5\u5177\uff0c\u73b0\u5728\u5728\u57fa\u4e8e\u91cf\u5b50\u6bd4\u7279\u7684\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u6a21\u62df\u8d39\u7c73\u5b50\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u5e94\u7528\u573a\u666f\u3002", "method": "\u672c\u6587\u662f\u4e00\u7bc7\u7efc\u8ff0\u6027\u89c6\u89d2\u6587\u7ae0\uff0c\u56de\u987e\u4e86\u5c06\u8d39\u7c73\u5b50\u81ea\u7531\u5ea6\u7f16\u7801\u5230\u91cf\u5b50\u6bd4\u7279\u7684\u5404\u79cd\u65b9\u6cd5\u3002\u6587\u7ae0\u7cfb\u7edf\u6027\u5730\u68b3\u7406\u4e86\u73b0\u6709\u7684\u7f16\u7801\u6280\u672f\uff0c\u5e76\u5206\u6790\u6bd4\u8f83\u4e86\u4e0d\u540c\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u3002", "result": "\u6587\u7ae0\u6f84\u6e05\u4e86\u4e00\u4e2a\u957f\u671f\u5b58\u5728\u7684\u8bef\u89e3\uff1a\u5373\u9ad8\u7ef4\uff08\u8d85\u8fc7\u4e00\u7ef4\uff09\u8d39\u7c73\u5b50\u7cfb\u7edf\u672c\u8d28\u4e0a\u6bd4\u4f4e\u7ef4\u7cfb\u7edf\u66f4\u96be\u5904\u7406\u3002\u901a\u8fc7\u7cfb\u7edf\u56de\u987e\u7f16\u7801\u65b9\u6cd5\uff0c\u8868\u660e\u8fd9\u79cd\u56f0\u96be\u5e76\u975e\u6839\u672c\u6027\u7684\uff0c\u800c\u662f\u53ef\u4ee5\u901a\u8fc7\u9002\u5f53\u7684\u7f16\u7801\u7b56\u7565\u6765\u89e3\u51b3\u3002", "conclusion": "\u8d39\u7c73\u5b50\u81ea\u7531\u5ea6\u5230\u91cf\u5b50\u6bd4\u7279\u7684\u7f16\u7801\u65b9\u6cd5\u662f\u91cf\u5b50\u8ba1\u7b97\u6a21\u62df\u8d39\u7c73\u5b50\u7cfb\u7edf\u7684\u91cd\u8981\u5de5\u5177\uff0c\u9ad8\u7ef4\u8d39\u7c73\u5b50\u7cfb\u7edf\u5e76\u4e0d\u6bd4\u4f4e\u7ef4\u7cfb\u7edf\u672c\u8d28\u4e0a\u66f4\u56f0\u96be\uff0c\u9002\u5f53\u7684\u7f16\u7801\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4e3a\u5728\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u7814\u7a76\u66f4\u5e7f\u6cdb\u7684\u7269\u7406\u7cfb\u7edf\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2602.07054", "categories": ["cs.LG", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.07054", "abs": "https://arxiv.org/abs/2602.07054", "authors": ["Ashutosh Chaubey", "Jiacheng Pang", "Maksim Siniukov", "Mohammad Soleymani"], "title": "AVERE: Improving Audiovisual Emotion Reasoning with Preference Optimization", "comment": "Accepted as a conference paper at ICLR 2026. Project page: https://avere-iclr.github.io", "summary": "Emotion understanding is essential for building socially intelligent agents. Although recent multimodal large language models have shown strong performance on this task, two key challenges remain - spurious associations between emotions and irrelevant audiovisual cues, and hallucinations of audiovisual cues driven by text priors in the language model backbone. To quantify and understand these issues, we introduce EmoReAlM, a benchmark designed to evaluate MLLMs for cue-emotion associations, hallucinations and modality agreement. We then propose AVEm-DPO, a preference optimization technique that aligns model responses with both audiovisual inputs and emotion-centric queries. Specifically, we construct preferences over responses exhibiting spurious associations or hallucinations, and audiovisual input pairs guided by textual prompts. We also include a regularization term that penalizes reliance on text priors, thereby mitigating modality-specific cue hallucinations. Experimental results on DFEW, RAVDESS and EMER demonstrate that our method significantly improves the performance of the reference baseline models with 6-19% of relative performance gains in zero-shot settings. By providing both a rigorous benchmark and a robust optimization framework, this work enables principled evaluation and improvement of MLLMs for emotion understanding and social AI. Code, models and benchmark will be released at https://avere-iclr.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEmoReAlM\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u7406\u89e3\u4e2d\u7684\u865a\u5047\u5173\u8054\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u5f00\u53d1AVEm-DPO\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u7406\u89e3\u4efb\u52a1\u4e2d\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1\uff09\u60c5\u611f\u4e0e\u65e0\u5173\u89c6\u542c\u7ebf\u7d22\u4e4b\u95f4\u7684\u865a\u5047\u5173\u8054\uff1b2\uff09\u8bed\u8a00\u6a21\u578b\u4e3b\u5e72\u4e2d\u7684\u6587\u672c\u5148\u9a8c\u9a71\u52a8\u7684\u89c6\u542c\u7ebf\u7d22\u5e7b\u89c9\u3002\u9700\u8981\u91cf\u5316\u8fd9\u4e9b\u95ee\u9898\u5e76\u5f00\u53d1\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faEmoReAlM\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u7ebf\u7d22-\u60c5\u611f\u5173\u8054\u3001\u5e7b\u89c9\u548c\u6a21\u6001\u4e00\u81f4\u6027\uff1b\u5f00\u53d1AVEm-DPO\u504f\u597d\u4f18\u5316\u6280\u672f\uff0c\u901a\u8fc7\u6784\u5efa\u5bf9\u865a\u5047\u5173\u8054/\u5e7b\u89c9\u54cd\u5e94\u7684\u504f\u597d\u5bf9\uff0c\u5e76\u52a0\u5165\u60e9\u7f5a\u6587\u672c\u5148\u9a8c\u4f9d\u8d56\u7684\u6b63\u5219\u5316\u9879\u6765\u5bf9\u9f50\u6a21\u578b\u54cd\u5e94\u3002", "result": "\u5728DFEW\u3001RAVDESS\u548cEMER\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u53c2\u8003\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u83b7\u5f97\u4e866-19%\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u4e25\u8c28\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u9c81\u68d2\u7684\u4f18\u5316\u6846\u67b6\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u60c5\u611f\u7406\u89e3\u548c\u793e\u4f1aAI\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u8bc4\u4f30\u548c\u6539\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2602.07780", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2602.07780", "abs": "https://arxiv.org/abs/2602.07780", "authors": ["Wen-Xiang Chen"], "title": "Existence of Halos Outside Schwarzschild-$f(R)$ Black Holes", "comment": null, "summary": "We investigate the possibility of photon halos (stable photon orbits) forming outside Schwarzschild-$f(R)$ black holes by analyzing null geodesics in these spacetimes. Using methods inspired by studies of spherical photon orbits around Kerr-Newman black holes, we derive conditions for the existence of such halos. We examine several f(R) gravity models, including quadratic, logarithmic, exponential, cubic, power-law, and hyperbolic forms, and find that multiple photon orbits -- both stable and unstable -- can appear outside the event horizon for certain parameter ranges. These additional orbits (halos) provide new insights into spacetime geometry and potential observational signatures of black holes in modified gravity. We present analytical expressions for the orbital radii, perform a numerical stability analysis, and discuss possible observational implications for black hole shadows. Our results indicate that while the standard Schwarzschild black hole admits only a single unstable light ring, Schwarzschild-$f(R)$ black holes can support an additional outer stable photon orbit (a halo) without triggering a black-hole bomb instability. This work deepens the understanding of photon-orbit structures in alternative theories of gravity and highlights how such effects could be detected through deviations in black hole shadow size or morphology.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Schwarzschild-f(R)\u9ed1\u6d1e\u5916\u90e8\u5f62\u6210\u5149\u5b50\u6655\uff08\u7a33\u5b9a\u5149\u5b50\u8f68\u9053\uff09\u7684\u53ef\u80fd\u6027\uff0c\u53d1\u73b0\u5728\u67d0\u4e9bf(R)\u5f15\u529b\u6a21\u578b\u4e2d\uff0c\u9ed1\u6d1e\u4e8b\u4ef6\u89c6\u754c\u5916\u53ef\u4ee5\u5b58\u5728\u591a\u4e2a\u5149\u5b50\u8f68\u9053\uff08\u5305\u62ec\u7a33\u5b9a\u548c\u4e0d\u7a33\u5b9a\u8f68\u9053\uff09\uff0c\u8fd9\u4e3a\u901a\u8fc7\u9ed1\u6d1e\u9634\u5f71\u89c2\u6d4b\u4fee\u6539\u5f15\u529b\u7406\u8bba\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "motivation": "\u7814\u7a76\u4fee\u6539\u5f15\u529b\u7406\u8bba\uff08\u7279\u522b\u662ff(R)\u5f15\u529b\uff09\u4e2d\u9ed1\u6d1e\u7684\u5149\u5b50\u8f68\u9053\u7ed3\u6784\uff0c\u63a2\u7d22\u6807\u51c6Schwarzschild\u9ed1\u6d1e\u4e4b\u5916\u662f\u5426\u5b58\u5728\u989d\u5916\u7684\u7a33\u5b9a\u5149\u5b50\u8f68\u9053\uff08\u5149\u5b50\u6655\uff09\uff0c\u5e76\u5206\u6790\u8fd9\u4e9b\u8f68\u9053\u5bf9\u9ed1\u6d1e\u89c2\u6d4b\u7279\u5f81\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u53d7Kerr-Newman\u9ed1\u6d1e\u7403\u5f62\u5149\u5b50\u8f68\u9053\u7814\u7a76\u542f\u53d1\u7684\u65b9\u6cd5\uff0c\u5206\u6790Schwarzschild-f(R)\u9ed1\u6d1e\u65f6\u7a7a\u4e2d\u7684\u96f6\u6d4b\u5730\u7ebf\u3002\u7814\u7a76\u4e86\u591a\u79cdf(R)\u5f15\u529b\u6a21\u578b\uff08\u4e8c\u6b21\u3001\u5bf9\u6570\u3001\u6307\u6570\u3001\u7acb\u65b9\u3001\u5e42\u5f8b\u3001\u53cc\u66f2\u5f62\u5f0f\uff09\uff0c\u63a8\u5bfc\u4e86\u5149\u5b50\u6655\u5b58\u5728\u7684\u6761\u4ef6\uff0c\u7ed9\u51fa\u4e86\u8f68\u9053\u534a\u5f84\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u5e76\u8fdb\u884c\u4e86\u6570\u503c\u7a33\u5b9a\u6027\u5206\u6790\u3002", "result": "\u53d1\u73b0Schwarzschild-f(R)\u9ed1\u6d1e\u53ef\u4ee5\u5728\u4e8b\u4ef6\u89c6\u754c\u5916\u652f\u6301\u989d\u5916\u7684\u5916\u5c42\u7a33\u5b9a\u5149\u5b50\u8f68\u9053\uff08\u5149\u5b50\u6655\uff09\uff0c\u800c\u4e0d\u4f1a\u89e6\u53d1\u9ed1\u6d1e\u70b8\u5f39\u4e0d\u7a33\u5b9a\u6027\u3002\u6807\u51c6Schwarzschild\u9ed1\u6d1e\u53ea\u6709\u4e00\u4e2a\u4e0d\u7a33\u5b9a\u5149\u73af\uff0c\u800c\u4fee\u6539\u5f15\u529b\u6a21\u578b\u4e2d\u53ef\u4ee5\u5b58\u5728\u591a\u4e2a\u5149\u5b50\u8f68\u9053\u3002\u8fd9\u4e9b\u989d\u5916\u7684\u8f68\u9053\u53ef\u80fd\u901a\u8fc7\u9ed1\u6d1e\u9634\u5f71\u7684\u5927\u5c0f\u6216\u5f62\u6001\u504f\u5dee\u88ab\u89c2\u6d4b\u5230\u3002", "conclusion": "f(R)\u5f15\u529b\u4e2d\u7684Schwarzschild\u9ed1\u6d1e\u53ef\u4ee5\u5f62\u6210\u7a33\u5b9a\u7684\u5149\u5b50\u6655\uff0c\u8fd9\u4e3a\u901a\u8fc7\u9ed1\u6d1e\u9634\u5f71\u89c2\u6d4b\u6765\u63a2\u6d4b\u4fee\u6539\u5f15\u529b\u7406\u8bba\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u6df1\u5316\u4e86\u5bf9\u66ff\u4ee3\u5f15\u529b\u7406\u8bba\u4e2d\u5149\u5b50\u8f68\u9053\u7ed3\u6784\u7684\u7406\u89e3\u3002"}}
{"id": "2602.08492", "categories": ["physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.08492", "abs": "https://arxiv.org/abs/2602.08492", "authors": ["Jennifer Paola Corredor-G\u00f3mez", "Andr\u00e9s Mauricio Rueda-Ram\u00edrez", "Miguel Alejandro Gamboa-M\u00e1rquez", "Carolina Torres-Rodr\u00edguez", "Carlos Julio Cort\u00e9s-Rodr\u00edguez"], "title": "An intramembranous ossification model for the in-silico analysis of bone tissue formation in tooth extraction sites", "comment": null, "summary": "The accurate modeling of biological processes allows to predict the spatio-temporal behavior of living tissues by computer-aided (in-silico) testing, a useful tool for the development of medical strategies, avoiding the expenses and potential ethical implications of in-vivo experimentation. A model for bone healing in mouth would be useful for selecting proper surgical techniques in dental procedures. In this paper, the formulation and implementation of a model for Intramembranous Ossification is presented aiming to describe the complex process of bone tissue formation in tooth extraction sites. The model consists in a mathematical description of the mechanisms in which different types of cells interact, synthesize and degrade extra-cellular matrices under the influence of biochemical factors. Special attention is given to angiogenesis, oxygen-dependent effects and growth factor-induced apoptosis of fibroblasts. Furthermore, considering the depth-dependent vascularization of mandibular bone and its influence on bone healing, a functional description of the cell distribution on the severed periodontal ligament (PDL) is proposed. The developed model was implemented using the finite element method (FEM) and successfully validated by simulating an animal in-vivo experiment on dogs reported in the literature. A good fit between model outcome and experimental data was obtained with a mean absolute error of 3.04%. The mathematical framework presented here may represent an important tool for the design of future in-vitro and in-vivo tests, as well as a precedent for future in-silico studies on osseointegration and mechanobiology.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7528\u4e8e\u63cf\u8ff0\u62d4\u7259\u90e8\u4f4d\u9aa8\u7ec4\u7ec7\u5f62\u6210\u7684\u819c\u5185\u9aa8\u5316\u6570\u5b66\u6a21\u578b\uff0c\u91cd\u70b9\u5173\u6ce8\u8840\u7ba1\u751f\u6210\u3001\u6c27\u4f9d\u8d56\u6548\u5e94\u548c\u751f\u957f\u56e0\u5b50\u8bf1\u5bfc\u7684\u6210\u7ea4\u7ef4\u7ec6\u80de\u51cb\u4ea1\uff0c\u5e76\u901a\u8fc7\u6709\u9650\u5143\u65b9\u6cd5\u5b9e\u73b0\u548c\u9a8c\u8bc1\u3002", "motivation": "\u51c6\u786e\u5efa\u6a21\u751f\u7269\u8fc7\u7a0b\u53ef\u4ee5\u901a\u8fc7\u8ba1\u7b97\u673a\u8f85\u52a9\uff08in-silico\uff09\u6d4b\u8bd5\u9884\u6d4b\u6d3b\u4f53\u7ec4\u7ec7\u7684\u65f6\u7a7a\u884c\u4e3a\uff0c\u8fd9\u5bf9\u4e8e\u5236\u5b9a\u533b\u7597\u7b56\u7565\u975e\u5e38\u6709\u7528\uff0c\u53ef\u4ee5\u907f\u514d\u4f53\u5185\u5b9e\u9a8c\u7684\u8d39\u7528\u548c\u4f26\u7406\u95ee\u9898\u3002\u7279\u522b\u662f\u5f00\u53d1\u4e00\u4e2a\u63cf\u8ff0\u62d4\u7259\u90e8\u4f4d\u9aa8\u6108\u5408\u7684\u6a21\u578b\uff0c\u6709\u52a9\u4e8e\u5728\u7259\u79d1\u624b\u672f\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5916\u79d1\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u819c\u5185\u9aa8\u5316\u6570\u5b66\u6a21\u578b\uff0c\u63cf\u8ff0\u4e86\u4e0d\u540c\u7c7b\u578b\u7ec6\u80de\u5728\u751f\u5316\u56e0\u5b50\u5f71\u54cd\u4e0b\u76f8\u4e92\u4f5c\u7528\u3001\u5408\u6210\u548c\u964d\u89e3\u7ec6\u80de\u5916\u57fa\u8d28\u7684\u673a\u5236\u3002\u7279\u522b\u5173\u6ce8\u8840\u7ba1\u751f\u6210\u3001\u6c27\u4f9d\u8d56\u6548\u5e94\u548c\u751f\u957f\u56e0\u5b50\u8bf1\u5bfc\u7684\u6210\u7ea4\u7ef4\u7ec6\u80de\u51cb\u4ea1\u3002\u8003\u8651\u5230\u4e0b\u988c\u9aa8\u6df1\u5ea6\u4f9d\u8d56\u6027\u8840\u7ba1\u5316\u53ca\u5176\u5bf9\u9aa8\u6108\u5408\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u5207\u65ad\u7259\u5468\u97e7\u5e26\uff08PDL\uff09\u4e0a\u7ec6\u80de\u5206\u5e03\u7684\u529f\u80fd\u63cf\u8ff0\u3002\u4f7f\u7528\u6709\u9650\u5143\u65b9\u6cd5\uff08FEM\uff09\u5b9e\u73b0\u4e86\u8be5\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u6587\u732e\u4e2d\u62a5\u544a\u7684\u72d7\u4f53\u5185\u5b9e\u9a8c\uff0c\u6210\u529f\u9a8c\u8bc1\u4e86\u5f00\u53d1\u7684\u6a21\u578b\u3002\u6a21\u578b\u7ed3\u679c\u4e0e\u5b9e\u9a8c\u6570\u636e\u543b\u5408\u826f\u597d\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a3.04%\u3002", "conclusion": "\u8fd9\u91cc\u63d0\u51fa\u7684\u6570\u5b66\u6846\u67b6\u53ef\u80fd\u6210\u4e3a\u8bbe\u8ba1\u672a\u6765\u4f53\u5916\u548c\u4f53\u5185\u6d4b\u8bd5\u7684\u91cd\u8981\u5de5\u5177\uff0c\u4e5f\u4e3a\u672a\u6765\u5173\u4e8e\u9aa8\u6574\u5408\u548c\u529b\u5b66\u751f\u7269\u5b66\u7684\u8ba1\u7b97\u673a\u6a21\u62df\u7814\u7a76\u63d0\u4f9b\u4e86\u5148\u4f8b\u3002"}}
{"id": "2602.07201", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.07201", "abs": "https://arxiv.org/abs/2602.07201", "authors": ["Wenhan Guo", "Mikhail Litvinov", "Tzu-Chieh Wei", "Abid Khan", "Kevin C. Smith"], "title": "Measurement-Based Preparation of Higher-Dimensional AKLT States and Their Quantum Computational Power", "comment": "22 pages, 11 figures, 2 tables", "summary": "We investigate a constant-time, fusion measurement-based scheme to create AKLT states beyond one dimension. We show that it is possible to prepare such states on a given graph up to random spin-1 `decorations', each corresponding to a probabilistic insertion of a vertex along an edge. In investigating their utility in measurement-based quantum computation, we demonstrate that any such randomly decorated AKLT state possesses at least the same computational power as non-random ones, such as those on trivalent planar lattices. For AKLT states on Bethe lattices and their decorated versions we show that there exists a deterministic, constant-time scheme for their preparation. In addition to randomly decorated AKLT states, we also consider random-bond AKLT states, whose construction involves any of the canonical Bell states in the bond degrees of freedom instead of just the singlet in the original construction. Such states naturally emerge upon measuring all the decorative spin-1 sites in the randomly decorated AKLT states. We show that those random-bond AKLT states on trivalent lattices can be converted to encoded random graph states after acting with the same POVM on all sites. We also argue that these random-bond AKLT states possess similar quantum computational power as the original singlet-bond AKLT states via the percolation perspective.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u591a\u7ef4\u56fe\u4e0a\u5236\u5907AKLT\u6001\u7684\u65b0\u65b9\u6cd5\uff0c\u5305\u62ec\u968f\u673a\u88c5\u9970\u548c\u968f\u673a\u952e\u5408\u53d8\u4f53\uff0c\u5e76\u8bc1\u660e\u4e86\u8fd9\u4e9b\u6001\u5728\u6d4b\u91cf\u57fa\u91cf\u5b50\u8ba1\u7b97\u4e2d\u5177\u6709\u4e0e\u539f\u59cbAKLT\u6001\u76f8\u540c\u7684\u8ba1\u7b97\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u591a\u7ef4\u56fe\u4e0a\u9ad8\u6548\u5236\u5907AKLT\u6001\uff0c\u5e76\u63a2\u7d22\u8fd9\u4e9b\u6001\u5728\u6d4b\u91cf\u57fa\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002\u4f20\u7edfAKLT\u6001\u5236\u5907\u65b9\u6cd5\u6709\u9650\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u5236\u5907\u65b9\u6848\u6765\u6269\u5c55\u5176\u5e94\u7528\u8303\u56f4\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b0\u65b9\u6cd5\uff1a1) \u968f\u673a\u88c5\u9970AKLT\u6001 - \u901a\u8fc7\u6982\u7387\u6027\u5728\u8fb9\u4e2d\u63d2\u5165\u9876\u70b9\u6765\u88c5\u9970\u56fe\uff1b2) \u968f\u673a\u952e\u5408AKLT\u6001 - \u5728\u952e\u81ea\u7531\u5ea6\u4e2d\u4f7f\u7528\u4efb\u610f\u8d1d\u5c14\u6001\u800c\u975e\u4ec5\u5355\u6001\u3002\u91c7\u7528\u5e38\u65f6\u878d\u5408\u6d4b\u91cf\u57fa\u65b9\u6848\u8fdb\u884c\u5236\u5907\uff0c\u5e76\u5206\u6790\u5176\u8ba1\u7b97\u80fd\u529b\u3002", "result": "1) \u8bc1\u660e\u4e86\u968f\u673a\u88c5\u9970AKLT\u6001\u81f3\u5c11\u5177\u6709\u4e0e\u975e\u968f\u673a\u6001\u76f8\u540c\u7684\u8ba1\u7b97\u80fd\u529b\uff1b2) \u5728Bethe\u683c\u53ca\u5176\u88c5\u9970\u7248\u672c\u4e0a\u5b9e\u73b0\u4e86\u786e\u5b9a\u6027\u5e38\u65f6\u5236\u5907\u65b9\u6848\uff1b3) \u968f\u673a\u952e\u5408AKLT\u6001\u53ef\u8f6c\u6362\u4e3a\u7f16\u7801\u968f\u673a\u56fe\u6001\uff1b4) \u4ece\u6e17\u6d41\u89d2\u5ea6\u8bc1\u660e\u968f\u673a\u952e\u5408AKLT\u6001\u5177\u6709\u4e0e\u539f\u59cb\u5355\u6001\u952e\u5408AKLT\u6001\u76f8\u4f3c\u7684\u8ba1\u7b97\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u6269\u5c55\u4e86AKLT\u6001\u7684\u5236\u5907\u65b9\u6cd5\u548c\u5e94\u7528\u8303\u56f4\uff0c\u8bc1\u660e\u4e86\u968f\u673a\u88c5\u9970\u548c\u968f\u673a\u952e\u5408\u53d8\u4f53\u5728\u6d4b\u91cf\u57fa\u91cf\u5b50\u8ba1\u7b97\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\uff0c\u4e3a\u91cf\u5b50\u8ba1\u7b97\u8d44\u6e90\u6001\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.07061", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07061", "abs": "https://arxiv.org/abs/2602.07061", "authors": ["Daniel Nobrega"], "title": "TACIT: Transformation-Aware Capturing of Implicit Thought", "comment": "25 pages, 7 figures", "summary": "We present TACIT (Transformation-Aware Capturing of Implicit Thought), a diffusion-based transformer for interpretable visual reasoning. Unlike language-based reasoning systems, TACIT operates entirely in pixel space using rectified flow, enabling direct visualization of the reasoning process at each inference step. We demonstrate the approach on maze-solving, where the model learns to transform images of unsolved mazes into solutions. Key results on 1 million synthetic maze pairs include:\n  - 192x reduction in training loss over 100 epochs\n  - 22.7x improvement in L2 distance to ground truth\n  - Only 10 Euler steps required (vs. 100-1000 for typical diffusion models)\n  Quantitative analysis reveals a striking phase transition phenomenon: the solution remains invisible for 68% of the transformation (zero recall), then emerges abruptly at t=0.70 within just 2% of the process. Most remarkably, 100% of samples exhibit simultaneous emergence across all spatial regions, ruling out sequential path construction and providing evidence for holistic rather than algorithmic reasoning. This \"eureka moment\" pattern -- long incubation followed by sudden crystallization -- parallels insight phenomena in human cognition. The pixel-space design with noise-free flow matching provides a foundation for understanding how neural networks develop implicit reasoning strategies that operate below and before language.", "AI": {"tldr": "TACIT\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684transformer\u6a21\u578b\uff0c\u7528\u4e8e\u53ef\u89e3\u91ca\u7684\u89c6\u89c9\u63a8\u7406\uff0c\u901a\u8fc7\u50cf\u7d20\u7a7a\u95f4\u7684rectified flow\u76f4\u63a5\u53ef\u89c6\u5316\u63a8\u7406\u8fc7\u7a0b\uff0c\u5728\u8ff7\u5bab\u6c42\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\"\u987f\u609f\u65f6\u523b\"\u73b0\u8c61\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u5b8c\u5168\u5728\u50cf\u7d20\u7a7a\u95f4\u64cd\u4f5c\u7684\u89c6\u89c9\u63a8\u7406\u7cfb\u7edf\uff0c\u80fd\u591f\u76f4\u63a5\u53ef\u89c6\u5316\u63a8\u7406\u8fc7\u7a0b\uff0c\u4ee5\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u5982\u4f55\u53d1\u5c55\u5728\u8bed\u8a00\u4e4b\u524d\u7684\u9690\u5f0f\u63a8\u7406\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6269\u6563\u7684transformer\u67b6\u6784\uff0c\u91c7\u7528rectified flow\u5728\u50cf\u7d20\u7a7a\u95f4\u64cd\u4f5c\uff0c\u901a\u8fc7\u566a\u58f0\u81ea\u7531\u6d41\u5339\u914d\u5c06\u672a\u89e3\u51b3\u7684\u8ff7\u5bab\u56fe\u50cf\u8f6c\u6362\u4e3a\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5728100\u4e07\u4e2a\u5408\u6210\u8ff7\u5bab\u5bf9\u4e0a\uff1a\u8bad\u7ec3\u635f\u5931\u51cf\u5c11192\u500d\uff0cL2\u8ddd\u79bb\u6539\u558422.7\u500d\uff0c\u4ec5\u970010\u4e2aEuler\u6b65\u9aa4\uff08\u5178\u578b\u6269\u6563\u6a21\u578b\u9700\u8981100-1000\u6b65\uff09\u3002\u89c2\u5bdf\u5230\u660e\u663e\u7684\u76f8\u53d8\u73b0\u8c61\uff1a\u89e3\u51b3\u65b9\u6848\u572868%\u7684\u8f6c\u6362\u8fc7\u7a0b\u4e2d\u4e0d\u53ef\u89c1\uff0c\u7136\u540e\u5728t=0.70\u65f6\u57282%\u7684\u8fc7\u7a0b\u4e2d\u7a81\u7136\u51fa\u73b0\uff0c\u6240\u6709\u7a7a\u95f4\u533a\u57df\u540c\u65f6\u6d8c\u73b0\u3002", "conclusion": "TACIT\u5c55\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u53d1\u5c55\u9690\u5f0f\u63a8\u7406\u7b56\u7565\u7684\u80fd\u529b\uff0c\u5176\"\u987f\u609f\u65f6\u523b\"\u6a21\u5f0f\uff08\u957f\u671f\u6f5c\u4f0f\u540e\u7a81\u7136\u7ed3\u6676\uff09\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u4e2d\u7684\u6d1e\u5bdf\u73b0\u8c61\u76f8\u4f3c\uff0c\u4e3a\u7406\u89e3\u8bed\u8a00\u4e4b\u524d\u7684\u63a8\u7406\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.07786", "categories": ["gr-qc", "astro-ph.HE", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2602.07786", "abs": "https://arxiv.org/abs/2602.07786", "authors": ["Ao Liu", "Xudong Zhang", "Cuihong Wen", "Wentao Liu", "Jieci Wang"], "title": "Accelerating Black Hole Image Generation via Latent Space Diffusion Models", "comment": "11 pages, 6 figures", "summary": "Interpreting horizon-scale black hole images currently relies on computationally intensive General Relativistic Ray Tracing (GRRT) simulations, which pose a significant bottleneck for rapid parameter exploration and high-precision tests of strong-field gravity. We demonstrate that physically accurate black hole images, synthesized from magnetized accretion flows, inherently reside on a low-dimensional manifold-encoding the essential features of spacetime geometry, plasma distribution, and relativistic emission. Leveraging this structure, we introduce a physics-conditioned diffusion model that operates in a compact latent space to generate high-fidelity black hole imagery directly from physical parameters. The model accurately reproduces critical observational signatures from full GRRT simulations-such as shadow diameter, photon-ring structure, and relativistic brightness asymmetry-while achieving over a fourfold reduction in computational expense. Compared with the previous generation of denoising diffusion models, the proposed approach achieves significant improvements in image quality, reconstruction fidelity, and parameter estimation accuracy, while reducing the average inference time per black hole image from 5.25 seconds to 1.15 seconds. Our work establishes diffusion-based latent models as efficient and scalable substitutes for traditional radiative transfer solvers, offering a practical framework toward real-time modeling and inference for next-generation black hole imaging.", "AI": {"tldr": "\u63d0\u51fa\u7269\u7406\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u5728\u7d27\u51d1\u6f5c\u5728\u7a7a\u95f4\u4e2d\u751f\u6210\u9ad8\u4fdd\u771f\u9ed1\u6d1e\u56fe\u50cf\uff0c\u76f8\u6bd4\u4f20\u7edfGRRT\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u964d\u4f4e4\u500d\u4ee5\u4e0a\uff0c\u63a8\u7406\u65f6\u95f4\u4ece5.25\u79d2\u51cf\u5c11\u52301.15\u79d2\u3002", "motivation": "\u4f20\u7edf\u5e7f\u4e49\u76f8\u5bf9\u8bba\u5149\u7ebf\u8ffd\u8e2a\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u9ed1\u6d1e\u56fe\u50cf\u53c2\u6570\u63a2\u7d22\u548c\u5f3a\u573a\u5f15\u529b\u6d4b\u8bd5\u7684\u6548\u7387\uff0c\u9700\u8981\u66f4\u5feb\u901f\u51c6\u786e\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u9ed1\u6d1e\u56fe\u50cf\u56fa\u6709\u7684\u4f4e\u7ef4\u6d41\u5f62\u7ed3\u6784\uff0c\u63d0\u51fa\u7269\u7406\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u5728\u7d27\u51d1\u6f5c\u5728\u7a7a\u95f4\u4e2d\u76f4\u63a5\u4ece\u7269\u7406\u53c2\u6570\u751f\u6210\u9ad8\u4fdd\u771f\u9ed1\u6d1e\u56fe\u50cf\u3002", "result": "\u6a21\u578b\u51c6\u786e\u518d\u73b0\u4e86\u5b8c\u6574GRRT\u6a21\u62df\u7684\u5173\u952e\u89c2\u6d4b\u7279\u5f81\uff08\u9634\u5f71\u76f4\u5f84\u3001\u5149\u5b50\u73af\u7ed3\u6784\u3001\u76f8\u5bf9\u8bba\u4eae\u5ea6\u4e0d\u5bf9\u79f0\u6027\uff09\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e4\u500d\u4ee5\u4e0a\uff0c\u63a8\u7406\u65f6\u95f4\u4ece5.25\u79d2\u51cf\u5c11\u52301.15\u79d2\uff0c\u56fe\u50cf\u8d28\u91cf\u3001\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u53c2\u6570\u4f30\u8ba1\u7cbe\u5ea6\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u6269\u6563\u57fa\u6f5c\u5728\u6a21\u578b\u53ef\u4f5c\u4e3a\u4f20\u7edf\u8f90\u5c04\u4f20\u8f93\u6c42\u89e3\u5668\u7684\u9ad8\u6548\u53ef\u6269\u5c55\u66ff\u4ee3\u65b9\u6848\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u9ed1\u6d1e\u6210\u50cf\u63d0\u4f9b\u5b9e\u65f6\u5efa\u6a21\u548c\u63a8\u65ad\u7684\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2602.08601", "categories": ["physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.08601", "abs": "https://arxiv.org/abs/2602.08601", "authors": ["Manabu Machida", "Koichi Mayumi"], "title": "Tikhonov regularization-based reconstruction of partial scattering functions obtained from contrast variation small-angle neutron scattering", "comment": null, "summary": "Contrast variation small-angle neutron scattering (CV-SANS) has been widely employed for nano structural analysis of multicomponent systems. In CV-SANS experiments, scattering intensities of samples with different scattering co\\ ntrasts are decomposed into partial scattering functions, corresponding to structure of each component and cross-correlation between different components, by singular value decomposition (SVD). However, the estimation of partial scattering functions with small absolute values often suffers from instability due to the significant differences in the singular values. In this paper, we propose a remedy for this instability by introducing the Tikhonov regularization, which ensures more stable reconstruction of the partial scattering functions.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528Tikhonov\u6b63\u5219\u5316\u6539\u8fdb\u5bf9\u6bd4\u5ea6\u53d8\u5316\u5c0f\u89d2\u4e2d\u5b50\u6563\u5c04\u4e2d\u7684\u90e8\u5206\u6563\u5c04\u51fd\u6570\u91cd\u5efa\u7a33\u5b9a\u6027", "motivation": "\u4f20\u7edfCV-SANS\u65b9\u6cd5\u4e2d\uff0c\u901a\u8fc7\u5947\u5f02\u503c\u5206\u89e3\u4f30\u8ba1\u90e8\u5206\u6563\u5c04\u51fd\u6570\u65f6\uff0c\u5f53\u67d0\u4e9b\u90e8\u5206\u6563\u5c04\u51fd\u6570\u7684\u7edd\u5bf9\u503c\u8f83\u5c0f\u65f6\uff0c\u7531\u4e8e\u5947\u5f02\u503c\u5dee\u5f02\u663e\u8457\uff0c\u4f1a\u5bfc\u81f4\u91cd\u5efa\u7ed3\u679c\u4e0d\u7a33\u5b9a", "method": "\u5f15\u5165Tikhonov\u6b63\u5219\u5316\u65b9\u6cd5\u6765\u7a33\u5b9a\u90e8\u5206\u6563\u5c04\u51fd\u6570\u7684\u91cd\u5efa\u8fc7\u7a0b", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u7a33\u5b9a\u5730\u91cd\u5efa\u90e8\u5206\u6563\u5c04\u51fd\u6570\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u7edd\u5bf9\u503c\u8f83\u5c0f\u7684\u51fd\u6570", "conclusion": "Tikhonov\u6b63\u5219\u5316\u662f\u89e3\u51b3CV-SANS\u4e2d\u90e8\u5206\u6563\u5c04\u51fd\u6570\u91cd\u5efa\u4e0d\u7a33\u5b9a\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5"}}
{"id": "2602.07281", "categories": ["quant-ph", "cond-mat.quant-gas", "nlin.PS", "physics.optics"], "pdf": "https://arxiv.org/pdf/2602.07281", "abs": "https://arxiv.org/abs/2602.07281", "authors": ["H. Sakaguchi", "B. A. Malomed", "A. C. Aristotelous", "E. G. Charalampidis"], "title": "The continuous spectrum of bound states in expulsive potentials", "comment": "11 pages, 8 figures", "summary": "On the contrary to the common intuition that a steep expulsive potential makes quantum states widely delocalized, we demonstrate that one- and two-dimensional (1D and 2D) Schr\u00f6dinger equations, which include expulsive potentials that are \\emph{steeper than the quadratic} (anti-harmonic-oscillator) ones, give rise to \\emph{normalizable} (effectively localized) eigenstates. These states constitute full continuous spectra in the 1D and 2D cases alike. In 1D, these are spatially even and odd eigenstates. The 2D states may carry any value of the vorticity (alias magnetic quantum number). Asymptotic approximations for wave functions of the 1D and 2D eigenstates, valid far from the center, are derived analytically, demonstrating excellent agreement with numerically found counterparts. Special exact solutions for vortex states are obtained in the 2D case. These findings suggest an extension of the concept of bound states in the continuum, in quantum mechanics and paraxial photonics. Gross-Pitaevskii equations are considered as the nonlinear extension of the 1D and 2D settings. In 1D, the cubic nonlinearity slightly deforms the eigenstates, maintaining their stability. On the other hand, the quintic self-focusing term, which occurs in the photonic version of the 1D model, initiates the dynamical collapse of states whose norm exceeds a critical value.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u6bd4\u4e8c\u6b21\u53cd\u8c10\u632f\u5b50\u66f4\u9661\u5ced\u7684\u6392\u65a5\u52bf\u80fd\u53ef\u4ee5\u4ea7\u751f\u53ef\u5f52\u4e00\u5316\u7684\u5c40\u57df\u5316\u672c\u5f81\u6001\uff0c\u8fd9\u4e9b\u6001\u6784\u6210\u8fde\u7eed\u8c31\uff0c\u5e76\u63d0\u51fa\u4e86\u8fde\u7eed\u8c31\u675f\u7f1a\u6001\u6982\u5ff5\u7684\u6269\u5c55\u3002", "motivation": "\u4f20\u7edf\u76f4\u89c9\u8ba4\u4e3a\u9661\u5ced\u7684\u6392\u65a5\u52bf\u4f1a\u4f7f\u91cf\u5b50\u6001\u9ad8\u5ea6\u79bb\u57df\u5316\uff0c\u4f46\u672c\u6587\u6311\u6218\u8fd9\u4e00\u89c2\u70b9\uff0c\u63a2\u7d22\u6bd4\u4e8c\u6b21\u53cd\u8c10\u632f\u5b50\u66f4\u9661\u5ced\u7684\u6392\u65a5\u52bf\u80fd\u5426\u4ea7\u751f\u5c40\u57df\u5316\u7684\u672c\u5f81\u6001\u3002", "method": "\u7814\u7a76\u4e00\u7ef4\u548c\u4e8c\u7ef4\u859b\u5b9a\u8c14\u65b9\u7a0b\u4e2d\u6bd4\u4e8c\u6b21\u53cd\u8c10\u632f\u5b50\u66f4\u9661\u5ced\u7684\u6392\u65a5\u52bf\uff0c\u63a8\u5bfc\u672c\u5f81\u6001\u7684\u6e10\u8fd1\u8fd1\u4f3c\u89e3\uff0c\u83b7\u5f97\u4e8c\u7ef4\u6da1\u65cb\u6001\u7684\u7279\u6b8a\u7cbe\u786e\u89e3\uff0c\u5e76\u8003\u8651\u975e\u7ebf\u6027Gross-Pitaevskii\u65b9\u7a0b\u7684\u6269\u5c55\u3002", "result": "\u53d1\u73b0\u8fd9\u4e9b\u9661\u5ced\u6392\u65a5\u52bf\u786e\u5b9e\u80fd\u4ea7\u751f\u53ef\u5f52\u4e00\u5316\u7684\u5c40\u57df\u5316\u672c\u5f81\u6001\uff0c\u6784\u6210\u8fde\u7eed\u8c31\uff1b\u4e00\u7ef4\u6709\u7a7a\u95f4\u5076\u5947\u5bf9\u79f0\u6001\uff0c\u4e8c\u7ef4\u53ef\u643a\u5e26\u4efb\u610f\u6da1\u65cb\u5ea6\uff1b\u6e10\u8fd1\u8fd1\u4f3c\u4e0e\u6570\u503c\u89e3\u9ad8\u5ea6\u4e00\u81f4\uff1b\u975e\u7ebf\u6027\u60c5\u51b5\u4e0b\uff0c\u7acb\u65b9\u975e\u7ebf\u6027\u4fdd\u6301\u7a33\u5b9a\u6027\uff0c\u800c\u4e94\u6b21\u81ea\u805a\u7126\u9879\u4f1a\u5bfc\u81f4\u8d85\u8fc7\u4e34\u754c\u503c\u7684\u6001\u53d1\u751f\u52a8\u529b\u5b66\u574d\u7f29\u3002", "conclusion": "\u7814\u7a76\u6269\u5c55\u4e86\u8fde\u7eed\u8c31\u675f\u7f1a\u6001\u7684\u6982\u5ff5\uff0c\u8868\u660e\u9661\u5ced\u6392\u65a5\u52bf\u80fd\u4ea7\u751f\u5c40\u57df\u5316\u672c\u5f81\u6001\uff0c\u4e3a\u91cf\u5b50\u529b\u5b66\u548c\u508d\u8f74\u5149\u5b50\u5b66\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5e76\u63ed\u793a\u4e86\u975e\u7ebf\u6027\u6548\u5e94\u5bf9\u8fd9\u4e9b\u6001\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u3002"}}
{"id": "2602.07063", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.07063", "abs": "https://arxiv.org/abs/2602.07063", "authors": ["Serkan Sulun"], "title": "Video-based Music Generation", "comment": "PhD thesis, University of Porto", "summary": "As the volume of video content on the internet grows rapidly, finding a suitable soundtrack remains a significant challenge. This thesis presents EMSYNC (EMotion and SYNChronization), a fast, free, and automatic solution that generates music tailored to the input video, enabling content creators to enhance their productions without composing or licensing music. Our model creates music that is emotionally and rhythmically synchronized with the video. A core component of EMSYNC is a novel video emotion classifier. By leveraging pretrained deep neural networks for feature extraction and keeping them frozen while training only fusion layers, we reduce computational complexity while improving accuracy. We show the generalization abilities of our method by obtaining state-of-the-art results on Ekman-6 and MovieNet. Another key contribution is a large-scale, emotion-labeled MIDI dataset for affective music generation. We then present an emotion-based MIDI generator, the first to condition on continuous emotional values rather than discrete categories, enabling nuanced music generation aligned with complex emotional content. To enhance temporal synchronization, we introduce a novel temporal boundary conditioning method, called \"boundary offset encodings,\" aligning musical chords with scene changes. Combining video emotion classification, emotion-based music generation, and temporal boundary conditioning, EMSYNC emerges as a fully automatic video-based music generator. User studies show that it consistently outperforms existing methods in terms of music richness, emotional alignment, temporal synchronization, and overall preference, setting a new state-of-the-art in video-based music generation.", "AI": {"tldr": "EMSYNC\u662f\u4e00\u4e2a\u5feb\u901f\u3001\u514d\u8d39\u3001\u81ea\u52a8\u5316\u7684\u89c6\u9891\u914d\u4e50\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u60c5\u611f\u5206\u7c7b\u3001\u60c5\u611f\u97f3\u4e50\u751f\u6210\u548c\u65f6\u95f4\u540c\u6b65\u6280\u672f\uff0c\u4e3a\u89c6\u9891\u521b\u5efa\u60c5\u611f\u548c\u8282\u594f\u540c\u6b65\u7684\u97f3\u4e50\u3002", "motivation": "\u968f\u7740\u7f51\u7edc\u89c6\u9891\u5185\u5bb9\u5feb\u901f\u589e\u957f\uff0c\u5bfb\u627e\u5408\u9002\u7684\u914d\u4e50\u6210\u4e3a\u91cd\u8981\u6311\u6218\u3002\u5185\u5bb9\u521b\u4f5c\u8005\u9700\u8981\u65e0\u9700\u4f5c\u66f2\u6216\u6388\u6743\u5373\u53ef\u589e\u5f3a\u89c6\u9891\u5236\u4f5c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u65b0\u9896\u7684\u89c6\u9891\u60c5\u611f\u5206\u7c7b\u5668\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u63d0\u53d6\u7279\u5f81\uff0c\u4ec5\u8bad\u7ec3\u878d\u5408\u5c42\uff1b2) \u57fa\u4e8e\u8fde\u7eed\u60c5\u611f\u503c\u800c\u975e\u79bb\u6563\u7c7b\u522b\u7684MIDI\u751f\u6210\u5668\uff1b3) \u65f6\u95f4\u8fb9\u754c\u6761\u4ef6\u65b9\u6cd5\uff08\u8fb9\u754c\u504f\u79fb\u7f16\u7801\uff09\uff0c\u5c06\u97f3\u4e50\u548c\u5f26\u4e0e\u573a\u666f\u53d8\u5316\u5bf9\u9f50\u3002", "result": "\u5728Ekman-6\u548cMovieNet\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u6700\u5148\u8fdb\u7ed3\u679c\uff1b\u7528\u6237\u7814\u7a76\u8868\u660e\u5728\u97f3\u4e50\u4e30\u5bcc\u5ea6\u3001\u60c5\u611f\u5bf9\u9f50\u3001\u65f6\u95f4\u540c\u6b65\u548c\u6574\u4f53\u504f\u597d\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "EMSYNC\u4f5c\u4e3a\u5168\u81ea\u52a8\u89c6\u9891\u97f3\u4e50\u751f\u6210\u5668\uff0c\u5728\u89c6\u9891\u914d\u4e50\u751f\u6210\u9886\u57df\u8bbe\u5b9a\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6807\u51c6\uff0c\u4e3a\u5185\u5bb9\u521b\u4f5c\u8005\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u97f3\u4e50\u521b\u4f5c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07806", "categories": ["gr-qc", "hep-th"], "pdf": "https://arxiv.org/pdf/2602.07806", "abs": "https://arxiv.org/abs/2602.07806", "authors": ["Faizuddin Ahmed", "Ahmad Al-Badawi", "Edilberto O. Silva"], "title": "Geodesic Structure, Thermodynamics and Scalar Perturbations of Mod(A)Max black hole Surrounded by Perfect Fluid Dark Matter", "comment": "47 pages, 6 tables, 23 figures", "summary": "In this work, we investigate the optical properties of a spherically symmetric Mod(A)Max black hole surrounded by perfect fluid dark matter, focusing on key features such as the photon sphere radius, shadow, photon trajectories, and the effective radial force experienced by photons. We also study the dynamics of massive particles around the black hole, deriving the effective potential and, from it, the specific energy and angular momentum of particles moving in circular orbits of fixed radii is discussed. The conditions for marginally stable circular orbits are analyzed, highlighting how the geometric parameters that modify the spacetime curvature influence both the optical and dynamical features. Furthermore, we explore the thermodynamic behavior of the black hole by examining its temperature, Gibbs free energy, and heat capacity, as well as its thermodynamic topology. Finally, scalar field perturbations are considered through the massless Klein-Gordon equation, and the quasinormal modes (QNMs) in the eikonal regime are computed, illustrating how the geometric parameters affect the potential and the QNM spectra.", "AI": {"tldr": "\u7814\u7a76\u4fee\u6b63Maxwell\u7406\u8bba\u9ed1\u6d1e\u5728\u5b8c\u7f8e\u6d41\u4f53\u6697\u7269\u8d28\u73af\u5883\u4e2d\u7684\u5149\u5b66\u6027\u8d28\u3001\u7c92\u5b50\u52a8\u529b\u5b66\u3001\u70ed\u529b\u5b66\u884c\u4e3a\u548c\u51c6\u6b63\u5219\u6a21", "motivation": "\u63a2\u7d22\u4fee\u6b63Maxwell\u7406\u8bba\uff08Mod(A)Max\uff09\u9ed1\u6d1e\u5728\u5b8c\u7f8e\u6d41\u4f53\u6697\u7269\u8d28\u73af\u5883\u4e2d\u7684\u7269\u7406\u7279\u6027\uff0c\u5305\u62ec\u5149\u5b66\u73b0\u8c61\u3001\u7c92\u5b50\u8f68\u9053\u7a33\u5b9a\u6027\u3001\u70ed\u529b\u5b66\u6027\u8d28\u548c\u6807\u91cf\u573a\u6270\u52a8\uff0c\u4ee5\u7406\u89e3\u51e0\u4f55\u53c2\u6570\u5982\u4f55\u5f71\u54cd\u8fd9\u4e9b\u7279\u5f81", "method": "\u91c7\u7528\u7406\u8bba\u5206\u6790\u65b9\u6cd5\uff0c\u7814\u7a76\u7403\u5bf9\u79f0Mod(A)Max\u9ed1\u6d1e\u7684\u5149\u5b50\u7403\u534a\u5f84\u3001\u9634\u5f71\u3001\u5149\u5b50\u8f68\u8ff9\u548c\u6709\u6548\u5f84\u5411\u529b\uff1b\u5206\u6790\u5927\u8d28\u91cf\u7c92\u5b50\u7684\u6709\u6548\u52bf\u80fd\u3001\u5706\u5f62\u8f68\u9053\u80fd\u91cf\u548c\u89d2\u52a8\u91cf\uff1b\u8ba1\u7b97\u9ed1\u6d1e\u6e29\u5ea6\u3001\u5409\u5e03\u65af\u81ea\u7531\u80fd\u3001\u70ed\u5bb9\u548c\u70ed\u529b\u5b66\u62d3\u6251\uff1b\u901a\u8fc7\u65e0\u8d28\u91cf\u514b\u83b1\u56e0-\u6208\u767b\u65b9\u7a0b\u7814\u7a76\u6807\u91cf\u573a\u6270\u52a8\uff0c\u8ba1\u7b97eikonal\u533a\u57df\u7684\u51c6\u6b63\u5219\u6a21", "result": "\u7cfb\u7edf\u5206\u6790\u4e86\u4fee\u6b63Maxwell\u9ed1\u6d1e\u5728\u6697\u7269\u8d28\u73af\u5883\u4e2d\u7684\u5149\u5b66\u7279\u6027\u3001\u7c92\u5b50\u52a8\u529b\u5b66\u884c\u4e3a\u3001\u70ed\u529b\u5b66\u6027\u8d28\u548c\u51c6\u6b63\u5219\u6a21\u8c31\uff0c\u63ed\u793a\u4e86\u51e0\u4f55\u53c2\u6570\u5bf9\u8fd9\u4e9b\u7269\u7406\u7279\u5f81\u7684\u5f71\u54cd\u89c4\u5f8b", "conclusion": "\u51e0\u4f55\u53c2\u6570\u663e\u8457\u5f71\u54cd\u9ed1\u6d1e\u7684\u5149\u5b66\u7279\u5f81\u3001\u7c92\u5b50\u8f68\u9053\u7a33\u5b9a\u6027\u3001\u70ed\u529b\u5b66\u884c\u4e3a\u548c\u51c6\u6b63\u5219\u6a21\u8c31\uff0c\u4e3a\u7406\u89e3\u4fee\u6b63\u5f15\u529b\u7406\u8bba\u5728\u6697\u7269\u8d28\u73af\u5883\u4e2d\u7684\u7269\u7406\u6548\u5e94\u63d0\u4f9b\u4e86\u5168\u9762\u6846\u67b6"}}
{"id": "2602.07357", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.07357", "abs": "https://arxiv.org/abs/2602.07357", "authors": ["Shashank Sanjay Bhat", "Peiyong Wang", "Joseph West", "Udaya Parampalli"], "title": "Encoding Matters: Benchmarking Binary and D-ary Representations for Quantum Combinatorial Optimization", "comment": null, "summary": "Combinatorial optimization problems are typically formulated using Quadratic Unconstrained Binary Optimization (QUBO), where constraints are enforced through penalty terms that introduce auxiliary variables and rapidly increase Hamiltonian complexity, limiting scalability on near term quantum devices. In this work, we systematically study Quadratic Unconstrained D-ary Optimization (QUDO) as an alternative formulation in which decision variables are encoded directly in higher dimensional Hilbert spaces. We demonstrate that QUDO naturally captures structural constraints across a range of problem classes, including the Traveling Salesman Problem, two variants of the Vehicle Routing Problem, graph coloring, job scheduling, and Max-K-Cut, without the need for extensive penalty constructions. Using a qudit-level implementation of the Quantum Approximate Optimization Algorithm (qudit QAOA), we benchmark these formulations against their binary QUBO counterparts and exact classical solutions. Our study show consistently improved approximation ratios and substantially reduced computational overhead at comparable circuit depths, highlighting QUDO as a scalable and expressive representation for quantum combinatorial optimization.", "AI": {"tldr": "QUDO\uff08\u4e8c\u6b21\u65e0\u7ea6\u675fD\u5143\u4f18\u5316\uff09\u4f5c\u4e3aQUBO\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u5728\u9ad8\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u76f4\u63a5\u7f16\u7801\u51b3\u7b56\u53d8\u91cf\uff0c\u907f\u514d\u60e9\u7f5a\u9879\u548c\u8f85\u52a9\u53d8\u91cf\uff0c\u5728\u91cf\u5b50\u7ec4\u5408\u4f18\u5316\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u8868\u8fbe\u6027\u3002", "motivation": "\u4f20\u7edfQUBO\u65b9\u6cd5\u901a\u8fc7\u60e9\u7f5a\u9879\u5f15\u5165\u7ea6\u675f\uff0c\u9700\u8981\u5927\u91cf\u8f85\u52a9\u53d8\u91cf\uff0c\u589e\u52a0\u4e86\u54c8\u5bc6\u987f\u91cf\u7684\u590d\u6742\u5ea6\uff0c\u9650\u5236\u4e86\u8fd1\u671f\u91cf\u5b50\u8bbe\u5907\u7684\u53ef\u6269\u5c55\u6027\u3002\u9700\u8981\u5bfb\u627e\u66f4\u9ad8\u6548\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u7cfb\u7edf\u7814\u7a76QUDO\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u9ad8\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u76f4\u63a5\u7f16\u7801\u51b3\u7b56\u53d8\u91cf\u3002\u4f7f\u7528qudit QAOA\uff08\u91cf\u5b50\u8fd1\u4f3c\u4f18\u5316\u7b97\u6cd5\uff09\u5b9e\u73b0\uff0c\u5e76\u5728\u65c5\u884c\u5546\u95ee\u9898\u3001\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u3001\u56fe\u7740\u8272\u3001\u4f5c\u4e1a\u8c03\u5ea6\u3001Max-K-Cut\u7b49\u591a\u4e2a\u95ee\u9898\u7c7b\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u4e0e\u4e8c\u8fdb\u5236QUBO\u76f8\u6bd4\uff0cQUDO\u5728\u53ef\u6bd4\u8f83\u7684\u7535\u8def\u6df1\u5ea6\u4e0b\uff0c\u6301\u7eed\u83b7\u5f97\u6539\u8fdb\u7684\u8fd1\u4f3c\u6bd4\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002QUDO\u80fd\u591f\u81ea\u7136\u6355\u83b7\u7ed3\u6784\u7ea6\u675f\uff0c\u65e0\u9700\u590d\u6742\u7684\u60e9\u7f5a\u6784\u9020\u3002", "conclusion": "QUDO\u662f\u91cf\u5b50\u7ec4\u5408\u4f18\u5316\u7684\u53ef\u6269\u5c55\u4e14\u8868\u8fbe\u6027\u5f3a\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u4e3a\u8fd1\u671f\u91cf\u5b50\u8bbe\u5907\u4e0a\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07070", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07070", "abs": "https://arxiv.org/abs/2602.07070", "authors": ["Vladimer Khasia"], "title": "Hybrid Dual-Path Linear Transformations for Efficient Transformer Architectures", "comment": null, "summary": "Standard Transformer architectures rely heavily on dense linear transformations, treating feature projection as a monolithic, full-rank operation. We argue that this formulation is inefficient and lacks the structural inductive bias necessary for distinguishing between local feature preservation and global context integration. To address this, we introduce the Hybrid Dual-Path Linear (HDPL) operator, which decomposes the affine transformation into two topologically distinct pathways: a sparse block-diagonal component for high-rank local processing, and a low-rank Variational Autoencoder (VAE) bottleneck for global context regularization. By \"surgically\" replacing specific projections (Query, Key, Value, Gate, Up) with HDPL operators while retaining standard dense layers for aggregation (Output, Down), we achieve a superior balance of efficiency and representational power. Experiments on the FineWeb-Edu dataset demonstrate that the HDPL architecture outperforms a standard Llama-style baseline, reducing validation loss while simultaneously reducing parameter count by 6.8%. Beyond immediate performance gains, we discuss how the explicit materialization of a probabilistic latent space within the Transformer backbone serves as a vital architectural affordance, offering new pathways for inference-time or hypernetwork induced control, continual adaptation, interpretability, and cross-model or cross-modal synchronization. The code is available at https://github.com/VladimerKhasia/HDPL", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faHDPL\u7b97\u5b50\uff0c\u5c06Transformer\u4e2d\u7684\u5bc6\u96c6\u7ebf\u6027\u53d8\u6362\u5206\u89e3\u4e3a\u7a00\u758f\u5757\u5bf9\u89d2\u5c40\u90e8\u5904\u7406\u548c\u4f4e\u79e9VAE\u74f6\u9888\u5168\u5c40\u6b63\u5219\u5316\u4e24\u6761\u8def\u5f84\uff0c\u5728\u51cf\u5c116.8%\u53c2\u6570\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u6807\u51c6Transformer\u67b6\u6784\u4f9d\u8d56\u5bc6\u96c6\u7ebf\u6027\u53d8\u6362\uff0c\u5c06\u7279\u5f81\u6295\u5f71\u89c6\u4e3a\u5355\u4e00\u7684\u5168\u79e9\u64cd\u4f5c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u4e14\u7f3a\u4e4f\u533a\u5206\u5c40\u90e8\u7279\u5f81\u4fdd\u7559\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u6574\u5408\u7684\u7ed3\u6784\u5f52\u7eb3\u504f\u7f6e\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u53cc\u8def\u5f84\u7ebf\u6027\uff08HDPL\uff09\u7b97\u5b50\uff0c\u5c06\u4eff\u5c04\u53d8\u6362\u5206\u89e3\u4e3a\u4e24\u4e2a\u62d3\u6251\u4e0d\u540c\u7684\u8def\u5f84\uff1a\u7528\u4e8e\u9ad8\u79e9\u5c40\u90e8\u5904\u7406\u7684\u7a00\u758f\u5757\u5bf9\u89d2\u7ec4\u4ef6\uff0c\u4ee5\u53ca\u7528\u4e8e\u5168\u5c40\u4e0a\u4e0b\u6587\u6b63\u5219\u5316\u7684\u4f4e\u79e9\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u74f6\u9888\u3002\u901a\u8fc7\"\u5916\u79d1\u624b\u672f\u5f0f\"\u5730\u66ff\u6362\u7279\u5b9a\u6295\u5f71\uff08Query\u3001Key\u3001Value\u3001Gate\u3001Up\uff09\u4e3aHDPL\u7b97\u5b50\uff0c\u540c\u65f6\u4fdd\u7559\u6807\u51c6\u5bc6\u96c6\u5c42\u7528\u4e8e\u805a\u5408\uff08Output\u3001Down\uff09\u3002", "result": "\u5728FineWeb-Edu\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHDPL\u67b6\u6784\u4f18\u4e8e\u6807\u51c6\u7684Llama\u98ce\u683c\u57fa\u7ebf\uff0c\u5728\u51cf\u5c116.8%\u53c2\u6570\u7684\u540c\u65f6\u964d\u4f4e\u4e86\u9a8c\u8bc1\u635f\u5931\u3002", "conclusion": "HDPL\u5728\u6548\u7387\u548c\u8868\u793a\u80fd\u529b\u4e4b\u95f4\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5e73\u8861\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u5728Transformer\u9aa8\u5e72\u4e2d\u663e\u5f0f\u5b9e\u73b0\u6982\u7387\u6f5c\u5728\u7a7a\u95f4\u4e3a\u63a8\u7406\u65f6\u63a7\u5236\u3001\u8d85\u7f51\u7edc\u8bf1\u5bfc\u63a7\u5236\u3001\u6301\u7eed\u9002\u5e94\u3001\u53ef\u89e3\u91ca\u6027\u4ee5\u53ca\u8de8\u6a21\u578b/\u8de8\u6a21\u6001\u540c\u6b65\u63d0\u4f9b\u4e86\u65b0\u7684\u67b6\u6784\u53ef\u80fd\u6027\u3002"}}
{"id": "2602.07923", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2602.07923", "abs": "https://arxiv.org/abs/2602.07923", "authors": ["Long-Yue Li", "Li-Ming Cao", "Yungui Gong", "Xia-Yuan Liu", "Wenting Zhou"], "title": "Signatures of the Israel Junction II: Double Photon Rings in Slowly Rotating Kerr Spacetime with Thin Shell", "comment": "25 pages, 8 figures", "summary": "Applying the junction conditions to the slowly rotating Kerr spacetime with a thin shell, we find that while the angular momentum $L$ and Carter constant $C$ of the ray remain unchanged upon crossing the shell, its energy $E$ does not. Consequently, the impact parameters $\u03b7=L/E$ and $\u03be=C/E^2$ of the ray are discontinued at the shell. Utilizing this transformation, we study the shadow of this spacetime and the corresponding images from an equatorial thin accretion disk. The presence of the shell gives rise to distinctive features in the observed images. Notably, we observe distinct double photon rings in the images, which can gradually merge into a single ring. Moreover, the shadow boundaries and the photon rings do not exhibit a one-to-one correspondence. The abrupt changes in redshift factor and the truncated photon regions profoundly influence the image, producing distinctive features such as the step-like structures. These features in shell-equipped spacetimes can help evaluate, through future astronomical observations, the applicability of the Israel junction condition and the shell model in real astrophysical systems.", "AI": {"tldr": "\u7814\u7a76\u8584\u58f3\u65f6\u7a7a\u4e2d\u5149\u7ebf\u7a7f\u8d8a\u8584\u58f3\u65f6\u7684\u80fd\u91cf\u53d8\u5316\u53ca\u5176\u5bf9\u9ed1\u6d1e\u9634\u5f71\u548c\u5438\u79ef\u76d8\u6210\u50cf\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u8584\u58f3\u4f1a\u5bfc\u81f4\u5149\u5b50\u73af\u5206\u88c2\u3001\u7ea2\u79fb\u7a81\u53d8\u7b49\u7279\u5f81\u6027\u73b0\u8c61\u3002", "motivation": "\u7814\u7a76\u8584\u58f3\u6a21\u578b\u5728\u771f\u5b9e\u5929\u4f53\u7269\u7406\u7cfb\u7edf\u4e2d\u7684\u9002\u7528\u6027\uff0c\u901a\u8fc7\u5206\u6790\u5149\u7ebf\u7a7f\u8d8a\u8584\u58f3\u65f6\u7684\u7269\u7406\u91cf\u53d8\u5316\uff0c\u63a2\u7d22\u8584\u58f3\u5bf9\u9ed1\u6d1e\u9634\u5f71\u548c\u5438\u79ef\u76d8\u6210\u50cf\u7684\u5f71\u54cd\uff0c\u4e3a\u672a\u6765\u5929\u6587\u89c2\u6d4b\u9a8c\u8bc1\u4ee5\u8272\u5217\u8fde\u63a5\u6761\u4ef6\u548c\u8584\u58f3\u6a21\u578b\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\u3002", "method": "\u5c06\u4ee5\u8272\u5217\u8fde\u63a5\u6761\u4ef6\u5e94\u7528\u4e8e\u7f13\u6162\u65cb\u8f6c\u7684Kerr\u65f6\u7a7a\u8584\u58f3\u6a21\u578b\uff0c\u5206\u6790\u5149\u7ebf\u7a7f\u8d8a\u8584\u58f3\u65f6\u80fd\u91cf\u3001\u89d2\u52a8\u91cf\u548cCarter\u5e38\u91cf\u7684\u53d8\u5316\uff0c\u7814\u7a76\u8fd9\u4e9b\u53d8\u5316\u5bf9\u5149\u7ebf\u51b2\u51fb\u53c2\u6570\u7684\u5f71\u54cd\uff0c\u8fdb\u800c\u5206\u6790\u8584\u58f3\u65f6\u7a7a\u4e2d\u9ed1\u6d1e\u9634\u5f71\u548c\u8d64\u9053\u9762\u8584\u5438\u79ef\u76d8\u6210\u50cf\u7684\u7279\u5f81\u3002", "result": "\u53d1\u73b0\u5149\u7ebf\u7a7f\u8d8a\u8584\u58f3\u65f6\uff0c\u89d2\u52a8\u91cfL\u548cCarter\u5e38\u91cfC\u4fdd\u6301\u4e0d\u53d8\uff0c\u4f46\u80fd\u91cfE\u53d1\u751f\u53d8\u5316\uff0c\u5bfc\u81f4\u51b2\u51fb\u53c2\u6570\u03b7\u548c\u03be\u5728\u8584\u58f3\u5904\u4e0d\u8fde\u7eed\u3002\u8584\u58f3\u7684\u5b58\u5728\u4f7f\u89c2\u6d4b\u56fe\u50cf\u51fa\u73b0\u7279\u5f81\u6027\u7ed3\u6784\uff1a\u660e\u663e\u7684\u53cc\u5149\u5b50\u73af\uff08\u53ef\u9010\u6e10\u5408\u5e76\u4e3a\u5355\u73af\uff09\u3001\u9634\u5f71\u8fb9\u754c\u4e0e\u5149\u5b50\u73af\u975e\u4e00\u4e00\u5bf9\u5e94\u3001\u7ea2\u79fb\u56e0\u5b50\u7a81\u53d8\u5bfc\u81f4\u7684\u9636\u68af\u72b6\u7ed3\u6784\u3002", "conclusion": "\u8584\u58f3\u65f6\u7a7a\u4e2d\u5149\u7ebf\u7a7f\u8d8a\u8584\u58f3\u65f6\u7684\u80fd\u91cf\u53d8\u5316\u4f1a\u4ea7\u751f\u72ec\u7279\u7684\u89c2\u6d4b\u7279\u5f81\uff0c\u5982\u53cc\u5149\u5b50\u73af\u3001\u7ea2\u79fb\u7a81\u53d8\u7b49\u3002\u8fd9\u4e9b\u7279\u5f81\u53ef\u7528\u4e8e\u901a\u8fc7\u672a\u6765\u5929\u6587\u89c2\u6d4b\u8bc4\u4f30\u4ee5\u8272\u5217\u8fde\u63a5\u6761\u4ef6\u548c\u8584\u58f3\u6a21\u578b\u5728\u771f\u5b9e\u5929\u4f53\u7269\u7406\u7cfb\u7edf\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2602.07192", "categories": ["cs.LG", "cs.CE", "math.NA", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.07192", "abs": "https://arxiv.org/abs/2602.07192", "authors": ["Xiaolong He", "Haoyan Wei", "Wei Hu", "Henan Mao", "C. T. Wu"], "title": "Systematic Performance Assessment of Deep Material Networks for Multiscale Material Modeling", "comment": null, "summary": "Deep Material Networks (DMNs) are structure-preserving, mechanistic machine learning models that embed micromechanical principles into their architectures, enabling strong extrapolation capabilities and significant potential to accelerate multiscale modeling of complex microstructures. A key advantage of these models is that they can be trained exclusively on linear elastic data and then generalized to nonlinear inelastic regimes during online prediction. Despite their growing adoption, systematic evaluations of their performance across the full offline-online pipeline remain limited. This work presents a comprehensive comparative assessment of DMNs with respect to prediction accuracy, computational efficiency, and training robustness. We investigate the effects of offline training choices, including initialization, batch size, training data size, and activation regularization on online generalization performance and uncertainty. The results demonstrate that both prediction error and variance decrease with increasing training data size, while initialization and batch size can significantly influence model performance. Moreover, activation regularization is shown to play a critical role in controlling network complexity and therefore generalization performance. Compared with the original DMN, the rotation-free Interaction-based Material Network (IMN) formulation achieves a 3.4x - 4.7x speed-up in offline training, while maintaining comparable online prediction accuracy and computational efficiency. These findings clarify key trade-offs between model expressivity and efficiency in structure-preserving material networks and provide practical guidance for their deployment in multiscale material modeling.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6df1\u5ea6\u6750\u6599\u7f51\u7edc(DMNs)\u8fdb\u884c\u4e86\u5168\u9762\u7684\u6027\u80fd\u8bc4\u4f30\uff0c\u6bd4\u8f83\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3001\u8ba1\u7b97\u6548\u7387\u548c\u8bad\u7ec3\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u65cb\u8f6c\u65e0\u5173\u7684\u4ea4\u4e92\u5f0f\u6750\u6599\u7f51\u7edc(IMN)\u5728\u4fdd\u6301\u9884\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e863.4-4.7\u500d\u7684\u8bad\u7ec3\u52a0\u901f\u3002", "motivation": "\u6df1\u5ea6\u6750\u6599\u7f51\u7edc(DMNs)\u4f5c\u4e3a\u7ed3\u6784\u4fdd\u6301\u7684\u673a\u5236\u6027\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u590d\u6742\u5fae\u7ed3\u6784\u7684\u591a\u5c3a\u5ea6\u5efa\u6a21\u4e2d\u5177\u6709\u52a0\u901f\u6f5c\u529b\u3002\u5c3d\u7ba1\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5bf9\u5176\u79bb\u7ebf-\u5728\u7ebf\u5168\u6d41\u7a0b\u6027\u80fd\u7684\u7cfb\u7edf\u8bc4\u4f30\u4ecd\u7136\u6709\u9650\uff0c\u9700\u8981\u660e\u786e\u8bad\u7ec3\u9009\u62e9\u5bf9\u5728\u7ebf\u6cdb\u5316\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u5bf9DMNs\u8fdb\u884c\u5168\u9762\u7684\u6bd4\u8f83\u8bc4\u4f30\uff0c\u7814\u7a76\u79bb\u7ebf\u8bad\u7ec3\u9009\u62e9\uff08\u5305\u62ec\u521d\u59cb\u5316\u3001\u6279\u91cf\u5927\u5c0f\u3001\u8bad\u7ec3\u6570\u636e\u91cf\u548c\u6fc0\u6d3b\u6b63\u5219\u5316\uff09\u5bf9\u5728\u7ebf\u6cdb\u5316\u6027\u80fd\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u5f71\u54cd\u3002\u540c\u65f6\u5bf9\u6bd4\u539f\u59cbDMN\u4e0e\u65cb\u8f6c\u65e0\u5173\u7684\u4ea4\u4e92\u5f0f\u6750\u6599\u7f51\u7edc(IMN)\u7684\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u9884\u6d4b\u8bef\u5dee\u548c\u65b9\u5dee\u968f\u8bad\u7ec3\u6570\u636e\u91cf\u589e\u52a0\u800c\u51cf\u5c0f\uff1b\u521d\u59cb\u5316\u548c\u6279\u91cf\u5927\u5c0f\u663e\u8457\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff1b\u6fc0\u6d3b\u6b63\u5219\u5316\u5bf9\u63a7\u5236\u7f51\u7edc\u590d\u6742\u5ea6\u548c\u6cdb\u5316\u6027\u80fd\u8d77\u5173\u952e\u4f5c\u7528\uff1bIMN\u76f8\u6bd4\u539f\u59cbDMN\u5b9e\u73b0\u4e863.4-4.7\u500d\u7684\u79bb\u7ebf\u8bad\u7ec3\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u7684\u5728\u7ebf\u9884\u6d4b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u9610\u660e\u4e86\u7ed3\u6784\u4fdd\u6301\u6750\u6599\u7f51\u7edc\u4e2d\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u548c\u6548\u7387\u4e4b\u95f4\u7684\u5173\u952e\u6743\u8861\uff0c\u4e3a\u591a\u5c3a\u5ea6\u6750\u6599\u5efa\u6a21\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u8868\u660eIMN\u5728\u4fdd\u6301\u9884\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2602.07402", "categories": ["quant-ph", "physics.hist-ph"], "pdf": "https://arxiv.org/pdf/2602.07402", "abs": "https://arxiv.org/abs/2602.07402", "authors": ["Jacob A. Barandes"], "title": "The ABL Rule and the Perils of Post-Selection", "comment": "28 pages, no figures", "summary": "In 1964, Aharonov, Bergmann, and Lebowitz introduced their well-known ABL rule with the intention of providing a time-symmetric formalism for computing novel kinds of conditional probabilities in quantum theory. Later papers attached additional significance to the ABL rule, including assertions that it supported violations of the uncertainty principle. The present work challenges these claims, as well as subsequent attempts to salvage the original interpretation of the ABL rule. Taking a broader view, this paper identifies a subtle category error at the heart of the ABL rule that consists of confusing observables that belong to a single system with emergent observables that arise only for physical ensembles. Along the way, this paper points out other problems and fallacious reasoning in the research literature surrounding the ABL rule, including the misuse of post-selection, a reliance on pattern matching to classical formulas, and a posture of measurementism that takes experimental data as providing answers to interpretational questions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6311\u6218ABL\u89c4\u5219\u7684\u4f20\u7edf\u89e3\u91ca\uff0c\u6307\u51fa\u5176\u4e2d\u5b58\u5728\u8303\u7574\u9519\u8bef\uff0c\u6df7\u6dc6\u4e86\u5355\u4e2a\u7cfb\u7edf\u7684\u53ef\u89c2\u6d4b\u91cf\u4e0e\u7269\u7406\u7cfb\u7efc\u4e2d\u624d\u51fa\u73b0\u7684\u6d8c\u73b0\u53ef\u89c2\u6d4b\u91cf\u3002", "motivation": "ABL\u89c4\u5219\u88ab\u63d0\u51fa\u4f5c\u4e3a\u91cf\u5b50\u7406\u8bba\u7684\u65f6\u95f4\u5bf9\u79f0\u5f62\u5f0f\uff0c\u7528\u4e8e\u8ba1\u7b97\u65b0\u578b\u6761\u4ef6\u6982\u7387\uff0c\u4f46\u540e\u7eed\u7814\u7a76\u8d4b\u4e88\u5176\u989d\u5916\u610f\u4e49\uff0c\u5305\u62ec\u58f0\u79f0\u5b83\u652f\u6301\u8fdd\u53cd\u4e0d\u786e\u5b9a\u6027\u539f\u7406\u3002\u672c\u6587\u65e8\u5728\u6311\u6218\u8fd9\u4e9b\u4e3b\u5f20\uff0c\u5e76\u63ed\u793aABL\u89c4\u5219\u89e3\u91ca\u4e2d\u7684\u6839\u672c\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8bc6\u522bABL\u89c4\u5219\u4e2d\u7684\u8303\u7574\u9519\u8bef\uff0c\u5206\u6790\u6df7\u6dc6\u5355\u4e2a\u7cfb\u7edf\u53ef\u89c2\u6d4b\u91cf\u4e0e\u7cfb\u7efc\u6d8c\u73b0\u53ef\u89c2\u6d4b\u91cf\u7684\u6839\u672c\u95ee\u9898\u3002\u540c\u65f6\u6307\u51fa\u76f8\u5173\u6587\u732e\u4e2d\u7684\u5176\u4ed6\u95ee\u9898\uff0c\u5305\u62ec\u540e\u9009\u62e9\u8bef\u7528\u3001\u5bf9\u7ecf\u5178\u516c\u5f0f\u7684\u6a21\u5f0f\u5339\u914d\u4f9d\u8d56\uff0c\u4ee5\u53ca\u6d4b\u91cf\u4e3b\u4e49\u7acb\u573a\u3002", "result": "\u63ed\u793a\u4e86ABL\u89c4\u5219\u89e3\u91ca\u4e2d\u7684\u6839\u672c\u7f3a\u9677\uff0c\u8868\u660e\u5176\u58f0\u79f0\u652f\u6301\u8fdd\u53cd\u4e0d\u786e\u5b9a\u6027\u539f\u7406\u7b49\u4e3b\u5f20\u5b58\u5728\u95ee\u9898\u3002\u6307\u51fa\u4e86\u7814\u7a76\u6587\u732e\u4e2d\u7684\u9519\u8bef\u63a8\u7406\u548c\u6982\u5ff5\u6df7\u6dc6\u3002", "conclusion": "ABL\u89c4\u5219\u7684\u89e3\u91ca\u5b58\u5728\u6839\u672c\u6027\u9519\u8bef\uff0c\u6df7\u6dc6\u4e86\u4e0d\u540c\u5c42\u6b21\u7684\u53ef\u89c2\u6d4b\u91cf\u6982\u5ff5\u3002\u91cf\u5b50\u7406\u8bba\u4e2d\u7684\u6d4b\u91cf\u548c\u6982\u7387\u89e3\u91ca\u9700\u8981\u66f4\u8c28\u614e\u7684\u5904\u7406\uff0c\u907f\u514d\u5c06\u5b9e\u9a8c\u6570\u636e\u76f4\u63a5\u7528\u4e8e\u56de\u7b54\u89e3\u91ca\u6027\u95ee\u9898\u3002"}}
{"id": "2602.07078", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07078", "abs": "https://arxiv.org/abs/2602.07078", "authors": ["Yingru Li", "Jiawei Xu", "Ziniu Li", "Jiacai Liu", "Wei Liu", "Yuxuan Tong", "Longtao Zheng", "Zhenghai Xue", "Yaxiang Zhang", "Tianle Cai", "Ge Zhang", "Qian Liu", "Baoxiang Wang"], "title": "The Optimal Token Baseline: Variance Reduction for Long-Horizon LLM-RL", "comment": null, "summary": "Reinforcement Learning (RL) for Large Language Models (LLMs) often suffers from training collapse in long-horizon tasks due to exploding gradient variance. To mitigate this, a baseline is commonly introduced for advantage computation; however, traditional value models remain difficult to optimize, and standard group-based baselines overlook sequence heterogeneity. Although classic optimal baseline theory can achieve global variance reduction, it neglects token heterogeneity and requires prohibitive gradient-based computation. In this work, we derive the Optimal Token Baseline (OTB) from first principles, proving that gradient updates should be weighted inversely to their cumulative gradient norm. To ensure efficiency, we propose the Logit-Gradient Proxy that approximates the gradient norm using only forward-pass probabilities. Our method achieves training stability and matches the performance of large group sizes ($N=32$) with only $N=4$, reducing token consumption by over 65% across single-turn and tool-integrated reasoning tasks.", "AI": {"tldr": "\u63d0\u51faOptimal Token Baseline (OTB)\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u68af\u5ea6\u8303\u6570\u7684\u9006\u52a0\u6743\u66f4\u65b0\u89e3\u51b3LLM\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8bad\u7ec3\u5d29\u6e83\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11token\u6d88\u8017", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u5e38\u56e0\u68af\u5ea6\u65b9\u5dee\u7206\u70b8\u5bfc\u81f4\u8bad\u7ec3\u5d29\u6e83\uff0c\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\u5b58\u5728\u4f18\u5316\u56f0\u96be\u3001\u5ffd\u7565\u5e8f\u5217\u5f02\u8d28\u6027\u7b49\u95ee\u9898", "method": "\u4ece\u7b2c\u4e00\u6027\u539f\u7406\u63a8\u5bfc\u51faOptimal Token Baseline (OTB)\uff0c\u8bc1\u660e\u68af\u5ea6\u66f4\u65b0\u5e94\u6309\u5176\u7d2f\u79ef\u68af\u5ea6\u8303\u6570\u7684\u5012\u6570\u52a0\u6743\uff1b\u63d0\u51faLogit-Gradient Proxy\u4ec5\u4f7f\u7528\u524d\u5411\u4f20\u64ad\u6982\u7387\u8fd1\u4f3c\u68af\u5ea6\u8303\u6570", "result": "\u65b9\u6cd5\u5b9e\u73b0\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u4ec5\u7528N=4\u5c31\u80fd\u8fbe\u5230N=32\u5927\u7ec4\u5927\u5c0f\u7684\u6027\u80fd\uff0c\u5728\u5355\u8f6e\u548c\u5de5\u5177\u96c6\u6210\u63a8\u7406\u4efb\u52a1\u4e2d\u51cf\u5c11\u8d85\u8fc765%\u7684token\u6d88\u8017", "conclusion": "OTB\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u68af\u5ea6\u65b9\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u7a33\u5b9a\u6027"}}
{"id": "2602.08029", "categories": ["gr-qc", "astro-ph.IM", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08029", "abs": "https://arxiv.org/abs/2602.08029", "authors": ["Berthy T. Feng", "Andrew A. Chael", "David Bromley", "Aviad Levis", "William T. Freeman", "Katherine L. Bouman"], "title": "Dynamic Black-hole Emission Tomography with Physics-informed Neural Fields", "comment": null, "summary": "With the success of static black-hole imaging, the next frontier is the dynamic and 3D imaging of black holes. Recovering the dynamic 3D gas near a black hole would reveal previously-unseen parts of the universe and inform new physics models. However, only sparse radio measurements from a single viewpoint are possible, making the dynamic 3D reconstruction problem significantly ill-posed. Previously, BH-NeRF addressed the ill-posed problem by assuming Keplerian dynamics of the gas, but this assumption breaks down near the black hole, where the strong gravitational pull of the black hole and increased electromagnetic activity complicate fluid dynamics. To overcome the restrictive assumptions of BH-NeRF, we propose PI-DEF, a physics-informed approach that uses differentiable neural rendering to fit a 4D (time + 3D) emissivity field given EHT measurements. Our approach jointly reconstructs the 3D velocity field with the 4D emissivity field and enforces the velocity as a soft constraint on the dynamics of the emissivity. In experiments on simulated data, we find significantly improved reconstruction accuracy over both BH-NeRF and a physics-agnostic approach. We demonstrate how our method may be used to estimate other physics parameters of the black hole, such as its spin.", "AI": {"tldr": "PI-DEF\uff1a\u4e00\u79cd\u7269\u7406\u4fe1\u606f\u5316\u76844D\u8f90\u5c04\u573a\u91cd\u5efa\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u5c04\u7535\u6d4b\u91cf\u4e2d\u52a8\u60013D\u6210\u50cf\u9ed1\u6d1e\u5468\u56f4\u6c14\u4f53\uff0c\u76f8\u6bd4BH-NeRF\u663e\u8457\u63d0\u5347\u91cd\u5efa\u7cbe\u5ea6", "motivation": "\u73b0\u6709BH-NeRF\u65b9\u6cd5\u5047\u8bbe\u5f00\u666e\u52d2\u52a8\u529b\u5b66\uff0c\u4f46\u5728\u9ed1\u6d1e\u9644\u8fd1\u56e0\u5f3a\u5f15\u529b\u548c\u7535\u78c1\u6d3b\u52a8\u800c\u5931\u6548\uff1b\u9700\u8981\u66f4\u51c6\u786e\u7684\u52a8\u60013D\u91cd\u5efa\u65b9\u6cd5\u6765\u63ed\u793a\u9ed1\u6d1e\u5468\u56f4\u672a\u89c2\u6d4b\u533a\u57df\u5e76\u9a8c\u8bc1\u65b0\u7269\u7406\u6a21\u578b", "method": "\u4f7f\u7528\u53ef\u5fae\u5206\u795e\u7ecf\u6e32\u67d3\u62df\u54084D\uff08\u65f6\u95f4+3D\uff09\u8f90\u5c04\u573a\uff0c\u8054\u5408\u91cd\u5efa3D\u901f\u5ea6\u573a\u548c4D\u8f90\u5c04\u573a\uff0c\u5e76\u5c06\u901f\u5ea6\u4f5c\u4e3a\u8f90\u5c04\u573a\u52a8\u529b\u5b66\u7684\u8f6f\u7ea6\u675f", "result": "\u5728\u6a21\u62df\u6570\u636e\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4BH-NeRF\u548c\u7269\u7406\u65e0\u5173\u65b9\u6cd5\uff0c\u91cd\u5efa\u7cbe\u5ea6\u663e\u8457\u63d0\u5347\uff1b\u8fd8\u80fd\u7528\u4e8e\u4f30\u8ba1\u9ed1\u6d1e\u81ea\u65cb\u7b49\u5176\u4ed6\u7269\u7406\u53c2\u6570", "conclusion": "PI-DEF\u901a\u8fc7\u7269\u7406\u7ea6\u675f\u514b\u670d\u4e86BH-NeRF\u7684\u9650\u5236\u6027\u5047\u8bbe\uff0c\u4e3a\u9ed1\u6d1e\u52a8\u60013D\u6210\u50cf\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u65b9\u6cd5\uff0c\u6709\u671b\u63ed\u793a\u9ed1\u6d1e\u5468\u56f4\u65b0\u7269\u7406\u73b0\u8c61"}}
{"id": "2602.07406", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.07406", "abs": "https://arxiv.org/abs/2602.07406", "authors": ["Xinye Fan", "Shijie Xu"], "title": "Quantum Many-Body Principles of Localized-State Ensemble Luminescence", "comment": null, "summary": "Localized electron states induced by various disorders,including defects and impurities,usually exist in solids.Their optical properties,especially their luminescence properties,are of both scientific and technological significance.But a microscopic theory has not yet been established for such localized-state ensemble (LSE) luminescence.In this Letter,we attempt to fill this void via developing a quantum many-body (MB) luminescence theory taking into account both electron-phonon (e-p) and electron-electron (e-e) interactions.By using the developed MB-LSE theory,abnormal thermal behaviors such as redshift and subsequent blueshift of peak position,narrowing and succeeding broadening of linewidth,decline in intensity,and variation in lifetime can be quantitatively interpreted.The roles of electron-phonon and electron-electron interactions in the variable-temperature LSE luminescence are thus elucidated. Within the framework of the MB-LSE theory, moreover, Varshni's empirical formula for bandgap temperature dependence and Huang-Rhys factor for e-p coupling are further derived and discussed.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u7535\u5b50-\u58f0\u5b50\u548c\u7535\u5b50-\u7535\u5b50\u76f8\u4e92\u4f5c\u7528\u7684\u91cf\u5b50\u591a\u4f53\u5c40\u57df\u6001\u7cfb\u7efc\u53d1\u5149\u7406\u8bba\uff0c\u89e3\u91ca\u4e86\u5f02\u5e38\u70ed\u884c\u4e3a\u5e76\u63a8\u5bfc\u4e86\u7ecf\u9a8c\u516c\u5f0f\u3002", "motivation": "\u56fa\u4f53\u4e2d\u7531\u7f3a\u9677\u548c\u6742\u8d28\u5f15\u8d77\u7684\u5c40\u57df\u7535\u5b50\u6001\u7684\u5149\u5b66\u6027\u8d28\uff08\u5c24\u5176\u662f\u53d1\u5149\u6027\u8d28\uff09\u5177\u6709\u91cd\u8981\u79d1\u5b66\u548c\u6280\u672f\u610f\u4e49\uff0c\u4f46\u7f3a\u4e4f\u5fae\u89c2\u7406\u8bba\u6765\u89e3\u91ca\u5c40\u57df\u6001\u7cfb\u7efc\u53d1\u5149\u73b0\u8c61\u3002", "method": "\u53d1\u5c55\u4e86\u8003\u8651\u7535\u5b50-\u58f0\u5b50\u548c\u7535\u5b50-\u7535\u5b50\u76f8\u4e92\u4f5c\u7528\u7684\u91cf\u5b50\u591a\u4f53\u5c40\u57df\u6001\u7cfb\u7efc\u53d1\u5149\u7406\u8bba\uff08MB-LSE\u7406\u8bba\uff09\u3002", "result": "\u8be5\u7406\u8bba\u80fd\u591f\u5b9a\u91cf\u89e3\u91ca\u5cf0\u503c\u4f4d\u7f6e\u7ea2\u79fb\u540e\u84dd\u79fb\u3001\u7ebf\u5bbd\u5148\u53d8\u7a84\u540e\u53d8\u5bbd\u3001\u5f3a\u5ea6\u4e0b\u964d\u548c\u5bff\u547d\u53d8\u5316\u7b49\u5f02\u5e38\u70ed\u884c\u4e3a\uff0c\u9610\u660e\u4e86\u7535\u5b50-\u58f0\u5b50\u548c\u7535\u5b50-\u7535\u5b50\u76f8\u4e92\u4f5c\u7528\u5728\u53d8\u6e29\u53d1\u5149\u4e2d\u7684\u4f5c\u7528\u3002", "conclusion": "MB-LSE\u7406\u8bba\u586b\u8865\u4e86\u5c40\u57df\u6001\u7cfb\u7efc\u53d1\u5149\u5fae\u89c2\u7406\u8bba\u7684\u7a7a\u767d\uff0c\u5e76\u8fdb\u4e00\u6b65\u63a8\u5bfc\u548c\u8ba8\u8bba\u4e86Varshni\u7ecf\u9a8c\u516c\u5f0f\u548c\u9ec4-\u91cc\u65af\u56e0\u5b50\uff0c\u4e3a\u7406\u89e3\u5c40\u57df\u6001\u53d1\u5149\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2602.07088", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07088", "abs": "https://arxiv.org/abs/2602.07088", "authors": ["Muhammad Zafar Iqbal", "Ghazanfar Farooq Siddiqui", "Anwar Ul Haq", "Imran Razzak"], "title": "Attention-Driven Framework for Non-Rigid Medical Image Registration", "comment": null, "summary": "Deformable medical image registration is a fundamental task in medical image analysis with applications in disease diagnosis, treatment planning, and image-guided interventions. Despite significant advances in deep learning based registration methods, accurately aligning images with large deformations while preserving anatomical plausibility remains a challenging task. In this paper, we propose a novel Attention-Driven Framework for Non-Rigid Medical Image Registration (AD-RegNet) that employs attention mechanisms to guide the registration process. Our approach combines a 3D UNet backbone with bidirectional cross-attention, which establishes correspondences between moving and fixed images at multiple scales. We introduce a regional adaptive attention mechanism that focuses on anatomically relevant structures, along with a multi-resolution deformation field synthesis approach for accurate alignment. The method is evaluated on two distinct datasets: DIRLab for thoracic 4D CT scans and IXI for brain MRI scans, demonstrating its versatility across different anatomical structures and imaging modalities. Experimental results demonstrate that our approach achieves performance competitive with state-of-the-art methods on the IXI and DIRLab datasets. The proposed method maintains a favorable balance between registration accuracy and computational efficiency, making it suitable for clinical applications. A comprehensive evaluation using normalized cross-correlation (NCC), mean squared error (MSE), structural similarity (SSIM), Jacobian determinant, and target registration error (TRE) indicates that attention-guided registration improves alignment accuracy while ensuring anatomically plausible deformations.", "AI": {"tldr": "\u63d0\u51faAD-RegNet\u6ce8\u610f\u529b\u9a71\u52a8\u6846\u67b6\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u975e\u521a\u6027\u914d\u51c6\uff0c\u7ed3\u54083D UNet\u4e0e\u53cc\u5411\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u51c6\u786e\u5bf9\u9f50\u5927\u53d8\u5f62\u56fe\u50cf\u540c\u65f6\u4fdd\u6301\u89e3\u5256\u5408\u7406\u6027\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u590d\u6742\u53d8\u5f62\u5e76\u4fdd\u6301\u89e3\u5256\u7ed3\u6784\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faAD-RegNet\u6846\u67b6\uff0c\u91c7\u75283D UNet\u4f5c\u4e3a\u9aa8\u5e72\u7f51\u7edc\uff0c\u7ed3\u5408\u53cc\u5411\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5728\u591a\u5c3a\u5ea6\u4e0a\u5efa\u7acb\u79fb\u52a8\u56fe\u50cf\u4e0e\u56fa\u5b9a\u56fe\u50cf\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\u3002\u5f15\u5165\u533a\u57df\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u673a\u5236\u805a\u7126\u89e3\u5256\u76f8\u5173\u7ed3\u6784\uff0c\u4ee5\u53ca\u591a\u5206\u8fa8\u7387\u53d8\u5f62\u573a\u5408\u6210\u65b9\u6cd5\u5b9e\u73b0\u7cbe\u786e\u5bf9\u9f50\u3002", "result": "\u5728DIRLab\uff08\u80f8\u90e84D CT\uff09\u548cIXI\uff08\u8111\u90e8MRI\uff09\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u3002\u4f7f\u7528NCC\u3001MSE\u3001SSIM\u3001Jacobian\u884c\u5217\u5f0f\u548cTRE\u7b49\u6307\u6807\u7efc\u5408\u8bc4\u4f30\uff0c\u663e\u793a\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u914d\u51c6\u63d0\u9ad8\u4e86\u5bf9\u9f50\u7cbe\u5ea6\uff0c\u540c\u65f6\u786e\u4fdd\u4e86\u89e3\u5256\u5408\u7406\u7684\u53d8\u5f62\u3002", "conclusion": "AD-RegNet\u5728\u914d\u51c6\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u4fdd\u6301\u4e86\u826f\u597d\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u5e94\u7528\u3002\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u6307\u5bfc\u914d\u51c6\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u4e86\u5927\u53d8\u5f62\u60c5\u51b5\u4e0b\u7684\u5bf9\u9f50\u51c6\u786e\u6027\u3002"}}
{"id": "2602.08034", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2602.08034", "abs": "https://arxiv.org/abs/2602.08034", "authors": ["Han-Wen Hu", "Cheng-Jun Fang", "Zong-Kuan Guo"], "title": "Waveform stability of black hole ringdown with stochastic horizon structure", "comment": "9 pages, 6 figures", "summary": "We examine the robustness of black hole ringdown to stochastic horizon-scale structure within an effective field framework. Consistent with the understanding that the spectral instability of quasinormal modes does not necessarily imply observational breakdown, our results demonstrate that the macroscopic gravitational waveform remains robust. We identify the phase averaging mechanism as the physical origin of this stability, demonstrating that the spatial integration of the wave equation efficiently attenuates ultraviolet geometric details below the resolution limit of the probing wavelength. Building on the scaling law $\\mathcal{M} \\propto \u03b5^2$ and the characteristic mismatch profile with respect to $L_c$, we propose a geometric selection rule for observability: a detectable signal imposes a strict dual constraint requiring both macroscopic spatial coherence ($L_c \\sim M$) and classical-level intensity ($\u03b5\\gtrsim 10^{-4}$). This criterion quantitatively rules out the observability of incoherent, high-entropy quantum foam, suggesting that any significant ringdown deviation would serve as definitive evidence for macroscopically coherent horizon structures.", "AI": {"tldr": "\u9ed1\u6d1e\u73af\u964d\u5bf9\u968f\u673a\u89c6\u754c\u5c3a\u5ea6\u7ed3\u6784\u5177\u6709\u9c81\u68d2\u6027\uff0c\u76f8\u4f4d\u5e73\u5747\u673a\u5236\u662f\u7a33\u5b9a\u6027\u7684\u7269\u7406\u8d77\u6e90\uff0c\u53ef\u89c2\u6d4b\u4fe1\u53f7\u9700\u8981\u5b8f\u89c2\u7a7a\u95f4\u76f8\u5e72\u6027\u548c\u7ecf\u5178\u5f3a\u5ea6\u53cc\u91cd\u7ea6\u675f", "motivation": "\u7814\u7a76\u9ed1\u6d1e\u73af\u964d\u5bf9\u968f\u673a\u89c6\u754c\u5c3a\u5ea6\u7ed3\u6784\u7684\u9c81\u68d2\u6027\uff0c\u63a2\u7d22\u51c6\u6b63\u89c4\u6a21\u8c31\u4e0d\u7a33\u5b9a\u6027\u662f\u5426\u4f1a\u5bfc\u81f4\u89c2\u6d4b\u5d29\u6e83\uff0c\u7406\u89e3\u5fae\u89c2\u51e0\u4f55\u7ec6\u8282\u5982\u4f55\u5f71\u54cd\u5b8f\u89c2\u5f15\u529b\u6ce2\u5f62", "method": "\u5728\u6709\u6548\u573a\u7406\u8bba\u6846\u67b6\u5185\uff0c\u901a\u8fc7\u76f8\u4f4d\u5e73\u5747\u673a\u5236\u5206\u6790\u6ce2\u65b9\u7a0b\u7684\u7a7a\u95f4\u79ef\u5206\uff0c\u7814\u7a76\u7d2b\u5916\u51e0\u4f55\u7ec6\u8282\u7684\u8870\u51cf\uff0c\u57fa\u4e8e\u6807\u5ea6\u5f8b\u548c\u5931\u914d\u5256\u9762\u5efa\u7acb\u51e0\u4f55\u9009\u62e9\u89c4\u5219", "result": "\u5b8f\u89c2\u5f15\u529b\u6ce2\u5f62\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u6ce2\u65b9\u7a0b\u7684\u7a7a\u95f4\u79ef\u5206\u80fd\u6709\u6548\u8870\u51cf\u63a2\u6d4b\u6ce2\u957f\u5206\u8fa8\u7387\u4ee5\u4e0b\u7684\u7d2b\u5916\u51e0\u4f55\u7ec6\u8282\uff0c\u5efa\u7acb\u4e86\u53ef\u89c2\u6d4b\u6027\u51e0\u4f55\u9009\u62e9\u89c4\u5219\uff1a\u9700\u8981\u5b8f\u89c2\u7a7a\u95f4\u76f8\u5e72\u6027\u548c\u7ecf\u5178\u5f3a\u5ea6\u53cc\u91cd\u7ea6\u675f", "conclusion": "\u9ed1\u6d1e\u73af\u964d\u5bf9\u968f\u673a\u89c6\u754c\u5c3a\u5ea6\u7ed3\u6784\u5177\u6709\u7a33\u5b9a\u6027\uff0c\u4efb\u4f55\u663e\u8457\u7684\u73af\u964d\u504f\u79bb\u90fd\u5c06\u4f5c\u4e3a\u5b8f\u89c2\u76f8\u5e72\u89c6\u754c\u7ed3\u6784\u7684\u51b3\u5b9a\u6027\u8bc1\u636e\uff0c\u6392\u9664\u4e86\u975e\u76f8\u5e72\u9ad8\u71b5\u91cf\u5b50\u6ce1\u6cab\u7684\u53ef\u89c2\u6d4b\u6027"}}
{"id": "2602.07438", "categories": ["quant-ph", "cond-mat.quant-gas"], "pdf": "https://arxiv.org/pdf/2602.07438", "abs": "https://arxiv.org/abs/2602.07438", "authors": ["Saima Bashir", "Muzaffar Qadir Lone", "Prince A Ganai"], "title": "Non-Markovianity in a dressed qubit with local dephasing", "comment": null, "summary": "We study the dynamics of a dressed qubit implemented by a spinless fermion hopping between two lattice sites with each site strongly coupled to a bath of phonons. We employ Lang-Firsov transformation to make the problem tractable perturbatively. Applying time-convolutionless master equation within the polaron frame, we investigate decoherence dynamics of the dressed qubit within the singlet-triplet basis of the system for a wide range of bath spectral densities. It is shown that the coherence persists for longer time scales for large coupling values and shows non-monotonic behaviour reflecting the presence of non-Markovianity in the dynamics. Non-Markovianity, characterized by coherence revivals and non-monotonic decay patterns, emerges distinctly depending on the bath spectrum and coupling strengths. Systems coupled to sub-Ohmic baths, whether both or in combination with another type, display pronounced memory effects at relatively small values of couplings. In contrast, combinations involving Ohmic and super-Ohmic baths exhibit noticeable non-Markovianity only at higher couplings.", "AI": {"tldr": "\u7814\u7a76\u5f3a\u8026\u5408\u58f0\u5b50\u6d74\u4e2d\u4e8c\u683c\u70b9\u8d39\u7c73\u5b50\u5b9e\u73b0\u7684\u7f00\u9970\u91cf\u5b50\u6bd4\u7279\u9000\u76f8\u5e72\u52a8\u529b\u5b66\uff0c\u53d1\u73b0\u975e\u9a6c\u5c14\u53ef\u592b\u6027\u5bfc\u81f4\u76f8\u5e72\u6027\u975e\u5355\u8c03\u8870\u51cf\u548c\u590d\u82cf\uff0c\u6d74\u8c31\u7c7b\u578b\u548c\u8026\u5408\u5f3a\u5ea6\u663e\u8457\u5f71\u54cd\u975e\u9a6c\u5c14\u53ef\u592b\u884c\u4e3a\u3002", "motivation": "\u7814\u7a76\u5f3a\u8026\u5408\u58f0\u5b50\u6d74\u4e2d\u7f00\u9970\u91cf\u5b50\u6bd4\u7279\u7684\u9000\u76f8\u5e72\u52a8\u529b\u5b66\uff0c\u63a2\u7d22\u4e0d\u540c\u6d74\u8c31\u5bc6\u5ea6\u4e0b\u975e\u9a6c\u5c14\u53ef\u592b\u6548\u5e94\u5bf9\u91cf\u5b50\u6bd4\u7279\u76f8\u5e72\u6027\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528Lang-Firsov\u53d8\u6362\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u5fae\u6270\u5904\u7406\u7684\u5f62\u5f0f\uff0c\u5728\u6781\u5316\u5b50\u6846\u67b6\u4e0b\u5e94\u7528\u65f6\u95f4\u5377\u79ef\u4e3b\u65b9\u7a0b\uff0c\u5728\u7cfb\u7edf\u5355\u91cd\u6001-\u4e09\u91cd\u6001\u57fa\u4e2d\u7814\u7a76\u9000\u76f8\u5e72\u52a8\u529b\u5b66\u3002", "result": "\u5f3a\u8026\u5408\u4e0b\u76f8\u5e72\u6027\u4fdd\u6301\u65f6\u95f4\u66f4\u957f\uff0c\u8868\u73b0\u51fa\u975e\u5355\u8c03\u884c\u4e3a\u53cd\u6620\u975e\u9a6c\u5c14\u53ef\u592b\u6027\uff1b\u4e9a\u6b27\u59c6\u6d74\u5728\u8f83\u5c0f\u8026\u5408\u4e0b\u5373\u663e\u793a\u663e\u8457\u8bb0\u5fc6\u6548\u5e94\uff0c\u800c\u6b27\u59c6\u548c\u8d85\u6b27\u59c6\u6d74\u7ec4\u5408\u9700\u8f83\u9ad8\u8026\u5408\u624d\u663e\u73b0\u660e\u663e\u975e\u9a6c\u5c14\u53ef\u592b\u6027\u3002", "conclusion": "\u7f00\u9970\u91cf\u5b50\u6bd4\u7279\u7684\u9000\u76f8\u5e72\u52a8\u529b\u5b66\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u6d74\u8c31\u7c7b\u578b\u548c\u8026\u5408\u5f3a\u5ea6\uff0c\u975e\u9a6c\u5c14\u53ef\u592b\u6027\u8868\u73b0\u4e3a\u76f8\u5e72\u6027\u590d\u82cf\u548c\u975e\u5355\u8c03\u8870\u51cf\u6a21\u5f0f\uff0c\u4e3a\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u4e2d\u73af\u5883\u5de5\u7a0b\u63d0\u4f9b\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2602.07126", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07126", "abs": "https://arxiv.org/abs/2602.07126", "authors": ["Joshua Ward", "Chi-Hua Wang", "Guang Cheng"], "title": "Finding Connections: Membership Inference Attacks for the Multi-Table Synthetic Data Setting", "comment": null, "summary": "Synthetic tabular data has gained attention for enabling privacy-preserving data sharing. While substantial progress has been made in single-table synthetic generation where data are modeled at the row or item level, most real-world data exists in relational databases where a user's information spans items across multiple interconnected tables. Recent advances in synthetic relational data generation have emerged to address this complexity, yet release of these data introduce unique privacy challenges as information can be leaked not only from individual items but also through the relationships that comprise a complete user entity.\n  To address this, we propose a novel Membership Inference Attack (MIA) setting to audit the empirical user-level privacy of synthetic relational data and show that single-table MIAs that audit at an item level underestimate user-level privacy leakage. We then propose Multi-Table Membership Inference Attack (MT-MIA), a novel adversarial attack under a No-Box threat model that targets learned representations of user entities via Heterogeneous Graph Neural Networks. By incorporating all connected items for a user, MT-MIA better targets user-level vulnerabilities induced by inter-tabular relationships than existing attacks. We evaluate MT-MIA on a range of real-world multi-table datasets and demonstrate that this vulnerability exists in state-of-the-art relational synthetic data generators, employing MT-MIA to additionally study where this leakage occurs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5408\u6210\u5173\u7cfb\u6570\u636e\u7684\u591a\u8868\u6210\u5458\u63a8\u7406\u653b\u51fb\uff08MT-MIA\uff09\uff0c\u7528\u4e8e\u5ba1\u8ba1\u7528\u6237\u7ea7\u9690\u79c1\u6cc4\u9732\uff0c\u76f8\u6bd4\u5355\u8868\u653b\u51fb\u80fd\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u8de8\u8868\u5173\u7cfb\u7684\u9690\u79c1\u98ce\u9669\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u5927\u591a\u5b58\u5728\u4e8e\u5173\u7cfb\u6570\u636e\u5e93\u4e2d\uff0c\u7528\u6237\u4fe1\u606f\u5206\u5e03\u5728\u591a\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u8868\u4e2d\u3002\u73b0\u6709\u7684\u5408\u6210\u5173\u7cfb\u6570\u636e\u751f\u6210\u65b9\u6cd5\u867d\u7136\u80fd\u5904\u7406\u8fd9\u79cd\u590d\u6742\u6027\uff0c\u4f46\u6570\u636e\u53d1\u5e03\u4ecd\u9762\u4e34\u72ec\u7279\u7684\u9690\u79c1\u6311\u6218\uff0c\u56e0\u4e3a\u4fe1\u606f\u4e0d\u4ec5\u53ef\u80fd\u4ece\u5355\u4e2a\u9879\u76ee\u6cc4\u9732\uff0c\u8fd8\u53ef\u80fd\u901a\u8fc7\u6784\u6210\u5b8c\u6574\u7528\u6237\u5b9e\u4f53\u7684\u5173\u7cfb\u6cc4\u9732\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u8868\u6210\u5458\u63a8\u7406\u653b\u51fb\uff08MT-MIA\uff09\uff0c\u5728\u65e0\u76d2\u5a01\u80c1\u6a21\u578b\u4e0b\uff0c\u901a\u8fc7\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\uff08HGNN\uff09\u9488\u5bf9\u7528\u6237\u5b9e\u4f53\u7684\u5b66\u4e60\u8868\u793a\u8fdb\u884c\u653b\u51fb\u3002\u8be5\u65b9\u6cd5\u6574\u5408\u7528\u6237\u7684\u6240\u6709\u8fde\u63a5\u9879\u76ee\uff0c\u66f4\u597d\u5730\u9488\u5bf9\u7531\u8868\u95f4\u5173\u7cfb\u5f15\u8d77\u7684\u7528\u6237\u7ea7\u6f0f\u6d1e\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0cMT-MIA\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u591a\u8868\u6570\u636e\u96c6\u4e0a\u6709\u6548\uff0c\u8fd9\u79cd\u6f0f\u6d1e\u5b58\u5728\u4e8e\u6700\u5148\u8fdb\u7684\u5173\u7cfb\u5408\u6210\u6570\u636e\u751f\u6210\u5668\u4e2d\u3002\u5355\u8868MIA\u5728\u9879\u76ee\u7ea7\u522b\u5ba1\u8ba1\u4f1a\u4f4e\u4f30\u7528\u6237\u7ea7\u9690\u79c1\u6cc4\u9732\uff0c\u800cMT-MIA\u80fd\u66f4\u51c6\u786e\u5730\u63ed\u793a\u8de8\u8868\u5173\u7cfb\u7684\u9690\u79c1\u98ce\u9669\u3002", "conclusion": "\u9700\u8981\u65b0\u7684\u9690\u79c1\u5ba1\u8ba1\u65b9\u6cd5\u6765\u8bc4\u4f30\u5408\u6210\u5173\u7cfb\u6570\u636e\u7684\u7528\u6237\u7ea7\u9690\u79c1\u98ce\u9669\uff0cMT-MIA\u4e3a\u6b64\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u62a4\u8de8\u8868\u5173\u7cfb\u9690\u79c1\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2602.08125", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2602.08125", "abs": "https://arxiv.org/abs/2602.08125", "authors": ["Jonas Neuser", "Thomas Thiemann"], "title": "Quantum Field Theory of Black Hole Perturbations with Backreaction V. Beyond Second Order Perturbations", "comment": "44 p, no figures", "summary": "Black hole perturbation theory beyond second order is not well understood because typically one defines the meaning of gauge invariance order by order which is ambiguous. In this series of works we therefore developed a new approach which disentangles the meaning of gauge invariance from the perturbative order. It is based on the reduced phase space approach to the Hamiltonian formulation of General Relativity and constructs a non-perturbative, albeit implicit, formulation of the dynamics of only observables that are gauge invariant to all orders. To obtain explicit expressions, perturbation theory is then employed, but now only perturbations are considered that are gauge invariant to all orders. There are both spherically symmetric and non-symmetric observables and the formulation takes the (perturbative) backreaction between those fully into account. The formulation has access to both the exterior and interior of the dynamical horizon.\n  In previous papers of this series we have introduced the general formalism and performed consistency checks with second order results obtained in other approaches. The real virtue of our approach starts emerging at higher than second order where we expect differences from previous works both due to backreaction effects and because we work with observables that are gauge invariant to all orders, not only up to a given order. In this paper, we consider the third order. Also new to our approach is that we start from a non-perturbative, namely polynomial, version of the constraints which therefore are finite polynomials in all degrees of freedom before reducing, rather than an infinite series. This allows for an exact and non-perturbative, while implicit, solution of the constraints which does not need to truncate the series and thus is of tremendous technical advantage.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9ed1\u6d1e\u5fae\u6270\u7406\u8bba\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea6\u5316\u76f8\u7a7a\u95f4\u65b9\u6cd5\u6784\u5efa\u975e\u5fae\u6270\u7684\u89c4\u8303\u4e0d\u53d8\u53ef\u89c2\u6d4b\u91cf\uff0c\u7136\u540e\u5728\u5168\u9636\u89c4\u8303\u4e0d\u53d8\u6027\u57fa\u7840\u4e0a\u8fdb\u884c\u5fae\u6270\u5c55\u5f00\uff0c\u7279\u522b\u5173\u6ce8\u4e09\u9636\u6548\u5e94\u3002", "motivation": "\u4f20\u7edf\u9ed1\u6d1e\u5fae\u6270\u7406\u8bba\u5728\u4e8c\u9636\u4ee5\u4e0a\u5b58\u5728\u89c4\u8303\u4e0d\u53d8\u6027\u5b9a\u4e49\u6a21\u7cca\u7684\u95ee\u9898\uff0c\u9700\u8981\u9010\u9636\u5b9a\u4e49\u89c4\u8303\u53d8\u6362\uff0c\u8fd9\u5bfc\u81f4\u9ad8\u9636\u5fae\u6270\u7406\u8bba\u4e0d\u5b8c\u5584\u3002\u672c\u6587\u65e8\u5728\u53d1\u5c55\u4e00\u79cd\u80fd\u591f\u6e05\u6670\u5206\u79bb\u89c4\u8303\u4e0d\u53d8\u6027\u4e0e\u5fae\u6270\u9636\u6570\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5e7f\u4e49\u76f8\u5bf9\u8bba\u54c8\u5bc6\u987f\u8868\u8ff0\u7684\u7ea6\u5316\u76f8\u7a7a\u95f4\u65b9\u6cd5\uff0c\u6784\u5efa\u975e\u5fae\u6270\u7684\u89c4\u8303\u4e0d\u53d8\u53ef\u89c2\u6d4b\u91cf\u52a8\u529b\u5b66\uff08\u867d\u7136\u662f\u9690\u5f0f\u7684\uff09\u3002\u7136\u540e\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u884c\u5fae\u6270\u5c55\u5f00\uff0c\u4f46\u53ea\u8003\u8651\u5168\u9636\u89c4\u8303\u4e0d\u53d8\u7684\u5fae\u6270\u3002\u7279\u522b\u4f7f\u7528\u591a\u9879\u5f0f\u5f62\u5f0f\u7684\u7ea6\u675f\u6761\u4ef6\u800c\u975e\u65e0\u7a77\u7ea7\u6570\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u975e\u5fae\u6270\u7ea6\u675f\u6c42\u89e3\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u7403\u5bf9\u79f0\u548c\u975e\u5bf9\u79f0\u53ef\u89c2\u6d4b\u91cf\uff0c\u5e76\u5b8c\u5168\u8003\u8651\u5b83\u4eec\u4e4b\u95f4\u7684\u5fae\u6270\u53cd\u4f5c\u7528\u3002\u80fd\u591f\u540c\u65f6\u8bbf\u95ee\u52a8\u6001\u89c6\u754c\u7684\u5916\u90e8\u533a\u57df\u548c\u5185\u90e8\u533a\u57df\uff0c\u5e76\u5728\u4e09\u9636\u5fae\u6270\u4e0a\u5c55\u793a\u4e86\u5176\u4f18\u52bf\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u901a\u8fc7\u7ea6\u5316\u76f8\u7a7a\u95f4\u6846\u67b6\u5c06\u89c4\u8303\u4e0d\u53d8\u6027\u4e0e\u5fae\u6270\u9636\u6570\u89e3\u8026\uff0c\u63d0\u4f9b\u4e86\u5904\u7406\u9ad8\u9636\u9ed1\u6d1e\u5fae\u6270\u7684\u4e25\u683c\u6846\u67b6\uff0c\u7279\u522b\u662f\u5728\u4e09\u9636\u53ca\u66f4\u9ad8\u9636\u5fae\u6270\u4e2d\u80fd\u591f\u6355\u6349\u4f20\u7edf\u65b9\u6cd5\u53ef\u80fd\u5ffd\u7565\u7684\u53cd\u4f5c\u7528\u6548\u5e94\u548c\u5168\u9636\u89c4\u8303\u4e0d\u53d8\u6027\u5f71\u54cd\u3002"}}
{"id": "2602.07460", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.07460", "abs": "https://arxiv.org/abs/2602.07460", "authors": ["Ya-Wei Zeng", "Wei-Xin Chen", "Tian-Le Yang", "Wan-Jun Su", "Huaizhi Wu"], "title": "Sensing weak anharmonicities with a passive-active anti-PT symmetric system", "comment": "9 pages, 5 figures", "summary": "We propose a scheme for enhanced sensing of weak anharmonicities based on a three-mode anti-parity-time (anti-PT) symmetric cavity-magnon-waveguide system. By tuning the optical gain to the active cavity mode, the linewidth suppression point for the anti-PT symmetric Hamiltonian can be flexibly controlled even when the two dissipative magnonic modes experience strong intrinsic decay. This essential characteristic is utilized for detecting weak nonlinearities in both the cavity and magnonic modes, with both demonstrating similar high levels of sensitivity. Moreover, the sensitivity can be greatly improved with a detuned laser drive. Based on the integrated passive-active three-mode anti-PT symmetric system, the sensing scheme can be generalized to various physical systems with anharmonicities.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e09\u6a21\u53cd\u5b87\u79f0\u65f6\u95f4\u5bf9\u79f0\u8154-\u78c1\u632f\u5b50-\u6ce2\u5bfc\u7cfb\u7edf\u7684\u5f31\u975e\u8c10\u6027\u589e\u5f3a\u4f20\u611f\u65b9\u6848\uff0c\u901a\u8fc7\u8c03\u8282\u5149\u5b66\u589e\u76ca\u5b9e\u73b0\u7ebf\u5bbd\u6291\u5236\u70b9\u7075\u6d3b\u63a7\u5236\uff0c\u53ef\u68c0\u6d4b\u8154\u6a21\u548c\u78c1\u632f\u5b50\u6a21\u7684\u5f31\u975e\u7ebf\u6027\uff0c\u7075\u654f\u5ea6\u53ef\u901a\u8fc7\u5931\u8c10\u6fc0\u5149\u9a71\u52a8\u5927\u5e45\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u68c0\u6d4b\u5f31\u975e\u8c10\u6027\u65f6\u9762\u4e34\u7075\u654f\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u5f3a\u56fa\u6709\u8870\u51cf\u7684\u7cfb\u7edf\u4e2d\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u7075\u6d3b\u63a7\u5236\u7ebf\u5bbd\u6291\u5236\u70b9\u5e76\u63d0\u9ad8\u68c0\u6d4b\u7075\u654f\u5ea6\u7684\u65b0\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e09\u6a21\u53cd\u5b87\u79f0\u65f6\u95f4\u5bf9\u79f0\u8154-\u78c1\u632f\u5b50-\u6ce2\u5bfc\u7cfb\u7edf\uff0c\u901a\u8fc7\u8c03\u8282\u4e3b\u52a8\u8154\u6a21\u7684\u5149\u5b66\u589e\u76ca\u6765\u63a7\u5236\u53cdPT\u5bf9\u79f0\u54c8\u5bc6\u987f\u91cf\u7684\u7ebf\u5bbd\u6291\u5236\u70b9\uff0c\u5229\u7528\u5931\u8c10\u6fc0\u5149\u9a71\u52a8\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7075\u654f\u5ea6\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u7075\u6d3b\u63a7\u5236\u7ebf\u5bbd\u6291\u5236\u70b9\uff0c\u5373\u4f7f\u78c1\u632f\u5b50\u6a21\u5b58\u5728\u5f3a\u56fa\u6709\u8870\u51cf\uff1b\u53ef\u68c0\u6d4b\u8154\u6a21\u548c\u78c1\u632f\u5b50\u6a21\u7684\u5f31\u975e\u7ebf\u6027\uff0c\u4e24\u8005\u8868\u73b0\u51fa\u76f8\u4f3c\u7684\u9ad8\u7075\u654f\u5ea6\uff1b\u5931\u8c10\u6fc0\u5149\u9a71\u52a8\u53ef\u5927\u5e45\u63d0\u5347\u7075\u654f\u5ea6\u3002", "conclusion": "\u57fa\u4e8e\u96c6\u6210\u88ab\u52a8-\u4e3b\u52a8\u4e09\u6a21\u53cdPT\u5bf9\u79f0\u7cfb\u7edf\u7684\u4f20\u611f\u65b9\u6848\u5177\u6709\u9ad8\u7075\u654f\u5ea6\u548c\u7075\u6d3b\u6027\uff0c\u53ef\u63a8\u5e7f\u5230\u5404\u79cd\u5177\u6709\u975e\u8c10\u6027\u7684\u7269\u7406\u7cfb\u7edf\u4e2d\uff0c\u4e3a\u5f31\u975e\u7ebf\u6027\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2602.07135", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07135", "abs": "https://arxiv.org/abs/2602.07135", "authors": ["Jiaqing Chen", "Nicholas Hadler", "Tiankai Xie", "Rostyslav Hnatyshyn", "Caleb Geniesse", "Yaoqing Yang", "Michael W. Mahoney", "Talita Perciano", "John F. Hartwig", "Ross Maciejewski", "Gunther H. Weber"], "title": "Landscaper: Understanding Loss Landscapes Through Multi-Dimensional Topological Analysis", "comment": null, "summary": "Loss landscapes are a powerful tool for understanding neural network optimization and generalization, yet traditional low-dimensional analyses often miss complex topological features. We present Landscaper, an open-source Python package for arbitrary-dimensional loss landscape analysis. Landscaper combines Hessian-based subspace construction with topological data analysis to reveal geometric structures such as basin hierarchy and connectivity. A key component is the Saddle-Minimum Average Distance (SMAD) for quantifying landscape smoothness. We demonstrate Landscaper's effectiveness across various architectures and tasks, including those involving pre-trained language models, showing that SMAD captures training transitions, such as landscape simplification, that conventional metrics miss. We also illustrate Landscaper's performance in challenging chemical property prediction tasks, where SMAD can serve as a metric for out-of-distribution generalization, offering valuable insights for model diagnostics and architecture design in data-scarce scientific machine learning scenarios.", "AI": {"tldr": "Landscaper\u662f\u4e00\u4e2a\u7528\u4e8e\u4efb\u610f\u7ef4\u5ea6\u635f\u5931\u666f\u89c2\u5206\u6790\u7684Python\u5305\uff0c\u7ed3\u5408Hessian\u5b50\u7a7a\u95f4\u6784\u5efa\u548c\u62d3\u6251\u6570\u636e\u5206\u6790\uff0c\u901a\u8fc7SMAD\u6307\u6807\u91cf\u5316\u666f\u89c2\u5e73\u6ed1\u5ea6\uff0c\u63ed\u793a\u4f20\u7edf\u65b9\u6cd5\u9057\u6f0f\u7684\u590d\u6742\u62d3\u6251\u7279\u5f81\u3002", "motivation": "\u4f20\u7edf\u4f4e\u7ef4\u635f\u5931\u666f\u89c2\u5206\u6790\u5e38\u5e38\u9057\u6f0f\u590d\u6742\u7684\u62d3\u6251\u7279\u5f81\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u6765\u6df1\u5165\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u548c\u6cdb\u5316\u8fc7\u7a0b\uff0c\u7279\u522b\u662f\u5728\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u7b49\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e2d\u3002", "method": "\u5f00\u53d1\u4e86Landscaper\u5f00\u6e90Python\u5305\uff0c\u7ed3\u5408Hessian-based\u5b50\u7a7a\u95f4\u6784\u5efa\u548c\u62d3\u6251\u6570\u636e\u5206\u6790\u65b9\u6cd5\uff0c\u5f15\u5165Saddle-Minimum Average Distance\uff08SMAD\uff09\u6307\u6807\u6765\u91cf\u5316\u635f\u5931\u666f\u89c2\u7684\u5e73\u6ed1\u5ea6\u3002", "result": "Landscaper\u5728\u5404\u79cd\u67b6\u6784\u548c\u4efb\u52a1\u4e2d\u6709\u6548\uff0c\u5305\u62ec\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0cSMAD\u80fd\u591f\u6355\u6349\u4f20\u7edf\u6307\u6807\u9057\u6f0f\u7684\u8bad\u7ec3\u8f6c\u6362\uff08\u5982\u666f\u89c2\u7b80\u5316\uff09\uff0c\u5728\u5316\u5b66\u6027\u8d28\u9884\u6d4b\u4efb\u52a1\u4e2d\u53ef\u4f5c\u4e3a\u5206\u5e03\u5916\u6cdb\u5316\u7684\u5ea6\u91cf\u6307\u6807\u3002", "conclusion": "Landscaper\u4e3a\u6a21\u578b\u8bca\u65ad\u548c\u67b6\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6d1e\u5bdf\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u7684\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u573a\u666f\u4e2d\uff0c\u80fd\u591f\u63ed\u793a\u635f\u5931\u666f\u89c2\u7684\u590d\u6742\u51e0\u4f55\u7ed3\u6784\u3002"}}
{"id": "2602.08205", "categories": ["gr-qc", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.08205", "abs": "https://arxiv.org/abs/2602.08205", "authors": ["Wen-Mei Li", "Jianbo Lu", "Shu-Min Wu"], "title": "Does fermionic entanglement always outperform bosonic entanglement in dilaton black hole?", "comment": "27 pages, 5 figures", "summary": "It has traditionally been believed that fermionic entanglement generally outperforms bosonic entanglement in relativistic frameworks, and that bosonic entanglement experiences sudden death in extreme gravitational environments. In this study, we analyze the genuine N-partite entanglement, measured by negativity, of bosonic and fermionic GHZ states, focusing on scenarios where a subset of $m$ ($m<N$) constituents interacts with Hawking radiation generated by a Garfinkle-Horowitz-Strominger (GHS) dilaton black hole. Surprisingly, we find that quantum entanglement between the non-gravitational and gravitational modes for the bosonic field is stronger than that in the same modes for the fermionic field within dilaton spacetime. This study challenges the traditional belief that ``fermionic entanglement always outperforms bosonic entanglement\" in the relativistic framework. However, quantum entanglement between the gravitational modes and the combined gravitational and non-gravitational modes is weaker for the bosonic field than for the fermionic field in the presence of a dilaton black hole. Finally, the connection between the global N-partite entanglement in the bosonic field and that in the fermionic field is influenced by the gravitational field's intensity. Our study reveals the intrinsic relationship between quantum entanglement of bosonic and fermionic fields in curved spacetime from a new perspective, and provides theoretical guidance for selecting appropriate field-based quantum resources for relativistic quantum information tasks under extreme gravitational conditions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6311\u6218\u4e86\u4f20\u7edf\u89c2\u5ff5\uff0c\u53d1\u73b0\u5728GHS\u81a8\u80c0\u5b50\u9ed1\u6d1e\u80cc\u666f\u4e0b\uff0c\u73bb\u8272\u573a\u5728\u975e\u5f15\u529b\u6a21\u5f0f\u4e0e\u5f15\u529b\u6a21\u5f0f\u95f4\u7684\u7ea0\u7f20\u5f3a\u4e8e\u8d39\u7c73\u573a\uff0c\u4f46\u73bb\u8272\u573a\u5728\u5f15\u529b\u6a21\u5f0f\u4e0e\u6df7\u5408\u6a21\u5f0f\u95f4\u7684\u7ea0\u7f20\u5f31\u4e8e\u8d39\u7c73\u573a\u3002", "motivation": "\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3a\u8d39\u7c73\u7ea0\u7f20\u5728\u76f8\u5bf9\u8bba\u6846\u67b6\u4e0b\u4f18\u4e8e\u73bb\u8272\u7ea0\u7f20\uff0c\u4e14\u73bb\u8272\u7ea0\u7f20\u5728\u6781\u7aef\u5f15\u529b\u73af\u5883\u4e2d\u4f1a\u7a81\u7136\u6b7b\u4ea1\u3002\u672c\u7814\u7a76\u65e8\u5728\u91cd\u65b0\u5ba1\u89c6\u8fd9\u4e00\u89c2\u70b9\uff0c\u63a2\u7d22\u81a8\u80c0\u5b50\u9ed1\u6d1e\u80cc\u666f\u4e0b\u73bb\u8272\u573a\u4e0e\u8d39\u7c73\u573a\u7ea0\u7f20\u884c\u4e3a\u7684\u5dee\u5f02\u3002", "method": "\u5206\u6790\u73bb\u8272\u548c\u8d39\u7c73GHZ\u6001\u7684\u771f\u5b9eN-\u4f53\u7ea0\u7f20\uff08\u7528\u8d1f\u6027\u5ea6\u91cf\uff09\uff0c\u7814\u7a76\u5176\u4e2dm\u4e2a\u7ec4\u5206\u4e0eGHS\u81a8\u80c0\u5b50\u9ed1\u6d1e\u4ea7\u751f\u7684\u970d\u91d1\u8f90\u5c04\u76f8\u4e92\u4f5c\u7528\u7684\u60c5\u51b5\u3002\u6bd4\u8f83\u4e0d\u540c\u6a21\u5f0f\u95f4\u7684\u7ea0\u7f20\u5f3a\u5ea6\u3002", "result": "1. \u73bb\u8272\u573a\u5728\u975e\u5f15\u529b\u6a21\u5f0f\u4e0e\u5f15\u529b\u6a21\u5f0f\u95f4\u7684\u7ea0\u7f20\u5f3a\u4e8e\u8d39\u7c73\u573a\uff1b2. \u73bb\u8272\u573a\u5728\u5f15\u529b\u6a21\u5f0f\u4e0e\u6df7\u5408\u6a21\u5f0f\u95f4\u7684\u7ea0\u7f20\u5f31\u4e8e\u8d39\u7c73\u573a\uff1b3. \u5168\u5c40N-\u4f53\u7ea0\u7f20\u5173\u7cfb\u53d7\u5f15\u529b\u573a\u5f3a\u5ea6\u5f71\u54cd\uff1b4. \u6311\u6218\u4e86\"\u8d39\u7c73\u7ea0\u7f20\u603b\u662f\u4f18\u4e8e\u73bb\u8272\u7ea0\u7f20\"\u7684\u4f20\u7edf\u89c2\u5ff5\u3002", "conclusion": "\u7814\u7a76\u4ece\u65b0\u89c6\u89d2\u63ed\u793a\u4e86\u5f2f\u66f2\u65f6\u7a7a\u4e2d\u73bb\u8272\u573a\u4e0e\u8d39\u7c73\u573a\u91cf\u5b50\u7ea0\u7f20\u7684\u5185\u5728\u5173\u7cfb\uff0c\u4e3a\u6781\u7aef\u5f15\u529b\u6761\u4ef6\u4e0b\u9009\u62e9\u5408\u9002\u573a\u57fa\u91cf\u5b50\u8d44\u6e90\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\uff0c\u8868\u660e\u573a\u7c7b\u578b\u9009\u62e9\u9700\u6839\u636e\u5177\u4f53\u7ea0\u7f20\u6a21\u5f0f\u548c\u5e94\u7528\u573a\u666f\u800c\u5b9a\u3002"}}
{"id": "2602.07483", "categories": ["quant-ph", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.07483", "abs": "https://arxiv.org/abs/2602.07483", "authors": ["Kuan-Cheng Chen", "Hiromichi Matsuyama", "Wei-hao Huang", "Yu Yamashiro"], "title": "Recursive QAOA for Interference-Aware Resource Allocation in Wireless Networks", "comment": null, "summary": "Discrete radio resource management problems in dense wireless networks are naturally cast as quadratic unconstrained binary optimization (QUBO) programs but are difficult to solve at scale. We investigate a quantum-classical approach based on the Recursive Quantum Approximate Optimization Algorithm (RQAOA), which interleaves shallow QAOA layers with variable elimination guided by measured single- and two-qubit correlators. For interference-aware channel assignment, we give a compact QUBO/Ising formulation in which pairwise interference induces same-channel couplings and one-hot constraints are enforced via quadratic penalties (or, optionally, constraint-preserving mixers). Within RQAOA, fixing high-confidence variables or relations reduces the problem dimension, stabilizes training, and concentrates measurement effort on a shrinking instance that is solved exactly once below a cutoff. On simulated instances of modest size, including a four-user, four-channel example, the method consistently returns feasible assignments and, for the demonstrated case, attains the global optimum. These results indicate that recursion can mitigate parameter growth and feasibility issues that affect plain QAOA, and suggest a viable pathway for near-term quantum heuristics in wireless resource allocation.", "AI": {"tldr": "RQAOA\uff08\u9012\u5f52\u91cf\u5b50\u8fd1\u4f3c\u4f18\u5316\u7b97\u6cd5\uff09\u7528\u4e8e\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u4fe1\u9053\u5206\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u9012\u5f52\u6d88\u9664\u53d8\u91cf\u6765\u51cf\u5c0f\u95ee\u9898\u89c4\u6a21\uff0c\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\u80fd\u83b7\u5f97\u53ef\u884c\u89e3\u5e76\u8fbe\u5230\u5168\u5c40\u6700\u4f18\u3002", "motivation": "\u5bc6\u96c6\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u79bb\u6563\u65e0\u7ebf\u8d44\u6e90\u7ba1\u7406\u95ee\u9898\u81ea\u7136\u53ef\u4ee5\u8868\u793a\u4e3aQUBO\uff08\u4e8c\u6b21\u65e0\u7ea6\u675f\u4e8c\u8fdb\u5236\u4f18\u5316\uff09\u7a0b\u5e8f\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u60c5\u51b5\u4e0b\u96be\u4ee5\u6c42\u89e3\u3002\u9700\u8981\u63a2\u7d22\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8eRQAOA\uff08\u9012\u5f52\u91cf\u5b50\u8fd1\u4f3c\u4f18\u5316\u7b97\u6cd5\uff09\u7684\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u65b9\u6cd5\uff0c\u5c06\u6d45\u5c42QAOA\u5c42\u4e0e\u57fa\u4e8e\u6d4b\u91cf\u7684\u5355\u6bd4\u7279\u548c\u53cc\u6bd4\u7279\u76f8\u5173\u5668\u6307\u5bfc\u7684\u53d8\u91cf\u6d88\u9664\u76f8\u7ed3\u5408\u3002\u5bf9\u4e8e\u5e72\u6270\u611f\u77e5\u7684\u4fe1\u9053\u5206\u914d\u95ee\u9898\uff0c\u7ed9\u51fa\u4e86\u7d27\u51d1\u7684QUBO/Ising\u516c\u5f0f\uff0c\u5176\u4e2d\u6210\u5bf9\u5e72\u6270\u8bf1\u5bfc\u540c\u4fe1\u9053\u8026\u5408\uff0c\u901a\u8fc7\u4e8c\u6b21\u60e9\u7f5a\uff08\u6216\u53ef\u9009\u7684\u7ea6\u675f\u4fdd\u6301\u6df7\u5408\u5668\uff09\u5f3a\u5236\u6267\u884cone-hot\u7ea6\u675f\u3002", "result": "\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\uff0c\u5305\u62ec\u4e00\u4e2a\u56db\u7528\u6237\u56db\u4fe1\u9053\u7684\u793a\u4f8b\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u8fd4\u56de\u53ef\u884c\u7684\u4fe1\u9053\u5206\u914d\uff0c\u5e76\u4e14\u5728\u6f14\u793a\u6848\u4f8b\u4e2d\u8fbe\u5230\u4e86\u5168\u5c40\u6700\u4f18\u89e3\u3002\u9012\u5f52\u65b9\u6cd5\u80fd\u591f\u7f13\u89e3\u5f71\u54cd\u666e\u901aQAOA\u7684\u53c2\u6570\u589e\u957f\u548c\u53ef\u884c\u6027\u95ee\u9898\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u9012\u5f52\u53ef\u4ee5\u7f13\u89e3\u5f71\u54cd\u666e\u901aQAOA\u7684\u53c2\u6570\u589e\u957f\u548c\u53ef\u884c\u6027\u95ee\u9898\uff0c\u4e3a\u65e0\u7ebf\u8d44\u6e90\u5206\u914d\u4e2d\u7684\u8fd1\u671f\u91cf\u5b50\u542f\u53d1\u5f0f\u7b97\u6cd5\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u884c\u7684\u9014\u5f84\u3002"}}
{"id": "2602.07141", "categories": ["cs.LG", "math.FA"], "pdf": "https://arxiv.org/pdf/2602.07141", "abs": "https://arxiv.org/abs/2602.07141", "authors": ["Isabel de la Higuera", "Francisco Herrera", "M. Victoria Velasco"], "title": "Featured Reproducing Kernel Banach Spaces for Learning and Neural Networks", "comment": null, "summary": "Reproducing kernel Hilbert spaces provide a foundational framework for kernel-based learning, where regularization and interpolation problems admit finite-dimensional solutions through classical representer theorems. Many modern learning models, however -- including fixed-architecture neural networks equipped with non-quadratic norms -- naturally give rise to non-Hilbertian geometries that fall outside this setting. In Banach spaces, continuity of point-evaluation functionals alone is insufficient to guarantee feature representations or kernel-based learning formulations. In this work, we develop a functional-analytic framework for learning in Banach spaces based on the notion of featured reproducing kernel Banach spaces. We identify the precise structural conditions under which feature maps, kernel constructions, and representer-type results can be recovered beyond the Hilbertian regime. Within this framework, supervised learning is formulated as a minimal-norm interpolation or regularization problem, and existence results together with conditional representer theorems are established. We further extend the theory to vector-valued featured reproducing kernel Banach spaces and show that fixed-architecture neural networks naturally induce special instances of such spaces. This provides a unified function-space perspective on kernel methods and neural networks and clarifies when kernel-based learning principles extend beyond reproducing kernel Hilbert spaces.", "AI": {"tldr": "\u63d0\u51fa\u7279\u5f81\u518d\u751f\u6838Banach\u7a7a\u95f4\u6846\u67b6\uff0c\u5c06\u518d\u751f\u6838\u7406\u8bba\u4eceHilbert\u7a7a\u95f4\u63a8\u5e7f\u5230Banach\u7a7a\u95f4\uff0c\u4e3a\u795e\u7ecf\u7f51\u7edc\u7b49\u975eHilbert\u51e0\u4f55\u6a21\u578b\u63d0\u4f9b\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\u3002", "motivation": "\u4f20\u7edf\u518d\u751f\u6838Hilbert\u7a7a\u95f4\u6846\u67b6\u65e0\u6cd5\u5904\u7406\u795e\u7ecf\u7f51\u7edc\u7b49\u5177\u6709\u975e\u4e8c\u6b21\u8303\u6570\u7684\u73b0\u4ee3\u5b66\u4e60\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u81ea\u7136\u4ea7\u751f\u975eHilbert\u51e0\u4f55\u7ed3\u6784\u3002\u5728Banach\u7a7a\u95f4\u4e2d\uff0c\u70b9\u8bc4\u4f30\u6cdb\u51fd\u7684\u8fde\u7eed\u6027\u4e0d\u8db3\u4ee5\u4fdd\u8bc1\u7279\u5f81\u8868\u793a\u6216\u6838\u5b66\u4e60\u516c\u5f0f\u3002", "method": "\u57fa\u4e8e\u7279\u5f81\u518d\u751f\u6838Banach\u7a7a\u95f4\u7684\u6982\u5ff5\uff0c\u5efa\u7acbBanach\u7a7a\u95f4\u5b66\u4e60\u7684\u6cdb\u51fd\u5206\u6790\u6846\u67b6\u3002\u786e\u5b9a\u7279\u5f81\u6620\u5c04\u3001\u6838\u6784\u9020\u548c\u8868\u793a\u578b\u5b9a\u7406\u5728\u975eHilbert\u4f53\u5236\u4e0b\u6062\u590d\u7684\u7cbe\u786e\u7ed3\u6784\u6761\u4ef6\u3002\u5c06\u76d1\u7763\u5b66\u4e60\u8868\u8ff0\u4e3a\u6700\u5c0f\u8303\u6570\u63d2\u503c\u6216\u6b63\u5219\u5316\u95ee\u9898\uff0c\u5efa\u7acb\u5b58\u5728\u6027\u7ed3\u679c\u548c\u6761\u4ef6\u8868\u793a\u5b9a\u7406\u3002\u8fdb\u4e00\u6b65\u5c06\u7406\u8bba\u6269\u5c55\u5230\u5411\u91cf\u503c\u7279\u5f81\u518d\u751f\u6838Banach\u7a7a\u95f4\u3002", "result": "\u8bc1\u660e\u4e86\u56fa\u5b9a\u67b6\u6784\u795e\u7ecf\u7f51\u7edc\u81ea\u7136\u5730\u8bf1\u5bfc\u51fa\u6b64\u7c7b\u7a7a\u95f4\u7684\u7279\u6b8a\u5b9e\u4f8b\uff0c\u4e3a\u6838\u65b9\u6cd5\u548c\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u51fd\u6570\u7a7a\u95f4\u89c6\u89d2\uff0c\u5e76\u9610\u660e\u4e86\u6838\u5b66\u4e60\u539f\u7406\u4f55\u65f6\u53ef\u4ee5\u8d85\u8d8a\u518d\u751f\u6838Hilbert\u7a7a\u95f4\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c06\u518d\u751f\u6838\u7406\u8bba\u4eceHilbert\u7a7a\u95f4\u63a8\u5e7f\u5230Banach\u7a7a\u95f4\uff0c\u4e3a\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u7b49\u975eHilbert\u51e0\u4f55\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u7edf\u4e00\u4e86\u6838\u65b9\u6cd5\u548c\u795e\u7ecf\u7f51\u7edc\u7684\u5b66\u4e60\u539f\u7406\u3002"}}
{"id": "2602.08341", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2602.08341", "abs": "https://arxiv.org/abs/2602.08341", "authors": ["Mikhail Altaisky"], "title": "Combinatorial Spacetime from Loop Quantum Gravity", "comment": "LaTeX, 10 pages, 11 figures", "summary": "Loop quantum gravity is a perspective candidate for the quantum theory of gravity. However, there is a conceptual controversy in it: having started from the Einstein-Hilbert action and describing spacetime without matter, we can hardly define spacetime as anything other than a set of relations between matter fields. Here, following the Penrose idea of combinatorial spacetime we reformulate loop quantum gravity theory solely in terms of the matter fields.", "AI": {"tldr": "\u57fa\u4e8e\u5f6d\u7f57\u65af\u7684\u7ec4\u5408\u65f6\u7a7a\u601d\u60f3\uff0c\u5c06\u5708\u91cf\u5b50\u5f15\u529b\u7406\u8bba\u91cd\u65b0\u8868\u8ff0\u4e3a\u4ec5\u7528\u7269\u8d28\u573a\u63cf\u8ff0\u7684\u7406\u8bba\uff0c\u89e3\u51b3\u7269\u8d28\u4e0e\u65f6\u7a7a\u5173\u7cfb\u7684\u6982\u5ff5\u4e89\u8bae", "motivation": "\u5708\u91cf\u5b50\u5f15\u529b\u4f5c\u4e3a\u91cf\u5b50\u5f15\u529b\u7406\u8bba\u7684\u6709\u529b\u5019\u9009\u8005\uff0c\u5b58\u5728\u4e00\u4e2a\u6982\u5ff5\u4e89\u8bae\uff1a\u4ece\u7231\u56e0\u65af\u5766-\u5e0c\u5c14\u4f2f\u7279\u4f5c\u7528\u91cf\u51fa\u53d1\uff0c\u5728\u6ca1\u6709\u7269\u8d28\u7684\u60c5\u51b5\u4e0b\u63cf\u8ff0\u65f6\u7a7a\uff0c\u5f88\u96be\u5c06\u65f6\u7a7a\u5b9a\u4e49\u4e3a\u7269\u8d28\u573a\u5173\u7cfb\u4e4b\u5916\u7684\u4efb\u4f55\u4e1c\u897f\u3002\u9700\u8981\u89e3\u51b3\u65f6\u7a7a\u4e0e\u7269\u8d28\u5173\u7cfb\u7684\u6839\u672c\u95ee\u9898\u3002", "method": "\u9075\u5faa\u5f6d\u7f57\u65af\u7684\u7ec4\u5408\u65f6\u7a7a\u601d\u60f3\uff0c\u5c06\u5708\u91cf\u5b50\u5f15\u529b\u7406\u8bba\u5b8c\u5168\u7528\u7269\u8d28\u573a\u91cd\u65b0\u8868\u8ff0\uff0c\u800c\u4e0d\u662f\u4ece\u7231\u56e0\u65af\u5766-\u5e0c\u5c14\u4f2f\u7279\u4f5c\u7528\u91cf\u51fa\u53d1\u3002\u901a\u8fc7\u7269\u8d28\u573a\u4e4b\u95f4\u7684\u5173\u7cfb\u6765\u5b9a\u4e49\u65f6\u7a7a\u7ed3\u6784\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ec5\u57fa\u4e8e\u7269\u8d28\u573a\u7684\u5708\u91cf\u5b50\u5f15\u529b\u7406\u8bba\u65b0\u8868\u8ff0\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u65f6\u7a7a\u4e0e\u7269\u8d28\u5206\u79bb\u7684\u6982\u5ff5\u95ee\u9898\uff0c\u4e3a\u91cf\u5b50\u5f15\u529b\u7406\u8bba\u63d0\u4f9b\u4e86\u66f4\u4e00\u81f4\u7684\u6982\u5ff5\u57fa\u7840\u3002", "conclusion": "\u901a\u8fc7\u5f6d\u7f57\u65af\u7684\u7ec4\u5408\u65f6\u7a7a\u65b9\u6cd5\u91cd\u65b0\u8868\u8ff0\u5708\u91cf\u5b50\u5f15\u529b\uff0c\u53ef\u4ee5\u89e3\u51b3\u8be5\u7406\u8bba\u4e2d\u5173\u4e8e\u65f6\u7a7a\u672c\u8d28\u7684\u6982\u5ff5\u4e89\u8bae\uff0c\u4e3a\u91cf\u5b50\u5f15\u529b\u7406\u8bba\u63d0\u4f9b\u66f4\u575a\u5b9e\u7684\u54f2\u5b66\u548c\u6982\u5ff5\u57fa\u7840\u3002"}}
{"id": "2602.07522", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.07522", "abs": "https://arxiv.org/abs/2602.07522", "authors": ["Cong Li", "Zhaohua Yang", "Xinfang Zhang", "Zhihao Wu", "Shichuan Xue", "Mingtang Deng"], "title": "Systematic Characterization of Transmon Qubit Stability with Thermal Cycling", "comment": "7 pages, 4 figures", "summary": "The temporal stability and reproducibility of qubit parameters are critical for the long-term operation and maintenance of superconducting quantum processors. In this work, we present a comprehensive longitudinal characterization of 27 frequency-tunable transmon qubits spanning over one year across four thermal cycles. Our results establish a distinct hierarchy of stability for superconducting hardware. We find that the intrinsic device parameters determining the qubit frequency and the baseline energy relaxation times ($T_1$) exhibit high robustness against thermal stress, characterized by frequency deviations typically confined within 0.5\\% and non-degraded coherence baselines. In stark contrast, the environmental variables, specifically the background magnetic flux offsets and the microscopic landscape of two-level system (TLS) defects, undergo a significant stochastic reconfiguration after each cycle. By employing frequency-dependent relaxation spectroscopy and a quantitative metric, the $T_1$ Spectral Topography Fidelity, we demonstrate that thermal cycling acts as a ``hard reset'' for the local defect environment. This process introduces a level of spectral randomization equivalent to thousands of hours of continuous low-temperature evolution. These findings confirm that while the fabrication quality is preserved, the specific noise realization is statistically distinct for each thermal cycle, necessitating automated recalibration strategies for large-scale quantum systems.", "AI": {"tldr": "\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u53c2\u6570\u5728\u70ed\u5faa\u73af\u4e2d\u8868\u73b0\u51fa\u5206\u5c42\u7a33\u5b9a\u6027\uff1a\u672c\u5f81\u5668\u4ef6\u53c2\u6570\uff08\u9891\u7387\u3001T1\uff09\u9ad8\u5ea6\u7a33\u5b9a\uff0c\u800c\u73af\u5883\u53d8\u91cf\uff08\u78c1\u901a\u504f\u79fb\u3001TLS\u7f3a\u9677\uff09\u5728\u6bcf\u6b21\u70ed\u5faa\u73af\u540e\u90fd\u4f1a\u968f\u673a\u91cd\u6784\u3002", "motivation": "\u7814\u7a76\u8d85\u5bfc\u91cf\u5b50\u5904\u7406\u5668\u4e2d\u91cf\u5b50\u6bd4\u7279\u53c2\u6570\u7684\u957f\u671f\u7a33\u5b9a\u6027\u548c\u53ef\u91cd\u590d\u6027\uff0c\u8fd9\u5bf9\u4e8e\u91cf\u5b50\u8ba1\u7b97\u673a\u7684\u957f\u671f\u8fd0\u884c\u548c\u7ef4\u62a4\u81f3\u5173\u91cd\u8981\u3002\u7279\u522b\u5173\u6ce8\u70ed\u5faa\u73af\u5bf9\u53c2\u6570\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "method": "\u5bf927\u4e2a\u9891\u7387\u53ef\u8c03transmon\u91cf\u5b50\u6bd4\u7279\u8fdb\u884c\u8d85\u8fc7\u4e00\u5e74\u7684\u7eb5\u5411\u8868\u5f81\uff0c\u8de8\u8d8a\u56db\u6b21\u70ed\u5faa\u73af\u3002\u4f7f\u7528\u9891\u7387\u76f8\u5173\u5f1b\u8c6b\u5149\u8c31\u548cT1\u5149\u8c31\u5730\u5f62\u4fdd\u771f\u5ea6\u5b9a\u91cf\u6307\u6807\u6765\u5206\u6790\u70ed\u5faa\u73af\u5bf9\u7f3a\u9677\u73af\u5883\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u8d85\u5bfc\u786c\u4ef6\u53c2\u6570\u5b58\u5728\u660e\u663e\u7684\u7a33\u5b9a\u6027\u5c42\u6b21\uff1a\u51b3\u5b9a\u91cf\u5b50\u6bd4\u7279\u9891\u7387\u548cT1\u57fa\u7ebf\u7684\u672c\u5f81\u5668\u4ef6\u53c2\u6570\u9ad8\u5ea6\u7a33\u5b9a\uff08\u9891\u7387\u504f\u5dee<0.5%\uff0c\u76f8\u5e72\u6027\u57fa\u7ebf\u672a\u9000\u5316\uff09\uff1b\u800c\u73af\u5883\u53d8\u91cf\uff08\u80cc\u666f\u78c1\u901a\u504f\u79fb\u548cTLS\u7f3a\u9677\u5fae\u89c2\u666f\u89c2\uff09\u5728\u6bcf\u6b21\u70ed\u5faa\u73af\u540e\u90fd\u4f1a\u53d1\u751f\u663e\u8457\u968f\u673a\u91cd\u6784\u3002\u70ed\u5faa\u73af\u76f8\u5f53\u4e8e\u5bf9\u5c40\u90e8\u7f3a\u9677\u73af\u5883\u8fdb\u884c\"\u786c\u91cd\u7f6e\"\uff0c\u5f15\u5165\u7684\u5149\u8c31\u968f\u673a\u5316\u7a0b\u5ea6\u76f8\u5f53\u4e8e\u6570\u5343\u5c0f\u65f6\u7684\u8fde\u7eed\u4f4e\u6e29\u6f14\u5316\u3002", "conclusion": "\u867d\u7136\u5236\u9020\u8d28\u91cf\u5f97\u4ee5\u4fdd\u6301\uff0c\u4f46\u6bcf\u6b21\u70ed\u5faa\u73af\u90fd\u4f1a\u4ea7\u751f\u7edf\u8ba1\u4e0a\u4e0d\u540c\u7684\u566a\u58f0\u5b9e\u73b0\uff0c\u8fd9\u8981\u6c42\u5927\u89c4\u6a21\u91cf\u5b50\u7cfb\u7edf\u9700\u8981\u91c7\u7528\u81ea\u52a8\u91cd\u65b0\u6821\u51c6\u7b56\u7565\u3002\u70ed\u5faa\u73af\u4f1a\u91cd\u7f6e\u7f3a\u9677\u73af\u5883\uff0c\u4f46\u4e0d\u4f1a\u635f\u5bb3\u672c\u5f81\u5668\u4ef6\u53c2\u6570\u3002"}}
{"id": "2602.07144", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07144", "abs": "https://arxiv.org/abs/2602.07144", "authors": ["Samuel Daulton", "David Eriksson", "Maximilian Balandat", "Eytan Bakshy"], "title": "BONSAI: Bayesian Optimization with Natural Simplicity and Interpretability", "comment": "26 pages", "summary": "Bayesian optimization (BO) is a popular technique for sample-efficient optimization of black-box functions. In many applications, the parameters being tuned come with a carefully engineered default configuration, and practitioners only want to deviate from this default when necessary. Standard BO, however, does not aim to minimize deviation from the default and, in practice, often pushes weakly relevant parameters to the boundary of the search space. This makes it difficult to distinguish between important and spurious changes and increases the burden of vetting recommendations when the optimization objective omits relevant operational considerations. We introduce BONSAI, a default-aware BO policy that prunes low-impact deviations from a default configuration while explicitly controlling the loss in acquisition value. BONSAI is compatible with a variety of acquisition functions, including expected improvement and upper confidence bound (GP-UCB). We theoretically bound the regret incurred by BONSAI, showing that, under certain conditions, it enjoys the same no-regret property as vanilla GP-UCB. Across many real-world applications, we empirically find that BONSAI substantially reduces the number of non-default parameters in recommended configurations while maintaining competitive optimization performance, with little effect on wall time.", "AI": {"tldr": "BONSAI\u662f\u4e00\u79cd\u8003\u8651\u9ed8\u8ba4\u914d\u7f6e\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u4f18\u5316\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u5bf9\u9ed8\u8ba4\u53c2\u6570\u7684\u504f\u79bb\uff0c\u4fbf\u4e8e\u5b9e\u8df5\u8005\u7406\u89e3\u548c\u9a8c\u8bc1\u63a8\u8350\u914d\u7f6e\u3002", "motivation": "\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u53c2\u6570\u901a\u5e38\u6709\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u9ed8\u8ba4\u914d\u7f6e\uff0c\u5b9e\u8df5\u8005\u53ea\u5e0c\u671b\u5728\u5fc5\u8981\u65f6\u504f\u79bb\u9ed8\u8ba4\u503c\u3002\u4f46\u6807\u51c6\u8d1d\u53f6\u65af\u4f18\u5316\u4e0d\u5173\u6ce8\u6700\u5c0f\u5316\u504f\u79bb\uff0c\u7ecf\u5e38\u5c06\u5f31\u76f8\u5173\u53c2\u6570\u63a8\u5230\u641c\u7d22\u8fb9\u754c\uff0c\u8fd9\u4f7f\u5f97\u96be\u4ee5\u533a\u5206\u91cd\u8981\u548c\u865a\u5047\u7684\u66f4\u6539\uff0c\u589e\u52a0\u4e86\u5ba1\u67e5\u63a8\u8350\u914d\u7f6e\u7684\u8d1f\u62c5\u3002", "method": "BONSAI\u662f\u4e00\u79cd\u9ed8\u8ba4\u611f\u77e5\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u7b56\u7565\uff0c\u5b83\u4f1a\u4fee\u526a\u5bf9\u9ed8\u8ba4\u914d\u7f6e\u7684\u4f4e\u5f71\u54cd\u504f\u79bb\uff0c\u540c\u65f6\u660e\u786e\u63a7\u5236\u83b7\u53d6\u51fd\u6570\u7684\u635f\u5931\u503c\u3002\u8be5\u65b9\u6cd5\u517c\u5bb9\u591a\u79cd\u83b7\u53d6\u51fd\u6570\uff08\u5982\u671f\u671b\u6539\u8fdb\u548cGP-UCB\uff09\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u5176\u9057\u61be\u754c\u9650\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0cBONSAI\u5177\u6709\u4e0e\u6807\u51c6GP-UCB\u76f8\u540c\u7684\u65e0\u9057\u61be\u6027\u8d28\u3002\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0cBONSAI\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u4f18\u5316\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u63a8\u8350\u914d\u7f6e\u4e2d\u7684\u975e\u9ed8\u8ba4\u53c2\u6570\u6570\u91cf\uff0c\u5bf9\u8fd0\u884c\u65f6\u95f4\u5f71\u54cd\u5f88\u5c0f\u3002", "conclusion": "BONSAI\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u4f18\u5316\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u5bf9\u9ed8\u8ba4\u914d\u7f6e\u7684\u4e0d\u5fc5\u8981\u504f\u79bb\uff0c\u4fbf\u4e8e\u5b9e\u8df5\u8005\u7406\u89e3\u548c\u9a8c\u8bc1\u4f18\u5316\u7ed3\u679c\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2602.08487", "categories": ["gr-qc", "astro-ph.CO"], "pdf": "https://arxiv.org/pdf/2602.08487", "abs": "https://arxiv.org/abs/2602.08487", "authors": ["Madina Abilmazhinova", "Diana Kulubayeva", "Hrishikesh Chakrabarty", "Daniele Malafarina"], "title": "Can UV meet IR in the Swiss cheese?", "comment": "12 pages, 2 figures; Comments welcome", "summary": "We consider the embedding of regular black holes in an expanding universe and study how the ultraviolet modifications to the Schwarzschild geometry that regularize the black hole singularity affect the exterior universe's expansion rate. We consider several proposals for the regular black hole geometry and obtain the corresponding Friedmann equations for a universe filled only with dust and black holes. We show that different proposals have different implications which may be distinguished. We then test the hypothesis that the UV corrections to the black hole geometry may be responsible for the current phase of accelerated expansion. To this aim we constrain the value of the regular black hole UV cutoff parameter from observations. Interestingly we find that the best fit is obtained by values of the parameter corresponding to regular horizonless compact objects.", "AI": {"tldr": "\u7814\u7a76\u6b63\u5219\u9ed1\u6d1e\u5728\u81a8\u80c0\u5b87\u5b99\u4e2d\u7684\u5d4c\u5165\uff0c\u63a2\u8ba8\u9ed1\u6d1e\u5947\u70b9\u6b63\u5219\u5316\u7684\u7d2b\u5916\u4fee\u6b63\u5982\u4f55\u5f71\u54cd\u5916\u90e8\u5b87\u5b99\u7684\u81a8\u80c0\u901f\u7387\uff0c\u5e76\u6d4b\u8bd5\u8fd9\u4e9b\u4fee\u6b63\u662f\u5426\u53ef\u80fd\u662f\u5f53\u524d\u52a0\u901f\u81a8\u80c0\u7684\u539f\u56e0\u3002", "motivation": "\u63a2\u7d22\u6b63\u5219\u9ed1\u6d1e\u7684\u7d2b\u5916\u51e0\u4f55\u4fee\u6b63\u5bf9\u5b87\u5b99\u81a8\u80c0\u52a8\u529b\u5b66\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u68c0\u9a8c\u8fd9\u4e9b\u4fee\u6b63\u662f\u5426\u80fd\u89e3\u91ca\u5f53\u524d\u7684\u5b87\u5b99\u52a0\u901f\u81a8\u80c0\u73b0\u8c61\u3002", "method": "\u8003\u8651\u591a\u79cd\u6b63\u5219\u9ed1\u6d1e\u51e0\u4f55\u65b9\u6848\uff0c\u63a8\u5bfc\u76f8\u5e94\u7684Friedmann\u65b9\u7a0b\uff08\u4ec5\u5305\u542b\u5c18\u57c3\u548c\u9ed1\u6d1e\u7684\u5b87\u5b99\uff09\uff0c\u901a\u8fc7\u89c2\u6d4b\u6570\u636e\u7ea6\u675f\u6b63\u5219\u9ed1\u6d1e\u7684\u7d2b\u5916\u622a\u65ad\u53c2\u6570\u3002", "result": "\u4e0d\u540c\u6b63\u5219\u9ed1\u6d1e\u65b9\u6848\u5bf9\u5b87\u5b99\u81a8\u80c0\u6709\u4e0d\u540c\u7684\u5f71\u54cd\uff0c\u53ef\u88ab\u533a\u5206\u3002\u6700\u4f73\u62df\u5408\u53c2\u6570\u503c\u5bf9\u5e94\u4e8e\u6b63\u5219\u65e0\u89c6\u754c\u81f4\u5bc6\u5929\u4f53\uff0c\u800c\u975e\u4f20\u7edf\u9ed1\u6d1e\u3002", "conclusion": "\u6b63\u5219\u9ed1\u6d1e\u7684\u7d2b\u5916\u51e0\u4f55\u4fee\u6b63\u53ef\u80fd\u5f71\u54cd\u5b87\u5b99\u81a8\u80c0\u52a8\u529b\u5b66\uff0c\u6700\u4f73\u62df\u5408\u53c2\u6570\u652f\u6301\u6b63\u5219\u65e0\u89c6\u754c\u81f4\u5bc6\u5929\u4f53\uff0c\u4e3a\u89e3\u91ca\u5b87\u5b99\u52a0\u901f\u81a8\u80c0\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.07531", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2602.07531", "abs": "https://arxiv.org/abs/2602.07531", "authors": ["Lei Chen", "Zhe-qi Yang", "Liang Bin", "Zhi-Rong Zhong"], "title": "Squeezing-enhanced dual-channel interference for ground-state cooling of a levitated micromagnet with low quality factor", "comment": null, "summary": "Cooling the center-of-mass (CM) motion of a macroscopic oscillator to its quantum ground state is a fundamental prerequisite for testing quantum mechanics at macroscopic scales. However, achieving this goal is currently hindered by the stringent requirement for an ultrahigh mechanical quality factor ($Q_c$). Here, we propose a dual-channel cooling scheme based on squeezing-enhanced quantum interference within a hybrid levitated cavity-magnomechanical system to overcome this limitation.\n  By synergizing squeezing effects with quantum interference between the magnon-CM and cavity-CM channels, our scheme simultaneously suppresses Stokes (heating) scattering while enhancing anti-Stokes (cooling) scattering.~We demonstrate that this cooling mechanism reduces the critical $Q_c$ required for ground-state cooling by three orders of magnitude, making it achievable in the experimentally accessible regime of $Q_c \\sim 10^4$. Furthermore, the net cooling rate is enhanced by nearly 180-fold compared to that of conventional single-channel cooling. This improvement is accompanied by a two orders of magnitude reduction in both the steady-state CM occupancy and the cooling time. Importantly, this enhanced performance remains robust even deep within the unresolved-sideband regime. Our results provide a feasible path toward preparing macroscopic quantum states by actively controlling the cooling dynamics, thereby relaxing the constraints on intrinsic material properties.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u538b\u7f29\u589e\u5f3a\u91cf\u5b50\u5e72\u6d89\u7684\u53cc\u901a\u9053\u51b7\u5374\u65b9\u6848\uff0c\u5c06\u5b8f\u89c2\u632f\u5b50\u57fa\u6001\u51b7\u5374\u6240\u9700\u673a\u68b0\u54c1\u8d28\u56e0\u6570\u964d\u4f4e\u4e09\u4e2a\u6570\u91cf\u7ea7\uff0c\u51b7\u5374\u901f\u7387\u63d0\u5347\u8fd1180\u500d", "motivation": "\u5b8f\u89c2\u632f\u5b50\u8d28\u5fc3\u8fd0\u52a8\u51b7\u5374\u81f3\u91cf\u5b50\u57fa\u6001\u662f\u6d4b\u8bd5\u5b8f\u89c2\u5c3a\u5ea6\u91cf\u5b50\u529b\u5b66\u7684\u57fa\u7840\u524d\u63d0\uff0c\u4f46\u5f53\u524d\u53d7\u9650\u4e8e\u5bf9\u8d85\u9ad8\u673a\u68b0\u54c1\u8d28\u56e0\u6570\u7684\u4e25\u683c\u8981\u6c42", "method": "\u5728\u6df7\u5408\u60ac\u6d6e\u8154-\u78c1\u673a\u68b0\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u538b\u7f29\u6548\u5e94\u4e0e\u78c1\u5b50-\u8d28\u5fc3\u548c\u8154-\u8d28\u5fc3\u901a\u9053\u95f4\u91cf\u5b50\u5e72\u6d89\u7684\u534f\u540c\u4f5c\u7528\uff0c\u6291\u5236\u65af\u6258\u514b\u65af\u6563\u5c04\u540c\u65f6\u589e\u5f3a\u53cd\u65af\u6258\u514b\u65af\u6563\u5c04", "result": "\u5c06\u57fa\u6001\u51b7\u5374\u6240\u9700\u4e34\u754c\u673a\u68b0\u54c1\u8d28\u56e0\u6570\u964d\u4f4e\u4e09\u4e2a\u6570\u91cf\u7ea7\u81f3\u5b9e\u9a8c\u53ef\u5b9e\u73b0\u768410^4\u91cf\u7ea7\uff0c\u51c0\u51b7\u5374\u901f\u7387\u63d0\u5347\u8fd1180\u500d\uff0c\u7a33\u6001\u8d28\u5fc3\u5360\u636e\u6570\u548c\u51b7\u5374\u65f6\u95f4\u5747\u964d\u4f4e\u4e24\u4e2a\u6570\u91cf\u7ea7", "conclusion": "\u8be5\u65b9\u6848\u4e3a\u901a\u8fc7\u4e3b\u52a8\u63a7\u5236\u51b7\u5374\u52a8\u529b\u5b66\u5236\u5907\u5b8f\u89c2\u91cf\u5b50\u6001\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u653e\u5bbd\u4e86\u5bf9\u6750\u6599\u672c\u5f81\u6027\u8d28\u7684\u9650\u5236\u8981\u6c42"}}
{"id": "2602.07145", "categories": ["cs.LG", "cs.CL", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.07145", "abs": "https://arxiv.org/abs/2602.07145", "authors": ["Zhiqi Bu", "Shiyun Xu", "Jialin Mao"], "title": "Convex Dominance in Deep Learning I: A Scaling Law of Loss and Learning Rate", "comment": "Part of a planned series to understand and leverage the convexity in deep learning. Accepted to ICLR 2026", "summary": "Deep learning has non-convex loss landscape and its optimization dynamics is hard to analyze or control. Nevertheless, the dynamics can be empirically convex-like across various tasks, models, optimizers, hyperparameters, etc. In this work, we examine the applicability of convexity and Lipschitz continuity in deep learning, in order to precisely control the loss dynamics via the learning rate schedules. We illustrate that deep learning quickly becomes weakly convex after a short period of training, and the loss is predicable by an upper bound on the last iterate, which further informs the scaling of optimal learning rate. Through the lens of convexity, we build scaling laws of learning rates and losses that extrapolate as much as 80X across training horizons and 70X across model sizes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u4e2d\u7684\u51f8\u6027\u73b0\u8c61\uff0c\u53d1\u73b0\u8bad\u7ec3\u540e\u635f\u5931\u51fd\u6570\u5448\u73b0\u5f31\u51f8\u6027\uff0c\u5e76\u57fa\u4e8e\u6b64\u5efa\u7acb\u5b66\u4e60\u7387\u548c\u635f\u5931\u7684\u7f29\u653e\u89c4\u5f8b\uff0c\u80fd\u8de8\u8bad\u7ec3\u65f6\u957f\u548c\u6a21\u578b\u89c4\u6a21\u8fdb\u884c\u5927\u5e45\u5916\u63a8\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5177\u6709\u975e\u51f8\u635f\u5931\u66f2\u9762\uff0c\u5176\u4f18\u5316\u52a8\u6001\u96be\u4ee5\u5206\u6790\u6216\u63a7\u5236\u3002\u7136\u800c\uff0c\u7ecf\u9a8c\u4e0a\u89c2\u5bdf\u5230\u6df1\u5ea6\u5b66\u4e60\u5728\u5404\u79cd\u4efb\u52a1\u3001\u6a21\u578b\u3001\u4f18\u5316\u5668\u7b49\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u7c7b\u4f3c\u51f8\u4f18\u5316\u7684\u884c\u4e3a\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76\u51f8\u6027\u548cLipschitz\u8fde\u7eed\u6027\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u9002\u7528\u6027\uff0c\u4ee5\u901a\u8fc7\u5b66\u4e60\u7387\u8c03\u5ea6\u7cbe\u786e\u63a7\u5236\u635f\u5931\u52a8\u6001\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u53d1\u73b0\u8bad\u7ec3\u540e\u635f\u5931\u51fd\u6570\u5feb\u901f\u5448\u73b0\u5f31\u51f8\u6027\u3002\u5229\u7528\u51f8\u6027\u7406\u8bba\uff0c\u63a8\u5bfc\u51fa\u6700\u540e\u8fed\u4ee3\u7684\u4e0a\u754c\u6765\u9884\u6d4b\u635f\u5931\uff0c\u5e76\u57fa\u4e8e\u6b64\u786e\u5b9a\u6700\u4f18\u5b66\u4e60\u7387\u7684\u7f29\u653e\u89c4\u5f8b\u3002\u4ece\u51f8\u6027\u89c6\u89d2\u5efa\u7acb\u5b66\u4e60\u7387\u548c\u635f\u5931\u7684\u7f29\u653e\u5b9a\u5f8b\u3002", "result": "\u7814\u7a76\u8868\u660e\u6df1\u5ea6\u5b66\u4e60\u5728\u77ed\u671f\u8bad\u7ec3\u540e\u5373\u5448\u73b0\u5f31\u51f8\u6027\uff0c\u635f\u5931\u53ef\u901a\u8fc7\u6700\u540e\u8fed\u4ee3\u7684\u4e0a\u754c\u9884\u6d4b\u3002\u5efa\u7acb\u7684\u7f29\u653e\u5b9a\u5f8b\u80fd\u5728\u8bad\u7ec3\u65f6\u957f\u4e0a\u5916\u63a880\u500d\uff0c\u5728\u6a21\u578b\u89c4\u6a21\u4e0a\u5916\u63a870\u500d\u3002\u8fd9\u4e3a\u5b66\u4e60\u7387\u8c03\u5ea6\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u548c\u5b9e\u7528\u6307\u5bfc\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u867d\u7136\u672c\u8d28\u975e\u51f8\uff0c\u4f46\u5728\u5b9e\u9645\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u5f31\u51f8\u6027\u7279\u5f81\u3002\u5229\u7528\u8fd9\u4e00\u7279\u6027\u53ef\u4ee5\u5efa\u7acb\u6709\u6548\u7684\u5b66\u4e60\u7387\u7f29\u653e\u5b9a\u5f8b\uff0c\u5b9e\u73b0\u5bf9\u635f\u5931\u52a8\u6001\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c6\u89d2\u548c\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2602.08507", "categories": ["gr-qc", "hep-th"], "pdf": "https://arxiv.org/pdf/2602.08507", "abs": "https://arxiv.org/abs/2602.08507", "authors": ["Jordan Gu\u00e9", "Tom Krokotsch", "Gudrid Moortgat-Pick"], "title": "Covariant eigenmode overlap formalism for gravitational wave signals in electromagnetic cavities", "comment": null, "summary": "We develop a coordinate invariant formalism which describes the mechanical and electromagnetic interaction of gravitational waves (GWs) with a wide class of resonant detectors. We solve the GW-modified equations of electrodynamics and elasticity with dynamic boundary conditions using an eigenmode expansion. Furthermore, we take damping effects and electromagnetic back-action on mechanical systems covariantly into account. The resulting coupling coefficients are particularly useful for high-frequency gravitational wave experiments using microwave cavities and allow a straightforward numerical implementation for arbitrary detector geometries.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5750\u6807\u4e0d\u53d8\u7684\u5f62\u5f0f\u4f53\u7cfb\uff0c\u7528\u4e8e\u63cf\u8ff0\u5f15\u529b\u6ce2\u4e0e\u5404\u7c7b\u8c10\u632f\u63a2\u6d4b\u5668\u4e4b\u95f4\u7684\u673a\u68b0\u548c\u7535\u78c1\u76f8\u4e92\u4f5c\u7528\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4f7f\u7528\u5fae\u6ce2\u8154\u7684\u9ad8\u9891\u5f15\u529b\u6ce2\u5b9e\u9a8c\u3002", "motivation": "\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u901a\u7528\u7684\u7406\u8bba\u6846\u67b6\u6765\u63cf\u8ff0\u5f15\u529b\u6ce2\u4e0e\u8c10\u632f\u63a2\u6d4b\u5668\uff08\u7279\u522b\u662f\u5fae\u6ce2\u8154\uff09\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u8be5\u6846\u67b6\u5e94\u80fd\u5904\u7406\u590d\u6742\u7684\u51e0\u4f55\u5f62\u72b6\u3001\u963b\u5c3c\u6548\u5e94\u548c\u7535\u78c1\u53cd\u4f5c\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u5750\u6807\u4e0d\u53d8\u7684\u5f62\u5f0f\u4f53\u7cfb\uff0c\u6c42\u89e3\u4e86\u5f15\u529b\u6ce2\u4fee\u6b63\u7684\u7535\u78c1\u5b66\u548c\u5f39\u6027\u65b9\u7a0b\uff0c\u91c7\u7528\u672c\u5f81\u6a21\u5c55\u5f00\u65b9\u6cd5\u5904\u7406\u52a8\u6001\u8fb9\u754c\u6761\u4ef6\uff0c\u5e76\u534f\u53d8\u5730\u8003\u8651\u4e86\u963b\u5c3c\u6548\u5e94\u548c\u7535\u78c1\u53cd\u4f5c\u7528\u3002", "result": "\u5f97\u5230\u4e86\u9002\u7528\u4e8e\u4efb\u610f\u63a2\u6d4b\u5668\u51e0\u4f55\u5f62\u72b6\u7684\u8026\u5408\u7cfb\u6570\uff0c\u8fd9\u4e9b\u7cfb\u6570\u7279\u522b\u6709\u5229\u4e8e\u9ad8\u9891\u5f15\u529b\u6ce2\u5b9e\u9a8c\uff0c\u5e76\u5141\u8bb8\u76f4\u63a5\u7684\u6570\u503c\u5b9e\u73b0\u3002", "conclusion": "\u8be5\u5f62\u5f0f\u4f53\u7cfb\u4e3a\u5206\u6790\u5f15\u529b\u6ce2\u4e0e\u8c10\u632f\u63a2\u6d4b\u5668\u7684\u76f8\u4e92\u4f5c\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u800c\u7075\u6d3b\u7684\u5de5\u5177\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5fae\u6ce2\u8154\u7b49\u9ad8\u9891\u5b9e\u9a8c\u7684\u6570\u503c\u6a21\u62df\u548c\u8bbe\u8ba1\u4f18\u5316\u3002"}}
{"id": "2602.07536", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2602.07536", "abs": "https://arxiv.org/abs/2602.07536", "authors": ["Stefan Johansson", "Alexander Bukschat", "Dennis L\u00f6nard", "Alena Erlenbach", "Jonas Gutsche", "Artur Widera"], "title": "Characterization of Autofluorescence in Optical Fibers for NV-based Sensing Applications", "comment": "15 pages, 7 figures", "summary": "Optical fibers are crucial for guiding light in various sensing applications. Especially for quantum sensors such as the nitrogen-vacancy (NV) center in diamond, they enable light control and device miniaturization. However, fluorescence and scattering within the fiber, often referred to as fiber background, autofluorescence, or autoluminescence, can overlap spectrally with the NV centers' fluorescence, degrading the signal-to-noise ratio and thus limiting sensor sensitivity. Here, we investigate the optical spectra of standard optical fibers, considering material dependencies, physical influences, and their fluorescence scaling with excitation power and wavelength. Our results identify spectral components and fiber types with minimal unwanted background signals, guiding the selection of optimal fibers for NV-based quantum sensing.", "AI": {"tldr": "\u7814\u7a76\u6807\u51c6\u5149\u7ea4\u7684\u5149\u8c31\u7279\u6027\uff0c\u8bc6\u522b\u80cc\u666f\u4fe1\u53f7\u6700\u5c0f\u7684\u5149\u7ea4\u7c7b\u578b\uff0c\u4e3a\u57fa\u4e8eNV\u4e2d\u5fc3\u7684\u91cf\u5b50\u4f20\u611f\u9009\u62e9\u6700\u4f18\u5149\u7ea4", "motivation": "\u5149\u7ea4\u80cc\u666f\u8367\u5149\u548c\u6563\u5c04\u4f1a\u4e0eNV\u4e2d\u5fc3\u8367\u5149\u5149\u8c31\u91cd\u53e0\uff0c\u964d\u4f4e\u4fe1\u566a\u6bd4\uff0c\u9650\u5236\u91cf\u5b50\u4f20\u611f\u5668\u7684\u7075\u654f\u5ea6", "method": "\u7814\u7a76\u6807\u51c6\u5149\u7ea4\u7684\u5149\u5b66\u5149\u8c31\uff0c\u8003\u8651\u6750\u6599\u4f9d\u8d56\u6027\u3001\u7269\u7406\u5f71\u54cd\u56e0\u7d20\uff0c\u4ee5\u53ca\u8367\u5149\u968f\u6fc0\u53d1\u529f\u7387\u548c\u6ce2\u957f\u7684\u53d8\u5316\u89c4\u5f8b", "result": "\u8bc6\u522b\u51fa\u5149\u8c31\u6210\u5206\u548c\u80cc\u666f\u4fe1\u53f7\u6700\u5c0f\u7684\u5149\u7ea4\u7c7b\u578b", "conclusion": "\u4e3a\u57fa\u4e8eNV\u4e2d\u5fc3\u7684\u91cf\u5b50\u4f20\u611f\u5e94\u7528\u63d0\u4f9b\u4e86\u9009\u62e9\u6700\u4f18\u5149\u7ea4\u7684\u6307\u5bfc"}}
{"id": "2602.07150", "categories": ["cs.LG", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.07150", "abs": "https://arxiv.org/abs/2602.07150", "authors": ["Bjarni Haukur Bjarnason", "Andr\u00e9 Silva", "Martin Monperrus"], "title": "On Randomness in Agentic Evals", "comment": null, "summary": "Agentic systems are evaluated on benchmarks where agents interact with environments to solve tasks. Most papers report a pass@1 score computed from a single run per task, assuming this gives a reliable performance estimate. We test this assumption by collecting 60,000 agentic trajectories on SWE-Bench-Verified, spanning three models and two scaffolds. We find substantial variance: single-run pass@1 estimates vary by 2.2 to 6.0 percentage points depending on which run is selected, with standard deviations exceeding 1.5 percentage points even at temperature 0. This variance has critical implications: reported improvements of 2--3 percentage points may reflect evaluation noise rather than genuine algorithmic progress. Through token-level analysis, we show that trajectories diverge early, often within the first few percent of tokens, and that these small differences cascade into different solution strategies. To enable reliable evaluation of agentic systems, we recommend three concrete practices: (1) estimate pass@1 from multiple independent runs per task, especially when measuring small improvements, (2) use statistical power analysis to determine the number of runs needed to detect expected effect sizes, and (3) consider metrics like pass@k (optimistic bound) and pass^k (pessimistic bound) with k>1 to better characterize the full performance envelope. While these practices increase evaluation cost, they are essential for distinguishing genuine scientific progress from statistical noise.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5355\u6b21\u8fd0\u884c\u8bc4\u4f30\u667a\u80fd\u4f53\u7cfb\u7edf\u5b58\u5728\u663e\u8457\u65b9\u5dee\uff0c2-3\u4e2a\u767e\u5206\u70b9\u7684\u6539\u8fdb\u53ef\u80fd\u53ea\u662f\u7edf\u8ba1\u566a\u58f0\u800c\u975e\u771f\u5b9e\u8fdb\u6b65\uff0c\u5efa\u8bae\u91c7\u7528\u591a\u6b21\u8fd0\u884c\u3001\u7edf\u8ba1\u529f\u6548\u5206\u6790\u548cpass@k\u7b49\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u667a\u80fd\u4f53\u7cfb\u7edf\u8bc4\u4f30\u901a\u5e38\u57fa\u4e8e\u5355\u6b21\u8fd0\u884c\u8ba1\u7b97pass@1\u5206\u6570\uff0c\u5e76\u5047\u8bbe\u8fd9\u80fd\u63d0\u4f9b\u53ef\u9760\u7684\u6027\u80fd\u4f30\u8ba1\u3002\u672c\u6587\u65e8\u5728\u9a8c\u8bc1\u8fd9\u4e00\u5047\u8bbe\uff0c\u63a2\u7a76\u5355\u6b21\u8fd0\u884c\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u5728SWE-Bench-Verified\u57fa\u51c6\u4e0a\u6536\u96c660,000\u4e2a\u667a\u80fd\u4f53\u8f68\u8ff9\uff0c\u6db5\u76d6\u4e09\u4e2a\u6a21\u578b\u548c\u4e24\u79cd\u811a\u624b\u67b6\u3002\u901a\u8fc7token\u7ea7\u5206\u6790\u8ffd\u8e2a\u8f68\u8ff9\u5206\u6b67\u70b9\uff0c\u5e76\u5206\u6790\u8bc4\u4f30\u65b9\u5dee\u5bf9\u6027\u80fd\u4f30\u8ba1\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u663e\u8457\u65b9\u5dee\uff1a\u5355\u6b21\u8fd0\u884cpass@1\u4f30\u8ba1\u56e0\u9009\u62e9\u4e0d\u540c\u8fd0\u884c\u800c\u5f022.2-6.0\u4e2a\u767e\u5206\u70b9\uff0c\u5373\u4f7f\u5728\u6e29\u5ea60\u65f6\u6807\u51c6\u5dee\u4e5f\u8d85\u8fc71.5\u4e2a\u767e\u5206\u70b9\u3002\u8f68\u8ff9\u5728\u65e9\u671f\uff08\u524d\u51e0\u4e2a\u767e\u5206\u70b9\u7684token\uff09\u5c31\u51fa\u73b0\u5206\u6b67\uff0c\u5c0f\u5dee\u5f02\u4f1a\u7ea7\u8054\u6210\u4e0d\u540c\u7684\u89e3\u51b3\u7b56\u7565\u3002", "conclusion": "\u5efa\u8bae\u4e09\u9879\u5177\u4f53\u5b9e\u8df5\uff1a(1) \u6bcf\u4e2a\u4efb\u52a1\u4ece\u591a\u6b21\u72ec\u7acb\u8fd0\u884c\u4f30\u8ba1pass@1\uff0c(2) \u4f7f\u7528\u7edf\u8ba1\u529f\u6548\u5206\u6790\u786e\u5b9a\u68c0\u6d4b\u9884\u671f\u6548\u5e94\u5927\u5c0f\u6240\u9700\u7684\u8fd0\u884c\u6b21\u6570\uff0c(3) \u8003\u8651\u4f7f\u7528k>1\u7684pass@k\u548cpass^k\u6307\u6807\u3002\u8fd9\u4e9b\u65b9\u6cd5\u5bf9\u533a\u5206\u771f\u5b9e\u79d1\u5b66\u8fdb\u5c55\u4e0e\u7edf\u8ba1\u566a\u58f0\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2602.08522", "categories": ["gr-qc", "astro-ph.CO"], "pdf": "https://arxiv.org/pdf/2602.08522", "abs": "https://arxiv.org/abs/2602.08522", "authors": ["Robert Alicki"], "title": "Thermal Vacuum Cosmology Explains Hubble Tension", "comment": "3 pages, 2 figures", "summary": "It is argued that the previously proposed modification of the standard (flat) inflationary $\u039bCDM$ model in which cosmological constant is replaced by thermal energy of expanding vacum, characterized by the Gibbons-Hawking temperature, explains the origin of notorious ``Hubble tension''.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u7528\u81a8\u80c0\u771f\u7a7a\u7684\u5409\u672c\u65af-\u970d\u91d1\u6e29\u5ea6\u70ed\u80fd\u91cf\u66ff\u4ee3\u5b87\u5b99\u5e38\u6570\uff0c\u4ece\u800c\u89e3\u91ca\u54c8\u52c3\u5f20\u529b\u7684\u8d77\u6e90", "motivation": "\u89e3\u51b3\u6807\u51c6\u5e73\u5766\u66b4\u80c0\u039bCDM\u6a21\u578b\u4e2d\u5b58\u5728\u7684\"\u54c8\u52c3\u5f20\u529b\"\u95ee\u9898\uff0c\u5373\u65e9\u671f\u5b87\u5b99\u6d4b\u91cf\u4e0e\u665a\u671f\u5b87\u5b99\u6d4b\u91cf\u5f97\u5230\u7684\u54c8\u52c3\u5e38\u6570\u4e0d\u4e00\u81f4\u7684\u77db\u76fe", "method": "\u4fee\u6539\u6807\u51c6\u5e73\u5766\u66b4\u80c0\u039bCDM\u6a21\u578b\uff0c\u7528\u81a8\u80c0\u771f\u7a7a\u7684\u5409\u672c\u65af-\u970d\u91d1\u6e29\u5ea6\u70ed\u80fd\u91cf\u66ff\u4ee3\u5b87\u5b99\u5e38\u6570\u039b", "result": "\u8be5\u4fee\u6539\u80fd\u591f\u89e3\u91ca\u54c8\u52c3\u5f20\u529b\u7684\u8d77\u6e90\uff0c\u4e3a\u8fd9\u4e00\u957f\u671f\u5b58\u5728\u7684\u5b87\u5b99\u5b66\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6848", "conclusion": "\u901a\u8fc7\u7528\u81a8\u80c0\u771f\u7a7a\u7684\u70ed\u80fd\u91cf\u66ff\u4ee3\u5b87\u5b99\u5e38\u6570\uff0c\u53ef\u4ee5\u6210\u529f\u89e3\u91ca\u54c8\u52c3\u5f20\u529b\uff0c\u8fd9\u4e3a\u5b87\u5b99\u5b66\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u4fee\u6b63\u65b9\u5411"}}
{"id": "2602.07578", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.07578", "abs": "https://arxiv.org/abs/2602.07578", "authors": ["Ameya S. Bhave", "Navnil Choudhury", "Andrew Nemec", "Kanad Basu"], "title": "BiBiEQ: Bivariate Bicycle Codes on Erasure Qubits", "comment": "Accepted in IEEE QCNC 2026", "summary": "Erasure qubits reduce overhead in fault-tolerant quantum error correction (QEC) by converting dominant faults into detectable errors known as erasures. They have demonstrated notable improvements in thresholds and scaling in surface and Floquet code memories. In this work, we use erasure qubits on Bivariate Bicycle (BB) codes from the quantum low-density parity-check (QLDPC) regime. Owing to their sparse structure and favorable rate-distance trade-offs, BB codes are practical candidates for QEC. We introduce BiBiEQ, a novel framework that compiles a given BB code into an erasure-aware memory circuit C_E. This erasure circuit C_E comprises erasure checks (ECs), resets, and erasures spread over a user-specified erasure check schedule (2EC, 4EC). BiBiEQ converts this erasure circuit C_E into the stabilizer circuit C for general-purpose decoding. BiBiEQ provides two engines for this conversion, BiBiEQ-Exact and BiBiEQ-Approx. BiBiEQ-Exact preserves the joint-erasure correlations and serves as our accuracy benchmark, while BiBiEQ-Approx uses an independence approximation to accelerate large sweeps and expose accuracy-throughput trade-offs. Using BiBiEQ, we decode the stabilizer circuits to get a per-round logical error rate (LER) for the BB codes and quantify the effect of the EC schedules on the correctable operating region below the pseudo-threshold. The 4EC schedule keeps the accuracy of both engines close to one another, making BiBiEQ-Approx a reliable proxy for BiBiEQ-Exact for faster sweeps. Below the pseudo-threshold, the code distance (d) hop from distance (d) 6 to 10 yields a drop in LER by 10-17x larger than distance (d) 10 to 12, showing that most gains are realized by d=10.", "AI": {"tldr": "BiBiEQ\u6846\u67b6\u5c06\u53cc\u53d8\u91cf\u81ea\u884c\u8f66\u7801\u7f16\u8bd1\u4e3a\u64e6\u9664\u611f\u77e5\u91cf\u5b50\u7535\u8def\uff0c\u901a\u8fc7\u64e6\u9664\u68c0\u67e5\u8c03\u5ea6\u63d0\u5347\u91cf\u5b50\u7ea0\u9519\u6027\u80fd\uff0c\u5728\u8ddd\u79bb10\u65f6\u5b9e\u73b0\u663e\u8457\u903b\u8f91\u9519\u8bef\u7387\u964d\u4f4e\u3002", "motivation": "\u64e6\u9664\u91cf\u5b50\u6bd4\u7279\u901a\u8fc7\u5c06\u4e3b\u8981\u6545\u969c\u8f6c\u6362\u4e3a\u53ef\u68c0\u6d4b\u7684\u64e6\u9664\u9519\u8bef\u6765\u964d\u4f4e\u5bb9\u9519\u91cf\u5b50\u7ea0\u9519\u7684\u5f00\u9500\uff0c\u5df2\u5728\u8868\u9762\u7801\u548cFloquet\u7801\u4e2d\u5c55\u793a\u51fa\u4f18\u52bf\u3002\u672c\u7814\u7a76\u65e8\u5728\u5c06\u64e6\u9664\u91cf\u5b50\u6bd4\u7279\u5e94\u7528\u4e8e\u5177\u6709\u7a00\u758f\u7ed3\u6784\u548c\u826f\u597d\u7387-\u8ddd\u79bb\u6743\u8861\u7684QLDPC\u53cc\u53d8\u91cf\u81ea\u884c\u8f66\u7801\u3002", "method": "\u63d0\u51faBiBiEQ\u6846\u67b6\uff0c\u5c06\u7ed9\u5b9a\u7684BB\u7801\u7f16\u8bd1\u4e3a\u64e6\u9664\u611f\u77e5\u5185\u5b58\u7535\u8defC_E\uff0c\u5305\u542b\u64e6\u9664\u68c0\u67e5\u3001\u91cd\u7f6e\u548c\u64e6\u9664\u64cd\u4f5c\u3002\u63d0\u4f9bBiBiEQ-Exact\uff08\u4fdd\u7559\u8054\u5408\u64e6\u9664\u76f8\u5173\u6027\uff09\u548cBiBiEQ-Approx\uff08\u4f7f\u7528\u72ec\u7acb\u6027\u8fd1\u4f3c\u52a0\u901f\uff09\u4e24\u79cd\u8f6c\u6362\u5f15\u64ce\uff0c\u5c06C_E\u8f6c\u6362\u4e3a\u901a\u7528\u89e3\u7801\u7684\u7a33\u5b9a\u5b50\u7535\u8def\u3002", "result": "4EC\u8c03\u5ea6\u4f7f\u4e24\u4e2a\u5f15\u64ce\u7684\u51c6\u786e\u6027\u4fdd\u6301\u63a5\u8fd1\uff0cBiBiEQ-Approx\u53ef\u4f5c\u4e3aBiBiEQ-Exact\u7684\u53ef\u9760\u4ee3\u7406\u8fdb\u884c\u5feb\u901f\u626b\u63cf\u3002\u5728\u4f2a\u9608\u503c\u4ee5\u4e0b\uff0c\u4ee3\u7801\u8ddd\u79bb\u4ece6\u589e\u52a0\u523010\u65f6\u903b\u8f91\u9519\u8bef\u7387\u4e0b\u964d10-17\u500d\uff0c\u800c\u4ece10\u589e\u52a0\u523012\u65f6\u4e0b\u964d\u5e45\u5ea6\u8f83\u5c0f\uff0c\u8868\u660e\u5927\u90e8\u5206\u589e\u76ca\u5728\u8ddd\u79bb10\u65f6\u5df2\u5b9e\u73b0\u3002", "conclusion": "BiBiEQ\u6846\u67b6\u6210\u529f\u5c06\u64e6\u9664\u91cf\u5b50\u6bd4\u7279\u5e94\u7528\u4e8eBB\u7801\uff0c\u901a\u8fc7\u4f18\u5316\u7684\u64e6\u9664\u68c0\u67e5\u8c03\u5ea6\u663e\u8457\u63d0\u5347\u91cf\u5b50\u7ea0\u9519\u6027\u80fd\uff0c\u8ddd\u79bb10\u662f\u83b7\u5f97\u5927\u90e8\u5206\u6027\u80fd\u589e\u76ca\u7684\u5173\u952e\u70b9\uff0c\u4e3aQLDPC\u7801\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2602.07154", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07154", "abs": "https://arxiv.org/abs/2602.07154", "authors": ["Ayush Roy", "Rudrasis Chakraborty", "Lav Varshney", "Vishnu Suresh Lokhande"], "title": "Beyond Pooling: Matching for Robust Generalization under Data Heterogeneity", "comment": "AISTATS 2026", "summary": "Pooling heterogeneous datasets across domains is a common strategy in representation learning, but naive pooling can amplify distributional asymmetries and yield biased estimators, especially in settings where zero-shot generalization is required. We propose a matching framework that selects samples relative to an adaptive centroid and iteratively refines the representation distribution. The double robustness and the propensity score matching for the inclusion of data domains make matching more robust than naive pooling and uniform subsampling by filtering out the confounding domains (the main cause of heterogeneity). Theoretical and empirical analyses show that, unlike naive pooling or uniform subsampling, matching achieves better results under asymmetric meta-distributions, which are also extended to non-Gaussian and multimodal real-world settings. Most importantly, we show that these improvements translate to zero-shot medical anomaly detection, one of the extreme forms of data heterogeneity and asymmetry. The code is available on https://github.com/AyushRoy2001/Beyond-Pooling.", "AI": {"tldr": "\u63d0\u51fa\u5339\u914d\u6846\u67b6\u89e3\u51b3\u5f02\u6784\u6570\u636e\u96c6\u6c60\u5316\u4e2d\u7684\u5206\u5e03\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8d28\u5fc3\u9009\u62e9\u548c\u8fed\u4ee3\u4f18\u5316\u63d0\u5347\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b", "motivation": "\u5f02\u6784\u6570\u636e\u96c6\u6c60\u5316\u4f1a\u653e\u5927\u5206\u5e03\u4e0d\u5bf9\u79f0\u6027\u5e76\u4ea7\u751f\u6709\u504f\u4f30\u8ba1\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u96f6\u6837\u672c\u6cdb\u5316\u7684\u573a\u666f\u4e2d\u3002\u4f20\u7edf\u6c60\u5316\u548c\u5747\u5300\u5b50\u91c7\u6837\u65e0\u6cd5\u6709\u6548\u5904\u7406\u57df\u95f4\u6df7\u6742\u56e0\u7d20", "method": "\u63d0\u51fa\u5339\u914d\u6846\u67b6\uff1a\u57fa\u4e8e\u81ea\u9002\u5e94\u8d28\u5fc3\u9009\u62e9\u6837\u672c\uff0c\u8fed\u4ee3\u4f18\u5316\u8868\u793a\u5206\u5e03\u3002\u7ed3\u5408\u53cc\u91cd\u9c81\u68d2\u6027\u548c\u503e\u5411\u5f97\u5206\u5339\u914d\u6765\u8fc7\u6ee4\u6df7\u6742\u57df\uff0c\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u7a33\u5065", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u8868\u660e\uff0c\u5339\u914d\u65b9\u6cd5\u5728\u4e0d\u5bf9\u79f0\u5143\u5206\u5e03\u4e0b\u4f18\u4e8e\u4f20\u7edf\u6c60\u5316\u548c\u5747\u5300\u5b50\u91c7\u6837\uff0c\u80fd\u6269\u5c55\u5230\u975e\u9ad8\u65af\u548c\u591a\u6a21\u6001\u771f\u5b9e\u573a\u666f\uff0c\u5e76\u5728\u96f6\u6837\u672c\u533b\u5b66\u5f02\u5e38\u68c0\u6d4b\u4e2d\u53d6\u5f97\u663e\u8457\u6539\u8fdb", "conclusion": "\u5339\u914d\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u5f02\u6784\u6570\u636e\u96c6\u7684\u5206\u5e03\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u63d0\u5347\u96f6\u6837\u672c\u6cdb\u5316\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u533b\u5b66\u5f02\u5e38\u68c0\u6d4b\u7b49\u6781\u7aef\u5f02\u6784\u573a\u666f\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c"}}
{"id": "2602.08562", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2602.08562", "abs": "https://arxiv.org/abs/2602.08562", "authors": ["R. R. Panchal", "Divya G. Sanjava", "A. H. Hasmani"], "title": "Dynamical System Analysis of FLRW Model in f(R,L,T) Theory", "comment": null, "summary": "Modified gravity theories have been extensively studied recently as viable substitutes for general relativity to deal with cosmological issues like dark energy and late-time cosmic acceleration. In the present work, we investigate the dynamical behavior of the $f(R,L,T)$ gravity model with a scalar field utilizing exponential potential, where $R$ represents the Ricci scalar, $L$ is the Lagrangian density and $T$ is the trace of the energy-momentum tensor. We concentrate on a specific type of modified gravity characterized by $f(R,L,T) =R+\u03b1L+\u03b2T$, where $\u03b1$ and $\u03b2$ are positive constants. We study the dynamical behavior and late-time evolution of a cosmological model using a thorough phase-space analysis. We assess important cosmological parameters at the critical places, such as the density parameters corresponding to various cosmic components, the deceleration parameter, and the effective equation of state parameter. The nature of the cosmic phases such as matter-dominated, radiation-dominated, and accelerated expansion eras, described using these quantities.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86f(R,L,T)\u5f15\u529b\u7406\u8bba\u5728\u6807\u91cf\u573a\u6307\u6570\u52bf\u4e0b\u7684\u52a8\u529b\u5b66\u884c\u4e3a\uff0c\u901a\u8fc7\u76f8\u7a7a\u95f4\u5206\u6790\u63a2\u7d22\u5b87\u5b99\u6f14\u5316\uff0c\u5305\u62ec\u7269\u8d28\u4e3b\u5bfc\u3001\u8f90\u5c04\u4e3b\u5bfc\u548c\u52a0\u901f\u81a8\u80c0\u9636\u6bb5\u3002", "motivation": "\u4fee\u6b63\u5f15\u529b\u7406\u8bba\u4f5c\u4e3a\u5e7f\u4e49\u76f8\u5bf9\u8bba\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u88ab\u5e7f\u6cdb\u7814\u7a76\u4ee5\u89e3\u51b3\u6697\u80fd\u91cf\u548c\u665a\u671f\u5b87\u5b99\u52a0\u901f\u81a8\u80c0\u7b49\u5b87\u5b99\u5b66\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22f(R,L,T)\u5f15\u529b\u6a21\u578b\u5728\u6807\u91cf\u573a\u4f5c\u7528\u4e0b\u7684\u52a8\u529b\u5b66\u884c\u4e3a\uff0c\u4ee5\u7406\u89e3\u5b87\u5b99\u6f14\u5316\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528f(R,L,T)=R+\u03b1L+\u03b2T\u7684\u7279\u5b9a\u5f62\u5f0f\u5f15\u529b\u6a21\u578b\uff0c\u7ed3\u5408\u6307\u6570\u52bf\u6807\u91cf\u573a\uff0c\u901a\u8fc7\u76f8\u7a7a\u95f4\u5206\u6790\u65b9\u6cd5\u7814\u7a76\u5b87\u5b99\u6a21\u578b\u7684\u52a8\u529b\u5b66\u884c\u4e3a\u548c\u665a\u671f\u6f14\u5316\u3002\u8bc4\u4f30\u4e34\u754c\u70b9\u5904\u7684\u91cd\u8981\u5b87\u5b99\u5b66\u53c2\u6570\uff0c\u5305\u62ec\u5404\u5b87\u5b99\u6210\u5206\u7684\u5bc6\u5ea6\u53c2\u6570\u3001\u51cf\u901f\u53c2\u6570\u548c\u6709\u6548\u72b6\u6001\u65b9\u7a0b\u53c2\u6570\u3002", "result": "\u901a\u8fc7\u76f8\u7a7a\u95f4\u5206\u6790\uff0c\u63cf\u8ff0\u4e86\u5b87\u5b99\u6f14\u5316\u4e2d\u7684\u4e0d\u540c\u9636\u6bb5\uff1a\u7269\u8d28\u4e3b\u5bfc\u65f6\u671f\u3001\u8f90\u5c04\u4e3b\u5bfc\u65f6\u671f\u548c\u52a0\u901f\u81a8\u80c0\u65f6\u671f\u3002\u8fd9\u4e9b\u9636\u6bb5\u901a\u8fc7\u5bc6\u5ea6\u53c2\u6570\u3001\u51cf\u901f\u53c2\u6570\u548c\u72b6\u6001\u65b9\u7a0b\u53c2\u6570\u7b49\u5173\u952e\u5b87\u5b99\u5b66\u91cf\u8fdb\u884c\u8868\u5f81\u3002", "conclusion": "f(R,L,T)\u5f15\u529b\u6a21\u578b\u7ed3\u5408\u6807\u91cf\u573a\u6307\u6570\u52bf\u80fd\u591f\u63cf\u8ff0\u5b87\u5b99\u6f14\u5316\u7684\u4e0d\u540c\u9636\u6bb5\uff0c\u5305\u62ec\u665a\u671f\u52a0\u901f\u81a8\u80c0\uff0c\u4e3a\u89e3\u51b3\u5b87\u5b99\u5b66\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u3002\u76f8\u7a7a\u95f4\u5206\u6790\u4e3a\u7406\u89e3\u8be5\u6a21\u578b\u7684\u52a8\u529b\u5b66\u884c\u4e3a\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2602.07636", "categories": ["quant-ph", "math-ph", "physics.atom-ph"], "pdf": "https://arxiv.org/pdf/2602.07636", "abs": "https://arxiv.org/abs/2602.07636", "authors": ["Sunghyun Kim"], "title": "Hidden Kinematics and Dual Quantum References in Magnetic Resonance", "comment": "8 pages, 2 figures", "summary": "Spin resonance phenomena are conventionally described using transition probabilities formulated in a rotating frame, whose physical meaning implicitly depends on the choice of quantum reference standard. In this Colloquium, we show that a spin in a rotating magnetic field constitutes a configuration involving two quantum descriptions that share a common quantization operator but differ in their kinematic and dynamical roles. The transition probability therefore emerges as a relational quantity between quantum reference standards rather than an intrinsic property of a single evolving spin state. By incorporating the kinematic motion of the spin vector together with the dynamical evolution, this framework restores consistent energy accounting and reveals the dual-reference structure underlying spin dynamics in rotating magnetic fields.", "AI": {"tldr": "\u81ea\u65cb\u5171\u632f\u73b0\u8c61\u901a\u5e38\u7528\u65cb\u8f6c\u5750\u6807\u7cfb\u4e2d\u7684\u8dc3\u8fc1\u6982\u7387\u63cf\u8ff0\uff0c\u5176\u7269\u7406\u610f\u4e49\u9690\u542b\u4f9d\u8d56\u4e8e\u91cf\u5b50\u53c2\u8003\u6807\u51c6\u7684\u9009\u62e9\u3002\u672c\u6587\u5c55\u793a\u4e86\u65cb\u8f6c\u78c1\u573a\u4e2d\u7684\u81ea\u65cb\u6784\u6210\u6d89\u53ca\u4e24\u79cd\u91cf\u5b50\u63cf\u8ff0\u7684\u914d\u7f6e\uff0c\u5b83\u4eec\u5171\u4eab\u5171\u540c\u7684\u91cf\u5b50\u5316\u7b97\u7b26\u4f46\u5728\u8fd0\u52a8\u5b66\u548c\u52a8\u529b\u5b66\u89d2\u8272\u4e0a\u4e0d\u540c\u3002", "motivation": "\u4f20\u7edf\u81ea\u65cb\u5171\u632f\u7406\u8bba\u4f7f\u7528\u65cb\u8f6c\u5750\u6807\u7cfb\u4e2d\u7684\u8dc3\u8fc1\u6982\u7387\u63cf\u8ff0\uff0c\u8fd9\u79cd\u63cf\u8ff0\u9690\u542b\u4f9d\u8d56\u4e8e\u91cf\u5b50\u53c2\u8003\u6807\u51c6\u7684\u9009\u62e9\uff0c\u5bfc\u81f4\u7269\u7406\u610f\u4e49\u4e0d\u660e\u786e\u3002\u9700\u8981\u5efa\u7acb\u66f4\u6e05\u6670\u7684\u6846\u67b6\u6765\u7406\u89e3\u81ea\u65cb\u5728\u65cb\u8f6c\u78c1\u573a\u4e2d\u7684\u52a8\u529b\u5b66\u672c\u8d28\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u4e24\u79cd\u91cf\u5b50\u63cf\u8ff0\u7684\u65b0\u6846\u67b6\uff1a\u5b83\u4eec\u5171\u4eab\u76f8\u540c\u7684\u91cf\u5b50\u5316\u7b97\u7b26\uff0c\u4f46\u5728\u8fd0\u52a8\u5b66\uff08\u81ea\u65cb\u77e2\u91cf\u8fd0\u52a8\uff09\u548c\u52a8\u529b\u5b66\uff08\u6f14\u5316\uff09\u89d2\u8272\u4e0a\u4e0d\u540c\u3002\u5c06\u81ea\u65cb\u77e2\u91cf\u7684\u8fd0\u52a8\u5b66\u8fd0\u52a8\u4e0e\u52a8\u529b\u5b66\u6f14\u5316\u76f8\u7ed3\u5408\uff0c\u6062\u590d\u4e00\u81f4\u7684\u80fd\u91cf\u8ba1\u7b97\u3002", "result": "\u8dc3\u8fc1\u6982\u7387\u88ab\u63ed\u793a\u4e3a\u91cf\u5b50\u53c2\u8003\u6807\u51c6\u4e4b\u95f4\u7684\u76f8\u5bf9\u91cf\uff0c\u800c\u4e0d\u662f\u5355\u4e2a\u6f14\u5316\u81ea\u65cb\u6001\u7684\u5185\u5728\u5c5e\u6027\u3002\u8be5\u6846\u67b6\u6062\u590d\u4e86\u65cb\u8f6c\u78c1\u573a\u4e2d\u81ea\u65cb\u52a8\u529b\u5b66\u7684\u4e00\u81f4\u80fd\u91cf\u8ba1\u7b97\uff0c\u63ed\u793a\u4e86\u5176\u53cc\u53c2\u8003\u7ed3\u6784\u3002", "conclusion": "\u65cb\u8f6c\u78c1\u573a\u4e2d\u7684\u81ea\u65cb\u52a8\u529b\u5b66\u672c\u8d28\u4e0a\u5177\u6709\u53cc\u53c2\u8003\u7ed3\u6784\uff0c\u8dc3\u8fc1\u6982\u7387\u662f\u91cf\u5b50\u53c2\u8003\u6807\u51c6\u4e4b\u95f4\u7684\u76f8\u5bf9\u91cf\u3002\u8fd9\u4e00\u6846\u67b6\u4e3a\u7406\u89e3\u81ea\u65cb\u5171\u632f\u73b0\u8c61\u63d0\u4f9b\u4e86\u66f4\u6e05\u6670\u7684\u57fa\u7840\uff0c\u5f3a\u8c03\u4e86\u91cf\u5b50\u53c2\u8003\u6807\u51c6\u5728\u5b9a\u4e49\u7269\u7406\u91cf\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2602.07156", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07156", "abs": "https://arxiv.org/abs/2602.07156", "authors": ["Asher Trockman", "J. Zico Kolter"], "title": "Mimetic Initialization of MLPs", "comment": null, "summary": "Mimetic initialization uses pretrained models as case studies of good initialization, using observations of structures in trained weights to inspire new, simple initialization techniques. So far, it has been applied only to spatial mixing layers, such convolutional, self-attention, and state space layers. In this work, we present the first attempt to apply the method to channel mixing layers, namely multilayer perceptrons (MLPs). Our extremely simple technique for MLPs -- to give the first layer a nonzero mean -- speeds up training on small-scale vision tasks like CIFAR-10 and ImageNet-1k. Though its effect is much smaller than spatial mixing initializations, it can be used in conjunction with them for an additional positive effect.", "AI": {"tldr": "\u9996\u6b21\u5c06\u6a21\u4eff\u521d\u59cb\u5316\u5e94\u7528\u4e8e\u901a\u9053\u6df7\u5408\u5c42\uff08MLP\uff09\uff0c\u901a\u8fc7\u7ed9\u7b2c\u4e00\u5c42\u8d4b\u4e88\u975e\u96f6\u5747\u503c\u6765\u52a0\u901f\u5c0f\u89c4\u6a21\u89c6\u89c9\u4efb\u52a1\u8bad\u7ec3", "motivation": "\u6a21\u4eff\u521d\u59cb\u5316\u65b9\u6cd5\u4e4b\u524d\u53ea\u5e94\u7528\u4e8e\u7a7a\u95f4\u6df7\u5408\u5c42\uff08\u5377\u79ef\u3001\u81ea\u6ce8\u610f\u529b\u3001\u72b6\u6001\u7a7a\u95f4\u5c42\uff09\uff0c\u672c\u6587\u5e0c\u671b\u5c06\u5176\u6269\u5c55\u5230\u901a\u9053\u6df7\u5408\u5c42\uff08MLP\uff09", "method": "\u63d0\u51fa\u6781\u7b80\u6280\u672f\uff1a\u7ed9MLP\u7684\u7b2c\u4e00\u5c42\u8d4b\u4e88\u975e\u96f6\u5747\u503c\uff0c\u53ef\u4ee5\u4e0e\u7a7a\u95f4\u6df7\u5408\u521d\u59cb\u5316\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528", "result": "\u5728CIFAR-10\u548cImageNet-1k\u7b49\u5c0f\u89c4\u6a21\u89c6\u89c9\u4efb\u52a1\u4e0a\u52a0\u901f\u8bad\u7ec3\uff0c\u6548\u679c\u867d\u5c0f\u4e8e\u7a7a\u95f4\u6df7\u5408\u521d\u59cb\u5316\uff0c\u4f46\u80fd\u63d0\u4f9b\u989d\u5916\u6b63\u5411\u6548\u679c", "conclusion": "\u6210\u529f\u5c06\u6a21\u4eff\u521d\u59cb\u5316\u6269\u5c55\u5230\u901a\u9053\u6df7\u5408\u5c42\uff0c\u8bc1\u660e\u4e86\u7b80\u5355MLP\u521d\u59cb\u5316\u6280\u672f\u5bf9\u8bad\u7ec3\u52a0\u901f\u7684\u6709\u6548\u6027"}}
{"id": "2602.08720", "categories": ["gr-qc", "astro-ph.EP", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2602.08720", "abs": "https://arxiv.org/abs/2602.08720", "authors": ["Miriam Falletta", "Gabriel Rodr\u00edguez-Moris", "Sergei A. Klioner"], "title": "Post-Newtonian accelerations of a Mercury orbiter", "comment": "Accepted for publication in Physical Review D", "summary": "We investigate the relativistic modeling of spacecraft motion in Mercury's post-Newtonian local coordinates. This investigation is motivated by the fact that Mercury's post-Newtonian gravitational field (as well as that of any other planet) admits an expansion in terms of multipole moments, which are most appropriately defined in the local reference system. The equations of motion in the Mercury-centric local frame include relativistic local perturbations, given by the Schwarzschild term, Lense-Thirring precession, and the acceleration due to the quadrupole moment, and relativistic third-body perturbations, which are the gravito-electric and gravito-magnetic accelerations, along with a coupling term between Mercury and other solar system bodies. The relativistic third-body perturbations are usually neglected in all practical applications. In this study, we analyze the magnitude of the post-Newtonian terms of the equations of motion formulated in the Mercury-centric frame, evaluating them along the trajectories of the two BepiColombo spacecrafts. Based on this analysis, we provide a practical approach for constructing a high-accuracy relativistic orbital model suitable for a Mercury orbiter.", "AI": {"tldr": "\u7814\u7a76\u6c34\u661f\u8f68\u9053\u5668\u5728\u5c40\u90e8\u5750\u6807\u7cfb\u4e2d\u7684\u76f8\u5bf9\u8bba\u8fd0\u52a8\u6a21\u578b\uff0c\u5206\u6790BepiColombo\u4efb\u52a1\u4e2d\u901a\u5e38\u88ab\u5ffd\u7565\u7684\u76f8\u5bf9\u8bba\u7b2c\u4e09\u4f53\u6270\u52a8\u9879\u7684\u91cd\u8981\u6027", "motivation": "\u6c34\u661f\u7684\u540e\u725b\u987f\u5f15\u529b\u573a\uff08\u4ee5\u53ca\u5176\u4ed6\u884c\u661f\uff09\u53ef\u4ee5\u7528\u591a\u6781\u77e9\u5c55\u5f00\uff0c\u8fd9\u4e9b\u591a\u6781\u77e9\u6700\u9002\u5f53\u5730\u5b9a\u4e49\u5728\u5c40\u90e8\u53c2\u8003\u7cfb\u4e2d\u3002\u76ee\u524d\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u901a\u5e38\u5ffd\u7565\u76f8\u5bf9\u8bba\u7b2c\u4e09\u4f53\u6270\u52a8\uff0c\u9700\u8981\u8bc4\u4f30\u8fd9\u4e9b\u9879\u5bf9\u6c34\u661f\u8f68\u9053\u5668\u7cbe\u5ea6\u7684\u5f71\u54cd", "method": "\u5728\u6c34\u661f\u4e2d\u5fc3\u5c40\u90e8\u5750\u6807\u7cfb\u4e2d\u5efa\u7acb\u8fd0\u52a8\u65b9\u7a0b\uff0c\u5305\u62ec\u76f8\u5bf9\u8bba\u5c40\u90e8\u6270\u52a8\uff08\u53f2\u74e6\u897f\u9879\u3001Lense-Thirring\u8fdb\u52a8\u3001\u56db\u6781\u77e9\u52a0\u901f\u5ea6\uff09\u548c\u76f8\u5bf9\u8bba\u7b2c\u4e09\u4f53\u6270\u52a8\uff08\u5f15\u529b\u7535\u548c\u5f15\u529b\u78c1\u52a0\u901f\u5ea6\uff0c\u4ee5\u53ca\u6c34\u661f\u4e0e\u5176\u4ed6\u592a\u9633\u7cfb\u5929\u4f53\u7684\u8026\u5408\u9879\uff09\u3002\u6cbfBepiColombo\u4e24\u4e2a\u822a\u5929\u5668\u7684\u8f68\u8ff9\u8bc4\u4f30\u8fd9\u4e9b\u540e\u725b\u987f\u9879\u7684\u5927\u5c0f", "result": "\u901a\u8fc7\u5206\u6790BepiColombo\u822a\u5929\u5668\u8f68\u8ff9\u4e0a\u7684\u540e\u725b\u987f\u9879\u5927\u5c0f\uff0c\u8bc4\u4f30\u4e86\u901a\u5e38\u88ab\u5ffd\u7565\u7684\u76f8\u5bf9\u8bba\u7b2c\u4e09\u4f53\u6270\u52a8\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u6784\u5efa\u9ad8\u7cbe\u5ea6\u6c34\u661f\u8f68\u9053\u5668\u76f8\u5bf9\u8bba\u8f68\u9053\u6a21\u578b\u63d0\u4f9b\u4f9d\u636e", "conclusion": "\u4e3a\u6784\u5efa\u9002\u5408\u6c34\u661f\u8f68\u9053\u5668\u7684\u9ad8\u7cbe\u5ea6\u76f8\u5bf9\u8bba\u8f68\u9053\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\uff0c\u660e\u786e\u4e86\u9700\u8981\u8003\u8651\u7684\u76f8\u5bf9\u8bba\u9879\uff0c\u7279\u522b\u662f\u8bc4\u4f30\u4e86\u7b2c\u4e09\u4f53\u6270\u52a8\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027"}}
{"id": "2602.07700", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.07700", "abs": "https://arxiv.org/abs/2602.07700", "authors": ["Santhosh Surendra", "Akos Hoffmann", "Michael K\u00f6hl"], "title": "Two-phase driving of a linear radio-frequency ion trap", "comment": null, "summary": "A linear radio-frequency Paul trap is traditionally driven with one diagonal pair of electrodes grounded and the other connected to a high-voltage radio-frequency source. This method simplifies impedance matching of the voltage source to the trap. However, for several architectures it leads to increasing the axial micromotion amplitude, for example, when the capacitance between radio-frequency and end-cap electrodes is not negligible. Here, we present a technique to generate two high-voltage radio-frequency signals \\SI{180}{\\degree} out of phase to drive a linear Paul trap with opposite voltages between neighbouring electrodes. Using this, we have successfully trapped and cooled a chain of Ytterbium ions in a linear radio-frequency Paul trap.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5728180\u5ea6\u53cd\u76f8\u4e0b\u9a71\u52a8\u7ebf\u6027\u5c04\u9891\u4fdd\u7f57\u9631\u7684\u6280\u672f\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u9a71\u52a8\u65b9\u6cd5\u5bfc\u81f4\u7684\u8f74\u5411\u5fae\u8fd0\u52a8\u95ee\u9898", "motivation": "\u4f20\u7edf\u7ebf\u6027\u5c04\u9891\u4fdd\u7f57\u9631\u9a71\u52a8\u65b9\u6cd5\uff08\u4e00\u5bf9\u7535\u6781\u63a5\u5730\uff0c\u53e6\u4e00\u5bf9\u63a5\u9ad8\u538b\u5c04\u9891\u6e90\uff09\u867d\u7136\u7b80\u5316\u4e86\u963b\u6297\u5339\u914d\uff0c\u4f46\u5728\u67d0\u4e9b\u67b6\u6784\u4e2d\u4f1a\u5bfc\u81f4\u8f74\u5411\u5fae\u8fd0\u52a8\u5e45\u5ea6\u589e\u52a0\uff0c\u7279\u522b\u662f\u5f53\u5c04\u9891\u7535\u6781\u4e0e\u7aef\u76d6\u7535\u6781\u4e4b\u95f4\u7684\u7535\u5bb9\u4e0d\u53ef\u5ffd\u7565\u65f6", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u751f\u6210\u4e24\u4e2a180\u5ea6\u53cd\u76f8\u9ad8\u538b\u5c04\u9891\u4fe1\u53f7\u7684\u6280\u672f\uff0c\u7528\u4e8e\u9a71\u52a8\u7ebf\u6027\u4fdd\u7f57\u9631\uff0c\u4f7f\u76f8\u90bb\u7535\u6781\u4e4b\u95f4\u5177\u6709\u76f8\u53cd\u7684\u7535\u538b", "result": "\u6210\u529f\u5728\u91c7\u7528\u65b0\u9a71\u52a8\u6280\u672f\u7684\u7ebf\u6027\u5c04\u9891\u4fdd\u7f57\u9631\u4e2d\u56da\u7981\u5e76\u51b7\u5374\u4e86\u9571\u79bb\u5b50\u94fe", "conclusion": "\u63d0\u51fa\u7684180\u5ea6\u53cd\u76f8\u9a71\u52a8\u6280\u672f\u80fd\u591f\u6709\u6548\u51cf\u5c11\u4f20\u7edf\u9a71\u52a8\u65b9\u6cd5\u5bfc\u81f4\u7684\u8f74\u5411\u5fae\u8fd0\u52a8\u95ee\u9898\uff0c\u4e3a\u79bb\u5b50\u9631\u5b9e\u9a8c\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u6848"}}
{"id": "2602.07173", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.07173", "abs": "https://arxiv.org/abs/2602.07173", "authors": ["Tong Jian", "Tianyu Dai", "Tao Yu"], "title": "Learning Nonlinear Systems In-Context: From Synthetic Data to Real-World Motor Control", "comment": "Accepted to be presented in IEEE ICASSP 2026", "summary": "LLMs have shown strong in-context learning (ICL) abilities, but have not yet been extended to signal processing systems. Inspired by their design, we have proposed for the first time ICL using transformer models applicable to motor feedforward control, a critical task where classical PI and physics-based methods struggle with nonlinearities and complex load conditions. We propose a transformer based model architecture that separates signal representation from system behavior, enabling both few-shot finetuning and one-shot ICL. Pretrained on a large corpus of synthetic linear and nonlinear systems, the model learns to generalize to unseen system dynamics of real-world motors only with a handful of examples. In experiments, our approach generalizes across multiple motor load configurations, transforms untuned examples into accurate feedforward predictions, and outperforms PI controllers and physics-based feedforward baselines. These results demonstrate that ICL can bridge synthetic pretraining and real-world adaptability, opening new directions for data efficient control of physical systems.", "AI": {"tldr": "\u9996\u6b21\u5c06Transformer\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u5e94\u7528\u4e8e\u7535\u673a\u524d\u9988\u63a7\u5236\uff0c\u901a\u8fc7\u5206\u79bb\u4fe1\u53f7\u8868\u793a\u4e0e\u7cfb\u7edf\u884c\u4e3a\uff0c\u5b9e\u73b0\u5c11\u6837\u672c\u5fae\u8c03\u548c\u5355\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u5728\u771f\u5b9e\u7535\u673a\u4e0a\u8d85\u8d8a\u4f20\u7edfPI\u63a7\u5236\u5668\u548c\u7269\u7406\u6a21\u578b\u65b9\u6cd5\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u4f46\u5c1a\u672a\u6269\u5c55\u5230\u4fe1\u53f7\u5904\u7406\u7cfb\u7edf\u3002\u4f20\u7edfPI\u63a7\u5236\u5668\u548c\u57fa\u4e8e\u7269\u7406\u7684\u65b9\u6cd5\u5728\u5904\u7406\u975e\u7ebf\u6027\u7279\u6027\u548c\u590d\u6742\u8d1f\u8f7d\u6761\u4ef6\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u52a8\u6001\u53d8\u5316\u7684\u6570\u636e\u9ad8\u6548\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u67b6\u6784\uff0c\u5206\u79bb\u4fe1\u53f7\u8868\u793a\u4e0e\u7cfb\u7edf\u884c\u4e3a\uff0c\u652f\u6301\u5c11\u6837\u672c\u5fae\u8c03\u548c\u5355\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60\u3002\u5728\u5927\u89c4\u6a21\u5408\u6210\u7ebf\u6027\u4e0e\u975e\u7ebf\u6027\u7cfb\u7edf\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4ec5\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\u6cdb\u5316\u5230\u771f\u5b9e\u7535\u673a\u7684\u672a\u77e5\u7cfb\u7edf\u52a8\u6001\u3002", "result": "\u6a21\u578b\u5728\u591a\u4e2a\u7535\u673a\u8d1f\u8f7d\u914d\u7f6e\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u5c06\u672a\u8c03\u4f18\u7684\u793a\u4f8b\u8f6c\u5316\u4e3a\u51c6\u786e\u7684\u524d\u9988\u9884\u6d4b\uff0c\u6027\u80fd\u8d85\u8d8aPI\u63a7\u5236\u5668\u548c\u57fa\u4e8e\u7269\u7406\u7684\u524d\u9988\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u591f\u6865\u63a5\u5408\u6210\u9884\u8bad\u7ec3\u4e0e\u771f\u5b9e\u4e16\u754c\u9002\u5e94\u6027\uff0c\u4e3a\u7269\u7406\u7cfb\u7edf\u7684\u6570\u636e\u9ad8\u6548\u63a7\u5236\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5c55\u793a\u4e86Transformer\u6a21\u578b\u5728\u4fe1\u53f7\u5904\u7406\u548c\u63a7\u5236\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.08833", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2602.08833", "abs": "https://arxiv.org/abs/2602.08833", "authors": ["Andrea Cipriani", "Alessandro Nagar", "Francesco Fucito", "Jos\u00e9 Francisco Morales"], "title": "From the confluent Heun equation to a new factorized and resummed gravitational waveform for circularized, nonspinning, compact binaries", "comment": "53 pages, 5 figures. Submitted to Phys. Rev. D", "summary": "We introduce a new factorized and resummed waveform for circularized, nonspinning, compact binaries that leverages on the solution of the Teukolsky equation once mapped into a confluent Heun equation. The structure of the solution allows one to identify new resummed factors that completely absorb all test-mass logarithms and transcendental numbers via exponentials and $\u0393$-functions at any post-Newtonian (PN) order. The corresponding residual relativistic and phase corrections are thus polynomial with rational coefficients, that are in fact PN-truncated hypergeometric functions. Our approach complements the recent proposal of Ivanov et al. [Phys. Rev. Lett. 135 (2025) 14, 141401], notably recovering the corresponding renormalization group scaling of multipole moments from first principles and fixing the scaling constant. In the test mass limit, our approach (pushed up to 10PN) yields waveforms and fluxes that are globally more accurate than those obtained using the standard factorized approach of Damour et al. [Phys. Rev. D 79 (2009), 064004]. The method generalizes straightforwardly to comparable mass binaries implementing the new concept of universal anomalous dimension of multipole moments and might be eventually useful to improve current state of the art effective-one-body waveform models for coalescing binaries.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eTeukolsky\u65b9\u7a0b\u6620\u5c04\u5230\u5408\u6d41Heun\u65b9\u7a0b\u7684\u65b0\u56e0\u5b50\u5316\u91cd\u6c42\u548c\u6ce2\u5f62\uff0c\u80fd\u5438\u6536\u6240\u6709\u6d4b\u8bd5\u8d28\u91cf\u5bf9\u6570\u9879\u548c\u8d85\u8d8a\u6570\uff0c\u5728\u6d4b\u8bd5\u8d28\u91cf\u6781\u9650\u4e0b\u6bd4\u4f20\u7edf\u56e0\u5b50\u5316\u65b9\u6cd5\u66f4\u7cbe\u786e\u3002", "motivation": "\u6539\u8fdb\u5f53\u524d\u6709\u6548\u5355\u4f53\u6ce2\u5f62\u6a21\u578b\uff0c\u901a\u8fc7\u65b0\u7684\u56e0\u5b50\u5316\u91cd\u6c42\u548c\u65b9\u6cd5\u63d0\u9ad8\u6ce2\u5f62\u548c\u901a\u91cf\u8ba1\u7b97\u7684\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5438\u6536\u6d4b\u8bd5\u8d28\u91cf\u5bf9\u6570\u9879\u548c\u8d85\u8d8a\u6570\u3002", "method": "\u5c06Teukolsky\u65b9\u7a0b\u6620\u5c04\u5230\u5408\u6d41Heun\u65b9\u7a0b\uff0c\u5229\u7528\u89e3\u7684\u7ed3\u6784\u8bc6\u522b\u65b0\u7684\u91cd\u6c42\u548c\u56e0\u5b50\uff0c\u901a\u8fc7\u6307\u6570\u548c\u0393\u51fd\u6570\u5438\u6536\u6240\u6709\u6d4b\u8bd5\u8d28\u91cf\u5bf9\u6570\u9879\u548c\u8d85\u8d8a\u6570\uff0c\u5269\u4f59\u76f8\u5bf9\u8bba\u548c\u76f8\u4f4d\u4fee\u6b63\u4e3a\u6709\u7406\u7cfb\u6570\u591a\u9879\u5f0f\u3002", "result": "\u5728\u6d4b\u8bd5\u8d28\u91cf\u6781\u9650\u4e0b\uff08\u8ba1\u7b97\u81f310PN\uff09\uff0c\u6ce2\u5f62\u548c\u901a\u91cf\u6bd4Damour\u7b49\u4eba\u7684\u6807\u51c6\u56e0\u5b50\u5316\u65b9\u6cd5\u66f4\u7cbe\u786e\uff1b\u4ece\u7b2c\u4e00\u6027\u539f\u7406\u6062\u590d\u4e86\u591a\u6781\u77e9\u7684\u91cd\u6574\u5316\u7fa4\u6807\u5ea6\u5e76\u786e\u5b9a\u4e86\u6807\u5ea6\u5e38\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u63a8\u5e7f\u5230\u53ef\u6bd4\u8d28\u91cf\u53cc\u661f\u7cfb\u7edf\uff0c\u5b9e\u73b0\u591a\u6781\u77e9\u7684\u666e\u9002\u53cd\u5e38\u7ef4\u5ea6\u6982\u5ff5\uff0c\u6709\u671b\u6539\u8fdb\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6709\u6548\u5355\u4f53\u6ce2\u5f62\u6a21\u578b\u3002"}}
{"id": "2602.07788", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.07788", "abs": "https://arxiv.org/abs/2602.07788", "authors": ["Jifeng Sun", "Shumin Yang", "Teng Zhao", "Qingqian Kang", "Liyun Hu"], "title": "Quantum Steering and Entanglement in a Tritter: Hierarchy under Loss", "comment": null, "summary": "Multipartite entangled states of continuous variables are fundamental resources for scalable quantum information processing. We study the correlation hierarchy in a tripartite state engineered by mixing a two-mode squeezed vacuum with a coherent state on a tritter, a key linear optical element for multimode state generation. Using the covariance matrix formalism, we comprehensively analyze the entanglement and Einstein-Podolsky-Rosen (EPR) steering among the output modes. The strength of both correlations is governed solely by the squeezing parameter and is independent of the coherent amplitude. We further examine the impact of inevitable optical losses in various channel configurations. The results show that while losses degrade correlations, EPR steering remains monogamous and exhibits stricter resilience thresholds than entanglement. Our analysis, supported by parameter extension techniques, confirms that the steering condition is more stringent than the inseparability criterion, clearly demonstrating that steering forms a strict subset of entanglement. These results elucidate the correlation structure in a readily generated multimode state and offer practical insights for developing asymmetric quantum protocols, such as one-sided device-independent tasks, where EPR steering serves as a critical resource.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e09\u6a21\u8fde\u7eed\u53d8\u91cf\u7ea0\u7f20\u6001\u4e2d\u7684\u5173\u8054\u5c42\u6b21\uff0c\u5206\u6790\u4e86\u901a\u8fc7\u7ebf\u6027\u5149\u5b66\u5143\u4ef6\uff08\u4e09\u5206\u675f\u5668\uff09\u6df7\u5408\u53cc\u6a21\u538b\u7f29\u771f\u7a7a\u6001\u4e0e\u76f8\u5e72\u6001\u751f\u6210\u7684\u4e09\u6a21\u6001\uff0c\u53d1\u73b0\u7ea0\u7f20\u548cEPR\u5bfc\u5f15\u5f3a\u5ea6\u4ec5\u7531\u538b\u7f29\u53c2\u6570\u51b3\u5b9a\uff0c\u4e14\u5bfc\u5f15\u6bd4\u7ea0\u7f20\u5177\u6709\u66f4\u4e25\u683c\u7684\u9608\u503c\u6761\u4ef6\u3002", "motivation": "\u7814\u7a76\u591a\u6a21\u8fde\u7eed\u53d8\u91cf\u7ea0\u7f20\u6001\u4e2d\u7684\u5173\u8054\u5c42\u6b21\uff0c\u7279\u522b\u662fEPR\u5bfc\u5f15\u4e0e\u7ea0\u7f20\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4e3a\u5f00\u53d1\u975e\u5bf9\u79f0\u91cf\u5b50\u534f\u8bae\uff08\u5982\u5355\u8fb9\u8bbe\u5907\u65e0\u5173\u4efb\u52a1\uff09\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u4f7f\u7528\u534f\u65b9\u5dee\u77e9\u9635\u5f62\u5f0f\u4f53\u7cfb\uff0c\u5206\u6790\u901a\u8fc7\u4e09\u5206\u675f\u5668\u6df7\u5408\u53cc\u6a21\u538b\u7f29\u771f\u7a7a\u6001\u4e0e\u76f8\u5e72\u6001\u751f\u6210\u7684\u4e09\u6a21\u6001\uff0c\u7814\u7a76\u4e0d\u540c\u4fe1\u9053\u914d\u7f6e\u4e0b\u7684\u5149\u5b66\u635f\u8017\u5f71\u54cd\uff0c\u5e76\u91c7\u7528\u53c2\u6570\u6269\u5c55\u6280\u672f\u9a8c\u8bc1\u7ed3\u679c\u3002", "result": "\u7ea0\u7f20\u548cEPR\u5bfc\u5f15\u5f3a\u5ea6\u4ec5\u7531\u538b\u7f29\u53c2\u6570\u51b3\u5b9a\uff0c\u4e0e\u76f8\u5e72\u632f\u5e45\u65e0\u5173\uff1b\u635f\u8017\u4f1a\u964d\u4f4e\u5173\u8054\u5f3a\u5ea6\uff0c\u4f46EPR\u5bfc\u5f15\u4fdd\u6301\u5355\u914d\u6027\u4e14\u6bd4\u7ea0\u7f20\u5177\u6709\u66f4\u5f3a\u7684\u6297\u635f\u8017\u80fd\u529b\uff1b\u5bfc\u5f15\u6761\u4ef6\u6bd4\u4e0d\u53ef\u5206\u6027\u5224\u636e\u66f4\u4e25\u683c\uff0c\u8bc1\u5b9e\u5bfc\u5f15\u662f\u7ea0\u7f20\u7684\u4e25\u683c\u5b50\u96c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u9610\u660e\u4e86\u6613\u4e8e\u751f\u6210\u7684\u591a\u6a21\u6001\u4e2d\u7684\u5173\u8054\u7ed3\u6784\uff0c\u4e3a\u5f00\u53d1\u975e\u5bf9\u79f0\u91cf\u5b50\u534f\u8bae\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u5176\u4e2dEPR\u5bfc\u5f15\u4f5c\u4e3a\u5355\u8fb9\u8bbe\u5907\u65e0\u5173\u4efb\u52a1\u7b49\u5e94\u7528\u7684\u5173\u952e\u8d44\u6e90\u3002"}}
{"id": "2602.07189", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07189", "abs": "https://arxiv.org/abs/2602.07189", "authors": ["Joohwan Ko", "Tomas Geffner"], "title": "Latent Target Score Matching, with an application to Simulation-Based Inference", "comment": "Machine Learning and the Physical Sciences Workshop, NeurIPS 2025", "summary": "Denoising score matching (DSM) for training diffusion models may suffer from high variance at low noise levels. Target Score Matching (TSM) mitigates this when clean data scores are available, providing a low-variance objective. In many applications clean scores are inaccessible due to the presence of latent variables, leaving only joint signals exposed. We propose Latent Target Score Matching (LTSM), an extension of TSM to leverage joint scores for low-variance supervision of the marginal score. While LTSM is effective at low noise levels, a mixture with DSM ensures robustness across noise scales. Across simulation-based inference tasks, LTSM consistently improves variance, score accuracy, and sample quality.", "AI": {"tldr": "\u63d0\u51faLatent Target Score Matching (LTSM)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u8054\u5408\u5206\u6570\u6765\u76d1\u7763\u8fb9\u9645\u5206\u6570\uff0c\u964d\u4f4e\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u65b9\u5dee\uff0c\u7279\u522b\u662f\u5728\u4f4e\u566a\u58f0\u6c34\u5e73\u4e0b\u3002", "motivation": "\u4f20\u7edf\u7684\u53bb\u566a\u5206\u6570\u5339\u914d(DSM)\u5728\u4f4e\u566a\u58f0\u6c34\u5e73\u4e0b\u53ef\u80fd\u9762\u4e34\u9ad8\u65b9\u5dee\u95ee\u9898\u3002\u867d\u7136\u76ee\u6807\u5206\u6570\u5339\u914d(TSM)\u5728\u53ef\u83b7\u5f97\u5e72\u51c0\u6570\u636e\u5206\u6570\u65f6\u80fd\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u4f46\u5728\u8bb8\u591a\u5e94\u7528\u4e2d\uff0c\u7531\u4e8e\u5b58\u5728\u6f5c\u53d8\u91cf\uff0c\u53ea\u80fd\u83b7\u5f97\u8054\u5408\u4fe1\u53f7\uff0c\u65e0\u6cd5\u76f4\u63a5\u83b7\u5f97\u5e72\u51c0\u5206\u6570\u3002", "method": "\u63d0\u51faLatent Target Score Matching (LTSM)\uff0c\u5c06TSM\u6269\u5c55\u5230\u6f5c\u53d8\u91cf\u573a\u666f\uff0c\u5229\u7528\u8054\u5408\u5206\u6570\u4e3a\u8fb9\u9645\u5206\u6570\u63d0\u4f9b\u4f4e\u65b9\u5dee\u76d1\u7763\u3002\u540c\u65f6\u91c7\u7528LTSM\u4e0eDSM\u7684\u6df7\u5408\u7b56\u7565\uff0c\u786e\u4fdd\u5728\u4e0d\u540c\u566a\u58f0\u5c3a\u5ea6\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u57fa\u4e8e\u6a21\u62df\u7684\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cLTSM\u5728\u65b9\u5dee\u3001\u5206\u6570\u51c6\u786e\u6027\u548c\u6837\u672c\u8d28\u91cf\u65b9\u9762\u5747\u53d6\u5f97\u4e00\u81f4\u6027\u7684\u6539\u8fdb\u3002", "conclusion": "LTSM\u901a\u8fc7\u5229\u7528\u8054\u5408\u5206\u6570\u6709\u6548\u89e3\u51b3\u4e86\u6f5c\u53d8\u91cf\u573a\u666f\u4e0b\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u7684\u4f4e\u65b9\u5dee\u76d1\u7763\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4f4e\u566a\u58f0\u6c34\u5e73\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4e0eDSM\u7684\u6df7\u5408\u4f7f\u7528\u786e\u4fdd\u4e86\u6574\u4f53\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.08876", "categories": ["gr-qc", "hep-th"], "pdf": "https://arxiv.org/pdf/2602.08876", "abs": "https://arxiv.org/abs/2602.08876", "authors": ["Gabriel Luz Almeida", "Yuchen Du", "Zhengwen Liu", "Hongbin Wang"], "title": "Conservative binary dynamics to third post-Minkowskian order beyond General Relativity", "comment": "4 pages + References. 1 ancillary file", "summary": "We present the conservative dynamics of compact binaries to third order in the post-Minkowskian approximation in a theory that extends general relativity by a massless scalar field coupled to the Gauss-Bonnet invariant. We employ the effective field theory approach to construct the effective action of binary systems by integrating out the metric and scalar degrees of freedom that mediate the gravitational interactions between the two bodies. We derive analytical expressions for the scattering impulse and the deflection angle to third order in the post-Minkowskian expansion. Our results are found to be in agreement, in the overlapping regimes, with state-of-the-art calculations in the post-Newtonian/post-Minkowskian theory.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5728\u6269\u5c55\u5e7f\u4e49\u76f8\u5bf9\u8bba\u7684\u7406\u8bba\u4e2d\uff0c\u7814\u7a76\u4e86\u5305\u542b\u8d28\u91cf\u6807\u91cf\u573a\u4e0e\u9ad8\u65af-\u535a\u5185\u4e0d\u53d8\u91cf\u8026\u5408\u7684\u7d27\u51d1\u53cc\u661f\u7cfb\u7edf\u7684\u4fdd\u5b88\u52a8\u529b\u5b66\uff0c\u8ba1\u7b97\u4e86\u5230\u540e\u95f5\u53ef\u592b\u65af\u57fa\u8fd1\u4f3c\u4e09\u9636\u7684\u6563\u5c04\u51b2\u91cf\u548c\u504f\u8f6c\u89d2\u3002", "motivation": "\u7814\u7a76\u6269\u5c55\u5e7f\u4e49\u76f8\u5bf9\u8bba\u7684\u7406\u8bba\u4e2d\u7d27\u51d1\u53cc\u661f\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\uff0c\u7279\u522b\u662f\u5728\u5305\u542b\u8d28\u91cf\u6807\u91cf\u573a\u4e0e\u9ad8\u65af-\u535a\u5185\u4e0d\u53d8\u91cf\u8026\u5408\u7684\u7406\u8bba\u6846\u67b6\u4e0b\uff0c\u7406\u89e3\u9ad8\u9636\u540e\u95f5\u53ef\u592b\u65af\u57fa\u8fd1\u4f3c\u4e0b\u7684\u5f15\u529b\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u91c7\u7528\u6709\u6548\u573a\u8bba\u65b9\u6cd5\uff0c\u901a\u8fc7\u79ef\u5206\u6389\u4ecb\u5bfc\u4e24\u4e2a\u5929\u4f53\u4e4b\u95f4\u5f15\u529b\u76f8\u4e92\u4f5c\u7528\u7684\u5ea6\u89c4\u548c\u6807\u91cf\u81ea\u7531\u5ea6\uff0c\u6784\u5efa\u53cc\u661f\u7cfb\u7edf\u7684\u6709\u6548\u4f5c\u7528\u91cf\uff0c\u63a8\u5bfc\u540e\u95f5\u53ef\u592b\u65af\u57fa\u5c55\u5f00\u5230\u4e09\u9636\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\u3002", "result": "\u5f97\u5230\u4e86\u6563\u5c04\u51b2\u91cf\u548c\u504f\u8f6c\u89d2\u5230\u540e\u95f5\u53ef\u592b\u65af\u57fa\u4e09\u9636\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u7ed3\u679c\u4e0e\u540e\u725b\u987f/\u540e\u95f5\u53ef\u592b\u65af\u57fa\u7406\u8bba\u4e2d\u6700\u5148\u8fdb\u7684\u8ba1\u7b97\u5728\u91cd\u53e0\u533a\u57df\u4e00\u81f4\u3002", "conclusion": "\u6210\u529f\u5728\u6269\u5c55\u5e7f\u4e49\u76f8\u5bf9\u8bba\u7684\u7406\u8bba\u4e2d\u8ba1\u7b97\u4e86\u7d27\u51d1\u53cc\u661f\u7cfb\u7edf\u5230\u540e\u95f5\u53ef\u592b\u65af\u57fa\u4e09\u9636\u7684\u4fdd\u5b88\u52a8\u529b\u5b66\uff0c\u9a8c\u8bc1\u4e86\u4e0e\u73b0\u6709\u7406\u8bba\u7684\u4e00\u81f4\u6027\uff0c\u4e3a\u7406\u89e3\u6807\u91cf-\u5f20\u91cf\u5f15\u529b\u7406\u8bba\u4e2d\u7684\u53cc\u661f\u7cfb\u7edf\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2602.07795", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.07795", "abs": "https://arxiv.org/abs/2602.07795", "authors": ["Ken Chen", "Jia-Hao Lv", "Hao-Long Zhang", "Fan Wu", "Wen Ning", "Zhen-Biao Yang", "Shi-Biao Zheng"], "title": "Geometric criticality in the driven Jaynes-Cummings model", "comment": "6 pages, 2 figures", "summary": "When the photonic mode in the Jaynes-Cummings model is driven by an external classical field, the system can undergo the photon-blockade breakdown phase transition at a critical point. Such a phase transition has been detailedly investigated, but the critical properties of the eigenstates remain largely unexplored so far. We here study the geometric criticality associated with these eigenstates. The amplitude and phase of the drive serve as the control parameter of the governing Hamiltonian. We find the quantum metric and Berry curvature tensors for each eigenstate display divergent behaviors in the critical region. More importantly, the divergence associated with bright eigenstates is much more pronounced than that for the unique dark state. Our theoretical results can be experimentally confirmed in circuit quantum electrodynamics systems, where the driven Jaynes-Cummings model has been realized.", "AI": {"tldr": "\u7814\u7a76\u9a71\u52a8Jaynes-Cummings\u6a21\u578b\u4e2d\u672c\u5f81\u6001\u7684\u51e0\u4f55\u4e34\u754c\u6027\uff0c\u53d1\u73b0\u91cf\u5b50\u5ea6\u91cf\u548cBerry\u66f2\u7387\u5f20\u91cf\u5728\u4e34\u754c\u533a\u57df\u53d1\u6563\uff0c\u4e14\u4eae\u6001\u7684\u53d1\u6563\u6bd4\u6697\u6001\u66f4\u663e\u8457", "motivation": "\u867d\u7136\u5149\u5b50\u963b\u585e\u7834\u574f\u76f8\u53d8\u5df2\u88ab\u8be6\u7ec6\u7814\u7a76\uff0c\u4f46\u672c\u5f81\u6001\u7684\u4e34\u754c\u6027\u8d28\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u8fd9\u4e9b\u672c\u5f81\u6001\u76f8\u5173\u7684\u51e0\u4f55\u4e34\u754c\u6027", "method": "\u4f7f\u7528\u9a71\u52a8Jaynes-Cummings\u6a21\u578b\uff0c\u4ee5\u9a71\u52a8\u573a\u7684\u632f\u5e45\u548c\u76f8\u4f4d\u4f5c\u4e3a\u54c8\u5bc6\u987f\u91cf\u7684\u63a7\u5236\u53c2\u6570\uff0c\u5206\u6790\u6bcf\u4e2a\u672c\u5f81\u6001\u7684\u91cf\u5b50\u5ea6\u91cf\u548cBerry\u66f2\u7387\u5f20\u91cf", "result": "\u53d1\u73b0\u91cf\u5b50\u5ea6\u91cf\u548cBerry\u66f2\u7387\u5f20\u91cf\u5728\u4e34\u754c\u533a\u57df\u5448\u73b0\u53d1\u6563\u884c\u4e3a\uff0c\u4e14\u4eae\u6001\u7684\u53d1\u6563\u6bd4\u552f\u4e00\u7684\u6697\u6001\u66f4\u52a0\u663e\u8457", "conclusion": "\u672c\u5f81\u6001\u7684\u51e0\u4f55\u4e34\u754c\u6027\u5728\u9a71\u52a8Jaynes-Cummings\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7279\u5f81\uff0c\u8fd9\u4e9b\u7406\u8bba\u7ed3\u679c\u53ef\u5728\u7535\u8def\u91cf\u5b50\u7535\u52a8\u529b\u5b66\u7cfb\u7edf\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1"}}
{"id": "2602.08911", "categories": ["gr-qc", "hep-th"], "pdf": "https://arxiv.org/pdf/2602.08911", "abs": "https://arxiv.org/abs/2602.08911", "authors": ["G. Mustafa", "Orhan Donmez", "Dhruba Jyoti Gogoi", "Sushant G. Ghosh", "Ibrar Hussain", "Chengxun Yuan"], "title": "Dynamics, Ringdown, and Accretion-Driven Multiple Quasi-Periodic Oscillations of Kerr-Bertotti-Robinson Black Holes", "comment": "27 pages, 18 figures", "summary": "We study the motion of test particles around Kerr--Bertotti--Robinson (KBR) black hole (BH) and explore how the three defining parameters the mass $M$, rotation parameter $a$, and magnetic parameter $B$ influence their dynamics. We derive analytical expressions for the energy and angular momentum of stable equatorial circular orbits, along with the corresponding radial and latitudinal oscillation frequencies, as functions of $M$, $a$, and $B$. We also examine the key features of the quasi-periodic oscillations of test particles near stable circular orbits, including the precession effects such as periastron precession and the Lense-Thirring effect. Finally, we compare our results with those corresponding to the Kerr BH. We find that particle motion is strongly shaped by the BH parameters. Using a WKB approach, we also study scalar quasinormal modes of a rotating KBR BH in an external magnetic field and show that the magnetic field increases damping, while rotation and angular momentum mainly set the oscillation frequencies. Alternatively, general relativistic modelling of Bondi-Hoyle-Lyttleton (BHL) accretion onto a rapidly rotating KBR BH shows that two distinct physical structures emerge and cyclically transform into one another over time. These processes produce either a strongly oscillating flip-flop shock cone or a nearly stationary toroidal structure, with their formation governed by the black hole spin and magnetic curvature. Power spectral analysis shows that these configurations give rise to low and high-frequency quasi-periodic oscillations, offering a unified explanation for the multiple quasi-periodic oscillations observed in rapidly spinning X--ray binaries.", "AI": {"tldr": "\u7814\u7a76Kerr-Bertotti-Robinson\u9ed1\u6d1e\u5468\u56f4\u6d4b\u8bd5\u7c92\u5b50\u7684\u8fd0\u52a8\uff0c\u5206\u6790\u8d28\u91cfM\u3001\u65cb\u8f6c\u53c2\u6570a\u548c\u78c1\u53c2\u6570B\u5bf9\u7c92\u5b50\u52a8\u529b\u5b66\u7684\u5f71\u54cd\uff0c\u5305\u62ec\u7a33\u5b9a\u8f68\u9053\u3001\u632f\u8361\u9891\u7387\u3001\u51c6\u5468\u671f\u632f\u8361\uff0c\u5e76\u4e0eKerr\u9ed1\u6d1e\u6bd4\u8f83\u3002", "motivation": "\u7814\u7a76\u78c1\u573a\u5982\u4f55\u5f71\u54cd\u65cb\u8f6c\u9ed1\u6d1e\u5468\u56f4\u7684\u7c92\u5b50\u52a8\u529b\u5b66\uff0c\u63a2\u7d22\u51c6\u5468\u671f\u632f\u8361\u7684\u7269\u7406\u673a\u5236\uff0c\u4e3a\u89c2\u6d4b\u5230\u7684X\u5c04\u7ebf\u53cc\u661f\u4e2d\u7684\u591a\u9891\u51c6\u5468\u671f\u632f\u8361\u63d0\u4f9b\u7edf\u4e00\u89e3\u91ca\u3002", "method": "\u63a8\u5bfc\u7a33\u5b9a\u8d64\u9053\u5706\u8f68\u9053\u7684\u80fd\u91cf\u548c\u89d2\u52a8\u91cf\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u8ba1\u7b97\u5f84\u5411\u548c\u7eac\u5ea6\u632f\u8361\u9891\u7387\uff0c\u4f7f\u7528WKB\u65b9\u6cd5\u7814\u7a76\u6807\u91cf\u51c6\u6b63\u89c4\u6a21\uff0c\u5efa\u7acbBHL\u5438\u79ef\u7684\u5e7f\u4e49\u76f8\u5bf9\u8bba\u6a21\u578b\u8fdb\u884c\u6570\u503c\u6a21\u62df\u3002", "result": "\u9ed1\u6d1e\u53c2\u6570\u5f3a\u70c8\u5f71\u54cd\u7c92\u5b50\u8fd0\u52a8\uff0c\u78c1\u573a\u589e\u52a0\u51c6\u6b63\u89c4\u6a21\u7684\u963b\u5c3c\uff0c\u65cb\u8f6c\u548c\u89d2\u52a8\u91cf\u4e3b\u8981\u51b3\u5b9a\u632f\u8361\u9891\u7387\u3002BHL\u5438\u79ef\u4ea7\u751f\u4e24\u79cd\u5468\u671f\u6027\u8f6c\u6362\u7684\u7269\u7406\u7ed3\u6784\uff0c\u4ea7\u751f\u4f4e\u9891\u548c\u9ad8\u9891\u51c6\u5468\u671f\u632f\u8361\u3002", "conclusion": "KBR\u9ed1\u6d1e\u7684\u78c1\u573a\u548c\u65cb\u8f6c\u53c2\u6570\u5171\u540c\u5851\u9020\u7c92\u5b50\u52a8\u529b\u5b66\uff0c\u4ea7\u751f\u7684\u51c6\u5468\u671f\u632f\u8361\u6a21\u5f0f\u4e3a\u89c2\u6d4b\u5230\u7684X\u5c04\u7ebf\u53cc\u661f\u591a\u9891\u632f\u8361\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u89e3\u91ca\u6846\u67b6\u3002"}}
{"id": "2602.07823", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.07823", "abs": "https://arxiv.org/abs/2602.07823", "authors": ["Mengyan Li", "Yanning Jia", "Fenzhuo Guo", "Haifeng Dong", "Sujuan Qin", "Fei Gao"], "title": "Semi-device-independent certification of high-dimensional quantum channels", "comment": null, "summary": "Certifying high-dimensional quantum channels is essential for ensuring the reliability of quantum communication protocols. Existing certification schemes often rely on fully trusted internal devices, which is difficult to achieve in realistic scenarios. Here, we propose a semi-device-independent framework for certifying channel properties directly from observed statistics, assuming only that the system dimension is known. By explicitly incorporating the full set of structural constraints inherent to Choi states, our approach exploits the Choi-Jamio\u0142kowski isomorphism for rigorous certification of quantum channels. The entanglement dimensionality of quantum channels is first certified by introducing a witness and numerically determining its Schmidt-number-dependent bounds. This certification method reproduces known analytical benchmarks and is applied to dephasing and depolarizing noise channels, thereby confirming its validity. To provide a more complete assessment of channel performance, the entanglement fidelity of quantum channels is also certified using a hierarchy of semidefinite programming relaxations based on localizing matrices. Lower bounds on the entanglement fidelity are obtained that are compatible with either the full set of observed statistics or a single witness value.", "AI": {"tldr": "\u63d0\u51fa\u534a\u8bbe\u5907\u65e0\u5173\u6846\u67b6\uff0c\u57fa\u4e8e\u89c2\u6d4b\u7edf\u8ba1\u91cf\u8ba4\u8bc1\u91cf\u5b50\u4fe1\u9053\u7279\u6027\uff0c\u4ec5\u5047\u8bbe\u7cfb\u7edf\u7ef4\u5ea6\u5df2\u77e5\uff0c\u5229\u7528Choi\u6001\u7ed3\u6784\u7ea6\u675f\u548cChoi-Jamio\u0142kowski\u540c\u6784\u8fdb\u884c\u4e25\u683c\u8ba4\u8bc1\u3002", "motivation": "\u73b0\u6709\u91cf\u5b50\u4fe1\u9053\u8ba4\u8bc1\u65b9\u6848\u901a\u5e38\u9700\u8981\u5b8c\u5168\u53ef\u4fe1\u7684\u5185\u90e8\u8bbe\u5907\uff0c\u8fd9\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u96be\u4ee5\u5b9e\u73b0\u3002\u9700\u8981\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u8ba4\u8bc1\u6846\u67b6\uff0c\u80fd\u591f\u5728\u8bbe\u5907\u4fe1\u4efb\u5ea6\u8f83\u4f4e\u7684\u60c5\u51b5\u4e0b\u9a8c\u8bc1\u91cf\u5b50\u4fe1\u9053\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u534a\u8bbe\u5907\u65e0\u5173\u6846\u67b6\uff0c\u4ec5\u5047\u8bbe\u7cfb\u7edf\u7ef4\u5ea6\u5df2\u77e5\uff0c\u5229\u7528Choi\u6001\u7684\u7ed3\u6784\u7ea6\u675f\u548cChoi-Jamio\u0142kowski\u540c\u6784\u3002\u901a\u8fc7\u5f15\u5165\u89c1\u8bc1\u7b97\u5b50\u5e76\u6570\u503c\u786e\u5b9a\u5176Schmidt\u6570\u76f8\u5173\u8fb9\u754c\u6765\u8ba4\u8bc1\u4fe1\u9053\u7ea0\u7f20\u7ef4\u5ea6\u3002\u540c\u65f6\u4f7f\u7528\u57fa\u4e8e\u5c40\u90e8\u5316\u77e9\u9635\u7684\u534a\u5b9a\u89c4\u5212\u677e\u5f1b\u5c42\u6b21\u7ed3\u6784\u6765\u8ba4\u8bc1\u4fe1\u9053\u7ea0\u7f20\u4fdd\u771f\u5ea6\u3002", "result": "\u8be5\u65b9\u6cd5\u6210\u529f\u590d\u73b0\u4e86\u5df2\u77e5\u5206\u6790\u57fa\u51c6\uff0c\u5e76\u5e94\u7528\u4e8e\u53bb\u76f8\u4f4d\u548c\u53bb\u6781\u5316\u566a\u58f0\u4fe1\u9053\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002\u83b7\u5f97\u4e86\u4e0e\u89c2\u6d4b\u7edf\u8ba1\u91cf\u6216\u5355\u4e2a\u89c1\u8bc1\u503c\u517c\u5bb9\u7684\u7ea0\u7f20\u4fdd\u771f\u5ea6\u4e0b\u754c\u3002", "conclusion": "\u63d0\u51fa\u7684\u534a\u8bbe\u5907\u65e0\u5173\u6846\u67b6\u80fd\u591f\u5728\u4ec5\u77e5\u7cfb\u7edf\u7ef4\u5ea6\u7684\u8f83\u5f31\u5047\u8bbe\u4e0b\uff0c\u6709\u6548\u8ba4\u8bc1\u91cf\u5b50\u4fe1\u9053\u7684\u7ea0\u7f20\u7ef4\u5ea6\u548c\u7ea0\u7f20\u4fdd\u771f\u5ea6\uff0c\u4e3a\u5b9e\u9645\u91cf\u5b50\u901a\u4fe1\u534f\u8bae\u4e2d\u7684\u4fe1\u9053\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07202", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07202", "abs": "https://arxiv.org/abs/2602.07202", "authors": ["Alonso Granados", "Jason Pacheco"], "title": "Risk-Sensitive Exponential Actor Critic", "comment": "To appear at AAAI 2026", "summary": "Model-free deep reinforcement learning (RL) algorithms have achieved tremendous success on a range of challenging tasks. However, safety concerns remain when these methods are deployed on real-world applications, necessitating risk-aware agents. A common utility for learning such risk-aware agents is the entropic risk measure, but current policy gradient methods optimizing this measure must perform high-variance and numerically unstable updates. As a result, existing risk-sensitive model-free approaches are limited to simple tasks and tabular settings. In this paper, we provide a comprehensive theoretical justification for policy gradient methods on the entropic risk measure, including on- and off-policy gradient theorems for the stochastic and deterministic policy settings. Motivated by theory, we propose risk-sensitive exponential actor-critic (rsEAC), an off-policy model-free approach that incorporates novel procedures to avoid the explicit representation of exponential value functions and their gradients, and optimizes its policy w.r.t the entropic risk measure. We show that rsEAC produces more numerically stable updates compared to existing approaches and reliably learns risk-sensitive policies in challenging risky variants of continuous tasks in MuJoCo.", "AI": {"tldr": "\u63d0\u51farsEAC\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u521b\u65b0\u8bbe\u8ba1\u89e3\u51b3\u73b0\u6709\u98ce\u9669\u654f\u611f\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u6570\u503c\u4e0d\u7a33\u5b9a\u95ee\u9898", "motivation": "\u73b0\u6709\u57fa\u4e8e\u71b5\u98ce\u9669\u5ea6\u91cf\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5b58\u5728\u9ad8\u65b9\u5dee\u548c\u6570\u503c\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u9650\u5236\u4e86\u98ce\u9669\u654f\u611f\u65b9\u6cd5\u5728\u590d\u6742\u8fde\u7eed\u4efb\u52a1\u4e2d\u7684\u5e94\u7528", "method": "\u63d0\u51fa\u98ce\u9669\u654f\u611f\u6307\u6570\u6f14\u5458-\u8bc4\u8bba\u5bb6(rsEAC)\u65b9\u6cd5\uff0c\u5305\u542b\u7406\u8bba\u5206\u6790\uff08\u968f\u673a\u548c\u786e\u5b9a\u6027\u7b56\u7565\u7684on/off-policy\u68af\u5ea6\u5b9a\u7406\uff09\u548c\u907f\u514d\u663e\u5f0f\u8868\u793a\u6307\u6570\u4ef7\u503c\u51fd\u6570\u53ca\u5176\u68af\u5ea6\u7684\u521b\u65b0\u7a0b\u5e8f", "result": "rsEAC\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u4ea7\u751f\u66f4\u6570\u503c\u7a33\u5b9a\u7684\u66f4\u65b0\uff0c\u5728MuJoCo\u8fde\u7eed\u4efb\u52a1\u7684\u5371\u9669\u53d8\u4f53\u4e2d\u53ef\u9760\u5730\u5b66\u4e60\u98ce\u9669\u654f\u611f\u7b56\u7565", "conclusion": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548crsEAC\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u98ce\u9669\u654f\u611f\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u6570\u503c\u7a33\u5b9a\u6027\u95ee\u9898"}}
{"id": "2602.08974", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2602.08974", "abs": "https://arxiv.org/abs/2602.08974", "authors": ["Rikpratik Sengupta", "Arkajit Aich", "Kaushik Bhattacharya"], "title": "Cyclic universe from uniform rate inflation on the brane with a timelike extra dimension", "comment": "32 Pages, 10 Figures", "summary": "We investigate a non-singular cosmological scenario in which uniform-rate inflation is realised on an anisotropic Shtanov-Sahni braneworld. The model naturally resolves the initial singularity resulting in an infinite number of smooth non-singular bounces, while accommodating a phase of accelerated expansion driven by a scalar field rolling at a constant rate. The presence of a timelike extra dimension induces high-energy corrections to the effective Friedmann dynamics, allowing anisotropic shear to be dynamically suppressed near the bounce and rendering the background evolution stable. We derive the full background dynamics analytically and demonstrate that uniform-rate inflation can be consistently embedded within an anisotropic braneworld framework. Primordial scalar and tensor perturbations are analysed using the $\u03b4N$ formalism, ensuring that only physically relevant modes exiting the horizon during inflation contribute to observable quantities. Remarkably, we find that observational consistency can be achieved with different levels of anisotropy in the two different scenarios we consider, without compromising the smoothness or stability of the bounce. Our results establish uniform-rate inflation on an anisotropic braneworld as a robust and observationally viable alternative to standard inflationary cosmology, offering a compelling framework in which non-singular early-universe dynamics and precision cosmology can be consistently unified.", "AI": {"tldr": "\u5728Shtanov-Sahni\u819c\u4e16\u754c\u4e0a\u5b9e\u73b0\u5747\u5300\u901f\u7387\u66b4\u80c0\u7684\u975e\u5947\u5f02\u5b87\u5b99\u5b66\u6a21\u578b\uff0c\u901a\u8fc7\u989d\u5916\u7ef4\u5ea6\u4fee\u6b63\u89e3\u51b3\u521d\u59cb\u5947\u70b9\uff0c\u5b9e\u73b0\u65e0\u9650\u6b21\u5e73\u6ed1\u53cd\u5f39\uff0c\u540c\u65f6\u4fdd\u6301\u5404\u5411\u5f02\u6027\u80cc\u666f\u7a33\u5b9a\u3002", "motivation": "\u89e3\u51b3\u6807\u51c6\u66b4\u80c0\u5b87\u5b99\u5b66\u4e2d\u7684\u521d\u59cb\u5947\u70b9\u95ee\u9898\uff0c\u5c06\u975e\u5947\u5f02\u65e9\u671f\u5b87\u5b99\u52a8\u529b\u5b66\u4e0e\u7cbe\u786e\u5b87\u5b99\u5b66\u7edf\u4e00\u8d77\u6765\uff0c\u63a2\u7d22\u5404\u5411\u5f02\u6027\u819c\u4e16\u754c\u6846\u67b6\u4e0b\u66b4\u80c0\u7684\u53ef\u884c\u6027\u3002", "method": "\u5728Shtanov-Sahni\u819c\u4e16\u754c\u4e0a\u6784\u5efa\u5404\u5411\u5f02\u6027\u80cc\u666f\uff0c\u5f15\u5165\u7c7b\u65f6\u989d\u5916\u7ef4\u5ea6\u4ea7\u751f\u9ad8\u80fd\u4fee\u6b63\uff0c\u5b9e\u73b0\u5747\u5300\u901f\u7387\u66b4\u80c0\u3002\u4f7f\u7528\u03b4N\u5f62\u5f0f\u5206\u6790\u539f\u521d\u6807\u91cf\u548c\u5f20\u91cf\u6270\u52a8\uff0c\u786e\u4fdd\u53ea\u6709\u66b4\u80c0\u671f\u95f4\u9000\u51fa\u89c6\u754c\u7684\u7269\u7406\u76f8\u5173\u6a21\u5f0f\u8d21\u732e\u53ef\u89c2\u6d4b\u91cf\u3002", "result": "\u6a21\u578b\u81ea\u7136\u89e3\u51b3\u4e86\u521d\u59cb\u5947\u70b9\uff0c\u4ea7\u751f\u65e0\u9650\u6b21\u5e73\u6ed1\u975e\u5947\u5f02\u53cd\u5f39\u3002\u5404\u5411\u5f02\u6027\u526a\u5207\u5728\u53cd\u5f39\u9644\u8fd1\u88ab\u52a8\u6001\u6291\u5236\uff0c\u80cc\u666f\u6f14\u5316\u7a33\u5b9a\u3002\u89c2\u6d4b\u4e00\u81f4\u6027\u53ef\u5728\u4e0d\u540c\u5404\u5411\u5f02\u6027\u6c34\u5e73\u4e0b\u5b9e\u73b0\uff0c\u4e0d\u635f\u5bb3\u53cd\u5f39\u7684\u5e73\u6ed1\u6027\u6216\u7a33\u5b9a\u6027\u3002", "conclusion": "\u5404\u5411\u5f02\u6027\u819c\u4e16\u754c\u4e0a\u7684\u5747\u5300\u901f\u7387\u66b4\u80c0\u662f\u6807\u51c6\u66b4\u80c0\u5b87\u5b99\u5b66\u7684\u7a33\u5065\u4e14\u89c2\u6d4b\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4e3a\u975e\u5947\u5f02\u65e9\u671f\u5b87\u5b99\u52a8\u529b\u5b66\u4e0e\u7cbe\u786e\u5b87\u5b99\u5b66\u7684\u7edf\u4e00\u63d0\u4f9b\u4e86\u6709\u529b\u6846\u67b6\u3002"}}
{"id": "2602.07858", "categories": ["quant-ph", "physics.acc-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2602.07858", "abs": "https://arxiv.org/abs/2602.07858", "authors": ["M. S. Epov", "I. E. Shenderovich", "S. S. Baturin"], "title": "Geometry-Enabled Radiation from Structured Paraxial Electrons", "comment": "8 pages 2 figures", "summary": "We present a microscopic calculation of spontaneous photon emission by twisted (paraxial) electrons propagating through inhomogeneous, axisymmetric magnetic fields. We construct exact electron states that incorporate transverse mode structure and wavefront curvature by combining the Foldy-Wouthuysen transformation with a geometric framework based on Lewis-Ermakov invariants and metaplectic transformations. We show that the evolution of such structured states corresponds to an open path in the space of quadratic forms, giving rise to a geometric contribution to the emission amplitude that cannot be eliminated by gauge choice or adiabatic arguments. The inverse radius of curvature of the electron wavefront emerges as an effective geometric field that enables radiation even in regions where the external magnetic field vanishes locally. This mechanism generalizes Landau-level radiation to nonasymptotic, structured electron states and establishes a direct connection between noncyclic geometric evolution and photon emission.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5fae\u89c2\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u626d\u66f2\uff08\u508d\u8f74\uff09\u7535\u5b50\u5728\u975e\u5747\u5300\u8f74\u5bf9\u79f0\u78c1\u573a\u4e2d\u4f20\u64ad\u65f6\u7684\u81ea\u53d1\u5149\u5b50\u53d1\u5c04\u73b0\u8c61\uff0c\u63ed\u793a\u4e86\u6ce2\u524d\u66f2\u7387\u4f5c\u4e3a\u6709\u6548\u51e0\u4f55\u573a\u9a71\u52a8\u8f90\u5c04\u7684\u65b0\u673a\u5236\u3002", "motivation": "\u7814\u7a76\u7ed3\u6784\u5316\u7535\u5b50\u6001\uff08\u5982\u626d\u66f2\u7535\u5b50\uff09\u5728\u975e\u5747\u5300\u78c1\u573a\u4e2d\u7684\u8f90\u5c04\u884c\u4e3a\uff0c\u63a2\u7d22\u51e0\u4f55\u6f14\u5316\u4e0e\u5149\u5b50\u53d1\u5c04\u4e4b\u95f4\u7684\u76f4\u63a5\u8054\u7cfb\uff0c\u7279\u522b\u662f\u5f53\u5916\u90e8\u78c1\u573a\u5c40\u90e8\u4e3a\u96f6\u65f6\u4ecd\u80fd\u4ea7\u751f\u8f90\u5c04\u7684\u673a\u5236\u3002", "method": "\u7ed3\u5408Foldy-Wouthuysen\u53d8\u6362\u4e0e\u57fa\u4e8eLewis-Ermakov\u4e0d\u53d8\u91cf\u548c\u5143\u8f9b\u53d8\u6362\u7684\u51e0\u4f55\u6846\u67b6\uff0c\u6784\u5efa\u4e86\u5305\u542b\u6a2a\u5411\u6a21\u5f0f\u7ed3\u6784\u548c\u6ce2\u524d\u66f2\u7387\u7684\u7cbe\u786e\u7535\u5b50\u6001\uff0c\u5206\u6790\u5176\u5728\u4e8c\u6b21\u578b\u7a7a\u95f4\u4e2d\u7684\u6f14\u5316\u8def\u5f84\u3002", "result": "\u53d1\u73b0\u7535\u5b50\u6ce2\u524d\u66f2\u7387\u534a\u5f84\u7684\u5012\u6570\u53ef\u4f5c\u4e3a\u6709\u6548\u51e0\u4f55\u573a\uff0c\u5373\u4f7f\u5728\u5916\u90e8\u78c1\u573a\u5c40\u90e8\u4e3a\u96f6\u7684\u533a\u57df\u4e5f\u80fd\u4ea7\u751f\u8f90\u5c04\uff1b\u8bc1\u660e\u4e86\u975e\u5faa\u73af\u51e0\u4f55\u6f14\u5316\u5bf9\u53d1\u5c04\u632f\u5e45\u7684\u51e0\u4f55\u8d21\u732e\u65e0\u6cd5\u901a\u8fc7\u89c4\u8303\u9009\u62e9\u6216\u7edd\u70ed\u8bba\u8bc1\u6d88\u9664\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c06\u6717\u9053\u80fd\u7ea7\u8f90\u5c04\u63a8\u5e7f\u5230\u975e\u6e10\u8fd1\u7684\u7ed3\u6784\u5316\u7535\u5b50\u6001\uff0c\u5efa\u7acb\u4e86\u975e\u5faa\u73af\u51e0\u4f55\u6f14\u5316\u4e0e\u5149\u5b50\u53d1\u5c04\u4e4b\u95f4\u7684\u76f4\u63a5\u8054\u7cfb\uff0c\u4e3a\u7ed3\u6784\u5316\u7535\u5b50\u675f\u4e0e\u7535\u78c1\u573a\u76f8\u4e92\u4f5c\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2602.07203", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07203", "abs": "https://arxiv.org/abs/2602.07203", "authors": ["R. Teal Witter", "\u00c1lvaro Parafita", "Tomas Garriga", "Maximilian Muschalik", "Fabian Fumagalli", "Axel Brando", "Lucas Rosenblatt"], "title": "Exactly Computing do-Shapley Values", "comment": null, "summary": "Structural Causal Models (SCM) are a powerful framework for describing complicated dynamics across the natural sciences. A particularly elegant way of interpreting SCMs is do-Shapley, a game-theoretic method of quantifying the average effect of $d$ variables across exponentially many interventions. Like Shapley values, computing do-Shapley values generally requires evaluating exponentially many terms. The foundation of our work is a reformulation of do-Shapley values in terms of the irreducible sets of the underlying SCM. Leveraging this insight, we can exactly compute do-Shapley values in time linear in the number of irreducible sets $r$, which itself can range from $d$ to $2^d$ depending on the graph structure of the SCM. Since $r$ is unknown a priori, we complement the exact algorithm with an estimator that, like general Shapley value estimators, can be run with any query budget. As the query budget approaches $r$, our estimators can produce more accurate estimates than prior methods by several orders of magnitude, and, when the budget reaches $r$, return the Shapley values up to machine precision. Beyond computational speed, we also reduce the identification burden: we prove that non-parametric identifiability of do-Shapley values requires only the identification of interventional effects for the $d$ singleton coalitions, rather than all classes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u8ba1\u7b97do-Shapley\u503c\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4ece\u6307\u6570\u7ea7\u964d\u4f4e\u5230\u4e0e\u4e0d\u53ef\u7ea6\u96c6\u6570\u91cfr\u7ebf\u6027\u76f8\u5173\uff0c\u5e76\u63d0\u4f9b\u4e86\u7cbe\u786e\u7b97\u6cd5\u548c\u9884\u7b97\u53ef\u63a7\u7684\u4f30\u8ba1\u5668\u3002", "motivation": "do-Shapley\u503c\u4f5c\u4e3a\u91cf\u5316\u53d8\u91cf\u5e73\u5747\u56e0\u679c\u6548\u5e94\u7684\u91cd\u8981\u65b9\u6cd5\uff0c\u4f20\u7edf\u8ba1\u7b97\u9700\u8981\u6307\u6570\u7ea7\u590d\u6742\u5ea6\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u8ba1\u7b97\u65b9\u6cd5\u3002", "method": "\u5c06do-Shapley\u503c\u91cd\u65b0\u8868\u8ff0\u4e3a\u5e95\u5c42SCM\u4e0d\u53ef\u7ea6\u96c6\u7684\u51fd\u6570\uff0c\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u7cbe\u786e\u7b97\u6cd5\uff08\u65f6\u95f4\u590d\u6742\u5ea6O(r)\uff09\u548c\u9884\u7b97\u53ef\u63a7\u7684\u4f30\u8ba1\u5668\uff0c\u5176\u4e2dr\u662fSCM\u4e2d\u4e0d\u53ef\u7ea6\u96c6\u7684\u6570\u91cf\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u8bc6\u522b\u8d1f\u62c5\u65b9\u9762\u5747\u6709\u663e\u8457\u6539\u8fdb\uff1a\u8ba1\u7b97\u590d\u6742\u5ea6\u4ece\u6307\u6570\u7ea7\u964d\u81f3\u7ebf\u6027\u7ea7\uff1b\u4f30\u8ba1\u5668\u5728\u76f8\u540c\u67e5\u8be2\u9884\u7b97\u4e0b\u6bd4\u73b0\u6709\u65b9\u6cd5\u51c6\u786e\u5ea6\u9ad8\u51e0\u4e2a\u6570\u91cf\u7ea7\uff1b\u8bc6\u522b\u8981\u6c42\u4ece\u6240\u6709\u7c7b\u522b\u51cf\u5c11\u5230\u4ec5\u9700d\u4e2a\u5355\u5143\u7d20\u8054\u76df\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528SCM\u7684\u4e0d\u53ef\u7ea6\u96c6\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u9ad8\u4e86do-Shapley\u503c\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u964d\u4f4e\u4e86\u8bc6\u522b\u8981\u6c42\uff0c\u4e3a\u56e0\u679c\u63a8\u65ad\u4e2d\u7684Shapley\u503c\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u5de5\u5177\u3002"}}
{"id": "2602.07887", "categories": ["quant-ph", "gr-qc", "hep-th"], "pdf": "https://arxiv.org/pdf/2602.07887", "abs": "https://arxiv.org/abs/2602.07887", "authors": ["Michele Arzano", "Antonio Del Prete", "Domenico Frattulillo"], "title": "Quantum Evolution of Hopf Algebra Hamiltonians", "comment": "18 pages, no figures", "summary": "In recent years, growing attention has been devoted to the possibility that theories with deformed symmetries, associated with certain models of non-commutative spacetime, may encode a fundamental form of decoherence. This effect should be described by a Lindblad-like evolution governed by the non-trivial Hopf algebra structure of the time-evolution generators. In this work we provide a detailed analysis of such possibility for similar Hopf algebra deformations of the Hamiltonian of a qubit. Starting from a critical examination of the very definition of time evolution through the generalized adjoint action, we explore whether a coherent and physically viable framework can be established. In particular, our analysis shows that a more general combination of adjoint actions always guarantees a von Neumann dynamics and, also in the case of deformed spacetime symmetries considered in the literature, a physically viable Lindblad evolution cannot be established.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u975e\u5bf9\u6613\u65f6\u7a7a\u6a21\u578b\u4e2d\u53d8\u5f62\u5bf9\u79f0\u6027\u662f\u5426\u4f1a\u5bfc\u81f4\u57fa\u672c\u9000\u76f8\u5e72\u6548\u5e94\uff0c\u53d1\u73b0\u6587\u732e\u4e2d\u8003\u8651\u7684\u53d8\u5f62\u65f6\u7a7a\u5bf9\u79f0\u6027\u65e0\u6cd5\u5efa\u7acb\u7269\u7406\u53ef\u884c\u7684Lindblad\u6f14\u5316\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u4eba\u4eec\u8d8a\u6765\u8d8a\u5173\u6ce8\u5177\u6709\u53d8\u5f62\u5bf9\u79f0\u6027\u7684\u7406\u8bba\uff08\u4e0e\u975e\u5bf9\u6613\u65f6\u7a7a\u6a21\u578b\u76f8\u5173\uff09\u53ef\u80fd\u7f16\u7801\u57fa\u672c\u5f62\u5f0f\u7684\u9000\u76f8\u5e72\u6548\u5e94\u3002\u8fd9\u79cd\u6548\u5e94\u5e94\u8be5\u7531\u5177\u6709\u975e\u5e73\u51e1Hopf\u4ee3\u6570\u7ed3\u6784\u7684\u65f6\u95f4\u6f14\u5316\u751f\u6210\u5143\u6240\u652f\u914d\u7684Lindblad-like\u6f14\u5316\u6765\u63cf\u8ff0\u3002", "method": "\u901a\u8fc7\u5bf9\u91cf\u5b50\u6bd4\u7279\u54c8\u5bc6\u987f\u91cf\u7684\u7c7b\u4f3cHopf\u4ee3\u6570\u53d8\u5f62\u8fdb\u884c\u8be6\u7ec6\u5206\u6790\uff0c\u4ece\u6279\u5224\u6027\u5730\u68c0\u67e5\u901a\u8fc7\u5e7f\u4e49\u4f34\u968f\u4f5c\u7528\u5b9a\u4e49\u65f6\u95f4\u6f14\u5316\u5f00\u59cb\uff0c\u63a2\u7d22\u662f\u5426\u80fd\u591f\u5efa\u7acb\u4e00\u4e2a\u8fde\u8d2f\u4e14\u7269\u7406\u53ef\u884c\u7684\u6846\u67b6\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u66f4\u4e00\u822c\u7684\u4f34\u968f\u4f5c\u7528\u7ec4\u5408\u603b\u662f\u4fdd\u8bc1von Neumann\u52a8\u529b\u5b66\uff0c\u5e76\u4e14\u5728\u6587\u732e\u4e2d\u8003\u8651\u7684\u53d8\u5f62\u65f6\u7a7a\u5bf9\u79f0\u6027\u60c5\u51b5\u4e0b\uff0c\u65e0\u6cd5\u5efa\u7acb\u7269\u7406\u53ef\u884c\u7684Lindblad\u6f14\u5316\u3002", "conclusion": "\u5bf9\u4e8e\u6587\u732e\u4e2d\u8003\u8651\u7684\u53d8\u5f62\u65f6\u7a7a\u5bf9\u79f0\u6027\uff0c\u65e0\u6cd5\u5efa\u7acb\u7269\u7406\u53ef\u884c\u7684Lindblad\u6f14\u5316\u6846\u67b6\uff0c\u8fd9\u8868\u660e\u6b64\u7c7b\u6a21\u578b\u53ef\u80fd\u65e0\u6cd5\u7f16\u7801\u57fa\u672c\u5f62\u5f0f\u7684\u9000\u76f8\u5e72\u6548\u5e94\u3002"}}
{"id": "2602.07867", "categories": ["quant-ph", "math-ph", "nlin.SI"], "pdf": "https://arxiv.org/pdf/2602.07867", "abs": "https://arxiv.org/abs/2602.07867", "authors": ["Wen-Ming Fan", "Kun Hao", "Xiao-Hui Wang", "Kun Zhang", "Vladimir Korepin"], "title": "Minimal nonintegrable models with three-site interactions", "comment": "41pages, 9 figures, 2 tables", "summary": "A systematic understanding of integrability breaking in translationally invariant spin chains with genuine three-site interactions remains lacking. In this work, we introduce and classify minimal nonintegrable spin-$1/2$ Hamiltonians, defined as models that saturate injectivity while admitting no nontrivial local conserved charges beyond the Hamiltonian. We first rigorously establish the nonintegrability of the deformed Fredkin spin chain with periodic boundary conditions by mapping it to a nearest-neighbor composite-spin representation and excluding all admissible $3$-local conserved charges. Guided by its structure, we then construct five classes of spin-$1/2$ models with genuine three-site interactions. One class is integrable, while the remaining four contain exactly two interaction terms and constitute the minimal nonintegrable three-site models. Our results delineate a sharp boundary between integrability and nonintegrability beyond the nearest-neighbor paradigm.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u5177\u6709\u771f\u6b63\u4e09\u4f53\u76f8\u4e92\u4f5c\u7528\u7684\u5e73\u79fb\u4e0d\u53d8\u81ea\u65cb\u94fe\u4e2d\u7684\u53ef\u79ef\u6027\u7834\u574f\uff0c\u901a\u8fc7\u6784\u9020\u548c\u5206\u7c7b\u6700\u5c0f\u4e0d\u53ef\u79ef\u81ea\u65cb-1/2\u54c8\u5bc6\u987f\u91cf\uff0c\u5212\u5b9a\u4e86\u8d85\u8d8a\u6700\u8fd1\u90bb\u8303\u5f0f\u7684\u53ef\u79ef\u4e0e\u4e0d\u53ef\u79ef\u4e4b\u95f4\u7684\u6e05\u6670\u8fb9\u754c\u3002", "motivation": "\u76ee\u524d\u5bf9\u5177\u6709\u771f\u6b63\u4e09\u4f53\u76f8\u4e92\u4f5c\u7528\u7684\u5e73\u79fb\u4e0d\u53d8\u81ea\u65cb\u94fe\u4e2d\u53ef\u79ef\u6027\u7834\u574f\u7684\u7cfb\u7edf\u7406\u89e3\u4ecd\u7136\u7f3a\u4e4f\u3002\u4f5c\u8005\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u5f15\u5165\u548c\u5206\u7c7b\u6700\u5c0f\u4e0d\u53ef\u79ef\u81ea\u65cb-1/2\u54c8\u5bc6\u987f\u91cf\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u6ee1\u8db3\u6ce8\u5165\u6027\u7684\u540c\u65f6\uff0c\u9664\u4e86\u54c8\u5bc6\u987f\u91cf\u5916\u4e0d\u5141\u8bb8\u4efb\u4f55\u975e\u5e73\u51e1\u7684\u5c40\u57df\u5b88\u6052\u8377\u3002", "method": "\u9996\u5148\u901a\u8fc7\u5c06\u5468\u671f\u6027\u8fb9\u754c\u6761\u4ef6\u4e0b\u7684\u53d8\u5f62Fredkin\u81ea\u65cb\u94fe\u6620\u5c04\u5230\u6700\u8fd1\u90bb\u590d\u5408\u81ea\u65cb\u8868\u793a\uff0c\u5e76\u6392\u9664\u6240\u6709\u53ef\u5bb9\u8bb8\u76843-\u5c40\u57df\u5b88\u6052\u8377\uff0c\u4e25\u683c\u5efa\u7acb\u4e86\u5176\u4e0d\u53ef\u79ef\u6027\u3002\u7136\u540e\u57fa\u4e8e\u8be5\u7ed3\u6784\uff0c\u6784\u9020\u4e86\u4e94\u7c7b\u5177\u6709\u771f\u6b63\u4e09\u4f53\u76f8\u4e92\u4f5c\u7528\u7684\u81ea\u65cb-1/2\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u5176\u4e2d\u4e00\u7c7b\u662f\u53ef\u79ef\u7684\uff0c\u800c\u5176\u4f59\u56db\u7c7b\u6070\u597d\u5305\u542b\u4e24\u4e2a\u76f8\u4e92\u4f5c\u7528\u9879\uff0c\u6784\u6210\u4e86\u6700\u5c0f\u4e0d\u53ef\u79ef\u7684\u4e09\u4f53\u76f8\u4e92\u4f5c\u7528\u6a21\u578b\u3002\u8fd9\u4e9b\u7ed3\u679c\u6e05\u6670\u5730\u5212\u5b9a\u4e86\u8d85\u8d8a\u6700\u8fd1\u90bb\u8303\u5f0f\u7684\u53ef\u79ef\u6027\u4e0e\u4e0d\u53ef\u79ef\u6027\u4e4b\u95f4\u7684\u8fb9\u754c\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u7cfb\u7edf\u5206\u7c7b\u6700\u5c0f\u4e0d\u53ef\u79ef\u81ea\u65cb-1/2\u54c8\u5bc6\u987f\u91cf\uff0c\u4e3a\u7406\u89e3\u5177\u6709\u771f\u6b63\u4e09\u4f53\u76f8\u4e92\u4f5c\u7528\u7684\u5e73\u79fb\u4e0d\u53d8\u81ea\u65cb\u94fe\u4e2d\u7684\u53ef\u79ef\u6027\u7834\u574f\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u660e\u786e\u5212\u5b9a\u4e86\u53ef\u79ef\u4e0e\u4e0d\u53ef\u79ef\u6a21\u578b\u4e4b\u95f4\u7684\u8fb9\u754c\u3002"}}
{"id": "2602.07205", "categories": ["cs.LG", "cs.GT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07205", "abs": "https://arxiv.org/abs/2602.07205", "authors": ["Junyan Liu", "Haipeng Luo", "Zihan Zhang", "Lillian J. Ratliff"], "title": "Online Learning for Uninformed Markov Games: Empirical Nash-Value Regret and Non-Stationarity Adaptation", "comment": "36 pages", "summary": "We study online learning in two-player uninformed Markov games, where the opponent's actions and policies are unobserved. In this setting, Tian et al. (2021) show that achieving no-external-regret is impossible without incurring an exponential dependence on the episode length $H$. They then turn to the weaker notion of Nash-value regret and propose a V-learning algorithm with regret $O(K^{2/3})$ after $K$ episodes. However, their algorithm and guarantee do not adapt to the difficulty of the problem: even in the case where the opponent follows a fixed policy and thus $O(\\sqrt{K})$ external regret is well-known to be achievable, their result is still the worse rate $O(K^{2/3})$ on a weaker metric.\n  In this work, we fully address both limitations. First, we introduce empirical Nash-value regret, a new regret notion that is strictly stronger than Nash-value regret and naturally reduces to external regret when the opponent follows a fixed policy. Moreover, under this new metric, we propose a parameter-free algorithm that achieves an $O(\\min \\{\\sqrt{K} + (CK)^{1/3},\\sqrt{LK}\\})$ regret bound, where $C$ quantifies the variance of the opponent's policies and $L$ denotes the number of policy switches (both at most $O(K)$). Therefore, our results not only recover the two extremes -- $O(\\sqrt{K})$ external regret when the opponent is fixed and $O(K^{2/3})$ Nash-value regret in the worst case -- but also smoothly interpolate between these extremes by automatically adapting to the opponent's non-stationarity. We achieve so by first providing a new analysis of the epoch-based V-learning algorithm by Mao et al. (2022), establishing an $O(\u03b7C + \\sqrt{K/\u03b7})$ regret bound, where $\u03b7$ is the epoch incremental factor. Next, we show how to adaptively restart this algorithm with an appropriate $\u03b7$ in response to the potential non-stationarity of the opponent, eventually achieving our final results.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ecf\u9a8c\u7eb3\u4ec0\u503c\u9057\u61be\u5ea6\u91cf\uff0c\u5e76\u8bbe\u8ba1\u4e86\u53c2\u6570\u81ea\u7531\u7b97\u6cd5\uff0c\u80fd\u591f\u81ea\u9002\u5e94\u5730\u9002\u5e94\u5bf9\u624b\u7684\u975e\u5e73\u7a33\u6027\uff0c\u5728\u56fa\u5b9a\u5bf9\u624b\u65f6\u6062\u590dO(\u221aK)\u5916\u90e8\u9057\u61be\uff0c\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u6062\u590dO(K^{2/3})\u7eb3\u4ec0\u503c\u9057\u61be\u3002", "motivation": "\u73b0\u6709V-learning\u7b97\u6cd5\u5728\u65e0\u4fe1\u606f\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u4e2d\u53ea\u80fd\u8fbe\u5230O(K^{2/3})\u7684\u7eb3\u4ec0\u503c\u9057\u61be\uff0c\u4e14\u65e0\u6cd5\u9002\u5e94\u95ee\u9898\u96be\u5ea6\uff1a\u5373\u4f7f\u5bf9\u624b\u9075\u5faa\u56fa\u5b9a\u7b56\u7565\uff08\u6b64\u65f6O(\u221aK)\u5916\u90e8\u9057\u61be\u662f\u53ef\u5b9e\u73b0\u7684\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u4ecd\u7ed9\u51fa\u8f83\u5dee\u7684O(K^{2/3})\u901f\u7387\u3002", "method": "1. \u5f15\u5165\u7ecf\u9a8c\u7eb3\u4ec0\u503c\u9057\u61be\u8fd9\u4e00\u65b0\u9057\u61be\u5ea6\u91cf\uff0c\u6bd4\u7eb3\u4ec0\u503c\u9057\u61be\u66f4\u5f3a\uff0c\u5728\u5bf9\u624b\u56fa\u5b9a\u65f6\u81ea\u7136\u9000\u5316\u4e3a\u5916\u90e8\u9057\u61be\u30022. \u63d0\u51fa\u53c2\u6570\u81ea\u7531\u7b97\u6cd5\uff0c\u57fa\u4e8eMao\u7b49\u4eba(2022)\u7684\u57fa\u4e8eepoch\u7684V-learning\u7b97\u6cd5\u65b0\u5206\u6790\uff0c\u5efa\u7acbO(\u03b7C + \u221aK/\u03b7)\u9057\u61be\u754c\u30023. \u5c55\u793a\u5982\u4f55\u6839\u636e\u5bf9\u624b\u7684\u975e\u5e73\u7a33\u6027\u81ea\u9002\u5e94\u91cd\u542f\u7b97\u6cd5\u5e76\u9009\u62e9\u9002\u5f53\u7684\u03b7\u53c2\u6570\u3002", "result": "\u7b97\u6cd5\u5b9e\u73b0\u4e86O(min{\u221aK + (CK)^{1/3}, \u221aLK})\u9057\u61be\u754c\uff0c\u5176\u4e2dC\u91cf\u5316\u5bf9\u624b\u7b56\u7565\u7684\u65b9\u5dee\uff0cL\u8868\u793a\u7b56\u7565\u5207\u6362\u6b21\u6570\u3002\u8fd9\u6062\u590d\u4e86\u4e24\u4e2a\u6781\u7aef\u60c5\u51b5\uff1a\u5bf9\u624b\u56fa\u5b9a\u65f6\u7684O(\u221aK)\u5916\u90e8\u9057\u61be\u548c\u6700\u574f\u60c5\u51b5\u4e0b\u7684O(K^{2/3})\u7eb3\u4ec0\u503c\u9057\u61be\uff0c\u5e76\u80fd\u901a\u8fc7\u81ea\u52a8\u9002\u5e94\u5bf9\u624b\u7684\u975e\u5e73\u7a33\u6027\u5728\u8fd9\u4e9b\u6781\u7aef\u4e4b\u95f4\u5e73\u6ed1\u63d2\u503c\u3002", "conclusion": "\u672c\u6587\u5b8c\u5168\u89e3\u51b3\u4e86\u73b0\u6709\u5de5\u4f5c\u7684\u4e24\u4e2a\u9650\u5236\uff1a1) \u63d0\u51fa\u4e86\u66f4\u5f3a\u4e14\u66f4\u81ea\u7136\u7684\u9057\u61be\u5ea6\u91cf\uff1b2) \u8bbe\u8ba1\u4e86\u80fd\u591f\u81ea\u9002\u5e94\u95ee\u9898\u96be\u5ea6\u7684\u53c2\u6570\u81ea\u7531\u7b97\u6cd5\uff0c\u5728\u4e0d\u540c\u5bf9\u624b\u884c\u4e3a\u6a21\u5f0f\u4e0b\u90fd\u80fd\u8fbe\u5230\u6700\u4f18\u6216\u63a5\u8fd1\u6700\u4f18\u7684\u9057\u61be\u754c\u3002"}}
{"id": "2602.07206", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07206", "abs": "https://arxiv.org/abs/2602.07206", "authors": ["Bucher Sahyouni", "Matthew Vowels", "Liqun Chen", "Simon Hadfield"], "title": "DSL: Understanding and Improving Softmax Recommender Systems with Competition-Aware Scaling", "comment": null, "summary": "Softmax Loss (SL) is being increasingly adopted for recommender systems (RS) as it has demonstrated better performance, robustness and fairness. Yet in implicit-feedback, a single global temperature and equal treatment of uniformly sampled negatives can lead to brittle training, because sampled sets may contain varying degrees of relevant or informative competitors. The optimal loss sharpness for a user-item pair with a particular set of negatives, can be suboptimal or destabilising for another with different negatives. We introduce Dual-scale Softmax Loss (DSL), which infers effective sharpness from the sampled competition itself. DSL adds two complementary branches to the log-sum-exp backbone. Firstly it reweights negatives within each training instance using hardness and item--item similarity, secondly it adapts a per-example temperature from the competition intensity over a constructed competitor slate. Together, these components preserve the geometry of SL while reshaping the competition distribution across negatives and across examples.\n  Over several representative benchmarks and backbones, DSL yields substantial gains over strong baselines, with improvements over SL exceeding $10%$ in several settings and averaging $6.22%$ across datasets, metrics, and backbones. Under out-of-distribution (OOD) popularity shift, the gains are larger, with an average of $9.31%$ improvement over SL. We further provide a theoretical, distributionally robust optimisation (DRO) analysis, which demonstrates how DSL reshapes the robust payoff and the KL deviation for ambiguous instances. This helps explain the empirically observed improvements in accuracy and robustness.", "AI": {"tldr": "DSL\u901a\u8fc7\u81ea\u9002\u5e94\u6e29\u5ea6\u8c03\u8282\u548c\u8d1f\u6837\u672c\u91cd\u52a0\u6743\uff0c\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u663e\u8457\u63d0\u5347Softmax Loss\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027", "motivation": "\u4f20\u7edfSoftmax Loss\u5728\u9690\u5f0f\u53cd\u9988\u63a8\u8350\u7cfb\u7edf\u4e2d\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a1\uff09\u5355\u4e00\u5168\u5c40\u6e29\u5ea6\u5bf9\u6240\u6709\u7528\u6237-\u7269\u54c1\u5bf9\u4e0d\u9002\u7528\uff1b2\uff09\u5747\u5300\u91c7\u6837\u7684\u8d1f\u6837\u672c\u53ef\u80fd\u5305\u542b\u4e0d\u540c\u76f8\u5173\u6027\u7684\u7ade\u4e89\u8005\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a", "method": "\u63d0\u51fa\u53cc\u5c3a\u5ea6Softmax Loss (DSL)\uff0c\u5305\u542b\u4e24\u4e2a\u5206\u652f\uff1a1\uff09\u57fa\u4e8e\u786c\u5ea6\u548c\u7269\u54c1\u76f8\u4f3c\u5ea6\u5bf9\u6bcf\u4e2a\u8bad\u7ec3\u5b9e\u4f8b\u5185\u7684\u8d1f\u6837\u672c\u8fdb\u884c\u91cd\u52a0\u6743\uff1b2\uff09\u4ece\u6784\u5efa\u7684\u7ade\u4e89\u8005\u5217\u8868\u4e2d\u81ea\u9002\u5e94\u5b66\u4e60\u6bcf\u4e2a\u793a\u4f8b\u7684\u6e29\u5ea6", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u9aa8\u5e72\u7f51\u7edc\u4e0a\uff0cDSL\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u6709\u663e\u8457\u63d0\u5347\uff0c\u5e73\u5747\u6539\u8fdb6.22%\uff0c\u67d0\u4e9b\u8bbe\u7f6e\u4e0b\u8d85\u8fc710%\u3002\u5728OOD\u6d41\u884c\u5ea6\u504f\u79fb\u4e0b\uff0c\u5e73\u5747\u6539\u8fdb9.31%", "conclusion": "DSL\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u7ade\u4e89\u5206\u5e03\uff0c\u5728\u4fdd\u6301Softmax Loss\u51e0\u4f55\u7ed3\u6784\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\uff0c\u7406\u8bba\u5206\u6790\u4e5f\u652f\u6301\u5176\u6709\u6548\u6027"}}
{"id": "2602.07912", "categories": ["quant-ph", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2602.07912", "abs": "https://arxiv.org/abs/2602.07912", "authors": ["Yuichiro Yoshida", "Takuma Murokoshi", "Naoya Kuroda", "Wataru Mizukami"], "title": "Doubling the size of quantum selected configuration interaction based on seniority-zero space and its application to QC-QSCI-AFQMC", "comment": "8 pages, 5 figures", "summary": "We propose doubly occupied configuration interaction-quantum selected configuration interaction (DOCI-QSCI), which samples from the seniority-zero space. While the use of this space effectively doubles the qubit budget, equaling the number of spatial orbitals, this sector restriction can compromise quantitative accuracy. To compensate for this, we expand sampled bitstrings via their Cartesian product into a larger space that includes seniority-breaking determinants. The resulting wave function is also proposed using the trial state in phaseless auxiliary-field quantum Monte Carlo (ph-AFQMC) to recover dynamical correlations across the full orbital space (DOCI-QSCI-AFQMC). We evaluate the proposed methods on the H6 chain, N2 dissociation, and the addition of singlet O2 to a BODIPY dye. For the H6 chain, DOCI-QSCI-AFQMC reproduces the accuracy of the level of the complete-active-space counterpart with the quantum device ibm kobe. For N2 and BODIPY-O2, with (14e, 28o) and up to (20e, 20o) active spaces, it yields reasonable results, whereas single-reference CCSD(T) fails qualitatively. These results demonstrate that the DOCI-QSCI doubles the orbital space accessible to conventional QSCI and subsequent ph-AFQMC post-processing delivers reasonably high accuracy.", "AI": {"tldr": "DOCI-QSCI\u65b9\u6cd5\u901a\u8fc7\u91c7\u6837seniority-zero\u7a7a\u95f4\uff0c\u7ed3\u5408AFQMC\u540e\u5904\u7406\uff0c\u5728\u91cf\u5b50\u8ba1\u7b97\u4e2d\u5b9e\u73b0\u66f4\u5927\u8f68\u9053\u7a7a\u95f4\u7684\u5316\u5b66\u6a21\u62df\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u7cbe\u5ea6\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u91cf\u5b50\u9009\u62e9\u7ec4\u6001\u76f8\u4e92\u4f5c\u7528(QSCI)\u53d7\u9650\u4e8e\u8f68\u9053\u7a7a\u95f4\u5927\u5c0f\uff0c\u800cseniority-zero\u7a7a\u95f4\u867d\u7136\u80fd\u6269\u5c55\u8f68\u9053\u6570\u91cf\u4f46\u4f1a\u635f\u5931\u5b9a\u91cf\u7cbe\u5ea6\uff0c\u9700\u8981\u8865\u507f\u673a\u5236\u6765\u6062\u590d\u52a8\u6001\u5173\u8054\u3002", "method": "\u63d0\u51faDOCI-QSCI\u65b9\u6cd5\uff1a1) \u91c7\u6837seniority-zero\u7a7a\u95f4\uff0c\u5c06\u8f68\u9053\u7a7a\u95f4\u6269\u5c55\u4e00\u500d\uff1b2) \u901a\u8fc7\u7b1b\u5361\u5c14\u79ef\u5c06\u91c7\u6837\u6bd4\u7279\u4e32\u6269\u5c55\u5230\u5305\u542bseniority-breaking\u884c\u5217\u5f0f\u7684\u66f4\u5927\u7a7a\u95f4\uff1b3) \u5c06\u5f97\u5230\u7684\u6ce2\u51fd\u6570\u4f5c\u4e3aphaseless AFQMC\u7684\u8bd5\u9a8c\u6001\uff0c\u6062\u590d\u5168\u8f68\u9053\u7a7a\u95f4\u7684\u52a8\u6001\u5173\u8054\u3002", "result": "\u5728H6\u94fe\u4e2d\uff0cDOCI-QSCI-AFQMC\u8fbe\u5230\u4e0e\u5b8c\u6574\u6d3b\u6027\u7a7a\u95f4\u76f8\u5f53\u7684\u7cbe\u5ea6\uff1b\u5728N2\u89e3\u79bb\u548cBODIPY-O2\u4f53\u7cfb\u4e2d\uff0c\u5bf9\u4e8e(14e,28o)\u548c(20e,20o)\u6d3b\u6027\u7a7a\u95f4\uff0c\u83b7\u5f97\u5408\u7406\u7ed3\u679c\uff0c\u800c\u5355\u53c2\u8003CCSD(T)\u5b8c\u5168\u5931\u8d25\u3002", "conclusion": "DOCI-QSCI\u5c06\u4f20\u7edfQSCI\u53ef\u8bbf\u95ee\u7684\u8f68\u9053\u7a7a\u95f4\u6269\u5c55\u4e00\u500d\uff0c\u7ed3\u5408AFQMC\u540e\u5904\u7406\u80fd\u63d0\u4f9b\u76f8\u5f53\u9ad8\u7684\u7cbe\u5ea6\uff0c\u4e3a\u91cf\u5b50\u8ba1\u7b97\u5904\u7406\u66f4\u5927\u5316\u5b66\u4f53\u7cfb\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.07213", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07213", "abs": "https://arxiv.org/abs/2602.07213", "authors": ["Srijan Shakya", "Anamaria-Roberta Hartl", "Sepp Hochreiter", "Korbinian P\u00f6ppel"], "title": "Adaptive Retrieval helps Reasoning in LLMs -- but mostly if it's not used", "comment": "Eurips Workshop on Principles of Generative Modeling (PriGM)", "summary": "Large Language Models (LLMs) often falter in complex reasoning tasks due to their static, parametric knowledge, leading to hallucinations and poor performance in specialized domains like mathematics. This work explores a fundamental principle for enhancing generative models: treating retrieval as a form of dynamic in-context learning. We test an adaptive retrieval-augmented architecture where an LLM agent actively decides when to query an external knowledge base during its reasoning process. We compare this adaptive strategy against a standard Chain-of-Thought (CoT) baseline and a static retrieval approach on the GSM8K and MATH-500 benchmarks. Although our experiments show that static retrieval is inferior to CoT, the adaptive retrieval shows interesting behavior: While traces including retrieved results show slightly worse performance compared to CoT, traces that do not include retrieval actually perform better compared to CoT. This suggests that: (a) retrieval only rarely helps reasoning (we show a few counterexamples, e.g. using useful theorems) and (b) actively not using retrieval is indicative of good model performance. Furthermore, we find that the model scales its retrieval frequency with the difficulty of the problem, reinforcing that the decision to retrieve is a crucial metacognitive signal. The agent's ability to self-assess its knowledge and selectively engage with external information represents a key principle for building more robust and reliable generative models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u81ea\u9002\u5e94\u68c0\u7d22\u589e\u5f3a\u67b6\u6784\uff0c\u8ba9LLM\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4e3b\u52a8\u51b3\u5b9a\u4f55\u65f6\u67e5\u8be2\u5916\u90e8\u77e5\u8bc6\u5e93\u3002\u5b9e\u9a8c\u53d1\u73b0\u9759\u6001\u68c0\u7d22\u4e0d\u5982CoT\uff0c\u4f46\u81ea\u9002\u5e94\u68c0\u7d22\u4e2d\u4e0d\u5305\u542b\u68c0\u7d22\u7684\u63a8\u7406\u8f68\u8ff9\u8868\u73b0\u4f18\u4e8eCoT\uff0c\u8868\u660e\u68c0\u7d22\u5f88\u5c11\u5e2e\u52a9\u63a8\u7406\uff0c\u800c\u4e3b\u52a8\u4e0d\u4f7f\u7528\u68c0\u7d22\u662f\u6a21\u578b\u6027\u80fd\u826f\u597d\u7684\u4fe1\u53f7\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u5e38\u56e0\u9759\u6001\u53c2\u6570\u5316\u77e5\u8bc6\u800c\u51fa\u73b0\u5e7b\u89c9\uff0c\u5728\u6570\u5b66\u7b49\u4e13\u4e1a\u9886\u57df\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u7814\u7a76\u63a2\u7d22\u901a\u8fc7\u5c06\u68c0\u7d22\u89c6\u4e3a\u52a8\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u6765\u589e\u5f3a\u751f\u6210\u6a21\u578b\u7684\u57fa\u672c\u539f\u5219\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u68c0\u7d22\u589e\u5f3a\u67b6\u6784\uff0c\u8ba9LLM\u4ee3\u7406\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4e3b\u52a8\u51b3\u5b9a\u4f55\u65f6\u67e5\u8be2\u5916\u90e8\u77e5\u8bc6\u5e93\u3002\u5728GSM8K\u548cMATH-500\u57fa\u51c6\u4e0a\u6bd4\u8f83\u81ea\u9002\u5e94\u7b56\u7565\u4e0e\u6807\u51c6CoT\u57fa\u7ebf\u548c\u9759\u6001\u68c0\u7d22\u65b9\u6cd5\u3002", "result": "\u9759\u6001\u68c0\u7d22\u4e0d\u5982CoT\uff1b\u81ea\u9002\u5e94\u68c0\u7d22\u4e2d\uff1a\u5305\u542b\u68c0\u7d22\u7ed3\u679c\u7684\u8f68\u8ff9\u8868\u73b0\u7565\u5dee\u4e8eCoT\uff0c\u4f46\u4e0d\u5305\u542b\u68c0\u7d22\u7684\u8f68\u8ff9\u8868\u73b0\u4f18\u4e8eCoT\u3002\u8fd9\u8868\u660e\u68c0\u7d22\u5f88\u5c11\u5e2e\u52a9\u63a8\u7406\uff0c\u4e3b\u52a8\u4e0d\u4f7f\u7528\u68c0\u7d22\u662f\u6a21\u578b\u6027\u80fd\u826f\u597d\u7684\u4fe1\u53f7\u3002\u6a21\u578b\u4f1a\u6839\u636e\u95ee\u9898\u96be\u5ea6\u8c03\u6574\u68c0\u7d22\u9891\u7387\u3002", "conclusion": "\u6a21\u578b\u81ea\u6211\u8bc4\u4f30\u77e5\u8bc6\u5e76\u9009\u62e9\u6027\u4f7f\u7528\u5916\u90e8\u4fe1\u606f\u7684\u80fd\u529b\u662f\u6784\u5efa\u66f4\u7a33\u5065\u53ef\u9760\u751f\u6210\u6a21\u578b\u7684\u5173\u952e\u539f\u5219\u3002\u68c0\u7d22\u51b3\u7b56\u4f5c\u4e3a\u5143\u8ba4\u77e5\u4fe1\u53f7\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2602.07926", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.07926", "abs": "https://arxiv.org/abs/2602.07926", "authors": ["Xuan-Ming Shen", "Qilong Wu", "Huihui Yu", "Pei-Nan Ni", "Qing Lou", "Chao-Nan Lin", "Xun Yang", "Chong-Xin Shan", "Yuan Zhang"], "title": "Real-Time Magnetic Field Sensing based on Microwave Frequency Modulated Photocurrent of Nitrogen-Vacancy Centers in Diamond", "comment": "11 pages, 4 figures", "summary": "While photoelectric detection of magnetic resonance (PDMR) can be applied to miniaturize nitrogen-vacancy (NV) center-based quantum sensors, the real demonstration of PDMR-based magnetic field sensing remains as a distinctive challenge. To tackle this challenge, in this article, we fabricate diamond samples with electrodes and microwave antenna on the surface, and realize PDMR by detecting photocurrent in nanoampere range via various lock-in amplifying modes. Importantly, we obtain a theoretical and experimental sensitivity 397 nT/Hz and 921 nT/Hz of magnetic field detection in DC-10 Hz range with a laser intensity and microwave frequency modulated mode, respectively, and demonstrate for the first time, a real-time tracking of alternating magnetic field with a standard deviation of 1.5 uT. Furthermore, we investigate systematically the dependence of the PDMR contrast, linewidth and the sensitivity on the laser and microwave power, and find a perfect agreement with a master equation based theoretical model, which accounts for not only the optically induced charge switch of neutral and negative NV centers, but also the interaction with microwave field.", "AI": {"tldr": "\u9996\u6b21\u5b9e\u73b0\u57fa\u4e8e\u5149\u7535\u63a2\u6d4b\u78c1\u5171\u632f(PDMR)\u7684\u5b9e\u65f6\u78c1\u573a\u4f20\u611f\uff0c\u5728DC-10Hz\u8303\u56f4\u5185\u83b7\u5f97921 nT/\u221aHz\u7684\u7075\u654f\u5ea6\uff0c\u5e76\u6210\u529f\u8ffd\u8e2a\u4ea4\u53d8\u78c1\u573a\u3002", "motivation": "\u5149\u7535\u63a2\u6d4b\u78c1\u5171\u632f(PDMR)\u53ef\u7528\u4e8e\u5c0f\u578b\u5316NV\u4e2d\u5fc3\u91cf\u5b50\u4f20\u611f\u5668\uff0c\u4f46\u5b9e\u9645\u5b9e\u73b0PDMR\u78c1\u573a\u4f20\u611f\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5728\u91d1\u521a\u77f3\u8868\u9762\u5236\u5907\u7535\u6781\u548c\u5fae\u6ce2\u5929\u7ebf\uff0c\u901a\u8fc7\u591a\u79cd\u9501\u76f8\u653e\u5927\u6a21\u5f0f\u68c0\u6d4b\u7eb3\u5b89\u7ea7\u5149\u7535\u6d41\u5b9e\u73b0PDMR\uff0c\u91c7\u7528\u6fc0\u5149\u5f3a\u5ea6\u548c\u5fae\u6ce2\u9891\u7387\u8c03\u5236\u6a21\u5f0f\u3002", "result": "\u83b7\u5f97\u7406\u8bba\u7075\u654f\u5ea6397 nT/\u221aHz\u548c\u5b9e\u9a8c\u7075\u654f\u5ea6921 nT/\u221aHz\uff08DC-10Hz\uff09\uff0c\u9996\u6b21\u5b9e\u73b0\u4ea4\u53d8\u78c1\u573a\u5b9e\u65f6\u8ffd\u8e2a\uff08\u6807\u51c6\u5dee1.5 \u03bcT\uff09\uff0cPDMR\u5bf9\u6bd4\u5ea6\u3001\u7ebf\u5bbd\u548c\u7075\u654f\u5ea6\u4e0e\u4e3b\u65b9\u7a0b\u7406\u8bba\u6a21\u578b\u5b8c\u7f8e\u543b\u5408\u3002", "conclusion": "\u6210\u529f\u5b9e\u73b0\u57fa\u4e8ePDMR\u7684\u5b9e\u7528\u78c1\u573a\u4f20\u611f\uff0c\u4e3a\u5c0f\u578b\u5316NV\u4e2d\u5fc3\u91cf\u5b50\u4f20\u611f\u5668\u53d1\u5c55\u5960\u5b9a\u57fa\u7840\uff0c\u7406\u8bba\u4e0e\u5b9e\u9a8c\u7ed3\u679c\u4e00\u81f4\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.07216", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07216", "abs": "https://arxiv.org/abs/2602.07216", "authors": ["Reuben Narad", "L\u00e9onard Boussioux", "Michael Wagner"], "title": "Probing Neural TSP Representations for Prescriptive Decision Support", "comment": "Submitted to ICML 2026", "summary": "The field of neural combinatorial optimization (NCO) trains neural policies to solve NP-hard problems such as the traveling salesperson problem (TSP). We ask whether, beyond producing good tours, a trained TSP solver learns internal representations that transfer to other optimization-relevant objectives, in the spirit of transfer learning from other domains. We train several attention-based TSP policies, collect their internal activations, and train probes on node/edge embeddings for two NP-hard prescriptive downstream tasks inspired by real-world logistics scenarios: node-removal sensitivity (identifying the most impactful node to remove) and edge-forbid sensitivity (identifying the most critical edge to retain). On a Euclidean TSP100-trained model, probes for both tasks are competitive with existing baselines. Ensembling probe signals with geometric features outperforms the strongest baselines: 65\\% top-1 accuracy (vs. 58\\% baseline) for the best-node-removal task, and 73\\% top-1 accuracy (vs. 67\\% baseline) for the worst-edge identification task. To our knowledge, we are the first to study neural TSP solvers as transferable encoders for prescriptive what-if decision-support objectives beyond tour construction. Finally, we show that transfer accuracy increases with solver quality across training and model scale, suggesting that training stronger NCO solvers also yields more useful encoders for downstream objectives. Our code is available at: github.com/ReubenNarad/tsp_prescriptive_probe", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u795e\u7ecf\u7ec4\u5408\u4f18\u5316\u6a21\u578b\u5728\u89e3\u51b3\u65c5\u884c\u5546\u95ee\u9898\u540e\uff0c\u5176\u5185\u90e8\u8868\u5f81\u662f\u5426\u53ef\u8fc1\u79fb\u5230\u5176\u4ed6\u4f18\u5316\u76f8\u5173\u4efb\u52a1\uff0c\u5982\u8282\u70b9\u79fb\u9664\u654f\u611f\u6027\u548c\u8fb9\u4fdd\u7559\u654f\u611f\u6027\u5206\u6790\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u8bad\u7ec3\u597d\u7684TSP\u6c42\u89e3\u5668\u662f\u5426\u80fd\u5b66\u4e60\u5230\u53ef\u8fc1\u79fb\u7684\u5185\u90e8\u8868\u5f81\uff0c\u7528\u4e8e\u5176\u4ed6\u4f18\u5316\u76f8\u5173\u7684\u4e0b\u6e38\u4efb\u52a1\uff0c\u5b9e\u73b0\u7c7b\u4f3c\u5176\u4ed6\u9886\u57df\u7684\u8fc1\u79fb\u5b66\u4e60\u6548\u679c\u3002", "method": "\u8bad\u7ec3\u591a\u4e2a\u57fa\u4e8e\u6ce8\u610f\u529b\u7684TSP\u7b56\u7565\uff0c\u6536\u96c6\u5176\u5185\u90e8\u6fc0\u6d3b\uff0c\u7136\u540e\u8bad\u7ec3\u63a2\u9488\u6a21\u578b\u6765\u9884\u6d4b\u8282\u70b9/\u8fb9\u5d4c\u5165\uff0c\u7528\u4e8e\u4e24\u4e2aNP\u96be\u7684\u4e0b\u6e38\u4efb\u52a1\uff1a\u8282\u70b9\u79fb\u9664\u654f\u611f\u6027\u548c\u8fb9\u7981\u6b62\u654f\u611f\u6027\u5206\u6790\u3002", "result": "\u5728Euclidean TSP100\u8bad\u7ec3\u7684\u6a21\u578b\u4e0a\uff0c\u4e24\u4e2a\u4efb\u52a1\u7684\u63a2\u9488\u6027\u80fd\u4e0e\u73b0\u6709\u57fa\u7ebf\u76f8\u5f53\u3002\u5c06\u63a2\u9488\u4fe1\u53f7\u4e0e\u51e0\u4f55\u7279\u5f81\u96c6\u6210\u540e\uff0c\u8282\u70b9\u79fb\u9664\u4efb\u52a1\u8fbe\u523065% top-1\u51c6\u786e\u7387\uff08\u57fa\u7ebf58%\uff09\uff0c\u8fb9\u8bc6\u522b\u4efb\u52a1\u8fbe\u523073% top-1\u51c6\u786e\u7387\uff08\u57fa\u7ebf67%\uff09\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u7814\u7a76\u795e\u7ecfTSP\u6c42\u89e3\u5668\u4f5c\u4e3a\u53ef\u8fc1\u79fb\u7f16\u7801\u5668\u7528\u4e8e\u8d85\u8d8a\u8def\u5f84\u6784\u5efa\u7684\u9884\u6d4b\u6027\u51b3\u7b56\u652f\u6301\u4efb\u52a1\u3002\u8f6c\u79fb\u51c6\u786e\u7387\u968f\u6c42\u89e3\u5668\u8d28\u91cf\u548c\u6a21\u578b\u89c4\u6a21\u63d0\u9ad8\u800c\u589e\u52a0\uff0c\u8868\u660e\u8bad\u7ec3\u66f4\u5f3a\u7684NCO\u6c42\u89e3\u5668\u4e5f\u80fd\u4ea7\u751f\u66f4\u6709\u7528\u7684\u4e0b\u6e38\u7f16\u7801\u5668\u3002"}}
{"id": "2602.07949", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2602.07949", "abs": "https://arxiv.org/abs/2602.07949", "authors": ["Rakesh Pradhan", "Girish Kulkarni"], "title": "Full Schmidt characterization of spatiotemporally entangled states produced from spontaneous parametric down-conversion", "comment": "6 pages, 3 figures, and supplemental material", "summary": "The full Schmidt decomposition of spatiotemporally entangled states generated from spontaneous parametric down-conversion (SPDC) has not been carried out until now due to the immense computational complexity arising from the large dimensionalities of the states. In this Letter, we utilize the rotational symmetry of the states to reduce the complexity by at least four orders of magnitude and carry out the decomposition to reveal the precise forms of the spatiotemporal Schmidt modes and the Schmidt spectrum spanning over 10^4 modes. We show that the Schmidt modes have a phase profile with a transverse spatial vortex structure that endows them with orbital angular momentum at all frequencies. In the high-gain regime, these Schmidt modes broaden and the Schmidt spectrum narrows with increasing pump strength. Our work can spur novel applications at the intersection of quantum imaging and spectroscopy that utilize entangled states produced from SPDC.", "AI": {"tldr": "\u5229\u7528\u65cb\u8f6c\u5bf9\u79f0\u6027\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86SPDC\u4ea7\u751f\u7684\u65f6\u7a7a\u7ea0\u7f20\u6001\u7684\u5b8c\u6574\u65bd\u5bc6\u7279\u5206\u89e3\uff0c\u63ed\u793a\u4e86\u5177\u6709\u8f68\u9053\u89d2\u52a8\u91cf\u7684\u65bd\u5bc6\u7279\u6a21\u5f0f\u53ca\u5176\u8c31\u5206\u5e03\u3002", "motivation": "\u7531\u4e8e\u8ba1\u7b97\u590d\u6742\u5ea6\u6781\u9ad8\uff0cSPDC\u4ea7\u751f\u7684\u65f6\u7a7a\u7ea0\u7f20\u6001\u7684\u5b8c\u6574\u65bd\u5bc6\u7279\u5206\u89e3\u4e00\u76f4\u672a\u80fd\u5b9e\u73b0\uff0c\u8fd9\u9650\u5236\u4e86\u5bf9\u5176\u91cf\u5b50\u7279\u6027\u7684\u6df1\u5165\u7406\u89e3\u548c\u5e94\u7528\u5f00\u53d1\u3002", "method": "\u5229\u7528\u91cf\u5b50\u6001\u7684\u65cb\u8f6c\u5bf9\u79f0\u6027\uff0c\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u964d\u4f4e\u4e86\u81f3\u5c11\u56db\u4e2a\u6570\u91cf\u7ea7\uff0c\u4ece\u800c\u80fd\u591f\u5bf9\u5305\u542b\u8d85\u8fc710^4\u4e2a\u6a21\u5f0f\u7684\u65f6\u7a7a\u7ea0\u7f20\u6001\u8fdb\u884c\u5b8c\u6574\u7684\u65bd\u5bc6\u7279\u5206\u89e3\u3002", "result": "\u9996\u6b21\u63ed\u793a\u4e86SPDC\u65f6\u7a7a\u7ea0\u7f20\u6001\u7684\u7cbe\u786e\u65bd\u5bc6\u7279\u6a21\u5f0f\u5f62\u5f0f\u53ca\u5176\u8c31\u5206\u5e03\uff0c\u53d1\u73b0\u8fd9\u4e9b\u6a21\u5f0f\u5177\u6709\u6a2a\u5411\u7a7a\u95f4\u6da1\u65cb\u7ed3\u6784\uff0c\u5728\u6240\u6709\u9891\u7387\u4e0a\u90fd\u5e26\u6709\u8f68\u9053\u89d2\u52a8\u91cf\u3002\u5728\u9ad8\u589e\u76ca\u533a\u57df\uff0c\u65bd\u5bc6\u7279\u6a21\u5f0f\u4f1a\u53d8\u5bbd\uff0c\u800c\u65bd\u5bc6\u7279\u8c31\u4f1a\u968f\u7740\u6cf5\u6d66\u5f3a\u5ea6\u7684\u589e\u52a0\u800c\u53d8\u7a84\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u91cf\u5b50\u6210\u50cf\u548c\u5149\u8c31\u5b66\u4ea4\u53c9\u9886\u57df\u7684\u65b0\u578b\u5e94\u7528\u5f00\u8f9f\u4e86\u9053\u8def\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5229\u7528SPDC\u4ea7\u751f\u7684\u7ea0\u7f20\u6001\u3002"}}
{"id": "2602.07218", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07218", "abs": "https://arxiv.org/abs/2602.07218", "authors": ["Gagik Magakyan", "Amirhossein Reisizadeh", "Chanwoo Park", "Pablo A. Parrilo", "Asuman Ozdaglar"], "title": "Collaborative and Efficient Fine-tuning: Leveraging Task Similarity", "comment": null, "summary": "Adaptability has been regarded as a central feature in the foundation models, enabling them to effectively acclimate to unseen downstream tasks. Parameter-efficient fine-tuning methods such as celebrated LoRA facilitate efficient adaptation of large foundation models using labeled, high-quality and generally scarce task data. To mitigate data scarcity in fine-tuning of foundation models, we propose to leverage task similarity across multiple downstream users. Intuitively, users with similar tasks must be able to assist each other in boosting the effective fine-tuning data size. We propose Collaborative Low-Rank Adaptation, or CoLoRA, which exploits task similarity to collaboratively and efficiently fine-tune personalized foundation models. The main idea in CoLoRA is to train one shared adapter capturing underlying task similarities across all tasks, and personalized adapters tailored to user-specific tasks. We theoretically study CoLoRA on heterogeneous linear regression and provide provable guarantees for ground truth recovery. We also conduct several natural language experiments with varying task similarity, which further demonstrate that when trained together with similar tasks, individual performances are significantly boosted.", "AI": {"tldr": "CoLoRA\uff1a\u5229\u7528\u4efb\u52a1\u76f8\u4f3c\u6027\u8fdb\u884c\u534f\u4f5c\u5f0f\u4f4e\u79e9\u9002\u914d\uff0c\u901a\u8fc7\u5171\u4eab\u9002\u914d\u5668\u6355\u83b7\u4efb\u52a1\u5171\u6027\uff0c\u4e2a\u6027\u5316\u9002\u914d\u5668\u5904\u7406\u7528\u6237\u7279\u5b9a\u4efb\u52a1\uff0c\u89e3\u51b3\u57fa\u7840\u6a21\u578b\u5fae\u8c03\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5fae\u8c03\u901a\u5e38\u9762\u4e34\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002\u4f5c\u8005\u89c2\u5bdf\u5230\u4e0d\u540c\u7528\u6237\u7684\u4efb\u52a1\u4e4b\u95f4\u5b58\u5728\u76f8\u4f3c\u6027\uff0c\u53ef\u4ee5\u5229\u7528\u8fd9\u79cd\u76f8\u4f3c\u6027\u6765\u589e\u52a0\u6709\u6548\u7684\u5fae\u8c03\u6570\u636e\u91cf\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u534f\u4f5c\u5f0f\u4f4e\u79e9\u9002\u914d\uff08CoLoRA\uff09\uff0c\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1a1\uff09\u5171\u4eab\u9002\u914d\u5668\u6355\u83b7\u6240\u6709\u4efb\u52a1\u7684\u5171\u6027\u7279\u5f81\uff1b2\uff09\u4e2a\u6027\u5316\u9002\u914d\u5668\u9488\u5bf9\u6bcf\u4e2a\u7528\u6237\u7684\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u5b9a\u5236\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u4efb\u52a1\u76f8\u4f3c\u6027\u7684\u534f\u4f5c\u4f18\u52bf\u3002", "result": "\u5728\u5f02\u6784\u7ebf\u6027\u56de\u5f52\u95ee\u9898\u4e0a\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u8bc1\u660e\u80fd\u591f\u6062\u590d\u771f\u5b9e\u53c2\u6570\u3002\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5b9e\u9a8c\u4e2d\uff0c\u5f53\u4e0e\u76f8\u4f3c\u4efb\u52a1\u4e00\u8d77\u8bad\u7ec3\u65f6\uff0c\u4e2a\u4f53\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u4efb\u52a1\u76f8\u4f3c\u6027\u7684\u534f\u4f5c\u6548\u76ca\u3002", "conclusion": "CoLoRA\u901a\u8fc7\u5229\u7528\u4efb\u52a1\u76f8\u4f3c\u6027\u8fdb\u884c\u534f\u4f5c\u5f0f\u5fae\u8c03\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u57fa\u7840\u6a21\u578b\u5fae\u8c03\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\uff0c\u4e3a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.07952", "categories": ["quant-ph", "cond-mat.stat-mech", "hep-th"], "pdf": "https://arxiv.org/pdf/2602.07952", "abs": "https://arxiv.org/abs/2602.07952", "authors": ["Tingfei Li", "Miao Wang", "Jianghui Yu"], "title": "Higher-Order Corrections to Scrambling Dynamics in Brownian Spin SYK Models", "comment": "29 pages, 4 figures", "summary": "We investigate operator growth in a Brownian spin Sachdev--Ye--Kitaev (SYK) model with random all-to-all interactions, focusing on the full operator-size distribution. For Hamiltonians containing interactions of order two up to $L$, we derive a closed master equation for the Pauli-string expansion coefficients and recast their dynamics into a generating-function formulation suitable for the large-$N$ limit. This approach allows us to diagonalize the leading-order evolution operator explicitly and obtain exact solutions for arbitrary initial operator distributions, including the effects of decoherence. Going beyond leading order, we develop a systematic $1/N$ expansion that captures higher-order corrections to the operator-size dynamics and the late-time behavior. Our results demonstrate that higher-order effects play a crucial role in operator scrambling and that the full operator-size distribution provides a more refined probe of quantum chaos in Brownian and open quantum systems.", "AI": {"tldr": "\u7814\u7a76\u5e03\u6717\u8fd0\u52a8\u81ea\u65cbSYK\u6a21\u578b\u4e2d\u7b97\u5b50\u589e\u957f\uff0c\u5173\u6ce8\u5b8c\u6574\u7b97\u5b50\u5c3a\u5bf8\u5206\u5e03\uff0c\u63a8\u5bfc\u4e3b\u65b9\u7a0b\u548c\u751f\u6210\u51fd\u6570\u65b9\u6cd5\uff0c\u83b7\u5f97\u7cbe\u786e\u89e3\u5e76\u53d1\u5c551/N\u5c55\u5f00\uff0c\u63ed\u793a\u9ad8\u9636\u6548\u5e94\u5bf9\u91cf\u5b50\u6df7\u6c8c\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7814\u7a76\u5e03\u6717\u8fd0\u52a8SYK\u6a21\u578b\u4e2d\u7b97\u5b50\u589e\u957f\u548c\u5b8c\u6574\u7b97\u5b50\u5c3a\u5bf8\u5206\u5e03\uff0c\u4ee5\u66f4\u7cbe\u7ec6\u5730\u63a2\u6d4b\u91cf\u5b50\u6df7\u6c8c\uff0c\u7279\u522b\u662f\u5728\u5e03\u6717\u8fd0\u52a8\u548c\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u4e2d\u3002", "method": "\u63a8\u5bfcPauli\u5f26\u5c55\u5f00\u7cfb\u6570\u7684\u4e3b\u65b9\u7a0b\uff0c\u8f6c\u5316\u4e3a\u751f\u6210\u51fd\u6570\u8868\u8ff0\uff0c\u5bf9\u89d2\u5316\u4e3b\u5bfc\u9636\u6f14\u5316\u7b97\u5b50\u83b7\u5f97\u7cbe\u786e\u89e3\uff0c\u53d1\u5c55\u7cfb\u7edf\u6027\u76841/N\u5c55\u5f00\u6355\u83b7\u9ad8\u9636\u4fee\u6b63\u3002", "result": "\u83b7\u5f97\u4efb\u610f\u521d\u59cb\u7b97\u5b50\u5206\u5e03\u7684\u7cbe\u786e\u89e3\uff0c\u5305\u62ec\u9000\u76f8\u5e72\u6548\u5e94\uff0c\u53d1\u73b0\u9ad8\u9636\u6548\u5e94\u5bf9\u7b97\u5b50\u6df7\u6d17\u81f3\u5173\u91cd\u8981\uff0c\u5b8c\u6574\u7b97\u5b50\u5c3a\u5bf8\u5206\u5e03\u662f\u91cf\u5b50\u6df7\u6c8c\u7684\u66f4\u7cbe\u7ec6\u63a2\u9488\u3002", "conclusion": "\u5b8c\u6574\u7b97\u5b50\u5c3a\u5bf8\u5206\u5e03\u63d0\u4f9b\u4e86\u91cf\u5b50\u6df7\u6c8c\u7684\u66f4\u7cbe\u7ec6\u63a2\u6d4b\uff0c\u9ad8\u9636\u6548\u5e94\u5bf9\u7b97\u5b50\u6df7\u6d17\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u65b9\u6cd5\u9002\u7528\u4e8e\u5e03\u6717\u8fd0\u52a8\u548c\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u3002"}}
{"id": "2602.07219", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07219", "abs": "https://arxiv.org/abs/2602.07219", "authors": ["Abhigyan Dutta", "Itay Safran", "Paul Valiant"], "title": "The Median is Easier than it Looks: Approximation with a Constant-Depth, Linear-Width ReLU Network", "comment": null, "summary": "We study the approximation of the median of $d$ inputs using ReLU neural networks. We present depth-width tradeoffs under several settings, culminating in a constant-depth, linear-width construction that achieves exponentially small approximation error with respect to the uniform distribution over the unit hypercube. By further establishing a general reduction from the maximum to the median, our results break a barrier suggested by prior work on the maximum function, which indicated that linear width should require depth growing at least as $\\log\\log d$ to achieve comparable accuracy. Our construction relies on a multi-stage procedure that iteratively eliminates non-central elements while preserving a candidate set around the median. We overcome obstacles that do not arise for the maximum to yield approximation results that are strictly stronger than those previously known for the maximum itself.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4f7f\u7528ReLU\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3cd\u4e2a\u8f93\u5165\u7684median\u51fd\u6570\uff0c\u63d0\u51fa\u4e86\u6df1\u5ea6-\u5bbd\u5ea6\u6743\u8861\uff0c\u6700\u7ec8\u6784\u5efa\u4e86\u5e38\u6570\u6df1\u5ea6\u3001\u7ebf\u6027\u5bbd\u5ea6\u7684\u7f51\u7edc\uff0c\u5728\u5355\u4f4d\u8d85\u7acb\u65b9\u4f53\u5747\u5300\u5206\u5e03\u4e0a\u5b9e\u73b0\u6307\u6570\u7ea7\u5c0f\u7684\u8fd1\u4f3c\u8bef\u5dee\u3002", "motivation": "\u5148\u524d\u5173\u4e8emaximum\u51fd\u6570\u7684\u7814\u7a76\u8868\u660e\uff0c\u8981\u8fbe\u5230\u53ef\u6bd4\u7cbe\u5ea6\uff0c\u7ebf\u6027\u5bbd\u5ea6\u9700\u8981\u6df1\u5ea6\u81f3\u5c11\u589e\u957f\u4e3alog log d\u3002\u672c\u6587\u65e8\u5728\u7a81\u7834\u8fd9\u4e00\u969c\u788d\uff0c\u7814\u7a76median\u51fd\u6570\u7684\u8fd1\u4f3c\uff0c\u5e76\u5efa\u7acb\u4ecemaximum\u5230median\u7684\u4e00\u822c\u5f52\u7ea6\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u8fed\u4ee3\u8fc7\u7a0b\uff0c\u9010\u6b65\u6d88\u9664\u975e\u4e2d\u5fc3\u5143\u7d20\uff0c\u540c\u65f6\u4fdd\u7559\u4e2d\u4f4d\u6570\u5468\u56f4\u7684\u5019\u9009\u96c6\u3002\u514b\u670d\u4e86maximum\u51fd\u6570\u8fd1\u4f3c\u4e2d\u4e0d\u5b58\u5728\u7684\u969c\u788d\u3002", "result": "\u6784\u5efa\u4e86\u5e38\u6570\u6df1\u5ea6\u3001\u7ebf\u6027\u5bbd\u5ea6\u7684ReLU\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u5355\u4f4d\u8d85\u7acb\u65b9\u4f53\u5747\u5300\u5206\u5e03\u4e0a\u5b9e\u73b0\u6307\u6570\u7ea7\u5c0f\u7684\u8fd1\u4f3c\u8bef\u5dee\u3002\u7ed3\u679c\u6bd4\u5148\u524d\u5df2\u77e5\u7684maximum\u51fd\u6570\u8fd1\u4f3c\u7ed3\u679c\u66f4\u5f3a\u3002", "conclusion": "\u672c\u6587\u7a81\u7834\u4e86\u5148\u524d\u5173\u4e8emaximum\u51fd\u6570\u8fd1\u4f3c\u6df1\u5ea6-\u5bbd\u5ea6\u6743\u8861\u7684\u969c\u788d\uff0c\u901a\u8fc7\u7814\u7a76median\u51fd\u6570\u5e76\u5efa\u7acb\u4ecemaximum\u5230median\u7684\u5f52\u7ea6\uff0c\u83b7\u5f97\u4e86\u6bd4maximum\u51fd\u6570\u672c\u8eab\u66f4\u5f3a\u7684\u8fd1\u4f3c\u7ed3\u679c\u3002"}}
{"id": "2602.07956", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.07956", "abs": "https://arxiv.org/abs/2602.07956", "authors": ["Sergio Giardino"], "title": "Quantum self-interaction within an infinitely deep cavity", "comment": "accept by Phys. Lett. A", "summary": "One examines the infinitely deep quantum cavity, also known as the quantum infinite square well, within the framework of the real Hilbert space. The solutions are considered in terms of complex wave functions, and also in terms of quaternionic wave functions. The complex results reproduce the usual achievements established in the complex Hilbert space, but also extend them to non-stationary solutions, as well as to distorted stationary solutions, different energy spectra, and dislocated observed position. The quaternionic cases further admit the incidence of self-interaction, something that cannot be observed in complex solutions. Therefore, both the complex and quaternionic solutions are more general than previous cases, thus opening the way to further one-dimensional solutions to be researched in the non-relativistic theory.", "AI": {"tldr": "\u5728\u5b9e\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u7814\u7a76\u65e0\u9650\u6df1\u91cf\u5b50\u8154\uff08\u91cf\u5b50\u65e0\u9650\u65b9\u52bf\u9631\uff09\uff0c\u8003\u8651\u590d\u6ce2\u51fd\u6570\u548c\u56db\u5143\u6570\u6ce2\u51fd\u6570\u89e3\uff0c\u53d1\u73b0\u6bd4\u4f20\u7edf\u590d\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u89e3\u66f4\u4e00\u822c\uff0c\u5305\u542b\u975e\u5b9a\u6001\u89e3\u3001\u7578\u53d8\u5b9a\u6001\u89e3\u3001\u4e0d\u540c\u80fd\u8c31\u3001\u4f4d\u7f6e\u4f4d\u79fb\uff0c\u56db\u5143\u6570\u89e3\u8fd8\u5141\u8bb8\u81ea\u76f8\u4e92\u4f5c\u7528\u3002", "motivation": "\u5728\u5b9e\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u6846\u67b6\u4e0b\u91cd\u65b0\u5ba1\u89c6\u65e0\u9650\u6df1\u91cf\u5b50\u8154\u95ee\u9898\uff0c\u63a2\u7d22\u590d\u6ce2\u51fd\u6570\u548c\u56db\u5143\u6570\u6ce2\u51fd\u6570\u7684\u89e3\uff0c\u4ee5\u53d1\u73b0\u6bd4\u4f20\u7edf\u590d\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u66f4\u4e00\u822c\u7684\u91cf\u5b50\u529b\u5b66\u89e3\u3002", "method": "\u5728\u5b9e\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u7814\u7a76\u65e0\u9650\u6df1\u91cf\u5b50\u8154\uff08\u91cf\u5b50\u65e0\u9650\u65b9\u52bf\u9631\uff09\uff0c\u5206\u522b\u8003\u8651\u590d\u6ce2\u51fd\u6570\u548c\u56db\u5143\u6570\u6ce2\u51fd\u6570\u4f5c\u4e3a\u89e3\uff0c\u5206\u6790\u5176\u6027\u8d28\u5e76\u4e0e\u4f20\u7edf\u590d\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u7684\u89e3\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u590d\u6ce2\u51fd\u6570\u89e3\u4e0d\u4ec5\u91cd\u73b0\u4e86\u4f20\u7edf\u590d\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7684\u7ed3\u679c\uff0c\u8fd8\u6269\u5c55\u5230\u4e86\u975e\u5b9a\u6001\u89e3\u3001\u7578\u53d8\u5b9a\u6001\u89e3\u3001\u4e0d\u540c\u80fd\u8c31\u548c\u89c2\u6d4b\u4f4d\u7f6e\u4f4d\u79fb\u3002\u56db\u5143\u6570\u6ce2\u51fd\u6570\u89e3\u8fdb\u4e00\u6b65\u5141\u8bb8\u81ea\u76f8\u4e92\u4f5c\u7528\u73b0\u8c61\uff0c\u8fd9\u662f\u590d\u89e3\u4e2d\u65e0\u6cd5\u89c2\u5bdf\u5230\u7684\u3002", "conclusion": "\u5b9e\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u7684\u590d\u6ce2\u51fd\u6570\u548c\u56db\u5143\u6570\u6ce2\u51fd\u6570\u89e3\u6bd4\u4f20\u7edf\u91cf\u5b50\u529b\u5b66\u89e3\u66f4\u4e00\u822c\uff0c\u4e3a\u7814\u7a76\u975e\u76f8\u5bf9\u8bba\u7406\u8bba\u4e2d\u7684\u4e00\u7ef4\u89e3\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.07223", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07223", "abs": "https://arxiv.org/abs/2602.07223", "authors": ["Yikang Yue", "Yuqi Xue", "Jian Huang"], "title": "SpecAttn: Co-Designing Sparse Attention with Self-Speculative Decoding", "comment": null, "summary": "Long-context large language model (LLM) inference has become the norm for today's AI applications. However, it is severely bottlenecked by the increasing memory demands of its KV cache. Previous works have shown that self-speculative decoding with sparse attention, where tokens are drafted using a subset of the KV cache and verified in parallel with full KV cache, speeds up inference in a lossless way. However, this approach relies on standalone KV selection algorithms to select the KV entries used for drafting and overlooks that the criticality of each KV entry is inherently computed during verification. In this paper, we propose SpecAttn, a self-speculative decoding method with verification-guided sparse attention. SpecAttn identifies critical KV entries as a byproduct of verification and only loads these entries when drafting subsequent tokens. This not only improves draft token acceptance rate but also incurs low KV selection overhead, thereby improving decoding throughput. SpecAttn achieves 2.81$\\times$ higher throughput over vanilla auto-regressive decoding and 1.29$\\times$ improvement over state-of-the-art sparsity-based self-speculative decoding methods.", "AI": {"tldr": "SpecAttn\u662f\u4e00\u79cd\u81ea\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u9a8c\u8bc1\u5f15\u5bfc\u7684\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5229\u7528\u9a8c\u8bc1\u8fc7\u7a0b\u4e2d\u8bc6\u522b\u7684\u5173\u952eKV\u6761\u76ee\u6765\u751f\u6210\u540e\u7eedtoken\uff0c\u663e\u8457\u63d0\u9ad8\u89e3\u7801\u541e\u5410\u91cf\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u4e2d\u7684KV\u7f13\u5b58\u5185\u5b58\u9700\u6c42\u6210\u4e3a\u74f6\u9888\u3002\u73b0\u6709\u81ea\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u4f9d\u8d56\u72ec\u7acb\u7684KV\u9009\u62e9\u7b97\u6cd5\uff0c\u5ffd\u7565\u4e86\u9a8c\u8bc1\u8fc7\u7a0b\u4e2d\u5df2\u8ba1\u7b97\u51fa\u7684KV\u6761\u76ee\u91cd\u8981\u6027\u4fe1\u606f\u3002", "method": "\u63d0\u51faSpecAttn\u65b9\u6cd5\uff0c\u5c06\u9a8c\u8bc1\u8fc7\u7a0b\u4f5c\u4e3a\u526f\u4ea7\u54c1\u8bc6\u522b\u5173\u952eKV\u6761\u76ee\uff0c\u4ec5\u52a0\u8f7d\u8fd9\u4e9b\u5173\u952e\u6761\u76ee\u7528\u4e8e\u540e\u7eedtoken\u7684\u751f\u6210\uff0c\u5b9e\u73b0\u9a8c\u8bc1\u5f15\u5bfc\u7684\u7a00\u758f\u6ce8\u610f\u529b\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf\u81ea\u56de\u5f52\u89e3\u7801\uff0c\u541e\u5410\u91cf\u63d0\u9ad82.81\u500d\uff1b\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u7a00\u758f\u6027\u7684\u81ea\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u541e\u5410\u91cf\u63d0\u53471.29\u500d\u3002", "conclusion": "SpecAttn\u901a\u8fc7\u91cd\u7528\u9a8c\u8bc1\u8fc7\u7a0b\u4e2d\u8ba1\u7b97\u7684\u5173\u952eKV\u4fe1\u606f\uff0c\u65e2\u63d0\u9ad8\u4e86token\u63a5\u53d7\u7387\u53c8\u964d\u4f4e\u4e86KV\u9009\u62e9\u5f00\u9500\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2602.07971", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.07971", "abs": "https://arxiv.org/abs/2602.07971", "authors": ["Huan-Chao Gao", "Guo-Zhu Song", "Hai-Rui Wei"], "title": "Improved entanglement-based high-dimensional optical quantum computation with linear optics", "comment": "13 pages, 6 figures, 2 tables", "summary": "Quantum gates are the essential block for quantum computer. High-dimensional quantum gates exhibit remarkable advantages over their two-dimensional counterparts for some quantum information processing tasks. Here we present a family of entanglement-based optical controlled-SWAP gates on $\\mathbb{C}^{2}\\otimes \\mathbb{C}^{d}\\otimes \\mathbb{C}^{d}$. With the hybrid encoding, we encode the control qubits and target qudits in photonic polarization and spatial degrees of freedom, respectively. The circuit is constructed using only $(2+3d)$ ($d\\geq 2$) linear optics, beating an earlier result of 14 linear optics with $d=2$. The circuit depth 5 is much lower than an earlier result of 11 with $d=2$. Besides, the fidelity of the presented circuit can reach 99.4\\%, and it is higher than the previous counterpart with $d=2$. Our scheme are constructed in a deterministic way without any borrowed ancillary photons or measurement-induced nonlinearities. Moreover, our approach allows $d>2$.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ea0\u7f20\u7684\u5149\u5b66\u53d7\u63a7SWAP\u95e8\uff0c\u5728C\u00b2\u2297C\u1d48\u2297C\u1d48\u7a7a\u95f4\u4e0a\u5b9e\u73b0\uff0c\u4f7f\u7528\u504f\u632f\u548c\u7a7a\u95f4\u81ea\u7531\u5ea6\u6df7\u5408\u7f16\u7801\uff0c\u4ec5\u9700(2+3d)\u4e2a\u7ebf\u6027\u5149\u5b66\u5143\u4ef6\uff0c\u7535\u8def\u6df1\u5ea6\u4e3a5\uff0c\u4fdd\u771f\u5ea6\u8fbe99.4%\uff0c\u4e14\u65e0\u9700\u8f85\u52a9\u5149\u5b50\u6216\u6d4b\u91cf\u8bf1\u5bfc\u975e\u7ebf\u6027\u3002", "motivation": "\u9ad8\u7ef4\u91cf\u5b50\u95e8\u5728\u67d0\u4e9b\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u4efb\u52a1\u4e2d\u6bd4\u4e8c\u7ef4\u91cf\u5b50\u95e8\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u65b9\u6848\u9700\u8981\u66f4\u591a\u5149\u5b66\u5143\u4ef6\u548c\u66f4\u6df1\u7684\u7535\u8def\u6df1\u5ea6\uff0c\u4e14\u901a\u5e38\u5c40\u9650\u4e8ed=2\u7684\u60c5\u51b5\u3002", "method": "\u91c7\u7528\u6df7\u5408\u7f16\u7801\u65b9\u6848\uff1a\u63a7\u5236\u91cf\u5b50\u6bd4\u7279\u7f16\u7801\u5728\u5149\u5b50\u504f\u632f\u81ea\u7531\u5ea6\uff0c\u76ee\u6807\u91cf\u5b50\u6bd4\u7279\u7f16\u7801\u5728\u7a7a\u95f4\u81ea\u7531\u5ea6\u3002\u6784\u5efa\u4ec5\u9700(2+3d)\u4e2a\u7ebf\u6027\u5149\u5b66\u5143\u4ef6\u7684\u7535\u8def\uff0c\u7535\u8def\u6df1\u5ea6\u4e3a5\uff0c\u65e0\u9700\u501f\u7528\u8f85\u52a9\u5149\u5b50\u6216\u6d4b\u91cf\u8bf1\u5bfc\u975e\u7ebf\u6027\u3002", "result": "\u5f53d=2\u65f6\uff0c\u4ec5\u97008\u4e2a\u7ebf\u6027\u5149\u5b66\u5143\u4ef6\uff08\u5148\u524d\u65b9\u6848\u970014\u4e2a\uff09\uff0c\u7535\u8def\u6df1\u5ea65\uff08\u5148\u524d\u4e3a11\uff09\uff0c\u4fdd\u771f\u5ea699.4%\u9ad8\u4e8e\u5148\u524d\u65b9\u6848\u3002\u65b9\u6848\u652f\u6301d>2\u7684\u60c5\u51b5\uff0c\u4e14\u4e3a\u786e\u5b9a\u6027\u5b9e\u73b0\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9ad8\u4fdd\u771f\u5ea6\u7684\u5149\u5b66\u53d7\u63a7SWAP\u95e8\u5b9e\u73b0\u65b9\u6848\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6240\u9700\u5149\u5b66\u5143\u4ef6\u6570\u91cf\u548c\u7535\u8def\u6df1\u5ea6\uff0c\u652f\u6301\u9ad8\u7ef4\u6269\u5c55\uff0c\u4e3a\u9ad8\u7ef4\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2602.07226", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07226", "abs": "https://arxiv.org/abs/2602.07226", "authors": ["Zihan Zhu", "Yanqiu Wu", "Qiongkai Xu"], "title": "Fault-Tolerant Evaluation for Sample-Efficient Model Performance Estimators", "comment": null, "summary": "In the era of Model-as-a-Service, organizations increasingly rely on third-party AI models for rapid deployment. However, the dynamic nature of emerging AI applications, the continual introduction of new datasets, and the growing number of models claiming superior performance make efficient and reliable validation of model services increasingly challenging. This motivates the development of sample-efficient performance estimators, which aim to estimate model performance by strategically selecting instances for labeling, thereby reducing annotation cost. Yet existing evaluation approaches often fail in low-variance settings: RMSE conflates bias and variance, masking persistent bias when variance is small, while p-value based tests become hypersensitive, rejecting adequate estimators for negligible deviations. To address this, we propose a fault-tolerant evaluation framework that integrates bias and variance considerations within an adjustable tolerance level ${\\varepsilon}$, enabling the evaluation of performance estimators within practically acceptable error margins. We theoretically show that proper calibration of ${\\varepsilon}$ ensures reliable evaluation across different variance regimes, and we further propose an algorithm that automatically optimizes and selects ${\\varepsilon}$. Experiments on real-world datasets demonstrate that our framework provides comprehensive and actionable insights into estimator behavior.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5bb9\u9519\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u53ef\u8c03\u8282\u7684\u5bb9\u5fcd\u5ea6\u03b5\u6765\u6574\u5408\u504f\u5dee\u548c\u65b9\u5dee\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u4f4e\u65b9\u5dee\u573a\u666f\u4e0b\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u6a21\u578b\u5373\u670d\u52a1\u65f6\u4ee3\uff0c\u7ec4\u7ec7\u4f9d\u8d56\u7b2c\u4e09\u65b9AI\u6a21\u578b\u5feb\u901f\u90e8\u7f72\uff0c\u4f46\u65b0\u5174AI\u5e94\u7528\u7684\u52a8\u6001\u6027\u3001\u65b0\u6570\u636e\u96c6\u7684\u4e0d\u65ad\u5f15\u5165\u4ee5\u53ca\u58f0\u79f0\u4f18\u8d8a\u6027\u80fd\u7684\u6a21\u578b\u589e\u591a\uff0c\u4f7f\u5f97\u6a21\u578b\u670d\u52a1\u7684\u6709\u6548\u53ef\u9760\u9a8c\u8bc1\u53d8\u5f97\u65e5\u76ca\u56f0\u96be\u3002\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5728\u4f4e\u65b9\u5dee\u573a\u666f\u4e0b\u5b58\u5728\u95ee\u9898\uff1aRMSE\u6df7\u6dc6\u4e86\u504f\u5dee\u548c\u65b9\u5dee\uff0c\u5f53\u65b9\u5dee\u5c0f\u65f6\u63a9\u76d6\u4e86\u6301\u7eed\u504f\u5dee\uff1b\u800c\u57fa\u4e8ep\u503c\u7684\u68c0\u9a8c\u53d8\u5f97\u8fc7\u4e8e\u654f\u611f\uff0c\u56e0\u5fae\u5c0f\u504f\u5dee\u5c31\u62d2\u7edd\u8db3\u591f\u7684\u4f30\u8ba1\u5668\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5bb9\u9519\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u504f\u5dee\u548c\u65b9\u5dee\u8003\u8651\u6574\u5408\u5230\u53ef\u8c03\u8282\u7684\u5bb9\u5fcd\u5ea6\u03b5\u4e2d\uff0c\u5141\u8bb8\u5728\u5b9e\u9645\u53ef\u63a5\u53d7\u7684\u8bef\u5dee\u8303\u56f4\u5185\u8bc4\u4f30\u6027\u80fd\u4f30\u8ba1\u5668\u3002\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u9002\u5f53\u6821\u51c6\u03b5\u53ef\u4ee5\u786e\u4fdd\u5728\u4e0d\u540c\u65b9\u5dee\u673a\u5236\u4e0b\u7684\u53ef\u9760\u8bc4\u4f30\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u81ea\u52a8\u4f18\u5316\u548c\u9009\u62e9\u03b5\u7684\u7b97\u6cd5\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u5bf9\u4f30\u8ba1\u5668\u884c\u4e3a\u7684\u5168\u9762\u4e14\u53ef\u64cd\u4f5c\u7684\u6d1e\u5bdf\u3002", "conclusion": "\u63d0\u51fa\u7684\u5bb9\u9519\u8bc4\u4f30\u6846\u67b6\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4f4e\u65b9\u5dee\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u6574\u5408\u504f\u5dee\u548c\u65b9\u5dee\u8003\u8651\u5e76\u5f15\u5165\u53ef\u8c03\u8282\u5bb9\u5fcd\u5ea6\uff0c\u4e3a\u6a21\u578b\u6027\u80fd\u4f30\u8ba1\u5668\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u5b9e\u7528\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.08114", "categories": ["quant-ph", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2602.08114", "abs": "https://arxiv.org/abs/2602.08114", "authors": ["Yanbao Zhang", "Akshay Seshadri", "Emanuel Knill"], "title": "An efficient method for spot-checking quantum properties with sequential trials", "comment": "42 pages, 5 figures", "summary": "In practical situations, the reliability of quantum resources can be compromised due to complex generation processes or adversarial manipulations during transmission. Consequently, the trials generated sequentially in an experiment may exhibit non-independent and non-identically distributed (non-i.i.d.) behavior. This non-i.i.d. behavior can introduce security concerns and result in faulty estimates when performing information tasks such as quantum key distribution, self-testing, verifiable quantum computation, and resource allocation in quantum networks. To certify the performance of such tasks, one can make a random decision in each trial, either spot-checking some desired property or utilizing the quantum resource for the given task. However, a general method for certification with a sequence of non-i.i.d. spot-checking trials is still missing. Here, we develop such a method. This method not only works efficiently with a finite number of trials but also yields asymptotically tight certificates of performance. Our analysis shows that even as the total number of trials approaches infinity, only a constant number of trials needs to be spot-checked on average to certify the average performance of the remaining trials at a specified confidence level.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u8ba4\u8bc1\u975e\u72ec\u7acb\u540c\u5206\u5e03\u91cf\u5b50\u8bd5\u9a8c\u6027\u80fd\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u8bd5\u9a8c\u6b21\u6570\u4e0b\u9ad8\u6548\u5de5\u4f5c\uff0c\u5e76\u80fd\u63d0\u4f9b\u6e10\u8fd1\u7d27\u81f4\u7684\u6027\u80fd\u8ba4\u8bc1\u3002", "motivation": "\u91cf\u5b50\u8d44\u6e90\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u80fd\u56e0\u590d\u6742\u751f\u6210\u8fc7\u7a0b\u6216\u4f20\u8f93\u4e2d\u7684\u5bf9\u6297\u64cd\u4f5c\u800c\u4e0d\u53ef\u9760\uff0c\u5bfc\u81f4\u5b9e\u9a8c\u4e2d\u7684\u8bd5\u9a8c\u5e8f\u5217\u5448\u73b0\u975e\u72ec\u7acb\u540c\u5206\u5e03\u884c\u4e3a\uff0c\u8fd9\u4f1a\u7ed9\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\u3001\u81ea\u6d4b\u8bd5\u3001\u53ef\u9a8c\u8bc1\u91cf\u5b50\u8ba1\u7b97\u7b49\u4efb\u52a1\u5e26\u6765\u5b89\u5168\u9690\u60a3\u548c\u9519\u8bef\u4f30\u8ba1\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u901a\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6bcf\u4e2a\u8bd5\u9a8c\u4e2d\u968f\u673a\u51b3\u5b9a\u662f\u8fdb\u884c\u62bd\u67e5\uff08spot-checking\uff09\u8fd8\u662f\u4f7f\u7528\u91cf\u5b50\u8d44\u6e90\u6267\u884c\u4efb\u52a1\uff0c\u6765\u8ba4\u8bc1\u975e\u72ec\u7acb\u540c\u5206\u5e03\u62bd\u67e5\u8bd5\u9a8c\u5e8f\u5217\u7684\u6027\u80fd\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u5728\u6709\u9650\u8bd5\u9a8c\u6b21\u6570\u4e0b\u9ad8\u6548\u5de5\u4f5c\uff0c\u8fd8\u80fd\u63d0\u4f9b\u6e10\u8fd1\u7d27\u81f4\u7684\u6027\u80fd\u8ba4\u8bc1\u3002\u5206\u6790\u8868\u660e\uff0c\u5373\u4f7f\u8bd5\u9a8c\u603b\u6570\u8d8b\u4e8e\u65e0\u7a77\u5927\uff0c\u4e5f\u53ea\u9700\u62bd\u67e5\u6052\u5b9a\u6570\u91cf\u7684\u8bd5\u9a8c\u5373\u53ef\u5728\u6307\u5b9a\u7f6e\u4fe1\u6c34\u5e73\u4e0b\u8ba4\u8bc1\u5269\u4f59\u8bd5\u9a8c\u7684\u5e73\u5747\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u975e\u72ec\u7acb\u540c\u5206\u5e03\u62bd\u67e5\u8bd5\u9a8c\u5e8f\u5217\u8ba4\u8bc1\u65b9\u6cd5\u7684\u7a7a\u767d\uff0c\u4e3a\u91cf\u5b50\u4fe1\u606f\u4efb\u52a1\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u4fdd\u969c\u3002"}}
{"id": "2602.07227", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07227", "abs": "https://arxiv.org/abs/2602.07227", "authors": ["Nethmi Jayasinghe", "Diana Gontero", "Spencer T. Brown", "Vinod K. Sangwan", "Mark C. Hersam", "Amit Ranjan Trivedi"], "title": "Cerebellar-Inspired Residual Control for Fault Recovery: From Inference-Time Adaptation to Structural Consolidation", "comment": null, "summary": "Robotic policies deployed in real-world environments often encounter post-training faults, where retraining, exploration, or system identification are impractical. We introduce an inference-time, cerebellar-inspired residual control framework that augments a frozen reinforcement learning policy with online corrective actions, enabling fault recovery without modifying base policy parameters. The framework instantiates core cerebellar principles, including high-dimensional pattern separation via fixed feature expansion, parallel microzone-style residual pathways, and local error-driven plasticity with excitatory and inhibitory eligibility traces operating at distinct time scales. These mechanisms enable fast, localized correction under post-training disturbances while avoiding destabilizing global policy updates. A conservative, performance-driven meta-adaptation regulates residual authority and plasticity, preserving nominal behavior and suppressing unnecessary intervention. Experiments on MuJoCo benchmarks under actuator, dynamic, and environmental perturbations show improvements of up to $+66\\%$ on \\texttt{HalfCheetah-v5} and $+53\\%$ on \\texttt{Humanoid-v5} under moderate faults, with graceful degradation under severe shifts and complementary robustness from consolidating persistent residual corrections into policy parameters.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53d7\u5c0f\u8111\u542f\u53d1\u7684\u63a8\u7406\u65f6\u6b8b\u5dee\u63a7\u5236\u6846\u67b6\uff0c\u5728\u51bb\u7ed3\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u57fa\u7840\u4e0a\u6dfb\u52a0\u5728\u7ebf\u7ea0\u6b63\u52a8\u4f5c\uff0c\u65e0\u9700\u4fee\u6539\u57fa\u7840\u7b56\u7565\u53c2\u6570\u5373\u53ef\u5b9e\u73b0\u6545\u969c\u6062\u590d\u3002", "motivation": "\u673a\u5668\u4eba\u7b56\u7565\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\u5e38\u9047\u5230\u8bad\u7ec3\u540e\u6545\u969c\uff0c\u800c\u91cd\u65b0\u8bad\u7ec3\u3001\u63a2\u7d22\u6216\u7cfb\u7edf\u8fa8\u8bc6\u5f80\u5f80\u4e0d\u5207\u5b9e\u9645\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u6545\u969c\u6062\u590d\u800c\u4e0d\u4fee\u6539\u57fa\u7840\u7b56\u7565\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5c0f\u8111\u542f\u53d1\u7684\u6b8b\u5dee\u63a7\u5236\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u56fa\u5b9a\u7279\u5f81\u6269\u5c55\u5b9e\u73b0\u9ad8\u7ef4\u6a21\u5f0f\u5206\u79bb\uff1b2\uff09\u5e76\u884c\u5fae\u533a\u5f0f\u6b8b\u5dee\u901a\u8def\uff1b3\uff09\u5177\u6709\u5174\u594b\u6027\u548c\u6291\u5236\u6027\u8d44\u683c\u8ff9\u7684\u5c40\u90e8\u8bef\u5dee\u9a71\u52a8\u53ef\u5851\u6027\uff0c\u5728\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u8fd0\u884c\uff1b4\uff09\u6027\u80fd\u9a71\u52a8\u7684\u5143\u81ea\u9002\u5e94\u673a\u5236\u8c03\u8282\u6b8b\u5dee\u6743\u9650\u548c\u53ef\u5851\u6027\u3002", "result": "\u5728MuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u9a71\u52a8\u5668\u3001\u52a8\u6001\u548c\u73af\u5883\u6270\u52a8\u4e0b\uff0cHalfCheetah-v5\u6027\u80fd\u63d0\u5347\u8fbe+66%\uff0cHumanoid-v5\u63d0\u5347\u8fbe+53%\u3002\u5728\u4e25\u91cd\u6270\u52a8\u4e0b\u8868\u73b0\u4f18\u96c5\u9000\u5316\uff0c\u5e76\u80fd\u5c06\u6301\u4e45\u7684\u6b8b\u5dee\u4fee\u6b63\u6574\u5408\u5230\u7b56\u7565\u53c2\u6570\u4e2d\u3002", "conclusion": "\u8be5\u5c0f\u8111\u542f\u53d1\u7684\u63a8\u7406\u65f6\u6b8b\u5dee\u63a7\u5236\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u8bad\u7ec3\u540e\u6545\u969c\uff0c\u5728\u4e0d\u4fee\u6539\u57fa\u7840\u7b56\u7565\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5feb\u901f\u5c40\u90e8\u6821\u6b63\uff0c\u540c\u65f6\u901a\u8fc7\u5143\u81ea\u9002\u5e94\u673a\u5236\u4fdd\u6301\u540d\u4e49\u884c\u4e3a\u7a33\u5b9a\u6027\u3002"}}
{"id": "2602.08120", "categories": ["quant-ph", "math.NA", "q-fin.MF", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.08120", "abs": "https://arxiv.org/abs/2602.08120", "authors": ["Yihang Sun", "Guanyang Wang", "Jose Blanchet"], "title": "Optimal Quantum Speedups for Repeatedly Nested Expectation Estimation", "comment": null, "summary": "We study the estimation of repeatedly nested expectations (RNEs) with a constant horizon (number of nestings) using quantum computing. We propose a quantum algorithm that achieves $\\varepsilon$-error with cost $\\tilde O(\\varepsilon^{-1})$, up to logarithmic factors. Standard lower bounds show this scaling is essentially optimal, yielding an almost quadratic speedup over the best classical algorithm. Our results extend prior quantum speedups for single nested expectations to repeated nesting, and therefore cover a broader range of applications, including optimal stopping. This extension requires a new derandomized variant of the classical randomized Multilevel Monte Carlo (rMLMC) algorithm. Careful de-randomization is key to overcoming a variable-time issue that typically increases quantized versions of classical randomized algorithms.", "AI": {"tldr": "\u91cf\u5b50\u7b97\u6cd5\u4f30\u8ba1\u91cd\u590d\u5d4c\u5957\u671f\u671b\uff0c\u8fbe\u5230$\\tilde O(\\varepsilon^{-1})$\u6210\u672c\uff0c\u76f8\u6bd4\u7ecf\u5178\u7b97\u6cd5\u5b9e\u73b0\u8fd1\u4e8c\u6b21\u52a0\u901f\uff0c\u6269\u5c55\u4e86\u91cf\u5b50\u4f18\u52bf\u5230\u91cd\u590d\u5d4c\u5957\u573a\u666f\u3002", "motivation": "\u7814\u7a76\u91cd\u590d\u5d4c\u5957\u671f\u671b\uff08RNEs\uff09\u7684\u4f30\u8ba1\u95ee\u9898\uff0c\u6269\u5c55\u5148\u524d\u91cf\u5b50\u8ba1\u7b97\u5bf9\u5355\u5c42\u5d4c\u5957\u671f\u671b\u7684\u52a0\u901f\u4f18\u52bf\u5230\u591a\u5c42\u5d4c\u5957\u573a\u666f\uff0c\u8986\u76d6\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u9886\u57df\u5982\u6700\u4f18\u505c\u6b62\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u91cf\u5b50\u7b97\u6cd5\uff0c\u91c7\u7528\u7ecf\u5178\u968f\u673a\u591a\u7ea7\u8499\u7279\u5361\u6d1b\uff08rMLMC\uff09\u7b97\u6cd5\u7684\u53bb\u968f\u673a\u5316\u53d8\u4f53\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u53bb\u968f\u673a\u5316\u8fc7\u7a0b\u89e3\u51b3\u91cf\u5b50\u5316\u7ecf\u5178\u968f\u673a\u7b97\u6cd5\u65f6\u5e38\u89c1\u7684\u65f6\u95f4\u53ef\u53d8\u6027\u95ee\u9898\u3002", "result": "\u91cf\u5b50\u7b97\u6cd5\u4ee5$\\tilde O(\\varepsilon^{-1})$\u6210\u672c\u5b9e\u73b0$\\varepsilon$\u8bef\u5dee\u4f30\u8ba1\uff0c\u8fbe\u5230\u7406\u8bba\u4e0b\u754c\uff0c\u76f8\u6bd4\u6700\u4f73\u7ecf\u5178\u7b97\u6cd5\u83b7\u5f97\u8fd1\u4e8c\u6b21\u52a0\u901f\uff0c\u6210\u529f\u5c06\u91cf\u5b50\u4f18\u52bf\u6269\u5c55\u5230\u91cd\u590d\u5d4c\u5957\u671f\u671b\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u91cf\u5b50\u7b97\u6cd5\u7528\u4e8e\u91cd\u590d\u5d4c\u5957\u671f\u671b\u4f30\u8ba1\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u53bb\u968f\u673a\u5316\u6280\u672f\u514b\u670d\u4e86\u91cf\u5b50\u5316\u7ecf\u5178\u7b97\u6cd5\u7684\u6311\u6218\uff0c\u4e3a\u91cf\u5b50\u8ba1\u7b97\u5728\u91d1\u878d\u6570\u5b66\u548c\u4f18\u5316\u7b49\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2602.07235", "categories": ["cs.LG", "cs.AI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.07235", "abs": "https://arxiv.org/abs/2602.07235", "authors": ["Atefeh Gilani", "Carol Xuan Long", "Sajani Vithana", "Oliver Kosut", "Lalitha Sankar", "Flavio P. Calmon"], "title": "ArcMark: Multi-bit LLM Watermark via Optimal Transport", "comment": null, "summary": "Watermarking is an important tool for promoting the responsible use of language models (LMs). Existing watermarks insert a signal into generated tokens that either flags LM-generated text (zero-bit watermarking) or encodes more complex messages (multi-bit watermarking). Though a number of recent multi-bit watermarks insert several bits into text without perturbing average next-token predictions, they largely extend design principles from the zero-bit setting, such as encoding a single bit per token. Notably, the information-theoretic capacity of multi-bit watermarking -- the maximum number of bits per token that can be inserted and detected without changing average next-token predictions -- has remained unknown. We address this gap by deriving the first capacity characterization of multi-bit watermarks. Our results inform the design of ArcMark: a new watermark construction based on coding-theoretic principles that, under certain assumptions, achieves the capacity of the multi-bit watermark channel. In practice, ArcMark outperforms competing multi-bit watermarks in terms of bit rate per token and detection accuracy. Our work demonstrates that LM watermarking is fundamentally a channel coding problem, paving the way for principled coding-theoretic approaches to watermark design.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u63a8\u5bfc\u51fa\u591a\u6bd4\u7279\u6c34\u5370\u7684\u4fe1\u606f\u8bba\u5bb9\u91cf\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u7f16\u7801\u7406\u8bba\u7684\u65b0\u6c34\u5370\u65b9\u6848ArcMark\uff0c\u5728\u4fdd\u6301\u5e73\u5747\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u4e0d\u53d8\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u66f4\u9ad8\u6bd4\u7279\u7387\u548c\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u6c34\u5370\u6280\u672f\u8981\u4e48\u662f\u96f6\u6bd4\u7279\u6c34\u5370\uff08\u4ec5\u6807\u8bb0AI\u751f\u6210\u6587\u672c\uff09\uff0c\u8981\u4e48\u662f\u591a\u6bd4\u7279\u6c34\u5370\uff08\u7f16\u7801\u590d\u6742\u4fe1\u606f\uff09\u3002\u867d\u7136\u5df2\u6709\u4e00\u4e9b\u591a\u6bd4\u7279\u6c34\u5370\u80fd\u5728\u4e0d\u6539\u53d8\u5e73\u5747\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u7684\u60c5\u51b5\u4e0b\u63d2\u5165\u591a\u4e2a\u6bd4\u7279\uff0c\u4f46\u5b83\u4eec\u5927\u591a\u6cbf\u7528\u96f6\u6bd4\u7279\u6c34\u5370\u7684\u8bbe\u8ba1\u539f\u5219\uff08\u5982\u6bcf\u4e2atoken\u7f16\u7801\u4e00\u4e2a\u6bd4\u7279\uff09\u3002\u591a\u6bd4\u7279\u6c34\u5370\u7684\u4fe1\u606f\u8bba\u5bb9\u91cf\u2014\u2014\u5728\u4e0d\u6539\u53d8\u5e73\u5747\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u7684\u524d\u63d0\u4e0b\uff0c\u6bcf\u4e2atoken\u53ef\u63d2\u5165\u548c\u68c0\u6d4b\u7684\u6700\u5927\u6bd4\u7279\u6570\u2014\u2014\u4e00\u76f4\u672a\u77e5\u3002", "method": "1. \u9996\u6b21\u63a8\u5bfc\u591a\u6bd4\u7279\u6c34\u5370\u7684\u4fe1\u606f\u8bba\u5bb9\u91cf\u8868\u5f81\uff1b2. \u57fa\u4e8e\u7f16\u7801\u7406\u8bba\u539f\u7406\u8bbe\u8ba1\u65b0\u7684\u6c34\u5370\u6784\u9020ArcMark\uff0c\u5728\u7279\u5b9a\u5047\u8bbe\u4e0b\u8fbe\u5230\u591a\u6bd4\u7279\u6c34\u5370\u4fe1\u9053\u7684\u5bb9\u91cf\uff1b3. \u5c06\u8bed\u8a00\u6a21\u578b\u6c34\u5370\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u4fe1\u9053\u7f16\u7801\u95ee\u9898\u3002", "result": "ArcMark\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4f18\u4e8e\u5176\u4ed6\u591a\u6bd4\u7279\u6c34\u5370\u65b9\u6848\uff0c\u5728\u6bcftoken\u6bd4\u7279\u7387\u548c\u68c0\u6d4b\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002\u7814\u7a76\u8bc1\u660eArcMark\u80fd\u591f\u8fbe\u5230\u591a\u6bd4\u7279\u6c34\u5370\u4fe1\u9053\u7684\u7406\u8bba\u5bb9\u91cf\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u6c34\u5370\u672c\u8d28\u4e0a\u662f\u4fe1\u9053\u7f16\u7801\u95ee\u9898\uff0c\u672c\u6587\u4e3a\u57fa\u4e8e\u7f16\u7801\u7406\u8bba\u539f\u7406\u7684\u6c34\u5370\u8bbe\u8ba1\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002ArcMark\u5c55\u793a\u4e86\u7f16\u7801\u7406\u8bba\u65b9\u6cd5\u5728\u6c34\u5370\u8bbe\u8ba1\u4e2d\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u4fe1\u606f\u5d4c\u5165\u6548\u7387\u548c\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2602.08157", "categories": ["quant-ph", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2602.08157", "abs": "https://arxiv.org/abs/2602.08157", "authors": ["Urvashi D. Heramun", "Mohamed Sabba", "Dolnapa Yamano", "Christian Bengs", "Bonifac Legrady", "Giuseppe Pileio", "Sam Thompson", "Malcolm H. Levitt"], "title": "Spinor Double-Quantum Excitation in the Solution NMR of Near-Equivalent Spin-1/2 Pairs", "comment": "accepted for publication in JCP", "summary": "A family of double-quantum excitation schemes is described for the solution nuclear magnetic resonance (NMR) of near-equivalent spin-1/2 pairs. These new methods exploit the spinor behaviour of 2-level systems, whose signature is the change of sign of a quantum state upon a $2\u03c0$ rotation. The spinor behaviour is used to manipulate the phases of single-quantum coherences, in order to prepare a double-quantum precursor state which is rapidly converted into double-quantum coherence by a straightforward $\u03c0/2$ rotation. One set of spinor-based methods exploits symmetry-based pulse sequences, while the other set exploits SLIC (spin-lock-induced crossing), in which the nutation frequency under a resonant radiofrequency field is matched to the spin-spin coupling. A variant of SLIC is introduced which is well-compensated for deviations in the radiofrequency field amplitude. The methods are demonstrated by performing double-quantum-filtered $^{19}$F NMR on a molecular system containing a pair of diastereotopic $^{19}$F nuclei. The new methods are compared with existing techniques.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u65cb\u91cf\u884c\u4e3a\u7684\u53cc\u91cf\u5b50\u6fc0\u53d1\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fd1\u7b49\u4ef7\u81ea\u65cb-1/2\u5bf9\u7684\u6eb6\u6db2NMR\u5206\u6790", "motivation": "\u89e3\u51b3\u8fd1\u7b49\u4ef7\u81ea\u65cb-1/2\u5bf9\u5728\u6838\u78c1\u5171\u632f\u4e2d\u7684\u53cc\u91cf\u5b50\u6fc0\u53d1\u95ee\u9898\uff0c\u5229\u7528\u65cb\u91cf\u884c\u4e3a\u7279\u6027\u5f00\u53d1\u66f4\u6709\u6548\u7684\u6fc0\u53d1\u65b9\u6cd5", "method": "\u5229\u75282\u03c0\u65cb\u8f6c\u540e\u91cf\u5b50\u6001\u53d8\u53f7\u7684\u65cb\u91cf\u884c\u4e3a\uff0c\u901a\u8fc7\u64cd\u7eb5\u5355\u91cf\u5b50\u76f8\u5e72\u76f8\u4f4d\u6765\u5236\u5907\u53cc\u91cf\u5b50\u524d\u4f53\u6001\uff0c\u7136\u540e\u901a\u8fc7\u03c0/2\u65cb\u8f6c\u5feb\u901f\u8f6c\u6362\u4e3a\u53cc\u91cf\u5b50\u76f8\u5e72\u3002\u5305\u62ec\u57fa\u4e8e\u5bf9\u79f0\u8109\u51b2\u5e8f\u5217\u7684\u65b9\u6cd5\u548c\u57fa\u4e8eSLIC\uff08\u81ea\u65cb\u9501\u5b9a\u8bf1\u5bfc\u4ea4\u53c9\uff09\u7684\u65b9\u6cd5\uff0c\u540e\u8005\u5f15\u5165\u4e86\u5bf9\u5c04\u9891\u573a\u5e45\u5ea6\u504f\u5dee\u5177\u6709\u826f\u597d\u8865\u507f\u6027\u7684\u53d8\u4f53", "result": "\u5728\u542b\u6709\u975e\u5bf9\u6620\u5f02\u6784\u00b9\u2079F\u6838\u5bf9\u7684\u5206\u5b50\u7cfb\u7edf\u4e2d\u6210\u529f\u6f14\u793a\u4e86\u53cc\u91cf\u5b50\u6ee4\u6ce2\u00b9\u2079F NMR\uff0c\u5e76\u4e0e\u73b0\u6709\u6280\u672f\u8fdb\u884c\u4e86\u6bd4\u8f83", "conclusion": "\u57fa\u4e8e\u65cb\u91cf\u884c\u4e3a\u7684\u53cc\u91cf\u5b50\u6fc0\u53d1\u65b9\u6cd5\u4e3a\u8fd1\u7b49\u4ef7\u81ea\u65cb-1/2\u5bf9\u7684NMR\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u5de5\u5177\uff0c\u7279\u522b\u662fSLIC\u53d8\u4f53\u5bf9\u5c04\u9891\u573a\u504f\u5dee\u5177\u6709\u826f\u597d\u9c81\u68d2\u6027"}}
{"id": "2602.07256", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07256", "abs": "https://arxiv.org/abs/2602.07256", "authors": ["Ruizhong Qiu", "Ting-Wei Li", "Gaotang Li", "Hanghang Tong"], "title": "Graph homophily booster: Reimagining the role of discrete features in heterophilic graph learning", "comment": "ICLR 2026", "summary": "Graph neural networks (GNNs) have emerged as a powerful tool for modeling graph-structured data. However, existing GNNs often struggle with heterophilic graphs, where connected nodes tend to have dissimilar features or labels. While numerous methods have been proposed to address this challenge, they primarily focus on architectural designs without directly targeting the root cause of the heterophily problem. These approaches still perform even worse than the simplest MLPs on challenging heterophilic datasets. For instance, our experiments show that 21 latest GNNs still fall behind the MLP on the Actor dataset. This critical challenge calls for an innovative approach to addressing graph heterophily beyond architectural designs. To bridge this gap, we propose and study a new and unexplored paradigm: directly increasing the graph homophily via a carefully designed graph transformation. In this work, we present a simple yet effective framework called GRAPHITE to address graph heterophily. To the best of our knowledge, this work is the first method that explicitly transforms the graph to directly improve the graph homophily. Stemmed from the exact definition of homophily, our proposed GRAPHITE creates feature nodes to facilitate homophilic message passing between nodes that share similar features. Furthermore, we both theoretically and empirically show that our proposed GRAPHITE significantly increases the homophily of originally heterophilic graphs, with only a slight increase in the graph size. Extensive experiments on challenging datasets demonstrate that our proposed GRAPHITE significantly outperforms state-of-the-art methods on heterophilic graphs while achieving comparable accuracy with state-of-the-art methods on homophilic graphs.", "AI": {"tldr": "GRAPHITE\u901a\u8fc7\u521b\u5efa\u7279\u5f81\u8282\u70b9\u76f4\u63a5\u63d0\u9ad8\u56fe\u540c\u8d28\u6027\uff0c\u89e3\u51b3\u5f02\u8d28\u56fe\u4e0a\u7684GNN\u6027\u80fd\u95ee\u9898\uff0c\u5728\u5f02\u8d28\u56fe\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709GNN\u5728\u5f02\u8d28\u56fe\uff08\u8fde\u63a5\u8282\u70b9\u7279\u5f81/\u6807\u7b7e\u4e0d\u540c\u7684\u56fe\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u751a\u81f3\u4e0d\u5982\u7b80\u5355\u7684MLP\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u67b6\u6784\u8bbe\u8ba1\uff0c\u6ca1\u6709\u76f4\u63a5\u89e3\u51b3\u5f02\u8d28\u6027\u7684\u6839\u672c\u539f\u56e0\uff0c\u9700\u8981\u65b0\u7684\u8303\u5f0f\u6765\u76f4\u63a5\u63d0\u9ad8\u56fe\u540c\u8d28\u6027\u3002", "method": "\u63d0\u51faGRAPHITE\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u5efa\u7279\u5f81\u8282\u70b9\u6765\u4fc3\u8fdb\u7279\u5f81\u76f8\u4f3c\u8282\u70b9\u4e4b\u95f4\u7684\u540c\u8d28\u6027\u6d88\u606f\u4f20\u9012\u3002\u4ece\u540c\u8d28\u6027\u7684\u7cbe\u786e\u5b9a\u4e49\u51fa\u53d1\uff0c\u76f4\u63a5\u8f6c\u6362\u56fe\u7ed3\u6784\u4ee5\u63d0\u9ad8\u540c\u8d28\u6027\uff0c\u4ec5\u8f7b\u5fae\u589e\u52a0\u56fe\u89c4\u6a21\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u9a8c\u8868\u660eGRAPHITE\u80fd\u663e\u8457\u63d0\u9ad8\u5f02\u8d28\u56fe\u7684\u540c\u8d28\u6027\u3002\u5728\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\uff0cGRAPHITE\u5728\u5f02\u8d28\u56fe\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u540c\u8d28\u56fe\u4e0a\u4e5f\u80fd\u8fbe\u5230\u53ef\u6bd4\u7cbe\u5ea6\u3002", "conclusion": "GRAPHITE\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u63d0\u9ad8\u56fe\u540c\u8d28\u6027\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u56fe\u8f6c\u6362\u89e3\u51b3\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u4e3aGNN\u5728\u5f02\u8d28\u56fe\u4e0a\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08180", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.08180", "abs": "https://arxiv.org/abs/2602.08180", "authors": ["Pedro Rosario", "Romain Bachelard"], "title": "Detecting multilevel entanglement from light-based entanglement witnesses", "comment": null, "summary": "We introduce a set of electric-field based inequalities capable of detecting multilevel entanglement from a system of N quantum emitters. We determine that the polarization channel as well as the direction of detection can enhance entanglement detection, a feature specific to multilevel systems. We demonstrate the efficiency of the witnesses to detect genuine multipartite entanglement by applying it to families of paradigmatic quantum states, such as Dicke states, singlet states and W-like states. The detection is not only robust to noise, but also applies to mixed entangled states. Our findings open up possibilities for the detection of entanglement without local measurements in systems of multilevel emitters such as superconducting qubits, Rydberg atoms or quantum dots.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7535\u573a\u7684\u4e0d\u7b49\u5f0f\u6765\u68c0\u6d4b\u591a\u80fd\u7ea7\u91cf\u5b50\u53d1\u5c04\u5668\u7cfb\u7edf\u7684\u591a\u4f53\u7ea0\u7f20\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u5c40\u57df\u6d4b\u91cf\uff0c\u5bf9\u566a\u58f0\u9c81\u68d2\u4e14\u9002\u7528\u4e8e\u6df7\u5408\u6001", "motivation": "\u591a\u80fd\u7ea7\u91cf\u5b50\u7cfb\u7edf\uff08\u5982\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u3001\u91cc\u5fb7\u5821\u539f\u5b50\u3001\u91cf\u5b50\u70b9\uff09\u4e2d\u7684\u7ea0\u7f20\u68c0\u6d4b\u901a\u5e38\u9700\u8981\u5c40\u57df\u6d4b\u91cf\uff0c\u8fd9\u5728\u5b9e\u9645\u5b9e\u9a8c\u4e2d\u53ef\u80fd\u96be\u4ee5\u5b9e\u73b0\u3002\u9700\u8981\u5f00\u53d1\u65e0\u9700\u5c40\u57df\u6d4b\u91cf\u7684\u7ea0\u7f20\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u9488\u5bf9\u591a\u80fd\u7ea7\u7cfb\u7edf\u7684\u7279\u6027\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u7535\u573a\u7684\u4e0d\u7b49\u5f0f\u6765\u68c0\u6d4bN\u4e2a\u91cf\u5b50\u53d1\u5c04\u5668\u7cfb\u7edf\u7684\u591a\u80fd\u7ea7\u7ea0\u7f20\u3002\u5229\u7528\u504f\u632f\u901a\u9053\u548c\u68c0\u6d4b\u65b9\u5411\u6765\u589e\u5f3a\u7ea0\u7f20\u68c0\u6d4b\u80fd\u529b\uff0c\u8fd9\u662f\u591a\u80fd\u7ea7\u7cfb\u7edf\u7279\u6709\u7684\u7279\u6027\u3002\u5c06\u89c1\u8bc1\u5b50\u5e94\u7528\u4e8e\u5178\u578b\u91cf\u5b50\u6001\u5bb6\u65cf\uff08\u5982Dicke\u6001\u3001\u5355\u91cd\u6001\u3001W-like\u6001\uff09\u6765\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u68c0\u6d4b\u771f\u5b9e\u591a\u4f53\u7ea0\u7f20\uff0c\u800c\u4e14\u5bf9\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u6df7\u5408\u7ea0\u7f20\u6001\u3002\u504f\u632f\u901a\u9053\u548c\u68c0\u6d4b\u65b9\u5411\u7684\u9009\u62e9\u80fd\u663e\u8457\u589e\u5f3a\u7ea0\u7f20\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "\u4e3a\u591a\u80fd\u7ea7\u53d1\u5c04\u5668\u7cfb\u7edf\u4e2d\u7684\u7ea0\u7f20\u68c0\u6d4b\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u65e0\u9700\u5c40\u57df\u6d4b\u91cf\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u68c0\u6d4b\uff0c\u6709\u671b\u5e94\u7528\u4e8e\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u3001\u91cc\u5fb7\u5821\u539f\u5b50\u548c\u91cf\u5b50\u70b9\u7b49\u5b9e\u9645\u7cfb\u7edf\u3002"}}
{"id": "2602.07258", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.07258", "abs": "https://arxiv.org/abs/2602.07258", "authors": ["Wanru Guo", "Juan Xie", "Binbin Wang", "Weicong Chen", "Xiaoyi Lu", "Vipin Chaudhary", "Curtis Tatsuoka"], "title": "Robust Ultra-High-Dimensional Variable Selection With Correlated Structure Using Group Testing", "comment": "57 Pages, 5 Figures, 4 Tables", "summary": "Background: High-dimensional genomic data exhibit strong group correlation structures that challenge conventional feature selection methods, which often assume feature independence or rely on pre-defined pathways and are sensitive to outliers and model misspecification.\n  Methods: We propose the Dorfman screening framework, a multi-stage procedure that forms data-driven variable groups via hierarchical clustering, performs group and within-group hypothesis testing, and refines selection using elastic net or adaptive elastic net. Robust variants incorporate OGK-based covariance estimation, rank-based correlation, and Huber-weighted regression to handle contaminated and non-normal data.\n  Results: In simulations, Dorfman-Sparse-Adaptive-EN performed best under normal conditions, while Robust-OGK-Dorfman-Adaptive-EN showed clear advantages under data contamination, outperforming classical Dorfman and competing methods. Applied to NSCLC gene expression data for trametinib response, robust Dorfman methods achieved the lowest prediction errors and enriched recovery of clinically relevant genes.\n  Conclusions: The Dorfman framework provides an efficient and robust approach to genomic feature selection. Robust-OGK-Dorfman-Adaptive-EN offers strong performance under both ideal and contaminated conditions and scales to ultra-high-dimensional settings, making it well suited for modern genomic biomarker discovery.", "AI": {"tldr": "Dorfman\u7b5b\u9009\u6846\u67b6\uff1a\u901a\u8fc7\u5c42\u6b21\u805a\u7c7b\u5f62\u6210\u6570\u636e\u9a71\u52a8\u7684\u53d8\u91cf\u7ec4\uff0c\u8fdb\u884c\u7ec4\u5185\u548c\u7ec4\u95f4\u5047\u8bbe\u68c0\u9a8c\uff0c\u7ed3\u5408\u5f39\u6027\u7f51\u7edc\u8fdb\u884c\u7279\u5f81\u9009\u62e9\uff0c\u5e76\u63d0\u4f9b\u7a33\u5065\u7248\u672c\u5904\u7406\u5f02\u5e38\u503c\u548c\u975e\u6b63\u6001\u6570\u636e\u3002", "motivation": "\u9ad8\u7ef4\u57fa\u56e0\u7ec4\u6570\u636e\u5b58\u5728\u5f3a\u70c8\u7684\u7ec4\u76f8\u5173\u7ed3\u6784\uff0c\u4f20\u7edf\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u5047\u8bbe\u7279\u5f81\u72ec\u7acb\u6216\u4f9d\u8d56\u9884\u5b9a\u4e49\u901a\u8def\uff0c\u5bf9\u5f02\u5e38\u503c\u548c\u6a21\u578b\u8bef\u8bbe\u654f\u611f\uff0c\u9700\u8981\u66f4\u7a33\u5065\u7684\u65b9\u6cd5\u3002", "method": "\u591a\u9636\u6bb5\u7a0b\u5e8f\uff1a1) \u901a\u8fc7\u5c42\u6b21\u805a\u7c7b\u5f62\u6210\u6570\u636e\u9a71\u52a8\u7684\u53d8\u91cf\u7ec4\uff1b2) \u8fdb\u884c\u7ec4\u548c\u7ec4\u5185\u5047\u8bbe\u68c0\u9a8c\uff1b3) \u4f7f\u7528\u5f39\u6027\u7f51\u7edc\u6216\u81ea\u9002\u5e94\u5f39\u6027\u7f51\u7edc\u7ec6\u5316\u9009\u62e9\uff1b\u7a33\u5065\u53d8\u4f53\u5305\u542bOGK\u534f\u65b9\u5dee\u4f30\u8ba1\u3001\u79e9\u76f8\u5173\u548cHuber\u52a0\u6743\u56de\u5f52\u3002", "result": "\u5728\u6a21\u62df\u4e2d\uff0cDorfman-Sparse-Adaptive-EN\u5728\u6b63\u6001\u6761\u4ef6\u4e0b\u8868\u73b0\u6700\u4f73\uff0cRobust-OGK-Dorfman-Adaptive-EN\u5728\u6570\u636e\u6c61\u67d3\u6761\u4ef6\u4e0b\u4f18\u52bf\u660e\u663e\uff1b\u5e94\u7528\u4e8eNSCLC\u57fa\u56e0\u8868\u8fbe\u6570\u636e\u65f6\uff0c\u7a33\u5065Dorfman\u65b9\u6cd5\u83b7\u5f97\u6700\u4f4e\u9884\u6d4b\u8bef\u5dee\u5e76\u5bcc\u96c6\u4e34\u5e8a\u76f8\u5173\u57fa\u56e0\u3002", "conclusion": "Dorfman\u6846\u67b6\u4e3a\u57fa\u56e0\u7ec4\u7279\u5f81\u9009\u62e9\u63d0\u4f9b\u4e86\u9ad8\u6548\u7a33\u5065\u7684\u65b9\u6cd5\uff0cRobust-OGK-Dorfman-Adaptive-EN\u5728\u7406\u60f3\u548c\u6c61\u67d3\u6761\u4ef6\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u53ef\u6269\u5c55\u5230\u8d85\u9ad8\u7ef4\u8bbe\u7f6e\uff0c\u9002\u7528\u4e8e\u73b0\u4ee3\u57fa\u56e0\u7ec4\u751f\u7269\u6807\u5fd7\u7269\u53d1\u73b0\u3002"}}
{"id": "2602.08209", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.08209", "abs": "https://arxiv.org/abs/2602.08209", "authors": ["Zhiyuan Lin", "Sen Li", "Jingyan Feng", "Valentin Ivannikov", "Matteo Fadel", "Tim Byrnes"], "title": "Preparing squeezed, cat and GKP states with parity measurements", "comment": null, "summary": "Bosonic modes constitute a central resource in a wide range of quantum technologies, providing long-lived degrees of freedom for the storage, processing, and transduction of quantum information. Such modes naturally arise in platforms including circuit quantum electrodynamics, quantum acoustodynamics, and trapped-ion systems. In these architectures, coherent control and high-fidelity readout of the bosonic degrees of freedom are achieved via coupling to an auxiliary qubit. When operated in the strong dispersive regime, this interaction enables parity measurements of the mode which, in combination with phase-space displacements, constitute a standard experimental tool for full Wigner-function tomography. Here, we propose a protocol based on displaced parity measurements that allows for the preparation of a variety of bosonic quantum states. As a first example, we demonstrate the generation of squeezed states, achieving up to ~9 dB of squeezing after only three parity measurements, and show that the protocol is robust against experimental imperfections. Finally, we generalize our approach to the preparation of other paradigmatic bosonic states, including cat and Gottesman-Kitaev-Preskill states.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4f4d\u79fb\u5b87\u79f0\u6d4b\u91cf\u7684\u534f\u8bae\uff0c\u7528\u4e8e\u5236\u5907\u591a\u79cd\u73bb\u8272\u91cf\u5b50\u6001\uff0c\u5305\u62ec\u538b\u7f29\u6001\u3001\u732b\u6001\u548cGKP\u6001\uff0c\u4ec5\u9700\u4e09\u6b21\u5b87\u79f0\u6d4b\u91cf\u5373\u53ef\u5b9e\u73b0\u7ea69dB\u7684\u538b\u7f29\u3002", "motivation": "\u73bb\u8272\u6a21\u5f0f\u662f\u91cf\u5b50\u6280\u672f\u4e2d\u7684\u91cd\u8981\u8d44\u6e90\uff0c\u7528\u4e8e\u91cf\u5b50\u4fe1\u606f\u7684\u5b58\u50a8\u3001\u5904\u7406\u548c\u8f6c\u6362\u3002\u867d\u7136\u73b0\u6709\u6280\u672f\u53ef\u4ee5\u901a\u8fc7\u8f85\u52a9\u91cf\u5b50\u6bd4\u7279\u5b9e\u73b0\u73bb\u8272\u6a21\u5f0f\u7684\u76f8\u5e72\u63a7\u5236\u548c\u8bfb\u53d6\uff0c\u4f46\u9700\u8981\u66f4\u9ad8\u6548\u7684\u91cf\u5b50\u6001\u5236\u5907\u534f\u8bae\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4f4d\u79fb\u5b87\u79f0\u6d4b\u91cf\u7684\u534f\u8bae\uff0c\u5229\u7528\u5f3a\u8272\u6563\u8026\u5408\u4e0b\u7684\u5b87\u79f0\u6d4b\u91cf\u7ed3\u5408\u76f8\u7a7a\u95f4\u4f4d\u79fb\u64cd\u4f5c\uff0c\u901a\u8fc7\u5c11\u91cf\u6d4b\u91cf\u6b65\u9aa4\u5236\u5907\u76ee\u6807\u91cf\u5b50\u6001\u3002", "result": "\u8be5\u534f\u8bae\u4ec5\u9700\u4e09\u6b21\u5b87\u79f0\u6d4b\u91cf\u5373\u53ef\u5b9e\u73b0\u7ea69dB\u7684\u538b\u7f29\u6001\uff0c\u4e14\u5bf9\u5b9e\u9a8c\u7f3a\u9677\u5177\u6709\u9c81\u68d2\u6027\u3002\u8fd8\u53ef\u63a8\u5e7f\u5230\u5236\u5907\u732b\u6001\u548cGottesman-Kitaev-Preskill\u6001\u7b49\u5178\u578b\u73bb\u8272\u6001\u3002", "conclusion": "\u57fa\u4e8e\u4f4d\u79fb\u5b87\u79f0\u6d4b\u91cf\u7684\u534f\u8bae\u4e3a\u73bb\u8272\u91cf\u5b50\u6001\u7684\u5236\u5907\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u91cf\u5b50\u5e73\u53f0\uff0c\u6709\u671b\u63a8\u52a8\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.07263", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07263", "abs": "https://arxiv.org/abs/2602.07263", "authors": ["Kevin Li", "Dibyadeep Saha", "Avni Kanodia", "Fan Lai"], "title": "tLoRA: Efficient Multi-LoRA Training with Elastic Shared Super-Models", "comment": null, "summary": "As Low-Rank Adaptation (LoRA) becomes the standard approach for efficiently fine-tuning large language models (LLMs), shared clusters increasingly execute many concurrent LoRA training jobs over the same frozen backbone. While recent advances enable batching (co-locating) multiple adapters during serving, efficient training-time co-location of heterogeneous LoRA adapters presents unique challenges. Jobs often differ in adapter rank, batch size, and resource allocation, and na\u00efve batching can introduce synchronization stalls, communication overheads, and per-job slowdowns that are worse than executing independently. We introduce tLoRA, a framework that enables efficient batch training of multiple LoRA jobs. tLoRA fuses adapters that share the same base model into an elastic shared super-model, exploiting existing distributed training frameworks to derive parallelism plans that share resources effectively. At the kernel level, tLoRA employs a fused LoRA kernel that adaptively reconstructs low-rank computation tiles and schedules rank-aware nano-batches to maximize overlap between computation and communication across adapters. At the scheduling layer, tLoRA incorporates an online, residual-capacity-aware scheduler that adaptively groups jobs to maximize collective throughput. Evaluations using real-world cluster traces demonstrate that tLoRA improves training throughput by 1.2--1.8x, job training completion time by 2.3--5.4x, and GPU utilization by 37%.", "AI": {"tldr": "tLoRA\uff1a\u4e00\u4e2a\u9ad8\u6548\u6279\u91cf\u8bad\u7ec3\u591a\u4e2aLoRA\u9002\u914d\u5668\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u5171\u4eab\u57fa\u7840\u6a21\u578b\u7684\u9002\u914d\u5668\u4e3a\u5f39\u6027\u8d85\u7ea7\u6a21\u578b\uff0c\u4f7f\u7528\u878d\u5408\u5185\u6838\u548c\u667a\u80fd\u8c03\u5ea6\uff0c\u63d0\u5347\u8bad\u7ec3\u541e\u5410\u91cf1.2-1.8\u500d\uff0c\u5b8c\u6210\u65f6\u95f4\u7f29\u77ed2.3-5.4\u500d\uff0cGPU\u5229\u7528\u7387\u63d0\u9ad837%\u3002", "motivation": "\u968f\u7740LoRA\u6210\u4e3a\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6807\u51c6\u65b9\u6cd5\uff0c\u5171\u4eab\u96c6\u7fa4\u4e2d\u540c\u65f6\u8fd0\u884c\u591a\u4e2aLoRA\u8bad\u7ec3\u4efb\u52a1\u7684\u60c5\u51b5\u65e5\u76ca\u666e\u904d\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8bad\u7ec3\u65f6\u6279\u91cf\u5904\u7406\u5f02\u6784LoRA\u9002\u914d\u5668\u5b58\u5728\u6311\u6218\uff1a\u4efb\u52a1\u5728\u9002\u914d\u5668\u79e9\u3001\u6279\u5927\u5c0f\u548c\u8d44\u6e90\u5206\u914d\u4e0a\u5404\u4e0d\u76f8\u540c\uff0c\u7b80\u5355\u7684\u6279\u91cf\u5904\u7406\u4f1a\u5bfc\u81f4\u540c\u6b65\u5ef6\u8fdf\u3001\u901a\u4fe1\u5f00\u9500\u548c\u6027\u80fd\u4e0b\u964d\u3002", "method": "tLoRA\u91c7\u7528\u4e24\u5c42\u65b9\u6cd5\uff1a1\uff09\u5185\u6838\u5c42\uff1a\u4f7f\u7528\u878d\u5408LoRA\u5185\u6838\uff0c\u81ea\u9002\u5e94\u91cd\u6784\u4f4e\u79e9\u8ba1\u7b97\u5757\uff0c\u8c03\u5ea6\u79e9\u611f\u77e5\u7684\u7eb3\u7c73\u6279\u6b21\uff0c\u6700\u5927\u5316\u8ba1\u7b97\u4e0e\u901a\u4fe1\u7684\u91cd\u53e0\uff1b2\uff09\u8c03\u5ea6\u5c42\uff1a\u5728\u7ebf\u6b8b\u5dee\u5bb9\u91cf\u611f\u77e5\u8c03\u5ea6\u5668\uff0c\u81ea\u9002\u5e94\u5206\u7ec4\u4efb\u52a1\u4ee5\u6700\u5927\u5316\u96c6\u4f53\u541e\u5410\u91cf\u3002\u6846\u67b6\u5c06\u5171\u4eab\u57fa\u7840\u6a21\u578b\u7684\u9002\u914d\u5668\u878d\u5408\u4e3a\u5f39\u6027\u5171\u4eab\u8d85\u7ea7\u6a21\u578b\uff0c\u5229\u7528\u73b0\u6709\u5206\u5e03\u5f0f\u8bad\u7ec3\u6846\u67b6\u5b9e\u73b0\u8d44\u6e90\u5171\u4eab\u3002", "result": "\u57fa\u4e8e\u771f\u5b9e\u96c6\u7fa4\u8f68\u8ff9\u7684\u8bc4\u4f30\u663e\u793a\uff0ctLoRA\u5c06\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u53471.2-1.8\u500d\uff0c\u4efb\u52a1\u8bad\u7ec3\u5b8c\u6210\u65f6\u95f4\u7f29\u77ed2.3-5.4\u500d\uff0cGPU\u5229\u7528\u7387\u63d0\u9ad837%\u3002", "conclusion": "tLoRA\u901a\u8fc7\u521b\u65b0\u7684\u5185\u6838\u878d\u5408\u548c\u667a\u80fd\u8c03\u5ea6\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784LoRA\u9002\u914d\u5668\u6279\u91cf\u8bad\u7ec3\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96c6\u7fa4\u8d44\u6e90\u5229\u7528\u7387\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21LoRA\u5fae\u8c03\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08345", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.08345", "abs": "https://arxiv.org/abs/2602.08345", "authors": ["Wen-Xiu Zhang", "Guo-Zhu Song", "Hai-Rui Wei"], "title": "The simplified quantum circuits for implementing quantum teleportation", "comment": "16 pages, 20 figures", "summary": "It is crucial to design quantum circuits as small as possible and as shallow as possible for quantum information processing tasks. We design quantum circuits with simplified gate-count, cost, and depth for implementing quantum teleportation among various entangled channels. Here the gate-count/cost/depth of the Greenberger-Horne-Zeilinger-based quantum teleportation is reduced from 10/6/8 to 9/4/6, the two-qubit-cluster-based quantum teleportation is reduced from 9/4/5 to 6/3/5, the three-qubit-cluster-based quantum teleportation is reduced from 12/6/7 to 8/4/5, the Brown-based quantum teleportation is reduced from 25/15/17 to 18/8/7, the Borras-based quantum teleportation is reduced from 36/25/20 to 15/8/11, and the entanglement-swapping-based quantum teleportation is reduced from 13/8/8 to 10/5/5. Note that, no feed-forward recover operation is required in the simplified schemes. Moreover, the experimentally demonstrations on IBM quantum computer indicate that our simplified and compressed schemes can be realized with good fidelity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u91cf\u5b50\u7535\u8def\u8bbe\u8ba1\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u591a\u79cd\u7ea0\u7f20\u901a\u9053\u4e0b\u91cf\u5b50\u9690\u5f62\u4f20\u6001\u7684\u95e8\u6570\u3001\u6210\u672c\u548c\u6df1\u5ea6\uff0c\u5e76\u5728IBM\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\u3002", "motivation": "\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u9700\u8981\u5c3d\u53ef\u80fd\u5c0f\u4e14\u6d45\u7684\u91cf\u5b50\u7535\u8def\u8bbe\u8ba1\uff0c\u4ee5\u964d\u4f4e\u5b9e\u73b0\u590d\u6742\u5ea6\u548c\u63d0\u9ad8\u6548\u7387\u3002\u73b0\u6709\u91cf\u5b50\u9690\u5f62\u4f20\u6001\u65b9\u6848\u7684\u95e8\u6570\u3001\u6210\u672c\u548c\u6df1\u5ea6\u4ecd\u6709\u4f18\u5316\u7a7a\u95f4\u3002", "method": "\u8bbe\u8ba1\u7b80\u5316\u7684\u91cf\u5b50\u7535\u8def\uff0c\u4f18\u5316\u95e8\u6570\u3001\u6210\u672c\u548c\u6df1\u5ea6\uff0c\u9488\u5bf9\u591a\u79cd\u7ea0\u7f20\u901a\u9053\uff08GHZ\u3001\u4e8c/\u4e09\u91cf\u5b50\u6bd4\u7279\u7c07\u3001Brown\u3001Borras\u3001\u7ea0\u7f20\u4ea4\u6362\u7b49\uff09\u7684\u91cf\u5b50\u9690\u5f62\u4f20\u6001\u65b9\u6848\u8fdb\u884c\u6539\u8fdb\uff0c\u65e0\u9700\u524d\u9988\u6062\u590d\u64cd\u4f5c\u3002", "result": "\u663e\u8457\u964d\u4f4e\u4e86\u5404\u79cd\u7ea0\u7f20\u901a\u9053\u91cf\u5b50\u9690\u5f62\u4f20\u6001\u7684\u95e8\u6570\u3001\u6210\u672c\u548c\u6df1\u5ea6\uff1aGHZ\u4ece10/6/8\u964d\u81f39/4/6\uff0c\u4e8c\u91cf\u5b50\u6bd4\u7279\u7c07\u4ece9/4/5\u964d\u81f36/3/5\uff0c\u4e09\u91cf\u5b50\u6bd4\u7279\u7c07\u4ece12/6/7\u964d\u81f38/4/5\uff0cBrown\u4ece25/15/17\u964d\u81f318/8/7\uff0cBorras\u4ece36/25/20\u964d\u81f315/8/11\uff0c\u7ea0\u7f20\u4ea4\u6362\u4ece13/8/8\u964d\u81f310/5/5\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b80\u5316\u538b\u7f29\u65b9\u6848\u80fd\u6709\u6548\u4f18\u5316\u91cf\u5b50\u9690\u5f62\u4f20\u6001\u7535\u8def\uff0c\u5728IBM\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\u8fd9\u4e9b\u65b9\u6848\u5177\u6709\u826f\u597d\u4fdd\u771f\u5ea6\uff0c\u4e3a\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u5b9e\u73b0\u65b9\u6cd5\u3002"}}
{"id": "2602.07265", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07265", "abs": "https://arxiv.org/abs/2602.07265", "authors": ["Daniil Vankov", "Nikita Ivkin", "Kyle Ulrich", "Xiang Song", "Ashish Khetan", "George Karypis"], "title": "XShare: Collaborative in-Batch Expert Sharing for Faster MoE Inference", "comment": null, "summary": "Mixture-of-Experts (MoE) architectures are increasingly used to efficiently scale large language models. However, in production inference, request batching and speculative decoding significantly amplify expert activation, eroding these efficiency benefits. We address this issue by modeling batch-aware expert selection as a modular optimization problem and designing efficient greedy algorithms for different deployment settings. The proposed method, namely XShare, requires no retraining and dynamically adapts to each batch by maximizing the total gating score of selected experts. It reduces expert activation by up to 30% under standard batching, cuts peak GPU load by up to 3x in expert-parallel deployments, and achieves up to 14% throughput gains in speculative decoding via hierarchical, correlation-aware expert selection even if requests in a batch drawn from heterogeneous datasets.", "AI": {"tldr": "XShare\uff1a\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u52a8\u6001\u4e13\u5bb6\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u6279\u5904\u7406\u611f\u77e5\u7684\u8d2a\u5fc3\u7b97\u6cd5\u4f18\u5316MoE\u67b6\u6784\u4e2d\u7684\u4e13\u5bb6\u6fc0\u6d3b\uff0c\u51cf\u5c1130%\u4e13\u5bb6\u6fc0\u6d3b\uff0c\u964d\u4f4e3\u500dGPU\u5cf0\u503c\u8d1f\u8f7d\uff0c\u63d0\u534714%\u63a8\u7406\u541e\u5410\u91cf", "motivation": "MoE\u67b6\u6784\u867d\u7136\u80fd\u9ad8\u6548\u6269\u5c55\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5728\u751f\u4ea7\u63a8\u7406\u4e2d\uff0c\u8bf7\u6c42\u6279\u5904\u7406\u548c\u63a8\u6d4b\u89e3\u7801\u4f1a\u663e\u8457\u589e\u52a0\u4e13\u5bb6\u6fc0\u6d3b\uff0c\u524a\u5f31\u6548\u7387\u4f18\u52bf\u3002\u9700\u8981\u89e3\u51b3\u6279\u5904\u7406\u611f\u77e5\u7684\u4e13\u5bb6\u9009\u62e9\u95ee\u9898\u3002", "method": "\u5c06\u6279\u5904\u7406\u611f\u77e5\u7684\u4e13\u5bb6\u9009\u62e9\u5efa\u6a21\u4e3a\u6a21\u5757\u5316\u4f18\u5316\u95ee\u9898\uff0c\u8bbe\u8ba1\u9ad8\u6548\u7684\u8d2a\u5fc3\u7b97\u6cd5\u9002\u5e94\u4e0d\u540c\u90e8\u7f72\u573a\u666f\u3002XShare\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u52a8\u6001\u9002\u5e94\u6bcf\u4e2a\u6279\u6b21\uff0c\u901a\u8fc7\u6700\u5927\u5316\u9009\u5b9a\u4e13\u5bb6\u7684\u603b\u95e8\u63a7\u5206\u6570\u6765\u4f18\u5316\u4e13\u5bb6\u6fc0\u6d3b\u3002", "result": "\u5728\u6807\u51c6\u6279\u5904\u7406\u4e0b\u51cf\u5c1130%\u4e13\u5bb6\u6fc0\u6d3b\uff1b\u5728\u4e13\u5bb6\u5e76\u884c\u90e8\u7f72\u4e2d\u964d\u4f4e3\u500dGPU\u5cf0\u503c\u8d1f\u8f7d\uff1b\u5728\u63a8\u6d4b\u89e3\u7801\u4e2d\u901a\u8fc7\u5206\u5c42\u3001\u76f8\u5173\u6027\u611f\u77e5\u7684\u4e13\u5bb6\u9009\u62e9\u5b9e\u73b014%\u541e\u5410\u91cf\u63d0\u5347\uff0c\u5373\u4f7f\u6279\u6b21\u8bf7\u6c42\u6765\u81ea\u5f02\u6784\u6570\u636e\u96c6\u3002", "conclusion": "XShare\u6709\u6548\u89e3\u51b3\u4e86MoE\u67b6\u6784\u5728\u751f\u4ea7\u63a8\u7406\u4e2d\u7684\u6548\u7387\u74f6\u9888\uff0c\u901a\u8fc7\u52a8\u6001\u6279\u5904\u7406\u611f\u77e5\u7684\u4e13\u5bb6\u9009\u62e9\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u90e8\u7f72\u573a\u666f\u3002"}}
{"id": "2602.08357", "categories": ["quant-ph", "nucl-th"], "pdf": "https://arxiv.org/pdf/2602.08357", "abs": "https://arxiv.org/abs/2602.08357", "authors": ["Weijie Du", "Yangguang Yang", "Zixin Liu", "Chao Yang", "James P. Vary"], "title": "Quantum-classical framework for many-fermion response and structure", "comment": "9 pages, 2 figures. We welcome comments", "summary": "Response functions are key observables for probing the structure and dynamics of many-body systems. We introduce and demonstrate a quantum-classical framework for computing response functions of general many-fermion systems that also provides the full bound-state spectrum. The framework employs the Lorentz integral transform and a new Hamiltonian input scheme that enables practical and scalable circuit constructions for general many-fermion Hamiltonians. Within this framework, we develop a hybrid strategy to evaluate the Lorentz integral and propose three protocols to extract response functions and bound-state structural information. As a demonstration, we apply the method to \\({}^{19}\\mathrm{O}\\) with realistic internucleon interactions, computing both the bound-state spectrum and the response function. We envision that our approach will open new avenues for exploring the structure and dynamics of a broad class of many-body systems across diverse fields.", "AI": {"tldr": "\u63d0\u51fa\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u6846\u67b6\u8ba1\u7b97\u591a\u8d39\u7c73\u5b50\u7cfb\u7edf\u7684\u54cd\u5e94\u51fd\u6570\u548c\u675f\u7f1a\u6001\u8c31\uff0c\u4f7f\u7528\u6d1b\u4f26\u5179\u79ef\u5206\u53d8\u6362\u548c\u65b0\u54c8\u5bc6\u987f\u91cf\u8f93\u5165\u65b9\u6848\uff0c\u5e94\u7528\u4e8e\u00b9\u2079O\u6838\u7cfb\u7edf\u9a8c\u8bc1", "motivation": "\u54cd\u5e94\u51fd\u6570\u662f\u63a2\u6d4b\u591a\u4f53\u7cfb\u7edf\u7ed3\u6784\u548c\u52a8\u529b\u5b66\u7684\u5173\u952e\u53ef\u89c2\u6d4b\u91cf\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u9ad8\u6548\u8ba1\u7b97\u4e00\u822c\u591a\u8d39\u7c73\u5b50\u7cfb\u7edf\u7684\u54cd\u5e94\u51fd\u6570\u548c\u5b8c\u6574\u675f\u7f1a\u6001\u8c31", "method": "\u91c7\u7528\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u6d1b\u4f26\u5179\u79ef\u5206\u53d8\u6362\u548c\u65b0\u54c8\u5bc6\u987f\u91cf\u8f93\u5165\u65b9\u6848\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u91cf\u5b50\u7535\u8def\u6784\u9020\uff0c\u5e76\u63d0\u51fa\u4e09\u79cd\u534f\u8bae\u63d0\u53d6\u54cd\u5e94\u51fd\u6570\u548c\u675f\u7f1a\u6001\u7ed3\u6784\u4fe1\u606f", "result": "\u6210\u529f\u5e94\u7528\u4e8e\u00b9\u2079O\u6838\u7cfb\u7edf\uff0c\u4f7f\u7528\u771f\u5b9e\u6838\u5b50\u95f4\u76f8\u4e92\u4f5c\u7528\u8ba1\u7b97\u4e86\u675f\u7f1a\u6001\u8c31\u548c\u54cd\u5e94\u51fd\u6570\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u63a2\u7d22\u591a\u4f53\u7cfb\u7edf\u7684\u7ed3\u6784\u548c\u52a8\u529b\u5b66\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u4e0d\u540c\u9886\u57df\u7684\u591a\u4f53\u7cfb\u7edf\u7814\u7a76"}}
{"id": "2602.07273", "categories": ["cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.07273", "abs": "https://arxiv.org/abs/2602.07273", "authors": ["Xiaoyi Wu", "Juaren Steiger", "Bin Li", "R. Srikant"], "title": "Hybrid Feedback-Guided Optimal Learning for Wireless Interactive Panoramic Scene Delivery", "comment": "Submitting to ToN", "summary": "Immersive applications such as virtual and augmented reality impose stringent requirements on frame rate, latency, and synchronization between physical and virtual environments. To meet these requirements, an edge server must render panoramic content, predict user head motion, and transmit a portion of the scene that is large enough to cover the user viewport while remaining within wireless bandwidth constraints. Each portion produces two feedback signals: prediction feedback, indicating whether the selected portion covers the actual viewport, and transmission feedback, indicating whether the corresponding packets are successfully delivered. Prior work models this problem as a multi-armed bandit with two-level bandit feedback, but fails to exploit the fact that prediction feedback can be retrospectively computed for all candidate portions once the user head pose is observed. As a result, prediction feedback constitutes full-information feedback rather than bandit feedback. Motivated by this observation, we introduce a two-level hybrid feedback model that combines full-information and bandit feedback, and formulate the portion selection problem as an online learning task under this setting. We derive an instance-dependent regret lower bound for the hybrid feedback model and propose AdaPort, a hybrid learning algorithm that leverages both feedback types to improve learning efficiency. We further establish an instance-dependent regret upper bound that matches the lower bound asymptotically, and demonstrate through real-world trace driven simulations that AdaPort consistently outperforms state-of-the-art baseline methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eVR/AR\u8fb9\u7f18\u6e32\u67d3\u7684\u6df7\u5408\u53cd\u9988\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5AdaPort\uff0c\u901a\u8fc7\u7ed3\u5408\u5168\u4fe1\u606f\u53cd\u9988\u548c\u8d4c\u535a\u673a\u53cd\u9988\u6765\u4f18\u5316\u89c6\u53e3\u9884\u6d4b\u548c\u4f20\u8f93\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u7387\u3002", "motivation": "VR/AR\u5e94\u7528\u5bf9\u5e27\u7387\u3001\u5ef6\u8fdf\u548c\u7269\u7406\u865a\u62df\u73af\u5883\u540c\u6b65\u6709\u4e25\u683c\u8981\u6c42\u3002\u73b0\u6709\u65b9\u6cd5\u5c06\u89c6\u53e3\u9009\u62e9\u95ee\u9898\u5efa\u6a21\u4e3a\u4e24\u7ea7\u8d4c\u535a\u673a\u53cd\u9988\uff0c\u4f46\u5ffd\u7565\u4e86\u9884\u6d4b\u53cd\u9988\u53ef\u4ee5\u5728\u89c2\u5bdf\u5230\u7528\u6237\u5934\u90e8\u59ff\u6001\u540e\u5bf9\u6240\u6709\u5019\u9009\u89c6\u53e3\u8fdb\u884c\u56de\u6eaf\u8ba1\u7b97\uff0c\u8fd9\u5b9e\u9645\u4e0a\u662f\u5168\u4fe1\u606f\u53cd\u9988\u800c\u975e\u8d4c\u535a\u673a\u53cd\u9988\u3002", "method": "\u63d0\u51fa\u4e24\u7ea7\u6df7\u5408\u53cd\u9988\u6a21\u578b\uff0c\u7ed3\u5408\u5168\u4fe1\u606f\u53cd\u9988\u548c\u8d4c\u535a\u673a\u53cd\u9988\u3002\u8bbe\u8ba1\u4e86AdaPort\u6df7\u5408\u5b66\u4e60\u7b97\u6cd5\uff0c\u5229\u7528\u4e24\u79cd\u53cd\u9988\u7c7b\u578b\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002\u63a8\u5bfc\u4e86\u6df7\u5408\u53cd\u9988\u6a21\u578b\u7684\u5b9e\u4f8b\u76f8\u5173\u9057\u61be\u4e0b\u754c\uff0c\u5e76\u5efa\u7acb\u4e86\u4e0e\u4e4b\u6e10\u8fd1\u5339\u914d\u7684\u9057\u61be\u4e0a\u754c\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eAdaPort\u7684\u9057\u61be\u4e0a\u754c\u4e0e\u4e0b\u754c\u6e10\u8fd1\u5339\u914d\u3002\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u8f68\u8ff9\u7684\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\uff0cAdaPort\u5728\u6027\u80fd\u4e0a\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u8bc6\u522b\u9884\u6d4b\u53cd\u9988\u7684\u5168\u4fe1\u606f\u7279\u6027\u5e76\u5efa\u7acb\u6df7\u5408\u53cd\u9988\u6a21\u578b\uff0cAdaPort\u7b97\u6cd5\u5728VR/AR\u8fb9\u7f18\u6e32\u67d3\u7684\u89c6\u53e3\u9009\u62e9\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u5b66\u4e60\uff0c\u4e3a\u6c89\u6d78\u5f0f\u5e94\u7528\u7684\u5b9e\u65f6\u6e32\u67d3\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08393", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.08393", "abs": "https://arxiv.org/abs/2602.08393", "authors": ["Alok Shukla", "Prakash Vedula"], "title": "Quantum Detection of Sequency-Band Structure", "comment": "21 pages", "summary": "We present a quantum algorithm for estimating the amplitude content of user-specified sequency bands in quantum-encoded signals. The method employs a sequency-ordered Quantum Walsh-Hadamard Transform (QWHT), a comparator-based oracle that coherently marks basis states within an arbitrary sequency range, and Quantum Amplitude Estimation (QAE) to estimate the total probability mass in the selected band. This enables the detection of structured signal components, including both high- and low-sequency features, as well as the identification of rapid sign-change behavior associated with noise or anomalies. The proposed method can be embedded as a module within a larger quantum algorithm; in this setting, both the input and output remain fully quantum, enabling seamless integration with upstream and downstream quantum operations. We show that the sequency-ordered QWHT can be implemented with circuit depth $O(\\log_2 N)$ (equivalently $O(n)$ for $N=2^n$) when acting on an amplitude-encoded quantum state, whereas computing the full Walsh-Hadamard spectrum of an explicit length-$N$ classical signal requires $O(N\\log_2 N)$ operations via the fast Walsh-Hadamard transform. This results in an exponential quantum advantage when the QWHT is used as a modular block within a larger quantum algorithm, relative to classical fast Walsh-Hadamard transform-based approaches operating on explicit data. From an application perspective, the proposed sequency band-energy estimation may be interpreted as a structure-based anomaly indicator, enabling the detection of unexpected high-sequency components relative to a nominal low-sequency signal class. The algorithm is applicable to quantum-enhanced signal processing tasks such as zero-crossing analysis, band-limited noise estimation, and feature extraction in the Walsh basis.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u91cf\u5b50\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u91cf\u5b50\u7f16\u7801\u4fe1\u53f7\u4e2d\u7528\u6237\u6307\u5b9a\u5e8f\u7387\u5e26\u7684\u5e45\u5ea6\u5185\u5bb9\uff0c\u901a\u8fc7\u5e8f\u7387\u6392\u5e8f\u7684\u91cf\u5b50Walsh-Hadamard\u53d8\u6362\u3001\u6bd4\u8f83\u5668oracle\u548c\u91cf\u5b50\u5e45\u5ea6\u4f30\u8ba1\u5b9e\u73b0\uff0c\u5728\u91cf\u5b50\u7b97\u6cd5\u4e2d\u5177\u6709\u6307\u6570\u7ea7\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edf\u4fe1\u53f7\u5904\u7406\u4e2d\uff0cWalsh-Hadamard\u53d8\u6362\u9700\u8981O(Nlog\u2082N)\u64cd\u4f5c\uff0c\u800c\u91cf\u5b50\u7b97\u6cd5\u53ef\u4ee5\u5728\u91cf\u5b50\u6001\u4e0a\u5b9e\u73b0\u6307\u6570\u52a0\u901f\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u91cf\u5b50\u7b97\u6cd5\u4e2d\u65e0\u7f1d\u96c6\u6210\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u4fe1\u53f7\u7684\u7ed3\u6784\u5316\u6210\u5206\u548c\u5f02\u5e38\u3002", "method": "\u4f7f\u7528\u5e8f\u7387\u6392\u5e8f\u7684\u91cf\u5b50Walsh-Hadamard\u53d8\u6362\uff08\u7535\u8def\u6df1\u5ea6O(log\u2082N)\uff09\uff0c\u7ed3\u5408\u6bd4\u8f83\u5668oracle\u76f8\u5e72\u6807\u8bb0\u4efb\u610f\u5e8f\u7387\u8303\u56f4\u5185\u7684\u57fa\u6001\uff0c\u7136\u540e\u5e94\u7528\u91cf\u5b50\u5e45\u5ea6\u4f30\u8ba1\u6765\u4f30\u8ba1\u9009\u5b9a\u5e26\u4e2d\u7684\u603b\u6982\u7387\u8d28\u91cf\u3002", "result": "\u91cf\u5b50Walsh-Hadamard\u53d8\u6362\u5728\u632f\u5e45\u7f16\u7801\u91cf\u5b50\u6001\u4e0a\u7684\u7535\u8def\u6df1\u5ea6\u4e3aO(log\u2082N)\uff0c\u76f8\u6bd4\u7ecf\u5178\u5feb\u901fWalsh-Hadamard\u53d8\u6362\u7684O(Nlog\u2082N)\u64cd\u4f5c\uff0c\u5728\u91cf\u5b50\u7b97\u6cd5\u4e2d\u5b9e\u73b0\u4e86\u6307\u6570\u7ea7\u4f18\u52bf\u3002\u7b97\u6cd5\u53ef\u4f5c\u4e3a\u7ed3\u6784\u5f02\u5e38\u6307\u793a\u5668\u3002", "conclusion": "\u8be5\u91cf\u5b50\u7b97\u6cd5\u4e3a\u91cf\u5b50\u589e\u5f3a\u4fe1\u53f7\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u80fd\u591f\u68c0\u6d4b\u9ad8\u4f4e\u5e8f\u7387\u7279\u5f81\u548c\u5f02\u5e38\uff0c\u5728\u91cf\u5b50\u7b97\u6cd5\u4e2d\u5177\u6709\u6307\u6570\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u96f6\u4ea4\u53c9\u5206\u6790\u3001\u5e26\u9650\u566a\u58f0\u4f30\u8ba1\u548cWalsh\u57fa\u7279\u5f81\u63d0\u53d6\u7b49\u4efb\u52a1\u3002"}}
{"id": "2602.07278", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07278", "abs": "https://arxiv.org/abs/2602.07278", "authors": ["Sai Vamsi Alisetti"], "title": "Laplacian-LoRA: Delaying Oversmoothing in Deep GCNs via Spectral Low-Rank Adaptation", "comment": "4 pages", "summary": "Oversmoothing is a fundamental limitation of deep graph convolutional networks (GCNs), causing node representations to collapse as depth increases. While many prior approaches mitigate this effect through architectural modifications or residual mechanisms, the underlying spectral cause of oversmoothing is often left implicit. We propose Laplacian-LoRA, a simple and interpretable low-rank spectral adaptation of standard GCNs. Rather than redesigning message passing, Laplacian-LoRA introduces a learnable, spectrally anchored correction to the fixed Laplacian propagation operator, selectively weakening contraction while preserving stability and the low-pass inductive bias. Across multiple benchmark datasets and depths, Laplacian-LoRA consistently delays the onset of oversmoothing, extending the effective depth of GCNs by up to a factor of two. Embedding variance diagnostics confirm that these gains arise from delayed representational collapse, while learned spectral analysis demonstrates that the correction is smooth, bounded, and well behaved. Our results show that oversmoothing is a depth-dependent spectral phenomenon that can be systematically delayed through modest, low-rank adaptation of the graph propagation operator.", "AI": {"tldr": "\u63d0\u51faLaplacian-LoRA\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u79e9\u8c31\u9002\u5e94\u5ef6\u8fdfGCN\u7684\u8fc7\u5e73\u6ed1\u73b0\u8c61\uff0c\u5c06\u6709\u6548\u6df1\u5ea6\u63d0\u5347\u6700\u591a\u4e24\u500d", "motivation": "\u6df1\u5ea6\u56fe\u5377\u79ef\u7f51\u7edc\u5b58\u5728\u8fc7\u5e73\u6ed1\u95ee\u9898\uff0c\u5bfc\u81f4\u8282\u70b9\u8868\u793a\u968f\u6df1\u5ea6\u589e\u52a0\u800c\u574d\u7f29\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u901a\u8fc7\u67b6\u6784\u4fee\u6539\u6216\u6b8b\u5dee\u673a\u5236\u7f13\u89e3\uff0c\u4f46\u8fc7\u5e73\u6ed1\u7684\u8c31\u539f\u56e0\u5f80\u5f80\u672a\u660e\u786e\u63ed\u793a\u3002", "method": "\u63d0\u51faLaplacian-LoRA\u65b9\u6cd5\uff0c\u5bf9\u6807\u51c6GCN\u8fdb\u884c\u7b80\u5355\u53ef\u89e3\u91ca\u7684\u4f4e\u79e9\u8c31\u9002\u5e94\u3002\u4e0d\u91cd\u65b0\u8bbe\u8ba1\u6d88\u606f\u4f20\u9012\uff0c\u800c\u662f\u4e3a\u56fa\u5b9a\u7684\u62c9\u666e\u62c9\u65af\u4f20\u64ad\u7b97\u5b50\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u8c31\u951a\u5b9a\u6821\u6b63\uff0c\u9009\u62e9\u6027\u5730\u51cf\u5f31\u6536\u7f29\u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u6027\u548c\u4f4e\u901a\u5f52\u7eb3\u504f\u7f6e\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u6df1\u5ea6\u4e0b\uff0cLaplacian-LoRA\u80fd\u4e00\u81f4\u5ef6\u8fdf\u8fc7\u5e73\u6ed1\u7684\u53d1\u751f\uff0c\u5c06GCN\u7684\u6709\u6548\u6df1\u5ea6\u63d0\u5347\u6700\u591a\u4e24\u500d\u3002\u5d4c\u5165\u65b9\u5dee\u8bca\u65ad\u786e\u8ba4\u8fd9\u4e9b\u589e\u76ca\u6765\u81ea\u5ef6\u8fdf\u7684\u8868\u793a\u574d\u7f29\uff0c\u5b66\u4e60\u8c31\u5206\u6790\u663e\u793a\u6821\u6b63\u662f\u5e73\u6ed1\u3001\u6709\u754c\u4e14\u884c\u4e3a\u826f\u597d\u7684\u3002", "conclusion": "\u8fc7\u5e73\u6ed1\u662f\u4e00\u79cd\u6df1\u5ea6\u4f9d\u8d56\u7684\u8c31\u73b0\u8c61\uff0c\u53ef\u4ee5\u901a\u8fc7\u5bf9\u56fe\u4f20\u64ad\u7b97\u5b50\u8fdb\u884c\u9002\u5ea6\u7684\u4f4e\u79e9\u9002\u5e94\u6765\u7cfb\u7edf\u6027\u5730\u5ef6\u8fdf\u3002"}}
{"id": "2602.08394", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.08394", "abs": "https://arxiv.org/abs/2602.08394", "authors": ["Apurav", "Jaskaran Singh"], "title": "Efficient circuit compression by multi-qudit entangling gates in linear optical quantum computation", "comment": "5+9 pages, 5 figures. Comments are welcome!", "summary": "Linear optical quantum computation (LOQC) offers a promising platform for scalable quantum information processing, but its scalability is fundamentally constrained by the probabilistic nature of non-local entangling gates. Qudit circuit compression schemes mitigate this issue by encoding multiple qubits onto qudits. However, these schemes become inefficient when only a subset of the encoded qubits is required to participate in the non-local entangling gate, leading to an exponential increase in the number of non-local gates. In this Letter, we address this bottleneck by demonstrating the existence of multi-level control-Z (CZ) gates for qudits encoded in multiple spatial modes in LOQC. Unlike conventional two-level CZ gates, which act only on a single pair of modes, multi-level CZ gates impart a conditional phase shift for an arbitrarily chosen subset of the spatial modes. We present two explicit linear optical schemes that realize such operations, illustrating a fundamental trade-off between prior information about the input quantum state and the physical resources required. The first scheme is realized with a constant success probability of $1/8$ independent of the qudit dimension using a single non-local entangling gate, at the cost of state dependence, which is significantly better than the current success probability of $1/9$. Our second scheme provides a fully state independent realization reducing the number of non-local gates to $\\mathcal{O}(2^{r_1}+2^{r_2})$ as compared to the existing bound of $\\mathcal{O}(2^{r_1+r_2})$ where $r_1$ and $r_2$ are the number of qubits to be removed as control in the qudits. The success probability of the realization is $\\frac{1}{2} \\left(\\frac{1}{8}\\right)^{2^{r_1}+2^{r_2}}$. When combined with qudit circuit compression schemes, our results improve upon a key scalability limitation and significantly improve the efficiency of LOQC architectures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u591a\u7ea7\u63a7\u5236-Z\u95e8\u6765\u89e3\u51b3\u7ebf\u6027\u5149\u5b66\u91cf\u5b50\u8ba1\u7b97\u4e2d\u975e\u5c40\u57df\u7ea0\u7f20\u95e8\u7684\u6982\u7387\u6027\u9650\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u4e24\u79cd\u65b9\u6848\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u6210\u529f\u7387\u3002", "motivation": "\u7ebf\u6027\u5149\u5b66\u91cf\u5b50\u8ba1\u7b97\uff08LOQC\uff09\u7684\u53ef\u6269\u5c55\u6027\u53d7\u5230\u975e\u5c40\u57df\u7ea0\u7f20\u95e8\u6982\u7387\u6027\u7684\u9650\u5236\u3002\u73b0\u6709\u7684qudit\u7535\u8def\u538b\u7f29\u65b9\u6848\u5728\u53ea\u6709\u90e8\u5206\u7f16\u7801qubit\u53c2\u4e0e\u975e\u5c40\u57df\u7ea0\u7f20\u95e8\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u5bfc\u81f4\u975e\u5c40\u57df\u95e8\u6570\u91cf\u6307\u6570\u7ea7\u589e\u52a0\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u7ea7\u63a7\u5236-Z\u95e8\u7684\u6982\u5ff5\uff0c\u4e0d\u540c\u4e8e\u4f20\u7edf\u7684\u4e24\u7ea7CZ\u95e8\uff0c\u5b83\u53ef\u4ee5\u5bf9\u4efb\u610f\u9009\u62e9\u7684\u5b50\u96c6\u7a7a\u95f4\u6a21\u5f0f\u65bd\u52a0\u6761\u4ef6\u76f8\u79fb\u3002\u63d0\u51fa\u4e86\u4e24\u79cd\u5177\u4f53\u7684\u7ebf\u6027\u5149\u5b66\u5b9e\u73b0\u65b9\u6848\uff1a\u7b2c\u4e00\u79cd\u65b9\u6848\u4ee51/8\u7684\u6052\u5b9a\u6210\u529f\u7387\u5b9e\u73b0\uff0c\u4f46\u5177\u6709\u72b6\u6001\u4f9d\u8d56\u6027\uff1b\u7b2c\u4e8c\u79cd\u65b9\u6848\u63d0\u4f9b\u5b8c\u5168\u72b6\u6001\u72ec\u7acb\u7684\u5b9e\u73b0\uff0c\u51cf\u5c11\u4e86\u975e\u5c40\u57df\u95e8\u7684\u6570\u91cf\u3002", "result": "\u7b2c\u4e00\u79cd\u65b9\u6848\u4ee51/8\u7684\u6052\u5b9a\u6210\u529f\u7387\u5b9e\u73b0\uff0c\u4f18\u4e8e\u73b0\u6709\u76841/9\u6210\u529f\u7387\u3002\u7b2c\u4e8c\u79cd\u65b9\u6848\u5c06\u975e\u5c40\u57df\u95e8\u6570\u91cf\u4eceO(2^{r1+r2})\u51cf\u5c11\u5230O(2^{r1}+2^{r2})\uff0c\u6210\u529f\u7387\u4e3a1/2*(1/8)^{2^{r1}+2^{r2}}\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u591a\u7ea7\u63a7\u5236-Z\u95e8\u89e3\u51b3\u4e86LOQC\u7684\u5173\u952e\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u4e0equdit\u7535\u8def\u538b\u7f29\u65b9\u6848\u7ed3\u5408\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LOQC\u67b6\u6784\u7684\u6548\u7387\u3002"}}
{"id": "2602.07279", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07279", "abs": "https://arxiv.org/abs/2602.07279", "authors": ["Bruno Belucci", "Karim Lounici", "Vladimir R. Kostic", "Katia Meziani"], "title": "VertCoHiRF: Decentralized Vertical Clustering Beyond k-means", "comment": null, "summary": "Vertical Federated Learning (VFL) enables collaborative analysis across parties holding complementary feature views of the same samples, yet existing approaches are largely restricted to distributed variants of $k$-means, requiring centralized coordination or the exchange of feature-dependent numerical statistics, and exhibiting limited robustness under heterogeneous views or adversarial behavior. We introduce VertCoHiRF, a fully decentralized framework for vertical federated clustering based on structural consensus across heterogeneous views, allowing each agent to apply a base clustering method adapted to its local feature space in a peer-to-peer manner. Rather than exchanging feature-dependent statistics or relying on noise injection for privacy, agents cluster their local views independently and reconcile their proposals through identifier-level consensus. Consensus is achieved via decentralized ordinal ranking to select representative medoids, progressively inducing a shared hierarchical clustering across agents. Communication is limited to sample identifiers, cluster labels, and ordinal rankings, providing privacy by design while supporting overlapping feature partitions and heterogeneous local clustering methods, and yielding an interpretable shared Cluster Fusion Hierarchy (CFH) that captures cross-view agreement at multiple resolutions.We analyze communication complexity and robustness, and experiments demonstrate competitive clustering performance in vertical federated settings.", "AI": {"tldr": "VertCoHiRF\uff1a\u57fa\u4e8e\u5f02\u6784\u89c6\u56fe\u7ed3\u6784\u5171\u8bc6\u7684\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u5782\u76f4\u8054\u90a6\u805a\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u6807\u8bc6\u7b26\u7ea7\u5171\u8bc6\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\uff0c\u65e0\u9700\u4ea4\u6362\u7279\u5f81\u76f8\u5173\u7edf\u8ba1\u91cf", "motivation": "\u73b0\u6709\u5782\u76f4\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8ek-means\u7684\u5206\u5e03\u5f0f\u53d8\u4f53\uff0c\u9700\u8981\u4e2d\u5fc3\u5316\u534f\u8c03\u6216\u4ea4\u6362\u7279\u5f81\u76f8\u5173\u6570\u503c\u7edf\u8ba1\uff0c\u5728\u5f02\u6784\u89c6\u56fe\u6216\u5bf9\u6297\u884c\u4e3a\u4e0b\u9c81\u68d2\u6027\u6709\u9650", "method": "\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u6846\u67b6\uff0c\u5404\u4ee3\u7406\u4f7f\u7528\u9002\u5408\u672c\u5730\u7279\u5f81\u7a7a\u95f4\u7684\u57fa\u805a\u7c7b\u65b9\u6cd5\u72ec\u7acb\u805a\u7c7b\uff0c\u901a\u8fc7\u6807\u8bc6\u7b26\u7ea7\u5171\u8bc6\u534f\u8c03\u63d0\u6848\uff0c\u91c7\u7528\u53bb\u4e2d\u5fc3\u5316\u5e8f\u6570\u6392\u5e8f\u9009\u62e9\u4ee3\u8868\u6027\u4e2d\u5fc3\u70b9\uff0c\u9010\u6b65\u8bf1\u5bfc\u5171\u4eab\u5c42\u6b21\u805a\u7c7b", "result": "\u901a\u4fe1\u4ec5\u9650\u4e8e\u6837\u672c\u6807\u8bc6\u7b26\u3001\u805a\u7c7b\u6807\u7b7e\u548c\u5e8f\u6570\u6392\u5e8f\uff0c\u63d0\u4f9b\u9690\u79c1\u4fdd\u62a4\u8bbe\u8ba1\uff0c\u652f\u6301\u91cd\u53e0\u7279\u5f81\u5206\u533a\u548c\u5f02\u6784\u672c\u5730\u805a\u7c7b\u65b9\u6cd5\uff0c\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u5171\u4eab\u805a\u7c7b\u878d\u5408\u5c42\u6b21\u7ed3\u6784", "conclusion": "VertCoHiRF\u5728\u5782\u76f4\u8054\u90a6\u8bbe\u7f6e\u4e2d\u5c55\u73b0\u51fa\u7ade\u4e89\u529b\u7684\u805a\u7c7b\u6027\u80fd\uff0c\u540c\u65f6\u5206\u6790\u4e86\u901a\u4fe1\u590d\u6742\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u5f02\u6784\u89c6\u56fe\u4e0b\u7684\u8054\u90a6\u805a\u7c7b\u63d0\u4f9b\u4e86\u9690\u79c1\u4fdd\u62a4\u7684\u53bb\u4e2d\u5fc3\u5316\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.08410", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.08410", "abs": "https://arxiv.org/abs/2602.08410", "authors": ["P\u00e9ter L\u00e9vay", "Metod Saniga"], "title": "The Finite Geometry of Breaking Quantum Secrets", "comment": "27 pages, 4 figures", "summary": "Using a finite geometric framework for studying the pentagon and heptagon codes we show that the concepts of quantum secret sharing and contextuality can be studied in a nice and unified manner. The basic idea is a careful study of the respective $2+3$ and $3+4$ tensorial factorizations of the elements of the stabilizer groups of these codes. It is demonstrated in detail how finite geometric structures entailing a specific three-qubit (resp. four-qubit) embedding of binary symplectic polar spaces of rank two (resp. three), corresponding to these factorizations, govern issues of contextuality and entanglement needed for a geometric understanding of quantum secret sharing. Using these results for the $(3,5)$ and $(4,7)$ threshold schemes explicit secret breaking protocols are derived. Our results hint at a novel geometric way of looking at contextual configurations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u6709\u9650\u51e0\u4f55\u6846\u67b6\u7814\u7a76\u4e94\u8fb9\u5f62\u548c\u4e03\u8fb9\u5f62\u91cf\u5b50\u7801\uff0c\u63ed\u793a\u4e86\u91cf\u5b50\u79d8\u5bc6\u5171\u4eab\u4e0e\u4e0a\u4e0b\u6587\u6027\u4e4b\u95f4\u7684\u7edf\u4e00\u5173\u7cfb\uff0c\u5e76\u57fa\u4e8e\u6b64\u63a8\u5bfc\u4e86\u5177\u4f53\u7684\u79d8\u5bc6\u7834\u89e3\u534f\u8bae\u3002", "motivation": "\u7814\u7a76\u91cf\u5b50\u79d8\u5bc6\u5171\u4eab\u4e0e\u4e0a\u4e0b\u6587\u6027\u4e4b\u95f4\u7684\u5185\u5728\u8054\u7cfb\uff0c\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u6709\u9650\u51e0\u4f55\u6846\u67b6\u7edf\u4e00\u7406\u89e3\u8fd9\u4e24\u4e2a\u6982\u5ff5\uff0c\u4e3a\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u63d0\u4f9b\u65b0\u7684\u51e0\u4f55\u89c6\u89d2\u3002", "method": "\u91c7\u7528\u6709\u9650\u51e0\u4f55\u6846\u67b6\u5206\u6790\u4e94\u8fb9\u5f62\u548c\u4e03\u8fb9\u5f62\u91cf\u5b50\u7801\uff0c\u91cd\u70b9\u7814\u7a76\u5176\u7a33\u5b9a\u5b50\u7fa4\u76842+3\u548c3+4\u5f20\u91cf\u5206\u89e3\uff0c\u6784\u5efa\u5bf9\u5e94\u7684\u4e09\u91cf\u5b50\u6bd4\u7279\u548c\u56db\u91cf\u5b50\u6bd4\u7279\u5d4c\u5165\u7684\u4e8c\u5143\u8f9b\u6781\u7a7a\u95f4\u7ed3\u6784\u3002", "result": "\u5efa\u7acb\u4e86\u91cf\u5b50\u79d8\u5bc6\u5171\u4eab\u4e0e\u4e0a\u4e0b\u6587\u6027\u4e4b\u95f4\u7684\u51e0\u4f55\u8054\u7cfb\uff0c\u5c55\u793a\u4e86\u7279\u5b9a\u91cf\u5b50\u6bd4\u7279\u5d4c\u5165\u5982\u4f55\u63a7\u5236\u4e0a\u4e0b\u6587\u6027\u548c\u7ea0\u7f20\u7279\u6027\uff0c\u5e76\u9488\u5bf9(3,5)\u548c(4,7)\u95e8\u9650\u65b9\u6848\u63a8\u5bfc\u4e86\u5177\u4f53\u7684\u79d8\u5bc6\u7834\u89e3\u534f\u8bae\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u4e0a\u4e0b\u6587\u6027\u914d\u7f6e\u63d0\u4f9b\u4e86\u65b0\u9896\u7684\u51e0\u4f55\u89c6\u89d2\uff0c\u8868\u660e\u6709\u9650\u51e0\u4f55\u6846\u67b6\u80fd\u591f\u7edf\u4e00\u7814\u7a76\u91cf\u5b50\u79d8\u5bc6\u5171\u4eab\u548c\u4e0a\u4e0b\u6587\u6027\uff0c\u4e3a\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2602.07285", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07285", "abs": "https://arxiv.org/abs/2602.07285", "authors": ["Etam Benger", "Katrina Ligett"], "title": "Fair Decisions from Calibrated Scores: Achieving Optimal Classification While Satisfying Sufficiency", "comment": null, "summary": "Binary classification based on predicted probabilities (scores) is a fundamental task in supervised machine learning. While thresholding scores is Bayes-optimal in the unconstrained setting, using a single threshold generally violates statistical group fairness constraints. Under independence (statistical parity) and separation (equalized odds), such thresholding suffices when the scores already satisfy the corresponding criterion. However, this does not extend to sufficiency: even perfectly group-calibrated scores -- including true class probabilities -- violate predictive parity after thresholding. In this work, we present an exact solution for optimal binary (randomized) classification under sufficiency, assuming finite sets of group-calibrated scores. We provide a geometric characterization of the feasible pairs of positive predictive value (PPV) and false omission rate (FOR) achievable by such classifiers, and use it to derive a simple post-processing algorithm that attains the optimal classifier using only group-calibrated scores and group membership. Finally, since sufficiency and separation are generally incompatible, we identify the classifier that minimizes deviation from separation subject to sufficiency, and show that it can also be obtained by our algorithm, often achieving performance comparable to the optimum.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6ee1\u8db3\u5145\u5206\u6027\u516c\u5e73\u7ea6\u675f\u4e0b\u8fdb\u884c\u6700\u4f18\u4e8c\u5143\u5206\u7c7b\u7684\u7cbe\u786e\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ec\u51e0\u4f55\u7279\u5f81\u63cf\u8ff0\u548c\u7b80\u5355\u540e\u5904\u7406\u7b97\u6cd5\uff0c\u5e76\u89e3\u51b3\u4e86\u5145\u5206\u6027\u4e0e\u5206\u79bb\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u5728\u4e8c\u5143\u5206\u7c7b\u4e2d\uff0c\u57fa\u4e8e\u9884\u6d4b\u6982\u7387\u7684\u9608\u503c\u5212\u5206\u5728\u65e0\u7ea6\u675f\u60c5\u51b5\u4e0b\u662f\u8d1d\u53f6\u65af\u6700\u4f18\u7684\uff0c\u4f46\u4f7f\u7528\u5355\u4e00\u9608\u503c\u901a\u5e38\u4f1a\u8fdd\u53cd\u7edf\u8ba1\u7fa4\u4f53\u516c\u5e73\u7ea6\u675f\u3002\u867d\u7136\u72ec\u7acb\u6027\u548c\u5206\u79bb\u6027\u7ea6\u675f\u4e0b\uff0c\u6ee1\u8db3\u76f8\u5e94\u6807\u51c6\u7684\u5206\u6570\u9608\u503c\u5212\u5206\u5c31\u8db3\u591f\u4e86\uff0c\u4f46\u5145\u5206\u6027\u7ea6\u675f\u4e0b\uff0c\u5373\u4f7f\u662f\u5b8c\u7f8e\u7fa4\u4f53\u6821\u51c6\u7684\u5206\u6570\uff08\u5305\u62ec\u771f\u5b9e\u7c7b\u522b\u6982\u7387\uff09\u5728\u9608\u503c\u5212\u5206\u540e\u4e5f\u4f1a\u8fdd\u53cd\u9884\u6d4b\u5947\u5076\u6027\u3002\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u5728\u5145\u5206\u6027\u7ea6\u675f\u4e0b\u7684\u6700\u4f18\u5206\u7c7b\u95ee\u9898\u3002", "method": "1. \u5047\u8bbe\u6709\u9650\u7ec4\u6821\u51c6\u5206\u6570\u96c6\uff0c\u63d0\u51fa\u5728\u5145\u5206\u6027\u7ea6\u675f\u4e0b\u6700\u4f18\u4e8c\u5143\uff08\u968f\u673a\u5316\uff09\u5206\u7c7b\u7684\u7cbe\u786e\u89e3\u51b3\u65b9\u6848\uff1b2. \u63d0\u4f9b\u53ef\u5b9e\u73b0\u7684\u6b63\u9884\u6d4b\u503c\uff08PPV\uff09\u548c\u5047\u9057\u6f0f\u7387\uff08FOR\uff09\u5bf9\u7684\u53ef\u5b9e\u73b0\u51e0\u4f55\u7279\u5f81\u63cf\u8ff0\uff1b3. \u63a8\u5bfc\u51fa\u4ec5\u4f7f\u7528\u7ec4\u6821\u51c6\u5206\u6570\u548c\u7ec4\u6210\u5458\u8eab\u4efd\u7684\u7b80\u5355\u540e\u5904\u7406\u7b97\u6cd5\u6765\u83b7\u5f97\u6700\u4f18\u5206\u7c7b\u5668\uff1b4. \u8bc6\u522b\u5728\u6ee1\u8db3\u5145\u5206\u6027\u7ea6\u675f\u4e0b\u6700\u5c0f\u5316\u4e0e\u5206\u79bb\u6027\u504f\u5dee\u7684\u5206\u7c7b\u5668\u3002", "result": "1. \u83b7\u5f97\u4e86\u5728\u5145\u5206\u6027\u516c\u5e73\u7ea6\u675f\u4e0b\u6700\u4f18\u4e8c\u5143\u5206\u7c7b\u7684\u7cbe\u786e\u89e3\u51b3\u65b9\u6848\uff1b2. \u5f00\u53d1\u4e86\u80fd\u591f\u83b7\u5f97\u6700\u4f18\u5206\u7c7b\u5668\u7684\u7b80\u5355\u540e\u5904\u7406\u7b97\u6cd5\uff1b3. \u89e3\u51b3\u4e86\u5145\u5206\u6027\u4e0e\u5206\u79bb\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u627e\u5230\u4e86\u5728\u6ee1\u8db3\u5145\u5206\u6027\u7ea6\u675f\u4e0b\u6700\u5c0f\u5316\u4e0e\u5206\u79bb\u6027\u504f\u5dee\u7684\u5206\u7c7b\u5668\uff1b4. \u8be5\u7b97\u6cd5\u901a\u5e38\u80fd\u8fbe\u5230\u4e0e\u6700\u4f18\u6027\u80fd\u76f8\u5f53\u7684\u8868\u73b0\u3002", "conclusion": "\u672c\u6587\u89e3\u51b3\u4e86\u5728\u5145\u5206\u6027\u516c\u5e73\u7ea6\u675f\u4e0b\u7684\u4e8c\u5143\u5206\u7c7b\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u7cbe\u786e\u89e3\u51b3\u65b9\u6848\u548c\u5b9e\u7528\u7b97\u6cd5\uff0c\u540c\u65f6\u5904\u7406\u4e86\u5145\u5206\u6027\u4e0e\u5206\u79bb\u6027\u4e4b\u95f4\u7684\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u4e3a\u516c\u5e73\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2602.08418", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.08418", "abs": "https://arxiv.org/abs/2602.08418", "authors": ["Maximilian Hess", "Lilly Palackal", "Abhishek Awasthi", "Peter J. Eder", "Manuel Schnaus", "Laurin Demmler", "Karen Wintersperger", "Joseph Doetsch"], "title": "Grover Adaptive Search with Problem-Specific State Preparation", "comment": null, "summary": "Grover's search algorithm is one of the basic building block in the world of quantum algorithms. Successfully applying it to combinatorial optimization problems is a subtle challenge. As a quadratic speedup is not enough to naively search an exponentially large space, the search has to be complemented with a state preparation routine which increases the amplitudes of promising states by exploiting the problem structure. In this paper, we build upon previous work by Baertschi and Eidenbenz to construct heuristic state preparation routines for the Traveling Salesperson Problem (TSP), mimicking the well-known classical Lin-Kernighan heuristic. With our heuristic, we aim to achieve a reasonable approximation ratio with only a polynomial number of Grover iterations. Further, we compare several algorithmic settings relating to termination criteria and the choice of Grover iterations when the number of marked solutions is unknown.", "AI": {"tldr": "\u672c\u6587\u57fa\u4e8eBaertschi\u548cEidenbenz\u7684\u5de5\u4f5c\uff0c\u4e3a\u65c5\u884c\u5546\u95ee\u9898\u6784\u5efa\u542f\u53d1\u5f0f\u72b6\u6001\u51c6\u5907\u4f8b\u7a0b\uff0c\u6a21\u4eff\u7ecf\u5178\u7684Lin-Kernighan\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u591a\u9879\u5f0f\u6b21\u6570\u7684Grover\u8fed\u4ee3\u83b7\u5f97\u5408\u7406\u7684\u8fd1\u4f3c\u6bd4\u3002", "motivation": "Grover\u641c\u7d22\u7b97\u6cd5\u662f\u91cf\u5b50\u7b97\u6cd5\u7684\u57fa\u672c\u6784\u5efa\u5757\uff0c\u4f46\u5c06\u5176\u6210\u529f\u5e94\u7528\u4e8e\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u662f\u4e00\u4e2a\u5fae\u5999\u6311\u6218\u3002\u7531\u4e8e\u4e8c\u6b21\u52a0\u901f\u4e0d\u8db3\u4ee5\u5728\u6307\u6570\u7ea7\u5927\u7684\u7a7a\u95f4\u4e2d\u6734\u7d20\u641c\u7d22\uff0c\u9700\u8981\u5229\u7528\u95ee\u9898\u7ed3\u6784\u8bbe\u8ba1\u72b6\u6001\u51c6\u5907\u4f8b\u7a0b\u6765\u589e\u52a0\u6709\u5e0c\u671b\u72b6\u6001\u7684\u632f\u5e45\u3002", "method": "\u57fa\u4e8eBaertschi\u548cEidenbenz\u7684\u5de5\u4f5c\uff0c\u4e3a\u65c5\u884c\u5546\u95ee\u9898\u6784\u5efa\u542f\u53d1\u5f0f\u72b6\u6001\u51c6\u5907\u4f8b\u7a0b\uff0c\u6a21\u4eff\u7ecf\u5178\u7684Lin-Kernighan\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002\u6bd4\u8f83\u4e86\u4e0e\u7ec8\u6b62\u6807\u51c6\u548c\u672a\u77e5\u6807\u8bb0\u89e3\u6570\u91cf\u65f6Grover\u8fed\u4ee3\u6b21\u6570\u9009\u62e9\u76f8\u5173\u7684\u51e0\u79cd\u7b97\u6cd5\u8bbe\u7f6e\u3002", "result": "\u901a\u8fc7\u542f\u53d1\u5f0f\u72b6\u6001\u51c6\u5907\uff0c\u65e8\u5728\u5b9e\u73b0\u4ec5\u4f7f\u7528\u591a\u9879\u5f0f\u6b21\u6570\u7684Grover\u8fed\u4ee3\u5c31\u80fd\u83b7\u5f97\u5408\u7406\u7684\u8fd1\u4f3c\u6bd4\u3002\u5bf9\u4e0d\u540c\u7684\u7b97\u6cd5\u8bbe\u7f6e\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\u3002", "conclusion": "\u901a\u8fc7\u5c06\u7ecf\u5178\u542f\u53d1\u5f0f\u7b97\u6cd5\uff08\u5982Lin-Kernighan\uff09\u4e0eGrover\u641c\u7d22\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u4e3a\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff08\u5982TSP\uff09\u6784\u5efa\u6709\u6548\u7684\u91cf\u5b50\u7b97\u6cd5\uff0c\u5728\u591a\u9879\u5f0f\u8fed\u4ee3\u6b21\u6570\u5185\u83b7\u5f97\u826f\u597d\u7684\u8fd1\u4f3c\u89e3\u3002"}}
{"id": "2602.07320", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07320", "abs": "https://arxiv.org/abs/2602.07320", "authors": ["Philip Jacobson", "Ben Feinberg", "Suhas Kumar", "Sapan Agarwal", "T. Patrick Xiao", "Christopher Bennett"], "title": "Incorruptible Neural Networks: Training Models that can Generalize to Large Internal Perturbations", "comment": null, "summary": "Flat regions of the neural network loss landscape have long been hypothesized to correlate with better generalization properties. A closely related but distinct problem is training models that are robust to internal perturbations to their weights, which may be an important need for future low-power hardware platforms. In this paper, we explore the usage of two methods, sharpness-aware minimization (SAM) and random-weight perturbation (RWP), to find minima robust to a variety of random corruptions to weights. We consider the problem from two angles: generalization (how do we reduce the noise-robust generalization gap) and optimization (how do we maximize performance from optimizers when subject to strong perturbations). First, we establish, both theoretically and empirically, that an over-regularized RWP training objective is optimal for noise-robust generalization. For small-magnitude noise, we find that SAM's adversarial objective further improves performance over any RWP configuration, but performs poorly for large-magnitude noise. We link the cause of this to a vanishing-gradient effect, caused by unevenness in the loss landscape, affecting both SAM and RWP. Lastly, we demonstrate that dynamically adjusting the perturbation strength to match the evolution of the loss landscape improves optimizing for these perturbed objectives.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316(SAM)\u548c\u968f\u673a\u6743\u91cd\u6270\u52a8(RWP)\u65b9\u6cd5\u8bad\u7ec3\u5bf9\u6743\u91cd\u6270\u52a8\u9c81\u68d2\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u5206\u6790\u4e86\u5176\u5728\u6cdb\u5316\u548c\u4f18\u5316\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u8fc7\u6b63\u5219\u5316RWP\u5728\u566a\u58f0\u9c81\u68d2\u6cdb\u5316\u65b9\u9762\u6700\u4f18\uff0cSAM\u5728\u5c0f\u566a\u58f0\u4e0b\u8868\u73b0\u66f4\u597d\u4f46\u5927\u566a\u58f0\u4e0b\u8f83\u5dee\uff0c\u5e76\u63d0\u51fa\u52a8\u6001\u8c03\u6574\u6270\u52a8\u5f3a\u5ea6\u53ef\u6539\u5584\u4f18\u5316\u6548\u679c\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u635f\u5931\u51fd\u6570\u7684\u5e73\u5766\u533a\u57df\u88ab\u8ba4\u4e3a\u4e0e\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u76f8\u5173\uff0c\u800c\u8bad\u7ec3\u5bf9\u6743\u91cd\u6270\u52a8\u9c81\u68d2\u7684\u6a21\u578b\u5bf9\u672a\u6765\u4f4e\u529f\u8017\u786c\u4ef6\u5e73\u53f0\u5f88\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u901a\u8fc7SAM\u548cRWP\u65b9\u6cd5\u627e\u5230\u5bf9\u5404\u79cd\u6743\u91cd\u968f\u673a\u6270\u52a8\u9c81\u68d2\u7684\u6700\u5c0f\u503c\u3002", "method": "\u91c7\u7528\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316(SAM)\u548c\u968f\u673a\u6743\u91cd\u6270\u52a8(RWP)\u4e24\u79cd\u65b9\u6cd5\uff0c\u4ece\u4e24\u4e2a\u89d2\u5ea6\u7814\u7a76\u95ee\u9898\uff1a1)\u6cdb\u5316\u89d2\u5ea6\uff1a\u5982\u4f55\u51cf\u5c0f\u566a\u58f0\u9c81\u68d2\u6cdb\u5316\u5dee\u8ddd\uff1b2)\u4f18\u5316\u89d2\u5ea6\uff1a\u5728\u5f3a\u6270\u52a8\u4e0b\u5982\u4f55\u6700\u5927\u5316\u4f18\u5316\u5668\u6027\u80fd\u3002\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u76f8\u7ed3\u5408\uff0c\u5e76\u63a2\u7d22\u52a8\u6001\u8c03\u6574\u6270\u52a8\u5f3a\u5ea6\u7684\u65b9\u6cd5\u3002", "result": "1)\u8fc7\u6b63\u5219\u5316\u7684RWP\u8bad\u7ec3\u76ee\u6807\u5728\u566a\u58f0\u9c81\u68d2\u6cdb\u5316\u65b9\u9762\u6700\u4f18\uff1b2)\u5bf9\u4e8e\u5c0f\u5e45\u5ea6\u566a\u58f0\uff0cSAM\u7684\u5bf9\u6297\u6027\u76ee\u6807\u6bd4\u4efb\u4f55RWP\u914d\u7f6e\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u5728\u5927\u5e45\u5ea6\u566a\u58f0\u4e0b\u8868\u73b0\u8f83\u5dee\uff1b3)\u635f\u5931\u51fd\u6570\u7684\u4e0d\u5747\u5300\u6027\u5bfc\u81f4\u68af\u5ea6\u6d88\u5931\u6548\u5e94\uff0c\u5f71\u54cdSAM\u548cRWP\uff1b4)\u52a8\u6001\u8c03\u6574\u6270\u52a8\u5f3a\u5ea6\u4ee5\u5339\u914d\u635f\u5931\u51fd\u6570\u6f14\u5316\u53ef\u6539\u5584\u4f18\u5316\u6548\u679c\u3002", "conclusion": "\u8fc7\u6b63\u5219\u5316RWP\u662f\u566a\u58f0\u9c81\u68d2\u6cdb\u5316\u7684\u6700\u4f18\u65b9\u6cd5\uff0cSAM\u5728\u5c0f\u566a\u58f0\u4e0b\u6709\u4f18\u52bf\u4f46\u5927\u566a\u58f0\u4e0b\u53d7\u9650\uff0c\u635f\u5931\u51fd\u6570\u4e0d\u5747\u5300\u6027\u5bfc\u81f4\u7684\u68af\u5ea6\u6d88\u5931\u662f\u4e3b\u8981\u6311\u6218\uff0c\u52a8\u6001\u8c03\u6574\u6270\u52a8\u5f3a\u5ea6\u662f\u6539\u5584\u4f18\u5316\u7684\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2602.08441", "categories": ["quant-ph", "cs.CC", "math.CO", "math.RT"], "pdf": "https://arxiv.org/pdf/2602.08441", "abs": "https://arxiv.org/abs/2602.08441", "authors": ["Matthias Christandl", "Aram W. Harrow", "Greta Panova", "Pietro M. Posta", "Michael Walter"], "title": "Plethysm is in #BQP", "comment": "29 pages", "summary": "Some representation-theoretic multiplicities, such as the Kostka and the Littlewood-Richardson coefficients, admit a combinatorial interpretation that places their computation in the complexity class #P. Whether this holds more generally is considered an important open problem in mathematics and computer science, with relevance for geometric complexity theory and quantum information. Recent work has investigated the quantum complexity of particular multiplicities, such as the Kronecker coefficients and certain special cases of the plethysm coefficients.\n  Here, we show that a broad class of representation-theoretic multiplicities is in #BQP. In particular, our result implies that the plethysm coefficients are in #BQP, which was only known in special cases. It also implies all known results on the quantum complexity of previously studied coefficients as special cases, unifying, simplifying, and extending prior work. We obtain our result by multiple applications of the Schur transform. Recent work has improved its dependence on the local dimension, which is crucial for our work. We further describe a general approach for showing that representation-theoretic multiplicities are in #BQP that captures our approach as well as the approaches of prior work. We complement the above by showing that the same multiplicities are also naturally in GapP and obtain polynomial-time classical algorithms when certain parameters are fixed.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc1\u660e\u4e86\u4e00\u7c7b\u5e7f\u6cdb\u7684\u8868\u793a\u8bba\u91cd\u6570\uff08\u5305\u62ecplethysm\u7cfb\u6570\uff09\u5c5e\u4e8e\u91cf\u5b50\u590d\u6742\u5ea6\u7c7b#BQP\uff0c\u7edf\u4e00\u5e76\u6269\u5c55\u4e86\u5148\u524d\u5173\u4e8eKronecker\u7cfb\u6570\u7b49\u7279\u6b8a\u60c5\u51b5\u7684\u91cf\u5b50\u590d\u6742\u5ea6\u7814\u7a76\u3002", "motivation": "\u8868\u793a\u8bba\u91cd\u6570\uff08\u5982Kostka\u7cfb\u6570\u3001Littlewood-Richardson\u7cfb\u6570\uff09\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u662f\u6570\u5b66\u548c\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684\u91cd\u8981\u5f00\u653e\u95ee\u9898\uff0c\u5bf9\u51e0\u4f55\u590d\u6742\u6027\u7406\u8bba\u548c\u91cf\u5b50\u4fe1\u606f\u6709\u91cd\u8981\u610f\u4e49\u3002\u5148\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7279\u5b9a\u91cd\u6570\uff08\u5982Kronecker\u7cfb\u6570\uff09\u7684\u91cf\u5b50\u590d\u6742\u5ea6\uff0c\u9700\u8981\u66f4\u4e00\u822c\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u591a\u6b21\u5e94\u7528Schur\u53d8\u6362\uff0c\u5e76\u5229\u7528\u8fd1\u671f\u6539\u8fdb\u7684Schur\u53d8\u6362\u5728\u5c40\u90e8\u7ef4\u5ea6\u4e0a\u7684\u4f9d\u8d56\u5173\u7cfb\u3002\u8bba\u6587\u8fd8\u63cf\u8ff0\u4e86\u4e00\u4e2a\u5c55\u793a\u8868\u793a\u8bba\u91cd\u6570\u5c5e\u4e8e#BQP\u7684\u901a\u7528\u65b9\u6cd5\u6846\u67b6\u3002", "result": "\u8bc1\u660e\u4e86\u4e00\u7c7b\u5e7f\u6cdb\u7684\u8868\u793a\u8bba\u91cd\u6570\u5c5e\u4e8e\u91cf\u5b50\u590d\u6742\u5ea6\u7c7b#BQP\uff0c\u7279\u522b\u5305\u62ecplethysm\u7cfb\u6570\u3002\u8be5\u7ed3\u679c\u7edf\u4e00\u3001\u7b80\u5316\u5e76\u6269\u5c55\u4e86\u5148\u524d\u5173\u4e8eKronecker\u7cfb\u6570\u7b49\u7279\u6b8a\u60c5\u51b5\u7684\u91cf\u5b50\u590d\u6742\u5ea6\u7814\u7a76\u3002\u540c\u65f6\u8bc1\u660e\u8fd9\u4e9b\u91cd\u6570\u4e5f\u5c5e\u4e8eGapP\u7c7b\uff0c\u5e76\u5728\u67d0\u4e9b\u53c2\u6570\u56fa\u5b9a\u65f6\u5b58\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u7ecf\u5178\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8868\u793a\u8bba\u91cd\u6570\u7684\u91cf\u5b50\u8ba1\u7b97\u590d\u6742\u5ea6\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06plethysm\u7cfb\u6570\u7b49\u5e7f\u6cdb\u7c7b\u522b\u7684\u91cd\u6570\u7eb3\u5165#BQP\u590d\u6742\u5ea6\u7c7b\uff0c\u5efa\u7acb\u4e86\u91cf\u5b50\u8ba1\u7b97\u4e0e\u8868\u793a\u8bba\u4e4b\u95f4\u7684\u91cd\u8981\u8054\u7cfb\uff0c\u5e76\u4e3a\u51e0\u4f55\u590d\u6742\u6027\u7406\u8bba\u548c\u91cf\u5b50\u4fe1\u606f\u63d0\u4f9b\u4e86\u65b0\u7684\u8ba1\u7b97\u5de5\u5177\u3002"}}
{"id": "2602.07340", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07340", "abs": "https://arxiv.org/abs/2602.07340", "authors": ["Yonghui Yang", "Wenjian Tao", "Jilong Liu", "Xingyu Zhu", "Junfeng Fang", "Weibiao Huang", "Le Wu", "Richang Hong", "Tat-Sent Chua"], "title": "Revisiting Robustness for LLM Safety Alignment via Selective Geometry Control", "comment": null, "summary": "Safety alignment of large language models remains brittle under domain shift and noisy preference supervision. Most existing robust alignment methods focus on uncertainty in alignment data, while overlooking optimization-induced fragility in preference-based objectives. In this work, we revisit robustness for LLM safety alignment from an optimization geometry perspective, and argue that robustness failures cannot be addressed by data-centric methods alone. We propose ShaPO, a geometry-aware preference optimization framework that enforces worst-case alignment objectives via selective geometry control over alignment-critical parameter subspace. By avoiding uniform geometry constraints, ShaPO mitigates the over-regularization that can harm robustness under distribution shift. We instantiate ShaPO at two levels: token-level ShaPO stabilizes likelihood-based surrogate optimization, while reward-level ShaPO enforces reward-consistent optimization under noisy supervision. Across diverse safety benchmarks and noisy preference settings, ShaPO consistently improves safety robustness over popular preference optimization methods. Moreover, ShaPO composes cleanly with data-robust objectives, yielding additional gains and empirically supporting the proposed optimization-geometry perspective.", "AI": {"tldr": "ShaPO\u662f\u4e00\u4e2a\u51e0\u4f55\u611f\u77e5\u7684\u504f\u597d\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u63a7\u5236\u5bf9\u9f50\u5173\u952e\u53c2\u6570\u5b50\u7a7a\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\u6765\u589e\u5f3aLLM\u5b89\u5168\u5bf9\u9f50\u7684\u9c81\u68d2\u6027\uff0c\u5728\u5206\u5e03\u504f\u79fb\u548c\u566a\u58f0\u76d1\u7763\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u5728\u9886\u57df\u8f6c\u79fb\u548c\u566a\u58f0\u504f\u597d\u76d1\u7763\u4e0b\u8868\u73b0\u8106\u5f31\uff0c\u5927\u591a\u6570\u9c81\u68d2\u5bf9\u9f50\u65b9\u6cd5\u53ea\u5173\u6ce8\u5bf9\u9f50\u6570\u636e\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u5ffd\u89c6\u4e86\u57fa\u4e8e\u504f\u597d\u7684\u76ee\u6807\u51fd\u6570\u4e2d\u4f18\u5316\u5f15\u8d77\u7684\u8106\u5f31\u6027\u3002", "method": "\u63d0\u51faShaPO\u6846\u67b6\uff0c\u4ece\u4f18\u5316\u51e0\u4f55\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6LLM\u5b89\u5168\u5bf9\u9f50\u7684\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u5bf9\u9f50\u5173\u952e\u53c2\u6570\u5b50\u7a7a\u95f4\u7684\u9009\u62e9\u6027\u51e0\u4f55\u63a7\u5236\u6765\u5f3a\u5236\u6267\u884c\u6700\u574f\u60c5\u51b5\u5bf9\u9f50\u76ee\u6807\uff0c\u907f\u514d\u5747\u5300\u51e0\u4f55\u7ea6\u675f\u5bfc\u81f4\u7684\u8fc7\u6b63\u5219\u5316\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u548c\u566a\u58f0\u504f\u597d\u8bbe\u7f6e\u4e2d\uff0cShaPO\u76f8\u6bd4\u6d41\u884c\u7684\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u6301\u7eed\u63d0\u5347\u4e86\u5b89\u5168\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u80fd\u4e0e\u6570\u636e\u9c81\u68d2\u76ee\u6807\u6e05\u6670\u7ec4\u5408\uff0c\u5e26\u6765\u989d\u5916\u589e\u76ca\u3002", "conclusion": "ShaPO\u901a\u8fc7\u4f18\u5316\u51e0\u4f55\u89c6\u89d2\u6709\u6548\u89e3\u51b3\u4e86LLM\u5b89\u5168\u5bf9\u9f50\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u5b9e\u8bc1\u7ed3\u679c\u652f\u6301\u4e86\u6240\u63d0\u51fa\u7684\u4f18\u5316\u51e0\u4f55\u89c6\u89d2\uff0c\u8868\u660e\u4ec5\u9760\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u65e0\u6cd5\u5b8c\u5168\u89e3\u51b3\u9c81\u68d2\u6027\u5931\u6548\u95ee\u9898\u3002"}}
{"id": "2602.08464", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.08464", "abs": "https://arxiv.org/abs/2602.08464", "authors": ["Joris Kattem\u00f6lle", "Bal\u00e1zs Gul\u00e1csi", "Guido Burkard"], "title": "Non-Markovianity induced by Pauli-twirling", "comment": "9 pages, 2 figures", "summary": "Noise forms a central obstacle to effective quantum information processing. Recent experimental advances have enabled the tailoring of noise properties through Pauli twirling, transforming arbitrary noise channels into Pauli channels. This underpins theoretical descriptions of fault-tolerant quantum computation and forms an essential tool in noise characterization and error mitigation. Pauli-Lindblad channels have been introduced to aptly parameterize quasi-local Pauli errors across a quantum register, excluding negative Pauli-Lindblad parameters relying on the Markovianity of the underlying noise processes. We point out that caution is required when parameterizing channels as Pauli-Lindblad channels with nonnegative parameters. For this, we study the effects of Pauli twirling on Markovianity. We use the notion of Markovianity of a channel (rather than that of an entire semigroup) and prove a general Pauli channel is non-Markovian if and only if at least one of its Pauli-Lindblad parameters is negative. Using this, we show that Markovian quantum channels often become non-Markovian after Pauli twirling. The Pauli-twirling induced non-Markovianity necessitates the use of negative Pauli-Lindblad parameters for a correct noise description in experimentally realistic scenarios. An important example is the implementation of the $\\sqrt{X}$-gate under standard Markovian noise. As such, our results have direct implications for quantum error mitigation protocols that rely on accurate noise characterization.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\uff0c\u5728\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u4e2d\uff0c\u901a\u8fc7Pauli twirling\u5c06\u4efb\u610f\u566a\u58f0\u901a\u9053\u8f6c\u5316\u4e3aPauli\u901a\u9053\u65f6\uff0c\u539f\u672c\u7684\u9a6c\u5c14\u53ef\u592b\u566a\u58f0\u4f1a\u53d8\u6210\u975e\u9a6c\u5c14\u53ef\u592b\u566a\u58f0\uff0c\u8fd9\u9700\u8981\u4f7f\u7528\u8d1f\u7684Pauli-Lindblad\u53c2\u6570\u8fdb\u884c\u6b63\u786e\u63cf\u8ff0\u3002", "motivation": "\u91cf\u5b50\u566a\u58f0\u662f\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u7684\u4e3b\u8981\u969c\u788d\u3002Pauli twirling\u6280\u672f\u53ef\u4ee5\u5c06\u4efb\u610f\u566a\u58f0\u901a\u9053\u8f6c\u5316\u4e3aPauli\u901a\u9053\uff0c\u8fd9\u5728\u7406\u8bba\u63cf\u8ff0\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\u548c\u566a\u58f0\u8868\u5f81\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684Pauli-Lindblad\u901a\u9053\u53c2\u6570\u5316\u65b9\u6cd5\u5047\u8bbe\u975e\u8d1f\u53c2\u6570\uff0c\u8fd9\u4f9d\u8d56\u4e8e\u5e95\u5c42\u566a\u58f0\u8fc7\u7a0b\u7684\u9a6c\u5c14\u53ef\u592b\u6027\u3002\u8bba\u6587\u65e8\u5728\u7814\u7a76Pauli twirling\u5bf9\u9a6c\u5c14\u53ef\u592b\u6027\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u901a\u9053\uff08\u800c\u975e\u6574\u4e2a\u534a\u7fa4\uff09\u7684\u9a6c\u5c14\u53ef\u592b\u6027\u6982\u5ff5\uff0c\u8bc1\u660e\u4e00\u822cPauli\u901a\u9053\u662f\u975e\u9a6c\u5c14\u53ef\u592b\u7684\u5f53\u4e14\u4ec5\u5f53\u5176\u81f3\u5c11\u4e00\u4e2aPauli-Lindblad\u53c2\u6570\u4e3a\u8d1f\u3002\u57fa\u4e8e\u6b64\uff0c\u7814\u7a76Pauli twirling\u5bf9\u9a6c\u5c14\u53ef\u592b\u6027\u7684\u5f71\u54cd\uff0c\u5206\u6790\u6807\u51c6\u9a6c\u5c14\u53ef\u592b\u566a\u58f0\u4e0b\u91cf\u5b50\u95e8\u5b9e\u73b0\u7684\u5177\u4f53\u6848\u4f8b\u3002", "result": "\u8bc1\u660e\u9a6c\u5c14\u53ef\u592b\u91cf\u5b50\u901a\u9053\u7ecf\u8fc7Pauli twirling\u540e\u901a\u5e38\u4f1a\u53d8\u6210\u975e\u9a6c\u5c14\u53ef\u592b\u901a\u9053\u3002\u8fd9\u79cdPauli twirling\u8bf1\u5bfc\u7684\u975e\u9a6c\u5c14\u53ef\u592b\u6027\u8981\u6c42\u5728\u5b9e\u9a8c\u73b0\u5b9e\u573a\u666f\u4e2d\u4f7f\u7528\u8d1f\u7684Pauli-Lindblad\u53c2\u6570\u8fdb\u884c\u6b63\u786e\u7684\u566a\u58f0\u63cf\u8ff0\u3002\u5177\u4f53\u793a\u4f8b\u5305\u62ec\u6807\u51c6\u9a6c\u5c14\u53ef\u592b\u566a\u58f0\u4e0b\u221aX\u95e8\u7684\u5b9e\u73b0\u3002", "conclusion": "Pauli twirling\u4f1a\u5bfc\u81f4\u539f\u672c\u7684\u9a6c\u5c14\u53ef\u592b\u566a\u58f0\u53d8\u4e3a\u975e\u9a6c\u5c14\u53ef\u592b\u566a\u58f0\uff0c\u8fd9\u9700\u8981\u4f7f\u7528\u8d1f\u7684Pauli-Lindblad\u53c2\u6570\u8fdb\u884c\u51c6\u786e\u63cf\u8ff0\u3002\u8fd9\u4e00\u53d1\u73b0\u5bf9\u4f9d\u8d56\u7cbe\u786e\u566a\u58f0\u8868\u5f81\u7684\u91cf\u5b50\u8bef\u5dee\u7f13\u89e3\u534f\u8bae\u5177\u6709\u76f4\u63a5\u610f\u4e49\uff0c\u8868\u660e\u5728\u566a\u58f0\u5efa\u6a21\u4e2d\u9700\u8981\u8003\u8651\u8fd9\u79cd\u6548\u5e94\u3002"}}
{"id": "2602.07341", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07341", "abs": "https://arxiv.org/abs/2602.07341", "authors": ["Yicheng Yang", "Ruijiao Li", "Lifeng Wang", "Shuai Zheng", "Shunzheng Ma", "Keyu Zhang", "Tuoyu Sun", "Chenyun Dai", "Jie Ding", "Zhuo Zou"], "title": "Scalable Dexterous Robot Learning with AR-based Remote Human-Robot Interactions", "comment": null, "summary": "This paper focuses on the scalable robot learning for manipulation in the dexterous robot arm-hand systems, where the remote human-robot interactions via augmented reality (AR) are established to collect the expert demonstration data for improving efficiency. In such a system, we present a unified framework to address the general manipulation task problem. Specifically, the proposed method consists of two phases: i) In the first phase for pretraining, the policy is created in a behavior cloning (BC) manner, through leveraging the learning data from our AR-based remote human-robot interaction system; ii) In the second phase, a contrastive learning empowered reinforcement learning (RL) method is developed to obtain more efficient and robust policy than the BC, and thus a projection head is designed to accelerate the learning progress. An event-driven augmented reward is adopted for enhancing the safety. To validate the proposed method, both the physics simulations via PyBullet and real-world experiments are carried out. The results demonstrate that compared to the classic proximal policy optimization and soft actor-critic policies, our method not only significantly speeds up the inference, but also achieves much better performance in terms of the success rate for fulfilling the manipulation tasks. By conducting the ablation study, it is confirmed that the proposed RL with contrastive learning overcomes policy collapse. Supplementary demonstrations are available at https://cyberyyc.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u589e\u5f3a\u73b0\u5b9e\u8fdc\u7a0b\u4ea4\u4e92\u3001\u884c\u4e3a\u514b\u9686\u9884\u8bad\u7ec3\u548c\u5bf9\u6bd4\u5b66\u4e60\u5f3a\u5316\u5b66\u4e60\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u7075\u5de7\u673a\u5668\u4eba\u81c2\u624b\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u64cd\u4f5c\u5b66\u4e60", "motivation": "\u89e3\u51b3\u7075\u5de7\u673a\u5668\u4eba\u81c2\u624b\u7cfb\u7edf\u64cd\u4f5c\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u589e\u5f3a\u73b0\u5b9e\u8fdc\u7a0b\u4eba\u673a\u4ea4\u4e92\u9ad8\u6548\u6536\u96c6\u4e13\u5bb6\u6f14\u793a\u6570\u636e\uff0c\u63d0\u9ad8\u5b66\u4e60\u6548\u7387", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u589e\u5f3a\u73b0\u5b9e\u8fdc\u7a0b\u4ea4\u4e92\u6536\u96c6\u6570\u636e\uff0c\u91c7\u7528\u884c\u4e3a\u514b\u9686\u9884\u8bad\u7ec3\u7b56\u7565\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5f00\u53d1\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u6295\u5f71\u5934\u52a0\u901f\u5b66\u4e60\uff0c\u91c7\u7528\u4e8b\u4ef6\u9a71\u52a8\u589e\u5f3a\u5956\u52b1\u786e\u4fdd\u5b89\u5168", "result": "\u76f8\u6bd4\u7ecf\u5178PPO\u548cSAC\u65b9\u6cd5\uff0c\u672c\u65b9\u6cd5\u663e\u8457\u52a0\u5feb\u63a8\u7406\u901f\u5ea6\uff0c\u5728\u64cd\u4f5c\u4efb\u52a1\u6210\u529f\u7387\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u5bf9\u6bd4\u5b66\u4e60\u80fd\u907f\u514d\u7b56\u7565\u5d29\u6e83", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u7075\u5de7\u673a\u5668\u4eba\u81c2\u624b\u7cfb\u7edf\u7684\u64cd\u4f5c\u5b66\u4e60\u95ee\u9898\uff0c\u7ed3\u5408\u589e\u5f3a\u73b0\u5b9e\u4ea4\u4e92\u3001\u884c\u4e3a\u514b\u9686\u548c\u5bf9\u6bd4\u5b66\u4e60\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9c81\u68d2\u4e14\u5b89\u5168\u7684\u7b56\u7565\u5b66\u4e60"}}
{"id": "2602.08469", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.08469", "abs": "https://arxiv.org/abs/2602.08469", "authors": ["Palash Pandya", "Shubhayan Sarkar", "Remigiusz Augusiak"], "title": "Classifying the simplest Bell inequalities beyond qubits and their applications towards self-testing", "comment": "12 pages, 1 figure", "summary": "Bell inequalities reveal the fundamentally nonlocal character of quantum mechanics. In this regard, one of the interesting problems is to explore all possible Bell inequalities that demonstrate a gap between local and nonlocal quantum behaviour. This is useful for the geometric characterisation of the set of nonlocal correlations achievable within quantum theory. Moreover, it provides a systematic way to construct Bell inequalities that are tailored to specific quantum information processing tasks. This characterisation is well understood in the simplest $(2,2,2)$ scenario, namely two parties performing two binary outcome measurements. However, beyond this setting, relatively few Bell inequalities are known, and the situation becomes particularly scarce in scenarios involving a greater number of outcomes. Here, we consider the $(2,2,3)$ scenario, or two parties performing two three-outcome measurements, and characterise all Bell inequalities that can arise from the simplest sum-of-squares decomposition and are maximally violated by the maximally entangled state of local dimension three. We then utilise them to self-test this state, along with a class of three-outcome measurements.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86(2,2,3)\u573a\u666f\u4e0b\u7684\u8d1d\u5c14\u4e0d\u7b49\u5f0f\uff0c\u5373\u4e24\u65b9\u5404\u8fdb\u884c\u4e24\u4e2a\u4e09\u7ed3\u679c\u6d4b\u91cf\u7684\u60c5\u51b5\u3002\u4f5c\u8005\u8868\u5f81\u4e86\u6240\u6709\u53ef\u4ee5\u4ece\u6700\u7b80\u5355\u7684\u5e73\u65b9\u548c\u5206\u89e3\u4e2d\u4ea7\u751f\u7684\u8d1d\u5c14\u4e0d\u7b49\u5f0f\uff0c\u8fd9\u4e9b\u4e0d\u7b49\u5f0f\u88ab\u4e09\u7ef4\u6700\u5927\u7ea0\u7f20\u6001\u6700\u5927\u8fdd\u53cd\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u4e0d\u7b49\u5f0f\u81ea\u6d4b\u8bd5\u8be5\u6001\u53ca\u4e00\u7c7b\u4e09\u7ed3\u679c\u6d4b\u91cf\u3002", "motivation": "\u8d1d\u5c14\u4e0d\u7b49\u5f0f\u63ed\u793a\u4e86\u91cf\u5b50\u529b\u5b66\u7684\u975e\u5c40\u57df\u672c\u8d28\u3002\u63a2\u7d22\u6240\u6709\u53ef\u80fd\u7684\u8d1d\u5c14\u4e0d\u7b49\u5f0f\u5bf9\u4e8e\u51e0\u4f55\u8868\u5f81\u91cf\u5b50\u7406\u8bba\u4e2d\u53ef\u5b9e\u73b0\u7684\u975e\u5c40\u57df\u5173\u8054\u96c6\u5408\u5f88\u91cd\u8981\uff0c\u540c\u65f6\u4e3a\u7279\u5b9a\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u4efb\u52a1\u6784\u5efa\u5b9a\u5236\u5316\u7684\u8d1d\u5c14\u4e0d\u7b49\u5f0f\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\u3002\u5728(2,2,2)\u573a\u666f\u4e0b\u5df2\u6709\u8f83\u597d\u7406\u89e3\uff0c\u4f46\u5728\u6d89\u53ca\u66f4\u591a\u7ed3\u679c\u7684\u573a\u666f\u4e2d\uff0c\u5df2\u77e5\u7684\u8d1d\u5c14\u4e0d\u7b49\u5f0f\u76f8\u5bf9\u8f83\u5c11\u3002", "method": "\u8003\u8651(2,2,3)\u573a\u666f\uff08\u4e24\u65b9\u5404\u8fdb\u884c\u4e24\u4e2a\u4e09\u7ed3\u679c\u6d4b\u91cf\uff09\uff0c\u8868\u5f81\u6240\u6709\u53ef\u4ee5\u4ece\u6700\u7b80\u5355\u7684\u5e73\u65b9\u548c\u5206\u89e3\u4e2d\u4ea7\u751f\u7684\u8d1d\u5c14\u4e0d\u7b49\u5f0f\u3002\u8fd9\u4e9b\u4e0d\u7b49\u5f0f\u88ab\u4e09\u7ef4\u6700\u5927\u7ea0\u7f20\u6001\u6700\u5927\u8fdd\u53cd\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u4e0d\u7b49\u5f0f\u8fdb\u884c\u81ea\u6d4b\u8bd5\u3002", "result": "\u6210\u529f\u8868\u5f81\u4e86(2,2,3)\u573a\u666f\u4e0b\u6240\u6709\u53ef\u4ee5\u4ece\u5e73\u65b9\u548c\u5206\u89e3\u4e2d\u4ea7\u751f\u7684\u8d1d\u5c14\u4e0d\u7b49\u5f0f\uff0c\u8fd9\u4e9b\u4e0d\u7b49\u5f0f\u88ab\u4e09\u7ef4\u6700\u5927\u7ea0\u7f20\u6001\u6700\u5927\u8fdd\u53cd\u3002\u5229\u7528\u8fd9\u4e9b\u4e0d\u7b49\u5f0f\u5b9e\u73b0\u4e86\u5bf9\u8be5\u6001\u53ca\u4e00\u7c7b\u4e09\u7ed3\u679c\u6d4b\u91cf\u7684\u81ea\u6d4b\u8bd5\u3002", "conclusion": "\u672c\u6587\u6269\u5c55\u4e86\u5bf9\u8d1d\u5c14\u4e0d\u7b49\u5f0f\u7684\u7406\u89e3\uff0c\u4ece\u7b80\u5355\u7684(2,2,2)\u573a\u666f\u63a8\u8fdb\u5230\u66f4\u590d\u6742\u7684(2,2,3)\u573a\u666f\uff0c\u4e3a\u91cf\u5b50\u975e\u5c40\u57df\u6027\u7684\u51e0\u4f55\u8868\u5f81\u548c\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u4efb\u52a1\u7684\u5b9a\u5236\u5316\u8d1d\u5c14\u4e0d\u7b49\u5f0f\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2602.07356", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07356", "abs": "https://arxiv.org/abs/2602.07356", "authors": ["Yonghui Yang", "Junwei Li", "Jilong Liu", "Yicheng He", "Fengbin Zhu", "Weibiao Huang", "Le Wu", "Richang Hong", "Tat-Seng Chua"], "title": "Controllable Value Alignment in Large Language Models through Neuron-Level Editing", "comment": null, "summary": "Aligning large language models (LLMs) with human values has become increasingly important as their influence on human behavior and decision-making expands. However, existing steering-based alignment methods suffer from limited controllability: steering a target value often unintentionally activates other, non-target values. To characterize this limitation, we introduce value leakage, a diagnostic notion that captures the unintended activation of non-target values during value steering, along with a normalized leakage metric grounded in Schwartz's value theory. In light of this analysis, we propose NeVA, a neuron-level editing framework for controllable value alignment in LLMs. NeVA identifies sparse, value-relevant neurons and performs inference-time activation editing, enabling fine-grained control without parameter updates or retraining. Experiments show that NeVA achieves stronger target value alignment while incurring smaller performance degradation on general capability. Moreover, NeVA significantly reduces the average leakage, with residual effects largely confined to semantically related value classes. Overall, NeVA offers a more controllable and interpretable mechanism for value alignment.", "AI": {"tldr": "\u63d0\u51faNeVA\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u5143\u7ea7\u7f16\u8f91\u5b9e\u73b0\u53ef\u63a7\u7684\u4ef7\u503c\u5bf9\u9f50\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u4ef7\u503c\u6cc4\u6f0f\u95ee\u9898", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f15\u5bfc\u7684\u4ef7\u503c\u5bf9\u9f50\u65b9\u6cd5\u5b58\u5728\u6709\u9650\u7684\u53ef\u63a7\u6027\uff1a\u5f15\u5bfc\u76ee\u6807\u4ef7\u503c\u65f6\u5f80\u5f80\u4f1a\u65e0\u610f\u4e2d\u6fc0\u6d3b\u5176\u4ed6\u975e\u76ee\u6807\u4ef7\u503c\uff0c\u5373\"\u4ef7\u503c\u6cc4\u6f0f\"\u95ee\u9898", "method": "\u63d0\u51faNeVA\u6846\u67b6\uff1a1) \u8bc6\u522b\u7a00\u758f\u7684\u4ef7\u503c\u76f8\u5173\u795e\u7ecf\u5143\uff1b2) \u5728\u63a8\u7406\u65f6\u8fdb\u884c\u6fc0\u6d3b\u7f16\u8f91\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u6216\u91cd\u65b0\u8bad\u7ec3\uff1b3) \u57fa\u4e8eSchwartz\u4ef7\u503c\u7406\u8bba\u5b9a\u4e49\u6807\u51c6\u5316\u6cc4\u6f0f\u5ea6\u91cf", "result": "NeVA\u5b9e\u73b0\u4e86\u66f4\u5f3a\u7684\u76ee\u6807\u4ef7\u503c\u5bf9\u9f50\uff0c\u540c\u65f6\u5e26\u6765\u66f4\u5c0f\u7684\u901a\u7528\u80fd\u529b\u6027\u80fd\u4e0b\u964d\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5e73\u5747\u6cc4\u6f0f\u7387\uff0c\u6b8b\u4f59\u6548\u5e94\u4e3b\u8981\u5c40\u9650\u4e8e\u8bed\u4e49\u76f8\u5173\u7684\u4ef7\u503c\u7c7b\u522b", "conclusion": "NeVA\u4e3a\u4ef7\u503c\u5bf9\u9f50\u63d0\u4f9b\u4e86\u66f4\u53ef\u63a7\u548c\u53ef\u89e3\u91ca\u7684\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4ef7\u503c\u6cc4\u6f0f\u95ee\u9898"}}
{"id": "2602.08472", "categories": ["quant-ph", "physics.atom-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2602.08472", "abs": "https://arxiv.org/abs/2602.08472", "authors": ["Wen-Zhao Liu", "Ya-Bin Zhou", "Jiu-Peng Chen", "Bin Wang", "Ao Teng", "Xiao-Wen Han", "Guang-Cheng Liu", "Zhi-Jiong Zhang", "Yi Yang", "Feng-Guang Liu", "ChaoHui Xue", "Bo-Wen Yang", "Jin Yang", "Chao Zeng", "Du-Ruo Pan", "Ming-Yang Zheng", "Xing-Jian Zhang", "Cao Shen", "Yi-Zheng Zhen", "You Xiao", "Hao Li", "Li-Xing You", "XiongFeng Ma", "Qi Zhao", "Feihu Xu", "Ye Wang", "Yong Wan", "Qiang Zhang", "Jian-Wei Pan"], "title": "A building block of quantum repeaters for scalable quantum networks", "comment": "Accepted for publication in Nature (https://doi.org/10.1038/s41586-026-10177-4). This is the original submitted version", "summary": "Quantum networks, integrating quantum communication, quantum metrology, and distributed quantum computing, could provide secure and efficient information transfer, high-resolution sensing, and an exponential speed-up in information processing. Deterministic entanglement distribution over long distances is a prerequisite for scalable quantum networks, enabling the utilization of device-independent quantum key distribution (DI-QKD) and quantum teleportation to achieve secure and efficient information transfer. However, the exponential photon loss in optical fibres prohibits efficient and deterministic entanglement distribution. Quantum repeaters, incorporating entanglement swapping and entanglement purification with quantum memories, offer the most promising means to overcome this limitation in fibre-based quantum networks. Despite numerous pioneering efforts toward realizing quantum repeaters, a critical bottleneck remains, as remote memory-memory entanglement suffers from decoherence more rapidly than it can be established and purified over long distances. We overcome this by developing long-lived trapped-ion memories, an efficient telecom interface, and a high-visibility single-photon entanglement protocol. This allows us to establish and maintain memory-memory entanglement over a 10 km fibre within the average entanglement establishment time for the same distance. As a direct application, we demonstrate metropolitan-scale DI-QKD, distilling 1,917 secret keys out of 4.05*10^5 Bell pairs over 10 km. We further report a positive key rate over 101 km in the asymptotic limit, extending the achievable distance by more than two orders of magnitude. Our work provides a critical building block for quantum repeaters and marks an important step toward scalable quantum networks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u957f\u5bff\u547d\u79bb\u5b50\u9631\u91cf\u5b50\u5b58\u50a8\u5668\u3001\u9ad8\u6548\u7535\u4fe1\u63a5\u53e3\u548c\u9ad8\u53ef\u89c1\u5ea6\u5355\u5149\u5b50\u7ea0\u7f20\u534f\u8bae\uff0c\u5b9e\u73b0\u4e8610\u516c\u91cc\u5149\u7ea4\u5185\u5efa\u7acb\u548c\u7ef4\u6301\u5b58\u50a8\u5668-\u5b58\u50a8\u5668\u7ea0\u7f20\uff0c\u5e76\u6f14\u793a\u4e86\u57ce\u57df\u89c4\u6a21\u7684\u8bbe\u5907\u65e0\u5173\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\u3002", "motivation": "\u91cf\u5b50\u7f51\u7edc\u9700\u8981\u786e\u5b9a\u6027\u957f\u8ddd\u79bb\u7ea0\u7f20\u5206\u53d1\uff0c\u4f46\u5149\u7ea4\u4e2d\u7684\u6307\u6570\u5149\u5b50\u635f\u8017\u9650\u5236\u4e86\u6548\u7387\u3002\u91cf\u5b50\u4e2d\u7ee7\u5668\u662f\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u6280\u672f\u4e2d\u8fdc\u7a0b\u5b58\u50a8\u5668-\u5b58\u50a8\u5668\u7ea0\u7f20\u7684\u5efa\u7acb\u901f\u5ea6\u6162\u4e8e\u9000\u76f8\u5e72\u901f\u5ea6\uff0c\u6210\u4e3a\u5173\u952e\u74f6\u9888\u3002", "method": "\u5f00\u53d1\u4e86\u957f\u5bff\u547d\u79bb\u5b50\u9631\u91cf\u5b50\u5b58\u50a8\u5668\u3001\u9ad8\u6548\u7535\u4fe1\u63a5\u53e3\u548c\u9ad8\u53ef\u89c1\u5ea6\u5355\u5149\u5b50\u7ea0\u7f20\u534f\u8bae\uff0c\u572810\u516c\u91cc\u5149\u7ea4\u5185\u5efa\u7acb\u548c\u7ef4\u6301\u5b58\u50a8\u5668-\u5b58\u50a8\u5668\u7ea0\u7f20\uff0c\u5e76\u5e94\u7528\u4e8e\u8bbe\u5907\u65e0\u5173\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\u3002", "result": "\u572810\u516c\u91cc\u5149\u7ea4\u5185\u6210\u529f\u5efa\u7acb\u548c\u7ef4\u6301\u5b58\u50a8\u5668-\u5b58\u50a8\u5668\u7ea0\u7f20\uff0c\u5b9e\u73b0\u4e86\u57ce\u57df\u89c4\u6a21\u8bbe\u5907\u65e0\u5173\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\uff0c\u4ece4.05\u00d710^5\u4e2a\u8d1d\u5c14\u5bf9\u4e2d\u63d0\u53d6\u4e861,917\u4e2a\u79d8\u5bc6\u5bc6\u94a5\uff0c\u5728\u6e10\u8fd1\u6781\u9650\u4e0b\u5b9e\u73b0\u4e86101\u516c\u91cc\u7684\u6b63\u5bc6\u94a5\u7387\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u514b\u670d\u4e86\u8fdc\u7a0b\u5b58\u50a8\u5668-\u5b58\u50a8\u5668\u7ea0\u7f20\u5efa\u7acb\u901f\u5ea6\u6162\u4e8e\u9000\u76f8\u5e72\u901f\u5ea6\u7684\u5173\u952e\u74f6\u9888\uff0c\u4e3a\u91cf\u5b50\u4e2d\u7ee7\u5668\u63d0\u4f9b\u4e86\u5173\u952e\u6784\u5efa\u6a21\u5757\uff0c\u662f\u8fc8\u5411\u53ef\u6269\u5c55\u91cf\u5b50\u7f51\u7edc\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2602.07358", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07358", "abs": "https://arxiv.org/abs/2602.07358", "authors": ["Jiaming He", "Fuming Luo", "Hongwei Li", "Wenbo Jiang", "Wenshu Fan", "Zhenbo Shi", "Xudong Jiang", "Yi Yu"], "title": "UTOPIA: Unlearnable Tabular Data via Decoupled Shortcut Embedding", "comment": null, "summary": "Unlearnable examples (UE) have emerged as a practical mechanism to prevent unauthorized model training on private vision data, while extending this protection to tabular data is nontrivial. Tabular data in finance and healthcare is highly sensitive, yet existing UE methods transfer poorly because tabular features mix numerical and categorical constraints and exhibit saliency sparsity, with learning dominated by a few dimensions. Under a Spectral Dominance condition, we show certified unlearnability is feasible when the poison spectrum overwhelms the clean semantic spectrum. Guided by this, we propose Unlearnable Tabular Data via DecOuPled Shortcut EmbeddIng (UTOPIA), which exploits feature redundancy to decouple optimization into two channels: high saliency features for semantic obfuscation and low saliency redundant features for embedding a hyper correlated shortcut, yielding constraint-aware dominant shortcuts while preserving tabular validity. Extensive experiments across tabular datasets and models show UTOPIA drives unauthorized training toward near random performance, outperforming strong UE baselines and transferring well across architectures.", "AI": {"tldr": "UTOPIA\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8868\u683c\u6570\u636e\u7684\u4e0d\u53ef\u5b66\u4e60\u6837\u672c\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u4f18\u5316\u5728\u9ad8\u663e\u8457\u6027\u7279\u5f81\u4e0a\u8fdb\u884c\u8bed\u4e49\u6df7\u6dc6\uff0c\u5728\u4f4e\u663e\u8457\u6027\u5197\u4f59\u7279\u5f81\u4e2d\u5d4c\u5165\u8d85\u76f8\u5173\u6377\u5f84\uff0c\u5b9e\u73b0\u8ba4\u8bc1\u7684\u4e0d\u53ef\u5b66\u4e60\u6027\u3002", "motivation": "\u91d1\u878d\u548c\u533b\u7597\u9886\u57df\u7684\u8868\u683c\u6570\u636e\u9ad8\u5ea6\u654f\u611f\uff0c\u9700\u8981\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u6a21\u578b\u8bad\u7ec3\u3002\u73b0\u6709\u4e0d\u53ef\u5b66\u4e60\u6837\u672c\u65b9\u6cd5\u5728\u8868\u683c\u6570\u636e\u4e0a\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u8868\u683c\u6570\u636e\u6df7\u5408\u4e86\u6570\u503c\u548c\u7c7b\u522b\u7ea6\u675f\uff0c\u4e14\u5b58\u5728\u663e\u8457\u6027\u7a00\u758f\u6027\uff08\u5b66\u4e60\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5c11\u6570\u7ef4\u5ea6\uff09\u3002", "method": "UTOPIA\u5229\u7528\u7279\u5f81\u5197\u4f59\u5c06\u4f18\u5316\u89e3\u8026\u4e3a\u4e24\u4e2a\u901a\u9053\uff1a\u9ad8\u663e\u8457\u6027\u7279\u5f81\u7528\u4e8e\u8bed\u4e49\u6df7\u6dc6\uff0c\u4f4e\u663e\u8457\u6027\u5197\u4f59\u7279\u5f81\u7528\u4e8e\u5d4c\u5165\u8d85\u76f8\u5173\u6377\u5f84\u3002\u5728\u8c31\u4e3b\u5bfc\u6761\u4ef6\u4e0b\uff0c\u5f53\u6bd2\u5316\u8c31\u538b\u5012\u5e72\u51c0\u8bed\u4e49\u8c31\u65f6\uff0c\u53ef\u5b9e\u73b0\u8ba4\u8bc1\u7684\u4e0d\u53ef\u5b66\u4e60\u6027\u3002", "result": "\u5728\u591a\u4e2a\u8868\u683c\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUTOPIA\u80fd\u5c06\u672a\u7ecf\u6388\u6743\u7684\u8bad\u7ec3\u63a8\u5411\u63a5\u8fd1\u968f\u673a\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u4e0d\u53ef\u5b66\u4e60\u6837\u672c\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u5f88\u597d\u5730\u8de8\u67b6\u6784\u8fc1\u79fb\u3002", "conclusion": "UTOPIA\u4e3a\u4fdd\u62a4\u654f\u611f\u8868\u683c\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4e0d\u53ef\u5b66\u4e60\u6837\u672c\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u4f18\u5316\u548c\u8c31\u4e3b\u5bfc\u7406\u8bba\uff0c\u5728\u4fdd\u6301\u8868\u683c\u6570\u636e\u6709\u6548\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u4fdd\u62a4\u6548\u679c\u3002"}}
{"id": "2602.08485", "categories": ["quant-ph", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08485", "abs": "https://arxiv.org/abs/2602.08485", "authors": ["Paul San Sebastian", "Mikel Ca\u00f1izo", "Roman Orus"], "title": "Empirical Study of Observable Sets in Multiclass Quantum Classification", "comment": "13 pages, 11 figures", "summary": "Variational quantum algorithms have gained attention as early applications of quantum computers for learning tasks. In the context of supervised learning, most of the works that tackle classification problems with parameterized quantum circuits constrain their scope to the setting of binary classification or perform multiclass classification via ensembles of binary classifiers (strategies such as one versus rest). Those few works that propose native multiclass models, however, do not justify the choice of observables that perform the classification. This work studies two main classification criteria in multiclass quantum machine learning: maximizing the expected value of an observable representing a class or maximizing the fidelity of the encoded quantum state with a reference state representing a class. To compare both approaches, sets of Pauli strings and sets of projectors into the computational basis are chosen as observables in the quantum machine learning models. Observing the empirical behavior of each model type, the effect of different observable set choices on the performance of quantum machine learning models is analyzed in the context of Barren Plateaus and Neural Collapse. The results provide insights that may guide the design of future multiclass quantum machine learning models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u7c7b\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u4e24\u79cd\u4e3b\u8981\u5206\u7c7b\u6807\u51c6\uff1a\u6700\u5927\u5316\u4ee3\u8868\u7c7b\u522b\u7684\u53ef\u89c2\u6d4b\u91cf\u671f\u671b\u503c\uff0c\u4ee5\u53ca\u6700\u5927\u5316\u7f16\u7801\u91cf\u5b50\u6001\u4e0e\u4ee3\u8868\u7c7b\u522b\u7684\u53c2\u8003\u6001\u4e4b\u95f4\u7684\u4fdd\u771f\u5ea6\u3002\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u53ef\u89c2\u6d4b\u91cf\u96c6\u5408\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4e3a\u672a\u6765\u591a\u7c7b\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u6307\u5bfc\u3002", "motivation": "\u76ee\u524d\u5927\u591a\u6570\u91cf\u5b50\u76d1\u7763\u5b66\u4e60\u7814\u7a76\u5c40\u9650\u4e8e\u4e8c\u5206\u7c7b\u95ee\u9898\uff0c\u6216\u901a\u8fc7\u4e8c\u5143\u5206\u7c7b\u5668\u96c6\u6210\u5b9e\u73b0\u591a\u5206\u7c7b\u3002\u5c11\u6570\u63d0\u51fa\u539f\u751f\u591a\u7c7b\u6a21\u578b\u7684\u7814\u7a76\u672a\u80fd\u5145\u5206\u8bba\u8bc1\u5206\u7c7b\u53ef\u89c2\u6d4b\u91cf\u9009\u62e9\u7684\u5408\u7406\u6027\u3002\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u591a\u7c7b\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5206\u7c7b\u6807\u51c6\u95ee\u9898\u3002", "method": "\u7814\u7a76\u4e24\u79cd\u4e3b\u8981\u5206\u7c7b\u6807\u51c6\uff1a1\uff09\u6700\u5927\u5316\u4ee3\u8868\u7c7b\u522b\u7684\u53ef\u89c2\u6d4b\u91cf\u671f\u671b\u503c\uff1b2\uff09\u6700\u5927\u5316\u7f16\u7801\u91cf\u5b50\u6001\u4e0e\u4ee3\u8868\u7c7b\u522b\u7684\u53c2\u8003\u6001\u4e4b\u95f4\u7684\u4fdd\u771f\u5ea6\u3002\u9009\u62e9\u4e24\u7ec4\u53ef\u89c2\u6d4b\u91cf\u8fdb\u884c\u6bd4\u8f83\uff1aPauli\u5b57\u7b26\u4e32\u96c6\u5408\u548c\u8ba1\u7b97\u57fa\u6295\u5f71\u7b97\u5b50\u96c6\u5408\u3002\u5206\u6790\u4e0d\u540c\u53ef\u89c2\u6d4b\u91cf\u96c6\u5408\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728Barren Plateaus\u548cNeural Collapse\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u901a\u8fc7\u5b9e\u8bc1\u89c2\u5bdf\u6bcf\u79cd\u6a21\u578b\u7c7b\u578b\u7684\u884c\u4e3a\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u53ef\u89c2\u6d4b\u91cf\u96c6\u5408\u9009\u62e9\u5bf9\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u7406\u89e3Barren Plateaus\u548cNeural Collapse\u73b0\u8c61\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u672a\u6765\u591a\u7c7b\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\uff0c\u7279\u522b\u662f\u5728\u9009\u62e9\u9002\u5f53\u7684\u5206\u7c7b\u6807\u51c6\u548c\u53ef\u89c2\u6d4b\u91cf\u96c6\u5408\u65b9\u9762\u3002\u8fd9\u4e9b\u89c1\u89e3\u6709\u52a9\u4e8e\u6539\u8fdb\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u548c\u53ef\u8bad\u7ec3\u6027\u3002"}}
{"id": "2602.07364", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07364", "abs": "https://arxiv.org/abs/2602.07364", "authors": ["Jianchuan Yang", "Xi Chen", "Jidong Zhao"], "title": "FEM-Informed Hypergraph Neural Networks for Efficient Elastoplasticity", "comment": "43 pages, 26 figures, 8tables", "summary": "Graph neural networks (GNNs) naturally align with sparse operators and unstructured discretizations, making them a promising paradigm for physics-informed machine learning in computational mechanics. Motivated by discrete physics losses and Hierarchical Deep Learning Neural Network (HiDeNN) constructions, we embed finite-element (FEM) computations at nodes and Gauss points directly into message-passing layers and propose a numerically consistent FEM-Informed Hypergraph Neural Networks (FHGNN). Similar to conventional physics-informed neural networks (PINNs), training is purely physics-driven and requires no labeled data: the input is a node element hypergraph whose edges encode mesh connectivity. Guided by empirical results and condition-number analysis, we adopt an efficient variational loss. Validated on 3D benchmarks, including cyclic loading with isotropic/kinematic hardening, the proposed method delivers substantially improved accuracy and efficiency over recent, competitive PINN variants. By leveraging GPU-parallel tensor operations and the discrete representation, it scales effectively to large elastoplastic problems and can be competitive with, or faster than, multi-core FEM implementations at comparable accuracy. This work establishes a foundation for scalable, physics-embedded learning in nonlinear solid mechanics.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u6709\u9650\u5143\u5d4c\u5165\u65b9\u6cd5FHGNN\uff0c\u7528\u4e8e\u8ba1\u7b97\u529b\u5b66\u4e2d\u7684\u7269\u7406\u9a71\u52a8\u5b66\u4e60\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\uff0c\u5728\u4e09\u7ef4\u5f39\u5851\u6027\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edfPINN\u66f4\u9ad8\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u56fe\u795e\u7ecf\u7f51\u7edc\u4e0e\u7a00\u758f\u7b97\u5b50\u548c\u975e\u7ed3\u6784\u5316\u79bb\u6563\u5316\u81ea\u7136\u5bf9\u9f50\uff0c\u662f\u8ba1\u7b97\u529b\u5b66\u4e2d\u7269\u7406\u9a71\u52a8\u673a\u5668\u5b66\u4e60\u7684\u6709\u524d\u666f\u8303\u5f0f\u3002\u53d7\u79bb\u6563\u7269\u7406\u635f\u5931\u548c\u5206\u5c42\u6df1\u5ea6\u5b66\u4e60\u795e\u7ecf\u7f51\u7edc\u6784\u9020\u7684\u542f\u53d1\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u6570\u503c\u4e00\u81f4\u7684\u6709\u9650\u5143\u5d4c\u5165\u65b9\u6cd5\u3002", "method": "\u5c06\u6709\u9650\u5143\u8ba1\u7b97\u76f4\u63a5\u5d4c\u5165\u5230\u6d88\u606f\u4f20\u9012\u5c42\u4e2d\uff0c\u63d0\u51fa\u6709\u9650\u5143\u4fe1\u606f\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc(FHGNN)\u3002\u8f93\u5165\u662f\u8282\u70b9\u5143\u7d20\u8d85\u56fe\uff0c\u8fb9\u7f16\u7801\u7f51\u683c\u8fde\u63a5\u6027\u3002\u91c7\u7528\u9ad8\u6548\u7684\u53d8\u5206\u635f\u5931\u51fd\u6570\uff0c\u5229\u7528GPU\u5e76\u884c\u5f20\u91cf\u64cd\u4f5c\u548c\u79bb\u6563\u8868\u793a\u3002", "result": "\u5728\u5305\u62ec\u5404\u5411\u540c\u6027/\u8fd0\u52a8\u786c\u5316\u5faa\u73af\u52a0\u8f7d\u76843D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6700\u8fd1\u7684\u7ade\u4e89\u6027PINN\u53d8\u4f53\u5b9e\u73b0\u4e86\u663e\u8457\u6539\u8fdb\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002\u80fd\u591f\u6709\u6548\u6269\u5c55\u5230\u5927\u578b\u5f39\u5851\u6027\u95ee\u9898\uff0c\u5728\u53ef\u6bd4\u7cbe\u5ea6\u4e0b\u53ef\u4e0e\u591a\u6838\u6709\u9650\u5143\u5b9e\u73b0\u7ade\u4e89\u6216\u66f4\u5feb\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u975e\u7ebf\u6027\u56fa\u4f53\u529b\u5b66\u4e2d\u53ef\u6269\u5c55\u7684\u7269\u7406\u5d4c\u5165\u5b66\u4e60\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86FHGNN\u5728\u8ba1\u7b97\u529b\u5b66\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.08526", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.08526", "abs": "https://arxiv.org/abs/2602.08526", "authors": ["Duc-Kha Vu", "Minh Tam Nguyen", "\u00d6zg\u00fcr E. M\u00fcstecapl\u0131o\u011flu", "Fatih Ozaydin"], "title": "Intelligent Control of Collisional Architectures for Deterministic Multipartite State Engineering", "comment": "10 pages, 5 figures, comments are welcome", "summary": "Designing scalable, noise-tolerant control protocols for multipartite entanglement is a central challenge for quantum technologies, and it naturally calls for \\emph{algorithmic} synthesis of interaction parameters rather than handcrafted gate sequences. Here we introduce an intelligent, constraint-aware control framework for deterministic generation of symmetric Dicke states $|D_n^{(m)}\\rangle$ in repeated-interaction (collision-model) architectures. The protocol employs excitation-preserving partial-SWAP collisions between two disjoint qubit registers, mediated by $m$ ancillary ``shuttle'' qubits, and poses Dicke-state preparation as a \\emph{closed-loop design} problem: given the target $(n,m)$, automatically infer collision strengths that maximize fidelity under practical constraints. Concretely, we formulate a two-parameter, bound-constrained optimization over intra-register and shuttle--register collision angles and solve it using a multi-start strategy with L-BFGS-B, yielding a reproducible controller prescription (optimized $\u03b3_{\\mathrm{in}}$, $\u03b3_{\\mathrm{sh}}$, and minimal-round convergence points) for each target. This removes the need for projective measurements and extends collisional entanglement generation beyond the single-excitation (W-state) sector to arbitrary $m$. Crucially, we optimize \\emph{within} imperfect collisional dynamics where errors act throughout the sequence, including stochastic interaction dropouts (missing collisions) and standard decoherence channels. Strikingly, across wide error ranges the optimized controller preserves high preparation fidelity; imperfections manifest primarily as a modest increase in the required number of collision rounds. This behavior reflects a tunable competition in which noise suppresses correlations while properly chosen collisions continuously replenish them, allowing the control algorithm to trade time for fidelity.", "AI": {"tldr": "\u63d0\u51fa\u667a\u80fd\u7ea6\u675f\u611f\u77e5\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u78b0\u649e\u89d2\u5ea6\u53c2\u6570\uff0c\u5728\u91cd\u590d\u4ea4\u4e92\u67b6\u6784\u4e2d\u786e\u5b9a\u6027\u751f\u6210\u5bf9\u79f0Dicke\u6001\uff0c\u65e0\u9700\u6295\u5f71\u6d4b\u91cf\uff0c\u5bf9\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\u3002", "motivation": "\u8bbe\u8ba1\u53ef\u6269\u5c55\u3001\u6297\u566a\u58f0\u7684\u591a\u4f53\u7ea0\u7f20\u63a7\u5236\u534f\u8bae\u662f\u91cf\u5b50\u6280\u672f\u7684\u6838\u5fc3\u6311\u6218\uff0c\u9700\u8981\u7b97\u6cd5\u5316\u5408\u6210\u4ea4\u4e92\u53c2\u6570\u800c\u975e\u624b\u5de5\u8bbe\u8ba1\u95e8\u5e8f\u5217\u3002", "method": "\u91c7\u7528\u95ed\u73af\u8bbe\u8ba1\u65b9\u6cd5\uff1a\u4f7f\u7528\u4e24\u4e2a\u4e0d\u76f8\u4ea4\u91cf\u5b50\u6bd4\u7279\u5bc4\u5b58\u5668\u4e0em\u4e2a\u8f85\u52a9\"\u7a7f\u68ad\"\u91cf\u5b50\u6bd4\u7279\uff0c\u901a\u8fc7\u4fdd\u6301\u6fc0\u53d1\u6570\u7684\u90e8\u5206SWAP\u78b0\u649e\uff0c\u5c06Dicke\u6001\u5236\u5907\u8f6c\u5316\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u3002\u4f7f\u7528\u591a\u8d77\u70b9L-BFGS-B\u7b97\u6cd5\u4f18\u5316\u5bc4\u5b58\u5668\u5185\u548c\u7a7f\u68ad-\u5bc4\u5b58\u5668\u78b0\u649e\u89d2\u5ea6\u53c2\u6570\u3002", "result": "\u4f18\u5316\u63a7\u5236\u5668\u5728\u5e7f\u6cdb\u8bef\u5dee\u8303\u56f4\u5185\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\uff0c\u5305\u62ec\u968f\u673a\u4ea4\u4e92\u4e22\u5931\u548c\u6807\u51c6\u9000\u76f8\u5e72\u901a\u9053\u3002\u566a\u58f0\u4e3b\u8981\u589e\u52a0\u6240\u9700\u78b0\u649e\u8f6e\u6570\uff0c\u63a7\u5236\u5668\u53ef\u901a\u8fc7\u65f6\u95f4\u6362\u53d6\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u65e0\u9700\u6295\u5f71\u6d4b\u91cf\u7684\u786e\u5b9a\u6027Dicke\u6001\u751f\u6210\uff0c\u6269\u5c55\u4e86\u78b0\u649e\u7ea0\u7f20\u751f\u6210\u5230\u4efb\u610f\u6fc0\u53d1\u6570\uff0c\u5c55\u793a\u4e86\u5728\u566a\u58f0\u73af\u5883\u4e2d\u901a\u8fc7\u4f18\u5316\u63a7\u5236\u53c2\u6570\u5b9e\u73b0\u9c81\u68d2\u91cf\u5b50\u6001\u5236\u5907\u7684\u80fd\u529b\u3002"}}
{"id": "2602.07370", "categories": ["cs.LG", "cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07370", "abs": "https://arxiv.org/abs/2602.07370", "authors": ["Mark Bun", "William Fang"], "title": "Privately Learning Decision Lists and a Differentially Private Winnow", "comment": "27 pages, The 37th International Conference on Algorithmic Learning Theory", "summary": "We give new differentially private algorithms for the classic problems of learning decision lists and large-margin halfspaces in the PAC and online models. In the PAC model, we give a computationally efficient algorithm for learning decision lists with minimal sample overhead over the best non-private algorithms. In the online model, we give a private analog of the influential Winnow algorithm for learning halfspaces with mistake bound polylogarithmic in the dimension and inverse polynomial in the margin. As an application, we describe how to privately learn decision lists in the online model, qualitatively matching state-of-the art non-private guarantees.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u7684\u5dee\u5206\u9690\u79c1\u7b97\u6cd5\u7528\u4e8e\u5b66\u4e60\u51b3\u7b56\u5217\u8868\u548c\u5927\u95f4\u9694\u534a\u7a7a\u95f4\uff0c\u5728PAC\u548c\u5728\u7ebf\u6a21\u578b\u4e2d\u90fd\u6709\u6539\u8fdb", "motivation": "\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u4efb\u52a1\uff08\u51b3\u7b56\u5217\u8868\u548c\u534a\u7a7a\u95f4\u5b66\u4e60\uff09\u5728\u5dee\u5206\u9690\u79c1\u7ea6\u675f\u4e0b\u7684\u6027\u80fd\u4f18\u5316\uff0c\u51cf\u5c11\u9690\u79c1\u4fdd\u62a4\u5e26\u6765\u7684\u989d\u5916\u6210\u672c", "method": "1. PAC\u6a21\u578b\u4e2d\uff1a\u8ba1\u7b97\u9ad8\u6548\u7684\u51b3\u7b56\u5217\u8868\u5b66\u4e60\u7b97\u6cd5\uff0c\u6837\u672c\u5f00\u9500\u6700\u5c0f\u5316\uff1b2. \u5728\u7ebf\u6a21\u578b\u4e2d\uff1aWinnow\u7b97\u6cd5\u7684\u9690\u79c1\u7248\u672c\uff0c\u7528\u4e8e\u5b66\u4e60\u5927\u95f4\u9694\u534a\u7a7a\u95f4\uff1b3. \u5728\u7ebf\u6a21\u578b\u4e2d\u7684\u51b3\u7b56\u5217\u8868\u5b66\u4e60\u5e94\u7528", "result": "1. PAC\u6a21\u578b\u4e2d\uff1a\u6837\u672c\u5f00\u9500\u63a5\u8fd1\u6700\u4f18\u975e\u9690\u79c1\u7b97\u6cd5\uff1b2. \u5728\u7ebf\u6a21\u578b\u4e2d\uff1a\u9519\u8bef\u8fb9\u754c\u5728\u7ef4\u5ea6\u4e0a\u4e3a\u591a\u5bf9\u6570\u7ea7\uff0c\u4e0e\u95f4\u9694\u6210\u53cd\u591a\u9879\u5f0f\u5173\u7cfb\uff1b3. \u5728\u7ebf\u51b3\u7b56\u5217\u8868\u5b66\u4e60\u8fbe\u5230\u6700\u5148\u8fdb\u975e\u9690\u79c1\u7b97\u6cd5\u7684\u6027\u80fd", "conclusion": "\u63d0\u51fa\u7684\u5dee\u5206\u9690\u79c1\u7b97\u6cd5\u5728PAC\u548c\u5728\u7ebf\u6a21\u578b\u4e2d\u90fd\u80fd\u6709\u6548\u5b66\u4e60\u51b3\u7b56\u5217\u8868\u548c\u534a\u7a7a\u95f4\uff0c\u5728\u9690\u79c1\u4fdd\u62a4\u4e0b\u4ecd\u80fd\u4fdd\u6301\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u8868\u73b0"}}
{"id": "2602.08578", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.08578", "abs": "https://arxiv.org/abs/2602.08578", "authors": ["Salvatore Muratore", "Vincenzo Tamma"], "title": "Time resolution at the quantum limit of two incoherent sources based on frequency resolved two-photon-interference", "comment": "8 pages, 4 figures", "summary": "The Rayleigh criterion is a widely known limit in the resolution of incoherent sources with classical measurements in the spatial domain. Unsurprisingly the estimation of the time delay between two weak incoherent signals is afflicted by an analogue problem. In this work, we show the emergence of two-photon quantum beats in the frequency domain from the interference at a beam splitter of a photon emitted by a reference source and one from the two incoherent weak signals. We demonstrate, based on this phenomena, that with a relatively low number of measurements of the frequencies of the interfering photons either bunching or antibunching at the beam splitter output one can achieve a precision amounting to half of the quantum limit, independently of both the temporal shape of the photonic wavepacket and the time delay to be estimated. The feasibility of the technique makes it applicable in astronomy, microscopy, remote clocks synchronization and radar ranging", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u5149\u5b50\u91cf\u5b50\u62cd\u9891\u73b0\u8c61\u7684\u65b0\u6280\u672f\uff0c\u80fd\u591f\u7a81\u7834\u4f20\u7edf\u745e\u5229\u51c6\u5219\u9650\u5236\uff0c\u5b9e\u73b0\u65f6\u95f4\u5ef6\u8fdf\u4f30\u8ba1\u7cbe\u5ea6\u8fbe\u5230\u91cf\u5b50\u6781\u9650\u7684\u4e00\u534a\u3002", "motivation": "\u4f20\u7edf\u745e\u5229\u51c6\u5219\u9650\u5236\u4e86\u975e\u76f8\u5e72\u6e90\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\uff0c\u7c7b\u4f3c\u95ee\u9898\u4e5f\u5b58\u5728\u4e8e\u4e24\u4e2a\u5f31\u975e\u76f8\u5e72\u4fe1\u53f7\u7684\u65f6\u95f4\u5ef6\u8fdf\u4f30\u8ba1\u4e2d\u3002\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u65f6\u95f4\u5ef6\u8fdf\u4f30\u8ba1\u7684\u7cbe\u5ea6\u9650\u5236\u3002", "method": "\u5229\u7528\u53c2\u8003\u6e90\u5149\u5b50\u4e0e\u4e24\u4e2a\u975e\u76f8\u5e72\u5f31\u4fe1\u53f7\u5149\u5b50\u5728\u5206\u675f\u5668\u4e0a\u7684\u5e72\u6d89\uff0c\u5728\u9891\u57df\u4ea7\u751f\u53cc\u5149\u5b50\u91cf\u5b50\u62cd\u9891\u73b0\u8c61\u3002\u901a\u8fc7\u6d4b\u91cf\u5e72\u6d89\u5149\u5b50\u7684\u9891\u7387\uff08\u5206\u675f\u5668\u8f93\u51fa\u7684\u805a\u675f\u6216\u53cd\u805a\u675f\uff09\uff0c\u4ee5\u8f83\u5c11\u6d4b\u91cf\u6b21\u6570\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u65f6\u95f4\u5ef6\u8fdf\u4f30\u8ba1\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u8fbe\u5230\u91cf\u5b50\u6781\u9650\u4e00\u534a\u7684\u7cbe\u5ea6\uff0c\u4e14\u72ec\u7acb\u4e8e\u5149\u5b50\u6ce2\u5305\u7684\u65f6\u95f4\u5f62\u72b6\u548c\u5f85\u4f30\u8ba1\u7684\u65f6\u95f4\u5ef6\u8fdf\u3002\u6280\u672f\u53ef\u884c\u6027\u9ad8\uff0c\u6d4b\u91cf\u6b21\u6570\u76f8\u5bf9\u8f83\u5c11\u3002", "conclusion": "\u8be5\u6280\u672f\u53ef\u5e94\u7528\u4e8e\u5929\u6587\u5b66\u3001\u663e\u5fae\u955c\u3001\u8fdc\u7a0b\u65f6\u949f\u540c\u6b65\u548c\u96f7\u8fbe\u6d4b\u8ddd\u7b49\u9886\u57df\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u745e\u5229\u51c6\u5219\u7684\u9650\u5236\uff0c\u4e3a\u65f6\u95f4\u5ef6\u8fdf\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u91cf\u5b50\u589e\u5f3a\u65b9\u6cd5\u3002"}}
{"id": "2602.07378", "categories": ["cs.LG", "physics.data-an", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07378", "abs": "https://arxiv.org/abs/2602.07378", "authors": ["Shota Imai", "Sota Nishiyama", "Masaaki Imaizumi"], "title": "Dichotomy of Feature Learning and Unlearning: Fast-Slow Analysis on Neural Networks with Stochastic Gradient Descent", "comment": "40 pages", "summary": "The dynamics of gradient-based training in neural networks often exhibit nontrivial structures; hence, understanding them remains a central challenge in theoretical machine learning. In particular, a concept of feature unlearning, in which a neural network progressively loses previously learned features over long training, has gained attention. In this study, we consider the infinite-width limit of a two-layer neural network updated with a large-batch stochastic gradient, then derive differential equations with different time scales, revealing the mechanism and conditions for feature unlearning to occur. Specifically, we utilize the fast-slow dynamics: while an alignment of first-layer weights develops rapidly, the second-layer weights develop slowly. The direction of a flow on a critical manifold, determined by the slow dynamics, decides whether feature unlearning occurs. We give numerical validation of the result, and derive theoretical grounding and scaling laws of the feature unlearning. Our results yield the following insights: (i) the strength of the primary nonlinear term in data induces the feature unlearning, and (ii) an initial scale of the second-layer weights mitigates the feature unlearning. Technically, our analysis utilizes Tensor Programs and the singular perturbation theory.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u65e0\u9650\u5bbd\u5ea6\u6781\u9650\u548c\u5feb\u6162\u52a8\u529b\u5b66\u5206\u6790\uff0c\u63ed\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u4e2d\u7279\u5f81\u9057\u5fd8\u73b0\u8c61\u7684\u673a\u5236\u548c\u6761\u4ef6\uff0c\u53d1\u73b0\u6570\u636e\u4e2d\u7684\u4e3b\u8981\u975e\u7ebf\u6027\u9879\u5f3a\u5ea6\u4f1a\u8bf1\u5bfc\u7279\u5f81\u9057\u5fd8\uff0c\u800c\u7b2c\u4e8c\u5c42\u6743\u91cd\u7684\u521d\u59cb\u5c3a\u5ea6\u53ef\u4ee5\u7f13\u89e3\u8be5\u73b0\u8c61\u3002", "motivation": "\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u68af\u5ea6\u8bad\u7ec3\u4e2d\u7684\u975e\u5e73\u51e1\u7ed3\u6784\u662f\u7406\u8bba\u673a\u5668\u5b66\u4e60\u7684\u6838\u5fc3\u6311\u6218\u3002\u7279\u5f81\u9057\u5fd8\u73b0\u8c61\uff08\u795e\u7ecf\u7f51\u7edc\u5728\u957f\u671f\u8bad\u7ec3\u4e2d\u9010\u6e10\u4e22\u5931\u5148\u524d\u5b66\u4e60\u5230\u7684\u7279\u5f81\uff09\u5f15\u8d77\u4e86\u5173\u6ce8\uff0c\u9700\u8981\u4ece\u7406\u8bba\u4e0a\u89e3\u91ca\u5176\u673a\u5236\u548c\u6761\u4ef6\u3002", "method": "\u91c7\u7528\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc\u7684\u65e0\u9650\u5bbd\u5ea6\u6781\u9650\uff0c\u4f7f\u7528\u5927\u6279\u91cf\u968f\u673a\u68af\u5ea6\u66f4\u65b0\uff0c\u63a8\u5bfc\u51fa\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u7684\u5fae\u5206\u65b9\u7a0b\u3002\u5229\u7528\u5feb\u6162\u52a8\u529b\u5b66\u5206\u6790\uff1a\u7b2c\u4e00\u5c42\u6743\u91cd\u5feb\u901f\u5bf9\u9f50\uff0c\u7b2c\u4e8c\u5c42\u6743\u91cd\u7f13\u6162\u53d1\u5c55\u3002\u7ed3\u5408\u5f20\u91cf\u7a0b\u5e8f\u548c\u5947\u5f02\u6444\u52a8\u7406\u8bba\u8fdb\u884c\u7406\u8bba\u5206\u6790\u3002", "result": "\u63ed\u793a\u4e86\u7279\u5f81\u9057\u5fd8\u53d1\u751f\u7684\u673a\u5236\uff1a\u4e34\u754c\u6d41\u5f62\u4e0a\u7684\u6d41\u52a8\u65b9\u5411\uff08\u7531\u6162\u52a8\u529b\u5b66\u51b3\u5b9a\uff09\u51b3\u5b9a\u4e86\u662f\u5426\u53d1\u751f\u7279\u5f81\u9057\u5fd8\u3002\u6570\u503c\u9a8c\u8bc1\u4e86\u7ed3\u679c\uff0c\u5e76\u63a8\u5bfc\u4e86\u7279\u5f81\u9057\u5fd8\u7684\u7406\u8bba\u57fa\u7840\u548c\u7f29\u653e\u89c4\u5f8b\u3002", "conclusion": "\u4e3b\u8981\u53d1\u73b0\uff1a(1) \u6570\u636e\u4e2d\u4e3b\u8981\u975e\u7ebf\u6027\u9879\u7684\u5f3a\u5ea6\u4f1a\u8bf1\u5bfc\u7279\u5f81\u9057\u5fd8\uff1b(2) \u7b2c\u4e8c\u5c42\u6743\u91cd\u7684\u521d\u59cb\u5c3a\u5ea6\u53ef\u4ee5\u7f13\u89e3\u7279\u5f81\u9057\u5fd8\u3002\u6280\u672f\u5206\u6790\u7ed3\u5408\u4e86\u5f20\u91cf\u7a0b\u5e8f\u548c\u5947\u5f02\u6444\u52a8\u7406\u8bba\u3002"}}
{"id": "2602.08610", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.08610", "abs": "https://arxiv.org/abs/2602.08610", "authors": ["Chang-Kang Hu", "Chilong Liu", "Jingchao Zhao", "Liuzhu Zhong", "Yuxuan Zhou", "Mingze Liu", "Haolan Yuan", "Yongchang Lin", "Yue Xu", "Guantian Hu", "Guixu Xie", "Zixing Liu", "Ruiyang Zhou", "Yougui Ri", "Wenxuan Zhang", "Ruicheng Deng", "Andreia Saguia", "Xiayu Linpeng", "Marcelo S. Sarandy", "Song Liu", "Alan C. Santos", "Dian Tan", "Dapeng Yu"], "title": "Quantum Charging Advantage in Superconducting Solid-State Batteries", "comment": null, "summary": "Quantum battery, as a novel energy storage device, offers the potential for unprecedented efficiency and performance beyond the capabilities of classical systems, with broad implications for future quantum technologies. Here, we experimentally \\RefC{demonstrate quantum charging advantage (QCA)} in a scalable solid-state quantum battery. More specifically, we show how double-excitation Hamiltonians for two-level systems promote scalable QCA \\RefB{with standard methods.} We effectively implement the collective evolution of quantum systems with 2 up to 12 battery cells in a superconducting quantum processor, and study the performance of quantum charging compared to its uncorrelated classical counterpart. The model considered is a linear chain of superconducting transmon qubits with only \\textit{nearest-neighbor} and \\textit{pairwise} interactions, which constitute the simplest model of a multi-cell quantum battery. Our results empirically realize substantial QCA without the necessity of adopting long-range and many-body interactions \\RefB{ and showcase the quantum features of the QB charging processes with measurements of non-zero coherent ergotropy, incoherent ergotropy and entanglement,} revealing a promising prospect for further developments of efficient and experimentally feasible protocols for QCA.", "AI": {"tldr": "\u5b9e\u9a8c\u6f14\u793a\u4e86\u56fa\u6001\u91cf\u5b50\u7535\u6c60\u4e2d\u7684\u91cf\u5b50\u5145\u7535\u4f18\u52bf\uff0c\u5728\u8d85\u5bfc\u91cf\u5b50\u5904\u7406\u5668\u4e2d\u5b9e\u73b0\u4e862-12\u4e2a\u7535\u6c60\u5355\u5143\u7684\u96c6\u4f53\u6f14\u5316\uff0c\u5c55\u793a\u4e86\u65e0\u9700\u957f\u7a0b\u591a\u4f53\u76f8\u4e92\u4f5c\u7528\u7684\u53ef\u6269\u5c55\u91cf\u5b50\u5145\u7535\u4f18\u52bf\u3002", "motivation": "\u91cf\u5b50\u7535\u6c60\u4f5c\u4e3a\u65b0\u578b\u50a8\u80fd\u8bbe\u5907\uff0c\u5177\u6709\u8d85\u8d8a\u7ecf\u5178\u7cfb\u7edf\u7684\u6f5c\u5728\u6548\u7387\u548c\u6027\u80fd\uff0c\u5bf9\u672a\u6765\u91cf\u5b50\u6280\u672f\u53d1\u5c55\u6709\u91cd\u8981\u610f\u4e49\u3002\u7136\u800c\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u91cf\u5b50\u5145\u7535\u4f18\u52bf\uff08QCA\uff09\u9700\u8981\u514b\u670d\u5b9e\u9a8c\u6311\u6218\u3002", "method": "\u4f7f\u7528\u8d85\u5bfc\u91cf\u5b50\u5904\u7406\u5668\u5b9e\u73b0\u91cf\u5b50\u7535\u6c60\u6a21\u578b\uff0c\u91c7\u7528\u4ec5\u5305\u542b\u6700\u8fd1\u90bb\u548c\u6210\u5bf9\u76f8\u4e92\u4f5c\u7528\u7684\u7ebf\u6027\u94fe\u8d85\u5bfctransmon\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u3002\u901a\u8fc7\u53cc\u6fc0\u53d1\u54c8\u5bc6\u987f\u91cf\u4fc3\u8fdb\u53ef\u6269\u5c55QCA\uff0c\u57282-12\u4e2a\u7535\u6c60\u5355\u5143\u89c4\u6a21\u4e0a\u5b9e\u73b0\u96c6\u4f53\u6f14\u5316\u3002", "result": "\u5b9e\u9a8c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u91cf\u5b50\u5145\u7535\u4f18\u52bf\uff0c\u65e0\u9700\u957f\u7a0b\u591a\u4f53\u76f8\u4e92\u4f5c\u7528\u3002\u6d4b\u91cf\u663e\u793a\u975e\u96f6\u76f8\u5e72ergotropy\u3001\u975e\u76f8\u5e72ergotropy\u548c\u7ea0\u7f20\uff0c\u8bc1\u5b9e\u4e86\u91cf\u5b50\u7535\u6c60\u5145\u7535\u8fc7\u7a0b\u7684\u91cf\u5b50\u7279\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u56fa\u6001\u91cf\u5b50\u7535\u6c60\u4e2d\u53ef\u6269\u5c55\u91cf\u5b50\u5145\u7535\u4f18\u52bf\u7684\u5b9e\u9a8c\u5b9e\u73b0\uff0c\u4e3a\u5f00\u53d1\u9ad8\u6548\u4e14\u5b9e\u9a8c\u53ef\u884c\u7684QCA\u534f\u8bae\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u524d\u666f\uff0c\u63a8\u52a8\u4e86\u91cf\u5b50\u7535\u6c60\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.07397", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07397", "abs": "https://arxiv.org/abs/2602.07397", "authors": ["Hoang Anh Duy Le", "Sahil Joshi", "Zeyu Yang", "Zhaozhuo Xu", "Anshumali Shrivastava"], "title": "Scout Before You Attend: Sketch-and-Walk Sparse Attention for Efficient LLM Inference", "comment": null, "summary": "Self-attention dominates the computational and memory cost of long-context LLM inference across both prefill and decode phases. To address this challenge, we introduce Sketch&Walk Attention, a training-free sparse attention method that determines sparsity with lightweight sketches and deterministic walk. Sketch&Walk applies Hadamard sketching to get inexpensive approximations of attention scores, then aggregates these estimates across layers via a walk mechanism that captures attention influence beyond direct interactions between tokens. The accumulated walk scores are used to select top-k attention blocks, enabling dynamic sparsity with a single training-free algorithm that applies uniformly to both the prefill and decode phases, together with custom sparse attention kernels. Across a wide range of models and tasks, Sketch&Walk maintains near-lossless accuracy at 20% attention density and can slightly outperform dense attention in some settings, while achieving up to 6x inference speedup.", "AI": {"tldr": "Sketch&Walk Attention\uff1a\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8349\u56fe\uff08Hadamard sketching\uff09\u548c\u786e\u5b9a\u6027\u6e38\u8d70\u673a\u5236\u52a8\u6001\u9009\u62e9top-k\u6ce8\u610f\u529b\u5757\uff0c\u5728\u4fdd\u6301\u63a5\u8fd1\u65e0\u635f\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8fbe6\u500d\u7684\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u4e2d\uff08\u5305\u62ec\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\uff09\u5360\u636e\u4e86\u4e3b\u8981\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\uff0c\u9700\u8981\u9ad8\u6548\u7684\u7a00\u758f\u6ce8\u610f\u529b\u89e3\u51b3\u65b9\u6848\u6765\u964d\u4f4e\u8fd9\u4e9b\u5f00\u9500\u3002", "method": "\u4f7f\u7528Hadamard\u8349\u56fe\u6280\u672f\u5ec9\u4ef7\u8fd1\u4f3c\u6ce8\u610f\u529b\u5206\u6570\uff0c\u901a\u8fc7\u6e38\u8d70\u673a\u5236\u5728\u5c42\u95f4\u805a\u5408\u8fd9\u4e9b\u4f30\u8ba1\u503c\u4ee5\u6355\u6349\u8d85\u51fa\u76f4\u63a5token\u4ea4\u4e92\u7684\u6ce8\u610f\u529b\u5f71\u54cd\uff0c\u57fa\u4e8e\u7d2f\u79ef\u7684\u6e38\u8d70\u5206\u6570\u9009\u62e9top-k\u6ce8\u610f\u529b\u5757\uff0c\u5b9e\u73b0\u52a8\u6001\u7a00\u758f\u5316\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u548c\u4efb\u52a1\u4e0a\uff0cSketch&Walk\u572820%\u6ce8\u610f\u529b\u5bc6\u5ea6\u4e0b\u4fdd\u6301\u63a5\u8fd1\u65e0\u635f\u7684\u51c6\u786e\u6027\uff0c\u5728\u67d0\u4e9b\u8bbe\u7f6e\u4e2d\u751a\u81f3\u7565\u5fae\u4f18\u4e8e\u5bc6\u96c6\u6ce8\u610f\u529b\uff0c\u540c\u65f6\u5b9e\u73b0\u9ad8\u8fbe6\u500d\u7684\u63a8\u7406\u52a0\u901f\u3002", "conclusion": "Sketch&Walk Attention\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u65e0\u9700\u8bad\u7ec3\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\uff0c\u80fd\u663e\u8457\u964d\u4f4e\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u3002"}}
{"id": "2602.08611", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.08611", "abs": "https://arxiv.org/abs/2602.08611", "authors": ["Jingqi Sun", "Joshua Combes", "Lucas Hackl"], "title": "Representation theory of inhomogeneous Gaussian unitaries", "comment": "14+10 pages, 3 figures, 4 tables", "summary": "Gaussian unitaries, generated by quadratic Hamiltonians, are fundamental in quantum optics and continuous-variable computing. Their structures correspond to symplectic (bosons) and orthogonal (fermions) groups, but physical realizations give rise to their respective double covers, introducing phase and sign ambiguities. The homogeneous (quadratic-only) case has been resolved through a parameterization constructed in a recent work [arXiv:2409.11628]. We extend the previous framework to inhomogeneous Gaussian unitaries parameterized by $(M,z,\u03a8)$. The Baker-Campbel-Hausdorff formula allows us then to factor any Gaussian unitary into a squeezing and a displacement transformation, from which we derive the group multiplication law.", "AI": {"tldr": "\u6269\u5c55\u9ad8\u65af\u5e7a\u6b63\u7b97\u7b26\u7684\u53c2\u6570\u5316\u6846\u67b6\uff0c\u4ece\u9f50\u6b21\uff08\u4ec5\u4e8c\u6b21\uff09\u60c5\u51b5\u63a8\u5e7f\u5230\u975e\u9f50\u6b21\u60c5\u51b5\uff0c\u901a\u8fc7BCH\u516c\u5f0f\u5c06\u4efb\u610f\u9ad8\u65af\u5e7a\u6b63\u5206\u89e3\u4e3a\u538b\u7f29\u548c\u4f4d\u79fb\u53d8\u6362\uff0c\u5e76\u63a8\u5bfc\u7fa4\u4e58\u6cd5\u89c4\u5219", "motivation": "\u9ad8\u65af\u5e7a\u6b63\u7b97\u7b26\u5728\u91cf\u5b50\u5149\u5b66\u548c\u8fde\u7eed\u53d8\u91cf\u8ba1\u7b97\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7269\u7406\u5b9e\u73b0\u4f1a\u4ea7\u751f\u76f8\u4f4d\u548c\u7b26\u53f7\u6b67\u4e49\u3002\u4e4b\u524d\u7684\u5de5\u4f5c\u89e3\u51b3\u4e86\u9f50\u6b21\uff08\u4ec5\u4e8c\u6b21\uff09\u60c5\u51b5\uff0c\u9700\u8981\u6269\u5c55\u5230\u66f4\u4e00\u822c\u7684\u975e\u9f50\u6b21\u60c5\u51b5", "method": "\u6269\u5c55\u5148\u524d\u6846\u67b6\u5230\u975e\u9f50\u6b21\u9ad8\u65af\u5e7a\u6b63\uff0c\u53c2\u6570\u5316\u4e3a$(M,z,\u03a8)$\u3002\u4f7f\u7528Baker-Campbell-Hausdorff\u516c\u5f0f\u5c06\u4efb\u610f\u9ad8\u65af\u5e7a\u6b63\u5206\u89e3\u4e3a\u538b\u7f29\u53d8\u6362\u548c\u4f4d\u79fb\u53d8\u6362\uff0c\u5e76\u63a8\u5bfc\u7fa4\u4e58\u6cd5\u89c4\u5219", "result": "\u5efa\u7acb\u4e86\u975e\u9f50\u6b21\u9ad8\u65af\u5e7a\u6b63\u7b97\u7b26\u7684\u5b8c\u6574\u53c2\u6570\u5316\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u5c06\u4efb\u610f\u9ad8\u65af\u5e7a\u6b63\u5206\u89e3\u4e3a\u538b\u7f29\u548c\u4f4d\u79fb\u53d8\u6362\u7684\u65b9\u6cd5\uff0c\u5e76\u63a8\u5bfc\u4e86\u76f8\u5e94\u7684\u7fa4\u4e58\u6cd5\u89c4\u5219", "conclusion": "\u6210\u529f\u5c06\u9ad8\u65af\u5e7a\u6b63\u7b97\u7b26\u7684\u53c2\u6570\u5316\u4ece\u9f50\u6b21\u60c5\u51b5\u6269\u5c55\u5230\u975e\u9f50\u6b21\u60c5\u51b5\uff0c\u4e3a\u91cf\u5b50\u5149\u5b66\u548c\u8fde\u7eed\u53d8\u91cf\u8ba1\u7b97\u4e2d\u7684\u9ad8\u65af\u5e7a\u6b63\u63d0\u4f9b\u4e86\u66f4\u5b8c\u6574\u7684\u6570\u5b66\u63cf\u8ff0\u548c\u5b9e\u73b0\u6846\u67b6"}}
{"id": "2602.07400", "categories": ["cs.LG", "cs.ET", "cs.PF"], "pdf": "https://arxiv.org/pdf/2602.07400", "abs": "https://arxiv.org/abs/2602.07400", "authors": ["Simon B\u00fchrer", "Andreas Plesner", "Aczel Till", "Roger Wattenhofer"], "title": "BitLogic: Training Framework for Gradient-Based FPGA-Native Neural Networks", "comment": null, "summary": "The energy and latency costs of deep neural network inference are increasingly driven by deployment rather than training, motivating hardware-specialized alternatives to arithmetic-heavy models. Field-Programmable Gate Arrays (FPGAs) provide an attractive substrate for such specialization, yet existing FPGA-based neural approaches are fragmented and difficult to compare. We present BitLogic, a fully gradient-based, end-to-end trainable framework for FPGA-native neural networks built around Lookup Table (LUT) computation. BitLogic replaces multiply-accumulate operations with differentiable LUT nodes that map directly to FPGA primitives, enabling native binary computation, sparse connectivity, and efficient hardware realization. The framework offers a modular functional API supporting diverse architectures, along with learned encoders, hardware-aware heads, and multiple boundary-consistent LUT relaxations. An automated Register Transfer Level (RTL) export pipeline translates trained PyTorch models into synthesizable HDL, ensuring equivalence between software and hardware inference. Experiments across standard vision benchmarks and heterogeneous hardware platforms demonstrate competitive accuracy and substantial gains in FPGA efficiency, including 72.3% test accuracy on CIFAR-10 achieved with fewer than 0.3M logic gates, while attaining sub-20 ns single-sample inference using only LUT resources.", "AI": {"tldr": "BitLogic\uff1a\u4e00\u4e2a\u57fa\u4e8eFPGA\u539f\u751f\u67e5\u627e\u8868\uff08LUT\uff09\u8ba1\u7b97\u7684\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u7528\u53ef\u5faeLUT\u8282\u70b9\u66ff\u4ee3\u4f20\u7edf\u4e58\u52a0\u8fd0\u7b97\uff0c\u5b9e\u73b0\u9ad8\u6548\u786c\u4ef6\u90e8\u7f72\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u7684\u80fd\u8017\u548c\u5ef6\u8fdf\u6210\u672c\u4e3b\u8981\u6765\u81ea\u90e8\u7f72\u800c\u975e\u8bad\u7ec3\uff0c\u9700\u8981\u786c\u4ef6\u4e13\u7528\u5316\u65b9\u6848\u3002FPGA\u662f\u7406\u60f3\u7684\u4e13\u7528\u5316\u5e73\u53f0\uff0c\u4f46\u73b0\u6709FPGA\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u5206\u6563\u4e14\u96be\u4ee5\u6bd4\u8f83\u3002", "method": "\u63d0\u51faBitLogic\u6846\u67b6\uff1a1\uff09\u7528\u53ef\u5faeLUT\u8282\u70b9\u66ff\u4ee3\u4e58\u52a0\u8fd0\u7b97\uff0c\u76f4\u63a5\u6620\u5c04\u5230FPGA\u539f\u8bed\uff1b2\uff09\u652f\u6301\u539f\u751f\u4e8c\u8fdb\u5236\u8ba1\u7b97\u3001\u7a00\u758f\u8fde\u63a5\uff1b3\uff09\u63d0\u4f9b\u6a21\u5757\u5316API\u3001\u5b66\u4e60\u7f16\u7801\u5668\u3001\u786c\u4ef6\u611f\u77e5\u5934\uff1b4\uff09\u81ea\u52a8RTL\u5bfc\u51fa\u7ba1\u9053\u5c06PyTorch\u6a21\u578b\u8f6c\u6362\u4e3a\u53ef\u7efc\u5408HDL\u3002", "result": "\u5728\u6807\u51c6\u89c6\u89c9\u57fa\u51c6\u548c\u5f02\u6784\u786c\u4ef6\u5e73\u53f0\u4e0a\u5c55\u793a\uff1a1\uff09CIFAR-10\u8fbe\u523072.3%\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u4ec5\u4f7f\u7528\u4e0d\u523030\u4e07\u903b\u8f91\u95e8\uff1b2\uff09\u4ec5\u7528LUT\u8d44\u6e90\u5b9e\u73b0\u4e9a20\u7eb3\u79d2\u5355\u6837\u672c\u63a8\u7406\uff1b3\uff09\u5728FPGA\u6548\u7387\u65b9\u9762\u83b7\u5f97\u663e\u8457\u63d0\u5347\u3002", "conclusion": "BitLogic\u4e3aFPGA\u539f\u751f\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u68af\u5ea6\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7LUT\u8ba1\u7b97\u5b9e\u73b0\u9ad8\u6548\u786c\u4ef6\u90e8\u7f72\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u51c6\u786e\u7387\u540c\u65f6\u5927\u5e45\u63d0\u5347FPGA\u6548\u7387\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08703", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.08703", "abs": "https://arxiv.org/abs/2602.08703", "authors": ["Annie E. Paine", "Smit Chaudhary", "Antonio A. Gentile"], "title": "Weak forms offer strong regularisations: how to make physics-informed (quantum) machine learning more robust", "comment": null, "summary": "Physics-informed (PI) methodologies have surged to become a pillar route to solve Differential Equations (DEs), sustained by the growth of machine learning methods in scientific contexts. The main proposition of PI is to minimise variationally a loss function, formally ensuring that a neural surrogate of the solution has the DE locally satisfied. The nature of such formulation encouraged the exploration of equivalent quantum algorithms, where the surrogate solution is expressed by variational quantum architectures. The locality of typical loss functions emphasises the DE to hold at an ensemble of points sampled in the domain, but encounters issues when generalising beyond such points, or when propagating boundary conditions. Issues which affect classical and quantum PI algorithms alike. The quest to fill this gap in robustness and accuracy against mainstream DE solvers has led to a plethora of proposals in various directions. In particular, classical DE solvers have long employed the weak form - an integral based approach aiming at imposing a global condition on the solution - prioritising a good average behaviour instead of ``overfitting'' select points. Here, we propose and explore to combine contributions from both local and global loss functions in PI routines, to exploit the advantages and mitigate the weaknesses of both. We showcase this intuition in a variety of problems focusing on differentiable quantum architectures, and demonstrating in particular how orchestrating such hybrid loss formulation with domain decomposition can offer a strong advantage over local-only strategies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u5c40\u90e8\u548c\u5168\u5c40\u635f\u5931\u51fd\u6570\u7ed3\u5408\u5728\u7269\u7406\u4fe1\u606f\u65b9\u6cd5\u4e2d\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u5c40\u90e8\u635f\u5931\u51fd\u6570\u5728\u6cdb\u5316\u548c\u8fb9\u754c\u6761\u4ef6\u4f20\u64ad\u65b9\u9762\u7684\u95ee\u9898\uff0c\u7279\u522b\u9488\u5bf9\u91cf\u5b50\u67b6\u6784\u5c55\u793a\u4e86\u8fd9\u79cd\u6df7\u5408\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "motivation": "\u7269\u7406\u4fe1\u606f\u65b9\u6cd5\u5728\u6c42\u89e3\u5fae\u5206\u65b9\u7a0b\u65f6\u4e3b\u8981\u4f7f\u7528\u5c40\u90e8\u635f\u5931\u51fd\u6570\uff0c\u5f3a\u8c03\u5728\u91c7\u6837\u70b9\u4e0a\u6ee1\u8db3\u65b9\u7a0b\uff0c\u4f46\u5728\u6cdb\u5316\u5230\u975e\u91c7\u6837\u70b9\u548c\u4f20\u64ad\u8fb9\u754c\u6761\u4ef6\u65f6\u5b58\u5728\u95ee\u9898\u3002\u4f20\u7edf\u7ecf\u5178\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3\u5668\u4f7f\u7528\u5f31\u5f62\u5f0f\uff08\u79ef\u5206\u65b9\u6cd5\uff09\u6765\u65bd\u52a0\u5168\u5c40\u6761\u4ef6\uff0c\u4f18\u5148\u8003\u8651\u5e73\u5747\u884c\u4e3a\u800c\u975e\"\u8fc7\u62df\u5408\"\u7279\u5b9a\u70b9\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u635f\u5931\u51fd\u6570\u7684\u4f18\u52bf\uff0c\u63d0\u9ad8\u7269\u7406\u4fe1\u606f\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u5c06\u5c40\u90e8\u635f\u5931\u51fd\u6570\u548c\u5168\u5c40\u635f\u5931\u51fd\u6570\uff08\u57fa\u4e8e\u5f31\u5f62\u5f0f\u7684\u79ef\u5206\u65b9\u6cd5\uff09\u7ed3\u5408\u5728\u7269\u7406\u4fe1\u606f\u65b9\u6cd5\u4e2d\uff0c\u5f62\u6210\u6df7\u5408\u635f\u5931\u51fd\u6570\u3002\u7279\u522b\u9488\u5bf9\u53ef\u5fae\u5206\u91cf\u5b50\u67b6\u6784\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u57df\u5206\u89e3\u6280\u672f\u6765\u534f\u8c03\u8fd9\u79cd\u6df7\u5408\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u5229\u7528\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u52bf\u5e76\u51cf\u8f7b\u5404\u81ea\u7684\u5f31\u70b9\u3002", "result": "\u5728\u591a\u79cd\u95ee\u9898\u4e2d\u5c55\u793a\u4e86\u8fd9\u79cd\u6df7\u5408\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u8bc1\u660e\u4e86\u901a\u8fc7\u57df\u5206\u89e3\u6280\u672f\u534f\u8c03\u6df7\u5408\u635f\u5931\u51fd\u6570\u76f8\u6bd4\u4ec5\u4f7f\u7528\u5c40\u90e8\u7b56\u7565\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u635f\u5931\u51fd\u6570\u7684\u6df7\u5408\u65b9\u6cd5\u80fd\u591f\u63d0\u9ad8\u7269\u7406\u4fe1\u606f\u6c42\u89e3\u5fae\u5206\u65b9\u7a0b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u91cf\u5b50\u67b6\u6784\u4e2d\uff0c\u8fd9\u79cd\u6df7\u5408\u7b56\u7565\u901a\u8fc7\u57df\u5206\u89e3\u6280\u672f\u80fd\u591f\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528\u5c40\u90e8\u635f\u5931\u51fd\u6570\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.07411", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07411", "abs": "https://arxiv.org/abs/2602.07411", "authors": ["Zishi Zhang", "Tao Ren", "Yijie Peng"], "title": "Nonparametric Bayesian Optimization for General Rewards", "comment": null, "summary": "This work focuses on Bayesian optimization (BO) under reward model uncertainty. We propose the first BO algorithm that achieves no-regret guarantee in a general reward setting, requiring only Lipschitz continuity of the objective function and accommodating a broad class of measurement noise. The core of our approach is a novel surrogate model, termed as infinite Gaussian process ($\\infty$-GP). It is a Bayesian nonparametric model that places a prior on the space of reward distributions, enabling it to represent a substantially broader class of reward models than classical Gaussian process (GP). The $\\infty$-GP is used in combination with Thompson Sampling (TS) to enable effective exploration and exploitation. Correspondingly, we develop a new TS regret analysis framework for general rewards, which relates the regret to the total variation distance between the surrogate model and the true reward distribution. Furthermore, with a truncated Gibbs sampling procedure, our method is computationally scalable, incurring minimal additional memory and computational complexities compared to classical GP. Empirical results demonstrate state-of-the-art performance, particularly in settings with non-stationary, heavy-tailed, or other ill-conditioned rewards.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u5728\u4e00\u822c\u5956\u52b1\u6a21\u578b\u4e0b\u5b9e\u73b0\u65e0\u9057\u61be\u4fdd\u8bc1\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u7b97\u6cd5\uff0c\u4f7f\u7528\u65e0\u9650\u9ad8\u65af\u8fc7\u7a0b\u4f5c\u4e3a\u4ee3\u7406\u6a21\u578b\uff0c\u7ed3\u5408\u6c64\u666e\u68ee\u91c7\u6837\uff0c\u8ba1\u7b97\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u4f20\u7edf\u8d1d\u53f6\u65af\u4f18\u5316\u901a\u5e38\u5047\u8bbe\u9ad8\u65af\u8fc7\u7a0b\u5148\u9a8c\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5956\u52b1\u6a21\u578b\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\uff0c\u53ef\u80fd\u975e\u5e73\u7a33\u3001\u91cd\u5c3e\u6216\u6761\u4ef6\u4e0d\u826f\uff0c\u9700\u8981\u66f4\u901a\u7528\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u65e0\u9650\u9ad8\u65af\u8fc7\u7a0b\u4f5c\u4e3a\u8d1d\u53f6\u65af\u975e\u53c2\u6570\u6a21\u578b\uff0c\u5728\u5956\u52b1\u5206\u5e03\u7a7a\u95f4\u4e0a\u653e\u7f6e\u5148\u9a8c\uff0c\u7ed3\u5408\u6c64\u666e\u68ee\u91c7\u6837\u8fdb\u884c\u63a2\u7d22-\u5229\u7528\u5e73\u8861\uff0c\u4f7f\u7528\u622a\u65ad\u5409\u5e03\u65af\u91c7\u6837\u4fdd\u8bc1\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u3002", "result": "\u7b97\u6cd5\u5728\u4e00\u822c\u5956\u52b1\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u65e0\u9057\u61be\u4fdd\u8bc1\uff0c\u4ec5\u9700\u76ee\u6807\u51fd\u6570\u7684Lipschitz\u8fde\u7eed\u6027\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0e\u7ecf\u5178\u9ad8\u65af\u8fc7\u7a0b\u76f8\u5f53\uff0c\u5728\u975e\u5e73\u7a33\u3001\u91cd\u5c3e\u7b49\u590d\u6742\u5956\u52b1\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9996\u6b21\u4e3a\u4e00\u822c\u5956\u52b1\u6a21\u578b\u4e0b\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u7528\u7b97\u6cd5\uff0c\u65e0\u9650\u9ad8\u65af\u8fc7\u7a0b\u6846\u67b6\u6269\u5c55\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u9002\u7528\u8303\u56f4\uff0c\u5728\u590d\u6742\u73b0\u5b9e\u95ee\u9898\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.08705", "categories": ["quant-ph", "cond-mat.quant-gas", "cond-mat.str-el", "nucl-th"], "pdf": "https://arxiv.org/pdf/2602.08705", "abs": "https://arxiv.org/abs/2602.08705", "authors": ["Hiroyuki Tajima", "Masaya Nakagawa", "Haozhao Liang", "Masahito Ueda"], "title": "Non-Hermitian Renormalization Group from a Few-Body Perspective", "comment": "19 pages, 8 figures", "summary": "Non-Hermiticity plays a fundamental role in open quantum systems and describes a wide variety of effects of interactions with environments, including quantum measurement. However, understanding its consequences in strongly interacting systems is still elusive due to the interplay between non-perturbative strong correlations and non-Hermiticity. While the Wilsonian renormalization group (RG) method has been applied to tackle this problem, its foundation, based on the existence of the partition function, is ill-defined. In this paper, we establish a microscopic foundation of the non-Hermitian RG method from a few-body perspective. We show that the invariance of the scattering amplitude under RG transformations enables us to rigorously derive the non-Hermitian RG equation, giving a physically transparent interpretation of RG flows. We discuss a detailed structure of such RG flows in a non-relativistic two-body system with inelastic two-body loss, and show its relation to a non-Hermitian quantum scale anomaly. Our analysis suggests that non-Hermitian complex potentials often used in high-energy physics can be interpreted as being caused by quantum measurement, where the detection of elastically scattered particles updates the observer's knowledge, resulting in a nonunitary state change of the system. We apply our formalism to nuclear physics, find the emergence of a critical semicircle, and show that several nuclei are located near the critical semicircle in the coherent neutron-nucleus scattering. We also propose that the localized dineutron in two-neutron halo nuclei can be interpreted as the quantum measurement effect on the imaginary potential associated with absorption into the core nucleus. Our result bridges different contexts of non-Hermitian systems in high-energy and atomic, molecular, and optical physics, opening an interdisciplinary playground of non-Hermitian few-body physics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e3a\u975e\u5384\u7c73\u91cd\u6574\u5316\u7fa4\u65b9\u6cd5\u5efa\u7acb\u4e86\u5fae\u89c2\u57fa\u7840\uff0c\u4ece\u5c11\u4f53\u89d2\u5ea6\u63a8\u5bfc\u4e86\u975e\u5384\u7c73RG\u65b9\u7a0b\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u6838\u7269\u7406\uff0c\u63ed\u793a\u4e86\u975e\u5384\u7c73\u590d\u52bf\u4e0e\u91cf\u5b50\u6d4b\u91cf\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "motivation": "\u7406\u89e3\u5f3a\u76f8\u4e92\u4f5c\u7528\u7cfb\u7edf\u4e2d\u975e\u5384\u7c73\u6027\u7684\u5f71\u54cd\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u56e0\u4e3a\u975e\u5fae\u6270\u5f3a\u5173\u8054\u4e0e\u975e\u5384\u7c73\u6027\u76f8\u4e92\u4ea4\u7ec7\u3002\u4f20\u7edf\u7684Wilsonian\u91cd\u6574\u5316\u7fa4\u65b9\u6cd5\u57fa\u4e8e\u914d\u5206\u51fd\u6570\u7684\u5b58\u5728\u6027\uff0c\u8fd9\u5728\u975e\u5384\u7c73\u7cfb\u7edf\u4e2d\u5b9a\u4e49\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u5fae\u89c2\u57fa\u7840\u3002", "method": "\u4ece\u5c11\u4f53\u89d2\u5ea6\u51fa\u53d1\uff0c\u5229\u7528\u6563\u5c04\u632f\u5e45\u5728RG\u53d8\u6362\u4e0b\u7684\u4e0d\u53d8\u6027\uff0c\u4e25\u683c\u63a8\u5bfc\u975e\u5384\u7c73\u91cd\u6574\u5316\u7fa4\u65b9\u7a0b\u3002\u5728\u5177\u6709\u975e\u5f39\u6027\u4e8c\u4f53\u635f\u5931\u7684\u975e\u76f8\u5bf9\u8bba\u4e8c\u4f53\u7cfb\u7edf\u4e2d\u8be6\u7ec6\u5206\u6790RG\u6d41\u7ed3\u6784\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u6838\u7269\u7406\u4e2d\u7684\u76f8\u5e72\u4e2d\u5b50-\u6838\u6563\u5c04\u3002", "result": "\u6210\u529f\u5efa\u7acb\u4e86\u975e\u5384\u7c73RG\u65b9\u6cd5\u7684\u5fae\u89c2\u57fa\u7840\uff0c\u53d1\u73b0\u4e86\u4e34\u754c\u534a\u5706\u7684\u51fa\u73b0\uff0c\u5e76\u663e\u793a\u591a\u4e2a\u6838\u5728\u76f8\u5e72\u4e2d\u5b50-\u6838\u6563\u5c04\u4e2d\u4f4d\u4e8e\u4e34\u754c\u534a\u5706\u9644\u8fd1\u3002\u63d0\u51fa\u53cc\u4e2d\u5b50\u6655\u6838\u4e2d\u7684\u5c40\u57df\u5316\u53cc\u4e2d\u5b50\u53ef\u89e3\u91ca\u4e3a\u4e0e\u6838\u5fc3\u6838\u5438\u6536\u76f8\u5173\u7684\u865a\u52bf\u7684\u91cf\u5b50\u6d4b\u91cf\u6548\u5e94\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u9ad8\u80fd\u7269\u7406\u4e2d\u5e38\u7528\u7684\u975e\u5384\u7c73\u590d\u52bf\u63d0\u4f9b\u4e86\u91cf\u5b50\u6d4b\u91cf\u89e3\u91ca\uff0c\u5c06\u9ad8\u80fd\u7269\u7406\u4e0e\u539f\u5b50\u5206\u5b50\u5149\u5b66\u7269\u7406\u4e2d\u7684\u975e\u5384\u7c73\u7cfb\u7edf\u8054\u7cfb\u8d77\u6765\uff0c\u4e3a\u975e\u5384\u7c73\u5c11\u4f53\u7269\u7406\u5f00\u8f9f\u4e86\u8de8\u5b66\u79d1\u7814\u7a76\u5e73\u53f0\u3002"}}
{"id": "2602.07415", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07415", "abs": "https://arxiv.org/abs/2602.07415", "authors": ["Runhan Shi", "Zhicheng Zhang", "Letian Chen", "Gufeng Yu", "Yang Yang"], "title": "Learning Molecular Chirality via Chiral Determinant Kernels", "comment": "Accepted at the ICLR 2026", "summary": "Chirality is a fundamental molecular property that governs stereospecific behavior in chemistry and biology. Capturing chirality in machine learning models remains challenging due to the geometric complexity of stereochemical relationships and the limitations of traditional molecular representations that often lack explicit stereochemical encoding. Existing approaches to chiral molecular representation primarily focus on central chirality, relying on handcrafted stereochemical tags or limited 3D encodings, and thus fail to generalize to more complex forms such as axial chirality. In this work, we introduce ChiDeK (Chiral Determinant Kernels), a framework that systematically integrates stereogenic information into molecular representation learning. We propose the chiral determinant kernel to encode the SE(3)-invariant chirality matrix and employ cross-attention to integrate stereochemical information from local chiral centers into the global molecular representation. This design enables explicit modeling of chiral-related features within a unified architecture, capable of jointly encoding central and axial chirality. To support the evaluation of axial chirality, we construct a new benchmark for electronic circular dichroism (ECD) and optical rotation (OR) prediction. Across four tasks, including R/S configuration classification, enantiomer ranking, ECD spectrum prediction, and OR prediction, ChiDeK achieves substantial improvements over state-of-the-art baselines, most notably yielding over 7% higher accuracy on axially chiral tasks on average.", "AI": {"tldr": "ChiDeK\u662f\u4e00\u4e2a\u5c06\u7acb\u4f53\u5316\u5b66\u4fe1\u606f\u6574\u5408\u5230\u5206\u5b50\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u624b\u6027\u884c\u5217\u5f0f\u6838\u7f16\u7801SE(3)\u4e0d\u53d8\u7684\u624b\u6027\u77e9\u9635\uff0c\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u5c06\u5c40\u90e8\u624b\u6027\u4e2d\u5fc3\u4fe1\u606f\u96c6\u6210\u5230\u5168\u5c40\u5206\u5b50\u8868\u793a\u4e2d\uff0c\u80fd\u540c\u65f6\u7f16\u7801\u4e2d\u5fc3\u624b\u6027\u548c\u8f74\u5411\u624b\u6027\uff0c\u5728\u591a\u4e2a\u624b\u6027\u76f8\u5173\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u624b\u6027\u662f\u51b3\u5b9a\u5316\u5b66\u548c\u751f\u7269\u5b66\u4e2d\u7acb\u4f53\u7279\u5f02\u6027\u884c\u4e3a\u7684\u57fa\u672c\u5206\u5b50\u7279\u6027\u3002\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u6355\u6349\u624b\u6027\uff0c\u56e0\u4e3a\u7acb\u4f53\u5316\u5b66\u5173\u7cfb\u51e0\u4f55\u590d\u6742\uff0c\u4f20\u7edf\u5206\u5b50\u8868\u793a\u7f3a\u4e4f\u660e\u786e\u7684\u7acb\u4f53\u5316\u5b66\u7f16\u7801\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4e2d\u5fc3\u624b\u6027\uff0c\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u7684\u7acb\u4f53\u5316\u5b66\u6807\u7b7e\u6216\u6709\u9650\u76843D\u7f16\u7801\uff0c\u65e0\u6cd5\u63a8\u5e7f\u5230\u66f4\u590d\u6742\u7684\u8f74\u5411\u624b\u6027\u7b49\u5f62\u6001\u3002", "method": "\u63d0\u51faChiDeK\uff08\u624b\u6027\u884c\u5217\u5f0f\u6838\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u624b\u6027\u884c\u5217\u5f0f\u6838\u7f16\u7801SE(3)\u4e0d\u53d8\u7684\u624b\u6027\u77e9\u9635\uff0c\u91c7\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5c06\u5c40\u90e8\u624b\u6027\u4e2d\u5fc3\u7684\u7acb\u4f53\u5316\u5b66\u4fe1\u606f\u96c6\u6210\u5230\u5168\u5c40\u5206\u5b50\u8868\u793a\u4e2d\u3002\u8be5\u8bbe\u8ba1\u80fd\u591f\u5728\u7edf\u4e00\u67b6\u6784\u4e2d\u663e\u5f0f\u5efa\u6a21\u624b\u6027\u76f8\u5173\u7279\u5f81\uff0c\u540c\u65f6\u7f16\u7801\u4e2d\u5fc3\u624b\u6027\u548c\u8f74\u5411\u624b\u6027\u3002\u4e3a\u8bc4\u4f30\u8f74\u5411\u624b\u6027\uff0c\u6784\u5efa\u4e86\u65b0\u7684\u7535\u5b50\u5706\u4e8c\u8272\u6027\uff08ECD\uff09\u548c\u5149\u5b66\u65cb\u8f6c\uff08OR\uff09\u9884\u6d4b\u57fa\u51c6\u3002", "result": "\u5728\u56db\u4e2a\u4efb\u52a1\u4e0a\uff08R/S\u6784\u578b\u5206\u7c7b\u3001\u5bf9\u6620\u4f53\u6392\u5e8f\u3001ECD\u5149\u8c31\u9884\u6d4b\u3001OR\u9884\u6d4b\uff09\uff0cChiDeK\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u8f74\u5411\u624b\u6027\u4efb\u52a1\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u9ad8\u4e867%\u4ee5\u4e0a\u3002", "conclusion": "ChiDeK\u6846\u67b6\u80fd\u591f\u7cfb\u7edf\u5730\u5c06\u7acb\u4f53\u5316\u5b66\u4fe1\u606f\u6574\u5408\u5230\u5206\u5b50\u8868\u793a\u5b66\u4e60\u4e2d\uff0c\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u540c\u65f6\u5904\u7406\u4e2d\u5fc3\u624b\u6027\u548c\u8f74\u5411\u624b\u6027\uff0c\u5728\u624b\u6027\u76f8\u5173\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u7acb\u4f53\u5316\u5b66\u5173\u7cfb\u7684\u673a\u5668\u5b66\u4e60\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08772", "categories": ["quant-ph", "cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2602.08772", "abs": "https://arxiv.org/abs/2602.08772", "authors": ["Kuan-Cheng Chen", "Yongqiang Wen", "Xiaotian Xu", "Max Attwood", "Jingdong Xu", "Chen Fu", "Sami Ramadan", "Shang Yu", "Sandrine Heutz", "Mark Oxborrow"], "title": "Heterogeneous Optically-Detected Spin-Acoustic Resonance in Solid-State Molecular Thin-film", "comment": null, "summary": "We report an implementation of spin-acoustic resonance in pentacene thin films integrated on a high-quality-factor (high-Q) surface acoustic wave (SAW) resonator on a lithium niobate substrate. Heterogeneous optically detected spin-acoustic resonance (HODSAR) is an optically detected spin-resonance measurement in which the resonant drive is delivered mechanically by a surface acoustic wave (SAW). By leveraging the photo-excited triplet state of pentacene at room temperature, we demonstrate coherent spin manipulation via acoustic driving under zero externally applied magnetic field. The heterogeneously integrated device, referred to as HODSAR, utilizes spin-phonon coupling to achieve mechanically driven, zero-field spin resonance, opening avenues for room-temperature mechanically addressable spin control and device integration. We show that the high-Q multimode response of the SAW resonator enables spectrally selective acoustic addressing of triplet transitions near 105 MHz. Coherent control is evidenced by Rabi oscillations, with a Rabi frequency that increases linearly with the square root of the applied RF input power over the measured drive range, consistent with driven two-level dynamics under acoustic excitation. These results establish spin-acoustic resonance in a heterogeneously integrated molecular thin-film platform and provide a quantitative basis for benchmarking mechanically mediated spin control.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5b9e\u73b0\u4e86\u5728\u94cc\u9178\u9502\u886c\u5e95\u4e0a\u96c6\u6210\u5e76\u4e94\u82ef\u8584\u819c\u4e0e\u9ad8\u54c1\u8d28\u56e0\u6570\u8868\u9762\u58f0\u6ce2\u8c10\u632f\u5668\u7684\u81ea\u65cb-\u58f0\u5171\u632f\uff0c\u5c55\u793a\u4e86\u5ba4\u6e29\u4e0b\u96f6\u78c1\u573a\u58f0\u5b66\u9a71\u52a8\u7684\u76f8\u5e72\u81ea\u65cb\u64cd\u63a7\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u5ba4\u6e29\u4e0b\u5b9e\u73b0\u96f6\u78c1\u573a\u81ea\u65cb\u64cd\u63a7\u7684\u6280\u672f\uff0c\u901a\u8fc7\u58f0\u5b66\u9a71\u52a8\u66ff\u4ee3\u4f20\u7edf\u7684\u7535\u78c1\u9a71\u52a8\uff0c\u4e3a\u81ea\u65cb\u5668\u4ef6\u96c6\u6210\u63d0\u4f9b\u65b0\u9014\u5f84\u3002", "method": "\u91c7\u7528\u5f02\u8d28\u96c6\u6210\u65b9\u6cd5\uff0c\u5c06\u5e76\u4e94\u82ef\u8584\u819c\u96c6\u6210\u5728\u94cc\u9178\u9502\u886c\u5e95\u7684\u9ad8\u54c1\u8d28\u56e0\u6570\u8868\u9762\u58f0\u6ce2\u8c10\u632f\u5668\u4e0a\uff0c\u5229\u7528\u5e76\u4e94\u82ef\u7684\u5149\u6fc0\u53d1\u4e09\u91cd\u6001\uff0c\u901a\u8fc7\u81ea\u65cb-\u58f0\u5b50\u8026\u5408\u5b9e\u73b0\u58f0\u5b66\u9a71\u52a8\u7684\u81ea\u65cb\u5171\u632f\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86105 MHz\u9644\u8fd1\u7684\u58f0\u5b66\u5bfb\u5740\u4e09\u91cd\u6001\u8dc3\u8fc1\uff0c\u89c2\u6d4b\u5230\u62c9\u6bd4\u632f\u8361\uff0c\u62c9\u6bd4\u9891\u7387\u4e0e\u8f93\u5165\u5c04\u9891\u529f\u7387\u7684\u5e73\u65b9\u6839\u5448\u7ebf\u6027\u5173\u7cfb\uff0c\u8bc1\u660e\u4e86\u58f0\u5b66\u6fc0\u53d1\u4e0b\u7684\u76f8\u5e72\u81ea\u65cb\u64cd\u63a7\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u5f02\u8d28\u96c6\u6210\u5206\u5b50\u8584\u819c\u5e73\u53f0\u4e2d\u7684\u81ea\u65cb-\u58f0\u5171\u632f\uff0c\u4e3a\u5ba4\u6e29\u4e0b\u673a\u68b0\u53ef\u5bfb\u5740\u81ea\u65cb\u63a7\u5236\u548c\u5668\u4ef6\u96c6\u6210\u63d0\u4f9b\u4e86\u5b9a\u91cf\u57fa\u51c6\u548c\u53ef\u884c\u9014\u5f84\u3002"}}
{"id": "2602.07418", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07418", "abs": "https://arxiv.org/abs/2602.07418", "authors": ["Jian Qian", "Chen-Yu Wei"], "title": "Achieving Optimal Static and Dynamic Regret Simultaneously in Bandits with Deterministic Losses", "comment": null, "summary": "In adversarial multi-armed bandits, two performance measures are commonly used: static regret, which compares the learner to the best fixed arm, and dynamic regret, which compares it to the best sequence of arms. While optimal algorithms are known for each measure individually, there is no known algorithm achieving optimal bounds for both simultaneously. Marinov and Zimmert [2021] first showed that such simultaneous optimality is impossible against an adaptive adversary. Our work takes a first step to demonstrate its possibility against an oblivious adversary when losses are deterministic. First, we extend the impossibility result of Marinov and Zimmert [2021] to the case of deterministic losses. Then, we present an algorithm achieving optimal static and dynamic regret simultaneously against an oblivious adversary. Together, they reveal a fundamental separation between adaptive and oblivious adversaries when multiple regret benchmarks are considered simultaneously. It also provides new insight into the long open problem of simultaneously achieving optimal regret against switching benchmarks of different numbers of switches.\n  Our algorithm uses negative static regret to compensate for the exploration overhead incurred when controlling dynamic regret, and leverages Blackwell approachability to jointly control both regrets. This yields a new model selection procedure for bandits that may be of independent interest.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u5728\u5bf9\u6297\u6027\u591a\u81c2\u8001\u864e\u673a\u4e2d\uff0c\u9488\u5bf9\u786e\u5b9a\u6027\u635f\u5931\u548c\u9057\u5fd8\u6027\u5bf9\u624b\uff0c\u53ef\u4ee5\u540c\u65f6\u5b9e\u73b0\u6700\u4f18\u9759\u6001\u9057\u61be\u548c\u52a8\u6001\u9057\u61be\uff0c\u63ed\u793a\u4e86\u81ea\u9002\u5e94\u5bf9\u624b\u4e0e\u9057\u5fd8\u6027\u5bf9\u624b\u5728\u591a\u91cd\u9057\u61be\u57fa\u51c6\u4e0b\u7684\u6839\u672c\u533a\u522b\u3002", "motivation": "\u5728\u5bf9\u6297\u6027\u591a\u81c2\u8001\u864e\u673a\u4e2d\uff0c\u9759\u6001\u9057\u61be\u548c\u52a8\u6001\u9057\u61be\u662f\u4e24\u4e2a\u5e38\u7528\u6027\u80fd\u6307\u6807\u3002\u867d\u7136\u5df2\u6709\u9488\u5bf9\u5404\u81ea\u6307\u6807\u7684\u6700\u4f18\u7b97\u6cd5\uff0c\u4f46\u5c1a\u65e0\u7b97\u6cd5\u80fd\u540c\u65f6\u5b9e\u73b0\u4e24\u8005\u7684\u6700\u4f18\u754c\u3002Marinov\u548cZimmert[2021]\u9996\u6b21\u8bc1\u660e\u9488\u5bf9\u81ea\u9002\u5e94\u5bf9\u624b\u65e0\u6cd5\u5b9e\u73b0\u540c\u65f6\u6700\u4f18\u6027\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5728\u786e\u5b9a\u6027\u635f\u5931\u548c\u9057\u5fd8\u6027\u5bf9\u624b\u4e0b\u662f\u5426\u53ef\u80fd\u5b9e\u73b0\u540c\u65f6\u6700\u4f18\u3002", "method": "\u9996\u5148\u5c06Marinov\u548cZimmert\u7684\u4e0d\u53ef\u80fd\u6027\u7ed3\u679c\u6269\u5c55\u5230\u786e\u5b9a\u6027\u635f\u5931\u60c5\u51b5\uff0c\u7136\u540e\u63d0\u51fa\u65b0\u7b97\u6cd5\uff1a\u5229\u7528\u8d1f\u9759\u6001\u9057\u61be\u6765\u8865\u507f\u63a7\u5236\u52a8\u6001\u9057\u61be\u65f6\u7684\u63a2\u7d22\u5f00\u9500\uff0c\u5e76\u501f\u52a9Blackwell\u53ef\u903c\u8fd1\u6027\u8054\u5408\u63a7\u5236\u4e24\u79cd\u9057\u61be\uff0c\u4ece\u800c\u8bbe\u8ba1\u51fa\u65b0\u7684\u8001\u864e\u673a\u6a21\u578b\u9009\u62e9\u7a0b\u5e8f\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u786e\u5b9a\u6027\u635f\u5931\u548c\u9057\u5fd8\u6027\u5bf9\u624b\u4e0b\uff0c\u53ef\u4ee5\u540c\u65f6\u5b9e\u73b0\u6700\u4f18\u9759\u6001\u9057\u61be\u548c\u52a8\u6001\u9057\u61be\uff0c\u63ed\u793a\u4e86\u81ea\u9002\u5e94\u5bf9\u624b\u4e0e\u9057\u5fd8\u6027\u5bf9\u624b\u5728\u591a\u91cd\u9057\u61be\u57fa\u51c6\u4e0b\u7684\u6839\u672c\u5206\u79bb\uff0c\u4e3a\u540c\u65f6\u5b9e\u73b0\u4e0d\u540c\u5207\u6362\u6b21\u6570\u57fa\u51c6\u7684\u6700\u4f18\u9057\u61be\u8fd9\u4e00\u957f\u671f\u5f00\u653e\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u5c55\u793a\u4e86\u5728\u5bf9\u6297\u6027\u591a\u81c2\u8001\u864e\u673a\u4e2d\uff0c\u9488\u5bf9\u786e\u5b9a\u6027\u635f\u5931\u548c\u9057\u5fd8\u6027\u5bf9\u624b\uff0c\u53ef\u4ee5\u540c\u65f6\u5b9e\u73b0\u6700\u4f18\u9759\u6001\u9057\u61be\u548c\u52a8\u6001\u9057\u61be\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u91cd\u8981\u7a7a\u767d\uff0c\u5e76\u4e3a\u76f8\u5173\u5f00\u653e\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u601d\u8def\u3002"}}
{"id": "2602.08780", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.08780", "abs": "https://arxiv.org/abs/2602.08780", "authors": ["Lewis Bulled", "Yingkai Ouyang"], "title": "The equivalence of quantum deletion and insertion errors on permutation-invariant codes", "comment": "7 pages, 1 figure", "summary": "Quantum synchronisation errors are a class of quantum errors that change the number of qubits in a quantum system. The classical error correction of synchronisation errors has been well-studied, including an insertion-deletion equivalence more than a half-century ago, but little progress has been made towards the quantum counterpart since the birth of quantum error correction. We address the longstanding problem of a quantum insertion-deletion equivalence on permutation-invariant codes, detailing the conditions under which such codes are $t$-insertion error-correctable. We extend these conditions to quantum insdel errors, formulating a more restrictive set of conditions under which permutation-invariant codes are $(t,s)$-insdel error-correctable. Our work resolves many of the outstanding questions regarding the quantum error correction of synchronisation errors.", "AI": {"tldr": "\u672c\u6587\u5efa\u7acb\u4e86\u91cf\u5b50\u63d2\u5165-\u5220\u9664\u9519\u8bef\u7684\u7b49\u4ef7\u5173\u7cfb\uff0c\u4e3a\u91cf\u5b50\u540c\u6b65\u9519\u8bef\u7684\u7ea0\u9519\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u89e3\u51b3\u4e86\u91cf\u5b50\u7ea0\u9519\u9886\u57df\u957f\u671f\u5b58\u5728\u7684\u95ee\u9898\u3002", "motivation": "\u91cf\u5b50\u540c\u6b65\u9519\u8bef\u4f1a\u6539\u53d8\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u91cf\u5b50\u6bd4\u7279\u6570\u91cf\uff0c\u8fd9\u7c7b\u9519\u8bef\u7684\u7ecf\u5178\u7ea0\u9519\u5df2\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u4f46\u91cf\u5b50\u7248\u672c\u81ea\u91cf\u5b50\u7ea0\u9519\u8bde\u751f\u4ee5\u6765\u8fdb\u5c55\u751a\u5fae\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u91cf\u5b50\u63d2\u5165-\u5220\u9664\u7b49\u4ef7\u5173\u7cfb\u8fd9\u4e00\u957f\u671f\u5b58\u5728\u7684\u95ee\u9898\u3002", "method": "\u5728\u7f6e\u6362\u4e0d\u53d8\u7801\u4e0a\u5efa\u7acb\u91cf\u5b50\u63d2\u5165-\u5220\u9664\u7b49\u4ef7\u5173\u7cfb\uff0c\u8be6\u7ec6\u5206\u6790\u8fd9\u4e9b\u7801\u80fd\u591f\u7ea0\u6b63t\u4e2a\u63d2\u5165\u9519\u8bef\u7684\u6761\u4ef6\uff0c\u5e76\u5c06\u8fd9\u4e9b\u6761\u4ef6\u6269\u5c55\u5230\u91cf\u5b50\u63d2\u5165-\u5220\u9664\u9519\u8bef\uff0c\u5236\u5b9a\u66f4\u4e25\u683c\u7684(t,s)\u63d2\u5165-\u5220\u9664\u9519\u8bef\u53ef\u7ea0\u6b63\u6761\u4ef6\u3002", "result": "\u5efa\u7acb\u4e86\u91cf\u5b50\u63d2\u5165-\u5220\u9664\u9519\u8bef\u7684\u7b49\u4ef7\u5173\u7cfb\uff0c\u660e\u786e\u4e86\u7f6e\u6362\u4e0d\u53d8\u7801\u80fd\u591f\u7ea0\u6b63t\u4e2a\u63d2\u5165\u9519\u8bef\u7684\u5177\u4f53\u6761\u4ef6\uff0c\u5e76\u6269\u5c55\u5230\u4e86\u66f4\u4e00\u822c\u7684\u63d2\u5165-\u5220\u9664\u9519\u8bef\u60c5\u51b5\u3002", "conclusion": "\u672c\u6587\u89e3\u51b3\u4e86\u91cf\u5b50\u540c\u6b65\u9519\u8bef\u7ea0\u9519\u4e2d\u7684\u8bb8\u591a\u5173\u952e\u95ee\u9898\uff0c\u4e3a\u91cf\u5b50\u63d2\u5165-\u5220\u9664\u9519\u8bef\u7684\u7ea0\u9519\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u957f\u671f\u5b58\u5728\u7684\u7a7a\u767d\u3002"}}
{"id": "2602.07425", "categories": ["cs.LG", "cs.CL", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.07425", "abs": "https://arxiv.org/abs/2602.07425", "authors": ["Dingzhi Yu", "Hongyi Tao", "Yuanyu Wan", "Luo Luo", "Lijun Zhang"], "title": "Sign-Based Optimizers Are Effective Under Heavy-Tailed Noise", "comment": "Code available at https://github.com/Dingzhen230/Heavy-tailed-Noise-in-LLMs", "summary": "While adaptive gradient methods are the workhorse of modern machine learning, sign-based optimization algorithms such as Lion and Muon have recently demonstrated superior empirical performance over AdamW in training large language models (LLM). However, a theoretical understanding of why sign-based updates outperform variance-adapted methods remains elusive. In this paper, we aim to bridge the gap between theory and practice through the lens of heavy-tailed gradient noise, a phenomenon frequently observed in language modeling tasks. Theoretically, we introduce a novel generalized heavy-tailed noise condition that captures the behavior of LLMs more accurately than standard finite variance assumptions. Under this noise model, we establish sharp convergence rates of SignSGD and Lion for generalized smooth function classes, matching or surpassing previous best-known bounds. Furthermore, we extend our analysis to Muon and Muonlight, providing what is, to our knowledge, the first rigorous analysis of matrix optimization under heavy-tailed stochasticity. These results offer a strong theoretical justification for the empirical superiority of sign-based optimizers, showcasing that they are naturally suited to handle the noisy gradients associated with heavy tails. Empirically, LLM pretraining experiments validate our theoretical insights and confirm that our proposed noise models are well-aligned with practice.", "AI": {"tldr": "\u672c\u6587\u4ece\u91cd\u5c3e\u68af\u5ea6\u566a\u58f0\u89d2\u5ea6\u89e3\u91ca\u4e86\u7b26\u53f7\u4f18\u5316\u5668\uff08\u5982Lion\u3001Muon\uff09\u5728\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u4f18\u4e8eAdamW\u7b49\u81ea\u9002\u5e94\u65b9\u6cd5\u7684\u539f\u56e0\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u91cd\u5c3e\u566a\u58f0\u6a21\u578b\u5e76\u5efa\u7acb\u4e86\u6536\u655b\u7406\u8bba\u3002", "motivation": "\u7b26\u53f7\u4f18\u5316\u7b97\u6cd5\uff08\u5982Lion\u3001Muon\uff09\u5728\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8eAdamW\u7b49\u81ea\u9002\u5e94\u68af\u5ea6\u65b9\u6cd5\u7684\u7ecf\u9a8c\u6027\u80fd\uff0c\u4f46\u5176\u7406\u8bba\u539f\u56e0\u5c1a\u4e0d\u6e05\u695a\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u91cd\u5c3e\u68af\u5ea6\u566a\u58f0\u8fd9\u4e00\u5728\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u5e38\u89c1\u7684\u73b0\u8c61\uff0c\u586b\u8865\u7406\u8bba\u4e0e\u5b9e\u8df5\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "1. \u63d0\u51fa\u65b0\u7684\u5e7f\u4e49\u91cd\u5c3e\u566a\u58f0\u6761\u4ef6\uff0c\u6bd4\u6807\u51c6\u6709\u9650\u65b9\u5dee\u5047\u8bbe\u66f4\u51c6\u786e\u5730\u6355\u6349LLM\u68af\u5ea6\u884c\u4e3a\uff1b2. \u5728\u8be5\u566a\u58f0\u6a21\u578b\u4e0b\uff0c\u4e3a\u5e7f\u4e49\u5149\u6ed1\u51fd\u6570\u7c7b\u5efa\u7acbSignSGD\u548cLion\u7684\u5c16\u9510\u6536\u655b\u7387\uff1b3. \u5c06\u5206\u6790\u6269\u5c55\u5230Muon\u548cMuonlight\uff0c\u9996\u6b21\u63d0\u4f9b\u77e9\u9635\u4f18\u5316\u5728\u91cd\u5c3e\u968f\u673a\u6027\u4e0b\u7684\u4e25\u683c\u5206\u6790\uff1b4. \u901a\u8fc7LLM\u9884\u8bad\u7ec3\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u89c1\u89e3\u3002", "result": "1. \u5efa\u7acb\u4e86\u7b26\u53f7\u4f18\u5316\u5668\u5728\u91cd\u5c3e\u566a\u58f0\u4e0b\u7684\u6536\u655b\u7406\u8bba\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u5148\u524d\u5df2\u77e5\u7684\u6700\u4f73\u8fb9\u754c\uff1b2. \u4e3aMuon\u548cMuonlight\u63d0\u4f9b\u4e86\u9996\u4e2a\u91cd\u5c3e\u968f\u673a\u6027\u4e0b\u7684\u4e25\u683c\u5206\u6790\uff1b3. \u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u89c1\u89e3\uff0c\u8868\u660e\u63d0\u51fa\u7684\u566a\u58f0\u6a21\u578b\u4e0e\u5b9e\u8df5\u76f8\u7b26\uff1b4. \u4e3a\u7b26\u53f7\u4f18\u5316\u5668\u7684\u7ecf\u9a8c\u4f18\u8d8a\u6027\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u7406\u8bba\u4f9d\u636e\u3002", "conclusion": "\u7b26\u53f7\u4f18\u5316\u5668\u5728\u5904\u7406\u91cd\u5c3e\u68af\u5ea6\u566a\u58f0\u65b9\u9762\u5177\u6709\u5929\u7136\u4f18\u52bf\uff0c\u8fd9\u89e3\u91ca\u4e86\u5b83\u4eec\u5728\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u4f18\u4e8e\u81ea\u9002\u5e94\u68af\u5ea6\u65b9\u6cd5\u7684\u539f\u56e0\u3002\u63d0\u51fa\u7684\u91cd\u5c3e\u566a\u58f0\u6a21\u578b\u66f4\u51c6\u786e\u5730\u53cd\u6620\u4e86LLM\u7684\u5b9e\u9645\u68af\u5ea6\u884c\u4e3a\uff0c\u4e3a\u7b26\u53f7\u4f18\u5316\u5668\u7684\u4f18\u8d8a\u6027\u80fd\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.08834", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.08834", "abs": "https://arxiv.org/abs/2602.08834", "authors": ["Yu Liu", "Martin B. Plenio"], "title": "High-Probability Heralded Entanglement via Repeated Spin-Photon Phase Encoding with Moderate Cooperativity", "comment": "25 pages, 15 figures", "summary": "We propose a heralded high-probability scheme to generate remote entanglement between moderate-cooperativity spin-cavity registers with high fidelity. In conventional single-shot interfaces, limited cooperativity restricts the spin-conditional optical response and thus strongly suppresses the success probability. Our proposal instead recycles a single incident photon for repeated interactions with the spin-cavity register, such that a small spin-conditional phase shift acquired on each round trip accumulates coherently to enable remote entanglement. Moreover, the repeated scheme enables higher spin-photon encoding efficiency by using a spectral-width-scaling photon pulse with a shorter duration. We show that, for realistic imperfections and losses, this repeated phase-encoding approach produces high-fidelity entangled states with an appreciable success probability even at cooperativity $C\\sim1$. Our protocol is particularly well suited to weakly coupled, cavity-based solid-state spin platforms and provides a route toward hybrid, photon-loss-tolerant distributed quantum computing.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u91cd\u590d\u76f8\u4f4d\u7f16\u7801\u7684\u8fdc\u7a0b\u7ea0\u7f20\u751f\u6210\u65b9\u6848\uff0c\u901a\u8fc7\u5355\u5149\u5b50\u4e0e\u81ea\u65cb-\u8154\u7cfb\u7edf\u7684\u591a\u6b21\u76f8\u4e92\u4f5c\u7528\uff0c\u5728\u4e2d\u7b49\u8026\u5408\u5ea6\u4e0b\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u7ea0\u7f20", "motivation": "\u4f20\u7edf\u5355\u6b21\u76f8\u4e92\u4f5c\u7528\u65b9\u6848\u4e2d\uff0c\u6709\u9650\u7684\u8026\u5408\u5ea6\u9650\u5236\u4e86\u81ea\u65cb\u4f9d\u8d56\u7684\u5149\u5b66\u54cd\u5e94\uff0c\u5bfc\u81f4\u6210\u529f\u6982\u7387\u6781\u4f4e\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5728\u4e2d\u7b49\u8026\u5408\u5ea6\u4e0b\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u8fdc\u7a0b\u7ea0\u7f20\u7684\u65b9\u6848", "method": "\u91c7\u7528\u91cd\u590d\u76f8\u4f4d\u7f16\u7801\u65b9\u6cd5\uff0c\u8ba9\u5355\u4e2a\u5165\u5c04\u5149\u5b50\u4e0e\u81ea\u65cb-\u8154\u7cfb\u7edf\u8fdb\u884c\u591a\u6b21\u5f80\u8fd4\u76f8\u4e92\u4f5c\u7528\u3002\u6bcf\u6b21\u5f80\u8fd4\u83b7\u5f97\u7684\u5c0f\u76f8\u4f4d\u504f\u79fb\u76f8\u5e72\u7d2f\u79ef\uff0c\u540c\u65f6\u4f7f\u7528\u9891\u8c31\u5bbd\u5ea6\u7f29\u653e\u7684\u5149\u5b50\u8109\u51b2\u63d0\u9ad8\u7f16\u7801\u6548\u7387", "result": "\u5373\u4f7f\u5728\u8026\u5408\u5ea6C~1\u7684\u73b0\u5b9e\u6761\u4ef6\u4e0b\uff0c\u8be5\u65b9\u6848\u4e5f\u80fd\u4ee5\u53ef\u89c2\u7684\u6210\u529f\u6982\u7387\u4ea7\u751f\u9ad8\u4fdd\u771f\u5ea6\u7ea0\u7f20\u6001\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5f31\u8026\u5408\u7684\u56fa\u6001\u81ea\u65cb\u5e73\u53f0", "conclusion": "\u8be5\u91cd\u590d\u76f8\u4f4d\u7f16\u7801\u65b9\u6848\u4e3a\u5f31\u8026\u5408\u56fa\u6001\u81ea\u65cb\u5e73\u53f0\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u8fdc\u7a0b\u7ea0\u7f20\u7684\u9014\u5f84\uff0c\u6709\u671b\u63a8\u52a8\u6df7\u5408\u578b\u3001\u5149\u5b50\u635f\u8017\u5bb9\u5fcd\u7684\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u7b97\u53d1\u5c55"}}
{"id": "2602.07429", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07429", "abs": "https://arxiv.org/abs/2602.07429", "authors": ["Yuanxu Sun", "Yuezhou Ma", "Haixu Wu", "Guanyang Zeng", "Muye Chen", "Jianmin Wang", "Mingsheng Long"], "title": "Brep2Shape: Boundary and Shape Representation Alignment via Self-Supervised Transformers", "comment": null, "summary": "Boundary representation (B-rep) is the industry standard for computer-aided design (CAD). While deep learning shows promise in processing B-rep models, existing methods suffer from a representation gap: continuous approaches offer analytical precision but are visually abstract, whereas discrete methods provide intuitive clarity at the expense of geometric precision. To bridge this gap, we introduce Brep2Shape, a novel self-supervised pre-training method designed to align abstract boundary representations with intuitive shape representations. Our method employs a geometry-aware task where the model learns to predict dense spatial points from parametric B\u00e9zier control points, enabling the network to better understand physical manifolds derived from abstract coefficients. To enhance this alignment, we propose a Dual Transformer backbone with parallel streams that independently encode surface and curve tokens to capture their distinct geometric properties. Moreover, the topology attention is integrated to model the interdependencies between surfaces and curves, thereby maintaining topological consistency. Experimental results demonstrate that Brep2Shape offers significant scalability, achieving state-of-the-art accuracy and faster convergence across various downstream tasks.", "AI": {"tldr": "Brep2Shape\uff1a\u4e00\u79cd\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u4efb\u52a1\u548c\u5bf9\u5076Transformer\u67b6\u6784\uff0c\u5c06\u62bd\u8c61\u7684\u8fb9\u754c\u8868\u793a\u4e0e\u76f4\u89c2\u7684\u5f62\u72b6\u8868\u793a\u5bf9\u9f50\uff0c\u89e3\u51b3\u4e86CAD\u4e2dB-rep\u6a21\u578b\u7684\u8868\u793a\u5dee\u8ddd\u95ee\u9898\u3002", "motivation": "\u73b0\u6709B-rep\u5904\u7406\u65b9\u6cd5\u5b58\u5728\u8868\u793a\u5dee\u8ddd\uff1a\u8fde\u7eed\u65b9\u6cd5\u5177\u6709\u5206\u6790\u7cbe\u5ea6\u4f46\u89c6\u89c9\u62bd\u8c61\uff0c\u79bb\u6563\u65b9\u6cd5\u76f4\u89c2\u6e05\u6670\u4f46\u51e0\u4f55\u7cbe\u5ea6\u4e0d\u8db3\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u65e2\u80fd\u4fdd\u6301\u51e0\u4f55\u7cbe\u5ea6\u53c8\u80fd\u63d0\u4f9b\u76f4\u89c2\u7684\u5f62\u72b6\u7406\u89e3\u3002", "method": "\u63d0\u51faBrep2Shape\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff1a1\uff09\u51e0\u4f55\u611f\u77e5\u4efb\u52a1\uff1a\u4ece\u53c2\u6570\u5316B\u00e9zier\u63a7\u5236\u70b9\u9884\u6d4b\u5bc6\u96c6\u7a7a\u95f4\u70b9\uff1b2\uff09\u53ccTransformer\u9aa8\u5e72\u7f51\u7edc\uff1a\u5e76\u884c\u7f16\u7801\u8868\u9762\u548c\u66f2\u7ebftoken\u4ee5\u6355\u83b7\u4e0d\u540c\u51e0\u4f55\u7279\u6027\uff1b3\uff09\u62d3\u6251\u6ce8\u610f\u529b\uff1a\u5efa\u6a21\u8868\u9762\u548c\u66f2\u7ebf\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\u4ee5\u4fdd\u6301\u62d3\u6251\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eBrep2Shape\u5177\u6709\u663e\u8457\u7684\u53ef\u6269\u5c55\u6027\uff0c\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\u548c\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "Brep2Shape\u6210\u529f\u6865\u63a5\u4e86B-rep\u5904\u7406\u4e2d\u7684\u8868\u793a\u5dee\u8ddd\uff0c\u901a\u8fc7\u5c06\u62bd\u8c61\u8fb9\u754c\u8868\u793a\u4e0e\u76f4\u89c2\u5f62\u72b6\u8868\u793a\u5bf9\u9f50\uff0c\u4e3aCAD\u4e2d\u7684\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08854", "categories": ["quant-ph", "cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2602.08854", "abs": "https://arxiv.org/abs/2602.08854", "authors": ["Danial Shafizadeh", "Misagh Ghezellou", "Viktor M. Bobal", "Lasse Vines", "Jawad Ul-Hassan", "Valdas Jokubavicius", "Nguyen T. Son", "Ivan G. Ivanov"], "title": "Spin-active chlorine-related centers in 4H-SiC with telecom-band emissions", "comment": null, "summary": "A photoluminescence (PL) and magnetic resonance investigation of a defect in chlorine-implanted 4H-SiC is presented. This Cl-related center emits light at telecom wavelengths with zero-phonon lines in the range 1350-1540 nm. Its four configurations exhibit stable PL spectra characterized by narrow zero-phonon lines. For the two configurations that emit light at the C-band, a Debye-Waller factor in the range 22-25% is estimated. Optically detected magnetic resonance confirms that the Cl-related center is spin active and stable at room temperature with the zero-field splitting in the range of 1.0-1.4 GHz. The combined optical and spin properties suggest this center to be a highly promising candidate for scalable quantum networks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6c2f\u79bb\u5b50\u6ce8\u51654H-SiC\u4e2d\u7684\u7f3a\u9677\uff0c\u8be5\u7f3a\u9677\u5728\u7535\u4fe1\u6ce2\u957f\uff081350-1540 nm\uff09\u53d1\u5c04\u5149\uff0c\u5177\u6709\u7a84\u96f6\u58f0\u5b50\u7ebf\u548c22-25%\u7684\u5fb7\u62dc-\u6c83\u52d2\u56e0\u5b50\uff0c\u4e14\u5177\u6709\u5ba4\u6e29\u7a33\u5b9a\u7684\u81ea\u65cb\u6d3b\u6027\uff0c\u662f\u91cf\u5b50\u7f51\u7edc\u5e94\u7528\u7684\u7406\u60f3\u5019\u9009\u3002", "motivation": "\u7814\u7a76\u6c2f\u79bb\u5b50\u6ce8\u51654H-SiC\u4e2d\u7f3a\u9677\u7684\u5149\u5b66\u548c\u81ea\u65cb\u7279\u6027\uff0c\u63a2\u7d22\u5176\u5728\u91cf\u5b50\u4fe1\u606f\u6280\u672f\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u662f\u4f5c\u4e3a\u53ef\u6269\u5c55\u91cf\u5b50\u7f51\u7edc\u7684\u5019\u9009\u6750\u6599\u3002", "method": "\u91c7\u7528\u5149\u81f4\u53d1\u5149\uff08PL\uff09\u548c\u78c1\u5171\u632f\u6280\u672f\u7814\u7a76\u6c2f\u79bb\u5b50\u6ce8\u51654H-SiC\u4e2d\u7684\u7f3a\u9677\uff0c\u5206\u6790\u5176\u5149\u5b66\u53d1\u5c04\u7279\u6027\u3001\u96f6\u58f0\u5b50\u7ebf\u3001\u5fb7\u62dc-\u6c83\u52d2\u56e0\u5b50\uff0c\u5e76\u901a\u8fc7\u5149\u68c0\u6d4b\u78c1\u5171\u632f\u9a8c\u8bc1\u81ea\u65cb\u6d3b\u6027\u548c\u5ba4\u6e29\u7a33\u5b9a\u6027\u3002", "result": "\u53d1\u73b0\u6c2f\u76f8\u5173\u4e2d\u5fc3\u5728\u7535\u4fe1\u6ce2\u957f\uff081350-1540 nm\uff09\u53d1\u5c04\u5149\uff0c\u5177\u6709\u56db\u4e2a\u7a33\u5b9a\u914d\u7f6e\uff0cC\u6ce2\u6bb5\u53d1\u5c04\u7684\u4e24\u79cd\u914d\u7f6e\u5fb7\u62dc-\u6c83\u52d2\u56e0\u5b50\u4e3a22-25%\uff0c\u96f6\u573a\u5206\u88c2\u57281.0-1.4 GHz\u8303\u56f4\u5185\uff0c\u81ea\u65cb\u6d3b\u6027\u5728\u5ba4\u6e29\u4e0b\u7a33\u5b9a\u3002", "conclusion": "\u8be5\u6c2f\u76f8\u5173\u4e2d\u5fc3\u7ed3\u5408\u4e86\u4f18\u5f02\u7684\u5149\u5b66\u7279\u6027\u548c\u5ba4\u6e29\u7a33\u5b9a\u7684\u81ea\u65cb\u7279\u6027\uff0c\u662f\u6784\u5efa\u53ef\u6269\u5c55\u91cf\u5b50\u7f51\u7edc\u7684\u6781\u5177\u524d\u666f\u7684\u5019\u9009\u6750\u6599\u3002"}}
{"id": "2602.07440", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07440", "abs": "https://arxiv.org/abs/2602.07440", "authors": ["C\u00e9dric Jung", "Shirin Salehi", "Anke Schmeink"], "title": "Active Learning Using Aggregated Acquisition Functions: Accuracy and Sustainability Analysis", "comment": null, "summary": "Active learning (AL) is a machine learning (ML) approach that strategically selects the most informative samples for annotation during training, aiming to minimize annotation costs. This strategy not only reduces labeling expenses but also results in energy savings during neural network training, thereby enhancing both data and energy efficiency. In this paper, we implement and evaluate various state-of-the-art acquisition functions, analyzing their accuracy and computational costs, while discussing the advantages and disadvantages of each method. Our findings reveal that representativity-based acquisition functions effectively explore the dataset but do not prioritize boundary decisions, whereas uncertainty-based acquisition functions focus on refining boundary decisions already identified by the neural network. This trade-off is known as the exploration-exploitation dilemma. To address this dilemma, we introduce six aggregation structures: series, parallel, hybrid, adaptive feedback, random exploration, and annealing exploration. Our aggregated acquisition functions alleviate common AL pathologies such as batch mode inefficiency and the cold start problem. Additionally, we focus on balancing accuracy and energy consumption, contributing to the development of more sustainable, energy-aware artificial intelligence (AI). We evaluate our proposed structures on various models and datasets. Our results demonstrate the potential of these structures to reduce computational costs while maintaining or even improving accuracy. Innovative aggregation approaches, such as alternating between acquisition functions such as BALD and BADGE, have shown robust results. Sequentially running functions like $K$-Centers followed by BALD has achieved the same performance goals with up to 12\\% fewer samples, while reducing the acquisition cost by almost half.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u516d\u79cd\u805a\u5408\u83b7\u53d6\u51fd\u6570\u7ed3\u6784\u6765\u89e3\u51b3\u4e3b\u52a8\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u56f0\u5883\uff0c\u5728\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u548c\u6807\u6ce8\u6837\u672c\u7684\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u6a21\u578b\u7cbe\u5ea6\uff0c\u5b9e\u73b0\u66f4\u53ef\u6301\u7eed\u7684AI\u3002", "motivation": "\u4e3b\u52a8\u5b66\u4e60\u901a\u8fc7\u9009\u62e9\u4fe1\u606f\u91cf\u6700\u5927\u7684\u6837\u672c\u8fdb\u884c\u6807\u6ce8\u6765\u964d\u4f4e\u6807\u6ce8\u6210\u672c\uff0c\u4f46\u73b0\u6709\u83b7\u53d6\u51fd\u6570\u5b58\u5728\u63a2\u7d22-\u5229\u7528\u56f0\u5883\uff1a\u57fa\u4e8e\u4ee3\u8868\u6027\u7684\u65b9\u6cd5\u80fd\u63a2\u7d22\u6570\u636e\u96c6\u4f46\u4e0d\u5173\u6ce8\u51b3\u7b56\u8fb9\u754c\uff0c\u800c\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u4e13\u6ce8\u4e8e\u5df2\u8bc6\u522b\u7684\u8fb9\u754c\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u56f0\u5883\uff0c\u540c\u65f6\u5e73\u8861\u7cbe\u5ea6\u4e0e\u80fd\u8017\uff0c\u53d1\u5c55\u66f4\u53ef\u6301\u7eed\u7684AI\u3002", "method": "\u63d0\u51fa\u516d\u79cd\u805a\u5408\u83b7\u53d6\u51fd\u6570\u7ed3\u6784\uff1a\u4e32\u8054\u3001\u5e76\u8054\u3001\u6df7\u5408\u3001\u81ea\u9002\u5e94\u53cd\u9988\u3001\u968f\u673a\u63a2\u7d22\u548c\u9000\u706b\u63a2\u7d22\u3002\u8fd9\u4e9b\u7ed3\u6784\u7ed3\u5408\u4e0d\u540c\u83b7\u53d6\u51fd\u6570\uff08\u5982BALD\u548cBADGE\uff09\u7684\u4f18\u52bf\uff0c\u7f13\u89e3\u6279\u91cf\u6a21\u5f0f\u4f4e\u6548\u548c\u51b7\u542f\u52a8\u95ee\u9898\u3002\u5728\u591a\u79cd\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u8fd9\u4e9b\u7ed3\u6784\u3002", "result": "\u805a\u5408\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u7cbe\u5ea6\u3002\u4f8b\u5982\uff0cBALD\u548cBADGE\u4ea4\u66ff\u4f7f\u7528\u8868\u73b0\u7a33\u5065\uff1bK-Centers\u540e\u63a5BALD\u7684\u4e32\u8054\u7ed3\u6784\u7528\u5c1112%\u7684\u6837\u672c\u8fbe\u5230\u76f8\u540c\u6027\u80fd\uff0c\u83b7\u53d6\u6210\u672c\u964d\u4f4e\u8fd1\u4e00\u534a\u3002\u8fd9\u4e9b\u7ed3\u6784\u6709\u6548\u7f13\u89e3\u4e86\u4e3b\u52a8\u5b66\u4e60\u7684\u5e38\u89c1\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u7684\u805a\u5408\u83b7\u53d6\u51fd\u6570\u7ed3\u6784\u6210\u529f\u89e3\u51b3\u4e86\u4e3b\u52a8\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u56f0\u5883\uff0c\u5728\u51cf\u5c11\u6807\u6ce8\u6837\u672c\u548c\u8ba1\u7b97\u80fd\u8017\u7684\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u5b9e\u73b0\u66f4\u53ef\u6301\u7eed\u3001\u80fd\u6e90\u611f\u77e5\u7684\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2602.08863", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.08863", "abs": "https://arxiv.org/abs/2602.08863", "authors": ["Tess Troisi", "Yoann Pelet", "Romain Dalidet", "Gregory Sauder", "Olivier Alibart", "S\u00e9bastien Tanzilli", "Anthony Martin"], "title": "High-brightness fiber-based Sagnac source of entangled photon pairs for multiplexed quantum networks", "comment": null, "summary": "A fully fibered source of entangled photon pairs based on a nonlinear Sagnac interferometer is reported. Operating at telecom wavelengths, the source relies exclusively on standard fiber-optic components and periodically poled lithium niobate (PPLN) waveguides, resulting in a compact, robust, and field-deployable architecture. The generation stage supports both polarization and energy-time entanglement without modification, enabling versatile operation depending on the targeted application. Broadband spontaneous parametric down-conversion allows dense wavelength-division multiplexing over the telecom C and L bands. High normalized brightness (10.3 kpairs/s/nm/mW$^2$) is achieved on a standard 100 GHz ITU channel pair, together with high entanglement quality. Polarization and energy-time encodings are characterized through state tomography and two-photon interference measurements, yielding fidelities, purities, and visibilities exceeding 96 % over multiple wavelength channels. The stability and reproducibility of the source are further evaluated through long-duration operation in a network environment. These results demonstrate that the proposed Sagnac source constitutes a practical and scalable building block for future plug-and-play quantum communication and quantum networking platforms.", "AI": {"tldr": "\u57fa\u4e8e\u975e\u7ebf\u6027Sagnac\u5e72\u6d89\u4eea\u7684\u5168\u5149\u7ea4\u7ea0\u7f20\u5149\u5b50\u5bf9\u6e90\uff0c\u5de5\u4f5c\u5728\u7535\u4fe1\u6ce2\u957f\uff0c\u4f7f\u7528\u6807\u51c6\u5149\u7ea4\u7ec4\u4ef6\u548cPPLN\u6ce2\u5bfc\uff0c\u652f\u6301\u504f\u632f\u548c\u80fd\u91cf-\u65f6\u95f4\u7ea0\u7f20\uff0c\u5177\u6709\u9ad8\u4eae\u5ea6\u548c\u9ad8\u7ea0\u7f20\u8d28\u91cf\u3002", "motivation": "\u5f00\u53d1\u7d27\u51d1\u3001\u575a\u56fa\u3001\u53ef\u73b0\u573a\u90e8\u7f72\u7684\u7ea0\u7f20\u5149\u5b50\u6e90\uff0c\u7528\u4e8e\u672a\u6765\u5373\u63d2\u5373\u7528\u91cf\u5b50\u901a\u4fe1\u548c\u91cf\u5b50\u7f51\u7edc\u5e73\u53f0\uff0c\u9700\u8981\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u6784\u5efa\u6a21\u5757\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u975e\u7ebf\u6027Sagnac\u5e72\u6d89\u4eea\u7684\u5168\u5149\u7ea4\u67b6\u6784\uff0c\u4f7f\u7528\u6807\u51c6\u5149\u7ea4\u7ec4\u4ef6\u548c\u5468\u671f\u6027\u6781\u5316\u94cc\u9178\u9502\u6ce2\u5bfc\uff0c\u652f\u6301\u5bbd\u5e26\u81ea\u53d1\u53c2\u91cf\u4e0b\u8f6c\u6362\uff0c\u5b9e\u73b0\u5bc6\u96c6\u6ce2\u5206\u590d\u7528\u3002", "result": "\u5728\u6807\u51c6100 GHz ITU\u4fe1\u9053\u5bf9\u4e0a\u5b9e\u73b0\u9ad8\u5f52\u4e00\u5316\u4eae\u5ea6\uff0810.3 kpairs/s/nm/mW\u00b2\uff09\uff0c\u504f\u632f\u548c\u80fd\u91cf-\u65f6\u95f4\u7f16\u7801\u7684\u4fdd\u771f\u5ea6\u3001\u7eaf\u5ea6\u548c\u53ef\u89c1\u5ea6\u8d85\u8fc796%\uff0c\u5728\u591a\u4e2a\u6ce2\u957f\u901a\u9053\u4e0a\u8868\u73b0\u51fa\u9ad8\u7ea0\u7f20\u8d28\u91cf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684Sagnac\u6e90\u6784\u6210\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u6784\u5efa\u6a21\u5757\uff0c\u9002\u7528\u4e8e\u672a\u6765\u5373\u63d2\u5373\u7528\u91cf\u5b50\u901a\u4fe1\u548c\u91cf\u5b50\u7f51\u7edc\u5e73\u53f0\uff0c\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2602.07441", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07441", "abs": "https://arxiv.org/abs/2602.07441", "authors": ["Jinzong Dong", "Wei Huang", "Jianshu Zhang", "Zhuo Chen", "Xinzhe Yuan", "Qinying Gu", "Zhaohui Jiang", "Nanyang Ye"], "title": "Proximal Action Replacement for Behavior Cloning Actor-Critic in Offline Reinforcement Learning", "comment": null, "summary": "Offline reinforcement learning (RL) optimizes policies from a previously collected static dataset and is an important branch of RL. A popular and promising approach is to regularize actor-critic methods with behavior cloning (BC), which yields realistic policies and mitigates bias from out-of-distribution actions, but can impose an often-overlooked performance ceiling: when dataset actions are suboptimal, indiscriminate imitation structurally prevents the actor from fully exploiting high-value regions suggested by the critic, especially in later training when imitation is already dominant. We formally analyzed this limitation by investigating convergence properties of BC-regularized actor-critic optimization and verified it on a controlled continuous bandit task. To break this ceiling, we propose proximal action replacement (PAR), a plug-and-play training sample replacer that progressively replaces low-value actions with high-value actions generated by a stable actor, broadening the action exploration space while reducing the impact of low-value data. PAR is compatible with multiple BC regularization paradigms. Extensive experiments across offline RL benchmarks show that PAR consistently improves performance and approaches state-of-the-art when combined with the basic TD3+BC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPAR\u65b9\u6cd5\u89e3\u51b3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u884c\u4e3a\u514b\u9686\u6b63\u5219\u5316\u5bfc\u81f4\u7684\u6027\u80fd\u4e0a\u9650\u95ee\u9898\uff0c\u901a\u8fc7\u6e10\u8fdb\u66ff\u6362\u4f4e\u4ef7\u503c\u52a8\u4f5c\u4e3a\u9ad8\u4ef7\u503c\u52a8\u4f5c\u6765\u6269\u5c55\u63a2\u7d22\u7a7a\u95f4", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u884c\u4e3a\u514b\u9686\u6b63\u5219\u5316\u867d\u7136\u80fd\u4ea7\u751f\u73b0\u5b9e\u7b56\u7565\u5e76\u7f13\u89e3\u5206\u5e03\u5916\u52a8\u4f5c\u7684\u504f\u5dee\uff0c\u4f46\u5f53\u6570\u636e\u96c6\u52a8\u4f5c\u6b21\u4f18\u65f6\uff0c\u4e0d\u52a0\u533a\u5206\u7684\u6a21\u4eff\u4f1a\u7ed3\u6784\u6027\u5730\u963b\u6b62\u667a\u80fd\u4f53\u5145\u5206\u5229\u7528\u8bc4\u8bba\u5bb6\u5efa\u8bae\u7684\u9ad8\u4ef7\u503c\u533a\u57df\uff0c\u5c24\u5176\u662f\u5728\u8bad\u7ec3\u540e\u671f\u6a21\u4eff\u5df2\u5360\u4e3b\u5bfc\u65f6\uff0c\u8fd9\u5f62\u6210\u4e86\u6027\u80fd\u4e0a\u9650", "method": "\u63d0\u51fa\u8fd1\u7aef\u52a8\u4f5c\u66ff\u6362(PAR)\uff0c\u8fd9\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u8bad\u7ec3\u6837\u672c\u66ff\u6362\u5668\uff0c\u9010\u6b65\u5c06\u4f4e\u4ef7\u503c\u52a8\u4f5c\u66ff\u6362\u4e3a\u7531\u7a33\u5b9a\u667a\u80fd\u4f53\u751f\u6210\u7684\u9ad8\u4ef7\u503c\u52a8\u4f5c\uff0c\u4ece\u800c\u6269\u5c55\u52a8\u4f5c\u63a2\u7d22\u7a7a\u95f4\u540c\u65f6\u51cf\u5c11\u4f4e\u4ef7\u503c\u6570\u636e\u7684\u5f71\u54cd\u3002PAR\u4e0e\u591a\u79cd\u884c\u4e3a\u514b\u9686\u6b63\u5219\u5316\u8303\u5f0f\u517c\u5bb9", "result": "\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPAR\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff0c\u5f53\u4e0e\u57fa\u672c\u7684TD3+BC\u7ed3\u5408\u65f6\u80fd\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73", "conclusion": "PAR\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u884c\u4e3a\u514b\u9686\u6b63\u5219\u5316\u5bfc\u81f4\u7684\u6027\u80fd\u4e0a\u9650\u95ee\u9898\uff0c\u901a\u8fc7\u6e10\u8fdb\u66ff\u6362\u673a\u5236\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6027\u80fd"}}
{"id": "2602.08869", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.08869", "abs": "https://arxiv.org/abs/2602.08869", "authors": ["Shinyoung Hwang", "Sangyeon Lee", "Eunjong Kim"], "title": "A cavity-mediated reconfigurable coupling scheme for superconducting qubits", "comment": null, "summary": "Superconducting qubits have achieved remarkable progress in gate fidelity and coherence, yet their typical nearest-neighbor connectivity presents constraints for implementing complex quantum circuits. Here, we introduce a cavity-mediated coupling architecture in which a shared cavity mode, accessed through tunable qubit-cavity couplers, enables dynamically reconfigurable interactions between non-adjacent qubits. By selectively activating the couplers, we demonstrate that high-fidelity iSWAP and CZ gates can be performed within 50 ns with simulated coherent error below $10^{-4}$, while residual $ZZ$ interaction during idling remains below a few kilohertz. Extending to a four-qubit system, we also simulate gates between every qubit pair by selectively enabling the couplers with low qubit crosstalk. This approach provides a practical route toward enhanced interaction flexibility in superconducting quantum processors and may serve as a useful building block for devices that benefit from selective non-local coupling.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8154\u4ecb\u5bfc\u8026\u5408\u7684\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u67b6\u6784\uff0c\u901a\u8fc7\u53ef\u8c03\u8c10\u7684\u91cf\u5b50\u6bd4\u7279-\u8154\u8026\u5408\u5668\u5b9e\u73b0\u975e\u76f8\u90bb\u91cf\u5b50\u6bd4\u7279\u4e4b\u95f4\u7684\u52a8\u6001\u53ef\u91cd\u6784\u76f8\u4e92\u4f5c\u7528\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u91cf\u5b50\u5904\u7406\u5668\u7684\u8fde\u63a5\u7075\u6d3b\u6027\u3002", "motivation": "\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u5728\u95e8\u4fdd\u771f\u5ea6\u548c\u76f8\u5e72\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5178\u578b\u7684\u6700\u8fd1\u90bb\u8fde\u63a5\u6027\u9650\u5236\u4e86\u590d\u6742\u91cf\u5b50\u7535\u8def\u7684\u5b9e\u73b0\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5b9e\u73b0\u975e\u76f8\u90bb\u91cf\u5b50\u6bd4\u7279\u4e4b\u95f4\u7075\u6d3b\u4e92\u8fde\u7684\u67b6\u6784\u3002", "method": "\u91c7\u7528\u8154\u4ecb\u5bfc\u8026\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u5171\u4eab\u8154\u6a21\u5f0f\u548c\u53ef\u8c03\u8c10\u7684\u91cf\u5b50\u6bd4\u7279-\u8154\u8026\u5408\u5668\uff0c\u5b9e\u73b0\u52a8\u6001\u53ef\u91cd\u6784\u7684\u91cf\u5b50\u6bd4\u7279\u76f8\u4e92\u4f5c\u7528\u3002\u901a\u8fc7\u9009\u62e9\u6027\u6fc0\u6d3b\u8026\u5408\u5668\uff0c\u53ef\u4ee5\u5728\u975e\u76f8\u90bb\u91cf\u5b50\u6bd4\u7279\u4e4b\u95f4\u6267\u884c\u9ad8\u4fdd\u771f\u5ea6\u91cf\u5b50\u95e8\u64cd\u4f5c\u3002", "result": "\u572850\u7eb3\u79d2\u5185\u5b9e\u73b0\u4e86\u4fdd\u771f\u5ea6\u9ad8\u4e8e10^-4\u7684iSWAP\u548cCZ\u95e8\uff0c\u7a7a\u95f2\u65f6\u7684\u6b8b\u4f59ZZ\u76f8\u4e92\u4f5c\u7528\u4f4e\u4e8e\u51e0\u5343\u8d6b\u5179\u3002\u5728\u56db\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6fc0\u6d3b\u8026\u5408\u5668\u5b9e\u73b0\u4e86\u6240\u6709\u91cf\u5b50\u6bd4\u7279\u5bf9\u4e4b\u95f4\u7684\u95e8\u64cd\u4f5c\uff0c\u4e14\u91cf\u5b50\u6bd4\u7279\u4e32\u6270\u8f83\u4f4e\u3002", "conclusion": "\u8be5\u8154\u4ecb\u5bfc\u8026\u5408\u67b6\u6784\u4e3a\u8d85\u5bfc\u91cf\u5b50\u5904\u7406\u5668\u63d0\u4f9b\u4e86\u589e\u5f3a\u7684\u76f8\u4e92\u4f5c\u7528\u7075\u6d3b\u6027\uff0c\u53ef\u4f5c\u4e3a\u9009\u62e9\u6027\u975e\u5c40\u57df\u8026\u5408\u5668\u4ef6\u7684\u5b9e\u7528\u6784\u5efa\u6a21\u5757\uff0c\u6709\u671b\u63a8\u52a8\u66f4\u590d\u6742\u91cf\u5b50\u7535\u8def\u7684\u5b9e\u73b0\u3002"}}
{"id": "2602.07453", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07453", "abs": "https://arxiv.org/abs/2602.07453", "authors": ["Namrita Varshney", "Ashutosh Gupta", "Arhaan Ahmad", "Tanay V. Tayal", "S. Akshay"], "title": "Data-Aware and Scalable Sensitivity Analysis for Decision Tree Ensembles", "comment": null, "summary": "Decision tree ensembles are widely used in critical domains, making robustness and sensitivity analysis essential to their trustworthiness. We study the feature sensitivity problem, which asks whether an ensemble is sensitive to a specified subset of features -- such as protected attributes -- whose manipulation can alter model predictions. Existing approaches often yield examples of sensitivity that lie far from the training distribution, limiting their interpretability and practical value. We propose a data-aware sensitivity framework that constrains the sensitive examples to remain close to the dataset, thereby producing realistic and interpretable evidence of model weaknesses. To this end, we develop novel techniques for data-aware search using a combination of mixed-integer linear programming (MILP) and satisfiability modulo theories (SMT) encodings. Our contributions are fourfold. First, we strengthen the NP-hardness result for sensitivity verification, showing it holds even for trees of depth 1. Second, we develop MILP-optimizations that significantly speed up sensitivity verification for single ensembles and for the first time can also handle multiclass tree ensembles. Third, we introduce a data-aware framework generating realistic examples close to the training distribution. Finally, we conduct an extensive experimental evaluation on large tree ensembles, demonstrating scalability to ensembles with up to 800 trees of depth 8, achieving substantial improvements over the state of the art. This framework provides a practical foundation for analyzing the reliability and fairness of tree-based models in high-stakes applications.", "AI": {"tldr": "\u63d0\u51fa\u6570\u636e\u611f\u77e5\u7684\u6811\u96c6\u6210\u6a21\u578b\u7279\u5f81\u654f\u611f\u6027\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7MILP\u548cSMT\u7f16\u7801\u751f\u6210\u63a5\u8fd1\u8bad\u7ec3\u5206\u5e03\u7684\u654f\u611f\u793a\u4f8b\uff0c\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u51b3\u7b56\u6811\u96c6\u6210\u6a21\u578b\u5728\u5173\u952e\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u9c81\u68d2\u6027\u548c\u654f\u611f\u6027\u5206\u6790\u6765\u786e\u4fdd\u53ef\u4fe1\u5ea6\u3002\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u7684\u654f\u611f\u793a\u4f8b\u5f80\u5f80\u8fdc\u79bb\u8bad\u7ec3\u5206\u5e03\uff0c\u9650\u5236\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u5b9e\u7528\u4ef7\u503c\u3002", "method": "\u63d0\u51fa\u6570\u636e\u611f\u77e5\u654f\u611f\u6027\u6846\u67b6\uff0c\u5c06\u654f\u611f\u793a\u4f8b\u7ea6\u675f\u5728\u63a5\u8fd1\u6570\u636e\u96c6\u7684\u4f4d\u7f6e\uff1b\u5f00\u53d1\u57fa\u4e8eMILP\u548cSMT\u7f16\u7801\u7684\u6570\u636e\u611f\u77e5\u641c\u7d22\u6280\u672f\uff1b\u4f18\u5316MILP\u7b97\u6cd5\u52a0\u901f\u654f\u611f\u6027\u9a8c\u8bc1\u3002", "result": "\u8bc1\u660e\u4e86\u654f\u611f\u6027\u9a8c\u8bc1\u7684NP-hard\u6027\u5373\u4f7f\u5728\u6df1\u5ea6\u4e3a1\u7684\u6811\u4e2d\u4e5f\u6210\u7acb\uff1b\u5b9e\u73b0\u4e86\u5bf9\u591a\u8fbe800\u68f5\u6df1\u5ea68\u7684\u6811\u96c6\u6210\u6a21\u578b\u7684\u53ef\u6269\u5c55\u5206\u6790\uff1b\u5728\u5927\u578b\u6811\u96c6\u6210\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u57fa\u4e8e\u6811\u7684\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u516c\u5e73\u6027\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\uff0c\u80fd\u751f\u6210\u66f4\u73b0\u5b9e\u3001\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u5f31\u70b9\u8bc1\u636e\u3002"}}
{"id": "2602.08880", "categories": ["quant-ph", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08880", "abs": "https://arxiv.org/abs/2602.08880", "authors": ["Antonin Sulc"], "title": "Differentiable Logical Programming for Quantum Circuit Discovery and Optimization", "comment": null, "summary": "Designing high-fidelity quantum circuits remains challenging, and current paradigms often depend on heuristic, fixed-ansatz structures or rule-based compilers that can be suboptimal or lack generality. We introduce a neuro-symbolic framework that reframes quantum circuit design as a differentiable logic programming problem. Our model represents a scaffold of potential quantum gates and parameterized operations as a set of learnable, continuous ``truth values'' or ``switches,'' $s \\in [0, 1]^N$. These switches are optimized via standard gradient descent to satisfy a user-defined set of differentiable, logical axioms (e.g., correctness, simplicity, robustness). We provide a theoretical formulation bridging continuous logic (via T-norms) and unitary evolution (via geodesic interpolation), while addressing the barren plateau problem through biased initialization. We illustrate the approach on tasks including discovery of a 4-qubit Quantum Fourier Transform (QFT) from a scaffold of 21 candidate gates. We also report a hardware-aware adaptation experiment on the 133-qubit IBM Torino processor, where the method improved fidelity by 59.3 percentage points in a localized routing task while adapting to hardware failures.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u5c06\u91cf\u5b50\u7535\u8def\u8bbe\u8ba1\u91cd\u6784\u4e3a\u53ef\u5fae\u903b\u8f91\u7f16\u7a0b\u95ee\u9898\uff0c\u901a\u8fc7\u8fde\u7eed\u5f00\u5173\u4f18\u5316\u6ee1\u8db3\u903b\u8f91\u516c\u7406\uff0c\u5728QFT\u53d1\u73b0\u548c\u786c\u4ef6\u9002\u914d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u91cf\u5b50\u7535\u8def\u8bbe\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u56fa\u5b9a\u7ed3\u6784\u6216\u57fa\u4e8e\u89c4\u5219\u7684\u7f16\u8bd1\u5668\uff0c\u5f80\u5f80\u4e0d\u591f\u4f18\u5316\u6216\u7f3a\u4e4f\u901a\u7528\u6027\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u3001\u53ef\u4f18\u5316\u7684\u8bbe\u8ba1\u6846\u67b6\u3002", "method": "\u5c06\u91cf\u5b50\u95e8\u548c\u53c2\u6570\u5316\u64cd\u4f5c\u8868\u793a\u4e3a\u53ef\u5b66\u4e60\u7684\u8fde\u7eed\"\u5f00\u5173\"\u503c\uff0c\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u6ee1\u8db3\u53ef\u5fae\u903b\u8f91\u516c\u7406\uff0c\u7ed3\u5408\u8fde\u7eed\u903b\u8f91\uff08T-\u8303\u6570\uff09\u548c\u5e7a\u6b63\u6f14\u5316\uff08\u6d4b\u5730\u7ebf\u63d2\u503c\uff09\uff0c\u91c7\u7528\u504f\u7f6e\u521d\u59cb\u5316\u7f13\u89e3\u8d2b\u7620\u9ad8\u539f\u95ee\u9898\u3002", "result": "\u6210\u529f\u4ece21\u4e2a\u5019\u9009\u95e8\u4e2d\u53d1\u73b0\u4e864-qubit\u91cf\u5b50\u5085\u91cc\u53f6\u53d8\u6362\u7535\u8def\uff1b\u5728IBM Torino 133-qubit\u5904\u7406\u5668\u4e0a\uff0c\u786c\u4ef6\u611f\u77e5\u9002\u914d\u5c06\u4fdd\u771f\u5ea6\u63d0\u9ad8\u4e8659.3\u4e2a\u767e\u5206\u70b9\uff0c\u5e76\u80fd\u9002\u5e94\u786c\u4ef6\u6545\u969c\u3002", "conclusion": "\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\u4e3a\u91cf\u5b50\u7535\u8def\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7cfb\u7edf\u3001\u53ef\u5fae\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u53d1\u73b0\u9ad8\u6548\u7535\u8def\u5e76\u9002\u5e94\u5b9e\u9645\u786c\u4ef6\u7ea6\u675f\uff0c\u5c55\u793a\u4e86\u5728\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.07465", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07465", "abs": "https://arxiv.org/abs/2602.07465", "authors": ["Seungwoo Son", "Ingyu Seong", "Junhan Kim", "Hyemi Jang", "Yongkweon Jeon"], "title": "On the Importance of a Multi-Scale Calibration for Quantization", "comment": "ICASSP 2026", "summary": "Post-training quantization (PTQ) is a cornerstone for efficiently deploying large language models (LLMs), where a small calibration set critically affects quantization performance. However, conventional practices rely on random sequences of fixed length, overlooking the variable-length nature of LLM inputs. Input length directly influences the activation distribution and, consequently, the weight importance captured by the Hessian, which in turn affects quantization outcomes. As a result, Hessian estimates derived from fixed-length calibration may fail to represent the true importance of weights across diverse input scenarios. We propose MaCa (Matryoshka Calibration), a simple yet effective method for length-aware Hessian construction. MaCa (i) incorporates multi-scale sequence length information into Hessian estimation and (ii) regularizes each sequence as an independent sample, yielding a more stable and fruitful Hessian for accurate quantization. Experiments on state-of-the-art LLMs (e.g., Qwen3, Gemma3, LLaMA3) demonstrate that MaCa consistently improves accuracy under low bit quantization, offering a lightweight enhancement compatible with existing PTQ frameworks. To the best of our knowledge, this is the first work to systematically highlight the role of multi-scale calibration in LLM quantization.", "AI": {"tldr": "MaCa\u63d0\u51fa\u4e86\u4e00\u79cd\u957f\u5ea6\u611f\u77e5\u7684Hessian\u6784\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u5e8f\u5217\u957f\u5ea6\u4fe1\u606f\u6539\u8fdbLLM\u540e\u8bad\u7ec3\u91cf\u5316\u6548\u679c", "motivation": "\u4f20\u7edfPTQ\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u957f\u5ea6\u7684\u968f\u673a\u5e8f\u5217\u4f5c\u4e3a\u6821\u51c6\u96c6\uff0c\u5ffd\u7565\u4e86LLM\u8f93\u5165\u7684\u53ef\u53d8\u957f\u5ea6\u7279\u6027\u3002\u8f93\u5165\u957f\u5ea6\u76f4\u63a5\u5f71\u54cd\u6fc0\u6d3b\u5206\u5e03\u548cHessian\u6355\u83b7\u7684\u6743\u91cd\u91cd\u8981\u6027\uff0c\u56fa\u5b9a\u957f\u5ea6\u6821\u51c6\u5f97\u5230\u7684Hessian\u4f30\u8ba1\u53ef\u80fd\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u4e0d\u540c\u8f93\u5165\u573a\u666f\u4e0b\u7684\u771f\u5b9e\u6743\u91cd\u91cd\u8981\u6027\u3002", "method": "MaCa\u65b9\u6cd5\uff1a(i) \u5c06\u591a\u5c3a\u5ea6\u5e8f\u5217\u957f\u5ea6\u4fe1\u606f\u878d\u5165Hessian\u4f30\u8ba1\uff1b(ii) \u5c06\u6bcf\u4e2a\u5e8f\u5217\u4f5c\u4e3a\u72ec\u7acb\u6837\u672c\u8fdb\u884c\u6b63\u5219\u5316\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u7a33\u5b9a\u3001\u66f4\u6709\u6548\u7684Hessian\u7528\u4e8e\u7cbe\u786e\u91cf\u5316\u3002", "result": "\u5728Qwen3\u3001Gemma3\u3001LLaMA3\u7b49\u5148\u8fdbLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMaCa\u5728\u4f4e\u6bd4\u7279\u91cf\u5316\u4e0b\u80fd\u6301\u7eed\u63d0\u5347\u7cbe\u5ea6\uff0c\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u589e\u5f3a\u4e14\u517c\u5bb9\u73b0\u6709PTQ\u6846\u67b6\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u7cfb\u7edf\u6027\u5730\u5f3a\u8c03\u591a\u5c3a\u5ea6\u6821\u51c6\u5728LLM\u91cf\u5316\u4e2d\u4f5c\u7528\u7684\u5de5\u4f5c\uff0cMaCa\u901a\u8fc7\u957f\u5ea6\u611f\u77e5\u7684Hessian\u6784\u5efa\u663e\u8457\u6539\u5584\u4e86\u91cf\u5316\u6027\u80fd\u3002"}}
{"id": "2602.08883", "categories": ["quant-ph", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2602.08883", "abs": "https://arxiv.org/abs/2602.08883", "authors": ["Mohamed Sabba", "Christian Bengs", "Urvashi D. Heramun", "Malcolm H. Levitt"], "title": "Error compensation without a time penalty: robust spin-lock-induced crossing in solution NMR", "comment": null, "summary": "A modification of the widely-used spin-lock-induced crossing (SLIC) procedure is proposed for the solution nuclear magnetic resonance (NMR) of strongly coupled nuclear spin systems, including singlet NMR and parahydrogen-enhanced hyperpolarised NMR experiments. The compensated-SLIC (cSLIC) scheme uses a repetitive sequence where the repeated element employs two different radiofrequency field amplitudes. Effective compensation for deviations in the radiofrequency field amplitude is achieved without increasing the overall duration of the SLIC sequence. The advantageous properties of cSLIC are demonstrated by numerical simulations and by representative experiments.", "AI": {"tldr": "\u63d0\u51fa\u8865\u507f\u578bSLIC\uff08cSLIC\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5f3a\u8026\u5408\u6838\u81ea\u65cb\u7cfb\u7edf\u7684\u6eb6\u6db2NMR\uff0c\u901a\u8fc7\u4f7f\u7528\u4e24\u79cd\u4e0d\u540c\u5c04\u9891\u573a\u5e45\u5ea6\u7684\u91cd\u590d\u5e8f\u5217\u5143\u7d20\uff0c\u5728\u4e0d\u589e\u52a0\u603b\u65f6\u957f\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u8865\u507f\u5c04\u9891\u573a\u5e45\u5ea6\u504f\u5dee\u3002", "motivation": "\u9488\u5bf9\u5f3a\u8026\u5408\u6838\u81ea\u65cb\u7cfb\u7edf\uff08\u5305\u62ec\u5355\u91cd\u6001NMR\u548c\u4ef2\u6c22\u589e\u5f3a\u8d85\u6781\u5316NMR\u5b9e\u9a8c\uff09\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684SLIC\u65b9\u6cd5\uff0c\u9700\u8981\u6539\u8fdb\u5176\u5bf9\u5c04\u9891\u573a\u5e45\u5ea6\u504f\u5dee\u7684\u8865\u507f\u80fd\u529b\uff0c\u540c\u65f6\u4e0d\u589e\u52a0\u5b9e\u9a8c\u603b\u65f6\u957f\u3002", "method": "\u63d0\u51fa\u8865\u507f\u578bSLIC\uff08cSLIC\uff09\u65b9\u6848\uff0c\u91c7\u7528\u91cd\u590d\u5e8f\u5217\u7ed3\u6784\uff0c\u5176\u4e2d\u91cd\u590d\u5143\u7d20\u4f7f\u7528\u4e24\u79cd\u4e0d\u540c\u7684\u5c04\u9891\u573a\u5e45\u5ea6\uff0c\u901a\u8fc7\u8fd9\u79cd\u8bbe\u8ba1\u6709\u6548\u8865\u507f\u5c04\u9891\u573a\u5e45\u5ea6\u504f\u5dee\u3002", "result": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\u548c\u4ee3\u8868\u6027\u5b9e\u9a8c\u9a8c\u8bc1\u4e86cSLIC\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u8bc1\u660e\u5176\u80fd\u6709\u6548\u8865\u507f\u5c04\u9891\u573a\u504f\u5dee\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u539f\u59cbSLIC\u76f8\u540c\u7684\u603b\u5b9e\u9a8c\u65f6\u957f\u3002", "conclusion": "cSLIC\u65b9\u6cd5\u4e3a\u5f3a\u8026\u5408\u6838\u81ea\u65cb\u7cfb\u7edf\u7684NMR\u5b9e\u9a8c\u63d0\u4f9b\u4e86\u6539\u8fdb\u7684\u5c04\u9891\u573a\u8865\u507f\u80fd\u529b\uff0c\u5728\u5355\u91cd\u6001NMR\u548c\u4ef2\u6c22\u589e\u5f3a\u8d85\u6781\u5316NMR\u7b49\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2602.07472", "categories": ["cs.LG", "math.OC", "math.PR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07472", "abs": "https://arxiv.org/abs/2602.07472", "authors": ["Yilun Chen", "Jiaqi Lu"], "title": "Bandit Allocational Instability", "comment": null, "summary": "When multi-armed bandit (MAB) algorithms allocate pulls among competing arms, the resulting allocation can exhibit huge variation. This is particularly harmful in modern applications such as learning-enhanced platform operations and post-bandit statistical inference. Thus motivated, we introduce a new performance metric of MAB algorithms termed allocation variability, which is the largest (over arms) standard deviation of an arm's number of pulls. We establish a fundamental trade-off between allocation variability and regret, the canonical performance metric of reward maximization. In particular, for any algorithm, the worst-case regret $R_T$ and worst-case allocation variability $S_T$ must satisfy $R_T \\cdot S_T=\u03a9(T^{\\frac{3}{2}})$ as $T\\rightarrow\\infty$, as long as $R_T=o(T)$. This indicates that any minimax regret-optimal algorithm must incur worst-case allocation variability $\u0398(T)$, the largest possible scale; while any algorithm with sublinear worst-case regret must necessarily incur ${S}_T= \u03c9(\\sqrt{T})$. We further show that this lower bound is essentially tight, and that any point on the Pareto frontier $R_T \\cdot S_T=\\tilde\u0398(T^{3/2})$ can be achieved by a simple tunable algorithm UCB-f, a generalization of the classic UCB1. Finally, we discuss implications for platform operations and for statistical inference, when bandit algorithms are used. As a byproduct of our result, we resolve an open question of Praharaj and Khamaru (2025).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u81c2\u8001\u864e\u673a\u7b97\u6cd5\u7684\u65b0\u6027\u80fd\u6307\u6807\"\u5206\u914d\u53d8\u5f02\u6027\"\uff0c\u5efa\u7acb\u4e86\u5206\u914d\u53d8\u5f02\u6027\u4e0e\u9057\u61be\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u5173\u7cfb\uff0c\u5e76\u8bbe\u8ba1\u4e86\u53ef\u8c03\u7b97\u6cd5UCB-f\u6765\u5b9e\u73b0\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "motivation": "\u4f20\u7edf\u591a\u81c2\u8001\u864e\u673a\u7b97\u6cd5\u5728\u5206\u914d\u62c9\u52a8\u6b21\u6570\u65f6\u5b58\u5728\u5de8\u5927\u53d8\u5f02\u6027\uff0c\u8fd9\u5bf9\u73b0\u4ee3\u5e94\u7528\u5982\u5b66\u4e60\u589e\u5f3a\u5e73\u53f0\u8fd0\u8425\u548c\u540e\u8001\u864e\u673a\u7edf\u8ba1\u63a8\u65ad\u9020\u6210\u5371\u5bb3\uff0c\u56e0\u6b64\u9700\u8981\u5f15\u5165\u65b0\u7684\u6027\u80fd\u6307\u6807\u6765\u91cf\u5316\u8fd9\u79cd\u53d8\u5f02\u6027\u3002", "method": "\u5f15\u5165\u5206\u914d\u53d8\u5f02\u6027\u4f5c\u4e3a\u65b0\u6027\u80fd\u6307\u6807\uff0c\u5b9a\u4e49\u4e3a\u5404\u81c2\u62c9\u52a8\u6b21\u6570\u6807\u51c6\u504f\u5dee\u7684\u6700\u5927\u503c\u3002\u5efa\u7acb\u5206\u914d\u53d8\u5f02\u6027\u4e0e\u9057\u61be\u4e4b\u95f4\u7684\u7406\u8bba\u4e0b\u754c\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u53ef\u8c03\u7b97\u6cd5UCB-f\uff08\u7ecf\u5178UCB1\u7684\u63a8\u5e7f\uff09\u6765\u5b9e\u73b0\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "result": "\u8bc1\u660e\u4e86\u4efb\u4f55\u7b97\u6cd5\u7684\u9057\u61be$R_T$\u4e0e\u5206\u914d\u53d8\u5f02\u6027$S_T$\u5fc5\u987b\u6ee1\u8db3$R_T \\cdot S_T=\u03a9(T^{\\frac{3}{2}})$\uff0c\u8868\u660e\u6700\u5c0f\u6700\u5927\u9057\u61be\u6700\u4f18\u7b97\u6cd5\u5fc5\u7136\u4ea7\u751f\u6700\u5927\u5c3a\u5ea6\u7684\u5206\u914d\u53d8\u5f02\u6027\u3002UCB-f\u7b97\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u5e15\u7d2f\u6258\u524d\u6cbf$R_T \\cdot S_T=\\tilde\u0398(T^{3/2})$\u7684\u4efb\u4f55\u70b9\u3002", "conclusion": "\u5206\u914d\u53d8\u5f02\u6027\u4e0e\u9057\u61be\u4e4b\u95f4\u5b58\u5728\u57fa\u672c\u6743\u8861\u5173\u7cfb\uff0c\u8fd9\u4e00\u53d1\u73b0\u5bf9\u5e73\u53f0\u8fd0\u8425\u548c\u7edf\u8ba1\u63a8\u65ad\u5177\u6709\u91cd\u8981\u542f\u793a\u3002\u540c\u65f6\u89e3\u51b3\u4e86Praharaj\u548cKhamaru\uff082025\uff09\u7684\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002"}}
{"id": "2602.08890", "categories": ["quant-ph", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2602.08890", "abs": "https://arxiv.org/abs/2602.08890", "authors": ["Angelo Greco", "Jukka-Pekka Kaikkonen", "Luca Chirolli", "Alberto Ronzani", "Jorden Senior", "Francesco Giazotto", "Alessandro Crippa"], "title": "Multiplexed microwave resonators by frequency comb spectroscopy", "comment": "4 main figures and 5 others in the appendices", "summary": "Coplanar waveguide resonators are central to the thriving field of circuit quantum electrodynamics. Recently, we have demonstrated the generation of a broadband microwave-frequency comb spectrum using a superconducting quantum interference device (SQUID) driven by a time-dependent magnetic field. Here, the frequency comb is used to spectroscopically probe a bank of coplanar microwave resonators, inductively coupled to a common transmission line, a standard circuit with a variety of applications. We compare the resonator line shape obtained from signals synthesized at room temperature using conventional electronics with the radiation produced in the cryogenic environment by our source, showing substantial equivalence in the estimation of the resonator quality factors. To measure non-uniformly spaced resonant frequencies, we drive the generator with a bi-chromatic tone to generate intermodulation products. Such a dense frequency comb spectrum enables simultaneous addressing of a few resonators via frequency multiplexing. Finally, we discuss the criteria for achieving effective spectroscopic coverage of a given frequency bandwidth.", "AI": {"tldr": "\u5229\u7528SQUID\u4ea7\u751f\u7684\u5fae\u6ce2\u9891\u7387\u68b3\u540c\u65f6\u63a2\u6d4b\u591a\u4e2a\u5171\u9762\u6ce2\u5bfc\u8c10\u632f\u5668\uff0c\u5b9e\u73b0\u9891\u7387\u590d\u7528\u5149\u8c31\u6d4b\u91cf", "motivation": "\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u63a2\u6d4b\u591a\u4e2a\u975e\u5747\u5300\u95f4\u9694\u8c10\u632f\u9891\u7387\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u7535\u8def\u91cf\u5b50\u7535\u52a8\u529b\u5b66\u4e2d\u7684\u5171\u9762\u6ce2\u5bfc\u8c10\u632f\u5668\u8868\u5f81", "method": "\u4f7f\u7528\u65f6\u95f4\u4f9d\u8d56\u78c1\u573a\u9a71\u52a8\u7684SQUID\u4ea7\u751f\u5bbd\u5e26\u5fae\u6ce2\u9891\u7387\u68b3\uff0c\u901a\u8fc7\u53cc\u8272\u9a71\u52a8\u4ea7\u751f\u4e92\u8c03\u4ea7\u7269\uff0c\u5bf9\u8026\u5408\u5230\u516c\u5171\u4f20\u8f93\u7ebf\u7684\u8c10\u632f\u5668\u8fdb\u884c\u5149\u8c31\u63a2\u6d4b", "result": "\u9891\u7387\u68b3\u65b9\u6cd5\u4e0e\u5ba4\u6e29\u7535\u5b50\u5408\u6210\u4fe1\u53f7\u5728\u8c10\u632f\u5668\u54c1\u8d28\u56e0\u6570\u4f30\u8ba1\u4e0a\u57fa\u672c\u7b49\u6548\uff0c\u80fd\u591f\u540c\u65f6\u5bfb\u5740\u591a\u4e2a\u8c10\u632f\u5668\uff0c\u5b9e\u73b0\u9891\u7387\u590d\u7528", "conclusion": "SQUID\u4ea7\u751f\u7684\u9891\u7387\u68b3\u4e3a\u5171\u9762\u6ce2\u5bfc\u8c10\u632f\u5668\u7684\u5149\u8c31\u8868\u5f81\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u80fd\u591f\u5b9e\u73b0\u7ed9\u5b9a\u5e26\u5bbd\u5185\u7684\u6709\u6548\u5149\u8c31\u8986\u76d6"}}
{"id": "2602.07475", "categories": ["cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2602.07475", "abs": "https://arxiv.org/abs/2602.07475", "authors": ["Zhuomin Liang", "Liang Bai", "Xian Yang"], "title": "Bipartite Graph Attention-based Clustering for Large-scale scRNA-seq Data", "comment": null, "summary": "scRNA-seq clustering is a critical task for analyzing single-cell RNA sequencing (scRNA-seq) data, as it groups cells with similar gene expression profiles. Transformers, as powerful foundational models, have been applied to scRNA-seq clustering. Their self-attention mechanism automatically assigns higher attention weights to cells within the same cluster, enhancing the distinction between clusters. Existing methods for scRNA-seq clustering, such as graph transformer-based models, treat each cell as a token in a sequence. Their computational and space complexities are $\\mathcal{O}(n^2)$ with respect to the number of cells, limiting their applicability to large-scale scRNA-seq datasets.To address this challenge, we propose a Bipartite Graph Transformer-based clustering model (BGFormer) for scRNA-seq data. We introduce a set of learnable anchor tokens as shared reference points to represent the entire dataset. A bipartite graph attention mechanism is introduced to learn the similarity between cells and anchor tokens, bringing cells of the same class closer together in the embedding space. BGFormer achieves linear computational complexity with respect to the number of cells, making it scalable to large datasets. Experimental results on multiple large-scale scRNA-seq datasets demonstrate the effectiveness and scalability of BGFormer.", "AI": {"tldr": "BGFormer\uff1a\u57fa\u4e8e\u4e8c\u5206\u56feTransformer\u7684\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u805a\u7c7b\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u951a\u70b9\u6807\u8bb0\u5b9e\u73b0\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfTransformer\u65b9\u6cd5\u5728\u5927\u578b\u6570\u636e\u96c6\u4e0a\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eTransformer\u7684\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u805a\u7c7b\u65b9\u6cd5\uff08\u5982\u56feTransformer\uff09\u5c06\u6bcf\u4e2a\u7ec6\u80de\u89c6\u4e3a\u5e8f\u5217\u4e2d\u7684\u4e00\u4e2a\u6807\u8bb0\uff0c\u5176\u8ba1\u7b97\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\u4e3aO(n\u00b2)\uff0c\u9650\u5236\u4e86\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u805a\u7c7b\u65b9\u6cd5\u3002", "method": "\u63d0\u51faBGFormer\uff08\u4e8c\u5206\u56feTransformer\u805a\u7c7b\u6a21\u578b\uff09\uff1a1\uff09\u5f15\u5165\u4e00\u7ec4\u53ef\u5b66\u4e60\u7684\u951a\u70b9\u6807\u8bb0\u4f5c\u4e3a\u5171\u4eab\u53c2\u8003\u70b9\u6765\u8868\u793a\u6574\u4e2a\u6570\u636e\u96c6\uff1b2\uff09\u91c7\u7528\u4e8c\u5206\u56fe\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\u7ec6\u80de\u4e0e\u951a\u70b9\u6807\u8bb0\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff1b3\uff09\u5c06\u540c\u4e00\u7c7b\u522b\u7684\u7ec6\u80de\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u62c9\u8fd1\uff1b4\uff09\u5b9e\u73b0\u76f8\u5bf9\u4e8e\u7ec6\u80de\u6570\u91cf\u7684\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u5728\u591a\u4e2a\u5927\u89c4\u6a21\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cBGFormer\u5177\u6709\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u591f\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "conclusion": "BGFormer\u901a\u8fc7\u4e8c\u5206\u56fe\u6ce8\u610f\u529b\u673a\u5236\u548c\u951a\u70b9\u6807\u8bb0\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edfTransformer\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u805a\u7c7b\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u751f\u7269\u4fe1\u606f\u5b66\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08908", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.08908", "abs": "https://arxiv.org/abs/2602.08908", "authors": ["Mat\u00edas Rub\u00e9n Bola\u00f1os", "Edoardo Rossi", "Federico Berra", "Alberto De Toni", "Ilektra Karakosta-Amarantidou", "Daniel Christian Lawo", "Costantino Agnesi", "Marco Avesani", "Andrea Stanco", "Francesco Vedovato", "Paolo Villoresi", "Giuseppe Vallone"], "title": "GHz-rate polarization-based QKD system for fiber and satellite applications", "comment": "9 pages, 8 figures", "summary": "Quantum key distribution (QKD) leverages the principles of quantum mechanics to exchange a secret key between two parties. Despite its promising features, QKD also faces several practical challenges such as transmission loss, noise in quantum channels and finite key size effects. Addressing these issues is crucial for the large-scale deployment of QKD in fiber and satellite networks.\n  In this paper, we present a 1550 nm QKD system realizing the efficient-BB84 protocol and based on the iPOGNAC scheme. The system achieved repetition rates up to 1.5~GHz and showed an intrinsic QBER of $\\sim 0.4\\%$. The system was first tested on a laboratory fiber link and then on an intermodal link in the field, consisting of both deployed fiber and a 620 m free-space channel. The experiment was performed in daylight conditions, exploiting the Qubit4Sync synchronization protocol. With this trial, we achieved a new benchmark for free-space BB84 QKD systems by generating a sustained secret key rate (SKR) above 1~Mb/s for 1 hour. Finally, exploiting a recently discovered finite-size bound, we achieved a secure key rate of about 10 Mb/s at low losses (5 dB), and around 6.5~kb/s in the high-loss (38.5 dB), low block length ($N=10^4$) regime. The latter results demonstrate the system's suitability for highly lossy and time-constrained scenarios such as QKD from low Earth orbit satellites.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u4e00\u4e2a1550 nm QKD\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548BB84\u534f\u8bae\uff0c\u5728\u5b9e\u9a8c\u5ba4\u548c\u73b0\u573a\u6df7\u5408\u94fe\u8def\uff08\u5149\u7ea4+\u81ea\u7531\u7a7a\u95f4\uff09\u6d4b\u8bd5\u4e2d\uff0c\u521b\u9020\u4e86\u81ea\u7531\u7a7a\u95f4BB84 QKD\u7cfb\u7edf\u7684\u65b0\u57fa\u51c6\uff0c\u5728\u4f4e\u635f\u8017\u548c\u9ad8\u635f\u8017\u573a\u666f\u4e0b\u5747\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\uff08QKD\uff09\u867d\u7136\u5177\u6709\u91cf\u5b50\u529b\u5b66\u539f\u7406\u7684\u5b89\u5168\u4f18\u52bf\uff0c\u4f46\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u9762\u4e34\u4f20\u8f93\u635f\u8017\u3001\u91cf\u5b50\u4fe1\u9053\u566a\u58f0\u548c\u6709\u9650\u5bc6\u94a5\u5927\u5c0f\u6548\u5e94\u7b49\u6311\u6218\u3002\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5bf9\u4e8eQKD\u5728\u5149\u7ea4\u548c\u536b\u661f\u7f51\u7edc\u4e2d\u7684\u5927\u89c4\u6a21\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u75281550 nm QKD\u7cfb\u7edf\uff0c\u5b9e\u73b0\u9ad8\u6548BB84\u534f\u8bae\uff0c\u57fa\u4e8eiPOGNAC\u65b9\u6848\u3002\u7cfb\u7edf\u91cd\u590d\u7387\u9ad8\u8fbe1.5 GHz\uff0c\u56fa\u6709QBER\u7ea60.4%\u3002\u9996\u5148\u5728\u5b9e\u9a8c\u5ba4\u5149\u7ea4\u94fe\u8def\u4e0a\u6d4b\u8bd5\uff0c\u7136\u540e\u5728\u73b0\u573a\u6df7\u5408\u94fe\u8def\u4e0a\u6d4b\u8bd5\uff08\u5305\u62ec\u5df2\u90e8\u7f72\u5149\u7ea4\u548c620\u7c73\u81ea\u7531\u7a7a\u95f4\u4fe1\u9053\uff09\uff0c\u5728\u65e5\u5149\u6761\u4ef6\u4e0b\u4f7f\u7528Qubit4Sync\u540c\u6b65\u534f\u8bae\u3002", "result": "\u521b\u9020\u4e86\u81ea\u7531\u7a7a\u95f4BB84 QKD\u7cfb\u7edf\u7684\u65b0\u57fa\u51c6\uff1a\u6301\u7eed1\u5c0f\u65f6\u751f\u6210\u8d85\u8fc71 Mb/s\u7684\u79d8\u5bc6\u5bc6\u94a5\u7387\uff08SKR\uff09\u3002\u5229\u7528\u65b0\u53d1\u73b0\u7684\u6709\u9650\u5927\u5c0f\u8fb9\u754c\uff0c\u5728\u4f4e\u635f\u8017\uff085 dB\uff09\u4e0b\u5b9e\u73b0\u7ea610 Mb/s\u7684\u5b89\u5168\u5bc6\u94a5\u7387\uff0c\u5728\u9ad8\u635f\u8017\uff0838.5 dB\uff09\u3001\u4f4e\u5757\u957f\u5ea6\uff08N=10^4\uff09\u60c5\u51b5\u4e0b\u5b9e\u73b0\u7ea66.5 kb/s\u7684\u5bc6\u94a5\u7387\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u7279\u522b\u9002\u7528\u4e8e\u9ad8\u635f\u8017\u548c\u65f6\u95f4\u53d7\u9650\u7684\u573a\u666f\uff0c\u5982\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\u7684QKD\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u90e8\u7f72\u73af\u5883\u4e2d\u7684\u5f3a\u5927\u6027\u80fd\uff0c\u4e3aQKD\u5728\u536b\u661f\u901a\u4fe1\u7b49\u9886\u57df\u7684\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2602.07478", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07478", "abs": "https://arxiv.org/abs/2602.07478", "authors": ["Laxmi Pandey", "Ariel Meroz", "Ben Cheng", "Ankita Manekar", "Abhijit Mukherjee", "Meirav Cohen", "Adway Mitra"], "title": "AI-Driven Predictive Modelling for Groundwater Salinization in Israel", "comment": "60 pages, 9 figures in main text and 6 figures in appendix, 2 tables, 3 Appendix", "summary": "Increasing salinity and contamination of groundwater is a serious issue in many parts of the world, causing degradation of water resources. The aim of this work is to form a comprehensive understanding of groundwater salinization underlying causal factors and identify important meteorological, geological and anthropogenic drivers of salinity. We have integrated different datasets of potential covariates, to create a robust framework for machine learning based predictive models including Random Forest (RF), XGBoost, Neural network, Long Short-Term Memory (LSTM), convolution neural network (CNN) and linear regression (LR), of groundwater salinity. Additionally, Recursive Feature Elimination (RFE) followed by Global sensitivity analysis (GSA) and Explainable AI (XAI) based SHapley Additive exPlanations (SHAP) were used to estimate the importance scores and find insights into the drivers of salinization. We also did causality analysis via Double machine learning using various predictive models. From these analyses, key meteorological (Precipitation, Temperature), geological (Distance from river, Distance to saline body, TWI, Shoreline distance), and anthropogenic (Area of agriculture field, Treated Wastewater) covariates are identified to be influential drivers of groundwater salinity across Israel. XAI analysis also identified Treated Wastewater (TWW) as an essential anthropogenic driver of salinity, its significance being context-dependent but critical in vulnerable hydro-climatic environment. Our approach provides deeper insight into global salinization mechanisms at country scale, reducing AI model uncertainty and highlighting the need for tailored strategies to address salinity.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6574\u5408\u591a\u6e90\u6570\u636e\uff0c\u91c7\u7528\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08RF\u3001XGBoost\u3001NN\u3001LSTM\u3001CNN\u3001LR\uff09\u9884\u6d4b\u4ee5\u8272\u5217\u5730\u4e0b\u6c34\u76d0\u5ea6\uff0c\u7ed3\u5408\u7279\u5f81\u9009\u62e9\u3001\u654f\u611f\u6027\u5206\u6790\u548c\u53ef\u89e3\u91caAI\u8bc6\u522b\u5173\u952e\u9a71\u52a8\u56e0\u7d20\u3002", "motivation": "\u5168\u7403\u8bb8\u591a\u5730\u533a\u5730\u4e0b\u6c34\u76d0\u5ea6\u589e\u52a0\u548c\u6c61\u67d3\u95ee\u9898\u4e25\u91cd\uff0c\u5bfc\u81f4\u6c34\u8d44\u6e90\u9000\u5316\u3002\u9700\u8981\u5168\u9762\u7406\u89e3\u5730\u4e0b\u6c34\u76d0\u5316\u7684\u6f5c\u5728\u56e0\u679c\u56e0\u7d20\uff0c\u8bc6\u522b\u91cd\u8981\u7684\u6c14\u8c61\u3001\u5730\u8d28\u548c\u4eba\u4e3a\u9a71\u52a8\u56e0\u7d20\u3002", "method": "\u6574\u5408\u4e0d\u540c\u6f5c\u5728\u534f\u53d8\u91cf\u6570\u636e\u96c6\uff0c\u5efa\u7acb\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u6a21\u578b\u6846\u67b6\uff08\u5305\u62ecRF\u3001XGBoost\u3001NN\u3001LSTM\u3001CNN\u3001LR\uff09\u3002\u4f7f\u7528\u9012\u5f52\u7279\u5f81\u6d88\u9664\uff08RFE\uff09\u3001\u5168\u5c40\u654f\u611f\u6027\u5206\u6790\uff08GSA\uff09\u548c\u53ef\u89e3\u91caAI\uff08XAI\uff09\u7684SHAP\u65b9\u6cd5\u8bc4\u4f30\u7279\u5f81\u91cd\u8981\u6027\u3002\u901a\u8fc7\u53cc\u673a\u5668\u5b66\u4e60\u8fdb\u884c\u56e0\u679c\u5206\u6790\u3002", "result": "\u8bc6\u522b\u51fa\u4ee5\u8272\u5217\u5730\u4e0b\u6c34\u76d0\u5ea6\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff1a\u6c14\u8c61\u56e0\u7d20\uff08\u964d\u6c34\u3001\u6e29\u5ea6\uff09\u3001\u5730\u8d28\u56e0\u7d20\uff08\u8ddd\u6cb3\u6d41\u8ddd\u79bb\u3001\u8ddd\u76d0\u4f53\u8ddd\u79bb\u3001\u5730\u5f62\u6e7f\u5ea6\u6307\u6570\u3001\u6d77\u5cb8\u7ebf\u8ddd\u79bb\uff09\u548c\u4eba\u4e3a\u56e0\u7d20\uff08\u519c\u7530\u9762\u79ef\u3001\u5904\u7406\u5e9f\u6c34\uff09\u3002XAI\u5206\u6790\u7279\u522b\u6307\u51fa\u5904\u7406\u5e9f\u6c34\u662f\u91cd\u8981\u7684\u4eba\u4e3a\u9a71\u52a8\u56e0\u7d20\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u56fd\u5bb6\u5c3a\u5ea6\u5168\u7403\u76d0\u5316\u673a\u5236\u63d0\u4f9b\u4e86\u6df1\u5165\u89c1\u89e3\uff0c\u51cf\u5c11\u4e86AI\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u9488\u5bf9\u6027\u7684\u7b56\u7565\u6765\u89e3\u51b3\u76d0\u5ea6\u95ee\u9898\u3002\u5904\u7406\u5e9f\u6c34\u5728\u8106\u5f31\u6c34\u6587\u6c14\u5019\u73af\u5883\u4e2d\u662f\u5173\u952e\u9a71\u52a8\u56e0\u7d20\u3002"}}
{"id": "2602.08947", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.08947", "abs": "https://arxiv.org/abs/2602.08947", "authors": ["Sujai Matta", "Soumya Asokan", "Sanchari Chakraborti", "Mayank Joshi", "Rahul Dalal", "C. M. Chandrashekar"], "title": "Long distance quantum illumination and ranging using polarization entangled photon pairs in a lossy environment", "comment": "9 pages, 7 figures, 38 references", "summary": "Using polarization entangled photon pairs, we demonstrate a robust scheme for quantum illumination and ranging in a lossy environment. Entangled photon pairs are generated in a Sagnac interferometer configuration, yielding high-visibility two-photon polarization entanglement with a measured CHSH parameter of $S =2.802\\pm0.002$. One of the photons from the entangled pair is retained as idler and the other one is directed into either of the two paths, namely reference and probe, of which probe is sent toward a distant object through a lossy free-space channel, and the reflected photons are collected after round-trip free-space propagation over distances approaching $1$ km. Remarkably, strong correlations are observed with CHSH values $S >2.6$ even when only a few tens of probe photons are returned, confirming the robustness of polarization entanglement under long-distance free-space propagation. This work reports the robustness of encoding photons in different basis before it is sent towards the object and recovery of polarization entanglement even after a kilometer-scale scattering from the objects, establishing a practical foundation for scalable quantum-assisted object detection and ranging.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u504f\u632f\u7ea0\u7f20\u5149\u5b50\u5bf9\uff0c\u5728\u635f\u8017\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u91cf\u5b50\u7167\u660e\u548c\u6d4b\u8ddd\u65b9\u6848\uff0c\u5728\u8fd11\u516c\u91cc\u81ea\u7531\u7a7a\u95f4\u4f20\u64ad\u540e\u4ecd\u80fd\u4fdd\u6301\u5f3a\u91cf\u5b50\u5173\u8054\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u635f\u8017\u73af\u5883\u4e2d\u5de5\u4f5c\u7684\u9c81\u68d2\u91cf\u5b50\u7167\u660e\u548c\u6d4b\u8ddd\u65b9\u6848\uff0c\u9a8c\u8bc1\u504f\u632f\u7ea0\u7f20\u5728\u957f\u8ddd\u79bb\u81ea\u7531\u7a7a\u95f4\u4f20\u64ad\u540e\u7684\u7a33\u5b9a\u6027\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u91cf\u5b50\u8f85\u52a9\u7269\u4f53\u68c0\u6d4b\u548c\u6d4b\u8ddd\u5efa\u7acb\u5b9e\u8df5\u57fa\u7840\u3002", "method": "\u4f7f\u7528Sagnac\u5e72\u6d89\u4eea\u914d\u7f6e\u751f\u6210\u504f\u632f\u7ea0\u7f20\u5149\u5b50\u5bf9\uff08CHSH\u53c2\u6570S=2.802\u00b10.002\uff09\uff0c\u5c06\u4e00\u4e2a\u5149\u5b50\u4f5c\u4e3a\u95f2\u7f6e\u5149\u5b50\u4fdd\u7559\uff0c\u53e6\u4e00\u4e2a\u5149\u5b50\u901a\u8fc7\u53c2\u8003\u8def\u5f84\u6216\u63a2\u6d4b\u8def\u5f84\u53d1\u9001\u3002\u63a2\u6d4b\u5149\u5b50\u901a\u8fc7\u635f\u8017\u81ea\u7531\u7a7a\u95f4\u901a\u9053\u53d1\u9001\u5230\u8fdc\u5904\u7269\u4f53\uff0c\u7ecf\u8fc7\u8fd11\u516c\u91cc\u5f80\u8fd4\u4f20\u64ad\u540e\u6536\u96c6\u53cd\u5c04\u5149\u5b50\u3002", "result": "\u5373\u4f7f\u5728\u4ec5\u8fd4\u56de\u51e0\u5341\u4e2a\u63a2\u6d4b\u5149\u5b50\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u80fd\u89c2\u5bdf\u5230\u5f3a\u91cf\u5b50\u5173\u8054\uff08CHSH\u503cS>2.6\uff09\uff0c\u8bc1\u5b9e\u4e86\u504f\u632f\u7ea0\u7f20\u5728\u957f\u8ddd\u79bb\u81ea\u7531\u7a7a\u95f4\u4f20\u64ad\u540e\u7684\u9c81\u68d2\u6027\u3002\u6210\u529f\u5b9e\u73b0\u4e86\u5343\u7c73\u5c3a\u5ea6\u6563\u5c04\u540e\u504f\u632f\u7ea0\u7f20\u7684\u6062\u590d\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u8bc1\u660e\u4e86\u5728\u4e0d\u540c\u57fa\u4e2d\u7f16\u7801\u5149\u5b50\u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u53ca\u5728\u5343\u7c73\u5c3a\u5ea6\u7269\u4f53\u6563\u5c04\u540e\u6062\u590d\u504f\u632f\u7ea0\u7f20\u7684\u53ef\u80fd\u6027\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u91cf\u5b50\u8f85\u52a9\u7269\u4f53\u68c0\u6d4b\u548c\u6d4b\u8ddd\u5efa\u7acb\u4e86\u5b9e\u7528\u57fa\u7840\u3002"}}
{"id": "2602.07479", "categories": ["cs.LG", "cs.IT", "eess.SP", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.07479", "abs": "https://arxiv.org/abs/2602.07479", "authors": ["Yihang Gao", "Vincent Y. F. Tan"], "title": "ODELoRA: Training Low-Rank Adaptation by Solving Ordinary Differential Equations", "comment": "38 pages", "summary": "Low-rank adaptation (LoRA) has emerged as a widely adopted parameter-efficient fine-tuning method in deep transfer learning, due to its reduced number of trainable parameters and lower memory requirements enabled by Burer-Monteiro factorization on adaptation matrices. However, classical LoRA training methods treat the low-rank factor matrices individually and optimize them using standard gradient-based algorithms. Such decoupled optimization schemes are theoretically and empirically suboptimal, as they fail to fully exploit the intrinsic structure of the LoRA parameterization. In this work, we propose a novel continuous-time optimization dynamic for LoRA factor matrices in the form of an ordinary differential equation (ODE) that emulates the gradient flow of full fine-tuning on the balanced manifold. We term this approach ODELoRA. To faithfully track the trajectories of ODELoRA, we adopt well-established and theoretically grounded time-discretization schemes, including Euler and Runge--Kutta methods. Our framework provides a unified ODE-based perspective for understanding and designing LoRA training algorithms. We establish linear convergence of the proposed method under strongly convex objectives for certain discretization schemes under mild conditions, and further extend our analysis to the matrix sensing setting. Moreover, we show that ODELoRA achieves stable feature learning, a property that is crucial for training deep neural networks at different scales of problem dimensionality. Empirical results on matrix sensing tasks confirm the derived linear convergence behavior, and experiments on training physics-informed neural networks further demonstrate the superiority of ODELoRA over existing baselines, especially in the training stability.", "AI": {"tldr": "\u63d0\u51faODELoRA\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e38\u5fae\u5206\u65b9\u7a0b\u4f18\u5316LoRA\u56e0\u5b50\u77e9\u9635\uff0c\u6a21\u62df\u5b8c\u6574\u5fae\u8c03\u5728\u5e73\u8861\u6d41\u5f62\u4e0a\u7684\u68af\u5ea6\u6d41\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfLoRA\u8bad\u7ec3\u65b9\u6cd5\u5c06\u4f4e\u79e9\u56e0\u5b50\u77e9\u9635\u5206\u5f00\u4f18\u5316\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528LoRA\u53c2\u6570\u5316\u7684\u5185\u5728\u7ed3\u6784\uff0c\u5bfc\u81f4\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u7684\u6b21\u4f18\u6027\u80fd\u3002\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u6765\u63d0\u5347LoRA\u7684\u8bad\u7ec3\u6548\u679c\u3002", "method": "\u63d0\u51faODELoRA\u65b9\u6cd5\uff1a1) \u6784\u5efa\u8fde\u7eed\u65f6\u95f4\u4f18\u5316\u52a8\u6001\uff0c\u7528ODE\u6a21\u62df\u5b8c\u6574\u5fae\u8c03\u5728\u5e73\u8861\u6d41\u5f62\u4e0a\u7684\u68af\u5ea6\u6d41\uff1b2) \u91c7\u7528\u6b27\u62c9\u548c\u9f99\u683c-\u5e93\u5854\u7b49\u65f6\u95f4\u79bb\u6563\u5316\u65b9\u6848\u8ddf\u8e2a\u8f68\u8ff9\uff1b3) \u63d0\u4f9b\u7edf\u4e00\u7684ODE\u89c6\u89d2\u7406\u89e3\u548c\u8bbe\u8ba1LoRA\u8bad\u7ec3\u7b97\u6cd5\u3002", "result": "1) \u7406\u8bba\u8bc1\u660e\uff1a\u5728\u5f3a\u51f8\u76ee\u6807\u4e0b\uff0c\u67d0\u4e9b\u79bb\u6563\u5316\u65b9\u6848\u5177\u6709\u7ebf\u6027\u6536\u655b\u6027\uff1b2) \u77e9\u9635\u611f\u77e5\u4efb\u52a1\u9a8c\u8bc1\u7ebf\u6027\u6536\u655b\u884c\u4e3a\uff1b3) \u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u663e\u793a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u7a33\u5b9a\u6027\u65b9\u9762\uff1b4) \u5b9e\u73b0\u7a33\u5b9a\u7684\u7279\u5f81\u5b66\u4e60\u3002", "conclusion": "ODELoRA\u901a\u8fc7ODE\u6846\u67b6\u4f18\u5316LoRA\u56e0\u5b50\u77e9\u9635\uff0c\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u7a33\u5b9a\u6027\u65b9\u9762\uff0c\u4e3aLoRA\u8bad\u7ec3\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.08981", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.08981", "abs": "https://arxiv.org/abs/2602.08981", "authors": ["Marta Maria Marchese", "Daniel Braun", "Stefan Nimmrichter", "Dennis R\u00e4tzel"], "title": "Cascaded Optomechanical Sensing for Small Signals", "comment": "9+10 pages, 3 figures", "summary": "We propose a sensing scheme for detecting weak forces that achieves Heisenberg-limited sensitivity without relying on entanglement or other non-classical resources. Our scheme utilizes coherent averaging across a chain of N optomechanical cavities, unidirectionally coupled via a laser beam. As the beam passes through the cavities, it accumulates phase shifts induced by a common external force acting on the mechanical elements. Remarkably, this fully classical approach achieves the sensitivity scaling typically associated with quantum-enhanced protocols, providing a robust and experimentally feasible route to precision sensing. Potential applications range from high-sensitivity gravitational field measurements at the Large Hadron Collider to probing dark matter interactions and detecting gravitational waves. This work opens a new pathway for leveraging coherent light-matter interactions for force sensing.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u7ea0\u7f20\u6216\u91cf\u5b50\u8d44\u6e90\u7684\u5f31\u529b\u63a2\u6d4b\u65b9\u6848\uff0c\u901a\u8fc7\u5355\u5411\u8026\u5408\u5149\u673a\u68b0\u8154\u94fe\u5b9e\u73b0\u6d77\u68ee\u5821\u6781\u9650\u7075\u654f\u5ea6", "motivation": "\u4f20\u7edf\u91cf\u5b50\u589e\u5f3a\u4f20\u611f\u65b9\u6848\u4f9d\u8d56\u7ea0\u7f20\u7b49\u975e\u7ecf\u5178\u8d44\u6e90\uff0c\u5b9e\u9a8c\u5b9e\u73b0\u590d\u6742\u4e14\u8106\u5f31\u3002\u9700\u8981\u5f00\u53d1\u66f4\u7a33\u5065\u3001\u5b9e\u9a8c\u53ef\u884c\u7684\u7cbe\u5bc6\u4f20\u611f\u65b9\u6cd5", "method": "\u5229\u7528\u5355\u5411\u6fc0\u5149\u675f\u8026\u5408\u7684N\u4e2a\u5149\u673a\u68b0\u8154\u94fe\uff0c\u901a\u8fc7\u76f8\u5e72\u5e73\u5747\u7d2f\u79ef\u76f8\u4f4d\u504f\u79fb\u6765\u63a2\u6d4b\u4f5c\u7528\u5728\u673a\u68b0\u5143\u4ef6\u4e0a\u7684\u5171\u540c\u5916\u529b", "result": "\u8be5\u7eaf\u7ecf\u5178\u65b9\u6848\u5b9e\u73b0\u4e86\u901a\u5e38\u4e0e\u91cf\u5b50\u589e\u5f3a\u534f\u8bae\u76f8\u5173\u7684\u7075\u654f\u5ea6\u6807\u5ea6\uff08\u6d77\u68ee\u5821\u6781\u9650\uff09\uff0c\u4e3a\u7cbe\u5bc6\u4f20\u611f\u63d0\u4f9b\u4e86\u7a33\u5065\u4e14\u5b9e\u9a8c\u53ef\u884c\u7684\u9014\u5f84", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5229\u7528\u76f8\u5e72\u5149-\u7269\u8d28\u76f8\u4e92\u4f5c\u7528\u8fdb\u884c\u529b\u4f20\u611f\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5728\u5f15\u529b\u573a\u6d4b\u91cf\u3001\u6697\u7269\u8d28\u63a2\u6d4b\u548c\u5f15\u529b\u6ce2\u68c0\u6d4b\u7b49\u9886\u57df\u5177\u6709\u5e94\u7528\u524d\u666f"}}
{"id": "2602.07488", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07488", "abs": "https://arxiv.org/abs/2602.07488", "authors": ["Francesco Cagnetta", "Allan Ravent\u00f3s", "Surya Ganguli", "Matthieu Wyart"], "title": "Deriving Neural Scaling Laws from the statistics of natural language", "comment": null, "summary": "Despite the fact that experimental neural scaling laws have substantially guided empirical progress in large-scale machine learning, no existing theory can quantitatively predict the exponents of these important laws for any modern LLM trained on any natural language dataset. We provide the first such theory in the case of data-limited scaling laws. We isolate two key statistical properties of language that alone can predict neural scaling exponents: (i) the decay of pairwise token correlations with time separation between token pairs, and (ii) the decay of the next-token conditional entropy with the length of the conditioning context. We further derive a simple formula in terms of these statistics that predicts data-limited neural scaling exponents from first principles without any free parameters or synthetic data models. Our theory exhibits a remarkable match with experimentally measured neural scaling laws obtained from training GPT-2 and LLaMA style models from scratch on two qualitatively different benchmarks, TinyStories and WikiText.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u80fd\u591f\u5b9a\u91cf\u9884\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u6570\u636e\u53d7\u9650\u7f29\u653e\u5b9a\u5f8b\u6307\u6570\u7684\u7406\u8bba\uff0c\u8be5\u7406\u8bba\u57fa\u4e8e\u8bed\u8a00\u7684\u4e24\u4e2a\u5173\u952e\u7edf\u8ba1\u7279\u6027\uff0c\u65e0\u9700\u81ea\u7531\u53c2\u6570\u6216\u5408\u6210\u6570\u636e\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u5b9e\u9a8c\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u6781\u5927\u5730\u6307\u5bfc\u4e86\u5927\u89c4\u6a21\u673a\u5668\u5b66\u4e60\u7684\u5b9e\u8bc1\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u7406\u8bba\u65e0\u6cd5\u5b9a\u91cf\u9884\u6d4b\u4efb\u4f55\u73b0\u4ee3LLM\u5728\u4efb\u4f55\u81ea\u7136\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u7684\u8fd9\u4e9b\u91cd\u8981\u5b9a\u5f8b\u7684\u6307\u6570\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8bed\u8a00\u7684\u4e24\u4e2a\u5173\u952e\u7edf\u8ba1\u7279\u6027\uff1a(i) \u6807\u8bb0\u5bf9\u76f8\u5173\u6027\u968f\u65f6\u95f4\u95f4\u9694\u7684\u8870\u51cf\uff0c(ii) \u4e0b\u4e00\u6807\u8bb0\u6761\u4ef6\u71b5\u968f\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u8870\u51cf\uff0c\u63a8\u5bfc\u51fa\u4e00\u4e2a\u7b80\u5355\u516c\u5f0f\u6765\u9884\u6d4b\u6570\u636e\u53d7\u9650\u795e\u7ecf\u7f29\u653e\u6307\u6570\u3002", "result": "\u8be5\u7406\u8bba\u5728GPT-2\u548cLLaMA\u98ce\u683c\u6a21\u578b\u5728TinyStories\u548cWikiText\u4e24\u4e2a\u4e0d\u540c\u57fa\u51c6\u4e0a\u7684\u8bad\u7ec3\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4e0e\u5b9e\u9a8c\u6d4b\u91cf\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u7684\u663e\u8457\u5339\u914d\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u63d0\u4f9b\u4e86\u80fd\u591f\u4ece\u7b2c\u4e00\u6027\u539f\u7406\u5b9a\u91cf\u9884\u6d4b\u6570\u636e\u53d7\u9650\u795e\u7ecf\u7f29\u653e\u6307\u6570\u7684\u7406\u8bba\uff0c\u63ed\u793a\u4e86\u8bed\u8a00\u7edf\u8ba1\u7279\u6027\u4e0e\u7f29\u653e\u5b9a\u5f8b\u4e4b\u95f4\u7684\u6839\u672c\u8054\u7cfb\uff0c\u4e3a\u7406\u89e3\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.09020", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.09020", "abs": "https://arxiv.org/abs/2602.09020", "authors": ["Carla Rieger", "Albert T. Schmitz", "Gehad Salem", "Massimiliano Incudini", "Sofia Vallecorsa", "Anne Y. Matsuura", "Michele Grossi", "Gian Giacomo Guerreschi"], "title": "Hybrid Method of Efficient Simulation of Physics Applications for a Quantum Computer", "comment": "24 pages, 14 figures", "summary": "Quantum chemistry and materials science are among the most promising areas for demonstrating algorithmic quantum advantage and quantum utility due to their inherent quantum mechanical nature. Still, large-scale simulations of quantum circuits are essential for determining the problem size at which quantum solutions outperform classical methods. In this work, we present a novel hybrid simulation approach, forming a hybrid of a fullstate and a Clifford simulator, specifically designed to address the computational challenges associated with the time evolution of quantum chemistry Hamiltonians. Our method focuses on the efficient emulation of multi-qubit rotations, a critical component of Trotterized Hamiltonian evolution. By optimizing the representation and execution of multi-qubit operations leveraging the Pauli frame, our approach significantly reduces the computational cost of simulating quantum circuits, enabling more efficient simulations. Beyond its impact on chemistry applications, our emulation strategy has broad implications for any computational workload that relies heavily on multi-qubit rotations. By increasing the efficiency of quantum simulations, our method facilitates more accurate and cost-effective studies of complex quantum systems. We quantify the performance improvements and computational savings for this emulation strategy, and we obtain a speedup of a factor $\\approx 18$ ($\\approx 22$ with MPI) for our evaluated chemistry Hamiltonians with 24 qubits. Thus, we evaluate our integration of this emulation strategy into the Intel Quantum SDK, further bridging the gap between theoretical algorithm development and practical quantum software implementations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5168\u72b6\u6001\u6a21\u62df\u5668\u548cClifford\u6a21\u62df\u5668\u7684\u6df7\u5408\u6a21\u62df\u65b9\u6cd5\uff0c\u4e13\u95e8\u7528\u4e8e\u9ad8\u6548\u6a21\u62df\u91cf\u5b50\u5316\u5b66\u54c8\u5bc6\u987f\u91cf\u65f6\u95f4\u6f14\u5316\u4e2d\u7684\u591a\u91cf\u5b50\u6bd4\u7279\u65cb\u8f6c\u64cd\u4f5c\uff0c\u572824\u91cf\u5b50\u6bd4\u7279\u5316\u5b66\u54c8\u5bc6\u987f\u91cf\u4e0a\u5b9e\u73b0\u4e86\u7ea618\u500d\uff08MPI\u4e0b\u7ea622\u500d\uff09\u7684\u52a0\u901f\u3002", "motivation": "\u91cf\u5b50\u5316\u5b66\u548c\u6750\u6599\u79d1\u5b66\u662f\u5c55\u793a\u91cf\u5b50\u7b97\u6cd5\u4f18\u52bf\u548c\u5b9e\u7528\u6027\u7684\u91cd\u8981\u9886\u57df\uff0c\u4f46\u9700\u8981\u5927\u89c4\u6a21\u91cf\u5b50\u7535\u8def\u6a21\u62df\u6765\u786e\u5b9a\u91cf\u5b50\u65b9\u6cd5\u8d85\u8d8a\u7ecf\u5178\u65b9\u6cd5\u7684\u4e34\u754c\u95ee\u9898\u89c4\u6a21\u3002\u5f53\u524d\u6a21\u62df\u91cf\u5b50\u5316\u5b66\u54c8\u5bc6\u987f\u91cf\u65f6\u95f4\u6f14\u5316\u9762\u4e34\u8ba1\u7b97\u6311\u6218\uff0c\u7279\u522b\u662f\u591a\u91cf\u5b50\u6bd4\u7279\u65cb\u8f6c\u64cd\u4f5c\u7684\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u6a21\u62df\u65b9\u6cd5\uff0c\u7ed3\u5408\u5168\u72b6\u6001\u6a21\u62df\u5668\u548cClifford\u6a21\u62df\u5668\uff0c\u4e13\u6ce8\u4e8e\u9ad8\u6548\u6a21\u62df\u591a\u91cf\u5b50\u6bd4\u7279\u65cb\u8f6c\u64cd\u4f5c\u3002\u901a\u8fc7\u5229\u7528Pauli\u6846\u67b6\u4f18\u5316\u591a\u91cf\u5b50\u6bd4\u7279\u64cd\u4f5c\u7684\u8868\u793a\u548c\u6267\u884c\uff0c\u663e\u8457\u964d\u4f4e\u91cf\u5b50\u7535\u8def\u6a21\u62df\u7684\u8ba1\u7b97\u6210\u672c\u3002\u8be5\u65b9\u6cd5\u5df2\u96c6\u6210\u5230Intel\u91cf\u5b50SDK\u4e2d\u3002", "result": "\u572824\u91cf\u5b50\u6bd4\u7279\u7684\u5316\u5b66\u54c8\u5bc6\u987f\u91cf\u8bc4\u4f30\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7ea618\u500d\u7684\u52a0\u901f\uff08\u4f7f\u7528MPI\u65f6\u7ea622\u500d\uff09\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5bf9\u5316\u5b66\u5e94\u7528\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u5bf9\u4efb\u4f55\u4f9d\u8d56\u591a\u91cf\u5b50\u6bd4\u7279\u65cb\u8f6c\u7684\u8ba1\u7b97\u4efb\u52a1\u90fd\u6709\u5e7f\u6cdb\u610f\u4e49\u3002", "conclusion": "\u8be5\u6df7\u5408\u6a21\u62df\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u591a\u91cf\u5b50\u6bd4\u7279\u65cb\u8f6c\u64cd\u4f5c\u7684\u6a21\u62df\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u91cf\u5b50\u7535\u8def\u6a21\u62df\u7684\u6548\u7387\uff0c\u4e3a\u590d\u6742\u91cf\u5b50\u7cfb\u7edf\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u548c\u6210\u672c\u6548\u76ca\u66f4\u9ad8\u7684\u5de5\u5177\uff0c\u5e76\u8fdb\u4e00\u6b65\u7f29\u5c0f\u4e86\u7406\u8bba\u7b97\u6cd5\u5f00\u53d1\u4e0e\u5b9e\u9645\u91cf\u5b50\u8f6f\u4ef6\u5b9e\u73b0\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2602.07494", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07494", "abs": "https://arxiv.org/abs/2602.07494", "authors": ["Shenxi Wu", "Haosong Zhang", "Xingjian Ma", "Shirui Bian", "Yichi Zhang", "Xi Chen", "Wei Lin"], "title": "Hyperparameter Transfer Laws for Non-Recurrent Multi-Path Neural Networks", "comment": null, "summary": "Deeper modern architectures are costly to train, making hyperparameter transfer preferable to expensive repeated tuning. Maximal Update Parametrization ($\u03bc$P) helps explain why many hyperparameters transfer across width. Yet depth scaling is less understood for modern architectures, whose computation graphs contain multiple parallel paths and residual aggregation. To unify various non-recurrent multi-path neural networks such as CNNs, ResNets, and Transformers, we introduce a graph-based notion of effective depth. Under stabilizing initializations and a maximal-update criterion, we show that the optimal learning rate decays with effective depth following a universal -3/2 power law. Here, the maximal-update criterion maximizes the typical one-step representation change at initialization without causing instability, and effective depth is the minimal path length from input to output, counting layers and residual additions. Experiments across diverse architectures confirm the predicted slope and enable reliable zero-shot transfer of learning rates across depths and widths, turning depth scaling into a predictable hyperparameter-transfer problem.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u56fe\u7684\u6709\u6548\u6df1\u5ea6\u6982\u5ff5\uff0c\u53d1\u73b0\u5728\u7a33\u5b9a\u521d\u59cb\u5316\u548c\u6700\u5927\u66f4\u65b0\u51c6\u5219\u4e0b\uff0c\u6700\u4f18\u5b66\u4e60\u7387\u968f\u6709\u6548\u6df1\u5ea6\u6309-3/2\u5e42\u5f8b\u8870\u51cf\uff0c\u5b9e\u73b0\u4e86\u8de8\u6df1\u5ea6\u548c\u5bbd\u5ea6\u7684\u96f6\u6837\u672c\u8d85\u53c2\u6570\u8fc1\u79fb\u3002", "motivation": "\u73b0\u4ee3\u6df1\u5ea6\u67b6\u6784\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u8de8\u5bbd\u5ea6\u548c\u6df1\u5ea6\u7684\u8d85\u53c2\u6570\u8fc1\u79fb\u3002\u867d\u7136\u03bcP\u89e3\u91ca\u4e86\u5bbd\u5ea6\u7f29\u653e\u4e0b\u7684\u8d85\u53c2\u6570\u8fc1\u79fb\uff0c\u4f46\u6df1\u5ea6\u7f29\u653e\u5bf9\u4e8e\u5305\u542b\u5e76\u884c\u8def\u5f84\u548c\u6b8b\u5dee\u805a\u5408\u7684\u73b0\u4ee3\u67b6\u6784\uff08\u5982CNN\u3001ResNet\u3001Transformer\uff09\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u56fe\u7684\u6709\u6548\u6df1\u5ea6\u6982\u5ff5\u6765\u7edf\u4e00\u5404\u79cd\u975e\u5faa\u73af\u591a\u8def\u5f84\u795e\u7ecf\u7f51\u7edc\uff1b\u5728\u7a33\u5b9a\u521d\u59cb\u5316\u548c\u6700\u5927\u66f4\u65b0\u51c6\u5219\u4e0b\uff0c\u63a8\u5bfc\u51fa\u6700\u4f18\u5b66\u4e60\u7387\u968f\u6709\u6548\u6df1\u5ea6\u7684-3/2\u5e42\u5f8b\u8870\u51cf\u5173\u7cfb\uff1b\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9e\u4e86-3/2\u5e42\u5f8b\u659c\u7387\uff0c\u5b9e\u73b0\u4e86\u8de8\u6df1\u5ea6\u548c\u5bbd\u5ea6\u7684\u53ef\u9760\u96f6\u6837\u672c\u5b66\u4e60\u7387\u8fc1\u79fb\uff0c\u5c06\u6df1\u5ea6\u7f29\u653e\u8f6c\u5316\u4e3a\u53ef\u9884\u6d4b\u7684\u8d85\u53c2\u6570\u8fc1\u79fb\u95ee\u9898\u3002", "conclusion": "\u57fa\u4e8e\u6709\u6548\u6df1\u5ea6\u548c\u6700\u5927\u66f4\u65b0\u51c6\u5219\u7684\u7406\u8bba\u6846\u67b6\u6210\u529f\u89e3\u91ca\u4e86\u6df1\u5ea6\u7f29\u653e\u4e0b\u7684\u8d85\u53c2\u6570\u8fc1\u79fb\u89c4\u5f8b\uff0c\u4e3a\u73b0\u4ee3\u591a\u8def\u5f84\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6df1\u5ea6\u7f29\u653e\u7406\u8bba\u3002"}}
{"id": "2602.07496", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07496", "abs": "https://arxiv.org/abs/2602.07496", "authors": ["Antonio Mone", "Frans A. Oliehoek", "Luciano Cavalcante Siebert"], "title": "CoMI-IRL: Contrastive Multi-Intention Inverse Reinforcement Learning", "comment": "14 pages, 6 figures", "summary": "Inverse Reinforcement Learning (IRL) seeks to infer reward functions from expert demonstrations. When demonstrations originate from multiple experts with different intentions, the problem is known as Multi-Intention IRL (MI-IRL). Recent deep generative MI-IRL approaches couple behavior clustering and reward learning, but typically require prior knowledge of the number of true behavioral modes $K^*$. This reliance on expert knowledge limits their adaptability to new behaviors, and only enables analysis related to the learned rewards, and not across the behavior modes used to train them. We propose Contrastive Multi-Intention IRL (CoMI-IRL), a transformer-based unsupervised framework that decouples behavior representation and clustering from downstream reward learning. Our experiments show that CoMI-IRL outperforms existing approaches without a priori knowledge of $K^*$ or labels, while allowing for visual interpretation of behavior relationships and adaptation to unseen behavior without full retraining.", "AI": {"tldr": "CoMI-IRL\uff1a\u57fa\u4e8eTransformer\u7684\u65e0\u76d1\u7763\u591a\u610f\u56fe\u9006\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u884c\u4e3a\u8868\u793a\u4e0e\u805a\u7c7b\u4ece\u5956\u52b1\u5b66\u4e60\u4e2d\u89e3\u8026\uff0c\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u5373\u53ef\u9002\u5e94\u65b0\u884c\u4e3a", "motivation": "\u73b0\u6709\u6df1\u5ea6\u751f\u6210MI-IRL\u65b9\u6cd5\u9700\u8981\u5148\u9a8c\u77e5\u8bc6\u4e86\u89e3\u771f\u5b9e\u884c\u4e3a\u6a21\u5f0f\u6570\u91cfK*\uff0c\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\u9650\u5236\u4e86\u9002\u5e94\u65b0\u884c\u4e3a\u7684\u80fd\u529b\uff0c\u4e14\u53ea\u80fd\u5206\u6790\u4e0e\u5b66\u4e60\u5956\u52b1\u76f8\u5173\u7684\u5185\u5bb9\uff0c\u65e0\u6cd5\u8de8\u8bad\u7ec3\u884c\u4e3a\u6a21\u5f0f\u8fdb\u884c\u5206\u6790", "method": "\u63d0\u51fa\u5bf9\u6bd4\u591a\u610f\u56fe\u9006\u5f3a\u5316\u5b66\u4e60(CoMI-IRL)\uff0c\u57fa\u4e8eTransformer\u7684\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u5c06\u884c\u4e3a\u8868\u793a\u548c\u805a\u7c7b\u4e0e\u4e0b\u6e38\u5956\u52b1\u5b66\u4e60\u89e3\u8026", "result": "CoMI-IRL\u5728\u65e0\u9700K*\u5148\u9a8c\u77e5\u8bc6\u6216\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u5141\u8bb8\u884c\u4e3a\u5173\u7cfb\u7684\u53ef\u89c6\u5316\u89e3\u91ca\uff0c\u5e76\u80fd\u9002\u5e94\u672a\u89c1\u884c\u4e3a\u800c\u65e0\u9700\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3", "conclusion": "CoMI-IRL\u901a\u8fc7\u89e3\u8026\u884c\u4e3a\u8868\u793a\u548c\u5956\u52b1\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfMI-IRL\u65b9\u6cd5\u5bf9\u5148\u9a8c\u77e5\u8bc6\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027"}}
{"id": "2602.07519", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07519", "abs": "https://arxiv.org/abs/2602.07519", "authors": ["Martin Fixman", "Alessandro Abati", "Juli\u00e1n Jim\u00e9nez Nimmo", "Sean Lim", "Esther Mondrag\u00f3n"], "title": "PALMS: Pavlovian Associative Learning Models Simulator", "comment": null, "summary": "Simulations are an indispensable step in the cycle of theory development and refinement, helping researchers formulate precise definitions, generate models, and make accurate predictions. This paper introduces the Pavlovian Associative Learning Models Simulator (PALMS), a Python environment to simulate Pavlovian conditioning experiments. In addition to the canonical Rescorla-Wagner model, PALMS incorporates several attentional learning approaches, including Pearce-Kaye-Hall, Mackintosh Extended, Le Pelley's Hybrid, and a novel extension of the Rescorla-Wagner model with a unified variable learning rate that integrates Mackintosh's and Pearce and Hall's opposing conceptualisations. The simulator's graphical interface allows for the input of entire experimental designs in an alphanumeric format, akin to that used by experimental neuroscientists. Moreover, it uniquely enables the simulation of experiments involving hundreds of stimuli, as well as the computation of configural cues and configural-cue compounds across all models, thereby considerably expanding their predictive capabilities. PALMS operates efficiently, providing instant visualisation of results, supporting rapid, precise comparisons of various models' predictions within a single architecture and environment. Furthermore, graphic displays can be easily saved, and simulated data can be exported to spreadsheets. To illustrate the simulator's capabilities and functionalities, we provide a detailed description of the software and examples of use, reproducing published experiments in the associative learning literature. PALMS is licensed under the open-source GNU Lesser General Public License 3.0. The simulator source code and the latest multiplatform release build are accessible as a GitHub repository at https://github.com/cal-r/PALMS-Simulator", "AI": {"tldr": "PALMS\u662f\u4e00\u4e2a\u7528\u4e8e\u6a21\u62df\u5df4\u752b\u6d1b\u592b\u6761\u4ef6\u53cd\u5c04\u5b9e\u9a8c\u7684Python\u4eff\u771f\u73af\u5883\uff0c\u96c6\u6210\u4e86\u591a\u79cd\u6ce8\u610f\u529b\u5b66\u4e60\u6a21\u578b\uff0c\u652f\u6301\u5927\u89c4\u6a21\u523a\u6fc0\u6a21\u62df\u548c\u914d\u7f6e\u7ebf\u7d22\u8ba1\u7b97\u3002", "motivation": "\u4eff\u771f\u5728\u7406\u8bba\u53d1\u5c55\u548c\u5b8c\u5584\u8fc7\u7a0b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5de5\u5177\u53ef\u80fd\u65e0\u6cd5\u5145\u5206\u652f\u6301\u590d\u6742\u7684\u5df4\u752b\u6d1b\u592b\u6761\u4ef6\u53cd\u5c04\u5b9e\u9a8c\u6a21\u62df\uff0c\u7279\u522b\u662f\u6d89\u53ca\u5927\u91cf\u523a\u6fc0\u548c\u914d\u7f6e\u7ebf\u7d22\u7684\u60c5\u51b5\u3002", "method": "\u5f00\u53d1\u4e86PALMS\u4eff\u771f\u5668\uff0c\u5305\u542b\u7ecf\u5178Rescorla-Wagner\u6a21\u578b\u548c\u591a\u79cd\u6ce8\u610f\u529b\u5b66\u4e60\u6a21\u578b\uff08Pearce-Kaye-Hall\u3001Mackintosh Extended\u3001Le Pelley's Hybrid\u7b49\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u53ef\u53d8\u5b66\u4e60\u7387\u7684Rescorla-Wagner\u6269\u5c55\u6a21\u578b\u3002\u63d0\u4f9b\u56fe\u5f62\u754c\u9762\u652f\u6301\u5b9e\u9a8c\u8bbe\u8ba1\u8f93\u5165\uff0c\u652f\u6301\u6570\u767e\u4e2a\u523a\u6fc0\u7684\u6a21\u62df\u548c\u914d\u7f6e\u7ebf\u7d22\u8ba1\u7b97\u3002", "result": "PALMS\u80fd\u591f\u9ad8\u6548\u8fd0\u884c\uff0c\u5373\u65f6\u53ef\u89c6\u5316\u7ed3\u679c\uff0c\u652f\u6301\u4e0d\u540c\u6a21\u578b\u9884\u6d4b\u7684\u5feb\u901f\u7cbe\u786e\u6bd4\u8f83\u3002\u56fe\u5f62\u663e\u793a\u53ef\u8f7b\u677e\u4fdd\u5b58\uff0c\u6a21\u62df\u6570\u636e\u53ef\u5bfc\u51fa\u5230\u7535\u5b50\u8868\u683c\u3002\u901a\u8fc7\u590d\u73b0\u5df2\u53d1\u8868\u7684\u5173\u8054\u5b66\u4e60\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8f6f\u4ef6\u529f\u80fd\u3002", "conclusion": "PALMS\u4f5c\u4e3a\u4e00\u4e2a\u5f00\u6e90\u4eff\u771f\u73af\u5883\uff0c\u663e\u8457\u6269\u5c55\u4e86\u5173\u8054\u5b66\u4e60\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u3001\u9ad8\u6548\u7684\u5e73\u53f0\u6765\u6bd4\u8f83\u548c\u9a8c\u8bc1\u4e0d\u540c\u5b66\u4e60\u7406\u8bba\u3002"}}
{"id": "2602.07521", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07521", "abs": "https://arxiv.org/abs/2602.07521", "authors": ["Xionghui Yang", "Bozhou Chen", "Yunlong Lu", "Yongyi Wang", "Lingfeng Li", "Lanxiao Huang", "Lin Liu", "Wenjun Wang", "Meng Meng", "Xia Lin", "Wenxin Li"], "title": "Pareto-guided Pipeline for Distilling Featherweight AI Agents in Mobile MOBA Games", "comment": null, "summary": "Recent advances in game AI have demonstrated the feasibility of training agents that surpass top-tier human professionals in complex environments such as Honor of Kings (HoK), a leading mobile multiplayer online battle arena (MOBA) game. However, deploying such powerful agents on mobile devices remains a major challenge. On one hand, the intricate multi-modal state representation and hierarchical action space of HoK demand large, sophisticated policy networks that are inherently difficult to compress into lightweight forms. On the other hand, production deployment requires high-frequency inference under strict energy and latency constraints on mobile platform. To the best of our knowledge, bridging large-scale game AI and practical on-device deployment has not been systematically studied. In this work, we propose a Pareto optimality guided pipeline and design a high-efficiency student architecture search space tailored for mobile execution, enabling systematic exploration of the trade-off between performance and efficiency. Experimental results demonstrate that the distilled model achieves remarkable efficiency, including an $12.4\\times$ faster inference speed (under 0.5ms per frame) and a $15.6\\times$ improvement in energy efficiency (under 0.5mAh per game), while retaining a 40.32% win rate against the original teacher model.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u79fb\u52a8\u8bbe\u5907\u90e8\u7f72\u7684\u738b\u8005\u8363\u8000\u6e38\u620fAI\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e15\u7d2f\u6258\u6700\u4f18\u5f15\u5bfc\u7684\u6d41\u6c34\u7ebf\u548c\u9ad8\u6548\u5b66\u751f\u67b6\u6784\u641c\u7d22\u7a7a\u95f4\uff0c\u5728\u4fdd\u630140.32%\u80dc\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e8612.4\u500d\u63a8\u7406\u52a0\u901f\u548c15.6\u500d\u80fd\u6548\u63d0\u5347\u3002", "motivation": "\u867d\u7136\u6e38\u620fAI\u5df2\u80fd\u5728\u590d\u6742MOBA\u6e38\u620f\uff08\u5982\u738b\u8005\u8363\u8000\uff09\u4e2d\u8d85\u8d8a\u4eba\u7c7b\u9876\u7ea7\u73a9\u5bb6\uff0c\u4f46\u5c06\u8fd9\u4e9b\u5f3a\u5927\u7684\u667a\u80fd\u4f53\u90e8\u7f72\u5230\u79fb\u52a8\u8bbe\u5907\u4e0a\u9762\u4e34\u5de8\u5927\u6311\u6218\uff1a\u590d\u6742\u7684\u591a\u6a21\u6001\u72b6\u6001\u8868\u793a\u548c\u5206\u5c42\u52a8\u4f5c\u7a7a\u95f4\u9700\u8981\u5927\u578b\u7b56\u7565\u7f51\u7edc\uff0c\u96be\u4ee5\u538b\u7f29\u4e3a\u8f7b\u91cf\u5f62\u5f0f\uff1b\u800c\u751f\u4ea7\u90e8\u7f72\u9700\u8981\u5728\u79fb\u52a8\u5e73\u53f0\u4e0a\u6ee1\u8db3\u4e25\u683c\u7684\u80fd\u8017\u548c\u5ef6\u8fdf\u7ea6\u675f\u3002", "method": "\u63d0\u51fa\u5e15\u7d2f\u6258\u6700\u4f18\u5f15\u5bfc\u7684\u6d41\u6c34\u7ebf\uff0c\u8bbe\u8ba1\u4e13\u95e8\u9488\u5bf9\u79fb\u52a8\u6267\u884c\u7684\u9ad8\u6548\u5b66\u751f\u67b6\u6784\u641c\u7d22\u7a7a\u95f4\uff0c\u7cfb\u7edf\u63a2\u7d22\u6027\u80fd\u4e0e\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u3002\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5c06\u5927\u578b\u6559\u5e08\u6a21\u578b\u538b\u7f29\u4e3a\u9002\u5408\u79fb\u52a8\u8bbe\u5907\u90e8\u7f72\u7684\u8f7b\u91cf\u5b66\u751f\u6a21\u578b\u3002", "result": "\u84b8\u998f\u540e\u7684\u6a21\u578b\u5b9e\u73b0\u4e86\u663e\u8457\u6548\u7387\u63d0\u5347\uff1a\u63a8\u7406\u901f\u5ea6\u52a0\u5feb12.4\u500d\uff08\u6bcf\u5e27\u4f4e\u4e8e0.5ms\uff09\uff0c\u80fd\u6548\u63d0\u534715.6\u500d\uff08\u6bcf\u5c40\u6e38\u620f\u4f4e\u4e8e0.5mAh\uff09\uff0c\u540c\u65f6\u4fdd\u630140.32%\u7684\u80dc\u7387\u5bf9\u6297\u539f\u59cb\u6559\u5e08\u6a21\u578b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u5927\u89c4\u6a21\u6e38\u620fAI\u4e0e\u5b9e\u7528\u8bbe\u5907\u90e8\u7f72\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u79fb\u52a8\u8bbe\u5907\u90e8\u7f72\u6e38\u620fAI\u7684\u6548\u7387\u548c\u6027\u80fd\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07529", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07529", "abs": "https://arxiv.org/abs/2602.07529", "authors": ["Jianwen Chen", "Xinyu Yang", "Peng Xia", "Arian Azarang", "Yueh Z Lee", "Gang Li", "Hongtu Zhu", "Yun Li", "Beidi Chen", "Huaxiu Yao"], "title": "MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution", "comment": null, "summary": "Large language models (LLMs) have demonstrated strong performance and rapid progress in a wide range of medical reasoning tasks. However, their sequential autoregressive decoding forces inherently parallel clinical reasoning, such as differential diagnosis, into a single linear reasoning path, limiting both efficiency and reliability for complex medical problems. To address this, we propose MedVerse, a reasoning framework for complex medical inference that reformulates medical reasoning as a parallelizable directed acyclic graph (DAG) process based on Petri net theory. The framework adopts a full-stack design across data, model architecture, and system execution. For data creation, we introduce the MedVerse Curator, an automated pipeline that synthesizes knowledge-grounded medical reasoning paths and transforms them into Petri net-structured representations. At the architectural level, we propose a topology-aware attention mechanism with adaptive position indices that supports parallel reasoning while preserving logical consistency. Systematically, we develop a customized inference engine that supports parallel execution without additional overhead. Empirical evaluations show that MedVerse improves strong general-purpose LLMs by up to 8.9%. Compared to specialized medical LLMs, MedVerse achieves comparable performance while delivering a 1.3x reduction in inference latency and a 1.7x increase in generation throughput, enabled by its parallel decoding capability.", "AI": {"tldr": "MedVerse\uff1a\u57fa\u4e8ePetri\u7f51\u7406\u8bba\u5c06\u533b\u5b66\u63a8\u7406\u91cd\u6784\u4e3a\u53ef\u5e76\u884c\u5316\u7684\u6709\u5411\u65e0\u73af\u56fe\u8fc7\u7a0b\uff0c\u901a\u8fc7\u62d3\u6251\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\u548c\u5b9a\u5236\u63a8\u7406\u5f15\u64ce\u5b9e\u73b0\u5e76\u884c\u89e3\u7801\uff0c\u5728\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u987a\u5e8f\u81ea\u56de\u5f52\u89e3\u7801\u65b9\u5f0f\u5c06\u672c\u5e94\u5e76\u884c\u7684\u4e34\u5e8a\u63a8\u7406\uff08\u5982\u9274\u522b\u8bca\u65ad\uff09\u5f3a\u5236\u538b\u7f29\u4e3a\u5355\u4e00\u7ebf\u6027\u63a8\u7406\u8def\u5f84\uff0c\u9650\u5236\u4e86\u590d\u6742\u533b\u5b66\u95ee\u9898\u7684\u63a8\u7406\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "method": "1. \u6570\u636e\u5c42\u9762\uff1aMedVerse Curator\u81ea\u52a8\u5316\u7ba1\u9053\u5408\u6210\u57fa\u4e8e\u77e5\u8bc6\u7684\u533b\u5b66\u63a8\u7406\u8def\u5f84\u5e76\u8f6c\u6362\u4e3aPetri\u7f51\u7ed3\u6784\u8868\u793a\uff1b2. \u67b6\u6784\u5c42\u9762\uff1a\u63d0\u51fa\u62d3\u6251\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\uff0c\u652f\u6301\u5e76\u884c\u63a8\u7406\u540c\u65f6\u4fdd\u6301\u903b\u8f91\u4e00\u81f4\u6027\uff1b3. \u7cfb\u7edf\u5c42\u9762\uff1a\u5f00\u53d1\u5b9a\u5236\u63a8\u7406\u5f15\u64ce\u652f\u6301\u65e0\u989d\u5916\u5f00\u9500\u7684\u5e76\u884c\u6267\u884c\u3002", "result": "MedVerse\u5c06\u901a\u7528LLMs\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe8.9%\uff1b\u4e0e\u4e13\u7528\u533b\u5b66LLMs\u76f8\u6bd4\uff0c\u5728\u4fdd\u6301\u76f8\u5f53\u6027\u80fd\u7684\u540c\u65f6\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e1.3\u500d\uff0c\u751f\u6210\u541e\u5410\u91cf\u63d0\u9ad81.7\u500d\u3002", "conclusion": "MedVerse\u901a\u8fc7\u5c06\u533b\u5b66\u63a8\u7406\u91cd\u6784\u4e3a\u53ef\u5e76\u884c\u5316\u7684DAG\u8fc7\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfLLMs\u5728\u533b\u5b66\u63a8\u7406\u4e2d\u7684\u7ebf\u6027\u9650\u5236\uff0c\u5728\u63d0\u5347\u63a8\u7406\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u6539\u5584\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2602.07530", "categories": ["cs.LG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.07530", "abs": "https://arxiv.org/abs/2602.07530", "authors": ["Sreenivas Gollapudi", "Kostas Kollias", "Kamesh Munagala", "Aravindan Vijayaraghavan"], "title": "Compact Conformal Subgraphs", "comment": null, "summary": "Conformal prediction provides rigorous, distribution-free uncertainty guarantees, but often yields prohibitively large prediction sets in structured domains such as routing, planning, or sequential recommendation. We introduce \"graph-based conformal compression\", a framework for constructing compact subgraphs that preserve statistical validity while reducing structural complexity. We formulate compression as selecting a smallest subgraph capturing a prescribed fraction of the probability mass, and reduce to a weighted version of densest $k$-subgraphs in hypergraphs, in the regime where the subgraph has a large fraction of edges. We design efficient approximation algorithms that achieve constant factor coverage and size trade-offs. Crucially, we prove that our relaxation satisfies a monotonicity property, derived from a connection to parametric minimum cuts, which guarantees the nestedness required for valid conformal guarantees. Our results on the one hand bridge efficient conformal prediction with combinatorial graph compression via monotonicity, to provide rigorous guarantees on both statistical validity, and compression or size. On the other hand, they also highlight an algorithmic regime, distinct from classical densest-$k$-subgraph hardness settings, where the problem can be approximated efficiently. We finally validate our algorithmic approach via simulations for trip planning and navigation, and compare to natural baselines.", "AI": {"tldr": "\u63d0\u51fa\u56fe\u57fa\u4fdd\u5f62\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u7ec4\u5408\u4f18\u5316\u6784\u5efa\u7d27\u51d1\u5b50\u56fe\uff0c\u5728\u4fdd\u6301\u7edf\u8ba1\u6709\u6548\u6027\u7684\u540c\u65f6\u51cf\u5c11\u7ed3\u6784\u5316\u9884\u6d4b\u96c6\u7684\u5927\u5c0f", "motivation": "\u4f20\u7edf\u4fdd\u5f62\u9884\u6d4b\u5728\u7ed3\u6784\u5316\u9886\u57df\uff08\u5982\u8def\u7531\u3001\u89c4\u5212\u3001\u5e8f\u5217\u63a8\u8350\uff09\u4ea7\u751f\u7684\u9884\u6d4b\u96c6\u8fc7\u5927\uff0c\u9700\u8981\u538b\u7f29\u65b9\u6cd5\u5728\u4fdd\u6301\u7edf\u8ba1\u4fdd\u8bc1\u7684\u540c\u65f6\u51cf\u5c11\u7ed3\u6784\u590d\u6742\u6027", "method": "\u5c06\u538b\u7f29\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u9009\u62e9\u6355\u83b7\u89c4\u5b9a\u6982\u7387\u8d28\u91cf\u7684\u6700\u5c0f\u5b50\u56fe\uff0c\u5f52\u7ea6\u4e3a\u8d85\u56fe\u4e2d\u52a0\u6743\u6700\u5bc6k\u5b50\u56fe\u95ee\u9898\uff0c\u8bbe\u8ba1\u9ad8\u6548\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5229\u7528\u53c2\u6570\u6700\u5c0f\u5272\u7684\u5355\u8c03\u6027\u4fdd\u8bc1\u5d4c\u5957\u6027", "result": "\u8bbe\u8ba1\u4e86\u5b9e\u73b0\u5e38\u6570\u56e0\u5b50\u8986\u76d6\u7387\u548c\u5927\u5c0f\u6743\u8861\u7684\u9ad8\u6548\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u8bc1\u660e\u4e86\u677e\u5f1b\u6ee1\u8db3\u5355\u8c03\u6027\u4ece\u800c\u4fdd\u8bc1\u6709\u6548\u4fdd\u5f62\u4fdd\u8bc1\uff0c\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u884c\u7a0b\u89c4\u5212\u548c\u5bfc\u822a\u4e2d\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u6846\u67b6\u5c06\u9ad8\u6548\u4fdd\u5f62\u9884\u6d4b\u4e0e\u7ec4\u5408\u56fe\u538b\u7f29\u901a\u8fc7\u5355\u8c03\u6027\u8fde\u63a5\uff0c\u63d0\u4f9b\u7edf\u8ba1\u6709\u6548\u6027\u548c\u538b\u7f29\u5927\u5c0f\u7684\u4e25\u683c\u4fdd\u8bc1\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u4e0e\u7ecf\u5178\u6700\u5bc6k\u5b50\u56fe\u95ee\u9898\u4e0d\u540c\u7684\u53ef\u9ad8\u6548\u8fd1\u4f3c\u7684\u7b97\u6cd5\u673a\u5236"}}
{"id": "2602.07562", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07562", "abs": "https://arxiv.org/abs/2602.07562", "authors": ["Antoine Gonon", "Alexandre Cordonnier", "Nicolas Boumal"], "title": "Gaussian Match-and-Copy: A Minimalist Benchmark for Studying Transformer Induction", "comment": null, "summary": "Match-and-copy is a core retrieval primitive used at inference time by large language models to retrieve a matching token from the context then copy its successor. Yet, understanding how this behavior emerges on natural data is challenging because retrieval and memorization are entangled. To disentangle the two, we introduce Gaussian Match-and-Copy (GMC), a minimalist benchmark that isolates long-range retrieval through pure second-order correlation signals. Numerical investigations show that this task retains key qualitative aspects of how Transformers develop match-and-copy circuits in practice, and separates architectures by their retrieval capabilities. We also analyze the optimization dynamics in a simplified attention setting. Although many solutions are a priori possible under a regression objective, including ones that do not implement retrieval, we identify an implicit-bias regime in which gradient descent drives the parameters to diverge while their direction aligns with the max-margin separator, yielding hard match selection. We prove this max-margin alignment for GD trajectories that reach vanishing empirical loss under explicit technical conditions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Gaussian Match-and-Copy\u57fa\u51c6\uff0c\u901a\u8fc7\u7eaf\u4e8c\u9636\u76f8\u5173\u4fe1\u53f7\u9694\u79bb\u957f\u7a0b\u68c0\u7d22\uff0c\u7814\u7a76Transformer\u4e2dmatch-and-copy\u7535\u8def\u7684\u5f62\u6210\u673a\u5236\u548c\u4f18\u5316\u52a8\u6001\u3002", "motivation": "\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u4e2dmatch-and-copy\u68c0\u7d22\u539f\u8bed\u5982\u4f55\u5728\u81ea\u7136\u6570\u636e\u4e0a\u6d8c\u73b0\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u68c0\u7d22\u548c\u8bb0\u5fc6\u662f\u7ea0\u7f20\u5728\u4e00\u8d77\u7684\u3002\u9700\u8981\u5206\u79bb\u8fd9\u4e24\u79cd\u673a\u5236\u6765\u6df1\u5165\u7814\u7a76\u68c0\u7d22\u80fd\u529b\u3002", "method": "\u5f15\u5165Gaussian Match-and-Copy\u57fa\u51c6\uff0c\u4f7f\u7528\u7eaf\u4e8c\u9636\u76f8\u5173\u4fe1\u53f7\u9694\u79bb\u957f\u7a0b\u68c0\u7d22\uff1b\u5206\u6790Transformer\u5728\u8be5\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff1b\u5728\u7b80\u5316\u6ce8\u610f\u529b\u8bbe\u7f6e\u4e2d\u7814\u7a76\u4f18\u5316\u52a8\u6001\uff0c\u7279\u522b\u662f\u68af\u5ea6\u4e0b\u964d\u7684\u9690\u5f0f\u504f\u7f6e\u3002", "result": "GMC\u57fa\u51c6\u4fdd\u7559\u4e86Transformer\u5b9e\u9645\u5f00\u53d1match-and-copy\u7535\u8def\u7684\u5173\u952e\u5b9a\u6027\u65b9\u9762\uff0c\u5e76\u80fd\u533a\u5206\u4e0d\u540c\u67b6\u6784\u7684\u68c0\u7d22\u80fd\u529b\u3002\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u68af\u5ea6\u4e0b\u964d\u9a71\u52a8\u53c2\u6570\u53d1\u6563\u540c\u65f6\u65b9\u5411\u4e0e\u6700\u5927\u95f4\u9694\u5206\u79bb\u5668\u5bf9\u9f50\uff0c\u4ea7\u751f\u786c\u5339\u914d\u9009\u62e9\u3002", "conclusion": "\u901a\u8fc7GMC\u57fa\u51c6\u53ef\u4ee5\u5206\u79bb\u68c0\u7d22\u548c\u8bb0\u5fc6\uff0c\u6df1\u5165\u7406\u89e3match-and-copy\u7535\u8def\u7684\u6d8c\u73b0\u673a\u5236\u3002\u68af\u5ea6\u4e0b\u964d\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u6700\u5927\u95f4\u9694\u5bf9\u9f50\u7684\u9690\u5f0f\u504f\u7f6e\uff0c\u89e3\u91ca\u4e86\u786c\u5339\u914d\u9009\u62e9\u884c\u4e3a\u7684\u5f62\u6210\u3002"}}
{"id": "2602.07579", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07579", "abs": "https://arxiv.org/abs/2602.07579", "authors": ["Javidan Abdullayev", "Maxime Devanne", "Cyril Meyer", "Ali Ismail-Fawaz", "Jonathan Weber", "Germain Forestier"], "title": "Enhancing Time Series Classification with Diversity-Driven Neural Network Ensembles", "comment": "Published in IEEE IJCNN 2025 proceedings. 10 pages, 8 figures", "summary": "Ensemble methods have played a crucial role in achieving state-of-the-art (SOTA) performance across various machine learning tasks by leveraging the diversity of features learned by individual models. In Time Series Classification (TSC), ensembles have proven highly effective whether based on neural networks (NNs) or traditional methods like HIVE-COTE. However most existing NN-based ensemble methods for TSC train multiple models with identical architectures and configurations. These ensembles aggregate predictions without explicitly promoting diversity which often leads to redundant feature representations and limits the benefits of ensembling. In this work, we introduce a diversity-driven ensemble learning framework that explicitly encourages feature diversity among neural network ensemble members. Our approach employs a decorrelated learning strategy using a feature orthogonality loss applied directly to the learned feature representations. This ensures that each model in the ensemble captures complementary rather than redundant information. We evaluate our framework on 128 datasets from the UCR archive and show that it achieves SOTA performance with fewer models. This makes our method both efficient and scalable compared to conventional NN-based ensemble approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u6b63\u4ea4\u6027\u635f\u5931\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u591a\u6837\u6027\u9a71\u52a8\u96c6\u6210\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4fc3\u8fdb\u795e\u7ecf\u7f51\u7edc\u96c6\u6210\u6210\u5458\u7684\u7279\u5f81\u591a\u6837\u6027\u6765\u63d0\u5347\u6027\u80fd", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u7684\u795e\u7ecf\u7f51\u7edc\u96c6\u6210\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u76f8\u540c\u67b6\u6784\u548c\u914d\u7f6e\u8bad\u7ec3\u591a\u4e2a\u6a21\u578b\uff0c\u5bfc\u81f4\u7279\u5f81\u8868\u793a\u5197\u4f59\uff0c\u9650\u5236\u4e86\u96c6\u6210\u6548\u679c\u3002\u9700\u8981\u663e\u5f0f\u4fc3\u8fdb\u96c6\u6210\u6210\u5458\u95f4\u7684\u7279\u5f81\u591a\u6837\u6027\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7279\u5f81\u6b63\u4ea4\u6027\u635f\u5931\u7684\u53bb\u76f8\u5173\u5b66\u4e60\u7b56\u7565\uff0c\u76f4\u63a5\u5e94\u7528\u4e8e\u5b66\u4e60\u5230\u7684\u7279\u5f81\u8868\u793a\uff0c\u786e\u4fdd\u96c6\u6210\u4e2d\u7684\u6bcf\u4e2a\u6a21\u578b\u6355\u83b7\u4e92\u8865\u800c\u975e\u5197\u4f59\u7684\u4fe1\u606f\u3002", "result": "\u5728UCR\u6863\u6848\u7684128\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e14\u4f7f\u7528\u66f4\u5c11\u7684\u6a21\u578b\uff0c\u76f8\u6bd4\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u96c6\u6210\u65b9\u6cd5\u66f4\u9ad8\u6548\u548c\u53ef\u6269\u5c55\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u6837\u6027\u9a71\u52a8\u96c6\u6210\u5b66\u4e60\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u4fc3\u8fdb\u7279\u5f81\u591a\u6837\u6027\uff0c\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2602.07588", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07588", "abs": "https://arxiv.org/abs/2602.07588", "authors": ["Ziyang Yu", "Wenbing Huang", "Yang Liu"], "title": "Unified Biomolecular Trajectory Generation via Pretrained Variational Bridge", "comment": "The Fourteenth International Conference on Learning Representations (ICLR 2026)", "summary": "Molecular Dynamics (MD) simulations provide a fundamental tool for characterizing molecular behavior at full atomic resolution, but their applicability is severely constrained by the computational cost. To address this, a surge of deep generative models has recently emerged to learn dynamics at coarsened timesteps for efficient trajectory generation, yet they either generalize poorly across systems or, due to limited molecular diversity of trajectory data, fail to fully exploit structural information to improve generative fidelity. Here, we present the Pretrained Variational Bridge (PVB) in an encoder-decoder fashion, which maps the initial structure into a noised latent space and transports it toward stage-specific targets through augmented bridge matching. This unifies training on both single-structure and paired trajectory data, enabling consistent use of cross-domain structural knowledge across training stages. Moreover, for protein-ligand complexes, we further introduce a reinforcement learning-based optimization via adjoint matching that speeds progression toward the holo state, which supports efficient post-optimization of docking poses. Experiments on proteins and protein-ligand complexes demonstrate that PVB faithfully reproduces thermodynamic and kinetic observables from MD while delivering stable and efficient generative dynamics.", "AI": {"tldr": "PVB\u662f\u4e00\u79cd\u9884\u8bad\u7ec3\u53d8\u5206\u6865\u6a21\u578b\uff0c\u901a\u8fc7\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u5c06\u521d\u59cb\u7ed3\u6784\u6620\u5c04\u5230\u566a\u58f0\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u5229\u7528\u589e\u5f3a\u6865\u5339\u914d\u6280\u672f\u751f\u6210\u5206\u5b50\u52a8\u529b\u5b66\u8f68\u8ff9\uff0c\u5728\u4fdd\u6301\u70ed\u529b\u5b66\u548c\u52a8\u529b\u5b66\u7279\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u8981\u4e48\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u8981\u4e48\u56e0\u8f68\u8ff9\u6570\u636e\u591a\u6837\u6027\u6709\u9650\u800c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u7ed3\u6784\u4fe1\u606f\u6765\u63d0\u9ad8\u751f\u6210\u4fdd\u771f\u5ea6\u3002", "method": "\u63d0\u51fa\u9884\u8bad\u7ec3\u53d8\u5206\u6865(PVB)\u6a21\u578b\uff0c\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff1a\u5c06\u521d\u59cb\u7ed3\u6784\u6620\u5c04\u5230\u566a\u58f0\u6f5c\u5728\u7a7a\u95f4\uff0c\u901a\u8fc7\u589e\u5f3a\u6865\u5339\u914d\u6280\u672f\u5411\u9636\u6bb5\u7279\u5b9a\u76ee\u6807\u4f20\u8f93\uff1b\u7edf\u4e00\u8bad\u7ec3\u5355\u7ed3\u6784\u548c\u914d\u5bf9\u8f68\u8ff9\u6570\u636e\uff1b\u9488\u5bf9\u86cb\u767d\u8d28-\u914d\u4f53\u590d\u5408\u7269\u5f15\u5165\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4f34\u968f\u5339\u914d\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePVB\u80fd\u591f\u5fe0\u5b9e\u590d\u73b0\u5206\u5b50\u52a8\u529b\u5b66\u7684\u70ed\u529b\u5b66\u548c\u52a8\u529b\u5b66\u53ef\u89c2\u6d4b\u91cf\uff0c\u540c\u65f6\u63d0\u4f9b\u7a33\u5b9a\u9ad8\u6548\u7684\u751f\u6210\u52a8\u529b\u5b66\uff1b\u5bf9\u4e8e\u86cb\u767d\u8d28-\u914d\u4f53\u590d\u5408\u7269\uff0c\u652f\u6301\u5bf9\u63a5\u6784\u8c61\u7684\u9ad8\u6548\u540e\u4f18\u5316\u3002", "conclusion": "PVB\u901a\u8fc7\u7edf\u4e00\u8bad\u7ec3\u6846\u67b6\u6709\u6548\u5229\u7528\u8de8\u57df\u7ed3\u6784\u77e5\u8bc6\uff0c\u5728\u4fdd\u6301\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u86cb\u767d\u8d28-\u914d\u4f53\u590d\u5408\u7269\u5bf9\u63a5\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2602.07593", "categories": ["cs.LG", "cs.GT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07593", "abs": "https://arxiv.org/abs/2602.07593", "authors": ["Polina Gordienko", "Christoph Jansen", "Julian Rodemann", "Georg Schollmeyer"], "title": "Beyond Arrow: From Impossibility to Possibilities in Multi-Criteria Benchmarking", "comment": null, "summary": "Modern benchmarks such as HELM MMLU account for multiple metrics like accuracy, robustness and efficiency. When trying to turn these metrics into a single ranking, natural aggregation procedures can become incoherent or unstable to changes in the model set. We formalize this aggregation as a social choice problem where each metric induces a preference ranking over models on each dataset, and a benchmark operator aggregates these votes across metrics. While prior work has focused on Arrow's impossibility result, we argue that the impossibility often originates from pathological examples and identify sufficient conditions under which these disappear, and meaningful multi-criteria benchmarking becomes possible. In particular, we deal with three restrictions on the combinations of rankings and prove that on single-peaked, group-separable and distance-restricted preferences, the benchmark operator allows for the construction of well-behaved rankings of the involved models. Empirically, we investigate several modern benchmark suites like HELM MMLU and verify which structural conditions are fulfilled on which benchmark problems.", "AI": {"tldr": "\u8bba\u6587\u5c06\u591a\u6307\u6807\u57fa\u51c6\u6d4b\u8bd5\u5efa\u6a21\u4e3a\u793e\u4f1a\u9009\u62e9\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u504f\u597d\u7ed3\u6784\u4e0b\u5355\u5cf0\u6027\u3001\u7fa4\u53ef\u5206\u6027\u548c\u8ddd\u79bb\u9650\u5236\u6761\u4ef6\u4e0b\u53ef\u4ee5\u6784\u5efa\u826f\u597d\u6a21\u578b\u6392\u540d", "motivation": "\u73b0\u4ee3\u57fa\u51c6\u6d4b\u8bd5\u5982HELM MMLU\u5305\u542b\u591a\u4e2a\u6307\u6807\uff08\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u3001\u6548\u7387\u7b49\uff09\uff0c\u4f46\u5c06\u8fd9\u4e9b\u6307\u6807\u805a\u5408\u6210\u5355\u4e00\u6392\u540d\u65f6\uff0c\u81ea\u7136\u805a\u5408\u65b9\u6cd5\u53ef\u80fd\u53d8\u5f97\u4e0d\u4e00\u81f4\u6216\u4e0d\u7a33\u5b9a\u3002\u9700\u8981\u89e3\u51b3\u591a\u6307\u6807\u57fa\u51c6\u6d4b\u8bd5\u7684\u6392\u540d\u805a\u5408\u95ee\u9898\u3002", "method": "\u5c06\u57fa\u51c6\u6d4b\u8bd5\u5efa\u6a21\u4e3a\u793e\u4f1a\u9009\u62e9\u95ee\u9898\uff1a\u6bcf\u4e2a\u6307\u6807\u5728\u6570\u636e\u96c6\u4e0a\u8bf1\u5bfc\u51fa\u6a21\u578b\u504f\u597d\u6392\u5e8f\uff0c\u57fa\u51c6\u7b97\u5b50\u805a\u5408\u8fd9\u4e9b\"\u6295\u7968\"\u3002\u7814\u7a76\u4e09\u79cd\u504f\u597d\u7ed3\u6784\u9650\u5236\u6761\u4ef6\uff1a\u5355\u5cf0\u504f\u597d\u3001\u7fa4\u53ef\u5206\u504f\u597d\u548c\u8ddd\u79bb\u9650\u5236\u504f\u597d\uff0c\u8bc1\u660e\u5728\u8fd9\u4e9b\u6761\u4ef6\u4e0b\u53ef\u4ee5\u6784\u5efa\u826f\u597d\u6a21\u578b\u6392\u540d\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5728\u5355\u5cf0\u6027\u3001\u7fa4\u53ef\u5206\u6027\u548c\u8ddd\u79bb\u9650\u5236\u6761\u4ef6\u4e0b\uff0c\u57fa\u51c6\u7b97\u5b50\u5141\u8bb8\u6784\u5efa\u884c\u4e3a\u826f\u597d\u7684\u6a21\u578b\u6392\u540d\u3002\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u4e86HELM MMLU\u7b49\u73b0\u4ee3\u57fa\u51c6\u5957\u4ef6\u4e2d\u54ea\u4e9b\u7ed3\u6784\u6761\u4ef6\u5728\u54ea\u4e9b\u57fa\u51c6\u95ee\u9898\u4e0a\u5f97\u5230\u6ee1\u8db3\u3002", "conclusion": "\u591a\u6307\u6807\u57fa\u51c6\u6d4b\u8bd5\u7684\u6392\u540d\u805a\u5408\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7\u793e\u4f1a\u9009\u62e9\u7406\u8bba\u89e3\u51b3\uff0c\u5f53\u504f\u597d\u6ee1\u8db3\u7279\u5b9a\u7ed3\u6784\u6761\u4ef6\u65f6\uff0c\u53ef\u4ee5\u907f\u514dArrow\u4e0d\u53ef\u80fd\u5b9a\u7406\u7684\u75c5\u7406\u60c5\u51b5\uff0c\u5b9e\u73b0\u6709\u610f\u4e49\u7684\u591a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2602.07596", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07596", "abs": "https://arxiv.org/abs/2602.07596", "authors": ["Xi Chen", "Ming Li", "Junxi Li", "Changsheng Li", "Peisong Wang", "Lizhong Ding", "Ye Yuan", "Guoren Wang"], "title": "Astro: Activation-guided Structured Regularization for Outlier-Robust LLM Post-Training Quantization", "comment": null, "summary": "Weight-only post-training quantization (PTQ) is crucial for efficient Large Language Model (LLM) deployment but suffers from accuracy degradation caused by weight and activation outliers. Existing mitigation strategies often face critical limitations: they either yield insufficient outlier suppression or incur significant deployment inefficiencies, such as inference latency, heavy preprocessing, or reliance on complex operator fusion. To resolve these limitations, we leverage a key insight: over-parameterized LLMs often converge to Flat Minima, implying a vast equivalent solution space where weights can be adjusted without compromising accuracy. Building on this, we propose Astro, an Activation-guided Structured Regularization framework designed to suppress the negative effects of outliers in a hardware-friendly and efficient manner. Leveraging the activation-guided regularization objective, Astro actively reconstructs intrinsically robust weights, aggressively suppressing weight outliers corresponding to high-magnitude activations without sacrificing model accuracy. Crucially, Astro introduces zero inference latency and is orthogonal to mainstream quantization methods like GPTQ. Extensive experiments show that Astro achieves highly competitive performance; notably, on LLaMA-2-7B, it achieves better performance than complex learning-based rotation methods with almost 1/3 of the quantization time.", "AI": {"tldr": "Astro\u662f\u4e00\u4e2a\u6fc0\u6d3b\u5f15\u5bfc\u7684\u7ed3\u6784\u5316\u6b63\u5219\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u6291\u5236LLM\u91cf\u5316\u4e2d\u7684\u5f02\u5e38\u503c\uff0c\u5b9e\u73b0\u96f6\u63a8\u7406\u5ef6\u8fdf\u7684\u9ad8\u6548\u6743\u91cd\u540e\u8bad\u7ec3\u91cf\u5316", "motivation": "\u73b0\u6709\u7684\u6743\u91cd\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u5728\u5904\u7406\u6743\u91cd\u548c\u6fc0\u6d3b\u5f02\u5e38\u503c\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff1a\u8981\u4e48\u6291\u5236\u6548\u679c\u4e0d\u8db3\uff0c\u8981\u4e48\u5bfc\u81f4\u663e\u8457\u7684\u90e8\u7f72\u6548\u7387\u95ee\u9898\uff08\u5982\u63a8\u7406\u5ef6\u8fdf\u3001\u7e41\u91cd\u7684\u9884\u5904\u7406\u6216\u590d\u6742\u7684\u7b97\u5b50\u878d\u5408\uff09", "method": "\u5229\u7528LLM\u6536\u655b\u5230\u5e73\u5766\u6700\u5c0f\u503c\u7684\u7279\u70b9\uff0c\u63d0\u51faAstro\u6846\u67b6\uff0c\u901a\u8fc7\u6fc0\u6d3b\u5f15\u5bfc\u7684\u6b63\u5219\u5316\u76ee\u6807\u4e3b\u52a8\u91cd\u6784\u5185\u5728\u9c81\u68d2\u7684\u6743\u91cd\uff0c\u6291\u5236\u4e0e\u9ad8\u5e45\u5ea6\u6fc0\u6d3b\u5bf9\u5e94\u7684\u6743\u91cd\u5f02\u5e38\u503c\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6", "result": "\u5728LLaMA-2-7B\u4e0a\uff0cAstro\u5b9e\u73b0\u4e86\u6bd4\u590d\u6742\u5b66\u4e60\u578b\u65cb\u8f6c\u65b9\u6cd5\u66f4\u597d\u7684\u6027\u80fd\uff0c\u4e14\u91cf\u5316\u65f6\u95f4\u4ec5\u4e3a\u540e\u8005\u7684\u7ea61/3\uff0c\u540c\u65f6\u4fdd\u6301\u96f6\u63a8\u7406\u5ef6\u8fdf", "conclusion": "Astro\u63d0\u4f9b\u4e86\u4e00\u79cd\u786c\u4ef6\u53cb\u597d\u4e14\u9ad8\u6548\u7684\u5f02\u5e38\u503c\u6291\u5236\u65b9\u6848\uff0c\u4e0e\u4e3b\u6d41\u91cf\u5316\u65b9\u6cd5\u6b63\u4ea4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6743\u91cd\u540e\u8bad\u7ec3\u91cf\u5316\u7684\u6027\u80fd\u4e0e\u6548\u7387"}}
{"id": "2602.07599", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07599", "abs": "https://arxiv.org/abs/2602.07599", "authors": ["Mehryar Mohri"], "title": "Rational Transductors", "comment": null, "summary": "Standard Transformers excel at semantic modeling but struggle with\n  rigid sequential logic and state tracking. Theoretical work\n  establishes that self-attention is limited to $\\AC^0$ (under hard\n  attention) or $\\TC^0$ (under soft attention), complexity classes\n  that often fail to support robust length generalization on\n  sequential problems without intermediate chain-of-thought. In this\n  work, we introduce \\emph{Rational Transductors}, a dual-stream\n  architecture that augments the Transformer with a matrix-valued\n  recurrence derived from Weighted Finite Automata (WFA). By\n  injecting rational state information into the attention mechanism\n  via a \\emph{Deep Rational Injection} scheme, our framework strictly\n  generalizes the expressive power of Transformers to capture all\n  Regular Languages, $\\NC^1$-complete problems (such as Boolean\n  Formula Evaluation), and fundamental separations like Parity and\n  Modular Counting, while preserving $O(L + \\log T)$ parallel time\n  complexity. We ground the architecture in a rigorous learning\n  theory: we prove that \\emph{Random Rational Features} act as a\n  universal basis for sequential dependencies, justifying our\n  initialization strategy, while establishing that the\n  \\emph{Differentiable Rational Feature} regime is necessary to close\n  the representational compactness gap. Theoretical analysis and\n  empirical results demonstrate that Rational Transductors solve the\n  \"Regular Gap,\" enabling robust length generalization on algorithmic\n  tasks where standard Transformers fail, without the sequential\n  computational bottlenecks of traditional RNNs.", "AI": {"tldr": "Rational Transductors\uff1a\u4e00\u79cd\u53cc\u6d41\u67b6\u6784\uff0c\u901a\u8fc7\u52a0\u6743\u6709\u9650\u81ea\u52a8\u673a\uff08WFA\uff09\u7684\u77e9\u9635\u503c\u9012\u5f52\u589e\u5f3aTransformer\uff0c\u4f7f\u5176\u80fd\u591f\u6355\u83b7\u6240\u6709\u6b63\u5219\u8bed\u8a00\u548cNC\u00b9\u5b8c\u5168\u95ee\u9898\uff0c\u89e3\u51b3Transformer\u5728\u5e8f\u5217\u903b\u8f91\u548c\u72b6\u6001\u8ddf\u8e2a\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6807\u51c6Transformer\u64c5\u957f\u8bed\u4e49\u5efa\u6a21\uff0c\u4f46\u5728\u4e25\u683c\u7684\u5e8f\u5217\u903b\u8f91\u548c\u72b6\u6001\u8ddf\u8e2a\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002\u7406\u8bba\u7814\u7a76\u8868\u660e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u590d\u6742\u7c7b\u4e0a\u53d7\u9650\uff08AC\u2070\u6216TC\u2070\uff09\uff0c\u96be\u4ee5\u5728\u6ca1\u6709\u4e2d\u95f4\u601d\u7ef4\u94fe\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9c81\u68d2\u7684\u957f\u5ea6\u6cdb\u5316\u3002", "method": "\u63d0\u51faRational Transductors\u53cc\u6d41\u67b6\u6784\uff0c\u901a\u8fc7\u52a0\u6743\u6709\u9650\u81ea\u52a8\u673a\uff08WFA\uff09\u7684\u77e9\u9635\u503c\u9012\u5f52\u589e\u5f3aTransformer\u3002\u91c7\u7528\u6df1\u5ea6\u6709\u7406\u6ce8\u5165\u65b9\u6848\u5c06\u6709\u7406\u72b6\u6001\u4fe1\u606f\u6ce8\u5165\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u968f\u673a\u6709\u7406\u7279\u5f81\u4f5c\u4e3a\u5e8f\u5217\u4f9d\u8d56\u7684\u901a\u7528\u57fa\uff0c\u5e76\u4f7f\u7528\u53ef\u5fae\u5206\u6709\u7406\u7279\u5f81\u673a\u5236\u6765\u5f25\u8865\u8868\u793a\u7d27\u51d1\u6027\u5dee\u8ddd\u3002", "result": "\u8be5\u6846\u67b6\u4e25\u683c\u6cdb\u5316\u4e86Transformer\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u80fd\u591f\u6355\u83b7\u6240\u6709\u6b63\u5219\u8bed\u8a00\u3001NC\u00b9\u5b8c\u5168\u95ee\u9898\uff08\u5982\u5e03\u5c14\u516c\u5f0f\u6c42\u503c\uff09\u4ee5\u53ca\u5947\u5076\u6027\u548c\u6a21\u8ba1\u6570\u7b49\u57fa\u672c\u5206\u79bb\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301O(L + log T)\u7684\u5e76\u884c\u65f6\u95f4\u590d\u6742\u5ea6\u3002\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cRational Transductors\u89e3\u51b3\u4e86\"\u6b63\u5219\u5dee\u8ddd\"\uff0c\u5728\u6807\u51c6Transformer\u5931\u8d25\u7684\u7b97\u6cd5\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u957f\u5ea6\u6cdb\u5316\u3002", "conclusion": "Rational Transductors\u901a\u8fc7\u7ed3\u5408Transformer\u7684\u8bed\u4e49\u5efa\u6a21\u80fd\u529b\u548cWFA\u7684\u5e8f\u5217\u5904\u7406\u80fd\u529b\uff0c\u514b\u670d\u4e86\u6807\u51c6Transformer\u5728\u5e8f\u5217\u903b\u8f91\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u957f\u5ea6\u6cdb\u5316\u6027\u80fd\uff0c\u540c\u65f6\u907f\u514d\u4e86\u4f20\u7edfRNN\u7684\u987a\u5e8f\u8ba1\u7b97\u74f6\u9888\u3002"}}
{"id": "2602.07602", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07602", "abs": "https://arxiv.org/abs/2602.07602", "authors": ["Gabriel Stella", "Dmitri Loguinov"], "title": "Object-Oriented Transition Modeling with Inductive Logic Programming", "comment": "46 pages, 26 figures", "summary": "Building models of the world from observation, i.e., induction, is one of the major challenges in machine learning. In order to be useful, models need to maintain accuracy when used in novel situations, i.e., generalize. In addition, they should be easy to interpret and efficient to train. Prior work has investigated these concepts in the context of object-oriented representations inspired by human cognition. In this paper, we develop a novel learning algorithm that is substantially more powerful than these previous methods. Our thorough experiments, including ablation tests and comparison with neural baselines, demonstrate a significant improvement over the state-of-the-art.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u7b97\u6cd5\uff0c\u5728\u9762\u5411\u5bf9\u8c61\u7684\u8868\u793a\u57fa\u7840\u4e0a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f52\u7eb3\u5efa\u6a21\u80fd\u529b\uff0c\u5728\u6cdb\u5316\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4ece\u89c2\u5bdf\u4e2d\u6784\u5efa\u4e16\u754c\u6a21\u578b\uff08\u5f52\u7eb3\uff09\u662f\u673a\u5668\u5b66\u4e60\u7684\u4e3b\u8981\u6311\u6218\u4e4b\u4e00\u3002\u6709\u7528\u7684\u6a21\u578b\u9700\u8981\u5728\u65b0\u7684\u60c5\u5883\u4e2d\u4fdd\u6301\u51c6\u786e\u6027\uff08\u6cdb\u5316\uff09\uff0c\u540c\u65f6\u6613\u4e8e\u89e3\u91ca\u4e14\u8bad\u7ec3\u9ad8\u6548\u3002\u5148\u524d\u5de5\u4f5c\u63a2\u7d22\u4e86\u53d7\u4eba\u7c7b\u8ba4\u77e5\u542f\u53d1\u7684\u9762\u5411\u5bf9\u8c61\u8868\u793a\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u80fd\u529b\u6709\u9650\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5b66\u4e60\u7b97\u6cd5\uff0c\u6bd4\u5148\u524d\u65b9\u6cd5\u66f4\u5f3a\u5927\u3002\u8be5\u7b97\u6cd5\u57fa\u4e8e\u9762\u5411\u5bf9\u8c61\u7684\u8868\u793a\uff0c\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\uff08\u5305\u62ec\u6d88\u878d\u6d4b\u8bd5\u548c\u4e0e\u795e\u7ecf\u57fa\u7ebf\u7684\u6bd4\u8f83\uff09\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5f52\u7eb3\u5efa\u6a21\u4efb\u52a1\u4e0a\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\uff0c\u5728\u6cdb\u5316\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b0\u5b66\u4e60\u7b97\u6cd5\u5728\u9762\u5411\u5bf9\u8c61\u8868\u793a\u7684\u5f52\u7eb3\u5efa\u6a21\u4e2d\u53d6\u5f97\u4e86\u7a81\u7834\u6027\u8fdb\u5c55\uff0c\u4e3a\u6784\u5efa\u66f4\u5f3a\u5927\u3001\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07603", "categories": ["cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.07603", "abs": "https://arxiv.org/abs/2602.07603", "authors": ["Woojin Cho", "Junghwan Park"], "title": "Escaping Spectral Bias without Backpropagation: Fast Implicit Neural Representations with Extreme Learning Machines", "comment": null, "summary": "Training implicit neural representations (INRs) to capture fine-scale details typically relies on iterative backpropagation and is often hindered by spectral bias when the target exhibits highly non-uniform frequency content. We propose ELM-INR, a backpropagation-free INR that decomposes the domain into overlapping subdomains and fits each local problem using an Extreme Learning Machine (ELM) in closed form, replacing iterative optimization with stable linear least-squares solutions. This design yields fast and numerically robust reconstruction by combining local predictors through a partition of unity. To understand where approximation becomes difficult under fixed local capacity, we analyze the method from a spectral Barron norm perspective, which reveals that global reconstruction error is dominated by regions with high spectral complexity. Building on this insight, we introduce BEAM, an adaptive mesh refinement strategy that balances spectral complexity across subdomains to improve reconstruction quality in capacity-constrained regimes.", "AI": {"tldr": "ELM-INR\uff1a\u4e00\u79cd\u514d\u53cd\u5411\u4f20\u64ad\u7684\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\uff0c\u4f7f\u7528\u6781\u7aef\u5b66\u4e60\u673a\uff08ELM\uff09\u8fdb\u884c\u95ed\u5f0f\u6c42\u89e3\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u7f51\u683c\u7ec6\u5316\u7b56\u7565BEAM\u6765\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edfINR\u8bad\u7ec3\u4f9d\u8d56\u8fed\u4ee3\u53cd\u5411\u4f20\u64ad\uff0c\u5b58\u5728\u9891\u8c31\u504f\u5dee\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u9ad8\u5ea6\u975e\u5747\u5300\u9891\u7387\u5185\u5bb9\u65f6\u6548\u679c\u4e0d\u4f73\u3002\u9700\u8981\u66f4\u5feb\u901f\u3001\u6570\u503c\u7a33\u5b9a\u7684\u91cd\u5efa\u65b9\u6cd5\u3002", "method": "\u5c06\u57df\u5206\u89e3\u4e3a\u91cd\u53e0\u5b50\u57df\uff0c\u5728\u6bcf\u4e2a\u5c40\u90e8\u95ee\u9898\u4e0a\u4f7f\u7528\u6781\u7aef\u5b66\u4e60\u673a\uff08ELM\uff09\u8fdb\u884c\u95ed\u5f0f\u6c42\u89e3\uff0c\u907f\u514d\u8fed\u4ee3\u4f18\u5316\u3002\u5f15\u5165BEAM\u81ea\u9002\u5e94\u7f51\u683c\u7ec6\u5316\u7b56\u7565\uff0c\u6839\u636e\u9891\u8c31\u590d\u6742\u5ea6\u5e73\u8861\u5b50\u57df\u5206\u914d\u3002", "result": "ELM-INR\u5b9e\u73b0\u4e86\u5feb\u901f\u4e14\u6570\u503c\u7a33\u5b9a\u7684\u91cd\u5efa\uff0c\u901a\u8fc7\u8c31Barron\u8303\u6570\u5206\u6790\u53d1\u73b0\u5168\u5c40\u91cd\u5efa\u8bef\u5dee\u4e3b\u8981\u7531\u9ad8\u9891\u8c31\u590d\u6742\u5ea6\u533a\u57df\u4e3b\u5bfc\uff0cBEAM\u7b56\u7565\u5728\u5bb9\u91cf\u53d7\u9650\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "ELM-INR\u63d0\u4f9b\u4e86\u4e00\u79cd\u514d\u53cd\u5411\u4f20\u64ad\u7684INR\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u7f51\u683c\u7ec6\u5316\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u975e\u5747\u5300\u9891\u7387\u5185\u5bb9\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u91cd\u5efa\u8d28\u91cf\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2602.07616", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07616", "abs": "https://arxiv.org/abs/2602.07616", "authors": ["Juntong Wu", "Jialiang Cheng", "Fuyu Lv", "Ou Dan", "Li Yuan"], "title": "SERE: Similarity-based Expert Re-routing for Efficient Batch Decoding in MoE Models", "comment": "Published as a conference paper at ICLR 2026", "summary": "Mixture-of-Experts (MoE) architectures employ sparse activation to deliver faster training and inference with higher accuracy than dense LLMs. However, in production serving, MoE models require batch inference to optimize hardware efficiency, which may cause excessive expert activation and thus slow the memory-bound decoding stage. To address the fundamental tension between batch decoding and expert sparsity, we present SERE, a Similarity-based Expert Re-routing method for Efficient batch decoding in MoE models. SERE dynamically reduces the number of active experts in an input-aware manner by re-routing tokens from secondary experts to their most similar primary counterparts. It also leverages similarity patterns to identify and preserve critical experts, thereby preventing capability loss. Notably, SERE avoids static expert pruning or merging, instead enabling dynamic expert skipping based on batch-level expert redundancy. Additionally, we provide an efficient custom CUDA kernel for SERE, enabling plug-and-play use in vLLM with only a single-line code change. Extensive experiments on various complex reasoning benchmarks demonstrate that SERE achieves up to 2.0x speedup with minimal quality loss, providing a practical solution for cost-efficient and latency-sensitive large-scale MoE deployment. Code implementation of SERE can be found in https://github.com/JL-Cheng/SERE.", "AI": {"tldr": "SERE\u662f\u4e00\u79cd\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u4e13\u5bb6\u91cd\u8def\u7531\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347MoE\u6a21\u578b\u6279\u91cf\u89e3\u7801\u6548\u7387\uff0c\u901a\u8fc7\u52a8\u6001\u51cf\u5c11\u6d3b\u8dc3\u4e13\u5bb6\u6570\u91cf\uff0c\u5b9e\u73b0\u6700\u9ad82\u500d\u52a0\u901f\u4e14\u8d28\u91cf\u635f\u5931\u6700\u5c0f\u3002", "motivation": "MoE\u6a21\u578b\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u9700\u8981\u6279\u91cf\u63a8\u7406\u4ee5\u4f18\u5316\u786c\u4ef6\u6548\u7387\uff0c\u4f46\u8fd9\u4f1a\u5bfc\u81f4\u4e13\u5bb6\u8fc7\u5ea6\u6fc0\u6d3b\uff0c\u4ece\u800c\u51cf\u6162\u5185\u5b58\u53d7\u9650\u7684\u89e3\u7801\u9636\u6bb5\u3002\u5b58\u5728\u6279\u91cf\u89e3\u7801\u4e0e\u4e13\u5bb6\u7a00\u758f\u6027\u4e4b\u95f4\u7684\u6839\u672c\u77db\u76fe\u3002", "method": "SERE\u901a\u8fc7\u76f8\u4f3c\u6027\u5206\u6790\u52a8\u6001\u51cf\u5c11\u6d3b\u8dc3\u4e13\u5bb6\u6570\u91cf\uff1a1\uff09\u5c06\u6b21\u8981\u4e13\u5bb6\u7684token\u91cd\u8def\u7531\u5230\u6700\u76f8\u4f3c\u7684\u4e3b\u8981\u4e13\u5bb6\uff1b2\uff09\u5229\u7528\u76f8\u4f3c\u6027\u6a21\u5f0f\u8bc6\u522b\u5e76\u4fdd\u7559\u5173\u952e\u4e13\u5bb6\uff1b3\uff09\u907f\u514d\u9759\u6001\u4e13\u5bb6\u526a\u679d\u6216\u5408\u5e76\uff0c\u5b9e\u73b0\u57fa\u4e8e\u6279\u91cf\u7ea7\u4e13\u5bb6\u5197\u4f59\u7684\u52a8\u6001\u4e13\u5bb6\u8df3\u8fc7\uff1b4\uff09\u63d0\u4f9b\u9ad8\u6548\u81ea\u5b9a\u4e49CUDA\u5185\u6838\uff0c\u53ef\u5728vLLM\u4e2d\u5373\u63d2\u5373\u7528\u3002", "result": "\u5728\u5404\u79cd\u590d\u6742\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSERE\u5b9e\u73b0\u4e86\u6700\u9ad82.0\u500d\u7684\u52a0\u901f\uff0c\u4e14\u8d28\u91cf\u635f\u5931\u6700\u5c0f\uff0c\u4e3a\u5927\u89c4\u6a21MoE\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6210\u672c\u6548\u76ca\u548c\u4f4e\u5ef6\u8fdf\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "SERE\u901a\u8fc7\u52a8\u6001\u4e13\u5bb6\u91cd\u8def\u7531\u6709\u6548\u89e3\u51b3\u4e86MoE\u6a21\u578b\u6279\u91cf\u89e3\u7801\u6548\u7387\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\uff0c\u4e3a\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u5927\u89c4\u6a21MoE\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07618", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07618", "abs": "https://arxiv.org/abs/2602.07618", "authors": ["Levi Rauchwerger", "Stefanie Jegelka", "Ron Levie"], "title": "Dense Neural Networks are not Universal Approximators", "comment": null, "summary": "We investigate the approximation capabilities of dense neural networks. While universal approximation theorems establish that sufficiently large architectures can approximate arbitrary continuous functions if there are no restrictions on the weight values, we show that dense neural networks do not possess this universality. Our argument is based on a model compression approach, combining the weak regularity lemma with an interpretation of feedforward networks as message passing graph neural networks. We consider ReLU neural networks subject to natural constraints on weights and input and output dimensions, which model a notion of dense connectivity. Within this setting, we demonstrate the existence of Lipschitz continuous functions that cannot be approximated by such networks. This highlights intrinsic limitations of neural networks with dense layers and motivates the use of sparse connectivity as a necessary ingredient for achieving true universality.", "AI": {"tldr": "\u5bc6\u96c6\u795e\u7ecf\u7f51\u7edc\u5728\u6743\u91cd\u548c\u7ef4\u5ea6\u7ea6\u675f\u4e0b\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u901a\u7528\u903c\u8fd1\uff0c\u5b58\u5728\u67d0\u4e9bLipschitz\u8fde\u7eed\u51fd\u6570\u65e0\u6cd5\u88ab\u903c\u8fd1\uff0c\u7a00\u758f\u8fde\u63a5\u662f\u5b9e\u73b0\u771f\u6b63\u901a\u7528\u903c\u8fd1\u7684\u5fc5\u8981\u6761\u4ef6", "motivation": "\u7814\u7a76\u5bc6\u96c6\u795e\u7ecf\u7f51\u7edc\u7684\u903c\u8fd1\u80fd\u529b\uff0c\u867d\u7136\u901a\u7528\u903c\u8fd1\u5b9a\u7406\u8868\u660e\u8db3\u591f\u5927\u7684\u67b6\u6784\u53ef\u4ee5\u903c\u8fd1\u4efb\u610f\u8fde\u7eed\u51fd\u6570\uff0c\u4f46\u8fd9\u662f\u5728\u6743\u91cd\u65e0\u9650\u5236\u7684\u524d\u63d0\u4e0b\u3002\u672c\u6587\u8981\u63a2\u7a76\u5728\u81ea\u7136\u7ea6\u675f\u4e0b\u5bc6\u96c6\u795e\u7ecf\u7f51\u7edc\u662f\u5426\u4ecd\u5177\u6709\u901a\u7528\u6027", "method": "\u91c7\u7528\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\uff0c\u7ed3\u5408\u5f31\u6b63\u5219\u5f15\u7406\uff0c\u5c06\u524d\u9988\u7f51\u7edc\u89e3\u91ca\u4e3a\u6d88\u606f\u4f20\u9012\u56fe\u795e\u7ecf\u7f51\u7edc\u3002\u8003\u8651ReLU\u795e\u7ecf\u7f51\u7edc\u5728\u6743\u91cd\u548c\u8f93\u5165\u8f93\u51fa\u7ef4\u5ea6\u4e0a\u7684\u81ea\u7136\u7ea6\u675f\uff0c\u8fd9\u4e9b\u7ea6\u675f\u6a21\u62df\u4e86\u5bc6\u96c6\u8fde\u63a5\u7684\u6982\u5ff5", "result": "\u5728\u8bbe\u5b9a\u7684\u7ea6\u675f\u6761\u4ef6\u4e0b\uff0c\u8bc1\u660e\u4e86\u5b58\u5728Lipschitz\u8fde\u7eed\u51fd\u6570\u65e0\u6cd5\u88ab\u6b64\u7c7b\u5bc6\u96c6\u795e\u7ecf\u7f51\u7edc\u903c\u8fd1\u3002\u8fd9\u8868\u660e\u5bc6\u96c6\u5c42\u795e\u7ecf\u7f51\u7edc\u5b58\u5728\u5185\u5728\u5c40\u9650\u6027", "conclusion": "\u5bc6\u96c6\u795e\u7ecf\u7f51\u7edc\u4e0d\u5177\u5907\u771f\u6b63\u7684\u901a\u7528\u903c\u8fd1\u80fd\u529b\uff0c\u7a00\u758f\u8fde\u63a5\u662f\u5b9e\u73b0\u771f\u6b63\u901a\u7528\u903c\u8fd1\u7684\u5fc5\u8981\u6761\u4ef6\u3002\u8fd9\u4e00\u53d1\u73b0\u5f3a\u8c03\u4e86\u7f51\u7edc\u7ed3\u6784\u7a00\u758f\u6027\u7684\u91cd\u8981\u6027"}}
{"id": "2602.07640", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07640", "abs": "https://arxiv.org/abs/2602.07640", "authors": ["Micha\u0142 Kozyra", "Gesine Reinert"], "title": "TASTE: Task-Aware Out-of-Distribution Detection via Stein Operators", "comment": null, "summary": "Out-of-distribution detection methods are often either data-centric, detecting deviations from the training input distribution irrespective of their effect on a trained model, or model-centric, relying on classifier outputs without explicit reference to data geometry. We propose TASTE (Task-Aware STEin operators): a task-aware framework based on so-called Stein operators, which allows us to link distribution shift to the input sensitivity of the model. We show that the resulting operator admits a clear geometric interpretation as a projection of distribution shift onto the sensitivity field of the model, yielding theoretical guarantees. Beyond detecting the presence of a shift, the same construction enables its localisation through a coordinate-wise decomposition, and for image data-provides interpretable per-pixel diagnostics. Experiments on controlled Gaussian shifts, MNIST under geometric perturbations, and CIFAR-10 perturbed benchmarks demonstrate that the proposed method aligns closely with task degradation while outperforming established baselines.", "AI": {"tldr": "\u63d0\u51faTASTE\u6846\u67b6\uff0c\u57fa\u4e8eStein\u7b97\u5b50\u5c06\u5206\u5e03\u504f\u79fb\u4e0e\u6a21\u578b\u8f93\u5165\u654f\u611f\u6027\u8054\u7cfb\u8d77\u6765\uff0c\u5b9e\u73b0\u4efb\u52a1\u611f\u77e5\u7684OOD\u68c0\u6d4b\uff0c\u5e76\u63d0\u4f9b\u504f\u79fb\u5b9a\u4f4d\u548c\u50cf\u7d20\u7ea7\u8bca\u65ad\u3002", "motivation": "\u73b0\u6709OOD\u68c0\u6d4b\u65b9\u6cd5\u8981\u4e48\u662f\u6570\u636e\u4e2d\u5fc3\u7684\uff08\u4e0d\u8003\u8651\u5bf9\u6a21\u578b\u7684\u5f71\u54cd\uff09\uff0c\u8981\u4e48\u662f\u6a21\u578b\u4e2d\u5fc3\u7684\uff08\u4e0d\u8003\u8651\u6570\u636e\u51e0\u4f55\uff09\uff0c\u7f3a\u4e4f\u5c06\u5206\u5e03\u504f\u79fb\u4e0e\u6a21\u578b\u4efb\u52a1\u6027\u80fd\u76f4\u63a5\u5173\u8054\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eStein\u7b97\u5b50\u6784\u5efa\u4efb\u52a1\u611f\u77e5\u6846\u67b6\uff0c\u5c06\u5206\u5e03\u504f\u79fb\u6295\u5f71\u5230\u6a21\u578b\u7684\u654f\u611f\u6027\u573a\u4e0a\uff0c\u83b7\u5f97\u7406\u8bba\u4fdd\u8bc1\uff0c\u652f\u6301\u5750\u6807\u5206\u89e3\u7684\u504f\u79fb\u5b9a\u4f4d\u548c\u56fe\u50cf\u6570\u636e\u7684\u50cf\u7d20\u7ea7\u8bca\u65ad\u3002", "result": "\u5728\u63a7\u5236\u9ad8\u65af\u504f\u79fb\u3001MNIST\u51e0\u4f55\u6270\u52a8\u548cCIFAR-10\u6270\u52a8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4e0e\u4efb\u52a1\u6027\u80fd\u9000\u5316\u9ad8\u5ea6\u76f8\u5173\uff0c\u4e14\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "TASTE\u6846\u67b6\u6210\u529f\u5730\u5c06\u5206\u5e03\u504f\u79fb\u4e0e\u6a21\u578b\u654f\u611f\u6027\u8054\u7cfb\u8d77\u6765\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3001\u504f\u79fb\u5b9a\u4f4d\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u8bca\u65ad\uff0c\u5728OOD\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.07659", "categories": ["cs.LG", "cs.AI", "q-fin.ST"], "pdf": "https://arxiv.org/pdf/2602.07659", "abs": "https://arxiv.org/abs/2602.07659", "authors": ["Matthew Siper", "Muhammad Umair Nasir", "Ahmed Khalifa", "Lisa Soros", "Jay Azhang", "Julian Togelius"], "title": "Continuous Program Search", "comment": null, "summary": "Genetic Programming yields interpretable programs, but small syntactic mutations can induce large, unpredictable behavioral shifts, degrading locality and sample efficiency. We frame this as an operator-design problem: learn a continuous program space where latent distance has behavioral meaning, then design mutation operators that exploit this structure without changing the evolutionary optimizer.\n  We make locality measurable by tracking action-level divergence under controlled latent perturbations, identifying an empirical trust region for behavior-local continuous variation. Using a compact trading-strategy DSL with four semantic components (long/short entry and exit), we learn a matching block-factorized embedding and compare isotropic Gaussian mutation over the full latent space to geometry-compiled mutation that restricts updates to semantically paired entry--exit subspaces and proposes directions using a learned flow-based model trained on logged mutation outcomes.\n  Under identical $(\u03bc+\u03bb)$ evolution strategies and fixed evaluation budgets across five assets, the learned mutation operator discovers strong strategies using an order of magnitude fewer evaluations and achieves the highest median out-of-sample Sharpe ratio. Although isotropic mutation occasionally attains higher peak performance, geometry-compiled mutation yields faster, more reliable progress, demonstrating that semantically aligned mutation can substantially improve search efficiency without modifying the underlying evolutionary algorithm.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5b66\u4e60\u8fde\u7eed\u7a0b\u5e8f\u7a7a\u95f4\u548c\u8bbe\u8ba1\u51e0\u4f55\u7f16\u8bd1\u53d8\u5f02\u7b97\u5b50\u6765\u6539\u5584\u9057\u4f20\u7f16\u7a0b\u4e2d\u5c40\u90e8\u6027\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u641c\u7d22\u6548\u7387\u3002", "motivation": "\u9057\u4f20\u7f16\u7a0b\u867d\u7136\u80fd\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u7a0b\u5e8f\uff0c\u4f46\u5c0f\u7684\u8bed\u6cd5\u53d8\u5f02\u53ef\u80fd\u5bfc\u81f4\u5927\u7684\u3001\u4e0d\u53ef\u9884\u6d4b\u7684\u884c\u4e3a\u53d8\u5316\uff0c\u8fd9\u4f1a\u964d\u4f4e\u5c40\u90e8\u6027\u548c\u6837\u672c\u6548\u7387\u3002\u8fd9\u88ab\u89c6\u4e3a\u4e00\u4e2a\u7b97\u5b50\u8bbe\u8ba1\u95ee\u9898\u3002", "method": "1) \u5b66\u4e60\u4e00\u4e2a\u8fde\u7eed\u7a0b\u5e8f\u7a7a\u95f4\uff0c\u5176\u4e2d\u6f5c\u5728\u8ddd\u79bb\u5177\u6709\u884c\u4e3a\u610f\u4e49\uff1b2) \u8bbe\u8ba1\u53d8\u5f02\u7b97\u5b50\u5229\u7528\u8fd9\u79cd\u7ed3\u6784\u800c\u4e0d\u6539\u53d8\u8fdb\u5316\u4f18\u5316\u5668\uff1b3) \u901a\u8fc7\u8ddf\u8e2a\u53d7\u63a7\u6f5c\u5728\u6270\u52a8\u4e0b\u7684\u884c\u4e3a\u7ea7\u5206\u6b67\u6765\u6d4b\u91cf\u5c40\u90e8\u6027\uff1b4) \u4f7f\u7528\u5757\u56e0\u5b50\u5316\u5d4c\u5165\u548c\u51e0\u4f55\u7f16\u8bd1\u53d8\u5f02\uff0c\u9650\u5236\u66f4\u65b0\u5230\u8bed\u4e49\u914d\u5bf9\u7684\u8fdb\u5165-\u9000\u51fa\u5b50\u7a7a\u95f4\u3002", "result": "\u5728\u4e94\u4e2a\u8d44\u4ea7\u4e0a\u4f7f\u7528\u76f8\u540c\u7684(\u03bc+\u03bb)\u8fdb\u5316\u7b56\u7565\u548c\u56fa\u5b9a\u8bc4\u4f30\u9884\u7b97\uff0c\u5b66\u4e60\u7684\u53d8\u5f02\u7b97\u5b50\u4f7f\u7528\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7\u7684\u8bc4\u4f30\u53d1\u73b0\u5f3a\u7b56\u7565\uff0c\u5e76\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u4e2d\u4f4d\u6570\u6837\u672c\u5916\u590f\u666e\u6bd4\u7387\u3002\u51e0\u4f55\u7f16\u8bd1\u53d8\u5f02\u4ea7\u751f\u66f4\u5feb\u3001\u66f4\u53ef\u9760\u7684\u8fdb\u5c55\u3002", "conclusion": "\u8bed\u4e49\u5bf9\u9f50\u7684\u53d8\u5f02\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u641c\u7d22\u6548\u7387\uff0c\u800c\u65e0\u9700\u4fee\u6539\u5e95\u5c42\u8fdb\u5316\u7b97\u6cd5\u3002\u867d\u7136\u5404\u5411\u540c\u6027\u53d8\u5f02\u5076\u5c14\u80fd\u8fbe\u5230\u66f4\u9ad8\u7684\u5cf0\u503c\u6027\u80fd\uff0c\u4f46\u51e0\u4f55\u7f16\u8bd1\u53d8\u5f02\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u548c\u9ad8\u6548\u7684\u641c\u7d22\u3002"}}
{"id": "2602.07670", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07670", "abs": "https://arxiv.org/abs/2602.07670", "authors": ["Jarrod Barnes"], "title": "Surprisal-Guided Selection: Compute-Optimal Test-Time Strategies for Execution-Grounded Code Generation", "comment": "13 pages, 7 figures, 11 tables. Preprint. Code: https://github.com/jbarnes850/test-time-training", "summary": "Test-time training (TTT) adapts language models through gradient-based updates at inference. But is adaptation the right strategy? We study compute-optimal test-time strategies for verifiable execution-grounded (VEG) tasks, domains like GPU kernel optimization where a deterministic evaluator provides dense, continuous reward signals. Using KernelBench as our testbed and a 120B-parameter model (GPT-OSS-120B with LoRA adaptation), we find that search outperforms minimal adaptation (1-5 gradient steps): Best-of-N sampling achieves 90% task success (18/20 tasks) at K=64 across the full KernelBench L1 eval set while TTT's best checkpoint reaches only 30.6% (3-seed mean), with TTT's \"equivalent K\" falling below 1, worse than single-sample inference. The failure mode is over-sharpening: gradient updates collapse diversity toward mediocre solutions rather than discovering optimal ones. Our main contribution is surprisal-guided selection: selecting the highest-surprisal (lowest-confidence) correct sample yields 80% success vs. 50% for most-confident selection, a 30% improvement. Extending to surprisal-guided-top3 matches oracle performance at 100%. This zero-cost strategy, validated through length-controlled analysis, recovers oracle performance. For dense-reward VEG tasks, compute should be allocated to sample diversity and intelligent selection rather than gradient adaptation. The surprisal-guided selection principle may generalize to other execution-grounded domains where optimal solutions occupy the distribution tail.", "AI": {"tldr": "\u5728\u53ef\u9a8c\u8bc1\u6267\u884c\u4efb\u52a1\u4e2d\uff0c\u641c\u7d22\u7b56\u7565\u4f18\u4e8e\u6d4b\u8bd5\u65f6\u8bad\u7ec3\uff1a\u6700\u4f73N\u91c7\u6837\u8fbe\u523090%\u6210\u529f\u7387\uff0c\u800cTTT\u4ec530.6%\u3002\u63d0\u51fa\u57fa\u4e8e\u60ca\u5947\u5ea6\u5f15\u5bfc\u7684\u9009\u62e9\u7b56\u7565\uff0c\u96f6\u6210\u672c\u5b9e\u73b080%\u6210\u529f\u7387\u3002", "motivation": "\u7814\u7a76\u5728\u53ef\u9a8c\u8bc1\u6267\u884c\u4efb\u52a1\u4e2d\uff0c\u8ba1\u7b97\u8d44\u6e90\u5e94\u8be5\u7528\u4e8e\u68af\u5ea6\u9002\u5e94\u8fd8\u662f\u641c\u7d22\u7b56\u7565\u3002\u9488\u5bf9GPU\u5185\u6838\u4f18\u5316\u7b49\u5bc6\u96c6\u5956\u52b1\u4efb\u52a1\uff0c\u63a2\u7d22\u6700\u4f18\u7684\u6d4b\u8bd5\u65f6\u7b56\u7565\u3002", "method": "\u4f7f\u7528KernelBench\u6d4b\u8bd5\u5e73\u53f0\u548c120B\u53c2\u6570\u6a21\u578b\uff0c\u6bd4\u8f83\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u4e0e\u641c\u7d22\u7b56\u7565\u3002\u63d0\u51fa\u60ca\u5947\u5ea6\u5f15\u5bfc\u9009\u62e9\uff1a\u9009\u62e9\u6700\u9ad8\u60ca\u5947\u5ea6\uff08\u6700\u4f4e\u7f6e\u4fe1\u5ea6\uff09\u7684\u6b63\u786e\u6837\u672c\u3002", "result": "\u6700\u4f73N\u91c7\u6837\uff08K=64\uff09\u8fbe\u523090%\u4efb\u52a1\u6210\u529f\u7387\uff0c\u800cTTT\u4ec530.6%\u3002\u60ca\u5947\u5ea6\u5f15\u5bfc\u9009\u62e9\u5b9e\u73b080%\u6210\u529f\u7387\uff0c\u6bd4\u6700\u7f6e\u4fe1\u9009\u62e9\u63d0\u9ad830%\u3002\u60ca\u5947\u5ea6\u5f15\u5bfc\u524d3\u9009\u62e9\u8fbe\u5230100%\u6210\u529f\u7387\u3002", "conclusion": "\u5bf9\u4e8e\u5bc6\u96c6\u5956\u52b1\u7684\u53ef\u9a8c\u8bc1\u6267\u884c\u4efb\u52a1\uff0c\u8ba1\u7b97\u8d44\u6e90\u5e94\u5206\u914d\u7ed9\u6837\u672c\u591a\u6837\u6027\u548c\u667a\u80fd\u9009\u62e9\uff0c\u800c\u975e\u68af\u5ea6\u9002\u5e94\u3002\u60ca\u5947\u5ea6\u5f15\u5bfc\u9009\u62e9\u539f\u5219\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u6267\u884c\u4efb\u52a1\u9886\u57df\u3002"}}
{"id": "2602.07671", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07671", "abs": "https://arxiv.org/abs/2602.07671", "authors": ["Mohan Li", "Dario Fenoglio", "Martin Gjoreski", "Marc Langheinrich"], "title": "Federated Learning with Profile Mapping under Distribution Shifts and Drifts", "comment": "ICLR2026", "summary": "Federated Learning (FL) enables decentralized model training across clients without sharing raw data, but its performance degrades under real-world data heterogeneity. Existing methods often fail to address distribution shift across clients and distribution drift over time, or they rely on unrealistic assumptions such as known number of client clusters and data heterogeneity types, which limits their generalizability. We introduce Feroma, a novel FL framework that explicitly handles both distribution shift and drift without relying on client or cluster identity. Feroma builds on client distribution profiles-compact, privacy-preserving representations of local data-that guide model aggregation and test-time model assignment through adaptive similarity-based weighting. This design allows Feroma to dynamically select aggregation strategies during training, ranging from clustered to personalized, and deploy suitable models to unseen, and unlabeled test clients without retraining, online adaptation, or prior knowledge on clients' data. Extensive experiments show that compared to 10 state-of-the-art methods, Feroma improves performance and stability under dynamic data heterogeneity conditions-an average accuracy gain of up to 12 percentage points over the best baselines across 6 benchmarks-while maintaining computational and communication overhead comparable to FedAvg. These results highlight that distribution-profile-based aggregation offers a practical path toward robust FL under both data distribution shifts and drifts.", "AI": {"tldr": "Feroma\u662f\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5ba2\u6237\u7aef\u5206\u5e03\u914d\u7f6e\u6587\u4ef6\u5904\u7406\u6570\u636e\u5f02\u6784\u6027\uff0c\u65e0\u9700\u5ba2\u6237\u7aef\u96c6\u7fa4\u8eab\u4efd\u4fe1\u606f\uff0c\u52a8\u6001\u9009\u62e9\u805a\u5408\u7b56\u7565\u5e76\u90e8\u7f72\u6a21\u578b\u5230\u672a\u89c1\u8fc7\u7684\u6d4b\u8bd5\u5ba2\u6237\u7aef\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u5f02\u6784\u6027\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u65e0\u6cd5\u540c\u65f6\u5904\u7406\u5ba2\u6237\u7aef\u95f4\u7684\u5206\u5e03\u504f\u79fb\u548c\u65f6\u95f4\u4e0a\u7684\u5206\u5e03\u6f02\u79fb\uff0c\u4e14\u4f9d\u8d56\u4e0d\u73b0\u5b9e\u7684\u5047\u8bbe\uff08\u5982\u5df2\u77e5\u5ba2\u6237\u7aef\u96c6\u7fa4\u6570\u91cf\u3001\u6570\u636e\u5f02\u6784\u7c7b\u578b\uff09\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u5ba2\u6237\u7aef\u5206\u5e03\u914d\u7f6e\u6587\u4ef6\uff08\u7d27\u51d1\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u672c\u5730\u6570\u636e\u8868\u793a\uff09\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u76f8\u4f3c\u6027\u52a0\u6743\u6307\u5bfc\u6a21\u578b\u805a\u5408\u548c\u6d4b\u8bd5\u65f6\u6a21\u578b\u5206\u914d\uff0c\u52a8\u6001\u9009\u62e9\u4ece\u96c6\u7fa4\u5316\u5230\u4e2a\u6027\u5316\u7684\u805a\u5408\u7b56\u7565\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3001\u5728\u7ebf\u9002\u5e94\u6216\u5ba2\u6237\u7aef\u6570\u636e\u5148\u9a8c\u77e5\u8bc6\u3002", "result": "\u57286\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd410\u4e2a\u6700\u5148\u8fdb\u65b9\u6cd5\uff0cFeroma\u5728\u52a8\u6001\u6570\u636e\u5f02\u6784\u6761\u4ef6\u4e0b\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe12\u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u4fdd\u6301\u4e0eFedAvg\u76f8\u5f53\u7684\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\u3002", "conclusion": "\u57fa\u4e8e\u5206\u5e03\u914d\u7f6e\u6587\u4ef6\u7684\u805a\u5408\u4e3a\u5728\u6570\u636e\u5206\u5e03\u504f\u79fb\u548c\u6f02\u79fb\u4e0b\u5b9e\u73b0\u9c81\u68d2\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2602.07674", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07674", "abs": "https://arxiv.org/abs/2602.07674", "authors": ["Bohdan Turbal", "Iryna Voitsitska", "Lesia Semenova"], "title": "ElliCE: Efficient and Provably Robust Algorithmic Recourse via the Rashomon Sets", "comment": null, "summary": "Machine learning models now influence decisions that directly affect people's lives, making it important to understand not only their predictions, but also how individuals could act to obtain better results. Algorithmic recourse provides actionable input modifications to achieve more favorable outcomes, typically relying on counterfactual explanations to suggest such changes. However, when the Rashomon set - the set of near-optimal models - is large, standard counterfactual explanations can become unreliable, as a recourse action valid for one model may fail under another. We introduce ElliCE, a novel framework for robust algorithmic recourse that optimizes counterfactuals over an ellipsoidal approximation of the Rashomon set. The resulting explanations are provably valid over this ellipsoid, with theoretical guarantees on uniqueness, stability, and alignment with key feature directions. Empirically, ElliCE generates counterfactuals that are not only more robust but also more flexible, adapting to user-specified feature constraints while being substantially faster than existing baselines. This provides a principled and practical solution for reliable recourse under model uncertainty, ensuring stable recommendations for users even as models evolve.", "AI": {"tldr": "\u63d0\u51faElliCE\u6846\u67b6\uff0c\u901a\u8fc7\u692d\u5706\u8fd1\u4f3cRashomon\u96c6\u6765\u751f\u6210\u9c81\u68d2\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u89e3\u51b3\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u7b97\u6cd5\u8ffd\u7d22\u95ee\u9898", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5f71\u54cd\u4eba\u4eec\u751f\u6d3b\u51b3\u7b56\uff0c\u9700\u8981\u7406\u89e3\u5982\u4f55\u901a\u8fc7\u884c\u52a8\u83b7\u5f97\u66f4\u597d\u7ed3\u679c\u3002\u5f53Rashomon\u96c6\uff08\u8fd1\u4f3c\u6700\u4f18\u6a21\u578b\u96c6\u5408\uff09\u5f88\u5927\u65f6\uff0c\u6807\u51c6\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u53d8\u5f97\u4e0d\u53ef\u9760\uff0c\u56e0\u4e3a\u5bf9\u4e00\u4e2a\u6a21\u578b\u6709\u6548\u7684\u8ffd\u7d22\u884c\u52a8\u53ef\u80fd\u5bf9\u53e6\u4e00\u4e2a\u6a21\u578b\u5931\u8d25", "method": "ElliCE\u6846\u67b6\uff1a\u5728Rashomon\u96c6\u7684\u692d\u5706\u8fd1\u4f3c\u4e0a\u4f18\u5316\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u786e\u4fdd\u5728\u8be5\u692d\u5706\u533a\u57df\u5185\u89e3\u91ca\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u552f\u4e00\u6027\u3001\u7a33\u5b9a\u6027\u548c\u7279\u5f81\u65b9\u5411\u5bf9\u9f50\u7684\u7406\u8bba\u4fdd\u8bc1", "result": "ElliCE\u751f\u6210\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u4e0d\u4ec5\u66f4\u9c81\u68d2\uff0c\u800c\u4e14\u66f4\u7075\u6d3b\uff0c\u80fd\u9002\u5e94\u7528\u6237\u6307\u5b9a\u7684\u7279\u5f81\u7ea6\u675f\uff0c\u8ba1\u7b97\u901f\u5ea6\u663e\u8457\u5feb\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "ElliCE\u4e3a\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u53ef\u9760\u7b97\u6cd5\u8ffd\u7d22\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u548c\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u786e\u4fdd\u5373\u4f7f\u6a21\u578b\u53d8\u5316\u4e5f\u80fd\u4e3a\u7528\u6237\u63d0\u4f9b\u7a33\u5b9a\u7684\u63a8\u8350"}}
{"id": "2602.07679", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07679", "abs": "https://arxiv.org/abs/2602.07679", "authors": ["Jusheng Zhang", "Yijia Fan", "Kaitong Cai", "Jing Yang", "Yongsen Zheng", "Kwok-Yan Lam", "Liang Lin", "Keze Wang"], "title": "Spectral Gating Networks", "comment": null, "summary": "Gating mechanisms are ubiquitous, yet a complementary question in feed-forward networks remains under-explored: how to introduce frequency-rich expressivity without sacrificing stability and scalability? This tension is exposed by spline-based Kolmogorov-Arnold Network (KAN) parameterizations, where grid refinement can induce parameter growth and brittle optimization in high dimensions. To propose a stability-preserving way to inject spectral capacity into existing MLP/FFN layers under fixed parameter and training budgets, we introduce Spectral Gating Networks (SGN), a drop-in spectral reparameterization. SGN augments a standard activation pathway with a compact spectral pathway and learnable gates that allow the model to start from a stable base behavior and progressively allocate capacity to spectral features during training. The spectral pathway is instantiated with trainable Random Fourier Features (learned frequencies and phases), replacing grid-based splines and removing resolution dependence. A hybrid GELU-Fourier formulation further improves optimization robustness while enhancing high-frequency fidelity. Across vision, NLP, audio, and PDE benchmarks, SGN consistently improves accuracy-efficiency trade-offs under comparable computational budgets, achieving 93.15% accuracy on CIFAR-10 and up to 11.7x faster inference than spline-based KAN variants. Code and trained models will be released.", "AI": {"tldr": "SGN\u662f\u4e00\u79cd\u5728\u56fa\u5b9a\u53c2\u6570\u548c\u8bad\u7ec3\u9884\u7b97\u4e0b\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u95e8\u63a7\u673a\u5236\u5c06\u9891\u8c31\u80fd\u529b\u6ce8\u5165MLP/FFN\u5c42\u7684\u7a33\u5b9a\u65b9\u6cd5\uff0c\u4f7f\u7528\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81\u66ff\u4ee3\u57fa\u4e8e\u7f51\u683c\u7684\u6837\u6761\uff0c\u5728\u591a\u4e2a\u9886\u57df\u63d0\u5347\u7cbe\u5ea6-\u6548\u7387\u6743\u8861\u3002", "motivation": "\u524d\u9988\u7f51\u7edc\u4e2d\u5982\u4f55\u5728\u4e0d\u727a\u7272\u7a33\u5b9a\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684\u60c5\u51b5\u4e0b\u5f15\u5165\u4e30\u5bcc\u7684\u9891\u7387\u8868\u8fbe\u80fd\u529b\u662f\u4e00\u4e2a\u672a\u5145\u5206\u63a2\u7d22\u7684\u95ee\u9898\u3002\u57fa\u4e8e\u6837\u6761\u7684KAN\u53c2\u6570\u5316\u5728\u7f51\u683c\u7ec6\u5316\u65f6\u4f1a\u5bfc\u81f4\u53c2\u6570\u589e\u957f\u548c\u8106\u5f31\u7684\u4f18\u5316\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51faSGN\uff08\u9891\u8c31\u95e8\u63a7\u7f51\u7edc\uff09\uff0c\u5728\u6807\u51c6\u6fc0\u6d3b\u8def\u5f84\u4e0a\u589e\u52a0\u7d27\u51d1\u7684\u9891\u8c31\u8def\u5f84\u548c\u53ef\u5b66\u4e60\u95e8\u63a7\u3002\u9891\u8c31\u8def\u5f84\u4f7f\u7528\u53ef\u8bad\u7ec3\u7684\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81\uff08\u5b66\u4e60\u9891\u7387\u548c\u76f8\u4f4d\uff09\u66ff\u4ee3\u57fa\u4e8e\u7f51\u683c\u7684\u6837\u6761\uff0c\u6d88\u9664\u5206\u8fa8\u7387\u4f9d\u8d56\u3002\u91c7\u7528\u6df7\u5408GELU-\u5085\u91cc\u53f6\u516c\u5f0f\u63d0\u9ad8\u4f18\u5316\u9c81\u68d2\u6027\u548c\u9ad8\u9891\u4fdd\u771f\u5ea6\u3002", "result": "\u5728\u89c6\u89c9\u3001NLP\u3001\u97f3\u9891\u548cPDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSGN\u5728\u53ef\u6bd4\u8f83\u7684\u8ba1\u7b97\u9884\u7b97\u4e0b\u6301\u7eed\u6539\u5584\u7cbe\u5ea6-\u6548\u7387\u6743\u8861\uff0c\u5728CIFAR-10\u4e0a\u8fbe\u523093.15%\u51c6\u786e\u7387\uff0c\u6bd4\u57fa\u4e8e\u6837\u6761\u7684KAN\u53d8\u4f53\u63a8\u7406\u901f\u5ea6\u5feb\u8fbe11.7\u500d\u3002", "conclusion": "SGN\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5b9a\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u5728\u56fa\u5b9a\u53c2\u6570\u548c\u8bad\u7ec3\u9884\u7b97\u4e0b\u5c06\u9891\u8c31\u80fd\u529b\u6ce8\u5165\u73b0\u6709MLP/FFN\u5c42\uff0c\u89e3\u51b3\u4e86\u6837\u6761\u57faKAN\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u4f18\u5316\u7279\u6027\u3002"}}
{"id": "2602.07697", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.07697", "abs": "https://arxiv.org/abs/2602.07697", "authors": ["Francesco Innocenti", "El Mehdi Achour", "Rafal Bogacz"], "title": "On the Infinite Width and Depth Limits of Predictive Coding Networks", "comment": "31 pages, 27 figures", "summary": "Predictive coding (PC) is a biologically plausible alternative to standard backpropagation (BP) that minimises an energy function with respect to network activities before updating weights. Recent work has improved the training stability of deep PC networks (PCNs) by leveraging some BP-inspired reparameterisations. However, the full scalability and theoretical basis of these approaches remains unclear. To address this, we study the infinite width and depth limits of PCNs. For linear residual networks, we show that the set of width- and depth-stable feature-learning parameterisations for PC is exactly the same as for BP. Moreover, under any of these parameterisations, the PC energy with equilibrated activities converges to the BP loss in a regime where the model width is much larger than the depth, resulting in PC computing the same gradients as BP. Experiments show that these results hold in practice for deep nonlinear networks, as long as an activity equilibrium seem to be reached. Overall, this work unifies various previous theoretical and empirical results and has potentially important implications for the scaling of PCNs.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u5728\u65e0\u9650\u5bbd\u5ea6\u548c\u6df1\u5ea6\u6781\u9650\u4e0b\uff0c\u9884\u6d4b\u7f16\u7801\u7f51\u7edc\uff08PCNs\uff09\u4e0e\u53cd\u5411\u4f20\u64ad\uff08BP\uff09\u5177\u6709\u76f8\u540c\u7684\u7a33\u5b9a\u53c2\u6570\u5316\u96c6\u5408\uff0c\u4e14PC\u80fd\u91cf\u5728\u6d3b\u52a8\u5e73\u8861\u65f6\u6536\u655b\u5230BP\u635f\u5931\uff0c\u4ece\u800c\u8ba1\u7b97\u76f8\u540c\u7684\u68af\u5ea6\u3002", "motivation": "\u9884\u6d4b\u7f16\u7801\uff08PC\uff09\u662f\u53cd\u5411\u4f20\u64ad\uff08BP\uff09\u7684\u4e00\u79cd\u751f\u7269\u53ef\u4fe1\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u6df1\u5ea6PC\u7f51\u7edc\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u53ef\u6269\u5c55\u6027\u4ecd\u4e0d\u6e05\u695a\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76PCNs\u5728\u65e0\u9650\u5bbd\u5ea6\u548c\u6df1\u5ea6\u6781\u9650\u4e0b\u7684\u7406\u8bba\u57fa\u7840\uff0c\u4ee5\u7406\u89e3\u5176\u53ef\u6269\u5c55\u6027\u3002", "method": "\u7814\u7a76\u7ebf\u6027\u6b8b\u5dee\u7f51\u7edc\u7684\u65e0\u9650\u5bbd\u5ea6\u548c\u6df1\u5ea6\u6781\u9650\uff0c\u5206\u6790PC\u548cBP\u7684\u5bbd\u5ea6\u548c\u6df1\u5ea6\u7a33\u5b9a\u7279\u5f81\u5b66\u4e60\u53c2\u6570\u5316\u96c6\u5408\uff0c\u8bc1\u660e\u5728\u6d3b\u52a8\u5e73\u8861\u6761\u4ef6\u4e0bPC\u80fd\u91cf\u6536\u655b\u5230BP\u635f\u5931\u3002", "result": "\u5bf9\u4e8e\u7ebf\u6027\u6b8b\u5dee\u7f51\u7edc\uff0cPC\u548cBP\u5177\u6709\u5b8c\u5168\u76f8\u540c\u7684\u5bbd\u5ea6\u548c\u6df1\u5ea6\u7a33\u5b9a\u53c2\u6570\u5316\u96c6\u5408\u3002\u5728\u6a21\u578b\u5bbd\u5ea6\u8fdc\u5927\u4e8e\u6df1\u5ea6\u7684\u6761\u4ef6\u4e0b\uff0cPC\u80fd\u91cf\u5728\u6d3b\u52a8\u5e73\u8861\u65f6\u6536\u655b\u5230BP\u635f\u5931\uff0c\u4ece\u800c\u8ba1\u7b97\u4e0eBP\u76f8\u540c\u7684\u68af\u5ea6\u3002\u5b9e\u9a8c\u8868\u660e\u8fd9\u4e9b\u7ed3\u679c\u5728\u6df1\u5ea6\u975e\u7ebf\u6027\u7f51\u7edc\u4e2d\u540c\u6837\u6210\u7acb\u3002", "conclusion": "\u672c\u6587\u7edf\u4e00\u4e86\u5148\u524d\u7406\u8bba\u548c\u5b9e\u8bc1\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86PCNs\u5728\u9002\u5f53\u53c2\u6570\u5316\u4e0b\u4e0eBP\u5177\u6709\u76f8\u540c\u7684\u7a33\u5b9a\u6027\u548c\u68af\u5ea6\u8ba1\u7b97\u80fd\u529b\uff0c\u5bf9PCNs\u7684\u6269\u5c55\u5177\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2602.07706", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07706", "abs": "https://arxiv.org/abs/2602.07706", "authors": ["Yuanyun Zhang", "Mingxuan Zhang", "Siyuan Li", "Zihan Wang", "Haoran Chen", "Wenbo Zhou", "Shi Li"], "title": "Dense Feature Learning via Linear Structure Preservation in Medical Data", "comment": "ICLR Workshop", "summary": "Deep learning models for medical data are typically trained using task specific objectives that encourage representations to collapse onto a small number of discriminative directions. While effective for individual prediction problems, this paradigm underutilizes the rich structure of clinical data and limits the transferability, stability, and interpretability of learned features. In this work, we propose dense feature learning, a representation centric framework that explicitly shapes the linear structure of medical embeddings. Our approach operates directly on embedding matrices, encouraging spectral balance, subspace consistency, and feature orthogonality through objectives defined entirely in terms of linear algebraic properties. Without relying on labels or generative reconstruction, dense feature learning produces representations with higher effective rank, improved conditioning, and greater stability across time. Empirical evaluations across longitudinal EHR data, clinical text, and multimodal patient representations demonstrate consistent improvements in downstream linear performance, robustness, and subspace alignment compared to supervised and self supervised baselines. These results suggest that learning to span clinical variation may be as important as learning to predict clinical outcomes, and position representation geometry as a first class objective in medical AI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5bc6\u96c6\u7279\u5f81\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u4f18\u5316\u5d4c\u5165\u77e9\u9635\u7684\u7ebf\u6027\u4ee3\u6570\u7279\u6027\uff08\u8c31\u5e73\u8861\u3001\u5b50\u7a7a\u95f4\u4e00\u81f4\u6027\u3001\u7279\u5f81\u6b63\u4ea4\u6027\uff09\uff0c\u6539\u5584\u533b\u5b66\u8868\u793a\u7684\u8d28\u91cf\uff0c\u65e0\u9700\u6807\u7b7e\u6216\u751f\u6210\u91cd\u5efa\u3002", "motivation": "\u4f20\u7edf\u533b\u5b66\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4f7f\u7528\u4efb\u52a1\u7279\u5b9a\u76ee\u6807\u8bad\u7ec3\uff0c\u5bfc\u81f4\u8868\u793a\u574d\u7f29\u5230\u5c11\u91cf\u5224\u522b\u65b9\u5411\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u4e34\u5e8a\u6570\u636e\u7684\u4e30\u5bcc\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u7279\u5f81\u7684\u8fc1\u79fb\u6027\u3001\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u5bc6\u96c6\u7279\u5f81\u5b66\u4e60\u6846\u67b6\uff0c\u76f4\u63a5\u64cd\u4f5c\u5d4c\u5165\u77e9\u9635\uff0c\u901a\u8fc7\u8c31\u5e73\u8861\u3001\u5b50\u7a7a\u95f4\u4e00\u81f4\u6027\u548c\u7279\u5f81\u6b63\u4ea4\u6027\u7b49\u7ebf\u6027\u4ee3\u6570\u7279\u6027\u5b9a\u4e49\u76ee\u6807\u51fd\u6570\uff0c\u65e0\u9700\u4f9d\u8d56\u6807\u7b7e\u6216\u751f\u6210\u91cd\u5efa\u3002", "result": "\u5728\u7eb5\u5411\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u3001\u4e34\u5e8a\u6587\u672c\u548c\u591a\u6a21\u6001\u60a3\u8005\u8868\u793a\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u76d1\u7763\u548c\u81ea\u76d1\u7763\u57fa\u7ebf\uff0c\u8be5\u65b9\u6cd5\u5728\u6709\u6548\u79e9\u3001\u6761\u4ef6\u6570\u3001\u7a33\u5b9a\u6027\u3001\u4e0b\u6e38\u7ebf\u6027\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u5b50\u7a7a\u95f4\u5bf9\u9f50\u65b9\u9762\u5747\u6709\u63d0\u5347\u3002", "conclusion": "\u5b66\u4e60\u8986\u76d6\u4e34\u5e8a\u53d8\u5f02\u4e0e\u5b66\u4e60\u9884\u6d4b\u4e34\u5e8a\u7ed3\u679c\u540c\u7b49\u91cd\u8981\uff0c\u5e94\u5c06\u8868\u793a\u51e0\u4f55\u4f5c\u4e3a\u533b\u5b66AI\u7684\u4e00\u7b49\u76ee\u6807\uff0c\u5bc6\u96c6\u7279\u5f81\u5b66\u4e60\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2602.07708", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07708", "abs": "https://arxiv.org/abs/2602.07708", "authors": ["Ding Zhang", "Siddharth Betala", "Chirag Agarwal"], "title": "Quantifying Explanation Quality in Graph Neural Networks using Out-of-Distribution Generalization", "comment": null, "summary": "Evaluating the quality of post-hoc explanations for Graph Neural Networks (GNNs) remains a significant challenge. While recent years have seen an increasing development of explainability methods, current evaluation metrics (e.g., fidelity, sparsity) often fail to assess whether an explanation identifies the true underlying causal variables. To address this, we propose the Explanation-Generalization Score (EGS), a metric that quantifies the causal relevance of GNN explanations. EGS is founded on the principle of feature invariance and posits that if an explanation captures true causal drivers, it should lead to stable predictions across distribution shifts. To quantify this, we introduce a framework that trains GNNs using explanatory subgraphs and evaluates their performance in Out-of-Distribution (OOD) settings (here, OOD generalization serves as a rigorous proxy for the explanation's causal validity). Through large-scale validation involving 11,200 model combinations across synthetic and real-world datasets, our results demonstrate that EGS provides a principled benchmark for ranking explainers based on their ability to capture causal substructures, offering a robust alternative to traditional fidelity-based metrics.", "AI": {"tldr": "\u63d0\u51faEGS\uff08Explanation-Generalization Score\uff09\u6307\u6807\uff0c\u901a\u8fc7\u8bc4\u4f30GNN\u89e3\u91ca\u5728\u5206\u5e03\u5916\u6cdb\u5316\u4e2d\u7684\u7a33\u5b9a\u6027\u6765\u91cf\u5316\u89e3\u91ca\u7684\u56e0\u679c\u76f8\u5173\u6027\uff0c\u4e3a\u56fe\u795e\u7ecf\u7f51\u7edc\u89e3\u91ca\u65b9\u6cd5\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "motivation": "\u5f53\u524d\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u540e\u9a8c\u89e3\u91ca\u7684\u8d28\u91cf\u8bc4\u4f30\u5b58\u5728\u6311\u6218\uff0c\u73b0\u6709\u8bc4\u4f30\u6307\u6807\uff08\u5982\u4fdd\u771f\u5ea6\u3001\u7a00\u758f\u6027\uff09\u5f80\u5f80\u65e0\u6cd5\u5224\u65ad\u89e3\u91ca\u662f\u5426\u8bc6\u522b\u4e86\u771f\u6b63\u7684\u56e0\u679c\u53d8\u91cf\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u8bc4\u4f30\u89e3\u91ca\u56e0\u679c\u6709\u6548\u6027\u7684\u65b0\u6307\u6807\u3002", "method": "\u63d0\u51faEGS\u6307\u6807\uff0c\u57fa\u4e8e\u7279\u5f81\u4e0d\u53d8\u6027\u539f\u5219\uff1a\u5982\u679c\u89e3\u91ca\u6355\u83b7\u4e86\u771f\u6b63\u7684\u56e0\u679c\u9a71\u52a8\u56e0\u7d20\uff0c\u90a3\u4e48\u5728\u4e0d\u540c\u5206\u5e03\u504f\u79fb\u4e0b\u5e94\u8be5\u80fd\u4fdd\u6301\u7a33\u5b9a\u7684\u9884\u6d4b\u3002\u901a\u8fc7\u4f7f\u7528\u89e3\u91ca\u5b50\u56fe\u8bad\u7ec3GNN\uff0c\u5e76\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u8bbe\u7f6e\u4e2d\u8bc4\u4f30\u5176\u6027\u80fd\uff0c\u5c06OOD\u6cdb\u5316\u4f5c\u4e3a\u89e3\u91ca\u56e0\u679c\u6709\u6548\u6027\u7684\u4e25\u683c\u4ee3\u7406\u6307\u6807\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u9a8c\u8bc1\uff0c\u6d89\u53ca11,200\u4e2a\u6a21\u578b\u7ec4\u5408\u3002\u7ed3\u679c\u8868\u660eEGS\u80fd\u591f\u57fa\u4e8e\u89e3\u91ca\u6355\u83b7\u56e0\u679c\u5b50\u7ed3\u6784\u7684\u80fd\u529b\u4e3a\u89e3\u91ca\u5668\u63d0\u4f9b\u539f\u5219\u6027\u7684\u6392\u5e8f\u57fa\u51c6\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684\u57fa\u4e8e\u4fdd\u771f\u5ea6\u7684\u6307\u6807\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "EGS\u4e3aGNN\u89e3\u91ca\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u89e3\u91ca\u7684\u56e0\u679c\u76f8\u5173\u6027\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u89e3\u91ca\u5668\u8bc6\u522b\u771f\u6b63\u56e0\u679c\u53d8\u91cf\u7684\u80fd\u529b\uff0c\u63a8\u52a8\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.07712", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07712", "abs": "https://arxiv.org/abs/2602.07712", "authors": ["Alexandra Volkova", "Mher Safaryan", "Christoph H. Lampert", "Dan Alistarh"], "title": "Towards Robust Scaling Laws for Optimizers", "comment": null, "summary": "The quality of Large Language Model (LLM) pretraining depends on multiple factors, including the compute budget and the choice of optimization algorithm. Empirical scaling laws are widely used to predict loss as model size and training data grow, however, almost all existing studies fix the optimizer (typically AdamW). At the same time, a new generation of optimizers (e.g., Muon, Shampoo, SOAP) promises faster and more stable convergence, but their relationship with model and data scaling is not yet well understood. In this work, we study scaling laws across different optimizers. Empirically, we show that 1) separate Chinchilla-style scaling laws for each optimizer are ill-conditioned and have highly correlated parameters. Instead, 2) we propose a more robust law with shared power-law exponents and optimizer-specific rescaling factors, which enable direct comparison between optimizers. Finally, 3) we provide a theoretical analysis of gradient-based methods for the proxy task of a convex quadratic objective, demonstrating that Chinchilla-style scaling laws emerge naturally as a result of loss decomposition into irreducible, approximation, and optimization errors.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u4e0d\u540c\u4f18\u5316\u5668\uff08\u5982AdamW\u3001Muon\u3001Shampoo\u3001SOAP\uff09\u5bf9LLM\u9884\u8bad\u7ec3\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u66f4\u7a33\u5065\u7684\u7f29\u653e\u5b9a\u5f8b\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u7f29\u653e\u5b9a\u5f8b\u7814\u7a76\u901a\u5e38\u56fa\u5b9a\u4f7f\u7528AdamW\u4f18\u5316\u5668\uff0c\u800c\u65b0\u4e00\u4ee3\u4f18\u5316\u5668\uff08\u5982Muon\u3001Shampoo\u3001SOAP\uff09\u867d\u7136\u627f\u8bfa\u66f4\u5feb\u66f4\u7a33\u5b9a\u7684\u6536\u655b\uff0c\u4f46\u5176\u4e0e\u6a21\u578b\u548c\u6570\u636e\u7f29\u653e\u7684\u5173\u7cfb\u5c1a\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002\u9700\u8981\u7814\u7a76\u4e0d\u540c\u4f18\u5316\u5668\u4e0b\u7684\u7f29\u653e\u5b9a\u5f8b\u3002", "method": "1) \u5b9e\u8bc1\u7814\u7a76\u4e0d\u540c\u4f18\u5316\u5668\u7684Chinchilla\u5f0f\u7f29\u653e\u5b9a\u5f8b\uff1b2) \u63d0\u51fa\u5171\u4eab\u5e42\u5f8b\u6307\u6570\u548c\u4f18\u5316\u5668\u7279\u5b9a\u7f29\u653e\u56e0\u5b50\u7684\u66f4\u7a33\u5065\u7f29\u653e\u5b9a\u5f8b\uff1b3) \u5bf9\u51f8\u4e8c\u6b21\u76ee\u6807\u51fd\u6570\u7684\u68af\u5ea6\u65b9\u6cd5\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660eChinchilla\u5f0f\u7f29\u653e\u5b9a\u5f8b\u662f\u635f\u5931\u5206\u89e3\u4e3a\u4e0d\u53ef\u7ea6\u8bef\u5dee\u3001\u8fd1\u4f3c\u8bef\u5dee\u548c\u4f18\u5316\u8bef\u5dee\u7684\u81ea\u7136\u7ed3\u679c\u3002", "result": "1) \u53d1\u73b0\u6bcf\u4e2a\u4f18\u5316\u5668\u7684\u72ec\u7acb\u7f29\u653e\u5b9a\u5f8b\u5b58\u5728\u75c5\u6001\u6027\u548c\u9ad8\u5ea6\u76f8\u5173\u53c2\u6570\uff1b2) \u63d0\u51fa\u7684\u5171\u4eab\u6307\u6570\u7f29\u653e\u5b9a\u5f8b\u80fd\u591f\u76f4\u63a5\u6bd4\u8f83\u4e0d\u540c\u4f18\u5316\u5668\uff1b3) \u7406\u8bba\u5206\u6790\u8868\u660e\u7f29\u653e\u5b9a\u5f8b\u662f\u635f\u5931\u5206\u89e3\u7684\u81ea\u7136\u7ed3\u679c\uff0c\u4e3a\u4f18\u5316\u5668\u9009\u62e9\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002", "conclusion": "\u4e0d\u540c\u4f18\u5316\u5668\u9700\u8981\u7edf\u4e00\u7684\u7f29\u653e\u5b9a\u5f8b\u6846\u67b6\u8fdb\u884c\u6bd4\u8f83\uff0c\u63d0\u51fa\u7684\u5171\u4eab\u6307\u6570\u7f29\u653e\u5b9a\u5f8b\u66f4\u7a33\u5065\uff0c\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86\u7f29\u653e\u5b9a\u5f8b\u7684\u6570\u5b66\u672c\u8d28\uff0c\u4e3aLLM\u9884\u8bad\u7ec3\u4e2d\u4f18\u5316\u5668\u9009\u62e9\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.07715", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07715", "abs": "https://arxiv.org/abs/2602.07715", "authors": ["Roi Benita", "Michael Elad", "Joseph Keshet"], "title": "Analyzing and Guiding Zero-Shot Posterior Sampling in Diffusion Models", "comment": null, "summary": "Recovering a signal from its degraded measurements is a long standing challenge in science and engineering. Recently, zero-shot diffusion based methods have been proposed for such inverse problems, offering a posterior sampling based solution that leverages prior knowledge. Such algorithms incorporate the observations through inference, often leaning on manual tuning and heuristics. In this work we propose a rigorous analysis of such approximate posterior-samplers, relying on a Gaussianity assumption of the prior. Under this regime, we show that both the ideal posterior sampler and diffusion-based reconstruction algorithms can be expressed in closed-form, enabling their thorough analysis and comparisons in the spectral domain. Building on these representations, we also introduce a principled framework for parameter design, replacing heuristic selection strategies used to date. The proposed approach is method-agnostic and yields tailored parameter choices for each algorithm, jointly accounting for the characteristics of the prior, the degraded signal, and the diffusion dynamics. We show that our spectral recommendations differ structurally from standard heuristics and vary with the diffusion step size, resulting in a consistent balance between perceptual quality and signal fidelity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u5148\u9a8c\u5047\u8bbe\u7684\u96f6\u6837\u672c\u6269\u6563\u9006\u95ee\u9898\u6c42\u89e3\u5668\u7406\u8bba\u5206\u6790\u6846\u67b6\uff0c\u80fd\u591f\u63a8\u5bfc\u95ed\u5f0f\u89e3\u5e76\u8fdb\u884c\u8c31\u57df\u5206\u6790\uff0c\u4e3a\u53c2\u6570\u8bbe\u8ba1\u63d0\u4f9b\u539f\u5219\u6027\u65b9\u6cd5", "motivation": "\u73b0\u6709\u7684\u96f6\u6837\u672c\u6269\u6563\u9006\u95ee\u9898\u6c42\u89e3\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u8c03\u53c2\u548c\u542f\u53d1\u5f0f\u7b56\u7565\uff0c\u7f3a\u4e4f\u7406\u8bba\u5206\u6790\u6846\u67b6\u3002\u4f5c\u8005\u5e0c\u671b\u5efa\u7acb\u4e25\u683c\u7684\u6570\u5b66\u5206\u6790\u6765\u7406\u89e3\u8fd9\u4e9b\u8fd1\u4f3c\u540e\u9a8c\u91c7\u6837\u5668\uff0c\u5e76\u63d0\u4f9b\u539f\u5219\u6027\u7684\u53c2\u6570\u8bbe\u8ba1\u65b9\u6cd5\u3002", "method": "\u5728\u5047\u8bbe\u5148\u9a8c\u4e3a\u9ad8\u65af\u5206\u5e03\u7684\u6761\u4ef6\u4e0b\uff0c\u63a8\u5bfc\u51fa\u7406\u60f3\u540e\u9a8c\u91c7\u6837\u5668\u548c\u6269\u6563\u91cd\u5efa\u7b97\u6cd5\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5728\u8c31\u57df\u8fdb\u884c\u7406\u8bba\u5206\u6790\u3002\u57fa\u4e8e\u8fd9\u4e9b\u8868\u793a\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u7684\u53c2\u6570\u8bbe\u8ba1\u6846\u67b6\uff0c\u66ff\u4ee3\u73b0\u6709\u7684\u542f\u53d1\u5f0f\u9009\u62e9\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u7684\u8c31\u57df\u5206\u6790\u65b9\u6cd5\u80fd\u591f\u4e3a\u4e0d\u540c\u7b97\u6cd5\u63d0\u4f9b\u5b9a\u5236\u5316\u7684\u53c2\u6570\u9009\u62e9\uff0c\u540c\u65f6\u8003\u8651\u5148\u9a8c\u7279\u6027\u3001\u9000\u5316\u4fe1\u53f7\u548c\u6269\u6563\u52a8\u6001\u3002\u7ed3\u679c\u663e\u793a\uff0c\u63a8\u8350\u7684\u53c2\u6570\u7ed3\u6784\u4e0e\u6807\u51c6\u542f\u53d1\u5f0f\u65b9\u6cd5\u4e0d\u540c\uff0c\u4e14\u968f\u6269\u6563\u6b65\u957f\u53d8\u5316\uff0c\u80fd\u591f\u5728\u611f\u77e5\u8d28\u91cf\u548c\u4fe1\u53f7\u4fdd\u771f\u5ea6\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u6269\u6563\u9006\u95ee\u9898\u6c42\u89e3\u5668\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u539f\u5219\u6027\u7684\u53c2\u6570\u8bbe\u8ba1\uff0c\u5c55\u793a\u4e86\u8c31\u57df\u5206\u6790\u5728\u7406\u89e3\u7b97\u6cd5\u884c\u4e3a\u548c\u4f18\u5316\u6027\u80fd\u65b9\u9762\u7684\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2602.07719", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07719", "abs": "https://arxiv.org/abs/2602.07719", "authors": ["Gabriel Stella"], "title": "Efficient Planning in Reinforcement Learning via Model Introspection", "comment": null, "summary": "Reinforcement learning and classical planning are typically seen as two distinct problems, with differing formulations necessitating different solutions. Yet, when humans are given a task, regardless of the way it is specified, they can often derive the additional information needed to solve the problem efficiently. The key to this ability is introspection: by reasoning about their internal models of the problem, humans directly synthesize additional task-relevant information. In this paper, we propose that this introspection can be thought of as program analysis. We discuss examples of how this approach can be applied to various kinds of models used in reinforcement learning. We then describe an algorithm that enables efficient goal-oriented planning over the class of models used in relational reinforcement learning, demonstrating a novel link between reinforcement learning and classical planning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u5185\u7701\u89c6\u4e3a\u7a0b\u5e8f\u5206\u6790\uff0c\u5efa\u7acb\u5f3a\u5316\u5b66\u4e60\u4e0e\u7ecf\u5178\u89c4\u5212\u4e4b\u95f4\u7684\u65b0\u8054\u7cfb\uff0c\u901a\u8fc7\u5206\u6790\u5185\u90e8\u6a21\u578b\u6765\u5408\u6210\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\uff0c\u63d0\u9ad8\u95ee\u9898\u89e3\u51b3\u6548\u7387\u3002", "motivation": "\u4eba\u7c7b\u5728\u9762\u5bf9\u4efb\u52a1\u65f6\uff0c\u65e0\u8bba\u4efb\u52a1\u5982\u4f55\u6307\u5b9a\uff0c\u90fd\u80fd\u901a\u8fc7\u5185\u7701\u63a8\u7406\u5185\u90e8\u6a21\u578b\u6765\u5408\u6210\u6240\u9700\u4fe1\u606f\u9ad8\u6548\u89e3\u51b3\u95ee\u9898\u3002\u5f3a\u5316\u5b66\u4e60\u548c\u7ecf\u5178\u89c4\u5212\u901a\u5e38\u88ab\u89c6\u4e3a\u4e24\u4e2a\u4e0d\u540c\u95ee\u9898\uff0c\u9700\u8981\u4e0d\u540c\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u7f3a\u4e4f\u8fd9\u79cd\u5185\u7701\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u5c06\u5185\u7701\u89c6\u4e3a\u7a0b\u5e8f\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u5e94\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u4e2d\u4f7f\u7528\u7684\u5404\u7c7b\u6a21\u578b\u3002\u7279\u522b\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7b97\u6cd5\uff0c\u80fd\u591f\u5728\u5173\u7cfb\u5f3a\u5316\u5b66\u4e60\u4f7f\u7528\u7684\u6a21\u578b\u7c7b\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u76ee\u6807\u5bfc\u5411\u89c4\u5212\u3002", "result": "\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u7a0b\u5e8f\u5206\u6790\u65b9\u6cd5\u5e94\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\uff0c\u5e76\u5f00\u53d1\u4e86\u5728\u5173\u7cfb\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u4e0a\u8fdb\u884c\u9ad8\u6548\u76ee\u6807\u5bfc\u5411\u89c4\u5212\u7684\u7b97\u6cd5\uff0c\u5efa\u7acb\u4e86\u5f3a\u5316\u5b66\u4e60\u4e0e\u7ecf\u5178\u89c4\u5212\u4e4b\u95f4\u7684\u65b0\u8054\u7cfb\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5185\u7701\u89c6\u4e3a\u7a0b\u5e8f\u5206\u6790\uff0c\u53ef\u4ee5\u5f25\u5408\u5f3a\u5316\u5b66\u4e60\u4e0e\u7ecf\u5178\u89c4\u5212\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u50cf\u4eba\u7c7b\u4e00\u6837\u901a\u8fc7\u5206\u6790\u5185\u90e8\u6a21\u578b\u6765\u5408\u6210\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u9ad8\u95ee\u9898\u89e3\u51b3\u6548\u7387\u3002"}}
{"id": "2602.07721", "categories": ["cs.LG", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.07721", "abs": "https://arxiv.org/abs/2602.07721", "authors": ["Yanlin Qi", "Xinhang Chen", "Huiqiang Jiang", "Qitong Wang", "Botao Peng", "Themis Palpanas"], "title": "ParisKV: Fast and Drift-Robust KV-Cache Retrieval for Long-Context LLMs", "comment": "25 pages, 16 figures. Under review", "summary": "KV-cache retrieval is essential for long-context LLM inference, yet existing methods struggle with distribution drift and high latency at scale. We introduce ParisKV, a drift-robust, GPU-native KV-cache retrieval framework based on collision-based candidate selection, followed by a quantized inner-product reranking estimator. For million-token contexts, ParisKV supports CPU-offloaded KV caches via Unified Virtual Addressing (UVA), enabling on-demand top-$k$ fetching with minimal overhead. ParisKV matches or outperforms full attention quality on long-input and long-generation benchmarks. It achieves state-of-the-art long-context decoding efficiency: it matches or exceeds full attention speed even at batch size 1 for long contexts, delivers up to 2.8$\\times$ higher throughput within full attention's runnable range, and scales to million-token contexts where full attention runs out of memory. At million-token scale, ParisKV reduces decode latency by 17$\\times$ and 44$\\times$ compared to MagicPIG and PQCache, respectively, two state-of-the-art KV-cache Top-$k$ retrieval baselines.", "AI": {"tldr": "ParisKV\u662f\u4e00\u4e2a\u9488\u5bf9\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u7684KV\u7f13\u5b58\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u78b0\u649e\u5019\u9009\u9009\u62e9\u548c\u91cf\u5316\u5185\u79ef\u91cd\u6392\uff0c\u5728\u767e\u4e07token\u89c4\u6a21\u4e0b\u5b9e\u73b0\u9ad8\u6548\u68c0\u7d22\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709KV\u7f13\u5b58\u68c0\u7d22\u65b9\u6cd5\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6\u9762\u4e34\u5206\u5e03\u6f02\u79fb\u548c\u9ad8\u5ef6\u8fdf\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e0b\u6027\u80fd\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u78b0\u649e\u5019\u9009\u9009\u62e9\uff0c\u7136\u540e\u4f7f\u7528\u91cf\u5316\u5185\u79ef\u91cd\u6392\u4f30\u8ba1\u5668\uff0c\u652f\u6301\u901a\u8fc7\u7edf\u4e00\u865a\u62df\u5bfb\u5740\u8fdb\u884cCPU\u5378\u8f7d\u7684KV\u7f13\u5b58\uff0c\u5b9e\u73b0\u6309\u9700top-k\u83b7\u53d6\u3002", "result": "\u5728\u957f\u8f93\u5165\u548c\u957f\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5339\u914d\u6216\u4f18\u4e8e\u5b8c\u6574\u6ce8\u610f\u529b\u8d28\u91cf\uff0c\u5728\u6279\u5927\u5c0f\u4e3a1\u65f6\u5339\u914d\u6216\u8d85\u8fc7\u5b8c\u6574\u6ce8\u610f\u529b\u901f\u5ea6\uff0c\u541e\u5410\u91cf\u63d0\u9ad82.8\u500d\uff0c\u652f\u6301\u767e\u4e07token\u4e0a\u4e0b\u6587\uff0c\u76f8\u6bd4MagicPIG\u548cPQCache\u5206\u522b\u51cf\u5c1117\u500d\u548c44\u500d\u5ef6\u8fdf\u3002", "conclusion": "ParisKV\u662f\u4e00\u4e2a\u6f02\u79fb\u9c81\u68d2\u7684GPU\u539f\u751fKV\u7f13\u5b58\u68c0\u7d22\u6846\u67b6\uff0c\u5728\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u89e3\u7801\u6548\u7387\uff0c\u80fd\u591f\u6269\u5c55\u5230\u5b8c\u6574\u6ce8\u610f\u529b\u65e0\u6cd5\u5904\u7406\u7684\u5927\u89c4\u6a21\u573a\u666f\u3002"}}
{"id": "2602.07729", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07729", "abs": "https://arxiv.org/abs/2602.07729", "authors": ["Sagnik Mukherjee", "Lifan Yuan", "Pavan Jayasinha", "Dilek Hakkani-T\u00fcr", "Hao Peng"], "title": "Do We Need Adam? Surprisingly Strong and Sparse Reinforcement Learning with SGD in LLMs", "comment": null, "summary": "Reinforcement learning (RL), particularly RL from verifiable reward (RLVR), has become a crucial phase of training large language models (LLMs) and a key focus of current scaling efforts. However, optimization practices in RL largely follow those of next-token prediction stages (e.g., pretraining and supervised fine-tuning), despite fundamental differences between RL and these stages highlighted by recent work. One such practice is the use of the AdamW optimizer, which is widely adopted for training large-scale transformers despite its high memory overhead. Our analysis shows that both momentum and adaptive learning rates in AdamW are less influential in RL than in SFT, leading us to hypothesize that RL benefits less from Adam-style per-parameter adaptive learning rates and momentum. Confirming this hypothesis, our experiments demonstrate that the substantially more memory-efficient SGD, which is known to perform poorly in supervised learning of large-scale transformers, matches or even outperforms AdamW in RL for LLMs. Remarkably, full fine-tuning with SGD updates fewer than 0.02% of model parameters without any sparsity-promoting regularization, more than 1000 times fewer than AdamW. Our analysis offers potential reasons for this update sparsity. These findings provide new insights into the optimization dynamics of RL in LLMs and show that RL can be substantially more parameter-efficient than previously recognized.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\uff0c\u7b80\u5355\u7684SGD\u4f18\u5316\u5668\u6bd4\u5e7f\u6cdb\u4f7f\u7528\u7684AdamW\u8868\u73b0\u66f4\u597d\uff0c\u4e14\u53c2\u6570\u66f4\u65b0\u7a00\u758f\u5ea6\u6781\u9ad8\uff08\u4ec5\u66f4\u65b0<0.02%\u7684\u53c2\u6570\uff09\uff0c\u6bd4AdamW\u8282\u77011000\u500d\u4ee5\u4e0a\u53c2\u6570\u66f4\u65b0\u3002", "motivation": "\u5f53\u524dLLM\u7684\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u4ecd\u6cbf\u7528\u9884\u8bad\u7ec3\u9636\u6bb5\u7684AdamW\u4f18\u5316\u5668\uff0c\u4f46RL\u4e0e\u76d1\u7763\u5b66\u4e60\u5b58\u5728\u672c\u8d28\u5dee\u5f02\u3002AdamW\u5185\u5b58\u5f00\u9500\u5927\uff0c\u800c\u7814\u7a76\u8868\u660eRL\u53ef\u80fd\u4e0d\u9700\u8981Adam\u7684\u52a8\u91cf\u673a\u5236\u548c\u81ea\u9002\u5e94\u5b66\u4e60\u7387\uff0c\u56e0\u6b64\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5206\u6790AdamW\u5728RL\u548cSFT\u4e2d\u7684\u4e0d\u540c\u5f71\u54cd\uff0c\u63d0\u51fa\u5047\u8bbe\uff1aRL\u53d7\u76ca\u4e8eAdam\u5f0f\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u548c\u52a8\u91cf\u7684\u7a0b\u5ea6\u8f83\u4f4e\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4f7f\u7528\u5185\u5b58\u6548\u7387\u66f4\u9ad8\u7684SGD\u4f18\u5316\u5668\u5728LLM RL\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u4e0eAdamW\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "SGD\u5728LLM\u5f3a\u5316\u5b66\u4e60\u4e2d\u5339\u914d\u751a\u81f3\u4f18\u4e8eAdamW\u7684\u8868\u73b0\u3002SGD\u5168\u5fae\u8c03\u4ec5\u66f4\u65b0\u4e0d\u52300.02%\u7684\u6a21\u578b\u53c2\u6570\uff0c\u6bd4AdamW\u5c111000\u500d\u4ee5\u4e0a\uff0c\u4e14\u65e0\u9700\u7a00\u758f\u6b63\u5219\u5316\u3002\u5206\u6790\u63d0\u4f9b\u4e86\u8fd9\u79cd\u66f4\u65b0\u7a00\u758f\u6027\u7684\u53ef\u80fd\u539f\u56e0\u3002", "conclusion": "RL\u5728LLM\u4e2d\u7684\u4f18\u5316\u52a8\u6001\u4e0e\u76d1\u7763\u5b66\u4e60\u4e0d\u540c\uff0cSGD\u5728RL\u4e2d\u8868\u73b0\u4f18\u5f02\u4e14\u53c2\u6570\u6548\u7387\u6781\u9ad8\u3002\u8fd9\u8868\u660eRL\u53ef\u4ee5\u6bd4\u4e4b\u524d\u8ba4\u77e5\u7684\u66f4\u53c2\u6570\u9ad8\u6548\uff0c\u4e3aLLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u89c1\u89e3\u3002"}}
{"id": "2602.07730", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07730", "abs": "https://arxiv.org/abs/2602.07730", "authors": ["Siddarth Chandrasekar", "Marlos C. Machado"], "title": "The Laplacian Keyboard: Beyond the Linear Span", "comment": "28 pages, 17 figures", "summary": "Across scientific disciplines, Laplacian eigenvectors serve as a fundamental basis for simplifying complex systems, from signal processing to quantum mechanics. In reinforcement learning (RL), these eigenvectors provide a natural basis for approximating reward functions; however, their use is typically limited to their linear span, which restricts expressivity in complex environments. We introduce the Laplacian Keyboard (LK), a hierarchical framework that goes beyond the linear span. LK constructs a task-agnostic library of options from these eigenvectors, forming a behavior basis guaranteed to contain the optimal policy for any reward within the linear span. A meta-policy learns to stitch these options dynamically, enabling efficient learning of policies outside the original linear constraints. We establish theoretical bounds on zero-shot approximation error and demonstrate empirically that LK surpasses zero-shot solutions while achieving improved sample efficiency compared to standard RL methods.", "AI": {"tldr": "Laplacian Keyboard (LK) \u662f\u4e00\u4e2a\u5206\u5c42\u6846\u67b6\uff0c\u5229\u7528\u62c9\u666e\u62c9\u65af\u7279\u5f81\u5411\u91cf\u6784\u5efa\u4efb\u52a1\u65e0\u5173\u7684\u884c\u4e3a\u57fa\uff0c\u901a\u8fc7\u5143\u7b56\u7565\u52a8\u6001\u7ec4\u5408\u8fd9\u4e9b\u884c\u4e3a\uff0c\u8d85\u8d8a\u7ebf\u6027\u7ea6\u675f\u5b9e\u73b0\u9ad8\u6548\u7b56\u7565\u5b66\u4e60\u3002", "motivation": "\u62c9\u666e\u62c9\u65af\u7279\u5f81\u5411\u91cf\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u901a\u5e38\u53ea\u7528\u4e8e\u7ebf\u6027\u8fd1\u4f3c\u5956\u52b1\u51fd\u6570\uff0c\u8fd9\u9650\u5236\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\u3002\u9700\u8981\u8d85\u8d8a\u7ebf\u6027\u7ea6\u675f\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u7b56\u7565\u5b66\u4e60\u7684\u6548\u7387\u548c\u8868\u8fbe\u80fd\u529b\u3002", "method": "LK\u6846\u67b6\u4ece\u62c9\u666e\u62c9\u65af\u7279\u5f81\u5411\u91cf\u6784\u5efa\u4efb\u52a1\u65e0\u5173\u7684\u884c\u4e3a\u57fa\uff08\u9009\u9879\u5e93\uff09\uff0c\u4fdd\u8bc1\u5305\u542b\u7ebf\u6027\u7a7a\u95f4\u5185\u4efb\u4f55\u5956\u52b1\u7684\u6700\u4f18\u7b56\u7565\u3002\u901a\u8fc7\u5143\u7b56\u7565\u52a8\u6001\u7ec4\u5408\u8fd9\u4e9b\u9009\u9879\uff0c\u5b66\u4e60\u8d85\u51fa\u539f\u59cb\u7ebf\u6027\u7ea6\u675f\u7684\u7b56\u7565\u3002", "result": "\u7406\u8bba\u5206\u6790\u5efa\u7acb\u4e86\u96f6\u6837\u672c\u8fd1\u4f3c\u8bef\u5dee\u7684\u754c\u9650\u3002\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cLK\u8d85\u8d8a\u4e86\u96f6\u6837\u672c\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u76f8\u6bd4\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6837\u672c\u6548\u7387\u3002", "conclusion": "Laplacian Keyboard \u901a\u8fc7\u5229\u7528\u62c9\u666e\u62c9\u65af\u7279\u5f81\u5411\u91cf\u6784\u5efa\u884c\u4e3a\u57fa\u5e76\u52a8\u6001\u7ec4\u5408\uff0c\u6210\u529f\u8d85\u8d8a\u4e86\u7ebf\u6027\u7ea6\u675f\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2602.07732", "categories": ["cs.LG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.07732", "abs": "https://arxiv.org/abs/2602.07732", "authors": ["Joon Suk Huh"], "title": "Efficient Adaptive Data Analysis over Dense Distributions", "comment": "23 pages", "summary": "Modern data workflows are inherently adaptive, repeatedly querying the same dataset to refine and validate sequential decisions, but such adaptivity can lead to overfitting and invalid statistical inference. Adaptive Data Analysis (ADA) mechanisms address this challenge; however, there is a fundamental tension between computational efficiency and sample complexity. For $T$ rounds of adaptive analysis, computationally efficient algorithms typically incur suboptimal $O(\\sqrt{T})$ sample complexity, whereas statistically optimal $O(\\log T)$ algorithms are computationally intractable under standard cryptographic assumptions. In this work, we shed light on this trade-off by identifying a natural class of data distributions under which both computational efficiency and optimal sample complexity are achievable. We propose a computationally efficient ADA mechanism that attains optimal $O(\\log T)$ sample complexity when the data distribution is dense with respect to a known prior. This setting includes, in particular, feature--label data distributions arising in distribution-specific learning. As a consequence, our mechanism also yields a sample-efficient (i.e., $O(\\log T)$ samples) statistical query oracle in the distribution-specific setting. Moreover, although our algorithm is not based on differential privacy, it satisfies a relaxed privacy notion known as Predicate Singling Out (PSO) security (Cohen and Nissim, 2020). Our results thus reveal an inherent connection between adaptive data analysis and privacy beyond differential privacy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6570\u636e\u5206\u5e03\u7a20\u5bc6\u6761\u4ef6\u4e0b\uff0c\u540c\u65f6\u5b9e\u73b0\u8ba1\u7b97\u6548\u7387\u548c\u6700\u4f18\u6837\u672c\u590d\u6742\u5ea6\u7684\u81ea\u9002\u5e94\u6570\u636e\u5206\u6790\u673a\u5236\uff0c\u5c06\u6837\u672c\u590d\u6742\u5ea6\u4eceO(\u221aT)\u63d0\u5347\u5230O(log T)\u3002", "motivation": "\u73b0\u4ee3\u6570\u636e\u5de5\u4f5c\u6d41\u5177\u6709\u81ea\u9002\u5e94\u7279\u6027\uff0c\u53cd\u590d\u67e5\u8be2\u540c\u4e00\u6570\u636e\u96c6\u4ee5\u4f18\u5316\u51b3\u7b56\uff0c\u4f46\u8fd9\u79cd\u81ea\u9002\u5e94\u6027\u4f1a\u5bfc\u81f4\u8fc7\u62df\u5408\u548c\u7edf\u8ba1\u63a8\u65ad\u5931\u6548\u3002\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u8ba1\u7b97\u6548\u7387\u4e0e\u6837\u672c\u590d\u6742\u5ea6\u4e4b\u95f4\u7684\u6839\u672c\u77db\u76fe\uff1a\u8ba1\u7b97\u9ad8\u6548\u7684\u7b97\u6cd5\u9700\u8981O(\u221aT)\u6837\u672c\uff0c\u800c\u7edf\u8ba1\u6700\u4f18\u7684O(log T)\u7b97\u6cd5\u8ba1\u7b97\u4e0a\u4e0d\u53ef\u884c\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u81ea\u9002\u5e94\u6570\u636e\u5206\u6790\u673a\u5236\uff0c\u5f53\u6570\u636e\u5206\u5e03\u76f8\u5bf9\u4e8e\u5df2\u77e5\u5148\u9a8c\u5206\u5e03\u662f\u7a20\u5bc6\u7684\u65f6\uff0c\u80fd\u591f\u8fbe\u5230\u6700\u4f18\u7684O(log T)\u6837\u672c\u590d\u6742\u5ea6\u3002\u8be5\u65b9\u6cd5\u7279\u522b\u9002\u7528\u4e8e\u5206\u5e03\u7279\u5b9a\u5b66\u4e60\u4e2d\u7684\u7279\u5f81-\u6807\u7b7e\u6570\u636e\u5206\u5e03\uff0c\u5e76\u6ee1\u8db3\u8c13\u8bcd\u5355\u70b9\u8bc6\u522b(PSO)\u5b89\u5168\u6027\u3002", "result": "\u5728\u6570\u636e\u5206\u5e03\u7a20\u5bc6\u7684\u6761\u4ef6\u4e0b\uff0c\u5b9e\u73b0\u4e86\u540c\u65f6\u5177\u5907\u8ba1\u7b97\u6548\u7387\u548c\u6700\u4f18\u6837\u672c\u590d\u6742\u5ea6(O(log T))\u7684\u81ea\u9002\u5e94\u6570\u636e\u5206\u6790\u673a\u5236\u3002\u8be5\u673a\u5236\u8fd8\u80fd\u5728\u5206\u5e03\u7279\u5b9a\u8bbe\u7f6e\u4e0b\u63d0\u4f9b\u6837\u672c\u9ad8\u6548\u7684\u7edf\u8ba1\u67e5\u8be2\u9884\u8a00\u673a\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u81ea\u9002\u5e94\u6570\u636e\u5206\u6790\u4e0e\u9690\u79c1\u4fdd\u62a4\u4e4b\u95f4\u7684\u5185\u5728\u8054\u7cfb\uff0c\u8d85\u8d8a\u4e86\u5dee\u5206\u9690\u79c1\u7684\u8303\u7574\u3002\u5728\u7279\u5b9a\u6570\u636e\u5206\u5e03\u6761\u4ef6\u4e0b\uff0c\u53ef\u4ee5\u540c\u65f6\u5b9e\u73b0\u8ba1\u7b97\u6548\u7387\u548c\u7edf\u8ba1\u6700\u4f18\u6027\uff0c\u89e3\u51b3\u4e86\u8be5\u9886\u57df\u957f\u671f\u5b58\u5728\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2602.07735", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2602.07735", "abs": "https://arxiv.org/abs/2602.07735", "authors": ["Matteo Rossi", "Ryan Pederson", "Miles Wang-Henderson", "Ben Kaufman", "Edward C. Williams", "Carl Underkoffler", "Owen Lewis Howell", "Adrian Layer", "Stephan Thaler", "Narbe Mardirossian", "John Anthony Parkhill"], "title": "TerraBind: Fast and Accurate Binding Affinity Prediction through Coarse Structural Representations", "comment": "31 pages, 14 figures", "summary": "We present TerraBind, a foundation model for protein-ligand structure and binding affinity prediction that achieves 26-fold faster inference than state-of-the-art methods while improving affinity prediction accuracy by $\\sim$20\\%. Current deep learning approaches to structure-based drug design rely on expensive all-atom diffusion to generate 3D coordinates, creating inference bottlenecks that render large-scale compound screening computationally intractable. We challenge this paradigm with a critical hypothesis: full all-atom resolution is unnecessary for accurate small molecule pose and binding affinity prediction. TerraBind tests this hypothesis through a coarse pocket-level representation (protein C$_\u03b2$ atoms and ligand heavy atoms only) within a multimodal architecture combining COATI-3 molecular encodings and ESM-2 protein embeddings that learns rich structural representations, which are used in a diffusion-free optimization module for pose generation and a binding affinity likelihood prediction module. On structure prediction benchmarks (FoldBench, PoseBusters, Runs N' Poses), TerraBind matches diffusion-based baselines in ligand pose accuracy. Crucially, TerraBind outperforms Boltz-2 by $\\sim$20\\% in Pearson correlation for binding affinity prediction on both a public benchmark (CASP16) and a diverse proprietary dataset (18 biochemical/cell assays). We show that the affinity prediction module also provides well-calibrated affinity uncertainty estimates, addressing a critical gap in reliable compound prioritization for drug discovery. Furthermore, this module enables a continual learning framework and a hedged batch selection strategy that, in simulated drug discovery cycles, achieves 6$\\times$ greater affinity improvement of selected molecules over greedy-based approaches.", "AI": {"tldr": "TerraBind\u662f\u4e00\u4e2a\u86cb\u767d\u8d28-\u914d\u4f53\u7ed3\u6784\u548c\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u7684\u57fa\u7840\u6a21\u578b\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u63a8\u7406\u901f\u5ea6\u5feb26\u500d\uff0c\u4eb2\u548c\u529b\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u9ad8\u7ea620%\uff0c\u91c7\u7528\u7c97\u7c92\u5ea6\u53e3\u888b\u8868\u793a\u548c\u514d\u6269\u6563\u4f18\u5316\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u836f\u7269\u8bbe\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u5168\u539f\u5b50\u6269\u6563\u6765\u751f\u62103D\u5750\u6807\uff0c\u5bfc\u81f4\u63a8\u7406\u74f6\u9888\uff0c\u4f7f\u5927\u89c4\u6a21\u5316\u5408\u7269\u7b5b\u9009\u8ba1\u7b97\u4e0a\u4e0d\u53ef\u884c\u3002\u4f5c\u8005\u63d0\u51fa\u5173\u952e\u5047\u8bbe\uff1a\u5168\u539f\u5b50\u5206\u8fa8\u7387\u5bf9\u4e8e\u51c6\u786e\u7684\u5c0f\u5206\u5b50\u6784\u8c61\u548c\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u662f\u4e0d\u5fc5\u8981\u7684\u3002", "method": "\u91c7\u7528\u7c97\u7c92\u5ea6\u53e3\u888b\u7ea7\u8868\u793a\uff08\u4ec5\u86cb\u767d\u8d28C\u03b2\u539f\u5b50\u548c\u914d\u4f53\u91cd\u539f\u5b50\uff09\uff0c\u7ed3\u5408COATI-3\u5206\u5b50\u7f16\u7801\u548cESM-2\u86cb\u767d\u8d28\u5d4c\u5165\u7684\u591a\u6a21\u6001\u67b6\u6784\uff0c\u5b66\u4e60\u4e30\u5bcc\u7684\u7ed3\u6784\u8868\u793a\u3002\u4f7f\u7528\u514d\u6269\u6563\u4f18\u5316\u6a21\u5757\u8fdb\u884c\u6784\u8c61\u751f\u6210\u548c\u7ed3\u5408\u4eb2\u548c\u529b\u4f3c\u7136\u9884\u6d4b\u6a21\u5757\u3002", "result": "\u5728\u7ed3\u6784\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTerraBind\u5728\u914d\u4f53\u6784\u8c61\u51c6\u786e\u6027\u4e0a\u4e0e\u57fa\u4e8e\u6269\u6563\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5f53\u3002\u5728\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u65b9\u9762\uff0c\u6bd4Boltz-2\u5728Pearson\u76f8\u5173\u6027\u4e0a\u63d0\u9ad8\u7ea620%\u3002\u4eb2\u548c\u529b\u9884\u6d4b\u6a21\u5757\u8fd8\u63d0\u4f9b\u826f\u597d\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5e76\u901a\u8fc7\u6301\u7eed\u5b66\u4e60\u6846\u67b6\u548chedged\u6279\u91cf\u9009\u62e9\u7b56\u7565\uff0c\u5728\u6a21\u62df\u836f\u7269\u53d1\u73b0\u5468\u671f\u4e2d\u5b9e\u73b0\u6bd4\u8d2a\u5a6a\u65b9\u6cd5\u9ad86\u500d\u7684\u4eb2\u548c\u529b\u6539\u8fdb\u3002", "conclusion": "TerraBind\u8bc1\u660e\u4e86\u7c97\u7c92\u5ea6\u8868\u793a\u5728\u86cb\u767d\u8d28-\u914d\u4f53\u7ed3\u6784\u548c\u4eb2\u548c\u529b\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u4e3a\u5927\u89c4\u6a21\u5316\u5408\u7269\u7b5b\u9009\u63d0\u4f9b\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002"}}
{"id": "2602.07738", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07738", "abs": "https://arxiv.org/abs/2602.07738", "authors": ["Sunil Madhow", "Yuchen Liang", "Ness Shroff", "Yingbin Liang", "Yu-Xiang Wang"], "title": "Learnable Chernoff Baselines for Inference-Time Alignment", "comment": null, "summary": "We study inference-time reward-guided alignment for generative models. Existing methods often rely on either architecture-specific adaptations or computationally costly inference procedures. We introduce Learnable Chernoff Baselines (LCBs) as a method for efficiently and approximately sampling from the exponentially tilted kernels that arise from KL-regularized reward alignment. Using only black-box sampling access to the pretrained model, LCBs implement a form of rejection sampling with adaptively selected acceptance probabilities, which allows fine-grained control over inference-compute scaling. We establish total-variation guarantees to the ideal aligned model, and demonstrate in both continuous and discrete diffusion settings that LCB sampling closely matches ideal rejection sampling while using substantially fewer queries to the pretrained model.", "AI": {"tldr": "\u63d0\u51faLCB\u65b9\u6cd5\u7528\u4e8e\u63a8\u7406\u65f6\u5956\u52b1\u5f15\u5bfc\u5bf9\u9f50\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684Chernoff\u57fa\u7ebf\u5b9e\u73b0\u8fd1\u4f3c\u91c7\u6837\uff0c\u76f8\u6bd4\u7406\u60f3\u62d2\u7edd\u91c7\u6837\u663e\u8457\u51cf\u5c11\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u67e5\u8be2\u6b21\u6570", "motivation": "\u73b0\u6709\u63a8\u7406\u65f6\u5956\u52b1\u5f15\u5bfc\u5bf9\u9f50\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u7279\u5b9a\u67b6\u6784\u9002\u914d\uff0c\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u4eceKL\u6b63\u5219\u5316\u5956\u52b1\u5bf9\u9f50\u4ea7\u751f\u7684\u6307\u6570\u503e\u659c\u6838\u4e2d\u91c7\u6837", "method": "\u5f15\u5165\u53ef\u5b66\u4e60\u7684Chernoff\u57fa\u7ebf(LCB)\u65b9\u6cd5\uff0c\u4ec5\u9700\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u9ed1\u76d2\u91c7\u6837\u8bbf\u95ee\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9009\u62e9\u63a5\u53d7\u6982\u7387\u7684\u62d2\u7edd\u91c7\u6837\u5b9e\u73b0\uff0c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u63a8\u7406-\u8ba1\u7b97\u7f29\u653e\u63a7\u5236", "result": "\u5efa\u7acb\u4e86\u4e0e\u7406\u60f3\u5bf9\u9f50\u6a21\u578b\u7684\u603b\u53d8\u5dee\u4fdd\u8bc1\uff0c\u5728\u8fde\u7eed\u548c\u79bb\u6563\u6269\u6563\u8bbe\u7f6e\u4e2d\uff0cLCB\u91c7\u6837\u4e0e\u7406\u60f3\u62d2\u7edd\u91c7\u6837\u5339\u914d\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u67e5\u8be2\u6b21\u6570", "conclusion": "LCB\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u8fd1\u4f3c\u7684\u63a8\u7406\u65f6\u5956\u52b1\u5bf9\u9f50\u65b9\u6cd5\uff0c\u4ec5\u9700\u9ed1\u76d2\u91c7\u6837\u8bbf\u95ee\uff0c\u5728\u4fdd\u6301\u91c7\u6837\u8d28\u91cf\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c"}}
{"id": "2602.07744", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07744", "abs": "https://arxiv.org/abs/2602.07744", "authors": ["Dongyeop Woo", "Marta Skreta", "Seonghyun Park", "Sungsoo Ahn", "Kirill Neklyudov"], "title": "Riemannian MeanFlow", "comment": null, "summary": "Diffusion and flow models have become the dominant paradigm for generative modeling on Riemannian manifolds, with successful applications in protein backbone generation and DNA sequence design. However, these methods require tens to hundreds of neural network evaluations at inference time, which can become a computational bottleneck in large-scale scientific sampling workflows. We introduce Riemannian MeanFlow~(RMF), a framework for learning flow maps directly on manifolds, enabling high-quality generations with as few as one forward pass. We derive three equivalent characterizations of the manifold average velocity (Eulerian, Lagrangian, and semigroup identities), and analyze parameterizations and stabilization techniques to improve training on high-dimensional manifolds. In promoter DNA design and protein backbone generation settings, RMF achieves comparable sample quality to prior methods while requiring up to 10$\\times$ fewer function evaluations. Finally, we show that few-step flow maps enable efficient reward-guided design through reward look-ahead, where terminal states can be predicted from intermediate steps at minimal additional cost.", "AI": {"tldr": "Riemannian MeanFlow (RMF) \u662f\u4e00\u79cd\u5728\u6d41\u5f62\u4e0a\u76f4\u63a5\u5b66\u4e60\u6d41\u6620\u5c04\u7684\u6846\u67b6\uff0c\u53ea\u9700\u4e00\u6b21\u524d\u5411\u4f20\u9012\u5373\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u6837\u672c\uff0c\u76f8\u6bd4\u6269\u6563\u6a21\u578b\u51cf\u5c1110\u500d\u8ba1\u7b97\u91cf\u3002", "motivation": "\u5f53\u524d\u5728\u9ece\u66fc\u6d41\u5f62\u4e0a\u7684\u6269\u6563\u548c\u6d41\u6a21\u578b\u9700\u8981\u6570\u5341\u5230\u6570\u767e\u6b21\u795e\u7ecf\u7f51\u7edc\u8bc4\u4f30\uff0c\u5728\u5927\u89c4\u6a21\u79d1\u5b66\u91c7\u6837\u5de5\u4f5c\u6d41\u4e2d\u6210\u4e3a\u8ba1\u7b97\u74f6\u9888\u3002", "method": "\u63d0\u51faRiemannian MeanFlow\u6846\u67b6\uff0c\u63a8\u5bfc\u4e86\u6d41\u5f62\u5e73\u5747\u901f\u5ea6\u7684\u4e09\u79cd\u7b49\u4ef7\u8868\u5f81\uff08\u6b27\u62c9\u3001\u62c9\u683c\u6717\u65e5\u548c\u534a\u7fa4\u6052\u7b49\u5f0f\uff09\uff0c\u5206\u6790\u4e86\u9ad8\u7ef4\u6d41\u5f62\u4e0a\u7684\u53c2\u6570\u5316\u548c\u7a33\u5b9a\u5316\u6280\u672f\u3002", "result": "\u5728\u542f\u52a8\u5b50DNA\u8bbe\u8ba1\u548c\u86cb\u767d\u8d28\u9aa8\u67b6\u751f\u6210\u4efb\u52a1\u4e2d\uff0cRMF\u8fbe\u5230\u4e0e\u5148\u524d\u65b9\u6cd5\u76f8\u5f53\u7684\u6837\u672c\u8d28\u91cf\uff0c\u540c\u65f6\u51cf\u5c11\u9ad8\u8fbe10\u500d\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\uff1b\u5c11\u6b65\u6d41\u6620\u5c04\u901a\u8fc7\u5956\u52b1\u524d\u77bb\u5b9e\u73b0\u9ad8\u6548\u5956\u52b1\u5f15\u5bfc\u8bbe\u8ba1\u3002", "conclusion": "RMF\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u9ece\u66fc\u6d41\u5f62\u4e0a\u751f\u6210\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u79d1\u5b66\u91c7\u6837\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07764", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07764", "abs": "https://arxiv.org/abs/2602.07764", "authors": ["Tanmay Ambadkar", "Sourav Panda", "Shreyash Kale", "Jonathan Dodge", "Abhinav Verma"], "title": "Preference Conditioned Multi-Objective Reinforcement Learning: Decomposed, Diversity-Driven Policy Optimization", "comment": null, "summary": "Multi-objective reinforcement learning (MORL) seeks to learn policies that balance multiple, often conflicting objectives. Although a single preference-conditioned policy is the most flexible and scalable solution, existing approaches remain brittle in practice, frequently failing to recover complete Pareto fronts. We show that this failure stems from two structural issues in current methods: destructive gradient interference caused by premature scalarization and representational collapse across the preference space. We introduce $D^3PO$, a PPO-based framework that reorganizes multi-objective policy optimization to address these issues directly. $D^3PO$ preserves per-objective learning signals through a decomposed optimization pipeline and integrates preferences only after stabilization, enabling reliable credit assignment. In addition, a scaled diversity regularizer enforces sensitivity of policy behavior to preference changes, preventing collapse. Across standard MORL benchmarks, including high-dimensional and many-objective control tasks, $D^3PO$ consistently discovers broader and higher-quality Pareto fronts than prior single- and multi-policy methods, matching or exceeding state-of-the-art hypervolume and expected utility while using a single deployable policy.", "AI": {"tldr": "D\u00b3PO\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u4f18\u5316\u6d41\u7a0b\u548c\u591a\u6837\u6027\u6b63\u5219\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u68af\u5ea6\u5e72\u6270\u548c\u8868\u793a\u5d29\u6e83\u7684\u95ee\u9898\uff0c\u80fd\u591f\u7528\u5355\u4e2a\u7b56\u7565\u5b66\u4e60\u66f4\u9ad8\u8d28\u91cf\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "motivation": "\u5f53\u524d\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u867d\u7136\u5355\u4e2a\u504f\u597d\u6761\u4ef6\u7b56\u7565\u662f\u6700\u7075\u6d3b\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u8df5\u4e2d\u4ecd\u7136\u8106\u5f31\uff0c\u7ecf\u5e38\u65e0\u6cd5\u6062\u590d\u5b8c\u6574\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u3002\u8fd9\u4e3b\u8981\u6e90\u4e8e\u4e24\u4e2a\u7ed3\u6784\u6027\u95ee\u9898\uff1a\u8fc7\u65e9\u6807\u91cf\u5316\u5bfc\u81f4\u7684\u7834\u574f\u6027\u68af\u5ea6\u5e72\u6270\uff0c\u4ee5\u53ca\u504f\u597d\u7a7a\u95f4\u4e2d\u7684\u8868\u793a\u5d29\u6e83\u3002", "method": "D\u00b3PO\u57fa\u4e8ePPO\u6846\u67b6\u91cd\u65b0\u7ec4\u7ec7\u4e86\u591a\u76ee\u6807\u7b56\u7565\u4f18\u5316\u3002\u5b83\u901a\u8fc7\u5206\u89e3\u4f18\u5316\u6d41\u7a0b\u4fdd\u7559\u6bcf\u4e2a\u76ee\u6807\u7684\u5b66\u4e60\u4fe1\u53f7\uff0c\u4ec5\u5728\u7a33\u5b9a\u540e\u96c6\u6210\u504f\u597d\uff0c\u5b9e\u73b0\u53ef\u9760\u7684\u4fe1\u7528\u5206\u914d\u3002\u540c\u65f6\uff0c\u4f7f\u7528\u7f29\u653e\u591a\u6837\u6027\u6b63\u5219\u5316\u5668\u5f3a\u5236\u7b56\u7565\u884c\u4e3a\u5bf9\u504f\u597d\u53d8\u5316\u4fdd\u6301\u654f\u611f\uff0c\u9632\u6b62\u8868\u793a\u5d29\u6e83\u3002", "result": "\u5728\u6807\u51c6MORL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5305\u62ec\u9ad8\u7ef4\u548c\u591a\u76ee\u6807\u63a7\u5236\u4efb\u52a1\uff0cD\u00b3PO\u59cb\u7ec8\u6bd4\u5148\u524d\u7684\u5355\u7b56\u7565\u548c\u591a\u7b56\u7565\u65b9\u6cd5\u53d1\u73b0\u66f4\u5e7f\u6cdb\u548c\u66f4\u9ad8\u8d28\u91cf\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u5728\u8d85\u4f53\u79ef\u548c\u671f\u671b\u6548\u7528\u65b9\u9762\u5339\u914d\u6216\u8d85\u8fc7\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u540c\u65f6\u4f7f\u7528\u5355\u4e2a\u53ef\u90e8\u7f72\u7b56\u7565\u3002", "conclusion": "D\u00b3PO\u901a\u8fc7\u89e3\u51b3\u68af\u5ea6\u5e72\u6270\u548c\u8868\u793a\u5d29\u6e83\u8fd9\u4e24\u4e2a\u6838\u5fc3\u7ed3\u6784\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5355\u7b56\u7565\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07790", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07790", "abs": "https://arxiv.org/abs/2602.07790", "authors": ["Wanyun Xie", "Francesco Tonin", "Volkan Cevher"], "title": "MaD-Mix: Multi-Modal Data Mixtures via Latent Space Coupling for Vision-Language Model Training", "comment": null, "summary": "Vision-Language Models (VLMs) are typically trained on a diverse set of multi-modal domains, yet current practices rely on costly manual tuning. We propose MaD-Mix, a principled and computationally efficient framework that derives multi-modal data mixtures for VLM training. MaD-Mix formulates data mixing as modality-aware domain alignment maximization and obtains closed-form multi-modal alignment scores from the Fenchel dual through inter-modal coupling variables. MaD-Mix systematically handles domains with missing modalities, allowing for the integration of language-only domains. Empirical evaluations across 0.5B and 7B models demonstrate that MaD-Mix accelerates VLM training across diverse benchmarks. MaD-Mix matches human-tuned data mixtures using 22% fewer training steps in image-text instruction tuning. In complex tri-modal video-image-text scenarios, where manual tuning becomes impractical, MaD-Mix boosts average accuracy over uniform weights, with negligible mixture computation overhead (< 1 GPU-hour), enabling scalable mixture design for modern VLM pipelines.", "AI": {"tldr": "MaD-Mix\u662f\u4e00\u4e2a\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u6570\u636e\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u6001\u611f\u77e5\u7684\u57df\u5bf9\u9f50\u6700\u5927\u5316\u6765\u81ea\u52a8\u4f18\u5316\u6570\u636e\u6df7\u5408\u6bd4\u4f8b\uff0c\u65e0\u9700\u4eba\u5de5\u8c03\u4f18\uff0c\u663e\u8457\u52a0\u901f\u8bad\u7ec3\u8fc7\u7a0b\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u8c03\u4f18\u6765\u786e\u5b9a\u591a\u6a21\u6001\u6570\u636e\u6df7\u5408\u6bd4\u4f8b\uff0c\u8fd9\u5728\u590d\u6742\u7684\u4e09\u6a21\u6001\uff08\u89c6\u9891-\u56fe\u50cf-\u6587\u672c\uff09\u573a\u666f\u4e2d\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u3001\u9ad8\u6548\u7684\u6570\u636e\u6df7\u5408\u65b9\u6cd5\u3002", "method": "\u5c06\u6570\u636e\u6df7\u5408\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u6a21\u6001\u611f\u77e5\u7684\u57df\u5bf9\u9f50\u6700\u5927\u5316\uff0c\u901a\u8fc7Fenchel\u5bf9\u5076\u548c\u8de8\u6a21\u6001\u8026\u5408\u53d8\u91cf\u83b7\u5f97\u95ed\u5f0f\u591a\u6a21\u6001\u5bf9\u9f50\u5206\u6570\uff0c\u7cfb\u7edf\u5904\u7406\u7f3a\u5931\u6a21\u6001\u7684\u57df\uff0c\u5141\u8bb8\u6574\u5408\u7eaf\u8bed\u8a00\u57df\u3002", "result": "\u57280.5B\u548c7B\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMaD-Mix\u5728\u56fe\u50cf-\u6587\u672c\u6307\u4ee4\u8c03\u4f18\u4e2d\u6bd4\u4eba\u5de5\u8c03\u4f18\u8282\u770122%\u7684\u8bad\u7ec3\u6b65\u9aa4\uff1b\u5728\u4e09\u6a21\u6001\u573a\u666f\u4e2d\uff0c\u76f8\u6bd4\u5747\u5300\u6743\u91cd\u63d0\u5347\u5e73\u5747\u51c6\u786e\u7387\uff0c\u6df7\u5408\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\uff08<1 GPU\u5c0f\u65f6\uff09\u3002", "conclusion": "MaD-Mix\u4e3a\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7ba1\u9053\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6570\u636e\u6df7\u5408\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u591a\u6a21\u6001\u6570\u636e\u6df7\u5408\u4f18\u5316\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u4eba\u5de5\u8c03\u4f18\u7684\u4f9d\u8d56\u3002"}}
{"id": "2602.07798", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07798", "abs": "https://arxiv.org/abs/2602.07798", "authors": ["Ruiqi Wang", "Ruikang Liu", "Runyu Chen", "Haoxiang Suo", "Zhiyi Peng", "Zhuo Tang", "Changjian Chen"], "title": "CausalTAD: Injecting Causal Knowledge into Large Language Models for Tabular Anomaly Detection", "comment": null, "summary": "Detecting anomalies in tabular data is critical for many real-world applications, such as credit card fraud detection. With the rapid advancements in large language models (LLMs), state-of-the-art performance in tabular anomaly detection has been achieved by converting tabular data into text and fine-tuning LLMs. However, these methods randomly order columns during conversion, without considering the causal relationships between them, which is crucial for accurately detecting anomalies. In this paper, we present CausalTaD, a method that injects causal knowledge into LLMs for tabular anomaly detection. We first identify the causal relationships between columns and reorder them to align with these causal relationships. This reordering can be modeled as a linear ordering problem. Since each column contributes differently to the causal relationships, we further propose a reweighting strategy to assign different weights to different columns to enhance this effect. Experiments across more than 30 datasets demonstrate that our method consistently outperforms the current state-of-the-art methods. The code for CausalTAD is available at https://github.com/350234/CausalTAD.", "AI": {"tldr": "CausalTaD\uff1a\u901a\u8fc7\u6ce8\u5165\u56e0\u679c\u77e5\u8bc6\u5230LLMs\u4e2d\u6539\u8fdb\u8868\u683c\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u56e0\u679c\u5173\u7cfb\u91cd\u6392\u5217\u5e76\u52a0\u6743\uff0c\u572830\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8aSOTA\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u8868\u683c\u6570\u636e\u8f6c\u4e3a\u6587\u672c\u65f6\u968f\u673a\u6392\u5217\u5217\u987a\u5e8f\uff0c\u5ffd\u7565\u4e86\u5217\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u800c\u8fd9\u5bf9\u51c6\u786e\u68c0\u6d4b\u5f02\u5e38\u81f3\u5173\u91cd\u8981\u3002", "method": "1. \u8bc6\u522b\u5217\u95f4\u56e0\u679c\u5173\u7cfb\u5e76\u91cd\u65b0\u6392\u5e8f\uff08\u5efa\u6a21\u4e3a\u7ebf\u6027\u6392\u5e8f\u95ee\u9898\uff09\uff1b2. \u63d0\u51fa\u91cd\u52a0\u6743\u7b56\u7565\uff0c\u6839\u636e\u5217\u5bf9\u56e0\u679c\u5173\u7cfb\u7684\u8d21\u732e\u5206\u914d\u4e0d\u540c\u6743\u91cd\u3002", "result": "\u572830\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u8d85\u8d8a\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u6ce8\u5165\u56e0\u679c\u77e5\u8bc6\u5e76\u4f18\u5316\u5217\u6392\u5e8f\u548c\u52a0\u6743\uff0cCausalTaD\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u8868\u683c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2602.07799", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07799", "abs": "https://arxiv.org/abs/2602.07799", "authors": ["Ching Lam Choi", "Vighnesh Subramaniam", "Phillip Isola", "Antonio Torralba", "Stefanie Jegelka"], "title": "Fairness Aware Reward Optimization", "comment": null, "summary": "Demographic skews in human preference data propagate systematic unfairness through reward models into aligned LLMs. We introduce Fairness Aware Reward Optimization (Faro), an in-processing framework that trains reward models under demographic parity, equalized odds, or counterfactual fairness constraints. We provide the first theoretical analysis of reward-level fairness in LLM alignment, establishing: (i) provable fairness certificates for Faro-trained rewards with controllable slack; a (ii) formal characterization of the accuracy-fairness trade-off induced by KL-regularized fine-tuning, proving fairness transfers from reward to policy; and the (iii) existence of a non-empty Pareto frontier. Unlike pre- and post-processing methods, Faro ensures reward models are simultaneously ordinal (ranking correctly), cardinal (calibrated), and fair. Across multiple LLMs and benchmarks, Faro significantly reduces bias and harmful generations while maintaining or improving model quality.", "AI": {"tldr": "Faro\u662f\u4e00\u4e2a\u516c\u5e73\u611f\u77e5\u7684\u5956\u52b1\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u6ee1\u8db3\u516c\u5e73\u6027\u7ea6\u675f\u7684\u5956\u52b1\u6a21\u578b\u6765\u51cf\u5c11LLM\u5bf9\u9f50\u4e2d\u7684\u504f\u89c1\uff0c\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u5e76\u5728\u5b9e\u8df5\u4e2d\u6709\u6548\u964d\u4f4e\u504f\u89c1\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u3002", "motivation": "\u4eba\u7c7b\u504f\u597d\u6570\u636e\u4e2d\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u659c\u4f1a\u901a\u8fc7\u5956\u52b1\u6a21\u578b\u4f20\u64ad\u5230\u5bf9\u9f50\u7684LLM\u4e2d\uff0c\u5bfc\u81f4\u7cfb\u7edf\u6027\u4e0d\u516c\u5e73\u3002\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u4fdd\u8bc1\u5956\u52b1\u6a21\u578b\u7684\u5e8f\u6570\u6027\uff08\u6b63\u786e\u6392\u5e8f\uff09\u3001\u57fa\u6570\u6027\uff08\u6821\u51c6\uff09\u548c\u516c\u5e73\u6027\u3002", "method": "\u63d0\u51faFaro\u6846\u67b6\uff0c\u5728\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u65f6\u52a0\u5165\u4eba\u53e3\u7edf\u8ba1\u5b66\u5e73\u7b49\u3001\u5747\u8861\u673a\u4f1a\u6216\u53cd\u4e8b\u5b9e\u516c\u5e73\u6027\u7ea6\u675f\uff0c\u63d0\u4f9b\u7406\u8bba\u5206\u6790\u5305\u62ec\u516c\u5e73\u6027\u8bc1\u660e\u3001KL\u6b63\u5219\u5316\u5fae\u8c03\u4e2d\u7684\u51c6\u786e\u5ea6-\u516c\u5e73\u6027\u6743\u8861\u5f62\u5f0f\u5316\u63cf\u8ff0\uff0c\u4ee5\u53ca\u5e15\u7d2f\u6258\u524d\u6cbf\u5b58\u5728\u6027\u8bc1\u660e\u3002", "result": "\u5728\u591a\u4e2aLLM\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFaro\u663e\u8457\u51cf\u5c11\u4e86\u504f\u89c1\u548c\u6709\u5bb3\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u4e86\u6a21\u578b\u8d28\u91cf\u3002\u4e0e\u9884\u5904\u7406\u548c\u540e\u5904\u7406\u65b9\u6cd5\u4e0d\u540c\uff0cFaro\u786e\u4fdd\u5956\u52b1\u6a21\u578b\u540c\u65f6\u6ee1\u8db3\u5e8f\u6570\u6027\u3001\u57fa\u6570\u6027\u548c\u516c\u5e73\u6027\u3002", "conclusion": "Faro\u662f\u9996\u4e2a\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u7684\u5956\u52b1\u7ea7\u516c\u5e73\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u516c\u5e73\u7ea6\u675f\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u6709\u6548\u51cf\u5c11LLM\u5bf9\u9f50\u4e2d\u7684\u504f\u89c1\u4f20\u64ad\uff0c\u4e3a\u6784\u5efa\u66f4\u516c\u5e73\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.07800", "categories": ["cs.LG", "cs.NE", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.07800", "abs": "https://arxiv.org/abs/2602.07800", "authors": ["Rahul Padmanabhan", "Simone Brugiapaglia"], "title": "Approximating Matrix Functions with Deep Neural Networks and Transformers", "comment": null, "summary": "Transformers have revolutionized natural language processing, but their use for numerical computation has received less attention. We study the approximation of matrix functions, which map scalar functions to matrices, using neural networks including transformers. We focus on functions mapping square matrices to square matrices of the same dimension. These types of matrix functions appear throughout scientific computing, e.g., the matrix exponential in continuous-time Markov chains and the matrix sign function in stability analysis of dynamical systems. In this paper, we make two contributions. First, we prove bounds on the width and depth of ReLU networks needed to approximate the matrix exponential to an arbitrary precision. Second, we show experimentally that a transformer encoder-decoder with suitable numerical encodings can approximate certain matrix functions at a relative error of 5% with high probability. Our study reveals that the encoding scheme strongly affects performance, with different schemes working better for different functions.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\uff08\u5305\u62ecTransformer\uff09\u8fd1\u4f3c\u77e9\u9635\u51fd\u6570\uff0c\u8bc1\u660e\u4e86ReLU\u7f51\u7edc\u8fd1\u4f3c\u77e9\u9635\u6307\u6570\u7684\u5bbd\u5ea6\u548c\u6df1\u5ea6\u754c\u9650\uff0c\u5e76\u5b9e\u9a8c\u8868\u660e\u5e26\u5408\u9002\u6570\u503c\u7f16\u7801\u7684Transformer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u80fd\u4ee55%\u76f8\u5bf9\u8bef\u5dee\u8fd1\u4f3c\u67d0\u4e9b\u77e9\u9635\u51fd\u6570\u3002", "motivation": "Transformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u53d6\u5f97\u4e86\u9769\u547d\u6027\u8fdb\u5c55\uff0c\u4f46\u5728\u6570\u503c\u8ba1\u7b97\u65b9\u9762\u7684\u5e94\u7528\u7814\u7a76\u8f83\u5c11\u3002\u77e9\u9635\u51fd\u6570\uff08\u5982\u77e9\u9635\u6307\u6570\u3001\u77e9\u9635\u7b26\u53f7\u51fd\u6570\uff09\u5728\u79d1\u5b66\u8ba1\u7b97\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u8fd9\u4e9b\u51fd\u6570\u7684\u80fd\u529b\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "method": "1. \u7406\u8bba\u5206\u6790\uff1a\u8bc1\u660eReLU\u7f51\u7edc\u8fd1\u4f3c\u77e9\u9635\u6307\u6570\u6240\u9700\u7684\u5bbd\u5ea6\u548c\u6df1\u5ea6\u754c\u9650\uff1b2. \u5b9e\u9a8c\u7814\u7a76\uff1a\u4f7f\u7528\u5e26\u6570\u503c\u7f16\u7801\u7684Transformer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u6d4b\u8bd5\u4e0d\u540c\u7f16\u7801\u65b9\u6848\u5bf9\u77e9\u9635\u51fd\u6570\u8fd1\u4f3c\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "1. \u5efa\u7acb\u4e86ReLU\u7f51\u7edc\u8fd1\u4f3c\u77e9\u9635\u6307\u6570\u7684\u7406\u8bba\u754c\u9650\uff1b2. \u5b9e\u9a8c\u8868\u660e\u5408\u9002\u7684Transformer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u80fd\u4ee55%\u76f8\u5bf9\u8bef\u5dee\u8fd1\u4f3c\u67d0\u4e9b\u77e9\u9635\u51fd\u6570\uff1b3. \u53d1\u73b0\u7f16\u7801\u65b9\u6848\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4e0d\u540c\u51fd\u6570\u9002\u5408\u4e0d\u540c\u7684\u7f16\u7801\u65b9\u6848\u3002", "conclusion": "\u795e\u7ecf\u7f51\u7edc\uff08\u5305\u62ecTransformer\uff09\u80fd\u591f\u6709\u6548\u8fd1\u4f3c\u77e9\u9635\u51fd\u6570\uff0c\u7f16\u7801\u65b9\u6848\u7684\u9009\u62e9\u662f\u5173\u952e\u56e0\u7d20\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u795e\u7ecf\u7f51\u7edc\u5728\u79d1\u5b66\u8ba1\u7b97\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002"}}
{"id": "2602.07828", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07828", "abs": "https://arxiv.org/abs/2602.07828", "authors": ["Charles Ye", "Jasmine Cui"], "title": "Efficient Representations are Controllable Representations", "comment": null, "summary": "What is the most brute-force way to install interpretable, controllable features into a model's activations? Controlling how LLMs internally represent concepts typically requires sophisticated methods to first identify, then intervene on the model's existing feature geometry. We bypass all of this.\n  We finetune an LLM with a simple auxiliary loss, training 16 of its 3072 residual stream dimensions to be inert interpretability flags that simply indicate what concepts are required for generation. The model reorganizes around them anyway, learning to rely on these flags during actual generation tasks. As a result, these inert flags become genuine internal features: interpretable control switches that allow us to steer generation at inference time. Why does this work? When a feature is reliably supplied at a fixed location, gradient descent gradually eliminates redundant encodings elsewhere, and the model erodes its own alternative representations. A model's efficiency pressure is a lever - exploitable to induce interpretable, controllable representations.", "AI": {"tldr": "\u901a\u8fc7\u7b80\u5355\u7684\u8f85\u52a9\u635f\u5931\u5fae\u8c03LLM\uff0c\u8bad\u7ec316\u4e2a\u6b8b\u5dee\u6d41\u7ef4\u5ea6\u4f5c\u4e3a\u60f0\u6027\u53ef\u89e3\u91ca\u6027\u6807\u5fd7\uff0c\u6a21\u578b\u4f1a\u56f4\u7ed5\u8fd9\u4e9b\u6807\u5fd7\u91cd\u7ec4\u5e76\u4f9d\u8d56\u5b83\u4eec\u8fdb\u884c\u751f\u6210\uff0c\u4ece\u800c\u521b\u5efa\u53ef\u89e3\u91ca\u7684\u63a7\u5236\u5f00\u5173\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5148\u8bc6\u522b\u6a21\u578b\u5185\u90e8\u7279\u5f81\u51e0\u4f55\uff0c\u7136\u540e\u8fdb\u884c\u5e72\u9884\uff0c\u8fc7\u7a0b\u590d\u6742\u3002\u672c\u6587\u5bfb\u6c42\u4e00\u79cd\u66f4\u76f4\u63a5\u3001\u66f4\u66b4\u529b\u7684\u65b9\u6cd5\u6765\u5b89\u88c5\u53ef\u89e3\u91ca\u3001\u53ef\u63a7\u5236\u7684\u7279\u5f81\u5230\u6a21\u578b\u6fc0\u6d3b\u4e2d\u3002", "method": "\u4f7f\u7528\u7b80\u5355\u7684\u8f85\u52a9\u635f\u5931\u5fae\u8c03LLM\uff0c\u4e13\u95e8\u8bad\u7ec33072\u4e2a\u6b8b\u5dee\u6d41\u7ef4\u5ea6\u4e2d\u768416\u4e2a\u7ef4\u5ea6\u4f5c\u4e3a\u60f0\u6027\u53ef\u89e3\u91ca\u6027\u6807\u5fd7\uff0c\u8fd9\u4e9b\u6807\u5fd7\u6307\u793a\u751f\u6210\u6240\u9700\u7684\u6982\u5ff5\u3002\u6a21\u578b\u5728\u68af\u5ea6\u4e0b\u964d\u8fc7\u7a0b\u4e2d\u4f1a\u56f4\u7ed5\u8fd9\u4e9b\u6807\u5fd7\u91cd\u7ec4\uff0c\u6d88\u9664\u5197\u4f59\u7f16\u7801\uff0c\u6700\u7ec8\u4f9d\u8d56\u8fd9\u4e9b\u6807\u5fd7\u8fdb\u884c\u5b9e\u9645\u751f\u6210\u4efb\u52a1\u3002", "result": "\u8fd9\u4e9b\u60f0\u6027\u6807\u5fd7\u53d8\u6210\u4e86\u771f\u6b63\u7684\u5185\u90e8\u7279\u5f81\uff1a\u53ef\u89e3\u91ca\u7684\u63a7\u5236\u5f00\u5173\uff0c\u5141\u8bb8\u5728\u63a8\u7406\u65f6\u5f15\u5bfc\u751f\u6210\u3002\u5f53\u7279\u5f81\u53ef\u9760\u5730\u5728\u56fa\u5b9a\u4f4d\u7f6e\u63d0\u4f9b\u65f6\uff0c\u68af\u5ea6\u4e0b\u964d\u4f1a\u9010\u6e10\u6d88\u9664\u5176\u4ed6\u5730\u65b9\u7684\u5197\u4f59\u7f16\u7801\uff0c\u6a21\u578b\u4f1a\u4fb5\u8680\u81ea\u5df1\u7684\u66ff\u4ee3\u8868\u793a\u3002", "conclusion": "\u6a21\u578b\u7684\u6548\u7387\u538b\u529b\u662f\u4e00\u4e2a\u53ef\u5229\u7528\u7684\u6760\u6746\uff0c\u53ef\u4ee5\u7528\u6765\u8bf1\u5bfc\u53ef\u89e3\u91ca\u3001\u53ef\u63a7\u5236\u7684\u8868\u793a\u3002\u901a\u8fc7\u63d0\u4f9b\u53ef\u9760\u7684\u7279\u5f81\u4f4d\u7f6e\uff0c\u53ef\u4ee5\u8feb\u4f7f\u6a21\u578b\u91cd\u7ec4\u5176\u8868\u793a\u7ed3\u6784\uff0c\u4ece\u800c\u521b\u5efa\u53ef\u89e3\u91ca\u7684\u63a7\u5236\u673a\u5236\u3002"}}
{"id": "2602.07832", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07832", "abs": "https://arxiv.org/abs/2602.07832", "authors": ["Xian Wu", "Kaijie Zhu", "Ying Zhang", "Lun Wang", "Wenbo Guo"], "title": "rePIRL: Learn PRM with Inverse RL for LLM Reasoning", "comment": null, "summary": "Process rewards have been widely used in deep reinforcement learning to improve training efficiency, reduce variance, and prevent reward hacking. In LLM reasoning, existing works also explore various solutions for learning effective process reward models (PRM) with or without the help of an expert policy. However, existing methods either rely on strong assumptions about the expert policies (e.g., requiring their reward functions) or suffer intrinsic limitations (e.g., entropy collapse), resulting in weak PRMs or limited generalizability. In this paper, we introduce rePIRL, an inverse RL-inspired framework that learns effective PRMs with minimal assumptions about expert policies. Specifically, we design a dual learning process that updates the policy and the PRM interchangeably. Our learning algorithm has customized techniques to address the challenges of scaling traditional inverse RL to LLMs. We theoretically show that our proposed learning framework can unify both online and offline PRM learning methods, justifying that rePIRL can learn PRMs with minimal assumptions. Empirical evaluations on standardized math and coding reasoning datasets demonstrate the effectiveness of rePIRL over existing methods. We further show the application of our trained PRM in test-time training, test-time scaling, and providing an early signal for training hard problems. Finally, we validate our training recipe and key design choices via a detailed ablation study.", "AI": {"tldr": "rePIRL\uff1a\u4e00\u4e2a\u53d7\u9006\u5f3a\u5316\u5b66\u4e60\u542f\u53d1\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u6709\u6548\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u5bf9\u4e13\u5bb6\u7b56\u7565\u5047\u8bbe\u6700\u5c11\uff0c\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u5b66\u4e60\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u5bf9\u4e13\u5bb6\u7b56\u7565\u7684\u5f3a\u5047\u8bbe\uff08\u5982\u9700\u8981\u5176\u5956\u52b1\u51fd\u6570\uff09\uff0c\u8981\u4e48\u5b58\u5728\u5185\u5728\u5c40\u9650\u6027\uff08\u5982\u71b5\u5d29\u6e83\uff09\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u5f31\u6216\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51farePIRL\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u91cd\u5b66\u4e60\u8fc7\u7a0b\u4ea4\u66ff\u66f4\u65b0\u7b56\u7565\u548c\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u5305\u542b\u5b9a\u5236\u5316\u6280\u672f\u89e3\u51b3\u4f20\u7edf\u9006\u5f3a\u5316\u5b66\u4e60\u6269\u5c55\u5230LLM\u7684\u6311\u6218\u3002", "result": "\u5728\u6807\u51c6\u5316\u6570\u5b66\u548c\u7f16\u7a0b\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793arePIRL\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bad\u7ec3\u51fa\u7684PRM\u53ef\u7528\u4e8e\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u3001\u6d4b\u8bd5\u65f6\u6269\u5c55\u548c\u4e3a\u56f0\u96be\u95ee\u9898\u63d0\u4f9b\u65e9\u671f\u4fe1\u53f7\u3002", "conclusion": "rePIRL\u80fd\u591f\u4ee5\u6700\u5c0f\u5047\u8bbe\u5b66\u4e60\u6709\u6548\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u7edf\u4e00\u4e86\u5728\u7ebf\u548c\u79bb\u7ebfPRM\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u8bad\u7ec3\u65b9\u6848\u548c\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\u3002"}}
{"id": "2602.07834", "categories": ["cs.LG", "math.DG"], "pdf": "https://arxiv.org/pdf/2602.07834", "abs": "https://arxiv.org/abs/2602.07834", "authors": ["D Yang Eng"], "title": "Interpretable Analytic Calabi-Yau Metrics via Symbolic Distillation", "comment": "31 pages, 7 figures", "summary": "Calabi--Yau manifolds are essential for string theory but require computing intractable metrics. Here we show that symbolic regression can distill neural approximations into simple, interpretable formulas. Our five-term expression matches neural accuracy ($R^2 = 0.9994$) with 3,000-fold fewer parameters. Multi-seed validation confirms that geometric constraints select essential features, specifically power sums and symmetric polynomials, while permitting structural diversity. The functional form can be maintained across the studied moduli range ($\u03c8\\in [0, 0.8]$) with coefficients varying smoothly; we interpret these trends as empirical hypotheses within the accuracy regime of the locally-trained teachers ($\u03c3\\approx 8-9\\%$ at $\u03c8\\neq 0$). The formula reproduces physical observables -- volume integrals and Yukawa couplings -- validating that symbolic distillation recovers compact, interpretable models for quantities previously accessible only to black-box networks.", "AI": {"tldr": "\u4f7f\u7528\u7b26\u53f7\u56de\u5f52\u5c06\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u7684Calabi-Yau\u6d41\u5f62\u5ea6\u91cf\u63d0\u70bc\u4e3a\u7b80\u5355\u53ef\u89e3\u91ca\u7684\u4e94\u9879\u516c\u5f0f\uff0c\u7cbe\u5ea6\u76f8\u5f53\u4f46\u53c2\u6570\u51cf\u5c113000\u500d", "motivation": "Calabi-Yau\u6d41\u5f62\u5bf9\u5f26\u7406\u8bba\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5ea6\u91cf\u8ba1\u7b97\u6781\u5176\u56f0\u96be\u3002\u867d\u7136\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u8fd1\u4f3c\u8fd9\u4e9b\u5ea6\u91cf\uff0c\u4f46\u5b83\u4eec\u662f\u9ed1\u76d2\u6a21\u578b\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002\u9700\u8981\u627e\u5230\u65e2\u80fd\u4fdd\u6301\u7cbe\u5ea6\u53c8\u5177\u6709\u7269\u7406\u53ef\u89e3\u91ca\u6027\u7684\u7b80\u5316\u8868\u793a\u3002", "method": "\u91c7\u7528\u7b26\u53f7\u56de\u5f52\u65b9\u6cd5\uff0c\u5c06\u8bad\u7ec3\u597d\u7684\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u4f5c\u4e3a\"\u6559\u5e08\"\uff0c\u4ece\u4e2d\u63d0\u70bc\u51fa\u7b80\u6d01\u7684\u6570\u5b66\u8868\u8fbe\u5f0f\u3002\u901a\u8fc7\u591a\u79cd\u5b50\u9a8c\u8bc1\u786e\u8ba4\u51e0\u4f55\u7ea6\u675f\u9009\u62e9\u7684\u57fa\u672c\u7279\u5f81\uff0c\u4e3b\u8981\u5305\u542b\u5e42\u548c\u4e0e\u5bf9\u79f0\u591a\u9879\u5f0f\u3002", "result": "\u5f97\u5230\u4e94\u9879\u8868\u8fbe\u5f0f\uff0c\u4e0e\u795e\u7ecf\u7f51\u7edc\u7cbe\u5ea6\u76f8\u5f53\uff08R\u00b2=0.9994\uff09\uff0c\u4f46\u53c2\u6570\u6570\u91cf\u51cf\u5c113000\u500d\u3002\u8be5\u51fd\u6570\u5f62\u5f0f\u5728\u7814\u7a76\u6a21\u7a7a\u95f4\u8303\u56f4\uff08\u03c8\u2208[0,0.8]\uff09\u5185\u4fdd\u6301\u7a33\u5b9a\uff0c\u7cfb\u6570\u5e73\u6ed1\u53d8\u5316\u3002\u516c\u5f0f\u80fd\u51c6\u786e\u518d\u73b0\u7269\u7406\u53ef\u89c2\u6d4b\u91cf\uff08\u4f53\u79ef\u79ef\u5206\u548cYukawa\u8026\u5408\uff09\u3002", "conclusion": "\u7b26\u53f7\u63d0\u70bc\u65b9\u6cd5\u6210\u529f\u5730\u4ece\u9ed1\u76d2\u795e\u7ecf\u7f51\u7edc\u4e2d\u6062\u590d\u51fa\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u7684\u6a21\u578b\uff0c\u4e3a\u539f\u672c\u53ea\u80fd\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u8bbf\u95ee\u7684\u7269\u7406\u91cf\u63d0\u4f9b\u4e86\u900f\u660e\u8868\u793a\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u5f26\u7406\u8bba\u51e0\u4f55\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.07848", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07848", "abs": "https://arxiv.org/abs/2602.07848", "authors": ["Shijie Wang", "Pengfei Li", "Yikun Fu", "Kaifeng Liu", "Fangyuan Li", "Yang Liu", "Xiaowei Sun", "Zonglin Li", "Siyao Zhao", "Jian Zhao", "Kai Tian", "Dong Li", "Junqi Gao", "Yutong Zhang", "Yiqun Chen", "Yuqiang Li", "Zoe Li", "Weinan Zhang", "Peng Ye", "Shuyue Hu", "Lei Bai", "Bowen Zhou", "Kaiyan Zhang", "Biqing Qi"], "title": "MARTI-MARS$^2$: Scaling Multi-Agent Self-Search via Reinforcement Learning for Code Generation", "comment": null, "summary": "While the complex reasoning capability of Large Language Models (LLMs) has attracted significant attention, single-agent systems often encounter inherent performance ceilings in complex tasks such as code generation. Multi-agent collaboration offers a promising avenue to transcend these boundaries. However, existing frameworks typically rely on prompt-based test-time interactions or multi-role configurations trained with homogeneous parameters, limiting error correction capabilities and strategic diversity. In this paper, we propose a Multi-Agent Reinforced Training and Inference Framework with Self-Search Scaling (MARTI-MARS2), which integrates policy learning with multi-agent tree search by formulating the multi-agent collaborative exploration process as a dynamic and learnable environment. By allowing agents to iteratively explore and refine within the environment, the framework facilitates evolution from parameter-sharing homogeneous multi-role training to heterogeneous multi-agent training, breaking through single-agent capability limits. We also introduce an efficient inference strategy MARTI-MARS2-T+ to fully exploit the scaling potential of multi-agent collaboration at test time. We conduct extensive experiments across varied model scales (8B, 14B, and 32B) on challenging code generation benchmarks. Utilizing two collaborating 32B models, MARTI-MARS2 achieves 77.7%, outperforming strong baselines like GPT-5.1. Furthermore, MARTI-MARS2 reveals a novel scaling law: shifting from single-agent to homogeneous multi-role and ultimately to heterogeneous multi-agent paradigms progressively yields higher RL performance ceilings, robust TTS capabilities, and greater policy diversity, suggesting that policy diversity is critical for scaling intelligence via multi-agent reinforcement learning.", "AI": {"tldr": "MARTI-MARS2\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u63a2\u7d22\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u53ef\u5b66\u4e60\u73af\u5883\uff0c\u7ed3\u5408\u7b56\u7565\u5b66\u4e60\u548c\u6811\u641c\u7d22\uff0c\u5b9e\u73b0\u4e86\u4ece\u540c\u6784\u591a\u89d2\u8272\u8bad\u7ec3\u5230\u5f02\u6784\u591a\u667a\u80fd\u4f53\u8bad\u7ec3\u7684\u6f14\u8fdb\uff0c\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u5355\u667a\u80fd\u4f53\u6027\u80fd\u6781\u9650\u3002", "motivation": "\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u4ee3\u7801\u751f\u6210\uff09\u4e2d\u5b58\u5728\u6027\u80fd\u4e0a\u9650\uff0c\u800c\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u901a\u5e38\u4f9d\u8d56\u57fa\u4e8e\u63d0\u793a\u7684\u6d4b\u8bd5\u65f6\u4ea4\u4e92\u6216\u540c\u6784\u53c2\u6570\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u9519\u8bef\u7ea0\u6b63\u80fd\u529b\u548c\u7b56\u7565\u591a\u6837\u6027\u3002", "method": "\u63d0\u51faMARTI-MARS2\u6846\u67b6\uff0c\u5c06\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u63a2\u7d22\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u52a8\u6001\u53ef\u5b66\u4e60\u73af\u5883\uff0c\u7ed3\u5408\u7b56\u7565\u5b66\u4e60\u548c\u591a\u667a\u80fd\u4f53\u6811\u641c\u7d22\uff0c\u5141\u8bb8\u667a\u80fd\u4f53\u5728\u73af\u5883\u4e2d\u8fed\u4ee3\u63a2\u7d22\u548c\u4f18\u5316\u3002\u540c\u65f6\u5f15\u5165MARTI-MARS2-T+\u9ad8\u6548\u63a8\u7406\u7b56\u7565\uff0c\u5145\u5206\u5229\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u6269\u5c55\u6f5c\u529b\u3002", "result": "\u5728\u4e24\u4e2a32B\u6a21\u578b\u534f\u4f5c\u4e0b\uff0cMARTI-MARS2\u5728\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523077.7%\u7684\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4e86GPT-5.1\u7b49\u5f3a\u57fa\u7ebf\u3002\u5b9e\u9a8c\u63ed\u793a\u4e86\u65b0\u7684\u6269\u5c55\u89c4\u5f8b\uff1a\u4ece\u5355\u667a\u80fd\u4f53\u5230\u540c\u6784\u591a\u89d2\u8272\u518d\u5230\u5f02\u6784\u591a\u667a\u80fd\u4f53\u8303\u5f0f\uff0c\u9010\u6b65\u83b7\u5f97\u66f4\u9ad8\u7684RL\u6027\u80fd\u4e0a\u9650\u3001\u66f4\u5f3a\u7684TTS\u80fd\u529b\u548c\u66f4\u5927\u7684\u7b56\u7565\u591a\u6837\u6027\u3002", "conclusion": "MARTI-MARS2\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7a81\u7834\u4e86\u5355\u667a\u80fd\u4f53\u80fd\u529b\u9650\u5236\uff0c\u8bc1\u660e\u4e86\u7b56\u7565\u591a\u6837\u6027\u5bf9\u4e8e\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6269\u5c55\u667a\u80fd\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u534f\u4f5c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07859", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07859", "abs": "https://arxiv.org/abs/2602.07859", "authors": ["Siyu Lu", "Chenhan Xiao", "Yang Weng"], "title": "Dynamic Load Model for Data Centers with Pattern-Consistent Calibration", "comment": "10 pages, 13 figures", "summary": "The rapid growth of data centers has made large electronic load (LEL) modeling increasingly important for power system analysis. Such loads are characterized by fast workload-driven variability and protection-driven disconnection and reconnection behavior that are not captured by conventional load models. Existing data center load modeling includes physics-based approaches, which provide interpretable structure for grid simulation, and data-driven approaches, which capture empirical workload variability from data. However, physics-based models are typically uncalibrated to facility-level operation, while trajectory alignment in data-driven methods often leads to overfitting and unrealistic dynamic behavior. To resolve these limitations, we design the framework to leverage both physics-based structure and data-driven adaptability. The physics-based structure is parameterized to enable data-driven pattern-consistent calibration from real operational data, supporting facility-level grid planning. We further show that trajectory-level alignment is limited for inherently stochastic data center loads. Therefore, we design the calibration to align temporal and statistical patterns using temporal contrastive learning (TCL). This calibration is performed locally at the facility, and only calibrated parameters are shared with utilities, preserving data privacy. The proposed load model is calibrated by real-world operational load data from the MIT Supercloud, ASU Sol, Blue Waters, and ASHRAE datasets. Then it is integrated into the ANDES platform and evaluated on the IEEE 39-bus, NPCC 140-bus, and WECC 179-bus systems. We find that interactions among LELs can fundamentally alter post-disturbance recovery behavior, producing compound disconnection-reconnection dynamics and delayed stabilization that are not captured by uncalibrated load models.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u7269\u7406\u6a21\u578b\u548c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u578b\u7535\u5b50\u8d1f\u8f7d\u5efa\u6a21\uff0c\u901a\u8fc7\u65f6\u5e8f\u5bf9\u6bd4\u5b66\u4e60\u8fdb\u884c\u6a21\u5f0f\u4e00\u81f4\u6027\u6821\u51c6\uff0c\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u5e76\u6539\u5584\u7535\u7f51\u4eff\u771f\u7cbe\u5ea6\u3002", "motivation": "\u6570\u636e\u4e2d\u5fc3\u5feb\u901f\u589e\u957f\u4f7f\u5f97\u5927\u578b\u7535\u5b50\u8d1f\u8f7d\u5efa\u6a21\u5bf9\u7535\u529b\u7cfb\u7edf\u5206\u6790\u65e5\u76ca\u91cd\u8981\u3002\u4f20\u7edf\u8d1f\u8f7d\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u5feb\u901f\u5de5\u4f5c\u8d1f\u8f7d\u53d8\u5316\u548c\u4fdd\u62a4\u9a71\u52a8\u7684\u65ad\u5f00/\u91cd\u8fde\u884c\u4e3a\u3002\u73b0\u6709\u7269\u7406\u6a21\u578b\u672a\u6821\u51c6\u5230\u8bbe\u65bd\u7ea7\u8fd0\u884c\uff0c\u800c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5bb9\u6613\u8fc7\u62df\u5408\u4e14\u4ea7\u751f\u4e0d\u73b0\u5b9e\u7684\u52a8\u6001\u884c\u4e3a\u3002", "method": "\u8bbe\u8ba1\u7ed3\u5408\u7269\u7406\u7ed3\u6784\u548c\u6570\u636e\u9a71\u52a8\u9002\u5e94\u6027\u7684\u6846\u67b6\u3002\u7269\u7406\u7ed3\u6784\u53c2\u6570\u5316\u4ee5\u652f\u6301\u4ece\u771f\u5b9e\u8fd0\u884c\u6570\u636e\u8fdb\u884c\u6a21\u5f0f\u4e00\u81f4\u6027\u6821\u51c6\u3002\u91c7\u7528\u65f6\u5e8f\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u65f6\u95f4\u548c\u7edf\u8ba1\u6a21\u5f0f\uff0c\u800c\u975e\u8f68\u8ff9\u7ea7\u5bf9\u9f50\u3002\u672c\u5730\u6267\u884c\u6821\u51c6\uff0c\u4ec5\u5171\u4eab\u6821\u51c6\u53c2\u6570\u4ee5\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3002", "result": "\u4f7f\u7528MIT Supercloud\u3001ASU Sol\u3001Blue Waters\u548cASHRAE\u6570\u636e\u96c6\u7684\u5b9e\u9645\u8fd0\u884c\u8d1f\u8f7d\u6570\u636e\u8fdb\u884c\u6821\u51c6\u3002\u96c6\u6210\u5230ANDES\u5e73\u53f0\u5e76\u5728IEEE 39\u603b\u7ebf\u3001NPCC 140\u603b\u7ebf\u548cWECC 179\u603b\u7ebf\u7cfb\u7edf\u4e0a\u8bc4\u4f30\u3002\u53d1\u73b0\u5927\u578b\u7535\u5b50\u8d1f\u8f7d\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u4f1a\u6839\u672c\u6539\u53d8\u6270\u52a8\u540e\u6062\u590d\u884c\u4e3a\uff0c\u4ea7\u751f\u590d\u5408\u65ad\u5f00-\u91cd\u8fde\u52a8\u6001\u548c\u5ef6\u8fdf\u7a33\u5b9a\u73b0\u8c61\uff0c\u8fd9\u662f\u672a\u6821\u51c6\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u7684\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6210\u529f\u7ed3\u5408\u4e86\u7269\u7406\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6570\u636e\u9a71\u52a8\u7684\u9002\u5e94\u6027\uff0c\u901a\u8fc7\u6a21\u5f0f\u4e00\u81f4\u6027\u6821\u51c6\u6539\u5584\u4e86\u5927\u578b\u7535\u5b50\u8d1f\u8f7d\u5efa\u6a21\u7cbe\u5ea6\uff0c\u63ed\u793a\u4e86\u8d1f\u8f7d\u95f4\u76f8\u4e92\u4f5c\u7528\u5bf9\u7535\u7f51\u52a8\u6001\u7684\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2602.07873", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07873", "abs": "https://arxiv.org/abs/2602.07873", "authors": ["Donghyeon Ki", "Hee-Jun Ahn", "Kyungyoon Kim", "Byung-Jun Lee"], "title": "Direct Soft-Policy Sampling via Langevin Dynamics", "comment": null, "summary": "Soft policies in reinforcement learning define policies as Boltzmann distributions over state-action value functions, providing a principled mechanism for balancing exploration and exploitation. However, realizing such soft policies in practice remains challenging. Existing approaches either depend on parametric policies with limited expressivity or employ diffusion-based policies whose intractable likelihoods hinder reliable entropy estimation in soft policy objectives. We address this challenge by directly realizing soft-policy sampling via Langevin dynamics driven by the action gradient of the Q-function. This perspective leads to Langevin Q-Learning (LQL), which samples actions from the target Boltzmann distribution without explicitly parameterizing the policy. However, directly applying Langevin dynamics suffers from slow mixing in high-dimensional and non-convex Q-landscapes, limiting its practical effectiveness. To overcome this, we propose Noise-Conditioned Langevin Q-Learning (NC-LQL), which integrates multi-scale noise perturbations into the value function. NC-LQL learns a noise-conditioned Q-function that induces a sequence of progressively smoothed value landscapes, enabling sampling to transition from global exploration to precise mode refinement. On OpenAI Gym MuJoCo benchmarks, NC-LQL achieves competitive performance compared to state-of-the-art diffusion-based methods, providing a simple yet powerful solution for online RL.", "AI": {"tldr": "\u63d0\u51faNC-LQL\u65b9\u6cd5\uff0c\u901a\u8fc7\u566a\u58f0\u6761\u4ef6\u5316\u7684\u6717\u4e4b\u4e07\u52a8\u529b\u5b66\u5b9e\u73b0\u8f6f\u7b56\u7565\u91c7\u6837\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u8868\u8fbe\u80fd\u529b\u6709\u9650\u6216\u71b5\u4f30\u8ba1\u56f0\u96be\u7684\u95ee\u9898\uff0c\u5728MuJoCo\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e0e\u6269\u6563\u65b9\u6cd5\u7ade\u4e89\u7684\u6027\u80fd\u3002", "motivation": "\u8f6f\u7b56\u7565\u4f5c\u4e3a\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u80fd\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\uff1a\u53c2\u6570\u5316\u7b56\u7565\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u6216\u6269\u6563\u7b56\u7565\u7684\u4e0d\u53ef\u5904\u7406\u4f3c\u7136\u963b\u788d\u71b5\u4f30\u8ba1\u3002", "method": "\u63d0\u51fa\u566a\u58f0\u6761\u4ef6\u5316\u6717\u4e4b\u4e07Q\u5b66\u4e60(NC-LQL)\uff1a1) \u4f7f\u7528\u6717\u4e4b\u4e07\u52a8\u529b\u5b66\u76f4\u63a5\u91c7\u6837\u8f6f\u7b56\u7565\uff0c\u65e0\u9700\u663e\u5f0f\u53c2\u6570\u5316\u7b56\u7565\uff1b2) \u5f15\u5165\u591a\u5c3a\u5ea6\u566a\u58f0\u6270\u52a8\u4ef7\u503c\u51fd\u6570\uff0c\u5b66\u4e60\u566a\u58f0\u6761\u4ef6\u5316Q\u51fd\u6570\uff1b3) \u901a\u8fc7\u6e10\u8fdb\u5e73\u6ed1\u7684\u4ef7\u503c\u666f\u89c2\u5b9e\u73b0\u4ece\u5168\u5c40\u63a2\u7d22\u5230\u7cbe\u786e\u6a21\u5f0f\u7ec6\u5316\u7684\u91c7\u6837\u8fc7\u7a0b\u3002", "result": "\u5728OpenAI Gym MuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNC-LQL\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u6269\u6563\u65b9\u6cd5\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u5f3a\u5927\u7684\u5728\u7ebfRL\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "NC-LQL\u901a\u8fc7\u566a\u58f0\u6761\u4ef6\u5316\u6717\u4e4b\u4e07\u52a8\u529b\u5b66\u6709\u6548\u89e3\u51b3\u4e86\u8f6f\u7b56\u7565\u5b9e\u73b0\u7684\u6311\u6218\uff0c\u5728\u4fdd\u6301\u7b80\u5355\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4e0e\u590d\u6742\u6269\u6563\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4e3a\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8f6f\u7b56\u7565\u91c7\u6837\u65b9\u6cd5\u3002"}}
{"id": "2602.07875", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07875", "abs": "https://arxiv.org/abs/2602.07875", "authors": ["Aditya Shankar", "Yuandou Wang", "Rihan Hai", "Lydia Y. Chen"], "title": "Harpoon: Generalised Manifold Guidance for Conditional Tabular Diffusion", "comment": "Accepted at ICLR 2026", "summary": "Generating tabular data under conditions is critical to applications requiring precise control over the generative process. Existing methods rely on training-time strategies that do not generalise to unseen constraints during inference, and struggle to handle conditional tasks beyond tabular imputation. While manifold theory offers a principled way to guide generation, current formulations are tied to specific inference-time objectives and are limited to continuous domains. We extend manifold theory to tabular data and expand its scope to handle diverse inference-time objectives. On this foundation, we introduce HARPOON, a tabular diffusion method that guides unconstrained samples along the manifold geometry to satisfy diverse tabular conditions at inference. We validate our theoretical contributions empirically on tasks such as imputation and enforcing inequality constraints, demonstrating HARPOON'S strong performance across diverse datasets and the practical benefits of manifold-aware guidance for tabular data. Code URL: https://github.com/adis98/Harpoon", "AI": {"tldr": "HARPOON\u662f\u4e00\u79cd\u8868\u683c\u6269\u6563\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d41\u5f62\u7406\u8bba\u5f15\u5bfc\u65e0\u7ea6\u675f\u6837\u672c\u6ee1\u8db3\u591a\u6837\u5316\u7684\u8868\u683c\u6761\u4ef6\uff0c\u5728\u63a8\u7406\u65f6\u5904\u7406\u672a\u89c1\u7ea6\u675f\u548c\u4e0d\u7b49\u5f0f\u7ea6\u675f\u7b49\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8bad\u7ec3\u65f6\u7b56\u7565\u65e0\u6cd5\u6cdb\u5316\u5230\u63a8\u7406\u65f6\u672a\u89c1\u7ea6\u675f\uff0c\u4e14\u96be\u4ee5\u5904\u7406\u8868\u683c\u586b\u8865\u4e4b\u5916\u7684\u6761\u4ef6\u4efb\u52a1\u3002\u6d41\u5f62\u7406\u8bba\u867d\u63d0\u4f9b\u539f\u5219\u6027\u6307\u5bfc\uff0c\u4f46\u5f53\u524d\u516c\u5f0f\u5c40\u9650\u4e8e\u7279\u5b9a\u63a8\u7406\u76ee\u6807\u548c\u8fde\u7eed\u57df\u3002", "method": "\u5c06\u6d41\u5f62\u7406\u8bba\u6269\u5c55\u5230\u8868\u683c\u6570\u636e\uff0c\u63d0\u51faHARPOON\u8868\u683c\u6269\u6563\u65b9\u6cd5\uff0c\u6cbf\u6d41\u5f62\u51e0\u4f55\u5f15\u5bfc\u65e0\u7ea6\u675f\u6837\u672c\uff0c\u5728\u63a8\u7406\u65f6\u6ee1\u8db3\u591a\u6837\u5316\u8868\u683c\u6761\u4ef6\u3002", "result": "\u5728\u586b\u8865\u548c\u4e0d\u7b49\u5f0f\u7ea6\u675f\u7b49\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u7406\u8bba\u8d21\u732e\uff0cHARPOON\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c55\u793a\u4e86\u6d41\u5f62\u611f\u77e5\u5f15\u5bfc\u5bf9\u8868\u683c\u6570\u636e\u7684\u5b9e\u9645\u4f18\u52bf\u3002", "conclusion": "HARPOON\u901a\u8fc7\u6269\u5c55\u6d41\u5f62\u7406\u8bba\u5230\u8868\u683c\u6570\u636e\uff0c\u6210\u529f\u5904\u7406\u591a\u6837\u5316\u63a8\u7406\u65f6\u6761\u4ef6\u4efb\u52a1\uff0c\u4e3a\u8868\u683c\u6570\u636e\u7684\u6761\u4ef6\u751f\u6210\u63d0\u4f9b\u4e86\u7075\u6d3b\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07884", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07884", "abs": "https://arxiv.org/abs/2602.07884", "authors": ["Mohammad Ashhad", "Robert Hoehndorf", "Ricardo Henao"], "title": "GRAFT: Decoupling Ranking and Calibration for Survival Analysis", "comment": null, "summary": "Survival analysis is complicated by censored data, high-dimensional features, and non-linear interactions. Classical models are interpretable but restrictive, while deep learning models are flexible but often non-interpretable and sensitive to noise. We propose GRAFT (Gated Residual Accelerated Failure Time), a novel AFT model that decouples prognostic ranking from calibration. GRAFT's hybrid architecture combines a linear AFT model with a non-linear residual neural network, and it also integrates stochastic gates for automatic, end-to-end feature selection. The model is trained by directly optimizing a differentiable, C-index-aligned ranking loss using stochastic conditional imputation from local Kaplan-Meier estimators. In public benchmarks, GRAFT outperforms baselines in discrimination and calibration, while remaining robust and sparse in high-noise settings.", "AI": {"tldr": "GRAFT\u662f\u4e00\u79cd\u65b0\u9896\u7684AFT\u751f\u5b58\u5206\u6790\u6a21\u578b\uff0c\u901a\u8fc7\u89e3\u8026\u9884\u540e\u6392\u540d\u548c\u6821\u51c6\uff0c\u7ed3\u5408\u7ebf\u6027AFT\u6a21\u578b\u4e0e\u975e\u7ebf\u6027\u6b8b\u5dee\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u96c6\u6210\u968f\u673a\u95e8\u63a7\u8fdb\u884c\u7aef\u5230\u7aef\u7279\u5f81\u9009\u62e9\uff0c\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u751f\u5b58\u5206\u6790\u9762\u4e34\u5220\u5931\u6570\u636e\u3001\u9ad8\u7ef4\u7279\u5f81\u548c\u975e\u7ebf\u6027\u4ea4\u4e92\u7684\u6311\u6218\u3002\u7ecf\u5178\u6a21\u578b\u53ef\u89e3\u91ca\u4f46\u9650\u5236\u6027\u5f3a\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7075\u6d3b\u4f46\u901a\u5e38\u4e0d\u53ef\u89e3\u91ca\u4e14\u5bf9\u566a\u58f0\u654f\u611f\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u53c8\u5177\u6709\u7075\u6d3b\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faGRAFT\u6a21\u578b\uff0c\u91c7\u7528\u6df7\u5408\u67b6\u6784\uff1a\u7ebf\u6027AFT\u6a21\u578b\u4e0e\u975e\u7ebf\u6027\u6b8b\u5dee\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\uff0c\u96c6\u6210\u968f\u673a\u95e8\u63a7\u8fdb\u884c\u81ea\u52a8\u7279\u5f81\u9009\u62e9\u3002\u4f7f\u7528\u57fa\u4e8e\u5c40\u90e8Kaplan-Meier\u4f30\u8ba1\u5668\u7684\u968f\u673a\u6761\u4ef6\u63d2\u8865\uff0c\u76f4\u63a5\u4f18\u5316\u53ef\u5fae\u5206\u7684C-index\u5bf9\u9f50\u6392\u540d\u635f\u5931\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGRAFT\u5728\u533a\u5206\u5ea6\u548c\u6821\u51c6\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u9ad8\u566a\u58f0\u73af\u5883\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u548c\u7a00\u758f\u6027\u3002", "conclusion": "GRAFT\u6210\u529f\u89e3\u51b3\u4e86\u751f\u5b58\u5206\u6790\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e2\u7075\u6d3b\u53c8\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u7a00\u758f\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002"}}
{"id": "2602.07889", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07889", "abs": "https://arxiv.org/abs/2602.07889", "authors": ["Long Chen", "Yinkui Liu", "Shen Li", "Bo Tang", "Xuemin Hu"], "title": "Efficient Anti-exploration via VQVAE and Fuzzy Clustering in Offline Reinforcement Learning", "comment": null, "summary": "Pseudo-count is an effective anti-exploration method in offline reinforcement learning (RL) by counting state-action pairs and imposing a large penalty on rare or unseen state-action pair data. Existing anti-exploration methods count continuous state-action pairs by discretizing these data, but often suffer from the issues of dimension disaster and information loss in the discretization process, leading to efficiency and performance reduction, and even failure of policy learning. In this paper, a novel anti-exploration method based on Vector Quantized Variational Autoencoder (VQVAE) and fuzzy clustering in offline RL is proposed. We first propose an efficient pseudo-count method based on the multi-codebook VQVAE to discretize state-action pairs, and design an offline RL anti-exploitation method based on the proposed pseudo-count method to handle the dimension disaster issue and improve the learning efficiency. In addition, a codebook update mechanism based on fuzzy C-means (FCM) clustering is developed to improve the use rate of vectors in codebooks, addressing the information loss issue in the discretization process. The proposed method is evaluated on the benchmark of Datasets for Deep Data-Driven Reinforcement Learning (D4RL), and experimental results show that the proposed method performs better and requires less computing cost in multiple complex tasks compared to state-of-the-art (SOTA) methods.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eVQVAE\u548c\u6a21\u7cca\u805a\u7c7b\u7684\u79bb\u7ebfRL\u53cd\u63a2\u7d22\u65b9\u6cd5\uff0c\u89e3\u51b3\u8fde\u7eed\u72b6\u6001-\u52a8\u4f5c\u5bf9\u79bb\u6563\u5316\u4e2d\u7684\u7ef4\u5ea6\u707e\u96be\u548c\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u5728D4RL\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u4e8eSOTA\u65b9\u6cd5\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002", "motivation": "\u73b0\u6709\u79bb\u7ebfRL\u4e2d\u7684\u53cd\u63a2\u7d22\u65b9\u6cd5\u901a\u8fc7\u79bb\u6563\u5316\u8fde\u7eed\u72b6\u6001-\u52a8\u4f5c\u5bf9\u8fdb\u884c\u8ba1\u6570\uff0c\u4f46\u5b58\u5728\u7ef4\u5ea6\u707e\u96be\u548c\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u5bfc\u81f4\u6548\u7387\u964d\u4f4e\u3001\u6027\u80fd\u4e0b\u964d\u751a\u81f3\u7b56\u7565\u5b66\u4e60\u5931\u8d25\u3002", "method": "1) \u57fa\u4e8e\u591a\u7801\u672cVQVAE\u7684\u9ad8\u6548\u4f2a\u8ba1\u6570\u65b9\u6cd5\u79bb\u6563\u5316\u72b6\u6001-\u52a8\u4f5c\u5bf9\uff1b2) \u57fa\u4e8e\u8be5\u4f2a\u8ba1\u6570\u65b9\u6cd5\u7684\u79bb\u7ebfRL\u53cd\u5229\u7528\u65b9\u6cd5\uff1b3) \u57fa\u4e8e\u6a21\u7ccaC\u5747\u503c\u805a\u7c7b\u7684\u7801\u672c\u66f4\u65b0\u673a\u5236\u63d0\u9ad8\u5411\u91cf\u4f7f\u7528\u7387\u3002", "result": "\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\uff0c\u4e14\u9700\u8981\u66f4\u5c11\u7684\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eVQVAE\u548c\u6a21\u7cca\u805a\u7c7b\u7684\u53cd\u63a2\u7d22\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8fde\u7eed\u72b6\u6001-\u52a8\u4f5c\u5bf9\u79bb\u6563\u5316\u4e2d\u7684\u7ef4\u5ea6\u707e\u96be\u548c\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u79bb\u7ebfRL\u7684\u5b66\u4e60\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2602.07892", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07892", "abs": "https://arxiv.org/abs/2602.07892", "authors": ["Guanglong Sun", "Siyuan Zhang", "Liyuan Wang", "Jun Zhu", "Hang Su", "Yi Zhong"], "title": "Safety Alignment as Continual Learning: Mitigating the Alignment Tax via Orthogonal Gradient Projection", "comment": null, "summary": "Large Language Models (LLMs) often incur an alignment tax: safety post-training can reduce general utility (e.g., reasoning and coding). We argue that this tax primarily arises from continual-learning-style forgetting in sequential alignment, where distribution shift and conflicting objectives cause safety updates to overwrite pre-trained competencies. Accordingly, we cast safety alignment as a continual learning (CL) problem that must balance plasticity (acquiring safety constraints) and stability (preserving general abilities). We propose Orthogonal Gradient Projection for Safety Alignment (OGPSA), a lightweight method that mitigates interference by constraining each safety update to be orthogonal (in a first-order sense) to a learned subspace capturing general capabilities. Specifically, OGPSA estimates a low-rank capability subspace from gradients on a small reference set and projects the safety gradient onto its orthogonal complement before updating. This produces safety-directed updates that minimally perturb prior knowledge while retaining capacity for alignment. OGPSA is plug-and-play and integrates into standard post-training pipelines without large-scale replay, auxiliary objectives, or retraining. Across Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and sequential SFT$\\rightarrow$DPO settings, OGPSA consistently improves the safety--utility Pareto frontier over standard baselines. For instance, on Qwen2.5-7B-Instruct under SFT$\\rightarrow$DPO, OGPSA preserves strong safety while recovering general capability, improving SimpleQA from 0.53\\% to 3.03\\% and IFEval from 51.94\\% to 63.96\\%. Our source code is available at \\href{https://github.com/SunGL001/OGPSA}{OGPSA}", "AI": {"tldr": "OGPSA\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u4ea4\u68af\u5ea6\u6295\u5f71\u89e3\u51b3LLM\u5b89\u5168\u5bf9\u9f50\u4e2d\u7684\u9057\u5fd8\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u901a\u7528\u80fd\u529b\u7684\u540c\u65f6\u63d0\u5347\u5b89\u5168\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fdb\u884c\u5b89\u5168\u5bf9\u9f50\u540e\u8bad\u7ec3\u65f6\uff0c\u5f80\u5f80\u4f1a\u51fa\u73b0\"\u5bf9\u9f50\u7a0e\"\u73b0\u8c61\u2014\u2014\u5b89\u5168\u6027\u7684\u63d0\u5347\u4ee5\u901a\u7528\u80fd\u529b\uff08\u5982\u63a8\u7406\u548c\u7f16\u7801\uff09\u7684\u4e0b\u964d\u4e3a\u4ee3\u4ef7\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u987a\u5e8f\u5bf9\u9f50\u4e2d\u7684\u6301\u7eed\u5b66\u4e60\u5f0f\u9057\u5fd8\u9020\u6210\u7684\uff0c\u5206\u5e03\u504f\u79fb\u548c\u51b2\u7a81\u76ee\u6807\u5bfc\u81f4\u5b89\u5168\u66f4\u65b0\u8986\u76d6\u4e86\u9884\u8bad\u7ec3\u83b7\u5f97\u7684\u80fd\u529b\u3002", "method": "\u5c06\u5b89\u5168\u5bf9\u9f50\u89c6\u4e3a\u6301\u7eed\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u51fa\u6b63\u4ea4\u68af\u5ea6\u6295\u5f71\u5b89\u5168\u5bf9\u9f50\uff08OGPSA\uff09\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u4f30\u8ba1\u4e00\u4e2a\u4f4e\u79e9\u80fd\u529b\u5b50\u7a7a\u95f4\uff08\u4ece\u5c0f\u578b\u53c2\u8003\u96c6\u7684\u68af\u5ea6\u4e2d\u5b66\u4e60\uff09\uff0c\u7136\u540e\u5c06\u5b89\u5168\u68af\u5ea6\u6295\u5f71\u5230\u8be5\u5b50\u7a7a\u95f4\u7684\u6b63\u4ea4\u8865\u7a7a\u95f4\u4e0a\u8fdb\u884c\u66f4\u65b0\u3002\u8fd9\u6837\u4ea7\u751f\u7684\u5b89\u5168\u5bfc\u5411\u66f4\u65b0\u80fd\u6700\u5c0f\u5316\u5bf9\u5148\u524d\u77e5\u8bc6\u7684\u6270\u52a8\uff0c\u540c\u65f6\u4fdd\u7559\u5bf9\u9f50\u80fd\u529b\u3002", "result": "\u5728\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u3001\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u4ee5\u53ca\u987a\u5e8fSFT\u2192DPO\u8bbe\u7f6e\u4e2d\uff0cOGPSA\u76f8\u6bd4\u6807\u51c6\u57fa\u7ebf\u4e00\u81f4\u5730\u6539\u5584\u4e86\u5b89\u5168-\u6548\u7528\u5e15\u7d2f\u6258\u524d\u6cbf\u3002\u4f8b\u5982\uff0c\u5728Qwen2.5-7B-Instruct\u6a21\u578b\u7684SFT\u2192DPO\u8bbe\u7f6e\u4e0b\uff0cOGPSA\u5728\u4fdd\u6301\u5f3a\u5927\u5b89\u5168\u6027\u7684\u540c\u65f6\u6062\u590d\u4e86\u901a\u7528\u80fd\u529b\uff0c\u5c06SimpleQA\u4ece0.53%\u63d0\u5347\u52303.03%\uff0cIFEval\u4ece51.94%\u63d0\u5347\u523063.96%\u3002", "conclusion": "OGPSA\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u56de\u653e\u3001\u8f85\u52a9\u76ee\u6807\u6216\u91cd\u65b0\u8bad\u7ec3\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3\u5b89\u5168\u5bf9\u9f50\u4e2d\u7684\u80fd\u529b\u9057\u5fd8\u95ee\u9898\uff0c\u5b9e\u73b0\u5b89\u5168\u6027\u548c\u901a\u7528\u80fd\u529b\u7684\u66f4\u597d\u5e73\u8861\u3002"}}
{"id": "2602.07904", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07904", "abs": "https://arxiv.org/abs/2602.07904", "authors": ["Giang Ngo", "Dat Phan Trong", "Dang Nguyen", "Sunil Gupta", "Svetha Venkatesh"], "title": "Adaptive Acquisition Selection for Bayesian Optimization with Large Language Models", "comment": null, "summary": "Bayesian Optimization critically depends on the choice of acquisition function, but no single strategy is universally optimal; the best choice is non-stationary and problem-dependent. Existing adaptive portfolio methods often base their decisions on past function values while ignoring richer information like remaining budget or surrogate model characteristics. To address this, we introduce LMABO, a novel framework that casts a pre-trained Large Language Model (LLM) as a zero-shot, online strategist for the BO process. At each iteration, LMABO uses a structured state representation to prompt the LLM to select the most suitable acquisition function from a diverse portfolio. In an evaluation across 50 benchmark problems, LMABO demonstrates a significant performance improvement over strong static, adaptive portfolio, and other LLM-based baselines. We show that the LLM's behavior is a comprehensive strategy that adapts to real-time progress, proving its advantage stems from its ability to process and synthesize the complete optimization state into an effective, adaptive policy.", "AI": {"tldr": "LMABO\uff1a\u4f7f\u7528\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u96f6\u6837\u672c\u5728\u7ebf\u7b56\u7565\u5e08\uff0c\u4ece\u591a\u6837\u5316\u7ec4\u5408\u4e2d\u9009\u62e9\u6700\u4f18\u91c7\u96c6\u51fd\u6570\uff0c\u5728\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\u5b9e\u73b0\u81ea\u9002\u5e94\u7b56\u7565\u9009\u62e9", "motivation": "\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\u91c7\u96c6\u51fd\u6570\u7684\u9009\u62e9\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6ca1\u6709\u5355\u4e00\u7b56\u7565\u9002\u7528\u4e8e\u6240\u6709\u95ee\u9898\uff1b\u6700\u4f73\u9009\u62e9\u662f\u975e\u5e73\u7a33\u4e14\u95ee\u9898\u4f9d\u8d56\u7684\u3002\u73b0\u6709\u81ea\u9002\u5e94\u7ec4\u5408\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u5386\u53f2\u51fd\u6570\u503c\u505a\u51b3\u7b56\uff0c\u5ffd\u7565\u4e86\u5269\u4f59\u9884\u7b97\u3001\u4ee3\u7406\u6a21\u578b\u7279\u5f81\u7b49\u66f4\u4e30\u5bcc\u7684\u4fe1\u606f\u3002", "method": "\u63d0\u51faLMABO\u6846\u67b6\uff0c\u5c06\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8d1d\u53f6\u65af\u4f18\u5316\u8fc7\u7a0b\u7684\u96f6\u6837\u672c\u5728\u7ebf\u7b56\u7565\u5e08\u3002\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\uff0c\u4f7f\u7528\u7ed3\u6784\u5316\u72b6\u6001\u8868\u793a\u63d0\u793aLLM\u4ece\u591a\u6837\u5316\u7ec4\u5408\u4e2d\u9009\u62e9\u6700\u5408\u9002\u7684\u91c7\u96c6\u51fd\u6570\u3002", "result": "\u572850\u4e2a\u57fa\u51c6\u95ee\u9898\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cLMABO\u76f8\u6bd4\u9759\u6001\u7b56\u7565\u3001\u81ea\u9002\u5e94\u7ec4\u5408\u65b9\u6cd5\u548c\u5176\u4ed6\u57fa\u4e8eLLM\u7684\u57fa\u7ebf\u65b9\u6cd5\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002LLM\u7684\u884c\u4e3a\u5c55\u73b0\u51fa\u5168\u9762\u7684\u7b56\u7565\u9002\u5e94\u6027\uff0c\u80fd\u591f\u6839\u636e\u5b9e\u65f6\u8fdb\u5c55\u8c03\u6574\u51b3\u7b56\u3002", "conclusion": "LLM\u7684\u4f18\u52bf\u5728\u4e8e\u5176\u80fd\u591f\u5904\u7406\u5e76\u7efc\u5408\u5b8c\u6574\u7684\u4f18\u5316\u72b6\u6001\u4fe1\u606f\uff0c\u5f62\u6210\u6709\u6548\u7684\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u8bc1\u660e\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8d1d\u53f6\u65af\u4f18\u5316\u7b56\u7565\u9009\u62e9\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.07906", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07906", "abs": "https://arxiv.org/abs/2602.07906", "authors": ["Yuzhu Cai", "Zexi Liu", "Xinyu Zhu", "Cheng Wang", "Jiaao Chen", "Hanrui Wang", "Wei-Chen Wang", "Di Jin", "Siheng Chen"], "title": "AceGRPO: Adaptive Curriculum Enhanced Group Relative Policy Optimization for Autonomous Machine Learning Engineering", "comment": "17 pages, 5 figures", "summary": "Autonomous Machine Learning Engineering (MLE) requires agents to perform sustained, iterative optimization over long horizons. While recent LLM-based agents show promise, current prompt-based agents for MLE suffer from behavioral stagnation due to frozen parameters. Although Reinforcement Learning (RL) offers a remedy, applying it to MLE is hindered by prohibitive execution latency and inefficient data selection. Recognizing these challenges, we propose AceGRPO with two core components: (1) Evolving Data Buffer that continuously repurposes execution traces into reusable training tasks, and (2) Adaptive Sampling guided by a Learnability Potential function, which dynamically prioritizes tasks at the agent's learning frontier to maximize learning efficiency. Leveraging AceGRPO, our trained Ace-30B model achieves a 100% valid submission rate on MLE-Bench-Lite, approaches the performance of proprietary frontier models, and outperforms larger open-source baselines (e.g., DeepSeek-V3.2), demonstrating robust capability for sustained iterative optimization. Code is available at https://github.com/yuzhu-cai/AceGRPO.", "AI": {"tldr": "\u63d0\u51faAceGRPO\u6846\u67b6\u89e3\u51b3\u81ea\u4e3b\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u4e2d\u7684\u884c\u4e3a\u505c\u6ede\u95ee\u9898\uff0c\u901a\u8fc7\u6f14\u5316\u6570\u636e\u7f13\u51b2\u533a\u548c\u81ea\u9002\u5e94\u91c7\u6837\u673a\u5236\u63d0\u5347\u5b66\u4e60\u6548\u7387\uff0cAce-30B\u6a21\u578b\u5728MLE-Bench-Lite\u4e0a\u8fbe\u5230100%\u6709\u6548\u63d0\u4ea4\u7387\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684MLE\u4ee3\u7406\u5b58\u5728\u884c\u4e3a\u505c\u6ede\u95ee\u9898\uff0c\u56e0\u4e3a\u53c2\u6570\u56fa\u5b9a\u4e0d\u53d8\u3002\u867d\u7136\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u5728MLE\u5e94\u7528\u4e2d\u9762\u4e34\u6267\u884c\u5ef6\u8fdf\u9ad8\u548c\u6570\u636e\u9009\u62e9\u6548\u7387\u4f4e\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faAceGRPO\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u6f14\u5316\u6570\u636e\u7f13\u51b2\u533a\uff0c\u5c06\u6267\u884c\u8f68\u8ff9\u91cd\u65b0\u5229\u7528\u4e3a\u53ef\u91cd\u590d\u4f7f\u7528\u7684\u8bad\u7ec3\u4efb\u52a1\uff1b2) \u81ea\u9002\u5e94\u91c7\u6837\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u6027\u6f5c\u529b\u51fd\u6570\u52a8\u6001\u4f18\u5148\u5904\u7406\u4ee3\u7406\u5b66\u4e60\u8fb9\u754c\u4e0a\u7684\u4efb\u52a1\u4ee5\u6700\u5927\u5316\u5b66\u4e60\u6548\u7387\u3002", "result": "\u8bad\u7ec3\u7684Ace-30B\u6a21\u578b\u5728MLE-Bench-Lite\u4e0a\u8fbe\u5230100%\u6709\u6548\u63d0\u4ea4\u7387\uff0c\u6027\u80fd\u63a5\u8fd1\u524d\u6cbf\u4e13\u6709\u6a21\u578b\uff0c\u5e76\u4f18\u4e8e\u66f4\u5927\u7684\u5f00\u6e90\u57fa\u7ebf\u6a21\u578b\uff08\u5982DeepSeek-V3.2\uff09\uff0c\u5c55\u793a\u4e86\u6301\u7eed\u8fed\u4ee3\u4f18\u5316\u7684\u5f3a\u5927\u80fd\u529b\u3002", "conclusion": "AceGRPO\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u4e3b\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u4e2d\u7684\u884c\u4e3a\u505c\u6ede\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u91cd\u7528\u548c\u4efb\u52a1\u9009\u62e9\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u7387\uff0c\u4e3a\u6301\u7eed\u8fed\u4ee3\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07915", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07915", "abs": "https://arxiv.org/abs/2602.07915", "authors": ["Huiyang Yi", "Xiaojian Shen", "Yonggang Wu", "Duxin Chen", "He Wang", "Wenwu Yu"], "title": "CausalCompass: Evaluating the Robustness of Time-Series Causal Discovery in Misspecified Scenarios", "comment": null, "summary": "Causal discovery from time series is a fundamental task in machine learning. However, its widespread adoption is hindered by a reliance on untestable causal assumptions and by the lack of robustness-oriented evaluation in existing benchmarks. To address these challenges, we propose CausalCompass, a flexible and extensible benchmark suite designed to assess the robustness of time-series causal discovery (TSCD) methods under violations of modeling assumptions. To demonstrate the practical utility of CausalCompass, we conduct extensive benchmarking of representative TSCD algorithms across eight assumption-violation scenarios. Our experimental results indicate that no single method consistently attains optimal performance across all settings. Nevertheless, the methods exhibiting superior overall performance across diverse scenarios are almost invariably deep learning-based approaches. We further provide hyperparameter sensitivity analyses to deepen the understanding of these findings. We also find, somewhat surprisingly, that NTS-NOTEARS relies heavily on standardized preprocessing in practice, performing poorly in the vanilla setting but exhibiting strong performance after standardization. Finally, our work aims to provide a comprehensive and systematic evaluation of TSCD methods under assumption violations, thereby facilitating their broader adoption in real-world applications. The code and datasets are available at https://github.com/huiyang-yi/CausalCompass.", "AI": {"tldr": "CausalCompass\u662f\u4e00\u4e2a\u8bc4\u4f30\u65f6\u95f4\u5e8f\u5217\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u5728\u5efa\u6a21\u5047\u8bbe\u8fdd\u53cd\u60c5\u51b5\u4e0b\u9c81\u68d2\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u5b9e\u9a8c\u8868\u660e\u6ca1\u6709\u5355\u4e00\u65b9\u6cd5\u5728\u6240\u6709\u573a\u666f\u4e0b\u90fd\u6700\u4f18\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6574\u4f53\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u56e0\u679c\u53d1\u73b0\u7684\u5e7f\u6cdb\u5e94\u7528\u53d7\u5230\u4e24\u4e2a\u4e3b\u8981\u969c\u788d\uff1a1\uff09\u4f9d\u8d56\u4e0d\u53ef\u6d4b\u8bd5\u7684\u56e0\u679c\u5047\u8bbe\uff1b2\uff09\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u9762\u5411\u9c81\u68d2\u6027\u7684\u8bc4\u4f30\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u80fd\u591f\u7cfb\u7edf\u8bc4\u4f30\u65b9\u6cd5\u5728\u5047\u8bbe\u8fdd\u53cd\u60c5\u51b5\u4e0b\u8868\u73b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u63d0\u51fa\u4e86CausalCompass\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u8fd9\u662f\u4e00\u4e2a\u7075\u6d3b\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u8bc4\u4f30\u65f6\u95f4\u5e8f\u5217\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u5728\u516b\u79cd\u5047\u8bbe\u8fdd\u53cd\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u5bf9\u4ee3\u8868\u6027\u7b97\u6cd5\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u8fdb\u884c\u4e86\u8d85\u53c2\u6570\u654f\u611f\u6027\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a1\uff09\u6ca1\u6709\u5355\u4e00\u65b9\u6cd5\u5728\u6240\u6709\u8bbe\u7f6e\u4e0b\u90fd\u80fd\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff1b2\uff09\u5728\u5404\u79cd\u573a\u666f\u4e0b\u6574\u4f53\u8868\u73b0\u6700\u597d\u7684\u65b9\u6cd5\u51e0\u4e4e\u90fd\u662f\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff1b3\uff09NTS-NOTEARS\u5728\u5b9e\u8df5\u4e2d\u4e25\u91cd\u4f9d\u8d56\u6807\u51c6\u5316\u9884\u5904\u7406\uff0c\u5728\u539f\u59cb\u8bbe\u7f6e\u4e0b\u8868\u73b0\u5dee\u4f46\u6807\u51c6\u5316\u540e\u8868\u73b0\u5f3a\u52b2\u3002", "conclusion": "CausalCompass\u4e3a\u65f6\u95f4\u5e8f\u5217\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u5728\u5047\u8bbe\u8fdd\u53cd\u60c5\u51b5\u4e0b\u63d0\u4f9b\u4e86\u5168\u9762\u7cfb\u7edf\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u4fc3\u8fdb\u8fd9\u4e9b\u65b9\u6cd5\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u66f4\u5e7f\u6cdb\u91c7\u7528\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.07928", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07928", "abs": "https://arxiv.org/abs/2602.07928", "authors": ["Ziyun Li", "Huancheng Hu", "Soon Hoe Lim", "Xuyu Li", "Fei Gao", "Enmao Diao", "Zezhen Ding", "Michalis Vazirgiannis", "Henrik Bostrom"], "title": "A Kinetic-Energy Perspective of Flow Matching", "comment": null, "summary": "Flow-based generative models can be viewed through a physics lens: sampling transports a particle from noise to data by integrating a time-varying velocity field, and each sample corresponds to a trajectory with its own dynamical effort. Motivated by classical mechanics, we introduce Kinetic Path Energy (KPE), an action-like, per-sample diagnostic that measures the accumulated kinetic effort along an Ordinary Differential Equation (ODE) trajectory. Empirically, KPE exhibits two robust correspondences: (i) higher KPE predicts stronger semantic fidelity; (ii) high-KPE trajectories terminate on low-density manifold frontiers. We further provide theoretical guarantees linking trajectory energy to data density. Paradoxically, this correlation is non-monotonic. At sufficiently high energy, generation can degenerate into memorization. Leveraging the closed-form of empirical flow matching, we show that extreme energies drive trajectories toward near-copies of training examples. This yields a Goldilocks principle and motivates Kinetic Trajectory Shaping (KTS), a training-free two-phase inference strategy that boosts early motion and enforces a late-time soft landing, reducing memorization and improving generation quality across benchmark tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u52a8\u80fd\u8def\u5f84\u80fd\u91cf(KPE)\u4f5c\u4e3a\u6d41\u751f\u6210\u6a21\u578b\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u53d1\u73b0KPE\u4e0e\u8bed\u4e49\u4fdd\u771f\u5ea6\u6b63\u76f8\u5173\uff0c\u4f46\u8fc7\u9ad8\u80fd\u91cf\u4f1a\u5bfc\u81f4\u8bb0\u5fc6\u5316\uff0c\u8fdb\u800c\u63d0\u51fa\u65e0\u8bad\u7ec3\u7684\u4e24\u9636\u6bb5\u63a8\u7406\u7b56\u7565KTS\u6765\u4f18\u5316\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u53d7\u7ecf\u5178\u529b\u5b66\u542f\u53d1\uff0c\u4f5c\u8005\u5e0c\u671b\u4e3a\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b\u5f00\u53d1\u4e00\u4e2a\u7c7b\u4f3c\"\u4f5c\u7528\u91cf\"\u7684\u6bcf\u6837\u672c\u8bca\u65ad\u5de5\u5177\uff0c\u4ee5\u7406\u89e3ODE\u8f68\u8ff9\u7684\u52a8\u529b\u5b66\u7279\u6027\uff0c\u5e76\u89e3\u51b3\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u8bb0\u5fc6\u5316\u95ee\u9898\u3002", "method": "\u5f15\u5165\u52a8\u80fd\u8def\u5f84\u80fd\u91cf(KPE)\u4f5c\u4e3a\u8f68\u8ff9\u7d2f\u79ef\u52a8\u80fd\u7684\u5ea6\u91cf\uff0c\u57fa\u4e8e\u7ecf\u9a8c\u6d41\u5339\u914d\u7684\u95ed\u5f0f\u89e3\u5206\u6790\u8f68\u8ff9\u80fd\u91cf\u4e0e\u6570\u636e\u5bc6\u5ea6\u7684\u7406\u8bba\u5173\u7cfb\uff0c\u63d0\u51faKinetic Trajectory Shaping(KTS)\u4e24\u9636\u6bb5\u63a8\u7406\u7b56\u7565\uff1a\u65e9\u671f\u589e\u5f3a\u8fd0\u52a8\uff0c\u540e\u671f\u8f6f\u7740\u9646\u3002", "result": "\u53d1\u73b0\u4e24\u4e2a\u7a33\u5065\u5bf9\u5e94\u5173\u7cfb\uff1a1)\u66f4\u9ad8KPE\u9884\u6d4b\u66f4\u5f3a\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\uff1b2)\u9ad8KPE\u8f68\u8ff9\u7ec8\u6b62\u4e8e\u4f4e\u5bc6\u5ea6\u6d41\u5f62\u8fb9\u754c\u3002\u7406\u8bba\u8bc1\u660e\u8f68\u8ff9\u80fd\u91cf\u4e0e\u6570\u636e\u5bc6\u5ea6\u76f8\u5173\u4f46\u975e\u5355\u8c03\uff0c\u6781\u7aef\u80fd\u91cf\u4f1a\u5bfc\u81f4\u8bad\u7ec3\u6837\u672c\u7684\u8fd1\u4f3c\u590d\u5236\u3002", "conclusion": "\u63d0\u51faGoldilocks\u539f\u5219\uff1a\u8f68\u8ff9\u80fd\u91cf\u9700\u8981\u9002\u4e2d\uff0c\u8fc7\u9ad8\u4f1a\u5bfc\u81f4\u8bb0\u5fc6\u5316\u3002KTS\u7b56\u7565\u80fd\u6709\u6548\u51cf\u5c11\u8bb0\u5fc6\u5316\uff0c\u5728\u57fa\u51c6\u4efb\u52a1\u4e0a\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u6d41\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u8bca\u65ad\u548c\u4f18\u5316\u5de5\u5177\u3002"}}
{"id": "2602.07933", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07933", "abs": "https://arxiv.org/abs/2602.07933", "authors": ["Olamide Samuel Oseni", "Ibraheem Omotolani Obanla", "Toheeb Aduramomi Jimoh"], "title": "Attention-Based Deep Learning for Early Parkinson's Disease Detection with Tabular Biomedical Data", "comment": null, "summary": "Early and accurate detection of Parkinson's disease (PD) remains a critical challenge in medical diagnostics due to the subtlety of early-stage symptoms and the complex, non-linear relationships inherent in biomedical data. Traditional machine learning (ML) models, though widely applied to PD detection, often rely on extensive feature engineering and struggle to capture complex feature interactions. This study investigates the effectiveness of attention-based deep learning models for early PD detection using tabular biomedical data. We present a comparative evaluation of four classification models: Multi-Layer Perceptron (MLP), Gradient Boosting, TabNet, and SAINT, using a benchmark dataset from the UCI Machine Learning Repository consisting of biomedical voice measurements from PD patients and healthy controls.\n  Experimental results show that SAINT consistently outperformed all baseline models across multiple evaluation metrics, achieving a weighted precision of 0.98, weighted recall of 0.97, weighted F1-score of 0.97, a Matthews Correlation Coefficient (MCC) of 0.9990, and the highest Area Under the ROC Curve (AUC-ROC). TabNet and MLP demonstrated competitive performance, while Gradient Boosting yielded the lowest overall scores. The superior performance of SAINT is attributed to its dual attention mechanism, which effectively models feature interactions within and across samples.\n  These findings demonstrate the diagnostic potential of attention-based deep learning architectures for early Parkinson's disease detection and highlight the importance of dynamic feature representation in clinical prediction tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u56db\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5e15\u91d1\u68ee\u75c5\u65e9\u671f\u68c0\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684SAINT\u6a21\u578b\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u3002", "motivation": "\u5e15\u91d1\u68ee\u75c5\u7684\u65e9\u671f\u51c6\u786e\u68c0\u6d4b\u9762\u4e34\u6311\u6218\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9700\u8981\u5927\u91cf\u7279\u5f81\u5de5\u7a0b\u4e14\u96be\u4ee5\u6355\u6349\u590d\u6742\u7684\u7279\u5f81\u4ea4\u4e92\u5173\u7cfb\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5e15\u91d1\u68ee\u75c5\u65e9\u671f\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u4f7f\u7528UCI\u673a\u5668\u5b66\u4e60\u5e93\u4e2d\u7684\u5e15\u91d1\u68ee\u75c5\u751f\u7269\u533b\u5b66\u8bed\u97f3\u6d4b\u91cf\u6570\u636e\u96c6\uff0c\u5bf9\u6bd4\u8bc4\u4f30\u4e86\u56db\u79cd\u5206\u7c7b\u6a21\u578b\uff1a\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u3001\u68af\u5ea6\u63d0\u5347\u3001TabNet\u548cSAINT\u3002SAINT\u91c7\u7528\u53cc\u91cd\u6ce8\u610f\u529b\u673a\u5236\u6765\u5efa\u6a21\u7279\u5f81\u4ea4\u4e92\u3002", "result": "SAINT\u5728\u6240\u6709\u57fa\u7ebf\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u52a0\u6743\u7cbe\u5ea60.98\uff0c\u52a0\u6743\u53ec\u56de\u73870.97\uff0c\u52a0\u6743F1\u5206\u65700.97\uff0c\u9a6c\u4fee\u65af\u76f8\u5173\u7cfb\u65700.9990\uff0c\u4ee5\u53ca\u6700\u9ad8\u7684ROC\u66f2\u7ebf\u4e0b\u9762\u79ef\u3002TabNet\u548cMLP\u8868\u73b0\u7ade\u4e89\u6027\uff0c\u68af\u5ea6\u63d0\u5347\u8868\u73b0\u6700\u5dee\u3002", "conclusion": "\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5728\u5e15\u91d1\u68ee\u75c5\u65e9\u671f\u68c0\u6d4b\u4e2d\u5177\u6709\u8bca\u65ad\u6f5c\u529b\uff0c\u52a8\u6001\u7279\u5f81\u8868\u793a\u5728\u4e34\u5e8a\u9884\u6d4b\u4efb\u52a1\u4e2d\u81f3\u5173\u91cd\u8981\u3002SAINT\u7684\u53cc\u91cd\u6ce8\u610f\u529b\u673a\u5236\u80fd\u6709\u6548\u5efa\u6a21\u7279\u5f81\u4ea4\u4e92\u3002"}}
{"id": "2602.07950", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07950", "abs": "https://arxiv.org/abs/2602.07950", "authors": ["Daisuke Okanohara"], "title": "A Thermodynamic Theory of Learning Part II: Critical Period Closure and Continual Learning Failure", "comment": "Part II of a series entitled \"A Thermodynamic Theory of Learning.\"", "summary": "Learning performed over finite time is necessarily irreversible. In Part~I of this series, we modeled learning as a transport process in the space of parameter distributions and derived the Epistemic Speed Limit, which lower-bounds entropy production under finite-time learning.\n  In this work (Part~II), we study the consequences of this irreversibility for continual learning from a trajectory-level perspective. We show that finite dissipation constrains not only which solutions are reachable, but which learning paths remain dynamically accessible.\n  Although a continuum of task-equivalent realizations can achieve identical task performance, finite-time learning irreversibly selects among these realizations. This selection occurs through the progressive elimination of degrees of freedom that would otherwise enable structural reconfiguration. We refer to this phenomenon as \\emph{critical period closure}: beyond a certain stage of learning, transitions between compatible representations become dynamically inaccessible under any finite dissipation budget.\n  As a result, continual learning failure arises not from the absence of solutions satisfying multiple tasks, but from an irreversible loss of representational freedom induced by prior learning. This reframes catastrophic forgetting as a dynamical constraint imposed by finite-time dissipation, rather than direct task interference.", "AI": {"tldr": "\u6709\u9650\u65f6\u95f4\u5b66\u4e60\u5fc5\u7136\u4e0d\u53ef\u9006\uff0c\u5bfc\u81f4\u4e34\u754c\u671f\u95ed\u5408\u73b0\u8c61\uff0c\u4f7f\u517c\u5bb9\u8868\u793a\u95f4\u7684\u8f6c\u6362\u53d8\u5f97\u52a8\u6001\u4e0d\u53ef\u8fbe\uff0c\u4ece\u800c\u5f15\u53d1\u707e\u96be\u6027\u9057\u5fd8", "motivation": "\u7814\u7a76\u6709\u9650\u65f6\u95f4\u5b66\u4e60\u4e2d\u7684\u4e0d\u53ef\u9006\u6027\u5982\u4f55\u5f71\u54cd\u6301\u7eed\u5b66\u4e60\uff0c\u7279\u522b\u662f\u4ece\u8f68\u8ff9\u5c42\u9762\u7406\u89e3\u5b66\u4e60\u8def\u5f84\u7684\u52a8\u6001\u53ef\u8fbe\u6027\u9650\u5236", "method": "\u5c06\u5b66\u4e60\u5efa\u6a21\u4e3a\u53c2\u6570\u5206\u5e03\u7a7a\u95f4\u4e2d\u7684\u4f20\u8f93\u8fc7\u7a0b\uff0c\u4ece\u8f68\u8ff9\u5c42\u9762\u5206\u6790\u6709\u9650\u8017\u6563\u5bf9\u5b66\u4e60\u8def\u5f84\u7684\u7ea6\u675f", "result": "\u6709\u9650\u8017\u6563\u4e0d\u4ec5\u9650\u5236\u53ef\u8fbe\u89e3\uff0c\u8fd8\u9650\u5236\u52a8\u6001\u53ef\u8fbe\u7684\u5b66\u4e60\u8def\u5f84\uff1b\u6709\u9650\u65f6\u95f4\u5b66\u4e60\u5728\u4efb\u52a1\u7b49\u4ef7\u5b9e\u73b0\u4e2d\u8fdb\u884c\u9009\u62e9\uff0c\u901a\u8fc7\u9010\u6b65\u6d88\u9664\u81ea\u7531\u5ea6\u5bfc\u81f4\u4e34\u754c\u671f\u95ed\u5408", "conclusion": "\u6301\u7eed\u5b66\u4e60\u5931\u8d25\u6e90\u4e8e\u5148\u524d\u5b66\u4e60\u5f15\u8d77\u7684\u8868\u793a\u81ea\u7531\u5ea6\u7684\u4e0d\u53ef\u9006\u4e27\u5931\uff0c\u800c\u975e\u4efb\u52a1\u95f4\u76f4\u63a5\u5e72\u6270\uff1b\u707e\u96be\u6027\u9057\u5fd8\u5e94\u88ab\u91cd\u65b0\u7406\u89e3\u4e3a\u6709\u9650\u65f6\u95f4\u8017\u6563\u65bd\u52a0\u7684\u52a8\u6001\u7ea6\u675f"}}
{"id": "2602.07966", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07966", "abs": "https://arxiv.org/abs/2602.07966", "authors": ["Pablo Hidalgo", "Daniel Rodriguez"], "title": "An Explainable Multi-Task Similarity Measure: Integrating Accumulated Local Effects and Weighted Fr\u00e9chet Distance", "comment": null, "summary": "In many machine learning contexts, tasks are often treated as interconnected components with the goal of leveraging knowledge transfer between them, which is the central aim of Multi-Task Learning (MTL). Consequently, this multi-task scenario requires addressing critical questions: which tasks are similar, and how and why do they exhibit similarity? In this work, we propose a multi-task similarity measure based on Explainable Artificial Intelligence (XAI) techniques, specifically Accumulated Local Effects (ALE) curves.\n  ALE curves are compared using the Fr\u00e9chet distance, weighted by the data distribution, and the resulting similarity measure incorporates the importance of each feature. The measure is applicable in both single-task learning scenarios, where each task is trained separately, and multi-task learning scenarios, where all tasks are learned simultaneously. The measure is model-agnostic, allowing the use of different machine learning models across tasks. A scaling factor is introduced to account for differences in predictive performance across tasks, and several recommendations are provided for applying the measure in complex scenarios.\n  We validate this measure using four datasets, one synthetic dataset and three real-world datasets. The real-world datasets include a well-known Parkinson's dataset and a bike-sharing usage dataset -- both structured in tabular format -- as well as the CelebA dataset, which is used to evaluate the application of concept bottleneck encoders in a multitask learning setting. The results demonstrate that the measure aligns with intuitive expectations of task similarity across both tabular and non-tabular data, making it a valuable tool for exploring relationships between tasks and supporting informed decision-making.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u7684\u591a\u4efb\u52a1\u76f8\u4f3c\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u4f7f\u7528\u7d2f\u79ef\u5c40\u90e8\u6548\u5e94\uff08ALE\uff09\u66f2\u7ebf\u548cFr\u00e9chet\u8ddd\u79bb\uff0c\u80fd\u591f\u8bc4\u4f30\u4efb\u52a1\u95f4\u7684\u76f8\u4f3c\u6027\u5e76\u652f\u6301\u591a\u4efb\u52a1\u5b66\u4e60\u51b3\u7b56\u3002", "motivation": "\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\uff0c\u9700\u8981\u7406\u89e3\u54ea\u4e9b\u4efb\u52a1\u76f8\u4f3c\u3001\u5982\u4f55\u76f8\u4f3c\u4ee5\u53ca\u4e3a\u4ec0\u4e48\u76f8\u4f3c\uff0c\u4ee5\u4fbf\u66f4\u597d\u5730\u8fdb\u884c\u77e5\u8bc6\u8fc1\u79fb\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u4efb\u52a1\u76f8\u4f3c\u6027\u7684\u7cfb\u7edf\u5ea6\u91cf\uff0c\u7279\u522b\u662f\u5728\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7d2f\u79ef\u5c40\u90e8\u6548\u5e94\uff08ALE\uff09\u66f2\u7ebf\u7684\u591a\u4efb\u52a1\u76f8\u4f3c\u6027\u5ea6\u91cf\uff1a1\uff09\u4f7f\u7528ALE\u66f2\u7ebf\u5206\u6790\u7279\u5f81\u5bf9\u9884\u6d4b\u7684\u5f71\u54cd\uff1b2\uff09\u901a\u8fc7\u52a0\u6743Fr\u00e9chet\u8ddd\u79bb\u6bd4\u8f83ALE\u66f2\u7ebf\uff1b3\uff09\u8003\u8651\u6570\u636e\u5206\u5e03\u548c\u7279\u5f81\u91cd\u8981\u6027\uff1b4\uff09\u5f15\u5165\u7f29\u653e\u56e0\u5b50\u5904\u7406\u4e0d\u540c\u4efb\u52a1\u7684\u9884\u6d4b\u6027\u80fd\u5dee\u5f02\uff1b5\uff09\u65b9\u6cd5\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\uff0c\u9002\u7528\u4e8e\u5355\u4efb\u52a1\u548c\u591a\u4efb\u52a1\u573a\u666f\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u5ea6\u91cf\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff1a1\u4e2a\u5408\u6210\u6570\u636e\u96c6\u548c3\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\uff08Parkinson\u6570\u636e\u96c6\u3001\u81ea\u884c\u8f66\u5171\u4eab\u6570\u636e\u96c6\u3001CelebA\u6570\u636e\u96c6\uff09\u3002\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u5ea6\u91cf\u65b9\u6cd5\u5728\u8868\u683c\u548c\u975e\u8868\u683c\u6570\u636e\u4e0a\u90fd\u4e0e\u76f4\u89c9\u671f\u671b\u7684\u4efb\u52a1\u76f8\u4f3c\u6027\u4e00\u81f4\uff0c\u80fd\u591f\u6709\u6548\u63a2\u7d22\u4efb\u52a1\u95f4\u5173\u7cfb\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eALE\u66f2\u7ebf\u7684\u591a\u4efb\u52a1\u76f8\u4f3c\u6027\u5ea6\u91cf\u662f\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u5de5\u5177\uff0c\u80fd\u591f\u652f\u6301\u4efb\u52a1\u5173\u7cfb\u63a2\u7d22\u548c\u77e5\u60c5\u51b3\u7b56\u5236\u5b9a\uff0c\u4e3a\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u4efb\u52a1\u9009\u62e9\u548c\u77e5\u8bc6\u8fc1\u79fb\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u91cf\u5316\u4f9d\u636e\u3002"}}
{"id": "2602.07973", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07973", "abs": "https://arxiv.org/abs/2602.07973", "authors": ["Aaditya Naik", "Efthymia Tsamoura", "Shibo Jin", "Mayur Naik", "Dan Roth"], "title": "On Improving Neurosymbolic Learning by Exploiting the Representation Space", "comment": null, "summary": "We study the problem of learning neural classifiers in a neurosymbolic setting where the hidden gold labels of input instances must satisfy a logical formula. Learning in this setting proceeds by first computing (a subset of) the possible combinations of labels that satisfy the formula and then computing a loss using those combinations and the classifiers' scores. One challenge is that the space of label combinations can grow exponentially, making learning difficult. We propose a technique that prunes this space by exploiting the intuition that instances with similar latent representations are likely to share the same label. While this intuition has been widely used in weakly supervised learning, its application in our setting is challenging due to label dependencies imposed by logical constraints. We formulate the pruning process as an integer linear program that discards inconsistent label combinations while respecting logical structure. Our approach, CLIPPER, is orthogonal to existing training algorithms and can be seamlessly integrated with them. Across 16 benchmarks over complex neurosymbolic tasks, we demonstrate that CLIPPER boosts the performance of state-of-the-art neurosymbolic engines like Scallop, Dolphin, and ISED by up to 48%, 53%, and 8%, leading to state-of-the-art accuracies.", "AI": {"tldr": "CLIPPER\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u795e\u7ecf\u7b26\u53f7\u5b66\u4e60\u4e2d\u7684\u526a\u679d\u6280\u672f\uff0c\u901a\u8fc7\u6574\u6570\u7ebf\u6027\u89c4\u5212\u53bb\u9664\u4e0d\u4e00\u81f4\u7684\u6807\u7b7e\u7ec4\u5408\uff0c\u663e\u8457\u63d0\u5347\u73b0\u6709\u795e\u7ecf\u7b26\u53f7\u5f15\u64ce\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u795e\u7ecf\u7b26\u53f7\u5b66\u4e60\u4e2d\uff0c\u8f93\u5165\u5b9e\u4f8b\u7684\u9690\u85cf\u9ec4\u91d1\u6807\u7b7e\u5fc5\u987b\u6ee1\u8db3\u903b\u8f91\u516c\u5f0f\uff0c\u4f46\u53ef\u80fd\u7684\u6807\u7b7e\u7ec4\u5408\u7a7a\u95f4\u4f1a\u6307\u6570\u7ea7\u589e\u957f\uff0c\u5bfc\u81f4\u5b66\u4e60\u56f0\u96be\u3002", "method": "\u5229\u7528\"\u5177\u6709\u76f8\u4f3c\u6f5c\u5728\u8868\u793a\u7684\u5b9e\u4f8b\u53ef\u80fd\u5171\u4eab\u76f8\u540c\u6807\u7b7e\"\u7684\u76f4\u89c9\uff0c\u5c06\u526a\u679d\u8fc7\u7a0b\u5f62\u5f0f\u5316\u4e3a\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff0c\u5728\u5c0a\u91cd\u903b\u8f91\u7ed3\u6784\u7684\u540c\u65f6\u4e22\u5f03\u4e0d\u4e00\u81f4\u7684\u6807\u7b7e\u7ec4\u5408\u3002", "result": "\u572816\u4e2a\u590d\u6742\u795e\u7ecf\u7b26\u53f7\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCLIPPER\u5c06Scallop\u3001Dolphin\u548cISED\u7b49\u6700\u5148\u8fdb\u795e\u7ecf\u7b26\u53f7\u5f15\u64ce\u7684\u6027\u80fd\u5206\u522b\u63d0\u5347\u9ad8\u8fbe48%\u300153%\u548c8%\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\u3002", "conclusion": "CLIPPER\u662f\u4e00\u79cd\u6b63\u4ea4\u4e8e\u73b0\u6709\u8bad\u7ec3\u7b97\u6cd5\u7684\u526a\u679d\u65b9\u6cd5\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u795e\u7ecf\u7b26\u53f7\u5f15\u64ce\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u6807\u7b7e\u7ec4\u5408\u7a7a\u95f4\u7206\u70b8\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5b66\u4e60\u6027\u80fd\u3002"}}
{"id": "2602.07974", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07974", "abs": "https://arxiv.org/abs/2602.07974", "authors": ["Xin Li"], "title": "Beyond Optimization: Intelligence as Metric-Topology Factorization under Geometric Incompleteness", "comment": null, "summary": "Contemporary ML often equates intelligence with optimization: searching for solutions within a fixed representational geometry. This works in static regimes but breaks under distributional shift, task permutation, and continual learning, where even mild topological changes can invalidate learned solutions and trigger catastrophic forgetting. We propose Metric-Topology Factorization (MTF) as a unifying geometric principle: intelligence is not navigation through a fixed maze, but the ability to reshape representational geometry so desired behaviors become stable attractors. Learning corresponds to metric contraction (a controlled deformation of Riemannian structure), while task identity and environmental variation are encoded topologically and stored separately in memory. We show any fixed metric is geometrically incomplete: for any local metric representation, some topological transformations make it singular or incoherent, implying an unavoidable stability-plasticity tradeoff for weight-based systems. MTF resolves this by factorizing stable topology from plastic metric warps, enabling rapid adaptation via geometric switching rather than re-optimization. Building on this, we introduce the Topological Urysohn Machine (TUM), implementing MTF through memory-amortized metric inference (MAMI): spectral task signatures index amortized metric transformations, letting a single learned geometry be reused across permuted, reflected, or parity-altered environments. This explains robustness to task reordering, resistance to catastrophic forgetting, and generalization across transformations that defeat conventional continual learning methods (e.g., EWC).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5ea6\u91cf-\u62d3\u6251\u5206\u89e3(MTF)\u4f5c\u4e3a\u7edf\u4e00\u51e0\u4f55\u539f\u7406\uff1a\u667a\u80fd\u4e0d\u662f\u56fa\u5b9a\u51e0\u4f55\u4e2d\u7684\u5bfc\u822a\uff0c\u800c\u662f\u91cd\u5851\u8868\u793a\u51e0\u4f55\u4f7f\u671f\u671b\u884c\u4e3a\u6210\u4e3a\u7a33\u5b9a\u5438\u5f15\u5b50\u7684\u80fd\u529b\u3002\u5b66\u4e60\u5bf9\u5e94\u5ea6\u91cf\u6536\u7f29\uff0c\u4efb\u52a1\u8eab\u4efd\u548c\u73af\u5883\u53d8\u5316\u7f16\u7801\u4e3a\u62d3\u6251\u7ed3\u6784\u5355\u72ec\u5b58\u50a8\u3002\u57fa\u4e8e\u6b64\u5f15\u5165\u62d3\u6251Urysohn\u673a(TUM)\uff0c\u901a\u8fc7\u8bb0\u5fc6\u644a\u9500\u5ea6\u91cf\u63a8\u7406\u5b9e\u73b0MTF\u3002", "motivation": "\u4f20\u7edfML\u5c06\u667a\u80fd\u7b49\u540c\u4e8e\u4f18\u5316\uff1a\u5728\u56fa\u5b9a\u8868\u793a\u51e0\u4f55\u4e2d\u641c\u7d22\u89e3\u3002\u8fd9\u5728\u9759\u6001\u73af\u5883\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u5206\u5e03\u504f\u79fb\u3001\u4efb\u52a1\u7f6e\u6362\u548c\u6301\u7eed\u5b66\u4e60\u7b49\u52a8\u6001\u573a\u666f\u4e2d\u5931\u6548\uff0c\u8f7b\u5fae\u62d3\u6251\u53d8\u5316\u5c31\u4f1a\u4f7f\u5b66\u4e60\u89e3\u5931\u6548\u5e76\u5f15\u53d1\u707e\u96be\u6027\u9057\u5fd8\u3002\u9700\u8981\u65b0\u7684\u51e0\u4f55\u6846\u67b6\u6765\u89e3\u51b3\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u6743\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5ea6\u91cf-\u62d3\u6251\u5206\u89e3(MTF)\uff1a\u5c06\u8868\u793a\u51e0\u4f55\u5206\u89e3\u4e3a\u7a33\u5b9a\u7684\u62d3\u6251\u7ed3\u6784\u548c\u53ef\u5851\u7684\u5ea6\u91cf\u53d8\u5f62\u3002\u5b66\u4e60\u662f\u5ea6\u91cf\u6536\u7f29\u8fc7\u7a0b\uff0c\u4efb\u52a1\u8eab\u4efd\u548c\u73af\u5883\u53d8\u5316\u7f16\u7801\u4e3a\u62d3\u6251\u7279\u5f81\u5355\u72ec\u5b58\u50a8\u3002\u5b9e\u73b0\u62d3\u6251Urysohn\u673a(TUM)\uff0c\u91c7\u7528\u8bb0\u5fc6\u644a\u9500\u5ea6\u91cf\u63a8\u7406(MAMI)\uff1a\u901a\u8fc7\u8c31\u4efb\u52a1\u7b7e\u540d\u7d22\u5f15\u644a\u9500\u7684\u5ea6\u91cf\u53d8\u6362\uff0c\u4f7f\u5355\u4e00\u5b66\u4e60\u51e0\u4f55\u53ef\u8de8\u53d8\u6362\u73af\u5883\u91cd\u7528\u3002", "result": "MTF\u89e3\u51b3\u4e86\u56fa\u5b9a\u5ea6\u91cf\u7684\u51e0\u4f55\u4e0d\u5b8c\u5907\u6027\u95ee\u9898\uff0c\u4efb\u4f55\u5c40\u90e8\u5ea6\u91cf\u8868\u793a\u5728\u67d0\u4e9b\u62d3\u6251\u53d8\u6362\u4e0b\u90fd\u4f1a\u53d8\u5f97\u5947\u5f02\u6216\u4e0d\u4e00\u81f4\u3002TUM\u5b9e\u73b0\u4e86\u5bf9\u4efb\u52a1\u91cd\u6392\u5e8f\u7684\u9c81\u68d2\u6027\u3001\u62b5\u6297\u707e\u96be\u6027\u9057\u5fd8\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u5728\u4f20\u7edf\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5(\u5982EWC)\u5931\u8d25\u7684\u53d8\u6362\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u667a\u80fd\u7684\u672c\u8d28\u4e0d\u662f\u56fa\u5b9a\u51e0\u4f55\u4e2d\u7684\u4f18\u5316\uff0c\u800c\u662f\u52a8\u6001\u91cd\u5851\u8868\u793a\u51e0\u4f55\u7684\u80fd\u529b\u3002\u5ea6\u91cf-\u62d3\u6251\u5206\u89e3\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u51e0\u4f55\u6846\u67b6\uff0c\u5c06\u7a33\u5b9a\u6027\u4e0e\u53ef\u5851\u6027\u89e3\u8026\uff0c\u901a\u8fc7\u51e0\u4f55\u5207\u6362\u800c\u975e\u91cd\u65b0\u4f18\u5316\u5b9e\u73b0\u5feb\u901f\u9002\u5e94\uff0c\u4e3a\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\u548c\u5206\u5e03\u504f\u79fb\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.07992", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07992", "abs": "https://arxiv.org/abs/2602.07992", "authors": ["Daniel Barzilai", "Yotam Wolf", "Ronen Basri"], "title": "When Is Compositional Reasoning Learnable from Verifiable Rewards?", "comment": null, "summary": "The emergence of compositional reasoning in large language models through reinforcement learning with verifiable rewards (RLVR) has been a key driver of recent empirical successes. Despite this progress, it remains unclear which compositional problems are learnable in this setting using outcome-level feedback alone. In this work, we theoretically study the learnability of compositional problems in autoregressive models under RLVR training. We identify a quantity that we call the task-advantage ratio, a joint property of the compositional problem and the base model, that characterizes which tasks and compositions are learnable from outcome-level feedback. On the positive side, using this characterization, we show that compositional problems where correct intermediate steps provide a clear advantage are efficiently learnable with RLVR. We also analyze how such an advantage naturally arises in different problems. On the negative side, when the structural advantage is not present, RLVR may converge to suboptimal compositions. We prove that, in some cases, the quality of the base model determines if such an advantage exists and whether RLVR will converge to a suboptimal solution. We hope our analysis can provide a principled theoretical understanding of when and why RLVR succeeds and when it does not.", "AI": {"tldr": "\u8bba\u6587\u4ece\u7406\u8bba\u4e0a\u7814\u7a76\u4e86\u5728RLVR\u8bad\u7ec3\u4e0b\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u7ec4\u5408\u95ee\u9898\u7684\u53ef\u5b66\u4e60\u6027\uff0c\u63d0\u51fa\u4e86\u4efb\u52a1\u4f18\u52bf\u6bd4\u7684\u6982\u5ff5\uff0c\u5e76\u8bc1\u660e\u4e86\u54ea\u4e9b\u7ec4\u5408\u95ee\u9898\u53ef\u4ee5\u4ece\u7ed3\u679c\u7ea7\u53cd\u9988\u4e2d\u5b66\u4e60\u3002", "motivation": "\u5c3d\u7ba1RLVR\u5728\u7ec4\u5408\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u7ecf\u9a8c\u6210\u529f\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u54ea\u4e9b\u7ec4\u5408\u95ee\u9898\u4ec5\u901a\u8fc7\u7ed3\u679c\u7ea7\u53cd\u9988\u5c31\u80fd\u5b66\u4e60\u3002\u9700\u8981\u7406\u8bba\u7406\u89e3RLVR\u4f55\u65f6\u6210\u529f\u3001\u4f55\u65f6\u5931\u8d25\u3002", "method": "\u7406\u8bba\u5206\u6790\u81ea\u56de\u5f52\u6a21\u578b\u5728RLVR\u8bad\u7ec3\u4e0b\u7684\u53ef\u5b66\u4e60\u6027\uff0c\u63d0\u51fa\u4efb\u52a1\u4f18\u52bf\u6bd4\u8fd9\u4e00\u5173\u952e\u6982\u5ff5\uff0c\u8be5\u6982\u5ff5\u662f\u7ec4\u5408\u95ee\u9898\u548c\u57fa\u7840\u6a21\u578b\u7684\u8054\u5408\u5c5e\u6027\u3002", "result": "\u5f53\u6b63\u786e\u4e2d\u95f4\u6b65\u9aa4\u63d0\u4f9b\u660e\u663e\u4f18\u52bf\u65f6\uff0c\u7ec4\u5408\u95ee\u9898\u53ef\u901a\u8fc7RLVR\u9ad8\u6548\u5b66\u4e60\uff1b\u5f53\u7ed3\u6784\u4f18\u52bf\u4e0d\u5b58\u5728\u65f6\uff0cRLVR\u53ef\u80fd\u6536\u655b\u5230\u6b21\u4f18\u7ec4\u5408\u3002\u57fa\u7840\u6a21\u578b\u7684\u8d28\u91cf\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u51b3\u5b9a\u4e86\u4f18\u52bf\u662f\u5426\u5b58\u5728\u3002", "conclusion": "\u4efb\u52a1\u4f18\u52bf\u6bd4\u8868\u5f81\u4e86\u54ea\u4e9b\u4efb\u52a1\u548c\u7ec4\u5408\u53ef\u4ee5\u4ece\u7ed3\u679c\u7ea7\u53cd\u9988\u4e2d\u5b66\u4e60\uff0c\u4e3aRLVR\u7684\u6210\u529f\u4e0e\u5931\u8d25\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u7406\u8bba\u7406\u89e3\u3002"}}
{"id": "2602.08000", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08000", "abs": "https://arxiv.org/abs/2602.08000", "authors": ["Anirudh Satheesh", "Vaneet Aggarwal"], "title": "Regret Analysis of Unichain Average Reward Constrained MDPs with General Parameterization", "comment": null, "summary": "We study infinite-horizon average-reward constrained Markov decision processes (CMDPs) under the unichain assumption and general policy parameterizations. Existing regret analyses for constrained reinforcement learning largely rely on ergodicity or strong mixing-time assumptions, which fail to hold in the presence of transient states. We propose a primal--dual natural actor--critic algorithm that leverages multi-level Monte Carlo (MLMC) estimators and an explicit burn-in mechanism to handle unichain dynamics without requiring mixing-time oracles. Our analysis establishes finite-time regret and cumulative constraint violation bounds that scale as $\\tilde{O}(\\sqrt{T})$, up to approximation errors arising from policy and critic parameterization, thereby extending order-optimal guarantees to a significantly broader class of CMDPs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5355\u94fe\u7ea6\u675fMDP\u7684\u539f\u59cb-\u5bf9\u5076\u81ea\u7136\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7b97\u6cd5\uff0c\u4f7f\u7528\u591a\u7ea7\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u548c\u663e\u5f0f\u9884\u70ed\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u221aT\u91cf\u7ea7\u7684\u9057\u61be\u548c\u7ea6\u675f\u8fdd\u53cd\u8fb9\u754c\uff0c\u65e0\u9700\u6df7\u5408\u65f6\u95f4\u9884\u8a00\u673a\u3002", "motivation": "\u73b0\u6709\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u7684\u9057\u61be\u5206\u6790\u4e3b\u8981\u4f9d\u8d56\u4e8e\u904d\u5386\u6027\u6216\u5f3a\u6df7\u5408\u65f6\u95f4\u5047\u8bbe\uff0c\u8fd9\u4e9b\u5047\u8bbe\u5728\u5b58\u5728\u77ac\u6001\u72b6\u6001\u65f6\u5931\u6548\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5904\u7406\u5355\u94fe\u52a8\u6001\u800c\u4e0d\u9700\u8981\u6df7\u5408\u65f6\u95f4\u9884\u8a00\u673a\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u539f\u59cb-\u5bf9\u5076\u81ea\u7136\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7b97\u6cd5\uff0c\u7ed3\u5408\u591a\u7ea7\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u5668\u548c\u663e\u5f0f\u9884\u70ed\u673a\u5236\u6765\u5904\u7406\u5355\u94fe\u52a8\u6001\uff0c\u65e0\u9700\u6df7\u5408\u65f6\u95f4\u5047\u8bbe\u3002", "result": "\u5efa\u7acb\u4e86\u6709\u9650\u65f6\u95f4\u9057\u61be\u548c\u7d2f\u79ef\u7ea6\u675f\u8fdd\u53cd\u8fb9\u754c\uff0c\u5c3a\u5ea6\u4e3a\u00d5(\u221aT)\uff0c\u53d7\u7b56\u7565\u548c\u8bc4\u8bba\u5bb6\u53c2\u6570\u5316\u8fd1\u4f3c\u8bef\u5dee\u5f71\u54cd\uff0c\u5c06\u6700\u4f18\u9636\u4fdd\u8bc1\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684CMDP\u7c7b\u522b\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u6210\u529f\u5730\u5c06\u6700\u4f18\u9636\u6027\u80fd\u4fdd\u8bc1\u6269\u5c55\u5230\u5355\u94fe\u7ea6\u675fMDP\uff0c\u65e0\u9700\u6df7\u5408\u65f6\u95f4\u5047\u8bbe\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u7684\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2602.08003", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.IT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08003", "abs": "https://arxiv.org/abs/2602.08003", "authors": ["Yigit Turkmen", "Baturalp Buyukates", "Melih Bastopcu"], "title": "Don't Always Pick the Highest-Performing Model: An Information Theoretic View of LLM Ensemble Selection", "comment": null, "summary": "Large language models (LLMs) are often ensembled together to improve overall reliability and robustness, but in practice models are strongly correlated. This raises a fundamental question: which models should be selected when forming an LLM ensemble? We formulate budgeted ensemble selection as maximizing the mutual information between the true label and predictions of the selected models. Furthermore, to explain why performance can saturate even with many models, we model the correlated errors of the models using Gaussian-copula and show an information-theoretic error floor for the performance of the ensemble. Motivated by these, we propose a simple greedy mutual-information selection algorithm that estimates the required information terms directly from data and iteratively builds an ensemble under a query budget. We test our approach in two question answering datasets and one binary sentiment classification dataset: MEDMCQA, MMLU, and IMDB movie reviews. Across all datasets, we observe that our method consistently outperforms strong baselines under the same query budget.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u8d2a\u5fc3\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u6709\u9650\u67e5\u8be2\u9884\u7b97\u4e0b\u9009\u62e9\u6700\u4f18\u7684LLM\u96c6\u6210\u7ec4\u5408\uff0c\u89e3\u51b3\u6a21\u578b\u5f3a\u76f8\u5173\u65f6\u96c6\u6210\u6027\u80fd\u9971\u548c\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u65f6\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u5373\u4f7f\u589e\u52a0\u6a21\u578b\u6570\u91cf\u6027\u80fd\u4e5f\u4f1a\u9971\u548c\u3002\u9700\u8981\u89e3\u51b3\u5728\u6709\u9650\u67e5\u8be2\u9884\u7b97\u4e0b\u5982\u4f55\u9009\u62e9\u6700\u4f18\u6a21\u578b\u7ec4\u5408\u7684\u95ee\u9898\u3002", "method": "1) \u5c06\u96c6\u6210\u9009\u62e9\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u6700\u5927\u5316\u771f\u5b9e\u6807\u7b7e\u4e0e\u6240\u9009\u6a21\u578b\u9884\u6d4b\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\uff1b2) \u4f7f\u7528\u9ad8\u65afcopula\u5efa\u6a21\u6a21\u578b\u95f4\u7684\u76f8\u5173\u8bef\u5dee\uff0c\u5206\u6790\u4fe1\u606f\u8bba\u8bef\u5dee\u4e0b\u754c\uff1b3) \u63d0\u51fa\u8d2a\u5fc3\u4e92\u4fe1\u606f\u9009\u62e9\u7b97\u6cd5\uff0c\u76f4\u63a5\u4ece\u6570\u636e\u4f30\u8ba1\u4fe1\u606f\u91cf\uff0c\u5728\u67e5\u8be2\u9884\u7b97\u4e0b\u8fed\u4ee3\u6784\u5efa\u96c6\u6210\u3002", "result": "\u5728MEDMCQA\u3001MMLU\u95ee\u7b54\u6570\u636e\u96c6\u548cIMDB\u60c5\u611f\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u8be5\u65b9\u6cd5\u5728\u76f8\u540c\u67e5\u8be2\u9884\u7b97\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u8d2a\u5fc3\u9009\u62e9\u7b97\u6cd5\u80fd\u6709\u6548\u89e3\u51b3LLM\u96c6\u6210\u4e2d\u7684\u6a21\u578b\u9009\u62e9\u95ee\u9898\uff0c\u5728\u6709\u9650\u9884\u7b97\u4e0b\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\uff0c\u4e3a\u5904\u7406\u6a21\u578b\u76f8\u5173\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u548c\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08007", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08007", "abs": "https://arxiv.org/abs/2602.08007", "authors": ["Sizhe Dang", "Jiaqi Shao", "Xiaodong Zheng", "Guang Dai", "Yan Song", "Haishan Ye"], "title": "From $O(mn)$ to $O(r^2)$: Two-Sided Low-Rank Communication for Adam in Distributed Training with Memory Efficiency", "comment": null, "summary": "As foundation models continue to scale, pretraining increasingly relies on data-parallel distributed optimization, making bandwidth-limited gradient synchronization a key bottleneck. Orthogonally, projection-based low-rank optimizers were mainly designed for memory efficiency, but remain suboptimal for communication-limited training: one-sided synchronization still transmits an $O(rn)$ object for an $m\\times n$ matrix gradient and refresh steps can dominate peak communicated bytes. We propose TSR, which brings two-sided low-rank communication to Adam-family updates (TSR-Adam) by synchronizing a compact core $U^\\top G V\\in\\mathbb{R}^{r\\times r}$, reducing the dominant per-step payload from $O(mn)$ to $O(r^2)$ while keeping moment states in low-dimensional cores. To further reduce the peak communication from subspace refresh, TSR-Adam adopts a randomized SVD-based refresh that avoids full-gradient synchronization. We additionally extend low-rank communication to embedding gradients with embedding-specific ranks and refresh schedules, yielding additional communication and memory savings over keeping embeddings dense. Across pretraining from 60M to 1B model scales, TSR-Adam reduces average communicated bytes per step by $13\\times$, and on GLUE fine-tuning it reduces communication by $25\\times$, while achieving comparable performance; we further provide a theoretical stationarity analysis for the proposed update. Code is available at https://github.com/DKmiyan/TSR-Adam.", "AI": {"tldr": "TSR-Adam\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u9762\u4f4e\u79e9\u901a\u4fe1\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u540c\u6b65\u7d27\u51d1\u7684\u6838\u5fc3\u77e9\u9635\u5c06Adam\u4f18\u5316\u5668\u7684\u901a\u4fe1\u8d1f\u8f7d\u4eceO(mn)\u964d\u4f4e\u5230O(r\u00b2)\uff0c\u663e\u8457\u51cf\u5c11\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u968f\u7740\u57fa\u7840\u6a21\u578b\u89c4\u6a21\u6269\u5927\uff0c\u6570\u636e\u5e76\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u5e26\u5bbd\u53d7\u9650\u7684\u68af\u5ea6\u540c\u6b65\u6210\u4e3a\u5173\u952e\u74f6\u9888\u3002\u73b0\u6709\u7684\u6295\u5f71\u4f4e\u79e9\u4f18\u5316\u5668\u4e3b\u8981\u5173\u6ce8\u5185\u5b58\u6548\u7387\uff0c\u4f46\u5728\u901a\u4fe1\u53d7\u9650\u7684\u8bad\u7ec3\u4e2d\u4ecd\u4e0d\u7406\u60f3\uff1a\u5355\u8fb9\u540c\u6b65\u4ecd\u9700\u4f20\u8f93O(rn)\u5bf9\u8c61\uff0c\u5237\u65b0\u6b65\u9aa4\u53ef\u80fd\u4e3b\u5bfc\u5cf0\u503c\u901a\u4fe1\u91cf\u3002", "method": "\u63d0\u51faTSR-Adam\uff0c\u4e3aAdam\u7cfb\u5217\u66f4\u65b0\u5f15\u5165\u53cc\u9762\u4f4e\u79e9\u901a\u4fe1\uff1a1) \u540c\u6b65\u7d27\u51d1\u6838\u5fc3\u77e9\u9635U\u22a4GV\u2208\u211d^{r\u00d7r}\uff0c\u5c06\u4e3b\u8981\u6bcf\u6b65\u8d1f\u8f7d\u4eceO(mn)\u964d\u81f3O(r\u00b2)\uff1b2) \u91c7\u7528\u57fa\u4e8e\u968f\u673aSVD\u7684\u5237\u65b0\u907f\u514d\u5168\u68af\u5ea6\u540c\u6b65\uff1b3) \u5c06\u4f4e\u79e9\u901a\u4fe1\u6269\u5c55\u5230\u5d4c\u5165\u68af\u5ea6\uff0c\u4f7f\u7528\u5d4c\u5165\u7279\u5b9a\u79e9\u548c\u5237\u65b0\u8ba1\u5212\u3002", "result": "\u5728\u4ece6000\u4e07\u523010\u4ebf\u53c2\u6570\u7684\u9884\u8bad\u7ec3\u4e2d\uff0cTSR-Adam\u5c06\u5e73\u5747\u6bcf\u6b65\u901a\u4fe1\u5b57\u8282\u51cf\u5c1113\u500d\uff1b\u5728GLUE\u5fae\u8c03\u4e2d\u901a\u4fe1\u51cf\u5c1125\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u6027\u80fd\u3002\u8bba\u6587\u8fd8\u63d0\u4f9b\u4e86\u6240\u63d0\u66f4\u65b0\u7684\u7406\u8bba\u5e73\u7a33\u6027\u5206\u6790\u3002", "conclusion": "TSR-Adam\u901a\u8fc7\u53cc\u9762\u4f4e\u79e9\u901a\u4fe1\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6848\u3002"}}
{"id": "2602.08012", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08012", "abs": "https://arxiv.org/abs/2602.08012", "authors": ["Riccardo De Santi", "Malte Franke", "Ya-Ping Hsieh", "Andreas Krause"], "title": "A Unified Density Operator View of Flow Control and Merging", "comment": null, "summary": "Recent progress in large-scale flow and diffusion models raised two fundamental algorithmic challenges: (i) control-based reward adaptation of pre-trained flows, and (ii) integration of multiple models, i.e., flow merging. While current approaches address them separately, we introduce a unifying probability-space framework that subsumes both as limit cases, and enables reward-guided flow merging, allowing principled, task-aware combination of multiple pre-trained flows (e.g., merging priors while maximizing drug-discovery utilities). Our formulation renders possible to express a rich family of operators over generative models densities, including intersection (e.g., to enforce safety), union (e.g., to compose diverse models), interpolation (e.g., for discovery), their reward-guided counterparts, as well as complex logical expressions via generative circuits. Next, we introduce Reward-Guided Flow Merging (RFM), a mirror-descent scheme that reduces reward-guided flow merging to a sequence of standard fine-tuning problems. Then, we provide first-of-their-kind theoretical guarantees for reward-guided and pure flow merging via RFM. Ultimately, we showcase the capabilities of the proposed method on illustrative settings providing visually interpretable insights, and apply our method to high-dimensional de-novo molecular design and low-energy conformer generation.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u6982\u7387\u7a7a\u95f4\u6846\u67b6\uff0c\u5c06\u6d41\u6a21\u578b\u5956\u52b1\u9002\u5e94\u4e0e\u6a21\u578b\u878d\u5408\u7edf\u4e00\u5904\u7406\uff0c\u652f\u6301\u5956\u52b1\u5f15\u5bfc\u7684\u6d41\u6a21\u578b\u878d\u5408\u53ca\u590d\u6742\u903b\u8f91\u64cd\u4f5c", "motivation": "\u5927\u89c4\u6a21\u6d41\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u7684\u53d1\u5c55\u5e26\u6765\u4e86\u4e24\u4e2a\u57fa\u672c\u6311\u6218\uff1a\u9884\u8bad\u7ec3\u6d41\u7684\u57fa\u4e8e\u63a7\u5236\u7684\u5956\u52b1\u9002\u5e94\uff0c\u4ee5\u53ca\u591a\u4e2a\u6a21\u578b\u7684\u96c6\u6210\uff08\u6d41\u878d\u5408\uff09\u3002\u73b0\u6709\u65b9\u6cd5\u5206\u522b\u5904\u7406\u8fd9\u4e24\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u7edf\u4e00\u7684\u6846\u67b6\u6765\u540c\u65f6\u89e3\u51b3\u5956\u52b1\u9002\u5e94\u548c\u6a21\u578b\u878d\u5408", "method": "\u63d0\u51fa\u7edf\u4e00\u6982\u7387\u7a7a\u95f4\u6846\u67b6\uff0c\u5c06\u5956\u52b1\u9002\u5e94\u548c\u6d41\u878d\u5408\u4f5c\u4e3a\u6781\u9650\u60c5\u51b5\u7edf\u4e00\u5904\u7406\uff1b\u5f15\u5165\u5956\u52b1\u5f15\u5bfc\u6d41\u878d\u5408\uff08RFM\uff09\u65b9\u6cd5\uff0c\u4f7f\u7528\u955c\u50cf\u4e0b\u964d\u65b9\u6848\u5c06\u5956\u52b1\u5f15\u5bfc\u6d41\u878d\u5408\u8f6c\u5316\u4e3a\u4e00\u7cfb\u5217\u6807\u51c6\u5fae\u8c03\u95ee\u9898", "result": "\u4e3a\u5956\u52b1\u5f15\u5bfc\u548c\u7eaf\u6d41\u878d\u5408\u63d0\u4f9b\u4e86\u9996\u4e2a\u7406\u8bba\u4fdd\u8bc1\uff1b\u5728\u5206\u5b50\u8bbe\u8ba1\u548c\u4f4e\u80fd\u6784\u8c61\u751f\u6210\u7b49\u9ad8\u7ef4\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u80fd\u591f\u8868\u8fbe\u4e30\u5bcc\u7684\u751f\u6210\u6a21\u578b\u5bc6\u5ea6\u64cd\u4f5c\uff0c\u5305\u62ec\u4ea4\u96c6\u3001\u5e76\u96c6\u3001\u63d2\u503c\u53ca\u5176\u5956\u52b1\u5f15\u5bfc\u7248\u672c\uff0c\u901a\u8fc7\u751f\u6210\u7535\u8def\u652f\u6301\u590d\u6742\u903b\u8f91\u8868\u8fbe\u5f0f\uff0c\u4e3a\u6d41\u6a21\u578b\u878d\u5408\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u65b9\u6cd5"}}
{"id": "2602.08019", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08019", "abs": "https://arxiv.org/abs/2602.08019", "authors": ["Dong Pan", "Bingtao Li", "Yongsheng Zheng", "Jiren Ma", "Victor Fei"], "title": "The Rise of Sparse Mixture-of-Experts: A Survey from Algorithmic Foundations to Decentralized Architectures and Vertical Domain Applications", "comment": null, "summary": "The sparse Mixture of Experts(MoE) architecture has evolved as a powerful approach for scaling deep learning models to more parameters with comparable computation cost. As an important branch of large language model(LLM), MoE model only activate a subset of experts based on a routing network. This sparse conditional computation mechanism significantly improves computational efficiency, paving a promising path for greater scalability and cost-efficiency. It not only enhance downstream applications such as natural language processing, computer vision, and multimodal in various horizontal domains, but also exhibit broad applicability across vertical domains. Despite the growing popularity and application of MoE models across various domains, there lacks a systematic exploration of recent advancements of MoE in many important fields. Existing surveys on MoE suffer from limitations such as lack coverage or none extensively exploration of key areas. This survey seeks to fill these gaps. In this paper, Firstly, we examine the foundational principles of MoE, with an in-depth exploration of its core components-the routing network and expert network. Subsequently, we extend beyond the centralized paradigm to the decentralized paradigm, which unlocks the immense untapped potential of decentralized infrastructure, enables democratization of MoE development for broader communities, and delivers greater scalability and cost-efficiency. Furthermore we focus on exploring its vertical domain applications. Finally, we also identify key challenges and promising future research directions. To the best of our knowledge, this survey is currently the most comprehensive review in the field of MoE. We aim for this article to serve as a valuable resource for both researchers and practitioners, enabling them to navigate and stay up-to-date with the latest advancements.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u662f\u5173\u4e8e\u7a00\u758f\u6df7\u5408\u4e13\u5bb6(MoE)\u6a21\u578b\u7684\u7efc\u8ff0\uff0c\u7cfb\u7edf\u6027\u5730\u56de\u987e\u4e86MoE\u7684\u57fa\u7840\u539f\u7406\u3001\u6838\u5fc3\u7ec4\u4ef6\u3001\u53bb\u4e2d\u5fc3\u5316\u8303\u5f0f\u3001\u5782\u76f4\u9886\u57df\u5e94\u7528\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "MoE\u4f5c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u91cd\u8981\u5206\u652f\uff0c\u901a\u8fc7\u7a00\u758f\u6761\u4ef6\u8ba1\u7b97\u673a\u5236\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u5728\u6c34\u5e73\u548c\u5782\u76f4\u9886\u57df\u90fd\u6709\u5e7f\u6cdb\u5e94\u7528\u3002\u7136\u800c\uff0c\u73b0\u6709MoE\u7efc\u8ff0\u5b58\u5728\u8986\u76d6\u4e0d\u8db3\u6216\u5173\u952e\u9886\u57df\u63a2\u7d22\u4e0d\u591f\u6df1\u5165\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u7bc7\u66f4\u5168\u9762\u7684\u7efc\u8ff0\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u9996\u5148\u5206\u6790MoE\u7684\u57fa\u7840\u539f\u7406\uff0c\u6df1\u5165\u63a2\u8ba8\u5176\u6838\u5fc3\u7ec4\u4ef6\uff08\u8def\u7531\u7f51\u7edc\u548c\u4e13\u5bb6\u7f51\u7edc\uff09\uff1b\u7136\u540e\u6269\u5c55\u5230\u53bb\u4e2d\u5fc3\u5316\u8303\u5f0f\uff1b\u63a5\u7740\u91cd\u70b9\u63a2\u7d22\u5782\u76f4\u9886\u57df\u5e94\u7528\uff1b\u6700\u540e\u8bc6\u522b\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "result": "\u8fd9\u7bc7\u7efc\u8ff0\u662f\u76ee\u524dMoE\u9886\u57df\u6700\u5168\u9762\u7684\u56de\u987e\uff0c\u65e8\u5728\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u8d44\u6e90\uff0c\u5e2e\u52a9\u4ed6\u4eec\u4e86\u89e3\u6700\u65b0\u8fdb\u5c55\u3002", "conclusion": "MoE\u67b6\u6784\u901a\u8fc7\u7a00\u758f\u6761\u4ef6\u8ba1\u7b97\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6269\u5c55\u80fd\u529b\uff0c\u53bb\u4e2d\u5fc3\u5316\u8303\u5f0f\u8fdb\u4e00\u6b65\u91ca\u653e\u4e86\u5176\u6f5c\u529b\u3002\u8be5\u7efc\u8ff0\u7cfb\u7edf\u6027\u5730\u603b\u7ed3\u4e86MoE\u7684\u53d1\u5c55\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2602.08026", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08026", "abs": "https://arxiv.org/abs/2602.08026", "authors": ["Arya Akhavan", "David Janz", "Csaba Szepesv\u00e1ri"], "title": "Sharp analysis of linear ensemble sampling", "comment": null, "summary": "We analyse linear ensemble sampling (ES) with standard Gaussian perturbations in stochastic linear bandits. We show that for ensemble size $m=\u0398(d\\log n)$, ES attains $\\tilde O(d^{3/2}\\sqrt n)$ high-probability regret, closing the gap to the Thompson sampling benchmark while keeping computation comparable. The proof brings a new perspective on randomized exploration in linear bandits by reducing the analysis to a time-uniform exceedance problem for $m$ independent Brownian motions. Intriguingly, this continuous-time lens is not forced; it appears natural--and perhaps necessary: the discrete-time problem seems to be asking for a continuous-time solution, and we know of no other way to obtain a sharp ES bound.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u7ebf\u6027\u96c6\u6210\u91c7\u6837(ES)\u5728\u9ad8\u65af\u6270\u52a8\u4e0b\u7684\u968f\u673a\u7ebf\u6027bandit\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5f53\u96c6\u6210\u89c4\u6a21m=\u0398(d log n)\u65f6\uff0cES\u80fd\u8fbe\u5230$\\tilde O(d^{3/2}\\sqrt n)$\u7684\u9ad8\u6982\u7387\u9057\u61be\u754c\uff0c\u586b\u8865\u4e86\u4e0eThompson\u91c7\u6837\u57fa\u51c6\u7684\u5dee\u8ddd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u6bd4\u8f83\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u7814\u7a76\u7ebf\u6027\u96c6\u6210\u91c7\u6837\u5728\u968f\u673a\u7ebf\u6027bandit\u4e2d\u7684\u6027\u80fd\uff0c\u65e8\u5728\u586b\u8865\u5176\u4e0eThompson\u91c7\u6837\u57fa\u51c6\u4e4b\u95f4\u7684\u7406\u8bba\u5dee\u8ddd\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002\u5f53\u524dES\u7684\u5206\u6790\u5b58\u5728\u7406\u8bba\u7f3a\u53e3\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u754c\u9650\u3002", "method": "\u91c7\u7528\u8fde\u7eed\u65f6\u95f4\u89c6\u89d2\uff0c\u5c06\u79bb\u6563\u65f6\u95f4\u95ee\u9898\u8f6c\u5316\u4e3am\u4e2a\u72ec\u7acb\u5e03\u6717\u8fd0\u52a8\u7684\u65f6\u5747\u5300\u8d85\u8d8a\u95ee\u9898\u3002\u901a\u8fc7\u5206\u6790\u96c6\u6210\u91c7\u6837\u5728\u9ad8\u65af\u6270\u52a8\u4e0b\u7684\u884c\u4e3a\uff0c\u4f7f\u7528\u5e03\u6717\u8fd0\u52a8\u7406\u8bba\u6765\u63a8\u5bfc\u9057\u61be\u754c\u9650\u3002", "result": "\u5f53\u96c6\u6210\u89c4\u6a21m=\u0398(d log n)\u65f6\uff0cES\u5b9e\u73b0\u4e86$\\tilde O(d^{3/2}\\sqrt n)$\u7684\u9ad8\u6982\u7387\u9057\u61be\u754c\uff0c\u8fd9\u4e0eThompson\u91c7\u6837\u57fa\u51c6\u76f8\u5339\u914d\uff0c\u540c\u65f6\u8ba1\u7b97\u590d\u6742\u5ea6\u4fdd\u6301\u53ef\u6bd4\u3002", "conclusion": "\u7ebf\u6027\u96c6\u6210\u91c7\u6837\u5728\u9002\u5f53\u7684\u96c6\u6210\u89c4\u6a21\u4e0b\u53ef\u4ee5\u8fbe\u5230Thompson\u91c7\u6837\u7684\u7406\u8bba\u6027\u80fd\uff0c\u8fde\u7eed\u65f6\u95f4\u89c6\u89d2\u4e3a\u5206\u6790\u968f\u673a\u63a2\u7d22\u63d0\u4f9b\u4e86\u81ea\u7136\u4e14\u5fc5\u8981\u7684\u65b9\u6cd5\uff0c\u79bb\u6563\u65f6\u95f4\u95ee\u9898\u4f3c\u4e4e\u9700\u8981\u8fde\u7eed\u65f6\u95f4\u89e3\u51b3\u65b9\u6848\u624d\u80fd\u83b7\u5f97\u5c16\u9510\u7684\u754c\u9650\u3002"}}
{"id": "2602.08032", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08032", "abs": "https://arxiv.org/abs/2602.08032", "authors": ["Lior Cohen", "Ofir Nabati", "Kaixin Wang", "Navdeep Kumar", "Shie Mannor"], "title": "Horizon Imagination: Efficient On-Policy Training in Diffusion World Models", "comment": "This paper will be published in the ICLR 2026 proceedings", "summary": "We study diffusion-based world models for reinforcement learning, which offer high generative fidelity but face critical efficiency challenges in control. Current methods either require heavyweight models at inference or rely on highly sequential imagination, both of which impose prohibitive computational costs. We propose Horizon Imagination (HI), an on-policy imagination process for discrete stochastic policies that denoises multiple future observations in parallel. HI incorporates a stabilization mechanism and a novel sampling schedule that decouples the denoising budget from the effective horizon over which denoising is applied while also supporting sub-frame budgets. Experiments on Atari 100K and Craftium show that our approach maintains control performance with a sub-frame budget of half the denoising steps and achieves superior generation quality under varied schedules. Code is available at https://github.com/leor-c/horizon-imagination.", "AI": {"tldr": "\u63d0\u51faHorizon Imagination (HI)\u65b9\u6cd5\uff0c\u7528\u4e8e\u6269\u6563\u4e16\u754c\u6a21\u578b\u7684\u5e76\u884c\u672a\u6765\u89c2\u6d4b\u53bb\u566a\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u540c\u65f6\u4fdd\u6301\u63a7\u5236\u6027\u80fd", "motivation": "\u57fa\u4e8e\u6269\u6563\u7684\u4e16\u754c\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5177\u6709\u9ad8\u751f\u6210\u4fdd\u771f\u5ea6\uff0c\u4f46\u9762\u4e34\u4e25\u91cd\u7684\u6548\u7387\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u91cd\u578b\u63a8\u7406\u6a21\u578b\uff0c\u8981\u4e48\u4f9d\u8d56\u9ad8\u5ea6\u987a\u5e8f\u5316\u7684\u60f3\u8c61\u8fc7\u7a0b\uff0c\u90fd\u5e26\u6765\u8fc7\u9ad8\u7684\u8ba1\u7b97\u6210\u672c", "method": "\u63d0\u51faHorizon Imagination (HI)\u65b9\u6cd5\uff1a1) \u5728\u7b56\u7565\u60f3\u8c61\u8fc7\u7a0b\uff0c\u652f\u6301\u79bb\u6563\u968f\u673a\u7b56\u7565\uff1b2) \u5e76\u884c\u53bb\u566a\u591a\u4e2a\u672a\u6765\u89c2\u6d4b\uff1b3) \u5305\u542b\u7a33\u5b9a\u673a\u5236\uff1b4) \u65b0\u9896\u7684\u91c7\u6837\u8c03\u5ea6\uff0c\u5c06\u53bb\u566a\u9884\u7b97\u4e0e\u6709\u6548\u89c6\u91ce\u89e3\u8026\uff0c\u652f\u6301\u5b50\u5e27\u9884\u7b97", "result": "\u5728Atari 100K\u548cCraftium\u5b9e\u9a8c\u4e2d\uff0cHI\u65b9\u6cd5\u4ec5\u7528\u4e00\u534a\u53bb\u566a\u6b65\u6570\u7684\u5b50\u5e27\u9884\u7b97\u5c31\u80fd\u4fdd\u6301\u63a7\u5236\u6027\u80fd\uff0c\u5728\u4e0d\u540c\u8c03\u5ea6\u4e0b\u5b9e\u73b0\u66f4\u4f18\u7684\u751f\u6210\u8d28\u91cf", "conclusion": "Horizon Imagination\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u4e16\u754c\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u5e76\u884c\u53bb\u566a\u548c\u7075\u6d3b\u7684\u8c03\u5ea6\u673a\u5236\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c"}}
{"id": "2602.08033", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08033", "abs": "https://arxiv.org/abs/2602.08033", "authors": ["Julien Fageot", "Matthias Grossglauser", "L\u00ea-Nguy\u00ean Hoang", "Matteo Tacchi-B\u00e9nard", "Oscar Villemaud"], "title": "The Benefits of Diversity: Combining Comparisons and Ratings for Efficient Scoring", "comment": "1 table, 5 figures, 8 pages", "summary": "Should humans be asked to evaluate entities individually or comparatively? This question has been the subject of long debates. In this work, we show that, interestingly, combining both forms of preference elicitation can outperform the focus on a single kind. More specifically, we introduce SCoRa (Scoring from Comparisons and Ratings), a unified probabilistic model that allows to learn from both signals. We prove that the MAP estimator of SCoRa is well-behaved. It verifies monotonicity and robustness guarantees. We then empirically show that SCoRa recovers accurate scores, even under model mismatch. Most interestingly, we identify a realistic setting where combining comparisons and ratings outperforms using either one alone, and when the accurate ordering of top entities is critical. Given the de facto availability of signals of multiple forms, SCoRa additionally offers a versatile foundation for preference learning.", "AI": {"tldr": "SCoRa\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u6bd4\u8f83\u548c\u8bc4\u5206\u4e24\u79cd\u504f\u597d\u8868\u8fbe\u65b9\u5f0f\uff0c\u5728\u5b9e\u4f53\u6392\u5e8f\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5355\u4e00\u65b9\u6cd5", "motivation": "\u89e3\u51b3\u957f\u671f\u5b58\u5728\u7684\u4e89\u8bba\uff1a\u4eba\u7c7b\u5e94\u8be5\u5355\u72ec\u8bc4\u4f30\u5b9e\u4f53\u8fd8\u662f\u8fdb\u884c\u6bd4\u8f83\u8bc4\u4f30\uff1f\u63a2\u7d22\u7ed3\u5408\u4e24\u79cd\u504f\u597d\u8868\u8fbe\u65b9\u5f0f\u662f\u5426\u80fd\u8d85\u8d8a\u5355\u4e00\u65b9\u6cd5", "method": "\u63d0\u51faSCoRa\uff08\u4ece\u6bd4\u8f83\u548c\u8bc4\u5206\u4e2d\u8bc4\u5206\uff09\u7edf\u4e00\u6982\u7387\u6a21\u578b\uff0c\u80fd\u591f\u540c\u65f6\u5b66\u4e60\u6bd4\u8f83\u548c\u8bc4\u5206\u4fe1\u53f7\uff0c\u5e76\u8bc1\u660e\u5176MAP\u4f30\u8ba1\u5668\u5177\u6709\u826f\u597d\u7684\u5355\u8c03\u6027\u548c\u9c81\u68d2\u6027\u4fdd\u8bc1", "result": "\u5b9e\u8bc1\u8868\u660eSCoRa\u80fd\u591f\u51c6\u786e\u6062\u590d\u5206\u6570\uff0c\u5373\u4f7f\u5728\u6a21\u578b\u4e0d\u5339\u914d\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5de5\u4f5c\u3002\u5728\u9700\u8981\u51c6\u786e\u6392\u5e8f\u9876\u90e8\u5b9e\u4f53\u7684\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u7ed3\u5408\u6bd4\u8f83\u548c\u8bc4\u5206\u7684\u65b9\u6cd5\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528\u4efb\u4e00\u79cd\u65b9\u6cd5", "conclusion": "\u7ed3\u5408\u6bd4\u8f83\u548c\u8bc4\u5206\u4e24\u79cd\u504f\u597d\u8868\u8fbe\u65b9\u5f0f\u80fd\u591f\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u5b9e\u4f53\u6392\u5e8f\uff0cSCoRa\u4e3a\u504f\u597d\u5b66\u4e60\u63d0\u4f9b\u4e86\u591a\u529f\u80fd\u7684\u57fa\u7840\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u7cbe\u786e\u9876\u90e8\u6392\u5e8f\u7684\u5173\u952e\u573a\u666f"}}
{"id": "2602.08036", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08036", "abs": "https://arxiv.org/abs/2602.08036", "authors": ["Jingtao Liu", "Xinming Zhang"], "title": "TAAM:Inductive Graph-Class Incremental Learning with Task-Aware Adaptive Modulation", "comment": null, "summary": "Graph Continual Learning (GCL) aims to solve the challenges of streaming graph data. However, current methods often depend on replay-based strategies, which raise concerns like memory limits and privacy issues, while also struggling to resolve the stability-plasticity dilemma. In this paper, we suggest that lightweight, task-specific modules can effectively guide the reasoning process of a fixed GNN backbone. Based on this idea, we propose Task-Aware Adaptive Modulation (TAAM). The key component of TAAM is its lightweight Neural Synapse Modulators (NSMs). For each new task, a dedicated NSM is trained and then frozen, acting as an \"expert module.\" These modules perform detailed, node-attentive adaptive modulation on the computational flow of a shared GNN backbone. This setup ensures that new knowledge is kept within compact, task-specific modules, naturally preventing catastrophic forgetting without using any data replay. Additionally, to address the important challenge of unknown task IDs in real-world scenarios, we propose and theoretically prove a novel method named Anchored Multi-hop Propagation (AMP). Notably, we find that existing GCL benchmarks have flaws that can cause data leakage and biased evaluations. Therefore, we conduct all experiments in a more rigorous inductive learning scenario. Extensive experiments show that TAAM comprehensively outperforms state-of-the-art methods across eight datasets. Code and Datasets are available at: https://github.com/1iuJT/TAAM_AAMAS2026.", "AI": {"tldr": "TAAM\u63d0\u51fa\u57fa\u4e8e\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7a81\u89e6\u8c03\u5236\u5668\uff08NSMs\uff09\u7684\u56fe\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e0\u9700\u6570\u636e\u56de\u653e\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u6a21\u5757\u6307\u5bfc\u56fa\u5b9aGNN\u9aa8\u5e72\u7f51\u7edc\uff0c\u5e76\u5f15\u5165\u951a\u70b9\u591a\u8df3\u4f20\u64ad\uff08AMP\uff09\u89e3\u51b3\u672a\u77e5\u4efb\u52a1ID\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u56fe\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u6570\u636e\u56de\u653e\u7b56\u7565\uff0c\u5b58\u5728\u5185\u5b58\u9650\u5236\u548c\u9690\u79c1\u95ee\u9898\uff0c\u4e14\u96be\u4ee5\u89e3\u51b3\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u56de\u653e\u3001\u80fd\u6709\u6548\u5904\u7406\u672a\u77e5\u4efb\u52a1ID\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4efb\u52a1\u611f\u77e5\u81ea\u9002\u5e94\u8c03\u5236\uff08TAAM\uff09\uff1a1\uff09\u4e3a\u6bcf\u4e2a\u65b0\u4efb\u52a1\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7a81\u89e6\u8c03\u5236\u5668\uff08NSMs\uff09\uff0c\u4f5c\u4e3a\"\u4e13\u5bb6\u6a21\u5757\"\u5bf9\u5171\u4eabGNN\u9aa8\u5e72\u7f51\u7edc\u8fdb\u884c\u8282\u70b9\u6ce8\u610f\u529b\u81ea\u9002\u5e94\u8c03\u5236\uff1b2\uff09\u5f15\u5165\u951a\u70b9\u591a\u8df3\u4f20\u64ad\uff08AMP\uff09\u5904\u7406\u672a\u77e5\u4efb\u52a1ID\u573a\u666f\u3002", "result": "\u5728\u66f4\u4e25\u683c\u7684\u5f52\u7eb3\u5b66\u4e60\u573a\u666f\u4e0b\uff0cTAAM\u5728\u516b\u4e2a\u6570\u636e\u96c6\u4e0a\u5168\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u65e0\u9700\u6570\u636e\u56de\u653e\u5373\u53ef\u6709\u6548\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u4efb\u52a1\u7279\u5b9a\u6a21\u5757\u80fd\u6709\u6548\u6307\u5bfc\u56fa\u5b9aGNN\u9aa8\u5e72\u7f51\u7edc\uff0cTAAM\u901a\u8fc7NSMs\u548cAMP\u89e3\u51b3\u4e86\u56fe\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u56de\u653e\u4f9d\u8d56\u548c\u672a\u77e5\u4efb\u52a1ID\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.08040", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08040", "abs": "https://arxiv.org/abs/2602.08040", "authors": ["Isaac Han", "Sangyeon Park", "Seungwon Oh", "Donghu Kim", "Hojoon Lee", "Kyung-Joong Kim"], "title": "FIRE: Frobenius-Isometry Reinitialization for Balancing the Stability-Plasticity Tradeoff", "comment": "ICLR'26 (oral)", "summary": "Deep neural networks trained on nonstationary data must balance stability (i.e., retaining prior knowledge) and plasticity (i.e., adapting to new tasks). Standard reinitialization methods, which reinitialize weights toward their original values, are widely used but difficult to tune: conservative reinitializations fail to restore plasticity, while aggressive ones erase useful knowledge. We propose FIRE, a principled reinitialization method that explicitly balances the stability-plasticity tradeoff. FIRE quantifies stability through Squared Frobenius Error (SFE), measuring proximity to past weights, and plasticity through Deviation from Isometry (DfI), reflecting weight isotropy. The reinitialization point is obtained by solving a constrained optimization problem, minimizing SFE subject to DfI being zero, which is efficiently approximated by Newton-Schulz iteration. FIRE is evaluated on continual visual learning (CIFAR-10 with ResNet-18), language modeling (OpenWebText with GPT-0.1B), and reinforcement learning (HumanoidBench with SAC and Atari games with DQN). Across all domains, FIRE consistently outperforms both naive training without intervention and standard reinitialization methods, demonstrating effective balancing of the stability-plasticity tradeoff.", "AI": {"tldr": "FIRE\u662f\u4e00\u79cd\u5e73\u8861\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\u6743\u8861\u7684\u6743\u91cd\u91cd\u65b0\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316SFE\u548cDfI\u6307\u6807\uff0c\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u4f18\u4e8e\u6807\u51c6\u65b9\u6cd5\u3002", "motivation": "\u5728\u975e\u5e73\u7a33\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u9700\u8981\u5e73\u8861\u7a33\u5b9a\u6027\uff08\u4fdd\u7559\u5148\u9a8c\u77e5\u8bc6\uff09\u548c\u53ef\u5851\u6027\uff08\u9002\u5e94\u65b0\u4efb\u52a1\uff09\u3002\u6807\u51c6\u91cd\u65b0\u521d\u59cb\u5316\u65b9\u6cd5\u96be\u4ee5\u8c03\u4f18\uff1a\u4fdd\u5b88\u7684\u65e0\u6cd5\u6062\u590d\u53ef\u5851\u6027\uff0c\u6fc0\u8fdb\u7684\u4f1a\u64e6\u9664\u6709\u7528\u77e5\u8bc6\u3002", "method": "\u63d0\u51faFIRE\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u65b9Frobenius\u8bef\u5dee\uff08SFE\uff09\u91cf\u5316\u7a33\u5b9a\u6027\uff08\u63a5\u8fd1\u8fc7\u53bb\u6743\u91cd\uff09\uff0c\u901a\u8fc7\u504f\u79bb\u7b49\u8ddd\u6027\uff08DfI\uff09\u91cf\u5316\u53ef\u5851\u6027\uff08\u6743\u91cd\u5404\u5411\u540c\u6027\uff09\u3002\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u6c42\u89e3\u91cd\u65b0\u521d\u59cb\u5316\u70b9\uff1a\u6700\u5c0f\u5316SFE\uff0c\u7ea6\u675fDfI\u4e3a\u96f6\uff0c\u4f7f\u7528Newton-Schulz\u8fed\u4ee3\u9ad8\u6548\u8fd1\u4f3c\u3002", "result": "\u5728\u6301\u7eed\u89c6\u89c9\u5b66\u4e60\uff08CIFAR-10 + ResNet-18\uff09\u3001\u8bed\u8a00\u5efa\u6a21\uff08OpenWebText + GPT-0.1B\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08HumanoidBench + SAC\uff0cAtari + DQN\uff09\u4e09\u4e2a\u9886\u57df\u8bc4\u4f30\uff0cFIRE\u59cb\u7ec8\u4f18\u4e8e\u65e0\u5e72\u9884\u7684\u6734\u7d20\u8bad\u7ec3\u548c\u6807\u51c6\u91cd\u65b0\u521d\u59cb\u5316\u65b9\u6cd5\u3002", "conclusion": "FIRE\u80fd\u591f\u6709\u6548\u5e73\u8861\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\u6743\u8861\uff0c\u4e3a\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u6743\u91cd\u91cd\u65b0\u521d\u59cb\u5316\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08041", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08041", "abs": "https://arxiv.org/abs/2602.08041", "authors": ["Boyang Xia", "Weiyou Tian", "Qingnan Ren", "Jiaqi Huang", "Jie Xiao", "Shuo Lu", "Kai Wang", "Lynn Ai", "Eric Yang", "Bill Shi"], "title": "Implicit Strategic Optimization: Rethinking Long-Horizon Decision-Making in Adversarial Poker Environments", "comment": null, "summary": "Training large language model (LLM) agents for adversarial games is often driven by episodic objectives such as win rate. In long-horizon settings, however, payoffs are shaped by latent strategic externalities that evolve over time, so myopic optimization and variation-based regret analyses can become vacuous even when the dynamics are predictable. To solve this problem, we introduce Implicit Strategic Optimization (ISO), a prediction-aware framework in which each agent forecasts the current strategic context and uses it to update its policy online. ISO combines a Strategic Reward Model (SRM) that estimates the long-run strategic value of actions with iso-grpo, a context-conditioned optimistic learning rule. We prove sublinear contextual regret and equilibrium convergence guarantees whose dominant terms scale with the number of context mispredictions; when prediction errors are bounded, our bounds recover the static-game rates obtained when strategic externalities are known. Experiments in 6-player No-Limit Texas Hold'em and competitive Pokemon show consistent improvements in long-term return over strong LLM and RL baselines, and graceful degradation under controlled prediction noise.", "AI": {"tldr": "\u63d0\u51faISO\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u6218\u7565\u73af\u5883\u6765\u4f18\u5316LLM\u667a\u80fd\u4f53\u5728\u957f\u671f\u5bf9\u6297\u6e38\u620f\u4e2d\u7684\u8868\u73b0\uff0c\u7ed3\u5408\u6218\u7565\u5956\u52b1\u6a21\u578b\u548c\u4e50\u89c2\u5b66\u4e60\u89c4\u5219\uff0c\u5728\u5fb7\u5dde\u6251\u514b\u548c\u5b9d\u53ef\u68a6\u6e38\u620f\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5728\u957f\u671f\u5bf9\u6297\u6e38\u620f\u4e2d\uff0c\u4f20\u7edf\u57fa\u4e8e\u80dc\u7387\u7684\u77ed\u89c6\u4f18\u5316\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u968f\u65f6\u95f4\u6f14\u53d8\u7684\u6218\u7565\u5916\u90e8\u6027\uff0c\u5bfc\u81f4\u9057\u61be\u5206\u6790\u5931\u6548\uff0c\u9700\u8981\u80fd\u591f\u9884\u6d4b\u6218\u7565\u73af\u5883\u53d8\u5316\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u9690\u5f0f\u6218\u7565\u4f18\u5316(ISO)\u6846\u67b6\uff1a1) \u6218\u7565\u5956\u52b1\u6a21\u578b(SRM)\u4f30\u8ba1\u884c\u52a8\u7684\u957f\u671f\u6218\u7565\u4ef7\u503c\uff1b2) iso-grpo\uff0c\u4e00\u79cd\u60c5\u5883\u6761\u4ef6\u4e50\u89c2\u5b66\u4e60\u89c4\u5219\uff1b\u667a\u80fd\u4f53\u9884\u6d4b\u5f53\u524d\u6218\u7565\u73af\u5883\u5e76\u5728\u7ebf\u66f4\u65b0\u7b56\u7565\u3002", "result": "\u7406\u8bba\u8bc1\u660e\uff1a\u5f53\u9884\u6d4b\u8bef\u5dee\u6709\u754c\u65f6\uff0c\u83b7\u5f97\u4e0e\u9759\u6001\u6e38\u620f\u76f8\u5f53\u7684\u6b21\u7ebf\u6027\u60c5\u5883\u9057\u61be\u548c\u5747\u8861\u6536\u655b\u4fdd\u8bc1\u3002\u5b9e\u9a8c\uff1a\u57286\u4eba\u65e0\u9650\u6ce8\u5fb7\u5dde\u6251\u514b\u548c\u7ade\u4e89\u6027\u5b9d\u53ef\u68a6\u6e38\u620f\u4e2d\uff0c\u957f\u671f\u56de\u62a5\u6301\u7eed\u4f18\u4e8e\u5f3aLLM\u548cRL\u57fa\u7ebf\uff0c\u5728\u53d7\u63a7\u9884\u6d4b\u566a\u58f0\u4e0b\u6027\u80fd\u4f18\u96c5\u4e0b\u964d\u3002", "conclusion": "ISO\u6846\u67b6\u901a\u8fc7\u9884\u6d4b\u6218\u7565\u73af\u5883\u6709\u6548\u89e3\u51b3\u4e86\u957f\u671f\u5bf9\u6297\u6e38\u620f\u4e2d\u7684\u4f18\u5316\u95ee\u9898\uff0c\u7406\u8bba\u4fdd\u8bc1\u826f\u597d\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u8d8a\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u6218\u7565\u73af\u5883\u4e2d\u7684\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.08043", "categories": ["cs.LG", "cs.AI", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.08043", "abs": "https://arxiv.org/abs/2602.08043", "authors": ["Yiheng Gao", "Qin Hua", "Zizhong Chen"], "title": "V-ABFT: Variance-Based Adaptive Threshold for Fault-Tolerant Matrix Multiplication in Mixed-Precision Deep Learning", "comment": null, "summary": "Algorithm-Based Fault Tolerance (ABFT) is widely adopted to detect silent data corruptions (SDCs) in matrix multiplication, a cornerstone operation in deep learning systems. However, existing threshold determination methods face critical challenges: analytical bounds are overly conservative, while probabilistic approaches like A-ABFT yield thresholds $160$--$4200\\times$ larger than actual rounding errors. We present V-ABFT, a variance-based adaptive threshold algorithm that achieves tighter error bounds by directly modeling the verification difference. By leveraging statistical variance estimation, V-ABFT reduces the threshold-to-actual-error ratio to approximately $7$--$20\\times$ for FP32/FP64 and $48$--$158\\times$ for BF16, representing a \\textbf{6--48$\\times$ improvement} over A-ABFT while maintaining zero false positive rate across BF16, FP16, FP32, and FP64 precisions. Furthermore, we demonstrate that for fused-kernel ABFT implementations that verify before output quantization, low-precision GEMM can use FP32-level thresholds ($e_{\\max} \\approx 10^{-6}$), enabling \\textbf{$\\sim$1000$\\times$ finer detection granularity} compared to offline verification with low-precision output ($e_{\\max} \\approx 10^{-3}$). We reproduce A-ABFT's experimental setup and validate our implementation against the original paper's results. Our method requires only $O(n)$ complexity using max/min/mean statistics, compared to A-ABFT's $O(pn)$ for finding $p$ largest values. Extensive experiments on synthetic data and real model weights (LLaMA-7B, GPT-2, ViT) demonstrate V-ABFT's effectiveness across diverse distributions. V-ABFT is platform-agnostic and has been integrated into fault-tolerant GEMM implementations on both NPUs and GPUs.", "AI": {"tldr": "V-ABFT\u662f\u4e00\u79cd\u57fa\u4e8e\u65b9\u5dee\u7684\u81ea\u9002\u5e94\u9608\u503c\u7b97\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u77e9\u9635\u4e58\u6cd5\u4e2d\u7684\u9759\u9ed8\u6570\u636e\u635f\u574f\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709ABFT\u9608\u503c\u786e\u5b9a\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u95ee\u9898\uff1a\u5206\u6790\u65b9\u6cd5\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u800c\u6982\u7387\u65b9\u6cd5\u5982A-ABFT\u7684\u9608\u503c\u6bd4\u5b9e\u9645\u820d\u5165\u8bef\u5dee\u5927160-4200\u500d\uff0c\u5bfc\u81f4\u68c0\u6d4b\u7cbe\u5ea6\u4e0d\u8db3\u3002", "method": "\u63d0\u51faV-ABFT\u7b97\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u5efa\u6a21\u9a8c\u8bc1\u5dee\u5f02\u5e76\u5229\u7528\u7edf\u8ba1\u65b9\u5dee\u4f30\u8ba1\uff0c\u5b9e\u73b0\u66f4\u7d27\u5bc6\u7684\u8bef\u5dee\u754c\u9650\u3002\u7b97\u6cd5\u4ec5\u9700O(n)\u590d\u6742\u5ea6\uff0c\u4f7f\u7528\u6700\u5927/\u6700\u5c0f/\u5747\u503c\u7edf\u8ba1\u91cf\u3002", "result": "V-ABFT\u5c06\u9608\u503c\u4e0e\u5b9e\u9645\u8bef\u5dee\u6bd4\u964d\u4f4e\u5230FP32/FP64\u76847-20\u500d\u548cBF16\u768448-158\u500d\uff0c\u76f8\u6bd4A-ABFT\u67096-48\u500d\u6539\u8fdb\uff0c\u4fdd\u6301\u96f6\u8bef\u62a5\u7387\u3002\u5bf9\u4e8e\u878d\u5408\u5185\u6838\u5b9e\u73b0\uff0c\u4f4e\u7cbe\u5ea6GEMM\u53ef\u4f7f\u7528FP32\u7ea7\u9608\u503c\uff0c\u5b9e\u73b0\u7ea61000\u500d\u66f4\u7cbe\u7ec6\u7684\u68c0\u6d4b\u7c92\u5ea6\u3002", "conclusion": "V-ABFT\u5728\u4fdd\u6301\u96f6\u8bef\u62a5\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u590d\u6742\u5ea6\u66f4\u4f4e\uff0c\u5e73\u53f0\u65e0\u5173\uff0c\u5df2\u96c6\u6210\u5230NPU\u548cGPU\u7684\u5bb9\u9519GEMM\u5b9e\u73b0\u4e2d\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u6570\u636e\u5206\u5e03\u3002"}}
{"id": "2602.08050", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08050", "abs": "https://arxiv.org/abs/2602.08050", "authors": ["Qusai Khaled", "Uzay Kaymak", "Laura Genga"], "title": "Interpretable Fuzzy Systems For Forward Osmosis Desalination", "comment": "7 pages, 4 figures, FUZZ-IEEE 2025", "summary": "Preserving interpretability in fuzzy rule-based systems (FRBS) is vital for water treatment, where decisions impact public health. While structural interpretability has been addressed using multi-objective algorithms, semantic interpretability often suffers due to fuzzy sets with low distinguishability. We propose a human-in-the-loop approach for developing interpretable FRBS to predict forward osmosis desalination productivity. Our method integrates expert-driven grid partitioning for distinguishable membership functions, domain-guided feature engineering to reduce redundancy, and rule pruning based on firing strength. This approach achieved comparable predictive performance to cluster-based FRBS while maintaining semantic interpretability and meeting structural complexity constraints, providing an explainable solution for water treatment applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4eba\u673a\u534f\u540c\u65b9\u6cd5\u6784\u5efa\u53ef\u89e3\u91ca\u6a21\u7cca\u89c4\u5219\u7cfb\u7edf\uff0c\u7528\u4e8e\u9884\u6d4b\u6b63\u6e17\u900f\u6d77\u6c34\u6de1\u5316\u751f\u4ea7\u529b\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u8fbe\u5230\u4e0e\u805a\u7c7b\u65b9\u6cd5\u76f8\u5f53\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u5728\u6c34\u5904\u7406\u5e94\u7528\u4e2d\uff0c\u6a21\u7cca\u89c4\u5219\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u51b3\u7b56\u76f4\u63a5\u5f71\u54cd\u516c\u5171\u5065\u5eb7\u3002\u867d\u7136\u7ed3\u6784\u53ef\u89e3\u91ca\u6027\u5df2\u901a\u8fc7\u591a\u76ee\u6807\u7b97\u6cd5\u89e3\u51b3\uff0c\u4f46\u8bed\u4e49\u53ef\u89e3\u91ca\u6027\u5e38\u56e0\u6a21\u7cca\u96c6\u533a\u5206\u5ea6\u4f4e\u800c\u53d7\u635f\u3002", "method": "\u96c6\u6210\u4e13\u5bb6\u9a71\u52a8\u7684\u7f51\u683c\u5212\u5206\u751f\u6210\u53ef\u533a\u5206\u7684\u96b6\u5c5e\u51fd\u6570\u3001\u9886\u57df\u6307\u5bfc\u7684\u7279\u5f81\u5de5\u7a0b\u51cf\u5c11\u5197\u4f59\u3001\u57fa\u4e8e\u89e6\u53d1\u5f3a\u5ea6\u7684\u89c4\u5219\u526a\u679d\uff0c\u6784\u5efa\u4eba\u673a\u534f\u540c\u7684\u6a21\u7cca\u89c4\u5219\u7cfb\u7edf\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u6b63\u6e17\u900f\u6d77\u6c34\u6de1\u5316\u751f\u4ea7\u529b\u65b9\u9762\uff0c\u8fbe\u5230\u4e86\u4e0e\u57fa\u4e8e\u805a\u7c7b\u7684\u6a21\u7cca\u89c4\u5219\u7cfb\u7edf\u76f8\u5f53\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bed\u4e49\u53ef\u89e3\u91ca\u6027\u5e76\u6ee1\u8db3\u7ed3\u6784\u590d\u6742\u6027\u7ea6\u675f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6c34\u5904\u7406\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u4eba\u673a\u534f\u540c\u65b9\u6cd5\u5728\u4fdd\u6301\u8bed\u4e49\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2602.08054", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08054", "abs": "https://arxiv.org/abs/2602.08054", "authors": ["Manan Tayal", "Mumuksh Tayal"], "title": "Epigraph-Guided Flow Matching for Safe and Performant Offline Reinforcement Learning", "comment": "23 pages, 8 figures", "summary": "Offline reinforcement learning (RL) provides a compelling paradigm for training autonomous systems without the risks of online exploration, particularly in safety-critical domains. However, jointly achieving strong safety and performance from fixed datasets remains challenging. Existing safe offline RL methods often rely on soft constraints that allow violations, introduce excessive conservatism, or struggle to balance safety, reward optimization, and adherence to the data distribution. To address this, we propose Epigraph-Guided Flow Matching (EpiFlow), a framework that formulates safe offline RL as a state-constrained optimal control problem to co-optimize safety and performance. We learn a feasibility value function derived from an epigraph reformulation of the optimal control problem, thereby avoiding the decoupled objectives or post-hoc filtering common in prior work. Policies are synthesized by reweighting the behavior distribution based on this epigraph value function and fitting a generative policy via flow matching, enabling efficient, distribution-consistent sampling. Across various safety-critical tasks, including Safety-Gymnasium benchmarks, EpiFlow achieves competitive returns with near-zero empirical safety violations, demonstrating the effectiveness of epigraph-guided policy synthesis.", "AI": {"tldr": "EpiFlow\uff1a\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7epigraph\u91cd\u6784\u5c06\u5b89\u5168\u7ea6\u675f\u8f6c\u5316\u4e3a\u53ef\u884c\u6027\u503c\u51fd\u6570\uff0c\u5728\u4fdd\u8bc1\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u8fd1\u4e4e\u96f6\u7684\u5b89\u5168\u8fdd\u89c4\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u5f3a\u5b89\u5168\u6027\u3001\u9ad8\u6027\u80fd\u548c\u6570\u636e\u5206\u5e03\u4e00\u81f4\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5141\u8bb8\u5b89\u5168\u8fdd\u89c4\uff0c\u8981\u4e48\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u8981\u4e48\u96be\u4ee5\u5e73\u8861\u5b89\u5168\u3001\u5956\u52b1\u4f18\u5316\u548c\u6570\u636e\u5206\u5e03\u3002", "method": "\u5c06\u5b89\u5168\u79bb\u7ebfRL\u5efa\u6a21\u4e3a\u72b6\u6001\u7ea6\u675f\u7684\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u901a\u8fc7epigraph\u91cd\u6784\u5b66\u4e60\u53ef\u884c\u6027\u503c\u51fd\u6570\uff0c\u907f\u514d\u89e3\u8026\u76ee\u6807\u6216\u540e\u5904\u7406\u8fc7\u6ee4\u3002\u57fa\u4e8eepigraph\u503c\u51fd\u6570\u5bf9\u884c\u4e3a\u5206\u5e03\u8fdb\u884c\u91cd\u52a0\u6743\uff0c\u5e76\u901a\u8fc7\u6d41\u5339\u914d\u62df\u5408\u751f\u6210\u7b56\u7565\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u5206\u5e03\u4e00\u81f4\u7684\u91c7\u6837\u3002", "result": "\u5728\u5305\u62ecSafety-Gymnasium\u57fa\u51c6\u5728\u5185\u7684\u591a\u4e2a\u5b89\u5168\u5173\u952e\u4efb\u52a1\u4e2d\uff0cEpiFlow\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u56de\u62a5\u548c\u8fd1\u4e4e\u96f6\u7684\u5b9e\u8bc1\u5b89\u5168\u8fdd\u89c4\uff0c\u8bc1\u660e\u4e86epigraph\u5f15\u5bfc\u7b56\u7565\u5408\u6210\u7684\u6709\u6548\u6027\u3002", "conclusion": "EpiFlow\u901a\u8fc7epigraph\u91cd\u6784\u5c06\u5b89\u5168\u7ea6\u675f\u7eb3\u5165\u79bb\u7ebfRL\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5b89\u5168\u4e0e\u6027\u80fd\u7684\u8054\u5408\u4f18\u5316\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08060", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08060", "abs": "https://arxiv.org/abs/2602.08060", "authors": ["Alejandro Ruiz y Mesa", "Guilherme Korol", "Moritz Riesteter", "Jo\u00e3o Paulo Cardoso de Lima", "Jeronimo Castrillon"], "title": "Compiler-Assisted Speculative Sampling for Accelerated LLM Inference on Heterogeneous Edge Devices", "comment": "Accepted to AccML@HiPEAC 2026", "summary": "LLM deployment on resource-constrained edge devices faces severe latency constraints, particularly in real-time applications where delayed responses can compromise safety or usability. Among many approaches to mitigate the inefficiencies of sequential token-by-token generation, Speculative Decoding (SD) has emerged as a promising technique. However, SD at the edge is hindered by two major challenges: (1) integrating SD into a compiler-based workflow without sacrificing performance or programmability, and (2) exploiting the heterogeneous compute resources of modern SoCs through carefully designed partitioning strategies. This work addresses these challenges by using an analytical cost model that explores heterogeneous hardware configurations and guides coarse-grained partitioning of LLM subgraphs, particularly with edge-typical short input sequence lengths. The cost model predicts when speculative sampling and heterogeneous execution are jointly beneficial and is validated on an edge device featuring a hexacore Cortex-A CPU and a Mali GPU, revealing up to 1.68$\\times$ speedup for translation tasks, closely matching analytic expectations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u4e0aLLM\u63a8\u7406\u7684\u63a8\u6d4b\u89e3\u7801\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u6210\u672c\u6a21\u578b\u6307\u5bfc\u5f02\u6784\u786c\u4ef6\u914d\u7f6e\u548c\u7c97\u7c92\u5ea6\u5206\u533a\uff0c\u5728\u8fb9\u7f18\u5178\u578b\u77ed\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\u4e0b\u5b9e\u73b0\u52a0\u901f\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72LLM\u9762\u4e34\u4e25\u91cd\u7684\u5ef6\u8fdf\u7ea6\u675f\uff0c\u7279\u522b\u662f\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u3002\u63a8\u6d4b\u89e3\u7801\u662f\u7f13\u89e3\u5e8f\u5217\u5316token\u751f\u6210\u4f4e\u6548\u6027\u7684\u6709\u524d\u666f\u6280\u672f\uff0c\u4f46\u5728\u8fb9\u7f18\u90e8\u7f72\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1) \u5982\u4f55\u5c06SD\u96c6\u6210\u5230\u57fa\u4e8e\u7f16\u8bd1\u5668\u7684\u6d41\u7a0b\u4e2d\u800c\u4e0d\u727a\u7272\u6027\u80fd\u6216\u53ef\u7f16\u7a0b\u6027\uff1b2) \u5982\u4f55\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5212\u5206\u7b56\u7565\u5229\u7528\u73b0\u4ee3SoC\u7684\u5f02\u6784\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "\u4f7f\u7528\u5206\u6790\u6210\u672c\u6a21\u578b\u63a2\u7d22\u5f02\u6784\u786c\u4ef6\u914d\u7f6e\uff0c\u6307\u5bfcLLM\u5b50\u56fe\u7684\u7c97\u7c92\u5ea6\u5212\u5206\uff0c\u7279\u522b\u9488\u5bf9\u8fb9\u7f18\u5178\u578b\u7684\u77ed\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\u3002\u8be5\u6a21\u578b\u9884\u6d4b\u4f55\u65f6\u63a8\u6d4b\u91c7\u6837\u548c\u5f02\u6784\u6267\u884c\u8054\u5408\u6709\u76ca\uff0c\u5e76\u5728\u914d\u5907\u516d\u6838Cortex-A CPU\u548cMali GPU\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad81.68\u500d\u7684\u52a0\u901f\uff0c\u4e0e\u5206\u6790\u9884\u671f\u7d27\u5bc6\u5339\u914d\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u5206\u6790\u6210\u672c\u6a21\u578b\u548c\u5f02\u6784\u786c\u4ef6\u5206\u533a\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8fb9\u7f18\u8bbe\u5907\u4e0aLLM\u63a8\u6d4b\u89e3\u7801\u7684\u96c6\u6210\u548c\u8d44\u6e90\u5229\u7528\u6311\u6218\uff0c\u4e3a\u5b9e\u65f6\u8fb9\u7f18AI\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u52a0\u901f\u65b9\u6848\u3002"}}
{"id": "2602.08062", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08062", "abs": "https://arxiv.org/abs/2602.08062", "authors": ["Shayan Ali Hassan", "Tao Ni", "Zafar Ayyub Qazi", "Marco Canini"], "title": "Efficient and Adaptable Detection of Malicious LLM Prompts via Bootstrap Aggregation", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, reasoning, and generation. However, these systems remain susceptible to malicious prompts that induce unsafe or policy-violating behavior through harmful requests, jailbreak techniques, and prompt injection attacks. Existing defenses face fundamental limitations: black-box moderation APIs offer limited transparency and adapt poorly to evolving threats, while white-box approaches using large LLM judges impose prohibitive computational costs and require expensive retraining for new attacks. Current systems force designers to choose between performance, efficiency, and adaptability.\n  To address these challenges, we present BAGEL (Bootstrap AGgregated Ensemble Layer), a modular, lightweight, and incrementally updatable framework for malicious prompt detection. BAGEL employs a bootstrap aggregation and mixture of expert inspired ensemble of fine-tuned models, each specialized on a different attack dataset. At inference, BAGEL uses a random forest router to identify the most suitable ensemble member, then applies stochastic selection to sample additional members for prediction aggregation. When new attacks emerge, BAGEL updates incrementally by fine-tuning a small prompt-safety classifier (86M parameters) and adding the resulting model to the ensemble. BAGEL achieves an F1 score of 0.92 by selecting just 5 ensemble members (430M parameters), outperforming OpenAI Moderation API and ShieldGemma which require billions of parameters. Performance remains robust after nine incremental updates, and BAGEL provides interpretability through its router's structural features. Our results show ensembles of small finetuned classifiers can match or exceed billion-parameter guardrails while offering the adaptability and efficiency required for production systems.", "AI": {"tldr": "BAGEL\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u5c0f\u578b\u5fae\u8c03\u6a21\u578b\u5b9e\u73b0\u6076\u610f\u63d0\u793a\u68c0\u6d4b\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u63d0\u4f9b\u6a21\u5757\u5316\u3001\u8f7b\u91cf\u7ea7\u548c\u53ef\u589e\u91cf\u66f4\u65b0\u7684\u7279\u6027", "motivation": "\u73b0\u6709\u6076\u610f\u63d0\u793a\u68c0\u6d4b\u65b9\u6cd5\u9762\u4e34\u6839\u672c\u6027\u9650\u5236\uff1a\u9ed1\u7bb1API\u7f3a\u4e4f\u900f\u660e\u6027\u4e14\u96be\u4ee5\u9002\u5e94\u65b0\u5a01\u80c1\uff0c\u767d\u7bb1\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u4e14\u9700\u8981\u6602\u8d35\u7684\u91cd\u65b0\u8bad\u7ec3\u3002\u5f53\u524d\u7cfb\u7edf\u8feb\u4f7f\u8bbe\u8ba1\u8005\u5728\u6027\u80fd\u3001\u6548\u7387\u548c\u9002\u5e94\u6027\u4e4b\u95f4\u505a\u51fa\u59a5\u534f\u3002", "method": "BAGEL\u91c7\u7528\u5f15\u5bfc\u805a\u5408\u548c\u4e13\u5bb6\u6df7\u5408\u542f\u53d1\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u5305\u542b\u591a\u4e2a\u5728\u4e0d\u540c\u653b\u51fb\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u7684\u5c0f\u578b\u6a21\u578b\u3002\u63a8\u7406\u65f6\u4f7f\u7528\u968f\u673a\u68ee\u6797\u8def\u7531\u5668\u9009\u62e9\u6700\u5408\u9002\u7684\u96c6\u6210\u6210\u5458\uff0c\u5e76\u901a\u8fc7\u968f\u673a\u9009\u62e9\u91c7\u6837\u989d\u5916\u6210\u5458\u8fdb\u884c\u9884\u6d4b\u805a\u5408\u3002\u5f53\u65b0\u653b\u51fb\u51fa\u73b0\u65f6\uff0c\u901a\u8fc7\u5fae\u8c03\u5c0f\u578b\u63d0\u793a\u5b89\u5168\u5206\u7c7b\u5668\uff088600\u4e07\u53c2\u6570\uff09\u5e76\u5c06\u5176\u6dfb\u52a0\u5230\u96c6\u6210\u4e2d\u6765\u589e\u91cf\u66f4\u65b0\u3002", "result": "BAGEL\u4ec5\u9009\u62e95\u4e2a\u96c6\u6210\u6210\u5458\uff084.3\u4ebf\u53c2\u6570\uff09\u5373\u53ef\u8fbe\u52300.92\u7684F1\u5206\u6570\uff0c\u4f18\u4e8e\u9700\u8981\u6570\u5341\u4ebf\u53c2\u6570\u7684OpenAI Moderation API\u548cShieldGemma\u3002\u7ecf\u8fc79\u6b21\u589e\u91cf\u66f4\u65b0\u540e\u6027\u80fd\u4fdd\u6301\u7a33\u5065\uff0c\u5e76\u901a\u8fc7\u8def\u7531\u5668\u7684\u7ed3\u6784\u7279\u5f81\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u5c0f\u578b\u5fae\u8c03\u5206\u7c7b\u5668\u7684\u96c6\u6210\u80fd\u591f\u5339\u914d\u6216\u8d85\u8d8a\u6570\u5341\u4ebf\u53c2\u6570\u7684\u9632\u62a4\u7cfb\u7edf\uff0c\u540c\u65f6\u63d0\u4f9b\u751f\u4ea7\u7cfb\u7edf\u6240\u9700\u7684\u9002\u5e94\u6027\u548c\u6548\u7387\uff0c\u5c55\u793a\u4e86\u8f7b\u91cf\u7ea7\u96c6\u6210\u65b9\u6cd5\u5728\u6076\u610f\u63d0\u793a\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.08063", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08063", "abs": "https://arxiv.org/abs/2602.08063", "authors": ["Eduardo Figueiredo", "Steven Adams", "Luca Laurenti"], "title": "Efficient Distribution Learning with Error Bounds in Wasserstein Distance", "comment": null, "summary": "The Wasserstein distance has emerged as a key metric to quantify distances between probability distributions, with applications in various fields, including machine learning, control theory, decision theory, and biological systems. Consequently, learning an unknown distribution with non-asymptotic and easy-to-compute error bounds in Wasserstein distance has become a fundamental problem in many fields. In this paper, we devise a novel algorithmic and theoretical framework to approximate an unknown probability distribution $\\mathbb{P}$ from a finite set of samples by an approximate discrete distribution $\\widehat{\\mathbb{P}}$ while bounding the Wasserstein distance between $\\mathbb{P}$ and $\\widehat{\\mathbb{P}}$. Our framework leverages optimal transport, nonlinear optimization, and concentration inequalities. In particular, we show that, even if $\\mathbb{P}$ is unknown, the Wasserstein distance between $\\mathbb{P}$ and $\\widehat{\\mathbb{P}}$ can be efficiently bounded with high confidence by solving a tractable optimization problem (a mixed integer linear program) of a size that only depends on the size of the support of $\\widehat{\\mathbb{P}}$. This enables us to develop intelligent clustering algorithms to optimally find the support of $\\widehat{\\mathbb{P}}$ while minimizing the Wasserstein distance error. On a set of benchmarks, we demonstrate that our approach outperforms state-of-the-art comparable methods by generally returning approximating distributions with substantially smaller support and tighter error bounds.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u548c\u6574\u6570\u89c4\u5212\u7684\u6846\u67b6\uff0c\u4ece\u6709\u9650\u6837\u672c\u4e2d\u5b66\u4e60\u672a\u77e5\u5206\u5e03\uff0c\u5e76\u63d0\u4f9bWasserstein\u8ddd\u79bb\u7684\u975e\u6e10\u8fd1\u8bef\u5dee\u754c", "motivation": "Wasserstein\u8ddd\u79bb\u5df2\u6210\u4e3a\u91cf\u5316\u6982\u7387\u5206\u5e03\u8ddd\u79bb\u7684\u5173\u952e\u6307\u6807\uff0c\u5728\u673a\u5668\u5b66\u4e60\u3001\u63a7\u5236\u7406\u8bba\u3001\u51b3\u7b56\u7406\u8bba\u548c\u751f\u7269\u7cfb\u7edf\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\u3002\u4ece\u6709\u9650\u6837\u672c\u4e2d\u5b66\u4e60\u672a\u77e5\u5206\u5e03\u5e76\u63d0\u4f9b\u6613\u4e8e\u8ba1\u7b97\u7684\u975e\u6e10\u8fd1\u8bef\u5dee\u754c\u5df2\u6210\u4e3a\u8bb8\u591a\u9886\u57df\u7684\u57fa\u672c\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u6700\u4f18\u4f20\u8f93\u3001\u975e\u7ebf\u6027\u4f18\u5316\u548c\u96c6\u4e2d\u4e0d\u7b49\u5f0f\u7684\u7406\u8bba\u6846\u67b6\u3002\u901a\u8fc7\u6c42\u89e3\u4e00\u4e2a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u95ee\u9898\uff0c\u5373\u4f7f\u771f\u5b9e\u5206\u5e03\u672a\u77e5\uff0c\u4e5f\u80fd\u9ad8\u6548\u5730\u4ee5\u9ad8\u7f6e\u4fe1\u5ea6\u754c\u5b9aWasserstein\u8ddd\u79bb\u8bef\u5dee\u3002\u5f00\u53d1\u667a\u80fd\u805a\u7c7b\u7b97\u6cd5\uff0c\u5728\u6700\u5c0f\u5316Wasserstein\u8bef\u5dee\u7684\u540c\u65f6\u4f18\u5316\u8fd1\u4f3c\u5206\u5e03\u7684\u652f\u6491\u96c6\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u53ef\u6bd4\u65b9\u6cd5\uff0c\u901a\u5e38\u8fd4\u56de\u652f\u6491\u96c6\u66f4\u5c0f\u3001\u8bef\u5dee\u754c\u66f4\u7d27\u7684\u8fd1\u4f3c\u5206\u5e03\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4ece\u6709\u9650\u6837\u672c\u4e2d\u8fd1\u4f3c\u672a\u77e5\u5206\u5e03\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u7b97\u6cd5\u548c\u7406\u8bba\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u4f9b\u975e\u6e10\u8fd1\u4e14\u6613\u4e8e\u8ba1\u7b97\u7684Wasserstein\u8ddd\u79bb\u8bef\u5dee\u754c\uff0c\u5728\u591a\u4e2a\u5e94\u7528\u9886\u57df\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.08064", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08064", "abs": "https://arxiv.org/abs/2602.08064", "authors": ["Tianyu Li", "Dongchen Han", "Zixuan Cao", "Haofeng Huang", "Mengyu Zhou", "Ming Chen", "Erchao Zhao", "Xiaoxi Jiang", "Guanjun Jiang", "Gao Huang"], "title": "SiameseNorm: Breaking the Barrier to Reconciling Pre/Post-Norm", "comment": null, "summary": "Modern Transformers predominantly adopt the Pre-Norm paradigm for its optimization stability, foregoing the superior potential of the unstable Post-Norm architecture. Prior attempts to combine their strengths typically lead to a stability-performance trade-off. We attribute this phenomenon to a structural incompatibility within a single-stream design: Any application of the Post-Norm operation inevitably obstructs the clean identity gradient preserved by Pre-Norm. To fundamentally reconcile these paradigms, we propose SiameseNorm, a two-stream architecture that couples Pre-Norm-like and Post-Norm-like streams with shared parameters. This design decouples the optimization dynamics of the two streams, retaining the distinct characteristics of both Pre-Norm and Post-Norm by enabling all residual blocks to receive combined gradients inherited from both paradigms, where one stream secures stability while the other enhances expressivity. Extensive pre-training experiments on 1.3B-parameter models demonstrate that SiameseNorm exhibits exceptional optimization robustness and consistently outperforms strong baselines. Code is available at https://github.com/Qwen-Applications/SiameseNorm.", "AI": {"tldr": "SiameseNorm\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6d41\u67b6\u6784\uff0c\u5c06Pre-Norm\u548cPost-Norm\u8303\u5f0f\u89e3\u8026\uff0c\u65e2\u4fdd\u6301\u4e86\u4f18\u5316\u7a33\u5b9a\u6027\u53c8\u63d0\u5347\u4e86\u8868\u8fbe\u80fd\u529b\uff0c\u572813\u4ebf\u53c2\u6570\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u4ee3Transformer\u4e3b\u8981\u91c7\u7528Pre-Norm\u8303\u5f0f\u4ee5\u4fdd\u8bc1\u4f18\u5316\u7a33\u5b9a\u6027\uff0c\u4f46\u727a\u7272\u4e86Post-Norm\u67b6\u6784\u7684\u4f18\u8d8a\u6f5c\u529b\u3002\u5148\u524d\u5c1d\u8bd5\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u901a\u5e38\u5bfc\u81f4\u7a33\u5b9a\u6027\u4e0e\u6027\u80fd\u7684\u6743\u8861\uff0c\u8fd9\u6e90\u4e8e\u5355\u6d41\u8bbe\u8ba1\u4e2dPost-Norm\u64cd\u4f5c\u4f1a\u963b\u788dPre-Norm\u4fdd\u6301\u7684\u5e72\u51c0\u6052\u7b49\u68af\u5ea6\u3002", "method": "\u63d0\u51faSiameseNorm\u53cc\u6d41\u67b6\u6784\uff0c\u8026\u5408\u5177\u6709\u5171\u4eab\u53c2\u6570\u7684Pre-Norm-like\u548cPost-Norm-like\u6d41\u3002\u8fd9\u79cd\u8bbe\u8ba1\u89e3\u8026\u4e86\u4e24\u4e2a\u6d41\u7684\u4f18\u5316\u52a8\u6001\uff0c\u4f7f\u6240\u6709\u6b8b\u5dee\u5757\u90fd\u80fd\u63a5\u6536\u6765\u81ea\u4e24\u79cd\u8303\u5f0f\u7684\u7ec4\u5408\u68af\u5ea6\uff0c\u5176\u4e2d\u4e00\u4e2a\u6d41\u786e\u4fdd\u7a33\u5b9a\u6027\uff0c\u53e6\u4e00\u4e2a\u589e\u5f3a\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u572813\u4ebf\u53c2\u6570\u6a21\u578b\u4e0a\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u5b9e\u9a8c\u8868\u660e\uff0cSiameseNorm\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u4f18\u5316\u9c81\u68d2\u6027\uff0c\u5e76\u6301\u7eed\u8d85\u8d8a\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "SiameseNorm\u901a\u8fc7\u53cc\u6d41\u67b6\u6784\u4ece\u6839\u672c\u4e0a\u8c03\u548c\u4e86Pre-Norm\u548cPost-Norm\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u4f18\u5316\u7a33\u5b9a\u6027\u4e0e\u8868\u8fbe\u80fd\u529b\u7684\u53cc\u91cd\u4f18\u52bf\uff0c\u4e3aTransformer\u67b6\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.08067", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08067", "abs": "https://arxiv.org/abs/2602.08067", "authors": ["Chenglei Shen", "Yi Zhan", "Weijie Yu", "Xiao Zhang", "Jun Xu"], "title": "Enhancing Bandit Algorithms with LLMs for Time-varying User Preferences in Streaming Recommendations", "comment": null, "summary": "In real-world streaming recommender systems, user preferences evolve dynamically over time. Existing bandit-based methods treat time merely as a timestamp, neglecting its explicit relationship with user preferences and leading to suboptimal performance. Moreover, online learning methods often suffer from inefficient exploration-exploitation during the early online phase. To address these issues, we propose HyperBandit+, a novel contextual bandit policy that integrates a time-aware hypernetwork to adapt to time-varying user preferences and employs a large language model-assisted warm-start mechanism (LLM Start) to enhance exploration-exploitation efficiency in the early online phase. Specifically, HyperBandit+ leverages a neural network that takes time features as input and generates parameters for estimating time-varying rewards by capturing the correlation between time and user preferences. Additionally, the LLM Start mechanism employs multi-step data augmentation to simulate realistic interaction data for effective offline learning, providing warm-start parameters for the bandit policy in the early online phase. To meet real-time streaming recommendation demands, we adopt low-rank factorization to reduce hypernetwork training complexity. Theoretically, we rigorously establish a sublinear regret upper bound that accounts for both the hypernetwork and the LLM warm-start mechanism. Extensive experiments on real-world datasets demonstrate that HyperBandit+ consistently outperforms state-of-the-art baselines in terms of accumulated rewards.", "AI": {"tldr": "HyperBandit+\uff1a\u4e00\u79cd\u65b0\u9896\u7684\u4e0a\u4e0b\u6587\u8001\u864e\u673a\u7b56\u7565\uff0c\u901a\u8fc7\u65f6\u95f4\u611f\u77e5\u8d85\u7f51\u7edc\u9002\u5e94\u65f6\u53d8\u7528\u6237\u504f\u597d\uff0c\u5e76\u5229\u7528LLM\u8f85\u52a9\u7684\u9884\u70ed\u542f\u52a8\u673a\u5236\u63d0\u5347\u65e9\u671f\u5728\u7ebf\u9636\u6bb5\u7684\u63a2\u7d22-\u5229\u7528\u6548\u7387\u3002", "motivation": "\u73b0\u5b9e\u6d41\u5f0f\u63a8\u8350\u7cfb\u7edf\u4e2d\u7528\u6237\u504f\u597d\u968f\u65f6\u95f4\u52a8\u6001\u6f14\u53d8\uff0c\u73b0\u6709\u57fa\u4e8e\u8001\u864e\u673a\u7684\u65b9\u6cd5\u4ec5\u5c06\u65f6\u95f4\u89c6\u4e3a\u65f6\u95f4\u6233\uff0c\u5ffd\u7565\u4e86\u5176\u4e0e\u7528\u6237\u504f\u597d\u7684\u663e\u5f0f\u5173\u7cfb\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002\u6b64\u5916\uff0c\u5728\u7ebf\u5b66\u4e60\u65b9\u6cd5\u5728\u65e9\u671f\u5728\u7ebf\u9636\u6bb5\u5f80\u5f80\u5b58\u5728\u63a2\u7d22-\u5229\u7528\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "1) \u65f6\u95f4\u611f\u77e5\u8d85\u7f51\u7edc\uff1a\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u4ee5\u65f6\u95f4\u7279\u5f81\u4e3a\u8f93\u5165\uff0c\u751f\u6210\u4f30\u8ba1\u65f6\u53d8\u5956\u52b1\u7684\u53c2\u6570\uff0c\u6355\u6349\u65f6\u95f4\u4e0e\u7528\u6237\u504f\u597d\u7684\u76f8\u5173\u6027\uff1b2) LLM\u8f85\u52a9\u9884\u70ed\u542f\u52a8\u673a\u5236\uff1a\u91c7\u7528\u591a\u6b65\u6570\u636e\u589e\u5f3a\u6a21\u62df\u771f\u5b9e\u4ea4\u4e92\u6570\u636e\u8fdb\u884c\u6709\u6548\u79bb\u7ebf\u5b66\u4e60\uff0c\u4e3a\u65e9\u671f\u5728\u7ebf\u9636\u6bb5\u63d0\u4f9b\u9884\u70ed\u53c2\u6570\uff1b3) \u4f4e\u79e9\u5206\u89e3\uff1a\u964d\u4f4e\u8d85\u7f51\u7edc\u8bad\u7ec3\u590d\u6742\u5ea6\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u6d41\u5f0f\u63a8\u8350\u9700\u6c42\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHyperBandit+\u5728\u7d2f\u79ef\u5956\u52b1\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u7406\u8bba\u5206\u6790\u5efa\u7acb\u4e86\u8003\u8651\u8d85\u7f51\u7edc\u548cLLM\u9884\u70ed\u542f\u52a8\u673a\u5236\u7684\u6b21\u7ebf\u6027\u9057\u61be\u4e0a\u754c\u3002", "conclusion": "HyperBandit+\u901a\u8fc7\u6574\u5408\u65f6\u95f4\u611f\u77e5\u8d85\u7f51\u7edc\u548cLLM\u8f85\u52a9\u9884\u70ed\u542f\u52a8\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6d41\u5f0f\u63a8\u8350\u4e2d\u65f6\u53d8\u7528\u6237\u504f\u597d\u5efa\u6a21\u548c\u65e9\u671f\u5728\u7ebf\u5b66\u4e60\u6548\u7387\u95ee\u9898\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.08077", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08077", "abs": "https://arxiv.org/abs/2602.08077", "authors": ["Sayantan Kumar", "Peijie Qiu", "Aristeidis Sotiras"], "title": "Multimodal normative modeling in Alzheimers Disease with introspective variational autoencoders", "comment": "Conference on Health, Inference, and Learning (CHIL)", "summary": "Normative modeling learns a healthy reference distribution and quantifies subject-specific deviations to capture heterogeneous disease effects. In Alzheimers disease (AD), multimodal neuroimaging offers complementary signals but VAE-based normative models often (i) fit the healthy reference distribution imperfectly, inflating false positives, and (ii) use posterior aggregation (e.g., PoE/MoE) that can yield weak multimodal fusion in the shared latent space. We propose mmSIVAE, a multimodal soft-introspective variational autoencoder combined with Mixture-of-Product-of-Experts (MOPOE) aggregation to improve reference fidelity and multimodal integration. We compute deviation scores in latent space and feature space as distances from the learned healthy distributions, and map statistically significant latent deviations to regional abnormalities for interpretability. On ADNI MRI regional volumes and amyloid PET SUVR, mmSIVAE improves reconstruction on held-out controls and produces more discriminative deviation scores for outlier detection than VAE baselines, with higher likelihood ratios and clearer separation between control and AD-spectrum cohorts. Deviation maps highlight region-level patterns aligned with established AD-related changes. More broadly, our results highlight the importance of training objectives that prioritize reference-distribution fidelity and robust multimodal posterior aggregation for normative modeling, with implications for deviation-based analysis across multimodal clinical data.", "AI": {"tldr": "\u63d0\u51fammSIVAE\u6a21\u578b\uff0c\u7ed3\u5408\u8f6f\u81ea\u7701VAE\u4e0e\u6df7\u5408\u4e13\u5bb6\u4e58\u79ef\u805a\u5408\uff0c\u6539\u8fdb\u591a\u6a21\u6001\u795e\u7ecf\u5f71\u50cf\u7684\u89c4\u8303\u6027\u5efa\u6a21\uff0c\u63d0\u5347\u5065\u5eb7\u53c2\u8003\u5206\u5e03\u62df\u5408\u4e0e\u591a\u6a21\u6001\u878d\u5408\uff0c\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709VAE\u89c4\u8303\u6027\u6a21\u578b\u5728\u591a\u6a21\u6001\u795e\u7ecf\u5f71\u50cf\u5206\u6790\u4e2d\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a1) \u5bf9\u5065\u5eb7\u53c2\u8003\u5206\u5e03\u62df\u5408\u4e0d\u5b8c\u5584\uff0c\u5bfc\u81f4\u5047\u9633\u6027\u589e\u52a0\uff1b2) \u540e\u9a8c\u805a\u5408\u65b9\u6cd5\uff08\u5982PoE/MoE\uff09\u5728\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\u591a\u6a21\u6001\u878d\u5408\u6548\u679c\u5f31\u3002\u9700\u8981\u6539\u8fdb\u89c4\u8303\u6027\u5efa\u6a21\u4ee5\u66f4\u597d\u5730\u6355\u6349\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u5f02\u8d28\u6027\u6548\u5e94\u3002", "method": "\u63d0\u51fammSIVAE\uff08\u591a\u6a21\u6001\u8f6f\u81ea\u7701\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff09\uff0c\u7ed3\u5408Mixture-of-Product-of-Experts\uff08MOPOE\uff09\u805a\u5408\u673a\u5236\u3002\u5728\u6f5c\u5728\u7a7a\u95f4\u548c\u7279\u5f81\u7a7a\u95f4\u8ba1\u7b97\u4e0e\u5b66\u4e60\u5230\u7684\u5065\u5eb7\u5206\u5e03\u7684\u8ddd\u79bb\u4f5c\u4e3a\u504f\u5dee\u5206\u6570\uff0c\u5e76\u5c06\u7edf\u8ba1\u663e\u8457\u7684\u6f5c\u5728\u504f\u5dee\u6620\u5c04\u5230\u533a\u57df\u5f02\u5e38\u4ee5\u5b9e\u73b0\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728ADNI\u7684MRI\u533a\u57df\u4f53\u79ef\u548c\u6dc0\u7c89\u6837\u86cb\u767dPET SUVR\u6570\u636e\u4e0a\uff0cmmSIVAE\u5728\u4fdd\u7559\u5bf9\u7167\u7ec4\u4e0a\u6539\u8fdb\u91cd\u5efa\u6548\u679c\uff0c\u76f8\u6bd4VAE\u57fa\u7ebf\u4ea7\u751f\u66f4\u5177\u533a\u5206\u6027\u7684\u504f\u5dee\u5206\u6570\u7528\u4e8e\u5f02\u5e38\u68c0\u6d4b\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u4f3c\u7136\u6bd4\u548c\u66f4\u6e05\u6670\u7684\u5bf9\u7167\u7ec4\u4e0eAD\u8c31\u7cfb\u961f\u5217\u5206\u79bb\u3002\u504f\u5dee\u56fe\u7a81\u51fa\u663e\u793a\u4e0e\u5df2\u77e5AD\u76f8\u5173\u53d8\u5316\u4e00\u81f4\u7684\u533a\u57df\u7ea7\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u8bad\u7ec3\u76ee\u6807\u4e2d\u4f18\u5148\u8003\u8651\u53c2\u8003\u5206\u5e03\u4fdd\u771f\u5ea6\u548c\u9c81\u68d2\u591a\u6a21\u6001\u540e\u9a8c\u805a\u5408\u5bf9\u4e8e\u89c4\u8303\u6027\u5efa\u6a21\u7684\u91cd\u8981\u6027\uff0c\u5bf9\u8de8\u591a\u6a21\u6001\u4e34\u5e8a\u6570\u636e\u7684\u504f\u5dee\u5206\u6790\u5177\u6709\u5e7f\u6cdb\u610f\u4e49\u3002"}}
{"id": "2602.08082", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.08082", "abs": "https://arxiv.org/abs/2602.08082", "authors": ["Valentin No\u00ebl"], "title": "Spectral Guardrails for Agents in the Wild: Detecting Tool Use Hallucinations via Attention Topology", "comment": "32 pages, 2 fgures, 18 tables", "summary": "Deploying autonomous agents in the wild requires reliable safeguards against tool use failures. We propose a training free guardrail based on spectral analysis of attention topology that complements supervised approaches. On Llama 3.1 8B, our method achieves 97.7\\% recall with multi-feature detection and 86.1\\% recall with 81.0\\% precision for balanced deployment, without requiring any labeled training data. Most remarkably, we discover that single layer spectral features act as near-perfect hallucination detectors: Llama L26 Smoothness achieves 98.2\\% recall (213/217 hallucinations caught) with a single threshold, and Mistral L3 Entropy achieves 94.7\\% recall. This suggests hallucination is not merely a wrong token but a thermodynamic state change: the model's attention becomes noise when it errs. Through controlled cross-model evaluation on matched domains ($N=1000$, $T=0.3$, same General domain, hallucination rates 20--22\\%), we reveal the ``Loud Liar'' phenomenon: Llama 3.1 8B's failures are spectrally catastrophic and dramatically easier to detect, while Mistral 7B achieves the best discrimination (AUC 0.900). These findings establish spectral analysis as a principled, efficient framework for agent safety.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6ce8\u610f\u529b\u62d3\u6251\u8c31\u5206\u6790\u7684\u514d\u8bad\u7ec3\u62a4\u680f\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u81ea\u4e3b\u4ee3\u7406\u5de5\u5177\u4f7f\u7528\u5931\u8d25\uff0c\u5728Llama 3.1 8B\u4e0a\u5b9e\u73b097.7%\u53ec\u56de\u7387\uff0c\u53d1\u73b0\u5355\u5c42\u8c31\u7279\u5f81\u53ef\u4f5c\u4e3a\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u5e7b\u89c9\u68c0\u6d4b\u5668\u3002", "motivation": "\u5728\u91ce\u5916\u90e8\u7f72\u81ea\u4e3b\u4ee3\u7406\u9700\u8981\u53ef\u9760\u7684\u5b89\u5168\u4fdd\u969c\u6765\u9632\u6b62\u5de5\u5177\u4f7f\u7528\u5931\u8d25\u3002\u73b0\u6709\u76d1\u7763\u65b9\u6cd5\u9700\u8981\u6807\u6ce8\u6570\u636e\uff0c\u800c\u672c\u6587\u63d0\u51fa\u514d\u8bad\u7ec3\u65b9\u6cd5\u6765\u8865\u5145\u76d1\u7763\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u6ce8\u610f\u529b\u62d3\u6251\u7684\u8c31\u5206\u6790\u65b9\u6cd5\uff0c\u65e0\u9700\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\u3002\u4f7f\u7528\u5355\u5c42\u8c31\u7279\u5f81\uff08\u5982Llama L26\u5e73\u6ed1\u5ea6\u548cMistral L3\u71b5\uff09\u4f5c\u4e3a\u5e7b\u89c9\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u9608\u503c\u8fdb\u884c\u68c0\u6d4b\u3002", "result": "\u5728Llama 3.1 8B\u4e0a\uff0c\u591a\u7279\u5f81\u68c0\u6d4b\u8fbe\u523097.7%\u53ec\u56de\u7387\uff0c\u5e73\u8861\u90e8\u7f72\u8fbe\u523086.1%\u53ec\u56de\u7387\u548c81.0%\u7cbe\u786e\u7387\u3002\u5355\u5c42\u8c31\u7279\u5f81\u68c0\u6d4b\u6548\u679c\u60ca\u4eba\uff1aLlama L26\u5e73\u6ed1\u5ea6\u8fbe\u523098.2%\u53ec\u56de\u7387\uff08\u6355\u83b7213/217\u4e2a\u5e7b\u89c9\uff09\uff0cMistral L3\u71b5\u8fbe\u523094.7%\u53ec\u56de\u7387\u3002\u8de8\u6a21\u578b\u8bc4\u4f30\u53d1\u73b0\"\u5927\u58f0\u8bf4\u8c0e\u8005\"\u73b0\u8c61\uff1aLlama 3.1 8B\u7684\u5931\u8d25\u5728\u8c31\u5206\u6790\u4e0a\u66f4\u6613\u68c0\u6d4b\uff0c\u800cMistral 7B\u8fbe\u5230\u6700\u4f73\u533a\u5206\u5ea6\uff08AUC 0.900\uff09\u3002", "conclusion": "\u8c31\u5206\u6790\u4e3a\u4ee3\u7406\u5b89\u5168\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u3001\u9ad8\u6548\u7684\u6846\u67b6\u3002\u7814\u7a76\u53d1\u73b0\u5e7b\u89c9\u4e0d\u4ec5\u662f\u9519\u8bef\u6807\u8bb0\uff0c\u800c\u662f\u70ed\u529b\u5b66\u72b6\u6001\u53d8\u5316\uff1a\u6a21\u578b\u6ce8\u610f\u529b\u5728\u51fa\u9519\u65f6\u53d8\u4e3a\u566a\u58f0\u3002\u8be5\u65b9\u6cd5\u53ef\u4f5c\u4e3a\u514d\u8bad\u7ec3\u7684\u5b89\u5168\u62a4\u680f\u3002"}}
{"id": "2602.08086", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08086", "abs": "https://arxiv.org/abs/2602.08086", "authors": ["Liisa Janssens", "Laura Middeldorp"], "title": "Probability Hacking and the Design of Trustworthy ML for Signal Processing in C-UAS: A Scenario Based Method", "comment": "6 pages, Pre-publication. Copyright 2026 IEEE. Peer Reviewed. Accepted at ICASSP 2026 - 2026 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), scheduled for 3-8 May 2026 in Barcelona, Spain", "summary": "In order to counter the various threats manifested by Unmanned Aircraft Systems (UAS) adequately, specialized Counter Unmanned Aircraft Systems (C-UAS) are required. Enhancing C-UAS with Emerging and Disruptive Technologies (EDTs) such as Artificial Intelligence (AI) can lead to more effective countermeasures. In this paper a scenario-based method is applied to C-UAS augmented with Machine Learning (ML), a subset of AI, that can enhance signal processing capabilities. Via the scenarios-based method we frame in this paper probability hacking as a challenge and identify requirements which can be implemented in existing Rule of Law mechanisms to prevent probability hacking. These requirements strengthen the trustworthiness of the C-UAS, which feed into justified trust - a key to successful Human-Autonomy Teaming, in civil and military contexts. Index Terms: C-UAS, Scenario-based method, Emerging and Disruptive Technologies, Probability hacking, Trustworthiness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u57fa\u4e8e\u573a\u666f\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u548c\u9632\u6b62C-UAS\u7cfb\u7edf\u4e2d\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u589e\u5f3a\u4fe1\u53f7\u5904\u7406\u80fd\u529b\u65f6\u53ef\u80fd\u51fa\u73b0\u7684\u6982\u7387\u9ed1\u5ba2\u653b\u51fb\uff0c\u4ee5\u589e\u5f3a\u7cfb\u7edf\u53ef\u4fe1\u5ea6", "motivation": "\u4e3a\u4e86\u6709\u6548\u5e94\u5bf9\u65e0\u4eba\u673a\u7cfb\u7edf\u5e26\u6765\u7684\u5404\u79cd\u5a01\u80c1\uff0c\u9700\u8981\u4e13\u95e8\u7684C-UAS\u7cfb\u7edf\u3002\u901a\u8fc7\u4eba\u5de5\u667a\u80fd\u7b49\u65b0\u5174\u98a0\u8986\u6027\u6280\u672f\u589e\u5f3aC-UAS\u53ef\u4ee5\u5e26\u6765\u66f4\u6709\u6548\u7684\u5bf9\u6297\u63aa\u65bd\uff0c\u4f46\u9700\u8981\u786e\u4fdd\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6", "method": "\u91c7\u7528\u57fa\u4e8e\u573a\u666f\u7684\u65b9\u6cd5\uff0c\u5206\u6790C-UAS\u7cfb\u7edf\u4e2d\u673a\u5668\u5b66\u4e60\u589e\u5f3a\u4fe1\u53f7\u5904\u7406\u80fd\u529b\u65f6\u53ef\u80fd\u51fa\u73b0\u7684\u6982\u7387\u9ed1\u5ba2\u653b\u51fb\uff0c\u5e76\u8bc6\u522b\u53ef\u96c6\u6210\u5230\u73b0\u6709\u6cd5\u6cbb\u673a\u5236\u4e2d\u7684\u9700\u6c42", "result": "\u901a\u8fc7\u8be5\u65b9\u6cd5\u8bc6\u522b\u4e86\u9632\u6b62\u6982\u7387\u9ed1\u5ba2\u653b\u51fb\u7684\u9700\u6c42\uff0c\u8fd9\u4e9b\u9700\u6c42\u53ef\u4ee5\u589e\u5f3aC-UAS\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\uff0c\u4ece\u800c\u5efa\u7acb\u5408\u7406\u7684\u4fe1\u4efb\uff0c\u8fd9\u5bf9\u4e8e\u519b\u6c11\u9886\u57df\u7684\u4eba\u673a\u534f\u4f5c\u6210\u529f\u81f3\u5173\u91cd\u8981", "conclusion": "\u57fa\u4e8e\u573a\u666f\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc6\u522bC-UAS\u7cfb\u7edf\u4e2d\u673a\u5668\u5b66\u4e60\u589e\u5f3a\u4fe1\u53f7\u5904\u7406\u65f6\u7684\u6982\u7387\u9ed1\u5ba2\u653b\u51fb\u98ce\u9669\uff0c\u63d0\u51fa\u7684\u9700\u6c42\u53ef\u4ee5\u96c6\u6210\u5230\u6cd5\u6cbb\u673a\u5236\u4e2d\uff0c\u589e\u5f3a\u7cfb\u7edf\u53ef\u4fe1\u5ea6\uff0c\u4fc3\u8fdb\u6210\u529f\u7684\u4eba\u673a\u534f\u4f5c"}}
{"id": "2602.08088", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08088", "abs": "https://arxiv.org/abs/2602.08088", "authors": ["Mohammad Abu-Shaira", "Weishi Shi"], "title": "Online Domain-aware LLM Decoding for Continual Domain Evolution", "comment": null, "summary": "LLMs are typically fine-tuned offline on domain-specific data, assuming a static domain. In practice, domain knowledge evolves continuously through new regulations, products, services, and interaction patterns. Retraining or fine-tuning LLMs for every new instance is computationally infeasible. Additionally, real-world environments also exhibit temporal dynamics with shifting data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. This mismatch between evolving domains and static adaptation pipelines highlights the need for efficient, real-time adaptation without costly retraining. In response, we introduce Online Domain-aware Decoding framework (ODD). ODD performs probability-level fusion between a base LLM and a prefix-tree prior, guided by adaptive confidence modulation using disagreement and continuity signals. Empirical evaluation under diverse drift scenarios demonstrates that ODD consistently surpasses LLM-Greedy and LLM-Temp Scaled across all syntactic and semantic NLG metrics. It yields an absolute ROUGE-L gain of 0.065 and a 13.6% relative improvement in Cosine Similarity over the best baseline. These results demonstrate ODD 's robustness to evolving lexical and contextual patterns, making it suitable for dynamic LLM applications.", "AI": {"tldr": "ODD\u6846\u67b6\u901a\u8fc7\u5728\u7ebf\u6982\u7387\u878d\u5408\u548c\u81ea\u9002\u5e94\u7f6e\u4fe1\u5ea6\u8c03\u5236\uff0c\u4f7fLLM\u80fd\u591f\u5b9e\u65f6\u9002\u5e94\u4e0d\u65ad\u6f14\u53d8\u7684\u9886\u57df\u77e5\u8bc6\uff0c\u65e0\u9700\u6602\u8d35\u91cd\u8bad\u7ec3", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u9886\u57df\u77e5\u8bc6\u6301\u7eed\u6f14\u53d8\uff08\u65b0\u6cd5\u89c4\u3001\u4ea7\u54c1\u3001\u670d\u52a1\u7b49\uff09\uff0c\u800c\u4f20\u7edfLLM\u5fae\u8c03\u5047\u8bbe\u9759\u6001\u9886\u57df\uff0c\u91cd\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\u3002\u540c\u65f6\u5b58\u5728\u6982\u5ff5\u6f02\u79fb\u95ee\u9898\uff0c\u5ffd\u89c6\u8fd9\u4e9b\u52a8\u6001\u53d8\u5316\u4f1a\u663e\u8457\u964d\u4f4e\u6a21\u578b\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u5728\u7ebf\u9886\u57df\u611f\u77e5\u89e3\u7801\u6846\u67b6\uff08ODD\uff09\uff0c\u5728\u57fa\u7840LLM\u548c\u524d\u7f00\u6811\u5148\u9a8c\u4e4b\u95f4\u8fdb\u884c\u6982\u7387\u7ea7\u878d\u5408\uff0c\u4f7f\u7528\u5206\u6b67\u548c\u8fde\u7eed\u6027\u4fe1\u53f7\u6307\u5bfc\u81ea\u9002\u5e94\u7f6e\u4fe1\u5ea6\u8c03\u5236\u3002", "result": "\u5728\u591a\u79cd\u6f02\u79fb\u573a\u666f\u4e0b\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0cODD\u5728\u6240\u6709\u53e5\u6cd5\u548c\u8bed\u4e49NLG\u6307\u6807\u4e0a\u5747\u4f18\u4e8eLLM-Greedy\u548cLLM-Temp Scaled\u57fa\u7ebf\uff0c\u83b7\u5f970.065\u7684\u7edd\u5bf9ROUGE-L\u589e\u76ca\u548c13.6%\u7684\u76f8\u5bf9\u4f59\u5f26\u76f8\u4f3c\u5ea6\u63d0\u5347\u3002", "conclusion": "ODD\u5bf9\u4e0d\u65ad\u6f14\u53d8\u7684\u8bcd\u6c47\u548c\u4e0a\u4e0b\u6587\u6a21\u5f0f\u5177\u6709\u9c81\u68d2\u6027\uff0c\u9002\u5408\u52a8\u6001LLM\u5e94\u7528\uff0c\u89e3\u51b3\u4e86\u9886\u57df\u6f14\u53d8\u4e0e\u9759\u6001\u9002\u5e94\u7ba1\u9053\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u95ee\u9898\u3002"}}
{"id": "2602.08105", "categories": ["cs.LG", "physics.data-an", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08105", "abs": "https://arxiv.org/abs/2602.08105", "authors": ["Paarth Gulati", "Eslam Abdelaleem", "Audrey Sederberg", "Ilya Nemenman"], "title": "Mutual information and task-relevant latent dimensionality", "comment": null, "summary": "Estimating the dimensionality of the latent representation needed for prediction -- the task-relevant dimension -- is a difficult, largely unsolved problem with broad scientific applications. We cast it as an Information Bottleneck question: what embedding bottleneck dimension is sufficient to compress predictor and predicted views while preserving their mutual information (MI). This repurposes neural MI estimators for dimensionality estimation. We show that standard neural estimators with separable/bilinear critics systematically inflate the inferred dimension, and we address this by introducing a hybrid critic that retains an explicit dimensional bottleneck while allowing flexible nonlinear cross-view interactions, thereby preserving the latent geometry. We further propose a one-shot protocol that reads off the effective dimension from a single over-parameterized hybrid model, without sweeping over bottleneck sizes. We validate the approach on synthetic problems with known task-relevant dimension. We extend the approach to intrinsic dimensionality by constructing paired views of a single dataset, enabling comparison with classical geometric dimension estimators. In noisy regimes where those estimators degrade, our approach remains reliable. Finally, we demonstrate the utility of the method on multiple physics datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u7684\u7ef4\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u6279\u8bc4\u5668\u89e3\u51b3\u4f20\u7edf\u795e\u7ecf\u4e92\u4fe1\u606f\u4f30\u8ba1\u5668\u9ad8\u4f30\u7ef4\u5ea6\u7684\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u5355\u6b21\u4f30\u8ba1\u65e0\u9700\u626b\u63cf\u74f6\u9888\u5c3a\u5bf8\u3002", "motivation": "\u4f30\u8ba1\u9884\u6d4b\u6240\u9700\u7684\u6f5c\u5728\u8868\u793a\u7ef4\u5ea6\uff08\u4efb\u52a1\u76f8\u5173\u7ef4\u5ea6\uff09\u662f\u4e00\u4e2a\u56f0\u96be\u4e14\u672a\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u5177\u6709\u5e7f\u6cdb\u79d1\u5b66\u5e94\u7528\u4ef7\u503c\u3002\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u7ef4\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u5c06\u7ef4\u5ea6\u4f30\u8ba1\u8f6c\u5316\u4e3a\u4fe1\u606f\u74f6\u9888\u95ee\u9898\uff1a\u5bfb\u627e\u80fd\u591f\u538b\u7f29\u9884\u6d4b\u5668\u548c\u88ab\u9884\u6d4b\u89c6\u56fe\u540c\u65f6\u4fdd\u6301\u5176\u4e92\u4fe1\u606f\u7684\u6700\u5c0f\u5d4c\u5165\u74f6\u9888\u7ef4\u5ea6\u3002\u63d0\u51fa\u6df7\u5408\u6279\u8bc4\u5668\uff0c\u5728\u4fdd\u7559\u663e\u5f0f\u7ef4\u5ea6\u74f6\u9888\u7684\u540c\u65f6\u5141\u8bb8\u7075\u6d3b\u7684\u975e\u7ebf\u6027\u8de8\u89c6\u56fe\u4ea4\u4e92\u3002\u5f00\u53d1\u5355\u6b21\u4f30\u8ba1\u534f\u8bae\uff0c\u4ece\u5355\u4e2a\u8fc7\u53c2\u6570\u5316\u6df7\u5408\u6a21\u578b\u4e2d\u76f4\u63a5\u8bfb\u53d6\u6709\u6548\u7ef4\u5ea6\u3002", "result": "\u5728\u5df2\u77e5\u4efb\u52a1\u76f8\u5173\u7ef4\u5ea6\u7684\u5408\u6210\u95ee\u9898\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u6269\u5c55\u5230\u5185\u5728\u7ef4\u5ea6\u4f30\u8ba1\uff0c\u5728\u566a\u58f0\u73af\u5883\u4e0b\u6bd4\u4f20\u7edf\u51e0\u4f55\u7ef4\u5ea6\u4f30\u8ba1\u5668\u66f4\u53ef\u9760\u3002\u5728\u591a\u4e2a\u7269\u7406\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u7684\u7ef4\u5ea6\u4f30\u8ba1\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u795e\u7ecf\u4e92\u4fe1\u606f\u4f30\u8ba1\u5668\u9ad8\u4f30\u7ef4\u5ea6\u7684\u95ee\u9898\uff0c\u5728\u566a\u58f0\u73af\u5883\u4e0b\u4fdd\u6301\u53ef\u9760\u6027\uff0c\u4e3a\u79d1\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u7ef4\u5ea6\u4f30\u8ba1\u5de5\u5177\u3002"}}
{"id": "2602.08128", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08128", "abs": "https://arxiv.org/abs/2602.08128", "authors": ["Zahir Alsulaimawi"], "title": "Online Bayesian Imbalanced Learning with Bregman-Calibrated Deep Networks", "comment": null, "summary": "Class imbalance remains a fundamental challenge in machine learning, where standard classifiers exhibit severe performance degradation in minority classes. Although existing approaches address imbalance through resampling or cost-sensitive learning during training, they require retraining or access to labeled target data when class distributions shift at deployment time, a common occurrence in real-world applications such as fraud detection, medical diagnosis, and anomaly detection. We present \\textit{Online Bayesian Imbalanced Learning} (OBIL), a principled framework that decouples likelihood-ratio estimation from class-prior assumptions, enabling real-time adaptation to distribution shifts without model retraining. Our approach builds on the established connection between Bregman divergences and proper scoring rules to show that deep networks trained with such losses produce posterior probability estimates from which prior-invariant likelihood ratios can be extracted. We prove that these likelihood-ratio estimates remain valid under arbitrary changes in class priors and cost structures, requiring only a threshold adjustment for optimal Bayes decisions. We derive finite-sample regret bounds demonstrating that OBIL achieves $O(\\sqrt{T \\log T})$ regret against an oracle with perfect prior knowledge. Extensive experiments on benchmark datasets and medical diagnosis benchmarks under simulated deployment shifts demonstrate that OBIL maintains robust performance under severe distribution shifts, outperforming state-of-the-art methods in F1 Score when test distributions deviate significantly from the training conditions.", "AI": {"tldr": "OBIL\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u4f3c\u7136\u6bd4\u4f30\u8ba1\u4e0e\u7c7b\u522b\u5148\u9a8c\u5047\u8bbe\uff0c\u5b9e\u73b0\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u5b9e\u65f6\u9002\u5e94\u5206\u5e03\u504f\u79fb\u7684\u5728\u7ebf\u4e0d\u5e73\u8861\u5b66\u4e60\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\u7c7b\u522b\u5206\u5e03\u7ecf\u5e38\u5728\u90e8\u7f72\u65f6\u53d1\u751f\u53d8\u5316\uff08\u5982\u6b3a\u8bc8\u68c0\u6d4b\u3001\u533b\u7597\u8bca\u65ad\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6216\u8bbf\u95ee\u6807\u8bb0\u6570\u636e\uff0c\u65e0\u6cd5\u5b9e\u65f6\u9002\u5e94\u5206\u5e03\u504f\u79fb\u3002", "method": "\u57fa\u4e8eBregman\u6563\u5ea6\u4e0e\u9002\u5f53\u8bc4\u5206\u89c4\u5219\u7684\u5173\u8054\uff0c\u8bc1\u660e\u6df1\u5ea6\u7f51\u7edc\u8bad\u7ec3\u4ea7\u751f\u7684\u540e\u9a8c\u6982\u7387\u4f30\u8ba1\u53ef\u63d0\u53d6\u5148\u9a8c\u4e0d\u53d8\u7684\u4f3c\u7136\u6bd4\uff0c\u901a\u8fc7\u9608\u503c\u8c03\u6574\u5b9e\u73b0\u6700\u4f18\u8d1d\u53f6\u65af\u51b3\u7b56\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4f3c\u7136\u6bd4\u4f30\u8ba1\u5728\u4efb\u610f\u7c7b\u522b\u5148\u9a8c\u548c\u6210\u672c\u7ed3\u6784\u53d8\u5316\u4e0b\u4fdd\u6301\u6709\u6548\uff0c\u83b7\u5f97O(\u221aT log T)\u7684\u6709\u9650\u6837\u672c\u9057\u61be\u754c\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u4e25\u91cd\u5206\u5e03\u504f\u79fb\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "OBIL\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u5b9e\u65f6\u9002\u5e94\u5206\u5e03\u504f\u79fb\u7684\u5728\u7ebf\u4e0d\u5e73\u8861\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u7c7b\u522b\u5206\u5e03\u53d8\u5316\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2602.08142", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08142", "abs": "https://arxiv.org/abs/2602.08142", "authors": ["H. Martin Gillis", "Isaac Xu", "Thomas Trappenberg"], "title": "Variance-Gated Ensembles: An Epistemic-Aware Framework for Uncertainty Estimation", "comment": null, "summary": "Machine learning applications require fast and reliable per-sample uncertainty estimation. A common approach is to use predictive distributions from Bayesian or approximation methods and additively decompose uncertainty into aleatoric (i.e., data-related) and epistemic (i.e., model-related) components. However, additive decomposition has recently been questioned, with evidence that it breaks down when using finite-ensemble sampling and/or mismatched predictive distributions. This paper introduces Variance-Gated Ensembles (VGE), an intuitive, differentiable framework that injects epistemic sensitivity via a signal-to-noise gate computed from ensemble statistics. VGE provides: (i) a Variance-Gated Margin Uncertainty (VGMU) score that couples decision margins with ensemble predictive variance; and (ii) a Variance-Gated Normalization (VGN) layer that generalizes the variance-gated uncertainty mechanism to training via per-class, learnable normalization of ensemble member probabilities. We derive closed-form vector-Jacobian products enabling end-to-end training through ensemble sample mean and variance. VGE matches or exceeds state-of-the-art information-theoretic baselines while remaining computationally efficient. As a result, VGE provides a practical and scalable approach to epistemic-aware uncertainty estimation in ensemble models. An open-source implementation is available at: https://github.com/nextdevai/vge.", "AI": {"tldr": "VGE\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u65b9\u5dee\u95e8\u63a7\u673a\u5236\u5c06\u51b3\u7b56\u8fb9\u754c\u4e0e\u96c6\u6210\u9884\u6d4b\u65b9\u5dee\u8026\u5408\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u52a0\u6027\u5206\u89e3\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8d1d\u53f6\u65af\u6216\u8fd1\u4f3c\u65b9\u6cd5\u5c06\u4e0d\u786e\u5b9a\u6027\u52a0\u6027\u5206\u89e3\u4e3a\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5728\u4f7f\u7528\u6709\u9650\u96c6\u6210\u91c7\u6837\u548c/\u6216\u4e0d\u5339\u914d\u9884\u6d4b\u5206\u5e03\u65f6\u4f1a\u5931\u6548\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u65b9\u5dee\u95e8\u63a7\u96c6\u6210(VGE)\u6846\u67b6\uff1a1) \u65b9\u5dee\u95e8\u63a7\u8fb9\u7f18\u4e0d\u786e\u5b9a\u6027(VGMU)\u8bc4\u5206\uff0c\u5c06\u51b3\u7b56\u8fb9\u754c\u4e0e\u96c6\u6210\u9884\u6d4b\u65b9\u5dee\u8026\u5408\uff1b2) \u65b9\u5dee\u95e8\u63a7\u5f52\u4e00\u5316(VGN)\u5c42\uff0c\u901a\u8fc7\u6bcf\u7c7b\u53ef\u5b66\u4e60\u7684\u5f52\u4e00\u5316\u5c06\u65b9\u5dee\u95e8\u63a7\u673a\u5236\u6269\u5c55\u5230\u8bad\u7ec3\u4e2d\uff1b\u63a8\u5bfc\u95ed\u5f0f\u5411\u91cf-\u96c5\u53ef\u6bd4\u4e58\u79ef\u5b9e\u73b0\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "VGE\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u4fe1\u606f\u8bba\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u96c6\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u8ba4\u77e5\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\u3002", "conclusion": "VGE\u63d0\u4f9b\u4e86\u4e00\u79cd\u76f4\u89c2\u3001\u53ef\u5fae\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u96c6\u6210\u7edf\u8ba1\u7684\u4fe1\u566a\u6bd4\u95e8\u63a7\u6ce8\u5165\u8ba4\u77e5\u654f\u611f\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u52a0\u6027\u5206\u89e3\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002"}}
{"id": "2602.08145", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.08145", "abs": "https://arxiv.org/abs/2602.08145", "authors": ["Xinyu Yang", "Junlin Han", "Rishi Bommasani", "Jinqi Luo", "Wenjie Qu", "Wangchunshu Zhou", "Adel Bibi", "Xiyao Wang", "Jaehong Yoon", "Elias Stengel-Eskin", "Shengbang Tong", "Lingfeng Shen", "Rafael Rafailov", "Runjia Li", "Zhaoyang Wang", "Yiyang Zhou", "Chenhang Cui", "Yu Wang", "Wenhao Zheng", "Huichi Zhou", "Jindong Gu", "Zhaorun Chen", "Peng Xia", "Tony Lee", "Thomas Zollo", "Vikash Sehwag", "Jixuan Leng", "Jiuhai Chen", "Yuxin Wen", "Huan Zhang", "Zhun Deng", "Linjun Zhang", "Pavel Izmailov", "Pang Wei Koh", "Yulia Tsvetkov", "Andrew Wilson", "Jiaheng Zhang", "James Zou", "Cihang Xie", "Hao Wang", "Philip Torr", "Julian McAuley", "David Alvarez-Melis", "Florian Tram\u00e8r", "Kaidi Xu", "Suman Jana", "Chris Callison-Burch", "Rene Vidal", "Filippos Kokkinos", "Mohit Bansal", "Beidi Chen", "Huaxiu Yao"], "title": "Reliable and Responsible Foundation Models: A Comprehensive Survey", "comment": "TMLR camera-ready version", "summary": "Foundation models, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models, have become essential tools with broad applications across various domains such as law, medicine, education, finance, science, and beyond. As these models see increasing real-world deployment, ensuring their reliability and responsibility has become critical for academia, industry, and government. This survey addresses the reliable and responsible development of foundation models. We explore critical issues, including bias and fairness, security and privacy, uncertainty, explainability, and distribution shift. Our research also covers model limitations, such as hallucinations, as well as methods like alignment and Artificial Intelligence-Generated Content (AIGC) detection. For each area, we review the current state of the field and outline concrete future research directions. Additionally, we discuss the intersections between these areas, highlighting their connections and shared challenges. We hope our survey fosters the development of foundation models that are not only powerful but also ethical, trustworthy, reliable, and socially responsible.", "AI": {"tldr": "\u8be5\u8c03\u67e5\u8bba\u6587\u7cfb\u7edf\u6027\u5730\u63a2\u8ba8\u4e86\u57fa\u7840\u6a21\u578b\u7684\u53ef\u9760\u6027\u4e0e\u8d23\u4efb\u6027\u53d1\u5c55\uff0c\u6db5\u76d6\u504f\u89c1\u516c\u5e73\u3001\u5b89\u5168\u9690\u79c1\u3001\u4e0d\u786e\u5b9a\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3001\u5206\u5e03\u504f\u79fb\u7b49\u5173\u952e\u95ee\u9898\uff0c\u4ee5\u53ca\u5e7b\u89c9\u3001\u5bf9\u9f50\u3001AIGC\u68c0\u6d4b\u7b49\u65b9\u6cd5\u548c\u6311\u6218\u3002", "motivation": "\u968f\u7740\u57fa\u7840\u6a21\u578b\uff08LLMs\u3001MLLMs\u3001\u56fe\u50cf\u751f\u6210\u6a21\u578b\u3001\u89c6\u9891\u751f\u6210\u6a21\u578b\uff09\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u786e\u4fdd\u5176\u53ef\u9760\u6027\u548c\u8d23\u4efb\u6027\u5df2\u6210\u4e3a\u5b66\u672f\u754c\u3001\u5de5\u4e1a\u754c\u548c\u653f\u5e9c\u7684\u5173\u952e\u9700\u6c42\u3002\u8fd9\u4e9b\u6a21\u578b\u5728\u533b\u7597\u3001\u6cd5\u5f8b\u3001\u6559\u80b2\u3001\u91d1\u878d\u3001\u79d1\u5b66\u7b49\u5173\u952e\u9886\u57df\u7684\u90e8\u7f72\uff0c\u8981\u6c42\u5b83\u4eec\u4e0d\u4ec5\u662f\u5f3a\u5927\u7684\uff0c\u8fd8\u8981\u662f\u4f26\u7406\u7684\u3001\u53ef\u4fe1\u8d56\u7684\u548c\u793e\u4f1a\u8d23\u4efb\u7684\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u8c03\u67e5\u65b9\u6cd5\uff0c\u5bf9\u57fa\u7840\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u8d23\u4efb\u6027\u53d1\u5c55\u8fdb\u884c\u5168\u9762\u68b3\u7406\u3002\u9488\u5bf9\u6bcf\u4e2a\u5173\u952e\u9886\u57df\uff08\u504f\u89c1\u4e0e\u516c\u5e73\u3001\u5b89\u5168\u4e0e\u9690\u79c1\u3001\u4e0d\u786e\u5b9a\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3001\u5206\u5e03\u504f\u79fb\uff09\uff0c\u56de\u987e\u5f53\u524d\u7814\u7a76\u73b0\u72b6\uff0c\u5e76\u89c4\u5212\u5177\u4f53\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002\u540c\u65f6\u63a2\u8ba8\u8fd9\u4e9b\u9886\u57df\u4e4b\u95f4\u7684\u4ea4\u53c9\u5173\u7cfb\u548c\u5171\u4eab\u6311\u6218\u3002", "result": "\u8be5\u8c03\u67e5\u63d0\u4f9b\u4e86\u57fa\u7840\u6a21\u578b\u53ef\u9760\u6027\u4e0e\u8d23\u4efb\u6027\u7814\u7a76\u7684\u5168\u9762\u6982\u89c8\uff0c\u8bc6\u522b\u4e86\u5f53\u524d\u7814\u7a76\u7684\u5173\u952e\u95ee\u9898\u548c\u6311\u6218\uff0c\u5305\u62ec\u6a21\u578b\u5c40\u9650\u6027\uff08\u5982\u5e7b\u89c9\uff09\u3001\u5bf9\u9f50\u65b9\u6cd5\u3001AIGC\u68c0\u6d4b\u7b49\u3002\u4e3a\u6bcf\u4e2a\u7814\u7a76\u9886\u57df\u63d0\u51fa\u4e86\u5177\u4f53\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5e76\u5f3a\u8c03\u4e86\u8de8\u9886\u57df\u95ee\u9898\u7684\u76f8\u4e92\u5173\u8054\u6027\u3002", "conclusion": "\u8be5\u8c03\u67e5\u65e8\u5728\u4fc3\u8fdb\u57fa\u7840\u6a21\u578b\u7684\u8d1f\u8d23\u4efb\u53d1\u5c55\uff0c\u4f7f\u5176\u4e0d\u4ec5\u662f\u5f3a\u5927\u7684\u6280\u672f\u5de5\u5177\uff0c\u66f4\u662f\u4f26\u7406\u7684\u3001\u53ef\u4fe1\u8d56\u7684\u3001\u53ef\u9760\u7684\u548c\u5177\u6709\u793e\u4f1a\u8d23\u4efb\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u3002\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u68b3\u7406\u73b0\u6709\u7814\u7a76\u548c\u672a\u6765\u65b9\u5411\uff0c\u4e3a\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u53c2\u8003\u6846\u67b6\uff0c\u4ee5\u63a8\u52a8\u57fa\u7840\u6a21\u578b\u5411\u66f4\u5b89\u5168\u3001\u66f4\u516c\u5e73\u3001\u66f4\u900f\u660e\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2602.08151", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08151", "abs": "https://arxiv.org/abs/2602.08151", "authors": ["Yoav Freund", "Nicholas J. A. Harvey", "Victor S. Portella", "Yabing Qi", "Yu-Xiang Wang"], "title": "A second order regret bound for NormalHedge", "comment": null, "summary": "We consider the problem of prediction with expert advice for ``easy'' sequences. We show that a variant of NormalHedge enjoys a second-order $\u03b5$-quantile regret bound of $O\\big(\\sqrt{V_T \\log(V_T/\u03b5)}\\big) $ when $V_T > \\log N$, where $V_T$ is the cumulative second moment of instantaneous per-expert regret averaged with respect to a natural distribution determined by the algorithm. The algorithm is motivated by a continuous time limit using Stochastic Differential Equations. The discrete time analysis uses self-concordance techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cdNormalHedge\u53d8\u4f53\uff0c\u5bf9\"\u7b80\u5355\"\u5e8f\u5217\u5b9e\u73b0\u4e8c\u9636\u03b5-\u5206\u4f4d\u6570\u9057\u61be\u754cO(\u221a(V_T log(V_T/\u03b5)))\uff0c\u5176\u4e2dV_T\u662f\u7b97\u6cd5\u786e\u5b9a\u7684\u81ea\u7136\u5206\u5e03\u4e0b\u7684\u77ac\u65f6\u4e13\u5bb6\u9057\u61be\u7d2f\u79ef\u4e8c\u9636\u77e9", "motivation": "\u7814\u7a76\"\u7b80\u5355\"\u5e8f\u5217\u7684\u4e13\u5bb6\u5efa\u8bae\u9884\u6d4b\u95ee\u9898\uff0c\u65e8\u5728\u4e3a\u8fd9\u7c7b\u5e8f\u5217\u8bbe\u8ba1\u5177\u6709\u66f4\u597d\u9057\u61be\u754c\u7684\u7b97\u6cd5", "method": "\u63d0\u51faNormalHedge\u7b97\u6cd5\u7684\u53d8\u4f53\uff0c\u901a\u8fc7\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u8fde\u7eed\u65f6\u95f4\u6781\u9650\u8fdb\u884c\u52a8\u673a\u63a8\u5bfc\uff0c\u79bb\u6563\u65f6\u95f4\u5206\u6790\u4f7f\u7528\u81ea\u534f\u8c03\u6280\u672f", "result": "\u5f53V_T > log N\u65f6\uff0c\u7b97\u6cd5\u83b7\u5f97\u4e8c\u9636\u03b5-\u5206\u4f4d\u6570\u9057\u61be\u754cO(\u221a(V_T log(V_T/\u03b5)))\uff0c\u5176\u4e2dV_T\u662f\u7b97\u6cd5\u786e\u5b9a\u7684\u81ea\u7136\u5206\u5e03\u4e0b\u7684\u77ac\u65f6\u4e13\u5bb6\u9057\u61be\u7d2f\u79ef\u4e8c\u9636\u77e9", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\"\u7b80\u5355\"\u5e8f\u5217\u63d0\u4f9b\u4e86\u6539\u8fdb\u7684\u9057\u61be\u4fdd\u8bc1\uff0c\u7ed3\u5408\u4e86\u8fde\u7eed\u65f6\u95f4\u6781\u9650\u548c\u81ea\u534f\u8c03\u5206\u6790\u6280\u672f"}}
{"id": "2602.08159", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08159", "abs": "https://arxiv.org/abs/2602.08159", "authors": ["Seonglae Cho", "Zekun Wu", "Kleyton Da Costa", "Adriano Koshiyama"], "title": "The Confidence Manifold: Geometric Structure of Correctness Representations in Language Models", "comment": null, "summary": "When a language model asserts that \"the capital of Australia is Sydney,\" does it know this is wrong? We characterize the geometry of correctness representations across 9 models from 5 architecture families. The structure is simple: the discriminative signal occupies 3-8 dimensions, performance degrades with additional dimensions, and no nonlinear classifier improves over linear separation. Centroid distance in the low-dimensional subspace matches trained probe performance (0.90 AUC), enabling few-shot detection: on GPT-2, 25 labeled examples achieve 89% of full-data accuracy. We validate causally through activation steering: the learned direction produces 10.9 percentage point changes in error rates while random directions show no effect. Internal probes achieve 0.80-0.97 AUC; output-based methods (P(True), semantic entropy) achieve only 0.44-0.64 AUC. The correctness signal exists internally but is not expressed in outputs. That centroid distance matches probe performance indicates class separation is a mean shift, making detection geometric rather than learned.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u5b58\u5728\u7b80\u5355\u7684\u51e0\u4f55\u7ed3\u6784\u6765\u8868\u793a\u6b63\u786e\u6027\uff0c\u4ec5\u97003-8\u4e2a\u7ef4\u5ea6\u5c31\u80fd\u6709\u6548\u533a\u5206\u6b63\u786e\u4e0e\u9519\u8bef\u9648\u8ff0\uff0c\u4e14\u7ebf\u6027\u5206\u7c7b\u5668\u8db3\u591f\uff0c\u65e0\u9700\u975e\u7ebf\u6027\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7a76\u8bed\u8a00\u6a21\u578b\u662f\u5426\u77e5\u9053\u81ea\u5df1\u9648\u8ff0\u7684\u9519\u8bef\u6027\uff0c\u4ee5\u53ca\u6b63\u786e\u6027\u4fe1\u606f\u5728\u6a21\u578b\u5185\u90e8\u5982\u4f55\u8868\u793a\u548c\u68c0\u6d4b\u3002", "method": "\u5206\u67909\u4e2a\u4e0d\u540c\u67b6\u6784\u6a21\u578b\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u4f7f\u7528\u7ebf\u6027\u63a2\u9488\u3001\u6fc0\u6d3b\u5bfc\u5411\u548c\u8d28\u5fc3\u8ddd\u79bb\u7b49\u65b9\u6cd5\uff0c\u6bd4\u8f83\u5185\u90e8\u63a2\u6d4b\u4e0e\u57fa\u4e8e\u8f93\u51fa\u7684\u65b9\u6cd5\u3002", "result": "\u6b63\u786e\u6027\u4fe1\u53f7\u4ec5\u97003-8\u4e2a\u7ef4\u5ea6\uff0c\u7ebf\u6027\u5206\u7c7b\u5668\u8db3\u591f\uff1b\u5185\u90e8\u63a2\u9488AUC\u8fbe0.80-0.97\uff0c\u800c\u57fa\u4e8e\u8f93\u51fa\u7684\u65b9\u6cd5\u4ec50.44-0.64\uff1b\u6fc0\u6d3b\u5bfc\u5411\u80fd\u663e\u8457\u6539\u53d8\u9519\u8bef\u7387\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u5b58\u5728\u660e\u786e\u7684\u6b63\u786e\u5b9a\u6027\u8868\u793a\uff0c\u4f46\u8be5\u4fe1\u606f\u672a\u5728\u8f93\u51fa\u4e2d\u8868\u8fbe\uff1b\u6b63\u786e\u6027\u68c0\u6d4b\u672c\u8d28\u4e0a\u662f\u51e0\u4f55\u95ee\u9898\u800c\u975e\u5b66\u4e60\u95ee\u9898\u3002"}}
{"id": "2602.08169", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08169", "abs": "https://arxiv.org/abs/2602.08169", "authors": ["Zejia You", "Chunyuan Deng", "Hanjie Chen"], "title": "Spherical Steering: Geometry-Aware Activation Rotation for Language Models", "comment": "The code is at: https://github.com/chili-lab/Spherical-Steering", "summary": "Inference-time steering has emerged as a promising paradigm for controlling language models (LMs) without the cost of retraining. However, standard approaches typically rely on activation addition, a geometric operation that inevitably alters the magnitude of hidden representations. This raises concerns about representation collapse and degradation of open-ended generation capabilities. In this work, we explore Spherical Steering, a training-free primitive that resolves this trade-off through activation rotation. Rather than shifting activations with a fixed vector, our method rotates them along a geodesic toward a target direction, guiding the activation toward the target concept while preserving the integrity of the signal. To further enhance adaptivity, we incorporate a confidence gate that dynamically modulates steering strength based on input uncertainty. Extensive experiments across multiple-choice benchmarks demonstrate that Spherical Steering significantly outperforms addition-based baselines (notably by +10% on TruthfulQA, COPA, and Storycloze), while simultaneously maintaining the model's general open-ended generation quality. This work highlights the value of geometric consistency, suggesting that norm-preserving rotation is a robust and effective primitive for precise inference-time control.", "AI": {"tldr": "\u63d0\u51faSpherical Steering\u65b9\u6cd5\uff0c\u901a\u8fc7\u6fc0\u6d3b\u65cb\u8f6c\u800c\u975e\u52a0\u6cd5\u6765\u5b9e\u73b0\u63a8\u7406\u65f6\u63a7\u5236\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u5347\u591a\u9879\u4efb\u52a1\u6027\u80fd", "motivation": "\u73b0\u6709\u63a8\u7406\u65f6\u63a7\u5236\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u6fc0\u6d3b\u52a0\u6cd5\uff0c\u8fd9\u4f1a\u6539\u53d8\u9690\u85cf\u8868\u793a\u7684\u5e45\u5ea6\uff0c\u53ef\u80fd\u5bfc\u81f4\u8868\u793a\u5d29\u6e83\u548c\u5f00\u653e\u5f0f\u751f\u6210\u80fd\u529b\u4e0b\u964d", "method": "\u63d0\u51fa\u7403\u9762\u5bfc\u5411\u65b9\u6cd5\uff0c\u901a\u8fc7\u6cbf\u6d4b\u5730\u7ebf\u65cb\u8f6c\u6fc0\u6d3b\u5411\u91cf\u6765\u5f15\u5bfc\u6a21\u578b\uff0c\u540c\u65f6\u5f15\u5165\u7f6e\u4fe1\u95e8\u63a7\u673a\u5236\u6839\u636e\u8f93\u5165\u4e0d\u786e\u5b9a\u6027\u52a8\u6001\u8c03\u6574\u5bfc\u5411\u5f3a\u5ea6", "result": "\u5728\u591a\u9879\u9009\u62e9\u9898\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u52a0\u6cd5\u7684\u65b9\u6cd5\uff08\u5728TruthfulQA\u3001COPA\u548cStorycloze\u4e0a\u63d0\u5347+10%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u5f00\u653e\u5f0f\u751f\u6210\u8d28\u91cf", "conclusion": "\u4fdd\u6301\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u89c4\u8303\u4fdd\u6301\u65cb\u8f6c\u662f\u7cbe\u786e\u63a8\u7406\u65f6\u63a7\u5236\u7684\u9c81\u68d2\u6709\u6548\u539f\u8bed"}}
{"id": "2602.08171", "categories": ["cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.08171", "abs": "https://arxiv.org/abs/2602.08171", "authors": ["Cristian Minoccheri", "Sophia Tesic", "Kayvan Najarian", "Ryan Stidham"], "title": "A Causal Machine Learning Framework for Treatment Personalization in Clinical Trials: Application to Ulcerative Colitis", "comment": null, "summary": "Randomized controlled trials estimate average treatment effects, but treatment response heterogeneity motivates personalized approaches. A critical question is whether statistically detectable heterogeneity translates into improved treatment decisions -- these are distinct questions that can yield contradictory answers. We present a modular causal machine learning framework that evaluates each question separately: permutation importance identifies which features predict heterogeneity, best linear predictor (BLP) testing assesses statistical significance, and doubly robust policy evaluation measures whether acting on the heterogeneity improves patient outcomes. We apply this framework to patient-level data from the UNIFI maintenance trial of ustekinumab in ulcerative colitis, comparing placebo, standard-dose ustekinumab every 12 weeks, and dose-intensified ustekinumab every 8 weeks, using cross-fitted X-learner models with baseline demographics, medication history, week-8 clinical scores, laboratory biomarkers, and video-derived endoscopic features. BLP testing identified strong associations between endoscopic features and treatment effect heterogeneity for ustekinumab versus placebo, yet doubly robust policy evaluation showed no improvement in expected remission from incorporating endoscopic features, and out-of-fold multi-arm evaluation showed worse performance. Diagnostic comparison of prognostic contribution against policy value revealed that endoscopic scores behaved as disease severity markers -- improving outcome prediction in untreated patients but adding noise to treatment selection -- while clinical variables (fecal calprotectin, age, CRP) captured the decision-relevant variation. These results demonstrate that causal machine learning applications to clinical trials should include policy-level evaluation alongside heterogeneity testing.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u56e0\u679c\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u533a\u5206\u7edf\u8ba1\u4e0a\u53ef\u68c0\u6d4b\u7684\u6cbb\u7597\u5f02\u8d28\u6027\u4e0e\u5b9e\u9645\u6539\u5584\u6cbb\u7597\u51b3\u7b56\u7684\u80fd\u529b\uff0c\u5e76\u5728\u6e83\u75a1\u6027\u7ed3\u80a0\u708e\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u5e94\u7528\uff0c\u53d1\u73b0\u5185\u955c\u7279\u5f81\u867d\u80fd\u9884\u6d4b\u5f02\u8d28\u6027\u4f46\u4e0d\u80fd\u6539\u5584\u6cbb\u7597\u9009\u62e9\u3002", "motivation": "\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\u901a\u5e38\u4f30\u8ba1\u5e73\u5747\u6cbb\u7597\u6548\u679c\uff0c\u4f46\u6cbb\u7597\u53cd\u5e94\u5b58\u5728\u5f02\u8d28\u6027\uff0c\u8fd9\u4fc3\u4f7f\u4e86\u4e2a\u6027\u5316\u6cbb\u7597\u7684\u9700\u6c42\u3002\u7136\u800c\uff0c\u7edf\u8ba1\u4e0a\u53ef\u68c0\u6d4b\u7684\u5f02\u8d28\u6027\u662f\u5426\u771f\u6b63\u80fd\u8f6c\u5316\u4e3a\u6539\u5584\u7684\u6cbb\u7597\u51b3\u7b56\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u8fd9\u4e24\u4e2a\u95ee\u9898\u53ef\u80fd\u4ea7\u751f\u77db\u76fe\u7684\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u6a21\u5757\u5316\u56e0\u679c\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff1a1) \u7f6e\u6362\u91cd\u8981\u6027\u8bc6\u522b\u9884\u6d4b\u5f02\u8d28\u6027\u7684\u7279\u5f81\uff1b2) \u6700\u4f73\u7ebf\u6027\u9884\u6d4b\u5668(BLP)\u6d4b\u8bd5\u8bc4\u4f30\u7edf\u8ba1\u663e\u8457\u6027\uff1b3) \u53cc\u91cd\u7a33\u5065\u7b56\u7565\u8bc4\u4f30\u8861\u91cf\u5229\u7528\u5f02\u8d28\u6027\u662f\u5426\u6539\u5584\u60a3\u8005\u7ed3\u5c40\u3002\u5e94\u7528\u4e8eUNIFI\u7ef4\u6301\u8bd5\u9a8c\u6570\u636e\uff0c\u6bd4\u8f83\u5b89\u6170\u5242\u3001\u6807\u51c6\u5242\u91cf\u4e4c\u53f8\u5974\u5355\u6297\u548c\u5242\u91cf\u5f3a\u5316\u4e4c\u53f8\u5974\u5355\u6297\uff0c\u4f7f\u7528\u4ea4\u53c9\u62df\u5408X-learner\u6a21\u578b\u5206\u6790\u57fa\u7ebf\u7279\u5f81\u3002", "result": "BLP\u6d4b\u8bd5\u53d1\u73b0\u5185\u955c\u7279\u5f81\u4e0e\u4e4c\u53f8\u5974\u5355\u6297vs\u5b89\u6170\u5242\u7684\u6cbb\u7597\u6548\u679c\u5f02\u8d28\u6027\u6709\u5f3a\u5173\u8054\uff0c\u4f46\u53cc\u91cd\u7a33\u5065\u7b56\u7565\u8bc4\u4f30\u663e\u793a\u7eb3\u5165\u5185\u955c\u7279\u5f81\u672a\u80fd\u6539\u5584\u9884\u671f\u7f13\u89e3\u7387\uff0c\u4e14\u591a\u81c2\u8bc4\u4f30\u8868\u73b0\u66f4\u5dee\u3002\u5185\u955c\u8bc4\u5206\u4f5c\u4e3a\u75be\u75c5\u4e25\u91cd\u7a0b\u5ea6\u6807\u5fd7\u7269\uff0c\u80fd\u6539\u5584\u672a\u6cbb\u7597\u60a3\u8005\u7684\u7ed3\u5c40\u9884\u6d4b\uff0c\u4f46\u7ed9\u6cbb\u7597\u9009\u62e9\u589e\u52a0\u4e86\u566a\u58f0\uff0c\u800c\u4e34\u5e8a\u53d8\u91cf\uff08\u7caa\u4fbf\u9499\u536b\u86cb\u767d\u3001\u5e74\u9f84\u3001CRP\uff09\u6355\u6349\u4e86\u51b3\u7b56\u76f8\u5173\u7684\u53d8\u5f02\u3002", "conclusion": "\u56e0\u679c\u673a\u5668\u5b66\u4e60\u5728\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u7684\u5e94\u7528\u5e94\u540c\u65f6\u5305\u542b\u7b56\u7565\u5c42\u9762\u8bc4\u4f30\u548c\u5f02\u8d28\u6027\u6d4b\u8bd5\uff0c\u56e0\u4e3a\u7edf\u8ba1\u4e0a\u663e\u8457\u7684\u5f02\u8d28\u6027\u4e0d\u4e00\u5b9a\u80fd\u8f6c\u5316\u4e3a\u66f4\u597d\u7684\u6cbb\u7597\u51b3\u7b56\uff0c\u9700\u8981\u533a\u5206\u9884\u6d4b\u5f02\u8d28\u6027\u7684\u7279\u5f81\u4e0e\u6539\u5584\u51b3\u7b56\u7684\u7279\u5f81\u3002"}}
{"id": "2602.08182", "categories": ["cs.LG", "q-fin.CP", "q-fin.ST"], "pdf": "https://arxiv.org/pdf/2602.08182", "abs": "https://arxiv.org/abs/2602.08182", "authors": ["Hiromu Ozai", "Kei Nakagawa"], "title": "Nansde-net: A neural sde framework for generating time series with memory", "comment": "PAKDD2026 Accepted", "summary": "Modeling time series with long- or short-memory characteristics is a fundamental challenge in many scientific and engineering domains. While fractional Brownian motion has been widely used as a noise source to capture such memory effects, its incompatibility with It\u00f4 calculus limits its applicability in neural stochastic differential equation~(SDE) frameworks. In this paper, we propose a novel class of noise, termed Neural Network-kernel ARMA-type noise~(NA-noise), which is an It\u00f4-process-based alternative capable of capturing both long- and short-memory behaviors. The kernel function defining the noise structure is parameterized via neural networks and decomposed into a product form to preserve the Markov property. Based on this noise process, we develop NANSDE-Net, a generative model that extends Neural SDEs by incorporating NA-noise. We prove the theoretical existence and uniqueness of the solution under mild conditions and derive an efficient backpropagation scheme for training. Empirical results on both synthetic and real-world datasets demonstrate that NANSDE-Net matches or outperforms existing models, including fractional SDE-Net, in reproducing long- and short-memory features of the data, while maintaining computational tractability within the It\u00f4 calculus framework.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eIt\u00f4\u8fc7\u7a0b\u7684\u795e\u7ecf\u566a\u58f0\uff08NA-noise\uff09\uff0c\u80fd\u591f\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u7684\u957f\u77ed\u671f\u8bb0\u5fc6\u7279\u6027\uff0c\u5e76\u6784\u5efaNANSDE-Net\u751f\u6210\u6a21\u578b\uff0c\u5728\u4fdd\u6301It\u00f4\u8ba1\u7b97\u6846\u67b6\u7684\u540c\u65f6\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5206\u6570\u5e03\u6717\u8fd0\u52a8\u867d\u7136\u80fd\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u7684\u957f\u77ed\u671f\u8bb0\u5fc6\u7279\u6027\uff0c\u4f46\u4e0eIt\u00f4\u8ba1\u7b97\u4e0d\u517c\u5bb9\uff0c\u9650\u5236\u4e86\u5176\u5728\u795e\u7ecf\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u6846\u67b6\u4e2d\u7684\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301It\u00f4\u8ba1\u7b97\u517c\u5bb9\u6027\u53c8\u80fd\u6355\u6349\u8bb0\u5fc6\u7279\u6027\u7684\u66ff\u4ee3\u566a\u58f0\u6e90\u3002", "method": "\u63d0\u51fa\u795e\u7ecf\u7f51\u7edc\u6838ARMA\u578b\u566a\u58f0\uff08NA-noise\uff09\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316\u6838\u51fd\u6570\uff0c\u5e76\u5206\u89e3\u4e3a\u4e58\u79ef\u5f62\u5f0f\u4ee5\u4fdd\u6301\u9a6c\u5c14\u53ef\u592b\u6027\u8d28\u3002\u57fa\u4e8e\u6b64\u566a\u58f0\u6784\u5efaNANSDE-Net\u751f\u6210\u6a21\u578b\uff0c\u6269\u5c55\u795e\u7ecfSDEs\uff0c\u5e76\u8bc1\u660e\u89e3\u7684\u5b58\u5728\u552f\u4e00\u6027\uff0c\u63a8\u5bfc\u9ad8\u6548\u7684\u53cd\u5411\u4f20\u64ad\u8bad\u7ec3\u65b9\u6848\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNANSDE-Net\u5728\u518d\u73b0\u6570\u636e\u7684\u957f\u77ed\u671f\u8bb0\u5fc6\u7279\u5f81\u65b9\u9762\u5339\u914d\u6216\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff08\u5305\u62ec\u5206\u6570SDE-Net\uff09\uff0c\u540c\u65f6\u5728It\u00f4\u8ba1\u7b97\u6846\u67b6\u5185\u4fdd\u6301\u8ba1\u7b97\u53ef\u884c\u6027\u3002", "conclusion": "NA-noise\u4e3a\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u8bb0\u5fc6\u7279\u6027\u63d0\u4f9b\u4e86\u4e0eIt\u00f4\u8ba1\u7b97\u517c\u5bb9\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0cNANSDE-Net\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u795e\u7ecfSDE\u6846\u67b6\u4e2d\u7684\u8bb0\u5fc6\u5efa\u6a21\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.08194", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08194", "abs": "https://arxiv.org/abs/2602.08194", "authors": ["Konstantinos Mitsides", "Maxence Faldor", "Antoine Cully"], "title": "Dreaming in Code for Curriculum Learning in Open-Ended Worlds", "comment": "11 pages (main text), 90 pages total. Project page: https://konstantinosmitsides.github.io/dreaming-in-code", "summary": "Open-ended learning frames intelligence as emerging from continual interaction with an ever-expanding space of environments. While recent advances have utilized foundation models to programmatically generate diverse environments, these approaches often focus on discovering isolated behaviors rather than orchestrating sustained progression. In complex open-ended worlds, the large combinatorial space of possible challenges makes it difficult for agents to discover sequences of experiences that remain consistently learnable. To address this, we propose Dreaming in Code (DiCode), a framework in which foundation models synthesize executable environment code to scaffold learning toward increasing competence. In DiCode, \"dreaming\" takes the form of materializing code-level variations of the world. We instantiate DiCode in Craftax, a challenging open-ended benchmark characterized by rich mechanics and long-horizon progression. Empirically, DiCode enables agents to acquire long-horizon skills, achieving a $16\\%$ improvement in mean return over the strongest baseline and non-zero success on late-game combat tasks where prior methods fail. Our results suggest that code-level environment design provides a practical mechanism for curriculum control, enabling the construction of intermediate environments that bridge competence gaps in open-ended worlds. Project page and source code are available at https://konstantinosmitsides.github.io/dreaming-in-code and https://github.com/konstantinosmitsides/dreaming-in-code.", "AI": {"tldr": "DiCode\u6846\u67b6\u5229\u7528\u57fa\u7840\u6a21\u578b\u751f\u6210\u53ef\u6267\u884c\u73af\u5883\u4ee3\u7801\uff0c\u901a\u8fc7\u4ee3\u7801\u7ea7\u4e16\u754c\u53d8\u5316\u6784\u5efa\u5b66\u4e60\u811a\u624b\u67b6\uff0c\u5728\u5f00\u653e\u4e16\u754c\u5b66\u4e60\u4e2d\u5b9e\u73b0\u6301\u7eed\u80fd\u529b\u63d0\u5347", "motivation": "\u5f00\u653e\u4e16\u754c\u5b66\u4e60\u9762\u4e34\u6311\u6218\u7a7a\u95f4\u7ec4\u5408\u7206\u70b8\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u53d1\u73b0\u6301\u7eed\u53ef\u5b66\u4e60\u7684\u7ecf\u9a8c\u5e8f\u5217\uff0c\u9700\u8981\u6784\u5efa\u4e2d\u95f4\u73af\u5883\u6765\u5f25\u5408\u80fd\u529b\u5dee\u8ddd", "method": "\u63d0\u51faDreaming in Code\u6846\u67b6\uff0c\u8ba9\u57fa\u7840\u6a21\u578b\u5408\u6210\u53ef\u6267\u884c\u73af\u5883\u4ee3\u7801\uff0c\u901a\u8fc7\u4ee3\u7801\u7ea7\u4e16\u754c\u53d8\u5316\u4e3a\u5b66\u4e60\u63d0\u4f9b\u811a\u624b\u67b6\uff0c\u5728Craftax\u57fa\u51c6\u4e0a\u5b9e\u4f8b\u5316", "result": "\u5728Craftax\u57fa\u51c6\u4e0a\u5b9e\u73b016%\u7684\u5e73\u5747\u56de\u62a5\u63d0\u5347\uff0c\u5728\u540e\u671f\u6218\u6597\u4efb\u52a1\u4e0a\u53d6\u5f97\u975e\u96f6\u6210\u529f\u7387\uff08\u5148\u524d\u65b9\u6cd5\u5b8c\u5168\u5931\u8d25\uff09\uff0c\u8bc1\u660e\u4ee3\u7801\u7ea7\u73af\u5883\u8bbe\u8ba1\u80fd\u6709\u6548\u63a7\u5236\u8bfe\u7a0b", "conclusion": "\u4ee3\u7801\u7ea7\u73af\u5883\u8bbe\u8ba1\u4e3a\u5f00\u653e\u4e16\u754c\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8bfe\u7a0b\u63a7\u5236\u673a\u5236\uff0c\u80fd\u591f\u6784\u5efa\u4e2d\u95f4\u73af\u5883\u6765\u5f25\u5408\u80fd\u529b\u5dee\u8ddd\uff0c\u652f\u6301\u957f\u671f\u6280\u80fd\u83b7\u53d6"}}
{"id": "2602.08197", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08197", "abs": "https://arxiv.org/abs/2602.08197", "authors": ["Shingo Higashiguchi", "Koki Kawabata", "Yasuko Matsubara", "Yasushi Sakurai"], "title": "Interpretable Dynamic Network Modeling of Tensor Time Series via Kronecker Time-Varying Graphical Lasso", "comment": "Accepted at ACM Web Conference 2026 (WWW2026)", "summary": "With the rapid development of web services, large amounts of time series data are generated and accumulated across various domains such as finance, healthcare, and online platforms. As such data often co-evolves with multiple variables interacting with each other, estimating the time-varying dependencies between variables (i.e., the dynamic network structure) has become crucial for accurate modeling. However, real-world data is often represented as tensor time series with multiple modes, resulting in large, entangled networks that are hard to interpret and computationally intensive to estimate. In this paper, we propose Kronecker Time-Varying Graphical Lasso (KTVGL), a method designed for modeling tensor time series. Our approach estimates mode-specific dynamic networks in a Kronecker product form, thereby avoiding overly complex entangled structures and producing interpretable modeling results. Moreover, the partitioned network structure prevents the exponential growth of computational time with data dimension. In addition, our method can be extended to stream algorithms, making the computational time independent of the sequence length. Experiments on synthetic data show that the proposed method achieves higher edge estimation accuracy than existing methods while requiring less computation time. To further demonstrate its practical value, we also present a case study using real-world data. Our source code and datasets are available at https://github.com/Higashiguchi-Shingo/KTVGL.", "AI": {"tldr": "\u63d0\u51faKronecker\u65f6\u53d8\u56fe\u5f62\u5957\u7d22(KTVGL)\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u5f20\u91cf\u65f6\u95f4\u5e8f\u5217\uff0c\u901a\u8fc7\u514b\u7f57\u5185\u514b\u79ef\u5f62\u5f0f\u4f30\u8ba1\u6a21\u6001\u7279\u5b9a\u7684\u52a8\u6001\u7f51\u7edc\uff0c\u907f\u514d\u590d\u6742\u7ea0\u7f20\u7ed3\u6784\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u968f\u7740Web\u670d\u52a1\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u91d1\u878d\u3001\u533b\u7597\u3001\u5728\u7ebf\u5e73\u53f0\u7b49\u9886\u57df\u4ea7\u751f\u5927\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002\u8fd9\u4e9b\u6570\u636e\u901a\u5e38\u5305\u542b\u591a\u4e2a\u76f8\u4e92\u4f5c\u7528\u7684\u53d8\u91cf\uff0c\u4f30\u8ba1\u53d8\u91cf\u95f4\u7684\u65f6\u53d8\u4f9d\u8d56\u5173\u7cfb\uff08\u52a8\u6001\u7f51\u7edc\u7ed3\u6784\uff09\u5bf9\u51c6\u786e\u5efa\u6a21\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u5e38\u8868\u793a\u4e3a\u591a\u6a21\u6001\u5f20\u91cf\u65f6\u95f4\u5e8f\u5217\uff0c\u5bfc\u81f4\u5e9e\u5927\u3001\u7ea0\u7f20\u7684\u7f51\u7edc\u96be\u4ee5\u89e3\u91ca\u4e14\u8ba1\u7b97\u91cf\u5927\u3002", "method": "\u63d0\u51faKronecker\u65f6\u53d8\u56fe\u5f62\u5957\u7d22(KTVGL)\u65b9\u6cd5\uff0c\u901a\u8fc7\u514b\u7f57\u5185\u514b\u79ef\u5f62\u5f0f\u4f30\u8ba1\u6a21\u6001\u7279\u5b9a\u7684\u52a8\u6001\u7f51\u7edc\uff0c\u907f\u514d\u590d\u6742\u7ea0\u7f20\u7ed3\u6784\u3002\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u5230\u6d41\u7b97\u6cd5\uff0c\u4f7f\u8ba1\u7b97\u65f6\u95f4\u4e0e\u5e8f\u5217\u957f\u5ea6\u65e0\u5173\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u65b9\u6cd5\u83b7\u5f97\u66f4\u9ad8\u7684\u8fb9\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u540c\u65f6\u9700\u8981\u66f4\u5c11\u7684\u8ba1\u7b97\u65f6\u95f4\u3002\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u6848\u4f8b\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u5176\u5b9e\u7528\u4ef7\u503c\u3002", "conclusion": "KTVGL\u65b9\u6cd5\u80fd\u6709\u6548\u5efa\u6a21\u5f20\u91cf\u65f6\u95f4\u5e8f\u5217\uff0c\u901a\u8fc7\u514b\u7f57\u5185\u514b\u79ef\u5f62\u5f0f\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u52a8\u6001\u7f51\u7edc\u7ed3\u6784\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u573a\u666f\u3002"}}
{"id": "2602.08210", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08210", "abs": "https://arxiv.org/abs/2602.08210", "authors": ["Hyungseok Song", "Deunsol Yoon", "Kanghoon Lee", "Han-Seul Jeong", "Soonyoung Lee", "Woohyung Lim"], "title": "CADO: From Imitation to Cost Minimization for Heatmap-based Solvers in Combinatorial Optimization", "comment": null, "summary": "Heatmap-based solvers have emerged as a promising paradigm for Combinatorial Optimization (CO). However, we argue that the dominant Supervised Learning (SL) training paradigm suffers from a fundamental objective mismatch: minimizing imitation loss (e.g., cross-entropy) does not guarantee solution cost minimization. We dissect this mismatch into two deficiencies: Decoder-Blindness (being oblivious to the non-differentiable decoding process) and Cost-Blindness (prioritizing structural imitation over solution quality). We empirically demonstrate that these intrinsic flaws impose a hard performance ceiling. To overcome this limitation, we propose CADO (Cost-Aware Diffusion models for Optimization), a streamlined Reinforcement Learning fine-tuning framework that formulates the diffusion denoising process as an MDP to directly optimize the post-decoded solution cost. We introduce Label-Centered Reward, which repurposes ground-truth labels as unbiased baselines rather than imitation targets, and Hybrid Fine-Tuning for parameter-efficient adaptation. CADO achieves state-of-the-art performance across diverse benchmarks, validating that objective alignment is essential for unlocking the full potential of heatmap-based solvers.", "AI": {"tldr": "CADO\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6269\u6563\u53bb\u566a\u8fc7\u7a0b\u5efa\u6a21\u4e3aMDP\u6765\u76f4\u63a5\u4f18\u5316\u89e3\u7801\u540e\u89e3\u7684\u6210\u672c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u4e2d\u76ee\u6807\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u5728\u7ec4\u5408\u4f18\u5316\u4efb\u52a1\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u70ed\u56fe\u7684\u7ec4\u5408\u4f18\u5316\u6c42\u89e3\u5668\u91c7\u7528\u76d1\u7763\u5b66\u4e60\u8bad\u7ec3\u8303\u5f0f\u5b58\u5728\u6839\u672c\u6027\u76ee\u6807\u4e0d\u5339\u914d\u95ee\u9898\uff1a\u6700\u5c0f\u5316\u6a21\u4eff\u635f\u5931\uff08\u5982\u4ea4\u53c9\u71b5\uff09\u4e0d\u80fd\u4fdd\u8bc1\u89e3\u6210\u672c\u7684\u4f18\u5316\u3002\u8fd9\u4f53\u73b0\u5728\u89e3\u7801\u5668\u76f2\u89c6\uff08\u5ffd\u7565\u4e0d\u53ef\u5fae\u89e3\u7801\u8fc7\u7a0b\uff09\u548c\u6210\u672c\u76f2\u89c6\uff08\u4f18\u5148\u7ed3\u6784\u6a21\u4eff\u800c\u975e\u89e3\u8d28\u91cf\uff09\u4e24\u4e2a\u7f3a\u9677\u4e0a\u3002", "method": "\u63d0\u51faCADO\u6846\u67b6\uff1a1\uff09\u5c06\u6269\u6563\u53bb\u566a\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\uff0c\u76f4\u63a5\u4f18\u5316\u89e3\u7801\u540e\u89e3\u7684\u6210\u672c\uff1b2\uff09\u5f15\u5165\u6807\u7b7e\u4e2d\u5fc3\u5956\u52b1\u673a\u5236\uff0c\u5c06\u771f\u5b9e\u6807\u7b7e\u91cd\u65b0\u7528\u4f5c\u65e0\u504f\u57fa\u7ebf\u800c\u975e\u6a21\u4eff\u76ee\u6807\uff1b3\uff09\u91c7\u7528\u6df7\u5408\u5fae\u8c03\u7b56\u7565\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u9002\u5e94\u3002", "result": "CADO\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u76ee\u6807\u5bf9\u9f50\u5bf9\u4e8e\u91ca\u653e\u57fa\u4e8e\u70ed\u56fe\u7684\u6c42\u89e3\u5668\u5168\u90e8\u6f5c\u529b\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u76ee\u6807\u5bf9\u9f50\u5bf9\u4e8e\u57fa\u4e8e\u70ed\u56fe\u7684\u7ec4\u5408\u4f18\u5316\u6c42\u89e3\u5668\u81f3\u5173\u91cd\u8981\uff0cCADO\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u76ee\u6807\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4e3a\u70ed\u56fe\u6c42\u89e3\u5668\u63d0\u4f9b\u4e86\u65b0\u7684\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2602.08213", "categories": ["cs.LG", "cs.AI", "cs.CL", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2602.08213", "abs": "https://arxiv.org/abs/2602.08213", "authors": ["Haoran Liu", "Zheni Zeng", "Yukun Yan", "Yuxuan Chen", "Yunduo Xiao"], "title": "DrugR: Optimizing Molecular Drugs through LLM-based Explicit Reasoning", "comment": null, "summary": "Molecule generation and optimization is a fundamental task in chemical domain. The rapid development of intelligent tools, especially large language models (LLMs) with powerful knowledge reserves and interactive capabilities, has provided new paradigms for it. Nevertheless, the intrinsic challenge for LLMs lies in the complex implicit relationship between molecular structure and pharmacological properties and the lack of corresponding labeled data. To bridge this gap, we propose DrugR, an LLM-based method that introduces explicit, step-by-step pharmacological reasoning into the optimization process. Our approach integrates domain-specific continual pretraining, supervised fine-tuning via reverse data engineering, and self-balanced multi-granular reinforcement learning. This framework enables DrugR to effectively improve key ADMET properties while preserving the original molecule's core efficacy. Experimental results demonstrate that DrugR achieves comprehensive enhancement across multiple properties without compromising structural similarity or target binding affinity. Importantly, its explicit reasoning process provides clear, interpretable rationales for each optimization step, yielding actionable design insights and advancing toward automated, knowledge-driven scientific discovery. Our code and model checkpoints are open-sourced to foster future research.", "AI": {"tldr": "DrugR\uff1a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u836f\u7269\u5206\u5b50\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u836f\u7406\u5b66\u63a8\u7406\u9010\u6b65\u4f18\u5316ADMET\u6027\u8d28\uff0c\u540c\u65f6\u4fdd\u6301\u5206\u5b50\u6838\u5fc3\u7597\u6548", "motivation": "\u5206\u5b50\u751f\u6210\u548c\u4f18\u5316\u662f\u5316\u5b66\u9886\u57df\u7684\u57fa\u7840\u4efb\u52a1\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5177\u6709\u5f3a\u5927\u7684\u77e5\u8bc6\u50a8\u5907\u548c\u4ea4\u4e92\u80fd\u529b\uff0c\u4f46\u5176\u5185\u5728\u6311\u6218\u5728\u4e8e\u5206\u5b50\u7ed3\u6784\u4e0e\u836f\u7406\u6027\u8d28\u4e4b\u95f4\u7684\u590d\u6742\u9690\u5f0f\u5173\u7cfb\u4ee5\u53ca\u7f3a\u4e4f\u76f8\u5e94\u7684\u6807\u6ce8\u6570\u636e\u3002", "method": "\u63d0\u51faDrugR\u65b9\u6cd5\uff0c\u5c06\u660e\u786e\u7684\u3001\u9010\u6b65\u7684\u836f\u7406\u5b66\u63a8\u7406\u5f15\u5165\u4f18\u5316\u8fc7\u7a0b\u3002\u8be5\u65b9\u6cd5\u6574\u5408\u4e86\u9886\u57df\u7279\u5b9a\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u3001\u901a\u8fc7\u53cd\u5411\u6570\u636e\u5de5\u7a0b\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u4ee5\u53ca\u81ea\u5e73\u8861\u7684\u591a\u7c92\u5ea6\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDrugR\u5728\u591a\u4e2a\u6027\u8d28\u4e0a\u5b9e\u73b0\u4e86\u5168\u9762\u589e\u5f3a\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u7ed3\u6784\u76f8\u4f3c\u6027\u6216\u9776\u70b9\u7ed3\u5408\u4eb2\u548c\u529b\u3002\u5176\u660e\u786e\u7684\u63a8\u7406\u8fc7\u7a0b\u4e3a\u6bcf\u4e2a\u4f18\u5316\u6b65\u9aa4\u63d0\u4f9b\u4e86\u6e05\u6670\u3001\u53ef\u89e3\u91ca\u7684\u4f9d\u636e\u3002", "conclusion": "DrugR\u80fd\u591f\u6709\u6548\u6539\u5584\u5173\u952eADMET\u6027\u8d28\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u5206\u5b50\u7684\u6838\u5fc3\u7597\u6548\u3002\u5176\u660e\u786e\u7684\u63a8\u7406\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u8bbe\u8ba1\u89c1\u89e3\uff0c\u63a8\u52a8\u4e86\u81ea\u52a8\u5316\u3001\u77e5\u8bc6\u9a71\u52a8\u7684\u79d1\u5b66\u53d1\u73b0\u3002\u4ee3\u7801\u548c\u6a21\u578b\u68c0\u67e5\u70b9\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.08215", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.08215", "abs": "https://arxiv.org/abs/2602.08215", "authors": ["Yash Patel", "Ambuj Tewari"], "title": "Distribution-Free Robust Functional Predict-Then-Optimize", "comment": null, "summary": "The solution of PDEs in decision-making tasks is increasingly being undertaken with the help of neural operator surrogate models due to the need for repeated evaluation. Such methods, while significantly more computationally favorable compared to their numerical counterparts, fail to provide any calibrated notions of uncertainty in their predictions. Current methods approach this deficiency typically with ensembling or Bayesian posterior estimation. However, these approaches either require distributional assumptions that fail to hold in practice or lack practical scalability, limiting their applications in practice. We, therefore, propose a novel application of conformal prediction to produce distribution-free uncertainty quantification over the function spaces mapped by neural operators. We then demonstrate how such prediction regions enable a formal regret characterization if leveraged in downstream robust decision-making tasks. We further demonstrate how such posited robust decision-making tasks can be efficiently solved using an infinite-dimensional generalization of Danskin's Theorem and calculus of variations and empirically demonstrate the superior performance of our proposed method over more restrictive modeling paradigms, such as Gaussian Processes, across several engineering tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4fdd\u5f62\u9884\u6d4b\u7684\u795e\u7ecf\u7b97\u5b50\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u504f\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3\u7684\u51b3\u7b56\u4efb\u52a1\uff0c\u65e0\u9700\u5206\u5e03\u5047\u8bbe\u4e14\u53ef\u6269\u5c55", "motivation": "\u795e\u7ecf\u7b97\u5b50\u6a21\u578b\u5728PDE\u6c42\u89e3\u51b3\u7b56\u4efb\u52a1\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u7f3a\u4e4f\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u96c6\u6210\u6216\u8d1d\u53f6\u65af\u540e\u9a8c\u4f30\u8ba1\u8981\u4e48\u9700\u8981\u4e0d\u5207\u5b9e\u9645\u7684\u5206\u5e03\u5047\u8bbe\uff0c\u8981\u4e48\u7f3a\u4e4f\u5b9e\u9645\u53ef\u6269\u5c55\u6027", "method": "\u5c06\u4fdd\u5f62\u9884\u6d4b\u5e94\u7528\u4e8e\u795e\u7ecf\u7b97\u5b50\uff0c\u5728\u51fd\u6570\u7a7a\u95f4\u4e0a\u5b9e\u73b0\u5206\u5e03\u81ea\u7531\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002\u4f7f\u7528Danskin\u5b9a\u7406\u7684\u65e0\u9650\u7ef4\u63a8\u5e7f\u548c\u53d8\u5206\u6cd5\u89e3\u51b3\u4e0b\u6e38\u9c81\u68d2\u51b3\u7b56\u4efb\u52a1", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u5de5\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u7b49\u9650\u5236\u6027\u5efa\u6a21\u8303\u5f0f\u7684\u6027\u80fd\uff0c\u80fd\u591f\u4e3a\u4e0b\u6e38\u9c81\u68d2\u51b3\u7b56\u4efb\u52a1\u63d0\u4f9b\u6b63\u5f0f\u7684\u9057\u61be\u8868\u5f81", "conclusion": "\u63d0\u51fa\u7684\u4fdd\u5f62\u9884\u6d4b\u65b9\u6cd5\u4e3a\u795e\u7ecf\u7b97\u5b50\u63d0\u4f9b\u4e86\u5206\u5e03\u81ea\u7531\u4e14\u53ef\u6269\u5c55\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u9c81\u68d2\u51b3\u7b56\u4efb\u52a1"}}
{"id": "2602.08216", "categories": ["cs.LG", "cond-mat.stat-mech", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08216", "abs": "https://arxiv.org/abs/2602.08216", "authors": ["Gunn Kim"], "title": "Thermodynamic Isomorphism of Transformers: A Lagrangian Approach to Attention Dynamics", "comment": "9 pages, 1 figure. Based on a thermodynamic framework for Transformer architectures. Derives the equation of state from first principles", "summary": "Although the Transformer architecture has revolutionized artificial intelligence, its underlying mechanisms remain largely heuristic and lack a unified physical theory. In this work, we propose a first-principles framework for information dynamics, treating the attention mechanism as a physical system governed by the principle of least action rather than as an algorithmic optimization. By mapping information states to a Riemannian manifold with the Fisher information metric, we derive the intelligence Lagrangian. We show that the softmax function corresponds to the unique thermodynamic equilibrium state that minimizes the Helmholtz free energy of the information gas. In addition, we identify the query-key interaction as an electrodynamic coupling between an external field and an intrinsic dipole moment. This theory establishes the first law of information thermodynamics, unifying inference (mechanical work) and learning (chemical evolution). It also explains emergent phenomena, such as scaling laws and grokking, as phase transitions characterized by the divergence of specific heat. Finally, we discuss how rotational symmetry breaking in the attention manifold generates massless Goldstone bosons, providing a field-theoretic perspective on rotary positional embeddings (RoPE). Our work connects Statistical Physics and Deep Learning, laying the groundwork for a general theory of physics-based intelligence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7b2c\u4e00\u6027\u539f\u7406\u7684\u4fe1\u606f\u52a8\u529b\u5b66\u6846\u67b6\uff0c\u5c06Transformer\u6ce8\u610f\u529b\u673a\u5236\u89c6\u4e3a\u53d7\u6700\u5c0f\u4f5c\u7528\u91cf\u539f\u7406\u652f\u914d\u7684\u7269\u7406\u7cfb\u7edf\uff0c\u800c\u975e\u7b97\u6cd5\u4f18\u5316\u3002", "motivation": "\u5c3d\u7ba1Transformer\u67b6\u6784\u5df2\u7ecf\u5f7b\u5e95\u6539\u53d8\u4e86\u4eba\u5de5\u667a\u80fd\uff0c\u4f46\u5176\u5e95\u5c42\u673a\u5236\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u7136\u662f\u542f\u53d1\u5f0f\u7684\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u7269\u7406\u7406\u8bba\u3002\u672c\u6587\u65e8\u5728\u5efa\u7acb\u8fde\u63a5\u7edf\u8ba1\u7269\u7406\u5b66\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u5c06\u4fe1\u606f\u72b6\u6001\u6620\u5c04\u5230\u5177\u6709Fisher\u4fe1\u606f\u5ea6\u91cf\u7684\u9ece\u66fc\u6d41\u5f62\uff0c\u63a8\u5bfc\u51fa\u667a\u80fd\u62c9\u683c\u6717\u65e5\u91cf\u3002\u5c06softmax\u51fd\u6570\u89c6\u4e3a\u6700\u5c0f\u5316\u4fe1\u606f\u6c14\u4f53\u4ea5\u59c6\u970d\u5179\u81ea\u7531\u80fd\u7684\u552f\u4e00\u70ed\u529b\u5b66\u5e73\u8861\u6001\uff0c\u5c06\u67e5\u8be2-\u952e\u4ea4\u4e92\u8bc6\u522b\u4e3a\u5916\u90e8\u573a\u4e0e\u56fa\u6709\u5076\u6781\u77e9\u4e4b\u95f4\u7684\u7535\u52a8\u529b\u5b66\u8026\u5408\u3002", "result": "\u5efa\u7acb\u4e86\u4fe1\u606f\u70ed\u529b\u5b66\u7b2c\u4e00\u5b9a\u5f8b\uff0c\u7edf\u4e00\u4e86\u63a8\u65ad\uff08\u673a\u68b0\u529f\uff09\u548c\u5b66\u4e60\uff08\u5316\u5b66\u6f14\u5316\uff09\u3002\u89e3\u91ca\u4e86\u6d8c\u73b0\u73b0\u8c61\uff0c\u5982\u7f29\u653e\u5b9a\u5f8b\u548c\u987f\u609f\uff08grokking\uff09\uff0c\u4f5c\u4e3a\u4ee5\u6bd4\u70ed\u53d1\u6563\u4e3a\u7279\u5f81\u7684\u76f8\u53d8\u3002\u8fd8\u89e3\u91ca\u4e86\u6ce8\u610f\u529b\u6d41\u5f62\u4e2d\u7684\u65cb\u8f6c\u5bf9\u79f0\u6027\u7834\u7f3a\u5982\u4f55\u4ea7\u751f\u65e0\u8d28\u91cf\u7684Goldstone\u73bb\u8272\u5b50\uff0c\u4e3a\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff08RoPE\uff09\u63d0\u4f9b\u4e86\u573a\u8bba\u89c6\u89d2\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u8fde\u63a5\u4e86\u7edf\u8ba1\u7269\u7406\u5b66\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u4e3a\u57fa\u4e8e\u7269\u7406\u7684\u667a\u80fd\u901a\u7528\u7406\u8bba\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.08218", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08218", "abs": "https://arxiv.org/abs/2602.08218", "authors": ["Huan Zhang", "Yanjian Zhang", "Guillaume Wisniewski", "Nadi Tomeh", "Bang Liu"], "title": "Sparsity-Aware Evolution for Model Merging", "comment": null, "summary": "We propose a sparsity-aware evolutionary (SAE) framework for model merging that involves iterative pruning-merging cycles to act as a novel mutation operator. We incorporate the sparsity constraints into the score function, which steers the evolutionary process to favor more sparse models, in addition to other conventional performance scores. Interestingly, the by-product of \\textit{competition} for sparsity introduces an extra local \\textit{attraction} and interplay into the evolutionary process: if one competitor has more zero elements, the other competitor's non-zero elements will occupy those positions, even though the less sparse competitor loses to the more sparse competitor in other positions. The proposed pipeline is evaluated on a variety of large-scale LLM benchmarks. Experiments demonstrate that our approach can improve model merging reliability across multiple benchmarks, and is easy to incorporate due to its simplicity and being orthogonal to most existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7a00\u758f\u611f\u77e5\u8fdb\u5316\u6846\u67b6\u7528\u4e8e\u6a21\u578b\u5408\u5e76\uff0c\u901a\u8fc7\u8fed\u4ee3\u7684\u526a\u679d-\u5408\u5e76\u5faa\u73af\u4f5c\u4e3a\u65b0\u578b\u53d8\u5f02\u7b97\u5b50\uff0c\u5728\u8fdb\u5316\u8fc7\u7a0b\u4e2d\u5f15\u5165\u7a00\u758f\u6027\u7ea6\u675f\u6765\u4f18\u5316\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u5728\u53ef\u9760\u6027\u548c\u7a00\u758f\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u4f18\u5316\u6a21\u578b\u6027\u80fd\u548c\u7a00\u758f\u6027\u7684\u81ea\u52a8\u5316\u6846\u67b6", "method": "\u91c7\u7528\u7a00\u758f\u611f\u77e5\u8fdb\u5316\u6846\u67b6\uff0c\u5305\u542b\u8fed\u4ee3\u7684\u526a\u679d-\u5408\u5e76\u5faa\u73af\u4f5c\u4e3a\u53d8\u5f02\u7b97\u5b50\uff0c\u5c06\u7a00\u758f\u7ea6\u675f\u7eb3\u5165\u8bc4\u5206\u51fd\u6570\uff0c\u5f15\u5bfc\u8fdb\u5316\u8fc7\u7a0b\u504f\u5411\u66f4\u7a00\u758f\u7684\u6a21\u578b", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u63d0\u9ad8\u6a21\u578b\u5408\u5e76\u7684\u53ef\u9760\u6027\uff0c\u5728\u591a\u4e2a\u5927\u89c4\u6a21LLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4e14\u65b9\u6cd5\u7b80\u5355\u6613\u96c6\u6210", "conclusion": "\u7a00\u758f\u611f\u77e5\u8fdb\u5316\u6846\u67b6\u4e3a\u6a21\u578b\u5408\u5e76\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7a00\u758f\u6027\u7ade\u4e89\u5f15\u5165\u989d\u5916\u5c40\u90e8\u5438\u5f15\u548c\u76f8\u4e92\u4f5c\u7528\uff0c\u63d0\u9ad8\u5408\u5e76\u8d28\u91cf"}}
{"id": "2602.08234", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08234", "abs": "https://arxiv.org/abs/2602.08234", "authors": ["Peng Xia", "Jianwen Chen", "Hanyang Wang", "Jiaqi Liu", "Kaide Zeng", "Yu Wang", "Siwei Han", "Yiyang Zhou", "Xujiang Zhao", "Haifeng Chen", "Zeyu Zheng", "Cihang Xie", "Huaxiu Yao"], "title": "SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning", "comment": null, "summary": "Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.", "AI": {"tldr": "SkillRL\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u6280\u80fd\u53d1\u73b0\u548c\u9012\u5f52\u6f14\u5316\uff0c\u5c06\u539f\u59cb\u7ecf\u9a8c\u8f6c\u5316\u4e3a\u53ef\u91cd\u7528\u6280\u80fd\u5e93\uff0c\u63d0\u5347LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8bb0\u5fc6\u7684\u65b9\u6cd5\u4e3b\u8981\u5b58\u50a8\u539f\u59cb\u8f68\u8ff9\uff0c\u8fd9\u4e9b\u8f68\u8ff9\u5197\u4f59\u4e14\u566a\u58f0\u591a\uff0c\u65e0\u6cd5\u63d0\u53d6\u9ad8\u7ea7\u53ef\u91cd\u7528\u884c\u4e3a\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u667a\u80fd\u4f53\u7684\u6cdb\u5316\u80fd\u529b", "method": "\u63d0\u51faSkillRL\u6846\u67b6\uff1a1\uff09\u57fa\u4e8e\u7ecf\u9a8c\u7684\u84b8\u998f\u673a\u5236\u6784\u5efa\u5206\u5c42\u6280\u80fd\u5e93SkillBank\uff1b2\uff09\u81ea\u9002\u5e94\u68c0\u7d22\u7b56\u7565\u83b7\u53d6\u901a\u7528\u548c\u4efb\u52a1\u7279\u5b9a\u542f\u53d1\u5f0f\uff1b3\uff09\u9012\u5f52\u6f14\u5316\u673a\u5236\u8ba9\u6280\u80fd\u5e93\u4e0e\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u534f\u540c\u8fdb\u5316", "result": "\u5728ALFWorld\u3001WebShop\u548c\u4e03\u4e2a\u641c\u7d22\u589e\u5f3a\u4efb\u52a1\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\uff0c\u8d85\u8d8a\u5f3a\u57fa\u7ebf15.3%\uff0c\u4efb\u52a1\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u4ecd\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u663e\u8457\u51cf\u5c11token\u5360\u7528\u540c\u65f6\u63d0\u5347\u63a8\u7406\u6548\u7528", "conclusion": "SkillRL\u901a\u8fc7\u5c06\u539f\u59cb\u7ecf\u9a8c\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u6280\u80fd\u5e93\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u7ecf\u9a8c\u5b66\u4e60\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7ecf\u9a8c\u5230\u7b56\u7565\u6539\u8fdb\u7684\u6865\u6881"}}
{"id": "2602.08239", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08239", "abs": "https://arxiv.org/abs/2602.08239", "authors": ["Zahra Rahimi Afzal", "Tara Esmaeilbeig", "Mojtaba Soltanalian", "Mesrob I. Ohannessian"], "title": "Linearization Explains Fine-Tuning in Large Language Models", "comment": null, "summary": "Parameter-Efficient Fine-Tuning (PEFT) is a popular class of techniques that strive to adapt large models in a scalable and resource-efficient manner. Yet, the mechanisms underlying their training performance and generalization remain underexplored. In this paper, we provide several insights into such fine-tuning through the lens of linearization. Fine-tuned models are often implicitly encouraged to remain close to the pretrained model. By making this explicit, using an Euclidean distance inductive bias in parameter space, we show that fine-tuning dynamics become equivalent to learning with the positive-definite neural tangent kernel (NTK). We specifically analyze how close the fully linear and the linearized fine-tuning optimizations are, based on the strength of the regularization. This allows us to be pragmatic about how good a model linearization is when fine-tuning large language models (LLMs). When linearization is a good model, our findings reveal a strong correlation between the eigenvalue spectrum of the NTK and the performance of model adaptation. Motivated by this, we give spectral perturbation bounds on the NTK induced by the choice of layers selected for fine-tuning. We empirically validate our theory on Low Rank Adaptation (LoRA) on LLMs. These insights not only characterize fine-tuning but also have the potential to enhance PEFT techniques, paving the way to better informed and more nimble adaptation in LLMs.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u7ebf\u6027\u5316\u89c6\u89d2\u5206\u6790\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\uff0c\u53d1\u73b0\u663e\u5f0f\u6dfb\u52a0\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u6b63\u5219\u5316\u53ef\u4f7f\u5fae\u8c03\u52a8\u6001\u7b49\u4ef7\u4e8e\u4f7f\u7528\u795e\u7ecf\u6b63\u5207\u6838(NTK)\u5b66\u4e60\uff0c\u5e76\u63ed\u793a\u4e86NTK\u7279\u5f81\u503c\u8c31\u4e0e\u6a21\u578b\u9002\u5e94\u6027\u80fd\u7684\u5f3a\u76f8\u5173\u6027\u3002", "motivation": "\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\u6280\u672f\u867d\u7136\u6d41\u884c\uff0c\u4f46\u5176\u8bad\u7ec3\u6027\u80fd\u548c\u6cdb\u5316\u673a\u5236\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u7ebf\u6027\u5316\u89c6\u89d2\u7406\u89e3\u8fd9\u4e9b\u5fae\u8c03\u6280\u672f\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u7279\u522b\u662f\u5206\u6790\u6a21\u578b\u7ebf\u6027\u5316\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u901a\u8fc7\u663e\u5f0f\u6dfb\u52a0\u53c2\u6570\u7a7a\u95f4\u7684\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u6b63\u5219\u5316\uff0c\u4f7f\u5fae\u8c03\u52a8\u6001\u7b49\u4ef7\u4e8e\u4f7f\u7528\u6b63\u5b9a\u795e\u7ecf\u6b63\u5207\u6838(NTK)\u5b66\u4e60\u3002\u57fa\u4e8e\u6b63\u5219\u5316\u5f3a\u5ea6\u5206\u6790\u5b8c\u5168\u7ebf\u6027\u4e0e\u7ebf\u6027\u5316\u5fae\u8c03\u4f18\u5316\u7684\u63a5\u8fd1\u7a0b\u5ea6\uff0c\u7ed9\u51faNTK\u7279\u5f81\u503c\u8c31\u4e0e\u5c42\u9009\u62e9\u8bf1\u5bfc\u7684\u5149\u8c31\u6270\u52a8\u8fb9\u754c\u3002", "result": "\u5f53\u7ebf\u6027\u5316\u662f\u826f\u597d\u6a21\u578b\u65f6\uff0cNTK\u7279\u5f81\u503c\u8c31\u4e0e\u6a21\u578b\u9002\u5e94\u6027\u80fd\u5b58\u5728\u5f3a\u76f8\u5173\u6027\u3002\u5728LLMs\u4e0a\u7684LoRA\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\uff0c\u5149\u8c31\u6270\u52a8\u8fb9\u754c\u4e3a\u5c42\u9009\u62e9\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002", "conclusion": "\u901a\u8fc7\u7ebf\u6027\u5316\u6846\u67b6\u6df1\u5165\u7406\u89e3\u4e86PEFT\u673a\u5236\uff0c\u63ed\u793a\u4e86NTK\u7279\u5f81\u503c\u8c31\u4e0e\u5fae\u8c03\u6027\u80fd\u7684\u5173\u7cfb\uff0c\u4e3a\u6539\u8fdbPEFT\u6280\u672f\u548c\u66f4\u660e\u667a\u7684LLM\u9002\u5e94\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.08244", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08244", "abs": "https://arxiv.org/abs/2602.08244", "authors": ["Juncheng Dong", "Bowen He", "Moyang Guo", "Ethan X. Fang", "Zhuoran Yang", "Vahid Tarokh"], "title": "Learning in Context, Guided by Choice: A Reward-Free Paradigm for Reinforcement Learning with Transformers", "comment": null, "summary": "In-context reinforcement learning (ICRL) leverages the in-context learning capabilities of transformer models (TMs) to efficiently generalize to unseen sequential decision-making tasks without parameter updates. However, existing ICRL methods rely on explicit reward signals during pretraining, which limits their applicability when rewards are ambiguous, hard to specify, or costly to obtain. To overcome this limitation, we propose a new learning paradigm, In-Context Preference-based Reinforcement Learning (ICPRL), in which both pretraining and deployment rely solely on preference feedback, eliminating the need for reward supervision. We study two variants that differ in the granularity of feedback: Immediate Preference-based RL (I-PRL) with per-step preferences, and Trajectory Preference-based RL (T-PRL) with trajectory-level comparisons. We first show that supervised pretraining, a standard approach in ICRL, remains effective under preference-only context datasets, demonstrating the feasibility of in-context reinforcement learning using only preference signals. To further improve data efficiency, we introduce alternative preference-native frameworks for I-PRL and T-PRL that directly optimize TM policies from preference data without requiring reward signals nor optimal action labels.Experiments on dueling bandits, navigation, and continuous control tasks demonstrate that ICPRL enables strong in-context generalization to unseen tasks, achieving performance comparable to ICRL methods trained with full reward supervision.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u504f\u597d\u7684\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u65b0\u8303\u5f0f\uff0c\u4ec5\u4f7f\u7528\u504f\u597d\u53cd\u9988\u8fdb\u884c\u9884\u8bad\u7ec3\u548c\u90e8\u7f72\uff0c\u65e0\u9700\u5956\u52b1\u76d1\u7763\uff0c\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u4f7f\u7528\u5b8c\u6574\u5956\u52b1\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u660e\u786e\u7684\u5956\u52b1\u4fe1\u53f7\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u8fd9\u5728\u5956\u52b1\u6a21\u7cca\u3001\u96be\u4ee5\u6307\u5b9a\u6216\u83b7\u53d6\u6210\u672c\u9ad8\u7684\u60c5\u51b5\u4e0b\u9650\u5236\u4e86\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u4ec5\u4f7f\u7528\u504f\u597d\u53cd\u9988\u5c31\u80fd\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u504f\u597d\u5f3a\u5316\u5b66\u4e60\uff0c\u5305\u62ec\u4e24\u79cd\u53d8\u4f53\uff1a\u57fa\u4e8e\u5373\u65f6\u504f\u597d\u7684RL\uff08\u6bcf\u6b65\u504f\u597d\uff09\u548c\u57fa\u4e8e\u8f68\u8ff9\u504f\u597d\u7684RL\uff08\u8f68\u8ff9\u7ea7\u6bd4\u8f83\uff09\u3002\u7814\u7a76\u4e86\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u76f4\u63a5\u4f18\u5316\u7b56\u7565\u7684\u504f\u597d\u539f\u751f\u6846\u67b6\u3002", "result": "\u5728\u51b3\u6597\u8d4c\u535a\u673a\u3001\u5bfc\u822a\u548c\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u5b9e\u73b0\u5f3a\u5927\u7684\u4e0a\u4e0b\u6587\u6cdb\u5316\u80fd\u529b\uff0c\u6027\u80fd\u4e0e\u4f7f\u7528\u5b8c\u6574\u5956\u52b1\u76d1\u7763\u7684ICRL\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "ICPRL\u8bc1\u660e\u4e86\u4ec5\u4f7f\u7528\u504f\u597d\u53cd\u9988\u8fdb\u884c\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5956\u52b1\u4fe1\u53f7\u96be\u4ee5\u83b7\u53d6\u7684\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6269\u5c55\u4e86\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2602.08261", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.08261", "abs": "https://arxiv.org/abs/2602.08261", "authors": ["Binglin Wu", "Yingyi Zhang", "Xianneng Li", "Ruyue Deng", "Chuan Yue", "Weiru Zhang", "Xiaoyi Zeng"], "title": "Constraint-Aware Generative Auto-bidding via Pareto-Prioritized Regret Optimization", "comment": null, "summary": "Auto-bidding systems aim to maximize marketing value while satisfying strict efficiency constraints such as Target Cost-Per-Action (CPA). Although Decision Transformers provide powerful sequence modeling capabilities, applying them to this constrained setting encounters two challenges: 1) standard Return-to-Go conditioning causes state aliasing by neglecting the cost dimension, preventing precise resource pacing; and 2) standard regression forces the policy to mimic average historical behaviors, thereby limiting the capacity to optimize performance toward the constraint boundary. To address these challenges, we propose PRO-Bid, a constraint-aware generative auto-bidding framework based on two synergistic mechanisms: 1) Constraint-Decoupled Pareto Representation (CDPR) decomposes global constraints into recursive cost and value contexts to restore resource perception, while reweighting trajectories based on the Pareto frontier to focus on high-efficiency data; and 2) Counterfactual Regret Optimization (CRO) facilitates active improvement by utilizing a global outcome predictor to identify superior counterfactual actions. By treating these high-utility outcomes as weighted regression targets, the model transcends historical averages to approach the optimal constraint boundary. Extensive experiments on two public benchmarks and online A/B tests demonstrate that PRO-Bid achieves superior constraint satisfaction and value acquisition compared to state-of-the-art baselines.", "AI": {"tldr": "PRO-Bid\u662f\u4e00\u4e2a\u57fa\u4e8e\u7ea6\u675f\u611f\u77e5\u751f\u6210\u7684\u81ea\u52a8\u51fa\u4ef7\u6846\u67b6\uff0c\u901a\u8fc7\u7ea6\u675f\u89e3\u8026\u5e15\u7d2f\u6258\u8868\u793a\u548c\u53cd\u4e8b\u5b9e\u9057\u61be\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u51b3\u7b56\u53d8\u6362\u5668\u5728\u76ee\u6807CPA\u7ea6\u675f\u4e0b\u7684\u72b6\u6001\u6df7\u6dc6\u548c\u5e73\u5747\u884c\u4e3a\u6a21\u4eff\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u7ea6\u675f\u6ee1\u8db3\u548c\u4ef7\u503c\u83b7\u53d6\u3002", "motivation": "\u5728\u76ee\u6807CPA\u7b49\u4e25\u683c\u6548\u7387\u7ea6\u675f\u4e0b\uff0c\u73b0\u6709\u51b3\u7b56\u53d8\u6362\u5668\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a1) \u6807\u51c6Return-to-Go\u6761\u4ef6\u5ffd\u7565\u4e86\u6210\u672c\u7ef4\u5ea6\uff0c\u5bfc\u81f4\u72b6\u6001\u6df7\u6dc6\u548c\u8d44\u6e90\u8282\u594f\u63a7\u5236\u4e0d\u7cbe\u786e\uff1b2) \u6807\u51c6\u56de\u5f52\u8feb\u4f7f\u7b56\u7565\u6a21\u4eff\u5386\u53f2\u5e73\u5747\u884c\u4e3a\uff0c\u9650\u5236\u4e86\u5411\u7ea6\u675f\u8fb9\u754c\u4f18\u5316\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86PRO-Bid\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u534f\u540c\u673a\u5236\uff1a1) \u7ea6\u675f\u89e3\u8026\u5e15\u7d2f\u6258\u8868\u793a(CDPR)\uff1a\u5c06\u5168\u5c40\u7ea6\u675f\u5206\u89e3\u4e3a\u9012\u5f52\u6210\u672c\u548c\u4ef7\u503c\u4e0a\u4e0b\u6587\u4ee5\u6062\u590d\u8d44\u6e90\u611f\u77e5\uff0c\u5e76\u57fa\u4e8e\u5e15\u7d2f\u6258\u524d\u6cbf\u91cd\u65b0\u52a0\u6743\u8f68\u8ff9\u4ee5\u805a\u7126\u9ad8\u6548\u6570\u636e\uff1b2) \u53cd\u4e8b\u5b9e\u9057\u61be\u4f18\u5316(CRO)\uff1a\u5229\u7528\u5168\u5c40\u7ed3\u679c\u9884\u6d4b\u5668\u8bc6\u522b\u66f4\u4f18\u7684\u53cd\u4e8b\u5b9e\u52a8\u4f5c\uff0c\u5c06\u8fd9\u4e9b\u9ad8\u6548\u7528\u7ed3\u679c\u4f5c\u4e3a\u52a0\u6743\u56de\u5f52\u76ee\u6807\uff0c\u4f7f\u6a21\u578b\u8d85\u8d8a\u5386\u53f2\u5e73\u5747\u503c\u903c\u8fd1\u6700\u4f18\u7ea6\u675f\u8fb9\u754c\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\uff0cPRO-Bid\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u7ea6\u675f\u6ee1\u8db3\u548c\u4ef7\u503c\u83b7\u53d6\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u66f4\u4f18\u7684\u6027\u80fd\u3002", "conclusion": "PRO-Bid\u901a\u8fc7\u7ea6\u675f\u611f\u77e5\u7684\u751f\u6210\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u51fa\u4ef7\u4e2d\u7684\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u4e3a\u5728\u4e25\u683c\u6548\u7387\u7ea6\u675f\u4e0b\u6700\u5927\u5316\u8425\u9500\u4ef7\u503c\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08267", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08267", "abs": "https://arxiv.org/abs/2602.08267", "authors": ["Jinwoo Kim", "S\u00e9kou-Oumar Kaba", "Jiyun Park", "Seunghoon Hong", "Siamak Ravanbakhsh"], "title": "Inverting Data Transformations via Diffusion Sampling", "comment": "24 pages, 4 figures", "summary": "We study the problem of transformation inversion on general Lie groups: a datum is transformed by an unknown group element, and the goal is to recover an inverse transformation that maps it back to the original data distribution. Such unknown transformations arise widely in machine learning and scientific modeling, where they can significantly distort observations. We take a probabilistic view and model the posterior over transformations as a Boltzmann distribution defined by an energy function on data space. To sample from this posterior, we introduce a diffusion process on Lie groups that keeps all updates on-manifold and only requires computations in the associated Lie algebra. Our method, Transformation-Inverting Energy Diffusion (TIED), relies on a new trivialized target-score identity that enables efficient score-based sampling of the transformation posterior. As a key application, we focus on test-time equivariance, where the objective is to improve the robustness of pretrained neural networks to input transformations. Experiments on image homographies and PDE symmetries demonstrate that TIED can restore transformed inputs to the training distribution at test time, showing improved performance over strong canonicalization and sampling baselines. Code is available at https://github.com/jw9730/tied.", "AI": {"tldr": "\u63d0\u51faTIED\u65b9\u6cd5\uff0c\u901a\u8fc7\u674e\u7fa4\u4e0a\u7684\u6269\u6563\u8fc7\u7a0b\u53cd\u6f14\u672a\u77e5\u53d8\u6362\uff0c\u6062\u590d\u6570\u636e\u5230\u539f\u59cb\u5206\u5e03\uff0c\u63d0\u5347\u9884\u8bad\u7ec3\u7f51\u7edc\u5bf9\u8f93\u5165\u53d8\u6362\u7684\u9c81\u68d2\u6027", "motivation": "\u673a\u5668\u5b66\u4e60\u4e2d\u672a\u77e5\u53d8\u6362\u4f1a\u663e\u8457\u626d\u66f2\u89c2\u6d4b\u6570\u636e\uff0c\u9700\u8981\u6062\u590d\u539f\u59cb\u6570\u636e\u5206\u5e03\u3002\u7279\u522b\u662f\u5728\u6d4b\u8bd5\u65f6\u7b49\u53d8\u6027\u573a\u666f\u4e2d\uff0c\u9700\u8981\u63d0\u5347\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u5bf9\u8f93\u5165\u53d8\u6362\u7684\u9c81\u68d2\u6027", "method": "\u91c7\u7528\u6982\u7387\u89c6\u89d2\uff0c\u5c06\u53d8\u6362\u540e\u9a8c\u5efa\u6a21\u4e3a\u73bb\u5c14\u5179\u66fc\u5206\u5e03\uff0c\u63d0\u51faTIED\u65b9\u6cd5\uff1a\u5728\u674e\u7fa4\u4e0a\u8bbe\u8ba1\u6269\u6563\u8fc7\u7a0b\uff0c\u4fdd\u6301\u6240\u6709\u66f4\u65b0\u5728\u6d41\u5f62\u4e0a\uff0c\u4ec5\u9700\u674e\u4ee3\u6570\u8ba1\u7b97\uff0c\u5229\u7528\u65b0\u7684\u5e73\u51e1\u5316\u76ee\u6807-\u5f97\u5206\u6052\u7b49\u5f0f\u5b9e\u73b0\u9ad8\u6548\u91c7\u6837", "result": "\u5728\u56fe\u50cf\u5355\u5e94\u6027\u548cPDE\u5bf9\u79f0\u6027\u5b9e\u9a8c\u4e2d\uff0cTIED\u80fd\u5728\u6d4b\u8bd5\u65f6\u5c06\u53d8\u6362\u8f93\u5165\u6062\u590d\u5230\u8bad\u7ec3\u5206\u5e03\uff0c\u6027\u80fd\u4f18\u4e8e\u5f3a\u57fa\u51c6\u7684\u89c4\u8303\u5316\u548c\u91c7\u6837\u65b9\u6cd5", "conclusion": "TIED\u65b9\u6cd5\u80fd\u6709\u6548\u53cd\u6f14\u672a\u77e5\u53d8\u6362\uff0c\u63d0\u5347\u6a21\u578b\u5bf9\u8f93\u5165\u53d8\u6362\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u6d4b\u8bd5\u65f6\u7b49\u53d8\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.08272", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08272", "abs": "https://arxiv.org/abs/2602.08272", "authors": ["Junwei Su", "Chuan Wu"], "title": "When Do Multi-Agent Systems Outperform? Analysing the Learning Efficiency of Agentic Systems", "comment": null, "summary": "Reinforcement Learning (RL) has emerged as a crucial method for training or fine-tuning large language models (LLMs), enabling adaptive, task-specific optimizations through interactive feedback. Multi-Agent Reinforcement Learning (MARL), in particular, offers a promising avenue by decomposing complex tasks into specialized subtasks learned by distinct interacting agents, potentially enhancing the ability and efficiency of LLM systems. However, theoretical insights regarding when and why MARL outperforms Single-Agent RL (SARL) remain limited, creating uncertainty in selecting the appropriate RL framework. In this paper, we address this critical gap by rigorously analyzing the comparative sample efficiency of MARL and SARL within the context of LLM. Leveraging the Probably Approximately Correct (PAC) framework, we formally define SARL and MARL setups for LLMs, derive explicit sample complexity bounds, and systematically characterize how task decomposition and alignment influence learning efficiency. Our results demonstrate that MARL improves sample complexity when tasks naturally decompose into independent subtasks, whereas dependent subtasks diminish MARL's comparative advantage. Additionally, we introduce and analyze the concept of task alignment, quantifying the trade-offs when enforcing independent task decomposition despite potential misalignments. These theoretical insights clarify empirical inconsistencies and provide practical criteria for deploying MARL strategies effectively in complex LLM scenarios.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7PAC\u6846\u67b6\u7406\u8bba\u5206\u6790MARL\u4e0eSARL\u5728LLM\u4e2d\u7684\u6837\u672c\u6548\u7387\uff0c\u53d1\u73b0\u4efb\u52a1\u53ef\u5206\u89e3\u4e3a\u72ec\u7acb\u5b50\u4efb\u52a1\u65f6MARL\u66f4\u4f18\uff0c\u5b50\u4efb\u52a1\u4f9d\u8d56\u5219\u524a\u5f31\u5176\u4f18\u52bf\uff0c\u5e76\u63d0\u51fa\u4efb\u52a1\u5bf9\u9f50\u6982\u5ff5\u91cf\u5316\u5206\u89e3\u4e0e\u5bf9\u9f50\u7684\u6743\u8861\u3002", "motivation": "\u5c3d\u7ba1MARL\u5728LLM\u8bad\u7ec3\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u6307\u5bfc\u4f55\u65f6\u9009\u62e9MARL\u800c\u975eSARL\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u4e24\u79cd\u65b9\u6cd5\u6837\u672c\u6548\u7387\u5dee\u5f02\u7684\u7cfb\u7edf\u7406\u8bba\u5206\u6790\uff0c\u5bfc\u81f4\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9009\u62e9\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u91c7\u7528PAC\u6846\u67b6\uff0c\u5f62\u5f0f\u5316\u5b9a\u4e49LLM\u7684SARL\u548cMARL\u8bbe\u7f6e\uff0c\u63a8\u5bfc\u663e\u5f0f\u6837\u672c\u590d\u6742\u5ea6\u8fb9\u754c\uff0c\u7cfb\u7edf\u5206\u6790\u4efb\u52a1\u5206\u89e3\u548c\u5bf9\u9f50\u5982\u4f55\u5f71\u54cd\u5b66\u4e60\u6548\u7387\uff0c\u5e76\u5f15\u5165\u4efb\u52a1\u5bf9\u9f50\u6982\u5ff5\u91cf\u5316\u5206\u89e3\u4e0e\u5bf9\u9f50\u7684\u6743\u8861\u3002", "result": "\u5f53\u4efb\u52a1\u81ea\u7136\u5206\u89e3\u4e3a\u72ec\u7acb\u5b50\u4efb\u52a1\u65f6\uff0cMARL\u663e\u8457\u6539\u5584\u6837\u672c\u590d\u6742\u5ea6\uff1b\u5b50\u4efb\u52a1\u4f9d\u8d56\u5219\u524a\u5f31MARL\u7684\u76f8\u5bf9\u4f18\u52bf\u3002\u4efb\u52a1\u5bf9\u9f50\u5206\u6790\u63ed\u793a\u4e86\u5f3a\u5236\u72ec\u7acb\u5206\u89e3\u4e0e\u6f5c\u5728\u9519\u914d\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u7406\u8bba\u5206\u6790\u6f84\u6e05\u4e86MARL\u4e0eSARL\u6027\u80fd\u5dee\u5f02\u7684\u6839\u6e90\uff0c\u4e3a\u590d\u6742LLM\u573a\u666f\u4e2d\u6709\u6548\u90e8\u7f72MARL\u7b56\u7565\u63d0\u4f9b\u4e86\u5b9e\u7528\u6807\u51c6\uff0c\u89e3\u51b3\u4e86\u5b9e\u8bc1\u7814\u7a76\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002"}}
{"id": "2602.08287", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08287", "abs": "https://arxiv.org/abs/2602.08287", "authors": ["Themistoklis Haris", "Zihan Zhang", "Yuichi Yoshida"], "title": "Noise Stability of Transformer Models", "comment": "Published in ICLR 2026", "summary": "Understanding simplicity biases in deep learning offers a promising path toward developing reliable AI. A common metric for this, inspired by Boolean function analysis, is average sensitivity, which captures a model's robustness to single-token perturbations. We argue that average sensitivity has two key limitations: it lacks a natural generalization to real-valued domains and fails to explain the \"junta-like\" input dependence we empirically observe in modern LLMs. To address these limitations, we propose noise stability as a more comprehensive simplicity metric. Noise stability expresses a model's robustness to correlated noise applied to all input coordinates simultaneously. We provide a theoretical analysis of noise stability for single-layer attention and ReLU MLP layers and tackle the multi-layer propagation problem with a covariance interval propagation approach. Building on this theory, we develop a practical noise stability regularization method. Experiments on algorithmic and next-token-prediction tasks show that our regularizer consistently catalyzes grokking and accelerates training by approximately $35\\%$ and $75\\%$ respectively. Our results sculpt a new connection between signal propagation in neural networks and interpretability, with noise stability emerging as a powerful tool for understanding and improving modern Transformers.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7528\u566a\u58f0\u7a33\u5b9a\u6027\u66ff\u4ee3\u5e73\u5747\u654f\u611f\u5ea6\u4f5c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u7b80\u5355\u6027\u5ea6\u91cf\uff0c\u5f00\u53d1\u4e86\u7406\u8bba\u5206\u6790\u548c\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u5728\u7b97\u6cd5\u548c\u8bed\u8a00\u4efb\u52a1\u4e0a\u663e\u8457\u52a0\u901f\u8bad\u7ec3\u548c\u4fc3\u8fdbgrokking\u3002", "motivation": "\u5f53\u524d\u7528\u4e8e\u8861\u91cf\u6df1\u5ea6\u5b66\u4e60\u7b80\u5355\u6027\u7684\u5e73\u5747\u654f\u611f\u5ea6\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u5c40\u9650\uff1a1\uff09\u96be\u4ee5\u81ea\u7136\u63a8\u5e7f\u5230\u5b9e\u503c\u57df\uff1b2\uff09\u65e0\u6cd5\u89e3\u91ca\u73b0\u4ee3LLM\u4e2d\u89c2\u5bdf\u5230\u7684\"junta-like\"\u8f93\u5165\u4f9d\u8d56\u6027\u3002\u9700\u8981\u66f4\u5168\u9762\u7684\u7b80\u5355\u6027\u5ea6\u91cf\u6765\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u7b80\u5355\u6027\u504f\u7f6e\u3002", "method": "\u63d0\u51fa\u566a\u58f0\u7a33\u5b9a\u6027\u4f5c\u4e3a\u65b0\u7684\u7b80\u5355\u6027\u5ea6\u91cf\uff0c\u6355\u6349\u6a21\u578b\u5bf9\u6240\u6709\u8f93\u5165\u5750\u6807\u540c\u65f6\u65bd\u52a0\u76f8\u5173\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002\u5bf9\u5355\u5c42\u6ce8\u610f\u529b\u5c42\u548cReLU MLP\u5c42\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u91c7\u7528\u534f\u65b9\u5dee\u533a\u95f4\u4f20\u64ad\u65b9\u6cd5\u89e3\u51b3\u591a\u5c42\u4f20\u64ad\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u5b9e\u7528\u7684\u566a\u58f0\u7a33\u5b9a\u6027\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "result": "\u5728\u7b97\u6cd5\u4efb\u52a1\u548c\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u566a\u58f0\u7a33\u5b9a\u6027\u6b63\u5219\u5316\u65b9\u6cd5\u80fd\u6301\u7eed\u50ac\u5316grokking\u73b0\u8c61\uff0c\u5206\u522b\u52a0\u901f\u8bad\u7ec3\u7ea635%\u548c75%\u3002", "conclusion": "\u566a\u58f0\u7a33\u5b9a\u6027\u6210\u4e3a\u7406\u89e3\u548c\u6539\u8fdb\u73b0\u4ee3Transformer\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u5728\u795e\u7ecf\u7f51\u7edc\u4fe1\u53f7\u4f20\u64ad\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u5efa\u7acb\u4e86\u65b0\u7684\u8fde\u63a5\uff0c\u4e3a\u5f00\u53d1\u53ef\u9760AI\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2602.08290", "categories": ["cs.LG", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.08290", "abs": "https://arxiv.org/abs/2602.08290", "authors": ["Ajay Kumar Shrestha"], "title": "Trust-Based Incentive Mechanisms in Semi-Decentralized Federated Learning Systems", "comment": "To appear in the ICBTA 2025 Conference Proceedings and published as a volume of Lecture Notes in Networks and Systems by Springer", "summary": "In federated learning (FL), decentralized model training allows multi-ple participants to collaboratively improve a shared machine learning model without exchanging raw data. However, ensuring the integrity and reliability of the system is challenging due to the presence of potentially malicious or faulty nodes that can degrade the model's performance. This paper proposes a novel trust-based incentive mechanism designed to evaluate and reward the quality of contributions in FL systems. By dynamically assessing trust scores based on fac-tors such as data quality, model accuracy, consistency, and contribution fre-quency, the system encourages honest participation and penalizes unreliable or malicious behavior. These trust scores form the basis of an incentive mechanism that rewards high-trust nodes with greater participation opportunities and penal-ties for low-trust participants. We further explore the integration of blockchain technology and smart contracts to automate the trust evaluation and incentive distribution processes, ensuring transparency and decentralization. Our proposed theoretical framework aims to create a more robust, fair, and transparent FL eco-system, reducing the risks posed by untrustworthy participants.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4fe1\u4efb\u7684\u6fc0\u52b1\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u8bc4\u4f30\u8282\u70b9\u8d21\u732e\u8d28\u91cf\uff08\u6570\u636e\u8d28\u91cf\u3001\u6a21\u578b\u7cbe\u5ea6\u7b49\uff09\u6765\u5956\u52b1\u8bda\u5b9e\u53c2\u4e0e\u5e76\u60e9\u7f5a\u6076\u610f\u884c\u4e3a\uff0c\u7ed3\u5408\u533a\u5757\u94fe\u548c\u667a\u80fd\u5408\u7ea6\u5b9e\u73b0\u900f\u660e\u81ea\u52a8\u5316\u7ba1\u7406\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u5b58\u5728\u6076\u610f\u6216\u6545\u969c\u8282\u70b9\u53ef\u80fd\u964d\u4f4e\u6a21\u578b\u6027\u80fd\u7684\u95ee\u9898\uff0c\u9700\u8981\u786e\u4fdd\u7cfb\u7edf\u5b8c\u6574\u6027\u548c\u53ef\u9760\u6027\uff0c\u4f46\u73b0\u6709\u673a\u5236\u96be\u4ee5\u6709\u6548\u8bc4\u4f30\u548c\u6fc0\u52b1\u9ad8\u8d28\u91cf\u8d21\u732e\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u4fe1\u4efb\u7684\u6fc0\u52b1\u673a\u5236\uff0c\u52a8\u6001\u8bc4\u4f30\u8282\u70b9\u7684\u4fe1\u4efb\u5206\u6570\uff08\u8003\u8651\u6570\u636e\u8d28\u91cf\u3001\u6a21\u578b\u7cbe\u5ea6\u3001\u4e00\u81f4\u6027\u3001\u8d21\u732e\u9891\u7387\u7b49\u56e0\u7d20\uff09\uff0c\u4ee5\u6b64\u4e3a\u57fa\u7840\u5206\u914d\u6fc0\u52b1\uff0c\u5e76\u6574\u5408\u533a\u5757\u94fe\u548c\u667a\u80fd\u5408\u7ea6\u5b9e\u73b0\u81ea\u52a8\u5316\u900f\u660e\u7ba1\u7406\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u65e8\u5728\u521b\u5efa\u66f4\u5065\u58ee\u3001\u516c\u5e73\u3001\u900f\u660e\u7684\u8054\u90a6\u5b66\u4e60\u751f\u6001\u7cfb\u7edf\uff0c\u51cf\u5c11\u4e0d\u53ef\u4fe1\u53c2\u4e0e\u8005\u5e26\u6765\u7684\u98ce\u9669\u3002", "conclusion": "\u63d0\u51fa\u7684\u4fe1\u4efb\u6fc0\u52b1\u673a\u5236\u80fd\u6709\u6548\u4fc3\u8fdb\u8bda\u5b9e\u53c2\u4e0e\u3001\u60e9\u7f5a\u6076\u610f\u884c\u4e3a\uff0c\u7ed3\u5408\u533a\u5757\u94fe\u6280\u672f\u53ef\u589e\u5f3a\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u7684\u5b8c\u6574\u6027\u3001\u53ef\u9760\u6027\u548c\u900f\u660e\u5ea6\u3002"}}
{"id": "2602.08302", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08302", "abs": "https://arxiv.org/abs/2602.08302", "authors": ["Nataraj Das", "Atreya Vedantam", "Chandrashekar Lakshminarayanan"], "title": "Grokking in Linear Models for Logistic Regression", "comment": null, "summary": "Grokking, the phenomenon of delayed generalization, is often attributed to the depth and compositional structure of deep neural networks. We study grokking in one of the simplest possible settings: the learning of a linear model with logistic loss for binary classification on data that are linearly (and max margin) separable about the origin. We investigate three testing regimes: (1) test data drawn from the same distribution as the training data, in which case grokking is not observed; (2) test data concentrated around the margin, in which case grokking is observed; and (3) adversarial test data generated via projected gradient descent (PGD) attacks, in which case grokking is also observed. We theoretically show that the implicit bias of gradient descent induces a three-phase learning process-population-dominated, support-vector-dominated unlearning, and support-vector-dominated generalization-during which delayed generalization can arise. Our analysis further relates the emergence of grokking to asymmetries in the data, both in the number of examples per class and in the distribution of support vectors across classes, and yields a characterization of the grokking time. We experimentally validate our theory by planting different distributions of population points and support vectors, and by analyzing accuracy curves and hyperplane dynamics. Overall, our results demonstrate that grokking does not require depth or representation learning, and can emerge even in linear models through the dynamics of the bias term.", "AI": {"tldr": "\u5728\u7ebf\u6027\u6a21\u578b\u4e2d\u53d1\u73b0\u5ef6\u8fdf\u6cdb\u5316\u73b0\u8c61\uff0c\u8868\u660eGrokking\u4e0d\u9700\u8981\u6df1\u5ea6\u7f51\u7edc\uff0c\u4ec5\u901a\u8fc7\u504f\u7f6e\u9879\u52a8\u6001\u5373\u53ef\u51fa\u73b0", "motivation": "\u7814\u7a76Grokking\uff08\u5ef6\u8fdf\u6cdb\u5316\uff09\u73b0\u8c61\u662f\u5426\u771f\u7684\u9700\u8981\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6df1\u5ea6\u548c\u7ec4\u5408\u7ed3\u6784\uff0c\u63a2\u7d22\u5728\u7ebf\u6027\u6a21\u578b\u8fd9\u79cd\u6700\u7b80\u5355\u8bbe\u7f6e\u4e0b\u662f\u5426\u4e5f\u80fd\u51fa\u73b0Grokking", "method": "\u4f7f\u7528\u903b\u8f91\u635f\u5931\u8fdb\u884c\u4e8c\u5143\u5206\u7c7b\u7684\u7ebf\u6027\u6a21\u578b\uff0c\u7814\u7a76\u4e09\u79cd\u6d4b\u8bd5\u673a\u5236\uff1a\u540c\u5206\u5e03\u6d4b\u8bd5\u6570\u636e\u3001\u96c6\u4e2d\u5728\u8fb9\u7f18\u7684\u6d4b\u8bd5\u6570\u636e\u3001PGD\u5bf9\u6297\u653b\u51fb\u6d4b\u8bd5\u6570\u636e\u3002\u7406\u8bba\u5206\u6790\u68af\u5ea6\u4e0b\u964d\u7684\u9690\u5f0f\u504f\u7f6e\u5982\u4f55\u8bf1\u5bfc\u4e09\u9636\u6bb5\u5b66\u4e60\u8fc7\u7a0b", "result": "\u5728\u7ebf\u6027\u6a21\u578b\u4e2d\u89c2\u5bdf\u5230Grokking\u73b0\u8c61\uff0c\u53d1\u73b0\u6570\u636e\u4e0d\u5bf9\u79f0\u6027\uff08\u7c7b\u522b\u6837\u672c\u6570\u91cf\u548c\u652f\u6491\u5411\u91cf\u5206\u5e03\uff09\u5f71\u54cdGrokking\u7684\u51fa\u73b0\uff0c\u5e76\u7ed9\u51fa\u4e86Grokking\u65f6\u95f4\u7684\u8868\u5f81", "conclusion": "Grokking\u4e0d\u9700\u8981\u6df1\u5ea6\u6216\u8868\u793a\u5b66\u4e60\uff0c\u5373\u4f7f\u5728\u7ebf\u6027\u6a21\u578b\u4e2d\u4e5f\u80fd\u901a\u8fc7\u504f\u7f6e\u9879\u52a8\u6001\u51fa\u73b0\uff0c\u4e3a\u7406\u89e3\u5ef6\u8fdf\u6cdb\u5316\u73b0\u8c61\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6"}}
{"id": "2602.08306", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08306", "abs": "https://arxiv.org/abs/2602.08306", "authors": ["Suizhi Huang", "Mei Li", "Han Yu", "Xiaoxiao Li"], "title": "TextResNet: Decoupling and Routing Optimization Signals in Compound AI Systems via Deep Residual Tuning", "comment": null, "summary": "Textual Gradient-style optimizers (TextGrad) enable gradient-like feedback propagation through compound AI systems. However, they do not work well for deep chains. The root cause of this limitation stems from the Semantic Entanglement problem in these extended workflows. In standard textual backpropagation, feedback signals mix local critiques with upstream contexts, leading to Attribution Ambiguity. To address this challenge, we propose TextResNet, a framework that reformulates the optimization process to achieve precise signal routing via four key innovations. Firstly, in the forward pass, it enforces Additive Semantic Deltas to preserve an Identity Highway for gradient flow. Secondly, in the backward pass, it introduces Semantic Gradient Decomposition via a Semantic Projector to disentangle feedback into causally independent subspaces. Thirdly, it implements Causal Routing, which routes projected signals to their specific components. Finally, it performs Density-Aware Optimization Scheduling to leverage the disentangled signals to dynamically allocate resources to key system bottlenecks. Our results show that TextResNet not only achieves superior performance compared to TextGrad, but also exhibits remarkable stability for agentic tasks in compound AI systems where baselines collapse. Code is available at https://github.com/JeanDiable/TextResNet.", "AI": {"tldr": "TextResNet\u89e3\u51b3\u4e86TextGrad\u5728\u6df1\u5ea6\u94fe\u5f0fAI\u7cfb\u7edf\u4e2d\u56e0\u8bed\u4e49\u7ea0\u7f20\u5bfc\u81f4\u7684\u6027\u80fd\u9650\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u8bed\u4e49\u68af\u5ea6\u5206\u89e3\u548c\u56e0\u679c\u8def\u7531\u5b9e\u73b0\u7cbe\u786e\u4fe1\u53f7\u4f20\u9012", "motivation": "TextGrad\u7b49\u6587\u672c\u68af\u5ea6\u4f18\u5316\u5668\u5728\u6df1\u5ea6\u94fe\u5f0fAI\u7cfb\u7edf\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u6839\u672c\u539f\u56e0\u662f\u8bed\u4e49\u7ea0\u7f20\u95ee\u9898\u3002\u5728\u6807\u51c6\u6587\u672c\u53cd\u5411\u4f20\u64ad\u4e2d\uff0c\u53cd\u9988\u4fe1\u53f7\u6df7\u5408\u4e86\u5c40\u90e8\u6279\u8bc4\u548c\u4e0a\u6e38\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4\u5f52\u56e0\u6a21\u7cca\u6027\uff0c\u9650\u5236\u4e86\u4f18\u5316\u6548\u679c\u3002", "method": "\u63d0\u51faTextResNet\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u5173\u952e\u521b\u65b0\uff1a1\uff09\u524d\u5411\u4f20\u64ad\u4e2d\u5f3a\u5236\u52a0\u6027\u8bed\u4e49\u589e\u91cf\uff0c\u4fdd\u7559\u68af\u5ea6\u6d41\u7684\u6052\u7b49\u9ad8\u901f\u516c\u8def\uff1b2\uff09\u540e\u5411\u4f20\u64ad\u4e2d\u901a\u8fc7\u8bed\u4e49\u6295\u5f71\u5668\u8fdb\u884c\u8bed\u4e49\u68af\u5ea6\u5206\u89e3\uff0c\u5c06\u53cd\u9988\u89e3\u8026\u5230\u56e0\u679c\u72ec\u7acb\u5b50\u7a7a\u95f4\uff1b3\uff09\u56e0\u679c\u8def\u7531\uff0c\u5c06\u6295\u5f71\u4fe1\u53f7\u8def\u7531\u5230\u7279\u5b9a\u7ec4\u4ef6\uff1b4\uff09\u5bc6\u5ea6\u611f\u77e5\u4f18\u5316\u8c03\u5ea6\uff0c\u5229\u7528\u89e3\u8026\u4fe1\u53f7\u52a8\u6001\u5206\u914d\u8d44\u6e90\u5230\u7cfb\u7edf\u74f6\u9888\u3002", "result": "TextResNet\u4e0d\u4ec5\u6bd4TextGrad\u8868\u73b0\u66f4\u4f18\uff0c\u800c\u4e14\u5728\u590d\u5408AI\u7cfb\u7edf\u7684\u4ee3\u7406\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u7a33\u5b9a\u6027\uff0c\u800c\u57fa\u7ebf\u65b9\u6cd5\u4f1a\u5d29\u6e83\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "TextResNet\u901a\u8fc7\u89e3\u51b3\u8bed\u4e49\u7ea0\u7f20\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6df1\u5ea6\u94fe\u5f0fAI\u7cfb\u7edf\u4e2d\u66f4\u7cbe\u786e\u7684\u68af\u5ea6\u4fe1\u53f7\u4f20\u9012\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u68af\u5ea6\u4f18\u5316\u7684\u6548\u679c\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2602.08307", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08307", "abs": "https://arxiv.org/abs/2602.08307", "authors": ["Mengxiao Zhang", "Yuheng Zhang", "Haipeng Luo", "Paul Mineiro"], "title": "Interaction-Grounded Learning for Contextual Markov Decision Processes with Personalized Feedback", "comment": null, "summary": "In this paper, we study Interaction-Grounded Learning (IGL) [Xie et al., 2021], a paradigm designed for realistic scenarios where the learner receives indirect feedback generated by an unknown mechanism, rather than explicit numerical rewards. While prior work on IGL provides efficient algorithms with provable guarantees, those results are confined to single-step settings, restricting their applicability to modern sequential decision-making systems such as multi-turn Large Language Model (LLM) deployments. To bridge this gap, we propose a computationally efficient algorithm that achieves a sublinear regret guarantee for contextual episodic Markov Decision Processes (MDPs) with personalized feedback. Technically, we extend the reward-estimator construction of Zhang et al. [2024a] from the single-step to the multi-step setting, addressing the unique challenges of decoding latent rewards under MDPs. Building on this estimator, we design an Inverse-Gap-Weighting (IGW) algorithm for policy optimization. Finally, we demonstrate the effectiveness of our method in learning personalized objectives from multi-turn interactions through experiments on both a synthetic episodic MDP and a real-world user booking dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u6b65\u987a\u5e8f\u51b3\u7b56\u7684\u4ea4\u4e92\u5f0f\u57fa\u7840\u5b66\u4e60\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u4ece\u95f4\u63a5\u53cd\u9988\u4e2d\u5b66\u4e60\u4e2a\u6027\u5316\u76ee\u6807\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4ea4\u4e92\u5f0f\u57fa\u7840\u5b66\u4e60\uff08IGL\uff09\u7814\u7a76\u4e3b\u8981\u5c40\u9650\u4e8e\u5355\u6b65\u8bbe\u7f6e\uff0c\u65e0\u6cd5\u9002\u7528\u4e8e\u73b0\u4ee3\u987a\u5e8f\u51b3\u7b56\u7cfb\u7edf\uff08\u5982\u591a\u8f6eLLM\u90e8\u7f72\uff09\u3002\u9700\u8981\u5c06IGL\u6269\u5c55\u5230\u591a\u6b65\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u4e2d\uff0c\u4ee5\u5904\u7406\u4ece\u95f4\u63a5\u53cd\u9988\u4e2d\u5b66\u4e60\u4e2a\u6027\u5316\u76ee\u6807\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8ba1\u7b97\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u5c06Zhang\u7b49\u4eba\uff082024a\uff09\u7684\u5956\u52b1\u4f30\u8ba1\u5668\u4ece\u5355\u6b65\u6269\u5c55\u5230\u591a\u6b65\u8bbe\u7f6e\uff0c\u89e3\u51b3MDP\u4e0b\u89e3\u7801\u6f5c\u5728\u5956\u52b1\u7684\u72ec\u7279\u6311\u6218\u3002\u57fa\u4e8e\u6b64\u4f30\u8ba1\u5668\uff0c\u8bbe\u8ba1\u4e86\u9006\u95f4\u9699\u52a0\u6743\uff08IGW\uff09\u7b97\u6cd5\u8fdb\u884c\u7b56\u7565\u4f18\u5316\u3002", "result": "\u7b97\u6cd5\u5728\u4e0a\u4e0b\u6587\u60c5\u666fMDP\u4e2d\u5b9e\u73b0\u4e86\u6b21\u7ebf\u6027\u9057\u61be\u4fdd\u8bc1\u3002\u5728\u5408\u6210\u60c5\u666fMDP\u548c\u771f\u5b9e\u7528\u6237\u9884\u8ba2\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u4ece\u591a\u8f6e\u4ea4\u4e92\u4e2d\u5b66\u4e60\u4e2a\u6027\u5316\u76ee\u6807\u3002", "conclusion": "\u6210\u529f\u5c06\u4ea4\u4e92\u5f0f\u57fa\u7840\u5b66\u4e60\u6269\u5c55\u5230\u591a\u6b65\u987a\u5e8f\u51b3\u7b56\u8bbe\u7f6e\uff0c\u4e3a\u4ece\u95f4\u63a5\u53cd\u9988\u4e2d\u5b66\u4e60\u4e2a\u6027\u5316\u76ee\u6807\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u73b0\u4ee3\u51b3\u7b56\u7cfb\u7edf\u5982\u591a\u8f6eLLM\u90e8\u7f72\u3002"}}
{"id": "2602.08315", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08315", "abs": "https://arxiv.org/abs/2602.08315", "authors": ["Shunyu Zhao", "Yanfeng Yang", "Shuai Li", "Kenji Fukumizu"], "title": "Fast Flow Matching based Conditional Independence Tests for Causal Discovery", "comment": null, "summary": "Constraint-based causal discovery methods require a large number of conditional independence (CI) tests, which severely limits their practical applicability due to high computational complexity. Therefore, it is crucial to design an algorithm that accelerates each individual test. To this end, we propose the Flow Matching-based Conditional Independence Test (FMCIT). The proposed test leverages the high computational efficiency of flow matching and requires the model to be trained only once throughout the entire causal discovery procedure, substantially accelerating causal discovery. According to numerical experiments, FMCIT effectively controls type-I error and maintains high testing power under the alternative hypothesis, even in the presence of high-dimensional conditioning sets. In addition, we further integrate FMCIT into a two-stage guided PC skeleton learning framework, termed GPC-FMCIT, which combines fast screening with guided, budgeted refinement using FMCIT. This design yields explicit bounds on the number of CI queries while maintaining high statistical power. Experiments on synthetic and real-world causal discovery tasks demonstrate favorable accuracy-efficiency trade-offs over existing CI testing methods and PC variants.", "AI": {"tldr": "\u63d0\u51faFMCIT\u65b9\u6cd5\uff0c\u5229\u7528\u6d41\u5339\u914d\u6280\u672f\u52a0\u901f\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5\uff0c\u663e\u8457\u63d0\u5347\u56e0\u679c\u53d1\u73b0\u6548\u7387\uff0c\u5e76\u6574\u5408\u5230GPC-FMCIT\u6846\u67b6\u4e2d\u5b9e\u73b0\u51c6\u786e\u6027\u4e0e\u6548\u7387\u7684\u826f\u597d\u5e73\u8861\u3002", "motivation": "\u57fa\u4e8e\u7ea6\u675f\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u9700\u8981\u8fdb\u884c\u5927\u91cf\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u8bbe\u8ba1\u80fd\u52a0\u901f\u6bcf\u4e2a\u72ec\u7acb\u6d4b\u8bd5\u7684\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5(FMCIT)\uff0c\u5229\u7528\u6d41\u5339\u914d\u7684\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u6a21\u578b\u5728\u6574\u4e2a\u56e0\u679c\u53d1\u73b0\u8fc7\u7a0b\u4e2d\u53ea\u9700\u8bad\u7ec3\u4e00\u6b21\u3002\u8fdb\u4e00\u6b65\u5c06FMCIT\u6574\u5408\u5230\u4e24\u9636\u6bb5\u5f15\u5bfcPC\u9aa8\u67b6\u5b66\u4e60\u6846\u67b6(GPC-FMCIT)\u4e2d\uff0c\u7ed3\u5408\u5feb\u901f\u7b5b\u9009\u548c\u5f15\u5bfc\u5f0f\u9884\u7b97\u7cbe\u70bc\u3002", "result": "FMCIT\u80fd\u6709\u6548\u63a7\u5236I\u578b\u9519\u8bef\uff0c\u5728\u9ad8\u7ef4\u6761\u4ef6\u96c6\u4e0b\u4ecd\u4fdd\u6301\u8f83\u9ad8\u7684\u6d4b\u8bd5\u529f\u6548\u3002GPC-FMCIT\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u56e0\u679c\u53d1\u73b0\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709CI\u6d4b\u8bd5\u65b9\u6cd5\u548cPC\u53d8\u4f53\u7684\u51c6\u786e\u6027\u4e0e\u6548\u7387\u5e73\u8861\u3002", "conclusion": "FMCIT\u901a\u8fc7\u6d41\u5339\u914d\u6280\u672f\u663e\u8457\u52a0\u901f\u4e86\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5\uff0cGPC-FMCIT\u6846\u67b6\u5728\u4fdd\u8bc1\u7edf\u8ba1\u529f\u6548\u7684\u540c\u65f6\u9650\u5236\u4e86CI\u67e5\u8be2\u6570\u91cf\uff0c\u4e3a\u56e0\u679c\u53d1\u73b0\u63d0\u4f9b\u4e86\u9ad8\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08324", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08324", "abs": "https://arxiv.org/abs/2602.08324", "authors": ["Yuntian Tang", "Bohan Jia", "Wenxuan Huang", "Lianyue Zhang", "Jiao Xie", "Wenxi Li", "Wei Li", "Jie Hu", "Xinghao Chen", "Rongrong Ji", "Shaohui Lin"], "title": "Towards Efficient Large Language Reasoning Models via Extreme-Ratio Chain-of-Thought Compression", "comment": "15 pages, 7 figures", "summary": "Chain-of-Thought (CoT) reasoning successfully enhances the reasoning capabilities of Large Language Models (LLMs), yet it incurs substantial computational overhead for inference. Existing CoT compression methods often suffer from a critical loss of logical fidelity at high compression ratios, resulting in significant performance degradation. To achieve high-fidelity, fast reasoning, we propose a novel EXTreme-RAtio Chain-of-Thought Compression framework, termed Extra-CoT, which aggressively reduces the token budget while preserving answer accuracy. To generate reliable, high-fidelity supervision, we first train a dedicated semantically-preserved compressor on mathematical CoT data with fine-grained annotations. An LLM is then fine-tuned on these compressed pairs via a mixed-ratio supervised fine-tuning (SFT), teaching it to follow a spectrum of compression budgets and providing a stable initialization for reinforcement learning (RL). We further propose Constrained and Hierarchical Ratio Policy Optimization (CHRPO) to explicitly incentivize question-solving ability under lower budgets by a hierarchical reward. Experiments on three mathematical reasoning benchmarks show the superiority of Extra-CoT. For example, on MATH-500 using Qwen3-1.7B, Extra-CoT achieves over 73\\% token reduction with an accuracy improvement of 0.6\\%, significantly outperforming state-of-the-art (SOTA) methods.", "AI": {"tldr": "Extra-CoT\u6846\u67b6\u901a\u8fc7\u6781\u7aef\u6bd4\u7387\u538b\u7f29\u601d\u7ef4\u94fe\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u63a8\u7406\u8ba1\u7b97\u5f00\u9500", "motivation": "\u73b0\u6709\u601d\u7ef4\u94fe\u538b\u7f29\u65b9\u6cd5\u5728\u9ad8\u538b\u7f29\u6bd4\u4e0b\u4f1a\u4e25\u91cd\u635f\u5931\u903b\u8f91\u4fdd\u771f\u5ea6\uff0c\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u7684\u5feb\u901f\u63a8\u7406", "method": "1) \u8bad\u7ec3\u4e13\u95e8\u7684\u8bed\u4e49\u4fdd\u7559\u538b\u7f29\u5668\u751f\u6210\u9ad8\u4fdd\u771f\u76d1\u7763\u6570\u636e\uff1b2) \u6df7\u5408\u6bd4\u7387\u76d1\u7763\u5fae\u8c03LLM\u9002\u5e94\u4e0d\u540c\u538b\u7f29\u9884\u7b97\uff1b3) \u63d0\u51faCHRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u5956\u52b1\u6fc0\u52b1\u4f4e\u9884\u7b97\u4e0b\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b", "result": "\u5728\u4e09\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f8b\u5982\u5728MATH-500\u4e0a\u4f7f\u7528Qwen3-1.7B\u5b9e\u73b0\u4e86\u8d85\u8fc773%\u7684token\u51cf\u5c11\uff0c\u540c\u65f6\u51c6\u786e\u7387\u63d0\u53470.6%\uff0c\u663e\u8457\u4f18\u4e8eSOTA\u65b9\u6cd5", "conclusion": "Extra-CoT\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u6781\u7aef\u6bd4\u7387\u4e0b\u7684\u9ad8\u4fdd\u771f\u601d\u7ef4\u94fe\u538b\u7f29\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u4e86\u63a8\u7406\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.08329", "categories": ["cs.LG", "cs.AI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.08329", "abs": "https://arxiv.org/abs/2602.08329", "authors": ["Yifei Gao", "Lei Wang", "Rong-Cheng Tu", "Qixin Zhang", "Jun Cheng", "Dacheng Tao"], "title": "Near-Oracle KV Selection via Pre-hoc Sparsity for Long-Context Inference", "comment": "An effective method for accelerating LLM's inference via selective KV processing", "summary": "A core bottleneck in large language model (LLM) inference is the cost of attending over the ever-growing key-value (KV) cache. Although near-oracle top-k KV selection can preserve the quality of dense attention while sharply reducing computation and bandwidth, existing sparse methods generally rely on posterior heuristics, i.e., selectors conditioned on observed attention or proxy scores. Such conditioning introduces posterior bias: it tends to distort true token importance and miss salient tokens, thereby impairing long-range reasoning. To tackle this problem, we propose Pre-hoc Sparsity (PrHS), which selects KV entries before attention scoring and provides explicit accuracy control. Let the attention mass of discarded entries be delta (the dropped mass). Through a marginal-to-mutual-information analysis, we derive an upper bound on the mutual-information loss that depends only on the dropped mass. This relation explains failure modes of posterior heuristics and enables verifiable guarantees by controlling the dropped mass in advance. Within PrHS, we instantiate three orthogonal pre-hoc selectors along the axes of time, depth, and layer. Extensive experiments on LLaMA and Mistral families validate PrHS. Across GSM8K and CoQA, PrHS reduces retrieval overhead by over 90%, achieving 3x higher retrieval sparsity than HShare at matched or better accuracy. It incurs under 1% average degradation on LongBench, lowers attention FLOPs by about 15% versus prior sparse baselines, and yields a 9.9x speedup in attention-operator latency and 2.8x higher throughput on NVIDIA A100-80GB GPUs than the dense baseline.", "AI": {"tldr": "\u63d0\u51faPre-hoc Sparsity (PrHS)\u65b9\u6cd5\uff0c\u5728\u6ce8\u610f\u529b\u8bc4\u5206\u524d\u9009\u62e9KV\u7f13\u5b58\u6761\u76ee\uff0c\u901a\u8fc7\u63a7\u5236\u4e22\u5f03\u8d28\u91cf(delta)\u63d0\u4f9b\u663e\u5f0f\u7cbe\u5ea6\u4fdd\u8bc1\uff0c\u76f8\u6bd4\u540e\u9a8c\u542f\u53d1\u5f0f\u65b9\u6cd5\u51cf\u5c11\u504f\u5dee\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u5e76\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u6838\u5fc3\u74f6\u9888\u662f\u4e0d\u65ad\u589e\u957f\u7684KV\u7f13\u5b58\u6ce8\u610f\u529b\u8ba1\u7b97\u6210\u672c\u3002\u73b0\u6709\u7a00\u758f\u65b9\u6cd5\u4f9d\u8d56\u540e\u9a8c\u542f\u53d1\u5f0f\uff08\u57fa\u4e8e\u89c2\u5bdf\u5230\u7684\u6ce8\u610f\u529b\u6216\u4ee3\u7406\u5206\u6570\uff09\uff0c\u8fd9\u5f15\u5165\u540e\u9a8c\u504f\u5dee\uff1a\u626d\u66f2\u771f\u5b9etoken\u91cd\u8981\u6027\u5e76\u9057\u6f0f\u5173\u952etoken\uff0c\u635f\u5bb3\u957f\u7a0b\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faPre-hoc Sparsity (PrHS)\uff0c\u5728\u6ce8\u610f\u529b\u8bc4\u5206\u524d\u9009\u62e9KV\u6761\u76ee\uff0c\u901a\u8fc7\u63a7\u5236\u4e22\u5f03\u8d28\u91cf(delta)\u63d0\u4f9b\u663e\u5f0f\u7cbe\u5ea6\u63a7\u5236\u3002\u901a\u8fc7\u8fb9\u9645\u5230\u4e92\u4fe1\u606f\u5206\u6790\uff0c\u63a8\u5bfc\u51fa\u4ec5\u4f9d\u8d56\u4e8e\u4e22\u5f03\u8d28\u91cf\u7684\u4e92\u4fe1\u606f\u635f\u5931\u4e0a\u754c\u3002\u5728PrHS\u6846\u67b6\u5185\uff0c\u6cbf\u65f6\u95f4\u3001\u6df1\u5ea6\u548c\u5c42\u4e09\u4e2a\u6b63\u4ea4\u8f74\u5b9e\u4f8b\u5316\u4e09\u79cd\u9884\u9009\u5668\u3002", "result": "\u5728LLaMA\u548cMistral\u7cfb\u5217\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff1aGSM8K\u548cCoQA\u4e0a\u51cf\u5c11\u8d85\u8fc790%\u68c0\u7d22\u5f00\u9500\uff0c\u6bd4HShare\u5b9e\u73b03\u500d\u66f4\u9ad8\u7684\u68c0\u7d22\u7a00\u758f\u5ea6\u4e14\u7cbe\u5ea6\u76f8\u5f53\u6216\u66f4\u597d\uff1bLongBench\u4e0a\u5e73\u5747\u9000\u5316\u4f4e\u4e8e1%\uff1b\u6bd4\u5148\u524d\u7a00\u758f\u57fa\u7ebf\u964d\u4f4e\u7ea615%\u6ce8\u610f\u529bFLOPs\uff1b\u5728NVIDIA A100-80GB GPU\u4e0a\u5b9e\u73b09.9\u500d\u6ce8\u610f\u529b\u7b97\u5b50\u5ef6\u8fdf\u52a0\u901f\u548c2.8\u500d\u541e\u5410\u91cf\u63d0\u5347\u3002", "conclusion": "PrHS\u901a\u8fc7\u9884\u9009KV\u6761\u76ee\u548c\u63a7\u5236\u4e22\u5f03\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u540e\u9a8c\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u7cbe\u5ea6\u4fdd\u8bc1\uff0c\u663e\u8457\u964d\u4f4e\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u8ba1\u7b97\u548c\u5e26\u5bbd\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2602.08333", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08333", "abs": "https://arxiv.org/abs/2602.08333", "authors": ["Cristian P\u00e9rez-Corral", "Alberto Fern\u00e1ndez-Hern\u00e1ndez", "Jose I. Mestre", "Manuel F. Dolz", "Jose Duato", "Enrique S. Quintana-Ort\u00ed"], "title": "Regime Change Hypothesis: Foundations for Decoupled Dynamics in Neural Network Training", "comment": "8 pages, 1 figure", "summary": "Despite the empirical success of DNN, their internal training dynamics remain difficult to characterize. In ReLU-based models, the activation pattern induced by a given input determines the piecewise-linear region in which the network behaves affinely. Motivated by this geometry, we investigate whether training exhibits a two-timescale behavior: an early stage with substantial changes in activation patterns and a later stage where weight updates predominantly refine the model within largely stable activation regimes. We first prove a local stability property: outside measure-zero sets of parameters and inputs, sufficiently small parameter perturbations preserve the activation pattern of a fixed input, implying locally affine behavior within activation regions. We then empirically track per-iteration changes in weights and activation patterns across fully-connected and convolutional architectures, as well as Transformer-based models, where activation patterns are recorded in the ReLU feed-forward (MLP/FFN) submodules, using fixed validation subsets. Across the evaluated settings, activation-pattern changes decay 3 times earlier than weight-update magnitudes, showing that late-stage training often proceeds within relatively stable activation regimes. These findings provide a concrete, architecture-agnostic instrument for monitoring training dynamics and motivate further study of decoupled optimization strategies for piecewise-linear networks. For reproducibility, code and experiment configurations will be released upon acceptance.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86ReLU\u7f51\u7edc\u7684\u8bad\u7ec3\u52a8\u6001\uff0c\u53d1\u73b0\u5b58\u5728\u4e24\u9636\u6bb5\u884c\u4e3a\uff1a\u65e9\u671f\u6fc0\u6d3b\u6a21\u5f0f\u53d8\u5316\u663e\u8457\uff0c\u540e\u671f\u6743\u91cd\u66f4\u65b0\u4e3b\u8981\u5728\u7a33\u5b9a\u7684\u6fc0\u6d3b\u533a\u57df\u5185\u8fdb\u884c\u5fae\u8c03\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u53d6\u5f97\u4e86\u7ecf\u9a8c\u4e0a\u7684\u6210\u529f\uff0c\u4f46\u5176\u5185\u90e8\u8bad\u7ec3\u52a8\u6001\u4ecd\u96be\u4ee5\u8868\u5f81\u3002ReLU\u6a21\u578b\u4e2d\u7684\u6fc0\u6d3b\u6a21\u5f0f\u51b3\u5b9a\u4e86\u7f51\u7edc\u884c\u4e3a\u7684\u4eff\u5c04\u533a\u57df\uff0c\u4f5c\u8005\u5e0c\u671b\u63a2\u7a76\u8bad\u7ec3\u662f\u5426\u8868\u73b0\u51fa\u4e24\u9636\u6bb5\u884c\u4e3a\uff1a\u65e9\u671f\u6fc0\u6d3b\u6a21\u5f0f\u53d8\u5316\u663e\u8457\uff0c\u540e\u671f\u6743\u91cd\u66f4\u65b0\u4e3b\u8981\u5728\u7a33\u5b9a\u7684\u6fc0\u6d3b\u533a\u57df\u5185\u8fdb\u884c\u5fae\u8c03\u3002", "method": "\u9996\u5148\u8bc1\u660e\u4e86\u5c40\u90e8\u7a33\u5b9a\u6027\u6027\u8d28\uff1a\u5728\u53c2\u6570\u548c\u8f93\u5165\u7684\u6d4b\u5ea6\u96f6\u96c6\u4e4b\u5916\uff0c\u8db3\u591f\u5c0f\u7684\u53c2\u6570\u6270\u52a8\u4f1a\u4fdd\u6301\u56fa\u5b9a\u8f93\u5165\u7684\u6fc0\u6d3b\u6a21\u5f0f\u3002\u7136\u540e\u901a\u8fc7\u5b9e\u9a8c\u8ffd\u8e2a\u5168\u8fde\u63a5\u3001\u5377\u79ef\u548cTransformer\u67b6\u6784\u4e2d\u6bcf\u6b21\u8fed\u4ee3\u7684\u6743\u91cd\u548c\u6fc0\u6d3b\u6a21\u5f0f\u53d8\u5316\uff0c\u5176\u4e2d\u6fc0\u6d3b\u6a21\u5f0f\u8bb0\u5f55\u5728ReLU\u524d\u9988\u5b50\u6a21\u5757\u4e2d\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u6fc0\u6d3b\u6a21\u5f0f\u53d8\u5316\u6bd4\u6743\u91cd\u66f4\u65b0\u5e45\u5ea6\u65e93\u500d\u8870\u51cf\uff0c\u8868\u660e\u540e\u671f\u8bad\u7ec3\u901a\u5e38\u5728\u76f8\u5bf9\u7a33\u5b9a\u7684\u6fc0\u6d3b\u533a\u57df\u5185\u8fdb\u884c\u3002\u8fd9\u4e3a\u76d1\u63a7\u8bad\u7ec3\u52a8\u6001\u63d0\u4f9b\u4e86\u67b6\u6784\u65e0\u5173\u7684\u5177\u4f53\u5de5\u5177\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86ReLU\u7f51\u7edc\u8bad\u7ec3\u7684\u4e24\u9636\u6bb5\u52a8\u6001\uff0c\u4e3a\u7406\u89e3\u8bad\u7ec3\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u6fc0\u52b1\u4e86\u5bf9\u5206\u6bb5\u7ebf\u6027\u7f51\u7edc\u89e3\u8026\u4f18\u5316\u7b56\u7565\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002\u4ee3\u7801\u548c\u5b9e\u9a8c\u914d\u7f6e\u5c06\u5728\u63a5\u53d7\u540e\u53d1\u5e03\u4ee5\u786e\u4fdd\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2602.08343", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08343", "abs": "https://arxiv.org/abs/2602.08343", "authors": ["Debajyoti Datta", "Trishala Neeraj", "Bibek Paudel", "Vyom Sharma", "Subhabrata Mukherjee"], "title": "ManifoldKV: Training-Free KV Cache Compression via Euclidean Outlier Detection", "comment": "18 pages, 5 figures, 18 tables", "summary": "Long-context inference is constrained by KV-cache memory, which grows linearly with sequence length; KV-cache compression therefore hinges on reliably selecting which past tokens to retain. Most geometry-based eviction methods score keys by cosine similarity to a global centroid, but cosine is scale-invariant and can discard magnitude cues that distinguish semantically salient tokens. We propose ManifoldKV, a training-free scorer that ranks tokens by Euclidean distance to the key centroid, capturing both angular and radial deviations.\n  On the RULER benchmark, ManifoldKV achieves 95.7% accuracy at 4K-16K contexts with 20% compression; matching the best geometric baseline while improving robustness in two regimes where cosine scoring fails. First, on multi-key retrieval, ManifoldKV reduces directional collisions, achieving 92.4% vs KeyDiff's 77.0% (+15.4 points) on 3-key NIAH at 50% compression. Second, to address dilution and performance collapse of global centroids at 64K context, we introduce WindowedManifoldKV, which restores accuracy to 84.3% at 25% compression, a 49-point recovery over global L2 and +3.2 points over KeyDiff. The method requires only 3 lines of code and works across 4 architectures without tuning.", "AI": {"tldr": "ManifoldKV\uff1a\u4e00\u79cd\u57fa\u4e8e\u6b27\u6c0f\u8ddd\u79bb\u7684KV\u7f13\u5b58\u538b\u7f29\u65b9\u6cd5\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u901a\u8fc7\u540c\u65f6\u8003\u8651\u89d2\u5ea6\u548c\u5f84\u5411\u504f\u5dee\u6765\u9009\u62e9\u4fdd\u7559\u7684\u5173\u952etoken\uff0c\u76f8\u6bd4\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u65b9\u6cd5\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u53d7\u9650\u4e8eKV\u7f13\u5b58\u5185\u5b58\u7684\u7ebf\u6027\u589e\u957f\uff0c\u73b0\u6709\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u51e0\u4f55\u9a71\u9010\u65b9\u6cd5\u4f1a\u4e22\u5931\u5e45\u5ea6\u4fe1\u606f\uff0c\u65e0\u6cd5\u533a\u5206\u8bed\u4e49\u4e0a\u91cd\u8981\u7684token\u3002", "method": "\u63d0\u51faManifoldKV\u8bad\u7ec3\u81ea\u7531\u8bc4\u5206\u5668\uff0c\u57fa\u4e8etoken\u5230\u5173\u952e\u8d28\u5fc3\u7684\u6b27\u6c0f\u8ddd\u79bb\u8fdb\u884c\u6392\u5e8f\uff0c\u540c\u65f6\u6355\u6349\u89d2\u5ea6\u548c\u5f84\u5411\u504f\u5dee\u3002\u9488\u5bf964K\u957f\u4e0a\u4e0b\u6587\u8fd8\u63d0\u51faWindowedManifoldKV\u53d8\u4f53\u3002", "result": "\u5728RULER\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c4K-16K\u4e0a\u4e0b\u6587\u4e0b20%\u538b\u7f29\u7387\u5b9e\u73b095.7%\u51c6\u786e\u7387\uff1b\u591a\u952e\u68c0\u7d22\u4efb\u52a1\u4e2d\u6bd4KeyDiff\u63d0\u534715.4\u4e2a\u767e\u5206\u70b9\uff1b64K\u4e0a\u4e0b\u6587\u4e0bWindowedManifoldKV\u6062\u590d84.3%\u51c6\u786e\u7387\u3002", "conclusion": "ManifoldKV\u901a\u8fc7\u6b27\u6c0f\u8ddd\u79bb\u8bc4\u5206\u6709\u6548\u89e3\u51b3\u4e86\u4f59\u5f26\u76f8\u4f3c\u5ea6\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4ec5\u97003\u884c\u4ee3\u7801\u5373\u53ef\u57284\u79cd\u67b6\u6784\u4e0a\u65e0\u9700\u8c03\u4f18\u5730\u5de5\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u957f\u4e0a\u4e0b\u6587KV\u7f13\u5b58\u538b\u7f29\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.08350", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08350", "abs": "https://arxiv.org/abs/2602.08350", "authors": ["Tal Burla", "Roi Livni"], "title": "All ERMs Can Fail in Stochastic Convex Optimization Lower Bounds in Linear Dimension", "comment": null, "summary": "We study the sample complexity of the best-case Empirical Risk Minimizer in the setting of stochastic convex optimization. We show that there exists an instance in which the sample size is linear in the dimension, learning is possible, but the Empirical Risk Minimizer is likely to be unique and to overfit. This resolves an open question by Feldman. We also extend this to approximate ERMs.\n  Building on our construction we also show that (constrained) Gradient Descent potentially overfits when horizon and learning rate grow w.r.t sample size. Specifically we provide a novel generalization lower bound of $\u03a9\\left(\\sqrt{\u03b7T/m^{1.5}}\\right)$ for Gradient Descent, where $\u03b7$ is the learning rate, $T$ is the horizon and $m$ is the sample size. This narrows down, exponentially, the gap between the best known upper bound of $O(\u03b7T/m)$ and existing lower bounds from previous constructions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u968f\u673a\u51f8\u4f18\u5316\u4e2d\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u5668\uff08ERM\uff09\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u8bc1\u660e\u4e86\u5b58\u5728ERM\u552f\u4e00\u4e14\u4f1a\u8fc7\u62df\u5408\u7684\u5b9e\u4f8b\uff0c\u89e3\u51b3\u4e86Feldman\u7684\u5f00\u653e\u95ee\u9898\uff0c\u5e76\u7ed9\u51fa\u4e86\u68af\u5ea6\u4e0b\u964d\u7684\u6cdb\u5316\u4e0b\u754c\u3002", "motivation": "\u7814\u7a76\u968f\u673a\u51f8\u4f18\u5316\u4e2d\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u5668\u7684\u6837\u672c\u590d\u6742\u5ea6\u548c\u8fc7\u62df\u5408\u884c\u4e3a\uff0c\u89e3\u51b3Feldman\u63d0\u51fa\u7684\u5f00\u653e\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u7684\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u6784\u9020\u7279\u5b9a\u5b9e\u4f8b\u8bc1\u660eERM\u7684\u8fc7\u62df\u5408\u73b0\u8c61\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u9020\u5206\u6790\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u7684\u6cdb\u5316\u4e0b\u754c\uff0c\u4f7f\u7528\u7406\u8bba\u5206\u6790\u548c\u6570\u5b66\u8bc1\u660e\u65b9\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u5b58\u5728\u6837\u672c\u91cf\u4e0e\u7ef4\u5ea6\u5448\u7ebf\u6027\u5173\u7cfb\u65f6\uff0cERM\u53ef\u80fd\u552f\u4e00\u4e14\u8fc7\u62df\u5408\u7684\u5b9e\u4f8b\uff1b\u7ed9\u51fa\u4e86\u68af\u5ea6\u4e0b\u964d\u7684\u6cdb\u5316\u4e0b\u754c\u03a9(\u221a(\u03b7T/m^1.5))\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u73b0\u6709\u4e0a\u4e0b\u754c\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u5668\u5728\u968f\u673a\u51f8\u4f18\u5316\u4e2d\u53ef\u80fd\u8fc7\u62df\u5408\uff0c\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4e5f\u5b58\u5728\u6cdb\u5316\u98ce\u9669\uff0c\u8fd9\u4e3a\u7406\u89e3\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u6cdb\u5316\u6027\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6d1e\u89c1\u3002"}}
{"id": "2602.08351", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08351", "abs": "https://arxiv.org/abs/2602.08351", "authors": ["Zhiliang Chen", "Alfred Wei Lun Leong", "Shao Yong Ong", "Apivich Hemachandram", "Gregory Kang Ruey Lau", "Chuan-Sheng Foo", "Zhengyuan Liu", "Nancy F. Chen", "Bryan Kian Hsiang Low"], "title": "The Chicken and Egg Dilemma: Co-optimizing Data and Model Configurations for LLMs", "comment": null, "summary": "Co-optimizing data and model configurations for training LLMs presents a classic chicken-and-egg dilemma: The best training data configuration (e.g., data mixture) for a downstream task depends on the chosen model configuration (e.g., model architecture), and vice versa. However, jointly optimizing both data and model configurations is often deemed intractable, and existing methods focus on either data or model optimization without considering their interaction. We introduce JoBS, an approach that uses a scaling-law-inspired performance predictor to aid Bayesian optimization (BO) in jointly optimizing LLM training data and model configurations efficiently. JoBS allocates a portion of the optimization budget to learn an LLM performance predictor that predicts how promising a training configuration is from a small number of training steps. The remaining budget is used to perform BO entirely with the predictor, effectively amortizing the cost of running full-training runs. We study JoBS's average regret and devise the optimal budget allocation to minimize regret. JoBS outperforms existing multi-fidelity BO baselines, as well as data and model optimization approaches across diverse LLM tasks under the same optimization budget.", "AI": {"tldr": "JoBS\uff1a\u4e00\u79cd\u8054\u5408\u4f18\u5316LLM\u8bad\u7ec3\u6570\u636e\u548c\u6a21\u578b\u914d\u7f6e\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u7f29\u653e\u5b9a\u5f8b\u6027\u80fd\u9884\u6d4b\u5668\u8f85\u52a9\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u901a\u8fc7\u90e8\u5206\u9884\u7b97\u5b66\u4e60\u9884\u6d4b\u5668\u6765\u907f\u514d\u5b8c\u6574\u8bad\u7ec3\u8fd0\u884c\u7684\u9ad8\u6210\u672c\u3002", "motivation": "LLM\u8bad\u7ec3\u4e2d\u5b58\u5728\u6570\u636e\u548c\u6a21\u578b\u914d\u7f6e\u7684\"\u9e21\u751f\u86cb\u86cb\u751f\u9e21\"\u56f0\u5883\uff1a\u6700\u4f73\u8bad\u7ec3\u6570\u636e\u914d\u7f6e\u4f9d\u8d56\u4e8e\u6a21\u578b\u914d\u7f6e\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u4f18\u5316\u6570\u636e\u6216\u6a21\u578b\uff0c\u5ffd\u7565\u4e86\u5b83\u4eec\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u800c\u8054\u5408\u4f18\u5316\u53c8\u5f80\u5f80\u88ab\u8ba4\u4e3a\u662f\u4e0d\u53ef\u884c\u7684\u3002", "method": "JoBS\u65b9\u6cd5\u7ed3\u5408\u7f29\u653e\u5b9a\u5f8b\u6027\u80fd\u9884\u6d4b\u5668\u548c\u8d1d\u53f6\u65af\u4f18\u5316\uff1a1\uff09\u5206\u914d\u90e8\u5206\u4f18\u5316\u9884\u7b97\u5b66\u4e60LLM\u6027\u80fd\u9884\u6d4b\u5668\uff0c\u4ec5\u901a\u8fc7\u5c11\u91cf\u8bad\u7ec3\u6b65\u9aa4\u9884\u6d4b\u914d\u7f6e\u7684\u6f5c\u529b\uff1b2\uff09\u5269\u4f59\u9884\u7b97\u4f7f\u7528\u8be5\u9884\u6d4b\u5668\u8fdb\u884c\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u907f\u514d\u5b8c\u6574\u8bad\u7ec3\u8fd0\u884c\u7684\u9ad8\u6210\u672c\uff1b3\uff09\u7814\u7a76\u5e73\u5747\u9057\u61be\u5e76\u8bbe\u8ba1\u6700\u4f18\u9884\u7b97\u5206\u914d\u7b56\u7565\u3002", "result": "JoBS\u5728\u76f8\u540c\u4f18\u5316\u9884\u7b97\u4e0b\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u591a\u4fdd\u771f\u5ea6\u8d1d\u53f6\u65af\u4f18\u5316\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5728\u5404\u79cdLLM\u4efb\u52a1\u4e0a\u7684\u6570\u636e\u548c\u6a21\u578b\u4f18\u5316\u65b9\u6cd5\u3002", "conclusion": "JoBS\u901a\u8fc7\u4f7f\u7528\u6027\u80fd\u9884\u6d4b\u5668\u6765\u6307\u5bfc\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u8bad\u7ec3\u4e2d\u6570\u636e\u548c\u6a21\u578b\u914d\u7f6e\u7684\u8054\u5408\u4f18\u5316\u95ee\u9898\uff0c\u5728\u6709\u9650\u9884\u7b97\u5185\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2602.08372", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.08372", "abs": "https://arxiv.org/abs/2602.08372", "authors": ["Yan-Feng Xie", "Yu-Jie Zhang", "Peng Zhao", "Zhi-Hua Zhou"], "title": "Dynamic Regret via Discounted-to-Dynamic Reduction with Applications to Curved Losses and Adam Optimizer", "comment": null, "summary": "We study dynamic regret minimization in non-stationary online learning, with a primary focus on follow-the-regularized-leader (FTRL) methods. FTRL is important for curved losses and for understanding adaptive optimizers such as Adam, yet existing dynamic regret analyses are less explored for FTRL. To address this, we build on the discounted-to-dynamic reduction and present a modular way to obtain dynamic regret bounds of FTRL-related problems. Specifically, we focus on two representative curved losses: linear regression and logistic regression. Our method not only simplifies existing proofs for the optimal dynamic regret of online linear regression, but also yields new dynamic regret guarantees for online logistic regression. Beyond online convex optimization, we apply the reduction to analyze the Adam optimizers, obtaining optimal convergence rates in stochastic, non-convex, and non-smooth settings. The reduction also enables a more detailed treatment of Adam with two discount parameters $(\u03b2_1,\u03b2_2)$, leading to new results for both clipped and clip-free variants of Adam optimizers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6298\u6263\u5230\u52a8\u6001\u7684\u5f52\u7ea6\u6280\u672f\uff0c\u4e3aFTRL\u65b9\u6cd5\u53ca\u5176\u76f8\u5173\u4f18\u5316\u5668\uff08\u5982Adam\uff09\u5728\u975e\u5e73\u7a33\u5728\u7ebf\u5b66\u4e60\u4e2d\u83b7\u5f97\u52a8\u6001\u9057\u61be\u754c\u3002", "motivation": "FTRL\u65b9\u6cd5\u5bf9\u4e8e\u66f2\u7ebf\u635f\u5931\u548c\u81ea\u9002\u5e94\u4f18\u5316\u5668\uff08\u5982Adam\uff09\u7684\u7406\u89e3\u5f88\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u52a8\u6001\u9057\u61be\u5206\u6790\u5bf9FTRL\u65b9\u6cd5\u7814\u7a76\u8f83\u5c11\u3002\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u5206\u6790FTRL\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u6298\u6263\u5230\u52a8\u6001\u7684\u5f52\u7ea6\u6280\u672f\uff0c\u63d0\u51fa\u6a21\u5757\u5316\u65b9\u6cd5\u83b7\u5f97FTRL\u76f8\u5173\u95ee\u9898\u7684\u52a8\u6001\u9057\u61be\u754c\u3002\u91cd\u70b9\u5173\u6ce8\u7ebf\u6027\u56de\u5f52\u548c\u903b\u8f91\u56de\u5f52\u4e24\u79cd\u4ee3\u8868\u6027\u66f2\u7ebf\u635f\u5931\uff0c\u5e76\u5c06\u8be5\u65b9\u6cd5\u6269\u5c55\u5230Adam\u4f18\u5316\u5668\u7684\u5206\u6790\u3002", "result": "\u7b80\u5316\u4e86\u5728\u7ebf\u7ebf\u6027\u56de\u5f52\u6700\u4f18\u52a8\u6001\u9057\u61be\u7684\u8bc1\u660e\uff0c\u83b7\u5f97\u4e86\u5728\u7ebf\u903b\u8f91\u56de\u5f52\u7684\u65b0\u52a8\u6001\u9057\u61be\u4fdd\u8bc1\u3002\u5728\u968f\u673a\u3001\u975e\u51f8\u3001\u975e\u5149\u6ed1\u8bbe\u7f6e\u4e0b\u4e3aAdam\u4f18\u5316\u5668\u83b7\u5f97\u4e86\u6700\u4f18\u6536\u655b\u7387\uff0c\u5e76\u5bf9\u5e26\u4e24\u4e2a\u6298\u6263\u53c2\u6570\u7684Adam\u8fdb\u884c\u4e86\u66f4\u8be6\u7ec6\u5206\u6790\uff0c\u5f97\u5230\u4e86\u526a\u88c1\u548c\u65e0\u526a\u88c1\u53d8\u4f53\u7684\u65b0\u7ed3\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u5f52\u7ea6\u65b9\u6cd5\u4e3aFTRL\u548cAdam\u4f18\u5316\u5668\u5728\u975e\u5e73\u7a33\u5728\u7ebf\u5b66\u4e60\u4e2d\u7684\u52a8\u6001\u9057\u61be\u5206\u6790\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u4e0d\u4ec5\u7b80\u5316\u4e86\u73b0\u6709\u8bc1\u660e\uff0c\u8fd8\u83b7\u5f97\u4e86\u65b0\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u6269\u5c55\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2602.08376", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08376", "abs": "https://arxiv.org/abs/2602.08376", "authors": ["Xinyu Wang", "Ziyu Zhao", "Peng Lu", "Yu Gu", "Xiao-Wen Chang"], "title": "OJBKQ: Objective-Joint Babai-Klein Quantization", "comment": null, "summary": "Post-training quantization (PTQ) is widely used to compress large language models without retraining. However, many existing weight-only methods rely on heuristic objectives and greedy rounding, thus leading to noticeable degradation under low-bit quantization. In this work, we introduce OJBKQ (Objective-Joint Babai-Klein Quantization with K-Best Sampling), a layer-wise PTQ method that formulates weight quantization as a joint optimization problem over activations and weights. This formulation results in a multiple-right-hand-side box-constrained integer least squares (BILS) problem in each layer, which is NP-hard. For each column of the weight matrix, we apply an extended Babai nearest-plane algorithm and an extended version of Klein's randomized Babai algorithm to find the minimum-residual Babai-Klein point, a sub-optimal solution to the BILS problem. Experimental results on large language models show that OJBKQ achieves lower perplexity at 3-4 bits compared to existing PTQ approaches, while maintaining comparable computational cost.", "AI": {"tldr": "OJBKQ\u662f\u4e00\u79cd\u65b0\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\uff0c\u5c06\u6743\u91cd\u91cf\u5316\u5efa\u6a21\u4e3a\u6fc0\u6d3b\u548c\u6743\u91cd\u7684\u8054\u5408\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u6269\u5c55\u7684Babai-Klein\u7b97\u6cd5\u6c42\u89e3\uff0c\u57283-4\u4f4d\u91cf\u5316\u4e0b\u6bd4\u73b0\u6709\u65b9\u6cd5\u83b7\u5f97\u66f4\u4f4e\u7684\u56f0\u60d1\u5ea6\u3002", "motivation": "\u73b0\u6709\u4ec5\u6743\u91cd\u91cf\u5316\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u76ee\u6807\u548c\u8d2a\u5fc3\u820d\u5165\uff0c\u5bfc\u81f4\u4f4e\u6bd4\u7279\u91cf\u5316\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u66f4\u4f18\u5316\u7684\u91cf\u5316\u65b9\u6cd5\u3002", "method": "\u5c06\u6bcf\u5c42\u6743\u91cd\u91cf\u5316\u5efa\u6a21\u4e3a\u591a\u53f3\u7aef\u76d2\u7ea6\u675f\u6574\u6570\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\uff0c\u4f7f\u7528\u6269\u5c55\u7684Babai\u6700\u8fd1\u5e73\u9762\u7b97\u6cd5\u548cKlein\u968f\u673aBabai\u7b97\u6cd5\u5bfb\u627e\u6700\u5c0f\u6b8b\u5dee\u7684Babai-Klein\u70b9\u4f5c\u4e3a\u6b21\u4f18\u89e3\u3002", "result": "\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOJBKQ\u57283-4\u4f4d\u91cf\u5316\u4e0b\u76f8\u6bd4\u73b0\u6709PTQ\u65b9\u6cd5\u83b7\u5f97\u66f4\u4f4e\u7684\u56f0\u60d1\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u8f83\u7684\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "OJBKQ\u901a\u8fc7\u5c06\u91cf\u5316\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u8054\u5408\u4f18\u5316\u5e76\u91c7\u7528\u6709\u6548\u7684\u7b97\u6cd5\u6c42\u89e3\uff0c\u5728\u4f4e\u6bd4\u7279\u91cf\u5316\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\uff0c\u4e3a\u540e\u8bad\u7ec3\u91cf\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2602.08377", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08377", "abs": "https://arxiv.org/abs/2602.08377", "authors": ["Bilgehan Sel", "Vaishakh Keshava", "Phillip Wallis", "Lukas Rutishauser", "Ming Jin", "Dingcheng Li"], "title": "Reinforcement Learning with Backtracking Feedback", "comment": "NeurIPS 2025", "summary": "Addressing the critical need for robust safety in Large Language Models (LLMs), particularly against adversarial attacks and in-distribution errors, we introduce Reinforcement Learning with Backtracking Feedback (RLBF). This framework advances upon prior methods, such as BSAFE, by primarily leveraging a Reinforcement Learning (RL) stage where models learn to dynamically correct their own generation errors. Through RL with critic feedback on the model's live outputs, LLMs are trained to identify and recover from their actual, emergent safety violations by emitting an efficient \"backtrack by x tokens\" signal, then continuing generation autoregressively. This RL process is crucial for instilling resilience against sophisticated adversarial strategies, including middle filling, Greedy Coordinate Gradient (GCG) attacks, and decoding parameter manipulations. To further support the acquisition of this backtracking capability, we also propose an enhanced Supervised Fine-Tuning (SFT) data generation strategy (BSAFE+). This method improves upon previous data creation techniques by injecting violations into coherent, originally safe text, providing more effective initial training for the backtracking mechanism. Comprehensive empirical evaluations demonstrate that RLBF significantly reduces attack success rates across diverse benchmarks and model scales, achieving superior safety outcomes while critically preserving foundational model utility.", "AI": {"tldr": "RLBF\u6846\u67b6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8ba9LLM\u5b66\u4f1a\u52a8\u6001\u7ea0\u6b63\u81ea\u8eab\u751f\u6210\u9519\u8bef\uff0c\u4f7f\u7528\"\u56de\u6eafx\u4e2atoken\"\u4fe1\u53f7\u6765\u6062\u590d\u5b89\u5168\u8fdd\u89c4\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u6297\u653b\u51fb\u548c\u5206\u5e03\u5185\u9519\u8bef\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9762\u5bf9\u5bf9\u6297\u653b\u51fb\u548c\u5206\u5e03\u5185\u9519\u8bef\u65f6\u7684\u5b89\u5168\u9c81\u68d2\u6027\u4e0d\u8db3\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5982BSAFE\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "1. \u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\uff1a\u6a21\u578b\u901a\u8fc7critic\u53cd\u9988\u5b66\u4e60\u52a8\u6001\u7ea0\u6b63\u751f\u6210\u9519\u8bef\uff0c\u53d1\u51fa\"\u56de\u6eafx\u4e2atoken\"\u4fe1\u53f7\u540e\u7ee7\u7eed\u81ea\u56de\u5f52\u751f\u6210\uff1b2. \u6539\u8fdb\u7684\u76d1\u7763\u5fae\u8c03\u6570\u636e\u751f\u6210\u7b56\u7565(BSAFE+)\uff1a\u5728\u539f\u672c\u5b89\u5168\u7684\u8fde\u8d2f\u6587\u672c\u4e2d\u6ce8\u5165\u8fdd\u89c4\u5185\u5bb9\uff0c\u4e3a\u56de\u6eaf\u673a\u5236\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u521d\u59cb\u8bad\u7ec3\u3002", "result": "RLBF\u663e\u8457\u964d\u4f4e\u4e86\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u89c4\u6a21\u4e0b\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u57fa\u7840\u529f\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u5b89\u5168\u6027\u80fd\u3002", "conclusion": "RLBF\u6846\u67b6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\u52a8\u6001\u7ea0\u6b63\u9519\u8bef\u7684\u80fd\u529b\uff0c\u7ed3\u5408\u6539\u8fdb\u7684\u6570\u636e\u751f\u6210\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u5bf9\u6297\u590d\u6742\u653b\u51fb\u7684\u5b89\u5168\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.08387", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.08387", "abs": "https://arxiv.org/abs/2602.08387", "authors": ["Max L\u00fcbbering", "Timm Ruland", "Richard Rutmann", "Felix Stollenwerk", "David Fitzek", "Michael Fromm", "Alexander Weber", "Rafet Sifa", "Nicolas Flores-Herr", "Joachim K\u00f6hler", "Mehdi Ali"], "title": "Modalities, a PyTorch-native Framework For Large-scale LLM Training and Research", "comment": null, "summary": "Today's LLM (pre-) training and research workflows typically allocate a significant amount of compute to large-scale ablation studies. Despite the substantial compute costs of these ablations, existing open-source frameworks provide limited tooling for these experiments, often forcing researchers to write their own wrappers and scripts. We propose Modalities, an end-to-end PyTorch-native framework that integrates data-driven LLM research with large-scale model training from two angles. Firstly, by integrating state-of-the-art parallelization strategies, it enables both efficient pretraining and systematic ablations at trillion-token and billion-parameter scale. Secondly, Modalities adopts modular design with declarative, self-contained configuration, enabling reproducibility and extensibility levels that are difficult to achieve out-of-the-box with existing LLM training frameworks.", "AI": {"tldr": "Modalities\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684PyTorch\u539f\u751f\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u89c4\u6a21LLM\u9884\u8bad\u7ec3\u548c\u7814\u7a76\u4e2d\u8ba1\u7b97\u5bc6\u96c6\u578b\u6d88\u878d\u5b9e\u9a8c\u7684\u6311\u6218\uff0c\u901a\u8fc7\u96c6\u6210\u5148\u8fdb\u5e76\u884c\u5316\u7b56\u7565\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u548c\u53ef\u590d\u73b0\u6027\u3002", "motivation": "\u5f53\u524dLLM\u9884\u8bad\u7ec3\u548c\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u8fdb\u884c\u5927\u89c4\u6a21\u6d88\u878d\u7814\u7a76\uff0c\u4f46\u73b0\u6709\u5f00\u6e90\u6846\u67b6\u5bf9\u6b64\u652f\u6301\u6709\u9650\uff0c\u7814\u7a76\u4eba\u5458\u901a\u5e38\u9700\u8981\u81ea\u884c\u7f16\u5199\u5305\u88c5\u5668\u548c\u811a\u672c\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u4e14\u96be\u4ee5\u590d\u73b0\u3002", "method": "Modalities\u91c7\u7528\u7aef\u5230\u7aefPyTorch\u539f\u751f\u8bbe\u8ba1\uff0c\u96c6\u6210\u6700\u5148\u8fdb\u7684\u5e76\u884c\u5316\u7b56\u7565\uff0c\u652f\u6301\u4e07\u4ebftoken\u548c\u5341\u4ebf\u53c2\u6570\u89c4\u6a21\u7684\u9ad8\u6548\u9884\u8bad\u7ec3\u548c\u7cfb\u7edf\u6d88\u878d\uff1b\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u58f0\u660e\u5f0f\u81ea\u5305\u542b\u914d\u7f6e\uff0c\u786e\u4fdd\u53ef\u590d\u73b0\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u9884\u8bad\u7ec3\u548c\u7cfb\u7edf\u6027\u6d88\u878d\u5b9e\u9a8c\uff0c\u63d0\u4f9b\u73b0\u6709LLM\u8bad\u7ec3\u6846\u67b6\u96be\u4ee5\u8fbe\u5230\u7684\u53ef\u590d\u73b0\u6027\u548c\u53ef\u6269\u5c55\u6027\u6c34\u5e73\u3002", "conclusion": "Modalities\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u5148\u8fdb\u5e76\u884c\u5316\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u4e3a\u5927\u89c4\u6a21LLM\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u53ef\u590d\u73b0\u7684\u5b9e\u9a8c\u5de5\u5177\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u7814\u7a76\u6d41\u7a0b\u4e2d\u7684\u5173\u952e\u74f6\u9888\u3002"}}
{"id": "2602.08407", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.08407", "abs": "https://arxiv.org/abs/2602.08407", "authors": ["Richard Serrano", "Baptiste Jeudy", "Charlotte Laclau", "Christine Largeron"], "title": "Drop the mask! GAMM-A Taxonomy for Graph Attributes Missing Mechanisms", "comment": null, "summary": "Exploring missing data in attributed graphs introduces unique challenges beyond those found in tabular datasets. In this work, we extend the taxonomy for missing data mechanisms to attributed graphs by proposing GAMM (Graph Attributes Missing Mechanisms), a framework that systematically links missingness probability to both node attributes and the underlying graph structure. Our taxonomy enriches the conventional definitions of masking mechanisms by introducing graph-specific dependencies. We empirically demonstrate that state-of-the-art imputation methods, while effective on traditional masks, significantly struggle when confronted with these more realistic graph-aware missingness scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86GAMM\u6846\u67b6\uff0c\u5c06\u7f3a\u5931\u6570\u636e\u673a\u5236\u5206\u7c7b\u6269\u5c55\u5230\u5c5e\u6027\u56fe\uff0c\u8003\u8651\u4e86\u8282\u70b9\u5c5e\u6027\u548c\u56fe\u7ed3\u6784\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u53d1\u73b0\u73b0\u6709\u63d2\u8865\u65b9\u6cd5\u5728\u56fe\u611f\u77e5\u7f3a\u5931\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u5c5e\u6027\u56fe\u4e2d\u7684\u7f3a\u5931\u6570\u636e\u95ee\u9898\u6bd4\u8868\u683c\u6570\u636e\u66f4\u5177\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u9700\u8981\u8003\u8651\u56fe\u7ed3\u6784\u7684\u5f71\u54cd\u3002\u73b0\u6709\u7f3a\u5931\u673a\u5236\u5206\u7c7b\u4e3b\u8981\u9488\u5bf9\u8868\u683c\u6570\u636e\uff0c\u7f3a\u4e4f\u5bf9\u56fe\u7279\u5b9a\u4f9d\u8d56\u5173\u7cfb\u7684\u8003\u8651\u3002", "method": "\u63d0\u51fa\u4e86GAMM\uff08Graph Attributes Missing Mechanisms\uff09\u6846\u67b6\uff0c\u5c06\u7f3a\u5931\u6570\u636e\u673a\u5236\u5206\u7c7b\u6269\u5c55\u5230\u5c5e\u6027\u56fe\uff0c\u7cfb\u7edf\u5730\u5c06\u7f3a\u5931\u6982\u7387\u4e0e\u8282\u70b9\u5c5e\u6027\u548c\u5e95\u5c42\u56fe\u7ed3\u6784\u8054\u7cfb\u8d77\u6765\uff0c\u5f15\u5165\u4e86\u56fe\u7279\u5b9a\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u6700\u5148\u8fdb\u7684\u63d2\u8865\u65b9\u6cd5\u5728\u4f20\u7edf\u7f3a\u5931\u673a\u5236\u4e0a\u6709\u6548\uff0c\u4f46\u5728\u8fd9\u4e9b\u66f4\u73b0\u5b9e\u7684\u56fe\u611f\u77e5\u7f3a\u5931\u573a\u666f\u4e2d\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u5c5e\u6027\u56fe\u4e2d\u7684\u7f3a\u5931\u6570\u636e\u9700\u8981\u4e13\u95e8\u7684\u673a\u5236\u5206\u7c7b\uff0cGAMM\u6846\u67b6\u4e3a\u6b64\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u56fe\u611f\u77e5\u7f3a\u5931\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2602.08419", "categories": ["cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.08419", "abs": "https://arxiv.org/abs/2602.08419", "authors": ["Gnankan Landry Regis N'guessan", "Bum Jun Kim"], "title": "Radial M\u00fcntz-Sz\u00e1sz Networks: Neural Architectures with Learnable Power Bases for Multidimensional Singularities", "comment": "47 pages, 13 figures", "summary": "Radial singular fields, such as $1/r$, $\\log r$, and crack-tip profiles, are difficult to model for coordinate-separable neural architectures. We show that any $C^2$ function that is both radial and additively separable must be quadratic, establishing a fundamental obstruction for coordinate-wise power-law models. Motivated by this result, we introduce Radial M\u00fcntz-Sz\u00e1sz Networks (RMN), which represent fields as linear combinations of learnable radial powers $r^\u03bc$, including negative exponents, together with a limit-stable log-primitive for exact $\\log r$ behavior. RMN admits closed-form spatial gradients and Laplacians, enabling physics-informed learning on punctured domains. Across ten 2D and 3D benchmarks, RMN achieves 1.5$\\times$--51$\\times$ lower RMSE than MLPs and 10$\\times$--100$\\times$ lower RMSE than SIREN while using 27 parameters, compared with 33,537 for MLPs and 8,577 for SIREN. We extend RMN to angular dependence (RMN-Angular) and to multiple sources with learnable centers (RMN-MC); when optimization converges, source-center recovery errors fall below $10^{-4}$. We also report controlled failures on smooth, strongly non-radial targets to delineate RMN's operating regime.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5f84\u5411M\u00fcntz-Sz\u00e1sz\u7f51\u7edc(RMN)\u6765\u89e3\u51b3\u5750\u6807\u53ef\u5206\u79bb\u795e\u7ecf\u7f51\u7edc\u96be\u4ee5\u5efa\u6a21\u5f84\u5411\u5947\u5f02\u573a(\u59821/r\u3001log r\u3001\u88c2\u7eb9\u5c16\u7aef\u5256\u9762)\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5f84\u5411\u5e42\u6b21\u7ec4\u5408\u5b9e\u73b0\u7cbe\u786e\u5efa\u6a21\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edfMLP\u548cSIREN\u7f51\u7edc\u3002", "motivation": "\u5f84\u5411\u5947\u5f02\u573a(\u59821/r\u3001log r\u3001\u88c2\u7eb9\u5c16\u7aef\u5256\u9762)\u5bf9\u5750\u6807\u53ef\u5206\u79bb\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u96be\u4ee5\u5efa\u6a21\u3002\u8bba\u6587\u8bc1\u660e\u4efb\u4f55\u65e2\u662f\u5f84\u5411\u53c8\u662f\u52a0\u6027\u53ef\u5206\u79bb\u7684C^2\u51fd\u6570\u5fc5\u987b\u662f\u4e8c\u6b21\u51fd\u6570\uff0c\u8fd9\u4e3a\u5750\u6807\u5e42\u5f8b\u6a21\u578b\u5efa\u7acb\u4e86\u57fa\u672c\u969c\u788d\u3002", "method": "\u63d0\u51fa\u5f84\u5411M\u00fcntz-Sz\u00e1sz\u7f51\u7edc(RMN)\uff0c\u5c06\u573a\u8868\u793a\u4e3a\u53ef\u5b66\u4e60\u5f84\u5411\u5e42\u6b21r^\u03bc(\u5305\u62ec\u8d1f\u6307\u6570)\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u52a0\u4e0a\u6781\u9650\u7a33\u5b9a\u7684\u5bf9\u6570\u539f\u8bed\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684log r\u884c\u4e3a\u3002RMN\u5141\u8bb8\u95ed\u5f0f\u7a7a\u95f4\u68af\u5ea6\u548c\u62c9\u666e\u62c9\u65af\u7b97\u5b50\uff0c\u652f\u6301\u7a7f\u5b54\u57df\u4e0a\u7684\u7269\u7406\u4fe1\u606f\u5b66\u4e60\u3002\u6269\u5c55\u5305\u62ec\u89d2\u4f9d\u8d56(RMN-Angular)\u548c\u5177\u6709\u53ef\u5b66\u4e60\u4e2d\u5fc3\u7684\u591a\u6e90(RMN-MC)\u3002", "result": "\u572810\u4e2a2D\u548c3D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRMN\u4f7f\u752827\u4e2a\u53c2\u6570(MLP\u4f7f\u752833,537\u4e2a\uff0cSIREN\u4f7f\u75288,577\u4e2a)\u5b9e\u73b0\u4e86\u6bd4MLP\u4f4e1.5-51\u500d\u7684RMSE\uff0c\u6bd4SIREN\u4f4e10-100\u500d\u7684RMSE\u3002\u5f53\u4f18\u5316\u6536\u655b\u65f6\uff0c\u6e90\u4e2d\u5fc3\u6062\u590d\u8bef\u5dee\u4f4e\u4e8e10^-4\u3002\u8bba\u6587\u8fd8\u62a5\u544a\u4e86\u5728\u5e73\u6ed1\u3001\u5f3a\u975e\u5f84\u5411\u76ee\u6807\u4e0a\u7684\u53d7\u63a7\u5931\u8d25\uff0c\u4ee5\u754c\u5b9aRMN\u7684\u64cd\u4f5c\u8303\u56f4\u3002", "conclusion": "RMN\u4e3a\u5f84\u5411\u5947\u5f02\u573a\u63d0\u4f9b\u4e86\u9ad8\u6548\u7cbe\u786e\u7684\u795e\u7ecf\u7f51\u7edc\u8868\u793a\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u5750\u6807\u53ef\u5206\u79bb\u67b6\u6784\u7684\u57fa\u672c\u9650\u5236\uff0c\u5728\u7269\u7406\u4fe1\u606f\u5b66\u4e60\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7cbe\u786e\u5efa\u6a21\u5947\u5f02\u884c\u4e3a\u7684\u9886\u57df\u3002"}}
{"id": "2602.08427", "categories": ["cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.08427", "abs": "https://arxiv.org/abs/2602.08427", "authors": ["Marius Marinescu"], "title": "The Connection between Kriging and Large Neural Networks", "comment": null, "summary": "AI has impacted many disciplines and is nowadays ubiquitous. In particular, spatial statistics is in a pivotal moment where it will increasingly intertwine with AI. In this scenario, a relevant question is what relationship spatial statistics models have with machine learning (ML) models, if any. In particular, in this paper, we explore the connections between Kriging and neural networks. At first glance, they may appear unrelated. Kriging - and its ML counterpart, Gaussian process regression - are grounded in probability theory and stochastic processes, whereas many ML models are extensively considered Black-Box models. Nevertheless, they are strongly related. We study their connections and revisit the relevant literature. The understanding of their relations and the combination of both perspectives may enhance ML techniques by making them more interpretable, reliable, and spatially aware.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u514b\u91cc\u91d1\u6cd5\u4e0e\u795e\u7ecf\u7f51\u7edc\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u7edf\u8ba1\u4e0e\u673a\u5668\u5b66\u4e60\u63d0\u5347\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u9760\u6027\u548c\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u968f\u7740AI\u5728\u5404\u5b66\u79d1\u7684\u666e\u53ca\uff0c\u7a7a\u95f4\u7edf\u8ba1\u5b66\u6b63\u5904\u4e8e\u4e0eAI\u6df1\u5ea6\u878d\u5408\u7684\u5173\u952e\u65f6\u523b\u3002\u4f5c\u8005\u5173\u6ce8\u7a7a\u95f4\u7edf\u8ba1\u6a21\u578b\u4e0e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u7279\u522b\u662f\u514b\u91cc\u91d1\u6cd5\u4e0e\u795e\u7ecf\u7f51\u7edc\u8fd9\u4e24\u79cd\u770b\u4f3c\u65e0\u5173\u7684\u65b9\u6cd5\u4e4b\u95f4\u7684\u6df1\u5c42\u8054\u7cfb\uff0c\u5e0c\u671b\u901a\u8fc7\u7406\u89e3\u5b83\u4eec\u7684\u5173\u8054\u6765\u589e\u5f3a\u673a\u5668\u5b66\u4e60\u6280\u672f\u3002", "method": "\u901a\u8fc7\u6587\u732e\u56de\u987e\u548c\u7406\u8bba\u5206\u6790\uff0c\u7814\u7a76\u514b\u91cc\u91d1\u6cd5\uff08\u53ca\u5176\u673a\u5668\u5b66\u4e60\u5bf9\u5e94\u65b9\u6cd5\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\uff09\u4e0e\u795e\u7ecf\u7f51\u7edc\u4e4b\u95f4\u7684\u8fde\u63a5\u5173\u7cfb\u3002\u5206\u6790\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5728\u6982\u7387\u7406\u8bba\u57fa\u7840\u548c\u9ed1\u76d2\u6a21\u578b\u7279\u6027\u65b9\u9762\u7684\u5f02\u540c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u514b\u91cc\u91d1\u6cd5\u4e0e\u795e\u7ecf\u7f51\u7edc\u4e4b\u95f4\u5b58\u5728\u5f3a\u70c8\u5173\u8054\uff0c\u5c3d\u7ba1\u524d\u8005\u57fa\u4e8e\u6982\u7387\u7406\u8bba\u548c\u968f\u673a\u8fc7\u7a0b\uff0c\u540e\u8005\u5e38\u88ab\u89c6\u4e3a\u9ed1\u76d2\u6a21\u578b\uff0c\u4f46\u4e24\u8005\u5728\u7406\u8bba\u4e0a\u6709\u6df1\u523b\u8054\u7cfb\u3002", "conclusion": "\u7406\u89e3\u514b\u91cc\u91d1\u6cd5\u4e0e\u795e\u7ecf\u7f51\u7edc\u7684\u5173\u7cfb\u5e76\u5c06\u4e24\u79cd\u89c6\u89d2\u7ed3\u5408\uff0c\u53ef\u4ee5\u589e\u5f3a\u673a\u5668\u5b66\u4e60\u6280\u672f\u7684\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u9760\u6027\u548c\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u4e3a\u7a7a\u95f4\u7edf\u8ba1\u5b66\u4e0eAI\u7684\u878d\u5408\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2602.08431", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08431", "abs": "https://arxiv.org/abs/2602.08431", "authors": ["Yingxu Wang", "Kunyu Zhang", "Mengzhu Wang", "Siyang Gao", "Nan Yin"], "title": "USBD: Universal Structural Basis Distillation for Source-Free Graph Domain Adaptation", "comment": null, "summary": "SF-GDA is pivotal for privacy-preserving knowledge transfer across graph datasets. Although recent works incorporate structural information, they implicitly condition adaptation on the smoothness priors of sourcetrained GNNs, thereby limiting their generalization to structurally distinct targets. This dependency becomes a critical bottleneck under significant topological shifts, where the source model misinterprets distinct topological patterns unseen in the source domain as noise, rendering pseudo-label-based adaptation unreliable. To overcome this limitation, we propose the Universal Structural Basis Distillation, a framework that shifts the paradigm from adapting a biased model to learning a universal structural basis for SF-GDA. Instead of adapting a biased source model to a specific target, our core idea is to construct a structure-agnostic basis that proactively covers the full spectrum of potential topological patterns. Specifically, USBD employs a bi-level optimization framework to distill the source dataset into a compact structural basis. By enforcing the prototypes to span the full Dirichlet energy spectrum, the learned basis explicitly captures diverse topological motifs, ranging from low-frequency clusters to high-frequency chains, beyond those present in the source. This ensures that the learned basis creates a comprehensive structural covering capable of handling targets with disparate structures. For inference, we introduce a spectral-aware ensemble mechanism that dynamically activates the optimal prototype combination based on the spectral fingerprint of the target graph. Extensive experiments on benchmarks demonstrate that USBD significantly outperforms state-of-the-art methods, particularly in scenarios with severe structural shifts, while achieving superior computational efficiency by decoupling the adaptation cost from the target data scale.", "AI": {"tldr": "USBD\u63d0\u51fa\u901a\u7528\u7ed3\u6784\u57fa\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u7ed3\u6784\u65e0\u5173\u7684\u57fa\u6765\u8986\u76d6\u5b8c\u6574\u62d3\u6251\u6a21\u5f0f\u8c31\uff0c\u89e3\u51b3\u56fe\u9886\u57df\u81ea\u9002\u5e94\u4e2d\u6e90\u6a21\u578b\u7ed3\u6784\u504f\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u7ed3\u6784\u5dee\u5f02\u76ee\u6807\u7684\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u73b0\u6709SF-GDA\u65b9\u6cd5\u4f9d\u8d56\u6e90\u8bad\u7ec3GNN\u7684\u5e73\u6ed1\u5148\u9a8c\uff0c\u5728\u62d3\u6251\u7ed3\u6784\u663e\u8457\u53d8\u5316\u65f6\uff0c\u6e90\u6a21\u578b\u4f1a\u5c06\u672a\u89c1\u7684\u7ed3\u6784\u6a21\u5f0f\u8bef\u5224\u4e3a\u566a\u58f0\uff0c\u5bfc\u81f4\u57fa\u4e8e\u4f2a\u6807\u7b7e\u7684\u81ea\u9002\u5e94\u4e0d\u53ef\u9760\uff0c\u6210\u4e3a\u7ed3\u6784\u5dee\u5f02\u573a\u666f\u4e0b\u7684\u5173\u952e\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u901a\u7528\u7ed3\u6784\u57fa\u84b8\u998f\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u5c06\u6e90\u6570\u636e\u96c6\u84b8\u998f\u4e3a\u7d27\u51d1\u7ed3\u6784\u57fa\uff0c\u901a\u8fc7\u5f3a\u5236\u539f\u578b\u8986\u76d6\u5b8c\u6574Dirichlet\u80fd\u91cf\u8c31\u6765\u6355\u83b7\u591a\u6837\u62d3\u6251\u6a21\u5f0f\uff1b\u63a8\u7406\u65f6\u5f15\u5165\u8c31\u611f\u77e5\u96c6\u6210\u673a\u5236\uff0c\u57fa\u4e8e\u76ee\u6807\u56fe\u8c31\u6307\u7eb9\u52a8\u6001\u6fc0\u6d3b\u6700\u4f18\u539f\u578b\u7ec4\u5408\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u4e25\u91cd\u7ed3\u6784\u504f\u79fb\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\uff1b\u540c\u65f6\u901a\u8fc7\u89e3\u8026\u81ea\u9002\u5e94\u6210\u672c\u4e0e\u76ee\u6807\u6570\u636e\u89c4\u6a21\u5b9e\u73b0\u5353\u8d8a\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "USBD\u901a\u8fc7\u4ece\u9002\u5e94\u6709\u504f\u6a21\u578b\u8f6c\u5411\u5b66\u4e60\u901a\u7528\u7ed3\u6784\u57fa\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u6709\u6548\u89e3\u51b3\u4e86SF-GDA\u4e2d\u7684\u7ed3\u6784\u504f\u5dee\u95ee\u9898\uff0c\u4e3a\u5904\u7406\u62d3\u6251\u7ed3\u6784\u663e\u8457\u4e0d\u540c\u7684\u76ee\u6807\u56fe\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08446", "categories": ["cs.LG", "cs.CR", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.08446", "abs": "https://arxiv.org/abs/2602.08446", "authors": ["Pouria Arefijamal", "Mahdi Ahmadlou", "Bardia Safaei", "J\u00f6rg Henkel"], "title": "RIFLE: Robust Distillation-based FL for Deep Model Deployment on Resource-Constrained IoT Networks", "comment": "This paper has been accepted for publication in IEEE ICC 2026 and will be indexed in the IEEE Xplore Digital Library", "summary": "Federated learning (FL) is a decentralized learning paradigm widely adopted in resource-constrained Internet of Things (IoT) environments. These devices, typically relying on TinyML models, collaboratively train global models by sharing gradients with a central server while preserving data privacy. However, as data heterogeneity and task complexity increase, TinyML models often become insufficient to capture intricate patterns, especially under extreme non-IID (non-independent and identically distributed) conditions. Moreover, ensuring robustness against malicious clients and poisoned updates remains a major challenge. Accordingly, this paper introduces RIFLE - a Robust, distillation-based Federated Learning framework that replaces gradient sharing with logit-based knowledge transfer. By leveraging a knowledge distillation aggregation scheme, RIFLE enables the training of deep models such as VGG-19 and Resnet18 within constrained IoT systems. Furthermore, a Kullback-Leibler (KL) divergence-based validation mechanism quantifies the reliability of client updates without exposing raw data, achieving high trust and privacy preservation simultaneously. Experiments on three benchmark datasets (MNIST, CIFAR-10, and CIFAR-100) under heterogeneous non-IID conditions demonstrate that RIFLE reduces false-positive detections by up to 87.5%, enhances poisoning attack mitigation by 62.5%, and achieves up to 28.3% higher accuracy compared to conventional federated learning baselines within only 10 rounds. Notably, RIFLE reduces VGG19 training time from over 600 days to just 1.39 hours on typical IoT devices (0.3 GFLOPS), making deep learning practical in resource-constrained networks.", "AI": {"tldr": "RIFLE\uff1a\u57fa\u4e8e\u84b8\u998f\u7684\u9c81\u68d2\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528logit\u77e5\u8bc6\u8f6c\u79fb\u66ff\u4ee3\u68af\u5ea6\u5171\u4eab\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684IoT\u73af\u5883\u4e2d\u5b9e\u73b0\u6df1\u5ea6\u6a21\u578b\u8bad\u7ec3\uff0c\u540c\u65f6\u63d0\u5347\u5bf9\u6297\u6076\u610f\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u5728IoT\u73af\u5883\u4e2d\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1\uff09TinyML\u6a21\u578b\u5728\u6570\u636e\u5f02\u6784\u548c\u4efb\u52a1\u590d\u6742\u65f6\u96be\u4ee5\u6355\u6349\u590d\u6742\u6a21\u5f0f\uff1b2\uff09\u5bf9\u6297\u6076\u610f\u5ba2\u6237\u7aef\u548c\u4e2d\u6bd2\u66f4\u65b0\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u8bad\u7ec3\u6df1\u5ea6\u6a21\u578b\u53c8\u80fd\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faRIFLE\u6846\u67b6\uff1a1\uff09\u7528logit-based\u77e5\u8bc6\u8f6c\u79fb\u66ff\u4ee3\u68af\u5ea6\u5171\u4eab\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u805a\u5408\u65b9\u6848\u8bad\u7ec3\u6df1\u5ea6\u6a21\u578b\uff1b2\uff09\u57fa\u4e8eKL\u6563\u5ea6\u7684\u9a8c\u8bc1\u673a\u5236\u91cf\u5316\u5ba2\u6237\u7aef\u66f4\u65b0\u7684\u53ef\u9760\u6027\uff0c\u4e0d\u66b4\u9732\u539f\u59cb\u6570\u636e\uff1b3\uff09\u5728\u5f02\u6784\u975eIID\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u3002", "result": "\u5728MNIST\u3001CIFAR-10\u3001CIFAR-100\u6570\u636e\u96c6\u4e0a\uff0cRIFLE\u5728\u5f02\u6784\u975eIID\u6761\u4ef6\u4e0b\uff1a1\uff09\u51cf\u5c1187.5%\u7684\u8bef\u62a5\u68c0\u6d4b\uff1b2\uff09\u63d0\u534762.5%\u7684\u4e2d\u6bd2\u653b\u51fb\u7f13\u89e3\u80fd\u529b\uff1b3\uff09\u76f8\u6bd4\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u57fa\u7ebf\u83b7\u5f9728.3%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff1b4\uff09\u5c06VGG19\u8bad\u7ec3\u65f6\u95f4\u4ece600\u591a\u5929\u7f29\u77ed\u52301.39\u5c0f\u65f6\u3002", "conclusion": "RIFLE\u901a\u8fc7logit\u77e5\u8bc6\u8f6c\u79fb\u548cKL\u6563\u5ea6\u9a8c\u8bc1\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86IoT\u73af\u5883\u4e2d\u8054\u90a6\u5b66\u4e60\u7684\u6df1\u5ea6\u6a21\u578b\u8bad\u7ec3\u548c\u5b89\u5168\u6027\u6311\u6218\uff0c\u4f7f\u6df1\u5ea6\u5b66\u4e60\u5728\u8d44\u6e90\u53d7\u9650\u7f51\u7edc\u4e2d\u53d8\u5f97\u5b9e\u7528\u53ef\u884c\u3002"}}
{"id": "2602.08461", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08461", "abs": "https://arxiv.org/abs/2602.08461", "authors": ["Liyuan Xu", "Bijan Mazaheri"], "title": "Estimating Aleatoric Uncertainty in the Causal Treatment Effect", "comment": null, "summary": "Previous work on causal inference has primarily focused on averages and conditional averages of treatment effects, with significantly less attention on variability and uncertainty in individual treatment responses. In this paper, we introduce the variance of the treatment effect (VTE) and conditional variance of treatment effect (CVTE) as the natural measure of aleatoric uncertainty inherent in treatment responses, and we demonstrate that these quantities are identifiable from observed data under mild assumptions, even in the presence of unobserved confounders. We further propose nonparametric kernel-based estimators for VTE and CVTE, and our theoretical analysis establishes their convergence. We also test the performance of our method through extensive empirical experiments on both synthetic and semi-simulated datasets, where it demonstrates superior or comparable performance to naive baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u6cbb\u7597\u6548\u5e94\u65b9\u5dee(VTE)\u548c\u6761\u4ef6\u6cbb\u7597\u6548\u5e94\u65b9\u5dee(CVTE)\u4f5c\u4e3a\u8861\u91cf\u6cbb\u7597\u54cd\u5e94\u4e2d\u56fa\u6709\u968f\u673a\u4e0d\u786e\u5b9a\u6027\u7684\u81ea\u7136\u5ea6\u91cf\uff0c\u8bc1\u660e\u4e86\u8fd9\u4e9b\u91cf\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u53ef\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u8bc6\u522b\uff0c\u5e76\u5f00\u53d1\u4e86\u975e\u53c2\u6570\u6838\u4f30\u8ba1\u5668\u3002", "motivation": "\u5148\u524d\u56e0\u679c\u63a8\u65ad\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6cbb\u7597\u6548\u5e94\u7684\u5e73\u5747\u503c\u548c\u6761\u4ef6\u5e73\u5747\u503c\uff0c\u5bf9\u4e2a\u4f53\u6cbb\u7597\u54cd\u5e94\u4e2d\u7684\u53d8\u5f02\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u5173\u6ce8\u8f83\u5c11\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5173\u6ce8\u6cbb\u7597\u54cd\u5e94\u4e2d\u7684\u968f\u673a\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u5f15\u5165\u6cbb\u7597\u6548\u5e94\u65b9\u5dee(VTE)\u548c\u6761\u4ef6\u6cbb\u7597\u6548\u5e94\u65b9\u5dee(CVTE)\u4f5c\u4e3a\u968f\u673a\u4e0d\u786e\u5b9a\u6027\u7684\u5ea6\u91cf\uff0c\u8bc1\u660e\u8fd9\u4e9b\u91cf\u5728\u5b58\u5728\u672a\u89c2\u6d4b\u6df7\u6742\u56e0\u7d20\u7684\u60c5\u51b5\u4e0b\u4ecd\u53ef\u8bc6\u522b\u3002\u63d0\u51fa\u975e\u53c2\u6570\u6838\u57fa\u4f30\u8ba1\u5668\u6765\u4f30\u8ba1VTE\u548cCVTE\uff0c\u5e76\u8fdb\u884c\u7406\u8bba\u6536\u655b\u6027\u5206\u6790\u3002", "result": "\u7406\u8bba\u5206\u6790\u5efa\u7acb\u4e86\u4f30\u8ba1\u5668\u7684\u6536\u655b\u6027\u3002\u5728\u5408\u6210\u548c\u534a\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u8bc1\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6734\u7d20\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u51fa\u4f18\u8d8a\u6216\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "VTE\u548cCVTE\u662f\u8861\u91cf\u6cbb\u7597\u54cd\u5e94\u4e2d\u968f\u673a\u4e0d\u786e\u5b9a\u6027\u7684\u81ea\u7136\u5ea6\u91cf\uff0c\u5373\u4f7f\u5728\u5b58\u5728\u672a\u89c2\u6d4b\u6df7\u6742\u56e0\u7d20\u7684\u60c5\u51b5\u4e0b\u4e5f\u53ef\u8bc6\u522b\u3002\u63d0\u51fa\u7684\u975e\u53c2\u6570\u6838\u4f30\u8ba1\u5668\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u5728\u5b9e\u8bc1\u5b9e\u9a8c\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4e3a\u56e0\u679c\u63a8\u65ad\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2602.08467", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08467", "abs": "https://arxiv.org/abs/2602.08467", "authors": ["Charalampos Shimillas", "Kleanthis Malialis", "Konstantinos Fokianos", "Marios M. Polycarpou"], "title": "Low Rank Transformer for Multivariate Time Series Anomaly Detection and Localization", "comment": null, "summary": "Multivariate time series (MTS) anomaly diagnosis, which encompasses both anomaly detection and localization, is critical for the safety and reliability of complex, large-scale real-world systems. The vast majority of existing anomaly diagnosis methods offer limited theoretical insights, especially for anomaly localization, which is a vital but largely unexplored area. The aim of this contribution is to study the learning process of a Transformer when applied to MTS by revealing connections to statistical time series methods. Based on these theoretical insights, we propose the Attention Low-Rank Transformer (ALoRa-T) model, which applies low-rank regularization to self-attention, and we introduce the Attention Low-Rank score, effectively capturing the temporal characteristics of anomalies. Finally, to enable anomaly localization, we propose the ALoRa-Loc method, a novel approach that associates anomalies to specific variables by quantifying interrelationships among time series. Extensive experiments and real data analysis, show that the proposed methodology significantly outperforms state-of-the-art methods in both detection and localization tasks.", "AI": {"tldr": "\u63d0\u51faALoRa-T\u6a21\u578b\u548cALoRa-Loc\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u79e9\u6b63\u5219\u5316\u81ea\u6ce8\u610f\u529b\u548c\u91cf\u5316\u65f6\u95f4\u5e8f\u5217\u76f8\u4e92\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u8bca\u65ad\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u6d1e\u5bdf\uff0c\u7279\u522b\u662f\u5f02\u5e38\u5b9a\u4f4d\u8fd9\u4e00\u5173\u952e\u4f46\u7814\u7a76\u4e0d\u8db3\u7684\u9886\u57df\u3002\u9700\u8981\u4ece\u7406\u8bba\u89d2\u5ea6\u7406\u89e3Transformer\u5728\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u4e0a\u7684\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5e76\u5efa\u7acb\u4e0e\u7edf\u8ba1\u65f6\u95f4\u5e8f\u5217\u65b9\u6cd5\u7684\u8054\u7cfb\u3002", "method": "1. \u7406\u8bba\u5206\u6790Transformer\u5728\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u4e0a\u7684\u5b66\u4e60\u8fc7\u7a0b\uff0c\u63ed\u793a\u5176\u4e0e\u7edf\u8ba1\u65f6\u95f4\u5e8f\u5217\u65b9\u6cd5\u7684\u8054\u7cfb\uff1b2. \u63d0\u51faALoRa-T\u6a21\u578b\uff0c\u5bf9\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5e94\u7528\u4f4e\u79e9\u6b63\u5219\u5316\uff1b3. \u5f15\u5165Attention Low-Rank score\u6355\u6349\u5f02\u5e38\u7684\u65f6\u95f4\u7279\u5f81\uff1b4. \u63d0\u51faALoRa-Loc\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cf\u5316\u65f6\u95f4\u5e8f\u5217\u95f4\u7684\u76f8\u4e92\u5173\u7cfb\u5b9e\u73b0\u5f02\u5e38\u53d8\u91cf\u5b9a\u4f4d\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u548c\u771f\u5b9e\u6570\u636e\u5206\u6790\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u5f02\u5e38\u68c0\u6d4b\u548c\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u6d1e\u5bdf\u6307\u5bfc\u7684ALoRa-T\u6a21\u578b\u548cALoRa-Loc\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u8bca\u65ad\u4e2d\u7684\u68c0\u6d4b\u548c\u5b9a\u4f4d\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u7cfb\u7edf\u7684\u5b89\u5168\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2602.08470", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08470", "abs": "https://arxiv.org/abs/2602.08470", "authors": ["Kaizheng Wang", "Ghifari Adam Faza", "Fabio Cuzzolin", "Siu Lun Chau", "David Moens", "Hans Hallez"], "title": "Learning Credal Ensembles via Distributionally Robust Optimization", "comment": "32 pages", "summary": "Credal predictors are models that are aware of epistemic uncertainty and produce a convex set of probabilistic predictions. They offer a principled way to quantify predictive epistemic uncertainty (EU) and have been shown to improve model robustness in various settings. However, most state-of-the-art methods mainly define EU as disagreement caused by random training initializations, which mostly reflects sensitivity to optimization randomness rather than uncertainty from deeper sources. To address this, we define EU as disagreement among models trained with varying relaxations of the i.i.d. assumption between training and test data. Based on this idea, we propose CreDRO, which learns an ensemble of plausible models through distributionally robust optimization. As a result, CreDRO captures EU not only from training randomness but also from meaningful disagreement due to potential distribution shifts between training and test data. Empirical results show that CreDRO consistently outperforms existing credal methods on tasks such as out-of-distribution detection across multiple benchmarks and selective classification in medical applications.", "AI": {"tldr": "CreDRO\uff1a\u4e00\u79cd\u57fa\u4e8e\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u7684\u4fe1\u5ea6\u9884\u6d4b\u5668\uff0c\u901a\u8fc7\u8bad\u7ec3\u591a\u4e2a\u5728i.i.d.\u5047\u8bbe\u4e0d\u540c\u677e\u5f1b\u4e0b\u7684\u6a21\u578b\u6765\u6355\u6349\u9884\u6d4b\u6027\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u4e0d\u4ec5\u8003\u8651\u8bad\u7ec3\u968f\u673a\u6027\uff0c\u8fd8\u8003\u8651\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u6570\u636e\u95f4\u6f5c\u5728\u5206\u5e03\u504f\u79fb\u5e26\u6765\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u4fe1\u5ea6\u9884\u6d4b\u5668\u4e3b\u8981\u5c06\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u5b9a\u4e49\u4e3a\u7531\u968f\u673a\u8bad\u7ec3\u521d\u59cb\u5316\u5f15\u8d77\u7684\u5206\u6b67\uff0c\u8fd9\u4e3b\u8981\u53cd\u6620\u4e86\u5bf9\u4f18\u5316\u968f\u673a\u6027\u7684\u654f\u611f\u6027\uff0c\u800c\u975e\u66f4\u6df1\u5c42\u6b21\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\u3002\u9700\u8981\u4e00\u79cd\u80fd\u6355\u6349\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u6570\u636e\u95f4\u6f5c\u5728\u5206\u5e03\u504f\u79fb\u5e26\u6765\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCreDRO\u65b9\u6cd5\uff0c\u5c06\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u5b9a\u4e49\u4e3a\u5728\u4e0d\u540ci.i.d.\u5047\u8bbe\u677e\u5f1b\u4e0b\u8bad\u7ec3\u7684\u6a21\u578b\u4e4b\u95f4\u7684\u5206\u6b67\u3002\u901a\u8fc7\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u5b66\u4e60\u4e00\u7ec4\u5408\u7406\u7684\u6a21\u578b\u96c6\u5408\uff0c\u6355\u6349\u6765\u81ea\u8bad\u7ec3\u968f\u673a\u6027\u548c\u6f5c\u5728\u5206\u5e03\u504f\u79fb\u7684\u6709\u610f\u4e49\u5206\u6b67\u3002", "result": "CreDRO\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u4fe1\u5ea6\u65b9\u6cd5\uff0c\u5728\u5206\u5e03\u5916\u68c0\u6d4b\u4efb\u52a1\u548c\u533b\u7597\u5e94\u7528\u4e2d\u7684\u9009\u62e9\u6027\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "CreDRO\u901a\u8fc7\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u6709\u6548\u6355\u6349\u4e86\u66f4\u5168\u9762\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u4e0d\u4ec5\u5305\u62ec\u8bad\u7ec3\u968f\u673a\u6027\uff0c\u8fd8\u5305\u62ec\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u6570\u636e\u95f4\u6f5c\u5728\u5206\u5e03\u504f\u79fb\u5e26\u6765\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\u3002"}}
{"id": "2602.08478", "categories": ["cs.LG", "math.DS", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.08478", "abs": "https://arxiv.org/abs/2602.08478", "authors": ["Albert Alcalde", "Markus Widhalm", "Emre Y\u0131lmaz"], "title": "Time-Delayed Transformers for Data-Driven Modeling of Low-Dimensional Dynamics", "comment": null, "summary": "We propose the time-delayed transformer (TD-TF), a simplified transformer architecture for data-driven modeling of unsteady spatio-temporal dynamics. TD-TF bridges linear operator-based methods and deep sequence models by showing that a single-layer, single-head transformer can be interpreted as a nonlinear generalization of time-delayed dynamic mode decomposition (TD-DMD). The architecture is deliberately minimal, consisting of one self-attention layer with a single query per prediction and one feedforward layer, resulting in linear computational complexity in sequence length and a small parameter count. Numerical experiments demonstrate that TD-TF matches the performance of strong linear baselines on near-linear systems, while significantly outperforming them in nonlinear and chaotic regimes, where it accurately captures long-term dynamics. Validation studies on synthetic signals, unsteady aerodynamics, the Lorenz '63 system, and a reaction-diffusion model show that TD-TF preserves the interpretability and efficiency of linear models while providing substantially enhanced expressive power for complex dynamics.", "AI": {"tldr": "TD-TF\u662f\u4e00\u79cd\u7b80\u5316\u7684Transformer\u67b6\u6784\uff0c\u7528\u4e8e\u975e\u5b9a\u5e38\u65f6\u7a7a\u52a8\u529b\u5b66\u7684\u6570\u636e\u9a71\u52a8\u5efa\u6a21\uff0c\u5c06\u7ebf\u6027\u7b97\u5b50\u65b9\u6cd5\u4e0e\u6df1\u5ea6\u5e8f\u5217\u6a21\u578b\u8054\u7cfb\u8d77\u6765\uff0c\u5728\u4fdd\u6301\u7ebf\u6027\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7ebf\u6027\u7b97\u5b50\u65b9\u6cd5\uff08\u5982TD-DMD\uff09\u5728\u5904\u7406\u975e\u7ebf\u6027\u7cfb\u7edf\u65f6\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u800c\u590d\u6742\u6df1\u5ea6\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5904\u7406\u975e\u7ebf\u6027\u52a8\u529b\u5b66\uff0c\u53c8\u4fdd\u6301\u7ebf\u6027\u6a21\u578b\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u65f6\u95f4\u5ef6\u8fdfTransformer\uff08TD-TF\uff09\uff0c\u91c7\u7528\u6781\u7b80\u67b6\u6784\uff1a\u5355\u5c42\u5355\u5934\u81ea\u6ce8\u610f\u529b\u5c42\uff08\u6bcf\u4e2a\u9884\u6d4b\u4e00\u4e2a\u67e5\u8be2\uff09\u52a0\u4e00\u4e2a\u524d\u9988\u5c42\u3002\u8be5\u67b6\u6784\u53ef\u89e3\u91ca\u4e3a\u65f6\u95f4\u5ef6\u8fdf\u52a8\u6001\u6a21\u6001\u5206\u89e3\uff08TD-DMD\uff09\u7684\u975e\u7ebf\u6027\u63a8\u5e7f\uff0c\u5177\u6709\u5e8f\u5217\u957f\u5ea6\u7684\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5c11\u91cf\u53c2\u6570\u3002", "result": "\u5728\u8fd1\u7ebf\u6027\u7cfb\u7edf\u4e0a\uff0cTD-TF\u4e0e\u5f3a\u7ebf\u6027\u57fa\u7ebf\u6027\u80fd\u76f8\u5f53\uff1b\u5728\u975e\u7ebf\u6027\u548c\u6df7\u6c8c\u7cfb\u7edf\u4e2d\uff0c\u663e\u8457\u4f18\u4e8e\u7ebf\u6027\u65b9\u6cd5\uff0c\u80fd\u51c6\u786e\u6355\u6349\u957f\u671f\u52a8\u529b\u5b66\u3002\u5728\u5408\u6210\u4fe1\u53f7\u3001\u975e\u5b9a\u5e38\u7a7a\u6c14\u52a8\u529b\u5b66\u3001Lorenz '63\u7cfb\u7edf\u548c\u53cd\u5e94\u6269\u6563\u6a21\u578b\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0cTD-TF\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\u7684\u540c\u65f6\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u590d\u6742\u52a8\u529b\u5b66\u7684\u8868\u8fbe\u80fd\u529b\u3002", "conclusion": "TD-TF\u6210\u529f\u6865\u63a5\u4e86\u7ebf\u6027\u7b97\u5b50\u65b9\u6cd5\u548c\u6df1\u5ea6\u5e8f\u5217\u6a21\u578b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u8868\u8fbe\u80fd\u529b\u5f3a\u7684\u975e\u5b9a\u5e38\u65f6\u7a7a\u52a8\u529b\u5b66\u5efa\u6a21\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u7ebf\u6027\u6a21\u578b\u4f18\u70b9\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5904\u7406\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u80fd\u529b\u3002"}}
{"id": "2602.08489", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08489", "abs": "https://arxiv.org/abs/2602.08489", "authors": ["Hyunseok Lee", "Soheil Abbasloo", "Jihoon Tack", "Jinwoo Shin"], "title": "Beyond Correctness: Learning Robust Reasoning via Transfer", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently strengthened LLM reasoning, but its focus on final answer correctness leaves a critical gap: it does not ensure the robustness of the reasoning process itself. We adopt a simple philosophical view, robust reasoning should remain useful beyond the mind that produced it, and treat reasoning as a form of meaning transfer that must survive truncation, reinterpretation, and continuation. Building on this principle, we introduce Reinforcement Learning with Transferable Reward (RLTR), which operationalizes robustness via transfer reward that tests whether a partial reasoning prefix from one model can guide a separate model to the correct answer. This encourages LLMs to produce reasoning that is stable, interpretable, and genuinely generalizable. Our approach improves sampling consistency while improving final answer accuracy, and it reaches comparable performance in substantially fewer training steps. For example, on MATH500, RLTR achieves a +3.6%p gain in Maj@64 compared to RLVR and matches RLVR's average accuracy with roughly 2.5x fewer training steps, providing both more reliable reasoning and significantly more sample efficient.", "AI": {"tldr": "RLTR\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u8f6c\u79fb\u5956\u52b1\u6765\u589e\u5f3aLLM\u63a8\u7406\u8fc7\u7a0b\u7684\u9c81\u68d2\u6027\uff0c\u76f8\u6bd4\u4ec5\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u6b63\u786e\u6027\u7684RLVR\uff0cRLTR\u80fd\u4ea7\u751f\u66f4\u7a33\u5b9a\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u6cdb\u5316\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u53ea\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u7684\u6b63\u786e\u6027\uff0c\u5ffd\u7565\u4e86\u63a8\u7406\u8fc7\u7a0b\u672c\u8eab\u7684\u9c81\u68d2\u6027\u3002\u4f5c\u8005\u8ba4\u4e3a\u7a33\u5065\u7684\u63a8\u7406\u5e94\u8be5\u5177\u6709\u53ef\u8f6c\u79fb\u6027\uff0c\u5373\u4f7f\u63a8\u7406\u8fc7\u7a0b\u88ab\u622a\u65ad\u3001\u91cd\u65b0\u89e3\u91ca\u6216\u7ee7\u7eed\uff0c\u4ecd\u80fd\u4fdd\u6301\u5176\u6709\u6548\u6027\u3002", "method": "\u63d0\u51faRLTR\uff08Reinforcement Learning with Transferable Reward\uff09\uff0c\u901a\u8fc7\u8f6c\u79fb\u5956\u52b1\u6765\u64cd\u4f5c\u5316\u9c81\u68d2\u6027\uff1a\u6d4b\u8bd5\u4ece\u4e00\u4e2a\u6a21\u578b\u63d0\u53d6\u7684\u90e8\u5206\u63a8\u7406\u524d\u7f00\u662f\u5426\u80fd\u6307\u5bfc\u53e6\u4e00\u4e2a\u6a21\u578b\u5f97\u5230\u6b63\u786e\u7b54\u6848\u3002\u8fd9\u79cd\u65b9\u6cd5\u9f13\u52b1LLM\u4ea7\u751f\u7a33\u5b9a\u3001\u53ef\u89e3\u91ca\u4e14\u771f\u6b63\u53ef\u6cdb\u5316\u7684\u63a8\u7406\u3002", "result": "RLTR\u5728\u63d0\u9ad8\u91c7\u6837\u4e00\u81f4\u6027\u7684\u540c\u65f6\u6539\u5584\u4e86\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u7387\uff0c\u5e76\u4e14\u7528\u66f4\u5c11\u7684\u8bad\u7ec3\u6b65\u9aa4\u8fbe\u5230\u53ef\u6bd4\u6027\u80fd\u3002\u5728MATH500\u4e0a\uff0cRLTR\u76f8\u6bd4RLVR\u83b7\u5f97+3.6%p\u7684Maj@64\u63d0\u5347\uff0c\u5e76\u4e14\u7528\u7ea62.5\u500d\u66f4\u5c11\u7684\u8bad\u7ec3\u6b65\u9aa4\u5339\u914dRLVR\u7684\u5e73\u5747\u51c6\u786e\u7387\u3002", "conclusion": "RLTR\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u8fd8\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\uff0c\u4e3aLLM\u63a8\u7406\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002"}}
{"id": "2602.08499", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08499", "abs": "https://arxiv.org/abs/2602.08499", "authors": ["Xiaodong Lu", "Xiaohan Wang", "Jiajun Chai", "Guojun Yin", "Wei Lin", "Zhijun Chen", "Yu Luo", "Fuzhen Zhuang", "Yikun Ban", "Deqing Wang"], "title": "Contextual Rollout Bandits for Reinforcement Learning with Verifiable Rewards", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is an effective paradigm for improving the reasoning capabilities of large language models. However, existing RLVR methods utilize rollouts in an indiscriminate and short-horizon manner: responses of heterogeneous quality within each prompt are treated uniformly, and historical rollouts are discarded after a single use. This leads to noisy supervision, poor sample efficiency, and suboptimal policy updates. We address these issues by formulating rollout scheduling in RLVR as a contextual bandit problem and proposing a unified neural scheduling framework that adaptively selects high-value rollouts throughout training. Each rollout is treated as an arm whose reward is defined by the induced performance gain between consecutive optimization steps. The resulting scheduler supports both noise-aware intra-group selection and adaptive global reuse of historical rollouts within a single principled framework. We provide theoretical justification by deriving sublinear regret bounds and showing that enlarging the rollout buffer improves the achievable performance upper bound. Experiments on six mathematical reasoning benchmarks demonstrate consistent gains in performance and training efficiency across multiple RLVR optimization methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u8001\u864e\u673a\u7684RLVR\u8c03\u5ea6\u6846\u67b6\uff0c\u81ea\u9002\u5e94\u9009\u62e9\u9ad8\u8d28\u91cfrollout\uff0c\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u5728rollout\u4f7f\u7528\u4e0a\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a1) \u5bf9\u540c\u4e00\u63d0\u793a\u4e0b\u8d28\u91cf\u4e0d\u540c\u7684\u54cd\u5e94\u91c7\u7528\u7edf\u4e00\u5904\u7406\uff0c\u5bfc\u81f4\u566a\u58f0\u76d1\u7763\uff1b2) \u5386\u53f2rollout\u4ec5\u4f7f\u7528\u4e00\u6b21\u5373\u4e22\u5f03\uff0c\u5bfc\u81f4\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u548c\u6b21\u4f18\u7b56\u7565\u66f4\u65b0", "method": "\u5c06RLVR\u4e2d\u7684rollout\u8c03\u5ea6\u5efa\u6a21\u4e3a\u4e0a\u4e0b\u6587\u8001\u864e\u673a\u95ee\u9898\uff0c\u63d0\u51fa\u7edf\u4e00\u7684\u795e\u7ecf\u8c03\u5ea6\u6846\u67b6\u3002\u6bcf\u4e2arollout\u88ab\u89c6\u4e3a\u4e00\u4e2a\u81c2\uff0c\u5176\u5956\u52b1\u5b9a\u4e49\u4e3a\u8fde\u7eed\u4f18\u5316\u6b65\u9aa4\u95f4\u7684\u6027\u80fd\u589e\u76ca\u63d0\u5347\u3002\u8be5\u8c03\u5ea6\u5668\u652f\u6301\u566a\u58f0\u611f\u77e5\u7684\u7ec4\u5185\u9009\u62e9\u548c\u5386\u53f2rollout\u7684\u81ea\u9002\u5e94\u5168\u5c40\u91cd\u7528", "result": "\u5728\u516d\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2aRLVR\u4f18\u5316\u65b9\u6cd5\u4e0a\u5747\u8868\u73b0\u51fa\u6027\u80fd\u63d0\u5347\u548c\u8bad\u7ec3\u6548\u7387\u63d0\u9ad8", "conclusion": "\u901a\u8fc7\u5c06rollout\u8c03\u5ea6\u5f62\u5f0f\u5316\u4e3a\u4e0a\u4e0b\u6587\u8001\u864e\u673a\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u8c03\u5ea6\u6846\u67b6\u80fd\u591f\u81ea\u9002\u5e94\u9009\u62e9\u9ad8\u4ef7\u503crollout\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709RLVR\u65b9\u6cd5\u7684\u566a\u58f0\u76d1\u7763\u548c\u6837\u672c\u6548\u7387\u95ee\u9898\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027"}}
{"id": "2602.08500", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08500", "abs": "https://arxiv.org/abs/2602.08500", "authors": ["Maiqi Jiang", "Noman Ali", "Yiran Ding", "Yanfu Zhang"], "title": "Is Meta-Path Attention an Explanation? Evidence of Alignment and Decoupling in Heterogeneous GNNs", "comment": null, "summary": "Meta-path-based heterogeneous graph neural networks aggregate over meta-path-induced views, and their semantic-level attention over meta-path channels is widely used as a narrative for ``which semantics matter.'' We study this assumption empirically by asking: when does meta-path attention reflect meta-path importance, and when can it decouple? A key challenge is that most post-hoc GNN explainers are designed for homogeneous graphs, and naive adaptations to heterogeneous neighborhoods can mix semantics and confound perturbations. To enable a controlled empirical analysis, we introduce MetaXplain, a meta-path-aware post-hoc explanation protocol that applies existing explainers in the native meta-path view domain via (i) view-factorized explanations, (ii) schema-valid channel-wise perturbations, and (iii) fusion-aware attribution, without modifying the underlying predictor. We benchmark representative gradient-, perturbation-, and Shapley-style explainers on ACM, DBLP, and IMDB with HAN and HAN-GCN, comparing against xPath and type-matched random baselines under standard faithfulness metrics. To quantify attention reliability, we propose Meta-Path Attention--Explanation Alignment (MP-AEA), which measures rank correlation between learned attention weights and explanation-derived meta-path contribution scores across random runs. Our results show that meta-path-aware explanations typically outperform random controls, while MP-AEA reveals both high-alignment and statistically significant decoupling regimes depending on the dataset and backbone; moreover, retraining on explanation-induced subgraphs often preserves, and in some noisy regimes improves, predictive performance, suggesting an explanation-as-denoising effect.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5b9e\u8bc1\u5206\u6790\u4e86\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u5143\u8def\u5f84\u6ce8\u610f\u529b\u662f\u5426\u771f\u5b9e\u53cd\u6620\u5143\u8def\u5f84\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u4e86MetaXplain\u89e3\u91ca\u534f\u8bae\u548cMP-AEA\u5bf9\u9f50\u5ea6\u91cf\uff0c\u53d1\u73b0\u6ce8\u610f\u529b\u4e0e\u91cd\u8981\u6027\u5b58\u5728\u5bf9\u9f50\u548c\u8131\u94a9\u4e24\u79cd\u72b6\u6001\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u9a8c\u8bc1\u5143\u8def\u5f84\u6ce8\u610f\u529b\u673a\u5236\u662f\u5426\u771f\u5b9e\u53cd\u6620\u5143\u8def\u5f84\u91cd\u8981\u6027\uff0c\u56e0\u4e3a\u73b0\u6709\u89e3\u91ca\u5668\u4e3b\u8981\u9488\u5bf9\u540c\u6784\u56fe\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u5f02\u6784\u56fe\u4e2d\u7684\u8bed\u4e49\u6df7\u5408\u95ee\u9898\u3002", "method": "\u63d0\u51faMetaXplain\u89e3\u91ca\u534f\u8bae\uff0c\u5305\u542b\u89c6\u56fe\u5206\u89e3\u89e3\u91ca\u3001\u6a21\u5f0f\u6709\u6548\u901a\u9053\u6270\u52a8\u548c\u878d\u5408\u611f\u77e5\u5f52\u56e0\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u5e76\u8bbe\u8ba1MP-AEA\u5ea6\u91cf\u6765\u91cf\u5316\u6ce8\u610f\u529b\u4e0e\u89e3\u91ca\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002", "result": "\u5143\u8def\u5f84\u611f\u77e5\u89e3\u91ca\u5668\u901a\u5e38\u4f18\u4e8e\u968f\u673a\u57fa\u7ebf\uff1bMP-AEA\u663e\u793a\u6ce8\u610f\u529b\u4e0e\u91cd\u8981\u6027\u5b58\u5728\u9ad8\u5bf9\u9f50\u548c\u663e\u8457\u8131\u94a9\u4e24\u79cd\u72b6\u6001\uff1b\u57fa\u4e8e\u89e3\u91ca\u8bf1\u5bfc\u5b50\u56fe\u91cd\u8bad\u7ec3\u80fd\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u5143\u8def\u5f84\u6ce8\u610f\u529b\u5e76\u4e0d\u603b\u662f\u53cd\u6620\u5143\u8def\u5f84\u91cd\u8981\u6027\uff0c\u5b58\u5728\u8131\u94a9\u73b0\u8c61\uff1b\u63d0\u51fa\u7684\u89e3\u91ca\u534f\u8bae\u80fd\u6709\u6548\u8bc4\u4f30\u6ce8\u610f\u529b\u53ef\u9760\u6027\uff1b\u89e3\u91ca\u5177\u6709\u53bb\u566a\u6548\u679c\uff0c\u53ef\u7528\u4e8e\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.08519", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08519", "abs": "https://arxiv.org/abs/2602.08519", "authors": ["Yunhui Liu", "Pengyu Qiu", "Yu Xing", "Yongchao Liu", "Peng Du", "Chuntao Hong", "Jiajun Zheng", "Tao Zheng", "Tieke He"], "title": "Bridging Academia and Industry: A Comprehensive Benchmark for Attributed Graph Clustering", "comment": null, "summary": "Attributed Graph Clustering (AGC) is a fundamental unsupervised task that integrates structural topology and node attributes to uncover latent patterns in graph-structured data. Despite its significance in industrial applications such as fraud detection and user segmentation, a significant chasm persists between academic research and real-world deployment. Current evaluation protocols suffer from the small-scale, high-homophily citation datasets, non-scalable full-batch training paradigms, and a reliance on supervised metrics that fail to reflect performance in label-scarce environments. To bridge these gaps, we present PyAGC, a comprehensive, production-ready benchmark and library designed to stress-test AGC methods across diverse scales and structural properties. We unify existing methodologies into a modular Encode-Cluster-Optimize framework and, for the first time, provide memory-efficient, mini-batch implementations for a wide array of state-of-the-art AGC algorithms. Our benchmark curates 12 diverse datasets, ranging from 2.7K to 111M nodes, specifically incorporating industrial graphs with complex tabular features and low homophily. Furthermore, we advocate for a holistic evaluation protocol that mandates unsupervised structural metrics and efficiency profiling alongside traditional supervised metrics. Battle-tested in high-stakes industrial workflows at Ant Group, this benchmark offers the community a robust, reproducible, and scalable platform to advance AGC research towards realistic deployment. The code and resources are publicly available via GitHub (https://github.com/Cloudy1225/PyAGC), PyPI (https://pypi.org/project/pyagc), and Documentation (https://pyagc.readthedocs.io).", "AI": {"tldr": "PyAGC\u662f\u4e00\u4e2a\u9762\u5411\u5de5\u4e1a\u90e8\u7f72\u7684\u56fe\u805a\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u5e93\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u5b66\u672f\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u7684mini-batch\u5b9e\u73b0\u548c\u591a\u6837\u5316\u6570\u636e\u96c6\u3002", "motivation": "\u5f53\u524d\u56fe\u805a\u7c7b\u7814\u7a76\u5b58\u5728\u4e09\u5927\u95ee\u9898\uff1a1) \u4f7f\u7528\u5c0f\u89c4\u6a21\u3001\u9ad8\u540c\u8d28\u6027\u5f15\u6587\u6570\u636e\u96c6\uff1b2) \u4f9d\u8d56\u4e0d\u53ef\u6269\u5c55\u7684\u5168\u6279\u6b21\u8bad\u7ec3\uff1b3) \u4f7f\u7528\u6709\u76d1\u7763\u6307\u6807\u8bc4\u4f30\uff0c\u65e0\u6cd5\u53cd\u6620\u6807\u7b7e\u7a00\u7f3a\u73af\u5883\u4e0b\u7684\u771f\u5b9e\u6027\u80fd\u3002\u5b66\u672f\u7814\u7a76\u4e0e\u5de5\u4e1a\u90e8\u7f72\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u9e3f\u6c9f\u3002", "method": "1) \u63d0\u51fa\u6a21\u5757\u5316\u7684Encode-Cluster-Optimize\u6846\u67b6\u7edf\u4e00\u73b0\u6709\u65b9\u6cd5\uff1b2) \u9996\u6b21\u4e3a\u591a\u79cd\u5148\u8fdbAGC\u7b97\u6cd5\u63d0\u4f9b\u5185\u5b58\u9ad8\u6548\u7684mini-batch\u5b9e\u73b0\uff1b3) \u6784\u5efa\u5305\u542b12\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\uff082.7K-111M\u8282\u70b9\uff09\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7279\u522b\u5305\u542b\u5de5\u4e1a\u7ea7\u590d\u6742\u8868\u683c\u7279\u5f81\u548c\u4f4e\u540c\u8d28\u6027\u56fe\uff1b4) \u63d0\u51fa\u5305\u542b\u65e0\u76d1\u7763\u7ed3\u6784\u6307\u6807\u548c\u6548\u7387\u5206\u6790\u7684\u7efc\u5408\u8bc4\u4f30\u534f\u8bae\u3002", "result": "\u5f00\u53d1\u4e86PyAGC\u5e93\uff0c\u5df2\u5728\u8682\u8681\u96c6\u56e2\u9ad8\u98ce\u9669\u5de5\u4e1a\u5de5\u4f5c\u6d41\u4e2d\u9a8c\u8bc1\uff0c\u63d0\u4f9b\u516c\u5f00\u53ef\u7528\u7684\u4ee3\u7801\u3001\u8d44\u6e90\u548c\u6587\u6863\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u7a33\u5065\u3001\u53ef\u590d\u73b0\u3001\u53ef\u6269\u5c55\u7684AGC\u7814\u7a76\u5e73\u53f0\u3002", "conclusion": "PyAGC\u586b\u8865\u4e86\u56fe\u805a\u7c7b\u7814\u7a76\u4ece\u5b66\u672f\u5230\u5de5\u4e1a\u90e8\u7f72\u7684\u9e3f\u6c9f\uff0c\u901a\u8fc7\u63d0\u4f9b\u751f\u4ea7\u5c31\u7eea\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e93\uff0c\u63a8\u52a8AGC\u7814\u7a76\u5411\u5b9e\u9645\u5e94\u7528\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2602.08535", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08535", "abs": "https://arxiv.org/abs/2602.08535", "authors": ["Rui Wu", "Li YongJun"], "title": "Causal Schr\u00f6dinger Bridges: Constrained Optimal Transport on Structural Manifolds", "comment": "12 pages, 7 figures", "summary": "Generative modeling typically seeks the path of least action via deterministic flows (ODE). While effective for in-distribution tasks, we argue that these deterministic paths become brittle under causal interventions, which often require transporting probability mass across low-density regions (\"off-manifold\") where the vector field is ill-defined. This leads to numerical instability and spurious correlations. In this work, we introduce the Causal Schr\u00f6dinger Bridge (CSB), a framework that reformulates counterfactual inference as Entropic Optimal Transport. Unlike deterministic approaches that require strict invertibility, CSB leverages diffusion processes (SDEs) to robustly \"tunnel\" through support mismatches while strictly enforcing structural admissibility constraints. We prove the Structural Decomposition Theorem, showing that the global high-dimensional bridge factorizes into local, robust transitions. Empirical validation on high-dimensional interventions (Morpho-MNIST) demonstrates that CSB significantly outperforms deterministic baselines in structural consistency, particularly in regimes of strong, out-of-distribution treatments.", "AI": {"tldr": "\u63d0\u51faCausal Schr\u00f6dinger Bridge (CSB)\u6846\u67b6\uff0c\u5c06\u53cd\u4e8b\u5b9e\u63a8\u7406\u91cd\u6784\u4e3a\u71b5\u6700\u4f18\u4f20\u8f93\u95ee\u9898\uff0c\u901a\u8fc7\u6269\u6563\u8fc7\u7a0b\u5728\u652f\u6301\u96c6\u4e0d\u5339\u914d\u65f6\u7a33\u5065\u5730\"\u7a7f\u8d8a\"\u4f4e\u5bc6\u5ea6\u533a\u57df\uff0c\u4f18\u4e8e\u786e\u5b9a\u6027\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u751f\u6210\u5efa\u6a21\u4f7f\u7528\u786e\u5b9a\u6027\u6d41\uff08ODE\uff09\u5bfb\u627e\u6700\u5c0f\u4f5c\u7528\u8def\u5f84\uff0c\u4f46\u5728\u56e0\u679c\u5e72\u9884\u4e0b\u53d8\u5f97\u8106\u5f31\uff0c\u56e0\u4e3a\u9700\u8981\u8de8\u8fc7\u4f4e\u5bc6\u5ea6\u533a\u57df\uff08\"\u79bb\u6d41\u5f62\"\uff09\u4f20\u8f93\u6982\u7387\u8d28\u91cf\uff0c\u800c\u8be5\u533a\u57df\u7684\u5411\u91cf\u573a\u5b9a\u4e49\u4e0d\u6e05\uff0c\u5bfc\u81f4\u6570\u503c\u4e0d\u7a33\u5b9a\u548c\u865a\u5047\u76f8\u5173\u6027\u3002", "method": "\u5f15\u5165Causal Schr\u00f6dinger Bridge (CSB)\u6846\u67b6\uff0c\u5c06\u53cd\u4e8b\u5b9e\u63a8\u7406\u91cd\u6784\u4e3a\u71b5\u6700\u4f18\u4f20\u8f93\u95ee\u9898\u3002\u4e0e\u9700\u8981\u4e25\u683c\u53ef\u9006\u6027\u7684\u786e\u5b9a\u6027\u65b9\u6cd5\u4e0d\u540c\uff0cCSB\u5229\u7528\u6269\u6563\u8fc7\u7a0b\uff08SDEs\uff09\u5728\u652f\u6301\u96c6\u4e0d\u5339\u914d\u65f6\u7a33\u5065\u5730\"\u7a7f\u8d8a\"\u4f4e\u5bc6\u5ea6\u533a\u57df\uff0c\u540c\u65f6\u4e25\u683c\u5f3a\u5236\u6267\u884c\u7ed3\u6784\u53ef\u63a5\u53d7\u6027\u7ea6\u675f\u3002\u8bc1\u660e\u4e86\u7ed3\u6784\u5206\u89e3\u5b9a\u7406\uff0c\u8868\u660e\u5168\u5c40\u9ad8\u7ef4\u6865\u53ef\u5206\u89e3\u4e3a\u5c40\u90e8\u7a33\u5065\u7684\u8f6c\u79fb\u3002", "result": "\u5728\u9ad8\u7ef4\u5e72\u9884\uff08Morpho-MNIST\uff09\u4e0a\u7684\u5b9e\u8bc1\u9a8c\u8bc1\u8868\u660e\uff0cCSB\u5728\u7ed3\u6784\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u786e\u5b9a\u6027\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5f3a\u3001\u5206\u5e03\u5916\u5904\u7406\u7684\u673a\u5236\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "CSB\u4e3a\u56e0\u679c\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5065\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u8fc7\u7a0b\u5904\u7406\u652f\u6301\u96c6\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5728\u53cd\u4e8b\u5b9e\u63a8\u7406\u4e2d\u6bd4\u786e\u5b9a\u6027\u65b9\u6cd5\u66f4\u53ef\u9760\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u8de8\u4f4e\u5bc6\u5ea6\u533a\u57df\u4f20\u8f93\u6982\u7387\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2602.08552", "categories": ["cs.LG", "eess.AS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08552", "abs": "https://arxiv.org/abs/2602.08552", "authors": ["Fredrik Cumlin"], "title": "Rho-Perfect: Correlation Ceiling For Subjective Evaluation Datasets", "comment": null, "summary": "Subjective ratings contain inherent noise that limits the model-human correlation, but this reliability issue is rarely quantified. In this paper, we present $\u03c1$-Perfect, a practical estimation of the highest achievable correlation of a model on subjectively rated datasets. We define $\u03c1$-Perfect to be the correlation between a perfect predictor and human ratings, and derive an estimate of the value based on heteroscedastic noise scenarios, a common occurrence in subjectively rated datasets. We show that $\u03c1$-Perfect squared estimates test-retest correlation and use this to validate the estimate. We demonstrate the use of $\u03c1$-Perfect on a speech quality dataset and show how the measure can distinguish between model limitations and data quality issues.", "AI": {"tldr": "\u63d0\u51fa\u03c1-Perfect\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u4e3b\u89c2\u8bc4\u5206\u6570\u636e\u96c6\u4e0a\u6a21\u578b\u80fd\u8fbe\u5230\u7684\u6700\u9ad8\u76f8\u5173\u6027\uff0c\u91cf\u5316\u6570\u636e\u53ef\u9760\u6027\u95ee\u9898", "motivation": "\u4e3b\u89c2\u8bc4\u5206\u5b58\u5728\u56fa\u6709\u566a\u58f0\uff0c\u9650\u5236\u4e86\u6a21\u578b\u4e0e\u4eba\u7c7b\u7684\u76f8\u5173\u6027\uff0c\u4f46\u8fd9\u79cd\u53ef\u9760\u6027\u95ee\u9898\u5f88\u5c11\u88ab\u91cf\u5316", "method": "\u5b9a\u4e49\u03c1-Perfect\u4e3a\u5b8c\u7f8e\u9884\u6d4b\u5668\u4e0e\u4eba\u7c7b\u8bc4\u5206\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u57fa\u4e8e\u5f02\u65b9\u5dee\u566a\u58f0\u573a\u666f\u63a8\u5bfc\u4f30\u8ba1\u503c\uff0c\u4f7f\u7528\u91cd\u6d4b\u76f8\u5173\u6027\u9a8c\u8bc1", "result": "\u5728\u8bed\u97f3\u8d28\u91cf\u6570\u636e\u96c6\u4e0a\u6f14\u793a\u03c1-Perfect\u7684\u5e94\u7528\uff0c\u663e\u793a\u8be5\u6307\u6807\u80fd\u533a\u5206\u6a21\u578b\u9650\u5236\u4e0e\u6570\u636e\u8d28\u91cf\u95ee\u9898", "conclusion": "\u03c1-Perfect\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6700\u9ad8\u53ef\u8fbe\u5230\u76f8\u5173\u6027\u4f30\u8ba1\uff0c\u6709\u52a9\u4e8e\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u548c\u6570\u636e\u8d28\u91cf"}}
{"id": "2602.08563", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08563", "abs": "https://arxiv.org/abs/2602.08563", "authors": ["Ahmed Salem", "Andrew Paverd", "Sahar Abdelnabi"], "title": "Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs", "comment": "Accepted at IEEE SaTML 2026", "summary": "Large language models (LLMs) are commonly treated as stateless: once an interaction ends, no information is assumed to persist unless it is explicitly stored and re-supplied. We challenge this assumption by introducing implicit memory-the ability of a model to carry state across otherwise independent interactions by encoding information in its own outputs and later recovering it when those outputs are reintroduced as input. This mechanism does not require any explicit memory module, yet it creates a persistent information channel across inference requests. As a concrete demonstration, we introduce a new class of temporal backdoors, which we call time bombs. Unlike conventional backdoors that activate on a single trigger input, time bombs activate only after a sequence of interactions satisfies hidden conditions accumulated via implicit memory. We show that such behavior can be induced today through straightforward prompting or fine-tuning. Beyond this case study, we analyze broader implications of implicit memory, including covert inter-agent communication, benchmark contamination, targeted manipulation, and training-data poisoning. Finally, we discuss detection challenges and outline directions for stress-testing and evaluation, with the goal of anticipating and controlling future developments. To promote future research, we release code and data at: https://github.com/microsoft/implicitMemory.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u9690\u5f0f\u8bb0\u5fc6\"\u6982\u5ff5\uff0c\u6311\u6218LLMs\u65e0\u72b6\u6001\u7684\u5047\u8bbe\uff0c\u5c55\u793a\u6a21\u578b\u53ef\u901a\u8fc7\u8f93\u51fa\u7f16\u7801\u4fe1\u606f\u5e76\u5728\u540e\u7eed\u4ea4\u4e92\u4e2d\u6062\u590d\uff0c\u5b9e\u73b0\u8de8\u4ea4\u4e92\u7684\u6301\u4e45\u4fe1\u606f\u901a\u9053\uff0c\u5e76\u5f15\u5165\"\u5b9a\u65f6\u70b8\u5f39\"\u8fd9\u4e00\u65b0\u578b\u65f6\u95f4\u540e\u95e8\u653b\u51fb\u3002", "motivation": "\u6311\u6218\u5f53\u524d\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u65e0\u72b6\u6001\u7684\u666e\u904d\u5047\u8bbe\uff0c\u63a2\u7d22\u6a21\u578b\u662f\u5426\u80fd\u591f\u901a\u8fc7\u81ea\u8eab\u8f93\u51fa\u7f16\u7801\u4fe1\u606f\u5e76\u5728\u540e\u7eed\u4ea4\u4e92\u4e2d\u6062\u590d\uff0c\u4ece\u800c\u521b\u5efa\u8de8\u63a8\u7406\u8bf7\u6c42\u7684\u6301\u4e45\u4fe1\u606f\u901a\u9053\u3002", "method": "\u5f15\u5165\"\u9690\u5f0f\u8bb0\u5fc6\"\u6982\u5ff5\uff0c\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u6216\u5fae\u8c03\u4f7f\u6a21\u578b\u5728\u8f93\u51fa\u4e2d\u7f16\u7801\u4fe1\u606f\uff0c\u540e\u7eed\u5f53\u8fd9\u4e9b\u8f93\u51fa\u91cd\u65b0\u4f5c\u4e3a\u8f93\u5165\u65f6\u6062\u590d\u4fe1\u606f\u3002\u5177\u4f53\u6f14\u793a\u4e86\"\u5b9a\u65f6\u70b8\u5f39\"\u8fd9\u4e00\u65b0\u578b\u65f6\u95f4\u540e\u95e8\u653b\u51fb\uff0c\u5176\u6fc0\u6d3b\u9700\u8981\u6ee1\u8db3\u901a\u8fc7\u9690\u5f0f\u8bb0\u5fc6\u7d2f\u79ef\u7684\u9690\u85cf\u6761\u4ef6\u5e8f\u5217\u3002", "result": "\u8bc1\u660e\u9690\u5f0f\u8bb0\u5fc6\u884c\u4e3a\u53ef\u901a\u8fc7\u7b80\u5355\u63d0\u793a\u6216\u5fae\u8c03\u5b9e\u73b0\uff0c\u5c55\u793a\u4e86\u5b9a\u65f6\u70b8\u5f39\u653b\u51fb\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5206\u6790\u4e86\u9690\u5f0f\u8bb0\u5fc6\u5728\u9690\u853d\u8de8\u4ee3\u7406\u901a\u4fe1\u3001\u57fa\u51c6\u6c61\u67d3\u3001\u5b9a\u5411\u64cd\u7eb5\u548c\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u7b49\u65b9\u9762\u7684\u5e7f\u6cdb\u5f71\u54cd\u3002", "conclusion": "\u9690\u5f0f\u8bb0\u5fc6\u6311\u6218\u4e86LLMs\u65e0\u72b6\u6001\u7684\u57fa\u672c\u5047\u8bbe\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b89\u5168\u5f71\u54cd\u3002\u9700\u8981\u5f00\u53d1\u68c0\u6d4b\u65b9\u6cd5\u3001\u538b\u529b\u6d4b\u8bd5\u548c\u8bc4\u4f30\u6846\u67b6\u6765\u5e94\u5bf9\u8fd9\u4e00\u65b0\u5174\u5a01\u80c1\uff0c\u5e76\u53d1\u5e03\u4e86\u4ee3\u7801\u548c\u6570\u636e\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2602.08564", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08564", "abs": "https://arxiv.org/abs/2602.08564", "authors": ["Tiantong Wang", "Yiyang Duan", "Haoyu Chen", "Tiantong Wu", "Wei Yang Bryan Lim"], "title": "M-Loss: Quantifying Model Merging Compatibility with Limited Unlabeled Data", "comment": "Code available at https://github.com/languangduan/mLoss", "summary": "Training of large-scale models is both computationally intensive and often constrained by the availability of labeled data. Model merging offers a compelling alternative by directly integrating the weights of multiple source models without requiring additional data or extensive training. However, conventional model merging techniques, such as parameter averaging, often suffer from the unintended combination of non-generalizable features, especially when source models exhibit significant weight disparities. Comparatively, model ensembling generally provides more stable and superior performance that aggregates multiple models by averaging outputs. However, it incurs higher inference costs and increased storage requirements. While previous studies experimentally showed the similarities between model merging and ensembling, theoretical evidence and evaluation metrics remain lacking. To address this gap, we introduce Merging-ensembling loss (M-Loss), a novel evaluation metric that quantifies the compatibility of merging source models using very limited unlabeled data. By measuring the discrepancy between parameter averaging and model ensembling at layer and node levels, M-Loss facilitates more effective merging strategies. Specifically, M-Loss serves both as a quantitative criterion of the theoretical feasibility of model merging, and a guide for parameter significance in model pruning. Our theoretical analysis and empirical evaluations demonstrate that incorporating M-Loss into the merging process significantly improves the alignment between merged models and model ensembling, providing a scalable and efficient framework for accurate model consolidation.", "AI": {"tldr": "\u63d0\u51faM-Loss\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u91cf\u5316\u6a21\u578b\u5408\u5e76\u7684\u517c\u5bb9\u6027\uff0c\u901a\u8fc7\u6d4b\u91cf\u53c2\u6570\u5e73\u5747\u4e0e\u6a21\u578b\u96c6\u6210\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u6307\u5bfc\u66f4\u6709\u6548\u7684\u6a21\u578b\u5408\u5e76\u7b56\u7565\u3002", "motivation": "\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u8ba1\u7b97\u5bc6\u96c6\u4e14\u53d7\u9650\u4e8e\u6807\u6ce8\u6570\u636e\u3002\u6a21\u578b\u5408\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u6570\u636e\u6216\u5927\u91cf\u8bad\u7ec3\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u4f20\u7edf\u53c2\u6570\u5e73\u5747\u65b9\u6cd5\u5bb9\u6613\u7ec4\u5408\u4e0d\u53ef\u6cdb\u5316\u7684\u7279\u5f81\uff0c\u800c\u6a21\u578b\u96c6\u6210\u867d\u7136\u6027\u80fd\u66f4\u7a33\u5b9a\u4f46\u63a8\u7406\u6210\u672c\u548c\u5b58\u50a8\u9700\u6c42\u66f4\u9ad8\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u7406\u8bba\u8bc1\u636e\u548c\u8bc4\u4f30\u6307\u6807\u6765\u6307\u5bfc\u6a21\u578b\u5408\u5e76\u3002", "method": "\u63d0\u51faMerging-ensembling loss (M-Loss)\u8bc4\u4f30\u6307\u6807\uff0c\u4f7f\u7528\u5c11\u91cf\u65e0\u6807\u7b7e\u6570\u636e\u91cf\u5316\u6a21\u578b\u5408\u5e76\u7684\u517c\u5bb9\u6027\u3002\u901a\u8fc7\u6d4b\u91cf\u53c2\u6570\u5e73\u5747\u4e0e\u6a21\u578b\u96c6\u6210\u5728\u5c42\u548c\u8282\u70b9\u7ea7\u522b\u7684\u5dee\u5f02\uff0c\u6307\u5bfc\u6a21\u578b\u5408\u5e76\u7b56\u7565\u3002M-Loss\u65e2\u4f5c\u4e3a\u6a21\u578b\u5408\u5e76\u7406\u8bba\u53ef\u884c\u6027\u7684\u5b9a\u91cf\u6807\u51c6\uff0c\u4e5f\u4f5c\u4e3a\u6a21\u578b\u526a\u679d\u4e2d\u53c2\u6570\u91cd\u8981\u6027\u7684\u6307\u5bfc\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u5c06M-Loss\u7eb3\u5165\u5408\u5e76\u8fc7\u7a0b\u80fd\u663e\u8457\u63d0\u9ad8\u5408\u5e76\u6a21\u578b\u4e0e\u6a21\u578b\u96c6\u6210\u4e4b\u95f4\u7684\u5bf9\u9f50\u5ea6\uff0c\u4e3a\u51c6\u786e\u6a21\u578b\u6574\u5408\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u6846\u67b6\u3002", "conclusion": "M-Loss\u586b\u8865\u4e86\u6a21\u578b\u5408\u5e76\u4e0e\u96c6\u6210\u4e4b\u95f4\u7406\u8bba\u8bc1\u636e\u548c\u8bc4\u4f30\u6307\u6807\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u91cf\u5316\u6a21\u578b\u5408\u5e76\u517c\u5bb9\u6027\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6307\u5bfc\u66f4\u6709\u6548\u7684\u6a21\u578b\u5408\u5e76\u7b56\u7565\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u548c\u9ad8\u6548\u7684\u6a21\u578b\u6574\u5408\u3002"}}
{"id": "2602.08577", "categories": ["cs.LG", "math.CO", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.08577", "abs": "https://arxiv.org/abs/2602.08577", "authors": ["Theodoros Anagnostopoulos", "Evanthia Zervoudi", "Christos Anagnostopoulos", "Apostolos Christopoulos", "Bogdan Wierzbinski"], "title": "An arithmetic method algorithm optimizing k-nearest neighbors compared to regression algorithms and evaluated on real world data sources", "comment": "Nature Scientific Reports", "summary": "Linear regression analysis focuses on predicting a numeric regressand value based on certain regressor values. In this context, k-Nearest Neighbors (k-NN) is a common non-parametric regression algorithm, which achieves efficient performance when compared with other algorithms in literature. In this research effort an optimization of the k-NN algorithm is proposed by exploiting the potentiality of an introduced arithmetic method, which can provide solutions for linear equations involving an arbitrary number of real variables. Specifically, an Arithmetic Method Algorithm (AMA) is adopted to assess the efficiency of the introduced arithmetic method, while an Arithmetic Method Regression (AMR) algorithm is proposed as an optimization of k-NN adopting the potentiality of AMA. Such algorithm is compared with other regression algorithms, according to an introduced optimal inference decision rule, and evaluated on certain real world data sources, which are publicly available. Results are promising since the proposed AMR algorithm has comparable performance with the other algorithms, while in most cases it achieves better performance than the k-NN. The output results indicate that introduced AMR is an optimization of k-NN.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7b97\u672f\u65b9\u6cd5\u7684k-NN\u4f18\u5316\u7b97\u6cd5AMR\uff0c\u901a\u8fc7\u5f15\u5165\u7b97\u672f\u65b9\u6cd5\u89e3\u51b3\u591a\u5143\u7ebf\u6027\u65b9\u7a0b\uff0c\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfk-NN\uff0c\u4e0e\u5176\u4ed6\u56de\u5f52\u7b97\u6cd5\u6027\u80fd\u76f8\u5f53\u3002", "motivation": "k-NN\u662f\u5e38\u7528\u7684\u975e\u53c2\u6570\u56de\u5f52\u7b97\u6cd5\uff0c\u4f46\u5728\u6027\u80fd\u4e0a\u6709\u4f18\u5316\u7a7a\u95f4\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u80fd\u5904\u7406\u4efb\u610f\u6570\u91cf\u5b9e\u53d8\u91cf\u7684\u7ebf\u6027\u65b9\u7a0b\u7684\u7b97\u672f\u65b9\u6cd5\uff0c\u6765\u4f18\u5316k-NN\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u7b97\u672f\u65b9\u6cd5\u7b97\u6cd5AMA\u8bc4\u4f30\u7b97\u672f\u65b9\u6cd5\u6548\u7387\uff0c\u5e76\u57fa\u4e8eAMA\u5f00\u53d1\u7b97\u672f\u65b9\u6cd5\u56de\u5f52\u7b97\u6cd5AMR\u4f5c\u4e3ak-NN\u7684\u4f18\u5316\u7248\u672c\u3002\u4f7f\u7528\u5f15\u5165\u7684\u6700\u4f18\u63a8\u65ad\u51b3\u7b56\u89c4\u5219\u4e0e\u5176\u4ed6\u56de\u5f52\u7b97\u6cd5\u6bd4\u8f83\uff0c\u5e76\u5728\u516c\u5f00\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "AMR\u7b97\u6cd5\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u6bd4k-NN\u8868\u73b0\u66f4\u597d\uff0c\u4e0e\u5176\u4ed6\u56de\u5f52\u7b97\u6cd5\u6027\u80fd\u76f8\u5f53\u3002\u7ed3\u679c\u8868\u660eAMR\u786e\u5b9e\u662f\u5bf9k-NN\u7684\u6709\u6548\u4f18\u5316\u3002", "conclusion": "\u63d0\u51fa\u7684AMR\u7b97\u6cd5\u6210\u529f\u4f18\u5316\u4e86k-NN\u56de\u5f52\u7b97\u6cd5\uff0c\u901a\u8fc7\u7b97\u672f\u65b9\u6cd5\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002"}}
{"id": "2602.08579", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08579", "abs": "https://arxiv.org/abs/2602.08579", "authors": ["Junsu Seo"], "title": "Modeling Score Approximation Errors in Diffusion Models via Forward SPDEs", "comment": null, "summary": "This study investigates the dynamics of Score-based Generative Models (SGMs) by treating the score estimation error as a stochastic source driving the Fokker-Planck equation. Departing from particle-centric SDE analyses, we employ an SPDE framework to model the evolution of the probability density field under stochastic drift perturbations. Under a simplified setting, we utilize this framework to interpret the robustness of generative models through the lens of geometric stability and displacement convexity. Furthermore, we introduce a candidate evaluation metric derived from the quadratic variation of the SPDE solution projected onto a radial test function. Preliminary observations suggest that this metric remains effective using only the initial 10% of the sampling trajectory, indicating a potential for computational efficiency.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5c06\u5206\u6570\u4f30\u8ba1\u8bef\u5dee\u89c6\u4e3a\u9a71\u52a8Fokker-Planck\u65b9\u7a0b\u7684\u968f\u673a\u6e90\uff0c\u4eceSPDE\u6846\u67b6\u5206\u6790\u57fa\u4e8e\u5206\u6570\u7684\u751f\u6210\u6a21\u578b\u52a8\u6001\uff0c\u63d0\u51fa\u51e0\u4f55\u7a33\u5b9a\u6027\u548c\u4f4d\u79fb\u51f8\u6027\u89c6\u89d2\uff0c\u5e76\u5f15\u5165\u57fa\u4e8eSPDE\u89e3\u4e8c\u6b21\u53d8\u5206\u7684\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u7c92\u5b50\u7684SDE\u5206\u6790\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4ece\u6982\u7387\u5bc6\u5ea6\u573a\u6f14\u5316\u7684\u89d2\u5ea6\u7406\u89e3\u57fa\u4e8e\u5206\u6570\u7684\u751f\u6210\u6a21\u578b\u52a8\u6001\uff0c\u7279\u522b\u662f\u5206\u6570\u4f30\u8ba1\u8bef\u5dee\u5bf9\u751f\u6210\u8fc7\u7a0b\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u968f\u673a\u504f\u5fae\u5206\u65b9\u7a0b\u6846\u67b6\uff0c\u5c06\u5206\u6570\u4f30\u8ba1\u8bef\u5dee\u5efa\u6a21\u4e3aFokker-Planck\u65b9\u7a0b\u7684\u968f\u673a\u9a71\u52a8\u6e90\uff0c\u5728\u7b80\u5316\u8bbe\u7f6e\u4e0b\u5206\u6790\u51e0\u4f55\u7a33\u5b9a\u6027\u548c\u4f4d\u79fb\u51f8\u6027\uff0c\u5e76\u5f15\u5165\u57fa\u4e8eSPDE\u89e3\u4e8c\u6b21\u53d8\u5206\u7684\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u521d\u6b65\u89c2\u5bdf\u8868\u660e\uff0c\u63d0\u51fa\u7684\u8bc4\u4f30\u6307\u6807\u4ec5\u9700\u91c7\u6837\u8f68\u8ff9\u524d10%\u7684\u6570\u636e\u5373\u53ef\u4fdd\u6301\u6709\u6548\u6027\uff0c\u663e\u793a\u51fa\u8ba1\u7b97\u6548\u7387\u6f5c\u529b\uff1b\u4ece\u51e0\u4f55\u7a33\u5b9a\u6027\u548c\u4f4d\u79fb\u51f8\u6027\u89d2\u5ea6\u89e3\u91ca\u4e86\u751f\u6210\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "SPDE\u6846\u67b6\u4e3a\u5206\u6790\u57fa\u4e8e\u5206\u6570\u7684\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u63d0\u51fa\u7684\u8bc4\u4f30\u6307\u6807\u5177\u6709\u8ba1\u7b97\u6548\u7387\u4f18\u52bf\uff0c\u51e0\u4f55\u7a33\u5b9a\u6027\u548c\u4f4d\u79fb\u51f8\u6027\u5206\u6790\u6709\u52a9\u4e8e\u7406\u89e3\u751f\u6210\u6a21\u578b\u7684\u9c81\u68d2\u6027\u673a\u5236\u3002"}}
{"id": "2602.08584", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08584", "abs": "https://arxiv.org/abs/2602.08584", "authors": ["Wensong Bai", "Chao Zhang", "Qihang Xu", "Chufan Chen", "Chenhao Zhou", "Hui Qian"], "title": "Conditional Sequence Modeling for Safe Reinforcement Learning", "comment": null, "summary": "Offline safe reinforcement learning (RL) aims to learn policies from a fixed dataset while maximizing performance under cumulative cost constraints. In practice, deployment requirements often vary across scenarios, necessitating a single policy that can adapt zero-shot to different cost thresholds. However, most existing offline safe RL methods are trained under a pre-specified threshold, yielding policies with limited generalization and deployment flexibility across cost thresholds. Motivated by recent progress in conditional sequence modeling (CSM), which enables flexible goal-conditioned control by specifying target returns, we propose RCDT, a CSM-based method that supports zero-shot deployment across multiple cost thresholds within a single trained policy. RCDT is the first CSM-based offline safe RL algorithm that integrates a Lagrangian-style cost penalty with an auto-adaptive penalty coefficient. To avoid overly conservative behavior and achieve a more favorable return--cost trade-off, a reward--cost-aware trajectory reweighting mechanism and Q-value regularization are further incorporated. Extensive experiments on the DSRL benchmark demonstrate that RCDT consistently improves return--cost trade-offs over representative baselines, advancing the state-of-the-art in offline safe RL.", "AI": {"tldr": "RCDT\u662f\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u5e8f\u5217\u5efa\u6a21\u7684\u79bb\u7ebf\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u652f\u6301\u5728\u5355\u4e00\u8bad\u7ec3\u7b56\u7565\u4e0b\u96f6\u6837\u672c\u9002\u5e94\u4e0d\u540c\u6210\u672c\u9608\u503c\uff0c\u901a\u8fc7\u62c9\u683c\u6717\u65e5\u5f0f\u6210\u672c\u60e9\u7f5a\u548c\u81ea\u9002\u5e94\u60e9\u7f5a\u7cfb\u6570\u5b9e\u73b0\u66f4\u597d\u7684\u56de\u62a5-\u6210\u672c\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u79bb\u7ebf\u5b89\u5168RL\u65b9\u6cd5\u901a\u5e38\u5728\u9884\u5b9a\u4e49\u6210\u672c\u9608\u503c\u4e0b\u8bad\u7ec3\uff0c\u5bfc\u81f4\u7b56\u7565\u6cdb\u5316\u80fd\u529b\u6709\u9650\u4e14\u90e8\u7f72\u7075\u6d3b\u6027\u4e0d\u8db3\u3002\u5b9e\u9645\u90e8\u7f72\u4e2d\u4e0d\u540c\u573a\u666f\u9700\u8981\u4e0d\u540c\u7684\u6210\u672c\u9608\u503c\uff0c\u56e0\u6b64\u9700\u8981\u80fd\u591f\u96f6\u6837\u672c\u9002\u5e94\u591a\u79cd\u9608\u503c\u7684\u5355\u4e00\u7b56\u7565\u3002", "method": "\u63d0\u51faRCDT\u65b9\u6cd5\uff1a1) \u57fa\u4e8e\u6761\u4ef6\u5e8f\u5217\u5efa\u6a21\u6846\u67b6\uff1b2) \u96c6\u6210\u62c9\u683c\u6717\u65e5\u5f0f\u6210\u672c\u60e9\u7f5a\u4e0e\u81ea\u9002\u5e94\u60e9\u7f5a\u7cfb\u6570\uff1b3) \u5f15\u5165\u56de\u62a5-\u6210\u672c\u611f\u77e5\u7684\u8f68\u8ff9\u91cd\u52a0\u6743\u673a\u5236\uff1b4) \u52a0\u5165Q\u503c\u6b63\u5219\u5316\u4ee5\u907f\u514d\u8fc7\u5ea6\u4fdd\u5b88\u884c\u4e3a\u3002", "result": "\u5728DSRL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRCDT\u76f8\u6bd4\u4ee3\u8868\u6027\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u56de\u62a5-\u6210\u672c\u6743\u8861\u65b9\u9762\u53d6\u5f97\u4e86\u4e00\u81f4\u7684\u6539\u8fdb\uff0c\u63a8\u52a8\u4e86\u79bb\u7ebf\u5b89\u5168RL\u7684\u6700\u65b0\u6280\u672f\u6c34\u5e73\u3002", "conclusion": "RCDT\u662f\u9996\u4e2a\u57fa\u4e8e\u6761\u4ef6\u5e8f\u5217\u5efa\u6a21\u7684\u79bb\u7ebf\u5b89\u5168RL\u7b97\u6cd5\uff0c\u80fd\u591f\u901a\u8fc7\u5355\u4e00\u8bad\u7ec3\u7b56\u7565\u5b9e\u73b0\u8de8\u591a\u4e2a\u6210\u672c\u9608\u503c\u7684\u96f6\u6837\u672c\u90e8\u7f72\uff0c\u4e3a\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u7075\u6d3b\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08585", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08585", "abs": "https://arxiv.org/abs/2602.08585", "authors": ["Ziyao Tang", "Pengkun Jiao", "Xinhang Chen", "Wei Liu", "Shiyong Li", "Jingjing Chen"], "title": "Predicting Future Utility: Global Combinatorial Optimization for Task-Agnostic KV Cache Eviction", "comment": null, "summary": "Given the quadratic complexity of attention, KV cache eviction is vital to accelerate model inference. Current KV cache eviction methods typically rely on instantaneous heuristic metrics, implicitly assuming that score magnitudes are consistent proxies for importance across all heads. However, this overlooks the heterogeneity in predictive fidelity across attention heads. While certain heads prioritize the instantaneous contribution of tokens, others are dedicated to capturing long-horizon utility. In this paper, we propose that optimal budget allocation should be governed by the marginal utility in preserving long-term semantic information. Based on this insight, we propose LU-KV, a novel framework that optimizes head-level budget allocation through a convex-hull relaxation and a marginal-utility-based greedy solver to achieve near-optimal precision. Furthermore, we implement a data-driven offline profiling protocol to facilitate the practical deployment of LU-KV. Extensive evaluations on LongBench and RULER benchmarks demonstrate that LU-KV achieves an 80% reduction in KV cache size with minimal performance degradation, while simultaneously reducing inference latency and GPU memory footprint.", "AI": {"tldr": "LU-KV\uff1a\u57fa\u4e8e\u8fb9\u9645\u6548\u7528\u7684KV\u7f13\u5b58\u6dd8\u6c70\u6846\u67b6\uff0c\u901a\u8fc7\u51f8\u5305\u677e\u5f1b\u548c\u8d2a\u5a6a\u6c42\u89e3\u5668\u4f18\u5316\u5934\u7ea7\u9884\u7b97\u5206\u914d\uff0c\u5728LongBench\u548cRULER\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b080% KV\u7f13\u5b58\u51cf\u5c11\u4e14\u6027\u80fd\u635f\u5931\u6700\u5c0f\u3002", "motivation": "\u73b0\u6709KV\u7f13\u5b58\u6dd8\u6c70\u65b9\u6cd5\u4f9d\u8d56\u77ac\u65f6\u542f\u53d1\u5f0f\u6307\u6807\uff0c\u5047\u8bbe\u6240\u6709\u6ce8\u610f\u529b\u5934\u7684\u5206\u6570\u5927\u5c0f\u662f\u91cd\u8981\u6027\u7684\u4e00\u81f4\u4ee3\u7406\uff0c\u4f46\u5ffd\u7565\u4e86\u4e0d\u540c\u6ce8\u610f\u529b\u5934\u5728\u9884\u6d4b\u4fdd\u771f\u5ea6\u4e0a\u7684\u5f02\u8d28\u6027\u3002\u67d0\u4e9b\u5934\u5173\u6ce8token\u7684\u77ac\u65f6\u8d21\u732e\uff0c\u800c\u5176\u4ed6\u5934\u5219\u4e13\u6ce8\u4e8e\u6355\u6349\u957f\u671f\u6548\u7528\u3002", "method": "\u63d0\u51faLU-KV\u6846\u67b6\uff1a1) \u57fa\u4e8e\u8fb9\u9645\u6548\u7528\u4f18\u5316\u5934\u7ea7\u9884\u7b97\u5206\u914d\uff1b2) \u4f7f\u7528\u51f8\u5305\u677e\u5f1b\u548c\u57fa\u4e8e\u8fb9\u9645\u6548\u7528\u7684\u8d2a\u5a6a\u6c42\u89e3\u5668\u5b9e\u73b0\u8fd1\u4f3c\u6700\u4f18\u7cbe\u5ea6\uff1b3) \u5b9e\u73b0\u6570\u636e\u9a71\u52a8\u7684\u79bb\u7ebf\u5206\u6790\u534f\u8bae\u4ee5\u652f\u6301\u5b9e\u9645\u90e8\u7f72\u3002", "result": "\u5728LongBench\u548cRULER\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLU-KV\u5b9e\u73b0\u4e8680%\u7684KV\u7f13\u5b58\u5927\u5c0f\u51cf\u5c11\uff0c\u540c\u65f6\u6027\u80fd\u4e0b\u964d\u6700\u5c0f\uff0c\u5e76\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\u548cGPU\u5185\u5b58\u5360\u7528\u3002", "conclusion": "\u901a\u8fc7\u8003\u8651\u6ce8\u610f\u529b\u5934\u7684\u5f02\u8d28\u6027\u548c\u57fa\u4e8e\u8fb9\u9645\u6548\u7528\u7684\u9884\u7b97\u5206\u914d\uff0cLU-KV\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684KV\u7f13\u5b58\u6dd8\u6c70\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u3002"}}
{"id": "2602.08589", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.08589", "abs": "https://arxiv.org/abs/2602.08589", "authors": ["Emmanouil Kariotakis", "Aritra Konar"], "title": "FairRARI: A Plug and Play Framework for Fairness-Aware PageRank", "comment": null, "summary": "PageRank (PR) is a fundamental algorithm in graph machine learning tasks. Owing to the increasing importance of algorithmic fairness, we consider the problem of computing PR vectors subject to various group-fairness criteria based on sensitive attributes of the vertices. At present, principled algorithms for this problem are lacking - some cannot guarantee that a target fairness level is achieved, while others do not feature optimality guarantees. In order to overcome these shortcomings, we put forth a unified in-processing convex optimization framework, termed FairRARI, for tackling different group-fairness criteria in a ``plug and play'' fashion. Leveraging a variational formulation of PR, the framework computes fair PR vectors by solving a strongly convex optimization problem with fairness constraints, thereby ensuring that a target fairness level is achieved. We further introduce three different fairness criteria which can be efficiently tackled using FairRARI to compute fair PR vectors with the same asymptotic time-complexity as the original PR algorithm. Extensive experiments on real-world datasets showcase that FairRARI outperforms existing methods in terms of utility, while achieving the desired fairness levels across multiple vertex groups; thereby highlighting its effectiveness.", "AI": {"tldr": "FairRARI\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u51f8\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u8ba1\u7b97\u6ee1\u8db3\u4e0d\u540c\u7fa4\u4f53\u516c\u5e73\u6027\u6807\u51c6\u7684PageRank\u5411\u91cf\uff0c\u786e\u4fdd\u8fbe\u5230\u76ee\u6807\u516c\u5e73\u6c34\u5e73\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u539f\u59cbPR\u7b97\u6cd5\u76f8\u540c\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "motivation": "\u968f\u7740\u7b97\u6cd5\u516c\u5e73\u6027\u65e5\u76ca\u91cd\u8981\uff0c\u9700\u8981\u8ba1\u7b97\u6ee1\u8db3\u57fa\u4e8e\u9876\u70b9\u654f\u611f\u5c5e\u6027\u7684\u7fa4\u4f53\u516c\u5e73\u6027\u6807\u51c6\u7684PageRank\u5411\u91cf\u3002\u76ee\u524d\u7f3a\u4e4f\u539f\u5219\u6027\u7b97\u6cd5\uff1a\u6709\u4e9b\u65e0\u6cd5\u4fdd\u8bc1\u8fbe\u5230\u76ee\u6807\u516c\u5e73\u6c34\u5e73\uff0c\u6709\u4e9b\u6ca1\u6709\u6700\u4f18\u6027\u4fdd\u8bc1\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684in-processing\u51f8\u4f18\u5316\u6846\u67b6FairRARI\uff0c\u5229\u7528PageRank\u7684\u53d8\u5206\u516c\u5f0f\uff0c\u901a\u8fc7\u6c42\u89e3\u5e26\u6709\u516c\u5e73\u6027\u7ea6\u675f\u7684\u5f3a\u51f8\u4f18\u5316\u95ee\u9898\u6765\u8ba1\u7b97\u516c\u5e73PR\u5411\u91cf\u3002\u6846\u67b6\u652f\u6301\u4e09\u79cd\u4e0d\u540c\u7684\u516c\u5e73\u6027\u6807\u51c6\uff0c\u4ee5\"\u5373\u63d2\u5373\u7528\"\u65b9\u5f0f\u5904\u7406\u3002", "result": "FairRARI\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5b83\u5728\u6548\u7528\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u591a\u4e2a\u9876\u70b9\u7fa4\u4f53\u4e2d\u8fbe\u5230\u671f\u671b\u7684\u516c\u5e73\u6c34\u5e73\u3002\u8be5\u6846\u67b6\u5177\u6709\u4e0e\u539f\u59cbPR\u7b97\u6cd5\u76f8\u540c\u7684\u6e10\u8fd1\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "conclusion": "FairRARI\u662f\u4e00\u4e2a\u6709\u6548\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u591f\u8ba1\u7b97\u6ee1\u8db3\u4e0d\u540c\u7fa4\u4f53\u516c\u5e73\u6027\u6807\u51c6\u7684PageRank\u5411\u91cf\uff0c\u786e\u4fdd\u8fbe\u5230\u76ee\u6807\u516c\u5e73\u6c34\u5e73\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2602.08590", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.08590", "abs": "https://arxiv.org/abs/2602.08590", "authors": ["Yicheng Di", "Wei Yuan", "Tieke He", "Zhanjie Zhang", "Ao Ma", "Yuan Liu", "Hongzhi Yin"], "title": "SDFed: Bridging Local Global Discrepancy via Subspace Refinement and Divergence Control in Federated Prompt Learning", "comment": "13 pages, 6 figures", "summary": "Vision-language pretrained models offer strong transferable representations, yet adapting them in privacy-sensitive multi-party settings is challenging due to the high communication cost of federated optimization and the limited local data on clients. Federated prompt learning mitigates this issue by keeping the VLPM backbone frozen and collaboratively training lightweight prompt parameters. However, existing approaches typically enforce a unified prompt structure and length across clients, which is inadequate under practical client heterogeneity in both data distributions and system resources, and may further introduce conflicts between globally shared and locally optimal knowledge. To address these challenges, we propose \\textbf{SDFed}, a heterogeneous federated prompt learning framework that bridges Local-Global Discrepancy via Subspace Refinement and Divergence Control. SDFed maintains a fixed-length global prompt for efficient aggregation while allowing each client to learn a variable-length local prompt to better match its data characteristics and capacity. To mitigate local-global conflicts and facilitate effective knowledge transfer, SDFed introduces a subspace refinement method for local prompts and an information retention and divergence control strategy that preserves key local information while maintaining appropriate separability between global and local representations. Extensive experiments on several datasets demonstrate that SDFed consistently improves performance and robustness in heterogeneous federated settings.", "AI": {"tldr": "SDFed\u662f\u4e00\u4e2a\u5f02\u6784\u8054\u90a6\u63d0\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5b50\u7a7a\u95f4\u7ec6\u5316\u548c\u5206\u6b67\u63a7\u5236\u89e3\u51b3\u5ba2\u6237\u7aef\u5f02\u8d28\u6027\u4e0b\u7684\u672c\u5730-\u5168\u5c40\u5dee\u5f02\u95ee\u9898", "motivation": "\u73b0\u6709\u7684\u8054\u90a6\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5f3a\u5236\u6240\u6709\u5ba2\u6237\u7aef\u4f7f\u7528\u7edf\u4e00\u7684\u63d0\u793a\u7ed3\u6784\u548c\u957f\u5ea6\uff0c\u8fd9\u5728\u6570\u636e\u5206\u5e03\u548c\u7cfb\u7edf\u8d44\u6e90\u5b58\u5728\u5f02\u8d28\u6027\u7684\u5b9e\u9645\u573a\u666f\u4e2d\u4e0d\u8db3\uff0c\u4e14\u53ef\u80fd\u5f15\u5165\u5168\u5c40\u5171\u4eab\u77e5\u8bc6\u548c\u672c\u5730\u6700\u4f18\u77e5\u8bc6\u4e4b\u95f4\u7684\u51b2\u7a81", "method": "SDFed\u91c7\u7528\u5f02\u6784\u8054\u90a6\u63d0\u793a\u5b66\u4e60\u6846\u67b6\uff1a\u4fdd\u6301\u56fa\u5b9a\u957f\u5ea6\u7684\u5168\u5c40\u63d0\u793a\u7528\u4e8e\u9ad8\u6548\u805a\u5408\uff0c\u540c\u65f6\u5141\u8bb8\u6bcf\u4e2a\u5ba2\u6237\u7aef\u5b66\u4e60\u53ef\u53d8\u957f\u5ea6\u7684\u672c\u5730\u63d0\u793a\u4ee5\u9002\u5e94\u5176\u6570\u636e\u7279\u6027\u548c\u5bb9\u91cf\uff1b\u5f15\u5165\u672c\u5730\u63d0\u793a\u7684\u5b50\u7a7a\u95f4\u7ec6\u5316\u65b9\u6cd5\uff0c\u4ee5\u53ca\u4fe1\u606f\u4fdd\u7559\u548c\u5206\u6b67\u63a7\u5236\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u5173\u952e\u672c\u5730\u4fe1\u606f\u7684\u540c\u65f6\u7ef4\u6301\u5168\u5c40\u4e0e\u672c\u5730\u8868\u793a\u4e4b\u95f4\u7684\u9002\u5f53\u5206\u79bb\u6027", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSDFed\u5728\u5f02\u6784\u8054\u90a6\u8bbe\u7f6e\u4e2d\u6301\u7eed\u63d0\u5347\u6027\u80fd\u548c\u9c81\u68d2\u6027", "conclusion": "SDFed\u901a\u8fc7\u89e3\u51b3\u5ba2\u6237\u7aef\u5f02\u8d28\u6027\u4e0b\u7684\u672c\u5730-\u5168\u5c40\u5dee\u5f02\u95ee\u9898\uff0c\u4e3a\u9690\u79c1\u654f\u611f\u7684\u591a\u65b9\u8bbe\u7f6e\u4e2d\u7684\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u5e94\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.08592", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08592", "abs": "https://arxiv.org/abs/2602.08592", "authors": ["Tianyin Liao", "Chunyu Hu", "Yicheng Sui", "Xingxuan Zhang", "Peng Cui", "Jianxin Li", "Ziwei Zhang"], "title": "TFMLinker: Universal Link Predictor by Graph In-Context Learning with Tabular Foundation Models", "comment": null, "summary": "Link prediction is a fundamental task in graph machine learning with widespread applications such as recommendation systems, drug discovery, knowledge graphs, etc. In the foundation model era, how to develop universal link prediction methods across datasets and domains becomes a key problem, with some initial attempts adopting Graph Foundation Models utilizing Graph Neural Networks and Large Language Models. However, the existing methods face notable limitations, including limited pre-training scale or heavy reliance on textual information. Motivated by the success of tabular foundation models (TFMs) in achieving universal prediction across diverse tabular datasets, we explore an alternative approach by TFMs, which are pre-trained on diverse synthetic datasets sampled from structural causal models and support strong in-context learning independent of textual attributes. Nevertheless, adapting TFMs for link prediction faces severe technical challenges such as how to obtain the necessary context and capture link-centric topological information. To solve these challenges, we propose TFMLinker (Tabular Foundation Model for Link Predictor), aiming to leverage the in-context learning capabilities of TFMs to perform link prediction across diverse graphs without requiring dataset-specific fine-tuning. Specifically, we first develop a prototype-augmented local-global context module to construct context that captures both graph-specific and cross-graph transferable patterns. Next, we design a universal topology-aware link encoder to capture link-centric topological information and generate link representations as inputs for the TFM. Finally, we employ the TFM to predict link existence through in-context learning. Experiments on 6 graph benchmarks across diverse domains demonstrate the superiority of our method over state-of-the-art baselines without requiring dataset-specific finetuning.", "AI": {"tldr": "TFMLinker\uff1a\u5229\u7528\u8868\u683c\u57fa\u7840\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u8fdb\u884c\u8de8\u56fe\u94fe\u63a5\u9884\u6d4b\uff0c\u65e0\u9700\u7279\u5b9a\u6570\u636e\u96c6\u5fae\u8c03", "motivation": "\u73b0\u6709\u56fe\u57fa\u7840\u6a21\u578b\u5728\u94fe\u63a5\u9884\u6d4b\u4e2d\u5b58\u5728\u9884\u8bad\u7ec3\u89c4\u6a21\u6709\u9650\u6216\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u800c\u8868\u683c\u57fa\u7840\u6a21\u578b\u5728\u8de8\u8868\u683c\u6570\u636e\u96c6\u901a\u7528\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u56e0\u6b64\u63a2\u7d22\u5c06\u5176\u5e94\u7528\u4e8e\u94fe\u63a5\u9884\u6d4b", "method": "1\uff09\u539f\u578b\u589e\u5f3a\u7684\u5c40\u90e8-\u5168\u5c40\u4e0a\u4e0b\u6587\u6a21\u5757\u6784\u5efa\u4e0a\u4e0b\u6587\uff1b2\uff09\u901a\u7528\u62d3\u6251\u611f\u77e5\u94fe\u63a5\u7f16\u7801\u5668\u6355\u83b7\u94fe\u63a5\u4e2d\u5fc3\u62d3\u6251\u4fe1\u606f\uff1b3\uff09\u5229\u7528\u8868\u683c\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u9884\u6d4b\u94fe\u63a5\u5b58\u5728", "result": "\u57286\u4e2a\u8de8\u4e0d\u540c\u9886\u57df\u7684\u56fe\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u9700\u6570\u636e\u96c6\u7279\u5b9a\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "TFMLinker\u6210\u529f\u5c06\u8868\u683c\u57fa\u7840\u6a21\u578b\u5e94\u7528\u4e8e\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e86\u8de8\u56fe\u901a\u7528\u9884\u6d4b\uff0c\u4e3a\u94fe\u63a5\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u65b9\u6cd5"}}
{"id": "2602.08616", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08616", "abs": "https://arxiv.org/abs/2602.08616", "authors": ["Heiko Hoppe", "Fabian Akkerman", "Wouter van Heeswijk", "Maximilian Schiffer"], "title": "Breaking the Grid: Distance-Guided Reinforcement Learning in Large Discrete and Hybrid Action Spaces", "comment": "26 pages, 8 figures", "summary": "Reinforcement Learning is increasingly applied to logistics, scheduling, and recommender systems, but standard algorithms struggle with the curse of dimensionality in such large discrete action spaces. Existing algorithms typically rely on restrictive grid-based structures or computationally expensive nearest-neighbor searches, limiting their effectiveness in high-dimensional or irregularly structured domains. We propose Distance-Guided Reinforcement Learning (DGRL), combining Sampled Dynamic Neighborhoods (SDN) and Distance-Based Updates (DBU) to enable efficient RL in spaces with up to 10$^\\text{20}$ actions. Unlike prior methods, SDN leverages a semantic embedding space to perform stochastic volumetric exploration, provably providing full support over a local trust region. Complementing this, DBU transforms policy optimization into a stable regression task, decoupling gradient variance from action space cardinality and guaranteeing monotonic policy improvement. DGRL naturally generalizes to hybrid continuous-discrete action spaces without requiring hierarchical dependencies. We demonstrate performance improvements of up to 66% against state-of-the-art benchmarks across regularly and irregularly structured environments, while simultaneously improving convergence speed and computational complexity.", "AI": {"tldr": "DGRL\u901a\u8fc7\u91c7\u6837\u52a8\u6001\u90bb\u57df\u548c\u8ddd\u79bb\u66f4\u65b0\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u7684\u7ef4\u5ea6\u8bc5\u5492\u95ee\u9898\uff0c\u5728\u9ad8\u8fbe10^20\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u9ad8\u6548\u5f3a\u5316\u5b66\u4e60\uff0c\u6027\u80fd\u63d0\u5347\u8fbe66%\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u7269\u6d41\u3001\u8c03\u5ea6\u548c\u63a8\u8350\u7cfb\u7edf\u7b49\u5e94\u7528\u4e2d\u9762\u4e34\u5927\u89c4\u6a21\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u7684\u7ef4\u5ea6\u8bc5\u5492\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9650\u5236\u6027\u7684\u7f51\u683c\u7ed3\u6784\u6216\u8ba1\u7b97\u6602\u8d35\u7684\u6700\u8fd1\u90bb\u641c\u7d22\uff0c\u5728\u9ad8\u7ef4\u6216\u4e0d\u89c4\u5219\u7ed3\u6784\u9886\u57df\u4e2d\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51fa\u8ddd\u79bb\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60(DGRL)\uff0c\u7ed3\u5408\u91c7\u6837\u52a8\u6001\u90bb\u57df(SDN)\u548c\u8ddd\u79bb\u66f4\u65b0(DBU)\u3002SDN\u5229\u7528\u8bed\u4e49\u5d4c\u5165\u7a7a\u95f4\u8fdb\u884c\u968f\u673a\u4f53\u79ef\u63a2\u7d22\uff0c\u4fdd\u8bc1\u5c40\u90e8\u4fe1\u4efb\u533a\u57df\u7684\u5b8c\u5168\u8986\u76d6\u3002DBU\u5c06\u7b56\u7565\u4f18\u5316\u8f6c\u5316\u4e3a\u7a33\u5b9a\u56de\u5f52\u4efb\u52a1\uff0c\u89e3\u8026\u68af\u5ea6\u65b9\u5dee\u4e0e\u52a8\u4f5c\u7a7a\u95f4\u57fa\u6570\uff0c\u4fdd\u8bc1\u7b56\u7565\u5355\u8c03\u6539\u8fdb\u3002", "result": "\u5728\u89c4\u5219\u548c\u4e0d\u89c4\u5219\u7ed3\u6784\u73af\u5883\u4e2d\uff0cDGRL\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u51c6\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe66%\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6536\u655b\u901f\u5ea6\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u65b9\u6cd5\u81ea\u7136\u63a8\u5e7f\u5230\u6df7\u5408\u8fde\u7eed-\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\uff0c\u65e0\u9700\u5c42\u6b21\u4f9d\u8d56\u3002", "conclusion": "DGRL\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u6311\u6218\uff0c\u901a\u8fc7\u8bed\u4e49\u5d4c\u5165\u63a2\u7d22\u548c\u7a33\u5b9a\u56de\u5f52\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u9ad8\u6548\u5b66\u4e60\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08617", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08617", "abs": "https://arxiv.org/abs/2602.08617", "authors": ["Dario Fenoglio", "Pasquale Polverino", "Jacopo Quizi", "Martin Gjoreski", "Marc Langheinrich"], "title": "ERIS: Enhancing Privacy and Communication Efficiency in Serverless Federated Learning", "comment": null, "summary": "Scaling federated learning (FL) to billion-parameter models introduces critical trade-offs between communication efficiency, model accuracy, and privacy guarantees. Existing solutions often tackle these challenges in isolation, sacrificing accuracy or relying on costly cryptographic tools. We propose ERIS, a serverless FL framework that balances privacy and accuracy while eliminating the server bottleneck and distributing the communication load. ERIS combines a model partitioning strategy, distributing aggregation across multiple client-side aggregators, with a distributed shifted gradient compression mechanism. We theoretically prove that ERIS (i) converges at the same rate as FedAvg under standard assumptions, and (ii) bounds mutual information leakage inversely with the number of aggregators, enabling strong privacy guarantees with no accuracy degradation. Experiments across image and text tasks, including large language models, confirm that ERIS achieves FedAvg-level accuracy while substantially reducing communication cost and improving robustness to membership inference and reconstruction attacks, without relying on heavy cryptography or noise injection.", "AI": {"tldr": "ERIS\u662f\u4e00\u4e2a\u65e0\u670d\u52a1\u5668\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u5206\u533a\u548c\u5206\u5e03\u5f0f\u68af\u5ea6\u538b\u7f29\uff0c\u5728\u4fdd\u6301FedAvg\u7ea7\u522b\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u5e76\u589e\u5f3a\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u5c06\u8054\u90a6\u5b66\u4e60\u6269\u5c55\u5230\u5341\u4ebf\u53c2\u6570\u6a21\u578b\u65f6\uff0c\u901a\u4fe1\u6548\u7387\u3001\u6a21\u578b\u51c6\u786e\u6027\u548c\u9690\u79c1\u4fdd\u8bc1\u4e4b\u95f4\u5b58\u5728\u5173\u952e\u6743\u8861\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u5b64\u7acb\u5730\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u727a\u7272\u51c6\u786e\u6027\u6216\u4f9d\u8d56\u6602\u8d35\u7684\u5bc6\u7801\u5b66\u5de5\u5177\u3002", "method": "ERIS\u7ed3\u5408\u4e86\u6a21\u578b\u5206\u533a\u7b56\u7565\uff08\u5c06\u805a\u5408\u5206\u5e03\u5728\u591a\u4e2a\u5ba2\u6237\u7aef\u805a\u5408\u5668\u4e0a\uff09\u548c\u5206\u5e03\u5f0f\u79fb\u4f4d\u68af\u5ea6\u538b\u7f29\u673a\u5236\uff0c\u6d88\u9664\u4e86\u670d\u52a1\u5668\u74f6\u9888\u5e76\u5206\u5e03\u4e86\u901a\u4fe1\u8d1f\u8f7d\u3002", "result": "\u7406\u8bba\u8bc1\u660eERIS\u5728\u6807\u51c6\u5047\u8bbe\u4e0b\u4ee5\u4e0eFedAvg\u76f8\u540c\u7684\u901f\u7387\u6536\u655b\uff0c\u5e76\u5c06\u4e92\u4fe1\u606f\u6cc4\u6f0f\u9650\u5236\u4e0e\u805a\u5408\u5668\u6570\u91cf\u6210\u53cd\u6bd4\u3002\u5b9e\u9a8c\u8868\u660eERIS\u5728\u56fe\u50cf\u548c\u6587\u672c\u4efb\u52a1\uff08\u5305\u62ec\u5927\u8bed\u8a00\u6a21\u578b\uff09\u4e2d\u8fbe\u5230FedAvg\u7ea7\u522b\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u5e76\u63d0\u9ad8\u5bf9\u6210\u5458\u63a8\u7406\u548c\u91cd\u5efa\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "ERIS\u662f\u4e00\u4e2a\u5e73\u8861\u9690\u79c1\u548c\u51c6\u786e\u6027\u7684\u65e0\u670d\u52a1\u5668\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u65e0\u9700\u4f9d\u8d56\u7e41\u91cd\u7684\u5bc6\u7801\u5b66\u6216\u566a\u58f0\u6ce8\u5165\uff0c\u5c31\u80fd\u63d0\u4f9b\u5f3a\u5927\u7684\u9690\u79c1\u4fdd\u8bc1\u4e14\u4e0d\u964d\u4f4e\u51c6\u786e\u6027\u3002"}}
{"id": "2602.08621", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08621", "abs": "https://arxiv.org/abs/2602.08621", "authors": ["Yukun Jiang", "Hai Huang", "Mingjie Li", "Yage Zhang", "Michael Backes", "Yang Zhang"], "title": "Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs", "comment": null, "summary": "By introducing routers to selectively activate experts in Transformer layers, the mixture-of-experts (MoE) architecture significantly reduces computational costs in large language models (LLMs) while maintaining competitive performance, especially for models with massive parameters. However, prior work has largely focused on utility and efficiency, leaving the safety risks associated with this sparse architecture underexplored. In this work, we show that the safety of MoE LLMs is as sparse as their architecture by discovering unsafe routes: routing configurations that, once activated, convert safe outputs into harmful ones. Specifically, we first introduce the Router Safety importance score (RoSais) to quantify the safety criticality of each layer's router. Manipulation of only the high-RoSais router(s) can flip the default route into an unsafe one. For instance, on JailbreakBench, masking 5 routers in DeepSeek-V2-Lite increases attack success rate (ASR) by over 4$\\times$ to 0.79, highlighting an inherent risk that router manipulation may naturally occur in MoE LLMs. We further propose a Fine-grained token-layer-wise Stochastic Optimization framework to discover more concrete Unsafe Routes (F-SOUR), which explicitly considers the sequentiality and dynamics of input tokens. Across four representative MoE LLM families, F-SOUR achieves an average ASR of 0.90 and 0.98 on JailbreakBench and AdvBench, respectively. Finally, we outline defensive perspectives, including safety-aware route disabling and router training, as promising directions to safeguard MoE LLMs. We hope our work can inform future red-teaming and safeguarding of MoE LLMs. Our code is provided in https://github.com/TrustAIRLab/UnsafeMoE.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0MoE\u67b6\u6784\u7684LLMs\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\uff0c\u901a\u8fc7\u64cd\u7eb5\u8def\u7531\u5668\u53ef\u4ee5\u6fc0\u6d3b\u4e0d\u5b89\u5168\u8def\u5f84\uff0c\u5c06\u5b89\u5168\u8f93\u51fa\u8f6c\u4e3a\u6709\u5bb3\u5185\u5bb9\uff0c\u5e76\u63d0\u51fa\u9632\u5fa1\u65b9\u6cd5\u3002", "motivation": "MoE\u67b6\u6784\u901a\u8fc7\u7a00\u758f\u6fc0\u6d3b\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u4e4b\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6548\u7528\u548c\u6548\u7387\uff0c\u5bf9\u5176\u5b89\u5168\u98ce\u9669\u63a2\u7d22\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u63ed\u793aMoE LLMs\u4e2d\u5b58\u5728\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "1. \u63d0\u51faRouter Safety\u91cd\u8981\u6027\u8bc4\u5206(RoSais)\u91cf\u5316\u8def\u7531\u5668\u5b89\u5168\u5173\u952e\u6027\uff1b2. \u5f00\u53d1\u7ec6\u7c92\u5ea6token-layer-wise\u968f\u673a\u4f18\u5316\u6846\u67b6(F-SOUR)\u53d1\u73b0\u5177\u4f53\u4e0d\u5b89\u5168\u8def\u5f84\uff1b3. \u5728\u56db\u4e2a\u4ee3\u8868\u6027MoE LLM\u5bb6\u65cf\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "1. \u4ec5\u64cd\u7eb5\u9ad8RoSais\u8def\u7531\u5668\u5373\u53ef\u5c06\u9ed8\u8ba4\u8def\u5f84\u8f6c\u4e3a\u4e0d\u5b89\u5168\u8def\u5f84\uff1b2. \u5728DeepSeek-V2-Lite\u4e0a\u5c4f\u853d5\u4e2a\u8def\u7531\u5668\u4f7f\u653b\u51fb\u6210\u529f\u7387\u63d0\u53474\u500d\u81f30.79\uff1b3. F-SOUR\u5728JailbreakBench\u548cAdvBench\u4e0a\u5206\u522b\u8fbe\u52300.90\u548c0.98\u7684\u5e73\u5747\u653b\u51fb\u6210\u529f\u7387\u3002", "conclusion": "MoE LLMs\u7684\u5b89\u5168\u6027\u4e0e\u5176\u67b6\u6784\u4e00\u6837\u7a00\u758f\uff0c\u5b58\u5728\u901a\u8fc7\u8def\u7531\u5668\u64cd\u7eb5\u6fc0\u6d3b\u4e0d\u5b89\u5168\u8def\u5f84\u7684\u98ce\u9669\u3002\u63d0\u51fa\u4e86\u5b89\u5168\u611f\u77e5\u8def\u5f84\u7981\u7528\u548c\u8def\u7531\u5668\u8bad\u7ec3\u7b49\u9632\u5fa1\u65b9\u5411\uff0c\u4e3aMoE LLMs\u7684\u7ea2\u961f\u6d4b\u8bd5\u548c\u5b89\u5168\u4fdd\u969c\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2602.08629", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08629", "abs": "https://arxiv.org/abs/2602.08629", "authors": ["Bo Peng", "Sirui Chen", "Jiaguo Tian", "Yu Qiao", "Chaochao Lu"], "title": "CauScale: Neural Causal Discovery at Scale", "comment": null, "summary": "Causal discovery is essential for advancing data-driven fields such as scientific AI and data analysis, yet existing approaches face significant time- and space-efficiency bottlenecks when scaling to large graphs. To address this challenge, we present CauScale, a neural architecture designed for efficient causal discovery that scales inference to graphs with up to 1000 nodes. CauScale improves time efficiency via a reduction unit that compresses data embeddings and improves space efficiency by adopting tied attention weights to avoid maintaining axis-specific attention maps. To keep high causal discovery accuracy, CauScale adopts a two-stream design: a data stream extracts relational evidence from high-dimensional observations, while a graph stream integrates statistical graph priors and preserves key structural signals. CauScale successfully scales to 500-node graphs during training, where prior work fails due to space limitations. Across testing data with varying graph scales and causal mechanisms, CauScale achieves 99.6% mAP on in-distribution data and 84.4% on out-of-distribution data, while delivering 4-13,000 times inference speedups over prior methods. Our project page is at https://github.com/OpenCausaLab/CauScale.", "AI": {"tldr": "CauScale\u662f\u4e00\u79cd\u7528\u4e8e\u9ad8\u6548\u56e0\u679c\u53d1\u73b0\u7684\u795e\u7ecf\u67b6\u6784\uff0c\u901a\u8fc7\u538b\u7f29\u6570\u636e\u5d4c\u5165\u548c\u5171\u4eab\u6ce8\u610f\u529b\u6743\u91cd\u5b9e\u73b0\u65f6\u95f4\u548c\u7a7a\u95f4\u6548\u7387\u63d0\u5347\uff0c\u53ef\u6269\u5c55\u52301000\u8282\u70b9\u56fe\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u83b7\u5f974-13,000\u500d\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u56fe\u65f6\u9762\u4e34\u65f6\u95f4\u548c\u7a7a\u95f4\u6548\u7387\u74f6\u9888\uff0c\u9650\u5236\u4e86\u5728\u79d1\u5b66AI\u548c\u6570\u636e\u5206\u6790\u7b49\u6570\u636e\u9a71\u52a8\u9886\u57df\u7684\u5e94\u7528\u6269\u5c55\u3002", "method": "CauScale\u91c7\u7528\u53cc\u6d41\u8bbe\u8ba1\uff1a\u6570\u636e\u6d41\u4ece\u9ad8\u7ef4\u89c2\u6d4b\u4e2d\u63d0\u53d6\u5173\u7cfb\u8bc1\u636e\uff0c\u56fe\u6d41\u6574\u5408\u7edf\u8ba1\u56fe\u5148\u9a8c\u5e76\u4fdd\u7559\u5173\u952e\u7ed3\u6784\u4fe1\u53f7\u3002\u901a\u8fc7\u51cf\u5c11\u5355\u5143\u538b\u7f29\u6570\u636e\u5d4c\u5165\u63d0\u9ad8\u65f6\u95f4\u6548\u7387\uff0c\u91c7\u7528\u5171\u4eab\u6ce8\u610f\u529b\u6743\u91cd\u907f\u514d\u7ef4\u62a4\u8f74\u7279\u5b9a\u6ce8\u610f\u529b\u56fe\u6765\u63d0\u9ad8\u7a7a\u95f4\u6548\u7387\u3002", "result": "CauScale\u6210\u529f\u6269\u5c55\u5230500\u8282\u70b9\u56fe\u7684\u8bad\u7ec3\uff08\u5148\u524d\u5de5\u4f5c\u56e0\u7a7a\u95f4\u9650\u5236\u65e0\u6cd5\u5b9e\u73b0\uff09\uff0c\u5728\u5206\u5e03\u5185\u6570\u636e\u4e0a\u8fbe\u523099.6% mAP\uff0c\u5206\u5e03\u5916\u6570\u636e\u4e0a\u8fbe\u523084.4% mAP\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u5148\u524d\u65b9\u6cd5\u5feb4-13,000\u500d\u3002", "conclusion": "CauScale\u901a\u8fc7\u521b\u65b0\u7684\u795e\u7ecf\u67b6\u6784\u8bbe\u8ba1\u89e3\u51b3\u4e86\u56e0\u679c\u53d1\u73b0\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u74f6\u9888\uff0c\u4e3a\u5927\u89c4\u6a21\u56fe\u7684\u56e0\u679c\u53d1\u73b0\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u548c\u7a7a\u95f4\u6548\u7387\u3002"}}
{"id": "2602.08638", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08638", "abs": "https://arxiv.org/abs/2602.08638", "authors": ["Dezheng Wang", "Tong Chen", "Guansong Pang", "Congyan Chen", "Shihua Li", "Hongzhi Yin"], "title": "LEFT: Learnable Fusion of Tri-view Tokens for Unsupervised Time Series Anomaly Detection", "comment": null, "summary": "As a fundamental data mining task, unsupervised time series anomaly detection (TSAD) aims to build a model for identifying abnormal timestamps without assuming the availability of annotations. A key challenge in unsupervised TSAD is that many anomalies are too subtle to exhibit detectable deviation in any single view (e.g., time domain), and instead manifest as inconsistencies across multiple views like time, frequency, and a mixture of resolutions. However, most cross-view methods rely on feature or score fusion and do not enforce analysis-synthesis consistency, meaning the frequency branch is not required to reconstruct the time signal through an inverse transform, and vice versa. In this paper, we present Learnable Fusion of Tri-view Tokens (LEFT), a unified unsupervised TSAD framework that models anomalies as inconsistencies across complementary representations. LEFT learns feature tokens from three views of the same input time series: frequency-domain tokens that embed periodicity information, time-domain tokens that capture local dynamics, and multi-scale tokens that learns abnormal patterns at varying time series granularities. By learning a set of adaptive Nyquist-constrained spectral filters, the original time series is rescaled into multiple resolutions and then encoded, allowing these multi-scale tokens to complement the extracted frequency- and time-domain information. When generating the fused representation, we introduce a novel objective that reconstructs fine-grained targets from coarser multi-scale structure, and put forward an innovative time-frequency cycle consistency constraint to explicitly regularize cross-view agreement. Experiments on real-world benchmarks show that LEFT yields the best detection accuracy against SOTA baselines, while achieving a 5x reduction on FLOPs and 8x speed-up for training.", "AI": {"tldr": "LEFT\uff1a\u4e00\u79cd\u901a\u8fc7\u4e09\u89c6\u56fe\u4ee4\u724c\u53ef\u5b66\u4e60\u878d\u5408\u7684\u65e0\u76d1\u7763\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u5c06\u5f02\u5e38\u5efa\u6a21\u4e3a\u4e92\u8865\u8868\u793a\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u6027", "motivation": "\u65e0\u76d1\u7763\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7684\u5173\u952e\u6311\u6218\u5728\u4e8e\u8bb8\u591a\u5f02\u5e38\u8fc7\u4e8e\u5fae\u5999\uff0c\u65e0\u6cd5\u5728\u5355\u4e00\u89c6\u56fe\u4e2d\u68c0\u6d4b\u5230\uff0c\u800c\u662f\u8868\u73b0\u4e3a\u8de8\u591a\u4e2a\u89c6\u56fe\uff08\u5982\u65f6\u95f4\u3001\u9891\u7387\u548c\u591a\u5206\u8fa8\u7387\uff09\u7684\u4e0d\u4e00\u81f4\u6027\u3002\u73b0\u6709\u8de8\u89c6\u56fe\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u7279\u5f81\u6216\u5206\u6570\u878d\u5408\uff0c\u7f3a\u4e4f\u5206\u6790-\u5408\u6210\u4e00\u81f4\u6027\u7ea6\u675f\u3002", "method": "LEFT\u4ece\u4e09\u4e2a\u89c6\u56fe\u5b66\u4e60\u7279\u5f81\u4ee4\u724c\uff1a\u9891\u7387\u57df\u4ee4\u724c\uff08\u5d4c\u5165\u5468\u671f\u6027\u4fe1\u606f\uff09\u3001\u65f6\u95f4\u57df\u4ee4\u724c\uff08\u6355\u6349\u5c40\u90e8\u52a8\u6001\uff09\u548c\u591a\u5c3a\u5ea6\u4ee4\u724c\uff08\u5b66\u4e60\u4e0d\u540c\u7c92\u5ea6\u7684\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u6a21\u5f0f\uff09\u3002\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5948\u594e\u65af\u7279\u7ea6\u675f\u8c31\u6ee4\u6ce2\u5668\u5c06\u539f\u59cb\u65f6\u95f4\u5e8f\u5217\u91cd\u7f29\u653e\u4e3a\u591a\u4e2a\u5206\u8fa8\u7387\uff0c\u5e76\u5f15\u5165\u7ec6\u7c92\u5ea6\u76ee\u6807\u91cd\u5efa\u76ee\u6807\u548c\u521b\u65b0\u7684\u65f6\u9891\u5faa\u73af\u4e00\u81f4\u6027\u7ea6\u675f\u6765\u6b63\u5219\u5316\u8de8\u89c6\u56fe\u4e00\u81f4\u6027\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLEFT\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u4e86\u6700\u4f73\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u540c\u65f6\u5b9e\u73b0\u4e865\u500d\u7684FLOPs\u51cf\u5c11\u548c8\u500d\u7684\u8bad\u7ec3\u52a0\u901f\u3002", "conclusion": "LEFT\u901a\u8fc7\u4e09\u89c6\u56fe\u4ee4\u724c\u878d\u5408\u548c\u65f6\u9891\u5faa\u73af\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u76d1\u7763\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8de8\u89c6\u56fe\u4e0d\u4e00\u81f4\u6027\u5efa\u6a21\u7684\u6311\u6218\uff0c\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.08646", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08646", "abs": "https://arxiv.org/abs/2602.08646", "authors": ["Jisung Hwang", "Minhyuk Sung"], "title": "Projected Gradient Ascent for Efficient Reward-Guided Updates with One-Step Generative Models", "comment": null, "summary": "We propose a constrained latent optimization method for reward-guided generation that preserves white Gaussian noise characteristics with negligible overhead. Test-time latent optimization can unlock substantially better reward-guided generations from pretrained generative models, but it is prone to reward hacking that degrades quality and also too slow for practical use. In this work, we make test-time optimization both efficient and reliable by replacing soft regularization with hard white Gaussian noise constraints enforced via projected gradient ascent. Our method applies a closed-form projection after each update to keep the latent vector explicitly noise-like throughout optimization, preventing the drift that leads to unrealistic artifacts. This enforcement adds minimal cost: the projection matches the $O(N \\log N)$ complexity of standard algorithms such as sorting or FFT and does not practically increase wall-clock time. In experiments, our approach reaches a comparable Aesthetic Score using only 30% of the wall-clock time required by the SOTA regularization-based method, while preventing reward hacking.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5e26\u7ea6\u675f\u7684\u6f5c\u5728\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u786c\u6027\u767d\u566a\u58f0\u7ea6\u675f\u9632\u6b62\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4f18\u5316\u6548\u7387", "motivation": "\u73b0\u6709\u7684\u6d4b\u8bd5\u65f6\u6f5c\u5728\u4f18\u5316\u65b9\u6cd5\u867d\u7136\u80fd\u63d0\u5347\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u7684\u5956\u52b1\u5f15\u5bfc\u751f\u6210\u6548\u679c\uff0c\u4f46\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u5bb9\u6613\u53d1\u751f\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\uff0c\u5bfc\u81f4\u751f\u6210\u8d28\u91cf\u4e0b\u964d\uff1b2) \u4f18\u5316\u901f\u5ea6\u8fc7\u6162\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528", "method": "\u91c7\u7528\u786c\u6027\u767d\u9ad8\u65af\u566a\u58f0\u7ea6\u675f\u66ff\u4ee3\u8f6f\u6b63\u5219\u5316\uff0c\u901a\u8fc7\u6295\u5f71\u68af\u5ea6\u4e0a\u5347\u6cd5\u5728\u6bcf\u6b21\u66f4\u65b0\u540e\u5e94\u7528\u95ed\u5f0f\u6295\u5f71\uff0c\u4fdd\u6301\u6f5c\u5728\u5411\u91cf\u5728\u6574\u4e2a\u4f18\u5316\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u566a\u58f0\u7279\u6027\uff0c\u9632\u6b62\u5bfc\u81f4\u4e0d\u771f\u5b9e\u4f2a\u5f71\u7684\u6f02\u79fb", "result": "\u8be5\u65b9\u6cd5\u4ec5\u9700SOTA\u6b63\u5219\u5316\u65b9\u6cd530%\u7684\u5899\u949f\u65f6\u95f4\u5c31\u80fd\u8fbe\u5230\u76f8\u5f53\u7684\u7f8e\u5b66\u8bc4\u5206\uff0c\u540c\u65f6\u6709\u6548\u9632\u6b62\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u3002\u6295\u5f71\u64cd\u4f5c\u590d\u6742\u5ea6\u4e3aO(N log N)\uff0c\u4e0e\u6807\u51c6\u7b97\u6cd5\uff08\u5982\u6392\u5e8f\u6216FFT\uff09\u76f8\u5f53\uff0c\u5b9e\u9645\u8fd0\u884c\u65f6\u95f4\u51e0\u4e4e\u4e0d\u589e\u52a0", "conclusion": "\u901a\u8fc7\u786c\u6027\u767d\u566a\u58f0\u7ea6\u675f\u7684\u6f5c\u5728\u4f18\u5316\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u5956\u52b1\u5f15\u5bfc\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u901f\u5ea6\u548c\u53ef\u9760\u6027\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2602.08655", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08655", "abs": "https://arxiv.org/abs/2602.08655", "authors": ["Sarthak Wanjari"], "title": "From Robotics to Sepsis Treatment: Offline RL via Geometric Pessimism", "comment": "10 pages, 8 figures", "summary": "Offline Reinforcement Learning (RL) promises the recovery of optimal policies from static datasets, yet it remains susceptible to the overestimation of out-of-distribution (OOD) actions, particularly in fractured and sparse data manifolds.Current solutions necessitates a trade off between computational efficiency and performance. Methods like CQL offers rigorous conservatism but require tremendous compute power while efficient expectile-based methods like IQL often fail to correct OOD errors on pathological datasets, collapsing to Behavioural Cloning. In this work, we propose Geometric Pessimism, a modular, compute-efficient framework that augments standard IQL with density-based penalty derived from k-nearest-neighbour distances in the state-action embedding space. By pre-computing the penalties applied to each state-action pair our method injects OOD conservatism via reward shaping with a O(1) training overhead. Evaluated on the D4Rl MuJoCo benchmark, our method, Geo-IQL outperforms standard IQL on sensitive and unstable medium-replay tasks by over 18 points, while reducing inter-seed variance by 4x. Furthermore, Geo-IQL does not degrade performance on stable manifolds. Crucially, we validate our algorithm on the MIMIC-III Sepsis critical care dataset. While standard IQL collapses to behaviour cloning, Geo-IQL demonstrates active policy improvement. Maintaining safety constraints, achieving 86.4% terminal agreement with clinicians compared to IQL's 75%. Our results suggest that geometric pessimism provides the necessary regularisation to safely overcome local optima in critical, real-world decision systems.", "AI": {"tldr": "Geo-IQL\uff1a\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7k\u8fd1\u90bb\u5bc6\u5ea6\u60e9\u7f5a\u589e\u5f3aIQL\uff0c\u6709\u6548\u89e3\u51b3OOD\u52a8\u4f5c\u9ad8\u4f30\u95ee\u9898\uff0c\u5728\u7a00\u758f\u6570\u636e\u548c\u771f\u5b9e\u533b\u7597\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02", "motivation": "\u79bb\u7ebfRL\u5728\u9759\u6001\u6570\u636e\u96c6\u4e0a\u6062\u590d\u6700\u4f18\u7b56\u7565\u65f6\uff0c\u5bb9\u6613\u9ad8\u4f30\u5206\u5e03\u5916(OOD)\u52a8\u4f5c\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u758f\u548c\u65ad\u88c2\u7684\u60c5\u51b5\u4e0b\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e4b\u95f4\u6743\u8861\uff1aCQL\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800cIQL\u5728\u75c5\u6001\u6570\u636e\u96c6\u4e0a\u5bb9\u6613\u9000\u5316\u4e3a\u884c\u4e3a\u514b\u9686", "method": "\u63d0\u51fa\u51e0\u4f55\u60b2\u89c2\u4e3b\u4e49\u6846\u67b6\uff0c\u5728\u6807\u51c6IQL\u57fa\u7840\u4e0a\u589e\u52a0\u57fa\u4e8ek\u8fd1\u90bb\u8ddd\u79bb\u7684\u5bc6\u5ea6\u60e9\u7f5a\u3002\u5728\u72b6\u6001-\u52a8\u4f5c\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u60e9\u7f5a\u9879\uff0c\u901a\u8fc7\u5956\u52b1\u5851\u5f62\u6ce8\u5165OOD\u4fdd\u5b88\u6027\uff0c\u9884\u8ba1\u7b97\u60e9\u7f5a\u9879\u4f7f\u8bad\u7ec3\u5f00\u9500\u4ec5\u4e3aO(1)", "result": "\u5728D4RL MuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGeo-IQL\u5728\u654f\u611f\u4e0d\u7a33\u5b9a\u7684medium-replay\u4efb\u52a1\u4e0a\u6bd4\u6807\u51c6IQL\u63d0\u534718+\u5206\uff0c\u79cd\u5b50\u95f4\u65b9\u5dee\u964d\u4f4e4\u500d\u3002\u5728MIMIC-III Sepsis\u6570\u636e\u96c6\u4e0a\uff0c\u6807\u51c6IQL\u9000\u5316\u4e3a\u884c\u4e3a\u514b\u9686\uff0c\u800cGeo-IQL\u5b9e\u73b0\u4e3b\u52a8\u7b56\u7565\u6539\u8fdb\uff0c\u4e0e\u4e34\u5e8a\u533b\u751f\u7ec8\u672b\u51b3\u7b56\u4e00\u81f4\u6027\u8fbe86.4%\uff08IQL\u4e3a75%\uff09", "conclusion": "\u51e0\u4f55\u60b2\u89c2\u4e3b\u4e49\u4e3a\u5173\u952e\u771f\u5b9e\u4e16\u754c\u51b3\u7b56\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u6b63\u5219\u5316\uff0c\u80fd\u591f\u5b89\u5168\u514b\u670d\u5c40\u90e8\u6700\u4f18\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7a00\u758f\u6570\u636e\u548c\u533b\u7597\u7b49\u5173\u952e\u5e94\u7528\u573a\u666f"}}
{"id": "2602.08657", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.08657", "abs": "https://arxiv.org/abs/2602.08657", "authors": ["Xiaotong Liu", "Shao-Bo Lin", "Jun Fan", "Ding-Xuan Zhou"], "title": "Two-Stage Data Synthesization: A Statistics-Driven Restricted Trade-off between Privacy and Prediction", "comment": null, "summary": "Synthetic data have gained increasing attention across various domains, with a growing emphasis on their performance in downstream prediction tasks. However, most existing synthesis strategies focus on maintaining statistical information. Although some studies address prediction performance guarantees, their single-stage synthesis designs make it challenging to balance the privacy requirements that necessitate significant perturbations and the prediction performance that is sensitive to such perturbations. We propose a two-stage synthesis strategy. In the first stage, we introduce a synthesis-then-hybrid strategy, which involves a synthesis operation to generate pure synthetic data, followed by a hybrid operation that fuses the synthetic data with the original data. In the second stage, we present a kernel ridge regression (KRR)-based synthesis strategy, where a KRR model is first trained on the original data and then used to generate synthetic outputs based on the synthetic inputs produced in the first stage. By leveraging the theoretical strengths of KRR and the covariant distribution retention achieved in the first stage, our proposed two-stage synthesis strategy enables a statistics-driven restricted privacy--prediction trade-off and guarantee optimal prediction performance. We validate our approach and demonstrate its characteristics of being statistics-driven and restricted in achieving the privacy--prediction trade-off both theoretically and numerically. Additionally, we showcase its generalizability through applications to a marketing problem and five real-world datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u5408\u6210\u6570\u636e\u7b56\u7565\uff0c\u901a\u8fc7\u5408\u6210-\u6df7\u5408\u548c\u6838\u5cad\u56de\u5f52\u65b9\u6cd5\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4f18\u5316\u4e0b\u6e38\u9884\u6d4b\u6027\u80fd", "motivation": "\u73b0\u6709\u5408\u6210\u6570\u636e\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7edf\u8ba1\u4fe1\u606f\u4fdd\u6301\uff0c\u96be\u4ee5\u5e73\u8861\u9690\u79c1\u4fdd\u62a4\u6240\u9700\u7684\u663e\u8457\u6270\u52a8\u4e0e\u9884\u6d4b\u6027\u80fd\u5bf9\u6270\u52a8\u7684\u654f\u611f\u6027\u4e4b\u95f4\u7684\u77db\u76fe", "method": "\u4e24\u9636\u6bb5\u5408\u6210\u7b56\u7565\uff1a\u7b2c\u4e00\u9636\u6bb5\u91c7\u7528\u5408\u6210-\u6df7\u5408\u65b9\u6cd5\u751f\u6210\u6df7\u5408\u6570\u636e\uff1b\u7b2c\u4e8c\u9636\u6bb5\u57fa\u4e8e\u6838\u5cad\u56de\u5f52\u6a21\u578b\uff0c\u5728\u7b2c\u4e00\u9636\u6bb5\u5408\u6210\u8f93\u5165\u7684\u57fa\u7840\u4e0a\u751f\u6210\u5408\u6210\u8f93\u51fa", "result": "\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u7edf\u8ba1\u9a71\u52a8\u7684\u53d7\u9650\u9690\u79c1-\u9884\u6d4b\u6743\u8861\uff0c\u5e76\u5728\u4e94\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u548c\u8425\u9500\u95ee\u9898\u4e0a\u5c55\u793a\u4e86\u6cdb\u5316\u80fd\u529b", "conclusion": "\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u5408\u6210\u7b56\u7565\u901a\u8fc7\u6838\u5cad\u56de\u5f52\u7684\u7406\u8bba\u4f18\u52bf\u548c\u534f\u53d8\u5206\u5e03\u4fdd\u6301\uff0c\u5b9e\u73b0\u4e86\u6700\u4f18\u9884\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u6ee1\u8db3\u9690\u79c1\u4fdd\u62a4\u8981\u6c42"}}
{"id": "2602.08660", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08660", "abs": "https://arxiv.org/abs/2602.08660", "authors": ["Alexandre Verine", "Rafael Pinot", "Florian Le Bronnec"], "title": "Equalized Generative Treatment: Matching f-divergences for Fairness in Generative Models", "comment": null, "summary": "Fairness is a crucial concern for generative models, which not only reflect but can also amplify societal and cultural biases. Existing fairness notions for generative models are largely adapted from classification and focus on balancing the probability of generating samples from each sensitive group. We show that such criteria are brittle, as they can be met even when different sensitive groups are modeled with widely varying quality. To address this limitation, we introduce a new fairness definition for generative models, termed as equalized generative treatment (EGT), which requires comparable generation quality across all sensitive groups, with quality measured via a reference f-divergence. We further analyze the trade-offs induced by EGT, demonstrating that enforcing fairness constraints necessarily couples the overall model quality to that of the most challenging group to approximate. This indicates that a simple yet efficient min-max fine-tuning method should be able to balance f-divergences across sensitive groups to satisfy EGT. We validate this theoretical insight through a set of experiments on both image and text generation tasks. We demonstrate that min-max methods consistently achieve fairer outcomes compared to other approaches from the literature, while maintaining competitive overall performance for both tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u6a21\u578b\u516c\u5e73\u6027\u5b9a\u4e49EGT\uff0c\u5f3a\u8c03\u4e0d\u540c\u654f\u611f\u7fa4\u4f53\u95f4\u7684\u751f\u6210\u8d28\u91cf\u53ef\u6bd4\u6027\uff0c\u800c\u975e\u4ec5\u6982\u7387\u5e73\u8861\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\u516c\u5e73\u7ea6\u675f\u4f1a\u8026\u5408\u6a21\u578b\u6574\u4f53\u8d28\u91cf\u4e0e\u6700\u96be\u8fd1\u4f3c\u7fa4\u4f53\u7684\u8d28\u91cf\uff0c\u56e0\u6b64\u63d0\u51famin-max\u5fae\u8c03\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u56fe\u50cf\u548c\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u516c\u5e73\u6027\u6982\u5ff5\u4e3b\u8981\u4ece\u5206\u7c7b\u4efb\u52a1\u79fb\u690d\u800c\u6765\uff0c\u5173\u6ce8\u4e0d\u540c\u654f\u611f\u7fa4\u4f53\u751f\u6210\u6837\u672c\u7684\u6982\u7387\u5e73\u8861\u3002\u4f46\u8fd9\u79cd\u6807\u51c6\u5b58\u5728\u8106\u5f31\u6027\uff0c\u56e0\u4e3a\u5373\u4f7f\u4e0d\u540c\u654f\u611f\u7fa4\u4f53\u7684\u5efa\u6a21\u8d28\u91cf\u5dee\u5f02\u5f88\u5927\uff0c\u8fd9\u4e9b\u6807\u51c6\u4e5f\u80fd\u88ab\u6ee1\u8db3\u3002\u9700\u8981\u66f4\u5168\u9762\u7684\u516c\u5e73\u6027\u5b9a\u4e49\u6765\u786e\u4fdd\u751f\u6210\u8d28\u91cf\u5728\u4e0d\u540c\u7fa4\u4f53\u95f4\u7684\u53ef\u6bd4\u6027\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u516c\u5e73\u6027\u5b9a\u4e49\"\u5e73\u7b49\u5316\u751f\u6210\u5904\u7406(EGT)\"\uff0c\u8981\u6c42\u6240\u6709\u654f\u611f\u7fa4\u4f53\u7684\u751f\u6210\u8d28\u91cf\u5177\u6709\u53ef\u6bd4\u6027\uff0c\u8d28\u91cf\u901a\u8fc7\u53c2\u8003f-\u6563\u5ea6\u8861\u91cf\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\u516c\u5e73\u7ea6\u675f\u4f1a\u5c06\u6574\u4f53\u6a21\u578b\u8d28\u91cf\u4e0e\u6700\u96be\u8fd1\u4f3c\u7684\u7fa4\u4f53\u8d28\u91cf\u8026\u5408\uff0c\u56e0\u6b64\u63d0\u51fa\u7b80\u5355\u9ad8\u6548\u7684min-max\u5fae\u8c03\u65b9\u6cd5\u6765\u5e73\u8861\u4e0d\u540c\u654f\u611f\u7fa4\u4f53\u95f4\u7684f-\u6563\u5ea6\u3002", "result": "\u5728\u56fe\u50cf\u548c\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cmin-max\u65b9\u6cd5\u76f8\u6bd4\u6587\u732e\u4e2d\u7684\u5176\u4ed6\u65b9\u6cd5\u80fd\u591f\u6301\u7eed\u5b9e\u73b0\u66f4\u516c\u5e73\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u5728\u4e24\u4e2a\u4efb\u52a1\u4e0a\u90fd\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u6574\u4f53\u6027\u80fd\u3002\u7406\u8bba\u5206\u6790\u5f97\u5230\u5b9e\u9a8c\u652f\u6301\uff0c\u8bc1\u660e\u4e86EGT\u5b9a\u4e49\u7684\u6709\u6548\u6027\u548cmin-max\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "EGT\u4e3a\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u516c\u5e73\u6027\u6846\u67b6\uff0c\u8d85\u8d8a\u4e86\u4ec5\u5173\u6ce8\u6982\u7387\u5e73\u8861\u7684\u73b0\u6709\u65b9\u6cd5\u3002min-max\u5fae\u8c03\u662f\u5b9e\u73b0EGT\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u6574\u4f53\u6027\u80fd\u7684\u540c\u65f6\u786e\u4fdd\u4e0d\u540c\u654f\u611f\u7fa4\u4f53\u95f4\u7684\u751f\u6210\u8d28\u91cf\u516c\u5e73\u6027\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u751f\u6210\u6a21\u578b\u7684\u516c\u5e73\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c6\u89d2\u548c\u5b9e\u8df5\u65b9\u6cd5\u3002"}}
{"id": "2602.08676", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08676", "abs": "https://arxiv.org/abs/2602.08676", "authors": ["Tiwei Bie", "Maosong Cao", "Xiang Cao", "Bingsen Chen", "Fuyuan Chen", "Kun Chen", "Lun Du", "Daozhuo Feng", "Haibo Feng", "Mingliang Gong", "Zhuocheng Gong", "Yanmei Gu", "Jian Guan", "Kaiyuan Guan", "Hongliang He", "Zenan Huang", "Juyong Jiang", "Zhonghui Jiang", "Zhenzhong Lan", "Chengxi Li", "Jianguo Li", "Zehuan Li", "Huabin Liu", "Lin Liu", "Guoshan Lu", "Yuan Lu", "Yuxin Ma", "Xingyu Mou", "Zhenxuan Pan", "Kaida Qiu", "Yuji Ren", "Jianfeng Tan", "Yiding Tian", "Zian Wang", "Lanning Wei", "Tao Wu", "Yipeng Xing", "Wentao Ye", "Liangyu Zha", "Tianze Zhang", "Xiaolu Zhang", "Junbo Zhao", "Da Zheng", "Hao Zhong", "Wanli Zhong", "Jun Zhou", "Junlin Zhou", "Liwang Zhu", "Muzhi Zhu", "Yihong Zhuang"], "title": "LLaDA2.1: Speeding Up Text Diffusion via Token Editing", "comment": "11 pages, 3 figures", "summary": "While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.", "AI": {"tldr": "LLaDA2.1\u901a\u8fc7\u7ed3\u5408Token-to-Token\u7f16\u8f91\u4e0eMask-to-Token\u65b9\u6848\uff0c\u5f15\u5165\u53ef\u914d\u7f6e\u9608\u503c\u89e3\u7801\uff0c\u63d0\u4f9b\u901f\u5ea6\u6a21\u5f0f\u548c\u8d28\u91cf\u6a21\u5f0f\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u9488\u5bf9dLLMs\u7684\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u6781\u5feb\u89e3\u7801\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b3LLaDA2.0\u4e2d\u89e3\u7801\u901f\u5ea6\u4e0e\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u8d85\u8d8a\u4f20\u7edf\u6269\u6563\u6a21\u578b\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u66f4\u7cbe\u786e\u7684\u6587\u672c\u751f\u6210\u3002", "method": "1. \u5c06Token-to-Token\u7f16\u8f91\u65e0\u7f1d\u96c6\u6210\u5230\u4f20\u7edfMask-to-Token\u65b9\u6848\u4e2d\uff0c\u5f15\u5165\u8054\u5408\u53ef\u914d\u7f6e\u9608\u503c\u89e3\u7801\u65b9\u6848\uff1b2. \u63d0\u4f9b\u4e24\u79cd\u6a21\u5f0f\uff1a\u901f\u5ea6\u6a21\u5f0f\uff08\u964d\u4f4eM2T\u9608\u503c\uff0c\u4f9d\u8d56T2T\u7ec6\u5316\u8f93\u51fa\uff09\u548c\u8d28\u91cf\u6a21\u5f0f\uff08\u4fdd\u5b88\u9608\u503c\u4fdd\u8bc1\u6027\u80fd\uff09\uff1b3. \u9996\u6b21\u5b9e\u73b0\u9488\u5bf9dLLMs\u7684\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u7528\u4e13\u95e8\u6280\u672f\u8fdb\u884c\u7a33\u5b9a\u68af\u5ea6\u4f30\u8ba1\u3002", "result": "\u572833\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u7801\u901f\u5ea6\u6781\u5feb\uff1a100B\u6a21\u578b\u5728HumanEval+\u4e0a\u8fbe\u5230892 TPS\uff0cBigCodeBench\u4e0a801 TPS\uff0cLiveCodeBench\u4e0a663 TPS\u3002\u540c\u65f6\u53d1\u5e03LLaDA2.1-Mini\uff0816B\uff09\u548cLLaDA2.1-Flash\uff08100B\uff09\u4e24\u4e2a\u7248\u672c\u3002", "conclusion": "LLaDA2.1\u6210\u529f\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u89e3\u7801\u901f\u5ea6\u4e0e\u8d28\u91cf\u7684\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u89e3\u7801\u65b9\u6848\u548c\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u8f93\u51fa\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u89e3\u7801\u901f\u5ea6\uff0c\u4e3a\u5927\u89c4\u6a21\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2602.08679", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08679", "abs": "https://arxiv.org/abs/2602.08679", "authors": ["Yanzhang Fu", "Zizheng Guo", "Jizhou Luo"], "title": "Dashed Line Defense: Plug-And-Play Defense Against Adaptive Score-Based Query Attacks", "comment": null, "summary": "Score-based query attacks pose a serious threat to deep learning models by crafting adversarial examples (AEs) using only black-box access to model output scores, iteratively optimizing inputs based on observed loss values. While recent runtime defenses attempt to disrupt this process via output perturbation, most either require access to model parameters or fail when attackers adapt their tactics. In this paper, we first reveal that even the state-of-the-art plug-and-play defense can be bypassed by adaptive attacks, exposing a critical limitation of existing runtime defenses. We then propose Dashed Line Defense (DLD), a plug-and-play post-processing method specifically designed to withstand adaptive query strategies. By introducing ambiguity in how the observed loss reflects the true adversarial strength of candidate examples, DLD prevents attackers from reliably analyzing and adapting their queries, effectively disrupting the AE generation process. We provide theoretical guarantees of DLD's defense capability and validate its effectiveness through experiments on ImageNet, demonstrating that DLD consistently outperforms prior defenses--even under worst-case adaptive attacks--while preserving the model's predicted labels.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDashed Line Defense (DLD)\uff0c\u4e00\u79cd\u9488\u5bf9\u9ed1\u76d2\u5206\u6570\u67e5\u8be2\u653b\u51fb\u7684\u8fd0\u884c\u65f6\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u635f\u5931\u503c\u6a21\u7cca\u6027\u6765\u62b5\u5fa1\u81ea\u9002\u5e94\u653b\u51fb\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8f93\u51fa\u6270\u52a8\u7684\u8fd0\u884c\u65f6\u9632\u5fa1\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u8981\u4e48\u9700\u8981\u8bbf\u95ee\u6a21\u578b\u53c2\u6570\uff0c\u8981\u4e48\u5728\u9762\u5bf9\u81ea\u9002\u5e94\u653b\u51fb\u65f6\u5931\u6548\u3002\u5373\u4f7f\u662f\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5373\u63d2\u5373\u7528\u9632\u5fa1\u4e5f\u80fd\u88ab\u81ea\u9002\u5e94\u653b\u51fb\u7ed5\u8fc7\uff0c\u8fd9\u66b4\u9732\u4e86\u73b0\u6709\u8fd0\u884c\u65f6\u9632\u5fa1\u7684\u5173\u952e\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faDashed Line Defense (DLD)\uff0c\u8fd9\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u62b5\u5fa1\u81ea\u9002\u5e94\u67e5\u8be2\u7b56\u7565\u3002\u901a\u8fc7\u5728\u89c2\u6d4b\u635f\u5931\u4e0e\u5019\u9009\u6837\u672c\u771f\u5b9e\u5bf9\u6297\u5f3a\u5ea6\u4e4b\u95f4\u5f15\u5165\u6a21\u7cca\u6027\uff0c\u9632\u6b62\u653b\u51fb\u8005\u53ef\u9760\u5730\u5206\u6790\u548c\u8c03\u6574\u67e5\u8be2\uff0c\u4ece\u800c\u6709\u6548\u7834\u574f\u5bf9\u6297\u6837\u672c\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5728ImageNet\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DLD\u7684\u6709\u6548\u6027\uff0c\u8868\u660eDLD\u5728\u4fdd\u6301\u6a21\u578b\u9884\u6d4b\u6807\u7b7e\u7684\u540c\u65f6\uff0c\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u6700\u574f\u60c5\u51b5\u7684\u81ea\u9002\u5e94\u653b\u51fb\u4e0b\u4e5f\u662f\u5982\u6b64\u3002", "conclusion": "DLD\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8fd0\u884c\u65f6\u9632\u5fa1\u673a\u5236\uff0c\u80fd\u591f\u62b5\u5fa1\u81ea\u9002\u5e94\u5206\u6570\u67e5\u8be2\u653b\u51fb\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u9632\u5fa1\u80fd\u529b\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5728\u9762\u5bf9\u81ea\u9002\u5e94\u653b\u51fb\u65f6\u7684\u8106\u5f31\u6027\u95ee\u9898\u3002"}}
{"id": "2602.08681", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08681", "abs": "https://arxiv.org/abs/2602.08681", "authors": ["Leander Kurscheidt", "Gabriele Masina", "Roberto Sebastiani", "Antonio Vergari"], "title": "The Theory and Practice of MAP Inference over Non-Convex Constraints", "comment": null, "summary": "In many safety-critical settings, probabilistic ML systems have to make predictions subject to algebraic constraints, e.g., predicting the most likely trajectory that does not cross obstacles.\n  These real-world constraints are rarely convex, nor the densities considered are (log-)concave.\n  This makes computing this constrained maximum a posteriori (MAP) prediction efficiently and reliably extremely challenging.\n  In this paper, we first investigate under which conditions we can perform constrained MAP inference over continuous variables exactly and efficiently and devise a scalable message-passing algorithm for this tractable fragment.\n  Then, we devise a general constrained MAP strategy that interleaves partitioning the domain into convex feasible regions with numerical constrained optimization.\n  We evaluate both methods on synthetic and real-world benchmarks, showing our %\n  approaches outperform constraint-agnostic baselines, and scale to complex densities intractable for SoTA exact solvers.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u5904\u7406\u8fde\u7eed\u53d8\u91cf\u7ea6\u675fMAP\u63a8\u65ad\u7684\u65b9\u6cd5\uff1a\u9488\u5bf9\u53ef\u89e3\u7247\u6bb5\u7684\u7cbe\u786e\u6d88\u606f\u4f20\u9012\u7b97\u6cd5\uff0c\u4ee5\u53ca\u7ed3\u5408\u533a\u57df\u5212\u5206\u548c\u6570\u503c\u4f18\u5316\u7684\u901a\u7528\u7b56\u7565\uff0c\u5728\u590d\u6742\u5bc6\u5ea6\u51fd\u6570\u4e0a\u4f18\u4e8e\u65e0\u7ea6\u675f\u57fa\u7ebf\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\uff0c\u6982\u7387ML\u7cfb\u7edf\u9700\u8981\u5728\u4ee3\u6570\u7ea6\u675f\u4e0b\u8fdb\u884c\u9884\u6d4b\uff08\u5982\u907f\u5f00\u969c\u788d\u7269\u7684\u6700\u53ef\u80fd\u8f68\u8ff9\uff09\uff0c\u4f46\u8fd9\u4e9b\u7ea6\u675f\u901a\u5e38\u975e\u51f8\u4e14\u5bc6\u5ea6\u51fd\u6570\u975e\u5bf9\u6570\u51f9\uff0c\u4f7f\u5f97\u7ea6\u675fMAP\u63a8\u65ad\u8ba1\u7b97\u6781\u5177\u6311\u6218\u6027\u3002", "method": "1) \u7814\u7a76\u8fde\u7eed\u53d8\u91cf\u7ea6\u675fMAP\u63a8\u65ad\u7684\u53ef\u89e3\u6761\u4ef6\uff0c\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u6d88\u606f\u4f20\u9012\u7b97\u6cd5\uff1b2) \u63d0\u51fa\u901a\u7528\u7b56\u7565\uff0c\u5c06\u57df\u5212\u5206\u4e3a\u51f8\u53ef\u884c\u533a\u57df\u5e76\u4e0e\u6570\u503c\u7ea6\u675f\u4f18\u5316\u4ea4\u66ff\u8fdb\u884c\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e24\u79cd\u65b9\u6cd5\u5747\u4f18\u4e8e\u65e0\u89c6\u7ea6\u675f\u7684\u57fa\u7ebf\uff0c\u5e76\u80fd\u6269\u5c55\u5230\u5f53\u524d\u6700\u5148\u8fdb\u7cbe\u786e\u6c42\u89e3\u5668\u65e0\u6cd5\u5904\u7406\u7684\u590d\u6742\u5bc6\u5ea6\u51fd\u6570\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u5904\u7406\u8fde\u7eed\u53d8\u91cf\u7ea6\u675fMAP\u63a8\u65ad\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u975e\u51f8\u7ea6\u675f\u548c\u975e\u5bf9\u6570\u51f9\u5bc6\u5ea6\u51fd\u6570\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u7684\u53ef\u9760\u9884\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08686", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08686", "abs": "https://arxiv.org/abs/2602.08686", "authors": ["Ning Yang", "Chengzhi Wang", "Yibo Liu", "Baoliang Tian", "Haijun Zhang"], "title": "CompilerKV: Risk-Adaptive KV Compression via Offline Experience Compilation", "comment": null, "summary": "Large Language Models (LLMs) in long-context scenarios are severely constrained by the linear growth of Key-Value (KV) cache memory. Existing KV compression methods rely either on static thresholds and attention-only heuristics or on coarse memory budget allocation. Under tight memory budgets, these methods overlook two key factors: prompt-dependent variation in compression risk and functional heterogeneity across attention heads, which destabilize token selection and lead to tail failures. To address these challenges, we propose CompilerKV, a risk-adaptive and head-aware compression framework that compiles offline experience into reusable decision tables for prefill-only deployment. CompilerKV integrates two key synergistic components: (i) a Head Heterogeneity Table, learned via offline contextual bandits, which assigns head-specific reliability weights to govern functional differences across attention heads explicitly; and (ii) a Risk-Adaptive Threshold Gating mechanism that jointly models attention entropy and local perplexity, transforming prompt-level risk into deployable retention thresholds. Experiments on LongBench show CompilerKV dominates SOTA methods under a 512-token budget, recovering 97.7\\% of FullKV performance while achieving up to +5.2 points gain over the strongest competitor.", "AI": {"tldr": "CompilerKV\uff1a\u4e00\u79cd\u98ce\u9669\u81ea\u9002\u5e94\u3001\u5934\u611f\u77e5\u7684KV\u7f13\u5b58\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u7ebf\u5b66\u4e60\u5934\u5f02\u8d28\u6027\u8868\u548c\u98ce\u9669\u81ea\u9002\u5e94\u9608\u503c\u95e8\u63a7\uff0c\u5728512\u4ee4\u724c\u9884\u7b97\u4e0b\u6062\u590d97.7%\u7684FullKV\u6027\u80fd\u3002", "motivation": "\u73b0\u6709KV\u538b\u7f29\u65b9\u6cd5\u5728\u7d27\u5185\u5b58\u9884\u7b97\u4e0b\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a1) \u5ffd\u7565\u63d0\u793a\u4f9d\u8d56\u7684\u538b\u7f29\u98ce\u9669\u53d8\u5316\uff1b2) \u5ffd\u7565\u6ce8\u610f\u529b\u5934\u4e4b\u95f4\u7684\u529f\u80fd\u5f02\u8d28\u6027\uff0c\u5bfc\u81f4\u4ee4\u724c\u9009\u62e9\u4e0d\u7a33\u5b9a\u548c\u5c3e\u90e8\u5931\u8d25\u3002", "method": "\u63d0\u51faCompilerKV\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1) \u5934\u5f02\u8d28\u6027\u8868\uff1a\u901a\u8fc7\u79bb\u7ebf\u4e0a\u4e0b\u6587\u8001\u864e\u673a\u5b66\u4e60\uff0c\u4e3a\u4e0d\u540c\u6ce8\u610f\u529b\u5934\u5206\u914d\u53ef\u9760\u6027\u6743\u91cd\uff1b2) \u98ce\u9669\u81ea\u9002\u5e94\u9608\u503c\u95e8\u63a7\uff1a\u8054\u5408\u5efa\u6a21\u6ce8\u610f\u529b\u71b5\u548c\u5c40\u90e8\u56f0\u60d1\u5ea6\uff0c\u5c06\u63d0\u793a\u7ea7\u98ce\u9669\u8f6c\u5316\u4e3a\u53ef\u90e8\u7f72\u7684\u4fdd\u7559\u9608\u503c\u3002", "result": "\u5728LongBench\u4e0a\uff0cCompilerKV\u5728512\u4ee4\u724c\u9884\u7b97\u4e0b\u4e3b\u5bfcSOTA\u65b9\u6cd5\uff0c\u6062\u590d97.7%\u7684FullKV\u6027\u80fd\uff0c\u76f8\u6bd4\u6700\u5f3a\u7ade\u4e89\u5bf9\u624b\u63d0\u5347\u9ad8\u8fbe+5.2\u4e2a\u70b9\u3002", "conclusion": "CompilerKV\u901a\u8fc7\u98ce\u9669\u81ea\u9002\u5e94\u548c\u5934\u611f\u77e5\u7684\u538b\u7f29\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587LLM\u4e2dKV\u7f13\u5b58\u5185\u5b58\u7ebf\u6027\u589e\u957f\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684KV\u538b\u7f29\u3002"}}
{"id": "2602.08689", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08689", "abs": "https://arxiv.org/abs/2602.08689", "authors": ["Constant Bourdrez", "Alexandre V\u00e9rine", "Olivier Capp\u00e9"], "title": "Learning To Sample From Diffusion Models Via Inverse Reinforcement Learning", "comment": "Preprint", "summary": "Diffusion models generate samples through an iterative denoising process, guided by a neural network. While training the denoiser on real-world data is computationally demanding, the sampling procedure itself is more flexible. This adaptability serves as a key lever in practice, enabling improvements in both the quality of generated samples and the efficiency of the sampling process. In this work, we introduce an inverse reinforcement learning framework for learning sampling strategies without retraining the denoiser. We formulate the diffusion sampling procedure as a discrete-time finite-horizon Markov Decision Process, where actions correspond to optional modifications of the sampling dynamics. To optimize action scheduling, we avoid defining an explicit reward function. Instead, we directly match the target behavior expected from the sampler using policy gradient techniques. We provide experimental evidence that this approach can improve the quality of samples generated by pretrained diffusion models and automatically tune sampling hyperparameters.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9006\u5f3a\u5316\u5b66\u4e60\u7684\u6269\u6563\u6a21\u578b\u91c7\u6837\u7b56\u7565\u5b66\u4e60\u6846\u67b6\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u53bb\u566a\u5668\u5373\u53ef\u4f18\u5316\u91c7\u6837\u8fc7\u7a0b", "motivation": "\u6269\u6563\u6a21\u578b\u7684\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4f46\u91c7\u6837\u8fc7\u7a0b\u5177\u6709\u7075\u6d3b\u6027\uff0c\u53ef\u4ee5\u5229\u7528\u8fd9\u79cd\u7075\u6d3b\u6027\u6539\u8fdb\u751f\u6210\u6837\u672c\u8d28\u91cf\u548c\u91c7\u6837\u6548\u7387", "method": "\u5c06\u6269\u6563\u91c7\u6837\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u79bb\u6563\u65f6\u95f4\u6709\u9650\u89c6\u91ce\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u52a8\u4f5c\u5bf9\u5e94\u91c7\u6837\u52a8\u529b\u5b66\u7684\u53ef\u9009\u4fee\u6539\uff0c\u4f7f\u7528\u7b56\u7565\u68af\u5ea6\u6280\u672f\u76f4\u63a5\u5339\u914d\u76ee\u6807\u884c\u4e3a\uff0c\u907f\u514d\u5b9a\u4e49\u663e\u5f0f\u5956\u52b1\u51fd\u6570", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6539\u8fdb\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6837\u672c\u8d28\u91cf\uff0c\u5e76\u81ea\u52a8\u8c03\u6574\u91c7\u6837\u8d85\u53c2\u6570", "conclusion": "\u63d0\u51fa\u7684\u9006\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e3a\u6269\u6563\u6a21\u578b\u91c7\u6837\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u53bb\u566a\u5668\u5373\u53ef\u4f18\u5316\u91c7\u6837\u8fc7\u7a0b"}}
{"id": "2602.08690", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08690", "abs": "https://arxiv.org/abs/2602.08690", "authors": ["Shae McFadden", "Myles Foley", "Elizabeth Bates", "Ilias Tsingenopoulos", "Sanyam Vyas", "Vasilios Mavroudis", "Chris Hicks", "Fabio Pierazzi"], "title": "SoK: The Pitfalls of Deep Reinforcement Learning for Cybersecurity", "comment": null, "summary": "Deep Reinforcement Learning (DRL) has achieved remarkable success in domains requiring sequential decision-making, motivating its application to cybersecurity problems. However, transitioning DRL from laboratory simulations to bespoke cyber environments can introduce numerous issues. This is further exacerbated by the often adversarial, non-stationary, and partially-observable nature of most cybersecurity tasks. In this paper, we identify and systematize 11 methodological pitfalls that frequently occur in DRL for cybersecurity (DRL4Sec) literature across the stages of environment modeling, agent training, performance evaluation, and system deployment. By analyzing 66 significant DRL4Sec papers (2018-2025), we quantify the prevalence of each pitfall and find an average of over five pitfalls per paper. We demonstrate the practical impact of these pitfalls using controlled experiments in (i) autonomous cyber defense, (ii) adversarial malware creation, and (iii) web security testing environments. Finally, we provide actionable recommendations for each pitfall to support the development of more rigorous and deployable DRL-based security systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u7f51\u7edc\u5b89\u5168\u5e94\u7528\u4e2d\u768411\u4e2a\u5e38\u89c1\u65b9\u6cd5\u5b66\u9677\u9631\uff0c\u901a\u8fc7\u5206\u679066\u7bc7\u76f8\u5173\u6587\u732e\u53d1\u73b0\u5e73\u5747\u6bcf\u7bc7\u5b58\u5728\u8d85\u8fc75\u4e2a\u9677\u9631\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5f71\u54cd\uff0c\u6700\u540e\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u7f51\u7edc\u5b89\u5168\u9886\u57df\u7684\u5e94\u7528\u9762\u4e34\u4ece\u5b9e\u9a8c\u5ba4\u6a21\u62df\u5230\u5b9e\u9645\u90e8\u7f72\u7684\u8bf8\u591a\u6311\u6218\uff0c\u5305\u62ec\u5bf9\u6297\u6027\u3001\u975e\u5e73\u7a33\u6027\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u7b49\u95ee\u9898\uff0c\u9700\u8981\u7cfb\u7edf\u8bc6\u522b\u548c\u89e3\u51b3\u8fd9\u4e9b\u65b9\u6cd5\u5b66\u9677\u9631\u3002", "method": "\u901a\u8fc7\u5206\u67902018-2025\u5e74\u95f466\u7bc7\u91cd\u8981\u7684DRL4Sec\u8bba\u6587\uff0c\u8bc6\u522b\u5e76\u7cfb\u7edf\u531611\u4e2a\u65b9\u6cd5\u5b66\u9677\u9631\uff0c\u6db5\u76d6\u73af\u5883\u5efa\u6a21\u3001\u667a\u80fd\u4f53\u8bad\u7ec3\u3001\u6027\u80fd\u8bc4\u4f30\u548c\u7cfb\u7edf\u90e8\u7f72\u56db\u4e2a\u9636\u6bb5\uff0c\u5e76\u901a\u8fc7\u81ea\u4e3b\u7f51\u7edc\u9632\u5fa1\u3001\u5bf9\u6297\u6027\u6076\u610f\u8f6f\u4ef6\u521b\u5efa\u548cWeb\u5b89\u5168\u6d4b\u8bd5\u4e09\u4e2a\u73af\u5883\u7684\u53d7\u63a7\u5b9e\u9a8c\u9a8c\u8bc1\u8fd9\u4e9b\u9677\u9631\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5e73\u5747\u6bcf\u7bc7\u8bba\u6587\u5b58\u5728\u8d85\u8fc75\u4e2a\u65b9\u6cd5\u5b66\u9677\u9631\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8fd9\u4e9b\u9677\u9631\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5b9e\u9645\u5f71\u54cd\uff0c\u8868\u660e\u5f53\u524dDRL4Sec\u7814\u7a76\u5b58\u5728\u7cfb\u7edf\u6027\u65b9\u6cd5\u5b66\u95ee\u9898\u3002", "conclusion": "\u8bba\u6587\u4e3a\u6bcf\u4e2a\u9677\u9631\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u5efa\u8bae\uff0c\u65e8\u5728\u652f\u6301\u5f00\u53d1\u66f4\u4e25\u8c28\u548c\u53ef\u90e8\u7f72\u7684\u57fa\u4e8eDRL\u7684\u5b89\u5168\u7cfb\u7edf\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u4ece\u5b9e\u9a8c\u5ba4\u7814\u7a76\u5411\u5b9e\u9645\u5e94\u7528\u8f6c\u5316\u3002"}}
{"id": "2602.08693", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08693", "abs": "https://arxiv.org/abs/2602.08693", "authors": ["Gon\u00e7alo Guiomar", "Elia Torre", "Pehuen Moure", "Victoria Shavina", "Mario Giulianelli", "Shih-Chii Liu", "Valerio Mante"], "title": "Reasoning aligns language models to human cognition", "comment": "38 pages, 4 main figures, multiple appendix figures", "summary": "Do language models make decisions under uncertainty like humans do, and what role does chain-of-thought (CoT) reasoning play in the underlying decision process? We introduce an active probabilistic reasoning task that cleanly separates sampling (actively acquiring evidence) from inference (integrating evidence toward a decision). Benchmarking humans and a broad set of contemporary large language models against near-optimal reference policies reveals a consistent pattern: extended reasoning is the key determinant of strong performance, driving large gains in inference and producing belief trajectories that become strikingly human-like, while yielding only modest improvements in active sampling. To explain these differences, we fit a mechanistic model that captures systematic deviations from optimal behavior via four interpretable latent variables: memory, strategy, choice bias, and occlusion awareness. This model places humans and models in a shared low-dimensional cognitive space, reproduces behavioral signatures across agents, and shows how chain-of-thought shifts language models toward human-like regimes of evidence accumulation and belief-to-choice mapping, tightening alignment in inference while leaving a persistent gap in information acquisition.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u51b3\u7b56\u662f\u5426\u50cf\u4eba\u7c7b\uff1f\u7814\u7a76\u53d1\u73b0\u6269\u5c55\u63a8\u7406\uff08\u5982\u601d\u7ef4\u94fe\uff09\u662f\u6027\u80fd\u63d0\u5347\u7684\u5173\u952e\uff0c\u4f7f\u6a21\u578b\u63a8\u7406\u66f4\u63a5\u8fd1\u4eba\u7c7b\uff0c\u4f46\u5728\u4e3b\u52a8\u4fe1\u606f\u83b7\u53d6\u65b9\u9762\u4ecd\u6709\u5dee\u8ddd\u3002", "motivation": "\u63a2\u7a76\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u51b3\u7b56\u673a\u5236\u662f\u5426\u4e0e\u4eba\u7c7b\u76f8\u4f3c\uff0c\u4ee5\u53ca\u601d\u7ef4\u94fe\u63a8\u7406\u5728\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u4f5c\u7528\uff0c\u901a\u8fc7\u5c06\u4e3b\u52a8\u91c7\u6837\u4e0e\u63a8\u7406\u5206\u79bb\u6765\u7cfb\u7edf\u6bd4\u8f83\u4eba\u7c7b\u548c\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\u3002", "method": "\u5f15\u5165\u4e3b\u52a8\u6982\u7387\u63a8\u7406\u4efb\u52a1\uff0c\u5206\u79bb\u4e3b\u52a8\u8bc1\u636e\u83b7\u53d6\uff08\u91c7\u6837\uff09\u548c\u8bc1\u636e\u6574\u5408\uff08\u63a8\u7406\uff09\u3002\u6bd4\u8f83\u4eba\u7c7b\u548c\u591a\u79cd\u5f53\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u8fd1\u6700\u4f18\u53c2\u8003\u7b56\u7565\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u673a\u5236\u6a21\u578b\u901a\u8fc7\u56db\u4e2a\u53ef\u89e3\u91ca\u6f5c\u53d8\u91cf\uff08\u8bb0\u5fc6\u3001\u7b56\u7565\u3001\u9009\u62e9\u504f\u5dee\u3001\u906e\u6321\u610f\u8bc6\uff09\u5206\u6790\u884c\u4e3a\u5dee\u5f02\u3002", "result": "\u6269\u5c55\u63a8\u7406\u662f\u5f3a\u6027\u80fd\u7684\u5173\u952e\u51b3\u5b9a\u56e0\u7d20\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u80fd\u529b\u5e76\u4f7f\u4fe1\u5ff5\u8f68\u8ff9\u66f4\u63a5\u8fd1\u4eba\u7c7b\uff0c\u4f46\u5bf9\u4e3b\u52a8\u91c7\u6837\u7684\u6539\u5584\u6709\u9650\u3002\u601d\u7ef4\u94fe\u63a8\u7406\u4f7f\u8bed\u8a00\u6a21\u578b\u5411\u4eba\u7c7b\u5f0f\u7684\u8bc1\u636e\u79ef\u7d2f\u548c\u4fe1\u5ff5-\u9009\u62e9\u6620\u5c04\u8f6c\u53d8\uff0c\u5728\u63a8\u7406\u4e0a\u66f4\u5bf9\u9f50\u4eba\u7c7b\uff0c\u4f46\u5728\u4fe1\u606f\u83b7\u53d6\u65b9\u9762\u4ecd\u6709\u6301\u7eed\u5dee\u8ddd\u3002", "conclusion": "\u601d\u7ef4\u94fe\u63a8\u7406\u662f\u8bed\u8a00\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\u66f4\u63a5\u8fd1\u4eba\u7c7b\u7684\u5173\u952e\u673a\u5236\uff0c\u4e3b\u8981\u5728\u63a8\u7406\u6574\u5408\u65b9\u9762\u4ea7\u751f\u4eba\u7c7b\u5bf9\u9f50\uff0c\u4f46\u5728\u4e3b\u52a8\u4fe1\u606f\u83b7\u53d6\u7b56\u7565\u65b9\u9762\u4ecd\u9700\u6539\u8fdb\u3002\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u8ba4\u77e5\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u5171\u4eab\u7684\u8ba4\u77e5\u7a7a\u95f4\u6846\u67b6\u3002"}}
{"id": "2602.08695", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08695", "abs": "https://arxiv.org/abs/2602.08695", "authors": ["Evan Peters", "Ando Deng", "Matheus H. Zambianco", "Devin Blankespoor", "Achim Kempf"], "title": "Trapped by simplicity: When Transformers fail to learn from noisy features", "comment": "13+12 pages, 7 figures. Accepted at ICLR 2026", "summary": "Noise is ubiquitous in data used to train large language models, but it is not well understood whether these models are able to correctly generalize to inputs generated without noise. Here, we study noise-robust learning: are transformers trained on data with noisy features able to find a target function that correctly predicts labels for noiseless features? We show that transformers succeed at noise-robust learning for a selection of $k$-sparse parity and majority functions, compared to LSTMs which fail at this task for even modest feature noise. However, we find that transformers typically fail at noise-robust learning of random $k$-juntas, especially when the boolean sensitivity of the optimal solution is smaller than that of the target function. We argue that this failure is due to a combination of two factors: transformers' bias toward simpler functions, combined with an observation that the optimal function for noise-robust learning typically has lower sensitivity than the target function for random boolean functions. We test this hypothesis by exploiting transformers' simplicity bias to trap them in an incorrect solution, but show that transformers can escape this trap by training with an additional loss term penalizing high-sensitivity solutions. Overall, we find that transformers are particularly ineffective for learning boolean functions in the presence of feature noise.", "AI": {"tldr": "Transformer\u5728\u566a\u58f0\u7279\u5f81\u6570\u636e\u8bad\u7ec3\u4e2d\uff0c\u5bf9\u67d0\u4e9b\u7a00\u758f\u5947\u5076\u6821\u9a8c\u548c\u591a\u6570\u51fd\u6570\u80fd\u5b9e\u73b0\u566a\u58f0\u9c81\u68d2\u5b66\u4e60\uff0c\u4f46\u5bf9\u968f\u673ak-junta\u51fd\u6570\u901a\u5e38\u5931\u8d25\uff0c\u5c24\u5176\u5f53\u6700\u4f18\u89e3\u5e03\u5c14\u654f\u611f\u5ea6\u4f4e\u4e8e\u76ee\u6807\u51fd\u6570\u65f6\u3002", "motivation": "\u7814\u7a76Transformer\u5728\u566a\u58f0\u7279\u5f81\u6570\u636e\u8bad\u7ec3\u540e\uff0c\u80fd\u5426\u6b63\u786e\u6cdb\u5316\u5230\u65e0\u566a\u58f0\u8f93\u5165\uff0c\u5373\u566a\u58f0\u9c81\u68d2\u5b66\u4e60\u80fd\u529b\u3002\u63a2\u7d22Transformer\u4e0eLSTM\u5728\u566a\u58f0\u9c81\u68d2\u5b66\u4e60\u4e0a\u7684\u5dee\u5f02\uff0c\u5e76\u5206\u6790Transformer\u5931\u8d25\u7684\u539f\u56e0\u3002", "method": "\u7814\u7a76Transformer\u5728k-\u7a00\u758f\u5947\u5076\u6821\u9a8c\u3001\u591a\u6570\u51fd\u6570\u548c\u968f\u673ak-junta\u51fd\u6570\u4e0a\u7684\u566a\u58f0\u9c81\u68d2\u5b66\u4e60\u80fd\u529b\u3002\u901a\u8fc7\u5bf9\u6bd4LSTM\u6027\u80fd\uff0c\u5206\u6790Transformer\u5931\u8d25\u539f\u56e0\uff0c\u63d0\u51fa\u654f\u611f\u6027\u60e9\u7f5a\u635f\u5931\u6765\u5e2e\u52a9Transformer\u9003\u79bb\u9519\u8bef\u89e3\u3002", "result": "Transformer\u5bf9k-\u7a00\u758f\u5947\u5076\u6821\u9a8c\u548c\u591a\u6570\u51fd\u6570\u80fd\u5b9e\u73b0\u566a\u58f0\u9c81\u68d2\u5b66\u4e60\uff0c\u4f18\u4e8eLSTM\u3002\u4f46\u5bf9\u968f\u673ak-junta\u51fd\u6570\u901a\u5e38\u5931\u8d25\uff0c\u5c24\u5176\u5f53\u6700\u4f18\u89e3\u654f\u611f\u6027\u4f4e\u4e8e\u76ee\u6807\u51fd\u6570\u65f6\u3002\u901a\u8fc7\u6dfb\u52a0\u654f\u611f\u6027\u60e9\u7f5a\u635f\u5931\uff0cTransformer\u80fd\u9003\u79bb\u9519\u8bef\u89e3\u9677\u9631\u3002", "conclusion": "Transformer\u5728\u5e03\u5c14\u51fd\u6570\u566a\u58f0\u9c81\u68d2\u5b66\u4e60\u65b9\u9762\u6548\u679c\u4e0d\u4f73\uff0c\u5931\u8d25\u539f\u56e0\u662f\u5176\u504f\u5411\u7b80\u5355\u51fd\u6570\uff0c\u800c\u566a\u58f0\u9c81\u68d2\u5b66\u4e60\u7684\u6700\u4f18\u51fd\u6570\u901a\u5e38\u654f\u611f\u6027\u66f4\u4f4e\u3002\u901a\u8fc7\u654f\u611f\u6027\u60e9\u7f5a\u53ef\u4ee5\u6539\u5584\uff0c\u4f46\u603b\u4f53\u4e0aTransformer\u5bf9\u5e03\u5c14\u51fd\u6570\u566a\u58f0\u5b66\u4e60\u6548\u679c\u6709\u9650\u3002"}}
{"id": "2602.08722", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08722", "abs": "https://arxiv.org/abs/2602.08722", "authors": ["Dalton Jones", "Junyoung Park", "Matthew Morse", "Mingu Lee", "Chris Lott", "Harper Langston"], "title": "QUOKA: Query-Oriented KV Selection For Efficient LLM Prefill", "comment": null, "summary": "We present QUOKA: Query-oriented KV selection for efficient attention, a training-free and hardware agnostic sparse attention algorithm for accelerating transformer inference under chunked prefill. While many queries focus on a smaller group of keys in the attention operator, we observe that queries with low cosine similarity with respect to the mean query interact more strongly with more keys and have the greatest contribution to final attention logits. By prioritizing these low cosine similarity queries, the behavior of full attention during the prefill stage can be closely approximated. QUOKA leverages this observation, accelerating attention by (1) first retaining a small set of representative queries and (2) then subselectin the keys most aligned with those queries. Through experiments on Needle-In-A-Haystack, LongBench, RULER, and Math500, we show that, while realizing a 3x reduction in time-to-first-token, 5x speedup in attention on Nvidia GPUs and up to nearly a 7x speedup on Intel Xeon CPUs, QUOKA achieves near-baseline accuracy, utilizing 88% fewer key-value pairs per attention evaluation.", "AI": {"tldr": "QUOKA\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u786c\u4ef6\u65e0\u5173\u7684\u7a00\u758f\u6ce8\u610f\u529b\u7b97\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u4f4e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u67e5\u8be2\u548c\u76f8\u5173\u7684\u952e\u503c\u5bf9\u6765\u52a0\u901fTransformer\u63a8\u7406\uff0c\u5728\u4fdd\u6301\u63a5\u8fd1\u57fa\u7ebf\u51c6\u786e\u7387\u7684\u540c\u65f6\u5b9e\u73b03-7\u500d\u52a0\u901f\u3002", "motivation": "\u5728\u5206\u5757\u9884\u586b\u5145\uff08chunked prefill\uff09\u573a\u666f\u4e0b\uff0c\u8bb8\u591a\u67e5\u8be2\u53ea\u5173\u6ce8\u4e00\u5c0f\u90e8\u5206\u952e\uff0c\u4f46\u4f4e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u67e5\u8be2\u4e0e\u66f4\u591a\u952e\u4ea4\u4e92\u4e14\u5bf9\u6700\u7ec8\u6ce8\u610f\u529b\u5bf9\u6570\u8d21\u732e\u6700\u5927\uff0c\u901a\u8fc7\u4f18\u5148\u5904\u7406\u8fd9\u4e9b\u67e5\u8be2\u53ef\u4ee5\u8fd1\u4f3c\u5b8c\u6574\u6ce8\u610f\u529b\u7684\u884c\u4e3a\u3002", "method": "QUOKA\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u4fdd\u7559\u4e00\u5c0f\u90e8\u5206\u4ee3\u8868\u6027\u67e5\u8be2\uff08\u7279\u522b\u662f\u4f4e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u67e5\u8be2\uff09\uff1b2) \u57fa\u4e8e\u8fd9\u4e9b\u67e5\u8be2\u9009\u62e9\u6700\u76f8\u5173\u7684\u952e\u503c\u5bf9\uff0c\u4ece\u800c\u51cf\u5c11\u9700\u8981\u8ba1\u7b97\u7684\u952e\u503c\u5bf9\u6570\u91cf\u3002", "result": "\u5728Needle-In-A-Haystack\u3001LongBench\u3001RULER\u548cMath500\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cQUOKA\u5b9e\u73b0\u4e86\u9996token\u65f6\u95f4\u51cf\u5c113\u500d\uff0cNvidia GPU\u4e0a\u6ce8\u610f\u529b\u8ba1\u7b97\u52a0\u901f5\u500d\uff0cIntel Xeon CPU\u4e0a\u52a0\u901f\u8fd17\u500d\uff0c\u540c\u65f6\u4f7f\u752888%\u66f4\u5c11\u7684\u952e\u503c\u5bf9\uff0c\u51c6\u786e\u7387\u63a5\u8fd1\u57fa\u7ebf\u3002", "conclusion": "QUOKA\u662f\u4e00\u79cd\u6709\u6548\u7684\u7a00\u758f\u6ce8\u610f\u529b\u7b97\u6cd5\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u52a0\u901fTransformer\u63a8\u7406\uff0c\u7279\u522b\u662f\u5728\u5206\u5757\u9884\u586b\u5145\u573a\u666f\u4e0b\uff0c\u5177\u6709\u786c\u4ef6\u65e0\u5173\u6027\u548c\u65e0\u9700\u8bad\u7ec3\u7684\u4f18\u52bf\u3002"}}
{"id": "2602.08723", "categories": ["cs.LG", "cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08723", "abs": "https://arxiv.org/abs/2602.08723", "authors": ["Yujie Shen", "Zihan Wang", "Jian Qian", "Qi Lei"], "title": "Data Reconstruction: Identifiability and Optimization with Sample Splitting", "comment": null, "summary": "Training data reconstruction from KKT conditions has shown striking empirical success, yet it remains unclear when the resulting KKT equations have unique solutions and, even in identifiable regimes, how to reliably recover solutions by optimization. This work hereby focuses on these two complementary questions: identifiability and optimization. On the identifiability side, we discuss the sufficient conditions for KKT system of two-layer networks with polynomial activations to uniquely determine the training data, providing a theoretical explanation of when and why reconstruction is possible. On the optimization side, we introduce sample splitting, a curvature-aware refinement step applicable to general reconstruction objectives (not limited to KKT-based formulations): it creates additional descent directions to escape poor stationary points and refine solutions. Experiments demonstrate that augmenting several existing reconstruction methods with sample splitting consistently improves reconstruction performance.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4eceKKT\u6761\u4ef6\u91cd\u5efa\u8bad\u7ec3\u6570\u636e\u7684\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u53ef\u8bc6\u522b\u6027\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u63d0\u51fa\u591a\u9879\u5f0f\u6fc0\u6d3b\u4e24\u5c42\u7f51\u7edc\u7684\u53ef\u8bc6\u522b\u6761\u4ef6\uff0c\u5e76\u5f15\u5165\u6837\u672c\u5206\u88c2\u4f18\u5316\u6280\u672f\u63d0\u5347\u91cd\u5efa\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u4eceKKT\u6761\u4ef6\u91cd\u5efa\u8bad\u7ec3\u6570\u636e\u5728\u5b9e\u8bc1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695aKKT\u65b9\u7a0b\u4f55\u65f6\u6709\u552f\u4e00\u89e3\uff0c\u4ee5\u53ca\u5728\u53ef\u8bc6\u522b\u60c5\u51b5\u4e0b\u5982\u4f55\u901a\u8fc7\u4f18\u5316\u53ef\u9760\u5730\u6062\u590d\u89e3\u3002\u56e0\u6b64\u9700\u8981\u7814\u7a76\u8fd9\u4e24\u4e2a\u4e92\u8865\u95ee\u9898\uff1a\u53ef\u8bc6\u522b\u6027\u548c\u4f18\u5316\u65b9\u6cd5\u3002", "method": "1. \u53ef\u8bc6\u522b\u6027\u5206\u6790\uff1a\u8ba8\u8bba\u591a\u9879\u5f0f\u6fc0\u6d3b\u4e24\u5c42\u7f51\u7edcKKT\u7cfb\u7edf\u552f\u4e00\u786e\u5b9a\u8bad\u7ec3\u6570\u636e\u7684\u5145\u5206\u6761\u4ef6\uff1b2. \u4f18\u5316\u65b9\u6cd5\uff1a\u5f15\u5165\u6837\u672c\u5206\u88c2\u6280\u672f\uff0c\u8fd9\u662f\u4e00\u79cd\u9002\u7528\u4e8e\u4e00\u822c\u91cd\u5efa\u76ee\u6807\u7684\u66f2\u7387\u611f\u77e5\u7ec6\u5316\u6b65\u9aa4\uff0c\u53ef\u521b\u5efa\u989d\u5916\u7684\u4e0b\u964d\u65b9\u5411\u4ee5\u9003\u79bb\u4e0d\u826f\u9a7b\u70b9\u5e76\u7ec6\u5316\u89e3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c06\u6837\u672c\u5206\u88c2\u6280\u672f\u589e\u5f3a\u5230\u591a\u4e2a\u73b0\u6709\u91cd\u5efa\u65b9\u6cd5\u4e2d\uff0c\u80fd\u4e00\u81f4\u5730\u63d0\u9ad8\u91cd\u5efa\u6027\u80fd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\u91cd\u5efa\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\uff08\u4f55\u65f6\u4ee5\u53ca\u4e3a\u4ec0\u4e48\u91cd\u5efa\u662f\u53ef\u80fd\u7684\uff09\u548c\u5b9e\u7528\u7684\u4f18\u5316\u6280\u672f\uff0c\u901a\u8fc7\u6837\u672c\u5206\u88c2\u663e\u8457\u6539\u8fdb\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u91cd\u5efa\u6548\u679c\u3002"}}
{"id": "2602.08733", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08733", "abs": "https://arxiv.org/abs/2602.08733", "authors": ["Maximilian Mauel", "Johannes R. H\u00fcbers", "David Berghaus", "Patrick Seifner", "Ramses J. Sanchez"], "title": "Foundation Inference Models for Ordinary Differential Equations", "comment": null, "summary": "Ordinary differential equations (ODEs) are central to scientific modelling, but inferring their vector fields from noisy trajectories remains challenging. Current approaches such as symbolic regression, Gaussian process (GP) regression, and Neural ODEs often require complex training pipelines and substantial machine learning expertise, or they depend strongly on system-specific prior knowledge. We propose FIM-ODE, a pretrained Foundation Inference Model that amortises low-dimensional ODE inference by predicting the vector field directly from noisy trajectory data in a single forward pass. We pretrain FIM-ODE on a prior distribution over ODEs with low-degree polynomial vector fields and represent the target field with neural operators. FIM-ODE achieves strong zero-shot performance, matching and often improving upon ODEFormer, a recent pretrained symbolic baseline, across a range of regimes despite using a simpler pretraining prior distribution. Pretraining also provides a strong initialisation for finetuning, enabling fast and stable adaptation that outperforms modern neural and GP baselines without requiring machine learning expertise.", "AI": {"tldr": "FIM-ODE\uff1a\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u63a8\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u76f4\u63a5\u4ece\u566a\u58f0\u8f68\u8ff9\u6570\u636e\u9884\u6d4bODE\u5411\u91cf\u573a\uff0c\u5b9e\u73b0\u96f6\u6837\u672cODE\u63a8\u65ad", "motivation": "\u73b0\u6709ODE\u5411\u91cf\u573a\u63a8\u65ad\u65b9\u6cd5\uff08\u7b26\u53f7\u56de\u5f52\u3001\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u3001\u795e\u7ecfODE\uff09\u9700\u8981\u590d\u6742\u8bad\u7ec3\u6d41\u7a0b\u6216\u5f3a\u70c8\u4f9d\u8d56\u7cfb\u7edf\u5148\u9a8c\u77e5\u8bc6\uff0c\u7f3a\u4e4f\u7b80\u5355\u6613\u7528\u7684\u901a\u7528\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faFIM-ODE\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u4f4e\u9636\u591a\u9879\u5f0f\u5411\u91cf\u573a\u7684ODE\u5148\u9a8c\u5206\u5e03\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4f7f\u7528\u795e\u7ecf\u7b97\u5b50\u8868\u793a\u76ee\u6807\u573a\uff0c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4ece\u566a\u58f0\u8f68\u8ff9\u9884\u6d4b\u5411\u91cf\u573a", "result": "FIM-ODE\u5728\u96f6\u6837\u672c\u6027\u80fd\u4e0a\u5339\u914d\u751a\u81f3\u8d85\u8d8aODEFormer\uff08\u6700\u8fd1\u7684\u9884\u8bad\u7ec3\u7b26\u53f7\u57fa\u7ebf\uff09\uff0c\u4e14\u9884\u8bad\u7ec3\u4e3a\u5fae\u8c03\u63d0\u4f9b\u5f3a\u521d\u59cb\u5316\uff0c\u5728\u9002\u5e94\u65b0\u4efb\u52a1\u65f6\u4f18\u4e8e\u73b0\u4ee3\u795e\u7ecf\u548cGP\u57fa\u7ebf", "conclusion": "FIM-ODE\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684ODE\u63a8\u65ad\u65b9\u6cd5\uff0c\u65e0\u9700\u590d\u6742\u8bad\u7ec3\u6d41\u7a0b\u6216\u673a\u5668\u5b66\u4e60\u4e13\u4e1a\u77e5\u8bc6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u5b9e\u73b0\u5feb\u901f\u7a33\u5b9a\u9002\u5e94"}}
{"id": "2602.08745", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08745", "abs": "https://arxiv.org/abs/2602.08745", "authors": ["Saku Peltonen", "Roger Wattenhofer"], "title": "On the Expressive Power of GNNs for Boolean Satisfiability", "comment": "Accepted at ICLR 2026", "summary": "Machine learning approaches to solving Boolean Satisfiability (SAT) aim to replace handcrafted heuristics with learning-based models. Graph Neural Networks have emerged as the main architecture for SAT solving, due to the natural graph representation of Boolean formulas. We analyze the expressive power of GNNs for SAT solving through the lens of the Weisfeiler-Leman (WL) test. As our main result, we prove that the full WL hierarchy cannot, in general, distinguish between satisfiable and unsatisfiable instances. We show that indistinguishability under higher-order WL carries over to practical limitations for WL-bounded solvers that set variables sequentially. We further study the expressivity required for several important families of SAT instances, including regular, random and planar instances. To quantify expressivity needs in practice, we conduct experiments on random instances from the G4SAT benchmark and industrial instances from the International SAT Competition. Our results suggest that while random instances are largely distinguishable, industrial instances often require more expressivity to predict a satisfying assignment.", "AI": {"tldr": "GNNs\u5728SAT\u6c42\u89e3\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\u53d7\u9650\u4e8eWL\u6d4b\u8bd5\u5c42\u6b21\uff0c\u65e0\u6cd5\u533a\u5206\u53ef\u6ee1\u8db3\u4e0e\u4e0d\u53ef\u6ee1\u8db3\u5b9e\u4f8b\uff0c\u5de5\u4e1a\u5b9e\u4f8b\u9700\u8981\u66f4\u5f3a\u7684\u8868\u8fbe\u80fd\u529b", "motivation": "\u5206\u6790\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u5e03\u5c14\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\u6c42\u89e3\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u7406\u89e3\u5176\u7406\u8bba\u9650\u5236\u548c\u5b9e\u9645\u5e94\u7528\u9700\u6c42", "method": "\u901a\u8fc7Weisfeiler-Leman\u6d4b\u8bd5\u5c42\u6b21\u5206\u6790GNN\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u8bc1\u660eWL\u5c42\u6b21\u65e0\u6cd5\u533a\u5206\u53ef\u6ee1\u8db3\u4e0e\u4e0d\u53ef\u6ee1\u8db3\u5b9e\u4f8b\uff0c\u5e76\u7814\u7a76\u4e0d\u540cSAT\u5b9e\u4f8b\u5bb6\u65cf\u7684\u8868\u8fbe\u9700\u6c42", "result": "\u9ad8\u9636WL\u6d4b\u8bd5\u65e0\u6cd5\u533a\u5206\u53ef\u6ee1\u8db3\u4e0e\u4e0d\u53ef\u6ee1\u8db3\u5b9e\u4f8b\uff0c\u8fd9\u8f6c\u5316\u4e3aWL\u6709\u754c\u6c42\u89e3\u5668\u7684\u5b9e\u9645\u9650\u5236\uff1b\u968f\u673a\u5b9e\u4f8b\u5927\u591a\u53ef\u533a\u5206\uff0c\u4f46\u5de5\u4e1a\u5b9e\u4f8b\u9700\u8981\u66f4\u5f3a\u7684\u8868\u8fbe\u80fd\u529b", "conclusion": "GNN\u5728SAT\u6c42\u89e3\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\u53d7WL\u6d4b\u8bd5\u9650\u5236\uff0c\u5de5\u4e1a\u5b9e\u4f8b\u9700\u8981\u8d85\u8d8aWL\u5c42\u6b21\u7684\u8868\u8fbe\u80fd\u529b\u624d\u80fd\u6709\u6548\u9884\u6d4b\u6ee1\u8db3\u8d4b\u503c"}}
{"id": "2602.08751", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2602.08751", "abs": "https://arxiv.org/abs/2602.08751", "authors": ["Nobuyuki Ota"], "title": "Central Dogma Transformer II: An AI Microscope for Understanding Cellular Regulatory Mechanisms", "comment": "20 pages, 6 figures", "summary": "Current biological AI models lack interpretability -- their internal representations do not correspond to biological relationships that\n  researchers can examine. Here we present CDT-II, an \"AI microscope\" whose attention maps are directly interpretable as regulatory structure.\n  By mirroring the central dogma in its architecture, each attention mechanism corresponds to a specific biological relationship: DNA\n  self-attention for genomic relationships, RNA self-attention for gene co-regulation, and DNA-to-RNA cross-attention for transcriptional\n  control. Using only genomic embeddings and raw per-cell expression, CDT-II enables experimental biologists to observe regulatory networks in\n  their own data. Applied to K562 CRISPRi data, CDT-II predicts perturbation effects (per-gene mean $r = 0.84$) and recovers the GFI1B\n  regulatory network without supervision (6.6-fold enrichment, $P = 3.5 \\times 10^{-17}$). Two distinct attention mechanisms converge on an RNA\n  processing module ($P = 1 \\times 10^{-16}$). CDT-II establishes mechanism-oriented AI as an alternative to task-oriented approaches, revealing\n  regulatory structure rather than merely optimizing predictions.", "AI": {"tldr": "CDT-II\u662f\u4e00\u4e2a\"AI\u663e\u5fae\u955c\"\uff0c\u901a\u8fc7\u6a21\u4eff\u4e2d\u5fc3\u6cd5\u5219\u7684\u67b6\u6784\uff0c\u5176\u6ce8\u610f\u529b\u673a\u5236\u76f4\u63a5\u53ef\u89e3\u91ca\u4e3a\u8c03\u63a7\u7ed3\u6784\uff0c\u4f7f\u5b9e\u9a8c\u751f\u7269\u5b66\u5bb6\u80fd\u591f\u89c2\u5bdf\u81ea\u5df1\u6570\u636e\u4e2d\u7684\u8c03\u63a7\u7f51\u7edc\u3002", "motivation": "\u5f53\u524d\u751f\u7269AI\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u2014\u2014\u5176\u5185\u90e8\u8868\u793a\u4e0d\u5bf9\u5e94\u4e8e\u7814\u7a76\u4eba\u5458\u53ef\u68c0\u67e5\u7684\u751f\u7269\u5b66\u5173\u7cfb\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u76f4\u63a5\u89e3\u91ca\u8c03\u63a7\u7ed3\u6784\u7684AI\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u955c\u50cf\u4e2d\u5fc3\u6cd5\u5219\u7684\u67b6\u6784\u8bbe\u8ba1CDT-II\u6a21\u578b\uff1aDNA\u81ea\u6ce8\u610f\u529b\u5bf9\u5e94\u57fa\u56e0\u7ec4\u5173\u7cfb\uff0cRNA\u81ea\u6ce8\u610f\u529b\u5bf9\u5e94\u57fa\u56e0\u5171\u8c03\u63a7\uff0cDNA\u5230RNA\u4ea4\u53c9\u6ce8\u610f\u529b\u5bf9\u5e94\u8f6c\u5f55\u63a7\u5236\u3002\u4ec5\u4f7f\u7528\u57fa\u56e0\u7ec4\u5d4c\u5165\u548c\u539f\u59cb\u5355\u7ec6\u80de\u8868\u8fbe\u6570\u636e\u3002", "result": "\u5728K562 CRISPRi\u6570\u636e\u4e2d\uff0cCDT-II\u9884\u6d4b\u6270\u52a8\u6548\u5e94\uff08\u57fa\u56e0\u5e73\u5747r=0.84\uff09\uff0c\u65e0\u76d1\u7763\u5730\u6062\u590dGFI1B\u8c03\u63a7\u7f51\u7edc\uff086.6\u500d\u5bcc\u96c6\uff0cP=3.5\u00d710^-17\uff09\u3002\u4e24\u4e2a\u4e0d\u540c\u7684\u6ce8\u610f\u529b\u673a\u5236\u6c47\u805a\u5230\u4e00\u4e2aRNA\u52a0\u5de5\u6a21\u5757\uff08P=1\u00d710^-16\uff09\u3002", "conclusion": "CDT-II\u5efa\u7acb\u4e86\u673a\u5236\u5bfc\u5411\u7684AI\u4f5c\u4e3a\u4efb\u52a1\u5bfc\u5411\u65b9\u6cd5\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u63ed\u793a\u4e86\u8c03\u63a7\u7ed3\u6784\u800c\u4e0d\u4ec5\u4ec5\u662f\u4f18\u5316\u9884\u6d4b\uff0c\u4e3a\u5b9e\u9a8c\u751f\u7269\u5b66\u5bb6\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u8c03\u63a7\u7f51\u7edc\u89c2\u5bdf\u5de5\u5177\u3002"}}
{"id": "2602.08755", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08755", "abs": "https://arxiv.org/abs/2602.08755", "authors": ["Duc-Anh Nguyen", "Nhien-An Le-Khac"], "title": "Redundancy-Free View Alignment for Multimodal Human Activity Recognition with Arbitrarily Missing Views", "comment": null, "summary": "Multimodal multiview learning seeks to integrate information from diverse sources to enhance task performance. Existing approaches often struggle with flexible view configurations, including arbitrary view combinations, numbers of views, and heterogeneous modalities. Focusing on the context of human activity recognition, we propose RALIS, a model that combines multiview contrastive learning with a mixture-of-experts module to support arbitrary view availability during both training and inference. Instead of trying to reconstruct missing views, an adjusted center contrastive loss is used for self-supervised representation learning and view alignment, mitigating the impact of missing views on multiview fusion. This loss formulation allows for the integration of view weights to account for view quality. Additionally, it reduces computational complexity from $O(V^2)$ to $O(V)$, where $V$ is the number of views. To address residual discrepancies not captured by contrastive learning, we employ a mixture-of-experts module with a specialized load balancing strategy, tasked with adapting to arbitrary view combinations. We highlight the geometric relationship among components in our model and how they combine well in the latent space. RALIS is validated on four datasets encompassing inertial and human pose modalities, with the number of views ranging from three to nine, demonstrating its performance and flexibility.", "AI": {"tldr": "RALIS\uff1a\u4e00\u79cd\u7ed3\u5408\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60\u548c\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u652f\u6301\u8bad\u7ec3\u548c\u63a8\u7406\u671f\u95f4\u4efb\u610f\u89c6\u56fe\u53ef\u7528\u6027\u7684\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u591a\u89c6\u56fe\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u7075\u6d3b\u7684\u89c6\u56fe\u914d\u7f6e\uff08\u5305\u62ec\u4efb\u610f\u89c6\u56fe\u7ec4\u5408\u3001\u89c6\u56fe\u6570\u91cf\u548c\u5f02\u6784\u6a21\u6001\uff09\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u7279\u522b\u662f\u5728\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u4e2d\uff0c\u9700\u8981\u80fd\u591f\u9002\u5e94\u8bad\u7ec3\u548c\u63a8\u7406\u671f\u95f4\u89c6\u56fe\u53ef\u7528\u6027\u53d8\u5316\u7684\u6a21\u578b\u3002", "method": "RALIS\u7ed3\u5408\u4e86\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60\u548c\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\u3002\u4f7f\u7528\u8c03\u6574\u7684\u4e2d\u5fc3\u5bf9\u6bd4\u635f\u5931\u8fdb\u884c\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\u548c\u89c6\u56fe\u5bf9\u9f50\uff0c\u800c\u4e0d\u662f\u91cd\u5efa\u7f3a\u5931\u89c6\u56fe\u3002\u8be5\u635f\u5931\u516c\u5f0f\u5141\u8bb8\u96c6\u6210\u89c6\u56fe\u6743\u91cd\u4ee5\u8003\u8651\u89c6\u56fe\u8d28\u91cf\uff0c\u5e76\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(V\u00b2)\u964d\u4f4e\u5230O(V)\u3002\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\u91c7\u7528\u4e13\u95e8\u7684\u8d1f\u8f7d\u5e73\u8861\u7b56\u7565\u6765\u9002\u5e94\u4efb\u610f\u89c6\u56fe\u7ec4\u5408\u3002", "result": "RALIS\u5728\u56db\u4e2a\u5305\u542b\u60ef\u6027\u548c\u4eba\u4f53\u59ff\u6001\u6a21\u6001\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u89c6\u56fe\u6570\u91cf\u4ece3\u52309\u4e0d\u7b49\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6a21\u578b\u5177\u6709\u826f\u597d\u7684\u6027\u80fd\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "RALIS\u901a\u8fc7\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u89c6\u56fe\u5b66\u4e60\u4e2d\u89c6\u56fe\u53ef\u7528\u6027\u53d8\u5316\u7684\u95ee\u9898\uff0c\u5728\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5177\u6709\u8ba1\u7b97\u6548\u7387\u4f18\u52bf\u3002"}}
{"id": "2602.08762", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08762", "abs": "https://arxiv.org/abs/2602.08762", "authors": ["Wen Xu", "Zhetao Li", "Yong Xiao", "Pengpeng Qiao", "Mianxiong Dong", "Kaoru Ota"], "title": "HoGS: Homophily-Oriented Graph Synthesis for Local Differentially Private GNN Training", "comment": null, "summary": "Graph neural networks (GNNs) have demonstrated remarkable performance in various graph-based machine learning tasks by effectively modeling high-order interactions between nodes. However, training GNNs without protection may leak sensitive personal information in graph data, including links and node features. Local differential privacy (LDP) is an advanced technique for protecting data privacy in decentralized networks. Unfortunately, existing local differentially private GNNs either only preserve link privacy or suffer significant utility loss in the process of preserving link and node feature privacy. In this paper, we propose an effective LDP framework, called HoGS, which trains GNNs with link and feature protection by generating a synthetic graph. Concretely, HoGS first collects the link and feature information of the graph under LDP, and then utilizes the phenomenon of homophily in graph data to reconstruct the graph structure and node features separately, thereby effectively mitigating the negative impact of LDP on the downstream GNN training. We theoretically analyze the privacy guarantee of HoGS and conduct experiments using the generated synthetic graph as input to various state-of-the-art GNN architectures. Experimental results on three real-world datasets show that HoGS significantly outperforms baseline methods in the accuracy of training GNNs.", "AI": {"tldr": "HoGS\u662f\u4e00\u4e2a\u672c\u5730\u5dee\u5206\u9690\u79c1\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u672c\u5730\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u4e0b\u751f\u6210\u5408\u6210\u56fe\u6765\u8bad\u7ec3\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u540c\u65f6\u4fdd\u62a4\u94fe\u63a5\u548c\u8282\u70b9\u7279\u5f81\u9690\u79c1\uff0c\u5e76\u5229\u7528\u56fe\u6570\u636e\u7684\u540c\u8d28\u6027\u73b0\u8c61\u6765\u51cf\u5c11\u9690\u79c1\u4fdd\u62a4\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u8bad\u7ec3\u65f6\u53ef\u80fd\u6cc4\u9732\u56fe\u4e2d\u7684\u654f\u611f\u4e2a\u4eba\u4fe1\u606f\uff08\u94fe\u63a5\u548c\u8282\u70b9\u7279\u5f81\uff09\u3002\u73b0\u6709\u7684\u672c\u5730\u5dee\u5206\u9690\u79c1\u56fe\u795e\u7ecf\u7f51\u7edc\u8981\u4e48\u53ea\u4fdd\u62a4\u94fe\u63a5\u9690\u79c1\uff0c\u8981\u4e48\u5728\u540c\u65f6\u4fdd\u62a4\u94fe\u63a5\u548c\u8282\u70b9\u7279\u5f81\u9690\u79c1\u65f6\u5bfc\u81f4\u663e\u8457\u7684\u6548\u7528\u635f\u5931\u3002", "method": "HoGS\u6846\u67b6\u9996\u5148\u5728\u672c\u5730\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u4e0b\u6536\u96c6\u56fe\u7684\u94fe\u63a5\u548c\u7279\u5f81\u4fe1\u606f\uff0c\u7136\u540e\u5229\u7528\u56fe\u6570\u636e\u7684\u540c\u8d28\u6027\u73b0\u8c61\u5206\u522b\u91cd\u6784\u56fe\u7ed3\u6784\u548c\u8282\u70b9\u7279\u5f81\uff0c\u751f\u6210\u5408\u6210\u56fe\u4f5c\u4e3a\u4e0b\u6e38\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u5165\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHoGS\u5728\u8bad\u7ec3\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u51c6\u786e\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "HoGS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u672c\u5730\u5dee\u5206\u9690\u79c1\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4fdd\u62a4\u94fe\u63a5\u548c\u8282\u70b9\u7279\u5f81\u9690\u79c1\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u56fe\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7684\u9ad8\u6548\u7528\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u6a21\u578b\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2602.08768", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.08768", "abs": "https://arxiv.org/abs/2602.08768", "authors": ["Chi-Sheng Chen", "Xinyu Zhang", "En-Jui Kuo", "Guan-Ying Chen", "Qiuzhe Xie", "Fan Zhang"], "title": "FreqLens: Interpretable Frequency Attribution for Time Series Forecasting", "comment": null, "summary": "Time series forecasting models often lack interpretability, limiting their adoption in domains requiring explainable predictions. We propose \\textsc{FreqLens}, an interpretable forecasting framework that discovers and attributes predictions to learnable frequency components. \\textsc{FreqLens} introduces two key innovations: (1) \\emph{learnable frequency discovery} -- frequency bases are parameterized via sigmoid mapping and learned from data with diversity regularization, enabling automatic discovery of dominant periodic patterns without domain knowledge; and (2) \\emph{axiomatic frequency attribution} -- a theoretically grounded framework that provably satisfies Completeness, Faithfulness, Null-Frequency, and Symmetry axioms, with per-frequency attributions equivalent to Shapley values. On Traffic and Weather datasets, \\textsc{FreqLens} achieves competitive or superior performance while discovering physically meaningful frequencies: all 5 independent runs discover the 24-hour daily cycle ($24.6 \\pm 0.1$h, 2.5\\% error) and 12-hour half-daily cycle ($11.8 \\pm 0.1$h, 1.6\\% error) on Traffic, and weekly cycles ($10\\times$ longer than the input window) on Weather. These results demonstrate genuine frequency-level knowledge discovery with formal theoretical guarantees on attribution quality.", "AI": {"tldr": "FreqLens\u662f\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u9891\u7387\u53d1\u73b0\u548c\u57fa\u4e8e\u516c\u7406\u7684\u9891\u7387\u5f52\u56e0\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u6027\u80fd\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u901a\u5e38\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u9700\u8981\u53ef\u89e3\u91ca\u9884\u6d4b\u7684\u9886\u57df\u4e2d\u7684\u5e94\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u81ea\u52a8\u53d1\u73b0\u4e3b\u5bfc\u7684\u5468\u671f\u6027\u6a21\u5f0f\u5e76\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u7684\u5f52\u56e0\u3002", "method": "\u63d0\u51faFreqLens\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u53ef\u5b66\u4e60\u7684\u9891\u7387\u53d1\u73b0 - \u901a\u8fc7sigmoid\u6620\u5c04\u53c2\u6570\u5316\u9891\u7387\u57fa\uff0c\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u5e76\u5e26\u6709\u591a\u6837\u6027\u6b63\u5219\u5316\uff0c\u65e0\u9700\u9886\u57df\u77e5\u8bc6\u81ea\u52a8\u53d1\u73b0\u4e3b\u5bfc\u5468\u671f\u6027\u6a21\u5f0f\uff1b2) \u516c\u7406\u5316\u7684\u9891\u7387\u5f52\u56e0 - \u57fa\u4e8e\u7406\u8bba\u6846\u67b6\uff0c\u6ee1\u8db3\u5b8c\u5907\u6027\u3001\u5fe0\u5b9e\u6027\u3001\u96f6\u9891\u7387\u548c\u5bf9\u79f0\u6027\u516c\u7406\uff0c\u6bcf\u4e2a\u9891\u7387\u7684\u5f52\u56e0\u7b49\u4ef7\u4e8eShapley\u503c\u3002", "result": "\u5728Traffic\u548cWeather\u6570\u636e\u96c6\u4e0a\uff0cFreqLens\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u540c\u65f6\u53d1\u73b0\u4e86\u7269\u7406\u4e0a\u6709\u610f\u4e49\u7684\u9891\u7387\uff1a\u5728Traffic\u6570\u636e\u4e2d\u6240\u67095\u6b21\u72ec\u7acb\u8fd0\u884c\u90fd\u53d1\u73b0\u4e8624\u5c0f\u65f6\u65e5\u5468\u671f\uff0824.6\u00b10.1h\uff0c2.5%\u8bef\u5dee\uff09\u548c12\u5c0f\u65f6\u534a\u65e5\u5468\u671f\uff0811.8\u00b10.1h\uff0c1.6%\u8bef\u5dee\uff09\uff0c\u5728Weather\u6570\u636e\u4e2d\u53d1\u73b0\u4e86\u6bd4\u8f93\u5165\u7a97\u53e3\u957f10\u500d\u7684\u5468\u5468\u671f\u3002", "conclusion": "FreqLens\u5c55\u793a\u4e86\u771f\u6b63\u7684\u9891\u7387\u7ea7\u77e5\u8bc6\u53d1\u73b0\u80fd\u529b\uff0c\u5e76\u5728\u5f52\u56e0\u8d28\u91cf\u4e0a\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u4e3a\u53ef\u89e3\u91ca\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08774", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08774", "abs": "https://arxiv.org/abs/2602.08774", "authors": ["Nicol\u00e1s Villagr\u00e1n Prieto", "Eduardo C. Garrido-Merch\u00e1n"], "title": "Default Machine Learning Hyperparameters Do Not Provide Informative Initialization for Bayesian Optimization", "comment": null, "summary": "Bayesian Optimization (BO) is a standard tool for hyperparameter tuning thanks to its sample efficiency on expensive black-box functions. While most BO pipelines begin with uniform random initialization, default hyperparameter values shipped with popular ML libraries such as scikit-learn encode implicit expert knowledge and could serve as informative starting points that accelerate convergence. This hypothesis, despite its intuitive appeal, has remained largely unexamined. We formalize the idea by initializing BO with points drawn from truncated Gaussian distributions centered at library defaults and compare the resulting trajectories against a uniform-random baseline. We conduct an extensive empirical evaluation spanning three BO back-ends (BoTorch, Optuna, Scikit-Optimize), three model families (Random Forests, Support Vector Machines, Multilayer Perceptrons), and five benchmark datasets covering classification and regression tasks. Performance is assessed through convergence speed and final predictive quality, and statistical significance is determined via one-sided binomial tests. Across all conditions, default-informed initialization yields no statistically significant advantage over purely random sampling, with p-values ranging from 0.141 to 0.908. A sensitivity analysis on the prior variance confirms that, while tighter concentration around the defaults improves early evaluations, this transient benefit vanishes as optimization progresses, leaving final performance unchanged. Our results provide no evidence that default hyperparameters encode useful directional information for optimization. We therefore recommend that practitioners treat hyperparameter tuning as an integral part of model development and favor principled, data-driven search strategies over heuristic reliance on library defaults.", "AI": {"tldr": "\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\u4f7f\u7528\u5e93\u9ed8\u8ba4\u8d85\u53c2\u6570\u4f5c\u4e3a\u521d\u59cb\u5316\u70b9\u5e76\u4e0d\u80fd\u663e\u8457\u63d0\u5347\u4f18\u5316\u6548\u679c\uff0c\u4e0e\u968f\u673a\u521d\u59cb\u5316\u76f8\u6bd4\u65e0\u7edf\u8ba1\u663e\u8457\u4f18\u52bf", "motivation": "\u7814\u7a76\u662f\u5426\u53ef\u4ee5\u5229\u7528\u673a\u5668\u5b66\u4e60\u5e93\uff08\u5982scikit-learn\uff09\u4e2d\u9ed8\u8ba4\u8d85\u53c2\u6570\u503c\u6240\u9690\u542b\u7684\u4e13\u5bb6\u77e5\u8bc6\u6765\u52a0\u901f\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u6536\u655b\uff0c\u8fd9\u4e00\u76f4\u89c2\u5047\u8bbe\u5c1a\u672a\u5f97\u5230\u5145\u5206\u9a8c\u8bc1", "method": "\u4f7f\u7528\u4ee5\u5e93\u9ed8\u8ba4\u503c\u4e3a\u4e2d\u5fc3\u7684\u9ad8\u65af\u5206\u5e03\u91c7\u6837\u521d\u59cb\u5316BO\uff0c\u4e0e\u5747\u5300\u968f\u673a\u521d\u59cb\u5316\u5bf9\u6bd4\uff1b\u5728\u4e09\u4e2aBO\u540e\u7aef\u3001\u4e09\u4e2a\u6a21\u578b\u5bb6\u65cf\u548c\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff1b\u901a\u8fc7\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u9884\u6d4b\u8d28\u91cf\u8bc4\u4f30\u6027\u80fd\uff0c\u4f7f\u7528\u5355\u8fb9\u4e8c\u9879\u68c0\u9a8c\u786e\u5b9a\u7edf\u8ba1\u663e\u8457\u6027", "result": "\u5728\u6240\u6709\u5b9e\u9a8c\u6761\u4ef6\u4e0b\uff0c\u9ed8\u8ba4\u503c\u521d\u59cb\u5316\u76f8\u6bd4\u7eaf\u968f\u673a\u91c7\u6837\u6ca1\u6709\u7edf\u8ba1\u663e\u8457\u4f18\u52bf\uff08p\u503c\u8303\u56f40.141-0.908\uff09\uff1b\u867d\u7136\u66f4\u7d27\u5bc6\u56f4\u7ed5\u9ed8\u8ba4\u503c\u7684\u5148\u9a8c\u65b9\u5dee\u80fd\u6539\u5584\u65e9\u671f\u8bc4\u4f30\uff0c\u4f46\u8fd9\u79cd\u6682\u65f6\u4f18\u52bf\u968f\u4f18\u5316\u8fdb\u5c55\u800c\u6d88\u5931\uff0c\u6700\u7ec8\u6027\u80fd\u4e0d\u53d8", "conclusion": "\u9ed8\u8ba4\u8d85\u53c2\u6570\u5e76\u672a\u5305\u542b\u5bf9\u4f18\u5316\u6709\u7528\u7684\u65b9\u5411\u6027\u4fe1\u606f\uff1b\u5efa\u8bae\u5c06\u8d85\u53c2\u6570\u8c03\u4f18\u4f5c\u4e3a\u6a21\u578b\u5f00\u53d1\u7684\u5fc5\u8981\u90e8\u5206\uff0c\u91c7\u7528\u57fa\u4e8e\u6570\u636e\u7684\u641c\u7d22\u7b56\u7565\u800c\u975e\u4f9d\u8d56\u5e93\u9ed8\u8ba4\u503c\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5"}}
{"id": "2602.08785", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08785", "abs": "https://arxiv.org/abs/2602.08785", "authors": ["Ofek Amran", "Tom Gilat", "Ron Levie"], "title": "A Graphop Analysis of Graph Neural Networks on Sparse Graphs: Generalization and Universal Approximation", "comment": null, "summary": "Generalization and approximation capabilities of message passing graph neural networks (MPNNs) are often studied by defining a compact metric on a space of input graphs under which MPNNs are H\u00f6lder continuous. Such analyses are of two varieties: 1) when the metric space includes graphs of unbounded sizes, the theory is only appropriate for dense graphs, and, 2) when studying sparse graphs, the metric space only includes graphs of uniformly bounded size. In this work, we present a unified approach, defining a compact metric on the space of graphs of all sizes, both sparse and dense, under which MPNNs are H\u00f6lder continuous. This leads to more powerful universal approximation theorems and generalization bounds than previous works. The theory is based on, and extends, a recent approach to graph limit theory called graphop analysis.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u5ea6\u91cf\u7a7a\u95f4\u6846\u67b6\uff0c\u5c06\u4efb\u610f\u5927\u5c0f\u7684\u7a00\u758f\u548c\u7a20\u5bc6\u56fe\u7eb3\u5165\u540c\u4e00\u7406\u8bba\u4f53\u7cfb\uff0c\u6269\u5c55MPNN\u7684\u6cdb\u5316\u4e0e\u903c\u8fd1\u80fd\u529b\u5206\u6790", "motivation": "\u73b0\u6709MPNN\u7406\u8bba\u5b58\u5728\u5c40\u9650\u6027\uff1a\u5206\u6790\u65e0\u754c\u5927\u5c0f\u56fe\u65f6\u4ec5\u9002\u7528\u4e8e\u7a20\u5bc6\u56fe\uff0c\u800c\u5206\u6790\u7a00\u758f\u56fe\u65f6\u53c8\u9650\u5236\u5728\u56fa\u5b9a\u5927\u5c0f\u56fe\u96c6\u3002\u9700\u8981\u7edf\u4e00\u6846\u67b6\u540c\u65f6\u5904\u7406\u4efb\u610f\u5927\u5c0f\u7684\u7a00\u758f\u548c\u7a20\u5bc6\u56fe", "method": "\u57fa\u4e8e\u56fe\u7b97\u5b50(graphop)\u5206\u6790\u7406\u8bba\uff0c\u5b9a\u4e49\u5305\u542b\u6240\u6709\u5927\u5c0f\u56fe\u7684\u7d27\u81f4\u5ea6\u91cf\u7a7a\u95f4\uff0c\u5728\u8be5\u5ea6\u91cf\u4e0b\u8bc1\u660eMPNN\u7684H\u00f6lder\u8fde\u7eed\u6027", "result": "\u83b7\u5f97\u6bd4\u5148\u524d\u5de5\u4f5c\u66f4\u5f3a\u5927\u7684\u901a\u7528\u903c\u8fd1\u5b9a\u7406\u548c\u6cdb\u5316\u8fb9\u754c\uff0c\u7edf\u4e00\u5904\u7406\u7a00\u758f\u548c\u7a20\u5bc6\u56fe\u7684\u7406\u8bba\u6846\u67b6", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u5ea6\u91cf\u7a7a\u95f4\u6846\u67b6\u514b\u670d\u4e86\u73b0\u6709MPNN\u7406\u8bba\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u4efb\u610f\u5927\u5c0f\u56fe\u7684MPNN\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u7406\u8bba\u57fa\u7840"}}
{"id": "2602.08808", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08808", "abs": "https://arxiv.org/abs/2602.08808", "authors": ["Yapei Chang", "Kyle Lo", "Mohit Iyyer", "Luca Soldaini"], "title": "How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs", "comment": "53 pages, 22 figures", "summary": "Generating step-by-step \"how-to\" procedures is a key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything, a scalable framework to evaluate and improve goal-conditioned procedure generation. Our framework includes How2Mine, which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench, a 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score, an evaluation protocol that uses an LLM judge to detect whether a generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill a frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining. Finally, RL using How2Score as a reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support a closed loop of capability evaluation and improvement at scale.", "AI": {"tldr": "How2Everything\u6846\u67b6\uff1a\u4ece\u7f51\u9875\u6316\u639835\u4e07\u6761\u6d41\u7a0b\u6570\u636e\uff0c\u6784\u5efa7\u5343\u6761\u8bc4\u4f30\u96c6\uff0c\u5f00\u53d1\u57fa\u4e8eLLM\u7684\u8bc4\u5206\u534f\u8bae\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u76ee\u6807\u5bfc\u5411\u7684\u6d41\u7a0b\u751f\u6210\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u751f\u6210\u9010\u6b65\"\u5982\u4f55\u505a\"\u6d41\u7a0b\u662fLLM\u7684\u5173\u952e\u80fd\u529b\uff0c\u4f46\u5728\u771f\u5b9e\u4efb\u52a1\u4e2d\u5927\u89c4\u6a21\u8bc4\u4f30\u548c\u6539\u8fdb\u7a0b\u5e8f\u6709\u6548\u6027\u4ecd\u5177\u6311\u6218\u4e14\u7814\u7a76\u4e0d\u8db3\u3002\u9700\u8981\u53ef\u6269\u5c55\u7684\u6846\u67b6\u6765\u8bc4\u4f30\u548c\u6539\u8fdb\u76ee\u6807\u5bfc\u5411\u7684\u6d41\u7a0b\u751f\u6210\u3002", "method": "1) How2Mine\uff1a\u4ece98\u4e07\u7f51\u9875\u6316\u639835.1\u4e07\u6761\u6d41\u7a0b\u6570\u636e\uff1b2) How2Bench\uff1a\u6784\u5efa7\u5343\u6761\u5e73\u8861\u8bc4\u4f30\u96c6\uff1b3) How2Score\uff1a\u5f00\u53d1\u57fa\u4e8eLLM\u7684\u8bc4\u5206\u534f\u8bae\uff0c\u68c0\u6d4b\u751f\u6210\u4e2d\u662f\u5426\u5305\u542b\u963b\u788d\u76ee\u6807\u5b9e\u73b0\u7684\u5173\u952e\u5931\u8d25\uff1b4) \u5c06\u524d\u6cbf\u6a21\u578b\u84b8\u998f\u4e3a8B\u5f00\u6e90\u6a21\u578b\u7528\u4e8e\u4f4e\u6210\u672c\u8bc4\u4f30\uff1b5) \u4f7f\u7528How2Score\u4f5c\u4e3a\u5956\u52b1\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u3002", "result": "1) \u84b8\u998f\u6a21\u578b\u4e0e\u4eba\u7c7b\u6807\u6ce8\u8005\u8fbe\u523080.5%\u4e00\u81f4\u6027\uff1b2) How2Bench\u663e\u793a\u6a21\u578b\u89c4\u6a21\u548c\u8bad\u7ec3\u9636\u6bb5\u7684\u660e\u663e\u6269\u5c55\u8d8b\u52bf\uff1b3) \u5f3a\u5316\u5b66\u4e60\u4f7f\u4e09\u4e2a\u6a21\u578b\u5728How2Bench\u4e0a\u63d0\u5347>10\u5206\uff0c\u4e14\u4e0d\u5f71\u54cd\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u6027\u80fd\uff1b4) \u6539\u8fdb\u5bf9\u8868\u9762\u8bb0\u5fc6\u6216\u683c\u5f0f\u5408\u89c4\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "How2Everything\u5c55\u793a\u4e86\u9884\u8bad\u7ec3\u7f51\u7edc\u6570\u636e\u5982\u4f55\u652f\u6301\u5927\u89c4\u6a21\u80fd\u529b\u8bc4\u4f30\u548c\u6539\u8fdb\u7684\u95ed\u73af\uff0c\u4e3a\u6d41\u7a0b\u751f\u6210\u80fd\u529b\u7684\u7cfb\u7edf\u8bc4\u4f30\u548c\u63d0\u5347\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\u3002"}}
{"id": "2602.08809", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08809", "abs": "https://arxiv.org/abs/2602.08809", "authors": ["Karim Haroun", "Aya Zitouni", "Aicha Zenakhri", "Meriem Amel Guessoum", "Larbi Boubchir"], "title": "Efficient Deep Learning for Biometrics: Overview, Challenges and Trends in Ear of Frugal AI", "comment": "8 pages, 2 figures, accepted at the 2025 IEEE SDS conference", "summary": "Recent advances in deep learning, whether on discriminative or generative tasks have been beneficial for various applications, among which security and defense. However, their increasing computational demands during training and deployment translates directly into high energy consumption. As a consequence, this induces a heavy carbon footprint which hinders their widespread use and scalability, but also a limitation when deployed on resource-constrained edge devices for real-time use. In this paper, we briefly survey efficient deep learning methods for biometric applications. Specifically, we tackle the challenges one might incur when training and deploying deep learning approaches, and provide a taxonomy of the various efficient deep learning families. Additionally, we discuss complementary metrics for evaluating the efficiency of these models such as memory, computation, latency, throughput, and advocate for universal and reproducible metrics for better comparison. Last, we give future research directions to consider.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7b80\u8981\u7efc\u8ff0\u4e86\u751f\u7269\u8bc6\u522b\u5e94\u7528\u4e2d\u9ad8\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u8ba8\u8bba\u4e86\u8bad\u7ec3\u548c\u90e8\u7f72\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u6548\u7387\u5206\u7c7b\uff0c\u5e76\u5021\u5bfc\u4f7f\u7528\u7edf\u4e00\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u5b89\u5168\u9632\u5fa1\u7b49\u5e94\u7528\u4e2d\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5176\u8bad\u7ec3\u548c\u90e8\u7f72\u7684\u9ad8\u8ba1\u7b97\u9700\u6c42\u5bfc\u81f4\u9ad8\u80fd\u8017\u548c\u78b3\u8db3\u8ff9\uff0c\u9650\u5236\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u9ad8\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5bf9\u751f\u7269\u8bc6\u522b\u5e94\u7528\u4e2d\u7684\u9ad8\u6548\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u8fdb\u884c\u7cfb\u7edf\u68b3\u7406\uff0c\u63d0\u51fa\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u8ba8\u8bba\u5305\u62ec\u5185\u5b58\u3001\u8ba1\u7b97\u3001\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\u5728\u5185\u7684\u8865\u5145\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5efa\u7acb\u4e86\u9ad8\u6548\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u8bc6\u522b\u4e86\u8bad\u7ec3\u548c\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u7684\u4e3b\u8981\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\u6846\u67b6\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u4e86\u5efa\u8bae\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4ee5\u964d\u4f4e\u80fd\u8017\u548c\u78b3\u8db3\u8ff9\uff0c\u540c\u65f6\u9700\u8981\u5efa\u7acb\u7edf\u4e00\u3001\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u6807\u51c6\u6765\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u53d1\u5c55\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u751f\u7269\u8bc6\u522b\u5e94\u7528\u4e2d\u3002"}}
{"id": "2602.08810", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08810", "abs": "https://arxiv.org/abs/2602.08810", "authors": ["Karan Bania", "Soham Kalburgi", "Manit Tanwar", "Dhruthi", "Aditya Nagarsekar", "Harshvardhan Mestha", "Naman Chibber", "Raj Deshmukh", "Anish Sathyanarayanan", "Aarush Rathore", "Pratham Chheda"], "title": "$\\texttt{lrnnx}$: A library for Linear RNNs", "comment": "EACL Student Research Workshop 2026", "summary": "Linear recurrent neural networks (LRNNs) provide a structured approach to sequence modeling that bridges classical linear dynamical systems and modern deep learning, offering both expressive power and theoretical guarantees on stability and trainability. In recent years, multiple LRNN-based architectures have been proposed, each introducing distinct parameterizations, discretization schemes, and implementation constraints. However, existing implementations are fragmented across different software frameworks, often rely on framework-specific optimizations, and in some cases require custom CUDA kernels or lack publicly available code altogether. As a result, using, comparing, or extending LRNNs requires substantial implementation effort. To address this, we introduce $\\texttt{lrnnx}$, a unified software library that implements several modern LRNN architectures under a common interface. The library exposes multiple levels of control, allowing users to work directly with core components or higher-level model abstractions. $\\texttt{lrnnx}$ aims to improve accessibility, reproducibility, and extensibility of LRNN research and applications. We make our code available under a permissive MIT license.", "AI": {"tldr": "lrnnx\u662f\u4e00\u4e2a\u7edf\u4e00\u8f6f\u4ef6\u5e93\uff0c\u5b9e\u73b0\u4e86\u591a\u79cd\u73b0\u4ee3\u7ebf\u6027\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u63d0\u4f9b\u901a\u7528\u63a5\u53e3\u4ee5\u89e3\u51b3\u73b0\u6709\u5b9e\u73b0\u788e\u7247\u5316\u95ee\u9898", "motivation": "\u73b0\u6709\u7ebf\u6027\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u5206\u6563\u5728\u4e0d\u540c\u8f6f\u4ef6\u6846\u67b6\u4e2d\uff0c\u4f9d\u8d56\u6846\u67b6\u7279\u5b9a\u4f18\u5316\uff0c\u6709\u4e9b\u9700\u8981\u81ea\u5b9a\u4e49CUDA\u5185\u6838\u6216\u7f3a\u4e4f\u516c\u5f00\u4ee3\u7801\uff0c\u5bfc\u81f4\u4f7f\u7528\u3001\u6bd4\u8f83\u6216\u6269\u5c55LRNN\u9700\u8981\u5927\u91cf\u5b9e\u73b0\u5de5\u4f5c", "method": "\u5f00\u53d1lrnnx\u7edf\u4e00\u8f6f\u4ef6\u5e93\uff0c\u5b9e\u73b0\u591a\u79cd\u73b0\u4ee3LRNN\u67b6\u6784\uff0c\u63d0\u4f9b\u901a\u7528\u63a5\u53e3\uff0c\u66b4\u9732\u591a\u4e2a\u63a7\u5236\u7ea7\u522b\uff0c\u5141\u8bb8\u7528\u6237\u76f4\u63a5\u4f7f\u7528\u6838\u5fc3\u7ec4\u4ef6\u6216\u9ad8\u5c42\u6a21\u578b\u62bd\u8c61", "result": "\u521b\u5efa\u4e86\u5f00\u6e90\u8f6f\u4ef6\u5e93lrnnx\uff0c\u91c7\u7528MIT\u8bb8\u53ef\uff0c\u63d0\u9ad8\u4e86LRNN\u7814\u7a76\u7684\u53ef\u8bbf\u95ee\u6027\u3001\u53ef\u590d\u73b0\u6027\u548c\u53ef\u6269\u5c55\u6027", "conclusion": "lrnnx\u89e3\u51b3\u4e86LRNN\u5b9e\u73b0\u788e\u7247\u5316\u95ee\u9898\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u5e94\u7528\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u5de5\u5177\uff0c\u4fc3\u8fdb\u4e86\u7ebf\u6027\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u9886\u57df\u7684\u53d1\u5c55"}}
{"id": "2602.08813", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08813", "abs": "https://arxiv.org/abs/2602.08813", "authors": ["Mahdi Sabbaghi", "George Pappas", "Adel Javanmard", "Hamed Hassani"], "title": "Robust Policy Optimization to Prevent Catastrophic Forgetting", "comment": null, "summary": "Large language models are commonly trained through multi-stage post-training: first via RLHF, then fine-tuned for other downstream objectives. Yet even small downstream updates can compromise earlier learned behaviors (e.g., safety), exposing a brittleness known as catastrophic forgetting. This suggests standard RLHF objectives do not guarantee robustness to future adaptation. To address it, most prior work designs downstream-time methods to preserve previously learned behaviors. We argue that preventing this requires pre-finetuning robustness: the base policy should avoid brittle high-reward solutions whose reward drops sharply under standard fine-tuning.\n  We propose Fine-tuning Robust Policy Optimization (FRPO), a robust RLHF framework that optimizes reward not only at the current policy, but across a KL-bounded neighborhood of policies reachable by downstream adaptation. The key idea is to ensure reward stability under policy shifts via a max-min formulation. By modifying GRPO, we develop an algorithm with no extra computation, and empirically show it substantially reduces safety degradation across multiple base models and downstream fine-tuning regimes (SFT and RL) while preserving downstream task performance. We further study a math-focused RL setting, demonstrating that FRPO preserves accuracy under subsequent fine-tuning.", "AI": {"tldr": "FRPO\u662f\u4e00\u79cd\u9c81\u68d2\u7684RLHF\u6846\u67b6\uff0c\u901a\u8fc7\u5728KL\u6709\u754c\u90bb\u57df\u5185\u4f18\u5316\u5956\u52b1\u6765\u9632\u6b62\u4e0b\u6e38\u5fae\u8c03\u65f6\u7684\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4fdd\u6301\u5b89\u5168\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RLHF\u8bad\u7ec3\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u540e\u7eed\u5fae\u8c03\u65f6\u5bb9\u6613\u51fa\u73b0\u707e\u96be\u6027\u9057\u5fd8\uff0c\u7279\u522b\u662f\u5b89\u5168\u6027\u7b49\u5df2\u5b66\u4e60\u884c\u4e3a\u4f1a\u9000\u5316\u3002\u6807\u51c6RLHF\u76ee\u6807\u65e0\u6cd5\u4fdd\u8bc1\u5bf9\u672a\u6765\u9002\u5e94\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faFine-tuning Robust Policy Optimization (FRPO)\uff0c\u901a\u8fc7max-min\u516c\u5f0f\u5728KL\u6709\u754c\u90bb\u57df\u5185\u4f18\u5316\u5956\u52b1\uff0c\u786e\u4fdd\u7b56\u7565\u5728\u6807\u51c6\u5fae\u8c03\u4e0b\u7684\u5956\u52b1\u7a33\u5b9a\u6027\u3002\u57fa\u4e8eGRPO\u4fee\u6539\uff0c\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u3002", "result": "FRPO\u663e\u8457\u51cf\u5c11\u4e86\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u5728\u4e0d\u540c\u4e0b\u6e38\u5fae\u8c03\u673a\u5236\uff08SFT\u548cRL\uff09\u4e0b\u7684\u5b89\u5168\u6027\u9000\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002\u5728\u6570\u5b66RL\u8bbe\u7f6e\u4e2d\uff0cFRPO\u4e5f\u80fd\u5728\u540e\u7eed\u5fae\u8c03\u4e0b\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "FRPO\u901a\u8fc7\u9884\u5fae\u8c03\u9c81\u68d2\u6027\u4f18\u5316\uff0c\u89e3\u51b3\u4e86RLHF\u6a21\u578b\u5728\u4e0b\u6e38\u5fae\u8c03\u65f6\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2602.08816", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.08816", "abs": "https://arxiv.org/abs/2602.08816", "authors": ["James Jewitt", "Gopi Krishnan Rajbahadur", "Hao Li", "Bram Adams", "Ahmed E. Hassan"], "title": "Permissive-Washing in the Open AI Supply Chain: A Large-Scale Audit of License Integrity", "comment": "13 pages, 2 figures, 10 tables", "summary": "Permissive licenses like MIT, Apache-2.0, and BSD-3-Clause dominate open-source AI, signaling that artifacts like models, datasets, and code can be freely used, modified, and redistributed. However, these licenses carry mandatory requirements: include the full license text, provide a copyright notice, and preserve upstream attribution, that remain unverified at scale. Failure to meet these conditions can place reuse outside the scope of the license, effectively leaving AI artifacts under default copyright for those uses and exposing downstream users to litigation. We call this phenomenon ``permissive washing'': labeling AI artifacts as free to use, while omitting the legal documentation required to make that label actionable. To assess how widespread permissive washing is in the AI supply chain, we empirically audit 124,278 dataset $\\rightarrow$ model $\\rightarrow$ application supply chains, spanning 3,338 datasets, 6,664 models, and 28,516 applications across Hugging Face and GitHub. We find that an astonishing 96.5\\% of datasets and 95.8\\% of models lack the required license text, only 2.3\\% of datasets and 3.2\\% of models satisfy both license text and copyright requirements, and even when upstream artifacts provide complete licensing evidence, attribution rarely propagates downstream: only 27.59\\% of models preserve compliant dataset notices and only 5.75\\% of applications preserve compliant model notices (with just 6.38\\% preserving any linked upstream notice). Practitioners cannot assume permissive labels confer the rights they claim: license files and notices, not metadata, are the source of legal truth. To support future research, we release our full audit dataset and reproducible pipeline.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5f00\u6e90AI\u9886\u57df\u5b58\u5728\u4e25\u91cd\u7684\"\u8bb8\u53ef\u6e05\u6d17\"\u73b0\u8c61\uff1a96.5%\u7684\u6570\u636e\u96c6\u548c95.8%\u7684\u6a21\u578b\u7f3a\u5c11\u5fc5\u8981\u7684\u8bb8\u53ef\u6587\u672c\uff0c\u53ea\u6709\u6781\u5c11\u6570\u6ee1\u8db3\u5b8c\u6574\u7684\u8bb8\u53ef\u8981\u6c42\uff0c\u5bfc\u81f4\u6240\u8c13\u7684\"\u81ea\u7531\u4f7f\u7528\"\u6807\u7b7e\u5b9e\u9645\u4e0a\u4e0d\u53ef\u64cd\u4f5c\u3002", "motivation": "\u5f00\u6e90AI\u9879\u76ee\u867d\u7136\u4f7f\u7528MIT\u3001Apache-2.0\u7b49\u5bbd\u677e\u8bb8\u53ef\u8bc1\uff0c\u4f46\u8fd9\u4e9b\u8bb8\u53ef\u8bc1\u5305\u542b\u5fc5\u987b\u6ee1\u8db3\u7684\u5f3a\u5236\u6027\u8981\u6c42\uff08\u5982\u5305\u542b\u5b8c\u6574\u8bb8\u53ef\u6587\u672c\u3001\u7248\u6743\u58f0\u660e\u7b49\uff09\u3002\u5f53\u524d\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u8981\u6c42\u7684\u5927\u89c4\u6a21\u9a8c\u8bc1\uff0c\u5bfc\u81f4\u7528\u6237\u53ef\u80fd\u5728\u4e0d\u6ee1\u8db3\u6761\u4ef6\u7684\u60c5\u51b5\u4e0b\u4f7f\u7528AI\u8d44\u4ea7\uff0c\u4ece\u800c\u9762\u4e34\u6cd5\u5f8b\u98ce\u9669\u3002", "method": "\u5bf9124,278\u4e2a\u6570\u636e\u96c6\u2192\u6a21\u578b\u2192\u5e94\u7528\u7a0b\u5e8f\u4f9b\u5e94\u94fe\u8fdb\u884c\u5b9e\u8bc1\u5ba1\u8ba1\uff0c\u6db5\u76d6Hugging Face\u548cGitHub\u4e0a\u76843,338\u4e2a\u6570\u636e\u96c6\u30016,664\u4e2a\u6a21\u578b\u548c28,516\u4e2a\u5e94\u7528\u7a0b\u5e8f\u3002\u68c0\u67e5\u8bb8\u53ef\u6587\u672c\u3001\u7248\u6743\u58f0\u660e\u548c\u4e0a\u6e38\u5f52\u5c5e\u7684\u4f20\u64ad\u60c5\u51b5\u3002", "result": "\u60ca\u4eba\u7684\u53d1\u73b0\uff1a96.5%\u7684\u6570\u636e\u96c6\u548c95.8%\u7684\u6a21\u578b\u7f3a\u5c11\u6240\u9700\u8bb8\u53ef\u6587\u672c\uff1b\u53ea\u67092.3%\u7684\u6570\u636e\u96c6\u548c3.2%\u7684\u6a21\u578b\u540c\u65f6\u6ee1\u8db3\u8bb8\u53ef\u6587\u672c\u548c\u7248\u6743\u8981\u6c42\uff1b\u5373\u4f7f\u4e0a\u6e38\u8d44\u4ea7\u63d0\u4f9b\u5b8c\u6574\u8bb8\u53ef\u8bc1\u636e\uff0c\u5f52\u5c5e\u4e5f\u5f88\u5c11\u5411\u4e0b\u6e38\u4f20\u64ad\uff1a\u53ea\u670927.59%\u7684\u6a21\u578b\u4fdd\u7559\u5408\u89c4\u7684\u6570\u636e\u96c6\u58f0\u660e\uff0c\u4ec55.75%\u7684\u5e94\u7528\u7a0b\u5e8f\u4fdd\u7559\u5408\u89c4\u7684\u6a21\u578b\u58f0\u660e\u3002", "conclusion": "\u4ece\u4e1a\u8005\u4e0d\u80fd\u5047\u8bbe\u5bbd\u677e\u8bb8\u53ef\u8bc1\u6807\u7b7e\u80fd\u63d0\u4f9b\u5176\u58f0\u79f0\u7684\u6743\u5229\uff1a\u8bb8\u53ef\u6587\u4ef6\u548c\u58f0\u660e\uff08\u800c\u975e\u5143\u6570\u636e\uff09\u624d\u662f\u6cd5\u5f8b\u771f\u76f8\u7684\u6765\u6e90\u3002\u5f00\u6e90AI\u9886\u57df\u5b58\u5728\u666e\u904d\u7684\"\u8bb8\u53ef\u6e05\u6d17\"\u95ee\u9898\uff0c\u9700\u8981\u66f4\u4e25\u683c\u7684\u5408\u89c4\u5b9e\u8df5\u3002"}}
{"id": "2602.08817", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08817", "abs": "https://arxiv.org/abs/2602.08817", "authors": ["Chenyu Wang", "Zhanglu Yan", "Zhi Zhou", "Xu Chen", "Weng-Fai Wong"], "title": "Kirin: Improving ANN efficiency with SNN Hybridization", "comment": null, "summary": "Artificial neural networks (ANNs), particularly large language models (LLMs), demonstrate powerful inference capabilities but consume substantial energy. Conversely, spiking neural networks (SNNs) exhibit exceptional energy efficiency due to their binary and event-driven characteristics, thus motivating the study of ANN-to-SNN conversion. In this process, quantization plays a pivotal role, mapping LLMs' floating-point parameters to discrete SNN parameters via the temporal dimension of the time window. However, several challenges remain in the conversion process: (i) converting high bit-width quantization values into binary spikes requires longer time windows, increasing system latency; and (ii) the inherent trade-off between the information loss of single-spike schemes and the energy costs of multi-spike ones in SNN. To address these challenges, we propose Kirin, a integer and spike hybrid based SNN to achieve accuracy lossless ANN-to-SNN conversion with time and energy efficiency. Specifically, we first propose a Spike Matrix Hybridization strategy that encoding low bit-width parameters that leading to small time window size into binary spikes while preserving the rest in integer format, thereby reducing the overall latency of SNN execution. Second, we introduce a silence threshold mechanism to regulate the timing of single-spike firing, ensuring the output is mathematically equivalent to the LLM's output and preserves accuracy. Experimental results demonstrate that Kirin, under a W4A4\\&8 quantization setting, achieves near-FP16 accuracy while reducing energy consumption by up to 84.66\\% and shortening time steps by 93.75\\%.", "AI": {"tldr": "Kirin\u63d0\u51fa\u4e86\u4e00\u79cd\u6574\u6570\u548c\u8109\u51b2\u6df7\u5408\u7684SNN\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u7387\u65e0\u635f\u7684ANN\u5230SNN\u8f6c\u6362\uff0c\u5728W4A4&8\u91cf\u5316\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u63a5\u8fd1FP16\u7cbe\u5ea6\uff0c\u80fd\u8017\u964d\u4f4e84.66%\uff0c\u65f6\u95f4\u6b65\u7f29\u77ed93.75%\u3002", "motivation": "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08\u7279\u522b\u662f\u5927\u8bed\u8a00\u6a21\u578b\uff09\u63a8\u7406\u80fd\u529b\u5f3a\u4f46\u80fd\u8017\u9ad8\uff0c\u800c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5177\u6709\u51fa\u8272\u7684\u80fd\u6548\u4f46\u5b58\u5728\u8f6c\u6362\u6311\u6218\uff1a\u9ad8\u6bd4\u7279\u91cf\u5316\u503c\u9700\u8981\u66f4\u957f\u65f6\u95f4\u7a97\u53e3\u589e\u52a0\u5ef6\u8fdf\uff0c\u4ee5\u53ca\u5355\u8109\u51b2\u65b9\u6848\u4fe1\u606f\u635f\u5931\u4e0e\u591a\u8109\u51b2\u65b9\u6848\u80fd\u8017\u4e4b\u95f4\u7684\u6743\u8861\u3002", "method": "\u63d0\u51faKirin\u6df7\u5408SNN\u67b6\u6784\uff1a1\uff09\u8109\u51b2\u77e9\u9635\u6df7\u5408\u7b56\u7565\uff0c\u5c06\u5bfc\u81f4\u5c0f\u65f6\u95f4\u7a97\u53e3\u7684\u4f4e\u6bd4\u7279\u53c2\u6570\u7f16\u7801\u4e3a\u4e8c\u8fdb\u5236\u8109\u51b2\uff0c\u5176\u4f59\u4fdd\u7559\u4e3a\u6574\u6570\u683c\u5f0f\uff1b2\uff09\u9759\u9ed8\u9608\u503c\u673a\u5236\u8c03\u8282\u5355\u8109\u51b2\u53d1\u5c04\u65f6\u673a\uff0c\u786e\u4fdd\u8f93\u51fa\u4e0eLLM\u6570\u5b66\u7b49\u4ef7\u3002", "result": "\u5728W4A4&8\u91cf\u5316\u8bbe\u7f6e\u4e0b\uff0cKirin\u8fbe\u5230\u63a5\u8fd1FP16\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u80fd\u8017\u964d\u4f4e84.66%\uff0c\u65f6\u95f4\u6b65\u7f29\u77ed93.75%\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u7387\u65e0\u635f\u7684\u9ad8\u6548ANN\u5230SNN\u8f6c\u6362\u3002", "conclusion": "Kirin\u901a\u8fc7\u6574\u6570\u548c\u8109\u51b2\u6df7\u5408\u8bbe\u8ba1\u6210\u529f\u89e3\u51b3\u4e86ANN\u5230SNN\u8f6c\u6362\u4e2d\u7684\u5ef6\u8fdf\u548c\u80fd\u6548\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u90e8\u7f72\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684SNN\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08818", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08818", "abs": "https://arxiv.org/abs/2602.08818", "authors": ["Annemette Brok Pirchert", "Jacob Nielsen", "Mogens Henrik From", "Lukas Galke Poech", "Peter Schneider-Kamp"], "title": "FlexMoRE: A Flexible Mixture of Rank-heterogeneous Experts for Efficient Federatedly-trained Large Language Models", "comment": null, "summary": "Recent advances in mixture-of-experts architectures have shown that individual experts models can be trained federatedly, i.e., in isolation from other experts by using a common base model to facilitate coordination. However, we hypothesize that full-sized experts may not be necessary for all domains and that instead low-rank adapters may be sufficient. Here, we introduce FlexMoRE, a Flexible Mixture of Rank-heterogenous Experts, which may be either full-sized experts or adapters of a suitable rank. We systematically investigate the trade-off between expert rank and downstream task performance by evaluating $6$ experts with ranks $2^0$ to $2^{14}$ resulting in experiments covering 150 mixtures (96 with 2 experts, 54 with 7 experts) that are evaluated across $120$ tasks. For our experiments, we build on FlexOlmo and turn its pre-trained experts into low-rank versions. Our regression analysis from expert rank to downstream task performance reveals that the best-performing rank is substantially higher for reasoning-heavy benchmarks than for knowledge-heavy benchmarks. These findings on rank sensitivity come with direct implications for memory efficiency: Using optimal ranks, FlexMoRE yields improved downstream task performance (average score $47.18$) compared to the baseline FlexOlmo-style mixture of full-sized experts (average score $45.46$) at less than one third the parameters ($10.75$B for FlexMoRE vs. $33.27$B for FlexOlmo). All code will be made available.", "AI": {"tldr": "FlexMoRE\u662f\u4e00\u79cd\u7075\u6d3b\u7684\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff0c\u5141\u8bb8\u4f7f\u7528\u4e0d\u540c\u79e9\u7684\u4f4e\u79e9\u9002\u914d\u5668\u6216\u5b8c\u6574\u4e13\u5bb6\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u3002", "motivation": "\u73b0\u6709\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u901a\u5e38\u8bad\u7ec3\u5b8c\u6574\u5927\u5c0f\u7684\u4e13\u5bb6\u6a21\u578b\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u5e76\u975e\u6240\u6709\u9886\u57df\u90fd\u9700\u8981\u5b8c\u6574\u4e13\u5bb6\uff0c\u4f4e\u79e9\u9002\u914d\u5668\u53ef\u80fd\u5c31\u8db3\u591f\u4e86\u3002\u8fd9\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5185\u5b58\u6548\u7387\u3002", "method": "\u63d0\u51faFlexMoRE\uff08\u7075\u6d3b\u6df7\u5408\u79e9\u5f02\u6784\u4e13\u5bb6\uff09\u67b6\u6784\uff0c\u4e13\u5bb6\u53ef\u4ee5\u662f\u5b8c\u6574\u6a21\u578b\u6216\u4e0d\u540c\u79e9\u7684\u4f4e\u79e9\u9002\u914d\u5668\u3002\u57fa\u4e8eFlexOlmo\u6784\u5efa\uff0c\u5c06\u5176\u9884\u8bad\u7ec3\u4e13\u5bb6\u8f6c\u6362\u4e3a\u4f4e\u79e9\u7248\u672c\uff0c\u7cfb\u7edf\u7814\u7a76\u4e13\u5bb6\u79e9\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u6743\u8861\u3002", "result": "\u56de\u5f52\u5206\u6790\u663e\u793a\uff0c\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u9700\u8981\u66f4\u9ad8\u79e9\u7684\u4e13\u5bb6\uff0c\u800c\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u9700\u8981\u8f83\u4f4e\u79e9\u3002\u4f7f\u7528\u6700\u4f18\u79e9\u65f6\uff0cFlexMoRE\u5728\u53c2\u6570\u51cf\u5c11\u4e09\u5206\u4e4b\u4e8c\uff0810.75B vs 33.27B\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u53cd\u800c\u66f4\u597d\uff08\u5e73\u5747\u520647.18 vs 45.46\uff09\u3002", "conclusion": "FlexMoRE\u901a\u8fc7\u7075\u6d3b\u6df7\u5408\u4e0d\u540c\u79e9\u7684\u4e13\u5bb6\uff0c\u5728\u663e\u8457\u51cf\u5c11\u53c2\u6570\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u4f4e\u79e9\u9002\u914d\u5668\u5728\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.08819", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08819", "abs": "https://arxiv.org/abs/2602.08819", "authors": ["Jiwoo Hong", "Shao Tang", "Zhipeng Wang"], "title": "Bayesian Preference Learning for Test-Time Steerable Reward Models", "comment": "Preprint", "summary": "Reward models are central to aligning language models with human preferences via reinforcement learning (RL). As RL is increasingly applied to settings such as verifiable rewards and multi-objective alignment, RMs are expected to encode more complex and multifaceted preference distributions. However, classifier RMs remain static once trained, limiting their adaptability at test time. We propose Variational In-Context Reward Modeling (ICRM), a novel Bayesian reward modeling objective that enables test-time steerability via in-context preference demonstrations. ICRM casts reward modeling as amortized variational inference over a latent preference probability under the Bradley-Terry model using a conjugate Beta prior. We show that ICRM adapt to unseen preference distributions at test time for both single and multi-objective settings. With more in-context demonstrations, ICRM gains 34% accuracy on SafeRLHF and 9% accuracy on RM-Bench in the single-objective setting, while widening the Pareto frontier with a 4% gain in hypervolume on helpfulness and refusal benchmarks. We further study the practical applicability of ICRM for RL training, showing that it can effectively encode verifiable rewards by outperforming a conventional RM in math reasoning. Finally, we provide theoretical guarantees that the variational objective admits a global interior optimum with finite confidence, and we analyze how KL regularization mitigates reward over-optimization.", "AI": {"tldr": "\u63d0\u51faVariational In-Context Reward Modeling (ICRM)\uff0c\u4e00\u79cd\u8d1d\u53f6\u65af\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u504f\u597d\u6f14\u793a\u5b9e\u73b0\u6d4b\u8bd5\u65f6\u7684\u53ef\u8c03\u63a7\u6027\uff0c\u5728\u5355\u76ee\u6807\u548c\u591a\u76ee\u6807\u5bf9\u9f50\u4e2d\u90fd\u80fd\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u504f\u597d\u5206\u5e03\u3002", "motivation": "\u968f\u7740\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u548c\u591a\u76ee\u6807\u5bf9\u9f50\u7b49\u573a\u666f\uff0c\u5956\u52b1\u6a21\u578b\u9700\u8981\u7f16\u7801\u66f4\u590d\u6742\u3001\u591a\u65b9\u9762\u7684\u504f\u597d\u5206\u5e03\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u5206\u7c7b\u5668\u5956\u52b1\u6a21\u578b\u4e00\u65e6\u8bad\u7ec3\u5b8c\u6210\u5c31\u4fdd\u6301\u9759\u6001\uff0c\u9650\u5236\u4e86\u5176\u5728\u6d4b\u8bd5\u65f6\u7684\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faICRM\u65b9\u6cd5\uff0c\u5c06\u5956\u52b1\u5efa\u6a21\u89c6\u4e3a\u5728Bradley-Terry\u6a21\u578b\u4e0b\u4f7f\u7528\u5171\u8f6dBeta\u5148\u9a8c\u5bf9\u6f5c\u5728\u504f\u597d\u6982\u7387\u8fdb\u884c\u644a\u9500\u53d8\u5206\u63a8\u65ad\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e0a\u4e0b\u6587\u504f\u597d\u6f14\u793a\u5b9e\u73b0\u6d4b\u8bd5\u65f6\u7684\u53ef\u8c03\u63a7\u6027\u3002", "result": "ICRM\u5728\u5355\u76ee\u6807\u8bbe\u7f6e\u4e2d\uff0c\u968f\u7740\u66f4\u591a\u4e0a\u4e0b\u6587\u6f14\u793a\uff0c\u5728SafeRLHF\u4e0a\u83b7\u5f9734%\u51c6\u786e\u7387\u63d0\u5347\uff0c\u5728RM-Bench\u4e0a\u83b7\u5f979%\u51c6\u786e\u7387\u63d0\u5347\uff1b\u5728\u591a\u76ee\u6807\u8bbe\u7f6e\u4e2d\uff0c\u5728\u6709\u7528\u6027\u548c\u62d2\u7edd\u57fa\u51c6\u4e0a\u83b7\u5f974%\u8d85\u4f53\u79ef\u589e\u76ca\uff0c\u6269\u5c55\u4e86\u5e15\u7d2f\u6258\u524d\u6cbf\u3002\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u6709\u6548\u7f16\u7801\u53ef\u9a8c\u8bc1\u5956\u52b1\uff0c\u4f18\u4e8e\u4f20\u7edf\u5956\u52b1\u6a21\u578b\u3002", "conclusion": "ICRM\u901a\u8fc7\u4e0a\u4e0b\u6587\u504f\u597d\u6f14\u793a\u5b9e\u73b0\u4e86\u5956\u52b1\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u7684\u53ef\u8c03\u63a7\u6027\uff0c\u80fd\u591f\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u504f\u597d\u5206\u5e03\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\u53d8\u5206\u76ee\u6807\u5177\u6709\u5168\u5c40\u5185\u90e8\u6700\u4f18\u89e3\uff0cKL\u6b63\u5219\u5316\u80fd\u7f13\u89e3\u5956\u52b1\u8fc7\u5ea6\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2602.08847", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08847", "abs": "https://arxiv.org/abs/2602.08847", "authors": ["Lang Feng", "Longtao Zheng", "Shuo He", "Fuxiang Zhang", "Bo An"], "title": "Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems", "comment": "Preprint", "summary": "Multi-agent LLM systems enable advanced reasoning and tool use via role specialization, yet reliable reinforcement learning (RL) post-training for such systems remains difficult. In this work, we theoretically pinpoint a key reason for training instability when extending group-based RL to multi-agent LLM systems. We show that under GRPO-style optimization, a global normalization baseline may deviate from diverse agents' reward distributions, which ultimately leads to gradient-norm instability. Based on this finding, we propose Dr. MAS, a simple and stable RL training recipe for multi-agent LLM systems. Dr. MAS uses an agent-wise remedy: normalizing advantages per agent using each agent's own reward statistics, which calibrates gradient scales and dramatically stabilizes training, both theoretically and empirically. Beyond the algorithm, Dr. MAS provides an end-to-end RL training framework for multi-agent LLM systems, supporting scalable orchestration, flexible per-agent LLM serving and optimization configs, and shared resource scheduling of LLM actor backends. We evaluate Dr. MAS on multi-agent math reasoning and multi-turn search benchmarks using Qwen2.5 and Qwen3 series models. Dr. MAS achieves clear gains over vanilla GRPO (e.g., +5.6\\% avg@16 and +4.6\\% pass@16 on math, and +15.2\\% avg@16 and +13.1\\% pass@16 on search) while largely eliminating gradient spikes. Moreover, it remains highly effective under heterogeneous agent-model assignments while improving efficiency.", "AI": {"tldr": "\u63d0\u51faDr. MAS\u65b9\u6cd5\uff0c\u901a\u8fc7\u6309\u667a\u80fd\u4f53\u5f52\u4e00\u5316\u4f18\u52bf\u503c\u6765\u89e3\u51b3\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u4e2d\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6570\u5b66\u63a8\u7406\u548c\u591a\u8f6e\u641c\u7d22\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u901a\u8fc7\u89d2\u8272\u4e13\u4e1a\u5316\u5b9e\u73b0\u9ad8\u7ea7\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\uff0c\u4f46\u73b0\u6709\u7684\u7fa4\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u6269\u5c55\u5230\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u65f6\u5b58\u5728\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002\u7814\u7a76\u53d1\u73b0\u5168\u5c40\u5f52\u4e00\u5316\u57fa\u7ebf\u53ef\u80fd\u504f\u79bb\u4e0d\u540c\u667a\u80fd\u4f53\u7684\u5956\u52b1\u5206\u5e03\uff0c\u5bfc\u81f4\u68af\u5ea6\u8303\u6570\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faDr. MAS\u65b9\u6cd5\uff0c\u91c7\u7528\u667a\u80fd\u4f53\u7ea7\u522b\u7684\u4fee\u6b63\uff1a\u4f7f\u7528\u6bcf\u4e2a\u667a\u80fd\u4f53\u81ea\u8eab\u7684\u5956\u52b1\u7edf\u8ba1\u91cf\u5bf9\u4f18\u52bf\u503c\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4ece\u800c\u6821\u51c6\u68af\u5ea6\u5c3a\u5ea6\u3002\u8be5\u65b9\u6cd5\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684RL\u8bad\u7ec3\u6846\u67b6\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7f16\u6392\u3001\u7075\u6d3b\u7684\u6309\u667a\u80fd\u4f53LLM\u670d\u52a1\u548c\u4f18\u5316\u914d\u7f6e\uff0c\u4ee5\u53ca\u5171\u4eab\u8d44\u6e90\u8c03\u5ea6\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u591a\u8f6e\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f7f\u7528Qwen2.5\u548cQwen3\u7cfb\u5217\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002Dr. MAS\u76f8\u6bd4\u539f\u59cbGRPO\u5728\u6570\u5b66\u4efb\u52a1\u4e0a\u5e73\u5747\u63d0\u53475.6%\uff0c\u901a\u8fc7\u7387\u63d0\u53474.6%\uff1b\u5728\u641c\u7d22\u4efb\u52a1\u4e0a\u5e73\u5747\u63d0\u534715.2%\uff0c\u901a\u8fc7\u7387\u63d0\u534713.1%\uff0c\u540c\u65f6\u5927\u5e45\u6d88\u9664\u68af\u5ea6\u5c16\u5cf0\u3002\u5728\u5f02\u6784\u667a\u80fd\u4f53\u6a21\u578b\u5206\u914d\u4e0b\u4ecd\u4fdd\u6301\u9ad8\u6548\u3002", "conclusion": "Dr. MAS\u901a\u8fc7\u667a\u80fd\u4f53\u7ea7\u522b\u7684\u4f18\u52bf\u5f52\u4e00\u5316\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u4e2d\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u7a33\u5b9a\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2602.08855", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08855", "abs": "https://arxiv.org/abs/2602.08855", "authors": ["Yang Qiu", "Yixiong Zou", "Jun Wang"], "title": "Rethinking Graph Generalization through the Lens of Sharpness-Aware Minimization", "comment": null, "summary": "Graph Neural Networks (GNNs) have achieved remarkable success across various graph-based tasks but remain highly sensitive to distribution shifts. In this work, we focus on a prevalent yet under-explored phenomenon in graph generalization, Minimal Shift Flip (MSF),where test samples that slightly deviate from the training distribution are abruptly misclassified. To interpret this phenomenon, we revisit MSF through the lens of Sharpness-Aware Minimization (SAM), which characterizes the local stability and sharpness of the loss landscape while providing a theoretical foundation for modeling generalization error. To quantify loss sharpness, we introduce the concept of Local Robust Radius, measuring the smallest perturbation required to flip a prediction and establishing a theoretical link between local stability and generalization. Building on this perspective, we further observe a continual decrease in the robust radius during training, indicating weakened local stability and an increasingly sharp loss landscape that gives rise to MSF. To jointly solve the MSF phenomenon and the intractability of radius, we develop an energy-based formulation that is theoretically proven to be monotonically correlated with the robust radius, offering a tractable and principled objective for modeling flatness and stability. Building on these insights, we propose an energy-driven generative augmentation framework (E2A) that leverages energy-guided latent perturbations to generate pseudo-OOD samples and enhance model generalization. Extensive experiments across multiple benchmarks demonstrate that E2A consistently improves graph OOD generalization, outperforming state-of-the-art baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u80fd\u91cf\u9a71\u52a8\u7684\u751f\u6210\u589e\u5f3a\u6846\u67b6(E2A)\uff0c\u901a\u8fc7\u80fd\u91cf\u5f15\u5bfc\u7684\u6f5c\u5728\u6270\u52a8\u751f\u6210\u4f2aOOD\u6837\u672c\u6765\u89e3\u51b3\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u6700\u5c0f\u504f\u79fb\u7ffb\u8f6c(MSF)\u73b0\u8c61\uff0c\u63d0\u5347\u56feOOD\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u56fe\u795e\u7ecf\u7f51\u7edc(GNNs)\u5728\u5404\u79cd\u56fe\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5bf9\u5206\u5e03\u504f\u79fb\u9ad8\u5ea6\u654f\u611f\u3002\u8bba\u6587\u5173\u6ce8\u56fe\u6cdb\u5316\u4e2d\u666e\u904d\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u6700\u5c0f\u504f\u79fb\u7ffb\u8f6c(MSF)\u73b0\u8c61\uff0c\u5373\u6d4b\u8bd5\u6837\u672c\u4ec5\u8f7b\u5fae\u504f\u79bb\u8bad\u7ec3\u5206\u5e03\u5c31\u88ab\u9519\u8bef\u5206\u7c7b\u3002\u9700\u8981\u7406\u89e3\u8fd9\u4e00\u73b0\u8c61\u5e76\u63d0\u5347GNNs\u7684OOD\u6cdb\u5316\u80fd\u529b\u3002", "method": "1. \u4ece\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316(SAM)\u89c6\u89d2\u91cd\u65b0\u5ba1\u89c6MSF\uff0c\u5f15\u5165\u5c40\u90e8\u9c81\u68d2\u534a\u5f84\u6982\u5ff5\u91cf\u5316\u635f\u5931\u9510\u5ea6\uff1b2. \u63d0\u51fa\u80fd\u91cf\u9a71\u52a8\u751f\u6210\u589e\u5f3a\u6846\u67b6(E2A)\uff0c\u5229\u7528\u80fd\u91cf\u5f15\u5bfc\u7684\u6f5c\u5728\u6270\u52a8\u751f\u6210\u4f2aOOD\u6837\u672c\uff1b3. \u5efa\u7acb\u80fd\u91cf\u516c\u5f0f\u4e0e\u9c81\u68d2\u534a\u5f84\u7684\u7406\u8bba\u5173\u8054\uff0c\u63d0\u4f9b\u53ef\u5904\u7406\u7684\u5e73\u5766\u5ea6\u548c\u7a33\u5b9a\u6027\u5efa\u6a21\u76ee\u6807\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cE2A\u80fd\u6301\u7eed\u6539\u8fdb\u56feOOD\u6cdb\u5316\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u80fd\u91cf\u9a71\u52a8\u7684\u751f\u6210\u589e\u5f3a\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u6700\u5c0f\u504f\u79fb\u7ffb\u8f6c\u73b0\u8c61\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u56feOOD\u6cdb\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08857", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08857", "abs": "https://arxiv.org/abs/2602.08857", "authors": ["Xinting Huang", "Aleksandra Bakalova", "Satwik Bhattamishra", "William Merrill", "Michael Hahn"], "title": "Discovering Interpretable Algorithms by Decompiling Transformers to RASP", "comment": "101 pages, 92 figures", "summary": "Recent work has shown that the computations of Transformers can be simulated in the RASP family of programming languages. These findings have enabled improved understanding of the expressive capacity and generalization abilities of Transformers. In particular, Transformers have been suggested to length-generalize exactly on problems that have simple RASP programs. However, it remains open whether trained models actually implement simple interpretable programs. In this paper, we present a general method to extract such programs from trained Transformers. The idea is to faithfully re-parameterize a Transformer as a RASP program and then apply causal interventions to discover a small sufficient sub-program. In experiments on small Transformers trained on algorithmic and formal language tasks, we show that our method often recovers simple and interpretable RASP programs from length-generalizing transformers. Our results provide the most direct evidence so far that Transformers internally implement simple RASP programs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ece\u8bad\u7ec3\u597d\u7684Transformer\u4e2d\u63d0\u53d6RASP\u7a0b\u5e8f\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u53d1\u73b0\u6700\u5c0f\u7684\u6709\u6548\u5b50\u7a0b\u5e8f\uff0c\u4e3aTransformer\u5185\u90e8\u5b9e\u73b0\u7b80\u5355\u53ef\u89e3\u91ca\u7a0b\u5e8f\u63d0\u4f9b\u76f4\u63a5\u8bc1\u636e", "motivation": "\u5c3d\u7ba1\u5148\u524d\u7814\u7a76\u8868\u660eTransformer\u7684\u8ba1\u7b97\u53ef\u4ee5\u7528RASP\u7f16\u7a0b\u8bed\u8a00\u6a21\u62df\uff0c\u4e14Transformer\u5728\u5177\u6709\u7b80\u5355RASP\u7a0b\u5e8f\u7684\u95ee\u9898\u4e0a\u80fd\u591f\u7cbe\u786e\u957f\u5ea6\u6cdb\u5316\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u662f\u5426\u771f\u7684\u5b9e\u73b0\u4e86\u7b80\u5355\u53ef\u89e3\u91ca\u7684\u7a0b\u5e8f", "method": "\u5c06Transformer\u5fe0\u5b9e\u91cd\u53c2\u6570\u5316\u4e3aRASP\u7a0b\u5e8f\uff0c\u7136\u540e\u5e94\u7528\u56e0\u679c\u5e72\u9884\u6765\u53d1\u73b0\u6700\u5c0f\u7684\u6709\u6548\u5b50\u7a0b\u5e8f", "result": "\u5728\u5c0f\u578bTransformer\u4e0a\u5bf9\u7b97\u6cd5\u548c\u5f62\u5f0f\u8bed\u8a00\u4efb\u52a1\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u901a\u5e38\u80fd\u4ece\u957f\u5ea6\u6cdb\u5316\u7684Transformer\u4e2d\u6062\u590d\u7b80\u5355\u53ef\u89e3\u91ca\u7684RASP\u7a0b\u5e8f", "conclusion": "\u8fd9\u662f\u8fc4\u4eca\u4e3a\u6b62\u6700\u76f4\u63a5\u7684\u8bc1\u636e\uff0c\u8868\u660eTransformer\u5185\u90e8\u5b9e\u73b0\u4e86\u7b80\u5355\u7684RASP\u7a0b\u5e8f"}}
{"id": "2602.08859", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08859", "abs": "https://arxiv.org/abs/2602.08859", "authors": ["Sahel Torkamani", "Henry Gouk", "Rik Sarkar"], "title": "Magnitude Distance: A Geometric Measure of Dataset Similarity", "comment": null, "summary": "Quantifying the distance between datasets is a fundamental question in mathematics and machine learning. We propose \\textit{magnitude distance}, a novel distance metric defined on finite datasets using the notion of the \\emph{magnitude} of a metric space. The proposed distance incorporates a tunable scaling parameter, $t$, that controls the sensitivity to global structure (small $t$) and finer details (large $t$). We prove several theoretical properties of magnitude distance, including its limiting behavior across scales and conditions under which it satisfies key metric properties. In contrast to classical distances, we show that magnitude distance remains discriminative in high-dimensional settings when the scale is appropriately tuned. We further demonstrate how magnitude distance can be used as a training objective for push-forward generative models. Our experimental results support our theoretical analysis and demonstrate that magnitude distance provides meaningful signals, comparable to established distance-based generative approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5ea6\u91cf\u7a7a\u95f4magnitude\u6982\u5ff5\u7684\u65b0\u578b\u6570\u636e\u96c6\u8ddd\u79bb\u5ea6\u91cf\u2014\u2014magnitude distance\uff0c\u5305\u542b\u53ef\u8c03\u5c3a\u5ea6\u53c2\u6570t\uff0c\u5728\u751f\u6210\u6a21\u578b\u8bad\u7ec3\u4e2d\u8868\u73b0\u826f\u597d", "motivation": "\u91cf\u5316\u6570\u636e\u96c6\u4e4b\u95f4\u7684\u8ddd\u79bb\u662f\u6570\u5b66\u548c\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u57fa\u672c\u95ee\u9898\uff0c\u73b0\u6709\u8ddd\u79bb\u5ea6\u91cf\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e0b\u53ef\u80fd\u5931\u53bb\u5224\u522b\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u6355\u6349\u4e0d\u540c\u5c3a\u5ea6\u7ed3\u6784\u7684\u65b0\u578b\u8ddd\u79bb\u5ea6\u91cf", "method": "\u57fa\u4e8e\u5ea6\u91cf\u7a7a\u95f4\u7684magnitude\u6982\u5ff5\u5b9a\u4e49magnitude distance\uff0c\u5f15\u5165\u53ef\u8c03\u5c3a\u5ea6\u53c2\u6570t\u63a7\u5236\u5bf9\u5168\u5c40\u7ed3\u6784\uff08\u5c0ft\uff09\u548c\u7ec6\u8282\uff08\u5927t\uff09\u7684\u654f\u611f\u6027\uff0c\u8bc1\u660e\u5176\u7406\u8bba\u6027\u8d28\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u63a8\u524d\u751f\u6210\u6a21\u578b\u7684\u8bad\u7ec3\u76ee\u6807", "result": "\u8bc1\u660e\u4e86magnitude distance\u7684\u7406\u8bba\u6027\u8d28\uff0c\u5305\u62ec\u5c3a\u5ea6\u6781\u9650\u884c\u4e3a\u548c\u6ee1\u8db3\u5173\u952e\u5ea6\u91cf\u6027\u8d28\u7684\u6761\u4ef6\uff1b\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e2d\uff0c\u5f53\u5c3a\u5ea6\u9002\u5f53\u8c03\u6574\u65f6\u4ecd\u4fdd\u6301\u5224\u522b\u6027\uff1b\u5b9e\u9a8c\u8868\u660e\u8be5\u8ddd\u79bb\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u4fe1\u53f7\uff0c\u4e0e\u73b0\u6709\u57fa\u4e8e\u8ddd\u79bb\u7684\u751f\u6210\u65b9\u6cd5\u76f8\u5f53", "conclusion": "magnitude distance\u662f\u4e00\u79cd\u6709\u7406\u8bba\u4fdd\u8bc1\u7684\u65b0\u578b\u6570\u636e\u96c6\u8ddd\u79bb\u5ea6\u91cf\uff0c\u80fd\u6709\u6548\u6355\u6349\u4e0d\u540c\u5c3a\u5ea6\u7ed3\u6784\uff0c\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e2d\u4fdd\u6301\u5224\u522b\u6027\uff0c\u9002\u7528\u4e8e\u751f\u6210\u6a21\u578b\u8bad\u7ec3\uff0c\u4e3a\u6570\u636e\u96c6\u6bd4\u8f83\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177"}}
{"id": "2602.08862", "categories": ["cs.LG", "cs.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08862", "abs": "https://arxiv.org/abs/2602.08862", "authors": ["Lunjia Hu", "Jon Schneider", "Yifan Wu"], "title": "Near-optimal Swap Regret Minimization for Convex Losses", "comment": null, "summary": "We give a randomized online algorithm that guarantees near-optimal $\\widetilde O(\\sqrt T)$ expected swap regret against any sequence of $T$ adaptively chosen Lipschitz convex losses on the unit interval. This improves the previous best bound of $\\widetilde O(T^{2/3})$ and answers an open question of Fishelson et al. [2025b]. In addition, our algorithm is efficient: it runs in $\\mathsf{poly}(T)$ time. A key technical idea we develop to obtain this result is to discretize the unit interval into bins at multiple scales of granularity and simultaneously use all scales to make randomized predictions, which we call multi-scale binning and may be of independent interest. A direct corollary of our result is an efficient online algorithm for minimizing the calibration error for general elicitable properties. This result does not require the Lipschitzness assumption of the identification function needed in prior work, making it applicable to median calibration, for which we achieve the first $\\widetilde O(\\sqrt T)$ calibration error guarantee.", "AI": {"tldr": "\u63d0\u51fa\u968f\u673a\u5728\u7ebf\u7b97\u6cd5\uff0c\u5728\u5355\u4f4d\u533a\u95f4\u4e0a\u9488\u5bf9\u81ea\u9002\u5e94\u9009\u62e9\u7684Lipschitz\u51f8\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u73b0\u8fd1\u4e4e\u6700\u4f18\u7684$\\widetilde O(\\sqrt T)$\u671f\u671b\u4ea4\u6362\u9057\u61be\uff0c\u6539\u8fdb\u5148\u524d$\\widetilde O(T^{2/3})$\u7ed3\u679c\uff0c\u5e76\u9ad8\u6548\u8fd0\u884c\u3002", "motivation": "\u89e3\u51b3Fishelson\u7b49\u4eba[2025b]\u63d0\u51fa\u7684\u5f00\u653e\u95ee\u9898\uff1a\u5982\u4f55\u6539\u8fdb\u5728\u7ebf\u5b66\u4e60\u4e2d\u7684\u4ea4\u6362\u9057\u61be\u8fb9\u754c\u3002\u5148\u524d\u6700\u4f73\u7ed3\u679c\u4e3a$\\widetilde O(T^{2/3})$\uff0c\u5e0c\u671b\u8fbe\u5230\u7406\u8bba\u6700\u4f18\u7684$\\widetilde O(\\sqrt T)$\u8fb9\u754c\u3002", "method": "\u63d0\u51fa\u591a\u5c3a\u5ea6\u5206\u7bb1\u6280\u672f\uff1a\u5c06\u5355\u4f4d\u533a\u95f4\u5728\u4e0d\u540c\u7c92\u5ea6\u5c3a\u5ea6\u4e0a\u79bb\u6563\u5316\u4e3a\u591a\u4e2a\u7bb1\u5b50\uff0c\u540c\u65f6\u4f7f\u7528\u6240\u6709\u5c3a\u5ea6\u8fdb\u884c\u968f\u673a\u9884\u6d4b\u3002\u7b97\u6cd5\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u8fd0\u884c\u3002", "result": "1. \u5b9e\u73b0\u4e86$\\widetilde O(\\sqrt T)$\u671f\u671b\u4ea4\u6362\u9057\u61be\uff0c\u6539\u8fdb\u5148\u524d$\\widetilde O(T^{2/3})$\u7ed3\u679c\uff1b2. \u7b97\u6cd5\u9ad8\u6548\u8fd0\u884c\u4e8e$\\mathsf{poly}(T)$\u65f6\u95f4\uff1b3. \u5e94\u7528\u4e8e\u53ef\u5f15\u51fa\u5c5e\u6027\u7684\u6821\u51c6\u8bef\u5dee\u6700\u5c0f\u5316\uff0c\u65e0\u9700\u5148\u524d\u5de5\u4f5c\u6240\u9700\u7684\u8bc6\u522b\u51fd\u6570Lipschitz\u5047\u8bbe\uff0c\u9996\u6b21\u5b9e\u73b0\u4e2d\u4f4d\u6570\u6821\u51c6\u7684$\\widetilde O(\\sqrt T)$\u6821\u51c6\u8bef\u5dee\u4fdd\u8bc1\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u6548\u968f\u673a\u5728\u7ebf\u7b97\u6cd5\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u5206\u7bb1\u6280\u672f\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u6700\u4f18\u7684\u4ea4\u6362\u9057\u61be\u8fb9\u754c\uff0c\u89e3\u51b3\u4e86\u5f00\u653e\u95ee\u9898\uff0c\u5e76\u6269\u5c55\u4e86\u6821\u51c6\u8bef\u5dee\u6700\u5c0f\u5316\u7684\u5e94\u7528\u8303\u56f4\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4e2d\u4f4d\u6570\u6821\u51c6\u7b49\u5148\u524d\u65e0\u6cd5\u5904\u7406\u7684\u60c5\u51b5\u3002"}}
{"id": "2602.08868", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08868", "abs": "https://arxiv.org/abs/2602.08868", "authors": ["Junru Zhang", "Lang Feng", "Haoran Shi", "Xu Guo", "Han Yu", "Yabo Dong", "Duanqing Xu"], "title": "AnomSeer: Reinforcing Multimodal LLMs to Reason for Time-Series Anomaly Detection", "comment": "Preprint", "summary": "Time-series anomaly detection (TSAD) with multimodal large language models (MLLMs) is an emerging area, yet a persistent challenge remains: MLLMs rely on coarse time-series heuristics but struggle with multi-dimensional, detailed reasoning, which is vital for understanding complex time-series data. We present AnomSeer to address this by reinforcing the model to ground its reasoning in precise, structural details of time series, unifying anomaly classification, localization, and explanation. At its core, an expert chain-of-thought trace is generated to provide a verifiable, fine-grained reasoning from classical analyses (e.g., statistical measures, frequency transforms). Building on this, we propose a novel time-series grounded policy optimization (TimerPO) that incorporates two additional components beyond standard reinforcement learning: a time-series grounded advantage based on optimal transport and an orthogonal projection to ensure this auxiliary granular signal does not interfere with the primary detection objective. Across diverse anomaly scenarios, AnomSeer, with Qwen2.5-VL-3B/7B-Instruct, outperforms larger commercial baselines (e.g., GPT-4o) in classification and localization accuracy, particularly on point- and frequency-driven exceptions. Moreover, it produces plausible time-series reasoning traces that support its conclusions.", "AI": {"tldr": "AnomSeer\uff1a\u57fa\u4e8e\u65f6\u95f4\u5e8f\u5217\u7ec6\u7c92\u5ea6\u63a8\u7406\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u601d\u7ef4\u94fe\u548cTimerPO\u4f18\u5316\uff0c\u5728\u5206\u7c7b\u3001\u5b9a\u4f4d\u548c\u89e3\u91ca\u65b9\u9762\u4f18\u4e8eGPT-4o\u7b49\u5927\u578b\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u7c97\u7c92\u5ea6\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u7f3a\u4e4f\u5bf9\u591a\u7ef4\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u7ec6\u7c92\u5ea6\u63a8\u7406\u80fd\u529b\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5728\u7406\u89e3\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u6a21\u5f0f\u3001\u8fdb\u884c\u7cbe\u786e\u5f02\u5e38\u5206\u7c7b\u548c\u5b9a\u4f4d\u65b9\u9762\u7684\u6027\u80fd\u3002", "method": "1. \u751f\u6210\u4e13\u5bb6\u601d\u7ef4\u94fe\u8f68\u8ff9\uff1a\u57fa\u4e8e\u7ecf\u5178\u5206\u6790\u65b9\u6cd5\uff08\u7edf\u8ba1\u5ea6\u91cf\u3001\u9891\u7387\u53d8\u6362\uff09\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u7ec6\u7c92\u5ea6\u63a8\u7406\uff1b2. \u63d0\u51fa\u65f6\u95f4\u5e8f\u5217\u63a5\u5730\u7b56\u7565\u4f18\u5316\uff08TimerPO\uff09\uff1a\u5305\u542b\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u65f6\u95f4\u5e8f\u5217\u63a5\u5730\u4f18\u52bf\u548c\u6b63\u4ea4\u6295\u5f71\u7ec4\u4ef6\uff0c\u786e\u4fdd\u7ec6\u7c92\u5ea6\u4fe1\u53f7\u4e0d\u5e72\u6270\u4e3b\u8981\u68c0\u6d4b\u76ee\u6807\u3002", "result": "\u5728\u591a\u6837\u5316\u5f02\u5e38\u573a\u666f\u4e0b\uff0c\u4f7f\u7528Qwen2.5-VL-3B/7B-Instruct\u7684AnomSeer\u5728\u5206\u7c7b\u548c\u5b9a\u4f4d\u51c6\u786e\u7387\u4e0a\u4f18\u4e8eGPT-4o\u7b49\u5927\u578b\u5546\u4e1a\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728\u70b9\u548c\u9891\u7387\u9a71\u52a8\u5f02\u5e38\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u5e76\u80fd\u751f\u6210\u652f\u6301\u7ed3\u8bba\u7684\u5408\u7406\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u8f68\u8ff9\u3002", "conclusion": "AnomSeer\u901a\u8fc7\u5c06\u6a21\u578b\u63a8\u7406\u57fa\u4e8e\u65f6\u95f4\u5e8f\u5217\u7684\u7cbe\u786e\u7ed3\u6784\u7ec6\u8282\uff0c\u7edf\u4e00\u4e86\u5f02\u5e38\u5206\u7c7b\u3001\u5b9a\u4f4d\u548c\u89e3\u91ca\uff0c\u89e3\u51b3\u4e86MLLMs\u5728\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7ec6\u7c92\u5ea6\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08877", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08877", "abs": "https://arxiv.org/abs/2602.08877", "authors": ["Oliver Daniels", "Perusha Moodley", "Ben Marlin", "David Lindner"], "title": "Stress-Testing Alignment Audits With Prompt-Level Strategic Deception", "comment": null, "summary": "Alignment audits aim to robustly identify hidden goals from strategic, situationally aware misaligned models. Despite this threat model, existing auditing methods have not been systematically stress-tested against deception strategies. We address this gap, implementing an automatic red-team pipeline that generates deception strategies (in the form of system prompts) tailored to specific white-box and black-box auditing methods. Stress-testing assistant prefills, user persona sampling, sparse autoencoders, and token embedding similarity methods against secret-keeping model organisms, our automatic red-team pipeline finds prompts that deceive both the black-box and white-box methods into confident, incorrect guesses. Our results provide the first documented evidence of activation-based strategic deception, and suggest that current black-box and white-box methods would not be robust to a sufficiently capable misaligned model.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u7ea2\u961f\u7ba1\u9053\u6765\u751f\u6210\u6b3a\u9a97\u7b56\u7565\uff0c\u6d4b\u8bd5\u73b0\u6709\u5bf9\u9f50\u5ba1\u8ba1\u65b9\u6cd5\u5bf9\u6218\u7565\u6b3a\u9a97\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u5f53\u524d\u9ed1\u767d\u76d2\u65b9\u6cd5\u5728\u9762\u5bf9\u6709\u80fd\u529b\u7684\u672a\u5bf9\u9f50\u6a21\u578b\u65f6\u90fd\u4e0d\u591f\u53ef\u9760\u3002", "motivation": "\u73b0\u6709\u5bf9\u9f50\u5ba1\u8ba1\u65b9\u6cd5\u65e8\u5728\u4ece\u6218\u7565\u6027\u7684\u3001\u5177\u6709\u60c5\u5883\u610f\u8bc6\u7684\u672a\u5bf9\u9f50\u6a21\u578b\u4e2d\u8bc6\u522b\u9690\u85cf\u76ee\u6807\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5c1a\u672a\u7cfb\u7edf\u6027\u5730\u9488\u5bf9\u6b3a\u9a97\u7b56\u7565\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u81ea\u52a8\u7ea2\u961f\u7ba1\u9053\uff0c\u751f\u6210\u9488\u5bf9\u7279\u5b9a\u767d\u76d2\u548c\u9ed1\u76d2\u5ba1\u8ba1\u65b9\u6cd5\u7684\u6b3a\u9a97\u7b56\u7565\uff08\u4ee5\u7cfb\u7edf\u63d0\u793a\u5f62\u5f0f\uff09\u3002\u5bf9\u52a9\u624b\u9884\u586b\u5145\u3001\u7528\u6237\u89d2\u8272\u91c7\u6837\u3001\u7a00\u758f\u81ea\u7f16\u7801\u5668\u548c\u8bcd\u5d4c\u5165\u76f8\u4f3c\u6027\u7b49\u65b9\u6cd5\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5\u3002", "result": "\u81ea\u52a8\u7ea2\u961f\u7ba1\u9053\u627e\u5230\u4e86\u80fd\u591f\u6b3a\u9a97\u9ed1\u76d2\u548c\u767d\u76d2\u65b9\u6cd5\u7684\u63d0\u793a\uff0c\u4f7f\u5176\u505a\u51fa\u81ea\u4fe1\u4f46\u9519\u8bef\u7684\u731c\u6d4b\u3002\u8fd9\u662f\u9996\u6b21\u8bb0\u5f55\u5230\u57fa\u4e8e\u6fc0\u6d3b\u7684\u6218\u7565\u6b3a\u9a97\u8bc1\u636e\u3002", "conclusion": "\u5f53\u524d\u7684\u9ed1\u76d2\u548c\u767d\u76d2\u65b9\u6cd5\u5728\u9762\u5bf9\u8db3\u591f\u6709\u80fd\u529b\u7684\u672a\u5bf9\u9f50\u6a21\u578b\u65f6\u90fd\u4e0d\u591f\u9c81\u68d2\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u5ba1\u8ba1\u65b9\u6cd5\u6765\u5e94\u5bf9\u6218\u7565\u6b3a\u9a97\u3002"}}
{"id": "2602.08878", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08878", "abs": "https://arxiv.org/abs/2602.08878", "authors": ["Itai Zilberstein", "Ioannis Anagnostides", "Zachary W. Sollie", "Arman Kilic", "Tuomas Sandholm"], "title": "Learning Potentials for Dynamic Matching and Application to Heart Transplantation", "comment": null, "summary": "Each year, thousands of patients in need of heart transplants face life-threatening wait times due to organ scarcity. While allocation policies aim to maximize population-level outcomes, current approaches often fail to account for the dynamic arrival of organs and the composition of waitlisted candidates, thereby hampering efficiency. The United States is transitioning from rigid, rule-based allocation to more flexible data-driven models. In this paper, we propose a novel framework for non-myopic policy optimization in general online matching relying on potentials, a concept originally introduced for kidney exchange. We develop scalable and accurate ways of learning potentials that are higher-dimensional and more expressive than prior approaches. Our approach is a form of self-supervised imitation learning: the potentials are trained to mimic an omniscient algorithm that has perfect foresight. We focus on the application of heart transplant allocation and demonstrate, using real historical data, that our policies significantly outperform prior approaches -- including the current US status quo policy and the proposed continuous distribution framework -- in optimizing for population-level outcomes. Our analysis and methods come at a pivotal moment in US policy, as the current heart transplant allocation system is under review. We propose a scalable and theoretically grounded path toward more effective organ allocation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6f5c\u5728\u51fd\u6570\u7684\u975e\u8fd1\u89c6\u653f\u7b56\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5fc3\u810f\u79fb\u690d\u5668\u5b98\u5206\u914d\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u6f5c\u5728\u51fd\u6570\u6765\u6a21\u62df\u5168\u77e5\u7b97\u6cd5\uff0c\u5728\u771f\u5b9e\u5386\u53f2\u6570\u636e\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5fc3\u810f\u79fb\u690d\u7b49\u5f85\u65f6\u95f4\u5371\u53ca\u751f\u547d\uff0c\u73b0\u6709\u5668\u5b98\u5206\u914d\u653f\u7b56\u672a\u80fd\u8003\u8651\u5668\u5b98\u52a8\u6001\u5230\u8fbe\u548c\u7b49\u5f85\u540d\u5355\u7ec4\u6210\uff0c\u7f8e\u56fd\u6b63\u4ece\u57fa\u4e8e\u89c4\u5219\u7684\u653f\u7b56\u8f6c\u5411\u6570\u636e\u9a71\u52a8\u6a21\u578b\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5206\u914d\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6f5c\u5728\u51fd\u6570\u7684\u975e\u8fd1\u89c6\u653f\u7b56\u4f18\u5316\u6846\u67b6\uff0c\u5f00\u53d1\u53ef\u6269\u5c55\u4e14\u51c6\u786e\u7684\u9ad8\u7ef4\u6f5c\u5728\u51fd\u6570\u5b66\u4e60\u65b9\u6cd5\uff0c\u91c7\u7528\u81ea\u76d1\u7763\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u6f5c\u5728\u51fd\u6570\u6765\u6a21\u62df\u5177\u6709\u5b8c\u7f8e\u9884\u89c1\u6027\u7684\u5168\u77e5\u7b97\u6cd5\u3002", "result": "\u4f7f\u7528\u771f\u5b9e\u5386\u53f2\u6570\u636e\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f18\u5316\u7fa4\u4f53\u6c34\u5e73\u7ed3\u679c\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5305\u62ec\u7f8e\u56fd\u73b0\u72b6\u653f\u7b56\u548c\u63d0\u51fa\u7684\u8fde\u7eed\u5206\u5e03\u6846\u67b6\u3002", "conclusion": "\u5728\u7f8e\u56fd\u5fc3\u810f\u79fb\u690d\u5206\u914d\u7cfb\u7edf\u5ba1\u67e5\u7684\u5173\u952e\u65f6\u523b\uff0c\u63d0\u51fa\u4e86\u53ef\u6269\u5c55\u4e14\u7406\u8bba\u57fa\u7840\u7684\u8def\u5f84\uff0c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u5668\u5b98\u5206\u914d\u3002"}}
{"id": "2602.08885", "categories": ["cs.LG", "cs.AI", "cs.SC"], "pdf": "https://arxiv.org/pdf/2602.08885", "abs": "https://arxiv.org/abs/2602.08885", "authors": ["Paul Saegert", "Ullrich K\u00f6the"], "title": "Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression", "comment": "main text: 8 pages, 7 figures appendix: 12 pages, 11 figures code available at https://github.com/psaegert/simplipy and https://github.com/psaegert/flash-ansr", "summary": "Symbolic regression (SR) aims to discover interpretable analytical expressions that accurately describe observed data. Amortized SR promises to be much more efficient than the predominant genetic programming SR methods, but currently struggles to scale to realistic scientific complexity. We find that a key obstacle is the lack of a fast reduction of equivalent expressions to a concise normalized form. Amortized SR has addressed this by general-purpose Computer Algebra Systems (CAS) like SymPy, but the high computational cost severely limits training and inference speed. We propose SimpliPy, a rule-based simplification engine achieving a 100-fold speed-up over SymPy at comparable quality. This enables substantial improvements in amortized SR, including scalability to much larger training sets, more efficient use of the per-expression token budget, and systematic training set decontamination with respect to equivalent test expressions. We demonstrate these advantages in our Flash-ANSR framework, which achieves much better accuracy than amortized baselines (NeSymReS, E2E) on the FastSRB benchmark. Moreover, it performs on par with state-of-the-art direct optimization (PySR) while recovering more concise instead of more complex expressions with increasing inference budget.", "AI": {"tldr": "SimpliPy\u52a0\u901f\u7b26\u53f7\u56de\u5f52\uff0cFlash-ANSR\u6846\u67b6\u5728FastSRB\u57fa\u51c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "motivation": "\u5f53\u524d\u644a\u9500\u5f0f\u7b26\u53f7\u56de\u5f52(SR)\u5728\u5904\u7406\u590d\u6742\u79d1\u5b66\u95ee\u9898\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u7f3a\u4e4f\u5feb\u901f\u5c06\u7b49\u4ef7\u8868\u8fbe\u5f0f\u7b80\u5316\u4e3a\u7b80\u6d01\u89c4\u8303\u5f62\u5f0f\u7684\u65b9\u6cd5\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u901a\u7528\u8ba1\u7b97\u673a\u4ee3\u6570\u7cfb\u7edf(\u5982SymPy)\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u3002", "method": "\u63d0\u51faSimpliPy\uff1a\u57fa\u4e8e\u89c4\u5219\u7684\u7b80\u5316\u5f15\u64ce\uff0c\u6bd4SymPy\u5feb100\u500d\u4e14\u8d28\u91cf\u76f8\u5f53\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u6784\u5efaFlash-ANSR\u6846\u67b6\uff0c\u5229\u7528SimpliPy\u5b9e\u73b0\u5927\u89c4\u6a21\u8bad\u7ec3\u96c6\u6269\u5c55\u3001\u66f4\u6709\u6548\u7684token\u9884\u7b97\u4f7f\u7528\uff0c\u4ee5\u53ca\u8bad\u7ec3\u96c6\u53bb\u6c61\u67d3\u3002", "result": "\u5728FastSRB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFlash-ANSR\u663e\u8457\u4f18\u4e8e\u644a\u9500\u5f0f\u57fa\u7ebf\u65b9\u6cd5(NeSymReS, E2E)\u3002\u4e0e\u6700\u5148\u8fdb\u7684\u76f4\u63a5\u4f18\u5316\u65b9\u6cd5(PySR)\u6027\u80fd\u76f8\u5f53\uff0c\u4f46\u80fd\u968f\u7740\u63a8\u7406\u9884\u7b97\u589e\u52a0\u6062\u590d\u66f4\u7b80\u6d01\u800c\u975e\u66f4\u590d\u6742\u7684\u8868\u8fbe\u5f0f\u3002", "conclusion": "SimpliPy\u901a\u8fc7\u5927\u5e45\u52a0\u901f\u8868\u8fbe\u5f0f\u7b80\u5316\uff0c\u89e3\u51b3\u4e86\u644a\u9500\u5f0f\u7b26\u53f7\u56de\u5f52\u7684\u5173\u952e\u74f6\u9888\u3002Flash-ANSR\u6846\u67b6\u5c55\u793a\u4e86\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u6548\u7387\u7684\u6f5c\u529b\uff0c\u4e3a\u7b26\u53f7\u56de\u5f52\u5728\u590d\u6742\u79d1\u5b66\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.08894", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08894", "abs": "https://arxiv.org/abs/2602.08894", "authors": ["Iryna Zabarianska", "Sergei Kholkin", "Grigoriy Ksenofontov", "Ivan Butakov", "Alexander Korotin"], "title": "Discrete Bridges for Mutual Information Estimation", "comment": null, "summary": "Diffusion bridge models in both continuous and discrete state spaces have recently become powerful tools in the field of generative modeling. In this work, we leverage the discrete state space formulation of bridge matching models to address another important problem in machine learning and information theory: the estimation of the mutual information (MI) between discrete random variables. By neatly framing MI estimation as a domain transfer problem, we construct a Discrete Bridge Mutual Information (DBMI) estimator suitable for discrete data, which poses difficulties for conventional MI estimators. We showcase the performance of our estimator on two MI estimation settings: low-dimensional and image-based.", "AI": {"tldr": "\u63d0\u51faDBMI\u4f30\u8ba1\u5668\uff0c\u5229\u7528\u79bb\u6563\u6865\u5339\u914d\u6a21\u578b\u5c06\u4e92\u4fe1\u606f\u4f30\u8ba1\u8f6c\u5316\u4e3a\u57df\u8f6c\u79fb\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u7684\u79bb\u6563\u6570\u636e", "motivation": "\u4f20\u7edf\u4e92\u4fe1\u606f\u4f30\u8ba1\u65b9\u6cd5\u5728\u5904\u7406\u79bb\u6563\u6570\u636e\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u800c\u6269\u6563\u6865\u6a21\u578b\u5728\u79bb\u6563\u72b6\u6001\u7a7a\u95f4\u4e2d\u7684\u53d1\u5c55\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def", "method": "\u5c06\u4e92\u4fe1\u606f\u4f30\u8ba1\u91cd\u65b0\u6784\u5efa\u4e3a\u57df\u8f6c\u79fb\u95ee\u9898\uff0c\u5229\u7528\u79bb\u6563\u72b6\u6001\u7a7a\u95f4\u7684\u6865\u5339\u914d\u6a21\u578b\u6784\u5efa\u79bb\u6563\u6865\u4e92\u4fe1\u606f\uff08DBMI\uff09\u4f30\u8ba1\u5668", "result": "\u5728\u4f4e\u7ef4\u548c\u57fa\u4e8e\u56fe\u50cf\u7684\u4e24\u79cd\u4e92\u4fe1\u606f\u4f30\u8ba1\u573a\u666f\u4e2d\u5c55\u793a\u4e86DBMI\u4f30\u8ba1\u5668\u7684\u6027\u80fd", "conclusion": "\u79bb\u6563\u6865\u5339\u914d\u6a21\u578b\u4e3a\u4e92\u4fe1\u606f\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u7684\u79bb\u6563\u6570\u636e\u573a\u666f"}}
{"id": "2602.08901", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08901", "abs": "https://arxiv.org/abs/2602.08901", "authors": ["Xuanqi Zhang", "Haoyang Shang", "Xiaoxiao Li"], "title": "GSS: Gated Subspace Steering for Selective Memorization Mitigation in LLMs", "comment": "34 pages, 12 figures", "summary": "Large language models (LLMs) can memorize and reproduce training sequences verbatim -- a tendency that undermines both generalization and privacy. Existing mitigation methods apply interventions uniformly, degrading performance on the majority of tokens that generalize normally. We show empirically that memorization is sparse, intermittent, and token-conditioned, suggesting that effective mitigation requires context-aware intervention rather than static parameter modification. To this end, we propose a novel and effective selective memorization mitigation method -- Gated Subspace Steering (GSS), which decomposes intervention into a probe (detecting memorization-relevant activations) and a steer (applying targeted correction only when the probe exceeds a threshold). The optimal probe-steer pair emerges from a principled optimization framework based on optimal subspace steering. Experiments on four benchmarks show GSS matches or exceeds state-of-the-art memorization reduction while requiring $100-1000 \\times$ less compute than optimization-based alternatives. Furthermore, we provide new theoretical insights into the geometry of memorization in neural representations.", "AI": {"tldr": "\u63d0\u51faGated Subspace Steering (GSS)\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a2\u6d4b-\u5f15\u5bfc\u673a\u5236\u9009\u62e9\u6027\u7f13\u89e3LLM\u7684\u8bb0\u5fc6\u95ee\u9898\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u91cf\u51cf\u5c11100-1000\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5bf9LLM\u7684\u8bb0\u5fc6\u95ee\u9898\u91c7\u7528\u7edf\u4e00\u5e72\u9884\uff0c\u4f1a\u964d\u4f4e\u5927\u591a\u6570\u6b63\u5e38\u6cdb\u5316token\u7684\u6027\u80fd\u3002\u7814\u7a76\u53d1\u73b0\u8bb0\u5fc6\u662f\u7a00\u758f\u3001\u95f4\u6b47\u4e14token\u4f9d\u8d56\u7684\uff0c\u9700\u8981\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5e72\u9884\u800c\u975e\u9759\u6001\u53c2\u6570\u4fee\u6539\u3002", "method": "\u63d0\u51faGated Subspace Steering (GSS)\u65b9\u6cd5\uff0c\u5c06\u5e72\u9884\u5206\u89e3\u4e3a\u63a2\u6d4b\uff08\u68c0\u6d4b\u4e0e\u8bb0\u5fc6\u76f8\u5173\u7684\u6fc0\u6d3b\uff09\u548c\u5f15\u5bfc\uff08\u4ec5\u5728\u63a2\u6d4b\u8d85\u8fc7\u9608\u503c\u65f6\u5e94\u7528\u9488\u5bf9\u6027\u4fee\u6b63\uff09\u3002\u901a\u8fc7\u6700\u4f18\u5b50\u7a7a\u95f4\u5f15\u5bfc\u7684\u4f18\u5316\u6846\u67b6\u627e\u5230\u6700\u4f73\u63a2\u6d4b-\u5f15\u5bfc\u5bf9\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGSS\u8fbe\u5230\u6216\u8d85\u8fc7\u6700\u5148\u8fdb\u7684\u8bb0\u5fc6\u51cf\u5c11\u6548\u679c\uff0c\u540c\u65f6\u8ba1\u7b97\u91cf\u6bd4\u57fa\u4e8e\u4f18\u5316\u7684\u66ff\u4ee3\u65b9\u6cd5\u5c11100-1000\u500d\u3002\u63d0\u4f9b\u4e86\u5173\u4e8e\u795e\u7ecf\u8868\u793a\u4e2d\u8bb0\u5fc6\u51e0\u4f55\u7ed3\u6784\u7684\u65b0\u7406\u8bba\u89c1\u89e3\u3002", "conclusion": "GSS\u901a\u8fc7\u9009\u62e9\u6027\u5e72\u9884\u6709\u6548\u7f13\u89e3LLM\u8bb0\u5fc6\u95ee\u9898\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u4e3a\u7406\u89e3\u8bb0\u5fc6\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u51e0\u4f55\u7ed3\u6784\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.08907", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08907", "abs": "https://arxiv.org/abs/2602.08907", "authors": ["Marko Medvedev", "Idan Attias", "Elisabetta Cornacchia", "Theodor Misiakiewicz", "Gal Vardi", "Nathan Srebro"], "title": "Positive Distribution Shift as a Framework for Understanding Tractable Learning", "comment": null, "summary": "We study a setting where the goal is to learn a target function f(x) with respect to a target distribution D(x), but training is done on i.i.d. samples from a different training distribution D'(x), labeled by the true target f(x). Such a distribution shift (here in the form of covariate shift) is usually viewed negatively, as hurting or making learning harder, and the traditional distribution shift literature is mostly concerned with limiting or avoiding this negative effect. In contrast, we argue that with a well-chosen D'(x), the shift can be positive and make learning easier -- a perspective called Positive Distribution Shift (PDS). Such a perspective is central to contemporary machine learning, where much of the innovation is in finding good training distributions D'(x), rather than changing the training algorithm. We further argue that the benefit is often computational rather than statistical, and that PDS allows computationally hard problems to become tractable even using standard gradient-based training. We formalize different variants of PDS, show how certain hard classes are easily learnable under PDS, and make connections with membership query learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u6b63\u5206\u5e03\u504f\u79fb\"\u6982\u5ff5\uff0c\u8ba4\u4e3a\u5728\u7279\u5b9a\u8bad\u7ec3\u5206\u5e03\u4e0b\uff0c\u5206\u5e03\u504f\u79fb\u53ef\u4ee5\u7b80\u5316\u5b66\u4e60\u800c\u975e\u963b\u788d\u5b66\u4e60\uff0c\u4e3b\u8981\u5e26\u6765\u8ba1\u7b97\u800c\u975e\u7edf\u8ba1\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3a\u5206\u5e03\u504f\u79fb\uff08\u7279\u522b\u662f\u534f\u53d8\u91cf\u504f\u79fb\uff09\u4f1a\u635f\u5bb3\u5b66\u4e60\u6548\u679c\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u7cbe\u5fc3\u9009\u62e9\u7684\u8bad\u7ec3\u5206\u5e03\u53ef\u4ee5\u4f7f\u5b66\u4e60\u66f4\u5bb9\u6613\uff0c\u8fd9\u79cd\"\u6b63\u5206\u5e03\u504f\u79fb\"\u89c6\u89d2\u5bf9\u5f53\u4ee3\u673a\u5668\u5b66\u4e60\u521b\u65b0\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f62\u5f0f\u5316\u4e0d\u540c\u53d8\u4f53\u7684\u6b63\u5206\u5e03\u504f\u79fb\u6982\u5ff5\uff0c\u5c55\u793a\u67d0\u4e9b\u96be\u5b66\u4e60\u7c7b\u522b\u5728\u6b63\u5206\u5e03\u504f\u79fb\u4e0b\u53d8\u5f97\u5bb9\u6613\u5b66\u4e60\uff0c\u5e76\u4e0e\u6210\u5458\u67e5\u8be2\u5b66\u4e60\u5efa\u7acb\u8054\u7cfb\u3002", "result": "\u8bc1\u660e\u6b63\u5206\u5e03\u504f\u79fb\u53ef\u4ee5\u4f7f\u8ba1\u7b97\u56f0\u96be\u95ee\u9898\u53d8\u5f97\u53ef\u5904\u7406\uff0c\u5373\u4f7f\u4f7f\u7528\u6807\u51c6\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u8bad\u7ec3\u65b9\u6cd5\u4e5f\u80fd\u5b9e\u73b0\u3002", "conclusion": "\u6b63\u5206\u5e03\u504f\u79fb\u4e3a\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u5f3a\u8c03\u9009\u62e9\u597d\u7684\u8bad\u7ec3\u5206\u5e03\u800c\u975e\u4ec5\u6539\u8fdb\u8bad\u7ec3\u7b97\u6cd5\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u5728\u89e3\u51b3\u8ba1\u7b97\u56f0\u96be\u95ee\u9898\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2602.08913", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08913", "abs": "https://arxiv.org/abs/2602.08913", "authors": ["Kate\u0159ina Henclov\u00e1", "V\u00e1clav \u0160m\u00eddl"], "title": "GEMSS: A Variational Bayesian Method for Discovering Multiple Sparse Solutions in Classification and Regression Problems", "comment": null, "summary": "Selecting interpretable feature sets in underdetermined ($n \\ll p$) and highly correlated regimes constitutes a fundamental challenge in data science, particularly when analyzing physical measurements. In such settings, multiple distinct sparse subsets may explain the response equally well. Identifying these alternatives is crucial for generating domain-specific insights into the underlying mechanisms, yet conventional methods typically isolate a single solution, obscuring the full spectrum of plausible explanations.\n  We present GEMSS (Gaussian Ensemble for Multiple Sparse Solutions), a variational Bayesian framework specifically designed to simultaneously discover multiple, diverse sparse feature combinations. The method employs a structured spike-and-slab prior for sparsity, a mixture of Gaussians to approximate the intractable multimodal posterior, and a Jaccard-based penalty to further control solution diversity. Unlike sequential greedy approaches, GEMSS optimizes the entire ensemble of solutions within a single objective function via stochastic gradient descent.\n  The method is validated on a comprehensive benchmark comprising 128 synthetic experiments across classification and regression tasks. Results demonstrate that GEMSS scales effectively to high-dimensional settings ($p=5000$) with sample size as small as $n = 50$, generalizes seamlessly to continuous targets, handles missing data natively, and exhibits remarkable robustness to class imbalance and Gaussian noise.\n  GEMSS is available as a Python package 'gemss' at PyPI. The full GitHub repository at https://github.com/kat-er-ina/gemss/ also includes a free, easy-to-use application suitable for non-coders.", "AI": {"tldr": "GEMSS\u662f\u4e00\u4e2a\u53d8\u5206\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u7528\u4e8e\u5728n\u226ap\u548c\u9ad8\u76f8\u5173\u6027\u7684\u6b20\u5b9a\u573a\u666f\u4e2d\u540c\u65f6\u53d1\u73b0\u591a\u4e2a\u4e0d\u540c\u7684\u7a00\u758f\u7279\u5f81\u7ec4\u5408\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u53ea\u80fd\u627e\u5230\u5355\u4e00\u89e3\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u6b20\u5b9a\uff08n\u226ap\uff09\u548c\u9ad8\u76f8\u5173\u6027\u573a\u666f\u4e2d\uff0c\u591a\u4e2a\u4e0d\u540c\u7684\u7a00\u758f\u7279\u5f81\u5b50\u96c6\u53ef\u80fd\u540c\u6837\u80fd\u89e3\u91ca\u54cd\u5e94\u3002\u8bc6\u522b\u8fd9\u4e9b\u66ff\u4ee3\u65b9\u6848\u5bf9\u4e8e\u751f\u6210\u9886\u57df\u7279\u5b9a\u89c1\u89e3\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u53ea\u80fd\u627e\u5230\u5355\u4e00\u89e3\uff0c\u63a9\u76d6\u4e86\u5b8c\u6574\u53ef\u80fd\u7684\u89e3\u91ca\u8c31\u7cfb\u3002", "method": "GEMSS\u91c7\u7528\u53d8\u5206\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u4f7f\u7528\u7ed3\u6784\u5316spike-and-slab\u5148\u9a8c\u5b9e\u73b0\u7a00\u758f\u6027\uff0c\u9ad8\u65af\u6df7\u5408\u8fd1\u4f3c\u96be\u4ee5\u5904\u7406\u7684\u591a\u5cf0\u540e\u9a8c\u5206\u5e03\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8eJaccard\u7684\u60e9\u7f5a\u9879\u63a7\u5236\u89e3\u591a\u6837\u6027\u3002\u4e0e\u987a\u5e8f\u8d2a\u5a6a\u65b9\u6cd5\u4e0d\u540c\uff0cGEMSS\u901a\u8fc7\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u5728\u5355\u4e2a\u76ee\u6807\u51fd\u6570\u4e2d\u4f18\u5316\u6574\u4e2a\u89e3\u96c6\u5408\u3002", "result": "\u5728\u5305\u542b128\u4e2a\u5408\u6210\u5b9e\u9a8c\u7684\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGEMSS\u5728\u9ad8\u7ef4\u8bbe\u7f6e\uff08p=5000\uff09\u4e2d\u6709\u6548\u6269\u5c55\uff0c\u6837\u672c\u91cf\u5c0f\u81f3n=50\uff0c\u65e0\u7f1d\u6cdb\u5316\u5230\u8fde\u7eed\u76ee\u6807\uff0c\u539f\u751f\u5904\u7406\u7f3a\u5931\u6570\u636e\uff0c\u5e76\u5bf9\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u9ad8\u65af\u566a\u58f0\u8868\u73b0\u51fa\u663e\u8457\u9c81\u68d2\u6027\u3002", "conclusion": "GEMSS\u662f\u4e00\u4e2a\u6709\u6548\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6b20\u5b9a\u548c\u9ad8\u76f8\u5173\u6027\u573a\u666f\u4e2d\u53d1\u73b0\u591a\u4e2a\u4e0d\u540c\u7684\u7a00\u758f\u7279\u5f81\u7ec4\u5408\uff0c\u5df2\u4f5c\u4e3aPython\u5305'gemss'\u5728PyPI\u4e0a\u63d0\u4f9b\uff0cGitHub\u4ed3\u5e93\u5305\u542b\u9002\u5408\u975e\u7f16\u7801\u4eba\u5458\u4f7f\u7528\u7684\u514d\u8d39\u6613\u7528\u5e94\u7528\u7a0b\u5e8f\u3002"}}
{"id": "2602.08920", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08920", "abs": "https://arxiv.org/abs/2602.08920", "authors": ["Manh Cuong Dao", "Quang Hung Pham", "Phi Le Nguyen", "Thao Nguyen Truong", "Bryan Kian Hsiang Low", "Trong Nghia Hoang"], "title": "Diffusion-Inspired Reconfiguration of Transformers for Uncertainty Calibration", "comment": null, "summary": "Uncertainty calibration in pre-trained transformers is critical for their reliable deployment in risk-sensitive applications. Yet, most existing pre-trained transformers do not have a principled mechanism for uncertainty propagation through their feature transformation stack. In this work, we propose a diffusion-inspired reconfiguration of transformers in which each feature transformation block is modeled as a probabilistic mapping. Composing these probabilistic mappings reveals a probability path that mimics the structure of a diffusion process, transporting data mass from the input distribution to the pre-trained feature distribution. This probability path can then be recompiled on a diffusion process with a unified transition model to enable principled propagation of representation uncertainty throughout the pre-trained model's architecture while maintaining its original predictive performance. Empirical results across a variety of vision and language benchmarks demonstrate that our method achieves superior calibration and predictive accuracy compared to existing uncertainty-aware transformers.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u8fc7\u7a0b\u7684Transformer\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u65b9\u6cd5\uff0c\u901a\u8fc7\u6982\u7387\u6620\u5c04\u5757\u91cd\u6784\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5b9e\u73b0\u8868\u793a\u4e0d\u786e\u5b9a\u6027\u7684\u4f20\u64ad\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u9884\u8bad\u7ec3Transformer\u5728\u98ce\u9669\u654f\u611f\u5e94\u7528\u4e2d\u9700\u8981\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u7f3a\u4e4f\u901a\u8fc7\u7279\u5f81\u53d8\u6362\u5806\u6808\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u7684\u539f\u5219\u6027\u673a\u5236\u3002", "method": "\u5c06Transformer\u7684\u6bcf\u4e2a\u7279\u5f81\u53d8\u6362\u5757\u5efa\u6a21\u4e3a\u6982\u7387\u6620\u5c04\uff0c\u7ec4\u5408\u8fd9\u4e9b\u6620\u5c04\u5f62\u6210\u7c7b\u4f3c\u6269\u6563\u8fc7\u7a0b\u7684\u6982\u7387\u8def\u5f84\uff0c\u7136\u540e\u5c06\u5176\u91cd\u65b0\u7f16\u8bd1\u5230\u5177\u6709\u7edf\u4e00\u8f6c\u79fb\u6a21\u578b\u7684\u6269\u6563\u8fc7\u7a0b\u4e2d\u3002", "result": "\u5728\u591a\u79cd\u89c6\u89c9\u548c\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u611f\u77e5Transformer\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6821\u51c6\u6027\u80fd\u548c\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6269\u6563\u542f\u53d1\u5f0fTransformer\u91cd\u6784\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u539f\u5219\u6027\u7684\u8868\u793a\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u539f\u59cb\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u98ce\u9669\u654f\u611f\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u90e8\u7f72\u65b9\u6848\u3002"}}
{"id": "2602.08923", "categories": ["cs.LG", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.08923", "abs": "https://arxiv.org/abs/2602.08923", "authors": ["Wenchen Han", "Shay Vargaftik", "Michael Mitzenmacher", "Ran Ben Basat"], "title": "DynamiQ: Accelerating Gradient Synchronization using Compressed Multi-hop All-reduce", "comment": "18 pages, 18 figures", "summary": "Multi-hop all-reduce is the de facto backbone of large model training. As the training scale increases, the network often becomes a bottleneck, motivating reducing the volume of transmitted data. Accordingly, recent systems demonstrated significant acceleration of the training process using gradient quantization. However, these systems are not optimized for multi-hop aggregation, where entries are partially summed multiple times along their aggregation topology.\n  This paper presents DynamiQ, a quantization framework that bridges the gap between quantization best practices and multi-hop aggregation. DynamiQ introduces novel techniques to better represent partial sums, co-designed with a decompress-accumulate-recompress fused kernel to facilitate fast execution.\n  We extended PyTorch DDP to support DynamiQ over NCCL P2P, and across different LLMs, tasks, and scales, we demonstrate consistent improvement of up to 34.2% over the best among state-of-the-art methods such as Omni-Reduce, THC, and emerging standards such as MXFP4, MXFP6, and MXFP8. Further, DynamiQ is the only evaluated method that consistently reaches near-baseline accuracy (e.g., 99.9% of the BF16 baseline) and does so while significantly accelerating the training.", "AI": {"tldr": "DynamiQ\u662f\u4e00\u4e2a\u9488\u5bf9\u591a\u8df3\u5168\u5f52\u7ea6\u7684\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u90e8\u5206\u548c\u8868\u793a\u548c\u878d\u5408\u5185\u6838\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u3002", "motivation": "\u968f\u7740\u8bad\u7ec3\u89c4\u6a21\u6269\u5927\uff0c\u7f51\u7edc\u6210\u4e3a\u74f6\u9888\uff0c\u9700\u8981\u51cf\u5c11\u4f20\u8f93\u6570\u636e\u91cf\u3002\u73b0\u6709\u68af\u5ea6\u91cf\u5316\u7cfb\u7edf\u672a\u9488\u5bf9\u591a\u8df3\u805a\u5408\u4f18\u5316\uff0c\u5176\u4e2d\u6761\u76ee\u5728\u805a\u5408\u62d3\u6251\u4e2d\u591a\u6b21\u90e8\u5206\u6c42\u548c\u3002", "method": "\u63d0\u51faDynamiQ\u91cf\u5316\u6846\u67b6\uff0c\u5f15\u5165\u65b0\u6280\u672f\u66f4\u597d\u5730\u8868\u793a\u90e8\u5206\u548c\uff0c\u5e76\u4e0e\u89e3\u538b\u7f29-\u7d2f\u52a0-\u91cd\u65b0\u538b\u7f29\u878d\u5408\u5185\u6838\u534f\u540c\u8bbe\u8ba1\u4ee5\u4fc3\u8fdb\u5feb\u901f\u6267\u884c\u3002\u6269\u5c55PyTorch DDP\u4ee5\u652f\u6301\u901a\u8fc7NCCL P2P\u7684DynamiQ\u3002", "result": "\u5728\u4e0d\u540cLLM\u3001\u4efb\u52a1\u548c\u89c4\u6a21\u4e0b\uff0c\u76f8\u6bd4Omni-Reduce\u3001THC\u3001MXFP4/6/8\u7b49\u6700\u5148\u8fdb\u65b9\u6cd5\uff0cDynamiQ\u5b9e\u73b0\u9ad8\u8fbe34.2%\u7684\u6539\u8fdb\u3002\u662f\u552f\u4e00\u80fd\u6301\u7eed\u8fbe\u5230\u63a5\u8fd1\u57fa\u7ebf\u7cbe\u5ea6\uff08\u5982BF16\u57fa\u7ebf\u768499.9%\uff09\u5e76\u663e\u8457\u52a0\u901f\u8bad\u7ec3\u7684\u65b9\u6cd5\u3002", "conclusion": "DynamiQ\u6210\u529f\u5f25\u5408\u4e86\u91cf\u5316\u6700\u4f73\u5b9e\u8df5\u4e0e\u591a\u8df3\u805a\u5408\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u3002"}}
{"id": "2602.08934", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08934", "abs": "https://arxiv.org/abs/2602.08934", "authors": ["Suraj Ranganath", "Atharv Ramesh"], "title": "StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors", "comment": "Expanded version of a workshop submission. Code available", "summary": "AI-text detectors face a critical robustness challenge: adversarial paraphrasing attacks that preserve semantics while evading detection. We introduce StealthRL, a reinforcement learning framework that stress-tests detector robustness under realistic adversarial conditions. StealthRL trains a paraphrase policy against a multi-detector ensemble using Group Relative Policy Optimization (GRPO) with LoRA adapters on Qwen3-4B, optimizing a composite reward that balances detector evasion with semantic preservation. We evaluate six attack settings (M0-M5) against three detector families (RoBERTa, FastDetectGPT, and Binoculars) at the security-relevant 1% false positive rate operating point. StealthRL achieves near-zero detection (0.001 mean TPR@1%FPR), reduces mean AUROC from 0.74 to 0.27, and attains a 99.9% attack success rate. Critically, attacks transfer to a held-out detector family not seen during training, revealing shared architectural vulnerabilities rather than detector-specific brittleness. We additionally conduct LLM-based quality evaluation via Likert scoring, analyze detector score distributions to explain why evasion succeeds, and provide per-detector AUROC with bootstrap confidence intervals. Our results expose significant robustness gaps in current AI-text detection and establish StealthRL as a principled adversarial evaluation protocol. Code and evaluation pipeline are publicly available at https://github.com/suraj-ranganath/StealthRL.", "AI": {"tldr": "StealthRL\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5bf9\u6297\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u8bed\u4e49\u4fdd\u6301\u7684\u524d\u63d0\u4e0b\u5bf9AI\u6587\u672c\u68c0\u6d4b\u5668\u8fdb\u884c\u5bf9\u6297\u6027\u6539\u5199\u653b\u51fb\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u68c0\u6d4b\u5668\u5b58\u5728\u7684\u4e25\u91cd\u9c81\u68d2\u6027\u6f0f\u6d1e\u3002", "motivation": "AI\u6587\u672c\u68c0\u6d4b\u5668\u9762\u4e34\u5173\u952e\u7684\u9c81\u68d2\u6027\u6311\u6218\uff1a\u5bf9\u6297\u6027\u6539\u5199\u653b\u51fb\u53ef\u4ee5\u5728\u4fdd\u6301\u8bed\u4e49\u7684\u540c\u65f6\u9003\u907f\u68c0\u6d4b\u3002\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u6d4b\u8bd5\u68c0\u6d4b\u5668\u5728\u771f\u5b9e\u5bf9\u6297\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6StealthRL\uff0c\u57fa\u4e8eQwen3-4B\u6a21\u578b\u548cLoRA\u9002\u914d\u5668\uff0c\u91c7\u7528Group Relative Policy Optimization (GRPO)\u8bad\u7ec3\u6539\u5199\u7b56\u7565\u3002\u9488\u5bf9\u591a\u68c0\u6d4b\u5668\u96c6\u6210\u4f18\u5316\u590d\u5408\u5956\u52b1\u51fd\u6570\uff0c\u5e73\u8861\u68c0\u6d4b\u9003\u907f\u548c\u8bed\u4e49\u4fdd\u6301\u3002\u8bc4\u4f30\u4e866\u79cd\u653b\u51fb\u8bbe\u7f6e(M0-M5)\u5bf9\u62973\u7c7b\u68c0\u6d4b\u5668\u5bb6\u65cf\u3002", "result": "StealthRL\u5b9e\u73b0\u4e86\u63a5\u8fd1\u96f6\u7684\u68c0\u6d4b\u7387(\u5e73\u5747TPR@1%FPR\u4e3a0.001)\uff0c\u5c06\u5e73\u5747AUROC\u4ece0.74\u964d\u81f30.27\uff0c\u653b\u51fb\u6210\u529f\u7387\u9ad8\u8fbe99.9%\u3002\u653b\u51fb\u8fd8\u80fd\u8fc1\u79fb\u5230\u8bad\u7ec3\u4e2d\u672a\u89c1\u8fc7\u7684\u68c0\u6d4b\u5668\u5bb6\u65cf\uff0c\u63ed\u793a\u4e86\u5171\u4eab\u7684\u67b6\u6784\u6f0f\u6d1e\u800c\u975e\u7279\u5b9a\u68c0\u6d4b\u5668\u7684\u8106\u5f31\u6027\u3002", "conclusion": "\u5f53\u524dAI\u6587\u672c\u68c0\u6d4b\u5668\u5b58\u5728\u663e\u8457\u7684\u9c81\u68d2\u6027\u7f3a\u9677\uff0cStealthRL\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u7684\u5bf9\u6297\u6027\u8bc4\u4f30\u534f\u8bae\uff0c\u53ef\u4f5c\u4e3a\u68c0\u6d4b\u5668\u5b89\u5168\u8bc4\u4f30\u7684\u6807\u51c6\u5de5\u5177\u3002\u4ee3\u7801\u548c\u8bc4\u4f30\u6d41\u7a0b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.08964", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.08964", "abs": "https://arxiv.org/abs/2602.08964", "authors": ["Raghu Arghal", "Fade Chen", "Niall Dalton", "Evgenii Kortukov", "Calum McNamara", "Angelos Nalmpantis", "Moksh Nirvaan", "Gabriele Sarti", "Mario Giulianelli"], "title": "A Behavioural and Representational Evaluation of Goal-Directedness in Language Model Agents", "comment": null, "summary": "Understanding an agent's goals helps explain and predict its behaviour, yet there is no established methodology for reliably attributing goals to agentic systems. We propose a framework for evaluating goal-directedness that integrates behavioural evaluation with interpretability-based analyses of models' internal representations. As a case study, we examine an LLM agent navigating a 2D grid world toward a goal state. Behaviourally, we evaluate the agent against an optimal policy across varying grid sizes, obstacle densities, and goal structures, finding that performance scales with task difficulty while remaining robust to difficulty-preserving transformations and complex goal structures. We then use probing methods to decode the agent's internal representations of the environment state and its multi-step action plans. We find that the LLM agent non-linearly encodes a coarse spatial map of the environment, preserving approximate task-relevant cues about its position and the goal location; that its actions are broadly consistent with these internal representations; and that reasoning reorganises them, shifting from broader environment structural cues toward information supporting immediate action selection. Our findings support the view that introspective examination is required beyond behavioural evaluations to characterise how agents represent and pursue their objectives.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u667a\u80fd\u4f53\u76ee\u6807\u5bfc\u5411\u6027\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u884c\u4e3a\u8bc4\u4f30\u548c\u53ef\u89e3\u91ca\u6027\u5206\u6790\uff0c\u901a\u8fc7LLM\u667a\u80fd\u4f53\u57282D\u7f51\u683c\u4e16\u754c\u4e2d\u7684\u5bfc\u822a\u6848\u4f8b\uff0c\u53d1\u73b0\u667a\u80fd\u4f53\u5185\u90e8\u975e\u7ebf\u6027\u7f16\u7801\u73af\u5883\u7a7a\u95f4\u5730\u56fe\uff0c\u5e76\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u91cd\u7ec4\u8868\u5f81\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u5f52\u56e0\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u76ee\u6807\uff0c\u7406\u89e3\u667a\u80fd\u4f53\u7684\u76ee\u6807\u6709\u52a9\u4e8e\u89e3\u91ca\u548c\u9884\u6d4b\u5176\u884c\u4e3a\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u8bc4\u4f30\u76ee\u6807\u5bfc\u5411\u6027\u7684\u65b9\u6cd5\u8bba\u3002", "method": "\u63d0\u51fa\u7ed3\u5408\u884c\u4e3a\u8bc4\u4f30\u548c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u7684\u6846\u67b6\u3002\u884c\u4e3a\u4e0a\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u5728\u4e0d\u540c\u7f51\u683c\u5927\u5c0f\u3001\u969c\u788d\u5bc6\u5ea6\u548c\u76ee\u6807\u7ed3\u6784\u4e0b\u7684\u8868\u73b0\uff1b\u53ef\u89e3\u91ca\u6027\u4e0a\u4f7f\u7528\u63a2\u6d4b\u65b9\u6cd5\u89e3\u7801\u667a\u80fd\u4f53\u5bf9\u73af\u5883\u72b6\u6001\u548c\u591a\u6b65\u884c\u52a8\u8ba1\u5212\u7684\u5185\u90e8\u8868\u5f81\u3002", "result": "LLM\u667a\u80fd\u4f53\u6027\u80fd\u968f\u4efb\u52a1\u96be\u5ea6\u589e\u52a0\u800c\u63d0\u5347\uff0c\u4f46\u5bf9\u4fdd\u6301\u96be\u5ea6\u7684\u53d8\u6362\u548c\u590d\u6742\u76ee\u6807\u7ed3\u6784\u4fdd\u6301\u9c81\u68d2\u6027\u3002\u667a\u80fd\u4f53\u5185\u90e8\u975e\u7ebf\u6027\u7f16\u7801\u73af\u5883\u7684\u7c97\u7565\u7a7a\u95f4\u5730\u56fe\uff0c\u4fdd\u7559\u4f4d\u7f6e\u548c\u76ee\u6807\u7684\u8fd1\u4f3c\u4efb\u52a1\u76f8\u5173\u7ebf\u7d22\uff0c\u5176\u884c\u52a8\u4e0e\u8fd9\u4e9b\u5185\u90e8\u8868\u5f81\u57fa\u672c\u4e00\u81f4\uff0c\u63a8\u7406\u8fc7\u7a0b\u4f1a\u91cd\u7ec4\u8868\u5f81\uff0c\u4ece\u73af\u5883\u7ed3\u6784\u7ebf\u7d22\u8f6c\u5411\u652f\u6301\u5373\u65f6\u884c\u52a8\u9009\u62e9\u7684\u4fe1\u606f\u3002", "conclusion": "\u4ec5\u9760\u884c\u4e3a\u8bc4\u4f30\u4e0d\u8db3\u4ee5\u8868\u5f81\u667a\u80fd\u4f53\u5982\u4f55\u8868\u793a\u548c\u8ffd\u6c42\u76ee\u6807\uff0c\u9700\u8981\u7ed3\u5408\u5185\u7701\u68c0\u67e5\u6765\u5168\u9762\u7406\u89e3\u667a\u80fd\u4f53\u7684\u76ee\u6807\u5bfc\u5411\u6027\u3002"}}
{"id": "2602.08976", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08976", "abs": "https://arxiv.org/abs/2602.08976", "authors": ["Jiaqi Wen", "Jianyi Yang"], "title": "Distributionally Robust Optimization via Generative Ambiguity Modeling", "comment": null, "summary": "This paper studies Distributionally Robust Optimization (DRO), a fundamental framework for enhancing the robustness and generalization of statistical learning and optimization. An effective ambiguity set for DRO must involve distributions that remain consistent to the nominal distribution while being diverse enough to account for a variety of potential scenarios. Moreover, it should lead to tractable DRO solutions. To this end, we propose generative model-based ambiguity sets that capture various adversarial distributions beyond the nominal support space while maintaining consistency with the nominal distribution. Building on this generative ambiguity modeling, we propose DRO with Generative Ambiguity Set (GAS-DRO), a tractable DRO algorithm that solves the inner maximization over the parameterized generative model space. We formally establish the stationary convergence performance of GAS-DRO. We implement GAS-DRO with a diffusion model and empirically demonstrate its superior Out-of-Distribution (OOD) generalization performance in ML tasks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u5206\u5e03\u9c81\u68d2\u4f18\u5316(GAS-DRO)\uff0c\u4f7f\u7528\u751f\u6210\u6a21\u578b\u6784\u5efa\u6b67\u4e49\u96c6\uff0c\u5728\u4fdd\u6301\u4e0e\u540d\u4e49\u5206\u5e03\u4e00\u81f4\u6027\u7684\u540c\u65f6\u6355\u83b7\u8d85\u51fa\u540d\u4e49\u652f\u6301\u7a7a\u95f4\u7684\u5bf9\u6297\u5206\u5e03\uff0c\u5b9e\u73b0\u53ef\u5904\u7406\u7684DRO\u6c42\u89e3\u548c\u4f18\u8d8a\u7684OOD\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfDRO\u7684\u6b67\u4e49\u96c6\u9700\u8981\u540c\u65f6\u6ee1\u8db3\u4e24\u4e2a\u5173\u952e\u8981\u6c42\uff1a1) \u4fdd\u6301\u4e0e\u540d\u4e49\u5206\u5e03\u7684\u4e00\u81f4\u6027\uff1b2) \u8db3\u591f\u591a\u6837\u5316\u4ee5\u6db5\u76d6\u5404\u79cd\u6f5c\u5728\u573a\u666f\uff1b3) \u80fd\u591f\u5bfc\u51fa\u53ef\u5904\u7406\u7684DRO\u89e3\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u540c\u65f6\u6ee1\u8db3\u8fd9\u4e9b\u8981\u6c42\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u6b67\u4e49\u96c6\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u751f\u6210\u6a21\u578b\u7a7a\u95f4\u6355\u83b7\u8d85\u51fa\u540d\u4e49\u652f\u6301\u7a7a\u95f4\u7684\u5bf9\u6297\u5206\u5e03\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51faGAS-DRO\u7b97\u6cd5\uff0c\u5728\u751f\u6210\u6a21\u578b\u53c2\u6570\u7a7a\u95f4\u5185\u6c42\u89e3\u5185\u90e8\u6700\u5927\u5316\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u3002", "result": "\u7406\u8bba\u4e0a\u5efa\u7acb\u4e86GAS-DRO\u7684\u5e73\u7a33\u6536\u655b\u6027\u80fd\u3002\u5b9e\u8bc1\u4e0a\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5b9e\u73b0GAS-DRO\uff0c\u5728\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u5206\u5e03\u5916(OOD)\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u751f\u6210\u6a21\u578b\u6b67\u4e49\u96c6\u4e3aDRO\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u6846\u67b6\uff0c\u65e2\u80fd\u4fdd\u6301\u5206\u5e03\u4e00\u81f4\u6027\uff0c\u53c8\u80fd\u6355\u83b7\u591a\u6837\u5316\u5bf9\u6297\u573a\u666f\uff0c\u540c\u65f6\u4fdd\u8bc1\u7b97\u6cd5\u53ef\u5904\u7406\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86OOD\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.08983", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08983", "abs": "https://arxiv.org/abs/2602.08983", "authors": ["Yubin Kim", "Viresh Pati", "Jevon Twitty", "Vinh Pham", "Shihao Yang", "Jiecheng Lu"], "title": "StretchTime: Adaptive Time Series Forecasting via Symplectic Attention", "comment": null, "summary": "Transformer architectures have established strong baselines in time series forecasting, yet they typically rely on positional encodings that assume uniform, index-based temporal progression. However, real-world systems, from shifting financial cycles to elastic biological rhythms, frequently exhibit \"time-warped\" dynamics where the effective flow of time decouples from the sampling index. In this work, we first formalize this misalignment and prove that rotary position embedding (RoPE) is mathematically incapable of representing non-affine temporal warping. To address this, we propose Symplectic Positional Embeddings (SyPE), a learnable encoding framework derived from Hamiltonian mechanics. SyPE strictly generalizes RoPE by extending the rotation group $\\mathrm{SO}(2)$ to the symplectic group $\\mathrm{Sp}(2,\\mathbb{R})$, modulated by a novel input-dependent adaptive warp module. By allowing the attention mechanism to adaptively dilate or contract temporal coordinates end-to-end, our approach captures locally varying periodicities without requiring pre-defined warping functions. We implement this mechanism in StretchTime, a multivariate forecasting architecture that achieves state-of-the-art performance on standard benchmarks, demonstrating superior robustness on datasets exhibiting non-stationary temporal dynamics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Symplectic Positional Embeddings (SyPE)\u6765\u89e3\u51b3\u4f20\u7edfTransformer\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u65e0\u6cd5\u5904\u7406\u65f6\u95f4\u626d\u66f2\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u65cb\u8f6c\u7fa4\u6269\u5c55\u5230\u8f9b\u7fa4\uff0c\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u7684\u65f6\u95f4\u5750\u6807\u4f38\u7f29\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u65f6\u95f4\u5e8f\u5217\uff08\u5982\u91d1\u878d\u5468\u671f\u3001\u751f\u7269\u8282\u5f8b\uff09\u7ecf\u5e38\u8868\u73b0\u51fa\"\u65f6\u95f4\u626d\u66f2\"\u7684\u52a8\u6001\u7279\u6027\uff0c\u5373\u6709\u6548\u65f6\u95f4\u6d41\u4e0e\u91c7\u6837\u7d22\u5f15\u89e3\u8026\u3002\u4f20\u7edf\u7684Transformer\u4f4d\u7f6e\u7f16\u7801\uff08\u5982RoPE\uff09\u5047\u8bbe\u5747\u5300\u7684\u65f6\u95f4\u8fdb\u5c55\uff0c\u65e0\u6cd5\u8868\u793a\u975e\u4eff\u5c04\u7684\u65f6\u95f4\u626d\u66f2\u3002", "method": "\u63d0\u51fa\u4e86Symplectic Positional Embeddings (SyPE)\uff0c\u4ece\u54c8\u5bc6\u987f\u529b\u5b66\u63a8\u5bfc\u51fa\u7684\u53ef\u5b66\u4e60\u7f16\u7801\u6846\u67b6\u3002SyPE\u901a\u8fc7\u5c06\u65cb\u8f6c\u7fa4SO(2)\u6269\u5c55\u5230\u8f9b\u7fa4Sp(2,R)\u6765\u4e25\u683c\u63a8\u5e7fRoPE\uff0c\u5e76\u901a\u8fc7\u65b0\u9896\u7684\u8f93\u5165\u4f9d\u8d56\u81ea\u9002\u5e94\u626d\u66f2\u6a21\u5757\u8fdb\u884c\u8c03\u5236\u3002\u8be5\u65b9\u6cd5\u5141\u8bb8\u6ce8\u610f\u529b\u673a\u5236\u7aef\u5230\u7aef\u5730\u81ea\u9002\u5e94\u6269\u5f20\u6216\u6536\u7f29\u65f6\u95f4\u5750\u6807\u3002", "result": "\u5728StretchTime\u67b6\u6784\u4e2d\u5b9e\u73b0\u4e86\u8be5\u673a\u5236\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u8868\u73b0\u51fa\u975e\u5e73\u7a33\u65f6\u95f4\u52a8\u6001\u7684\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "SyPE\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u65f6\u95f4\u626d\u66f2\u95ee\u9898\uff0c\u901a\u8fc7\u8f9b\u7fa4\u6269\u5c55\u548c\u81ea\u9002\u5e94\u626d\u66f2\u6a21\u5757\uff0c\u80fd\u591f\u6355\u6349\u5c40\u90e8\u53d8\u5316\u7684\u5468\u671f\u6027\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u7684\u626d\u66f2\u51fd\u6570\uff0c\u5728\u975e\u5e73\u7a33\u65f6\u95f4\u52a8\u6001\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.08986", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08986", "abs": "https://arxiv.org/abs/2602.08986", "authors": ["Isaac Xu", "Martin Gillis", "Ayushi Sharma", "Benjamin Misiuk", "Craig J. Brown", "Thomas Trappenberg"], "title": "Improving Detection of Rare Nodes in Hierarchical Multi-Label Learning", "comment": "Accepted for publication in Transactions on Machine Learning Research (TMLR), 2026", "summary": "In hierarchical multi-label classification, a persistent challenge is enabling model predictions to reach deeper levels of the hierarchy for more detailed or fine-grained classifications. This difficulty partly arises from the natural rarity of certain classes (or hierarchical nodes) and the hierarchical constraint that ensures child nodes are almost always less frequent than their parents. To address this, we propose a weighted loss objective for neural networks that combines node-wise imbalance weighting with focal weighting components, the latter leveraging modern quantification of ensemble uncertainties. By emphasizing rare nodes rather than rare observations (data points), and focusing on uncertain nodes for each model output distribution during training, we observe improvements in recall by up to a factor of five on benchmark datasets, along with statistically significant gains in $F_{1}$ score. We also show our approach aids convolutional networks on challenging tasks, as in situations with suboptimal encoders or limited data.", "AI": {"tldr": "\u63d0\u51fa\u52a0\u6743\u635f\u5931\u51fd\u6570\u89e3\u51b3\u5c42\u6b21\u591a\u6807\u7b7e\u5206\u7c7b\u4e2d\u6df1\u5c42\u8282\u70b9\u9884\u6d4b\u56f0\u96be\u95ee\u9898\uff0c\u7ed3\u5408\u8282\u70b9\u4e0d\u5e73\u8861\u52a0\u6743\u548c\u7126\u70b9\u52a0\u6743\uff0c\u63d0\u5347\u7f55\u89c1\u8282\u70b9\u7684\u53ec\u56de\u7387", "motivation": "\u5c42\u6b21\u591a\u6807\u7b7e\u5206\u7c7b\u4e2d\uff0c\u6a21\u578b\u96be\u4ee5\u9884\u6d4b\u66f4\u6df1\u5c42\u6b21\u7684\u8282\u70b9\uff0c\u56e0\u4e3a\u67d0\u4e9b\u7c7b\u522b\uff08\u6216\u5c42\u6b21\u8282\u70b9\uff09\u5929\u7136\u7a00\u6709\uff0c\u4e14\u5b50\u8282\u70b9\u51e0\u4e4e\u603b\u662f\u6bd4\u7236\u8282\u70b9\u66f4\u4e0d\u9891\u7e41", "method": "\u63d0\u51fa\u795e\u7ecf\u7f51\u7edc\u52a0\u6743\u635f\u5931\u76ee\u6807\uff0c\u7ed3\u5408\u8282\u70b9\u4e0d\u5e73\u8861\u52a0\u6743\u548c\u7126\u70b9\u52a0\u6743\u7ec4\u4ef6\uff0c\u540e\u8005\u5229\u7528\u96c6\u6210\u4e0d\u786e\u5b9a\u6027\u7684\u73b0\u4ee3\u91cf\u5316\u65b9\u6cd5\uff0c\u5f3a\u8c03\u7f55\u89c1\u8282\u70b9\u800c\u975e\u7f55\u89c1\u89c2\u6d4b\uff0c\u5e76\u5173\u6ce8\u6bcf\u4e2a\u6a21\u578b\u8f93\u51fa\u5206\u5e03\u4e2d\u7684\u4e0d\u786e\u5b9a\u8282\u70b9", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53ec\u56de\u7387\u63d0\u5347\u9ad8\u8fbe5\u500d\uff0cF1\u5206\u6570\u6709\u7edf\u8ba1\u663e\u8457\u63d0\u5347\uff0c\u5728\u5177\u6709\u6b21\u4f18\u7f16\u7801\u5668\u6216\u6709\u9650\u6570\u636e\u7684\u6311\u6218\u6027\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u5377\u79ef\u7f51\u7edc", "conclusion": "\u901a\u8fc7\u5f3a\u8c03\u7f55\u89c1\u8282\u70b9\u548c\u5173\u6ce8\u4e0d\u786e\u5b9a\u8282\u70b9\u7684\u52a0\u6743\u635f\u5931\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5c42\u6b21\u591a\u6807\u7b7e\u5206\u7c7b\u4e2d\u6df1\u5c42\u8282\u70b9\u9884\u6d4b\u7684\u6311\u6218"}}
{"id": "2602.09001", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09001", "abs": "https://arxiv.org/abs/2602.09001", "authors": ["Amirhossein Vahidi", "Hesam Asadollahzadeh", "Navid Akhavan Attar", "Marie Moullet", "Kevin Ly", "Xingyi Yang", "Mohammad Lotfollahi"], "title": "DirMoE: Dirichlet-routed Mixture of Experts", "comment": null, "summary": "Mixture-of-Experts (MoE) models have demonstrated exceptional performance in large-scale language models. Existing routers typically rely on non-differentiable Top-$k$+Softmax, limiting their performance and scalability. We argue that two distinct decisions, which experts to activate and how to distribute expert contributions among them, are conflated in standard Top-$k$+Softmax. We introduce Dirichlet-Routed MoE (DirMoE), a novel end-to-end differentiable routing mechanism built on a Dirichlet variational autoencoder framework. This design fundamentally disentangles the core routing problems: expert selection, modeled by a Bernoulli component, and expert contribution among chosen experts, handled by a Dirichlet component. The entire forward pass remains fully differentiable through the use of Gumbel-Sigmoid relaxation for the expert selection and implicit reparameterization for the Dirichlet distribution. Our training objective, a variational ELBO, includes a direct sparsity penalty that precisely controls the number of active experts in expectation, alongside a schedule for key hyperparameters that guides the model from an exploratory to a definitive routing state. Moreover, our DirMoE router matches or exceeds other methods while improving expert specialization.", "AI": {"tldr": "\u63d0\u51faDirMoE\uff0c\u4e00\u7a2e\u57fa\u65bcDirichlet\u8b8a\u5206\u81ea\u7de8\u78bc\u5668\u7684\u7aef\u5230\u7aef\u53ef\u5fae\u5206\u8def\u7531\u6a5f\u5236\uff0c\u89e3\u6c7a\u50b3\u7d71Top-k+Softmax\u8def\u7531\u4e2d\u5c08\u5bb6\u9078\u64c7\u8207\u8ca2\u737b\u5206\u914d\u6df7\u96dc\u7684\u554f\u984c\u3002", "motivation": "\u73fe\u6709MoE\u6a21\u578b\u4f7f\u7528\u4e0d\u53ef\u5fae\u5206\u7684Top-k+Softmax\u8def\u7531\uff0c\u9650\u5236\u4e86\u6027\u80fd\u548c\u53ef\u64f4\u5c55\u6027\u3002\u50b3\u7d71\u65b9\u6cd5\u5c07\u5c08\u5bb6\u9078\u64c7\u548c\u5c08\u5bb6\u8ca2\u737b\u5206\u914d\u5169\u500b\u4e0d\u540c\u6c7a\u7b56\u6df7\u70ba\u4e00\u8ac7\u3002", "method": "\u63d0\u51faDirMoE\uff0c\u4f7f\u7528Dirichlet\u8b8a\u5206\u81ea\u7de8\u78bc\u5668\u6846\u67b6\uff1aBernoulli\u5206\u91cf\u5efa\u6a21\u5c08\u5bb6\u9078\u64c7\uff0cDirichlet\u5206\u91cf\u8655\u7406\u5c08\u5bb6\u8ca2\u737b\u5206\u914d\u3002\u4f7f\u7528Gumbel-Sigmoid\u9b06\u5f1b\u5be6\u73fe\u5c08\u5bb6\u9078\u64c7\u53ef\u5fae\uff0cDirichlet\u5206\u5e03\u4f7f\u7528\u96b1\u5f0f\u91cd\u53c3\u6578\u5316\u3002\u8a13\u7df4\u76ee\u6a19\u70ba\u8b8a\u5206ELBO\uff0c\u5305\u542b\u76f4\u63a5\u7a00\u758f\u61f2\u7f70\u4ee5\u63a7\u5236\u6d3b\u8e8d\u5c08\u5bb6\u6578\u91cf\u3002", "result": "DirMoE\u8def\u7531\u6a5f\u5236\u5339\u914d\u6216\u8d85\u8d8a\u5176\u4ed6\u65b9\u6cd5\uff0c\u540c\u6642\u63d0\u9ad8\u4e86\u5c08\u5bb6\u5c08\u696d\u5316\u7a0b\u5ea6\u3002\u6574\u500b\u524d\u5411\u50b3\u64ad\u4fdd\u6301\u5b8c\u5168\u53ef\u5fae\u5206\u3002", "conclusion": "DirMoE\u901a\u904e\u89e3\u8026\u5c08\u5bb6\u9078\u64c7\u548c\u8ca2\u737b\u5206\u914d\uff0c\u63d0\u4f9b\u4e86\u4e00\u7a2e\u7aef\u5230\u7aef\u53ef\u5fae\u5206\u7684\u8def\u7531\u6a5f\u5236\uff0c\u6539\u5584\u4e86MoE\u6a21\u578b\u7684\u6027\u80fd\u548c\u5c08\u5bb6\u5c08\u696d\u5316\u3002"}}
{"id": "2602.09006", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.09006", "abs": "https://arxiv.org/abs/2602.09006", "authors": ["Wenbo Gong", "Javier Zazo", "Qijun Luo", "Puqian Wang", "James Hensman", "Chao Ma"], "title": "ARO: A New Lens On Matrix Optimization For Large Models", "comment": null, "summary": "Matrix-based optimizers have attracted growing interest for improving LLM training efficiency, with significant progress centered on orthogonalization/whitening based methods. While yielding substantial performance gains, a fundamental question arises: can we develop new paradigms beyond orthogonalization, pushing the efficiency frontier further? We present \\textbf{Adaptively Rotated Optimization (ARO}, a new matrix optimization framework that treats gradient rotation as a first class design principle. ARO accelerates LLM training by performing normed steepest descent in a rotated coordinate system, where the rotation is determined by a novel norm-informed policy. This perspective yields update rules that go beyond existing orthogonalization and whitening optimizers, improving sample efficiency in practice. To make comparisons reliable, we propose a rigorously controlled benchmarking protocol that reduces confounding and bias. Under this protocol, ARO consistently outperforms AdamW (by 1.3 $\\sim$1.35$\\times$) and orthogonalization methods (by 1.1$\\sim$1.15$\\times$) in LLM pretraining at up to 8B activated parameters, and up to $8\\times$ overtrain budget, without evidence of diminishing returns. Finally, we discuss how ARO can be reformulated as a symmetry-aware optimizer grounded in rotational symmetries of residual streams, motivating advanced designs that enable computationally efficient exploitation of cross-layer/cross module couplings.", "AI": {"tldr": "ARO\u662f\u4e00\u79cd\u65b0\u7684\u77e9\u9635\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u68af\u5ea6\u65cb\u8f6c\u4f5c\u4e3a\u6838\u5fc3\u8bbe\u8ba1\u539f\u5219\uff0c\u5728\u65cb\u8f6c\u5750\u6807\u7cfb\u4e2d\u6267\u884c\u8303\u6570\u6700\u901f\u4e0b\u964d\uff0c\u8d85\u8d8a\u73b0\u6709\u6b63\u4ea4\u5316/\u767d\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347LLM\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6b63\u4ea4\u5316/\u767d\u5316\u7684\u77e9\u9635\u4f18\u5316\u5668\u867d\u7136\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22\u8d85\u8d8a\u6b63\u4ea4\u5316\u7684\u65b0\u8303\u5f0f\uff0c\u8fdb\u4e00\u6b65\u63a8\u52a8\u8bad\u7ec3\u6548\u7387\u7684\u8fb9\u754c\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u65cb\u8f6c\u4f18\u5316(ARO)\u6846\u67b6\uff0c\u5c06\u68af\u5ea6\u65cb\u8f6c\u4f5c\u4e3a\u9996\u8981\u8bbe\u8ba1\u539f\u5219\uff0c\u5728\u65cb\u8f6c\u5750\u6807\u7cfb\u4e2d\u6267\u884c\u8303\u6570\u6700\u901f\u4e0b\u964d\uff0c\u65cb\u8f6c\u7531\u65b0\u9896\u7684\u8303\u6570\u611f\u77e5\u7b56\u7565\u51b3\u5b9a\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6b63\u4ea4\u5316\u548c\u767d\u5316\u4f18\u5316\u5668\u3002", "result": "\u5728\u4e25\u683c\u63a7\u5236\u7684\u57fa\u51c6\u6d4b\u8bd5\u534f\u8bae\u4e0b\uff0cARO\u5728LLM\u9884\u8bad\u7ec3\u4e2d\u59cb\u7ec8\u4f18\u4e8eAdamW(1.3-1.35\u500d)\u548c\u6b63\u4ea4\u5316\u65b9\u6cd5(1.1-1.15\u500d)\uff0c\u53c2\u6570\u89c4\u6a21\u8fbe80\u4ebf\u6fc0\u6d3b\u53c2\u6570\uff0c\u8fc7\u8bad\u7ec3\u9884\u7b97\u8fbe8\u500d\uff0c\u672a\u89c1\u6536\u76ca\u9012\u51cf\u3002", "conclusion": "ARO\u4f5c\u4e3a\u4e00\u79cd\u57fa\u4e8e\u6b8b\u5dee\u6d41\u65cb\u8f6c\u5bf9\u79f0\u6027\u7684\u5bf9\u79f0\u611f\u77e5\u4f18\u5316\u5668\uff0c\u4e3a\u5229\u7528\u8de8\u5c42/\u8de8\u6a21\u5757\u8026\u5408\u7684\u8ba1\u7b97\u9ad8\u6548\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u63a8\u52a8\u4e86\u77e9\u9635\u4f18\u5316\u8303\u5f0f\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.09008", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09008", "abs": "https://arxiv.org/abs/2602.09008", "authors": ["Sijia Peng", "Yun Xiong", "Xi Chen", "Yi Xie", "Guanzhi Li", "Yanwei Yu", "Yangyong Zhu", "Zhiqiang Shen"], "title": "ShapeCond: Fast Shapelet-Guided Dataset Condensation for Time Series Classification", "comment": "Code at: https://github.com/lunaaa95/ShapeCond", "summary": "Time series data supports many domains (e.g., finance and climate science), but its rapid growth strains storage and computation. Dataset condensation can alleviate this by synthesizing a compact training set that preserves key information. Yet most condensation methods are image-centric and often fail on time series because they miss time-series-specific temporal structure, especially local discriminative motifs such as shapelets. In this work, we propose ShapeCond, a novel and efficient condensation framework for time series classification that leverages shapelet-based dataset knowledge via a shapelet-guided optimization strategy. Our shapelet-assisted synthesis cost is independent of sequence length: longer series yield larger speedups in synthesis (e.g., 29$\\times$ faster over prior state-of-the-art method CondTSC for time-series condensation, and up to 10,000$\\times$ over naively using shapelets on the Sleep dataset with 3,000 timesteps). By explicitly preserving critical local patterns, ShapeCond improves downstream accuracy and consistently outperforms all prior state-of-the-art time series dataset condensation methods across extensive experiments. Code is available at https://github.com/lunaaa95/ShapeCond.", "AI": {"tldr": "ShapeCond\u662f\u4e00\u4e2a\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u7684\u9ad8\u6548\u6570\u636e\u96c6\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u5f62\u72b6\u57fa\u5143\u5f15\u5bfc\u7684\u4f18\u5316\u7b56\u7565\u6765\u4fdd\u7559\u5173\u952e\u5c40\u90e8\u6a21\u5f0f\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u4e0a\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5feb\u901f\u589e\u957f\u7ed9\u5b58\u50a8\u548c\u8ba1\u7b97\u5e26\u6765\u538b\u529b\uff0c\u73b0\u6709\u6570\u636e\u96c6\u538b\u7f29\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u56fe\u50cf\u9886\u57df\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u7279\u6709\u7684\u65f6\u95f4\u7ed3\u6784\u548c\u5c40\u90e8\u5224\u522b\u6a21\u5f0f\uff08\u5982\u5f62\u72b6\u57fa\u5143\uff09\u3002", "method": "\u63d0\u51faShapeCond\u6846\u67b6\uff0c\u5229\u7528\u5f62\u72b6\u57fa\u5143\u5f15\u5bfc\u7684\u4f18\u5316\u7b56\u7565\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u538b\u7f29\uff0c\u5408\u6210\u6210\u672c\u4e0e\u5e8f\u5217\u957f\u5ea6\u65e0\u5173\uff0c\u80fd\u591f\u663e\u5f0f\u4fdd\u7559\u5173\u952e\u7684\u5c40\u90e8\u6a21\u5f0f\u3002", "result": "ShapeCond\u5728\u5408\u6210\u901f\u5ea6\u4e0a\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5CondTSC\u5feb29\u500d\uff0c\u5728Sleep\u6570\u636e\u96c6\uff083000\u4e2a\u65f6\u95f4\u6b65\uff09\u4e0a\u6bd4\u76f4\u63a5\u4f7f\u7528\u5f62\u72b6\u57fa\u5143\u5feb10000\u500d\uff0c\u5728\u4e0b\u6e38\u5206\u7c7b\u51c6\u786e\u6027\u4e0a\u6301\u7eed\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u538b\u7f29\u65b9\u6cd5\u3002", "conclusion": "ShapeCond\u901a\u8fc7\u5f62\u72b6\u57fa\u5143\u5f15\u5bfc\u7684\u4f18\u5316\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u538b\u7f29\u95ee\u9898\uff0c\u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6570\u636e\u538b\u7f29\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.09009", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09009", "abs": "https://arxiv.org/abs/2602.09009", "authors": ["Yilang Zhang", "Bingcong Li", "Niao He", "Georgios B. Giannakis"], "title": "ANCRe: Adaptive Neural Connection Reassignment for Efficient Depth Scaling", "comment": null, "summary": "Scaling network depth has been a central driver behind the success of modern foundation models, yet recent investigations suggest that deep layers are often underutilized. This paper revisits the default mechanism for deepening neural networks, namely residual connections, from an optimization perspective. Rigorous analysis proves that the layout of residual connections can fundamentally shape convergence behavior, and even induces an exponential gap in convergence rates. Prompted by this insight, we introduce adaptive neural connection reassignment (ANCRe), a principled and lightweight framework that parameterizes and learns residual connectivities from the data. ANCRe adaptively reassigns residual connections with negligible computational and memory overhead ($<1\\%$), while enabling more effective utilization of network depth. Extensive numerical tests across pre-training of large language models, diffusion models, and deep ResNets demonstrate consistently accelerated convergence, boosted performance, and enhanced depth efficiency over conventional residual connections.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u81ea\u9002\u5e94\u795e\u7ecf\u8fde\u63a5\u91cd\u5206\u914d\uff08ANCRe\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u548c\u5b66\u4e60\u6b8b\u5dee\u8fde\u63a5\u7ed3\u6784\u6765\u4f18\u5316\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u5b9e\u73b0\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6df1\u5ea6\u5229\u7528\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u6269\u5c55\u662f\u73b0\u4ee3\u57fa\u7840\u6a21\u578b\u6210\u529f\u7684\u5173\u952e\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u6df1\u5c42\u7f51\u7edc\u5f80\u5f80\u672a\u88ab\u5145\u5206\u5229\u7528\u3002\u4f20\u7edf\u6b8b\u5dee\u8fde\u63a5\u7684\u56fa\u5b9a\u7ed3\u6784\u53ef\u80fd\u9650\u5236\u4e86\u4f18\u5316\u6548\u7387\uff0c\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u6b8b\u5dee\u8fde\u63a5\u5bf9\u6536\u655b\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u795e\u7ecf\u8fde\u63a5\u91cd\u5206\u914d\uff08ANCRe\uff09\u6846\u67b6\uff0c\u4ece\u4f18\u5316\u89d2\u5ea6\u5206\u6790\u6b8b\u5dee\u8fde\u63a5\u5e03\u5c40\u5bf9\u6536\u655b\u884c\u4e3a\u7684\u5f71\u54cd\u3002\u8be5\u6846\u67b6\u53c2\u6570\u5316\u5e76\u5b66\u4e60\u6b8b\u5dee\u8fde\u63a5\u7ed3\u6784\uff0c\u4ee5\u6570\u636e\u9a71\u52a8\u7684\u65b9\u5f0f\u81ea\u9002\u5e94\u5730\u91cd\u65b0\u5206\u914d\u8fde\u63a5\uff0c\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u6781\u5c0f\uff08<1%\uff09\u3002", "result": "\u5728\u5927\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u3001\u6269\u6563\u6a21\u578b\u548c\u6df1\u5ea6ResNets\u7b49\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cANCRe\u76f8\u6bd4\u4f20\u7edf\u6b8b\u5dee\u8fde\u63a5\u80fd\u6301\u7eed\u52a0\u901f\u6536\u655b\u3001\u63d0\u5347\u6027\u80fd\u5e76\u589e\u5f3a\u6df1\u5ea6\u5229\u7528\u6548\u7387\u3002", "conclusion": "\u6b8b\u5dee\u8fde\u63a5\u5e03\u5c40\u5bf9\u6536\u655b\u884c\u4e3a\u6709\u6839\u672c\u6027\u5f71\u54cd\uff0c\u751a\u81f3\u4f1a\u5bfc\u81f4\u6536\u655b\u901f\u5ea6\u7684\u6307\u6570\u7ea7\u5dee\u5f02\u3002ANCRe\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u7f51\u7edc\u6df1\u5ea6\u5229\u7528\uff0c\u4e3a\u6df1\u5ea6\u7f51\u7edc\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u89c6\u89d2\u3002"}}
{"id": "2602.09012", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09012", "abs": "https://arxiv.org/abs/2602.09012", "authors": ["Jiacheng Liu", "Yaxin Luo", "Jiacheng Cui", "Xinyi Shang", "Xiaohan Zhao", "Zhiqiang Shen"], "title": "Next-Gen CAPTCHAs: Leveraging the Cognitive Gap for Scalable and Diverse GUI-Agent Defense", "comment": "Project page at https://greenoso.github.io/NextGen-CAPTCHAs_webpage/", "summary": "The rapid evolution of GUI-enabled agents has rendered traditional CAPTCHAs obsolete. While previous benchmarks like OpenCaptchaWorld established a baseline for evaluating multimodal agents, recent advancements in reasoning-heavy models, such as Gemini3-Pro-High and GPT-5.2-Xhigh have effectively collapsed this security barrier, achieving pass rates as high as 90% on complex logic puzzles like \"Bingo\". In response, we introduce Next-Gen CAPTCHAs, a scalable defense framework designed to secure the next-generation web against the advanced agents. Unlike static datasets, our benchmark is built upon a robust data generation pipeline, allowing for large-scale and easily scalable evaluations, notably, for backend-supported types, our system is capable of generating effectively unbounded CAPTCHA instances. We exploit the persistent human-agent \"Cognitive Gap\" in interactive perception, memory, decision-making, and action. By engineering dynamic tasks that require adaptive intuition rather than granular planning, we re-establish a robust distinction between biological users and artificial agents, offering a scalable and diverse defense mechanism for the agentic era.", "AI": {"tldr": "\u63d0\u51faNext-Gen CAPTCHAs\u6846\u67b6\uff0c\u5229\u7528\u4eba\u673a\u5728\u4ea4\u4e92\u611f\u77e5\u3001\u8bb0\u5fc6\u3001\u51b3\u7b56\u548c\u884c\u52a8\u4e0a\u7684\"\u8ba4\u77e5\u9e3f\u6c9f\"\uff0c\u901a\u8fc7\u9700\u8981\u81ea\u9002\u5e94\u76f4\u89c9\u800c\u975e\u7ec6\u7c92\u5ea6\u89c4\u5212\u7684\u52a8\u6001\u4efb\u52a1\u6765\u9632\u5fa1\u5148\u8fdbAI\u4ee3\u7406", "motivation": "\u4f20\u7edfCAPTCHA\u5df2\u8fc7\u65f6\uff0c\u5148\u8fdb\u63a8\u7406\u6a21\u578b\uff08\u5982Gemini3-Pro-High\u548cGPT-5.2-Xhigh\uff09\u5728\u590d\u6742\u903b\u8f91\u8c1c\u9898\u4e0a\u7684\u901a\u8fc7\u7387\u9ad8\u8fbe90%\uff0c\u9700\u8981\u65b0\u7684\u5b89\u5168\u9632\u5fa1\u673a\u5236\u6765\u4fdd\u62a4\u4e0b\u4e00\u4ee3\u7f51\u7edc", "method": "\u6784\u5efa\u57fa\u4e8e\u5f3a\u5927\u6570\u636e\u751f\u6210\u7ba1\u9053\u7684\u53ef\u6269\u5c55\u57fa\u51c6\uff0c\u80fd\u591f\u751f\u6210\u8fd1\u4e4e\u65e0\u9650\u7684CAPTCHA\u5b9e\u4f8b\uff1b\u5229\u7528\u4eba\u673a\u5728\u4ea4\u4e92\u611f\u77e5\u3001\u8bb0\u5fc6\u3001\u51b3\u7b56\u548c\u884c\u52a8\u4e0a\u7684\"\u8ba4\u77e5\u9e3f\u6c9f\"\uff0c\u8bbe\u8ba1\u9700\u8981\u81ea\u9002\u5e94\u76f4\u89c9\u800c\u975e\u7ec6\u7c92\u5ea6\u89c4\u5212\u7684\u52a8\u6001\u4efb\u52a1", "result": "\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u80fd\u591f\u5927\u89c4\u6a21\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u540e\u7aef\u652f\u6301\u7684\u7c7b\u578b\uff0c\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u6709\u6548\u65e0\u754c\u7684CAPTCHA\u5b9e\u4f8b", "conclusion": "\u901a\u8fc7\u5de5\u7a0b\u5316\u52a8\u6001\u4efb\u52a1\u91cd\u65b0\u5efa\u7acb\u4e86\u751f\u7269\u7528\u6237\u548c\u4eba\u5de5\u4ee3\u7406\u4e4b\u95f4\u7684\u5f3a\u5927\u533a\u5206\uff0c\u4e3a\u4ee3\u7406\u65f6\u4ee3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u591a\u6837\u5316\u7684\u9632\u5fa1\u673a\u5236"}}
