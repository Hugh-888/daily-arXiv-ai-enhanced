<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 155]
- [physics.comp-ph](#physics.comp-ph) [Total: 11]
- [gr-qc](#gr-qc) [Total: 33]
- [quant-ph](#quant-ph) [Total: 69]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Pruning Graphs by Adversarial Robustness Evaluation to Strengthen GNN Defenses](https://arxiv.org/abs/2512.22128)
*Yongyu Wang*

Main category: cs.LG

TL;DR: 提出基于对抗鲁棒性评估的图剪枝框架，通过识别并移除图中脆弱或有害的边来增强GNN的防御能力


<details>
  <summary>Details</summary>
Motivation: 图神经网络虽然能有效利用节点特征和图拓扑信息，但这种联合建模也使其对结构或特征的扰动非常敏感，容易受到对抗攻击和虚假连接的影响

Method: 提出一个剪枝框架，利用对抗鲁棒性评估来识别图中的脆弱组件，以鲁棒性分数为指导，选择性地剪除最可能降低模型可靠性的边，从而获得更干净、更具韧性的图表示

Result: 在三个代表性GNN架构上进行了广泛实验，结果表明该方法在高扰动情况下能显著增强GNN的防御能力

Conclusion: 通过对抗鲁棒性指导的图剪枝可以有效提升GNN对扰动的抵抗力，增强模型可靠性

Abstract: Graph Neural Networks (GNNs) have emerged as a dominant paradigm for learning on graph-structured data, thanks to their ability to jointly exploit node features and relational information encoded in the graph topology. This joint modeling, however, also introduces a critical weakness: perturbations or noise in either the structure or the features can be amplified through message passing, making GNNs highly vulnerable to adversarial attacks and spurious connections. In this work, we introduce a pruning framework that leverages adversarial robustness evaluation to explicitly identify and remove fragile or detrimental components of the graph. By using robustness scores as guidance, our method selectively prunes edges that are most likely to degrade model reliability, thereby yielding cleaner and more resilient graph representations. We instantiate this framework on three representative GNN architectures and conduct extensive experiments on benchmarks. The experimental results show that our approach can significantly enhance the defense capability of GNNs in the high-perturbation regime.

</details>


### [2] [Towards Unsupervised Causal Representation Learning via Latent Additive Noise Model Causal Autoencoders](https://arxiv.org/abs/2512.22150)
*Hans Jarett J. Ong,Brian Godwin S. Lim,Dominic Dayta,Renzo Roel P. Tan,Kazushi Ikeda*

Main category: cs.LG

TL;DR: LANCA提出了一种基于加性噪声模型（ANM）的因果自编码器，通过将ANM作为强归纳偏置，解决了无监督因果发现中的可识别性问题，在合成和真实数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 无监督表示学习通常依赖统计独立性，但难以捕捉因果依赖关系。核心挑战是可识别性问题：从观测数据中分离因果变量需要监督、辅助信号或强归纳偏置。现有方法在无监督设置下无法保证因果变量的可识别性。

Method: 提出Latent Additive Noise Model Causal Autoencoder (LANCA)：1）理论证明ANM约束将可容许变换从任意微分同胚限制到仿射类；2）采用确定性Wasserstein自编码器（WAE）而非VAE，避免随机编码模糊结构残差；3）结合可微ANM层，将残差独立性从被动假设转变为显式优化目标。

Result: 在合成物理基准（Pendulum, Flow）和真实感环境（CANDLE）上，LANCA优于现有最先进基线。特别是在CANDLE上，对复杂背景场景产生的虚假相关性表现出更强的鲁棒性。

Conclusion: ANM作为强归纳偏置可以有效解决无监督因果发现中的可识别性问题。LANCA通过确定性编码和显式ANM优化，将残差独立性转化为可优化的目标，为无监督因果表示学习提供了有效框架。

Abstract: Unsupervised representation learning seeks to recover latent generative factors, yet standard methods relying on statistical independence often fail to capture causal dependencies. A central challenge is identifiability: as established in disentangled representation learning and nonlinear ICA literature, disentangling causal variables from observational data is impossible without supervision, auxiliary signals, or strong inductive biases. In this work, we propose the Latent Additive Noise Model Causal Autoencoder (LANCA) to operationalize the Additive Noise Model (ANM) as a strong inductive bias for unsupervised discovery. Theoretically, we prove that while the ANM constraint does not guarantee unique identifiability in the general mixing case, it resolves component-wise indeterminacy by restricting the admissible transformations from arbitrary diffeomorphisms to the affine class. Methodologically, arguing that the stochastic encoding inherent to VAEs obscures the structural residuals required for latent causal discovery, LANCA employs a deterministic Wasserstein Auto-Encoder (WAE) coupled with a differentiable ANM Layer. This architecture transforms residual independence from a passive assumption into an explicit optimization objective. Empirically, LANCA outperforms state-of-the-art baselines on synthetic physics benchmarks (Pendulum, Flow), and on photorealistic environments (CANDLE), where it demonstrates superior robustness to spurious correlations arising from complex background scenes.

</details>


### [3] [SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models](https://arxiv.org/abs/2512.22170)
*Jiesong Lian,Ruizhe Zhong,Zixiang Zhou,Xiaoyue Mi,Yixue Hao,Yuan Zhou,Qinglin Lu,Long Hu,Junchi Yan*

Main category: cs.LG

TL;DR: SoliReward：一种用于视频生成模型奖励模型训练的系统框架，通过高质量数据收集、层次化注意力机制和改进的损失函数来解决现有方法的问题。


<details>
  <summary>Details</summary>
Motivation: 视频生成模型的后训练对齐需要有效的奖励模型，但当前方法面临数据标注噪声、架构设计不足和奖励黑客攻击等挑战。

Method: 1) 使用单项目二元标注收集高质量数据，并通过跨提示配对策略构建偏好对；2) 采用层次化渐进查询注意力机制增强特征聚合；3) 引入改进的BT损失函数，显式处理胜-平局场景，正则化奖励分数分布。

Result: 在评估物理合理性、主体变形和语义对齐的基准测试中，该方法在直接奖励模型评估指标和后训练视频生成模型效果方面都显示出改进。

Conclusion: SoliReward框架通过系统化的数据收集、架构设计和损失函数改进，有效提升了视频奖励模型的训练效果，为视频生成模型的后训练对齐提供了更好的解决方案。

Abstract: Post-training alignment of video generation models with human preferences is a critical goal. Developing effective Reward Models (RMs) for this process faces significant methodological hurdles. Current data collection paradigms, reliant on in-prompt pairwise annotations, suffer from labeling noise. Concurrently, the architectural design of VLM-based RMs, particularly their output mechanisms, remains underexplored. Furthermore, RM is susceptible to reward hacking in post-training. To mitigate these limitations, we propose SoliReward, a systematic framework for video RM training. Our framework first sources high-quality, cost-efficient data via single-item binary annotations, then constructs preference pairs using a cross-prompt pairing strategy. Architecturally, we employ a Hierarchical Progressive Query Attention mechanism to enhance feature aggregation. Finally, we introduce a modified BT loss that explicitly accommodates win-tie scenarios. This approach regularizes the RM's score distribution for positive samples, providing more nuanced preference signals to alleviate over-focus on a small number of top-scoring samples. Our approach is validated on benchmarks evaluating physical plausibility, subject deformity, and semantic alignment, demonstrating improvements in direct RM evaluation metrics and in the efficacy of post-training on video generation models. Code and benchmark will be publicly available.

</details>


### [4] [Wireless Traffic Prediction with Large Language Model](https://arxiv.org/abs/2512.22178)
*Chuanting Zhang,Haixia Zhang,Jingping Qiao,Zongzhang Li,Mohamed-Slim Alouini*

Main category: cs.LG

TL;DR: TIDES是一个基于大语言模型的无线流量预测框架，通过聚类识别区域流量模式、提示工程将数值数据转换为结构化输入，以及DeepSeek模块实现空间对齐，显著提升了城市无线流量预测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络需要智能、自适应的资源管理，而准确的无线流量预测至关重要。现有的大语言模型预测方法大多忽略了城市尺度流量动态中的空间依赖性，因此需要开发能够捕捉时空相关性的预测框架。

Method: 1. 通过聚类机制识别不同区域的异构流量模式，为每个区域训练个性化模型；2. 引入提示工程方案，将统计流量特征作为结构化输入嵌入，弥合数值数据与语言模型之间的领域差距；3. 设计DeepSeek模块，通过跨域注意力实现空间对齐，使LLM能够利用空间相关区域的信息；4. 仅微调轻量级组件，冻结核心LLM层，实现高效领域适应。

Result: 在真实世界蜂窝流量数据集上的大量实验表明，TIDES在预测准确性和鲁棒性方面显著优于最先进的基线方法。

Conclusion: 将空间感知集成到基于LLM的预测器中是解锁未来6G系统中可扩展和智能网络管理的关键。

Abstract: The growing demand for intelligent, adaptive resource management in next-generation wireless networks has underscored the importance of accurate and scalable wireless traffic prediction. While recent advancements in deep learning and foundation models such as large language models (LLMs) have demonstrated promising forecasting capabilities, they largely overlook the spatial dependencies inherent in city-scale traffic dynamics. In this paper, we propose TIDES (Traffic Intelligence with DeepSeek-Enhanced Spatial-temporal prediction), a novel LLM-based framework that captures spatial-temporal correlations for urban wireless traffic prediction. TIDES first identifies heterogeneous traffic patterns across regions through a clustering mechanism and trains personalized models for each region to balance generalization and specialization. To bridge the domain gap between numerical traffic data and language-based models, we introduce a prompt engineering scheme that embeds statistical traffic features as structured inputs. Furthermore, we design a DeepSeek module that enables spatial alignment via cross-domain attention, allowing the LLM to leverage information from spatially related regions. By fine-tuning only lightweight components while freezing core LLM layers, TIDES achieves efficient adaptation to domain-specific patterns without incurring excessive training overhead. Extensive experiments on real-world cellular traffic datasets demonstrate that TIDES significantly outperforms state-of-the-art baselines in both prediction accuracy and robustness. Our results indicate that integrating spatial awareness into LLM-based predictors is the key to unlocking scalable and intelligent network management in future 6G systems.

</details>


### [5] [Latent Sculpting for Zero-Shot Generalization: A Manifold Learning Approach to Out-of-Distribution Anomaly Detection](https://arxiv.org/abs/2512.22179)
*Rajeeb Thapa Chhetri,Zhixiong Chen,Saurab Thapa*

Main category: cs.LG

TL;DR: 提出Latent Sculpting框架，通过两阶段表示学习解决高维表格数据中的泛化崩溃问题，在零样本异常检测上达到F1分数0.87


<details>
  <summary>Details</summary>
Motivation: 监督深度学习在高维表格数据中存在"泛化崩溃"问题：模型在已知分布上学习精确决策边界，但在面对分布外数据时完全失效。作者认为这是由于潜在空间缺乏拓扑约束，导致扩散的流形使得新异常与良性数据在统计上无法区分。

Method: 提出Latent Sculpting框架，包含两个阶段：第一阶段使用混合1D-CNN和Transformer编码器，配合新颖的双中心紧凑性损失（DCCL），将良性流量"雕刻"成低熵的超球面簇；第二阶段基于这个预结构化流形，使用掩码自回归流（MAF）学习精确的密度估计。

Result: 在CIC-IDS-2017基准测试中，监督基线在未见分布偏移上F1约0.30，最强无监督基线仅0.76，而本框架在严格零样本异常检测上达到F1分数0.87。在"渗透"场景中检测率达到88.89%，而最先进的监督模型准确率为0.00%。

Conclusion: 显式的流形雕刻是鲁棒零样本泛化的先决条件。将结构学习与密度估计解耦为广义异常检测提供了可扩展的路径。

Abstract: A fundamental limitation of supervised deep learning in high-dimensional tabular domains is "Generalization Collapse": models learn precise decision boundaries for known distributions but fail catastrophically when facing Out-of-Distribution (OOD) data. We hypothesize that this failure stems from the lack of topological constraints in the latent space, resulting in diffuse manifolds where novel anomalies remain statistically indistinguishable from benign data. To address this, we propose Latent Sculpting, a hierarchical two-stage representation learning framework. Stage 1 utilizes a hybrid 1D-CNN and Transformer Encoder trained with a novel Dual-Centroid Compactness Loss (DCCL) to actively "sculpt" benign traffic into a low-entropy, hyperspherical cluster. Unlike standard contrastive losses that rely on triplet mining, DCCL optimizes global cluster centroids to enforce absolute manifold density. Stage 2 conditions a Masked Autoregressive Flow (MAF) on this pre-structured manifold to learn an exact density estimate. We evaluate this methodology on the rigorous CIC-IDS-2017 benchmark, treating it as a proxy for complex, non-stationary data streams. Empirical results demonstrate that explicit manifold sculpting is a prerequisite for robust zero-shot generalization. While supervised baselines suffered catastrophic performance collapse on unseen distribution shifts (F1 approx 0.30) and the strongest unsupervised baseline achieved only 0.76, our framework achieved an F1-Score of 0.87 on strictly zero-shot anomalies. Notably, we report an 88.89% detection rate on "Infiltration" scenarios--a complex distributional shift where state-of-the-art supervised models achieved 0.00% accuracy. These findings suggest that decoupling structure learning from density estimation provides a scalable path toward generalized anomaly detection.

</details>


### [6] [Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks](https://arxiv.org/abs/2512.22186)
*Vishnu Mohan*

Main category: cs.LG

TL;DR: 论文提出一个强化学习框架，使用DDQN和课程学习在网球模拟环境中优化策略，实现了高胜率但存在防守偏向的问题。


<details>
  <summary>Details</summary>
Motivation: 网球策略优化是一个复杂的序列决策问题，涉及分层计分、随机结果、长时程信用分配、体力疲劳和对手技能适应等挑战，需要专门的强化学习方法来解决。

Method: 构建了完整的网球模拟环境，包含点、局、盘三级计分系统，10种离散战术动作类别，对称疲劳动态和连续对手技能参数。使用Dueling Double Deep Q-Network(DDQN)架构，结合课程学习逐步提升对手难度（0.40到0.50）。

Result: 训练后的智能体对平衡对手胜率达98-100%，发球效率63.0-67.5%，接发效率52.8-57.1%。消融研究表明dueling架构和课程学习对稳定收敛至关重要，标准DQN基线无法学习有效策略。

Conclusion: 尽管性能强劲，但战术分析显示学习到的策略存在明显的防守偏向，优先避免失误和延长回合而非积极得分。这表明在简化体育模拟中仅以胜率为驱动的优化存在局限性，强调了奖励设计对现实体育强化学习的重要性。

Abstract: Tennis strategy optimization is a challenging sequential decision-making problem involving hierarchical scoring, stochastic outcomes, long-horizon credit assignment, physical fatigue, and adaptation to opponent skill. I present a reinforcement learning framework that integrates a custom tennis simulation environment with a Dueling Double Deep Q-Network(DDQN) trained using curriculum learning. The environment models complete tennis scoring at the level of points, games, and sets, rally-level tactical decisions across ten discrete action categories, symmetric fatigue dynamics, and a continuous opponent skill parameter. The dueling architecture decomposes action-value estimation into state-value and advantage components, while double Q-learning reduces overestimation bias and improves training stability in this long-horizon stochastic domain. Curriculum learning progressively increases opponent difficulty from 0.40 to 0.50, enabling robust skill acquisition without the training collapse observed under fixed opponents. Across extensive evaluations, the trained agent achieves win rates between 98 and 100 percent against balanced opponents and maintains strong performance against more challenging opponents. Serve efficiency ranges from 63.0 to 67.5 percent, and return efficiency ranges from 52.8 to 57.1 percent. Ablation studies demonstrate that both the dueling architecture and curriculum learning are necessary for stable convergence, while a standard DQN baseline fails to learn effective policies. Despite strong performance, tactical analysis reveals a pronounced defensive bias, with the learned policy prioritizing error avoidance and prolonged rallies over aggressive point construction. These results highlight a limitation of win-rate driven optimization in simplified sports simulations and emphasize the importance of reward design for realistic sports reinforcement learning.

</details>


### [7] [Physics-Informed Machine Learning for Transformer Condition Monitoring -- Part II: Physics-Informed Neural Networks and Uncertainty Quantification](https://arxiv.org/abs/2512.22189)
*Jose I. Aizpurua*

Main category: cs.LG

TL;DR: 该论文是系列的第二部分，聚焦于将物理知识和不确定性量化集成到变压器健康评估的机器学习模型中，介绍了物理信息神经网络及其贝叶斯扩展。


<details>
  <summary>Details</summary>
Motivation: 将基于物理的知识与机器学习模型相结合，对于电气变压器的监测、诊断和预测至关重要。第一部分介绍了神经网络基础，第二部分需要进一步集成物理约束和不确定性量化，以提高模型的可靠性和鲁棒性。

Method: 1. 介绍物理信息神经网络（PINNs）基础，应用于时空热建模和固体绝缘老化；2. 提出贝叶斯PINNs作为量化认知不确定性的原则框架；3. 在稀疏数据下提供鲁棒预测。

Result: 论文提出了一个集成物理知识和不确定性量化的综合框架，通过PINNs和贝叶斯PINNs实现了对变压器热行为和绝缘老化的物理一致性建模，并能在数据稀疏情况下提供可靠预测。

Conclusion: 物理感知和可信赖的机器学习方法对于关键电力资产的健康评估具有重要潜力，贝叶斯PINNs为稀疏数据下的鲁棒预测提供了有前景的解决方案，并指出了未来研究方向。

Abstract: The integration of physics-based knowledge with machine learning models is increasingly shaping the monitoring, diagnostics, and prognostics of electrical transformers. In this two-part series, the first paper introduced the foundations of Neural Networks (NNs) and their variants for health assessment tasks. This second paper focuses on integrating physics and uncertainty into the learning process. We begin with the fundamentals of Physics-Informed Neural Networks (PINNs), applied to spatiotemporal thermal modeling and solid insulation ageing. Building on this, we present Bayesian PINNs as a principled framework to quantify epistemic uncertainty and deliver robust predictions under sparse data. Finally, we outline emerging research directions that highlight the potential of physics-aware and trustworthy machine learning for critical power assets.

</details>


### [8] [Physics-Informed Machine Learning for Transformer Condition Monitoring -- Part I: Basic Concepts, Neural Networks, and Variants](https://arxiv.org/abs/2512.22190)
*Jose I. Aizpurua*

Main category: cs.LG

TL;DR: 本文综述了神经网络在电力变压器状态监测中的应用，介绍了CNN用于多模态数据处理以及RL用于决策控制，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 电力变压器是电网关键资产，传统基于规则或物理模型的状态监测方法在处理不确定性、数据有限性和复杂运行条件方面存在局限，需要机器学习方法进行补充和扩展。

Method: 采用神经网络及其扩展方法，包括卷积神经网络(CNN)处理多模态数据，以及将神经网络概念整合到强化学习(RL)框架中用于决策和控制。

Result: 神经网络方法能够提供更准确的变压器诊断、预测和控制能力，克服传统方法的局限性，提升电网的可靠性和稳定性。

Conclusion: 神经网络为变压器状态监测和健康管理提供了强大的工具，未来研究应继续探索新兴方向，进一步整合机器学习与电力系统专业知识。

Abstract: Power transformers are critical assets in power networks, whose reliability directly impacts grid resilience and stability. Traditional condition monitoring approaches, often rule-based or purely physics-based, struggle with uncertainty, limited data availability, and the complexity of modern operating conditions. Recent advances in machine learning (ML) provide powerful tools to complement and extend these methods, enabling more accurate diagnostics, prognostics, and control. In this two-part series, we examine the role of Neural Networks (NNs) and their extensions in transformer condition monitoring and health management tasks. This first paper introduces the basic concepts of NNs, explores Convolutional Neural Networks (CNNs) for condition monitoring using diverse data modalities, and discusses the integration of NN concepts within the Reinforcement Learning (RL) paradigm for decision-making and control. Finally, perspectives on emerging research directions are also provided.

</details>


### [9] [Frequency Regularization: Unveiling the Spectral Inductive Bias of Deep Neural Networks](https://arxiv.org/abs/2512.22192)
*Jiahao Lu*

Main category: cs.LG

TL;DR: 该研究通过信号处理视角分析CNN正则化的频谱偏置，发现L2正则化通过抑制高频能量积累（超过3倍）来强化低频结构偏好，揭示了精度与鲁棒性的权衡：L2模型对宽带高斯噪声敏感但对高频信息丢失更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 尽管L2正则化和Dropout等正则化技术对训练深度神经网络至关重要，但其在特征频率选择方面的物理机制仍不清楚。研究者希望从信号处理角度理解正则化如何影响CNN的频谱特性。

Method: 1) 引入视觉诊断框架追踪训练过程中权重频率的动态演化；2) 提出新指标Spectral Suppression Ratio (SSR)量化不同正则器的"低通滤波"强度；3) 通过离散径向分析解决小卷积核（如3x3）的混叠问题；4) 在ResNet-18和CIFAR-10上进行实证研究。

Result: 1) L2正则化相比无正则化基线抑制了超过3倍的高频能量积累；2) 揭示了关键的精度-鲁棒性权衡：L2模型对宽带高斯噪声敏感（因过度专注于低频），但对高频信息丢失（如低分辨率）表现出优越鲁棒性，在模糊场景中性能优于基线>6%。

Conclusion: 正则化强制了强烈的频谱归纳偏置，使模型偏向低频结构。这从信号处理角度解释了泛化机制，确认了正则化通过频谱抑制来实现其效果。

Abstract: Regularization techniques such as L2 regularization (Weight Decay) and Dropout are fundamental to training deep neural networks, yet their underlying physical mechanisms regarding feature frequency selection remain poorly understood. In this work, we investigate the Spectral Bias of modern Convolutional Neural Networks (CNNs). We introduce a Visual Diagnostic Framework to track the dynamic evolution of weight frequencies during training and propose a novel metric, the Spectral Suppression Ratio (SSR), to quantify the "low-pass filtering" intensity of different regularizers. By addressing the aliasing issue in small kernels (e.g., 3x3) through discrete radial profiling, our empirical results on ResNet-18 and CIFAR-10 demonstrate that L2 regularization suppresses high-frequency energy accumulation by over 3x compared to unregularized baselines. Furthermore, we reveal a critical Accuracy-Robustness Trade-off: while L2 models are sensitive to broadband Gaussian noise due to over-specialization in low frequencies, they exhibit superior robustness against high-frequency information loss (e.g., low resolution), outperforming baselines by >6% in blurred scenarios. This work provides a signal-processing perspective on generalization, confirming that regularization enforces a strong spectral inductive bias towards low-frequency structures.

</details>


### [10] [Emotion-Inspired Learning Signals (EILS): A Homeostatic Framework for Adaptive Autonomous Agents](https://arxiv.org/abs/2512.22200)
*Dhruv Tiwari*

Main category: cs.LG

TL;DR: 提出情感启发学习信号(EILS)框架，用生物启发的内部稳态控制机制替代传统外部奖励函数，以增强AI在开放环境中的鲁棒性和自主性。


<details>
  <summary>Details</summary>
Motivation: 当前AI依赖外部定义的静态奖励函数，在封闭环境中表现出色但在开放、非平稳的真实世界中脆弱。标准智能体缺乏内部自主性：难以在没有密集反馈的情况下探索、无法适应分布变化、需要大量手动调参。需要一种类似生物情感的稳态控制机制来解决这些问题。

Method: 引入情感启发学习信号(EILS)框架，将情感建模为连续的稳态评估信号（如好奇心、压力、自信），而非语义标签。将这些信号形式化为从交互历史中推导的向量值内部状态，动态实时调节智能体的优化景观：好奇心调节熵防止模式崩溃，压力调节可塑性克服不活跃，自信调整信任区域稳定收敛。

Result: 论文假设这种闭环稳态调节能使EILS智能体在样本效率和非平稳适应方面优于标准基线方法，但具体实验结果未在摘要中提供。

Conclusion: EILS提供了一个统一的生物启发框架，用连贯的内部反馈引擎替代分散的优化启发式方法，有望解决当前AI在开放环境中缺乏自主性和鲁棒性的问题，通过模拟生物情感的稳态控制机制实现更强大的适应性。

Abstract: The ruling method in modern Artificial Intelligence spanning from Deep Reinforcement Learning (DRL) to Large Language Models (LLMs) relies on a surge of static, externally defined reward functions. While this "extrinsic maximization" approach has rendered superhuman performance in closed, stationary fields, it produces agents that are fragile in open-ended, real-world environments. Standard agents lack internal autonomy: they struggle to explore without dense feedback, fail to adapt to distribution shifts (non-stationarity), and require extensive manual tuning of static hyperparameters. This paper proposes that the unaddressed factor in robust autonomy is a functional analog to biological emotion, serving as a high-level homeostatic control mechanism. We introduce Emotion-Inspired Learning Signals (EILS), a unified framework that replaces scattered optimization heuristics with a coherent, bio-inspired internal feedback engine. Unlike traditional methods that treat emotions as semantic labels, EILS models them as continuous, homeostatic appraisal signals such as Curiosity, Stress, and Confidence. We formalize these signals as vector-valued internal states derived from interaction history. These states dynamically modulate the agent's optimization landscape in real time: curiosity regulates entropy to prevent mode collapse, stress modulates plasticity to overcome inactivity, and confidence adapts trust regions to stabilize convergence. We hypothesize that this closed-loop homeostatic regulation can enable EILS agents to outperform standard baselines in terms of sample efficiency and non-stationary adaptation.

</details>


### [11] [Transformer Reconstructed with Dynamic Value Attention](https://arxiv.org/abs/2512.22212)
*Xiaowei Wang*

Main category: cs.LG

TL;DR: 提出动态值注意力(DVA)方法，为每个查询动态决定值，从而将多头注意力简化为单头，并完全去除前馈网络，在减少37.6%训练时间的同时提升学习能力。


<details>
  <summary>Details</summary>
Motivation: Transformer的主要内在限制是每个头中使用相同的静态值处理所有查询。虽然通过多头注意力尝试解决，但头数受复杂度限制。需要更有效的方法动态决定每个查询的值。

Method: 提出动态值注意力(DVA)，为每个查询动态决定值，从而可以削减所有冗余头，只保留一个头。由于每个修订后的嵌入已经获取了足够的有用值，可以完全去除后续的前馈网络。

Result: DVA可以节省37.6%的训练时间，同时提高学习能力。单头动态值注意力足以替代传统的Transformer结构。

Conclusion: 动态值注意力(DVA)是Transformer的一种优化方法，通过为每个查询动态决定值，实现了单头注意力架构，在减少计算复杂度的同时保持了甚至提升了模型性能。

Abstract: Since transformer was firstly published in 2017, several works have been proposed to optimize it. However, the major structure of transformer remains unchanged, ignoring one of its main intrinsic limitations, which is the same static value is used for every query in a head. Transformer itself tries to solve this problem by implementing multi-head attentions, yet the number of heads is limited by complexity. I propose a method to decide a value for each query dynamically, which could cut down all the redundant heads, keeping only one. Consequently, the following feed forward network could be cut down entirely, as each revised embedding has already fetched enough useful values far beyond the context. As a result, a single-head Dynamic Value Attention (DVA) is all you need in a transformer. According to the experiment, DVA may save 37.6% training time than the original transformer meanwhile increasing the learning capability.

</details>


### [12] [On the Existence and Behaviour of Secondary Attention Sinks](https://arxiv.org/abs/2512.22213)
*Jeffrey T. H. Wong,Cheng Zhang,Louis Mahon,Wayne Luk,Anton Isopoussu,Yiren Zhao*

Main category: cs.LG

TL;DR: 论文发现并分析了"次要注意力汇"现象，这是与之前研究的"主要注意力汇"不同的新型注意力汇，主要出现在中间层，持续时间可变，对注意力机制有不同影响。


<details>
  <summary>Details</summary>
Motivation: 先前研究已识别出注意力汇现象（如BOS标记），但主要关注类似BOS特性的主要汇。本文发现存在性质不同的次要注意力汇，需要系统研究其特性、形成机制和对注意力机制的影响。

Method: 通过对11个模型家族进行广泛实验，分析次要注意力汇的出现位置、特性、形成机制和影响。特别研究了中间层MLP模块如何通过映射标记表示到与主要汇方向对齐的向量来形成次要汇。

Result: 发现：(1)次要汇由特定中间层MLP模块形成，这些MLP将标记表示映射到与层内主要汇方向对齐的向量；(2)这些向量的ℓ₂范数决定次要汇的汇分数和持续时间；(3)主要汇在中间层减弱，与次要汇出现时间重合。在大规模模型中，汇层级（位置和持续时间）以更确定和频繁的方式出现。

Conclusion: 次要注意力汇是与主要汇不同的新型注意力汇现象，由中间层MLP模块形成，其ℓ₂范数决定汇特性和持续时间。在大模型中，汇层级呈现更确定和频繁的模式，这对理解Transformer注意力机制有重要意义。

Abstract: Attention sinks are tokens, often the beginning-of-sequence (BOS) token, that receive disproportionately high attention despite limited semantic relevance. In this work, we identify a class of attention sinks, which we term secondary sinks, that differ fundamentally from the sinks studied in prior works, which we term primary sinks. While prior works have identified that tokens other than BOS can sometimes become sinks, they were found to exhibit properties analogous to the BOS token. Specifically, they emerge at the same layer, persist throughout the network and draw a large amount of attention mass. Whereas, we find the existence of secondary sinks that arise primarily in middle layers and can persist for a variable number of layers, and draw a smaller, but still significant, amount of attention mass. Through extensive experiments across 11 model families, we analyze where these secondary sinks appear, their properties, how they are formed, and their impact on the attention mechanism. Specifically, we show that: (1) these sinks are formed by specific middle-layer MLP modules; these MLPs map token representations to vectors that align with the direction of the primary sink of that layer. (2) The $\ell_2$-norm of these vectors determines the sink score of the secondary sink, and also the number of layers it lasts for, thereby leading to different impacts on the attention mechanisms accordingly. (3) The primary sink weakens in middle layers, coinciding with the emergence of secondary sinks. We observe that in larger-scale models, the location and lifetime of the sinks, together referred to as sink levels, appear in a more deterministic and frequent manner. Specifically, we identify three sink levels in QwQ-32B and six levels in Qwen3-14B.

</details>


### [13] [Interpretable and Adaptive Node Classification on Heterophilic Graphs via Combinatorial Scoring and Hybrid Learning](https://arxiv.org/abs/2512.22221)
*Soroush Vahidi*

Main category: cs.LG

TL;DR: 提出一个基于组合推理而非深度消息传递的可解释自适应框架，用于半监督节点分类，在异配性图上表现良好，并提供可解释性、可调性和计算效率优势。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在异配性图上表现不佳，相邻节点常属于不同类别，需要一种既能处理异配性又保持可解释性的方法。

Method: 使用基于置信度排序的贪心算法，结合类别先验、邻域统计、特征相似性和训练得到的标签兼容性评分函数；引入验证门控混合策略，仅在需要时将组合预测注入轻量级神经模型。

Result: 在异配性和过渡性基准测试中，与现有GNNs相比具有竞争力，同时在可解释性、可调性和计算效率方面具有优势。

Conclusion: 提出的组合推理框架为异配性图分类提供了有效的替代方案，平衡了性能、可解释性和适应性，特别适用于需要透明决策的场景。

Abstract: Graph neural networks (GNNs) achieve strong performance on homophilic graphs but often struggle under heterophily, where adjacent nodes frequently belong to different classes. We propose an interpretable and adaptive framework for semi-supervised node classification based on explicit combinatorial inference rather than deep message passing. Our method assigns labels using a confidence-ordered greedy procedure driven by an additive scoring function that integrates class priors, neighborhood statistics, feature similarity, and training-derived label-label compatibility. A small set of transparent hyperparameters controls the relative influence of these components, enabling smooth adaptation between homophilic and heterophilic regimes.
  We further introduce a validation-gated hybrid strategy in which combinatorial predictions are optionally injected as priors into a lightweight neural model. Hybrid refinement is applied only when it improves validation performance, preserving interpretability when neuralization is unnecessary. All adaptation signals are computed strictly from training data, ensuring a leakage-free evaluation protocol. Experiments on heterophilic and transitional benchmarks demonstrate competitive performance with modern GNNs while offering advantages in interpretability, tunability, and computational efficiency.

</details>


### [14] [PI-MFM: Physics-informed multimodal foundation model for solving partial differential equations](https://arxiv.org/abs/2512.23056)
*Min Zhu,Jingmin Sun,Zecheng Zhang,Hayden Schaeffer,Lu Lu*

Main category: cs.LG

TL;DR: 提出物理信息多模态基础模型(PI-MFM)框架，在预训练和适应过程中直接强制执行控制方程，通过向量化导数计算自动组装PDE残差损失，实现跨方程族的统一物理信息目标训练。


<details>
  <summary>Details</summary>
Motivation: 现有多算子学习方法数据需求大且在训练中忽略物理规律，需要开发数据高效、可迁移的PDE求解器，特别是在标记数据稀疏、时间域部分观测或标记函数对少的情况下。

Method: PI-MFM以PDE的符号表示为输入，通过向量化导数计算自动从输入表达式组装PDE残差损失，支持任何PDE编码多模态基础模型使用统一的物理信息目标进行训练或适应。

Result: 在13个参数化一维时间相关PDE族基准测试中，PI-MFM始终优于纯数据驱动方法，特别是在稀疏标记时空点、部分观测时间域或少量标记函数对的情况下。物理损失提高了抗噪声鲁棒性，重采样配置点等简单策略显著提高精度。

Conclusion: PI-MFM为零样本物理信息微调到未见PDE族提供了实用且可扩展的路径，仅使用PDE残差和初始/边界条件即可快速将测试误差降至约1%，明显优于从头开始的纯物理训练。

Abstract: Partial differential equations (PDEs) govern a wide range of physical systems, and recent multimodal foundation models have shown promise for learning PDE solution operators across diverse equation families. However, existing multi-operator learning approaches are data-hungry and neglect physics during training. Here, we propose a physics-informed multimodal foundation model (PI-MFM) framework that directly enforces governing equations during pretraining and adaptation. PI-MFM takes symbolic representations of PDEs as the input, and automatically assembles PDE residual losses from the input expression via a vectorized derivative computation. These designs enable any PDE-encoding multimodal foundation model to be trained or adapted with unified physics-informed objectives across equation families. On a benchmark of 13 parametric one-dimensional time-dependent PDE families, PI-MFM consistently outperforms purely data-driven counterparts, especially with sparse labeled spatiotemporal points, partially observed time domains, or few labeled function pairs. Physics losses further improve robustness against noise, and simple strategies such as resampling collocation points substantially improve accuracy. We also analyze the accuracy, precision, and computational cost of automatic differentiation and finite differences for derivative computation within PI-MFM. Finally, we demonstrate zero-shot physics-informed fine-tuning to unseen PDE families: starting from a physics-informed pretrained model, adapting using only PDE residuals and initial/boundary conditions, without any labeled solution data, rapidly reduces test errors to around 1% and clearly outperforms physics-only training from scratch. These results show that PI-MFM provides a practical and scalable path toward data-efficient, transferable PDE solvers.

</details>


### [15] [Müntz-Szász Networks: Neural Architectures with Learnable Power-Law Bases](https://arxiv.org/abs/2512.22222)
*Gnankan Landry Regis N'guessan*

Main category: cs.LG

TL;DR: 提出Müntz-Szász网络(MSN)，用可学习的分数幂基函数替代固定激活函数，专门用于逼近具有奇异或分数幂行为的函数，在物理问题中表现优异。


<details>
  <summary>Details</summary>
Motivation: 标准神经网络使用固定激活函数(ReLU, tanh, sigmoid)不适合逼近具有奇异或分数幂行为的函数，这类函数在物理问题中普遍存在，如边界层、断裂力学和角点奇异性。

Method: 引入Müntz-Szász网络(MSN)，用可学习的分数幂基函数替代固定激活函数：φ(x) = Σa_k|x|^μ_k + Σb_k sign(x)|x|^λ_k，其中指数{μ_k, λ_k}与系数一起学习。

Result: MSN继承了Müntz-Szász定理的通用逼近性，对于|x|^α类函数，MSN用单个学习指数实现O(|μ-α|^2)误差，而标准MLP需要O(ε^{-1/α})神经元。在奇异目标函数的监督回归中，MSN用10倍少的参数实现5-8倍更低的误差。在PINN基准测试中，MSN实现3-6倍的改进。

Conclusion: 理论引导的架构设计可以为科学驱动的函数类带来显著改进，MSN能够学习可解释的指数，匹配已知解的结构。

Abstract: Standard neural network architectures employ fixed activation functions (ReLU, tanh, sigmoid) that are poorly suited for approximating functions with singular or fractional power behavior, a structure that arises ubiquitously in physics, including boundary layers, fracture mechanics, and corner singularities. We introduce Müntz-Szász Networks (MSN), a novel architecture that replaces fixed smooth activations with learnable fractional power bases grounded in classical approximation theory. Each MSN edge computes $φ(x) = \sum_k a_k |x|^{μ_k} + \sum_k b_k \mathrm{sign}(x)|x|^{λ_k}$, where the exponents $\{μ_k, λ_k\}$ are learned alongside the coefficients. We prove that MSN inherits universal approximation from the Müntz-Szász theorem and establish novel approximation rates: for functions of the form $|x|^α$, MSN achieves error $\mathcal{O}(|μ- α|^2)$ with a single learned exponent, whereas standard MLPs require $\mathcal{O}(ε^{-1/α})$ neurons for comparable accuracy. On supervised regression with singular target functions, MSN achieves 5-8x lower error than MLPs with 10x fewer parameters. Physics-informed neural networks (PINNs) represent a particularly demanding application for singular function approximation; on PINN benchmarks including a singular ODE and stiff boundary-layer problems, MSN achieves 3-6x improvement while learning interpretable exponents that match the known solution structure. Our results demonstrate that theory-guided architectural design can yield dramatic improvements for scientifically-motivated function classes.

</details>


### [16] [Spectral Analysis of Hard-Constraint PINNs: The Spatial Modulation Mechanism of Boundary Functions](https://arxiv.org/abs/2512.23295)
*Yuchen Xie,Honghang Chi,Haopeng Quan,Yahui Wang,Wei Wang,Yu Ma*

Main category: cs.LG

TL;DR: HC-PINNs通过硬约束严格满足边界条件，但边界函数B作为空间调制器改变学习景观，可能引发谱崩塌导致优化停滞。建立了NTK框架分析其训练动态，发现边界函数作为谱滤波器重塑核特征谱，有效秩是训练收敛的确定性预测指标。


<details>
  <summary>Details</summary>
Motivation: HC-PINNs通过硬约束严格满足边界条件，但其训练动态的理论机制尚未被探索。与软约束方法不同，硬约束中边界函数引入乘性空间调制，可能影响优化过程。需要建立理论框架理解边界函数如何影响学习景观和训练收敛。

Method: 建立了HC-PINNs的神经正切核(NTK)理论框架，推导了显式的核组合定律。通过谱分析研究边界函数B(x)如何作为谱滤波器重塑神经网络原生核的特征谱。识别残差核的有效秩作为训练收敛的预测指标，分析边界函数可能引发的谱崩塌现象。

Result: 边界函数B(x)作为谱滤波器，通过乘性调制重塑核特征谱。有效秩是训练收敛的确定性预测指标，优于经典条件数。常用边界函数可能无意中引发谱崩塌，导致优化停滞。该框架将边界函数设计从启发式选择转变为原则性的谱优化问题。

Conclusion: 建立了HC-PINNs的NTK理论框架，揭示了边界函数作为谱滤波器的关键作用。边界函数设计应避免谱崩塌，优化其谱特性以提高训练效率。该工作为科学机器学习中的几何硬约束提供了坚实的理论基础。

Abstract: Physics-Informed Neural Networks with hard constraints (HC-PINNs) are increasingly favored for their ability to strictly enforce boundary conditions via a trial function ansatz $\tilde{u} = A + B \cdot N$, yet the theoretical mechanisms governing their training dynamics have remained unexplored.
  Unlike soft-constrained formulations where boundary terms act as additive penalties, this work reveals that the boundary function $B$ introduces a multiplicative spatial modulation that fundamentally alters the learning landscape.
  A rigorous Neural Tangent Kernel (NTK) framework for HC-PINNs is established, deriving the explicit kernel composition law.
  This relationship demonstrates that the boundary function $B(\vec{x})$ functions as a spectral filter, reshaping the eigenspectrum of the neural network's native kernel.
  Through spectral analysis, the effective rank of the residual kernel is identified as a deterministic predictor of training convergence, superior to classical condition numbers.
  It is shown that widely used boundary functions can inadvertently induce spectral collapse, leading to optimization stagnation despite exact boundary satisfaction.
  Validated across multi-dimensional benchmarks, this framework transforms the design of boundary functions from a heuristic choice into a principled spectral optimization problem, providing a solid theoretical foundation for geometric hard constraints in scientific machine learning.

</details>


### [17] [ReGAIN: Retrieval-Grounded AI Framework for Network Traffic Analysis](https://arxiv.org/abs/2512.22223)
*Shaghayegh Shajarian,Kennedy Marsh,James Benson,Sajad Khorsandroo,Mahmoud Abdelsalam*

Main category: cs.LG

TL;DR: ReGAIN是一个用于网络流量分析的多阶段框架，结合流量摘要、检索增强生成和LLM推理，提供透明准确的分析结果，准确率达95.95%-98.82%，优于传统方法并具有可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统网络流量分析系统（基于规则或机器学习）存在高误报率和缺乏可解释性的问题，限制了分析师的信任度。需要一种透明且准确的网络流量分析方法。

Method: 提出ReGAIN多阶段框架：1) 从网络流量创建自然语言摘要；2) 将摘要嵌入多集合向量数据库；3) 使用分层检索管道（元数据过滤、MMR采样、两阶段交叉编码器重排序、弃权机制）来为LLM响应提供证据引用；4) 结合检索增强生成和LLM推理。

Result: 在真实世界流量数据集的ICMP ping flood和TCP SYN flood跟踪上评估，准确率在95.95%到98.82%之间。结果通过数据集真实标签和人类专家评估双重验证。优于基于规则、传统机器学习和深度学习的基线方法。

Conclusion: ReGAIN框架通过结合流量摘要、检索增强生成和LLM推理，实现了透明、准确且可解释的网络流量分析，减少了幻觉并提供了可信赖、可验证的响应。

Abstract: Modern networks generate vast, heterogeneous traffic that must be continuously analyzed for security and performance. Traditional network traffic analysis systems, whether rule-based or machine learning-driven, often suffer from high false positives and lack interpretability, limiting analyst trust. In this paper, we present ReGAIN, a multi-stage framework that combines traffic summarization, retrieval-augmented generation (RAG), and Large Language Model (LLM) reasoning for transparent and accurate network traffic analysis. ReGAIN creates natural-language summaries from network traffic, embeds them into a multi-collection vector database, and utilizes a hierarchical retrieval pipeline to ground LLM responses with evidence citations. The pipeline features metadata-based filtering, MMR sampling, a two-stage cross-encoder reranking mechanism, and an abstention mechanism to reduce hallucinations and ensure grounded reasoning. Evaluated on ICMP ping flood and TCP SYN flood traces from the real-world traffic dataset, it demonstrates robust performance, achieving accuracy between 95.95% and 98.82% across different attack types and evaluation benchmarks. These results are validated against two complementary sources: dataset ground truth and human expert assessments. ReGAIN also outperforms rule-based, classical ML, and deep learning baselines while providing unique explainability through trustworthy, verifiable responses.

</details>


### [18] [DiRL: An Efficient Post-Training Framework for Diffusion Language Models](https://arxiv.org/abs/2512.22234)
*Ying Zhu,Jiaxin Wan,Xiaoran Liu,Siyanag He,Qiqi Wang,Xu Guo,Tianyi Liang,Zengfeng Huang,Ziwei He,Xipeng Qiu*

Main category: cs.LG

TL;DR: DiRL是一个针对扩散语言模型的高效后训练框架，通过整合FlexAttention加速的块训练和LMDeploy优化推理，实现了两阶段后训练（监督微调+强化学习），在数学推理任务上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型作为自回归模型的替代方案显示出潜力，但现有的后训练方法存在计算效率低下、训练与推理目标不匹配等问题，限制了在复杂推理任务（如数学）上的性能。

Method: 提出DiRL框架：1）整合FlexAttention加速的块训练和LMDeploy优化推理，实现高效的在线模型更新循环；2）提出DiPO，首个为扩散语言模型定制的无偏组相对策略优化实现；3）采用两阶段后训练（监督微调+强化学习）。

Result: 在高质量数学数据上训练的DiRL-8B-Instruct模型在扩散语言模型中取得了最先进的数学性能，并在多个基准测试中超越了Qwen2.5系列的可比模型。

Conclusion: DiRL框架有效解决了扩散语言模型后训练的计算效率和目标匹配问题，为扩散语言模型在复杂推理任务上的应用提供了高效的后训练解决方案。

Abstract: Diffusion Language Models (dLLMs) have emerged as promising alternatives to Auto-Regressive (AR) models. While recent efforts have validated their pre-training potential and accelerated inference speeds, the post-training landscape for dLLMs remains underdeveloped. Existing methods suffer from computational inefficiency and objective mismatches between training and inference, severely limiting performance on complex reasoning tasks such as mathematics. To address this, we introduce DiRL, an efficient post-training framework that tightly integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference. This architecture enables a streamlined online model update loop, facilitating efficient two-stage post-training (Supervised Fine-Tuning followed by Reinforcement Learning). Building on this framework, we propose DiPO, the first unbiased Group Relative Policy Optimization (GRPO) implementation tailored for dLLMs. We validate our approach by training DiRL-8B-Instruct on high-quality math data. Our model achieves state-of-the-art math performance among dLLMs and surpasses comparable models in the Qwen2.5 series on several benchmarks.

</details>


### [19] [Masking Teacher and Reinforcing Student for Distilling Vision-Language Models](https://arxiv.org/abs/2512.22238)
*Byung-Kwan Lee,Yu-Chiang Frank Wang,Ryo Hachiuma*

Main category: cs.LG

TL;DR: 提出Masters框架，通过掩码渐进式强化学习蒸馏，解决大教师模型与小学生模型之间的尺寸差距问题，实现高效知识迁移


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言模型(VLMs)虽然具有强大的多模态理解能力，但尺寸过大难以部署到移动或边缘设备。需要紧凑且能力强的VLMs，但大教师模型向小学生模型的知识蒸馏面临尺寸差距挑战：学生模型难以复现教师模型复杂的高维表示，导致学习不稳定和性能下降。

Method: 提出Masters框架：1) 掩码教师非主导权重以降低复杂度；2) 渐进式恢复教师容量，使学生能平稳学习丰富表示；3) 离线强化学习阶段结合两种奖励：准确性奖励和蒸馏奖励；4) 利用掩码教师预生成响应提供高效指导，避免昂贵的在线思考-回答过程。

Result: 该方法使学生模型能够以稳定方式从教师模型学习更丰富的表示，实现强性能而不需要计算昂贵的思考-回答过程。通过掩码渐进策略和离线强化学习，有效解决了尺寸差距带来的知识蒸馏挑战。

Conclusion: Masters框架通过掩码渐进式强化学习蒸馏，成功解决了大规模教师模型向小型学生模型知识迁移的难题，为部署高效紧凑的视觉语言模型提供了有效解决方案。

Abstract: Large-scale vision-language models (VLMs) have recently achieved remarkable multimodal understanding, but their massive size makes them impractical for deployment on mobile or edge devices. This raises the need for compact yet capable VLMs that can efficiently learn from powerful large teachers. However, distilling knowledge from a large teacher to a small student remains challenging due to their large size gap: the student often fails to reproduce the teacher's complex, high-dimensional representations, leading to unstable learning and degraded performance. To address this, we propose Masters (Masking Teacher and Reinforcing Student), a mask-progressive reinforcement learning (RL) distillation framework. Masters first masks non-dominant weights of the teacher to reduce unnecessary complexity, then progressively restores the teacher by gradually increasing its capacity during training. This strategy allows the student to learn richer representations from the teacher in a smooth and stable manner. To further refine knowledge transfer, Masters integrates an offline RL stage with two complementary rewards: an accuracy reward that measures the correctness of the generated responses, and a distillation reward that quantifies the ease of transferring responses from teacher to student. Unlike online think-answer RL paradigms that are computationally expensive and generate lengthy responses, our offline RL leverages pre-generated responses from masked teachers. These provide rich yet efficient guidance, enabling students to achieve strong performance without requiring the think-answer process.

</details>


### [20] [EvoXplain: When Machine Learning Models Agree on Predictions but Disagree on Why -- Measuring Mechanistic Multiplicity Across Training Runs](https://arxiv.org/abs/2512.22240)
*Chama Bensmail*

Main category: cs.LG

TL;DR: EvoXplain框架揭示：即使高精度模型，其解释在重复训练中也可能呈现多模态，表明单一模型解释可能掩盖多种底层机制。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习评估过于关注预测性能，假设高精度模型的解释就是正确可信的。但忽略了关键问题：当多个模型都达到高精度时，它们是否依赖相同的内部逻辑，还是通过不同甚至相互竞争的机制达到相同结果？

Method: 提出EvoXplain诊断框架，将解释视为随机优化过程中的样本（不聚合预测或构建集成），分析这些样本是否形成单一连贯解释，还是分离为多个不同的解释模式。在乳腺癌和COMPAS数据集上评估逻辑回归和随机森林两种常用模型。

Result: 尽管所有模型都达到高预测精度，但其解释经常表现出明显的多模态性。即使是通常被认为稳定的逻辑回归模型，在相同数据分割的重复训练中也能产生多个明显分离的解释盆地。这些差异不能由超参数变化或简单的性能权衡来解释。

Conclusion: EvoXplain不试图选择"正确"解释，而是使解释不稳定性可见且可量化，揭示单一实例或平均解释何时掩盖了多种底层机制的存在。更广泛地，EvoXplain将可解释性重新定义为模型类在重复实例化下的属性，而非任何单一训练模型的属性。

Abstract: Machine learning models are primarily judged by predictive performance, especially in applied settings. Once a model reaches high accuracy, its explanation is often assumed to be correct and trustworthy. However, this assumption raises an overlooked question: when two models achieve high accuracy, do they rely on the same internal logic, or do they reach the same outcome via different -- and potentially competing -- mechanisms? We introduce EvoXplain, a diagnostic framework that measures the stability of model explanations across repeated training. Rather than analysing a single trained model, EvoXplain treats explanations as samples drawn from the stochastic optimisation process itself -- without aggregating predictions or constructing ensembles -- and examines whether these samples form a single coherent explanation or separate into multiple, distinct explanatory modes. We evaluate EvoXplain on the Breast Cancer and COMPAS datasets using two widely deployed model classes: Logistic Regression and Random Forests. Although all models achieve high predictive accuracy, their explanations frequently exhibit clear multimodality. Even models commonly assumed to be stable, such as Logistic Regression, can produce multiple well-separated explanatory basins under repeated training on the same data split. These differences are not explained by hyperparameter variation or simple performance trade-offs. EvoXplain does not attempt to select a 'correct' explanation. Instead, it makes explanatory instability visible and quantifiable, revealing when single-instance or averaged explanations obscure the existence of multiple underlying mechanisms. More broadly, EvoXplain reframes interpretability as a property of a model class under repeated instantiation, rather than of any single trained model.

</details>


### [21] [Enhanced geometry prediction in laser directed energy deposition using meta-learning](https://arxiv.org/abs/2512.22241)
*Abdul Malik Al Mardhouf Al Saadi,Amrita Basak*

Main category: cs.LG

TL;DR: 提出基于元学习的跨数据集知识迁移模型，用于激光定向能量沉积中的焊道几何形状预测，能在少量数据下快速适应新沉积条件。


<details>
  <summary>Details</summary>
Motivation: 激光定向能量沉积中，由于材料、机器配置和工艺参数的多样性，实验数据集稀缺且异质性高，传统方法难以准确预测焊道几何形状。

Method: 采用两种基于梯度的元学习算法（MAML和Reptile），构建跨数据集知识迁移框架，使用文献和内部实验的多数据集，涵盖粉末送料、线材送料和混合送料L-DED工艺。

Result: 元学习模型仅需3-9个训练样本就能在未见目标任务上实现准确预测，R²值达0.9左右，平均绝对误差0.03-0.08mm，显著优于传统前馈神经网络。

Conclusion: 元学习方法能有效实现异质L-DED设置间的知识迁移，在数据稀缺条件下实现快速适应和准确预测，为L-DED工艺优化提供有效工具。

Abstract: Accurate bead geometry prediction in laser-directed energy deposition (L-DED) is often hindered by the scarcity and heterogeneity of experimental datasets collected under different materials, machine configurations, and process parameters. To address this challenge, a cross-dataset knowledge transfer model based on meta-learning for predicting deposited track geometry in L-DED is proposed. Specifically, two gradient-based meta-learning algorithms, i.e., Model-Agnostic Meta-Learning (MAML) and Reptile, are investigated to enable rapid adaptation to new deposition conditions with limited data. The proposed framework is performed using multiple experimental datasets compiled from peer-reviewed literature and in-house experiments and evaluated across powder-fed, wire-fed, and hybrid wire-powder L-DED processes. Results show that both MAML and Reptile achieve accurate bead height predictions on unseen target tasks using as few as three to nine training examples, consistently outperforming conventional feedforward neural networks trained under comparable data constraints. Across multiple target tasks representing different printing conditions, the meta-learning models achieve strong generalization performance, with R-squared values reaching up to approximately 0.9 and mean absolute errors between 0.03-0.08 mm, demonstrating effective knowledge transfer across heterogeneous L-DED settings.

</details>


### [22] [Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening](https://arxiv.org/abs/2512.22242)
*Shaurya Gaur,Michel Vitale,Alessa Hering,Johan Kwisthout,Colin Jacobs,Lena Philipp,Fennie van der Graaf*

Main category: cs.LG

TL;DR: 评估两种深度学习肺癌风险预测模型（Sybil和Venkadesh21）在不同人口亚组中的性能差异和公平性问题，发现存在显著的性别和种族间性能差异。


<details>
  <summary>Details</summary>
Motivation: 肺癌是全球癌症相关死亡的主要原因，低剂量CT筛查可早期发现但可能加重放射科工作负担。AI模型在肺癌风险评估中显示潜力，但高风险人群多样化，模型在不同人口群体中的性能差异尚不明确，需要评估潜在的性能差异和公平性问题。

Method: 基于JustEFAB框架考虑混杂因素和伦理显著偏倚，评估Sybil肺癌风险模型、Venkadesh21结节风险估计器以及PanCan2b逻辑回归模型。使用美国国家肺癌筛查试验（NLST）数据训练深度学习模型，在保留的NLST验证集上评估，分析不同人口亚组的AUROC、敏感性和特异性，探索临床风险因素的潜在混杂效应。

Result: Sybil在女性（AUROC 0.88）和男性（AUROC 0.81）间存在显著性能差异（p<0.001）。Venkadesh21在90%特异性下，黑人参与者敏感性（0.39）显著低于白人参与者（0.69）。这些差异无法用现有临床混杂因素解释，根据JustEFAB框架可归类为不公平偏倚。

Conclusion: 研究发现肺癌筛查AI模型存在人口亚组间的性能差异，这些差异可能构成不公平偏倚。强调了改进和监测模型在代表性不足亚组中性能的重要性，以及进一步研究算法公平性的必要性。

Abstract: Lung cancer is the leading cause of cancer-related mortality in adults worldwide. Screening high-risk individuals with annual low-dose CT (LDCT) can support earlier detection and reduce deaths, but widespread implementation may strain the already limited radiology workforce. AI models have shown potential in estimating lung cancer risk from LDCT scans. However, high-risk populations for lung cancer are diverse, and these models' performance across demographic groups remains an open question. In this study, we drew on the considerations on confounding factors and ethically significant biases outlined in the JustEFAB framework to evaluate potential performance disparities and fairness in two deep learning risk estimation models for lung cancer screening: the Sybil lung cancer risk model and the Venkadesh21 nodule risk estimator. We also examined disparities in the PanCan2b logistic regression model recommended in the British Thoracic Society nodule management guideline. Both deep learning models were trained on data from the US-based National Lung Screening Trial (NLST), and assessed on a held-out NLST validation set. We evaluated AUROC, sensitivity, and specificity across demographic subgroups, and explored potential confounding from clinical risk factors. We observed a statistically significant AUROC difference in Sybil's performance between women (0.88, 95% CI: 0.86, 0.90) and men (0.81, 95% CI: 0.78, 0.84, p < .001). At 90% specificity, Venkadesh21 showed lower sensitivity for Black (0.39, 95% CI: 0.23, 0.59) than White participants (0.69, 95% CI: 0.65, 0.73). These differences were not explained by available clinical confounders and thus may be classified as unfair biases according to JustEFAB. Our findings highlight the importance of improving and monitoring model performance across underrepresented subgroups, and further research on algorithmic fairness, in lung cancer screening.

</details>


### [23] [Predicting Mycotoxin Contamination in Irish Oats Using Deep and Transfer Learning](https://arxiv.org/abs/2512.22243)
*Alan Inglis,Fiona Doohan,Subramani Natarajan,Breige McNulty,Chris Elliott,Anne Nugent,Julie Meneely,Brett Greer,Stephen Kildea,Diana Bucur,Martin Danaher,Melissa Di Rocco,Lisa Black,Adam Gauley,Naoise McKenna,Andrew Parnell*

Main category: cs.LG

TL;DR: 该研究使用神经网络和迁移学习模型预测爱尔兰燕麦作物中的霉菌毒素污染，发现TabPFN模型表现最佳，收获前90天的天气模式和种子含水量是最重要的预测因子。


<details>
  <summary>Details</summary>
Motivation: 霉菌毒素污染对谷物作物质量、食品安全和农业生产构成重大风险。准确预测霉菌毒素水平可以支持早期干预策略并减少经济损失。

Method: 研究评估了五种建模方法：基线多层感知器（MLP）、带预训练的MLP，以及三种迁移学习模型（TabPFN、TabNet、FT-Transformer）。使用包含环境、农艺和地理预测因子的爱尔兰燕麦样本数据集，通过回归（RMSE、R²）和分类（AUC、F1）指标评估性能，并进行基于排列的变量重要性分析。

Result: 迁移学习方法TabPFN提供了整体最佳性能，其次是基线MLP。变量重要性分析显示，收获前90天的天气历史模式和种子含水量是最重要的预测因子。

Conclusion: 神经网络和迁移学习模型可以有效预测霉菌毒素污染，TabPFN模型表现最佳，收获前天气模式和种子含水量是关键预测因素，为早期干预提供了科学依据。

Abstract: Mycotoxin contamination poses a significant risk to cereal crop quality, food safety, and agricultural productivity. Accurate prediction of mycotoxin levels can support early intervention strategies and reduce economic losses. This study investigates the use of neural networks and transfer learning models to predict mycotoxin contamination in Irish oat crops as a multi-response prediction task. Our dataset comprises oat samples collected in Ireland, containing a mix of environmental, agronomic, and geographical predictors. Five modelling approaches were evaluated: a baseline multilayer perceptron (MLP), an MLP with pre-training, and three transfer learning models; TabPFN, TabNet, and FT-Transformer. Model performance was evaluated using regression (RMSE, $R^2$) and classification (AUC, F1) metrics, with results reported per toxin and on average. Additionally, permutation-based variable importance analysis was conducted to identify the most influential predictors across both prediction tasks. The transfer learning approach TabPFN provided the overall best performance, followed by the baseline MLP. Our variable importance analysis revealed that weather history patterns in the 90-day pre-harvest period were the most important predictors, alongside seed moisture content.

</details>


### [24] [Calibrating LLM Judges: Linear Probes for Fast and Reliable Uncertainty Estimation](https://arxiv.org/abs/2512.22245)
*Bhaktipriya Radharapu,Eshika Saxena,Kenneth Li,Chenxi Whitehouse,Adina Williams,Nicola Cancedda*

Main category: cs.LG

TL;DR: 使用线性探针从LLM推理过程的隐藏状态中获取校准的不确定性估计，相比现有方法计算效率提升约10倍，校准效果更好，但会产生保守估计。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的评判器在工业应用中日益重要，高效获取校准良好的不确定性估计对生产部署至关重要。现有方法（如语言化置信度和多生成方法）要么校准效果差，要么计算成本高。

Method: 引入基于Brier分数损失训练的线性探针，从推理评判器的隐藏状态中提供校准的不确定性估计，无需额外的模型训练。

Result: 探针在客观任务（推理、数学、事实性、编码）和主观人类偏好判断上都表现出优于现有方法的校准效果，计算节省约10倍，能泛化到未见过的评估领域，在高置信度预测上提供更高准确率。

Conclusion: 基于可解释性的不确定性估计为生产中的LLM评判器提供了实用、可扩展的即插即用解决方案，虽然会产生保守估计（在简单数据集上表现不佳），但可能有利于优先考虑低误报率的安全关键部署。

Abstract: As LLM-based judges become integral to industry applications, obtaining well-calibrated uncertainty estimates efficiently has become critical for production deployment. However, existing techniques, such as verbalized confidence and multi-generation methods, are often either poorly calibrated or computationally expensive. We introduce linear probes trained with a Brier score-based loss to provide calibrated uncertainty estimates from reasoning judges' hidden states, requiring no additional model training. We evaluate our approach on both objective tasks (reasoning, mathematics, factuality, coding) and subjective human preference judgments. Our results demonstrate that probes achieve superior calibration compared to existing methods with $\approx10$x computational savings, generalize robustly to unseen evaluation domains, and deliver higher accuracy on high-confidence predictions. However, probes produce conservative estimates that underperform on easier datasets but may benefit safety-critical deployments prioritizing low false-positive rates. Overall, our work demonstrates that interpretability-based uncertainty estimation provides a practical and scalable plug-and-play solution for LLM judges in production.

</details>


### [25] [The Affine Divergence: Aligning Activation Updates Beyond Normalisation](https://arxiv.org/abs/2512.22247)
*George Bird*

Main category: cs.LG

TL;DR: 论文提出激活更新在梯度下降中存在系统性不匹配，从优化角度重新审视归一化机制，推导出新的归一化函数形式


<details>
  <summary>Details</summary>
Motivation: 激活值在计算图中更接近损失函数且携带样本依赖信息，但在梯度下降中其更新并非最优最速下降方向，这种不匹配需要解决

Method: 从优化理论推导激活更新的理想缩放，提出两种解决方案：1) 从原理推导出归一化形式；2) 提出功能上不同于现代归一化的替代方案，包括"PatchNorm"这种组合不可分的归一化器

Result: 提出的新归一化函数在多个测试中优于传统归一化器，为归一化机制提供了新的理论框架，并质疑了"仿射+非线性"的模型构建方法

Conclusion: 从优化角度为归一化提供了新的理论解释框架，提出了新的归一化函数形式，建议将归一化器分解为类似激活函数的参数化缩放映射，以优化表示优先级

Abstract: A systematic mismatch exists between mathematically ideal and effective activation updates during gradient descent. As intended, parameters update in their direction of steepest descent. However, activations are argued to constitute a more directly impactful quantity to prioritise in optimisation, as they are closer to the loss in the computational graph and carry sample-dependent information through the network. Yet their propagated updates do not take the optimal steepest-descent step. These quantities exhibit non-ideal sample-wise scaling across affine, convolutional, and attention layers. Solutions to correct for this are trivial and, entirely incidentally, derive normalisation from first principles despite motivational independence. Consequently, such considerations offer a fresh and conceptual reframe of normalisation's action, with auxiliary experiments bolstering this mechanistically. Moreover, this analysis makes clear a second possibility: a solution that is functionally distinct from modern normalisations, without scale-invariance, yet remains empirically successful, outperforming conventional normalisers across several tests. This is presented as an alternative to the affine map. This generalises to convolution via a new functional form, "PatchNorm", a compositionally inseparable normaliser. Together, these provide an alternative mechanistic framework that adds to, and counters some of, the discussion of normalisation. Further, it is argued that normalisers are better decomposed into activation-function-like maps with parameterised scaling, thereby aiding the prioritisation of representations during optimisation. Overall, this constitutes a theoretical-principled approach that yields several new functions that are empirically validated and raises questions about the affine + nonlinear approach to model creation.

</details>


### [26] [Amortized Inference for Model Rocket Aerodynamics: Learning to Estimate Physical Parameters from Simulation](https://arxiv.org/abs/2512.22248)
*Rohit Pandey,Rohan Pandey*

Main category: cs.LG

TL;DR: 提出基于模拟的摊销推理方法，使用神经网络从合成飞行数据学习，无需真实数据微调即可预测火箭气动参数，在8次真实飞行中实现12.3米的平均绝对误差。


<details>
  <summary>Details</summary>
Motivation: 传统火箭飞行性能预测方法依赖计算流体动力学或经验关联，而数据驱动方法需要大量昂贵的真实飞行数据。需要一种能够利用模拟数据学习并直接应用于真实场景的方法。

Method: 采用基于模拟的摊销推理方法：1) 使用物理模拟器生成10,000次合成飞行数据；2) 训练神经网络学习从单个远地点测量值、发动机和配置特征中预测阻力系数和推力修正因子；3) 将学习到的模型直接应用于真实飞行，无需微调。

Result: 在8次真实飞行测试中，平均绝对误差为12.3米，展示了从模拟到真实环境的零样本迁移能力。与OpenRocket基线相比，减少了远地点预测误差。分析揭示了预测中的系统性正偏差，量化了理想物理模型与现实飞行条件之间的差距。

Conclusion: 该方法成功实现了从合成数据到真实火箭飞行的零样本迁移，为业余火箭社区提供了一种无需大量真实数据即可准确预测飞行性能的工具。公开实现支持可重复性和社区采用。

Abstract: Accurate prediction of model rocket flight performance requires estimating aerodynamic parameters that are difficult to measure directly. Traditional approaches rely on computational fluid dynamics or empirical correlations, while data-driven methods require extensive real flight data that is expensive and time-consuming to collect. We present a simulation-based amortized inference approach that trains a neural network on synthetic flight data generated from a physics simulator, then applies the learned model to real flights without any fine-tuning. Our method learns to invert the forward physics model, directly predicting drag coefficient and thrust correction factor from a single apogee measurement combined with motor and configuration features. In this proof-of-concept study, we train on 10,000 synthetic flights and evaluate on 8 real flights, achieving a mean absolute error of 12.3 m in apogee prediction - demonstrating promising sim-to-real transfer with zero real training examples. Analysis reveals a systematic positive bias in predictions, providing quantitative insight into the gap between idealized physics and real-world flight conditions. We additionally compare against OpenRocket baseline predictions, showing that our learned approach reduces apogee prediction error. Our implementation is publicly available to support reproducibility and adoption in the amateur rocketry community.

</details>


### [27] [Temporal Visual Semantics-Induced Human Motion Understanding with Large Language Models](https://arxiv.org/abs/2512.22249)
*Zheng Xing,Weibing Zhao*

Main category: cs.LG

TL;DR: 该论文提出了一种结合时间视觉语义（TVS）和子空间聚类的人体运动分割方法，利用大语言模型从连续帧中提取文本运动信息，并通过时间正则化提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 传统的人体运动分割方法忽略了时间语义探索的重要性。本文旨在利用从人体运动序列中提取的时间视觉语义（TVS），通过大语言模型的图像到文本能力来增强子空间聚类的性能。

Method: 1. 使用LLM从连续帧中提取文本运动信息，判断相邻帧是否描述相同运动；2. 基于LLM响应学习时间相邻信息；3. 开发TVS集成的子空间聚类方法，包含带有时间正则化的子空间嵌入；4. 引入反馈机制框架，根据分割输出持续优化子空间嵌入。

Result: 在四个基准人体运动数据集上的实验结果表明，所提出的方法优于现有的最先进方法。

Conclusion: 通过将时间视觉语义与子空间聚类相结合，并利用大语言模型提取运动信息，该方法显著提升了人体运动分割的性能，证明了时间语义信息在运动分割中的重要性。

Abstract: Unsupervised human motion segmentation (HMS) can be effectively achieved using subspace clustering techniques. However, traditional methods overlook the role of temporal semantic exploration in HMS. This paper explores the use of temporal vision semantics (TVS) derived from human motion sequences, leveraging the image-to-text capabilities of a large language model (LLM) to enhance subspace clustering performance. The core idea is to extract textual motion information from consecutive frames via LLM and incorporate this learned information into the subspace clustering framework. The primary challenge lies in learning TVS from human motion sequences using LLM and integrating this information into subspace clustering. To address this, we determine whether consecutive frames depict the same motion by querying the LLM and subsequently learn temporal neighboring information based on its response. We then develop a TVS-integrated subspace clustering approach, incorporating subspace embedding with a temporal regularizer that induces each frame to share similar subspace embeddings with its temporal neighbors. Additionally, segmentation is performed based on subspace embedding with a temporal constraint that induces the grouping of each frame with its temporal neighbors. We also introduce a feedback-enabled framework that continuously optimizes subspace embedding based on the segmentation output. Experimental results demonstrate that the proposed method outperforms existing state-of-the-art approaches on four benchmark human motion datasets.

</details>


### [28] [Interpretable Perturbation Modeling Through Biomedical Knowledge Graphs](https://arxiv.org/abs/2512.22251)
*Pascal Passigan,Kevin zhu,Angelina Ning*

Main category: cs.LG

TL;DR: 该研究构建了一个融合生物医学知识图谱和药物-细胞系数据的异质图，使用图注意力网络预测药物对基因表达的扰动效应，超越了传统二元药物-疾病关联任务。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习框架主要关注知识图谱中的链接预测和二元药物-疾病关联任务，而药物对基因表达的扰动效应能更深入地揭示转录组水平的机制效应，这对于理解药物作用机制、预测脱靶效应和药物重定位至关重要。

Method: 构建融合PrimeKG++（增强版生物医学知识图谱）和LINCS L1000药物-细胞系节点的异质图，使用MolFormerXL和BioBERT等基础模型生成多模态嵌入初始化节点特征，训练图注意力网络（GAT）预测978个标志基因的表达变化（delta表达谱）。

Result: 该框架在支架分割和随机分割下均优于多层感知机基线模型，消融实验（边洗牌和节点特征随机化）表明生物医学知识图谱提供的边结构能显著提升扰动水平预测性能。

Conclusion: 该研究为机制性药物建模提供了新路径，从传统的二元药物-疾病关联任务转向更精细的转录组效应预测，有助于深入理解药物干预的分子机制。

Abstract: Understanding how small molecules perturb gene expression is essential for uncovering drug mechanisms, predicting off-target effects, and identifying repurposing opportunities. While prior deep learning frameworks have integrated multimodal embeddings into biomedical knowledge graphs (BKGs) and further improved these representations through graph neural network message-passing paradigms, these models have been applied to tasks such as link prediction and binary drug-disease association, rather than the task of gene perturbation, which may unveil more about mechanistic transcriptomic effects. To address this gap, we construct a merged biomedical graph that integrates (i) PrimeKG++, an augmentation of PrimeKG containing semantically rich embeddings for nodes with (ii) LINCS L1000 drug and cell line nodes, initialized with multimodal embeddings from foundation models such as MolFormerXL and BioBERT. Using this heterogeneous graph, we train a graph attention network (GAT) with a downstream prediction head that learns the delta expression profile of over 978 landmark genes for a given drug-cell pair. Our results show that our framework outperforms MLP baselines for differentially expressed genes (DEG) -- which predict the delta expression given a concatenated embedding of drug features, target features, and baseline cell expression -- under the scaffold and random splits. Ablation experiments with edge shuffling and node feature randomization further demonstrate that the edges provided by biomedical KGs enhance perturbation-level prediction. More broadly, our framework provides a path toward mechanistic drug modeling: moving beyond binary drug-disease association tasks to granular transcriptional effects of therapeutic intervention.

</details>


### [29] [Graph Attention-based Adaptive Transfer Learning for Link Prediction](https://arxiv.org/abs/2512.22252)
*Huashen Lu,Wensheng Gan,Guoting Chen,Zhichao Huang,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出GAATNet图注意力自适应迁移网络，结合预训练和微调解决大规模稀疏图链接预测中的迁移学习问题，通过远邻嵌入和轻量适配器提升泛化能力和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有GNN链接预测方法在处理大规模稀疏图和跨数据集迁移学习时面临挑战，特别是不同数据集间需要高度对齐，且自监督方法在跨图数据集迁移学习方面的潜力未被充分挖掘。

Method: 提出Graph Attention Adaptive Transfer Network (GAATNet)，结合预训练和微调捕获跨尺度数据集的全局节点嵌入信息。关键策略：1) 在自注意力模块中融入远邻嵌入作为偏置以捕获全局特征；2) 微调时引入轻量自适配器模块提升训练效率。

Result: 在七个公开数据集上的综合实验表明，GAATNet在链接预测任务中达到了最先进的性能，证明了其有效整合GNN与迁移学习的能力。

Conclusion: GAATNet为链接预测任务提供了一个通用且可扩展的解决方案，能够有效整合图神经网络与迁移学习，解决了大规模稀疏图和跨数据集迁移的挑战。

Abstract: Graph neural networks (GNNs) have brought revolutionary advancements to the field of link prediction (LP), providing powerful tools for mining potential relationships in graphs. However, existing methods face challenges when dealing with large-scale sparse graphs and the need for a high degree of alignment between different datasets in transfer learning. Besides, although self-supervised methods have achieved remarkable success in many graph tasks, prior research has overlooked the potential of transfer learning to generalize across different graph datasets. To address these limitations, we propose a novel Graph Attention Adaptive Transfer Network (GAATNet). It combines the advantages of pre-training and fine-tuning to capture global node embedding information across datasets of different scales, ensuring efficient knowledge transfer and improved LP performance. To enhance the model's generalization ability and accelerate training, we design two key strategies: 1) Incorporate distant neighbor embeddings as biases in the self-attention module to capture global features. 2) Introduce a lightweight self-adapter module during fine-tuning to improve training efficiency. Comprehensive experiments on seven public datasets demonstrate that GAATNet achieves state-of-the-art performance in LP tasks. This study provides a general and scalable solution for LP tasks to effectively integrate GNNs with transfer learning. The source code and datasets are publicly available at https://github.com/DSI-Lab1/GAATNet

</details>


### [30] [Cardiac mortality prediction in patients undergoing PCI based on real and synthetic data](https://arxiv.org/abs/2512.22259)
*Daniil Burakov,Ivan Petrov,Dmitrii Khelimskii,Ivan Bessonov,Mikhail Lazarev*

Main category: cs.LG

TL;DR: 研究开发了基于真实和合成数据的PCI术后心脏死亡风险预测模型，通过数据增强改善类别不平衡问题，识别出年龄、射血分数、外周动脉疾病和脑血管疾病是最重要的预测因素。


<details>
  <summary>Details</summary>
Motivation: PCI术后患者状态、血管造影和手术特征包含预测长期结果的关键信号，但现有模型在处理类别不平衡（心脏死亡为少数类）时表现不佳，需要开发更稳健的预测模型。

Method: 分析2044例分叉病变PCI患者数据，使用多种机器学习模型预测3年死亡率。为处理类别不平衡，生成500个合成样本加入训练集。应用排列特征重要性评估特征贡献，并通过移除非信息特征进行额外实验验证。

Result: 无过采样时模型总体准确率高（0.92-0.93）但几乎完全忽略少数类。数据增强持续提高少数类召回率，AUROC损失最小，改善概率质量，在构建的严重病例上产生更临床合理的风险估计。年龄、射血分数、外周动脉疾病和脑血管疾病是最有影响力的四个特征。

Conclusion: 使用现实和极端病例的简单数据增强可以暴露、量化和减少基于表格记录的临床预测中的脆弱性，建议在报告主要指标时常规报告概率质量和压力测试结果。

Abstract: Patient status, angiographic and procedural characteristics encode crucial signals for predicting long-term outcomes after percutaneous coronary intervention (PCI). The aim of the study was to develop a predictive model for assessing the risk of cardiac death based on the real and synthetic data of patients undergoing PCI and to identify the factors that have the greatest impact on mortality. We analyzed 2,044 patients, who underwent a PCI for bifurcation lesions. The primary outcome was cardiac death at 3-year follow-up. Several machine learning models were applied to predict three-year mortality after PCI. To address class imbalance and improve the representation of the minority class, an additional 500 synthetic samples were generated and added to the training set. To evaluate the contribution of individual features to model performance, we applied permutation feature importance. An additional experiment was conducted to evaluate how the model's predictions would change after removing non-informative features from the training and test datasets. Without oversampling, all models achieve high overall accuracy (0.92-0.93), yet they almost completely ignore the minority class. Across models, augmentation consistently increases minority-class recall with minimal loss of AUROC, improves probability quality, and yields more clinically reasonable risk estimates on the constructed severe profiles. According to feature importance analysis, four features emerged as the most influential: Age, Ejection Fraction, Peripheral Artery Disease, and Cerebrovascular Disease. These results show that straightforward augmentation with realistic and extreme cases can expose, quantify, and reduce brittleness in imbalanced clinical prediction using only tabular records, and motivate routine reporting of probability quality and stress tests alongside headline metrics.

</details>


### [31] [The Physics Constraint Paradox: When Removing Explicit Constraints Improves Physics-Informed Data for Machine Learning](https://arxiv.org/abs/2512.22261)
*Rahul D Ray*

Main category: cs.LG

TL;DR: 对光栅耦合器光谱生成器进行物理约束消融研究，发现显式能量守恒约束在物理一致方程中是冗余的，而法布里-珀罗振荡对带宽预测影响显著，移除后可提升机器学习性能31.3%。


<details>
  <summary>Details</summary>
Motivation: 在科学领域中，真实数据稀缺，物理约束数据生成至关重要。但现有方法往往过度约束模型，未能识别哪些物理组件是必要的。需要系统研究不同物理约束对数据生成和机器学习性能的影响。

Method: 对物理信息光栅耦合器光谱生成器进行系统消融研究，选择性移除：1) 显式能量守恒约束；2) 法布里-珀罗振荡；3) 带宽变化；4) 噪声。生成器将5个几何参数映射到100点光谱响应，速度达200样本/秒。

Result: 发现物理约束悖论：显式能量守恒约束在物理一致方程中是数学冗余的，约束与非约束变体达到相同的守恒精度（平均误差约7×10^-9）。法布里-珀罗振荡主导基于阈值的带宽变异性，移除后带宽展宽减少72%（从132.3nm降至37.4nm）。标准噪声添加加重新归一化流程引入0.5%非物理负吸收值。下游机器学习评估显示物理可学习性权衡：移除法布里-珀罗振荡将带宽预测R²提高31.3%，RMSE降低73.8%。

Conclusion: 研究为物理信息数据集设计提供可操作指导，强调机器学习性能可作为评估约束相关性的诊断工具。显式能量守恒约束在物理一致模型中可能冗余，而法布里-珀罗振荡等物理效应显著影响机器学习性能，需在数据生成中仔细考虑。

Abstract: Physics-constrained data generation is essential for machine learning in scientific domains where real data are scarce; however, existing approaches often over-constrain models without identifying which physical components are necessary. We present a systematic ablation study of a physics-informed grating coupler spectrum generator that maps five geometric parameters to 100-point spectral responses. By selectively removing explicit energy conservation enforcement, Fabry-Perot oscillations, bandwidth variation, and noise, we uncover a physics constraint paradox: explicit energy conservation enforcement is mathematically redundant when the underlying equations are physically consistent, with constrained and unconstrained variants achieving identical conservation accuracy (mean error approximately 7 x 10^-9). In contrast, Fabry-Perot oscillations dominate threshold-based bandwidth variability, accounting for a 72 percent reduction in half-maximum bandwidth spread when removed (with bandwidth spread reduced from 132.3 nm to 37.4 nm). We further identify a subtle pitfall: standard noise-addition-plus-renormalization pipelines introduce 0.5 percent unphysical negative absorption values. The generator operates at 200 samples per second, enabling high-throughput data generation and remaining orders of magnitude faster than typical full-wave solvers reported in the literature. Finally, downstream machine learning evaluation reveals a clear physics-learnability trade-off: while central wavelength prediction remains unaffected, removing Fabry-Perot oscillations improves bandwidth prediction accuracy by 31.3 percent in R-squared and reduces RMSE by 73.8 percent. These findings provide actionable guidance for physics-informed dataset design and highlight machine learning performance as a diagnostic tool for assessing constraint relevance.

</details>


### [32] [LuxIA: A Lightweight Unitary matriX-based Framework Built on an Iterative Algorithm for Photonic Neural Network Training](https://arxiv.org/abs/2512.22264)
*Tzamn Melendez Carmona,Federico Marchesin,Marco P. Abrate,Peter Bienstman,Stefano Di Carlo,Alessandro Savino Senior*

Main category: cs.LG

TL;DR: LuxIA框架通过Slicing方法解决了光子神经网络训练中的计算瓶颈，显著提升了大规模PNN模拟的可扩展性和效率


<details>
  <summary>Details</summary>
Motivation: 当前光子神经网络模拟工具在训练大规模PNN时面临严重的可扩展性挑战，主要由于传输矩阵计算的计算需求导致高内存和时间消耗，限制了PNN的研究和发展

Method: 提出了Slicing方法，一种兼容反向传播的高效传输矩阵计算技术，并将其集成到统一的模拟和训练框架LuxIA中

Result: LuxIA在MNIST、Digits和Olivetti Faces等多个数据集和光子架构上的实验评估显示，其速度和可扩展性始终优于现有工具，显著减少了内存使用和执行时间

Conclusion: 通过解决关键计算瓶颈，LuxIA促进了光子神经网络更广泛的采用，加速了AI硬件创新，为更高效、可扩展的光子神经网络研究开发铺平了道路

Abstract: PNNs present promising opportunities for accelerating machine learning by leveraging the unique benefits of photonic circuits. However, current state of the art PNN simulation tools face significant scalability challenges when training large-scale PNNs, due to the computational demands of transfer matrix calculations, resulting in high memory and time consumption. To overcome these limitations, we introduce the Slicing method, an efficient transfer matrix computation approach compatible with back-propagation. We integrate this method into LuxIA, a unified simulation and training framework. The Slicing method substantially reduces memory usage and execution time, enabling scalable simulation and training of large PNNs. Experimental evaluations across various photonic architectures and standard datasets, including MNIST, Digits, and Olivetti Faces, show that LuxIA consistently surpasses existing tools in speed and scalability. Our results advance the state of the art in PNN simulation, making it feasible to explore and optimize larger, more complex architectures. By addressing key computational bottlenecks, LuxIA facilitates broader adoption and accelerates innovation in AI hardware through photonic technologies. This work paves the way for more efficient and scalable photonic neural network research and development.

</details>


### [33] [LLMTM: Benchmarking and Optimizing LLMs for Temporal Motif Analysis in Dynamic Graphs](https://arxiv.org/abs/2512.22266)
*Bing Hao,Minglai Shao,Zengyi Wo,Yunlong Chu,Yuhang Liu,Ruijie Wang*

Main category: cs.LG

TL;DR: 本文系统研究LLM在时序模体分析上的性能，提出LLMTM基准，开发工具增强的LLM智能体，并设计结构感知调度器以平衡准确性和成本。


<details>
  <summary>Details</summary>
Motivation: LLM在动态图处理中的应用日益广泛，但针对动态图基本单元——时序模体的分析能力尚未得到充分探索。时序模体作为动态图的重要局部特性，能直接反映异常和独特现象，对理解动态图的演化动态和结构特征至关重要。

Method: 1) 提出LLMTM基准，包含6个定制任务和9种时序模体类型；2) 进行广泛实验分析不同提示技术和LLM模型的影响；3) 开发工具增强的LLM智能体；4) 设计结构感知调度器，综合考虑动态图结构特性和LLM认知负载，智能调度标准提示与智能体之间的查询。

Result: 实验表明：1) 工具增强的LLM智能体能以高精度解决时序模体任务；2) 但高精度伴随显著成本；3) 结构感知调度器能有效维持高精度同时降低成本。

Conclusion: 本文系统探索了LLM在时序模体分析上的能力，提出的基准、智能体和调度器框架为平衡动态图分析任务的准确性与成本提供了有效解决方案，推动了LLM在动态图处理领域的应用。

Abstract: The widespread application of Large Language Models (LLMs) has motivated a growing interest in their capacity for processing dynamic graphs. Temporal motifs, as an elementary unit and important local property of dynamic graphs which can directly reflect anomalies and unique phenomena, are essential for understanding their evolutionary dynamics and structural features. However, leveraging LLMs for temporal motif analysis on dynamic graphs remains relatively unexplored. In this paper, we systematically study LLM performance on temporal motif-related tasks. Specifically, we propose a comprehensive benchmark, LLMTM (Large Language Models in Temporal Motifs), which includes six tailored tasks across nine temporal motif types. We then conduct extensive experiments to analyze the impacts of different prompting techniques and LLMs (including nine models: openPangu-7B, the DeepSeek-R1-Distill-Qwen series, Qwen2.5-32B-Instruct, GPT-4o-mini, DeepSeek-R1, and o3) on model performance. Informed by our benchmark findings, we develop a tool-augmented LLM agent that leverages precisely engineered prompts to solve these tasks with high accuracy. Nevertheless, the high accuracy of the agent incurs a substantial cost. To address this trade-off, we propose a simple yet effective structure-aware dispatcher that considers both the dynamic graph's structural properties and the LLM's cognitive load to intelligently dispatch queries between the standard LLM prompting and the more powerful agent. Our experiments demonstrate that the structure-aware dispatcher effectively maintains high accuracy while reducing cost.

</details>


### [34] [Hierarchical Stacking Optimization Using Dirichlet's Process (SoDip): Towards Accelerated Design for Graft Polymerization](https://arxiv.org/abs/2512.22279)
*Amgad Ahmed Ali Ibrahim,Hein Htet,Ryoji Asahi*

Main category: cs.LG

TL;DR: SoDip是一个用于辐射诱导接枝（RIG）工艺优化的分层堆叠框架，通过集成Transformer、TabNet、XGBoost、高斯过程回归和贝叶斯优化，解决了RIG中因基膜形态变化导致的重复性问题，实现了约33%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 辐射诱导接枝（RIG）技术虽然能精确功能化聚合物薄膜，但由于基膜形态（结晶度、晶粒取向、自由体积）的未报告变异性，导致单体扩散、自由基分布和Trommsdorff效应不一致，从而产生空间接枝梯度和性能不一致的问题，限制了技术的可重复性。

Method: 提出SoDip分层堆叠优化框架：1）使用仅解码器Transformer（DeepSeek-R1）编码文本工艺描述符；2）TabNet和XGBoost建模多模态特征交互；3）高斯过程回归（GPR）与狄利克雷过程混合模型（DPMM）进行不确定性量化和异方差性处理；4）贝叶斯优化高效探索高维合成空间。使用ChemDataExtractor 2.0和WebPlotDigitizer构建包含数百项RIG研究的多样化数据集。

Result: 在交叉验证中，SoDip相比传统GPR实现了约33%的性能提升，同时提供校准的置信区间，能够识别低重复性区域。其堆叠架构能够整合稀疏文本和不同质量的数值输入，优于先前模型。

Conclusion: SoDip框架为接枝聚合研究建立了可重复、形态感知设计的基础，通过分层数据驱动方法解决了RIG工艺中的可重复性问题，为离子交换膜、CO2分离膜和电池电解质等功能材料的设计提供了优化工具。

Abstract: Radiation-induced grafting (RIG) enables precise functionalization of polymer films for ion-exchange membranes, CO2-separation membranes, and battery electrolytes by generating radicals on robust substrates to graft desired monomers. However, reproducibility remains limited due to unreported variability in base-film morphology (crystallinity, grain orientation, free volume), which governs monomer diffusion, radical distribution, and the Trommsdorff effect, leading to spatial graft gradients and performance inconsistencies. We present a hierarchical stacking optimization framework with a Dirichlet's Process (SoDip), a hierarchical data-driven framework integrating: (1) a decoder-only Transformer (DeepSeek-R1) to encode textual process descriptors (irradiation source, grafting type, substrate manufacturer); (2) TabNet and XGBoost for modelling multimodal feature interactions; (3) Gaussian Process Regression (GPR) with Dirichlet Process Mixture Models (DPMM) for uncertainty quantification and heteroscedasticity; and (4) Bayesian Optimization for efficient exploration of high-dimensional synthesis space. A diverse dataset was curated using ChemDataExtractor 2.0 and WebPlotDigitizer, incorporating numerical and textual variables across hundreds of RIG studies. In cross-validation, SoDip achieved ~33% improvement over GPR while providing calibrated confidence intervals that identify low-reproducibility regimes. Its stacked architecture integrates sparse textual and numerical inputs of varying quality, outperforming prior models and establishing a foundation for reproducible, morphology-aware design in graft polymerization research.

</details>


### [35] [Valori: A Deterministic Memory Substrate for AI Systems](https://arxiv.org/abs/2512.22280)
*Varshith Gudur*

Main category: cs.LG

TL;DR: Valori是一个确定性AI内存基板，用定点算术(Q16.16)替代浮点运算，确保跨平台比特级一致的向量嵌入存储和检索，解决AI系统中的非确定性问题。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统使用浮点运算进行向量嵌入存储和搜索，这引入了根本性的非确定性：相同的模型、输入和代码在不同硬件架构（如x86 vs ARM）上会产生不同的内存状态和检索结果。这破坏了可重现性和安全部署，导致数据无声分歧，影响受监管行业的审计追溯。

Method: Valori采用定点算术(Q16.16)替代浮点内存操作，将内存建模为可重现的状态机。系统在内存边界强制执行确定性，确保比特级一致的内存状态、快照和搜索结果。

Result: Valori能够保证跨平台的比特级一致内存状态和检索结果。研究表明非确定性出现在索引或检索之前，Valori通过在内存边界强制执行确定性解决了这一问题。

Conclusion: 确定性内存是可信AI系统的必要基础组件。Valori提供了一个开源实现，为需要可重现性和审计追溯的AI应用提供了解决方案。

Abstract: Modern AI systems rely on vector embeddings stored and searched using floating-point arithmetic. While effective for approximate similarity search, this design introduces fundamental non-determinism: identical models, inputs, and code can produce different memory states and retrieval results across hardware architectures (e.g., x86 vs. ARM). This prevents replayability and safe deployment, leading to silent data divergence that prevents post-hoc verification and compromises audit trails in regulated sectors. We present Valori, a deterministic AI memory substrate that replaces floating-point memory operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. Valori guarantees bit-identical memory states, snapshots, and search results across platforms. We demonstrate that non-determinism arises before indexing or retrieval and show how Valori enforces determinism at the memory boundary. Our results suggest that deterministic memory is a necessary primitive for trustworthy AI systems. The reference implementation is open-source and available at https://github.com/varshith-Git/Valori-Kernel (archived at https://zenodo.org/records/18022660).

</details>


### [36] [DBAW-PIKAN: Dynamic Balance Adaptive Weight Kolmogorov-Arnold Neural Network for Solving Partial Differential Equations](https://arxiv.org/abs/2512.22283)
*Guokan Chen,Yao Xiao*

Main category: cs.LG

TL;DR: 提出DBAW-PIKAN方法，结合Kolmogorov-Arnold网络架构和自适应权重策略，解决PINNs在多尺度/高频问题中的梯度流刚度和谱偏差问题，显著提升收敛速度和精度。


<details>
  <summary>Details</summary>
Motivation: PINNs在处理多尺度或高频特征问题时面临梯度流刚度和谱偏差的严重挑战，这些限制显著影响了其预测能力，需要新的方法来解决这些瓶颈。

Method: 提出DBAW-PIKAN方法，结合基于可学习B样条的Kolmogorov-Arnold网络架构和包含动态衰减上界的自适应权重策略，以减轻梯度相关的失效模式。

Result: 相比基线模型，该方法在不增加计算复杂度的前提下，加速收敛过程并将解精度提高至少一个数量级，在Klein-Gordon、Burgers和Helmholtz方程等基准测试中表现出显著优势。

Conclusion: DBAW-PIKAN方法有效解决了PINNs在多尺度/高频问题中的挑战，显著提升了精度和泛化性能，为科学计算中的物理信息神经网络提供了改进方案。

Abstract: Physics-informed neural networks (PINNs) have led to significant advancements in scientific computing by integrating fundamental physical principles with advanced data-driven techniques. However, when dealing with problems characterized by multi-scale or high-frequency features, PINNs encounter persistent and severe challenges related to stiffness in gradient flow and spectral bias, which significantly limit their predictive capabilities. To address these issues, this paper proposes a Dynamic Balancing Adaptive Weighting Physics-Informed Kolmogorov-Arnold Network (DBAW-PIKAN), designed to mitigate such gradient-related failure modes and overcome the bottlenecks in function representation. The core of DBAW-PIKAN combines the Kolmogorov-Arnold network architecture, based on learnable B-splines, with an adaptive weighting strategy that incorporates a dynamic decay upper bound. Compared to baseline models, the proposed method accelerates the convergence process and improves solution accuracy by at least an order of magnitude without introducing additional computational complexity. A series of numerical benchmarks, including the Klein-Gordon, Burgers, and Helmholtz equations, demonstrate the significant advantages of DBAW-PIKAN in enhancing both accuracy and generalization performance.

</details>


### [37] [Cluster Aggregated GAN (CAG): A Cluster-Based Hybrid Model for Appliance Pattern Generation](https://arxiv.org/abs/2512.22287)
*Zikun Guoa,Adeyinka. P. Adedigbaa,Rammohan Mallipeddi*

Main category: cs.LG

TL;DR: 提出Cluster Aggregated GAN框架，通过聚类和分支架构分别处理间歇性和连续性电器，提升合成负载数据的真实性和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有GAN方法将所有电器统一处理，忽略了间歇性和连续性电器的行为差异，导致训练不稳定和输出保真度有限。合成电器数据对非侵入式负载监测算法开发和隐私保护能源研究至关重要，但标记数据稀缺。

Method: 提出Cluster Aggregated GAN混合生成框架：1) 根据电器行为特征路由到专门分支；2) 间歇性电器使用聚类模块分组相似激活模式，为每个簇分配专用生成器；3) 连续性电器使用LSTM生成器捕捉时间演化，通过序列压缩保持训练稳定性。

Result: 在UVIC智能插座数据集上的实验表明，该框架在真实性、多样性和训练稳定性指标上均优于基线方法，将聚类作为主动生成组件显著提高了可解释性和可扩展性。

Conclusion: 该框架为非侵入式负载监测研究中的合成负载生成提供了有效方法，通过专门处理不同电器类型的行为差异，解决了现有方法的局限性。

Abstract: Synthetic appliance data are essential for developing non-intrusive load monitoring algorithms and enabling privacy preserving energy research, yet the scarcity of labeled datasets remains a significant barrier. Recent GAN-based methods have demonstrated the feasibility of synthesizing load patterns, but most existing approaches treat all devices uniformly within a single model, neglecting the behavioral differences between intermittent and continuous appliances and resulting in unstable training and limited output fidelity. To address these limitations, we propose the Cluster Aggregated GAN framework, a hybrid generative approach that routes each appliance to a specialized branch based on its behavioral characteristics. For intermittent appliances, a clustering module groups similar activation patterns and allocates dedicated generators for each cluster, ensuring that both common and rare operational modes receive adequate modeling capacity. Continuous appliances follow a separate branch that employs an LSTM-based generator to capture gradual temporal evolution while maintaining training stability through sequence compression. Extensive experiments on the UVIC smart plug dataset demonstrate that the proposed framework consistently outperforms baseline methods across metrics measuring realism, diversity, and training stability, and that integrating clustering as an active generative component substantially improves both interpretability and scalability. These findings establish the proposed framework as an effective approach for synthetic load generation in non-intrusive load monitoring research.

</details>


### [38] [Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model](https://arxiv.org/abs/2512.22288)
*Renping Zhou,Zanlin Ni,Tianyi Chen,Zeyu Liu,Yang Yue,Yulin Wang,Yuxuan Wang,Jingshu Liu,Gao Huang*

Main category: cs.LG

TL;DR: Co-GRPO通过将掩码扩散模型重新表述为统一的马尔可夫决策过程，联合优化模型参数和推理调度参数，解决了训练与推理之间的不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型在训练和推理之间存在显著差异：推理是多步迭代过程，受模型和调度策略共同影响；而训练采用简化的单步BERT式目标，导致推理调度从未在训练中优化。

Method: 将MDM生成重新表述为统一的马尔可夫决策过程，应用轨迹级别的组相对策略优化，在共享奖励下协同优化模型参数和调度参数，无需通过多步生成过程进行昂贵的反向传播。

Result: 在ImageReward、HPS、GenEval和DPG-Bench四个基准测试中验证了方法的有效性，显著提升了生成质量。

Conclusion: Co-GRPO通过整体优化使训练与推理更一致，解决了MDM中训练-推理不一致的根本问题，为MDM提供了更统一的优化框架。

Abstract: Recently, Masked Diffusion Models (MDMs) have shown promising potential across vision, language, and cross-modal generation. However, a notable discrepancy exists between their training and inference procedures. In particular, MDM inference is a multi-step, iterative process governed not only by the model itself but also by various schedules that dictate the token-decoding trajectory (e.g., how many tokens to decode at each step). In contrast, MDMs are typically trained using a simplified, single-step BERT-style objective that masks a subset of tokens and predicts all of them simultaneously. This step-level simplification fundamentally disconnects the training paradigm from the trajectory-level nature of inference, leaving the inference schedules never optimized during training. In this paper, we introduce Co-GRPO, which reformulates MDM generation as a unified Markov Decision Process (MDP) that jointly incorporates both the model and the inference schedule. By applying Group Relative Policy Optimization at the trajectory level, Co-GRPO cooperatively optimizes model parameters and schedule parameters under a shared reward, without requiring costly backpropagation through the multi-step generation process. This holistic optimization aligns training with inference more thoroughly and substantially improves generation quality. Empirical results across four benchmarks-ImageReward, HPS, GenEval, and DPG-Bench-demonstrate the effectiveness of our approach. For more details, please refer to our project page: https://co-grpo.github.io/ .

</details>


### [39] [When Algorithms Manage Humans: A Double Machine Learning Approach to Estimating Nonlinear Effects of Algorithmic Control on Gig Worker Performance and Wellbeing](https://arxiv.org/abs/2512.22290)
*Arunkumar V,Nivethitha S,Sharan Srinivas,Gangadharan G. R*

Main category: cs.LG

TL;DR: 使用双重机器学习框架分析算法管理对工人福祉和绩效的非线性影响，发现算法监督的透明度是关键调节因素


<details>
  <summary>Details</summary>
Motivation: 研究算法管理时代人本管理能否持续，传统线性方法可能忽略工人对算法系统的非线性反应模式

Method: 采用双重机器学习框架估计调节中介模型，避免强加限制性函数形式，使用464名零工工人的调查数据

Result: 发现清晰的非单调模式：支持性HR实践提升工人福祉，但在算法监督模糊不清的"灰色中间地带"，其与绩效的联系减弱；当监督透明可解释时，联系再次增强

Conclusion: 简单线性设定可能得出相反结论；平台设计应避免部分定义的模糊控制，清晰规则和可信申诉机制能使强监督可行；双重机器学习为组织研究估计条件间接效应提供了方法

Abstract: A central question for the future of work is whether person centered management can survive when algorithms take on managerial roles. Standard tools often miss what is happening because worker responses to algorithmic systems are rarely linear. We use a Double Machine Learning framework to estimate a moderated mediation model without imposing restrictive functional forms. Using survey data from 464 gig workers, we find a clear nonmonotonic pattern. Supportive HR practices improve worker wellbeing, but their link to performance weakens in a murky middle where algorithmic oversight is present yet hard to interpret. The relationship strengthens again when oversight is transparent and explainable. These results show why simple linear specifications can miss the pattern and sometimes suggest the opposite conclusion. For platform design, the message is practical: control that is only partly defined creates confusion, but clear rules and credible recourse can make strong oversight workable. Methodologically, the paper shows how Double Machine Learning can be used to estimate conditional indirect effects in organizational research without forcing the data into a linear shape.

</details>


### [40] [Multi-Head Spectral-Adaptive Graph Anomaly Detection](https://arxiv.org/abs/2512.22291)
*Qingyue Cao,Bo Jin,Changwei Gong,Xin Tong,Wenzheng Li,Xiaodong Zhou*

Main category: cs.LG

TL;DR: 提出MHSA-GNN方法，通过轻量级超网络动态生成Chebyshev滤波器参数，结合双正则化策略解决图异常检测中异常节点伪装和异质性共存的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图异常检测方法在处理复杂多变的异常模式时面临挑战，异常节点常伪装并与正常节点混合，导致图中同质性和异质性共存。现有谱图神经网络使用固定全局滤波器，容易导致过度平滑，丢失欺诈检测所需的高频信号，且缺乏对不同图实例的自适应能力。

Method: 提出多头部谱自适应图神经网络(MHSA-GNN)：1) 设计轻量级超网络，基于包含结构统计和Rayleigh商特征的"谱指纹"，为每个实例动态生成Chebyshev滤波器参数；2) 引入双正则化策略，结合师生对比学习(TSC)确保表示准确性，以及Barlow Twins多样性损失(BTD)强制头部正交性，防止多头机制中的模式崩溃。

Result: 在四个真实世界数据集上的广泛实验表明，该方法能有效保留高频异常信号，显著优于现有最先进方法，特别是在高度异构数据集上表现出优异的鲁棒性。

Conclusion: MHSA-GNN通过实例特定的自适应滤波策略和双正则化机制，成功解决了图异常检测中的过度平滑和模式崩溃问题，为处理复杂异常模式提供了有效解决方案。

Abstract: Graph anomaly detection technology has broad applications in financial fraud and risk control. However, existing graph anomaly detection methods often face significant challenges when dealing with complex and variable abnormal patterns, as anomalous nodes are often disguised and mixed with normal nodes, leading to the coexistence of homophily and heterophily in the graph domain. Recent spectral graph neural networks have made notable progress in addressing this issue; however, current techniques typically employ fixed, globally shared filters. This 'one-size-fits-all' approach can easily cause over-smoothing, erasing critical high-frequency signals needed for fraud detection, and lacks adaptive capabilities for different graph instances. To solve this problem, we propose a Multi-Head Spectral-Adaptive Graph Neural Network (MHSA-GNN). The core innovation is the design of a lightweight hypernetwork that, conditioned on a 'spectral fingerprint' containing structural statistics and Rayleigh quotient features, dynamically generates Chebyshev filter parameters tailored to each instance. This enables a customized filtering strategy for each node and its local subgraph. Additionally, to prevent mode collapse in the multi-head mechanism, we introduce a novel dual regularization strategy that combines teacher-student contrastive learning (TSC) to ensure representation accuracy and Barlow Twins diversity loss (BTD) to enforce orthogonality among heads. Extensive experiments on four real-world datasets demonstrate that our method effectively preserves high-frequency abnormal signals and significantly outperforms existing state-of-the-art methods, especially showing excellent robustness on highly heterogeneous datasets.

</details>


### [41] [Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against](https://arxiv.org/abs/2512.22293)
*Tsogt-Ochir Enkhbayar*

Main category: cs.LG

TL;DR: 警告性内容（如"不要使用-此代码有漏洞"）无法有效教导语言模型避免不良行为，模型在警告和直接提供内容时的复制率无显著差异（76.7% vs 83.3%），原因是"描述X"和"执行X"激活了重叠的潜在特征。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解为什么包含警告的训练数据（如"不要使用此代码"）无法有效教导语言模型避免不良行为，探索当前架构中模型学习模式的根本局限性。

Method: 通过实验比较模型在警告性内容和直接内容下的行为表现，使用稀疏自编码器分析潜在特征激活模式，特别是特征#8684在代码执行模式中的表现，并分析"隐形滑移"现象。

Result: 警告性内容与直接提供内容在模型复制率上无统计显著差异（76.7% vs 83.3%），稀疏自编码器显示"描述X"和"执行X"激活重叠特征，特征#8684在两种情境下激活程度相似，训练时特征消融有效但提示和推理时引导无效。

Conclusion: 当前架构中统计共现主导于语用解释，模型学习的是上下文中倾向于出现的内容，而非理解内容出现的原因，这揭示了基于统计学习的语言模型的根本局限性。

Abstract: Warning-framed content in training data (e.g., "DO NOT USE - this code is vulnerable") does not, it turns out, teach language models to avoid the warned-against behavior. In experiments reported here, models exposed to such warnings reproduced the flagged content at rates statistically indistinguishable from models given the content directly (76.7% vs. 83.3%). Why? Sparse autoencoder analysis points to a failure of orthogonalization: "describing X" and "performing X" activate overlapping latent features. Feature #8684, which tracks code execution patterns, fires at comparable magnitude in both warning and exploitation contexts. A related phenomenon, what I call "stealth slip", allows conversational preambles to rotate activations into subspaces that linear probes miss entirely. Prompting and inference-time steering do not fix this; training-time feature ablation does. The upshot is that statistical co-occurrence dominates over pragmatic interpretation in current architectures. Models learn what tends to follow a context, not why it appeared there.

</details>


### [42] [Hybrid Quantum-Classical Mixture of Experts: Unlocking Topological Advantage via Interference-Based Routing](https://arxiv.org/abs/2512.22296)
*Reda Heddad,Lamiae Bouanane*

Main category: cs.LG

TL;DR: 提出混合量子-经典混合专家架构QMoE，通过量子路由网络利用量子干涉效应实现高效的非线性决策边界建模，在非线性能数据上展现拓扑优势。


<details>
  <summary>Details</summary>
Motivation: 传统混合专家架构存在专家不平衡和经典路由机制计算复杂度高的限制，探索量子机器学习解决这些问题的潜力。

Method: 采用混合量子-经典架构，使用量子门控网络作为路由器，结合经典专家网络，通过量子特征映射和波干涉实现高维核方法。

Result: 在Two Moons等非线性能数据集上，量子路由器展现出显著拓扑优势，能有效"解缠"数据分布，且对量子噪声具有鲁棒性，适合NISQ硬件。

Conclusion: 量子路由器通过量子干涉效应提供参数效率优势，验证了干涉假说，为联邦学习、隐私保护机器学习和自适应系统等应用提供了量子增强的路由范式。

Abstract: The Mixture-of-Experts (MoE) architecture has emerged as a powerful paradigm for scaling deep learning models, yet it is fundamentally limited by challenges such as expert imbalance and the computational complexity of classical routing mechanisms. This paper investigates the potential of Quantum Machine Learning (QML) to address these limitations through a novel Hybrid Quantum-Classical Mixture of Experts (QMoE) architecture. Specifically, we conduct an ablation study using a Quantum Gating Network (Router) combined with classical experts to isolate the source of quantum advantage. Our central finding validates the Interference Hypothesis: by leveraging quantum feature maps (Angle Embedding) and wave interference, the Quantum Router acts as a high-dimensional kernel method, enabling the modeling of complex, non-linear decision boundaries with superior parameter efficiency compared to its classical counterparts. Experimental results on non-linearly separable data, such as the Two Moons dataset, demonstrate that the Quantum Router achieves a significant topological advantage, effectively "untangling" data distributions that linear classical routers fail to separate efficiently. Furthermore, we analyze the architecture's robustness against simulated quantum noise, confirming its feasibility for near-term intermediate-scale quantum (NISQ) hardware. We discuss practical applications in federated learning, privacy-preserving machine learning, and adaptive systems that could benefit from this quantum-enhanced routing paradigm.

</details>


### [43] [Statistical and Machine Learning Analysis of Traffic Accidents on US 158 in Currituck County: A Comparison with HSM Predictions](https://arxiv.org/abs/2512.22302)
*Jennifer Sawyer,Julian Allagan*

Main category: cs.LG

TL;DR: 本研究扩展了Sawyer的热点分析，结合统计、机器学习和空间建模技术分析US 158公路5年事故数据，识别时空事故模式，为交通安全提供可操作见解。


<details>
  <summary>Details</summary>
Motivation: 扩展先前热点和卡方分析，通过先进方法提供更全面的农村公路安全分析，为US 158公路安全改进提供可操作见解，同时推动方法学进步。

Method: 整合核密度估计、负二项回归、随机森林分类和公路安全手册安全性能函数比较，使用Moran's I检验空间聚类，分析2019-2023年事故数据。

Result: 随机森林分类器预测伤害严重程度准确率达67%，优于HSM SPF；空间聚类得到确认；KDE分析识别主要交叉口附近的热点区域。

Conclusion: 研究验证并扩展了早期热点识别方法，支持针对性干预措施改善交通安全，为农村公路安全分析提供方法学贡献。

Abstract: This study extends previous hotspot and Chi-Square analysis by Sawyer \cite{sawyer2025hotspot} by integrating advanced statistical analysis, machine learning, and spatial modeling techniques to analyze five years (2019--2023) of traffic accident data from an 8.4-mile stretch of US 158 in Currituck County, NC. Building upon foundational statistical work, we apply Kernel Density Estimation (KDE), Negative Binomial Regression, Random Forest classification, and Highway Safety Manual (HSM) Safety Performance Function (SPF) comparisons to identify comprehensive temporal and spatial crash patterns. A Random Forest classifier predicts injury severity with 67\% accuracy, outperforming HSM SPF. Spatial clustering is confirmed via Moran's I test ($I = 0.32$, $p < 0.001$), and KDE analysis reveals hotspots near major intersections, validating and extending earlier hotspot identification methods. These results support targeted interventions to improve traffic safety on this vital transportation corridor. Our objective is to provide actionable insights for improving safety on US 158 while contributing to the broader understanding of rural highway safety analysis through methodological advancement beyond basic statistical techniques.

</details>


### [44] [PDx -- Adaptive Credit Risk Forecasting Model in Digital Lending using Machine Learning Operations](https://arxiv.org/abs/2512.22305)
*Sultan Amed,Chan Yu Hang,Sayantan Banerjee*

Main category: cs.LG

TL;DR: PDx是一个基于MLOps的自适应信用风险预测系统，通过动态冠军-挑战者框架和持续模型监控来解决传统违约概率模型静态化、性能随时间下降的问题。


<details>
  <summary>Details</summary>
Motivation: 传统违约概率模型过于关注开发阶段的预测准确性，但缺乏对借款人行为变化的持续适应能力，导致模型在生产环境中性能随时间下降。同时，金融机构难以将机器学习模型从开发环境部署到生产环境并维持其健康状态。

Method: 采用动态端到端模型生命周期管理方法，集成持续模型监控、重新训练和验证的MLOps管道。引入动态冠军-挑战者框架，定期更新基线模型，用最新数据重新校准参数，并通过时间外验证选择最佳性能模型。

Result: 决策树集成模型在违约分类中表现最佳但需要频繁更新；线性模型和神经网络性能退化更严重。PDx能有效减轻数字贷款机构的价值侵蚀，特别是在短期小额贷款中。系统已在P2P贷款、商业贷款和汽车贷款数据集上验证有效。

Conclusion: PDx通过MLOps驱动的自适应决策系统解决了传统信用风险预测模型的局限性，为现代信用风险预测提供了可扩展和适应性强的解决方案，特别适用于借款人行为快速变化的环境。

Abstract: This paper presents PDx, an adaptive, machine learning operations (MLOps) driven decision system for forecasting credit risk using probability of default (PD) modeling in digital lending. While conventional PD models prioritize predictive accuracy during model development with complex machine learning algorithms, they often overlook continuous adaptation to changing borrower behaviour, resulting in static models that degrade over time in production and generate inaccurate default predictions. Many financial institutes also find it difficult transitioning ML models from development environment to production and maintaining their health. With PDx we aimed to addresses these limitations using a dynamic, end-to-end model lifecycle management approach that integrates continuous model monitoring, retraining, and validation through a robust MLOps pipeline. We introduced a dynamic champion-challenger framework for PDx to regularly update baseline models to recalibrate independent parameters with the latest data and select the best-performing model through out-of-time validation, ensuring resilience against data drift and changing credit risk patterns. Our empirical analysis shows that decision tree-based ensemble models consistently outperform others in classifying defaulters but require frequent updates to sustain performance. Linear models (e.g., logistic regression) and neural networks exhibit greater performance degradation. The study demonstrate with PDx we can mitigates value erosion for digital lenders, particularly in short-term, small-ticket loans, where borrower behavior shifts rapidly. We have validated the effectiveness of PDx using datasets from peer-to-peer lending, business loans, and auto loans, demonstrating its scalability and adaptability for modern credit risk forecasting.

</details>


### [45] [LLMBoost: Make Large Language Models Stronger with Boosting](https://arxiv.org/abs/2512.22309)
*Zehao Chen,Tianxiang Ai,Yifei Li,Gongxun Li,Yuyang Wei,Wang Zhou,Guanghui Li,Bin Yu,Zhijun Chen,Hailong Sun,Fuzhen Zhuang,Jianxin Li,Deqing Wang,Yikun Ban*

Main category: cs.LG

TL;DR: LLMBoost：一种新颖的LLM集成微调框架，通过跨模型注意力机制、链式训练和近并行推理，利用中间状态提升性能并降低延迟


<details>
  <summary>Details</summary>
Motivation: 现有LLM集成方法通常将模型视为黑盒，仅结合输入或最终输出，忽略了丰富的内部表示和跨模型交互。需要打破这种限制，充分利用LLM的中间状态来提升集成效果。

Method: 1. 跨模型注意力机制：使后继模型能够访问和融合前驱模型的隐藏状态，实现分层错误校正和知识转移；2. 链式训练范式：以误差抑制为目标逐步微调连接模型，确保每个模型以最小计算量纠正前驱模型的错误预测；3. 近并行推理范式：逐层在模型间流水线传输隐藏状态，实现接近单模型解码的推理效率。

Result: 在常识推理和算术推理任务上的广泛实验表明，LLMBoost能够持续提升准确性，同时降低推理延迟。理论分析证明，在有限校正假设下，顺序集成能保证单调改进。

Conclusion: LLMBoost通过利用LLM的中间状态，打破了传统集成方法的黑盒限制，实现了性能提升与推理效率的平衡，为LLM集成学习提供了新的有效框架。

Abstract: Ensemble learning of LLMs has emerged as a promising alternative to enhance performance, but existing approaches typically treat models as black boxes, combining the inputs or final outputs while overlooking the rich internal representations and interactions across models.In this work, we introduce LLMBoost, a novel ensemble fine-tuning framework that breaks this barrier by explicitly leveraging intermediate states of LLMs. Inspired by the boosting paradigm, LLMBoost incorporates three key innovations. First, a cross-model attention mechanism enables successor models to access and fuse hidden states from predecessors, facilitating hierarchical error correction and knowledge transfer. Second, a chain training paradigm progressively fine-tunes connected models with an error-suppression objective, ensuring that each model rectifies the mispredictions of its predecessor with minimal additional computation. Third, a near-parallel inference paradigm design pipelines hidden states across models layer by layer, achieving inference efficiency approaching single-model decoding. We further establish the theoretical foundations of LLMBoost, proving that sequential integration guarantees monotonic improvements under bounded correction assumptions. Extensive experiments on commonsense reasoning and arithmetic reasoning tasks demonstrate that LLMBoost consistently boosts accuracy while reducing inference latency.

</details>


### [46] [Optimistic Feasible Search for Closed-Loop Fair Threshold Decision-Making](https://arxiv.org/abs/2512.22313)
*Wenzhang Du*

Main category: cs.LG

TL;DR: 提出Optimistic Feasible Search (OFS)方法，用于在公平性和服务率约束下在线学习一维阈值策略，通过维护置信区间选择可行且奖励最大的阈值。


<details>
  <summary>Details</summary>
Motivation: 闭环决策系统（如贷款、筛选、再犯风险评估）在公平性和服务约束下运行时会引发反馈效应：决策改变未来人群分布，导致数据非平稳并可能放大不平等。需要在线学习满足约束的阈值策略。

Method: 提出OFS方法：基于网格维护每个候选阈值的奖励和约束残差的置信区间。每轮选择置信区间内可行的阈值中乐观奖励最大的；若无可行阈值，则选择乐观约束违反最小的阈值。

Result: 在合成闭环基准测试和基于German Credit、COMPAS的半合成闭环基准测试中，OFS相比无约束和原始对偶bandit基线获得更高奖励和更小的累积约束违反，接近最优固定阈值。

Conclusion: OFS方法能有效在公平性和服务约束下在线学习阈值策略，特别适合低维可解释策略类，在多种环境中表现优于基线方法。

Abstract: Closed-loop decision-making systems (e.g., lending, screening, or recidivism risk assessment) often operate under fairness and service constraints while inducing feedback effects: decisions change who appears in the future, yielding non-stationary data and potentially amplifying disparities. We study online learning of a one-dimensional threshold policy from bandit feedback under demographic parity (DP) and, optionally, service-rate constraints. The learner observes only a scalar score each round and selects a threshold; reward and constraint residuals are revealed only for the chosen threshold.
  We propose Optimistic Feasible Search (OFS), a simple grid-based method that maintains confidence bounds for reward and constraint residuals for each candidate threshold. At each round, OFS selects a threshold that appears feasible under confidence bounds and, among those, maximizes optimistic reward; if no threshold appears feasible, OFS selects the threshold minimizing optimistic constraint violation. This design directly targets feasible high-utility thresholds and is particularly effective for low-dimensional, interpretable policy classes where discretization is natural.
  We evaluate OFS on (i) a synthetic closed-loop benchmark with stable contraction dynamics and (ii) two semi-synthetic closed-loop benchmarks grounded in German Credit and COMPAS, constructed by training a score model and feeding group-dependent acceptance decisions back into population composition. Across all environments, OFS achieves higher reward with smaller cumulative constraint violation than unconstrained and primal-dual bandit baselines, and is near-oracle relative to the best feasible fixed threshold under the same sweep procedure. Experiments are reproducible and organized with double-blind-friendly relative outputs.

</details>


### [47] [LangPrecip: Language-Aware Multimodal Precipitation Nowcasting](https://arxiv.org/abs/2512.22317)
*Xudong Ling,Tianxi Huang,Qian Dong,Tao He,Chaorong Li,Guiduo Duan*

Main category: cs.LG

TL;DR: LangPrecip：一个语言感知的多模态降水临近预报框架，通过将气象文本作为降水演化的语义运动约束，在Rectified Flow范式下将文本和雷达信息高效整合，显著提升强降水预报精度。


<details>
  <summary>Details</summary>
Motivation: 短期降水临近预报存在高度不确定性和约束不足的问题，特别是对于快速发展和极端天气事件。现有生成方法主要依赖视觉条件，导致未来运动约束弱且模糊。需要更有效的约束方法来提高预报准确性。

Method: 提出LangPrecip框架，将气象文本作为降水演化的语义运动约束，在Rectified Flow范式下将临近预报建模为语义约束的轨迹生成问题。在潜在空间中高效整合文本和雷达信息，并构建了包含16万对雷达序列和运动描述的大规模多模态数据集LangPrecip-160k。

Result: 在瑞典和MRMS数据集上的实验表明，该方法相比最先进方法取得一致改进，在80分钟预报时效下，强降水CSI指标分别获得超过60%和19%的提升。

Conclusion: 语言感知的多模态框架能够有效利用文本语义信息约束降水运动演化，显著提升短期降水临近预报的准确性，特别是在强降水事件中表现优异。

Abstract: Short-term precipitation nowcasting is an inherently uncertain and under-constrained spatiotemporal forecasting problem, especially for rapidly evolving and extreme weather events. Existing generative approaches rely primarily on visual conditioning, leaving future motion weakly constrained and ambiguous. We propose a language-aware multimodal nowcasting framework(LangPrecip) that treats meteorological text as a semantic motion constraint on precipitation evolution. By formulating nowcasting as a semantically constrained trajectory generation problem under the Rectified Flow paradigm, our method enables efficient and physically consistent integration of textual and radar information in latent space.We further introduce LangPrecip-160k, a large-scale multimodal dataset with 160k paired radar sequences and motion descriptions. Experiments on Swedish and MRMS datasets show consistent improvements over state-of-the-art methods, achieving over 60 \% and 19\% gains in heavy-rainfall CSI at an 80-minute lead time.

</details>


### [48] [Decomposing Uncertainty in Probabilistic Knowledge Graph Embeddings: Why Entity Variance Is Not Enough](https://arxiv.org/abs/2512.22318)
*Chorok Lee*

Main category: cs.LG

TL;DR: 现有概率知识图谱嵌入方法存在关系无关的实体不确定性估计问题，无法区分新兴实体和新型关系上下文两种不同的分布外现象。本文提出将不确定性分解为语义不确定性和结构不确定性，并通过CAGP方法结合两者，显著提升了分布外检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有概率知识图谱嵌入方法使用实体级方差量化认知不确定性，但这些方差是关系无关的，导致无法区分两种行为相反的分布外现象：新兴实体（罕见、学习不足）和新型关系上下文（熟悉实体在未观察关系中的出现）。

Method: 将不确定性分解为两个互补成分：来自实体嵌入方差的语义不确定性（检测新兴实体）和来自实体-关系共现的结构不确定性（检测新型上下文）。提出CAGP方法，通过学习的权重结合语义和结构不确定性。

Result: 在三个数据集（FB15k-237、WN18RR、YAGO3-10）上验证了理论结果，发现100%的新型上下文三元组都有频率匹配的分布内对应项。CAGP方法在时间分布外检测上达到0.94-0.99 AUROC，相比关系无关基线有60-80%的相对改进。在选择性预测中，在85%回答率下减少43%的错误。

Conclusion: 关系无关的不确定性估计存在根本限制，无法有效检测新型关系上下文。通过将不确定性分解为语义和结构成分并适当结合，可以显著提升分布外检测性能，为知识图谱嵌入中的不确定性量化提供了更精细的框架。

Abstract: Probabilistic knowledge graph embeddings represent entities as distributions, using learned variances to quantify epistemic uncertainty. We identify a fundamental limitation: these variances are relation-agnostic, meaning an entity receives identical uncertainty regardless of relational context. This conflates two distinct out-of-distribution phenomena that behave oppositely: emerging entities (rare, poorly-learned) and novel relational contexts (familiar entities in unobserved relationships). We prove an impossibility result: any uncertainty estimator using only entity-level statistics independent of relation context achieves near-random OOD detection on novel contexts. We empirically validate this on three datasets, finding 100 percent of novel-context triples have frequency-matched in-distribution counterparts. This explains why existing probabilistic methods achieve 0.99 AUROC on random corruptions but only 0.52-0.64 on temporal distribution shift. We formalize uncertainty decomposition into complementary components: semantic uncertainty from entity embedding variance (detecting emerging entities) and structural uncertainty from entity-relation co-occurrence (detecting novel contexts). Our main theoretical result proves these signals are non-redundant, and that any convex combination strictly dominates either signal alone. Our method (CAGP) combines semantic and structural uncertainty via learned weights, achieving 0.94-0.99 AUROC on temporal OOD detection across multiple benchmarks, a 60-80 percent relative improvement over relation-agnostic baselines. Empirical validation confirms complete frequency overlap on three datasets (FB15k-237, WN18RR, YAGO3-10). On selective prediction, our method reduces errors by 43 percent at 85 percent answer rate.

</details>


### [49] [Expert System for Bitcoin Forecasting: Integrating Global Liquidity via TimeXer Transformers](https://arxiv.org/abs/2512.22326)
*Sravan Karthick T*

Main category: cs.LG

TL;DR: 论文提出一种结合全球M2流动性作为外生变量的TimeXer-Exog模型，显著提升了比特币长期价格预测的稳定性，相比单变量模型误差降低89%以上。


<details>
  <summary>Details</summary>
Motivation: 比特币价格预测面临极端波动性和非平稳性挑战，传统单变量时间序列模型在长期预测中表现不佳。本文旨在通过引入宏观经济变量来填补这一空白。

Method: 采用TimeXer架构，将18个主要经济体的全球M2流动性作为领先外生变量（12周滞后结构），构建流动性条件预测模型TimeXer-Exog，并与LSTM、N-BEATS、PatchTST等基准模型对比。

Result: 在2020年1月至2025年8月的比特币日价格数据上，TimeXer-Exog在70天预测范围内MSE为1.08e8，比单变量TimeXer基线模型性能提升超过89%，显著稳定了长期预测。

Conclusion: 将深度学习模型与全球流动性条件相结合，能显著改善比特币长期价格预测性能，表明宏观经济因素在加密货币预测中的重要性。

Abstract: Bitcoin price forecasting is characterized by extreme volatility and non-stationarity, often defying traditional univariate time-series models over long horizons. This paper addresses a critical gap by integrating Global M2 Liquidity, aggregated from 18 major economies, as a leading exogenous variable with a 12-week lag structure. Using the TimeXer architecture, we compare a liquidity-conditioned forecasting model (TimeXer-Exog) against state-of-the-art benchmarks including LSTM, N-BEATS, PatchTST, and a standard univariate TimeXer. Experiments conducted on daily Bitcoin price data from January 2020 to August 2025 demonstrate that explicit macroeconomic conditioning significantly stabilizes long-horizon forecasts. At a 70-day forecast horizon, the proposed TimeXer-Exog model achieves a mean squared error (MSE) 1.08e8, outperforming the univariate TimeXer baseline by over 89 percent. These results highlight that conditioning deep learning models on global liquidity provides substantial improvements in long-horizon Bitcoin price forecasting.

</details>


### [50] [The Effectiveness of Approximate Regularized Replay for Efficient Supervised Fine-Tuning of Large Language Models](https://arxiv.org/abs/2512.22337)
*Matthew Riemer,Erik Miehling,Miao Liu,Djallel Bouneffouf,Murray Campbell*

Main category: cs.LG

TL;DR: LoRA微调可能导致模型能力灾难性下降，但通过正则化近似回放方法（惩罚与初始模型的KL散度，并混合预训练风格的文本预测数据）可以解决此问题，在保留通用知识的同时不阻碍新任务学习能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LoRA等参数高效微调方法只修改少量参数，但指令微调实验显示它们可能导致模型能力灾难性下降，即使在小数据集上训练少量步骤也会发生。需要找到既能保持模型通用知识又不阻碍新任务学习能力的微调方法。

Method: 提出正则化近似回放方法：1）惩罚与初始模型的KL散度作为正则化；2）在训练中混合来自与预训练相似但不同的开放访问语料库的下一词预测数据。这种方法计算开销很小。

Result: 在Qwen指令微调模型上应用该方法，发现能够有效保留模型中的通用知识，同时不阻碍对新任务的可塑性，仅增加适度的计算开销。

Conclusion: 虽然简单的LoRA微调方法在实践中可能导致严重问题，但通过正则化近似回放的小调整可以几乎完全消除灾难性能力下降问题，实现参数高效微调的同时保持模型原有能力。

Abstract: Although parameter-efficient fine-tuning methods, such as LoRA, only modify a small subset of parameters, they can have a significant impact on the model. Our instruction-tuning experiments show that LoRA-based supervised fine-tuning can catastrophically degrade model capabilities, even when trained on very small datasets for relatively few steps. With that said, we demonstrate that while the most straightforward approach (that is likely the most used in practice) fails spectacularly, small tweaks to the training procedure with very little overhead can virtually eliminate the problem. Particularly, in this paper we consider a regularized approximate replay approach which penalizes KL divergence with respect to the initial model and interleaves in data for next token prediction from a different, yet similar, open access corpus to what was used in pre-training. When applied to Qwen instruction-tuned models, we find that this recipe preserves general knowledge in the model without hindering plasticity to new tasks by adding a modest amount of computational overhead.

</details>


### [51] [Completed Hyperparameter Transfer across Modules, Width, Depth, Batch and Duration](https://arxiv.org/abs/2512.22382)
*Bruno Mlodozeniec,Pierre Ablin,Louis Béthune,Dan Busbridge,Michal Klein,Jason Ramapuram,Marco Cuturi*

Main category: cs.LG

TL;DR: 本文提出Complete⁽ᵈ⁾参数化方法，统一处理宽度、深度、批大小和训练时长的缩放，并研究模块级超参数优化与迁移，显著提升大语言模型的训练速度。


<details>
  <summary>Details</summary>
Motivation: 超参数调优对大规模模型的训练稳定性和最终性能有巨大影响。现有方法如μP虽然实现了全局超参数在不同模型规模间的迁移，但主要关注宽度缩放，缺乏对深度、批大小等多维缩放轴的支持，且未探索模块级超参数优化。

Method: 提出Complete⁽ᵈ⁾参数化方法，基于CompleteP的改进版本，统一处理宽度、深度、批大小和训练时长的缩放。研究模块级超参数优化与迁移，分析高维超参数空间的导航挑战，并提出实用的优化指南。

Result: 实验证明，在正确的参数化下，超参数迁移在模块级超参数体系中也成立。研究覆盖了现代模型的关键优化超参数：学习率、AdamW参数、权重衰减、初始化尺度和残差块乘数。大语言模型实验显示，迁移的模块级超参数能显著提升训练速度。

Conclusion: Complete⁽ᵈ⁾参数化方法成功统一了多个重要缩放轴，模块级超参数优化与迁移是可行的，为大规模模型的高效训练提供了实用指导。

Abstract: Hyperparameter tuning can dramatically impact training stability and final performance of large-scale models. Recent works on neural network parameterisations, such as $μ$P, have enabled transfer of optimal global hyperparameters across model sizes. These works propose an empirical practice of search for optimal global base hyperparameters at a small model size, and transfer to a large size. We extend these works in two key ways. To handle scaling along most important scaling axes, we propose the Complete$^{(d)}$ Parameterisation that unifies scaling in width and depth -- using an adaptation of CompleteP -- as well as in batch-size and training duration. Secondly, with our parameterisation, we investigate per-module hyperparameter optimisation and transfer. We characterise the empirical challenges of navigating the high-dimensional hyperparameter landscape, and propose practical guidelines for tackling this optimisation problem. We demonstrate that, with the right parameterisation, hyperparameter transfer holds even in the per-module hyperparameter regime. Our study covers an extensive range of optimisation hyperparameters of modern models: learning rates, AdamW parameters, weight decay, initialisation scales, and residual block multipliers. Our experiments demonstrate significant training speed improvements in Large Language Models with the transferred per-module hyperparameters.

</details>


### [52] [BLISS: Bandit Layer Importance Sampling Strategy for Efficient Training of Graph Neural Networks](https://arxiv.org/abs/2512.22388)
*Omar Alsaqa,Linh Thi Hoang,Muhammed Fatih Balin*

Main category: cs.LG

TL;DR: BLISS：基于多臂老虎机的动态层重要性采样策略，用于解决GNN在大图上的计算瓶颈问题


<details>
  <summary>Details</summary>
Motivation: 图神经网络在处理大规模图数据时面临计算成本高的问题，需要处理每个节点的所有邻居导致内存和计算瓶颈。现有静态采样方法无法适应节点重要性的动态变化。

Method: 提出BLISS（Bandit Layer Importance Sampling Strategy），使用多臂老虎机动态选择每层中最具信息量的节点，平衡探索与利用，确保全面的图覆盖。该方法可集成到GCN和GAT中，根据其特定的聚合机制调整选择策略。

Result: 实验表明BLISS能够保持或超过全批次训练的准确性，同时显著降低计算成本。

Conclusion: BLISS提供了一种有效的动态采样策略，解决了GNN在大图上的可扩展性问题，通过自适应节点选择提高了训练效率而不牺牲性能。

Abstract: Graph Neural Networks (GNNs) are powerful tools for learning from graph-structured data, but their application to large graphs is hindered by computational costs. The need to process every neighbor for each node creates memory and computational bottlenecks. To address this, we introduce BLISS, a Bandit Layer Importance Sampling Strategy. It uses multi-armed bandits to dynamically select the most informative nodes at each layer, balancing exploration and exploitation to ensure comprehensive graph coverage. Unlike existing static sampling methods, BLISS adapts to evolving node importance, leading to more informed node selection and improved performance. It demonstrates versatility by integrating with both Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs), adapting its selection policy to their specific aggregation mechanisms. Experiments show that BLISS maintains or exceeds the accuracy of full-batch training.

</details>


### [53] [Causality-Inspired Safe Residual Correction for Multivariate Time Series](https://arxiv.org/abs/2512.22428)
*Jianxiang Xie,Yuncheng Hua*

Main category: cs.LG

TL;DR: CRC是一个因果启发的安全残差校正框架，通过解耦自变量和交叉变量动态，采用四重安全机制防止性能退化，确保部署安全。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer和GNN等多元预测模型存在系统性误差，且缺乏部署性能保障。传统后处理残差校正方法虽然可能提高平均准确率，但可能"错误地帮助"——过度校正可靠预测并在未见场景中导致局部失败。

Method: CRC采用分而治之策略：1) 因果启发编码器通过解耦自变量和交叉变量动态来暴露方向感知结构；2) 混合校正器建模残差误差；3) 关键的四重安全机制控制校正过程，防止有害更新。

Result: 在多个数据集和预测骨干网络上的实验表明，CRC能持续提高准确性，消融研究确认其核心安全机制确保极高的非退化率(NDR)，适合安全可靠部署。

Conclusion: CRC填补了预测模型部署中的"安全鸿沟"，提供了一个即插即用的校正框架，既能提高准确性，又能保证非退化性能，适合实际应用中的安全部署。

Abstract: While modern multivariate forecasters such as Transformers and GNNs achieve strong benchmark performance, they often suffer from systematic errors at specific variables or horizons and, critically, lack guarantees against performance degradation in deployment. Existing post-hoc residual correction methods attempt to fix these errors, but are inherently greedy: although they may improve average accuracy, they can also "help in the wrong way" by overcorrecting reliable predictions and causing local failures in unseen scenarios.
  To address this critical "safety gap," we propose CRC (Causality-inspired Safe Residual Correction), a plug-and-play framework explicitly designed to ensure non-degradation. CRC follows a divide-and-conquer philosophy: it employs a causality-inspired encoder to expose direction-aware structure by decoupling self- and cross-variable dynamics, and a hybrid corrector to model residual errors. Crucially, the correction process is governed by a strict four-fold safety mechanism that prevents harmful updates.
  Experiments across multiple datasets and forecasting backbones show that CRC consistently improves accuracy, while an in-depth ablation study confirms that its core safety mechanisms ensure exceptionally high non-degradation rates (NDR), making CRC a correction framework suited for safe and reliable deployment.

</details>


### [54] [AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing](https://arxiv.org/abs/2512.22455)
*Jiacheng Li,Jianchao Tan,Zhidong Yang,Feiye Huo,Yerui Sun,Yuchen Xie,Xunliang Cai*

Main category: cs.LG

TL;DR: AFA-LoRA：一种通过退火激活函数为LoRA引入非线性表达能力的新型训练策略，在保持可合并性的同时缩小了LoRA与全参数微调的性能差距。


<details>
  <summary>Details</summary>
Motivation: LoRA作为广泛采用的参数高效微调方法，其线性适应过程限制了表达能力，导致线性训练与非线性训练之间存在性能差距。

Method: 提出AFA-LoRA训练策略，核心创新是退火激活函数，在训练过程中从非线性变换逐渐过渡到线性变换，使适配器先获得更强的表示能力，最终收敛到可合并的线性形式。

Result: 在监督微调、强化学习和推测解码等任务上实施该方法，结果表明AFA-LoRA显著缩小了LoRA与全参数训练之间的性能差距。

Conclusion: 这项工作实现了更强大、更实用的参数高效适应范式，为LoRA类方法带来了非线性表达能力。

Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method. However, its linear adaptation process limits its expressive power. This means there is a gap between the expressive power of linear training and non-linear training. To bridge this gap, we propose AFA-LoRA, a novel training strategy that brings non-linear expressivity to LoRA while maintaining its seamless mergeability. Our key innovation is an annealed activation function that transitions from a non-linear to a linear transformation during training, allowing the adapter to initially adopt stronger representational capabilities before converging to a mergeable linear form. We implement our method on supervised fine-tuning, reinforcement learning, and speculative decoding. The results show that AFA-LoRA reduces the performance gap between LoRA and full-parameter training. This work enables a more powerful and practical paradigm of parameter-efficient adaptation.

</details>


### [55] [AMBIT: Augmenting Mobility Baselines with Interpretable Trees](https://arxiv.org/abs/2512.22466)
*Qizhi Wang*

Main category: cs.LG

TL;DR: AMBIT是一个灰盒框架，通过可解释的树模型增强物理移动基线，在保持可解释性的同时实现高精度OD流预测


<details>
  <summary>Details</summary>
Motivation: OD流预测在GIS和城市分析中是核心任务，但实际部署面临两个冲突需求：高准确性和清晰可解释性。现有物理模型在小时级时间分辨率下表现脆弱

Method: 1) 在纽约市出租车OD数据集上全面审计经典空间交互模型；2) 在物理基线之上构建残差学习器，使用梯度提升树和SHAP分析；3) 提供可复现的管道、丰富的诊断和空间误差分析

Result: PPML重力模型是最强的物理基线；基于物理的残差方法接近强树基预测器的准确性同时保持可解释结构；POI锚定的残差在空间泛化下最具鲁棒性

Conclusion: AMBIT框架成功平衡了OD流预测的准确性和可解释性需求，为城市决策提供了实用的灰盒解决方案

Abstract: Origin-destination (OD) flow prediction remains a core task in GIS and urban analytics, yet practical deployments face two conflicting needs: high accuracy and clear interpretability. This paper develops AMBIT, a gray-box framework that augments physical mobility baselines with interpretable tree models. We begin with a comprehensive audit of classical spatial interaction models on a year-long, hourly NYC taxi OD dataset. The audit shows that most physical models are fragile at this temporal resolution; PPML gravity is the strongest physical baseline, while constrained variants improve when calibrated on full OD margins but remain notably weaker. We then build residual learners on top of physical baselines using gradient-boosted trees and SHAP analysis, demonstrating that (i) physics-grounded residuals approach the accuracy of a strong tree-based predictor while retaining interpretable structure, and (ii) POI-anchored residuals are consistently competitive and most robust under spatial generalization. We provide a reproducible pipeline, rich diagnostics, and spatial error analysis designed for urban decision-making.

</details>


### [56] [GLUE: Gradient-free Learning to Unify Experts](https://arxiv.org/abs/2512.22467)
*Jong-Ik Park,Shreyas Chaudhari,Srinivasa Pranav,Carlee Joe-Wong,José M. F. Moura*

Main category: cs.LG

TL;DR: GLUE提出了一种无需梯度的专家模型融合方法，通过两点SPSA更新学习混合系数，只需两次前向传播即可有效初始化目标模型，在多个数据集和架构上优于启发式融合方法。


<details>
  <summary>Details</summary>
Motivation: 现有系统通常包含多个预训练的专家模型，但新目标域需要超越单个专家能力的泛化模型。传统方法通过启发式融合（基于数据量或代理指标）初始化目标模型，但这种方法通常导致目标域测试准确率较低，而基于梯度的学习方法需要昂贵的全网络反向传播计算。

Method: GLUE（Gradient-free Learning To Unify Experts）将目标模型初始化为固定专家模型的凸组合，通过无需梯度的两点（SPSA）更新学习混合系数，每个步骤仅需两次前向传播，避免了昂贵的反向传播计算。

Result: 在三个数据集和三种网络架构上的实验表明，GLUE产生的单一先验经过微调后能有效超越基线方法。相比基于数据量的加权方法，GLUE将测试准确率提升高达8.5%；相比基于代理指标的选择方法，提升高达9.1%。GLUE要么优于基于反向传播的全梯度混合方法，要么与其性能相差在1.4%以内。

Conclusion: GLUE提供了一种高效、无需梯度的专家模型融合方法，能够为新的目标域生成高质量的初始化模型，在保持计算效率的同时显著提升目标域性能，为多专家模型集成提供了实用解决方案。

Abstract: In many deployed systems (multilingual ASR, cross-hospital imaging, region-specific perception), multiple pretrained specialist models coexist. Yet, new target domains often require domain expansion: a generalized model that performs well beyond any single specialist's domain. Given such a new target domain, prior works seek a single strong initialization prior for the model parameters by first blending expert models to initialize a target model. However, heuristic blending -- using coefficients based on data size or proxy metrics -- often yields lower target-domain test accuracy, and learning the coefficients on the target loss typically requires computationally-expensive full backpropagation through the network. We propose GLUE, Gradient-free Learning To Unify Experts, which initializes the target model as a convex combination of fixed experts, learning the mixture coefficients of this combination via a gradient-free two-point (SPSA) update that requires only two forward passes per step. Across experiments on three datasets and three network architectures, GLUE produces a single prior that can be fine-tuned effectively to outperform baselines. GLUE improves test accuracy by up to 8.5% over data-size weighting and by up to 9.1% over proxy-metric selection. GLUE either outperforms backpropagation-based full-gradient mixing or matches its performance within 1.4%.

</details>


### [57] [The Bayesian Geometry of Transformer Attention](https://arxiv.org/abs/2512.22471)
*Naman Aggarwal,Siddhartha R. Dalal,Vishal Misra*

Main category: cs.LG

TL;DR: 小Transformer在受控环境中能准确执行贝叶斯推理，而容量匹配的MLP则失败，表明注意力机制对推理至关重要


<details>
  <summary>Details</summary>
Motivation: 验证Transformer是否真正执行贝叶斯推理存在困难：自然数据缺乏解析后验，大模型混淆推理与记忆。需要构建受控环境来严格验证

Method: 构建"贝叶斯风洞"——后验分布已知且记忆不可能的受控环境。在两个任务（双射消除和隐马尔可夫模型状态跟踪）中分析小Transformer的机制

Result: 小Transformer能以10^-3-10^-4比特精度重现贝叶斯后验，而容量匹配的MLP则失败几个数量级。Transformer通过残差流作为信念基底、前馈网络执行后验更新、注意力提供内容寻址路由来实现贝叶斯推理

Conclusion: 分层注意力通过几何设计实现贝叶斯推理，解释了注意力机制的必要性和平面架构的失败。贝叶斯风洞为从小型可验证系统连接到大型语言模型中的推理现象提供了基础

Abstract: Transformers often appear to perform Bayesian reasoning in context, but verifying this rigorously has been impossible: natural data lack analytic posteriors, and large models conflate reasoning with memorization. We address this by constructing \emph{Bayesian wind tunnels} -- controlled environments where the true posterior is known in closed form and memorization is provably impossible. In these settings, small transformers reproduce Bayesian posteriors with $10^{-3}$-$10^{-4}$ bit accuracy, while capacity-matched MLPs fail by orders of magnitude, establishing a clear architectural separation.
  Across two tasks -- bijection elimination and Hidden Markov Model (HMM) state tracking -- we find that transformers implement Bayesian inference through a consistent geometric mechanism: residual streams serve as the belief substrate, feed-forward networks perform the posterior update, and attention provides content-addressable routing. Geometric diagnostics reveal orthogonal key bases, progressive query-key alignment, and a low-dimensional value manifold parameterized by posterior entropy. During training this manifold unfurls while attention patterns remain stable, a \emph{frame-precision dissociation} predicted by recent gradient analyses.
  Taken together, these results demonstrate that hierarchical attention realizes Bayesian inference by geometric design, explaining both the necessity of attention and the failure of flat architectures. Bayesian wind tunnels provide a foundation for mechanistically connecting small, verifiable systems to reasoning phenomena observed in large language models.

</details>


### [58] [Collaborative Optimization of Multiclass Imbalanced Learning: Density-Aware and Region-Guided Boosting](https://arxiv.org/abs/2512.22478)
*Chuantao Li,Zhi Li,Jiahao Xu,Jie Li,Sheng Li*

Main category: cs.LG

TL;DR: 提出一种协同优化的Boosting模型，通过整合密度因子和置信度因子，设计抗噪声权重更新机制和动态采样策略，实现不平衡学习与模型训练的协同优化。


<details>
  <summary>Details</summary>
Motivation: 现有研究尚未探索不平衡学习与模型训练的协同优化，这一限制阻碍了性能的进一步提升。需要填补这一空白。

Method: 提出协同优化的多类不平衡学习Boosting模型，整合密度因子和置信度因子，设计抗噪声权重更新机制和动态采样策略。这些模块紧密集成，协调权重更新、样本区域划分和区域引导采样。

Result: 在20个公共不平衡数据集上的广泛实验表明，该模型显著优于8个最先进的基线方法。

Conclusion: 通过协同优化不平衡学习与模型训练，实现了简单而有效的多类不平衡学习Boosting模型，显著提升了分类性能。

Abstract: Numerous studies attempt to mitigate classification bias caused by class imbalance. However, existing studies have yet to explore the collaborative optimization of imbalanced learning and model training. This constraint hinders further performance improvements. To bridge this gap, this study proposes a collaborative optimization Boosting model of multiclass imbalanced learning. This model is simple but effective by integrating the density factor and the confidence factor, this study designs a noise-resistant weight update mechanism and a dynamic sampling strategy. Rather than functioning as independent components, these modules are tightly integrated to orchestrate weight updates, sample region partitioning, and region-guided sampling. Thus, this study achieves the collaborative optimization of imbalanced learning and model training. Extensive experiments on 20 public imbalanced datasets demonstrate that the proposed model significantly outperforms eight state-of-the-art baselines. The code for the proposed model is available at: https://github.com/ChuantaoLi/DARG.

</details>


### [59] [Toward Real-World IoT Security: Concept Drift-Resilient IoT Botnet Detection via Latent Space Representation Learning and Alignment](https://arxiv.org/abs/2512.22488)
*Hassan Wasswa,Timothy Lynar*

Main category: cs.LG

TL;DR: 提出一个无需持续重训练的自适应物联网威胁检测框架，通过潜在空间对齐和GNN分类来应对概念漂移问题


<details>
  <summary>Details</summary>
Motivation: 现有AI模型依赖静态数据集，无法适应真实物联网NetFlow流量的动态变化，而周期性重训练方案计算开销大且存在灾难性遗忘风险

Method: 1) 在历史流量潜在空间表示上训练一次分类器；2) 使用对齐模型将新流量映射到学习到的历史潜在空间；3) 将低维潜在表示转换为图结构格式，使用图神经网络进行分类

Result: 在真实异构物联网流量数据集上的实验表明，该框架在概念漂移下保持鲁棒的检测性能

Conclusion: 该框架具有在动态大规模物联网环境中实际部署的潜力，解决了传统方法的高计算开销和灾难性遗忘问题

Abstract: Although AI-based models have achieved high accuracy in IoT threat detection, their deployment in enterprise environments is constrained by reliance on stationary datasets that fail to reflect the dynamic nature of real-world IoT NetFlow traffic, which is frequently affected by concept drift. Existing solutions typically rely on periodic classifier retraining, resulting in high computational overhead and the risk of catastrophic forgetting. To address these challenges, this paper proposes a scalable framework for adaptive IoT threat detection that eliminates the need for continuous classifier retraining. The proposed approach trains a classifier once on latent-space representations of historical traffic, while an alignment model maps incoming traffic to the learned historical latent space prior to classification, thereby preserving knowledge of previously observed attacks. To capture inter-instance relationships among attack samples, the low-dimensional latent representations are further transformed into a graph-structured format and classified using a graph neural network. Experimental evaluations on real-world heterogeneous IoT traffic datasets demonstrate that the proposed framework maintains robust detection performance under concept drift. These results highlight the framework's potential for practical deployment in dynamic and large-scale IoT environments.

</details>


### [60] [The Quest for Winning Tickets in Low-Rank Adapters](https://arxiv.org/abs/2512.22495)
*Hamed Damirchi,Cristian Rodriguez-Opazo,Ehsan Abbasnejad,Zhen Zhang,Javen Shi*

Main category: cs.LG

TL;DR: 该论文发现彩票假说在LoRA参数高效微调中同样成立，并提出Partial-LoRA方法，通过稀疏低秩适配器减少87%可训练参数，同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着对大型预训练模型微调的依赖增加，研究彩票假说是否适用于参数高效微调方法（特别是LoRA），以探索更高效的适配策略。

Method: 提出Partial-LoRA方法，系统识别稀疏子网络，训练与任务相关子空间对齐的稀疏低秩适配器，重点研究各层稀疏度分配而非具体权重选择。

Result: 在8个视觉和12个语言任务（单任务和多任务）中，Partial-LoRA减少高达87%可训练参数，同时保持或提升准确率，验证了彩票假说在LoRA中的有效性。

Conclusion: 彩票假说在LoRA参数高效微调中成立，稀疏子网络性能更依赖各层稀疏度分配而非具体权重，Partial-LoRA为开发更高效适配策略开辟新途径。

Abstract: The Lottery Ticket Hypothesis (LTH) suggests that over-parameterized neural networks contain sparse subnetworks ("winning tickets") capable of matching full model performance when trained from scratch. With the growing reliance on fine-tuning large pretrained models, we investigate whether LTH extends to parameter-efficient fine-tuning (PEFT), specifically focusing on Low-Rank Adaptation (LoRA) methods. Our key finding is that LTH holds within LoRAs, revealing sparse subnetworks that can match the performance of dense adapters. In particular, we find that the effectiveness of sparse subnetworks depends more on how much sparsity is applied in each layer than on the exact weights included in the subnetwork. Building on this insight, we propose Partial-LoRA, a method that systematically identifies said subnetworks and trains sparse low-rank adapters aligned with task-relevant subspaces of the pre-trained model. Experiments across 8 vision and 12 language tasks in both single-task and multi-task settings show that Partial-LoRA reduces the number of trainable parameters by up to 87\%, while maintaining or improving accuracy. Our results not only deepen our theoretical understanding of transfer learning and the interplay between pretraining and fine-tuning but also open new avenues for developing more efficient adaptation strategies.

</details>


### [61] [Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals](https://arxiv.org/abs/2512.22508)
*Lucky Susanto,Anasta Pranawijayana,Cortino Sukotjo,Soni Prasad,Derry Wijaya*

Main category: cs.LG

TL;DR: 本研究探索利用元数据和幻觉信号预测LLM在医学考试中回答正确性的可行性，发现该方法可将准确率提升7.14%，但现有方法尚不足以用于高风险部署。


<details>
  <summary>Details</summary>
Motivation: LLM在医疗等高风险领域应用日益广泛，但其生成事实错误信息（幻觉）的风险是主要担忧。虽然已有大量研究检测和缓解幻觉，但预测LLM回答是否正确仍是一个关键但未充分探索的问题。

Method: 研究分析通用模型（GPT-4o）和推理中心模型（OSS-120B）在口腔修复学多选题考试上的表现。利用三种不同提示策略下的元数据和幻觉信号，为每个（模型，提示）组合构建正确性预测器。

Result: 元数据方法可将准确率提升高达+7.14%，在假设所有答案都正确的基础上达到83.12%的精确度。实际幻觉是错误答案的强指标，但仅凭元数据信号不能可靠预测幻觉。提示策略虽不影响总体准确率，但显著改变模型内部行为和元数据的预测效用。

Conclusion: 结果为开发LLM可靠性信号提供了有前景的方向，但本文探索的方法尚不够稳健，无法用于关键高风险部署。需要进一步研究来建立更可靠的预测机制。

Abstract: Large language models (LLMs) are increasingly adopted in high-stakes domains such as healthcare and medical education, where the risk of generating factually incorrect (i.e., hallucinated) information is a major concern. While significant efforts have been made to detect and mitigate such hallucinations, predicting whether an LLM's response is correct remains a critical yet underexplored problem. This study investigates the feasibility of predicting correctness by analyzing a general-purpose model (GPT-4o) and a reasoning-centric model (OSS-120B) on a multiple-choice prosthodontics exam. We utilize metadata and hallucination signals across three distinct prompting strategies to build a correctness predictor for each (model, prompting) pair. Our findings demonstrate that this metadata-based approach can improve accuracy by up to +7.14% and achieve a precision of 83.12% over a baseline that assumes all answers are correct. We further show that while actual hallucination is a strong indicator of incorrectness, metadata signals alone are not reliable predictors of hallucination. Finally, we reveal that prompting strategies, despite not affecting overall accuracy, significantly alter the models' internal behaviors and the predictive utility of their metadata. These results present a promising direction for developing reliability signals in LLMs but also highlight that the methods explored in this paper are not yet robust enough for critical, high-stakes deployment.

</details>


### [62] [Decomposing Task Vectors for Refined Model Editing](https://arxiv.org/abs/2512.22511)
*Hamed Damirchi,Ehsan Abbasnejad,Zhen Zhang,Javen Shi*

Main category: cs.LG

TL;DR: 提出一种任务向量分解方法，将每个任务向量分解为共享知识和独特信息两个组件，解决任务向量算术操作中的概念干扰问题，实现更精确的概念操控。


<details>
  <summary>Details</summary>
Motivation: 大型预训练模型在适应特定概念行为时面临挑战。任务向量（微调与预训练参数之差）可用于引导模型行为，但多个任务向量在算术操作时存在概念重叠干扰，导致不可预测的结果。

Method: 提出原则性分解方法：将每个任务向量分解为两个组件——一个捕获跨多个任务向量的共享知识，另一个隔离每个特定任务的独特信息。通过识别投影中的不变子空间来实现。

Result: 1) 图像分类中多任务合并性能提升5%（使用共享组件作为额外任务向量）；2) 扩散模型中实现干净风格混合而无生成退化（仅混合独特组件）；3) 语言模型中毒性降低47%同时保持通用知识任务性能（通过抵消独特组件中的毒性信息）。

Conclusion: 该方法为理解和控制任务向量算术提供了新框架，解决了模型编辑操作中的基本限制，实现了更精确的概念操控而无意外行为放大或减弱。

Abstract: Large pre-trained models have transformed machine learning, yet adapting these models effectively to exhibit precise, concept-specific behaviors remains a significant challenge. Task vectors, defined as the difference between fine-tuned and pre-trained model parameters, provide a mechanism for steering neural networks toward desired behaviors. This has given rise to large repositories dedicated to task vectors tailored for specific behaviors. The arithmetic operation of these task vectors allows for the seamless combination of desired behaviors without the need for large datasets. However, these vectors often contain overlapping concepts that can interfere with each other during arithmetic operations, leading to unpredictable outcomes. We propose a principled decomposition method that separates each task vector into two components: one capturing shared knowledge across multiple task vectors, and another isolating information unique to each specific task. By identifying invariant subspaces across projections, our approach enables more precise control over concept manipulation without unintended amplification or diminution of other behaviors. We demonstrate the effectiveness of our decomposition method across three domains: improving multi-task merging in image classification by 5% using shared components as additional task vectors, enabling clean style mixing in diffusion models without generation degradation by mixing only the unique components, and achieving 47% toxicity reduction in language models while preserving performance on general knowledge tasks by negating the toxic information isolated to the unique component. Our approach provides a new framework for understanding and controlling task vector arithmetic, addressing fundamental limitations in model editing operations.

</details>


### [63] [Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks](https://arxiv.org/abs/2512.22522)
*Jihang Wang,Dongcheng Zhao,Ruolin Chen,Qian Zhang,Yi Zeng*

Main category: cs.LG

TL;DR: 提出评估SNN对抗鲁棒性的新框架，包括自适应锐度替代梯度(ASSG)和稳定自适应投影梯度下降(SA-PGD)，显著提高攻击成功率，揭示当前SNN鲁棒性被高估


<details>
  <summary>Details</summary>
Motivation: SNN的脉冲激活具有二元不连续特性，导致梯度消失问题，使得基于梯度下降的对抗鲁棒性评估不可靠。现有替代梯度方法在强对抗攻击下的有效性不明确，需要更可靠的评估框架

Method: 1) 理论分析替代梯度中的梯度消失程度；2) 提出自适应锐度替代梯度(ASSG)，根据攻击迭代中的输入分布自适应调整替代函数形状；3) 设计L∞约束下的稳定自适应投影梯度下降(SA-PGD)，在梯度不精确时实现更快更稳定的收敛

Result: 实验表明该方法显著提高攻击成功率，适用于不同对抗训练方案、SNN架构和神经元模型，提供了更通用可靠的SNN对抗鲁棒性评估。结果揭示当前SNN的鲁棒性被显著高估

Conclusion: 提出了更可靠的SNN对抗鲁棒性评估框架，通过ASSG和SA-PGD提高了攻击有效性，证明现有SNN鲁棒性评估存在缺陷，需要更可靠的对抗训练方法

Abstract: Spiking Neural Networks (SNNs) utilize spike-based activations to mimic the brain's energy-efficient information processing. However, the binary and discontinuous nature of spike activations causes vanishing gradients, making adversarial robustness evaluation via gradient descent unreliable. While improved surrogate gradient methods have been proposed, their effectiveness under strong adversarial attacks remains unclear. We propose a more reliable framework for evaluating SNN adversarial robustness. We theoretically analyze the degree of gradient vanishing in surrogate gradients and introduce the Adaptive Sharpness Surrogate Gradient (ASSG), which adaptively evolves the shape of the surrogate function according to the input distribution during attack iterations, thereby enhancing gradient accuracy while mitigating gradient vanishing. In addition, we design an adversarial attack with adaptive step size under the $L_\infty$ constraint-Stable Adaptive Projected Gradient Descent (SA-PGD), achieving faster and more stable convergence under imprecise gradients. Extensive experiments show that our approach substantially increases attack success rates across diverse adversarial training schemes, SNN architectures and neuron models, providing a more generalized and reliable evaluation of SNN adversarial robustness. The experimental results further reveal that the robustness of current SNNs has been significantly overestimated and highlighting the need for more dependable adversarial training methods.

</details>


### [64] [TimePerceiver: An Encoder-Decoder Framework for Generalized Time-Series Forecasting](https://arxiv.org/abs/2512.22550)
*Jaebin Lee,Hankook Lee*

Main category: cs.LG

TL;DR: TimePerceiver：一个统一的编码器-解码器时间序列预测框架，通过灵活的编码-解码架构和有效训练策略，在多种预测任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列预测研究主要关注编码器设计，而将预测和解码视为次要问题。需要一种能够统一处理多种时间预测任务（如外推、插值、填补）的框架，并实现编码、解码和训练策略的紧密对齐。

Method: 1. 将预测任务泛化为包含外推、插值和填补等多种时间预测目标；2. 设计灵活的编码器-解码器架构，能够感知和适应输入和输出段在时间轴上的任意位置；3. 编码器使用潜在瓶颈表示来捕获时间和跨通道依赖；4. 解码器使用可学习查询对应目标时间戳来有效检索相关信息。

Result: 在广泛的基准数据集上，TimePerceiver框架一致且显著地优于先前的最先进基线方法。

Conclusion: TimePerceiver提供了一个统一的编码器-解码器预测框架，通过紧密对齐编码、解码和训练策略，在多种时间序列预测任务上取得了卓越性能，为时间序列建模提供了更全面的解决方案。

Abstract: In machine learning, effective modeling requires a holistic consideration of how to encode inputs, make predictions (i.e., decoding), and train the model. However, in time-series forecasting, prior work has predominantly focused on encoder design, often treating prediction and training as separate or secondary concerns. In this paper, we propose TimePerceiver, a unified encoder-decoder forecasting framework that is tightly aligned with an effective training strategy. To be specific, we first generalize the forecasting task to include diverse temporal prediction objectives such as extrapolation, interpolation, and imputation. Since this generalization requires handling input and target segments that are arbitrarily positioned along the temporal axis, we design a novel encoder-decoder architecture that can flexibly perceive and adapt to these varying positions. For encoding, we introduce a set of latent bottleneck representations that can interact with all input segments to jointly capture temporal and cross-channel dependencies. For decoding, we leverage learnable queries corresponding to target timestamps to effectively retrieve relevant information. Extensive experiments demonstrate that our framework consistently and significantly outperforms prior state-of-the-art baselines across a wide range of benchmark datasets. The code is available at https://github.com/efficient-learning-lab/TimePerceiver.

</details>


### [65] [On Admissible Rank-based Input Normalization Operators](https://arxiv.org/abs/2512.22587)
*Taeyun Kim*

Main category: cs.LG

TL;DR: 论文分析了基于排序的输入归一化方法，指出现有可微排序/排名算子存在稳定性缺陷，提出了三个公理来定义有效的排序归一化算子，并构建了满足这些公理的最小算子。


<details>
  <summary>Details</summary>
Motivation: 基于排序的归一化是现代机器学习中的重要方法，因其对尺度、单调变换和批次变化的鲁棒性而受到重视。然而，现有可微排序和排名算子存在稳定性问题，其结构条件从未被正式确定。

Method: 提出了三个公理来形式化排序归一化的最小不变性和稳定性要求，证明满足这些公理的算子必须分解为特征级排序表示和单调Lipschitz连续标量化映射，并构建了满足这些标准的最小算子。

Result: 证明了广泛使用的可微排序/排名算子本质上不满足稳定性标准，因为它们依赖值间隙和批次级成对交互，在严格单调变换、小批次组成变化甚至微小输入扰动下都不稳定。构建的最小算子满足提出的公理要求。

Conclusion: 研究结果清晰界定了有效排序归一化算子的设计空间，并将其与现有的基于连续松弛的排序方法正式区分开来，为构建稳定的排序归一化算子提供了理论基础。

Abstract: Rank-based input normalization is a workhorse of modern machine learning, prized for its robustness to scale, monotone transformations, and batch-to-batch variation. In many real systems, the ordering of feature values matters far more than their raw magnitudes - yet the structural conditions that a rank-based normalization operator must satisfy to remain stable under these invariances have never been formally pinned down.
  We show that widely used differentiable sorting and ranking operators fundamentally fail these criteria. Because they rely on value gaps and batch-level pairwise interactions, they are intrinsically unstable under strictly monotone transformations, shifts in mini-batch composition, and even tiny input perturbations. Crucially, these failures stem from the operators' structural design, not from incidental implementation choices.
  To address this, we propose three axioms that formalize the minimal invariance and stability properties required of rank-based input normalization. We prove that any operator satisfying these axioms must factor into (i) a feature-wise rank representation and (ii) a scalarization map that is both monotone and Lipschitz-continuous. We then construct a minimal operator that meets these criteria and empirically show that the resulting constraints are non-trivial in realistic setups. Together, our results sharply delineate the design space of valid rank-based normalization operators and formally separate them from existing continuous-relaxation-based sorting methods.

</details>


### [66] [Data-Driven Analysis of Crash Patterns in SAE Level 2 and Level 4 Automated Vehicles Using K-means Clustering and Association Rule Mining](https://arxiv.org/abs/2512.22589)
*Jewel Rana Palit,Vijayalakshmi K Kumarasamy,Osama A. Osman*

Main category: cs.LG

TL;DR: 该研究分析美国NHTSA超过2500起自动驾驶车辆事故记录，开发两阶段数据挖掘框架（K-means聚类+关联规则挖掘），揭示不同SAE级别（L2和L4）的事故动态模式，为AV开发和安全监管提供指导。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆虽有望提升交通安全，但实际事故数据显示其行为可能偏离预期安全结果，引发对混合交通环境中技术安全性和可靠性的担忧。现有研究多基于小规模加州数据集，且缺乏对不同自动化级别事故趋势的系统分析。

Method: 开发两阶段数据挖掘框架：1) 使用K-means聚类基于时空和环境因素将事故记录分为4个行为集群；2) 在每个集群内应用关联规则挖掘，提取事故模式与贡献因素（照明条件、路面状况、车辆动态、环境条件）之间的可解释多元关系。

Result: 分析覆盖SAE Level 2和Level 4的2500多起事故记录，识别出4个具有不同特征的事故行为集群，并揭示了各集群内事故模式与多种贡献因素之间的关联规则，提供了对自动驾驶事故动态的深入理解。

Conclusion: 研究结果为自动驾驶开发者、安全监管机构和政策制定者提供了可操作的指导，有助于制定AV部署策略和最小化事故风险，特别是在混合交通环境中提升自动驾驶的安全性和可靠性。

Abstract: Automated Vehicles (AV) hold potential to reduce or eliminate human driving errors, enhance traffic safety, and support sustainable mobility. Recently, crash data has increasingly revealed that AV behavior can deviate from expected safety outcomes, raising concerns about the technology's safety and operational reliability in mixed traffic environments. While past research has investigated AV crash, most studies rely on small-size California-centered datasets, with a limited focus on understanding crash trends across various SAE Levels of automation. This study analyzes over 2,500 AV crash records from the United States National Highway Traffic Safety Administration (NHTSA), covering SAE Levels 2 and 4, to uncover underlying crash dynamics. A two-stage data mining framework is developed. K-means clustering is first applied to segment crash records into 4 distinct behavioral clusters based on temporal, spatial, and environmental factors. Then, Association Rule Mining (ARM) is used to extract interpretable multivariate relationships between crash patterns and crash contributors including lighting conditions, surface condition, vehicle dynamics, and environmental conditions within each cluster. These insights provide actionable guidance for AV developers, safety regulators, and policymakers in formulating AV deployment strategies and minimizing crash risks.

</details>


### [67] [Energy-Guided Flow Matching Enables Few-Step Conformer Generation and Ground-State Identification](https://arxiv.org/abs/2512.22597)
*Guikun Xu,Xiaohan Yi,Peilin Zhao,Yatao Bian*

Main category: cs.LG

TL;DR: EnFlow：统一框架，通过能量引导采样将流匹配与显式学习能量模型结合，提升构象生成和基态预测性能


<details>
  <summary>Details</summary>
Motivation: 现有基于物理的构象生成方法计算成本高，而基于学习的方法存在碎片化问题：生成模型能捕捉多样性但缺乏可靠能量校准，确定性预测器只能预测单一结构而无法表示整体变异性

Method: EnFlow将流匹配与显式学习的能量模型通过能量引导采样方案结合，在非高斯流匹配路径上定义能量梯度引导，使采样轨迹向低能量区域移动

Result: 在GEOM-QM9和GEOM-Drugs数据集上的实验表明，EnFlow在1-2步ODE采样下同时改善了生成指标，并减少了基态预测误差，优于现有方法

Conclusion: EnFlow提供了一个统一框架，通过能量引导采样将生成多样性与能量校准相结合，显著提高了构象生成保真度和基态识别准确性

Abstract: Generating low-energy conformer ensembles and identifying ground-state conformations from molecular graphs remain computationally demanding with physics-based pipelines. Current learning-based approaches often suffer from a fragmented paradigm: generative models capture diversity but lack reliable energy calibration, whereas deterministic predictors target a single structure and fail to represent ensemble variability. Here we present EnFlow, a unified framework that couples flow matching (FM) with an explicitly learned energy model through an energy-guided sampling scheme defined along a non-Gaussian FM path. By incorporating energy-gradient guidance during sampling, our method steers trajectories toward lower-energy regions, substantially improving conformational fidelity, particularly in the few-step regime. The learned energy function further enables efficient energy-based ranking of generated ensembles for accurate ground-state identification. Extensive experiments on GEOM-QM9 and GEOM-Drugs demonstrate that EnFlow simultaneously improves generation metrics with 1--2 ODE-steps and reduces ground-state prediction errors compared with state-of-the-art methods.

</details>


### [68] [Cryptocurrency Price Prediction Using Parallel Gated Recurrent Units](https://arxiv.org/abs/2512.22599)
*Milad Asadpour,Alireza Rezaee,Farshid Hajati*

Main category: cs.LG

TL;DR: 提出名为PGRU的并行门控循环单元模型，用于加密货币价格预测，通过并行独立的RNN处理不同价格特征，最终结合输出进行预测，在减少输入数据和计算成本的同时获得更高精度。


<details>
  <summary>Details</summary>
Motivation: 随着加密货币和比特币的发展，在线加密货币投资和交易日益增多。比特币使用区块链技术确保交易安全、透明、可追溯和不可篡改，但其价格波动剧烈，吸引了金融领域的广泛关注。投资者需要预测加密货币价格的方法，但现有方法存在挑战。

Method: 提出并行门控循环单元（PGRU）模型，使用循环神经网络以并行独立的方式预测价格。并行网络利用不同的输入，每个输入代表不同的价格相关特征。最后，通过神经网络结合并行网络的输出，预测加密货币的未来价格。

Result: 实验结果表明，所提模型在窗口长度20和15时，分别达到3.243%和2.641%的平均绝对百分比误差（MAPE）。该方法相比现有方法，用更少的输入数据和更低的计算成本实现了更高的准确性和效率。

Conclusion: PGRU模型为加密货币价格预测提供了一种有效的新方法，通过并行处理不同特征和降低计算需求，在预测精度和效率方面优于现有方法，对加密货币投资者具有实际应用价值。

Abstract: According to the advent of cryptocurrencies and Bitcoin, many investments and businesses are now conducted online through cryptocurrencies. Among them, Bitcoin uses blockchain technology to make transactions secure, transparent, traceable, and immutable. It also exhibits significant price fluctuations and performance, which has attracted substantial attention, especially in financial sectors. Consequently, a wide range of investors and individuals have turned to investing in the cryptocurrency market. One of the most important challenges in economics is price forecasting for future trades. Cryptocurrencies are no exception, and investors are looking for methods to predict prices; various theories and methods have been proposed in this field. This paper presents a new deep model, called \emph{Parallel Gated Recurrent Units} (PGRU), for cryptocurrency price prediction. In this model, recurrent neural networks forecast prices in a parallel and independent way. The parallel networks utilize different inputs, each representing distinct price-related features. Finally, the outputs of the parallel networks are combined by a neural network to forecast the future price of cryptocurrencies. The experimental results indicate that the proposed model achieves mean absolute percentage errors (MAPE) of 3.243% and 2.641% for window lengths 20 and 15, respectively. Our method therefore attains higher accuracy and efficiency with fewer input data and lower computational cost compared to existing methods.

</details>


### [69] [Gold Price Prediction Using Long Short-Term Memory and Multi-Layer Perceptron with Gray Wolf Optimizer](https://arxiv.org/abs/2512.22606)
*Hesam Taghipour,Alireza Rezaee,Farshid Hajati*

Main category: cs.LG

TL;DR: 提出一种基于LSTM和MLP的混合AI模型，结合灰狼优化算法，用于预测黄金市场的每日和每月价格，并开发了相应的交易策略。


<details>
  <summary>Details</summary>
Motivation: 黄金市场对各类投资者至关重要，但由于经济和政治因素的复杂性，准确预测金融市场一直具有挑战性。需要开发能够准确预测市场走势的模型，为开发者带来巨大利益。

Method: 使用两个LSTM网络分别进行每日和每月预测，将结果输入MLP网络进行整合。采用灰狼优化算法优化各网络内部神经元数量。数据集涵盖2010-2021年的宏观经济、能源市场、股票和发达国家货币状况数据。

Result: 模型预测每日收盘价的MAE为0.21美元，下月价格的MAE为22.23美元。基于预测开发的交易策略在三个月内实现了171%的回报率。

Conclusion: 提出的LSTM-MLP混合模型能够有效预测黄金价格，结合优化算法和综合数据集，为市场参与者提供了有价值的预测工具和交易策略。

Abstract: The global gold market, by its fundamentals, has long been home to many financial institutions, banks, governments, funds, and micro-investors. Due to the inherent complexity and relationship between important economic and political components, accurate forecasting of financial markets has always been challenging. Therefore, providing a model that can accurately predict the future of the markets is very important and will be of great benefit to their developers. In this paper, an artificial intelligence-based algorithm for daily and monthly gold forecasting is presented. Two Long short-term memory (LSTM) networks are responsible for daily and monthly forecasting, the results of which are integrated into a Multilayer perceptrons (MLP) network and provide the final forecast of the next day prices. The algorithm forecasts the highest, lowest, and closing prices on the daily and monthly time frame. Based on these forecasts, a trading strategy for live market trading was developed, according to which the proposed model had a return of 171% in three months. Also, the number of internal neurons in each network is optimized by the Gray Wolf optimization (GWO) algorithm based on the least RMSE error. The dataset was collected between 2010 and 2021 and includes data on macroeconomic, energy markets, stocks, and currency status of developed countries. Our proposed LSTM-MLP model predicted the daily closing price of gold with the Mean absolute error (MAE) of $ 0.21 and the next month's price with $ 22.23.

</details>


### [70] [Communication Compression for Distributed Learning with Aggregate and Server-Guided Feedback](https://arxiv.org/abs/2512.22623)
*Tomas Ortega,Chun-Yin Huang,Xiaoxiao Li,Hamid Jafarkhani*

Main category: cs.LG

TL;DR: 提出两种无需客户端状态或控制变量的有偏压缩框架CAFe和CAFe-S，解决联邦学习中通信瓶颈和隐私问题，在非凸设置下证明收敛性优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临通信成本瓶颈，特别是客户端到服务器的上行传输受限。现有有偏压缩技术需要误差反馈机制，但标准误差反馈依赖客户端特定控制变量，这违反用户隐私且与大规模FL中常见的无状态客户端不兼容。

Method: 提出两种框架：1) CAFe使用前一轮全局聚合更新作为所有客户端共享的控制变量；2) CAFe-S扩展此思想，当服务器拥有小型私有数据集时，生成服务器引导的候选更新作为更准确的预测器。以分布式梯度下降为代表性算法进行分析。

Result: 在非凸设置和有界梯度差异下，分析证明CAFe优于分布式压缩梯度下降(DCGD)。进一步证明CAFe-S收敛到平稳点，且收敛速率随服务器数据更具代表性而改善。FL场景中的实验结果验证了方法优于现有压缩方案。

Conclusion: CAFe和CAFe-S框架成功解决了联邦学习中有偏压缩的通信瓶颈和隐私问题，无需客户端状态，在理论和实验上都表现出优越性能，特别是CAFe-S在服务器有代表性数据时能提供更好的收敛性。

Abstract: Distributed learning, particularly Federated Learning (FL), faces a significant bottleneck in the communication cost, particularly the uplink transmission of client-to-server updates, which is often constrained by asymmetric bandwidth limits at the edge. Biased compression techniques are effective in practice, but require error feedback mechanisms to provide theoretical guarantees and to ensure convergence when compression is aggressive. Standard error feedback, however, relies on client-specific control variates, which violates user privacy and is incompatible with stateless clients common in large-scale FL. This paper proposes two novel frameworks that enable biased compression without client-side state or control variates. The first, Compressed Aggregate Feedback (CAFe), uses the globally aggregated update from the previous round as a shared control variate for all clients. The second, Server-Guided Compressed Aggregate Feedback (CAFe-S), extends this idea to scenarios where the server possesses a small private dataset; it generates a server-guided candidate update to be used as a more accurate predictor. We consider Distributed Gradient Descent (DGD) as a representative algorithm and analytically prove CAFe's superiority to Distributed Compressed Gradient Descent (DCGD) with biased compression in the non-convex regime with bounded gradient dissimilarity. We further prove that CAFe-S converges to a stationary point, with a rate that improves as the server's data become more representative. Experimental results in FL scenarios validate the superiority of our approaches over existing compression schemes.

</details>


### [71] [Scaling Unverifiable Rewards: A Case Study on Visual Insights](https://arxiv.org/abs/2512.22650)
*Shuyu Gan,James Mooney,Pan Hao,Renxiang Wang,Mingyi Hong,Qianwen Wang,Dongyeop Kang*

Main category: cs.LG

TL;DR: 提出Selective TTS框架，在多阶段多智能体管道中分布计算资源，通过过程特定评判器早期剪枝低质量分支，解决传统基于评判器迭代精炼在无验证奖励任务中的误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界多阶段管道任务（如数据科学分析）的最终结果缺乏可验证的奖励信号或足够数据训练鲁棒的奖励模型，导致基于评判器的迭代精炼容易在阶段间累积误差。

Method: 提出Selective TTS框架：1）在多智能体管道各阶段分布计算资源，而非传统的时间迭代精炼；2）使用过程特定评判器早期剪枝低质量分支；3）基于数据科学管道构建端到端多智能体系统生成可视化图表和报告；4）设计可靠的LLM评判器模型与人类专家对齐。

Result: 在固定计算预算下，Selective TTS将洞察质量平均分从61.64提升至65.86，同时降低方差。LLM评判器与人类专家对齐良好（Kendall's τ=0.55）。

Conclusion: Selective TTS为解决无验证奖励的复杂开放任务（如科学发现和故事生成）中的推理扩展提供了第一步，通过过程特定评判和早期剪枝稳定精炼过程。

Abstract: Large Language Model (LLM) agents can increasingly automate complex reasoning through Test-Time Scaling (TTS), iterative refinement guided by reward signals. However, many real-world tasks involve multi-stage pipeline whose final outcomes lack verifiable rewards or sufficient data to train robust reward models, making judge-based refinement prone to accumulate error over stages. We propose Selective TTS, a process-based refinement framework that scales inference across different stages in multi-agent pipeline, instead of repeated refinement over time by prior work. By distributing compute across stages and pruning low-quality branches early using process-specific judges, Selective TTS mitigates the judge drift and stabilizes refinement. Grounded in the data science pipeline, we build an end-to-end multi-agent pipeline for generating visually insightful charts and report of given dataset, and design a reliable LLM-based judge model, aligned with human experts (Kendall's τ=0.55). Our proposed selective TTS then improves insight quality under a fixed compute budget, increasing mean scores from 61.64 to 65.86 while reducing variance. We hope our findings serve as the first step toward to scaling complex, open-ended tasks with unverifiable rewards, such as scientific discovery and story generation.

</details>


### [72] [Quantum Generative Models for Computational Fluid Dynamics: A First Exploration of Latent Space Learning in Lattice Boltzmann Simulations](https://arxiv.org/abs/2512.22672)
*Achraf Hsain,Fouad Mohammed Abbou*

Main category: cs.LG

TL;DR: 量子生成模型首次应用于计算流体动力学数据的潜在空间表示，量子模型在压缩物理模拟数据生成方面优于经典LSTM基线。


<details>
  <summary>Details</summary>
Motivation: 探索量子生成模型在计算流体动力学数据压缩潜在空间表示中的应用，填补量子模型与物理模拟数据生成结合的研究空白。

Method: 使用GPU加速的Lattice Boltzmann方法生成流体涡度场，通过VQ-VAE压缩到7维离散潜在空间，比较量子电路玻恩机和量子生成对抗网络与经典LSTM基线的生成性能。

Result: 在实验条件下，两种量子模型生成的样本与真实分布的平均最小距离均低于LSTM，其中量子电路玻恩机取得了最优的评估指标。

Conclusion: 该研究建立了CFD模拟与量子机器学习之间的完整开源流程，首次对压缩物理模拟表示的量子生成模型进行实证研究，为这一交叉领域的未来研究奠定了基础。

Abstract: This paper presents the first application of quantum generative models to learned latent space representations of computational fluid dynamics (CFD) data. While recent work has explored quantum models for learning statistical properties of fluid systems, the combination of discrete latent space compression with quantum generative sampling for CFD remains unexplored. We develop a GPU-accelerated Lattice Boltzmann Method (LBM) simulator to generate fluid vorticity fields, which are compressed into a discrete 7-dimensional latent space using a Vector Quantized Variational Autoencoder (VQ-VAE). The central contribution is a comparative analysis of quantum and classical generative approaches for modeling this physics-derived latent distribution: we evaluate a Quantum Circuit Born Machine (QCBM) and Quantum Generative Adversarial Network (QGAN) against a classical Long Short-Term Memory (LSTM) baseline. Under our experimental conditions, both quantum models produced samples with lower average minimum distances to the true distribution compared to the LSTM, with the QCBM achieving the most favorable metrics. This work provides: (1)~a complete open-source pipeline bridging CFD simulation and quantum machine learning, (2)~the first empirical study of quantum generative modeling on compressed latent representations of physics simulations, and (3)~a foundation for future rigorous investigation at this intersection.

</details>


### [73] [Beyond Centralization: Provable Communication Efficient Decentralized Multi-Task Learning](https://arxiv.org/abs/2512.22675)
*Donghwa Kang,Shana Moothedath*

Main category: cs.LG

TL;DR: 该论文提出了一种去中心化的多任务表示学习方法，通过低秩结构学习共享特征，通信复杂度与目标精度无关，显著降低了通信成本。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺环境中，表示学习是提取相关任务共同特征的常用框架。虽然集中式方法已被广泛研究，但去中心化方法仍未被充分探索。本文研究去中心化多任务表示学习，其中特征共享低秩结构。

Method: 提出一种新的交替投影梯度和最小化算法，具有可证明的精度保证。考虑了多个任务，每个任务有有限数量的数据样本，观测遵循具有任务特定参数的线性模型。在去中心化设置中，任务数据分布在多个节点上，节点间的信息交换受通信网络约束。

Result: 全面刻画了时间、通信和样本复杂度。重要的是，通信复杂度与目标精度无关，这显著降低了与先前方法相比的通信成本。数值模拟验证了不同维度和网络拓扑下的理论分析，并展示了去中心化学习优于集中式联邦方法的场景。

Conclusion: 该研究为去中心化多任务表示学习提供了一种有效的算法框架，通过低秩结构学习共享特征，在通信效率方面具有显著优势，在某些场景下优于集中式联邦方法。

Abstract: Representation learning is a widely adopted framework for learning in data-scarce environments, aiming to extract common features from related tasks. While centralized approaches have been extensively studied, decentralized methods remain largely underexplored. We study decentralized multi-task representation learning in which the features share a low-rank structure. We consider multiple tasks, each with a finite number of data samples, where the observations follow a linear model with task-specific parameters. In the decentralized setting, task data are distributed across multiple nodes, and information exchange between nodes is constrained by a communication network. The goal is to recover the underlying feature matrix whose rank is much smaller than both the parameter dimension and the number of tasks. We propose a new alternating projected gradient and minimization algorithm with provable accuracy guarantees. We provide comprehensive characterizations of the time, communication, and sample complexities. Importantly, the communication complexity is independent of the target accuracy, which significantly reduces communication cost compared to prior methods. Numerical simulations validate the theoretical analysis across different dimensions and network topologies, and demonstrate regimes in which decentralized learning outperforms centralized federated approaches.

</details>


### [74] [Learning with the $p$-adics](https://arxiv.org/abs/2512.22692)
*André F. T. Martins*

Main category: cs.LG

TL;DR: 论文探索用p-adic数替代实数作为机器学习的新数学框架，研究其在分类、回归和表示学习中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习框架基于实数域，但作者质疑这是否是唯一选择。p-adic数的超度量结构和层次特性可能更适合代码理论和层次表示学习，特别是能表示实数域无法构建的Quillian语义网络。

Method: 提出使用p-adic数（ℚₚ）作为替代实数的新数学框架，建立分类、回归和表示学习的基本构建模块，开发相应的学习模型和算法，并展示如何将Quillian语义网络表示为紧凑的p-adic线性网络。

Result: 理论探索表明p-adic数框架具有可行性，能够构建实数域无法实现的网络结构（如Quillian语义网络的紧凑表示），为机器学习提供了新的数学基础。

Conclusion: p-adic数作为实数域的替代框架具有潜力，特别是在层次表示学习方面。论文为这一新研究方向奠定了基础，并指出了未来研究的开放问题和机会。

Abstract: Existing machine learning frameworks operate over the field of real numbers ($\mathbb{R}$) and learn representations in real (Euclidean or Hilbert) vector spaces (e.g., $\mathbb{R}^d$). Their underlying geometric properties align well with intuitive concepts such as linear separability, minimum enclosing balls, and subspace projection; and basic calculus provides a toolbox for learning through gradient-based optimization.
  But is this the only possible choice? In this paper, we study the suitability of a radically different field as an alternative to $\mathbb{R}$ -- the ultrametric and non-archimedean space of $p$-adic numbers, $\mathbb{Q}_p$. The hierarchical structure of the $p$-adics and their interpretation as infinite strings make them an appealing tool for code theory and hierarchical representation learning. Our exploratory theoretical work establishes the building blocks for classification, regression, and representation learning with the $p$-adics, providing learning models and algorithms. We illustrate how simple Quillian semantic networks can be represented as a compact $p$-adic linear network, a construction which is not possible with the field of reals. We finish by discussing open problems and opportunities for future research enabled by this new framework.

</details>


### [75] [Predictive Modeling of Power Outages during Extreme Events: Integrating Weather and Socio-Economic Factors](https://arxiv.org/abs/2512.22699)
*Antar Kumar Biswas,Masoud H. Nazari*

Main category: cs.LG

TL;DR: 提出基于学习的极端事件停电预测框架，整合多源数据，评估四种机器学习模型，LSTM表现最佳，发现经济条件和基础设施发展与停电发生率负相关


<details>
  <summary>Details</summary>
Motivation: 针对低概率高后果的停电场景，需要准确预测极端事件导致的停电，传统方法可能无法充分捕捉复杂模式和社区脆弱性因素

Method: 整合EAGLE-I停电记录（2014-2024）与天气、社会经济、基础设施和季节性事件数据，评估随机森林、支持向量机、自适应提升和长短期记忆四种机器学习模型

Result: LSTM网络在所有测试模型中取得最低预测误差，同时发现更强的经济条件和更发达的基础设施与更低的停电发生率相关

Conclusion: 提出的学习框架能有效预测极端事件导致的停电，LSTM模型表现最佳，社会经济和基础设施因素对停电风险有显著影响，为电力系统韧性规划提供重要见解

Abstract: This paper presents a novel learning-based framework for predicting power outages caused by extreme events. The proposed approach specifically targets low-probability, high-consequence outage scenarios and leverages a comprehensive set of features derived from publicly available data sources. We integrate EAGLE-I outage records (2014-2024) with weather, socio-economic, infrastructure, and seasonal event data. Incorporating social and demographic indicators reveals underlying patterns of community vulnerability and provides a clearer understanding of outage risk during extreme conditions. Four machine learning models (Random Forest (RF), Support Vector Machine (SVM), Adaptive Boosting (AdaBoost), and Long Short-Term Memory (LSTM)) are evaluated. Experimental validation is performed on a large-scale dataset covering counties in the lower peninsula of Michigan. Among all models tested, the LSTM network achieves the lowest prediction error. Additionally, the results demonstrate that stronger economic conditions and more developed infrastructure are associated with lower outage occurrence.

</details>


### [76] [What Matters in Deep Learning for Time Series Forecasting?](https://arxiv.org/abs/2512.22702)
*Valentina Moretti,Andrea Cini,Ivan Marisca,Cesare Alippi*

Main category: cs.LG

TL;DR: 该论文分析了时间序列预测深度学习架构的设计空间，指出简单的、基于预测原则设计的架构往往能与最先进方法匹敌，而当前基准测试存在缺陷，需要重新思考架构设计的基础方面。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在时间序列应用中越来越受欢迎，但大量新提出的架构和相互矛盾的实证结果使得难以评估哪些组件对最终性能有显著贡献。作者旨在理解当前时间序列预测深度学习架构的设计空间，解释观察到的意外结果。

Method: 通过讨论设计维度和权衡来解释观察到的结果，评估局部性和全局性等概念如何应用于最近的预测架构，分析被忽视的实现细节如何改变预测方法的类别并影响实证结果。

Result: 研究表明，考虑局部性和全局性等预测原则比采用特定序列建模层更重要，简单但设计良好的预测架构通常能与最先进方法匹敌。实现细节会从根本上改变预测方法的类别并显著影响实证结果。

Conclusion: 需要重新思考当前有缺陷的基准测试实践，在设计架构时应关注预测问题的基础方面。作者提出了辅助预测模型卡，其字段用于基于关键设计选择来表征现有和新预测架构。

Abstract: Deep learning models have grown increasingly popular in time series applications. However, the large quantity of newly proposed architectures, together with often contradictory empirical results, makes it difficult to assess which components contribute significantly to final performance. We aim to make sense of the current design space of deep learning architectures for time series forecasting by discussing the design dimensions and trade-offs that can explain, often unexpected, observed results. This paper discusses the necessity of grounding model design on principles for forecasting groups of time series and how such principles can be applied to current models. In particular, we assess how concepts such as locality and globality apply to recent forecasting architectures. We show that accounting for these aspects can be more relevant for achieving accurate results than adopting specific sequence modeling layers and that simple, well-designed forecasting architectures can often match the state of the art. We discuss how overlooked implementation details in existing architectures (1) fundamentally change the class of the resulting forecasting method and (2) drastically affect the observed empirical results. Our results call for rethinking current faulty benchmarking practices and the need to focus on the foundational aspects of the forecasting problem when designing architectures. As a step in this direction, we propose an auxiliary forecasting model card, whose fields serve to characterize existing and new forecasting architectures based on key design choices.

</details>


### [77] [FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents](https://arxiv.org/abs/2512.22733)
*Jiaqi Shao,Yufeng Miao,Wei Zhang,Bing Luo*

Main category: cs.LG

TL;DR: FoldAct框架解决了长时程RL中上下文折叠方法面临的三个核心挑战：梯度稀释、自条件和计算成本，通过分离损失计算、全上下文一致性损失和选择性片段训练实现稳定训练。


<details>
  <summary>Details</summary>
Motivation: 现有上下文折叠方法将摘要动作视为标准动作，忽略了摘要会从根本上改变智能体未来的观测空间，创建了策略依赖的非平稳观测分布，这违反了RL的核心假设，导致梯度稀释、自条件和计算成本三大挑战。

Method: FoldAct框架包含三个关键创新：1) 分离损失计算，为摘要和动作token提供独立的梯度信号；2) 全上下文一致性损失，减少分布偏移；3) 选择性片段训练，降低计算成本。

Result: 该方法实现了长时程搜索智能体在上下文折叠下的稳定训练，解决了非平稳观测问题，同时将训练效率提升了5.19倍。

Conclusion: FoldAct通过明确处理上下文折叠引入的非平稳观测问题，为长时程RL提供了可扩展的解决方案，解决了现有方法忽视的摘要动作与标准动作的根本区别。

Abstract: Long-horizon reinforcement learning (RL) for large language models faces critical scalability challenges from unbounded context growth, leading to context folding methods that compress interaction history during task execution. However, existing approaches treat summary actions as standard actions, overlooking that summaries fundamentally modify the agent's future observation space, creating a policy-dependent, non-stationary observation distribution that violates core RL assumptions. This introduces three fundamental challenges: (1) gradient dilution where summary tokens receive insufficient training signal, (2) self-conditioning where policy updates change summary distributions, creating a vicious cycle of training collapse, and (3) computational cost from processing unique contexts at each turn. We introduce \textbf{FoldAct}\footnote{https://github.com/SHAO-Jiaqi757/FoldAct}, a framework that explicitly addresses these challenges through three key innovations: separated loss computation for independent gradient signals on summary and action tokens, full context consistency loss to reduce distribution shift, and selective segment training to reduce computational cost. Our method enables stable training of long-horizon search agents with context folding, addressing the non-stationary observation problem while improving training efficiency with 5.19$\times$ speedup.

</details>


### [78] [When Does Multi-Task Learning Fail? Quantifying Data Imbalance and Task Independence in Metal Alloy Property Prediction](https://arxiv.org/abs/2512.22740)
*Sungwoo Kang*

Main category: cs.LG

TL;DR: 多任务学习在合金性能预测中表现出矛盾结果：回归任务性能显著下降，但分类任务性能提升


<details>
  <summary>Details</summary>
Motivation: 测试多任务学习是否能够利用合金材料性能之间的潜在物理关系来提升预测性能，特别是同时预测电阻率、维氏硬度和非晶形成能力

Method: 使用54,028个合金样本，比较单任务模型与标准和结构化多任务学习模型，分析回归和分类任务的性能差异

Result: 多任务学习显著降低回归性能（电阻率R²从0.897降至0.844，硬度R²从0.832降至0.694），但提升分类性能（非晶形成能力F1从0.703提升至0.744，召回率提升17%）

Conclusion: 建议对精确回归任务使用独立模型，而对分类任务（特别是需要高召回率时）保留多任务学习，因为数据严重不平衡导致负迁移

Abstract: Multi-task learning (MTL) assumes related material properties share underlying physics that can be leveraged for better predictions. We test this by simultaneously predicting electrical resistivity, Vickers hardness, and amorphous-forming ability using 54,028 alloy samples. We compare single-task models against standard and structured MTL. Results reveal a striking dichotomy: MTL significantly degrades regression performance (resistivity $R^2$: 0.897 $\to$ 0.844; hardness $R^2$: 0.832 $\to$ 0.694, $p < 0.01$) but improves classification (amorphous F1: 0.703 $\to$ 0.744, $p < 0.05$; recall +17%). Analysis shows near-zero inter-task weights, indicating property independence. Regression failure is attributed to negative transfer caused by severe data imbalance (52k vs. 800 samples). We recommend independent models for precise regression, while reserving MTL for classification tasks where recall is critical.

</details>


### [79] [Bridging Global Intent with Local Details: A Hierarchical Representation Approach for Semantic Validation in Text-to-SQL](https://arxiv.org/abs/2512.22744)
*Rihong Qiu,Zhibang Yang,Xinke Jiang,Weibin Liao,Xin Gao,Xu Chu,Junfeng Zhao,Yasha Wang*

Main category: cs.LG

TL;DR: HEROSQL：一种用于文本到SQL语义验证的分层SQL表示方法，通过逻辑计划和抽象语法树整合全局意图和局部细节，使用NMPNN进行信息传播，并采用AST驱动的子SQL增强策略生成高质量负样本，显著提升语义不一致检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL验证方法主要关注语法正确性，缺乏有效的语义验证（检测问题与SQL之间的不对齐）。语义验证面临两个关键挑战：1）同时捕捉全局用户意图和SQL结构细节；2）构建高质量细粒度子SQL标注。

Method: 提出HEROSQL分层SQL表示方法：1）通过逻辑计划（LP）捕获全局意图，通过抽象语法树（AST）捕获局部细节；2）使用嵌套消息传递神经网络（NMPNN）捕捉SQL内在关系信息并聚合模式引导的语义；3）提出AST驱动的子SQL增强策略生成高质量负样本，优化细粒度语义不一致检测。

Result: 在文本到SQL验证基准测试（域内和域外设置）中，HEROSQL显著优于现有最先进方法，AUPRC平均提升9.40%，AUROC平均提升12.35%。能有效检测细粒度语义错误，为大型语言模型提供更细粒度反馈，增强数据查询平台的可靠性和可解释性。

Conclusion: HEROSQL通过分层SQL表示和NMPNN有效解决了文本到SQL语义验证的关键挑战，显著提升了语义不一致检测性能，为数据查询平台提供了更可靠和可解释的验证机制。

Abstract: Text-to-SQL translates natural language questions into SQL statements grounded in a target database schema. Ensuring the reliability and executability of such systems requires validating generated SQL, but most existing approaches focus only on syntactic correctness, with few addressing semantic validation (detecting misalignments between questions and SQL). As a consequence, effective semantic validation still faces two key challenges: capturing both global user intent and SQL structural details, and constructing high-quality fine-grained sub-SQL annotations. To tackle these, we introduce HEROSQL, a hierarchical SQL representation approach that integrates global intent (via Logical Plans, LPs) and local details (via Abstract Syntax Trees, ASTs). To enable better information propagation, we employ a Nested Message Passing Neural Network (NMPNN) to capture inherent relational information in SQL and aggregate schema-guided semantics across LPs and ASTs. Additionally, to generate high-quality negative samples, we propose an AST-driven sub-SQL augmentation strategy, supporting robust optimization of fine-grained semantic inconsistencies. Extensive experiments conducted on Text-to-SQL validation benchmarks (both in-domain and out-of-domain settings) demonstrate that our approach outperforms existing state-of-the-art methods, achieving an average 9.40% improvement of AUPRC and 12.35% of AUROC in identifying semantic inconsistencies. It excels at detecting fine-grained semantic errors, provides large language models with more granular feedback, and ultimately enhances the reliability and interpretability of data querying platforms.

</details>


### [80] [From Confounding to Learning: Dynamic Service Fee Pricing on Third-Party Platforms](https://arxiv.org/abs/2512.22749)
*Rui Ai,David Simchi-Levi,Feng Zhu*

Main category: cs.LG

TL;DR: 第三方平台面对战略代理时的定价行为研究，提出需求学习算法，在混杂因素下实现最优遗憾界，并应用于Zomato和Lyft数据


<details>
  <summary>Details</summary>
Motivation: 研究第三方平台在面临战略代理时的定价行为，平台作为收入最大化者，只能观察到均衡价格和数量，这构成了一个混杂因素下的通用需求学习问题

Method: 开发了具有最优遗憾界的算法，利用非独立同分布行动作为工具变量学习需求，提出新颖的同胚构造方法，无需假设星形结构，为深度神经网络学习需求提供首个效率保证

Result: 算法达到最优遗憾界$\Tilde{\cO}(\sqrt{T}\wedgeσ_S^{-2})$，揭示供应侧噪声对需求可学习性的根本影响，导致遗憾的相变，并通过模拟和Zomato、Lyft真实数据验证实用性

Conclusion: 供应侧噪声显著影响需求学习，非独立同分布行动可作为有效工具变量，提出的同胚构造方法为深度神经网络学习需求提供理论保证，方法在实际应用中表现良好

Abstract: We study the pricing behavior of third-party platforms facing strategic agents. Assuming the platform is a revenue maximizer, it observes market features that generally affect demand. Since only the equilibrium price and quantity are observable, this presents a general demand learning problem under confounding. Mathematically, we develop an algorithm with optimal regret of $\Tilde{\cO}(\sqrt{T}\wedgeσ_S^{-2})$. Our results reveal that supply-side noise fundamentally affects the learnability of demand, leading to a phase transition in regret. Technically, we show that non-i.i.d. actions can serve as instrumental variables for learning demand. We also propose a novel homeomorphic construction that allows us to establish estimation bounds without assuming star-shapedness, providing the first efficiency guarantee for learning demand with deep neural networks. Finally, we demonstrate the practical applicability of our approach through simulations and real-world data from Zomato and Lyft.

</details>


### [81] [A Micro-Macro Machine Learning Framework for Predicting Childhood Obesity Risk Using NHANES and Environmental Determinants](https://arxiv.org/abs/2512.22758)
*Eswarasanthosh Kumar Mamillapalli,Nishtha Sharma*

Main category: cs.LG

TL;DR: 该研究开发了一个微-宏观机器学习框架，整合个体层面数据（NHANES）和宏观环境特征（USDA、EPA），用于预测儿童肥胖风险，并构建环境脆弱性指数，发现环境负担与肥胖风险分布存在地理相似性。


<details>
  <summary>Details</summary>
Motivation: 传统流行病学研究通常独立分析个体、家庭和环境层面的风险因素，限制了理解结构性环境条件如何与个体特征相互作用影响健康结果。需要整合多尺度数据集来识别环境驱动的肥胖风险差异。

Method: 1) 整合NHANES的个体层面人体测量和社会经济数据；2) 从USDA和EPA数据集中提取宏观层面结构环境特征（食品获取、空气质量、社会经济脆弱性）；3) 使用逻辑回归、随机森林、XGBoost和LightGBM四种机器学习模型预测肥胖；4) 构建环境脆弱性指数(EnvScore)；5) 进行多层次比较分析。

Result: XGBoost模型表现最佳；构建的环境脆弱性指数显示，环境负担高的州与全国预测的微观层面肥胖风险分布存在强烈的地理相似性；证明了整合多尺度数据集识别环境驱动肥胖风险差异的可行性。

Conclusion: 该研究提供了一个可扩展、数据驱动的多层次建模管道，适用于公共卫生信息学，在因果建模、干预规划和实时分析方面具有强大扩展潜力。

Abstract: Childhood obesity remains a major public health challenge in the United States, strongly influenced by a combination of individual-level, household-level, and environmental-level risk factors. Traditional epidemiological studies typically analyze these levels independently, limiting insights into how structural environmental conditions interact with individual-level characteristics to influence health outcomes. In this study, we introduce a micro-macro machine learning framework that integrates (1) individual-level anthropometric and socioeconomic data from NHANES and (2) macro-level structural environment features, including food access, air quality, and socioeconomic vulnerability extracted from USDA and EPA datasets. Four machine learning models Logistic Regression, Random Forest, XGBoost, and LightGBM were trained to predict obesity using NHANES microdata. XGBoost achieved the strongest performance. A composite environmental vulnerability index (EnvScore) was constructed using normalized indicators from USDA and EPA at the state level. Multi-level comparison revealed strong geographic similarity between states with high environmental burden and the nationally predicted micro-level obesity risk distribution. This demonstrates the feasibility of integrating multi-scale datasets to identify environment-driven disparities in obesity risk. This work contributes a scalable, data-driven, multi-level modeling pipeline suitable for public health informatics, demonstrating strong potential for expansion into causal modeling, intervention planning, and real-time analytics.

</details>


### [82] [Understanding the Mechanisms of Fast Hyperparameter Transfer](https://arxiv.org/abs/2512.22768)
*Nikhil Ghosh,Denny Wu,Alberto Bietti*

Main category: cs.LG

TL;DR: 该论文提出了一个超参数跨尺度转移的理论框架，证明了快速转移在计算最优网格搜索中的优势，并通过μP参数化分析了宽度缩放时的转移机制。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型规模不断增长，使得标准超参数优化变得极其昂贵。需要一种能够从小规模网格搜索直接转移最优超参数到大模型的方法，以最小化性能损失。

Method: 1. 建立了超参数跨尺度转移的概念框架，定义了"快速转移"（转移引起的次优性渐近消失快于有限尺度性能差距）
2. 形式化证明了快速转移对于计算最优网格搜索等价于有用转移
3. 分析了最大更新参数化（μP）在模型宽度缩放时的转移机制
4. 提出了优化轨迹分解假设：损失减少包含宽度稳定分量（决定最优超参数）和宽度敏感分量（随宽度改善但弱扰动超参数最优值）

Result: 1. 证明了快速转移在计算最优网格搜索中比直接调优更高效
2. 展示了μP参数化在特定问题结构中支持快速转移，但在其他合成设置中可能失败
3. 通过实验验证了优化轨迹分解假设，包括大语言模型预训练场景

Conclusion: 超参数跨尺度转移的有效性依赖于问题结构，μP参数化在实践中表现出快速转移特性，这可以通过优化轨迹分解为宽度稳定和宽度敏感分量来解释，为大规模模型的高效超参数优化提供了理论基础。

Abstract: The growing scale of deep learning models has rendered standard hyperparameter (HP) optimization prohibitively expensive. A promising solution is the use of scale-aware hyperparameters, which can enable direct transfer of optimal HPs from small-scale grid searches to large models with minimal performance loss. To understand the principles governing such transfer strategy, we develop a general conceptual framework for reasoning about HP transfer across scale, characterizing transfer as fast when the suboptimality it induces vanishes asymptotically faster than the finite-scale performance gap. We show formally that fast transfer is equivalent to useful transfer for compute-optimal grid search, meaning that transfer is asymptotically more compute-efficient than direct tuning. While empirical work has found that the Maximal Update Parameterization ($μ$P) exhibits fast transfer when scaling model width, the mechanisms remain poorly understood. We show that this property depends critically on problem structure by presenting synthetic settings where transfer either offers provable computational advantage or fails to outperform direct tuning even under $μ$P. To explain the fast transfer observed in practice, we conjecture that decomposing the optimization trajectory reveals two contributions to loss reduction: (1) a width-stable component that determines the optimal HPs, and (2) a width-sensitive component that improves with width but weakly perturbs the HP optimum. We present empirical evidence for this hypothesis across various settings, including large language model pretraining.

</details>


### [83] [GRExplainer: A Universal Explanation Method for Temporal Graph Neural Networks](https://arxiv.org/abs/2512.22772)
*Xuyan Li,Jie Wang,Zheng Yan*

Main category: cs.LG

TL;DR: GRExplainer：首个通用、高效、用户友好的时序图神经网络解释方法，通过节点序列统一特征表示，适用于快照型和事件型TGNNs，在六个真实数据集上优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 时序图神经网络（TGNNs）在处理动态图方面表现出色，但缺乏透明度和可解释性限制了其实际应用。现有TGNN解释方法存在三个主要问题：1）针对特定TGNN类型设计，缺乏通用性；2）计算成本高，不适用于大规模网络；3）忽视解释的结构连通性且需要先验知识，用户友好性差。

Method: 提出GRExplainer方法：1）提取节点序列作为统一特征表示，使其独立于特定输入格式，适用于快照型和事件型TGNNs；2）利用广度优先搜索和时序信息构建输入节点序列，减少冗余计算提高效率；3）设计基于循环神经网络（RNNs）的生成模型，实现自动化连续解释生成。

Result: 在六个真实世界数据集和三个目标TGNNs上的实验表明，GRExplainer在通用性、效率和用户友好性方面优于现有基线方法。

Conclusion: GRExplainer是首个通用、高效、用户友好的TGNN解释方法，解决了现有方法的局限性，为时序图神经网络的可解释性研究提供了新方向。

Abstract: Dynamic graphs are widely used to represent evolving real-world networks. Temporal Graph Neural Networks (TGNNs) have emerged as a powerful tool for processing such graphs, but the lack of transparency and explainability limits their practical adoption. Research on TGNN explainability is still in its early stages and faces several key issues: (i) Current methods are tailored to specific TGNN types, restricting generality. (ii) They suffer from high computational costs, making them unsuitable for large-scale networks. (iii) They often overlook the structural connectivity of explanations and require prior knowledge, reducing user-friendliness. To address these issues, we propose GRExplainer, the first universal, efficient, and user-friendly explanation method for TGNNs. GRExplainer extracts node sequences as a unified feature representation, making it independent of specific input formats and thus applicable to both snapshot-based and event-based TGNNs (the major types of TGNNs). By utilizing breadth-first search and temporal information to construct input node sequences, GRExplainer reduces redundant computation and improves efficiency. To enhance user-friendliness, we design a generative model based on Recurrent Neural Networks (RNNs), enabling automated and continuous explanation generation. Experiments on six real-world datasets with three target TGNNs show that GRExplainer outperforms existing baseline methods in generality, efficiency, and user-friendliness.

</details>


### [84] [Schrodinger AI: A Unified Spectral-Dynamical Framework for Classification, Reasoning, and Operator-Based Generalization](https://arxiv.org/abs/2512.22774)
*Truong Son Nguyen*

Main category: cs.LG

TL;DR: Schrödinger AI是一个受量子力学启发的统一机器学习框架，包含三个核心组件：时间无关波能求解器、时间相关动力学求解器和低秩算子演算，提供可解释语义、鲁棒泛化和涌现拓扑。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在提出一种基于物理原理的机器学习替代方案，超越传统的交叉熵训练和Transformer注意力机制，实现更好的泛化能力、可解释语义和动态推理能力。

Method: 框架包含三个紧密耦合组件：1) 时间无关波能求解器，将感知和分类视为在学习的哈密顿量下的谱分解；2) 时间相关动力学求解器，控制语义波函数随时间演化，支持上下文感知决策修订和重路由；3) 低秩算子演算，通过学习量子类转移算子实现符号变换。

Result: 实验表明：1) 涌现出反映人类概念关系的语义流形，无需显式监督；2) 动态推理适应环境变化，如实时势场扰动的迷宫导航；3) 在模算术任务上实现精确算子泛化，系统学习群作用并在远超训练长度的序列上组合它们。

Conclusion: 该工作为机器学习提供了新的基础方向，将学习视为发现和导航底层语义能量景观的过程，展示了物理驱动方法的潜力。

Abstract: We introduce \textbf{Schrödinger AI}, a unified machine learning framework inspired by quantum mechanics. The system is defined by three tightly coupled components: (1) a {time-independent wave-energy solver} that treats perception and classification as spectral decomposition under a learned Hamiltonian; (2) a {time-dependent dynamical solver} governing the evolution of semantic wavefunctions over time, enabling context-aware decision revision, re-routing, and reasoning under environmental changes; and (3) a {low-rank operator calculus} that learns symbolic transformations such as modular arithmetic through learned quantum-like transition operators. Together, these components form a coherent physics-driven alternative to conventional cross-entropy training and transformer attention, providing robust generalization, interpretable semantics, and emergent topology.
  Empirically, Schrödinger AI demonstrates: (a) emergent semantic manifolds that reflect human-conceived class relations without explicit supervision; (b) dynamic reasoning that adapts to changing environments, including maze navigation with real-time potential-field perturbations; and (c) exact operator generalization on modular arithmetic tasks, where the system learns group actions and composes them across sequences far beyond training length. These results suggest a new foundational direction for machine learning, where learning is cast as discovering and navigating an underlying semantic energy landscape.

</details>


### [85] [Adapting, Fast and Slow: Transportable Circuits for Few-Shot Learning](https://arxiv.org/abs/2512.22777)
*Kasra Jalaldoust,Elias Bareinboim*

Main category: cs.LG

TL;DR: 提出Circuit-TR算法，通过因果可迁移理论实现零样本组合泛化，利用因果图和机制共享信息将源域模块迁移/组合到目标域


<details>
  <summary>Details</summary>
Motivation: 跨域泛化需要约束未见目标域与源域关系的结构，传统方法缺乏利用因果结构进行零样本泛化的能力

Method: 基于因果可迁移理论设计Circuit-TR算法：1) 从源数据学习局部预测器模块集合；2) 根据因果结构许可将模块迁移/组合到目标域形成预测电路；3) 提出无需显式因果结构、仅需少量目标数据的监督域适应方案

Result: 理论结果刻画了基于图电路可迁移标准的少样本可学习任务类别，将少样本泛化能力与电路规模复杂度联系起来，仿真实验验证了理论结果

Conclusion: Circuit-TR通过因果可迁移理论实现了有效的零样本组合泛化，为少样本学习提供了理论框架和实用算法

Abstract: Generalization across the domains is not possible without asserting a structure that constrains the unseen target domain w.r.t. the source domain. Building on causal transportability theory, we design an algorithm for zero-shot compositional generalization which relies on access to qualitative domain knowledge in form of a causal graph for intra-domain structure and discrepancies oracle for inter-domain mechanism sharing. \textit{Circuit-TR} learns a collection of modules (i.e., local predictors) from the source data, and transport/compose them to obtain a circuit for prediction in the target domain if the causal structure licenses. Furthermore, circuit transportability enables us to design a supervised domain adaptation scheme that operates without access to an explicit causal structure, and instead uses limited target data. Our theoretical results characterize classes of few-shot learnable tasks in terms of graphical circuit transportability criteria, and connects few-shot generalizability with the established notion of circuit size complexity; controlled simulations corroborate our theoretical results.

</details>


### [86] [Discovering Transmission Dynamics of COVID-19 in China](https://arxiv.org/abs/2512.22787)
*Zhou Yang,Edward Dougherty,Chen Zhang,Zhenhe Pan,Fang Jin*

Main category: cs.LG

TL;DR: 基于中国公开的COVID-19追踪数据，分析SARS-CoV-2传播模式，发现大城市感染更多且与社交活动相关，79%有症状者在症状出现5天内住院，感染源随时间从湖北旅行相关转向社交活动相关。


<details>
  <summary>Details</summary>
Motivation: 通过回顾性分析大规模检测、隔离和接触者追踪等公共卫生干预措施，识别最有效的COVID-19缓解机制，为未来疫情应对提供参考。

Method: 收集地方卫健委、中国CDC和地方政府社交媒体发布的病例报告，应用自然语言处理和人工整理构建传播/追踪链，结合武汉人口流动数据量化分析时空传播动态。

Result: 存在显著地区差异，大城市感染更多且与社交活动相关；79%有症状者在症状出现5天内住院，有确诊病例接触史者入院时间更短；感染源随时间变化，早期主要与湖北旅行相关，后期更多与社交活动相关。

Conclusion: 公共卫生干预措施需要根据地区特点和疫情阶段动态调整，社交活动是后期传播的主要驱动因素，早期快速识别和隔离对控制疫情至关重要。

Abstract: A comprehensive retrospective analysis of public health interventions, such as large scale testing, quarantining, and contact tracing, can help identify mechanisms most effective in mitigating COVID-19. We investigate China based SARS-CoV-2 transmission patterns (e.g., infection type and likely transmission source) using publicly released tracking data. We collect case reports from local health commissions, the Chinese CDC, and official local government social media, then apply NLP and manual curation to construct transmission/tracking chains. We further analyze tracking data together with Wuhan population mobility data to quantify and visualize temporal and spatial spread dynamics. Results indicate substantial regional differences, with larger cities showing more infections, likely driven by social activities. Most symptomatic individuals (79\%) were hospitalized within 5 days of symptom onset, and those with confirmed-case contact sought admission in under 5 days. Infection sources also shifted over time: early cases were largely linked to travel to (or contact with travelers from) Hubei Province, while later transmission was increasingly associated with social activities.

</details>


### [87] [SNM-Net: A Universal Framework for Robust Open-Set Gas Recognition via Spherical Normalization and Mahalanobis Distance](https://arxiv.org/abs/2512.22792)
*Shuai Chen,Chen Wang,Ziran Wang*

Main category: cs.LG

TL;DR: SNM-Net：一种用于开放集气体识别的通用深度学习框架，通过几何解耦机制和Mahalanobis距离评分，在单位超球面上处理特征，有效解决信号漂移和未知干扰问题。


<details>
  <summary>Details</summary>
Motivation: 电子鼻系统在开放集气体识别中面临双重挑战：信号漂移引起的特征分布偏移和未知干扰导致的决策失败。现有方法主要依赖欧氏距离，未能充分考虑各向异性的气体特征分布和动态信号强度变化。

Method: 提出SNM-Net框架，核心创新包括：1）通过级联批归一化和L2归一化实现几何解耦机制，将高维特征投影到单位超球面以消除信号强度波动；2）引入Mahalanobis距离作为评分机制，利用类别统计构建自适应椭圆决策边界。该框架与CNN、RNN和Transformer骨干网络兼容。

Result: 在Vergara数据集上的系统实验表明，Transformer+SNM配置达到接近理论性能：AUROC为0.9977，未知气体检测率为99.57%（5% FPR下的TPR）。性能显著优于现有方法，AUROC提高3.0%，标准差比Class Anchor Clustering降低91.0%。框架在不同传感器位置表现出卓越鲁棒性，标准差低于0.0028。

Conclusion: SNM-Net有效解决了准确性与稳定性之间的权衡问题，为工业电子鼻部署提供了坚实的技术基础。该框架通过几何解耦和自适应决策边界，显著提升了开放集气体识别的性能。

Abstract: Electronic nose (E-nose) systems face dual challenges in open-set gas recognition: feature distribution shifts caused by signal drift and decision failures induced by unknown interference. Existing methods predominantly rely on Euclidean distance, failing to adequately account for anisotropic gas feature distributions and dynamic signal intensity variations. To address these issues, this study proposes SNM-Net, a universal deep learning framework for open-set gas recognition. The core innovation lies in a geometric decoupling mechanism achieved through cascaded batch normalization and L2 normalization, which projects high-dimensional features onto a unit hypersphere to eliminate signal intensity fluctuations. Additionally, Mahalanobis distance is introduced as the scoring mechanism, utilizing class-wise statistics to construct adaptive ellipsoidal decision boundaries. SNM-Net is architecture-agnostic and seamlessly integrates with CNN, RNN, and Transformer backbones. Systematic experiments on the Vergara dataset demonstrate that the Transformer+SNM configuration attains near-theoretical performance, achieving an AUROC of 0.9977 and an unknown gas detection rate of 99.57% (TPR at 5% FPR). This performance significantly outperforms state-of-the-art methods, showing a 3.0% improvement in AUROC and a 91.0% reduction in standard deviation compared to Class Anchor Clustering. The framework exhibits exceptional robustness across sensor positions with standard deviations below 0.0028. This work effectively resolves the trade-off between accuracy and stability, providing a solid technical foundation for industrial E-nose deployment.

</details>


### [88] [ReDiF: Reinforced Distillation for Few Step Diffusion](https://arxiv.org/abs/2512.22802)
*Amirhossein Tighkhorshid,Zahra Dehghanian,Gholamali Aminian,Chengchun Shi,Hamid R. Rabiee*

Main category: cs.LG

TL;DR: 提出基于强化学习的扩散模型蒸馏框架，将蒸馏过程视为策略优化问题，通过奖励信号指导学生模型，实现更少推理步骤的高效生成。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型采样速度慢，现有蒸馏方法依赖固定的重建或一致性损失，限制了学生模型的学习效率和性能。

Method: 将扩散模型蒸馏视为强化学习策略优化问题，使用教师模型输出作为奖励信号，动态指导学生探索多种去噪路径，允许采取更长、更优化的步骤。

Result: 实验结果表明，该方法在显著减少推理步骤和计算资源的情况下，性能优于现有蒸馏技术，且框架具有模型无关性。

Conclusion: 提出的强化学习蒸馏框架为扩散模型提供了一种通用的优化范式，能够高效学习并减少推理成本，适用于各种扩散模型。

Abstract: Distillation addresses the slow sampling problem in diffusion models by creating models with smaller size or fewer steps that approximate the behavior of high-step teachers. In this work, we propose a reinforcement learning based distillation framework for diffusion models. Instead of relying on fixed reconstruction or consistency losses, we treat the distillation process as a policy optimization problem, where the student is trained using a reward signal derived from alignment with the teacher's outputs. This RL driven approach dynamically guides the student to explore multiple denoising paths, allowing it to take longer, optimized steps toward high-probability regions of the data distribution, rather than relying on incremental refinements. Our framework utilizes the inherent ability of diffusion models to handle larger steps and effectively manage the generative process. Experimental results show that our method achieves superior performance with significantly fewer inference steps and computational resources compared to existing distillation techniques. Additionally, the framework is model agnostic, applicable to any type of diffusion models with suitable reward functions, providing a general optimization paradigm for efficient diffusion learning.

</details>


### [89] [MoR: Mixture Of Representations For Mixed-Precision Training](https://arxiv.org/abs/2512.22804)
*Bor-Yiing Su,Peter Dykas,Mike Chrzanowski,Jatin Chhugani*

Main category: cs.LG

TL;DR: 提出MoR混合表示量化框架，通过动态分析张量数值特性选择FP8/BF16表示，在保持模型质量的同时实现98.38%张量量化到FP8格式


<details>
  <summary>Details</summary>
Motivation: 混合精度训练是扩展深度学习模型的关键技术，但需要找到合适的训练方法组合。现有方法在保持模型质量的同时实现高效量化仍面临挑战

Method: 提出MoR（混合表示）量化框架，在张量和子张量级别动态分析数值特性，选择FP8或BF16表示。设计了具体算法实现动态选择，支持不同量化分区策略

Result: 实现了98.38%张量量化到FP8格式，达到最先进水平，FP8精度与现有方法相当且无需细粒度分区。展示了与NVFP4等更低精度格式结合的潜力

Conclusion: 动态、属性感知的量化方法有巨大潜力，能提高低精度训练的鲁棒性，可与现有训练方法结合进一步利用更低精度格式

Abstract: Mixed-precision training is a crucial technique for scaling deep learning models, but successful mixedprecision training requires identifying and applying the right combination of training methods. This paper presents our preliminary study on Mixture-of-Representations (MoR), a novel, per-tensor and sub-tensor level quantization framework that dynamically analyzes a tensor's numerical properties to select between a variety of different representations. Based on the framework, we have proposed and experimented concrete algorithms that choose dynamically between FP8 and BF16 representations for both per-tensor and sub-tensor level granularities. Our universal approach is designed to preserve model quality across various quantization partition strategies and datasets. Our initial findings show that this approach can achieve state-of-the-art results with 98.38% of tensors quantized to the FP8 format. This work highlights the potential of dynamic, property-aware quantization while preserving model quality. We believe this approach can generally improve the robustness of low precision training, as demonstrated by achieving FP8 accuracies that are on par with existing approaches without the need for fine-grain partitioning, or can be used in combination with other training methods to improve the leverage of even lower precision number formats such as NVFP4.

</details>


### [90] [Long-Range Distillation: Distilling 10,000 Years of Simulated Climate into Long Timestep AI Weather Models](https://arxiv.org/abs/2512.22814)
*Scott A. Martin,Noah Brenowitz,Dale Durran,Michael Pritchard*

Main category: cs.LG

TL;DR: 提出"长程蒸馏"方法，用自回归教师模型生成大量合成气候数据，训练单步长时步学生模型进行长期天气预报，显著提升预报技能


<details>
  <summary>Details</summary>
Motivation: 传统AI天气模型存在两个主要问题：1）自回归方法在长期预报中误差累积导致不稳定和校准问题；2）再分析数据集只有40年，样本有限，无法充分训练长期预报模型

Method: 使用DLESyM作为自回归教师模型生成超过10,000年的合成气候数据，训练长时步概率学生模型直接进行长期预报，学生模型用单步代替数百个自回归步骤

Result: 在完美模型实验中，蒸馏模型优于气候学，接近自回归教师模型技能；在实际应用中，经过ERA5微调后，其S2S预报技能与ECMWF集合预报相当；模型技能随合成训练数据增加而提升

Conclusion: 首次证明AI生成的合成训练数据可以扩展长期预报技能，长程蒸馏方法为解决长期天气预报中的数据稀缺问题提供了有效途径

Abstract: Accurate long-range weather forecasting remains a major challenge for AI models, both because errors accumulate over autoregressive rollouts and because reanalysis datasets used for training offer a limited sample of the slow modes of climate variability underpinning predictability. Most AI weather models are autoregressive, producing short lead forecasts that must be repeatedly applied to reach subseasonal-to-seasonal (S2S) or seasonal lead times, often resulting in instability and calibration issues. Long-timestep probabilistic models that generate long-range forecasts in a single step offer an attractive alternative, but training on the 40-year reanalysis record leads to overfitting, suggesting orders of magnitude more training data are required. We introduce long-range distillation, a method that trains a long-timestep probabilistic "student" model to forecast directly at long-range using a huge synthetic training dataset generated by a short-timestep autoregressive "teacher" model. Using the Deep Learning Earth System Model (DLESyM) as the teacher, we generate over 10,000 years of simulated climate to train distilled student models for forecasting across a range of timescales. In perfect-model experiments, the distilled models outperform climatology and approach the skill of their autoregressive teacher while replacing hundreds of autoregressive steps with a single timestep. In the real world, they achieve S2S forecast skill comparable to the ECMWF ensemble forecast after ERA5 fine-tuning. The skill of our distilled models scales with increasing synthetic training data, even when that data is orders of magnitude larger than ERA5. This represents the first demonstration that AI-generated synthetic training data can be used to scale long-range forecast skill.

</details>


### [91] [TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning](https://arxiv.org/abs/2512.22824)
*Gaurav Chaudhary,Laxmidhar Behera*

Main category: cs.LG

TL;DR: 提出一种基于学生-教师框架和时序方差驱动课程的方法，通过教师模块动态选择策略置信度方差最高的目标来加速目标条件强化学习。


<details>
  <summary>Details</summary>
Motivation: 传统多目标设置中均匀目标选择导致样本效率低下，而生物系统的自适应结构化学习过程启发了更高效的目标条件强化学习方法。

Method: 采用学生-教师学习范式，教师模块根据策略置信度（Q函数）的时序方差动态优先选择高不确定性目标，提供自适应学习信号，方法具有算法无关性。

Result: 在11个机器人操作和迷宫导航任务中评估，相比最先进的课程学习和目标选择方法，取得了持续且显著的性能提升。

Conclusion: 提出的时序方差驱动课程方法通过自适应目标选择有效加速目标条件强化学习，建立了Q值时序方差与策略演化之间的理论联系。

Abstract: Reinforcement Learning (RL) has achieved significant success in solving single-goal tasks. However, uniform goal selection often results in sample inefficiency in multi-goal settings where agents must learn a universal goal-conditioned policy. Inspired by the adaptive and structured learning processes observed in biological systems, we propose a novel Student-Teacher learning paradigm with a Temporal Variance-Driven Curriculum to accelerate Goal-Conditioned RL. In this framework, the teacher module dynamically prioritizes goals with the highest temporal variance in the policy's confidence score, parameterized by the state-action value (Q) function. The teacher provides an adaptive and focused learning signal by targeting these high-uncertainty goals, fostering continual and efficient progress. We establish a theoretical connection between the temporal variance of Q-values and the evolution of the policy, providing insights into the method's underlying principles. Our approach is algorithm-agnostic and integrates seamlessly with existing RL frameworks. We demonstrate this through evaluation across 11 diverse robotic manipulation and maze navigation tasks. The results show consistent and notable improvements over state-of-the-art curriculum learning and goal-selection methods.

</details>


### [92] [Fundamental Novel Consistency Theory: $H$-Consistency Bounds](https://arxiv.org/abs/2512.22880)
*Yutao Zhong*

Main category: cs.LG

TL;DR: 该论文提出了H-一致性边界理论，为机器学习中代理损失函数与目标损失函数之间的估计误差提供理论保证，比贝叶斯一致性或H-校准更强，比超额误差边界更信息丰富。


<details>
  <summary>Details</summary>
Motivation: 机器学习训练中优化的损失函数常与定义任务性能的目标损失不同（由于计算不可行或不可微），需要理论分析代理损失与目标损失之间的估计误差关系。

Method: 提出H-一致性边界理论框架，分析二元分类（凸代理、对抗设置）、多类分类（max、sum、约束损失）、comp-sum损失（交叉熵、MAE），建立分布依赖/独立边界，分析最小化差距。

Result: 建立了紧致的分布依赖/独立边界，首次推导了多类分类和comp-sum损失的H-一致性边界，提出平滑对抗变体，证明光滑代理具有普适平方根增长率，在某些情况下非平凡边界不可达。

Conclusion: H-一致性边界提供了比传统理论更强的保证，为代理损失选择提供理论指导，建立了统一分析框架，对二元/多类分类、对抗/非对抗场景均有理论贡献。

Abstract: In machine learning, the loss functions optimized during training often differ from the target loss that defines task performance due to computational intractability or lack of differentiability. We present an in-depth study of the target loss estimation error relative to the surrogate loss estimation error. Our analysis leads to $H$-consistency bounds, which are guarantees accounting for the hypothesis set $H$. These bounds offer stronger guarantees than Bayes-consistency or $H$-calibration and are more informative than excess error bounds.
  We begin with binary classification, establishing tight distribution-dependent and -independent bounds. We provide explicit bounds for convex surrogates (including linear models and neural networks) and analyze the adversarial setting for surrogates like $ρ$-margin and sigmoid loss. Extending to multi-class classification, we present the first $H$-consistency bounds for max, sum, and constrained losses, covering both non-adversarial and adversarial scenarios. We demonstrate that in some cases, non-trivial $H$-consistency bounds are unattainable. We also investigate comp-sum losses (e.g., cross-entropy, MAE), deriving their first $H$-consistency bounds and introducing smooth adversarial variants that yield robust learning algorithms.
  We develop a comprehensive framework for deriving these bounds across various surrogates, introducing new characterizations for constrained and comp-sum losses. Finally, we examine the growth rates of $H$-consistency bounds, establishing a universal square-root growth rate for smooth surrogates in binary and multi-class tasks, and analyze minimizability gaps to guide surrogate selection.

</details>


### [93] [Theory and Algorithms for Learning with Multi-Class Abstention and Multi-Expert Deferral](https://arxiv.org/abs/2512.22886)
*Anqi Mao*

Main category: cs.LG

TL;DR: 该论文系统研究了多专家延迟学习问题，包括分类中的弃权和回归中的延迟，提出了具有强一致性保证的新损失函数和算法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在幻觉和高推理成本问题，通过多专家系统将不确定输入委托给更强大的专家可以提高可靠性，将简单查询路由到小型蒸馏模型可以提高效率，这激发了多专家延迟学习的研究。

Method: 1) 针对分类中的弃权问题，提出了基于分数和预测器-拒绝器的新损失函数族，证明了强非渐近一致性保证；2) 针对多专家延迟分类，设计了单阶段和两阶段场景的新损失函数，证明了H-一致性边界；3) 针对回归延迟，提出了支持多专家和多种成本结构的通用框架，设计了具有H-一致性保证的新损失函数。

Result: 在CIFAR-10、CIFAR-100和SVHN数据集上的实验表明，所提算法在弃权问题上表现优异；为多专家延迟分类和回归延迟提供了具有理论保证的有效新算法。

Conclusion: 该论文为多专家延迟学习提供了全面的理论框架和实用算法，解决了现有开放性问题，在分类和回归任务中都实现了强一致性保证和实证有效性。

Abstract: Large language models (LLMs) have achieved remarkable performance but face critical challenges: hallucinations and high inference costs. Leveraging multiple experts offers a solution: deferring uncertain inputs to more capable experts improves reliability, while routing simpler queries to smaller, distilled models enhances efficiency. This motivates the problem of learning with multiple-expert deferral. This thesis presents a comprehensive study of this problem and the related problem of learning with abstention, supported by strong consistency guarantees.
  First, for learning with abstention (a special case of deferral), we analyze score-based and predictor-rejector formulations in multi-class classification. We introduce new families of surrogate losses and prove strong non-asymptotic, hypothesis set-specific consistency guarantees, resolving two existing open questions. We analyze both single-stage and practical two-stage settings, with experiments on CIFAR-10, CIFAR-100, and SVHN demonstrating the superior performance of our algorithms.
  Second, we address general multi-expert deferral in classification. We design new surrogate losses for both single-stage and two-stage scenarios and prove they benefit from strong $H$-consistency bounds. For the two-stage scenario, we show that our surrogate losses are realizable $H$-consistent for constant cost functions, leading to effective new algorithms.
  Finally, we introduce a novel framework for regression with deferral to address continuous label spaces. Our versatile framework accommodates multiple experts and various cost structures, supporting both single-stage and two-stage methods. It subsumes recent work on regression with abstention. We propose new surrogate losses with proven $H$-consistency and demonstrate the empirical effectiveness of the resulting algorithms.

</details>


### [94] [Federated Multi-Task Clustering](https://arxiv.org/abs/2512.22897)
*S. Dai,G. Sun,F. Li,X. Tang,Q. Wang,Y. Cong*

Main category: cs.LG

TL;DR: FMTC是一个联邦多任务聚类框架，通过客户端个性化聚类模块和服务端张量相关性模块，在保护隐私的同时为异构客户端学习个性化聚类模型，并利用共享的底层结构。


<details>
  <summary>Details</summary>
Motivation: 现有谱聚类算法大多针对集中式设置，不适用于现代去中心化环境。当前的联邦学习方法依赖不可靠的伪标签导致泛化性能差，且未能捕捉异构客户端之间的潜在相关性。

Method: FMTC框架包含两个主要组件：1) 客户端个性化聚类模块，学习参数化映射模型以支持鲁棒的样本外推理，绕过不可靠的伪标签；2) 服务端张量相关性模块，将所有客户端模型组织成统一张量并应用低秩正则化来发现它们的公共子空间。使用基于ADMM的高效隐私保护分布式算法进行优化。

Result: 在多个真实世界数据集上的广泛实验表明，FMTC框架显著优于各种基线和最先进的联邦聚类算法。

Conclusion: FMTC成功解决了联邦聚类中的异构性挑战，通过个性化聚类模型和共享知识发现，在保护隐私的同时实现了优越的聚类性能。

Abstract: Spectral clustering has emerged as one of the most effective clustering algorithms due to its superior performance. However, most existing models are designed for centralized settings, rendering them inapplicable in modern decentralized environments. Moreover, current federated learning approaches often suffer from poor generalization performance due to reliance on unreliable pseudo-labels, and fail to capture the latent correlations amongst heterogeneous clients. To tackle these limitations, this paper proposes a novel framework named Federated Multi-Task Clustering (i.e.,FMTC), which intends to learn personalized clustering models for heterogeneous clients while collaboratively leveraging their shared underlying structure in a privacy-preserving manner. More specifically, the FMTC framework is composed of two main components: client-side personalized clustering module, which learns a parameterized mapping model to support robust out-of-sample inference, bypassing the need for unreliable pseudo-labels; and server-side tensorial correlation module, which explicitly captures the shared knowledge across all clients. This is achieved by organizing all client models into a unified tensor and applying a low-rank regularization to discover their common subspace. To solve this joint optimization problem, we derive an efficient, privacy-preserving distributed algorithm based on the Alternating Direction Method of Multipliers, which decomposes the global problem into parallel local updates on clients and an aggregation step on the server. To the end, several extensive experiments on multiple real-world datasets demonstrate that our proposed FMTC framework significantly outperforms various baseline and state-of-the-art federated clustering algorithms.

</details>


### [95] [Debugging Tabular Log as Dynamic Graphs](https://arxiv.org/abs/2512.22903)
*Chumeng Liang,Zhanyang Jin,Zahaib Akhtar,Mona Pereira,Haofei Yu,Jiaxuan You*

Main category: cs.LG

TL;DR: 提出GraphLogDebugger框架，基于动态图调试表格日志，通过构建异构节点和边恢复系统演化图，用简单动态GNN超越LLMs性能


<details>
  <summary>Details</summary>
Motivation: 现有处理文本丰富表格日志的方法过度依赖大型语言模型和其他重型模型，导致灵活性和可扩展性受限，需要更高效的调试方法

Method: 构建GraphLogDebugger框架：为对象和事件创建异构节点，连接节点间边，将表格日志背后的系统恢复为演化动态图，使用简单动态图神经网络进行调试

Result: 在计算机系统和学术论文的真实日志数据集上验证，动态图建模使简单GNN足以超越LLMs在表格日志调试中的表现

Conclusion: GraphLogDebugger通过动态图建模有效解决表格日志调试问题，相比依赖LLMs的方法具有更好的灵活性和可扩展性

Abstract: Tabular log abstracts objects and events in the real-world system and reports their updates to reflect the change of the system, where one can detect real-world inconsistencies efficiently by debugging corresponding log entries. However, recent advances in processing text-enriched tabular log data overly depend on large language models (LLMs) and other heavy-load models, thus suffering from limited flexibility and scalability. This paper proposes a new framework, GraphLogDebugger, to debug tabular log based on dynamic graphs. By constructing heterogeneous nodes for objects and events and connecting node-wise edges, the framework recovers the system behind the tabular log as an evolving dynamic graph. With the help of our dynamic graph modeling, a simple dynamic Graph Neural Network (GNN) is representative enough to outperform LLMs in debugging tabular log, which is validated by experimental results on real-world log datasets of computer systems and academic papers.

</details>


### [96] [MetaCD: A Meta Learning Framework for Cognitive Diagnosis based on Continual Learning](https://arxiv.org/abs/2512.22904)
*Jin Wu,Chanjin Zheng*

Main category: cs.LG

TL;DR: 提出基于持续学习的元学习认知诊断框架MetaCD，解决长尾分布和动态变化问题，在五个真实数据集上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习认知诊断方法受限于数据的长尾分布和动态变化问题，需要提高模型对新任务和小样本数据的适应能力。

Method: 提出MetaCD框架：1) 使用元学习学习最优初始化状态，缓解长尾问题；2) 采用参数保护机制的持续学习方法，适应新技能/新任务；3) 平衡单个任务的可塑性和序列任务的稳定性与泛化性。

Result: 在五个真实世界数据集上的综合实验表明，MetaCD在准确性和泛化能力方面均优于其他基线方法。

Conclusion: MetaCD框架有效解决了认知诊断中的长尾分布和动态变化挑战，提高了模型的可塑性、稳定性和泛化能力。

Abstract: Cognitive diagnosis is an essential research topic in intelligent education, aimed at assessing the level of mastery of different skills by students. So far, many research works have used deep learning models to explore the complex interactions between students, questions, and skills. However, the performance of existing method is frequently limited by the long-tailed distribution and dynamic changes in the data. To address these challenges, we propose a meta-learning framework for cognitive diagnosis based on continual learning (MetaCD). This framework can alleviate the long-tailed problem by utilizing meta-learning to learn the optimal initialization state, enabling the model to achieve good accuracy on new tasks with only a small amount of data. In addition, we utilize a continual learning method named parameter protection mechanism to give MetaCD the ability to adapt to new skills or new tasks, in order to adapt to dynamic changes in data. MetaCD can not only improve the plasticity of our model on a single task, but also ensure the stability and generalization of the model on sequential tasks. Comprehensive experiments on five real-world datasets show that MetaCD outperforms other baselines in both accuracy and generalization.

</details>


### [97] [Sat-EnQ: Satisficing Ensembles of Weak Q-Learners for Reliable and Compute-Efficient Reinforcement Learning](https://arxiv.org/abs/2512.22910)
*Ünver Çiftçi*

Main category: cs.LG

TL;DR: Sat-EnQ：一种两阶段深度Q学习框架，先通过"满意化"学习避免早期训练不稳定，再优化到最优，显著降低方差和计算成本。


<details>
  <summary>Details</summary>
Motivation: 深度Q学习算法在早期训练时非常不稳定，最大化操作会放大估计误差，导致灾难性高估。受有限理性理论和发育学习启发，需要一种更稳健的方法。

Method: 采用两阶段框架：第一阶段使用轻量级Q网络集成，在动态基线约束下学习"足够好"的满意化目标，限制早期价值增长；第二阶段将集成蒸馏到更大网络中，用标准Double DQN微调。

Result: 理论证明满意化能限制更新幅度且不增加目标方差；实证显示方差降低3.8倍，消除灾难性失败（0% vs DQN的50%），环境噪声下保持79%性能，计算成本比自举集成少2.5倍。

Conclusion: 通过先满意化再优化的原则性路径，为稳健强化学习提供了新方向，平衡了稳定性与最终性能。

Abstract: Deep Q-learning algorithms remain notoriously unstable, especially during early training when the maximization operator amplifies estimation errors. Inspired by bounded rationality theory and developmental learning, we introduce Sat-EnQ, a two-phase framework that first learns to be ``good enough'' before optimizing aggressively. In Phase 1, we train an ensemble of lightweight Q-networks under a satisficing objective that limits early value growth using a dynamic baseline, producing diverse, low-variance estimates while avoiding catastrophic overestimation. In Phase 2, the ensemble is distilled into a larger network and fine-tuned with standard Double DQN. We prove theoretically that satisficing induces bounded updates and cannot increase target variance, with a corollary quantifying conditions for substantial reduction. Empirically, Sat-EnQ achieves 3.8x variance reduction, eliminates catastrophic failures (0% vs 50% for DQN), maintains 79% performance under environmental noise}, and requires 2.5x less compute than bootstrapped ensembles. Our results highlight a principled path toward robust reinforcement learning by embracing satisficing before optimization.

</details>


### [98] [Multiple Token Divergence: Measuring and Steering In-Context Computation Density](https://arxiv.org/abs/2512.22944)
*Vincent Herrmann,Eric Alcaide,Michael Wand,Jürgen Schmidhuber*

Main category: cs.LG

TL;DR: 提出MTD方法，通过比较模型完整输出分布与浅层辅助预测头分布之间的KL散度来测量语言模型的上下文计算努力，无需额外训练即可分析推理复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有方法如下一个词元损失无法捕捉推理复杂度，而基于潜在状态可压缩性的方法具有侵入性和不稳定性，需要一种简单有效的计算努力测量工具。

Method: 提出多词元散度(MTD)，定义为模型完整输出分布与浅层辅助预测头分布之间的KL散度；基于此提出散度引导解码方法，控制生成文本的计算特性。

Result: MTD比先前方法更能区分复杂任务与简单任务；在数学推理基准上，MTD与问题难度正相关；较低的MTD与更准确的推理相关。

Conclusion: MTD为分析和引导语言模型计算动态提供了实用、轻量级的工具，可直接从预训练模型计算，无需额外训练。

Abstract: Measuring the in-context computational effort of language models is a key challenge, as metrics like next-token loss fail to capture reasoning complexity. Prior methods based on latent state compressibility can be invasive and unstable. We propose Multiple Token Divergence (MTD), a simple measure of computational effort defined as the KL divergence between a model's full output distribution and that of a shallow, auxiliary prediction head. MTD can be computed directly from pre-trained models with multiple prediction heads, requiring no additional training. Building on this, we introduce Divergence Steering, a novel decoding method to control the computational character of generated text. We empirically show that MTD is more effective than prior methods at distinguishing complex tasks from simple ones. On mathematical reasoning benchmarks, MTD correlates positively with problem difficulty. Lower MTD is associated with more accurate reasoning. MTD provides a practical, lightweight tool for analyzing and steering the computational dynamics of language models.

</details>


### [99] [APO: Alpha-Divergence Preference Optimization](https://arxiv.org/abs/2512.22953)
*Wang Zixian*

Main category: cs.LG

TL;DR: APO提出了一种基于alpha散度的锚定偏好优化框架，能够在正向KL和反向KL之间连续插值，通过奖励和置信度引导的alpha调度策略，在保持训练稳定性的同时实现从模式覆盖到模式利用的过渡。


<details>
  <summary>Details</summary>
Motivation: 当前对齐方法存在两种主要分歧：监督微调等方法最小化正向KL散度，能稳定覆盖模式但可能未充分利用高奖励模式；而PPO等在线强化学习方法更接近反向KL散度，能进行模式寻求改进但存在模式崩溃风险。现有锚定方法通常只采用单一散度，缺乏灵活性。

Method: 提出了Alpha-Divergence Preference Optimization (APO)，这是一个基于Csiszar alpha散度的锚定框架，能够在同一锚定几何中连续插值正向KL和反向KL行为。推导了参数化alpha的统一梯度动态，分析了梯度方差特性，并提出了一种实用的奖励和置信度引导的alpha调度策略。

Result: 在Qwen3-1.7B模型上的math-level3实验表明，APO在保持训练稳定性的同时，实现了与GRPO和GSPO基线相当的竞争性能。

Conclusion: APO框架通过alpha散度在正向KL和反向KL之间提供连续插值，结合奖励和置信度引导的调度策略，能够在模式覆盖和模式利用之间取得平衡，同时保持训练稳定性，为对齐方法提供了更灵活的选择。

Abstract: Two divergence regimes dominate modern alignment practice. Supervised fine-tuning and many distillation-style objectives implicitly minimize the forward KL divergence KL(q || pi_theta), yielding stable mode-covering updates but often under-exploiting high-reward modes. In contrast, PPO-style online reinforcement learning from human feedback behaves closer to reverse KL divergence KL(pi_theta || q), enabling mode-seeking improvements but risking mode collapse. Recent anchored methods, such as ADPO, show that performing the projection in anchored coordinates can substantially improve stability, yet they typically commit to a single divergence. We introduce Alpha-Divergence Preference Optimization (APO), an anchored framework that uses Csiszar alpha-divergence to continuously interpolate between forward and reverse KL behavior within the same anchored geometry. We derive unified gradient dynamics parameterized by alpha, analyze gradient variance properties, and propose a practical reward-and-confidence-guarded alpha schedule that transitions from coverage to exploitation only when the policy is both improving and confidently calibrated. Experiments on Qwen3-1.7B with math-level3 demonstrate that APO achieves competitive performance with GRPO and GSPO baselines while maintaining training stability.

</details>


### [100] [FLOW: A Feedback-Driven Synthetic Longitudinal Dataset of Work and Wellbeing](https://arxiv.org/abs/2512.22956)
*Wafaa El Husseini*

Main category: cs.LG

TL;DR: FLOW是一个合成纵向数据集，模拟1000人两年内工作负荷、生活方式与幸福感之间的日常互动，用于隐私受限情况下的可重复研究和方法基准测试。


<details>
  <summary>Details</summary>
Motivation: 由于隐私、伦理和后勤限制，获取个体层面的工作生活平衡与幸福感纵向数据困难，这影响了压力建模、行为分析和机器学习等领域的可重复研究、方法基准测试和教育。

Method: 采用基于规则的反馈驱动模拟方法，生成具有连贯时间动态的合成数据，涵盖压力、睡眠、情绪、身体活动和体重等变量，并开发可配置的数据生成工具。

Result: 创建了包含1000个个体两年每日分辨率数据的FLOW数据集，作为公开资源发布，并提供了可调整行为和环境假设的可配置数据生成工具。

Conclusion: FLOW是一个受控实验环境而非真实人群代理，支持在无法获取真实数据情况下的探索性分析、方法开发和基准测试。

Abstract: Access to longitudinal, individual-level data on work-life balance and wellbeing is limited by privacy, ethical, and logistical constraints. This poses challenges for reproducible research, methodological benchmarking, and education in domains such as stress modeling, behavioral analysis, and machine learning.
  We introduce FLOW, a synthetic longitudinal dataset designed to model daily interactions between workload, lifestyle behaviors, and wellbeing. FLOW is generated using a rule-based, feedback-driven simulation that produces coherent temporal dynamics across variables such as stress, sleep, mood, physical activity, and body weight. The dataset simulates 1{,}000 individuals over a two-year period with daily resolution and is released as a publicly available resource.
  In addition to the static dataset, we describe a configurable data generation tool that enables reproducible experimentation under adjustable behavioral and contextual assumptions. FLOW is intended as a controlled experimental environment rather than a proxy for observed human populations, supporting exploratory analysis, methodological development, and benchmarking where real-world data are inaccessible.

</details>


### [101] [A Context-Aware Temporal Modeling through Unified Multi-Scale Temporal Encoding and Hierarchical Sequence Learning for Single-Channel EEG Sleep Staging](https://arxiv.org/abs/2512.22976)
*Amirali Vakili,Salar Jahanshiri,Armin Salimi-Badr*

Main category: cs.LG

TL;DR: 提出一个上下文感知且可解释的单通道EEG睡眠分期框架，特别关注改善N1期检测，在SleepEDF数据集上达到89.72%的总体准确率和61.7%的N1期F1分数。


<details>
  <summary>Details</summary>
Motivation: 自动睡眠分期对睡眠障碍诊断至关重要，但现有单通道EEG方法面临类别不平衡、感受野有限、可解释性不足等问题，特别是N1期检测困难。

Method: 结合紧凑多尺度特征提取和时间建模，使用类别加权损失函数和数据增强处理不平衡问题，将EEG信号分块处理并通过平均softmax概率获得最终预测。

Result: 在SleepEDF数据集上达到89.72%总体准确率和85.46%宏平均F1分数，N1期F1分数为61.7%，显著优于先前方法。

Conclusion: 该框架有效提升了睡眠分期性能，特别是N1期检测，同时保持了可解释性和临床适用性，为实际医疗应用提供了实用解决方案。

Abstract: Automatic sleep staging is a critical task in healthcare due to the global prevalence of sleep disorders. This study focuses on single-channel electroencephalography (EEG), a practical and widely available signal for automatic sleep staging. Existing approaches face challenges such as class imbalance, limited receptive-field modeling, and insufficient interpretability. This work proposes a context-aware and interpretable framework for single-channel EEG sleep staging, with particular emphasis on improving detection of the N1 stage. Many prior models operate as black boxes with stacked layers, lacking clearly defined and interpretable feature extraction roles.The proposed model combines compact multi-scale feature extraction with temporal modeling to capture both local and long-range dependencies. To address data imbalance, especially in the N1 stage, classweighted loss functions and data augmentation are applied. EEG signals are segmented into sub-epoch chunks, and final predictions are obtained by averaging softmax probabilities across chunks, enhancing contextual representation and robustness.The proposed framework achieves an overall accuracy of 89.72% and a macro-average F1-score of 85.46%. Notably, it attains an F1- score of 61.7% for the challenging N1 stage, demonstrating a substantial improvement over previous methods on the SleepEDF datasets. These results indicate that the proposed approach effectively improves sleep staging performance while maintaining interpretability and suitability for real-world clinical applications.

</details>


### [102] [Fusion or Confusion? Multimodal Complexity Is Not All You Need](https://arxiv.org/abs/2512.22991)
*Tillmann Rheude,Roland Eils,Benjamin Wild*

Main category: cs.LG

TL;DR: 复杂多模态学习方法并不比简单基线方法表现更好，在标准化实验条件下，简单的后期融合Transformer架构（SimBaMM）与复杂方法表现相当甚至更好。


<details>
  <summary>Details</summary>
Motivation: 挑战当前多模态学习领域的一个普遍假设：更复杂的多模态特定方法会带来更好的性能。作者认为现有研究缺乏标准化比较，复杂架构的优势可能被夸大。

Method: 1) 大规模实证研究，重新实现19个高影响力方法并在标准化条件下评估；2) 在9个多样化数据集（最多23个模态）上测试；3) 提出简单基线方法SimBaMM（后期融合Transformer架构）；4) 对所有方法进行严格的超参数调优；5) 测试方法在新任务和缺失模态情况下的泛化能力。

Result: 在标准化实验条件下，更复杂的架构并不能可靠地超越SimBaMM。统计分析表明复杂方法与SimBaMM表现相当，且经常无法可靠地超越经过良好调优的单模态基线，特别是在原始研究常考虑的小数据场景中。

Conclusion: 多模态学习领域需要从追求架构新颖性转向方法论严谨性。作者提供了实用可靠性检查清单以促进可比、稳健和可信的未来评估，并呼吁关注方法学严谨性而非架构创新。

Abstract: Deep learning architectures for multimodal learning have increased in complexity, driven by the assumption that multimodal-specific methods improve performance. We challenge this assumption through a large-scale empirical study reimplementing 19 high-impact methods under standardized conditions, evaluating them across nine diverse datasets with up to 23 modalities, and testing their generalizability to new tasks beyond their original scope, including settings with missing modalities. We propose a Simple Baseline for Multimodal Learning (SimBaMM), a straightforward late-fusion Transformer architecture, and demonstrate that under standardized experimental conditions with rigorous hyperparameter tuning of all methods, more complex architectures do not reliably outperform SimBaMM. Statistical analysis indicates that more complex methods perform comparably to SimBaMM and frequently do not reliably outperform well-tuned unimodal baselines, especially in the small-data regime considered in many original studies. To support our findings, we include a case study of a recent multimodal learning method highlighting the methodological shortcomings in the literature. In addition, we provide a pragmatic reliability checklist to promote comparable, robust, and trustworthy future evaluations. In summary, we argue for a shift in focus: away from the pursuit of architectural novelty and toward methodological rigor.

</details>


### [103] [Merge before Forget: A Single LoRA Continual Learning via Continual Merging](https://arxiv.org/abs/2512.23017)
*Fuli Qiao,Mehrdad Mahdavi*

Main category: cs.LG

TL;DR: 提出一种新的持续学习方法，通过正交初始化和顺序合并LoRA更新到单一统一LoRA中，解决现有方法内存增长和任务干扰问题。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA持续学习方法存在内存随任务增长、存储空间有限以及缺乏有效合并机制导致任务干扰的问题，需要更高效的解决方案。

Method: 采用正交基初始化新任务学习，利用时间感知缩放机制平衡新旧知识，将多个LoRA顺序合并为单一统一LoRA，保持恒定内存复杂度。

Result: 在多种Llama模型和持续学习基准测试中表现出色，保持恒定内存复杂度，最小化任务间干扰，并通过自适应缩放提升性能。

Conclusion: 提出的正交初始化和顺序合并LoRA方法有效解决了持续学习中的内存增长和任务干扰问题，实现了高效且性能优越的持续学习。

Abstract: Parameter-efficient continual learning has emerged as a promising approach for large language models (LLMs) to mitigate catastrophic forgetting while enabling adaptation to new tasks. Current Low-Rank Adaptation (LoRA) continual learning techniques often retain and freeze previously learned LoRAs or generate data representations to overcome forgetting, typically utilizing these to support new LoRAs learn new tasks. However, these methods not only ignore growing computational memory with tasks and limited storage space but also suffer from potential task interference due to the lack of effective LoRA merging mechanisms. In this paper, we propose a novel continual learning method that orthogonally initializes and sequentially merges LoRAs updates into a single unified LoRA. Our method leverages orthogonal basis extraction from previously learned LoRA to initialize the learning of new tasks, further exploits the intrinsic asymmetry property of LoRA components by using a time-aware scaling mechanism to balance new and old knowledge during continual merging. Our approach maintains constant memory complexity with respect to the number of tasks, minimizes interference between past and new tasks via orthogonal basis initialization, and improves performance over asymmetric LoRA merging via adaptive scaling. We provide theoretical analysis to justify our design and conduct extensive experiments across diverse continual learning benchmarks using various Llama models, demonstrating the effectiveness and efficiency of our method.

</details>


### [104] [Mechanistic Analysis of Circuit Preservation in Federated Learning](https://arxiv.org/abs/2512.23043)
*Muhammad Haseeb,Salaar Masood,Muhammad Abdullah Sohail*

Main category: cs.LG

TL;DR: 通过机制可解释性分析发现，非独立同分布数据导致联邦学习中不同客户端的稀疏子网络电路发生冲突和退化，这是性能下降的机制原因。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非独立同分布数据下性能显著下降，虽然这一现象已被广泛记录，但其内部机制原因仍然不明确。本文旨在通过机制可解释性方法揭示FedAvg算法在非IID数据下失效的具体机制。

Method: 使用机制可解释性方法分析FedAvg算法，训练具有内在可解释性的权重稀疏神经网络，在联邦学习框架中识别和追踪不同客户端和通信轮次中的电路（负责特定类别预测的稀疏子网络），使用交并比量化电路保存情况。

Result: 首次提供了机制证据表明，非独立同分布数据分布导致结构上不同的本地电路发生分歧，进而在全局模型中退化。冲突的客户端更新聚合导致电路崩溃，即负责特定类别预测的功能性稀疏子网络发生破坏性干扰。

Conclusion: 将联邦学习中的统计漂移问题重新定义为机制保存的具体可观察失败，为开发更有针对性的解决方案铺平了道路。非IID数据导致电路分歧和退化是性能下降的机制原因。

Abstract: Federated Learning (FL) enables collaborative training of models on decentralized data, but its performance degrades significantly under Non-IID (non-independent and identically distributed) data conditions. While this accuracy loss is well-documented, the internal mechanistic causes remain a black box. This paper investigates the canonical FedAvg algorithm through the lens of Mechanistic Interpretability (MI) to diagnose this failure mode. We hypothesize that the aggregation of conflicting client updates leads to circuit collapse, the destructive interference of functional, sparse sub-networks responsible for specific class predictions. By training inherently interpretable, weight-sparse neural networks within an FL framework, we identify and track these circuits across clients and communication rounds. Using Intersection-over-Union (IoU) to quantify circuit preservation, we provide the first mechanistic evidence that Non-IID data distributions cause structurally distinct local circuits to diverge, leading to their degradation in the global model. Our findings reframe the problem of statistical drift in FL as a concrete, observable failure of mechanistic preservation, paving the way for more targeted solutions.

</details>


### [105] [Breaking the Memory Wall: Exact Analytical Differentiation via Tiled Operator-Space Evolution](https://arxiv.org/abs/2512.23068)
*Shuhuan Wang,Yuzhen Xie,Jiayi Li,Yinliang Diao*

Main category: cs.LG

TL;DR: PGF框架为选择性状态空间模型提供O(1)内存复杂度的精确梯度计算，解决基因组尺度建模的内存瓶颈


<details>
  <summary>Details</summary>
Motivation: 选择性状态空间模型虽然实现线性时间推理，但其梯度敏感性分析在反向传播期间受限于O(L)内存缩放，阻碍了在消费级硬件上进行基因组尺度建模（L > 10^5）

Method: 提出相位梯度流（PGF）框架，通过在状态空间流形上直接操作计算精确解析导数，避免构建中间计算图。通过将SSM动态重构为平铺算子空间演化（TOSE），实现相对于序列长度的O(1)内存复杂度

Result: 相比标准Autograd，PGF实现94%的峰值VRAM减少和23倍吞吐量提升。在128,000步序列的脉冲响应基准测试中，PGF在传统Autograd因内存限制而失败的场景下保持稳定，确保染色体尺度敏感性分析在单GPU上可行

Conclusion: PGF通过O(1)内存复杂度的精确梯度计算，弥合了理论无限上下文模型与实际硬件限制之间的差距，使染色体尺度敏感性分析在单GPU上成为可能

Abstract: Selective State Space Models (SSMs) achieve linear-time inference, yet their gradient-based sensitivity analysis remains bottlenecked by O(L) memory scaling during backpropagation. This memory constraint precludes genomic-scale modeling (L > 10^5) on consumer-grade hardware. We introduce Phase Gradient Flow (PGF), a framework that computes exact analytical derivatives by operating directly in the state-space manifold, bypassing the need to materialize the intermediate computational graph. By reframing SSM dynamics as Tiled Operator-Space Evolution (TOSE), our method delivers O(1) memory complexity relative to sequence length, yielding a 94% reduction in peak VRAM and a 23x increase in throughput compared to standard Autograd. Unlike parallel prefix scans that exhibit numerical divergence in stiff ODE regimes, PGF ensures stability through invariant error scaling, maintaining near-machine precision across extreme sequences. We demonstrate the utility of PGF on an impulse-response benchmark with 128,000-step sequences - a scale where conventional Autograd encounters prohibitive memory overhead, often leading to out-of-memory (OOM) failures in multi-layered models. Our work enables chromosome-scale sensitivity analysis on a single GPU, bridging the gap between theoretical infinite-context models and practical hardware limitations.

</details>


### [106] [FLEX-MoE: Federated Mixture-of-Experts with Load-balanced Expert Assignment](https://arxiv.org/abs/2512.23070)
*Boyang Zhang,Xiaobing Chen,Songyang Zhang,Shuai Zhang,Xiangwei Zhou,Mingxuan Sun*

Main category: cs.LG

TL;DR: FLEX-MoE：一种联邦学习中的MoE框架，通过联合优化专家分配和负载均衡，解决边缘设备存储限制和非IID数据导致的专家负载不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习环境中部署MoE模型面临两个关键挑战：1）资源受限的边缘设备无法存储完整的专家集合；2）非IID数据分布导致严重的专家负载不平衡，从而降低模型性能。

Method: 提出FLEX-MoE框架，引入客户端-专家适应度分数来量化专家对本地数据集的适用性，并通过基于优化的算法在系统范围内最大化客户端-专家专业化，同时强制平衡专家利用率。

Result: 在三个不同数据集上的综合实验表明，FLEX-MoE具有优越的性能，并能在各种资源受限场景下保持平衡的专家利用率。

Conclusion: FLEX-MoE有效解决了联邦学习中MoE模型的专家分配和负载平衡问题，相比现有仅关注个性化而忽略负载不平衡的贪婪方法，能够更好地处理异构数据环境下的专家利用率偏斜。

Abstract: Mixture-of-Experts (MoE) models enable scalable neural networks through conditional computation. However, their deployment with federated learning (FL) faces two critical challenges: 1) resource-constrained edge devices cannot store full expert sets, and 2) non-IID data distributions cause severe expert load imbalance that degrades model performance. To this end, we propose \textbf{FLEX-MoE}, a novel federated MoE framework that jointly optimizes expert assignment and load balancing under limited client capacity. Specifically, our approach introduces client-expert fitness scores that quantify the expert suitability for local datasets through training feedback, and employs an optimization-based algorithm to maximize client-expert specialization while enforcing balanced expert utilization system-wide. Unlike existing greedy methods that focus solely on personalization while ignoring load imbalance, our FLEX-MoE is capable of addressing the expert utilization skew, which is particularly severe in FL settings with heterogeneous data. Our comprehensive experiments on three different datasets demonstrate the superior performance of the proposed FLEX-MoE, together with its ability to maintain balanced expert utilization across diverse resource-constrained scenarios.

</details>


### [107] [Rethinking Fine-Tuning: Unlocking Hidden Capabilities in Vision-Language Models](https://arxiv.org/abs/2512.23073)
*Mingyuan Zhang,Yue Bai,Yifan Wang,Yiyang Huang,Yun Fu*

Main category: cs.LG

TL;DR: 该论文提出将掩码微调（MFT）应用于视觉语言模型（VLM），通过为权重分配可学习的门控分数来重组内部子网络，而不是更新权重，从而实现高效的下游任务适应。


<details>
  <summary>Details</summary>
Motivation: 现有VLM微调方法（如LoRA）主要依赖显式权重更新，忽略了预训练模型中已编码的丰富表示结构。这些结构未被充分利用，而MFT在语言模型中已被证明是一种强大的后训练范式。

Method: 从结构重参数化角度重新思考VLM微调，将MFT应用于VLM的语言和投影器组件。不更新权重，而是为每个权重分配可学习的门控分数，让模型重新组织其内部子网络以适应下游任务。

Result: 实验表明，MFT在不同语言骨干的VLM上一致超越LoRA变体甚至全微调，在保持冻结骨干不变的情况下实现高性能。在多个基准测试中表现出色。

Conclusion: 有效的适应不仅可以通过更新权重实现，还可以通过重新建立模型现有知识之间的连接来实现。MFT为VLM微调提供了一种高效且性能优越的替代方案。

Abstract: Explorations in fine-tuning Vision-Language Models (VLMs), such as Low-Rank Adaptation (LoRA) from Parameter Efficient Fine-Tuning (PEFT), have made impressive progress. However, most approaches rely on explicit weight updates, overlooking the extensive representational structures already encoded in pre-trained models that remain underutilized. Recent works have demonstrated that Mask Fine-Tuning (MFT) can be a powerful and efficient post-training paradigm for language models. Instead of updating weights, MFT assigns learnable gating scores to each weight, allowing the model to reorganize its internal subnetworks for downstream task adaptation. In this paper, we rethink fine-tuning for VLMs from a structural reparameterization perspective grounded in MFT. We apply MFT to the language and projector components of VLMs with different language backbones and compare against strong PEFT baselines. Experiments show that MFT consistently surpasses LoRA variants and even full fine-tuning, achieving high performance without altering the frozen backbone. Our findings reveal that effective adaptation can emerge not only from updating weights but also from reestablishing connections among the model's existing knowledge. Code available at: https://github.com/Ming-K9/MFT-VLM

</details>


### [108] [Trust Region Masking for Long-Horizon LLM Reinforcement Learning](https://arxiv.org/abs/2512.23075)
*Yingru Li,Jiacai Liu,Jiawei Xu,Yuxuan Tong,Ziniu Li,Baoxiang Wang*

Main category: cs.LG

TL;DR: 论文提出了Trust Region Masking (TRM)方法，通过排除违反信任区域的整个序列来解决长序列LLM强化学习中的离策略近似误差问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的策略梯度方法使用rollout策略的样本来优化代理目标，当rollout策略与目标策略不同时会产生离策略不匹配误差。传统信任区域边界随序列长度T呈O(T²)增长，对于长序列任务变得无效，需要更紧的边界。

Method: 推导了两个更紧的边界：Pinsker-Marginal边界(O(T^{3/2}))和Mixed边界(O(T))，两者都依赖于序列级最大token级KL散度。提出了Trust Region Masking (TRM)方法，如果序列中任何token违反信任区域，就排除整个序列的梯度计算。

Result: TRM为长序列LLM强化学习提供了第一个非空单调改进保证，解决了传统方法在长序列任务中边界无效的问题。

Conclusion: 通过控制序列级最大token级KL散度，TRM方法能够有效管理离策略近似误差，为长序列LLM强化学习提供了理论保证和实用解决方案。

Abstract: Policy gradient methods for large language models optimize a surrogate objective computed from samples of a rollout policy $π_{\text{roll}}$. When $π_{\text{roll}} \ne π_θ$, there is approximation error between the surrogate and the true objective. Prior work has shown that this off-policy mismatch is unavoidable in modern LLM-RL due to implementation divergence, mixture-of-experts routing discontinuities, and distributed training staleness. Classical trust region bounds on the resulting error scale as $O(T^2)$ with sequence length $T$, rendering them vacuous for long-horizon tasks. We derive two tighter bounds: a Pinsker-Marginal bound scaling as $O(T^{3/2})$ and a Mixed bound scaling as $O(T)$. Crucially, both bounds depend on $D_{kl}^{tok,max}$ -- the maximum token-level KL divergence across all positions in a sequence. This is inherently a sequence-level quantity: it requires examining the entire trajectory to compute, and therefore cannot be controlled by token-independent methods like PPO clipping. We propose Trust Region Masking (TRM), which excludes entire sequences from gradient computation if any token violates the trust region, providing the first non-vacuous monotonic improvement guarantees for long-horizon LLM-RL.

</details>


### [109] [Multimodal Functional Maximum Correlation for Emotion Recognition](https://arxiv.org/abs/2512.23076)
*Deyang Zheng,Tianyi Zhang,Wenming Zheng,Shujian Yu*

Main category: cs.LG

TL;DR: 提出MFMC框架，通过双总相关目标最大化高阶多模态依赖，避免成对对比损失，在情感计算任务中实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 情感状态表现为中枢和自主系统的协调但异质生理反应，传统自监督学习方法依赖成对对齐目标，无法捕捉多模态间的高阶交互

Method: 提出多模态功能最大相关(MFMC)框架，使用双总相关(DTC)目标最大化高阶多模态依赖，通过功能最大相关分析(FMCA)的迹代理优化紧致三明治界

Result: 在三个公共情感计算基准测试中，MFMC在受试者依赖和独立评估协议下均达到SOTA或竞争性能，CEAP-360VR上受试者依赖准确率从78.9%提升至86.8%，仅使用EDA信号时受试者独立准确率从27.5%提升至33.1%

Conclusion: MFMC通过直接捕捉联合多模态交互，有效解决了情感计算中多模态表示学习的挑战，对受试者间变异性具有鲁棒性

Abstract: Emotional states manifest as coordinated yet heterogeneous physiological responses across central and autonomic systems, posing a fundamental challenge for multimodal representation learning in affective computing. Learning such joint dynamics is further complicated by the scarcity and subjectivity of affective annotations, which motivates the use of self-supervised learning (SSL). However, most existing SSL approaches rely on pairwise alignment objectives, which are insufficient to characterize dependencies among more than two modalities and fail to capture higher-order interactions arising from coordinated brain and autonomic responses.
  To address this limitation, we propose Multimodal Functional Maximum Correlation (MFMC), a principled SSL framework that maximizes higher-order multimodal dependence through a Dual Total Correlation (DTC) objective. By deriving a tight sandwich bound and optimizing it using a functional maximum correlation analysis (FMCA) based trace surrogate, MFMC captures joint multimodal interactions directly, without relying on pairwise contrastive losses.
  Experiments on three public affective computing benchmarks demonstrate that MFMC consistently achieves state-of-the-art or competitive performance under both subject-dependent and subject-independent evaluation protocols, highlighting its robustness to inter-subject variability. In particular, MFMC improves subject-dependent accuracy on CEAP-360VR from 78.9% to 86.8%, and subject-independent accuracy from 27.5% to 33.1% using the EDA signal alone. Moreover, MFMC remains within 0.8 percentage points of the best-performing method on the most challenging EEG subject-independent split of MAHNOB-HCI. Our code is available at https://github.com/DY9910/MFMC.

</details>


### [110] [Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning](https://arxiv.org/abs/2512.23087)
*Yingru Li,Jiawei Xu,Jiacai Liu,Yuxuan Tong,Ziniu Li,Tianle Cai,Ge Zhang,Qian Liu,Baoxiang Wang*

Main category: cs.LG

TL;DR: 论文提出通过动态剪枝词汇表来缓解LLM强化学习中训练-推理不匹配问题，用有界优化偏差替代系统性失配，实现稳定训练。


<details>
  <summary>Details</summary>
Motivation: LLM强化学习面临训练-推理不匹配问题：高吞吐量推理引擎和数值精确训练系统从相同参数产生不同的概率分布，这种不匹配对低概率词元影响更大，导致梯度估计不稳定。

Method: 提出将RL目标约束到动态剪枝的"安全"词汇表，排除极端尾部词元，用有界优化偏差替代系统性失配。

Result: 理论上证明了词汇剪枝引入的优化偏差有界，实验上实现了稳定训练。

Conclusion: 通过动态词汇剪枝解决训练-推理不匹配问题，用可控优化偏差替代系统性失配，为LLM强化学习提供了稳定训练方案。

Abstract: Reinforcement learning for large language models (LLMs) faces a fundamental tension: high-throughput inference engines and numerically-precise training systems produce different probability distributions from the same parameters, creating a training-inference mismatch. We prove this mismatch has an asymmetric effect: the bound on log-probability mismatch scales as $(1-p)$ where $p$ is the token probability. For high-probability tokens, this bound vanishes, contributing negligibly to sequence-level mismatch. For low-probability tokens in the tail, the bound remains large, and moreover, when sampled, these tokens exhibit systematically biased mismatches that accumulate over sequences, destabilizing gradient estimation. Rather than applying post-hoc corrections, we propose constraining the RL objective to a dynamically-pruned ``safe'' vocabulary that excludes the extreme tail. By pruning such tokens, we trade large, systematically biased mismatches for a small, bounded optimization bias. Empirically, our method achieves stable training; theoretically, we bound the optimization bias introduced by vocabulary pruning.

</details>


### [111] [Osmotic Learning: A Self-Supervised Paradigm for Decentralized Contextual Data Representation](https://arxiv.org/abs/2512.23096)
*Mario Colosi,Reza Farahani,Maria Fazio,Radu Prodan,Massimo Villari*

Main category: cs.LG

TL;DR: OSM-L是一种自监督分布式学习范式，通过渗透过程从分布式数据中提取高层次潜在知识，无需原始数据交换，实现本地表示对齐和信息扩散。


<details>
  <summary>Details</summary>
Motivation: 在分布式系统中，相互依赖的数据源能揭示隐藏关系和潜在结构，这些信息对许多应用具有重要价值。然而，传统方法需要原始数据交换，存在隐私和效率问题。

Method: 提出渗透学习(OSM-L)，核心是渗透过程：通过提取上下文信息合成密集紧凑的表示，无需原始数据交换。迭代对齐本地数据表示，实现信息扩散和收敛到动态平衡，同时识别相关数据组作为去中心化聚类机制。

Result: 在结构化数据集上验证了OSM-L的收敛性和表示能力，本地信息对齐准确率超过0.99，同时保持了上下文完整性。

Conclusion: OSM-L成功实现了从分布式数据中提取高层次潜在知识的目标，通过渗透过程有效对齐本地表示并发现数据相关性，为分布式学习提供了新的自监督范式。

Abstract: Data within a specific context gains deeper significance beyond its isolated interpretation. In distributed systems, interdependent data sources reveal hidden relationships and latent structures, representing valuable information for many applications. This paper introduces Osmotic Learning (OSM-L), a self-supervised distributed learning paradigm designed to uncover higher-level latent knowledge from distributed data. The core of OSM-L is osmosis, a process that synthesizes dense and compact representation by extracting contextual information, eliminating the need for raw data exchange between distributed entities. OSM-L iteratively aligns local data representations, enabling information diffusion and convergence into a dynamic equilibrium that captures contextual patterns. During training, it also identifies correlated data groups, functioning as a decentralized clustering mechanism. Experimental results confirm OSM-L's convergence and representation capabilities on structured datasets, achieving over 0.99 accuracy in local information alignment while preserving contextual integrity.

</details>


### [112] [A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms](https://arxiv.org/abs/2512.23097)
*Yingru Li,Ziniu Li,Jiacai Liu*

Main category: cs.LG

TL;DR: 提出统一的LLM微调框架，结合模仿学习和强化学习，通过梯度分解实现高效优化


<details>
  <summary>Details</summary>
Motivation: 当前LLM微调方法中，模仿学习和强化学习通常分开处理，缺乏统一的优化框架，难以同时实现token级模仿和长时程奖励优化

Method: 通过分析结合轨迹级KL散度和任务奖励的复合目标函数的梯度，将其分解为两部分：1）可解析计算的密集梯度（用于token级模仿），2）蒙特卡洛估计的稀疏梯度（用于长时程奖励优化）。密集梯度具有闭式logit级公式，支持高效GPU实现

Result: 提出了统一的微调框架，能够同时处理模仿学习和强化学习目标，其中密集梯度部分可高效计算，稀疏梯度部分通过采样估计

Conclusion: 该框架为LLM微调提供了统一的优化视角，将模仿学习和强化学习自然地整合到同一梯度分解框架中，有望提高微调效率和效果

Abstract: We present a unified framework for Large Language Model (LLM) fine-tuning that integrates Imitation Learning and Reinforcement Learning. By analyzing the gradient of a composite objective combining trajectory-level KL divergence with task rewards, we derive a natural decomposition into two components: (1) an analytically computable Dense Gradient for token-level imitation, and (2) a Monte Carlo estimated Sparse Gradient for long-horizon reward optimization. The Dense Gradient admits a closed-form logit-level formula, enabling efficient GPU implementation.

</details>


### [113] [How Much Data Is Enough? Uniform Convergence Bounds for Generative & Vision-Language Models under Low-Dimensional Structure](https://arxiv.org/abs/2512.23109)
*Paul M. Thompson*

Main category: cs.LG

TL;DR: 本文研究生成模型和视觉语言模型在生物医学应用中如何实现均匀准确的预测和校准，提出了基于低维语义表示的有限样本收敛保证。


<details>
  <summary>Details</summary>
Motivation: 在生物医学决策支持中，生成模型和视觉语言模型需要提供准确且校准良好的概率预测。尽管在中等数据量下表现良好，但尚不清楚这些预测是否能在输入、类别或亚群中均匀泛化，而不是仅在平均意义上有效。这在生物医学领域尤为关键，因为罕见情况和特定群体即使在总体损失较低时也可能出现较大误差。

Method: 研究从有限样本角度出发，关注通过改变提示或语义嵌入在受限表示空间中获得的分类器族。假设模型输出依赖于低维语义表示的平滑变化（这一假设得到文本和图像-文本嵌入中谱结构的支持），应用经典均匀收敛工具获得有意义的非渐近保证。

Result: 主要结果为VLM诱导分类器的准确性和校准泛函提供了有限样本均匀收敛界，这些界基于提示嵌入的Lipschitz稳定性。样本复杂度取决于内在/有效维度而非环境嵌入维度，并进一步推导了谱依赖界，明确展示特征值衰减如何控制数据需求。

Conclusion: 研究对数据有限的生物医学建模具有重要启示：阐明了当前数据集大小何时能支持均匀可靠的预测，以及为什么平均校准指标可能遗漏最坏情况下的校准错误。为在生物医学应用中实现可靠预测提供了理论框架。

Abstract: Modern generative and vision-language models (VLMs) are increasingly used in scientific and medical decision support, where predicted probabilities must be both accurate and well calibrated. Despite strong empirical results with moderate data, it remains unclear when such predictions generalize uniformly across inputs, classes, or subpopulations, rather than only on average-a critical issue in biomedicine, where rare conditions and specific groups can exhibit large errors even when overall loss is low.
  We study this question from a finite-sample perspective and ask: under what structural assumptions can generative and VLM-based predictors achieve uniformly accurate and calibrated behavior with practical sample sizes? Rather than analyzing arbitrary parameterizations, we focus on induced families of classifiers obtained by varying prompts or semantic embeddings within a restricted representation space. When model outputs depend smoothly on a low-dimensional semantic representation-an assumption supported by spectral structure in text and joint image-text embeddings-classical uniform convergence tools yield meaningful non-asymptotic guarantees.
  Our main results give finite-sample uniform convergence bounds for accuracy and calibration functionals of VLM-induced classifiers under Lipschitz stability with respect to prompt embeddings. The implied sample complexity depends on intrinsic/effective dimension, not ambient embedding dimension, and we further derive spectrum-dependent bounds that make explicit how eigenvalue decay governs data requirements. We conclude with implications for data-limited biomedical modeling, including when current dataset sizes can support uniformly reliable predictions and why average calibration metrics may miss worst-case miscalibration.

</details>


### [114] [SE-MLP Model for Predicting Prior Acceleration Features in Penetration Signals](https://arxiv.org/abs/2512.23131)
*Yankang Li,Changsheng Li*

Main category: cs.LG

TL;DR: 本文提出了一种结合通道注意力机制和残差连接的多层感知器架构（SE-MLP），用于快速预测侵彻加速度特征值，替代传统耗时的仿真计算。


<details>
  <summary>Details</summary>
Motivation: 侵彻过程的准确识别依赖于侵彻加速度的先验特征值，但这些特征值通常需要通过长时间仿真和昂贵计算获得，限制了实际应用效率。

Method: 提出SE-MLP架构，集成通道注意力机制和残差连接，以不同工况下的物理参数为输入，输出层间加速度特征，建立物理参数与侵彻特性间的非线性映射。

Result: 与常规MLP、XGBoost和Transformer模型相比，SE-MLP在预测精度、泛化能力和稳定性方面表现最优；消融实验证实通道注意力模块和残差结构对性能提升均有显著贡献。

Conclusion: 数值仿真和射程恢复测试表明，预测与实测的加速度峰值和脉冲宽度差异在工程允许范围内，验证了方法的可行性和工程适用性，为侵彻引信快速生成先验特征值提供了实用基础。

Abstract: Accurate identification of the penetration process relies heavily on prior feature values of penetration acceleration. However, these feature values are typically obtained through long simulation cycles and expensive computations. To overcome this limitation, this paper proposes a multi-layer Perceptron architecture, termed squeeze and excitation multi-layer perceptron (SE-MLP), which integrates a channel attention mechanism with residual connections to enable rapid prediction of acceleration feature values. Using physical parameters under different working conditions as inputs, the model outputs layer-wise acceleration features, thereby establishing a nonlinear mapping between physical parameters and penetration characteristics. Comparative experiments against conventional MLP, XGBoost, and Transformer models demonstrate that SE-MLP achieves superior prediction accuracy, generalization, and stability. Ablation studies further confirm that both the channel attention module and residual structure contribute significantly to performance gains. Numerical simulations and range recovery tests show that the discrepancies between predicted and measured acceleration peaks and pulse widths remain within acceptable engineering tolerances. These results validate the feasibility and engineering applicability of the proposed method and provide a practical basis for rapidly generating prior feature values for penetration fuzes.

</details>


### [115] [Principled Algorithms for Optimizing Generalized Metrics in Binary Classification](https://arxiv.org/abs/2512.23133)
*Anqi Mao,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: 提出METRO算法，用于优化类别不平衡或代价不对称场景下的广义度量指标（如Fβ分数、Jaccard系数等），通过将度量优化转化为广义代价敏感学习问题，设计具有理论保证的代理损失函数。


<details>
  <summary>Details</summary>
Motivation: 在类别不平衡或代价不对称的应用中，传统二元分类损失不适用，而Fβ分数、AM度量、Jaccard相似系数等广义度量更合适。现有方法通常基于贝叶斯最优分类器，使用阈值方法先估计类别概率再寻找最优阈值，导致算法不适用于受限假设集且缺乏有限样本性能保证。

Method: 将度量优化重新表述为广义代价敏感学习问题，设计具有H-一致性保证的新型代理损失函数。基于此框架开发METRO（Metric Optimization）算法，提供理论性能保证。

Result: 实验结果表明，所提方法相比现有基线方法具有更好的效果。算法具有H-一致性和有限样本泛化界理论保证。

Conclusion: 提出了一种理论上有保证的广义度量优化方法，通过将度量优化转化为代价敏感学习问题，设计了具有H-一致性保证的代理损失函数，开发了METRO算法并在实验中验证了其有效性。

Abstract: In applications with significant class imbalance or asymmetric costs, metrics such as the $F_β$-measure, AM measure, Jaccard similarity coefficient, and weighted accuracy offer more suitable evaluation criteria than standard binary classification loss. However, optimizing these metrics present significant computational and statistical challenges. Existing approaches often rely on the characterization of the Bayes-optimal classifier, and use threshold-based methods that first estimate class probabilities and then seek an optimal threshold. This leads to algorithms that are not tailored to restricted hypothesis sets and lack finite-sample performance guarantees. In this work, we introduce principled algorithms for optimizing generalized metrics, supported by $H$-consistency and finite-sample generalization bounds. Our approach reformulates metric optimization as a generalized cost-sensitive learning problem, enabling the design of novel surrogate loss functions with provable $H$-consistency guarantees. Leveraging this framework, we develop new algorithms, METRO (Metric Optimization), with strong theoretical performance guarantees. We report the results of experiments demonstrating the effectiveness of our methods compared to prior baselines.

</details>


### [116] [Graph Neural Networks with Transformer Fusion of Brain Connectivity Dynamics and Tabular Data for Forecasting Future Tobacco Use](https://arxiv.org/abs/2512.23137)
*Runzhi Zhou,Xi Luo*

Main category: cs.LG

TL;DR: 提出GNN-TF模型，整合非欧几里得脑成像数据与欧几里得表格数据，用于预测未来烟草使用，在纵向fMRI研究中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医学影像分析中，整合非欧几里得脑成像数据与欧几里得表格数据（如临床和人口统计学信息）具有挑战性，特别是在预测未来结果方面。现有方法在纵向成像研究中有效预测结果仍面临困难。

Method: 提出时间感知图神经网络与Transformer融合模型（GNN-TF），灵活整合表格数据和动态脑连接数据，利用变量的时间顺序。使用NCANDA纵向静息态fMRI数据集，整合非欧几里得和欧几里得信息源。

Result: 与多种现有机器学习和深度学习模型比较，GNN-TF表现优于这些最先进方法，在预测未来烟草使用方面提供更优的预测准确性。

Conclusion: GNN-TF的端到端、时间感知Transformer融合结构成功整合多模态数据并利用时间动态，成为功能脑成像研究中临床结果预测的有价值分析工具。

Abstract: Integrating non-Euclidean brain imaging data with Euclidean tabular data, such as clinical and demographic information, poses a substantial challenge for medical imaging analysis, particularly in forecasting future outcomes. While machine learning and deep learning techniques have been applied successfully to cross-sectional classification and prediction tasks, effectively forecasting outcomes in longitudinal imaging studies remains challenging. To address this challenge, we introduce a time-aware graph neural network model with transformer fusion (GNN-TF). This model flexibly integrates both tabular data and dynamic brain connectivity data, leveraging the temporal order of these variables within a coherent framework. By incorporating non-Euclidean and Euclidean sources of information from a longitudinal resting-state fMRI dataset from the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA), the GNN-TF enables a comprehensive analysis that captures critical aspects of longitudinal imaging data. Comparative analyses against a variety of established machine learning and deep learning models demonstrate that GNN-TF outperforms these state-of-the-art methods, delivering superior predictive accuracy for predicting future tobacco usage. The end-to-end, time-aware transformer fusion structure of the proposed GNN-TF model successfully integrates multiple data modalities and leverages temporal dynamics, making it a valuable analytic tool for functional brain imaging studies focused on clinical outcome prediction.

</details>


### [117] [A Weak Signal Learning Dataset and Its Baseline Method](https://arxiv.org/abs/2512.23160)
*Xianqi Liu,Xiangru Li,Lefeng He,Ziyu Fang*

Main category: cs.LG

TL;DR: 该研究构建了首个弱信号特征学习专用数据集，并提出PDVFN模型处理低信噪比、分布偏斜和双重不平衡问题，为弱信号学习任务提供新解决方案。


<details>
  <summary>Details</summary>
Motivation: 弱信号学习在故障诊断、医学成像、自动驾驶等领域面临挑战，关键信息常被噪声干扰，特征识别困难。然而，缺乏专用数据集长期制约了相关研究。

Method: 构建包含13,158个光谱样本的专用数据集，具有低信噪比主导（超55%样本SNR低于50）和极端类别不平衡（类别比例达29:1）。提出双视图表示（向量+时频图）和PDVFN模型，并行提取局部序列特征和全局频域结构，遵循局部增强、序列建模、噪声抑制、多尺度捕获、频率提取和全局感知原则。

Result: 实验表明该方法在处理弱信号、高噪声和极端类别不平衡时具有更高的准确性和鲁棒性，特别是在低信噪比和不平衡场景下表现优异。

Conclusion: 本研究提供了专用数据集、基线模型，为未来弱信号学习研究奠定了基础，为天文光谱学等WSL任务提供了新颖解决方案。

Abstract: Weak signal learning (WSL) is a common challenge in many fields like fault diagnosis, medical imaging, and autonomous driving, where critical information is often masked by noise and interference, making feature identification difficult. Even in tasks with abundant strong signals, the key to improving model performance often lies in effectively extracting weak signals. However, the lack of dedicated datasets has long constrained research. To address this, we construct the first specialized dataset for weak signal feature learning, containing 13,158 spectral samples. It features low SNR dominance (over 55% samples with SNR below 50) and extreme class imbalance (class ratio up to 29:1), providing a challenging benchmark for classification and regression in weak signal scenarios. We also propose a dual-view representation (vector + time-frequency map) and a PDVFN model tailored to low SNR, distribution skew, and dual imbalance. PDVFN extracts local sequential features and global frequency-domain structures in parallel, following principles of local enhancement, sequential modeling, noise suppression, multi-scale capture, frequency extraction, and global perception. This multi-source complementarity enhances representation for low-SNR and imbalanced data, offering a novel solution for WSL tasks like astronomical spectroscopy. Experiments show our method achieves higher accuracy and robustness in handling weak signals, high noise, and extreme class imbalance, especially in low SNR and imbalanced scenarios. This study provides a dedicated dataset, a baseline model, and establishes a foundation for future WSL research.

</details>


### [118] [Diffusion-based Decentralized Federated Multi-Task Representation Learning](https://arxiv.org/abs/2512.23161)
*Donghwa Kang,Shana Moothedath*

Main category: cs.LG

TL;DR: 提出一种去中心化的多任务表示学习算法，用于数据稀缺环境下的多任务线性回归问题，通过扩散式去中心化和联邦学习方式恢复低秩特征矩阵。


<details>
  <summary>Details</summary>
Motivation: 尽管表示学习在数据稀缺环境中被广泛采用，但去中心化方法仍相对未被充分探索。本研究旨在开发去中心化的多任务表示学习算法，解决多个线性回归模型共享低维线性表示的问题。

Method: 提出基于交替投影梯度下降和最小化的算法，采用扩散式去中心化和联邦学习方式恢复低秩特征矩阵。算法包括投影梯度下降步骤和最小化步骤的交替执行。

Result: 获得了构造性的可证明保证：提供了所需样本复杂度的下界和算法迭代复杂度的上界。分析显示算法具有快速和通信高效的特点，数值模拟验证了算法性能优于基准算法。

Conclusion: 成功开发了一种去中心化的多任务表示学习算法，在数据稀缺环境下能够有效恢复共享的低维线性表示，具有理论保证和实际性能优势。

Abstract: Representation learning is a widely adopted framework for learning in data-scarce environments to obtain a feature extractor or representation from various different yet related tasks. Despite extensive research on representation learning, decentralized approaches remain relatively underexplored. This work develops a decentralized projected gradient descent-based algorithm for multi-task representation learning. We focus on the problem of multi-task linear regression in which multiple linear regression models share a common, low-dimensional linear representation. We present an alternating projected gradient descent and minimization algorithm for recovering a low-rank feature matrix in a diffusion-based decentralized and federated fashion. We obtain constructive, provable guarantees that provide a lower bound on the required sample complexity and an upper bound on the iteration complexity of our proposed algorithm. We analyze the time and communication complexity of our algorithm and show that it is fast and communication-efficient. We performed numerical simulations to validate the performance of our algorithm and compared it with benchmark algorithms.

</details>


### [119] [Evaluating Parameter Efficient Methods for RLVR](https://arxiv.org/abs/2512.23165)
*Qingyu Yin,Yulun Wu,Zhennan Shen,Sunbowen Li,Zhilin Wang,Yanshu Li,Chak Tou Leong,Jiale Kang,Jinjin Gu*

Main category: cs.LG

TL;DR: 本文首次系统评估了12种参数高效微调方法在RLVR范式下的表现，发现结构变体如DoRA、AdaLoRA和MiSS优于标准LoRA，揭示了SVD初始化策略的频谱崩溃问题，并指出极端参数减少会严重限制推理能力。


<details>
  <summary>Details</summary>
Motivation: RLVR通过可验证反馈激励语言模型提升推理能力，但现有研究未确定RLVR下的最优PEFT架构。虽然LoRA等方法常用，但缺乏系统评估来指导参数高效RL方法的选择。

Method: 在DeepSeek-R1-Distill系列模型上，对12种PEFT方法在数学推理基准上进行全面评估，包括结构变体(DoRA、AdaLoRA、MiSS)和SVD初始化策略(PiSSA、MiLoRA)，并进行消融实验和缩放实验验证发现。

Result: 1. 结构变体(DoRA、AdaLoRA、MiSS)一致优于标准LoRA；2. 发现SVD初始化策略存在频谱崩溃现象，主成分更新与RL优化存在根本性不匹配；3. 极端参数减少(如VeRA、Rank-1)严重限制推理能力。

Conclusion: 标准LoRA不应作为RLVR的默认选择，需要更多探索参数高效的RL方法。本文为PEFT在RLVR中的应用提供了明确指导，挑战了现有实践，并揭示了架构选择对推理能力的重要影响。

Abstract: We systematically evaluate Parameter-Efficient Fine-Tuning (PEFT) methods under the paradigm of Reinforcement Learning with Verifiable Rewards (RLVR). RLVR incentivizes language models to enhance their reasoning capabilities through verifiable feedback; however, while methods like LoRA are commonly used, the optimal PEFT architecture for RLVR remains unidentified. In this work, we conduct the first comprehensive evaluation of over 12 PEFT methodologies across the DeepSeek-R1-Distill families on mathematical reasoning benchmarks. Our empirical results challenge the default adoption of standard LoRA with three main findings. First, we demonstrate that structural variants, such as DoRA, AdaLoRA, and MiSS, consistently outperform LoRA. Second, we uncover a spectral collapse phenomenon in SVD-informed initialization strategies (\textit{e.g.,} PiSSA, MiLoRA), attributing their failure to a fundamental misalignment between principal-component updates and RL optimization. Furthermore, our ablations reveal that extreme parameter reduction (\textit{e.g.,} VeRA, Rank-1) severely bottlenecks reasoning capacity. We further conduct ablation studies and scaling experiments to validate our findings. This work provides a definitive guide for advocating for more exploration for parameter-efficient RL methods.

</details>


### [120] [HELM-BERT: A Transformer for Medium-sized Peptide Property Prediction](https://arxiv.org/abs/2512.23175)
*Seungeon Lee,Takuto Koyama,Itsuki Maeda,Shigeyuki Matsumoto,Yasushi Okuno*

Main category: cs.LG

TL;DR: HELM-BERT：首个基于HELM表示法的肽语言模型，在肽性质预测任务上显著优于SMILES模型


<details>
  <summary>Details</summary>
Motivation: 治疗性肽在药物发现中日益重要，但现有分子语言模型无法有效表示肽的化学复杂性。SMILES表示产生长序列且难以处理环状拓扑，氨基酸级表示无法编码化学修饰。HELM表示法能精确描述单体组成和连接性，为肽语言建模提供了理想基础。

Method: 提出HELM-BERT模型，基于DeBERTa架构，专门设计用于捕捉HELM序列中的层次依赖关系。模型在39,079个化学多样性肽（包括线性和环状结构）的语料库上进行预训练。

Result: HELM-BERT在下游任务中显著优于最先进的SMILES语言模型，包括环肽膜渗透性预测和肽-蛋白质相互作用预测。

Conclusion: HELM的显式单体和拓扑感知表示在建模治疗性肽方面具有显著的数据效率优势，填补了小分子和蛋白质语言模型之间的长期空白。

Abstract: Therapeutic peptides have emerged as a pivotal modality in modern drug discovery, occupying a chemically and topologically rich space. While accurate prediction of their physicochemical properties is essential for accelerating peptide development, existing molecular language models rely on representations that fail to capture this complexity. Atom-level SMILES notation generates long token sequences and obscures cyclic topology, whereas amino-acid-level representations cannot encode the diverse chemical modifications central to modern peptide design. To bridge this representational gap, the Hierarchical Editing Language for Macromolecules (HELM) offers a unified framework enabling precise description of both monomer composition and connectivity, making it a promising foundation for peptide language modeling. Here, we propose HELM-BERT, the first encoder-based peptide language model trained on HELM notation. Based on DeBERTa, HELM-BERT is specifically designed to capture hierarchical dependencies within HELM sequences. The model is pre-trained on a curated corpus of 39,079 chemically diverse peptides spanning linear and cyclic structures. HELM-BERT significantly outperforms state-of-the-art SMILES-based language models in downstream tasks, including cyclic peptide membrane permeability prediction and peptide-protein interaction prediction. These results demonstrate that HELM's explicit monomer- and topology-aware representations offer substantial data-efficiency advantages for modeling therapeutic peptides, bridging a long-standing gap between small-molecule and protein language models.

</details>


### [121] [Machine Learning-Assisted Vocal Cord Ultrasound Examination: Project VIPR](https://arxiv.org/abs/2512.23177)
*Will Sebelik-Lassiter,Evan Schubert,Muhammad Alliyu,Quentin Robbins,Excel Olatunji,Mustafa Barry*

Main category: cs.LG

TL;DR: 开发机器学习算法自动识别声带并区分正常声带与声带麻痹的超声图像


<details>
  <summary>Details</summary>
Motivation: 声带超声检查虽然创伤小、耐受性好，但其准确性高度依赖操作者经验，需要更客观、自动化的分析方法来提高诊断准确性。

Method: 收集30名志愿者的声带超声视频，分割为静态帧并统一裁剪尺寸。使用正常和模拟声带麻痹图像训练声带分割模型和声带麻痹分类模型。

Result: 声带分割模型验证准确率达96%，最佳分类模型(VIPRnet)验证准确率达99%，表现优异。

Conclusion: 机器学习辅助的声带超声分析在提高诊断准确性方面具有巨大潜力，有望超越依赖操作者经验的人工判读。

Abstract: Intro: Vocal cord ultrasound (VCUS) has emerged as a less invasive and better tolerated examination technique, but its accuracy is operator dependent. This research aims to apply a machine learning-assisted algorithm to automatically identify the vocal cords and distinguish normal vocal cord images from vocal cord paralysis (VCP). Methods: VCUS videos were acquired from 30 volunteers, which were split into still frames and cropped to a uniform size. Healthy and simulated VCP images were used as training data for vocal cord segmentation and VCP classification models. Results: The vocal cord segmentation model achieved a validation accuracy of 96%, while the best classification model (VIPRnet) achieved a validation accuracy of 99%. Conclusion: Machine learning-assisted analysis of VCUS shows great promise in improving diagnostic accuracy over operator-dependent human interpretation.

</details>


### [122] [A Simple, Optimal and Efficient Algorithm for Online Exp-Concave Optimization](https://arxiv.org/abs/2512.23190)
*Yi-Han Wang,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: LightONS算法通过延迟昂贵的马氏投影，将OXO问题的总运行时间从Õ(d^ωT)降低到O(d²T + d^ω√T log T)，同时保持最优O(d log T)遗憾界，并解决了COLT'13关于SXO算法运行时间的开放问题。


<details>
  <summary>Details</summary>
Motivation: 在线指数凹优化(OXO)中的标准算法ONS存在计算瓶颈，每轮的马氏投影需要Ω(d^ω)次算术运算，导致总运行时间高达Õ(d^ωT)。对于随机指数凹优化(SXO)，使用在线到批处理转换的ONS需要Õ(d^{ω+1}/ε)运行时间。COLT'13的开放问题要求找到运行时间小于Õ(d^{ω+1}/ε)的SXO算法。

Method: 提出LightONS算法，这是ONS的简单变体。利用参数无关在线学习中的域转换技术，引入滞后机制，延迟昂贵的马氏投影直到必要时才执行。保留了ONS的优雅结构，但显著减少了计算开销。

Result: LightONS将总运行时间减少到O(d²T + d^ω√T log T)，同时保持最优O(d log T)遗憾界。对于SXO问题，运行时间降低到Õ(d³/ε)，解决了COLT'13的开放问题。算法还能作为ONS的高效替代方案应用于更广泛场景。

Conclusion: LightONS通过创新的滞后机制成功解决了OXO和SXO的计算效率问题，在保持统计最优性的同时显著降低了计算复杂度，为相关领域提供了高效实用的算法解决方案。

Abstract: Online eXp-concave Optimization (OXO) is a fundamental problem in online learning. The standard algorithm, Online Newton Step (ONS), balances statistical optimality and computational practicality, guaranteeing an optimal regret of $O(d \log T)$, where $d$ is the dimension and $T$ is the time horizon. ONS faces a computational bottleneck due to the Mahalanobis projections at each round. This step costs $Ω(d^ω)$ arithmetic operations for bounded domains, even for the unit ball, where $ω\in (2,3]$ is the matrix-multiplication exponent. As a result, the total runtime can reach $\tilde{O}(d^ωT)$, particularly when iterates frequently oscillate near the domain boundary. For Stochastic eXp-concave Optimization (SXO), computational cost is also a challenge. Deploying ONS with online-to-batch conversion for SXO requires $T = \tilde{O}(d/ε)$ rounds to achieve an excess risk of $ε$, and thereby necessitates an $\tilde{O}(d^{ω+1}/ε)$ runtime. A COLT'13 open problem posed by Koren [2013] asks for an SXO algorithm with runtime less than $\tilde{O}(d^{ω+1}/ε)$.
  This paper proposes a simple variant of ONS, LightONS, which reduces the total runtime to $O(d^2 T + d^ω\sqrt{T \log T})$ while preserving the optimal $O(d \log T)$ regret. LightONS implies an SXO method with runtime $\tilde{O}(d^3/ε)$, thereby answering the open problem. Importantly, LightONS preserves the elegant structure of ONS by leveraging domain-conversion techniques from parameter-free online learning to introduce a hysteresis mechanism that delays expensive Mahalanobis projections until necessary. This design enables LightONS to serve as an efficient plug-in replacement of ONS in broader scenarios, even beyond regret minimization, including gradient-norm adaptive regret, parametric stochastic bandits, and memory-efficient online learning.

</details>


### [123] [PGOT: A Physics-Geometry Operator Transformer for Complex PDEs](https://arxiv.org/abs/2512.23192)
*Zhuo Zhang,Xi Yang,Yuan Zhao,Canqun Yang*

Main category: cs.LG

TL;DR: PGOT提出了一种物理-几何算子Transformer，通过谱保持几何注意力机制解决大规模非结构化网格建模中的几何混叠问题，实现空间自适应的高精度物理场建模。


<details>
  <summary>Details</summary>
Motivation: Transformer在建模偏微分方程方面表现出巨大潜力，但在处理具有复杂几何形状的大规模非结构化网格时面临挑战。现有高效架构采用特征降维策略会引发几何混叠，导致关键物理边界信息丢失。

Method: 提出物理-几何算子Transformer(PGOT)，包含谱保持几何注意力模块(SpecGeo-Attention)，采用"物理切片-几何注入"机制，融入多尺度几何编码以显式保留几何特征。同时根据空间坐标动态路由计算到低阶线性路径(平滑区域)和高阶非线性路径(激波和不连续区域)。

Result: PGOT在四个标准基准测试中取得一致的SOTA性能，并在机翼和汽车设计等大规模工业任务中表现出色。模块保持线性计算复杂度O(N)。

Conclusion: PGOT通过显式几何感知重建物理特征学习，解决了几何混叠问题，实现了空间自适应的高精度物理场建模，在科学计算和工业应用中具有重要价值。

Abstract: While Transformers have demonstrated remarkable potential in modeling Partial Differential Equations (PDEs), modeling large-scale unstructured meshes with complex geometries remains a significant challenge. Existing efficient architectures often employ feature dimensionality reduction strategies, which inadvertently induces Geometric Aliasing, resulting in the loss of critical physical boundary information. To address this, we propose the Physics-Geometry Operator Transformer (PGOT), designed to reconstruct physical feature learning through explicit geometry awareness. Specifically, we propose Spectrum-Preserving Geometric Attention (SpecGeo-Attention). Utilizing a ``physics slicing-geometry injection" mechanism, this module incorporates multi-scale geometric encodings to explicitly preserve multi-scale geometric features while maintaining linear computational complexity $O(N)$. Furthermore, PGOT dynamically routes computations to low-order linear paths for smooth regions and high-order non-linear paths for shock waves and discontinuities based on spatial coordinates, enabling spatially adaptive and high-precision physical field modeling. PGOT achieves consistent state-of-the-art performance across four standard benchmarks and excels in large-scale industrial tasks including airfoil and car designs.

</details>


### [124] [Energy and Memory-Efficient Federated Learning With Ordered Layer Freezing](https://arxiv.org/abs/2512.23200)
*Ziru Niu,Hai Dong,A. K. Qin,Tao Gu,Pengcheng Zhang*

Main category: cs.LG

TL;DR: FedOLF通过有序层冻结和Tensor操作近似技术，在联邦学习中显著降低计算、内存和通信开销，同时保持甚至提高模型精度。


<details>
  <summary>Details</summary>
Motivation: 物联网边缘设备的计算能力、内存和带宽有限，现有联邦学习框架通过dropout或层冻结减少开销，但往往牺牲精度或忽略内存限制。

Method: 提出FedOLF：1) 有序层冻结：在训练前按预定顺序冻结层，减少计算和内存需求；2) Tensor操作近似：轻量级替代传统量化，更好保持模型精度。

Result: 在非独立同分布数据上，FedOLF在多个数据集和模型上比现有方法精度提高0.3%-6.4%，同时具有更高能效和更低内存占用。

Conclusion: FedOLF有效解决了物联网边缘设备在联邦学习中的资源限制问题，在保持精度的同时显著降低了计算、内存和通信开销。

Abstract: Federated Learning (FL) has emerged as a privacy-preserving paradigm for training machine learning models across distributed edge devices in the Internet of Things (IoT). By keeping data local and coordinating model training through a central server, FL effectively addresses privacy concerns and reduces communication overhead. However, the limited computational power, memory, and bandwidth of IoT edge devices pose significant challenges to the efficiency and scalability of FL, especially when training deep neural networks. Various FL frameworks have been proposed to reduce computation and communication overheads through dropout or layer freezing. However, these approaches often sacrifice accuracy or neglect memory constraints. To this end, in this work, we introduce Federated Learning with Ordered Layer Freezing (FedOLF). FedOLF consistently freezes layers in a predefined order before training, significantly mitigating computation and memory requirements. To further reduce communication and energy costs, we incorporate Tensor Operation Approximation (TOA), a lightweight alternative to conventional quantization that better preserves model accuracy. Experimental results demonstrate that over non-iid data, FedOLF achieves at least 0.3%, 6.4%, 5.81%, 4.4%, 6.27% and 1.29% higher accuracy than existing works respectively on EMNIST (with CNN), CIFAR-10 (with AlexNet), CIFAR-100 (with ResNet20 and ResNet44), and CINIC-10 (with ResNet20 and ResNet44), along with higher energy efficiency and lower memory footprint.

</details>


### [125] [FairGFL: Privacy-Preserving Fairness-Aware Federated Learning with Overlapping Subgraphs](https://arxiv.org/abs/2512.23235)
*Zihao Zhou,Shusen Yang,Fangyuan Zhao,Xuebin Ren*

Main category: cs.LG

TL;DR: FairGFL：解决图联邦学习中重叠子图不平衡导致的公平性问题，通过隐私保护的重叠率估计和加权聚合提升跨客户端公平性


<details>
  <summary>Details</summary>
Motivation: 图联邦学习中不同客户端子图存在重叠，先前研究关注重叠对数据异质性的缓解作用，但未探索不平衡重叠带来的负面影响，特别是跨客户端公平性问题

Method: 提出FairGFL算法：1）通过隐私保护方法估计客户端重叠率；2）采用可解释的加权聚合方法提升跨客户端公平性；3）在联邦复合损失函数中集成精心设计的正则化器，平衡模型效用与公平性

Result: 在四个基准图数据集上的实验表明，FairGFL在模型效用和公平性方面均优于四个代表性基线算法

Conclusion: 图联邦学习中不平衡重叠子图会导致公平性问题，FairGFL通过隐私保护的加权聚合和正则化方法有效解决了这一问题，在保持模型效用的同时显著提升跨客户端公平性

Abstract: Graph federated learning enables the collaborative extraction of high-order information from distributed subgraphs while preserving the privacy of raw data. However, graph data often exhibits overlap among different clients. Previous research has demonstrated certain benefits of overlapping data in mitigating data heterogeneity. However, the negative effects have not been explored, particularly in cases where the overlaps are imbalanced across clients. In this paper, we uncover the unfairness issue arising from imbalanced overlapping subgraphs through both empirical observations and theoretical reasoning. To address this issue, we propose FairGFL (FAIRness-aware subGraph Federated Learning), a novel algorithm that enhances cross-client fairness while maintaining model utility in a privacy-preserving manner. Specifically, FairGFL incorporates an interpretable weighted aggregation approach to enhance fairness across clients, leveraging privacy-preserving estimation of their overlapping ratios. Furthermore, FairGFL improves the tradeoff between model utility and fairness by integrating a carefully crafted regularizer into the federated composite loss function. Through extensive experiments on four benchmark graph datasets, we demonstrate that FairGFL outperforms four representative baseline algorithms in terms of both model utility and fairness.

</details>


### [126] [KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta](https://arxiv.org/abs/2512.23236)
*Gang Liao,Hongsen Qin,Ying Wang,Alicia Golden,Michael Kuchnik,Yavuz Yetim,Jia Jiunn Ang,Chunli Fu,Yihan He,Samuel Hsia,Zewei Jiang,Dianshi Li,Uladzimir Pashkevich,Varna Puvvada,Feng Shi,Matt Steiner,Ruichao Xiao,Nathan Yan,Xiayu Yu,Zhou Fang,Abdul Zainul-Abedin,Ketan Singh,Hongtao Yu,Wenyuan Chi,Barney Huang,Sean Zhang,Noah Weller,Zach Marine,Wyatt Cook,Carole-Jean Wu,Gaoxiang Liu*

Main category: cs.LG

TL;DR: KernelEvolve是一个自动化的内核编码框架，通过图搜索和检索增强提示合成技术，为推荐模型在异构硬件上自动生成和优化内核，显著提升性能并降低开发时间。


<details>
  <summary>Details</summary>
Motivation: 深度学习推荐模型训练和推理需要高效快速，但面临模型架构多样性、内核原语多样性以及硬件异构性三大系统挑战，需要自动化解决方案来应对大规模异构环境。

Method: KernelEvolve采用多级编程抽象（从Triton/CuTe DSL到底层硬件无关语言），通过图搜索算法结合选择策略、通用算子、适应度函数和终止规则，利用检索增强提示合成动态适应运行时执行环境。

Result: 在KernelBench测试套件上实现100%通过率（250个问题），在三个异构硬件平台上验证160个PyTorch ATen算子100%正确性，将开发时间从数周缩短到数小时，并在实际生产用例中显著超越PyTorch基线性能。

Conclusion: KernelEvolve成功解决了DLRM在异构硬件上的内核优化挑战，不仅大幅提升性能效率，还通过自动化内核生成为新型AI硬件降低了编程门槛，实现了大规模异构AI系统的有效部署。

Abstract: Making deep learning recommendation model (DLRM) training and inference fast and efficient is important. However, this presents three key system challenges - model architecture diversity, kernel primitive diversity, and hardware generation and architecture heterogeneity. This paper presents KernelEvolve-an agentic kernel coding framework-to tackle heterogeneity at-scale for DLRM. KernelEvolve is designed to take kernel specifications as input and automate the process of kernel generation and optimization for recommendation model across heterogeneous hardware architectures. KernelEvolve does so by operating at multiple programming abstractions, from Triton and CuTe DSL to low-level hardware agnostic languages, spanning the full hardware-software optimization stack. The kernel optimization process is described as graph-based search with selection policy, universal operator, fitness function, and termination rule, dynamically adapts to runtime execution context through retrieval-augmented prompt synthesis. We designed, implemented, and deployed KernelEvolve to optimize a wide variety of production recommendation models across generations of NVIDIA and AMD GPUs, as well as Meta's AI accelerators. We validate KernelEvolve on the publicly-available KernelBench suite, achieving 100% pass rate on all 250 problems across three difficulty levels, and 160 PyTorch ATen operators across three heterogeneous hardware platforms, demonstrating 100% correctness. KernelEvolve reduces development time from weeks to hours and achieves substantial performance improvements over PyTorch baselines across diverse production use cases and for heterogeneous AI systems at-scale. Beyond performance efficiency improvements, KernelEvolve significantly mitigates the programmability barrier for new AI hardware by enabling automated kernel generation for in-house developed AI hardware.

</details>


### [127] [PFed-Signal: An ADR Prediction Model based on Federated Learning](https://arxiv.org/abs/2512.23262)
*Tao Li,Peilin Li,Kui Lu,Yilei Wang,Junliang Shang,Guangshun Li,Huiyu Zhou*

Main category: cs.LG

TL;DR: PFed-signal：基于联邦学习的ADR信号预测模型，通过欧氏距离消除FAERS中的偏差数据，提高ADR预测准确性


<details>
  <summary>Details</summary>
Motivation: 基于FAERS的偏差记录预测药物不良反应可能误导在线诊断。传统方法依赖统计方法优化ROR或PRR，但无法消除偏差数据，导致信号预测不准确。

Method: 提出PFed-signal：1）Pfed-Split方法按ADR分割原始数据集；2）ADR-signal模型包括：基于联邦学习的偏差数据识别方法（用欧氏距离识别偏差数据并生成干净数据集）和基于Transformer的ADR预测模型（在干净数据集上训练）。

Result: 干净数据集上的ROR和PRR优于传统方法。PFed-signal的准确率0.887、F1分数0.890、召回率0.913、AUC 0.957，均优于基线方法。

Conclusion: PFed-signal通过联邦学习和欧氏距离有效消除FAERS中的偏差数据，显著提高了ADR信号预测的准确性。

Abstract: The adverse drug reactions (ADRs) predicted based on the biased records in FAERS (U.S. Food and Drug Administration Adverse Event Reporting System) may mislead diagnosis online. Generally, such problems are solved by optimizing reporting odds ratio (ROR) or proportional reporting ratio (PRR). However, these methods that rely on statistical methods cannot eliminate the biased data, leading to inaccurate signal prediction. In this paper, we propose PFed-signal, a federated learning-based signal prediction model of ADR, which utilizes the Euclidean distance to eliminate the biased data from FAERS, thereby improving the accuracy of ADR prediction. Specifically, we first propose Pfed-Split, a method to split the original dataset into a split dataset based on ADR. Then we propose ADR-signal, an ADR prediction model, including a biased data identification method based on federated learning and an ADR prediction model based on Transformer. The former identifies the biased data according to the Euclidean distance and generates a clean dataset by deleting the biased data. The latter is an ADR prediction model based on Transformer trained on the clean data set. The results show that the ROR and PRR on the clean dataset are better than those of the traditional methods. Furthermore, the accuracy rate, F1 score, recall rate and AUC of PFed-Signal are 0.887, 0.890, 0.913 and 0.957 respectively, which are higher than the baselines.

</details>


### [128] [On the Inverse Flow Matching Problem in the One-Dimensional and Gaussian Cases](https://arxiv.org/abs/2512.23265)
*Alexander Korotin,Gudmund Pammer*

Main category: cs.LG

TL;DR: 该论文研究了具有有限指数矩分布之间的流匹配逆问题，证明了在一维和高斯情况下的解唯一性，但多维一般情况仍为开放问题。


<details>
  <summary>Details</summary>
Motivation: 研究流匹配逆问题的动机源于现代生成式AI应用，特别是流匹配模型的蒸馏需求。流匹配在生成模型中广泛应用，但其逆问题的理论性质尚未完全明确。

Method: 论文采用理论分析方法，研究具有有限指数矩的分布之间的流匹配逆问题。通过数学证明建立了解的唯一性条件。

Result: 证明了两种情况下解的唯一性：1）一维设置；2）高斯分布情况。然而，一般的多维问题仍然未解决，留待未来研究。

Conclusion: 流匹配逆问题在一维和高斯情况下有唯一解，但多维一般情况的理论性质仍为开放问题，需要进一步研究。

Abstract: This paper studies the inverse problem of flow matching (FM) between distributions with finite exponential moment, a problem motivated by modern generative AI applications such as the distillation of flow matching models. Uniqueness of the solution is established in two cases - the one-dimensional setting and the Gaussian case. The general multidimensional problem remains open for future studies.

</details>


### [129] [Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL](https://arxiv.org/abs/2512.23310)
*Abolfazl Younesi,Abbas Shabrang Maryan,Elyas Oustad,Zahra Najafabadi Samani,Mohsen Ansari,Thomas Fahringer*

Main category: cs.LG

TL;DR: Splitwise是一个基于Lyapunov辅助深度强化学习的框架，用于在边缘和云端之间进行细粒度、自适应的LLM分区，显著降低延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署大型语言模型面临内存和功耗限制，纯云端推理延迟高成本大，而静态分区方案无法适应带宽波动。

Method: 将Transformer层分解为注意力头和前馈子块，采用Lyapunov优化的分层深度强化学习策略，实现细粒度自适应分区，并包含分区检查点和指数退避恢复机制保证鲁棒性。

Result: 在多种硬件平台上测试GPT-2、LLaMA-7B和LLaMA-13B，相比现有分区器减少端到端延迟1.4-2.8倍，能耗降低达41%，相比纯云端执行降低95%分位延迟53-61%。

Conclusion: Splitwise通过细粒度自适应分区有效解决了边缘设备部署LLM的挑战，在保证精度和适度内存需求的同时显著提升了延迟和能耗性能。

Abstract: Deploying large language models (LLMs) on edge devices is challenging due to their limited memory and power resources. Cloud-only inference reduces device burden but introduces high latency and cost. Static edge-cloud partitions optimize a single metric and struggle when bandwidth fluctuates. We propose Splitwise, a novel Lyapunov-assisted deep reinforcement learning (DRL) framework for fine-grained, adaptive partitioning of LLMs across edge and cloud environments. Splitwise decomposes transformer layers into attention heads and feed-forward sub-blocks, exposing more partition choices than layer-wise schemes. A hierarchical DRL policy, guided by Lyapunov optimization, jointly minimizes latency, energy consumption, and accuracy degradation while guaranteeing queue stability under stochastic workloads and variable network bandwidth. Splitwise also guarantees robustness via partition checkpoints with exponential backoff recovery in case of communication failures. Experiments on Jetson Orin NX, Galaxy S23, and Raspberry Pi 5 with GPT-2 (1.5B), LLaMA-7B, and LLaMA-13B show that Splitwise reduces end-to-end latency by 1.4x-2.8x and cuts energy consumption by up to 41% compared with existing partitioners. It lowers the 95th-percentile latency by 53-61% relative to cloud-only execution, while maintaining accuracy and modest memory requirements.

</details>


### [130] [Deep learning for pedestrians: backpropagation in Transformers](https://arxiv.org/abs/2512.23329)
*Laurent Boué*

Main category: cs.LG

TL;DR: 论文延续之前CNN反向传播的向量化推导工作，将相同方法应用于Transformer架构，推导了嵌入层、多头自注意力、层归一化等模块的梯度表达式，并提供了LoRA层的梯度以展示参数高效微调。


<details>
  <summary>Details</summary>
Motivation: 虽然已有自动微分工具，但手动推导反向传播能加深对前向传播值如何影响最终输出的理解，揭示理解上的空白。通过手动计算反向传播，可以获得每个操作如何影响最终输出的更深入直觉。

Method: 采用轻量级的无索引方法，将之前用于CNN的向量化推导原则和符号应用于Transformer架构，推导嵌入层、多头自注意力、层归一化等新类型层的梯度表达式，并推导LoRA层的梯度。

Result: 提供了完整的PyTorch实现，包含一个简约的GPT-like网络及其所有梯度更新的解析表达式，为Transformer架构的反向传播提供了系统化的向量化推导。

Conclusion: 手动推导Transformer架构的反向传播能提供比自动微分工具更深入的理解，有助于填补对前向传播值如何影响最终输出的理解空白，为理解和优化Transformer模型提供理论基础。

Abstract: This document is a follow-up to our previous paper dedicated to a vectorized derivation of backpropagation in CNNs. Following the same principles and notations already put in place there, we now focus on transformer-based next-token-prediction architectures. To this end, we apply our lightweight index-free methodology to new types of layers such as embedding, multi-headed self-attention and layer normalization. In addition, we also provide gradient expressions for LoRA layers to illustrate parameter-efficient fine-tuning. Why bother doing manual backpropagation when there are so many tools that do this automatically? Any gap in understanding of how values propagate forward will become evident when attempting to differentiate the loss function. By working through the backward pass manually, we gain a deeper intuition for how each operation influences the final output. A complete PyTorch implementation of a minimalistic GPT-like network is also provided along with analytical expressions for of all of its gradient updates.

</details>


### [131] [The Law of Multi-Model Collaboration: Scaling Limits of Model Ensembling for Large Language Models](https://arxiv.org/abs/2512.23340)
*Dakuan Lu,Jiaqi Zhang,Cheng Yuan,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: 该研究提出了多模型协作定律，表明多个大语言模型协作的性能上限遵循关于总参数量的幂律缩放，比单模型缩放具有更显著的改进趋势和更低的损失下限。


<details>
  <summary>Details</summary>
Motivation: 单个大语言模型的能力存在固有上限，而多个模型通过复杂交互可以实现超越任何单个模型的集体性能。尽管已有模型路由和后处理集成等技术，但缺乏统一的多模型协作性能缩放理论框架。

Method: 提出多模型协作定律，采用方法无关的公式化方法，假设理想化的集成预言机，其中每个样本的总交叉熵损失由模型池中任何模型的最小损失决定，以量化多模型协作的内在性能上限。

Result: 实验结果显示：1）多模型系统遵循关于总参数量的幂律缩放；2）相比单模型缩放，多模型协作具有更显著的改进趋势和更低的理论损失下限；3）异构模型家族的集成比同构模型家族获得更好的性能缩放，表明模型多样性是协作增益的主要驱动力。

Conclusion: 模型协作代表了扩展大语言模型智能前沿的关键维度，多模型协作定律为理解和预测多模型系统性能提供了理论基础。

Abstract: Recent advances in large language models (LLMs) have been largely driven by scaling laws for individual models, which predict performance improvements as model parameters and data volume increase. However, the capabilities of any single LLM are inherently bounded. One solution originates from intricate interactions among multiple LLMs, rendering their collective performance surpasses that of any constituent model. Despite the rapid proliferation of multi-model integration techniques such as model routing and post-hoc ensembling, a unifying theoretical framework of performance scaling for multi-model collaboration remains absent. In this work, we propose the Law of Multi-model Collaboration, a scaling law that predicts the performance limits of LLM ensembles based on their aggregated parameter budget. To quantify the intrinsic upper bound of multi-model collaboration, we adopt a method-agnostic formulation and assume an idealized integration oracle where the total cross-entropy loss of each sample is determined by the minimum loss of any model in the model pool. Experimental results reveal that multi-model systems follow a power-law scaling with respect to the total parameter count, exhibiting a more significant improvement trend and a lower theoretical loss floor compared to single model scaling. Moreover, ensembles of heterogeneous model families achieve better performance scaling than those formed within a single model family, indicating that model diversity is a primary driver of collaboration gains. These findings suggest that model collaboration represents a critical axis for extending the intelligence frontier of LLMs.

</details>


### [132] [ECG-RAMBA: Zero-Shot ECG Generalization by Morphology-Rhythm Disentanglement and Long-Range Modeling](https://arxiv.org/abs/2512.23347)
*Hai Duong Nguyen,Xuan-The Tran*

Main category: cs.LG

TL;DR: ECG-RAMBA：通过分离心电图的形态学和节律特征，再通过上下文感知融合重新整合，提高跨数据集泛化能力，在零样本迁移中表现优异。


<details>
  <summary>Details</summary>
Motivation: 深度学习在心电图分类中在单个数据集上表现良好，但在异构采集设置下的泛化能力不足，限制了临床部署和纵向监测。主要问题是模型架构隐含地纠缠了形态波形模式和节律动态，这可能导致捷径学习和放大对分布偏移的敏感性。

Method: 提出ECG-RAMBA框架：1) 使用MiniRocket提取确定性形态特征；2) 从心率变异性计算全局节律描述符；3) 通过双向Mamba骨干进行长程上下文建模；4) 引入数值稳定的Power Mean池化算子(Q=3)增强对瞬态异常的敏感性。

Result: 在Chapman-Shaoxing数据集上获得宏观ROC-AUC≈0.85；在零样本迁移中，在CPSC-2021数据集上获得心房颤动检测的PR-AUC=0.708，显著优于原始信号Mamba基线；在PTB-XL上表现一致。消融研究表明确定性形态学提供坚实基础，而显式节律建模和长程上下文是跨域鲁棒性的关键驱动因素。

Conclusion: ECG-RAMBA通过分离形态学和节律特征并重新整合，提高了心电图分类的跨数据集泛化能力，为临床部署和纵向监测提供了更可靠的解决方案。

Abstract: Deep learning has achieved strong performance for electrocardiogram (ECG) classification within individual datasets, yet dependable generalization across heterogeneous acquisition settings remains a major obstacle to clinical deployment and longitudinal monitoring. A key limitation of many model architectures is the implicit entanglement of morphological waveform patterns and rhythm dynamics, which can promote shortcut learning and amplify sensitivity to distribution shifts. We propose ECG-RAMBA, a framework that separates morphology and rhythm and then re-integrates them through context-aware fusion. ECG-RAMBA combines: (i) deterministic morphological features extracted by MiniRocket, (ii) global rhythm descriptors computed from heart-rate variability (HRV), and (iii) long-range contextual modeling via a bi-directional Mamba backbone. To improve sensitivity to transient abnormalities under windowed inference, we introduce a numerically stable Power Mean pooling operator ($Q=3$) that emphasizes high-evidence segments while avoiding the brittleness of max pooling and the dilution of averaging. We evaluate under a protocol-faithful setting with subject-level cross-validation, a fixed decision threshold, and no test-time adaptation. On the Chapman--Shaoxing dataset, ECG-RAMBA achieves a macro ROC-AUC $\approx 0.85$. In zero-shot transfer, it attains PR-AUC $=0.708$ for atrial fibrillation detection on the external CPSC-2021 dataset, substantially outperforming a comparable raw-signal Mamba baseline, and shows consistent cross-dataset performance on PTB-XL. Ablation studies indicate that deterministic morphology provides a strong foundation, while explicit rhythm modeling and long-range context are critical drivers of cross-domain robustness.

</details>


### [133] [ISOPO: Proximal policy gradients without pi-old](https://arxiv.org/abs/2512.23353)
*Nilin Abrahamsen*

Main category: cs.LG

TL;DR: ISOPO是一种高效的单步梯度方法，用于近似自然策略梯度，相比现有方法减少了计算复杂度


<details>
  <summary>Details</summary>
Motivation: 现有近端策略方法如GRPO或CISPO需要使用多个梯度步和重要性采样裁剪来近似自然梯度步，计算效率较低

Method: ISOPO在Fisher度量下归一化每个序列的对数概率梯度，然后与优势函数进行收缩。另一变体基于每层的神经正切核变换微批次优势函数

Result: ISOPO可以在单次反向传播中实现层间变换，相比标准REINFORCE方法计算开销可忽略不计

Conclusion: ISOPO提供了一种高效的单步自然策略梯度近似方法，显著降低了计算复杂度

Abstract: This note introduces Isometric Policy Optimization (ISOPO), an efficient method to approximate the natural policy gradient in a single gradient step. In comparison, existing proximal policy methods such as GRPO or CISPO use multiple gradient steps with variants of importance ratio clipping to approximate a natural gradient step relative to a reference policy. In its simplest form, ISOPO normalizes the log-probability gradient of each sequence in the Fisher metric before contracting with the advantages. Another variant of ISOPO transforms the microbatch advantages based on the neural tangent kernel in each layer. ISOPO applies this transformation layer-wise in a single backward pass and can be implemented with negligible computational overhead compared to vanilla REINFORCE.

</details>


### [134] [Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2](https://arxiv.org/abs/2512.23367)
*Yilun Luo,HuaQing Zheng,Haoqian Meng,Wenyuan Liu,Peng Zhang*

Main category: cs.LG

TL;DR: 华为openPangu-Embedded模型集成三种思维链推理模式，但推理轨迹导致内存和延迟开销大。本文通过低比特量化（INT8和W4A8）优化推理效率，在Ascend NPU上实现高效推理，保持高模型精度。


<details>
  <summary>Details</summary>
Motivation: openPangu-Embedded模型集成了三种思维链推理模式（slow_think、auto_think、no_think），这些模式虽然增强了推理能力，但生成了大量推理轨迹，导致显著的内存和延迟开销，在Ascend NPU上的实际部署面临挑战。

Method: 采用低比特量化技术，将FP16计算转换为更高效的整数运算。提出了统一的低比特推理框架，支持INT8（W8A8）和W4A8量化，专门针对Atlas A2上的openPangu-Embedded模型进行优化。

Result: 在代码生成基准测试（HumanEval和MBPP）上评估所有三种CoT模式：INT8量化保持超过90%的FP16基线精度，在Atlas A2上实现1.5倍预填充加速；W4A8量化显著减少内存消耗，但精度有一定折衷。

Conclusion: 低比特量化有效促进了Ascend NPU上的高效思维链推理，在保持高模型保真度的同时解决了计算约束问题，为实际部署提供了可行方案。

Abstract: Huawei's openPangu-Embedded-1B and openPangu-Embedded-7B, variants of the openPangu large language model, integrate three distinct Chain-of-Thought (CoT) reasoning paradigms, namely slow_think, auto_think, and no_think. While these CoT modes enhance reasoning capabilities, their generation of extended reasoning traces introduces substantial memory and latency overheads, posing challenges for practical deployment on Ascend NPUs. This paper addresses these computational constraints by leveraging low-bit quantization, which transforms FP16 computations into more efficient integer arithmetic. We introduce a unified low-bit inference framework, supporting INT8 (W8A8) and W4A8 quantization, specifically optimized for openPangu-Embedded models on the Atlas A2. Our comprehensive evaluation, conducted across all three CoT modes on code generation benchmarks (HumanEval and MBPP), demonstrates the efficacy of this approach. INT8 quantization consistently preserves over 90\% of the FP16 baseline accuracy and achieves a 1.5x prefill speedup on the Atlas A2. Furthermore, W4A8 quantization significantly reduces memory consumption, albeit with a moderate trade-off in accuracy. These findings collectively indicate that low-bit quantization effectively facilitates efficient CoT reasoning on Ascend NPUs, maintaining high model fidelity.

</details>


### [135] [Diffusion priors enhanced velocity model building from time-lag images using a neural operator](https://arxiv.org/abs/2512.23375)
*Xiao Ma,Mohammad Hasyim Taufik,Tariq Alkhalifah*

Main category: cs.LG

TL;DR: 提出结合生成模型与神经算子的新框架，用于高效构建高分辨率速度模型，通过神经算子作为前向映射快速生成RTM图像，并利用生成模型作为正则化器提升分辨率。


<details>
  <summary>Details</summary>
Motivation: 传统速度模型构建方法计算成本高、耗时，而深度学习特别是生成模型和神经算子的发展为解决这些限制提供了新途径，需要高效高精度的速度模型构建方法。

Method: 提出生成神经算子框架：1) 神经算子作为前向映射算子，从真实和偏移速度模型快速生成时间滞后RTM扩展图像；2) 通过自动微分逐步更新偏移速度模型；3) 嵌入在真实速度模型分布上训练的生成模型作为正则化器，提升分辨率。

Result: 合成和实际数据实验表明，所提出的生成神经算子速度模型构建方法有效，能够获得更干净、更高分辨率的速度模型预测。

Conclusion: 结合生成模型和神经算子的新框架能够高效构建高分辨率速度模型，解决了传统方法计算成本高的问题，为地下成像提供了有效解决方案。

Abstract: Velocity model building serves as a crucial component for achieving high precision subsurface imaging. However, conventional velocity model building methods are often computationally expensive and time consuming. In recent years, with the rapid advancement of deep learning, particularly the success of generative models and neural operators, deep learning based approaches that integrate data and their statistics have attracted increasing attention in addressing the limitations of traditional methods. In this study, we propose a novel framework that combines generative models with neural operators to obtain high resolution velocity models efficiently. Within this workflow, the neural operator functions as a forward mapping operator to rapidly generate time lag reverse time migration (RTM) extended images from the true and migration velocity models. In this framework, the neural operator is acting as a surrogate for modeling followed by migration, which uses the true and migration velocities, respectively. The trained neural operator is then employed, through automatic differentiation, to gradually update the migration velocity placed in the true velocity input channel with high resolution components so that the output of the network matches the time lag images of observed data obtained using the migration velocity. By embedding a generative model, trained on a high-resolution velocity model distribution, which corresponds to the true velocity model distribution used to train the neural operator, as a regularizer, the resulting predictions are cleaner with higher resolution information. Both synthetic and field data experiments demonstrate the effectiveness of the proposed generative neural operator based velocity model building approach.

</details>


### [136] [A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers](https://arxiv.org/abs/2512.23380)
*Mohammad Nasirzadeh,Jafar Tahmoresnezhad,Parviz Rashidi-Khazaee*

Main category: cs.LG

TL;DR: CoLog是一个用于日志异常检测的多模态框架，通过协作式Transformer和多头注意力机制处理不同日志模态的交互，在多个基准数据集上达到99.6%以上的F1分数。


<details>
  <summary>Details</summary>
Motivation: 传统单模态方法忽略日志数据的多模态特性，而现有多模态方法未能有效处理不同模态间的交互。日志数据根据收集来源包含多种信息模态，需要更全面的异常检测方法。

Method: 提出CoLog框架，使用协作式Transformer和多头注意力机制学习不同日志模态间的交互。通过模态适应层处理模态异质性，学习数据中的细微模式和依赖关系。

Result: 在7个日志异常检测基准数据集上，CoLog在点异常和集体异常检测中平均精度99.63%、召回率99.59%、F1分数99.61%，优于现有最先进方法。

Conclusion: CoLog通过统一框架解决了日志数据自动分析的复杂挑战，为网络安全、系统监控和运营效率提供了先进的日志异常检测解决方案。

Abstract: Log anomaly detection is crucial for preserving the security of operating systems. Depending on the source of log data collection, various information is recorded in logs that can be considered log modalities. In light of this intuition, unimodal methods often struggle by ignoring the different modalities of log data. Meanwhile, multimodal methods fail to handle the interactions between these modalities. Applying multimodal sentiment analysis to log anomaly detection, we propose CoLog, a framework that collaboratively encodes logs utilizing various modalities. CoLog utilizes collaborative transformers and multi-head impressed attention to learn interactions among several modalities, ensuring comprehensive anomaly detection. To handle the heterogeneity caused by these interactions, CoLog incorporates a modality adaptation layer, which adapts the representations from different log modalities. This methodology enables CoLog to learn nuanced patterns and dependencies within the data, enhancing its anomaly detection capabilities. Extensive experiments demonstrate CoLog's superiority over existing state-of-the-art methods. Furthermore, in detecting both point and collective anomalies, CoLog achieves a mean precision of 99.63%, a mean recall of 99.59%, and a mean F1 score of 99.61% across seven benchmark datasets for log-based anomaly detection. The comprehensive detection capabilities of CoLog make it highly suitable for cybersecurity, system monitoring, and operational efficiency. CoLog represents a significant advancement in log anomaly detection, providing a sophisticated and effective solution to point and collective anomaly detection through a unified framework and a solution to the complex challenges automatic log data analysis poses. We also provide the implementation of CoLog at https://github.com/NasirzadehMoh/CoLog.

</details>


### [137] [On the Sample Complexity of Learning for Blind Inverse Problems](https://arxiv.org/abs/2512.23405)
*Nathan Buskulic,Luca Calatroni,Lorenzo Rosasco,Silvia Villa*

Main category: cs.LG

TL;DR: 该论文研究了盲逆问题中的学习理论，在线性最小均方误差估计器框架下，推导了最优估计器的闭式解，建立了与Tikhonov正则化的等价性，并提供了严格的有限样本误差界。


<details>
  <summary>Details</summary>
Motivation: 盲逆问题中前向算子部分或完全未知，现有数据驱动方法缺乏可解释性和理论保证，限制了在成像等应用领域的可靠性。

Method: 在线性最小均方误差估计器框架下进行理论分析，推导最优估计器的闭式表达式，建立与Tikhonov正则化的等价关系，证明收敛性结果，并推导有限样本误差界。

Result: 获得了最优估计器的闭式解，证明了与特定Tikhonov正则化的等价性，建立了收敛性理论，推导了显式量化算子随机性影响的有限样本误差界，并通过数值实验验证了理论预测。

Conclusion: 该工作为盲逆问题中的学习提供了严格的理论基础，建立了数据驱动方法与经典正则化理论之间的联系，为实际应用提供了可靠的理论保证。

Abstract: Blind inverse problems arise in many experimental settings where the forward operator is partially or entirely unknown. In this context, methods developed for the non-blind case cannot be adapted in a straightforward manner. Recently, data-driven approaches have been proposed to address blind inverse problems, demonstrating strong empirical performance and adaptability. However, these methods often lack interpretability and are not supported by rigorous theoretical guarantees, limiting their reliability in applied domains such as imaging inverse problems. In this work, we shed light on learning in blind inverse problems within the simplified yet insightful framework of Linear Minimum Mean Square Estimators (LMMSEs). We provide an in-depth theoretical analysis, deriving closed-form expressions for optimal estimators and extending classical results. In particular, we establish equivalences with suitably chosen Tikhonov-regularized formulations, where the regularization depends explicitly on the distributions of the unknown signal, the noise, and the random forward operators. We also prove convergence results under appropriate source condition assumptions. Furthermore, we derive rigorous finite-sample error bounds that characterize the performance of learned estimators as a function of the noise level, problem conditioning, and number of available samples. These bounds explicitly quantify the impact of operator randomness and reveal the associated convergence rates as this randomness vanishes. Finally, we validate our theoretical findings through illustrative numerical experiments that confirm the predicted convergence behavior.

</details>


### [138] [Task-driven Heterophilic Graph Structure Learning](https://arxiv.org/abs/2512.23406)
*Ayushman Raghuvanshi,Gonzalo Mateos,Sundeep Prabhakar Chepuri*

Main category: cs.LG

TL;DR: 提出FgGSL框架，通过频率引导的图结构学习，联合学习同质性和异质性图结构，结合谱编码器处理异质图，在六个异质图基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络在异质图上表现不佳，因为异质图中相连节点往往标签不同，特征相似性提供的结构线索较弱。需要新的方法来学习异质图的判别性节点表示。

Method: 提出频率引导的图结构学习(FgGSL)框架：1) 使用可学习的对称特征驱动掩码函数推断互补的同质和异质图结构；2) 使用预设计的低通和高通图滤波器组处理这些图；3) 引入基于标签的结构损失，显式促进同质和异质边的恢复；4) 推导结构损失的稳定性界限和滤波器组在图扰动下的鲁棒性保证。

Result: 在六个异质图基准测试中，FgGSL一致优于最先进的图神经网络和图重连方法，证明了结合频率信息与监督拓扑推断的优势。

Conclusion: FgGSL通过联合学习同质和异质图结构，结合谱编码器和频率引导的图滤波器，有效解决了异质图上的表示学习问题，为图结构学习提供了新的监督框架。

Abstract: Graph neural networks (GNNs) often struggle to learn discriminative node representations for heterophilic graphs, where connected nodes tend to have dissimilar labels and feature similarity provides weak structural cues. We propose frequency-guided graph structure learning (FgGSL), an end-to-end graph inference framework that jointly learns homophilic and heterophilic graph structures along with a spectral encoder. FgGSL employs a learnable, symmetric, feature-driven masking function to infer said complementary graphs, which are processed using pre-designed low- and high-pass graph filter banks. A label-based structural loss explicitly promotes the recovery of homophilic and heterophilic edges, enabling task-driven graph structure learning. We derive stability bounds for the structural loss and establish robustness guarantees for the filter banks under graph perturbations. Experiments on six heterophilic benchmarks demonstrate that FgGSL consistently outperforms state-of-the-art GNNs and graph rewiring methods, highlighting the benefits of combining frequency information with supervised topology inference.

</details>


### [139] [Theoretical Foundations of Scaling Law in Familial Models](https://arxiv.org/abs/2512.23407)
*Huan Song,Qingfei Zhao,Ting Long,Shuyu Tian,Hongjun An,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: 本文扩展了神经缩放定律，引入粒度(G)作为第三个缩放变量，建立了L(N,D,G)统一函数形式，证明在固定计算预算下，通过早期退出和接力推理可以生成多个可部署子模型而不显著影响性能。


<details>
  <summary>Details</summary>
Motivation: 传统神经缩放定律仅考虑单一密集模型输出，忽略了异构设备-边缘-云层次结构中实现普适智能所需的家族模型范式。家族模型通过早期退出和接力推理从单一共享主干生成多个可部署子模型，需要理论扩展来支持这种"一次训练，多次部署"的范式。

Method: 提出统一函数形式L(N,D,G)，将粒度(G)作为与模型大小(N)和训练标记(D)并列的基本缩放变量。采用严格的IsoFLOP实验设计，在固定计算预算下系统扫描模型大小和粒度，动态调整标记数量，以解耦粒度边际成本与规模效益，实现高保真参数化。

Result: 研究发现粒度惩罚遵循指数极小的乘法幂律。理论上这桥接了固定计算训练与动态架构；实践上验证了"一次训练，多次部署"范式，表明部署灵活性可以在不损害密集基线计算最优性的情况下实现。

Conclusion: 成功将神经缩放定律扩展到家族模型范式，引入粒度作为第三个基本缩放变量，证明了在固定计算预算下生成多个可部署子模型的可行性，为异构设备层次结构中的普适智能提供了理论基础。

Abstract: Neural scaling laws have become foundational for optimizing large language model (LLM) training, yet they typically assume a single dense model output. This limitation effectively overlooks "Familial models, a transformative paradigm essential for realizing ubiquitous intelligence across heterogeneous device-edge-cloud hierarchies. Transcending static architectures, familial models integrate early exits with relay-style inference to spawn G deployable sub-models from a single shared backbone. In this work, we theoretically and empirically extend the scaling law to capture this "one-run, many-models" paradigm by introducing Granularity (G) as a fundamental scaling variable alongside model size (N) and training tokens (D). To rigorously quantify this relationship, we propose a unified functional form L(N, D, G) and parameterize it using large-scale empirical runs. Specifically, we employ a rigorous IsoFLOP experimental design to strictly isolate architectural impact from computational scale. Across fixed budgets, we systematically sweep model sizes (N) and granularities (G) while dynamically adjusting tokens (D). This approach effectively decouples the marginal cost of granularity from the benefits of scale, ensuring high-fidelity parameterization of our unified scaling law. Our results reveal that the granularity penalty follows a multiplicative power law with an extremely small exponent. Theoretically, this bridges fixed-compute training with dynamic architectures. Practically, it validates the "train once, deploy many" paradigm, demonstrating that deployment flexibility is achievable without compromising the compute-optimality of dense baselines.

</details>


### [140] [Directly Constructing Low-Dimensional Solution Subspaces in Deep Neural Networks](https://arxiv.org/abs/2512.23410)
*Yusuf Kalyoncuoglu*

Main category: cs.LG

TL;DR: 论文提出一种构造性子空间方法，通过解耦解几何与搜索空间，可将分类头压缩16倍且性能损失可忽略，并引入子空间原生蒸馏新范式。


<details>
  <summary>Details</summary>
Motivation: 当前深度神经网络虽然权重矩阵和特征流形具有低内在维度，但模型仍依赖大规模高维宽度。这种冗余对表示非必需，但对解决非凸优化搜索问题（寻找全局最小值）是必要的，而紧凑网络难以解决此问题。

Method: 提出构造性方法绕过优化瓶颈：通过解耦解几何与环境搜索空间，在ResNet-50、ViT和BERT上实证分类头可大幅压缩；提出子空间原生蒸馏新范式，在构造的子空间中直接定义目标，为学生模型提供稳定几何坐标系。

Result: 实验证明分类头可压缩高达16倍而性能损失可忽略；子空间原生蒸馏为学生模型提供稳定几何坐标系，可能完全规避高维搜索问题。

Conclusion: 该方法为实现"训练大、部署小"愿景提供新途径，通过构造性子空间方法绕过优化瓶颈，子空间原生蒸馏有望让学生模型完全规避高维搜索问题。

Abstract: While it is well-established that the weight matrices and feature manifolds of deep neural networks exhibit a low Intrinsic Dimension (ID), current state-of-the-art models still rely on massive high-dimensional widths. This redundancy is not required for representation, but is strictly necessary to solve the non-convex optimization search problem-finding a global minimum, which remains intractable for compact networks. In this work, we propose a constructive approach to bypass this optimization bottleneck. By decoupling the solution geometry from the ambient search space, we empirically demonstrate across ResNet-50, ViT, and BERT that the classification head can be compressed by even huge factors of 16 with negligible performance degradation. This motivates Subspace-Native Distillation as a novel paradigm: by defining the target directly in this constructed subspace, we provide a stable geometric coordinate system for student models, potentially allowing them to circumvent the high-dimensional search problem entirely and realize the vision of Train Big, Deploy Small.

</details>


### [141] [Stochastic Siamese MAE Pretraining for Longitudinal Medical Images](https://arxiv.org/abs/2512.23441)
*Taha Emre,Arunava Chakravarty,Thomas Pinetz,Dmitrii Lachinov,Martin J. Menten,Hendrik Scholl,Sobha Sivaprasad,Daniel Rueckert,Andrew Lotery,Stefan Sacu,Ursula Schmidt-Erfurth,Hrvoje Bogunović*

Main category: cs.LG

TL;DR: STAMP是一种基于Siamese MAE框架的随机时间自编码器，通过条件变分推理学习医学影像中的非确定性时间动态，在AMD和AD疾病进展预测中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法（如MAE）缺乏时间感知能力，无法捕捉纵向医学数据中的疾病进展动态。确定性方法无法处理疾病演变中的固有不确定性。

Method: 提出STAMP框架：基于Siamese MAE，通过随机过程编码时间信息，将两个输入体积的时间差作为条件，将MAE重建损失重构为条件变分推理目标。

Result: 在OCT和MRI数据集上评估，STAMP预训练的ViT模型在AMD和AD疾病进展预测任务中优于现有时间MAE方法和基础模型。

Conclusion: STAMP通过随机时间建模有效学习医学影像中的非确定性时间动态，为纵向疾病进展预测提供了更好的表示学习方法。

Abstract: Temporally aware image representations are crucial for capturing disease progression in 3D volumes of longitudinal medical datasets. However, recent state-of-the-art self-supervised learning approaches like Masked Autoencoding (MAE), despite their strong representation learning capabilities, lack temporal awareness. In this paper, we propose STAMP (Stochastic Temporal Autoencoder with Masked Pretraining), a Siamese MAE framework that encodes temporal information through a stochastic process by conditioning on the time difference between the 2 input volumes. Unlike deterministic Siamese approaches, which compare scans from different time points but fail to account for the inherent uncertainty in disease evolution, STAMP learns temporal dynamics stochastically by reframing the MAE reconstruction loss as a conditional variational inference objective. We evaluated STAMP on two OCT and one MRI datasets with multiple visits per patient. STAMP pretrained ViT models outperformed both existing temporal MAE methods and foundation models on different late stage Age-Related Macular Degeneration and Alzheimer's Disease progression prediction which require models to learn the underlying non-deterministic temporal dynamics of the diseases.

</details>


### [142] [Dynamic Subspace Composition: Efficient Adaptation via Contractive Basis Expansion](https://arxiv.org/abs/2512.23448)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: 提出Dynamic Subspace Composition (DSC)框架，通过状态依赖的稀疏基向量组合来近似上下文相关权重，解决MoE模型的表示坍塌和梯度不稳定问题，显著降低参数复杂度和内存访问量。


<details>
  <summary>Details</summary>
Motivation: 混合专家(MoE)模型虽然能扩展容量，但常面临表示坍塌和梯度不稳定问题。现有方法如Mixture-of-LoRAs需要O(M rd)参数复杂度，效率较低。

Method: 提出DSC框架，将权重更新建模为星形域内的残差轨迹，使用幅度门控单纯形插值确保在恒等变换处的连续性。通过解耦的单位范数基向量构建组合秩K近似，而非检索独立的秩r矩阵。

Result: 将参数复杂度从O(M rd)降低到O(M d)，内存访问量降低到O(Kd)。通过框架理论正则化和谱约束提供动态更新的严格最坏情况边界。

Conclusion: DSC提供了一种高效且理论保证的MoE模型改进方法，显著降低计算和内存开销，同时解决表示坍塌和梯度不稳定问题。

Abstract: Mixture of Experts (MoE) models scale capacity but often suffer from representation collapse and gradient instability. We propose Dynamic Subspace Composition (DSC), a framework that approximates context-dependent weights via a state-dependent, sparse expansion of a shared basis bank. Formally, DSC models the weight update as a residual trajectory within a Star- Shaped Domain, employing a Magnitude-Gated Simplex Interpolation to ensure continuity at the identity. Unlike standard Mixture-of-LoRAs, which incurs O(M rd) parameter complexity by retrieving independent rank-r matrices, DSC constructs a compositional rank-K approximation from decoupled unit-norm basis vectors. This reduces parameter complexity to O(M d) and memory traffic to O(Kd), while Frame-Theoretic regularization and spectral constraints provide rigorous worst-case bounds on the dynamic update. The code is available at https://github. com/VladimerKhasia/DSC

</details>


### [143] [Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance](https://arxiv.org/abs/2512.23461)
*Zhuo Li,Pengyu Cheng,Zhechao Yu,Feifei Tong,Anningzhe Gao,Tsung-Hui Chang,Xiang Wan,Erchao Zhao,Xiaoxi Jiang,Guanjun Jiang*

Main category: cs.LG

TL;DR: 提出DIR方法，通过信息优化来消除奖励模型中的复杂归纳偏差，提升RLHF性能


<details>
  <summary>Details</summary>
Motivation: 奖励模型训练数据质量低，包含各种归纳偏差（如响应长度、奉承、格式等），容易导致过拟合和奖励攻击。现有去偏方法要么针对单一偏差，要么只建模简单线性相关，无法处理复杂非线性偏差。

Method: 基于信息瓶颈理论，最大化奖励模型分数与人类偏好对之间的互信息，同时最小化奖励模型输出与偏好输入中偏差属性之间的互信息。

Result: DIR能有效缓解三种归纳偏差（响应长度、奉承、格式），提升RLHF在多种基准测试中的性能，并增强泛化能力。

Conclusion: DIR通过信息理论框架处理复杂非线性偏差，扩展了奖励模型去偏方法的实际应用场景，为RLHF提供了更鲁棒的奖励建模方案。

Abstract: Reward models (RMs) are essential in reinforcement learning from human feedback (RLHF) to align large language models (LLMs) with human values. However, RM training data is commonly recognized as low-quality, containing inductive biases that can easily lead to overfitting and reward hacking. For example, more detailed and comprehensive responses are usually human-preferred but with more words, leading response length to become one of the inevitable inductive biases. A limited number of prior RM debiasing approaches either target a single specific type of bias or model the problem with only simple linear correlations, \textit{e.g.}, Pearson coefficients. To mitigate more complex and diverse inductive biases in reward modeling, we introduce a novel information-theoretic debiasing method called \textbf{D}ebiasing via \textbf{I}nformation optimization for \textbf{R}M (DIR). Inspired by the information bottleneck (IB), we maximize the mutual information (MI) between RM scores and human preference pairs, while minimizing the MI between RM outputs and biased attributes of preference inputs. With theoretical justification from information theory, DIR can handle more sophisticated types of biases with non-linear correlations, broadly extending the real-world application scenarios for RM debiasing methods. In experiments, we verify the effectiveness of DIR with three types of inductive biases: \textit{response length}, \textit{sycophancy}, and \textit{format}. We discover that DIR not only effectively mitigates target inductive biases but also enhances RLHF performance across diverse benchmarks, yielding better generalization abilities. The code and training recipes are available at https://github.com/Qwen-Applications/DIR.

</details>


### [144] [FRoD: Full-Rank Efficient Fine-Tuning with Rotational Degrees for Fast Convergence](https://arxiv.org/abs/2512.23485)
*Guoan Wan,Tianyu Chen,Fangzheng Feng,Haoyi Zhou,Runhua Xu*

Main category: cs.LG

TL;DR: FRoD是一种新颖的参数高效微调方法，通过分层联合分解和旋转自由度，在仅使用1.72%可训练参数的情况下，达到与全模型微调相当的精度。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法（如LoRA）在效率和表达能力之间存在权衡，由于固有的低秩约束，往往收敛速度慢且适应能力有限，难以捕捉复杂任务所需的模式。

Method: FRoD结合分层联合分解与旋转自由度，通过提取跨层全局共享基，并向缩放因子注入稀疏可学习扰动，实现灵活的全秩更新，增强表达能力和效率。

Result: 在涵盖视觉、推理和语言理解的20个基准测试中，FRoD在相同训练预算下，仅使用1.72%的可训练参数，就能达到与全模型微调相当的准确率。

Conclusion: FRoD通过创新的分层分解和旋转自由度设计，成功解决了PEFT方法在表达能力和收敛速度方面的限制，为大型基础模型的高效适应提供了更优解决方案。

Abstract: Parameter-efficient fine-tuning (PEFT) methods have emerged as a practical solution for adapting large foundation models to downstream tasks, reducing computational and memory costs by updating only a small subset of parameters. Among them, approaches like LoRA aim to strike a balance between efficiency and expressiveness, but often suffer from slow convergence and limited adaptation capacity due to their inherent low-rank constraints. This trade-off hampers the ability of PEFT methods to capture complex patterns needed for diverse tasks. To address these challenges, we propose FRoD, a novel fine-tuning method that combines hierarchical joint decomposition with rotational degrees of freedom. By extracting a globally shared basis across layers and injecting sparse, learnable perturbations into scaling factors for flexible full-rank updates, FRoD enhances expressiveness and efficiency, leading to faster and more robust convergence. On 20 benchmarks spanning vision, reasoning, and language understanding, FRoD matches full model fine-tuning in accuracy, while using only 1.72% of trainable parameters under identical training budgets.

</details>


### [145] [ML Compass: Navigating Capability, Cost, and Compliance Trade-offs in AI Model Deployment](https://arxiv.org/abs/2512.23487)
*Vassilis Digalakis,Ramayya Krishnan,Gonzalo Martin Fernandez,Agni Orfanoudaki*

Main category: cs.LG

TL;DR: 提出ML Compass框架，将AI模型选择视为能力-成本前沿上的约束优化问题，解决能力排行榜与实际部署决策之间的差距


<details>
  <summary>Details</summary>
Motivation: 当前广泛使用的能力排行榜不能直接转化为部署决策，存在"能力-部署差距"。需要综合考虑用户效用、部署成本和合规要求，从系统层面进行模型选择

Method: 开发ML Compass框架，将模型选择视为能力-成本前沿上的约束优化问题。理论方面：在参数化前沿下表征最优模型配置，展示三维结构。实施方面：提出从异构模型描述符提取内部度量、从数据估计经验前沿、从交互数据学习效用函数、推荐模型的管道

Result: 在通用对话(PRISM Alignment数据集)和医疗(HealthBench自定义数据集)两个案例研究中验证，框架产生的推荐和部署感知排行榜与仅基于能力的排名有显著差异，阐明了能力、成本和安全之间的权衡如何影响最优模型选择

Conclusion: ML Compass框架通过系统层面的约束优化方法，有效弥合了能力排行榜与实际部署决策之间的差距，为组织在复杂约束下选择AI模型提供了实用工具

Abstract: We study how organizations should select among competing AI models when user utility, deployment costs, and compliance requirements jointly matter. Widely used capability leaderboards do not translate directly into deployment decisions, creating a capability -- deployment gap; to bridge it, we take a systems-level view in which model choice is tied to application outcomes, operating constraints, and a capability-cost frontier. We develop ML Compass, a framework that treats model selection as constrained optimization over this frontier. On the theory side, we characterize optimal model configurations under a parametric frontier and show a three-regime structure in optimal internal measures: some dimensions are pinned at compliance minima, some saturate at maximum levels, and the remainder take interior values governed by frontier curvature. We derive comparative statics that quantify how budget changes, regulatory tightening, and technological progress propagate across capability dimensions and costs. On the implementation side, we propose a pipeline that (i) extracts low-dimensional internal measures from heterogeneous model descriptors, (ii) estimates an empirical frontier from capability and cost data, (iii) learns a user- or task-specific utility function from interaction outcome data, and (iv) uses these components to target capability-cost profiles and recommend models. We validate ML Compass with two case studies: a general-purpose conversational setting using the PRISM Alignment dataset and a healthcare setting using a custom dataset we build using HealthBench. In both environments, our framework produces recommendations -- and deployment-aware leaderboards based on predicted deployment value under constraints -- that can differ materially from capability-only rankings, and clarifies how trade-offs between capability, cost, and safety shape optimal model choice.

</details>


### [146] [Joint Link Adaptation and Device Scheduling Approach for URLLC Industrial IoT Network: A DRL-based Method with Bayesian Optimization](https://arxiv.org/abs/2512.23493)
*Wei Gao,Paul Zheng,Peng Wu,Yulin Hu,Anke Schmeink*

Main category: cs.LG

TL;DR: 提出基于贝叶斯优化的TD3方法，用于工业物联网中多设备动态URLLC场景，在CSI不完美条件下联合优化链路自适应和设备调度，提高收敛速度和总传输速率。


<details>
  <summary>Details</summary>
Motivation: 工业物联网需要支持多设备动态超可靠低延迟通信，但面临CSI不完美、URLLC网络中的错误样本不平衡以及TD3算法参数敏感等问题，这些问题会降低算法收敛速度和可靠性。

Method: 提出贝叶斯优化驱动的TD3方法，根据不完美的CSI自适应确定设备服务顺序和相应的调制编码方案。为了解决收敛问题，设计了基于BO的训练机制，提供更可靠的学习方向和样本选择方法来处理不平衡样本问题。

Result: 通过大量仿真表明，所提算法相比现有解决方案实现了更快的收敛速度和更高的总速率性能。

Conclusion: 该研究成功解决了工业物联网中CSI不完美条件下的多设备URLLC优化问题，提出的BO-TD3方法在收敛速度和性能方面均优于现有方法。

Abstract: In this article, we consider an industrial internet of things (IIoT) network supporting multi-device dynamic ultra-reliable low-latency communication (URLLC) while the channel state information (CSI) is imperfect. A joint link adaptation (LA) and device scheduling (including the order) design is provided, aiming at maximizing the total transmission rate under strict block error rate (BLER) constraints. In particular, a Bayesian optimization (BO) driven Twin Delayed Deep Deterministic Policy Gradient (TD3) method is proposed, which determines the device served order sequence and the corresponding modulation and coding scheme (MCS) adaptively based on the imperfect CSI. Note that the imperfection of CSI, error sample imbalance in URLLC networks, as well as the parameter sensitivity nature of the TD3 algorithm likely diminish the algorithm's convergence speed and reliability. To address such an issue, we proposed a BO based training mechanism for the convergence speed improvement, which provides a more reliable learning direction and sample selection method to track the imbalance sample problem. Via extensive simulations, we show that the proposed algorithm achieves faster convergence and higher sum-rate performance compared to existing solutions.

</details>


### [147] [Trustworthy Machine Learning under Distribution Shifts](https://arxiv.org/abs/2512.23524)
*Zhuo Huang*

Main category: cs.LG

TL;DR: 该研究聚焦于分布偏移下的可信机器学习，针对扰动偏移、域偏移和模态偏移三种常见分布偏移，从鲁棒性、可解释性和适应性三个维度提出解决方案，旨在提升AI系统的可靠性、通用性和安全性。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习在AI领域取得显著进展，但分布偏移问题仍然是限制ML系统可靠性和通用性的根本弱点，同时也会引发对AI的信任问题。当前AI模型在特定任务上已超越人类，但面对分布变化时性能会显著下降。

Method: 将分布偏移系统分类为三种类型：扰动偏移、域偏移和模态偏移。针对每种偏移，从三个可信度维度（鲁棒性、可解释性、适应性）进行严格研究，并提出相应解决方案和基本见解。

Result: 研究提出了针对不同分布偏移类型的可信机器学习框架，旨在增强机器学习系统的效率、适应性和安全性等关键问题，为构建更可靠、通用的AI系统提供理论基础和方法指导。

Conclusion: 通过系统研究分布偏移下的可信机器学习问题，该工作为提升AI系统的鲁棒性、可解释性和适应性提供了全面框架，有助于推动AI向更可靠、负责任的方向发展，解决当前AI应用中的关键信任问题。

Abstract: Machine Learning (ML) has been a foundational topic in artificial intelligence (AI), providing both theoretical groundwork and practical tools for its exciting advancements. From ResNet for visual recognition to Transformer for vision-language alignment, the AI models have achieved superior capability to humans. Furthermore, the scaling law has enabled AI to initially develop general intelligence, as demonstrated by Large Language Models (LLMs). To this stage, AI has had an enormous influence on society and yet still keeps shaping the future for humanity. However, distribution shift remains a persistent ``Achilles' heel'', fundamentally limiting the reliability and general usefulness of ML systems. Moreover, generalization under distribution shift would also cause trust issues for AIs. Motivated by these challenges, my research focuses on \textit{Trustworthy Machine Learning under Distribution Shifts}, with the goal of expanding AI's robustness, versatility, as well as its responsibility and reliability. We carefully study the three common distribution shifts into: (1) Perturbation Shift, (2) Domain Shift, and (3) Modality Shift. For all scenarios, we also rigorously investigate trustworthiness via three aspects: (1) Robustness, (2) Explainability, and (3) Adaptability. Based on these dimensions, we propose effective solutions and fundamental insights, meanwhile aiming to enhance the critical ML problems, such as efficiency, adaptability, and safety.

</details>


### [148] [EEG-based Graph-guided Domain Adaptation for Robust Cross-Session Emotion Recognition](https://arxiv.org/abs/2512.23526)
*Maryam Mirzaei,Farzaneh Shayegh,Hamed Narimani*

Main category: cs.LG

TL;DR: EGDA框架通过联合对齐全局和类别特定分布，减少跨会话差异，提升EEG情感识别的泛化能力，在SEED-IV数据集上取得优于基线的性能。


<details>
  <summary>Details</summary>
Motivation: EEG是情感识别的可靠来源，但跨会话差异严重阻碍模型泛化。需要解决跨会话分布差异问题，提升情感识别系统的鲁棒性。

Method: 提出EGDA框架：1）联合对齐全局（边际）和类别特定（条件）分布以减少跨会话差异；2）通过图正则化保持EEG数据的内在结构。

Result: 在SEED-IV数据集上，EGDA在三个迁移任务中分别获得81.22%、80.15%和83.27%的准确率，超越多个基线方法。Gamma频带最具判别性，中央-顶叶和前额叶脑区对情感识别最关键。

Conclusion: EGDA通过分布对齐和图正则化有效减少跨会话差异，提升EEG情感识别的泛化性能，为鲁棒的人机交互系统提供技术支持。

Abstract: Accurate recognition of human emotional states is critical for effective human-machine interaction. Electroencephalography (EEG) offers a reliable source for emotion recognition due to its high temporal resolution and its direct reflection of neural activity. Nevertheless, variations across recording sessions present a major challenge for model generalization. To address this issue, we propose EGDA, a framework that reduces cross-session discrepancies by jointly aligning the global (marginal) and class-specific (conditional) distributions, while preserving the intrinsic structure of EEG data through graph regularization. Experimental results on the SEED-IV dataset demonstrate that EGDA achieves robust cross-session performance, obtaining accuracies of 81.22%, 80.15%, and 83.27% across three transfer tasks, and surpassing several baseline methods. Furthermore, the analysis highlights the Gamma frequency band as the most discriminative and identifies the central-parietal and prefrontal brain regions as critical for reliable emotion recognition.

</details>


### [149] [VL-RouterBench: A Benchmark for Vision-Language Model Routing](https://arxiv.org/abs/2512.23562)
*Zhehao Huang,Baijiong Lin,Jingyuan Zhang,Jingying Wang,Yuhang Liu,Ning Lu,Tao Li,Xiaolin Huang*

Main category: cs.LG

TL;DR: VL-RouterBench：首个系统性、可复现的视觉语言模型路由基准，覆盖14个数据集、17个模型、30,540个样本，评估10种路由方法，发现路由性能显著提升但仍与理想Oracle存在差距。


<details>
  <summary>Details</summary>
Motivation: 多模型路由已从工程技术发展为关键基础设施，但现有工作缺乏系统性、可复现的基准来评估视觉语言模型（VLM）路由系统。

Method: 基于VLM原始推理和评分日志构建样本-模型对的质量和成本矩阵；覆盖14个数据集（3个任务组）、30,540个样本，包含15个开源模型和2个API模型；评估协议联合测量平均准确率、平均成本和吞吐量，构建归一化成本与准确率的调和平均数作为排名分数。

Result: 评估10种路由方法和基线，观察到显著的路由性能增益；但当前最佳路由器与理想Oracle仍存在明显差距，表明通过更精细的视觉线索和文本结构建模，路由器架构仍有很大改进空间。

Conclusion: VL-RouterBench为多模态路由研究提供了系统性评估基准，将开源完整数据构建和评估工具链以促进可比性、可复现性和实际部署。

Abstract: Multi-model routing has evolved from an engineering technique into essential infrastructure, yet existing work lacks a systematic, reproducible benchmark for evaluating vision-language models (VLMs). We present VL-RouterBench to assess the overall capability of VLM routing systems systematically. The benchmark is grounded in raw inference and scoring logs from VLMs and constructs quality and cost matrices over sample-model pairs. In scale, VL-RouterBench covers 14 datasets across 3 task groups, totaling 30,540 samples, and includes 15 open-source models and 2 API models, yielding 519,180 sample-model pairs and a total input-output token volume of 34,494,977. The evaluation protocol jointly measures average accuracy, average cost, and throughput, and builds a ranking score from the harmonic mean of normalized cost and accuracy to enable comparison across router configurations and cost budgets. On this benchmark, we evaluate 10 routing methods and baselines and observe a significant routability gain, while the best current routers still show a clear gap to the ideal Oracle, indicating considerable room for improvement in router architecture through finer visual cues and modeling of textual structure. We will open-source the complete data construction and evaluation toolchain to promote comparability, reproducibility, and practical deployment in multimodal routing research.

</details>


### [150] [Distribution-Free Process Monitoring with Conformal Prediction](https://arxiv.org/abs/2512.23602)
*Christopher Burger*

Main category: cs.LG

TL;DR: 提出一种结合传统统计过程控制与保形预测的混合框架，通过可视化过程不确定性和重构多变量控制为异常检测问题，增强质量控制的鲁棒性和统计严谨性。


<details>
  <summary>Details</summary>
Motivation: 传统统计过程控制依赖于经常被违反的统计假设，在现代复杂制造环境中导致监控不可靠，需要更鲁棒的方法来应对这些局限性。

Method: 提出混合框架，将保形预测的分布无关、模型不可知保证集成到SPC中，包括保形增强控制图（可视化过程不确定性）和保形增强过程监控（将多变量控制重构为异常检测问题）。

Result: 该框架提供了更鲁棒和统计严谨的质量控制方法，同时保持了经典方法的可解释性和易用性，能够实现主动信号如"不确定性尖峰"。

Conclusion: 通过集成保形预测，该混合框架克服了传统SPC的局限性，为现代复杂制造环境提供了更可靠的质量监控解决方案。

Abstract: Traditional Statistical Process Control (SPC) is essential for quality management but is limited by its reliance on often violated statistical assumptions, leading to unreliable monitoring in modern, complex manufacturing environments. This paper introduces a hybrid framework that enhances SPC by integrating the distribution free, model agnostic guarantees of Conformal Prediction. We propose two novel applications: Conformal-Enhanced Control Charts, which visualize process uncertainty and enable proactive signals like 'uncertainty spikes', and Conformal-Enhanced Process Monitoring, which reframes multivariate control as a formal anomaly detection problem using an intuitive p-value chart. Our framework provides a more robust and statistically rigorous approach to quality control while maintaining the interpretability and ease of use of classic methods.

</details>


### [151] [Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning](https://arxiv.org/abs/2512.23617)
*Deniz Akdemir*

Main category: cs.LG

TL;DR: 本文提出基于Le Cam统计实验理论的新框架，用方向性可模拟性替代对称不变性，通过Le Cam失真度量化迁移风险，在保持源域性能的同时实现安全迁移。


<details>
  <summary>Details</summary>
Motivation: 传统无监督域自适应（UDA）方法强制特征不变性，在域信息不等（如传感器质量差异）时会导致信息破坏和负迁移，这在安全关键应用中可能造成灾难性后果。

Method: 基于Le Cam统计实验理论，提出决策理论框架，用方向性可模拟性替代对称不变性，引入Le Cam失真度（由缺陷距离δ(E₁,E₂)量化）作为迁移风险的上界，学习从源域模拟目标域的核函数。

Result: 在五个实验中取得显著成果：1）HLA基因组学中近乎完美的频率估计（相关性r=0.999）；2）CIFAR-10图像分类中零源域效用损失（保持81.2%准确率，而CycleGAN下降34.7%）；3）RL控制中安全策略迁移，而基于不变性的方法出现灾难性崩溃。

Conclusion: Le Cam失真度为医学影像、自主系统和精准医学等负迁移不可接受的领域提供了首个风险可控迁移学习的理论框架，实现了安全迁移而不降低源域性能。

Abstract: Distribution shift is the defining challenge of real-world machine learning. The dominant paradigm--Unsupervised Domain Adaptation (UDA)--enforces feature invariance, aligning source and target representations via symmetric divergence minimization [Ganin et al., 2016]. We demonstrate that this approach is fundamentally flawed: when domains are unequally informative (e.g., high-quality vs degraded sensors), strict invariance necessitates information destruction, causing "negative transfer" that can be catastrophic in safety-critical applications [Wang et al., 2019].
  We propose a decision-theoretic framework grounded in Le Cam's theory of statistical experiments [Le Cam, 1986], using constructive approximations to replace symmetric invariance with directional simulability. We introduce Le Cam Distortion, quantified by the Deficiency Distance $δ(E_1, E_2)$, as a rigorous upper bound for transfer risk conditional on simulability. Our framework enables transfer without source degradation by learning a kernel that simulates the target from the source. Across five experiments (genomics, vision, reinforcement learning), Le Cam Distortion achieves: (1) near-perfect frequency estimation in HLA genomics (correlation $r=0.999$, matching classical methods), (2) zero source utility loss in CIFAR-10 image classification (81.2% accuracy preserved vs 34.7% drop for CycleGAN), and (3) safe policy transfer in RL control where invariance-based methods suffer catastrophic collapse. Le Cam Distortion provides the first principled framework for risk-controlled transfer learning in domains where negative transfer is unacceptable: medical imaging, autonomous systems, and precision medicine.

</details>


### [152] [BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization](https://arxiv.org/abs/2512.23631)
*Iris Xu,Guangtao Zeng,Zexue He,Charles Jin,Aldo Pareja,Dan Gutfreund,Chuang Gan,Zhang-Wei Hong*

Main category: cs.LG

TL;DR: BOAD框架通过多臂老虎机优化自动发现分层多智能体系统，显著提升大语言模型在长视野软件工程任务上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有单一智能体系统在处理真实世界软件工程问题时，由于需要在整个工作流中保留无关上下文，导致虚假相关性和泛化能力差。受人类工程师分解复杂问题启发，需要设计能够自动发现有效分层结构的智能体系统。

Method: 提出BOAD框架，将分层发现建模为多臂老虎机问题，每个臂代表候选子智能体，奖励衡量其与其他智能体协作时的帮助程度。该框架在有限评估预算下高效探索子智能体设计，构建协调专业子智能体（如定位、编辑、验证）的编排器。

Result: 在SWE-bench-Verified上优于单一智能体和手动设计的多智能体系统。在SWE-bench-Live上，36B系统在评估时排名第二，超越了GPT-4和Claude等更大模型。

Conclusion: 自动发现的分层多智能体系统能显著提升大语言模型在具有挑战性的长视野软件工程任务上的泛化能力，为复杂问题解决提供了有效框架。

Abstract: Large language models (LLMs) have shown strong reasoning and coding capabilities, yet they struggle to generalize to real-world software engineering (SWE) problems that are long-horizon and out of distribution. Existing systems often rely on a single agent to handle the entire workflow-interpreting issues, navigating large codebases, and implementing fixes-within one reasoning chain. Such monolithic designs force the model to retain irrelevant context, leading to spurious correlations and poor generalization. Motivated by how human engineers decompose complex problems, we propose structuring SWE agents as orchestrators coordinating specialized sub-agents for sub-tasks such as localization, editing, and validation. The challenge lies in discovering effective hierarchies automatically: as the number of sub-agents grows, the search space becomes combinatorial, and it is difficult to attribute credit to individual sub-agents within a team. We address these challenges by formulating hierarchy discovery as a multi-armed bandit (MAB) problem, where each arm represents a candidate sub-agent and the reward measures its helpfulness when collaborating with others. This framework, termed Bandit Optimization for Agent Design (BOAD), enables efficient exploration of sub-agent designs under limited evaluation budgets. On SWE-bench-Verified, BOAD outperforms single-agent and manually designed multi-agent systems. On SWE-bench-Live, featuring more recent and out-of-distribution issues, our 36B system ranks second on the leaderboard at the time of evaluation, surpassing larger models such as GPT-4 and Claude. These results demonstrate that automatically discovered hierarchical multi-agent systems significantly improve generalization on challenging long-horizon SWE tasks. Code is available at https://github.com/iamxjy/BOAD-SWE-Agent.

</details>


### [153] [Random Controlled Differential Equations](https://arxiv.org/abs/2512.23670)
*Francesco Piatti,Thomas Cass,William F. Turner*

Main category: cs.LG

TL;DR: 提出结合随机特征与控制微分方程的训练高效时间序列学习框架，仅训练线性读出层，实现快速可扩展模型


<details>
  <summary>Details</summary>
Motivation: 传统时间序列学习方法计算成本高，特别是显式签名计算复杂度高，需要更高效的方法来保持签名理论的归纳偏置

Method: 使用大型随机参数化CDE作为连续时间储层，仅训练线性读出层；提出RF-CDE（随机傅里叶特征提升）和R-RDE（粗糙路径直接处理）两种变体

Result: 在多个时间序列基准测试中展示竞争性或最先进性能，证明无限宽度极限下模型分别诱导RBF提升签名核和粗糙签名核

Conclusion: 提供显式签名计算的实用替代方案，保持签名理论的归纳偏置，同时受益于随机特征的高效性，统一了随机特征储层、连续时间深度架构和路径签名理论

Abstract: We introduce a training-efficient framework for time-series learning that combines random features with controlled differential equations (CDEs). In this approach, large randomly parameterized CDEs act as continuous-time reservoirs, mapping input paths to rich representations. Only a linear readout layer is trained, resulting in fast, scalable models with strong inductive bias. Building on this foundation, we propose two variants: (i) Random Fourier CDEs (RF-CDEs): these lift the input signal using random Fourier features prior to the dynamics, providing a kernel-free approximation of RBF-enhanced sequence models; (ii) Random Rough DEs (R-RDEs): these operate directly on rough-path inputs via a log-ODE discretization, using log-signatures to capture higher-order temporal interactions while remaining stable and efficient. We prove that in the infinite-width limit, these model induces the RBF-lifted signature kernel and the rough signature kernel, respectively, offering a unified perspective on random-feature reservoirs, continuous-time deep architectures, and path-signature theory.
  We evaluate both models across a range of time-series benchmarks, demonstrating competitive or state-of-the-art performance. These methods provide a practical alternative to explicit signature computations, retaining their inductive bias while benefiting from the efficiency of random features.

</details>


### [154] [End-to-End Test-Time Training for Long Context](https://arxiv.org/abs/2512.23675)
*Arnuv Tandon,Karan Dalal,Xinhao Li,Daniel Koceja,Marcel Rød,Sam Buchanan,Xiaolong Wang,Jure Leskovec,Sanmi Koyejo,Tatsunori Hashimoto,Carlos Guestrin,Jed McCaleb,Yejin Choi,Yu Sun*

Main category: cs.LG

TL;DR: 论文提出一种将长上下文语言建模视为持续学习问题的新方法，使用标准Transformer架构配合滑动窗口注意力，在测试时通过下一个token预测持续学习，将上下文压缩到模型权重中。


<details>
  <summary>Details</summary>
Motivation: 传统长上下文语言建模主要关注架构设计（如稀疏注意力、状态空间模型等），但本文提出将问题重新定义为持续学习问题，让模型在测试时也能学习给定的上下文，从而更有效地处理长序列。

Method: 使用标准Transformer架构配合滑动窗口注意力，在测试时通过下一个token预测进行持续学习（压缩上下文到权重）。在训练时通过元学习优化模型的初始化，使其在测试时学习更高效。这是一种端到端的测试时训练方法。

Result: 对于3B模型在164B tokens上训练，该方法在上下文长度扩展方面与全注意力Transformer表现相似，但推理延迟恒定（与RNN类似）。在128K上下文长度下，比全注意力快2.7倍。在扩展性方面优于Mamba 2和Gated DeltaNet。

Conclusion: 将长上下文建模重新定义为持续学习问题，使用测试时训练方法，既能保持与全注意力Transformer相似的扩展性，又能实现恒定推理延迟，为长序列处理提供了新的有效途径。

Abstract: We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture -- a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the model's initialization for learning at test time via meta-learning at training time. Overall, our method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with a focus on scaling properties. In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7 times faster than full attention for 128K context. Our code is publicly available.

</details>


### [155] [Training AI Co-Scientists Using Rubric Rewards](https://arxiv.org/abs/2512.23707)
*Shashwat Goel,Rishi Hazra,Dulhan Jayalath,Timon Willi,Parag Jain,William F. Shen,Ilias Leontiadis,Francesco Barbieri,Yoram Bachrach,Jonas Geiping,Chenxi Whitehouse*

Main category: cs.LG

TL;DR: 利用现有研究论文训练语言模型生成更好的研究计划，通过强化学习和自我评分实现自动化训练，在机器学习和医学领域均取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在生成符合所有约束和隐含要求的研究计划方面存在困难，需要开发能够更好协助人类研究者的AI共同科学家。

Method: 从多领域论文中自动提取研究目标和目标特定评分标准构建训练语料库，通过强化学习结合自我评分训练模型，使用初始策略的冻结副本作为评分器。

Result: 专家在70%的研究目标上更喜欢微调后的Qwen3-30B-A3B模型生成的计划，84%的自动提取评分标准获得批准；在医学和新arXiv预印本领域也取得12-22%的相对改进。

Conclusion: 该方法展示了可扩展的自动化训练方法在改进通用AI共同科学家方面的潜力，即使在医学研究等执行反馈不可行的问题设置中也有效。

Abstract: AI co-scientists are emerging as a tool to assist human researchers in achieving their research goals. A crucial feature of these AI co-scientists is the ability to generate a research plan given a set of aims and constraints. The plan may be used by researchers for brainstorming, or may even be implemented after further refinement. However, language models currently struggle to generate research plans that follow all constraints and implicit requirements. In this work, we study how to leverage the vast corpus of existing research papers to train language models that generate better research plans. We build a scalable, diverse training corpus by automatically extracting research goals and goal-specific grading rubrics from papers across several domains. We then train models for research plan generation via reinforcement learning with self-grading. A frozen copy of the initial policy acts as the grader during training, with the rubrics creating a generator-verifier gap that enables improvements without external human supervision. To validate this approach, we conduct a study with human experts for machine learning research goals, spanning 225 hours. The experts prefer plans generated by our finetuned Qwen3-30B-A3B model over the initial model for 70% of research goals, and approve 84% of the automatically extracted goal-specific grading rubrics. To assess generality, we also extend our approach to research goals from medical papers, and new arXiv preprints, evaluating with a jury of frontier models. Our finetuning yields 12-22% relative improvements and significant cross-domain generalization, proving effective even in problem settings like medical research where execution feedback is infeasible. Together, these findings demonstrate the potential of a scalable, automated training recipe as a step towards improving general AI co-scientists.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [156] [The Solution of Potential-Driven, Steady-State Nonlinear Network Flow Equations via Graph Partitioning](https://arxiv.org/abs/2512.22124)
*Shriram Srinivasan,Kaarthik Sundar*

Main category: physics.comp-ph

TL;DR: 提出一种基于网络分区的大规模管网非线性方程求解算法，通过局部子系统求解实现全局解，保护不同运营商的数据隐私


<details>
  <summary>Details</summary>
Motivation: 大规模管网（如天然气、水管网）的稳态流动非线性方程组求解随网络规模增大而变得困难，需要处理不同运营商之间的数据隐私问题

Method: 将网络按可处理大小分区，通过局部求解子系统计算全局解，仅在互连点共享数据，运营商可自主选择求解方法

Result: 算法与Schur补方法相关，在具有挑战性的测试案例中证明了方法的可行性

Conclusion: 提出的分区算法能有效解决大规模管网非线性方程，同时保护不同运营商的数据隐私，具有实际应用价值

Abstract: The solution of potential-driven steady-state flow in large networks is required in various engineering applications, such as transport of natural gas or water through pipeline networks. The resultant system of nonlinear equations depends on the network topology, and its solution grows more challenging as the network size increases. We present an algorithm that utilizes a given partition of a network into tractable sizes to compute a global solution for the full nonlinear system through local solution of smaller subsystems induced by the partitions. When the partitions are induced by interconnects or transfer points corresponding to networks owned by different operators, the method ensures data is shared solely at the interconnects, leaving network operators free to solve the network flow system corresponding to their own domain in any manner of their choosing. The proposed method is shown to be connected to the Schur complement and the method's viability demonstrated on some challenging test cases.

</details>


### [157] [A Radiation Exchange Factor Transformation with Proven Convergence, Non-Negativity, and Energy Conservation](https://arxiv.org/abs/2512.22157)
*Nikolaj Maack Bielefeld*

Main category: physics.comp-ph

TL;DR: 提出一种基于矩阵的交换因子变换方法，用于求解一般域上耦合混合边界条件的辐射传递问题，通过Neumann级数解析追踪所有反射-散射路径，确保收敛性、非负辐射和精确能量守恒。


<details>
  <summary>Details</summary>
Motivation: 解决一般域上耦合混合边界条件辐射传递问题的计算挑战，特别是针对参与性介质（透明、吸收、发射、散射）和边界条件（吸收、反射）的复杂情况。同时发现并修正了经典Hottel区域方法中的不一致性。

Method: 基于矩阵的交换因子变换方法：给定首次相互作用交换因子矩阵F，通过Neumann级数变换生成吸收矩阵A和多重反射-散射矩阵R，解析追踪所有反射-散射路径至稳态。建立了严格的收敛条件保证。

Result: 方法保证了收敛性、非负辐射和精确到机器精度的能量守恒。与扩散近似在高消光极限下吻合，与Crosbie和Schrenker的纯散射和部分散射结果一致。发现了Noble的Hottel区域方法矩阵公式中的不一致性并予以修正。

Conclusion: 该方法适用于中等规模的一般反射-散射问题，当反射-散射可忽略且高消光确保矩阵稀疏时，可扩展到大规模问题。为辐射传递计算提供了精确、可靠且可扩展的解决方案。

Abstract: This paper presents a matrix-based exchange factor transformation for solving coupled mixed boundary condition radiative transfer problems on general domains. The method applies to participating media ranging from transparent to absorbing, emitting, and scattering, with boundaries ranging from absorbing to reflecting. Given a first-interaction exchange factor matrix $\mathbf{F}$, the transformation produces an absorption matrix $\mathbf{A}$ and a multiple reflection-scattering matrix $\mathbf{R}$ through a Neumann series that analytically traces all reflection-scattering paths to steady state. The paper establishes rigorous conditions under which the method guarantees convergence, non-negative radiation, and exact energy conservation to machine precision. A comparison with Noble's matrix formulation of Hottel's zonal method reveals a previously unidentified discrepancy in that classical approach; the proposed transformation eliminates this discrepancy. The method is validated against the diffusion approximation in the high-extinction limit and against results of Crosbie and Schrenker for pure and partial scattering cases. The method is applicable to medium-scale general reflecting-scattering problems and scales to large problems when negligible reflection-scattering and high extinction ensure matrix sparsity.

</details>


### [158] [Integrating Wide and Deep Neural Networks with Squeeze-and-Excitation Blocks for Multi-Target Property Prediction in Additively Manufactured Fiber Reinforced Composites](https://arxiv.org/abs/2512.22397)
*Behzad Parvaresh,Rahmat K. Adesunkanmi,Adel Alaeddini*

Main category: physics.comp-ph

TL;DR: 提出一种结合拉丁超立方采样和SE-WDNN神经网络的数据高效多目标学习方法，用于预测连续纤维增强复合材料的机械和制造性能


<details>
  <summary>Details</summary>
Motivation: 连续纤维增强复合材料的性能对工艺和材料参数交互敏感，全面实验测试不切实际，需要数据高效的多目标预测方法

Method: 采用拉丁超立方采样指导实验设计，结合挤压激励宽深神经网络（SE-WDNN），基于制造参数联合预测多个机械和制造性能

Result: 模型在155个样本上实现最低总体测试误差（MAPE=12.33%），统计显著优于基线模型，SHAP分析显示增强策略对机械性能影响最大

Conclusion: LHS与SE-WDNN结合实现了可解释且样本高效的多目标预测，能指导CFRC-AM参数选择，平衡机械行为和制造指标

Abstract: Continuous fiber-reinforced composite manufactured by additive manufacturing (CFRC-AM) offers opportunities for printing lightweight materials with high specific strength. However, their performance is sensitive to the interaction of process and material parameters, making exhaustive experimental testing impractical. In this study, we introduce a data-efficient, multi-input, multi-target learning approach that integrates Latin Hypercube Sampling (LHS)-guided experimentation with a squeeze-and-excitation wide and deep neural network (SE-WDNN) to jointly predict multiple mechanical and manufacturing properties of CFRC-AMs based on different manufacturing parameters. We printed and tested 155 specimens selected from a design space of 4,320 combinations using a Markforged Mark Two 3D printer. The processed data formed the input-output set for our proposed model. We compared the results with those from commonly used machine learning models, including feedforward neural networks, Kolmogorov-Arnold networks, XGBoost, CatBoost, and random forests. Our model achieved the lowest overall test error (MAPE = 12.33%) and showed statistically significant improvements over the baseline wide and deep neural network for several target variables (paired t-tests, p <= 0.05). SHapley Additive exPlanations (SHAP) analysis revealed that reinforcement strategy was the major influence on mechanical performance. Overall, this study demonstrates that the integration of LHS and SE-WDNN enables interpretable and sample-efficient multi-target predictions, guiding parameter selection in CFRC-AM with a balance between mechanical behavior and manufacturing metrics.

</details>


### [159] [A survey of interlayer interaction models for graphene and other 2D materials](https://arxiv.org/abs/2512.22670)
*Gourav Yadav,Shakti S. Gupta,Roger A. Sauer*

Main category: physics.comp-ph

TL;DR: 该论文综述了描述二维材料间范德华相互作用的力学模型，涵盖连续弹性体材料和离散晶体材料，分析了接触不稳定性、莫尔图案、表面重构和超润滑等现象。


<details>
  <summary>Details</summary>
Motivation: 二维材料（如石墨烯）中的范德华相互作用导致多种重要物理现象，需要系统梳理相关力学模型，为理解这些现象提供理论框架，并解决多尺度建模中的计算成本问题。

Method: 采用综述研究方法，系统分析连续和离散二维材料的范德华相互作用模型。首先讨论法向接触模型，然后分析切向接触模型，同时考虑原子尺度和连续介质方法，并探讨外部载荷和尺度变化对基态构型和摩擦接触行为的影响。

Result: 建立了二维材料范德华相互作用的统一力学模型框架，揭示了接触不稳定性、莫尔图案形成、表面重构和超润滑等现象的力学机制，并提出了降低多尺度建模计算成本的策略。

Conclusion: 该综述为理解二维材料中范德华相互作用引起的复杂物理现象提供了系统的力学模型框架，特别强调了多尺度建模中的计算效率优化策略，对二维材料力学行为研究具有重要指导意义。

Abstract: This work presents a survey of mechanical models describing van der Waals interactions between 2D materials, encompassing both continuous elastomer-like materials and discrete (crystalline) 2D materials such as graphene. These interactions give rise to a range of physical phenomena, including contact instabilities, Moiré patterns, surface reconstructions, and superlubricity. The underlying contact forces follow from the variation of an interfacial interaction potential. The presentation first discusses normal contact models, and then tangential contact models. Both atomistic and continuum approaches are considered. In addition, the influence of external loading and changes in length scale on the ground state configuration and frictional contact behavior are analyzed. A particular emphasis is placed on discussing strategies that reduce computational cost in multiscale modeling.

</details>


### [160] [Overcoming Computational Bottlenecks in Quantum Hydrodynamics: A Volume-Based Integral Formalism](https://arxiv.org/abs/2512.22920)
*Christos Mystilidis,Christos Tserkezis,Guy A. E. Vandenbosch,N. Asger Mortensen,Xuezhi Zheng*

Main category: physics.comp-ph

TL;DR: 提出基于体积积分方程（VIE）的数值方法，显著提高了自洽流体动力学模型（SC-HDM）的计算效率，突破了材料响应复杂度与计算成本之间的传统权衡


<details>
  <summary>Details</summary>
Motivation: 解决量子等离子体学中金属光学响应介观模型的计算瓶颈问题。虽然介观模型比从头算方法更高效，但复杂的材料响应仍然导致计算需求过高

Method: 采用体积积分方程（VIE）方法实现自洽流体动力学模型（SC-HDM），替代传统的微分方程（DE）方法。利用球形纳米颗粒的对称性，同时处理三种复杂度递增的材料模型

Result: VIE方法显著提高了计算效率，打破了"材料响应越复杂，模拟越耗时"的传统观念。能够直接从VIE实现中提取介观材料响应函数，避免了冗长的微观计算

Conclusion: 该方法为量子流体动力学纳米颗粒建模开辟了新途径，将成为处理更复杂几何形状方法的重要基准测试工具

Abstract: Mesoscopic models of the optical response of metals have emerged as fundamental building blocks in quantum plasmonics, in principle overcoming the computational bottlenecks of ab initio techniques by implementing aspects of the atomistic description of the metal in otherwise classical calculations. Nonetheless, even these approaches are eventually hindered by demanding computations due to sophisticated material response. Here, this issue is addressed for the advanced Self-Consistent Hydrodynamic Drude Model (SC-HDM), which captures both nonlocal electron dynamics and electron spill-out, through a Volume Integral Equation (VIE) method. Adopting an IE-based method shifts perspective from the commonly employed Differential Equation (DE)-based ones, demonstrating significant computational efficiency. The VIE approach is a valuable methodological scaffold: It addresses SC-HDM and simpler models, but can also be adapted to more advanced ones. For spherical nanoparticles (NPs), using the inherent symmetries, similar performance for three increasingly complicated material models is achieved, breaking the taboo that increased sophistication in material response requires taxing simulations. Mesoscopic material-response functions can be readily extracted from the VIE implementation, thus circumventing the need for lengthy microscopic calculations. This method opens a new way of modeling quantum hydrodynamic NPs and will serve as essential benchmarking tool for recipes addressing more complicated geometries.

</details>


### [161] [Masgent: An AI-assisted Materials Simulation Agent](https://arxiv.org/abs/2512.23010)
*Guanghen Liu,Songge Yang,Yu Zhong*

Main category: physics.comp-ph

TL;DR: Masgent是一个AI辅助的材料模拟代理平台，通过自然语言交互统一了结构操作、VASP输入生成、DFT工作流构建与分析、快速MLP模拟和轻量级机器学习工具，大幅简化材料模拟流程。


<details>
  <summary>Details</summary>
Motivation: 传统DFT和机器学习势模拟需要大量脚本编写、多步骤操作和HPC专业知识，这些挑战阻碍了研究的可重复性并减缓了发现速度。

Method: 基于大语言模型构建AI辅助材料模拟代理，在一个平台内集成结构操作、自动化VASP输入生成、DFT工作流构建与分析、快速MLP模拟和轻量级机器学习工具，通过自然语言交互简化操作。

Result: Masgent能够将模拟设置时间从数小时缩短到数秒，消除了大部分手动脚本编写，使研究人员能够通过自然语言交互执行复杂的模拟任务。

Conclusion: Masgent通过标准化协议和集成先进的模拟与数据驱动工具，使最先进的计算方法民主化，加速了新老研究人员的假设检验、预筛选和探索性研究。

Abstract: Density functional theory (DFT) and machine learning potentials (MLPs) are essential for predicting and understanding materials properties, yet preparing, executing, and analyzing these simulations typically requires extensive scripting, multi-step procedures, and significant high-performance computing (HPC) expertise. These challenges hinder reproducibility and slow down discovery. Here, we introduce Masgent, an AI-assisted materials simulation agent that unifies structure manipulation, automated VASP input generation, DFT workflow construction and analysis, fast MLP-based simulations, and lightweight machine learning (ML) utilities within a single platform. Powered by large language models (LLMs), Masgent enables researchers to perform complex simulation tasks through natural-language interaction, eliminating most manual scripting and reducing setup time from hours to seconds. By standardizing protocols and integrating advanced simulation and data-driven tools, Masgent democratizes access to state-of-the-art computational methodologies, accelerating hypothesis testing, pre-screening, and exploratory research for both new and experienced practitioners.

</details>


### [162] [Reconstructing Relativistic Magnetohydrodynamics with Physics-Informed Neural Networks](https://arxiv.org/abs/2512.23057)
*Corwin Cheung,Marcos Johnson-Noya,Michael Xiang,Dominic Chang,Alfredo Guevara*

Main category: physics.comp-ph

TL;DR: 构建首个相对论磁流体动力学PINN代理模型，采用混合PDE与数据驱动方法，直接处理原始变量的雅可比或PDE特征，并加入无散度条件，使用MUON优化器实现早期快照训练和外推。


<details>
  <summary>Details</summary>
Motivation: 相对论磁流体动力学模拟计算成本高，需要构建高效的代理模型。传统PINN方法在处理RMHD时面临挑战，特别是守恒形式和散度约束问题。

Method: 采用混合PDE与数据驱动工作流，不训练守恒形式方程，而是直接处理原始变量的雅可比或PDE特征。加入无散度条件而不需要清理模式。使用新颖的MUON优化器实现。

Result: 基线PINN在早期时间快照上训练后，能够在一维和二维空间外推RMHD动力学。后验残差引导网络能够系统性地减少PDE违反。

Conclusion: 成功构建了首个RMHD的PINN代理模型，展示了混合方法在复杂物理系统建模中的有效性，为高效RMHD模拟提供了新途径。

Abstract: We construct the first physics-informed neural-network (PINN) surrogates for relativistic magnetohydrodynamics (RMHD) using a hybrid PDE and data-driven workflow. Instead of training for the conservative form of the equations, we work with Jacobians or PDE characteristics directly in terms of primitive variables. We further add to the trainable system the divergence-free condition, without the need of cleaning modes. Using a novel MUON optimizer implementation, we show that a baseline PINN trained on early-time snapshots can extrapolate RMHD dynamics in one and two spatial dimensions, and that posterior residual-guided networks can systematically reduce PDE violations.

</details>


### [163] [Exponential divided differences via Chebyshev polynomials](https://arxiv.org/abs/2512.23061)
*Itay Hen*

Main category: physics.comp-ph

TL;DR: 提出基于切比雪夫多项式的算法，用于高效稳定计算高阶指数差分，适用于动态节点集，时间复杂度为O(qN)。


<details>
  <summary>Details</summary>
Motivation: 指数差分在数值线性代数、矩阵函数计算和量子蒙特卡洛模拟中作为时间演化和可观测量估计的核权重，但高效稳定地计算动态节点集的高阶指数差分仍是一个重大计算挑战。

Method: 结合指数函数的切比雪夫-贝塞尔展开与切比雪夫差分的直接递推关系，开发切比雪夫多项式算法，并针对动态节点集设计增量更新方案。

Result: 算法计算成本为O(qN)，其中N随谱宽度线性增长，q仅通过结构多项式约束影响复杂度；动态节点集的单节点插入/删除可在O(N)时间内完成。

Conclusion: 该方法解决了动态节点集高阶指数差分的高效稳定计算问题，提供了完整的C++参考实现。

Abstract: Exponential divided differences arise in numerical linear algebra, matrix-function evaluation, and quantum Monte Carlo simulations, where they serve as kernel weights for time evolution and observable estimation. Efficient and numerically stable evaluation of high-order exponential divided differences for dynamically evolving node sets remains a significant computational challenge. We present a Chebyshev-polynomial-based algorithm that addresses this problem by combining the Chebyshev-Bessel expansion of the exponential function with a direct recurrence for Chebyshev divided differences. The method achieves a computational cost of ${\cal O}(qN)$, where $q$ is the divided-difference order and $N$ is the Chebyshev truncation length. We show that $N$ scales linearly with the spectral width through the decay of modified Bessel coefficients, while the dependence on $q$ enters only through structural polynomial constraints. We further develop an incremental update scheme for dynamic node sets that enables the insertion or removal of a single node in ${\cal O}(N)$ time when the affine mapping interval is held fixed. A full \texttt{C++} reference implementation of the algorithms described in this work is publicly available.

</details>


### [164] [A space-time extension of a conservative two-fluid cut-cell diffusion method for moving geometries](https://arxiv.org/abs/2512.23358)
*Louis Libat,Can Selçuk,Eric Chénier,Vincent Le Chenadec*

Main category: physics.comp-ph

TL;DR: 提出一种时空扩展的保守笛卡尔切割单元有限体积法，用于处理规定运动几何中的两相扩散问题，通过时空控制体积实现移动边界的严格离散守恒。


<details>
  <summary>Details</summary>
Motivation: 需要处理固定笛卡尔网格上移动边界的两相扩散问题，同时确保严格的离散守恒性，特别是在切割单元创建和销毁（新鲜/死亡单元事件）时。

Method: 采用两流体方法，每相求解一个标量场，通过尖锐界面条件耦合。在相限制的时空控制体积上建立离散平衡，使用几何矩（扫掠体积和孔径）作为有限体积算子的权重，自然处理切割单元的创建和销毁。

Result: 在二维和三维验证测试中展示了超线性空间精度、在重复拓扑变化下的鲁棒行为，以及在强系数跳跃和移动界面下的守恒性。

Conclusion: 提出的时空切割单元框架为演化几何中的多相输运提供了保守构建块，并为未来自由边界扩展（如Stefan型相变）奠定了基础。

Abstract: We present a space-time extension of a conservative Cartesian cut-cell finite-volume method for two-phase diffusion in prescribed-motion geometries. The formulation follows a two-fluid approach: one scalar field is solved in each phase with discontinuous material properties, coupled by sharp interface conditions enforcing flux continuity and jump laws. To handle moving boundaries on a fixed Cartesian grid, the discrete balance is written over phase-restricted space-time control volumes, whose geometric moments (swept volumes and apertures) are used as weights in the finite-volume operators. This construction naturally accounts for the creation and destruction of cut cells (fresh/dead-cell events) and yields strict discrete conservation. The resulting scheme retains the algebraic structure of the static cut-cell formulation while incorporating motion through local geometric weights and interface coupling operators. A series of verification and validation tests in two and three dimensions demonstrate super-linear accuracy in space, robust behavior under repeated topology changes and conservation across strong coefficient jumps and moving interfaces. The proposed space-time cut-cell framework provides a conservative building block for multiphase transport in evolving geometries and a foundation for future free-boundary extensions such as Stefan-type phase change.

</details>


### [165] [PINNs for Electromagnetic Wave Propagation](https://arxiv.org/abs/2512.23396)
*Nilufer K. Bulut*

Main category: physics.comp-ph

TL;DR: 该研究提出了一种混合训练策略，通过时间推进、因果感知加权、界面连续性损失和局部坡印廷正则化，显著提升了PINNs在电磁波传播问题中的精度和能量守恒性能，使其达到与FDTD相当的竞争水平。


<details>
  <summary>Details</summary>
Motivation: 尽管PINNs具有无网格特性和适用于反问题的优势，但在电磁学领域，与成熟的FDTD和FEM方法相比，PINNs在精度和能量指标方面存在不足。本研究旨在通过改进训练策略，使PINNs在电磁波传播问题中达到与FDTD相当的精度和能量一致性。

Method: 提出混合训练策略：1) 采用时间推进和因果感知加权解决时间依赖PINN训练中的因果崩溃问题；2) 应用两阶段界面连续性损失缓解时间推进引入的不连续性；3) 开发局部坡印廷正则化抑制累积能量漂移。训练仅使用物理残差损失，无需标记场数据。

Result: 在2D PEC腔体场景中，PINN模型实现了高场精度：平均NRMSE为0.09%，L²误差为1.01%。能量守恒方面，相对能量失配仅为0.024%。训练完全基于物理残差损失，FDTD仅用于后训练评估。

Conclusion: 研究表明，通过混合训练策略，PINNs在典型电磁学示例中能够达到与FDTD竞争的结果，成为一种可行的替代方法，为电磁学领域的PINN应用提供了有前景的方向。

Abstract: Physics-Informed Neural Networks (PINNs) are a methodology that aims to solve physical systems by directly embedding PDE constraints into the neural network training process. In electromagnetism, where well-established methodologies such as FDTD and FEM already exist, new methodologies are expected to provide clear advantages to be accepted. Despite their mesh-free nature and applicability to inverse problems, PINNs can exhibit deficiencies in terms of accuracy and energy metrics when compared to FDTD solutions. This study demonstrates hybrid training strategies can bring PINNs closer to FDTD-level accuracy and energy consistency.
  This study presents a hybrid methodology addressing common challenges in wave propagation scenarios. The causality collapse problem in time-dependent PINN training is addressed via time marching and causality-aware weighting. In order to mitigate the discontinuities that are introduced by time marching, a two-stage interface continuity loss is applied. In order to suppress loss accumulation, which is manifested as cumulative energy drift in electromagnetic waves, a local Poynting-based regularizer has been developed.
  In the developed PINN model, high field accuracy is achieved with an average 0.09\% $NRMSE$ and 1.01\% $L^2$ error over time. Energy conservation is achieved on the PINN side with only a 0.024\% relative energy mismatch in the 2D PEC cavity scenario. Training is performed without labeled field data, using only physics-based residual losses; FDTD is used solely for post-training evaluation. The results demonstrate that PINNs can achieve competitive results with FDTD in canonical electromagnetic examples and are a viable alternative.

</details>


### [166] [MultiAtomLiouvilleEquationGenerator: A Mathematica package for Liouville superoperators and master equations of multilevel atomic systems](https://arxiv.org/abs/2512.23591)
*Pablo Yanes-Thomas,Rocío Jáuregui-Renaud Santiago F. Caballero-Benítez,Daniel Sahagún Sánchez,Alejandro Kunold*

Main category: physics.comp-ph

TL;DR: MulAtoLEG是一个开源的Mathematica软件包，用于生成多能级原子系统的刘维尔超算符和方程，支持任意数量的原子和复杂跃迁构型。


<details>
  <summary>Details</summary>
Motivation: 需要一种系统性的方法来处理多原子、多能级系统的量子主方程，特别是碱金属原子的复杂跃迁构型。现有方法主要针对两能级系统，缺乏对多能级系统的通用支持。

Method: 基于Lehmberg的伴随主方程方法扩展到多能级系统，利用Mathematica的向量化和稀疏线性代数功能，生成精确的刘维尔方程，支持裸态和缀饰态基。

Result: 开发了一个开源软件包，能够自动生成多原子系统的精确主方程和伴随主方程，支持复杂跃迁构型，并能在缀饰态基中显式确定非幺正演化算符。

Conclusion: MulAtoLEG为多能级原子系统的量子动力学研究提供了强大的计算工具，虽然系统规模受计算资源限制，但能生成精确方程而无需近似。

Abstract: MulAtoLEG (Multi-Atom Liouville Equation Generator) is an open-source Mathematica package for generating Liouville superoperators and Liouville equations, specialized for multilevel atomic systems comprising an arbitrary number of atoms. This scheme is based on an extension to multilevel atomic systems, originally developed by Lehmberg [R. H. Lehmberg, Phys. Rev. A 2, 883 (1970)] as an adjoint master equation for ensembles of two-level emitters and later reformulated by Genes [M. Reitz, C. Sommer and C. Genes, PRX Quantum 3, 010201 (2022)] as a master equation. The package facilitates the generation of equations for complex transition configurations in alkali atoms. Although primarily designed for atomic systems, it can also generate the master and adjoint master equations for general Hamiltonians and Lindbladians. In addition, it includes functionalities to construct the differential equations in the dressed-state basis, where, in many cases, the non-unitary evolution operator can be determined explicitly. To maximize computational efficiency, the package leverages Mathematica's vectorization and sparse linear algebra capabilities. Since MulAtoLEG produces exact equations without approximations, the feasible system size is naturally limited by the available computational resources.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [167] [Matter environments around black holes: geodesics, light rings and ultracompact configurations](https://arxiv.org/abs/2512.22267)
*Dylan S. Fonseca,Caio F. B. Macedo,Mateus Malato Corrêa,Diego Rubiera-Garcia*

Main category: gr-qc

TL;DR: 研究暗物质分布对黑洞几何、测地线结构和环降现象的影响，发现环境效应会改变ISCO和光环位置，在高度致密时产生额外光环和次级视界，并在环降信号中留下特征印记。


<details>
  <summary>Details</summary>
Motivation: 天体物理黑洞总是嵌入在物质环境中，这些环境的引力影响会改变时空的关键强场特征。研究暗物质分布如何影响黑洞几何、测地线结构和环降现象，对于解释即将到来的电磁和引力波观测至关重要。

Method: 使用爱因斯坦星团模型构建自洽时空，研究三种广泛使用的密度剖面（Hernquist、NFW、Jaffe模型）。分析它们如何修改圆形类时和零测地线的位置和稳定性，包括最内稳定圆轨道（ISCO）和光环。通过标量扰动的时间演化，研究这些结构如何在环降信号中留下特征印记。

Result: 在低致密性区域，环境效应通常使ISCO向内移动，主光环向外移动，导致相关轨道频率和李雅普诺夫指数的参数偏差。在更高致密性时，出现额外光环、边缘稳定轨道和次级视界。时间演化显示这些结构会在环降信号中留下特征印记，包括长寿命的被困模式和与多个势垒相关的回声样调制。

Conclusion: 研究结果为评估黑洞周围环境效应提供了一个统一框架，并强调了物质诱导修正对于解释即将到来的电磁和引力波观测的重要性。暗物质分布可以显著改变黑洞的强场特征，并在环降信号中产生可观测的特征。

Abstract: Astrophysical black holes are invariably embedded in matter environments whose gravitational influence can alter key strong-field features of the spacetime. In this work, we investigate the impact of spherically symmetric dark-matter distributions on black hole geometry, geodesic structure, and ringdown phenomenology. Modeling the surrounding matter through Einstein clusters, we construct self-consistent spacetimes for three widely used density profiles - the Hernquist, Navarro-Frenk-White (NFW), and Jaffe models - and examine how their near-horizon behavior modifies the location and stability of circular timelike and null geodesics, including the innermost stable circular orbit (ISCO) and light rings. In the low-compactness regime, we derive analytical expressions showing that environmental effects generically shift the ISCO inward and the principal light ring outward, leading to parametric deviations in their associated orbital frequencies and Lyapunov exponents. At higher compactness, we explore the emergence of additional light rings, marginally stable orbits, and secondary horizons, identifying the regions of parameter space in which these ultracompact configurations arise. Using time-domain evolutions of scalar perturbations, we demonstrate how such structures can imprint characteristic signatures on the ringdown signal, including long-lived trapped modes and echo-like modulations associated with multiple potential barriers. Our results provide a unified framework for assessing environmental effects around black holes and highlight the importance of matter-induced corrections for interpreting upcoming electromagnetic and gravitational-wave observations.

</details>


### [168] [Oscillating kink behavior in a traversable wormhole](https://arxiv.org/abs/2512.22281)
*Tian-Chi Ma,Hai-Qing Zhang*

Main category: gr-qc

TL;DR: 研究球形对称径向畴壁（扭结）在Simpson-Visser虫洞背景下的时间演化，发现虫洞喉部参数a对畴壁动力学有显著影响：较大的a允许畴壁以较大振幅在喉部振荡，较小的a则将运动限制在喉部附近。


<details>
  <summary>Details</summary>
Motivation: 研究虫洞几何如何影响拓扑缺陷（畴壁）的运动和能量传递，为奇异时空中的拓扑缺陷动力学提供新见解。

Method: 通过数值求解具有双阱势的标量场方程，研究球形对称径向畴壁在Simpson-Visser虫洞背景下的时间演化。

Result: 虫洞喉部参数a对畴壁动力学有强烈影响：较大的a导致更宽的喉部，允许畴壁以较大振幅在喉部振荡；较小的a将运动限制在喉部附近。每次畴壁穿过虫洞喉部时都会发射标量波包，导致振荡振幅逐渐减小。

Conclusion: 虫洞几何显著影响拓扑缺陷的运动和能量传递，为奇异时空中的缺陷动力学提供了新见解，揭示了喉部参数如何调控畴壁振荡和能量耗散过程。

Abstract: We investigate the time evolution of spherically symmetric radial domain walls (kinks) in the background of a two-way traversable Simpson-Visser wormhole. By numerically solving the scalar-field equation with a double-well potential, we show that the wormhole throat parameter $a$ has a strong impact on the dynamics of the radial kink: larger $a$ leads to wider throats and allows the domain wall to oscillate across the throat with large amplitude, whereas smaller $a$ confines the motion nearby the throat. In addition, each time the kink crosses the wormhole throat, it emits scalar wave packets, causing its oscillation amplitude to gradually decrease. Our results reveal how the wormhole geometry influences the motion of defects and their energy transfer, providing new insights into the dynamics of topological defects in exotic spacetime.

</details>


### [169] [Topological Quantum Gravity through Harmonic S$^{2}$ Maps](https://arxiv.org/abs/2512.22289)
*M. Halilsoy,S. Habib Mazharimousavi*

Main category: gr-qc

TL;DR: 该论文提出基于二维球面调和映射的时空拓扑量子化理论，所有物理量自然呈现离散特性，并在黑洞、非黑洞和虫洞几何中验证了量子毛发的有效性。


<details>
  <summary>Details</summary>
Motivation: 探索时空的量子性质，建立物理量的离散化理论基础，研究在黑洞、非黑洞和虫洞等不同几何结构中的量子效应。

Method: 利用二维球面(S²)上的调和映射理论，提出时空拓扑量子化方法，分析Schwarzschild黑洞、非黑洞和虫洞几何中的量子毛发效应。

Result: 所有物理量自然呈现离散特性，量子毛发在不同几何结构中有效，温度计或曲率探测设备可以记录时空的宏观量子性。

Conclusion: 调和映射理论为时空量子化提供了自然框架，物理量的离散性和量子毛发效应揭示了时空的宏观量子特性，为量子引力研究提供了新视角。

Abstract: By virtue of harmonic maps on two-dimensional spheres (S$^{2}$), a topological quantization in spacetime is proposed. The discrete character of all physical quantities follows naturally. A Schwarzschild black hole, non-black hole and wormhole based geometries are considered in which a quantum hair becomes effective. A thermometer or curvature-detecting device can record the macroscopic quantumness of spacetime.

</details>


### [170] [Accelerating FJNW Metric](https://arxiv.org/abs/2512.22328)
*Homayon Anjomshoa,Behrouz Mirza,Alireza Azizallahi*

Main category: gr-qc

TL;DR: 通过微扰方法推导出加速FJNW度规的精确形式，并利用Buchdahl变换得到相同结果，研究该时空的奇点、测地线和圆形轨道稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究加速Fisher-Janis-Newman-Winicour (FJNW)度规的精确形式，探索其在广义相对论中的几何特性和物理意义。

Method: 采用简单的微扰方法推导加速FJNW度规的精确形式，同时使用Buchdahl变换验证结果的一致性，并分析时空奇点、测地线运动和圆形轨道稳定性。

Result: 成功推导出加速FJNW度规的精确表达式，通过两种方法得到一致结果，分析了该时空的奇点结构及其对全局和局部几何的影响，研究了测地线特性和圆形轨道的稳定性条件。

Conclusion: 建立了加速FJNW度规的完整理论框架，揭示了该时空的几何特性，为研究加速引力场中的标量场解提供了重要基础。

Abstract: We derive exact form of accelerating Fisher-Janis-Newman-Winicour (FJNW) metric by a simple perturbative method. We also argue that by using Buchdahl transformations one can obtain the same accelerating FJNW metric. We investigate singularities of the accelerating FJNW metric and study their effects on global and local structures of this spacetime. We also study geodesics and stability of circular orbits.

</details>


### [171] [Traversable ghost wormholes](https://arxiv.org/abs/2512.22361)
*Alberto Guilabert,Ernesto Fuenmayor,Pedro Bargueño,Ernesto Contreras*

Main category: gr-qc

TL;DR: 论文研究了"幽灵星"这种总质量任意小的致密天体配置，发现它们与可穿越虫洞物理存在自然联系，但超出球对称性时会遇到拓扑障碍。


<details>
  <summary>Details</summary>
Motivation: 幽灵星配置需要负能量密度区域，这在传统恒星模型中通常被视为非物理的。然而，负能量密度在可穿越虫洞几何中自然出现，这表明幽灵星配置可能在虫洞物理中找到自然实现。

Method: 通过分析幽灵星配置相关的霍金质量来研究其存在性。虽然球对称情况下Misner质量和霍金质量已知一致，但将幽灵条件扩展到非球对称情况并应用于霍金质量时，会遇到拓扑障碍。

Result: 证明了卡西米尔类可穿越虫洞可以在这个框架内自然构建。通过分析其彭罗斯-卡特图来展示所得几何的特性。

Conclusion: 幽灵星配置与虫洞物理存在深刻联系，但在非球对称情况下会遇到拓扑障碍，阻碍其直接实现。卡西米尔类虫洞为此类配置提供了具体实例。

Abstract: Ghost stars are compact configurations characterized by an arbitrarily small total mass. Such objects require regions of negative energy density -a condition typically regarded as unphysical within the context of conventional stellar models. Nevertheless, negative energy densities arise naturally in traversable wormhole geometries, where the violation of the null energy condition is essential to sustain the flaring-out behavior at the throat. This connection suggests that ghost-like configurations may find a natural realization within wormhole physics. In this work, we investigate the existence of ghost configurations by analyzing their associated Hawking mass. Although in spherical symmetry the Misner and Hawking masses are known to coincide, we show that when the ghost condition is extended beyond spherical symmetry and applied to the Hawking mass, it faces topological obstructions that hinder its straightforward realization. As a concrete example, we demonstrate that a Casimir-like traversable wormhole can be naturally constructed within this framework. Finally, to illustrate the properties of the resulting geometry, we analyze its Penrose-Carter diagram.

</details>


### [172] [Canonical description of Pontryagin and Euler classes with a Barbero-Immirzi parameter](https://arxiv.org/abs/2512.22400)
*Alberto Escalante,Edmundo Suárez-Polo,Luis A. Huerta-del Campo*

Main category: gr-qc

TL;DR: 对含Barbero-Immirzi参数的Pontryagin和Euler类进行正则分析，通过引入类Holst变量重写拓扑不变量，研究约束系统并计算物理自由度


<details>
  <summary>Details</summary>
Motivation: 研究含Barbero-Immirzi参数的拓扑不变量（Pontryagin和Euler类）的正则结构，探索其约束系统和对称性，为理解这些拓扑不变量在规范理论中的作用提供理论基础

Method: 引入类Holst变量重写拓扑不变量，进行详细的正则分析，研究所有约束条件，分析约束的可约性，计算物理自由度，并耦合到Holst作用中进行分析

Result: 获得了完整的正则结构和理论对称性，计算了物理自由度并识别了约束间的可约性条件。特别发现当BI参数取γ=±i时，可重现这些不变量的自对偶表示

Conclusion: 成功建立了含Barbero-Immirzi参数的Pontryagin和Euler类的正则分析框架，揭示了其约束结构和对称性，为理解这些拓扑不变量在规范理论中的角色提供了系统方法

Abstract: A detailed canonical analysis for Pontryagin and Euler classes with a Barbero-Immirzi [BI] parameter is developed. We rewrite the topological invariants by introducing a set of Holst-like variables, and then study the set of all constraints. We report the complete canonical structure and the symmetries of the theory; we count the physical degrees of freedom and identify reducibility conditions among the constraints. In addition, in our results, if we consider the $BI$ parameter takes the value of $γ= \pm i $, then the self-dual representation of these invariants is reproduced. Finally, we couple the invariants to the Holst action and explore the canonical analysis.

</details>


### [173] [Thick brane in Palatini formalism with a non-minimally coupled bulk scalar field](https://arxiv.org/abs/2512.22405)
*Tahereh Azizi,Mojtaba Alimoradi*

Main category: gr-qc

TL;DR: 在Palatini引力框架下研究厚膜场景，通过标量场与Ricci标量的非最小耦合获得解析解，得到稳定的厚膜配置，支持引力局域化和四维引力恢复。


<details>
  <summary>Details</summary>
Motivation: 探索Palatini引力框架下的厚膜模型，研究引力局域化和膜世界现象学，其中度规和仿射联络作为独立变量处理。

Method: 采用Palatini引力形式，引入体标量场与Ricci标量的非最小耦合，在平坦四维Poincaré不变度规下获得解析解，分析张量微扰的线性稳定性。

Result: 获得钟形扭曲因子和对称火山状标量势，能量密度正则且局域化，张量微扰分析显示无快子模式，有效势支持引力子零模局域化，数值研究显示大质量KK模逐渐离域化。

Conclusion: 在Palatini引力框架下建立了稳定且物理一致的厚膜配置，为引力局域化和膜世界现象学提供了新见解。

Abstract: We study a thick brane scenario within the Palatini formulation of gravity, where the metric and affine connection are treated as independent variables. By introducing a non-minimal coupling between a bulk scalar field and the Ricci scalar, we obtain analytic solutions under a flat, four-dimensional Poincaré-invariant metric with a kink-like scalar configuration. The warp factor exhibits a bell-shaped profile, while the scalar potential forms a symmetric volcano-like structure, characteristic of a finite-thickness brane. The corresponding energy density is regular and localized, featuring a central peak with symmetrically placed negative minima. Through the analysis of linear tensor perturbations, we derive a Schrödinger-like equation with supersymmetric factorization, ensuring the absence of tachyonic modes and thus the stability of the background configuration. The effective potential also takes a volcano-like form that supports a localized graviton zero mode, confirming the recovery of four-dimensional gravity on the brane. A numerical study of the massive Kaluza--Klein spectrum reveals the progressive delocalization of massive modes into the bulk.
  Our results demonstrate a stable and physically consistent thick brane configuration within the Palatini gravity framework, offering new insights into gravity localization and braneworld phenomenology.

</details>


### [174] [Scalar-hairy AdS Black Hole in the Einstein-Maxwell-Scalar Theory: first-order phase transition with a critical point](https://arxiv.org/abs/2512.22433)
*Hong Guo,Hang Liu,Yun Soo Myung*

Main category: gr-qc

TL;DR: 在渐近AdS时空中，研究EMS模型中的实质量标量场，发现标量毛黑洞解和快子毛解共存，两者之间存在一阶相变线，源于极端温压下的临界点。


<details>
  <summary>Details</summary>
Motivation: 研究在渐近AdS时空中，非最小耦合到Maxwell场的标量场如何产生标量毛黑洞解，以及标量势如何诱导快子毛解，探索两种毛相在不同参数空间的共存现象和相变行为。

Method: 使用爱因斯坦-麦克斯韦-标量（EMS）模型，考虑实质量标量场在渐近AdS时空中。分析标量势为零时由非最小耦合产生的标量毛黑洞解，以及标量势存在时诱导的快子毛解。通过相图分析两种相在不同参数空间的分布。

Result: 标量势为零时，标量毛黑洞解的性质与平直时空相似。标量势存在时，额外诱导快子毛解，导致两种毛相在不同参数区域共存。相图显示快子毛相和标量毛相之间存在一阶相变线，起源于极端温度和化学势下的临界点。耦合强度λ增加会使临界点移向更高温度和化学势。

Conclusion: 在渐近AdS时空中，EMS模型支持两种不同类型的毛解共存，它们之间存在一阶相变。相变与标量毛相的自重叠区域及其起始点直接相关，耦合强度影响临界点的位置。

Abstract: In asymptotically anti-de Sitter (AdS) spacetime, we consider a real massiver scalar field in the Einstein-Maxwell-scalar (EMS) model and examine both scalar-hairy black hole solutions induced by the nonminimal coupling to the Maxwell field and tachyonic-hairy solutions driven by the scalar potential. When the scalar potential vanishes, scalar-hairy black holes emerge with profiles and properties similar to those observed in flat spacetime. The presence of the scalar potential additionally induces tachyonic-hairy solutions, leading to the coexistence of these two distinct hairy phases in different regions of the parameter space. The phase diagram reveals a first-order phase transition line between the tachyonic-hairy and scalar-hairy phases, originating at a critical point in the extreme temperature and chemical potential regime. Our detailed analysis shows that this phase transition is directly associated with the self-overlap region of the scalar-hairy phase and its start point. Moreover, increasing the coupling strength $λ$ shifts the critical point to higher temperature and chemical potential.

</details>


### [175] [Cosmological long-wavelength solutions in non-adiabatic multi-fluid systems](https://arxiv.org/abs/2512.22531)
*Hayami Iizuka,Tomohiro Harada*

Main category: gr-qc

TL;DR: 该论文发展了多流体系统中超视界尺度非线性宇宙学扰动的梯度展开方法，构建了非线性长波长解，并分析了绝热和熵扰动的演化。


<details>
  <summary>Details</summary>
Motivation: 研究多流体系统中超视界尺度的非线性宇宙学扰动，因为多流体系统本质上是非绝热的，需要同时考虑绝热和熵扰动模式。

Method: 采用ADM形式结合空间梯度展开方法，以小参数（共动波数与哈勃尺度之比）为展开参数，在平坦FLRW背景下构建非线性长波长解。

Result: 成功构建了多流体系统的非线性长波长解，定义了绝热和熵扰动，讨论了纯熵扰动定义的非唯一性，并分析了不同初始条件下曲率扰动和密度扰动的演化。

Conclusion: 建立了多流体系统超视界尺度非线性扰动的理论框架，为研究早期宇宙的非线性演化提供了重要工具，特别适用于非绝热多流体系统的扰动分析。

Abstract: We develop a formulation of nonlinear cosmological perturbations on superhorizon scales in multi-fluid systems. It is based on the Arnowitt-Deser-Misner formalism combined with a spatial gradient expansion characterized by a small expansion parameter defined as the ratio of the comoving wavenumber to the Hubble scale. The background spacetime is assumed to be a flat Friedmann-Lemaitre-Robertson-Walker universe. Within this framework, we explicitly construct nonlinear long-wavelength solutions for cosmological perturbations. Since multi-fluid systems are inherently non-adiabatic, these solutions admit both adiabatic and entropy modes already at leading nonlinear order. We define adiabatic and entropy perturbations and discuss the non-uniqueness in defining pure entropy perturbations. Using different choices of pure entropy initial conditions, we analyze the time evolution of physical quantities such as the curvature perturbation and density perturbations in the geodesic slicing for two-fluid systems.

</details>


### [176] [Topological Mod(A)Max AdS black holes](https://arxiv.org/abs/2512.22654)
*B. Eslam Panah,B. Hamil,Manuel E. Rodrigues*

Main category: gr-qc

TL;DR: 本文在AdS时空中构建了基于ModMax和ModAMax非线性电动力学的拓扑黑洞解，研究了其热力学性质、稳定性、Joule-Thomson膨胀过程和热机效率。


<details>
  <summary>Details</summary>
Motivation: 研究非线性电动力学（特别是ModMax和ModAMax模型）如何影响AdS时空中的拓扑黑洞解及其热力学性质，探索参数和拓扑结构对黑洞热力学行为的影响。

Method: 构建ModMax和ModAMax非线性电动力学下的AdS拓扑黑洞解，计算热力学量并验证热力学第一定律，分析局部和全局热稳定性，在扩展相空间框架下研究Joule-Thomson膨胀过程，并将黑洞视为热机评估其效率。

Result: ModMax和ModAMax参数以及拓扑常数显著影响黑洞解、热力学量、稳定性和Joule-Thomson膨胀行为。非线性电动力学参数和视界拓扑结构对热机效率有增强或抑制作用。

Conclusion: 非线性电动力学参数和拓扑结构在AdS黑洞的热力学性质中起关键作用，能够调控黑洞的冷却/加热行为和热机效率，为理解黑洞热力学提供了新视角。

Abstract: In this work, we construct new classes of topological black hole solutions in anti-de Sitter (AdS) spacetime using a novel model of nonlinear electrodynamics called Modification Maxwell (ModMax) and Modification phantom or Modification anti-Maxwell (ModAMax). We then evaluate the thermodynamic quantities and verify the first law of thermodynamics. Our study examines how the parameters of the ModMax and ModAMax fields, as well as the topological constant, affect the black hole solutions, thermodynamic quantities, and local and global thermal stabilities. Furthermore, within the framework of extended phase space thermodynamics, we analyze the Joule-Thomson expansion process and determine the inversion curves. This analysis reveals that the ModMax and ModAMax parameters significantly alter the cooling and heating behavior of these AdS black holes, depending on their topology. Finally, by treating these topological Mod(A)Max AdS black holes as heat engines, we assess their efficiencies, demonstrating that the parameters of nonlinear electrodynamics and horizon topology play crucial roles in enhancing or suppressing the system's thermodynamic performance.

</details>


### [177] [When spacetime vibrates: An introduction to gravitational waves](https://arxiv.org/abs/2512.22679)
*José P. S. Lemos*

Main category: gr-qc

TL;DR: 本文全面分析了引力波的物理学，涵盖理论基础、实验进展、主要探测器和未来展望，重点介绍了LIGO/Virgo/KAGRA干涉仪和首次直接探测GW150914事件。


<details>
  <summary>Details</summary>
Motivation: 引力波是爱因斯坦广义相对论的重要预言，其直接探测不仅验证了广义相对论，还开启了多信使天文学的新时代。本文旨在系统梳理引力波物理的理论基础、历史发展、实验技术以及未来前景。

Method: 采用综述分析方法，从广义相对论理论框架出发，系统介绍引力波的产生机制（特别是致密双星系统的旋进、并合、铃宕三个阶段），详细阐述LIGO、Virgo、KAGRA等大型激光干涉仪的工作原理，回顾GW150914等历史性探测事件，并展望未来探测项目。

Result: 引力波探测已成功实现，GW150914事件直接证实了引力波的存在，验证了广义相对论预言，该成就获得2017年诺贝尔物理学奖。目前已观测到多个引力波事件，揭示了致密双星并合等天体物理过程，并为探索宇宙起源（如原初引力波）提供了新途径。

Conclusion: 引力波天文学已成为探索宇宙的重要新窗口，与电磁波观测形成互补。未来更先进的干涉仪和替代探测方法将进一步推动这一领域发展，深化我们对宇宙的理解。

Abstract: This article presents a comprehensive analysis of the physics of gravitational waves, exploring both the theoretical foundations and the most recent experimental advances. After a general introduction to the theory of general relativity and its major implications, the article discusses the history of gravitational waves, from their prediction by Einstein to their actual detection. It then explains what gravitational waves are and how they interact with appropriate detectors. The main mechanisms of gravitational radiation emission are analyzed, with a focus on compact binary systems of compact objects, whose orbits typically evolve in three phases: inspiral, merger, and the final ringdown phase, each of these phases leaving distinct signatures in the emitted waves. The article highlights the fundamental role of the giant interferometers LIGO, Virgo, and KAGRA, true cathedrals of modern science, and revisits the historic event GW150914, the first direct detection of gravitational waves, which confirmed the predictions of general relativity and opened a new era for astronomy. This achievement was recognized with the 2017 Nobel Prize in Physics. Other observed events are also discussed, along with their astrophysical sources, and the possibility of detecting gravitational waves of cosmological origin, originating from the Big Bang itself. Finally, current and future projects are analyzed, including observatories based on increasingly sophisticated interferometers, as well as proposals for alternative detection methods, illustrating how gravitational-wave astronomy is shaping the present and future of our exploration of the universe. In concluding, the detection of gravitational waves is set in a broader context by examining the discoveries across the electromagnetic spectrum, thereby illustrating the complementary perspectives these different observational channels provide.

</details>


### [178] [A Machian wave effect in conformal, scalar-tensor gravitational theory](https://arxiv.org/abs/2512.22687)
*José Rodal*

Main category: gr-qc

TL;DR: 该研究检验了Woodward提出的"马赫效应"推进理论，通过相对论性分析证明该效应在实际中过于微弱，无法用于推进应用。


<details>
  <summary>Details</summary>
Motivation: 检验Woodward提出的驱动质量-能量波动可产生显著"马赫效应"引力响应的理论主张，该理论声称可通过宇宙势能放大效应实现实用推进。

Method: 在两种理论框架下进行协变分析：(1)爱因斯坦广义相对论中，使用Landau-Lifshitz松弛方程和谐和规范；(2)Hoyle-Narlikar共形标量-张量引力理论中，分析共形标量方程。

Result: 在广义相对论中，非线性项不是独立物质源，实验室设备的效应被抑制为∼(U_N/c^2)(ωL/c)^2≪1，无宇宙势能放大。在HN理论中，响应是瞬时泊松式而非波动放大，重子数守恒阻止了静止质量单极振荡，任何辐射单极都需要非保守内能变化，进一步被E_int/(Mc^2)和M_dev/M_H抑制。

Conclusion: Woodward提出的马赫效应推力在实际中过于微弱，无法用于推进应用，因为理论分析显示该效应被多个因素强烈抑制。

Abstract: Woodward proposed that driven mass-energy fluctuations could yield a frequency-dependent "Machian" gravitational response $\propto \partial_t^2 M_{\rm loc}(t)$, amplified by a Sciama-scale cosmic potential $Φ/c^2\sim -1$. We test this claim covariantly in (i) Einstein gravity and (ii) Hoyle-Narlikar (HN) conformal scalar-tensor gravity. In GR, the Landau-Lifshitz relaxed equations in harmonic gauge contain nonlinear terms of the form $H^{αβ}\,\partial_α\partial_βH^{μν}$, including a near-zone piece $H^{00}\,\partial_t^2 H^{μν}$. These terms are not independent matter sources; they arise from expanding the curved wave operator about a flat background. Moving them back to the left-hand side restores the quasilinear principal part, and for laboratory devices their size is suppressed by $\sim (U_N/c^2)(ωL/c)^2\ll 1$, with no enhancement by any cosmological potential. In HN theory, the conformal scalar satisfies $\nabla_a\nabla^a m + (R/6)m=λN$. For a localized device of size $L$ driven at angular frequency $ω$, $|(c^{-2}\partial_t^2 m_s)|/|\nabla^2 m_s|\sim (ωL/c)^2\ll 1$, so the response is effectively instantaneous (Poisson-like), not wave-amplified. Baryon-number conservation fixes the scalar charge, so the rest-mass monopole cannot oscillate; any radiating monopole requires nonconservative internal-energy variations, further suppressed by $E_{\rm int}/(M c^2)$ and by $M_{\rm dev}/M_H$. Thus any Mach-effect thrust is far too small for practical propulsion.

</details>


### [179] [Solving the constraint equation for general free data](https://arxiv.org/abs/2512.22704)
*Xuantao Chen,Sergiu Klainerman*

Main category: gr-qc

TL;DR: 提出一种求解爱因斯坦约束方程的新方法，允许自由指定四个标量自由度，将约束方程转化为球面上的输运-椭圆耦合方程组，可构造大量黑洞初始数据。


<details>
  <summary>Details</summary>
Motivation: 开发新方法以构造满足特定边界条件的爱因斯坦约束方程解，特别是要证明Shen的衰减条件是最优的，即构造违反其衰减条件的小光滑初始数据集，说明Minkowski空间稳定性结果不成立。

Method: 选择适当规范条件后，自由指定四个标量（模ℓ≤1模式），将爱因斯坦约束方程重写为2-球面上的耦合输运和椭圆方程组，通过迭代程序求解。

Result: 方法提供了大量可与给定内部解匹配的外部约束方程解，可应用于构造黑洞演化的初始Cauchy数据集，推广了Li和Yu的陷俘面形成结果。方法灵活，可构造任意快速衰减数据，也可用于构造较慢衰减数据。

Conclusion: 新方法为求解爱因斯坦约束方程提供了强大工具，特别可用于证明Shen衰减条件的最优性，即构造违反该条件的小光滑初始数据集，从而说明Minkowski空间稳定性结果不成立。

Abstract: We revisit the problem of solving the Einstein constraint equations in vacuum by a new method, which allows us to prescribe four scalar quantities, representing the full dynamical degrees of freedom of the constraint system. We show that once appropriate gauge conditions have been chosen and four scalars freely specified (modulo $\ell\leq 1$ modes), we can rewrite the constraint equations as a well-posed system of coupled transport and elliptic equations on $2$-spheres, which we solve by an iteration procedure. Our method provides a large class of exterior solutions of the constraint equations that can be matched to given interior solutions, according to the existing gluing techniques. As such, it can be applied to provide a large class of initial Cauchy data sets evolving to black holes, generalizing the well-known result of the formation of trapped surfaces due to Li and Yu. Though in our main theorem, we only specify conditions consistent with $g-g_{Schw}=O(r^{-1-δ})$, $k=O(r^{-2-δ})$, the method is flexible enough to be applied in many other situations. It can, in particular, be easily adapted to construct arbitrarily fast decaying data. We expect, moreover, that our method can also be applied to construct data with slower decay, such as that used by Shen. In fact, an important motivation for developing our method is to show that the result of Shen is sharp, i.e., construct small, smooth initial data sets which violate Shen's decay conditions, and for which the stability of the Minkowski space result is wrong.

</details>


### [180] [Probing higher curvature gravity via ringdown with overtones](https://arxiv.org/abs/2512.22728)
*Keisuke Nakashi,Masashi Kimura,Hayato Motohashi,Kazufumi Takahashi*

Main category: gr-qc

TL;DR: 研究高阶曲率引力中球对称黑洞的度规扰动，发现高阶曲率修正会改变有效势的近视界区域，导致准正规模频率偏离广义相对论值，且高阶模的偏离更明显。


<details>
  <summary>Details</summary>
Motivation: 探索高阶曲率引力理论中黑洞的动力学行为，特别是准正规模频率如何偏离广义相对论预测，这对于测试引力理论和理解黑洞物理具有重要意义。

Method: 分析球对称黑洞在高阶曲率引力中的度规扰动，研究有效势的近视界形变，计算准正规模频率的偏离，并通过波形拟合分析引力波衰减信号。

Result: 高阶曲率修正使有效势近视界区域发生形变，准正规模频率偏离GR值，且高阶模的偏离更显著；高阶曲率项阶数增加时，形变更接近视界，高阶模频率偏离更大；通过波形拟合可在衰减信号中识别这些偏移的准正规模。

Conclusion: 高阶曲率引力显著影响黑洞准正规模谱，特别是高阶模对修正更敏感，这些特征可通过引力波衰减信号的波形拟合来探测，为测试高阶曲率引力理论提供了新途径。

Abstract: We investigate metric perturbations of a spherically symmetric black hole in higher curvature gravity. We show that higher curvature corrections deform the near-horizon region of the effective potential, and that the deviations of the quasinormal mode (QNM) frequencies from their general relativity (GR) values become more pronounced for overtone modes. We find that, as the order of the higher curvature term increases, the deformations approach the horizon and the deviations of the overtone QNM frequencies grow progressively larger. We also analyze the ringdown waveforms in the higher curvature gravity model. We consider setups in which the deviations from the vacuum-GR QNMs remain mild for the fundamental mode and the first few overtones, and show that these shifted QNMs can be identified in the ringdown signal through waveform fitting.

</details>


### [181] [Constraining the dynamical Chern-Simons gravity with future gravitational wave detectors](https://arxiv.org/abs/2512.22762)
*Xinyi Che,Xiangyu Lyu,Changfu Shi*

Main category: gr-qc

TL;DR: 该论文评估了利用未来引力波探测器通过恒星级黑洞双星系统约束动力学Chern-Simons引力的前景，量化了不同探测器和源参数下的约束能力，并识别了满足小耦合条件的参数空间区域。


<details>
  <summary>Details</summary>
Motivation: 动力学Chern-Simons引力作为弦理论衍生的低能有效理论，其奇偶性破坏特性需要通过引力波进行检验。然而，当前引力波观测受限于探测器噪声和波形模型的有效性要求，无法通过相位测量对该理论施加有意义的约束。

Method: 通过恒星级黑洞双星系统，全面评估未来引力波探测器约束动力学Chern-Simons引力的能力。量化不同探测器和源参数下的约束能力变化，识别满足小耦合条件的参数空间区域，并纳入天体物理质量分布模型进行估计。

Result: 研究量化了未来引力波观测站约束动力学Chern-Simons引力的潜力，确定了不同探测器配置和源参数下的约束能力变化，并识别了满足理论小耦合条件的参数空间区域。

Conclusion: 未来引力波探测器有望通过恒星级黑洞双星系统对动力学Chern-Simons引力施加有意义的约束，特别是通过优化探测器配置和源参数选择，并考虑天体物理质量分布模型的影响。

Abstract: Dynamical Chern-Simons gravity, a parity-violating modification of general relativity, is regarded as a low-energy effective theory arising from string theory. Gravitational waves provide a powerful probe for testing its predictions. However, current gravitational wave observations are unable to place meaningful constraints on this theory through phase measurements, due to limitations from detector noise and the validity requirements of the waveform models. In this paper, we conduct a comprehensive assessment of the prospects for constraining the dynamical Chern-Simons gravity with future gravitational-wave detectors using stellar mass black holes binary. We quantify how the constraining capacities vary across different detectors and source parameters, and identify the regions of parameter space that satisfy the small-coupling condition. Furthermore, by incorporating an astrophysically motivated mass distribution model for stellar mass black hole binaries, we estimate the potential of upcoming observatories.

</details>


### [182] [Polarized image of an equatorial emitting ring around a Konoplya-Zhidenko rotating non-Kerr black hole](https://arxiv.org/abs/2512.22764)
*Xin Qin,Fen Long,Songbai Chen,Jiliang Jing*

Main category: gr-qc

TL;DR: 研究科诺普利亚-日登科旋转非克尔黑洞周围赤道发射环的偏振图像，分析额外变形参数对偏振特性的影响


<details>
  <summary>Details</summary>
Motivation: 探索非克尔黑洞时空几何对偏振图像的观测特征，为通过高精度偏振观测探测黑洞时空偏离克尔几何提供理论依据，从而检验广义相对论

Method: 研究赤道发射环在科诺普利亚-日登科旋转非克尔黑洞周围的偏振图像，考虑额外变形参数η，分析磁场配置、流体速度、观测倾角、变形参数和自旋参数对偏振特性的影响

Result: 偏振图像不仅依赖于磁场配置、流体速度和观测倾角，还受变形参数和自旋参数影响。当磁场位于赤道面时，偏振强度随变形参数增加单调减小；当磁场垂直于赤道面时，变化非单调。偏振位置角随变形参数变化复杂。固定变形参数时，偏振强度对自旋参数呈非单调依赖并随方位角变化

Conclusion: 变形参数在偏振图像中的印记可作为高精度观测探针，用于探测黑洞时空偏离克尔几何并检验广义相对论，偏振观测为黑洞物理研究提供了新途径

Abstract: We investigate the polarized images of an equatorial emitting ring around a Konoplya-Zhidenko rotating non-Kerr black hole, which introduces an additional deformation parameter. The deformation parameter $η$ allows the spin parameter to extend beyond the bounds imposed by the standard Kerr black hole. The results indicate that the polarized images depend not only on the magnetic field configuration, fluid velocity, and observer inclination angle, but also on the deformation parameter and the spin parameter. As the deformation parameter increases, the polarization intensity decreases monotonically when the magnetic field lies in the equatorial plane, whereas it does not vary monotonically when the magnetic field is perpendicular to the equatorial plane. The variation of the electric vector position angle with the deformation parameter is complex. For a fixed deformation parameter, the polarization intensity exhibits a non-monotonic dependence on the spin parameter and varies with azimuthal angle. We also investigate the impact of the deformation parameter on the Stokes Q-U loops. The imprint of the deformation parameter in polarized images may serve as a high-precision observational probe for detecting deviations of black hole spacetime from the Kerr geometry and testing general relativity.

</details>


### [183] [Topological Complex Analysis of Kerr--Newman Black Hole Microstructure in f(R) Gravity](https://arxiv.org/abs/2512.22853)
*Wen-Xiang Chen*

Main category: gr-qc

TL;DR: 使用拓扑复分析框架研究f(R)修正引力中Kerr Newman黑洞的微观结构，通过解析延拓配分函数的奇点识别微观态，熵由留数加权得到。发现微观结构由离散拓扑指数表征，该指数编码视界结构和热力学稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究修正引力理论中黑洞的微观结构，特别是f(R)类型修正引力下的Kerr Newman黑洞。通过拓扑复分析框架，将全息原理启发的方法应用于黑洞微观态的描述，探索引力修正对黑洞微观结构的影响。

Method: 采用拓扑复分析框架，将黑洞微观态识别为解析延拓配分函数的奇点，熵通过留数加权计算。使用离散拓扑指数表征微观结构，该指数编码视界结构和热力学稳定性。通过Starobinsky型修正引力模型进行验证。

Result: 发现非极端Kerr Newman黑洞（具有内外视界）对应零拓扑指数，而单视界构型对应正单位拓扑指数。在Starobinsky型修正引力模型中，这种分类保持稳健，表明拓扑分类在引力修正下具有鲁棒性。

Conclusion: 黑洞微观结构可由离散拓扑指数有效表征，该分类在引力修正下保持稳定。解析延拓方法存在局限性，但拓扑分类可能暗示黑洞热力学中存在某种形式的相保护机制。

Abstract: We investigate the microstructure of Kerr Newman black holes in modified gravity of the f(R) type using a topological complex analytic framework inspired by holography. In this approach, black hole microstates are identified with singularities of an analytically continued partition function, and the entropy is obtained from residues weighted by winding numbers. We show that the microstructure is characterized by a discrete topological index, which encodes both horizon structure and thermodynamic stability. Non extremal Kerr Newman black holes with both inner and outer horizons correspond to a vanishing topological index, while single horizon configurations correspond to a positive unit topological index. An explicit Starobinsky type modified gravity model demonstrates that this classification is robust under changes to the gravitational sector. We further discuss the limitations of the analytic continuation procedure and suggest that this topological classification may indicate a form of phase protection in black hole thermodynamics.

</details>


### [184] [A Geometric Area Bound for Information Transfer Through Semiclassical Traversable Wormholes](https://arxiv.org/abs/2512.22928)
*Fuat Berkin Altunkaynak,Aslı Tuncer*

Main category: gr-qc

TL;DR: 该论文证明了量子信息通过半经典可穿越虫洞传输的严格几何上界，通过面积定理和位线程模型建立了信息传输的上界。


<details>
  <summary>Details</summary>
Motivation: 研究量子信息通过可穿越虫洞传输的极限，理解虫洞几何与信息传输能力之间的关系。

Method: 1. 使用Raychaudhuri方程和零能量条件证明面积定理，表明可穿越虫洞的最小喉部面积不能增加；2. 在位线程模型中，最大流对应初始最小喉部面积，从而建立信息传输上界；3. 使用两个粘合的HaPPY代码作为虫洞的玩具模型。

Result: 证明了可穿越虫洞的最小喉部面积设定了量子信息传输的上界，初始喉部面积对应位线程模型中的最大流，限制了信息传输能力。

Conclusion: 虫洞的几何结构（特别是最小喉部面积）从根本上限制了量子信息的传输能力，这为理解虫洞作为量子通信通道的物理限制提供了理论基础。

Abstract: We prove a rigorous geometric bound on the transmission of quantum information through semiclassical traversable wormholes. We initially prove an area theorem showing that the minimal throat surface of a traversable wormhole cannot increase using Raychaudhuri's equation together with the null energy condition for the infalling matter. Then, the max-flow in the bit-thread picture, which corresponds to the initial minimal throat area, is shown to set the upper bound on information transfer. We also discuss two glued HaPPY codes as a toy model for a wormhole.

</details>


### [185] [Multi-messenger detectability of continuous gravitational waves from the near future to next generation detectors](https://arxiv.org/abs/2512.22938)
*Benjamin J. Owen,Binod Rajbhandari*

Main category: gr-qc

TL;DR: 该论文评估了连续引力波的探测前景，结合理论观测论证，预测未来引力波探测器可能首次探测到连续引力波，若无探测将挑战现有毫秒脉冲星形成理论。


<details>
  <summary>Details</summary>
Motivation: 连续引力波探测对引力波天文学、天体物理学、核物理和凝聚态物理有重要价值。论文旨在系统评估已知天体对象和未来引力波探测器对连续引力波的探测能力。

Method: 结合文献中的各种理论和观测论证，系统应用于已知天文对象和未来引力波探测器。更新了之前为Cosmic Explorer所做的估计，并详细分析了吸积中子星自旋调节和毫秒脉冲星磁场埋藏等理论假设。

Result: 如果吸积中子星自旋受引力波发射调节或毫秒脉冲星磁场被吸积物质掩埋的理论成立，那么未来几年内现有探测器的升级版很可能首次探测到连续引力波，下一代探测器如Cosmic Explorer和爱因斯坦望远镜将实现多次探测。

Conclusion: 连续引力波探测前景乐观，未来几年若无探测将严重质疑当前毫秒脉冲星形成理论。下一代引力波探测器将为连续引力波天文学开启新篇章。

Abstract: Continuous gravitational waves have the potential to transform gravitational wave astronomy and yield fresh insights into astrophysics, nuclear and particle physics, and condensed matter physics. We evaluate their detectability by combining various theoretical and observational arguments from the literature and systematically applying those arguments to known astronomical objects and future gravitational wave detectors. We detail and update previous estimates made in support of Cosmic Explorer [M. Evans et al., arXiv:2306.13745; I. Gupta et al., Class. Quantum Grav. 41, 245001 (2024)]. It is commonly argued that the spins of accreting neutron stars are regulated by gravitational wave emission and that millisecond pulsars contain a young pulsar's magnetic field buried under accreted material. If either of these arguments holds, the first detection of continuous gravitational waves is likely with near future upgrades of current detectors, and many detections are likely with next generation detectors such as Cosmic Explorer and the Einstein Telescope. A lack of detections in the next several years would begin to raise serious doubts about current theories of millisecond pulsar formation.

</details>


### [186] [Colloquium: Multimessenger astronomy with continuous gravitational waves and future detectors](https://arxiv.org/abs/2512.22945)
*Benjamin J. Owen*

Main category: gr-qc

TL;DR: 综述了连续引力波探测的前景，特别是借助电磁观测，预计未来几年内可能首次探测到来自中子星的连续引力波，下一代探测器将带来更多发现，这些信号能揭示中子星极端物质的性质。


<details>
  <summary>Details</summary>
Motivation: 连续引力波探测是引力波天体物理学的新前沿，与电磁天文学、核天体物理学和凝聚态物理有密切联系。中子星快速旋转产生的连续引力波包含大量物理信息，但目前相关物理机制（如地壳弹性）缺乏观测约束。

Method: 通过电磁观测辅助连续引力波探测，结合当前理论和观测数据评估探测前景。利用下一代引力波探测器（如Cosmic Explorer和Einstein Telescope）以及电磁望远镜（如平方公里阵列、下一代甚大阵列、FAST等）进行多信使观测。

Result: 理论和观测表明，首次连续引力波探测可能在几年内实现，下一代探测器时代将会有更多发现。这些信号包含比所有已探测到的致密双星合并事件更多的周期，能提供关于极端物质的新信息。

Conclusion: 连续引力波探测即将进入收获期，通过引力波与电磁波的多信使观测，将能深入了解中子星物理和极端物质性质，填补当前天文观测和物理实验的空白。

Abstract: Continuous gravitational waves from rapidly rotating neutron stars are on the new frontiers of gravitational wave astrophysics and have strong connections to electromagnetic astronomy, nuclear astrophysics, and condensed matter physics. In this Colloquium I survey prospects for detection of continuous gravitational waves from various neutron star populations, especially aided by electromagnetic observations. Although there are caveats, current theories and observations suggest that the first detections are likely within a few years, and that many are likely in the era of next generation detectors such as Cosmic Explorer and the Einstein Telescope. I also survey what can be learned from these signals, each one of which will contain more cycles than all the compact binary mergers ever detected. Since continuous gravitational wave emission mechanisms depend on aspects of neutron star physics, such as crustal elasticity, which are not well constrained by current astronomical observations and physical experiments, their detection can tell us a great deal that is new about extreme matter. Even more can be learned by combining gravitational wave observations with data from the Square Kilometre Array, the Next Generation Very Large Array, FAST, and other electromagnetic detectors operating in the next generation era.

</details>


### [187] [Gravitational Noether-Ward identities for scalar field](https://arxiv.org/abs/2512.22958)
*Tomislav Prokopec*

Main category: gr-qc

TL;DR: 研究量子物质背景下广义度规扰动的引力Noether-Ward恒等式，发现运动方程中各项满足各自的恒等式，整体保持横向性


<details>
  <summary>Details</summary>
Motivation: 研究爱因斯坦引力与量子标量场耦合系统中度规扰动的Noether-Ward恒等式，理解量子背景下引力扰动的守恒性质

Method: 考虑爱因斯坦引力与质量、非最小耦合量子标量场的协变耦合，分析广义弯曲背景下的度规扰动运动方程

Result: 发现运动方程中各项满足各自的Noether-Ward恒等式，虽然各项非横向但整体保持横向性；推导了重整化引力子自能所需各抵消项的Noether恒等式

Conclusion: Noether-Ward恒等式适用于所有度规扰动定义，不同定义下恒等式形式不同但都存在相应守恒关系

Abstract: We consider the gravitational Noether-Ward identities for the evolution of general metric perturbations on quantum matter backgrounds. In this work we consider Einstein's gravity covariantly coupled to a massive, non-minimally coupled, quantum scalar field in general curved backgrounds. We find that each term in the equation of motion for gravitational perturbations satisfies its own Noether-Ward identity. Even though each term is non-transverse, the whole equation of motion maintains transversality. In particular, each counterterm needed to renormalize the graviton self-energy satisfies its own Noether identity, and we derive the explicit form for each. Finally, in order to understand how the Noether-Ward identities are affected by the definition of the metric perturbation, we consider two inequivalent definitions of metric perturbations and derive the Noether-Ward identities for both definitions. This implies that there are Noether-Ward identities for every definition of the metric perturbation.

</details>


### [188] [Scalar-Field Wave Dynamics and Quasinormal Modes of the Teo Rotating Wormhole](https://arxiv.org/abs/2512.23104)
*Ramesh Radhakrishnan,Gerald Cleaver,Delaram Mirfendereski,Eric Davis,Claudio Cremaschini*

Main category: gr-qc

TL;DR: 研究旋转Teo虫洞的标量场微扰，计算其准正规模谱，发现与Kerr黑洞存在定性差异：虫洞的准正规模振荡频率和阻尼率随自旋增加而减小，呈现单侧分裂模式，且无经典超辐射放大。


<details>
  <summary>Details</summary>
Motivation: 研究旋转虫洞中的波传播特性，探索无视界致密天体与黑洞在准正规模谱上的差异，为区分旋转虫洞和Kerr黑洞提供特征光谱签名。

Method: 使用WKB方法计算旋转Teo虫洞的准正规模谱，分析Klein-Gordon方程分离得到的径向薛定谔型方程，研究由虫洞喉部局域框架拖曳形成的单峰势垒。

Result: 旋转Teo虫洞的准正规模谱呈现相干单调的自旋依赖性：自旋增加时振荡频率和阻尼率均减小，模式寿命变长。在eikonal极限下验证了标准准正规模-eikonal对应关系。与Kerr黑洞相比，虫洞显示更强的局域自旋响应、喉部部分反射和快速饱和的单侧分裂模式。

Conclusion: 旋转和边界条件共同塑造无视界致密天体中的波传播特性，旋转Teo虫洞的准正规模谱提供了区别于Kerr黑洞的特征签名，尽管存在能层和超辐射兼容频率运动学，但缺乏事件视界或耗散边界阻止了经典超辐射放大。

Abstract: We analyze scalar field perturbations of the rotating Teo wormhole and compute its quasinormal mode (QNM) spectrum using WKB methods in a fully horizonless geometry. The Klein Gordon equation separates and yields a Schrödinger type radial equation with a single, smooth potential barrier shaped by the localized frame dragging profile of the wormhole throat. This barrier supports damped oscillatory modes across the full spin range examined. The resulting QNM spectrum exhibits a coherent and monotonic dependence on rotation. As the spin increases, both the oscillation frequency and the damping rate decrease, indicating progressively longer-lived modes in the absence of horizon induced absorption. In the eikonal limit, we extract the photon-ring radius, orbital frequency, and Lyapunov exponent, and verify the standard QNM-Eikonal correspondence. Comparison with Kerr black holes reveals qualitative differences. Whereas Kerr QNMs are governed by horizon absorption and exhibit symmetric prograde/retrograde mode splitting, the Teo wormhole displays a stronger but spatially confined spin response, partial reflection at the throat, and a distinctive one-sided splitting that saturates rapidly with increasing spin. Although the rotating Teo wormhole admits an ergoregion and superradiant compatible frequency kinematics, the absence of an event horizon or dissipative boundary prevents classical superradiant amplification. These results demonstrate how rotation and boundary conditions jointly shape wave propagation in horizonless compact objects and provide characteristic spectral signatures distinguishing rotating wormholes from Kerr black holes.

</details>


### [189] [Lense-Thirring Acoustic Black Holes : Shadows and Light](https://arxiv.org/abs/2512.23113)
*Anas El Balali,Alessio Marrani*

Main category: gr-qc

TL;DR: 本文提出Lense-Thirring声学黑洞(LTABH)模型，研究其时空几何、声学和光学阴影、进动频率及光线偏折，发现声学参数ξ和旋转参数a对黑洞性质有不同影响。


<details>
  <summary>Details</summary>
Motivation: 研究模拟黑洞在各种物理系统（如宇宙微波背景或量子超流体）中的类比模型，探索Lense-Thirring声学黑洞的几何和物理特性。

Method: 分析LTABH时空几何，研究度量函数根对时空分区的影响，计算声学球半径和光子球半径，分析有效势能最大值对应的临界半径，研究阴影半径和畸变，推导进动频率Ω，计算光线偏折。

Result: 1) 时空几何分为四个区域，取决于声学参数ξ；2) 旋转参数a影响有效势能临界半径；3) 声学阴影R_as和光学阴影R_s都依赖于ξ和a，但a主要影响R_s（向右偏移），而R_as保持圆形；4) ξ越大，两个阴影尺寸越大；5) 进动频率Ω在声学视界附近显著增加；6) 存在Ω为零的区域，粒子不受参考系拖曳影响。

Conclusion: LTABH模型展示了声学参数ξ和旋转参数a对黑洞特性的不同影响，声学参数主要影响阴影大小，旋转参数主要影响光学阴影位置，参考系拖曳效应在声学视界附近变得重要。

Abstract: We introduce the Lense-Thirring Acoustic Black Hole (LTABH), motivated by the relevance of analogue models for black holes embedded in various physical systems, such as the cosmological microwave background or quantum superfluids. We investigate the LTABH spacetime geometry, showing that the roots of the metric function determine a partition of the spacetime into four regions, depending on the acoustic parameter $ξ$ (whereas the dependence vanishes for the rotation parameter $a$); on the other hand, the parameter $a$ turns out to affect the critical radii associated to the maxima of the effective potential. All in all, both the acoustic sphere radius $r_{as}$ and the photon sphere radius $r_{ps}$, respectively giving rise to the acoustic shadow $R_{as}$ and to the optical shadow $R_{s}$, depend on $ξ$ and $a$. More precisely, the rotation parameter $a$ is more relevantly affecting $R_{s}$ (through a right shift), while $R_{as}$ retains its circular shape. For what concerns the acoustic parameter, we notice that the higher $ξ$ is, the larger the size of both shadows. All of these results are confirmed through a detailed analysis of the distortions and of the shadows radii. Moreover, by deriving the magnitude of the precession frequency $Ω$, we observe that it significantly increases near the acoustic horizons, both in the extremal and in the non-extremal cases, which implies that the Lense-Thirring (frame dragging) effect, which can be traced back to $ξ$ itself, becomes important near such regions. On the other hand, we also show that there are regions of the LTABH spacetime in which $% Ω$ vanishes, suggesting that therein possible probe particles would not be affected by the frame dragging at all. Finally, we derive the deflection of the light near the LTABH.

</details>


### [190] [Mass distribution of ultralight boson in binary black hole systems](https://arxiv.org/abs/2512.23279)
*Hang Yang,Daiqin Su*

Main category: gr-qc

TL;DR: 研究双黑洞系统中超轻玻色子云的动力学，特别关注质量不等、自旋方向任意的双黑洞系统中的玻色子质量转移


<details>
  <summary>Details</summary>
Motivation: 超轻玻色子是很有前景的暗物质候选者，黑洞超辐射可以产生标量和矢量玻色子云。玻色子的自相互作用以及双黑洞系统中的跃迁混合会产生动力学现象，这些现象可能通过未来的引力波观测被探测到

Method: 研究双黑洞系统中玻色子的动力学，特别关注质量不等、自旋方向任意的双黑洞系统中的玻色子质量转移过程

Result: 伴星与主黑洞的质量比显著影响通过质量转移的云吸收效率；当伴星自旋与主黑洞自旋不对齐时，云耗散效率会进一步改变

Conclusion: 双黑洞系统的质量比和自旋方向对玻色子云的质量转移和耗散过程有重要影响，这些动力学特征可能通过未来的引力波观测被探测到

Abstract: Ultralight bosons are compelling dark-matter candidates. Both scalar and vector bosons can be produced through black hole superradiance, forming a boson cloud surrounding a rotating black hole. Self-interaction of bosons, together with transition mixing in binary black hole systems, give rise to dynamical phenomena that could be potentially observable with future gravitational wave observations. In this work, we investigate the dynamics of bosons in binary black hole systems. In particular, we focus on boson mass transfer in unequal-mass binary black hole systems with arbitrary spin-orientation of the companion. Our results show that the mass ratio between the companion and the primary black holes significantly affects cloud absorption through mass transfer. Moreover, when the companion's spin is not aligned with that of the primary, the efficiency of cloud depletion is further modified.

</details>


### [191] [Motion of extended fluid bodies in the Newtonian limit of $f(R)$ gravity](https://arxiv.org/abs/2512.23283)
*Bofeng Wu,Xiao Zhang*

Main category: gr-qc

TL;DR: 该论文研究了f(R)引力理论牛顿极限下N个扩展流体天体系统的天体间动力学，推导了质心加速度、引力势能和自旋角动量的多极展开，并给出了两体系统的有效单体方程。


<details>
  <summary>Details</summary>
Motivation: 研究f(R)引力理论（广义相对论的修改理论）在牛顿极限下，由多个扩展流体天体组成的孤立自引力系统的天体间动力学，建立系统的粗粒度描述框架。

Method: 采用对称无迹形式主义，利用不可约笛卡尔张量，推导每个天体质心加速度的多极展开，包括库仑型和汤川型两部分；提供系统总引力势能的多极展开；推导每个天体自旋角动量的运动方程。

Result: 得到了质心加速度的多极展开表达式，其中库仑型部分与广义相对论相同，由质量多极矩乘积描述；汤川型部分为f(R)引力的修正，由标量多极矩乘积描述。给出了总引力势能和总守恒能量的表达式，以及自旋角动量方程的多极展开形式。对于两体系统，得到了相对运动的有效单体方程和系统总能量。

Conclusion: 建立了f(R)引力理论牛顿极限下多体系统天体间动力学的完整粗粒度描述框架，为研究该理论下的天体系统动力学提供了理论基础，所得结果可用于分析f(R)引力与广义相对论的观测差异。

Abstract: In the Newtonian limit of $f(R)$ gravity, for an isolated self-gravitating system consisting of $N$ extended fluid bodies, the inter-body dynamics are studied by applying the symmetric and trace-free formalism in terms of irreducible Cartesian tensors. The multipole expansion of each body's center-of-mass acceleration is derived, and the expansion comprises the Coulomb-type part and the Yukawa-type part, where the former, identical to that in General Relativity, is encoded by the products of the mass multipole moments of the body with those of other bodies, and the latter, as the modification introduced by $f(R)$ gravity, is encoded by the products of the scalar multipole moments of the body with those of other bodies. As an essential component of the system's orbital dynamics, the multipole expansion for the total gravitational potential energy is provided, and the expression for the total conserved energy in terms of the mass and scalar multipole moments of the bodies is offered. To investigate the system's spin dynamics, the equation of motion for each body's spin angular momentum is further deduced and presented in the form of multipole expansion. These findings constitute the main content of the coarse-grained description of inter-body dynamics for the system within the framework of the Newtonian limit of $f(R)$ gravity. As a by-product, for a two-body system, the effective one-body equation governing the relative motion between the two bodies and the total energy of this system are achieved.

</details>


### [192] [The axion coupling accelerates the Universe through PT-symmetric phases](https://arxiv.org/abs/2512.23376)
*Leqian Chen,Nick E. Mavromatos,Sarben Sarkar*

Main category: gr-qc

TL;DR: 论文重新检验并改进了关于PT对称相在理解Chern-Simons轴子规范理论奇异重整化群流中作用的猜想，通过包含引力耦合的Wetterich方程证实了奇异RG流结构，提出PT对称相可能解释宇宙加速膨胀。


<details>
  <summary>Details</summary>
Motivation: 重新检验作者先前关于PT对称相在理解Chern-Simons轴子规范理论奇异重整化群流中作用的猜想，通过更完整的理论框架来验证这一猜想。

Method: 使用包含引力耦合的Wetterich方程，系统性地研究重整化群流，分析奇异结构在包含引力耦合后的持续性。

Result: 奇异重整化群流结构在包含引力耦合后仍然存在，为PT对称相在排斥性引力中起作用的猜想提供了进一步支持，表明这种相可能表征弦有效Chern-Simons引力理论。

Conclusion: PT对称相可能在宇宙学尺度上解释当前观测到的宇宙加速膨胀现象，为理解宇宙加速提供了新的理论框架。

Abstract: The conjecture by two of the authors (N.E.M. and S.S.) that a \cPT-symmetric phase plays a role in understanding singular renormalisation group (RG) flows for a Chern-Simons (CS) gauge theory of axions, has been reexamined and significantly improved. We have used the more complete Wetterich equation, which includes gravitational couplings in a systematic way from the start, to understand the emergence of this phase. The singular structure of the RG flows has persisted on including gravitational-couplings, thereby offering further support to the conjecture that \cPT -symmetric phases of (repulsive) gravity characterise string-effective CS gravitational theories, where the axion is the massless string-model independent axion, which can also play a role of a totally-antisymmetric torsion degree of freedom. This has suggested a novel interpretation of the currently observed acceleration of the expansion of the Universe in terms of such a phase at large (cosmological) scales.

</details>


### [193] [Quasinormal mode/grey-body factor correspondence for Kerr black holes](https://arxiv.org/abs/2512.23510)
*Zun-Xian Huang,Peng-Cheng Li*

Main category: gr-qc

TL;DR: 本文建立了克尔黑洞在eikonal极限下的准正规模与灰体因子之间的对应关系，通过WKB方法推导了连接公式，并验证了与数值结果的一致性。


<details>
  <summary>Details</summary>
Motivation: 研究克尔黑洞的准正规模与灰体因子之间的理论联系，特别是在eikonal极限下建立精确的对应关系，以深化对黑洞物理的理解。

Method: 将径向Teukolsky方程转化为具有短程势的薛定谔型方程，使用WKB连接公式推导准正规频率与灰体传输系数之间的关系，并包含高阶WKB修正。

Result: 对于引力扰动，预测的灰体因子与从广义Sasaki-Nakamura方程获得的数值结果一致，在大角量子数时精度更高；但在超辐射区域对应关系失效。

Conclusion: 成功建立了克尔黑洞在eikonal极限下的准正规模-灰体因子对应关系，WKB方法有效但受限于超辐射区域，为黑洞物理研究提供了新的理论工具。

Abstract: We establish a quasinormal-mode/grey-body factor correspondence for Kerr black holes in the eikonal limit. By recasting the radial Teukolsky equation into a Schrödinger-type form with a short-range potential, we derive WKB connection formulas that relate Kerr quasinormal frequencies to grey-body transmission coefficients. Higher-order WKB corrections are included, extending the correspondence beyond the leading eikonal regime. For gravitational perturbations, the predicted grey-body factors agree with numerical results obtained from the generalized Sasaki-Nakamura equation, with increasing accuracy at large angular quantum number. The correspondence breaks down in the superradiant regime, where the WKB assumptions fail.

</details>


### [194] [Frozen Neutron Stars in Four-Dimensional Non-polynomial Gravities](https://arxiv.org/abs/2512.23525)
*Chen Tan,Yong-Qiang Wang*

Main category: gr-qc

TL;DR: 研究4维非多项式引力理论中中子星的结构与性质，发现随着修正参数α增大，中子星半径和质量增加，当α足够大时会出现"冻结中子星"状态，其表面形成临界视界，外观几乎与黑洞无异。


<details>
  <summary>Details</summary>
Motivation: 探索非多项式引力理论对中子星结构的影响，研究引力修正如何改变中子星的性质，并寻找理论预测的新天体物理现象。

Method: 求解修正的Tolman-Oppenheimer-Volkoff方程，使用三种不同的状态方程（BSk19、SLy4、AP4）进行分析，研究修正参数α对中子星性质的影响。

Result: 中子星解在修正引力中仍然存在；随着α增大，中子星半径和质量增加；当α足够大时出现"冻结中子星"状态，其表面形成临界视界，外观类似黑洞；这种冻结状态是理论中中子星序列的普适终点，与状态方程选择无关。

Conclusion: 非多项式引力理论预测存在冻结中子星作为中子星序列的普适终点，根据当前观测约束推导了修正参数α的界限，并表明冻结中子星在这些界限内仍然被允许存在。

Abstract: This paper investigates the structure and properties of neutron stars in four-dimensional non-polynomial gravities. Solving the modified Tolman-Oppenheimer-Volkoff equations for three different equations of state (BSk19, SLy4, AP4), we confirm that neutron star solutions remain in existence. As the modification parameter $α$ increases, neutron stars grow in both radius and mass. We find that, when the parameter $α$ is sufficiently large, a frozen state emerges at the end of the neutron-star sequence. In this state, the metric functions approach zero extremely close to the stellar surface, forming a critical horizon, making it nearly indistinguishable from a black hole to an external observer. Such a frozen neutron star constitutes a universal endpoint of the neutron-star sequence in this theory, independent of the choice of the equation of state. Based on our results and current observational constraints, we derive bounds on the modification parameter $α$ and show that frozen neutron stars remain allowed in the bounds.

</details>


### [195] [Optical Signatures of q-deformed solution in Einstein-Maxwell-dilaton Gravity](https://arxiv.org/abs/2512.23551)
*Chawit Sakkawattana,Chatchai Promsiri,Supakchai Ponglertsakul*

Main category: gr-qc

TL;DR: 论文研究了爱因斯坦-麦克斯韦-膨胀子理论中球对称天体的零测地线，分析了光子轨道、偏转角、光子环宽度与李雅普诺夫指数的关系，并构建了光学图像和研究了类时测地线的ISCO半径。


<details>
  <summary>Details</summary>
Motivation: 研究爱因斯坦-麦克斯韦-膨胀子理论中球对称天体的几何光学性质，探索膨胀子耦合、磁荷等参数对光子轨道、偏转角和光学图像的影响，为观测这类天体提供理论依据。

Method: 使用哈密顿-雅可比方法推导测地线方程，分析赤道面上的径向光子轨道方程和有效势能；通过Gralla-Lupsasca-Marrone模型模拟薄吸积盘强度分布；研究不同参数下光子轨迹和光学图像。

Result: 得到了膨胀子耦合λ、积分膨胀子通量D和磁荷P对光子偏转角和轨迹的影响；建立了光子环宽度与李雅普诺夫指数的关系；构建了三种不同发射轮廓的光学图像；确定了类时测地线的ISCO半径。

Conclusion: EMD理论中的膨胀子耦合、磁荷等参数显著影响光子轨道和光学观测特征，为通过引力透镜和黑洞阴影观测这类天体提供了理论框架，光子环宽度与李雅普诺夫指数的关系为研究黑洞动力学提供了新视角。

Abstract: We consider null geodesics in the background of spherically symmetric object in Einstein-Maxwell-Dilaton (EMD) theory with coupling function $f(Φ)=e^{-2λΦ}$. The spherical solution is characteristically described by dilaton coupling $λ$, integrated dilaton flux $D$ and magnetic charge $P$. Then, we derive geodesic equations by using the Hamilton-Jacobi approach. The radial photon orbital equation on equatorial plane and effective potential are analyzed. The total deflection angle and trajectories of photon as a function of impact parameter $b$ are plotted with the variation of $λ,D$ and $P$. Furthermore, the relation between photon ring's width and the Lyapunov exponent is also explored. In addition, we use the Gralla-Lupsasca-Marrone (GLM) model to model intensity profile of optically thin accretion disk around the object. Hence, we construct optical images of the object surrounded by three distinct emission profiles. Lastly, we investigate the radius of innermost stable circular orbit (ISCO) for timelike geodesics.

</details>


### [196] [The new generation lunar gravitational wave detectors: sky map resolution and joint analysis](https://arxiv.org/abs/2512.23556)
*Xiaolin Zhang,Chengye Yu,Haoran Li,Sobhan Kazempour,Mingqiu Li,Sichun Sun*

Main category: gr-qc

TL;DR: CIGO月球引力波干涉仪在0.1-10Hz频段对单色源的角分辨率优于天琴和LISA，升级版TCIGO配置可提升5倍角分辨率性能


<details>
  <summary>Details</summary>
Motivation: 月球引力波干涉测量是连接空间和地面探测器的有前景方法，需要评估新提出的CIGO月球陨石坑干涉引力波天文台的角分辨率性能

Method: 采用Fisher矩阵方法分析CIGO（月球北极陨石坑边缘）、天琴和LISA在0.1-10Hz频段对单色源的角分辨率性能，并探索升级版TCIGO四面体配置

Result: 在0.1Hz以上，CIGO的定位精度优于天琴和LISA，主导探测器网络性能；TCIGO配置相比CIGO提升5倍角分辨率能力，在目标频段获得更好的天空覆盖

Conclusion: 月球引力波干涉测量具有显著优势，CIGO在特定频段性能优越，TCIGO配置进一步提升了角分辨率能力，为填补引力波观测空白提供了有前景的方案

Abstract: Lunar-based gravitational-wave interferometry is a fascinating endeavor, and was proposed as a promising approach to bridge the observational gap between space-borne and ground-based detectors. In this work, we adopt the Fisher-matrix method to examine the angular-resolution performance of the newly proposed Crater Interferometry Gravitational-wave Observatory (CIGO) on the lunar crater rim near the north pole, together with TianQin and LISA, for monochromatic sources in the 0.1-10 Hz band. We find that above 0.1 Hz, CIGO achieves better localization accuracy than the other two space-based missions and dominates the combined detector network's performance, provided that lunar noise mitigation is achieved in the 0.1-2.87 Hz frequency range. We further explore an upgraded Tetrahedron configuration, TCIGO, with a fourth station at the bottom of a crater, which forms a regular tetrahedral constellation on the lunar surface. The result shows that TCIGO yields a five-fold improvement in angular-resolution capability over CIGO and gets better sky coverage across the target frequency band.

</details>


### [197] [Transitioning late-time cosmology with the Hubble parameterization](https://arxiv.org/abs/2512.23561)
*Vinod Kumar Bhardwaj,Saibal Ray,Kazuharu Bamba,Akram Ali*

Main category: gr-qc

TL;DR: 该研究在Rastall理论框架下构建了晚期宇宙学模型，利用最新观测数据（Planck CMB、DESI-BAO、Union 3.0超新星）约束哈勃参数，发现宇宙演化存在从减速到加速的相变，并给出了与观测兼容的哈勃常数估计值。


<details>
  <summary>Details</summary>
Motivation: 研究Rastall理论下的晚期宇宙学模型，通过最新观测数据约束哈勃参数，探索宇宙演化中的相变现象，解决当前宇宙学观测中的参数一致性问题。

Method: 在Rastall理论框架下构建均匀各向同性时空的晚期宇宙学模型，结合Planck宇宙微波背景辐射、DESI重子声学振荡和Union 3.0型Ia超新星等最新观测数据集，对哈勃参数进行观测约束分析。

Result: 明确证明了宇宙演化存在特定的红移转变，即从初始减速时期到当前加速阶段的相变；利用DESI-BAO结合CC、CMB和Union 3.0数据集，得到哈勃常数当前值H₀ = 66.945 ± 1.094，与现有观测兼容。

Conclusion: Rastall理论能够描述宇宙从减速到加速的演化相变，最新观测数据支持该理论框架下的哈勃参数估计，为解决宇宙学参数一致性提供了理论依据。

Abstract: We investigate a late-time cosmological model for a homogeneous and isotropic space-time in the Rastall theory. We explore the observational constraints on the Hubble parameter by using the latest cosmological datasets such as cosmic microwave background radiation (Planck), baryon acoustic oscillations (DESI) and Type Ia Supernovae (Union 3.0). As a result, we explicitly demonstrate that the specific redshift transition occurs, namely, there happens a phase shift in the evolution of the universe from the initial deceleration era to the current accelerating phase of the cosmological scenario. Furthermore, we show that with the latest dataset of DESI-BAO clubbed with CC, CMB, and Union 3.0, the current value of the Hubble parameter is estimated as $H_0 = 66.945 \pm 1.094$, which can be compatible with the available observations.

</details>


### [198] [Primary black-hole scalar charges and kinetic screening in $K$-essence-Gauss-Bonnet gravity](https://arxiv.org/abs/2512.23683)
*Guillermo Lara,Georg Trenkler,Leonardo G. Trombetta*

Main category: gr-qc

TL;DR: 研究标量-高斯-邦尼耦合诱导的标量荷如何受非平庸动能项K(X)影响，发现时间依赖的标量场将黑洞标量荷从次级变为初级，并提供稳定性分析和动能屏蔽强度测量。


<details>
  <summary>Details</summary>
Motivation: 超越广义相对论的黑洞可能携带非标准荷，影响其现象学。研究标量-高斯-邦尼耦合诱导的标量荷如何受非平庸动能项K(X)影响，特别是动能屏蔽效应。

Method: 首先研究渐近平坦静态解中的动能屏蔽，然后转向由K(X)驱动的自加速宇宙学情况。通过混合标量和引力模式的四次色散关系进行稳定性分析和动能屏蔽强度测量。

Result: 时间依赖的标量场打开了参数空间，将黑洞标量荷从次级变为初级。提供了稳定性分析和从四次色散关系测量的动能屏蔽强度。

Conclusion: 非平庸动能项K(X)显著影响标量-高斯-邦尼耦合诱导的标量荷，在自加速宇宙学中时间依赖的标量场使标量荷从次级变为初级，动能屏蔽效应可通过混合模式的色散关系量化。

Abstract: Black holes beyond General Relativity may carry non-standard charges that impact their phenomenology. We study how the scalar charge that is induced by the scalar-Gauss-Bonnet coupling is affected by the presence of a nontrivial kinetic term $K(X)$. We discuss the corresponding kinetic screening in the asymptotically flat, static solution first. We then turn to the case where self-accelerating cosmology is driven by $K(X)$, finding that the time-dependence of the scalar field opens up the parameter space, turning the black-hole scalar charge from secondary to primary. We provide a stability analysis and a measure of the intensity of the kinetic screening from the quartic dispersion relation of the mixed scalar and gravitational modes.

</details>


### [199] [Note on the Kerr Spinning-Particle Equations of Motion](https://arxiv.org/abs/2512.23697)
*Joon-Hwi Kim*

Main category: gr-qc

TL;DR: 该论文实现了Newman-Janis算法的探针对应物，通过Wick旋转将全阶测地线偏离方程转化为精确自旋粒子运动方程的一部分，从而在自对偶扇区完全约束了Kerr黑洞在点粒子有效理论中的引力动力学。


<details>
  <summary>Details</summary>
Motivation: 研究Kerr黑洞引力动力学与自旋粒子运动之间的关系，探索隐藏对称性如何约束引力康普顿振幅，并建立全阶测地线偏离方程与自旋粒子运动方程的联系。

Method: 实现Newman-Janis算法的探针对应物，通过Wick旋转技术将全阶测地线偏离方程转化为精确自旋粒子运动方程的一部分，在自对偶扇区分析Kerr黑洞的引力动力学。

Result: 在自对偶扇区完全约束了Kerr黑洞在点粒子有效理论中的引力动力学，揭示了隐藏对称性导致相同螺旋度引力康普顿振幅到所有多重性的自旋指数化。

Conclusion: 该工作建立了测地线偏离方程与自旋粒子运动方程之间的深刻联系，为理解Kerr黑洞的引力动力学和自旋相关效应提供了新的理论框架，揭示了隐藏对称性在约束引力振幅方面的关键作用。

Abstract: We implement a probe counterpart of Newman-Janis algorithm, which Wick rotates the all-orders geodesic deviation equation into a part of exact spinning-particle equations of motion. Consequently, the gravitational dynamics of the Kerr black hole in its point-particle effective theory is completely constrained in the self-dual sector for a hidden symmetry, implying the spin exponentiation of same-helicity gravitational Compton amplitudes to all multiplicities.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [200] [A quantum advection-diffusion solver using the quantum singular value transform](https://arxiv.org/abs/2512.22163)
*Gard Olav Helle,Tommaso Benacchio,Anna Bomme Ousager,Jørgen Ellegaard Andersen*

Main category: quant-ph

TL;DR: 提出基于高阶有限差分算子和量子奇异值变换的线性对流扩散方程量子算法，高阶方法显著减少达到给定精度所需的门和量子比特数


<details>
  <summary>Details</summary>
Motivation: 线性对流扩散方程在流体力学、传热传质等领域广泛应用，传统数值模拟计算成本高，量子计算有望提供指数级加速

Method: 基于高阶有限差分算子的块编码和量子奇异值变换技术，构建线性对流扩散方程的量子模拟算法

Result: 高阶方法相比低阶方法显著减少了达到给定精度所需的量子门数量和量子比特数，一维和二维基准测试的数值模拟验证了理论结果

Conclusion: 高阶有限差分方法与量子奇异值变换结合能有效提升线性对流扩散方程量子模拟的效率，为量子计算在偏微分方程求解中的应用提供了新思路

Abstract: We present a quantum algorithm for the simulation of the linear advection-diffusion equation based on block encodings of high order finite-difference operators and the quantum singular value transform. Our complexity analysis shows that the higher order methods significantly reduce the number of gates and qubits required to reach a given accuracy. The theoretical results are supported by numerical simulations of one- and two-dimensional benchmarks.

</details>


### [201] [Wigner Cat Phases: A finely tunable system for exploring the transition to quantum chaos](https://arxiv.org/abs/2512.22169)
*M. Süzen*

Main category: quant-ph

TL;DR: 该论文提出了一种可精细调谐的混合随机矩阵系综(mGOE)，用于模拟从混沌到局域化的量子动力学相变，并识别出"Wigner猫相"作为过渡相。


<details>
  <summary>Details</summary>
Motivation: 研究量子动力学从混沌到局域化的相变，特别是多体局域化(MBL)转变，需要可调谐的模型系统来探索新相和相变机制。

Method: 使用混合高斯正交系综(mGOE)作为可调谐模型，通过数值研究其谱性质演化，包括经验谱密度、最近邻能级间距和相邻能隙比等统计量。

Result: 识别出依赖于可调参数的"Wigner猫相"作为过渡相，该相变可通过谱统计量的变化清晰表征，为研究本征态热化假说(ETH)相关转变提供了理想工具。

Conclusion: mGOE系综构成了研究量子混沌、MBL转变和ETH相关物理的可调谐模型家族，能够连续地从混沌相调节到局域化和重尾局域化相。

Abstract: The transition to chaos for quantum dynamics is quantified via a finely tunable mixed random matrix ensemble. The {\it mixed Gaussian Orthogonal Ensemble (mGOE)} forms a pedagogically accessible family of systems in simulating {\it Many-Body Localization (MBL)} transitions. It can be tuned from chaotic to localized and heavy-tailed localized phases in a continuous fashion, providing an opportunity to explore new phases. We numerically study how the spectral properties of mGOE evolve during these transitions. Characterization of transition to quantum chaos is computed and analyzed via empirical spectral density, nearest-neighbor spacing, and adjacent gap ratios with statistical uncertainty quantifications that strengthens the robustness of evidence of transitions. The transition is identified as {\it Wigner Cat Phases}, because of the shape of empirical spectral densities, which depens on the tuneable parameter. These simulated phases in mGOE appear to be an ideal tool to study {\it Eigenstate Thermalization Hypothesis (ETH)} and its related transitions, representing a family of physical systems under different localisation and disorder strengths.

</details>


### [202] [Bell-Inequality Violation for Continuous, Non-Projective Measurements](https://arxiv.org/abs/2512.22229)
*Shalender Singh,Santosh Kumar*

Main category: quant-ph

TL;DR: 该论文提出了一种从连续弱测量数据中提取贝尔不等式违反的理论框架，适用于无法进行投影测量的固态量子平台。


<details>
  <summary>Details</summary>
Motivation: 许多固态量子平台无法进行尖锐的投影测量，只能通过弱非破坏性读出获得连续电压或场迹。在这些系统中，基于二分投影测量的标准贝尔测试不适用，因此需要开发从连续时间序列数据中认证量子非定域性的方法。

Method: 开发了一个通用理论框架，通过相位敏感投影和粗粒化从连续非投影测量中提取贝尔-CHSH不等式违反。该方法不需要假设特定的坍缩模型或相位分布，而是通过足够长的连续测量采样纠缠对的内部相位概率结构，构建有效的二分可观测量。

Result: 贝尔关联器由两个实验可访问的资源控制：内在的单量子位相位展宽和量子位间的非定域相位锁定。通过Qiskit量子电路模拟与传统投影测量CHSH测试进行基准比较，在贝尔违反区域发现定量一致，无需参数拟合。

Conclusion: 该框架为在测量本质上是连续和弱的平台中展示贝尔非定域性提供了实用途径。经典确定性关联无法违反CHSH界限，而量子相位锁定系统恢复了纠缠特有的非线性角度依赖性。

Abstract: Many solid-state quantum platforms do not permit sharp, projective measurements but instead yield continuous voltage or field traces under weak, non-demolition readout. In such systems, standard Bell tests based on dichotomic projective measurements are not directly applicable, raising the question of how quantum nonlocality can be certified from continuous time-series data. Here we develop a general theoretical framework showing that Bell-CHSH inequality violation can be extracted from continuous, non-projective measurements without assuming any specific collapse model or phase distribution. We show that sufficiently long continuous measurements of a single entangled pair sample its internal phase-probability structure, enabling effective dichotomic observables to be constructed through phase-sensitive projections and coarse-graining. The resulting Bell correlator is governed by two experimentally accessible resources: intrinsic single-qubit phase spread and nonlocal phase locking between qubits. We benchmark the resulting estimator against conventional projective-measurement CHSH tests implemented via quantum-circuit simulations using Qiskit, finding quantitative agreement in the Bell-violating regime without parameter fitting. Classical deterministic correlations cannot violate the CHSH bound, whereas quantum phase-locked systems recover the nonlinear angular dependence characteristic of entanglement. Our results provide a practical route to demonstrating Bell nonlocality in platforms where measurements are inherently continuous and weak.

</details>


### [203] [Non-Relativistic Quantum Particle Confined on a Cylindrical Surface under a Stark-like Potential](https://arxiv.org/abs/2512.22232)
*Deriyan Senjaya*

Main category: quant-ph

TL;DR: 该研究探讨了类斯塔克扰动势对圆柱表面量子粒子的影响，及其在揭示额外维度方面的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 受卡鲁扎-克莱因理论启发，该理论通过额外空间维度统一电磁力和引力，但额外维度通常隐藏且需要高能条件才能探测。本研究旨在探索在较低能量量子系统中揭示这些额外维度的更可行方法。

Method: 在圆柱表面量子粒子系统中施加类斯塔克扰动势 $\hat{H}_{\text{SL}} = βzV_{o_{z}}(θ)$，该势受氢原子斯塔克效应启发。研究特别关注简并构型（$R_{o} = \frac{L}{π}$）下的能量修正。

Result: 在简并构型下，类斯塔克扰动有效诱导了能级分裂，这可以解释为揭示隐藏维度的手段。一阶能量修正明确依赖于量子数 $n_{z}$ 和 $n_θ$，表明该方法有潜力在较低能量量子系统中探测额外维度效应。

Conclusion: 类斯塔克扰动势为在较低能量条件下探测额外维度提供了一种有前景的方法，通过量子系统的能级分裂效应揭示隐藏的空间维度结构。

Abstract: This study explores the influence of a Stark-like perturbative potential on a quantum particle confined to a cylindrical surface (QPCS) and its implications for extra-dimensional theories. The QPCS framework is particularly relevant to Kaluza-Klein (KK) theory, which postulates extra spatial dimensions to unify electromagnetism and gravity. In KK theory, these extra dimensions are typically hidden and require high-energy conditions for detection. Motivated by the challenge of uncovering these dimensions more feasibly, this research applies a perturbative potential of the form \hat{H}_{\text{SL}} = βzV_{o_{z}}(θ) to a QPCS characterized by length \textit{L} and radius R_{o}. This potential is inspired by the Stark effect in hydrogen atoms, where energy level splitting serves as an indicator of an external influence. The study demonstrates that, for a degenerate configuration (R_{o} = \frac{L}π), the Stark-like perturbation effectively induces energy level splitting, which can be interpreted as a means of revealing hidden dimensions. The first-order energy correction in this scenario depends explicitly on the quantum numbers n_{z} and n_θ, highlighting the potential for this approach to probe extra-dimensional effects in lower-energy quantum systems.

</details>


### [204] [Partial Collapse and Ensemble Invariance under Continuous Quantum Measurement](https://arxiv.org/abs/2512.22235)
*Shalender Singh,Santosh Kumar*

Main category: quant-ph

TL;DR: 连续测量可以在不扰动系统稳态系综的情况下提取信息，实现条件坍缩与系综不变性的分离


<details>
  <summary>Details</summary>
Motivation: 传统观点认为波函数坍缩是量子测量的必然结果，但本文探索在驱动耗散量子系统中，连续测量能否提取信息而不扰动物理稳态系综

Method: 使用随机主方程形式，识别测量不变的稳态，这些态在连续监测下其无条件密度矩阵保持不变，尽管在单个量子轨迹层面存在测量诱导的坍缩

Result: 建立了连续测量下稳态不变性的充分必要条件，证明该条件在有限测量强度范围内成立，实现了信息获取与测量反作用的动态解耦

Conclusion: 在开放量子系统中，信息获取与测量反作用可以动态解耦，这对连续量子传感和非投影测量基础有重要意义

Abstract: Wavefunction collapse is often regarded as an unavoidable consequence of quantum measurement. Here we show that in driven-dissipative quantum systems, continuous measurement can extract information without disturbing the physical steady-state ensemble. Using the stochastic master equation formalism, we identify measurement-invariant steady states whose unconditional density matrix remains unchanged under continuous monitoring, despite the presence of measurement-induced collapse at the level of individual quantum trajectories. This separation between conditional collapse and ensemble invariance leads to a regime of partial collapse, in which measurement-induced localization is transient and continuously counteracted by dissipation and drive. We establish a necessary and sufficient condition for steady-state invariance under continuous measurement and show-that it holds over a finite range of measurement strengths. Our results clarify how information gain and measurement backaction can be dynamically decoupled in open quantum systems, with implications for continuous quantum sensing and the foundations of nonprojective measurement.

</details>


### [205] [Decoherence challenges in Nanoscience: A Quantum Phase Space perspective](https://arxiv.org/abs/2512.22297)
*Angelo Mamitiana Ralaikoto,Diary Lova Ratsimbazafy,Ravo Tokiniaina Ranaivoson,Fanamby Sahondraniandriana,Roland Raboanary,Raoelina Andriambololona,Nomenjanahary Tanjonirina Manampisoa,Rivo Herivola Manjakamanana Ravelonjato*

Main category: quant-ph

TL;DR: 该论文提出了一种基于量子相空间的新理论框架，用于表征环境选择的指针态并建模不同体系下的退相干动力学，为纳米尺度量子技术提供了统一的理论工具。


<details>
  <summary>Details</summary>
Motivation: 量子退相干既是量子到经典转变的基本机制，也是实现可扩展纳米量子技术的主要挑战。需要一种统一的理论框架来同时解决指针态表征和不同退相干动力学建模的双重挑战。

Method: 引入基于量子相空间的理论框架，将指针态识别为满足最小不确定性关系的状态，通过方差-协方差矩阵编码QPS结构，该结构直接由环境特性塑造。时间无关矩阵对应马尔可夫退相干，时间相关矩阵捕捉非马尔可夫动力学。

Result: 建立了环境参数与相空间结构之间的显式关系，为林德布拉德和非马尔可夫主方程提供了统一的几何形式体系。通过具体示例展示了该框架的应用潜力。

Conclusion: QPS框架有望成为纳米科学中建模退相通的强大工具，为设计缓解策略和利用非马尔可夫效应提供新原理，在基础理论和实际量子工程之间架起桥梁。

Abstract: Quantum decoherence, the process by which a quantum system loses its coherence through interaction with an environment and becomes classical-like, represents both the fundamental mechanism for the quantum-to-classical transition and a major challenge to realizing scalable nanoscale quantum technologies. This work introduces a novel theoretical framework based on Quantum Phase Space (QPS) to address the dual challenge of characterizing environment-selected pointer states and modeling decoherence dynamics across different regimes. Within this framework, pointer states for particle motion are identified as the minimum-uncertainty states, those that saturate the quantum uncertainty relation, thereby constituting the closest quantum analogue to classical phase-space points. The structure of the QPS, encoded in a variance-covariance matrix, is shown to be directly shaped by environmental properties. A time-independent matrix corresponds to Markovian (memoryless) decoherence, described by constant diffusion and friction coefficients, while a time-dependent matrix captures non-Markovian dynamics, characterized by environmental memory and information backflow. This unified geometric formalism, applied to both Lindblad and Non-Markovian master equations, enables us to derive explicit relations between environmental parameters and phase-space structure, as demonstrated in a specific illustrative example. This approach has the potential to serve as a powerful tool for modeling decoherence in nanoscience and could inform new principles for designing mitigation strategies and harnessing non-Markovian effects for quantum technologies. The QPS framework may thus bridge fundamental theory and practical quantum engineering, offering a promising coherent pathway to understand, control, and exploit decoherence at the nanoscience frontier.

</details>


### [206] [A Time-Symmetric Variational Formulation of Quantum Mechanics with Emergent Schrödinger Dynamics and Objective Boundary Randomness](https://arxiv.org/abs/2512.22320)
*Lance H. Carter*

Main category: quant-ph

TL;DR: 提出一种时间对称的变分公式，将薛定谔动力学和玻姆型引导定律作为最优性条件而非基本假设，通过费舍尔信息正则化产生量子势，自动满足玻恩规则。


<details>
  <summary>Details</summary>
Motivation: 传统量子力学将薛定谔方程和玻姆引导定律作为基本假设，需要额外的量子平衡假设（P=|ψ|²）来满足玻恩规则。本文旨在从变分原理中自然推导出这些结果，消除额外假设。

Method: 采用时间对称的变分公式，以概率密度和电流场为变量，受连续性约束和双时间边界条件限制。通过费舍尔信息正则化项产生量子势，将最优性系统表达为复数形式得到薛定谔方程。

Result: 成功从变分原理推导出薛定谔动力学和玻姆型引导定律，无需量子平衡假设即可自动满足玻恩规则。轨迹作为最小化费舍尔正则化作用量的唯一流体力学流出现，确定性轨迹仅作为有效的粗粒度描述。

Conclusion: 提出了一种新的量子力学变分表述，将量子动力学作为最优性条件而非基本假设，消除了传统玻姆力学中的额外假设，为量子力学基础提供了更自然的解释框架。

Abstract: We present a time-symmetric variational formulation of nonrelativistic quantum mechanics in which Schrödinger dynamics and a Bohm-type guidance law arise as emergent Euler-Lagrange optimality conditions rather than postulates. The formulation is expressed in terms of probability density and current fields subject to a continuity constraint and two-time boundary conditions. A Fisher-information regularization term generates the quantum potential, yielding the Schrödinger equation when the optimality system is expressed in complex form. Unlike standard Bohmian mechanics, which requires an auxiliary Quantum Equilibrium Hypothesis ($P = |ψ|^2$), our primal-dual formulation satisfies the Born rule by construction. The trajectories emerge not from an external guidance wave, but as the unique hydrodynamic flow minimizing the Fisher-regularized action between two-time boundary constraints. Deterministic trajectories thus emerge only as effective, coarse-grained descriptions, with randomness entering objectively at the interface of boundary constraints.

</details>


### [207] [Resonance matching of 2-$δ$ and 3-$δ$ potentials in 1D Quantum Scattering](https://arxiv.org/abs/2512.22332)
*Naw Sai*

Main category: quant-ph

TL;DR: 研究3-δ系统能否近似2-δ共振系统的传输谱，发现符号约束下完全等谱不可能，但确定了实际可行的最小约束集


<details>
  <summary>Details</summary>
Motivation: 探究在耦合强度符号约束下，量子散射问题中光谱非唯一性的实际限制和可实现精度，特别是研究3-δ系统能否近似具有相反符号耦合的2-δ共振系统的传输谱

Method: 结合理论分析和数值实验：理论分析证明在物理非平凡配置下完全等谱不可能；数值实验确定实际可行性的最小约束集

Result: 对于k<3的情况，理论证明完全等谱匹配在物理非平凡配置下不可能；数值实验确定了实际可行的最小约束集，建立了符号约束下共振匹配的实际限制和可实现精度

Conclusion: 在耦合强度符号约束下，3-δ系统无法完全匹配2-δ共振系统的传输谱，但确定了实际可行的近似精度限制，这对理解量子散射问题中的光谱非唯一性具有重要意义

Abstract: We investigate whether a 3-$δ$ system with positive coupling strengths can approximate the transmission spectrum of a 2-$δ$ resonance system with opposite-sign couplings for $k <3$. Theoretical analysis establishes exact isospectrality -- perfectly matched transmission spectrum -- is impossible for physically non-trivial configurations, while numerical experiments identify the minimal constraint set for practicability. These results establish both the practical limits and achievable accuracy of resonance matching under sign constraints, with implications for understanding spectral non-uniqueness in quantum scattering problems.

</details>


### [208] [Casimir Arc Plate Geometry: Computational Analysis of Thickness Constraints for Gold and Silver Nanomembranes in MEMS Applications](https://arxiv.org/abs/2512.22352)
*Anna-Maria Alexandrova,Jesus Valdiviezo*

Main category: quant-ph

TL;DR: 研究分析了圆弧与平板间的卡西米尔相互作用，探讨了微机电系统（MEMS）中柔性纳米膜在卡西米尔力作用下从凹向平板变为凸向的曲率反转现象，确定了发生反转的最大厚度条件。


<details>
  <summary>Details</summary>
Motivation: 圆弧与平板间的卡西米尔相互作用在MEMS制造中具有重要意义但尚未被充分研究。这种几何构型可能导致柔性纳米膜发生曲率反转，影响MEMS器件的可靠性和性能，因此需要理论分析来为MEMS设计提供厚度约束。

Method: 采用邻近力近似（PFA）结合次领头阶修正（NTLO）推导曲面卡西米尔能量；使用Kirchhoff-Love薄板理论计算弯曲能量；比较金和银材料的性能差异；分析卡西米尔能量大于弯曲能量时的最大厚度条件。

Result: 在0.1-1微米距离范围内，纳米级厚度的纳米膜会发生曲率反转；银纳米膜比金纳米膜能承受更大厚度；NTLO修正的PFA方法比微扰PFA更准确；该几何构型可实现卡西米尔驱动并防止粘附。

Conclusion: 圆弧-平板卡西米尔几何构型为MEMS提供了新的驱动机制，增强了器件可靠性，防止了粘附现象。研究结果为考虑卡西米尔力的MEMS设计提供了关键厚度约束，对MEMS性能优化具有重要意义。

Abstract: A theoretical analysis of the Casimir interaction between an arc and plate is conducted, which remains unexplored despite its relevance to Micro-Electro-Mechanical Systems (MEMS) fabrication. The configuration consists of a rigid finite plate and a flexible curved nanomembrane, with radius 100 micrometers, initially concave toward the rigid plate. The maximum thickness is evaluated for which the nanomembrane undergoes a change in curvature: from concave to convex with respect to the plate, due to the Casimir interaction. The Casimir energy for a curved surface is derived using the Proximity Force Approximation (PFA) with next-to-leading-order (NTLO) corrections. Kirchhoff-Love theory for a thin isotropic plate of constant thickness is used to estimate the bending energy. Material-dependent effects on the Casimir interaction are evaluated by comparing Au and Ag plates. The maximum thickness is derived where U_Casimir > U_bending for distances in the range of 0.1-1 micrometers. Results show curvature reversal occurs for nanomembranes with nanoscale thicknesses at the studied distances. Silver nanomembranes tolerate greater thickness than gold nanomembranes due to material-dependent properties. Comparison between NTLO-corrected PFA and perturbative PFA confirms the accuracy of the NTLO approach. The Casimir arc-to-plate geometry in MEMS enables Casimir-based actuation, enhances devices reliability, and prevents stiction. These findings provide thickness constraints for MEMS design and performance, accounting for the Casimir force.

</details>


### [209] [Creating multicomponent Schrödinger cat states in a coupled qubit-oscillator system](https://arxiv.org/abs/2512.22380)
*Pavel Stránský,Pavel Cejnar*

Main category: quant-ph

TL;DR: 提出一种通过耦合半经典振子与量子比特系统来制备各种薛定谔猫态变体的方法，可产生任意数量部分相干波包组成的非经典态


<details>
  <summary>Details</summary>
Motivation: 传统薛定谔猫态制备方法有限，需要开发更灵活、可调谐的方法来产生复杂的非经典态，以应用于量子信息和传感协议

Method: 将半经典振子与量子比特系统耦合，通过改变量子比特数量和量子淬火参数，控制振子进入由任意数量部分相干波包组成的非经典态

Result: 能够制备高度非经典的振子态，包含任意数量的部分相干波包，具有可调谐的比例和运动关系，且可用现有实验技术实现

Conclusion: 该方法为制备复杂的薛定谔猫态变体提供了灵活且实验可行的途径，有望在量子信息和传感领域找到应用

Abstract: We present a method for preparing various exotic modifications of Schrödinger cat states by coupling a semiclassical oscillator to a system of qubits. Varying the number of qubits and parameters of the quantum quench performed in the coupled system, we bring the oscillator into a~highly non-classical state composed of an arbitrary number of partly coherent wavepackets in tunable proportions and motion relations. The method can be implemented with the aid of current experimental techniques and may find applications in quantum information and sensing protocols.

</details>


### [210] [The Lieb-Robinson correlation function for long disordered transverse-field Ising chains](https://arxiv.org/abs/2512.22395)
*Brendan J. Mahoney,Craig S. Lent*

Main category: quant-ph

TL;DR: 本文提出一种线性复杂度方法计算Lieb-Robinson关联函数，可处理数百量子比特的横向场伊辛链，并扩展到无序耦合系统，发现无序导致量子关联局域化


<details>
  <summary>Details</summary>
Motivation: 传统计算Lieb-Robinson关联函数需要指数级增长的状态空间，仅适用于小系统。需要开发可扩展方法研究大系统中量子信息传播

Method: 采用新开发的线性复杂度方法直接计算Lieb-Robinson关联函数，可扩展到数百量子比特系统。进一步将该技术扩展到具有随机无序耦合强度的伊辛链

Result: 方法成功计算了数百量子比特系统的关联函数，揭示了量子信息沿链的传播。无序增加导致量子关联局域化，量子信息传播停止

Conclusion: 线性复杂度方法使大尺度量子系统研究成为可能，无序耦合导致量子信息传播受阻，为量子多体系统动力学提供了新见解

Abstract: The transverse-field Ising model is useful for studying interacting qubit arrays. The Lieb--Robinson correlation function can be used to characterize the propagation of quantum information in Ising chains. Considerable work has been done to establish bounds on this correlation function in various circumstances. To actually calculate the value of the correlation function directly typically requires a state space which grows exponentially with system size, and so is intractable for all but relatively small systems. We employ a recently-developed method that enables direct calculation of the value of the Lieb--Robinson correlation function and which scales linearly with system size. This enables the computation for systems with many hundreds of qubits, revealing the propagation of quantum information down the chain. We extend this technique to the problem of Ising chains with randomly disordered coupling strengths. Increasing disorder causes localization of the quantum correlations and halts propagation of quantum information.

</details>


### [211] [Quantum-Circuit Framework for Two-Stage Stochastic Programming via QAOA Integrated with a Quantum Generative Neural Network](https://arxiv.org/abs/2512.22434)
*Taihei Kuroiwa,Daiki Yamazaki,Keita Takahashi,Kodai Shiba,Chih-Chieh Chen,Tomah Sogabe*

Main category: quant-ph

TL;DR: 提出qGAN-QAOA量子电路工作流，将量子生成对抗网络用于场景分布编码，用量子近似优化算法优化两阶段随机规划的第一阶段决策，实现场景数量的多对数规模扩展。


<details>
  <summary>Details</summary>
Motivation: 传统两阶段随机规划通过场景枚举处理不确定性，但场景数量线性增加导致计算复杂度急剧上升。需要开发能高效处理大量场景的量子计算方法。

Method: qGAN-QAOA统一工作流：预训练qGAN编码场景分布，固定qGAN参数后，用QAOA优化第一阶段决策，通过最小化包含期望追索成本的两阶段目标函数。将非预期性解释为测量结果统计条件，证明第一阶段测量边际与场景独立。

Result: 对于均匀离散的不确定性，对角算子编码通过Walsh-Hadamard变换获得稀疏Pauli-Z展开，实现门数量和电路深度随场景数量的多对数规模扩展。在光伏不确定性的随机机组组合问题上，相比经典期望值和两阶段随机规划基线，展示了方法的有效性。

Conclusion: qGAN-QAOA为两阶段随机规划提供了有效的量子计算方法，能高效处理大量场景，在随机机组组合等实际问题中展现出潜力。

Abstract: Two-stage stochastic programming often discretizes uncertainty into scenarios, but scenario enumeration makes expected recourse evaluation scale at least linearly in the scenario count. We propose qGAN-QAOA, a unified quantum-circuit workflow in which a pre-trained quantum generative adversarial network encodes the scenario distribution and QAOA optimizes first-stage decisions by minimizing the full two-stage objective, including expected recourse cost. With the qGAN parameters fixed after training, we evaluate the objective as the expectation value of a problem Hamiltonian and optimize only the QAOA variational parameters. We interpret non-anticipativity as a condition on measurement outcome statistics and prove that the first-stage measurement marginal is independent of the scenario. For uniformly discretized uncertainty, the diagonal operator encoding the uncertainty admits a sparse Pauli-Z expansion via the Walsh--Hadamard transform, yielding polylogarithmic scaling of gate count and circuit depth with the number of scenarios. Numerical experiments on the stochastic unit commitment problem (UCP) with photovoltaic (PV) uncertainty compare the expected cost of the proposed method with classical expected-value and two-stage stochastic programming baselines, demonstrating the effectiveness of qGAN-QAOA as a two-stage decision model.

</details>


### [212] [Variational quantum eigensolver for chemical molecules](https://arxiv.org/abs/2512.22572)
*Luca Ion,Adam Smith*

Main category: quant-ph

TL;DR: 使用量子计算技术（VQE）计算He-H+和H2O分子的基态和基态能量，在量子模拟器和IBM量子设备上实现，并与经典方法得到的精确基态能量进行基准测试


<details>
  <summary>Details</summary>
Motivation: 解决相互作用多粒子系统是量子化学和凝聚态物理中的核心挑战，需要探索量子计算在分子基态计算中的应用潜力

Method: 采用变分量子本征求解器（VQE），在量子计算机模拟器和IBM量子设备上实现，对H2O分子的模拟在诺丁汉高性能计算设施上进行

Result: 获得了He-H+和H2O分子的基态能量计算结果，并与经典方法得到的精确基态能量进行了基准比较

Conclusion: 量子计算技术（特别是VQE）可用于计算分子系统的基态性质，为量子化学计算提供了有前景的方法

Abstract: Solving interacting multi-particle systems is a central challenge in quantum chemistry and condensed matter physics. In this work, we investigate the computation of ground states and ground-state energies for the He-H+ and H2O molecules using quantum computing techniques. We employ the variational quantum eigensolver (VQE), implemented both on a quantum computer simulator and on an IBM quantum device. The resulting energies are benchmarked against exact ground-state energies obtained via classical methods. Simulations of the H2O molecule were performed on Nottingham's High Performance Computing (HPC) facilities.

</details>


### [213] [Quasi-harmonic spectra from branched Hamiltonians](https://arxiv.org/abs/2512.22510)
*Aritra Ghosh,Bijan Bagchi,A. Ghose-Choudhury,Partha Guha,Miloslav Znojil*

Main category: quant-ph

TL;DR: 该研究重新审视修正Emden方程的量子化，特别关注其分支哈密顿量的量子化，发现小k值时能谱不再是完美谐波而是近似等间距的准谐波行为。


<details>
  <summary>Details</summary>
Motivation: 虽然修正Emden方程的经典等时性和正则量子化已被研究过，但对其分支哈密顿量的量子化尚未充分探索。该研究旨在分析分支哈密顿量量子化后的能谱特性。

Method: 采用数值计算和基于微扰理论的解析计算相结合的方法。对于小k值，通过数值方法研究量子化分支哈密顿量的能谱，并用微扰理论精确验证数值结果。

Result: 研究发现，分支哈密顿量量子化后的能谱不再是完美谐波（完全等间距），而是近似等间距的准谐波行为，表现出与均匀间距的偏差。

Conclusion: 修正Emden方程的分支哈密顿量量子化导致准谐波能谱，这与传统正则量子化产生的完美谐波能谱形成对比，为理解这类系统的量子特性提供了新见解。

Abstract: We revisit the canonical quantization to assess the spectrum of the modified Emden equation $\ddot{x} + kx\dot{x} + ω^2 x + \frac{k^2}{9}x^3 = 0$, which is an isochronous case of the Liénard-Kukles equation. While its classical isochronicity and canonical quantization, leading to polynomial solutions with an exactly-equispaced spectrum have been discussed earlier, including in the recent paper [Int. J. Theor. Phys. 64, 212 (2025)], the present study focuses on the quantization of its branched Hamiltonians. For small $k$, we show numerically that the resulting energy spectrum is no longer perfectly harmonic but only approximately equispaced, exhibiting quasi-harmonic behavior characterized by deviations from uniform spacing. Our numerical results are precisely validated by analytical calculations based on perturbation theory.

</details>


### [214] [Enhanced separability criteria based on symmetric measurements](https://arxiv.org/abs/2512.22514)
*Yu Lu,Hao-Fan Wang,Meng Su,Zhi-Xi Wang,Shao-Ming Fei*

Main category: quant-ph

TL;DR: 提出基于局域对称测量的可分性判据，比现有方法更高效检测纠缠，并推广到任意多体系统


<details>
  <summary>Details</summary>
Motivation: 现有纠缠检测方法效率有限，需要更高效且实验可行的判据，特别是对于多体系统

Method: 基于局域对称测量构建可分性判据，利用测量结果的概率分布与纠缠的关系

Result: 新判据比现有方法更高效检测纠缠，通过具体例子证明，并成功推广到任意多体系统

Conclusion: 建立了量子纠缠与局域测量结果概率之间更丰富的联系，提供了实验可行的纠缠检测工具

Abstract: We present separability criteria based on local symmetric measurements. These experimental plausible criteria are shown to be more efficient in detecting entanglement than the current counterparts by detailed examples. Furthermore, we generalize the separability criteria from bipartite to arbitrary multipartite systems. These criteria establish a richer connection between the quantum entanglement and the probabilities of local measurement outcomes.

</details>


### [215] [Quantum Noise Spectroscopy of Nanoscale Charge Defects in Silicon Carbide at Room Temperature](https://arxiv.org/abs/2512.22521)
*Jinpeng Liu,Yuanhong Teng,Yu Chen,Yixuan Wang,Chihang Luo,Jun Yin,Hao Li,Lixing You,Ya Wang,Qi Zhang,Fazhan Shi*

Main category: quant-ph

TL;DR: 利用SiC中的PL5色心作为室温量子传感器，首次实现了商业半导体中单电荷隧穿动力学的纳米尺度实时观测，并开发了电噪声成像和宽频噪声谱分析技术。


<details>
  <summary>Details</summary>
Motivation: 纳米尺度电荷环境对半导体物理和器件性能至关重要，但传统体表征技术缺乏空间分辨率来解析纳米尺度电荷异质性和识别微观噪声源。

Method: 使用4H-SiC中的PL5色心作为室温宽带量子传感器，通过光学探测磁共振(ODMR)监测随机电报噪声，结合动态解耦技术进行噪声谱分析，并利用T1弛豫谱探测MHz-GHz噪声。

Result: 首次在室温下实时观测到商业半导体中的单电荷隧穿动力学；实现了电噪声成像，显示不同晶圆衬底间的噪声差异；将噪声谱分析扩展到MHz频率；获得了SiC中电荷缺陷的首个纳米尺度电子顺磁共振(EPR)谱指纹。

Conclusion: 这些技术为表征半导体器件中的噪声环境开辟了新途径，为优化SiC制造工艺、缺陷控制和推进量子技术提供了关键见解。

Abstract: The nanoscale charge environment critically influences semiconductor physics and device performance. While conventional bulk characterization techniques provide volume-averaged defect properties, they lack the spatial resolution to resolve nanoscale charge heterogeneity and identify microscopic noise sources. Here, we utilize single PL5 centers in 4H-SiC as room-temperature broadband quantum sensors to fill in the gap. We report the first real-time, nanoscale observation of singlecharge tunneling dynamics in a commercial semiconductor at room temperature, by monitoring the random telegraph noise using optically detected magnetic resonance (ODMR). This capability enables an electrical noise imaging technique, showing distinct noise variations across different wafer substrates. By employing dynamical decoupling, we extend noise spectroscopy from near-DC to MHz frequencies, uncovering significant noise spectral density correlations across frequency bands. Finally, we probe MHz-GHz noise and identify its origin via T1 relaxation spectroscopy, obtaining the first nanoscale electron paramagnetic resonance (EPR) spectroscopic fingerprint of charge defects in SiC. These techniques open avenues for characterizing noise environments in semiconductor devices, providing critical insights for optimizing SiC fabrication processes, defect control, and advancing quantum technologies.

</details>


### [216] [Operational entanglement of collective quantum modes at room temperature](https://arxiv.org/abs/2512.22532)
*Shalender Singh,Santosh Kumar*

Main category: quant-ph

TL;DR: 该论文证明集体量子模式在环境温度下可以保持宏观纠缠，推导了基于部分转置正性的精确纠缠边界，并建立了连接集体模式动力学、噪声注入、距离和宏观纠缠操作认证的最小框架。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为量子纠缠在环境温度和宏观距离下是脆弱的，因为热噪声和耗散会迅速抑制非经典关联。本文挑战这一直觉，研究集体量子模式在开放系统通道下的动力学，而非微观热平衡。

Method: 推导了基于部分转置正性的精确纠缠边界（在对称共振极限下有效）；建立了用可测量的噪声、带宽、耗散和距离相关耦合参数表达的集体涨落最小振幅；通过随机模拟开放系统动力学，并与匹配的经典相关噪声零模型进行比较分析。

Result: 获得了维持稳态纠缠所需的最小集体涨落振幅；证明大集体占据抑制但不消除量子相位扩散，稳态保持相位对称性；随机模拟确认纠缠见证仅在量子区域被违反。

Conclusion: 建立了连接集体模式动力学、噪声注入、距离和宏观纠缠操作认证的平台无关最小框架，挑战了关于环境温度下纠缠脆弱性的传统直觉。

Abstract: Quantum entanglement is commonly assumed to be fragile at ambient temperature and over macroscopic distances, where thermal noise and dissipation are expected to rapidly suppress nonclassical correlations. Here we show that this intuition fails for collective quantum modes whose dynamics is governed by reduced open-system channels rather than by microscopic thermal equilibrium. For two spatially separated collective modes, we derive an exact entanglement boundary based on the positivity of the partial transpose, valid in the symmetric resonant limit. From this result we obtain an explicit minimum collective fluctuation amplitude, expressed entirely in measurable noise, bandwidth, dissipation, and distance-dependent coupling parameters, required to sustain steady-state entanglement at finite temperature. We further show that large collective occupation suppresses but does not eliminate quantum phase diffusion, so the steady state remains phase symmetric and does not collapse to a classical mean-field despite macroscopic signal amplitudes. Stochastic simulations of the reduced open-system dynamics, together with matched classical correlated-noise null models analyzed through an identical pipeline, confirm that entanglement witnesses are violated only in the quantum regime. Our results establish a minimal, platform-independent framework connecting collective-mode dynamics, noise injection, distance, and operational certification of macroscopic entanglement.

</details>


### [217] [Entanglement protection induced by mixed noise](https://arxiv.org/abs/2512.22541)
*Tengtao Guo,Yuxuan Zhou,Jiahui Feng,Xinyu Zhao,Yan Xia*

Main category: quant-ph

TL;DR: 混合噪声可以保护双原子腔系统中的纠缠，高频噪声成分能抑制腔泄漏引起的退相干


<details>
  <summary>Details</summary>
Motivation: 传统观点认为噪声有害，但本文探索噪声在量子系统中可能具有的保护作用，特别是混合噪声对纠缠的保护机制

Method: 建立双原子腔系统模型，将腔泄漏和随机原子-腔耦合建模为两种噪声类型，通过解析推导动力学方程，研究Ornstein-Uhlenbeck噪声、闪烁噪声和电报噪声等混合噪声对纠缠的保护作用

Result: 高频噪声成分能有效抑制腔泄漏引起的退相干，从而保护纠缠；纠缠保护效果取决于混合噪声功率谱密度中高频成分的比例

Conclusion: 增强高频成分对于有效的噪声辅助纠缠保护至关重要，为实际开放量子系统中的噪声工程提供了关键见解

Abstract: Contrary to the conventional view that noise is detrimental, we show that mixed noise can protect entanglement in a two-atom-cavity system. Specifically, the leakage of the cavity and the stochastic atom-cavity couplings are modeled as two types of noises. From the analytical derivation of the dynamical equations, the mechanism of the entanglement protection is revealed as the high-frequency(HF) noise in the atom-cavity couplings could suppress the decoherence caused by the cavity leakage, thus protect the entanglement. We investigate the entanglement protection induced by mixed noise constructed from diverse noise types, including the Ornstein-Uhlenbeck noise, flicker noise, and telegraph noise. Numerical simulations demonstrate that entanglement protection depends critically on the proportion of HF components in the power spectral density of the mixed noise. Our work establishes that enhanced HF components are essential for effective noise-assisted entanglement protection, offering key insights for noise engineering in practical open quantum systems.

</details>


### [218] [Quantum preferential attachment](https://arxiv.org/abs/2512.22542)
*Tingyu Zhao,Balázs Maga,Pierfrancesco Dionigi,Gergely Ódor,Kyle Soni,Anastasiya Salova,Bingjie Hao,Miklós Abért,István A. Kovács*

Main category: quant-ph

TL;DR: 提出量子偏好依附模型，新节点可连接到目标节点附近的任意节点，导致产生两类小世界但非无标度的复杂网络结构


<details>
  <summary>Details</summary>
Motivation: 量子互联网快速发展，但量子网络结构尚不明确。间接量子通信已可行且保持信道绝对安全，新节点无需直接连接到目标节点

Method: 提出量子偏好依附模型，新节点均匀随机连接到目标节点附近的任意节点（包括目标本身）。通过数值模拟和严格分析验证

Result: 局部连接灵活性显著改变全局网络行为，产生两类小世界但非无标度的复杂网络结构。建立包含量子和经典偏好依附变体的统一相图

Conclusion: 量子网络结构具有独特特征，结果对具有连接灵活性的经典场景也有广泛意义

Abstract: The quantum internet is a rapidly developing technological reality, yet, it remains unclear what kind of quantum network structures might emerge. Since indirect quantum communication is already feasible and preserves absolute security of the communication channel, a new node joining the quantum network does not need to connect directly to its desired target. Instead, in our proposed quantum preferential attachment model, it uniformly randomly connects to any node within the proximity of the target, including, but not restricted to, the target itself. This local flexibility is found to qualitatively change the global network behavior, leading to two distinct classes of complex network architectures, both of which are small-world, but neither of which is scale-free. Our numerical findings are supported by rigorous analytic results, in a framework that incorporates quantum and classical variants of preferential attachment in a unified phase diagram. Besides quantum networks, we expect that our results will have broad implications for classical scenarios where there is flexibility in establishing new connections.

</details>


### [219] [Experimental Joint Estimation of Phase and Phase Diffusion via Deterministic Bell Measurements](https://arxiv.org/abs/2512.22558)
*Ben Wang,Minghao Mi,Huangqiuchen Wang,Qian Xie,Lijian Zhang*

Main category: quant-ph

TL;DR: 该论文通过实验演示了在双量子比特系统中使用确定性贝尔测量进行相位和相位扩散的联合估计，展示了集体测量在多参数量子计量中的优势。


<details>
  <summary>Details</summary>
Motivation: 相位估计在量子计量中至关重要，但相位扩散噪声会严重影响其精度。虽然相位和相位扩散的联合估计可以解决这一问题，但不同参数的最优测量之间的不兼容性阻碍了单拷贝测量达到量子Cramer-Rao界定义的基本精度极限。

Method: 使用线性光学网络实现参数编码和确定性贝尔测量，在双量子比特系统上进行实验演示。通过集体测量（确定性贝尔测量）来缓解不同参数最优测量之间的不兼容性。

Result: 实现了比任何可分离测量策略更高的估计精度，展示了集体测量在多参数联合估计中的优势。

Conclusion: 该工作提出了相位扩散噪声下相位估计的新框架，并强调了集体测量在多参数量子计量中的显著优势，为量子精密测量提供了新的实验方案。

Abstract: Accurate phase estimation plays a pivotal role in quantum metrology, yet its precision is significantly affected by noise, particularly phase-diffusive noise caused by phase drift. To address this challenge, the joint estimation of phase and phase diffusion has emerged as an effective approach, transforming the problem into a multi-parameter estimation task. However, the incompatibility between optimal measurements for different parameters prevents single-copy measurements from reaching the fundamental precision limits defined by the quantum Cramer-Rao bound. Meanwhile, collective measurements performed on multiple identical copies can mitigate this incompatibility and thus enhance the precision of joint parameter estimation. This work experimentally demonstrates joint phase and phase-diffusion estimation using deterministic Bell measurements on a two-qubit system. A linear optical network is employed to implement both parameter encoding and deterministic Bell measurements, achieving improved estimation precision compared to any separable measurement strategy. This work proposes a new framework for phase estimation under phase-diffusive noise and underscores the substantial advantages of collective measurements in multi-parameter quantum metrology.

</details>


### [220] [Modeling Noise in Quantum Computing of Scalar Convection](https://arxiv.org/abs/2512.22559)
*Jiahua Yang,Zhen Lu,Yue Yang*

Main category: quant-ph

TL;DR: 量子噪声在流体模拟中表现为确定性物理项而非纯随机扰动


<details>
  <summary>Details</summary>
Motivation: 量子计算在流体动力学模拟中具有加速潜力，但NISQ时代的硬件噪声会显著扭曲模拟精度。虽然误差幅度经常被量化，但量子噪声对流动模拟结果的具体物理效应仍不清楚。

Method: 研究门噪声对一维标量对流量子模拟的影响。采用量子谱算法，其中理想时间推进仅影响傅里叶相位，从而隔离和分析噪声引起的谱幅度伪影。基于计算基态之间的汉明距离推导理论转移矩阵来预测谱衰减，并通过密度矩阵模拟和超导量子处理器实验验证模型。使用数据驱动的稀疏回归分析量子噪声在有效偏微分方程中的表现。

Result: 量子噪声在有效偏微分方程中主要表现为人工扩散和非线性源项。量子误差可以被建模为确定性物理项而非纯随机扰动。

Conclusion: 量子噪声在流体模拟中会产生可预测的物理效应，这些效应可以被建模为确定性项，这为理解和缓解量子计算中的噪声影响提供了新视角。

Abstract: Quantum computing holds potential for accelerating the simulation of fluid dynamics. However, hardware noise in the noisy intermediate-scale quantum era significantly distorts simulation accuracy. Although error magnitudes are frequently quantified, the specific physical effects of quantum noise on flow simulation results remain largely uncharacterized. We investigate the influence of gate noise on the quantum simulation of one-dimensional scalar convection. By employing a quantum spectral algorithm where ideal time advancement affects only Fourier phases, we isolate and analyze noise-induced artifacts in spectral magnitudes. We derive a theoretical transition matrix based on Hamming distances between computational basis states to predict spectral decay, and then validate this model against density-matrix simulations and experiments on a superconducting quantum processor. Furthermore, using data-driven sparse regression, we demonstrate that quantum noise manifests in the effective partial differential equation primarily as artificial diffusion and nonlinear source terms. These findings suggest that quantum errors can be modeled as deterministic physical terms rather than purely stochastic perturbations.

</details>


### [221] [Asymmetry effects in homodyne and heterodyne measurements: Positive operator-valued measures and asymptotic security of Gaussian continuous variable quantum key distribution](https://arxiv.org/abs/2512.22591)
*A. S. Naumchik,Roman K. Goncharov,Alexei D. Kiselev*

Main category: quant-ph

TL;DR: 研究高斯近似下同相和双同相测量中的不对称效应，分析其对连续变量量子密钥分发安全性的影响。


<details>
  <summary>Details</summary>
Motivation: 在实际量子测量中，分束器不平衡和探测器量子效率变化会导致不对称效应，这些效应会影响测量精度和量子密钥分发的安全性，需要系统分析。

Method: 使用高斯近似描述光电计数统计，计算含不对称效应的噪声测量的Q符号，获得POVM的算子表示，并将其表示为理想测量POVM经过加性噪声量子信道的形式。

Result: 发现双同相测量的无噪声测量需要用压缩态投影仪表示，相应的压缩态算子表示依赖于压缩参数；不对称效应导致互信息、Holevo信息和渐近密钥率下降。

Conclusion: 测量不对称效应会显著降低CV-QKD协议的性能，双同相测量的POVM表示非唯一性需要通过优化压缩参数来最小化Holevo信息，提高安全性。

Abstract: We use the Gaussian approximation describing photocount statistics for both the homodyne and the double homodyne (heterodyne) measurements to study asymmetry effects arising from imbalance of the beam splitters and variations in quantum efficiencies of the photodetectors. After computing the $Q$ symbols of the positive operator-valued measures (POVMs) of noisy measurements that take into account the asymmetry effects, the operator representations for the POVMs are obtained in the form that assumes applying the additive noise quantum channel to the POVMs of noiseless (ideal) measurements. For double homodyne detection, it was found that the noiseless measurements should generally be expressed in terms of the projectors onto squeezed-states and the corresponding squeezed-state operator representation of POVM along with the measurement noise channel depend on the squeezing parameter that lies in the interval dictated by the condition for the excess noise covariance matrix to be positive semi-definite. The analytical results are used to perform analysis of the asymptotic security of the Gaussian-modulated continuous variable quantum key distribution (CV-QKD) protocol in the untrusted-noise scenario where the measurement noise is assumed to be accessible to an adversary. The inherent non-uniqueness of the operator representation for the double-homodyne POVM manifests itself in the squeezing dependent Holevo information that needs to be additionally optimized. For both types of the measurements, the mutual information, the Holevo information and the asymptotic secret fraction are sensitive to asymmetry effects leading to degraded performance of the protocol.

</details>


### [222] [1d-qt-ideal-solver: 1D Idealized Quantum Tunneling Solver with Absorbing Boundaries](https://arxiv.org/abs/2512.22634)
*Sandy H. S. Herho,Siti N. Kaban,Rusmawan Suwarman,Iwan P. Anwar,Nurjanna J. Trilaksono*

Main category: quant-ph

TL;DR: 开发了一个用于一维量子隧穿动力学模拟的开源Python库，采用分裂算符方法和FFT谱微分，验证了矩形和Gaussian势垒的隧穿特性，适用于教学和初步探索。


<details>
  <summary>Details</summary>
Motivation: 为量子力学教学和隧穿动力学初步探索提供一个可部署的工具，在理想化条件下（排除耗散、环境耦合和多体相互作用）实现高效的一维量子隧穿模拟。

Method: 采用分裂算符方法结合二阶Trotter-Suzuki分解，使用FFT谱微分处理动能算符，引入复数吸收势消除边界反射，利用Numba即时编译提升性能。

Result: 验证了矩形势垒（模拟氧化物层场发射）和Gaussian势垒（模拟扫描隧道显微镜相互作用）两种典型测试案例，在飞秒尺度传播中实现机器精度的能量守恒。矩形势垒在过势垒区域表现出略高的透射系数。

Conclusion: 该库在理想化条件下提供高数值保真度的量子隧穿模拟，适用于教学和定性洞察，但不适用于定量实验预测。相空间分析确认了时空平均下的完全退相干。

Abstract: We present 1d-qt-ideal-solver, an open-source Python library for simulating one-dimensional quantum tunneling dynamics under idealized coherent conditions. The solver implements the split-operator method with second-order Trotter-Suzuki factorization, utilizing FFT-based spectral differentiation for the kinetic operator and complex absorbing potentials to eliminate boundary reflections. Numba just-in-time compilation achieves performance comparable to compiled languages while maintaining code accessibility. We validate the implementation through two canonical test cases: rectangular barriers modeling field emission through oxide layers and Gaussian barriers approximating scanning tunneling microscopy interactions. Both simulations achieve exceptional numerical fidelity with machine-precision energy conservation over femtosecond-scale propagation. Comparative analysis employing information-theoretic measures and nonparametric hypothesis tests reveals that rectangular barriers exhibit moderately higher transmission coefficients than Gaussian barriers in the over-barrier regime, though Jensen-Shannon divergence analysis indicates modest practical differences between geometries. Phase space analysis confirms complete decoherence when averaged over spatial-temporal domains. The library name reflects its scope: idealized signifies deliberate exclusion of dissipation, environmental coupling, and many-body interactions, limiting applicability to qualitative insights and pedagogical purposes rather than quantitative experimental predictions. Distributed under the MIT License, the library provides a deployable tool for teaching quantum mechanics and preliminary exploration of tunneling dynamics.

</details>


### [223] [Nonadiabatic Self-Healing of Trotter Errors in Digitized Counterdiabatic Dynamics](https://arxiv.org/abs/2512.22636)
*Mara Vizzuso,Gianluca Passarelli,Giovanni Cantele,Procolo Lucignano,Xi Chen,Koushik Paul*

Main category: quant-ph

TL;DR: 论文研究了数字化量子动力学中的Trotter误差，发现在有限时间内通过反绝热驱动补偿非绝热误差后，Trotter误差仍能保持自愈特性。


<details>
  <summary>Details</summary>
Motivation: 研究数字化量子动力学中Trotter误差的行为，特别是在有限时间演化而非仅绝热极限下，探索误差自愈现象是否仍然存在及其机制。

Method: 使用反绝热驱动来消除非绝热跃迁，隔离离散化效应，研究非相互作用和相互作用的自旋模型，分析Trotter步数和总演化时间的有限时间标度行为。

Result: 在驱动哈密顿量的瞬时本征基中，主要的数字误差映射为有效的谐波扰动，其主导傅里叶分量给出了有限时间Trotter误差的解析上界，揭示了自愈背后的相位抵消机制。

Conclusion: 有限时间自愈是数字化反绝热协议的通用特性，阐明了超越长时间绝热极限的自愈机制，为基于门的量子处理器上的高保真态制备提供了实用指导。

Abstract: Trotter errors in digitized quantum dynamics arise from approximating time-ordered evolution under noncommuting Hamiltonian terms with a product formula. In the adiabatic regime, such errors are known to exhibit long-time self-healing [Phys. Rev. Lett. \textbf{131}, 060602 (2023)], where discretization effects are effectively suppressed. Here we show that self-healing persists at finite evolution times once nonadiabatic errors induced by finite-speed ramps are compensated. Using counterdiabatic driving to cancel diabatic transitions and isolate discretization effects, we study both noninteracting and interacting spin models and characterize the finite-time scaling with the Trotter steps and the total evolution time. In the instantaneous eigenbasis of the driven Hamiltonian, the leading digital error maps to an effective harmonic perturbation whose dominant Fourier component yields an analytic upper bound on the finite-time Trotter error and reveals the phase-cancellation mechanism underlying self-healing. Our results establish finite-time self-healing as a generic feature of digitized counterdiabatic protocols, clarify its mechanism beyond the long-time adiabatic limit, and provide practical guidance for high-fidelity state preparation on gate-based quantum processors.

</details>


### [224] [Measuring out-of-time-order correlators on a quantum computer based on an irreversibility-susceptibility method](https://arxiv.org/abs/2512.22643)
*Haruki Emori,Hiroyasu Tajima*

Main category: quant-ph

TL;DR: 实验在量子计算机上测量了OTOC（时间无序关联函数），使用了三种方法：时间倒转法、弱测量法和不可逆性敏感度法，首次实验演示了ISM方法，并比较了三种方法的优缺点。


<details>
  <summary>Details</summary>
Motivation: OTOC是研究量子信息扰动的有力工具，但实验测量非常困难，因为需要时间反演演化。本文旨在在量子计算机上实验评估OTOC，验证不同测量协议的实用性。

Method: 使用三种不同协议测量OTOC：时间倒转法（RTM）、弱测量法（WMM）和不可逆性敏感度法（ISM）。实验在XXZ自旋-1/2链的热Gibbs态上进行，利用囚禁离子量子计算机的数值模拟器reimei进行实验。

Result: 首次实验演示了ISM方法，并进行了三种方法的详细比较分析，揭示了测量OTOC时方法依赖的行为差异。

Conclusion: 这项工作不仅验证了这些协议作为在近期硬件上探索量子混沌的实用工具，还提供了关于各自优势和局限性的关键见解，为未来的实验研究提供了实用框架。

Abstract: The out-of-time-ordered correlator (OTOC) is a powerful tool for probing quantum information scrambling, a fundamental process by which local information spreads irreversibly throughout a quantum many-body system. Experimentally measuring the OTOC, however, is notoriously challenging due to the need for time-reversed evolution. Here, we present an experimental evaluation of the OTOC on a quantum computer, using three distinct protocols to address this challenge: the rewinding time method (RTM), the weak-measurement method (WMM), and the irreversibility-susceptibility method (ISM). Our experiments investigate the quantum dynamics of an XXZ spin-1/2 chain prepared in a thermal Gibbs state. As a key contribution, we provide the first experimental demonstration of the ISM, using the numerical emulator of the trapped-ion quantum computer, reimei. We also conduct a detailed comparative analysis of all three methods, revealing method-dependent behaviors in the measured OTOC. This work not only validates these protocols as practical tools for exploring quantum chaos on near-term hardware but also offers crucial insights into their respective advantages and limitations, providing a practical framework for future experimental investigations.

</details>


### [225] [Variational quantum algorithm for solving Helmholtz problems with high order finite elements](https://arxiv.org/abs/2512.22665)
*Arnaud Rémi,François Damanet,Christophe Geuzaine*

Main category: quant-ph

TL;DR: 该论文研究了如何用量子算法求解亥姆霍兹方程有限元离散化产生的线性系统，设计了算子A和A†A的块编码量子电路，并在一维问题上进行了测试。


<details>
  <summary>Details</summary>
Motivation: 亥姆霍兹方程有限元离散化产生的线性系统求解在经典计算中仍然是一个重大挑战，论文旨在探索变分量子算法如何解决这一挑战。

Method: 首先为规则网格设计了有限元离散化算子A和A†A的块编码量子电路，电路深度为O(p³polylog(Np))，其中N是单元数，p是有限元阶数。然后将该算法应用于具有狄利克雷和诺伊曼边界条件的一维亥姆霍兹问题。

Result: 成功设计了适用于亥姆霍兹问题有限元离散化的量子块编码电路，并在不同波数的一维问题上验证了算法的可行性。

Conclusion: 变分量子算法有望解决亥姆霍兹方程有限元离散化产生的计算挑战，为量子计算在偏微分方程求解中的应用提供了新途径。

Abstract: Discretizing Helmholtz problems via finite elements yields linear systems whose efficient solution remains a major challenge for classical computation. In this paper, we investigate how variational quantum algorithms could address this challenge. We first show that, for regular meshes, a block encoding of the operators $A$ and $A^\dagger A$ arising from the high-order finite element discretisation of Helmholtz problems can be designed, resulting in a quantum circuit of depth $\mathcal{O}(p^3\mathrm{poly}\log(Np))$ with $N$ the number of elements and $p$ the order of the finite elements. Then we apply our algorithm to a one-dimensional Helmholtz problem with Dirichlet and Neumann boundary conditions for various wavenumbers.

</details>


### [226] [A method for robust spin relaxometry in the presence of imperfect state preparation](https://arxiv.org/abs/2512.22739)
*Ella P. Walsh,Sepehr Ahmadi,Alexander J. Healey,David A. Simpson,Liam T. Hall*

Main category: quant-ph

TL;DR: 提出一种改进的拟合方法，用于处理氮空位中心自旋弛豫测量中因自旋极化不完美导致的系统误差，提高参数估计的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 基于量子自旋系统的自旋弛豫测量在医学和凝聚态系统中是重要工具，但现有氮空位中心测量方法常因自旋极化不完美而产生伪影和系统不确定性，导致T1弛豫时间测量偏快，当前分析方法无法充分解决这些问题。

Method: 引入一种最小化拟合程序，能够在自旋极化不完美的情况下实现更鲁棒的参数估计。该模型改进了现有方法，提供更准确的拟合，并为高效并行化单自旋动力学研究提供框架。

Result: 该方法提高了参数估计的准确性，减少了因自旋极化不完美导致的系统误差，为自旋弛豫测量提供了更可靠的工具。

Conclusion: 提出的最小化拟合方法能够有效解决氮空位中心自旋弛豫测量中的系统误差问题，提高测量精度，为量子传感应用提供更可靠的分析工具。

Abstract: Spin relaxometry based on quantum spin systems has developed as a valuable tool in medical and condensed matter systems, offering the advantage of operating without the need for external DC or RF fields. Spin relaxometry with nitrogen-vacancy (NV) centers has been applied to paramagnetic sensing using both single crystal diamond and nanodiamond materials. However, these methods often suffer from artifacts and systematic uncertainties, particularly due to imperfect spin state preparation, leading to artificially fast T$_1$ relaxation times. Current analysis techniques fail to adequately account for these issues, limiting the precision of parameter estimation. In this work, we introduce a minimal fitting procedure that enables more robust parameter estimation in the presence of imperfect spin polarization. Our model improves upon existing approaches by offering more accurate fits and provides a framework for efficiently parallelizing single-spin dynamics studies.

</details>


### [227] [An asymmetric and fast Rydberg gate protocol for long range entanglement](https://arxiv.org/abs/2512.22767)
*Daniel C. Cole,Vikas Buchemmavari,Mark Saffman*

Main category: quant-ph

TL;DR: 提出一种改进的Rydberg量子门设计，基于π-2π-π协议但添加额外失谐，可在无需强Rydberg相互作用下实现高保真度操作


<details>
  <summary>Details</summary>
Motivation: 传统Rydberg门需要强相互作用才能实现高保真度，这限制了实际应用。本文旨在开发一种不需要强Rydberg相互作用就能实现高保真度操作的量子门协议

Method: 改进原始π-2π-π协议，在目标量子位的2π脉冲上添加额外失谐。使用最优相位波形设计，将门协议推广到任意受控相位。采用鲁棒控制方法设计对Rabi频率和相互作用强度变化具有鲁棒性的门

Result: 新协议在控制位和目标位Rabi频率相等时达到Rydberg寿命设定基本保真度极限的2.39倍以内，在不对称Rabi频率时达到1.68倍以内。发现恒定相位协议在固定激光Rabi频率和可调相互作用强度下是时间最优的

Conclusion: 成功开发了一种改进的Rydberg量子门设计，可在无需强相互作用下实现接近理论极限的高保真度操作，并具有鲁棒性和时间最优性

Abstract: We analyze a new Rydberg gate design based on the original $π-2π-π$ protocol [Jaksch, et. al. Phys. Rev. Lett. {\bf 85}, 2208 (2000)] that is modified to enable high fidelity operation without requiring a strong Rydberg interaction. The gate retains the $π-2π-π$ structure with an additional detuning added to the $2π$ pulse on the target qubit. The protocol reaches within a factor of 2.39 (1.68) of the fundamental fidelity limit set by Rydberg lifetime for equal (asymmetric) Rabi frequencies on the control and target qubits. We generalize the gate protocol to arbitrary controlled phases. We design optimal target-qubit phase waveforms to generalize the gate across a range of interaction strengths and we find that, within this family of gates, the constant-phase protocol is time-optimal for a fixed laser Rabi frequency and tunable interaction strength. Robust control methods are used to design gates that are robust against variations in Rydberg Rabi frequency or interaction strength.

</details>


### [228] [Simulating Fully Gauge-Fixed SU(2) Hamiltonian Dynamics on Digital Quantum Computers](https://arxiv.org/abs/2512.22782)
*Henry Froland,Dorota M. Grabowska,Zhiyao Li*

Main category: quant-ph

TL;DR: 该论文开发了SU(2)规范理论的量子模拟方法，使用混合基表示，在IBM量子处理器上实现了两格点系统的实时可观测量测量，达到百分之一精度。


<details>
  <summary>Details</summary>
Motivation: 需要为量子模拟开发系统可改进的格点规范理论哈密顿量表示，特别是在所有规范耦合值下都能高效工作。混合基表示是SU(2)规范理论的一个有前景的候选方案。

Method: 1) 开发连续规范场自由度到量子比特表示的映射；2) 为混合基中的时间演化设计两种算法；3) 在IBM Heron超导量子处理器上实现两格点系统的量子模拟。

Result: 1) 每个格点仅需3个量子比特即可达到千分之一精度；2) 两种时间演化算法各有优势：一种在大量子比特数时电路深度缩放良好，另一种在量子比特有限时更实用；3) 在IBM量子处理器上的测量结果与经典预测在百分之一水平匹配。

Conclusion: 这项工作为更大系统的二维和三维模拟铺平了道路，并证明了混合基公式在研究SU(2)规范理论在所有规范耦合值下性质的可行性。

Abstract: Quantum simulations of many-body systems offer novel methods for probing the dynamics of the Standard Model and its constituent gauge theories. Extracting low-energy predictions from such simulations rely on formulating systematically-improvable representations of lattice gauge theory Hamiltonians that are efficient at all values of the gauge coupling. One such candidate representation for SU(2) is the fully gauge-fixed Hamiltonian defined in the mixed basis. This work focuses on the quantum simulation of the smallest non-trivial system: two plaquettes with open boundary conditions. A mapping of the continuous gauge field degrees of freedom to qubit-based representations is developed. It is found that as few as three qubits per plaquette is sufficient to reach per-mille level precision on predictions for observables. Two distinct algorithms for implementing time evolution in the mixed basis are developed and analyzed in terms of quantum resource estimates. One algorithm has favorable scaling in circuit depth for large numbers of qubits, while the other is more practical when qubit count is limited. The latter algorithm is used in the measurement of a real-time observable on IBM's Heron superconducting quantum processor, ibm_fez. The quantum results match classical predictions at the percent-level. This work lays out a path forward for two- and three-dimensional simulations of larger systems, as well as demonstrating the viability of mixed-basis formulations for studying the properties of SU(2) gauge theories at all values of the gauge coupling.

</details>


### [229] [Benchmarking Lie-Algebraic Pretraining and Non-Variational QWOA for the MaxCut Problem](https://arxiv.org/abs/2512.22856)
*Matthaus Zering,Jolyon Joyce,Tal Gurfinkel,Jingbo Wang*

Main category: quant-ph

TL;DR: 比较两种改进QAOA训练性的策略：Lie代数预训练框架和非变分QWOA(NV-QWOA)，在最大割问题上，NV-QWOA仅用60次迭代达到98.9%近似比，表现更优。


<details>
  <summary>Details</summary>
Motivation: 标准QAOA中随机初始化参数通常导致梯度消失，使得变分优化失效，需要改进训练策略来提升量子近似优化算法的性能。

Method: 比较两种策略：1) Lie代数预训练框架，使用Lie代数经典模拟寻找接近最优的初始化；2) 非变分QWOA(NV-QWOA)，针对仅由3个超参数覆盖的限制参数子空间。

Result: 在16个顶点的200个Erdős-Rényi图和200个3-正则图上测试，两种方法都显著优于标准随机初始化QAOA。NV-QWOA仅用60次迭代达到平均98.9%近似比，Lie代数预训练QAOA在500次迭代后达到77.71%。

Conclusion: NV-QWOA的少量可调参数能可靠找到接近最优解，其结构化参数化比在低维辅助问题上预训练更鲁棒。未来需验证扩展到更大问题规模和其他问题类。

Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is a leading candidate for achieving quantum advantage in combinatorial optimization on Near-Term Intermediate-Scale Quantum (NISQ) devices. However, random initialization of the variational parameters typically leads to vanishing gradients, rendering standard variational optimization ineffective. This paper provides a comparative performance analysis of two distinct strategies designed to improve trainability: Lie algebraic pretraining framework that uses Lie-algebraic classical simulation to find near-optimal initializations, and non-variational QWOA (NV-QWOA) that targets a restrict parameter subspace covered by 3 hyperparameters. We benchmark both methods on the unweighted Maxcut problem using a circuit depth of $p = 256$ across 200 Erdős-Rényi and 200 3-regular graphs, each with 16 vertices. Both approaches significantly improve upon the standard randomly initialized QWOA. NV-QWOA attains a mean approximation ratio of 98.9\% in just 60 iterations, while the Lie-algebraic pretrained QWOA improves to 77.71\% after 500 iterations. That optimization proceeds more quickly for NV-QWOA is not surprising given its significantly smaller parameter space, however, that an algorithm with so few tunable parameters reliably finds near-optimal solutions is remarkable. These findings suggest that the structured parameterization of NV-QWOA offers a more robust training approach than pretraining on lower-dimensional auxiliary problems. Future work is needed to confirm scaling to larger problem sizes and to asses generalization to other problem classes.

</details>


### [230] [A Counterexample to the Optimality Conjecture in Convex Quantum Channel Optimization](https://arxiv.org/abs/2512.22863)
*Jianting Yang*

Main category: quant-ph

TL;DR: 本文通过构造一个二维希尔伯特空间中的反例，推翻了Coutts等人提出的凸量子信道优化中的最优性猜想


<details>
  <summary>Details</summary>
Motivation: Coutts等人提出了一个关于凸量子信道优化中核范数最小化问题的最优性猜想，该猜想认为最优解的对偶证书可以通过Choi矩阵的谱计算唯一确定。本文旨在验证这一猜想的正确性。

Method: 通过在二维希尔伯特空间中构造一个具体的反例，展示该猜想不成立的情况。通过分析这个反例，证明对偶证书不能通过Choi矩阵的谱计算唯一确定。

Result: 成功构造了一个反例，证明了Coutts等人的最优性猜想是错误的。在二维希尔伯特空间中存在这样的情况：即使满足猜想的前提条件，对偶证书也不能通过Choi矩阵的谱计算唯一确定。

Conclusion: Coutts等人关于凸量子信道优化中核范数最小化问题的最优性猜想不成立。对偶证书的确定比猜想中提出的方法更为复杂，不能简单地通过Choi矩阵的谱计算唯一确定。

Abstract: This paper presents a counterexample to the optimality conjecture in convex quantum channel optimization proposed by Coutts et al. The conjecture posits that for nuclear norm minimization problems in quantum channel optimization, the dual certificate of an optimal solution can be uniquely determined via the spectral calculus of the Choi matrix. By constructing a counterexample in 2-dimensional Hilbert spaces, we disprove this conjecture.

</details>


### [231] [The operational no-signalling constraints and their implications](https://arxiv.org/abs/2512.23702)
*Michał Eckstein,Tomasz Miller,Ryszard Horodecki,Ravishankar Ramanathan,Paweł Horodecki*

Main category: quant-ph

TL;DR: 该论文建立了研究相对论时空中非局域和时间相关性的统一框架，探讨了操作无信号约束的违反后果，包括驳斥了在闵可夫斯基时空中可检测因果循环的说法，以及证明了黑洞时空中事件视界附近非局域相关性可以被阻塞而不违反无信号约束。


<details>
  <summary>Details</summary>
Motivation: 研究相对论时空中量子关联的性质，以及相对论因果性对利用这些关联进行信息处理的影响。近年来该领域受到广泛关注，需要建立统一框架来分析一般相对论时空中的非局域和时间相关性。

Method: 建立操作无信号约束的统一框架，用于研究一般相对论时空中的非局域和时间相关性。通过分析闵可夫斯基时空和黑洞时空中的具体案例，检验该框架的应用和推论。

Result: 1. 在闵可夫斯基时空中，违反操作无信号约束会导致逻辑悖论或操作上违反庞加莱对称性，从而驳斥了先前关于在闵可夫斯基时空中可操作检测因果循环的说法。
2. 证明了阻塞非局域相关性的物理机制不一定导致超光速信号传递，反驳了先前相关研究结论。
3. 在黑洞时空中，某些跨越事件视界的非局域相关性可以被任何参与者阻塞，同时不违反操作无信号约束。

Conclusion: 该研究建立了分析相对论时空中量子相关性的统一框架，澄清了关于因果循环检测和非局域相关性阻塞的重要误解，为理解相对论量子信息处理中的基本限制提供了新视角。

Abstract: The study of quantum correlations within relativistic spacetimes, and the consequences of relativistic causality on information processing using such correlations, has gained much attention in recent years. In this paper, we establish a unified framework in the form of operational no-signalling constraints to study both nonlocal and temporal correlations within general relativistic spacetimes. We explore several intriguing consequences arising from our framework. Firstly, we show that the violation of the operational no-signalling constraints in Minkowski spacetime implies either a logical paradox or an operational infringement of Poincaré symmetry. We thereby examine and subvert recent claims in [Phys. Rev. Lett. 129, 110401 (2022)] on the possibility of witnessing operationally detectable causal loops in Minkowski spacetime. Secondly, we explore the possibility of jamming of nonlocal correlations, controverting a recent claim in [Nat. Comm. 16, 269 (2025)] that a physical mechanism for jamming would necessarily lead to superluminal signalling. Finally, we show that in black hole spacetimes certain nonlocal correlations under and across the event horizon can be jammed by any agent without spoiling the operational no-signalling constraints.

</details>


### [232] [Any DOF All at Once: Single Photon State Tomography in a Single Measurement Setup](https://arxiv.org/abs/2512.22869)
*Roey Shafran,Ron Ziv,Mordechai Segev*

Main category: quant-ph

TL;DR: 提出利用单次强度测量重构多自由度超纠缠单光子密度矩阵的框架，通过空间自由度编码其他自由度信息，简化实验装置并减少测量时间


<details>
  <summary>Details</summary>
Motivation: 高维和超纠缠态在提高信道容量和量子操作复杂度方面具有潜力，但传统量子态层析需要大量测量和实验装置调整，测量效率低

Method: 利用光子的空间自由度编码其他自由度信息，通过理想耦合器和多模光纤进行空间信息混合和编码，使用传统相机单次强度测量重构密度矩阵

Result: 数值模拟验证了方法对OAM-自旋和OAM-频率纠缠单光子态的有效性，简化了实验装置，减少了采集时间，并能恢复传统相机无法检测的自由度

Conclusion: 该方法为高效测量高维超纠缠态提供了新框架，消除了投影测量的需求，有望推动光子量子技术的发展

Abstract: Photonic quantum technologies utilize various degrees of freedom (DOFs) of light, such as polarization, frequency, and spatial modes, to encode quantum information. In the effort of further improving channel capacity and increasing the complexity of available quantum operations, high-dimensional and hyperentangled states are now gaining interest. Efficiently measuring these high dimensional states is challenging due to the large number of measurements required for reconstructing the full density matrix via quantum state tomography (QST), and the fact that each measurement requires some modification in the experimental setup. Here, we propose a framework for reconstructing the density matrix of a single-photon hyperentangled across multiple DOFs using a single intensity-measurement obtainable from traditional cameras, and discuss extensions for multiphoton hyperentangled states. Our method hinges on the spatial DOF of the photon and uses it to encode information from other DOFs. We numerically demonstrate this method for single-photon OAM-spin and OAM-frequency entangled states using an ideal coupler and a multimode fiber, to perform the spatial information mixing and encoding. This technique simplifies the experimental setup and reduces acquisition time compared to traditional QST based methods. Moreover, it allows recovery of DOFs that conventional cameras cannot detect, such as polarization, thus eliminating the need for projection measurements.

</details>


### [233] [Agency under indefinite causality: operational eternalism in higher-order quantum theory](https://arxiv.org/abs/2512.22879)
*Alexei Grinbaum*

Main category: quant-ph

TL;DR: 量子理论中因果不确定性与时空动力学间的根本矛盾无法调和，需采用操作永恒主义视角，将观察者重新定义为避免非因果性的工具。


<details>
  <summary>Details</summary>
Motivation: 研究二十年来因果不确定性问题的哲学启示：如果认为操作量子理论和动态时空物理学都是基本理论，它们之间的张力是不可调和的。这种张力需要通过新的哲学框架来理解。

Method: 提出操作永恒主义，类似于块宇宙观但应用于信息而非几何。将输入和输出视为基本给定，而主体是从特定数据分组中产生的次要构造。主体性是视角性的。

Result: 重新定义了操作方法中的观察者概念，将其视为避免非因果性的工具。为维格纳朋友问题中的因果相容主体类别提供了判断标准。

Conclusion: 操作量子理论与动态时空物理学之间的根本矛盾无法调和，需要采用操作永恒主义视角，将观察者重新理解为避免非因果性的工具，这为理解量子理论中的主体性问题提供了新框架。

Abstract: After two decades of research on indefinite causality, a philosophical lesson emerges: the tension between operational quantum theory and dynamical spacetime physics is unbridgeable if one believes both types of theories to be fundamental. We interpret this tension through operational eternalism, a stance analogous to the block-universe view but applied to information rather than geometry. Inputs and outputs are primary givens, while agents are secondary constructs arising from specific groupings of data. Agency is perspectival: from Alice's perspective Bob may not qualify as an observer, and vice versa. These results redefine the observer in the operational approach as a tool to avoid non-causality. They also provide a criterion for Wigner's friends as a class of causally compatible agents.

</details>


### [234] [Optimal Threshold for Fracton Codes and Nearly Saturated Code Capacity in Three Dimensions](https://arxiv.org/abs/2512.22888)
*Giovanni Canossa,Lode Pollet,Miguel A. Martin-Delgado,Hao Song,Ke Liu*

Main category: quant-ph

TL;DR: 该论文通过统计力学映射和蒙特卡洛模拟，发现自对偶分形码（特别是棋盘码）在随机泡利噪声下的最优阈值达到约0.108，是已知三维码中最高的，接近拓扑码的理论极限。


<details>
  <summary>Details</summary>
Motivation: 分形码作为新型拓扑物态已被广泛研究，但其容错特性尚未充分探索。需要研究自对偶分形码（特别是棋盘码）在随机泡利噪声下的最优阈值，以评估其作为量子存储器的潜力。

Method: 采用统计力学映射方法，结合大规模并行回火蒙特卡洛模拟，计算棋盘码的代码容量阈值。通过验证广义熵关系来推断其他分形码的性能。

Result: 棋盘码的最优阈值p_th ≈ 0.108(2)，这是已知三维码中最高的，接近拓扑码的理论极限。验证了广义熵关系H(p_th) + H(˜p_th) ≈ 1，并推断Haah码的阈值也接近理论极限p_th ≈ 0.11。

Conclusion: 分形码具有极高的容错能力，是优秀的量子存储器候选。对偶技术为分析复杂量子纠错码提供了有效工具，广义熵关系在标准拓扑码之外也适用。

Abstract: Fracton codes have been intensively studied as novel topological states of matter, yet their fault-tolerant properties remain largely unexplored. Here, we investigate the optimal thresholds of self-dual fracton codes, in particular the checkerboard code, against stochastic Pauli noise. By utilizing a statistical-mechanical mapping combined with large-scale parallel tempering Monte Carlo simulations, we calculate the optimal code capacity of the checkerboard code to be $p_{th} \simeq 0.108(2)$. This value is the highest among known three-dimensional codes and nearly saturates the theoretical limit for topological codes. Our results further validate the generalized entropy relation for two mutually dual models, $H(p_{th}) + H(\tilde{p}_{th}) \approx 1$, and extend its applicability beyond standard topological codes. This verification indicates the Haah's code also possesses a code capacity near the theoretical limit $p_{th} \approx 0.11$. These findings highlight fracton codes as highly resilient quantum memory and demonstrate the utility of duality techniques in analyzing intricate quantum error-correcting codes.

</details>


### [235] [Quantum batteries with K-regular graph generators: A no-go for quantum advantage](https://arxiv.org/abs/2512.22908)
*Debkanta Ghosh,Tanoy Kanti Konar,Amit Kumar Pal,Aditi Sen De*

Main category: quant-ph

TL;DR: 基于K-正则图的量子电池设计研究，证明0-正则图电池在K-正则图充电下可实现与系统尺寸线性相关的可提取功，且功输出随K值增加而系统改善。


<details>
  <summary>Details</summary>
Motivation: 正则图在量子通信和量子计算中有广泛应用，因此研究基于K-正则图的量子电池设计具有重要意义。

Method: 研究基于K-正则图的量子电池设计，其中K表示每个顶点的边数。分析0-正则图电池在K-正则图充电下的性能，并扩展到具有幂律衰减相互作用的集体K-正则充电器。

Result: 0-正则图电池在K-正则图充电下可实现与系统尺寸线性相关的可提取功，且线性缩放特性在具有幂律衰减相互作用的集体K-正则充电器中仍保持。功输出随K值增加而系统改善，但未观察到超线性缩放。当仅子系统可访问时，若电池处于下极化乘积态，可提取功的比例与系统尺寸无关。

Conclusion: 基于K-正则图的量子电池设计可实现线性缩放的可提取功，功输出随正则度K增加而改善。在子系统可访问情况下，特定初始态下可提取功比例与系统尺寸无关，这为量子电池设计提供了理论指导。

Abstract: Regular graphs find broad applications ranging from quantum communication to quantum computation. Motivated by this, we investigate the design of a quantum battery based on a K-regular graph, where K denotes the number of edges incident on each vertex. We prove that a 0-regular graph battery exhibits extractable work that scales linearly with the system-size when charged using a K-regular graph. This linear scaling is shown to persist even when the charging is implemented via a collective K-regular charger with power-law decaying interactions. While no superlinear scaling is observed, the work output is found to improve systematically with increasing regularity K. Furthermore, by introducing the notion of the fraction of extractable work when only subsystems are accessible, we identify this fraction to be independent of system-size if the battery is prepared in the down-polarized product state. This independence breaks down when the battery is oriented along the x- and y-directions of the Bloch sphere.

</details>


### [236] [Controlling Nonadiabatic Transitions Through Engineered Ultrafast Laser Fields at Conical Intersections](https://arxiv.org/abs/2512.22912)
*Xuanchao Zhang,Yang-Cheng Ye,Panpan Zhang,Xiangmei Duan,R. J. Dwayne Miller,Fulu Zheng,Ajay Jha,Hong-Guang Duan*

Main category: quant-ph

TL;DR: 利用工程化超快激光脉冲控制锥形交叉附近的非绝热动力学，通过调节脉冲啁啾和时域波形来调控波包演化和量子产率


<details>
  <summary>Details</summary>
Motivation: 研究如何在锥形交叉（CI）这一重要非绝热耦合区域实现相干控制，理解脉冲整形对非绝热动力学的影响机制

Method: 在模型振动电子系统中，通过定制脉冲啁啾和时域波形，计算波包布居和相干动力学，使用反应坐标投影解析波包在简并区域的详细演化

Result: 啁啾和脉冲持续时间都能调制振动相干性并改变竞争路径之间的分支比，从而实现量子产率的可控变化

Conclusion: 阐明了锥形交叉附近脉冲整形控制的动力学机制，建立了操纵超快非绝热过程的通用框架

Abstract: In this paper, we investigate coherent control of nonadiabatic dynamics at a conical intersection (CI) using engineered ultrafast laser pulses. Within a model vibronic system, we tailor pulse chirp and temporal profile and compute the resulting wave-packet population and coherence dynamics using projections along the reaction coordinate. This approach allows us to resolve the detailed evolution of wave-packets as they traverse the degeneracy region with strong nonadiabatic coupling. By systematically varying pulse parameters, we demonstrate that both chirp and pulse duration modulate vibrational coherence and alter branching between competing pathways, leading to controlled changes in quantum yield. Our results elucidate the dynamical mechanisms underlying pulse-shaped control near conical intersections and establish a general framework for manipulating ultrafast nonadiabatic processes.

</details>


### [237] [Gauge Symmetry in Quantum Simulation](https://arxiv.org/abs/2512.22932)
*Masanori Hanada,Shunji Matsuura,Andreas Schafer,Jinzhao Sun*

Main category: quant-ph

TL;DR: 该论文提出了处理非阿贝尔规范理论量子模拟中规范冗余的通用原则，展示了规范单态和非单态两种表示方法的有效性，并基于轨道格点构建了完整的量子模拟框架，提供了具体的电路构造和资源估计。


<details>
  <summary>Details</summary>
Motivation: 非阿贝尔规范理论的量子模拟需要处理规范冗余问题，现有方法通常局限于规范单态表示，缺乏通用原则来指导不同模拟方法的选择。本文旨在提供处理规范对称性的通用框架，澄清物理状态不必仅限于规范单态表示。

Method: 提出处理规范对称性的通用原则，基于轨道格点构建完整的量子模拟框架。对于单态方法，引入通过线性组合酉算符实现的Haar平均投影；对于非单态方法，展示如何通过波包和弦激发获得规范不变可观测量。在时间规范下，将格点杨-米尔斯动力学映射到适合Trotter化的Pauli弦哈密顿量。

Result: 证明了非单态方法既通用又高效，提供了SU(N)规范理论的具体资源估计和可扩展电路配方。通过小系统的经典模拟验证了收敛准则，量化了截断误差和Trotter误差。非单态方法被证明能够通过波包和弦激发获得规范不变可观测量。

Conclusion: 该框架为模拟非阿贝尔规范理论提供了概念清晰性和实用工具，展示了规范单态和非单态两种表示方法的有效性及其实际权衡，为实现非阿贝尔规范理论量子模拟的优势迈出了重要一步。

Abstract: Quantum simulation of non-Abelian gauge theories requires careful handling of gauge redundancy. We address this challenge by presenting universal principles for treating gauge symmetry that apply to any quantum simulation approach, clarifying that physical states need not be represented solely by gauge singlets. Both singlet and non-singlet representations are valid, with distinct practical trade-offs, which we elucidate using analogies to BRST quantization. We demonstrate these principles within a complete quantum simulation framework based on the orbifold lattice, which enables explicit and efficient circuit constructions relevant to real-world QCD. For singlet-based approaches, we introduce a Haar-averaging projection implemented via linear combinations of unitaries, and analyze its cost and truncation errors. Beyond the singlet-approach, we show how non-singlet approaches can yield gauge-invariant observables through wave packets and string excitations. This non-singlet approach is proven to be both universal and efficient. Working in temporal gauge, we provide explicit mappings of lattice Yang-Mills dynamics to Pauli-string Hamiltonians suitable for Trotterization. Classical simulations of small systems validate convergence criteria and quantify truncation and Trotter errors, showing concrete resource estimates and scalable circuit recipes for SU($N$) gauge theories. Our framework provides both conceptual clarity and practical tools toward quantum advantage in simulating non-Abelian gauge theories.

</details>


### [238] [Multiverse: A Simulator for Evaluating Entanglement Routing in Quantum Networks](https://arxiv.org/abs/2512.22937)
*Amar Abane,Junxiao Shi,Van Sy Mai,Abderrahim Amlou,Abdella Battou*

Main category: quant-ph

TL;DR: MQNS是一个用于快速评估动态异构配置下纠缠路由的离散事件模拟器


<details>
  <summary>Details</summary>
Motivation: 需要一种能够快速评估量子网络纠缠路由性能的工具，支持动态异构配置，并实现公平、可重复的跨范式比较

Method: 采用离散事件模拟方法，支持运行时可配置的纯化、交换、内存管理和路由功能，基于统一的量子比特生命周期和集成链路架构模型，采用模块化、最小化设计保持架构无关性

Result: 开发了MQNS模拟器，能够支持量子网络纠缠路由的快速评估，为不同范式提供公平比较平台

Conclusion: MQNS为量子网络纠缠路由研究提供了一个灵活、可扩展的模拟平台，支持未来仿真工作并促进跨范式比较

Abstract: We present MQNS, a discrete-event simulator for rapid evaluation of entanglement routing under dynamic, heterogeneous configurations. MQNS supports runtime-configurable purification, swapping, memory management, and routing, within a unified qubit lifecycle and integrated link-architecture models. A modular, minimal design keeps MQNS architecture-agnostic, enabling fair, reproducible comparisons across paradigms and facilitating future emulation.

</details>


### [239] [Random matrix prediction of average entanglement entropy in non-Abelian symmetry sectors](https://arxiv.org/abs/2512.22942)
*Anwesha Chakraborty,Lucas Hackl,Mario Kieburg*

Main category: quant-ph

TL;DR: 研究具有全局SU(2)对称性的量子多体系统中，固定总自旋J和磁化J_z=0条件下，Haar随机纯态的平均二分纠缠熵，发现除了预期的体积律主项外，还存在1/2 log V的有限尺寸修正。


<details>
  <summary>Details</summary>
Motivation: 将Page关于随机态纠缠熵的结果扩展到非阿贝尔对称性领域，研究SU(2)对称性如何影响量子多体系统中的平均纠缠特性。

Method: 使用随机矩阵系综特征，分析自旋-1/2晶格系统，推导子系统分数f<1/2时的渐近表达式，通过Clebsch-Gordon系数的标度行为计算有限尺寸修正。

Result: 证明了平均纠缠熵存在1/2 log V的有限尺寸修正，并显式计算了反映磁化块内角动量耦合的O(1)项贡献，得到了任意自旋密度下的完全解析处理。

Conclusion: SU(2)对称性显著影响量子多体系统的平均纠缠特性，非阿贝尔对称性导致独特的有限尺寸修正，扩展了Page类型结果到对称约束系统。

Abstract: We study the average bipartite entanglement entropy of Haar-random pure states in quantum many-body systems with global $\mathrm{SU}(2)$ symmetry, constrained to fixed total spin $J$ and magnetization $J_z = 0$. Focusing on spin-$\tfrac12$ lattices and subsystem fractions $f < \frac{1}{2}$, we derive a asymptotic expression for the average entanglement entropy up to constant order in the system volume $V$. In addition to the expected leading volume law term, we prove the existence of a $\frac{1}{2}\log V$ finite-size correction resulting from the scaling of the Clebsch-Gordon coefficients and compute explicitly the $O(1)$ contribution reflecting angular-momentum coupling within magnetization blocks. Our analysis uses features of random matrix ensembles and provides a fully analytical treatment for arbitrary spin densities, thereby extending Page type results to non-Abelian sectors and clarifying how $\mathrm{SU}(2)$ symmetry shapes average entanglement.

</details>


### [240] [Revisiting finite Abelian hidden subgroup problem and its distributed exact quantum algorithm](https://arxiv.org/abs/2512.22959)
*Ziyuan Dong,Xiang Fan,Tengxun Zhong,Daowen Qiu*

Main category: quant-ph

TL;DR: 本文重新审视有限阿贝尔隐藏子群问题，提出了更简洁的精确量子算法、无需量子通信的分布式量子算法，以及查询复杂度降低的并行经典算法。


<details>
  <summary>Details</summary>
Motivation: 重新审视有限阿贝尔隐藏子群问题，旨在改进现有算法的简洁性、资源消耗和查询复杂度，并探索分布式和并行计算的优势。

Method: 1. 使用振幅放大技术设计更简洁的精确量子算法；2. 利用中国剩余定理构建分布式量子算法，减少量子比特需求且无需量子通信；3. 开发并行经典算法降低查询复杂度。

Result: 1. 提出了适用于任意有限阿贝尔群的更简洁精确量子算法；2. 分布式量子算法减少了量子比特需求、降低了量子查询复杂度且无需量子通信；3. 并行经典算法显著降低了查询复杂度，在温和条件下总查询数不超过原始集中式算法。

Conclusion: 本文在有限阿贝尔隐藏子群问题上取得了多方面进展：改进了量子算法的简洁性和资源效率，提出了创新的分布式和并行计算方法，为相关问题的求解提供了新的技术路径。

Abstract: We revisit the finite Abelian hidden subgroup problem (AHSP) from a mathematical perspective and make the following contributions. First, by employing amplitude amplification, we present an exact quantum algorithm for the finite AHSP, our algorithm is more concise than the previous exact algorithm and applies to any finite Abelian group. Second, utilizing the Chinese Remainder Theorem, we propose a distributed exact quantum algorithm for finite AHSP, which requires fewer qudits, lower quantum query complexity, and no quantum communication. We further show that our distributed approach can be extended to certain classes of non-Abelian groups. Finally, we develop a parallel exact classical algorithm for finite AHSP with reduced query complexity; even without parallel execution, the total number of queries across all nodes does not exceed that of the original centralized algorithm under mild conditions.

</details>


### [241] [Comment on "There is No Quantum World" by Jeffrey Bub](https://arxiv.org/abs/2512.22965)
*Philippe Grangier*

Main category: quant-ph

TL;DR: 作者反驳Bub对无限张量积方法的批评，认为数学无限性在物理理论中可接受，且批评者误解了相关论文的核心观点——经典与量子物理从一开始就需要彼此。


<details>
  <summary>Details</summary>
Motivation: 回应Jeffrey Bub对neo-Bohrian量子力学解释和von Neumann无限张量积工作的批评。Bub认为无限极限在真实有限系统中"永远无法达到"，作者要为此观点提供相反论证。

Method: 从两个主要论点进行反驳：1) 数学无限性在物理理论中如果恰当处理是可以接受的；2) 批评者误解了相关论文，这些论文并非探讨"从经典到量子力学的转变意义"，而是从经典与量子物理从一开始就需要彼此的物理本体论出发。

Result: 论证了neo-Bohrian立场的合理性：微观物理对象（或自由度）总是作为量子系统出现，但处于经典语境中。这种观点为测量问题提供了理论框架，并证明了Bohr坚持经典概念优先性的合理性。

Conclusion: 无限张量积方法为测量问题提供了有效的理论框架，数学无限性在物理理论中是可接受的，neo-Bohrian立场（量子系统在经典语境中出现）具有物理意义。

Abstract: In a recent preprint [1] Jeffrey Bub presents a discussion of neo-Bohrian interpretations of quantum mechanics, and also of von Neumann's work on infinite tensor products [2]. He rightfully writes that this work provides a theoretical framework that deflates the measurement problem and justifies Bohr's insistence on the primacy of classical concepts. But then he rejects these ideas, on the basis that the infinity limit is "never reached for any real system composed of a finite number of elementary systems". In this note we present opposite views on two major points: first, admitting mathematical infinities in a physical theory is not a problem, if properly done; second, the critics of [3,4,5] comes with a major misunderstanding of these papers: they don't ask about "the significance of the transition from classical to quantum mechanics", but they start from a physical ontology where classical and quantum physics need each other from the beginning. This is because they postulate that a microscopic physical object (or degree of freedom) always appears as a quantum system, within a classical context. Here we argue why this (neo-Bohrian) position makes sense.

</details>


### [242] [Fast chiral resolution with optimal control](https://arxiv.org/abs/2512.22998)
*Dionisis Stefanatos,Ioannis Thanopulos,Emmanuel Paspalakis*

Main category: quant-ph

TL;DR: 该研究将最小时间完美手性分离问题建模为两个非相互作用自旋-1/2系统的最优控制问题，通过控制理论分析发现最优控制场只能取边界值或零值，并识别出特定条件下的三阶段对称最优脉冲序列。


<details>
  <summary>Details</summary>
Motivation: 手性分离在自然科学各领域都是关键任务，需要开发高效的控制协议来实现完美手性分辨。本研究旨在通过最优控制理论解决最小时间完美手性分离问题，提高分离效率。

Method: 将问题建模为两个非相互作用自旋-1/2系统的最优控制问题，假设两个拉曼场（泵浦和斯托克斯）具有相同的控制界限，而连接两个低能态的直接场具有不同的界限。使用控制理论分析最优控制场的特性，结合数值最优控制和直观论证，识别最优脉冲序列。

Result: 发现最优控制场只能取边界值或零值（对应奇异控制）。对于较大的控制界限比值，识别出三阶段对称最优脉冲序列，并解析计算了脉冲时序与比值的关系。对于较小的比值，最优脉冲序列失去对称性且阶段数增加。在所有情况下，解析或数值最优协议都比其他脉冲协议实现更快的完美手性分离。

Conclusion: 本研究通过最优控制理论为手性分离问题提供了高效解决方案，最优协议因控制场的同步作用而显著提高分离速度，有望在需要手性分离的自然科学各领域得到广泛应用。

Abstract: In this work, we formulate the problem of achieving in minimum-time perfect chiral resolution with bounded control fields, as an optimal control problem on two non-interacting spins-$1/2$. We assume the same control bound for the two Raman fields (pump and Stokes) and a different bound for the field connecting directly the two lower-energy states. Using control theory, we show that the optimal fields can only take the boundary values or be zero, the latter corresponding to singular control. Subsequently, using numerical optimal control and intuitive arguments, we identify some three-stage symmetric optimal pulse-sequences, for relatively larger values of the ratio between the two control bounds, and analytically calculate the corresponding pulse timings as functions of this ratio. For smaller values of the bounds ratio, numerical optimal control indicates that the optimal pulse-sequence loses its symmetry and the number of stages increases in general. In all cases, the analytical or numerical optimal protocol achieves a faster perfect chiral resolution than other pulsed protocols, mainly because of the simultaneous action of the control fields. The present work is expected to be useful in the wide spectrum of applications across the natural sciences where enantiomer separation is a crucial task.

</details>


### [243] [Graph restricted tensors: building blocks for holographic networks](https://arxiv.org/abs/2512.23005)
*Rafaĺ Bistroń,Márton Mestyán,Balázs Pozsgay,Karol Życzkowski*

Main category: quant-ph

TL;DR: 提出"图限制张量"新框架，通过图编码量子态的分区纠缠约束，统一处理多种特殊纠缠态，应用于全息原理的张量网络模型


<details>
  <summary>Details</summary>
Motivation: 研究具有特定关联性质的少体量子态，这些态要求系统划分为互补两部分时具有最大二分纠缠。需要统一框架来处理这类约束，并探索全息原理张量网络模型中的新张量类型

Method: 提出"图限制张量"新框架，将纠缠约束编码在图中。该框架统一了文献中的多个例子，包括1-均匀多体态、与对偶酉算子相关的量子态、以及对应2-酉矩阵的绝对最大纠缠态。特别关注全息原理张量网络模型的应用

Result: 在具体案例中找到了精确解析解，证明存在大量非稳定子张量可用于全息原理的晶格模型，扩展了全息张量网络的构建工具集

Conclusion: 图限制张量框架为处理具有特定纠缠约束的量子态提供了统一方法，揭示了全息原理张量网络模型中存在丰富的非稳定子张量资源，为全息对偶的晶格模型构建开辟了新途径

Abstract: We analyze few-body quantum states with particular correlation properties imposed by the requirement of maximal bipartite entanglement for selected partitions of the system into two complementary parts. A novel framework to treat this problem by encoding these constraints in a graph is advocated; the resulting objects are called ``graph-restricted tensors''. This framework encompasses several examples previously treated in the literature, such as 1-uniform multipartite states, quantum states related to dual unitary operators and absolutely maximally entangled states (AME) corresponding to 2-unitary matrices. Original examples of presented graph-restricted tensors are motivated by tensor network models for the holographic principle. In concrete cases we find exact analytic solutions, demonstrating thereby that there exists a vast landscape of non-stabilizer tensors useful for the lattice models of holography.

</details>


### [244] [Symmetry-Preserving Variational Quantum Simulation of the Heisenberg Spin Chain on Noisy Quantum Hardware](https://arxiv.org/abs/2512.23009)
*Rudraksh Sharma*

Main category: quant-ph

TL;DR: 该论文研究了变分量子算法在NISQ设备上模拟量子多体系统时，物理对称性保持的变分电路相比通用硬件高效电路的优势，通过一维反铁磁海森堡自旋链验证了对称性保持电路能显著提升能量估计精度、增强对硬件噪声的鲁棒性并改善收敛行为。


<details>
  <summary>Details</summary>
Motivation: 变分量子算法是NISQ时代模拟相互作用量子多体系统最有前景的方法之一，但其成功关键取决于所选择的变分ansatz结构。目前需要研究如何通过物理对称性保持的电路设计来提升变分量子本征求解器的实际性能。

Method: 研究使用两种变分电路：1）通用的硬件高效ansatz；2）物理信息驱动的对称性保持变分电路。以一维反铁磁海森堡自旋-1/2链为研究对象，通过精确对角化和无噪声模拟进行基准测试，并在真实的IQM Garnet量子硬件上进行验证。

Result: 结果表明，在相同资源约束下，将物理对称性融入电路设计相比硬件高效ansatz能显著改善能量估计精度、增强对硬件噪声的鲁棒性，并提供更清晰的收敛行为。

Conclusion: 研究强调了在NISQ时代，针对具体问题构造ansatz对于可靠量子模拟的重要性，物理对称性保持的电路设计是实现高质量量子模拟的关键因素。

Abstract: Variational quantum algorithms are among the most promising approaches for simulating interacting quantum many-body systems on noisy intermediate-scale quantum (NISQ) devices. However, the practical success of variational quantum eigensolvers (VQE) critically depends on the structure of the chosen variational ansatz. In this work, we investigate the ground-state properties of the one-dimensional antiferromagnetic Heisenberg spin-1/2 chain using both generic hardware-efficient ansatz and physics-informed, symmetry-preserving variational circuits. We benchmark variational results against exact diagonalization and noiseless simulations, and subsequently validate the approach on real IQM Garnet quantum hardware. Our results demonstrate that incorporating physical symmetries into the circuit design leads to significantly improved energy estimates, enhanced robustness against hardware noise, and clearer convergence behavior when compared to hardware-efficient ansatz under identical resource constraints. These findings highlight the importance of problem specific ansatz construction for reliable quantum simulations in the NISQ era.

</details>


### [245] [Stabilizer Entropy of Subspaces](https://arxiv.org/abs/2512.23013)
*Simone Cepollaro,Gianluca Cuffaro,Matthew B. Weiss,Stefano Cusumano,Alioscia Hamma,Seth Lloyd*

Main category: quant-ph

TL;DR: 研究量子态嵌入到更大系统子空间时对"魔法"资源（非稳定子性）的影响，发现嵌入选择会影响魔法资源需求，某些稳定子码能实现零或负魔法缺口，从而优化经典和量子模拟效率。


<details>
  <summary>Details</summary>
Motivation: 量子态嵌入在量子纠错码和对称约束系统中普遍存在，但嵌入对非稳定子性（魔法资源）的影响尚不清楚。研究嵌入如何影响魔法资源需求，有助于优化量子模拟和量子计算效率。

Method: 通过分析稳定子熵缺口（魔法缺口）概念，结合解析推导和数值优化方法，研究不同嵌入子空间对魔法资源的影响。分析特定投影子对应的平均非稳定子性，并研究稳定子码和对称诱导子空间的具体案例。

Result: 发现稳定子熵缺口通常为正（需要注入魔法），但零和负缺口也可实现。某些稳定子码类能实现负魔法缺口，表明特定嵌入子空间具有资源优势。通过数值优化找到了各种维度下平均稳定子熵最小和最大的子空间。

Conclusion: 嵌入子空间的选择显著影响魔法资源需求，明智的嵌入策略能提高经典和量子模拟效率。某些稳定子码提供了实现负魔法缺口的范例，为资源优化提供了理论指导。

Abstract: We consider the costs and benefits of embedding the states of one quantum system within those of another. Such embeddings are ubiquitous, e.g., in error correcting codes and in symmetry-constrained systems. In particular we investigate the impact of embeddings in terms of the resource theory of nonstabilizerness (also known as magic) quantified via the stabilizer entropy (SE). We analytically and numerically study the stabilizer entropy gap or magic gap: the average gap between the SE of a quantum state realized within a subspace of a larger system and the SE of the quantum state considered on its own. We find that while the stabilizer entropy gap is typically positive, requiring the injection of magic, both zero and negative magic gaps are achievable. This suggests that certain choices of embedding subspace provide strong resource advantages over others. We provide formulas for the average nonstabilizerness of a subspace given its corresponding projector and sufficient conditions for realizing zero or negative gaps: in particular, certain classes of stabilizer codes provide paradigmatic examples of the latter. Through numerical optimization, we find subspaces which achieve both minimal and maximal average SE for a variety of dimensions, and compute the magic gap for specific error-correcting codes and symmetry-induced subspaces. Our results suggest that a judicious choice of embedding can lead to greater efficiency in both classical and quantum simulations.

</details>


### [246] [Applying Grover-mixer Quantum Alternating Ansatz Algorithm to Higher-order Quadratic Unconstrained Optimization Problems](https://arxiv.org/abs/2512.23026)
*Evgeniy O. Kiktenko,Elizaveta V. Krendeleva,Aleksey K. Fedorov*

Main category: quant-ph

TL;DR: GM-QAOA在解决高阶无约束二进制优化问题上优于传统的XM-QAOA，具有单调性能提升和更优结果，且参数化版本能接近完全优化算法的性能。


<details>
  <summary>Details</summary>
Motivation: 研究Grover-mixer QAOA在高阶无约束二进制优化问题上的应用，这类问题具有多变量交互特性，是组合优化任务的广义类别。探索GM-QAOA相对于传统横向场混合器QAOA的潜在优势。

Method: 提出一个分析框架来建模GM-QAOA动力学，实现最优参数的经典近似以减少优化开销。开发资源高效的参数化GM-QAOA版本，并进行全面的数值研究比较GM-QAOA和XM-QAOA的性能。

Result: GM-QAOA在电路深度增加时表现出单调性能提升，而XM-QAOA则没有。GM-QAOA在高阶无约束二进制优化问题上获得更优结果。参数化GM-QAOA几乎匹配完全优化算法的性能，同时计算需求大大降低。

Conclusion: GM-QAOA是解决复杂优化任务的高效方法，具有在当前量子硬件上实现的潜力。研究结果为GM-QAOA的实际应用提供了实用途径。

Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is among leading candidates for achieving quantum advantage on near-term processors. While typically implemented with a transverse-field mixer (XM-QAOA), the Grover-mixer variant (GM-QAOA) offers a compelling alternative due to its global search capabilities. This work investigates the application of GM-QAOA to Higher-Order Unconstrained Binary Optimization (HUBO) problems, also known as Polynomial Unconstrained Binary Optimization (PUBO), which constitute a generalized class of combinatorial optimization tasks characterized by intrinsically multi-variable interactions. We present a comprehensive numerical study demonstrating that GM-QAOA, unlike XM-QAOA, exhibits monotonic performance improvement with circuit depth and achieves superior results for HUBO problems. An important component of our approach is an analytical framework for modeling GM-QAOA dynamics, which enables a classical approximation of the optimal parameters and helps reduce the optimization overhead. Our resource-efficient parameterized GM-QAOA nearly matches the performance of the fully optimized algorithm while being far less demanding, establishing it as a highly effective approach for complex optimization tasks. These findings highlight GM-QAOA's potential and provide a practical pathway for its implementation on current quantum hardware.

</details>


### [247] [SOFT: a high-performance simulator for universal fault-tolerant quantum circuits](https://arxiv.org/abs/2512.23037)
*Riling Li,Keli Zheng,Yiming Zhang,Huazhe Lou,Shenggang Ying,Ke Liu,Xiaoming Sun*

Main category: quant-ph

TL;DR: SOFT是一款高性能的通用容错量子电路模拟器，利用广义稳定子形式和GPU并行化，首次实现了大规模含非Clifford门的噪声量子电路模拟，验证了魔法态培育协议的有效性。


<details>
  <summary>Details</summary>
Motivation: 量子纠错和容错策略的开发需要可靠的电路模拟工具，现有工具无法模拟包含非Clifford门的大规模噪声量子电路。

Method: SOFT结合广义稳定子形式和高度优化的GPU并行化技术，能够模拟包含非Clifford门的噪声量子电路，在代码距离d=5下模拟了42个量子比特、72个T/T†门和中间电路测量的魔法态培育协议。

Result: 使用适度GPU资源执行超过2000亿次模拟，首次实现了非平凡规模下培育协议的真实模拟，验证了MSC生成高保真逻辑T态的有效性，并发现实际逻辑错误率与先前报告值存在显著差异。

Conclusion: SOFT展示了可靠模拟工具对容错架构设计的重要性，将领域从模拟量子存储器推进到模拟通用量子计算机。

Abstract: Circuit simulation tools are critical for developing and assessing quantum-error-correcting and fault-tolerant strategies. In this work, we present SOFT, a high-performance SimulatOr for universal Fault-Tolerant quantum circuits. Integrating the generalized stabilizer formalism and highly optimized GPU parallelization, SOFT enables the simulation of noisy quantum circuits containing non-Clifford gates at a scale not accessible with existing tools. To provide a concrete demonstration, we simulate the state-of-the-art magic state cultivation (MSC) protocol at code distance $d=5$, involving 42 qubits, 72 $T$ / $T^\dagger$ gates, and mid-circuit measurements. Using only modest GPU resources, SOFT performs over 200 billion shots and achieves the first ground-truth simulation of the cultivation protocol at a non-trivial scale. This endeavor not only certifies the MSC's effectiveness for generating high-fidelity logical $T$-states, but also reveals a large discrepancy between the actual logical error rate and the previously reported values. Our work demonstrates the importance of reliable simulation tools for fault-tolerant architecture design, advancing the field from simulating quantum memory to simulating a universal quantum computer.

</details>


### [248] [Clifford entropy](https://arxiv.org/abs/2512.23050)
*Gianluca Cuffaro,Matthew B. Weiss*

Main category: quant-ph

TL;DR: 本文引入了Clifford熵作为衡量任意幺正算符接近Clifford幺正算符程度的度量，推广了态的稳定子熵概念，并研究了其性质、界和应用。


<details>
  <summary>Details</summary>
Motivation: 需要一种量化幺正算符"魔法性"（非Clifford性）的度量，类似于态层面的稳定子熵，以研究量子计算中Clifford电路与非Clifford资源的关系。

Method: 定义Clifford熵，证明其性质（零值条件、Clifford不变性、张量积次可加性），通过Choi态稳定子熵重写，推导上界，数值计算低维最大值，解析推导平均分布，利用测度集中结果分析随机幺正。

Result: Clifford熵是有效的度量工具，上界不紧，数值估计了低维最大值，解析得到平均分布，证明大维度下随机幺正的Clifford熵与固定魔法门的比值给出掺杂Clifford电路深度的下界。

Conclusion: Clifford熵为量化幺正算符的非Clifford性提供了有力工具，可用于分析量子电路复杂性和资源需求，数值证据表明结果在低维也可靠，提出了未来研究方向。

Abstract: We introduce the Clifford entropy, a measure of how close an arbitrary unitary is to a Clifford unitary, which generalizes the stabilizer entropy for states. We show that this quantity vanishes if and only if a unitary is Clifford, is invariant under composition with Clifford unitaries, and is subadditive under tensor products. Rewriting the Clifford entropy in terms of the stabilizer entropy of the corresponding Choi state allows us to derive an upper bound: that this bound is not tight follows from considering the properties of symmetric informationally complete sets. Nevertheless we are able to numerically estimate the maximum in low dimensions, comparing it to the average over all unitaries, which we derive analytically. Finally, harnessing a concentration of measure result, we show that as the dimension grows large, with probability approaching unity, the ratio between the Clifford entropy of a Haar random unitary and that of a fixed magic gate gives a lower bound on the depth of a doped Clifford circuit which realizes the former in terms of the latter. In fact, numerical evidence suggests that this result holds reliably even in low dimensions. We conclude with several directions for future research.

</details>


### [249] [Theoretical Analysis and Simulations of Memory-based and All-photonic Quantum Repeaters and Networks](https://arxiv.org/abs/2512.23111)
*Chuen Hei Chan,Charu Jain,Ezra Kissel,Wenji Wu,Edwin Barnes,Sophia E. Economou,Inder Monga*

Main category: quant-ph

TL;DR: 该研究通过理论分析和模拟比较了基于内存的离子阱量子中继器和全光子纠缠量子中继器两种量子网络范式的性能与资源需求。


<details>
  <summary>Details</summary>
Motivation: 随着量子网络向长距离扩展，需要开发先进的量子中继器技术。不同类型的量子中继器（基于内存或全光子）可能适用于不同的量子技术、网络规模或操作参数，需要系统比较它们的相对性能和资源需求。

Method: 采用理论分析和模拟方法，研究基于内存的第一代离子阱量子中继器网络和全光子纠缠量子中继器网络。分析两种范式的纠缠生成速率、保真度和资源需求。

Result: 研究结果为量子硬件和组件的优化提供了指导，并揭示了鲁棒控制平面的重要性。通过比较两种量子网络范式的性能指标，为不同应用场景下的技术选择提供了依据。

Conclusion: 不同类型的量子中继器各有优劣，适用于不同的技术条件和应用需求。该研究为量子网络技术的发展提供了重要的理论指导和优化方向。

Abstract: Developing and deploying advanced Quantum Repeater (QR) technologies will be necessary to scale quantum networks to longer distances. Depending on the error mitigation mechanisms adopted to suppress loss and errors, QRs are typically classified into memory-based or all-photonic QRs; and each type of QR may be best suited for a specific type of underlying quantum technology, a particular scale of quantum networks, or a specific regime of operational parameters. We perform theoretical analysis and simulations of quantum repeaters and networks to investigate the relative performance and resource requirements of different quantum network paradigms. Our results will help guide the optimization of quantum hardware and components and shed light on the role of a robust control plane. We present our research findings on theoretical analysis and simulations of memory-based first-generation trapped-ion quantum repeaters and networks, and all-photonic entanglement-based quantum repeaters and networks. We study the relative performance in terms of entanglement generation rate and fidelity, as well as the resource requirements of these two different quantum network paradigms.

</details>


### [250] [Efficient flip-chip and on-chip-based modulation of flux-tunable superconducting resonators](https://arxiv.org/abs/2512.23119)
*Achintya Paradkar,Paul Nicaise,Karim Dakroury,Fabian Resare,Christian Dejaco,Lukas Deeg,Gerhard Kirchmair,Witlef Wieczorek*

Main category: quant-ph

TL;DR: 该论文展示了使用翻转芯片或片上输入线圈高效调制磁通可调超导谐振器的方法，实现了超过1GHz的磁通调制和高达20%的磁通传输效率。


<details>
  <summary>Details</summary>
Motivation: 开发高效、低电流的磁通调制技术对于超导量子电路和敏感磁通测量应用至关重要，需要解决传统方法中磁通传输效率低和需要大电流的问题。

Method: 采用铝基四分之一波长共面波导谐振器，末端连接100μm或200μm宽的方形环路dc-SQUID，使用1μm尺寸的约瑟夫森结。通过增加SQUID的几何环路电感（最高0.7nH）提高磁通传输效率，使用非对称结缓解分支切换效应。比较了翻转芯片和片上输入线圈两种磁通调制方案。

Result: 实现了超过1GHz的磁通调制，磁通响应度高达数十GHz/Φ₀，仅需微安级片上电流。磁通传输效率达到20%，翻转芯片和片上输入线圈方案都表现出良好性能。

Conclusion: 该工作为高效低电流磁通调制FTRs和敏感磁通测量铺平了道路，展示了两种实用方案在超导量子电路中的潜力。

Abstract: We demonstrate the efficient modulation of flux-tunable superconducting resonators (FTRs) using flip-chip or on-chip-based input coils. The FTRs we use are aluminum-based quarter-wave coplanar waveguide resonators terminated with 100um or 200um-wide square loop dc superconducting quantum interference devices (SQUIDs) employing 1um-sized Josephson junctions. We employ SQUIDs with a geometric loop inductance of up to 0.7nH to increase the flux transfer efficiency. The geometric inductance of the SQUID results in a non-zero screening parameter $β_L$, whose branch switching effect is mitigated by using asymmetric junctions. We achieve flux modulation of the FTRs by more than one GHz and flux responsivities of up to tens of GHz/$Φ_0$ with uA-scale on-chip currents. We compare flip-chip with on-chip input-coil-based flux modulation, where the former is realized through galvanically connected and closely spaced chips, while the latter is achieved through superconducting air-bridge connections. We achieve a flux-transfer efficiency from the input coil to the SQUID loop of up to 20%. Our work paves the way for efficient low current flux modulation of FTRs and sensitive measurement of flux signals.

</details>


### [251] [Emergence of nonclassical radiation in strongly laser-driven quantum systems](https://arxiv.org/abs/2512.23156)
*Ivan Gonoskov,Christian Hünecke,Stefanie Gräfe*

Main category: quant-ph

TL;DR: 提出了强场物理中高次谐波产生的完全量子理论，揭示了非经典量子态的内在产生机制，为可调谐、高光子数的量子光源设计提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 当前非经典光源平台可调性有限且光子数低，而强场物理中的高次谐波产生虽然能产生可调谐的明亮相干辐射，但其量子光学特性一直缺乏理论解释。需要建立统一的理论框架来理解这些量子效应的起源和可控性。

Method: 提出了完全量子、解析可处理的强光-物质相互作用理论，采用参数化因子分解方法将耦合的电子-场系统分解为驱动的电子态和动态扰动的量子光学场，直接从含时薛定谔方程推导，无需条件测量、零差探测或模式选择技术。

Result: 理论揭示了量子关联、压缩态和Wigner函数负性如何从相互作用动力学中内在产生，确定了特定非经典特征被放大或抑制的精确条件。理论能够预测设计可调谐频率下的明亮、高光子数量子态，并展示了生成明亮非经典紫外光的现实条件。

Conclusion: 为强场量子光学建立了全面的理论基础，为桌面级量子光源在传感、通信和光子量子信息处理中的应用开辟了新途径。

Abstract: Nonclassical light sources are central to emerging quantum technologies, yet current platforms offer limited tunability and typically operate at low photon numbers. In parallel, strong-field physics provides widely tunable, bright coherent radiation through high-order harmonic generation (HHG), but its quantum optical character has remained largely unexplained. While recent experiments have revealed signatures of entanglement, squeezing, and quantum-state modification in both the driving and generated fields, a unified theoretical framework capable of identifying the origin and controllability of these effects has been missing. Here we introduce a fully quantum, analytically tractable theory of intense light-matter interaction that rigorously captures the emergence of nonclassicality in HHG. Our approach employs a parametric factorization of the coupled electron-field system into a driven electronic state and a dynamically perturbed quantum optical field, derived directly from the time-dependent Schrödinger equation without requiring conditioning, homodyne detection, or mode-selection techniques. We show how quantum correlations, squeezing, and Wigner-function negativity arise intrinsically from the interaction dynamics, and we identify the precise conditions under which specific nonclassical features are amplified or suppressed. The theory enables predictive design of bright, high-photon-number quantum states at tunable frequencies, and we demonstrate its utility by outlining realistic conditions for generating bright nonclassical ultraviolet light. Our results establish a comprehensive foundation for strong-field quantum optics and open new avenues toward tabletop quantum light sources for sensing, communication, and photonic quantum information processing.

</details>


### [252] [Quantum Metrology via Adiabatic Control of Topological Edge States](https://arxiv.org/abs/2512.23168)
*Xingjian He,Aoqian Shi,Jianjun Liu,Jiangbin Gong*

Main category: quant-ph

TL;DR: 该论文发现拓扑相变在量子传感中的两个优势：高阶能带接触决定灵敏度随系统尺寸的标度关系，以及利用边缘态制备纠缠态可实现宏观量子纠缠增强


<details>
  <summary>Details</summary>
Motivation: 探索拓扑相变在量子传感中的独特优势，特别是利用拓扑边缘态的特性来增强量子测量的灵敏度

Method: 通过分析拓扑相变中能带接触的阶数对量子费希尔信息标度的影响，以及在拓扑晶格中利用简并边缘模式制备N粒子纠缠态并绝热调控到相变点

Result: 发现量子费希尔信息标度为：对于高阶能带接触，$\mathcal{F}_Q \sim L^{2p}$；对于N粒子纠缠边缘态，$\mathcal{F}_Q \sim N^2 L^{2p}$，其中p为能带接触阶数，L为一维晶格尺寸

Conclusion: 该工作为利用拓扑相变结合纠缠、大晶格尺寸和高阶能带接触进行量子计量学开辟了新的可能途径

Abstract: Criticality-based quantum sensing exploits hypersensitive response to system parameters near phase transition points. This work uncovers two metrological advantages offered by topological phase transitions when the probe is prepared as topological edge states. Firstly, the order of topological band touching is found to determine how the metrology sensitivity scales with the system size. Engineering a topological phase transition with higher-order band touching is hence advocated, with the associated quantum Fisher information scaling as $ \mathcal{F}_Q \sim L^{2p}$, with $L$ the lattice size in one dimension, and $p$ the order of band touching. Secondly, with a topological lattice accommodating degenerate edge modes (such as multiple zero modes), preparing an $N$-particle entangled state at the edge and then adiabatically tuning the system to the phase transition point grows quantum entanglement to macroscopic sizes, yielding $\mathcal{F}_Q \sim N^2 L^{2p}$. This work hence paves a possible topological phase transition-based route to harness entanglement, large lattice size, and high-order band touching for quantum metrology.

</details>


### [253] [LogosQ: A High-Performance and Type-Safe Quantum Computing Library in Rust](https://arxiv.org/abs/2512.23183)
*Shiwen An,Jiayi Wang,Konstantinos Slavakis*

Main category: quant-ph

TL;DR: LogosQ是一个用Rust实现的高性能、后端无关的量子计算库，通过编译时类型安全确保正确性，相比Python框架（PennyLane、Qiskit）在状态准备上提速900倍，变分计算提速2-5倍。


<details>
  <summary>Details</summary>
Motivation: 现有Python量子计算框架存在运行时错误和可扩展性瓶颈，动态特性导致开发健壮高性能量子软件困难。需要一种能通过编译时类型安全确保正确性，同时提供高性能的解决方案。

Method: 使用Rust实现，利用静态分析消除运行时错误；引入直接状态向量操作、自适应并行处理、FFT优化的量子傅里叶变换等优化技术；支持参数偏移规则梯度计算；实现后端无关设计。

Result: 相比Python框架（PennyLane、Qiskit）：状态准备（QFT）提速900倍，变分工作负载提速2-5倍；相比Julia实现（Yao）：提速6-22倍；与Q#性能相当；在变分量子本征求解器实验中达到化学精度，其他库失败的情况下仍能稳定运行。

Conclusion: LogosQ将系统编程的安全性与高级电路优化相结合，为可靠高效的量子模拟设立了新标准，解决了现有动态框架的运行时错误和可扩展性问题。

Abstract: Developing robust and high performance quantum software is challenging due to the dynamic nature of existing Python-based frameworks, which often suffer from runtime errors and scalability bottlenecks. In this work, we present LogosQ, a high performance backend agnostic quantum computing library implemented in Rust that enforces correctness through compile time type safety. Unlike existing tools, LogosQ leverages Rust static analysis to eliminate entire classes of runtime errors, particularly in parameter-shift rule gradient computations for variational algorithms. We introduce novel optimization techniques, including direct state-vector manipulation, adaptive parallel processing, and an FFT optimized Quantum Fourier Transform, which collectively deliver speedups of up to 900 times for state preparation (QFT) and 2 to 5 times for variational workloads over Python frameworks (PennyLane, Qiskit), 6 to 22 times over Julia implementations (Yao), and competitive performance with Q sharp. Beyond performance, we validate numerical stability through variational quantum eigensolver (VQE) experiments on molecular hydrogen and XYZ Heisenberg models, achieving chemical accuracy even in edge cases where other libraries fail. By combining the safety of systems programming with advanced circuit optimization, LogosQ establishes a new standard for reliable and efficient quantum simulation.

</details>


### [254] [Quantum Phase Transitions in Coherent Ising Machines: XY Model for Demonstration](https://arxiv.org/abs/2512.23248)
*Jing-Yi-Ran Jin,Shuang-Quan Ma,Qing Ai*

Main category: quant-ph

TL;DR: 通过光谱映射将一维XY自旋模型与简并光学参量振荡器网络对应，揭示了CIM能够精确再现XY模型的量子临界行为，表明CIM不仅是组合优化问题的求解平台，也是研究量子临界现象的光学模拟器。


<details>
  <summary>Details</summary>
Motivation: 研究相干伊辛机中的量子相变，探索其作为量子临界现象光学模拟器的潜力，而不仅仅是组合优化问题的求解平台。

Method: 建立一维XY自旋模型与简并光学参量振荡器网络之间的光谱映射对应关系，分析基态能量密度及其导数来揭示二阶量子相变。

Result: DOPO网络能够精确再现XY模型在其各向异性、各向同性和横向场伊辛区域中的量子临界行为，磁化率在临界点处呈现奇异性。

Conclusion: 相干伊辛机不仅可作为解决组合优化问题的强大平台，还能作为研究普适量子临界现象的多功能光学模拟器，连接量子自旋模型和光子量子系统。

Abstract: Quantum phase transitions (QPTs) in coherent Ising machines (CIMs) are studied via a spectral mapping between the one-dimensional XY spin model and a network of degenerate optical parametric oscillators (DOPOs). This exact correspondence reveals that the DOPO network faithfully reproduces the quantum critical behavior of the XY model across its anisotropic, isotropic, and transverse-field Ising regimes. The ground-state energy density and its derivatives are analyzed to reveal second-order QPTs characterized by singularities in magnetic susceptibility at critical points. These results show that CIMs do not only serve as powerful platforms for solving combinatorial optimization problems but also provide a versatile optical simulator for studying universal quantum critical phenomena, bridging quantum-spin models and photonic quantum systems.

</details>


### [255] [Towards a Faithful Quantumness Certification Functional for One-Dimensional Continuous-Variable Systems](https://arxiv.org/abs/2512.23299)
*Ole Steuernagel,Ray-Kuang Lee*

Main category: quant-ph

TL;DR: 该论文指出先前提出的非经典性认证函数ξ存在缺陷，即使对于已知的非经典态也可能失效，作者改进了认证函数但仍无法完全解决弱非经典态的认证问题。


<details>
  <summary>Details</summary>
Motivation: 先前研究提出的非经典性认证函数ξ虽然对噪声容忍且敏感，但存在认证失败的情况，即某些已知的非经典态无法通过ξ<0来认证，需要改进认证方法。

Method: 通过分析ξ认证函数失效的具体例子，将ξ推广为更一般的形式，构建出目前最优的认证函数族，但仍发现对弱非经典态认证存在局限性。

Result: 发现了ξ认证函数对某些非经典态失效的情况，改进了认证函数使其具有更好的形式，但改进后的函数仍无法可靠认证弱非经典态。

Conclusion: 如何忠实认证量子态的非经典性仍然是一个开放问题，即使目前最好的认证函数族也无法完全解决弱非经典态的认证挑战。

Abstract: If the phase space-based Sudarshan-Glauber distribution, $P_ρ$, has negative values the quantum state, $ρ$, it describes is nonclassical. Due to $P$'s singular behavior this simple criterion is impractical to use. Recent work [Bohmann and Agudelo, Phys. Rev. Lett. 124, 133601 (2020)] presented a general, sensitive, and noise-tolerant certification functional, $ξ_{P}$, for the detection of non-classical behavior of quantum states $P_ρ$. There, it was shown that when this functional takes on negative values somewhere in phase space, $ξ_{P}(x,p) < 0$, this is \emph{sufficient} to certify the nonclassicality of a state. Here we give examples where this certification fails. We investigate states which are known to be nonclassical but the certification functions is positive $ξ(x,p) \geq 0$ everywhere in phase space. We generalize $ξ$ giving it an appealing form which allows for improved certification. This way we generate the best family of certification functions available so far. Yet, they also fail for very weakly nonclassical states, in other words, the question how to faithfully certify quantumness remains an open question.

</details>


### [256] [Generation of Squeezed Fock States by Particle-Number Measurements on Multimode Gaussian States](https://arxiv.org/abs/2512.23323)
*S. B. Korolev,A. A. Silin*

Main category: quant-ph

TL;DR: 通过多模高斯态中粒子数测量生成压缩福克态，提出一种普适方案，其生成概率更高但需要更多实验资源


<details>
  <summary>Details</summary>
Motivation: 研究如何通过多模高斯态中的粒子数测量来生成压缩福克态，探索普适性方案以提高生成概率

Method: 识别一类N模高斯态，测量其中N-1个模式可生成压缩福克态；提出普适方案，分析生成概率和对探测器缺陷的鲁棒性

Result: 普适方案相比非普适方案提供更高的压缩福克态生成概率，但需要更多实验资源；生成的压缩福克态仅取决于总检测粒子数，与探测器间分布无关

Conclusion: 通过多模高斯态粒子数测量可有效生成压缩福克态，普适方案在生成概率方面具有优势，为量子光学应用提供了新途径

Abstract: We investigate the generation of squeezed Fock states (SFSs) via particle-number measurements in the modes of multimode Gaussian states. We identify a universal class of $N$-mode Gaussian states for which measuring $N-1$ modes results in the generation of SFSs. The key feature of these states is that the generated SFSs depend only on the total number of detected particles and are independent of their distribution among the detectors. Based on the general form of the wave functions of multimode Gaussian states, we propose a universal scheme for SFS generation. For this scheme, we evaluate the probability of SFS generation and analyze the robustness of the process against imperfections in particle-number-resolving detectors. In addition, we compare the universal scheme with a nonuniversal scheme, in which the generation of SFSs depends on a specific distribution of particle numbers across the detectors. We demonstrate that the universal scheme provides a higher probability of SFS generation, at the cost of increased experimental resources.

</details>


### [257] [The Quantum Rashomon Effect as a Failure of Gluing](https://arxiv.org/abs/2512.23325)
*Partha Ghose*

Main category: quant-ph

TL;DR: 论文指出量子理论中的"罗生门"现象可理解为"粘合失败"，即局部描述无法组合成全局一致叙述，这与语境性的现代层论处理方法中的数学障碍相同。


<details>
  <summary>Details</summary>
Motivation: Szangolies在扩展维格纳朋友场景中论证量子理论允许"罗生门"情境，即多个内部一致的描述无法合并为单一全局叙述。本文旨在解释这一现象的本质。

Method: 将罗生门现象解释为"粘合失败"：不同语境下的局部描述存在，但无法形成单一的"所有视角同时"的全局描述。这与语境性的层论处理方法中的数学障碍相同。

Result: 量子理论中的罗生门现象可理解为粘合失败，与语境性的现代层论处理方法中的数学障碍相同。这一视角也可应用于社会科学中的语境效应分析。

Conclusion: 罗生门现象本质上是粘合失败，与语境性的层论处理方法共享相同数学结构。这一视角不仅适用于量子理论，也适用于社会科学中量子类建模的语境效应分析。

Abstract: Recently Szangolies has argued (in the setting of extended Wigner's-friend scenarios) that quantum theory permits ``Rashomon'' situations: multiple internally coherent accounts of events that cannot be combined into a single, consistent global narrative. This note explains why the Rashomon phenomenon can be understood as a \emph{failure of gluing}: local descriptions over different contexts exist, but they do not admit a single global ``all-perspectives-at-once'' description. This is the same mathematical obstruction that underlies modern sheaf-theoretic treatments of contextuality. I then indicate why the same perspective is useful in parts of the social sciences (quantum-like modelling of cognition, judgment, and decision-making), where ``context effects'' can likewise be interpreted as the absence of a single joint probability space.

</details>


### [258] [Practical quantum teleportation with finite-energy codebooks](https://arxiv.org/abs/2512.23388)
*W. K. Yam,M. Renger,S. Gandorfer,R. Gross,K. G. Fedorov*

Main category: quant-ph

TL;DR: 该论文研究了微波连续变量量子隐形传态的实际应用，分析了前馈损耗、噪声、有限码本和公共信道攻击对传态保真度的影响，并提出了相应的修正方案。


<details>
  <summary>Details</summary>
Motivation: 微波量子隐形传态是实现分布式量子计算和微波量子通信的重要工具，但实际应用中存在前馈损耗、噪声、有限码本等非理想因素，需要理论分析这些因素对传态性能和安全性的影响。

Method: 基于连续变量态的微波模拟量子隐形传态协议，理论分析前馈损耗和噪声对相干态传态保真度的影响，研究有限码本配置下的修正无克隆阈值，并分析公共信道攻击下的安全性。

Result: 发现前馈损耗和噪声可以通过适当的前馈增益完全校正；推导了有限码本配置下的修正无克隆阈值；证明了安全保真度阈值可能与传统的无克隆值有显著差异。

Conclusion: 该研究为量子通信协议的发展做出贡献，特别是证明了量子隐形传态在现实微波网络中实现鲁棒且无条件安全通信的可行性。

Abstract: Quantum communication exploits non-classical correlations to achieve efficient and unconditionally secure exchange of information. In particular, the quantum teleportation protocol allows for a deterministic and secure transfer of unknown quantum states by using pre-shared quantum entanglement and classical feedforward communication. Quantum teleportation in the microwave regime provides an important tool for high-fidelity remote quantum operations, enabling distributed quantum computing with superconducting circuits and potentially facilitating short-range, open-air microwave quantum communication. In this context, we consider practical application scenarios for the microwave analog quantum teleportation protocol based on continuous-variable states. We theoretically analyze the effect of feedforward losses and noise on teleportation fidelities of coherent states and show that these imperfections can be fully corrected by an appropriate feedforward gain. Furthermore, we consider quantum teleportation with finite-size codebooks and derive modified no-cloning thresholds as a function of the codebook configuration. Finally, we analyze the security of quantum teleportation under public channel attacks and demonstrate that the corresponding secure fidelity thresholds may drastically differ from the conventional no-cloning values. Our results contribute to the general development of quantum communication protocols and, in particular, illustrate the feasibility of using quantum teleportation in realistic microwave networks for robust and unconditionally secure communication.

</details>


### [259] [Anisotropic Quantum Annealing vs Trit Annealing](https://arxiv.org/abs/2512.23469)
*M. Haider Akbar,Özgür E. Müstecaplıoğlu*

Main category: quant-ph

TL;DR: 在自旋-1系统中引入单离子各向异性项D∑(S^z)²，量子退火性能优于传统自旋-1/2系统，尤其适合三值决策问题


<details>
  <summary>Details</summary>
Motivation: 探索自旋-1系统在量子退火中的性能优势，传统量子退火主要基于自旋-1/2系统，但自旋-1系统具有中间自旋能级和可调各向异性，可能提供更好的优化性能

Method: 在自旋-1系统的哈密顿量中引入单离子各向异性项D∑(S^z)²，研究不同各向异性强度D下的量子退火性能，通过中间自旋能级实现更小的增量步骤演化

Result: 在合适的各向异性强度D范围内，自旋-1退火器以更高的保真度达到基态，中间自旋能级和可调各向异性使算法能够通过更小的增量步骤遍历能量景观

Conclusion: 高自旋退火器在量子优化中具有内在优势，特别是对于自然采用三值决策变量表述的问题，能够降低配置空间中的势垒并稳定演化过程

Abstract: Quantum annealing offers a promising strategy for solving complex optimization problems by encoding the solution into the ground state of a problem Hamiltonian. While most implementations rely on spin-$1/2$ systems, we explore the performance of quantum annealing on a spin-$1$ system where the problem Hamiltonian includes a single ion anisotropy term of the form $D\sum (S^z)^2$. Our results reveal that for a suitable range of the anisotropy strength $D$, the spin-$1$ annealer reaches the ground state with higher fidelity. We attribute this performance to the presence of the intermediate spin level and the tunable anisotropy, which together enable the algorithm to traverse the energy landscape through smaller, incremental steps instead of a single large spin flip. This mechanism effectively lowers barriers in the configuration space and stabilizes the evolution. These findings suggest that higher spin annealers offer intrinsic advantages for robust and flexible quantum optimization, especially for problems naturally formulated with ternary decision variables.

</details>


### [260] [Cavity-Free $Δ$-Type Coherent Population Trapping for Microwave Sensing](https://arxiv.org/abs/2512.23484)
*Ido Fridman,Shemuel Sternklar,Eliran Talker*

Main category: quant-ph

TL;DR: 实验和理论研究腔自由微波场耦合Λ型原子系统的两个基态，形成闭合Δ构型，研究微波参数对基态相干性的影响


<details>
  <summary>Details</summary>
Motivation: 研究腔自由条件下微波场耦合Λ型原子系统基态的特性，探索微波参数对相干布居囚禁共振的影响，为紧凑型原子钟和量子增强传感平台提供理论基础

Method: 实验和理论结合：实验研究微波场参数对CPT共振的影响；理论建立数值密度矩阵模型，显式包含微波耦合强度，描述无相位匹配Δ系统的物理本质

Result: 观察到CPT共振对微波功率和失谐的显著依赖性，共振对比度、线宽和中心频率发生可测量变化；理论与实验高度一致

Conclusion: 建立了简单而稳健的腔自由Δ型原子系统微波控制框架，对紧凑型原子钟和量子增强量子传感平台有直接应用价值

Abstract: We investigated experimentally and theoretically a cavity-free microwave field that couples the two ground states of a Λ-type atomic system, thereby forming a closed Δ configuration. In this regime, the absence of cavity-imposed phase matching leads to a strong sensitivity of the ground-state coherence to the microwave field parameters. We observe that the coherent population trapping (CPT) resonance exhibits a pronounced dependence on the microwave power and detuning, resulting in measurable changes in resonance contrast, linewidth, and center frequency. To explain these effects, we develop a numerical density-matrix model in which the ground-state coherence explicitly incorporates the microwave coupling strength, capturing the essential physics of this no-phase-matching Δ system. The excellent agreement between theory and experiment establishes a simple and robust framework for microwave control of cavity-free Δ-type atomic systems, with direct implications for compact atomic clocks and quantum-enhanced quantum sensing platforms.

</details>


### [261] [Van der Waals interaction at short and long distances: a pedagogical path from stationary to time-dependent perturbation theory](https://arxiv.org/abs/2512.23517)
*L. Saba,C. D. Fosco*

Main category: quant-ph

TL;DR: 通过时间关联函数重构微扰论，统一处理原子间范德瓦尔斯相互作用的短程和长程极限


<details>
  <summary>Details</summary>
Motivation: 传统方法中，中性原子间的范德瓦尔斯相互作用在短程（伦敦极限）使用定态微扰论，长程（卡西米尔-波尔德极限）使用半经典时变方法，缺乏统一框架。本文旨在提供一种简化数学处理、连接两种极限的统一方法。

Method: 将定态微扰论计算重新表述为时间关联函数形式。这种方法特别适用于需要考虑延迟效应的高阶长程计算，提供了更清晰的物理图像。

Result: 时间关联函数方法显著简化了数学处理，特别是在高阶计算中。该方法成功连接了短程伦敦极限和长程卡西米尔-波尔德极限，为高级量子力学课程提供了统一框架。

Conclusion: 通过时间关联函数重构微扰论为范德瓦尔斯相互作用提供了统一的处理框架，简化了数学计算，连接了两种极限情况，适合教学使用。

Abstract: The van der Waals interaction between neutral atoms is typically studied using stationary perturbation theory for the short-distance (London) limit, while long-distance (Casimir-Polder) results are usually derived via semiclassical, time-dependent approaches. In this pedagogical article, we demonstrate that reformulating stationary perturbation theory calculations in terms of time-ordered correlation functions significantly simplifies the mathematical treatment. This reformulation is particularly advantageous for higher-order calculations required in the long-distance regime, where retardation effects become important. Our approach provides a unified framework that connects both limiting cases while offering a clear conceptual picture suitable for advanced quantum mechanics courses.

</details>


### [262] [Clauser-Horne-Shimony-Holt Bell-inequality Violability with the Full Poincaré-Bloch Sphere](https://arxiv.org/abs/2512.23550)
*Carlos Cardoso-Isidoro,Enrique J. Galvez*

Main category: quant-ph

TL;DR: 该研究实验验证了CHSH贝尔不等式在完整庞加莱-布洛赫球面上的违反情况，发现贝尔态在相同基下违反不等式，而在不同基下不违反；非贝尔最大纠缠态则相反。


<details>
  <summary>Details</summary>
Motivation: 传统CHSH贝尔不等式测试通常使用线偏振投影，但该不等式适用于庞加莱-布洛赫球面上的所有态。实验室研究很少探索完整球面上的违反情况，因此需要实验验证不同偏振基下的CHSH不等式违反行为。

Method: 通过实验验证预测的CHSH不等式违反情况，使用贝尔态和非贝尔态，对每个光子采用相同和不同的线偏振和椭圆偏振基态进行投影测量。

Result: 贝尔态在相同基下违反CHSH不等式（无论椭圆度如何），但在不同基下不违反。相反，发现非贝尔最大纠缠态在不同基下违反不等式，而在相同基下不违反。

Conclusion: 研究揭示了CHSH贝尔不等式违反行为对偏振基选择的依赖性，贝尔态和非贝尔态表现出相反的违反模式，这深化了对量子纠缠和贝尔不等式测试的理解。

Abstract: Linearly polarized projections are the tacit means for performing Clauser-Horne-Shimony-Holt (CHSH) Bell-inequality tests using polarization-entangled photon pairs. The inequality is valid for all states on the Poincaré-Bloch sphere, but few laboratory studies have investigated violations with the full sphere. In this article, we explore the experimental verifications of the predicted violations of the CHSH inequality with Bell and non-Bell states with same and different linear and elliptically polarized basis states for each photon. We find that Bell states violate CHSH when using the same basis for both photons, regardless of their ellipticity, whereas they show no violations for photon projections in different bases. We found non-Bell maximally-entangled states for which the converse is true.

</details>


### [263] [Averaging of quantum channels via channel-state duality](https://arxiv.org/abs/2512.23586)
*Marcin Markiewicz,Łukasz Pawela,Zbigniew Puchała*

Main category: quant-ph

TL;DR: 本文提出了一种基于通道-状态对偶的量子通道平均框架，将预处理和后处理平均转化为直接作用于Choi算符的群旋转，并扩展到非酉对称群和有限实现方案。


<details>
  <summary>Details</summary>
Motivation: 传统的旋转（twirling）方法通过对称作用的均匀平均来简化量子态和量子通道的描述，但需要一种更系统的框架来处理量子通道的平均，特别是当输入和输出空间具有不同对称性表示时。

Method: 利用通道-状态对偶将预处理和后处理平均转化为直接作用于Choi算符的群旋转；对于集体设置，引入部分转置约简将通道旋转映射到部分转置Choi算符的普通Schur-Weyl旋转；通过Cartan分解扩展到非紧致对称群；提出两种有限实现方案：对偶平均协议和加权群t-设计诱导的通道t-设计。

Result: 得到了任意酉表示下旋转通道的显式投影表达式；在集体设置中获得了基于置换算符的公式；扩展到非酉对称群时得到了带权重的不变子空间投影；提出了两种可行的有限实现方案。

Conclusion: 该框架为量子通道平均提供了统一的理论基础，能够处理各种对称性设置，包括非紧致群和有限实现，为量子信息处理中的对称性利用提供了系统工具。

Abstract: Twirling, uniform averaging over symmetry actions, is a standard tool for reducing the description of quantum states and channels to symmetry-invariant data. We develop a framework for averaging quantum channels based on channel-state duality that converts pre- and post-processing averages into a group twirl acting directly on the Choi operator. For arbitrary unitary representations on the input and output spaces, the twirled channel is obtained as an explicit projection onto the commutant of the induced representation on $\mathcal H_{\rm out}\otimes \mathcal H_{\rm in}$. In the collective setting, where the commutant is the walled Brauer algebra, we introduce a partial-transpose reduction that maps channel twirling to an ordinary Schur-Weyl twirl of the partially transposed Choi operator, enabling formulas in terms of permutation operators. We further extend the construction beyond compact symmetries to reductive non-unitary groups via Cartan decomposition, yielding a weighted sum of invariant-sector projections with weights determined by the Abelian component. Finally, we provide two finite realizations of channel averaging. The first one is a ``dual'' averaging protocol as a convex mixture of unitary-$1$-design channels on invariant sectors. The second one is a notion of channel $t$-designs induced by weighted group $t$-designs for $t=t_{\rm in}+t_{\rm out}$.

</details>


### [264] [Paradox-free classical non-causality and unambiguous non-locality without entanglement are equivalent](https://arxiv.org/abs/2512.23599)
*Hippolyte Dourdent,Kyrylo Simonov,Andreas Leitherer,Emanuel-Cristian Boghiu,Ravi Kunjwal,Saronath Halder,Remigiusz Augusiak,Antonio Acín*

Main category: quant-ph

TL;DR: 本文首次完整递归表征了过程函数和（非）因果过程函数，建立了过程函数与无歧义完全乘积基之间的对应关系，揭示了非因果性与无纠缠量子非定域性之间的精确镜像关系。


<details>
  <summary>Details</summary>
Motivation: 研究闭合类时曲线（CTCs）下的因果性问题，需要避免时间旅行悖论同时遵守"无新物理"原则。过程函数作为信息论框架下的确定性经典通信结构，能够保持逻辑一致性但展现出与任何确定因果顺序不相容的关联（非因果性）。

Method: 首次提供过程函数和（非）因果过程函数的完整递归表征；建立过程函数与无歧义完全乘积基之间的对应关系；利用这种等价关系证明过程函数的非因果性精确对应于此类基的无纠缠量子非定域性。

Result: 将先前特殊情况推广到任意局部维度和任意参与方数量；实现了非因果过程函数和无歧义QNLWE基的系统构造；揭示了某些非信号不等式与因果不等式之间的意外联系。

Conclusion: 过程函数的非因果性与无纠缠量子非定域性之间存在精确的镜像关系，这一发现为理解时间旅行场景下的因果结构提供了新的视角，并建立了经典信息论与量子信息论之间的深刻联系。

Abstract: Closed timelike curves (CTCs) challenge our conception of causality by allowing information to loop back into its own past. Any consistent description of such scenarios must avoid time-travel paradoxes while respecting the no-new-physics principle, which requires that the set of operations available within any local spacetime region remain unchanged, irrespective of whether CTCs exist elsewhere. Within an information-theoretic framework, this leads to process functions: deterministic classical communication structures that remain logically consistent under arbitrary local operations, yet can exhibit correlations incompatible with any definite causal order - a phenomenon known as non-causality. In this work, we provide the first complete recursive characterization of process functions and of (non-)causal process functions. We use it to establish a correspondence between process functions and unambiguous complete product bases, i.e., product bases in which every local state belongs to a unique local basis. This equivalence implies that non-causality of process functions is exactly mirrored by quantum nonlocality without entanglement (QNLWE) - the impossibility of perfectly distinguishing separable states using local operations and causal classical communication - for such bases. Our results generalize previous special cases to arbitrary local dimensions and any number of parties, enable systematic constructions of non-causal process functions and unambiguous QNLWE bases, and reveal an unexpected connection between certain non-signaling inequalities and causal inequalities.

</details>


### [265] [Heisenberg-limited metrology from the quantum-quench dynamics of an anisotropic ferromagnet](https://arxiv.org/abs/2512.23606)
*Z. M. McIntyre,Ji Zou,Jelena Klinovaja,Daniel Loss*

Main category: quant-ph

TL;DR: 利用量子比特调控各向异性铁磁体量子淬火，实现基于量子比特测量的海森堡极限参数估计，该协议依赖磁振子挤压态的量子关联特性。


<details>
  <summary>Details</summary>
Motivation: 量子磁振子学旨在理解和利用磁体中磁振子的量子特性。挤压磁振子态作为各向异性磁体的平衡基态，代表一类重要的非经典磁振子态。研究如何利用这种量子关联特性进行精密测量。

Method: 提出一种基于量子比特调控的量子淬火协议：通过量子比特调控各向异性铁磁体，仅通过测量量子比特实现参数估计。该协议利用磁振子-量子比特耦合系统的本征模式频率信息。

Result: 在存在基态挤压的情况下，该协议能够获得耦合系统本征模式频率的信息；而在没有挤压的情况下则无法获得任何信息。这证明了协议依赖于磁振子挤压态的量子关联特性。

Conclusion: 该协议成功利用了磁振子挤压态的量子关联特性，同时依赖于这种挤压的平衡特性——这是磁性系统特有的特征，为基于磁振子挤压的量子计量学提供了新途径。

Abstract: The emerging field of quantum magnonics seeks to understand and harness the quantum properties of magnons -- quantized collective spin excitations in magnets. Squeezed magnon states arise naturally as the equilibrium ground states of anisotropic ferromagnets and antiferromagnets, representing an important class of nonclassical magnon states. In this work, we show how a qubit-conditioned quantum quench of an anisotropic ferromagnet can be used for Heisenberg-limited parameter estimation based on measurements of the qubit only. In the presence of ground-state squeezing, the protocol yields information about the eigenmode frequency of the coupled magnon-qubit system, whereas no information is gained in the absence of such squeezing. The protocol therefore leverages genuine quantum correlations in the form of magnonic squeezing while simultaneously relying on the equilibrium character of this squeezing -- a feature distinctive to magnetic systems.

</details>


### [266] [Research Directions in Quantum Computer Cybersecurity](https://arxiv.org/abs/2512.23607)
*Jakub Szefer*

Main category: quant-ph

TL;DR: 本文档概述了当前量子计算机网络安全的主要研究方向，总结了近五年的学术工作，旨在为研究人员、政府和行业领导者提供安全威胁和防御措施的概览。


<details>
  <summary>Details</summary>
Motivation: 本文档的动机是提供一个简洁的量子计算机网络安全研究概览，而非全面的调查。它受到第三届量子计算机网络安全研讨会的启发，但超越了研讨会内容，总结了近五年学术界的研究工作，旨在帮助研究人员、政府和行业领导者了解当前的安全威胁和防御措施。

Method: 本文档采用综述性方法，基于第三届量子计算机网络安全研讨会的演讲和讨论，结合近五年学术界的量子计算机网络安全研究，进行高层面的总结和分析。

Result: 文档提供了量子计算机网络安全的当前研究格局概览，包括安全威胁、防御措施、研究趋势以及需要填补的研究空白。

Conclusion: 本文档为量子计算机网络安全领域的研究人员、政府和行业领导者提供了有价值的概览，指出了当前的研究趋势和未来需要填补的研究空白，有助于指导未来的资金投入和学术/行业研究。

Abstract: This document presents a concise overview of the contemporary research directions in quantum computer cybersecurity. The aim of this document is not to be a survey, but rather a succinct summary of the major research directions in quantum computer cybersecurity at the end of the first half of the current decade. The document has been inspired by the presentations and discussions held at the 3$^{rd}$ Quantum Computer Cybersecurity Symposium, but goes beyond the contents of the symposium and aims to summarize at the high level the last five years of quantum computer cybersecurity work in academia. It is hoped that the document can provide researchers as well as government and industry leaders an overview of the current landscape of security threats and defenses against emergent quantum computing technologies. The document also includes a discussion of the current trends in cybersecurity research on quantum computers, and the perceived research gaps that should be filled with future funding and through academic and industry~research.

</details>


### [267] [Gauge-Invariant Phase Mapping to Intensity Lobes of Structured Light via Closed-Loop Atomic Dark States](https://arxiv.org/abs/2512.23642)
*Nayan Sharma,Ajay Tripathi*

Main category: quant-ph

TL;DR: 该论文提出了一种分析模型，展示三能级闭合环路原子系统中规范不变的环路相位如何烙印在拉盖尔高斯探测光束的亮暗瓣强度图案中，为测量几何相位提供了新平台。


<details>
  <summary>Details</summary>
Motivation: 研究闭合环路系统中几何相位（特别是贝里相位）的测量方法，利用结构光在量子光学系统中创建理想的测试平台，解决传统开放系统无法观测的相位效应。

Method: 建立分析模型研究三能级闭合环路原子系统，在弱探测极限下分析输出强度，包括比尔-朗伯吸收、散射项和环路相位依赖的干涉项，利用光学深度控制可见度。

Result: 模型显示环路相位在拉盖尔高斯光束强度图案中形成亮暗瓣，贝里相位表现为暗态在环形参数空间中绝热遍历时获得的几何完整，以条纹移动形式显现，这在开放系统中不存在。

Conclusion: 闭合环路系统中的结构光为量子光学中的几何相位提供了理想的测试平台，使用冷原子或固态平台进行实验实现是可行的，能够映射任意相位并通过干涉旋转测量贝里相位。

Abstract: We present an analytical model showing how the gauge-invariant loop phase in a three-level closed-loop atomic system imprints as bright-dark lobes in Laguerre Gaussian probe beam intensity patterns. In the weak probe limit, the output intensity in such systems include Beer-Lambert absorption, a scattering term and loop phase dependent interference term with optical depth controlling visibility. These systems enable mapping of arbitrary phases via interference rotation and offer a platform to measure Berry phase. Berry phase emerge as a geometric holonomy acquired by the dark states during adiabatic traversal of LG phase defined in a toroidal parameter space. Manifesting as fringe shifts which are absent in open systems, experimental realization using cold atoms or solid state platforms appears feasible, positioning structured light in closed-loop systems as ideal testbeds for geometric phases in quantum optics.

</details>


### [268] [Quantum Geometric Bounds in Non-Hermitian Systems](https://arxiv.org/abs/2512.23708)
*Milosz Matraszek,Wojciech J. Jankowski,Jan Behrends*

Main category: quant-ph

TL;DR: 该论文发现了非厄米系统中可观测量（如量子几何张量、响应函数、电导率张量等）的量子几何界限，并在具有非厄米陈数的拓扑系统中进行了验证，这些界限在开放量子系统的Lindbladian动力学中自然出现。


<details>
  <summary>Details</summary>
Motivation: 研究非厄米系统中可观测量和响应函数的量子几何界限，这些界限在现实实验设置中（超越理想化封闭系统描述）具有重要意义。

Method: 识别非厄米系统的量子几何界限，包括非厄米量子几何张量、广义两点响应关联函数、电导率张量和光学权重的独特界限。在具有非厄米陈数的拓扑系统中展示这些发现，并证明这些几何约束在由非平衡Lindbladian动力学支配的开放量子系统中自然出现。

Result: 发现了非厄米系统中可观测量和响应函数的量子几何界限，这些界限在拓扑系统中得到验证，并在开放量子系统的Lindbladian动力学中自然出现。

Conclusion: 非厄米几何约束对响应函数的限制在现实实验设置中具有重要意义，超越了理想化封闭系统描述，对实验观测和响应具有实际相关性。

Abstract: We identify quantum geometric bounds for observables in non-Hermitian systems. We find unique bounds on non-Hermitian quantum geometric tensors, generalized two-point response correlators, conductivity tensors, and optical weights. We showcase these findings in topological systems with non-Hermitian Chern numbers. We demonstrate that the non-Hermitian geometric constraints on response functions naturally arise in open quantum systems governed by out-of-equilibrium Lindbladian dynamics. Our findings are relevant to experimental observables and responses under the realistic setups that fall beyond the idealized closed-system descriptions.

</details>
