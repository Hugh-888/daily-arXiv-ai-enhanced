<div id=toc></div>

# Table of Contents

- [gr-qc](#gr-qc) [Total: 13]
- [quant-ph](#quant-ph) [Total: 71]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [cs.LG](#cs.LG) [Total: 68]


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [1] [Multibanded Reduced Order Quadrature Techniques for Gravitational Wave Inference](https://arxiv.org/abs/2601.09819)
*Murdoc Newell,Alexis Boudon,Hong Qi*

Main category: gr-qc

TL;DR: 提出基于PyROQ的改进ROQ基构建策略，使用多波段波形加速基搜索，在亚太阳质量范围内构建ROQ基，减少基大小20-30%，构建时间降低约10倍


<details>
  <summary>Details</summary>
Motivation: 传统ROQ基构建方法计算成本高，尤其对于长持续时间信号，需要更高效的构建策略

Method: 基于PyROQ的改进策略，使用多波段波形进行基搜索，采用IMRPhenomXAS_NRTidalV3波形在亚太阳质量范围内构建ROQ基

Result: 基大小减少20-30%，基构建时间降低约10倍，同时保持似然精度和参数估计结果的一致性

Conclusion: 改进的ROQ基构建方法显著提高了构建效率，减少了基大小，同时保持了计算精度，适用于引力波天文学中的参数估计加速

Abstract: Reduced-order quadrature (ROQ) is commonly used to speed up parameter estimation in gravitational wave astronomy; however, the construction of ROQ bases can be computationally costly, particularly for longer duration signals. We propose a modified construction strategy based on PyROQ that accelerates this process by performing the basis search using multiband waveforms, without compromising the desired likelihood speed and accuracy. We use this altered method to construct a set of ROQs in the sub-solar mass range using the \texttt{IMRPhenomXAS\_NRTidalV3} waveform. We find a 20\% to 30\% decrease in basis size and a $\sim 10$ times decrease in basis construction time. We verify the altered method preserves the likelihood accuracy and mantains consitent parameter estimation results.

</details>


### [2] [Quantum Optical Inspired Models for Unitary Black Hole Evaporation](https://arxiv.org/abs/2601.09820)
*Paul M. Alsing*

Main category: gr-qc

TL;DR: 该论文提出了一种光学启发的模型来描述幺正黑洞蒸发，使用高斯态和光学元件（分束器、压缩器）来模拟黑洞与霍金辐射的相互作用，旨在保持霍金辐射的热性质并重现Page曲线。


<details>
  <summary>Details</summary>
Motivation: 建立操作简单、能保持霍金辐射热性质、并能重现Page曲线的幺正黑洞蒸发模型。Page曲线表明当黑洞蒸发到约一半初始质量时，信息开始从黑洞流出。

Method: 将黑洞建模为单模压缩态，通过分束器和压缩器与视界附近的真空模式连续相互作用，产生代表外部霍金辐射和视界内伙伴粒子的纠缠对。使用辛形式体系跟踪复合系统的演化，通过正交算符的均值和方差计算相关性和纠缠。

Result: 由于所有态和操作都是高斯的，该模型能够轻松计算黑洞与霍金辐射之间的相关性和纠缠，以及黑洞早期和晚期状态之间的相关性。

Conclusion: 该光学启发模型为幺正黑洞蒸发提供了一个操作简单、保持热性质、并能重现Page曲线的理论框架，使用高斯态和辛形式体系实现了对黑洞蒸发过程的系统分析。

Abstract: In this work, we describe optically inspired models for unitary black hole (BH) evaporation. The goal of these models are (i) to be operationally simple, (ii) approximately preserve the thermal nature of the emitted Hawking Radiation (HR), and (iii) attempt to reproduce the Page Curve that purports that information flows forth from the BH when it has evaporated to approximately half its initial mass. We concentrate on modeling the BH as a single mode squeezed state successively interacting, by means of beam splitters and squeezers, with vacuum modes near the horizon, giving rise to entangled pairs representing the external Hawking radiation and its partner particle inside the horizon. Since all states and operations are Gaussian throughout, we use a symplectic formalism to track the evolution of the composite system through the evolving means and variances of their quadrature operators. This allows us to easily compute correlations and entanglement between the BH and the HR, as well as calculate correlations between the BH at early and late times.

</details>


### [3] [Non-Monotonic Enhancement of the Magnetic Penrose Process in Kerr-Bertotti-Robinson Spacetime and its Implication for Electron Acceleration](https://arxiv.org/abs/2601.09919)
*Mirjavoxir Mirkhaydarov,Tursunali Xamidov,Pankaj Sheoran,Sanjar Shaymatov,Hemwati Nandan*

Main category: gr-qc

TL;DR: 研究Kerr-Bertotti-Robinson时空中带电粒子的磁Penrose过程，发现背景磁场B对时空结构有非单调影响，能量提取效率随B先增后减，并将模型应用于SgrA*黑洞的电子加速。


<details>
  <summary>Details</summary>
Motivation: 研究在均匀电磁场背景下的旋转黑洞（KBR时空）中，磁场如何影响磁Penrose过程（MPP）的能量提取效率，探索时空结构对磁场的非单调响应特性。

Method: 分析KBR时空中带电粒子的运动行为，研究事件视界和静态极限表面对磁场B的非单调响应，计算能量提取效率，并建立中子β衰变模型来估计电子从黑洞能层逃逸的最大能量。

Result: 发现时空结构对磁场B有非单调响应：事件视界和静态极限表面随B增加先扩大后收缩；能量提取效率也呈现先升高后降低的非单调趋势。应用于SgrA*黑洞，电子可加速至~10^15 eV，但辐射损失会将其降至TeV尺度。

Conclusion: KBR时空中的磁Penrose过程展现出与通常Kerr几何不同的非单调行为，磁场对能量提取效率有最优值。该机制可为银河系中心黑洞SgrA*产生高能电子提供解释，辐射损失使其能量降至观测到的TeV范围。

Abstract: We studied the magnetic Penrose process (MPP) in the Kerr-Bertotti-Robinson (KBR) spacetime, an exact rotating electrovacuum solution describing a black hole (BH) immersed in an intrinsic, uniform electromagnetic field. We analyze the behavior of charged particles in this geometry and find that the spacetime structure itself responds non-monotonically to the background magnetic field $B$. Specifically, both the event horizon and the static limit surface first expand as $B$ increases, reach a maximum size at an intermediate field strength, and then contract toward the extremal limit. Although the ergoregion itself shrinks monotonically with $B$, this structural feature gives rise to a pronounced non-monotonic dependence of the energy extraction efficiency on the magnetic field $B$, i.e., the efficiency initially rises, attains a maximum value, and subsequently falls as the extremal condition is approached. This contrasts sharply with the monotonic trends usually associated with magnetic enhancements in the Kerr geometry. We further explore an astrophysical application of the MPP by estimating the maximum energy of electrons escaping from the ergoregion of the KBR BH. Modeling neutron beta decay occurring near the event horizon, we derive an analytical expression for the energy gained by electrons accelerated by the magnetic field. Applying our results to the supermassive BH at the Galactic center, $\mathrm{SgrA}^*$, we find that electrons can be accelerated up to energies of $\sim 10^{15}\,\mathrm{eV}$ for realistic values of the spin and magnetic field. Although these energies exceed the observed upper range of cosmic-ray electrons, radiative losses such as synchrotron emission and inverse-Compton scattering can efficiently reduce them to the observed $\mathrm{TeV}$ scale.

</details>


### [4] [Hubble Tension and Dark Energy in Teleparallel Gauss-Bonnet Gravity: New Constraints from DESI BAO, Pantheon$^+$ and Hubble Data](https://arxiv.org/abs/2601.10127)
*Santosh V. Lohakare,S. K. Maurya,Aaisha Al Qassabi,B. Mishra*

Main category: gr-qc

TL;DR: 该论文研究了基于挠率标量T和挠率型高斯-博内不变量T_G的f(T, T_G)引力模型，通过数值求解和贝叶斯分析，证明该模型能在没有宇宙学常数的情况下模拟暗能量，部分缓解哈勃张力，为ΛCDM模型提供了可行的替代方案。


<details>
  <summary>Details</summary>
Motivation: 探索f(T, T_G)引力模型作为ΛCDM范式的替代方案，旨在解决宇宙加速膨胀问题，同时缓解哈勃张力等基本宇宙学难题，而不需要引入宇宙学常数。

Method: 推导平坦FLRW宇宙的修正弗里德曼方程和线性标量扰动方程，采用数值方法求解无压物质情况下的哈勃参数演化，结合宇宙计时器、Pantheon+与SH0ES、DESI BAO等晚期观测数据进行贝叶斯马尔可夫链蒙特卡洛分析。

Result: f(T, T_G)模型能在没有宇宙学常数的情况下模拟暗能量，有效状态方程ω_eff(z=0) ≈ -0.664到-0.693，哈勃常数估计值H_0为69-71.5 km/s/Mpc，部分缓解哈勃张力，标量扰动分析确认了稳定性。

Conclusion: f(T, T_G)引力为宇宙加速膨胀问题提供了稳健的理论框架，能够与晚期观测数据一致，是ΛCDM范式的可行替代方案，有望解决基本宇宙学难题。

Abstract: We explore the cosmological dynamics of a teleparallel Gauss-Bonnet gravity model defined by the torsion scalar $T$ and the torsion-based Gauss-Bonnet invariant $T_{\mathcal{G}}$, deriving modified Friedmann equations for a flat FLRW Universe and corresponding linear scalar perturbation equations. Using a numerical approach, we solve these equations for pressureless matter, predicting the redshift evolution of the Hubble parameter $H(z)$. Bayesian Markov chain Monte Carlo analysis, incorporating late-time observations from Cosmic Chronometers, Pantheon$^+$ with SH0ES, and DESI BAO (Data Release 1 and Data Release 2), constrains the model parameters, revealing that $f(T, T_{\mathcal{G}})$ mimics dark energy in the absence of a cosmological constant, presenting a viable alternative to $Λ$CDM paradigm. Stability is confirmed via scalar perturbation analysis of Hubble and matter density fluctuations, positioning $f(T, T_{\mathcal{G}})$ gravity as a robust framework to address cosmic acceleration challenges. The model yields a present-day effective equation of state $ω_{\mathrm{eff}}(z=0) \approx -0.664$ to \(-0.693\), consistent with observations, and partially alleviates the Hubble tension with $H_0$ estimates of 69 to 71.5\kms. These findings highlight the potential of $f(T, T_{\mathcal{G}})$ gravity to resolve fundamental cosmological puzzles while aligning with late-time observational data.

</details>


### [5] [Gravitational lensing beyond the eikonal approximation](https://arxiv.org/abs/2601.10239)
*Emma Bruyère,Cyril Pitrou*

Main category: gr-qc

TL;DR: 研究引力透镜中的波光学效应：当波长与透镜尺度可比时，标量波在弯曲时空中传播会出现超越几何光学的修正，这些修正依赖于背景物质分布。


<details>
  <summary>Details</summary>
Motivation: 当波的波长与引力透镜尺度可比时，会出现波光学效应，这些效应在几何光学近似下被忽略。需要研究标量波在弯曲时空中的传播，以理解振幅和相位的修正，而不考虑电磁波和引力波的偏振问题。

Method: 使用Newman-Penrose形式体系，在逆频率展开中获得了超越几何光学的第一阶修正。在真空中（Weyl张量透镜）和物质密度非零的情况下分别分析修正的阶数。通过数值求解修正方程验证解析结果，在史瓦西黑洞附近和透明恒星中进行了数值模拟。

Result: 在真空中（Weyl张量透镜），波效应在G的一阶为零，从G²阶开始出现。相反，如果波通过非零物质密度传播，第一阶修正从G阶开始。数值模拟验证了这些解析结果。

Conclusion: 引力透镜中的波光学效应显著依赖于背景物质分布：真空中的效应从二阶引力耦合开始，而通过物质传播时一阶效应就存在。这为引力透镜观测中的波光学效应提供了理论框架。

Abstract: Waves propagating through a gravitational potential exhibit wave-optics effects when their wavelength is not significantly smaller than the lensing scales. We study the propagation of a scalar wave, governed by the Klein-Gordon equation in curved spacetime, to focus on effects on amplitude and phase, while leaving aside the issue of wave polarization which affects electromagnetic and gravitational waves. Using the Newman-Penrose formalism, we obtain the first corrections beyond the geometric optics in the expansion in the inverse frequency. In vacuum, that is for Weyl tensor lensing, there is no wave effect at first order in $G$ and wave effects start at order $G^2$. Conversely, if the wave travels through a non-vanishing matter density, the first corrections start at order $G$. We check these analytic results by solving numerically the equations dictating the evolution of the corrections either in the vicinity of a Schwarzschild black hole or through a transparent star.

</details>


### [6] [Effects of spontaneous Lorentz Symmetry breaking on Letelier-AdS charged black boles within Kalb-Ramond gravity](https://arxiv.org/abs/2601.10303)
*Faizuddin Ahmed,Ahmad Al-Badawi,İzzet Sakallı*

Main category: gr-qc

TL;DR: 研究带电AdS黑洞在弦云和Kalb-Ramond场中的光子测地线运动、光子球、黑洞阴影，并用EHT观测数据约束参数，同时分析中性测试粒子的动力学和QPO频率。


<details>
  <summary>Details</summary>
Motivation: 探索弦云和Kalb-Ramond场如何影响带电AdS黑洞的时空几何，研究这些额外场对光子运动、黑洞阴影以及测试粒子动力学的影响，为引力波和黑洞成像观测提供理论依据。

Method: 分析光子有效势、光子球位置和有效径向力；研究黑洞阴影几何；使用EHT对M87*和Sgr A*的观测数据约束弦云参数α和KR场参数ℓ；分析中性测试粒子动力学和准周期振荡频率。

Result: 弦云参数α和KR场参数ℓ显著影响光子球位置、黑洞阴影形状以及测试粒子动力学；这些参数可以通过EHT观测数据约束；额外场改变了标准带电黑洞场景的动力学行为。

Conclusion: 弦云和Kalb-Ramond场为AdS黑洞时空带来了丰富的结构，对光子运动、黑洞阴影和测试粒子动力学产生可观测影响，在引力波和黑洞成像天文学中具有潜在观测意义。

Abstract: In this study, we investigate the geodesic motion of massless particles -- specifically photons -- in the spacetime of a charged anti-de Sitter (AdS) black hole (BH) surrounded by a cloud of strings (CoS) within the framework of Kalb-Ramond (KR) gravity. We analyze the effective potential that governs photon trajectories, explore the properties and location of the photon sphere (PS), and examine the effective radial force acting on photons. The resulting BH shadow is also studied, highlighting the roles of both the CoS parameter $α$ and the KR field parameter $\ell$ in shaping its geometry. We constrain these parameters using observational data from M87* and Sgr A* obtained by the Event Horizon Telescope (EHT). Furthermore, we extend our investigation to the motion of neutral test particles in the same gravitational background. By examining the impact of the CoS and KR field, we show how these additional fields modify the dynamics relative to standard charged BH scenarios. Finally, we study the fundamental frequencies associated with quasiperiodic oscillations (QPOs) of test particles, demonstrating how these frequencies are affected by the presence of the CoS and KR field. Our results reveal the rich structure of AdS-BH spacetimes influenced by string clouds and antisymmetric tensor fields, with potential observational consequences in gravitational wave and BH imaging astronomy.

</details>


### [7] [Distinguishing Quantum Matter by Gravity with Differential Scattering Cross Section at Tree Level](https://arxiv.org/abs/2601.10339)
*Xue-Nan Chen*

Main category: gr-qc

TL;DR: 本文提出了基于微分散射截面的量子弱等效原理，指出该原理会被量子物质的旋量性质破坏，并分析了非相对论和相对论情况下不同自旋性质粒子散射截面的差异。


<details>
  <summary>Details</summary>
Motivation: 量子物质的弱等效原理定义目前是一个开放问题。为了在量子版本的弱等效原理中反映量子系统的概率特性，需要提出一个能够体现量子特性的等效原理表述。

Method: 提出基于树图水平微分散射截面的量子弱等效原理：当靶粒子取大质量极限时，微分散射截面不依赖于散射粒子的质量和性质。研究了该原理被量子物质自旋性质破坏的情况，分析了非相对论和相对论情况下不同自旋性质粒子的散射截面差异。

Result: 量子弱等效原理会被量子物质的自旋性质破坏。非相对论情况下，不同自旋性质散射粒子的微分散射截面差异主要体现为 $\mathcal O(p_{\mathrm{cm}}^2)$ 阶；相对论情况下，靶粒子为标量粒子时差异主要体现为 $\mathcal O(1/θ^2)$ 阶，靶粒子为狄拉克粒子时差异主要体现为 $\mathcal O(1/θ^4)$ 阶。还研究了狄拉克粒子散射截面的极化特性。

Conclusion: 基于微分散射截面的量子弱等效原理为量子引力研究提供了新视角，但该原理会受到量子物质自旋性质的破坏，表明量子效应在弱等效原理中起着重要作用。

Abstract: The definition of weak equivalence principle of quantum matter is an open problem at present. In order to reflect the probability of quantum system in the quantum version of weak equivalence principle, we proposed a quantum weak equivalence principle based on differential scattering cross section at tree level, that is, the differential scattering cross section does not depend on the mass and properties of the scattered particles when the target particles take the large mass limit. This version of the quantum equivalence principle we proposed will be broken by the spin properties of quantum matter. In the non-relativistic case, the difference of differential scattering cross sections of scattered particles with different spin properties scattered by target particles is mainly reflected in the order of $ \mathcal O (p _ {\mathrm{cm}} ^2) $. In the relativistic case , we studied the asymptotic behavior of differential scattering cross sections at small angles. When the target particles are scalar particles, the difference of light particles with different spin properties is mainly reflected in the $ \mathcal O (1/θ^2) $ order. When the target particles are Dirac particles, the difference of light particles with different spin properties is mainly reflected in the $ \mathcal O (1/θ^4) $ order. The polarization of differential scattering cross section when scattered particles are Dirac particles is investigated. The result of the degree of polarization depends on the polarization direction of the incident particles.

</details>


### [8] [Analyzing intermittent stochastic gravitational wave background I:Effect of detector response](https://arxiv.org/abs/2601.10428)
*Xiaolin Liu,Sachiko Kuroyanagi*

Main category: gr-qc

TL;DR: 研究提出了一种改进的非高斯引力波背景检测方法，通过考虑探测器天线方向图的完整积分而非平均响应，解决了参数估计偏差问题。


<details>
  <summary>Details</summary>
Motivation: 随着引力波探测数量的增加，特别是双黑洞合并事件，预计在不久的将来会观测到由大量微弱高红移事件形成的天体物理背景。这个背景预计会表现出非高斯统计特性，需要开发鲁棒的检测方法。

Method: 基于高斯混合似然模型重新审视最优检测策略，提出考虑完整探测器天线方向图而非平均响应的方法。通过引入二阶修正作为源分布全积分的近似，开发了计算上可行的方法。

Result: 模拟显示，使用平均响应会引入显著的参数估计偏差，而提出的方法能有效消除这些偏差。即使在考虑各向异性背景时，该方法仍保持鲁棒性。

Conclusion: 正确考虑探测器天线方向图对于非高斯引力波背景检测至关重要。提出的二阶修正方法在计算可行的情况下能有效消除参数估计偏差，为未来引力波背景探测提供了改进工具。

Abstract: With the growing number of gravitational-wave detections, particularly from binary black hole mergers, there is increasing anticipation that an astrophysical background, formed by an ensemble of faint, high-redshift events, will be observed in the near future by the ground-based detector network. This background is anticipated to exhibit non-Gaussian statistical properties. To develop a robust method for detecting such a non-Gaussian gravitational-wave background, we revisit optimal detection strategies based on the Gaussian-mixture likelihood model. In this work, we demonstrate that properly accounting for the detector antenna pattern is essential. Current approaches typically rely on the overlap reduction function averaged over the sky. Through simulations, we show that using such an averaged response introduces significant biases in parameter estimation. In addition, we propose a computationally feasible method that incorporates second-order corrections as an approximation of the full integral over the source distribution. Our results indicate that this approach effectively eliminates these biases. We also show that our method remains robust even when considering anisotropic backgrounds.

</details>


### [9] [Charged Simpson-Visser AdS Black Holes: Geodesic Structure and Thermodynamic Properties](https://arxiv.org/abs/2601.10469)
*Faizuddin Ahmed,Ahmad Al-Badawi,Mohsen Fathi*

Main category: gr-qc

TL;DR: 将Simpson-Visser正则化方案应用于AdS带电黑洞，研究正则化后的时空几何、测地线结构和热力学行为，并与EHT观测数据对比。


<details>
  <summary>Details</summary>
Motivation: 研究正则化参数如何影响带电AdS黑洞的物理可观测量，包括粒子动力学和热力学性质，并与实际观测数据（M87*和Sgr A*）进行验证。

Method: 应用Simpson-Visser正则化方案到AdS带电黑洞，分析测地线结构（光子球、黑洞阴影、光子轨迹、带电粒子轨道）和热力学变量（霍金温度、吉布斯自由能、比热容）。

Result: 正则化参数显著影响黑洞的测地线结构和热力学性质，理论预测与EHT对M87*和Sgr A*的观测结果一致，揭示了正则化效应与物理可观测量之间的相互作用。

Conclusion: Simpson-Visser正则化方案为研究带电AdS黑洞提供了新视角，正则化参数在决定黑洞动力学行为和热力学稳定性方面起关键作用，与观测数据的一致性验证了该模型的有效性。

Abstract: In this article, we apply the Simpson-Visser (SV) regularization scheme to Anti-de Sitter (AdS) charged black holes and investigate the resulting spacetime geometry in detail, with emphasis on both geodesic structure and thermodynamic behavior. In particular, we analyze the motion of massless particle, focusing on key features such as the photon sphere, black hole shadow, photon trajectory and the dynamics of charged particles, including the characteristics of the circular and type of orbits. Furthermore, we compare the theoretical predictions of the charged SV-AdS black hole with recent observations reported by the Event Horizon telescope (EHT) for M87* and Sgr~A*. Beyond the geodesic analysis, we explore the thermodynamics of the regularized charged SV-AdS black hole by deriving essential quantities such as the Hawking temperature, Gibbs free energy, and specific heat capacity. Through a systematic examination of these thermodynamic variables, we demonstrate how the regularization parameter inherent in the SV regularization influences particle dynamics, stability conditions, and the overall thermal properties of the modified black hole solution. This comprehensive study highlights the interplay between regularization effects and the physical observables associated with charged AdS black holes.

</details>


### [10] [The emergence of our Universe](https://arxiv.org/abs/2601.10499)
*Jan Ambjorn,Yoshiyuki Watabiki*

Main category: gr-qc

TL;DR: 论文提出宇宙可以从多分量W3代数的对称性破缺中产生，其中分量构成Jordan代数，通过复数和八元数上的Jordan代数对称性破缺可得到扩展的四维时空，其膨胀由修正的Friedmann方程描述。


<details>
  <summary>Details</summary>
Motivation: 探索宇宙起源的新理论框架，试图从代数结构（W3代数和Jordan代数）的对称性破缺来解释宇宙的诞生和演化，为一些令人困惑的宇宙学观测提供解释。

Method: 采用多分量W3代数的对称性破缺理论，其中分量构成Jordan代数。特别关注复数和八元数上的Jordan代数H3(C)和H3(O)，通过它们的对称性破缺推导出扩展的四维时空结构。

Result: 得到修正的Friedmann方程来描述宇宙膨胀，该方程可能解释一系列令人困惑的宇宙学观测现象。

Conclusion: 宇宙可以从代数结构的对称性破缺中产生，修正的Friedmann方程为解释宇宙学观测提供了新的理论框架。

Abstract: We show how our Universe can emerge from a symmetry breaking of a multicomponent $W_3$ algebra, where the components in addition form a Jordan algebra. We discuss how symmetry breaking related to the Jordan algebras $H_3(C)$ and $H_3(O)$ over the complex and octonion numbers can lead to an extended four-dimensional spacetime, where the expansion of the Universe is governed by a modified Friedmann equation. We finally discuss how this modified Friedmann equation might explain a number of puzzling cosmological observations.

</details>


### [11] [Numerical simulations of oscillating and differentially rotating neutron stars](https://arxiv.org/abs/2601.10550)
*Santiago Jaraba,Jérôme Novak,Micaela Oertel*

Main category: gr-qc

TL;DR: 扩展ROXAS代码以模拟具有微分旋转的中子星振荡，首次在共形平坦度下提供非轴对称模式的频率值


<details>
  <summary>Details</summary>
Motivation: 双中子星合并后的残骸预期是快速旋转的大质量恒星，其振荡会产生千赫兹频段的引力波。微分旋转程度和旋转剖面强烈影响其结构、稳定性和振荡谱，因此在建模时必须考虑这些因素。

Method: 扩展伪谱代码ROXAS，使其能够演化振荡的微分旋转中子星。基于原始变量和共形平坦度近似，将先前形式扩展到微分旋转。运行一系列轴对称和非轴对称的扰动微分旋转中子星模拟，提取其振荡频率。

Result: 轴对称模式以及Cowling近似下的模式与已发表结果高度一致。发现Cowling近似下的次级基本模式是人为产物，在动态时空下不出现。首次提供了在共形平坦度下演化的微分旋转配置中非轴对称模式的频率值。

Conclusion: 这一扩展拓宽了ROXAS可研究的物理场景范围，代表了向更真实地建模合并后残骸及其引力波辐射迈出的重要一步。

Abstract: The remnants of binary neutron star mergers are expected to be massive, rapidly rotating stars whose oscillations produce gravitational waves in the kilohertz band. The degree of differential rotation and the rotation profiles strongly influence their structure, stability and oscillation spectrum, and must therefore be taken into account when modeling their dynamics. We extend the pseudospectral code ROXAS (Relativistic Oscillations of non-aXisymmetric neutron stArS) to enable the dynamical evolution of oscillating, differentially rotating neutron stars. Using the updated code, we aim to study the star's oscillation frequencies. We extend the previous formalism, based on primitive variables and the conformal flatness approximation, to differential rotation. Within this framework, we run a series of axisymmetric and non-axisymmetric simulations of perturbed, differentially rotating neutron stars with different rotation rates, and extract their oscillation frequencies. Axisymmetric modes, as well as those under the Cowling approximation, show excellent agreement with published results. We show that the secondary fundamental mode in the Cowling approximation is an artifact that does not appear in dynamical spacetimes. In addition, we provide, for the first time, frequency values for non-axisymmetric modes in differentially rotating configurations evolved in conformal flatness. This extension broadens the range of physical scenarios that can be studied with ROXAS, and represents a step toward more realistic modeling of post-merger remnants and their gravitational-wave emission.

</details>


### [12] [Rapid post-merger signal of circularly polarized gravitational wave from magnetic black hole superradiance: novel approach to detect magnetic monopole](https://arxiv.org/abs/2601.10552)
*Zhong-Hao Luo,Fa Peng Huang,Pengming Zhang,Chen Zhang*

Main category: gr-qc

TL;DR: 带磁荷的旋转黑洞对带电标量场的超辐射不稳定性显著增强，产生更强的单色连续引力波，可用于探测磁单极子和超轻玻色子。


<details>
  <summary>Details</summary>
Motivation: 研究带磁荷的旋转黑洞如何增强超辐射不稳定性，探索利用引力波信号探测磁单极子和超轻玻色子的新方法。

Method: 建立解析框架分析带磁荷旋转黑洞的超辐射不稳定性，研究单极子如何降低离心势垒，加深引力束缚态势阱，计算增强的不稳定性增长率。

Result: 磁荷使超辐射不稳定性增强数个量级，产生更强的近单色连续引力波，形成圆偏振信号，可在黑洞合并后数周内探测到增强的mHz频段引力波。

Conclusion: 带磁荷黑洞的超辐射增强效应为探测磁单极子和超轻玻色子提供了新途径，通过快速跟进双黑洞合并后的引力波信号，结合偏振测量可区分磁荷与克尔黑洞场景。

Abstract: We present an analytic framework demonstrating that a spinning black hole endowed with a net magnetic charge exhibits a dramatically amplified superradiant instability against charged scalar fields, enhanced by several orders of magnitude compared with the neutral Kerr case. The amplification arises from a monopole induced reduction of the centrifugal barrier. This shift deepens the gravitational bound-state potential well and produces a parametrically larger instability growth rate. This resulting rapid growth yields a macroscopic boson cloud that acts as a coherent source of near monochromatic continuous gravitational waves (GWs). We find an enhanced GW power. Monopole harmonic selection rules restrict the emission from the north (south) clouds corresponding to opposite helicities. Their superposition generates an (approximately) circularly polarized continuous GWs at a fixed sky location within even parity general relativity, distinct from the generic elliptical polarization of the Kerr case. In light of these new findings, we propose a potential smoking-gun search strategy for magnetic monopole and ultralight boson: the rapid post-merger follow-up GW signals from binary-black-hole merger remnants through ground-based and space-based GW experiments. In contrast to the Kerr case, where the signal turn-on can be delayed to decades-centuries, a magnetic remnant can form a cloud and emit a stronger, circularly polarized continuous GWs within weeks to months. Taking the magnetic supermassive remnants as an example, we demonstrate that the rapid follow-up GW signal in the mHz band appears just in few weeks after binary black hole mergers. Moreover, future polarization (ellipticity) measurements can distinguish the magnetic scenario from Kerr while providing a parity-even mechanism for circularly polarized GWs in general relativity.

</details>


### [13] [Dynamics of Late time cosmology in $f(Q,L_{m})$ Gravity with Constraints from DESI DR2 BAO Data](https://arxiv.org/abs/2601.10627)
*Rajdeep Mazumdar,Kalyan Malakar,Kalyan Bhuyan*

Main category: gr-qc

TL;DR: 该研究在修正引力理论f(Q,L_m)框架下，通过非线性模型f(Q,L_m)=αQ+βL_m^n+λ研究晚期宇宙学，利用最新观测数据约束参数，成功描述宇宙加速膨胀现象。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索修正引力理论f(Q,L_m)作为广义相对论替代方案的可能性，特别是在解释晚期宇宙加速膨胀现象方面。通过引入非线性物质耦合项，试图为宇宙加速提供理论解释。

Method: 采用非线性模型f(Q,L_m)=αQ+βL_m^n+λ，推导修正的Friedmann方程，获得Hubble参数H(z)的解析解。使用DESI DR2 BAO数据、先前BAO汇编(P-BAO)和宇宙计时器(CC)数据集，通过马尔可夫链蒙特卡洛分析约束模型参数。

Result: 模型成功描述观测到的晚期宇宙加速：H_0≈69.5 km/s/Mpc，q_0≈-0.57，加速转变红移z_tr~0.56-0.77。状态诊断器显示模型在DESI数据下类似Chaplygin气体，在P-BAO+CC下类似quintessence主导演化。能量条件和稳定性分析表明模型可行。

Conclusion: 该f(Q,L_m)模型为基于广义相对论的宇宙学提供了可行的替代方案，能够成功描述晚期宇宙加速膨胀现象，并与多种观测数据一致。

Abstract: We investigate late-time cosmology in the context of modified $f(Q,L_m)$ gravity, considering a non-linear model$ f(Q,L_m) = αQ + βL_m^n + λ$ where, $α$, $β$, $λ$, and $n$ are some free parameters. The modified Friedmann equations are derived for a barotropic cosmic fluid, and an analytical solution for the Hubble parameter $H(z)$ is obtained. Using the latest DESI DR2 BAO data, previous BAO compilations (P-BAO), and cosmic chronometer (CC) datasets, we constrain the model parameters through a Markov Chain Monte Carlo analysis. Our results show that the model successfully describes the observed late-time cosmic acceleration with slightly tighter constraints from the inclusion of DESI dataset. The present-day Hubble constant is determined as $H_0 \simeq 69.5\ \mathrm{km\ s^{-1}\ Mpc^{-1}}$, while the deceleration parameter confirms accelerated expansion with $q_0 \simeq -0.57$. The transition redshift, where the universe switches from deceleration to acceleration, occurs in the range $z_{\rm tr} \sim 0.56 - 0.77$. Similarly, a smooth and physically consistent transition from a matter-dominated decelerated period at high redshifts to an accelerated phase at late times is revealed by the evolution of $ω_{eff}(z)$. While statefinder diagnostic shows the model favours a Chaplygin gas like nature for DESI and DESI+CC, whereas the model favours as quintessence dominated evolution for P-BAO+CC in the late time regime. Conclusively, all these results along with the study of the energy conditions and stability analysis showcases the given $f(Q,L_m)$ model offers a viable alternative to GR-based cosmology

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [14] [Limits of Rank Recovery in Bilinear Observation Problems](https://arxiv.org/abs/2601.09754)
*Seungbeom Choi*

Main category: quant-ph

TL;DR: 论文研究双线性观测问题中的秩恢复极限，发现数值细化无法消除的稳定秩平台，揭示了问题结构修改与数值细化的本质区别。


<details>
  <summary>Details</summary>
Motivation: 双线性观测问题在物理和信息论中普遍存在，通常使用基于秩的诊断方法来评估可观测的有效维度。但现有方法隐含假设秩缺陷可以通过数值细化解决，这一假设需要验证。

Method: 直接分析双线性观测算子的秩和零度，通过系统变化容差来研究秩平台现象。将零空间分解为代数扇区，分析其内部结构，并比较数值细化与问题结构修改的效果。

Result: 发现了跨越广泛容差范围的稳定秩平台，表明存在无法通过数值细化消除的维度缺陷。零空间在特定扇区中集中但非独占分布，显示出有组织的内部结构。秩恢复需要改变观测问题的结构本身。

Conclusion: 双线性观测问题中的秩恢复存在根本限制，数值细化无法解决某些维度缺陷。必须区分数值细化（保持问题结构）与问题修改（改变双线性结构），后者才是实现秩恢复的关键。

Abstract: Bilinear observation problems arise in many physical and information-theoretic settings, where observables and states enter multiplicatively. Rank-based diagnostics are commonly used in such problems to assess the effective dimensionality accessible to observation, often under the implicit assumption that rank deficiency can be resolved through numerical refinement. Here we examine this assumption by analyzing the rank and nullity of a bilinear observation operator under systematic tolerance variation. Rather than focusing on a specific reconstruction algorithm, we study the operator directly and identify extended rank plateaus that persist across broad tolerance ranges. These plateaus indicate stable dimensional deficits that are not removed by refinement procedures applied within a fixed problem definition. To investigate the origin of this behavior, we resolve the nullspace into algebraic sectors defined by the block structure of the variables. The nullspace exhibits a pronounced but nonexclusive concentration in specific sectors, revealing an organized internal structure rather than uniform dimensional loss. Comparing refinement with explicit modification of the problem formulation further shows that rank recovery in the reported setting requires a change in the structure of the observation problem itself. Here, "problem modification" refers to changes that alter the bilinear observation structure (e.g., admissible operator/state families or coupling constraints), in contrast to refinements that preserve the original formulation such as tolerance adjustment and numerical reparameterizations. Together, these results delineate limits of rank recovery in bilinear observation problems and clarify the distinction between numerical refinement and problem modification in accessing effective dimensional structure.

</details>


### [15] [Fractional Revival Dynamics in Kerr-Type Systems: Angular Momentum Moments and Classical Analogs](https://arxiv.org/abs/2601.09763)
*Ashish Kumar Patra,Saikumar Krithivasan*

Main category: quant-ph

TL;DR: 该论文研究量子波包复兴和分数复兴现象，重点分析角动量可观测量中的分数复兴动力学，并探讨量子复兴现象在经典系统中的类比。


<details>
  <summary>Details</summary>
Motivation: 虽然波包复兴和分数复兴作为非线性能谱系统中的量子干涉现象已被广泛研究，但作者希望扩展分析范围：一是研究角动量可观测量中的分数复兴动力学，二是探索量子复兴现象在经典系统中的类比，以提供更统一的视角。

Method: 使用Kerr型非线性哈密顿量作为范例模型，分析自相关函数、矩动力学和相空间结构，通过量子地毯等可视化工具支持分析，推导角动量矩的时间演化显式表达式。

Result: 研究发现高阶角动量矩提供了分数复兴的清晰选择性特征，揭示了量子分数复兴与代表性经典系统中回归行为之间的结构相似性，扩展了分数复兴的实验可诊断范围。

Conclusion: 该研究拓宽了分数复兴的实验诊断工具，为量子与经典动力学系统中的复兴现象提供了统一视角，表明高阶角动量矩是检测分数复兴的有效探针，且量子复兴现象在经典系统中存在对应结构。

Abstract: Wave packet revivals and fractional revivals are hallmark quantum interference phenomena that arise in systems with nonlinear energy spectra, and their signatures in expectation values of observables have been studied extensively in earlier work. In this article, we build on these studies and extend the analysis in two important directions. First, we investigate fractional revival dynamics in angular momentum observables, deriving explicit expressions for the time evolution of their moments and demonstrating that higher-order angular momentum moments provide clear and selective signatures of fractional revivals. Second, we examine classical analogs of quantum revival phenomena and elucidate structural similarities between quantum fractional revivals and recurrence behavior in representative classical systems. Using the Kerr-type nonlinear Hamiltonian as a paradigmatic model, we analyze the autocorrelation function, moment dynamics, and phase-space structures, supported by visualizations such as quantum carpets. Our results broaden the range of experimentally accessible diagnostics of fractional revivals and provide a unified perspective on revival phenomena across quantum and classical dynamical systems.

</details>


### [16] [Three questions on the future of quantum science and technology](https://arxiv.org/abs/2601.09769)
*S. Radenkovic,M. Dugic,I. Radojevic*

Main category: quant-ph

TL;DR: 量子科学与技术现状与未来发展综述


<details>
  <summary>Details</summary>
Motivation: 总结量子科学与技术当前发展状况，展望未来发展方向，为相关领域提供参考

Method: 综述性分析，基于现有研究成果和发展趋势进行系统性总结

Result: 全面呈现量子科学与技术当前状态，识别关键技术进展和挑战

Conclusion: 量子科学与技术正处于快速发展阶段，未来将在计算、通信、传感等领域产生革命性影响

Abstract: The answers on the current status and future development of Quantum Science and Technology are presented.

</details>


### [17] [Computing Statistical Properties of Velocity Fields on Current Quantum Hardware](https://arxiv.org/abs/2601.10166)
*Miriam Goldack,Yosi Atia,Ori Alberton,Karl Jansen*

Main category: quant-ph

TL;DR: 提出直接从参数化量子电路提取空间速度场统计特性（中心矩和结构函数）的方法，避免完全量子态层析，并在IBMQ Heron2系统上验证了高精度计算。


<details>
  <summary>Details</summary>
Motivation: 量子算法在计算流体力学中因良好的扩展性受到关注，但量子CFD中模拟结果的高效读出是一个关键挑战，现有文献关注有限。

Method: 开发直接从参数化ansatz电路提取空间速度场统计特性的方法，包括中心矩和结构函数，避免完全量子态层析。在1D速度场上实现，用4个量子比特编码16个空间点，分析正弦波信号和Burgers方程演化的四个快照。

Result: 使用Qedma的误差缓解软件QESEM，在IBMQ的Heron2系统ibm_fez上展示了此类计算在当前量子设备上能达到高精度。

Conclusion: 提出的方法能够高效地从量子CFD模拟中提取统计特性，避免了计算成本高的完全量子态层析，并在当前量子硬件上验证了可行性。

Abstract: Quantum algorithms are gaining attention in Computational Fluid Dynamics (CFD) for their favorable scaling, as encoding physical fields into quantum probability amplitudes enables representation of two to the power of n spatial points with only n qubits. A key challenge in Quantum CFD is the efficient readout of simulation results, a topic that has received limited attention in literature. This work presents methods to extract statistical properties of spatial velocity fields, such as central moments and structure functions, directly from parameterized ansatz circuits, avoiding full quantum state tomography. As a proof of concept, we implement our approach for 1D velocity fields, encoding 16 spatial points with 4 qubits, and analyze both a sine wave signal and four snapshots from Burgers' equation evolution. Using Qedma's error mitigation software QESEM, we demonstrate that such computations achieve high accuracy on current quantum devices, specifically IBMQ's Heron2 system ibm_fez.

</details>


### [18] [Hierarchical time crystals](https://arxiv.org/abs/2601.09779)
*Jan Carlo Schumann,Igor Lesanovsky,Parvinder Solanki*

Main category: quant-ph

TL;DR: 论文提出了一种新型的"分层时间晶体"相，通过离散时间晶体和连续时间晶体的耦合，实现了同时的双重时间对称性破缺。


<details>
  <summary>Details</summary>
Motivation: 时间晶体作为自发破缺时间平移对称性的奇异物质相，主要分为离散和连续两类。虽然这两类时间晶体已作为独立系统被广泛研究，但它们之间的相互作用可能产生有趣的新效应。

Method: 构建了一个时间无关的耦合系统，将离散时间晶体和连续时间晶体相互连接。研究了不同耦合方案下系统的动力学行为，并分析了系统参数范围内的稳定性。

Result: 发现耦合系统诱导了同时的双重时间对称性破缺，形成了分层时间晶体相。其中一个子系统破缺了在动力学生成器中不存在的、而是动态涌现的离散时间对称性，形成了复杂的非平衡相。

Conclusion: 分层时间晶体具有鲁棒性，能够在不同的耦合方案中涌现，并在广泛的系统参数范围内持续存在，为研究非平衡相和对称性破缺提供了新的平台。

Abstract: Spontaneous symmetry breaking is one of the central organizing principles in physics. Time crystals have emerged as an exotic phase of matter, spontaneously breaking the time translational symmetry, and are mainly categorized as discrete or continuous. While these distinct types of time crystals have been extensively explored as standalone systems, intriguing effects can arise from their mutual interaction. Here, we demonstrate that a time-independent coupled system of discrete and continuous time crystals induces a simultaneous two-fold temporal symmetry breaking, resulting in a hierarchical time crystal phase. Interestingly, one of the subsystems breaks an emergent discrete temporal symmetry that does not exist in the dynamical generator but rather emerges dynamically, leading to a convoluted non-equilibrium phase. We demonstrate that hierarchical time crystals are robust, emerging for fundamentally different coupling schemes and persisting across wide ranges of system parameters.

</details>


### [19] [Zero-Error List Decoding for Classical-Quantum Channels](https://arxiv.org/abs/2601.09786)
*Marco Dalai,Filippo Girardi,Ludovico Lami*

Main category: quant-ph

TL;DR: 研究纯态经典-量子信道在列表译码下的零错误容量，给出了列表大小为2的可达界和适用于任意固定列表大小的逆界，并在特定条件下两界重合。发现与经典情况不同，球填充界的发散速率可能无法通过零错误列表码实现。


<details>
  <summary>Details</summary>
Motivation: 研究纯态经典-量子信道在列表译码设置下的零错误容量问题。与完全经典设置不同，经典-量子信道在零错误列表编码方面表现出独特的特性，特别是球填充界的发散速率可能无法通过零错误列表码实现，这值得深入研究。

Method: 提供了列表大小为2的可达性界（achievability bound）和适用于任意固定列表大小的逆界（converse bound）。对于状态对绝对重叠形成半正定矩阵的信道，这两个界重合。还讨论了经典-量子情况与完全经典设置的区别。

Result: 对于状态对绝对重叠形成半正定矩阵的信道，可达界和逆界重合。发现了一个显著特性：与完全经典设置不同，球填充界发散时的速率可能无法通过零错误列表码实现，即使考虑固定但任意大的列表大小极限。

Conclusion: 经典-量子信道在零错误列表译码方面表现出与经典信道不同的特性，特别是球填充界的发散速率可能无法通过零错误列表码实现。对于满足特定条件的信道，可达界和逆界能够重合，为这类信道的容量分析提供了重要结果。

Abstract: The aim of this work is to study the zero-error capacity of pure-state classical-quantum channels in the setting of list decoding. We provide an achievability bound for list-size two and a converse bound holding for every fixed list size. The two bounds coincide for channels whose pairwise absolute state overlaps form a positive semi-definite matrix. Finally, we discuss a remarkable peculiarity of the classical-quantum case: differently from the fully classical setting, the rate at which the sphere-packing bound diverges might not be achievable by zero-error list codes, even when we take the limit of fixed but arbitrarily large list size.

</details>


### [20] [Background cancellation for frequency-selective quantum sensing](https://arxiv.org/abs/2601.09792)
*Ricard Puig,Nathan Constantinides,Bharath Hebbe Madhusudhana,Daniel Bowring,C. Huerta Alderete,Andrew T. Sornborger*

Main category: quant-ph

TL;DR: 提出一种基于时间无关相互作用和纠缠的量子传感器，作为被动、可调谐、带阈值的频率滤波器，无需复杂控制即可检测特定频率的弱信号


<details>
  <summary>Details</summary>
Motivation: 量子传感中的关键挑战是检测弱时变信号，特别是那些在背景场中作为特定频率扰动的信号。传统方法通常需要复杂的量子传感器动态控制和繁重的经典后处理。

Method: 利用时间无关相互作用和纠缠构建量子传感器，将其设计为被动、可调谐、带阈值的频率滤波器。通过将频率选择性和阈值行为直接编码到动力学中，使传感器仅对所选目标频率且振幅超过阈值的信号响应。

Result: 该方法绕过了对复杂控制方案的需求，减少了后处理开销，能够有效检测特定频率的弱时变信号。

Conclusion: 提出的量子传感器架构通过将频率选择性和阈值检测直接嵌入量子动力学，为弱信号检测提供了一种简化且高效的方法，降低了传统量子传感的复杂性和计算负担。

Abstract: A key challenge in quantum sensing is the detection of weak time dependent signals, particularly those that arise as specific frequency perturbations over a background field. Conventional methods usually demand complex dynamical control of the quantum sensor and heavy classical post-processing. We propose a quantum sensor that leverages time independent interactions and entanglement to function as a passive, tunable, thresholded frequency filter. By encoding the frequency selectivity and thresholding behavior directly into the dynamics, the sensor is responsive only to a target frequency of choice whose amplitude is above a threshold. This approach circumvents the need for complex control schemes and reduces the post-processing overhead.

</details>


### [21] [Localization of quantum states within subspaces](https://arxiv.org/abs/2601.09817)
*L. L. Salcedo*

Main category: quant-ph

TL;DR: 提出量子态在希尔伯特空间子空间中定位概率的精确定义，识别定位分量，建立数学性质，并讨论量子信息应用


<details>
  <summary>Details</summary>
Motivation: 为量子态在希尔伯特空间特定子空间中的定位概率提供严格的数学定义，解决量子信息理论中的基础问题

Method: 提出精确定义，明确识别量子态的定位分量，建立相关数学性质

Result: 建立了量子态定位概率的数学框架，识别了定位分量，为量子信息应用提供了理论基础

Conclusion: 该工作为量子态在子空间中的定位提供了严格的数学定义和理论框架，在量子信息领域具有重要应用价值

Abstract: A precise definition is proposed for the localization probability of a quantum state within a given subspace of the full Hilbert space of a quantum system. The corresponding localized component of the state is explicitly identified, and several mathematical properties are established. Applications and interpretations in the context of quantum information are also discussed.

</details>


### [22] [Fragmented Topological Excitations in Generalized Hypergraph Product Codes](https://arxiv.org/abs/2601.09850)
*Meng-Yuan Li,Yue Wu*

Main category: quant-ph

TL;DR: 本文研究了一类广义超图乘积码中的分形拓扑序，发现了包括非单调基态简并度和非阿贝尔晶格缺陷在内的有趣性质，并在4D中发现了碎片化拓扑激发。


<details>
  <summary>Details</summary>
Motivation: 产品码构造是构建量子稳定子码的有力工具，而稳定子码与精确可解自旋模型基态之间的自然映射关系，促使研究者探索稳定子码中的多体序。本文旨在研究通过最近提出的通用构造获得的一类码中的分形拓扑序。

Method: 研究一类广义超图乘积(HGP)码，将其对应的精确可解自旋模型称为"正交多面体模型"。通过分析3D和4D正交多面体模型，探索其拓扑性质。

Result: 在3D模型中发现了非单调基态简并度（随系统尺寸变化）和非阿贝尔晶格缺陷。在4D中发现了碎片化拓扑激发：这些激发在实空间中表现为离散的孤立点，但在低维子系统上的投影形成环路等连通对象，揭示了这些激发的内在拓扑性质。

Conclusion: 碎片化激发构成了点状和空间扩展拓扑激发之间的有趣中间类别。广义HGP码为研究分形序物理提供了一个多功能且解析可处理的平台。

Abstract: Product code construction is a powerful tool for constructing quantum stabilizer codes, which serve as a promising paradigm for realizing fault-tolerant quantum computation. Furthermore, the natural mapping between stabilizer codes and the ground states of exactly solvable spin models also motivates the exploration of many-body orders in the stabilizer codes. In this work, we investigate the fracton topological orders in a family of codes obtained by a recently proposed general construction. More specifically, this code family can be regarded as a class of generalized hypergraph product (HGP) codes. We term the corresponding exactly solvable spin models \textit{orthoplex models}, based on the geometry of the stabilizers. In the 3D orthoplex model, we identify a series of intriguing properties within this model family, including non-monotonic ground state degeneracy (GSD) as a function of system size and non-Abelian lattice defects. Most remarkably, in 4D we discover \textit{fragmented topological excitations}: while such excitations manifest as discrete, isolated points in real space, their projections onto lower-dimensional subsystems form connected objects such as loops, revealing the intrinsic topological nature of these excitations. Therefore, fragmented excitations constitute an intriguing intermediate class between point-like and spatially extended topological excitations. In addition, these rich features establish the generalized HGP codes as a versatile and analytically tractable platform for studying the physics of fracton orders.

</details>


### [23] [Multi-level quantum emitter in an optical waveguide: paradoxes and resolutions](https://arxiv.org/abs/2601.09854)
*Ben Lang*

Main category: quant-ph

TL;DR: 研究多能级量子系统与任意局部偏振单模光波导的光学偶极相互作用，发现反直觉现象：非正交量子态可产生相反方向光子流，各向同性发射器可随偏振变化在透射与反射间切换。


<details>
  <summary>Details</summary>
Motivation: 探索量子系统与波导耦合中的反直觉物理现象，特别是看似违反量子力学基本原理的悖论情况，如非正交态产生相反光子流、各向同性发射器的偏振依赖行为等。

Method: 理论分析多能级量子系统与单模光波导的光学偶极相互作用，考虑任意局部偏振条件。通过数学推导和示例（四能级系统）验证理论预测。

Result: 发现：1）存在非正交量子态各自产生相反方向光子流但不违反量子力学幺正性；2）各向同性发射器在零损耗极限下，微小偏振旋转可实现从100%透射到100%反射的转变；3）四能级系统可实现光子数的非破坏性宇称测量。

Conclusion: 量子系统与波导耦合中存在丰富反直觉现象，这些现象在量子力学框架内自洽，为量子信息处理和光子器件设计提供了新思路，特别是偏振控制的光子路由和非破坏性测量方案。

Abstract: We theoretically investigate the optical dipole interaction between a multi-level quantum system and a single-mode optical waveguide of any local polarisation. We investigate several paradoxical seeming situations, for example we find a situation in which there exist two non-orthogonal quantum states, each of which results in a photon flux in the opposite direction to the other. We show how, despite appearances, this does not break the unitary requirements of quantum mechanics. We also find that an isotropic quantum emitter can be either reflective or transmissive to light depending on the waveguide polarisation at the emitter location, indeed in the zero loss limit such a system changes from 100% transmission to 100% reflection due to an infinitesimal polarisation rotation. An example case for a four level system is also considered, which is found to operate as a non-destructive parity measurement of the photon number.

</details>


### [24] [Time-Dynamic Circuits for Fault-Tolerant Shift Automorphisms in Quantum LDPC Codes](https://arxiv.org/abs/2601.09911)
*Younghun Kim,Spiro Gicev,Martin Sevior,Muhammad Usman*

Main category: quant-ph

TL;DR: 提出动态电路实现qLDPC码的移位自同构，相比SWAP方案显著降低逻辑错误率，接近空闲操作性能


<details>
  <summary>Details</summary>
Motivation: qLDPC码作为低开销量子存储器有前景，移位自同构是实现通用逻辑门的基础组件，但现有SWAP方案逻辑错误率远高于容错空闲操作，需要改进

Method: 通过动态变化综合征测量电路实现移位自同构，不降低电路距离。在扭曲和非扭曲权重6广义环面码（包括Gross码族）上测试，使用BP-OSD解码器

Result: 动态电路在电路级噪声模型（SI1000）下性能接近空闲操作。在物理错误率10^-3时，Gross码逻辑错误率相比SWAP方案降低超过一个数量级

Conclusion: 动态电路提高了qLDPC码移位自同构的错误恢复能力和时间开销，为超越表面码的qLDPC码动态电路应用提供实用途径，可引导替代综合征提取电路设计

Abstract: Quantum low-density parity-check (qLDPC) codes have emerged as a promising approach for realizing low-overhead logical quantum memories. Recent theoretical developments have established shift automorphisms as a fundamental building block for completing the universal set of logical gates for qLDPC codes. However, practical challenges remain because the existing SWAP-based shift automorphism yields logical error rates that are orders of magnitude higher than those for fault-tolerant idle operations. In this work, we address this issue by dynamically varying the syndrome measurement circuits to implement the shift automorphisms without reducing the circuit distance. We benchmark our approach on both twisted and untwisted weight-6 generalized toric codes, including the gross code family. Our time-dynamic circuits for shift automorphisms achieve performance comparable to the idle operations under the circuit-level noise model (SI1000). Specifically, the dynamic circuits achieve more than an order of magnitude reduction in logical error rates relative to the SWAP-based scheme for the gross code at a physical error rate of $10^{-3}$, employing the BP-OSD decoder. Our findings improve both the error resilience and the time overhead of the shift automorphisms in qLDPC codes. Furthermore, our work can lead to alternative syndrome extraction circuit designs, such as leakage removal protocols, providing a practical pathway to utilizing dynamic circuits that extend beyond surface codes towards qLDPC codes.

</details>


### [25] [Learning to Decode in Parallel: Self-Coordinating Neural Network for Real-Time Quantum Error Correction](https://arxiv.org/abs/2601.09921)
*Kai Zhang,Zhengzhong Yi,Shaojun Guo,Linghang Kong,Situ Wang,Xiaoyu Zhan,Tan He,Weiping Lin,Tao Jiang,Dongxin Gao,Yiming Zhang,Fangming Liu,Fang Zhang,Zhengfeng Ji,Fusheng Chen,Jianxin Chen*

Main category: quant-ph

TL;DR: 提出首个可扩展的神经网络并行解码框架，解决了AlphaQubit类解码器在实时量子纠错中的吞吐瓶颈，在保持SOTA精度的同时满足实时解码的严格要求。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络解码器（如AlphaQubit）虽然精度高，但缺乏并行性，无法实时解码超导逻辑量子比特产生的综合征流。将AlphaQubit与滑动窗口并行解码方案集成存在挑战，因为它只输出整个内存实验的全局逻辑校正位，而非易于集成的局部物理校正。

Method: 训练一个专门为并行窗口解码设计的循环transformer神经网络。虽然仍输出单个比特，但从一致的局部校正集推导训练标签，并在多种解码窗口类型上同时训练。这种方法使网络能够在相邻窗口间自我协调，实现对任意长内存实验的高精度并行解码。

Result: 1. 在祖冲之3.2超导量子处理器上对距离达7的表面码进行端到端实验基准测试，展示了优越的精度；2. 使用单个TPU v6e能够在1微秒内解码距离达25的表面码；3. 克服了先前阻碍AlphaQubit类解码器在FTQC中使用的吞吐瓶颈。

Conclusion: 提出了首个同时实现SOTA精度和实时量子纠错所需严格吞吐量的可扩展神经网络并行解码框架，为容错量子计算中的实时解码问题提供了有效解决方案。

Abstract: Fast, reliable decoders are pivotal components for enabling fault-tolerant quantum computation (FTQC). Neural network decoders like AlphaQubit have demonstrated potential, achieving higher accuracy than traditional human-designed decoding algorithms. However, existing implementations of neural network decoders lack the parallelism required to decode the syndrome stream generated by a superconducting logical qubit in real time. Moreover, integrating AlphaQubit with sliding window-based parallel decoding schemes presents non-trivial challenges: AlphaQubit is trained solely to output a single bit corresponding to the global logical correction for an entire memory experiment, rather than local physical corrections that can be easily integrated. We address this issue by training a recurrent, transformer-based neural network specifically tailored for parallel window decoding. While it still outputs a single bit, we derive training labels from a consistent set of local corrections and train on various types of decoding windows simultaneously. This approach enables the network to self-coordinate across neighboring windows, facilitating high-accuracy parallel decoding of arbitrarily long memory experiments.
  As a result, we overcome the throughput bottleneck that previously precluded the use of AlphaQubit-type decoders in FTQC. Our work presents the first scalable, neural-network-based parallel decoding framework that simultaneously achieves SOTA accuracy and the stringent throughput required for real-time quantum error correction. Using an end-to-end experimental workflow, we benchmark our decoder on the Zuchongzhi 3.2 superconducting quantum processor on surface codes with distances up to 7, demonstrating its superior accuracy. Moreover, we demonstrate that, using our approach, a single TPU v6e is capable of decoding surface codes with distances up to 25 within 1us per decoding round.

</details>


### [26] [Beyond Optimization: Harnessing Quantum Annealer Dynamics for Machine Learning](https://arxiv.org/abs/2601.09938)
*Akitada Sakurai,Aoi Hayashi,Tadayoshi Matumori,Daisuke Kaji,Tadashi Kadowaki,Kae Nemoto*

Main category: quant-ph

TL;DR: 量子退火用于机器学习：将经典数据编码到伊辛哈密顿量中，通过量子退火器演化，用得到的概率分布作为特征映射进行分类。实验表明短退火时间获得更高分类精度，长时间降低精度但减少采样成本。参与比作为有效模型大小的度量，与泛化能力强相关。


<details>
  <summary>Details</summary>
Motivation: 量子退火通常被视为组合优化的工具，但其相干动力学也为机器学习提供了潜力。研究者希望探索如何利用量子退火器的量子动力学来处理机器学习任务，特别是分类问题。

Method: 提出一个模型：1）将经典数据编码到伊辛哈密顿量中；2）在量子退火器上演化该哈密顿量；3）使用得到的概率分布作为特征映射进行分类。在量子退火器上使用Digits数据集进行实验，并在MNIST上进行模拟。

Result: 实验发现：短退火时间产生更高的分类准确率，而长退火时间降低准确率但减少采样成本。引入参与比作为有效模型大小的度量，并显示其与泛化能力有强相关性。

Conclusion: 量子退火可以有效地用于机器学习任务，特别是分类问题。退火时间在精度和采样成本之间存在权衡，参与比是衡量模型泛化能力的有用指标。

Abstract: Quantum annealing is typically regarded as a tool for combinatorial optimization, but its coherent dynamics also offer potential for machine learning. We present a model that encodes classical data into an Ising Hamiltonian, evolves it on a quantum annealer, and uses the resulting probability distributions as feature maps for classification. Experiments on the quantum annealer machine with the Digits dataset, together with simulations on MNIST, demonstrate that short annealing times yield higher classification accuracy, while longer times reduce accuracy but lower sampling costs. We introduce the participation ratio as a measure of the effective model size and show its strong correlation with generalization.

</details>


### [27] [Three Months in the Life of Cloud Quantum Computing](https://arxiv.org/abs/2601.09943)
*Darrell Teegarden,Allison Casey,F. Gino Serpa,Patrick Becker,Asmita Brahme,Saanvi Kataria,Paul Lopata*

Main category: quant-ph

TL;DR: 本文系统评估了云量子计算环境的实际使用体验，通过三个月的数据收集，分析了不同平台在连接性、执行时间、成本等方面的表现，为量子计算潜在用户提供实用参考。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算从实验室走向商业化云服务，用户需要面对复杂的工具链和不断变化的硬件环境。理解这些环境的实际使用体验、性能和成本权衡对于评估量子计算的实用价值至关重要。

Method: 采用系统化方法，在三个月内使用各种量子编程环境，收集连接指标、算法执行、量子比特数变化影响、与模拟对比、执行时间和成本等元数据。使用开箱即用设置和工具，在不同机器和云平台上执行单一算法。

Result: 获得了不同量子计算服务的连接指标、算法执行数据、量子比特数扩展性分析、与模拟的对比结果、执行时间统计和成本数据。这些数据揭示了不同平台的实际性能和可用性差异。

Conclusion: 通过精心策划的数据收集，为探索量子计算潜力的用户提供了具体的数据和见解。这项工作不是要提出新算法或优化特定机器性能，而是提供一个跨平台、跨时间的量子计算环境使用基准。

Abstract: Quantum Computing (QC) has evolved from a few custom quantum computers, which were only accessible to their creators, to an array of commercial quantum computers that can be accessed on the cloud by anyone. Accessing these cloud quantum computers requires a complex chain of tools that facilitate connecting, programming, simulating algorithms, estimating resources, submitting quantum computing jobs, retrieving results, and more. Some steps in the chain are hardware dependent and subject to change as both hardware and software tools, such as available gate sets and optimizing compilers, evolve. Understanding the trade-offs inherent in this process is essential for evaluating the power and utility of quantum computers. ARLIS has been systematically investigating these environments to understand these complexities. The work presented here is a detailed summary of three months of using such quantum programming environments. We show metadata obtained from these environments, including the connection metrics to the different services, the execution of algorithms, the testing of the effects of varying the number of qubits, comparisons to simulations, execution times, and cost. Our objective is to provide concrete data and insights for those who are exploring the potential of quantum computing. It is not our objective to present any new algorithms or optimize performance on any particular machine or cloud platform; rather, this work is focused on providing a consistent view of a single algorithm executed using out-of-the-box settings and tools across machines, cloud platforms, and time. We present insights only available from these carefully curated data.

</details>


### [28] [Parallelizing the Variational Quantum Eigensolver: From JIT Compilation to Multi-GPU Scaling](https://arxiv.org/abs/2601.09951)
*Rylan Malarchick,Ashton Steed*

Main category: quant-ph

TL;DR: 本文通过多种并行化技术优化了VQE算法，在HPC集群上实现了117倍加速，将H2分子势能面计算时间从10分钟缩短到5秒。


<details>
  <summary>Details</summary>
Motivation: 变分量子本征求解器(VQE)是计算分子基态能量的重要算法，但实际应用中面临计算效率瓶颈。本文旨在通过高性能计算技术优化VQE，实现量子化学计算的交互式探索。

Method: 采用四阶段并行化策略：1) 优化器+JIT编译；2) GPU设备加速；3) MPI并行化；4) 多GPU扩展。在配备4×NVIDIA H100 GPU的HPC集群上，使用PennyLane框架实现H2分子势能面计算。

Result: 总加速比达到117倍（593.95秒→5.04秒）。各阶段加速效果：优化器+JIT 4.13倍，GPU加速3.60-80.5倍（4-26量子比特），MPI并行28.5倍，多GPU扩展3.98倍（99.4%并行效率）。单H100 GPU可模拟最多29量子比特。

Conclusion: 通过系统性的并行化优化，VQE算法实现了显著的性能提升，将计算时间从近10分钟缩短到5秒，为交互式量子化学探索提供了可行方案，证明了高性能计算在量子算法加速中的关键作用。

Abstract: The Variational Quantum Eigensolver (VQE) is a hybrid quantum-classical algorithm for computing ground state energies of molecular systems. We implement VQE to calculate the potential energy surface of the hydrogen molecule (H$_2$) across 100 bond lengths using the PennyLane quantum computing framework on an HPC cluster featuring 4$\times$ NVIDIA H100 GPUs (80GB each). We present a comprehensive parallelization study with four phases: (1) Optimizer + JIT compilation achieving 4.13$\times$ speedup, (2) GPU device acceleration achieving 3.60$\times$ speedup at 4 qubits scaling to 80.5$\times$ at 26 qubits, (3) MPI parallelization achieving 28.5$\times$ speedup, and (4) Multi-GPU scaling achieving 3.98$\times$ speedup with 99.4% parallel efficiency across 4 H100 GPUs. The combined effect yields 117$\times$ total speedup for the H$_2$ potential energy surface (593.95s $\rightarrow$ 5.04s). We conduct a CPU vs GPU scaling study from 4--26 qubits, finding GPU advantage at all scales with speedups ranging from 10.5$\times$ to 80.5$\times$. Multi-GPU benchmarks demonstrate near-perfect scaling with 99.4% efficiency and establish that a single H100 can simulate up to 29 qubits before hitting memory limits. The optimized implementation reduces runtime from nearly 10 minutes to 5 seconds, enabling interactive quantum chemistry exploration.

</details>


### [29] [Statistical-noise-enhanced multi-photon interference](https://arxiv.org/abs/2601.09977)
*Rikizo Ikuta*

Main category: quant-ph

TL;DR: 三光子干涉中，可见度与强度关联函数的单调关系不成立，超泊松光子数涨落可最大化可见度，量子与经典优势在干涉中互斥


<details>
  <summary>Details</summary>
Motivation: 研究多光子干涉中的光子统计规律，探索三光子干涉与标准双光子干涉（Hong-Ou-Mandel干涉）的不同特性，揭示光子统计在干涉中的非单调关系

Method: 使用对称电路研究三光子干涉，在离散傅里叶变换电路中，通过调制激光器实现工程化的超泊松光子数涨落，并调整对称电路参数

Result: 发现三光子干涉中可见度与强度关联函数不存在单调关系，超泊松光子数涨落可使可见度最大化并超过单光子特征值，通过调整电路参数可使可见度层次相对于泊松统计基准发生反转

Conclusion: 量子优势和经典优势在干涉中是相互排斥的资源，表明存在一种统计互补性，光子统计在多光子干涉中表现出比传统理解更复杂的特性

Abstract: Photon statistics plays a governing role in multi-photon interference. While interference visibility in the standard two-photon case, known as Hong-Ou-Mandel interference, monotonically degrades with higher intensity correlation functions, we show that this monotonicity does not hold for three-photon interference in symmetric circuits. We reveal that, in the discrete Fourier transform circuit, engineered super-Poissonian photon-number fluctuations, realized using a modulated laser, maximize the visibility, surpassing the magnitude of the single-photon signature. In addition, by tuning the symmetric circuit parameters, we demonstrate that the visibility hierarchy inverts relative to the benchmark of Poissonian statistics. This trade-off implies that quantum and classical advantages are mutually exclusive resources for interference, indicating a form of statistical complementarity.

</details>


### [30] [Double Markovity for quantum systems](https://arxiv.org/abs/2601.09995)
*Masahito Hayashi,Jinpei Zhao*

Main category: quant-ph

TL;DR: 该论文建立了量子系统中的双重马尔可夫性类比，解决了将SDR技术扩展到量子系统的关键瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 经典信息论中的SDR技术依赖于严格次可加性和其等式情况分析，其中双重马尔可夫性是标准工具。为了将SDR类型论证扩展到量子系统，需要建立量子类比。

Method: 对于三方态，通过B和C上的兼容投影测量来表征同时满足A-B-C和A-C-B的马尔可夫条件，这些测量产生共同的经典标签J，使得A-J-(BC)成立。对于严格正的四方态，证明A-(BD)-C和A-(CD)-B成立当且仅当A-D-(BC)成立。

Result: 建立了量子双重马尔可夫性的精确数学表征，为量子SDR论证提供了必要的理论工具。

Conclusion: 这些结果消除了将SDR类型论证扩展到量子系统的关键瓶颈，为量子信息论中的高斯最优性研究开辟了新途径。

Abstract: The subadditivity-doubling-rotation (SDR) technique is a powerful route to Gaussian optimality in classical information theory and relies on strict subadditivity and its equality-case analysis, where double Markovity is a standard tool. We establish quantum analogues of double Markovity. For tripartite states, we characterize the simultaneous Markov conditions A-B-C and A-C-B via compatible projective measurements on B and C that induce a common classical label J yielding A-J-(BC). For strictly positive four-party states, we show that A-(BD)-C and A-(CD)-B hold if and only if A-D-(BC) holds. These results remove a key bottleneck in extending SDR-type arguments to quantum systems.

</details>


### [31] [Reentrant topological phases and entanglement scalings in moiré-modulated extended Su-Schrieffer-Heeger Model](https://arxiv.org/abs/2601.09997)
*Guo-Qing Zhang,L. F. Quezada,Shi-Hai Dong*

Main category: quant-ph

TL;DR: 该研究探讨了摩尔调制扩展SSH模型中由摩尔强度驱动的重入相变序列和普适类不变性，揭示了重入现象、体边对应关系以及纠缠谱与零能边缘模式之间的关联。


<details>
  <summary>Details</summary>
Motivation: 尽管摩尔物理为量子相变领域提供了丰富机会，但由摩尔强度驱动的重入相变特性尚未被充分理解。本研究旨在探究一维凝聚态系统中摩尔诱导的重入相变的普遍特性和体边对应关系。

Method: 1. 对于胞间跃迁w=0的简化情况，解析推导哈密顿量参数的重整化关系来解释重入现象；2. 对于一般情况，在热力学极限下计算数值相边界；3. 通过纠缠谱简并度揭示零能边缘模式与体边对应关系；4. 分析纠缠熵中心荷与相变过程中绕数变化之间的对应关系。

Result: 1. 成功解释了摩尔调制扩展SSH模型中的重入相变序列；2. 确定了相变的普适类不变性；3. 揭示了纠缠谱简并度与零能边缘模式之间的体边对应关系；4. 建立了中心荷与绕数变化之间的对应关系。

Conclusion: 该研究为理解一维凝聚态系统中摩尔诱导的重入相变的普遍特性和体边对应关系提供了重要见解，有助于推动摩尔物理在量子相变领域的应用。

Abstract: Recent studies of moiré physics have unveiled a wealth of opportunities for significantly advancing the field of quantum phase transitions. However, properties of reentrant phase transitions driven by moiré strength are poorly understood. Here, we investigate the reentrant sequence of phase transitions and the invariant of universality class in moiré-modulated extended Su-Schrieffer-Heeger (SSH) model. For the simplified case with intercell hopping $w=0$, we analytically derive renormalization relations of Hamiltonian parameters to explain the reentrant phenomenon. For the general case, numerical phase boundaries are calculated in the thermodynamic limit. The bulk boundary correspondence between zero-energy edge modes and entanglement spectrum is revealed from the degeneracy of both quantities. We also address the correspondence between the central charge obtained from entanglement entropy and the change in winding number during the phase transition. Our results shed light on the understanding of universal characteristics and bulk-boundary correspondence for moiré induced reentrant phase transitions in 1D condensed-matter systems.

</details>


### [32] [Contextuality Derived from Minimal Decision Dynamics: Quantum Tug-of-War Decision Making](https://arxiv.org/abs/2601.10034)
*Song-Ju Kim*

Main category: quant-ph

TL;DR: 量子概率不是决策建模的便利假设，而是自适应决策动力学不可避免的有效理论，因为基于物理约束的决策过程自然产生上下文依赖性。


<details>
  <summary>Details</summary>
Motivation: 决策制定常表现出挑战经典概率论的上下文依赖性。虽然量子认知已成功建模此类现象，但尚不清楚量子概率仅是便利假设还是决策动力学的必然结果。

Method: 开发了Tug-of-War模型的量子扩展，展示基于守恒的内部状态更新和测量诱导的干扰排除了任何具有单一统一内部状态的非上下文经典描述。

Result: 上下文性作为自适应学习动力学的结构结果出现，产生的测量结构在最小单系统设置中允许KCBS型上下文性见证。

Conclusion: 量子概率不仅是描述性便利，而是自适应决策动力学不可避免的有效理论，上下文性从决策制定的物理约束中生成性地产生。

Abstract: Decision making often exhibits context dependence that challenges classical probability theory. While quantum cognition has successfully modeled such phenomena, it remains unclear whether quantum probability is merely a convenient assumption or a necessary consequence of decision dynamics. Here we present a theoretical framework in which contextuality arises generatively from physically grounded constraints on decision making. By developing a quantum extension of the Tug-of-War (TOW) model, we show that conservation-based internal state updates and measurement-induced disturbance preclude any non-contextual classical description with a single, unified internal state. Contextuality therefore emerges as a structural consequence of adaptive learning dynamics. We further show that the resulting measurement structure admits Klyachko-Can-Binicioglu-Shumovsky (KCBS)-type contextuality witnesses in a minimal single-system setting. These results indicate that quantum probability is not merely a descriptive convenience, but an unavoidable effective theory for adaptive decision dynamics.

</details>


### [33] [Towards Minimal Fault-tolerant Error-Correction Sequence with Quantum Hamming Codes](https://arxiv.org/abs/2601.10042)
*Sha Shi,Xiao-Yang Xu,Min-Quan Cheng,Dong-Sheng Wang,Yun-Jiang Wang*

Main category: quant-ph

TL;DR: 为量子汉明码构建了高效的容错测量序列，将序列长度减少到仅比原始非容错序列多一次测量，同时通过对称性实现硬件复用。


<details>
  <summary>Details</summary>
Motivation: 容错测量序列的高开销是实现量子稳定子码的主要挑战，需要设计更高效的FTMS来降低时间和硬件开销。

Method: 利用循环矩阵变换系统性地组合初始稳定子矩阵的行，保持类似原始量子汉明码的自对偶CSS对称性，通过切换边界Hadamard门实现电路复用。

Result: 序列长度可减少到精确的2r+1，仅比原始非容错序列多一次测量，建立了紧下界，同时通过对称性实现硬件高效复用。

Conclusion: 该方法同时减少了距离-3容错纠错的时间和硬件开销，为量子汉明码最小FTMS设计提供了重要进展，对其他量子稳定子码也有启发意义。

Abstract: The high overhead of fault-tolerant measurement sequences (FTMSs) poses a major challenge for implementing quantum stabilizer codes. Here, we address this problem by constructing efficient FTMSs for the class of quantum Hamming codes $[\![2^r-1, 2^r-1-2r, 3]\!]$ with $r=3k+1$ ($k \in \mathbb{Z}^+$). Our key result demonstrates that the sequence length can be reduced to exactly $2r+1$-only one additional measurement beyond the original non-fault-tolerant sequence, establishing a tight lower bound. The proposed method leverages cyclic matrix transformations to systematically combine rows of the initial stabilizer matrix and preserving a self-dual CSS-like symmetry analogous to that of the original quantum Hamming codes. This induced symmetry enables hardware-efficient circuit reuse: the measurement circuits for the first $r$ stabilizers are transformed into circuits for the remaining $r$ stabilizers simply by toggling boundary Hadamard gates, eliminating redundant hardware. For distance-3 fault-tolerant error correction, our approach simultaneously reduces the time overhead via shorting the FTMS length and the hardware overhead through symmetry-enabled circuit multiplexing. These results provide an important advance towards the important open problem regarding the design of minimal FTMSs for quantum Hamming codes and may shed light on similar challenges in other quantum stabilizer codes.

</details>


### [34] [Optimal qudit overlapping tomography and optimal measurement order](https://arxiv.org/abs/2601.10059)
*Shuowei Ma,Qianfan Wang,Lvzhou Li,Fei Shi*

Main category: quant-ph

TL;DR: 该论文研究了量子比特系统的重叠层析成像，建立了基于广义盖尔曼矩阵的局部测量方案，通过组合覆盖阵列对应关系提出了两种最优构造方法，并开发了优化测量顺序的算法以减少实验切换开销。


<details>
  <summary>Details</summary>
Motivation: 量子态层析成像对于表征量子系统至关重要，但随着系统规模增大，资源需求呈指数增长而变得不可行。虽然重叠层析成像已在量子比特系统中得到研究，但对于高维量子比特系统的扩展仍未被充分探索。

Method: 使用广义盖尔曼矩阵构建局部测量设置，建立与组合覆盖阵列的对应关系，提出两种最优测量方案的显式构造。对于n-三能级系统，证明了成对层析成像所需测量设置的上界，并开发了优化测量顺序的算法以最小化实验切换开销。

Result: 对于n-三能级系统，证明了成对层析成像最多需要8 + 56⌈log₈ n⌉个测量设置，并提供了实现该上界的显式方案。优化算法将切换成本相比最坏情况减少了约50%。

Conclusion: 该研究为高效表征量子比特系统提供了实用途径，促进了量子比特系统在量子通信和计算中的应用，通过最优测量方案和顺序优化显著减少了实验开销。

Abstract: Quantum state tomography is essential for characterizing quantum systems, but it becomes infeasible for large systems due to exponential resource scaling. Overlapping tomography addresses this challenge by reconstructing all $k$-body marginals using few measurement settings, enabling the efficient extraction of key information for many quantum tasks. While optimal schemes are known for qubits, the extension to higher-dimensional qudit systems remains largely unexplored. Here, we investigate optimal qudit overlapping tomography, constructing local measurement settings from generalized Gell-Mann matrices. By establishing a correspondence with combinatorial covering arrays, we present two explicit constructions of optimal measurement schemes. For $n$-qutrit systems, we prove that pairwise tomography requires at most $8 + 56\left\lceil \log_{8} n \right\rceil$ measurement settings, and provide an explicit scheme achieving this bound. Furthermore, we develop an efficient algorithm to determine the optimal order of these measurement settings, minimizing the experimental overhead associated with switching configurations. Compared to the worst-case ordering, our optimized schedule reduces switching costs by approximately 50\%. These results provide a practical pathway for efficient characterization of qudit systems, facilitating their application in quantum communication and computation.

</details>


### [35] [Geometric Criteria for Complete Mode Conversion in Detuned Systems via Piecewise-Coherent Modulation](https://arxiv.org/abs/2601.10066)
*Awanish Pandey*

Main category: quant-ph

TL;DR: 提出基于布洛赫球面几何的相干调制方法，解决失谐系统中模式转换的静态相位失谐约束，实现光学隔离器和任意失谐下的确定性传输


<details>
  <summary>Details</summary>
Motivation: 静态相位失谐从根本上限制了非对称经典和量子系统中的相干态传输，需要新的控制方法来克服这一限制

Method: 引入布洛赫球面表述，将分段相干调制重新表述为几何轨迹，将代数控制转化为路径优化，揭示目标极点处的不可达锥面，并推导失谐系统中完全模式转换的精确测地线准则

Result: 1) 利用该框架打破时间反演对称性，实现了无磁光学隔离器，具有接近完美的对比度；2) 对于失谐大于模式间耦合的情况，开发了递归多步协议，能够在任意失谐下实现确定性传输；3) 推导了所需耦合切换事件的通用几何下界

Conclusion: 基于布洛赫球面几何的相干调制方法为克服静态相位失谐约束提供了系统框架，在光学隔离和任意失谐下的确定性传输方面取得了突破性进展

Abstract: Static phase detuning fundamentally constrains coherent state transfer in asymmetric classical and quantum systems. We introduce a Bloch-sphere formulation for piecewise-coherent modulation that recasts coupled-mode dynamics as geometric trajectories, transforming algebraic control into path optimization. The approach reveals a cone of inaccessibility at the target pole and yields exact geodesic criteria for complete mode conversion in detuned systems. Leveraging this framework, we break time-reversal symmetry to realize a magnet-free optical isolator with near-unity contrast. Furthermore, for detuning larger than coupling between modes, we develop a recursive multi-step protocol enabling deterministic transfer for arbitrary detunings and derive a universal geometric lower bound on the required number of coupling-switching events.

</details>


### [36] [Pseudomode approach to Fano effect in dissipative cavity quantum electrodynamics](https://arxiv.org/abs/2601.10087)
*Kazuki Kobayashi,Tatsuro Yuge*

Main category: quant-ph

TL;DR: 该论文研究了耗散腔量子电动力学中的Fano效应，建立了描述单模腔QED系统中Fano效应的统一框架，并阐明了其非马尔可夫起源。


<details>
  <summary>Details</summary>
Motivation: 研究耗散腔量子电动力学中的Fano效应，该效应源于发射体直接辐射与腔模介导辐射之间的干涉。旨在建立描述单模腔QED系统中Fano效应的统一理论框架。

Method: 1. 从耦合到结构库的两能级系统出发，通过伪模方法引入单个辅助模式重新推导量子主方程；2. 识别系统-环境相互作用的谱函数，证明其由常数项和形成Fano轮廓的非洛伦兹贡献组成；3. 应用Fano对角化到包含显式腔模的共环境设置中，在最强调干涉区域独立推导相同谱函数。

Result: 1. 证明了常数项对于获得Lindblad主方程至关重要，且直接与Fano干涉相关的速率相关；2. 在最强调干涉区域独立推导出相同的谱函数；3. 建立了描述单模腔QED系统中Fano效应的统一框架；4. 阐明了Fano效应的非马尔可夫起源编码在谱函数中。

Conclusion: 该研究成功建立了耗散腔量子电动力学中Fano效应的统一理论框架，阐明了其非马尔可夫性质，为理解单模腔QED系统中的干涉效应提供了理论基础。

Abstract: We study the Fano effect in dissipative cavity quantum electrodynamics, which originates from the interference between the emitter's direct radiation and that mediated by a cavity mode. Starting from a two-level system coupled to a structured reservoir, we show that a quantum master equation previously derived within the Born-Markov approximation can be rederived by introducing a single auxiliary mode via pseudomode approach. We identify the corresponding spectral function of the system--environment interaction and demonstrate that it consists of a constant and a non-Lorentzian contribution forming the Fano profile. The constant term is shown to be essential for obtaining a Lindblad master equation and is directly related to the rate associated with this Fano interference. Furthermore, by applying Fano diagonalization to a common-environment setup including an explicit cavity mode, we independently derive the same spectral function in the strongest-interference regime. Our results establish a unified framework for describing the Fano effect in single-mode cavity QED systems and clarify its non-Markovian origin encoded in the spectral function.

</details>


### [37] [Classical simulation of a quantum circuit with noisy magic inputs](https://arxiv.org/abs/2601.10111)
*Jiwon Heo,Sojeong Park,Changhun Oh*

Main category: quant-ph

TL;DR: 研究噪声魔法态对量子电路经典可模拟性的影响，建立了噪声依赖的经典采样算法，确定了从量子优势到经典可模拟的相变阈值。


<details>
  <summary>Details</summary>
Motivation: 魔法态是实现通用量子计算的关键资源，但实际设备中不可避免地存在噪声。需要理解噪声如何影响量子电路的经典可模拟性，以及何时会发生从量子难解到经典可模拟的转变。

Method: 采用资源中心噪声模型，仅魔法态注入部分有噪声，而基础状态、操作和测量属于高效可模拟家族。开发了具有可控误差的近似经典采样算法，证明了噪声依赖的多项式时间运行条件。

Result: 建立了明确的噪声依赖条件，在这些条件下算法能在多项式时间内运行。框架适用于量子比特电路（Clifford基线）和费米子电路（matchgate基线），涵盖退相干和粒子损失等典型噪声通道。提供了模拟成本的具体阈值和运行时标度。

Conclusion: 噪声魔法态资源可以显著改变量子电路的经典可模拟性，建立了从量子优势到经典可模拟的相变条件，为实际量子设备的优势评估提供了理论框架。

Abstract: Magic states are essential for universal quantum computation and are widely viewed as a key source of quantum advantage, yet in realistic devices they are inevitably noisy. In this work, we characterize how noise on injected magic resources changes the classical simulability of quantum circuits and when it induces a transition from classically intractable behavior to efficient classical simulation. We adopt a resource-centric noise model in which only the injected magic components are noisy, while the baseline states, operations, and measurements belong to an efficiently simulable family. Within this setting, we develop an approximate classical sampling algorithm with controlled error and prove explicit noise-dependent conditions under which the algorithm runs in polynomial time. Our framework applies to both qubit circuits with Clifford baselines and fermionic circuits with matchgate baselines, covering representative noise channels such as dephasing and particle loss. We complement the analysis with numerical estimates of the simulation cost, providing concrete thresholds and runtime scaling across practically relevant parameter regimes.

</details>


### [38] [Casimir interactions as a probe of broadband optical response](https://arxiv.org/abs/2601.10118)
*Calum F. Shelden,Jeremy N. Munday*

Main category: quant-ph

TL;DR: 利用机器学习反演Lifshitz理论，从单一Casimir力-距离曲线重建材料在七个数量级频率范围内的复介电常数，将Casimir相互作用转化为宽带光谱工具


<details>
  <summary>Details</summary>
Motivation: Casimir力依赖于材料在整个频率范围内的介电响应，但传统Lifshitz理论使用虚频率下的介电函数，模糊了与实频率光学性质的直接联系，限制了Casimir相互作用作为材料探测工具的应用

Method: 使用监督机器学习反演Lifshitz理论，从单个Casimir力-距离曲线中提取信息，通过不同间距的测量选择性地约束介电响应在不同频率范围的特征

Result: 成功从单一力-距离曲线重建材料在超过七个数量级频率范围内的复介电常数，证明不同间距的测量能够选择性地约束介电响应在不同频率范围的特征

Conclusion: Casimir相互作用可作为一种物理约束的宽带光谱工具，为传统技术无法触及的光学表征领域开辟新机会，揭示了量子涨落如何采样电磁频谱的直接物理洞察

Abstract: Casimir forces arise from quantum electromagnetic fluctuations and depend on the dielectric response of interacting materials across the entire frequency spectrum. Although this dependence is central to Lifshitz theory of the Casimir effect, the formulation of the force in terms of dielectric functions evaluated at imaginary frequencies has largely obscured its connection to real-frequency optical properties, limiting the use of Casimir interactions as a probe of materials. Here we demonstrate that Casimir force measurements encode sufficient information to reconstruct a material's broadband optical response. Using supervised machine learning to invert Lifshitz theory, we determine the complex permittivity of a material over more than seven orders of magnitude in frequency from a single force-distance curve. We show that measurements at different separations selectively constrain distinct frequency ranges of the dielectric response, providing direct physical insight into how quantum fluctuations sample the electromagnetic spectrum. These results establish Casimir interactions as a physically constrained, broadband spectroscopic tool and open new opportunities for optical characterization in regimes inaccessible to conventional techniques.

</details>


### [39] [Bridging Superconducting and Neutral-Atom Platforms for Efficient Fault-Tolerant Quantum Architectures](https://arxiv.org/abs/2601.10144)
*Xiang Fang,Jixuan Ruan,Sharanya Prabhu,Ang Li,Travis Humble,Dean Tullsen,Yufei Ding*

Main category: quant-ph

TL;DR: 该论文提出异构量子架构(HQA)，结合超导和中性原子平台优势，通过两种策略实现显著性能提升：MagicAcc将关键魔法态制备卸载到超导设备，MCSep实现内存-计算分离。


<details>
  <summary>Details</summary>
Motivation: 同质量子系统无法同时满足操作速度、连接性和可扩展性的最优需求，限制了容错量子计算的发展。需要探索跨平台异构架构来克服单一平台的局限性。

Method: 提出两种异构架构策略：1) MagicAcc：将延迟关键的魔法态工厂(MSF)卸载到快速的超导设备，在可扩展的中性原子阵列上执行计算；2) 内存-计算分离(MCSep)：利用中性原子阵列进行高密度qLDPC内存存储，使用超导设备进行快速表面码处理。

Result: 基于端到端成本模型的评估显示，异构架构相比纯中性原子基线平均获得752倍加速，相比纯超导系统减少超过10倍的物理量子比特占用空间。

Conclusion: 异构量子架构通过跨模态互连优化了未来容错量子计算机的时空效率，为量子计算系统设计提供了明确的技术路径。

Abstract: The transition to the fault-tolerant era exposes the limitations of homogeneous quantum systems, where no single qubit modality simultaneously offers optimal operation speed, connectivity, and scalability. In this work, we propose a strategic approach to Heterogeneous Quantum Architectures (HQA) that synthesizes the distinct advantages of the superconducting (SC) and neutral atom (NA) platforms. We explore two architectural role assignment strategies based on hardware characteristics: (1) We offload the latency-critical Magic State Factory (MSF) to fast SC devices while performing computation on scalable NA arrays, a design we term MagicAcc, which effectively mitigates the resource-preparation bottleneck. (2) We explore a Memory-Compute Separation (MCSep) paradigm that utilizes NA arrays for high-density qLDPC memory storage and SC devices for fast surface-code processing. Our evaluation, based on a comprehensive end-to-end cost model, demonstrates that principled heterogeneity yields significant performance gains. Specifically, our designs achieve $752\times$ speedup over NA-only baselines on average and reduce the physical qubit footprint by over $10\times$ compared to SC-only systems. These results chart a clear pathway for leveraging cross-modality interconnects to optimize the space-time efficiency of future fault-tolerant quantum computers.

</details>


### [40] [Fluctuation-induced quenching of chaos in quantum optics](https://arxiv.org/abs/2601.10147)
*Mei-Qi Gao,Song-hai Li,Xun Li,Xingli Li,Jiong Cheng,Wenlin Li*

Main category: quant-ph

TL;DR: 量子光学系统中混沌动力学的平均场近似在考虑热涨落时失效，室温热涨落足以抑制期望值层面的混沌，非线性增强会降低混沌抑制的噪声阈值。


<details>
  <summary>Details</summary>
Motivation: 量子光学系统中混沌动力学通常使用平均场近似研究，但混沌对初始条件的敏感性意味着微小涨落可能被放大，这质疑了该近似的适用性。

Method: 使用随机朗之万方程或林德布拉德主方程分析混沌效应，研究频率在10^5到10^7 Hz的系统。

Result: 室温热涨落足以抑制期望值层面的混沌，非线性诱导量子态相空间分布偏离高斯分布，Wigner函数显示类似吸引子的特征，非线性增强会降低混沌抑制的噪声阈值至接近真空涨落水平。

Conclusion: 研究结果为量子力学抑制混沌提供了双向验证，表明在考虑量子涨落时，经典混沌行为在量子层面被抑制。

Abstract: Recent studies have extensively explored chaotic dynamics in quantum optical systems through the mean-field approximation, which corresponds to an ideal, fluctuation-free scenario. However, the inherent sensitivity of chaos to initial conditions implies that even minute fluctuations can be amplified, thereby questioning the applicability of this approximation. Here, we analyze these chaotic effects using stochastic Langevin equations or the Lindblad master equation. For systems operating at frequencies of $10^5$ to $10^7$ Hz, we demonstrate that room-temperature thermal fluctuations are sufficient to suppress chaos at the level of expectation values, even under weak nonlinearity. Furthermore, nonlinearity induces deviations from Gaussian phase-space distributions of the quantum state, revealing attractor-like features in the Wigner function. With increasing nonlinearity, the noise threshold for chaos suppression decreases, approaching the scale of vacuum fluctuations. These results provide a bidirectional validation of the quantum mechanical suppression of chaos.

</details>


### [41] [Exponential Analysis for Entanglement Distillation](https://arxiv.org/abs/2601.10190)
*Zhiwen Lin,Ke Li,Kun Fang*

Main category: quant-ph

TL;DR: 本文研究纠缠蒸馏的可靠性函数，将框架从已知状态扩展到黑盒设置，建立了与复合相关假设检验的联系，并分析了不同自由操作类别下的性能。


<details>
  <summary>Details</summary>
Motivation: 传统纠缠蒸馏研究主要集中在可蒸馏纠缠上，且假设初始状态完全已知。本文旨在研究纠缠蒸馏的可靠性函数（蒸馏误差衰减的最优指数），并将框架扩展到更具操作意义的黑盒设置，考虑从一组可能状态中进行蒸馏。

Method: 1. 建立与复合相关假设检验的精确有限块长结果，无需冗余修正项；2. 使用正则化量子Hoeffding散度表征纠缠蒸馏的可靠性函数；3. 在已知状态情况下构建具体最优蒸馏协议；4. 分析纠缠蒸馏的强逆指数；5. 研究不同自由操作类别（PPT保持、对偶非纠缠、对偶PPT保持操作）。

Result: 1. 纠缠蒸馏的可靠性函数由正则化量子Hoeffding散度表征；2. 纯初始状态情况下，结果简化为Hayashi等人2003年推导的纠缠浓缩误差指数；3. 在完全先验知识下，构建了具体最优蒸馏协议；4. 分析了强逆指数；5. 在不同自由操作类别下进行了扩展研究。

Conclusion: 本文建立了纠缠蒸馏可靠性函数的完整理论框架，将传统已知状态设置扩展到更具操作意义的黑盒设置，建立了与假设检验的精确联系，为不同自由操作类别下的纠缠蒸馏性能分析提供了理论基础。

Abstract: Historically, the focus in entanglement distillation has predominantly been on the distillable entanglement, and the framework assumes complete knowledge of the initial state. In this paper, we study the reliability function of entanglement distillation, which specifies the optimal exponent of the decay of the distillation error when the distillation rate is below the distillable entanglement. Furthermore, to capture greater operational significance, we extend the framework from the standard setting of known states to a black-box setting, where distillation is performed from a set of possible states. We establish an exact finite blocklength result connecting to composite correlated hypothesis testing without any redundant correction terms. Based on this, the reliability function of entanglement distillation is characterized by the regularized quantum Hoeffding divergence. In the special case of a pure initial state, our result reduces to the error exponent for entanglement concentration derived by Hayashi et al. in 2003. Given full prior knowledge of the state, we construct a concrete optimal distillation protocol. Additionally, we analyze the strong converse exponent of entanglement distillation. While all the above results assume the free operations to be non-entangling, we also investigate other free operation classes, including PPT-preserving, dually non-entangling, and dually PPT-preserving operations.

</details>


### [42] [Autonomous Quantum Simulation through Large Language Model Agents](https://arxiv.org/abs/2601.10194)
*Weitang Li,Jiajun Ren,Lixue Cheng,Cunxi Gong*

Main category: quant-ph

TL;DR: LLM智能体可自主执行量子多体系统的张量网络模拟，成功率约90%，通过上下文学习和多智能体分解实现


<details>
  <summary>Details</summary>
Motivation: 张量网络方法是量子模拟的强大工具，但需要多年研究生训练才能掌握的专业知识。本文旨在让AI智能体快速掌握专业计算领域知识

Method: 结合上下文学习、精选文档和多智能体分解，创建自主AI智能体。比较三种配置：基线、单智能体上下文学习、多智能体上下文学习

Result: 在量子相变、开放量子系统动力学和光化学反应等基准任务上达到约90%成功率。上下文学习和多智能体架构都至关重要，多智能体配置显著减少实现错误和幻觉

Conclusion: LLM智能体可自主执行复杂的张量网络模拟，多智能体架构结合上下文学习能有效减少错误，为专业计算领域提供快速训练方案

Abstract: We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks. Tensor network methods are powerful tools for quantum simulation, but their effective use requires expertise typically acquired through years of graduate training. By combining in-context learning with curated documentation and multi-agent decomposition, we create autonomous AI agents that can be trained in specialized computational domains within minutes. We benchmark three configurations (baseline, single-agent with in-context learning, and multi-agent with in-context learning) on problems spanning quantum phase transitions, open quantum system dynamics, and photochemical reactions. Systematic evaluation using DeepSeek-V3.2, Gemini 2.5 Pro, and Claude Opus 4.5 demonstrates that both in-context learning and multi-agent architecture are essential. Analysis of failure modes reveals characteristic patterns across models, with the multi-agent configuration substantially reducing implementation errors and hallucinations compared to simpler architectures.

</details>


### [43] [On the average-case complexity of learning states from the circular and Gaussian ensembles](https://arxiv.org/abs/2601.10197)
*Maxwell West*

Main category: quant-ph

TL;DR: 本文证明了在统计查询模型中，从圆形和高斯系综中均匀采样的Born分布的平均情况学习难度，补充了经典紧致群采样的类似结果。


<details>
  <summary>Details</summary>
Motivation: 研究从不同系综采样的量子态复杂度是量子信息理论的核心问题。本文旨在建立从圆形和费米子高斯系综均匀采样的Born分布在统计查询模型中的平均情况学习难度，以补充经典紧致群采样的类似结果。

Method: 采用非常规的紧致群积分方法，该方法具有独立的理论价值。具体分析了AI、AII和DIII型紧致对称空间上的均匀测度诱导的态系综。

Result: 证明了在统计查询模型中，从圆形和费米子高斯系综均匀采样的Born分布具有平均情况学习难度。同时，该方法能够精确计算Haar随机酉电路和正交电路输出分布与常数分布之间的总变差距离，而之前仅知道近似结果。

Conclusion: 本文建立了从圆形和高斯系综采样的Born分布的平均情况学习难度，补充了经典紧致群采样的结果。提出的非常规积分方法具有独立的理论价值，能够精确计算之前仅知近似的结果。

Abstract: Studying the complexity of states sampled from various ensembles is a central component of quantum information theory. In this work we establish the average-case hardness of learning, in the statistical query model, the Born distributions of states sampled uniformly from the circular and (fermionic) Gaussian ensembles. These ensembles of states are induced variously by the uniform measures on the compact symmetric spaces of type AI, AII, and DIII. This finding complements analogous recent results for states sampled from the classical compact groups. On the technical side, we employ a somewhat unconventional approach to integrating over the compact groups which may be of some independent interest. For example, our approach allows us to exactly evaluate the total variation distances between the output distributions of Haar random unitary and orthogonal circuits and the constant distribution, which were previously known only approximately.

</details>


### [44] [Topology-Aware Block Coordinate Descent for Qubit Frequency Calibration of Superconducting Quantum Processors](https://arxiv.org/abs/2601.10203)
*Zheng Zhao,Weifeng Zhuang,Yanwu Gu,Peng Qian,Xiao Xiao,Dong E. Liu*

Main category: quant-ph

TL;DR: 该论文将量子比特频率校准中的Snake优化器形式化为块坐标下降(BCD)，提出基于序列依赖旅行商问题(SD-TSP)的拓扑感知块排序方法，显著降低校准时间同时保持精度。


<details>
  <summary>Details</summary>
Motivation: 超导量子处理器中预执行校准是主要瓶颈，量子比特频率分配尤其困难，因为存在串扰耦合目标。需要建立理论基础并开发可扩展的校准工作流。

Method: 1) 证明Snake优化器数学等价于块坐标下降(BCD)；2) 将块排序问题建模为序列依赖旅行商问题(SD-TSP)，用最近邻启发式高效求解；3) SD-TSP成本反映块选择对评估所需缩减电路规模的影响；4) 在局部串扰/有界度假设下实现线性复杂度。

Result: 模拟显示BCD-NNA排序相比图启发式(BFS、DFS)和随机排序，在相同优化精度下显著降低运行时间，对测量噪声鲁棒，能容忍适度非局部串扰。

Conclusion: 该方法为NISQ时代处理器提供了可扩展、可直接实施的频率校准工作流，建立了校准目标的形式化框架，分析了含噪声测量的非精确BCD收敛性。

Abstract: Pre-execution calibration is a major bottleneck for operating superconducting quantum processors, and qubit frequency allocation is especially challenging due to crosstalk-coupled objectives. We establish that the widely-used Snake optimizer is mathematically equivalent to Block Coordinate Descent (BCD), providing a rigorous theoretical foundation for this calibration strategy. Building on this formalization, we present a topology-aware block ordering obtained by casting order selection as a Sequence-Dependent Traveling Salesman Problem (SD-TSP) and solving it efficiently with a nearest-neighbor heuristic. The SD-TSP cost reflects how a given block choice expands the reduced-circuit footprint required to evaluate the block-local objective, enabling orders that minimize per-epoch evaluation time. Under local crosstalk/bounded-degree assumptions, the method achieves linear complexity in qubit count per epoch, while retaining calibration quality. We formalize the calibration objective, clarify when reduced experiments are equivalent or approximate to the full objective, and analyze convergence of the resulting inexact BCD with noisy measurements. Simulations on multi-qubit models show that the proposed BCD-NNA ordering attains the same optimization accuracy at markedly lower runtime than graph-based heuristics (BFS, DFS) and random orders, and is robust to measurement noise and tolerant to moderate non-local crosstalk. These results provide a scalable, implementation-ready workflow for frequency calibration on NISQ-era processors.

</details>


### [45] [Noise-Resilient Quantum Evolution in Open Systems through Error-Correcting Frameworks](https://arxiv.org/abs/2601.10206)
*Nirupam Basak,Goutam Paul,Pritam Chattopadhyay*

Main category: quant-ph

TL;DR: 该研究在微观系统-浴模型中嵌入量子纠错码，分析开放量子系统中的量子态保真度，发现五量子比特码在低温下能有效抑制退相干，在高温下仍优于Steane码和toric码，为近量子技术提供了噪声评估框架。


<details>
  <summary>Details</summary>
Motivation: 传统量子纠错研究多基于抽象量子信道模型，缺乏对实际物理环境中系统-浴相互作用的考虑。本研究旨在将量子纠错码嵌入微观系统-浴模型，评估其在真实噪声环境下的性能，为近量子技术提供更现实的评估框架。

Method: 采用多量子比特寄存器与玻色热浴耦合的微观模型，推导二阶主方程描述约化动力学。在此基础上，对五量子比特码、Steane码和toric码在局部和集体噪声下进行基准测试，计算逻辑量子比特的态保真度随耦合强度、浴温度和纠错周期数的变化。

Result: 低温下，五量子比特码的重复纠错能强烈抑制退相干和弛豫；高温下，热激发主导动力学，所有码的效益降低，但五量子比特码仍优于Steane码和toric码。对于两量子比特Werner态，存在临界演化时间，在此之前纠错不提高保真度，且该时间随纠缠增加而延长。五量子比特码（最小完美码）在这些开放系统设置中始终提供比拓扑和级联架构更高的保真度。

Conclusion: 研究建立了在真实噪声环境下评估量子纠错的定量框架，为开发近量子技术中的噪声弹性量子架构提供了指导。五量子比特码在多种条件下表现优异，表明完美码在实际物理环境中具有重要优势。

Abstract: We analyze quantum state preservation in open quantum systems using quantum error-correcting (QEC) codes that are explicitly embedded into microscopic system-bath models. Instead of abstract quantum channels, we consider multi-qubit registers coupled to bosonic thermal environments, derive a second-order master equation for the reduced dynamics, and use it to benchmark the five-qubit, Steane, and toric codes under local and collective noise. We compute state fidelities for logical qubits as functions of coupling strength, bath temperature, and the number of correction cycles. In the low-temperature regime, we find that repeated error-correction with the five-qubit code strongly suppresses decoherence and relaxation, while in the high-temperature regime, thermal excitations dominate the dynamics and reduce the benefit of all codes, though the five-qubit code still outperforms the Steane and toric codes. For two-qubit Werner states, we identify a critical evolution time before which QEC does not improve fidelity, and this time increases as entanglement grows. After this critical time, QEC does improve fidelity. Comparative analysis further reveals that the five-qubit code (the smallest perfect code) offers consistently higher fidelities than topological and concatenated architectures in these open-system settings. These findings establish a quantitative framework for evaluating QEC under realistic noise environments and provide guidance for developing noise-resilient quantum architectures in near-term quantum technologies.

</details>


### [46] [Coherence Limits in Interference-Based cos(2$\varphi$) Qubits](https://arxiv.org/abs/2601.10209)
*S. Messelot,A. Leblanc,J. -S. Tettekpoe,F. Lefloch,Q. Ficheux,J. Renard,É. Dumur*

Main category: quant-ph

TL;DR: 该研究分析了基于约瑟夫森干涉的cos(2φ)量子比特的相干特性，发现存在电荷和磁通噪声退相干之间的基本权衡，限制了此类量子比特的实际性能。


<details>
  <summary>Details</summary>
Motivation: 研究具有宇称保护的cos(2φ)量子比特的相干特性，这类量子比特通过抑制单库珀对隧穿提供保护，但实际性能限制尚不清楚。

Method: 将各种cos(2φ)量子比特实现统一描述为SQUID几何中的两个多谐波约瑟夫森结，通过数值模拟分析弛豫和退相干速率对外部磁通和电路参数的依赖关系。

Result: 发现电荷和磁通噪声退相干通道之间存在基本权衡，当前参数下量子比特寿命T1可超过毫秒，但退相干时间Tφ仅限几微秒，受磁通或电荷噪声限制。

Conclusion: 建立了此类量子比特相干性的实际限制，对该方法的长期潜力提出了疑问，需要进一步改进以克服电荷和磁通噪声之间的权衡。

Abstract: We investigate the coherence properties of parity-protected $\cos(2\varphi)$ qubits based on interferences between two Josephson elements in a superconducting loop. We show that qubit implementations of a $\cos(2\varphi)$ potential using a single loop, such as those employing semiconducting junctions, rhombus circuits, flowermon and KITE structures, can be described by the same Hamiltonian as two multi-harmonic Josephson junctions in a SQUID geometry. We find that, despite the parity protection arising from the suppression of single Cooper pair tunneling, there exists a fundamental trade-off between charge and flux noise dephasing channels. Using numerical simulations, we examine how relaxation and dephasing rates depend on external flux and circuit parameters, and we identify the best compromise for maximum coherence. With currently existing circuit parameters, the qubit lifetime $T_1$ can exceed milliseconds while the dephasing time $T_\varphi$ remains limited to only a few microseconds due to either flux or charge noise. Our findings establish practical limits on the coherence of this class of qubits and raise questions about the long-term potential of this approach.

</details>


### [47] [Quantitative approach for the Dicke-Ising chain with an effective self-consistent matter Hamiltonian](https://arxiv.org/abs/2601.10210)
*J. Leibig,M. Hörmann,A. Langheld,A. Schellenberger,K. P. Schmidt*

Main category: quant-ph

TL;DR: Dicke-Ising链在热力学极限下可映射为有效的自洽物质哈密顿量，光子场仅作为自洽有效场，无需考虑光子与自旋的量子关联即可确定量子相图。通过NLCE+DMRG方法求解该自洽哈密顿量，精确确定了相图，改进了铁磁耦合中多临界点位置，证实了反铁磁超辐射相的存在。


<details>
  <summary>Details</summary>
Motivation: 研究Dicke-Ising链的量子相图，传统方法需要考虑光子与自旋的量子关联，计算复杂。本文发现热力学极限下系统可简化为有效的自洽物质哈密顿量，光子场仅作为自洽有效场，从而简化计算并提高精度。

Method: 将Dicke-Ising链映射为有效的自洽物质哈密顿量，光子场作为自洽有效场。采用数值链接团簇展开结合密度矩阵重整化群计算（NLCE+DMRG）求解该自洽哈密顿量，在热力学极限下确定量子相图。

Result: 1. 铁磁Ising耦合：改进了超辐射相变阶数变化的多临界点位置，相对精度达到10^{-4}。2. 反铁磁Ising耦合：在热力学极限下证实了狭窄的反铁磁超辐射相存在，该相被识别为具有纵向场的反铁磁横向场Ising模型的多体基态。3. 揭示了相变机制：反铁磁超辐射相通过连续Dicke型极化子凝聚从反铁磁正常相产生，随后通过一级相变转变为顺磁超辐射相。

Conclusion: 通过将Dicke-Ising链映射为有效的自洽物质哈密顿量，并采用NLCE+DMRG方法求解，实现了对一维Dicke-Ising相图的精确确定。该方法避免了光子与自旋量子关联的复杂计算，为理解这类系统的量子相图提供了高效精确的计算框架。

Abstract: In the thermodynamic limit, the Dicke-Ising chain maps exactly onto an effective self-consistent matter Hamiltonian with the photon field acting solely as a self-consistent effective field. As a consequence, no quantum correlations between photons and spins are needed to understand the quantum phase diagram. This enables us to determine the quantum phase diagram in the thermodynamic limit using numerical linked-cluster expansions combined with density matrix renormalization group calculations (NLCE+DMRG) to solve the resulting self-consistent matter Hamiltonian. This includes magnetically ordered phases with significantly improved accuracy compared to previous estimates. For ferromagnetic Ising couplings, we refine the location of the multicritical point governing the change in the order of the superradiant phase transition, reaching a relative accuracy of $10^{-4}$. For antiferromagnetic Ising couplings, we confirm the existence of the narrow antiferromagnetic superradiant phase in the thermodynamic limit. The effective matter Hamiltonian framework identifies the antiferromagnetic superradiant phase as the many-body ground state of an antiferromagnetic transverse-field Ising model with longitudinal field. This phase emerges through continuous Dicke-type polariton condensation from the antiferromagnetic normal phase, followed by a first-order transition to the paramagnetic superradiant phase. Thus, NLCE+DMRG provides a precise determination of the Dicke-Ising phase diagram in one dimension by solving the self-consistent effective matter Hamiltonian.

</details>


### [48] [Adversarial Hypothesis Testing for Quantum Channels](https://arxiv.org/abs/2601.10243)
*Masahito Hayashi,Hao-Chung Cheng,Li Gao*

Main category: quant-ph

TL;DR: 本文系统研究了量子-量子(QQ)和经典-量子(CQ)信道的对抗性假设检验，发现CQ信道在对抗性假设检验中并不只是QQ信道的特例，表现出独特行为。


<details>
  <summary>Details</summary>
Motivation: 传统信道区分通常假设发送方合作，而本文研究对抗性场景，其中发送方Alice选择信道输入以最小化接收方Bob的区分能力。这种对抗性框架更符合实际安全通信场景，需要理解在这种对抗设置下量子信道的区分能力。

Method: 研究了四种设置：基于Alice使用i.i.d.输入或一般输入，以及Bob是否被告知具体输入选择（允许其测量依赖于输入）。通过分析这四种设置下的Stein指数来表征对抗性假设检验的性能。

Result: 发现QQ信道和CQ信道在对抗性假设检验中表现出显著差异：对于QQ信道，当使用i.i.d.输入时，Bob知道输入能显著增强区分能力，但当允许一般输入时这种优势消失；对于CQ信道，Bob被告知输入在i.i.d.和一般输入情况下都能提供相对于相应纠缠破坏信道的持续优势。

Conclusion: 对抗性假设检验中CQ信道并不只是QQ信道的特例，表现出独特现象。这些结果揭示了量子信道在对抗性环境中的基本特性差异，对量子信息处理和量子密码学具有重要意义。

Abstract: This paper presents a systematic study of adversarial hypothesis testing for both quantum-quantum (QQ) and classical-quantum (CQ) channels. Unlike conventional channel discrimination, we consider a framework where the sender, Alice, selects the channel input adversarially to minimize Bob's distinguishability. We analyze this problem across four settings based on whether Alice employs i.i.d. or general inputs and whether the receiver, Bob, is informed of the specific input choice (allowing his measurement to depend on the input). We characterize the Stein exponents for each setting and reveal a striking distinction in behavior: for QQ channels with i.i.d. inputs, Bob's knowledge of the input significantly enhances distinguishability, yet this advantage vanishes when general inputs are permitted. In contrast, for CQ channels, Bob being informed provides a consistent advantage over the corresponding entanglement-breaking channels for both i.i.d. and general inputs. These results demonstrate a unique phenomenon in adversarial hypothesis testing where the CQ channel does not merely behave as a special case of the QQ channel.

</details>


### [49] [Optimal control of a dissipative micromaser quantum battery in the ultrastrong coupling regime](https://arxiv.org/abs/2601.10281)
*Maristella Crotti,Luca Razzoli,Luigi Giannelli,Giuseppe A. Falci,Giuliano Benenti*

Main category: quant-ph

TL;DR: 研究超强耦合下微脉泽量子电池在环境耗散中的开放系统动力学，通过优化控制和耗散管理实现高效充电和长期稳定性。


<details>
  <summary>Details</summary>
Motivation: 探索在超强耦合（USC）机制下量子电池的性能，特别是考虑环境耗散的影响。传统量子电池研究中，USC机制的反旋转项能显著提高充电速度，但也会导致能量无限增长和高度混合的腔态，需要研究耗散如何影响这些效应。

Method: 采用微脉泽量子电池模型：单模电磁腔通过Rabi哈密顿量与一系列作为充电器的量子比特顺序相互作用。考虑系统与热浴的弱耦合引入耗散效应。通过优化控制策略：1）优化充电协议最大化存储的功；2）基于测量的被动反馈策略稳定存储的功对抗耗散损失。

Result: 数值结果表明：1）耗散在每次量子比特-腔相互作用期间减轻了USC机制的有害效应，产生有限能量和功的稳态；2）反旋转项显著提高充电速度；3）优化控制策略（充电协议和被动反馈）能同时增强充电性能和长期稳定性。

Conclusion: 超强光-物质耦合、受控耗散和优化控制策略的相互作用使微脉泽量子电池在现实条件下既能实现增强的充电性能，又能保持长期稳定性，为实用量子能量存储系统提供了有前景的途径。

Abstract: We investigate the open system dynamics of a micromaser quantum battery operating in the ultrastrong coupling (USC) regime under environmental dissipation. The battery consists of a single-mode electromagnetic cavity sequentially interacting, via the Rabi Hamiltonian, with a stream of qubits acting as chargers. Dissipative effects arise from the weak coupling of the qubit-cavity system to a thermal bath. Non-negligible in the USC regime, the counter-rotating terms substantially improve the charging speed, but also lead, in the absence of dissipation, to unbounded energy growth and highly mixed cavity states. Dissipation during each qubit-cavity interaction mitigates these detrimental effects, yielding steady-state of finite energy and ergotropy. Optimal control on qubit preparation and interaction times enhances battery's performance in: (i) Maximizing the stored ergotropy trhough an optimized charging protocol; (ii) Stabilizing the stored ergotropy against dissipative losses through an optimized measurement-based passive-feedback strategy. Overall, our numerical results demonstrate that the interplay of ultrastrong light-matter coupling, controlled dissipation, and optimized control strategies enables micromaser quantum batteries to achieve both enhanced charging performance and long-term stability under realistic conditions.

</details>


### [50] [Exponential improvement in benchmarking multiphoton interference](https://arxiv.org/abs/2601.10289)
*Rodrigo M. Sanz,Emilio Annoni,Stephen C. Wein,Carmen G. Almudever,Shane Mansfield,Ellen Derbyshire,Rawad Mezher*

Main category: quant-ph

TL;DR: 提出使用量子傅里叶变换干涉仪的新协议，用于基准测试多光子不可区分性，相比现有方法实现了指数级改进的样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 多光子量子技术需要生成多个不可区分光子，但现有的多光子不可区分性测试协议样本复杂度随光子数指数增长，限制了可扩展性。

Method: 通过新定理强化对不可区分性与量子傅里叶变换干涉仪抑制定律关系的理解，提出使用QFT干涉仪的新协议来基准测试真实n光子不可区分性。

Result: 新协议对素数光子数实现常数样本复杂度，其他情况实现次多项式缩放，相比现有方法有指数级改进。在实验验证中显示出运行时间和精度的明显优势。

Conclusion: 建立了首个可扩展的多光子不可区分性计算方法，适用于当前和近期的光子量子硬件。

Abstract: Several photonic quantum technologies rely on the ability to generate multiple indistinguishable photons. Benchmarking the level of indistinguishability of these photons is essential for scalability. The Hong-Ou-Mandel dip provides a benchmark for the indistinguishability between two photons, and extending this test to the multi-photon setting has so far resulted in a protocol that computes the genuine n-photon indistinguishability (GI). However, this protocol has a sample complexity that increases exponentially with the number of input photons for an estimation of GI up to a given additive error. To address this problem, we introduce new theorems that strengthen our understanding of the relationship between distinguishability and the suppression laws of the quantum Fourier transform interferometer (QFT). Building on this, we propose a protocol using the QFT for benchmarking GI that achieves constant sample complexity for the estimation of GI up to a given additive error for prime photon numbers, and sub-polynomial scaling otherwise, representing an exponential improvement over the state of the art. We prove the optimality of our protocol in many relevant scenarios and validate our approach experimentally on Quandela's reconfigurable photonic quantum processor, where we observe a clear advantage in runtime and precision over the state of the art. We therefore establish the first scalable method for computing multi-photon indistinguishability, which applies naturally to current and near-term photonic quantum hardware.

</details>


### [51] [Complex scalar relativistic field as a probability amplitude](https://arxiv.org/abs/2601.10302)
*Yu. M. Poluektov*

Main category: quant-ph

TL;DR: 提出中性复场的相对论性概率幅方程，得到概率密度的连续性方程，发现两类激发分别对应正能粒子和不同色散关系，基于拉格朗日形式得到守恒律，并考虑二次量子化


<details>
  <summary>Details</summary>
Motivation: 为中性复场建立相对论性概率幅描述框架，探索其作为量子力学波函数的可能性，并研究其基本物理性质

Method: 提出中性复场的相对论性方程，推导概率密度连续性方程，分析场激发类型，基于拉格朗日形式推导守恒定律，最后考虑二次量子化

Result: 得到概率密度的连续性方程，发现两类激发分别描述具有正能量的粒子但具有不同色散关系，从拉格朗日量得到守恒定律，建立了二次量子化框架

Conclusion: 成功构建了中性复场的相对论性概率幅理论框架，为描述这类量子系统提供了完整的数学和物理基础

Abstract: A relativistic equation for a neutral complex field as a probability amplitude is proposed. The continuity equation for the probability density is obtained. It is shown that there are two types of excitations of this field, which describe particles with positive energy and different dispersion laws. Based on the Lagrangian formalism, conservation laws are obtained. The transition to secondary quantization is considered.

</details>


### [52] [Addition to the dynamic Stark shift of the coherent population trapping resonance](https://arxiv.org/abs/2601.10319)
*Gavriil Voloshin,Konstantin Barantsev,Andrey Litvinov*

Main category: quant-ph

TL;DR: 该论文研究了光诱导相干布居囚禁共振的频移，提出了包含额外激发态能级的Λ型原子系统理论模型，揭示了除传统动态斯塔克位移外，还存在由非共振跃迁导致的额外频移。


<details>
  <summary>Details</summary>
Motivation: 研究光诱导相干布居囚禁共振的频移机制，特别是在量子频率标准等精密原子器件中，需要深入理解非共振跃迁对共振线形的影响，以更好地控制光频移效应。

Method: 提出了一个包含额外激发态能级的Λ型原子系统解析模型，考虑双色激光辐射与非共振原子跃迁的相互作用，分析了弱耦合和强耦合两种机制。

Result: 发现除了传统的动态斯塔克位移外，还存在由共振线形畸变引起的额外频移；在弱耦合极限下推导了该额外频移的解析表达式，并证明在强耦合条件下该频移与光强度的关系显著偏离线性。

Conclusion: 该研究揭示了非共振跃迁对相干布居囚禁共振频移的重要影响，为精密原子器件（如量子频率标准）中光频移的控制提供了新的理论依据和调控可能性。

Abstract: This paper presents a theoretical study of the light-induced shift of the coherent population trapping resonance. An analytical model is proposed that describes the interaction of two radiation components with an atomic system using a $Λ$ scheme and takes into account an additional level of excited state. Both weak and strong coupling regimes with off-resonant transitions are considered. It is shown that, in addition to the conventional dynamic Stark shift, an extra shift arises due to the distortion of the resonance line shape when bichromatic laser radiation interacts with off-resonant atomic transitions. An analytical expression for this additional shift is derived in the weak-coupling limit, and its significant impact on the resonance shape and sensitivity to the intensities of the laser field components is demonstrated. It is found that under strong coupling conditions, the additional shift can deviate substantially from a linear dependence on light intensity, suggesting new opportunities for controlling light shifts in precision atomic devices such as quantum frequency standards.

</details>


### [53] [Principles of Optics in the Fock Space: Scalable Manipulation of Giant Quantum States](https://arxiv.org/abs/2601.10325)
*Yifang Xu,Yilong Zhou,Ziyue Hua,Lida Sun,Jie Zhou,Weiting Wang,Weizhou Cai,Hongwei Huang,Lintao Xiao,Guangming Xue,Haifeng Yu,Ming Li,Chang-Ling Zou,Luyan Sun*

Main category: quant-ph

TL;DR: 提出"Fock空间光学"概念框架，将光子数作为合成维度，在微波谐振腔中实验演示了Fock空间的光学类比现象，建立了单玻色子模式薛定谔演化与经典傍轴波传播的基本对应关系。


<details>
  <summary>Details</summary>
Motivation: 经典光学的波光学原理在时空域提供了优雅且可扩展的光控制，但Fock空间的量子态工程主要局限于少光子体系，受限于大希尔伯特空间的计算和实验挑战。

Method: 引入"Fock空间光学"概念框架，将光子数视为合成维度。使用超导微波谐振腔实验演示Fock空间中的光学类比：传播、折射、透镜、色散和干涉，最多可达180个光子。

Result: 成功建立了单玻色子模式薛定谔演化与经典傍轴波传播的基本对应关系，实现了对多达180个光子的高维量子态的可扩展控制。

Conclusion: 通过将直观的光学概念映射到高维量子态工程，这项工作为大规模量子系统（数千个光子）的可扩展控制和先进玻色子信息处理开辟了新路径。

Abstract: The manipulation of distinct degrees of freedom of photons plays a critical role in both classical and quantum information processing. While the principles of wave optics provide elegant and scalable control over classical light in spatial and temporal domains, engineering quantum states in Fock space has been largely restricted to few-photon regimes, hindered by the computational and experimental challenges of large Hilbert spaces. Here, we introduce ``Fock-space optics", establishing a conceptual framework of wave propagation in the quantum domain by treating photon number as a synthetic dimension. Using a superconducting microwave resonator, we experimentally demonstrate Fock-space analogues of optical propagation, refraction, lensing, dispersion, and interference with up to 180 photons. These results establish a fundamental correspondence between Schrödinger evolution in a single bosonic mode and classical paraxial wave propagation. By mapping intuitive optical concepts onto high-dimensional quantum state engineering, our work opens a path toward scalable control of large-scale quantum systems with thousands of photons and advanced bosonic information processing.

</details>


### [54] [Realistic prospects for testing a relativistic local quantum measurement inequality](https://arxiv.org/abs/2601.10354)
*Riccardo Falcone,Claudio Conti*

Main category: quant-ph

TL;DR: 研究如何通过实验测试一个相对论性局域量子测量不等式，该不等式量化了有限尺寸探测器对真空不敏感性与对激发响应性之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是探索量子测量理论中的基本限制，特别是在相对论性局域量子场论背景下，研究探测器尺寸和测量时间窗口如何影响测量精度。这有助于理解量子测量中的基本权衡关系，并为实际光探测实验提供理论指导。

Method: 基于Reeh-Schlieder近似处理相干态，推导出适用于任意相干态的明确且实用的界限。为了与现实光探测场景连接，将探测区域建模为在有限时间窗口内工作的方形棱柱，并考虑正入射的单模相干态。通过数值计算验证理论结果。

Result: 数值结果显示了预期的定性行为：抑制暗计数必然会导致可实现的点击概率变紧。这表明在量子测量中存在基本权衡，即提高探测器对真空的不敏感性会降低其对真实激发的响应能力。

Conclusion: 该研究为实验测试相对论性局域量子测量不等式提供了理论基础和具体方法，揭示了量子测量中真空不敏感性与激发响应性之间的基本权衡关系，对实际量子探测器的设计和优化具有指导意义。

Abstract: We investigate the experimental prospects for testing a relativistic local quantum measurement inequality that quantifies the trade-off between vacuum insensitivity and responsiveness to excitations for finite-size detectors. Building on the Reeh--Schlieder approximation for coherent states, we derive an explicit and practically applicable bound for arbitrary coherent states. To connect with realistic photodetection scenarios, we model the detection region as a square prism operating over a finite time window and consider a normally incident single-mode coherent state. Numerical results exhibit the expected qualitative behavior: suppressing dark counts necessarily tightens the achievable click probability.

</details>


### [55] [Learning Hamiltonians in the Heisenberg limit with static single-qubit fields](https://arxiv.org/abs/2601.10380)
*Shrigyan Brahmachari,Shuchen Zhu,Iman Marvian,Yu Tong*

Main category: quant-ph

TL;DR: 提出一种仅使用强度与目标精度无关的静态单比特控制场就能实现海森堡极限标度的哈密顿量学习协议，克服了现有协议对多比特操作或高频率/强度单比特控制的需求限制。


<details>
  <summary>Details</summary>
Motivation: 现有海森堡极限哈密顿量学习协议要么需要易受噪声影响的多比特操作，要么需要频率或强度随精度增加的单比特操作，这限制了它们在近期量子平台上的应用。

Method: 使用强度与目标精度无关的静态单比特控制场（静态场），构建对状态制备和测量误差鲁棒的学习协议，通过数学证明和数值实验验证其有效性。

Result: 协议实现了最优的海森堡极限标度，且对SPAM误差具有鲁棒性，信息论下界证明除非使用大量离散控制操作，否则非零静态场强度是实现海森堡极限的必要条件。

Conclusion: 该协议克服了现有哈密顿量学习方法的局限性，为量子设备表征和量子传感提供了新工具，特别适用于近期量子平台。

Abstract: Learning the Hamiltonian governing a quantum system is a central task in quantum metrology, sensing, and device characterization. Existing Heisenberg-limited Hamiltonian learning protocols either require multi-qubit operations that are prone to noise, or single-qubit operations whose frequency or strength increases with the desired precision. These two requirements limit the applicability of Hamiltonian learning on near-term quantum platforms. We present a protocol that learns a quantum Hamiltonian with the optimal Heisenberg-limited scaling using only single-qubit control in the form of static fields with strengths that are independent of the target precision. Our protocol is robust against the state preparation and measurement (SPAM) error. By overcoming these limitations, our protocol provides new tools for device characterization and quantum sensing. We demonstrate that our method achieves the Heisenberg-limited scaling through rigorous mathematical proof and numerical experiments. We also prove an information-theoretic lower bound showing that a non-vanishing static field strength is necessary for achieving the Heisenberg limit unless one employs an extensive number of discrete control operations.

</details>


### [56] [Experimental Realization of Rabi-Driven Reset for Fast Cooling of a High-Q Cavity](https://arxiv.org/abs/2601.10385)
*Eliya Blumenthal,Natan Karaev,Shay Hacohen-Gourgy*

Main category: quant-ph

TL;DR: 提出Rabi驱动重置(RDR)技术，通过强Rabi驱动和边带驱动将色散相互作用转化为有效Jaynes-Cummings耦合，实现超导腔模式的连续、无测量冷却，解决高Q玻色子存储器快速重置瓶颈。


<details>
  <summary>Details</summary>
Motivation: 高Q玻色子存储器对硬件高效的量子纠错至关重要，但其隔离性使得快速、高保真重置成为持续瓶颈。现有方法要么依赖弱模间交叉Kerr转换，要么依赖具有显著延迟的基于测量的序列。

Method: 提出Rabi驱动重置(RDR)：在transmon上施加强共振Rabi驱动，同时在存储器和读取模式上施加与Rabi频率失谐的边带驱动，将色散相互作用转化为量子比特修饰态与各模式之间的有效Jaynes-Cummings耦合，从而创建从存储器到冷读取浴的可调谐耗散通道。

Result: 单光子衰减时间1.2μs，比本征寿命快两个数量级以上；在约80μs内重置约30个热光子，达到稳态平均光子数0.045±0.025。

Conclusion: RDR技术实现了超导腔模式的快速、连续、无测量冷却，解决了高Q玻色子存储器重置瓶颈，特别适用于故意抑制直接模间耦合的弱耦合架构。

Abstract: High-Q bosonic memories are central to hardware-efficient quantum error correction, but their isolation makes fast, high-fidelity reset a persistent bottleneck. Existing approaches either rely on weak intermode cross-Kerr conversion or on measurement-based sequences with substantial latency. Here we demonstrate a hardware-efficient Rabi-Driven Reset (RDR) that implements continuous, measurement-free cooling of a superconducting cavity mode. A strong resonant Rabi drive on a transmon, together with sideband drives on the memory and readout modes detuned by the Rabi frequency, converts the dispersive interaction into an effective Jaynes-Cummings coupling between the qubit dressed states and each mode. This realizes a tunable dissipation channel from the memory to the cold readout bath. Crucially, the engineered coupling scales with the qubit-mode dispersive interaction and the drive amplitude, rather than with the intermode cross-Kerr, enabling fast cooling even in very weakly coupled architectures that deliberately suppress direct mode-mode coupling. We demonstrate RDR of a single photon with a decay time of $1.2 μs$, more than two orders of magnitude faster than the intrinsic lifetime. Furthermore, we reset about 30 thermal photons in about $80 μs$ to a steady-state average photon number of $\bar{n} = 0.045 \pm 0.025$.

</details>


### [57] [A Collection of Pinsker-type Inequalities for Quantum Divergences](https://arxiv.org/abs/2601.10395)
*Kläre Wienecke,Gereon Koßmann,René Schwonnek*

Main category: quant-ph

TL;DR: 该论文为多种量子/经典散度（包括f-散度、Rényi散度等）建立了与迹距离相关的下界估计，并提供了如何将这些界限推广到平滑散度的策略。


<details>
  <summary>Details</summary>
Motivation: Pinsker不等式为量子态的Umegaki散度与迹距离之间建立了下界关系，但其他类型的散度（如f-散度、Rényi散度等）缺乏类似的估计。本文旨在为更广泛的散度类型建立类似的界限关系。

Method: 为多种量子和经典散度（包括Hellinger散度、χ²散度、Rényi散度及其特例如Umegaki散度、碰撞散度、最大散度等）制定与迹距离相关的下界估计。同时提供将这些界限扩展到平滑散度的策略方法。

Result: 建立了多种散度与迹距离之间的下界关系，为量子信息理论中的散度分析提供了更全面的数学工具。特别地，将Pinsker不等式的思想推广到了更广泛的散度类别。

Conclusion: 成功扩展了Pinsker不等式的适用范围，为量子信息处理中的散度分析提供了更丰富的数学框架，这些结果对于量子态区分、量子信息论和量子计算等领域具有重要意义。

Abstract: Pinsker's inequality sets a lower bound on the Umegaki divergence of two quantum states in terms of their trace distance. In this work, we formulate corresponding estimates for a variety of quantum and classical divergences including $f$-divergences like Hellinger and $χ^2$-divergences as well as Rényi divergences and special cases thereof like the Umegaki divergence, collision divergence, max divergence. We further provide a strategy on how to adapt these bounds to smoothed divergences.

</details>


### [58] [Bounding many-body properties under partial information and finite measurement statistics](https://arxiv.org/abs/2601.10408)
*Luke Mortimer,Leonardo Zambrano,Antonio Acín,Donato Farina*

Main category: quant-ph

TL;DR: 提出一种可扩展的量子多体系统性质边界计算方法，利用矩矩阵松弛技术，结合系统特定知识（如哈密顿量基态、对称性等），通过半定规划松弛和含噪声的实验测量实现实际认证方案。


<details>
  <summary>Details</summary>
Motivation: 量子多体系统性质的边界计算对于理解涌现量子现象至关重要，但现有方法在量子比特数量增加时难以扩展。需要开发既能利用有限测量数据，又能适应系统特定知识，且能处理实验噪声的可扩展认证方法。

Method: 采用矩矩阵松弛技术替代传统半定规划方法，使计算在量子比特数量上可扩展。方法可结合系统特定知识进行适配，如哈密顿量基态、对称性约束或Lindbladian稳态。通过半定规划松弛和含噪声的实验测量实现实际认证。

Result: 提出了一种可扩展的量子多体系统性质边界计算框架，能够处理有限测量数据中的噪声，并灵活结合各种系统特定约束，为实际量子系统的实验认证提供了可行的方案。

Conclusion: 该方法通过矩矩阵松弛技术实现了量子比特数量上的可扩展性，结合系统特定知识和含噪声的实验测量，为量子多体系统的实际认证提供了有效的半定规划框架，有望推动量子系统性质边界计算的实际应用。

Abstract: Calculating bounds of properties of many-body quantum systems is of paramount importance, since they guide our understanding of emergent quantum phenomena and complement the insights obtained from estimation methods. Recent semidefinite programming approaches enable probabilistic bounds from finite-shot measurements of easily accessible, yet informationally incomplete, observables. Here we render these methods scalable in the number of qubits by instead utilizing moment-matrix relaxations. After introducing the general formalism, we show how the approach can be adapted with specific knowledge of the system, such as it being the ground state of a given Hamiltonian, possessing specific symmetries or being the steady state of a given Lindbladian. Our approach defines a scalable real-world certification scheme leveraging semidefinite programming relaxations and experimental estimations which, unavoidably, contain shot noise.

</details>


### [59] [Tight bounds on recurrence time in closed quantum systems](https://arxiv.org/abs/2601.10409)
*Marcin Kotowski,Michał Oszmaniec*

Main category: quant-ph

TL;DR: 该论文建立了量子系统回归时间的上界，揭示了回归时间与希尔伯特空间维度、邻域大小和逃逸时间的关系，并分析了初始态相干性对回归行为的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管量子系统的回归现象具有基本重要性，但对其定量理解一直缺乏。本文旨在建立回归时间的严格上界，深化对量子动力学基本性质的理解。

Method: 通过将逃逸时间估计转化为逆量子速度极限问题，利用哈密顿量方差分析逃逸时间，建立回归时间上界，并通过随机哈密顿量验证上界的饱和性。

Result: 建立了回归时间上界 t_rec ≲ t_exit(ε)(1/ε)^d，其中逃逸时间 t_exit(ε) ≈ ε/√Δ(H^2)。该上界在随机哈密顿量下通常饱和，初始态相干性显著影响回归行为。

Conclusion: 本文首次为量子回归时间提供了严格的定量上界，揭示了回归时间与系统参数的基本关系，为理解量子动力学中的时间尺度提供了新视角。

Abstract: The evolution of an isolated quantum system inevitably exhibits recurrence: the state returns to the vicinity of its initial condition after finite time. Despite its fundamental nature, a rigorous quantitative understanding of recurrence has been lacking. We establish upper bounds on the recurrence time, $t_{\mathrm{rec}} \lesssim t_{\mathrm{exit}}(ε)(1/ε)^d$, where $d$ is the Hilbert-space dimension, $ε$ the neighborhood size, and $t_{\mathrm{exit}}(ε)$ the escape time from this neighborhood. For pure states evolving under a Hamiltonian $H$, estimating $t_{\mathrm{exit}}$ is equivalent to an inverse quantum speed limit problem: finding upper bounds on the time a time-evolved state $ψ_t$ needs to depart from the $ε$-vicinity of the initial state $ψ_0$. We provide a partial solution, showing that under mild assumptions $t_{\mathrm{exit}}(ε) \approx ε/\sqrt{ Δ(H^2)}$, with $Δ(H^2)$ the Hamiltonian variance in $ψ_0$. We show that our upper bound on $t_{\mathrm{rec}}$ is generically saturated for random Hamiltonians. Finally, we analyze the impact of coherence of the initial state in the eigenbasis of $H$ on recurrence behavior.

</details>


### [60] [Unifying Quantum and Classical Dynamics](https://arxiv.org/abs/2601.10423)
*Abdul Rahaman Shaikh,Tabish Qureshi*

Main category: quant-ph

TL;DR: 该论文展示了量子力学与经典力学动力学方程的精确等价性，证明海森堡运动方程可以转化为与牛顿运动方程完全相同的形式，其中不含普朗克常数ħ。


<details>
  <summary>Details</summary>
Motivation: 量子物理被视为比经典物理更基本的理论，经典力学应在某些极限条件下从量子力学中涌现，但这一直是个挑战性目标。本文旨在探索统一经典与量子物理动力学的可能性。

Method: 将海森堡运动方程重新表述，证明其可以转化为与牛顿运动方程完全相同的形式，其中普朗克常数ħ从公式中消失，海森堡算符取代经典可观测量。

Result: 展示了量子可观测量动力学与其经典对应物之间的精确等价性，证明量子与经典动力学由相同的方程支配，区别仅在于海森堡算符取代了经典可观测量。

Conclusion: 量子力学与经典力学的动力学方程在形式上完全等价，这为理解两种理论之间的关系提供了新的视角，表明经典行为并非从量子力学中涌现，而是动力学方程的精确对应。

Abstract: Classical and quantum physics represent two distinct theories; however, quantum physics is regarded as the more fundamental of the two. It is posited that classical mechanics should arise from quantum mechanics under certain limiting conditions. Nevertheless, this remains a challenging objective. In this work, we explore the potential for unifying the dynamics of classical and quantum physics. This discussion does not suggest that classical behavior emerges from quantum mechanics; rather, it demonstrates the exact equivalence between the dynamics of quantum observables and their classical counterparts. It is shown that the Heisenberg equations of motion can be cast in a form that is identical to Newton's equations of motion, with $\hbar$ being absent from the formulation. This implies that both quantum and classical dynamics are governed by the same equations, with the Heisenberg operators substituting the classical observables.

</details>


### [61] [Reduction of thermodynamic uncertainty by a virtual qubit](https://arxiv.org/abs/2601.10429)
*Yang Li,Fu-Lin Zhang*

Main category: quant-ph

TL;DR: 量子热机模型中热力学不确定关系（TUR）的分析，揭示了经典贡献与相干贡献的分解，相干贡献在共振条件下可为负值，使系统能够超越经典TUR界限。


<details>
  <summary>Details</summary>
Motivation: 研究量子系统违反经典热力学不确定关系（TUR）的现象，揭示纯量子热力学效应，这对于提升量子技术性能及优化至关重要。

Method: 分析一类典型量子热机模型，其运行依赖于两个能级之间形成虚拟量子比特的相干耦合。稳态相干被限制在虚拟量子比特子空间中，而无相干耦合时系统满足热库的细致平衡条件且无稳态热流。

Result: 稳态电流和熵产生可由有效经典马尔可夫过程完全再现，而电流涨落获得源于相干的额外纯量子修正。热力学不确定关系自然分解为经典（对角）贡献和相干贡献，后者在共振条件下为负值，并在最大化稳态相干的耦合强度处达到最小值。

Conclusion: 确定了优化条件及在可逆极限附近超越经典TUR界限的判据，揭示了相干在量子热力学不确定关系中的关键作用。

Abstract: The thermodynamic uncertainty relation (TUR) imposes a fundamental constraint between current fluctuations and entropy production, providing a refined formulation of the second law for micro- and nanoscale systems. Quantum violations of the classical TUR reveal genuinely quantum thermodynamic effects, which are essential for improving performance and enabling optimization in quantum technologies. In this work, we analyze the TUR in a class of paradigmatic quantum thermal-machine models whose operation is enabled by coherent coupling between two energy levels forming a virtual qubit. Steady-state coherences are confined to this virtual-qubit subspace, while in the absence of coherent coupling the system satisfies detailed balance with the thermal reservoirs and supports no steady-state heat currents. We show that the steady-state currents and entropy production can be fully reproduced by an effective classical Markov process, whereas current fluctuations acquire an additional purely quantum correction originating from coherence. As a result, the thermodynamic uncertainty naturally decomposes into a classical (diagonal) contribution and a coherent contribution. The latter becomes negative under resonant conditions and reaches its minimum at the coupling strength that maximizes steady-state coherence. We further identify the optimization conditions and the criteria for surpassing the classical TUR bound in the vicinity of the reversible limit.

</details>


### [62] [The SpinPulse library for transpilation and noise-accurate simulation of spin qubit quantum computers](https://arxiv.org/abs/2601.10435)
*Benoît Vermersch,Oscar Gravier,Nathan Miscopein,Julia Guignon,Carlos Ramos Marimón,Jonathan Durandau,Matthieu Dartiailh,Tristan Meunier,Valentin Savin*

Main category: quant-ph

TL;DR: SpinPulse是一个开源的Python包，用于在脉冲级别模拟基于自旋量子比特的量子计算机，包含经典非马尔可夫噪声，支持硬件开发。


<details>
  <summary>Details</summary>
Motivation: 需要为自旋量子比特量子计算机提供脉冲级别的模拟工具，特别是包含经典非马尔可夫噪声，以支持硬件开发和量子电路设计。

Method: SpinPulse将量子电路首先编译到模型的原生门集，然后转换为脉冲序列，在模拟的噪声实验环境中进行数值积分。集成了张量网络库quimb进行大规模模拟。

Result: 展示了包括编译、脉冲级编译、硬件基准测试、量子误差缓解和大规模模拟在内的工作流程，能够进行现实的量子电路模拟。

Conclusion: SpinPulse将成为量子计算社区的宝贵开源工具，有助于设计高保真量子电路和改进量子误差缓解与校正策略。

Abstract: We introduce SpinPulse, an open-source python package for simulating spin qubit-based quantum computers at the pulse-level. SpinPulse models the specific physics of spin qubits, particularly through the inclusion of classical non-Markovian noise. This enables realistic simulations of native gates and quantum circuits, in order to support hardware development. In SpinPulse, a quantum circuit is first transpiled into the native gate set of our model and then converted to a pulse sequence. This pulse sequence is subsequently integrated numerically in the presence of a simulated noisy experimental environment. We showcase workflows including transpilation, pulse-level compilation, hardware benchmarking, quantum error mitigation, and large-scale simulations via integration with the tensor-network library quimb. We expect SpinPulse to be a valuable open-source tool for the quantum computing community, fostering efforts to devise high-fidelity quantum circuits and improved strategies for quantum error mitigation and correction.

</details>


### [63] [Minimal-Energy Optimal Control of Tunable Two-Qubit Gates in Superconducting Platforms Using Continuous Dynamical Decoupling](https://arxiv.org/abs/2601.10446)
*Adonai Hilário da Silva,Octávio da Motta,Leonardo Kleber Castelano,Reginaldo de Jesus Napolitano*

Main category: quant-ph

TL;DR: 提出结合连续动态解耦和变分最小能量最优控制的统一方案，用于生成超导平台中的高保真纠缠门


<details>
  <summary>Details</summary>
Motivation: 需要解决超导量子计算中残余耦合、校准漂移和准静态噪声等问题，实现稳定有效的纠缠门设计

Method: 采用连续动态解耦抑制噪声和残余耦合，在稳定SU(4)流形中使用变分测地线优化过程计算平滑低能量单量子比特控制函数

Result: 成功应用于CZ、CX和通用纠缠门，实现接近单位保真度和鲁棒性，控制场符合实验实际要求

Conclusion: CDD增强的变分几何最优控制为设计超导纠缠门提供了实用且抗噪声的方案

Abstract: We present a unified scheme for generating high-fidelity entangling gates in superconducting platforms by continuous dynamical decoupling (CDD) combined with variational minimal-energy optimal control. During the CDD stage, we suppress residual couplings, calibration drifting, and quasistatic noise, resulting in a stable effective Hamiltonian that preserves the designed ZZ interaction intended for producing tunable couplers. In this stable $\mathrm{SU}(4)$ manifold, we calculate smooth low-energy single-quibt control functions using a variational geodesic optimization process that directly minimizes gate infidelity. We illustrate the methodology by applying it to CZ, CX, and generic engangling gates, achieving virtually unit fidelity and robustness under restricted single-qubit action, with experimentally realistic control fields. These results establish CDD-enhanced variational geometric optimal control as a practical and noise-resilient scheme for designing superconducting entangling gates.

</details>


### [64] [Localization Landscape in Non-Hermitian and Floquet quantum systems](https://arxiv.org/abs/2601.10451)
*David Guéry-Odelin,François Impens*

Main category: quant-ph

TL;DR: 提出了一种推广的Filoche-Mayboroda局域化景观理论，将其扩展到非静态、非椭圆和非厄米系统，同时保持几何可解释性，能够预测非厄米、Floquet和拓扑系统中的局域化现象。


<details>
  <summary>Details</summary>
Motivation: 传统Filoche-Mayboroda局域化景观理论主要适用于静态、椭圆和厄米系统，需要扩展到更广泛的量子系统，包括非厄米、Floquet和拓扑系统，同时保持其预测局域化的能力。

Method: 使用正算子H†H构建推广的局域化景观，通过奇异值塌缩揭示谱不稳定性与皮肤效应，采用Sambe公式描述相干隧穿破坏，从景观中直接提取拓扑零模。

Result: 在Hatano-Nelson链、驱动二能级系统和驱动Aubry-André-Harper模型中验证了定量准确性，证明该方法能够统一预测平衡态和驱动量子物质中的局域化现象。

Conclusion: 建立了一个统一的局域化预测框架，无需计算本征态即可预测非厄米、Floquet和拓扑系统中的局域化行为，为研究平衡态和驱动量子系统提供了新工具。

Abstract: We propose a generalization of the Filoche--Mayboroda localization landscape that extends the theory well beyond the static, elliptic and Hermitian settings while preserving its geometric interpretability. Using the positive operator $H^\dagger H$, we obtain a landscape that predicts localization across non-Hermitian, Floquet, and topological systems without computing eigenstates. Singular-value collapse reveals spectral instabilities and skin effects, the Sambe formulation captures coherent destruction of tunneling, and topological zero modes emerge directly from the landscape. Applications to Hatano--Nelson chains, driven two-level systems, and driven Aubry--André--Harper models confirm quantitative accuracy, establishing a unified predictor for localization in equilibrium and driven quantum matter.

</details>


### [65] [Erasure conversion for singlet-triplet spin qubits enables high-performance shuttling-based quantum error correction](https://arxiv.org/abs/2601.10461)
*Adam Siegel,Simon Benjamin*

Main category: quant-ph

TL;DR: 该论文提出了一种基于双自旋量子比特（单重态-三重态编码）的容错量子纠错框架，将其作为半导体架构中擦除量子比特的自然实现，通过硬件高效的泄漏检测协议和XZZX表面码，显著提高了纠错阈值和逻辑错误率。


<details>
  <summary>Details</summary>
Motivation: 在半导体量子点器件中，自旋量子比特的高速高保真度穿梭已被证明。基于穿梭的多种架构被提出，其中单重态-三重态（双自旋）量子比特被认为可能实现最高的穿梭保真度。然而，需要建立一个容错框架来充分发挥这种量子比特在量子纠错中的潜力。

Method: 1. 将双自旋量子比特建立为半导体架构中擦除量子比特的自然实现；2. 引入硬件高效的泄漏检测协议，无需测量反馈或增加经典控制开销，自动将泄漏的量子比特投影回计算子空间；3. 结合XZZX表面码和泄漏感知解码。

Result: 1. 纠错阈值提高了两倍；2. 逻辑错误率实现了数量级的降低；3. 建立了单重态-三重态编码作为实现高保真度穿梭和基于擦除的容错量子计算的实用途径。

Conclusion: 单重态-三重态编码为半导体器件中实现高保真度穿梭和基于擦除的容错量子计算提供了一条实用路线。该框架充分利用了双自旋量子比特作为擦除量子比特的特性，通过硬件高效的泄漏检测和优化的纠错方案，显著提升了量子计算的容错性能。

Abstract: Fast and high fidelity shuttling of spin qubits has been demonstrated in semiconductor quantum dot devices. Several architectures based on shuttling have been proposed; it has been suggested that singlet-triplet (dual-spin) qubits could be optimal for the highest shuttling fidelities. Here we present a fault-tolerant framework for quantum error correction based on such dual-spin qubits, establishing them as a natural realisation of erasure qubits within semiconductor architectures. We introduce a hardware-efficient leakage-detection protocol that automatically projects leaked qubits back onto the computational subspace, without the need for measurement feedback or increased classical control overheads. When combined with the XZZX surface code and leakage-aware decoding, we demonstrate a twofold increase in the error correction threshold and achieve orders-of-magnitude reductions in logical error rates. This establishes the singlet-triplet encoding as a practical route toward high-fidelity shuttling and erasure-based, fault-tolerant quantum computation in semiconductor devices.

</details>


### [66] [Nonlinear quantum Kibble-Zurek ramps in open systems at finite temperature](https://arxiv.org/abs/2601.10465)
*Johannes N. Kriel,Emma C. King,Michael Kastner*

Main category: quant-ph

TL;DR: 该论文研究量子系统在温度和哈密顿量控制参数同时非线性变化趋近量子临界点的协议，展示了如何通过开放系统动力学在有限温度下探测零温量子相变的普适类。


<details>
  <summary>Details</summary>
Motivation: 传统有限温度协议在固定温度下无法有效探测量子临界点的普适类特征。该研究旨在开发一种新方法，在非平衡、有限温度条件下，通过同时调控温度和哈密顿量参数来探测量子相变的临界指数。

Method: 使用Kitaev量子线作为开放系统模型，分析温度和哈密顿量控制参数同时非线性变化趋近量子临界点的协议。研究相干和非相干动力学对激发密度的影响，识别出能抑制渐近标度律次主导项的具体变化路径。

Result: 研究发现某些协议中相干和非相干动力学都会显著影响激发密度，从而允许在有限温度下探测量子相变的临界指数ν和z。识别出了能抑制次主导项的具体变化路径，为实验上在有限温度条件下动态探测量子临界指数提供了指导。

Conclusion: 通过温度和哈密顿量参数的同时非线性变化协议，可以在非平衡、有限温度条件下有效探测量子相变的普适类，为实验研究量子临界现象提供了新方法。

Abstract: We analyze quantum systems under a broad class of protocols in which the temperature and a Hamiltonian control parameter are ramped simultaneously and, in general, in a nonlinear fashion toward a quantum critical point. Using an open-system version of a Kitaev quantum wire as an example, we show that, unlike finite-temperature protocols at fixed temperature, these protocols allow us to probe, in an out-of-equilibrium situation and at finite temperature, the universality class (characterized by the critical exponents $ν$ and $z$) of an equilibrium quantum phase transition at zero temperature. Key to this is the identification of ramps in which both coherent and incoherent parts of the open-system dynamics affect the excitation density in a non-negligible way. We also identify the specific ramps for which subleading corrections to the asymptotic scaling laws are suppressed, which serves as a guide to dynamically probing quantum critical exponents in experimentally realistic finite-temperature situations.

</details>


### [67] [Analysis and Experimental Demonstration of Amplitude Amplification for Combinatorial Optimization](https://arxiv.org/abs/2601.10473)
*Daniel Koch,Brian Pardo,Kip Nieman*

Main category: quant-ph

TL;DR: 本文扩展了量子振幅放大(QAA)算法，将其从传统的2维表示推广到能编码QUBO等成本函数的oracle，并针对线性成本函数给出了确定最优oracle参数设置的精确公式。


<details>
  <summary>Details</summary>
Motivation: 量子振幅放大(QAA)作为Grover算法的推广，能够以高概率获得组合优化问题的最优解。然而，传统的2维表示限制了其在更复杂成本函数（如QUBO）中的应用，需要扩展理论框架并验证实际量子硬件上的性能。

Method: 1. 将传统的2维Grover表示扩展到能编码QUBO成本函数的oracle；2. 针对线性成本函数推导出确定最优oracle参数设置的精确公式；3. 通过模拟（最多40量子比特）验证算法性能；4. 在IBMQ（超导）和IonQ（囚禁离子）量子比特上进行实验验证。

Result: 1. 成功将QAA扩展到更一般的成本函数编码；2. 针对线性成本函数获得了精确的参数设置公式；3. 模拟显示QAA在所有可能解上都有良好的算法性能，特别是接近全局最优解时表现出类似Grover的性能；4. 实验验证了观测到的各基态概率与理论公式一致，且随oracle和扩散算子中自由参数的变化而变化。

Conclusion: 本文成功扩展了量子振幅放大算法，使其能够处理更一般的成本函数，特别是针对线性成本函数给出了精确的参数优化方法。实验验证表明理论预测与实际量子硬件性能相符，为QAA在组合优化问题中的实际应用奠定了基础。

Abstract: Quantum Amplitude Amplification (QAA), the generalization of Grover's algorithm, is capable of yielding optimal solutions to combinatorial optimization problems with high probabilities. In this work we extend the conventional 2-dimensional representation of Grover's (orthogonal collective states) to oracles which encode cost functions such as QUBO, and show that linear cost functions are a special case whereby an exact formula exists for determining optimal oracle parameter settings. Using simulations of problem sizes up to 40 qubits we demonstrate QAA's algorithmic performance across all possible solutions, with an emphasis on the closeness in Grover-like performance for solutions near the global optimum. We conclude with experimental demonstrations of generalized QAA on both IBMQ (superconducting) and IonQ (trapped ion) qubits, showing that the observed probabilities of each basis state match our equations as a function of varying the free parameters in the oracle and diffusion operators.

</details>


### [68] [H-EFT-VA: An Effective-Field-Theory Variational Ansatz with Provable Barren Plateau Avoidance](https://arxiv.org/abs/2601.10479)
*Eyad I. B Hamid*

Main category: quant-ph

TL;DR: 提出H-EFT变分架构，通过分层UV截断初始化避免贫瘠高原，保持体积律纠缠的同时保证梯度方差有逆多项式下界，在多个模型上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 贫瘠高原现象严重威胁变分量子算法的实用性，现有避免贫瘠高原的方法通常限制纠缠，但会降低表达能力。需要一种既能避免贫瘠高原又能保持足够表达能力的方法。

Method: 提出H-EFT变分架构，受有效场论启发，通过分层"UV截断"初始化限制电路状态探索，防止形成近似酉2-design，从而保证梯度方差有逆多项式下界。

Result: 理论证明梯度方差有Ω(1/poly(N))下界，实验验证在16个实验中（包括横向场伊辛模型和海森堡XXZ模型）能量收敛提升109倍，基态保真度提升10.7倍，统计显著性p<10^{-88}。

Conclusion: H-EFT架构有效解决了贫瘠高原问题，同时保持体积律纠缠和接近Haar纯度，为变分量子算法提供了既避免梯度消失又保持足够表达能力的新方案。

Abstract: Variational Quantum Algorithms (VQAs) are critically threatened by the Barren Plateau (BP) phenomenon. In this work, we introduce the H-EFT Variational Ansatz (H-EFT-VA), an architecture inspired by Effective Field Theory (EFT). By enforcing a hierarchical "UV-cutoff" on initialization, we theoretically restrict the circuit's state exploration, preventing the formation of approximate unitary 2-designs. We provide a rigorous proof that this localization guarantees an inverse-polynomial lower bound on the gradient variance: $Var[\partial θ] \in Ω(1/poly(N))$. Crucially, unlike approaches that avoid BPs by limiting entanglement, we demonstrate that H-EFT-VA maintains volume-law entanglement and near-Haar purity, ensuring sufficient expressibility for complex quantum states. Extensive benchmarking across 16 experiments -- including Transverse Field Ising and Heisenberg XXZ models -- confirms a 109x improvement in energy convergence and a 10.7x increase in ground-state fidelity over standard Hardware-Efficient Ansatze (HEA), with a statistical significance of $p < 10^{-88}$.

</details>


### [69] [Optimized readout strategies for neutral atom quantum processors](https://arxiv.org/abs/2601.10492)
*Liang Chen,Wen-Yi Zhu,Zi-Jie Chen,Zhu-Bo Wang,Ya-Dong Hu,Qing-Xuan Jie,Guang-Can Guo,Chang-Ling Zou*

Main category: quant-ph

TL;DR: 该论文提出了一个理论框架来量化中性原子量子处理器中读取保真度与原子保留之间的权衡，并引入量子电路迭代率(qCIR)作为系统性能指标，通过优化读取策略实现了197.2Hz和154.5Hz的高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 中性原子量子处理器虽然具有高保真度操作和优异量子比特可扩展性，但在实现实际应用时面临关键挑战：如何在保持高系统吞吐量的同时有效提取读取结果。需要解决读取保真度与原子保留之间的权衡问题。

Method: 1. 开发理论框架量化读取保真度与原子保留的权衡关系；2. 引入量子电路迭代率(qCIR)作为系统性能指标；3. 使用归一化量子Fisher信息表征系统整体性能；4. 通过平衡保真度和保留来优化信息获取效率；5. 基于87Rb原子的实验可行参数进行验证。

Result: 使用单光子探测器和相机分别实现了197.2Hz和154.5Hz的量子电路迭代率(qCIR)，为构建可扩展、高吞吐量的中性原子量子处理器提供了实用指导。

Conclusion: 通过优化读取策略平衡保真度与原子保留，显著提高了中性原子量子处理器的吞吐量，为传感、模拟和近期算法实现等应用中的可扩展高吞吐量系统构建提供了实用指导。

Abstract: Neutral atom quantum processors have emerged as a promising platform for scalable quantum information processing, offering high-fidelity operations and exceptional qubit scalability. A key challenge in realizing practical applications is efficiently extracting readout outcomes while maintaining high system throughput, i.e., the rate of quantum task executions. In this work, we develop a theoretical framework to quantify the trade-off between readout fidelity and atomic retention. Moreover, we introduce a metric of quantum circuit iteration rate (qCIR) and employ normalized quantum Fisher information to characterize system overall performance. Further, by carefully balancing fidelity and retention, we demonstrate a readout strategy for optimizing information acquisition efficiency. Considering the experimentally feasible parameters for 87Rb atoms, we demonstrate that qCIRs of 197.2Hz and 154.5Hz are achievable using single photon detectors and cameras, respectively. These results provide practical guidance for constructing scalable and high-throughput neutral atom quantum processors for applications in sensing, simulation, and near-term algorithm implementation.

</details>


### [70] [A Mirror-Descent Algorithm for Computing the Petz-Rényi Capacity of Classical-Quantum Channels](https://arxiv.org/abs/2601.10558)
*Yu-Hong Lai,Hao-Chung Cheng*

Main category: quant-ph

TL;DR: 提出一种计算经典-量子信道α-Rényi容量的指数梯度算法，推广了Blahut-Arimoto算法，证明了全局次线性收敛和局部线性收敛


<details>
  <summary>Details</summary>
Motivation: 计算经典-量子信道的α-Rényi容量（α∈(0,1)）是一个重要问题，需要推广经典的Blahut-Arimoto算法来处理量子信道

Method: 提出指数梯度（镜像下降）迭代算法，基于熵几何的相对光滑性分析，在截断概率单纯形上建立收敛性

Result: 证明了全局目标值的次线性收敛，在切空间非退化条件下证明了Kullback-Leibler散度的局部线性收敛，并给出了显式收缩因子

Conclusion: 成功推广了Blahut-Arimoto算法到经典-量子信道，建立了算法的收敛性理论，为计算α-Rényi容量提供了有效方法

Abstract: We study the computation of the $α$-Rényi capacity of a classical-quantum (c-q) channel for $α\in(0,1)$. We propose an exponentiated-gradient (mirror descent) iteration that generalizes the Blahut-Arimoto algorithm. Our analysis establishes relative smoothness with respect to the entropy geometry, guaranteeing a global sublinear convergence of the objective values. Furthermore, under a natural tangent-space nondegeneracy condition (and a mild spectral lower bound in one regime), we prove local linear (geometric) convergence in Kullback-Leibler divergence on a truncated probability simplex, with an explicit contraction factor once the local curvature constants are bounded.

</details>


### [71] [Deterministic and scalable generation of large Fock states](https://arxiv.org/abs/2601.10559)
*Mo Xiong,Jize Han,Chuanzhen Cao,Jinbin Li,Qi Liu,Zhiguo Huang,Ming Xue*

Main category: quant-ph

TL;DR: 提出一种可扩展的协议，使用混合遗传-Adam优化框架生成高达100个光子的高保真度Fock态，仅需原生控制操作，保真度超过0.9


<details>
  <summary>Details</summary>
Motivation: 大规模Fock态的可扩展确定性制备是量子科学中长期存在的挑战，对量子计量学、通信和模拟有直接影响。虽然在小规模实现方面取得了进展，但扩展到大量子数同时保持高保真度仍然困难。

Method: 采用混合遗传-Adam优化框架，结合遗传算法的全局搜索效率和Adam的自适应收敛性，优化由Jaynes-Cummings相互作用和位移操作组成的多脉冲控制序列，这些都是实验平台的原生操作。

Result: 实现了高达100个光子的Fock态生成，保真度超过0.9，控制协议具有浅电路深度和对参数变化的强鲁棒性。可选的后选择步骤可进一步提高性能。

Conclusion: 为高保真度非经典态生成建立了高效可扩展的途径，对精密计量学和容错量子技术具有重要意义。

Abstract: The scalable and deterministic preparation of large Fock-number states represents a long-standing frontier in quantum science, with direct implications for quantum metrology, communication, and simulation. Despite significant progress in small-scale implementations, extending such state generation to large excitation numbers while maintaining high fidelity remains a formidable challenge. Here, we present a scalable protocol for generating large Fock states with fidelities exceeding 0.9 up to photon numbers on the order of 100, achieved using only native control operations and, when desired, further enhanced by an optional post-selection step. Our method employs a hybrid Genetic-Adam optimization framework that combines the global search efficiency of genetic algorithms with the adaptive convergence of Adam to optimize multi-pulse control sequences comprising Jaynes-Cummings interactions and displacement operations, both of which are native to leading experimental platforms. The resulting control protocols achieve high fidelities with shallow circuit depths and strong robustness against parameter variations. These results establish an efficient and scalable pathway toward high-fidelity non-classical state generation for precision metrology and fault-tolerant quantum technologies.

</details>


### [72] [Searching for Quantum Effects in the Brain: A Bell-Type Test for Nonclassical Latent Representations in Autoencoders](https://arxiv.org/abs/2601.10588)
*I. K. Kominis,C. Xie,S. Li,M. Skotiniotis,G. P. Tsironis*

Main category: quant-ph

TL;DR: 提出一种模型无关的信息论非经典性测试，通过潜在空间的贝尔型一致性检验来探测神经表征结构，而非微观假设。


<details>
  <summary>Details</summary>
Motivation: 神经信息处理是完全经典的还是涉及量子力学元素仍然是一个开放问题。现有方法依赖微观假设，需要绕过这些假设直接探测神经表征本身的结构。

Method: 使用自编码器作为透明模型系统，在潜在空间中引入贝尔型一致性测试，检验在多个读出上下文下获得的解码统计是否可以用单一的正潜在变量分布联合解释。

Result: 该方法将神经系统中量子样特征的搜索从微观动力学转移到信息处理的可实验测试约束上。

Conclusion: 这项工作为探测神经计算的基本物理开辟了新途径，通过信息处理约束而非微观假设来检验神经系统的非经典性。

Abstract: Whether neural information processing is entirely classical or involves quantum-mechanical elements remains an open question. Here we propose a model-agnostic, information-theoretic test of nonclassicality that bypasses microscopic assumptions and instead probes the structure of neural representations themselves. Using autoencoders as a transparent model system, we introduce a Bell-type consistency test in latent space, and ask whether decoding statistics obtained under multiple readout contexts can be jointly explained by a single positive latent-variable distribution. By shifting the search for quantum-like signatures in neural systems from microscopic dynamics to experimentally testable constraints on information processing, this work opens a new route for probing the fundamental physics of neural computation.

</details>


### [73] [Quantum solver for single-impurity Anderson models with particle-hole symmetry](https://arxiv.org/abs/2601.10594)
*Mariia Karabin,Tanvir Sohail,Dmytro Bykov,Eduardo Antonio Coello Pérez,Swarnava Ghosh,Murali Gopalakrishnan Meena,Seongmin Kim,Amir Shehata,In-Saeng Suh,Hanna Terletska,Markus Eisenbach*

Main category: quant-ph

TL;DR: 开发用于DMFT的量子-经典混合求解器，使用VQE准备Anderson杂质模型的基态，通过参数偏移电路重建格林函数，在噪声条件下评估性能。


<details>
  <summary>Details</summary>
Motivation: DMFT是研究强关联材料的有力框架，但其核心计算瓶颈在于求解Anderson杂质模型（AIM）。对于大浴尺寸，AIM的精确解在经典计算上是难以处理的，需要开发量子-经典混合求解器来应对这一挑战。

Method: 使用变分量子本征求解器（VQE）通过浅层量子电路准备AIM的基态。采用统一的ansatz框架，通过参数偏移电路准备基态的粒子和空穴激发，通过连分式展开重建杂质格林函数。在噪声、有限测量次数条件下评估性能，比较三种优化算法（COBYLA、Adam、L-BFGS-B），评估量子计算矩（QCM）校正的益处。

Result: 该方法在不同浴尺寸和相互作用强度下表现出可行性，比较了优化算法的收敛性和保真度，评估了QCM校正的效果，并将重建的态密度与经典管道结果进行基准测试。结果表明在近期设备上重建格林函数是可行的。

Conclusion: 该工作证明了在近期量子设备上重建格林函数的可行性，为嵌入自洽DMFT循环中的量子杂质求解器建立了实用基准，为量子-经典混合方法在强关联材料研究中的应用奠定了基础。

Abstract: Quantum embedding methods, such as dynamical mean-field theory (DMFT), provide a powerful framework for investigating strongly correlated materials. A central computational bottleneck in DMFT is in solving the Anderson impurity model (AIM), whose exact solution is classically intractable for large bath sizes. In this work, we develop and benchmark a quantum-classical hybrid solver tailored for DMFT applications, using the variational quantum eigensolver (VQE) to prepare the ground state of the AIM with shallow quantum circuits. The solver uses a unified ansatz framework to prepare the particle and hole excitations of the ground-state from parameter-shifted circuits, enabling the reconstruction of the impurity Green's function through a continued-fraction expansion. We evaluate the performance of this approach across a few bath sizes and interaction strengths under noisy, shot-limited conditions. We compare three optimization routines (COBYLA, Adam, and L-BFGS-B) in terms of convergence and fidelity, assess the benefits of estimating a quantum-computed moment (QCM) correction to the variational energies, and benchmark the approach by comparing the reconstructed density of states (DOS) against that obtained using a classical pipeline. Our results demonstrate the feasibility of Green's function reconstruction on near-term devices and establish practical benchmarks for quantum impurity solvers embedded within self-consistent DMFT loops.

</details>


### [74] [Quantifying the properties of evolutionary quantum states of the XXZ spin model using quantum computing](https://arxiv.org/abs/2601.10650)
*M. P. Tonne,Kh. P. Gnatenko*

Main category: quant-ph

TL;DR: 研究两自旋XXZ模型中演化量子态的纠缠距离，通过解析和量子计算分析纠缠距离与耦合常数、初始态参数的关系，并研究演化速度的依赖性。


<details>
  <summary>Details</summary>
Motivation: 研究两自旋XXZ模型中量子态的演化特性，特别是纠缠距离和演化速度如何依赖于系统参数，为量子信息处理和量子计算提供理论基础。

Method: 结合解析方法和量子计算，分析两自旋XXZ模型的演化量子态，推导纠缠距离和演化速度的解析表达式，并通过量子计算验证理论结果。

Result: 获得了纠缠距离和演化速度与模型耦合常数、初始态参数的显式依赖关系，量子计算结果与理论预测高度一致。

Conclusion: 成功建立了XXZ模型中纠缠距离和演化速度的解析描述，验证了量子计算在量子系统演化研究中的有效性，为量子信息处理提供了重要参考。

Abstract: The entanglement distance of evolutionary quantum states of a two-spin system with the XXZ model has been studied. The analysis has been conducted both analytically and using quantum computing. An analytical dependence of the entanglement distance on the values of the model coupling constants and the parameters of the initial states has been obtained. The speed of evolution of a two-spin system has been investigated. The analysis has been performed analytically and using quantum computing. An explicit dependence of the speed of evolution on the coupling constants and on the parameters of the initial state has been obtained. The results of quantum computations are in good agreement with the theoretical predictions.

</details>


### [75] [Symmetry-based Perspectives on Hamiltonian Quantum Search Algorithms and Schrodinger's Dynamics between Orthogonal States](https://arxiv.org/abs/2601.10655)
*Carlo Cafaro,James Schneeloch*

Main category: quant-ph

TL;DR: 本文证明：在由初始态和终态张成的二维子空间中，使用恒定哈密顿量无法突破正交态间的时间最优演化；只有时变哈密顿量或高维空间中的恒定哈密顿量才能实现次优时间演化。


<details>
  <summary>Details</summary>
Motivation: Grover搜索算法的连续时间变体通常使用恒定哈密顿量，其演化被限制在源态和目标态张成的二维子空间中。当源态和目标态正交时，这种搜索方法失效。本文旨在探究在何种条件下可以突破正交态间的时间最优演化限制。

Method: 利用归一化、正交性和能量限制等数学约束条件，分析在二维子空间中使用恒定哈密顿量进行量子演化的可行性。通过定量分析证明时间最优演化的限制条件。

Result: 证明在由初始态和终态张成的二维子空间中，使用恒定哈密顿量无法实现正交态间的次优时间演化。只有两种可能突破时间最优性：使用时变哈密顿量，或在更高维子空间中使用恒定哈密顿量。

Conclusion: 正交态间的时间最优演化限制与模拟量子搜索的失效机制密切相关，两者都源于系统内在的对称性。这一发现深化了对量子搜索算法和量子控制理论之间关系的理解。

Abstract: It is known that the continuous-time variant of Grover's search algorithm is characterized by quantum search frameworks that are governed by stationary Hamiltonians, which result in search trajectories confined to the two-dimensional subspace of the complete Hilbert space formed by the source and target states. Specifically, the search approach is ineffective when the source and target states are orthogonal. In this paper, we employ normalization, orthogonality, and energy limitations to demonstrate that it is unfeasible to breach time-optimality between orthogonal states with constant Hamiltonians when the evolution is limited to the two-dimensional space spanned by the initial and final states. Deviations from time-optimality for unitary evolutions between orthogonal states can only occur with time-dependent Hamiltonian evolutions or, alternatively, with constant Hamiltonian evolutions in higher-dimensional subspaces of the entire Hilbert space. Ultimately, we employ our quantitative analysis to provide meaningful insights regarding the relationship between time-optimal evolutions and analog quantum search methods. We determine that the challenge of transitioning between orthogonal states with a constant Hamiltonian in a sub-optimal time is closely linked to the shortcomings of analog quantum search when the source and target states are orthogonal and not interconnected by the search Hamiltonian. In both scenarios, the fundamental cause of the failure lies in the existence of an inherent symmetry within the system.

</details>


### [76] [Counterdiabatic driving for random-gap Landau-Zener transitions](https://arxiv.org/abs/2601.10659)
*Georgios Theologou,Mikkel F. Andersen,Sandro Wimberger*

Main category: quant-ph

TL;DR: 该论文研究了如何为具有能隙分布的Landau-Zener型哈密顿量集合设计单一控制场，以最小化平均跃迁概率，发现了瞬时绝热性与最终跃迁概率之间的系统权衡关系。


<details>
  <summary>Details</summary>
Motivation: 传统Landau-Zener模型中，绝热极限下跃迁概率为零，但需要为每个特定系统设计控制场。本文旨在为具有能隙分布的LZ型哈密顿量集合构建单一控制场，实现统计最优控制。

Method: 基于绝热驱动场H_CD的启发，构建特殊类别的单一控制场H_1，通过最小化平均跃迁概率来优化控制效果。对线性扫描的极限情况进行了解析处理，包括Dirac δ(t)函数的LZ系统，并通过全面系统的数值模拟验证和扩展解析结果。

Result: 发现了瞬时绝热性与最终跃迁概率之间的系统权衡关系。某些具有线性扫描的极限情况可以解析处理，其中一个特例是包含Dirac δ(t)函数的LZ系统。数值模拟结果支持并扩展了解析发现。

Conclusion: 成功构建了适用于LZ型哈密顿量集合的单一控制场，揭示了控制性能的基本权衡关系，为量子系统集合的统计最优控制提供了理论和数值基础。

Abstract: The Landau--Zener (LZ) model describes a two-level quantum system that undergoes an avoided crossing. In the adiabatic limit, the transition probability vanishes. An auxiliary control field $H_\text{CD}$ can be reverse-engineered so that the full Hamiltonian $H_0 + H_\text{CD}$ reproduces adiabaticity for all parameter values. Our aim is to construct a single control field $H_1$ that drives an ensemble of LZ-type Hamiltonians with a distribution of energy gaps. $H_1$ works best statistically, minimizing the average transition probability. We restrict our attention to a special class of $H_1$ controls, motivated by $H_\text{CD}$. We found a systematic trade-off between instantaneous adiabaticity and the final transition probability. Certain limiting cases with a linear sweep can be treated analytically; one of them being the LZ system with Dirac $δ(t)$ function. Comprehensive and systematic numerical simulations support and extend the analytic results.

</details>


### [77] [Geometric Aspects of Entanglement Generating Hamiltonian Evolutions](https://arxiv.org/abs/2601.10662)
*Carlo Cafaro,James Schneeloch*

Main category: quant-ph

TL;DR: 研究两比特量子态从可分离到最大纠缠的哈密顿演化几何特性，发现时间最优演化具有高测地效率、零曲率、无能量浪费，且平均路径纠缠低于次优演化。


<details>
  <summary>Details</summary>
Motivation: 探索哈密顿演化从可分离态到最大纠缠态的几何特性与纠缠度量之间的关系，理解时间最优演化在几何和纠缠方面的特征。

Method: 从几何角度使用测地效率、速度效率和曲率系数表征演化；从纠缠角度使用并发度、纠缠能力和纠缠功率等度量。分析正交与非正交初始态的时间最优与次优演化。

Result: 时间最优演化具有高测地效率、零曲率、无能量浪费，平均路径纠缠低于次优演化。对于非正交态，时间最优演化展示更强的短时非局域性；对于正交态则相反。

Conclusion: 时间最优演化在几何上更高效直接，而次优演化路径更长、曲率更小但能量浪费更大。实现最大纠缠需要演化算符具有较高的初始非局域性。

Abstract: We examine the pertinent geometric characteristics of entanglement that arise from stationary Hamiltonian evolutions transitioning from separable to maximally entangled two-qubit quantum states. From a geometric perspective, each evolution is characterized by means of geodesic efficiency, speed efficiency, and curvature coefficient. Conversely, from the standpoint of entanglement, these evolutions are quantified using various metrics, such as concurrence, entanglement power, and entangling capability. Overall, our findings indicate that time-optimal evolution trajectories are marked by high geodesic efficiency, with no energy resource wastage, no curvature (i.e., zero bending), and an average path entanglement that is less than that observed in time-suboptimal evolutions. Additionally, when analyzing separable-to-maximally entangled evolutions between nonorthogonal states, time-optimal evolutions demonstrate a greater short-time degree of nonlocality compared to time-suboptimal evolutions between the same initial and final states. Interestingly, the reverse is generally true for separable-to-maximally entangled evolutions involving orthogonal states. Our investigation suggests that this phenomenon arises because suboptimal trajectories between orthogonal states are characterized by longer path lengths with smaller curvature, which are traversed with a higher energy resource wastage compared to suboptimal trajectories between nonorthogonal states. Consequently, a higher initial degree of nonlocality in the unitary time propagators appears to be essential for achieving the maximally entangled state from a separable state. Furthermore, when assessing optimal and suboptimal evolutions...

</details>


### [78] [Efficiency, Curvature, and Complexity of Quantum Evolutions for Qubits in Nonstationary Magnetic Fields](https://arxiv.org/abs/2601.10672)
*Carlo Cafaro,James Schneeloch*

Main category: quant-ph

TL;DR: 该论文研究了量子演化中的曲率问题，针对两能级量子系统在时变磁场下的演化，推导了曲率的精确解析表达式，并分析了曲率与量子演化效率、复杂性之间的关系。


<details>
  <summary>Details</summary>
Motivation: 理想量子演化应具有最小路径长度和最优时间，或沿既定路径无能量浪费且100%速度效率。但实际物理场景常导致次优演化，表现为效率不足、非零曲率和高复杂性。因此需要精确分析量子演化的曲率特性。

Method: 针对两能级量子系统在时变磁场下的演化，采用两参数非平稳厄米哈密顿量，推导量子演化曲率的精确解析表达式。通过分析曲率与测地线效率、速度效率及演化复杂性（定义为从初态到末态的可访问与已访问Bloch球体积差与可访问体积之比）的关系，深入理解曲率的物理意义。

Result: 研究发现：1）高效量子演化通常比低效演化具有更低的复杂性；2）复杂性不仅取决于路径长度，足够弯曲的较长路径可能比曲率系数较低的较短路径具有更低的复杂性。

Conclusion: 量子演化的复杂性不能仅用路径长度衡量，曲率在决定演化复杂性中起关键作用。高效演化通常伴随较低复杂性，但路径的弯曲程度也能显著影响复杂性度量，这为优化量子控制提供了新视角。

Abstract: In optimal quantum-mechanical evolutions, motion can take place along paths of minimal length within an optimal time frame. Alternatively, optimal evolutions may occur along established paths without any waste of energy resources and achieving 100% speed efficiency. Unfortunately, realistic physical scenarios often lead to less-than-ideal evolutions that demonstrate suboptimal efficiency, nonzero curvature, and a high level of complexity. In this paper, we provide an exact analytical expression for the curvature of a quantum evolution pertaining to a two-level quantum system subjected to various time-dependent magnetic fields. Specifically, we examine the dynamics produced by a two-parameter nonstationary Hermitian Hamiltonian with unit speed efficiency. To enhance our understanding of the physical implications of the curvature coefficient, we analyze the curvature behavior in relation to geodesic efficiency, speed efficiency, and the complexity of the quantum evolution (as described by the ratio of the difference between accessible and accessed Bloch-sphere volumes for the evolution from initial to final state to the accessible volume for the given quantum evolution). Our findings indicate that, generally, efficient quantum evolutions exhibit lower complexity compared to inefficient ones. However, we also note that complexity transcends mere length. In fact, longer paths that are sufficiently curved can demonstrate a complexity that is less than that of shorter paths with a lower curvature coefficient.

</details>


### [79] [Optimal lower bound for quantum channel tomography in away-from-boundary regime](https://arxiv.org/abs/2601.10683)
*Kean Chen,Zhicheng Zhang,Nengkun Yu*

Main category: quant-ph

TL;DR: 量子信道层析的查询复杂度下界：在远离边界区域(rd₂≥2d₁)达到最优Ω(rd₁d₂/ε²)，与现有上界匹配，完全解决了d₁=d₂且r≥2的情况


<details>
  <summary>Details</summary>
Motivation: 研究量子信道层析的查询复杂度，特别关注输入维度d₁、输出维度d₂和Kraus秩r满足rd₂≥d₁的约束条件。在边界区域rd₂=d₁和远离边界区域rd₂≥2d₁之间，查询复杂度存在差异，需要确定最优下界

Method: 通过理论分析证明量子信道层析的查询下界，使用数学推导和复杂度理论方法，针对不同参数区域(边界区域和远离边界区域)进行分析

Result: 在远离边界区域(rd₂≥2d₁)证明了最优查询下界Ω(rd₁d₂/ε²)，与现有上界O(rd₁d₂/ε²)匹配，完全解决了d₁=d₂且r≥2的情况，与酉信道(r=1)的海森堡标度Θ(d²/ε)形成鲜明对比

Conclusion: 量子信道层析的查询复杂度在远离边界区域达到最优Ω(rd₁d₂/ε²)，这一结果完全解决了常见情况下(d₁=d₂且r≥2)的复杂度问题，揭示了与酉信道情况的根本差异

Abstract: Consider quantum channels with input dimension $d_1$, output dimension $d_2$ and Kraus rank at most $r$. Any such channel must satisfy the constraint $rd_2\geq d_1$, and the parameter regime $rd_2=d_1$ is called the boundary regime. In this paper, we show an optimal query lower bound $Ω(rd_1d_2/\varepsilon^2)$ for quantum channel tomography to within diamond norm error $\varepsilon$ in the away-from-boundary regime $rd_2\geq 2d_1$, matching the existing upper bound $O(rd_1d_2/\varepsilon^2)$. In particular, this lower bound fully settles the query complexity for the commonly studied case of equal input and output dimensions $d_1=d_2=d$ with $r\geq 2$, in sharp contrast to the unitary case $r=1$ where Heisenberg scaling $Θ(d^2/\varepsilon)$ is achievable.

</details>


### [80] [Mitigating nonlinear transduction noise in high-cooperativity cavity optomechanics](https://arxiv.org/abs/2601.10689)
*Daniel Allepuz-Requena,Zohran Ali,Dennis Høj,Yingxuan Chen,Luiz Couto Correa Pinto Filho,Alexander Huck,Ulrik L. Andersen*

Main category: quant-ph

TL;DR: 通过非线性变换消除热互调噪声，提高室温腔光机械系统的信噪比


<details>
  <summary>Details</summary>
Motivation: 在腔光机械系统中，增加光机械耦合强度会导致探测光学谐振器的非线性响应，热互调噪声（TIN）会显著增加测量误差，超过标准量子极限。虽然之前通过"魔幻失谐"操作可以消除二阶非线性，但高阶TIN仍然存在。

Method: 使用膜在中间微腔系统在室温下运行，实现高协同性（C>n_th），记录系统输出，并应用非线性变换来消除所有阶数的热互调噪声。

Result: 通过非线性变换，机械信噪比提高了近10 dB，该方法可以应用于受三阶TIN影响的实验，预计三阶TIN是高协同性室温腔光机械系统中的主要固有噪声源。

Conclusion: 开发了一种非线性变换方法，能够有效消除所有阶数的热互调噪声，显著提高室温高协同性腔光机械系统的测量精度，为克服非线性噪声提供了有效解决方案。

Abstract: Coupling mechanical motion to an optical resonator enables displacement measurements approaching the standard quantum limit (SQL). However, increasing the optomechanical coupling strength will inevitably lead to probing of the nonlinear response of the optical resonator. Thermal intermodulation noise (TIN) arising from the nonlinear mixing of thermomechanical motion can further increase the imprecision well above the SQL and has hitherto been canceled up to second order of nonlinearity via operation at the "magic detuning". In this work, we record the output of a membrane-in-the-middle microcavity system operating at room temperature and achieving high cooperativity, $C>n_\text{th}$, and apply a nonlinear transform that removes all orders of TIN, improving the mechanical signal-to-noise ratio by nearly 10 dB. Our results can be applied to experiments affected by third-order TIN, which we expect to be the dominating intrinsic source of noise in high-cooperativity room-temperature cavity optomechanical systems.

</details>


### [81] [Constant-Depth Unitary Preparation of Dicke States](https://arxiv.org/abs/2601.10693)
*Francisca Vasconcelos,Malvika Raj Joshi*

Main category: quant-ph

TL;DR: 首次提出基于全局相互作用的常数深度幺正协议，精确制备Dicke态，突破了对数深度限制


<details>
  <summary>Details</summary>
Motivation: Dicke态在量子计量、通信和计算中至关重要，但传统幺正制备方法在标准电路模型中需要对数深度，现有常数深度协议需要测量和前馈操作

Method: 超越标准电路模型，利用中性原子和囚禁离子等架构固有的全局相互作用，使用无界CZ门（QAC⁰电路类）和量子FAN-OUT操作（QACₓ⁰电路类）

Result: 实现了常数权重Dicke态的精确计算（使用多项式辅助比特）和权重-1 Dicke态（W态）的近似（仅需常数辅助比特），在量子FAN-OUT支持下可精确制备任意权重Dicke态

Conclusion: 这些协议基于连接性区分了量子架构的常数深度能力，为解决长期存在的量子复杂性猜想提供了新途径

Abstract: Dicke states serve as a critical resource in quantum metrology, communication, and computation. However, unitary preparation of Dicke states is limited to logarithmic depth in standard circuit models and existing constant-depth protocols require measurement and feed-forward. In this work, we present the first unitary, constant-depth protocols for exact Dicke state preparation. We overcome the logarithmic-depth barrier by moving beyond the standard circuit model and leveraging global interactions (native to architectures such as neutral atoms and trapped ions). Specifically, utilizing unbounded CZ gates (i.e. within the QAC$^0$ circuit class), we offer circuits for exact computation of constant-weight Dicke states, using polynomial ancillae, and approximation of weight-1 Dicke states (i.e. $W$ states), using only constant ancillae. Granted additional access to the quantum FAN-OUT operation (i.e. upgrading to the QAC$_f^0$ circuit class), we also achieve exact preparation of arbitrary-weight Dicke states, with polynomial ancillae. These protocols distinguish the constant-depth capabilities of quantum architectures based on connectivity and offer a novel path toward resolving a long-standing quantum complexity conjecture.

</details>


### [82] [Madelung hydrodynamics of spin-orbit coupling: action principles, currents, and correlations](https://arxiv.org/abs/2601.10698)
*Cesare Tronci*

Main category: quant-ph

TL;DR: 该论文利用量子流体动力学变分和哈密顿结构，揭示了自旋轨道耦合在电子运动中的相关性和扭矩机制，分离出SOC诱导的轨道量子力，阐明了自旋输运特征。


<details>
  <summary>Details</summary>
Motivation: 研究自旋轨道耦合在量子流体动力学中的作用机制，特别是SOC诱导的量子力如何影响轨道运动，以及自旋输运中的相关效应。

Method: 基于泡利方程的哈密顿作用原理，利用量子流体动力学的变分和哈密顿结构，分离出SOC诱导的轨道量子力，并通过Madelung-Rashba方程进行具体说明。

Result: 识别出SOC诱导的轨道量子力，这些力源于特定的电流算符，与已知的自旋流体动力学力不同；阐明了自旋霍尔效应中的电流偏移和相关诱导的量子扭矩。

Conclusion: SOC诱导的轨道量子力在自旋轨道相关机制中起关键作用，为理解自旋输运现象提供了新视角，并提出了基于粒子的数值实现方案。

Abstract: We exploit the variational and Hamiltonian structures of quantum hydrodynamics with spin to unfold the correlation and torque mechanisms accompanying spin-orbit coupling (SOC) in electronic motion. Using Hamilton's action principle for the Pauli equation, we isolate SOC-induced quantum forces that act on the orbital Madelung--Bohm trajectories and complement the usual force terms known to appear in quantum hydrodynamics with spin. While the latter spin-hydrodynamic forces relate to the quantum geometric tensor (QGT), SOC-induced orbital forces originate from a particular current operator that contributes prominently to the spin current and whose contribution was overlooked in the past. The distinction between different force terms reveals two fundamentally different mechanisms generating quantum spin-orbit correlations. Leveraging the Hamiltonian structure of the hydrodynamic system, we also elucidate spin transport features such as the current shift in the spin Hall effect and the correlation-induced quantum torques. Finally, we illustrate the framework via the Madelung--Rashba equations for planar SOC configurations and propose a particle-based scheme for numerical implementation.

</details>


### [83] [Scalable Spin Squeezing in Power-Law Interacting XXZ Models with Disorder](https://arxiv.org/abs/2601.10703)
*Samuel E. Begg,Bishal K. Ghosh,Chong Zu,Chuanwei Zhang,Michael Kolodrubetz*

Main category: quant-ph

TL;DR: 该论文研究了二维晶格中自旋压缩对位置无序的鲁棒性，发现在幂律相互作用系统中存在可扩展的自旋压缩，直到一个无序阈值，并解释了先前NV中心实验中缺乏可扩展压缩的原因。


<details>
  <summary>Details</summary>
Motivation: 传统自旋压缩研究主要关注全连接相互作用模型，但最近研究表明幂律相互作用系统也能产生自旋压缩。然而，Wu等人（Nature 646, 2025）在金刚石NV中心实验中显示，位置无序会严重影响自旋压缩，降低其实际应用价值。因此需要研究无序条件下自旋压缩的鲁棒性。

Method: 使用半经典建模方法，研究二维晶格中具有部分未占据格点的系统。分析幂律相互作用的XXZ模型，探索自旋压缩在无序条件下的行为。

Result: 发现存在一个无序阈值，在该阈值以下，自旋压缩是可扩展的（随系统尺寸缩放）；超过该阈值则不可扩展。绘制了可扩展压缩的相图，并解释了先前NV实验中缺乏可扩展压缩的原因。

Conclusion: 该工作确定了在各种量子模拟器中实现可扩展自旋压缩所允许的最大无序程度，发现了一个具有显著无序容忍度的区域，并指出受控缺陷创建是实现固态系统中可扩展压缩的有前景途径。

Abstract: While spin squeezing has been traditionally considered in all-to-all interacting models, recent works have shown that spin squeezing can occur in systems with power-law interactions, leading to direct testing in Rydberg atoms, trapped ions, ultracold atoms and nitrogen vacancy (NV) centers in diamond. For the latter, Wu. et al. Nature 646 (2025) demonstrated that spin squeezing is heavily affected by positional disorder, reducing any capacity for a practical squeezing advantage, which requires scalability with the system size. In this Letter we explore the robustness of spin-squeezing in two-dimensional lattices with a fraction of unoccupied lattice sites. Using semi-classical modeling, we demonstrate the existence of scalable squeezing in power-law interacting XXZ models up to a disorder threshold, above which squeezing is not scalable. We produce a phase diagram for scalable squeezing, and explain its absence in the aforementioned NV experiment. Our work illustrates the maximum disorder allowed for realizing scalable spin squeezing in a host of quantum simulators, highlights a regime with substantial tolerance to disorder, and identifies controlled defect creation as a promising route for scalable squeezing in solid-state systems.

</details>


### [84] [Quantum Maxwell Erasure Decoder for qLDPC codes](https://arxiv.org/abs/2601.10713)
*Bruno Costa Alves Freire,François-Marie Le Régent,Anthony Leverrier*

Main category: quant-ph

TL;DR: 提出量子麦克斯韦擦除解码器，用于CSS量子低密度奇偶校验码，通过有界猜测扩展剥离解码，在复杂度和性能间提供可调平衡


<details>
  <summary>Details</summary>
Motivation: 现有量子LDPC码解码方法在性能和复杂度之间存在权衡，需要一种既能接近最大似然性能又能保持线性时间复杂度的解码方案

Method: 扩展剥离解码方法，引入有界猜测机制，符号化跟踪猜测，通过限制性检查消除猜测，使用猜测预算参数在复杂度和性能间调节

Result: 无约束预算时恢复最大似然性能，常数预算时实现线性时间解码并近似最大似然性能，在双变量自行车码和量子Tanner码上表现优异

Conclusion: 量子麦克斯韦擦除解码器为CSS量子LDPC码提供了一种灵活的解码框架，通过猜测预算参数在解码性能和计算复杂度间实现可控权衡

Abstract: We introduce a quantum Maxwell erasure decoder for CSS quantum low-density parity-check (qLDPC) codes that extends peeling with bounded guessing. Guesses are tracked symbolically and can be eliminated by restrictive checks, giving a tunable tradeoff between complexity and performance via a guessing budget: an unconstrained budget recovers Maximum-Likelihood (ML) performance, while a constant budget yields linear-time decoding and approximates ML. We provide theoretical guarantees on asymptotic performance and demonstrate strong performance on bivariate bicycle and quantum Tanner codes.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [85] [RLC Parameters of a Two-Wire Line with the Finite Element Method](https://arxiv.org/abs/2601.09829)
*Marc Boulé*

Main category: physics.comp-ph

TL;DR: 使用有限元方法计算平行导线对的直流电阻、电感和电容，包含三维无限域建模和开源软件实现


<details>
  <summary>Details</summary>
Motivation: 为平行导线对的低频电磁特性提供准确的计算方法，考虑绝缘层和物理缺陷的影响，并通过开源工具实现可复现的仿真

Method: 采用有限元方法进行三维无限域建模，结合静电、静磁和电流流动的电动力学公式，使用开源ONELAB软件进行仿真

Result: 实现了平行导线对电阻、电感和电容的计算，考虑了绝缘层和缺陷的影响，并通过解析模型和商业软件验证了仿真结果的准确性

Conclusion: 该方法为平行导线对的低频电磁特性分析提供了有效的开源解决方案，可用于实际工程应用中的导线参数计算和缺陷分析

Abstract: This tutorial paper shows how to compute the DC (or low-frequency) resistance, inductance and capacitance of a pair of parallel wires using the finite element method. A three-dimensional infinite domain (open boundary) modeling of electrostatic and magnetostatic fields is presented, along with the electrokinetic formulation for the current flow inside the wires. The effects of the insulation and of a proposed physical defect in the wires are also considered. The open-source ONELAB software is used to perform the simulations and the code listing is provided. Comparisons using analytical models (when applicable) and the Altair Flux software are performed to help validate the simulations.

</details>


### [86] [A Level Set Method on Particle Flow Maps](https://arxiv.org/abs/2601.09939)
*Jinjin He,Taiyuan Zhang,Zhiqi Li,Junwei Zhou,Duowen Chen,Bo Zhu*

Main category: physics.comp-ph

TL;DR: PFM-LS方法结合粒子流映射和水平集，在界面附近使用粒子存储高阶几何信息，实现高保真界面追踪，在复杂变形中保持几何精度和亚网格特征。


<details>
  <summary>Details</summary>
Motivation: 传统水平集方法在复杂界面变形中难以保持几何精度，容易丢失亚网格特征，需要更精确的界面追踪方法。

Method: 在界面窄带区域使用粒子存储水平集值、梯度和Hessian矩阵，通过双向流映射进行平流；采用双时间尺度方法，长程映射处理值和梯度，短程映射频繁重初始化处理Hessian；结合自适应粒子控制和混合粒子-网格准牛顿重距离化方案。

Result: 在2D和3D基准测试中，PFM-LS方法在体积保持和形状保真度方面达到最先进水平，优于现有水平集方法。

Conclusion: PFM-LS方法通过粒子流映射和水平集的结合，实现了高保真界面追踪，能够处理复杂变形并保持亚网格特征，为界面追踪问题提供了新的解决方案。

Abstract: This paper introduces a Particle Flow Map Level Set (PFM-LS) method for high-fidelity interface tracking. We store level-set values, gradients, and Hessians on particles concentrated in a narrow band around the interface, advecting them via bidirectional flow maps while using a conventional grid-based representation elsewhere. By interpreting the level set value as a 3-form and its gradient as a 1-form, PFM-LS achieves exceptional geometric fidelity during complex deformations and preserves sub-grid features that traditional methods cannot capture. Our dual-timescale approach utilizes long-range maps for values and gradients, with frequent reinitialization of short-range maps for the distortion-sensitive Hessian, alongside adaptive particle control that maintains sufficient density within the narrow band. We also develop a hybrid particle-grid quasi-Newton redistancing scheme that preserves fine-scale features while enforcing the signed-distance property. Benchmark comparisons in 2D and 3D demonstrate that PFM-LS achieves state-of-the-art volume preservation and shape fidelity against a broad range of existing level-set methods.

</details>


### [87] [A volume penalization method for solving conjugate scalar transport with interfacial jump conditions](https://arxiv.org/abs/2601.10134)
*Ming Liu,Yosuke Hasegawa*

Main category: physics.comp-ph

TL;DR: 提出了一种新的体积惩罚法中两相界面处理方法，用于求解具有一般界面边界条件的共轭标量输运问题，通过附加源项统一两相控制方程，提高了精度。


<details>
  <summary>Details</summary>
Motivation: 复杂界面几何上的共轭标量输运在热过程和化学过程中很常见，但其准确高效的模拟仍然具有挑战性。现有方法在处理具有界面通量和标量跳跃的一般界面边界条件时存在困难。

Method: 开发了一种体积惩罚法（一种浸入边界法）中的两相界面处理新方法。首先提出了处理Neumann边界条件的对流-扩散方程界面处理方法，然后扩展到具有界面通量和标量跳跃的一般共轭标量输运问题。通过附加源项表示界面标量通量的局部跳跃条件，统一了两相的控制方程。

Result: 一维扩散问题的验证表明该方法提高了精度。进一步应用于流体-固体耦合标量扩散和对流-扩散问题，模拟结果与贴体网格模拟的参考结果吻合良好，平均相对偏差小于3.0%。

Conclusion: 该方法能够准确高效地模拟具有复杂界面几何和一般界面边界条件的共轭标量输运问题，为热过程和化学过程的数值模拟提供了有效的工具。

Abstract: Conjugate scalar transport with interfacial jump conditions on complex interfacial geometries is common in thermal and chemical processes, while its accurate and efficient simulations are still quite challenging. In the present study, a novel treatment of a two-phase interface in the volume penalization method, a kind of immersed boundary method, for solving conjugate scalar transport with general interfacial boundary conditions is developed. We first propose an interfacial treatment for solving an advection-diffusion equation with a Neumann boundary condition, and then extend it to general conjugate scalar transport with both interfacial flux and scalar jumps. A one-dimensional diffusion problem is solved to verify the present scheme and demonstrate the advantage of the present scheme in improving accuracy and unifying the governing equations in the two phases with an additional source term representing the local jump condition of the interfacial scalar flux. Then, the present scheme is further applied to fluid-solid coupled scalar diffusion and advection-diffusion problems with the scalar and its flux jumps across the interface. The simulation results of the present scheme generally show good agreement with reference results obtained by body-fitted mesh simulations with average relative deviations less than 3.0%.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [88] [Social Determinants of Health Prediction for ICD-9 Code with Reasoning Models](https://arxiv.org/abs/2601.09709)
*Sharim Khan,Paul Landes,Adam Cross,Jimeng Sun*

Main category: cs.LG

TL;DR: 该研究探索使用推理模型和传统大语言模型在MIMIC-III数据集上进行医院入院多标签社会健康决定因素ICD-9代码分类，利用现有ICD-9代码预测入院情况，达到89% F1分数


<details>
  <summary>Details</summary>
Motivation: 社会健康决定因素与患者预后相关，但很少在结构化数据中捕获。虽然大语言模型在从句子中识别这些标记方面表现良好，但在大型入院或纵向记录中预测具有挑战性，因为存在长距离依赖关系

Method: 在MIMIC-III数据集上使用推理模型和传统大语言模型进行医院入院多标签社会健康决定因素ICD-9代码分类，利用现有ICD-9代码进行预测

Result: 达到了89%的F1分数，发现了139个入院记录中缺失的社会健康决定因素代码

Conclusion: 该方法能有效从临床文本中提取社会健康决定因素标记，补充诊断系统对患者社会环境的了解，并提供了可复现结果的代码

Abstract: Social Determinants of Health correlate with patient outcomes but are rarely captured in structured data. Recent attention has been given to automatically extracting these markers from clinical text to supplement diagnostic systems with knowledge of patients' social circumstances. Large language models demonstrate strong performance in identifying Social Determinants of Health labels from sentences. However, prediction in large admissions or longitudinal notes is challenging given long distance dependencies. In this paper, we explore hospital admission multi-label Social Determinants of Health ICD-9 code classification on the MIMIC-III dataset using reasoning models and traditional large language models. We exploit existing ICD-9 codes for prediction on admissions, which achieved an 89% F1. Our contributions include our findings, missing SDoH codes in 139 admissions, and code to reproduce the results.

</details>


### [89] [The Geometry of Thought: Disclosing the Transformer as a Tropical Polynomial Circuit](https://arxiv.org/abs/2601.09775)
*Faruk Alpay,Bilge Senturk*

Main category: cs.LG

TL;DR: Transformer自注意力机制在高温极限下（β→∞）等价于热带半环（max-plus代数）中的矩阵乘法，揭示了Transformer前向传播本质上是基于token相似度定义的隐式图上执行动态规划（Bellman-Ford路径查找算法）。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer自注意力机制的数学本质，特别是在高温极限下的代数结构，为理解Transformer的计算机制和链式推理能力提供新的理论视角。

Method: 通过分析softmax注意力在逆温度β→∞时的极限行为，证明其收敛到热带半环中的矩阵乘法，将Transformer前向传播解释为隐式图上的动态规划算法。

Result: 证明了Transformer自注意力在高温极限下等价于热带矩阵乘法，揭示了网络计算本质上是在执行Bellman-Ford路径查找算法，为链式推理提供了最短路径/最长路径的几何解释。

Conclusion: Transformer的链式推理能力源于其内在的图路径优化算法，这一热带代数视角为理解Transformer的计算机制和推理能力提供了新的理论基础。

Abstract: We prove that the Transformer self-attention mechanism in the high-confidence regime ($β\to \infty$, where $β$ is an inverse temperature) operates in the tropical semiring (max-plus algebra). In particular, we show that taking the tropical limit of the softmax attention converts it into a tropical matrix product. This reveals that the Transformer's forward pass is effectively executing a dynamic programming recurrence (specifically, a Bellman-Ford path-finding update) on a latent graph defined by token similarities. Our theoretical result provides a new geometric perspective for chain-of-thought reasoning: it emerges from an inherent shortest-path (or longest-path) algorithm being carried out within the network's computation.

</details>


### [90] [TimeSAE: Sparse Decoding for Faithful Explanations of Black-Box Time Series Models](https://arxiv.org/abs/2601.09776)
*Khalid Oublal,Quentin Bouniot,Qi Gan,Stephan Clémençon,Zeynep Akata*

Main category: cs.LG

TL;DR: TimeSAE：基于稀疏自编码器和因果关系的时序黑盒模型解释框架，提供更忠实和鲁棒的解释


<details>
  <summary>Details</summary>
Motivation: 随着黑盒模型和预训练模型在时序应用中日益普及，理解其预测变得至关重要，特别是在需要可解释性和信任的高风险领域。现有方法大多只涉及分布内解释，无法泛化到训练支持之外，缺乏泛化能力。

Method: 基于稀疏自编码器概念，提出TimeSAE框架，通过稀疏自编码器和因果关系的双重视角来解释时序数据的黑盒模型。

Result: 在合成和真实世界时序数据集上的广泛评估表明，TimeSAE相比现有基线方法提供了更忠实和鲁棒的解释，定量指标和定性分析均支持这一结论。

Conclusion: TimeSAE为解决时序黑盒模型解释问题提供了一个有效的框架，能够处理分布偏移问题，提高解释的泛化能力，并已开源为易用库TimeSAE-Lib。

Abstract: As black box models and pretrained models gain traction in time series applications, understanding and explaining their predictions becomes increasingly vital, especially in high-stakes domains where interpretability and trust are essential. However, most of the existing methods involve only in-distribution explanation, and do not generalize outside the training support, which requires the learning capability of generalization. In this work, we aim to provide a framework to explain black-box models for time series data through the dual lenses of Sparse Autoencoders (SAEs) and causality. We show that many current explanation methods are sensitive to distributional shifts, limiting their effectiveness in real-world scenarios. Building on the concept of Sparse Autoencoder, we introduce TimeSAE, a framework for black-box model explanation. We conduct extensive evaluations of TimeSAE on both synthetic and real-world time series datasets, comparing it to leading baselines. The results, supported by both quantitative metrics and qualitative insights, show that TimeSAE provides more faithful and robust explanations. Our code is available in an easy-to-use library TimeSAE-Lib: https://anonymous.4open.science/w/TimeSAE-571D/.

</details>


### [91] [QFed: Parameter-Compact Quantum-Classical Federated Learning](https://arxiv.org/abs/2601.09809)
*Samar Abdelghani,Soumaya Cherkaoui*

Main category: cs.LG

TL;DR: 量子辅助联邦学习框架QFed在保持精度的同时，将VGG类模型的参数量减少了77.6%，为边缘设备上的联邦学习提供了更高效的计算方案。


<details>
  <summary>Details</summary>
Motivation: 医疗、金融和科研等领域需要在保护隐私和遵守法规的前提下，从分布式数据中提取集体智能。传统联邦学习面临统计异质性、系统多样性和复杂模型计算负担等挑战，量子计算有望通过多对数因子减少参数数量来降低训练开销。

Method: 提出QFed量子增强联邦学习框架，利用量子计算减少经典模型参数数量。在FashionMNIST数据集上评估框架性能，使用VGG类模型进行实验验证。

Result: QFed实现了77.6%的参数减少，同时保持了与经典方法相当的准确率，在可扩展环境中验证了量子辅助联邦学习的有效性。

Conclusion: 量子计算与联邦学习的结合能够显著增强边缘设备的联邦学习能力，通过减少参数数量来降低计算开销，为解决隐私保护下的分布式学习问题提供了新思路。

Abstract: Organizations and enterprises across domains such as healthcare, finance, and scientific research are increasingly required to extract collective intelligence from distributed, siloed datasets while adhering to strict privacy, regulatory, and sovereignty requirements. Federated Learning (FL) enables collaborative model building without sharing sensitive raw data, but faces growing challenges posed by statistical heterogeneity, system diversity, and the computational burden from complex models. This study examines the potential of quantum-assisted federated learning, which could cut the number of parameters in classical models by polylogarithmic factors and thus lessen training overhead. Accordingly, we introduce QFed, a quantum-enabled federated learning framework aimed at boosting computational efficiency across edge device networks. We evaluate the proposed framework using the widely adopted FashionMNIST dataset. Experimental results show that QFed achieves a 77.6% reduction in the parameter count of a VGG-like model while maintaining an accuracy comparable to classical approaches in a scalable environment. These results point to the potential of leveraging quantum computing within a federated learning context to strengthen FL capabilities of edge devices.

</details>


### [92] [Eluder dimension: localise it!](https://arxiv.org/abs/2601.09825)
*Alireza Bakhtiari,Alex Ayoub,Samuel Robertson,David Janz,Csaba Szepesvári*

Main category: cs.LG

TL;DR: 论文提出了eluder维度的局部化方法，解决了标准eluder维度分析无法获得一阶遗憾界的问题，并在伯努利多臂赌博机和有限时域强化学习中取得了改进结果。


<details>
  <summary>Details</summary>
Motivation: 标准eluder维度分析无法为广义线性模型类获得一阶遗憾界，这限制了其在更复杂任务中的应用。

Method: 引入了eluder维度的局部化方法，通过局部化分析来改进遗憾界。

Result: 恢复了伯努利多臂赌博机的经典结果并有所改进，首次为有界累积回报的有限时域强化学习任务获得真正的一阶遗憾界。

Conclusion: eluder维度的局部化方法有效解决了标准分析的限制，为更广泛的强化学习任务提供了一阶遗憾界分析框架。

Abstract: We establish a lower bound on the eluder dimension of generalised linear model classes, showing that standard eluder dimension-based analysis cannot lead to first-order regret bounds. To address this, we introduce a localisation method for the eluder dimension; our analysis immediately recovers and improves on classic results for Bernoulli bandits, and allows for the first genuine first-order bounds for finite-horizon reinforcement learning tasks with bounded cumulative returns.

</details>


### [93] [A New Convergence Analysis of Plug-and-Play Proximal Gradient Descent Under Prior Mismatch](https://arxiv.org/abs/2601.09831)
*Guixian Xu,Jinglai Li,Junqi Tang*

Main category: cs.LG

TL;DR: 本文提出了在先验不匹配情况下（即去噪器训练数据分布与推理任务不同）的PnP-PGD新收敛理论，这是首个在此条件下的收敛性证明，相比现有理论移除了多个限制性且难以验证的假设。


<details>
  <summary>Details</summary>
Motivation: 现有PnP算法的收敛理论需要多个限制性且难以验证的假设，特别是在先验不匹配情况下（去噪器训练数据与推理任务分布不同）缺乏理论保证。本文旨在填补这一理论空白。

Method: 提出了plug-and-play proximal gradient descent (PnP-PGD)的新收敛理论，特别关注先验不匹配情况，移除了现有理论中的多个限制性假设。

Result: 首次证明了在先验不匹配情况下PnP-PGD的收敛性，相比现有理论移除了多个限制性且难以验证的假设，为实际应用提供了更可靠的理论基础。

Conclusion: 本文为PnP-PGD在先验不匹配情况下提供了首个收敛性证明，显著推进了PnP算法的理论发展，使其在实际应用中更具可靠性。

Abstract: In this work, we provide a new convergence theory for plug-and-play proximal gradient descent (PnP-PGD) under prior mismatch where the denoiser is trained on a different data distribution to the inference task at hand. To the best of our knowledge, this is the first convergence proof of PnP-PGD under prior mismatch. Compared with the existing theoretical results for PnP algorithms, our new results removed the need for several restrictive and unverifiable assumptions.

</details>


### [94] [A pipeline for enabling path-specific causal fairness in observational health data](https://arxiv.org/abs/2601.09841)
*Aparajita Kashyap,Sara Matijevic,Noémie Elhadad,Steven A. Kushner,Shalmali Joshi*

Main category: cs.LG

TL;DR: 该研究提出了一个用于训练因果公平机器学习模型的通用流程，重点关注医疗保健领域中的偏见问题，特别是直接和间接的医疗偏见。


<details>
  <summary>Details</summary>
Motivation: 在医疗保健环境中部署机器学习模型时，需要确保模型不会复制或加剧现有的医疗偏见。虽然存在多种公平性定义，但研究关注路径特定的因果公平性，以更好地考虑偏见发生的社会和医疗背景（如临床医生或模型的直接歧视与医疗系统差异访问导致的偏见）。

Method: 将结构公平模型映射到观察性医疗保健设置中，创建一个通用的因果公平模型训练流程。该流程明确考虑特定的医疗背景和差异来定义目标"公平"模型，利用未受公平约束的基础模型在观察性健康数据上训练，生成具有因果公平性的下游预测。

Result: 该工作填补了两个主要空白：1）通过解耦直接和间接偏见来源，扩展了对"公平性-准确性"权衡的表征；2）展示了如何在具有已知社会和医疗差异的任务中，利用未受公平约束的基础模型生成因果公平的下游预测。

Conclusion: 该研究提出了一个模型无关的流程，用于训练因果公平的机器学习模型，同时解决直接和间接形式的医疗保健偏见，为医疗AI的公平部署提供了实用框架。

Abstract: When training machine learning (ML) models for potential deployment in a healthcare setting, it is essential to ensure that they do not replicate or exacerbate existing healthcare biases. Although many definitions of fairness exist, we focus on path-specific causal fairness, which allows us to better consider the social and medical contexts in which biases occur (e.g., direct discrimination by a clinician or model versus bias due to differential access to the healthcare system) and to characterize how these biases may appear in learned models. In this work, we map the structural fairness model to the observational healthcare setting and create a generalizable pipeline for training causally fair models. The pipeline explicitly considers specific healthcare context and disparities to define a target "fair" model. Our work fills two major gaps: first, we expand on characterizations of the "fairness-accuracy" tradeoff by detangling direct and indirect sources of bias and jointly presenting these fairness considerations alongside considerations of accuracy in the context of broadly known biases. Second, we demonstrate how a foundation model trained without fairness constraints on observational health data can be leveraged to generate causally fair downstream predictions in tasks with known social and medical disparities. This work presents a model-agnostic pipeline for training causally fair machine learning models that address both direct and indirect forms of healthcare bias.

</details>


### [95] [Advancing Model Refinement: Muon-Optimized Distillation and Quantization for LLM Deployment](https://arxiv.org/abs/2601.09865)
*Jacob Sander,Brian Jalaian,Venkat R. Dasari*

Main category: cs.LG

TL;DR: 提出一个集成框架，结合GPTQ量化、LoRA和专门的数据蒸馏过程，显著减小LLM大小和复杂度，同时保持或提升任务特定性能，实现高达2倍内存压缩。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在资源受限的边缘设备上部署面临计算、内存和能耗挑战，需要解决任务特定数据获取、性能微调和模型压缩三个关键问题。

Method: 集成框架结合GPTQ量化、低秩适应(LoRA)和专门的数据蒸馏过程，利用数据蒸馏、KL散度知识蒸馏、贝叶斯超参数优化和Muon优化器。

Result: 实现高达2倍内存压缩（如6GB模型减至3GB），在标准LLM基准测试中表现优于单独GPTQ量化，Muon优化器显著增强微调模型在量化过程中的抗精度衰减能力。

Conclusion: 提出的集成框架能有效压缩LLM模型，在资源受限的边缘设备上实现高效推理，同时保持或提升任务特定性能，为边缘部署提供了可行解决方案。

Abstract: Large Language Models (LLMs) enable advanced natural language processing but face deployment challenges on resource-constrained edge devices due to high computational, memory, and energy demands. Optimizing these models requires addressing three key challenges: acquiring task-specific data, fine-tuning for performance, and compressing models to accelerate inference while reducing resource demands. We propose an integrated framework combining GPTQ-based quantization, low-rank adaptation (LoRA), and a specialized data distillation process to significantly reduce model size and complexity while preserving or enhancing task-specific performance. By leveraging data distillation, knowledge distillation via Kullback-Leibler divergence, Bayesian hyperparameter optimization, and the Muon optimizer, our pipeline achieves up to 2x memory compression (e.g., reducing a 6GB model to 3GB) and enables efficient inference for specialized tasks. Empirical results demonstrate superior performance on standard LLM benchmarks compared to GPTQ quantization alone, with the Muon optimizer notably enhancing fine-tuned models' resistance to accuracy decay during quantization.

</details>


### [96] [The PROPER Approach to Proactivity: Benchmarking and Advancing Knowledge Gap Navigation](https://arxiv.org/abs/2601.09926)
*Kirandeep Kaur,Vinayak Gupta,Aditya Gupta,Chirag Shah*

Main category: cs.LG

TL;DR: ProPer是一个两智能体架构，通过生成用户未表达的潜在维度，实现个性化、主动的AI助手响应，相比传统被动式助手显著提升质量。


<details>
  <summary>Details</summary>
Motivation: 现有语言助手多为被动问答模式，用户需明确表达需求，导致相关但未表达的需求无法满足。现有主动式助手要么需要用户澄清（增加负担），要么从上下文推断未来需求（常导致不必要或时机不当的干预）。

Method: 提出ProPer两智能体架构：1) 维度生成智能体(DGA)：微调LLM，利用显式用户数据生成多个隐式维度（用户任务相关但用户未考虑的潜在方面）或知识缺口；2) 响应生成智能体(RGA)：平衡显式和隐式维度，生成个性化响应并进行及时主动干预。使用基于质量、多样性和任务相关性的重排序器筛选维度。

Result: 在多领域评估中，ProPer在所有领域均提升质量分数和胜率，单轮评估中最高提升84%，在多轮交互中持续占优。使用结构化、缺口感知的评估标准衡量覆盖率、主动性适当性和意图对齐。

Conclusion: ProPer通过两智能体架构有效识别和利用用户未表达的潜在维度，实现更个性化、主动且时机恰当的AI助手响应，解决了传统被动式助手的局限性。

Abstract: Most language-based assistants follow a reactive ask-and-respond paradigm, requiring users to explicitly state their needs. As a result, relevant but unexpressed needs often go unmet. Existing proactive agents attempt to address this gap either by eliciting further clarification, preserving this burden, or by extrapolating future needs from context, often leading to unnecessary or mistimed interventions. We introduce ProPer, Proactivity-driven Personalized agents, a novel two-agent architecture consisting of a Dimension Generating Agent (DGA) and a Response Generating Agent (RGA). DGA, a fine-tuned LLM agent, leverages explicit user data to generate multiple implicit dimensions (latent aspects relevant to the user's task but not considered by the user) or knowledge gaps. These dimensions are selectively filtered using a reranker based on quality, diversity, and task relevance. RGA then balances explicit and implicit dimensions to tailor personalized responses with timely and proactive interventions. We evaluate ProPer across multiple domains using a structured, gap-aware rubric that measures coverage, initiative appropriateness, and intent alignment. Our results show that ProPer improves quality scores and win rates across all domains, achieving up to 84% gains in single-turn evaluation and consistent dominance in multi-turn interactions.

</details>


### [97] [Interpolation-Based Optimization for Enforcing lp-Norm Metric Differential Privacy in Continuous and Fine-Grained Domains](https://arxiv.org/abs/2601.09946)
*Chenxi Qiu*

Main category: cs.LG

TL;DR: 提出基于插值的框架优化细粒度连续域中的lp范数度量差分隐私，通过锚点优化和log凸组合插值，在保持隐私的同时提升效用。


<details>
  <summary>Details</summary>
Motivation: 现有基于优化的度量差分隐私方法在粗粒度域中能有效减少效用损失，但在细粒度或连续设置中，由于需要构建密集的扰动矩阵并满足逐点约束，计算成本高昂，优化仍然具有挑战性。

Method: 提出插值框架：在稀疏锚点集上优化扰动分布，通过log凸组合在非锚点位置插值分布（可证明保持mDP）。针对高维空间中朴素插值导致的隐私违规，将插值过程分解为一系列一维步骤，推导出强制lp范数mDP的修正公式。进一步探索扰动分布和隐私预算跨维度分配的联合优化。

Result: 在真实世界位置数据集上的实验表明，该方法在细粒度域中提供严格的隐私保证和具有竞争力的效用，优于基线机制。

Conclusion: 提出的插值框架成功解决了细粒度连续域中优化度量差分隐私的挑战，通过锚点优化和修正插值实现了计算效率和隐私保证的平衡，为高维空间中的隐私保护提供了有效解决方案。

Abstract: Metric Differential Privacy (mDP) generalizes Local Differential Privacy (LDP) by adapting privacy guarantees based on pairwise distances, enabling context-aware protection and improved utility. While existing optimization-based methods reduce utility loss effectively in coarse-grained domains, optimizing mDP in fine-grained or continuous settings remains challenging due to the computational cost of constructing dense perterubation matrices and satisfying pointwise constraints.
  In this paper, we propose an interpolation-based framework for optimizing lp-norm mDP in such domains. Our approach optimizes perturbation distributions at a sparse set of anchor points and interpolates distributions at non-anchor locations via log-convex combinations, which provably preserve mDP. To address privacy violations caused by naive interpolation in high-dimensional spaces, we decompose the interpolation process into a sequence of one-dimensional steps and derive a corrected formulation that enforces lp-norm mDP by design. We further explore joint optimization over perturbation distributions and privacy budget allocation across dimensions. Experiments on real-world location datasets demonstrate that our method offers rigorous privacy guarantees and competitive utility in fine-grained domains, outperforming baseline mechanisms. in high-dimensional spaces, we decompose the interpolation process into a sequence of one-dimensional steps and derive a corrected formulation that enforces lp-norm mDP by design. We further explore joint optimization over perturbation distributions and privacy budget allocation across dimensions. Experiments on real-world location datasets demonstrate that our method offers rigorous privacy guarantees and competitive utility in fine-grained domains, outperforming baseline mechanisms.

</details>


### [98] [Kinematic Tokenization: Optimization-Based Continuous-Time Tokens for Learnable Decision Policies in Noisy Time Series](https://arxiv.org/abs/2601.09949)
*Griffin Kearney*

Main category: cs.LG

TL;DR: 论文提出Kinematic Tokenization方法，将连续时间信号表示为样条系数，在金融时间序列中优于离散标记化方法，特别是在噪声环境下具有不对称惩罚的任务中。


<details>
  <summary>Details</summary>
Motivation: Transformer设计用于离散标记，但现实世界信号是连续过程且存在噪声采样。传统离散标记化方法（原始值、补丁、有限差分）在低信噪比环境下脆弱，特别是当下游目标施加不对称惩罚时，理性策略会选择弃权。

Method: 提出Kinematic Tokenization方法：基于优化的连续时间表示，从噪声测量中重建显式样条，并将局部样条系数（位置、速度、加速度、急动度）作为标记。应用于金融时间序列数据（资产价格和交易量分布）。

Result: 在多资产日频股票测试平台上，使用风险厌恶不对称分类目标作为学习性压力测试。在此目标下，多个离散基线方法崩溃为吸收现金策略（清算均衡），而连续样条标记能够维持校准的非平凡动作分布和稳定策略。

Conclusion: 显式连续时间标记可以改善噪声时间序列中在弃权诱导损失下的选择性决策策略的学习性和校准性。

Abstract: Transformers are designed for discrete tokens, yet many real-world signals are continuous processes observed through noisy sampling. Discrete tokenizations (raw values, patches, finite differences) can be brittle in low signal-to-noise regimes, especially when downstream objectives impose asymmetric penalties that rationally encourage abstention. We introduce Kinematic Tokenization, an optimization-based continuous-time representation that reconstructs an explicit spline from noisy measurements and tokenizes local spline coefficients (position, velocity, acceleration, jerk). This is applied to financial time series data in the form of asset prices in conjunction with trading volume profiles. Across a multi-asset daily-equity testbed, we use a risk-averse asymmetric classification objective as a stress test for learnability. Under this objective, several discrete baselines collapse to an absorbing cash policy (the Liquidation Equilibrium), whereas the continuous spline tokens sustain calibrated, non-trivial action distributions and stable policies. These results suggest that explicit continuous-time tokens can improve the learnability and calibration of selective decision policies in noisy time series under abstention-inducing losses.

</details>


### [99] [A Sustainable AI Economy Needs Data Deals That Work for Generators](https://arxiv.org/abs/2601.09966)
*Ruoxi Jia,Luis Oala,Wenjie Xiong,Suqin Ge,Jiachen T. Wang,Feiyang Kang,Dawn Song*

Main category: cs.LG

TL;DR: 机器学习价值链存在结构性不可持续性，数据生成者在价值分配中被边缘化，大部分价值流向聚合者，需要建立公平的数据价值交换框架。


<details>
  <summary>Details</summary>
Motivation: 本文动机是揭示机器学习价值链中的结构性不平等问题。研究发现数据从输入到模型权重再到合成输出的过程中，技术信号被提炼但经济权益从数据生成者身上被剥离，这不仅是经济福利问题，也威胁到当前学习算法的可持续性。

Method: 通过分析73个公开数据交易案例，发现大多数价值流向聚合者，创作者版税几乎为零且交易条款普遍不透明。识别出三个结构性缺陷：缺失来源追溯、不对称议价能力和非动态定价。

Result: 分析显示机器学习价值链存在严重的经济数据处理不平等，数据生成者获得的价值极少，交易透明度低。这种不平等不仅影响经济公平，还威胁到机器学习系统的可持续性。

Conclusion: 提出公平数据价值交换（EDVEX）框架，旨在建立一个使所有参与者受益的最小化市场。同时为研究社区指明在数据交易方面可以做出具体贡献的研究方向。

Abstract: We argue that the machine learning value chain is structurally unsustainable due to an economic data processing inequality: each state in the data cycle from inputs to model weights to synthetic outputs refines technical signal but strips economic equity from data generators. We show, by analyzing seventy-three public data deals, that the majority of value accrues to aggregators, with documented creator royalties rounding to zero and widespread opacity of deal terms. This is not just an economic welfare concern: as data and its derivatives become economic assets, the feedback loop that sustains current learning algorithms is at risk. We identify three structural faults - missing provenance, asymmetric bargaining power, and non-dynamic pricing - as the operational machinery of this inequality. In our analysis, we trace these problems along the machine learning value chain and propose an Equitable Data-Value Exchange (EDVEX) Framework to enable a minimal market that benefits all participants. Finally, we outline research directions where our community can make concrete contributions to data deals and contextualize our position with related and orthogonal viewpoints.

</details>


### [100] [An Exploratory Study to Repurpose LLMs to a Unified Architecture for Time Series Classification](https://arxiv.org/abs/2601.09971)
*Hansen He,Shuheng Li*

Main category: cs.LG

TL;DR: 该研究探索了将专门的时间序列编码器与冻结的大型语言模型（LLM）主干结合的混合架构，发现Inception模型是唯一能持续带来性能提升的编码器架构。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类（TSC）是机器学习核心问题，现有研究主要关注将时间序列数据映射到文本域的校准策略，但时间序列编码器架构的选择尚未充分探索。

Method: 研究采用探索性方法，评估多种时间序列编码器家族（包括Inception、卷积、残差、基于Transformer和多层感知机架构）与冻结LLM主干结合的混合架构。

Result: 在所有评估的编码器架构中，只有Inception模型在与LLM主干集成时能持续产生正向性能提升。

Conclusion: 时间序列编码器的选择对混合LLM架构有重要影响，基于Inception的模型是未来LLM驱动时间序列学习的有前景方向。

Abstract: Time series classification (TSC) is a core machine learning problem with broad applications. Recently there has been growing interest in repurposing large language models (LLMs) for TSC, motivated by their strong reasoning and generalization ability. Prior work has primarily focused on alignment strategies that explicitly map time series data into the textual domain; however, the choice of time series encoder architecture remains underexplored. In this work, we conduct an exploratory study of hybrid architectures that combine specialized time series encoders with a frozen LLM backbone. We evaluate a diverse set of encoder families, including Inception, convolutional, residual, transformer-based, and multilayer perceptron architectures, among which the Inception model is the only encoder architecture that consistently yields positive performance gains when integrated with an LLM backbone. Overall, this study highlights the impact of time series encoder choice in hybrid LLM architectures and points to Inception-based models as a promising direction for future LLM-driven time series learning.

</details>


### [101] [In-Context Operator Learning on the Space of Probability Measures](https://arxiv.org/abs/2601.09979)
*Frank Cole,Dixi Wang,Yineng Chen,Yulong Lu,Rongjie Lai*

Main category: cs.LG

TL;DR: 提出一种基于概率测度空间的上下文算子学习方法，用于最优传输问题，通过少量样本作为提示学习从分布对到OT映射的算子，无需推理时的梯度更新。


<details>
  <summary>Details</summary>
Motivation: 传统最优传输方法通常需要为每个新分布对重新计算，计算成本高。本文旨在学习一个通用的OT映射算子，仅需少量样本作为上下文提示，实现快速推理。

Method: 参数化解算子，在非参数和参数两种设置下开发理论。非参数设置中，当任务集中在低内在维度的源-目标对流形时，建立泛化边界；参数设置中（如高斯族），给出显式架构以精确恢复OT映射。

Result: 建立了上下文准确率与提示大小、任务内在维度和模型容量之间的缩放规律理论。在合成传输和生成建模基准测试中验证了框架的有效性。

Conclusion: 提出的上下文算子学习方法能够在概率测度空间上有效学习最优传输映射，仅需少量样本作为提示，无需推理时的梯度更新，为OT问题提供了高效的新范式。

Abstract: We introduce \emph{in-context operator learning on probability measure spaces} for optimal transport (OT). The goal is to learn a single solution operator that maps a pair of distributions to the OT map, using only few-shot samples from each distribution as a prompt and \emph{without} gradient updates at inference. We parameterize the solution operator and develop scaling-law theory in two regimes. In the \emph{nonparametric} setting, when tasks concentrate on a low-intrinsic-dimension manifold of source--target pairs, we establish generalization bounds that quantify how in-context accuracy scales with prompt size, intrinsic task dimension, and model capacity. In the \emph{parametric} setting (e.g., Gaussian families), we give an explicit architecture that recovers the exact OT map in context and provide finite-sample excess-risk bounds. Our numerical experiments on synthetic transports and generative-modeling benchmarks validate the framework.

</details>


### [102] [FaTRQ: Tiered Residual Quantization for LLM Vector Search in Far-Memory-Aware ANNS Systems](https://arxiv.org/abs/2601.09985)
*Tianqi Zhang,Flavio Ponzina,Tajana Rosing*

Main category: cs.LG

TL;DR: FaTRQ是一个面向远内存感知的ANNS精炼系统，通过分层内存和渐进距离估计消除从存储中获取完整向量的需求，显著提升存储效率和查询吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现代ANNS引擎虽然使用预建索引和向量量化加速检索，但仍需从SSD等慢速存储读取完整精度向量进行二次精炼，这已成为整个查询延迟的主要瓶颈，特别是对于现代文本和多模态嵌入。

Method: 1) 提出渐进距离估计器，使用从远内存流式传输的紧凑残差来精炼粗略分数；2) 提出分层残差量化，将残差编码为三元值高效存储在远内存中；3) 在CXL Type-2设备中部署定制加速器进行低延迟本地精炼。

Result: FaTRQ相比最先进的GPU ANNS系统，存储效率提升2.4倍，吞吐量提升高达9倍。

Conclusion: FaTRQ通过消除从存储获取完整向量的需求，有效解决了ANNS精炼阶段的性能瓶颈，为大规模向量检索提供了高效解决方案。

Abstract: Approximate Nearest-Neighbor Search (ANNS) is a key technique in retrieval-augmented generation (RAG), enabling rapid identification of the most relevant high-dimensional embeddings from massive vector databases. Modern ANNS engines accelerate this process using prebuilt indexes and store compressed vector-quantized representations in fast memory. However, they still rely on a costly second-pass refinement stage that reads full-precision vectors from slower storage like SSDs. For modern text and multimodal embeddings, these reads now dominate the latency of the entire query. We propose FaTRQ, a far-memory-aware refinement system using tiered memory that eliminates the need to fetch full vectors from storage. It introduces a progressive distance estimator that refines coarse scores using compact residuals streamed from far memory. Refinement stops early once a candidate is provably outside the top-k. To support this, we propose tiered residual quantization, which encodes residuals as ternary values stored efficiently in far memory. A custom accelerator is deployed in a CXL Type-2 device to perform low-latency refinement locally. Together, FaTRQ improves the storage efficiency by 2.4$\times$ and improves the throughput by up to 9$ \times$ than SOTA GPU ANNS system.

</details>


### [103] [Continuous-Depth Transformers with Learned Control Dynamics](https://arxiv.org/abs/2601.10007)
*Peter Jemley*

Main category: cs.LG

TL;DR: 提出混合Transformer架构，用连续深度神经ODE块替代离散中间层，通过学习到的控制信号实现推理时生成属性控制


<details>
  <summary>Details</summary>
Motivation: 标准Transformer通过固定离散层处理表示，缺乏推理时对生成属性的灵活控制能力。本文旨在将深度视为连续变量，通过控制信号实现可操控的语言生成

Method: 设计混合Transformer架构，用连续深度神经ODE块替换离散中间层，使用学习向量场F_θ(H, τ, u)控制深度，其中u是通过显式拼接注入的低维控制信号

Result: 梯度流稳定无爆炸/消失问题；语义控制正负情感准确率达98%/88%；连续插值轨迹差异仅0.068%；延迟与标准基线相当；控制信号将向量场划分为不同曲率特性的动态机制

Conclusion: 带有学习控制信号的连续深度动力学为可操控语言生成提供了可行高效的机制，伴随方法实现O(1)内存训练，自适应ODE求解器揭示了学习动力学中的几何结构

Abstract: We present a hybrid transformer architecture that replaces discrete middle layers with a continuous-depth Neural Ordinary Differential Equation (ODE) block, enabling inference-time control over generation attributes via a learned steering signal. Unlike standard transformers that process representations through fixed discrete layers, our approach treats depth as a continuous variable governed by a learned vector field $F_θ(H, τ, u)$, where $u$ is a low-dimensional control signal injected via explicit concatenation. We validate the architecture through four experiments: (1) gradient flow stability with zero exploding/vanishing gradient events, (2) semantic steering achieving 98\%/88\% accuracy for positive/negative sentiment control, (3) continuous interpolation validated by a negligible 0.068\% trajectory divergence between fixed and adaptive solvers, and (4) efficiency benchmarking demonstrating latency parity with standard discrete baselines. Additionally, we show that adaptive ODE solvers reveal geometric structure in the learned dynamics: the control signal partitions the vector field into distinct dynamical regimes with different curvature characteristics. The adjoint method enables $O(1)$ memory training regardless of integration depth. Our results demonstrate that continuous-depth dynamics with learned control signals provide a viable, efficient mechanism for steerable language generation.

</details>


### [104] [PID-Guided Partial Alignment for Multimodal Decentralized Federated Learning](https://arxiv.org/abs/2601.10012)
*Yanhang Shi,Xiaoyu Wang,Houwei Cao,Jian Li,Yong Liu*

Main category: cs.LG

TL;DR: PARSE：基于部分信息分解的多模态去中心化联邦学习框架，通过特征分裂和切片级对齐解决异构代理间的梯度冲突


<details>
  <summary>Details</summary>
Motivation: 多模态去中心化联邦学习面临挑战：代理在可用模态和模型架构上存在差异，且必须在无中心协调器的P2P网络中协作。传统的多模态方法学习所有模态的单一共享嵌入，在DFL中会导致单模态与多模态代理间的梯度不对齐，抑制异构共享和跨模态交互。

Method: PARSE框架基于部分信息分解原理，每个代理执行特征分裂，将潜在表示分解为冗余、独特和协同切片。通过切片级部分对齐实现异构代理间的P2P知识共享：仅交换具有对应模态的语义可共享分支。无需中心协调和梯度手术，解决了单/多模态梯度冲突。

Result: 在基准测试和代理混合实验中，PARSE相比任务、模态和混合共享的DFL基线方法取得了一致的性能提升。对融合算子和分裂比率的消融研究以及定性可视化进一步证明了所提设计的效率和鲁棒性。

Conclusion: PARSE通过部分信息分解和切片级对齐，有效解决了多模态去中心化联邦学习中的异构协作难题，在保持与标准DFL约束兼容的同时，克服了多模态DFL困境。

Abstract: Multimodal decentralized federated learning (DFL) is challenging because agents differ in available modalities and model architectures, yet must collaborate over peer-to-peer (P2P) networks without a central coordinator. Standard multimodal pipelines learn a single shared embedding across all modalities. In DFL, such a monolithic representation induces gradient misalignment between uni- and multimodal agents; as a result, it suppresses heterogeneous sharing and cross-modal interaction. We present PARSE, a multimodal DFL framework that operationalizes partial information decomposition (PID) in a server-free setting. Each agent performs feature fission to factorize its latent representation into redundant, unique, and synergistic slices. P2P knowledge sharing among heterogeneous agents is enabled by slice-level partial alignment: only semantically shareable branches are exchanged among agents that possess the corresponding modality. By removing the need for central coordination and gradient surgery, PARSE resolves uni-/multimodal gradient conflicts, thereby overcoming the multimodal DFL dilemma while remaining compatible with standard DFL constraints. Across benchmarks and agent mixes, PARSE yields consistent gains over task-, modality-, and hybrid-sharing DFL baselines. Ablations on fusion operators and split ratios, together with qualitative visualizations, further demonstrate the efficiency and robustness of the proposed design.

</details>


### [105] [CAFEDistill: Learning Personalized and Dynamic Models through Federated Early-Exit Network Distillation](https://arxiv.org/abs/2601.10015)
*Boyi Liu,Zimu Zhou,Yongxin Tong*

Main category: cs.LG

TL;DR: CAFEDistill是一个冲突感知的联邦退出蒸馏框架，将早期退出网络集成到个性化联邦学习中，通过深度优先的学生协调机制解决客户端异构性和深度干扰冲突，实现自适应推理并降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有PFL方法产生静态模型，在准确性和效率之间固定权衡，无法适应不同上下文和资源可用性的推理需求。早期退出网络提供自适应推理，但集成到PFL中面临客户端异构性和深度干扰的挑战，现有方法无法同时解决这两个冲突。

Method: 提出CAFEDistill框架：1）采用渐进式、深度优先的学生协调机制，缓解浅层和深层退出之间的干扰；2）实现跨客户端的有效个性化知识转移；3）通过客户端解耦公式减少通信开销。

Result: CAFEDistill在广泛评估中优于现有方法，实现更高准确性，并将推理成本降低30.79%-46.86%。

Conclusion: CAFEDistill成功将早期退出网络扩展到个性化联邦学习，通过冲突感知的蒸馏框架同时解决客户端异构性和深度干扰问题，实现自适应推理和效率提升。

Abstract: Personalized Federated Learning (PFL) enables collaboratively model training on decentralized, heterogeneous data while tailoring them to each client's unique distribution. However, existing PFL methods produce static models with a fixed tradeoff between accuracy and efficiency, limiting their applicability in environments where inference requirements vary with contexts and resource availability. Early-exit networks (EENs) offer adaptive inference by attaching intermediate classifiers. Yet integrating them into PFL is challenging due to client-wise heterogeneity and depth-wise interference arising from conflicting exit objectives. Prior studies fail to resolve both conflicts simultaneously, leading to suboptimal performance. In this paper, we propose CAFEDistill, a Conflict-Aware Federated Exit Distillation framework that jointly addresses these conflicts and extends PFL to early-exit networks. Through a progressive, depth-prioritized student coordination mechanism, CAFEDistill mitigates interference among shallow and deep exits while allowing effective personalized knowledge transfer across clients. Furthermore, it reduces communication overhead via a client-decoupled formulation. Extensive evaluations show that CAFEDistill outperforms the state-of-the-arts, achieving higher accuracy and reducing inference costs by 30.79%-46.86%.

</details>


### [106] [Time Aggregation Features for XGBoost Models](https://arxiv.org/abs/2601.10019)
*Mykola Pinchuk*

Main category: cs.LG

TL;DR: 本文研究了点击率预测中XGBoost模型的时间聚合特征，发现简单的滑动窗口设计在Avazu数据集上表现最佳，相比目标编码基线能提升ROC AUC约0.0066-0.0082。


<details>
  <summary>Details</summary>
Motivation: 研究在点击率预测中，如何有效利用时间聚合特征来提升模型性能，特别是在严格的时间外样本划分和无前瞻性特征约束的设置下。

Method: 使用Avazu点击率预测数据集，采用严格的时间外样本划分和无前瞻性特征约束。比较了基于时间的目标编码基线与多种时间聚合窗口设计（滑动窗口、事件计数窗口、间隔窗口、分桶窗口）的实体历史时间聚合模型。

Result: 滑动窗口设计相比目标编码基线，在ROC AUC上提升约0.0066-0.0082，在PR AUC上提升约0.0084-0.0094。事件计数窗口提供了一致的微小改进，而间隔窗口和分桶窗口表现不如简单滑动窗口。

Conclusion: 推荐使用滑动窗口作为实践中的默认选择，当边际ROC AUC增益重要时，可考虑使用事件计数窗口。更复杂的时间聚合窗口设计在该数据集和协议下并未带来显著优势。

Abstract: This paper studies time aggregation features for XGBoost models in click-through rate prediction. The setting is the Avazu click-through rate prediction dataset with strict out-of-time splits and a no-lookahead feature constraint. Features for hour H use only impressions from hours strictly before H. This paper compares a strong time-aware target encoding baseline to models augmented with entity history time aggregation under several window designs. Across two rolling-tail folds on a deterministic ten percent sample, a trailing window specification improves ROC AUC by about 0.0066 to 0.0082 and PR AUC by about 0.0084 to 0.0094 relative to target encoding alone. Within the time aggregation design grid, event count windows provide the only consistent improvement over trailing windows, and the gain is small. Gap windows and bucketized windows underperform simple trailing windows in this dataset and protocol. These results support a practical default of trailing windows, with an optional event count window when marginal ROC AUC gains matter.

</details>


### [107] [BPE: Behavioral Profiling Ensemble](https://arxiv.org/abs/2601.10024)
*Yanxin Liu,Yunqi Zhang*

Main category: cs.LG

TL;DR: BPE框架通过构建模型内在的"行为画像"，根据模型对特定测试实例的响应与其行为画像的偏差来确定集成权重，相比传统集成方法在预测精度、计算效率和存储资源方面都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统静态集成方法（如Stacking）将每个基学习器视为整体分配权重，忽略了模型在不同实例空间区域的差异性能力。动态集成选择（DES）虽然考虑了这种差异性，但传统方法主要依赖模型间的差异性作为集成基础，这种模型间视角忽略了模型自身的内在特性，并且严重依赖验证集进行能力估计。

Method: 提出行为画像集成（BPE）框架，为每个模型构建内在的"行为画像"，根据模型对特定测试实例的响应与其已建立的行为画像之间的偏差来推导集成权重，实现了集成范式的根本转变。

Result: 在合成和真实世界数据集上的大量实验表明，基于BPE框架的算法相比最先进的集成基线方法取得了显著改进，不仅在预测准确性方面，而且在计算效率和存储资源利用方面，在各种场景下都表现出优势。

Conclusion: BPE框架通过关注模型内在行为特征而非模型间差异，提供了一种更有效、更高效的集成学习方法，减少了对验证集的依赖，并在多个维度上超越了现有集成方法。

Abstract: Ensemble learning is widely recognized as a pivotal strategy for pushing the boundaries of predictive performance. Traditional static ensemble methods, such as Stacking, typically assign weights by treating each base learner as a holistic entity, thereby overlooking the fact that individual models exhibit varying degrees of competence across different regions of the instance space. To address this limitation, Dynamic Ensemble Selection (DES) was introduced. However, both static and dynamic approaches predominantly rely on the divergence among different models as the basis for integration. This inter-model perspective neglects the intrinsic characteristics of the models themselves and necessitates a heavy reliance on validation sets for competence estimation. In this paper, we propose the Behavioral Profiling Ensemble (BPE) framework, which introduces a novel paradigm shift. Unlike traditional methods, BPE constructs a ``behavioral profile'' intrinsic to each model and derives integration weights based on the deviation between the model's response to a specific test instance and its established behavioral profile. Extensive experiments on both synthetic and real-world datasets demonstrate that the algorithm derived from the BPE framework achieves significant improvements over state-of-the-art ensemble baselines. These gains are evident not only in predictive accuracy but also in computational efficiency and storage resource utilization across various scenarios.

</details>


### [108] [Unlabeled Data Can Provably Enhance In-Context Learning of Transformers](https://arxiv.org/abs/2601.10058)
*Renpu Liu,Jing Yang*

Main category: cs.LG

TL;DR: 提出增强上下文学习框架，在提示中加入未标记数据块，通过思维链提示让Transformer隐式执行EM算法，提升多类线性分类任务的ICL性能。


<details>
  <summary>Details</summary>
Motivation: 传统上下文学习受限于提示中能容纳的少量标记示例，而现实中存在大量未标记数据。如何利用这些未标记数据来提升ICL性能成为一个重要问题。

Method: 提出增强ICL框架，提示包含少量标记示例和未标记输入块。在多类线性分类设置下，通过思维链提示让多层Transformer隐式执行期望最大化算法，从标记和未标记数据中提取有用信息。

Result: 增强ICL框架在实验中始终优于传统少样本ICL，为理论发现提供了实证支持。Transformer可以通过教师强制训练，参数以线性速率收敛到期望解。

Conclusion: 这是首个关于未标记数据对Transformer ICL性能影响的理论研究，证明了增强ICL框架能通过隐式EM算法利用未标记数据提升性能，为ICL的理论理解提供了新视角。

Abstract: Large language models (LLMs) exhibit impressive in-context learning (ICL) capabilities, yet the quality of their predictions is fundamentally limited by the few costly labeled demonstrations that can fit into a prompt. Meanwhile, there exist vast and continuously growing amounts of unlabeled data that may be closely related to the ICL task. How to utilize such unlabeled data to provably enhance the performance of ICL thus becomes an emerging fundamental question. In this work, we propose a novel augmented ICL framework, in which the prompt includes a small set of labeled examples alongside a block of unlabeled inputs. We focus on the multi-class linear classification setting and demonstrate that, with chain-of-thought (CoT) prompting, a multi-layer transformer can effectively emulate an expectation-maximization (EM) algorithm. This enables the transformer to implicitly extract useful information from both labeled and unlabeled data, leading to provable improvements in ICL accuracy. Moreover, we show that such a transformer can be trained via teacher forcing, with its parameters converging to the desired solution at a linear rate. Experiments demonstrate that the augmented ICL framework consistently outperforms conventional few-shot ICL, providing empirical support for our theoretical findings. To the best of our knowledge, this is the first theoretical study on the impact of unlabeled data on the ICL performance of transformers.

</details>


### [109] [Efficient Content-based Recommendation Model Training via Noise-aware Coreset Selection](https://arxiv.org/abs/2601.10067)
*Hung Vinh Tran,Tong Chen,Hechuan Wen,Quoc Viet Hung Nguyen,Bin Cui,Hongzhi Yin*

Main category: cs.LG

TL;DR: 提出NaCS框架，通过子模优化构建核心集，同时纠正噪声标签，利用不确定性量化过滤低置信度样本，显著提升基于内容的推荐系统训练效率。


<details>
  <summary>Details</summary>
Motivation: 基于内容的推荐系统需要大规模甚至持续训练以适应多样用户偏好，导致高昂计算成本。核心集选择虽能减少训练开销，但所选小规模核心集易受用户-物品交互噪声影响。

Method: 提出噪声感知核心集选择框架：1) 基于训练梯度进行子模优化构建核心集；2) 使用渐进训练模型纠正噪声标签；3) 通过不确定性量化过滤低置信度样本，避免不可靠交互训练。

Result: 实验表明NaCS为基于内容的推荐系统生成更高质量核心集，效率优于现有技术。仅使用1%训练数据即可恢复93-95%的全数据集训练性能。

Conclusion: NaCS有效解决了基于内容的推荐系统中核心集选择对噪声敏感的问题，在显著降低训练成本的同时保持了模型性能，为高效推荐系统训练提供了实用解决方案。

Abstract: Content-based recommendation systems (CRSs) utilize content features to predict user-item interactions, serving as essential tools for helping users navigate information-rich web services. However, ensuring the effectiveness of CRSs requires large-scale and even continuous model training to accommodate diverse user preferences, resulting in significant computational costs and resource demands. A promising approach to this challenge is coreset selection, which identifies a small but representative subset of data samples that preserves model quality while reducing training overhead. Yet, the selected coreset is vulnerable to the pervasive noise in user-item interactions, particularly when it is minimally sized. To this end, we propose Noise-aware Coreset Selection (NaCS), a specialized framework for CRSs. NaCS constructs coresets through submodular optimization based on training gradients, while simultaneously correcting noisy labels using a progressively trained model. Meanwhile, we refine the selected coreset by filtering out low-confidence samples through uncertainty quantification, thereby avoid training with unreliable interactions. Through extensive experiments, we show that NaCS produces higher-quality coresets for CRSs while achieving better efficiency than existing coreset selection techniques. Notably, NaCS recovers 93-95\% of full-dataset training performance using merely 1\% of the training data. The source code is available at \href{https://github.com/chenxing1999/nacs}{https://github.com/chenxing1999/nacs}.

</details>


### [110] [Comparative Evaluation of Deep Learning-Based and WHO-Informed Approaches for Sperm Morphology Assessment](https://arxiv.org/abs/2601.10070)
*Mohammad Abbadi*

Main category: cs.LG

TL;DR: 本研究比较了基于图像的深度学习模型HuSHeM与增强版WHO标准(WHO(+SIRI))在精子形态评估中的表现，发现深度学习模型在判别性能、校准和临床效用方面均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 精子形态评估是男性生育力评估的关键但主观的组成部分，受到观察者间变异性和资源限制的影响。传统方法存在主观性和不一致性问题，需要更客观、可重复的评估工具。

Method: 提出了一个比较性生物医学人工智能框架，评估基于图像的深度学习模型HuSHeM与临床基线方法WHO(+SIRI)。HuSHeM在高分辨率精子形态图像上训练，使用独立临床队列进行评估。采用判别性能、校准分析和临床效用分析来评估模型表现。

Result: HuSHeM模型显示出更高的判别性能（更高的AUC值，置信区间更窄），在类别不平衡情况下表现更好（更高的PR-AUC值），校准分析显示预测概率与观察结果更一致，决策曲线分析表明在临床相关阈值概率范围内具有更大的净临床效益。

Conclusion: 基于图像的深度学习相比传统规则基础和炎症增强标准可能提供更好的预测可靠性和临床效用。该框架支持精子形态的客观、可重复评估，可作为生育筛查和转诊工作流程中的决策支持工具，但不替代临床判断或实验室评估。

Abstract: Assessment of sperm morphological quality remains a critical yet subjective component of male fertility evaluation, often limited by inter-observer variability and resource constraints. This study presents a comparative biomedical artificial intelligence framework evaluating an image-based deep learning model (HuSHeM) alongside a clinically grounded baseline derived from World Health Organization criteria augmented with the Systemic Inflammation Response Index (WHO(+SIRI)).
  The HuSHeM model was trained on high-resolution sperm morphology images and evaluated using an independent clinical cohort. Model performance was assessed using discrimination, calibration, and clinical utility analyses. The HuSHeM model demonstrated higher discriminative performance, as reflected by an increased area under the receiver operating characteristic curve with relatively narrow confidence intervals compared to WHO(+SIRI). Precision-recall analysis further indicated improved performance under class imbalance, with higher precision-recall area values across evaluated thresholds. Calibration analysis indicated closer agreement between predicted probabilities and observed outcomes for HuSHeM, while decision curve analysis suggested greater net clinical benefit across clinically relevant threshold probabilities.
  These findings suggest that image-based deep learning may offer improved predictive reliability and clinical utility compared with traditional rule-based and inflammation-augmented criteria. The proposed framework supports objective and reproducible assessment of sperm morphology and may serve as a decision-support tool within fertility screening and referral workflows. The proposed models are intended as decision-support or referral tools and are not designed to replace clinical judgment or laboratory assessment.

</details>


### [111] [Sparse-RL: Breaking the Memory Wall in LLM Reinforcement Learning via Stable Sparse Rollouts](https://arxiv.org/abs/2601.10079)
*Sijia Luo,Xiaokang Zhang,Yuxuan Hu,Bohan Zhang,Ke Wang,Jinbo Su,Mengshu Sun,Lei Liang,Jing Zhang*

Main category: cs.LG

TL;DR: Sparse-RL：一种在稀疏rollouts下实现稳定RL训练的方法，通过稀疏感知拒绝采样和重要性重加权来纠正压缩引起的策略不匹配问题，显著降低KV缓存开销同时保持性能。


<details>
  <summary>Details</summary>
Motivation: RL在激发LLMs复杂推理能力方面至关重要，但长时程rollouts中存储KV缓存的巨大内存开销成为关键瓶颈，限制了在有限硬件上的高效训练。现有KV压缩技术虽能缓解推理问题，但直接应用于RL训练会导致严重的策略不匹配和性能崩溃。

Method: 提出Sparse-RL框架，识别不稳定源于密集旧策略、稀疏采样策略和学习者策略之间的根本性策略不匹配。引入稀疏感知拒绝采样和基于重要性的重加权，纠正压缩引起的信息损失带来的离策略偏差。

Result: 实验表明Sparse-RL相比密集基线显著降低rollouts开销，同时保持性能不变。此外，Sparse-RL实现了稀疏感知训练，显著增强了模型在稀疏推理部署时的鲁棒性。

Conclusion: Sparse-RL成功解决了RL训练中KV压缩导致的策略不匹配问题，实现了高效内存使用和稳定训练，为在有限硬件上进行长时程RL训练提供了可行方案，同时增强了模型在稀疏推理场景下的鲁棒性。

Abstract: Reinforcement Learning (RL) has become essential for eliciting complex reasoning capabilities in Large Language Models (LLMs). However, the substantial memory overhead of storing Key-Value (KV) caches during long-horizon rollouts acts as a critical bottleneck, often prohibiting efficient training on limited hardware. While existing KV compression techniques offer a remedy for inference, directly applying them to RL training induces a severe policy mismatch, leading to catastrophic performance collapse. To address this, we introduce Sparse-RL empowers stable RL training under sparse rollouts. We show that instability arises from a fundamental policy mismatch among the dense old policy, the sparse sampler policy, and the learner policy. To mitigate this issue, Sparse-RL incorporates Sparsity-Aware Rejection Sampling and Importance-based Reweighting to correct the off-policy bias introduced by compression-induced information loss. Experimental results show that Sparse-RL reduces rollout overhead compared to dense baselines while preserving the performance. Furthermore, Sparse-RL inherently implements sparsity-aware training, significantly enhancing model robustness during sparse inference deployment.

</details>


### [112] [Adaptive Label Error Detection: A Bayesian Approach to Mislabeled Data Detection](https://arxiv.org/abs/2601.10084)
*Zan Chaudhry,Noam H. Rotenberg,Brian Caffo,Craig K. Jones,Haris I. Sair*

Main category: cs.LG

TL;DR: ALED是一种新颖的标签错误检测方法，通过提取CNN特征、降噪、建模类别分布并进行似然比检验来识别错误标注样本，在医学影像数据上表现出比现有方法更高的敏感性和精度。


<details>
  <summary>Details</summary>
Motivation: 机器学习分类系统容易受到错误标注标签的影响，即使数据由专家精心标注。随着机器学习应用越来越广泛，识别和纠正错误标注对于开发更强大的模型变得至关重要。

Method: ALED方法包括：1）从深度卷积神经网络提取中间特征空间；2）对特征进行降噪；3）用多维高斯分布建模每个类别的降维流形；4）执行简单的似然比检验来识别错误标注样本。

Result: ALED在多个医学影像数据集上相比现有标签错误检测方法显著提高了敏感性，同时不损害精度。在修正数据上微调神经网络可使测试集错误减少33.8%。

Conclusion: ALED是一种有效的标签错误检测方法，能够显著提高模型性能，已部署在Python包statlab中，为终端用户提供强大益处。

Abstract: Machine learning classification systems are susceptible to poor performance when trained with incorrect ground truth labels, even when data is well-curated by expert annotators. As machine learning becomes more widespread, it is increasingly imperative to identify and correct mislabeling to develop more powerful models. In this work, we motivate and describe Adaptive Label Error Detection (ALED), a novel method of detecting mislabeling. ALED extracts an intermediate feature space from a deep convolutional neural network, denoises the features, models the reduced manifold of each class with a multidimensional Gaussian distribution, and performs a simple likelihood ratio test to identify mislabeled samples. We show that ALED has markedly increased sensitivity, without compromising precision, compared to established label error detection methods, on multiple medical imaging datasets. We demonstrate an example where fine-tuning a neural network on corrected data results in a 33.8% decrease in test set errors, providing strong benefits to end users. The ALED detector is deployed in the Python package statlab.

</details>


### [113] [Bayesian Meta-Analyses Could Be More: A Case Study in Trial of Labor After a Cesarean-section Outcomes and Complications](https://arxiv.org/abs/2601.10089)
*Ashley Klein,Edward Raff,Marcia DesJardin*

Main category: cs.LG

TL;DR: 本文提出一种贝叶斯方法，用于解决医学研究中关键决策变量未被记录时元分析的局限性，并以剖宫产后试产评估为例验证其效用。


<details>
  <summary>Details</summary>
Motivation: 医学研究中，元分析的可靠性依赖于先前研究是否准确记录了感兴趣的变量。然而，在医疗决策中，影响医生决策的关键变量常常未被记录，导致效应大小未知和结论不可靠。

Method: 开发了一种贝叶斯分析方法来处理这种常见的医学场景。该方法允许在关键决策变量缺失的情况下，分析是否仍能支持阳性效应的主张。

Result: 通过协助专业妇产科医生评估剖宫产后试产情况，证明了该方法的实用性。在患者干预选择有限的情况下，该方法为医生推进患者护理提供了必要的支持。

Conclusion: 贝叶斯方法能够解决医学元分析中关键变量缺失的问题，为临床决策提供更可靠的依据，特别是在剖宫产后试产等复杂医疗场景中。

Abstract: The meta-analysis's utility is dependent on previous studies having accurately captured the variables of interest, but in medical studies, a key decision variable that impacts a physician's decisions was not captured. This results in an unknown effect size and unreliable conclusions. A Bayesian approach may allow analysis to determine if the claim of a positive effect is still warranted, and we build a Bayesian approach to this common medical scenario. To demonstrate its utility, we assist professional OBGYNs in evaluating Trial of Labor After a Cesarean-section (TOLAC) situations where few interventions are available for patients and find the support needed for physicians to advance patient care.

</details>


### [114] [LeMoF: Level-guided Multimodal Fusion for Heterogeneous Clinical Data](https://arxiv.org/abs/2601.10092)
*Jongseok Kim,Seongae Kang,Jonghwan Shin,Yuhan Lee,Ohyun Jo*

Main category: cs.LG

TL;DR: LeMoF是一种新颖的多模态临床预测框架，通过层级引导的模态融合策略，选择性地整合每个模态内不同编码器层级的表示，实现预测稳定性与判别能力的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有多模态临床预测方法依赖静态模态整合方案和简单融合策略，未能充分利用模态特定的表示，在异构临床环境中表现有限。

Method: 提出LeMoF框架，明确分离并学习全局模态级预测和层级特定的判别表示，通过层级引导的表示选择性地整合每个模态内的不同编码器层级信息。

Result: 在ICU数据上的住院时间预测实验中，LeMoF在各种编码器配置下均优于现有最先进的多模态融合技术，证实层级整合是实现稳健预测性能的关键因素。

Conclusion: LeMoF通过层级引导的模态融合，在异构临床环境中实现了预测稳定性与判别能力的平衡，为多模态临床预测提供了更有效的解决方案。

Abstract: Multimodal clinical prediction is widely used to integrate heterogeneous data such as Electronic Health Records (EHR) and biosignals. However, existing methods tend to rely on static modality integration schemes and simple fusion strategies. As a result, they fail to fully exploit modality-specific representations. In this paper, we propose Level-guided Modal Fusion (LeMoF), a novel framework that selectively integrates level-guided representations within each modality. Each level refers to a representation extracted from a different layer of the encoder. LeMoF explicitly separates and learns global modality-level predictions from level-specific discriminative representations. This design enables LeMoF to achieve a balanced performance between prediction stability and discriminative capability even in heterogeneous clinical environments. Experiments on length of stay prediction using Intensive Care Unit (ICU) data demonstrate that LeMoF consistently outperforms existing state-of-the-art multimodal fusion techniques across various encoder configurations. We also confirmed that level-wise integration is a key factor in achieving robust predictive performance across various clinical conditions.

</details>


### [115] [Multilingual-To-Multimodal (M2M): Unlocking New Languages with Monolingual Text](https://arxiv.org/abs/2601.10096)
*Piyush Singh Pasi*

Main category: cs.LG

TL;DR: METAL：一种轻量级对齐方法，仅使用英语文本学习少量线性层，将多语言文本嵌入映射到多模态空间，实现强大的零样本跨语言迁移。


<details>
  <summary>Details</summary>
Motivation: 多模态模型在英语上表现优异，但在其他语言上性能显著下降，主要因为缺乏多语言多模态数据。现有解决方案过度依赖机器翻译，而多语言文本建模的进展未得到充分利用。

Method: METAL方法仅学习少量线性层，使用英语文本将多语言文本嵌入映射到多模态空间。该方法简单但有效，通过几何变换而非简单旋转来重塑嵌入空间。

Result: 在英语上达到94.9% Recall@10，在11种语言（10种未见语言）上平均达到89.5% Recall@10的零样本迁移性能。方法可泛化到音频-文本检索和跨语言文本到图像生成。

Conclusion: METAL提供了一种高效的多语言多模态对齐方法，仅需英语数据即可实现强大的跨语言迁移。作者发布了代码、检查点和多语言评估数据集以促进进一步研究。

Abstract: Multimodal models excel in English, supported by abundant image-text and audio-text data, but performance drops sharply for other languages due to limited multilingual multimodal resources. Existing solutions rely heavily on machine translation, while advances in multilingual text modeling remain underutilized. We introduce METAL, a lightweight alignment method that learns only a few linear layers using English text alone to map multilingual text embeddings into a multimodal space. Despite its simplicity, METAL matches baseline performance in English (94.9 percent Recall at 10) and achieves strong zero-shot transfer (89.5 percent Recall at 10 averaged across 11 languages, 10 unseen) on XTD text-to-image retrieval. Qualitative t-SNE visualizations show that multilingual embeddings align tightly with multimodal representations, while weight analysis reveals that the transformation reshapes embedding geometry rather than performing trivial rotations. Beyond image-text retrieval, METAL generalizes to audio-text retrieval and cross-lingual text-to-image generation. We release code and checkpoints at https://github.com/m2m-codebase/M2M , as well as multilingual evaluation datasets including MSCOCO Multilingual 30K (https://huggingface.co/datasets/piyushsinghpasi/mscoco-multilingual-30k ), AudioCaps Multilingual (https://huggingface.co/datasets/piyushsinghpasi/audiocaps-multilingual ), and Clotho Multilingual (https://huggingface.co/datasets/piyushsinghpasi/clotho-multilingual ), to facilitate further research.

</details>


### [116] [Step-by-Step Causality: Transparent Causal Discovery with Multi-Agent Tree-Query and Adversarial Confidence Estimation](https://arxiv.org/abs/2601.10137)
*Ziyi Ding,Chenfei Ye-Hao,Zheyuan Wang,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: Tree-Query：基于树状多专家LLM框架的因果发现方法，通过结构化查询减少误差传播，提供可解释的因果判断和置信度评分


<details>
  <summary>Details</summary>
Motivation: 传统约束性因果发现方法（如PC、FCI）存在误差传播问题，而近期基于LLM的因果预测器往往是不透明、无置信度的黑盒模型。需要一种既能减少误差传播又能提供可解释性判断的因果发现方法。

Method: 提出Tree-Query框架，将成对因果发现转化为一系列关于后门路径、(不)独立性、潜在混杂因素和因果方向的查询，采用树状多专家LLM结构，生成可解释的判断并附带鲁棒性感知的置信度评分。

Result: 在基于Mooij等人和UCI因果图的数据无关基准测试中，Tree-Query在结构指标上优于直接LLM基线；饮食-体重案例研究展示了混杂因素筛选和稳定、高置信度的因果结论。理论保证提供了四种成对关系的渐近可识别性。

Conclusion: Tree-Query提供了一种从LLM获取数据无关因果先验的原则性方法，可以补充下游数据驱动的因果发现，为因果发现提供了可解释、有置信度评估的解决方案。

Abstract: Causal discovery aims to recover ``what causes what'', but classical constraint-based methods (e.g., PC, FCI) suffer from error propagation, and recent LLM-based causal oracles often behave as opaque, confidence-free black boxes. This paper introduces Tree-Query, a tree-structured, multi-expert LLM framework that reduces pairwise causal discovery to a short sequence of queries about backdoor paths, (in)dependence, latent confounding, and causal direction, yielding interpretable judgments with robustness-aware confidence scores. Theoretical guarantees are provided for asymptotic identifiability of four pairwise relations. On data-free benchmarks derived from Mooij et al. and UCI causal graphs, Tree-Query improves structural metrics over direct LLM baselines, and a diet--weight case study illustrates confounder screening and stable, high-confidence causal conclusions. Tree-Query thus offers a principled way to obtain data-free causal priors from LLMs that can complement downstream data-driven causal discovery. Code is available at https://anonymous.4open.science/r/Repo-9B3E-4F96.

</details>


### [117] [Understanding and Preserving Safety in Fine-Tuned LLMs](https://arxiv.org/abs/2601.10141)
*Jiawen Zhang,Yangfan Hu,Kejia Chen,Lipeng He,Jiachen Ma,Jian Lou,Dan Li,Jian Liu,Xiaohu Yang,Ruoxi Jia*

Main category: cs.LG

TL;DR: 本文提出SPF方法，通过分析安全与效用梯度的几何关系，在微调时移除与安全子空间冲突的梯度分量，解决LLM微调中的安全-效用困境。


<details>
  <summary>Details</summary>
Motivation: 大语言模型微调会严重破坏安全对齐，即使使用无害数据也会增加越狱攻击风险。现有方法面临安全-效用困境：强调安全会损害任务性能，而优先效用需要深度微调导致安全大幅下降。

Method: 提出安全保护微调（SPF）：1）发现安全梯度位于低秩子空间，效用梯度跨越更广的高维空间；2）这些子空间负相关导致微调方向冲突；3）从单个样本可高效估计主导安全方向。SPF显式移除与低秩安全子空间冲突的梯度分量。

Result: 理论上证明SPF保证效用收敛同时限制安全漂移。实证显示SPF保持下游任务性能，恢复几乎所有预训练安全对齐，即使在对抗性微调场景下。SPF对深度微调和动态越狱攻击表现出强鲁棒性。

Conclusion: 研究提供了对LLM微调中安全-效用困境的新机制理解，为始终对齐的LLM微调提供了实用指导。SPF方法轻量且有效，解决了微调中的关键安全问题。

Abstract: Fine-tuning is an essential and pervasive functionality for applying large language models (LLMs) to downstream tasks. However, it has the potential to substantially degrade safety alignment, e.g., by greatly increasing susceptibility to jailbreak attacks, even when the fine-tuning data is entirely harmless. Despite garnering growing attention in defense efforts during the fine-tuning stage, existing methods struggle with a persistent safety-utility dilemma: emphasizing safety compromises task performance, whereas prioritizing utility typically requires deep fine-tuning that inevitably leads to steep safety declination.
  In this work, we address this dilemma by shedding new light on the geometric interaction between safety- and utility-oriented gradients in safety-aligned LLMs. Through systematic empirical analysis, we uncover three key insights: (I) safety gradients lie in a low-rank subspace, while utility gradients span a broader high-dimensional space; (II) these subspaces are often negatively correlated, causing directional conflicts during fine-tuning; and (III) the dominant safety direction can be efficiently estimated from a single sample. Building upon these novel insights, we propose safety-preserving fine-tuning (SPF), a lightweight approach that explicitly removes gradient components conflicting with the low-rank safety subspace. Theoretically, we show that SPF guarantees utility convergence while bounding safety drift. Empirically, SPF consistently maintains downstream task performance and recovers nearly all pre-trained safety alignment, even under adversarial fine-tuning scenarios. Furthermore, SPF exhibits robust resistance to both deep fine-tuning and dynamic jailbreak attacks. Together, our findings provide new mechanistic understanding and practical guidance toward always-aligned LLM fine-tuning.

</details>


### [118] [Simple Network Graph Comparative Learning](https://arxiv.org/abs/2601.10150)
*Qiang Yu,Xinran Cheng,Shiqiang Xu,Chuanyi Liu*

Main category: cs.LG

TL;DR: 提出SNGCL方法，通过叠加多层拉普拉斯平滑滤波器处理数据，使用改进的三元重组损失函数，在节点分类任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 对比学习在图学习中效果显著，但在节点分类任务中面临挑战：现有数据增强技术可能导致视图差异过大，以及大多数方法依赖大量负样本

Method: 提出SNGCL方法，使用叠加多层拉普拉斯平滑滤波器获取全局和局部特征平滑矩阵，传入孪生网络的目标和在线网络，采用改进的三元重组损失函数

Result: 与最先进模型相比，SNGCL在大多数节点分类任务中表现出强大的竞争力

Conclusion: SNGCL方法有效解决了现有对比学习在节点分类中的挑战，通过改进的数据处理和损失函数设计取得了优异性能

Abstract: The effectiveness of contrastive learning methods has been widely recognized in the field of graph learning, especially in contexts where graph data often lack labels or are difficult to label. However, the application of these methods to node classification tasks still faces a number of challenges. First, existing data enhancement techniques may lead to significant differences from the original view when generating new views, which may weaken the relevance of the view and affect the efficiency of model training. Second, the vast majority of existing graph comparison learning algorithms rely on the use of a large number of negative samples. To address the above challenges, this study proposes a novel node classification contrast learning method called Simple Network Graph Comparative Learning (SNGCL). Specifically, SNGCL employs a superimposed multilayer Laplace smoothing filter as a step in processing the data to obtain global and local feature smoothing matrices, respectively, which are thus passed into the target and online networks of the siamese network, and finally employs an improved triple recombination loss function to bring the intra-class distance closer and the inter-class distance farther. We have compared SNGCL with state-of-the-art models in node classification tasks, and the experimental results show that SNGCL is strongly competitive in most tasks.

</details>


### [119] [LOOKAT: Lookup-Optimized Key-Attention for Memory-Efficient Transformers](https://arxiv.org/abs/2601.10155)
*Aryan Karmore*

Main category: cs.LG

TL;DR: LOOKAT：一种基于乘积量化的KV缓存压缩方法，将注意力计算转化为查表操作，实现64倍压缩且保持95.7%输出保真度


<details>
  <summary>Details</summary>
Motivation: 现有量化方法虽然压缩了KV缓存存储，但无法减少带宽，因为注意力计算需要将INT4/INT8键向量反量化为FP16。需要一种既能压缩存储又能减少带宽的方法来在边缘设备上部署大语言模型。

Method: 将注意力评分视为内积相似度搜索，应用向量数据库的压缩技术。具体采用乘积量化和非对称距离计算，将键向量分解为子空间，学习码本，通过查表计算注意力表，将注意力从内存受限转为计算受限。

Result: 在GPT-2上测试，实现64倍压缩时输出保真度95.7%，32倍压缩时95.0%保真度。无需架构修改或训练，保持秩相关ρ>0.95。理论分析显示秩相关退化与dk/mK成正比，在1024个token序列长度内得到验证。

Conclusion: LOOKAT通过乘积量化和查表机制有效压缩KV缓存，显著减少带宽需求，为在边缘设备部署大语言模型提供了实用解决方案。

Abstract: Compressing the KV cache is a required step to deploy large language models on edge devices. Current quantization methods compress storage but fail to reduce bandwidth as attention calculation requires dequantizing keys from INT4/INT8 to FP16 before use. We observe that attention scoring is mathematically equivalent to the inner product similarity search and we can apply some compression techniques from vector databases to compress KV-cache better. We propose LOOKAT, which applies product quantization and asymmetric distance computation, to transformer architecture by decomposing key vectors into subspaces, learning codebooks and computing attention tables via lookup tables. This transforms attention from memory-bound to compute-bound. LOOKAT achieves 64 $\times$ compression at 95.7\% output fidelity and 32 $\times$ compression at 95.0\% fidelity when tested on GPT-2. LOOKAT requires no architecture changes or training while maintaining rank correlation $ρ> 0.95$. Theoretical analysis confirms that rank correlation degrades as $O(d_k/mK)$, with guarantees validated across sequence lengths up to 1024 tokens.

</details>


### [120] [CC-OR-Net: A Unified Framework for LTV Prediction through Structural Decoupling](https://arxiv.org/abs/2601.10176)
*Mingyu Zhao,Haoran Bai,Yu Tian,Bing Zhu,Hengliang Luo*

Main category: cs.LG

TL;DR: 提出CC-OR-Net框架，通过结构分解解决LTV预测中的零膨胀长尾分布问题，在保持全局准确性的同时提升高价值用户预测精度。


<details>
  <summary>Details</summary>
Motivation: 客户终身价值预测面临零膨胀和长尾分布的两大挑战：1）低中价值用户数量上压倒高价值"鲸鱼"用户；2）低中价值用户内部存在显著异质性。现有方法要么依赖刚性统计假设，要么通过损失约束而非结构设计来解耦排序和回归，难以平衡全局准确性和高价值精度。

Method: 提出条件级联序数残差网络（CC-OR-Net），通过结构分解实现排序和回归的稳健解耦。包含三个组件：1）结构序数分解模块确保稳健排序；2）桶内残差模块实现细粒度回归；3）定向高价值增强模块提升顶级用户预测精度。

Result: 在超过3亿用户的真实数据集上评估，CC-OR-Net在所有关键业务指标上实现了优越的权衡，超越了现有最先进方法，提供了全面且有商业价值的LTV预测解决方案。

Conclusion: CC-OR-Net通过结构分解方法有效解决了LTV预测中的长尾分布问题，在架构上保证了排序性能，同时实现了细粒度回归和高价值用户精度，为实际商业应用提供了更优的解决方案。

Abstract: Customer Lifetime Value (LTV) prediction, a central problem in modern marketing, is characterized by a unique zero-inflated and long-tail data distribution. This distribution presents two fundamental challenges: (1) the vast majority of low-to-medium value users numerically overwhelm the small but critically important segment of high-value "whale" users, and (2) significant value heterogeneity exists even within the low-to-medium value user base. Common approaches either rely on rigid statistical assumptions or attempt to decouple ranking and regression using ordered buckets; however, they often enforce ordinality through loss-based constraints rather than inherent architectural design, failing to balance global accuracy with high-value precision. To address this gap, we propose \textbf{C}onditional \textbf{C}ascaded \textbf{O}rdinal-\textbf{R}esidual Networks \textbf{(CC-OR-Net)}, a novel unified framework that achieves a more robust decoupling through \textbf{structural decomposition}, where ranking is architecturally guaranteed. CC-OR-Net integrates three specialized components: a \textit{structural ordinal decomposition module} for robust ranking, an \textit{intra-bucket residual module} for fine-grained regression, and a \textit{targeted high-value augmentation module} for precision on top-tier users. Evaluated on real-world datasets with over 300M users, CC-OR-Net achieves a superior trade-off across all key business metrics, outperforming state-of-the-art methods in creating a holistic and commercially valuable LTV prediction solution.

</details>


### [121] [Bias in the Shadows: Explore Shortcuts in Encrypted Network Traffic Classification](https://arxiv.org/abs/2601.10180)
*Chuyi Wang,Xiaohui Xie,Tongze Wang,Yong Cui*

Main category: cs.LG

TL;DR: BiasSeeker是一个模型无关、数据驱动的半自动化框架，用于检测加密网络流量分类中的数据集特定捷径特征，通过统计相关性分析识别虚假特征，提高模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 预训练模型在加密网络流量分类中表现良好但存在捷径学习问题，即依赖虚假相关性而无法泛化到真实世界数据。现有解决方案过度依赖模型特定的解释技术，缺乏跨模型架构和部署场景的适应性和通用性。

Method: 提出BiasSeeker框架：1）直接在原始二进制流量上进行统计相关性分析，识别可能损害泛化能力的虚假或环境纠缠特征；2）引入系统化的捷径特征分类，并应用类别特定的验证策略；3）强调上下文感知的特征选择和数据集特定诊断。

Result: 在三个网络流量分类任务的19个公共数据集上进行了评估，BiasSeeker能够有效识别捷径特征，减少偏差同时保留有意义的信息。

Conclusion: BiasSeeker为理解和解决加密网络流量分类中的捷径学习问题提供了新视角，强调特征选择应该是模型训练前有意且场景敏感的一步，提高了对特征选择重要性的认识。

Abstract: Pre-trained models operating directly on raw bytes have achieved promising performance in encrypted network traffic classification (NTC), but often suffer from shortcut learning-relying on spurious correlations that fail to generalize to real-world data. Existing solutions heavily rely on model-specific interpretation techniques, which lack adaptability and generality across different model architectures and deployment scenarios.
  In this paper, we propose BiasSeeker, the first semi-automated framework that is both model-agnostic and data-driven for detecting dataset-specific shortcut features in encrypted traffic. By performing statistical correlation analysis directly on raw binary traffic, BiasSeeker identifies spurious or environment-entangled features that may compromise generalization, independent of any classifier. To address the diverse nature of shortcut features, we introduce a systematic categorization and apply category-specific validation strategies that reduce bias while preserving meaningful information.
  We evaluate BiasSeeker on 19 public datasets across three NTC tasks. By emphasizing context-aware feature selection and dataset-specific diagnosis, BiasSeeker offers a novel perspective for understanding and addressing shortcut learning in encrypted network traffic classification, raising awareness that feature selection should be an intentional and scenario-sensitive step prior to model training.

</details>


### [122] [Reinforcement Learning to Discover a NorthEast Monsoon Index for Monthly Rainfall Prediction in Thailand](https://arxiv.org/abs/2601.10181)
*Kiattikun Chobtham*

Main category: cs.LG

TL;DR: 提出基于强化学习优化的东北季风气候指数，结合LSTM模型显著提升泰国地区长期月降雨预测精度


<details>
  <summary>Details</summary>
Motivation: 现有全球气候指数（如ENSO）在泰国特定区域长期降雨预测中存在局限性，缺乏能够提升区域预测精度的本地化气候指数

Method: 1. 基于海表温度计算新型东北季风气候指数；2. 使用深度Q网络强化学习智能体优化指数计算区域；3. 将降雨站点聚类为12个区域；4. 将优化指数输入LSTM模型进行长期降雨预测

Result: 优化后的指数显著提升了大多数聚类区域的长期月降雨预测能力，有效降低了12个月提前预测的均方根误差

Conclusion: 强化学习优化的本地化气候指数能够有效提升区域长期降雨预测精度，为气候预测提供了新的方法框架

Abstract: Climate prediction is a challenge due to the intricate spatiotemporal patterns within Earth systems. Global climate indices, such as the El Niño Southern Oscillation, are standard input features for long-term rainfall prediction. However, a significant gap persists regarding local-scale indices capable of improving predictive accuracy in specific regions of Thailand. This paper introduces a novel NorthEast monsoon climate index calculated from sea surface temperature to reflect the climatology of the boreal winter monsoon. To optimise the calculated areas used for this index, a Deep Q-Network reinforcement learning agent explores and selects the most effective rectangles based on their correlation with seasonal rainfall. Rainfall stations were classified into 12 distinct clusters to distinguish rainfall patterns between southern and upper Thailand. Experimental results show that incorporating the optimised index into Long Short-Term Memory models significantly improves long-term monthly rainfall prediction skill in most cluster areas. This approach effectively reduces the Root Mean Square Error for 12-month-ahead forecasts.

</details>


### [123] [Graph Regularized PCA](https://arxiv.org/abs/2601.10199)
*Antonio Briola,Marwin Schmidt,Fabio Caccioli,Carlos Ros Perez,James Singleton,Christian Michler,Tomaso Aste*

Main category: cs.LG

TL;DR: 提出Graph Regularized PCA (GR-PCA)，通过图正则化处理非独立同分布噪声的高维数据，利用图拉普拉斯偏置加载向量，抑制高频信号并保留图一致的低频信号。


<details>
  <summary>Details</summary>
Motivation: 高维数据中变量间的依赖关系常常违反PCA的等方差噪声假设，当噪声在特征间非独立同分布时，传统PCA不再最优，需要一种能结合数据依赖结构的降维方法。

Method: GR-PCA通过图正则化PCA，学习稀疏精度图，并将加载向量偏置到对应图拉普拉斯的低频傅里叶模式，抑制高频信号，保留图一致的低频信号。

Result: 在多种图拓扑、信噪比和稀疏度水平的合成数据上评估，相比主流方法，GR-PCA能将方差集中在目标支撑集上，产生更低图拉普拉斯能量的加载向量，并在样本外重建保持竞争力。

Conclusion: GR-PCA提供了一种实用、可扩展的结构感知降维方法，当高频信号与图相关时优势明显，能提高结构保真度而不牺牲预测性能，实现简单且模块化。

Abstract: High-dimensional data often exhibit dependencies among variables that violate the isotropic-noise assumption under which principal component analysis (PCA) is optimal. For cases where the noise is not independent and identically distributed across features (i.e., the covariance is not spherical) we introduce Graph Regularized PCA (GR-PCA). It is a graph-based regularization of PCA that incorporates the dependency structure of the data features by learning a sparse precision graph and biasing loadings toward the low-frequency Fourier modes of the corresponding graph Laplacian. Consequently, high-frequency signals are suppressed, while graph-coherent low-frequency ones are preserved, yielding interpretable principal components aligned with conditional relationships. We evaluate GR-PCA on synthetic data spanning diverse graph topologies, signal-to-noise ratios, and sparsity levels. Compared to mainstream alternatives, it concentrates variance on the intended support, produces loadings with lower graph-Laplacian energy, and remains competitive in out-of-sample reconstruction. When high-frequency signals are present, the graph Laplacian penalty prevents overfitting, reducing the reconstruction accuracy but improving structural fidelity. The advantage over PCA is most pronounced when high-frequency signals are graph-correlated, whereas PCA remains competitive when such signals are nearly rotationally invariant. The procedure is simple to implement, modular with respect to the precision estimator, and scalable, providing a practical route to structure-aware dimensionality reduction that improves structural fidelity without sacrificing predictive performance.

</details>


### [124] [PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary](https://arxiv.org/abs/2601.10201)
*Jiarui Yao,Ruida Wang,Tong Zhang*

Main category: cs.LG

TL;DR: 本文提出Process Reward Learning (PRL)，一种将结果奖励分解为过程监督信号的强化学习方法，用于提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多基于轨迹级的结果奖励，缺乏推理过程中的细粒度监督。其他结合过程信号的方法依赖MCTS、训练单独奖励模型等繁琐步骤，效率低下。过程信号设计缺乏严格理论支持，优化机制不透明。

Method: 提出Process Reward Learning (PRL)，将熵正则化强化学习目标分解为中间步骤，为模型分配严格的过程奖励。从理论动机出发，推导出PRL公式，本质上是奖励最大化加上策略模型与参考模型之间的KL散度惩罚项。

Result: 实验结果表明，PRL不仅提高了LLM推理能力的平均性能（average @ n），还通过改进pass @ n指标拓宽了推理边界。大量实验验证了PRL的有效性和泛化能力。

Conclusion: PRL能够将结果奖励转化为过程监督信号，更好地指导RL优化过程中的探索，为提升大语言模型推理能力提供了一种有效且理论支持的方法。

Abstract: Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show the effectiveness of PRL could be verified and generalized.

</details>


### [125] [Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD](https://arxiv.org/abs/2601.10237)
*Murat Bilgehan Ertan,Marten van Dijk*

Main category: cs.LG

TL;DR: DP-SGD在f-差分隐私框架下存在隐私-效用权衡的基本限制：在单轮混洗采样中，实现强隐私需要高噪声水平，导致模型精度显著下降。


<details>
  <summary>Details</summary>
Motivation: 研究DP-SGD在标准最坏情况对抗模型下的基本限制，理解其在f-差分隐私框架中的隐私-效用权衡关系，揭示实际应用中可能存在的瓶颈。

Method: 在f-差分隐私框架下分析单轮混洗采样的DP-SGD，推导隐私权衡曲线的显式次优上界，建立分离度κ与高斯噪声乘子σ之间的几何下界关系。

Result: 证明实现强隐私（小κ）需要高噪声水平（大σ），具体约束为σ≥1/√(2lnM)或κ≥1/√8(1-1/√(4πlnM))，且该限制也适用于泊松子采样。

Conclusion: DP-SGD在标准最坏情况对抗模型下无法同时实现强隐私和高效用，即使在大M渐近情况下收敛极慢，实际训练中噪声水平导致精度显著下降，揭示了DP-SGD的基本瓶颈。

Abstract: Differentially Private Stochastic Gradient Descent (DP-SGD) is the dominant paradigm for private training, but its fundamental limitations under worst-case adversarial privacy definitions remain poorly understood. We analyze DP-SGD in the $f$-differential privacy framework, which characterizes privacy via hypothesis-testing trade-off curves, and study shuffled sampling over a single epoch with $M$ gradient updates. We derive an explicit suboptimal upper bound on the achievable trade-off curve. This result induces a geometric lower bound on the separation $κ$ which is the maximum distance between the mechanism's trade-off curve and the ideal random-guessing line. Because a large separation implies significant adversarial advantage, meaningful privacy requires small $κ$. However, we prove that enforcing a small separation imposes a strict lower bound on the Gaussian noise multiplier $σ$, which directly limits the achievable utility. In particular, under the standard worst-case adversarial model, shuffled DP-SGD must satisfy
  $σ\ge \frac{1}{\sqrt{2\ln M}}$ $\quad\text{or}\quad$ $κ\ge\ \frac{1}{\sqrt{8}}\!\left(1-\frac{1}{\sqrt{4π\ln M}}\right)$,
  and thus cannot simultaneously achieve strong privacy and high utility. Although this bound vanishes asymptotically as $M \to \infty$, the convergence is extremely slow: even for practically relevant numbers of updates the required noise magnitude remains substantial. We further show that the same limitation extends to Poisson subsampling up to constant factors. Our experiments confirm that the noise levels implied by this bound leads to significant accuracy degradation at realistic training settings, thus showing a critical bottleneck in DP-SGD under standard worst-case adversarial assumptions.

</details>


### [126] [X-SAM: Boosting Sharpness-Aware Minimization with Dominant-Eigenvector Gradient Correction](https://arxiv.org/abs/2601.10251)
*Hongru Duan,Yongle Chen,Lei Guan*

Main category: cs.LG

TL;DR: 提出X-SAM方法，通过沿Hessian矩阵主特征向量正交分解修正梯度，更直接有效地正则化最大特征值，解决SAM可能仍指向尖锐区域的问题。


<details>
  <summary>Details</summary>
Motivation: SAM旨在通过最小化参数邻域内的最坏扰动损失来提升泛化能力，但训练中其优化行为并不总是符合理论预期，因为尖锐和平坦区域都可能产生小的扰动损失，导致梯度仍可能指向尖锐区域，未能实现SAM的预期效果。

Method: 从谱和几何角度分析SAM，利用梯度与Hessian矩阵主特征向量之间的夹角作为尖锐度度量；提出显式特征向量对齐的SAM（X-SAM），通过沿主特征向量进行正交分解来修正梯度，实现对Hessian最大特征值的更直接有效正则化。

Result: 证明了X-SAM的收敛性和更优的泛化性能，大量实验评估证实了理论和实践上的优势。

Conclusion: X-SAM通过修正梯度方向，更有效地正则化尖锐度，解决了传统SAM可能失效的问题，在理论和实验上都表现出优越性。

Abstract: Sharpness-Aware Minimization (SAM) aims to improve generalization by minimizing a worst-case perturbed loss over a small neighborhood of model parameters. However, during training, its optimization behavior does not always align with theoretical expectations, since both sharp and flat regions may yield a small perturbed loss. In such cases, the gradient may still point toward sharp regions, failing to achieve the intended effect of SAM. To address this issue, we investigate SAM from a spectral and geometric perspective: specifically, we utilize the angle between the gradient and the leading eigenvector of the Hessian as a measure of sharpness. Our analysis illustrates that when this angle is less than or equal to ninety degrees, the effect of SAM's sharpness regularization can be weakened. Furthermore, we propose an explicit eigenvector-aligned SAM (X-SAM), which corrects the gradient via orthogonal decomposition along the top eigenvector, enabling more direct and efficient regularization of the Hessian's maximum eigenvalue. We prove X-SAM's convergence and superior generalization, with extensive experimental evaluations confirming both theoretical and practical advantages.

</details>


### [127] [In-Context Source and Channel Coding](https://arxiv.org/abs/2601.10267)
*Ziqiong Wang,Tianqi Ren,Rongpeng Li,Zhifeng Zhao,Honggang Zhang*

Main category: cs.LG

TL;DR: 提出接收端上下文解码框架，通过错误纠正码Transformer获取比特可靠性，构建置信度排序候选池，结合LLM算术解码器提升SSCC在低信噪比下的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 传统分离信源信道编码在低信噪比下存在明显的"悬崖效应"，信道解码后的残留比特错误会灾难性地破坏无损信源解码，特别是基于大语言模型的算术编码

Method: 提出接收端上下文解码框架：1) 使用错误纠正码Transformer获取比特级可靠性；2) 基于上下文一致性比特流，通过可靠性引导的比特翻转构建置信度排序候选池；3) 采样紧凑多样的候选子集；4) 应用基于LLM的算术解码器获取重构和序列级对数似然；5) 通过可靠性-似然融合规则选择最终输出

Result: 在加性高斯白噪声和瑞利衰落信道上的大量实验表明，相比传统SSCC基线和代表性联合信源信道编码方案，该方法获得了持续的性能增益

Conclusion: 提出的接收端上下文解码框架在不修改发射端的情况下，有效增强了分离信源信道编码的鲁棒性，解决了低信噪比下的悬崖效应问题，并提供了采样过程的稳定性和收敛性理论保证

Abstract: Separate Source-Channel Coding (SSCC) remains attractive for text transmission due to its modularity and compatibility with mature entropy coders and powerful channel codes. However, SSCC often suffers from a pronounced cliff effect in low Signal-to-Noise Ratio (SNR) regimes, where residual bit errors after channel decoding can catastrophically break lossless source decoding, especially for Arithmetic Coding (AC) driven by Large Language Models (LLMs). This paper proposes a receiver-side In-Context Decoding (ICD) framework that enhances SSCC robustness without modifying the transmitter. ICD leverages an Error Correction Code Transformer (ECCT) to obtain bit-wise reliability for the decoded information bits. Based on the context-consistent bitstream, ICD constructs a confidence-ranked candidate pool via reliability-guided bit flipping, samples a compact yet diverse subset of candidates, and applies an LLM-based arithmetic decoder to obtain both reconstructions and sequence-level log-likelihoods. A reliability-likelihood fusion rule then selects the final output. We further provide theoretical guarantees on the stability and convergence of the proposed sampling procedure. Extensive experiments over Additive White Gaussian Noise (AWGN) and Rayleigh fading channels demonstrate consistent gains compared with conventional SSCC baselines and representative Joint Source-Channel Coding (JSCC) schemes.

</details>


### [128] [Early Fault Detection on CMAPSS with Unsupervised LSTM Autoencoders](https://arxiv.org/abs/2601.10269)
*P. Sánchez,K. Reyes,B. Radu,E. Fernández*

Main category: cs.LG

TL;DR: 提出无需故障标签的无监督涡扇发动机健康监测框架，通过回归归一化消除工况影响，使用LSTM自编码器在健康数据上训练，采用自适应阈值触发实时警报。


<details>
  <summary>Details</summary>
Motivation: 传统发动机健康监测需要运行至故障的标签数据，这在实际应用中难以获取且成本高昂。需要开发无需故障标签的无监督方法，能够快速部署并适应不同机队。

Method: 1. 使用基于回归的归一化方法消除NASA CMAPSS传感器数据中的工况影响；2. 仅使用每个轨迹的健康部分训练LSTM自编码器；3. 采用自适应数据驱动阈值估计持续重构误差，触发实时警报，无需手动调参规则。

Result: 基准测试显示该方法在多种运行工况下具有高召回率和低误报率，能够快速部署、适应不同机队规模，并可作为剩余使用寿命模型的补充预警层。

Conclusion: 该无监督框架成功实现了无需故障标签的涡扇发动机健康监测，通过自适应阈值触发警报，具有良好的实用性和可扩展性，可作为现有剩余使用寿命模型的补充预警系统。

Abstract: This paper introduces an unsupervised health-monitoring framework for turbofan engines that does not require run-to-failure labels. First, operating-condition effects in NASA CMAPSS sensor streams are removed via regression-based normalisation; then a Long Short-Term Memory (LSTM) autoencoder is trained only on the healthy portion of each trajectory. Persistent reconstruction error, estimated using an adaptive data-driven threshold, triggers real-time alerts without hand-tuned rules. Benchmark results show high recall and low false-alarm rates across multiple operating regimes, demonstrating that the method can be deployed quickly, scale to diverse fleets, and serve as a complementary early-warning layer to Remaining Useful Life models.

</details>


### [129] [Queueing-Aware Optimization of Reasoning Tokens for Accuracy-Latency Trade-offs in LLM Servers](https://arxiv.org/abs/2601.10274)
*Emre Ozbas,Melih Bastopcu*

Main category: cs.LG

TL;DR: 本文研究LLM服务器中不同任务类型的token分配优化问题，在准确率-延迟权衡下，通过队列模型和优化算法寻找最优token分配策略。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型服务器需要处理多种类型的查询任务，每个任务需要不同的计算资源（token分配）。如何在有限的token预算下，平衡准确率和延迟，同时保持系统稳定，是一个重要的优化问题。

Method: 1. 建立M/G/1队列模型，服务时间与分配的token数量呈近似仿射关系；2. 将问题形式化为带约束的优化问题，最大化加权平均准确率并惩罚平均系统时间；3. 证明目标函数在稳定区域内严格凹，确保最优解存在唯一；4. 提出迭代求解算法和投影梯度法；5. 通过舍入获得整数token分配。

Result: 1. 证明了最优解的存在性和唯一性；2. 推导出一阶最优条件，得到耦合投影定点特征；3. 开发了可计算全局步长边界的投影梯度法；4. 通过仿真评估了整数化分配的性能损失。

Conclusion: 本文为LLM服务器中的多任务token分配提供了理论框架和实用算法，能够在准确率-延迟权衡下实现最优资源分配，并通过仿真验证了方法的有效性。

Abstract: We consider a single large language model (LLM) server that serves a heterogeneous stream of queries belonging to $N$ distinct task types. Queries arrive according to a Poisson process, and each type occurs with a known prior probability. For each task type, the server allocates a fixed number of internal thinking tokens, which determines the computational effort devoted to that query. The token allocation induces an accuracy-latency trade-off: the service time follows an approximately affine function of the allocated tokens, while the probability of a correct response exhibits diminishing returns. Under a first-in, first-out (FIFO) service discipline, the system operates as an $M/G/1$ queue, and the mean system time depends on the first and second moments of the resulting service-time distribution. We formulate a constrained optimization problem that maximizes a weighted average accuracy objective penalized by the mean system time, subject to architectural token-budget constraints and queue-stability conditions. The objective function is shown to be strictly concave over the stability region, which ensures existence and uniqueness of the optimal token allocation. The first-order optimality conditions yield a coupled projected fixed-point characterization of the optimum, together with an iterative solution and an explicit sufficient condition for contraction. Moreover, a projected gradient method with a computable global step-size bound is developed to guarantee convergence beyond the contractive regime. Finally, integer-valued token allocations are attained via rounding of the continuous solution, and the resulting performance loss is evaluated in simulation results.

</details>


### [130] [SPIKE: Sparse Koopman Regularization for Physics-Informed Neural Networks](https://arxiv.org/abs/2601.10282)
*Jose Marie Antonio Minoza*

Main category: cs.LG

TL;DR: SPIKE框架通过将连续时间Koopman算子与PINNs结合，使用L1正则化学习稀疏生成矩阵，提升PINNs在时空外推和长期预测的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在训练域内容易过拟合，在时空外推时泛化能力差，需要一种能学习简约动力学表示的方法来改善外推性能。

Method: 提出SPIKE框架，将连续时间Koopman算子作为正则化项嵌入PINNs训练，在学习的可观测量空间中强制线性动力学dz/dt=Az，并通过L1正则化使生成矩阵A稀疏化。

Result: 在抛物线、双曲线、色散和刚性PDE以及流体动力学（Navier-Stokes）和混沌ODE（Lorenz）等实验中，SPIKE在时间外推、空间泛化和长期预测精度方面均表现出一致性改进。

Conclusion: SPIKE通过结合连续时间Koopman算子的稀疏正则化，有效解决了PINNs的过拟合问题，提升了外推泛化能力，且连续时间公式通过矩阵指数积分为刚性系统提供了无条件稳定性。

Abstract: Physics-Informed Neural Networks (PINNs) provide a mesh-free approach for solving differential equations by embedding physical constraints into neural network training. However, PINNs tend to overfit within the training domain, leading to poor generalization when extrapolating beyond trained spatiotemporal regions. This work presents SPIKE (Sparse Physics-Informed Koopman-Enhanced), a framework that regularizes PINNs with continuous-time Koopman operators to learn parsimonious dynamics representations. By enforcing linear dynamics $dz/dt = Az$ in a learned observable space, both PIKE (without explicit sparsity) and SPIKE (with L1 regularization on $A$) learn sparse generator matrices, embodying the parsimony principle that complex dynamics admit low-dimensional structure. Experiments across parabolic, hyperbolic, dispersive, and stiff PDEs, including fluid dynamics (Navier-Stokes) and chaotic ODEs (Lorenz), demonstrate consistent improvements in temporal extrapolation, spatial generalization, and long-term prediction accuracy. The continuous-time formulation with matrix exponential integration provides unconditional stability for stiff systems while avoiding diagonal dominance issues inherent in discrete-time Koopman operators.

</details>


### [131] [We Need a More Robust Classifier: Dual Causal Learning Empowers Domain-Incremental Time Series Classification](https://arxiv.org/abs/2601.10312)
*Zhipeng Liu,Peibo Duan,Xuan Tang,Haodong Jing,Mingyang Geng,Yongsheng Huang,Jialu Xu,Bin Zhang,Binwu Wang*

Main category: cs.LG

TL;DR: 提出DualCD框架，通过双重因果解耦增强时间序列分类模型在领域增量学习中的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分类研究在领域增量学习方面面临挑战，需要提升模型在领域变化场景下的鲁棒性

Method: 提出轻量级双重因果解耦框架：1) 时间特征解耦模块分离类别因果特征和虚假特征；2) 双重因果干预机制消除类内和类间混杂特征影响

Result: 在多个数据集和模型上的实验表明，DualCD能有效提升领域增量场景下的性能，并建立了综合基准

Conclusion: DualCD框架通过因果解耦和干预机制，显著增强了时间序列分类模型在领域增量学习中的鲁棒性

Abstract: The World Wide Web thrives on intelligent services that rely on accurate time series classification, which has recently witnessed significant progress driven by advances in deep learning. However, existing studies face challenges in domain incremental learning. In this paper, we propose a lightweight and robust dual-causal disentanglement framework (DualCD) to enhance the robustness of models under domain incremental scenarios, which can be seamlessly integrated into time series classification models. Specifically, DualCD first introduces a temporal feature disentanglement module to capture class-causal features and spurious features. The causal features can offer sufficient predictive power to support the classifier in domain incremental learning settings. To accurately capture these causal features, we further design a dual-causal intervention mechanism to eliminate the influence of both intra-class and inter-class confounding features. This mechanism constructs variant samples by combining the current class's causal features with intra-class spurious features and with causal features from other classes. The causal intervention loss encourages the model to accurately predict the labels of these variant samples based solely on the causal features. Extensive experiments on multiple datasets and models demonstrate that DualCD effectively improves performance in domain incremental scenarios. We summarize our rich experiments into a comprehensive benchmark to facilitate research in domain incremental time series classification.

</details>


### [132] [Meta Dynamic Graph for Traffic Flow Prediction](https://arxiv.org/abs/2601.10328)
*Yiqing Zou,Hanning Yuan,Qianyu Yang,Ziqiang Yuan,Shuliang Wang,Sijie Ruan*

Main category: cs.LG

TL;DR: 提出MetaDG框架，通过动态图结构建模时空动态，统一处理时空异质性，提升交通流量预测性能


<details>
  <summary>Details</summary>
Motivation: 现有交通预测方法在处理时空依赖时存在两个主要局限：1) 动态建模通常局限于空间拓扑变化；2) 时空异质性分别建模。需要更全面的动态建模方法来统一处理时空动态和异质性。

Method: 提出MetaDG框架，利用节点表示的动态图结构显式建模时空动态，生成动态邻接矩阵和元参数，将动态建模扩展到拓扑之外，并将时空异质性捕获统一到单一维度。

Result: 在四个真实世界数据集上的广泛实验验证了MetaDG的有效性，表明该方法能够显著提升交通流量预测性能。

Conclusion: MetaDG通过动态图结构统一建模时空动态和异质性，解决了现有方法的局限性，为交通预测提供了更有效的框架。

Abstract: Traffic flow prediction is a typical spatio-temporal prediction problem and has a wide range of applications. The core challenge lies in modeling the underlying complex spatio-temporal dependencies. Various methods have been proposed, and recent studies show that the modeling of dynamics is useful to meet the core challenge. While handling spatial dependencies and temporal dependencies using separate base model structures may hinder the modeling of spatio-temporal correlations, the modeling of dynamics can bridge this gap. Incorporating spatio-temporal heterogeneity also advances the main goal, since it can extend the parameter space and allow more flexibility. Despite these advances, two limitations persist: 1) the modeling of dynamics is often limited to the dynamics of spatial topology (e.g., adjacency matrix changes), which, however, can be extended to a broader scope; 2) the modeling of heterogeneity is often separated for spatial and temporal dimensions, but this gap can also be bridged by the modeling of dynamics. To address the above limitations, we propose a novel framework for traffic prediction, called Meta Dynamic Graph (MetaDG). MetaDG leverages dynamic graph structures of node representations to explicitly model spatio-temporal dynamics. This generates both dynamic adjacency matrices and meta-parameters, extending dynamic modeling beyond topology while unifying the capture of spatio-temporal heterogeneity into a single dimension. Extensive experiments on four real-world datasets validate the effectiveness of MetaDG.

</details>


### [133] [SuS: Strategy-aware Surprise for Intrinsic Exploration](https://arxiv.org/abs/2601.10349)
*Mark Kashirskiy,Ilya Makarov*

Main category: cs.LG

TL;DR: 提出Strategy-aware Surprise (SuS)框架，通过策略稳定性与策略惊喜的互补信号增强强化学习探索，在数学推理任务上显著提升准确性和解决方案多样性。


<details>
  <summary>Details</summary>
Motivation: 传统基于好奇心的探索方法仅依赖状态预测误差，忽略了行为策略的稳定性与策略层面的意外性，限制了探索效率。

Method: 引入策略稳定性(SS)衡量时间步间行为策略一致性，策略惊喜(SuS)捕捉相对于当前策略表示的意外结果，通过学习权重系数结合两种信号形成综合奖励。

Result: 在数学推理任务上，SuS相比基线方法在Pass@1提升17.4%，Pass@5提升26.4%，同时保持更高的策略多样性；消融研究表明移除任一组件会导致至少10%性能下降。

Conclusion: SuS框架通过策略稳定性与策略惊喜的协同作用，有效增强了强化学习的探索能力，在准确性和多样性方面均取得显著改进。

Abstract: We propose Strategy-aware Surprise (SuS), a novel intrinsic motivation framework that uses pre-post prediction mismatch as a novelty signal for exploration in reinforcement learning. Unlike traditional curiosity-driven methods that rely solely on state prediction error, SuS introduces two complementary components: Strategy Stability (SS) and Strategy Surprise (SuS). SS measures consistency in behavioral strategy across temporal steps, while SuS captures unexpected outcomes relative to the agent's current strategy representation. Our combined reward formulation leverages both signals through learned weighting coefficients. We evaluate SuS on mathematical reasoning tasks using large language models, demonstrating significant improvements in both accuracy and solution diversity. Ablation studies confirm that removing either component results in at least 10% performance degradation, validating the synergistic nature of our approach. SuS achieves 17.4% improvement in Pass@1 and 26.4% improvement in Pass@5 compared to baseline methods, while maintaining higher strategy diversity throughout training.

</details>


### [134] [EvoMorph: Counterfactual Explanations for Continuous Time-Series Extrinsic Regression Applied to Photoplethysmography](https://arxiv.org/abs/2601.10356)
*Mesut Ceylan,Alexis Tabin,Patrick Langer,Elgar Fleisch,Filipe Barata*

Main category: cs.LG

TL;DR: 提出EvoMorph框架，通过多目标进化算法为时间序列回归模型生成生理合理的反事实解释，解决现有方法在生物医学连续信号中的局限性。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备能够连续监测PPG等生理信号，但临床应用中需要理解模型预测的稳定性和敏感性。现有反事实解释方法主要针对分类任务，忽视波形形态，且常产生生理上不合理的信号，限制了在连续生物医学时间序列中的应用。

Method: 提出EvoMorph多目标进化框架，优化基于可解释信号描述符的形态感知目标，应用变换以保持波形结构，生成生理合理且多样化的反事实解释。

Result: 在三个PPG数据集（心率、呼吸率、血氧饱和度）上评估，优于最近非邻居基线。案例研究表明EvoMorph可作为不确定性量化工具，将反事实敏感性与bootstrap集成不确定性和数据密度度量相关联。

Conclusion: EvoMorph能够为连续生物医学信号生成生理感知的反事实解释，支持不确定性感知的可解释性，推进临床时间序列应用中可信赖的模型分析。

Abstract: Wearable devices enable continuous, population-scale monitoring of physiological signals, such as photoplethysmography (PPG), creating new opportunities for data-driven clinical assessment. Time-series extrinsic regression (TSER) models increasingly leverage PPG signals to estimate clinically relevant outcomes, including heart rate, respiratory rate, and oxygen saturation. For clinical reasoning and trust, however, single point estimates alone are insufficient: clinicians must also understand whether predictions are stable under physiologically plausible variations and to what extent realistic, attainable changes in physiological signals would meaningfully alter a model's prediction. Counterfactual explanations (CFE) address these "what-if" questions, yet existing time series CFE generation methods are largely restricted to classification, overlook waveform morphology, and often produce physiologically implausible signals, limiting their applicability to continuous biomedical time series. To address these limitations, we introduce EvoMorph, a multi-objective evolutionary framework for generating physiologically plausible and diverse CFE for TSER applications. EvoMorph optimizes morphology-aware objectives defined on interpretable signal descriptors and applies transformations to preserve the waveform structure. We evaluated EvoMorph on three PPG datasets (heart rate, respiratory rate, and oxygen saturation) against a nearest-unlike-neighbor baseline. In addition, in a case study, we evaluated EvoMorph as a tool for uncertainty quantification by relating counterfactual sensitivity to bootstrap-ensemble uncertainty and data-density measures. Overall, EvoMorph enables the generation of physiologically-aware counterfactuals for continuous biomedical signals and supports uncertainty-aware interpretability, advancing trustworthy model analysis for clinical time-series applications.

</details>


### [135] [PLGC: Pseudo-Labeled Graph Condensation](https://arxiv.org/abs/2601.10358)
*Jay Nandy,Arnab Kumar Mondal,Anuj Rathore,Mahesh Chandran*

Main category: cs.LG

TL;DR: PLGC是一种无需真实标签的自监督图压缩方法，通过构建潜在伪标签来匹配原图的结构和特征统计，在标签噪声和分布偏移下表现鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有图压缩方法依赖干净监督标签，在标签稀缺、噪声或不一致时可靠性受限，需要一种无需真实标签的鲁棒压缩方法。

Method: 提出PLGC框架：1) 从节点嵌入构建潜在伪标签；2) 联合学习潜在原型和节点分配；3) 优化压缩图以匹配原图的结构和特征统计，无需真实标签。

Result: 在节点分类和链接预测任务中，PLGC在干净数据集上与最先进的监督压缩方法竞争，在标签噪声下显著优于所有基线方法。

Conclusion: 自监督图压缩在噪声或弱标签环境中具有实际和理论优势，PLGC为标签不可靠场景提供了鲁棒的解决方案。

Abstract: Large graph datasets make training graph neural networks (GNNs) computationally costly. Graph condensation methods address this by generating small synthetic graphs that approximate the original data. However, existing approaches rely on clean, supervised labels, which limits their reliability when labels are scarce, noisy, or inconsistent. We propose Pseudo-Labeled Graph Condensation (PLGC), a self-supervised framework that constructs latent pseudo-labels from node embeddings and optimizes condensed graphs to match the original graph's structural and feature statistics -- without requiring ground-truth labels. PLGC offers three key contributions: (1) A diagnosis of why supervised condensation fails under label noise and distribution shift. (2) A label-free condensation method that jointly learns latent prototypes and node assignments. (3) Theoretical guarantees showing that pseudo-labels preserve latent structural statistics of the original graph and ensure accurate embedding alignment. Empirically, across node classification and link prediction tasks, PLGC achieves competitive performance with state-of-the-art supervised condensation methods on clean datasets and exhibits substantial robustness under label noise, often outperforming all baselines by a significant margin. Our findings highlight the practical and theoretical advantages of self-supervised graph condensation in noisy or weakly-labeled environments.

</details>


### [136] [Discrete Feynman-Kac Correctors](https://arxiv.org/abs/2601.10403)
*Mohsin Hasan,Viktor Ohanesian,Artem Gazizov,Yoshua Bengio,Alán Aspuru-Guzik,Roberto Bondesan,Marta Skreta,Kirill Neklyudov*

Main category: cs.LG

TL;DR: 提出Discrete Feynman-Kac Correctors框架，可在推理时控制离散扩散模型的生成分布，无需额外训练或微调


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在生成离散序列方面表现出色，但缺乏对生成样本分布的灵活控制能力

Method: 基于离散掩码扩散模型，推导Sequential Monte Carlo算法，实现退火采样、多扩散过程边际乘积采样、以及奖励函数加权采样

Result: 框架在多个应用中有效：伊辛模型的退火玻尔兹曼分布采样、代码生成语言模型性能提升、奖励加权的蛋白质序列生成

Conclusion: Discrete Feynman-Kac Correctors为离散扩散模型提供了灵活的控制能力，无需额外训练即可实现多种采样控制

Abstract: Discrete diffusion models have recently emerged as a promising alternative to the autoregressive approach for generating discrete sequences. Sample generation via gradual denoising or demasking processes allows them to capture hierarchical non-sequential interdependencies in the data. These custom processes, however, do not assume a flexible control over the distribution of generated samples. We propose Discrete Feynman-Kac Correctors, a framework that allows for controlling the generated distribution of discrete masked diffusion models at inference time. We derive Sequential Monte Carlo (SMC) algorithms that, given a trained discrete diffusion model, control the temperature of the sampled distribution (i.e. perform annealing), sample from the product of marginals of several diffusion processes (e.g. differently conditioned processes), and sample from the product of the marginal with an external reward function, producing likely samples from the target distribution that also have high reward. Notably, our framework does not require any training of additional models or fine-tuning of the original model. We illustrate the utility of our framework in several applications including: efficient sampling from the annealed Boltzmann distribution of the Ising model, improving the performance of language models for code generation and amortized learning, as well as reward-tilted protein sequence generation.

</details>


### [137] [CS-GBA: A Critical Sample-based Gradient-guided Backdoor Attack for Offline Reinforcement Learning](https://arxiv.org/abs/2601.10407)
*Yuanjie Zhao,Junnan Qiu,Yue Ding,Jie Li*

Main category: cs.LG

TL;DR: CS-GBA是一种针对离线强化学习的后门攻击框架，通过关键样本选择、相关性破坏触发器和梯度引导动作生成，在5%的有限中毒预算下实现高隐蔽性和破坏性。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击策略难以对抗安全约束的离线RL算法（如CQL），因为存在随机中毒效率低和使用易被检测的OOD触发器等问题。

Method: 1. 基于TD误差的关键样本选择策略，将攻击预算集中在最有影响力的状态转移上；2. 相关性破坏触发器机制，利用状态特征的物理互斥性保持统计隐蔽性；3. 梯度引导动作生成机制，在数据流形中搜索最坏动作替代传统的标签反转。

Result: 在D4RL基准测试中，该方法显著优于现有基线，在仅5%的中毒预算下对代表性安全约束算法实现了高攻击成功率，同时在干净环境中保持智能体性能。

Conclusion: CS-GBA框架成功解决了离线RL后门攻击中的隐蔽性和效率问题，通过理论驱动的关键样本选择和统计隐蔽的触发器设计，实现了在严格预算下的高效攻击。

Abstract: Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to backdoor attacks. Existing attack strategies typically struggle against safety-constrained algorithms (e.g., CQL) due to inefficient random poisoning and the use of easily detectable Out-of-Distribution (OOD) triggers. In this paper, we propose CS-GBA (Critical Sample-based Gradient-guided Backdoor Attack), a novel framework designed to achieve high stealthiness and destructiveness under a strict budget. Leveraging the theoretical insight that samples with high Temporal Difference (TD) errors are pivotal for value function convergence, we introduce an adaptive Critical Sample Selection strategy that concentrates the attack budget on the most influential transitions. To evade OOD detection, we propose a Correlation-Breaking Trigger mechanism that exploits the physical mutual exclusivity of state features (e.g., 95th percentile boundaries) to remain statistically concealed. Furthermore, we replace the conventional label inversion with a Gradient-Guided Action Generation mechanism, which searches for worst-case actions within the data manifold using the victim Q-network's gradient. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms state-of-the-art baselines, achieving high attack success rates against representative safety-constrained algorithms with a minimal 5% poisoning budget, while maintaining the agent's performance in clean environments.

</details>


### [138] [Reinforcement Learning with Multi-Step Lookahead Information Via Adaptive Batching](https://arxiv.org/abs/2601.10418)
*Nadav Merlis*

Main category: cs.LG

TL;DR: 研究具有多步前瞻信息的表格强化学习问题，提出自适应批处理策略（ABP）及其学习算法，获得接近最优的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 在多步前瞻信息（agent在行动前可以观察到ℓ步的未来转移和奖励）的强化学习场景中，现有两种启发式方法（固定批处理策略和模型预测控制）存在不足，需要更有效的策略利用前瞻信息。

Method: 提出自适应批处理策略（ABP），根据状态自适应地处理前瞻信息。推导了ABP的最优贝尔曼方程，并设计了乐观的遗憾最小化算法来学习未知环境中的最优ABP。

Result: 获得了阶最优的遗憾界，最多只差一个前瞻视野ℓ的因子（通常ℓ是较小的常数）。

Conclusion: 自适应批处理策略（ABP）能更有效地利用前瞻信息，相比固定批处理和模型预测控制有优势，且可通过提出的算法在未知环境中学习到最优ABP。

Abstract: We study tabular reinforcement learning problems with multiple steps of lookahead information. Before acting, the learner observes $\ell$ steps of future transition and reward realizations: the exact state the agent would reach and the rewards it would collect under any possible course of action. While it has been shown that such information can drastically boost the value, finding the optimal policy is NP-hard, and it is common to apply one of two tractable heuristics: processing the lookahead in chunks of predefined sizes ('fixed batching policies'), and model predictive control. We first illustrate the problems with these two approaches and propose utilizing the lookahead in adaptive (state-dependent) batches; we refer to such policies as adaptive batching policies (ABPs). We derive the optimal Bellman equations for these strategies and design an optimistic regret-minimizing algorithm that enables learning the optimal ABP when interacting with unknown environments. Our regret bounds are order-optimal up to a potential factor of the lookahead horizon $\ell$, which can usually be considered a small constant.

</details>


### [139] [DeFlow: Decoupling Manifold Modeling and Value Maximization for Offline Policy Extraction](https://arxiv.org/abs/2601.10471)
*Zhancun Mu*

Main category: cs.LG

TL;DR: DeFlow是一个解耦的离线强化学习框架，使用流匹配技术捕捉复杂行为流形，通过轻量级精炼模块在信任区域内优化，避免了ODE求解器反向传播的计算负担。


<details>
  <summary>Details</summary>
Motivation: 传统生成策略优化需要通过ODE求解器进行反向传播，计算成本过高。现有方法要么牺牲迭代生成能力进行单步蒸馏，要么需要平衡多个损失项，导致训练不稳定。

Method: 提出解耦框架：1) 使用流匹配捕捉复杂行为流形；2) 在流形信任区域内学习轻量级精炼模块，避免对ODE求解器求导；3) 保持流的迭代表达能力，无需平衡损失项。

Result: 在挑战性的OGBench基准测试中取得优越性能，并展示了高效的离线到在线适应能力。

Conclusion: DeFlow通过解耦设计和流匹配技术，在保持迭代生成能力的同时实现了计算高效的策略优化，为离线RL提供了稳定且表达力强的解决方案。

Abstract: We present DeFlow, a decoupled offline RL framework that leverages flow matching to faithfully capture complex behavior manifolds. Optimizing generative policies is computationally prohibitive, typically necessitating backpropagation through ODE solvers. We address this by learning a lightweight refinement module within an explicit, data-derived trust region of the flow manifold, rather than sacrificing the iterative generation capability via single-step distillation. This way, we bypass solver differentiation and eliminate the need for balancing loss terms, ensuring stable improvement while fully preserving the flow's iterative expressivity. Empirically, DeFlow achieves superior performance on the challenging OGBench benchmark and demonstrates efficient offline-to-online adaptation.

</details>


### [140] [Communication-Efficient Federated Learning by Exploiting Spatio-Temporal Correlations of Gradients](https://arxiv.org/abs/2601.10491)
*Shenlong Zheng,Zhen Zhang,Yuhui Deng,Geyong Min,Lin Cui*

Main category: cs.LG

TL;DR: GradESTC是一种利用梯度的空间和时间相关性来减少联邦学习通信开销的压缩技术，通过传输轻量级组合系数和少量更新的基向量替代完整梯度，平均减少39.79%的上行通信。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的通信开销是带宽受限网络中的关键挑战。现有方法主要关注压缩单个梯度，忽略了梯度之间的时间相关性。研究发现梯度不仅具有空间相关性（低秩结构），还在相邻轮次间表现出强时间相关性。

Method: 提出GradESTC压缩技术：1）利用空间相关性将完整梯度分解为紧凑的基向量集和组合系数；2）利用时间相关性，每轮只需动态更新少量基向量；3）传输轻量级组合系数和少量更新的基向量替代完整梯度。

Result: 实验表明，在达到接近收敛的目标精度水平时，GradESTC相比最强基线平均减少39.79%的上行通信，同时保持与未压缩FedAvg相当的收敛速度和最终精度。

Conclusion: 通过有效利用梯度的时空结构，GradESTC为通信高效的联邦学习提供了实用且可扩展的解决方案。

Abstract: Communication overhead is a critical challenge in federated learning, particularly in bandwidth-constrained networks. Although many methods have been proposed to reduce communication overhead, most focus solely on compressing individual gradients, overlooking the temporal correlations among them. Prior studies have shown that gradients exhibit spatial correlations, typically reflected in low-rank structures. Through empirical analysis, we further observe a strong temporal correlation between client gradients across adjacent rounds. Based on these observations, we propose GradESTC, a compression technique that exploits both spatial and temporal gradient correlations. GradESTC exploits spatial correlations to decompose each full gradient into a compact set of basis vectors and corresponding combination coefficients. By exploiting temporal correlations, only a small portion of the basis vectors need to be dynamically updated in each round. GradESTC significantly reduces communication overhead by transmitting lightweight combination coefficients and a limited number of updated basis vectors instead of the full gradients. Extensive experiments show that, upon reaching a target accuracy level near convergence, GradESTC reduces uplink communication by an average of 39.79% compared to the strongest baseline, while maintaining comparable convergence speed and final accuracy to uncompressed FedAvg. By effectively leveraging spatio-temporal gradient structures, GradESTC offers a practical and scalable solution for communication-efficient federated learning.

</details>


### [141] [Projected Microbatch Accumulation yields reference-free proximal policy updates for reinforcement learning](https://arxiv.org/abs/2601.10498)
*Nilin Abrahamsen*

Main category: cs.LG

TL;DR: PROMA是一种用于大语言模型微调的近端策略更新方法，通过投影去除序列级梯度分量，在微批次间累积策略梯度，实现更稳定的策略学习。


<details>
  <summary>Details</summary>
Motivation: 现有方法如PPO和GRPO在策略更新中存在局限性：PPO依赖参考策略和似然比裁剪，GRPO可能导致熵崩溃，且两者对局部KL散度的控制不够紧密。需要一种更稳定、无需参考策略且能有效控制KL散度的近端策略更新方法。

Method: PROMA在反向传播过程中逐层投影去除序列级梯度分量，然后在微批次间累积策略梯度。该方法无需额外的前向或反向传播，实现高效。通过投影操作，能够更紧密地控制局部KL散度，避免熵崩溃。

Result: 实验表明，与GRPO相比，PROMA能够更紧密地控制局部KL散度，实现更稳定的策略学习。与PPO和GRPO不同，PROMA实现了近端更新而不会导致熵崩溃，且不依赖参考策略或似然比裁剪。

Conclusion: PROMA是一种有效的大语言模型微调方法，通过投影微批次累积实现了更稳定、无需参考策略的近端策略更新，解决了现有方法的局限性。

Abstract: This note introduces Projected Microbatch Accumulation (PROMA), a proximal policy update method for large language model fine-tuning. PROMA accumulates policy gradients across microbatches by projecting out sequence-wise gradient components before microbatch aggregation. The projection is applied layer-wise during the backward pass, enabling efficient implementation without additional forward or backward passes. Empirically, PROMA enforces tighter control of local KL divergence than GRPO, resulting in more stable policy learning. Unlike PPO and GRPO, PROMA achieves proximal updates without inducing entropy collapse and does not rely on a reference policy or likelihood-ratio clipping.

</details>


### [142] [Transformer-Based Cognitive Radio: Adaptive Modulation Strategies Using Transformer Models](https://arxiv.org/abs/2601.10519)
*Andrea Melis,Andrea Piroddi,Roberto Girau*

Main category: cs.LG

TL;DR: 该研究探索使用GPT-2 Transformer模型生成新型调制方案，与传统方法相比，在SNR和PSD等指标上表现出相当或更优的性能。


<details>
  <summary>Details</summary>
Motivation: 认知无线电系统需要动态适应变化的频谱环境，机器学习技术特别是Transformer模型可以提升频谱效率、鲁棒性和安全性。传统调制方案可能无法充分利用频谱资源，需要创新的调制方案生成方法。

Method: 使用GPT-2 Transformer架构，在现有调制公式数据集上进行训练，生成新的调制方案。然后将生成的方案与传统方法在信噪比(SNR)和功率谱密度(PSD)等关键性能指标上进行比较评估。

Result: Transformer生成的调制方案在性能上与传统方法相当，在某些情况下甚至优于传统方法。这表明Transformer模型能够生成有效的调制方案，为认知无线电系统提供新的技术路径。

Conclusion: 先进的认知无线电系统可以通过实施Transformer模型获得显著益处，从而实现更高效、更鲁棒、更安全的通信系统。Transformer模型在调制方案生成方面具有实际应用价值。

Abstract: Cognitive Radio (CR) systems, which dynamically adapt to changing spectrum environments, could benefit significantly from advancements in machine learning technologies. These systems can be enhanced in terms of spectral efficiency, robustness, and security through innovative approaches such as the use of Transformer models. This work investigates the application of Transformer models, specifically the GPT-2 architecture, to generate novel modulation schemes for wireless communications. By training a GPT-2 model on a dataset of existing modulation formulas, new modulation schemes has been created. These generated schemes are then compared to traditional methods using key performance metrics such as Signal-to-Noise Ratio (SNR) and Power Spectrum Density (PSD). The results show that Transformer-generated modulation schemes can achieve performance comparable to, and in some cases outperforming, traditional methods. This demonstrates that advanced CR systems could greatly benefit from the implementation of Transformer models, leading to more efficient, robust, and secure communication systems.

</details>


### [143] [Mixtures of Transparent Local Models](https://arxiv.org/abs/2601.10541)
*Niffa Cheick Oumar Diaby,Thierry Duchesne,Mario Marchand*

Main category: cs.LG

TL;DR: 提出一种混合透明局部模型的方法，通过多预测器损失函数学习透明标注函数及其适用区域，在输入空间不同区域使用简单透明函数建模，建立PAC-Bayesian风险界限。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在人类活动中日益普及，但缺乏透明度会影响安全性、公平性等关键因素。需要设计可解释的透明模型，特别是在某些输入区域适合使用简单透明函数建模，但不同区域间函数可能发生突变的情况。

Method: 提出混合透明局部模型方法，通过多预测器（多区域）损失函数同时学习透明标注函数及其适用区域。为二元线性分类和线性回归问题建立了严格的PAC-Bayesian风险界限，并在合成数据集上验证算法有效性。

Result: 在合成数据集上展示了学习算法的工作原理，在真实数据集上的结果表明该方法与其他现有方法以及某些不透明模型相比具有竞争力。

Conclusion: 混合透明局部模型方法提供了一种有效的可解释模型设计替代方案，能够处理输入空间不同区域需要不同简单透明函数的情况，同时保持模型的透明性和可解释性。

Abstract: The predominance of machine learning models in many spheres of human activity has led to a growing demand for their transparency. The transparency of models makes it possible to discern some factors, such as security or non-discrimination. In this paper, we propose a mixture of transparent local models as an alternative solution for designing interpretable (or transparent) models. Our approach is designed for the situations where a simple and transparent function is suitable for modeling the label of instances in some localities/regions of the input space, but may change abruptly as we move from one locality to another. Consequently, the proposed algorithm is to learn both the transparent labeling function and the locality of the input space where the labeling function achieves a small risk in its assigned locality. By using a new multi-predictor (and multi-locality) loss function, we established rigorous PAC-Bayesian risk bounds for the case of binary linear classification problem and that of linear regression. In both cases, synthetic data sets were used to illustrate how the learning algorithms work. The results obtained from real data sets highlight the competitiveness of our approach compared to other existing methods as well as certain opaque models. Keywords: PAC-Bayes, risk bounds, local models, transparent models, mixtures of local transparent models.

</details>


### [144] [Process-Guided Concept Bottleneck Model](https://arxiv.org/abs/2601.10562)
*Reza M. Asiyabi,SEOSAW Partnership,Steven Hancock,Casey Ryan*

Main category: cs.LG

TL;DR: PG-CBM扩展概念瓶颈模型，通过领域定义的因果机制约束学习，使用生物物理意义的中介概念，在稀疏监督的科学领域提高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 标准CBM忽略领域特定关系和因果机制，且依赖完整概念标签，限制了在监督稀疏但过程定义明确的科学领域的应用。

Method: 提出过程引导概念瓶颈模型(PG-CBM)，通过生物物理意义的中介概念，约束学习遵循领域定义的因果机制，利用多源异构训练数据。

Result: 以地球观测数据估算地上生物量密度为例，PG-CBM相比多个基准方法减少了误差和偏差，同时产生可解释的中介输出。

Conclusion: PG-CBM不仅提高准确性，还增强透明度，能够检测虚假学习，提供科学洞见，是迈向科学应用中更可信AI系统的一步。

Abstract: Concept Bottleneck Models (CBMs) improve the explainability of black-box Deep Learning (DL) by introducing intermediate semantic concepts. However, standard CBMs often overlook domain-specific relationships and causal mechanisms, and their dependence on complete concept labels limits applicability in scientific domains where supervision is sparse but processes are well defined. To address this, we propose the Process-Guided Concept Bottleneck Model (PG-CBM), an extension of CBMs which constrains learning to follow domain-defined causal mechanisms through biophysically meaningful intermediate concepts. Using above ground biomass density estimation from Earth Observation data as a case study, we show that PG-CBM reduces error and bias compared to multiple benchmarks, whilst leveraging multi-source heterogeneous training data and producing interpretable intermediate outputs. Beyond improved accuracy, PG-CBM enhances transparency, enables detection of spurious learning, and provides scientific insights, representing a step toward more trustworthy AI systems in scientific applications.

</details>


### [145] [Kolmogorov Arnold Networks and Multi-Layer Perceptrons: A Paradigm Shift in Neural Modelling](https://arxiv.org/abs/2601.10563)
*Aradhya Gaonkar,Nihal Jain,Vignesh Chougule,Nikhil Deshpande,Sneha Varur,Channabasappa Muttal*

Main category: cs.LG

TL;DR: KAN在非线性函数逼近、时间序列预测和多变量分类任务中全面优于MLP，在保持更高预测精度的同时显著降低计算成本，特别适合资源受限和实时应用场景。


<details>
  <summary>Details</summary>
Motivation: 传统多层感知器（MLP）在计算效率和精度平衡方面存在局限，而基于Kolmogorov表示定理的Kolmogorov-Arnold网络（KAN）提供了一种新的神经网络框架，本研究旨在系统比较KAN与MLP在不同计算任务中的性能表现。

Method: 采用全面的比较分析方法，在多种数据集上评估KAN和MLP，包括数学函数估计（二次和三次函数）、实际应用（日温度预测）和分类任务（葡萄酒分类）。使用均方误差（MSE）衡量预测精度，浮点运算（FLOPs）评估计算成本。

Result: KAN在所有基准测试中均可靠地超越MLP，获得更高的预测精度且计算成本显著降低，展现了在计算效率和精度之间的优越平衡能力。

Conclusion: KAN因其自适应样条激活函数和网格结构，在资源受限和实时操作环境中具有明显优势，为特定任务选择最合适的神经网络架构提供了系统框架，并展示了在需要可解释性和计算效率的场景中的变革潜力。

Abstract: The research undertakes a comprehensive comparative analysis of Kolmogorov-Arnold Networks (KAN) and Multi-Layer Perceptrons (MLP), highlighting their effectiveness in solving essential computational challenges like nonlinear function approximation, time-series prediction, and multivariate classification. Rooted in Kolmogorov's representation theorem, KANs utilize adaptive spline-based activation functions and grid-based structures, providing a transformative approach compared to traditional neural network frameworks. Utilizing a variety of datasets spanning mathematical function estimation (quadratic and cubic) to practical uses like predicting daily temperatures and categorizing wines, the proposed research thoroughly assesses model performance via accuracy measures like Mean Squared Error (MSE) and computational expense assessed through Floating Point Operations (FLOPs). The results indicate that KANs reliably exceed MLPs in every benchmark, attaining higher predictive accuracy with significantly reduced computational costs. Such an outcome highlights their ability to maintain a balance between computational efficiency and accuracy, rendering them especially beneficial in resource-limited and real-time operational environments. By elucidating the architectural and functional distinctions between KANs and MLPs, the paper provides a systematic framework for selecting the most suitable neural architectures for specific tasks. Furthermore, the proposed study highlights the transformative capabilities of KANs in progressing intelligent systems, influencing their use in situations that require both interpretability and computational efficiency.

</details>


### [146] [Combinatorial Optimization Augmented Machine Learning](https://arxiv.org/abs/2601.10583)
*Maximilian Schiffer,Heiko Hoppe,Yue Su,Louis Bouvier,Axel Parmentier*

Main category: cs.LG

TL;DR: COAML（组合优化增强机器学习）综述：介绍将组合优化预言机嵌入机器学习流程的新范式，构建数据驱动且保持可行性的策略，涵盖机器学习、运筹学和随机优化的交叉领域。


<details>
  <summary>Details</summary>
Motivation: COAML作为新兴范式，能够将预测模型与组合决策相结合，构建既数据驱动又保持可行性的策略，填补机器学习、运筹学和随机优化之间的空白，需要系统性的综述来统一框架并指导未来研究。

Method: 提出统一的COAML流程框架，描述方法构建模块，形式化其与经验成本最小化的联系，建立基于不确定性和决策结构的问题分类法，回顾静态和动态问题的算法方法，并综合方法论贡献。

Result: 创建了全面的COAML分类体系，涵盖了调度、车辆路径、随机规划和强化学习等领域的应用，系统化了经验成本最小化、模仿学习和强化学习等方法论贡献，识别了关键研究前沿。

Conclusion: COAML是组合优化与机器学习交叉领域的重要范式，该综述既可作为该领域的入门教程，也可作为未来研究的路线图，为构建数据驱动且保持可行性的决策策略提供了系统框架。

Abstract: Combinatorial optimization augmented machine learning (COAML) has recently emerged as a powerful paradigm for integrating predictive models with combinatorial decision-making. By embedding combinatorial optimization oracles into learning pipelines, COAML enables the construction of policies that are both data-driven and feasibility-preserving, bridging the traditions of machine learning, operations research, and stochastic optimization. This paper provides a comprehensive overview of the state of the art in COAML. We introduce a unifying framework for COAML pipelines, describe their methodological building blocks, and formalize their connection to empirical cost minimization. We then develop a taxonomy of problem settings based on the form of uncertainty and decision structure. Using this taxonomy, we review algorithmic approaches for static and dynamic problems, survey applications across domains such as scheduling, vehicle routing, stochastic programming, and reinforcement learning, and synthesize methodological contributions in terms of empirical cost minimization, imitation learning, and reinforcement learning. Finally, we identify key research frontiers. This survey aims to serve both as a tutorial introduction to the field and as a roadmap for future research at the interface of combinatorial optimization and machine learning.

</details>


### [147] [ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition](https://arxiv.org/abs/2601.10591)
*Arundeep Chinta,Lucas Vinh Tran,Jay Katukuri*

Main category: cs.LG

TL;DR: 提出了ProbFM，一个基于深度证据回归的Transformer概率框架，用于金融时间序列预测，提供理论基础的认知-偶然不确定性分解，相比现有方法具有更好的不确定性量化能力。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列基础模型在金融预测中缺乏有效的不确定性量化方法：现有方法要么依赖限制性分布假设，要么混淆不同不确定性来源，或者缺乏原则性校准机制。需要一种能够提供理论基础的认知-偶然不确定性分解的方法。

Method: 提出了ProbFM（概率基础模型），基于Transformer架构，采用深度证据回归（DER）方法，通过高阶证据学习来学习最优不确定性表示，同时保持单次计算效率。为了独立评估DER方法，还使用一致的LSTM架构对五种概率方法进行了对比研究。

Result: 在加密货币收益预测评估中，DER方法在保持竞争力的预测准确性的同时，提供了明确的认知-偶然不确定性分解。实验表明DER在金融应用中具有有效性。

Conclusion: 这项工作为基于基础模型的原则性不确定性量化建立了一个可扩展的框架，并为DER在金融应用中的有效性提供了实证证据，解决了当前时间序列基础模型在不确定性量化方面的核心限制。

Abstract: Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs employ sophisticated techniques such as mixture models, Student's t-distributions, or conformal prediction, they fail to address the core challenge of providing theoretically-grounded uncertainty decomposition. For the very first time, we present a novel transformer-based probabilistic framework, ProbFM (probabilistic foundation model), that leverages Deep Evidential Regression (DER) to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches that pre-specify distributional forms or require sampling-based inference, ProbFM learns optimal uncertainty representations through higher-order evidence learning while maintaining single-pass computational efficiency. To rigorously evaluate the core DER uncertainty quantification approach independent of architectural complexity, we conduct an extensive controlled comparison study using a consistent LSTM architecture across five probabilistic methods: DER, Gaussian NLL, Student's-t NLL, Quantile Loss, and Conformal Prediction. Evaluation on cryptocurrency return forecasting demonstrates that DER maintains competitive forecasting accuracy while providing explicit epistemic-aleatoric uncertainty decomposition. This work establishes both an extensible framework for principled uncertainty quantification in foundation models and empirical evidence for DER's effectiveness in financial applications.

</details>


### [148] [STEM: Scaling Transformers with Embedding Modules](https://arxiv.org/abs/2601.10639)
*Ranajoy Sadhukhan,Sheng Cao,Harry Dong,Changsheng Zhao,Attiano Purpura-Pontoniere,Yuandong Tian,Zechun Liu,Beidi Chen*

Main category: cs.LG

TL;DR: STEM是一种静态、基于token索引的稀疏Transformer架构，用嵌入查找替代FFN上投影，在减少计算和参数访问的同时提升模型性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 细粒度稀疏性虽然能提高参数容量而不增加计算成本，但存在训练不稳定、负载均衡和通信开销等问题。需要一种既能提升容量又能保持稳定性和效率的方法。

Method: STEM用静态的token索引嵌入查找替换FFN的上投影层，保持门控和下投影层密集。这种方法消除了运行时路由，支持CPU异步预取，并将容量与计算和通信解耦。

Result: STEM训练稳定，在350M和1B规模上带来3-4%的准确率提升，特别是在知识和推理密集型任务上表现突出。减少约三分之一的FFN参数访问，增强长上下文性能，并提升可解释性。

Conclusion: STEM是扩展参数内存的有效方法，提供更好的可解释性、训练稳定性和效率，同时支持知识编辑和注入，实现测试时容量随序列长度增长而扩展。

Abstract: Fine-grained sparsity promises higher parametric capacity without proportional per-token compute, but often suffers from training instability, load balancing, and communication overhead. We introduce STEM (Scaling Transformers with Embedding Modules), a static, token-indexed approach that replaces the FFN up-projection with a layer-local embedding lookup while keeping the gate and down-projection dense. This removes runtime routing, enables CPU offload with asynchronous prefetch, and decouples capacity from both per-token FLOPs and cross-device communication. Empirically, STEM trains stably despite extreme sparsity. It improves downstream performance over dense baselines while reducing per-token FLOPs and parameter accesses (eliminating roughly one-third of FFN parameters). STEM learns embedding spaces with large angular spread which enhances its knowledge storage capacity. More interestingly, this enhanced knowledge capacity comes with better interpretability. The token-indexed nature of STEM embeddings allows simple ways to perform knowledge editing and knowledge injection in an interpretable manner without any intervention in the input text or additional computation. In addition, STEM strengthens long-context performance: as sequence length grows, more distinct parameters are activated, yielding practical test-time capacity scaling. Across 350M and 1B model scales, STEM delivers up to ~3--4% accuracy improvements overall, with notable gains on knowledge and reasoning-heavy benchmarks (ARC-Challenge, OpenBookQA, GSM8K, MMLU). Overall, STEM is an effective way of scaling parametric memory while providing better interpretability, better training stability and improved efficiency.

</details>


### [149] [Single-Stage Huffman Encoder for ML Compression](https://arxiv.org/abs/2601.10673)
*Aditya Agrawal,Albert Magyar,Hiteshwar Eswaraiah,Patrick Sheridan,Pradeep Janedula,Ravi Krishnan Venkatesan,Krishna Nair,Ravi Iyer*

Main category: cs.LG

TL;DR: 提出单阶段霍夫曼编码器，使用基于历史数据平均概率分布的固定码本，消除传统三阶段霍夫曼编码的计算、延迟和数据开销，实现接近理想压缩率的实时压缩。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练和服务中，多加速器间的集体操作常受网络带宽限制。传统无损霍夫曼编码虽然有效，但其三阶段设计（频率分析、码本生成、码本传输）带来计算、延迟和数据开销，在延迟敏感场景（如芯片间通信）中不可行。

Method: 提出单阶段霍夫曼编码器，使用从先前数据批次平均概率分布推导出的固定码本。分析Gemma 2B模型发现张量在层和分片间具有高度统计相似性，因此可用固定码本替代动态生成的码本。

Result: 压缩率在分片级霍夫曼编码的0.5%以内，接近理想香农可压缩性的1%以内，实现了高效的实时压缩。

Conclusion: 单阶段霍夫曼编码器消除了传统霍夫曼编码的开销，在保持接近理想压缩率的同时，适用于延迟敏感的大语言模型通信场景。

Abstract: Training and serving Large Language Models (LLMs) require partitioning data across multiple accelerators, where collective operations are frequently bottlenecked by network bandwidth. Lossless compression using Huffman codes is an effective way to alleviate the issue, however, its three-stage design requiring on-the-fly frequency analysis, codebook generation and transmission of codebook along with data introduces computational, latency and data overheads which are prohibitive for latency-sensitive scenarios such as die-to-die communication. This paper proposes a single-stage Huffman encoder that eliminates these overheads by using fixed codebooks derived from the average probability distribution of previous data batches. Through our analysis of the Gemma 2B model, we demonstrate that tensors exhibit high statistical similarity across layers and shards. Using this approach we achieve compression within 0.5% of per-shard Huffman coding and within 1% of the ideal Shannon compressibility, enabling efficient on-the-fly compression.

</details>


### [150] [On the origin of neural scaling laws: from random graphs to natural language](https://arxiv.org/abs/2601.10684)
*Maissam Barkeshli,Alberto Alfarano,Andrey Gromov*

Main category: cs.LG

TL;DR: 该研究通过简化实验（如图上的随机游走和简化语言模型）探索神经缩放定律的起源，发现即使在没有幂律结构的数据中也会出现缩放定律，并提供了对现有缩放定律分析方法的批判性评估。


<details>
  <summary>Details</summary>
Motivation: 理解神经缩放定律的起源，特别是检验"缩放定律源于数据中已有的幂律结构"这一常见假设。通过简化实验设置来系统研究缩放定律的出现条件。

Method: 1) 在具有可调复杂度的图上训练transformer预测随机游走（二元语法）；2) 通过从简化生成语言模型（4层、2层、1层transformer语言模型到语言二元语法）采样序列来系统降低自然语言复杂度；3) 在Erdös-Renyi和Barabási-Albert随机图上进行随机游走训练；4) 使用2层transformer和50上下文长度重现传统语言建模缩放定律。

Result: 1) 即使在没有幂律结构的数据相关性中，简化设置也会产生神经缩放定律；2) 缩放指数随语言复杂度降低而单调演化；3) 可以在小型模型上重现传统缩放定律的关键结果；4) 提供了现有拟合方法的批判性分析；5) 初步证据表明最大更新参数化可能比标准参数化更参数高效。

Conclusion: 神经缩放定律可以在没有数据幂律结构的情况下出现，表明缩放定律可能有更基本的起源。简化实验环境为研究缩放定律提供了可控框架，同时现有缩放定律分析方法需要更严谨的评估。

Abstract: Scaling laws have played a major role in the modern AI revolution, providing practitioners predictive power over how the model performance will improve with increasing data, compute, and number of model parameters. This has spurred an intense interest in the origin of neural scaling laws, with a common suggestion being that they arise from power law structure already present in the data. In this paper we study scaling laws for transformers trained to predict random walks (bigrams) on graphs with tunable complexity. We demonstrate that this simplified setting already gives rise to neural scaling laws even in the absence of power law structure in the data correlations. We further consider dialing down the complexity of natural language systematically, by training on sequences sampled from increasingly simplified generative language models, from 4,2,1-layer transformer language models down to language bigrams, revealing a monotonic evolution of the scaling exponents. Our results also include scaling laws obtained from training on random walks on random graphs drawn from Erdös-Renyi and scale-free Barabási-Albert ensembles. Finally, we revisit conventional scaling laws for language modeling, demonstrating that several essential results can be reproduced using 2 layer transformers with context length of 50, provide a critical analysis of various fits used in prior literature, demonstrate an alternative method for obtaining compute optimal curves as compared with current practice in published literature, and provide preliminary evidence that maximal update parameterization may be more parameter efficient than standard parameterization.

</details>


### [151] [Data-driven stochastic reduced-order modeling of parametrized dynamical systems](https://arxiv.org/abs/2601.10690)
*Andrew F. Ilersich,Kevin Course,Prasanth B. Nair*

Main category: cs.LG

TL;DR: 提出基于摊销随机变分推理的数据驱动框架，学习连续时间随机降阶模型，能在参数空间和强迫条件下泛化，无需昂贵前向求解器训练。


<details>
  <summary>Details</summary>
Motivation: 复杂动力系统在不同条件下的建模计算成本高，现有降阶模型方法难以处理随机动力学且无法量化预测不确定性，限制了在鲁棒决策中的应用。

Method: 基于摊销随机变分推理，利用马尔可夫高斯过程的重参数化技巧，联合学习概率自编码器和控制潜在动力学的随机微分方程，训练成本与数据集大小和系统刚度无关。

Result: 在三个挑战性测试问题上展示了优秀泛化能力（对未见参数组合和强迫条件），相比现有方法获得显著效率提升。

Conclusion: 该框架为复杂随机动力系统提供高效、可泛化的降阶建模方法，支持物理先验知识融入，在计算效率和不确定性量化方面优于现有方法。

Abstract: Modeling complex dynamical systems under varying conditions is computationally intensive, often rendering high-fidelity simulations intractable. Although reduced-order models (ROMs) offer a promising solution, current methods often struggle with stochastic dynamics and fail to quantify prediction uncertainty, limiting their utility in robust decision-making contexts. To address these challenges, we introduce a data-driven framework for learning continuous-time stochastic ROMs that generalize across parameter spaces and forcing conditions. Our approach, based on amortized stochastic variational inference, leverages a reparametrization trick for Markov Gaussian processes to eliminate the need for computationally expensive forward solvers during training. This enables us to jointly learn a probabilistic autoencoder and stochastic differential equations governing the latent dynamics, at a computational cost that is independent of the dataset size and system stiffness. Additionally, our approach offers the flexibility of incorporating physics-informed priors if available. Numerical studies are presented for three challenging test problems, where we demonstrate excellent generalization to unseen parameter combinations and forcings, and significant efficiency gains compared to existing approaches.

</details>


### [152] [Communication-Efficient and Privacy-Adaptable Mechanism -- a Federated Learning Scheme with Convergence Analysis](https://arxiv.org/abs/2601.10701)
*Chun Hei Michael Shiu,Chih Wei Ling*

Main category: cs.LG

TL;DR: 本文对CEPAM（通信高效且隐私可调机制）进行了理论分析和实验评估，该机制通过RSUQ量化器实现联邦学习中的通信效率和隐私保护平衡。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在数据治理约束下实现隐私保护协作，但面临通信效率和隐私保护的关键挑战。CEPAM作为同时实现这两个目标的新方法需要进一步理论分析和实验验证。

Method: 使用RSUQ（拒绝采样通用量化器）作为随机向量量化器，其量化误差等效于预设噪声，可调节以定制各方之间的隐私保护。对CEPAM进行隐私保证和收敛特性的理论分析。

Result: 通过实验评估CEPAM的效用性能，包括与其他基线的收敛曲线比较，以及不同参与方之间的准确率-隐私权衡分析。

Conclusion: CEPAM在联邦学习中有效平衡了通信效率和隐私保护，通过理论分析和实验验证了其隐私保证和收敛特性，为实际应用提供了实用解决方案。

Abstract: Federated learning enables multiple parties to jointly train learning models without sharing their own underlying data, offering a practical pathway to privacy-preserving collaboration under data-governance constraints. Continued study of federated learning is essential to address key challenges in it, including communication efficiency and privacy protection between parties. A recent line of work introduced a novel approach called the Communication-Efficient and Privacy-Adaptable Mechanism (CEPAM), which achieves both objectives simultaneously. CEPAM leverages the rejection-sampled universal quantizer (RSUQ), a randomized vector quantizer whose quantization error is equivalent to a prescribed noise, which can be tuned to customize privacy protection between parties. In this work, we theoretically analyze the privacy guarantees and convergence properties of CEPAM. Moreover, we assess CEPAM's utility performance through experimental evaluations, including convergence profiles compared with other baselines, and accuracy-privacy trade-offs between different parties.

</details>


### [153] [Distributed Perceptron under Bounded Staleness, Partial Participation, and Noisy Communication](https://arxiv.org/abs/2601.10705)
*Keval Jain,Anant Raj,Saurav Prakash,Girish Varma*

Main category: cs.LG

TL;DR: 该论文研究了在联邦学习环境中使用迭代参数混合训练的异步客户端-服务器感知机，分析了系统延迟、部分参与和通信噪声对性能的影响，并提出了一种确定性的聚合规则来管理更新延迟。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决联邦学习和分布式部署中的三个关键系统效应：1) 由于模型交付延迟和客户端计算延迟导致的陈旧更新（双向版本滞后）；2) 部分参与（客户端间歇性可用性）；3) 下行和上行链路上的不完美通信（建模为有界二阶矩的零均值加性噪声）。这些因素在实际部署中普遍存在但缺乏系统分析。

Method: 提出了一种称为"带填充的陈旧性桶聚合"的服务器端聚合规则，该规则确定性地强制执行预定的陈旧性配置文件，而不假设延迟或参与的随机模型。在边缘可分性和有界数据半径的假设下，分析了感知机在迭代参数混合训练中的性能。

Result: 证明了在给定服务器轮数内，感知机错误累积数的有限时域期望界：延迟的影响仅通过平均强制执行陈旧性体现，而通信噪声贡献了一个额外项，该项随噪声总能量的平方根增长。在无噪声情况下，展示了有限的期望错误预算如何在温和的新鲜参与条件下产生明确的有限轮稳定界。

Conclusion: 该研究为联邦学习环境中异步感知机训练提供了理论保证，表明通过适当的聚合策略可以控制延迟和噪声的影响，为实际部署中的系统效应提供了理论分析框架。

Abstract: We study a semi-asynchronous client-server perceptron trained via iterative parameter mixing (IPM-style averaging): clients run local perceptron updates and a server forms a global model by aggregating the updates that arrive in each communication round. The setting captures three system effects in federated and distributed deployments: (i) stale updates due to delayed model delivery and delayed application of client computations (two-sided version lag), (ii) partial participation (intermittent client availability), and (iii) imperfect communication on both downlink and uplink, modeled as effective zero-mean additive noise with bounded second moment. We introduce a server-side aggregation rule called staleness-bucket aggregation with padding that deterministically enforces a prescribed staleness profile over update ages without assuming any stochastic model for delays or participation. Under margin separability and bounded data radius, we prove a finite-horizon expected bound on the cumulative weighted number of perceptron mistakes over a given number of server rounds: the impact of delay appears only through the mean enforced staleness, whereas communication noise contributes an additional term that grows on the order of the square root of the horizon with the total noise energy. In the noiseless case, we show how a finite expected mistake budget yields an explicit finite-round stabilization bound under a mild fresh-participation condition.

</details>


### [154] [High-accuracy and dimension-free sampling with diffusions](https://arxiv.org/abs/2601.10708)
*Khashayar Gatmiry,Sitan Chen,Adil Salim*

Main category: cs.LG

TL;DR: 提出一种新的扩散模型求解器，将迭代复杂度从多项式降低到多对数级，实现首个仅需近似访问数据分布分数的高精度采样保证


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型求解需要大量小步长迭代才能生成高质量样本，迭代复杂度随维度和精度呈多项式增长，效率较低

Method: 结合低阶近似和配置法（Lee, Song, Vempala 2018）设计新求解器，利用数据分布分数的近似访问

Result: 新求解器的迭代复杂度在1/ε上呈多对数级，不显式依赖环境维度，仅通过目标分布支撑集的有效半径影响复杂度

Conclusion: 首次实现了仅需近似访问数据分布分数的高精度扩散采样器，显著提高了采样效率

Abstract: Diffusion models have shown remarkable empirical success in sampling from rich multi-modal distributions. Their inference relies on numerically solving a certain differential equation. This differential equation cannot be solved in closed form, and its resolution via discretization typically requires many small iterations to produce \emph{high-quality} samples.
  More precisely, prior works have shown that the iteration complexity of discretization methods for diffusion models scales polynomially in the ambient dimension and the inverse accuracy $1/\varepsilon$. In this work, we propose a new solver for diffusion models relying on a subtle interplay between low-degree approximation and the collocation method (Lee, Song, Vempala 2018), and we prove that its iteration complexity scales \emph{polylogarithmically} in $1/\varepsilon$, yielding the first ``high-accuracy'' guarantee for a diffusion-based sampler that only uses (approximate) access to the scores of the data distribution. In addition, our bound does not depend explicitly on the ambient dimension; more precisely, the dimension affects the complexity of our solver through the \emph{effective radius} of the support of the target distribution only.

</details>


### [155] [DInf-Grid: A Neural Differential Equation Solver with Differentiable Feature Grids](https://arxiv.org/abs/2601.10715)
*Navami Kairanda,Shanthika Naik,Marc Habermann,Avinash Sharma,Christian Theobalt,Vladislav Golyanik*

Main category: cs.LG

TL;DR: 提出DInf-Grid：一种基于可微分网格的表示方法，结合特征网格效率和径向基函数插值，用于快速求解微分方程，相比坐标MLP方法提速5-20倍。


<details>
  <summary>Details</summary>
Motivation: 现有神经求解器存在效率问题：坐标MLP方法计算密集且训练慢，而基于线性插值的网格方法（如Instant-NGP）虽然训练快但无法计算高阶导数，不适合求解微分方程。

Method: 结合特征网格效率与径向基函数插值（无限可微），引入多分辨率分解和共位网格来捕捉高频解并稳定计算全局梯度，使用微分方程作为损失函数进行隐式训练。

Result: 在泊松方程图像重建、亥姆霍兹方程波场、基尔霍夫-洛夫边界值问题布料模拟等任务上验证，相比坐标MLP方法实现5-20倍加速，在秒或分钟级别求解微分方程，同时保持精度和紧凑性。

Conclusion: DInf-Grid通过网格效率和无限可微插值的结合，为微分方程求解提供了高效准确的解决方案，显著优于现有神经求解器方法。

Abstract: We present a novel differentiable grid-based representation for efficiently solving differential equations (DEs). Widely used architectures for neural solvers, such as sinusoidal neural networks, are coordinate-based MLPs that are both computationally intensive and slow to train. Although grid-based alternatives for implicit representations (e.g., Instant-NGP and K-Planes) train faster by exploiting signal structure, their reliance on linear interpolation restricts their ability to compute higher-order derivatives, rendering them unsuitable for solving DEs. Our approach overcomes these limitations by combining the efficiency of feature grids with radial basis function interpolation, which is infinitely differentiable. To effectively capture high-frequency solutions and enable stable and faster computation of global gradients, we introduce a multi-resolution decomposition with co-located grids. Our proposed representation, DInf-Grid, is trained implicitly using the differential equations as loss functions, enabling accurate modelling of physical fields. We validate DInf-Grid on a variety of tasks, including the Poisson equation for image reconstruction, the Helmholtz equation for wave fields, and the Kirchhoff-Love boundary value problem for cloth simulation. Our results demonstrate a 5-20x speed-up over coordinate-based MLP-based methods, solving differential equations in seconds or minutes while maintaining comparable accuracy and compactness.

</details>
