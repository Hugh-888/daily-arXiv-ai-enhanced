{"id": "2601.04898", "categories": ["physics.comp-ph", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2601.04898", "abs": "https://arxiv.org/abs/2601.04898", "authors": ["Ao Zhou", "Salma Zahran", "Chi Chen", "Zhengyang Zhang", "Yanming Wang"], "title": "A joint voxel flow - phase field framework for ultra-long microstructure evolution prediction with physical regularization", "comment": "33 pages, 7 figures.Submitted waiting for review", "summary": "Phase-field (PF) modeling is a powerful tool for simulating microstructure evolution. To overcome the high computational cost of PF in solving complex PDEs, machine learning methods such as PINNs, convLSTM have been used to predict PF evolution. However, current methods still face shortages of low flexibility, poor generalization and short predicting time length. In this work, we present a joint framework coupling voxel-flow network (VFN) with PF simulations in an alternating manner for long-horizon temporal prediction of microstructure evolution. The VFN iteratively predicts future evolution by learning the flow of pixels from past snapshots, with periodic boundaries preserved in the process. Periodical PF simulations suppresses nonphysical artifacts, reduces accumulated error, and extends reliable prediction time length. The VFN is about 1,000 times faster than PF simulation on GPU. In validation using grain growth and spinodal decomposition, MSE and SSIM remain 6.76% and 0.911 when predicted 18 frames from only 2 input frames, outperforming similar predicting methods. For an ultra-long grain growth prediction for 82 frames from 2 input frames, grain number decreases from 600 to 29 with NMSE of average grain area remaining 1.64%. This joint framework enables rapid, generalized, flexible and physically consistent microstructure forecasting from image-based data for ultra-long time scales."}
{"id": "2601.05161", "categories": ["quant-ph", "cond-mat.mtrl-sci", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.05161", "abs": "https://arxiv.org/abs/2601.05161", "authors": ["Ioannis Kolotouros", "Adithya Sireesh", "Stuart Ferguson", "Sean Thrasher", "Petros Wallden", "Julien Michel"], "title": "Quantum Elastic Network Models and their Application to Graphene", "comment": "42 pages, 11 figures", "summary": "Molecular dynamics simulations are a central computational methodology in materials design for relating atomic composition to mechanical properties. However, simulating materials with atomic-level resolution on a macroscopic scale is infeasible on current classical hardware, even when using the simplest elastic network models (ENMs) that represent molecular vibrations as a network of coupled oscillators. To address this issue, we introduce Quantum Elastic Network Models (QENMs) and utilize the quantum algorithm of Babbush et al. (PRX, 2023), which offers an exponential advantage when simulating systems of coupled oscillators under some specific conditions and assumptions. Here, we demonstrate how our method enables the efficient simulation of planar materials. As an example, we apply our algorithm to the task of simulating a 2D graphene sheet. We analyze the exact complexity for initial-state preparation, Hamiltonian simulation, and measurement of this material, and provide two real-world applications: heat transfer and the out-of-plane rippling effect. We estimate that an atomistic simulation of a graphene sheet on the centimeter scale, classically requiring hundreds of petabytes of memory and prohibitive runtimes, could be encoded and simulated with as few as $\\sim 160$ logical qubits."}
{"id": "2601.04290", "categories": ["gr-qc", "astro-ph.CO", "astro-ph.GA"], "pdf": "https://arxiv.org/pdf/2601.04290", "abs": "https://arxiv.org/abs/2601.04290", "authors": ["Tejinder P. Singh"], "title": "A Relativistic MOND", "comment": "4 pages; prepared for the proceedings of the 22nd Lomonosov conference, Moscow, August 21-27, 2025", "summary": "We present a minimal relativistic completion of MOND in which (i) General Relativity is recovered exactly in the high-acceleration regime, while (ii) the Bekenstein--Milgrom (AQUAL) equation emerges in the low-acceleration regime, without introducing additional propagating fields beyond those already present in a right-handed gauge sector. The construction is motivated by an $E_6\\times E_6$ framework in which $SU(3)_R\\rightarrow SU(2)_R\\times U(1)_{Y'}\\rightarrow U(1)_{\\rm dem}$, leaving a healthy repulsive $U(1)_{\\rm dem}$ interaction whose charge is the square-root mass label. Gravity itself arises from the $SU(2)_R$ connection via a Plebanski/MacDowell--Mansouri mechanism, yielding an emergent tetrad and the Einstein--Hilbert action. MOND is implemented by an infrared (IR) metric deformation $ΔS_{\\rm IR}[g]$ that is UV-vanishing (so GR is recovered) while its deep-MOND/static limit is fixed by a symmetry principle: in three spatial dimensions, the deep-MOND action is conformally invariant with a 10-parameter group isomorphic to $SO(4,1)$ (the de Sitter group). The single MOND acceleration scale is set by a de Sitter radius selected dynamically in the IR, $a_0=c^2/(ξ\\,\\ell_{\\rm dS})$ with $ξ={ O}(1)$ fixed by matching to the static limit. MOND resides in perturbations and quasistatic systems; the homogeneous FRW background is controlled by the IR vacuum kinematics rather than an ad hoc cosmological constant."}
{"id": "2601.04305", "categories": ["quant-ph", "cond-mat.quant-gas", "cond-mat.stat-mech", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2601.04305", "abs": "https://arxiv.org/abs/2601.04305", "authors": ["Umberto Borla", "Achilleas Lazarides", "Christian Groß", "Jad C. Halimeh"], "title": "Microscopic Dynamics of False Vacuum Decay in the $2+1$D Quantum Ising Model", "comment": "$9+3$ pages, $5+3$ figures", "summary": "False vacuum decay, which is understood to happen through bubble nucleation, is a prominent phenomenon relevant to elementary particle physics and early-universe cosmology. Understanding its microscopic dynamics in higher spatial dimensions is currently a major challenge and research thrust. Recent advances in numerical techniques allow for the extraction of related signatures in tractable systems in two spatial dimensions over intermediate timescales. Here, we focus on the $2+1$D quantum Ising model, where a longitudinal field is used to energetically separate the two $\\mathbb{Z}_2$ symmetry-broken ferromagnetic ground states, turning them into a ``true'' and ``false'' vacuum. Using tree tensor networks, we simulate the microscopic dynamics of a spin-down domain in a spin-up background after a homogeneous quench, with parameters chosen so that the domain corresponds to a bubble of the true vacuum in a false-vacuum background. Our study identifies how the ultimate fate of the bubble -- indefinite expansion or collapse -- depends on its geometrical features and on the microscopic parameters of the Ising Hamiltonian. We further provide a realistic quantum-simulation scheme, aimed at probing bubble dynamics on atomic Rydberg arrays."}
{"id": "2601.04199", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04199", "abs": "https://arxiv.org/abs/2601.04199", "authors": ["Jiale Zhao", "Xing Mou", "Jinlin Wu", "Hongyuan Yu", "Mingrui Sun", "Yang Shi", "Xuanwu Yin", "Zhen Chen", "Zhen Lei", "Yaohua Wang"], "title": "The Forgotten Shield: Safety Grafting in Parameter-Space for Medical MLLMs", "comment": null, "summary": "Medical Multimodal Large Language Models (Medical MLLMs) have achieved remarkable progress in specialized medical tasks; however, research into their safety has lagged, posing potential risks for real-world deployment. In this paper, we first establish a multidimensional evaluation framework to systematically benchmark the safety of current SOTA Medical MLLMs. Our empirical analysis reveals pervasive vulnerabilities across both general and medical-specific safety dimensions in existing models, particularly highlighting their fragility against cross-modality jailbreak attacks. Furthermore, we find that the medical fine-tuning process frequently induces catastrophic forgetting of the model's original safety alignment. To address this challenge, we propose a novel \"Parameter-Space Intervention\" approach for efficient safety re-alignment. This method extracts intrinsic safety knowledge representations from original base models and concurrently injects them into the target model during the construction of medical capabilities. Additionally, we design a fine-grained parameter search algorithm to achieve an optimal trade-off between safety and medical performance. Experimental results demonstrate that our approach significantly bolsters the safety guardrails of Medical MLLMs without relying on additional domain-specific safety data, while minimizing degradation to core medical performance."}
{"id": "2601.04379", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2601.04379", "abs": "https://arxiv.org/abs/2601.04379", "authors": ["George Ruppeiner"], "title": "Could black hole thermodynamics play a role in black hole mergers?", "comment": "13 pages, 9 figures", "summary": "Gravitational waves from binary black hole mergers yield values for both the black hole remnant mass $M$ and it's spin $a$, with the $169$ $a$ values collected so far crowding significantly around their average $\\bar{a}=0.6869\\pm 0.087$. Could this crowding relate directly to the Davies phase transition point at $a=0.68125$ from black hole thermodynamics? I argue that a necessary challenge for such a connection requires a consistent application of the thermodynamic fluctuation theory that follows from black hole thermodynamics (BHT). Specifically, necessary are a correct choice of fluctuating variables, as well as thermal equilibrium between the event horizon at the Hawking temperature $\\sim μK$ and the outside universe $\\sim 3 K$. I show that the former requirement follows in straightforward fashion from the BHT of the Kerr model, while the later requires an accretion disk following the Novikov-Thorne accretion disk model. I construct a thermodynamic fluctuation theory meeting both these requirements. My results open the possibility that black hole mergers are based on some dynamical model (not known to me) with a limiting attractor state at the Davies point."}
{"id": "2601.04313", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04313", "abs": "https://arxiv.org/abs/2601.04313", "authors": ["Robert Ott", "Torsten V. Zache", "Soonwon Choi", "Adam M. Kaufman", "Hannes Pichler"], "title": "Rare-Event Quantum Sensing using Logical Qubits", "comment": "12 pages, 4 figures", "summary": "We present a novel protocol to detect rare signals in a noisy environment using quantum error correction (QEC). The key feature of our protocol is the discrimination between signal and noise through distinct higher-order correlations, realized by the non-linear processing that occurs during syndrome extraction in QEC. In this scheme, QEC has two effects: First, it sacrifices part of the signal $ε$ by recording a reduced, stochastic, logical phase $φ_L = \\mathcal{O}(ε^3)$. Second, it corrects the physical noise and extends the (logical) coherence time for signal acquisition. For rare signals occurring at random times in the presence of local Markovian noise, we explicitly demonstrate an improved sensitivity of our approach over more conventional sensing strategies."}
{"id": "2601.04250", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04250", "abs": "https://arxiv.org/abs/2601.04250", "authors": ["Mustapha Hamdi", "Mourad Jabou"], "title": "Green MLOps: Closed-Loop, Energy-Aware Inference with NVIDIA Triton, FastAPI, and Bio-Inspired Thresholding", "comment": "6 pages, 4 figures. Code available at:https://github.com/InnoDeep-repos/Green_MLOps", "summary": "Energy efficiency is a first-order concern in AI deployment, as long-running inference can exceed training in cumulative carbon impact. We propose a bio-inspired framework that maps protein-folding energy basins to inference cost landscapes and controls execution via a decaying, closed-loop threshold. A request is admitted only when the expected utility-to-energy trade-off is favorable (high confidence/utility at low marginal energy and congestion), biasing operation toward the first acceptable local basin rather than pursuing costly global minima. We evaluate DistilBERT and ResNet-18 served through FastAPI with ONNX Runtime and NVIDIA Triton on an RTX 4000 Ada GPU. Our ablation study reveals that the bio-controller reduces processing time by 42% compared to standard open-loop execution (0.50s vs 0.29s on A100 test set), with a minimal accuracy degradation (<0.5%). Furthermore, we establish the efficiency boundaries between lightweight local serving (ORT) and managed batching (Triton). The results connect biophysical energy models to Green MLOps and offer a practical, auditable basis for closed-loop energy-aware inference in production."}
{"id": "2601.04414", "categories": ["gr-qc", "astro-ph.CO", "hep-th"], "pdf": "https://arxiv.org/pdf/2601.04414", "abs": "https://arxiv.org/abs/2601.04414", "authors": ["Ayush Bidlan", "Paulo Moniz", "Oem Trivedi"], "title": "Future Rip Scenarios in Fractional Holographic Dark Energy", "comment": "Contains 12 figures and spans 13 pages; prepared for submission to EPJC", "summary": "In this paper, we investigate the occurrence of late-time cosmological singularities, namely, the rip scenarios within the framework of interacting Fractional Holographic Dark Energy (FHDE). We start our investigation with the Granda-Oliveros (GO) cutoff, i.e., $L=(γH^{2}+δ\\dot{H})^{-\\frac{1}{2}}$, and highlight the range of allowed $α$ (Lévy's index) values for which big, little and pseudo rip can occur. In particular, we highlight the occurrence of a big rip for fractional values of the Lévy's index in the allowed range $1<α\\leq2$. Moreover, we conclude that the occurrence of a pseudo-rip requires Lévy's index to be $α>2$. Therefore, we reject the possibility of pseudo-rip within the GO cutoff. Furthermore, we demonstrate that the occurrence of the little rip in FHDE equipped with a GO cutoff is rather contrived and requires a specific functional form of the IR cutoff $L\\sim(γH^{2}+g(H))^{-\\frac{1}{2}}$, which belongs to a larger class of Nojiri-Odintsov (NO) cutoffs. To extend our perspective beyond the GO cutoff, we investigate the interacting FHDE framework equipped with the Hubble cutoff, i.e., $L=H^{-1}$, in developing an ansatz-based approach to the little and pseudo-rip singularities as they fail to appear in the GO cutoff. Within this approach, we invoke the expression of the Hubble parameter, $H(t)$, which corresponds to the little and pseudo-rip, into the cosmological parameters such as the Equation of State (EoS) and Squared Sound Speed (SSS) as a function of cosmic time $t$. We produce numerical plots of these parameters in both linear and non-linear $Q$ regimes, which supplement our theoretical findings. In summary, our results highlight the occurrence of little and pseudo-rip singularities within a Hubble cutoff for a non-linear $Q$ term within the FHDE framework."}
{"id": "2601.04364", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2601.04364", "abs": "https://arxiv.org/abs/2601.04364", "authors": ["Yinan Chen", "Sara Murciano", "Pablo Sala", "Jason Alicea"], "title": "Quantum sensing with critical systems: impact of symmetry, imperfections, and decoherence", "comment": null, "summary": "Entangled many-body states enable high-precision quantum sensing beyond the standard quantum limit. We develop interferometric sensing protocols based on quantum critical wavefunctions and compare their performance with Greenberger-Horne-Zeilinger (GHZ) and spin-squeezed states. Building on the idea of symmetries as a metrological resource, we introduce a symmetry-based algorithm to identify optimal measurement strategies. We illustrate this algorithm both for magnetic systems with internal symmetries and Rydberg-atom arrays with spatial symmetries. We study the robustness of criticality for quantum sensing under non-unitary deformations, symmetry-preserving and symmetry-breaking decoherence, and qubit loss -- identifying regimes where critical systems outperform GHZ states and showing that non-unitary deformation can even enhance sensing precision. Combined with recent results on log-depth preparation of critical wavefunctions, interferometric sensing in this setting appears increasingly promising."}
{"id": "2601.04262", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04262", "abs": "https://arxiv.org/abs/2601.04262", "authors": ["Wang Cai", "Yilin Wen", "Jinchang Hou", "Du Su", "Guoqiu Wang", "Zhonghou Lv", "Chenfu Bao", "Yunfang Wu"], "title": "Safety-Utility Conflicts Are Not Global: Surgical Alignment via Head-Level Diagnosis", "comment": null, "summary": "Safety alignment in Large Language Models (LLMs) inherently presents a multi-objective optimization conflict, often accompanied by an unintended degradation of general capabilities. Existing mitigation strategies typically rely on global gradient geometry to resolve these conflicts, yet they overlook Modular Heterogeneity within Transformers, specifically that the functional sensitivity and degree of conflict vary substantially across different attention heads. Such global approaches impose uniform update rules across all parameters, often resulting in suboptimal trade-offs by indiscriminately updating utility sensitive heads that exhibit intense gradient conflicts. To address this limitation, we propose Conflict-Aware Sparse Tuning (CAST), a framework that integrates head-level diagnosis with sparse fine-tuning. CAST first constructs a pre-alignment conflict map by synthesizing Optimization Conflict and Functional Sensitivity, which then guides the selective update of parameters. Experiments reveal that alignment conflicts in LLMs are not uniformly distributed. We find that the drop in general capabilities mainly comes from updating a small group of ``high-conflict'' heads. By simply skipping these heads during training, we significantly reduce this loss without compromising safety, offering an interpretable and parameter-efficient approach to improving the safety-utility trade-off."}
{"id": "2601.04639", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2601.04639", "abs": "https://arxiv.org/abs/2601.04639", "authors": ["Carlos E. Romero-Figueroa", "Hernando Quevedo"], "title": "Quasi-Homogeneous Thermodynamics and Microscopic Structure of the Quantum-Corrected FLRW Universe", "comment": null, "summary": "The analysis of phase transitions in cosmological spacetimes shows that their existence requires a time-dependent apparent horizon radius, which in turn implies an equation of state different from that of a dark energy fluid. This condition is not compatible with the simultaneous fulfillment of Hayward's unified gravitational first law and the fundamental thermodynamic equation of the apparent horizon. To solve this problem, we introduce an alternative formulation in which the cosmological horizon is modeled as a quasi-homogeneous thermodynamic system. We apply this approach to the Friedmann-Lemaître-Robertson-Walker (FLRW) universe under quantum gravity corrections encoded by the Generalized Uncertainty Principle (GUP), promote the deformation parameter to a thermodynamic variable, and obtain a consistent thermodynamic description without relying on the usual pressure-volume interpretation. Using Geometrothermodynamics (GTD), we show that fluctuations of the GUP parameter can induce phase transitions closely resembling those of black hole configurations. Finally, we perform a numerical analysis of the behavior of the GTD scalar curvature near the phase transition point, where we find a scaling behavior characterized by the critical exponent close to 1, independently of the dimension of the equilibrium space. This reveals that quantum gravity corrections not only modify the thermodynamic consistency of cosmological models but also strengthen the notion of thermodynamic universality across gravitational systems. Our findings confirm GTD as a powerful geometric tool to unveil the emergent thermodynamic microstructure of spacetime."}
{"id": "2601.04372", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04372", "abs": "https://arxiv.org/abs/2601.04372", "authors": ["Nikolaos Cheimarios"], "title": "Solving nonlinear PDEs with Quantum Neural Networks: A variational approach to the Bratu Equation", "comment": null, "summary": "We present a variational quantum algorithm (VQA) to solve the nonlinear one-dimensional Bratu equation. By formulating the boundary value problem within a variational framework and encoding the solution in a parameterized quantum neural network (QNN), the problem reduces to an optimization task over quantum circuit parameters. The trial solution incorporates both classical approximations and boundary-enforcing terms, allowing the circuit to focus on minimizing the residual of the differential operator. Using a noiseless quantum simulator, we demonstrate that the method accurately captures both solution branches of the Bratu equation and shows excellent agreement with classical pseudo arc-length continuation results."}
{"id": "2601.04263", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04263", "abs": "https://arxiv.org/abs/2601.04263", "authors": ["Nilushika Udayangani Hewa Dehigahawattage", "Kishor Nandakishor", "Marimuthu Palaniswami"], "title": "Learning to Reason: Temporal Saliency Distillation for Interpretable Knowledge Transfer", "comment": "In Proceedings of the 27th European Conference on Artificial Intelligence (ECAI 2025), IOS Press", "summary": "Knowledge distillation has proven effective for model compression by transferring knowledge from a larger network called the teacher to a smaller network called the student. Current knowledge distillation in time series is predominantly based on logit and feature aligning techniques originally developed for computer vision tasks. These methods do not explicitly account for temporal data and fall short in two key aspects. First, the mechanisms by which the transferred knowledge helps the student model learning process remain unclear due to uninterpretability of logits and features. Second, these methods transfer only limited knowledge, primarily replicating the teacher predictive accuracy. As a result, student models often produce predictive distributions that differ significantly from those of their teachers, hindering their safe substitution for teacher models. In this work, we propose transferring interpretable knowledge by extending conventional logit transfer to convey not just the right prediction but also the right reasoning of the teacher. Specifically, we induce other useful knowledge from the teacher logits termed temporal saliency which captures the importance of each input timestep to the teacher prediction. By training the student with Temporal Saliency Distillation we encourage it to make predictions based on the same input features as the teacher. Temporal Saliency Distillation requires no additional parameters or architecture specific assumptions. We demonstrate that Temporal Saliency Distillation effectively improves the performance of baseline methods while also achieving desirable properties beyond predictive accuracy. We hope our work establishes a new paradigm for interpretable knowledge distillation in time series analysis."}
{"id": "2601.04691", "categories": ["gr-qc", "hep-ph", "hep-th"], "pdf": "https://arxiv.org/pdf/2601.04691", "abs": "https://arxiv.org/abs/2601.04691", "authors": ["Aarav Shah", "Paulo Moniz", "Maxim Khlopov", "Oem Trivedi", "Maxim Krasnov"], "title": "Inflationary Dynamics and Perturbations in Fractal Cosmology", "comment": "This manuscript has 18 pages and 2 figures. Comments and suggestions from readers are welcome", "summary": "We study inflationary dynamics within the framework of fractal cosmology, where spacetime exhibits a non-integer effective dimension, sourced through a relaxation of the cosmological principle. Using Friedmann and continuity equations, modified by an effective fractal dimension $D$; we derive generalized slow-roll parameters and examine their evolution for cubic, Starobinsky and Natural inflationary potentials. We then formulate a fractal extension of the Mukhanov-Sasaki equation by introducing an effective momentum term $k_{\\text{eff}}$, arising from the fractal decomposition of the spatial Laplacian, that captures the geometric influence of fractal cosmology on scalar perturbations. This leads to corrections in the power spectrum and a scalar spectral index $n_s$ that depends explicitly on both the fractal dimension $D$ and a fractional scale $L$, which controls the strength of the fractal deformation. Comparison with the Planck 2018 data ($n_s=0.9649\\pm 0.0042$) constrains the allowed range of $D$ ($2.7\\lesssim D\\lesssim3$) depending on the cosmological and inflationary model assumed."}
{"id": "2601.04402", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04402", "abs": "https://arxiv.org/abs/2601.04402", "authors": ["Emery Doucet", "Zakaria Mzaouali", "Reece Robertson", "Bartłomiej Gardas", "Sebastian Deffner", "Krzysztof Domino"], "title": "Thermodynamic significance of QUBO encoding on quantum annealers", "comment": "17 pages, 8 figures", "summary": "Quadratic unconstrained binary optimization (QUBO) is the standard interface to quantum annealers, yet a single constrained task admits many QUBO encodings whose penalty choices reshape the energy landscape experienced by hardware. We study a Job Shop Scheduling instance using a two-parameter family of encodings controlled by penalty weights $p_{\\rm sum}$ (one-hot/sum constraints) and $p_{\\rm pair}$ (precedence constraints). Sweeping $(p_{\\rm sum},p_{\\rm pair})$, we observe sharp transitions in feasibility and solver success across classical annealing-inspired heuristics and on a D-Wave Advantage processor. Going beyond solution probability, we treat the annealer as an open thermodynamic system and perform cyclic reverse-annealing experiments initialized from thermal samples, measuring the stochastic processor energy change. From the first two moments of this energy change we infer lower bounds on entropy production, work, and exchanged heat via thermodynamic uncertainty relations, and corroborate the observed trends with adiabatic master equation simulations. We find that the same encoding transitions that govern computational hardness also reorganize dissipation: weak penalties generate low-energy infeasible manifolds, while overly strong penalties suppress the effective problem energy scale and increase irreversibility, reducing the thermodynamic efficiency. Our results establish QUBO penalties as thermodynamic control knobs and motivate thermodynamics-aware encoding strategies for noisy intermediate-scale quantum annealers."}
{"id": "2601.04264", "categories": ["cs.LG", "cs.CE"], "pdf": "https://arxiv.org/pdf/2601.04264", "abs": "https://arxiv.org/abs/2601.04264", "authors": ["Nilushika Udayangani", "Kishor Nandakishor", "Marimuthu Palaniswami"], "title": "MemKD: Memory-Discrepancy Knowledge Distillation for Efficient Time Series Classification", "comment": "In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2025), Hyderabad, India", "summary": "Deep learning models, particularly recurrent neural networks and their variants, such as long short-term memory, have significantly advanced time series data analysis. These models capture complex, sequential patterns in time series, enabling real-time assessments. However, their high computational complexity and large model sizes pose challenges for deployment in resource-constrained environments, such as wearable devices and edge computing platforms. Knowledge Distillation (KD) offers a solution by transferring knowledge from a large, complex model (teacher) to a smaller, more efficient model (student), thereby retaining high performance while reducing computational demands. Current KD methods, originally designed for computer vision tasks, neglect the unique temporal dependencies and memory retention characteristics of time series models. To this end, we propose a novel KD framework termed Memory-Discrepancy Knowledge Distillation (MemKD). MemKD leverages a specialized loss function to capture memory retention discrepancies between the teacher and student models across subsequences within time series data, ensuring that the student model effectively mimics the teacher model's behaviour. This approach facilitates the development of compact, high-performing recurrent neural networks suitable for real-time, time series analysis tasks. Our extensive experiments demonstrate that MemKD significantly outperforms state-of-the-art KD methods. It reduces parameter size and memory usage by approximately 500 times while maintaining comparable performance to the teacher model."}
{"id": "2601.04892", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2601.04892", "abs": "https://arxiv.org/abs/2601.04892", "authors": ["T. Torres", "S. R. Dolan"], "title": "Dynamical system approach to the spectral (in)stability of black holes under localised potential perturbations", "comment": "21 pages, 9 figures", "summary": "The aim of this work is to improve understanding of the resonant spectra of black holes under perturbations arising from e.g. compact objects or accretion disks in their vicinity. It is known that adding a weak perturbation to the radial potential can strongly disrupt the spectrum of quasinormal modes and Regge poles of a black hole spacetime. Here we examine the effect of (weak or strong) localised delta-function perturbations on the resonant spectra of spherically-symmetric systems, to address fundamental questions around linear and non-linear spectral stability. We examine two cases: the Nariai spacetime with a Poschl-Teller potential and the Schwarzschild spacetime. We show that, in either case, the spectrum deforms in a smooth and continuous manner as the position and strength of the perturbation is varied. As the strength of the perturbation is increased, resonances migrate along trajectories in the complex plane which ultimately tend towards attracting points determined by a hard-wall scenario. However, for weak perturbations the trajectory near the unperturbed resonance is typically strongly influenced by a set of repelling points which, for perturbations far from the system, lie very close to the unperturbed resonances; hence there arises a non-linear instability (i.e. the failure of a linearised approximation). Taking a dynamical systems perspective, the sets of attracting and repelling spectral points follow their own trajectories as the position of the perturbation is varied, and these are tracked and understood."}
{"id": "2601.04407", "categories": ["quant-ph", "cond-mat.mes-hall", "math-ph"], "pdf": "https://arxiv.org/pdf/2601.04407", "abs": "https://arxiv.org/abs/2601.04407", "authors": ["Mustafa Bakr", "Robin Wopalenski"], "title": "Exact Multimode Quantization of Superconducting Circuits via Boundary Admittance", "comment": null, "summary": "We show that the Schur complement of the nodal admittance matrix, which reduces a multiport electromagnetic environment to the driving-point admittance $Y_{\\mathrm{in}}(s)$ at the Josephson junction, naturally leads to an eigenvalue-dependent boundary condition determining the dressed mode spectrum. This identification provides a four-step quantization procedure: (i) compute or measure $Y_{\\mathrm{in}}(s)$, (ii) solve the boundary condition $sY_{\\mathrm{in}}(s) + 1/L_J = 0$ for dressed frequencies, (iii) synthesize an equivalent passive network, (iv) quantize with the full cosine nonlinearity retained. Within passive lumped-element circuit theory, we prove that junction participation decays as, we prove that junction participation decays as $O(ω_n^{-1})$ at high frequencies when the junction port has finite shunt capacitance, ensuring ultraviolet convergence of perturbative sums without imposed cutoffs. The standard circuit QED parameters, coupling strength $g$, anharmonicity $α$, and dispersive shift $χ$, emerge as controlled limits with explicit validity conditions."}
{"id": "2601.04268", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2601.04268", "abs": "https://arxiv.org/abs/2601.04268", "authors": ["Pritthijit Nath", "Sebastian Schemm", "Henry Moss", "Peter Haynes", "Emily Shuckburgh", "Mark J. Webb"], "title": "Making Tunable Parameters State-Dependent in Weather and Climate Models with Reinforcement Learning", "comment": "66 pages, 22 figures", "summary": "Weather and climate models rely on parametrisations to represent unresolved sub-grid processes. Traditional schemes rely on fixed coefficients that are weakly constrained and tuned offline, contributing to persistent biases that limit their ability to adapt to the underlying physics. This study presents a framework that learns components of parametrisation schemes online as a function of the evolving model state using reinforcement learning (RL) and evaluates the resulting RL-driven parameter updates across a hierarchy of idealised testbeds spanning a simple climate bias correction (SCBC), a radiative-convective equilibrium (RCE), and a zonal mean energy balance model (EBM) with both single-agent and federated multi-agent settings. Across nine RL algorithms, Truncated Quantile Critics (TQC), Deep Deterministic Policy Gradient (DDPG), and Twin Delayed DDPG (TD3) achieved the highest skill and the most stable convergence across configurations, with performance assessed against a static baseline using area-weighted RMSE, temperature profile and pressure-level diagnostics. For the EBM, single-agent RL outperformed static parameter tuning with the strongest gains in tropical and mid-latitude bands, while federated RL on multi-agent setups enabled geographically specialised control and faster convergence, with a six-agent DDPG configuration using frequent aggregation yielding the lowest area-weighted RMSE across the tropics and mid-latitudes. The learnt corrections were also physically meaningful as agents modulated EBM radiative parameters to reduce meridional biases, adjusted RCE lapse rates to match vertical temperature errors, and stabilised SCBC heating increments to limit drift. Overall, results highlight RL to deliver skilful state-dependent, and regime-aware parametrisations, offering a scalable pathway for online learning within numerical models."}
{"id": "2601.04961", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2601.04961", "abs": "https://arxiv.org/abs/2601.04961", "authors": ["Erick Aguiar", "A. A. Araújo Filho", "Valdir B. Bezerra", "Gilson A. Ferreira", "Iarley P. Lobo"], "title": "Fermi Acceleration Mechanisms Beyond Lorentz Symmetry", "comment": "19+2 pages, 13 figures", "summary": "We construct models for first- and second-order Fermi acceleration of particles, incorporating generic frame transformations, dispersion relations, and conservation laws. Within this framework, we study deformations of Lorentz symmetry via the $κ$-Poincaré algebra in the bicrossproduct and classical bases, which respectively deform and preserve the relativistic dispersion relation. We also examine explicit Lorentz symmetry violation and compare the results with deformed relativity and special relativity."}
{"id": "2601.04422", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04422", "abs": "https://arxiv.org/abs/2601.04422", "authors": ["Aaron C. Hoyt", "Jonathan S. Bersson", "Sean Garner", "Chenxu Liu", "Ang Li"], "title": "Implementation of Tensor Network Simulation TN-Sim under NWQ-Sim", "comment": null, "summary": "Large-scale tensor network simulations are crucial for developing robust complexity-theoretic bounds on classical quantum simulation, enabling circuit cutting approaches, and optimizing circuit compilation, all of which aid efficient quantum computation on limited quantum resources. Modern exascale high-performance computing platforms offer significant potential for advancing tensor network quantum circuit simulation capabilities. We implement TN-Sim, a tensor network simulator backend within the NWQ-Sim software package that utilizes the Tensor Algebra for Many-body Methods (TAMM) framework to support both distributed HPC-scale computations and local simulations with ITensor. To optimize the scale up in computation across multiple nodes we implement a task based parallelization scheme to demonstrate parallelized gate contraction for wide quantum circuits with many gates per layer. Through the integration of the TAMM framework with Matrix Product State (MPS) tensor network approaches, we deliver a simulation environment that can scale from local systems to HPC clusters. We demonstrate an MPS tensor network simulator running on the state-of-the-art Perlmutter (NVIDIA) supercomputer and discuss the potential portability of this software to HPC clusters such as Frontier (AMD) and Aurora (Intel). We also discuss future improvements including support for different tensor network topologies and enhanced computational efficiency."}
{"id": "2601.04270", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04270", "abs": "https://arxiv.org/abs/2601.04270", "authors": ["Anherutowa Calvo"], "title": "Predictable Gradient Manifolds in Deep Learning: Temporal Path-Length and Intrinsic Rank as a Complexity Regime", "comment": "12 Pages. Preprint", "summary": "Deep learning optimization exhibits structure that is not captured by worst-case gradient bounds. Empirically, gradients along training trajectories are often temporally predictable and evolve within a low-dimensional subspace. In this work we formalize this observation through a measurable framework for predictable gradient manifolds.\n  We introduce two computable quantities: a prediction-based path length that measures how well gradients can be forecast from past information, and a predictable rank that quantifies the intrinsic temporal dimension of gradient increments. We show how classical online and nonconvex optimization guarantees can be restated so that convergence and regret depend explicitly on these quantities, rather than on worst-case variation.\n  Across convolutional networks, vision transformers, language models, and synthetic control tasks, we find that gradient trajectories are locally predictable and exhibit strong low-rank structure over time. These properties are stable across architectures and optimizers, and can be diagnosed directly from logged gradients using lightweight random projections.\n  Our results provide a unifying lens for understanding optimization dynamics in modern deep learning, reframing standard training as operating in a low-complexity temporal regime. This perspective suggests new directions for adaptive optimizers, rank-aware tracking, and prediction-based algorithm design grounded in measurable properties of real training runs."}
{"id": "2601.05040", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2601.05040", "abs": "https://arxiv.org/abs/2601.05040", "authors": ["Khadije Jafarzade", "Saira Yasmin", "Mubasher Jamil"], "title": "Shadow of F(R)-EH Black Hole and Constraints from EHT Observations", "comment": "17 pages,9 figures", "summary": "This work investigates the optical properties of a static, spherically symmetric, electrically charged black hole in f(R) gravity coupled to Euler-Heisenberg(EH) nonlinear electrodynamics(NLED). By analyzing photon trajectories in this background spacetime, we show how the model parameters affect light propagation, leading to wider ranges of lensed trajectories and photon rings. We identify regions of parameter space that admit physically consistent black hole shadows, characterized by the existence of a photon sphere located outside the event horizon and a shadow formed beyond it. These viable regions expand with increasing electric charge and increasing fR0, illustrating the interplay between gravitational and electromagnetic effects. By constraining the model using Event Horizon Telescope observations of M87*, we find that de Sitter black hole solutions remain compatible with the observational data, whereas anti-de Sitter solutions are disfavored for low electric charge and fR0 > -1. Finally, an analysis of the energy emission rate shows that higher electric charge enhances black hole evaporation, while stronger nonlinear electrodynamics effects and larger values of fR0 suppress it."}
{"id": "2601.04439", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04439", "abs": "https://arxiv.org/abs/2601.04439", "authors": ["Karla Baumann", "Youcef Modheb", "Roman Randrianarisoa", "Roland Katz", "Aoife Boyle", "Frédéric Holweck"], "title": "Solving nonlinear differential equations on noisy $156$-qubit quantum computers", "comment": "12 pages, 8 figues, 3 tables", "summary": "In this paper, we report on the resolution of nonlinear differential equations using IBM's quantum platform. More specifically, we demonstrate that the hybrid classical-quantum algorithm H-DES successfully solves a one-dimensional material deformation problem and the inviscid Burgers' equation on IBM's 156-qubit quantum computers. These results constitute a step toward performing physically relevant simulations on present-day Noisy Intermediate-Scale Quantum (NISQ) devices."}
{"id": "2601.04277", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04277", "abs": "https://arxiv.org/abs/2601.04277", "authors": ["Beier Luo", "Cheng Wang", "Hongxin Wei", "Sharon Li", "Xuefeng Du"], "title": "Unlocking the Pre-Trained Model as a Dual-Alignment Calibrator for Post-Trained LLMs", "comment": null, "summary": "Post-training improves large language models (LLMs) but often worsens confidence calibration, leading to systematic overconfidence. Recent unsupervised post-hoc methods for post-trained LMs (PoLMs) mitigate this by aligning PoLM confidence to that of well-calibrated pre-trained counterparts. However, framing calibration as static output-distribution matching overlooks the inference-time dynamics introduced by post-training. In particular, we show that calibration errors arise from two regimes: (i) confidence drift, where final confidence inflates despite largely consistent intermediate decision processes, and (ii) process drift, where intermediate inference pathways diverge. Guided by this diagnosis, we propose Dual-Align, an unsupervised post-hoc framework for dual alignment in confidence calibration. Dual-Align performs confidence alignment to correct confidence drift via final-distribution matching, and introduces process alignment to address process drift by locating the layer where trajectories diverge and realigning the stability of subsequent inference. This dual strategy learns a single temperature parameter that corrects both drift types without sacrificing post-training performance gains. Experiments show consistent improvements over baselines, reducing calibration errors and approaching a supervised oracle."}
{"id": "2601.05129", "categories": ["gr-qc", "hep-th"], "pdf": "https://arxiv.org/pdf/2601.05129", "abs": "https://arxiv.org/abs/2601.05129", "authors": ["Paul M. Saffin", "Qi-Xin Xie"], "title": "Quantum fields in boson star spacetime", "comment": "38 pages, 12 figures", "summary": "Boson stars have been extensively studied in classical gravity, but their quantum properties remain comparatively unexplored. In this paper, we compute the quantum scalar fields and stress tensor in boson star spacetimes within the framework of semiclassical gravity. Divergences are regularized using Pauli-Villars fields, and accurate numerical results are obtained through spectral methods. Employing coherent states enables a direct comparison between the classical part of the stress tensor and the quantum fluctuation. Our results indicate that strong spacetime curvature is the primary source of large quantum effects. The renormalized quantum energy density is mostly positive but the radial pressure is negative, suggesting that classical boson star solutions require modification once quantum effects are included. Moreover, in regimes of large curvature, the quantum fluctuations can constitute a significant fraction of the total stress tensor. The methods developed here can be generalized to other compact objects and used to study their response to quantum corrections."}
{"id": "2601.04440", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04440", "abs": "https://arxiv.org/abs/2601.04440", "authors": ["Sayan Gangopadhyay", "Sasan V. Grayli", "Sathursan Kokilathasan", "Michael E. Reimer"], "title": "A Broadband Nanowire Quantum Dot Cavity Design for the Efficient Extraction of Entangled Photons", "comment": null, "summary": "A bright source of on-demand entangled photons is needed for quantum networks. A single quantum dot in a site-selected nanowire waveguide is a promising candidate for realizing such sources. However, such sources are associated with poor single-photon indistinguishability, limiting their applicability in quantum networks. A common approach for enhancing the single-photon indistinguishability in quantum dot-based entangled photon sources is to implement a broadband optical cavity. Achieving a high-Purcell cavity while retaining the advantages of the nanowire, such as directional emission, a broad operational bandwidth, and high light extraction efficiency, has been a significant challenge. Here, we propose a nanowire cavity based on quasi-bound states in the continuum formed by the strong coupling of two resonant optical modes. We numerically predict this design to support a cavity mode with 4 nm bandwidth and a Purcell enhancement of $\\sim$17. This cavity mode enables a directional far-field emission profile (88% overlap with a Gaussian) with a light extraction efficiency of $\\sim$74%. Our solution opens up a route for generating entangled photon pairs with enhanced extraction efficiency and single-photon indistinguishability for the practical realization of quantum networks."}
{"id": "2601.04279", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04279", "abs": "https://arxiv.org/abs/2601.04279", "authors": ["Pau Esteve", "Massimiliano Zanin"], "title": "Generation of synthetic delay time series for air transport applications", "comment": "18 pages, 13 figures", "summary": "The generation of synthetic data is receiving increasing attention from the scientific community, thanks to its ability to solve problems like data scarcity and privacy, and is starting to find applications in air transport. We here tackle the problem of generating synthetic, yet realistic, time series of delays at airports, starting from large collections of operations in Europe and the US. We specifically compare three models, two of them based on state of the art Deep Learning algorithms, and one simplified Genetic Algorithm approach. We show how the latter can generate time series that are almost indistinguishable from real ones, while maintaining a high variability. We further validate the resulting time series in a problem of detecting delay propagations between airports. We finally make the synthetic data available to the scientific community."}
{"id": "2601.05141", "categories": ["gr-qc", "cond-mat.quant-gas"], "pdf": "https://arxiv.org/pdf/2601.05141", "abs": "https://arxiv.org/abs/2601.05141", "authors": ["Christian F. Schmidt", "Stefan Floerchinger"], "title": "Superluminal modes in a quantum field simulator for cosmology from analog Transplanckian physics", "comment": "(17 + 6) pages, 12 figures, comments are welcome", "summary": "The quantum-field-theoretic description for the U(1)-Goldstone boson of a scalar Bose-Einstein condensate with time-dependent contact interactions is developed beyond the acoustic approximation in accordance with Bogoliubov theory. The resulting effective action is mapped to a relativistic quantum field theory on a dispersive (or rainbow) cosmological spacetime which has a superluminal Corley-Jacobson dispersion relation. Time-dependent changes of the s-wave scattering length to quantum-simulate cosmological particle production are accompanied by a time-dependent healing length that can be interpreted as an analog Planck length in the comoving frame. Non-adiabatic transitions acquire a dispersive character, which is thoroughly discussed. The framework is applied to exponentially expanding or power-law contracting $(2+1)$-dimensional spacetimes which are known to produce scale-invariant cosmological power spectra. The sensitivity of these scenarios to the time-dependence of the Bogoliubov dispersion is investigated: We find a violation of scale-invariance via analytically trackable Transplanckian damping effects if the cut-off scale is not well separated from the horizon-crossing scale. In case of the exponential expansion, these damping effects remarkably settle and converge to another scale-invariant plateau in the far ultraviolet regime where non-adiabatic transitions are suppressed by the high dispersion. The developed framework enables quantitative access to more drastic analog cosmological scenarios with improved predictability in the ultraviolet regime that ultimately may lead to the observation of a scale-invariant cosmological power spectrum in the laboratory."}
{"id": "2601.04444", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04444", "abs": "https://arxiv.org/abs/2601.04444", "authors": ["Sabee Grewal", "Meghal Gupta", "William He", "Aniruddha Sen", "Mihir Singhal"], "title": "Pauli Measurements Are Near-Optimal for Pure State Tomography", "comment": null, "summary": "We give an algorithm for pure state tomography with near-optimal copy complexity using single-qubit measurements. Specifically, given $\\widetilde{O}(2^n/ε)$ copies of an unknown pure $n$-qubit state $\\lvertψ\\rangle$, the algorithm performs only \\textit{nonadaptive Pauli measurements}, runs in time $\\mathrm{poly}(2^n,1/ε)$, and outputs $\\lvert \\widehatψ \\rangle$ that has fidelity $1-ε$ with $\\lvert ψ\\rangle$ with high probability. This improves upon the previous best copy complexity bound of $\\widetilde{O}(3^n/ε)$."}
{"id": "2601.04282", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04282", "abs": "https://arxiv.org/abs/2601.04282", "authors": ["Qiang Chen", "Chun-Wun Cheng", "Xiu Su", "Hongyan Xu", "Xi Lin", "Shan You", "Angelica I. Aviles-Rivero", "Yi Chen"], "title": "LEGATO: Good Identity Unlearning Is Continuous", "comment": null, "summary": "Machine unlearning has become a crucial role in enabling generative models trained on large datasets to remove sensitive, private, or copyright-protected data. However, existing machine unlearning methods face three challenges in learning to forget identity of generative models: 1) inefficient, where identity erasure requires fine-tuning all the model's parameters; 2) limited controllability, where forgetting intensity cannot be controlled and explainability is lacking; 3) catastrophic collapse, where the model's retention capability undergoes drastic degradation as forgetting progresses. Forgetting has typically been handled through discrete and unstable updates, often requiring full-model fine-tuning and leading to catastrophic collapse. In this work, we argue that identity forgetting should be modeled as a continuous trajectory, and introduce LEGATO - Learn to ForgEt Identity in GenerAtive Models via Trajectory-consistent Neural Ordinary Differential Equations. LEGATO augments pre-trained generators with fine-tunable lightweight Neural ODE adapters, enabling smooth, controllable forgetting while keeping the original model weights frozen. This formulation allows forgetting intensity to be precisely modulated via ODE step size, offering interpretability and robustness. To further ensure stability, we introduce trajectory consistency constraints that explicitly prevent catastrophic collapse during unlearning. Extensive experiments across in-domain and out-of-domain identity unlearning benchmarks show that LEGATO achieves state-of-the-art forgetting performance, avoids catastrophic collapse and reduces fine-tuned parameters."}
{"id": "2601.05176", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2601.05176", "abs": "https://arxiv.org/abs/2601.05176", "authors": ["Samuel D. Tootle", "Terrence Pierre Jacques", "Marie Cassing"], "title": "A new code for computing differentially rotating neutron stars", "comment": "16 pages, 10 figures", "summary": "We present new initial data codes for constructing stationary, axisymmetric equilibrium models of differentially rotating neutron stars in full general relativity within the Frankfurt University/KADATH (FUKA) suite of initial data codes. FUKA leverages the KADATH spectral library to solve the Einstein equations under the assumption of an isentropic fluid without magnetic fields while incorporating GRHayLEOS to support 3D tabulated equations of state in \\textit{stellar collapse} format. The two solvers explored in this work include one using quasi-isotropic coordinates (QIC) in Spherical coordinates while the other solves the eXtended Conformal Thin Sandwich (XCTS) decomposition in Cartesian coordinates, enabling the construction of equilibrium configurations with high accuracy and efficiency. In this work we adopt the Komatsu-Eriguchi-Hachisu differential rotation law, however, the code is designed to be extensible to other rotation laws, allowing for exploration of physically relevant sequences and critical rotation thresholds. Furthermore, we perform convergence tests demonstrating the exponential accuracy of the spectral approach, we validate QIC and XCTS solutions against models well-studied in the literature, and we also compare FUKA solutions against the well-known RNS code. Finally, we explore the impact that initial data resolution has on dynamical simulations and recover the convergence order of the evolution scheme, the dominate source of error in this study. The new FUKA codes and results presented here lay the foundation for future extensions to more general configurations, including magnetic fields, removal of isentropic assumptions, and binary systems, and have been made publicly available to support community efforts in modeling differentially rotating relativistic stars."}
{"id": "2601.04467", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04467", "abs": "https://arxiv.org/abs/2601.04467", "authors": ["Kwok Ho Wan", "H. C. W. Price", "Qing Yao"], "title": "Holographic codes seen through ZX-calculus", "comment": null, "summary": "We re-visit the pentagon holographic quantum error correcting code from a ZX-calculus perspective. By expressing the underlying tensors as ZX-diagrams, we study the stabiliser structure of the code via Pauli webs. In addition, we obtain a diagrammatic understanding of its logical operators, encoding isometries, Rényi entropy and toy models of black holes/wormholes. Then, motivated by the pentagon holographic code's ZX-diagram, we introduce a family of codes constructed from ZX-diagrams on its dual hyperbolic tessellations and study their logical error rates using belief propagation decoders."}
{"id": "2601.04283", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04283", "abs": "https://arxiv.org/abs/2601.04283", "authors": ["Nikolay Yudin"], "title": "Mitigating Position-Shift Failures in Text-Based Modular Arithmetic via Position Curriculum and Template Diversity", "comment": null, "summary": "Building on insights from the grokking literature, we study character-level Transformers trained to compute modular addition from text, and focus on robustness under input-format variation rather than only in-distribution accuracy. We identify a previously under-emphasized failure mode: models that achieve high in-distribution accuracy can fail catastrophically when the same expression is shifted to different absolute character positions (\"position shift\") or presented under out-of-distribution natural-language templates. Using a disjoint-pair split over all ordered pairs for p=97, we show that a baseline model reaches strong in-distribution performance yet collapses under position shift and template OOD. We then introduce a simple training recipe that combines (i) explicit expression boundary markers, (ii) position curriculum that broadens the range of absolute positions seen during training, (iii) diverse template mixtures, and (iv) consistency training across multiple variants per example. Across three seeds, this intervention substantially improves robustness to position shift and template OOD while maintaining high in-distribution accuracy, whereas an ALiBi-style ablation fails to learn the task under our setup. Our results suggest that steering procedural generalization under noisy supervision benefits from explicitly training invariances that are otherwise absent from the data distribution, and we provide a reproducible evaluation protocol and artifacts."}
{"id": "2601.05190", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2601.05190", "abs": "https://arxiv.org/abs/2601.05190", "authors": ["G. Alencar", "R. S. Almeida", "R. N. Costa Filho", "T. M. Crispim", "Francisco S. N. Lobo"], "title": "The End of the Road for Bulk Fields in Braneworlds", "comment": "26 pages", "summary": "In this manuscript we generalize Ref. [1] and derive a complete set of local consistency conditions for bulk fields in braneworld scenarios with an arbitrary number of dimensions. This provides the first fully local and dimension-independent generalization of all known criteria for bulk fields. Within this framework, we show that a free scalar field is consistent and localized, whereas minimally and non-minimally coupled Maxwell fields violate the conditions, leading to a no-go theorem valid in any dimension. For nonlinear electrodynamics, we find that only the model $L(F)=b\\sqrt{F}$ admits a consistent and normalizable zero mode, and that among p-forms, consistency occurs solely for the free 0-form. We also demonstrate that Dirac fermions, with or without Yukawa terms, are inconsistent within this framework and therefore cannot propagate in the bulk. Our local approach makes explicit that these conclusions do not depend on any particular internal geometry or warp factor: previously known results arise merely as special cases of a broader and strictly local structure, highlighting the universality of the constraints derived here."}
{"id": "2601.04535", "categories": ["quant-ph", "cond-mat.quant-gas"], "pdf": "https://arxiv.org/pdf/2601.04535", "abs": "https://arxiv.org/abs/2601.04535", "authors": ["Kaiyuan Cao", "Mingzhi Li", "Xiang-Ping Jiang", "Shu Chen", "Jian Wang"], "title": "Momentum-Space Entanglement Entropy as a Universal Signature of Dynamical Quantum Phase Transitions", "comment": "5 pages", "summary": "We introduce a momentum-space entanglement entropy to quantify quantum correlations between distinct momentum modes following a quench. We prove analytically in the transverse-field Ising (TFI) model and the Su-Schrieffer-Heeger (SSH) chain that every critical momentum $k^{*}$ associated with a dynamical quantum phase transition (DQPT) saturates its entanglement entropy to the maximal value $\\ln{d}$ ($d=2$ in TFI and SSH models), coinciding with the vanishing of the Loschmidt echo. This saturation of mode entanglement thus provides a universal, direct signature of DQPTs. Our work thus establishes a unified, entanglement-based perspective on dynamical quantum phase transitions."}
{"id": "2601.04286", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04286", "abs": "https://arxiv.org/abs/2601.04286", "authors": ["Niklas Kueper", "Kartik Chari", "Elsa Andrea Kirchner"], "title": "Enhancing Robustness of Asynchronous EEG-Based Movement Prediction using Classifier Ensembles", "comment": null, "summary": "Objective: Stroke is one of the leading causes of disabilities. One promising approach is to extend the rehabilitation with self-initiated robot-assisted movement therapy. To enable this, it is required to detect the patient's intention to move to trigger the assistance of a robotic device. This intention to move can be detected from human surface electroencephalography (EEG) signals; however, it is particularly challenging to decode when classifications are performed online and asynchronously. In this work, the effectiveness of classifier ensembles and a sliding-window postprocessing technique was investigated to enhance the robustness of such asynchronous classification. Approach: To investigate the effectiveness of classifier ensembles and a sliding-window postprocessing, two EEG datasets with 14 healthy subjects who performed self-initiated arm movements were analyzed. Offline and pseudo-online evaluations were conducted to compare ensemble combinations of the support vector machine (SVM), multilayer perceptron (MLP), and EEGNet classification models. Results: The results of the pseudo-online evaluation show that the two model ensembles significantly outperformed the best single model for the optimal number of postprocessing windows. In particular, for single models, an increased number of postprocessing windows significantly improved classification performances. Interestingly, we found no significant improvements between performances of the best single model and classifier ensembles in the offline evaluation. Significance: We demonstrated that classifier ensembles and appropriate postprocessing methods effectively enhance the asynchronous detection of movement intentions from EEG signals. In particular, the classifier ensemble approach yields greater improvements in online classification than in offline classification, and reduces false detections, i.e., early false positives."}
{"id": "2601.05218", "categories": ["gr-qc", "astro-ph.CO", "hep-ph", "hep-th"], "pdf": "https://arxiv.org/pdf/2601.05218", "abs": "https://arxiv.org/abs/2601.05218", "authors": ["Joan Solà Peracaula", "Àlex González-Fuentes", "Cristian Moreno-Pulido"], "title": "Towards a unified quantum field theory of dark energy and inflation: unstable de Sitter vacuum and running vacuum", "comment": "77 pages, 8 figures", "summary": "Inflation is a necessary cosmic mechanism to cure basic inconsistencies of the standard model of cosmology. These problems are usually `fixed' by postulating the existence of a scalar field (called the ``inflaton''). However, other less ad hoc options are possible. In the running vacuum model (RVM) framework, the vacuum energy density (VED) is a function of the Hubble rate $H$ and its time derivatives: $ρ_{\\rm vac}=ρ_{\\rm vac}(H, \\dot{H},\\ddot{H},\\dots)$. In this context, the VED is dynamical (there is no rigid cosmological constant $Λ$). In the FLRW epoch, $ρ_{\\rm vac}$ evolves very slowly with expansion, as befits the observed $Λ\\simeq$const. behavior. In contrast, in the very early universe the vacuum fluctuations induce higher powers $H^N$ capable of unleashing fast inflation in a short period in which $H\\simeq$ const. We call this mechanism `RVM-inflation'. It does not require an inflaton field since inflation is brought about by pure quantum field theory (QFT) effects on the dynamical background. It is different from Starobinsky's inflation, in which $H$ is never constant. In this work, we study a closely related scenario: the decay of the exact de Sitter vacuum into FLRW spacetime in its radiation epoch and the subsequent impact on the current universe, and compare with the RVM. We find that in both cases inflation is driven by $H^4$ powers together with subleading contributions of order $H^2$ that ease a graceful-exit transition into the radiation-dominated epoch, where the FLRW regime starts and ultimately develops a mildly evolving VED in the late universe: $δρ_{\\rm vac}\\sim {\\cal O}(m_{\\rm Pl} ^2 H^2)$. The outcome is an unified QFT approach to inflation and dark energy (conceived as dynamical vacuum energy) with potentially measurable phenomenological consequences in the present universe which can help to cure the cosmological tensions."}
{"id": "2601.04543", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2601.04543", "abs": "https://arxiv.org/abs/2601.04543", "authors": ["Venkat Abhignan", "Mohit Mittal", "Aditi Das", "Megha Shrivastava"], "title": "Increasing the secret key rates and point-to-multipoint extension for experimental coherent-one-way quantum key distribution protocol", "comment": null, "summary": "Using quantum key distribution (QKD) protocols, a secret key is created between two distant users (transmitter and receiver) at a particular key rate. Quantum technology can facilitate secure communication for cryptographic applications, combining QKD with one-time-pad (OTP) encryption. In order to ensure the continuous operation of QKD in real-world networks, efforts have been concentrated on optimizing the use of components and effective QKD protocols to improve secret key rates and increase the transmission between multiple users. Generally, in experimental implementations, the secret key rates are limited by single-photon detectors, which are used at the receivers of QKD and create a bottleneck due to their limited detection rates (detectors with low detection efficiency and high detector dead-time). We experimentally show that secret key rates can be increased by combining the time-bin information of two such detectors on the data line of the receiver for the coherent-one-way (COW) QKD protocol with a minimal increase in quantum bit error rate (QBER, the proportion of erroneous bits). Further, we implement a point-to-multipoint COW QKD protocol, introducing an additional receiver module. The three users (one transmitter and two receivers) share the secret key in post-processing, relying on OTP encryption. Typically, the dual-receiver extension can improve the combined secret key rates of the system; however, one has to optimise the experimental parameters to achieve this within security margins. These methods are general and can be applied to any implementation of the COW protocol."}
{"id": "2601.04287", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.04287", "abs": "https://arxiv.org/abs/2601.04287", "authors": ["Ben Carvell", "George De Ath", "Eseoghene Benjamin", "Richard Everson"], "title": "Online Action-Stacking Improves Reinforcement Learning Performance for Air Traffic Control", "comment": null, "summary": "We introduce online action-stacking, an inference-time wrapper for reinforcement learning policies that produces realistic air traffic control commands while allowing training on a much smaller discrete action space. Policies are trained with simple incremental heading or level adjustments, together with an action-damping penalty that reduces instruction frequency and leads agents to issue commands in short bursts. At inference, online action-stacking compiles these bursts of primitive actions into domain-appropriate compound clearances. Using Proximal Policy Optimisation and the BluebirdDT digital twin platform, we train agents to navigate aircraft along lateral routes, manage climb and descent to target flight levels, and perform two-aircraft collision avoidance under a minimum separation constraint. In our lateral navigation experiments, action stacking greatly reduces the number of issued instructions relative to a damped baseline and achieves comparable performance to a policy trained with a 37-dimensional action space, despite operating with only five actions. These results indicate that online action-stacking helps bridge a key gap between standard reinforcement learning formulations and operational ATC requirements, and provides a simple mechanism for scaling to more complex control scenarios."}
{"id": "2601.05223", "categories": ["gr-qc"], "pdf": "https://arxiv.org/pdf/2601.05223", "abs": "https://arxiv.org/abs/2601.05223", "authors": ["David Trestini", "Zachary Nasipak", "Adam Pound"], "title": "Constants of motion in gravitational self-force theory", "comment": "19 pages + Appendices, 5 figures", "summary": "Synergies between self-force theory and other approaches to the gravitational two-body problem have traditionally relied on calculations of gauge-invariant observables as functions of orbital frequencies. However, in self-force theory one can also define a complete set of constants of motion: energy, azimuthal angular momentum, and radial and polar actions. Here we outline how directly utilizing these constants allows for more straightforward comparisons and hybridizations across the parameter space, as well as more streamlined waveform generation through flux-balance laws. Restricting to the case of nonspinning binaries and first order in self-force, we compute the constants of motion and the corrections to fundamental frequencies numerically as well as analytically (to 9PN in a post-Newtonian expansion), establishing consistency with the highest-order (4PN) results available from post-Newtonian theory. We also apply the results to identify the perturbed locations of special curves in the parameter space: circular orbits and the separatrix between bound and plunging orbits."}
{"id": "2601.04549", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04549", "abs": "https://arxiv.org/abs/2601.04549", "authors": ["Jie Feng", "XiaoDi Liu", "Haian Xu", "Pu Wang", "Graeme J. Ackland", "Eugene Gregoryanz"], "title": "Observation of ΔJ=0 Rotational Excitation in Dense Hydrogens", "comment": null, "summary": "Raman measurements performed on dense H2, D2 and H2+D2 in a wide pressure-temperature range reveal the presence of the ΔJ=0 rotational excitation. In the gas/fluid state this excitation has zero Raman shift, but in the solid, the crystal field drive s it away from the zero value e.g. 75 cm-1 at around 50 GPa and 10 K for both isotopes and their mixture. In the case of deuterium, the ΔJ=0 mode splits upon entering phase II suggesting a very complex molecular environment of the broken symmetry phase (BSP). In the fluid state and phases I and II the frequencies (energies) of the ΔJ=0 transition for H2 and D2 do not scale either as rotational (by factor of 2) nor vibrational (by square 2) modes and appear to be completely isotope independent. This independence on mass marks this transition as unique and a fundamentally different type of excitation from the commonly considered harmonic oscillator and quantum rotor."}
{"id": "2601.04297", "categories": ["cs.LG", "cs.CV", "cs.HC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.04297", "abs": "https://arxiv.org/abs/2601.04297", "authors": ["Behrad Binaei-Haghighi", "Nafiseh Sadat Sajadi", "Mehrad Liviyan", "Reyhane Akhavan Kharazi", "Fatemeh Amirkhani", "Behnam Bahrak"], "title": "ArtCognition: A Multimodal AI Framework for Affective State Sensing from Visual and Kinematic Drawing Cues", "comment": "12 pages, 7 figures", "summary": "The objective assessment of human affective and psychological states presents a significant challenge, particularly through non-verbal channels. This paper introduces digital drawing as a rich and underexplored modality for affective sensing. We present a novel multimodal framework, named ArtCognition, for the automated analysis of the House-Tree-Person (HTP) test, a widely used psychological instrument. ArtCognition uniquely fuses two distinct data streams: static visual features from the final artwork, captured by computer vision models, and dynamic behavioral kinematic cues derived from the drawing process itself, such as stroke speed, pauses, and smoothness. To bridge the gap between low-level features and high-level psychological interpretation, we employ a Retrieval-Augmented Generation (RAG) architecture. This grounds the analysis in established psychological knowledge, enhancing explainability and reducing the potential for model hallucination. Our results demonstrate that the fusion of visual and behavioral kinematic cues provides a more nuanced assessment than either modality alone. We show significant correlations between the extracted multimodal features and standardized psychological metrics, validating the framework's potential as a scalable tool to support clinicians. This work contributes a new methodology for non-intrusive affective state assessment and opens new avenues for technology-assisted mental healthcare."}
{"id": "2601.04591", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04591", "abs": "https://arxiv.org/abs/2601.04591", "authors": ["Wonhyeong Choi", "Jiyong Kang", "Kyunghye Kim", "Jaehun You", "Kyungmin Lee", "Taehyun Kim"], "title": "Multimode Fock-State Measurements using Dispersive Shifts in a Trapped Ion", "comment": null, "summary": "Trapped ions naturally host multiple motional modes alongside long-lived spin qubits, providing a scalable multimode bosonic register. Efficiently characterizing such bosonic registers requires the ability to access many motional modes with limited spin resources. Here we introduce a single-spin, multimode measurement primitive using dispersive shifts in the far-detuned multimode Jaynes-Cummings interaction. We implement a Ramsey sequence that maps phonon-number-dependent phases onto the spin, thereby realizing a multimode spin-dependent rotation (SDR). We also introduce a selective-decoupling scheme that cancels the phase induced by the carrier AC-Stark shift while preserving the phonon-number-dependent phase induced by the dispersive shift. Using this SDR-based Ramsey sequence on a single trapped ion, we experimentally extract two-mode Fock-state distributions, perform parity-based filtering of two-mode motional states, and realize a nondestructive single-shot measurement of a single-mode Fock state via repeated filtering steps."}
{"id": "2601.04299", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.04299", "abs": "https://arxiv.org/abs/2601.04299", "authors": ["Pir Bakhsh Khokhar", "Carmine Gravino", "Fabio Palomba", "Sule Yildrim Yayilgan", "Sarang Shaikh"], "title": "Transformer-Based Multi-Modal Temporal Embeddings for Explainable Metabolic Phenotyping in Type 1 Diabetes", "comment": null, "summary": "Type 1 diabetes (T1D) is a highly metabolically heterogeneous disease that cannot be adequately characterized by conventional biomarkers such as glycated hemoglobin (HbA1c). This study proposes an explainable deep learning framework that integrates continuous glucose monitoring (CGM) data with laboratory profiles to learn multimodal temporal embeddings of individual metabolic status. Temporal dependencies across modalities are modeled using a transformer encoder, while latent metabolic phenotypes are identified via Gaussian mixture modeling. Model interpretability is achieved through transformer attention visualization and SHAP-based feature attribution. Five latent metabolic phenotypes, ranging from metabolic stability to elevated cardiometabolic risk, were identified among 577 individuals with T1D. These phenotypes exhibit distinct biochemical profiles, including differences in glycemic control, lipid metabolism, renal markers, and thyrotropin (TSH) levels. Attention analysis highlights glucose variability as a dominant temporal factor, while SHAP analysis identifies HbA1c, triglycerides, cholesterol, creatinine, and TSH as key contributors to phenotype differentiation. Phenotype membership shows statistically significant, albeit modest, associations with hypertension, myocardial infarction, and heart failure. Overall, this explainable multimodal temporal embedding framework reveals physiologically coherent metabolic subgroups in T1D and supports risk stratification beyond single biomarkers."}
{"id": "2601.04604", "categories": ["quant-ph", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2601.04604", "abs": "https://arxiv.org/abs/2601.04604", "authors": ["Amartya Bose"], "title": "Path Integral Lindblad Dynamics in Presence of Time-Dependent Fields", "comment": "4 pages, 2 figures", "summary": "The path integral Lindblad dynamics (PILD) method [A. Bose, J. Phys. Chem. Lett. 15(12), 3363-3368 (2024)] had been introduced as a way of incorporating the impact of certain empirical processes like pumps and drains on the dynamics of quantum systems interacting with thermal environments. The method being based on the time-translational invariance of the Nakajima-Zwanzig memory kernel, however, was not able to account for time-dependent external fields. In this communication, we give an alternate, simpler formulation of PILD, that allows us to go beyond this limitation. It does not require the evaluation of the non-Markovian memory kernel directly, and consequently can be applied to Floquet systems as well."}
{"id": "2601.04301", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04301", "abs": "https://arxiv.org/abs/2601.04301", "authors": ["Rylan Schaeffer", "Joshua Kazdan", "Baber Abbasi", "Ken Ziyu Liu", "Brando Miranda", "Ahmed Ahmed", "Abhay Puri", "Niloofar Mireshghallah", "Sanmi Koyejo"], "title": "Quantifying the Effect of Test Set Contamination on Generative Evaluations", "comment": null, "summary": "As frontier AI systems are pretrained on web-scale data, test set contamination has become a critical concern for accurately assessing their capabilities. While research has thoroughly investigated the impact of test set contamination on discriminative evaluations like multiple-choice question-answering, comparatively little research has studied the impact of test set contamination on generative evaluations. In this work, we quantitatively assess the effect of test set contamination on generative evaluations through the language model lifecycle. We pretrain language models on mixtures of web data and the MATH benchmark, sweeping model sizes and number of test set replicas contaminating the pretraining corpus; performance improves with contamination and model size. Using scaling laws, we make a surprising discovery: including even a single test set replica enables models to achieve lower loss than the irreducible error of training on the uncontaminated corpus. We then study further training: overtraining with fresh data reduces the effects of contamination, whereas supervised finetuning on the training set can either increase or decrease performance on test data, depending on the amount of pretraining contamination. Finally, at inference, we identify factors that modulate memorization: high sampling temperatures mitigate contamination effects, and longer solutions are exponentially more difficult to memorize than shorter ones, presenting a contrast with discriminative evaluations, where solutions are only a few tokens in length. By characterizing how generation and memorization interact, we highlight a new layer of complexity for trustworthy evaluation of AI systems."}
{"id": "2601.04636", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04636", "abs": "https://arxiv.org/abs/2601.04636", "authors": ["Duc Manh Doan", "Hung Q. Nguyen"], "title": "Hardy nonlocality for entangled pairs in a four-particle system", "comment": null, "summary": "Nonlocality can be studied through different approaches, such as Bell's inequalities, and it can be found in numerous quantum states, including GHZ states or graph states. Hardy's paradox, or Hardy-type nonlocality, provides a way to investigate nonlocality for entangled states of particles without using inequalities. Previous studies of Hardy's nonlocality have mostly focused on the fully entangled systems, while other entanglement configurations remain less explored. In this work, the system under investigation consists of four particles arranged in a cyclic entanglement configuration, where each particle forms entangled pairs with two neighbors, while non-neighboring particles remain unentangled. We found that this entanglement structure offers a larger set of conditions that lead to the contradiction with the LHV model, compared to the fully entangled systems. This enhancement can be attributed to the presence of multiple excluded states and correlations, in which the measurement result of a particle only influences the result of its paired partners. We implement quantum circuits compatible with the cyclic entanglement structure, and through simulation, the correlation patterns and the states of interest are identified. We further execute the proposed circuits on IBM Brisbane, a practical backend; however, the results show considerable deviations from the simulation counterparts."}
{"id": "2601.04361", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04361", "abs": "https://arxiv.org/abs/2601.04361", "authors": ["Mohammad Ali Javidian"], "title": "Causally-Aware Information Bottleneck for Domain Adaptation", "comment": "An extended abstract version of this work was accepted for the Proceedings of the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026)", "summary": "We tackle a common domain adaptation setting in causal systems. In this setting, the target variable is observed in the source domain but is entirely missing in the target domain. We aim to impute the target variable in the target domain from the remaining observed variables under various shifts. We frame this as learning a compact, mechanism-stable representation. This representation preserves information relevant for predicting the target while discarding spurious variation. For linear Gaussian causal models, we derive a closed-form Gaussian Information Bottleneck (GIB) solution. This solution reduces to a canonical correlation analysis (CCA)-style projection and offers Directed Acyclic Graph (DAG)-aware options when desired. For nonlinear or non-Gaussian data, we introduce a Variational Information Bottleneck (VIB) encoder-predictor. This approach scales to high dimensions and can be trained on source data and deployed zero-shot to the target domain. Across synthetic and real datasets, our approach consistently attains accurate imputations, supporting practical use in high-dimensional causal models and furnishing a unified, lightweight toolkit for causal domain adaptation."}
{"id": "2601.04645", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04645", "abs": "https://arxiv.org/abs/2601.04645", "authors": ["Xinxuan Chen", "Hongxiang Zhu", "Zhaohui Yang", "Zhaofeng Su", "Jianxin Chen", "Feng Wu", "Hui-Hai Zhao"], "title": "SurgeQ: A Hybrid Framework for Ultra-Fast Quantum Processor Design and Crosstalk-Aware Circuit Execution", "comment": "7 pages, 4 figures; accepted by DATE 2026", "summary": "Executing quantum circuits on superconducting platforms requires balancing the trade-off between gate errors and crosstalk. To address this, we introduce SurgeQ, a hardware-software co-design strategy consisting of a design phase and an execution phase, to achieve accelerated circuit execution and improve overall program fidelity. SurgeQ employs coupling-strengthened, faster two-qubit gates while mitigating their increased crosstalk through a tailored scheduling strategy. With detailed consideration of composite noise models, we establish a systematic evaluation pipeline to identify the optimal coupling strength. Evaluations on a comprehensive suite of real-world benchmarks show that SurgeQ generally achieves higher fidelity than up-to-date baselines, and remains effective in combating exponential fidelity decay, achieving up to a million-fold improvement in large-scale circuits."}
{"id": "2601.04362", "categories": ["cs.LG", "cs.NE", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2601.04362", "abs": "https://arxiv.org/abs/2601.04362", "authors": ["Rodja Trappe"], "title": "Phasor Agents: Oscillatory Graphs with Three-Factor Plasticity and Sleep-Staged Learning", "comment": "22 pages, 14 figures", "summary": "Phasor Agents are dynamical systems whose internal state is a Phasor Graph: a weighted graph of coupled Stuart-Landau oscillators. A Stuart-Landau oscillator is a minimal stable \"rhythm generator\" (the normal form near a Hopf bifurcation); each oscillator is treated as an abstract computational unit (inspired by, but not claiming to model, biological oscillatory populations). In this interpretation, oscillator phase tracks relative timing (coherence), while amplitude tracks local gain or activity. Relative phase structure serves as a representational medium; coupling weights are learned via three-factor local plasticity - eligibility traces gated by sparse global modulators and oscillation-timed write windows - without backpropagation.\n  A central challenge in oscillatory substrates is stability: online weight updates can drive the network into unwanted regimes (e.g., global synchrony), collapsing representational diversity. We therefore separate wake tagging from offline consolidation, inspired by synaptic tagging-and-capture and sleep-stage dynamics: deep-sleep-like gated capture commits tagged changes safely, while REM-like replay reconstructs and perturbs experience for planning.\n  A staged experiment suite validates each mechanism with ablations and falsifiers: eligibility traces preserve credit under delayed modulation; compression-progress signals pass timestamp-shuffle controls; phase-coherent retrieval reaches 4x diffusive baselines under noise; wake/sleep separation expands stable learning by 67 percent under matched weight-norm budgets; REM replay improves maze success rate by +45.5 percentage points; and a Tolman-style latent-learning signature - immediate competence and detour advantage after unrewarded exploration, consistent with an internal model - emerges from replay (Tolman, 1948).\n  The codebase and all artifacts are open-source."}
{"id": "2601.04685", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04685", "abs": "https://arxiv.org/abs/2601.04685", "authors": ["Eliahu Cohen", "Tomer Shushi"], "title": "Regularization from Superpositions of Time Evolutions", "comment": "11 pages, comments are welcome", "summary": "Short-time approximations and path integrals can be dominated by high-energy or large-field contributions, especially in the presence of singular interactions, motivating regulators that are suppressive yet removable. Standard regulators typically impose such suppressions by hand (e.g. cutoffs, higher-derivative terms, heat-kernel smearing, lattice discretizations), while here we show that closely related smooth filters can arise as the conditional map produced by interference in a coherently controlled, postselected superposition of evolutions. A successful postselection implements a single heralded operator that is a coherent linear combination of time-evolution operators. For a Gaussian superposition of time translations in quantum mechanics, the postselected step is $V_{σ,Δt}=e^{-iHΔt}\\,e^{-\\frac12σ^2Δt^2H^2}$, i.e.\\ the desired unitary step multiplied by a Gaussian energy filter suppressing energies above order $1/(σΔt)$. This renders short-time kernels in time-sliced path-integral approximations well behaved for singular potentials, while the target unitary dynamics is recovered as $σ\\to0$ and (for fixed $σ$) also as $Δt\\to0$ at fixed $t$. In scalar QFT, a local Gaussian smearing of the quartic coupling induces a positive $(σ^2/2)φ^8$ term in the Euclidean action, providing a symmetry-compatible large-field stabilizer; it is naturally viewed as an irrelevant operator whose effects can be renormalized at fixed $σ$ (together with a conventional UV regulator) and removed by taking $σ\\to0$. We give short-time error bounds and analyze multi-step success probabilities."}
{"id": "2601.04365", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04365", "abs": "https://arxiv.org/abs/2601.04365", "authors": ["Anton Roupassov-Ruiz", "Yiyang Zuo"], "title": "Survival Dynamics of Neural and Programmatic Policies in Evolutionary Reinforcement Learning", "comment": null, "summary": "In evolutionary reinforcement learning tasks (ERL), agent policies are often encoded as small artificial neural networks (NERL). Such representations lack explicit modular structure, limiting behavioral interpretation. We investigate whether programmatic policies (PERL), implemented as soft, differentiable decision lists (SDDL), can match the performance of NERL. To support reproducible evaluation, we provide the first fully specified and open-source reimplementation of the classic 1992 Artificial Life (ALife) ERL testbed. We conduct a rigorous survival analysis across 4000 independent trials utilizing Kaplan-Meier curves and Restricted Mean Survival Time (RMST) metrics absent in the original study. We find a statistically significant difference in survival probability between PERL and NERL. PERL agents survive on average 201.69 steps longer than NERL agents. Moreover, SDDL agents using learning alone (no evolution) survive on average 73.67 steps longer than neural agents using both learning and evaluation. These results demonstrate that programmatic policies can exceed the survival performance of neural policies in ALife."}
{"id": "2601.04732", "categories": ["quant-ph", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04732", "abs": "https://arxiv.org/abs/2601.04732", "authors": ["Dominik Freinberger", "Philipp Moser"], "title": "The Role of Quantum in Hybrid Quantum-Classical Neural Networks: A Realistic Assessment", "comment": "16 pages, 6 figures", "summary": "Quantum machine learning has emerged as a promising application domain for near-term quantum hardware, particularly through hybrid quantum-classical models that leverage both classical and quantum processing. Although numerous hybrid architectures have been proposed and demonstrated successfully on benchmark tasks, a significant open question remains regarding the specific contribution of quantum components to the overall performance of these models. In this work, we aim to shed light on the impact of quantum processing within hybrid quantum-classical neural network architectures through a rigorous statistical study. We systematically assess common hybrid models on medical signal data as well as planar and volumetric images, examining the influence attributable to classical and quantum aspects such as encoding schemes, entanglement, and circuit size. We find that in best-case scenarios, hybrid models show performance comparable to their classical counterparts, however, in most cases, performance metrics deteriorate under the influence of quantum components. Our multi-modal analysis provides realistic insights into the contributions of quantum components and advocates for cautious claims and design choices for hybrid models in near-term applications."}
{"id": "2601.04366", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.04366", "abs": "https://arxiv.org/abs/2601.04366", "authors": ["Selcuk Koyuncu", "Ronak Nouri", "Stephen Providence"], "title": "Machine Learning Model for Sparse PCM Completion", "comment": null, "summary": "In this paper, we propose a machine learning model for sparse pairwise comparison matrices (PCMs), combining classical PCM approaches with graph-based learning techniques. Numerical results are provided to demonstrate the effectiveness and scalability of the proposed method."}
{"id": "2601.04733", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2601.04733", "abs": "https://arxiv.org/abs/2601.04733", "authors": ["Nicholas S. Yama", "Chun-Chi Wu", "Fariba Hatami", "Kai-Mei C. Fu"], "title": "A scalable gallium-phosphide-on-diamond spin-photon interface", "comment": null, "summary": "The efficient interfacing of quantum emitters and photons is fundamental to quantum networking. Quantum defects embedded in integrated nanophotonic circuits are promising for such applications due to the deterministic light-matter interactions of high-cooperativity ($C>1$) cavity quantum electrodynamics and potential for scalable integration with active photonic processing. Silicon-vacancy (SiV) centers embedded in diamond nanophotonic cavities are a leading approach due to their excellent optical and spin coherence, however their long-term scalability is limited by the diamond itself, as its suspended geometry and weak nonlinearity necessitates coupling to a second processing chip. Here we realize the first high-cooperativity coupling of quantum defects to hybrid-integrated nanophotonics in a scalable, planar platform. We integrate more than 600 gallium phosphide (GaP) nanophotonic cavities on a diamond substrate with near-surface SiV centers. We examine a particular device with two strongly coupled SiV centers in detail, confirming above-unity cooperativity via multiple independent measurements. Application of an external magnetic field via a permanent magnet enables optical resolution of the SiV spin transitions from which we determine a spin-relaxation time $T_1>0.4$ ms at 4 K. We utilize the high cooperativity coupling to observe spin-dependent transmission switching and the quantum jumps of the SiV spin via single-shot readout. These results, coupled with GaP's strong nonlinear properties, establish GaP-on-diamond as a scalable planar platform for quantum network applications."}
{"id": "2601.04378", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.04378", "abs": "https://arxiv.org/abs/2601.04378", "authors": ["Corentin Lobet", "Francesca Chiaromonte"], "title": "Aligned explanations in neural networks", "comment": null, "summary": "Feature attribution is the dominant paradigm for explaining deep neural networks. However, most existing methods only loosely reflect the model's prediction-making process, thereby merely white-painting the black box. We argue that explanatory alignment is a key aspect of trustworthiness in prediction tasks: explanations must be directly linked to predictions, rather than serving as post-hoc rationalizations. We present model readability as a design principle enabling alignment, and PiNets as a modeling framework to pursue it in a deep learning context. PiNets are pseudo-linear networks that produce instance-wise linear predictions in an arbitrary feature space, making them linearly readable. We illustrate their use on image classification and segmentation tasks, demonstrating how PiNets produce explanations that are faithful across multiple criteria in addition to alignment."}
{"id": "2601.04806", "categories": ["quant-ph", "math-ph", "physics.atom-ph"], "pdf": "https://arxiv.org/pdf/2601.04806", "abs": "https://arxiv.org/abs/2601.04806", "authors": ["Mohamed Améziane Sadoun", "Redouane Zamoum", "Abdellah Touati"], "title": "Bound state solutions with a linear combination of Yuakawa plus four-parameter diatomic potentials using path integral approach: Thermodynamic properties", "comment": null, "summary": "In this paper, we investigate the approximate analytical bound states with a linear combination of two diatomic molecule potentials, Yukawa and four parameters potentials, within the framework of the path integral formalism. With the help of an appropriate approximation to evaluate the centrifugal term, the energy spectrum and the normalized wave functions of the bound states are derived from the poles of Green's function and its residues. The partition function and other thermodynamic properties were obtained using the compact form of the energy equation."}
{"id": "2601.04392", "categories": ["cs.LG", "cs.AI", "cs.RO", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.04392", "abs": "https://arxiv.org/abs/2601.04392", "authors": ["Mohsen Jalaeian-Farimani"], "title": "Enhanced-FQL($λ$), an Efficient and Interpretable RL with novel Fuzzy Eligibility Traces and Segmented Experience Replay", "comment": "Submitted to ECC26 conference", "summary": "This paper introduces a fuzzy reinforcement learning framework, Enhanced-FQL($λ$), that integrates novel Fuzzified Eligibility Traces (FET) and Segmented Experience Replay (SER) into fuzzy Q-learning with Fuzzified Bellman Equation (FBE) for continuous control tasks. The proposed approach employs an interpretable fuzzy rule base instead of complex neural architectures, while maintaining competitive performance through two key innovations: a fuzzified Bellman equation with eligibility traces for stable multi-step credit assignment, and a memory-efficient segment-based experience replay mechanism for enhanced sample efficiency. Theoretical analysis proves the proposed method convergence under standard assumptions. Extensive evaluations in continuous control domains demonstrate that Enhanced-FQL($λ$) achieves superior sample efficiency and reduced variance compared to n-step fuzzy TD and fuzzy SARSA($λ$) baselines, while maintaining substantially lower computational complexity than deep RL alternatives such as DDPG. The framework's inherent interpretability, combined with its computational efficiency and theoretical convergence guarantees, makes it particularly suitable for safety-critical applications where transparency and resource constraints are essential."}
{"id": "2601.04810", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04810", "abs": "https://arxiv.org/abs/2601.04810", "authors": ["Alexander van Lomwel", "Paul M. Schindler", "Modesto Orozco-Ruiz", "Marin Bukov", "Nguyen H. Le", "Florian Mintert"], "title": "Fast thermal state preparation beyond native interactions", "comment": null, "summary": "While questions on quantum simulation of ground state physics are mostly focussed on the realization of effective interactions, most work on quantum simulation of thermal physics explores the realization of dynamics towards a thermal mixed state under native interactions. Many open questions that could be answered with quantum simulations, however, involve thermal states with respect to synthetic interactions. We present a framework based solely on unitary dynamics to design quantum simulations for thermal states with respect to Hamiltonians that include non-native interactions, suitable for both present-day digital and analogue devices. By classical means, our method finds the control sequence to reach a target thermal state for system sizes well out of reach of state-vector or density-matrix control methods, even though quantum hardware is required to explicitly simulate the thermal state dynamics. With the illustrative example of the cluster Ising model that includes non-native three-body interactions, we find that required experimental resources, such as the total evolution time, are independent of temperature and criticality."}
{"id": "2601.04411", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04411", "abs": "https://arxiv.org/abs/2601.04411", "authors": ["Ali Rad", "Khashayar Filom", "Darioush Keivan", "Peyman Mohajerin Esfahani", "Ehsan Kamalinejad"], "title": "Rate or Fate? RLV$^\\varepsilon$R: Reinforcement Learning with Verifiable Noisy Rewards", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) is a simple but powerful paradigm for training LLMs: sample a completion, verify it, and update. In practice, however, the verifier is almost never clean--unit tests probe only limited corner cases; human and synthetic labels are imperfect; and LLM judges (e.g., RLAIF) are noisy and can be exploited--and this problem worsens on harder domains (especially coding) where tests are sparse and increasingly model-generated. We ask a pragmatic question: does the verification noise merely slow down the learning (rate), or can it flip the outcome (fate)?\n  To address this, we develop an analytically tractable multi-armed bandit view of RLVR dynamics, instantiated with GRPO and validated in controlled experiments. Modeling false positives and false negatives and grouping completions into recurring reasoning modes yields a replicator-style (natural-selection) flow on the probability simplex. The dynamics decouples into within-correct-mode competition and a one-dimensional evolution for the mass on incorrect modes, whose drift is determined solely by Youden's index J=TPR-FPR. This yields a sharp phase transition: when J>0, the incorrect mass is driven toward extinction (learning); when J=0, the process is neutral; and when J<0, incorrect modes amplify until they dominate (anti-learning and collapse). In the learning regime J>0, noise primarily rescales convergence time (\"rate, not fate\"). Experiments on verifiable programming tasks under synthetic noise reproduce the predicted J=0 boundary. Beyond noise, the framework offers a general lens for analyzing RLVR stability, convergence, and algorithmic interventions."}
{"id": "2601.04812", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04812", "abs": "https://arxiv.org/abs/2601.04812", "authors": ["Alessio Benavoli", "Felix Binder"], "title": "Quantum Wiener architecture for quantum reservoir computing", "comment": null, "summary": "This work focuses on quantum reservoir computing and, in particular, on quantum Wiener architectures (qWiener), consisting of quantum linear dynamic networks with weak continuous measurements and classical nonlinear static readouts. We provide the first rigorous proof that qWiener systems retain the fading-memory property and universality of classical Wiener architectures, despite quantum constraints on linear dynamics and measurement back-action. Furthermore, we develop a kernel-theoretic interpretation showing that qWiener reservoirs naturally induce deep kernels, providing a principled framework for analysing their expressiveness. We further characterise the simplest qWiener instantiation, consisting of concatenated quantum harmonic oscillators, and show the difference with respect to the classical case. Finally, we empirically evaluate the architecture on standard reservoir computing benchmarks, demonstrating systematic performance gains over prior classical and quantum reservoir computing models."}
{"id": "2601.04413", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04413", "abs": "https://arxiv.org/abs/2601.04413", "authors": ["Nausherwan Malik", "Zubair Khalid", "Muhammad Faryad"], "title": "Distribution-Guided and Constrained Quantum Machine Unlearning", "comment": "8 pages", "summary": "Machine unlearning aims to remove the influence of specific training data from a learned model without full retraining. While recent work has begun to explore unlearning in quantum machine learning, existing approaches largely rely on fixed, uniform target distributions and do not explicitly control the trade-off between forgetting and retained model behaviour. In this work, we propose a distribution-guided framework for class-level quantum machine unlearning that treats unlearning as a constrained optimization problem. Our method introduces a tunable target distribution derived from model similarity statistics, decoupling the suppression of forgotten-class confidence from assumptions about redistribution among retained classes. We further incorporate an anchor-based preservation constraint that explicitly maintains predictive behaviour on selected retained data, yielding a controlled optimization trajectory that limits deviation from the original model. We evaluate the approach on variational quantum classifiers trained on the Iris and Covertype datasets. Results demonstrate sharp suppression of forgotten-class confidence, minimal degradation of retained-class performance, and closer alignment with the gold retrained model baselines compared to uniform-target unlearning. These findings highlight the importance of target design and constraint-based formulations for reliable and interpretable quantum machine unlearning."}
{"id": "2601.04827", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04827", "abs": "https://arxiv.org/abs/2601.04827", "authors": ["Tran Xuan Hieu Le", "Tuan Hai Vu", "Vu Trung Duong Le", "Hoai Luan Pham", "Yasuhiko Nakashima"], "title": "PACOX: A FPGA-based Pauli Composer Accelerator for Pauli String Computation", "comment": "5 pages, 6 figures. This paper is submitted to IEEE Signal Processing Letter", "summary": "Pauli strings are a fundamental computational primitive in hybrid quantum-classical algorithms. However, classical computation of Pauli strings suffers from exponential complexity and quickly becomes a performance bottleneck as the number of qubits increases. To address this challenge, this paper proposes the Pauli Composer Accelerator (PACOX), the first dedicated FPGA-based accelerator for Pauli string computation. PACOX employs a compact binary encoding with XOR-based index permutation and phase accumulation. Based on this formulation, we design a parallel and pipelined processing element (PE) cluster architecture that efficiently exploits data-level parallelism on FPGA. Experimental results on a Xilinx ZCU102 FPGA show that PACOX operates at 250 MHz with a dynamic power consumption of 0.33 W, using 8,052 LUTs, 10,934 FFs, and 324 BRAMs. For Pauli strings of up to 19 qubits, PACOX achieves speedups of up to 100 times compared with state-of-the-art CPU-based methods, while requiring significantly less memory and achieving a much lower power-delay product. These results demonstrate that PACOX delivers high computational speed with superior energy efficiency for Pauli-based workloads in hybrid quantum-classical systems."}
{"id": "2601.04441", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04441", "abs": "https://arxiv.org/abs/2601.04441", "authors": ["Matthew Landers", "Taylor W. Killian", "Thomas Hartvigsen", "Afsaneh Doryab"], "title": "Improving and Accelerating Offline RL in Large Discrete Action Spaces with Structured Policy Initialization", "comment": null, "summary": "Reinforcement learning in discrete combinatorial action spaces requires searching over exponentially many joint actions to simultaneously select multiple sub-actions that form coherent combinations. Existing approaches either simplify policy learning by assuming independence across sub-actions, which often yields incoherent or invalid actions, or attempt to learn action structure and control jointly, which is slow and unstable. We introduce Structured Policy Initialization (SPIN), a two-stage framework that first pre-trains an Action Structure Model (ASM) to capture the manifold of valid actions, then freezes this representation and trains lightweight policy heads for control. On challenging discrete DM Control benchmarks, SPIN improves average return by up to 39% over the state of the art while reducing time to convergence by up to 12.8$\\times$."}
{"id": "2601.04830", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04830", "abs": "https://arxiv.org/abs/2601.04830", "authors": ["Thibault Scoquart", "Hugo Perrin", "Kyrylo Snizhko"], "title": "Noise tailoring for error mitigation and for diagnozing digital quantum computers", "comment": "27 pages, 16 figures", "summary": "Error mitigation (EM) methods are crucial for obtaining reliable results in the realm of noisy intermediate-scale quantum (NISQ) computers, where noise significantly impacts output accuracy. Some EM protocols are particularly efficient for specific types of noise. Yet the noise in the actual hardware may not align with that. In this article, we introduce Noise Tailoring (NT) -- an innovative strategy designed to modify the structure of the noise associated with two-qubit gates through statistical sampling. We perform classical emulation of the protocol behavior and find that the NT+EM results can be up to 5 times more accurate than the results of EM alone for realistic Pauli noise acting on two-qubit gates. At the same time, on actual IBM quantum computers, the NT method falls victim to various small error sources beyond Markovian Pauli noise. We propose to use the NT method for characterizing such error sources on quantum computers in order to inform hardware development."}
{"id": "2601.04447", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04447", "abs": "https://arxiv.org/abs/2601.04447", "authors": ["Gal Fybish", "Teo Susnjak"], "title": "When Predictions Shape Reality: A Socio-Technical Synthesis of Performative Predictions in Machine Learning", "comment": null, "summary": "Machine learning models are increasingly used in high-stakes domains where their predictions can actively shape the environments in which they operate, a phenomenon known as performative prediction. This dynamic, in which the deployment of the model influences the very outcome it seeks to predict, can lead to unintended consequences, including feedback loops, performance issues, and significant societal risks. While the literature in the field has grown rapidly in recent years, a socio-technical synthesis that systemises the phenomenon concepts and provides practical guidance has been lacking. This Systematisation of Knowledge (SoK) addresses this gap by providing a comprehensive review of the literature on performative predictions. We provide an overview of the primary mechanisms through which performativity manifests, present a typology of associated risks, and survey the proposed solutions offered in the literature. Our primary contribution is the ``Performative Strength vs. Impact Matrix\" assessment framework. This practical tool is designed to help practitioners assess the potential influence and severity of performativity on their deployed predictive models and select the appropriate level of algorithmic or human intervention."}
{"id": "2601.04848", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04848", "abs": "https://arxiv.org/abs/2601.04848", "authors": ["Mariagrazia Iuliano", "Nicolas Demetriou", "H. Benjamin van Ommen", "Constantijn Karels", "Tim H. Taminiau", "Ronald Hanson"], "title": "Unconditionally teleported quantum gates between remote solid-state qubit registers", "comment": null, "summary": "Quantum networks connecting quantum processing nodes via photonic links enable distributed and modular quantum computation. In this framework, quantum gates between remote qubits can be realized using quantum teleportation protocols. The essential requirements for such non-local gates are remote entanglement, local quantum logic within each processor, and classical communication between nodes to perform operations based on measurement outcomes. Here, we demonstrate an unconditional Controlled-NOT quantum gate between remote diamond-based qubit devices. The control and target qubits are Carbon-13 nuclear spins, while NV electron spins enable local logic, readout, and remote entanglement generation. We benchmark the system by creating a Greenberger-Horne-Zeilinger state, showing genuine 4-partite entanglement shared between nodes. Using deterministic logic, single-shot readout, and real-time feed-forward, we implement non-local gates without post-selection. These results demonstrate a key capability for solid-state quantum networks, enabling exploration of distributed quantum computing and testing of complex network protocols on fully integrated systems."}
{"id": "2601.04449", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04449", "abs": "https://arxiv.org/abs/2601.04449", "authors": ["Daniel Sierra-Botero", "Ana Molina-Taborda", "Leonardo Espinosa-Leal", "Alexander Karpenko", "Alejandro Hernandez", "Olga Lopez-Acevedo"], "title": "Explainable Admission-Level Predictive Modeling for Prolonged Hospital Stay in Elderly Populations: Challenges in Low- and Middle-Income Countries", "comment": "23 pages, 6 figures", "summary": "Prolonged length of stay (pLoS) is a significant factor associated with the risk of adverse in-hospital events. We develop and explain a predictive model for pLos using admission-level patient and hospital administrative data. The approach includes a feature selection method by selecting non-correlated features with the highest information value. The method uses features weights of evidence to select a representative within cliques from graph theory. The prognosis study analyzed the records from 120,354 hospital admissions at the Hospital Alma Mater de Antioquia between January 2017 and March 2022. After a cleaning process the dataset was split into training (67%), test (22%), and validation (11%) cohorts. A logistic regression model was trained to predict the pLoS in two classes: less than or greater than 7 days. The performance of the model was evaluated using accuracy, precision, sensitivity, specificity, and AUC-ROC metrics. The feature selection method returns nine interpretable variables, enhancing the models' transparency. In the validation cohort, the pLoS model achieved a specificity of 0.83 (95% CI, 0.82-0.84), sensitivity of 0.64 (95% CI, 0.62-0.65), accuracy of 0.76 (95% CI, 0.76-0.77), precision of 0.67 (95% CI, 0.66-0.69), and AUC-ROC of 0.82 (95% CI, 0.81-0.83). The model exhibits strong predictive performance and offers insights into the factors that influence prolonged hospital stays. This makes it a valuable tool for hospital management and for developing future intervention studies aimed at reducing pLoS."}
{"id": "2601.04856", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04856", "abs": "https://arxiv.org/abs/2601.04856", "authors": ["Zeyu Liu", "Pengfei Zhang"], "title": "Distinguishing Coherent and Incoherent Errors in Multi-Round Time-Reversed Dynamics via Scramblons", "comment": "8 pages, 4 figures + Supplementary Material", "summary": "Despite the rapid development of quantum science and technology, errors are inevitable and play a crucial role in quantum simulation and quantum computation. In quantum chaotic systems, coherent errors arising from imperfect Hamiltonian control and incoherent errors induced by coupling to the environment are both exponentially amplified during time evolution due to information scrambling. A fundamental question is how these two classes of errors imprint distinct signatures on the emergent irreversibility of many-body dynamics. In this Letter, we address this question by investigating multi-round time-reversed dynamics in the presence of both coherent and incoherent errors. By applying scramblon theory, we obtain closed-form expressions for the Loschmidt echo over different rounds of time-reversed evolution. For incoherent errors, the error accumulates linearly with the number of rounds, whereas coherent errors exhibit a crossover from quadratic to linear accumulation. These predictions are explicitly verified using the solvable Sachdev-Ye-Kitaev model. Our results provide a theoretical foundation for characterizing and calibrating coherent and incoherent errors in reversed dynamics, with particular relevance to nuclear magnetic resonance systems."}
{"id": "2601.04458", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04458", "abs": "https://arxiv.org/abs/2601.04458", "authors": ["Jiayi Zhang", "Conrad Borchers", "Clayton Cohn", "Namrata Srivastava", "Caitlin Snyder", "Siyuan Guo", "Ashwin T S", "Naveeduddin Mohammed", "Haley Noh", "Gautam Biswas"], "title": "Using Large Language Models to Detect Socially Shared Regulation of Collaborative Learning", "comment": "Short research paper accepted at Learning Analytics and Knowledge (LAK '26)", "summary": "The field of learning analytics has made notable strides in automating the detection of complex learning processes in multimodal data. However, most advancements have focused on individualized problem-solving instead of collaborative, open-ended problem-solving, which may offer both affordances (richer data) and challenges (low cohesion) to behavioral prediction. Here, we extend predictive models to automatically detect socially shared regulation of learning (SSRL) behaviors in collaborative computational modeling environments using embedding-based approaches. We leverage large language models (LLMs) as summarization tools to generate task-aware representations of student dialogue aligned with system logs. These summaries, combined with text-only embeddings, context-enriched embeddings, and log-derived features, were used to train predictive models. Results show that text-only embeddings often achieve stronger performance in detecting SSRL behaviors related to enactment or group dynamics (e.g., off-task behavior or requesting assistance). In contrast, contextual and multimodal features provide complementary benefits for constructs such as planning and reflection. Overall, our findings highlight the promise of embedding-based models for extending learning analytics by enabling scalable detection of SSRL behaviors, ultimately supporting real-time feedback and adaptive scaffolding in collaborative learning environments that teachers value."}
{"id": "2601.04880", "categories": ["quant-ph", "math-ph"], "pdf": "https://arxiv.org/pdf/2601.04880", "abs": "https://arxiv.org/abs/2601.04880", "authors": ["Tobias Starke"], "title": "Quantenlogische Systeme und Tensorproduktraeume", "comment": "in German language", "summary": "In this work we present an intuitive construction of the quantum logical axiomatic system provided by George Mackey. The goal of this work is a detailed discussion of the results from the paper 'Physical justification for using the tensor product to describe two quantum systems as one joint system' [1] published by Diederik Aerts and Ingrid Daubechies. This means that we want to show how certain composed physical systems from classical and quantum mechanics should be described logically. To reach this goal, we will, like in [1], discuss a special class of axiomatically defined composed physical systems. With the help of certain results from lattice and c-morphism theory (see [2] and [23]), we will present a detailed proof of the statement, that in the quantum mechanical case, a composed physical system must be described via a tensor product space."}
{"id": "2601.04462", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04462", "abs": "https://arxiv.org/abs/2601.04462", "authors": ["Kevin Zhang", "Yixin Wang"], "title": "Meta-probabilistic Modeling", "comment": null, "summary": "While probabilistic graphical models can discover latent structure in data, their effectiveness hinges on choosing well-specified models. Identifying such models is challenging in practice, often requiring iterative checking and revision through trial and error. To this end, we propose meta-probabilistic modeling (MPM), a meta-learning algorithm that learns generative model structure directly from multiple related datasets. MPM uses a hierarchical architecture where global model specifications are shared across datasets while local parameters remain dataset-specific. For learning and inference, we propose a tractable VAE-inspired surrogate objective, and optimize it through bi-level optimization: local variables are updated analytically via coordinate ascent, while global parameters are trained with gradient-based methods. We evaluate MPM on object-centric image modeling and sequential text modeling, demonstrating that it adapts generative models to data while recovering meaningful latent representations."}
{"id": "2601.04905", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2601.04905", "abs": "https://arxiv.org/abs/2601.04905", "authors": ["Sachin Sonkar", "Ramandeep S. Johal"], "title": "Virtual temperatures as a key quantifier for passive states in quantum thermodynamic processes", "comment": "20 pages, 3 figures. Comments are welcome", "summary": "We analyze the role of virtual temperatures for passive quantum states through the lens of majorization theory. A mean temperature over the virtual temperatures of adjacent energy levels is defined to compare the passive states of the system resulting from isoenergetic and isoentropic transformations. The role of the minimum and the maximum (min-max) values of the virtual temperatures in determining the direction of heat flow between the system and the environment is argued based on majorization relations. We characterize the intermediate passive states in a quantum Otto engine using these virtual temperatures and derive an upper bound for the Otto efficiency that can be expressed in terms of the min-max virtual temperatures of the working medium. An explicit example of the coupled-spins system is worked out. Moreover, virtual temperatures serve to draw interesting parallels between the quantum thermodynamic processes and their classical counterparts. Thus, virtual temperature emerges as a key operational quantity linking passivity and majorization to the optimal performance of quantum thermal machines."}
{"id": "2601.04480", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04480", "abs": "https://arxiv.org/abs/2601.04480", "authors": ["Wes Gurnee", "Emmanuel Ameisen", "Isaac Kauvar", "Julius Tarng", "Adam Pearce", "Chris Olah", "Joshua Batson"], "title": "When Models Manipulate Manifolds: The Geometry of a Counting Task", "comment": null, "summary": "Language models can perceive visual properties of text despite receiving only sequences of tokens-we mechanistically investigate how Claude 3.5 Haiku accomplishes one such task: linebreaking in fixed-width text. We find that character counts are represented on low-dimensional curved manifolds discretized by sparse feature families, analogous to biological place cells. Accurate predictions emerge from a sequence of geometric transformations: token lengths are accumulated into character count manifolds, attention heads twist these manifolds to estimate distance to the line boundary, and the decision to break the line is enabled by arranging estimates orthogonally to create a linear decision boundary. We validate our findings through causal interventions and discover visual illusions--character sequences that hijack the counting mechanism. Our work demonstrates the rich sensory processing of early layers, the intricacy of attention algorithms, and the importance of combining feature-based and geometric views of interpretability."}
{"id": "2601.04949", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04949", "abs": "https://arxiv.org/abs/2601.04949", "authors": ["Xin Liu", "Zhicheng Luo", "Kaibiao Qin", "Jiawang Liu", "Zhenrong Zhang", "Kejin Wei"], "title": "High-Rate Free-Running Reference-Frame-Independent Measurement-Device-Independent Quantum Key Distribution with Classified Distillation", "comment": "5 pages, 4 figures", "summary": "Reference-frame-independent measurement-device-independent quantum key distribution (RFI-MDI-QKD) eliminates detector side-channel attacks and avoids reference-frame calibration. While its feasibility has been widely demonstrated, existing implementations typically assume fixed or slowly drifting reference-frame misalignment, conditions rarely satisfied outside the laboratory. In realistic environments, rapid and free-running reference-frame variations can severely degrade both the key rate and transmission distance of conventional RFI-MDI-QKD. Here we propose a free-running RFI-MDI-QKD protocol that maintains high-rate key generation under rapid reference-frame variations. By introducing a classification-distillation method that reclassifies total detection events, secure keys can be extracted without modifying the experimental setup. Our protocol achieves a key rate more than nine times higher than the best previous RFI-MDI-QKD scheme and tolerates channel losses exceeding 24 dB, where earlier approaches fail. These results enable practical quantum key distribution on mobile platforms, including satellite-to-ground links and airborne nodes."}
{"id": "2601.04483", "categories": ["cs.LG", "cs.AI", "cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.04483", "abs": "https://arxiv.org/abs/2601.04483", "authors": ["Yongjun Kim", "Hyeongjun Park", "Hwanjin Kim", "Junil Choi"], "title": "Hybrid Federated Learning for Noise-Robust Training", "comment": null, "summary": "Federated learning (FL) and federated distillation (FD) are distributed learning paradigms that train UE models with enhanced privacy, each offering different trade-offs between noise robustness and learning speed. To mitigate their respective weaknesses, we propose a hybrid federated learning (HFL) framework in which each user equipment (UE) transmits either gradients or logits, and the base station (BS) selects the per-round weights of FL and FD updates. We derive convergence of HFL framework and introduce two methods to exploit degrees of freedom (DoF) in HFL, which are (i) adaptive UE clustering via Jenks optimization and (ii) adaptive weight selection via a damped Newton method. Numerical results show that HFL achieves superior test accuracy at low SNR when both DoF are exploited."}
{"id": "2601.04975", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04975", "abs": "https://arxiv.org/abs/2601.04975", "authors": ["Guillaume Beaulieu", "Jun-Zhe Chen", "Marco Scigliuzzo", "Othmane Benhayoune-Khadraoui", "Alex A. Chapple", "Peter A. Spring", "Alexandre Blais", "Pasquale Scarlino"], "title": "Fast, high-fidelity Transmon readout with intrinsic Purcell protection via nonperturbative cross-Kerr coupling", "comment": "22 pages, 22 figures", "summary": "Dispersive readout of superconducting qubits relies on a transverse capacitive coupling that hybridizes the qubit with the readout resonator, subjecting the qubit to Purcell decay and measurement-induced state transitions (MIST). Despite the widespread use of Purcell filters to suppress qubit decay and near-quantum-limited amplifiers, dispersive readout often lags behind single- and two-qubit gates in both speed and fidelity. Here, we experimentally demonstrate junction readout, a simple readout architecture that realizes a strong qubit-resonator cross-Kerr interaction without relying on a transverse coupling. This interaction is achieved by coupling a transmon qubit to its readout resonator through both a capacitance and a Josephson junction. By varying the qubit frequency, we show that this hybrid coupling provides intrinsic Purcell protection and enhanced resilience to MIST, enabling readout at high photon numbers. While junction readout is compatible with conventional linear measurement, in this work we exploit the nonlinear coupling to intentionally engineer a large Kerr nonlinearity in the resonator, enabling bifurcation-based readout. Using this approach, we achieve a 99.4 % assignment fidelity with a 68 ns integration time and a 98.4 % QND fidelity without an external Purcell filter or a near-quantum-limited amplifier. These results establish the junction readout architecture with bifurcation-based readout as a scalable and practical alternative to dispersive readout, enabling fast, high-fidelity qubit measurement with reduced hardware overhead."}
{"id": "2601.04498", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.04498", "abs": "https://arxiv.org/abs/2601.04498", "authors": ["Yinghao Tang", "Xueding Liu", "Boyuan Zhang", "Tingfeng Lan", "Yupeng Xie", "Jiale Lao", "Yiyao Wang", "Haoxuan Li", "Tingting Gao", "Bo Pan", "Luoxuan Weng", "Xiuqi Huang", "Minfeng Zhu", "Yingchaojie Feng", "Yuyu Luo", "Wei Chen"], "title": "IGenBench: Benchmarking the Reliability of Text-to-Infographic Generation", "comment": null, "summary": "Infographics are composite visual artifacts that combine data visualizations with textual and illustrative elements to communicate information. While recent text-to-image (T2I) models can generate aesthetically appealing images, their reliability in generating infographics remains unclear. Generated infographics may appear correct at first glance but contain easily overlooked issues, such as distorted data encoding or incorrect textual content. We present IGENBENCH, the first benchmark for evaluating the reliability of text-to-infographic generation, comprising 600 curated test cases spanning 30 infographic types. We design an automated evaluation framework that decomposes reliability verification into atomic yes/no questions based on a taxonomy of 10 question types. We employ multimodal large language models (MLLMs) to verify each question, yielding question-level accuracy (Q-ACC) and infographic-level accuracy (I-ACC). We comprehensively evaluate 10 state-of-the-art T2I models on IGENBENCH. Our systematic analysis reveals key insights for future model development: (i) a three-tier performance hierarchy with the top model achieving Q-ACC of 0.90 but I-ACC of only 0.49; (ii) data-related dimensions emerging as universal bottlenecks (e.g., Data Completeness: 0.21); and (iii) the challenge of achieving end-to-end correctness across all models. We release IGENBENCH at https://igen-bench.vercel.app/."}
{"id": "2601.04976", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04976", "abs": "https://arxiv.org/abs/2601.04976", "authors": ["Ting Lin", "Zhihua Chen", "Kai Wu", "Zhihua Guo", "Zhihao Ma", "Shao-Ming Fei"], "title": "Machine learning-aided direct estimation of coherence and entanglement for unknown states", "comment": null, "summary": "Quantum coherence and entanglement are fundamental resources in quantum technologies, yet their efficient estimation for unknown states by employing minimal resources in experimental settings remains challenging, particularly in high-dimensional systems. We present a machine learning approach based on support vector regression (SVR) that directly estimates the coherence measures and the geometric measure of quantum entanglement using minimal experimental resources. Our method requires only the diagonal entries of the density matrix, along with the traces of the squared and cubed density matrices for quantum coherence, and additionally along with the traces of the squared and cubed reduced density matrix for estimating quantum entanglement. These quantities can be obtained through random measurements or a hybrid quantum-classical framework. This approach significantly reduces the resource overhead compared to quantum state tomography while maintaining high accuracy. {Furthermore, the support vector quantile regression (SVQR) with pinball loss is employed to prevent SVR overestimation. This model not only ensures that over 95\\% of predictions are conservative lower bounds in most cases, but also maintains this lower-bound reliability for over 93\\% of predictions, despite 2\\% perturbations in the input features.} The proposed technique provides a practical and scalable tool for characterizing quantum resources across computation, communication, and metrology applications."}
{"id": "2601.04506", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2601.04506", "abs": "https://arxiv.org/abs/2601.04506", "authors": ["Fang Wu", "Zhengyuan Zhou", "Shuting Jin", "Xiangxiang Zeng", "Jure Leskovec", "Jinbo Xu"], "title": "Surface-based Molecular Design with Multi-modal Flow Matching", "comment": null, "summary": "Therapeutic peptides show promise in targeting previously undruggable binding sites, with recent advancements in deep generative models enabling full-atom peptide co-design for specific protein receptors. However, the critical role of molecular surfaces in protein-protein interactions (PPIs) has been underexplored. To bridge this gap, we propose an omni-design peptides generation paradigm, called SurfFlow, a novel surface-based generative algorithm that enables comprehensive co-design of sequence, structure, and surface for peptides. SurfFlow employs a multi-modality conditional flow matching (CFM) architecture to learn distributions of surface geometries and biochemical properties, enhancing peptide binding accuracy. Evaluated on the comprehensive PepMerge benchmark, SurfFlow consistently outperforms full-atom baselines across all metrics. These results highlight the advantages of considering molecular surfaces in de novo peptide discovery and demonstrate the potential of integrating multiple protein modalities for more effective therapeutic peptide discovery."}
{"id": "2601.04981", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2601.04981", "abs": "https://arxiv.org/abs/2601.04981", "authors": ["Hanna T. Fridman", "Rotem Malkinson", "Amir Hen", "Shira Yochelis", "Yossi Paltiel", "Nir Bar-gill"], "title": "Signatures of Spin Coherence in Chiral Coupled Quantum Dots", "comment": null, "summary": "Chiral-induced spin selectivity (CISS) enables spin selectivity of charge carriers in chiral molecular systems without magnetic materials. While spin selectivity has been widely investigated, its quantum coherence has not yet been explored. Here, we investigate spin-dependent photoluminescence (PL) dynamics in multilayer quantum-dot (QD) assemblies coupled by chiral linkers. Using circularly polarized excitation in the presence of an external magnetic field, we observe a pronounced modulation of the PL lifetime that depends on the magnetic field magnitude and geometry. The lifetime difference between left- and right-circularly polarized excitations exhibits a field-angle dependence, consistent with spin precession driven by the transverse magnetic-field component relative to the chiral axis. A model incorporating coupled spin precession and decay processes reproduces the experimental trends. These results establish chiral QD assemblies as a room-temperature platform for probing quantum coherent manifestations of the CISS effect, with implications for spintronic and quantum technologies."}
{"id": "2601.04521", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04521", "abs": "https://arxiv.org/abs/2601.04521", "authors": ["Jacob Ede Levine", "Yun Lyan Luo", "Sai Chandra Kosaraju"], "title": "TSSR: Two-Stage Swap-Reward-Driven Reinforcement Learning for Character-Level SMILES Generation", "comment": "Under Review", "summary": "The design of reliable, valid, and diverse molecules is fundamental to modern drug discovery, as improved molecular generation supports efficient exploration of the chemical space for potential drug candidates and reduces the cost of early design efforts. Despite these needs, current chemical language models that generate molecules as SMILES strings are vulnerable to compounding token errors: many samples are unparseable or chemically implausible, and hard constraints meant to prevent failure can restrict exploration. To address this gap, we introduce TSSR, a Two-Stage, Swap-Reward-driven reinforcement learning (RL) framework for character-level SMILES generation. Stage one rewards local token swaps that repair syntax, promoting transitions from invalid to parseable strings. Stage two provides chemistry-aware feedback from RDKit diagnostics, rewarding reductions in valence, aromaticity, and connectivity issues. The reward decomposes into interpretable terms (swap efficiency, error reduction, distance to validity), is model agnostic, and requires no task-specific labels or hand-crafted grammars. We evaluated TSSR on the MOSES benchmark using a GRU policy trained with PPO in both pure RL (P-RL) from random initialization and fine-tuning RL (F-RL) starting from a pretrained chemical language model, assessing 10,000 generated SMILES per run. In P-RL, TSSR significantly improves syntactic validity, chemical validity, and novelty. In F-RL, TSSR preserves drug-likeness and synthesizability while increasing validity and novelty. Token-level analysis shows that syntax edits and chemistry fixes act jointly to reduce RDKit detected errors. TSSR converts a sparse terminal objective into a denser and more interpretable reward, improving both syntactic and chemical quality without reducing diversity. TSSR is dataset-agnostic and can be adapted to various reinforcement learning approaches."}
{"id": "2601.04983", "categories": ["quant-ph", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.04983", "abs": "https://arxiv.org/abs/2601.04983", "authors": ["Rupayan Bhattacharjee", "Sergi Abadal", "Carmen G. Almudever", "Eduard Alarcon"], "title": "Quantum Neural Network Training and Inference with Low Resolution Control Electronics", "comment": "5 pages, 4 figures", "summary": "Scaling quantum computers requires tight integration of cryogenic control electronics with quantum processors, where Digital-to-Analog Converters (DACs) face severe power and area constraints. We investigate quantum neural network (QNN) training and inference under finite DAC resolution constraints across various DAC resolutions. Pre-trained QNNs achieve accuracy nearly indistinguishable from infinite-precision baselines when deployed on quantum systems with 6-bit DAC control electronics, exhibiting an elbow curve with diminishing returns beyond 4 bits. However, training under quantization reveals gradient deadlock below 12-bit resolution as gradient magnitudes fall below quantization step sizes. We introduce temperature-controlled stochasticity that overcomes this through probabilistic parameter updates, enabling successful training at 4-10 bit resolutions that remarkably matches or exceeds infinite-precision baseline performance. Our findings demonstrate that low-resolution control electronics need not compromise QML performance, enabling significant power and area reduction in cryogenic control systems for practical deployment as quantum hardware scales."}
{"id": "2601.04537", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04537", "abs": "https://arxiv.org/abs/2601.04537", "authors": ["Tianle Wang", "Zhongyuan Wu", "Shenghao Jin", "Hao Xu", "Wei Chen", "Ning Miao"], "title": "Not All Steps are Informative: On the Linearity of LLMs' RLVR Training", "comment": "pre-print", "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a central component of large language model (LLM) post-training. Unlike supervised fine-tuning (SFT), RLVR lets an LLM generate multiple candidate solutions and reinforces those that lead to a verifiably correct final answer. However, in practice, RLVR often requires thousands of training steps to reach strong performance, incurring substantial computation largely attributed to prolonged exploration. In this work, we make a surprising observation: during RLVR, LLMs evolve in a strongly linear manner. Specifically, both model weights and model output log-probabilities exhibit strong linear correlations with RL training steps. This suggests that RLVR predominantly amplifies trends that emerge early in training, rather than continuously discovering new behaviors throughout the entire optimization trajectory. Motivated by this linearity, we investigate whether future model states can be predicted from intermediate checkpoints via extrapolation, avoiding continued expensive training. We show that Weight Extrapolation produces models with performance comparable to standard RL training while requiring significantly less computation. Moreover, Logits Extrapolation consistently outperforms continued RL training on all four benchmarks by extrapolating beyond the step range where RL training remains stable."}
{"id": "2601.04998", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04998", "abs": "https://arxiv.org/abs/2601.04998", "authors": ["Yiting Mao", "Peigeng Zhong", "Haiqing Lin", "Xiaoqun Wang", "Shijie Hu"], "title": "Encoding complex-balanced thermalization in quantum circuits", "comment": "Main Text (4 pages + 4 figures), End Matter (1 page), Supplemental Material (3 pages + 2 figures)", "summary": "We propose a protocol for effectively implementing complex-balanced thermalization via Markovian processes on a quantum-circuit platform that couples the system with engineered reservoir qubits. The non-orthogonality of qubit eigenstates facilitates non-uniform heating through a modified Kubo-Martin-Schwinger relation, while simultaneously supports amplification-dissipation dynamics by violating microscopic time-reversibility. This offers a new approach to realizing out-of-equilibrium states at given temperatures. We show two applications of this platform: temporally-correlated dichromatic emission and Liouvillian exception point protected quantum synchronization at finite temperatures, both of which are challenging to achieve with conventional thermal reservoirs."}
{"id": "2601.04542", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.04542", "abs": "https://arxiv.org/abs/2601.04542", "authors": ["Mengmeng Zhu", "Yuxuan Sun", "Yukuan Jia", "Wei Chen", "Bo Ai", "Sheng Zhou"], "title": "Timeliness-Oriented Scheduling and Resource Allocation in Multi-Region Collaborative Perception", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Collaborative perception (CP) is a critical technology in applications like autonomous driving and smart cities. It involves the sharing and fusion of information among sensors to overcome the limitations of individual perception, such as blind spots and range limitations. However, CP faces two primary challenges. First, due to the dynamic nature of the environment, the timeliness of the transmitted information is critical to perception performance. Second, with limited computational power at the sensors and constrained wireless bandwidth, the communication volume must be carefully designed to ensure feature representations are both effective and sufficient. This work studies the dynamic scheduling problem in a multi-region CP scenario, and presents a Timeliness-Aware Multi-region Prioritized (TAMP) scheduling algorithm to trade-off perception accuracy and communication resource usage. Timeliness reflects the utility of information that decays as time elapses, which is manifested by the perception performance in CP tasks. We propose an empirical penalty function that maps the joint impact of Age of Information (AoI) and communication volume to perception performance. Aiming to minimize this timeliness-oriented penalty in the long-term, and recognizing that scheduling decisions have a cumulative effect on subsequent system states, we propose the TAMP scheduling algorithm. TAMP is a Lyapunov-based optimization policy that decomposes the long-term average objective into a per-slot prioritization problem, balancing the scheduling worth against resource cost. We validate our algorithm in both intersection and corridor scenarios with the real-world Roadside Cooperative perception (RCooper) dataset. Extensive simulations demonstrate that TAMP outperforms the best-performing baseline, achieving an Average Precision (AP) improvement of up to 27% across various configurations."}
{"id": "2601.05013", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.05013", "abs": "https://arxiv.org/abs/2601.05013", "authors": ["Mohammad Abdullah Sadi", "Tiamike Dudley", "Luca Basso", "Thomas Poirier", "James H. Edgar", "Jacob Henshaw", "Peter A. Bermel", "Yong P. Chen", "Andrew Mounce"], "title": "Landau Zener Interaction Enhanced Quantum Sensing in Spin Defects of Hexagonal Boron Nitride", "comment": null, "summary": "Negatively charged boron vacancies (V$_{\\text{B}}^{-}$) in hexagonal boron nitride (hBN) comprise a promising quantum sensing platform, optically addressable at room temperature and transferrable onto samples. However, broad hyperfine-split spin transitions of the ensemble pose challenges for quantum sensing with conventional resonant excitation due to limited spectral coverage. While isotopically enriched hBN using $^{10}$B and $^{15}$N isotopes (h$^{10}$B$^{15}$N) exhibits sharper spectral features, significant inhomogeneous broadening persists. We demonstrate that, implemented via frequency modulation on an FPGA, a frequency-ramped microwave pulse achieves around 4-fold greater $|0\\rangle\\rightarrow|-1\\rangle$ spin-state population transfer and thus contrast than resonant microwave excitation and thus 16-fold shorter measurement time for spin relaxation based quantum sensing. Quantum dynamics simulations reveal that an effective two-state Landau-Zener model captures the complex relationship between population inversion and pulse length with relaxations incorporated. Our approach is robust and valuable for quantum relaxometry with spin defects in hBN in noisy environments."}
{"id": "2601.04550", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04550", "abs": "https://arxiv.org/abs/2601.04550", "authors": ["Zhiyan Zhou", "Junjie Liao", "Manho Zhang", "Yingyi Liao", "Ziai Wang"], "title": "GEnSHIN: Graphical Enhanced Spatio-temporal Hierarchical Inference Network for Traffic Flow Prediction", "comment": null, "summary": "With the acceleration of urbanization, intelligent transportation systems have an increasing demand for accurate traffic flow prediction. This paper proposes a novel Graph Enhanced Spatio-temporal Hierarchical Inference Network (GEnSHIN) to handle the complex spatio-temporal dependencies in traffic flow prediction. The model integrates three innovative designs: 1) An attention-enhanced Graph Convolutional Recurrent Unit (GCRU), which strengthens the modeling capability for long-term temporal dependencies by introducing Transformer modules; 2) An asymmetric dual-embedding graph generation mechanism, which leverages the real road network and data-driven latent asymmetric topology to generate graph structures that better fit the characteristics of actual traffic flow; 3) A dynamic memory bank module, which utilizes learnable traffic pattern prototypes to provide personalized traffic pattern representations for each sensor node, and introduces a lightweight graph updater during the decoding phase to adapt to dynamic changes in road network states. Extensive experiments on the public dataset METR-LA show that GEnSHIN achieves or surpasses the performance of comparative models across multiple metrics such as Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and Mean Absolute Percentage Error (MAPE). Notably, the model demonstrates excellent prediction stability during peak morning and evening traffic hours. Ablation experiments further validate the effectiveness of each core module and its contribution to the final performance."}
{"id": "2601.05036", "categories": ["quant-ph", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05036", "abs": "https://arxiv.org/abs/2601.05036", "authors": ["Milan Liepelt", "Julien Baglio"], "title": "Exponential capacity scaling of classical GANs compared to hybrid latent style-based quantum GANs", "comment": "34 pages, 7 figures, 7 tables", "summary": "Quantum generative modeling is a very active area of research in looking for practical advantage in data analysis. Quantum generative adversarial networks (QGANs) are leading candidates for quantum generative modeling and have been applied to diverse areas, from high-energy physics to image generation. The latent style-based QGAN, relying on a classical variational autoencoder to encode the input data into a latent space and then using a style-based QGAN for data generation has been proven to be efficient for image generation or drug design, hinting at the use of far less trainable parameters than their classical counterpart to achieve comparable performance, however this advantage has never been systematically studied. We present in this work the first comprehensive experimental analysis of this advantage of QGANS applied to SAT4 image generation, obtaining an exponential advantage in capacity scaling for a quantum generator in the hybrid latent style-based QGAN architecture. Careful tuning of the autoencoder is crucial to obtain stable, reliable results. Once this tuning is performed and defining training optimality as when the training is stable and the FID score is low and stable as well, the optimal capacity (or number of trainable parameters) of the classical discriminator scales exponentially with respect to the capacity of the quantum generator, and the same is true for the capacity of the classical generator. This hints toward a type of quantum advantage for quantum generative modeling."}
{"id": "2601.04555", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04555", "abs": "https://arxiv.org/abs/2601.04555", "authors": ["Shogo Nakayama", "Masahiro Okuda"], "title": "Improving Semi-Supervised Contrastive Learning via Entropy-Weighted Confidence Integration of Anchor-Positive Pairs", "comment": null, "summary": "Conventional semi-supervised contrastive learning methods assign pseudo-labels only to samples whose highest predicted class probability exceeds a predefined threshold, and then perform supervised contrastive learning using those selected samples. In this study, we propose a novel loss function that estimates the confidence of each sample based on the entropy of its predicted probability distribution and applies confidence-based adaptive weighting. This approach enables pseudo-label assignment even to samples that were previously excluded from training and facilitates contrastive learning that accounts for the confidence of both anchor and positive samples in a more principled manner. Experimental results demonstrate that the proposed method improves classification accuracy and achieves more stable learning performance even under low-label conditions."}
{"id": "2601.05046", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.05046", "abs": "https://arxiv.org/abs/2601.05046", "authors": ["Pritam Chattopadhyay", "Jonas F. G. Santos", "Avijit Misra"], "title": "Anomaly to Resource: The Mpemba Effect in Quantum Thermometry", "comment": null, "summary": "Quantum thermometry provides a key capability for nanoscale devices and quantum technologies, but most existing strategies rely on probes initialized near equilibrium. This equilibrium paradigm imposes intrinsic limitations: sensitivity is tied to long-time thermalization and often cannot be improved in fast, noisy, or nonstationary settings. In contrast, the \\textit{Mpemba effect}, the counterintuitive phenomenon where hotter states relax faster than colder ones, has mostly been viewed as a thermodynamic anomaly. Here, we bridge this gap by proving that Mpemba-type inversions generically yield a finite-time enhancement of the quantum Fisher information (QFI) for temperature estimation, thereby converting an anomalous relaxation effect into a concrete metrological resource. Through explicit analyses of two-level and $Λ$-level probes coupled to bosonic baths, we show that nonequilibrium initializations can transiently outperform both equilibrium strategies and colder states, realizing a \\emph{metrological Mpemba effect}. Our results establish anomalous relaxation as a general design principle for nonequilibrium quantum thermometry, enabling ultrafast and nanoscale sensing protocols that exploit, rather than avoid, transient dynamics."}
{"id": "2601.04563", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.04563", "abs": "https://arxiv.org/abs/2601.04563", "authors": ["Paul Pu Liang"], "title": "A Vision for Multisensory Intelligence: Sensing, Synergy, and Science", "comment": null, "summary": "Our experience of the world is multisensory, spanning a synthesis of language, sight, sound, touch, taste, and smell. Yet, artificial intelligence has primarily advanced in digital modalities like text, vision, and audio. This paper outlines a research vision for multisensory artificial intelligence over the next decade. This new set of technologies can change how humans and AI experience and interact with one another, by connecting AI to the human senses and a rich spectrum of signals from physiological and tactile cues on the body, to physical and social signals in homes, cities, and the environment. We outline how this field must advance through three interrelated themes of sensing, science, and synergy. Firstly, research in sensing should extend how AI captures the world in richer ways beyond the digital medium. Secondly, developing a principled science for quantifying multimodal heterogeneity and interactions, developing unified modeling architectures and representations, and understanding cross-modal transfer. Finally, we present new technical challenges to learn synergy between modalities and between humans and AI, covering multisensory integration, alignment, reasoning, generation, generalization, and experience. Accompanying this vision paper are a series of projects, resources, and demos of latest advances from the Multisensory Intelligence group at the MIT Media Lab, see https://mit-mi.github.io/."}
{"id": "2601.05077", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.05077", "abs": "https://arxiv.org/abs/2601.05077", "authors": ["Gumaro Rendon", "Stepan Smid"], "title": "Preconditioned Multivariate Quantum Solution Extraction", "comment": null, "summary": "Numerically solving partial differential equations is a ubiquitous computational task with broad applications in many fields of science. Quantum computers can potentially provide high-degree polynomial speed-ups for solving PDEs, however many algorithms simply end with preparing the quantum state encoding the solution in its amplitudes. Trying to access explicit properties of the solution naively with quantum amplitude estimation can subsequently diminish the potential speed-up. In this work, we present a technique for extracting a smooth positive function encoded in the amplitudes of a quantum state, which achieves the Heisenberg limit scaling. We improve upon previous methods by allowing higher dimensional functions, by significantly reducing the quantum complexity with respect to the number of qubits encoding the function, and by removing the dependency on the minimum of the function using preconditioning. Our technique works by sampling the cumulative distribution of the given function, fitting it with Chebyshev polynomials, and subsequently extracting a representation of the whole encoded function. Finally, we trial our method by carrying out small scale numerical simulations."}
{"id": "2601.04572", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04572", "abs": "https://arxiv.org/abs/2601.04572", "authors": ["Xiaowei Mao", "Huihu Ding", "Yan Lin", "Tingrui Wu", "Shengnan Guo", "Dazhuo Qiu", "Feiling Fang", "Jilin Hu", "Huaiyu Wan"], "title": "Spatial-Temporal Feedback Diffusion Guidance for Controlled Traffic Imputation", "comment": null, "summary": "Imputing missing values in spatial-temporal traffic data is essential for intelligent transportation systems. Among advanced imputation methods, score-based diffusion models have demonstrated competitive performance. These models generate data by reversing a noising process, using observed values as conditional guidance. However, existing diffusion models typically apply a uniform guidance scale across both spatial and temporal dimensions, which is inadequate for nodes with high missing data rates. Sparse observations provide insufficient conditional guidance, causing the generative process to drift toward the learned prior distribution rather than closely following the conditional observations, resulting in suboptimal imputation performance.\n  To address this, we propose FENCE, a spatial-temporal feedback diffusion guidance method designed to adaptively control guidance scales during imputation. First, FENCE introduces a dynamic feedback mechanism that adjusts the guidance scale based on the posterior likelihood approximations. The guidance scale is increased when generated values diverge from observations and reduced when alignment improves, preventing overcorrection. Second, because alignment to observations varies across nodes and denoising steps, a global guidance scale for all nodes is suboptimal. FENCE computes guidance scales at the cluster level by grouping nodes based on their attention scores, leveraging spatial-temporal correlations to provide more accurate guidance. Experimental results on real-world traffic datasets show that FENCE significantly enhances imputation accuracy."}
{"id": "2601.05113", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.05113", "abs": "https://arxiv.org/abs/2601.05113", "authors": ["Luis Colmenarez", "Remmy Zen", "Jan Olle", "Florian Marquardt", "Markus Müller"], "title": "Unitary fault-tolerant encoding of Pauli states in surface codes", "comment": null, "summary": "In fault-tolerant quantum computation, the preparation of logical states is a ubiquitous subroutine, yet significant challenges persist even for the simplest states required. In the present work, we present a unitary, scalable, distance-preserving encoding scheme for preparing Pauli eigenstates in surface codes. Unlike previous unitary approaches whose fault-distance remains constant with increasing code distance, our scheme ensures that the protection offered by the code is preserved during state preparation. Building on strategies discovered by reinforcement learning for the surface-17 code, we generalize the construction to arbitrary code distances and both rotated and unrotated surface codes. The proposed encoding relies only on geometrically local gates, and is therefore fully compatible with planar 2D qubit connectivity, and it achieves circuit depth scaling as $\\mathcal{O}(d)$, consistent with fundamental entanglement-generation bounds. We design explicit stabilizer-expanding circuits with and without ancilla-mediated connectivity and analyze their error-propagation behavior. Numerical simulations under depolarizing noise show that our unitary encoding without ancillas outperforms standard stabilizer-measurement-based schemes, reducing logical error rates by up to an order of magnitude. These results make the scheme particularly relevant for platforms such as trapped ions and neutral atoms, where measurements are costly relative to gates and idling noise is considerably weaker than gate noise. Our work bridges the gap between measurement-based and unitary encodings of surface-code states and opens new directions for distance-preserving state preparation in fault-tolerant quantum computation."}
{"id": "2601.04587", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04587", "abs": "https://arxiv.org/abs/2601.04587", "authors": ["Quang-Tu Pham", "Hoang-Dieu Vu", "Dinh-Dat Pham", "Hieu H. Pham"], "title": "FedKDX: Federated Learning with Negative Knowledge Distillation for Enhanced Healthcare AI Systems", "comment": null, "summary": "This paper introduces FedKDX, a federated learning framework that addresses limitations in healthcare AI through Negative Knowledge Distillation (NKD). Unlike existing approaches that focus solely on positive knowledge transfer, FedKDX captures both target and non-target information to improve model generalization in healthcare applications. The framework integrates multiple knowledge transfer techniques--including traditional knowledge distillation, contrastive learning, and NKD--within a unified architecture that maintains privacy while reducing communication costs. Through experiments on healthcare datasets (SLEEP, UCI-HAR, and PAMAP2), FedKDX demonstrates improved accuracy (up to 2.53% over state-of-the-art methods), faster convergence, and better performance on non-IID data distributions. Theoretical analysis supports NKD's contribution to addressing statistical heterogeneity in distributed healthcare data. The approach shows promise for privacy-sensitive medical applications under regulatory frameworks like HIPAA and GDPR, offering a balanced solution between performance and practical implementation requirements in decentralized healthcare settings. The code and model are available at https://github.com/phamdinhdat-ai/Fed_2024."}
{"id": "2601.05118", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2601.05118", "abs": "https://arxiv.org/abs/2601.05118", "authors": ["Ming Li", "Weizhou Cai", "Ziyue Hua", "Yifang Xu", "Yilong Zhou", "Zi-Jie Chen", "Xu-Bo Zou", "Guang-Can Guo", "Luyan Sun", "Chang-Ling Zou"], "title": "Scalable Generation of Macroscopic Fock States Exceeding 10,000 Photons", "comment": "6 pages, 3 figures", "summary": "The scalable preparation of bosonic quantum states with macroscopic excitations poses a fundamental challenge in quantum technologies, limited by control complexity and photon-loss rates that severely constrain prior theoretical and experimental efforts to merely dozens of excitations per mode. Here, based on the duality of the quantum state evolution in Fock state space and the optical wave-function propagation in a waveguide array, we introduce a Kerr-engineered multi-lens protocol in a single bosonic mode to deterministically generate Fock states exceeding $10,000$ photons. By optimizing phase and displacement operations across lens groups, our approach compensates for non-paraxial aberrations, achieving fidelities above $73\\%$ in numerical simulations for photon numbers up to $N=100,000$. Counterintuitively, the protocol's execution time scales as $N^{-1/2}$ with the target photon number $N$, exhibiting robustness against the photon loss. Our framework enables exploration of quantum-to-classical transitions of giant Fock states, paving the way for advanced quantum metrology with significant quantum gains, and error-corrected quantum information processing in high-dimensional Hilbert spaces."}
{"id": "2601.04592", "categories": ["cs.LG", "cs.SD", "math-ph"], "pdf": "https://arxiv.org/pdf/2601.04592", "abs": "https://arxiv.org/abs/2601.04592", "authors": ["Joonwon Seo", "Mariana Montiel"], "title": "Density Matrix RNN (DM-RNN): A Quantum Information Theoretic Framework for Modeling Musical Context and Polyphony", "comment": "Submitted to the 10th International Conference on Mathematics and Computation in Music (MCM 2026)", "summary": "Classical Recurrent Neural Networks (RNNs) summarize musical context into a deterministic hidden state vector, imposing an information bottleneck that fails to capture the inherent ambiguity in music. We propose the Density Matrix RNN (DM-RNN), a novel theoretical architecture utilizing the Density Matrix. This allows the model to maintain a statistical ensemble of musical interpretations (a mixed state), capturing both classical probabilities and quantum coherences. We rigorously define the temporal dynamics using Quantum Channels (CPTP maps). Crucially, we detail a parameterization strategy based on the Choi-Jamiolkowski isomorphism, ensuring the learned dynamics remain physically valid (CPTP) by construction. We introduce an analytical framework using Von Neumann Entropy to quantify musical uncertainty and Quantum Mutual Information (QMI) to measure entanglement between voices. The DM-RNN provides a mathematically rigorous framework for modeling complex, ambiguous musical structures."}
{"id": "2601.05131", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.05131", "abs": "https://arxiv.org/abs/2601.05131", "authors": ["Janek Denzler", "Jose Carrasco", "Jens Eisert", "Tommaso Guaita"], "title": "Simulation of noisy quantum circuits using frame representations", "comment": "17 pages, 5 figures", "summary": "One of the core research questions in the theory of quantum computing is to find out to what precise extent the classical simulation of a noisy quantum circuits is possible and where potential quantum advantages can set in. In this work, we introduce a unified framework for the classical simulation of quantum circuits based on frame theory, encompassing and generalizing a broad class of existing simulation strategies. Within this framework, the computational cost of a simulation algorithm is determined by the one-norm of an associated quasi-probability distribution, providing a common quantitative measure across different simulation approaches. This enables a comprehensive perspective on common methods for the simulation of noisy circuits based on different quantum resources, such as entanglement or non-stabilizerness. It further provides a clear scheme for generating novel classical simulation algorithms. Indeed, by exploring different choices of frames within this formalism and resorting to tools of convex optimization, we are able not only to obtain new insights and improved bounds for existing methods -- such as stabilizer state simulation or Pauli back-propagation -- but also to discover a new approach with an improved performance based on a generalization of the Pauli frame. We, thereby, show that classical simulation techniques can directly benefit from a perspective -- that of frames -- that goes beyond the traditional classification of quantum resources."}
{"id": "2601.04616", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04616", "abs": "https://arxiv.org/abs/2601.04616", "authors": ["Shuhan Zhang", "Zhi Wang", "Rui Gao", "Shuang Li"], "title": "DeepHalo: A Neural Choice Model with Controllable Context Effects", "comment": null, "summary": "Modeling human decision-making is central to applications such as recommendation, preference learning, and human-AI alignment. While many classic models assume context-independent choice behavior, a large body of behavioral research shows that preferences are often influenced by the composition of the choice set itself -- a phenomenon known as the context effect or Halo effect. These effects can manifest as pairwise (first-order) or even higher-order interactions among the available alternatives. Recent models that attempt to capture such effects either focus on the featureless setting or, in the feature-based setting, rely on restrictive interaction structures or entangle interactions across all orders, which limits interpretability. In this work, we propose DeepHalo, a neural modeling framework that incorporates features while enabling explicit control over interaction order and principled interpretation of context effects. Our model enables systematic identification of interaction effects by order and serves as a universal approximator of context-dependent choice functions when specialized to a featureless setting. Experiments on synthetic and real-world datasets demonstrate strong predictive performance while providing greater transparency into the drivers of choice."}
{"id": "2601.05158", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.05158", "abs": "https://arxiv.org/abs/2601.05158", "authors": ["Matilde Baroni", "Dominik Leichtle", "Ivan Šupić", "Damian Markham", "Marco Túlio Quintino"], "title": "Composable simultaneous purification: when all communication scenarios reduce to spatial correlations", "comment": "10 pages, 3 figures", "summary": "Bell non-locality is a powerful framework to distinguish classical, quantum and post-quantum resources, which relies on non-communicating players. Under which restriction can we have the same separations, if we allow for communication? Non-signalling state assemblages, and the fact that they can always be simultaneously purified, turned out to be the key element to restrict the simplest bipartite communication scenario, the prepare-and-measure, to the standard bipartite Bell scenario. Yet, many distinctive features of quantum theory are genuinely multipartite and cannot be reduced to two-party behaviour. In this work we are interested in extending this simultaneous purification inspired result to all multipartite communication schemes. As a first step, we unify and extend the simultaneous purification result from states to instruments and super-instruments, which are composable structures, and open up the possibility to explore more complex communication scenarios. Our main contribution is to establish that arbitrary compositions of non-signalling assemblages cannot escape the standard spatial quantum Bell correlations set. As a consequence, any interactive quantum realization of correlations outside of this set must involve at least one signalling assemblage of quantum operations, even when the resulting correlations are non-signalling."}
{"id": "2601.04670", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04670", "abs": "https://arxiv.org/abs/2601.04670", "authors": ["Akiyoshi Tomihari"], "title": "Learning Dynamics in RL Post-Training for Language Models", "comment": null, "summary": "Reinforcement learning (RL) post-training is a critical stage in modern language model development, playing a key role in improving alignment and reasoning ability. However, several phenomena remain poorly understood, including the reduction in output diversity. To gain a broader understanding of RL post-training, we analyze the learning dynamics of RL post-training from a perspective that has been studied in supervised learning but remains underexplored in RL. We adopt an empirical neural tangent kernel (NTK) framework and decompose the NTK into two components to characterize how RL updates propagate across training samples. Our analysis reveals that limited variability in feature representations can cause RL updates to systematically increase model confidence, providing an explanation for the commonly observed reduction in output diversity after RL post-training. Furthermore, we show that effective learning in this regime depends on rapidly shaping the classifier, which directly affects the gradient component of the NTK. Motivated by these insights, we propose classifier-first reinforcement learning (CF-RL), a simple two-stage training strategy that prioritizes classifier updates before standard RL optimization. Experimental results validate our theoretical analysis by demonstrating increased model confidence and accelerated optimization under CF-RL. Additional analysis shows that the mechanism underlying CF-RL differs from that of linear-probing-then-fine-tuning in supervised learning. Overall, our study formalizes the learning dynamics of RL post-training and motivates further analysis and improvement."}
{"id": "2601.05161", "categories": ["quant-ph", "cond-mat.mtrl-sci", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.05161", "abs": "https://arxiv.org/abs/2601.05161", "authors": ["Ioannis Kolotouros", "Adithya Sireesh", "Stuart Ferguson", "Sean Thrasher", "Petros Wallden", "Julien Michel"], "title": "Quantum Elastic Network Models and their Application to Graphene", "comment": "42 pages, 11 figures", "summary": "Molecular dynamics simulations are a central computational methodology in materials design for relating atomic composition to mechanical properties. However, simulating materials with atomic-level resolution on a macroscopic scale is infeasible on current classical hardware, even when using the simplest elastic network models (ENMs) that represent molecular vibrations as a network of coupled oscillators. To address this issue, we introduce Quantum Elastic Network Models (QENMs) and utilize the quantum algorithm of Babbush et al. (PRX, 2023), which offers an exponential advantage when simulating systems of coupled oscillators under some specific conditions and assumptions. Here, we demonstrate how our method enables the efficient simulation of planar materials. As an example, we apply our algorithm to the task of simulating a 2D graphene sheet. We analyze the exact complexity for initial-state preparation, Hamiltonian simulation, and measurement of this material, and provide two real-world applications: heat transfer and the out-of-plane rippling effect. We estimate that an atomistic simulation of a graphene sheet on the centimeter scale, classically requiring hundreds of petabytes of memory and prohibitive runtimes, could be encoded and simulated with as few as $\\sim 160$ logical qubits."}
{"id": "2601.04673", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.04673", "abs": "https://arxiv.org/abs/2601.04673", "authors": ["Aurghya Maiti", "Prateek Jain"], "title": "Estimating Causal Effects in Gaussian Linear SCMs with Finite Data", "comment": "Accepted at the Workshop on Scaling Up Intervention Models at the 42nd International Conference on Machine Learning (ICML 2025)", "summary": "Estimating causal effects from observational data remains a fundamental challenge in causal inference, especially in the presence of latent confounders. This paper focuses on estimating causal effects in Gaussian Linear Structural Causal Models (GL-SCMs), which are widely used due to their analytical tractability. However, parameter estimation in GL-SCMs is often infeasible with finite data, primarily due to overparameterization. To address this, we introduce the class of Centralized Gaussian Linear SCMs (CGL-SCMs), a simplified yet expressive subclass where exogenous variables follow standardized distributions. We show that CGL-SCMs are equally expressive in terms of causal effect identifiability from observational distributions and present a novel EM-based estimation algorithm that can learn CGL-SCM parameters and estimate identifiable causal effects from finite observational samples. Our theoretical analysis is validated through experiments on synthetic data and benchmark causal graphs, demonstrating that the learned models accurately recover causal distributions."}
{"id": "2601.05226", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.05226", "abs": "https://arxiv.org/abs/2601.05226", "authors": ["Giorgio Facelli", "Hamza Fawzi", "Omar Fawzi"], "title": "Fast convergence of Majorana Propagation for weakly interacting fermions", "comment": "22 pages, 2 figures", "summary": "Simulating the time dynamics of an observable under Hamiltonian evolution is one of the most promising candidates for quantum advantage as we do not expect efficient classical algorithms for this problem except in restricted settings. Here, we introduce such a setting by showing that Majorana Propagation, a simple algorithm combining Trotter steps and truncations, efficiently finds a low-degree approximation of the time-evolved observable as soon as such an approximation exists. This provides the first provable guarantee about Majorana Propagation for Hamiltonian evolution. As an application of this result, we prove that Majorana Propagation can efficiently simulate the time dynamics of any sparse quartic Hamiltonian up to time $t_{\\text{max}}(u)$ depending on the interaction strength $u$. For a time horizon $t \\leq t_{\\text{max}}(u)$, the runtime of the algorithm is $N^{O(\\log(t/\\varepsilon))}$ where $N$ is the number of Majorana modes and $\\varepsilon$ is the error measured in the normalized Frobenius norm. Importantly, in the limit of small $u$, $t_{\\text{max}}(u)$ goes to $+\\infty$, formalizing the intuition that the algorithm is accurate at all times when the Hamiltonian is quadratic."}
{"id": "2601.04686", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.04686", "abs": "https://arxiv.org/abs/2601.04686", "authors": ["Oluwatosin Oseni", "Shengjie Wang", "Jun Zhu", "Micah Corah"], "title": "Nightmare Dreamer: Dreaming About Unsafe States And Planning Ahead", "comment": "RSS'25: Multi-Objective Optimization and Planning in Robotics Workshop: 5 pages, 8 figures", "summary": "Reinforcement Learning (RL) has shown remarkable success in real-world applications, particularly in robotics control. However, RL adoption remains limited due to insufficient safety guarantees. We introduce Nightmare Dreamer, a model-based Safe RL algorithm that addresses safety concerns by leveraging a learned world model to predict potential safety violations and plan actions accordingly. Nightmare Dreamer achieves nearly zero safety violations while maximizing rewards. Nightmare Dreamer outperforms model-free baselines on Safety Gymnasium tasks using only image observations, achieving nearly a 20x improvement in efficiency."}
{"id": "2601.05231", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2601.05231", "abs": "https://arxiv.org/abs/2601.05231", "authors": ["Hui-Hang Chen", "Chiao-Hsuan Wang"], "title": "Scalable Suppression of XY Crosstalk by Pulse-Level Control in Superconducting Quantum Processors", "comment": "19 pages, 19 figures", "summary": "As superconducting quantum processors continue to scale, high-performance quantum control becomes increasingly critical. In densely integrated architectures, unwanted interactions between nearby qubits give rise to crosstalk errors that limit operational performance. In particular, direct exchange-type (XY) interactions are typically minimized by designing large frequency detunings between neighboring qubits at the hardware level. However, frequency crowding in large-scale systems ultimately restricts the achievable frequency separation. While such XY coupling facilitates entangling gate operations, its residual presence poses a key challenge during single-qubit controls. Here, we propose a scalable pulse-level control framework, incorporating frequency modulation (FM) and dynamical decoupling (DD), to suppress XY crosstalk errors. This framework operates independently of coupling strengths, reducing calibration overhead and naturally supporting multi-qubit connectivity. Numerical simulations show orders-of-magnitude reductions in infidelity for both idle and single-qubit gates in a two-qubit system. We further validate scalability in a five-qubit layout, where crosstalk between a central qubit and four neighbors is simultaneously suppressed. Our crosstalk suppression framework provides a practical route toward high-fidelity operation in dense superconducting architectures."}
{"id": "2601.04690", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04690", "abs": "https://arxiv.org/abs/2601.04690", "authors": ["Mir Rayat Imtiaz Hossain", "Leo Feng", "Leonid Sigal", "Mohamed Osama Ahmed"], "title": "Do LLMs Benefit from User and Item Embeddings in Recommendation Tasks?", "comment": "Presented in Multimodal Algorithmic Reasoning Workshop at NeurIPS 2025", "summary": "Large Language Models (LLMs) have emerged as promising recommendation systems, offering novel ways to model user preferences through generative approaches. However, many existing methods often rely solely on text semantics or incorporate collaborative signals in a limited manner, typically using only user or item embeddings. These methods struggle to handle multiple item embeddings representing user history, reverting to textual semantics and neglecting richer collaborative information. In this work, we propose a simple yet effective solution that projects user and item embeddings, learned from collaborative filtering, into the LLM token space via separate lightweight projector modules. A finetuned LLM then conditions on these projected embeddings alongside textual tokens to generate recommendations. Preliminary results show that this design effectively leverages structured user-item interaction data, improves recommendation performance over text-only LLM baselines, and offers a practical path for bridging traditional recommendation systems with modern LLMs."}
{"id": "2601.04413", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.04413", "abs": "https://arxiv.org/abs/2601.04413", "authors": ["Nausherwan Malik", "Zubair Khalid", "Muhammad Faryad"], "title": "Distribution-Guided and Constrained Quantum Machine Unlearning", "comment": "8 pages", "summary": "Machine unlearning aims to remove the influence of specific training data from a learned model without full retraining. While recent work has begun to explore unlearning in quantum machine learning, existing approaches largely rely on fixed, uniform target distributions and do not explicitly control the trade-off between forgetting and retained model behaviour. In this work, we propose a distribution-guided framework for class-level quantum machine unlearning that treats unlearning as a constrained optimization problem. Our method introduces a tunable target distribution derived from model similarity statistics, decoupling the suppression of forgotten-class confidence from assumptions about redistribution among retained classes. We further incorporate an anchor-based preservation constraint that explicitly maintains predictive behaviour on selected retained data, yielding a controlled optimization trajectory that limits deviation from the original model. We evaluate the approach on variational quantum classifiers trained on the Iris and Covertype datasets. Results demonstrate sharp suppression of forgotten-class confidence, minimal degradation of retained-class performance, and closer alignment with the gold retrained model baselines compared to uniform-target unlearning. These findings highlight the importance of target design and constraint-based formulations for reliable and interpretable quantum machine unlearning."}
{"id": "2601.04705", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04705", "abs": "https://arxiv.org/abs/2601.04705", "authors": ["Àngel Ruiz-Fas", "Carlos Granell", "José Francisco Ramos", "Joaquín Huerta", "Sergio Trilles"], "title": "A zone-based training approach for last-mile routing using Graph Neural Networks and Pointer Networks", "comment": "Accepted in SMF 2026. 8 pages, 3 figures", "summary": "Rapid e-commerce growth has pushed last-mile delivery networks to their limits, where small routing gains translate into lower costs, faster service, and fewer emissions. Classical heuristics struggle to adapt when travel times are highly asymmetric (e.g., one-way streets, congestion). A deep learning-based approach to the last-mile routing problem is presented to generate geographical zones composed of stop sequences to minimize last-mile delivery times.\n  The presented approach is an encoder-decoder architecture. Each route is represented as a complete directed graph whose nodes are stops and whose edge weights are asymmetric travel times. A Graph Neural Network encoder produces node embeddings that captures the spatial relationships between stops. A Pointer Network decoder then takes the embeddings and the route's start node to sequentially select the next stops, assigning a probability to each unvisited node as the next destination.\n  Cells of a Discrete Global Grid System which contain route stops in the training data are obtained and clustered to generate geographical zones of similar size in which the process of training and inference are divided. Subsequently, a different instance of the model is trained per zone only considering the stops of the training routes which are included in that zone.\n  This approach is evaluated using the Los Angeles routes from the 2021 Amazon Last Mile Routing Challenge. Results from general and zone-based training are compared, showing a reduction in the average predicted route length in the zone-based training compared to the general training. The performance improvement of the zone-based approach becomes more pronounced as the number of stops per route increases."}
{"id": "2601.04707", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.04707", "abs": "https://arxiv.org/abs/2601.04707", "authors": ["Irfan Ullah", "Young-Koo Lee"], "title": "MQ-GNN: A Multi-Queue Pipelined Architecture for Scalable and Efficient GNN Training", "comment": "IEEE Access 2025", "summary": "Graph Neural Networks (GNNs) are powerful tools for learning graph-structured data, but their scalability is hindered by inefficient mini-batch generation, data transfer bottlenecks, and costly inter-GPU synchronization. Existing training frameworks fail to overlap these stages, leading to suboptimal resource utilization. This paper proposes MQ-GNN, a multi-queue pipelined framework that maximizes training efficiency by interleaving GNN training stages and optimizing resource utilization. MQ-GNN introduces Ready-to-Update Asynchronous Consistent Model (RaCoM), which enables asynchronous gradient sharing and model updates while ensuring global consistency through adaptive periodic synchronization. Additionally, it employs global neighbor sampling with caching to reduce data transfer overhead and an adaptive queue-sizing strategy to balance computation and memory efficiency. Experiments on four large-scale datasets and ten baseline models demonstrate that MQ-GNN achieves up to \\boldmath $\\bm{4.6\\,\\times}$ faster training time and 30% improved GPU utilization while maintaining competitive accuracy. These results establish MQ-GNN as a scalable and efficient solution for multi-GPU GNN training."}
{"id": "2601.04719", "categories": ["cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.04719", "abs": "https://arxiv.org/abs/2601.04719", "authors": ["Maanas Taneja", "Purab Shingvi"], "title": "GPU-Accelerated INT8 Quantization for KV Cache Compression in Large Language Models", "comment": null, "summary": "The key-value (KV) cache in large language models presents a significant memory bottleneck during inference, growing linearly with sequence length and often exceeding the memory footprint of model weights themselves. We implement and evaluate GPU-accelerated INT8 quantization for KV cache compression, achieving 4$\\times$ memory reduction with minimal accuracy degradation. We develop four CUDA kernel variants -- naive, tiled, coarsened, and vectorized -- and benchmark them across realistic workload sizes up to 1 billion elements. Our vectorized kernel achieves up to 1,694$\\times$ speedup over CPU baselines while maintaining reconstruction error below 0.004 and attention score error below 0.1 even for 8K-dimensional heads. These results demonstrate that INT8 quantization provides a practical approach for reducing memory pressure in LLM inference with negligible computational overhead (6--58ms) and minimal impact on downstream model behavior"}
{"id": "2601.04728", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04728", "abs": "https://arxiv.org/abs/2601.04728", "authors": ["Elizabeth Donoway", "Hailey Joren", "Fabien Roger", "Jan Leike"], "title": "Excess Description Length of Learning Generalizable Predictors", "comment": null, "summary": "Understanding whether fine-tuning elicits latent capabilities or teaches new ones is a fundamental question for language model evaluation and safety. We develop a formal information-theoretic framework for quantifying how much predictive structure fine-tuning extracts from the train dataset and writes into a model's parameters. Our central quantity, Excess Description Length (EDL), is defined via prequential coding and measures the gap between the bits required to encode training labels sequentially using an evolving model (trained online) and the residual encoding cost under the final trained model. We establish that EDL is non-negative in expectation, converges to surplus description length in the infinite-data limit, and provides bounds on expected generalization gain. Through a series of toy models, we clarify common confusions about information in learning: why random labels yield EDL near zero, how a single example can eliminate many bits of uncertainty about the underlying rule(s) that describe the data distribution, why structure learned on rare inputs contributes proportionally little to expected generalization, and how format learning creates early transients distinct from capability acquisition. This framework provides rigorous foundations for the empirical observation that capability elicitation and teaching exhibit qualitatively distinct scaling signatures."}
{"id": "2601.04741", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04741", "abs": "https://arxiv.org/abs/2601.04741", "authors": ["Kota Nakamura", "Koki Kawabata", "Yasuko Matsubara", "Yasushi Sakurai"], "title": "Fast Mining and Dynamic Time-to-Event Prediction over Multi-sensor Data Streams", "comment": "Accepted by KDD 2026", "summary": "Given real-time sensor data streams obtained from machines, how can we continuously predict when a machine failure will occur? This work aims to continuously forecast the timing of future events by analyzing multi-sensor data streams. A key characteristic of real-world data streams is their dynamic nature, where the underlying patterns evolve over time. To address this, we present TimeCast, a dynamic prediction framework designed to adapt to these changes and provide accurate, real-time predictions of future event time. Our proposed method has the following properties: (a) Dynamic: it identifies the distinct time-evolving patterns (i.e., stages) and learns individual models for each, enabling us to make adaptive predictions based on pattern shifts. (b) Practical: it finds meaningful stages that capture time-varying interdependencies between multiple sensors and improve prediction performance; (c) Scalable: our algorithm scales linearly with the input size and enables online model updates on data streams. Extensive experiments on real datasets demonstrate that TimeCast provides higher prediction accuracy than state-of-the-art methods while finding dynamic changes in data streams with a great reduction in computational time."}
{"id": "2601.04751", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04751", "abs": "https://arxiv.org/abs/2601.04751", "authors": ["Luca Lanzilao", "Angela Meyer"], "title": "Intraday spatiotemporal PV power prediction at national scale using satellite-based solar forecast models", "comment": null, "summary": "We present a novel framework for spatiotemporal photovoltaic (PV) power forecasting and use it to evaluate the reliability, sharpness, and overall performance of seven intraday PV power nowcasting models. The model suite includes satellite-based deep learning and optical-flow approaches and physics-based numerical weather prediction models, covering both deterministic and probabilistic formulations. Forecasts are first validated against satellite-derived surface solar irradiance (SSI). Irradiance fields are then converted into PV power using station-specific machine learning models, enabling comparison with production data from 6434 PV stations across Switzerland. To our knowledge, this is the first study to investigate spatiotemporal PV forecasting at a national scale. We additionally provide the first visualizations of how mesoscale cloud systems shape national PV production on hourly and sub-hourly timescales. Our results show that satellite-based approaches outperform the Integrated Forecast System (IFS-ENS), particularly at short lead times. Among them, SolarSTEPS and SHADECast deliver the most accurate SSI and PV power predictions, with SHADECast providing the most reliable ensemble spread. The deterministic model IrradianceNet achieves the lowest root mean square error, while probabilistic forecasts of SolarSTEPS and SHADECast provide better-calibrated uncertainty. Forecast skill generally decreases with elevation. At a national scale, satellite-based models forecast the daily total PV generation with relative errors below 10% for 82% of the days in 2019-2020, demonstrating robustness and their potential for operational use."}
{"id": "2601.04761", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04761", "abs": "https://arxiv.org/abs/2601.04761", "authors": ["Rupsa Rani Mishra", "D. Chandrasekhar Rao", "Ajaya Kumar Tripathy"], "title": "Smart IoT-Based Wearable Device for Detection and Monitoring of Common Cow Diseases Using a Novel Machine Learning Technique", "comment": null, "summary": "Manual observation and monitoring of individual cows for disease detection present significant challenges in large-scale farming operations, as the process is labor-intensive, time-consuming, and prone to reduced accuracy. The reliance on human observation often leads to delays in identifying symptoms, as the sheer number of animals can hinder timely attention to each cow. Consequently, the accuracy and precision of disease detection are significantly compromised, potentially affecting animal health and overall farm productivity. Furthermore, organizing and managing human resources for the manual observation and monitoring of cow health is a complex and economically demanding task. It necessitates the involvement of skilled personnel, thereby contributing to elevated farm maintenance costs and operational inefficiencies. Therefore, the development of an automated, low-cost, and reliable smart system is essential to address these challenges effectively. Although several studies have been conducted in this domain, very few have simultaneously considered the detection of multiple common diseases with high prediction accuracy. However, advancements in Internet of Things (IoT), Machine Learning (ML), and Cyber-Physical Systems have enabled the automation of cow health monitoring with enhanced accuracy and reduced operational costs. This study proposes an IoT-enabled Cyber-Physical System framework designed to monitor the daily activities and health status of cow. A novel ML algorithm is proposed for the diagnosis of common cow diseases using collected physiological and behavioral data. The algorithm is designed to predict multiple diseases by analyzing a comprehensive set of recorded physiological and behavioral features, enabling accurate and efficient health assessment."}
{"id": "2601.04786", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04786", "abs": "https://arxiv.org/abs/2601.04786", "authors": ["Lang Feng", "Fuchao Yang", "Feng Chen", "Xin Cheng", "Haiyang Xu", "Zhenglin Wan", "Ming Yan", "Bo An"], "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression", "comment": "Work in progress", "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression."}
{"id": "2601.04799", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.04799", "abs": "https://arxiv.org/abs/2601.04799", "authors": ["Marios Thoma", "Vassilis Vassiliades", "Loizos Michael"], "title": "Neural-Symbolic Integration with Evolvable Policies", "comment": "18 pages, 12 figures, related code available at https://github.com/CYENS/evolvable-nesy", "summary": "Neural-Symbolic (NeSy) Artificial Intelligence has emerged as a promising approach for combining the learning capabilities of neural networks with the interpretable reasoning of symbolic systems. However, existing NeSy frameworks typically require either predefined symbolic policies or policies that are differentiable, limiting their applicability when domain expertise is unavailable or when policies are inherently non-differentiable. We propose a framework that addresses this limitation by enabling the concurrent learning of both non-differentiable symbolic policies and neural network weights through an evolutionary process. Our approach casts NeSy systems as organisms in a population that evolve through mutations (both symbolic rule additions and neural weight changes), with fitness-based selection guiding convergence toward hidden target policies. The framework extends the NEUROLOG architecture to make symbolic policies trainable, adapts Valiant's Evolvability framework to the NeSy context, and employs Machine Coaching semantics for mutable symbolic representations. Neural networks are trained through abductive reasoning from the symbolic component, eliminating differentiability requirements. Through extensive experimentation, we demonstrate that NeSy systems starting with empty policies and random neural weights can successfully approximate hidden non-differentiable target policies, achieving median correct performance approaching 100%. This work represents a step toward enabling NeSy research in domains where the acquisition of symbolic knowledge from experts is challenging or infeasible."}
{"id": "2601.04807", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04807", "abs": "https://arxiv.org/abs/2601.04807", "authors": ["Oscar Llorente", "Jaime Boal", "Eugenio F. Sánchez-Úbeda", "Antonio Diaz-Cano", "Miguel Familiar"], "title": "Parallelizing Node-Level Explainability in Graph Neural Networks", "comment": null, "summary": "Graph Neural Networks (GNNs) have demonstrated remarkable performance in a wide range of tasks, such as node classification, link prediction, and graph classification, by exploiting the structural information in graph-structured data. However, in node classification, computing node-level explainability becomes extremely time-consuming as the size of the graph increases, while batching strategies often degrade explanation quality. This paper introduces a novel approach to parallelizing node-level explainability in GNNs through graph partitioning. By decomposing the graph into disjoint subgraphs, we enable parallel computation of explainability for node neighbors, significantly improving the scalability and efficiency without affecting the correctness of the results, provided sufficient memory is available. For scenarios where memory is limited, we further propose a dropout-based reconstruction mechanism that offers a controllable trade-off between memory usage and explanation fidelity. Experimental results on real-world datasets demonstrate substantial speedups, enabling scalable and transparent explainability for large-scale GNN models."}
{"id": "2601.04855", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04855", "abs": "https://arxiv.org/abs/2601.04855", "authors": ["Francesco Ferrini", "Veronica Lachi", "Antonio Longa", "Bruno Lepri", "Matono Akiyoshi", "Andrea Passerini", "Xin Liu", "Manfred Jaeger"], "title": "Rethinking GNNs and Missing Features: Challenges, Evaluation and a Robust Solution", "comment": null, "summary": "Handling missing node features is a key challenge for deploying Graph Neural Networks (GNNs) in real-world domains such as healthcare and sensor networks. Existing studies mostly address relatively benign scenarios, namely benchmark datasets with (a) high-dimensional but sparse node features and (b) incomplete data generated under Missing Completely At Random (MCAR) mechanisms. For (a), we theoretically prove that high sparsity substantially limits the information loss caused by missingness, making all models appear robust and preventing a meaningful comparison of their performance. To overcome this limitation, we introduce one synthetic and three real-world datasets with dense, semantically meaningful features. For (b), we move beyond MCAR and design evaluation protocols with more realistic missingness mechanisms. Moreover, we provide a theoretical background to state explicit assumptions on the missingness process and analyze their implications for different methods. Building on this analysis, we propose GNNmim, a simple yet effective baseline for node classification with incomplete feature data. Experiments show that GNNmim is competitive with respect to specialized architectures across diverse datasets and missingness regimes."}
{"id": "2601.04873", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04873", "abs": "https://arxiv.org/abs/2601.04873", "authors": ["Elisa Roldan", "Kirstie Andrews", "Stephen M. Richardson", "Reyhaneh Fatahian", "Glen Cooper", "Rasool Erfani", "Tasneem Sabir", "Neil D. Reeves"], "title": "FibreCastML: An Open Web Platform for Predicting Electrospun Nanofibre Diameter Distributions", "comment": null, "summary": "Electrospinning is a scalable technique for producing fibrous scaffolds with tunable micro- and nanoscale architectures for applications in tissue engineering, drug delivery, and wound care. While machine learning (ML) has been used to support electrospinning process optimisation, most existing approaches predict only mean fibre diameters, neglecting the full diameter distribution that governs scaffold performance. This work presents FibreCastML, an open, distribution-aware ML framework that predicts complete fibre diameter spectra from routinely reported electrospinning parameters and provides interpretable insights into process structure relationships.\n  A meta-dataset comprising 68538 individual fibre diameter measurements extracted from 1778 studies across 16 biomedical polymers was curated. Six standard processing parameters, namely solution concentration, applied voltage, flow rate, tip to collector distance, needle diameter, and collector rotation speed, were used to train seven ML models using nested cross validation with leave one study out external folds. Model interpretability was achieved using variable importance analysis, SHapley Additive exPlanations, correlation matrices, and three dimensional parameter maps.\n  Non linear models consistently outperformed linear baselines, achieving coefficients of determination above 0.91 for several widely used polymers. Solution concentration emerged as the dominant global driver of fibre diameter distributions. Experimental validation across different electrospinning systems demonstrated close agreement between predicted and measured distributions. FibreCastML enables more reproducible and data driven optimisation of electrospun scaffold architectures."}
{"id": "2601.04890", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04890", "abs": "https://arxiv.org/abs/2601.04890", "authors": ["Maksim Velikanov", "Ilyas Chahed", "Jingwei Zuo", "Dhia Eddine Rhaiem", "Younes Belkada", "Hakim Hacid"], "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers", "comment": null, "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon."}
{"id": "2601.04907", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04907", "abs": "https://arxiv.org/abs/2601.04907", "authors": ["Sifan Yang", "Wenhao Yang", "Wei Jiang", "Lijun Zhang"], "title": "Distributed Online Convex Optimization with Efficient Communication: Improved Algorithm and Lower bounds", "comment": null, "summary": "We investigate distributed online convex optimization with compressed communication, where $n$ learners connected by a network collaboratively minimize a sequence of global loss functions using only local information and compressed data from neighbors. Prior work has established regret bounds of $O(\\max\\{ω^{-2}ρ^{-4}n^{1/2},ω^{-4}ρ^{-8}\\}n\\sqrt{T})$ and $O(\\max\\{ω^{-2}ρ^{-4}n^{1/2},ω^{-4}ρ^{-8}\\}n\\ln{T})$ for convex and strongly convex functions, respectively, where $ω\\in(0,1]$ is the compression quality factor ($ω=1$ means no compression) and $ρ<1$ is the spectral gap of the communication matrix. However, these regret bounds suffer from a \\emph{quadratic} or even \\emph{quartic} dependence on $ω^{-1}$. Moreover, the \\emph{super-linear} dependence on $n$ is also undesirable. To overcome these limitations, we propose a novel algorithm that achieves improved regret bounds of $\\tilde{O}(ω^{-1/2}ρ^{-1}n\\sqrt{T})$ and $\\tilde{O}(ω^{-1}ρ^{-2}n\\ln{T})$ for convex and strongly convex functions, respectively. The primary idea is to design a \\emph{two-level blocking update framework} incorporating two novel ingredients: an online gossip strategy and an error compensation scheme, which collaborate to \\emph{achieve a better consensus} among learners. Furthermore, we establish the first lower bounds for this problem, justifying the optimality of our results with respect to both $ω$ and $T$. Additionally, we consider the bandit feedback scenario, and extend our method with the classic gradient estimators to enhance existing regret bounds."}
{"id": "2601.04941", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04941", "abs": "https://arxiv.org/abs/2601.04941", "authors": ["Miguel O'Malley"], "title": "Cardinality augmented loss functions", "comment": "12 pages, 3 figures", "summary": "Class imbalance is a common and pernicious issue for the training of neural networks. Often, an imbalanced majority class can dominate training to skew classifier performance towards the majority outcome. To address this problem we introduce cardinality augmented loss functions, derived from cardinality-like invariants in modern mathematics literature such as magnitude and the spread. These invariants enrich the concept of cardinality by evaluating the `effective diversity' of a metric space, and as such represent a natural solution to overly homogeneous training data. In this work, we establish a methodology for applying cardinality augmented loss functions in the training of neural networks and report results on both artificially imbalanced datasets as well as a real-world imbalanced material science dataset. We observe significant performance improvement among minority classes, as well as improvement in overall performance metrics."}
{"id": "2601.04954", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04954", "abs": "https://arxiv.org/abs/2601.04954", "authors": ["Yirong Zeng", "Yufei Liu", "Xiao Ding", "Yutai Hou", "Yuxian Wang", "Haonan Song", "Wu Ning", "Dandan Tu", "Qixun Zhang", "Bibo Cai", "Yuxiang He", "Ting Liu"], "title": "Precision over Diversity: High-Precision Reward Generalizes to Robust Instruction Following", "comment": "ACL under review 13 pages, 8 figures", "summary": "A central belief in scaling reinforcement learning with verifiable rewards for instruction following (IF) tasks is that, a diverse mixture of verifiable hard and unverifiable soft constraints is essential for generalizing to unseen instructions. In this work, we challenge this prevailing consensus through a systematic empirical investigation. Counter-intuitively, we find that models trained on hard-only constraints consistently outperform those trained on mixed datasets. Extensive experiments reveal that reward precision, rather than constraint diversity, is the primary driver of effective alignment. The LLM judge suffers from a low recall rate in detecting false response, which leads to severe reward hacking, thereby undermining the benefits of diversity. Furthermore, analysis of the attention mechanism reveals that high-precision rewards develop a transferable meta-skill for IF. Motivated by these insights, we propose a simple yet effective data-centric refinement strategy that prioritizes reward precision. Evaluated on five benchmarks, our approach outperforms competitive baselines by 13.4\\% in performance while achieving a 58\\% reduction in training time, maintaining strong generalization beyond instruction following. Our findings advocate for a paradigm shift: moving away from the indiscriminate pursuit of data diversity toward high-precision rewards."}
{"id": "2601.04977", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04977", "abs": "https://arxiv.org/abs/2601.04977", "authors": ["James Hinns", "Sofie Goethals", "Stephan Van der Veeken", "Theodoros Evgeniou", "David Martens"], "title": "On the Definition and Detection of Cherry-Picking in Counterfactual Explanations", "comment": null, "summary": "Counterfactual explanations are widely used to communicate how inputs must change for a model to alter its prediction. For a single instance, many valid counterfactuals can exist, which leaves open the possibility for an explanation provider to cherry-pick explanations that better suit a narrative of their choice, highlighting favourable behaviour and withholding examples that reveal problematic behaviour. We formally define cherry-picking for counterfactual explanations in terms of an admissible explanation space, specified by the generation procedure, and a utility function. We then study to what extent an external auditor can detect such manipulation. Considering three levels of access to the explanation process: full procedural access, partial procedural access, and explanation-only access, we show that detection is extremely limited in practice. Even with full procedural access, cherry-picked explanations can remain difficult to distinguish from non cherry-picked explanations, because the multiplicity of valid counterfactuals and flexibility in the explanation specification provide sufficient degrees of freedom to mask deliberate selection. Empirically, we demonstrate that this variability often exceeds the effect of cherry-picking on standard counterfactual quality metrics such as proximity, plausibility, and sparsity, making cherry-picked explanations statistically indistinguishable from baseline explanations. We argue that safeguards should therefore prioritise reproducibility, standardisation, and procedural constraints over post-hoc detection, and we provide recommendations for algorithm developers, explanation providers, and auditors."}
{"id": "2601.05002", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05002", "abs": "https://arxiv.org/abs/2601.05002", "authors": ["Aleksandar Fontana", "Marco Simoni", "Giulio Rossolini", "Andrea Saracino", "Paolo Mori"], "title": "On the Hidden Objective Biases of Group-based Reinforcement Learning", "comment": null, "summary": "Group-based reinforcement learning methods, like Group Relative Policy Optimization (GRPO), are widely used nowadays to post-train large language models. Despite their empirical success, they exhibit structural mismatches between reward optimization and the underlying training objective. In this paper, we present a theoretical analysis of GRPO style methods by studying them within a unified surrogate formulation. This perspective reveals recurring properties that affect all the methods under analysis: (i) non-uniform group weighting induces systematic gradient biases on shared prefix tokens; (ii) interactions with the AdamW optimizer make training dynamics largely insensitive to reward scaling; and (iii) optimizer momentum can push policy updates beyond the intended clipping region under repeated optimization steps. We believe that these findings highlight fundamental limitations of current approaches and provide principled guidance for the design of future formulations."}
{"id": "2601.05017", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05017", "abs": "https://arxiv.org/abs/2601.05017", "authors": ["Xiaopeng Luo", "Zexi Tan", "Zhuowei Wang"], "title": "HMVI: Unifying Heterogeneous Attributes with Natural Neighbors for Missing Value Inference", "comment": "Submitted to ICASSP 2026", "summary": "Missing value imputation is a fundamental challenge in machine intelligence, heavily dependent on data completeness. Current imputation methods often handle numerical and categorical attributes independently, overlooking critical interdependencies among heterogeneous features. To address these limitations, we propose a novel imputation approach that explicitly models cross-type feature dependencies within a unified framework. Our method leverages both complete and incomplete instances to ensure accurate and consistent imputation in tabular data. Extensive experimental results demonstrate that the proposed approach achieves superior performance over existing techniques and significantly enhances downstream machine learning tasks, providing a robust solution for real-world systems with missing data."}
{"id": "2601.05028", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05028", "abs": "https://arxiv.org/abs/2601.05028", "authors": ["Torben Berndt", "Jan Stühmer"], "title": "Approximate equivariance via projection-based regularisation", "comment": null, "summary": "Equivariance is a powerful inductive bias in neural networks, improving generalisation and physical consistency. Recently, however, non-equivariant models have regained attention, due to their better runtime performance and imperfect symmetries that might arise in real-world applications. This has motivated the development of approximately equivariant models that strike a middle ground between respecting symmetries and fitting the data distribution. Existing approaches in this field usually apply sample-based regularisers which depend on data augmentation at training time, incurring a high sample complexity, in particular for continuous groups such as $SO(3)$. This work instead approaches approximate equivariance via a projection-based regulariser which leverages the orthogonal decomposition of linear layers into equivariant and non-equivariant components. In contrast to existing methods, this penalises non-equivariance at an operator level across the full group orbit, rather than point-wise. We present a mathematical framework for computing the non-equivariance penalty exactly and efficiently in both the spatial and spectral domain. In our experiments, our method consistently outperforms prior approximate equivariance approaches in both model performance and efficiency, achieving substantial runtime gains over sample-based regularisers."}
{"id": "2601.05033", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05033", "abs": "https://arxiv.org/abs/2601.05033", "authors": ["Anees Fatima", "Mohammad Abdus Salam"], "title": "A Data-Driven Predictive Framework for Inventory Optimization Using Context-Augmented Machine Learning Models", "comment": null, "summary": "Demand forecasting in supply chain management (SCM) is critical for optimizing inventory, reducing waste, and improving customer satisfaction. Conventional approaches frequently neglect external influences like weather, festivities, and equipment breakdowns, resulting in inefficiencies. This research investigates the use of machine learning (ML) algorithms to improve demand prediction in retail and vending machine sectors. Four machine learning algorithms. Extreme Gradient Boosting (XGBoost), Autoregressive Integrated Moving Average (ARIMA), Facebook Prophet (Fb Prophet), and Support Vector Regression (SVR) were used to forecast inventory requirements. Ex-ternal factors like weekdays, holidays, and sales deviation indicators were methodically incorporated to enhance precision. XGBoost surpassed other models, reaching the lowest Mean Absolute Error (MAE) of 22.7 with the inclusion of external variables. ARIMAX and Fb Prophet demonstrated noteworthy enhancements, whereas SVR fell short in performance. Incorporating external factors greatly improves the precision of demand forecasting models, and XGBoost is identified as the most efficient algorithm. This study offers a strong framework for enhancing inventory management in retail and vending machine systems."}
{"id": "2601.05052", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.05052", "abs": "https://arxiv.org/abs/2601.05052", "authors": ["Saumya Gupta", "Scott Biggs", "Moritz Laber", "Zohair Shafi", "Robin Walters", "Ayan Paul"], "title": "DeepWeightFlow: Re-Basined Flow Matching for Generating Neural Network Weights", "comment": "25 pages, 20 tables, 2 figures", "summary": "Building efficient and effective generative models for neural network weights has been a research focus of significant interest that faces challenges posed by the high-dimensional weight spaces of modern neural networks and their symmetries. Several prior generative models are limited to generating partial neural network weights, particularly for larger models, such as ResNet and ViT. Those that do generate complete weights struggle with generation speed or require finetuning of the generated models. In this work, we present DeepWeightFlow, a Flow Matching model that operates directly in weight space to generate diverse and high-accuracy neural network weights for a variety of architectures, neural network sizes, and data modalities. The neural networks generated by DeepWeightFlow do not require fine-tuning to perform well and can scale to large networks. We apply Git Re-Basin and TransFusion for neural network canonicalization in the context of generative weight models to account for the impact of neural network permutation symmetries and to improve generation efficiency for larger model sizes. The generated networks excel at transfer learning, and ensembles of hundreds of neural networks can be generated in minutes, far exceeding the efficiency of diffusion-based methods. DeepWeightFlow models pave the way for more efficient and scalable generation of diverse sets of neural networks."}
{"id": "2601.05073", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05073", "abs": "https://arxiv.org/abs/2601.05073", "authors": ["Jianlong Chen", "Daocheng Fu", "Shengze Xu", "Jiawei Chen", "Yuan Feng", "Yue Yang", "Junchi Yan", "Hongyuan Zha", "Renqiu Xia"], "title": "Milestones over Outcome: Unlocking Geometric Reasoning with Sub-Goal Verifiable Reward", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) struggle with complex geometric reasoning, largely because \"black box\" outcome-based supervision fails to distinguish between lucky guesses and rigorous deduction. To address this, we introduce a paradigm shift towards subgoal-level evaluation and learning. We first construct GeoGoal, a benchmark synthesized via a rigorous formal verification data engine, which converts abstract proofs into verifiable numeric subgoals. This structure reveals a critical divergence between reasoning quality and outcome accuracy. Leveraging this, we propose the Sub-Goal Verifiable Reward (SGVR) framework, which replaces sparse signals with dense rewards based on the Skeleton Rate. Experiments demonstrate that SGVR not only enhances geometric performance (+9.7%) but also exhibits strong generalization, transferring gains to general math (+8.0%) and other general reasoning tasks (+2.8%), demonstrating broad applicability across diverse domains."}
{"id": "2601.05082", "categories": ["cs.LG", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.05082", "abs": "https://arxiv.org/abs/2601.05082", "authors": ["Hayk Asatryan", "Basile Tousside", "Janis Mohr", "Malte Neugebauer", "Hildo Bijl", "Paul Spiegelberg", "Claudia Frohn-Schauf", "Jörg Frochte"], "title": "Exploring Student Expectations and Confidence in Learning Analytics", "comment": "7 pages, Keywords: Learning Analytics, Survey, Data Protection, Clustering", "summary": "Learning Analytics (LA) is nowadays ubiquitous in many educational systems, providing the ability to collect and analyze student data in order to understand and optimize learning and the environments in which it occurs. On the other hand, the collection of data requires to comply with the growing demand regarding privacy legislation. In this paper, we use the Student Expectation of Learning Analytics Questionnaire (SELAQ) to analyze the expectations and confidence of students from different faculties regarding the processing of their data for Learning Analytics purposes. This allows us to identify four clusters of students through clustering algorithms: Enthusiasts, Realists, Cautious and Indifferents. This structured analysis provides valuable insights into the acceptance and criticism of Learning Analytics among students."}
{"id": "2601.05134", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05134", "abs": "https://arxiv.org/abs/2601.05134", "authors": ["Polina Dolgova", "Sebastian U. Stich"], "title": "Sequential Subspace Noise Injection Prevents Accuracy Collapse in Certified Unlearning", "comment": null, "summary": "Certified unlearning based on differential privacy offers strong guarantees but remains largely impractical: the noisy fine-tuning approaches proposed so far achieve these guarantees but severely reduce model accuracy. We propose sequential noise scheduling, which distributes the noise budget across orthogonal subspaces of the parameter space, rather than injecting it all at once. This simple modification mitigates the destructive effect of noise while preserving the original certification guarantees. We extend the analysis of noisy fine-tuning to the subspace setting, proving that the same $(\\varepsilon,δ)$ privacy budget is retained. Empirical results on image classification benchmarks show that our approach substantially improves accuracy after unlearning while remaining robust to membership inference attacks. These results show that certified unlearning can achieve both rigorous guarantees and practical utility."}
{"id": "2601.05152", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05152", "abs": "https://arxiv.org/abs/2601.05152", "authors": ["Timofey Tomashevskiy"], "title": "Safe Continual Reinforcement Learning Methods for Nonstationary Environments. Towards a Survey of the State of the Art", "comment": "20 pages, 4 figures", "summary": "This work provides a state-of-the-art survey of continual safe online reinforcement learning (COSRL) methods. We discuss theoretical aspects, challenges, and open questions in building continual online safe reinforcement learning algorithms. We provide the taxonomy and the details of continual online safe reinforcement learning methods based on the type of safe learning mechanism that takes adaptation to nonstationarity into account. We categorize safety constraints formulation for online reinforcement learning algorithms, and finally, we discuss prospects for creating reliable, safe online learning algorithms.\n  Keywords: safe RL in nonstationary environments, safe continual reinforcement learning under nonstationarity, HM-MDP, NSMDP, POMDP, safe POMDP, constraints for continual learning, safe continual reinforcement learning review, safe continual reinforcement learning survey, safe continual reinforcement learning, safe online learning under distribution shift, safe continual online adaptation, safe reinforcement learning, safe exploration, safe adaptation, constrained Markov decision processes, safe reinforcement learning, partially observable Markov decision process, safe reinforcement learning and hidden Markov decision processes, Safe Online Reinforcement Learning, safe online reinforcement learning, safe online reinforcement learning, safe meta-learning, safe meta-reinforcement learning, safe context-based reinforcement learning, formulating safety constraints for continual learning"}
{"id": "2601.05174", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05174", "abs": "https://arxiv.org/abs/2601.05174", "authors": ["Yiji Zhao", "Zihao Zhong", "Ao Wang", "Haomin Wen", "Ming Jin", "Yuxuan Liang", "Huaiyu Wan", "Hao Wu"], "title": "FaST: Efficient and Effective Long-Horizon Forecasting for Large-Scale Spatial-Temporal Graphs via Mixture-of-Experts", "comment": "Accepted to KDD 2026", "summary": "Spatial-Temporal Graph (STG) forecasting on large-scale networks has garnered significant attention. However, existing models predominantly focus on short-horizon predictions and suffer from notorious computational costs and memory consumption when scaling to long-horizon predictions and large graphs. Targeting the above challenges, we present FaST, an effective and efficient framework based on heterogeneity-aware Mixture-of-Experts (MoEs) for long-horizon and large-scale STG forecasting, which unlocks one-week-ahead (672 steps at a 15-minute granularity) prediction with thousands of nodes. FaST is underpinned by two key innovations. First, an adaptive graph agent attention mechanism is proposed to alleviate the computational burden inherent in conventional graph convolution and self-attention modules when applied to large-scale graphs. Second, we propose a new parallel MoE module that replaces traditional feed-forward networks with Gated Linear Units (GLUs), enabling an efficient and scalable parallel structure. Extensive experiments on real-world datasets demonstrate that FaST not only delivers superior long-horizon predictive accuracy but also achieves remarkable computational efficiency compared to state-of-the-art baselines. Our source code is available at: https://github.com/yijizhao/FaST."}
{"id": "2601.05194", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05194", "abs": "https://arxiv.org/abs/2601.05194", "authors": ["Fardin Ganjkhanloo", "Emmett Springer", "Erik H. Hoyer", "Daniel L. Young", "Holley Farley", "Kimia Ghobadi"], "title": "An interpretable data-driven approach to optimizing clinical fall risk assessment", "comment": "arXiv admin note: substantial text overlap with arXiv:2510.20714", "summary": "In this study, we aim to better align fall risk prediction from the Johns Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically meaningful measures via a data-driven modelling approach. We conducted a retrospective cohort analysis of 54,209 inpatient admissions from three Johns Hopkins Health System hospitals between March 2022 and October 2023. A total of 20,208 admissions were included as high fall risk encounters, and 13,941 were included as low fall risk encounters. To incorporate clinical knowledge and maintain interpretability, we employed constrained score optimization (CSO) models to reweight the JHFRAT scoring weights, while preserving its additive structure and clinical thresholds. Recalibration refers to adjusting item weights so that the resulting score can order encounters more consistently by the study's risk labels, and without changing the tool's form factor or deployment workflow. The model demonstrated significant improvements in predictive performance over the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). This performance improvement translates to protecting an additional 35 high-risk patients per week across the Johns Hopkins Health System. The constrained score optimization models performed similarly with and without the EHR variables. Although the benchmark black-box model (XGBoost), improves upon the performance metrics of the knowledge-based constrained logistic regression (AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk labeling. This evidence-based approach provides a robust foundation for health systems to systematically enhance inpatient fall prevention protocols and patient safety using data-driven optimization techniques, contributing to improved risk assessment and resource allocation in healthcare settings."}
{"id": "2601.05205", "categories": ["cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.05205", "abs": "https://arxiv.org/abs/2601.05205", "authors": ["Zain Iqbal", "Lorenzo Valerio"], "title": "EARL: Energy-Aware Optimization of Liquid State Machines for Pervasive AI", "comment": "6 pages, 9 figures, 2 Tables, conference [Submitted in PerConAI-2026]", "summary": "Pervasive AI increasingly depends on on-device learning systems that deliver low-latency and energy-efficient computation under strict resource constraints. Liquid State Machines (LSMs) offer a promising approach for low-power temporal processing in pervasive and neuromorphic systems, but their deployment remains challenging due to high hyperparameter sensitivity and the computational cost of traditional optimization methods that ignore energy constraints. This work presents EARL, an energy-aware reinforcement learning framework that integrates Bayesian optimization with an adaptive reinforcement learning based selection policy to jointly optimize accuracy and energy consumption. EARL employs surrogate modeling for global exploration, reinforcement learning for dynamic candidate prioritization, and an early termination mechanism to eliminate redundant evaluations, substantially reducing computational overhead. Experiments on three benchmark datasets demonstrate that EARL achieves 6 to 15 percent higher accuracy, 60 to 80 percent lower energy consumption, and up to an order of magnitude reduction in optimization time compared to leading hyperparameter tuning frameworks. These results highlight the effectiveness of energy-aware adaptive search in improving the efficiency and scalability of LSMs for resource-constrained on-device AI applications."}
{"id": "2601.05240", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.AI", "hep-th"], "pdf": "https://arxiv.org/pdf/2601.05240", "abs": "https://arxiv.org/abs/2601.05240", "authors": ["Ilmo Sung"], "title": "Robust Reasoning as a Symmetry-Protected Topological Phase", "comment": null, "summary": "Large language models suffer from \"hallucinations\"-logical inconsistencies induced by semantic noise. We propose that current architectures operate in a \"Metric Phase,\" where causal order is vulnerable to spontaneous symmetry breaking. Here, we identify robust inference as an effective Symmetry-Protected Topological phase, where logical operations are formally isomorphic to non-Abelian anyon braiding, replacing fragile geometric interpolation with robust topological invariants. Empirically, we demonstrate a sharp topological phase transition: while Transformers and RNNs exhibit gapless decay, our Holonomic Network reveals a macroscopic \"mass gap,\" maintaining invariant fidelity below a critical noise threshold. Furthermore, in a variable-binding task on $S_{10}$ ($3.6 \\times 10^6$ states) representing symbolic manipulation, we demonstrate holonomic generalization: the topological model maintains perfect fidelity extrapolating $100\\times$ beyond training ($L=50 \\to 5000$), consistent with a theoretically indefinite causal horizon, whereas Transformers lose logical coherence. Ablation studies indicate this protection emerges strictly from non-Abelian gauge symmetry. This provides strong evidence for a new universality class for logical reasoning, linking causal stability to the topology of the semantic manifold."}
{"id": "2601.05245", "categories": ["cs.LG", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.05245", "abs": "https://arxiv.org/abs/2601.05245", "authors": ["Natalie Collina", "Jiuyao Lu", "Georgy Noarov", "Aaron Roth"], "title": "Optimal Lower Bounds for Online Multicalibration", "comment": null, "summary": "We prove tight lower bounds for online multicalibration, establishing an information-theoretic separation from marginal calibration.\n  In the general setting where group functions can depend on both context and the learner's predictions, we prove an $Ω(T^{2/3})$ lower bound on expected multicalibration error using just three disjoint binary groups. This matches the upper bounds of Noarov et al. (2025) up to logarithmic factors and exceeds the $O(T^{2/3-\\varepsilon})$ upper bound for marginal calibration (Dagan et al., 2025), thereby separating the two problems.\n  We then turn to lower bounds for the more difficult case of group functions that may depend on context but not on the learner's predictions. In this case, we establish an $\\widetildeΩ(T^{2/3})$ lower bound for online multicalibration via a $Θ(T)$-sized group family constructed using orthogonal function systems, again matching upper bounds up to logarithmic factors."}
{"id": "2601.04732", "categories": ["quant-ph", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04732", "abs": "https://arxiv.org/abs/2601.04732", "authors": ["Dominik Freinberger", "Philipp Moser"], "title": "The Role of Quantum in Hybrid Quantum-Classical Neural Networks: A Realistic Assessment", "comment": "16 pages, 6 figures", "summary": "Quantum machine learning has emerged as a promising application domain for near-term quantum hardware, particularly through hybrid quantum-classical models that leverage both classical and quantum processing. Although numerous hybrid architectures have been proposed and demonstrated successfully on benchmark tasks, a significant open question remains regarding the specific contribution of quantum components to the overall performance of these models. In this work, we aim to shed light on the impact of quantum processing within hybrid quantum-classical neural network architectures through a rigorous statistical study. We systematically assess common hybrid models on medical signal data as well as planar and volumetric images, examining the influence attributable to classical and quantum aspects such as encoding schemes, entanglement, and circuit size. We find that in best-case scenarios, hybrid models show performance comparable to their classical counterparts, however, in most cases, performance metrics deteriorate under the influence of quantum components. Our multi-modal analysis provides realistic insights into the contributions of quantum components and advocates for cautious claims and design choices for hybrid models in near-term applications."}
{"id": "2601.05036", "categories": ["quant-ph", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05036", "abs": "https://arxiv.org/abs/2601.05036", "authors": ["Milan Liepelt", "Julien Baglio"], "title": "Exponential capacity scaling of classical GANs compared to hybrid latent style-based quantum GANs", "comment": "34 pages, 7 figures, 7 tables", "summary": "Quantum generative modeling is a very active area of research in looking for practical advantage in data analysis. Quantum generative adversarial networks (QGANs) are leading candidates for quantum generative modeling and have been applied to diverse areas, from high-energy physics to image generation. The latent style-based QGAN, relying on a classical variational autoencoder to encode the input data into a latent space and then using a style-based QGAN for data generation has been proven to be efficient for image generation or drug design, hinting at the use of far less trainable parameters than their classical counterpart to achieve comparable performance, however this advantage has never been systematically studied. We present in this work the first comprehensive experimental analysis of this advantage of QGANS applied to SAT4 image generation, obtaining an exponential advantage in capacity scaling for a quantum generator in the hybrid latent style-based QGAN architecture. Careful tuning of the autoencoder is crucial to obtain stable, reliable results. Once this tuning is performed and defining training optimality as when the training is stable and the FID score is low and stable as well, the optimal capacity (or number of trainable parameters) of the classical discriminator scales exponentially with respect to the capacity of the quantum generator, and the same is true for the capacity of the classical generator. This hints toward a type of quantum advantage for quantum generative modeling."}
