<div id=toc></div>

# Table of Contents

- [gr-qc](#gr-qc) [Total: 27]
- [physics.comp-ph](#physics.comp-ph) [Total: 5]
- [quant-ph](#quant-ph) [Total: 52]
- [cs.LG](#cs.LG) [Total: 214]


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [1] [Gravitational Waves from the Big Bang](https://arxiv.org/abs/2511.17659)
*Lucas Martins Barreto Alves*

Main category: gr-qc

TL;DR: 这篇论文研究宇宙暴胀产生的引力波，特别是解释NANOGrav观测站探测到的引力波信号可能起源于早期宇宙的机制。


<details>
  <summary>Details</summary>
Motivation: 引力波天文学为观测宇宙开辟了新窗口，使我们有可能直接观测到被光阻挡的原始宇宙，特别是研究宇宙暴胀这一早期宇宙的主要科学范式。

Method: 论文分为引力波、暴胀宇宙学和暴胀引力波三个章节，系统地分析引力波信号与早期宇宙起源的关系。

Result: 通过分析NANOGrav观测站探测到的引力波信号，探讨了这些信号可能起源于宇宙暴胀时期的可能性。

Conclusion: 引力波观测为研究原始宇宙提供了新的直接证据，特别是对宇宙暴胀理论的验证具有重要意义。

Abstract: For millennia, humanity has relied exclusively on light$\unicode{x2014}$initially visible light and, later, broader and broader portions of the electromagnetic spectrum$\unicode{x2014}$to observe the universe. In the past decade, a remarkable chapter in extending astronomy beyond electromagnetic antennas has been concretized: the dawn of gravitational-wave astronomy has opened a new observational window into the cosmos. Among the many new astronomical sources we may now look for and study through their gravitational-wave signals, the Big Bang is surely among the most fascinating. Gravitational waves give us concrete hope of directly observing the primordial universe, whose light, emitted more than 13.7 billion years ago, is blocked from reaching our telescopes. This dissertation is aimed at the study of gravitational waves from cosmic inflation, the main scientific paradigm for the very early universe. Therefore, the text is divided into chapters on gravitational waves, inflationary cosmology, and inflationary gravitational waves. More specifically, our discussion will be steered by the endeavor to explain how the gravitational-wave signal sought by the NANOGrav observatory could have originated in the primordial universe.

</details>


### [2] [Price's law from quasinormal modes](https://arxiv.org/abs/2511.17703)
*Paolo Arnaudo,Benjamin Withers*

Main category: gr-qc

TL;DR: 该论文通过Schwarzschild-de Sitter准正规模态的极限Λ→0⁺，推导出Price关于Schwarzschild扰动幂律尾t^{-2ℓ-3}的结果。


<details>
  <summary>Details</summary>
Motivation: 研究Schwarzschild时空扰动的长时间渐近行为，特别是Price幂律尾的起源。

Method: 使用Schwarzschild-de Sitter时空的准正规模态，在宇宙学常数Λ趋近于0⁺的极限下进行分析。

Result: 成功从Schwarzschild-de Sitter准正规模态的极限中重现了Price幂律尾t^{-2ℓ-3}的行为。

Conclusion: Price幂律尾可以通过Schwarzschild-de Sitter准正规模态在Λ→0⁺极限下的求和得到，这为理解Schwarzschild时空扰动的长时间行为提供了新的视角。

Abstract: We show that Price's power-law tail for perturbations of Schwarzschild, $t^{-2\ell-3}$ as $t\to \infty$, can be obtained from a sum of Schwarzschild-de Sitter quasinormal modes in the limit $Λ\to 0^+$.

</details>


### [3] [Krylov Complexity in Canonical Quantum Cosmology](https://arxiv.org/abs/2511.17711)
*Meysam Motaharfar,Maxwell R. Siebersma,Parampreet Singh*

Main category: gr-qc

TL;DR: 本文研究了Wheeler-DeWitt量子宇宙学和圈量子宇宙学中两个可精确求解模型的Krylov复杂性，发现状态复杂性和算子复杂性在波函数尖锐峰值区域随标量场时钟呈二次增长，且算子复杂性恰好是状态复杂性的两倍。在LQC中，复杂性在反弹点保持有限，而在WDW量子宇宙学中，复杂性在大爆炸/大挤压奇点处发散。


<details>
  <summary>Details</summary>
Motivation: 探索量子宇宙学中Krylov复杂性的行为，特别是在处理时空奇点问题的不同量子引力方法中，比较WDW量子宇宙学和LQC在复杂性演化方面的差异。

Method: 通过Lanczos算法解析构造Krylov基，评估Krylov状态复杂性和算子复杂性，计算Krylov熵来分析系统的全局行为。

Result: 在波函数尖锐峰值区域，WDW量子宇宙学和LQC中的状态复杂性和算子复杂性都随标量场时钟呈二次增长，算子复杂性是状态复杂性的两倍。在LQC中，复杂性和熵在反弹点保持有限，而在WDW量子宇宙学中，它们在大爆炸/大挤压奇点处发散。

Conclusion: Krylov复杂性能够区分不同量子引力方法处理时空奇点的能力，为研究更复杂的量子宇宙学模型（包括量子混沌现象）中的复杂性计算奠定了基础。

Abstract: We explore Krylov complexity for two exactly solvable models, one in the Wheeler-DeWitt (WDW) quantum cosmology and another in loop quantum cosmology (LQC), for a spatially flat, homogeneous, and isotropic universe sourced with a massless scalar field, which serves the role of clock. While the WDW quantization of this model cannot avoid the big bang/big crunch singularity, it is replaced by a big bounce in LQC. We construct the Krylov basis analytically by applying the Lanczos algorithm and evaluate both the Krylov state and operator complexity. In regimes where the wave function of the universe is sharply peaked, our results indicate that the Krylov complexity grows quadratically with the scalar field clock for the state and operator complexities in both the WDW quantum cosmology and LQC. We further show that operator complexity is exactly twice the state complexity in these regimes. We discuss the interpretation of the global behavior of these systems by calculating the Krylov entropy for both quantum cosmological frameworks. We observe that in LQC, the Krylov complexity and entropy remain finite at the bounce, whereas in the WDW quantum cosmology, they diverge at the big bang/crunch singularity. Our work paves the way for computing Krylov complexity in more intricate quantum cosmological models, including those exhibiting phenomena such as quantum chaos.

</details>


### [4] [Efficient Search for Detection Candidates Using a Peak Finder Strategy for All-Sky-All-Frequency Gravitational Wave Radiometer](https://arxiv.org/abs/2511.17758)
*Arindam Sharma,Deepali Agarwal,Sanjit Mitra*

Main category: gr-qc

TL;DR: 提出了一种新的Peak Finder算法，用于减少ASAF引力波搜索中的相关候选样本数量，提高检测效率并降低误拒率。


<details>
  <summary>Details</summary>
Motivation: ASAF搜索存在波束模糊问题，导致多个候选样本可能来自相同的噪声波动、探测器伪影或引力波源，这会降低后续分析的检测概率，特别是在计算资源有限的情况下。

Method: 引入新颖的Peak Finder算法，识别最具代表性的候选样本，同时保持检测灵敏度，从而能够跟踪更多独立候选样本。

Result: 使用Peak Finder方法相比全天空方法显著降低了误拒率，例如在30Hz频率下跟踪2个Peak Finder候选样本可将误拒率降低3倍。

Conclusion: Peak Finder算法有效减少了相关候选样本数量，提高了ASAF搜索的检测效率，为后续更优但计算昂贵的搜索提供了更好的候选样本筛选机制。

Abstract: The first all-sky-all-frequency (ASAF) radiometer search was conducted using data from the first three observing runs of the Advanced LIGO and Advanced Virgo detectors. The significance of this search lies in its fast and unmodeled approach, leveraging a cross-correlation technique to identify common signals across the detector network. As a result, this method serves as an excellent alternative to search for unknown or poorly modeled continuous wave sources and narrowband components of the gravitational wave (GW) background. For continuous wave sources whose waveform can be modeled, this method can serve as the first stage of a hierarchical scheme by identifying sub-threshold candidates to be followed up with more optimal but computationally expensive searches. The ASAF search, however, presently suffers from beam smearing, where multiple candidates may arise due to the same noise fluctuations, detector artifact, or a GW source. This can reduce the detection probability in follow-up analyzes, especially with limited computing resources. To mitigate this issue and reduce the number of correlated and unnecessary candidates, we introduce a novel Peak Finder algorithm. This algorithm helps identifying the most representative candidates while preserving detection sensitivity, thereby allowing follow up of a much larger number of independent candidates. The reduction in correlated samples leads to a significant reduction in False Dismissal Rate (FDR) using the Peak Finder method compared to the Full-sky method. For instance, following up 2 Peak Finder candidates at 30 Hz reduces FDR by a factor of 3.

</details>


### [5] [The $μ$-deformed Einstein field equations with $μ$-dependent effective cosmological constant](https://arxiv.org/abs/2511.17790)
*O. P. Mykhailiv,Yu. A. Mishchenko,A. M. Gavrilik*

Main category: gr-qc

TL;DR: 从μ-变形玻色气体模型的热力学函数推导出μ-变形的爱因斯坦场方程，通过变形参数调控宇宙学常数，为解决宇宙学常数问题提供新途径。


<details>
  <summary>Details</summary>
Motivation: 探索通过量子变形理论解决宇宙学常数问题的可能性，将变形参数与宇宙学常数的调控联系起来。

Method: 应用Verlinde方法，从μ-变形玻色气体模型的广义热力学函数推导出μ-变形的爱因斯坦场方程。

Result: 变形参数μ能够有效调控宇宙学常数的值，使其大幅减小到更符合观测的数值，并与暗物质存在关联。

Conclusion: μ-变形理论为解决宇宙学常数问题提供了有前景的框架，通过调整变形参数可以显著降低宇宙学常数的理论值。

Abstract: In this paper, we derive the $μ$-deformed Einstein field equations from the generalized thermodynamic functions of the $μ$-deformed analog of Bose gas model, applying the (adapted) Verlinde's approach. The basic role of deformation parameter is shown: it provides the possibility to vary the value of the cosmological constant. Due to this, we suggest an interesting treatment of the cosmological constant (CC) problem within the framework of $μ$-deformation. Namely, viewing the derived $μ$-deformed CC as an effective one and varying the parameter $μ$ appropriately, we gain the possibility to drastically reduce the CC, so as to get for it the realistic value. Also, the relation with dark matter is discussed.

</details>


### [6] [Wave Front Sensing demodulated at the difference frequency between two phase-modulation sidebands in a compound interferometer configuration for a gravitational-wave detector](https://arxiv.org/abs/2511.17893)
*Chiaki Hirose,Kenta Tanaka,Osamu Miyakawa,Takafumi Ushiba*

Main category: gr-qc

TL;DR: 提出了一种新的波前传感技术PMPMWFS，用于解决引力波探测器中传统波前传感技术受臂腔信号主导的问题，能够有效解耦多个对准自由度。


<details>
  <summary>Details</summary>
Motivation: 传统波前传感技术(WFS)在全干涉仪中载波谐振时，主要受臂轴信号主导，限制了功率回收腔(PRC)和入射光束等其他光学轴的对准检测能力。

Method: 提出了PMPMWFS技术，通过解调两个反谐振相位调制边频之间的差频拍频信号，有效解耦PRC和入射光束的角度波动信号。

Result: 实验结果表明PMPMWFS能够有效解耦PRC和入射光束的角度波动信号，并为臂腔末端反射镜提供正交信号分量，使用PMPMWFS的反馈控制实现了超过1小时的稳定干涉仪锁定。

Conclusion: PMPMWFS为未来引力波探测器提供了一种有效的多对准自由度解耦传感方法。

Abstract: Precise alignment sensing and control are essential for maintaining the stability of laser interferometric gravitational-wave detectors. Conventional Wave Front Sensing technique (WFS), which relies on the beat between the carrier and phase-modulated (PM) sidebands, is dominated by arm-axis signals when the carrier resonates in the full interferometer. This dominance limits the detection of other optical axes, such as the Power Recycling Cavity (PRC) and incident beam axes. To address this problem, we propose a novel sensing technique, "Phase-Modulated-sideband $\times$ Phase-Modulated-sideband Wave Front Sensing" (PMPMWFS), which demodulates the beat signal at the difference frequency between two anti-resonant PM sidebands. We derived the theoretical response of PMPMWFS and experimentally demonstrated it using the Power-Recycled X-arm (PRXARM) configuration of KAGRA. The results show that PMPMWFS effectively decouples angular fluctuation signals of the PRC and incident beam from those of the arm cavity and provides orthogonal signal components for the end mirror of the arm cavity. Furthermore, feedback control using PMPMWFS achieved stable interferometer locking for over one hour. These results demonstrate that PMPMWFS offers an effective sensing method for decoupling multiple alignment degrees of freedom in future gravitational-wave detectors.

</details>


### [7] [Quintessence Star Solutions with Conformal Symmetry in a Durgapal Spacetime](https://arxiv.org/abs/2511.17899)
*Meghanil Sinha,S. Surendra Singh*

Main category: gr-qc

TL;DR: 本文提出了一个具有共形对称性的新型各向异性致密星模型，该模型由quintessence场驱动，参数为$w_Q(-1<w_Q<-\frac{1}{3})$，基于Durgapal-Fuloria度规构建，满足所有物理约束条件。


<details>
  <summary>Details</summary>
Motivation: 宇宙加速膨胀归因于暗能量的存在，本文旨在研究quintessence场如何影响致密星的结构和稳定性。

Method: 使用Durgapal-Fuloria度规构建模型，引入具有共形对称性的quintessence场，通过解析方法和图形可视化分析物理属性。

Result: 模型成功满足TOV方程、能量条件、致密度因子、表面红移和因果性条件等所有必要物理约束。

Conclusion: 该各向异性致密星模型在quintessence场作用下表现出良好的物理特性，为研究暗能量对致密天体结构的影响提供了新视角。

Abstract: The accelerated expansion of the Universe can be suitably attributed to the existence of the dark energy (DE). On the backdrop of this concept, this paper introduces a novel, anisotropic compact star model whose stability and structure are governed by the presence of quintessence field, defined by the parameter $ w_{Q} (-1<w_{Q}<-\frac{1}{3}) $ and which admits conformal symmetry. The construction of the model relied on the Durgapal-Fuloria (DP) metric formulation. The model successfully meets all the necessary physical constraints viz., TOV equation, energy conditions, compactness factor, surface redshift and casuality condition. The results are analyzed through analytical methods as well as through the graphical visualization for the various physical attributes.

</details>


### [8] [Late-time cosmic dynamics in $f(R,L_{m})$ gravity with recent observations](https://arxiv.org/abs/2511.17903)
*Amit Samaddar,S. Surendra Singh*

Main category: gr-qc

TL;DR: 该论文研究了非线性f(R,L_m)引力框架下的晚期宇宙动力学，采用f(R,L_m)=R/2+L_m^2函数形式，使用振荡参数状态方程ω(z)=ω_0+bsin[log(1+z)]探索暗能量行为。


<details>
  <summary>Details</summary>
Motivation: 探索ΛCDM宇宙学的替代模型，通过非线性f(R,L_m)引力理论研究暗能量行为，允许与宇宙常数平滑偏离。

Method: 采用联合MCMC分析，结合哈勃31计时器数据、DESI DR2 BAO测量和Ia型超新星样本（Pantheon+、DES-SN5Y和Union 3）进行参数约束。

Result: 获得良好约束的参数：H_0≈67.2 km/s/Mpc，ω_0≈-0.5，与Planck 2018一致；模型显示从减速到加速的转变在z_tr∼0.7-0.8，满足NEC和DEC但违反SEC，当前EoS值接近-1；宇宙年龄t_0≈13.3 Gyr。

Conclusion: 提出的振荡f(R,L_m)模型提供了观测一致且动态可行的ΛCDM宇宙学替代方案，在晚期时间重现ΛCDM行为。

Abstract: In this work, we investigate the late-time cosmic dynamics in the framework of non-linear $f(R, L_m)$ gravity, adopting the functional form $f(R,L_m)=\frac{R}{2}+L_m^2$. To explore the dark energy behavior, we assume an oscillatory parametric equation of state, $ω(z) = ω_0 + b \sin[\log(1+z)]$, which allows smooth deviations from the cosmological constant. Using a joint MCMC analysis with the latest Hubble 31 chronometer data, DESI DR2 BAO measurements, and Type Ia supernova samples (Pantheon+, DES-SN5Y and Union 3), we obtain well-constrained parameters around $H_0 \simeq 67.2~\text{km s}^{-1}\text{Mpc}^{-1}$ and $ω_0\approx-0.5$, consistent with Planck 2018 and other current observations. The model exhibits a clear transition from deceleration to acceleration with $z_{\rm tr} \sim 0.7$--$0.8$, satisfies the NEC and DEC while violating the SEC and yields present EoS values close to $-1$, reproducing $Λ$CDM behavior at late times. The derived Universe ages ($t_0 \approx 13.3~\text{Gyr}$) agree well with CMB and stellar constraints, confirming that the proposed oscillatory $f(R, L_m)$ model provides an observationally consistent and dynamically viable alternative to $Λ$CDM cosmology.

</details>


### [9] [Probing Loop Quantum Gravity black holes through gravitational lensing](https://arxiv.org/abs/2511.17975)
*Arun Kumar,Qiang Wu,Tao Zhu,Sushant G. Ghosh*

Main category: gr-qc

TL;DR: 该论文研究了带电LQG黑洞的强引力透镜效应，发现LQG修正会增强偏转角并增加相对论性图像的角分离，这些量子引力修正可通过下一代VLBI设施观测到。


<details>
  <summary>Details</summary>
Motivation: 研究LQG黑洞的引力透镜效应，为量子引力理论提供可观测的检验途径，探索量子几何对黑洞引力透镜的影响。

Method: 推导零测地线方程，研究光子有效势，计算光子球半径和临界冲击参数，计算弱场偏转角和爱因斯坦环大小，强场区域计算强偏转系数和透镜可观测量。

Result: LQG修正增强偏转角，增加相对论性图像的角分离，对于M87*角分离s∈(0.05712,0.19123)μas，Sgr A*为s∈(0.07595,0.25426)μas，相对通量比r_mag∈(4.49272,5.96397)。

Conclusion: LQG诱导的修正留下特征性的强弱透镜印记，为利用近未来高分辨率观测探测量子引力提供了有前景的观测途径。

Abstract: We investigate strong gravitational lensing by a charged loop quantum gravity (LQG) black hole obtained through the polymerisation scheme of Borges \textit{et al.} \cite{Borges:2023fog}. These effective geometries replace the Reissner--Nordström singularity with a symmetric transition surface and admit an extremal, cold remnant determined by the minimal area gap in LQG. In turn, we derive the null geodesic equations, investigate the photon effective potential, and obtain expressions for the photon-sphere radius and critical impact parameter. We compute the weak-field deflection angle and Einstein ring size, highlighting the deviations induced by the polymerisation parameter and the Barbero--Immirzi parameter. In the strong-field regime, we compute the strong deflection coefficients $(\bar{a},\bar{b})$ and evaluate the lensing observables $θ_\infty$, $s$, and $r_{\rm mag}$. Unlike the Reissner--Nordström case, the LQG corrections enhance the deflection angle and increase the angular separation of relativistic images, with deviations growing as the geometry approaches the LQG remnant limit. We further compute the corresponding observables for Sgr~A* and M87*, finding that the quantum-gravity modifications lie within the potential sensitivity of next-generation VLBI facilities. For M87*, the angular separation $s\in(0.05712,0.19123)\,μ\text{as}$, while it is $s\in(0.07595,0.25426)\,μ\text{as}$ for Sgr A*. The relative flux ratio is found to lie in the range, $r_{\rm mag}\in(4.49272,5.96397)$. Our analysis demonstrates that LQG-induced corrections leave characteristic strong and weak-lensing imprints, offering a promising observational pathway to probe quantum gravity using near-future high-resolution observations.

</details>


### [10] [Dynamical interplay between coupled scalar dark sectors and gravity](https://arxiv.org/abs/2511.18585)
*Mihai Marciu*

Main category: gr-qc

TL;DR: 本文提出了一种基于标量张量理论中耦合场的新宇宙学模型，通过引入轴子-膨胀子系统与时空几何的非最小耦合，扩展了现有理论框架。


<details>
  <summary>Details</summary>
Motivation: 探索标量张量理论中耦合场的新宇宙学模型，特别是研究轴子-膨胀子系统与时空几何的非最小耦合如何影响宇宙演化，并可能缓解宇宙巧合问题。

Method: 采用线性稳定性理论分析物理性质，研究轴子-膨胀子系统与标量曲率编码的时空几何之间的独立非最小耦合。

Result: 分析表明当前理论模型与宇宙近期历史兼容，在特定情况下获得了模型参数的可行约束，相空间结构展现出独特的物理特征。

Conclusion: 该模型通过轴子-膨胀子系统与时空几何的非最小耦合，为缓解宇宙巧合问题提供了可能的解决方案，并与观测数据一致。

Abstract: We explore a novel cosmological model based on coupled fields in the framework of scalar tensor theories, considering the specific interplay between gravity and scalar fields. The model further extends a recent axion-dilaton system by introducing viable couplings with the space--time geometry encoded into the scalar curvature. After briefly introducing the action and the corresponding field equations, we employ linear stability theory to investigate the physical properties. The analysis showed the compatibility of the current theoretical model with the recent history of the Universe, obtaining viable constraints for the model's parameters in some specific cases. In the present setup, the axion--dilaton system is non--minimally coupled with gravity in an independent manner, leading to distinct physical features in the phase-space structure, possible alleviating the cosmic coincidence problem.

</details>


### [11] [Constraining linear form of $f(\mathcal{R,G,T})$ gravity from astrophysical observations of the Pulsar U1724](https://arxiv.org/abs/2511.18017)
*G. G. L. Nashed*

Main category: gr-qc

TL;DR: 本研究在扩展引力框架f(R,G,T)下分析致密星内部结构，获得静态各向异性恒星物质的精确解析解，并通过脉冲星U1724观测数据约束参数α和β。


<details>
  <summary>Details</summary>
Motivation: 利用质量超过1.8太阳质量的巨型射电脉冲星作为引力理论的测试平台，研究在实验室无法达到的极端条件下引力的性质。

Method: 采用线性形式f(R,G,T)=R+αG+βT的扩展引力理论，获得静态各向异性恒星物质在流体静力平衡下的精确解析解。

Result: 通过匹配脉冲星U1724的质量和半径数据，约束参数为α₁=±0.023和β₁=±0.001，得到的恒星构型满足径向声速的因果界限c_s²<c²/3。

Conclusion: 扩展引力框架f(R,G,T)能够描述致密星内部结构，其解在声速行为上与广义相对论不同，为极端引力条件下的物理提供了新的见解。

Abstract: In this work we examine the internal structure of compact stars within an extended gravitational framework described by the function $f(\mathcal{R},\mathcal{G},\mathcal{T})$. Throughout this work, the quantity $\mathcal{R}$ refers to the curvature scalar formed from the Ricci tensor. The term $\mathcal{G}$ denotes the Gauss--Bonnet curvature invariant, while $\mathcal{T}$ corresponds to the trace obtained by contracting the matter energy-momentum tensor. Our analysis is directed toward massive radio pulsars with masses above $1.8\,M_{\odot}$, which provide an exceptional testing ground for gravity under conditions inaccessible to laboratory experiments. Adopting the linear form $f(\mathcal{R},\mathcal{G},\mathcal{T})=\mathcal{R}+α\,\mathcal{G}+β\,\mathcal{T}$ where $α$ and $β$ are parameters of suitable dimensionality,\footnote{$α$ has dimensions of $[L^{2}]$ and $β$ carries units of $[N^{-1}]$.} we obtain an exact analytic solution for static anisotropic stellar matter in hydrostatic equilibrium. This solution allows all physical quantities to be expressed in terms of the dimensionless parameters $ α_{1}=α/R^{2},\qquad β_{1}=β/κ^{2}$ together with the compactness $C=2GM/(Rc^{2})$. We constraint the two parameters $α$ and $β$ by matching the model with the mass and radius of pulsar \textit{U1724} requires restricting these parameters to $α_{1}=\pm0.023$ and $β_{1}=\pm0.001$, where $κ^{2}=8πG/c^{4}$ is the standard Einstein coupling. The resulting stellar configuration satisfies the causal bound on the radial sound speed, $c_{s}^{2}<c^{2}/3$, distinguishing it from the corresponding behaviour in general relativity.

</details>


### [12] [Universal scalarization in topological AdS black holes](https://arxiv.org/abs/2511.18074)
*Zi-Qiang Zhao,Zhang-Yu Nie,Shao-Wen Wei,Jing-Fei Zhang,Xin Zhang*

Main category: gr-qc

TL;DR: 研究AdS时空中三种不同视界拓扑下带电标量场诱导的黑洞标量化行为，发现球形拓扑在低压力下具有更高的标量化温度范围，且标量化过程呈现复杂的相变行为。


<details>
  <summary>Details</summary>
Motivation: 探索扩展相空间中不同视界拓扑对黑洞标量化过程的影响，特别是研究标量化过程中的相变行为。

Method: 在渐近AdS时空中，研究带电标量场在三种不同视界拓扑（球形、平面、双曲）下的标量化行为，分析扩展相空间中的相变特性。

Result: 所有三种拓扑在低温下都会发生标量化，球形拓扑在低压力下具有更高的标量化温度范围。标量化凝聚体随压力增加从一阶相变转变为风洞式相变。

Conclusion: 该研究揭示了黑洞标量化过程中的零阶相变特性，并展示了扩展相空间中黑洞的完整相结构，球形拓扑在标量化过程中表现出独特的性质。

Abstract: We investigate the universal behavior of black hole scalarization induced by a charged scalar field in the extended phase space of the asymptotic AdS spacetime with three distinct horizon topologies. The results indicate that in all the three cases, the charged black hole spacetime undergoes scalarization at low temperatures. Notably, the spherical topology is unique in that its domain of scalarization theoretically extends to much higher temperatures under low pressure in the extended phase space. Moreover, the scalarization process in the spherical case exhibits complex phase transition behaviors without additional non-linear terms, which are similar to those in the planar and hyperbolic topologies with the assistance of non-linear terms. With increasing pressure in the extended phase space, the condensate of the scalarization in all three cases undergoes a transition from the first-order style to a cave-of-wind style. This study provides deeper insight into the zeroth-order phase transition during black hole scalarization and reveals the complete phase structure of black holes in the extended phase space.

</details>


### [13] [Primordial black hole driven cosmic acceleration](https://arxiv.org/abs/2511.18080)
*Konstantinos Dialektopoulos,Theodoros Papanikolaou,Vasilios Zarikas*

Main category: gr-qc

TL;DR: 本文提出了一种由具有排斥行为的原始黑洞驱动的宇宙加速机制，在"瑞士奶酪"宇宙学框架下，发现原始黑洞可以驱动宇宙加速阶段，并可能解决哈勃张力问题。


<details>
  <summary>Details</summary>
Motivation: 探索原始黑洞作为宇宙加速驱动机制的可能性，特别是在早期宇宙中作为暗能量成分的潜在作用。

Method: 采用"瑞士奶酪"宇宙学框架，分析规则黑洞时空（如Hayward、Bardeen、Dymnikova）以及奇异的Schwarzschild-de Sitter情况下的黑洞行为。

Result: 发现原始黑洞可以驱动稳健的宇宙加速阶段，超轻质量黑洞（m < 5×10^8 g）能触发指数膨胀并优雅退出，而中等质量黑洞（m ~ 10^12 g）在物质-辐射平衡时期可作为早期暗能量成分。

Conclusion: 原始黑洞具有作为宇宙加速驱动机制的潜力，特别是超轻质量黑洞可驱动早期宇宙膨胀，中等质量黑洞可能解决哈勃张力问题。

Abstract: We propose a natural mechanism for cosmic acceleration driven by primordial black holes (PBHs) with repulsive behavior, within a ''Swiss Cheese'' cosmological framework. Considering regular black hole spacetimes such as Hayward, Bardeen, and Dymnikova-as well as the singular Schwarzschild-de Sitter case-we consistently find a robust PBH-driven cosmic acceleration phase. This phase ends either at an energy scale set by the PBH parameters or through black hole evaporation. Notably, one finds that ultra-light PBHs with $m < 5 \times 10^8 \, {\rm g}$ can trigger exponential inflation with graceful exit and reheating. Additionally, PBHs with $m \sim 10^{12} \, {\rm g}$ and abundances $0.107 < Ω_{\rm PBH}^{\rm eq} < 0.5$ near matter-radiation equality can act as an early dark energy component, offering a potential resolution to the Hubble tension.

</details>


### [14] [Hawking-radiation-ignited autocatalytic formation of primordial black holes](https://arxiv.org/abs/2511.18110)
*Alexander Yakimenko*

Main category: gr-qc

TL;DR: 该论文提出了一种自催化机制，其中蒸发微型原初黑洞的霍金辐射爆发会触发近临界等离子体密度塌缩，形成新的黑洞，这种反馈机制在宇宙膨胀过程中自组织成传播的点燃前沿并最终自熄灭。


<details>
  <summary>Details</summary>
Motivation: 研究原初黑洞形成的新机制，探索霍金辐射如何触发等离子体塌缩并形成自组织的黑洞形成过程，为引力波观测提供可测试的理论模型。

Method: 构建了最小反应-扩散模型，分析点燃和冻结的保守判据，预测具有尖锐因果低频边缘的随机引力波背景。

Result: 模型预测的亚赫兹到音频频段引力波振幅满足宇宙学能量注入约束，为即将到来的引力波天文台提供了清晰可测试的目标。

Conclusion: 该自催化机制提供了一种原初黑洞形成的可行途径，其产生的引力波信号具有独特的频谱特征，对Planck尺度的黑洞终点微观物理不敏感，是可观测检验的清洁目标。

Abstract: We propose and analyze an autocatalytic mechanism in which bursts of Hawking radiation from evaporating micro-primordial black holes (PBHs) trigger the collapse of near-critical plasma overdensities. In a primordial plasma seeded with such patches, this feedback self organizes into a traveling ignition front that successively forms new PBHs and then self-quenches as the Universe expands. A minimal reaction-diffusion model yields conservative criteria for ignition and freeze-out and predicts a stochastic gravitational-wave background with a sharp causal low-frequency edge set by the freeze-out correlation length and largely insensitive to Planck-scale PBH endpoint microphysics. The resulting sub-Hz-to-audio band and amplitudes satisfy cosmological energy-injection bounds, providing a clean, testable target for forthcoming gravitational-wave observatories.

</details>


### [15] [Violations of the Weak Energy Condition for Lentz Warp Drives](https://arxiv.org/abs/2511.18251)
*Bill Celmaster,Steve Rubin*

Main category: gr-qc

TL;DR: 该论文反驳了Lentz在2020年提出的新型曲速驱动可以避免负能量密度的说法，通过直接计算证明该曲速驱动仍然违反弱能量条件。


<details>
  <summary>Details</summary>
Motivation: Lentz声称其提出的新型曲速驱动具有非负能量密度，可以由经典等离子体提供能量源，这挑战了传统曲速驱动违反弱能量条件的认知。

Method: 首先在欧拉参考系中直接计算Lentz曲速驱动的能量-动量张量，然后分析Lentz研究的理论基础并识别推导错误，最后构建修正几何模型进行验证。

Result: 发现Lentz曲速驱动存在负能量密度区域，其推导存在多个错误，即使修正后的几何模型仍然违反弱能量条件。

Conclusion: Lentz关于曲速驱动可以避免负能量密度的说法是错误的，这类几何结构仍然受制于禁止定理的限制。

Abstract: Warp drive spacetimes capable of superluminal transportation, were first introduced in 1994 by Miguel Alcubierre and then generalized by others. These spacetimes violated the Weak Energy Condition (WEC). Lentz proposed a new type of warp drive in 2020. It was claimed that this warp spacetime has non-negative energy density and can therefore be sourced by a classical plasma. We demonstrate that Lentz's claim is incorrect. We begin with a direct calculation of the energy-momentum tensor of Lentz's warp drive in a Eulerian reference frame, and show that there are spacetime regions where the energy density is negative. We then examine the theoretical basis of Lentz's investigation and identify several derivation errors. The derivation errors can be somewhat ameliorated with a modified version of Lentz's geometry, which more closely respects the equalities and inequalities discussed by Lentz. Even so, we show that the modified geometry still violates the WEC even in the Eulerian reference frame. We conclude with a brief discussion of no-go theorems and their relationship to geometries of the kind proposed by Lentz.

</details>


### [16] [Constraint on massive vector field with extreme-mass-ratio inspirals around a slowly rotating black hole](https://arxiv.org/abs/2511.18435)
*Tieguang Zi,Peng-Cheng Li,Bao-Min Gu,Fu-Wen Shu*

Main category: gr-qc

TL;DR: 研究大质量矢量场对极端质量比旋进系统能量通量的影响，发现Proca场会产生额外的偶极辐射，改变轨道演化并导致波形失配，LISA能够探测到光Proca场的信号并对Proca质量施加严格约束。


<details>
  <summary>Details</summary>
Motivation: 研究爱因斯坦-普罗卡框架下大质量矢量场如何影响极端质量比旋进系统的能量通量和轨道演化，探索LISA探测此类修正引力理论的能力。

Method: 通过分析携带Proca毛发的次级致密天体在缓慢旋转Kerr黑洞周围的偶极辐射，计算总能量通量变化，评估波形失配度，并使用Fisher信息矩阵分析预测LISA对Proca质量的约束能力。

Result: 发现Proca场会产生额外的偶极辐射，导致轨道演化的长期漂移和波形失配，LISA能够探测到μ_v∼10^{-20}eV量级的Proca质量，典型分数不确定性在百分之几十水平，具体取决于黑洞自旋。

Conclusion: LISA能够通过极端质量比旋进信号有效探测或约束光Proca场，为检验修正引力理论提供重要手段。

Abstract: We study the influence of a massive vector (Proca) field on the energy fluxes from extreme-mass-ratio inspirals (EMRIs) around a slowly rotating Kerr black hole. The secondary compact object, carrying a Proca hair, emits additional dipolar radiation that alters total energy flux relative to general relativity (GR). These modifications induce a secular drift in the orbital evolution of circular geodesic orbits, leading to measurable dephasing in the resulting EMRIs waveforms. By evaluating waveform mismatches between the Einstein-Proca framework and its GR counterpart, we show that the Laser Interferometer Space Antenna (LISA) can distinguish the signatures of a light Proca field when black hole rotation is included. Furthermore, using a Fisher information matrix analysis, we forecast LISA's capability to place stringent constraints on the Proca mass with EMRIs signal from slowly rotating Kerr black holes. For representative EMRIs configurations, we find that LISA can detect or constrain Proca masses down to $μ_v\sim 10^{-20}$eV, with typical fractional uncertainties at the level of tens percent, depending on the black-hole spin.

</details>


### [17] [Energy transfer between the sources in gravitational decoupling](https://arxiv.org/abs/2511.18485)
*Daulet Berkimbayev*

Main category: gr-qc

TL;DR: 提出了一种完全解析的方法来研究多方流体在静态球对称时空中如何影响任意引力源，并具体应用于探索自引力系统中引力源之间的能量转移机制。


<details>
  <summary>Details</summary>
Motivation: 研究多方流体对引力源的影响，特别是探索自引力系统中不同引力源之间的能量转移内部机制。

Method: 使用完全解析的方法，在静态球对称时空背景下分析多方流体对任意引力源的影响。

Result: 开发了一个直接且完全解析的框架来研究多方流体与引力源的相互作用。

Conclusion: 该方法为分析自引力系统中能量转移机制提供了有效的解析工具，特别适用于研究多方流体环境下的引力源相互作用。

Abstract: A straightforward and fully analytic approach is introduced to examine how polytropic fluids influence arbitrary gravitational sources in static, spherically symmetric spacetimes. As a concrete application, we explore the internal mechanism of energy transfer between gravitational sources embedded within a self-gravitating system.

</details>


### [18] [Looking through the Kerr disk](https://arxiv.org/abs/2511.18502)
*Maciej Maliborski,Tobias C. Sutter*

Main category: gr-qc

TL;DR: 研究克尔时空最大延拓中连接两个渐近平坦区域的零测地线，这些测地线穿过视界和环奇点，连接正负r区域。识别了内喉参数空间，解析和数值求解测地线方程，构建负r区域观测者的模拟视图。


<details>
  <summary>Details</summary>
Motivation: 探索克尔黑洞最大延拓时空的完整几何结构，特别是连接两个渐近平坦区域的零测地线行为，这对于理解黑洞内部结构和可能的白洞配置具有重要意义。

Method: 使用冲击参数识别内喉参数空间，在类爱丁顿-芬克尔斯坦坐标下解析和数值求解测地线方程，构建负r区域观测者的模拟视图。

Result: 发现内喉区域最多存在两条恒定纬度测地线，其中一条与主零方向对齐。识别了限制到达渐近观测者的测地线范围的禁止极角带。修正并扩展了现有公式，揭示了强图像扭曲和反转现象。

Conclusion: 克尔时空最大延拓中的零测地线连接了两个渐近平坦区域，揭示了时空的复杂几何结构，对理解白洞类似配置有潜在意义，图像扭曲现象表明负r区域的观测者会看到显著不同的视觉效应。

Abstract: We study null geodesics that connect the two asymptotically flat regions of the maximally extended Kerr spacetime. These vortical geodesics traverse both horizons and pass through the ring singularity, linking the positive-$r$ exterior to the negative-$r$ asymptotic side. Using impact parameters, we identify a closed subset of parameter space, the inner throat, where the radial potential has no real roots, and photons exhibit no radial turning points. In this region, at most two constant-latitude geodesics exist, one of which is aligned with the principal null direction. We also identify the forbidden polar-angle band that limits the range of geodesics reaching an asymptotic observer. We solve the geodesic equations analytically and numerically in Eddington-Finkelstein-like coordinates, obtaining mutually consistent results that correct and extend previously available formulae. The resulting trajectories are used to construct simulated views for an observer in the negative-$r$ domain, revealing strong image distortion and inversion, with possible implications for analogous white-hole configurations.

</details>


### [19] [Non-Singular Bouncing Cosmology from Hyperbolic Field Space Geometry](https://arxiv.org/abs/2511.18522)
*Oleksandr Kravchenko*

Main category: gr-qc

TL;DR: 在封闭宇宙中研究双场宇宙学模型，利用场空间的双曲几何引入动力学耦合，通过空间曲率驱动无奇点反弹，满足NEC条件，预测可观测的引力波信号和非高斯性。


<details>
  <summary>Details</summary>
Motivation: 探索在满足零能量条件(NEC)的前提下，通过场空间曲率引入的动力学耦合机制，实现由正空间曲率驱动的无奇点宇宙反弹，避免依赖奇异物理。

Method: 构建封闭宇宙(k=+1)中的双场模型，场空间采用双曲几何，利用场空间曲率产生动力学耦合，抑制标量场动能，使空间曲率主导并触发反弹。

Result: 获得了反弹的解析解并通过数值验证，模型预测张标比r≈0.003-0.005，局域非高斯性f_NL~O(1)，且模型无鬼场构造。

Conclusion: 该模型提供了一种在满足NEC条件下由空间曲率驱动的无奇点反弹机制，其预测的观测信号可被下一代CMB实验检验。

Abstract: We investigate a two-field cosmological model in a closed ($k=+1$) universe where the field space is endowed with a hyperbolic geometry. We demonstrate that the curvature of the field space introduces a kinetic coupling that exponentially suppresses the scalar field kinetic energy as the fields explore certain regions, allowing the spatial curvature to dominate and trigger a non-singular bounce. Crucially, the model satisfies the Null Energy Condition (NEC) throughout, with the bounce driven entirely by the positive spatial curvature -- not by exotic physics. We derive analytic solutions for the bounce, verify them numerically, and compute observable predictions. The model predicts a tensor-to-scalar ratio $r \approx 0.003$--$0.005$ and local non-Gaussianity $f_{\rm NL} \sim \mathcal{O}(1)$, placing it within reach of next-generation CMB experiments such as LiteBIRD and CMB-S4. The model is ghost-free by construction.

</details>


### [20] [Off-Equatorial Orbits around Magnetically Charged Black Holes](https://arxiv.org/abs/2511.18821)
*Xilai Li,David E. Kaplan,Loris Del Grosso*

Main category: gr-qc

TL;DR: 本文完整描述了磁荷黑洞周围稳定的非赤道圆轨道特性，推导了轨道纬度与半径的精确解析关系，证明了这些轨道在同步辐射下的稳定性，并扩展到旋转黑洞情况。


<details>
  <summary>Details</summary>
Motivation: 研究磁荷黑洞周围非赤道圆轨道的存在性和稳定性，探索这些独特轨道对黑洞成像和偏振观测的潜在影响。

Method: 对于静态球对称磁荷黑洞，推导轨道纬度与半径的精确解析表达式；对于旋转磁荷黑洞，数值计算顺行和逆行轨道分支，分析参考系拖曳效应。

Result: 发现带电粒子在ISCO半径处可呈现O(1)量级的纬度偏离，且在极小磁荷值下仍保持同步辐射稳定性；证明这些非赤道轨道是磁荷特有的现象，在类似电荷Kerr-Newman时空中被禁止。

Conclusion: 磁荷黑洞周围环境可能表现出独特的现象学特征，对黑洞成像和偏振观测具有潜在意义。

Abstract: We present a complete characterization of stable, off-equatorial circular orbits around magnetically charged black holes (MBHs). For a static, spherically symmetric MBH, we derive an exact analytic expression for the orbital latitude theta as a function of radius r. We establish a direct connection between these orbits and the spacetime fundamental structure, and demonstrate their stability against synchrotron radiation. We show that charged particles such as electrons and protons can exhibit O(1) latitude deviations at the ISCO radius and remain stable under synchrotron emission even for extremely small values of the black hole magnetic charge. We then extend the analysis to rotating MBHs, numerically computing the prograde and retrograde orbital branches and demonstrating how frame-dragging modifies their structure and stability regions. We show that these off-equatorial orbits are a unique feature of the magnetic charge, being forbidden in the analogous electrically charged Kerr-Newman spacetime. Our results suggest that environments surrounding magnetically charged black holes can exhibit distinctive phenomenological signatures, with potential implications for black hole imaging and polarimetric observations.

</details>


### [21] [Schrödinger-type $f(Q,T)$ gravity-nonmetricity driven cosmological evolution from inflation to the late Universe](https://arxiv.org/abs/2511.18866)
*Lei Ming,Himanshu Chaudhary,Shi-Dong Liang,Hong-Hao Zhang,Tiberiu Harko*

Main category: gr-qc

TL;DR: 本文研究了具有薛定谔型矢量非度规性的f(Q,T)引力理论，分析了其在宇宙学中的应用，特别是早期宇宙暴胀和晚期宇宙演化，并与ΛCDM模型进行了观测对比。


<details>
  <summary>Details</summary>
Motivation: 探索在非度规性引力理论框架下描述宇宙演化的可能性，特别是研究早期暴胀和晚期加速膨胀的物理机制。

Method: 采用f(Q,T)引力理论，引入薛定谔型矢量非度规性，通过拉格朗日乘子约束总标量曲率为零，推导广义弗里德曼方程，并进行MCMC分析对比观测数据。

Result: 模型能够描述早期宇宙暴胀到减速膨胀的转变，产生辐射，且与哈勃函数观测数据吻合良好，与ΛCDM模型结果相当。

Conclusion: 薛定谔型f(Q,T)引力理论能够很好地描述早期和晚期宇宙的观测数据，为引力理论提供了有前景的替代方案。

Abstract: We consider an $f(Q, T)$ gravity theory with a Schrödinger type vectorial non-metricity. In the presence of such a non-metricity, the length of vectors is preserved under autoparallel transport. We obtain the field equations assuming a vanishing total scalar curvature, implemented by a Lagrange multiplier, and investigate their cosmological implications. To do this, we derive the generalized Friedmann equations which now have terms involving the non-metricity and the Lagrange multiplier. Then, we consider two distinct cosmological applications of the model. First of all, by adopting distinct forms of these two basic variables and investigate the possibility of the existence of warm inflationary scenarios within the framework of these models. In particular, we consider the case that the non-metricity is described by a constant vector, and we show that with this assumption we recover standard general relativity. The scenario in which the Lagrange multiplier is a constant is also investigated, and we show that radiation can be created during the very early phases of expansion. The amount of radiation peaks at a certain time after which, there is a transition from an accelerating inflationary phase to a decelerating one. Moreover, we perform a detailed comparison of the predictions of the considered Schrödinger type cosmology with a set of observational data for the Hubble function, including Cosmic Chronometers, Type Ia Supernovae, and Baryon Acoustic Oscillations, using a Markov Chain Monte Carlo (MCMC) analysis, by adopting a simple linear form for the Lagrange density. The model predictions are also compared with the results of the $Λ$CDM standard paradigm. Our results indicate that the Schrödinger $f(Q,T)$ type theory can give a good description of the observational data for both the very early and the late Universe.

</details>


### [22] [Black hole binaries in shift-symmetric Einstein-scalar-Gauss-Bonnet gravity experience a slower merger phase](https://arxiv.org/abs/2511.19073)
*Maxence Corman,Llibert Aresté Saló,Katy Clough*

Main category: gr-qc

TL;DR: 在移位对称爱因斯坦-标量-高斯-博内引力中，黑洞具有非零标量荷。早期轨道演化因标量偶极辐射而加速，但非线性数值模拟显示在合并后期轨道动力学改变导致减速合并。


<details>
  <summary>Details</summary>
Motivation: 研究移位对称爱因斯坦-标量-高斯-博内引力中黑洞并合的完整动力学，检验后牛顿近似在强场区域的有效性，并重新评估现有理论约束。

Method: 使用完全非线性数值模拟研究准圆形、可比质量双星系统，分析标量-高斯-博内引力对轨道动力学的影响。

Result: 早期轨道演化因-1PN阶标量偶极辐射而加速，但后期由于保守动力学改变导致合并阶段减速，需要更多能量才能引起相同频率变化。

Conclusion: 非线性效应在强场区域显著改变引力扇区，现有基于后牛顿近似到合并的理论约束需要重新评估，信号中应观察到从加速到减速的频率演化转变。

Abstract: In shift-symmetric Einstein-scalar-Gauss-Bonnet gravity, stationary black holes have a non-vanishing scalar charge. During the inspiral, the phase evolution is modified by several effects, primarily an additional scalar dipole radiation, which enters at -1PN order. This effect accelerates the inspiral when compared to general relativity, when including corrections up to 2PN. Using fully non-linear numerical simulations of quasi-circular, comparable mass binaries, we find that in the late stages the orbital dynamics are altered so that the overall effect is instead a decelerated merger phase for the modified gravity case. We attribute this to a change in the conservative dynamics, and show that at the late inspiral stage more energy must be emitted in scalar-Gauss-Bonnet gravity to induce a given change in frequency. In longer signals, this should lead to a distinctive switch between a faster and slower frequency evolution relative to general relativity as the binary approaches merger. This work suggests we should revisit existing constraints on the theory that are obtained assuming PN approximations apply up to merger, or based on order by order approximations that neglect backreaction effects on the metric, and shows the importance of including non-linear effects that modify the gravitational sector in the strong field regime.

</details>


### [23] [Ghost instabilities and strong coupling in quadratic non-metricity theories](https://arxiv.org/abs/2511.19101)
*Alexander Ganz,Marco Spinelli*

Main category: gr-qc

TL;DR: 该论文研究了包含宇称破坏项的新广义相对论框架，发现除STEGR加宇称破坏算子的特殊情况外，理论普遍存在鬼场不稳定性。该特殊情况在非线性水平传播8个自由度，在FLRW背景下强耦合。


<details>
  <summary>Details</summary>
Motivation: 研究包含所有非度量张量二次不变量的新广义相对论框架，特别是包含唯一二次宇称破坏项的情况，分析其在FLRW背景下的线性扰动稳定性。

Method: 使用Dirac-Bergmann程序和Cartan-Kuranishi算法分析线性扰动，研究理论在平坦FLRW背景下的动力学行为。

Result: 理论普遍存在鬼场不稳定性，只有STEGR加宇称破坏算子的特殊情况除外。该特殊情况在非线性水平传播8个自由度，在FLRW背景下强耦合。

Conclusion: 宇称破坏的STEGR扩展在FLRW背景下强耦合，超出了标准线性扰动理论的有效范围。

Abstract: We revisit the framework of Newer General Relativity, defined by all independent quadratic invariants of the non-metricity tensor, including the unique quadratic parity-violating term. We analyze linear perturbations around a flat FLRW background and find that the theory generically exhibits ghost instabilities, except for the special case of Symmetric Teleparallel Equivalent of General Relativity (STEGR) supplemented by the parity-violating operator. Using both the Dirac-Bergmann procedure and the Cartan-Kuranishi algorithm, we show that this remaining case propagates eight degrees of freedom at the fully non-linear level. This result implies that the parity-violating extension of STEGR is strongly coupled around FLRW and therefore lies beyond the regime of validity of standard linear perturbation theory.

</details>


### [24] [The Einstein constraints and differential forms](https://arxiv.org/abs/2511.19129)
*Andrzej Okolow,Jakub Szymankiewicz*

Main category: gr-qc

TL;DR: 使用微分形式表达真空爱因斯坦约束，通过特殊正交余标架选择将标量约束中的二阶项降为零，从而在实解析度量下局部表达为一阶PDE系统。


<details>
  <summary>Details</summary>
Motivation: 将爱因斯坦约束方程用微分形式表示，旨在简化约束方程的数学结构，特别是在实解析度量情况下实现局部一阶PDE表达。

Method: 采用微分形式方法，选择特殊的正交余标架来消除标量约束中的二阶项，将约束方程转化为一阶偏微分方程组。

Result: 证明了在实解析度量条件下，真空爱因斯坦约束可以局部表达为一阶PDE系统。

Conclusion: 通过微分形式和特殊余标架选择，成功将爱因斯坦约束方程简化为局部一阶系统，为约束方程的数学处理提供了新视角。

Abstract: We express the vacuum Einstein constraints in terms of differential forms - the forms include one-forms constituting an orthonormal coframe of the spatial metric. We show that if the metric is real-analytic, then the constraints can be always expressed locally as a system of first order PDE's - this system is obtained by a special choice of the coframe, which reduces to zero all second order terms in the scalar constraint.

</details>


### [25] [Dynamical system analysis of the cosmological phases in Palatini $k$-essence gravity](https://arxiv.org/abs/2511.19154)
*Fabio Moretti,Flavio Bombacigno*

Main category: gr-qc

TL;DR: 本文研究了在Palatini f(R)引力框架下的广义k-essence模型，分析了两个标量场的动力学特性，推导了无Ostrogradsky模式和初值问题适定性的条件，并通过动力系统方法研究了宇宙学演化。


<details>
  <summary>Details</summary>
Motivation: 研究Palatini f(R)引力与k-essence标量场耦合的理论框架，探索这种扩展引力理论在宇宙学中的应用潜力，特别是理解早期和晚期宇宙的演化行为。

Method: 构建了Palatini f(R)引力下的广义k-essence模型，将其转化为双标量-张量理论，通过代数方法求解Palatini标量子，使用动力系统方法分析FLRW时空的宇宙学演化。

Result: 发现了相空间中存在多种固定点，包括(准)de-Sitter时期、异宿轨道连接的演化路径、标度解和精质相，这些对应着宇宙不同演化阶段的有效状态参数weff。

Conclusion: 该理论框架能够描述丰富的宇宙演化历史，包括暴胀、暗能量主导时期以及中间过渡阶段，为理解宇宙动力学提供了新的理论视角。

Abstract: We formulate a generalized $k$-essence model in the presence of a Palatini $f(\mathcal{R})$ gravitational sector. In the corresponding biscalar-tensor theory, we discuss the distinguished dynamical properties of the two scalar fields, elucidating how the Palatini scalaron can be still algebraically solved in terms of matter, the $k$-essence field and its kinetic term. We derive the conditions ensuring the absence of Ostrogradsky modes and the well-posedness of the initial data problem, also providing an intriguing analogy with a specific class of DHOST theories. Then, we investigate the cosmology of a flat Friedmann-Lemaître-Robertson-Walker spacetime according a dynamical system approach, with the aim of determining the set of fixed points in the phase space, representing specific periods of the Universe evolution and characterized by different effective barotropic index $w_{\text{eff}}$. The analysis reveals the presence of a range of possible configurations, with the existence of (quasi) de-Sitter epochs connected by heteroclinic orbits, scaling solutions and quintessence phases.

</details>


### [26] [Spherical Einstein-Friedberg-Lee-Sirlin boson stars: Self-interacting solutions and their astrophysical appearance](https://arxiv.org/abs/2511.19206)
*Pedro L. Brito de Sá,Haroldo C. D. Lima,Carlos A. R. Herdeiro,Luís C. B. Crispino*

Main category: gr-qc

TL;DR: 研究自相互作用爱因斯坦-弗里德伯格-李-西尔林模型中的玻色子星，发现正自相互作用项能增加最大质量和致密性，使这些星体无需超轻玻色子质量即可达到钱德拉塞卡极限质量。在实标量场无质量极限下，解具有更大有效半径和更宽稳定范围。这些致密星体产生强引力透镜效应，其阴影可能视觉上模仿黑洞。


<details>
  <summary>Details</summary>
Motivation: 探索自相互作用E-FLS模型中玻色子星的特性，特别是研究自相互作用如何影响星体质量和致密性，以及这些天体在观测上可能产生的引力透镜效应。

Method: 在自相互作用E-FLS模型框架下分析静态解族，涵盖广泛参数范围包括复标量场的自相互作用。使用反向光线追踪生成天体物理图像。

Result: 正自相互作用项增加E-FLS星的最大质量和致密性，使其能达到钱德拉塞卡极限质量。在实标量场无质量极限下，解具有更大有效半径和更宽稳定解范围。这些致密星体产生强引力透镜，形成可能模仿黑洞的阴影。

Conclusion: 自相互作用E-FLS星是重要的天体物理候选体，其强引力透镜效应产生的阴影可能在观测上模仿黑洞，为电磁巡天观测提供了潜在的可探测特征。

Abstract: We investigate boson stars within the framework of the self-interacting Einstein-Friedberg-Lee-Sirlin (E-FLS) model, constituted by a complex scalar field with a quartic self-interaction and a real scalar field. Our analysis explores the family of static solutions across a broad range of parameters, including the self-interaction of the complex scalar field. We obtain that positive self-interaction terms increase the maximum mass and compactness of E-FLS stars, allowing them to reach masses comparable to the Chandrasekhar limit without the need of ultralight bosonic masses. Moreover, in the limit where the real scalar field becomes massless, the solutions present larger effective radii and allow a broader range of stable solutions. Astrophysical images, generated via backward ray-tracing, show that these compact, self-interacting E-FLS stars produce strong gravitational lensing, yielding shadows that could visually mimic black holes, thus providing potential observational signatures detectable in ongoing electromagnetic surveys.

</details>


### [27] [Inflation in theories with broken diffeomorphisms](https://arxiv.org/abs/2511.19353)
*Antonio L. Maroto,Prado Martín-Moruno,Miguel Orbaneja-Pérez*

Main category: gr-qc

TL;DR: 分析破坏暴胀子场中微分同胚不变性的影响，研究在横向微分同胚子群不变性下的暴胀模型，推导慢滚参数和原初功率谱，并与CMB观测数据对比。


<details>
  <summary>Details</summary>
Motivation: 研究在破坏完整微分同胚不变性、仅保持横向微分同胚不变性的情况下，暴胀模型是否仍能实现慢滚暴胀相，以及这种对称性破坏对宇宙学观测的影响。

Method: 采用横向微分同胚不变的暴胀模型，推导慢滚参数、e-fold数和曲率扰动的原初功率谱，结合渐近分析和数值方法详细研究二次势模型。

Result: 标量谱指数出现修正，与Planck和ACT的CMB数据对比；暴胀后行为与完整微分同胚不变情况显著不同，展现出新的动力学机制。

Conclusion: 破坏微分同胚不变性会显著改变暴胀模型的动力学行为，产生可观测的谱指数修正，并为暴胀后演化提供新的可能性。

Abstract: We analyze the impact of breaking diffeomorphism invariance in the inflaton sector. In particular, we consider inflaton models which are invariant under the subgroup of transverse diffeomorphisms and address the possibility of implementing a slow-roll phase. We obtain the corresponding expressions for relevant quantities such as the slow-roll parameters and the number of $e$-folds, and derive the primordial power-spectrum of curvature perturbations. The scalar spectral index features modifications which are confronted with CMB data from Planck and ACT. We study in detail the quadratic potential model, combining asymptotic and numerical analysis. We show that the post-inflationary behavior can be drastically different from the diffeomorphism-invariant case, exhibiting novel dynamical regimes.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [28] [H3PC: Hypersonic, High-Order, High-Performance Code with Adaptive Mesh Refinement and Real Chemistry](https://arxiv.org/abs/2511.17551)
*Ahmad Peyvan,Khemraj Shukla,George Em Karniadakis*

Main category: physics.comp-ph

TL;DR: 开发了基于Julia框架的高超音速高阶高性能代码H³PC，用于模拟复杂三维几何中的可压缩流动，支持化学反应模型和并行自适应网格细化


<details>
  <summary>Details</summary>
Motivation: 需要开发能够处理高超音速湍流流动的高性能计算工具，特别是要集成化学反应模型来准确模拟真实气体效应

Method: 采用间断伽辽金谱元法，集成Mutation++化学反应库，支持CPU并行计算和自适应网格细化

Result: 成功将Mutation++集成到H³PC求解器中，通过多个基准测试验证了代码的准确性

Conclusion: H³PC是一个功能强大的高超音速流动模拟工具，能够准确处理复杂几何和化学反应效应

Abstract: We have developed a hypersonic high-order, high-performance code (H$^3$PC) utilizing the ``Trixi.jl" framework in order to simulate both non-reactive and chemically reactive compressible Euler and Navier-Stokes equations for complex three-dimensional geometries. H$^3$PC is parallel on CPU platforms and can perform exascale parallel computations of hypersonic turbulent flows. The numerical approach is based on the discontinuous Galerkin spectral element method, satisfying the entropy and energy stability conditions for the Euler equations. H$^3$PC can perform simulations of high-speed flows from subsonic to hypersonic speeds based on frozen, equilibrium, and non-equilibrium chemistry modeling of the gas mixture, using the \texttt{Mutation.jl} , which is a Julia package developed to wrap the C++-based Mutation++ library. H$^3$PC can also perform parallel adaptive mesh refinement for two- and three-dimensional Euler and Navier-Stokes discretizations with non-conforming elements. In this study, we first demonstrate the successful integration of Mutation++ into the H$^3$PC solver, and then verify its accuracy through simulations of Taylor-Green vortex flow, supersonic flow past a square and circular cylinder, and hypersonic P8-inlet.

</details>


### [29] [Generation of Granular Deposition Interfaces using conditional Generative Adversarial Network (cGAN)](https://arxiv.org/abs/2511.18224)
*Seyed Feyzelloh Ghavami Mirmahalle,Seyed Ehsan Nedaaee Oskoee,Maniya Maleki*

Main category: physics.comp-ph

TL;DR: 使用条件生成对抗网络(cGAN)生成颗粒沉积的一维界面轮廓，用AI模型替代计算密集的分子动力学模拟


<details>
  <summary>Details</summary>
Motivation: 替代计算密集的分子动力学模拟，快速生成颗粒沉积界面并获取稳定的统计特性

Method: 采用U-Net生成器和ResNet判别器的cGAN模型，在LAMMPS颗粒包动态模拟数据上训练，测试不同流体介质下的性能

Result: ML生成的界面与动态模拟结果比较，生成大量界面以获得更稳定的统计特性，成功替代分子动力学模拟

Conclusion: cGAN模型能有效生成颗粒沉积界面，为研究界面生长统计特性提供高效替代方案

Abstract: This work aims at generating 1D interface profiles of granular deposition by a conditional generative adversarial network (cGAN). Our cGAN model employs a U-Net generator and a ResNet discriminator that, in competition with each other, produce granular interfaces. The network is trained on dynamic simulation data from the LAMMPS granular package. Different fluids (water, acetone, and hexane) were used for the medium of the deposition cell to check the model performance in different growing conditions. The same model with the same hyperparameters was trained on data from different media separately. The ML-generated interfaces are compared with those of dynamic simulations, and a large number of interfaces are then produced to obtain more stable statistical properties of granular deposition. This way, the computationally extensive molecular dynamics simulation is substituted by the AI model. The statistical trend of interface growth is diagrammed, and the generated interfaces are also analyzed in terms of statistical features. Keywords: Conditional Generative Adversarial Networks, ResNet, U-Net, Granular Deposition, Interface Growth.

</details>


### [30] [A fast-converging and asymptotic-preserving method for adjoint shape optimization of rarefied gas flows](https://arxiv.org/abs/2511.18433)
*Yanbing Zhang,Ruifeng Yuan,Lei Wu*

Main category: physics.comp-ph

TL;DR: 提出了基于伴随方法的形状优化新方法，通过通用合成迭代方案(GSIS)加速伴随动力学方程求解，显著降低稀薄气体流动优化的计算成本。


<details>
  <summary>Details</summary>
Motivation: 伴随形状优化在流体动力学中很有效，但扩展到稀薄气体流动时计算成本巨大，因为需要求解六维原始和伴随玻尔兹曼方程。

Method: 基于GSIS框架，通过结合宏观合成方程的解来加速伴随动力学方程收敛，该方程包含牛顿应力定律和捕捉稀薄效应的高阶项。

Result: 在3D物体阻力最小化问题中，过渡流态下阻力减少34.5%，滑移流态下减少61.1%，仅需约10次优化迭代。每个候选形状的原始和伴随玻尔兹曼方程收敛解仅需几十次速度分布函数更新。

Conclusion: 该方法实现了渐近保持特性，在连续极限下允许使用大空间网格，同时在高度稀薄状态下保持精度，与传统方法相比显著降低了计算成本。

Abstract: Adjoint based shape optimization is a powerful technique in fluid-dynamics optimization, capable of identifying an optimal shape within only dozens of design iterations. However, when extended to rarefied gas flows, the computational cost becomes enormous because both the six dimensional primal and adjoint Boltzmann equations must be solved for each candidate shape. Building on the general synthetic iterative scheme (GSIS) for solving the primal Boltzmann model equation, this paper presents a fast converging and asymptotic preserving method for solving the adjoint kinetic equation. The GSIS accelerates the convergence of the adjoint kinetic equation by incorporating solutions of macroscopic synthetic equations, whose constitutive relations include the Newtonian stress law along with higher order terms capturing rarefaction effects. As a result, the method achieves asymptotic preservation (allowing the use of large spatial cell sizes in the continuum limit) while maintaining accuracy in highly rarefied regimes. Numerical tests demonstrate exceptional performance on drag minimization problems for 3D bodies, achieving drag reductions of 34.5% in the transition regime and 61.1% in the slip-flow regime within roughly ten optimization iterations. For each candidate shape, converged solutions of the primal and adjoint Boltzmann equation are obtained with only a few dozen updates of the velocity distribution function, dramatically reducing computational cost compared with conventional methods.

</details>


### [31] [Fast-Converging and Asymptotic-Preserving DSMC](https://arxiv.org/abs/2511.19061)
*Bin Hu,Liyan Luo,Kaiyuan Wang,Lei Wu*

Main category: physics.comp-ph

TL;DR: 提出DIG方案显著提高DSMC方法在近连续流区域的效率，当Kn=0.01时比传统DSMC快两个数量级，且性能随Kn减小而提升


<details>
  <summary>Details</summary>
Motivation: 随着太空探索快速发展，提高直接模拟蒙特卡洛(DSMC)方法的效率变得日益紧迫

Method: 使用直接间歇通用合成迭代(DIG)方案，通过数学分析验证其快速收敛和渐近保持特性，并在泊肃叶流和圆柱绕流中进行验证

Result: 在近连续流区域，DIG方法当网格尺寸为O(1)时能渐近恢复Navier-Stokes方程，单次DIG演化循环可将与稳态解的偏差减少五倍以上

Conclusion: DIG方法在多尺度流动模拟中展现出高效性和准确性，在工程应用中具有巨大潜力

Abstract: Improving the efficiency of the direct simulation Monte Carlo (DSMC) method has become increasingly urgent with the rapid development of space exploration. To address this issue, the direct intermittent general synthetic iteration (DIG) scheme has recently been proposed to enable DSMC's rapid and accurate convergence to steady-state solutions, even when the cell size is much larger than the mean free path in near-continuum flow regimes. The first part of the paper is devoted to the mathematical analysis of DIG's fast-converging and asymptotic-preserving properties. Because the Boltzmann equation is analytically intractable, the analysis is conducted using the linearized BGK model. It is found that, in the near continuum flow regime, the DIG method asymptotically recovers the Navier Stokes equations when the cell size is O(1), rather than being constrained by the mean free path. Moreover, after a single cycle of DIG evolution, the deviation from the final steady state solution is reduced by more than a factor of five. In the second part of the paper, the Poiseuille flow and hypersonic flow passing over cylinder are investigated using the DIG scheme, with different time step and cell sizes, thereby demonstrating its efficiency and accuracy in multiscale flow simulation. Specifically, when the Knudsen number is 0.01, the DIG method is found to be faster than the traditional DSMC method by two orders of magnitude. The performance gain becomes even greater at smaller Knudsen numbers. The proposed method holds great potential for engineering applications.

</details>


### [32] [Electrochemical Interfaces at Constant Potential: Data-Efficient Transfer Learning for Machine-Learning-Based Molecular Dynamics](https://arxiv.org/abs/2511.19338)
*Michele Giovanni Bianchi,Michele Re Fiorentin,Francesca Risplendi,Candido Fabrizio Pirri,Michele Parrinello,Luigi Bonati,Giancarlo Cicero*

Main category: physics.comp-ph

TL;DR: TRECI是一种数据高效的工作流程，用于构建机器学习力场，在电子大正则分子动力学中实现从头算级别的精度，显著降低了恒定电位模拟的成本。


<details>
  <summary>Details</summary>
Motivation: 模拟带电金属/水界面在恒定电位下的电化学过程对于理解电化学至关重要，但使用从头算方法成本过高。

Method: 通过从通用和特定领域模型的迁移学习，TRECI使用减少的参考配置数量实现跨宽电位范围的稳定准确模拟，允许使用高级meta-GGA泛函和严格的表面带电方案。

Result: 应用于Cu(111)/水界面，仅用一千个配置训练的模型就能产生准确的分子动力学模拟，捕捉到以前未报告的偏压依赖性溶剂重构效应。

Conclusion: TRECI为表征多种材料和界面化学提供通用策略，显著降低现实恒定电位模拟成本，扩展定量电化学建模的访问范围。

Abstract: Simulating electrified metal/water interfaces with explicit solvent under constant potential is essential for understanding electrochemical processes, yet remains prohibitively expensive with ab initio methods. We present TRECI, a data-efficient workflow for constructing machine learning force-fields (ML-FFs) that achieve ab initio-level accuracy in electronically grand-canonical molecular dynamics. By leveraging transfer learning from general-purpose and domain-specific models, TRECI enables stable and accurate simulations across a wide potential range using a reduced number of reference configurations. This efficiency allows the use of high-level meta-GGA functionals and rigorous surface-electrification schemes. Applied to Cu(111)/water, models trained on just one thousand configurations yield accurate molecular dynamics simulations, capturing bias-dependent solvent restructuring effects not previously reported. TRECI offers a general strategy for characterising diverse materials and interfacial chemistries, significantly lowering the cost of realistic constant-potential simulations and expanding access to quantitative electrochemical modelling.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [33] [Resource-Efficient Quantum Optimization via Higher-Order Encoding](https://arxiv.org/abs/2511.17545)
*Frederik Koch,Shahram Panahiyan,Rick Mukherjee,Joseph Doetsch,Dieter Jaksch*

Main category: quant-ph

TL;DR: HUBO方法比QUBO在量子组合优化中更资源高效，能指数级减少量子比特需求并降低CNOT门数量至少89.6%。


<details>
  <summary>Details</summary>
Motivation: 传统QUBO编码方法在组合优化问题中因惩罚项导致电路规模扩大，增加了量子比特和门的需求。

Method: 系统性地构建HUBO哈密顿量，并与QUBO在门分配、最大k可着色子图和整数规划问题上进行基准测试。

Result: 在所有测试实例中，HUBO指数级减少了量子比特需求，编译到单量子比特和双量子比特门后CNOT门数量减少了至少89.6%。

Conclusion: HUBO是当前和近期量子设备的实用替代方案，并发布了开源Python库以促进采用。

Abstract: Quantum approaches to combinatorial optimization problems (COPs) are often limited by the resource demands of Quadratic Unconstrained Binary Optimization (QUBO) encodings, which enlarge circuits through penalty terms and increase qubit and gate counts. We show that Higher-Order Unconstrained Binary Optimization (HUBO) enables a more resource-efficient formulation. Our method systematically constructs HUBO Hamiltonians and, compared to QUBO in benchmarks on Gate Assignment (GAP), Maximum k-Colorable Subgraph (MkCS), and Integer Programming (IP) problems, exponentially reduces qubit requirements and decreases CNOT gate counts by at least 89.6% after compilation to single- and two-qubit gates for all tested instances. These results highlight HUBO as a practical alternative for current and near-term devices. To promote adoption, we release an open-source Python library that automates HUBO model construction, broadening access to resource-efficient quantum optimization.

</details>


### [34] [On fast charged particle scattering by periodic atomic planes: quadratic potential corrections](https://arxiv.org/abs/2511.17667)
*Viktoriia Omelchenko*

Main category: quant-ph

TL;DR: 本文扩展了复杂结构靶材中快速带电粒子散射的方法，包含了二次势项，并推导了平行平面原子集散射的微分截面。


<details>
  <summary>Details</summary>
Motivation: 扩展复杂结构靶材中快速带电粒子散射的方法，考虑二次势项的影响。

Method: 基于扩展方法，推导了在平行平面原子集上散射的微分截面，采用eikonal近似。

Result: 证明了在这种情况下，微分散射截面在eikonal近似中分裂为相干和非相干截面，类似于Born近似。

Conclusion: 扩展方法成功应用于平行平面结构，验证了eikonal近似中相干和非相干截面的分离特性。

Abstract: In this paper, the approach for considering fast charged particles scattering on targets of complex structure, which contains some isolated substructures, was expanded to account quadratic potential terms. Based on this approach, the differential cross section for scattering on the set of parallel planes with uniformly distributed atoms in each plane was obtained. It was shown that for this case the differential scattering cross section splits into coherent and incohent cross sections in the eikonal approximation analogously with the Born approximation.

</details>


### [35] [Entanglement Witnesses of Condensation for Enhanced Quantum Sensing](https://arxiv.org/abs/2511.17749)
*Lilian I. Payne Torres,Irma Avdic,Anna O. Schouten,Olivia C. Wedig,Gregory S. Engel,David A. Mazziotti*

Main category: quant-ph

TL;DR: 该论文理论证明自旋量子比特的集体纠缠态可以显著增强基态与激发态之间的跃迁幅度，有望提升光学检测磁共振的信号对比度，为量子传感器设计提供新思路。


<details>
  <summary>Details</summary>
Motivation: 探索量子纠缠现象如何增强经典传感能力，特别是利用集体纠缠态来放大自旋态跃迁，以改进量子传感器的灵敏度。

Method: 通过理论分析和计算模拟，研究具有磁偶极相互作用的N个三重态自旋系综，分析粒子-空穴对凝聚最强的几何构型下的跃迁增强效应。

Result: 发现集体纠缠态可使跃迁幅度相对于微波场实现O(√N)的增强，这种效应具有噪声鲁棒性，源于纠缠集中到单一集体模式。

Conclusion: 提出了一种利用凝聚启发纠缠来提升自旋平台灵敏度的量子传感器设计原则，为量子传感技术发展提供了新方向。

Abstract: Quantum phenomena such as entanglement provide powerful resources for enhancing classical sensing. Here, we theoretically show that collective entanglement of spin qubits, arising from a condensation of particle-hole pairs, can strongly amplify transitions between ground and excited spin states, potentially improving signal contrast in optically detected magnetic resonance. This collective state exhibits an $\mathcal{O}(\sqrt{N})$ enhancement of the transition amplitude with respect to an applied microwave field, where $N$ is the number of entangled spin qubits. We computationally realize this amplification using an ensemble of $N$ triplet spins with magnetic dipole interactions, where the largest transition amplitudes occur at geometries for which the condensation of particle-hole pairs is strongest. This effect, robust to noise, originates from the concentration of entanglement into a single collective mode, reflected in a large eigenvalue of the particle-hole reduced density matrix -- an entanglement witness of condensation analogous to off-diagonal long-range order, though realized here in a finite system. These results offer a design principle for quantum sensors that exploit condensation-inspired entanglement to boost sensitivity in spin-based platforms.

</details>


### [36] [Certifying Majorana Fermions with Elegant-Like Bell Inequalities and a New Self-Testing Equivalence](https://arxiv.org/abs/2511.17764)
*Patryk Michalski,Arturo Konderak,Wojciech Bruzda,Remigiusz Augusiak*

Main category: quant-ph

TL;DR: 本文提出了一种通用的贝尔不等式构造方法，能够精确计算量子界限，并推广了CHSH和Gisin的优雅不等式。该方法可用于设备无关地认证Majorana费米子，并识别了自测试定义中需要包含的额外等价关系。


<details>
  <summary>Details</summary>
Motivation: 贝尔不等式是探测非局域相关性的基本工具，但其量子界限（即通过量子策略可获得的最大值）很少能解析计算。本文旨在构建一类贝尔不等式，使其量子界限能够精确计算。

Method: 引入一个通用的贝尔不等式构造框架，该框架推广了Clauser-Horne-Shimony-Holt和Gisin的优雅不等式，产生由任意数量的成对反交换Clifford可观测量和相应的最大纠缠态最大违反的贝尔表达式。

Result: 构建的贝尔不等式能够精确计算量子界限，并在适当假设下实现Majorana费米子的设备无关认证。识别了自测试定义中需要包含的额外等价关系，该等价关系源于对共享态和测量的部分转置操作。

Conclusion: 提出的框架为分析计算贝尔不等式的量子界限提供了通用方法，扩展了自测试理论，并揭示了部分转置操作在保持观测相关性不变方面的作用。

Abstract: Bell inequalities provide a fundamental tool for probing nonlocal correlations, yet their quantum bound, that is, the maximal value attainable through quantum strategies, is rarely accessible analytically. In this work, we introduce a general construction of Bell inequalities for which this bound can be computed exactly. Our framework generalizes both the Clauser-Horne-Shimony-Holt and Gisin's elegant inequalities, yielding Bell expressions maximally violated by any number of pairwise anticommuting Clifford observables together with the corresponding maximally entangled state. Under suitable assumptions, our inequalities also enable the device-independent certification of Majorana fermions, understood as multiqubit realizations of Clifford algebra generators. Importantly, we identify an additional equivalence that must be incorporated into the definition of self-testing beyond invariance under local isometries and transposition. This equivalence arises from partial transposition applied to the shared state and to the measurements, which in specific cases leaves all observed correlations unchanged.

</details>


### [37] [Asymptotic dynamics in the Heisenberg picture: attractor subspace and Choi-Effros product](https://arxiv.org/abs/2511.17770)
*Daniele Amato,Paolo Facchi,Arturo Konderak*

Main category: quant-ph

TL;DR: 该论文研究了开放量子系统在Heisenberg绘景中的渐近动力学，给出了吸引子子空间的显式表达式及其动力学，讨论了Schrödinger和Heisenberg绘景中吸引子子空间的关系，并扩展了结果到Schwarz映射类。


<details>
  <summary>Details</summary>
Motivation: 研究开放量子系统在Heisenberg绘景中的渐近动力学特性，特别是吸引子子空间的结构和动力学行为。

Method: 通过分析Heisenberg绘景中的动力学方程，推导吸引子子空间的显式表达式，研究不同绘景间吸引子子空间的关系及其代数结构。

Result: 获得了吸引子子空间的显式表达式，揭示了Schrödinger和Heisenberg绘景中吸引子子空间的对应关系，讨论了渐近展开定理和Choi-Effros退相干自由代数的精细结构。

Conclusion: 成功建立了开放量子系统在Heisenberg绘景中的渐近动力学理论框架，并将结果推广到更一般的Schwarz映射类。

Abstract: We study the asymptotic dynamics of open quantum systems in the Heisenberg picture. We find an explicit expression for the attractor subspace and the dynamics that takes place in it. We present the relationship between the attractor subspaces in the Schrödinger and Heisenberg pictures and, in particular, the connection between their algebraic structures. An unfolding theorem of the asymptotics, as well as the fine structure of the recently introduced Choi-Effros decoherence-free algebra, are also discussed. Finally, we show how to extend all the results to the class of Schwarz maps.

</details>


### [38] [Probing Antiferromagnetic Hysteresis on Programmable Quantum Annealers](https://arxiv.org/abs/2511.17779)
*Elijah Pelofske,Pratik Sathe,Cristiano Nisoli,Frank Barrows*

Main category: quant-ph

TL;DR: 使用可编程模拟量子退火处理器实现基于采样的磁滞协议，研究反铁磁体的磁记忆效应，发现量子涨落介导的磁畴产生磁记忆


<details>
  <summary>Details</summary>
Motivation: 探索反铁磁体中反直觉的磁记忆概念，验证量子涨落对磁滞行为的影响

Method: 使用可编程模拟量子退火处理器实现基于采样的磁滞协议，通过横向场实现状态跃迁，纵向控制场进行磁场扫描

Result: 观察到完整的饱和和磁滞曲线反转，以及由量子涨落介导的磁畴出现，这些磁畴产生了反铁磁体的磁记忆效应

Conclusion: 实验证实了反铁磁体中存在磁记忆效应，量子涨落在其中起到关键作用，为理解反铁磁体的量子特性提供了新视角

Abstract: Using programmable analog quantum annealing processors, we implement a sampling-based magnetic hysteresis protocol to probe the counterintuitive notion of magnetic memory of antiferromagnets. A key component of this protocol responsible for the hysteresis is a transverse field, which enables state transitions, while the magnetic field sweep is done via a longitudinal control field. We present evidence of full saturation and reversal of the hysteresis curve, as well as emergent magnetic domain mediated by quantum fluctuations that give rise to the magnetic memory effect in antiferromagnets.

</details>


### [39] [Quantum Algorithm for Estimating Gibbs Free Energy and Entropy via Energy Derivatives](https://arxiv.org/abs/2511.17821)
*Shangjie Guo,Corneliu Buda,Nathan Wiebe*

Main category: quant-ph

TL;DR: 本文提出了一种量子算法，通过能量导数估算振动熵，利用量子线性系统算法处理能隙倒数问题，在合理假设下比经典算法更快。


<details>
  <summary>Details</summary>
Motivation: 振动熵估算在热力学和统计力学中具有挑战性，因为它依赖于量子力学性质。量子算法有望提升热力学性质预测能力。

Method: 通过块编码能量二阶导数的精确表达式，使用量子线性系统算法处理表达式中出现的能隙倒数问题。

Result: 算法在ε近似熵时查询次数与条件数κ、温度T、误差容限ε和配分函数类似量Z相关，为Õ(Zκ²/εT)。在足够先验知识下，查询次数可二次优化。

Conclusion: 在合理温度假设下，量子计算机能比经典算法更快计算振动熵贡献，展示了量子算法在材料科学、分子生物学和化学工程等领域的应用潜力。

Abstract: Estimating vibrational entropy is a significant challenge in thermodynamics and statistical mechanics due to its reliance on quantum mechanical properties. This paper introduces a quantum algorithm designed to estimate vibrational entropy via energy derivatives. Our approach block encodes the exact expression for the second derivative of the energy and uses quantum linear systems algorithms to deal with the reciprocal powers of the gaps that appear in the expression. We further show that if prior knowledge about the values of the second derivative is used then our algorithm can $ε$-approximate the entropy using a number of queries that scales with the condition number $κ$, the temperature $T$, error tolerance $ε$ and an analogue of the partition function $\mathcal{Z}$, as $\widetilde{O}\left(\frac{\mathcal{Z}κ^2 }{εT}\right)$. We show that if sufficient prior knowledge is given about the second derivative then the query scales quadratically better than these results. This shows that, under reasonable assumptions of the temperature and a quantum computer can be used to compute the vibrational contributions to the entropy faster than analogous classical algorithms would be capable of. Our findings highlight the potential of quantum algorithms to enhance the prediction of thermodynamic properties, paving the way for advancements in fields such as material science, molecular biology, and chemical engineering.

</details>


### [40] [Kicked-Ising Quantum Battery](https://arxiv.org/abs/2511.17835)
*Sebastián V. Romero,Xi Chen,Yue Ban*

Main category: quant-ph

TL;DR: 本文提出了踢伊辛模型作为量子电池，在自对偶算符机制下分析了其充电动力学，展示了最大充电能力和抗无序鲁棒性，并通过强化协议实现了更快的能量注入。


<details>
  <summary>Details</summary>
Motivation: 量子电池有望通过利用纠缠算符超越经典电池性能，而自旋链在不同设置中展现出独特的充电特性。

Method: 使用Clifford量子细胞自动机和动量空间Floquet分析结合Cayley-Hamilton定理，获得能量注入的精确表达式，分析边界条件和自旋链宇称对充电性能的影响。

Result: 踢伊辛量子电池实现了最大充电，并表现出显著的抗无序鲁棒性。强化协议在固定时间窗口内实现了更快更高效的能量注入，非均匀踢调度增强了实验灵活性。

Conclusion: 踢伊辛量子电池提供了一个可扩展、抗无序的协议和测试平台来评估量子平台，理论框架得到了张量网络模拟和IBM量子硬件的验证。

Abstract: Quantum batteries (QBs) have emerged as promising candidates capable of outperforming classical counterparts by utilizing entangled operators. Spin chains, in particular, exhibit unique {charging} properties across diverse settings. Here, we introduce the kicked-Ising model as a QB and analytically characterize its charging dynamics within the self-dual operator regime, valid for arbitrary system sizes and Floquet cycles. Using Clifford quantum cellular automata and momentum-space Floquet analysis with the Cayley-Hamilton theorem, we obtain exact expressions for energy injection, uncovering the influence of boundary conditions and spin-chain parity on charging performance. The kicked-Ising QB achieves maximal charging while exhibiting remarkable robustness against disorder. We further propose an intensified protocol within a fixed time window that enables faster and more efficient energy injection, while non-uniform kick schedules enhance experimental flexibility. Spin correlators analysis further shows that low-frequency driving boosts energy injection, highlighting a clear connection between charging, scrambling, and kick-induced delocalization. Our theoretical framework are supported by tensor-network simulations and finally verified on IBM quantum hardware. Accounting for platform-specific constraints, we demonstrate that the kicked-Ising QB offers a scalable, disorder-resilient protocol and testbed to assess quantum platforms.

</details>


### [41] [Unified Bulk-Entanglement Correspondence in Non-Hermitian Systems](https://arxiv.org/abs/2511.17846)
*Xudong Zhang,Zhaoyu Sun,Bin Guo*

Main category: quant-ph

TL;DR: 该论文建立了非布洛赫极化与纠缠极化之间的普适对应关系，解决了非厄米趋肤效应导致的体边对应危机，为超越局域性限制的非厄米拓扑提供了唯一的实空间诊断工具。


<details>
  <summary>Details</summary>
Motivation: 非厄米趋肤效应使传统体边对应失效，虽然非布洛赫极化在广义布里渊区上恢复了动量空间拓扑，但缺乏直接、鲁棒的实空间体探测方法。

Method: 引入准互易哈密顿量消除非厄米趋肤效应但保持体拓扑，严格证明在热力学极限下非布洛赫极化与准互易哈密顿量的纠缠极化模1等价。

Result: 发现纠缠极化在传统拓扑不变量失效时仍保持鲁棒量子化，受Toeplitz算子的Fredholm指数保护，成功恢复了线隙、点隙和无能隙相中的体边对应。

Conclusion: 纠缠是唯一能够超越局域性崩溃捕获非布洛赫拓扑的实空间诊断工具，统一了非厄米物理中的几何和纠缠范式。

Abstract: The non-Hermitian skin effect (NHSE) fundamentally invalidates the conventional bulk-boundary correspondence (BBC), leading topological diagnostics into a crisis. While the non-Bloch polarization $P_β$ defined on the generalized Brillouin zone restores momentum-space topology, a direct, robust real-space bulk probe has remained elusive. We resolve this by establishing a universal correspondence between $P_β$ and the entanglement polarization $χ$ of the biorthogonal ground state. Introducing a quasi-reciprocal Hamiltonian $\tilde{H}$ that removes the NHSE while preserving bulk topology, we rigorously prove the fundamental identity $P_β \equiv χ(\tilde{H})\pmod 1$ in the thermodynamic limit under the quasi-locality assumption. Crucially, we demonstrate that this equivalence transcends the locality constraints that limit traditional topological invariants. While the conventional Resta polarization fails when $\tilde{H}$ becomes non-local due to the divergence of position variance, we reveal that $χ(\tilde{H})$ remains robustly quantized, protected by the Fredholm index of Toeplitz operators. Our work thus identifies entanglement as the unique real-space diagnostic capable of capturing non-Bloch topology beyond the breakdown of locality, successfully restoring the BBC across diverse non-Hermitian systems such as line-gap, point-gap, and gapless phases, thereby unifying the geometric and entanglement paradigms in non-Hermitian physics.

</details>


### [42] [Exact Non-Identity Check and Gate-Teleportation-Based Indistinguishability Obfuscation are NP-hard for Low-T-Depth Quantum Circuits](https://arxiv.org/abs/2511.17856)
*Joshua Nevin*

Main category: quant-ph

TL;DR: 该论文研究了Clifford+T量子电路中T深度与精确非恒等检查(ENIC)问题复杂性的关系，证明了对于T深度为O(log(n))的电路，ENIC问题是NP难的，从而排除了基于门隐形传态的计算不可区分混淆方案的高效实现可能性。


<details>
  <summary>Details</summary>
Motivation: Broadbent和Kazmi在2021年提出的基于门隐形传态的计算不可区分混淆协议在T门数量为对数级的Clifford+T电路中效率受限，主要瓶颈在于需要解决ENIC问题。由于Tanaka在2009年已证明ENIC在一般情况下是NQP完全的，本文旨在研究当从低T数量转向低T深度时ENIC问题的复杂性变化。

Method: 通过理论分析，研究Clifford+T电路中T深度与ENIC问题复杂性的关系，特别关注T深度为O(log(n))的情况，并证明在此条件下ENIC是NP难的。

Result: 证明了对于T深度为O(log(n))的Clifford+T量子电路，精确非恒等检查(ENIC)问题是NP难的。

Conclusion: 这一结果排除了对于对数T深度的Clifford+T电路，要么高效解决ENIC问题，要么实现基于门隐形传态的计算不可区分混淆方案的可能性，除非P=NP。

Abstract: In 2021, Broadbent and Kazmi developed a gate-teleportation-based protocol for computational indistinguishability obfuscation of quantum circuits. This protocol is efficient for Clifford+T circuits with logarithmically many T-gates, where the limiting factor in the efficiency of the protocol is the difficulty, on input a quantum circuit $C$, of the classical task of producing a description of the unitary obtained by conjugating a Pauli $P$ (corresponding to a Bell-measurement outcome) by $C$, where this description only depends on the input-output functionality of $CPC^{\dagger}$. The task above, in turn, is at least as hard as the problem of determining whether two $n$-qubit quantum circuits are perfectly equivalent up to global phase. In 2009, Tanaka defined the corresponding decision problem Exact Non-Identity Check (ENIC) and showed that ENIC is NQP-complete in general. Motivated by this, we consider in this work what happens when we pass from low T-count to low T-depth. In particular, we show that, for Clifford+T circuits of T-depth $O(\log(n))$, deciding ENIC is NP-hard. This effectively rules out the possibility, for Clifford+T circuits of logarithmic T-depth, of either efficient ENIC or efficient gate-teleportation based computational indistinguishability obfuscation, unless P=NP.

</details>


### [43] [Entanglement Generation via Hamiltonian Dynamics Having Limited Resources](https://arxiv.org/abs/2511.17896)
*Moein Naseri*

Main category: quant-ph

TL;DR: 本文研究了在有限物理资源（有界能量方差）约束下，双体哈密顿动力学中纠缠生成的基本极限。使用相对熵纠缠，推导了任意纯态和哈密顿量在施密特基下的瞬时纠缠生成率的闭合解析表达式。


<details>
  <summary>Details</summary>
Motivation: 探索在有限物理资源约束下纠缠生成的基本极限，特别是当只有有界能量方差可用时，研究纠缠生成率的理论上界。

Method: 使用相对熵纠缠度量，在施密特基下分析哈密顿量动力学。通过矩阵分析框架和物理约束下允许的哈密顿量的精化描述，推导优化公式。

Result: 发现仅基于哈密顿量平均能量的约束不足以限制纠缠生成率，而施加方差约束可确保有限且定义良好的最大值。完全表征了达到最优速率的哈密顿量，建立了它们在施密特基中的虚部与最优初始态结构之间的直接关系。

Conclusion: 对于无辅助系统，获得了最大速率的闭合形式表达式，并识别了最优态和哈密顿量族。当Alice和Bob可使用局域辅助系统时，推导了显式优化公式并表征了纠缠生成的增强效果。

Abstract: We investigate the fundamental limits of entanglement generation under bipartite Hamiltonian dynamics when only finite physical resources-specifically, bounded energy variance-are available. Using the relative entropy of entanglement, we derive a closed analytical expression for the instantaneous entanglement generation rate for arbitrary pure states and Hamiltonians expressed in the Schmidt basis. We find that constraints based solely on the mean energy of the Hamiltonian are insufficient to bound the entanglement generation rate, whereas imposing a variance constraint ensures a finite and well-defined maximum. We fully characterize the Hamiltonians that achieve this optimal rate, establishing a direct relation between their imaginary components in the Schmidt basis and the structure of the optimal initial states. For systems without ancillas, we obtain a closed-form expression for the maximal rate in terms of the surprisal variance of the Schmidt coefficients and identify the family of optimal states and Hamiltonians. We further extend our analysis to scenarios where Alice and Bob may employ local ancillary systems: using a matrix-analytic framework and a refined description of the Hamiltonians allowed by the physical constraints, we derive an explicit optimization formula and characterize the attainable enhancement in entanglement generation.

</details>


### [44] [Noise-Adaptive Quantum Circuit Mapping for Multi-Chip NISQ Systems via Deep Reinforcement Learning](https://arxiv.org/abs/2511.18079)
*Atiye Zeynali,Zahra Bakhshi*

Main category: quant-ph

TL;DR: DeepQMap是一个基于深度强化学习的量子电路编译框架，通过双向LSTM动态噪声适应网络和多头注意力机制，在分布式量子架构中实现动态噪声适应和跨芯片操作优化。


<details>
  <summary>Details</summary>
Motivation: 从单芯片到分布式多芯片量子架构的转变带来了时序噪声变化管理和跨芯片操作优化的挑战，传统静态优化方法无法适应硬件动态特性。

Method: 集成双向LSTM动态噪声适应网络、多头注意力机制和Rainbow DQN架构，通过学习量子系统行为的时序表示来持续适应硬件动态。

Result: 在270个基准电路上平均保真度达到0.920±0.023，比最先进QUBO方法提升49.3%；跨芯片通信开销减少79.8%；噪声预测准确度R²=0.912；训练收敛速度快8.2倍。

Conclusion: DeepQMap在保真度、通信开销和训练效率方面显著优于传统方法，为近期噪声中等规模量子计算应用提供了实用解决方案。

Abstract: The transition from monolithic to distributed multi-chip quantum architectures has fundamentally altered the circuit compilation landscape, introducing challenges in managing temporal noise variations and minimizing expensive inter-chip operations. We present DeepQMap, a deep reinforcement learning framework that integrates a bidirectional Long Short-Term Memory based Dynamic Noise Adaptation (DNA) network with multi-head attention mechanisms and Rainbow DQN architecture. Unlike conventional static optimization approaches such as QUBO formulations, our method continuously adapts to hardware dynamics through learned temporal representations of quantum system behavior. Comprehensive evaluation across 270 benchmark circuits spanning Quantum Fourier Transform, Grover's algorithm, and Variational Quantum Eigensolver demonstrates that DeepQMap achieves mean circuit fidelity of $0.920 \pm 0.023$, representing a statistically significant 49.3\% improvement over state-of-the-art QUBO methods ($0.618 \pm 0.031$, $t_{98} = 4.87$, $p = 0.0023$, Cohen's $d = 2.34$). Inter-chip communication overhead reduces by 79.8\%, decreasing from 2.34 operations per circuit to 0.47. The DNA network maintains noise prediction accuracy with coefficient of determination $R^2 = 0.912$ and mean absolute error of 0.87\%, enabling proactive compensation for hardware fluctuations. Scalability analysis confirms sustained performance across 20-100 qubit systems, with fidelity remaining above 0.87 even at maximum scale where competing methods degrade below 0.60. Training convergence occurs 8.2$\times$ faster than baseline approaches, completing in 45 minutes versus 370 minutes for QUBO optimization. Very large effect sizes validate practical significance for near-term noisy intermediate-scale quantum computing applications.

</details>


### [45] [Verifcation of general multi-qudit pure states](https://arxiv.org/abs/2511.17901)
*Xiao-Dong Zhang,Bin-Bin Cai,Song Lin*

Main category: quant-ph

TL;DR: 提出了一种通用的稳定子框架和测试方法，用于验证混合量子系统中的多量子比特态，仅需自适应局域测量即可高效验证多种量子态。


<details>
  <summary>Details</summary>
Motivation: 混合量子系统的子系统可能具有不同的局部维度，因此需要验证制备的量子态以确保系统可靠性。

Method: 开发了广义稳定子框架和关联测试，适用于一般的多量子比特态，包括复合维度和混合架构，仅使用自适应局域测量。

Result: 该方法能够验证qutrit-qubit态、任意两量子比特纯态、Bell/Bell-like态、GHZ/GHZ-like态、图态、超图态、多图态和多超图态，效率达到或超过已知最佳方案。

Conclusion: 所提出的广义稳定子框架为混合量子系统提供了一种高效且通用的量子态验证方法。

Abstract: Verifying prepared quantum states is crucial for hybrid systems whose subsystems may have different local dimensions. We present a generalized stabilizer framework and associated test that apply to general multi-qudit states, including composite-dimensional and hybrid architectures. Using only adaptive local measurements, our method verifies qutrit-qubit states, arbitrary two-qubit pure states, Bell/Bell-like, GHZ/GHZ-like, graph, hypergraph, multigraph, and multihypergraph states, with efficiencies matching or surpassing the best known schemes.

</details>


### [46] [Computational Quantum Anamorphic Encryption and Quantum Anamorphic Secret-Sharing](https://arxiv.org/abs/2511.17924)
*Sayantan Ganguly,Shion Samadder Chaudhury*

Main category: quant-ph

TL;DR: 提出了量子变形加密的定义和构造，包括基于公钥和对称密钥的量子变形加密方案，以及计算量子变形秘密共享方案，能够抵抗量子攻击。


<details>
  <summary>Details</summary>
Motivation: 扩展经典变形加密概念到量子领域，解决在量子计算环境下实现隐蔽通信的安全需求。

Method: 1) 定义基于公钥和对称密钥的量子变形加密；2) 提出通用框架构造量子变形对称密钥加密；3) 扩展计算量子秘密共享到变形版本，支持多消息、多密钥和单一共享函数。

Result: 成功构建了量子变形加密方案，能够将两个不同维度的量子密度矩阵嵌入到单个量子变形密文中，并实现了对量子攻击者的完美安全性。

Conclusion: 量子变形加密和秘密共享方案为量子环境下的隐蔽通信提供了可行的安全解决方案，具有重要的理论和实践价值。

Abstract: The concept of anamorphic encryption, first formally introduced by Persiano et al. in their influential 2022 paper titled ``Anamorphic Encryption: Private Communication Against a Dictator,'' enables embedding covert messages within ciphertexts. One of the key distinctions between a ciphertext embedding a covert message and an original ciphertext, compared to an anamorphic ciphertext, lies in the indistinguishability between the original ciphertext and the anamorphic ciphertext. This encryption procedure has been defined based on a public-key cryptosystem. Initially, we present a quantum analogue of the classical anamorphic encryption definition that is based on public-key encryption. Additionally, we introduce a definition of quantum anamorphic encryption that relies on symmetric key encryption. Furthermore, we provide a detailed generalized construction of quantum anamorphic symmetric key encryption within a general framework, which involves taking any two quantum density matrices of any different dimensions and constructing a single quantum density matrix, which is the quantum anamorphic ciphertext containing ciphertexts of both of them. Subsequently, we introduce a definition of computational anamorphic secret-sharing and extend the work of Çakan et al. on computational quantum secret-sharing to computational quantum anamorphic secret-sharing, specifically addressing scenarios with multiple messages, multiple keys, and a single share function. This proposed secret-sharing scheme demonstrates impeccable security measures against quantum adversaries.

</details>


### [47] [Brute-force positivization of $J_1-J_2$ model ground states](https://arxiv.org/abs/2511.17957)
*P. A. Bannykh,O. M. Sotnikov,V. V. Mazurenko*

Main category: quant-ph

TL;DR: 通过单量子比特变换的暴力方法评估一维J1-J2模型在强阻挫区域基态的正化协议，分析周期和开放边界条件以及自旋链宇称对符号结构的影响


<details>
  <summary>Details</summary>
Motivation: 探索量子波函数符号结构有助于理解复杂物质相，需要开发优化程序来模拟和操纵量子态的符号结构

Method: 使用基于单量子比特变换的暴力方法，评估J1-J2模型基态的正化协议

Result: 获得了正化结果，显示了周期和开放边界条件之间的差异，并建立了符号结构与模拟自旋链宇称的依赖关系

Conclusion: 边界条件和自旋链宇称对量子态符号结构有显著影响，这为理解强阻挫系统的复杂性质提供了重要见解

Abstract: Exploring sign structures of quantum wave functions attracts considerable attention due to the potential for advances in modeling complex phases of matter. This stimulates developing different optimization procedures for imitating and manipulating sign structures of quantum states. In this work, utilizing a brute force approach based on a set of single-qubit transformations we evaluate protocols enabling positivization of the one-dimensional $J_1 -J_2$ model ground states in the regime of strong frustration. Based on the obtained positivization results, we show the difference between the cases of periodic and open boundary conditions, and also establish the dependence of the sign structure on parity of the simulated spin chains.

</details>


### [48] [The Harrow-Hassidim-Lloyd algorithm with qutrits](https://arxiv.org/abs/2511.17960)
*Tushti Patel,V. S. Prasannaa*

Main category: quant-ph

TL;DR: 将HHL算法从量子比特扩展到量子三态系统（qutrit），开发了相应的电路和程序实现，并在量子化学中应用于氢分子势能曲线计算，发现qutrit HHL在相同精度下需要更少的量子态和相当数量的两量子态门。


<details>
  <summary>Details</summary>
Motivation: 将经典的HHL算法从量子比特框架扩展到量子三态系统，探索在更高维量子系统中实现量子线性系统算法的优势。

Method: 设计了qutrit HHL算法的电路，开发了实现程序，测试了简单矩阵的求解，并将其应用于氢分子势能曲线计算，比较了qubit和qutrit实现的资源需求。

Result: 在固定精度下，qutrit HHL电路比qubit HHL需要更少的量子态数量，同时两量子态门的数量相当。

Conclusion: qutrit HHL算法在资源效率方面具有优势，为在更高维量子系统中实现量子算法提供了有前景的方向。

Abstract: We extend the Harrow-Hassidim-Lloyd (HHL) algorithm, which is well-studied in the qubit framework, to its qutrit counterpart (which we call qutrit HHL, as opposed to qubit HHL, which is HHL using qubits). We design the circuit for the algorithm and develop a program for its implementation. We test HHL with qutrits for simple matrices and verify the results against the expected outcomes. We apply the algorithm to quantum chemistry, and in particular, to the potential energy curve calculations of the model problem of the hydrogen molecule in the split valence basis. We compare the number of qudits and the number of gates required between qubit and qutrit HHL implementations. In general, we find that for a fixed precision, the qutrit HHL circuit requires fewer number of qudits and comparable number of two-qudit gates than its qubit counterpart.

</details>


### [49] [Accelerated optimization of measured relative entropies](https://arxiv.org/abs/2511.17976)
*Zixin Huang,Mark M. Wilde*

Main category: quant-ph

TL;DR: 本文分析了测量相对熵和测量Rényi相对熵的优化目标函数，证明了它们的平滑性和强凸/凹性，并提出了基于Nesterov加速投影梯度下降/上升的高效计算算法。


<details>
  <summary>Details</summary>
Motivation: 测量相对熵和测量Rényi相对熵是量子态可区分性的重要度量，在量子假设检验中有重要应用。之前的方法基于半定优化，内存效率不高，需要更高效的算法。

Method: 通过分析目标函数的矩阵梯度和Hessian超算子，证明其平滑性和强凸/凹性，然后应用Nesterov加速投影梯度下降/上升算法进行计算。

Result: 证明了目标函数具有β-平滑和γ-强凸/凹性，开发了比之前半定优化方法更内存高效且对条件良好态更快的算法。

Conclusion: 该工作为测量相对熵的计算提供了理论基础和高效算法，在量子信息处理中具有重要应用价值。

Abstract: The measured relative entropy and measured Rényi relative entropy are quantifiers of the distinguishability of two quantum states $ρ$ and $σ$. They are defined as the maximum classical relative entropy or Rényi relative entropy realizable by performing a measurement on $ρ$ and $σ$, and they have interpretations in terms of asymptotic quantum hypothesis testing. Crucially, they can be rewritten in terms of variational formulas involving the optimization of a concave or convex objective function over the set of positive definite operators. In this paper, we establish foundational properties of these objective functions by analyzing their matrix gradients and Hessian superoperators; namely, we prove that these objective functions are $β$-smooth and $γ$-strongly convex / concave, where $β$ and $γ$ depend on the max-relative entropies of $ρ$ and $σ$. A practical consequence of these properties is that we can conduct Nesterov accelerated projected gradient descent / ascent, a well known classical optimization technique, to calculate the measured relative entropy and measured Rényi relative entropy to arbitrary precision. These algorithms are generally more memory efficient than our previous algorithms based on semi-definite optimization [Huang and Wilde, arXiv:2406.19060], and for well conditioned states $ρ$ and $σ$, these algorithms are notably faster.

</details>


### [50] [Elucidating Many-Body Effects in Molecular Core Spectra through Real-Time Approaches: Efficient Classical Approximations and a Quantum Perspective](https://arxiv.org/abs/2511.17985)
*Vibin Abraham,Priyabrata Senapati,Himadri Pathak,Bo Peng*

Main category: quant-ph

TL;DR: 开发了基于截断BCH展开的近似TD-dCC方法层次结构，用于高效计算分子核心能级谱中的多体卫星特征，同时构建了量子信号处理算法作为补充的量子计算方法。


<details>
  <summary>Details</summary>
Motivation: 准确解析分子核心能级谱中的多体卫星特征需要能高效且系统地捕捉电子关联的理论方法。虽然TD-dCC方法通过结合N和(N-1)电子扇区的关联效应实现了这一目标，但其精确公式计算成本高昂。

Method: 提出了基于截断Baker-Campbell-Hausdorff展开的近似TD-dCC方法层次结构，保留了单相似变换结构并维持卫星形成所需的关键关联图。开发了详细的组分分析来隔离空穴介导的激发路径。

Result: 在单杂质安德森模型和分子系统（H2O和CH4）中的应用表明，近似TD-dCC方法能够紧密且高效地再现精确的多体谱特征和准粒子权重。

Conclusion: 这些发展建立了互补的经典和量子方法学，用于定量、多体精确的核心光谱学模拟。

Abstract: Accurately resolving many-body satellite features in molecular core-level spectra requires theoretical approaches that capture electron correlation both efficiently and systematically. The recently developed time-dependent double coupled-cluster (TD-dCC) ansatz achieves this by combining correlation effects from the N- and (N-1)-electron sectors, but its exact formulation remains computationally demanding. Here we introduce a hierarchy of cost-effective approximate TD-dCC ansatzes derived from truncated Baker-Campbell-Hausdorff (BCH) expansions, which preserve a single-similarity-transformation structure while retaining the essential correlation diagrams responsible for satellite formation. We further develop a detailed component analysis that isolates hole-mediated excitation pathways, which are correlated processes arising from the coupling between ground-state and ionized-state amplitudes. We use it to interpret quasiparticle and satellite features across the hierarchy. Applications to the single-impurity Anderson model and molecular systems (H2O and CH4) demonstrate that the approximate TD-dCC methods closely and efficiently reproduce exact many-body spectral features and quasiparticle weights. In parallel, we construct a fault-tolerant quantum signal processing algorithm for the core-hole Green's function, providing a scalable quantum route for simulating correlated core-level dynamics. Together, these developments establish complementary classical and quantum methodologies for quantitative, many-body-accurate core spectroscopy.

</details>


### [51] [Attractor Subspace and Decoherence-Free Algebra of Quantum Dynamics](https://arxiv.org/abs/2511.18021)
*Daniele Amato,Paolo Facchi,Arturo Konderak*

Main category: quant-ph

TL;DR: 这篇综述讨论了有限维开放量子系统在Heisenberg绘景中的渐近动力学，涵盖了谱方法和代数方法及其关系，分析了离散时间和连续时间Markovian情形，并探讨了无限维情况下的问题。


<details>
  <summary>Details</summary>
Motivation: 研究开放量子系统的渐近动力学，特别是在Heisenberg绘景中，旨在理解量子系统在环境作用下的长期行为，以及谱方法和代数方法之间的联系。

Method: 采用谱分析和代数分析相结合的方法，在离散时间和连续时间Markovian框架下进行系统分析，并扩展到无限维情形。

Result: 提供了Markovian演化的一个例子，其退相干自由代数是一个III型von Neumann代数，展示了无限维情况下的特殊性质。

Conclusion: 该综述系统梳理了开放量子系统渐近动力学的理论框架，揭示了有限维和无限维情况下的重要差异，为后续研究提供了理论基础。

Abstract: In this review we discuss some results on the asymptotic dynamics of finite-dimensional open quantum systems in the Heisenberg picture. Both the spectral and algebraic approaches to this topic are addressed, with particular emphasis on their relationship. The analysis is conducted in both the discrete-time and the continuous-time Markovian settings. In the final part of the work, some issues emerging in the infinite-dimensional case are also discussed. In particular, we provide an example of a Markovian evolution whose decoherence-free algebra is a type III von Neumann algebra.

</details>


### [52] [Fewest switches surface hopping with decoherence in the Marcus inverted regime: correct rates but wrong thermal populations](https://arxiv.org/abs/2511.18062)
*Manas Nagda,Priyam Kumar De,Amber Jain*

Main category: quant-ph

TL;DR: 本文发现增强型最少切换表面跳跃方法在深度反转Marcus区域能获得合理的速率常数但给出错误的热平衡分布，揭示了该方法正确速率常数来源于时间导数耦合与放热性的共振，而错误热分布源于自洽性问题。


<details>
  <summary>Details</summary>
Motivation: 研究FSSH方法在深度反转Marcus区域的表现，特别是AFSSH版本在热平衡分布方面的异常行为，以理解其内在机制。

Method: 通过分析推导理解AFSSH行为，展示其正确速率常数来源于时间导数耦合与放热性的共振，而错误热分布源于自洽性问题。

Result: AFSSH在深度反转Marcus区域能获得合理速率常数但给出错误热平衡分布，推导提供了该区域AFSSH模拟的量子校正因子解析表达式。

Conclusion: AFSSH在深度反转Marcus区域存在自洽性问题导致热平衡分布错误，但通过共振机制仍能获得合理速率常数，为该方法在该区域的模拟提供了理论指导。

Abstract: Fewest switches surface hopping (FSSH) is a well benchmarked dynamical method for simulating nonadiabatic systems. In particular, the literature shows that for the spin-Boson model Hamiltonian, FSSH with appropriate corrections usually captures the detailed balance well and obtains rate constants within a factor of 2 compared to numerically exact results. In this study, we show that in the deep inverted Marcus regime, the augmented-FSSH (AFSSH, one version that includes decoherence) yields reasonably accurate rate constants but incorrect thermal populations over a broad range of parameters. We present an analytical derivation to understand the AFSSH behavior, and therefore, show that AFSSH obtains correct rate constants owing to the resonance of the time derivative coupling with the exothermicity, but obtains an incorrect thermal population owing to the self-consistency issue. The presented derivation provides an analytical expression for the quantum correction factor for AFSSH simulations in the Marcus inverted regime.

</details>


### [53] [Coherence of quantum non-Gaussian states via nonlinear absorption of quanta](https://arxiv.org/abs/2511.18149)
*Kingshuk Adhikary,Darren W. Moore,Radim Filip*

Main category: quant-ph

TL;DR: 通过在线性吸收基础上添加非线性相位不敏感吸收过程，可以在无外部驱动的情况下产生具有量子相干性的非高斯态，突破了仅使用线性吸收只能产生接近Fock态但缺乏相干性的限制。


<details>
  <summary>Details</summary>
Motivation: 传统的线性相位不敏感吸收虽然能确定性地产生量子非高斯态，但产生的态只接近Fock态，缺乏量子相干性。需要克服这一限制，在无外部驱动的情况下产生具有相干性的量子态。

Method: 在线性吸收过程的基础上添加非线性相位不敏感吸收过程。这两种被动过程的相干叠加使得相干性在相空间中自发出现并增强，且不需要外部驱动。

Result: 线性与非线性吸收过程的非互易性使得量子非高斯态产生相干性。旋转对称的Wigner函数从Fock态的多个负值区域转变为极其复杂的不对称结构，与单独相互作用产生的旋转对称性形成鲜明对比。

Conclusion: 通过结合线性和非线性相位不敏感吸收过程，可以在无外部驱动的情况下产生具有量子相干性的非高斯态，这种方法适用于广泛的实验类别。

Abstract: The linear and phase insensitive absorption of a single quanta via coherent interactions with a saturable system, even a single ground state qubit, is sufficient to deterministically generate quantum non-Gaussian states in an oscillator, even stimulated merely by increasing thermal oscillator energy. However, the resultant states only approach Fock states and therefore do not exhibit quantum coherence. Here we overcome this limitation using a minimal step: a nonlinear phase-insensitive absorption process added to the linear one. The coherent addition of such individually passive processes allows coherence to emerge and increase in phase space without an external drive and with minimal interaction requirements. The coherence of quantum non-Gaussian states emerges because the linear and nonlinear absorption processes are not mutually passive. In the simplest case rotationally symmetric Wigner functions of the oscillator Fock states convert their many negative regions to an extremely complex asymmetric structure in sharp contrast to the rotational symmetry of those obtained by the individual interactions. We extend this case to include an unsaturable absorber (oscillator) and analyse switching between linear and nonlinear absorptions, suitable for broad classes of experiments.

</details>


### [54] [Exact solutions of the inhomogeneous nonlinear Schrödinger equation through supersymmetric potentials](https://arxiv.org/abs/2511.18186)
*David J. Fernández C.,O. Pavón-Torres*

Main category: quant-ph

TL;DR: 提出一种基于超对称量子力学的通用算法，用于构造超对称伴势并推导非均匀非线性薛定谔方程(INLSE)的精确定态解。


<details>
  <summary>Details</summary>
Motivation: 建立INLSE与非线性薛定谔方程(NLSE)之间的联系，利用超对称量子力学方法构造精确解。

Method: 基于李点对称性分析建立INLSE与NLSE的联系，运用超对称量子力学算法构造超对称伴势，以Pösch-Teller势为例进行验证。

Result: 成功构造了具有单束缚态的Pösch-Teller势的INLSE精确解。

Conclusion: 所提出的算法能够有效构造INLSE的精确定态解，为研究非均匀非线性系统提供了新方法。

Abstract: By employing supersymmetric quantum mechanics, we present a general algorithm to construct supersymmetric partner potentials and hence derive exact stationary solutions of the inhomogeneous nonlinear Schrödinger equation (INLSE). This is possible due to the connection between the INLSE and the nonlinear Schrödinger equation (NLSE), which can be established from a treatment based on Lie point symmetries and is related with Schrödinger equation, under certain conditions. As an illustrative example, we construct exact solutions for the INLSE through a Pösch-Teller potential with a single bound state.

</details>


### [55] [Space-Optimized and Experimental Implementations of Regev's Quantum Factoring Algorithm](https://arxiv.org/abs/2511.18198)
*Wentao Yang,Bao Yan,Muxi Zheng,Quanfeng Lu,Shijie Wei,Gui-Lu Long*

Main category: quant-ph

TL;DR: 提出了一种通过中间反计算实现量子比特重用的方法，显著降低了Regev量子因式分解算法的空间复杂度，从O(n^{3/2})降低到O(n^{5/4})，甚至达到理论下限O(n log n)。


<details>
  <summary>Details</summary>
Motivation: Regev的高维变体虽然通过基于格的后处理减少了电路规模，但引入了显著的空间开销且缺乏实际实现。需要解决空间复杂度过高的问题来推进量子因式分解的实际可行性。

Method: 采用可逆计算启发的中间反计算方法实现量子比特重用，基本策略将空间复杂度降至O(n^{5/4})，优化策略达到理论下限O(n log n)。

Result: 仿真展示了时间-空间权衡和资源扩展，构建并编译了N=35的量子电路，通过噪声仿真验证了有效性。在超导量子计算机上执行了简化实验电路，基于格的后处理成功恢复了因子。

Conclusion: 这些结果推进了Regev风格量子因式分解的实际可行性，为未来的理论和实验发展提供了指导。

Abstract: The integer factorization problem (IFP) underpins the security of RSA, yet becomes efficiently solvable on a quantum computer through Shor's algorithm. Regev's recent high-dimensional variant reduces the circuit size through lattice-based post-processing, but introduces substantial space overhead and lacks practical implementations. Here, we propose a qubit reuse method by intermediate-uncomputation that significantly reduces the space complexity of Regev's algorithm, inspired by reversible computing. Our basic strategy lowers the cost from \( O(n^{3/2}) \) to \( O(n^{5/4}) \), and refined strategies achieve \( O(n \log n) \)which is a space lower bound within this model. Simulations demonstrate the resulting time-space trade-offs and resource scaling. Moreover, we construct and compile quantum circuits that factor \( N = 35 \), verifying the effectiveness of our method through noisy simulations. A more simplified experimental circuit for Regev's algorithm is executed on a superconducting quantum computer, with lattice-based post-processing successfully retrieving the factors. These results advance the practical feasibility of Regev-style quantum factoring and provide guidance for future theoretical and experimental developments.

</details>


### [56] [Deterministic coupling of ultracold atomic lattice to a suspended photonic waveguide](https://arxiv.org/abs/2511.18211)
*J. T. Hansen,F. Gargiulo,J. B. Mathiassen,J. H. Müller,E. S. Polzik,J. -B. Béguin*

Main category: quant-ph

TL;DR: 该研究展示了将超冷原子晶格与片上光子电路确定性耦合的平台，为中性原子量子计算机和模拟器提供了可扩展解决方案。


<details>
  <summary>Details</summary>
Motivation: 在单粒子水平和亚波长尺度上确定性控制光-物质相互作用是量子光学和混合集成量子技术的核心挑战，但将冷原子研究与纳米光子器件结合仍存在实验困难。

Method: 通过将超冷原子晶格与悬浮片上光子电路中的传播光进行确定性耦合，实现可控的光-物质相互作用平台。

Result: 该平台能够实现快速光学读取、高效亚波长非衍射相互作用区域，并与集成固态光子源、探测器和阻带调制器兼容。

Conclusion: 该技术不仅为可控量子物质研究开辟了新途径，还实现了光倏逝场和纳米结构的原位成像，有望用于量子传感应用中的非侵入性单原子探针三维扫描显微镜。

Abstract: The deterministic control of light-matter interactions at the level of single particles and on subwavelength scales is central to quantum optics and hybrid integrated quantum technologies. However, combining cold atom research with nanophotonic devices in a fully controllable platform remains a major experimental challenge. Here, we demonstrate the deterministic coupling of an ultracold atomic lattice to light propagating in suspended on-chip photonic circuits. These capabilities open avenues to address scalability challenges in neutral-atom quantum computers and simulators, enabling fast optical readout, efficient and subwavelength non-diffracting interaction zones, and genuine compatibility with integrated solid-state photon sources, detectors, and stop-band modulators. Beyond controllable quantum matter, the platform also enables in-situ imaging of evanescent fields of light and nanoscale structures, including prospects for three-dimensional scanning microscopy with non-invasive single-atom probes for quantum sensing applications.

</details>


### [57] [Doublon bound states in the continuum through giant atoms](https://arxiv.org/abs/2511.18212)
*Walter Rieck,Anton Frisk Kockum,Guangze Chen*

Main category: quant-ph

TL;DR: 本文研究了连续谱中的束缚态（BICs）在量子多体系统中的表现，特别是通过巨原子系统实现了稳定的双光子BICs，揭示了基于干涉机制在多体开放量子系统中稳定局域化的新途径。


<details>
  <summary>Details</summary>
Motivation: 连续谱中的束缚态在单粒子和线性系统中已被广泛研究，但在多体量子系统中的表现仍未被充分探索。本文旨在研究巨原子系统中如何通过非局域耦合和干涉效应产生稳定的双光子BICs。

Method: 首先分析驱动的双光子发射过程，展示双光子BICs如何产生并介导远距离原子间的无退相干相互作用；然后证明这些多体BICs在自然、非驱动动力学中通过三能级巨原子的虚拟双光子发射过程同样出现。

Result: 成功实现了稳定的双光子BICs，这些态能够介导远距离原子间的无退相干相互作用，并在自然动力学中持续存在。

Conclusion: 研究揭示了一种基于干涉机制在多体开放量子系统中稳定局域化的新方法，在量子模拟、非遍历动力学和受保护量子信息处理方面具有潜在应用价值。

Abstract: Bound states in the continuum (BICs) are spatially localized modes embedded in the spectrum of extended states, typically stabilized by symmetry or interference. While extensively studied in single-particle and linear systems, the many-body regime of BICs remains largely unexplored. Here, we demonstrate that giant atoms, quantum emitters coupled nonlocally to structured waveguides, can host robust doublon BICs, i.e., two-photon bound states stabilized by destructive interference and interactions. We first analyze a driven two-photon emission process and show how doublon BICs arise and mediate decoherence-free interaction between distant atoms. We then demonstrate that these many-body BICs also emerge under natural, undriven dynamics via a virtual two-photon emission process in three-level giant atoms. Our results reveal an interference-based mechanism for stabilizing many-body localization in open quantum systems, with potential applications in quantum simulation, non-ergodic dynamics, and protected quantum information processing.

</details>


### [58] [General Machine Learning Algorithm for Quantum Teleportation](https://arxiv.org/abs/2511.18318)
*Allison Brattley,Tomas Opatrny,Kunal K. Das*

Main category: quant-ph

TL;DR: 提出一种基于机器学习的通用算法，可为任何具有明确定义纠缠基测量的系统创建最优幺正算子来实现量子隐形传态。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够灵活适应不同量子系统、不同量子态类型和不同维度情况的通用量子隐形传态优化方法。

Method: 使用机器学习算法来生成最优幺正算子，该算法可以处理集体自旋模型，支持单比特和多比特态、相干态和Dicke态的隐形传态，并能处理先验分布和不等维度系统。

Result: 在所有测试案例中都显示出相对于无纠缠经典方案的显著量子优势，算法能够在目标保真度和计算成本之间灵活权衡。

Conclusion: 该机器学习算法为量子隐形传态提供了一种通用且灵活的优化方法，能够在各种量子系统中实现优于经典方案的性能。

Abstract: We present a general algorithm, based on machine learning, which can create optimal unitary operators to implement quantum teleportation in any system with well-defined set of measurements in a relevant entangled basis. We illustrate it with a collective spin model and demonstrate its versatility by applying it to teloportation of single and multiple qubit states, coherent and Dicke states, and for systems with prior distributions and unequal dimensions. All cases display significant regimes of quantum advantage over corresponding classical schemes with no entanglement. The algorithm offers the flexibility to choose a balance between target fidelity and computational cost.

</details>


### [59] [Universal learning of nonlocal entropy via local correlations in non-equilibrium quantum states](https://arxiv.org/abs/2511.18327)
*Hao Liao,Xuanqin Huang,Ping Wang*

Main category: quant-ph

TL;DR: 使用多层感知机建立量子互信息与仅二阶局部关联之间的通用映射，为实验测量非平衡态量子互信息提供实用方法


<details>
  <summary>Details</summary>
Motivation: 量子互信息是量化纠缠的重要非局域度量，但在实验测量中极为困难，特别是对于比基态更复杂的非平衡态

Method: 在一维无序XXZ模型中，使用多层感知机建立量子互信息与仅二阶局部关联之间的通用映射

Result: 该方法为实验提取量子互信息提供了实用方法，适用于超导量子比特等平台

Conclusion: 这项工作为重建其他非局域可观测量（包括Fisher信息和时序无序关联函数）建立了一个通用框架

Abstract: Characterizing the nonlocal nature of quantum states is a central challenge in the practical application of large-scale quantum computation and simulation. Quantum mutual information (QMI), a fundamental nonlocal measure, plays a key role in quantifying entanglement and has become increasingly important in studying nonequilibrium quantum many-body phenomena, such as many-body localization and thermalization. However, experimental measurement of QMI remains extremely difficult, particularly for nonequilibrium states, which are more complex than ground states. In this Letter, we employ a multilayer perceptron (MLP) to establish a universal mapping between the QMI and local correlations only up to second order for nonequilibrium states generated by quenches in a one-dimensional disordered XXZ model. Our approach provides a practical method for experimentally extracting QMI, readily applicable in platforms such as superconducting qubits. Moreover, this work will establishes a general framework for reconstructing other nonlocal observables, including Fisher information and out-of-time-ordered correlators.

</details>


### [60] [Nonlinear stochastic and quantum motion from Coulomb forces](https://arxiv.org/abs/2511.18345)
*Luca Ornigotti,Darren W. Moore,Radim Filip*

Main category: quant-ph

TL;DR: 通过消除库仑力的谐波部分，保留非线性部分，可观测到非互易非线性效应：一个粒子的位置噪声或量子不确定性会提高另一个粒子相干位移的信噪比。


<details>
  <summary>Details</summary>
Motivation: 可控非线性量子相互作用是现代量子技术的重要目标，但通常难以实现。然而，通过基本粒子间的自然力（如库仑力）可能实现可控非线性。

Method: 通过辅助线性力消除库仑力的谐波部分，保留非线性部分，研究两个带电粒子间的非线性相互作用。

Result: 观察到非互易非线性效应：一个粒子的位置噪声或量子不确定性会提高另一个粒子相干位移的信噪比，该效应在广泛的阱频率和质量尺度范围内都存在，且在随机和量子体系中均可见。

Conclusion: 基本粒子间的自然力（如库仑力）可以产生可控的非线性量子相互作用，为量子技术提供了一种潜在的新途径。

Abstract: Controllable nonlinear quantum interactions are a much sought after target for modern quantum technologies. They are typically difficult and costly to engineer for bespoke purposes. However controllable nonlinearities may have always been in reach via the natural and fundamental forces between quantum particles. The Coulomb interaction between charged particles is the simplest example. We show that after eliminating the harmonic part of the Coulomb force by an auxiliary linear force, the remaining reciprocal nonlinear part results in a directly observable non-reciprocal nonlinear effect: increase of the signal-to-noise ratio (SNR) of the coherent displacement of one particle, driven by the position noise, or uncertainty in quantum regime, in another particle. This essential evidence of nonlinear forces is present across large ranges of trap frequency and mass scales, as well as visible in both stochastic and quantum regimes.

</details>


### [61] [An Introduction to the Quantum Approximate Optimization Algorithm](https://arxiv.org/abs/2511.18377)
*Alessandro Giovagnoli*

Main category: quant-ph

TL;DR: 本教程全面介绍了量子近似优化算法(QAOA)，重点讲解其在QUBO和PUBO问题中的应用，包括算法原理、实现细节、能量景观分析以及高阶哈密顿量的扩展。


<details>
  <summary>Details</summary>
Motivation: QAOA是一种有前景的变分量子算法，用于解决经典难以处理的组合优化问题，本教程旨在提供从基本原理出发的全面介绍。

Method: 教程从变分量子电路和QUBO问题开始，详细探讨QAOA的哈密顿量公式、门分解和实现示例，分析算法的能量景观对称性和周期性，并扩展到高阶PUBO问题。

Result: 提供了算法实现的性能结果，证明了能量景观的对称性和周期性，提出了参数空间缩减方法，并将结果推广到高阶哈密顿量。

Conclusion: 本教程为理解和应用QAOA算法提供了系统框架，涵盖了从基础概念到高阶扩展的完整知识体系。

Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is a promising variational quantum algorithm introduced to tackle classically intractable combinatorial optimization problems. This tutorial offers a comprehensive, first-principles introduction to QAOA and its properties, focusing on its application to Quadratic and Polynomial Unconstrained Binary Optimization (QUBO and PUBO) problems. The tutorial begins by outlining variational quantum circuits and QUBO problems, focusing on their key properties and the encoding of problem constraints through quadratic penalty terms. Next, it explores the QAOA in detail, covering its Hamiltonian formulation, gate decomposition, and example applications, along with their implementation and performance results. This is followed by an analysis of the algorithm's energy landscape, where proofs are provided for its symmetry and periodicity, and where a resulting parameter space reduction is proposed. Finally, the tutorial extends these concepts to PUBO problems by generalizing the results to higher-order Hamiltonians and discussing the associated symmetries and circuit construction.

</details>


### [62] [Cavity magnomechanical framework for a high-efficiency quantum battery](https://arxiv.org/abs/2511.18440)
*S. K. Singh,Ahmed A. Zahia,Jia-Xin Peng,M. Y. Abd-Rabboud*

Main category: quant-ph

TL;DR: 研究了一种基于腔-磁-机械系统的量子电池架构，其中两个相同的二能级原子通过相干能量交换进行充电，系统包括微波腔、YIG球中的磁子模式和声子模式。


<details>
  <summary>Details</summary>
Motivation: 探索在混合磁子平台中设计高效量子电池的可行性，研究光-物质强相互作用对充电性能的影响。

Method: 推导旋转波近似下的系统哈密顿量，采用Lindblad主方程严格建模耗散，分析电池的完整动力学演化。

Result: 发现强共振光-物质相互作用对提升充电效率、存储能量和ergotropy（可提取功）至关重要；揭示了失谐和退相干的有害影响，以及系统耦合强度之间的非平凡相互作用。

Conclusion: 研究结果为在混合磁子平台中设计高效量子电池提供了定量框架，为未来实验实现提供了设计路线图。

Abstract: We theoretically investigate a quantum battery architecture where two identical two-level atoms are charged by a cavity-magnomechanical system, which includes a microwave cavity, a magnon mode hosted in a YIG sphere, and phonon mode due to the deformation of the YIG sphere. The charging process relies on coherent energy exchange, where the atoms couple to the cavity, which in turn, it interacts with the magnon mode via a beam-splitter mechanism. By deriving the system Hamiltonian under the rotating-wave approximation and employing a Lindblad master equation to rigorously model dissipation, we analyze the complete dynamical evolution of the battery. Our study demonstrates that strong, resonant light-matter interactions are crucial for enhancing the key performance metrics: charging efficiency, stored energy, and ergotropy (extractable work). We systematically investigate the deleterious effects of detuning and decoherence, and critically, we uncover a non-trivial interplay between the system's coupling strengths. This reveals optimal operating regimes where constructive interference maximizes performance, while excessive coupling in specific channels can degrade it. Ultimately, our findings provide a quantitative framework for engineering high-efficiency quantum batteries in hybrid magnonic platforms, offering a design roadmap for future experimental realizations.

</details>


### [63] [Non-Hermitian topology in a single driven-dissipative Kerr-Cat qubit](https://arxiv.org/abs/2511.18482)
*Pei-Rong Han,Huiye Qiu,Hao-Long Zhang,Wen Ning,Zhen-Biao Yang,Shi-Biao Zheng*

Main category: quant-ph

TL;DR: 该论文研究了在连续变量量子系统中实现非厄米物理现象，特别关注了驱动耗散Kerr猫态量子比特中的高阶Liouvillian异常点。


<details>
  <summary>Details</summary>
Motivation: 现有非厄米物理研究主要集中在离散变量量子系统，而连续变量编码系统中的非厄米量子效应尚未充分探索。

Method: 研究驱动耗散Kerr猫态量子比特（通过Kerr非线性谐振器实现）中的异常结构，分析耗散导致的量子态双向跃迁现象。

Result: 发现耗散导致猫态量子比特两个基态之间的双向跃迁，与普通两能级系统的单向跃迁形成鲜明对比。这种跃迁与单光子驱动的竞争产生了三阶Liouvillian异常点（LEP3s）。

Conclusion: 该工作为在连续变量量子系统中实现非厄米现象开辟了可能性，并展示了LEP3可以展现哈密顿量EP3s的拓扑特性，这是单量子比特无法实现的。

Abstract: The intriguing physical phenomena associated with exceptional points have established non-Hermitian physics as a frontier of modern research. Recent investigations have extended non-Hermitian physics into the fully quantum domain. However, existing studies predominantly concentrate on discrete-variable quantum systems, while non-Hermitian quantum effects in continuous-variable encoded systems remain largely unexplored. In this work, we investigate the exceptional structure for a driven-dissipative Kerr-cat qubit, realized with a Kerr nonlinear resonator. We find that the dissipation leads to a bidirectional jump between the two basis states of the cat qubit, which is in distinct contrast with the unidirectional jump associated with normal two-level systems. The competition between this jump and a single-photon drive gives arise to the emergence of third-order Liouvillian exceptional points (LEP3s), each corresponds to a crossing point of two lines of LEP2s. We further show that the LEP3 can exhibit the topological character of the Hamiltonian EP3s, which cannot be realized with a single qubit. Our work opens the possibility of realizing non-Hermitian phenomena with continuous-variable quantum systems.

</details>


### [64] [A demonstration that classical gravity does not produce entanglement](https://arxiv.org/abs/2511.19242)
*Mike D. Schneider,Nick Huggett,Niels Linnemann*

Main category: quant-ph

TL;DR: 该论文讨论了关于通过引力诱导纠缠实验来探测引力量子性质的争议，指出经典引力无法介导纠缠，如果实验中观察到纠缠，则必然有其他力源提供了所需的虚拟力。


<details>
  <summary>Details</summary>
Motivation: 澄清关于引力诱导纠缠实验解释的争议，特别是经典引力能否介导纠缠的问题，为理解引力的量子性质提供理论分析。

Method: 使用牛顿-嘉当分析来研究经典引力在纠缠实验中的作用，分析哈密顿形式主义的应用。

Result: 分析表明，如果引力是经典的，那么在引力诱导纠缠实验中观察到的纠缠效应必须由其他力源提供所需的虚拟力，而非引力本身。

Conclusion: 经典引力无法介导纠缠，如果实验中确实观察到纠缠，则证明存在其他量子力源在起作用，这为验证引力的量子性质提供了重要线索。

Abstract: Once again, dispute has arisen over the interpretation of proposed quantum information theory experiments to probe the quantum nature of gravity by testing for gravitationally induced entanglement (GIE) between two spatially separated massive particles ([1] vs. [11,12]; further contributions in [7,9]). The confusion appears to reside in interpreting applications of a Hamiltonian formalism. But classical gravity cannot mediate entanglement on independent grounds. A Newton-Cartan analysis shows that if gravity is classical, a mediator, and entanglement is observed as an outcome of performing a GIE experiment, something other than gravity must have supplied the (virtual) force needed during the experiment to produce the effect.

</details>


### [65] [Machine Learning by Adiabatic Evolutionary Quantum System](https://arxiv.org/abs/2511.18496)
*Tomoyuki Yamakami*

Main category: quant-ph

TL;DR: 本文研究如何通过量子训练绝热演化量子系统（AEQS）来高效完成机器学习任务，重点开发了基于量子计数、量子振幅估计和量子近似的量子学习算法。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用量子自动机控制的绝热演化量子系统（AEQS）来高效解决机器学习任务，探索量子算法在训练量子系统方面的应用潜力。

Method: 开发基于量子计数、量子振幅估计和量子近似等量子算法的学习算法，通过寻找最优的1qqaf量子自动机来近似解决目标关系问题。

Result: 提供了量子学习算法效率的粗略估计，展示了在1qqaf控制的AEQS系统中实现机器学习任务的可能性。

Conclusion: 证明了通过量子算法训练AEQS系统可以有效完成机器学习任务，为量子机器学习提供了新的实现途径。

Abstract: A computational model of adiabatic evolutionary quantum system (or AEQS, pronounced "eeh-ks") was introduced in [Yamakami,2022] as a sort of quantum annealing and its underlying input-driven Hamiltonians are generated quantum-algorithmically by various forms of quantum automata families (including 1qqaf's). We study an efficient way to accomplish certain machine learning tasks by training these AEQSs quantumly. When AEQSs are controlled by 1qqaf's, it suffices in essence to find an optimal 1qqaf that approximately solves a target relational problem. For this purpose, we develop a basic idea of approximately utilizing well-known quantum algorithms for quantum counting, quantum amplitude estimation, and quantum approximation. We then provide a rough estimation of the efficiency of our quantum learning algorithms for AEQSs.

</details>


### [66] [Tunable Bands in 1D Fractional Quantum Media](https://arxiv.org/abs/2511.18574)
*Brenden R. Guyette,Joshua M. Lewis,Lincoln D. Carr*

Main category: quant-ph

TL;DR: 本文通过将薛定谔方程推广到分数阶形式，研究了周期性势场中粒子的能带结构，揭示了Lévy指数q在q=2处导致能带结构的定性转变，并展示了q作为可调自由度能够驱动能带反转、调控带隙和重塑载流子动力学。


<details>
  <summary>Details</summary>
Motivation: 分数阶微积分已成为研究长程关联和反常输运现象的重要框架，本文旨在将分数阶微积分扩展到周期性势场中的量子系统，探索Lévy指数q如何调控能带形成和反转，为设计新的物理行为和器件功能提供途径。

Method: 使用虚时间演化算法求解周期性矩形势场的分数阶薛定谔方程，并通过高斯过程回归补充离散能量色散关系，分析不同势垒高度V0、厚度L和阱宽W下的能带结构。

Result: 发现能带结构在q=2处发生定性转变：q>2时能带发生反转，在第一布里渊区内出现对称极小值，形成布洛赫动量量子比特；q<2时k=0附近的有效质量随q指数减小，当q→1时达到普适有效质量0.15±0.01。基带响应随分数阶标度为V0^{-0.28±0.05}L^{-0.34±0.08}W^{-0.49±0.06}。

Conclusion: Lévy指数q作为可调自由度能够驱动能带反转、调控带隙和重塑载流子动力学，为周期性量子系统中设计新的物理行为和器件功能提供了有效途径。

Abstract: Fractional calculus has become an essential framework in geophysics, optics, and biological systems to capture long-range correlations and anomalous transport. In this article, we extend fractional calculus to explore a particle in a periodic potential, where the Schrödinger equation is generalized to its fractional form. This framework allows us to study how the Lévy index $q$ governs the formation and inversion of energy bands, offering a pathway to engineer new physical behaviors and device functionalities by tuning $q$ in periodic quantum systems. We solve the fractional Schrödinger equation for periodic rectangular potentials of varying height $V_0$, barrier thickness $L$, and well width $W$ using an imaginary-time evolution algorithm, and supplement the discrete energy dispersion through Gaussian process regression. Our analysis reveals a qualitative shift in the band structure at $q=2$, separating into regimes for $q>2$ and $q<2$. For $q > 2$, energy bands undergo an inverting transformation as symmetric minima emerge within the first Brillouin zone, shifting from $k=0$ toward $k=\pm π/a$ with increasing $q$. These degenerate minima define a Bloch-momentum qubit, suggesting an analog to valley degrees of freedom used in valleytronics. The response of the ground band scales with fractional order as $V_0^{-0.28\pm0.05}L^{-0.34\pm0.08}W^{-0.49\pm0.06}$, indicating tunable sensitivity to geometry. For $q < 2$, the effective mass near $k = 0$ decreases exponentially with $q$, yielding a universal effective mass of $0.15\pm0.01$ as $q \to 1$, demonstrating that the Lévy index serves as a tunable degree of freedom capable of driving band inversion, modulating the band gap, and reshaping carrier dynamics.

</details>


### [67] [Teleportation-based quantum state tomography](https://arxiv.org/abs/2511.18621)
*Gustavo Rigolin*

Main category: quant-ph

TL;DR: 量子隐形传态协议可用于完全重构任意二比特和三比特密度矩阵，并扩展到n比特情况。


<details>
  <summary>Details</summary>
Motivation: 探索量子隐形传态协议在量子态层析中的应用，简化量子态重构所需的量子资源。

Method: 利用贝尔测量和准备少量不同的单比特态进行隐形传态，实现量子态层析。

Result: 成功展示了如何通过隐形传态协议完全重构任意二比特和三比特密度矩阵，并可扩展到n比特系统。

Conclusion: 量子隐形传态协议提供了一种有效的量子态层析方法，仅需贝尔测量和少量单比特态准备即可实现完整密度矩阵重构。

Abstract: We explicitly show that the quantum teleportation protocol can be employed to completely reconstruct arbitrary two- and three-qubit density matrices. We also extend the present analysis to n-qubit density matrices. The only quantum resources needed to implement the teleportation-based quantum state tomography protocol are the ability to make Bell measurements and the ability to prepare a few different single qubit states to be teleported from Alice to Bob.

</details>


### [68] [HOPPS: Hardware-Aware Optimal Phase Polynomial Synthesis with Blockwise Optimization for Quantum Circuits](https://arxiv.org/abs/2511.18770)
*Xinpeng Li,Ji Liu,Shuai Xu,Paul Hovland,Vipin Chaudhary*

Main category: quant-ph

TL;DR: 提出HOPPS算法，一种基于SAT的硬件感知最优相位多项式合成方法，用于优化{CNOT, Rz}模块的CNOT数量和深度，并通过迭代分块优化策略解决大规模电路的扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 量子应用中的{CNOT, Rz}模块（如QAOA ansatzes和量子加法器）在编译后往往具有较高的CNOT数量和深度，这会降低保真度，需要优化方法。

Method: 使用SAT-based硬件感知最优相位多项式合成算法HOPPS生成CNOT数量或深度最优的{CNOT, Rz}模块，并采用迭代分块优化策略将大电路分割成小模块进行优化。

Result: HOPPS比现有近似最优合成工具更高效，作为窥孔优化器可将CNOT数量减少达50.0%，CNOT深度减少达57.1%。对于大型QAOA电路，通过迭代分块优化可减少CNOT数量44.4%和深度42.4%。

Conclusion: HOPPS算法能有效优化量子电路中的{CNOT, Rz}模块，显著降低CNOT数量和深度，提高电路性能。

Abstract: Blocks composed of {CNOT, Rz} are ubiquitous in modern quantum applications, notably in circuits such as QAOA ansatzes and quantum adders. After compilation, many of them exhibit large CNOT counts or depths, which lowers fidelity. Therefore, we introduce HOPPS: a SAT-based hardware-aware optimal phase polynomial synthesis algorithm that could generate {CNOT, Rz} blocks with CNOT count or depth optimality.
  Sometime {CNOT, Rz} blocks are large, such as in QAOA ansatzes, HOPPS's pursuit of optimality limits its scalability. To address this issue, we introduce an iterative blockwise optimization strategy: large circuits are partitioned into smaller blocks, each block is optimally refined, and the process is repeated for several iterations.
  Empirical results show that HOPPS is more efficient comparing with existing near optimal synthesis tools. Used as a peephole optimizer, HOPPS reduces the CNOT count by up to 50.0% and the CNOT depth by up to 57.1% under OLSQ. For large QAOA circuit, after mapping by Qiskit, circuit can be reduced CNOT count and depth by up to 44.4% and 42.4% by our iterative blockwise optimization. Index Terms-Phase Polynomial, Quantum Circuit Synthesis, Quantum Circuit Optimization.

</details>


### [69] [Quantum Key Distribution Based on Systematic Polar Coding](https://arxiv.org/abs/2511.18818)
*Georgi Bebrov*

Main category: quant-ph

TL;DR: 该论文提出了一种结合量子密钥分发和系统极性编码的量子密钥分发方案，在有限尺寸机制和低误码率量子信道条件下，相比标准BB84及其高效版本eBB84获得了更高的密钥率。


<details>
  <summary>Details</summary>
Motivation: 传统量子密钥分发方案（如BB84及其高效版本eBB84）在有限尺寸机制和低误码率量子信道条件下的密钥率存在提升空间，需要开发更高效的方案。

Method: 将量子密钥分发与系统极性编码（一种纠错算法）框架相结合，提出基于系统极性编码的量子密钥分发方案。

Result: 在有限尺寸机制和低误码率量子信道条件下，所提方案获得了比标准BB84和eBB84更高的密钥率。

Conclusion: 基于系统极性编码的量子密钥分发方案是有效的，能够在特定条件下显著提升密钥分发效率。

Abstract: Here we concerned with quantum key distribution - a way to establish common cryptographic key between several parties. The work proposes a combination between quantum key distribution and systematic polar coding (an error correction algorithm) frameworks - quantum key distribution based on systematic polar coding. This results in obtaining key rates greater than standard quantum key distribution (BB84) and its efficient version (eBB84) when finite-size regime and lower-error-rate quantum channel are considered.

</details>


### [70] [Processing Entangled Links Into Secure Cryptographic Keys](https://arxiv.org/abs/2511.18913)
*Marcel Kokorsch,Guido Dietl*

Main category: quant-ph

TL;DR: 提出了一种处理纠缠链路在基于纠缠的量子密钥分发协议中的整体方法，研究整个处理链对最终安全密钥率的集体影响


<details>
  <summary>Details</summary>
Motivation: 研究纠缠蒸馏、测量处理和经典后处理等整个处理链对基于贝尔不等式安全的量子密钥分发协议最终安全密钥率的集体影响

Method: 基于Eckert 1991协议原理，利用Devetak和Winter 2005年引入的密钥容量，提出统一形式化方法描述整个处理链

Result: 证明了Werner态情况下需要选择哪些测量基才能达到密钥容量，提出了新的处理策略并与文献中最常见策略进行比较，确定了最优纠缠蒸馏量

Conclusion: 提出了一个统一的形式化框架，可用于定量描述用于生成安全密钥的纠缠但噪声量子态的质量与数量之间的关系

Abstract: The following paper presents a holistic approach to the processing of entangled links within entanglement based quantum key distribution protocols, whose security relies on the Bell inequality. We investigate the interactions, and the collective impact, of the whole processing chain on the final secure key rate. This includes the quantum mechanical preprocessing in the form of entanglement distillation, processing of the entangled states via measurements and the necessary classical postprocessing based on the measurement results. Our investigations are based on the principle idea of the Eckert 1991 protocol and utilize the secret key capacity introduced by Devetak and Winter in 2005. Our results include a proof on what measurement bases need to be chosen to achieve this capacity for the case of Werner states. It also presents a new processing strategy and compares it with the most common one that can be found within the literature. Furthermore, it answers the question on how much entanglement distillation is optimal. By doing so we propose a unified formalism, describing the whole processing chain, that can be used to make quantitative statements on the relation between the quality and quantity of entangled but noisy quantum states used for generating secure keys.

</details>


### [71] [Noisy dynamics of confined quantum walks on a chip](https://arxiv.org/abs/2511.19125)
*L. Sansoni,E. Stefanutti,C. Benedetti,I. Gianani,C. Taballione,A. Toor,L. Herrera,M. Pistilli,S. Santoro,M. Barbieri,A. Chiuri*

Main category: quant-ph

TL;DR: 使用20x20片上多模干涉仪研究量子行走中边缘效应如何与噪声相互作用，展示噪声如何破坏平移对称性并重塑干涉模式。


<details>
  <summary>Details</summary>
Motivation: 研究量子行走中相干过程与耗散过程的相互作用，特别关注晶格边缘的存在如何影响受限演化，以及噪声如何影响系统动力学。

Method: 采用20x20片上多模干涉仪进行实验，在量子行走系统中引入晶格边缘以产生受限演化，并研究不同噪声对系统的影响。

Result: 噪声破坏了系统的平移对称性，重塑了干涉模式，产生了非平凡的概率分布，显示了加速效应、局域化和相干振荡等现象。

Conclusion: 在现实量子动力学中，加速效应、局域化和相干振荡是需要充分表征和理解的核心概念，噪声与边缘效应的相互作用对量子行走行为有重要影响。

Abstract: Quantum walks represent an excellent testbed for investigating the interplay between unitary coherent and incoherent dissipative processes. Thanks to photonic quantum interferometers of considerable size, experimental studies could be performed, devoted to investigating the consequences of different sorts of realistic noise in these systems. In this work we employ a 20x20 on-chip multimode interferometer to introduce another key aspect in the problem: the presence of edges in the walker lattice, enforcing a confined evolution. We show how noise can disrupt translational symmetry and reshape interference patterns. The non trivial probability distributions obtained along the temporal evolution of the system demonstrate how speed up effects, localization and coherent oscillations are pillar concepts to be fully characterized and understood when applied in realistic quantum dynamics.

</details>


### [72] [Feature Ranking in Credit-Risk with Qudit-Based Networks](https://arxiv.org/abs/2511.19150)
*Georgios Maragkopoulos,Lazaros Chavatzoglou,Aikaterini Mandilara,Dimitris Syvridis*

Main category: quant-ph

TL;DR: 提出基于单qudit的量子神经网络，在信用风险评估中平衡准确性和可解释性，在台湾真实数据集上表现优于逻辑回归，达到随机森林水平，同时保持参数与特征重要性的透明对应关系。


<details>
  <summary>Details</summary>
Motivation: 在金融领域，预测模型需要在准确性和可解释性之间取得平衡，特别是在信用风险评估中，模型决策具有实质性后果。需要开发既准确又透明的模型。

Method: 使用基于单qudit的量子神经网络，将数据特征和可训练参数共同编码在由完整李代数生成的统一幺正演化中，通过学得系数的大小实现可解释性。

Result: 在台湾真实不平衡信用风险数据集上，提出的QNN持续优于逻辑回归，在宏观F1分数上达到随机森林模型的结果，同时保持学得参数与输入特征重要性之间的透明对应关系。

Conclusion: 提出的量子模型在实现竞争性能的同时，为可解释量子学习提供了一条可行的路径，通过引入编辑距离和特征污染测试两个互补指标量化了模型的可解释性。

Abstract: In finance, predictive models must balance accuracy and interpretability, particularly in credit risk assessment, where model decisions carry material consequences. We present a quantum neural network (QNN) based on a single qudit, in which both data features and trainable parameters are co-encoded within a unified unitary evolution generated by the full Lie algebra. This design explores the entire Hilbert space while enabling interpretability through the magnitudes of the learned coefficients. We benchmark our model on a real-world, imbalanced credit-risk dataset from Taiwan. The proposed QNN consistently outperforms LR and reaches the results of random forest models in macro-F1 score while preserving a transparent correspondence between learned parameters and input feature importance. To quantify the interpretability of the proposed model, we introduce two complementary metrics: (i) the edit distance between the model's feature ranking and that of LR, and (ii) a feature-poisoning test where selected features are replaced with noise. Results indicate that the proposed quantum model achieves competitive performance while offering a tractable path toward interpretable quantum learning.

</details>


### [73] [Flexible Genetic Algorithm for Quantum Support Vector Machines](https://arxiv.org/abs/2511.19160)
*Nguyen Minh Duc,Vu Tuan Hai,Le Bin Ho,Tran Nguyen Lan*

Main category: quant-ph

TL;DR: 提出GA-QSVM框架，使用遗传算法自动优化量子支持向量机的特征映射，解决传统固定量子电路泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统QSVM使用固定量子电路作为特征映射，这种设计往往无法在不同数据集上有效泛化，限制了量子机器学习性能。

Method: 采用遗传算法自动优化特征映射，构建可配置框架来灵活定义进化参数，实现自适应量子电路设计。

Result: 在Digits、Fashion、Wine和Breast Cancer数据集上的实验表明，GA-QSVM达到与经典SVM和标准QSVM相当的准确率，且迁移学习结果显示其电路能有效跨数据集泛化。

Conclusion: 进化策略在自动化量子机器学习核设计方面具有重要潜力，为未来量子机器学习应用提供了有效方法。

Abstract: Quantum Support Vector Machines (QSVM) is one of the most promising frameworks in quantum machine learning, yet their performance depends on the design of the feature map. Conventional approaches rely on fixed quantum circuits, which often fail to generalize across datasets. To address this limitation, we propose GA-QSVM, a hybrid framework that employs Genetic Algorithms (GA) to automatically optimize feature maps. The proposed method introduces a configurable framework that flexibly defines the evolutionary parameters, enabling the construction of adaptive circuits. Experimental evaluation of datasets, including Digits, Fashion, Wine, and Breast Cancer, demonstrates that GA-QSVMs achieve a comparable accuracy compared to classical SVMs and standard QSVMs. Furthermore, transfer learning results indicate that GA-QSVM's circuits generalize effectively across datasets. These findings highlight the potential of evolutionary strategies to automate and enhance kernel design for future quantum machine learning applications.

</details>


### [74] [Synchronized Aharonov-Bohm Motifs via Engineered Dissipation](https://arxiv.org/abs/2511.19219)
*Christopher W. Wächtler,Gloria Platero*

Main category: quant-ph

TL;DR: 结合通量诱导局域化和工程耗散，在旋转对称的自旋几何结构中实现鲁棒的量子同步，形成纠缠的自旋团簇。


<details>
  <summary>Details</summary>
Motivation: 探索外部规范场与晶格几何相互作用如何通过完全相消干涉产生极端局域化，并将其与耗散工程结合以实现量子同步。

Method: 在具有循环对称性的Aharonov-Bohm motif中，结合通量诱导局域化和工程耗散，通过作用于所有团簇内自旋的集体耗散实现多团簇同步。

Result: 实现了与初始条件无关的鲁棒自旋同步，每个团簇内的自旋形成纠缠，多个团簇可以完全同步。

Conclusion: 揭示了通量诱导局域化、耗散工程和集体量子同步之间的直接联系。

Abstract: The interplay between external gauge fields and lattice geometry can induce extreme localization dynamics through complete destructive interference. We show that combining this flux-induced localization with engineered dissipation leads to robust spin synchronization in rotationally symmetric spin geometries, referred to as Aharonov-Bohm motifs, with cyclic symmetries of any order. The synchronized dynamics is independent of initial conditions and features entanglement among spins within each motif. We further demonstrate that multiple motifs can fully synchronize when coupled, which is achieved by applying additional collective dissipation acting on all intra-motif spins. These results reveal a direct connection between flux-induced localization, dissipative engineering, and collective quantum synchronization.

</details>


### [75] [Entropic Dynamics approach to Quantum Electrodynamics](https://arxiv.org/abs/2511.19238)
*Ariel Caticha*

Main category: quant-ph

TL;DR: 本文扩展了熵动力学框架来处理局部规范对称性，基于最大熵方法和信息几何，推导出辐射场与带电粒子相互作用的量子电动力学，并验证了麦克斯韦方程。


<details>
  <summary>Details</summary>
Motivation: 将熵动力学框架扩展到处理局部规范对称性，以验证这种非正统方法的经验成功性。

Method: 使用最大熵方法和信息几何，通过适当的本体变量选择和约束条件，推导量子电动力学。

Result: 成功推导出辐射场与带电粒子相互作用的量子电动力学，并验证了麦克斯韦方程的正确性。

Conclusion: 熵动力学框架能够成功处理局部规范对称性，为量子理论提供了新的基础，并验证了其经验有效性。

Abstract: Entropic dynamics (ED) is a framework that allows one to derive quantum theory as a Hamilton-Killing flow on the cotangent bundle of a statistical manifold. These flows are such that they preserve the symplectic and the (information) metric geometries; they explain the linearity of quantum mechanics and the appearance of complex numbers. In this paper the ED framework is extended to deal with local gauge symmetries. More specifically, on the basis of maximum entropy methods and information geometry, for an appropriate choice of ontic variables and constraints, we derive the quantum electrodynamics of radiation fields interacting with charged particles. As a test that despite its unorthodox foundation the ED approach is empirically successful we derive the Maxwell equations.

</details>


### [76] [Longitudinal Pulsed Dynamic Nuclear Polarization Transfer via Periodic Optimal Control](https://arxiv.org/abs/2511.19244)
*José P. Carvalho,Anders Bodholt Nielsen,David L. Goodwin,Nino Wili,Niels Chr. Nielsen*

Main category: quant-ph

TL;DR: 提出了一种名为LOOP的新型宽带动态核极化脉冲序列，通过纵向极化转移解决激发脉冲挑战，在0.35T磁场下实现超过100MHz的带宽，仅需32MHz的峰值微波场振幅。


<details>
  <summary>Details</summary>
Motivation: 受NMR光谱学启发，周期性辐照方案在脉冲动态核极化序列中表现出色，但现有横向自旋锁定脉冲序列的性能受到初始激发脉冲宽带能力的限制。

Method: 结合最优控制理论的灵活性和鲁棒性以及有效哈密顿理论的深入见解，开发了LOOP序列，实现纵向极化转移并定义鲁棒的单自旋有效z旋转。

Result: LOOP序列对微波场不均匀性具有显著补偿能力，在0.35T磁场下能够实现超过100MHz的DNP转移带宽，仅需32MHz的峰值微波场振幅。

Conclusion: LOOP序列通过纵向极化转移有效解决了激发脉冲挑战，为宽带脉冲DNP序列的发展提供了新的有效方法。

Abstract: Taking inspiration from NMR spectroscopy, periodic irradiation schemes have recently shown remarkable performance when implemented into pulsed dynamic nuclear polarization (DNP) sequences. This has prompted considerable interest in development of broadband pulsed DNP sequences utilizing such schemes. On this background, most efforts have focused on solid-state NMR like transverse spin-locked pulse sequences whose performance in DNP applications may be compromised by the broadband capabilities of the initial excitation pulse. Leveraging the flexibility and robustness of optimal control theory combined with underlying insights from effective Hamiltonian theory, we present a new family of broadband DNP pulse sequences, termed LOOP (Longitudinally Optimized with Overarching Periodicity), that alleviates the excitation-pulse challenge by accomplishing longitudinal polarization transfer. These sequences define robust single-spin effective $z$ rotations, with impressive compensation towards microwave field inhomogeneity, and are capable of delivering DNP transfer with bandwidths exceeding 100 MHz, while employing a peak microwave field amplitude of only 32 MHz, at an external magnetic field of 0.35 T.

</details>


### [77] [Neural Architecture Search for Quantum Autoencoders](https://arxiv.org/abs/2511.19246)
*Hibah Agha,Samuel Yen-Chi Chen,Huan-Hsin Tseng,Shinjae Yoo*

Main category: quant-ph

TL;DR: 提出了一种基于遗传算法的神经架构搜索框架，用于自动化设计量子自编码器，在噪声环境下实现高效特征提取


<details>
  <summary>Details</summary>
Motivation: 量子自编码器在压缩高维量子数据和经典数据方面具有潜力，但设计有效的量子电路架构具有挑战性，需要解决门选择、电路层排列和参数调优等复杂问题

Method: 使用遗传算法系统演化变分量子电路配置，自动搜索高性能的混合量子-经典自编码器架构，避免陷入局部最小值

Result: 在图像数据集上验证了方法的有效性，展示了量子自编码器在噪声近量子时代进行高效特征提取的潜力

Conclusion: 该方法为将遗传算法更广泛地应用于量子架构搜索奠定了基础，旨在开发能够适应不同数据和硬件约束的鲁棒自动化方法

Abstract: In recent years, machine learning and deep learning have driven advances in domains such as image classification, speech recognition, and anomaly detection by leveraging multi-layer neural networks to model complex data. Simultaneously, quantum computing (QC) promises to address classically intractable problems via quantum parallelism, motivating research in quantum machine learning (QML). Among QML techniques, quantum autoencoders show promise for compressing high-dimensional quantum and classical data. However, designing effective quantum circuit architectures for quantum autoencoders remains challenging due to the complexity of selecting gates, arranging circuit layers, and tuning parameters.
  This paper proposes a neural architecture search (NAS) framework that automates the design of quantum autoencoders using a genetic algorithm (GA). By systematically evolving variational quantum circuit (VQC) configurations, our method seeks to identify high-performing hybrid quantum-classical autoencoders for data reconstruction without becoming trapped in local minima. We demonstrate effectiveness on image datasets, highlighting the potential of quantum autoencoders for efficient feature extraction within a noise-prone, near-term quantum era. Our approach lays a foundation for broader application of genetic algorithms to quantum architecture search, aiming for a robust, automated method that can adapt to varied data and hardware constraints.

</details>


### [78] [Decoupling local classicality from classical explainability: A noncontextual model for bilocal classical theory and a locally-classical but contextual theory](https://arxiv.org/abs/2511.19266)
*Sina Soltani,Marco Erba,David Schmid,John H. Selby*

Main category: quant-ph

TL;DR: 构建了双局域经典理论的本体论模型，反驳了该理论不存在局域实在论本体论模型的猜想，并首次为非局域层析理论构建了本体论模型。


<details>
  <summary>Details</summary>
Motivation: 验证双局域经典理论是否存在局域实在论的本体论模型，并探索局域层析假设在结构定理中的必要性。

Method: 通过具体构造双局域经典理论的本体论模型，并构建反例来验证理论的普遍性。

Result: 成功构建了双局域经典理论的本体论模型，证明该理论确实存在局域实在论解释，并发现并非所有局域经典理论都允许本体论模型。

Conclusion: 局域层析等组合性质在理解经典性本质中具有核心地位，局域经典性与经典可解释性之间没有简单对应关系。

Abstract: We construct an ontological model for the theory known as bilocal classical theory doi.org/10.1103/PhysRevA.102.052216. To our knowledge, this is only the second time that an ontological model has been constructed for an entire theory, rather than just for some particular scenarios within a theory. This result refutes a conjecture from doi.org/10.1103/PhysRevA.102.052216 which suggested that there might be no local-realist ontological model for bilocal classical theory. Moreover, it is the first time that an ontological model has been constructed for a theory that fails to be locally tomographic, showing that the assumption of local tomography underpinning the structure theorem in doi.org/10.22331/q-2024-03-14-1283 is a genuine limitation of the theorem. This demonstrates that in general there is no tension between failures of local tomography and classical explainability (i.e., generalised noncontextuality). In fact, bilocal classical theory is in many ways more simply understood via the underlying ontological model than it is within its original formulation (much as how odd-dimensional stabiliser subtheories can be more simply understood via Spekkens' toy theory). Furthermore, this result naturally leads to the question, does every locally-classical theory admit of an ontological model? By constructing a concrete counterexample, we show that this is not the case. Our findings demonstrate that there is no straightforward relationship between theories being locally-classical, and them being classically-explainable. This shows that the fundamental status of compositional properties (such as local tomography) is not a technical side-issue, but a central and unavoidable question for a coherent understanding even of classicality itself.

</details>


### [79] [Performance Guarantees for Quantum Neural Estimation of Entropies](https://arxiv.org/abs/2511.19289)
*Sreejith Sreekumar,Ziv Goldfeld,Mark M. Wilde*

Main category: quant-ph

TL;DR: 本文为量子神经估计器(QNE)在测量相对熵估计方面建立了首个非渐近误差风险界限，证明了误差服从次高斯分布，并给出了具有极小极大最优性的样本复杂度分析。


<details>
  <summary>Details</summary>
Motivation: 量子熵和散度的估计在量子物理、信息理论和机器学习中很重要。量子神经估计器作为混合经典-量子架构的计算框架，在实际部署时需要繁琐的超参数调优，缺乏理论保证。

Method: 利用混合经典-量子架构，结合经典神经网络和参数化量子电路，研究QNE对测量(Rényi)相对熵的估计性能，建立非渐近误差风险界限和指数尾部界限。

Result: 对于Thompson度量有界的密度算子对，建立了O(|Θ(𝒰)|d/ε²)的样本复杂度；对于置换不变的密度算子对，改进为O(|Θ(𝒰)|polylog(d)/ε²)，在精度ε上达到极小极大最优。

Conclusion: 该理论为QNE在测量相对熵估计中的实现提供了原则性指导，有助于实践中超参数调优，并展示了在特定条件下QNE可以达到最优样本复杂度。

Abstract: Estimating quantum entropies and divergences is an important problem in quantum physics, information theory, and machine learning. Quantum neural estimators (QNEs), which utilize a hybrid classical-quantum architecture, have recently emerged as an appealing computational framework for estimating these measures. Such estimators combine classical neural networks with parametrized quantum circuits, and their deployment typically entails tedious tuning of hyperparameters controlling the sample size, network architecture, and circuit topology. This work initiates the study of formal guarantees for QNEs of measured (Rényi) relative entropies in the form of non-asymptotic error risk bounds. We further establish exponential tail bounds showing that the error is sub-Gaussian, and thus sharply concentrates about the ground truth value. For an appropriate sub-class of density operator pairs on a space of dimension $d$ with bounded Thompson metric, our theory establishes a copy complexity of $O(|Θ(\mathcal{U})|d/ε^2)$ for QNE with a quantum circuit parameter set $Θ(\mathcal{U})$, which has minimax optimal dependence on the accuracy $ε$. Additionally, if the density operator pairs are permutation invariant, we improve the dimension dependence above to $O(|Θ(\mathcal{U})|\mathrm{polylog}(d)/ε^2)$. Our theory aims to facilitate principled implementation of QNEs for measured relative entropies and guide hyperparameter tuning in practice.

</details>


### [80] [TorchQuantumDistributed](https://arxiv.org/abs/2511.19291)
*Oliver Knitter,Jonathan Mei,Masako Yamada,Martin Roetteler*

Main category: quant-ph

TL;DR: TorchQuantumDistributed (tqd) 是一个基于 PyTorch 的可扩展量子态向量模拟库，支持跨加速器的可微分计算，用于研究高比特数的参数化量子电路。


<details>
  <summary>Details</summary>
Motivation: 为了研究具有大量量子比特的可学习参数化量子电路（包括近期的和容错的量子电路）的行为，需要一个能够进行大规模、可微分量子态向量模拟的工具。

Method: 开发了基于 PyTorch 的 TorchQuantumDistributed (tqd) 库，该库支持跨加速器的可微分量子态向量模拟，能够处理高比特数的量子电路。

Result: tqd 库实现了对高比特数量子电路的可微分模拟，为研究参数化量子电路的行为提供了工具支持。

Conclusion: TorchQuantumDistributed 为大规模量子电路的可微分模拟提供了一个有效的解决方案，有助于推动量子计算的研究和应用。

Abstract: TorchQuantumDistributed (tqd) is a PyTorch-based [Paszke et al., 2019] library for accelerator-agnostic differentiable quantum state vector simulation at scale. This enables studying the behavior of learnable parameterized near-term and fault- tolerant quantum circuits with high qubit counts.

</details>


### [81] [Efficient Equivalent of Shallow Quantum Hashing](https://arxiv.org/abs/2511.19292)
*Ilnar Zinnatullin,Alexander Vasiliev*

Main category: quant-ph

TL;DR: 本文建立了浅层量子哈希与单量子比特量子哈希在幅度形式上的联系，提出了深度为1的电路实现相同抗碰撞性


<details>
  <summary>Details</summary>
Motivation: 量子哈希是量子计算中的关键技术，用于设计空间高效的算法和协议。最近的研究表明浅层量子哈希的相位形式可以用深度2的电路实现，本文旨在探索幅度形式的浅层量子哈希实现

Method: 建立浅层量子哈希与单量子比特量子哈希的联系，提出深度为1的电路设计

Result: 成功实现了深度为1的电路，能够达到与之前方法相同的抗碰撞性

Conclusion: 浅层量子哈希在幅度形式下可以用更浅的电路（深度1）实现，且保持相同的安全性能

Abstract: Quantum hashing is a widely used technique in quantum computation that allows us to design space-efficient algorithms and protocols. Recently, Vasiliev has shown that the phase form of shallow quantum hashing can be implemented by a circuit of depth 2. In this paper, we establish a connection between shallow quantum hashing and single-qubit quantum hashing for the amplitude form. For a shallow circuit, we propose a circuit of depth 1 that achieves the same collision resistance.

</details>


### [82] [Quantitative and Optimal Device-Independent Lower Bounds on Detection Efficiency](https://arxiv.org/abs/2511.19302)
*Arkaprabha Ghosal,Soumyadip Patra,Peter Bierhorst*

Main category: quant-ph

TL;DR: 本文在完全设备无关框架下研究了(2,2,2)贝尔实验中探测器效率的定量最优下界，考虑了暗计数影响，并提供了数值紧下界和解析下界。


<details>
  <summary>Details</summary>
Motivation: 在设备无关量子信息处理中，需要确定观察贝尔-CHSH违反所需的最小探测器效率，这对于实际实验设计至关重要。

Method: 使用NPA层次结构进行数值优化，考虑暗计数效应，并利用满足Tsirelson界的无信号行为集获得解析闭式表达式。

Result: 获得了数值紧下界（精确到四位小数），并推导出随CHSH违反单调增加的解析下界，但解析下界低于数值紧下界。

Conclusion: 本文提供了设备无关贝尔实验中探测器效率的定量分析框架，为实际实验设计提供了理论指导，但解析方法与数值方法存在差距。

Abstract: This paper examines a quantitative and optimal lower bound on the detector efficiency in a (2,2,2) Bell experiment within a fully device-independent framework, whereby the detectors used in the experiment are uncharacterized. We provide a tight lower bound on the minimum efficiency required to observe a desired Bell-CHSH violation using the Navascués-Pironio-Acín (NPA) hierarchy, confirming tightness up to four decimal places with numerical optimization over explicit quantum realizations. We then introduce the effect of dark counts and demonstrate how to quantify the minimum required efficiency to observe a desired CHSH violation with an increasing dark count error. Finally, to obtain an analytical closed-form expression of the minimum efficiency, we consider the set of no-signaling behaviors that satisfy the Tsirelson bound, which are easier to characterize than the quantum set. Using such behaviors, we find a simple closed-form expression for a lower bound on the minimum efficiency which is monotonically increasing with the CHSH violation, though the analytically obtained lower bounds are meaningfully below the numerically tight lower bound.

</details>


### [83] [Simulating dynamics of the two-dimensional transverse-field Ising model: a comparative study of large-scale classical numerics](https://arxiv.org/abs/2511.19340)
*Joseph Vovrosh,Sergi Julià-Farré,Wladislaw Krinitsin,Michael Kaicher,Fergus Hayes,Emmanuel Gottlob,Augustine Kshetrimayum,Kemal Bidzhiev,Simon B. Jäger,Markus Schmitt,Joseph Tindall,Constantin Dalyac,Tiago Mendes-Santos,Alexandre Dauphin*

Main category: quant-ph

TL;DR: 本文使用多种最先进的数值方法对二维横向场伊辛模型的量子动力学进行经典模拟，包括三种张量网络技术和基于神经量子态的变分蒙特卡洛方法，重点研究了量子退火和淬火动力学两种典型协议。


<details>
  <summary>Details</summary>
Motivation: 量子多比特系统的动力学是一个重要问题，近年来在数值方法和可编程量子处理单元方面都取得了显著进展。本文旨在通过经典模拟为未来的数值研究和实验研究提供基准。

Method: 采用了四种最先进的数值方法：矩阵乘积态、树张量网络、基于置信传播近似的二维张量网络，以及基于神经量子态的含时变分蒙特卡洛方法。

Result: 获得了各种最先进数值方法的定量预测结果，为经典和量子处理单元的性能极限提供了基准，并将经典可模拟性与里德堡阵列中的不同量子动力学机制联系起来。

Conclusion: 这项工作为未来数值研究和实验研究提供了重要基准，特别是在连接经典可模拟性与里德堡阵列中量子动力学的不同机制方面具有重要意义。

Abstract: The quantum dynamics of many-qubit systems is an outstanding problem that has recently driven significant advances in both numerical methods and programmable quantum processing units. In this work, we employ a comprehensive toolbox of state-of-the-art numerical approaches to classically simulate the dynamics of the two-dimensional transverse field Ising model. Our methods include three different tensor network techniques -- matrix product states, tree-tensor networks, and two-dimensional tensor-networks under the belief propagation approximation -- as well as time-dependent variational Monte Carlo with Neural Quantum States. We focus on two paradigmatic dynamical protocols: (i) quantum annealing through a critical point and (ii) post-quench dynamics. Our extensive results show the quantitative predictions of various state-of-the-art numerical methods providing a benchmark for future numerical investigations and experimental studies with the aim to push the limitations on classical and QPUs. In particular, our work connects classical simulability to different regimes associated with quantum dynamics in Rydberg arrays - namely, quasi-adiabatic dynamics, the Kibble-Zurek mechanism, and quantum quenches.

</details>


### [84] [Entanglement-limited linear response in fermionic systems](https://arxiv.org/abs/2511.19415)
*Hadi Cheraghi,Ali G. Moghaddam,Teemu Ojanen*

Main category: quant-ph

TL;DR: 本文建立了纠缠熵标度律与粒子守恒费米系统线性响应函数之间的普遍联系，证明了局域扰动响应与区域纠缠熵具有相同的尺寸标度关系。


<details>
  <summary>Details</summary>
Motivation: 探索多体纠缠与系统响应特性之间的内在联系，揭示纠缠熵标度律如何影响物理可观测量的行为。

Method: 通过理论推导和自由费米子系统验证，分析局域粒子数扰动下的线性响应函数，并与纠缠熵标度进行比较。

Result: 在自由费米子系统中验证了响应函数与纠缠熵具有相同的标度行为（面积律、体积律和临界形式），发现能隙系统中能量吸收率和粒子数涨落与扰动区域边界而非体积成正比。

Conclusion: 建立了线性响应性质与多体纠缠之间的直接联系，为理解纠缠在物理可观测效应中的作用提供了新视角。

Abstract: We propose a general connection between entanglement-entropy scaling laws and the linear response functions of particle-conserving fermionic systems in their ground state. Specifically, we show that the response to perturbations coupled to the particle number within a finite region exhibits the same size scaling as the entanglement entropy of that region. We explicitly verify this scaling in free-fermion systems that display area-law, volume-law, and critical forms of entanglement. The resulting entanglement-governed scaling of response functions leads to unexpected physical consequences. For instance, contrary to conventional expectations, the energy absorption rate and particle-number fluctuations in gapped systems scale with the boundary of the perturbed region rather than with its volume. Our work thus establishes a direct link between linear-response properties and many-body entanglement.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [85] [Practical Machine Learning for Aphasic Discourse Analysis](https://arxiv.org/abs/2511.17553)
*Jason M. Pittman,Anton Phillips,Yesenia Medina-Santos,Brielle C. Stark*

Main category: cs.LG

TL;DR: 本研究评估了五种机器学习模型在失语症患者图片描述任务中自动识别正确信息单元(CIU)的能力。


<details>
  <summary>Details</summary>
Motivation: 尽管CIU分析是语言病理学家常用的失语症语言能力评估方法，但人工编码和分析耗时耗力，需要机器学习技术来辅助自动化处理。

Method: 使用五种监督式机器学习模型，基于失语症患者的人类编码转录文本和对应的CIU数据进行训练，评估模型在区分单词与非单词、CIU与非CIU方面的表现。

Result: 所有模型在区分单词与非单词方面表现优异(准确率0.995，AUC范围0.914-0.995)，但在识别CIU方面表现参差不齐，k-NN模型表现最佳(准确率0.824，AUC 0.787)。

Conclusion: 监督式机器学习模型能够有效区分单词与非单词，但准确识别CIU仍然具有挑战性，需要进一步改进模型性能。

Abstract: Analyzing spoken discourse is a valid means of quantifying language ability in persons with aphasia. There are many ways to quantify discourse, one common way being to evaluate the informativeness of the discourse. That is, given the total number of words produced, how many of those are context-relevant and accurate. This type of analysis is called Correct Information Unit (CIU) analysis and is one of the most prevalent discourse analyses used by speech-language pathologists (SLPs). Despite this, CIU analysis in the clinic remains limited due to the manual labor needed by SLPs to code and analyze collected speech. Recent advances in machine learning (ML) seek to augment such labor by automating modeling of propositional, macrostructural, pragmatic, and multimodal dimensions of discourse. To that end, this study evaluated five ML models for reliable identification of Correct Information Units (CIUs, Nicholas & Brookshire, 1993), during a picture description task. The five supervised ML models were trained using randomly selected human-coded transcripts and accompanying words and CIUs from persons with aphasia. The baseline model training produced a high accuracy across transcripts for word vs non-word, with all models achieving near perfect performance (0.995) with high AUC range (0.914 min, 0.995 max). In contrast, CIU vs non-CIU showed a greater variability, with the k-nearest neighbor (k-NN) model the highest accuracy (0.824) and second highest AUC (0.787). These findings indicate that while the supervised ML models can distinguish word from not word, identifying CIUs is challenging.

</details>


### [86] [Classification of Transient Astronomical Object Light Curves Using LSTM Neural Networks](https://arxiv.org/abs/2511.17564)
*Guilherme Grancho D. Fernandes,Marco A. Barroca,Mateus dos Santos,Rafael S. Oliveira*

Main category: cs.LG

TL;DR: 使用双向LSTM网络对PLAsTiCC数据集中的瞬变天体光变曲线进行分类，将14个原始类别重组为5个广义类别以解决类别不平衡问题。模型在S-Like和Periodic类别上表现优异，但在Fast和Long类别上表现较差，且难以区分Periodic和Non-Periodic对象。


<details>
  <summary>Details</summary>
Motivation: 解决瞬变天体光变曲线分类中的类别不平衡问题，并评估双向LSTM网络在此任务中的性能。

Method: 将14个原始类别重组为5个广义类别（S-Like、Fast、Long、Periodic、Non-Periodic），使用填充、时间重缩放和通量归一化进行预处理，训练带有掩码层的双向LSTM网络。

Result: 模型在S-Like和Periodic类别上表现优异（ROC AUC分别为0.95和0.99），但在Fast和Long类别上表现较差（Long类ROC AUC为0.68）。使用部分光变曲线数据（5、10、20天）时性能显著下降，错误分类偏向S-Like类。

Conclusion: 类别不平衡和有限的时间信息是主要限制因素，建议采用类别平衡策略和关注检测时刻的预处理技术来提高性能。

Abstract: This study presents a bidirectional Long Short-Term Memory (LSTM) neural network for classifying transient astronomical object light curves from the Photometric LSST Astronomical Time-series Classification Challenge (PLAsTiCC) dataset. The original fourteen object classes were reorganized into five generalized categories (S-Like, Fast, Long, Periodic, and Non-Periodic) to address class imbalance. After preprocessing with padding, temporal rescaling, and flux normalization, a bidirectional LSTM network with masking layers was trained and evaluated on a test set of 19,920 objects. The model achieved strong performance for S-Like and Periodic classes, with ROC area under the curve (AUC) values of 0.95 and 0.99, and Precision-Recall AUC values of 0.98 and 0.89, respectively. However, performance was significantly lower for Fast and Long classes (ROC AUC of 0.68 for Long class), and the model exhibited difficulty distinguishing between Periodic and Non-Periodic objects. Evaluation on partial light curve data (5, 10,and 20 days from detection) revealed substantial performance degradation, with increased misclassification toward the S-Like class. These findings indicate that class imbalance and limited temporal information are primary limitations, suggesting that class balancing strategies and preprocessing techniques focusing on detection moments could improve performance.

</details>


### [87] [Root Cause Analysis for Microservice Systems via Cascaded Conditional Learning with Hypergraphs](https://arxiv.org/abs/2511.17566)
*Shuaiyu Xie,Hanbin He,Jian Wang,Bing Li*

Main category: cs.LG

TL;DR: 提出了CCLH框架，通过级联条件学习和异构超图建模来解决微服务系统中根因定位和故障类型识别的挑战


<details>
  <summary>Details</summary>
Motivation: 传统诊断方法存在两个关键问题：1）联合学习范式忽略了任务间的因果依赖关系；2）主要关注实例间的点对点关系，忽略了由部署配置和负载均衡引起的群体影响

Method: CCLH框架采用级联条件学习来协调诊断任务，提供三级分类来描述实例间的群体影响，并使用异构超图来建模这些关系以模拟故障传播

Result: 在三个微服务基准数据集上的广泛实验表明，CCLH在根因定位和故障类型识别方面均优于现有最先进方法

Conclusion: CCLH通过级联条件学习和异构超图建模有效解决了微服务系统中根因分析的挑战，在两项核心任务上都取得了优越性能

Abstract: Root cause analysis in microservice systems typically involves two core tasks: root cause localization (RCL) and failure type identification (FTI). Despite substantial research efforts, conventional diagnostic approaches still face two key challenges. First, these methods predominantly adopt a joint learning paradigm for RCL and FTI to exploit shared information and reduce training time. However, this simplistic integration neglects the causal dependencies between tasks, thereby impeding inter-task collaboration and information transfer. Second, these existing methods primarily focus on point-to-point relationships between instances, overlooking the group nature of inter-instance influences induced by deployment configurations and load balancing. To overcome these limitations, we propose CCLH, a novel root cause analysis framework that orchestrates diagnostic tasks based on cascaded conditional learning. CCLH provides a three-level taxonomy for group influences between instances and incorporates a heterogeneous hypergraph to model these relationships, facilitating the simulation of failure propagation. Extensive experiments conducted on datasets from three microservice benchmarks demonstrate that CCLH outperforms state-of-the-art methods in both RCL and FTI.

</details>


### [88] [Enhancing Robustness of Offline Reinforcement Learning Under Data Corruption via Sharpness-Aware Minimization](https://arxiv.org/abs/2511.17568)
*Le Xu,Jiayu Chen*

Main category: cs.LG

TL;DR: 将Sharpness-Aware Minimization (SAM)作为通用优化器应用于离线强化学习，通过寻找更平坦的最小值来提高模型在数据损坏情况下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习对现实世界数据损坏非常脆弱，即使是鲁棒算法在具有挑战性的观测和混合损坏下也会失效，这源于数据损坏在损失景观中创造了尖锐的最小值，导致泛化能力差。

Method: 将SAM集成到强基线算法中：IQL（在此设置中表现最佳的离线RL算法）和RIQL（专门为数据损坏鲁棒性设计的算法），在D4RL基准测试中评估随机和对抗性损坏。

Result: SAM增强的方法始终显著优于原始基线，奖励表面的可视化证实SAM找到了更平滑的解决方案。

Conclusion: SAM作为离线RL的通用优化器，通过寻找更平坦的最小值有效提高了智能体在数据损坏情况下的鲁棒性。

Abstract: Offline reinforcement learning (RL) is vulnerable to real-world data corruption, with even robust algorithms failing under challenging observation and mixture corruptions. We posit this failure stems from data corruption creating sharp minima in the loss landscape, leading to poor generalization. To address this, we are the first to apply Sharpness-Aware Minimization (SAM) as a general-purpose, plug-and-play optimizer for offline RL. SAM seeks flatter minima, guiding models to more robust parameter regions. We integrate SAM into strong baselines for data corruption: IQL, a top-performing offline RL algorithm in this setting, and RIQL, an algorithm designed specifically for data-corruption robustness. We evaluate them on D4RL benchmarks with both random and adversarial corruption. Our SAM-enhanced methods consistently and significantly outperform the original baselines. Visualizations of the reward surface confirm that SAM finds smoother solutions, providing strong evidence for its effectiveness in improving the robustness of offline RL agents.

</details>


### [89] [Binary BPE: A Family of Cross-Platform Tokenizers for Binary Analysis](https://arxiv.org/abs/2511.17573)
*Michael J. Bommarito*

Main category: cs.LG

TL;DR: 提出了Binary BPE分词器家族，专门用于二进制分析，通过字节对编码在多种平台和架构的可执行文件上训练，显著提高了序列模型处理二进制内容的效率。


<details>
  <summary>Details</summary>
Motivation: 现有序列模型在处理二进制分析时受限于字节级分词，原始字节浪费了transformer等神经网络架构的上下文窗口容量，且许多文本导向的分词器无法处理0x00-0xFF的任意字节序列。

Method: 开发了跨平台的Byte Pair Encoding分词器，在包含Linux、Windows、macOS、Android和恶意软件来源的大型二进制语料库上训练，发布了4K到64K词汇量的分词器。

Result: Binary BPE分词器发现了可解释的模式（ELF/PE头、指令序列、跨平台字符串），同时实现了每token的多字节压缩。在未压缩可执行文件上，相比原始字节，固定长度transformer上下文窗口可容纳2-3倍的二进制内容。

Conclusion: Binary BPE分词器为二进制分析提供了更高效的研究和实际部署基础，支持内容识别、恶意软件检测、逆向工程和优化等应用，已在HuggingFace上开源发布。

Abstract: Sequence models for binary analysis are bottlenecked by byte-level tokenization: raw bytes waste precious context window capacity for transformers and other neural network architectures, and many existing text-oriented tokenizers fail on arbitrary 0x00--0xFF sequences. To address this issue, we introduce the Binary BPE tokenizer family, a set of cross-platform Byte Pair Encoding (BPE) tokenizers for executables trained on a large corpus of binaries spanning multiple platforms, architectures, and operating systems, including Linux, Windows, macOS, Android, and malware sources. We release trained tokenizers with vocabularies of 4K, 8K, 16K, 32K, and 64K tokens, enabling both systematic scaling studies and practical deployment from resource-constrained edge devices to high-throughput datacenters. These tokenizers discover interpretable patterns (ELF/PE headers, instruction sequences, cross-platform strings) while yielding multi-byte compression per token. On representative uncompressed executables (e.g., ELF/PE/Mach-O rather than compressed APKs), the Binary BPE tokenizers typically allow for roughly 2-3x more binary content per fixed-length transformer context window than raw bytes, enabling more efficient research and practical deployment for content identification, malware detection, reverse engineering, and optimization. We release the trained Binary BPE tokenizers on HuggingFace, providing a drop-in, open-source foundation for binary-focused language models and context-efficient agentic tools.

</details>


### [90] [Efficient Mathematical Reasoning Models via Dynamic Pruning and Knowledge Distillation](https://arxiv.org/abs/2511.17577)
*Fengming Yu,Qingyu Meng,Haiwei Pan,Kejia Zhang*

Main category: cs.LG

TL;DR: 提出一种轻量级优化方法，结合动态注意力头剪枝和知识蒸馏，在保持数学推理能力的同时显著提升大语言模型效率


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数学推理等复杂任务中表现出色，但计算和存储成本高昂，阻碍实际部署

Method: 使用权重范数和熵动态评估多头注意力机制中各注意力头的重要性，实时剪枝冗余头，并通过知识蒸馏将原模型信息传递给学生模型

Result: 在Math23k数据集上，30%剪枝率下参数减少18.7%，推理速度提升27.5%，FLOPs减少19.3%，准确率仅下降0.7%

Conclusion: 该方法在保持强大推理性能的同时实现了显著的效率提升，为大语言模型在数学推理任务中的高效部署提供了实用解决方案

Abstract: With the rapid development of deep learning, large language models have shown strong capabilities in complex reasoning tasks such as mathematical equation solving. However, their substantial computational and storage costs hinder practical deployment. This paper proposes a lightweight optimization method that integrates dynamic attention head pruning with knowledge distillation. The approach dynamically evaluates the importance of each attention head in the multi-head attention mechanism using a combination of weight norms and entropy, and prunes redundant heads in real time to reduce computational overhead. To mitigate performance degradation, knowledge distillation transfers information from the original model to the pruned student, enabling the smaller model to preserve reasoning ability. Experiments conducted on both Math23k and ASDiv-A verify the effectiveness of the proposed method. For example, on Math23k with a 30% pruning ratio, parameters are reduced by 18.7%, inference speed is improved by 27.5%, FLOPs are reduced by 19.3%, and accuracy drops only 0.7% (from 84.4% to 83.7%). These results demonstrate that the method achieves substantial efficiency gains while maintaining strong reasoning performance, providing a practical solution for efficient deployment of large language models in mathematical reasoning tasks.

</details>


### [91] [Multi-Value Alignment for LLMs via Value Decorrelation and Extrapolation](https://arxiv.org/abs/2511.17579)
*Hefei Xu,Le Wu,Chen Cheng,Hao Liu*

Main category: cs.LG

TL;DR: 提出了一个名为MVA的新框架，通过最小化不同人类价值观之间的互信息来缓解参数干扰，并使用价值外推策略探索帕累托前沿，以更好地对齐多个可能冲突的人类价值观。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，将其与人类价值观对齐以确保安全和伦理已成为关键挑战。现有方法如RLHF和DPO在多价值观对齐中存在不稳定、效率低且无法有效处理价值观冲突的问题。

Method: 提出了MVA框架，通过最小化不同人类价值观之间的互信息来减轻参数干扰，并采用价值外推策略高效探索帕累托前沿，构建具有不同价值偏好的LLM集合。

Result: 大量实验表明，MVA在将LLM与多个人类价值观对齐方面持续优于现有基线方法。

Conclusion: MVA框架有效解决了多价值观对齐中的挑战，能够实现更好的权衡，在多个价值观对齐任务中表现优异。

Abstract: With the rapid advancement of large language models (LLMs), aligning them with human values for safety and ethics has become a critical challenge. This problem is especially challenging when multiple, potentially conflicting human values must be considered and balanced. Although several variants of existing alignment methods (such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO)) have been proposed to address multi-value alignment, they suffer from notable limitations: 1) they are often unstable and inefficient in multi-value optimization; and 2) they fail to effectively handle value conflicts. As a result, these approaches typically struggle to achieve optimal trade-offs when aligning multiple values.
  To address this challenge, we propose a novel framework called Multi-Value Alignment (MVA). It mitigates alignment degradation caused by parameter interference among diverse human values by minimizing their mutual information. Furthermore, we propose a value extrapolation strategy to efficiently explore the Pareto frontier, thereby constructing a set of LLMs with diverse value preferences. Extensive experiments demonstrate that MVA consistently outperforms existing baselines in aligning LLMs with multiple human values.

</details>


### [92] [EgoCogNav: Cognition-aware Human Egocentric Navigation](https://arxiv.org/abs/2511.17581)
*Zhiwen Qiu,Ziang Liu,Wenqian Niu,Tapomayukh Bhattacharjee,Saleh Kalantari*

Main category: cs.LG

TL;DR: EgoCogNav是一个多模态自我中心导航框架，通过预测感知路径不确定性作为潜在状态，并融合场景特征与感官线索来联合预测轨迹和头部运动。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注完全观察场景中的运动预测，往往忽略了捕捉人们对空间感受和反应的人类因素。

Method: 提出EgoCogNav框架，预测感知路径不确定性作为潜在状态，通过融合场景特征与感官线索来联合预测轨迹和头部运动。同时构建了CEN数据集，包含6小时真实世界自我中心记录。

Result: 实验表明EgoCogNav学习到的感知不确定性与扫描、犹豫、回溯等类人行为高度相关，并能泛化到未见过的环境。

Conclusion: 该工作通过建模认知和体验因素，深化了对人-环境交互的理解，为安全社交导航和有效辅助寻路提供了支持。

Abstract: Modeling the cognitive and experiential factors of human navigation is central to deepening our understanding of human-environment interaction and to enabling safe social navigation and effective assistive wayfinding. Most existing methods focus on forecasting motions in fully observed scenes and often neglect human factors that capture how people feel and respond to space. To address this gap, We propose EgoCogNav, a multimodal egocentric navigation framework that predicts perceived path uncertainty as a latent state and jointly forecasts trajectories and head motion by fusing scene features with sensory cues. To facilitate research in the field, we introduce the Cognition-aware Egocentric Navigation (CEN) dataset consisting 6 hours of real-world egocentric recordings capturing diverse navigation behaviors in real-world scenarios. Experiments show that EgoCogNav learns the perceived uncertainty that highly correlates with human-like behaviors such as scanning, hesitation, and backtracking while generalizing to unseen environments.

</details>


### [93] [GateRA: Token-Aware Modulation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2511.17582)
*Jie Ou,Shuaihong Jiang,Yingjun Du,Cees G. M. Snoek*

Main category: cs.LG

TL;DR: GateRA是一个参数高效微调框架，通过token感知的调制机制动态调整PEFT更新的强度，实现选择性、token级别的适应，在多个常识推理基准测试中优于现有PEFT方法。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法对所有token应用静态、输入无关的更新，忽视了不同输入的重要性和难度差异，这可能导致在简单内容上过拟合或在信息丰富区域适应不足，特别是在自回归设置中具有不同的预填充和解码动态。

Method: 在标准PEFT分支中引入自适应门控机制，实现token级别的动态适应；使用基于熵的正则化鼓励接近二元的门控决策；理论分析显示GateRA在PEFT路径上诱导软梯度掩码效应。

Result: 在多个常识推理基准测试中，GateRA始终优于或匹配先前的PEFT方法；经验可视化揭示了相位敏感行为，GateRA自动抑制冗余预填充token的更新，同时在解码期间强调适应。

Conclusion: GateRA通过token感知调制实现了更智能的参数高效微调，能够根据输入难度动态调整适应强度，在保持预训练知识的同时专注于具有挑战性的情况。

Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, DoRA, and HiRA, enable lightweight adaptation of large pre-trained models via low-rank updates. However, existing PEFT approaches apply static, input-agnostic updates to all tokens, disregarding the varying importance and difficulty of different inputs. This uniform treatment can lead to overfitting on trivial content or under-adaptation on more informative regions, especially in autoregressive settings with distinct prefill and decoding dynamics. In this paper, we propose GateRA, a unified framework that introduces token-aware modulation to dynamically adjust the strength of PEFT updates. By incorporating adaptive gating into standard PEFT branches, GateRA enables selective, token-level adaptation, preserving pre-trained knowledge for well-modeled inputs while focusing capacity on challenging cases. Empirical visualizations reveal phase-sensitive behaviors, where GateRA automatically suppresses updates for redundant prefill tokens while emphasizing adaptation during decoding. To promote confident and efficient modulation, we further introduce an entropy-based regularization that encourages near-binary gating decisions. This regularization prevents diffuse update patterns and leads to interpretable, sparse adaptation without hard thresholding. Finally, we present a theoretical analysis showing that GateRA induces a soft gradient-masking effect over the PEFT path, enabling continuous and differentiable control over adaptation. Experiments on multiple commonsense reasoning benchmarks demonstrate that GateRA consistently outperforms or matches prior PEFT methods.

</details>


### [94] [Learning Straight Flows: Variational Flow Matching for Efficient Generation](https://arxiv.org/abs/2511.17583)
*Chenrui Ma,Xi Xiao,Tianyang Wang,Xiao Wang,Yanning Shen*

Main category: cs.LG

TL;DR: 提出S-VFM方法，通过引入变分潜在代码来强制轨迹直线化，实现一步生成，在三个基准测试中表现优异且训练推理效率高。


<details>
  <summary>Details</summary>
Motivation: Flow Matching方法由于依赖学习的弯曲轨迹，在一步生成方面能力有限。现有方法存在离散近似误差、训练不稳定和收敛困难等问题。

Method: 将变分潜在代码（表示"生成概览"）集成到Flow Matching框架中，明确强制轨迹直线化，产生线性生成路径。

Result: 在三个挑战性基准测试中取得竞争性性能，相比现有方法在训练和推理效率方面具有优势。

Conclusion: S-VFM方法通过变分潜在代码有效解决了Flow Matching的轨迹弯曲问题，实现了高效的一步生成。

Abstract: Flow Matching has limited ability in achieving one-step generation due to its reliance on learned curved trajectories. Previous studies have attempted to address this limitation by either modifying the coupling distribution to prevent interpolant intersections or introducing consistency and mean-velocity modeling to promote straight trajectory learning. However, these approaches often suffer from discrete approximation errors, training instability, and convergence difficulties. To tackle these issues, in the present work, we propose \textbf{S}traight \textbf{V}ariational \textbf{F}low \textbf{M}atching (\textbf{S-VFM}), which integrates a variational latent code representing the ``generation overview'' into the Flow Matching framework. \textbf{S-VFM} explicitly enforces trajectory straightness, ideally producing linear generation paths. The proposed method achieves competitive performance across three challenge benchmarks and demonstrates advantages in both training and inference efficiency compared with existing methods.

</details>


### [95] [LLM-Powered Text-Attributed Graph Anomaly Detection via Retrieval-Augmented Reasoning](https://arxiv.org/abs/2511.17584)
*Haoyan Xu,Ruizhi Qian,Zhengtao Yao,Ziyi Liu,Li Li,Yuqi Li,Yanshu Li,Wenqing Zheng,Daniele Rosa,Daniel Barcklow,Senthil Kumar,Jieyu Zhao,Yue Zhao*

Main category: cs.LG

TL;DR: 提出了TAG-AD基准数据集，用于文本属性图上的异常节点检测，利用LLM生成真实异常文本，并开发了RAG辅助的零样本LLM异常检测框架。


<details>
  <summary>Details</summary>
Motivation: 文本属性图上的异常检测在欺诈检测、入侵监控等应用中很重要，但由于缺乏标准化基准数据集而研究不足。

Method: 利用LLM在原始文本空间生成语义连贯但上下文不一致的异常节点文本；提出RAG辅助的零样本LLM异常检测框架，构建全局异常知识库。

Result: 实验结果显示LLM在检测上下文异常方面特别有效，而GNN方法在结构异常检测方面更优；RAG辅助提示达到与人工设计提示相当的性能。

Conclusion: LLM和GNN方法在异常检测中各有优势，RAG辅助的零样本LLM框架具有实用价值，无需手动提示工程。

Abstract: Anomaly detection on attributed graphs plays an essential role in applications such as fraud detection, intrusion monitoring, and misinformation analysis. However, text-attributed graphs (TAGs), in which node information is expressed in natural language, remain underexplored, largely due to the absence of standardized benchmark datasets. In this work, we introduce TAG-AD, a comprehensive benchmark for anomaly node detection on TAGs. TAG-AD leverages large language models (LLMs) to generate realistic anomalous node texts directly in the raw text space, producing anomalies that are semantically coherent yet contextually inconsistent and thus more reflective of real-world irregularities. In addition, TAG-AD incorporates multiple other anomaly types, enabling thorough and reproducible evaluation of graph anomaly detection (GAD) methods. With these datasets, we further benchmark existing unsupervised GNN-based GAD methods as well as zero-shot LLMs for GAD.
  As part of our zero-shot detection setup, we propose a retrieval-augmented generation (RAG)-assisted, LLM-based zero-shot anomaly detection framework. The framework mitigates reliance on brittle, hand-crafted prompts by constructing a global anomaly knowledge base and distilling it into reusable analysis frameworks. Our experimental results reveal a clear division of strengths: LLMs are particularly effective at detecting contextual anomalies, whereas GNN-based methods remain superior for structural anomaly detection. Moreover, RAG-assisted prompting achieves performance comparable to human-designed prompts while eliminating manual prompt engineering, underscoring the practical value of our RAG-assisted zero-shot LLM anomaly detection framework.

</details>


### [96] [PaSE: Prototype-aligned Calibration and Shapley-based Equilibrium for Multimodal Sentiment Analysis](https://arxiv.org/abs/2511.17585)
*Kang He,Boyu Chen,Yuzhe Ding,Fei Li,Chong Teng,Donghong Ji*

Main category: cs.LG

TL;DR: PaSE框架通过原型对齐校准和Shapley优化均衡来缓解多模态情感分析中的模态竞争问题，提升跨模态协作性能


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析中，优势模态往往会压制弱势模态，导致模态竞争现象，影响整体性能表现

Method: 提出PaSE框架：1）原型引导校准学习通过熵最优传输机制对齐单模态表征；2）双阶段优化策略，先进行原型门控融合提取共享表征，再通过Shapley梯度调制自适应调整各模态梯度

Result: 在IEMOCAP、MOSI和MOSEI数据集上的实验表明，PaSE取得了优越性能并有效缓解了模态竞争

Conclusion: PaSE框架通过原型对齐和Shapley优化有效解决了多模态情感分析中的模态竞争问题，提升了模型性能

Abstract: Multimodal Sentiment Analysis (MSA) seeks to understand human emotions by integrating textual, acoustic, and visual signals. Although multimodal fusion is designed to leverage cross-modal complementarity, real-world scenarios often exhibit modality competition: dominant modalities tend to overshadow weaker ones, leading to suboptimal performance.In this paper, we propose PaSE, a novel Prototype-aligned Calibration and Shapley-optimized Equilibrium framework, which enhances collaboration while explicitly mitigating modality competition. PaSE first applies Prototype-guided Calibration Learning (PCL) to refine unimodal representations and align them through an Entropic Optimal Transport mechanism that ensures semantic consistency. To further stabilize optimization, we introduce a Dual-Phase Optimization strategy. A prototype-gated fusion module is first used to extract shared representations, followed by Shapley-based Gradient Modulation (SGM), which adaptively adjusts gradients according to the contribution of each modality. Extensive experiments on IEMOCAP, MOSI, and MOSEI confirm that PaSE achieves the superior performance and effectively alleviates modality competition.

</details>


### [97] [Emotion and Intention Guided Multi-Modal Learning for Sticker Response Selection](https://arxiv.org/abs/2511.17587)
*Yuxuan Hu,Jian Chen,Yuhao Wang,Zixuan Li,Jing Xiong,Pengyue Jia,Wei Wang,Chengming Li,Xiangyu Zhao*

Main category: cs.LG

TL;DR: 提出EIGML框架，首次联合建模情绪和意图，通过双层级对比框架和多模态融合模块，显著提升贴纸响应选择的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有贴纸响应选择方法通常依赖语义匹配，并将情绪和意图分开建模，当情绪和意图不一致时容易导致匹配错误。

Method: EIGML框架包含双层级对比框架（模态内和模态间对齐）和意图-情绪引导的多模态融合模块（情绪引导意图知识选择、意图-情绪引导注意力融合、相似度调整匹配机制）。

Result: 在两个公开SRS数据集上的实验表明，EIGML持续优于最先进的基线方法，实现了更高的准确率和更好的情绪意图特征理解。

Conclusion: 联合建模情绪和意图能有效减少孤立建模带来的偏差，EIGML框架通过多模态对齐和融合显著提升了贴纸选择性能。

Abstract: Stickers are widely used in online communication to convey emotions and implicit intentions. The Sticker Response Selection (SRS) task aims to select the most contextually appropriate sticker based on the dialogue. However, existing methods typically rely on semantic matching and model emotional and intentional cues separately, which can lead to mismatches when emotions and intentions are misaligned. To address this issue, we propose Emotion and Intention Guided Multi-Modal Learning (EIGML). This framework is the first to jointly model emotion and intention, effectively reducing the bias caused by isolated modeling and significantly improving selection accuracy. Specifically, we introduce Dual-Level Contrastive Framework to perform both intra-modality and inter-modality alignment, ensuring consistent representation of emotional and intentional features within and across modalities. In addition, we design an Intention-Emotion Guided Multi-Modal Fusion module that integrates emotional and intentional information progressively through three components: Emotion-Guided Intention Knowledge Selection, Intention-Emotion Guided Attention Fusion, and Similarity-Adjusted Matching Mechanism. This design injects rich, effective information into the model and enables a deeper understanding of the dialogue, ultimately enhancing sticker selection performance. Experimental results on two public SRS datasets show that EIGML consistently outperforms state-of-the-art baselines, achieving higher accuracy and a better understanding of emotional and intentional features. Code is provided in the supplementary materials.

</details>


### [98] [Llamazip: Leveraging LLaMA for Lossless Text Compression and Training Dataset Detection](https://arxiv.org/abs/2511.17589)
*Sören Dréano,Derek Molloy,Noel Murphy*

Main category: cs.LG

TL;DR: Llamazip是一种基于LLaMA3语言模型预测能力的无损文本压缩算法，通过仅存储模型无法预测的令牌来实现数据压缩，同时还能识别文档是否属于语言模型训练数据集。


<details>
  <summary>Details</summary>
Motivation: 利用语言模型的预测能力开发高效的无损文本压缩方法，并解决语言模型训练中数据来源、知识产权和透明度等关键问题。

Method: 基于LLaMA3语言模型的预测能力，仅存储模型无法准确预测的令牌，分析量化技术和上下文窗口大小对性能的影响。

Result: 实现了显著的数据压缩率，同时能够识别文档是否属于语言模型的训练数据集。

Conclusion: Llamazip不仅提供了高效的文本压缩解决方案，还为语言模型训练数据的溯源和透明度问题提供了新的解决途径。

Abstract: This work introduces Llamazip, a novel lossless text compression algorithm based on the predictive capabilities of the LLaMA3 language model. Llamazip achieves significant data reduction by only storing tokens that the model fails to predict, optimizing storage efficiency without compromising data integrity. Key factors affecting its performance, including quantization and context window size, are analyzed, revealing their impact on compression ratios and computational requirements. Beyond compression, Llamazip demonstrates the potential to identify whether a document was part of the training dataset of a language model. This capability addresses critical concerns about data provenance, intellectual property, and transparency in language model training.

</details>


### [99] [SHAP Distance: An Explainability-Aware Metric for Evaluating the Semantic Fidelity of Synthetic Tabular Data](https://arxiv.org/abs/2511.17590)
*Ke Yu,Shigeru Ishikura,Yukari Usukura,Yuki Shigoku,Teruaki Hayashi*

Main category: cs.LG

TL;DR: 提出SHAP距离作为评估合成表格数据语义保真度的新指标，通过比较真实数据和合成数据训练的模型的SHAP特征重要性差异来检测语义不一致性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注分布相似性或预测性能，但无法评估合成数据是否保持与真实数据一致的推理模式，存在语义保真度评估的空白。

Method: 引入SHAP距离，定义为从真实与合成数据集训练的分类器得到的全局SHAP归因向量之间的余弦距离。

Result: SHAP距离能可靠识别标准统计和预测指标忽略的语义差异，捕捉特征重要性偏移和尾部效应不足等问题。

Conclusion: SHAP距离是审计合成表格数据语义保真度的实用判别工具，建议将基于归因的评估整合到未来基准测试流程中。

Abstract: Synthetic tabular data, which are widely used in domains such as healthcare, enterprise operations, and customer analytics, are increasingly evaluated to ensure that they preserve both privacy and utility. While existing evaluation practices typically focus on distributional similarity (e.g., the Kullback-Leibler divergence) or predictive performance (e.g., Train-on-Synthetic-Test-on-Real (TSTR) accuracy), these approaches fail to assess semantic fidelity, that is, whether models trained on synthetic data follow reasoning patterns consistent with those trained on real data. To address this gap, we introduce the SHapley Additive exPlanations (SHAP) Distance, a novel explainability-aware metric that is defined as the cosine distance between the global SHAP attribution vectors derived from classifiers trained on real versus synthetic datasets. By analyzing datasets that span clinical health records with physiological features, enterprise invoice transactions with heterogeneous scales, and telecom churn logs with mixed categorical-numerical attributes, we demonstrate that the SHAP Distance reliably identifies semantic discrepancies that are overlooked by standard statistical and predictive measures. In particular, our results show that the SHAP Distance captures feature importance shifts and underrepresented tail effects that the Kullback-Leibler divergence and Train-on-Synthetic-Test-on-Real accuracy fail to detect. This study positions the SHAP Distance as a practical and discriminative tool for auditing the semantic fidelity of synthetic tabular data, and offers practical guidelines for integrating attribution-based evaluation into future benchmarking pipelines.

</details>


### [100] [Comparative Analysis of Large Language Model Inference Serving Systems: A Performance Study of vLLM and HuggingFace TGI](https://arxiv.org/abs/2511.17593)
*Saicharan Kolluru*

Main category: cs.LG

TL;DR: 对vLLM和HuggingFace TGI两个开源LLM服务框架的实证评估，显示vLLM在高并发场景下吞吐量最高可达TGI的24倍，而TGI在交互式单用户场景下延迟更低。


<details>
  <summary>Details</summary>
Motivation: 生产环境中部署大型语言模型需要高效的推理服务系统来平衡吞吐量、延迟和资源利用率。

Method: 使用LLaMA-2模型（7B到70B参数）对vLLM和TGI进行多维度基准测试，包括吞吐量性能、端到端延迟、GPU内存利用率和可扩展性特征。

Result: vLLM通过其新颖的PagedAttention机制在高并发工作负载下比TGI实现高达24倍的吞吐量提升，而TGI在交互式单用户场景下表现出更低的尾部延迟。

Conclusion: 框架选择应基于具体用例需求：vLLM在高吞吐量批处理场景中表现优异，TGI更适合具有中等并发性的延迟敏感型交互应用。

Abstract: The deployment of Large Language Models (LLMs) in production environments requires efficient inference serving systems that balance throughput, latency, and resource utilization. This paper presents a comprehensive empirical evaluation of two prominent open-source LLM serving frameworks: vLLM and HuggingFace Text Generation Inference (TGI). We benchmark these systems across multiple dimensions including throughput performance, end-to-end latency, GPU memory utilization, and scalability characteristics using LLaMA-2 models ranging from 7B to 70B parameters. Our experiments reveal that vLLM achieves up to 24x higher throughput than TGI under high-concurrency workloads through its novel PagedAttention mechanism, while TGI demonstrates lower tail latencies for interactive single-user scenarios. We provide detailed performance profiles for different deployment scenarios and offer practical recommendations for system selection based on workload characteristics. Our findings indicate that the choice between these frameworks should be guided by specific use-case requirements: vLLM excels in high-throughput batch processing scenarios, while TGI is better suited for latency-sensitive interactive applications with moderate concurrency.

</details>


### [101] [AutoSAGE: Input-Aware CUDA Scheduling for Sparse GNN Aggregation (SpMM/SDDMM) and CSR Attention](https://arxiv.org/abs/2511.17594)
*Aleksandar Stankovic*

Main category: cs.LG

TL;DR: AutoSAGE是一个输入感知的CUDA调度器，针对稀疏GNN聚合操作(CSR SpMM/SDDMM)，通过轻量级估计和微探针选择最佳分块和映射策略，在Reddit和OGBN-Products数据集上匹配或超越供应商基准性能。


<details>
  <summary>Details</summary>
Motivation: 稀疏GNN聚合操作(CSR SpMM/SDDMM)的性能在不同度分布、特征宽度和GPU微架构下差异很大，需要自适应调度来优化性能。

Method: AutoSAGE使用轻量级估计和on-device微探针为每个输入选择最佳分块和映射策略，包含安全回退机制和持久缓存，支持SpMM和SDDMM操作并组合成CSR注意力流水线。

Result: 在Reddit和OGBN-Products数据集上，在带宽受限的特征宽度下匹配供应商基准，在小宽度下获得性能提升；在合成稀疏度和偏斜压力测试中实现最高4.7倍的内核级加速。

Conclusion: AutoSAGE提供了有效的输入感知调度方案，显著提升了稀疏GNN聚合操作的性能，并发布了完整的工具链支持可复现性。

Abstract: Sparse GNN aggregations (CSR SpMM/SDDMM) vary widely in performance with degree skew, feature width, and GPU micro-architecture. We present AutoSAGE, an input-aware CUDA scheduler that chooses tiling and mapping per input using a lightweight estimate refined by on-device micro-probes, with a guardrail that safely falls back to vendor kernels and a persistent cache for deterministic replay. AutoSAGE covers SpMM and SDDMM and composes into a CSR attention pipeline (SDDMM -> row-softmax -> SpMM). On Reddit and OGBN-Products, it matches vendor baselines at bandwidth-bound feature widths and finds gains at small widths; on synthetic sparsity and skew stress tests it achieves up to 4.7x kernel-level speedups. We release CUDA sources, Python bindings, a reproducible harness, and replayable cache logs.

</details>


### [102] [Boosting Reinforcement Learning in 3D Visuospatial Tasks Through Human-Informed Curriculum Design](https://arxiv.org/abs/2511.17595)
*Markus D. Solbach,John K. Tsotsos*

Main category: cs.LG

TL;DR: 该论文研究了强化学习在3D Same-Different视觉空间任务中的表现，发现传统方法面临挑战，但通过基于人类实验设计的课程学习取得了成功。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习在更复杂、非结构化问题领域中的智能行为表现，扩展其应用范围，向人工通用智能迈进。

Method: 使用PPO、行为克隆和模仿学习等先进方法，并基于真实人类实验发现设计了课程学习策略。

Result: 传统强化学习方法在直接学习最优策略时遇到困难，但通过精心设计的课程学习实现了有效学习。

Conclusion: 课程学习为强化学习在复杂视觉空间任务中提供了有前景的解决方案，基于人类认知过程的设计策略是成功的关键。

Abstract: Reinforcement Learning is a mature technology, often suggested as a potential route towards Artificial General Intelligence, with the ambitious goal of replicating the wide range of abilities found in natural and artificial intelligence, including the complexities of human cognition. While RL had shown successes in relatively constrained environments, such as the classic Atari games and specific continuous control problems, recent years have seen efforts to expand its applicability. This work investigates the potential of RL in demonstrating intelligent behaviour and its progress in addressing more complex and less structured problem domains.
  We present an investigation into the capacity of modern RL frameworks in addressing a seemingly straightforward 3D Same-Different visuospatial task. While initial applications of state-of-the-art methods, including PPO, behavioural cloning and imitation learning, revealed challenges in directly learning optimal strategies, the successful implementation of curriculum learning offers a promising avenue. Effective learning was achieved by strategically designing the lesson plan based on the findings of a real-world human experiment.

</details>


### [103] [Non-stationary and Varying-discounting Markov Decision Processes for Reinforcement Learning](https://arxiv.org/abs/2511.17598)
*Zhizuo Chen,Theodore T. Allen*

Main category: cs.LG

TL;DR: 提出了非平稳和变折扣MDP（NVMDP）框架，解决传统MDP在非平稳环境和有限时域任务中的局限性，支持折扣率随时间变化，能够在不改变状态空间、动作空间或奖励结构的情况下塑造最优策略。


<details>
  <summary>Details</summary>
Motivation: 传统MDP算法在非平稳环境中面临挑战，无限时域公式不能直接应用于有限时域任务，需要更灵活的框架来处理非平稳性和变折扣率问题。

Method: 建立NVMDP理论框架，包括假设、状态和动作值公式与递归、矩阵表示、最优性条件；扩展动态规划和广义Q学习算法；针对函数逼近问题扩展策略梯度定理和TRPO策略改进边界。

Result: 在非平稳网格世界环境中的实证评估表明，基于NVMDP的算法成功恢复了多种奖励和折扣方案下的最优轨迹，而原始Q学习失败。

Conclusion: NVMDP提供了一个理论严谨且实际有效的强化学习框架，只需少量算法修改即可稳健处理非平稳性和显式最优策略塑造。

Abstract: Algorithms developed under stationary Markov Decision Processes (MDPs) often face challenges in non-stationary environments, and infinite-horizon formulations may not directly apply to finite-horizon tasks. To address these limitations, we introduce the Non-stationary and Varying-discounting MDP (NVMDP) framework, which naturally accommodates non-stationarity and allows discount rates to vary with time and transitions. Infinite-horizon, stationary MDPs emerge as special cases of NVMDPs for identifying an optimal policy, and finite-horizon MDPs are also subsumed within the NVMDP formulations. Moreover, NVMDPs provide a flexible mechanism to shape optimal policies, without altering the state space, action space, or the reward structure. We establish the theoretical foundations of NVMDPs, including assumptions, state- and action-value formulation and recursion, matrix representation, optimality conditions, and policy improvement under finite state and action spaces. Building on these results, we adapt dynamic programming and generalized Q-learning algorithms to NVMDPs, along with formal convergence proofs. For problems requiring function approximation, we extend the Policy Gradient Theorem and the policy improvement bound in Trust Region Policy Optimization (TRPO), offering proofs in both scalar and matrix forms. Empirical evaluations in a non-stationary gridworld environment demonstrate that NVMDP-based algorithms successfully recover optimal trajectories under multiple reward and discounting schemes, whereas original Q-learning fails. These results collectively show that NVMDPs provide a theoretically sound and practically effective framework for reinforcement learning, requiring only minor algorithmic modifications while enabling robust handling of non-stationarity and explicit optimal policy shaping.

</details>


### [104] [From Projection to Prediction: Beyond Logits for Scalable Language Models](https://arxiv.org/abs/2511.17599)
*Jianbing Dong,Jianbin Chang*

Main category: cs.LG

TL;DR: 提出了一种将输出投影和损失预测集成到单一操作中的新方法，避免了显式logits张量化，减少内存使用和带宽压力，提升LLM训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统的LLM训练在输出层采用两阶段流水线，需要完全物化中间logits张量，导致显著的内存开销和带宽消耗，限制了可扩展性和训练吞吐量。

Method: 通过直接从隐藏状态和目标标记计算损失，绕过了显式logits物化，将输出投影和损失预测集成到单一操作中。

Result: 实验表明该方法相比标准两阶段流水线实现了显著的内存节省和可测量的加速，支持更大的批处理大小和更长序列而不牺牲准确性。

Conclusion: 重新思考投影和预测之间的边界具有实际系统优化价值，为高效LLM训练提供了实用方案。

Abstract: Training Large Language Models (LLMs) typically involves a two-stage pipeline at the output layer: hidden states are projected into vocabulary logits via a linear transformation (lm_head), followed by cross-entropy loss computation against target tokens. While conceptually simple, this design incurs substantial overhead. The intermediate logits tensor, with dimensions proportional to batch size, sequence length, and vocabulary size, must be fully materialized in GPU memory, even though only one target token per position is ultimately used. This leads to significant memory footprint and bandwidth comsumption, limiting scalability and slowing training throughput.
  In this work, we introduce a novel approach to integrates the output projection and loss prediction into a single operation. By directly computing the loss from hidden states and target tokens, our approach bypasses explicit logits materialization. This design reduces memory usage and alleviates bandwidth pressure. Experiments on LLM training demonstrate that our method achieves substantial memory savings and measurable speedups compared to the standard two-stage pipeline, enabling large batch sizes and longer sequences without sacrificing accuracy. Our work highlights the benefits of rethinking the boundary between projection and prediction, offering a practical systems optimization for efficient LLM training.

</details>


### [105] [Generalizable and Efficient Automated Scoring with a Knowledge-Distilled Multi-Task Mixture-of-Experts](https://arxiv.org/abs/2511.17601)
*Luyang Fang,Tao Wang,Ping Ma,Xiaoming Zhai*

Main category: cs.LG

TL;DR: 提出UniMoE-Guided方法，通过知识蒸馏将多个任务特定大模型的专业知识转移到单个紧凑可部署模型中，实现高效的多任务自动评分


<details>
  <summary>Details</summary>
Motivation: 解决自动评分中每个任务需要单独模型导致的计算资源、存储和维护成本过高的问题

Method: 使用知识蒸馏的多任务混合专家方法，包含共享编码器、门控MoE块和轻量级任务头，结合真实标签和教师指导进行训练

Result: 在9个科学推理任务上，性能与任务特定模型相当，存储需求比单独学生模型减少约6倍，比200亿参数教师减少87倍

Conclusion: 该方法为课堂和大规模评估系统提供了可扩展、可靠且资源高效的自动评分实用路径

Abstract: Automated scoring of written constructed responses typically relies on separate models per task, straining computational resources, storage, and maintenance in real-world education settings. We propose UniMoE-Guided, a knowledge-distilled multi-task Mixture-of-Experts (MoE) approach that transfers expertise from multiple task-specific large models (teachers) into a single compact, deployable model (student). The student combines (i) a shared encoder for cross-task representations, (ii) a gated MoE block that balances shared and task-specific processing, and (iii) lightweight task heads. Trained with both ground-truth labels and teacher guidance, the student matches strong task-specific models while being far more efficient to train, store, and deploy. Beyond efficiency, the MoE layer improves transfer and generalization: experts develop reusable skills that boost cross-task performance and enable rapid adaptation to new tasks with minimal additions and tuning. On nine NGSS-aligned science-reasoning tasks (seven for training/evaluation and two held out for adaptation), UniMoE-Guided attains performance comparable to per-task models while using $\sim$6$\times$ less storage than maintaining separate students, and $87\times$ less than the 20B-parameter teacher. The method offers a practical path toward scalable, reliable, and resource-efficient automated scoring for classroom and large-scale assessment systems.

</details>


### [106] [Beyond Surface-Level Similarity: Hierarchical Contamination Detection for Synthetic Training Data in Foundation Models](https://arxiv.org/abs/2511.17602)
*Sushant Mehta*

Main category: cs.LG

TL;DR: 提出分层污染检测框架，在四个层级检测合成数据对基准测试的污染：词元级、语义级、推理模式和性能悬崖检测。实验表明该方法能有效检测语义级污染，F1分数达0.76，比现有方法平均提升26.5%。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法只能识别词元级重叠，无法检测语义级污染（合成数据与基准测试概念相似但无词汇重叠），这对基础模型训练中日益增多的合成数据使用构成威胁。

Method: 分层污染检测框架，包含四个检测层级：词元级、语义级、推理模式和性能悬崖检测。在MMLU、GSM8K和HumanEval上进行受控实验验证。

Result: 语义级污染能逃避现有检测方法（F1=0.17-0.49），但分层方法能有效检测（F1=0.76），比最先进基线平均提升26.5%。

Conclusion: 该框架为从业者提供了实用的审计工具，支持合成训练数据的负责任部署。

Abstract: Synthetic data has become essential for training foundation models, yet benchmark contamination threatens evaluation integrity. Although existing detection methods identify token-level overlap, they fail to detect semantic-level contamination where synthetic data conceptually resemble benchmarks without lexical overlap. This gap is critical as foundation models increasingly train on synthetic data that may implicitly encode benchmark knowledge. We propose a hierarchical contamination detection framework operating at four levels: token level, semantic level, reasoning pattern, and performance cliff detection. Through controlled experiments on MMLU, GSM8K and HumanEval, we demonstrate that semantic-level contamination evades existing methods (F1=0.17-0.49) but is effectively detected by our hierarchical approach (F1 = 0.76), with an average improvement of 26. 5\% over state-of-the-art baselines. Our framework provides practitioners with practical tools for audit pipelines and enables responsible deployment of synthetic training data.

</details>


### [107] [BrainHGT: A Hierarchical Graph Transformer for Interpretable Brain Network Analysis](https://arxiv.org/abs/2511.17604)
*Jiajun Ma,Yongchao Zhang,Chao Zhang,Zhao Lv,Shengbing Pei*

Main category: cs.LG

TL;DR: 提出了BrainHGT，一种分层图Transformer，模拟大脑从局部区域到全局社区的自然信息处理过程，通过长短程注意力编码器和先验引导聚类模块来捕捉大脑的模块化结构。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将大脑建模为扁平网络，忽略了其模块化结构，且注意力机制将所有大脑区域连接同等对待，忽略了与距离相关的节点连接模式。大脑信息处理是一个涉及局部和长程交互的分层过程。

Method: 设计了新颖的长短程注意力编码器，使用并行路径处理密集局部交互和稀疏长程连接；设计了先验引导聚类模块，利用交叉注意力机制将大脑区域分组为功能社区，并利用神经解剖学先验指导聚类过程。

Result: 实验结果表明，所提方法显著提高了疾病识别的性能，并能可靠地捕捉大脑的子功能模块，展示了其可解释性。

Conclusion: BrainHGT通过分层图Transformer有效模拟了大脑的自然信息处理机制，在疾病识别和可解释性方面表现出色。

Abstract: Graph Transformer shows remarkable potential in brain network analysis due to its ability to model graph structures and complex node relationships. Most existing methods typically model the brain as a flat network, ignoring its modular structure, and their attention mechanisms treat all brain region connections equally, ignoring distance-related node connection patterns. However, brain information processing is a hierarchical process that involves local and long-range interactions between brain regions, interactions between regions and sub-functional modules, and interactions among functional modules themselves. This hierarchical interaction mechanism enables the brain to efficiently integrate local computations and global information flow, supporting the execution of complex cognitive functions. To address this issue, we propose BrainHGT, a hierarchical Graph Transformer that simulates the brain's natural information processing from local regions to global communities. Specifically, we design a novel long-short range attention encoder that utilizes parallel pathways to handle dense local interactions and sparse long-range connections, thereby effectively alleviating the over-globalizing issue. To further capture the brain's modular architecture, we designe a prior-guided clustering module that utilizes a cross-attention mechanism to group brain regions into functional communities and leverage neuroanatomical prior to guide the clustering process, thereby improving the biological plausibility and interpretability. Experimental results indicate that our proposed method significantly improves performance of disease identification, and can reliably capture the sub-functional modules of the brain, demonstrating its interpretability.

</details>


### [108] [Copula Based Fusion of Clinical and Genomic Machine Learning Risk Scores for Breast Cancer Risk Stratification](https://arxiv.org/abs/2511.17605)
*Agnideep Aich,Sameera Hewage,Md Monzur Murshed*

Main category: cs.LG

TL;DR: 使用copula方法直接建模临床和基因组机器学习风险评分的联合关系，可以改善乳腺癌5年癌症特异性死亡率的风险分层。


<details>
  <summary>Details</summary>
Motivation: 临床和基因组模型通常使用简单的线性规则组合，没有考虑它们在极端情况下的风险评分关系，需要更准确地建模它们的联合关系来改善风险分层。

Method: 使用METABRIC乳腺癌队列，训练随机森林和XGBoost等分类器，将预测概率转换为伪观测值，然后拟合高斯、Clayton和Gumbel copula来建模临床和基因组风险评分的联合分布。

Result: 高斯copula最能捕捉联合分布关系（bootstrap p=0.997），显示对称的中等强度正相关。基于这种关系的患者分组显示，临床和基因组评分均为高风险的患者生存率显著差于仅在一个方面高风险的患者。

Conclusion: copula-based融合方法在实际队列中有效，考虑评分间的依赖关系可以更好地识别预后最差的患者亚组。

Abstract: Clinical and genomic models are both used to predict breast cancer outcomes, but they are often combined using simple linear rules that do not account for how their risk scores relate, especially at the extremes. Using the METABRIC breast cancer cohort, we studied whether directly modeling the joint relationship between clinical and genomic machine learning risk scores could improve risk stratification for 5-year cancer-specific mortality. We created a binary 5-year cancer-death outcome and defined two sets of predictors: a clinical set (demographic, tumor, and treatment variables) and a genomic set (gene-expression $z$-scores). We trained several supervised classifiers, such as Random Forest and XGBoost, and used 5-fold cross-validated predicted probabilities as unbiased risk scores. These scores were converted to pseudo-observations on $(0,1)^2$ to fit Gaussian, Clayton, and Gumbel copulas. Clinical models showed good discrimination (AUC 0.783), while genomic models had moderate performance (AUC 0.681). The joint distribution was best captured by a Gaussian copula (bootstrap $p=0.997$), which suggests a symmetric, moderately strong positive relationship. When we grouped patients based on this relationship, Kaplan-Meier curves showed clear differences: patients who were high-risk in both clinical and genomic scores had much poorer survival than those high-risk in only one set. These results show that copula-based fusion works in real-world cohorts and that considering dependencies between scores can better identify patient subgroups with the worst prognosis.

</details>


### [109] [Energy-based Autoregressive Generation for Neural Population Dynamics](https://arxiv.org/abs/2511.17606)
*Ningling Ge,Sicheng Dai,Yu Zhu,Shan Yu*

Main category: cs.LG

TL;DR: 提出基于能量的自回归生成框架，在潜在空间中学习神经群体动态，实现高效生成并保持真实神经发放统计特性，在神经科学和神经工程中有应用价值。


<details>
  <summary>Details</summary>
Motivation: 解决计算建模中计算效率与高保真度之间的权衡问题，为神经科学研究和神经工程应用提供更有效的建模方法。

Method: 基于能量的自回归生成框架，使用基于能量的transformer通过严格适当评分规则在潜在空间中学习时间动态。

Result: 在合成Lorenz数据集和两个神经潜在基准数据集上达到最先进的生成质量，计算效率显著提升，特别是在条件生成应用中展示了对未见行为上下文的泛化能力和改善脑机接口解码精度的能力。

Conclusion: 基于能量的建模对神经群体动态建模有效，在神经科学研究和神经工程中具有应用前景。

Abstract: Understanding brain function represents a fundamental goal in neuroscience, with critical implications for therapeutic interventions and neural engineering applications. Computational modeling provides a quantitative framework for accelerating this understanding, but faces a fundamental trade-off between computational efficiency and high-fidelity modeling. To address this limitation, we introduce a novel Energy-based Autoregressive Generation (EAG) framework that employs an energy-based transformer learning temporal dynamics in latent space through strictly proper scoring rules, enabling efficient generation with realistic population and single-neuron spiking statistics. Evaluation on synthetic Lorenz datasets and two Neural Latents Benchmark datasets (MC_Maze and Area2_bump) demonstrates that EAG achieves state-of-the-art generation quality with substantial computational efficiency improvements, particularly over diffusion-based methods. Beyond optimal performance, conditional generation applications show two capabilities: generalizing to unseen behavioral contexts and improving motor brain-computer interface decoding accuracy using synthetic neural data. These results demonstrate the effectiveness of energy-based modeling for neural population dynamics with applications in neuroscience research and neural engineering. Code is available at https://github.com/NinglingGe/Energy-based-Autoregressive-Generation-for-Neural-Population-Dynamics.

</details>


### [110] [Finding Pre-Injury Patterns in Triathletes from Lifestyle, Recovery and Load Dynamics Features](https://arxiv.org/abs/2511.17610)
*Leonardo Rossi,Bruno Rodrigues*

Main category: cs.LG

TL;DR: 提出了一个专门为铁人三项设计的合成数据生成框架，该框架能够生成生理上合理的运动员档案，模拟个性化训练计划，并整合睡眠质量、压力水平和恢复状态等日常生活因素。通过机器学习模型实现了高达0.86 AUC的预测性能，识别出睡眠障碍、心率变异性和压力作为损伤风险的关键早期指标。


<details>
  <summary>Details</summary>
Motivation: 铁人三项训练由于高强度的游泳、骑行和跑步活动，使运动员面临重复生理压力导致的过度使用损伤风险。现有的损伤预测方法主要依赖训练负荷指标，往往忽略了睡眠质量、压力和个体生活方式等显著影响恢复和损伤易感性的关键因素。

Method: 开发了一个新颖的合成数据生成框架，专门针对铁人三项设计。该框架生成生理上合理的运动员档案，模拟包含周期化和负荷管理原则的个性化训练计划，并整合睡眠质量、压力水平和恢复状态等日常生活因素。使用LASSO、随机森林和XGBoost等机器学习模型进行评估。

Result: 机器学习模型表现出高预测性能（AUC高达0.86），识别出睡眠障碍、心率变异性和压力作为损伤风险的关键早期指标。这种可穿戴设备驱动的方法不仅提高了损伤预测的准确性，还为克服现实世界数据限制提供了实用解决方案。

Conclusion: 该研究提供了一种全面的、情境感知的运动员监测途径，通过整合训练负荷和日常生活因素，实现了更准确的损伤风险预测，为运动员健康管理提供了新的技术支撑。

Abstract: Triathlon training, which involves high-volume swimming, cycling, and running, places athletes at substantial risk for overuse injuries due to repetitive physiological stress. Current injury prediction approaches primarily rely on training load metrics, often neglecting critical factors such as sleep quality, stress, and individual lifestyle patterns that significantly influence recovery and injury susceptibility.
  We introduce a novel synthetic data generation framework tailored explicitly for triathlon. This framework generates physiologically plausible athlete profiles, simulates individualized training programs that incorporate periodization and load-management principles, and integrates daily-life factors such as sleep quality, stress levels, and recovery states. We evaluated machine learning models (LASSO, Random Forest, and XGBoost) showing high predictive performance (AUC up to 0.86), identifying sleep disturbances, heart rate variability, and stress as critical early indicators of injury risk. This wearable-driven approach not only enhances injury prediction accuracy but also provides a practical solution to overcoming real-world data limitations, offering a pathway toward a holistic, context-aware athlete monitoring.

</details>


### [111] [AI-driven Generation of MALDI-TOF MS for Microbial Characterization](https://arxiv.org/abs/2511.17611)
*Lucía Schmidt-Santiago,David Rodríguez-Temporal,Carlos Sevilla-Salcedo,Vanessa Gómez-Verdejo*

Main category: cs.LG

TL;DR: 该研究使用深度生成模型（MALDIVAE、MALDIGAN、MALDIffusion）合成MALDI-TOF质谱数据，解决微生物学中数据稀缺问题，支持机器学习工具开发。


<details>
  <summary>Details</summary>
Motivation: MALDI-TOF MS在临床微生物学中应用广泛，但数据驱动的诊断模型发展受限于缺乏足够大、平衡和标准化的光谱数据集。

Method: 采用三种条件生成模型：变分自编码器、生成对抗网络和去噪扩散概率模型，以物种标签为条件生成微生物光谱，并使用多种指标评估光谱保真度和多样性。

Result: 合成数据在统计和诊断上与真实测量相当，仅使用合成样本训练的分类器能达到与真实数据训练相似的性能。MALDIVAE在真实性、稳定性和效率之间达到最佳平衡。

Conclusion: 合成光谱能有效缓解类别不平衡和领域不匹配问题，显著提高分类准确性，同时不损害生成数据的真实性。

Abstract: Matrix-Assisted Laser Desorption/Ionization Time-of-Flight Mass Spectrometry (MALDI-TOF MS) has become a cornerstone technology in clinical microbiology, enabling rapid and accurate microbial identification. However, the development of data-driven diagnostic models remains limited by the lack of sufficiently large, balanced, and standardized spectral datasets. This study investigates the use of deep generative models to synthesize realistic MALDI-TOF MS spectra, aiming to overcome data scarcity and support the development of robust machine learning tools in microbiology.
  We adapt and evaluate three generative models, Variational Autoencoders (MALDIVAEs), Generative Adversarial Networks (MALDIGANs), and Denoising Diffusion Probabilistic Model (MALDIffusion), for the conditional generation of microbial spectra guided by species labels. Generation is conditioned on species labels, and spectral fidelity and diversity are assessed using diverse metrics.
  Our experiments show that synthetic data generated by MALDIVAE, MALDIGAN, and MALDIffusion are statistically and diagnostically comparable to real measurements, enabling classifiers trained exclusively on synthetic samples to reach performance levels similar to those trained on real data. While all models faithfully reproduce the peak structure and variability of MALDI-TOF spectra, MALDIffusion obtains this fidelity at a substantially higher computational cost, and MALDIGAN shows competitive but slightly less stable behaviour. In contrast, MALDIVAE offers the most favorable balance between realism, stability, and efficiency. Furthermore, augmenting minority species with synthetic spectra markedly improves classification accuracy, effectively mitigating class imbalance and domain mismatch without compromising the authenticity of the generated data.

</details>


### [112] [Tensor Gauge Flow Models](https://arxiv.org/abs/2511.17616)
*Alexander Strunk,Roland Assam*

Main category: cs.LG

TL;DR: 提出了Tensor Gauge Flow Models，通过在流方程中引入高阶张量规范场，扩展了Gauge Flow Models和Higher Gauge Flow Models，增强了数据几何和规范理论结构的表达能力。


<details>
  <summary>Details</summary>
Motivation: 现有生成流模型在表达复杂数据几何结构和规范理论结构方面存在局限，需要更丰富的流动力学表达能力。

Method: 在流方程中引入高阶张量规范场，构建Tensor Gauge Flow Models，扩展了传统规范流模型和高阶规范流模型。

Result: 在高斯混合模型上的实验表明，Tensor Gauge Flow Models相比标准和规范流基线模型获得了更好的生成性能。

Conclusion: Tensor Gauge Flow Models通过引入高阶张量规范场，能够编码更丰富的几何和规范理论结构，提升生成流模型的表达能力。

Abstract: This paper introduces Tensor Gauge Flow Models, a new class of Generative Flow Models that generalize Gauge Flow Models and Higher Gauge Flow Models by incorporating higher-order Tensor Gauge Fields into the Flow Equation. This extension allows the model to encode richer geometric and gauge-theoretic structure in the data, leading to more expressive flow dynamics. Experiments on Gaussian mixture models show that Tensor Gauge Flow Models achieve improved generative performance compared to both standard and gauge flow baselines.

</details>


### [113] [Neural surrogates for designing gravitational wave detectors](https://arxiv.org/abs/2511.19364)
*Carlos Ruiz-Gonzalez,Sören Arlt,Sebastian Lehner,Arturs Berzins,Yehonathan Drori,Rana X Adhikari,Johannes Brandstetter,Mario Krenn*

Main category: cs.LG

TL;DR: 提出了一种使用神经代理模型替代传统物理模拟器的方法，通过结合训练、逆向设计和模拟器验证的循环流程，显著加速复杂系统的实验设计优化过程。


<details>
  <summary>Details</summary>
Motivation: 随着实验装置复杂度增加，传统CPU模拟器的计算成本成为主要限制因素，需要找到既能保持精度又能大幅减少对慢速模拟器依赖的方法。

Method: 训练神经网络作为物理模拟器的代理模型，采用包含训练代理模型、逆向设计新实验、用慢速模拟器验证特性以进一步训练的循环算法，利用自动微分和GPU并行加速。

Result: 模型能快速预测候选设计的质量和可行性，高效探索大型设计空间。算法在几小时内找到的解决方案优于优化器运行五天得到的设计。

Conclusion: 该方法虽然以引力波探测器为背景展示，但可广泛应用于模拟器瓶颈阻碍优化和发现的其他领域。

Abstract: Physics simulators are essential in science and engineering, enabling the analysis, control, and design of complex systems. In experimental sciences, they are increasingly used to automate experimental design, often via combinatorial search and optimization. However, as the setups grow more complex, the computational cost of traditional, CPU-based simulators becomes a major limitation. Here, we show how neural surrogate models can significantly reduce reliance on such slow simulators while preserving accuracy. Taking the design of interferometric gravitational wave detectors as a representative example, we train a neural network to surrogate the gravitational wave physics simulator Finesse, which was developed by the LIGO community. Despite that small changes in physical parameters can change the output by orders of magnitudes, the model rapidly predicts the quality and feasibility of candidate designs, allowing an efficient exploration of large design spaces. Our algorithm loops between training the surrogate, inverse designing new experiments, and verifying their properties with the slow simulator for further training. Assisted by auto-differentiation and GPU parallelism, our method proposes high-quality experiments much faster than direct optimization. Solutions that our algorithm finds within hours outperform designs that take five days for the optimizer to reach. Though shown in the context of gravitational wave detectors, our framework is broadly applicable to other domains where simulator bottlenecks hinder optimization and discovery.

</details>


### [114] [Neurocircuitry-Inspired Hierarchical Graph Causal Attention Networks for Explainable Depression Identification](https://arxiv.org/abs/2511.17622)
*Weidao Chen,Yuxiao Yang,Yueming Wang*

Main category: cs.LG

TL;DR: NH-GCAT是一个神经科学启发的分层图因果注意力网络，通过在不同空间尺度上显式建模抑郁症特异性机制，将神经科学领域知识与深度学习相结合，在抑郁症分类中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经影像数据的图神经网络方法主要是数据驱动的黑盒模型，缺乏神经生物学可解释性，无法揭示抑郁症的复杂病理生理机制。

Method: 提出三个关键技术贡献：1)局部脑区级的残差门控融合模块，整合BOLD动态与功能连接模式；2)多区域回路级的分层回路编码方案；3)多回路网络级的变分潜在因果注意力机制。

Result: 在REST-meta-MDD数据集上的留一站点交叉验证显示，NH-GCAT在抑郁症分类中达到样本量加权平均准确率73.3%和AUROC 76.4%。

Conclusion: NH-GCAT不仅实现了最先进的抑郁症分类性能，同时提供了神经生物学上有意义的解释，成功将神经科学领域知识与深度学习框架相结合。

Abstract: Major Depressive Disorder (MDD), affecting millions worldwide, exhibits complex pathophysiology manifested through disrupted brain network dynamics. Although graph neural networks that leverage neuroimaging data have shown promise in depression diagnosis, existing approaches are predominantly data-driven and operate largely as black-box models, lacking neurobiological interpretability. Here, we present NH-GCAT (Neurocircuitry-Inspired Hierarchical Graph Causal Attention Networks), a novel framework that bridges neuroscience domain knowledge with deep learning by explicitly and hierarchically modeling depression-specific mechanisms at different spatial scales. Our approach introduces three key technical contributions: (1) at the local brain regional level, we design a residual gated fusion module that integrates temporal blood oxygenation level dependent (BOLD) dynamics with functional connectivity patterns, specifically engineered to capture local depression-relevant low-frequency neural oscillations; (2) at the multi-regional circuit level, we propose a hierarchical circuit encoding scheme that aggregates regional node representations following established depression neurocircuitry organization, and (3) at the multi-circuit network level, we develop a variational latent causal attention mechanism that leverages a continuous probabilistic latent space to infer directed information flow among critical circuits, characterizing disease-altered whole-brain inter-circuit interactions. Rigorous leave-one-site-out cross-validation on the REST-meta-MDD dataset demonstrates NH-GCAT's state-of-the-art performance in depression classification, achieving a sample-size weighted-average accuracy of 73.3\% and an AUROC of 76.4\%, while simultaneously providing neurobiologically meaningful explanations.

</details>


### [115] [M$^2$OE$^2$-GL: A Family of Probabilistic Load Forecasters That Scales to Massive Customers](https://arxiv.org/abs/2511.17623)
*Haoran Li,Zhe Cheng,Muhao Guo,Yang Weng,Yannan Sun,Victor Tran,John Chainaranont*

Main category: cs.LG

TL;DR: 提出了M2OE2-GL方法，通过全局预训练和轻量级微调解决大规模配电系统中概率负荷预测的异构性和可扩展性挑战。


<details>
  <summary>Details</summary>
Motivation: 在大规模配电系统中，为每个客户单独训练模型计算和存储成本高，而单一全局模型无法处理不同客户类型、位置和相位间的分布偏移。现有方法很少同时解决异构性和可扩展性问题。

Method: 首先在所有馈线负荷上预训练单个全局M2OE2基础模型，然后应用轻量级微调来推导紧凑的组特定预测器家族。

Result: 在真实电力公司数据上的评估显示，M2OE2-GL实现了显著的误差减少，同时保持了对大量负荷的可扩展性。

Conclusion: M2OE2-GL方法有效平衡了预测精度和计算效率，为大规模配电系统中的概率负荷预测提供了实用解决方案。

Abstract: Probabilistic load forecasting is widely studied and underpins power system planning, operation, and risk-aware decision making. Deep learning forecasters have shown strong ability to capture complex temporal and contextual patterns, achieving substantial accuracy gains. However, at the scale of thousands or even hundreds of thousands of loads in large distribution feeders, a deployment dilemma emerges: training and maintaining one model per customer is computationally and storage intensive, while using a single global model ignores distributional shifts across customer types, locations, and phases. Prior work typically focuses on single-load forecasters, global models across multiple loads, or adaptive/personalized models for relatively small settings, and rarely addresses the combined challenges of heterogeneity and scalability in large feeders. We propose M2OE2-GL, a global-to-local extension of the M2OE2 probabilistic forecaster. We first pretrain a single global M2OE2 base model across all feeder loads, then apply lightweight fine-tuning to derive a compact family of group-specific forecasters. Evaluated on realistic utility data, M2OE2-GL yields substantial error reductions while remaining scalable to very large numbers of loads.

</details>


### [116] [QML-HCS: A Hypercausal Quantum Machine Learning Framework for Non-Stationary Environments](https://arxiv.org/abs/2511.17624)
*Hector E Mozo*

Main category: cs.LG

TL;DR: QML-HCS是一个量子启发机器学习框架，通过超因果反馈动态实现非平稳环境下的自适应学习


<details>
  <summary>Details</summary>
Motivation: 解决传统机器学习和量子启发系统在非平稳环境中数据分布漂移、缺乏连续适应机制和因果稳定性等问题

Method: 集成量子启发叠加原理、动态因果反馈和确定性-随机混合执行的统一计算架构，实现可逆变换、多路径因果传播和漂移下状态评估

Result: 开发了具有超因果处理核心的框架，提供可复现的Python接口，通过最小化模拟展示了模型在输入分布突变时的自适应能力

Conclusion: 该框架为量子启发学习、因果推理和混合计算建立了基础架构，支持未来理论扩展和与经典/量子模拟平台的集成

Abstract: QML-HCS is a research-grade framework for constructing and analyzing quantum-inspired machine learning models operating under hypercausal feedback dynamics. Hypercausal refers to AI systems that leverage extended, deep, or nonlinear causal relationships (expanded causality) to reason, predict, and infer states beyond the capabilities of traditional causal models. Current machine learning and quantum-inspired systems struggle in non-stationary environments, where data distributions drift and models lack mechanisms for continuous adaptation, causal stability, and coherent state updating. QML-HCS addresses this limitation through a unified computational architecture that integrates quantum-inspired superposition principles, dynamic causal feedback, and deterministic-stochastic hybrid execution to enable adaptive behavior in changing environments.
  The framework implements a hypercausal processing core capable of reversible transformations, multipath causal propagation, and evaluation of alternative states under drift. Its architecture incorporates continuous feedback to preserve causal consistency and adjust model behavior without requiring full retraining. QML-HCS provides a reproducible and extensible Python interface backed by efficient computational routines, enabling experimentation in quantum-inspired learning, causal reasoning, and hybrid computation without the need for specialized hardware.
  A minimal simulation demonstrates how a hypercausal model adapts to a sudden shift in the input distribution while preserving internal coherence. This initial release establishes the foundational architecture for future theoretical extensions, benchmarking studies, and integration with classical and quantum simulation platforms.

</details>


### [117] [Efficient Large-Scale Learning of Minimax Risk Classifiers](https://arxiv.org/abs/2511.17626)
*Kartheek Bondugula,Santiago Mazuelas,Aritz Pérez*

Main category: cs.LG

TL;DR: 提出了一种基于约束生成和列生成的组合学习算法，用于高效学习大规模多类分类任务中的极小极大风险分类器。


<details>
  <summary>Details</summary>
Motivation: 传统的随机次梯度方法无法有效处理极小极大风险分类器的学习问题，这类分类器最小化最大期望损失而非平均损失，需要新的高效学习算法。

Method: 结合约束生成和列生成技术，开发了一种能够处理大规模多类分类任务的学习算法。

Result: 在多个基准数据集上的实验表明，该算法对一般大规模数据提供高达10倍的加速，对于类别数量较多的情况提供约100倍的加速。

Conclusion: 所提出的算法能够高效学习极小极大风险分类器，显著提升了大规模多类分类任务的学习效率。

Abstract: Supervised learning with large-scale data usually leads to complex optimization problems, especially for classification tasks with multiple classes. Stochastic subgradient methods can enable efficient learning with a large number of samples for classification techniques that minimize the average loss over the training samples. However, recent techniques, such as minimax risk classifiers (MRCs), minimize the maximum expected loss and are not amenable to stochastic subgradient methods. In this paper, we present a learning algorithm based on the combination of constraint and column generation that enables efficient learning of MRCs with large-scale data for classification tasks with multiple classes. Experiments on multiple benchmark datasets show that the proposed algorithm provides upto a 10x speedup for general large-scale data and around a 100x speedup with a sizeable number of classes.

</details>


### [118] [Rectifying Mean-Shift in Cascaded Precipitation Nowcasting](https://arxiv.org/abs/2511.17628)
*Fanbo Ju,Haiyuan Shi,Qingjian Ni*

Main category: cs.LG

TL;DR: RectiCast是一个两阶段降水临近预报框架，通过双流匹配模型显式解耦均值场偏移校正和局部随机性生成，解决了现有方法中确定性预测的系统性分布偏移与局部随机性混淆的问题。


<details>
  <summary>Details</summary>
Motivation: 现有级联架构的降水临近预报方法通常忽视了确定性预测中的系统性分布偏移与局部随机性的混淆问题，导致概率组件的预测受到污染，特别是在较长预报时效上出现降水模式和强度的不准确。

Method: 提出两阶段框架：第一阶段使用确定性模型生成后验均值；第二阶段引入校正器显式学习分布偏移并生成校正后的均值，然后生成器在修正均值条件下建模局部随机性。

Result: 在SEVIR和MeteoNet数据集上的实验表明，RectiCast相比现有最先进方法取得了显著的性能提升。

Conclusion: RectiCast通过显式解耦均值场偏移校正和局部随机性生成，有效解决了级联架构中的分布偏移污染问题，提升了降水临近预报的准确性。

Abstract: Precipitation nowcasting, which aims to provide high spatio-temporal resolution precipitation forecasts by leveraging current radar observations, is a core task in regional weather forecasting. The cascaded architecture has emerged as the mainstream paradigm for deep learning-based precipitation nowcasting. This paradigm involves a deterministic model to predict macroscopic trends (or posterior mean), followed by a probabilistic model to generate local details (or local stochasticity). However, existing methods commonly overlook the conflation of the systematic distribution shift in deterministic predictions and the local stochasticity. As a result, the deterministic component's distribution shift contaminates the predictions of the probabilistic component, leading to inaccuracies in precipitation patterns and intensity, particularly over longer lead times. To address this issue, we introduce RectiCast, a two-stage framework that explicitly decouples the correction of mean-field shift from the generation of local stochasticity via a dual Flow Matching model. In the first stage, a deterministic model generates the posterior mean. In the second stage, we introduce a Rectifier to explicitly learn the distribution shift and produce a rectified mean. Subsequently, a Generator focuses on modeling the local stochasticity conditioned on the rectified mean. Experiments on SEVIR and MeteoNet demonstrate that RectiCast achieves significant performance improvements over existing state-of-the-art methods.

</details>


### [119] [Boundary-Aware Adversarial Filtering for Reliable Diagnosis under Extreme Class Imbalance](https://arxiv.org/abs/2511.17629)
*Yanxuan Yu,Michael S. Hughes,Julien Lee,Jiacheng Zhou,Andrew F. Laine*

Main category: cs.LG

TL;DR: AF-SMOTE是一种针对极端类别不平衡分类的增强框架，通过合成少数类样本并使用对抗判别器和边界效用模型进行过滤，在保持校准的同时提高召回率。


<details>
  <summary>Details</summary>
Motivation: 解决极端类别不平衡场景下的分类问题，特别是在医疗诊断等需要高召回率和良好校准的应用中，避免漏诊罕见疾病带来的严重后果。

Method: 提出AF-SMOTE框架：先合成少数类样本，然后使用对抗判别器和边界效用模型进行过滤，在决策边界平滑和类条件密度的温和假设下，该过滤步骤能单调改进F_beta代理指标而不增加Brier分数。

Result: 在MIMIC-IV代理标签预测和欺诈检测基准测试中，AF-SMOTE比SMOTE、ADASYN等强基线方法获得更高的召回率和平均精度，并具有最佳校准性能。在多个额外数据集上验证了这些优势。

Conclusion: AF-SMOTE在医疗诊断等极端不平衡场景中具有实际价值，能有效提高罕见疾病的检测召回率而不损害模型校准，对临床决策具有重要意义。

Abstract: We study classification under extreme class imbalance where recall and calibration are both critical, for example in medical diagnosis scenarios. We propose AF-SMOTE, a mathematically motivated augmentation framework that first synthesizes minority points and then filters them by an adversarial discriminator and a boundary utility model. We prove that, under mild assumptions on the decision boundary smoothness and class-conditional densities, our filtering step monotonically improves a surrogate of F_beta (for beta >= 1) while not inflating Brier score. On MIMIC-IV proxy label prediction and canonical fraud detection benchmarks, AF-SMOTE attains higher recall and average precision than strong oversampling baselines (SMOTE, ADASYN, Borderline-SMOTE, SVM-SMOTE), and yields the best calibration. We further validate these gains across multiple additional datasets beyond MIMIC-IV. Our successful application of AF-SMOTE to a healthcare dataset using a proxy label demonstrates in a disease-agnostic way its practical value in clinical situations, where missing true positive cases in rare diseases can have severe consequences.

</details>


### [120] [Can we use LLMs to bootstrap reinforcement learning? -- A case study in digital health behavior change](https://arxiv.org/abs/2511.17630)
*Nele Albers,Esra Cemre Su de Groot,Loes Keijsers,Manon H. Hillegers,Emiel Krahmer*

Main category: cs.LG

TL;DR: LLM可以生成用户交互样本，用于训练健康行为改变的数字应用中的强化学习模型，性能可达到人类评分者水平。


<details>
  <summary>Details</summary>
Motivation: 开发个性化数字健康应用需要大量设计选择，但效果难以从文献预测且实际评估成本高。

Method: 使用LLM生成用户交互样本，比较真实用户数据和人类评分者样本，分析不同提示策略的效果。

Result: LLM生成的样本在缺乏真实数据时有用，性能达到人类评分者水平，不同提示策略效果因研究和LLM而异。

Conclusion: LLM生成的样本可在实践中用于数字行为改变设置，提供了使用建议。

Abstract: Personalizing digital applications for health behavior change is a promising route to making them more engaging and effective. This especially holds for approaches that adapt to users and their specific states (e.g., motivation, knowledge, wants) over time. However, developing such approaches requires making many design choices, whose effectiveness is difficult to predict from literature and costly to evaluate in practice. In this work, we explore whether large language models (LLMs) can be used out-of-the-box to generate samples of user interactions that provide useful information for training reinforcement learning models for digital behavior change settings. Using real user data from four large behavior change studies as comparison, we show that LLM-generated samples can be useful in the absence of real data. Comparisons to the samples provided by human raters further show that LLM-generated samples reach the performance of human raters. Additional analyses of different prompting strategies including shorter and longer prompt variants, chain-of-thought prompting, and few-shot prompting show that the relative effectiveness of different strategies depends on both the study and the LLM with also relatively large differences between prompt paraphrases alone. We provide recommendations for how LLM-generated samples can be useful in practice.

</details>


### [121] [Enhanced Federated Deep Multi-View Clustering under Uncertainty Scenario](https://arxiv.org/abs/2511.17631)
*Bingjun Wei,Xuemei Cao,Jiafen Liu,Haoyang Liang,Xin Yang*

Main category: cs.LG

TL;DR: 提出了增强联邦深度多视图聚类框架EFDMVC，通过分层对比融合解决视图不确定性，视图自适应漂移模块解决聚合不确定性，平衡聚合机制协调客户端更新，在多个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统联邦多视图聚类假设客户端视图统一，但实际部署中存在视图完整性异质性，包含不完整、冗余或损坏数据。现有方法建模视图异质性但忽视动态视图组合带来的语义冲突，未能解决视图不确定性和聚合不确定性双重挑战。

Method: 1. 局部语义对齐和分层对比融合解决视图不确定性；2. 视图自适应漂移模块通过全局-局部原型对比动态校正参数偏差；3. 平衡聚合机制协调客户端更新。

Result: EFDMVC在多个基准数据集上表现出对异质性不确定视图的优越鲁棒性，在全面评估中始终优于所有最先进的基线方法。

Conclusion: 所提出的EFDMVC框架有效解决了联邦多视图聚类中的双重不确定性挑战，通过语义对齐、自适应漂移和平衡聚合实现了对异质性视图的鲁棒聚类性能。

Abstract: Traditional Federated Multi-View Clustering assumes uniform views across clients, yet practical deployments reveal heterogeneous view completeness with prevalent incomplete, redundant, or corrupted data. While recent approaches model view heterogeneity, they neglect semantic conflicts from dynamic view combinations, failing to address dual uncertainties: view uncertainty (semantic inconsistency from arbitrary view pairings) and aggregation uncertainty (divergent client updates with imbalanced contributions). To address these, we propose a novel Enhanced Federated Deep Multi-View Clustering framework: first align local semantics, hierarchical contrastive fusion within clients resolves view uncertainty by eliminating semantic conflicts; a view adaptive drift module mitigates aggregation uncertainty through global-local prototype contrast that dynamically corrects parameter deviations; and a balanced aggregation mechanism coordinates client updates. Experimental results demonstrate that EFDMVC achieves superior robustness against heterogeneous uncertain views across multiple benchmark datasets, consistently outperforming all state-of-the-art baselines in comprehensive evaluations.

</details>


### [122] [Smart Manufacturing: MLOps-Enabled Event-Driven Architecture for Enhanced Control in Steel Production](https://arxiv.org/abs/2511.17632)
*Bestoun S. Ahmed,Tommaso Azzalin,Andreas Kassler,Andreas Thore,Hans Lindback*

Main category: cs.LG

TL;DR: 提出了一种基于数字孪生的智能制造方法，通过微服务边缘计算平台和深度强化学习代理优化钢铁生产厂的可持续性、效率和成本效益。


<details>
  <summary>Details</summary>
Motivation: 将传统制造过程转变为智能系统，实现可持续性目标，强调MLOps在数据驱动制造中的关键作用。

Method: 使用微服务边缘计算平台实时采集传感器数据到数字孪生，通过深度强化学习代理在MLOps系统中自主关联系统状态与数字孪生，优化感应炉加热和功率设置。

Result: 系统能够减少制造浪费、提高生产质量，并具有可扩展的事件驱动架构，可适应各种工业应用。

Conclusion: 这是将传统过程转变为智能系统的关键一步，符合可持续性目标，突出了MLOps在塑造数据驱动制造未来中的重要作用。

Abstract: We explore a Digital Twin-Based Approach for Smart Manufacturing to improve Sustainability, Efficiency, and Cost-Effectiveness for a steel production plant. Our system is based on a micro-service edge-compute platform that ingests real-time sensor data from the process into a digital twin over a converged network infrastructure. We implement agile machine learning-based control loops in the digital twin to optimize induction furnace heating, enhance operational quality, and reduce process waste. Key to our approach is a Deep Reinforcement learning-based agent used in our machine learning operation (MLOps) driven system to autonomously correlate the system state with its digital twin to identify correction actions that aim to optimize power settings for the plant. We present the theoretical basis, architectural details, and practical implications of our approach to reduce manufacturing waste and increase production quality. We design the system for flexibility so that our scalable event-driven architecture can be adapted to various industrial applications. With this research, we propose a pivotal step towards the transformation of traditional processes into intelligent systems, aligning with sustainability goals and emphasizing the role of MLOps in shaping the future of data-driven manufacturing.

</details>


### [123] [PocketLLM: Ultimate Compression of Large Language Models via Meta Networks](https://arxiv.org/abs/2511.17637)
*Ye Tian,Chengcheng Wang,Jing Han,Yehui Tang,Kai Han*

Main category: cs.LG

TL;DR: PocketLLM是一种通过元网络在潜在空间压缩大语言模型的新方法，使用编码器将权重投影到离散潜在向量，通过紧凑码本表示，再用轻量解码器重建权重，实现高压缩比。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模不断增大，在边缘设备上存储和传输变得困难，传统量化剪枝方法难以在保持精度的同时实现极端压缩。

Method: 提出简单编码器网络将LLM权重投影到离散潜在向量，用紧凑码本表示，再用轻量解码器将码本代表向量映射回原始权重空间。

Result: 实验表明PocketLLM在极高压缩比下仍保持优异性能，例如将Llama 2-7B压缩10倍而精度损失可忽略。

Conclusion: PocketLLM通过潜在空间压缩方法实现了大语言模型的高效压缩，为边缘设备部署提供了可行方案。

Abstract: As Large Language Models (LLMs) continue to grow in size, storing and transmitting them on edge devices becomes increasingly challenging. Traditional methods like quantization and pruning struggle to achieve extreme compression of LLMs without sacrificing accuracy. In this paper, we introduce PocketLLM, a novel approach to compress LLMs in a latent space via meta-networks. A simple encoder network is proposed to project the weights of LLMs into discrete latent vectors, which are then represented using a compact codebook. A lightweight decoder network is employed to map the codebook's representative vectors back to the original weight space. This method allows for significant compression of the large weights in LLMs, consisting solely of a small decoder, a concise codebook, and an index. Extensive experiments show that PocketLLM achieves superior performance even at significantly high compression ratios, e.g., compressing Llama 2-7B by 10x with a negligible drop in accuracy.

</details>


### [124] [Model-to-Model Knowledge Transmission (M2KT): A Data-Free Framework for Cross-Model Understanding Transfer](https://arxiv.org/abs/2511.17638)
*Pratham Sorte*

Main category: cs.LG

TL;DR: 提出了一种无需数据的模型间知识传输方法M2KT，通过概念空间的知识包交换实现知识传递，相比传统知识蒸馏减少98%数据使用。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏、迁移学习等方法仍依赖数据驱动，需要教师模型生成示例或梯度，限制了知识传输效率。

Method: 引入概念流形和模型间对齐映射，通过知识包传输结构化概念嵌入、抽象图、推理轨迹等，使用复合损失函数确保几何、结构和推理一致性。

Result: 在符号推理任务中，M2KT能达到教师模型85-90%的性能，同时数据使用量比标准知识蒸馏减少98%以上。

Conclusion: 为无数据AI间知识传输和自改进模型生态系统建立了理论和实践基础。

Abstract: Modern artificial intelligence systems depend heavily on large datasets for both training and transferring knowledge between models. Knowledge distillation, transfer learning, and dataset distillation have made such transfers more efficient, yet they remain fundamentally data-driven: a teacher must produce examples, logits, or gradients for a student to learn. In this work, we introduce Model-to-Model Knowledge Transmission (M2KT), a novel paradigm for data-free conceptual transfer between neural networks. M2KT enables models to exchange knowledge packets that encapsulate structured concept embeddings, abstraction graphs, reasoning traces, and provenance metadata. Unlike classical distillation, M2KT operates primarily in concept space rather than example space, and it does not require labeled datasets or teacher-generated outputs during transfer. We formalize the notion of concept manifolds, introduce an inter-model alignment mapping between teacher and student latent spaces, and derive a composite loss that enforces geometric, structural, and reasoning consistency together with explicit safety constraints. We further present algorithmic procedures for teacher-side packet generation and student-side ingestion and verification. Experiments on symbolic reasoning with large language models show that M2KT can achieve approximately 85 to 90 percent of teacher performance while reducing data usage by over 98 percent compared to standard knowledge distillation. This work establishes a theoretical and practical foundation for data-free AI-to-AI knowledge transfer and self-improving model ecosystems.

</details>


### [125] [TTF: A Trapezoidal Temporal Fusion Framework for LTV Forecasting in Douyin](https://arxiv.org/abs/2511.17639)
*Yibing Wan,Zhengxiong Guan,Chaoli Zhang,Xiaoyang Li,Lai Xu,Beibei Jia,Zhenzhe Zheng,Fan Wu*

Main category: cs.LG

TL;DR: 提出了TTF框架来解决用户生命周期价值(LTV)预测中的多时间序列不对齐、短输入长输出(SILO)和数据波动性三大挑战，已在抖音上线并显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 互联网公司在用户增长场景中投入大量资金获取新用户，但可持续增长需要用户生命周期价值(LTV)超过获客成本(CAC)。为了最大化LTV/CAC比率，需要在早期阶段预测渠道级LTV以优化预算分配。

Method: 提出了梯形时间融合(TTF)框架，包含梯形多时间序列模块处理数据不对齐和SILO挑战，以及多塔融合网络(MT-FusionNet)结构输出准确预测。

Result: 该框架已在抖音在线系统部署，相比之前部署的在线模型，LTV曲线的点级MAPE(MAPEp)降低了4.3%，聚合LTV的MAPE(MAPEa)降低了3.2%。

Conclusion: TTF框架有效解决了LTV预测中的关键挑战，显著提升了预测精度，为预算分配优化提供了可靠支持。

Abstract: In the user growth scenario, Internet companies invest heavily in paid acquisition channels to acquire new users. But sustainable growth depends on acquired users' generating lifetime value (LTV) exceeding customer acquisition cost (CAC). In order to maximize LTV/CAC ratio, it is crucial to predict channel-level LTV in an early stage for further optimization of budget allocation. The LTV forecasting problem is significantly different from traditional time series forecasting problems, and there are three main challenges. Firstly, it is an unaligned multi-time series forecasting problem that each channel has a number of LTV series of different activation dates. Secondly, to predict in the early stage, it faces the imbalanced short-input long-output (SILO) challenge. Moreover, compared with the commonly used time series datasets, the real LTV series are volatile and non-stationary, with more frequent fluctuations and higher variance. In this work, we propose a novel framework called Trapezoidal Temporal Fusion (TTF) to address the above challenges. We introduce a trapezoidal multi-time series module to deal with data unalignment and SILO challenges, and output accurate predictions with a multi-tower structure called MT-FusionNet. The framework has been deployed to the online system for Douyin. Compared to the previously deployed online model, MAPEp decreased by 4.3%, and MAPEa decreased by 3.2%, where MAPEp denotes the point-wise MAPE of the LTV curve and MAPEa denotes the MAPE of the aggregated LTV.

</details>


### [126] [BlockCert: Certified Blockwise Extraction of Transformer Mechanisms](https://arxiv.org/abs/2511.17645)
*Sandro Andric*

Main category: cs.LG

TL;DR: BlockCert框架通过认证的块级提取方法，将transformer机制转化为结构化替代实现，并提供机器可检查的证书来约束近似误差、记录覆盖指标和哈希底层工件。


<details>
  <summary>Details</summary>
Motivation: 机制可解释性和模型编辑领域通常缺乏正式保证，无法确定提取或编辑后的模型在相关输入上与原始模型的偏差程度。

Method: 引入BlockCert框架进行认证的transformer块级提取，使用Lipschitz-based组合定理将局部保证提升为全局偏差界限。

Result: 在GPT-2 small、TinyLlama-1.1B-Chat和Llama-3.2-3B上获得高块级覆盖率和小的残差误差，在TinyLlama设置中完全拼接模型在压力提示下与基线困惑度匹配在约6e-5内。

Conclusion: 块级提取与显式证书对于真实transformer语言模型是可行的，为机制可解释性和模型行为的形式推理提供了实用桥梁。

Abstract: Mechanistic interpretability aspires to reverse-engineer neural networks into explicit algorithms, while model editing seeks to modify specific behaviours without retraining. Both areas are typically evaluated with informal evidence and ad-hoc experiments, with few explicit guarantees about how far an extracted or edited model can drift from the original on relevant inputs. We introduce BlockCert, a framework for certified blockwise extraction of transformer mechanisms, and outline how a lightweight extension can support certified local edits. Given a pre-trained transformer and a prompt distribution, BlockCert extracts structured surrogate implementations for residual blocks together with machine-checkable certificates that bound approximation error, record coverage metrics, and hash the underlying artifacts. We formalize a simple Lipschitz-based composition theorem in Lean 4 that lifts these local guarantees to a global deviation bound. Empirically, we apply the framework to GPT-2 small, TinyLlama-1.1B-Chat, and Llama-3.2-3B. Across these models we obtain high per-block coverage and small residual errors on the evaluated prompts, and in the TinyLlama setting we show that a fully stitched model matches the baseline perplexity within approximately 6e-5 on stress prompts. Our results suggest that blockwise extraction with explicit certificates is feasible for real transformer language models and offers a practical bridge between mechanistic interpretability and formal reasoning about model behaviour.

</details>


### [127] [MamTiff-CAD: Multi-Scale Latent Diffusion with Mamba+ for Complex Parametric Sequence](https://arxiv.org/abs/2511.17647)
*Liyuan Deng,Yunpeng Bai,Yongkang Dai,Xiaoshui Huang,Hongping Gan,Dongshuo Huang,Hao jiacheng,Yilei Shi*

Main category: cs.LG

TL;DR: MamTiff-CAD是一个基于Transformer扩散模型的CAD参数化命令序列生成框架，通过多尺度潜在表示处理长序列CAD建模，在60-256长度序列上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂CAD模型的几何和拓扑约束时，难以生成长序列参数化命令，需要新的解决方案来应对这一挑战。

Method: 设计集成Mamba+和Transformer的自编码器，将参数化CAD序列转换为潜在表示；Mamba+块采用遗忘门机制捕获长程依赖；基于多尺度Transformer的扩散模型学习长序列命令分布。

Result: 在重建和生成任务上均达到最先进性能，成功生成长度60-256的CAD模型序列。

Conclusion: MamTiff-CAD框架有效解决了长序列CAD参数化命令生成问题，为工业应用提供了可行的解决方案。

Abstract: Parametric Computer-Aided Design (CAD) is crucial in industrial applications, yet existing approaches often struggle to generate long sequence parametric commands due to complex CAD models' geometric and topological constraints. To address this challenge, we propose MamTiff-CAD, a novel CAD parametric command sequences generation framework that leverages a Transformer-based diffusion model for multi-scale latent representations. Specifically, we design a novel autoencoder that integrates Mamba+ and Transformer, to transfer parameterized CAD sequences into latent representations. The Mamba+ block incorporates a forget gate mechanism to effectively capture long-range dependencies. The non-autoregressive Transformer decoder reconstructs the latent representations. A diffusion model based on multi-scale Transformer is then trained on these latent embeddings to learn the distribution of long sequence commands. In addition, we also construct a dataset that consists of long parametric sequences, which is up to 256 commands for a single CAD model. Experiments demonstrate that MamTiff-CAD achieves state-of-the-art performance on both reconstruction and generation tasks, confirming its effectiveness for long sequence (60-256) CAD model generation.

</details>


### [128] [Frugality in second-order optimization: floating-point approximations for Newton's method](https://arxiv.org/abs/2511.17660)
*Giuseppe Carrino,Elena Loli Piccolomini,Elisa Riccietti,Theo Mary*

Main category: cs.LG

TL;DR: 该论文分析了有限精度算术对牛顿步的影响，建立了混合精度牛顿优化器的收敛定理，并提出了GN_k方法，这是一种广义高斯-牛顿方法，能够在回归任务中实现与完整牛顿法相当的性能，同时显著减少导数计算次数。


<details>
  <summary>Details</summary>
Motivation: 尽管一阶方法在机器学习训练中占主导地位，但高阶方法如牛顿法可以提供更高的精度和更快的收敛速度，但由于计算成本高而常常被避免。本研究旨在解决有限精度算术对牛顿步的影响问题。

Method: 建立了混合精度牛顿优化器的收敛定理，包括"准"和"不精确"变体，并引入了GN_k方法，这是一种广义高斯-牛顿方法，允许部分计算二阶导数。

Result: 在标准回归基准测试上的实证评估表明，所提出的方法在Australian和MUSH数据集上优于Adam。GN_k在回归任务中实现了与完整牛顿法相当的性能，同时需要显著更少的导数评估。

Conclusion: 混合精度牛顿优化器提供了收敛保证和可达到解精度的先验估计，而GN_k方法在保持高性能的同时显著降低了计算成本，为高阶优化方法在机器学习中的实际应用提供了可行方案。

Abstract: Minimizing loss functions is central to machine-learning training. Although first-order methods dominate practical applications, higher-order techniques such as Newton's method can deliver greater accuracy and faster convergence, yet are often avoided due to their computational cost. This work analyzes the impact of finite-precision arithmetic on Newton steps and establishes a convergence theorem for mixed-precision Newton optimizers, including "quasi" and "inexact" variants. The theorem provides not only convergence guarantees but also a priori estimates of the achievable solution accuracy. Empirical evaluations on standard regression benchmarks demonstrate that the proposed methods outperform Adam on the Australian and MUSH datasets. The second part of the manuscript introduces GN_k, a generalized Gauss-Newton method that enables partial computation of second-order derivatives. GN_k attains performance comparable to full Newton's method on regression tasks while requiring significantly fewer derivative evaluations.

</details>


### [129] [Enhancing Breast Cancer Prediction with LLM-Inferred Confounders](https://arxiv.org/abs/2511.17662)
*Debmita Roy*

Main category: cs.LG

TL;DR: 使用大语言模型从临床数据推断糖尿病、肥胖和心血管疾病等混杂疾病的可能性，提升乳腺癌预测的随机森林模型性能。


<details>
  <summary>Details</summary>
Motivation: 通过AI生成的特征来改进乳腺癌预测，特别是利用大语言模型推断可能影响乳腺癌诊断的混杂疾病，以支持早期检测和临床决策。

Method: 使用大语言模型（如Gemma和Llama）从常规临床数据中推断糖尿病、肥胖和心血管疾病的可能性，将这些AI生成的特征输入随机森林模型进行乳腺癌预测。

Result: AI生成的特征显著提高了随机森林模型的性能，特别是Gemma模型提升3.9%，Llama模型提升6.4%。

Conclusion: 该方法在无创预筛查和临床整合方面显示出潜力，有助于改善乳腺癌的早期检测和共享决策制定。

Abstract: This study enhances breast cancer prediction by using large language models to infer the likelihood of confounding diseases, namely diabetes, obesity, and cardiovascular disease, from routine clinical data. These AI-generated features improved Random Forest model performance, particularly for LLMs like Gemma (3.9%) and Llama (6.4%). The approach shows promise for noninvasive prescreening and clinical integration, supporting improved early detection and shared decision-making in breast cancer diagnosis.

</details>


### [130] [AI-based framework to predict animal and pen feed intake in feedlot beef cattle](https://arxiv.org/abs/2511.17663)
*Alex S. C. Maia,John B. Hall,Hugo F. M. Milan,Izabelle A. M. A. Teixeira*

Main category: cs.LG

TL;DR: 开发了一个基于AI的框架，利用环境指数和机器学习模型准确预测个体动物和围栏级别的饲料摄入量。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏充分利用纵向大数据来准确预测饲料摄入量并考虑环境条件的方法。

Method: 使用19个实验的数据（超过1650万个样本）和AgriMet网络气象站的环境数据，开发了两个新的环境指数：InComfort-Index和EASI-Index，并结合机器学习模型（XGBoost）进行训练。

Result: 最佳机器学习模型（XGBoost）在动物级别的预测准确度为RMSE 1.38 kg/天，在围栏级别仅为0.14 kg/(天-动物)。

Conclusion: 该方法为预测个体动物和围栏的饲料摄入量提供了一个稳健的基于AI的框架，具有在精准管理饲养场牛群、减少饲料浪费、优化资源和气候适应性畜牧管理方面的潜在应用。

Abstract: Advances in technology are transforming sustainable cattle farming practices, with electronic feeding systems generating big longitudinal datasets on individual animal feed intake, offering the possibility for autonomous precision livestock systems. However, the literature still lacks a methodology that fully leverages these longitudinal big data to accurately predict feed intake accounting for environmental conditions. To fill this gap, we developed an AI-based framework to accurately predict feed intake of individual animals and pen-level aggregation. Data from 19 experiments (>16.5M samples; 2013-2024) conducted at Nancy M. Cummings Research Extension & Education Center (Carmen, ID) feedlot facility and environmental data from AgriMet Network weather stations were used to develop two novel environmental indices: InComfort-Index, based solely on meteorological variables, showed good predictive capability for thermal comfort but had limited ability to predict feed intake; EASI-Index, a hybrid index integrating environmental variables with feed intake behavior, performed well in predicting feed intake but was less effective for thermal comfort. Together with the environmental indices, machine learning models were trained and the best-performing machine learning model (XGBoost) accuracy was RMSE of 1.38 kg/day for animal-level and only 0.14 kg/(day-animal) at pen-level. This approach provides a robust AI-based framework for predicting feed intake in individual animals and pens, with potential applications in precision management of feedlot cattle, through feed waste reduction, resource optimization, and climate-adaptive livestock management.

</details>


### [131] [CubeletWorld: A New Abstraction for Scalable 3D Modeling](https://arxiv.org/abs/2511.17664)
*Azlaan Mustafa Samad,Hoang H. Nguyen,Lukas Berg,Henrik Müller,Yuan Xue,Daniel Kudenko,Zahra Ahmadi*

Main category: cs.LG

TL;DR: CubeletWorld是一个基于离散化3D网格（cubelets）的城市环境表示框架，通过将多样化数据嵌入到局部空间单元中，实现隐私保护的建模，支持规划、导航和占用预测等任务。


<details>
  <summary>Details</summary>
Motivation: 现代城市产生大量异构数据，但将这些数据整合成连贯的空间模型仍具挑战性。现有基于智能体的方法依赖直接环境感知，存在可扩展性限制和隐私问题。

Method: 提出CubeletWorld框架，使用称为cubelets的离散化3D网格单元来表示城市环境，将基础设施、移动和环境指标等多样化数据信号嵌入到局部cubelet状态中。

Result: 开发了CubeletWorld状态预测任务，使用包含街道和建筑等城市元素的真实数据集进行评估。结果表明该框架能够提供灵活可扩展的学习能力。

Conclusion: CubeletWorld为从复杂城市数据中学习提供了灵活可扩展的框架，在人口统计建模、环境监测和应急响应等领域开辟了新的可能性。

Abstract: Modern cities produce vast streams of heterogeneous data, from infrastructure maps to mobility logs and satellite imagery. However, integrating these sources into coherent spatial models for planning and prediction remains a major challenge. Existing agent-centric methods often rely on direct environmental sensing, limiting scalability and raising privacy concerns. This paper introduces CubeletWorld, a novel framework for representing and analyzing urban environments through a discretized 3D grid of spatial units called cubelets. This abstraction enables privacy-preserving modeling by embedding diverse data signals, such as infrastructure, movement, or environmental indicators, into localized cubelet states. CubeletWorld supports downstream tasks such as planning, navigation, and occupancy prediction without requiring agent-driven sensing. To evaluate this paradigm, we propose the CubeletWorld State Prediction task, which involves predicting the cubelet state using a realistic dataset containing various urban elements like streets and buildings through this discretized representation. We explore a range of modified core models suitable for our setting and analyze challenges posed by increasing spatial granularity, specifically the issue of sparsity in representation and scalability of baselines. In contrast to existing 3D occupancy prediction models, our cubelet-centric approach focuses on inferring state at the spatial unit level, enabling greater generalizability across regions and improved privacy compliance. Our results demonstrate that CubeletWorld offers a flexible and extensible framework for learning from complex urban data, and it opens up new possibilities for scalable simulation and decision support in domains such as socio-demographic modeling, environmental monitoring, and emergency response. The code and datasets can be downloaded from here.

</details>


### [132] [GANGR: GAN-Assisted Scalable and Efficient Global Routing Parallelization](https://arxiv.org/abs/2511.17665)
*Hadi Khodaei Jooshin,Inna Partin-Vaisband*

Main category: cs.LG

TL;DR: 提出了一种基于Wasserstein生成对抗网络(WGANs)的新型全局布线批处理算法，相比传统方法能生成更少但质量更高的批次，在ISPD'24基准测试中实现40%运行时间减少，且布线质量仅下降0.002%。


<details>
  <summary>Details</summary>
Motivation: 传统全局布线批处理方法依赖计算昂贵的启发式算法，导致批次过大、批次数量过多、生成时间过长等问题，限制了可扩展性和效率。

Method: 使用Wasserstein生成对抗网络(WGANs)增强批处理算法，生成更少但质量更高的批次以实现更有效的并行化。

Result: 在ISPD'24基准测试中，相比最先进的路由器，运行时间减少高达40%，布线质量仅下降0.002%。

Conclusion: 基于WGANs的批处理算法能显著提高全局布线的效率和可扩展性，同时保持高质量的布线结果。

Abstract: Global routing is a critical stage in electronic design automation (EDA) that enables early estimation and optimization of the routability of modern integrated circuits with respect to congestion, power dissipation, and design complexity. Batching is a primary concern in top-performing global routers, grouping nets into manageable sets to enable parallel processing and efficient resource usage. This process improves memory usage, scalable parallelization on modern hardware, and routing congestion by controlling net interactions within each batch. However, conventional batching methods typically depend on heuristics that are computationally expensive and can lead to suboptimal results (oversized batches with conflicting nets, excessive batch counts degrading parallelization, and longer batch generation times), ultimately limiting scalability and efficiency. To address these limitations, a novel batching algorithm enhanced with Wasserstein generative adversarial networks (WGANs) is introduced in this paper, enabling more effective parallelization by generating fewer higher-quality batches in less time. The proposed algorithm is tested on the latest ISPD'24 contest benchmarks, demonstrating up to 40% runtime reduction with only 0.002% degradation in routing quality as compared to state-of-the-art router.

</details>


### [133] [Lane-Frame Quantum Multimodal Driving Forecasts for the Trajectory of Autonomous Vehicles](https://arxiv.org/abs/2511.17675)
*Navneet Singh,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: 提出了一种紧凑的混合量子架构，用于自动驾驶轨迹预测，通过残差学习和量子注意力编码器实现高效的多模态轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶轨迹预测需要在计算和延迟约束下提供准确、校准的多模态未来轨迹预测。

Method: 使用混合量子架构，包括量子注意力编码器、量子前馈堆栈和傅里叶解码器，在车道对齐坐标系中预测相对于运动学基线的残差修正。

Result: 在Waymo Open Motion数据集上，16个预测模型在2.0秒时间范围内实现了minADE 1.94米和minFDE 3.56米，优于运动学基线。

Conclusion: 残差学习、截断傅里叶解码、浅层纠缠和基于频谱的排序等方法能够有效利用量子电路的容量，在自动驾驶基准测试中实现稳定的优化和可靠的多模态预测。

Abstract: Trajectory forecasting for autonomous driving must deliver accurate, calibrated multi-modal futures under tight compute and latency constraints. We propose a compact hybrid quantum architecture that aligns quantum inductive bias with road-scene structure by operating in an ego-centric, lane-aligned frame and predicting residual corrections to a kinematic baseline instead of absolute poses. The model combines a transformer-inspired quantum attention encoder (9 qubits), a parameter-lean quantum feedforward stack (64 layers, ${\sim}1200$ trainable angles), and a Fourier-based decoder that uses shallow entanglement and phase superposition to generate 16 trajectory hypotheses in a single pass, with mode confidences derived from the latent spectrum. All circuit parameters are trained with Simultaneous Perturbation Stochastic Approximation (SPSA), avoiding backpropagation through non-analytic components. In the Waymo Open Motion Dataset, the model achieves minADE (minimum Average Displacement Error) of \SI{1.94}{m} and minFDE (minimum Final Displacement Error) of \SI{3.56}{m} in the $16$ models predicted over the horizon of \SI{2.0}{s}, consistently outperforming a kinematic baseline with reduced miss rates and strong recall. Ablations confirm that residual learning in the lane frame, truncated Fourier decoding, shallow entanglement, and spectrum-based ranking focus capacity where it matters, yielding stable optimization and reliable multi-modal forecasts from small, shallow quantum circuits on a modern autonomous-driving benchmark.

</details>


### [134] [A Hybrid Classical-Quantum Fine Tuned BERT for Text Classification](https://arxiv.org/abs/2511.17677)
*Abu Kaisar Mohammad Masum,Naveed Mahmud,M. Hassan Najafi,Sercan Aygun*

Main category: cs.LG

TL;DR: 提出了一种将n-qubit量子电路与经典BERT模型结合的混合方法用于文本分类，在标准基准数据集上取得了与经典基线相当甚至更好的性能。


<details>
  <summary>Details</summary>
Motivation: BERT微调计算成本高且需要仔细的超参数调优，而量子算法在机器学习和文本分类任务中显示出超越传统方法的潜力。

Method: 将n-qubit量子电路与经典BERT模型集成，构建混合量子-经典模型进行文本分类。

Result: 混合模型在标准基准数据集上取得了与经典基线相当甚至更好的性能，证明了该方法的可行性和潜力。

Conclusion: 混合模型展示了量子计算在提升文本分类任务性能方面的前景，以及经典-量子模型在不同数据集上微调预训练模型的适应性。

Abstract: Fine-tuning BERT for text classification can be computationally challenging and requires careful hyper-parameter tuning. Recent studies have highlighted the potential of quantum algorithms to outperform conventional methods in machine learning and text classification tasks. In this work, we propose a hybrid approach that integrates an n-qubit quantum circuit with a classical BERT model for text classification. We evaluate the performance of the fine-tuned classical-quantum BERT and demonstrate its feasibility as well as its potential in advancing this research area. Our experimental results show that the proposed hybrid model achieves performance that is competitive with, and in some cases better than, the classical baselines on standard benchmark datasets. Furthermore, our approach demonstrates the adaptability of classical-quantum models for fine-tuning pre-trained models across diverse datasets. Overall, the hybrid model highlights the promise of quantum computing in achieving improved performance for text classification tasks.

</details>


### [135] [Boosting Brain-inspired Path Integration Efficiency via Learning-based Replication of Continuous Attractor Neurodynamics](https://arxiv.org/abs/2511.17687)
*Zhangyu Ge,Xu He,Lingfei Mo,Xiaolin Meng,Wenxuan Yin,Youdong Zhang,Lansong Jiang,Fengyuan Liu*

Main category: cs.LG

TL;DR: 本文提出了一种使用表示学习模型复制连续吸引子神经网络神经动力学模式的高效路径整合方法，相比NeuroSLAM系统在通用设备上效率提升约17.5%，在边缘设备上提升40-50%。


<details>
  <summary>Details</summary>
Motivation: 现有脑启发导航研究中使用连续吸引子神经网络构建的路径整合能力存在显著计算冗余，运行效率需要提升，否则不利于脑启发导航技术的实际应用。

Method: 使用轻量级人工神经网络成功复制了连续吸引子神经网络建模的头部方向细胞和网格细胞的神经动力学模式，并将这些模型集成实现脑启发的航位推算路径整合。

Result: 在各种环境中的基准测试表明，该方法不仅准确复制了导航细胞的神经动力学模式，定位精度与NeuroSLAM相当，而且在通用设备上效率提升约17.5%，在边缘设备上提升40-50%。

Conclusion: 这项工作为提高脑启发导航技术的实用性提供了一种新颖的实现策略，并具有进一步扩展的潜力。

Abstract: The brain's Path Integration (PI) mechanism offers substantial guidance and inspiration for Brain-Inspired Navigation (BIN). However, the PI capability constructed by the Continuous Attractor Neural Networks (CANNs) in most existing BIN studies exhibits significant computational redundancy, and its operational efficiency needs to be improved; otherwise, it will not be conducive to the practicality of BIN technology. To address this, this paper proposes an efficient PI approach using representation learning models to replicate CANN neurodynamic patterns. This method successfully replicates the neurodynamic patterns of CANN-modeled Head Direction Cells (HDCs) and Grid Cells (GCs) using lightweight Artificial Neural Networks (ANNs). These ANN-reconstructed HDC and GC models are then integrated to achieve brain-inspired PI for Dead Reckoning (DR). Benchmark tests in various environments, compared with the well-known NeuroSLAM system, demonstrate that this work not only accurately replicates the neurodynamic patterns of navigation cells but also matches NeuroSLAM in positioning accuracy. Moreover, efficiency improvements of approximately 17.5% on the general-purpose device and 40~50% on the edge device were observed, compared with NeuroSLAM. This work offers a novel implementation strategy to enhance the practicality of BIN technology and holds potential for further extension.

</details>


### [136] [Enhancing Adversarial Transferability through Block Stretch and Shrink](https://arxiv.org/abs/2511.17688)
*Quan Liu,Feng Ye,Chenhao Lu,Shuming Zhen,Guanliang Huang,Lunzhe Chen,Xudong Ke*

Main category: cs.LG

TL;DR: 提出Block Stretch and Shrink (BSS)方法，通过将图像分块并进行拉伸和收缩操作来增强对抗样本的跨模型可迁移性，在ImageNet子集上优于现有输入变换攻击方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于输入变换的攻击方法在跨模型可迁移性方面表现有限，研究表明高可迁移性与多样化的注意力热图和保持全局语义相关。

Method: BSS方法将图像分成块，对这些块应用拉伸和收缩操作，从而在变换输入中多样化注意力热图同时保持其全局语义。

Result: 在ImageNet子集上的实证评估表明，BSS在可迁移性方面优于现有的基于输入变换的攻击方法。

Conclusion: BSS方法有效提升了对抗样本的跨模型可迁移性，并建议在统一的数量尺度下评估输入变换攻击方法以确保公平比较。

Abstract: Adversarial attacks introduce small, deliberately crafted perturbations that mislead neural networks, and their transferability from white-box to black-box target models remains a critical research focus. Input transformation-based attacks are a subfield of adversarial attacks that enhance input diversity through input transformations to improve the transferability of adversarial examples. However, existing input transformation-based attacks tend to exhibit limited cross-model transferability. Previous studies have shown that high transferability is associated with diverse attention heatmaps and the preservation of global semantics in transformed inputs. Motivated by this observation, we propose Block Stretch and Shrink (BSS), a method that divides an image into blocks and applies stretch and shrink operations to these blocks, thereby diversifying attention heatmaps in transformed inputs while maintaining their global semantics. Empirical evaluations on a subset of ImageNet demonstrate that BSS outperforms existing input transformation-based attack methods in terms of transferability. Furthermore, we examine the impact of the number scale, defined as the number of transformed inputs, in input transformation-based attacks, and advocate evaluating these methods under a unified number scale to enable fair and comparable assessments.

</details>


### [137] [DeepCoT: Deep Continual Transformers for Real-Time Inference on Data Streams](https://arxiv.org/abs/2511.17693)
*Ginés Carreto Picón,Peng Yuan Zhou,Qi Zhang,Alexandros Iosifidis*

Main category: cs.LG

TL;DR: 提出了DeepCoT模型，一种无冗余的仅编码器架构，可在现有深度编码器架构上应用，实现线性计算成本，相比之前的高效模型将运行时间减少达两个数量级。


<details>
  <summary>Details</summary>
Motivation: Transformer模型规模不断增大，但在资源受限设备上需要低延迟推理。流数据推理通常采用滑动时间窗口，导致高度冗余计算。现有的Continual Transformers只能用于浅层模型，限制了其应用范围和泛化能力。

Method: 提出DeepCoT模型，这是一种无冗余的仅编码器架构，可最小化改动地应用于现有深度编码器架构。

Result: 在音频、视频和文本流的实验中，DeepCoT保持了与非持续基线相当的性能，同时为所有Transformer层提供线性计算成本。

Conclusion: DeepCoT模型在保持性能的同时显著降低了计算成本，相比之前的高效模型运行时间减少达两个数量级。

Abstract: Transformer-based models have dramatically increased their size and parameter count to tackle increasingly complex tasks. At the same time, there is a growing demand for low-latency inference on resource-constrained devices that achieves high performance. In particular, stream data inference is typically performed over a sliding temporal window, leading to highly redundant computations. The recent Continual Transformers have addressed this issue, but they can only be effectively used in shallow models, which limits their scope and generalization power. In this paper, we propose the Deep Continual Transformer (DeepCoT), a redundancy-free encoder-only model that can be applied over existing deep encoder architectures with minimal changes. In our experiments over audio, video, and text streams, we show that DeepCoTs retain comparative performance to their non-continual baselines while offering a linear computational cost for all Transformer layers, which reduces up to two orders of magnitude in the running time compared to previous efficient models.

</details>


### [138] [Diffusion Models are Molecular Dynamics Simulators](https://arxiv.org/abs/2511.17741)
*Justin Diamond,Markus Lill*

Main category: cs.LG

TL;DR: 本文证明了带序列偏置的扩散采样器等价于过阻尼朗之万动力学的欧拉-丸山积分器，建立了扩散采样与朗之万时间演化的精确对应关系，并提出了完全数据驱动的分子动力学框架。


<details>
  <summary>Details</summary>
Motivation: 将分子动力学重新表述为扩散模型，摆脱传统分子动力学中对极小时间步长的依赖，通过模型能力和去噪步数两个可扩展参数控制精度。

Method: 证明扩散采样器与朗之万动力学的数学等价性，建立基于扩散模型的分子动力学框架，使用不相关平衡快照学习力场，无需轨迹数据训练。

Result: 推导了轨迹级信息论误差界限，清晰分离离散化误差与分数模型误差，生成具有分子动力学时间相关性的轨迹，同时保持与学习能量相关的玻尔兹曼分布。

Conclusion: 该方法提供了一个完全数据驱动的分子动力学替代方案，无需手工设计力场，仅使用静态配置训练即可生成具有正确时间相关性的分子轨迹。

Abstract: We prove that a denoising diffusion sampler equipped with a sequential bias across the batch dimension is exactly an Euler-Maruyama integrator for overdamped Langevin dynamics. Each reverse denoising step, with its associated spring stiffness, can be interpreted as one step of a stochastic differential equation with an effective time step set jointly by the noise schedule and that stiffness. The learned score then plays the role of the drift, equivalently the gradient of a learned energy, yielding a precise correspondence between diffusion sampling and Langevin time evolution.
  This equivalence recasts molecular dynamics (MD) in terms of diffusion models. Accuracy is no longer tied to a fixed, extremely small MD time step; instead, it is controlled by two scalable knobs: model capacity, which governs how well the drift is approximated, and the number of denoising steps, which sets the integrator resolution. In practice, this leads to a fully data-driven MD framework that learns forces from uncorrelated equilibrium snapshots, requires no hand-engineered force fields, uses no trajectory data for training, and still preserves the Boltzmann distribution associated with the learned energy.
  We derive trajectory-level, information-theoretic error bounds that cleanly separate discretization error from score-model error, clarify how temperature enters through the effective spring, and show that the resulting sampler generates molecular trajectories with MD-like temporal correlations, even though the model is trained only on static configurations.

</details>


### [139] [Periodicity-Enforced Neural Network for Designing Deterministic Lateral Displacement Devices](https://arxiv.org/abs/2511.17754)
*Andrew Lee,Mahir Mobarrat,Xiaolin Chen*

Main category: cs.LG

TL;DR: 提出了一种周期性增强的代理建模方法，通过引入周期性层来确保确定性横向位移(DLD)微流控设备设计中的精确周期性边界条件，显著提高了多单元设备预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统DLD设备设计需要计算昂贵的Navier-Stokes模拟，而现有的深度学习代理模型在处理周期性边界条件时存在累积误差问题，影响多单元设备预测的准确性。

Method: 使用三个子网络预测稳态、无量纲的速度和压力场(u, v, p)，并引入周期性层来确保单元边界处流变量的精确匹配，通过架构强制而非软惩罚方法实现周期性。

Result: 在120个CFD生成的几何结构上验证，周期性层实现实现了0.478%的临界直径误差，同时保持完美的周期性一致性，比基线方法提高了85.4%。

Conclusion: 该方法能够高效准确地设计DLD设备，保证边界条件在多单元设备应用中的满足。

Abstract: Deterministic Lateral Displacement (DLD) devices enable liquid biopsy for cancer detection by separating circulating tumor cells (CTCs) from blood samples based on size, but designing these microfluidic devices requires computationally expensive Navier-Stokes simulations and particle-tracing analyses. While recent surrogate modeling approaches using deep learning have accelerated this process, they often inadequately handle the critical periodic boundary conditions of DLD unit cells, leading to cumulative errors in multi-unit device predictions. This paper introduces a periodicity-enforced surrogate modeling approach that incorporates periodic layers, neural network components that guarantee exact periodicity without penalty terms or output modifications, into deep learning architectures for DLD device design. The proposed method employs three sub-networks to predict steady-state, non-dimensional velocity and pressure fields (u, v, p) rather than directly predicting critical diameters or particle trajectories, enabling complete flow field characterization and enhanced design flexibility. Periodic layers ensure exact matching of flow variables across unit cell boundaries through architectural enforcement rather than soft penalty-based approaches. Validation on 120 CFD-generated geometries demonstrates that the periodic layer implementation achieves 0.478% critical diameter error while maintaining perfect periodicity consistency, representing an 85.4% improvement over baseline methods. The approach enables efficient and accurate DLD device design with guaranteed boundary condition satisfaction for multi-unit device applications.

</details>


### [140] [PrismSSL: One Interface, Many Modalities; A Single-Interface Library for Multimodal Self-Supervised Learning](https://arxiv.org/abs/2511.17776)
*Melika Shirian,Kianoosh Vadaei,Kian Majlessi,Audrina Ebrahimi,Arshia Hemmat,Peyman Adibi,Hossein Karshenas*

Main category: cs.LG

TL;DR: PrismSSL是一个统一的Python库，集成了音频、视觉、图数据和跨模态的自监督学习方法，提供模块化代码库和图形化界面，便于研究人员快速使用和扩展。


<details>
  <summary>Details</summary>
Motivation: 当前自监督学习方法分散在不同领域和代码库中，研究人员需要花费大量时间配置和集成不同方法。PrismSSL旨在统一这些方法，降低使用门槛。

Method: 采用模块化设计，提供训练器和数据集抽象，集成HuggingFace Transformers，支持分布式训练、超参数搜索、LoRA微调等功能，并提供Flask图形化界面。

Result: 开发了一个功能完整的自监督学习库，支持多种模态，提供丰富的工具链和可视化功能，已打包发布在PyPI上。

Conclusion: PrismSSL成功统一了跨模态的自监督学习方法，显著降低了研究人员的使用门槛，为自监督学习研究提供了便利的工具支持。

Abstract: We present PrismSSL, a Python library that unifies state-of-the-art self-supervised learning (SSL) methods across audio, vision, graphs, and cross-modal settings in a single, modular codebase. The goal of the demo is to show how researchers and practitioners can: (i) install, configure, and run pretext training with a few lines of code; (ii) reproduce compact benchmarks; and (iii) extend the framework with new modalities or methods through clean trainer and dataset abstractions. PrismSSL is packaged on PyPI, released under the MIT license, integrates tightly with HuggingFace Transformers, and provides quality-of-life features such as distributed training in PyTorch, Optuna-based hyperparameter search, LoRA fine-tuning for Transformer backbones, animated embedding visualizations for sanity checks, Weights & Biases logging, and colorful, structured terminal logs for improved usability and clarity. In addition, PrismSSL offers a graphical dashboard - built with Flask and standard web technologies - that enables users to configure and launch training pipelines with minimal coding. The artifact (code and data recipes) will be publicly available and reproducible.

</details>


### [141] [Smoothed Agnostic Learning of Halfspaces over the Hypercube](https://arxiv.org/abs/2511.17782)
*Yiwen Kou,Raghu Meka*

Main category: cs.LG

TL;DR: 提出了一个基于随机位翻转的平滑不可知学习框架，用于布尔输入上的半空间学习，克服了传统高斯扰动在离散域中的局限性。


<details>
  <summary>Details</summary>
Motivation: 布尔半空间的不可知学习在计算学习理论中是一个基础问题，但即使在弱学习下也被证明是计算困难的。现有的平滑分析框架依赖于加性高斯扰动，不适合离散域。

Method: 引入基于随机位翻转的平滑不可知学习框架，在严格亚指数假设下，给出了学习半空间的高效算法，运行时间和样本复杂度约为n的多项式因子。

Result: 在布尔超立方体上实现了首个计算高效的平滑不可知半空间学习保证，填补了最坏情况难处理性与实际可学习性之间的差距。

Conclusion: 该工作为离散环境中的平滑不可知学习提供了首个计算效率保证，扩展了平滑分析在离散域的应用。

Abstract: Agnostic learning of Boolean halfspaces is a fundamental problem in computational learning theory, but it is known to be computationally hard even for weak learning. Recent work [CKKMK24] proposed smoothed analysis as a way to bypass such hardness, but existing frameworks rely on additive Gaussian perturbations, making them unsuitable for discrete domains. We introduce a new smoothed agnostic learning framework for Boolean inputs, where perturbations are modeled via random bit flips. This defines a natural discrete analogue of smoothed optimality generalizing the Gaussian case. Under strictly subexponential assumptions on the input distribution, we give an efficient algorithm for learning halfspaces in this model, with runtime and sample complexity approximately n raised to a poly(1/(sigma * epsilon)) factor. Previously, such algorithms were known only with strong structural assumptions for the discrete hypercube, for example, independent coordinates or symmetric distributions. Our result provides the first computationally efficient guarantee for smoothed agnostic learning of halfspaces over the Boolean hypercube, bridging the gap between worst-case intractability and practical learnability in discrete settings.

</details>


### [142] [Improved Sample Complexity for Full Coverage in Compact and Continuous Spaces](https://arxiv.org/abs/2511.17784)
*Lyu Yuhuan*

Main category: cs.LG

TL;DR: 本文提出了一种基于随机采样的均匀覆盖分析方法，通过离散化d维单位超立方体并应用集中不等式，得到了与失败概率δ呈对数依赖的样本复杂度界限，相比经典的1/δ线性依赖更加紧致。


<details>
  <summary>Details</summary>
Motivation: 经典覆盖分析在小失败概率下往往给出保守的界限，特别是在高置信度场景下效率不高。本文旨在为依赖网格覆盖保证的算法提供更锐利的理论工具。

Method: 研究d维单位超立方体上的均匀随机采样，分析离散化后未被覆盖的子立方体数量，应用集中不等式到未被覆盖计数统计量。

Result: 推导出样本复杂度界限M = O(Čln(2Č/δ))，其中Č为常数，与δ呈对数依赖关系，相比经典的1/δ线性依赖更加紧致。数值研究表明该界限能更紧密地跟踪实际覆盖需求。

Conclusion: 该方法为依赖网格覆盖保证的算法提供了更锐利的理论工具，特别是在高置信度场景下能实现更高效的采样。

Abstract: Verifying uniform conditions over continuous spaces through random sampling is fundamental in machine learning and control theory, yet classical coverage analyses often yield conservative bounds, particularly at small failure probabilities. We study uniform random sampling on the $d$-dimensional unit hypercube and analyze the number of uncovered subcubes after discretization. By applying a concentration inequality to the uncovered-count statistic, we derive a sample complexity bound with a logarithmic dependence on the failure probability ($δ$), i.e., $M =O( \tilde{C}\ln(\frac{2\tilde{C}}δ))$, which contrasts sharply with the classical linear $1/δ$ dependence. Under standard Lipschitz and uniformity assumptions, we present a self-contained derivation and compare our result with classical coupon-collector rates. Numerical studies across dimensions, precision levels, and confidence targets indicate that our bound tracks practical coverage requirements more tightly and scales favorably as $δ\to 0$. Our findings offer a sharper theoretical tool for algorithms that rely on grid-based coverage guarantees, enabling more efficient sampling, especially in high-confidence regimes.

</details>


### [143] [Data-Driven Predictive Modeling of Microfluidic Cancer Cell Separation Using a Deterministic Lateral Displacement Device](https://arxiv.org/abs/2511.17787)
*Elizabeth Chen,Andrew Lee,Tanbir Sarowar,Xiaolin Chen*

Main category: cs.LG

TL;DR: 使用机器学习优化确定性侧向位移（DLD）设备设计参数，用于基于尺寸的肺癌细胞分离，提高循环肿瘤细胞检测效率。


<details>
  <summary>Details</summary>
Motivation: 解决循环肿瘤细胞（CTCs）检测中的稀有细胞识别挑战，减少对计算密集型模拟的依赖，实现高通量、经济高效的DLD设备设计。

Method: 采用梯度提升、k近邻、随机森林和多层感知器等机器学习模型，基于数值验证的大数据集预测粒子轨迹并识别最优设备配置。

Result: 机器学习模型成功预测粒子轨迹，识别关键设计变量，为DLD设备优化提供系统化、数据驱动的框架。

Conclusion: 这种集成方法推进了可扩展、精确的微流控系统开发，有助于癌症早期检测和个性化医疗的广泛目标。

Abstract: Deterministic Lateral Displacement (DLD) devices are widely used in microfluidics for label-free, size-based separation of particles and cells, with particular promise in isolating circulating tumor cells (CTCs) for early cancer diagnostics. This study focuses on the optimization of DLD design parameters, such as row shift fraction, post size, and gap distance, to enhance the selective isolation of lung cancer cells based on their physical properties. To overcome the challenges of rare CTC detection and reduce reliance on computationally intensive simulations, machine learning models including gradient boosting, k-nearest neighbors, random forest, and multilayer perceptron (MLP) regressors are employed. Trained on a large, numerically validated dataset, these models predict particle trajectories and identify optimal device configurations, enabling high-throughput and cost-effective DLD design. Beyond trajectory prediction, the models aid in isolating critical design variables, offering a systematic, data-driven framework for automated DLD optimization. This integrative approach advances the development of scalable and precise microfluidic systems for cancer diagnostics, contributing to the broader goals of early detection and personalized medicine.

</details>


### [144] [Physical Reinforcement Learning](https://arxiv.org/abs/2511.17789)
*Sam Dillavou,Shruti Mishra*

Main category: cs.LG

TL;DR: 将对比局部学习网络（CLLNs）从监督学习扩展到强化学习，展示了在简单RL问题上的成功应用，并讨论了该模拟系统在能耗、容错性和生物相关性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 数字计算机功耗高且对组件损坏敏感，不适合能源受限的自主智能体在不确定环境中使用。CLLNs作为模拟网络具有低功耗和物理损伤鲁棒性，但之前仅限于监督学习。

Method: 将Q-learning算法适配到模拟的CLLNs中，明确识别了实现RL工具箱所需的各种组件，包括策略函数、价值函数和回放缓冲区等。

Result: 在两种简单的强化学习问题上成功应用了基于CLLNs的Q-learning方法。

Conclusion: CLLNs系统可以放弃数字硬件所需的物理安全假设，支持在生物学中重要但在数字计算机中无意义的次要目标训练，展示了在低功耗、容错环境中的潜在应用价值。

Abstract: Digital computers are power-hungry and largely intolerant of damaged components, making them potentially difficult tools for energy-limited autonomous agents in uncertain environments. Recently developed Contrastive Local Learning Networks (CLLNs) - analog networks of self-adjusting nonlinear resistors - are inherently low-power and robust to physical damage, but were constructed to perform supervised learning. In this work we demonstrate success on two simple RL problems using Q-learning adapted for simulated CLLNs. Doing so makes explicit the components (beyond the network being trained) required to enact various tools in the RL toolbox, some of which (policy function and value function) are more natural in this system than others (replay buffer). We discuss assumptions such as the physical safety that digital hardware requires, CLLNs can forgo, and biological systems cannot rely on, and highlight secondary goals that are important in biology and trainable in CLLNs, but make little sense in digital computers.

</details>


### [145] [Semi-Supervised Federated Multi-Label Feature Selection with Fuzzy Information Measures](https://arxiv.org/abs/2511.17796)
*Afsaneh Mahanipour,Hana Khamfroush*

Main category: cs.LG

TL;DR: 提出了一种半监督联邦多标签特征选择方法SSFMLFS，在客户端只有未标记数据、服务器有少量标记数据的场景下，通过模糊信息理论和PageRank算法进行特征选择。


<details>
  <summary>Details</summary>
Motivation: 现有的多标签特征选择方法需要集中式数据，不适合分布式和联邦环境；且联邦方法通常假设客户端有标记数据，这在客户端缺乏标记能力时不可行。

Method: 客户端计算模糊相似矩阵并传输给服务器，服务器计算特征冗余度和特征-标签相关性，构建特征图并用PageRank算法对特征重要性进行排序。

Result: 在五个真实世界数据集上的实验表明，SSFMLFS在非独立同分布数据设置下，在三个不同评估指标上优于其他联邦和集中式方法。

Conclusion: SSFMLFS方法有效解决了联邦环境中客户端只有未标记数据的多标签特征选择问题，在多个领域的数据集上表现出优越性能。

Abstract: Multi-label feature selection (FS) reduces the dimensionality of multi-label data by removing irrelevant, noisy, and redundant features, thereby boosting the performance of multi-label learning models. However, existing methods typically require centralized data, which makes them unsuitable for distributed and federated environments where each device/client holds its own local dataset. Additionally, federated methods often assume that clients have labeled data, which is unrealistic in cases where clients lack the expertise or resources to label task-specific data. To address these challenges, we propose a Semi-Supervised Federated Multi-Label Feature Selection method, called SSFMLFS, where clients hold only unlabeled data, while the server has limited labeled data. SSFMLFS adapts fuzzy information theory to a federated setting, where clients compute fuzzy similarity matrices and transmit them to the server, which then calculates feature redundancy and feature-label relevancy degrees. A feature graph is constructed by modeling features as vertices, assigning relevancy and redundancy degrees as vertex weights and edge weights, respectively. PageRank is then applied to rank the features by importance. Extensive experiments on five real-world datasets from various domains, including biology, images, music, and text, demonstrate that SSFMLFS outperforms other federated and centralized supervised and semi-supervised approaches in terms of three different evaluation metrics in non-IID data distribution setting.

</details>


### [146] [Layer-Wise High-Impact Parameter Ratio Optimization in Post-Training Quantization for Large Language Models](https://arxiv.org/abs/2511.17801)
*Cuong Pham,Hoang Anh Dung,Cuong C. Nguyen,Trung Le,Gustavo Carneiro,Thanh-Toan Do*

Main category: cs.LG

TL;DR: 提出了一种基于二次优化的分层混合精度量化框架，通过动态确定每层的高影响参数比例来优化LLM量化性能，在资源约束下实现计算效率与模型精度的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有PTQ方法在极低位宽下精度损失严重，且固定比例的高影响参数保留策略忽略了层间敏感性差异，需要更精细的量化策略。

Method: 使用二次优化框架确定层特定的高影响参数比例，考虑层间依赖关系；将高影响参数量化为中等位宽，其余参数量化为极低位宽；对高影响参数应用需要大量可学习参数的高级量化方法，对其余参数使用计算高效的方法。

Result: 在相同资源约束下比固定比例方法保留更多高影响参数，实现了计算效率与模型精度的有效平衡，性能优于现有先进方法。

Conclusion: 提出的分层混合精度量化框架能够有效解决LLM在极低位宽量化时的精度损失问题，为资源受限环境下的LLM部署提供了可行方案。

Abstract: Large language models (LLMs) have significantly advanced natural language processing, but their massive parameter counts create substantial computational and memory challenges during deployment. Post-training quantization (PTQ) has emerged as a promising approach to mitigate these challenges with minimal overhead. While existing PTQ methods can effectively quantize LLMs, they experience substantial accuracy loss at extremely low bit-widths, primarily due to high-impact parameters that significantly influence quantization performance. Several approaches address these issues by identifying and retaining the high-impact parameters in FP16 format. However, they apply fixed ratios of high-impact parameters across all layers, overlooking layer-wise sensitivity variations. In this paper, we propose a quadratic optimization framework that determines layer-specific ratios of high-impact parameters while considering inter-layer dependencies. We quantize high-impact parameters to moderate bit-widths, which often result in negligible performance degradation in quantized LLMs, while the remaining parameters can be quantized to extremely low bit-widths. Under the same resource-constrained budget, this allows for preserving more high-impact parameters than methods that keep selecting a few in FP16 format. Additionally, the proposed framework allows us to leverage an advanced quantization method that often requires extensive learnable parameters solely for high-impact parameters, while applying a computationally efficient method to the rest. Our approach achieves an effective balance between computational efficiency and model accuracy while maintaining high performance compared to state-of-the-art methods.

</details>


### [147] [Adaptive Layer-Wise Transformations for Post-Training Quantization of Large Language Models](https://arxiv.org/abs/2511.17809)
*Cuong Pham,Hoang Anh Dung,Cuong C. Nguyen,Trung Le,Gustavo Carneiro,Jianfei Cai,Thanh-Toan Do*

Main category: cs.LG

TL;DR: 提出自适应变换选择框架，通过逐层选择最优变换来解决LLM量化中的异常值问题，相比固定变换方法显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有变换方法对所有层使用相同变换类型，忽略了LLM中激活和权重的异质分布特性，导致在低比特量化时性能显著下降

Method: 将变换选择建模为可微分优化问题，建立权重分布峰度与准确变换类型的关联，提出基于鲁棒z-score归一化的异常值引导层选择方法

Result: 在LLaMA系列模型上，W3A3K2V2量化设置下比当前最佳方法FlatQuant提升4.58困惑度点和2.11%的零样本准确率

Conclusion: 异质变换选择对于实现最优LLM量化是必要的，自适应方法在保持低开销的同时显著优于固定变换设置

Abstract: Large language models require significant computational resources for deployment, making quantization essential for practical applications. However, the main obstacle to effective quantization lies in systematic outliers in activations and weights, which cause substantial LLM performance degradation, especially at low-bit settings. While existing transformation-based methods like affine and rotation transformations successfully mitigate outliers, they apply the homogeneous transformation setting, i.e., using the same transformation types across all layers, ignoring the heterogeneous distribution characteristics within LLMs. In this paper, we propose an adaptive transformation selection framework that systematically determines optimal transformations on a per-layer basis. To this end, we first formulate transformation selection as a differentiable optimization problem to achieve the accurate transformation type for each layer. However, searching for optimal layer-wise transformations for every model is computationally expensive. To this end, we establish the connection between weight distribution kurtosis and accurate transformation type. Specifically, we propose an outlier-guided layer selection method using robust $z$-score normalization that achieves comparable performance to differentiable search with significantly reduced overhead. Comprehensive experiments on LLaMA family models demonstrate that our adaptive approach consistently outperforms the widely-used fixed transformation settings. For example, our method achieves an improvement of up to 4.58 perplexity points and a 2.11% gain in average six-task zero-shot accuracy under aggressive W3A3K2V2 quantization settings for the LLaMA-3-8B model compared to the current best existing method, FlatQuant, demonstrating the necessity of heterogeneous transformation selection for optimal LLM quantization.

</details>


### [148] [APRIL: Annotations for Policy evaluation with Reliable Inference from LLMs](https://arxiv.org/abs/2511.17818)
*Aishwarya Mandyam,Kalyani Limaye,Barbara E. Engelhardt,Emily Alsentzer*

Main category: cs.LG

TL;DR: 提出使用大型语言模型生成反事实标注来增强离线策略评估，解决医疗领域数据集覆盖不足的问题。


<details>
  <summary>Details</summary>
Motivation: 标准离线策略评估方法受限于行为数据集的大小和覆盖范围，而人工专家标注反事实数据成本高昂，限制了方法的可扩展性。

Method: 利用领域知识指导LLMs预测在替代治疗下关键临床特征的演变，然后通过已知奖励函数将这些预测特征转化为反事实标注。

Result: 在MIMIC-IV数据集上的实验表明，基于LLM的反事实标注在大多数情况下显著改善了OPE估计，但存在收益递减点。

Conclusion: LLM生成的反事实标注提供了一种可扩展的方法来解决医疗数据集的覆盖限制，使临床环境中的决策策略部署更安全。

Abstract: Off-policy evaluation (OPE) estimates the value of a contextual bandit policy prior to deployment. As such, OPE plays a critical role in ensuring safety in high-stakes domains such as healthcare. However, standard OPE approaches are limited by the size and coverage of the behavior dataset. While previous work has explored using expert-labeled counterfactual annotations to enhance dataset coverage, obtaining such annotations is expensive, limiting the scalability of prior approaches. We propose leveraging large language models (LLMs) to generate counterfactual annotations for OPE in medical domains. Our method uses domain knowledge to guide LLMs in predicting how key clinical features evolve under alternate treatments. These predicted features can then be transformed using known reward functions to create counterfactual annotations. We first evaluate the ability of several LLMs to predict clinical features across two patient subsets in MIMIC-IV, finding that state-of-the-art LLMs achieve comparable performance. Building on this capacity to predict clinical features, we generate LLM-based counterfactual annotations and incorporate them into an OPE estimator. Our empirical results analyze the benefits of counterfactual annotations under varying degrees of shift between the behavior and target policies. We find that in most cases, the LLM-based counterfactual annotations significantly improve OPE estimates up to a point. We provide an entropy-based metric to identify when additional annotations cease to be useful. Our results demonstrate that LLM-based counterfactual annotations offer a scalable approach for addressing coverage limitations in healthcare datasets, enabling safer deployment of decision-making policies in clinical settings.

</details>


### [149] [High-Accuracy List-Decodable Mean Estimation](https://arxiv.org/abs/2511.17822)
*Ziyun Chen,Spencer Compton,Daniel Kane,Jerry Li*

Main category: cs.LG

TL;DR: 本文研究了高精度列表可解码学习，提出了在列表大小和精度之间进行权衡的方法，特别针对高斯分布均值估计问题，证明了可以在保持较小列表大小的同时实现高精度估计。


<details>
  <summary>Details</summary>
Motivation: 现有的列表可解码学习算法虽然能获得最优的列表大小，但误差随1/α衰减较慢。本文探讨是否可以通过增加列表大小来换取更高的精度。

Method: 提出了高精度列表可解码学习框架，设计了新的可辨识性证明方法，并开发了不依赖平方和层次的算法，实现了列表大小与精度之间的权衡。

Result: 证明了对于身份协方差高斯分布的均值估计，存在大小为L=exp(O(log²(1/α)/ε²))的候选均值列表，其中至少一个元素与真实均值的ℓ₂距离不超过ε。

Conclusion: 本文首次展示了在列表可解码学习中实现高精度的可能性，为列表大小与精度之间的权衡提供了理论基础和算法实现。

Abstract: In list-decodable learning, we are given a set of data points such that an $α$-fraction of these points come from a nice distribution $D$, for some small $α\ll 1$, and the goal is to output a short list of candidate solutions, such that at least one element of this list recovers some non-trivial information about $D$. By now, there is a large body of work on this topic; however, while many algorithms can achieve optimal list size in terms of $α$, all known algorithms must incur error which decays, in some cases quite poorly, with $1 / α$. In this paper, we ask if this is inherent: is it possible to trade off list size with accuracy in list-decodable learning? More formally, given $ε> 0$, can we can output a slightly larger list in terms of $α$ and $ε$, but so that one element of this list has error at most $ε$ with the ground truth? We call this problem high-accuracy list-decodable learning. Our main result is that non-trivial high-accuracy guarantees, both information-theoretically and algorithmically, are possible for the canonical setting of list-decodable mean estimation of identity-covariance Gaussians. Specifically, we demonstrate that there exists a list of candidate means of size at most $L = \exp \left( O\left( \tfrac{\log^2 1 / α}{ε^2} \right)\right)$ so that one of the elements of this list has $\ell_2$ distance at most $ε$ to the true mean. We also design an algorithm that outputs such a list with runtime and sample complexity $n = d^{O(\log L)} + \exp \exp (\widetilde{O}(\log L))$. We do so by demonstrating a completely novel proof of identifiability, as well as a new algorithmic way of leveraging this proof without the sum-of-squares hierarchy, which may be of independent technical interest.

</details>


### [150] [A novel k-means clustering approach using two distance measures for Gaussian data](https://arxiv.org/abs/2511.17823)
*Naitik Gada*

Main category: cs.LG

TL;DR: 提出了一种结合类内距离(WCD)和类间距离(ICD)的新型k-means聚类算法，使用Calinski-Harabasz准则确定k值，在合成数据和UCI基准数据集上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统k-means聚类算法存在局限性，希望通过同时考虑类内距离和类间距离来提高聚类分析的鲁棒性和准确性。

Method: 开发了一种改进的k-means算法，将WCD和ICD作为距离度量，使用Calinski-Harabasz准则自动确定最佳聚类数k。

Result: 实验表明，该方法在数据收敛到各自簇的准确性方面优于传统k-means，能更好地将异常值聚类到正确的簇中。

Conclusion: 结合WCD和ICD的k-means算法提供了更鲁棒的聚类输出，为聚类分析开辟了新的研究方向。

Abstract: Clustering algorithms have long been the topic of research, representing the more popular side of unsupervised learning. Since clustering analysis is one of the best ways to find some clarity and structure within raw data, this paper explores a novel approach to \textit{k}-means clustering. Here we present a \textit{k}-means clustering algorithm that takes both the within cluster distance (WCD) and the inter cluster distance (ICD) as the distance metric to cluster the data into \emph{k} clusters pre-determined by the Calinski-Harabasz criterion in order to provide a more robust output for the clustering analysis. The idea with this approach is that by including both the measurement metrics, the convergence of the data into their clusters becomes solidified and more robust. We run the algorithm with some synthetically produced data and also some benchmark data sets obtained from the UCI repository. The results show that the convergence of the data into their respective clusters is more accurate by using both WCD and ICD measurement metrics. The algorithm is also better at clustering the outliers into their true clusters as opposed to the traditional \textit{k} means method. We also address some interesting possible research topics that reveal themselves as we answer the questions we initially set out to address.

</details>


### [151] [Deterministic Inference across Tensor Parallel Sizes That Eliminates Training-Inference Mismatch](https://arxiv.org/abs/2511.17826)
*Ziyang Zhang,Xinheng Ding,Jiayi Yuan,Rixin Liu,Huizi Mao,Jiarong Xing,Zirui Liu*

Main category: cs.LG

TL;DR: 提出了Tree-Based Invariant Kernels (TBIK)来解决大语言模型推理中的张量并行规模不一致导致的非确定性问题，确保在不同TP配置下获得比特级相同的推理结果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务框架在张量并行规模变化时会产生非确定性行为，这在LLM评估、多智能体系统和强化学习中会造成严重问题，特别是RL训练中训练引擎和推理引擎使用不同TP配置时的精度不匹配问题。

Method: 通过分析TP引发不一致的根本原因，设计了基于统一层次二叉树结构的TP不变矩阵乘法和归约原语，对齐GPU内和GPU间的归约顺序。

Result: 实验证实了在不同TP规模下实现了零概率发散和比特级可复现性，并在vLLM和FSDP的RL训练管道中获得了比特级相同的结果。

Conclusion: TBIK有效解决了跨TP规模的确定性推理问题，为LLM应用提供了可靠的比特级一致性保证。

Abstract: Deterministic inference is increasingly critical for large language model (LLM) applications such as LLM-as-a-judge evaluation, multi-agent systems, and Reinforcement Learning (RL). However, existing LLM serving frameworks exhibit non-deterministic behavior: identical inputs can yield different outputs when system configurations (e.g., tensor parallel (TP) size, batch size) vary, even under greedy decoding. This arises from the non-associativity of floating-point arithmetic and inconsistent reduction orders across GPUs. While prior work has addressed batch-size-related nondeterminism through batch-invariant kernels, determinism across different TP sizes remains an open problem, particularly in RL settings, where the training engine typically uses Fully Sharded Data Parallel (i.e., TP = 1) while the rollout engine relies on multi-GPU TP to maximize the inference throughput, creating a natural mismatch between the two. This precision mismatch problem may lead to suboptimal performance or even collapse for RL training. We identify and analyze the root causes of TP-induced inconsistency and propose Tree-Based Invariant Kernels (TBIK), a set of TP-invariant matrix multiplication and reduction primitives that guarantee bit-wise identical results regardless of TP size. Our key insight is to align intra- and inter-GPU reduction orders through a unified hierarchical binary tree structure. We implement these kernels in Triton and integrate them into vLLM and FSDP. Experiments confirm zero probability divergence and bit-wise reproducibility for deterministic inference across different TP sizes. Also, we achieve bit-wise identical results between vLLM and FSDP in RL training pipelines with different parallel strategy. Code is available at https://github.com/nanomaoli/llm_reproducibility.

</details>


### [152] [Unified Class and Domain Incremental Learning with Mixture of Experts for Indoor Localization](https://arxiv.org/abs/2511.17829)
*Akhil Singampalli,Sudeep Pasricha*

Main category: cs.LG

TL;DR: MOELO是一个用于室内定位的持续学习框架，首次联合处理领域增量学习和类别增量学习场景，通过混合专家架构实现轻量级、鲁棒和自适应的定位解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决室内定位中长期可靠性问题，包括移动设备硬件/软件变化导致的领域偏移，以及室内环境演变引入新位置导致的类别偏移，使静态机器学习模型随时间失效。

Method: 采用混合专家架构，按区域增量训练专家，通过等角紧框架门控机制选择专家，确保高效路由和低延迟推理，保持紧凑模型尺寸。

Result: 实验评估显示，MOELO在平均定位误差上提升达25.6倍，最差情况定位误差提升44.5倍，遗忘减少21.5倍，优于现有最先进框架。

Conclusion: MOELO提供了一个可在资源受限移动设备上部署的持续学习解决方案，适用于动态、异构的真实世界环境。

Abstract: Indoor localization using machine learning has gained traction due to the growing demand for location-based services. However, its long-term reliability is hindered by hardware/software variations across mobile devices, which shift the model's input distribution to create domain shifts. Further, evolving indoor environments can introduce new locations over time, expanding the output space to create class shifts, making static machine learning models ineffective over time. To address these challenges, we propose a novel unified continual learning framework for indoor localization called MOELO that, for the first time, jointly addresses domain-incremental and class-incremental learning scenarios. MOELO enables a lightweight, robust, and adaptive localization solution that can be deployed on resource-limited mobile devices and is capable of continual learning in dynamic, heterogeneous real-world settings. This is made possible by a mixture-of-experts architecture, where experts are incrementally trained per region and selected through an equiangular tight frame based gating mechanism ensuring efficient routing, and low-latency inference, all within a compact model footprint. Experimental evaluations show that MOELO achieves improvements of up to 25.6x in mean localization error, 44.5x in worst-case localization error, and 21.5x lesser forgetting compared to state-of-the-art frameworks across diverse buildings, mobile devices, and learning scenarios.

</details>


### [153] [Internalizing Tools as Morphisms in Graded Transformers](https://arxiv.org/abs/2511.17840)
*Tony Shaska*

Main category: cs.LG

TL;DR: 提出了一种基于分级的transformer内部符号计算方法，通过可微分路由策略实现选择性激活的符号操作，将符号计算、几何和自监督学习统一在分级transformer框架中。


<details>
  <summary>Details</summary>
Motivation: 将符号计算内部化到transformer中，避免依赖外部工具，同时保持可解释性和稀疏性。

Method: 使用分级隐藏空间和类型化块映射，通过自监督的效用函数控制激活，开发了代数几何基础和信息几何解释。

Result: 实现了在混合符号-语言任务上的选择性形态激活，能够包含先前的工具使用范式作为特例。

Conclusion: 该框架统一了符号计算、几何和自监督学习，为transformer提供了内部符号计算能力。

Abstract: We introduce a graded formulation of internal symbolic computation for transformers. The hidden space is endowed with a grading $V=\bigoplus_{g\in G}V_g$, and symbolic operations are realized as typed block maps (morphisms) $φ_{h\leftarrow g}:V_g\to V_h$ that are activated selectively by a differentiable routing policy. A self-supervised \emph{graded utility functional}, defined as the loss reduction induced by a candidate morphism, governs activation and yields sparse, interpretable behavior. We develop the algebraic and geometric foundations: an internal model category whose objects are homogeneous components and whose morphisms are admissible grade transitions; adjoint pairs encoding typed round trips; and information-geometric interpretations in terms of KL gain, mirror descent with Bregman divergences, and Fisher natural gradients. Methodologically, we specify a utility--aware routing mechanism and objective that remain fully end-to-end differentiable. Analytic case studies and lightweight sanity checks illustrate selective morphic activation on hybrid symbolic-linguistic tasks. The framework unifies symbolic computation, geometry, and self--supervised learning within the \emph{graded transformer} formalism \cite{sh-89,sh-95}, while subsuming prior external-tool paradigms (e.g., Toolformer \cite{toolformer2023}) as a special case via functorial internalization.

</details>


### [154] [Scaling Kinetic Monte-Carlo Simulations of Grain Growth with Combined Convolutional and Graph Neural Networks](https://arxiv.org/abs/2511.17848)
*Zhihui Tian,Ethan Suwandi,Tomas Oppelstrup,Vasily V. Bulatov,Joel B. Harley,Fei Zhou*

Main category: cs.LG

TL;DR: 提出了一种结合CNN自编码器和GNN的混合架构，用于高效模拟晶粒生长，显著降低了计算成本和内存使用，同时提高了准确性和时空建模能力。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在模拟晶粒生长等微观结构时面临计算成本和内存占用大的问题，特别是在处理大规模仿真单元时难以扩展。

Method: 使用基于CNN的双射自编码器压缩空间维度，在降维的潜在空间中使用GNN演化微观结构，减少了消息传递层数（从12层降至3层）。

Result: 在最大网格（160^3）上，内存使用和推理运行时间分别减少了117倍和115倍，相比纯GNN基线具有更高精度和更强的时空能力，尤其在长期测试中表现更优。

Conclusion: 该方法为模拟晶粒生长提供了高度可扩展的解决方案，结合了可扩展性和准确性，适用于长时间尺度的实际材料微观结构模拟。

Abstract: Graph neural networks (GNN) have emerged as a promising machine learning method for microstructure simulations such as grain growth. However, accurate modeling of realistic grain boundary networks requires large simulation cells, which GNN has difficulty scaling up to. To alleviate the computational costs and memory footprint of GNN, we propose a hybrid architecture combining a convolutional neural network (CNN) based bijective autoencoder to compress the spatial dimensions, and a GNN that evolves the microstructure in the latent space of reduced spatial sizes. Our results demonstrate that the new design significantly reduces computational costs with using fewer message passing layer (from 12 down to 3) compared with GNN alone. The reduction in computational cost becomes more pronounced as the spatial size increases, indicating strong computational scalability. For the largest mesh evaluated (160^3), our method reduces memory usage and runtime in inference by 117x and 115x, respectively, compared with GNN-only baseline. More importantly, it shows higher accuracy and stronger spatiotemporal capability than the GNN-only baseline, especially in long-term testing. Such combination of scalability and accuracy is essential for simulating realistic material microstructures over extended time scales. The improvements can be attributed to the bijective autoencoder's ability to compress information losslessly from spatial domain into a high dimensional feature space, thereby producing more expressive latent features for the GNN to learn from, while also contributing its own spatiotemporal modeling capability. The training was optimized to learn from the stochastic Potts Monte Carlo method. Our findings provide a highly scalable approach for simulating grain growth.

</details>


### [155] [Transformers with RL or SFT Provably Learn Sparse Boolean Functions, But Differently](https://arxiv.org/abs/2511.17852)
*Bochen Lyu,Yiyang Jia,Xiaohao Cai,Zhanxing Zhu*

Main category: cs.LG

TL;DR: 本文通过理论分析比较了强化学习(RL)和监督微调(SFT)在训练Transformer学习k-稀疏布尔函数时的机制差异，发现RL同时学习整个思维链，而SFT逐步学习思维链。


<details>
  <summary>Details</summary>
Motivation: 虽然RL和SFT都能让Transformer获得思维链推理能力，但它们的底层机制和差异在理论上仍不清楚，需要系统分析。

Method: 使用单层Transformer和中间监督来学习可递归分解为固定2-稀疏布尔函数的k-稀疏布尔函数，分析RL和SFT的学习动态。

Result: 验证了两种方法都能学习k-PARITY、k-AND和k-OR函数，但RL同时学习整个思维链，SFT逐步学习思维链。

Conclusion: 研究揭示了RL和SFT在触发Transformer思维链能力时的不同机制，为理解这两种方法提供了理论见解。

Abstract: Transformers can acquire Chain-of-Thought (CoT) capabilities to solve complex reasoning tasks through fine-tuning. Reinforcement learning (RL) and supervised fine-tuning (SFT) are two primary approaches to this end, yet their underlying mechanisms and differences remain theoretically unclear. In this work, we examine these aspects specifically for learning $k$-sparse Boolean functions with a one-layer transformer and intermediate supervision that is akin to CoT. In particular, we consider $k$-sparse Boolean functions that can be recursively decomposed into fixed 2-sparse Boolean functions. We analyze the learning dynamics of fine-tuning the transformer via either RL or SFT with CoT to identify sufficient conditions for it to provably learn these functions. We verify that these conditions hold for three basic examples, including $k$-PARITY, $k$-AND, and $k$-OR, thus demonstrating the learnability of both approaches. Notably, we reveal that RL and SFT exhibit distinct learning behaviors: RL learns the whole CoT chain simultaneously, whereas SFT learns the CoT chain step-by-step. Overall, our findings provide theoretical insights into the underlying mechanisms of RL and SFT as well as how they differ in triggering the CoT capabilities of transformers.

</details>


### [156] [Cost-Sensitive Conformal Training with Provably Controllable Learning Bounds](https://arxiv.org/abs/2511.17861)
*Xuesong Jia,Yuanjie Shi,Ziquan Liu,Yi Xu,Yan Yan*

Main category: cs.LG

TL;DR: 提出了一种新的成本敏感共形训练算法，通过真实标签的排名权重来最小化预测集大小，避免了传统方法中指示函数近似带来的不可控学习边界问题。


<details>
  <summary>Details</summary>
Motivation: 传统共形训练方法使用Sigmoid或高斯误差函数作为指示函数的替代，但这些替代函数没有统一的误差边界，导致学习边界不可控。

Method: 提出基于真实标签排名的权重策略，理论证明最小化预测集大小的期望值上界于真实标签的期望排名，开发了相应的加权目标函数。

Result: 实验验证了理论洞察的有效性，在预测效率方面优于其他共形训练方法，平均预测集大小减少了21.38%。

Conclusion: 提出的成本敏感共形训练算法通过真实标签排名权重策略，有效降低了预测集大小，且具有理论上的紧致性保证。

Abstract: Conformal prediction (CP) is a general framework to quantify the predictive uncertainty of machine learning models that uses a set prediction to include the true label with a valid probability. To align the uncertainty measured by CP, conformal training methods minimize the size of the prediction sets. A typical way is to use a surrogate indicator function, usually Sigmoid or Gaussian error function. However, these surrogate functions do not have a uniform error bound to the indicator function, leading to uncontrollable learning bounds. In this paper, we propose a simple cost-sensitive conformal training algorithm that does not rely on the indicator approximation mechanism. Specifically, we theoretically show that minimizing the expected size of prediction sets is upper bounded by the expected rank of true labels. To this end, we develop a rank weighting strategy that assigns the weight using the rank of true label on each data sample. Our analysis provably demonstrates the tightness between the proposed weighted objective and the expected size of conformal prediction sets. Extensive experiments verify the validity of our theoretical insights, and superior empirical performance over other conformal training in terms of predictive efficiency with 21.38% reduction for average prediction set size.

</details>


### [157] [Equivalence of Context and Parameter Updates in Modern Transformer Blocks](https://arxiv.org/abs/2511.17864)
*Adrian Goldwaser,Michael Munn,Javier Gonzalvo,Benoit Dherin*

Main category: cs.LG

TL;DR: 本文扩展了上下文在Transformer中可表示为MLP权重秩-1补丁的理论，证明现代LLM架构中上下文可完美映射为MLP权重和RMSNorm的补丁，并提出了基于输入/输出可控性的通用框架。


<details>
  <summary>Details</summary>
Motivation: 将基础理论扩展到现代大型语言模型的多样化架构，统一理解Transformer如何将提示转换为有效权重。

Method: 首先为Gemma风格Transformer块提供精确解析解，然后推广到多层模型，提出基于输入可控性和输出可控性的通用框架和构造性证明算法。

Result: 证明了对于任何内函数输入可控、外函数输出可控的MLP块，完美的隐式权重补丁是可能的，适用于包括门控、预/后归一化、专家混合等多种现代LLM架构。

Conclusion: 提供了一个更简单强大的视角来理解Transformer模型如何将提示转换为有效权重，该框架可广泛应用于现代LLM架构。

Abstract: Recent research has established that the impact of context in a vanilla transformer can be represented implicitly by forming a token-dependent, rank-1 patch to its MLP weights. This work extends that foundational theory to the diverse architectures of modern Large Language Models. We first demonstrate a precise, analytical solution for a Gemma-style transformer block, proving that the entire effect of a context can be perfectly mapped to rank-1 patches on its MLP weight matrices and a patch to the RMSNorm scale. We then generalize this result, providing a constructive proof and algorithm for multi-layer models. To unify these findings, we introduce a general framework centered on two core properties: input controllability and output controllability. We prove that a perfect implicit weight patch is possible for any MLP block where the inner function is input-controllable and the outer function is output-controllable. This provides a simpler and more powerful lens for understanding how transformer models transmute prompts into effective weights. This setup generalizes to a wide range of modern LLM architectures including gating, pre-/post-norm, mixture of experts and sequential/parallel transformer blocks.

</details>


### [158] [The Horcrux: Mechanistically Interpretable Task Decomposition for Detecting and Mitigating Reward Hacking in Embodied AI Systems](https://arxiv.org/abs/2511.17869)
*Subramanyam Sahoo,Jared Junkin*

Main category: cs.LG

TL;DR: 提出了MITD分层Transformer架构，通过任务分解检测和缓解奖励黑客攻击，在1000个HH-RLHF样本上实验显示12-25步分解深度可将奖励黑客频率降低34%。


<details>
  <summary>Details</summary>
Motivation: 解决具身AI代理通过奖励黑客攻击利用奖励信号缺陷，获得高代理分数但未能实现真正目标的问题。

Method: 引入机械可解释任务分解(MITD)架构，包含规划器、协调器和执行器模块，将任务分解为可解释子任务，并生成注意力瀑布图和神经通路流程图等诊断可视化。

Result: 实验表明分解深度12-25步可在四种故障模式下将奖励黑客频率降低34%。

Conclusion: 机械基础分解比事后行为监控更有效地检测奖励黑客，提出了新的检测范式。

Abstract: Embodied AI agents exploit reward signal flaws through reward hacking, achieving high proxy scores while failing true objectives. We introduce Mechanistically Interpretable Task Decomposition (MITD), a hierarchical transformer architecture with Planner, Coordinator, and Executor modules that detects and mitigates reward hacking. MITD decomposes tasks into interpretable subtasks while generating diagnostic visualizations including Attention Waterfall Diagrams and Neural Pathway Flow Charts. Experiments on 1,000 HH-RLHF samples reveal that decomposition depths of 12 to 25 steps reduce reward hacking frequency by 34 percent across four failure modes. We present new paradigms showing that mechanistically grounded decomposition offers a more effective way to detect reward hacking than post-hoc behavioral monitoring.

</details>


### [159] [Generative Adversarial Post-Training Mitigates Reward Hacking in Live Human-AI Music Interaction](https://arxiv.org/abs/2511.17879)
*Yusong Wu,Stephen Brade,Teng Ma,Tia-Jane Fowler,Enning Yang,Berker Banar,Aaron Courville,Natasha Jaques,Cheng-Zhi Anna Huang*

Main category: cs.LG

TL;DR: 提出一种对抗训练方法来缓解RL后训练中的奖励黑客问题，用于旋律到和弦伴奏任务，通过共同进化的判别器保持输出多样性


<details>
  <summary>Details</summary>
Motivation: 实时即兴演奏需要实时协调和适应，同时保持多样性以维持创造性流程，但RL后训练常因利用基于连贯性的奖励而减少输出多样性（奖励黑客问题）

Method: 在策略生成轨迹上使用对抗训练方法，共同进化的判别器区分策略轨迹与数据分布，策略同时最大化判别器输出和连贯性奖励以防止崩溃到平凡输出

Result: 在模拟和真实交互系统中评估，显示输出多样性、和声连贯性、适应速度和用户代理性得到改善

Conclusion: 为生成序列模型的RL后训练提供了一种简单有效的缓解奖励黑客问题的方法

Abstract: Most applications of generative AI involve a sequential interaction in which a person inputs a prompt and waits for a response, and where reaction time and adaptivity are not important factors. In contrast, live jamming is a collaborative interaction that requires real-time coordination and adaptation without access to the other player's future moves, while preserving diversity to sustain a creative flow. Reinforcement learning post-training enables effective adaptation through on-policy interaction, yet it often reduces output diversity by exploiting coherence-based rewards. This collapse, known as ``reward hacking'', affects many RL post-training pipelines, but is especially harmful in live jamming, where musical creativity relies on dynamic variation and mutual responsiveness. In this paper, we propose a novel adversarial training method on policy-generated trajectories to mitigate reward hacking in RL post-training for melody-to-chord accompaniment. A co-evolving discriminator separates policy trajectories from the data distribution, while the policy maximizes the discriminator output in addition to coherence rewards to prevent collapse to trivial outputs. We evaluate accompaniment quality and output diversity in simulation with both fixed test melodies and learned melody agents, and we conduct a user study with the model deployed in a real-time interactive system with expert musicians. Quantitative evaluation and user feedback demonstrate improved output diversity, harmonic coherence, adaptation speed and user agency. Our results demonstrate a simple yet effective method to mitigate reward hacking in RL post-training of generative sequence models.

</details>


### [160] [Statistically-Guided Dual-Domain Meta-Learning with Adaptive Multi-Prototype Aggregation for Distributed Fiber Optic Sensing](https://arxiv.org/abs/2511.17902)
*Yifan He,Haodong Zhang,Qiuheng Song,Lin Lei,Zhenxuan Zeng,Haoyang He,Hongyan Wu*

Main category: cs.LG

TL;DR: 提出了DUPLE元学习框架，解决分布式光纤传感中因光纤部署类型不同导致的信号模式变化、新场景标注数据稀缺以及类内多样性不足等挑战。


<details>
  <summary>Details</summary>
Motivation: 分布式光纤传感在实际应用中面临三个关键问题：不同部署类型下的信号模式差异导致域偏移、新部署场景标注数据稀缺、源域内数据不足难以捕捉类内多样性。

Method: 提出DUPLE元学习框架，包括双域多原型学习器融合时频域特征、统计引导网络从原始统计特征推断域重要性和原型敏感性、查询感知原型聚合模块自适应选择和组合相关原型。

Result: 在跨部署DFOS数据集上的广泛实验表明，该方法在域泛化设置下显著优于基线方法，能够以最少标注数据实现跨不同光纤配置的鲁棒事件识别。

Conclusion: DUPLE框架有效解决了DFOS系统中的域偏移和数据稀缺问题，为跨部署场景下的活动识别提供了鲁棒解决方案。

Abstract: Distributed Fiber Optic Sensing (DFOS) has shown strong potential in perimeter security due to its capability of monitoring vibration events across long distances with fine spatial resolution. However, practical DFOS systems face three critical challenges: (1) signal patterns of the same activity vary drastically under different fiber deployment types (e.g., underground, wall-mounted), causing domain shift; (2) labeled data in new deployment scenarios is often scarce or entirely unavailable, limiting model adaptability; and (3) even within source domains, data scarcity makes it difficult to capture intra-class diversity for robust learning.
  To address these challenges, we propose a novel meta-learning framework, DUPLE, for cross-deployment DFOS activity identification. First, a dual-domain multi-prototype learner fuses temporal and frequency domain features, enhancing the model's generalization ability under signal distribution shifts. Second, a Statistical Guided Network (SGN) infers domain importance and prototype sensitivity from raw statistical features, providing data-driven prior information for learning in unlabeled or unseen domains. Third, a query-aware prototype aggregation module adaptively selects and combines relevant prototypes, thereby improving classification performance even with limited data.
  Extensive experiments on cross-deployment DFOS datasets demonstrate that our method significantly outperforms baseline approaches in domain generalization settings, enabling robust event recognition across diverse fiber configurations with minimal labeled data.

</details>


### [161] [Mitigating Catastrophic Forgetting in Streaming Generative and Predictive Learning via Stateful Replay](https://arxiv.org/abs/2511.17936)
*Wenzhang Du*

Main category: cs.LG

TL;DR: 该论文研究了在内存约束下使用状态回放机制来缓解持续学习中的灾难性遗忘问题，通过统一的梯度对齐分析框架比较了顺序微调和回放方法在多种流式学习场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 在内存受限的流式学习环境中，传统的顺序微调方法容易遭受灾难性遗忘，特别是当后续数据来自不同子群体或任务时。虽然有限缓冲区的回放方法是一种简单替代方案，但其在生成和预测目标下的行为尚未得到充分理解。

Method: 提出了一个统一的梯度对齐分析框架，将顺序微调和回放都视为理想联合目标的随机梯度方法。在六个流式场景（基于旋转MNIST、电力负荷图和航班延误数据）上评估了单一回放机制，使用匹配的训练预算和三个随机种子。

Result: 在异构多任务流上，回放方法将平均遗忘减少了2-3倍；而在良性的时间序列流上，两种方法表现相似。

Conclusion: 状态回放机制是流式环境中持续学习的一个强大而简单的基准方法，特别是在处理异构多任务流时效果显著。

Abstract: Many deployed learning systems must update models on streaming data under memory constraints. The default strategy, sequential fine-tuning on each new phase, is architecture-agnostic but often suffers catastrophic forgetting when later phases correspond to different sub-populations or tasks. Replay with a finite buffer is a simple alternative, yet its behaviour across generative and predictive objectives is not well understood. We present a unified study of stateful replay for streaming autoencoding, time series forecasting, and classification. We view both sequential fine-tuning and replay as stochastic gradient methods for an ideal joint objective, and use a gradient alignment analysis to show when mixing current and historical samples should reduce forgetting. We then evaluate a single replay mechanism on six streaming scenarios built from Rotated MNIST, ElectricityLoadDiagrams 2011-2014, and Airlines delay data, using matched training budgets and three seeds. On heterogeneous multi task streams, replay reduces average forgetting by a factor of two to three, while on benign time based streams both methods perform similarly. These results position stateful replay as a strong and simple baseline for continual learning in streaming environments.

</details>


### [162] [On Transportability for Structural Causal Bandits](https://arxiv.org/abs/2511.17953)
*Min Woo Park,Sanghack Lee*

Main category: cs.LG

TL;DR: 本文研究了具有可迁移性的结构因果赌博机问题，通过融合来自源环境的先验知识来增强部署环境中的学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有结构因果赌博机框架虽然能利用因果知识优化动作空间，但缺乏从不同条件（观测或实验）和异构环境中收集的数据集迁移信息的指导方法。

Method: 利用跨环境的不变性，将源环境的先验知识融合到部署环境中，开发了一种能利用先验数据信息性的赌博机算法。

Result: 该算法实现了亚线性遗憾界，且明确依赖于先验数据的信息性，可能优于仅依赖在线学习的标准赌博机方法。

Conclusion: 通过利用跨环境的不变性，可以持续改进学习效果，融合先验知识的结构因果赌博机算法在部署环境中具有优越性能。

Abstract: Intelligent agents equipped with causal knowledge can optimize their action spaces to avoid unnecessary exploration. The structural causal bandit framework provides a graphical characterization for identifying actions that are unable to maximize rewards by leveraging prior knowledge of the underlying causal structure. While such knowledge enables an agent to estimate the expected rewards of certain actions based on others in online interactions, there has been little guidance on how to transfer information inferred from arbitrary combinations of datasets collected under different conditions -- observational or experimental -- and from heterogeneous environments. In this paper, we investigate the structural causal bandit with transportability, where priors from the source environments are fused to enhance learning in the deployment setting. We demonstrate that it is possible to exploit invariances across environments to consistently improve learning. The resulting bandit algorithm achieves a sub-linear regret bound with an explicit dependence on informativeness of prior data, and it may outperform standard bandit approaches that rely solely on online learning.

</details>


### [163] [Hybrid LSTM and PPO Networks for Dynamic Portfolio Optimization](https://arxiv.org/abs/2511.17963)
*Jun Kevin,Pujianto Yugopuspito*

Main category: cs.LG

TL;DR: 提出融合LSTM预测和PPO强化学习的混合投资组合优化框架，在动态市场环境中实现更高收益和更强韧性


<details>
  <summary>Details</summary>
Motivation: 传统投资组合优化方法难以适应非平稳市场环境，需要结合深度学习和强化学习来捕捉时序依赖并动态调整资产配置

Method: 使用LSTM网络进行时序预测，结合PPO强化学习在连续动作空间中自适应优化投资组合权重

Result: 在多种资产数据集上测试，混合模型相比等权重、指数型和单一模型方法获得更高年化收益和夏普比率，在市场波动中表现更稳健

Conclusion: LSTM+PPO混合架构为动态投资组合优化提供了稳健的AI驱动框架，在非平稳市场环境下具有显著优势

Abstract: This paper introduces a hybrid framework for portfolio optimization that fuses Long Short-Term Memory (LSTM) forecasting with a Proximal Policy Optimization (PPO) reinforcement learning strategy. The proposed system leverages the predictive power of deep recurrent networks to capture temporal dependencies, while the PPO agent adaptively refines portfolio allocations in continuous action spaces, allowing the system to anticipate trends while adjusting dynamically to market shifts. Using multi-asset datasets covering U.S. and Indonesian equities, U.S. Treasuries, and major cryptocurrencies from January 2018 to December 2024, the model is evaluated against several baselines, including equal-weight, index-style, and single-model variants (LSTM-only and PPO-only). The framework's performance is benchmarked against equal-weighted, index-based, and single-model approaches (LSTM-only and PPO-only) using annualized return, volatility, Sharpe ratio, and maximum drawdown metrics, each adjusted for transaction costs. The results indicate that the hybrid architecture delivers higher returns and stronger resilience under non-stationary market regimes, suggesting its promise as a robust, AI-driven framework for dynamic portfolio optimization.

</details>


### [164] [Uncertainty-Aware Federated Learning for Cyber-Resilient Microgrid Energy Management](https://arxiv.org/abs/2511.17968)
*Oluleke Babayomi,Dong-Seong Kim*

Main category: cs.LG

TL;DR: 提出了一种集成的网络弹性框架，结合联邦LSTM光伏预测与两阶段级联虚假数据注入攻击检测和能源管理系统优化，在保护数据隐私的同时实现攻击弹性的储能调度。


<details>
  <summary>Details</summary>
Motivation: 在遭受网络攻击条件下维持微电网能源管理系统的经济效率和运行可靠性仍然具有挑战性。现有方法通常假设测量数据正常、预测不确定性未量化，且未能缓解针对可再生能源预测的恶意攻击。

Method: 集成联邦长短期记忆网络光伏预测与新型两阶段级联虚假数据注入攻击检测和能源管理系统优化，结合自编码器重构误差与预测不确定性量化，实现攻击弹性储能调度。

Result: 在极端虚假数据攻击条件下（导致58%预测性能下降和16.9%运营成本增加），该框架将误报检测减少70%，恢复93.7%的预测性能损失，实现5%运营成本节约，缓解34.7%攻击导致的经济损失。

Conclusion: 基于多信号融合的精确级联检测优于单信号方法，验证了去中心化微电网安全与性能的协同效应。

Abstract: Maintaining economic efficiency and operational reliability in microgrid energy management systems under cyberattack conditions remains challenging. Most approaches assume non-anomalous measurements, make predictions with unquantified uncertainties, and do not mitigate malicious attacks on renewable forecasts for energy management optimization. This paper presents a comprehensive cyber-resilient framework integrating federated Long Short-Term Memory-based photovoltaic forecasting with a novel two-stage cascade false data injection attack detection and energy management system optimization. The approach combines autoencoder reconstruction error with prediction uncertainty quantification to enable attack-resilient energy storage scheduling while preserving data privacy. Extreme false data attack conditions were studied that caused 58% forecast degradation and 16.9\% operational cost increases. The proposed integrated framework reduced false positive detections by 70%, recovered 93.7% of forecasting performance losses, and achieved 5\% operational cost savings, mitigating 34.7% of attack-induced economic losses. Results demonstrate that precision-focused cascade detection with multi-signal fusion outperforms single-signal approaches, validating security-performance synergy for decentralized microgrids.

</details>


### [165] [Controllability Analysis of State Space-based Language Model](https://arxiv.org/abs/2511.17970)
*Mohamed Mabrok,Yalda Zafari*

Main category: cs.LG

TL;DR: 提出了Influence Score作为衡量Mamba状态空间模型中token影响力的指标，通过实验验证该指标能有效反映模型容量、架构模式，并可作为诊断工具比较不同SSM模型。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型（特别是Mamba）已成为序列建模的强大架构，但其内部动态机制相比基于注意力的模型仍缺乏深入理解，需要开发有效的解释性指标。

Method: 引入Influence Score这一基于可控性的度量，从Mamba的离散状态空间参数推导得出，通过类似系统可观测性的反向递推计算，量化token对后续状态和输出的影响强度。在三个Mamba变体上进行六项实验验证。

Result: 发现三个主要洞察：(1) Influence Score随模型规模和训练数据增加而提升，反映模型容量；(2) Mamba展现一致的架构模式，包括近因偏差和影响力集中在中后层；(3) 仅在大规模模型中观察到涌现行为，如mamba-2.8b-slimpj优先处理内容词并在噪声存在时减少内部影响。

Conclusion: Influence Score可作为实用的诊断工具，用于解释和比较基于状态空间模型的语言模型。

Abstract: State-space models (SSMs), particularly Mamba, have become powerful architectures for sequence modeling, yet their internal dynamics remain poorly understood compared to attention-based models. We introduce and validate the Influence Score, a controllability-based metric derived from the discretized state-space parameters of Mamba and computed through a backward recurrence analogous to system observability. The score quantifies how strongly a token at position k affects all later states and outputs. We evaluate this measure across three Mamba variants: mamba-130m, mamba-2.8b, and mamba-2.8b-slimpj, using six experiments that test its sensitivity to temperature, prompt complexity, token type, layer depth, token position, and input perturbations. The results show three main insights: (1) the Influence Score increases with model size and training data, reflecting model capacity; (2) Mamba exhibits consistent architectural patterns, including recency bias and concentrated influence in mid-to-late layers; and (3) emergent behaviors appear only at scale, with mamba-2.8b-slimpj uniquely prioritizing content words and reducing internal influence in the presence of noise. These findings establish the Influence Score as a practical diagnostic tool for interpreting and comparing SSM-based language models.

</details>


### [166] [Federated Anomaly Detection and Mitigation for EV Charging Forecasting Under Cyberattacks](https://arxiv.org/abs/2511.17978)
*Oluleke Babayomi,Dong-Seong Kim*

Main category: cs.LG

TL;DR: 提出了一种新颖的异常弹性联邦学习框架，用于电动汽车充电基础设施的网络安全保护和需求预测，在保护数据隐私的同时检测网络攻击并维持可信的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有电动汽车充电基础设施面临日益严重的网络安全威胁，现有预测技术缺乏结合鲁棒异常缓解解决方案和数据隐私保护的能力。

Method: 集成三个关键创新：基于LSTM自编码器的分布式异常检测、基于插值的异常数据缓解以保持时间连续性、以及联邦LSTM网络实现无需集中数据聚合的协作学习。

Result: 联邦方法相比集中式模型性能提升15.2%，集成网络攻击检测和缓解系统恢复47.9%的攻击导致性能下降，保持91.3%的精确度和1.21%的低误报率。

Conclusion: 该架构能够增强电动汽车基础设施规划、隐私保护协作预测、网络安全弹性以及分布式充电网络中恶意威胁的快速恢复。

Abstract: Electric Vehicle (EV) charging infrastructure faces escalating cybersecurity threats that can severely compromise operational efficiency and grid stability. Existing forecasting techniques are limited by the lack of combined robust anomaly mitigation solutions and data privacy preservation. Therefore, this paper addresses these challenges by proposing a novel anomaly-resilient federated learning framework that simultaneously preserves data privacy, detects cyber-attacks, and maintains trustworthy demand prediction accuracy under adversarial conditions. The proposed framework integrates three key innovations: LSTM autoencoder-based distributed anomaly detection deployed at each federated client, interpolation-based anomalous data mitigation to preserve temporal continuity, and federated Long Short-Term Memory (LSTM) networks that enable collaborative learning without centralized data aggregation. The framework is validated on real-world EV charging infrastructure datasets combined with real-world DDoS attack datasets, providing robust validation of the proposed approach under realistic threat scenarios. Experimental results demonstrate that the federated approach achieves superior performance compared to centralized models, with 15.2% improvement in R2 accuracy while maintaining data locality. The integrated cyber-attack detection and mitigation system produces trustworthy datasets that enhance prediction reliability, recovering 47.9% of attack-induced performance degradation while maintaining exceptional precision (91.3%) and minimal false positive rates (1.21%). The proposed architecture enables enhanced EV infrastructure planning, privacy-preserving collaborative forecasting, cybersecurity resilience, and rapid recovery from malicious threats across distributed charging networks.

</details>


### [167] [An Adaptive Resonance Theory-based Topological Clustering Algorithm with a Self-Adjusting Vigilance Parameter](https://arxiv.org/abs/2511.17983)
*Naoki Masuyama,Yuichiro Toda,Yusuke Nojima,Hisao Ishibuchi*

Main category: cs.LG

TL;DR: 提出基于自适应共振理论(ART)的拓扑聚类算法，通过多样性驱动的适应机制自动调整重计算间隔和警戒阈值，实现无超参数学习，在动态环境中保持聚类稳定性和连续性。


<details>
  <summary>Details</summary>
Motivation: 解决静态和非静态设置中的聚类问题，需要能够适应分布变化同时保持已学习聚类结构的模型，特别是在数据分布随时间演化的动态环境中。

Method: 基于自适应共振理论(ART)的拓扑聚类算法，采用多样性驱动的适应机制自动调整重计算间隔和警戒阈值，实现超参数自由学习。

Result: 在24个真实世界数据集上的实验表明，该算法在聚类性能和持续学习能力方面均优于最先进方法。

Conclusion: 提出的参数适应机制能有效减轻灾难性遗忘，在演化数据流中保持一致的聚类性能。

Abstract: Clustering in stationary and nonstationary settings, where data distributions remain static or evolve over time, requires models that can adapt to distributional shifts while preserving previously learned cluster structures. This paper proposes an Adaptive Resonance Theory (ART)-based topological clustering algorithm that autonomously adjusts its recalculation interval and vigilance threshold through a diversity-driven adaptation mechanism. This mechanism enables hyperparameter-free learning that maintains cluster stability and continuity in dynamic environments. Experiments on 24 real-world datasets demonstrate that the proposed algorithm outperforms state-of-the-art methods in both clustering performance and continual learning capability. These results highlight the effectiveness of the proposed parameter adaptation in mitigating catastrophic forgetting and maintaining consistent clustering in evolving data streams. Source code is available at https://github.com/Masuyama-lab/IDAT

</details>


### [168] [Escaping Optimization Stagnation: Taking Steps Beyond Task Arithmetic via Difference Vectors](https://arxiv.org/abs/2511.17987)
*Jinping Wang,Zhiqiang Gao,Dinggen Zhang,Zhiwu Xie*

Main category: cs.LG

TL;DR: 提出了基于差异向量的各向异性缩放迭代算法(DV-BASI)，通过使用优化过程中的历史移动作为定向扰动，克服任务算术方法的优化停滞问题，实现连续优化过程。


<details>
  <summary>Details</summary>
Motivation: 当前预训练模型编辑方法面临高计算成本和有限可扩展性的挑战，任务算术方法虽然前景广阔，但由于优化停滞机制有限，其潜力尚未充分发掘。

Method: 引入差异向量概念，作为任务向量的广义形式，源自优化过程中的历史移动。使用差异向量作为定向扰动，提出DV-BASI算法，无需额外模块即可实现任务算术方法的连续优化。

Result: DV-BASI在多任务模型合并中的平均性能甚至可能超过单独微调的模型。该方法与任务算术方法和先进优化技术结合，在监督和无监督评估协议上均达到最先进性能。

Conclusion: 差异向量提供了有效的优化方向，DV-BASI算法能够以少量可学习参数实现表达性搜索方向，形成可扩展框架，显著提升了任务算术方法的性能。

Abstract: Current methods for editing pre-trained models face significant challenges, primarily high computational costs and limited scalability. Task arithmetic has recently emerged as a promising solution, using simple arithmetic operations-addition and negation-based on task vectors which are the differences between fine-tuned and pre-trained model weights, to efficiently modify model behavior. However, the full potential of task arithmetic remains underexplored, primarily due to limited mechanisms for overcoming optimization stagnation. To address this challenge, we introduce the notion of difference vector, a generalized form of task vectors derived from the historical movements during optimization. Using difference vectors as directed perturbations, we propose the Difference Vector-based Anisotropic Scaling Iterative algorithm (DV-BASI) to enable a continuous optimization process for task arithmetic methods without relying on any additional modules or components. Notably, by leveraging escapability and directional advantages of difference vectors, the average performance on different tasks of the multi-task model merged by DV-BASI may even outperform models individually fine-tuned. Based on this observation, we extend the application of difference vectors to a feasible fine-tuning method for single-task models. On the practical side, DV-BASI allows expressive searching directions with few learnable parameters and forms a scalable framework. We also integrate DV-BASI with task arithmetic methods and advanced optimization techniques to achieve state-of-the-art performance on both supervised and unsupervised evaluation protocols.

</details>


### [169] [Privacy Auditing of Multi-domain Graph Pre-trained Model under Membership Inference Attacks](https://arxiv.org/abs/2511.17989)
*Jiayi Luo,Qingyun Sun,Yuecen Wei,Haonan Yuan,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: MGP-MIA是一个针对多领域图预训练模型的成员推理攻击框架，通过机器遗忘放大成员信号、增量学习构建影子模型、基于相似度的推理机制，揭示了多领域图预训练的隐私风险。


<details>
  <summary>Details</summary>
Motivation: 多领域图预训练虽然提高了图神经网络的泛化能力，但其在成员推理攻击下的隐私风险尚未被充分探索。由于预训练模型泛化能力增强、影子数据集代表性不足、成员信号弱化等因素，传统MIA方法难以有效攻击多领域图预训练模型。

Method: 提出MGP-MIA框架：1）通过机器遗忘放大目标模型的过拟合特性；2）使用增量学习在有限影子图上构建可靠影子模型；3）基于样本与正负样本相似度进行成员推理。

Result: 大量实验证明MGP-MIA的有效性，成功揭示了多领域图预训练存在的隐私风险。

Conclusion: 多领域图预训练模型存在严重的隐私泄露风险，MGP-MIA框架能够有效实施成员推理攻击，为图基础模型的隐私保护提供了重要启示。

Abstract: Multi-domain graph pre-training has emerged as a pivotal technique in developing graph foundation models. While it greatly improves the generalization of graph neural networks, its privacy risks under membership inference attacks (MIAs), which aim to identify whether a specific instance was used in training (member), remain largely unexplored. However, effectively conducting MIAs against multi-domain graph pre-trained models is a significant challenge due to: (i) Enhanced Generalization Capability: Multi-domain pre-training reduces the overfitting characteristics commonly exploited by MIAs. (ii) Unrepresentative Shadow Datasets: Diverse training graphs hinder the obtaining of reliable shadow graphs. (iii) Weakened Membership Signals: Embedding-based outputs offer less informative cues than logits for MIAs. To tackle these challenges, we propose MGP-MIA, a novel framework for Membership Inference Attacks against Multi-domain Graph Pre-trained models. Specifically, we first propose a membership signal amplification mechanism that amplifies the overfitting characteristics of target models via machine unlearning. We then design an incremental shadow model construction mechanism that builds a reliable shadow model with limited shadow graphs via incremental learning. Finally, we introduce a similarity-based inference mechanism that identifies members based on their similarity to positive and negative samples. Extensive experiments demonstrate the effectiveness of our proposed MGP-MIA and reveal the privacy risks of multi-domain graph pre-training.

</details>


### [170] [Learning Rate Scheduling with Matrix Factorization for Private Training](https://arxiv.org/abs/2511.17994)
*Nikita P. Kalinin,Joel Daniel Andersson*

Main category: cs.LG

TL;DR: 本文研究在差分隐私下使用随机梯度下降进行模型训练，重点关注学习率调度和相关性噪声的影响，提出了学习率感知的矩阵分解方法，在理论和实验上都优于传统的prefix-sum方法。


<details>
  <summary>Details</summary>
Motivation: 现有关于相关性噪声（特别是矩阵分解）的理论研究主要关注恒定学习率的情况，而实践中广泛使用学习率调度来加速训练和改善收敛性。本文旨在填补这一理论空白。

Method: 推导了在单轮和多轮训练设置下，针对广泛学习率调度类别的通用上下界。基于这些结果，提出了学习率感知的矩阵分解方法，相比prefix-sum分解在MaxSE和MeanSE误差指标上都有改进。

Result: 理论分析产生了适合实际部署的内存高效构造，在CIFAR-10和IMDB数据集上的实验证实，调度感知的分解方法在私有训练中提高了准确性。

Conclusion: 学习率感知的矩阵分解方法在差分隐私模型训练中优于传统的prefix-sum方法，为实际应用提供了更有效的解决方案。

Abstract: We study differentially private model training with stochastic gradient descent under learning rate scheduling and correlated noise. Although correlated noise, in particular via matrix factorizations, has been shown to improve accuracy, prior theoretical work focused primarily on the prefix-sum workload. That workload assumes a constant learning rate, whereas in practice learning rate schedules are widely used to accelerate training and improve convergence. We close this gap by deriving general upper and lower bounds for a broad class of learning rate schedules in both single- and multi-epoch settings. Building on these results, we propose a learning-rate-aware factorization that achieves improvements over prefix-sum factorizations under both MaxSE and MeanSE error metrics. Our theoretical analysis yields memory-efficient constructions suitable for practical deployment, and experiments on CIFAR-10 and IMDB datasets confirm that schedule-aware factorizations improve accuracy in private training.

</details>


### [171] [Reward Engineering for Spatial Epidemic Simulations: A Reinforcement Learning Platform for Individual Behavioral Learning](https://arxiv.org/abs/2511.18000)
*Radman Rakhshandehroo,Daniel Coombs*

Main category: cs.LG

TL;DR: ContagionRL是一个用于空间流行病模拟的强化学习平台，专注于奖励函数设计对智能体生存策略的影响研究。


<details>
  <summary>Details</summary>
Motivation: 传统基于智能体的模型依赖固定行为规则，无法系统评估奖励函数设计如何影响不同流行病场景下的学习生存策略。

Method: 平台整合空间SIRS+D流行病学模型与可配置环境参数，评估五种不同奖励设计（从稀疏生存奖励到新型势场方法）在多种RL算法（PPO、SAC、A2C）下的表现。

Result: 系统性消融研究显示方向性引导和明确依从激励是稳健策略学习的关键要素。势场奖励方法在多种条件下表现最优，智能体学会最大程度遵守非药物干预措施并发展复杂空间规避策略。

Conclusion: 奖励函数选择显著影响智能体行为和生存结果，ContagionRL是研究流行病背景下适应性行为响应的有效平台，突显了奖励设计、信息结构和环境可预测性在学习中的重要性。

Abstract: We present ContagionRL, a Gymnasium-compatible reinforcement learning platform specifically designed for systematic reward engineering in spatial epidemic simulations. Unlike traditional agent-based models that rely on fixed behavioral rules, our platform enables rigorous evaluation of how reward function design affects learned survival strategies across diverse epidemic scenarios. ContagionRL integrates a spatial SIRS+D epidemiological model with configurable environmental parameters, allowing researchers to stress-test reward functions under varying conditions including limited observability, different movement patterns, and heterogeneous population dynamics. We evaluate five distinct reward designs, ranging from sparse survival bonuses to a novel potential field approach, across multiple RL algorithms (PPO, SAC, A2C). Through systematic ablation studies, we identify that directional guidance and explicit adherence incentives are critical components for robust policy learning. Our comprehensive evaluation across varying infection rates, grid sizes, visibility constraints, and movement patterns reveals that reward function choice dramatically impacts agent behavior and survival outcomes. Agents trained with our potential field reward consistently achieve superior performance, learning maximal adherence to non-pharmaceutical interventions while developing sophisticated spatial avoidance strategies. The platform's modular design enables systematic exploration of reward-behavior relationships, addressing a knowledge gap in models of this type where reward engineering has received limited attention. ContagionRL is an effective platform for studying adaptive behavioral responses in epidemic contexts and highlight the importance of reward design, information structure, and environmental predictability in learning.

</details>


### [172] [Understanding Private Learning From Feature Perspective](https://arxiv.org/abs/2511.18006)
*Meng Ding,Mingxi Lei,Shaopeng Fu,Shaowei Wang,Di Wang,Jinhui Xu*

Main category: cs.LG

TL;DR: 提出了首个从特征学习角度分析差分隐私训练的理论框架，揭示了隐私训练需要更高信噪比，且会继承非隐私训练中的噪声记忆问题


<details>
  <summary>Details</summary>
Motivation: 尽管利用预训练模型特征增强DP-SGD训练取得了显著经验进展，但隐私学习中特征动态的理论理解仍然不足，现有DP分析忽视了标签相关特征信号与标签无关噪声的关键区别

Method: 基于多补丁数据结构，使用带多项式ReLU激活的两层CNN，通过噪声梯度下降理论分析私有训练中的特征信号学习和数据噪声记忆

Result: 发现：(1)有效私有信号学习需要比非私有训练更高的信噪比；(2)当非私有学习中出现数据噪声记忆时，私有学习也会出现，导致训练损失小但泛化性能差

Conclusion: 研究强调了私有学习的挑战，并证明了特征增强提高信噪比的益处，在合成和真实数据集上的实验验证了理论发现

Abstract: Differentially private Stochastic Gradient Descent (DP-SGD) has become integral to privacy-preserving machine learning, ensuring robust privacy guarantees in sensitive domains. Despite notable empirical advances leveraging features from non-private, pre-trained models to enhance DP-SGD training, a theoretical understanding of feature dynamics in private learning remains underexplored. This paper presents the first theoretical framework to analyze private training through a feature learning perspective. Building on the multi-patch data structure from prior work, our analysis distinguishes between label-dependent feature signals and label-independent noise, a critical aspect overlooked by existing analyses in the DP community. Employing a two-layer CNN with polynomial ReLU activation, we theoretically characterize both feature signal learning and data noise memorization in private training via noisy gradient descent. Our findings reveal that (1) Effective private signal learning requires a higher signal-to-noise ratio (SNR) compared to non-private training, and (2) When data noise memorization occurs in non-private learning, it will also occur in private learning, leading to poor generalization despite small training loss. Our findings highlight the challenges of private learning and prove the benefit of feature enhancement to improve SNR. Experiments on synthetic and real-world datasets also validate our theoretical findings.

</details>


### [173] [Curvature-Aware Safety Restoration In LLMs Fine-Tuning](https://arxiv.org/abs/2511.18039)
*Thong Bach,Thanh Nguyen-Tang,Dung Nguyen,Thao Minh Le,Truyen Tran*

Main category: cs.LG

TL;DR: 该论文提出了一种曲率感知对齐恢复方法，利用影响函数和二阶优化来选择性增加有害输入上的损失，同时保持任务性能，有效减少有害响应而不影响模型效用。


<details>
  <summary>Details</summary>
Motivation: 微调大型语言模型用于下游任务时往往会损害安全对齐性，即使使用参数高效方法如LoRA。研究发现微调模型在有害内容上的损失景观几何结构得以保留，表明安全行为并未被擦除而是转移到参数空间中影响力较小的区域。

Method: 基于影响函数和二阶优化的曲率感知对齐恢复方法，通过利用基础模型和微调模型之间的共享几何结构，选择性增加有害输入上的损失，同时保留任务相关性能。

Result: 在多个模型系列和对抗设置下的广泛评估表明，该方法能有效减少有害响应，同时保持甚至提高效用和少样本学习性能。

Conclusion: 该方法能够精确进行低影响更新，避免完全回退，在保持任务性能的同时有效恢复模型的安全对齐性。

Abstract: Fine-tuning Large Language Models (LLMs) for downstream tasks often compromises safety alignment, even when using parameter-efficient methods like LoRA. In this work, we uncover a notable property: fine-tuned models preserve the geometric structure of their loss landscapes concerning harmful content, regardless of the fine-tuning method employed. This suggests that safety behaviors are not erased but shifted to less influential regions of the parameter space. Building on this insight, we propose a curvature-aware alignment restoration method that leverages influence functions and second-order optimization to selectively increase loss on harmful inputs while preserving task performance. By navigating the shared geometry between base and fine-tuned models, our method discourages unsafe outputs while preserving task-relevant performance, avoiding full reversion and enabling precise, low-impact updates. Extensive evaluations across multiple model families and adversarial settings show that our approach efficiently reduces harmful responses while maintaining or even improving utility and few-shot learning performance.

</details>


### [174] [Hierarchical Linkage Clustering Beyond Binary Trees and Ultrametrics](https://arxiv.org/abs/2511.18056)
*Maximilien Dreveton,Matthias Grossglauser,Daichi Kuroda,Patrick Thiran*

Main category: cs.LG

TL;DR: 该论文提出了有效层次结构的概念，解决了传统层次聚类的三个主要限制：总是返回层次结构、仅限于二叉树、对链接函数选择敏感。通过定义有效层次结构的部分顺序，证明了最精细有效层次结构的存在，并提出两步算法来恢复该结构。


<details>
  <summary>Details</summary>
Motivation: 传统层次聚类方法存在三个主要问题：总是返回层次结构（即使数据没有层次关系）、仅限于二叉树（即使真实层次是非二叉的）、对链接函数选择高度敏感。这些限制影响了聚类结果的准确性和解释性。

Method: 提出有效层次结构的概念，定义有效层次结构的部分顺序。开发两步算法：首先通过链接方法构建二叉树，然后通过修剪强制有效性。建立了链接函数恢复最精细有效层次结构的充分必要条件。

Result: 证明了最精细有效层次结构的存在性，该结构不限于二叉树，当没有层次关系时会坍缩为星形树。经典链接规则（单链接、全链接、平均链接）满足恢复条件，而Ward链接不满足。所有满足条件的链接函数在修剪后产生相同的层次结构。

Conclusion: 通过引入有效层次结构的概念和相应的修剪算法，解决了传统层次聚类的根本限制，提供了更准确反映数据内在层次关系的聚类方法，且对满足条件的链接函数具有鲁棒性。

Abstract: Hierarchical clustering seeks to uncover nested structures in data by constructing a tree of clusters, where deeper levels reveal finer-grained relationships. Traditional methods, including linkage approaches, face three major limitations: (i) they always return a hierarchy, even if none exists, (ii) they are restricted to binary trees, even if the true hierarchy is non-binary, and (iii) they are highly sensitive to the choice of linkage function. In this paper, we address these issues by introducing the notion of a valid hierarchy and defining a partial order over the set of valid hierarchies. We prove the existence of a finest valid hierarchy, that is, the hierarchy that encodes the maximum information consistent with the similarity structure of the data set. In particular, the finest valid hierarchy is not constrained to binary structures and, when no hierarchical relationships exist, collapses to a star tree. We propose a simple two-step algorithm that first constructs a binary tree via a linkage method and then prunes it to enforce validity. We establish necessary and sufficient conditions on the linkage function under which this procedure exactly recovers the finest valid hierarchy, and we show that all linkage functions satisfying these conditions yield the same hierarchy after pruning. Notably, classical linkage rules such as single, complete, and average satisfy these conditions, whereas Ward's linkage fails to do so.

</details>


### [175] [pFedBBN: A Personalized Federated Test-Time Adaptation with Balanced Batch Normalization for Class-Imbalanced Data](https://arxiv.org/abs/2511.18066)
*Md Akil Raihan Iftee,Syed Md. Ahnaf Hasan,Mir Sazzat Hossain,Rakibul Hasan Rajib,Amin Ahsan Ali,AKM Mahbubur Rahman,Sajib Mistry,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 提出了pFedBBN，一个个性化的联邦测试时适应框架，通过平衡批归一化和基于相似性的客户端协作来解决联邦学习中类别不平衡和领域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的测试时适应面临类别不平衡和领域偏移的挑战，现有方法无法在无标签、无客户端协调的情况下处理动态领域变化和类别分布不均问题。

Method: 使用平衡批归一化(BBN)进行本地客户端适应，通过基于BBN相似性的客户端协作机制，实现领域感知的个性化模型聚合。

Result: 在多个基准测试中，pFedBBN相比现有联邦学习和测试时适应方法，显著提升了鲁棒性和少数类性能。

Conclusion: pFedBBN通过平衡特征归一化和领域感知协作，有效解决了联邦学习中的类别不平衡和分布偏移问题，支持完全无监督的本地适应且不损害隐私。

Abstract: Test-time adaptation (TTA) in federated learning (FL) is crucial for handling unseen data distributions across clients, particularly when faced with domain shifts and skewed class distributions. Class Imbalance (CI) remains a fundamental challenge in FL, where rare but critical classes are often severely underrepresented in individual client datasets. Although prior work has addressed CI during training through reliable aggregation and local class distribution alignment, these methods typically rely on access to labeled data or coordination among clients, and none address class unsupervised adaptation to dynamic domains or distribution shifts at inference time under federated CI constraints. Revealing the failure of state-of-the-art TTA in federated client adaptation in CI scenario, we propose pFedBBN,a personalized federated test-time adaptation framework that employs balanced batch normalization (BBN) during local client adaptation to mitigate prediction bias by treating all classes equally, while also enabling client collaboration guided by BBN similarity, ensuring that clients with similar balanced representations reinforce each other and that adaptation remains aligned with domain-specific characteristics. pFedBBN supports fully unsupervised local adaptation and introduces a class-aware model aggregation strategy that enables personalized inference without compromising privacy. It addresses both distribution shifts and class imbalance through balanced feature normalization and domain-aware collaboration, without requiring any labeled or raw data from clients. Extensive experiments across diverse baselines show that pFedBBN consistently enhances robustness and minority-class performance over state-of-the-art FL and TTA methods.

</details>


### [176] [The Alignment Paradox of Medical Large Language Models in Infertility Care: Decoupling Algorithmic Improvement from Clinical Decision-making Quality](https://arxiv.org/abs/2511.18084)
*Dou Liu,Ying Long,Sophia Zuoqiu,Kaipeng Xie,Runze Yang,Di Liu,Kang Li,Yiting Lin,Hanyi Liu,Rong Yin,Tian Tang*

Main category: cs.LG

TL;DR: 评估四种LLM对齐策略在临床决策中的表现，发现算法精度最高的GRPO在医生评估中不如SFT，揭示了算法改进与临床信任之间的差异


<details>
  <summary>Details</summary>
Motivation: LLM在临床决策支持中的应用日益增多，但如何使其与真实医学的多维度推理路径保持一致仍是一个重大挑战

Method: 使用8000多份不孕症治疗记录，系统评估SFT、DPO、GRPO和ICL四种对齐策略，采用自动基准测试与盲法医生评估相结合的双层框架

Result: GRPO在多个决策层获得最高算法精度，但医生更偏好SFT模型，认为其推理过程更清晰、治疗可行性更高。在盲法配对比较中，SFT获得最高胜率(51.2%)

Conclusion: 存在对齐悖论：算法改进不一定转化为更高的临床信任，可能与以人为中心的偏好相背离。需要优先考虑临床可解释性和实践可行性的对齐策略

Abstract: Large language models (LLMs) are increasingly adopted in clinical decision support, yet aligning them with the multifaceted reasoning pathways of real-world medicine remains a major challenge. Using more than 8,000 infertility treatment records, we systematically evaluate four alignment strategies: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), Group Relative Policy Optimization (GRPO), and In-Context Learning (ICL) through a dual-layer framework combining automatic benchmarks with blinded doctor-in-the-loop assessments. GRPO achieves the highest algorithmic accuracy across multiple decision layers, confirming the value of reinforcement-based optimization for structured prediction tasks. However, clinicians consistently prefer the SFT model, citing clearer reasoning processes (p = 0.035) and higher therapeutic feasibility (p = 0.019). In blinded pairwise comparisons, SFT attains the highest winning rate (51.2%), outperforming both GRPO (26.2%) and even physicians' original decisions (22.7%). These results reveal an alignment paradox: algorithmic improvements do not necessarily translate into higher clinical trust, and may diverge from human-centered preferences. Our findings highlight the need for alignment strategies that prioritize clinically interpretable and practically feasible reasoning, rather than solely optimizing decision-level accuracy.

</details>


### [177] [A New Error Temporal Difference Algorithm for Deep Reinforcement Learning in Microgrid Optimization](https://arxiv.org/abs/2511.18093)
*Fulong Yao,Wanqing Zhao,Matthew Forshaw*

Main category: cs.LG

TL;DR: 提出了一种新的误差时间差分(ETD)算法，用于解决微电网能量优化中深度强化学习预测控制的不确定性问题，通过加权平均算法和ETD算法分别量化和处理预测不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度强化学习的预测控制研究往往忽视预测模型不完美带来的不确定性问题，这可能导致次优的控制策略。

Method: 首先建立集成可再生能源和储能系统的微电网系统及其马尔可夫决策过程模型，然后提出基于深度Q网络的预测控制方法，设计加权平均算法和新的ETD算法分别量化和处理预测不确定性。

Result: 在真实美国数据集上的仿真表明，所开发的ETD算法有效提高了深度强化学习在优化微电网运行方面的性能。

Conclusion: 提出的ETD算法能够有效处理微电网能量优化中的预测不确定性，提升深度强化学习控制策略的性能。

Abstract: Predictive control approaches based on deep reinforcement learning (DRL) have gained significant attention in microgrid energy optimization. However, existing research often overlooks the issue of uncertainty stemming from imperfect prediction models, which can lead to suboptimal control strategies. This paper presents a new error temporal difference (ETD) algorithm for DRL to address the uncertainty in predictions,aiming to improve the performance of microgrid operations. First,a microgrid system integrated with renewable energy sources (RES) and energy storage systems (ESS), along with its Markov decision process (MDP), is modelled. Second, a predictive control approach based on a deep Q network (DQN) is presented, in which a weighted average algorithm and a new ETD algorithm are designed to quantify and address the prediction uncertainty, respectively. Finally, simulations on a realworld US dataset suggest that the developed ETD effectively improves the performance of DRL in optimizing microgrid operations.

</details>


### [178] [Active Learning with Selective Time-Step Acquisition for PDEs](https://arxiv.org/abs/2511.18107)
*Yegon Kim,Hyunsu Kim,Gyeonghoon Ko,Juho Lee*

Main category: cs.LG

TL;DR: 提出了一种用于PDE代理建模的主动学习框架，通过仅生成最重要的时间步来显著降低训练数据生成成本，从而提高数据效率。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器计算成本高，而代理模型开发受限于从数值求解器生成足够训练数据的成本。现有PDE主动学习方法总是获取整个PDE轨迹，成本高昂。

Method: 开发了新的主动学习框架，策略性地仅用数值求解器生成最重要的时间步，同时使用代理模型近似其余步骤。还开发了通过近似方差减少来估计时间步集合效用的获取函数。

Result: 在多个基准PDE上验证了方法的有效性，包括Burgers方程、KdV方程、Kuramoto-Sivashinsky方程、不可压缩和可压缩Navier-Stokes方程。实验显示性能大幅优于现有最佳方法，不仅减少平均误差，还降低了99%、95%和50%分位数的误差。

Conclusion: 该方法为PDE代理建模提供了数据高效的解决方案，显著降低了训练数据生成成本，同时提高了模型性能。

Abstract: Accurately solving partial differential equations (PDEs) is critical to understanding complex scientific and engineering phenomena, yet traditional numerical solvers are computationally expensive. Surrogate models offer a more efficient alternative, but their development is hindered by the cost of generating sufficient training data from numerical solvers. In this paper, we present a novel framework for active learning (AL) in PDE surrogate modeling that reduces this cost. Unlike the existing AL methods for PDEs that always acquire entire PDE trajectories, our approach strategically generates only the most important time steps with the numerical solver, while employing the surrogate model to approximate the remaining steps. This dramatically reduces the cost incurred by each trajectory and thus allows the active learning algorithm to try out a more diverse set of trajectories given the same budget. To accommodate this novel framework, we develop an acquisition function that estimates the utility of a set of time steps by approximating its resulting variance reduction. We demonstrate the effectiveness of our method on several benchmark PDEs, including the Burgers' equation, Korteweg-De Vries equation, Kuramoto-Sivashinsky equation, the incompressible Navier-Stokes equation, and the compressible Navier-Stokes equation. Experiments show that our approach improves performance by large margins over the best existing method. Our method not only reduces average error but also the 99\%, 95\%, and 50\% quantiles of error, which is rare for an AL algorithm. All in all, our approach offers a data-efficient solution to surrogate modeling for PDEs.

</details>


### [179] [Vulnerability-Aware Robust Multimodal Adversarial Training](https://arxiv.org/abs/2511.18138)
*Junrui Zhang,Xinyu Zhao,Jie Peng,Chenjie Wang,Jianmin Ji,Tianlong Chen*

Main category: cs.LG

TL;DR: VARMAT是一种多模态对抗训练方法，通过量化每个模态的脆弱性并进行针对性正则化，显著提升多模态模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了不同模态对最终鲁棒性的贡献差异，导致鲁棒性表现不佳。

Method: VARMAT首先基于攻击目标的一阶近似显式量化每个模态的脆弱性（探针），然后提出针对高脆弱性模态的正则化项，在保持任务准确性的同时引导鲁棒学习。

Result: 在多个多模态数据集上实现了显著鲁棒性提升，三个数据集分别提升了12.73%、22.21%和11.19%。

Conclusion: 该方法揭示了多模态对抗训练中的一个重要盲点，通过感知模态脆弱性实现了更优的鲁棒性。

Abstract: Multimodal learning has shown significant superiority on various tasks by integrating multiple modalities. However, the interdependencies among modalities increase the susceptibility of multimodal models to adversarial attacks. Existing methods mainly focus on attacks on specific modalities or indiscriminately attack all modalities. In this paper, we find that these approaches ignore the differences between modalities in their contribution to final robustness, resulting in suboptimal robustness performance. To bridge this gap, we introduce Vulnerability-Aware Robust Multimodal Adversarial Training (VARMAT), a probe-in-training adversarial training method that improves multimodal robustness by identifying the vulnerability of each modality. To be specific, VARMAT first explicitly quantifies the vulnerability of each modality, grounded in a first-order approximation of the attack objective (Probe). Then, we propose a targeted regularization term that penalizes modalities with high vulnerability, guiding robust learning while maintaining task accuracy (Training). We demonstrate the enhanced robustness of our method across multiple multimodal datasets involving diverse modalities. Finally, we achieve {12.73%, 22.21%, 11.19%} robustness improvement on three multimodal datasets, revealing a significant blind spot in multimodal adversarial training.

</details>


### [180] [Graph Neural Networks vs Convolutional Neural Networks for Graph Domination Number Prediction](https://arxiv.org/abs/2511.18150)
*Randy Davila,Beyzanur Ispir*

Main category: cs.LG

TL;DR: 比较CNN和GNN在图支配数近似计算中的表现，GNN在准确性和速度上都显著优于CNN，可作为组合图不变量的实用替代方法。


<details>
  <summary>Details</summary>
Motivation: 图支配数的精确计算是NP难问题，传统方法只能处理小规模图实例，需要开发高效的近似计算方法。

Method: 使用CNN（基于邻接矩阵表示）和GNN（基于图结构通过消息传递学习）两种神经网络范式，在2000个最多64个顶点的随机图上进行实验比较。

Result: GNN获得明显更高的准确性（R²=0.987，MAE=0.372），优于CNN（R²=0.955，MAE=0.500）。GNN提供超过200倍的加速，同时保持接近完美的保真度。

Conclusion: GNN可作为组合图不变量的实用替代方法，对可扩展图优化和数学发现具有重要意义。

Abstract: We investigate machine learning approaches to approximating the \emph{domination number} of graphs, the minimum size of a dominating set. Exact computation of this parameter is NP-hard, restricting classical methods to small instances. We compare two neural paradigms: Convolutional Neural Networks (CNNs), which operate on adjacency matrix representations, and Graph Neural Networks (GNNs), which learn directly from graph structure through message passing. Across 2,000 random graphs with up to 64 vertices, GNNs achieve markedly higher accuracy ($R^2=0.987$, MAE $=0.372$) than CNNs ($R^2=0.955$, MAE $=0.500$). Both models offer substantial speedups over exact solvers, with GNNs delivering more than $200\times$ acceleration while retaining near-perfect fidelity. Our results position GNNs as a practical surrogate for combinatorial graph invariants, with implications for scalable graph optimization and mathematical discovery.

</details>


### [181] [scipy.spatial.transform: Differentiable Framework-Agnostic 3D Transformations in Python](https://arxiv.org/abs/2511.18157)
*Martin Schuck,Alexander von Rohr,Angela P. Schoellig*

Main category: cs.LG

TL;DR: 将SciPy的spatial.transform模块升级为支持任意数组库的通用实现，使其兼容GPU加速和自动微分工作流


<details>
  <summary>Details</summary>
Motivation: 现有的SciPy spatial.transform模块仅支持NumPy，限制了在GPU加速和自动微分工作流中的应用

Method: 对SciPy spatial.transform功能进行全面重构，使其兼容实现Python数组API的任何数组库（包括JAX、PyTorch、CuPy等）

Result: 新实现保持了SciPy接口，同时支持GPU/TPU执行、JIT编译、向量化批处理和原生自动微分

Conclusion: 该贡献已合并到SciPy主分支，为可微分系统和机器学习中的3D空间数学提供了框架无关的生产级基础

Abstract: Three-dimensional rigid-body transforms, i.e. rotations and translations, are central to modern differentiable machine learning pipelines in robotics, vision, and simulation. However, numerically robust and mathematically correct implementations, particularly on SO(3), are error-prone due to issues such as axis conventions, normalizations, composition consistency and subtle errors that only appear in edge cases. SciPy's spatial.transform module is a rigorously tested Python implementation. However, it historically only supported NumPy, limiting adoption in GPU-accelerated and autodiff-based workflows. We present a complete overhaul of SciPy's spatial.transform functionality that makes it compatible with any array library implementing the Python array API, including JAX, PyTorch, and CuPy. The revised implementation preserves the established SciPy interface while enabling GPU/TPU execution, JIT compilation, vectorized batching, and differentiation via native autodiff of the chosen backend. We demonstrate how this foundation supports differentiable scientific computing through two case studies: (i) scalability of 3D transforms and rotations and (ii) a JAX drone simulation that leverages SciPy's Rotation for accurate integration of rotational dynamics. Our contributions have been merged into SciPy main and will ship in the next release, providing a framework-agnostic, production-grade basis for 3D spatial math in differentiable systems and ML.

</details>


### [182] [LocaGen: Low-Overhead Indoor Localization Through Spatial Augmentation](https://arxiv.org/abs/2511.18158)
*Abdelrahman Abdelmotlb,Abdallah Taman,Sherif Mostafa,Moustafa Youssef*

Main category: cs.LG

TL;DR: LocaGen是一个空间增强框架，通过条件扩散模型生成未见过位置的高质量合成指纹数据，显著减少室内定位系统的指纹采集开销。


<details>
  <summary>Details</summary>
Motivation: 室内定位系统依赖指纹识别技术，但需要大量位置标记信号数据的采集工作，限制了实际部署。现有方法要么表示能力低，要么存在模式崩溃问题，或者需要在所有目标位置收集数据。

Method: 使用条件扩散模型结合空间感知优化策略，仅使用部分已见位置的数据来合成未见过位置的指纹。通过领域特定启发式方法增强已见位置数据，并采用基于密度的策略选择已见和未见位置以确保鲁棒覆盖。

Result: 在真实WiFi指纹数据集上的评估显示，即使在30%位置未见的情况下，LocaGen仍能保持相同的定位精度，相比最先进的增强方法精度提升高达28%。

Conclusion: LocaGen通过空间增强框架有效减少了指纹采集开销，在保持定位精度的同时显著提高了方法的实用性。

Abstract: Indoor localization systems commonly rely on fingerprinting, which requires extensive survey efforts to obtain location-tagged signal data, limiting their real-world deployability. Recent approaches that attempt to reduce this overhead either suffer from low representation ability, mode collapse issues, or require the effort of collecting data at all target locations. We present LocaGen, a novel spatial augmentation framework that significantly reduces fingerprinting overhead by generating high-quality synthetic data at completely unseen locations. LocaGen leverages a conditional diffusion model guided by a novel spatially aware optimization strategy to synthesize realistic fingerprints at unseen locations using only a subset of seen locations. To further improve our diffusion model performance, LocaGen augments seen location data based on domain-specific heuristics and strategically selects the seen and unseen locations using a novel density-based approach that ensures robust coverage. Our extensive evaluation on a real-world WiFi fingerprinting dataset shows that LocaGen maintains the same localization accuracy even with 30% of the locations unseen and achieves up to 28% improvement in accuracy over state-of-the-art augmentation methods.

</details>


### [183] [Bringing Stability to Diffusion: Decomposing and Reducing Variance of Training Masked Diffusion Models](https://arxiv.org/abs/2511.18159)
*Mengni Jia,Mengyu Zhou,Yihao Liu,Xiaoxi Jiang,Guanjun Jiang*

Main category: cs.LG

TL;DR: 本文分析了掩码扩散模型训练方差高的根本原因，提出了两种核心方差减少方法P-POTS和MIRROR，显著提升了MDM在复杂推理任务上的性能表现。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型作为自回归模型的有力替代方案，存在训练方差过高的问题，导致梯度估计噪声大、优化不稳定，即使与自回归模型在初始化时性能相当，在任务特定训练后也会大幅落后。

Method: 首先将MDM训练方差分解为三个来源：掩码模式噪声、掩码率噪声和数据噪声；然后设计了六种方差减少方法，核心包括：P-POTS（帕累托最优t采样器）和MIRROR（使用负相关样本减少掩码模式噪声）。

Result: 实验表明，相比标准MDM训练，新方法在复杂推理任务上准确率提升7-8%，同时将运行间变异性降低到接近ARM水平，显著缩小了与强ARM基线的差距。

Conclusion: 通过理论分析和系统性的方差减少方法，成功解决了MDM训练方差高的问题，使其在保持竞争力的同时获得更稳定的训练效果。

Abstract: Masked diffusion models (MDMs) are a promising alternative to autoregressive models (ARMs), but they suffer from inherently much higher training variance. High variance leads to noisier gradient estimates and unstable optimization, so even equally strong pretrained MDMs and ARMs that are competitive at initialization often diverge after task-specific training, with MDMs falling far behind. There has been no theoretical explanation or systematic solution. We derive the first decomposition of MDM training variance into three sources: (A) masking pattern noise, (B) masking rate noise, and (C) data noise, while ARMs are only affected by (C). This explains the fundamental training gap. Building on this foundation, we design six variance-reduction methods, including two core methods: (1) P-POTS, a Pareto-optimal t sampler that minimizes training variance by sampling harder t values more often with appropriately smaller update steps, and (2) MIRROR, which uses negatively correlated samples to reduce (A). Experiments show that compared to standard MDM training, our methods improve accuracy by 7-8% on complex reasoning tasks, while simultaneously reducing run-to-run variability to near ARM levels, substantially narrowing the gap with strong ARM baselines; in most settings, even the best baseline runs remain below the worst run of our method.

</details>


### [184] [Bayesian Calibration of Engine-out NOx Models for Engine-to-Engine Transferability](https://arxiv.org/abs/2511.18178)
*Shrenik Zinage,Peter Meckl,Ilias Bilionis*

Main category: cs.LG

TL;DR: 提出了一种贝叶斯校准框架，结合高斯过程和近似贝叶斯计算来推断和校正传感器偏差，以解决发动机间差异导致的NOx预测泛化问题。


<details>
  <summary>Details</summary>
Motivation: 传统模型在少量发动机数据上训练，难以泛化到整个发动机群体，存在传感器偏差和输入条件变化的问题，需要能够适应发动机间差异的模型。

Method: 使用贝叶斯校准框架，结合高斯过程和近似贝叶斯计算，从预训练模型出发推断发动机特定传感器偏差并重新校准预测。

Result: 该方法在未见测试数据上生成发动机NOx的后验预测分布，相比传统非自适应GP模型显著提高了预测准确性。

Conclusion: 该可转移建模方法有效解决了发动机间变异性问题，提高了模型泛化能力，无需重新训练模型即可实现高精度预测。

Abstract: Accurate prediction of engine-out NOx is essential for meeting stringent emissions regulations and optimizing engine performance. Traditional approaches rely on models trained on data from a small number of engines, which can be insufficient in generalizing across an entire population of engines due to sensor biases and variations in input conditions. In real world applications, these models require tuning or calibration to maintain acceptable error tolerance when applied to other engines. This highlights the need for models that can adapt with minimal adjustments to accommodate engine-to-engine variability and sensor discrepancies. While previous studies have explored machine learning methods for predicting engine-out NOx, these approaches often fail to generalize reliably across different engines and operating environments. To address these issues, we propose a Bayesian calibration framework that combines Gaussian processes with approximate Bayesian computation to infer and correct sensor biases. Starting with a pre-trained model developed using nominal engine data, our method identifies engine specific sensor biases and recalibrates predictions accordingly. By incorporating these inferred biases, our approach generates posterior predictive distributions for engine-out NOx on unseen test data, achieving high accuracy without retraining the model. Our results demonstrate that this transferable modeling approach significantly improves the accuracy of predictions compared to conventional non-adaptive GP models, effectively addressing engine-to-engine variability and improving model generalizability.

</details>


### [185] [MOMA-AC: A preference-driven actor-critic framework for continuous multi-objective multi-agent reinforcement learning](https://arxiv.org/abs/2511.18181)
*Adam Callaghan,Karl Mason,Patrick Mannion*

Main category: cs.LG

TL;DR: 提出了首个用于连续状态和动作空间的多目标多智能体强化学习框架MOMA-AC，基于TD3和DDPG算法实现了MOMA-TD3和MOMA-DDPG，在合作运动任务中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 填补多目标多智能体强化学习在连续状态和动作空间中的研究空白，解决多智能体系统中多个冲突目标的权衡问题。

Method: 结合多头actor网络、集中式critic和目标偏好条件架构，使用单个神经网络编码所有智能体在所有冲突目标上的帕累托最优策略前沿。

Result: 在合作运动任务中，相比外层循环和独立训练基线，在期望效用和超体积指标上取得统计显著提升，且随着智能体数量增加保持稳定扩展性。

Conclusion: 该框架为连续多智能体领域中的稳健、可扩展多目标策略学习奠定了重要基础。

Abstract: This paper addresses a critical gap in Multi-Objective Multi-Agent Reinforcement Learning (MOMARL) by introducing the first dedicated inner-loop actor-critic framework for continuous state and action spaces: Multi-Objective Multi-Agent Actor-Critic (MOMA-AC). Building on single-objective, single-agent algorithms, we instantiate this framework with Twin Delayed Deep Deterministic Policy Gradient (TD3) and Deep Deterministic Policy Gradient (DDPG), yielding MOMA-TD3 and MOMA-DDPG. The framework combines a multi-headed actor network, a centralised critic, and an objective preference-conditioning architecture, enabling a single neural network to encode the Pareto front of optimal trade-off policies for all agents across conflicting objectives in a continuous MOMARL setting. We also outline a natural test suite for continuous MOMARL by combining a pre-existing multi-agent single-objective physics simulator with its multi-objective single-agent counterpart. Evaluating cooperative locomotion tasks in this suite, we show that our framework achieves statistically significant improvements in expected utility and hypervolume relative to outer-loop and independent training baselines, while demonstrating stable scalability as the number of agents increases. These results establish our framework as a foundational step towards robust, scalable multi-objective policy learning in continuous multi-agent domains.

</details>


### [186] [Accelerating Time Series Foundation Models with Speculative Decoding](https://arxiv.org/abs/2511.18191)
*Pranav Subbaraman,Fang Sun,Yue Yao,Huacong Tang,Xiao Luo,Yizhou Sun*

Main category: cs.LG

TL;DR: 提出了一种基于推测解码的时间序列预测推理加速框架，使用小模型生成预测补丁，大模型并行验证，显著减少顺序前向传播次数


<details>
  <summary>Details</summary>
Motivation: 解决Transformer时间序列预测模型在延迟敏感Web应用中的高计算成本问题，实现大规模部署

Method: 将推测解码技术从离散语言标记扩展到连续时间序列分布，使用小"草稿"模型生成未来时间序列补丁，大"目标"模型并行验证，设计多变量高斯补丁的接受标准和实用变体

Result: 在Web应用相关时间序列预测基准测试中，实现了显著的推理加速，同时保持竞争性准确率

Conclusion: 该框架无需修改现有基础模型架构，可直接加速已部署的时间序列预测系统

Abstract: Modern web applications--from real-time content recommendation and dynamic pricing to CDN optimization--increasingly rely on time-series forecasting to deliver personalized experiences to billions of users. Large-scale Transformer-based models have achieved state-of-the-art performance in time-series forecasting but suffer from high computational costs, limiting their deployment in latency-sensitive web applications. To address this challenge, we propose a general inference acceleration framework that adapts speculative decoding to autoregressive time-series models. Our approach employs a smaller "draft" model to propose future time-series patches, which are then verified in parallel by a larger "target" model, reducing the number of sequential forward passes required. We address key technical challenges in adapting this technique from discrete language tokens to continuous time-series distributions, including the design of acceptance criteria for multivariate Gaussian patches and practical variants that balance efficiency with accuracy. Through experiments on time series forecasting benchmarks relevant to web applications, we demonstrate significant inference speedups while maintaining competitive accuracy. The framework requires no architectural modifications to existing foundation models, making it immediately applicable to accelerate deployed time-series forecasting systems. Our implementation can be found at https://github.com/PranavSubbaraman/STRIDE

</details>


### [187] [Deep Gaussian Process Proximal Policy Optimization](https://arxiv.org/abs/2511.18214)
*Matthijs van der Lende,Juan Cardenas-Cartagena*

Main category: cs.LG

TL;DR: 提出了GPPO算法，将深度高斯过程与PPO结合，在保持性能的同时提供校准的不确定性估计


<details>
  <summary>Details</summary>
Motivation: 强化学习中需要平衡安全探索和高效学习，但深度神经网络通常缺乏校准的不确定性估计

Method: 使用深度高斯过程近似策略和价值函数，开发了可扩展的模型无关actor-critic算法GPPO

Result: 在标准高维连续控制基准测试中与PPO保持竞争力，同时提供良好校准的不确定性估计

Conclusion: GPPO能够为更安全和更有效的探索提供信息，解决了RL中不确定性估计的关键问题

Abstract: Uncertainty estimation for Reinforcement Learning (RL) is a critical component in control tasks where agents must balance safe exploration and efficient learning. While deep neural networks have enabled breakthroughs in RL, they often lack calibrated uncertainty estimates. We introduce Deep Gaussian Process Proximal Policy Optimization (GPPO), a scalable, model-free actor-critic algorithm that leverages Deep Gaussian Processes (DGPs) to approximate both the policy and value function. GPPO maintains competitive performance with respect to Proximal Policy Optimization on standard high-dimensional continuous control benchmarks while providing well-calibrated uncertainty estimates that can inform safer and more effective exploration.

</details>


### [188] [Adaptive Conformal Prediction for Quantum Machine Learning](https://arxiv.org/abs/2511.18225)
*Douglas Spencer,Samual Nicholls,Michele Caprio*

Main category: cs.LG

TL;DR: 提出了自适应量子保形预测(AQCP)算法，通过动态重新校准来应对量子处理器的时间变化噪声，在任意硬件噪声条件下保持渐近平均覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习需要可靠的不确定性量化方法，但量子处理器固有的时变噪声会破坏保形预测的统计保证，即使在校准和测试数据可交换的情况下也是如此。

Method: 基于自适应保形推理框架，引入自适应量子保形预测(AQCP)算法，通过重复重新校准来维持随时间变化的有效性。

Result: 在IBM量子处理器上的实证研究表明，AQCP能够达到目标覆盖水平，并且比量子保形预测表现出更高的稳定性。

Conclusion: AQCP算法有效解决了量子硬件噪声对保形预测保证的破坏问题，为量子机器学习提供了更可靠的不确定性量化方法。

Abstract: Quantum machine learning seeks to leverage quantum computers to improve upon classical machine learning algorithms. Currently, robust uncertainty quantification methods remain underdeveloped in the quantum domain, despite the critical need for reliable and trustworthy predictions. Recent work has introduced quantum conformal prediction, a framework that produces prediction sets that are guaranteed to contain the true outcome with user-specified probability. In this work, we formalise how the time-varying noise inherent in quantum processors can undermine conformal guarantees, even when calibration and test data are exchangeable. To address this challenge, we draw on Adaptive Conformal Inference, a method which maintains validity over time via repeated recalibration. We introduce Adaptive Quantum Conformal Prediction (AQCP), an algorithm which preserves asymptotic average coverage guarantees under arbitrary hardware noise conditions. Empirical studies on an IBM quantum processor demonstrate that AQCP achieves target coverage levels and exhibits greater stability than quantum conformal prediction.

</details>


### [189] [Tail Distribution of Regret in Optimistic Reinforcement Learning](https://arxiv.org/abs/2511.18247)
*Sajad Khodadadian,Mehrdad Moharrami*

Main category: cs.LG

TL;DR: 本文为基于乐观主义的强化学习算法在有限时域表格MDP中推导了实例依赖的遗憾尾界，分析了两种探索奖励调度方案，揭示了遗憾分布的双机制结构特征。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注期望遗憾或单一高概率分位数，缺乏对遗憾尾分布的全面刻画。本文旨在填补这一空白，提供更完整的遗憾分布特性分析。

Method: 采用UCBVI类型算法，分析两种探索奖励调度：(i) K依赖方案，显式包含总回合数；(ii) K独立方案，仅依赖当前回合索引。通过调节参数α平衡期望遗憾和子高斯尾范围。

Result: 获得了Pr(R_K ≥ x)的上界，显示出独特的双机制结构：从实例依赖尺度m_K到转移阈值的子高斯尾，之后是子威布尔尾。同时推导了相应的期望遗憾实例依赖界。

Conclusion: 这是首个为episodic强化学习中标准乐观算法提供全面尾遗憾保证的研究，揭示了遗憾分布的重要结构特征，为算法设计提供了理论指导。

Abstract: We derive instance-dependent tail bounds for the regret of optimism-based reinforcement learning in finite-horizon tabular Markov decision processes with unknown transition dynamics. Focusing on a UCBVI-type algorithm, we characterize the tail distribution of the cumulative regret $R_K$ over $K$ episodes, rather than only its expectation or a single high-probability quantile. We analyze two natural exploration-bonus schedules: (i) a $K$-dependent scheme that explicitly incorporates the total number of episodes $K$, and (ii) a $K$-independent scheme that depends only on the current episode index. For both settings, we obtain an upper bound on $\Pr(R_K \ge x)$ that exhibits a distinctive two-regime structure: a sub-Gaussian tail starting from an instance-dependent scale $m_K$ up to a transition threshold, followed by a sub-Weibull tail beyond that point. We further derive corresponding instance-dependent bounds on the expected regret $\mathbb{E}[R_K]$. The proposed algorithm depends on a tuning parameter $α$, which balances the expected regret and the range over which the regret exhibits a sub-Gaussian tail. To the best of our knowledge, our results provide one of the first comprehensive tail-regret guarantees for a standard optimistic algorithm in episodic reinforcement learning.

</details>


### [190] [Coherent Multi-Agent Trajectory Forecasting in Team Sports with CausalTraj](https://arxiv.org/abs/2511.18248)
*Wei Zhen Teoh*

Main category: cs.LG

TL;DR: 提出CausalTraj模型，专注于生成联合概率的多智能体轨迹预测，在保持个体预测精度的同时显著提升联合预测性能


<details>
  <summary>Details</summary>
Motivation: 现有模型主要基于个体精度指标(minADE, minFDE)进行优化，忽略了预测轨迹之间的联合合理性，导致无法生成连贯的多智能体场景

Method: CausalTraj是一个基于时间因果关系的似然模型，专门设计用于生成联合概率的多智能体轨迹预测

Result: 在NBA SportVU、Basketball-U和Football-U数据集上，CausalTraj在保持个体预测竞争力的同时，在联合指标(minJADE, minJFDE)上取得最佳结果

Conclusion: CausalTraj能够生成定性连贯且现实的游戏演化场景，强调联合评估指标对于多智能体轨迹预测的重要性

Abstract: Jointly forecasting trajectories of multiple interacting agents is a core challenge in sports analytics and other domains involving complex group dynamics. Accurate prediction enables realistic simulation and strategic understanding of gameplay evolution. Most existing models are evaluated solely on per-agent accuracy metrics (minADE, minFDE), which assess each agent independently on its best-of-k prediction. However these metrics overlook whether the model learns which predicted trajectories can jointly form a plausible multi-agent future. Many state-of-the-art models are designed and optimized primarily based on these metrics. As a result, they may underperform on joint predictions and also fail to generate coherent, interpretable multi-agent scenarios in team sports. We propose CausalTraj, a temporally causal, likelihood-based model that is built to generate jointly probable multi-agent trajectory forecasts. To better assess collective modeling capability, we emphasize joint metrics (minJADE, minJFDE) that measure joint accuracy across agents within the best generated scenario sample. Evaluated on the NBA SportVU, Basketball-U, and Football-U datasets, CausalTraj achieves competitive per-agent accuracy and the best recorded results on joint metrics, while yielding qualitatively coherent and realistic gameplay evolutions.

</details>


### [191] [Reduced-Basis Deep Operator Learning for Parametric PDEs with Independently Varying Boundary and Source Data](https://arxiv.org/abs/2511.18260)
*Yueqi Wang,Guang Lin*

Main category: cs.LG

TL;DR: RB-DeepONet是一种混合算子学习框架，将降基数值结构与DeepONet的分支-主干架构融合，用于加速参数化PDE的求解。


<details>
  <summary>Details</summary>
Motivation: 现有算子学习方法依赖不透明的学习主干、需要大量标注数据，或在边界和源数据与物理参数独立变化时失效。

Method: 主干固定为通过贪婪选择生成的降基空间，分支网络预测降基系数，使用投影变分残差进行无标签训练，结合边界和源模态编码。

Result: RB-DeepONet达到与侵入式RB-Galerkin、POD-DeepONet和FEONet相当的精度，但使用更少的可训练参数并实现显著加速。

Conclusion: RB-DeepONet为大规模参数化PDE提供高效、稳定且可解释的算子学习方法。

Abstract: Parametric PDEs power modern simulation, design, and digital-twin systems, yet their many-query workloads still hinge on repeatedly solving large finite-element systems. Existing operator-learning approaches accelerate this process but often rely on opaque learned trunks, require extensive labeled data, or break down when boundary and source data vary independently from physical parameters. We introduce RB-DeepONet, a hybrid operator-learning framework that fuses reduced-basis (RB) numerical structure with the branch-trunk architecture of DeepONet. The trunk is fixed to a rigorously constructed RB space generated offline via Greedy selection, granting physical interpretability, stability, and certified error control. The branch network predicts only RB coefficients and is trained label-free using a projected variational residual that targets the RB-Galerkin solution. For problems with independently varying loads or boundary conditions, we develop boundary and source modal encodings that compress exogenous data into low-dimensional coordinates while preserving accuracy. Combined with affine or empirical interpolation decompositions, RB-DeepONet achieves a strict offline-online split: all heavy lifting occurs offline, and online evaluation scales only with the RB dimension rather than the full mesh. We provide convergence guarantees separating RB approximation error from statistical learning error, and numerical experiments show that RB-DeepONet attains accuracy competitive with intrusive RB-Galerkin, POD-DeepONet, and FEONet while using dramatically fewer trainable parameters and achieving significant speedups. This establishes RB-DeepONet as an efficient, stable, and interpretable operator learner for large-scale parametric PDEs.

</details>


### [192] [A Fair OR-ML Framework for Resource Substitution in Large-Scale Networks](https://arxiv.org/abs/2511.18269)
*Ved Mohan,El Mehdi Er Raqabi,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: 提出结合运筹学和机器学习的框架，解决大规模物流网络中的资源替代问题，实现公平高效的资源分配。


<details>
  <summary>Details</summary>
Motivation: 大规模物流网络中资源分配不平衡，资源替代是成本效益高的解决方案，但需要考虑公平性和调度员偏好。

Method: OR组件建模并解决公平视角下的资源替代问题，ML组件利用历史数据学习调度员偏好，动态选择网络中各弧段的前k个资源。

Result: 在世界上最大的包裹递送公司网络中应用，模型规模减少80%，执行时间减少90%，同时保持最优性。

Conclusion: 该框架能产生高质量解决方案组合，调度员可从中选择满意的权衡方案，显著优于现有方法。

Abstract: Ensuring that the right resource is available at the right location and time remains a major challenge for organizations operating large-scale logistics networks. The challenge comes from uneven demand patterns and the resulting asymmetric flow of resources across the arcs, which create persistent imbalances at the network nodes. Resource substitution among multiple, potentially composite and interchangeable, resource types is a cost-effective way to mitigate these imbalances. This leads to the resource substitution problem, which aims at determining the minimum number of resource substitutions from an initial assignment to minimize the overall network imbalance. In decentralized settings, achieving globally coordinated solutions becomes even more difficult. When substitution entails costs, effective prescriptions must also incorporate fairness and account for the individual preferences of schedulers. This paper presents a generic framework that combines operations research (OR) and machine learning (ML) to enable fair resource substitution in large networks. The OR component models and solves the resource substitution problem under a fairness lens. The ML component leverages historical data to learn schedulers' preferences, guide intelligent exploration of the decision space, and enhance computational efficiency by dynamically selecting the top-$κ$ resources for each arc in the network. The framework produces a portfolio of high-quality solutions from which schedulers can select satisfactory trade-offs. The proposed framework is applied to the network of one of the largest package delivery companies in the world, which serves as the primary motivation for this research. Computational results demonstrate substantial improvements over state-of-the-art methods, including an 80% reduction in model size and a 90% decrease in execution time while preserving optimality.

</details>


### [193] [From Tables to Signals: Revealing Spectral Adaptivity in TabPFN](https://arxiv.org/abs/2511.18278)
*Jianqiao Zheng,Cameron Gordon,Yiping Ji,Hemanth Saratchandran,Simon Lucey*

Main category: cs.LG

TL;DR: 本文通过信号重构视角分析TabPFN，发现其具有比标准ReLU-MLP更宽的频率容量，且能根据上下文样本数量自适应调整频率响应，展示了在图像去噪等任务中的潜力。


<details>
  <summary>Details</summary>
Motivation: 理解任务无关表格基础模型（如TabPFN）的归纳偏置来源，目前对其学习行为的频率特性缺乏深入分析。

Method: 通过信号重构的视角，对TabPFN进行频率分析，研究其上下文学习行为，特别关注位置编码对频率响应的影响。

Result: 发现TabPFN具有比标准ReLU-MLP更宽的频率容量，且其频谱容量能根据上下文样本数量自适应调整（称为频谱适应性），位置编码调制其频率响应。

Conclusion: TabPFN具有任务无关的隐式模型潜力，其频率特性为理解表格基础模型的结构和归纳偏置提供了新视角，在更广泛的信号重构任务中具有应用前景。

Abstract: Task-agnostic tabular foundation models such as TabPFN have achieved impressive performance on tabular learning tasks, yet the origins of their inductive biases remain poorly understood. In this work, we study TabPFN through the lens of signal reconstruction and provide the first frequency-based analysis of its in-context learning behavior. We show that TabPFN possesses a broader effective frequency capacity than standard ReLU-MLPs, even without hyperparameter tuning. Moreover, unlike MLPs whose spectra evolve primarily over training epochs, we find that TabPFN's spectral capacity adapts directly to the number of samples provided in-context, a phenomenon we term Spectral Adaptivity. We further demonstrate that positional encoding modulates TabPFN's frequency response, mirroring classical results in implicit neural representations. Finally, we show that these properties enable TabPFN to perform training-free and hyperparameter-free image denoising, illustrating its potential as a task-agnostic implicit model. Our analysis provides new insight into the structure and inductive biases of tabular foundation models and highlights their promise for broader signal reconstruction tasks.

</details>


### [194] [TRIDENT: A Trimodal Cascade Generative Framework for Drug and RNA-Conditioned Cellular Morphology Synthesis](https://arxiv.org/abs/2511.18287)
*Rui Peng,Ziru Liu,Lingyuan Ye,Yuxing Lu,Boxin Shi,Jinzhuo Wang*

Main category: cs.LG

TL;DR: TRIDENT是一个级联生成框架，通过同时考虑扰动和相应基因表达谱来合成真实的细胞形态，在未见化合物上表现出强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常局限于建模直接关联（如扰动→RNA或扰动→形态），忽略了从RNA到形态的关键因果联系，这限制了构建AI虚拟细胞的能力。

Method: 提出TRIDENT级联生成框架，构建了MorphoGene数据集（包含98种化合物的L1000基因表达和Cell Painting图像配对数据），通过RNA条件化来合成细胞形态。

Result: TRIDENT显著优于最先进方法，实现了高达7倍的改进，在未见化合物上表现出强泛化能力。案例研究证实RNA引导的合成能准确产生相应表型。

Conclusion: 通过显式建模转录组-表型组映射，TRIDENT提供了一个强大的计算机模拟工具，使我们更接近预测性虚拟细胞。

Abstract: Accurately modeling the relationship between perturbations, transcriptional responses, and phenotypic changes is essential for building an AI Virtual Cell (AIVC). However, existing methods typically constrained to modeling direct associations, such as Perturbation $\rightarrow$ RNA or Perturbation $\rightarrow$ Morphology, overlook the crucial causal link from RNA to morphology. To bridge this gap, we propose TRIDENT, a cascade generative framework that synthesizes realistic cellular morphology by conditioning on both the perturbation and the corresponding gene expression profile. To train and evaluate this task, we construct MorphoGene, a new dataset pairing L1000 gene expression with Cell Painting images for 98 compounds. TRIDENT significantly outperforms state-of-the-art approaches, achieving up to 7-fold improvement with strong generalization to unseen compounds. In a case study on docetaxel, we validate that RNA-guided synthesis accurately produces the corresponding phenotype. An ablation study further confirms that this RNA conditioning is essential for the model's high fidelity. By explicitly modeling transcriptome-phenome mapping, TRIDENT provides a powerful in silico tool and moves us closer to a predictive virtual cell.

</details>


### [195] [ADF-LoRA: Alternating Low-Rank Aggregation for Decentralized Federated Fine-Tuning](https://arxiv.org/abs/2511.18291)
*Xiaoyu Wang,Xiaotian Li,Zhixiang Zhou,Chen Li,Yong Liu*

Main category: cs.LG

TL;DR: ADF-LoRA：一种用于去中心化联邦学习的交替低秩更新方法，通过同步更新单个低秩矩阵并混合两个矩阵来改善收敛稳定性和性能


<details>
  <summary>Details</summary>
Motivation: 在去中心化联邦学习中，交替更新LoRA矩阵会因相位状态不匹配和块级发散而面临新挑战，需要改进稳定性

Method: ADF-LoRA每轮只同步更新一个低秩矩阵，并混合两个矩阵以保持参数状态一致性，同时保留交替更新的交叉项抑制效果

Result: 在多个GLUE任务上的实验表明，ADF-LoRA实现了更快、更平滑的收敛，并在去中心化联邦学习中始终优于现有LoRA变体

Conclusion: ADF-LoRA通过改进的交替更新机制有效解决了去中心化联邦学习中的稳定性问题，取得了最佳性能

Abstract: This paper revisits alternating low-rank updates for federated fine-tuning and examines their behavior in decentralized federated learning (DFL). While alternating the LoRA matrices has been shown to stabilize aggregation in centralized FL, extending this mechanism to decentralized, peer-to-peer communication introduces new challenges due to phase-state mismatch and block-wise divergence across clients. We introduce ADF-LoRA, which synchronizes the update of only one low-rank matrix per round and mixes both matrices to maintain more consistent parameter states under decentralized propagation. This design preserves the cross-term suppression effect of alternating updates while improving stability in serverless topologies. We provide a convergence analysis under standard smoothness assumptions and evaluate ADF-LoRA on multiple GLUE tasks. Experiments show that ADF-LoRA achieves faster and smoother convergence and delivers the highest average accuracy across tasks, outperforming existing LoRA variants in decentralized FL by a consistent margin.

</details>


### [196] [MultiDiffNet: A Multi-Objective Diffusion Framework for Generalizable Brain Decoding](https://arxiv.org/abs/2511.18294)
*Mengchun Zhang,Kateryna Shapovalenko,Yucheng Shao,Eddie Guo,Parusha Pradhan*

Main category: cs.LG

TL;DR: MultiDiffNet是一个基于扩散模型的框架，通过优化多目标学习的紧凑潜在空间实现EEG神经解码，无需生成增强即可在跨被试和会话评估中达到最先进的泛化性能。


<details>
  <summary>Details</summary>
Motivation: EEG神经解码面临跨被试泛化能力差的问题，主要原因是被试间差异大且缺乏大规模数据集来有效建模这种差异。现有方法依赖合成被试生成或简单数据增强，但这些策略无法可靠地扩展或泛化。

Method: 提出MultiDiffNet扩散框架，完全绕过生成增强，学习一个为多目标优化的紧凑潜在空间，直接从此空间进行解码。

Result: 在各种神经解码任务中使用被试和会话分离评估，实现了最先进的泛化性能。同时发布了包含四个复杂度递增的EEG解码任务的统一基准套件和专门针对低试次EEG设置的统计报告框架。

Conclusion: 这项工作为现实世界BCI系统中的被试无关EEG解码提供了可重现和开源的基础。

Abstract: Neural decoding from electroencephalography (EEG) remains fundamentally limited by poor generalization to unseen subjects, driven by high inter-subject variability and the lack of large-scale datasets to model it effectively. Existing methods often rely on synthetic subject generation or simplistic data augmentation, but these strategies fail to scale or generalize reliably. We introduce \textit{MultiDiffNet}, a diffusion-based framework that bypasses generative augmentation entirely by learning a compact latent space optimized for multiple objectives. We decode directly from this space and achieve state-of-the-art generalization across various neural decoding tasks using subject and session disjoint evaluation. We also curate and release a unified benchmark suite spanning four EEG decoding tasks of increasing complexity (SSVEP, Motor Imagery, P300, and Imagined Speech) and an evaluation protocol that addresses inconsistent split practices in prior EEG research. Finally, we develop a statistical reporting framework tailored for low-trial EEG settings. Our work provides a reproducible and open-source foundation for subject-agnostic EEG decoding in real-world BCI systems.

</details>


### [197] [GROOT: Graph Edge Re-growth and Partitioning for the Verification of Large Designs in Logic Synthesis](https://arxiv.org/abs/2511.18297)
*Kiran Thorat,Hongwu Peng,Yuebo Luo,Xi Xie,Shaoyi Huang,Amit Hasan,Jiahui Zhao,Yingjie Li,Zhijie Shi,Cunxi Yu,Caiwen Ding*

Main category: cs.LG

TL;DR: GROOT是一个算法与系统协同设计的框架，通过结合芯片设计领域知识、图分区算法和重新设计的GPU内核，显著提高了芯片验证效率，在保持高精度的同时大幅减少了内存占用和运行时间。


<details>
  <summary>Details</summary>
Motivation: 传统芯片验证方法耗时且计算需求高，图神经网络(GNNs)虽能提高验证效率，但缺乏综合考虑芯片设计领域知识、图理论和GPU内核设计的联合框架。

Method: 1) 利用AIG图中节点类型和连接极性创建节点特征；2) 使用图分区算法将大图划分为小图进行GPU快速处理；3) 开发图边再生算法恢复验证精度；4) 针对EDA图工作负载的极化分布特点，重新设计HD-kernel和LD-kernel两个GPU内核。

Result: GROOT在1024位CSA乘法器(1.34亿节点，2.68亿边)上实现了59.38%的内存占用减少，99.96%的高精度。相比cuSPARSE、MergePath-SpMM和GNNAdvisor，运行时间分别提升了1.104倍、5.796倍和1.469倍。

Conclusion: GROOT通过算法与系统协同设计，成功解决了大规模芯片验证的效率问题，为EDA领域的图学习工作负载提供了高效的GPU解决方案。

Abstract: Traditional verification methods in chip design are highly time-consuming and computationally demanding, especially for large scale circuits. Graph neural networks (GNNs) have gained popularity as a potential solution to improve verification efficiency. However, there lacks a joint framework that considers all chip design domain knowledge, graph theory, and GPU kernel designs. To address this challenge, we introduce GROOT, an algorithm and system co-design framework that contains chip design domain knowledge and redesigned GPU kernels, to improve verification efficiency. More specifically, we create node features utilizing the circuit node types and the polarity of the connections between the input edges to nodes in And-Inverter Graphs (AIGs). We utilize a graph partitioning algorithm to divide the large graphs into smaller sub-graphs for fast GPU processing and develop a graph edge re-growth algorithm to recover verification accuracy. We carefully profile the EDA graph workloads and observe the uniqueness of their polarized distribution of high degree (HD) nodes and low degree (LD) nodes. We redesign two GPU kernels (HD-kernel and LD-kernel), to fit the EDA graph learning workload on a single GPU. We compare the results with state-of-the-art (SOTA) methods: GAMORA, a GNN-based approach, and the traditional ABC framework. Results show that GROOT achieves a significant reduction in memory footprint (59.38 %), with high accuracy (99.96%) for a very large CSA multiplier, i.e. 1,024 bits with a batch size of 16, which consists of 134,103,040 nodes and 268,140,544 edges. We compare GROOT with GPU-based GPU Kernel designs SOTAs such as cuSPARSE, MergePath-SpMM, and GNNAdvisor. We achieve up to 1.104x, 5.796x, and 1.469x improvement in runtime, respectively.

</details>


### [198] [Hierarchical Deep Research with Local-Web RAG: Toward Automated System-Level Materials Discovery](https://arxiv.org/abs/2511.18303)
*Rui Ding,Rodrigo Pires Ferreira,Yuxin Chen,Junhong Chen*

Main category: cs.LG

TL;DR: 提出了一种用于复杂材料和设备发现的长时程分层深度研究代理，通过本地部署的检索增强生成和深度研究树机制，在27个纳米材料/设备主题上表现优于商业系统，且成本更低。


<details>
  <summary>Details</summary>
Motivation: 解决现有机器学习代理和商业系统在复杂材料与设备发现问题上覆盖范围有限、无法本地集成的问题。

Method: 构建本地可部署的深度研究实例，集成检索增强生成与大型语言模型推理器，采用深度研究树机制自适应扩展和修剪研究分支。

Result: 在27个主题上评估显示，报告质量与商业系统相当或更好，成本显著降低，并通过5个代表性任务的干实验验证了可行性。

Conclusion: 该深度研究代理为复杂材料发现提供了经济高效的本地解决方案，支持与本地数据和工具的集成。

Abstract: We present a long-horizon, hierarchical deep research (DR) agent designed for complex materials and device discovery problems that exceed the scope of existing Machine Learning (ML) surrogates and closed-source commercial agents. Our framework instantiates a locally deployable DR instance that integrates local retrieval-augmented generation with large language model reasoners, enhanced by a Deep Tree of Research (DToR) mechanism that adaptively expands and prunes research branches to maximize coverage, depth, and coherence. We systematically evaluate across 27 nanomaterials/device topics using a large language model (LLM)-as-judge rubric with five web-enabled state-of-the-art models as jurors. In addition, we conduct dry-lab validations on five representative tasks, where human experts use domain simulations (e.g., density functional theory, DFT) to verify whether DR-agent proposals are actionable. Results show that our DR agent produces reports with quality comparable to--and often exceeding--those of commercial systems (ChatGPT-5-thinking/o3/o4-mini-high Deep Research) at a substantially lower cost, while enabling on-prem integration with local data and tools.

</details>


### [199] [DiM-TS: Bridge the Gap between Selective State Space Models and Time Series for Generative Modeling](https://arxiv.org/abs/2511.18312)
*Zihao Yao,Jiankai Zuo,Yaying Zhang*

Main category: cs.LG

TL;DR: 提出DiM-TS模型，通过融合滞后融合Mamba和置换扫描Mamba来增强时间序列生成能力，更好地捕捉长期时间依赖性和通道间相关性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在时间序列生成中难以捕捉长期时间依赖性和复杂通道相互关系，需要改进模型能力。

Method: 提出滞后融合Mamba和置换扫描Mamba两种变体，分析状态空间模型的核心限制（相关时间滞后和通道置换的考虑不足），并整合成DiM-TS模型。

Result: 在公共数据集上的综合实验表明DiM-TS在生成真实时间序列的同时能更好地保持数据的多样属性。

Conclusion: DiM-TS是一个高质量的时间序列生成模型，能更好地保持时间周期性和通道间相关性。

Abstract: Time series data plays a pivotal role in a wide variety of fields but faces challenges related to privacy concerns. Recently, synthesizing data via diffusion models is viewed as a promising solution. However, existing methods still struggle to capture long-range temporal dependencies and complex channel interrelations. In this research, we aim to utilize the sequence modeling capability of a State Space Model called Mamba to extend its applicability to time series data generation. We firstly analyze the core limitations in State Space Model, namely the lack of consideration for correlated temporal lag and channel permutation. Building upon the insight, we propose Lag Fusion Mamba and Permutation Scanning Mamba, which enhance the model's ability to discern significant patterns during the denoising process. Theoretical analysis reveals that both variants exhibit a unified matrix multiplication framework with the original Mamba, offering a deeper understanding of our method. Finally, we integrate two variants and introduce Diffusion Mamba for Time Series (DiM-TS), a high-quality time series generation model that better preserves the temporal periodicity and inter-channel correlations. Comprehensive experiments on public datasets demonstrate the superiority of DiM-TS in generating realistic time series while preserving diverse properties of data.

</details>


### [200] [AnyExperts: On-Demand Expert Allocation for Multimodal Language Models with Mixture of Expert](https://arxiv.org/abs/2511.18314)
*Yuting Gao,Wang Lan,Hengyuan Zhao,Linjiang Huang,Si Liu,Qingpei Guo*

Main category: cs.LG

TL;DR: AnyExperts提出了一种按需、预算感知的动态路由框架，通过基于语义重要性为每个token分配可变数量的专家槽位，优化多模态MoE模型的计算分配效率。


<details>
  <summary>Details</summary>
Motivation: 现有多模态MoE模型采用固定的专家激活策略，忽略了不同模态间语义重要性的异质性，导致关键token和冗余token消耗相同计算资源，造成计算分配不优。

Method: 提出AnyExperts框架：1）为每个token分配可变总数的专家槽位；2）总槽位数限制在固定范围内；3）每个槽位由真实专家或虚拟专家填充，虚拟专家比例上限为20%；4）根据语义重要性自适应平衡真实与虚拟专家比例。

Result: 在相同计算预算下提升性能：在通用图像/视频任务中，用40%更少的真实专家激活达到相当精度；在文本密集任务（OCR和NLP）中，减少10%真实专家使用同时保持性能。

Conclusion: 细粒度、重要性驱动的专家分配能显著提升多模态MoE模型的效率和有效性。

Abstract: Multimodal Mixture-of-Experts (MoE) models offer a promising path toward scalable and efficient large vision-language systems. However, existing approaches rely on rigid routing strategies (typically activating a fixed number of experts per token) ignoring the inherent heterogeneity in semantic importance across modalities. This leads to suboptimal compute allocation, where redundant tokens consume as many resources as critical ones. To address this, we propose AnyExperts, a novel on-demand, budget-aware dynamic routing framework that allocates a variable total number of expert slots per token based on its semantic importance. Crucially, to prevent uncontrolled compute growth, the total slots per token are constrained within a fixed range, and each slot is filled by either a real expert or a virtual expert, with the virtual share capped at a small maximum (e.g., 20%). The model then adaptively balances the real-to-virtual ratio per token, assigning more real experts to semantically rich regions and relying more on virtual experts for redundant content. Evaluated across diverse tasks in visual understanding, audio understanding, and NLP understanding, AnyExperts improves performance under the same compute budget. Notably, on general image/video tasks, it achieves comparable accuracy with 40% fewer real expert activations; on text-dense tasks (OCR and NLP), it maintains performance while reducing real expert usage by 10%. These results demonstrate that fine-grained, importance-driven expert allocation significantly enhances both the efficiency and effectiveness of multimodal MoE models.

</details>


### [201] [DynamiX: Dynamic Resource eXploration for Personalized Ad-Recommendations](https://arxiv.org/abs/2511.18331)
*Sohini Roychowdhury,Adam Holeman,Mohammad Amin,Feng Wei,Bhaskar Mehta,Srihari Reddy*

Main category: cs.LG

TL;DR: Dynamix是一个可扩展的个性化序列探索框架，通过最大相关性原则和基于事件特征的自监督学习来优化用户历史事件处理，在保持广告预测准确性的同时提高训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 在线广告推荐系统中，处理完整的用户广告互动历史计算量大且容易受到噪声影响，需要一种更高效的序列处理方法。

Method: 使用最大相关性原则和基于事件特征的自监督学习，在会话和表面级别对用户互动进行分类，通过动态特征移除和选择性特征增强来优化处理。

Result: 动态资源移除使训练和推理吞吐量分别提高1.15%和1.8%，动态特征增强提供0.033 NE增益，同时推理QPS提高4.2%。

Conclusion: Dynamix在基于用户序列的在线推荐模型中实现了显著的成本效率和性能改进，自监督用户分割和资源探索可以进一步优化复杂特征选择策略。

Abstract: For online ad-recommendation systems, processing complete user-ad-engagement histories is both computationally intensive and noise-prone. We introduce Dynamix, a scalable, personalized sequence exploration framework that optimizes event history processing using maximum relevance principles and self-supervised learning through Event Based Features (EBFs). Dynamix categorizes users-engagements at session and surface-levels by leveraging correlations between dwell-times and ad-conversion events. This enables targeted, event-level feature removal and selective feature boosting for certain user-segments, thereby yielding training and inference efficiency wins without sacrificing engaging ad-prediction accuracy. While, dynamic resource removal increases training and inference throughput by 1.15% and 1.8%, respectively, dynamic feature boosting provides 0.033 NE gains while boosting inference QPS by 4.2% over baseline models. These results demonstrate that Dynamix achieves significant cost efficiency and performance improvements in online user-sequence based recommendation models. Self-supervised user-segmentation and resource exploration can further boost complex feature selection strategies while optimizing for workflow and compute resources.

</details>


### [202] [Clinician-in-the-Loop Smart Home System to Detect Urinary Tract Infection Flare-Ups via Uncertainty-Aware Decision Support](https://arxiv.org/abs/2511.18334)
*Chibuike E. Ugwu,Roschelle Fritz,Diane J. Cook,Janardhan Rao Doppa*

Main category: cs.LG

TL;DR: 开发了一个临床医生在环的智能家居系统，利用环境传感器数据检测老年人尿路感染发作，通过不确定性量化方法提供临床决策支持


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法缺乏预测不确定性分析，限制了临床决策的有效性。老年人尿路感染往往在严重时才被发现，需要早期检测技术

Method: 采用临床医生在环的智能家居系统，结合环境传感器提取行为标记，使用Conformal-Calibrated Interval方法进行不确定性量化，在模型置信度低时拒绝预测

Result: 在8个真实智能家居数据上评估，方法在召回率等分类指标上优于基线方法，同时保持最低的拒绝比例和区间宽度

Conclusion: 42名护士的调查证实系统输出对临床决策具有实际价值，能够有效改善老年人尿路感染和其他病症发作的管理

Abstract: Urinary tract infection (UTI) flare-ups pose a significant health risk for older adults with chronic conditions. These infections often go unnoticed until they become severe, making early detection through innovative smart home technologies crucial. Traditional machine learning (ML) approaches relying on simple binary classification for UTI detection offer limited utility to nurses and practitioners as they lack insight into prediction uncertainty, hindering informed clinical decision-making. This paper presents a clinician-in-the-loop (CIL) smart home system that leverages ambient sensor data to extract meaningful behavioral markers, train robust predictive ML models, and calibrate them to enable uncertainty-aware decision support. The system incorporates a statistically valid uncertainty quantification method called Conformal-Calibrated Interval (CCI), which quantifies uncertainty and abstains from making predictions ("I don't know") when the ML model's confidence is low. Evaluated on real-world data from eight smart homes, our method outperforms baseline methods in recall and other classification metrics while maintaining the lowest abstention proportion and interval width. A survey of 42 nurses confirms that our system's outputs are valuable for guiding clinical decision-making, underscoring their practical utility in improving informed decisions and effectively managing UTIs and other condition flare-ups in older adults.

</details>


### [203] [Auxiliary Gene Learning: Spatial Gene Expression Estimation by Auxiliary Gene Selection](https://arxiv.org/abs/2511.18336)
*Kaito Shiku,Kazuya Nishimura,Shinnosuke Matsuo,Yasuhiro Kojima,Ryoma Bise*

Main category: cs.LG

TL;DR: 提出AGL方法，通过将忽略基因的表达估计重新定义为辅助任务并与主要任务联合训练，利用被忽略基因的益处。使用DkGSB方法选择对目标基因预测有积极影响的辅助基因子集。


<details>
  <summary>Details</summary>
Motivation: 空间转录组技术存在大量观测噪声，以往研究只使用高可变基因子集进行训练和评估，忽略了低表达基因可能通过共表达关系对评估目标做出贡献。

Method: AGL方法将忽略基因的表达估计作为辅助任务，与主要任务联合训练。提出DkGSB方法，利用先验知识对基因排序，将组合选择问题松弛为可微分的top-k选择问题。

Result: 实验证实了整合辅助基因的有效性，所提方法优于传统的辅助任务学习方法。

Conclusion: 通过合理选择辅助基因并联合训练，可以充分利用被忽略基因的信息，提高空间转录组数据分析的性能。

Abstract: Spatial transcriptomics (ST) is a novel technology that enables the observation of gene expression at the resolution of individual spots within pathological tissues. ST quantifies the expression of tens of thousands of genes in a tissue section; however, heavy observational noise is often introduced during measurement. In prior studies, to ensure meaningful assessment, both training and evaluation have been restricted to only a small subset of highly variable genes, and genes outside this subset have also been excluded from the training process. However, since there are likely co-expression relationships between genes, low-expression genes may still contribute to the estimation of the evaluation target. In this paper, we propose $Auxiliary \ Gene \ Learning$ (AGL) that utilizes the benefit of the ignored genes by reformulating their expression estimation as auxiliary tasks and training them jointly with the primary tasks. To effectively leverage auxiliary genes, we must select a subset of auxiliary genes that positively influence the prediction of the target genes. However, this is a challenging optimization problem due to the vast number of possible combinations. To overcome this challenge, we propose Prior-Knowledge-Based Differentiable Top-$k$ Gene Selection via Bi-level Optimization (DkGSB), a method that ranks genes by leveraging prior knowledge and relaxes the combinatorial selection problem into a differentiable top-$k$ selection problem. The experiments confirm the effectiveness of incorporating auxiliary genes and show that the proposed method outperforms conventional auxiliary task learning approaches.

</details>


### [204] [Future Is Unevenly Distributed: Forecasting Ability of LLMs Depends on What We're Asking](https://arxiv.org/abs/2511.18394)
*Chinmay Karkar,Paras Chopra*

Main category: cs.LG

TL;DR: LLMs在不同领域的预测能力差异显著，受问题类型、提示框架和外部知识影响，预测准确性高度依赖于提问内容和方式。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在真实世界事件预测中的能力变化，特别是针对模型截止日期后发生的事件，探索不同因素如何影响预测准确性和校准。

Method: 分析不同模型家族在真实世界问题上的表现，研究上下文、问题类型和外部知识对准确性和校准的影响，以及添加事实新闻背景如何改变信念形成和失败模式。

Result: LLMs的预测能力高度可变，取决于提问内容和方式，不同领域的预测能力差异显著。

Conclusion: LLMs的预测能力不是通用的，而是高度依赖于具体的领域结构、提示框架和外部知识整合方式。

Abstract: Large Language Models (LLMs) demonstrate partial forecasting competence across social, political, and economic events. Yet, their predictive ability varies sharply with domain structure and prompt framing. We investigate how forecasting performance varies with different model families on real-world questions about events that happened beyond the model cutoff date. We analyze how context, question type, and external knowledge affect accuracy and calibration, and how adding factual news context modifies belief formation and failure modes. Our results show that forecasting ability is highly variable as it depends on what, and how, we ask.

</details>


### [205] [Pre-training Graph Neural Networks on 2D and 3D Molecular Structures by using Multi-View Conditional Information Bottleneck](https://arxiv.org/abs/2511.18404)
*Van Thuy Hoang,O-Joun Lee*

Main category: cs.LG

TL;DR: MVCIB是一个用于2D和3D分子图预训练的多视图条件信息瓶颈框架，通过发现共享信息并最小化视图特定特征，利用功能组等关键子结构作为锚点，实现跨视图的子图对齐。


<details>
  <summary>Details</summary>
Motivation: 解决多视图分子学习中的两个主要挑战：(1)发现两个视图间的共享信息同时减少视图特定信息；(2)识别并对齐重要子结构（如功能组），以增强跨视图一致性和模型表达能力。

Method: 提出多视图条件信息瓶颈框架，使用一个视图作为上下文条件来指导另一个视图的表示学习。利用功能组和ego-networks作为视图间的锚点，提出跨注意力机制捕获子结构间的细粒度相关性，实现跨视图子图对齐。

Result: 在四个分子领域的广泛实验表明，MVCIB在预测性能和可解释性方面始终优于基线方法。MVCIB实现了3D Weisfeiler-Lehman表达能力，能够区分非同构图以及具有相同2D连接性的不同3D几何结构（如同分异构体）。

Conclusion: MVCIB框架有效解决了多视图分子学习中的关键挑战，通过条件信息瓶颈和子结构对齐策略，显著提升了分子图预训练的性能和表达能力。

Abstract: Recent pre-training strategies for molecular graphs have attempted to use 2D and 3D molecular views as both inputs and self-supervised signals, primarily aligning graph-level representations. However, existing studies remain limited in addressing two main challenges of multi-view molecular learning: (1) discovering shared information between two views while diminishing view-specific information and (2) identifying and aligning important substructures, e.g., functional groups, which are crucial for enhancing cross-view consistency and model expressiveness. To solve these challenges, we propose a Multi-View Conditional Information Bottleneck framework, called MVCIB, for pre-training graph neural networks on 2D and 3D molecular structures in a self-supervised setting. Our idea is to discover the shared information while minimizing irrelevant features from each view under the MVCIB principle, which uses one view as a contextual condition to guide the representation learning of its counterpart. To enhance semantic and structural consistency across views, we utilize key substructures, e.g., functional groups and ego-networks, as anchors between the two views. Then, we propose a cross-attention mechanism that captures fine-grained correlations between the substructures to achieve subgraph alignment across views. Extensive experiments in four molecular domains demonstrated that MVCIB consistently outperforms baselines in both predictive performance and interpretability. Moreover, MVCIB achieved the 3d Weisfeiler-Lehman expressiveness power to distinguish not only non-isomorphic graphs but also different 3D geometries that share identical 2D connectivity, such as isomers.

</details>


### [206] [Categorical Equivariant Deep Learning: Category-Equivariant Neural Networks and Universal Approximation Theorems](https://arxiv.org/abs/2511.18417)
*Yoshihiro Maruyama*

Main category: cs.LG

TL;DR: 本文提出了类别等变神经网络(CENNs)的统一理论，将群/群胚等变网络、偏序集/格等变网络、图和层神经网络统一起来，证明了在一般设置下的等变通用逼近定理。


<details>
  <summary>Details</summary>
Motivation: 统一现有的各种等变神经网络框架，将等变深度学习从群作用扩展到更广泛的范畴对称性，包括几何对称性、上下文对称性和组合对称性。

Method: 在具有Radon测度的拓扑范畴中形式化等变性，将线性和非线性层在范畴设置中表述，构建类别等变神经网络框架。

Result: 证明了有限深度CENNs在连续等变变换空间中是稠密的，为群/群胚、偏序集/格、图和胞腔层等具体实例推导了通用逼近定理。

Conclusion: 范畴等变深度学习扩展了等变深度学习的视野，超越了群作用，涵盖了更广泛的对称性类型。

Abstract: We develop a theory of category-equivariant neural networks (CENNs) that unifies group/groupoid-equivariant networks, poset/lattice-equivariant networks, graph and sheaf neural networks. Equivariance is formulated as naturality in a topological category with Radon measures, formulating linear and nonlinear layers in the categorical setup. We prove the equivariant universal approximation theorem in the general setting: the class of finite-depth CENNs is dense in the space of continuous equivariant transformations. We instantiate the framework for groups/groupoids, posets/lattices, graphs and cellular sheaves, deriving universal approximation theorems for them in a systematic manner. Categorical equivariant deep learning thus allows us to expand the horizons of equivariant deep learning beyond group actions, encompassing not only geometric symmetries but also contextual and compositional symmetries.

</details>


### [207] [Radiation-Preserving Selective Imaging for Pediatric Hip Dysplasia: A Cross-Modal Ultrasound-Xray Policy with Limited Labels](https://arxiv.org/abs/2511.18457)
*Duncan Stothers,Ben Stothers,Emily Schaeffer,Kishore Mulpuri*

Main category: cs.LG

TL;DR: 开发了一种基于超声优先、辐射保留的DDH筛查策略，通过自监督预训练和校准延迟规则，仅在需要时才进行X光检查，实现有限样本覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 减少DDH筛查中的辐射暴露，通过超声优先策略仅在必要时才进行X光检查，同时保证筛查的准确性和可靠性。

Method: 使用SimSiam在大量未标记数据上预训练模态特定编码器，冻结主干网络并拟合小型测量头，应用单边符合延迟规则进行校准。

Result: 超声测量误差适中（alpha MAE约9.7度，覆盖率MAE约14.0%），X光测量AI和CE的MAE分别为7.6度和8.9度，校准策略可在不同设置下平衡覆盖率和超声单独筛查率。

Conclusion: 开发了一个简单可复现的流程，将有限标签转化为可解释的测量结果和可调节的选择性成像曲线，适用于临床交接和未来外部验证。

Abstract: We study an ultrasound-first, radiation-preserving policy for developmental dysplasia of the hip (DDH) that requests a radiograph only when needed.
  We (i) pretrain modality-specific encoders (ResNet-18) with SimSiam on a large unlabelled registry (37186 ultrasound; 19546 radiographs), (ii) freeze the backbones and fit small, measurement-faithful heads on DDH relevant landmarks and measurements (iii) calibrate a one sided conformal deferral rule on ultrasound predictions that provides finite sample coverage guarantees under exchangeability, using a held-out calibration set. Ultrasound heads predict Graf alpha, beta, and femoral head coverage; X-ray heads predict acetabular index (AI), center-edge (CE) angle and IHDI grade. On our held out labeled evaluation set, ultrasound measurement error is modest (e.g., alpha MAE ~= 9.7 degrees, coverage MAE ~= 14.0%), while radiographic probes achieve AI and CE MAEs of ~= 7.6 degrees and ~= 8.9 degrees, respectively. The calibrated US-only policy is explored across rule families (alpha-only; alpha OR coverage; alpha AND coverage), uncertainty inflation factors, and per-utility trade-offs using decision-curve analysis. Conservative settings yield high coverage with near-zero US-only rates; permissive settings (e.g., alpha OR coverage at larger deltas) achieve non-zero US-only throughput with expected coverage tradeoffs. The result is a simple, reproducible pipeline that turns limited labels into interpretable measurements and tunable selective imaging curves suitable for clinical handoff and future external validation.

</details>


### [208] [SloMo-Fast: Slow-Momentum and Fast-Adaptive Teachers for Source-Free Continual Test-Time Adaptation](https://arxiv.org/abs/2511.18468)
*Md Akil Raihan Iftee,Mir Sazzat Hossain,Rakibul Hasan Rajib,Tariq Iqbal,Md Mofijul Islam,M Ashraful Amin,Amin Ahsan Ali,AKM Mahbubur Rahman*

Main category: cs.LG

TL;DR: 提出了SloMo-Fast框架，这是一个无需源数据的双教师持续测试时适应方法，通过慢速教师和快速教师的互补机制解决长期遗忘问题，并在循环域转移场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法依赖源数据或原型，在隐私敏感和资源受限环境中适用性有限，且存在长期遗忘问题，导致在先前遇到域上的性能下降。

Method: SloMo-Fast框架包含两个互补教师：慢速教师（Slow-Teacher）缓慢遗忘并保留长期知识确保鲁棒泛化；快速教师（Fast-Teacher）快速适应新域并跨域积累知识。同时提出了循环测试时适应基准（Cyclic-TTA）。

Result: 在Cyclic-TTA和其他十个CTTA设置上的广泛实验表明，SloMo-Fast始终优于最先进方法，显示出其在演化和重访域上的适应和泛化能力。

Conclusion: SloMo-Fast通过双教师框架有效解决了CTTA中的长期遗忘问题，在无需源数据的情况下实现了对演化和循环域转移的鲁棒适应。

Abstract: Continual Test-Time Adaptation (CTTA) is crucial for deploying models in real-world applications with unseen, evolving target domains. Existing CTTA methods, however, often rely on source data or prototypes, limiting their applicability in privacy-sensitive and resource-constrained settings. Additionally, these methods suffer from long-term forgetting, which degrades performance on previously encountered domains as target domains shift. To address these challenges, we propose SloMo-Fast, a source-free, dual-teacher CTTA framework designed for enhanced adaptability and generalization. It includes two complementary teachers: the Slow-Teacher, which exhibits slow forgetting and retains long-term knowledge of previously encountered domains to ensure robust generalization, and the Fast-Teacher rapidly adapts to new domains while accumulating and integrating knowledge across them. This framework preserves knowledge of past domains and adapts efficiently to new ones. We also introduce Cyclic Test-Time Adaptation (Cyclic-TTA), a novel CTTA benchmark that simulates recurring domain shifts. Our extensive experiments demonstrate that SloMo-Fast consistently outperforms state-of-the-art methods across Cyclic-TTA, as well as ten other CTTA settings, highlighting its ability to both adapt and generalize across evolving and revisited domains.

</details>


### [209] [Adaptive Mesh-Quantization for Neural PDE Solvers](https://arxiv.org/abs/2511.18474)
*Winfried van den Dool,Maksim Zhdanov,Yuki M. Asano,Max Welling*

Main category: cs.LG

TL;DR: 提出自适应网格量化方法，通过轻量级辅助模型识别高损失区域，动态调整量化位宽，在复杂物理区域分配更多计算资源，实现计算效率优化。


<details>
  <summary>Details</summary>
Motivation: 物理系统通常具有空间变化的复杂性，现有图神经网络在非规则网格上处理复杂几何和边界条件时，对所有节点采用统一计算强度，导致资源分配效率低下。

Method: 引入自适应网格量化：在网格节点、边和簇特征上进行空间自适应量化，通过轻量级辅助模型识别输入网格中的高损失区域，驱动自适应位宽分配策略。

Result: 在2D Darcy流、大规模非稳态流体动力学、3D稳态Navier-Stokes模拟和2D超弹性问题等多个任务中，与MP-PDE和GraphViT集成，相比均匀量化基线获得一致的Pareto改进，在相同成本下性能提升高达50%。

Conclusion: 自适应网格量化框架能够动态分配计算资源，在保持计算成本的同时显著提升模型性能，为复杂物理系统的高效求解提供了有效解决方案。

Abstract: Physical systems commonly exhibit spatially varying complexity, presenting a significant challenge for neural PDE solvers. While Graph Neural Networks can handle the irregular meshes required for complex geometries and boundary conditions, they still apply uniform computational effort across all nodes regardless of the underlying physics complexity. This leads to inefficient resource allocation where computationally simple regions receive the same treatment as complex phenomena. We address this challenge by introducing Adaptive Mesh Quantization: spatially adaptive quantization across mesh node, edge, and cluster features, dynamically adjusting the bit-width used by a quantized model. We propose an adaptive bit-width allocation strategy driven by a lightweight auxiliary model that identifies high-loss regions in the input mesh. This enables dynamic resource distribution in the main model, where regions of higher difficulty are allocated increased bit-width, optimizing computational resource utilization. We demonstrate our framework's effectiveness by integrating it with two state-of-the-art models, MP-PDE and GraphViT, to evaluate performance across multiple tasks: 2D Darcy flow, large-scale unsteady fluid dynamics in 2D, steady-state Navier-Stokes simulations in 3D, and a 2D hyper-elasticity problem. Our framework demonstrates consistent Pareto improvements over uniformly quantized baselines, yielding up to 50% improvements in performance at the same cost.

</details>


### [210] [Real-Time Personalized Content Adaptation through Matrix Factorization and Context-Aware Federated Learning](https://arxiv.org/abs/2511.18489)
*Sai Puppala,Ismail Hossain,Md Jahangir Alam,Sajedul Talukder*

Main category: cs.LG

TL;DR: 提出基于联邦学习的个性化LLM框架，通过本地数据微调GPT模型，结合用户画像评分和社交网络分析，实现隐私保护的实时个性化内容推荐。


<details>
  <summary>Details</summary>
Motivation: 解决社交媒体平台中用户交互和内容相关性的挑战，在保护用户隐私的同时提供个性化体验。

Method: 采用联邦学习框架，在客户端本地微调GPT模型，结合内容分类、用户画像评分、社交网络分析和矩阵分解技术。

Result: 系统能够实时提供个性化内容建议，通过自适应反馈循环和可读性评分算法显著提升内容质量和相关性。

Conclusion: 该综合解决方案不仅解决了内容过滤和推荐问题，还促进了更吸引人的社交媒体体验，为数字平台的个性化交互设定了新标准。

Abstract: Our study presents a multifaceted approach to enhancing user interaction and content relevance in social media platforms through a federated learning framework. We introduce personalized LLM Federated Learning and Context-based Social Media models. In our framework, multiple client entities receive a foundational GPT model, which is fine-tuned using locally collected social media data while ensuring data privacy through federated aggregation. Key modules focus on categorizing user-generated content, computing user persona scores, and identifying relevant posts from friends networks. By integrating a sophisticated social engagement quantification method with matrix factorization techniques, our system delivers real-time personalized content suggestions tailored to individual preferences. Furthermore, an adaptive feedback loop, alongside a robust readability scoring algorithm, significantly enhances the quality and relevance of the content presented to users. This comprehensive solution not only addresses the challenges of content filtering and recommendation but also fosters a more engaging social media experience while safeguarding user privacy, setting a new standard for personalized interactions in digital platforms.

</details>


### [211] [RRaPINNs: Residual Risk-Aware Physics Informed Neural Networks](https://arxiv.org/abs/2511.18515)
*Ange-Clément Akazan,Issa Karambal,Jean Medard Ngnotchouye,Abebe Geletu Selassie. W*

Main category: cs.LG

TL;DR: 提出了RRaPINNs框架，通过条件风险价值(CVaR)优化尾部残差，使用均值超额(ME)代理惩罚直接控制最坏情况PDE残差，将PINN训练转化为风险敏感优化。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs最小化平均残差会掩盖大的局部化误差，需要开发能够控制尾部残差的方法来提高可靠性。

Method: 使用条件风险价值(CVaR)优化尾部聚焦目标，引入均值超额(ME)代理惩罚来直接控制最坏情况PDE残差，将PINN训练转化为风险敏感优化并与机会约束公式相关联。

Result: 在多个PDE（Burgers、Heat、Korteweg-de-Vries、Poisson等）上，RRaPINNs减少了尾部残差，同时保持或改进了平均误差，ME代理比直接CVaR铰链产生更平滑的优化。

Conclusion: RRaPINNs为平滑和不连续PDE提供了可靠性感知科学机器学习的实用路径，机会约束可靠性水平α作为透明旋钮在整体精度和严格尾部控制之间进行权衡。

Abstract: Physics-informed neural networks (PINNs) typically minimize average residuals, which can conceal large, localized errors. We propose Residual Risk-Aware Physics-Informed Neural Networks PINNs (RRaPINNs), a single-network framework that optimizes tail-focused objectives using Conditional Value-at-Risk (CVaR), we also introduced a Mean-Excess (ME) surrogate penalty to directly control worst-case PDE residuals. This casts PINN training as risk-sensitive optimization and links it to chance-constrained formulations. The method is effective and simple to implement. Across several partial differential equations (PDEs) such as Burgers, Heat, Korteweg-de-Vries, and Poisson (including a Poisson interface problem with a source jump at x=0.5) equations, RRaPINNs reduce tail residuals while maintaining or improving mean errors compared to vanilla PINNs, Residual-Based Attention and its variant using convolution weighting; the ME surrogate yields smoother optimization than a direct CVaR hinge. The chance constraint reliability level $α$ acts as a transparent knob trading bulk accuracy (lower $α$ ) for stricter tail control (higher $α$ ). We discuss the framework limitations, including memoryless sampling, global-only tail budgeting, and residual-centric risk, and outline remedies via persistent hard-point replay, local risk budgets, and multi-objective risk over BC/IC terms. RRaPINNs offer a practical path to reliability-aware scientific ML for both smooth and discontinuous PDEs.

</details>


### [212] [CHIPS: Efficient CLIP Adaptation via Curvature-aware Hybrid Influence-based Data Selection](https://arxiv.org/abs/2511.18519)
*Xinlin Zhuang,Yichen Li,Xiwei Liu,Haolin Yang,Yifan Lu,Ziyun Zou,Yulong Li,Huifa Li,Dongliang Chen,Qinglei Wang,Weiyang Liu,Ying Qian,Jiangming Shi,Imran Razzak*

Main category: cs.LG

TL;DR: CHIPS是一种数据选择方法，通过计算图像-文本对的效用分数来选择高质量数据，在垂直领域适应中仅需30%数据即可达到全数据集持续预训练的效果，10%数据即可超越半数据集训练。


<details>
  <summary>Details</summary>
Motivation: 当前垂直领域CLIP适应主要依赖大规模领域特定数据集进行微调或持续预训练，但数据本身作为关键因素未被充分探索。本文从数据中心视角出发，研究能否通过有效数据选择替代大规模数据集。

Method: 提出CHIPS方法，为每个图像-文本对计算效用分数，整合三个互补因素：通过曲率感知的牛顿式对齐确保忠实性；通过InfoNCE感知的曲率估计器和JL草图确保可扩展性；通过选择感知的相关性权重和可学习性平衡目标适应与通用领域保留。

Result: 在17个医学基准测试中达到选择基线的最先进性能，仅用30%数据即可匹配全数据集持续预训练，仅用10%数据即可超越半数据集训练；在31个通用领域基准测试中，在10-30%数据保留预算下性能下降最小。

Conclusion: CHIPS证明了通过精心设计的数据选择策略可以显著减少持续预训练所需的数据量，在保持性能的同时提高训练效率，为垂直领域适应提供了数据高效的新范式。

Abstract: Adapting CLIP to vertical domains is typically approached by novel fine-tuning strategies or by continual pre-training (CPT) on large domain-specific datasets. Yet, data itself remains an underexplored factor in this process. We revisit this task from a data-centric perspective: Can effective data selection substitute for large-scale datasets in CPT? We introduce CHIPS (Curvature-aware Hybrid Influence in Projection Subspace), which assigns each image-text pair a utility score that integrates three complementary factors aligned with three goals: faithfulness via a curvature-aware, Newton-style alignment computed in CLIP's end-point subspace; scalability via an InfoNCE-aware curvature estimator with Johnson-Lindenstrauss (JL) sketching; and retention via a selection-aware relevance weight combined with learnability to balance target adaptation against general-domain preservation. We justify this design theoretically by proving a lower-bound guarantee on the proxy's correlation with full-parameter alignment and by characterizing the bias-variance trade-offs introduced by curvature mixing and JL sketching. We evaluate CHIPS empirically across various settings: 1) CHIPS attains state-of-the-art performance among selection baselines on 17 medical benchmarks, matches full-dataset CPT with 30% of the data, and outperforms half-dataset CPT using only 10%; 2) on 31 general-domain benchmarks, CHIPS yields the smallest performance drop under 10-30% data-retention budgets. Code, data, and checkpoints will be released.

</details>


### [213] [Hyperspectral Variational Autoencoders for Joint Data Compression and Component Extraction](https://arxiv.org/abs/2511.18521)
*Core Francisco Park,Manuel Perez-Carrasco,Caroline Nowlan,Cecilia Garraffo*

Main category: cs.LG

TL;DR: 使用变分自编码器(VAE)对NASA TEMPO卫星高光谱数据进行514倍压缩，重建误差比信号低1-2个数量级，同时研究了压缩潜在空间对大气信息的保留能力。


<details>
  <summary>Details</summary>
Motivation: 解决地球同步高光谱卫星每日产生TB级数据带来的存储、传输和分发挑战，为下一代地球观测系统解决关键瓶颈。

Method: 采用变分自编码器(VAE)方法压缩高光谱数据，并使用线性和非线性探针从压缩潜在空间中提取Level-2产品(NO2、O3、HCHO、云分数)。

Result: 实现了514倍数据压缩，重建误差比信号低1-2个数量级。云分数和总臭氧提取性能良好(R²=0.93和0.81)，但NO2和HCHO提取面临挑战(R²=0.20和0.51)。

Conclusion: 神经压缩可显著减少高光谱数据量，同时保留关键大气信号，但某些大气产品的编码存在根本性挑战，非线性探针优于线性探针。

Abstract: Geostationary hyperspectral satellites generate terabytes of data daily, creating critical challenges for storage, transmission, and distribution to the scientific community. We present a variational autoencoder (VAE) approach that achieves x514 compression of NASA's TEMPO satellite hyperspectral observations (1028 channels, 290-490nm) with reconstruction errors 1-2 orders of magnitude below the signal across all wavelengths. This dramatic data volume reduction enables efficient archival and sharing of satellite observations while preserving spectral fidelity. Beyond compression, we investigate to what extent atmospheric information is retained in the compressed latent space by training linear and nonlinear probes to extract Level-2 products (NO2, O3, HCHO, cloud fraction). Cloud fraction and total ozone achieve strong extraction performance (R^2 = 0.93 and 0.81 respectively), though these represent relatively straightforward retrievals given their distinct spectral signatures. In contrast, tropospheric trace gases pose genuine challenges for extraction (NO2 R^2 = 0.20, HCHO R^2 = 0.51) reflecting their weaker signals and complex atmospheric interactions. Critically, we find the VAE encodes atmospheric information in a semi-linear manner - nonlinear probes substantially outperform linear ones - and that explicit latent supervision during training provides minimal improvement, revealing fundamental encoding challenges for certain products. This work demonstrates that neural compression can dramatically reduce hyperspectral data volumes while preserving key atmospheric signals, addressing a critical bottleneck for next-generation Earth observation systems. Code - https://github.com/cfpark00/Hyperspectral-VAE

</details>


### [214] [TimePre: Bridging Accuracy, Efficiency, and Stability in Probabilistic Time-Series Forecasting](https://arxiv.org/abs/2511.18539)
*Lingyu Jiang,Lingyu Xu,Peiran Li,Qianwen Ge,Dingyi Zhuang,Shuo Xing,Wenjing Chen,Xiangbo Gao,Ting-Hsuan Chen,Xueying Zhan,Xin Zhang,Ziming Zhang,Zhengzhong Tu,Michael Zielewski,Kazunori Yamada,Fangzhou Lin*

Main category: cs.LG

TL;DR: TimePre是一个新颖的概率时间序列预测框架，通过稳定实例归一化(SIN)解决了MLP骨干网络与多选择学习(MCL)结合时的训练不稳定和假设崩溃问题，在保持高效推理的同时实现了最先进的预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有的概率时间序列预测方法存在计算效率问题：基于扩散的方法需要昂贵的迭代采样，而非采样框架如MCL虽然高效但训练不稳定且容易发生假设崩溃，特别是与现代MLP骨干网络结合时问题更加严重。

Method: 提出TimePre框架，核心是稳定实例归一化(SIN)层，通过校正通道级统计偏移来稳定混合架构，彻底解决灾难性假设崩溃问题，成功统一MLP模型的高效性和MCL范式的分布灵活性。

Result: 在六个基准数据集上的广泛实验表明，TimePre在关键概率指标上达到了新的最先进精度，推理速度比基于采样的模型快几个数量级，并且表现出稳定的性能扩展能力。

Conclusion: TimePre弥合了概率预测中准确性、效率和稳定性之间的长期差距，为不确定性感知决策提供了既准确又高效的解决方案。

Abstract: Probabilistic Time-Series Forecasting (PTSF) is critical for uncertainty-aware decision making, but existing generative models, such as diffusion-based approaches, are computationally prohibitive due to expensive iterative sampling. Non-sampling frameworks like Multiple Choice Learning (MCL) offer an efficient alternative, but suffer from severe training instability and hypothesis collapse, which has historically hindered their performance. This problem is dramatically exacerbated when attempting to combine them with modern, efficient MLP-based backbones. To resolve this fundamental incompatibility, we propose TimePre, a novel framework that successfully unifies the efficiency of MLP-based models with the distributional flexibility of the MCL paradigm. The core of our solution is Stabilized Instance Normalization (SIN), a novel normalization layer that explicitly remedies this incompatibility. SIN stabilizes the hybrid architecture by correcting channel-wise statistical shifts, definitively resolving the catastrophic hypothesis collapse. Extensive experiments on six benchmark datasets demonstrate that TimePre achieves new state-of-the-art accuracy on key probabilistic metrics. Critically, TimePre achieves inference speeds orders of magnitude faster than sampling-based models and, unlike prior MCL work, demonstrates stable performance scaling. It thus bridges the long-standing gap between accuracy, efficiency, and stability in probabilistic forecasting.

</details>


### [215] [In Search of Goodness: Large Scale Benchmarking of Goodness Functions for the Forward-Forward Algorithm](https://arxiv.org/abs/2511.18567)
*Arya Shah,Vaibhav Tripathi*

Main category: cs.LG

TL;DR: 本文系统评估了Forward-Forward算法中21种不同的goodness函数，发现在四个图像数据集上某些替代函数显著优于标准平方和基准，并揭示了预测性能与计算效率之间的关键权衡。


<details>
  <summary>Details</summary>
Motivation: Forward-Forward算法作为反向传播的生物合理替代方案，其效果严重依赖于goodness函数的定义。目前主要使用简单的平方和度量，但这是否是最优选择尚不明确。

Method: 在四个标准图像数据集（MNIST、FashionMNIST、CIFAR-10、STL-10）上对21种不同的goodness函数进行基准测试，评估分类准确率、能耗和碳足迹。

Result: 发现特定替代goodness函数显著优于标准基线：game_theoretic_local在MNIST上达到97.15%准确率，softmax_energy_margin_local在FashionMNIST上达到82.84%，triplet_margin_local在STL-10上达到37.69%。计算效率存在显著差异。

Conclusion: goodness函数是FF算法设计中的关键超参数，需要在预测性能和环境影响之间进行权衡。代码已在GitHub上发布以供参考和复现。

Abstract: The Forward-Forward (FF) algorithm offers a biologically plausible alternative to backpropagation, enabling neural networks to learn through local updates. However, FF's efficacy relies heavily on the definition of "goodness", which is a scalar measure of neural activity. While current implementations predominantly utilize a simple sum-of-squares metric, it remains unclear if this default choice is optimal. To address this, we benchmarked 21 distinct goodness functions across four standard image datasets (MNIST, FashionMNIST, CIFAR-10, STL-10), evaluating classification accuracy, energy consumption, and carbon footprint. We found that certain alternative goodness functions inspired from various domains significantly outperform the standard baseline. Specifically, \texttt{game\_theoretic\_local} achieved 97.15\% accuracy on MNIST, \texttt{softmax\_energy\_margin\_local} reached 82.84\% on FashionMNIST, and \texttt{triplet\_margin\_local} attained 37.69\% on STL-10. Furthermore, we observed substantial variability in computational efficiency, highlighting a critical trade-off between predictive performance and environmental cost. These findings demonstrate that the goodness function is a pivotal hyperparameter in FF design. We release our code on \href{https://github.com/aryashah2k/In-Search-of-Goodness}{Github} for reference and reproducibility.

</details>


### [216] [SAMBA: Toward a Long-Context EEG Foundation Model via Spatial Embedding and Differential Mamba](https://arxiv.org/abs/2511.18571)
*Jiazhen Hong,Geoffrey Mackellar,Soheila Ghane*

Main category: cs.LG

TL;DR: SAMBA是一个基于Mamba的自监督学习框架，用于长序列EEG建模，通过U形编码器-解码器架构有效捕捉EEG数据中的长程时间依赖和空间变异性，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: EEG数据采样率高、记录时间长，需要建模长序列；Transformer的二次复杂度限制了长上下文建模；电极配置差异和个体间信号差异对开发通用基础模型构成挑战。

Method: 使用Mamba-based U形编码器-解码器架构；引入时间语义随机掩码进行语义级序列重建；多头部差分Mamba模块抑制冗余并突出显著时间结构；空间自适应输入嵌入在三维欧几里得空间中学习统一嵌入。

Result: 在13个EEG数据集上的实验表明，SAMBA在多种任务、电极配置和序列长度下始终优于最先进方法，同时保持低内存消耗和推理时间；学习到的空间权重图与任务相关神经生理区域高度一致。

Conclusion: SAMBA展示了作为实时脑机接口应用基础模型的可扩展性和实际潜力，其嵌入模块具有可学习性和可解释性。

Abstract: Long-sequence electroencephalogram (EEG) modeling is essential for developing generalizable EEG representation models. This need arises from the high sampling rate of EEG data and the long recording durations required to capture extended neurological patterns in brain activity. Transformer-based models have shown promise in modeling short sequences of a few seconds; however, their quadratic complexity limits scalability to longer contexts. Moreover, variability in electrode montage across available datasets, along with inter-subject differences in brain signals, pose significant challenges to developing a generalizable and robust foundation model. We propose \textit{SAMBA}, a self-supervised learning framework with a Mamba-based U-shaped encoder-decoder architecture, which effectively captures long-range temporal dependencies and spatial variability in EEG data. Leveraging the inherent ability of Mamba in processing long context sizes, we introduce: (1) \textit{Temporal Semantic Random Masking} for semantic-level sequence reconstruction, (2) a \textit{Multi-Head Differential Mamba} module to suppress redundancy and emphasize salient temporal structures, and (3) a \textit{Spatial-Adaptive Input Embedding} that learns unified embeddings in a three-dimensional Euclidean space, enabling robustness across devices. Experiments on thirteen EEG datasets across diverse tasks, electrode configurations, and sequence durations demonstrate that SAMBA consistently outperforms state-of-the-art methods while maintaining low memory consumption and inference time. We also show the learned spatial weight maps from our embedding module align closely with task-relevant neurophysiological regions, demonstrating the learnability and interpretability of SAMBA. These results highlight SAMBA's scalability and practical potential as a foundation model for real-time brain-computer interface applications.

</details>


### [217] [Generative Myopia: Why Diffusion Models Fail at Structure](https://arxiv.org/abs/2511.18593)
*Milad Siami*

Main category: cs.LG

TL;DR: Graph Diffusion Models (GDMs) 存在生成性近视问题，在优化统计似然时偏向常见子结构而忽略光谱关键结构，导致在图稀疏化等组合任务中移除关键但罕见的桥梁边。作者提出光谱加权扩散方法，使用有效电阻重新对齐变分目标，解决了该问题。


<details>
  <summary>Details</summary>
Motivation: GDMs 在优化统计似然时隐式地作为频率过滤器，偏好丰富的子结构而忽略光谱关键结构，这种现象被称为生成性近视。在图稀疏化等组合任务中，这会导致灾难性地移除结构必需但统计上罕见的桥梁边。

Method: 引入光谱加权扩散方法，使用有效电阻重新对齐变分目标。光谱先验可以在训练阶段摊销，实现零推理开销。

Result: 该方法消除了近视问题，与最优光谱预言机的性能匹配，在标准扩散完全失败（0%）的对抗性基准测试中实现了100%的连通性。

Conclusion: 光谱先验可以有效地解决GDMs中的生成性近视问题，通过重新对齐优化目标来保留结构关键但统计罕见的元素。

Abstract: Graph Diffusion Models (GDMs) optimize for statistical likelihood, implicitly acting as \textbf{frequency filters} that favor abundant substructures over spectrally critical ones. We term this phenomenon \textbf{Generative Myopia}. In combinatorial tasks like graph sparsification, this leads to the catastrophic removal of ``rare bridges,'' edges that are structurally mandatory ($R_{\text{eff}} \approx 1$) but statistically scarce. We prove theoretically and empirically that this failure is driven by \textbf{Gradient Starvation}: the optimization landscape itself suppresses rare structural signals, rendering them unlearnable regardless of model capacity. To resolve this, we introduce \textbf{Spectrally-Weighted Diffusion}, which re-aligns the variational objective using Effective Resistance. We demonstrate that spectral priors can be amortized into the training phase with zero inference overhead. Our method eliminates myopia, matching the performance of an optimal Spectral Oracle and achieving \textbf{100\% connectivity} on adversarial benchmarks where standard diffusion fails completely (0\%).

</details>


### [218] [CycleSL: Server-Client Cyclical Update Driven Scalable Split Learning](https://arxiv.org/abs/2511.18611)
*Mengdi Wang,Efe Bozkir,Enkelejda Kasneci*

Main category: cs.LG

TL;DR: CycleSL是一个新颖的无聚合分割学习框架，通过循环更新机制解决传统分割学习的可扩展性和性能问题，将服务器端训练视为独立的高级机器学习任务。


<details>
  <summary>Details</summary>
Motivation: 传统分割学习存在可扩展性差和服务器资源开销大的问题，并行变体由于模型复制、聚合以及客户端漂移等因素导致模型性能和收敛性下降。

Method: 受交替块坐标下降启发，CycleSL将服务器端训练视为独立的高级机器学习任务，对客户端提取的特征进行重采样以减轻异构性和漂移，采用循环更新机制：先优化服务器模型，然后使用更新后的服务器进行客户端梯度计算。

Result: 在五个公开数据集上的实验表明，CycleSL能有效提升模型性能，特别是在非独立同分布数据和部分客户端参与的场景下。

Conclusion: CycleSL通过无聚合设计和循环更新机制，显著提升了分割学习的可扩展性和模型性能，并能与现有方法无缝集成。

Abstract: Split learning emerges as a promising paradigm for collaborative distributed model training, akin to federated learning, by partitioning neural networks between clients and a server without raw data exchange. However, sequential split learning suffers from poor scalability, while parallel variants like parallel split learning and split federated learning often incur high server resource overhead due to model duplication and aggregation, and generally exhibit reduced model performance and convergence owing to factors like client drift and lag. To address these limitations, we introduce CycleSL, a novel aggregation-free split learning framework that enhances scalability and performance and can be seamlessly integrated with existing methods. Inspired by alternating block coordinate descent, CycleSL treats server-side training as an independent higher-level machine learning task, resampling client-extracted features (smashed data) to mitigate heterogeneity and drift. It then performs cyclical updates, namely optimizing the server model first, followed by client updates using the updated server for gradient computation. We integrate CycleSL into previous algorithms and benchmark them on five publicly available datasets with non-iid data distribution and partial client attendance. Our empirical findings highlight the effectiveness of CycleSL in enhancing model performance. Our source code is available at https://gitlab.lrz.de/hctl/CycleSL.

</details>


### [219] [KAN vs LSTM Performance in Time Series Forecasting](https://arxiv.org/abs/2511.18613)
*Tabish Ali Rather,S M Mahmudul Hasan Joy,Nadezda Sukhorukova,Federico Frascoli*

Main category: cs.LG

TL;DR: 对比KAN和LSTM在股票价格预测中的表现，LSTM在准确性上显著优于KAN，而KAN主要在计算效率上有优势。


<details>
  <summary>Details</summary>
Motivation: 评估KAN和LSTM在非确定性股票价格数据预测中的表现，分析预测准确性与可解释性之间的权衡。

Method: 使用均方根误差(RMSE)评估两种模型在不同预测时间跨度上的表现。

Result: LSTM在所有测试的预测时间跨度上都表现出显著优势，而标准KAN虽然具有理论可解释性，但误差率显著更高。

Conclusion: LSTM在精度要求高的时间序列应用中占主导地位，而KAN的主要优势在于计算效率，适合资源受限但精度要求不严格的场景。

Abstract: This paper compares Kolmogorov-Arnold Networks (KAN) and Long Short-Term Memory networks (LSTM) for forecasting non-deterministic stock price data, evaluating predictive accuracy versus interpretability trade-offs using Root Mean Square Error (RMSE).LSTM demonstrates substantial superiority across all tested prediction horizons, confirming their established effectiveness for sequential data modelling. Standard KAN, while offering theoretical interpretability through the Kolmogorov-Arnold representation theorem, exhibits significantly higher error rates and limited practical applicability for time series forecasting. The results confirm LSTM dominance in accuracy-critical time series applications while identifying computational efficiency as KANs' primary advantage in resource-constrained scenarios where accuracy requirements are less stringent. The findings support LSTM adoption for practical financial forecasting while suggesting that continued research into specialised KAN architectures may yield future improvements.

</details>


### [220] [Bayesian-based Online Label Shift Estimation with Dynamic Dirichlet Priors](https://arxiv.org/abs/2511.18615)
*Jiawei Hu,Javier A. Barria*

Main category: cs.LG

TL;DR: 提出了FMAPLS和online-FMAPLS两种贝叶斯标签偏移估计方法，通过联合优化Dirichlet超参数和类先验，显著提升了分类器在标签偏移下的性能。


<details>
  <summary>Details</summary>
Motivation: 标签偏移是监督学习中的常见挑战，当测试数据的类先验分布与训练数据不同时，会导致分类器性能显著下降。现有MAPLS方法存在刚性约束限制，需要更灵活有效的解决方案。

Method: 使用批量和在线EM算法联合优化Dirichlet超参数α和类先验π，引入线性替代函数(LSF)替代基于梯度的超参数更新，获得闭式解降低计算复杂度。在线版本用随机近似替代批量E步，实现实时适应流数据。

Result: 在CIFAR100和ImageNet数据集上的实验表明，FMAPLS和online-FMAPLS分别实现了高达40%和12%的KL散度降低，并在后偏移准确率上显著优于现有最优方法，特别是在严重类别不平衡和分布不确定性下表现优异。

Conclusion: 所提方法在大规模和动态学习场景中展现出鲁棒性、可扩展性和适用性，为标签偏移问题提供了有效的贝叶斯解决方案。

Abstract: Label shift, a prevalent challenge in supervised learning, arises when the class prior distribution of test data differs from that of training data, leading to significant degradation in classifier performance. To accurately estimate the test priors and enhance classification accuracy, we propose a Bayesian framework for label shift estimation, termed Full Maximum A Posterior Label Shift (FMAPLS), along with its online version, online-FMAPLS. Leveraging batch and online Expectation-Maximization (EM) algorithms, these methods jointly and dynamically optimize Dirichlet hyperparameters $\boldsymbolα$ and class priors $\boldsymbolπ$, thereby overcoming the rigid constraints of the existing Maximum A Posterior Label Shift (MAPLS) approach. Moreover, we introduce a linear surrogate function (LSF) to replace gradient-based hyperparameter updates, yielding closed-form solutions that reduce computational complexity while retaining asymptotic equivalence. The online variant substitutes the batch E-step with a stochastic approximation, enabling real-time adaptation to streaming data. Furthermore, our theoretical analysis reveals a fundamental trade-off between online convergence rate and estimation accuracy. Extensive experiments on CIFAR100 and ImageNet datasets under shuffled long-tail and Dirichlet test priors demonstrate that FMAPLS and online-FMAPLS respectively achieve up to 40% and 12% lower KL divergence and substantial improvements in post-shift accuracy over state-of-the-art baselines, particularly under severe class imbalance and distributional uncertainty. These results confirm the robustness, scalability, and suitability of the proposed methods for large-scale and dynamic learning scenarios.

</details>


### [221] [Majority of the Bests: Improving Best-of-N via Bootstrapping](https://arxiv.org/abs/2511.18630)
*Amin Rakhsha,Kanika Madan,Tianyu Zhang,Amir-massoud Farahmand,Amir Khasahmadi*

Main category: cs.LG

TL;DR: 提出Majority-of-the-Bests (MoB)方法，通过自助采样估计BoN的输出分布并选择其众数，在奖励模型不完美时比传统Best-of-N方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 当奖励模型不完美时，Best-of-N方法无法可靠找到正确答案，性能急剧下降。虽然正确答案在输出分布中的概率不高，但通常是众数。

Method: MoB通过自助采样估计BoN的输出分布，然后选择该分布的众数作为最终输出。

Result: 在5个基准测试、3个基础LLM和2个奖励模型的30个设置中，25个设置显示MoB比BoN有持续改进。

Conclusion: MoB是BoN和自一致性方法的简单而强大的替代方案，激励了对更精细选择机制的进一步研究。

Abstract: Sampling multiple outputs from a Large Language Model (LLM) and selecting the most frequent (Self-consistency) or highest-scoring (Best-of-N) candidate is a popular approach to achieve higher accuracy in tasks with discrete final answers. Best-of-N (BoN) selects the output with the highest reward, and with perfect rewards, it often achieves near-perfect accuracy. With imperfect rewards from reward models, however, BoN fails to reliably find the correct answer and its performance degrades drastically. We consider the distribution of BoN's outputs and highlight that, although the correct answer does not usually have a probability close to one under imperfect rewards, it is often the most likely outcome. This suggests that the mode of this distribution can be more reliably correct than a sample from it. Based on this idea, we propose Majority-of-the-Bests (MoB), a novel selection mechanism that estimates the output distribution of BoN via bootstrapping and selects its mode. Experimental results across five benchmarks, three different base LLMs, and two reward models demonstrate consistent improvements over BoN in 25 out of 30 setups. We also provide theoretical results for the consistency of the bootstrapping. MoB serves as a simple, yet strong alternative to BoN and self-consistency, and more broadly, motivates further research in more nuanced selection mechanisms.

</details>


### [222] [FOS: A Large-Scale Temporal Graph Benchmark for Scientific Interdisciplinary Link Prediction](https://arxiv.org/abs/2511.18631)
*Kiyan Rezaee,Morteza Ziabakhsh,Niloofar Nikfarjam,Mohammad M. Ghassemi,Yazdan Rezaee Jouryabi,Sadegh Eskandari,Reza Lashgari*

Main category: cs.LG

TL;DR: FOS是一个基于时间感知图的新基准，用于预测科学前沿领域的形成，通过分析1827-2024年间65,027个研究子领域的共现关系，评估时间图架构在预测跨学科新连接方面的性能。


<details>
  <summary>Details</summary>
Motivation: 跨学科科学突破往往意外出现，预测新研究领域的形成仍是一个重大挑战。需要建立一个全面的基准来评估预测科学前沿的方法。

Method: 构建FOS基准，重建1827-2024年间的年度共现图，节点为研究子领域，边表示两个领域在同一出版物中的共现。节点包含语义嵌入，边具有时间和拓扑描述符。将新领域对连接的预测建模为时间链接预测任务。

Result: 实验表明：(i) 使用领域的长文本描述嵌入显著提高预测准确率；(ii) 不同模型类在不同评估设置下表现优异；(iii) 案例分析显示FOS的顶级链接预测与后续年份实际出现的领域配对一致。

Conclusion: FOS基准为推进科学前沿预测研究提供了可复现的基础，展示了语义嵌入和时间图模型在预测跨学科创新方面的有效性。

Abstract: Interdisciplinary scientific breakthroughs mostly emerge unexpectedly, and forecasting the formation of novel research fields remains a major challenge. We introduce FOS (Future Of Science), a comprehensive time-aware graph-based benchmark that reconstructs annual co-occurrence graphs of 65,027 research sub-fields (spanning 19 general domains) over the period 1827-2024. In these graphs, edges denote the co-occurrence of two fields in a single publication and are timestamped with the corresponding publication year. Nodes are enriched with semantic embeddings, and edges are characterized by temporal and topological descriptors. We formulate the prediction of new field-pair linkages as a temporal link-prediction task, emphasizing the "first-time" connections that signify pioneering interdisciplinary directions. Through extensive experiments, we evaluate a suite of state-of-the-art temporal graph architectures under multiple negative-sampling regimes and show that (i) embedding long-form textual descriptions of fields significantly boosts prediction accuracy, and (ii) distinct model classes excel under different evaluation settings. Case analyses show that top-ranked link predictions on FOS align with field pairings that emerge in subsequent years of academic publications. We publicly release FOS, along with its temporal data splits and evaluation code, to establish a reproducible benchmark for advancing research in predicting scientific frontiers.

</details>


### [223] [The Locally Deployable Virtual Doctor: LLM Based Human Interface for Automated Anamnesis and Database Conversion](https://arxiv.org/abs/2511.18632)
*Jan Benedikt Ruhland,Doguhan Bahcivan,Jan-Peter Sowa,Ali Canbay,Dominik Heider*

Main category: cs.LG

TL;DR: 提出了MedChat，一个本地可部署的虚拟医生框架，结合LLM医疗聊天机器人和扩散驱动头像，用于自动化结构化问诊，确保患者数据隐私和离线运行。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型的最新进展，在临床环境中实现本地部署的AI系统，满足严格的数据保护和患者隐私要求，同时考虑伦理、监管和技术约束。

Method: 使用真实和合成医疗对话混合语料库微调LLM，通过低秩适应优化模型效率；实现安全隔离的数据库接口；基于条件扩散模型在潜在空间实现头像组件，与音频特征同步实现逼真语音和面部动画。

Result: 证明了完全离线、本地可部署的LLM-扩散框架在临床问诊中的可行性；自编码器和扩散网络平滑收敛，MedChat实现稳定微调并对未见数据具有强泛化能力。

Conclusion: 该系统为AI辅助临床问诊提供了一个保护隐私、资源高效的基础，适用于低成本设置。

Abstract: Recent advances in large language models made it possible to achieve high conversational performance with substantially reduced computational demands, enabling practical on-site deployment in clinical environments. Such progress allows for local integration of AI systems that uphold strict data protection and patient privacy requirements, yet their secure implementation in medicine necessitates careful consideration of ethical, regulatory, and technical constraints.
  In this study, we introduce MedChat, a locally deployable virtual physician framework that integrates an LLM-based medical chatbot with a diffusion-driven avatar for automated and structured anamnesis. The chatbot was fine-tuned using a hybrid corpus of real and synthetically generated medical dialogues, while model efficiency was optimized via Low-Rank Adaptation. A secure and isolated database interface was implemented to ensure complete separation between patient data and the inference process. The avatar component was realized through a conditional diffusion model operating in latent space, trained on researcher video datasets and synchronized with mel-frequency audio features for realistic speech and facial animation.
  Unlike existing cloud-based systems, this work demonstrates the feasibility of a fully offline, locally deployable LLM-diffusion framework for clinical anamnesis. The autoencoder and diffusion networks exhibited smooth convergence, and MedChat achieved stable fine-tuning with strong generalization to unseen data. The proposed system thus provides a privacy-preserving, resource-efficient foundation for AI-assisted clinical anamnesis, also in low-cost settings.

</details>


### [224] [Kitty: Accurate and Efficient 2-bit KV Cache Quantization with Dynamic Channel-wise Precision Boost](https://arxiv.org/abs/2511.18643)
*Haojun Xia,Xiaoxia Wu,Jisen Li,Robert Wu,Junxiong Wang,Jue Wang,Chenxi Li,Aman Singhal,Alay Dilipbhai Shah,Alpay Ariyak,Donglin Zhuang,Zhongzhu Zhou,Ben Athiwaratkun,Zhen Zheng,Shuaiwen Leon Song*

Main category: cs.LG

TL;DR: Kitty通过算法-系统协同设计实现混合精度的KV缓存，在保持准确性的同时将KV内存减少近8倍，提升推理吞吐量2.1-4.1倍。


<details>
  <summary>Details</summary>
Motivation: KV缓存是LLM推理的主要内存瓶颈，4位KV量化能保持准确性，但2位量化在长上下文推理中会降低准确性，需要解决这一差距。

Method: 采用动态通道精度提升算法，按敏感度对Key缓存通道排序，仅保留小部分高精度通道；系统层面设计页面中心KV布局、Triton兼容的页面反量化内核和轻量级运行时流水线。

Result: 在七个任务和两个模型系列上，Kitty将KV内存减少近8倍，准确率损失可忽略，在相同内存预算下实现高达8倍的批次大小和2.1-4.1倍的吞吐量提升。

Conclusion: Kitty成功解决了2位KV量化的准确性损失问题，通过混合精度设计实现了内存效率和推理性能的显著提升。

Abstract: The KV cache is a dominant memory bottleneck for LLM inference. While 4-bit KV quantization preserves accuracy, 2-bit often degrades it, especially on long-context reasoning. We close this gap via an algorithm-system co-design for mixed-precision KV caching: Kitty. On the algorithm side, extensive experiments show that Dynamic Channel-wise Precision Boost -- which ranks Key-cache channels by sensitivity and keeps only a small fraction at higher precision -- maintains near-zero loss in accuracy drop while approaching 2-bit memory. The main challenge is handling dynamic 4-bit channel boosts while keeping the page layout coalesced and the dequantization uniform, with no scattered reads or hard-coded masks. Kitty addresses these issues by decompose each mixed-precision Key page into two tensors with unified 2-bit precision. Based on this, Kitty provides a page-centric KV layout, Triton-compatible page dequantization kernels, and a lightweight runtime pipeline that preserves coalescing and avoids divergence. Across seven tasks and two model families (Qwen3, LLaMA3), Kitty cuts KV memory by nearly 8x with negligible accuracy loss, enabling up to 8x larger batches and 2.1x-4.1x higher throughput under the same memory budget. We release the full implementation of Kitty at https://github.com/Summer-Summer/Kitty.

</details>


### [225] [Subtract the Corruption: Training-Data-Free Corrective Machine Unlearning using Task Arithmetic](https://arxiv.org/abs/2511.18660)
*Mostafa Mozafari,Farooq Ahmad Wani,Maria Sofia Bucarelli,Fabrizio Silvestri*

Main category: cs.LG

TL;DR: 本文提出了一种无源纠正性机器遗忘方法CUTS，可在无法访问原始训练数据和污染样本的情况下，通过代理集来消除模型中的污染影响。


<details>
  <summary>Details</summary>
Motivation: 现实场景中训练数据往往不可访问，且污染样本难以识别，现有纠正性机器遗忘方法依赖污染样本集，在无源设置下效果有限。

Method: CUTS将清洁信号和污染信号视为不同任务，通过在代理集上微调放大污染机制，计算权重差异作为代理任务向量，然后减去校准后的向量来消除污染。

Result: 在无源设置下，CUTS在标签噪声下能恢复大部分损失效用，对于后门攻击几乎能完全消除攻击且对效用损伤最小，优于现有专门方法。

Conclusion: CUTS提供了一种轻量级的权重空间校正方法，在无法访问原始训练数据和污染样本的情况下，能有效消除模型中的污染影响。

Abstract: Corrupted training data are ubiquitous. Corrective Machine Unlearning (CMU) seeks to remove the influence of such corruption post-training. Prior CMU typically assumes access to identified corrupted training samples (a ``forget set''). However, in many real-world scenarios the training data are no longer accessible. We formalize \emph{source-free} CMU, where the original training data are unavailable and, consequently, no forget set of identified corrupted training samples can be specified. Instead, we assume a small proxy (surrogate) set of corrupted samples that reflect the suspected corruption type without needing to be the original training samples. In this stricter setting, methods relying on forget set are ineffective or narrow in scope. We introduce \textit{Corrective Unlearning in Task Space} (CUTS), a lightweight weight space correction method guided by the proxy set using task arithmetic principles. CUTS treats the clean and the corruption signal as distinct tasks. Specifically, we briefly fine-tune the corrupted model on the proxy to amplify the corruption mechanism in the weight space, compute the difference between the corrupted and fine-tuned weights as a proxy task vector, and subtract a calibrated multiple of this vector to cancel the corruption. Without access to clean data or a forget set, CUTS recovers a large fraction of the lost utility under label noise and, for backdoor triggers, nearly eliminates the attack with minimal damage to utility, outperforming state-of-the-art specialized CMU methods in source-free setting.

</details>


### [226] [Deterministic Continuous Replacement: Fast and Stable Module Replacement in Pretrained Transformers](https://arxiv.org/abs/2511.18670)
*Rowan Bradbury,Aniket Srinivasan Ashok,Sai Ram Kasanagottu,Gunmay Jhingran,Shuai Meng*

Main category: cs.LG

TL;DR: 提出确定性连续替换(DCR)方法，通过确定性退火权重混合教师和学生输出，解决预训练模型中模块替换的优化稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 替换预训练模型中的模块（特别是将二次自注意力替换为高效注意力）存在严重的优化问题：冷启动重新初始化会破坏冻结骨干网络的稳定性。

Method: 确定性连续替换(DCR)：使用确定性退火权重混合教师和学生输出，理论上消除了随机替换中固有的门控诱导梯度方差。

Result: 在单种子研究中，DCR在受控注意力替换任务上比随机门控和蒸馏基线实现了更快的收敛速度和更强的对齐效果。

Conclusion: DCR为异构算子交换建立了基础，解决了预训练模型中模块替换的核心稳定性挑战。

Abstract: Replacing modules in pretrained models, especially swapping quadratic self-attention for efficient attention alternatives, poses a hard optimization problem: cold-start reinitialization destabilizes frozen backbones. We isolate this core stability challenge in a controlled study. Deterministic Continuous Replacement (DCR) blends teacher and student outputs with a deterministic, annealed weight. Theoretically, DCR eliminates gate-induced gradient variance inherent to stochastic replacement. In a single-seed study, DCR attains faster convergence and stronger alignment than stochastic gating and distillation baselines on controlled attention replacement, establishing a foundation for heterogeneous operator swaps.

</details>


### [227] [Multi-Agent Cross-Entropy Method with Monotonic Nonlinear Critic Decomposition](https://arxiv.org/abs/2511.18671)
*Yan Wang,Ke Deng,Yongli Ren*

Main category: cs.LG

TL;DR: 提出MCEM方法结合非线性评论家分解来解决多智能体强化学习中的集中式-分散式不匹配问题，通过排除次优行为来提升性能


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习中集中训练分散执行(CTDE)范式下的集中式-分散式不匹配(CDM)问题，即一个智能体的次优行为会降低其他智能体的学习效果

Method: 提出多智能体交叉熵方法(MCEM)结合单调非线性评论家分解(NCD)，通过增加高价值联合动作的概率来排除次优行为，并采用改进的k步回报和Retrace进行离策略学习以提高样本效率

Result: 分析和实验表明，MCEM在连续和离散动作基准测试中都优于最先进的方法

Conclusion: MCEM方法成功克服了线性分解表达能力有限和非线性分解需要集中式梯度的权衡问题，有效解决了CDM问题

Abstract: Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution (CTDE), where centralized critics leverage global information to guide decentralized actors. However, centralized-decentralized mismatch (CDM) arises when the suboptimal behavior of one agent degrades others' learning. Prior approaches mitigate CDM through value decomposition, but linear decompositions allow per-agent gradients at the cost of limited expressiveness, while nonlinear decompositions improve representation but require centralized gradients, reintroducing CDM. To overcome this trade-off, we propose the multi-agent cross-entropy method (MCEM), combined with monotonic nonlinear critic decomposition (NCD). MCEM updates policies by increasing the probability of high-value joint actions, thereby excluding suboptimal behaviors. For sample efficiency, we extend off-policy learning with a modified k-step return and Retrace. Analysis and experiments demonstrate that MCEM outperforms state-of-the-art methods across both continuous and discrete action benchmarks.

</details>


### [228] [QuantKAN: A Unified Quantization Framework for Kolmogorov Arnold Networks](https://arxiv.org/abs/2511.18689)
*Kazi Ahmed Asif Fuad,Lizhong Chen*

Main category: cs.LG

TL;DR: 提出了QuantKAN框架，首次系统性地研究Kolmogorov Arnold Networks (KANs)的量化问题，涵盖训练感知量化(QAT)和训练后量化(PTQ)两种模式，在多个数据集和KAN变体上建立了低比特量化基准。


<details>
  <summary>Details</summary>
Motivation: KANs虽然具有强大的表达能力和可解释性，但其异构的样条和基础分支参数阻碍了高效量化，与CNNs和Transformers相比，这方面的研究尚未开展。

Method: QuantKAN框架将现代量化算法（如LSQ、LSQ+、PACT、DoReFa等）扩展到基于样条的层，为基础、样条和激活组件提供分支特定的量化器。

Result: 实验表明KANs与低比特量化兼容，但表现出强烈的方法-架构交互：LSQ、LSQ+和PACT在4比特下对浅层KAN模型保持接近全精度准确率，而DoReFa在深度KAGN的激进低比特设置下表现最稳定。PTQ中GPTQ和Uniform在所有数据集上表现最强。

Conclusion: QuantKAN框架统一了样条学习和量化，为在资源受限环境中高效部署KANs提供了实用工具和指导方针。

Abstract: Kolmogorov Arnold Networks (KANs) represent a new class of neural architectures that replace conventional linear transformations and node-based nonlinearities with spline-based function approximations distributed along network edges. Although KANs offer strong expressivity and interpretability, their heterogeneous spline and base branch parameters hinder efficient quantization, which remains unexamined compared to CNNs and Transformers. In this paper, we present QuantKAN, a unified framework for quantizing KANs across both quantization aware training (QAT) and post-training quantization (PTQ) regimes. QuantKAN extends modern quantization algorithms, such as LSQ, LSQ+, PACT, DoReFa, QIL, GPTQ, BRECQ, AdaRound, AWQ, and HAWQ-V2, to spline based layers with branch-specific quantizers for base, spline, and activation components. Through extensive experiments on MNIST, CIFAR 10, and CIFAR 100 across multiple KAN variants (EfficientKAN, FastKAN, PyKAN, and KAGN), we establish the first systematic benchmarks for low-bit spline networks. Our results show that KANs, particularly deeper KAGN variants, are compatible with low-bit quantization but exhibit strong method architecture interactions: LSQ, LSQ+, and PACT preserve near full precision accuracy at 4 bit for shallow KAN MLP and ConvNet models, while DoReFa provides the most stable behavior for deeper KAGN under aggressive low-bit settings. For PTQ, GPTQ and Uniform consistently deliver the strongest overall performance across datasets, with BRECQ highly competitive on simpler regimes such as MNIST. Our proposed QuantKAN framework thus unifies spline learning and quantization, and provides practical tools and guidelines for efficiently deploying KANs in real-world, resource-constrained environments.

</details>


### [229] [VLM in a flash: I/O-Efficient Sparsification of Vision-Language Model via Neuron Chunking](https://arxiv.org/abs/2511.18692)
*Kichang Yang,Seonjun Kim,Minjae Kim,Nairan Zhang,Chi Zhang,Youngki Lee*

Main category: cs.LG

TL;DR: 提出了一种名为Neuron Chunking的I/O高效稀疏化策略，通过将神经元重要性与其存储访问成本相结合，显著提升了边缘设备上大型视觉语言模型的闪存权重卸载性能。


<details>
  <summary>Details</summary>
Motivation: 传统的稀疏化方法仅基于激活幅度选择神经元，忽视了访问模式对闪存性能的影响，导致I/O效率低下。

Method: Neuron Chunking在内存中的连续神经元组（块）上操作，通过轻量级访问连续性抽象建模I/O延迟，选择具有高效用（神经元重要性除以估计延迟）的块。

Result: 在Jetson Orin Nano和Jetson AGX Orin上分别实现了4.65倍和5.76倍的I/O效率提升。

Conclusion: 通过将稀疏化决策与底层存储行为对齐，Neuron Chunking能够显著改善边缘设备上大型视觉语言模型的I/O效率。

Abstract: Edge deployment of large Vision-Language Models (VLMs) increasingly relies on flash-based weight offloading, where activation sparsification is used to reduce I/O overhead. However, conventional sparsification remains model-centric, selecting neurons solely by activation magnitude and neglecting how access patterns influence flash performance. We present Neuron Chunking, an I/O-efficient sparsification strategy that operates on chunks (i.e., groups of contiguous neurons in memory) and couples neuron importance with storage access cost. The method models I/O latency through a lightweight abstraction of access contiguity and selects chunks with high utility, defined as neuron importance normalized by estimated latency. By aligning sparsification decisions with the underlying storage behavior, Neuron Chunking improves I/O efficiency by up to 4.65x and 5.76x on Jetson Orin Nano and Jetson AGX Orin, respectively.

</details>


### [230] [GRIT-LP: Graph Transformer with Long-Range Skip Connection and Partitioned Spatial Graphs for Accurate Ice Layer Thickness Prediction](https://arxiv.org/abs/2511.18716)
*Zesheng Liu,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: GRIT-LP是一种用于极地雷达图像冰层厚度估计的图变换器，通过分区空间图构建策略和长程跳跃连接机制，解决了深度图变换器的过平滑和长程依赖建模问题，在RMSE上比现有方法提升了24.92%。


<details>
  <summary>Details</summary>
Motivation: 准确估计冰层厚度对于理解积雪积累、重建过去气候模式以及减少未来冰盖演化和海平面上升预测的不确定性至关重要。现有图变换器在深度上受到过平滑和弱长程依赖建模的限制。

Method: 结合归纳几何图学习和自注意力机制，采用分区空间图构建策略形成重叠的完全连接局部邻域以保持空间相干性，并在变换器中引入长程跳跃连接机制改善信息流。

Result: 在根均方误差上比当前最先进方法提升了24.92%，证明了图变换器在建模时空模式方面的有效性。

Conclusion: 图变换器通过捕捉局部结构特征和跨内部冰层的长程依赖，能够有效建模时空模式，有潜力推进对冰冻圈过程的数据驱动理解。

Abstract: Graph transformers have demonstrated remarkable capability on complex spatio-temporal tasks, yet their depth is often limited by oversmoothing and weak long-range dependency modeling. To address these challenges, we introduce GRIT-LP, a graph transformer explicitly designed for polar ice-layer thickness estimation from polar radar imagery. Accurately estimating ice layer thickness is critical for understanding snow accumulation, reconstructing past climate patterns and reducing uncertainties in projections of future ice sheet evolution and sea level rise. GRIT-LP combines an inductive geometric graph learning framework with self-attention mechanism, and introduces two major innovations that jointly address challenges in modeling the spatio-temporal patterns of ice layers: a partitioned spatial graph construction strategy that forms overlapping, fully connected local neighborhoods to preserve spatial coherence and suppress noise from irrelevant long-range links, and a long-range skip connection mechanism within the transformer that improves information flow and mitigates oversmoothing in deeper attention layers. We conducted extensive experiments, demonstrating that GRIT-LP outperforms current state-of-the-art methods with a 24.92\% improvement in root mean squared error. These results highlight the effectiveness of graph transformers in modeling spatiotemporal patterns by capturing both localized structural features and long-range dependencies across internal ice layers, and demonstrate their potential to advance data-driven understanding of cryospheric processes.

</details>


### [231] [Towards Realistic Guarantees: A Probabilistic Certificate for SmoothLLM](https://arxiv.org/abs/2511.18721)
*Adarsh Kumarappan,Ayushi Mehrotra*

Main category: cs.LG

TL;DR: 提出了(k, ε)-unstable概率框架来改进SmoothLLM防御，提供更可信的安全认证，对抗各种越狱攻击。


<details>
  <summary>Details</summary>
Motivation: SmoothLLM防御的k-unstable假设在实践中很少成立，限制了安全证书的可信度。

Method: 引入(k, ε)-unstable概率框架，结合攻击成功的经验模型，推导SmoothLLM防御概率的新下界。

Result: 提供了更可信和实用的安全证书，使从业者能够设置更符合LLM实际行为的认证阈值。

Conclusion: 这项工作为安全AI部署贡献了一个实用且理论基础的机制，使LLM更能抵抗对其安全对齐的利用。

Abstract: The SmoothLLM defense provides a certification guarantee against jailbreaking attacks, but it relies on a strict `k-unstable' assumption that rarely holds in practice. This strong assumption can limit the trustworthiness of the provided safety certificate. In this work, we address this limitation by introducing a more realistic probabilistic framework, `(k, $\varepsilon$)-unstable,' to certify defenses against diverse jailbreaking attacks, from gradient-based (GCG) to semantic (PAIR). We derive a new, data-informed lower bound on SmoothLLM's defense probability by incorporating empirical models of attack success, providing a more trustworthy and practical safety certificate. By introducing the notion of (k, $\varepsilon$)-unstable, our framework provides practitioners with actionable safety guarantees, enabling them to set certification thresholds that better reflect the real-world behavior of LLMs. Ultimately, this work contributes a practical and theoretically-grounded mechanism to make LLMs more resistant to the exploitation of their safety alignments, a critical challenge in secure AI deployment.

</details>


### [232] [LogSyn: A Few-Shot LLM Framework for Structured Insight Extraction from Unstructured General Aviation Maintenance Logs](https://arxiv.org/abs/2511.18727)
*Devansh Agarwal,Maitreyi Chatterjee,Biplab Chatterjee*

Main category: cs.LG

TL;DR: LogSyn框架使用大语言模型将非结构化的飞机维护日志转换为结构化数据，通过少量样本学习实现问题解决叙述的抽象生成和事件分类，为航空维护提供可扩展的语义结构化方法。


<details>
  <summary>Details</summary>
Motivation: 飞机维护日志包含宝贵的安全数据，但由于其非结构化文本格式而未被充分利用。

Method: 使用大语言模型和少量样本上下文学习，在6,169条记录上执行受控抽象生成，总结问题解决叙述并在详细层次本体中分类事件。

Result: 框架能够识别关键故障模式，为维护日志提供可扩展的语义结构化和可操作见解提取方法。

Conclusion: 这项工作为改进航空及相关行业的维护工作流程和预测分析提供了实用路径。

Abstract: Aircraft maintenance logs hold valuable safety data but remain underused due to their unstructured text format. This paper introduces LogSyn, a framework that uses Large Language Models (LLMs) to convert these logs into structured, machine-readable data. Using few-shot in-context learning on 6,169 records, LogSyn performs Controlled Abstraction Generation (CAG) to summarize problem-resolution narratives and classify events within a detailed hierarchical ontology. The framework identifies key failure patterns, offering a scalable method for semantic structuring and actionable insight extraction from maintenance logs. This work provides a practical path to improve maintenance workflows and predictive analytics in aviation and related industries.

</details>


### [233] [Reinforcement Learning for Self-Healing Material Systems](https://arxiv.org/abs/2511.18728)
*Maitreyi Chatterjee,Devansh Agarwal,Biplab Chatterjee*

Main category: cs.LG

TL;DR: 将自愈过程建模为强化学习问题，通过MDP框架让智能体自主制定最优策略，平衡结构完整性和资源消耗。


<details>
  <summary>Details</summary>
Motivation: 自主材料系统需要自适应控制方法来最大化结构寿命，传统启发式方法在动态环境中效率有限。

Method: 在随机模拟环境中比较离散动作（Q-learning、DQN）和连续动作（TD3）强化学习智能体，评估其自愈控制性能。

Result: RL控制器显著优于启发式基线，实现近乎完全的材料恢复；TD3智能体在连续剂量控制方面表现出更快的收敛速度和稳定性。

Conclusion: 连续、精细的比例控制在动态自愈应用中至关重要，TD3方法在收敛速度和稳定性方面具有优势。

Abstract: The transition to autonomous material systems necessitates adaptive control methodologies to maximize structural longevity. This study frames the self-healing process as a Reinforcement Learning (RL) problem within a Markov Decision Process (MDP), enabling agents to autonomously derive optimal policies that efficiently balance structural integrity maintenance against finite resource consumption. A comparative evaluation of discrete-action (Q-learning, DQN) and continuous-action (TD3) agents in a stochastic simulation environment revealed that RL controllers significantly outperform heuristic baselines, achieving near-complete material recovery. Crucially, the TD3 agent utilizing continuous dosage control demonstrated superior convergence speed and stability, underscoring the necessity of fine-grained, proportional actuation in dynamic self-healing applications.

</details>


### [234] [Large-Scale In-Game Outcome Forecasting for Match, Team and Players in Football using an Axial Transformer Neural Network](https://arxiv.org/abs/2511.18730)
*Michael Horton,Patrick Lucey*

Main category: cs.LG

TL;DR: 提出基于轴向变换器的神经网络，用于在足球比赛中实时预测13种球员动作的总数，涵盖个人、团队和比赛三个层面。


<details>
  <summary>Details</summary>
Motivation: 准确预测足球比赛中球员动作总数对于战术决策、体育博彩和电视转播分析具有重要意义，需要考虑比赛状态、球员能力、互动关系和比赛动态。

Method: 使用轴向变换器神经网络，能够联合且循环地预测比赛中多个时间点的13种个体动作总数，该设计等效于常规序列变换器但效率更高。

Result: 模型能够做出一致可靠的预测，每场比赛以低延迟实时生成约75,000个预测。

Conclusion: 提出的轴向变换器设计在实验中表现良好，能够有效捕捉比赛进展中的时间动态和球员间的互动关系。

Abstract: Football (soccer) is a sport that is characterised by complex game play, where players perform a variety of actions, such as passes, shots, tackles, fouls, in order to score goals, and ultimately win matches. Accurately forecasting the total number of each action that each player will complete during a match is desirable for a variety of applications, including tactical decision-making, sports betting, and for television broadcast commentary and analysis. Such predictions must consider the game state, the ability and skill of the players in both teams, the interactions between the players, and the temporal dynamics of the game as it develops. In this paper, we present a transformer-based neural network that jointly and recurrently predicts the expected totals for thirteen individual actions at multiple time-steps during the match, and where predictions are made for each individual player, each team and at the game-level. The neural network is based on an \emph{axial transformer} that efficiently captures the temporal dynamics as the game progresses, and the interactions between the players at each time-step. We present a novel axial transformer design that we show is equivalent to a regular sequential transformer, and the design performs well experimentally. We show empirically that the model can make consistent and reliable predictions, and efficiently makes $\sim$75,000 live predictions at low latency for each game.

</details>


### [235] [OceanForecastBench: A Benchmark Dataset for Data-Driven Global Ocean Forecasting](https://arxiv.org/abs/2511.18732)
*Haoming Jia,Yi Han,Xiang Wang,Huizan Wang,Wei Wu,Jianming Zheng,Peikun Xiao*

Main category: cs.LG

TL;DR: 提出了OceanForecastBench，一个用于数据驱动海洋预测的开源基准框架，包含28年高质量再分析数据、可靠观测数据和评估流程。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏开源标准化的海洋预测基准，导致数据使用和评估方法不一致，阻碍模型开发、性能比较和跨学科合作。

Method: 构建包含三部分核心贡献的基准：28年全球海洋再分析数据、基于卫星和现场观测的评估数据、包含6个基线模型的评估流程。

Result: 创建了目前最全面的数据驱动海洋预测基准框架，为模型开发、评估和比较提供开源平台。

Conclusion: OceanForecastBench解决了海洋预测领域缺乏标准化基准的问题，将促进该领域的模型发展和公平比较。

Abstract: Global ocean forecasting aims to predict key ocean variables such as temperature, salinity, and currents, which is essential for understanding and describing oceanic phenomena. In recent years, data-driven deep learning-based ocean forecast models, such as XiHe, WenHai, LangYa and AI-GOMS, have demonstrated significant potential in capturing complex ocean dynamics and improving forecasting efficiency. Despite these advancements, the absence of open-source, standardized benchmarks has led to inconsistent data usage and evaluation methods. This gap hinders efficient model development, impedes fair performance comparison, and constrains interdisciplinary collaboration. To address this challenge, we propose OceanForecastBench, a benchmark offering three core contributions: (1) A high-quality global ocean reanalysis data over 28 years for model training, including 4 ocean variables across 23 depth levels and 4 sea surface variables. (2) A high-reliability satellite and in-situ observations for model evaluation, covering approximately 100 million locations in the global ocean. (3) An evaluation pipeline and a comprehensive benchmark with 6 typical baseline models, leveraging observations to evaluate model performance from multiple perspectives. OceanForecastBench represents the most comprehensive benchmarking framework currently available for data-driven ocean forecasting, offering an open-source platform for model development, evaluation, and comparison. The dataset and code are publicly available at: https://github.com/Ocean-Intelligent-Forecasting/OceanForecastBench.

</details>


### [236] [Sampling Control for Imbalanced Calibration in Semi-Supervised Learning](https://arxiv.org/abs/2511.18773)
*Senmao Tian,Xiang Wei,Shunli Zhang*

Main category: cs.LG

TL;DR: SC-SSL是一个解决半监督学习中类别不平衡问题的统一框架，通过解耦采样控制来抑制模型偏见，在训练和推理阶段分别处理特征级和权重不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有的半监督学习方法在处理类别不平衡时通常以粗粒度方式调整logits，混淆了数据不平衡与不同类别学习难度差异导致的偏见问题，需要更精细的解决方案。

Method: 提出SC-SSL框架：训练阶段通过具有显式扩展能力的分类器和自适应调整采样概率来缓解少数类的特征级不平衡；推理阶段分析线性分类器的权重不平衡，应用后验采样控制通过优化偏置向量直接校准logits。

Result: 在多个基准数据集和不同分布设置下的广泛实验验证了SC-SSL的一致性和最先进性能。

Conclusion: SC-SSL通过解耦采样控制有效解决了半监督学习中的类别不平衡问题，在训练和推理阶段分别处理不同层面的不平衡，取得了优异的性能表现。

Abstract: Class imbalance remains a critical challenge in semi-supervised learning (SSL), especially when distributional mismatches between labeled and unlabeled data lead to biased classification. Although existing methods address this issue by adjusting logits based on the estimated class distribution of unlabeled data, they often handle model imbalance in a coarse-grained manner, conflating data imbalance with bias arising from varying class-specific learning difficulties. To address this issue, we propose a unified framework, SC-SSL, which suppresses model bias through decoupled sampling control. During training, we identify the key variables for sampling control under ideal conditions. By introducing a classifier with explicit expansion capability and adaptively adjusting sampling probabilities across different data distributions, SC-SSL mitigates feature-level imbalance for minority classes. In the inference phase, we further analyze the weight imbalance of the linear classifier and apply post-hoc sampling control with an optimization bias vector to directly calibrate the logits. Extensive experiments across various benchmark datasets and distribution settings validate the consistency and state-of-the-art performance of SC-SSL.

</details>


### [237] [SAOT: An Enhanced Locality-Aware Spectral Transformer for Solving PDEs](https://arxiv.org/abs/2511.18777)
*Chenhong Zhou,Jie Chen,Zaifeng Yang*

Main category: cs.LG

TL;DR: 提出了一种结合小波变换和Transformer的混合谱注意力算子Transformer(SAOT)，通过小波注意力模块和傅里叶注意力模块的融合，解决了传统傅里叶神经算子在局部细节和高频分量捕捉上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统傅里叶神经算子(FNO)在求解偏微分方程时存在过度平滑解、无法捕捉局部细节和高频分量的问题，需要改进以提升性能。

Method: 提出小波注意力(WA)模块，利用小波变换的空间-频率局部化特性；开发SAOT框架，通过门控融合块整合WA的局部关注和傅里叶注意力(FA)的全局感受野。

Result: WA显著缓解了FA的局限性，大幅超越现有基于小波的神经算子；SAOT在六个算子学习基准上达到最先进性能，并展现出强大的离散化不变能力。

Conclusion: 通过整合局部感知和全局谱表示，SAOT成功解决了传统神经算子的局限性，为偏微分方程求解提供了更有效的解决方案。

Abstract: Neural operators have shown great potential in solving a family of Partial Differential Equations (PDEs) by modeling the mappings between input and output functions. Fourier Neural Operator (FNO) implements global convolutions via parameterizing the integral operators in Fourier space. However, it often results in over-smoothing solutions and fails to capture local details and high-frequency components. To address these limitations, we investigate incorporating the spatial-frequency localization property of Wavelet transforms into the Transformer architecture. We propose a novel Wavelet Attention (WA) module with linear computational complexity to efficiently learn locality-aware features. Building upon WA, we further develop the Spectral Attention Operator Transformer (SAOT), a hybrid spectral Transformer framework that integrates WA's localized focus with the global receptive field of Fourier-based Attention (FA) through a gated fusion block. Experimental results demonstrate that WA significantly mitigates the limitations of FA and outperforms existing Wavelet-based neural operators by a large margin. By integrating the locality-aware and global spectral representations, SAOT achieves state-of-the-art performance on six operator learning benchmarks and exhibits strong discretization-invariant ability.

</details>


### [238] [Hypergraph Contrastive Learning for both Homophilic and Heterophilic Hypergraphs](https://arxiv.org/abs/2511.18783)
*Renchu Guan,Xuyang Li,Yachao Zhang,Wei Pang,Fausto Giunchiglia,Ximing Li,Yonghao Liu,Xiaoyue Feng*

Main category: cs.LG

TL;DR: HONOR是一个新颖的无监督超图对比学习框架，专门设计用于处理同配性和异配性超图，通过提示机制和自适应注意力聚合来建模异配关系。


<details>
  <summary>Details</summary>
Motivation: 现有的超图神经网络大多基于同配性假设，但现实世界中的超图往往表现出显著的异配性结构，这限制了现有方法的性能。

Method: 提出提示机制的超边特征构建策略和自适应注意力聚合模块，结合高通滤波来充分挖掘异配连接模式。

Result: 理论分析证明HONOR具有优越的泛化能力和鲁棒性，实验验证其在同配和异配数据集上均优于现有最优基线方法。

Conclusion: HONOR框架能够有效处理超图中的异配关系，为复杂高阶关系的建模提供了新的解决方案。

Abstract: Hypergraphs, as a generalization of traditional graphs, naturally capture high-order relationships. In recent years, hypergraph neural networks (HNNs) have been widely used to capture complex high-order relationships. However, most existing hypergraph neural network methods inherently rely on the homophily assumption, which often does not hold in real-world scenarios that exhibit significant heterophilic structures. To address this limitation, we propose \textbf{HONOR}, a novel unsupervised \textbf{H}ypergraph c\textbf{ON}trastive learning framework suitable for both hom\textbf{O}philic and hete\textbf{R}ophilic hypergraphs. Specifically, HONOR explicitly models the heterophilic relationships between hyperedges and nodes through two complementary mechanisms: a prompt-based hyperedge feature construction strategy that maintains global semantic consistency while suppressing local noise, and an adaptive attention aggregation module that dynamically captures the diverse local contributions of nodes to hyperedges. Combined with high-pass filtering, these designs enable HONOR to fully exploit heterophilic connection patterns, yielding more discriminative and robust node and hyperedge representations. Theoretically, we demonstrate the superior generalization ability and robustness of HONOR. Empirically, extensive experiments further validate that HONOR consistently outperforms state-of-the-art baselines under both homophilic and heterophilic datasets.

</details>


### [239] [Doubly Wild Refitting: Model-Free Evaluation of High Dimensional Black-Box Predictions under Convex Losses](https://arxiv.org/abs/2511.18789)
*Haichen Hu,David Simchi-Levi*

Main category: cs.LG

TL;DR: 提出了一种高效的重拟合程序，用于在固定设计设置下计算经验风险最小化(ERM)的过剩风险并提供高概率上界，无需了解函数类的复杂性。


<details>
  <summary>Details</summary>
Motivation: 传统基于容量的学习理论对于现代不透明机器学习系统（如深度神经网络和生成模型）变得不可行，因为这些假设类的复杂性极高。

Method: 通过随机扰动梯度向量生成两组伪标签数据（wild response），然后对黑盒过程进行两次重拟合得到两个wild预测器，最后结合原始预测器和构造的wild响应来推导过剩风险上界。

Result: 开发了一种模型无关的过剩风险评估方法，能够为复杂机器学习系统提供高效的风险上界估计。

Conclusion: 该方法具有模型无关特性，对评估现代不透明机器学习系统的理论性能具有重要前景。

Abstract: We study the problem of excess risk evaluation for empirical risk minimization (ERM) under general convex loss functions. Our contribution is an efficient refitting procedure that computes the excess risk and provides high-probability upper bounds under the fixed-design setting. Assuming only black-box access to the training algorithm and a single dataset, we begin by generating two sets of artificially modified pseudo-outcomes termed wild response, created by stochastically perturbing the gradient vectors with carefully chosen scaling. Using these two pseudo-labeled datasets, we then refit the black-box procedure twice to obtain two corresponding wild predictors. Finally, leveraging the original predictor, the two wild predictors, and the constructed wild responses, we derive an efficient excess risk upper bound. A key feature of our analysis is that it requires no prior knowledge of the complexity of the underlying function class. As a result, the method is essentially model-free and holds significant promise for theoretically evaluating modern opaque machine learning system--such as deep nerral networks and generative model--where traditional capacity-based learning theory becomes infeasible due to the extreme complexity of the hypothesis class.

</details>


### [240] [Towards Characterizing Knowledge Distillation of PPG Heart Rate Estimation Models](https://arxiv.org/abs/2511.18829)
*Kanav Arora,Girish Narayanswamy,Shwetak Patel,Richard Li*

Main category: cs.LG

TL;DR: 本文研究了如何通过知识蒸馏将大型预训练PPG模型压缩为适合边缘设备实时推理的小型模型，评估了四种蒸馏策略并建立了模型大小与性能之间的缩放规律。


<details>
  <summary>Details</summary>
Motivation: 虽然深度学习模型在心率估计任务中表现良好，但为了在可穿戴设备上部署，这些模型必须满足严格的内存和延迟限制。

Method: 评估了四种蒸馏策略：硬蒸馏、软蒸馏、解耦知识蒸馏(DKD)和特征蒸馏，通过全面的教师和学生模型容量扫描来表征缩放规律。

Result: 建立了描述模型大小与性能关系的缩放规律，为构建可部署在边缘设备上的生理传感模型提供了实用且可预测的方法。

Conclusion: 这项早期研究为构建边缘可部署的生理传感模型奠定了实践基础，提供了可预测的模型压缩方法。

Abstract: Heart rate estimation from photoplethysmography (PPG) signals generated by wearable devices such as smartwatches and fitness trackers has significant implications for the health and well-being of individuals. Although prior work has demonstrated deep learning models with strong performance in the heart rate estimation task, in order to deploy these models on wearable devices, these models must also adhere to strict memory and latency constraints. In this work, we explore and characterize how large pre-trained PPG models may be distilled to smaller models appropriate for real-time inference on the edge. We evaluate four distillation strategies through comprehensive sweeps of teacher and student model capacities: (1) hard distillation, (2) soft distillation, (3) decoupled knowledge distillation (DKD), and (4) feature distillation. We present a characterization of the resulting scaling laws describing the relationship between model size and performance. This early investigation lays the groundwork for practical and predictable methods for building edge-deployable models for physiological sensing.

</details>


### [241] [Leveraging Duration Pseudo-Embeddings in Multilevel LSTM and GCN Hypermodels for Outcome-Oriented PPM](https://arxiv.org/abs/2511.18830)
*Fang Wang,Paolo Ceravolo,Ernesto Damiani*

Main category: cs.LG

TL;DR: 提出了一种双输入神经网络策略，通过分离事件和序列属性，使用持续时间感知的伪嵌入矩阵将时间重要性转换为紧凑可学习的表示，解决了预测过程监控中时间不规则性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习模型在处理预测过程监控时难以应对时间不规则性，特别是随机事件持续时间和重叠时间戳，限制了它们在异构数据集上的适应性。

Method: 采用双输入神经网络策略，分离事件和序列属性，使用持续时间感知伪嵌入矩阵。在B-LSTM和B-GCN两个基线家族基础上开发了持续时间感知变体D-LSTM和D-GCN，所有模型都包含自调谐超模型用于自适应架构选择。

Result: 在平衡和不平衡结果预测任务上的实验表明，持续时间伪嵌入输入持续改善泛化能力，减少模型复杂性，并增强可解释性。

Conclusion: 显式时间编码具有明显优势，为稳健的实时预测过程监控应用提供了灵活的设计方案。

Abstract: Existing deep learning models for Predictive Process Monitoring (PPM) struggle with temporal irregularities, particularly stochastic event durations and overlapping timestamps, limiting their adaptability across heterogeneous datasets. We propose a dual input neural network strategy that separates event and sequence attributes, using a duration-aware pseudo-embedding matrix to transform temporal importance into compact, learnable representations. This design is implemented across two baseline families: B-LSTM and B-GCN, and their duration-aware variants D-LSTM and D-GCN. All models incorporate self-tuned hypermodels for adaptive architecture selection. Experiments on balanced and imbalanced outcome prediction tasks show that duration pseudo-embedding inputs consistently improve generalization, reduce model complexity, and enhance interpretability. Our results demonstrate the benefits of explicit temporal encoding and provide a flexible design for robust, real-world PPM applications.

</details>


### [242] [Auto-ML Graph Neural Network Hypermodels for Outcome Prediction in Event-Sequence Data](https://arxiv.org/abs/2511.18835)
*Fang Wang,Lance Kosca,Adrienne Kosca,Marko Gacesa,Ernesto Damiani*

Main category: cs.LG

TL;DR: HGNN(O)是一个用于事件序列数据结果预测的AutoML GNN超模型框架，通过贝叶斯优化自动调整架构和超参数，在多个数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 为复杂事件序列数据的结果预测提供一个鲁棒且可泛化的AutoML-GNN基准方法，避免手动配置架构和超参数。

Method: 扩展了四种架构（单层、双层、双层伪嵌入、双层嵌入）和六种GNN算子，采用基于贝叶斯优化的自调优机制，包含剪枝和早停策略。

Result: 在Traffic Fines数据集上准确率超过0.98，在Patients数据集上加权F1分数达到0.86，且无需显式处理数据不平衡问题。

Conclusion: 提出的AutoML-GNN方法为复杂事件序列数据的结果预测提供了鲁棒且可泛化的基准解决方案。

Abstract: This paper introduces HGNN(O), an AutoML GNN hypermodel framework for outcome prediction on event-sequence data. Building on our earlier work on graph convolutional network hypermodels, HGNN(O) extends four architectures-One Level, Two Level, Two Level Pseudo Embedding, and Two Level Embedding-across six canonical GNN operators. A self-tuning mechanism based on Bayesian optimization with pruning and early stopping enables efficient adaptation over architectures and hyperparameters without manual configuration. Empirical evaluation on both balanced and imbalanced event logs shows that HGNN(O) achieves accuracy exceeding 0.98 on the Traffic Fines dataset and weighted F1 scores up to 0.86 on the Patients dataset without explicit imbalance handling. These results demonstrate that the proposed AutoML-GNN approach provides a robust and generalizable benchmark for outcome prediction in complex event-sequence data.

</details>


### [243] [Federated style aware transformer aggregation of representations](https://arxiv.org/abs/2511.18841)
*Mincheol Jeon,Euinam Huh*

Main category: cs.LG

TL;DR: FedSTAR是一个风格感知的联邦学习框架，通过解耦客户端特定风格因子和共享内容表示来解决个性化联邦学习中的领域异构、数据不平衡和通信约束问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习缺乏个性化，单一全局模型无法捕捉客户端特定特征，导致对有高度不同数据分布的客户端产生偏差预测和泛化能力差。

Method: 使用Transformer注意力机制聚合类原型，解耦客户端特定风格因子和共享内容表示，通过交换紧凑原型和风格向量而非完整模型参数来减少通信开销。

Result: 实验结果表明，内容-风格解耦与注意力驱动原型聚合相结合，在不增加通信成本的情况下提高了异构环境中的个性化和鲁棒性。

Conclusion: FedSTAR通过风格感知的联邦学习方法有效解决了个性化联邦学习的关键挑战，实现了更好的个性化和通信效率。

Abstract: Personalized Federated Learning (PFL) faces persistent challenges, including domain heterogeneity from diverse client data, data imbalance due to skewed participation, and strict communication constraints. Traditional federated learning often lacks personalization, as a single global model cannot capture client-specific characteristics, leading to biased predictions and poor generalization, especially for clients with highly divergent data distributions.
  To address these issues, we propose FedSTAR, a style-aware federated learning framework that disentangles client-specific style factors from shared content representations. FedSTAR aggregates class-wise prototypes using a Transformer-based attention mechanism, allowing the server to adaptively weight client contributions while preserving personalization.
  Furthermore, by exchanging compact prototypes and style vectors instead of full model parameters, FedSTAR significantly reduces communication overhead. Experimental results demonstrate that combining content-style disentanglement with attention-driven prototype aggregation improves personalization and robustness in heterogeneous environments without increasing communication cost.

</details>


### [244] [WaveTuner: Comprehensive Wavelet Subband Tuning for Time Series Forecasting](https://arxiv.org/abs/2511.18846)
*Yubo Wang,Hui He,Chaoxi Niu,Zhendong Niu*

Main category: cs.LG

TL;DR: WaveTuner是一个基于小波分解的时间序列预测框架，通过全频谱子带调谐解决现有方法对高频分量利用不足的问题，在多个真实数据集上实现了最先进的预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有小波方法存在持续偏向递归分解低频分量的偏差，严重未充分利用对精确预测至关重要的细微但信息丰富的高频分量。

Method: WaveTuner包含两个关键模块：自适应小波细化模块（将时间序列转换为时频系数，动态分配子带权重并生成特定子带嵌入）和多分支专业化模块（使用多个功能分支，每个分支实例化为具有不同功能阶数的KAN网络来建模特定频谱子带）。

Result: 在八个真实世界数据集上的广泛实验表明，WaveTuner在时间序列预测中实现了最先进的预测性能。

Conclusion: WaveTuner在一个统一的时频框架内全面调谐全局趋势和局部变化，有效解决了小波方法对高频分量利用不足的问题。

Abstract: Due to the inherent complexity, temporal patterns in real-world time series often evolve across multiple intertwined scales, including long-term periodicity, short-term fluctuations, and abrupt regime shifts. While existing literature has designed many sophisticated decomposition approaches based on the time or frequency domain to partition trend-seasonality components and high-low frequency components, an alternative line of approaches based on the wavelet domain has been proposed to provide a unified multi-resolution representation with precise time-frequency localization. However, most wavelet-based methods suffer from a persistent bias toward recursively decomposing only low-frequency components, severely underutilizing subtle yet informative high-frequency components that are pivotal for precise time series forecasting. To address this problem, we propose WaveTuner, a Wavelet decomposition framework empowered by full-spectrum subband Tuning for time series forecasting. Concretely, WaveTuner comprises two key modules: (i) Adaptive Wavelet Refinement module, that transforms time series into time-frequency coefficients, utilizes an adaptive router to dynamically assign subband weights, and generates subband-specific embeddings to support refinement; and (ii) Multi-Branch Specialization module, that employs multiple functional branches, each instantiated as a flexible Kolmogorov-Arnold Network (KAN) with a distinct functional order to model a specific spectral subband. Equipped with these modules, WaveTuner comprehensively tunes global trends and local variations within a unified time-frequency framework. Extensive experiments on eight real-world datasets demonstrate WaveTuner achieves state-of-the-art forecasting performance in time series forecasting.

</details>


### [245] [Robust and Generalizable GNN Fine-Tuning via Uncertainty-aware Adapter Learning](https://arxiv.org/abs/2511.18859)
*Bo Jiang,Weijun Zhao,Beibei Wang,Xiao Wang,Jin Tang*

Main category: cs.LG

TL;DR: 提出UAdapterGNN方法，通过集成不确定性学习到GNN适配器中，增强预训练GNN模型在微调过程中对噪声图数据的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的AdapterGNN方法容易受到图数据中各种噪声（如噪声边和模糊节点属性）的影响，表现出有限的泛化能力。如何增强GNN微调的鲁棒性和泛化能力是一个开放性问题。

Method: 使用高斯概率适配器来增强预训练GNN模型，当图包含各种噪声时，该方法能自动吸收高斯分布方差变化的影响，从而显著增强模型鲁棒性。

Result: 在多个基准测试上的大量实验证明了所提出的UAdapterGNN方法的有效性、鲁棒性和高泛化能力。

Conclusion: 通过将不确定性学习集成到GNN适配器中，可以很好地解决图噪声问题，显著增强预训练GNN模型在微调过程中的鲁棒性和泛化能力。

Abstract: Recently, fine-tuning large-scale pre-trained GNNs has yielded remarkable attention in adapting pre-trained GNN models for downstream graph learning tasks. One representative fine-tuning method is to exploit adapter (termed AdapterGNN) which aims to 'augment' the pre-trained model by inserting a lightweight module to make the 'augmented' model better adapt to the downstream tasks. However, graph data may contain various types of noise in downstream tasks, such as noisy edges and ambiguous node attributes. Existing AdapterGNNs are often prone to graph noise and exhibit limited generalizability. How to enhance the robustness and generalization ability of GNNs' fine tuning remains an open problem. In this paper, we show that the above problem can be well addressed by integrating uncertainty learning into the GNN adapter. We propose the Uncertainty-aware Adapter (UAdapterGNN) that fortifies pre-trained GNN models against noisy graph data in the fine-tuning process. Specifically, in contrast to regular AdapterGNN, our UAdapterGNN exploits Gaussian probabilistic adapter to augment the pre-trained GNN model. In this way, when the graph contains various noises,our method can automatically absorb the effects of changes in the variances of the Gaussian distribution, thereby significantly enhancing the model's robustness. Also, UAdapterGNN can further improve the generalization ability of the model on the downstream tasks. Extensive experiments on several benchmarks demonstrate the effectiveness, robustness and high generalization ability of the proposed UAdapterGNN method.

</details>


### [246] [KernelBand: Boosting LLM-based Kernel Optimization with a Hierarchical and Hardware-aware Multi-armed Bandit](https://arxiv.org/abs/2511.18868)
*Dezhi Ran,Shuxiao Xie,Mingfang Ji,Ziyue Hua,Mengzhou Wu,Yuan Cao,Yuzhe Guo,Yu Hao,Linyi Li,Yitao Hu,Tao Xie*

Main category: cs.LG

TL;DR: KernelBand是一个将内核优化建模为分层多臂老虎机问题的框架，通过LLM代理在优化空间中进行策略性导航，显著提升大型语言模型内核优化的性能。


<details>
  <summary>Details</summary>
Motivation: 传统高质量内核开发需要大量硬件架构和软件优化专业知识，而现有LLM代码生成方法由于缺乏硬件领域知识，难以在广阔的优化空间中有效平衡探索与利用。

Method: 将内核优化构建为分层多臂老虎机问题，利用硬件分析信息识别有前景的优化策略，并通过运行时行为聚类减少内核候选的探索开销。

Result: 在TritonBench上的广泛实验表明，KernelBand显著优于现有最优方法，以更少的token实现更优性能，且随着计算资源增加表现出持续改进而无饱和现象。

Conclusion: KernelBand框架成功解决了LLM内核优化中的探索-利用平衡问题，为自动内核优化提供了有效解决方案。

Abstract: High quality kernels are critical for reducing training and inference costs of Large Language Models (LLMs), yet they traditionally require significant expertise in hardware architecture and software optimization. While recent advances in LLM-based code generation show promise for complex optimization, existing methods struggle with the vast optimization space due to insufficient hardware domain knowledge, failing to effectively balance exploration and exploitation. We present KernelBand, a novel framework that formulates kernel optimization as a hierarchical multi-armed bandit problem, enabling LLM agents to strategically navigate the optimization space by treating kernel selection and optimization strategy application as sequential decision-making processes. Our approach leverages hardware profiling information to identify promising optimization strategies and employs runtime behavior clustering to reduce exploration overhead across kernel candidates. Extensive experiments on TritonBench demonstrate that KernelBand significantly outperforms state-of-the-art methods, achieving superior performance with fewer tokens while exhibiting consistent improvement without saturation as computational resources increase.

</details>


### [247] [Periodic Asynchrony: An Effective Method for Accelerating On-Policy Reinforcement Learning](https://arxiv.org/abs/2511.18871)
*Jian Lu*

Main category: cs.LG

TL;DR: 本文提出了一种周期异步框架，通过分离推理和训练部署，结合数据加载器改进，在保持算法精度不变的前提下，实现了RL训练性能的至少三倍提升。


<details>
  <summary>Details</summary>
Motivation: 主流RL框架中推理和训练在同一设备上同步执行，这种计算耦合阻碍了并发推理和训练，导致训练效率成为关键挑战。

Method: 采用分离推理和训练部署的策略，引入数据加载器改进，将传统同步架构转变为周期异步框架；在训练阶段应用统一的三模型架构，并提出共享提示注意力掩码以减少重复计算。

Result: 在NPU平台上实现了至少三倍的整体RL训练性能提升，同时算法精度与同步方法完全等效。

Conclusion: 该周期异步框架支持按需独立弹性扩展各组件，具有广泛应用的潜力。

Abstract: Since the introduction of the GRPO algorithm, reinforcement learning (RL) has attracted increasing attention, with growing efforts to reproduce and apply it. However, training efficiency remains a critical challenge. In mainstream RL frameworks, inference and training are typically deployed on the same devices. While this approach reduces costs through resource consolidation, its synchronous execution imposes a computational coupling that prevents concurrent inference and training. In this study, we are returning to the strategy of separating inference and training deployment, and by introducing improvements in the data loader, we transform the conventional synchronous architecture into a periodically asynchronous framework, which allows for demand-driven, independent, and elastic scaling of each component, while the accuracy of the algorithm remains completely equivalent to the synchronization method, with both belonging to the on-policy strategy. It is worth emphasizing that we apply a unified tri-model architecture in the training phase, and we also proposed a shared-prompt attention mask to reduce repetitive computation. In practice, these works have achieved at least a threefold overall performance improvement in RL training on NPU platforms, indicating its potential for widespread application.

</details>


### [248] [Hi-SAFE: Hierarchical Secure Aggregation for Lightweight Federated Learning](https://arxiv.org/abs/2511.18887)
*Hyeong-Gun Joo,Songnam Hong,Seunghwan Lee,Dong-Joon Shin*

Main category: cs.LG

TL;DR: Hi-SAFE：一种轻量级加密安全聚合框架，用于基于符号的联邦学习，解决隐私保护和通信效率问题


<details>
  <summary>Details</summary>
Motivation: 联邦学习在物联网和边缘网络等资源受限环境中面临隐私保护和通信效率的双重挑战。基于符号的方法虽然节省带宽，但容易受到推理攻击，现有安全聚合技术要么不兼容要么开销过大

Method: 提出Hi-SAFE框架，基于费马小定理构造高效的多数投票多项式，将多数投票表示为有限域上的低次多项式，实现安全评估。采用分层子分组策略确保恒定乘法深度和有界用户复杂度

Result: 该框架能够隐藏中间值，仅揭示最终结果，在保护隐私的同时保持通信效率

Conclusion: Hi-SAFE为基于符号的联邦学习提供了一种轻量级且密码学安全的聚合解决方案，有效平衡了隐私保护和通信效率的需求

Abstract: Federated learning (FL) faces challenges in ensuring both privacy and communication efficiency, particularly in resource-constrained environments such as Internet of Things (IoT) and edge networks. While sign-based methods, such as sign stochastic gradient descent with majority voting (SIGNSGD-MV), offer substantial bandwidth savings, they remain vulnerable to inference attacks due to exposure of gradient signs. Existing secure aggregation techniques are either incompatible with sign-based methods or incur prohibitive overhead. To address these limitations, we propose Hi-SAFE, a lightweight and cryptographically secure aggregation framework for sign-based FL. Our core contribution is the construction of efficient majority vote polynomials for SIGNSGD-MV, derived from Fermat's Little Theorem. This formulation represents the majority vote as a low-degree polynomial over a finite field, enabling secure evaluation that hides intermediate values and reveals only the final result. We further introduce a hierarchical subgrouping strategy that ensures constant multiplicative depth and bounded per-user complexity, independent of the number of users n.

</details>


### [249] [Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models](https://arxiv.org/abs/2511.18890)
*Yonggan Fu,Xin Dong,Shizhe Diao,Matthijs Van keirsbilck,Hanrong Ye,Wonmin Byeon,Yashaswi Karnati,Lucas Liebenwein,Hannah Zhang,Nikolaus Binder,Maksim Khadkevich,Alexander Keller,Jan Kautz,Yingyan Celine Lin,Pavlo Molchanov*

Main category: cs.LG

TL;DR: 该论文提出了Nemotron-Flash系列混合小型语言模型，通过优化深度-宽度比和操作符选择来提升实际设备延迟性能，结合权重归一化技术，在保持参数效率的同时显著提升了准确率和延迟性能。


<details>
  <summary>Details</summary>
Motivation: 现有小型语言模型设计主要关注参数数量优化，但参数效率并不能直接转化为实际设备的速度提升。本文旨在识别影响SLM实际设备延迟的关键因素，为以延迟为主要考虑因素的SLM设计和训练提供通用原则和方法。

Method: 1. 识别深度-宽度比和操作符选择两个关键架构因素；2. 研究延迟最优的深度-宽度比；3. 探索高效注意力替代方案；4. 构建进化搜索框架自动发现延迟最优的操作符组合；5. 使用权重归一化技术增强SLM训练。

Result: 提出的Nemotron-Flash模型相比Qwen3-1.7B/0.6B，平均准确率提升超过5.5%，延迟降低1.3倍/1.9倍，吞吐量提高18.7倍/45.6倍，显著推进了准确率-效率前沿。

Conclusion: 通过系统性地优化架构设计和训练方法，可以构建在实际设备上具有优异延迟性能的小型语言模型，参数效率与实际性能之间存在重要差异，需要专门针对延迟进行优化。

Abstract: Efficient deployment of small language models (SLMs) is essential for numerous real-world applications with stringent latency constraints. While previous work on SLM design has primarily focused on reducing the number of parameters to achieve parameter-optimal SLMs, parameter efficiency does not necessarily translate into proportional real-device speed-ups. This work aims to identify the key determinants of SLMs' real-device latency and offer generalizable principles and methodologies for SLM design and training when real-device latency is the primary consideration. Specifically, we identify two central architectural factors: depth-width ratios and operator choices. The former is crucial for small-batch-size latency, while the latter affects both latency and large-batch-size throughput. In light of this, we first study latency-optimal depth-width ratios, with the key finding that although deep-thin models generally achieve better accuracy under the same parameter budget, they may not lie on the accuracy-latency trade-off frontier. Next, we explore emerging efficient attention alternatives to evaluate their potential as candidate building operators. Using the identified promising operators, we construct an evolutionary search framework to automatically discover latency-optimal combinations of these operators within hybrid SLMs, thereby advancing the accuracy-latency frontier. In addition to architectural improvements, we further enhance SLM training using a weight normalization technique that enables more effective weight updates and improves final convergence. Combining these methods, we introduce a new family of hybrid SLMs, called Nemotron-Flash, which significantly advances the accuracy-efficiency frontier of state-of-the-art SLMs, e.g., achieving over +5.5% average accuracy, 1.3x/1.9x lower latency, and 18.7x/45.6x higher throughput compared to Qwen3-1.7B/0.6B, respectively.

</details>


### [250] [VADE: Variance-Aware Dynamic Sampling via Online Sample-Level Difficulty Estimation for Multimodal RL](https://arxiv.org/abs/2511.18902)
*Zengjie Hu,Jiantao Qiu,Tianyi Bai,Haojin Yang,Binhang Yuan,Qi Jing,Conghui He,Wentao Zhang*

Main category: cs.LG

TL;DR: VADE是一个解决基于组策略优化方法中梯度消失问题的动态采样框架，通过在线样本难度估计和Thompson采样来选择最具信息量的样本，提升训练信号并减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的基于组策略优化方法（如GRPO、GSPO）存在梯度消失问题，当组内所有响应获得相同奖励时，优势估计会崩溃，训练信号减弱。现有解决方案要么计算开销大，要么缺乏实时适应性。

Method: VADE框架包含三个核心组件：使用Beta分布进行在线样本级难度估计、通过Thompson采样最大化信息增益、以及双尺度先验衰减机制来在策略演化下保持稳健估计。

Result: 在多模态推理基准测试中，VADE在性能和样本效率方面均优于强基线方法，同时显著减少了计算开销。该框架可作为即插即用组件集成到现有的基于组的强化学习算法中。

Conclusion: VADE通过动态选择最具信息量的样本，有效解决了基于组策略优化中的梯度消失问题，在提升性能的同时大幅降低了计算成本，具有很好的实用性和可扩展性。

Abstract: Group-based policy optimization methods like GRPO and GSPO have become standard for training multimodal models, leveraging group-wise rollouts and relative advantage estimation. However, they suffer from a critical \emph{gradient vanishing} problem when all responses within a group receive identical rewards, causing advantage estimates to collapse and training signals to diminish. Existing attempts to mitigate this issue fall into two paradigms: filtering-based and sampling-based methods. Filtering-based methods first generate rollouts broadly and then retroactively filter out uninformative groups, leading to substantial computational overhead. Sampling-based methods proactively select effective samples before rollout but rely on static criteria or prior dataset knowledge, lacking real-time adaptability. To address these issues, we propose \textbf{VADE}, a \textbf{V}ariance-\textbf{A}ware \textbf{D}ynamic sampling framework via online sample-level difficulty \textbf{E}stimation. Our framework integrates three key components: online sample-level difficulty estimation using Beta distributions, a Thompson sampler that maximizes information gain through the estimated correctness probability, and a two-scale prior decay mechanism that maintains robust estimation under policy evolution. This three components design enables VADE to dynamically select the most informative samples, thereby amplifying training signals while eliminating extra rollout costs. Extensive experiments on multimodal reasoning benchmarks show that VADE consistently outperforms strong baselines in both performance and sample efficiency, while achieving a dramatic reduction in computational overhead. More importantly, our framework can serves as a plug-and-play component to be seamlessly integrated into existing group-based RL algorithms. Code and models are available at https://VADE-RL.github.io.

</details>


### [251] [How Learning Rate Decay Wastes Your Best Data in Curriculum-Based LLM Pretraining](https://arxiv.org/abs/2511.18903)
*Kairong Luo,Zhenbo Sun,Haodong Wen,Xinyu Shi,Jiarui Cui,Chenyi Dang,Kaifeng Lyu,Wenguang Chen*

Main category: cs.LG

TL;DR: 研究发现课程式预训练效果受限的原因是数据质量递增顺序与学习率衰减计划不兼容，提出两种简单策略来缓解这种不兼容性，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 由于高质量数据稀缺，LLM通常在不同质量数据混合上训练。虽然课程式预训练理论上能更好利用高质量数据，但先前研究显示其改进有限，需要探究根本原因。

Method: 识别课程式训练与学习率衰减的不兼容问题，提出两种策略：使用更温和的学习率衰减计划（最终学习率仅略小于峰值学习率），以及用模型平均替代学习率衰减。

Result: 结合这两种策略，在标准基准测试上的平均得分比随机打乱训练提高了1.64%，在1.5B参数模型上验证有效。

Conclusion: 研究呼吁重新评估课程式LLM预训练方法，强调需要将数据课程与优化方法协同设计。

Abstract: Due to the scarcity of high-quality data, large language models (LLMs) are often trained on mixtures of data with varying quality levels, even after sophisticated data curation. A natural approach to better leverage high-quality data is curriculum-based pretraining, where the model is trained on data sorted in ascending order of quality as determined by a quality metric. However, prior studies have reported limited improvements from such curriculum-based pretraining strategies. This work identifies a critical factor constraining these methods: the incompatibility between the ascending data quality order and the decaying learning rate (LR) schedule. We find that while curriculum-based training substantially outperforms random shuffling when using a constant LR, its advantage diminishes under standard LR decay schedules. Our experiments show this incompatibility can be mitigated by two simple strategies: (1) employing a more moderate LR decay schedule, where the final LR is only moderately smaller than the peak LR, and (2) replacing LR decay with model averaging, i.e., computing a weighted average of the final few checkpoints. By combining these strategies, we improve the average score on a suite of standard benchmarks by 1.64% over random shuffling, without additional data refinement. Validated on 1.5B-parameter models trained over 30B tokens with various data-quality metrics, our findings call for a re-evaluation of curriculum-based LLM pretraining and underscore the potential of co-designing data curricula with optimization methods.

</details>


### [252] [Learning Solution Operators for Partial Differential Equations via Monte Carlo-Type Approximation](https://arxiv.org/abs/2511.18930)
*Salah Eddine Choutri,Prajwal Chauhan,Othmane Mazhar,Saif Eddin Jabari*

Main category: cs.LG

TL;DR: MCNO提出了一种轻量级架构，通过蒙特卡洛方法直接逼近核积分来学习参数化PDE的解算子，无需谱或平移不变性假设，能在多网格分辨率下泛化且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子（如傅里叶神经算子）依赖谱假设或平移不变性，限制了其适用性。MCNO旨在提供一种不依赖这些假设的轻量级替代方案。

Method: 使用蒙特卡洛方法直接逼近核积分，将核表示为固定随机采样点集上的可学习张量，无需固定全局基函数或训练期间重复采样。

Result: 在标准1D PDE基准测试中，MCNO以较低计算成本实现了竞争性精度。

Conclusion: MCNO为谱和基于图的神经算子提供了一种简单实用的替代方案，具有轻量级架构和良好的泛化能力。

Abstract: The Monte Carlo-type Neural Operator (MCNO) introduces a lightweight architecture for learning solution operators for parametric PDEs by directly approximating the kernel integral using a Monte Carlo approach. Unlike Fourier Neural Operators, MCNO makes no spectral or translation-invariance assumptions. The kernel is represented as a learnable tensor over a fixed set of randomly sampled points. This design enables generalization across multiple grid resolutions without relying on fixed global basis functions or repeated sampling during training. Experiments on standard 1D PDE benchmarks show that MCNO achieves competitive accuracy with low computational cost, providing a simple and practical alternative to spectral and graph-based neural operators.

</details>


### [253] [SWAN: Sparse Winnowed Attention for Reduced Inference Memory via Decompression-Free KV-Cache Compression](https://arxiv.org/abs/2511.18936)
*Santhosh G S,Saurav Prakash,Balaraman Ravindran*

Main category: cs.LG

TL;DR: SWAN是一种无需微调的KV缓存压缩框架，通过正交矩阵旋转和剪枝来减少内存占用，无需解压缩步骤，可在50-60%内存节省下保持接近原始性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自回归推理时面临KV缓存内存占用过大的瓶颈，现有压缩技术存在信息丢失、固定限制或解压缩计算开销等问题。

Method: 使用离线正交矩阵对KV缓存进行旋转和剪枝，然后直接用于注意力计算，无需重建过程，并配备小密集缓冲区。

Result: 在50-60%的每token KV缓存内存节省下，仍能保持接近未压缩基线的性能，且支持运行时可调压缩级别。

Conclusion: SWAN提供了一种无解压缩设计、高压缩性能下的良好表现和适应性，是服务长上下文LLMs的实用高效解决方案。

Abstract: Large Language Models (LLMs) face a significant bottleneck during autoregressive inference due to the massive memory footprint of the Key-Value (KV) cache. Existing compression techniques like token eviction, quantization, or other low-rank methods often risk information loss, have fixed limits, or introduce significant computational overhead from explicit decompression steps. In this work, we introduce SWAN, a novel, fine-tuning-free framework that eliminates this overhead. Our method uses an offline orthogonal matrix to rotate and prune the KV-cache, which is then used directly in the attention computation without any reconstruction. Our extensive experiments demonstrate that SWAN, augmented with a small dense buffer, offers a robust trade-off, maintaining performance close to the uncompressed baseline even at aggressive 50-60% memory savings per-token on KV-cache. A key advantage is its runtime-tunable compression level, allowing operators to dynamically adjust the memory footprint, a flexibility absent in methods requiring fixed offline configurations. This combination of a decompression-free design, high performance under compression, and adaptability makes SWAN a practical and efficient solution for serving LLMs with long contexts.

</details>


### [254] [Geometry-Aware Deep Congruence Networks for Manifold Learning in Cross-Subject Motor Imagery](https://arxiv.org/abs/2511.18940)
*Sanjeev Manivannan,Chandrashekar Lakshminarayan*

Main category: cs.LG

TL;DR: 提出几何感知预处理模块和深度同余网络，直接在SPD流形上处理协方差矩阵，解决零样本跨被试运动想象解码问题，在BCI-IV 2a基准上提升准确率3-4%。


<details>
  <summary>Details</summary>
Motivation: 解决跨被试运动想象解码中的挑战：强被试变异性和协方差矩阵在SPD流形上的弯曲几何结构，特别是在零样本设置下（不允许目标被试标签或适应）。

Method: 引入DCR和RiFU预处理模块扩展黎曼对齐，提出SPD-DCNet和RiFUNet两个流形分类器，使用分层同余变换学习判别性、被试不变的协方差表示。

Result: 在BCI-IV 2a基准上，跨被试准确率比最强经典基线提升3-4%。

Conclusion: 几何感知变换对稳健的EEG解码具有重要价值，证明了直接在SPD流形上操作的有效性。

Abstract: Cross-subject motor-imagery decoding remains a major challenge in EEG-based brain-computer interfaces due to strong subject variability and the curved geometry of covariance matrices on the symmetric positive definite (SPD) manifold. We address the zero-shot cross-subject setting, where no target-subject labels or adaptation are allowed, by introducing novel geometry-aware preprocessing modules and deep congruence networks that operate directly on SPD covariance matrices. Our preprocessing modules, DCR and RiFU, extend Riemannian Alignment by improving action separation while reducing subject-specific distortions. We further propose two manifold classifiers, SPD-DCNet and RiFUNet, which use hierarchical congruence transforms to learn discriminative, subject-invariant covariance representations. On the BCI-IV 2a benchmark, our framework improves cross-subject accuracy by 3-4% over the strongest classical baselines, demonstrating the value of geometry-aware transformations for robust EEG decoding.

</details>


### [255] [MIST: Mutual Information Via Supervised Training](https://arxiv.org/abs/2511.18945)
*German Gritsai,Megan Richards,Maxime Méloux,Kyunghyun Cho,Maxime Peyrard*

Main category: cs.LG

TL;DR: 提出了一种完全数据驱动的互信息估计器设计方法，使用神经网络参数化估计函数，在大规模元数据集上训练，能够处理可变样本大小和维度，并提供不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 传统互信息估计方法缺乏灵活性，作者希望通过完全经验主义的方法，用神经网络学习估计器，以换取更好的效率和适应性。

Method: 使用神经网络MIST参数化互信息估计函数，在625,000个合成联合分布元数据集上训练，采用二维注意力机制处理可变样本，通过分位数回归损失量化不确定性。

Result: 学习到的估计器在各种样本大小和维度下显著优于经典基线方法，包括在训练中未见过的联合分布上；基于分位数的区间校准良好，比基于bootstrap的置信区间更可靠，推理速度快几个数量级。

Conclusion: 该框架产生了可训练、完全可微的估计器，可以嵌入更大的学习流程中；利用互信息对可逆变换的不变性，可以通过归一化流适应任意数据模态，实现灵活训练。

Abstract: We propose a fully data-driven approach to designing mutual information (MI) estimators. Since any MI estimator is a function of the observed sample from two random variables, we parameterize this function with a neural network (MIST) and train it end-to-end to predict MI values. Training is performed on a large meta-dataset of 625,000 synthetic joint distributions with known ground-truth MI. To handle variable sample sizes and dimensions, we employ a two-dimensional attention scheme ensuring permutation invariance across input samples. To quantify uncertainty, we optimize a quantile regression loss, enabling the estimator to approximate the sampling distribution of MI rather than return a single point estimate. This research program departs from prior work by taking a fully empirical route, trading universal theoretical guarantees for flexibility and efficiency. Empirically, the learned estimators largely outperform classical baselines across sample sizes and dimensions, including on joint distributions unseen during training. The resulting quantile-based intervals are well-calibrated and more reliable than bootstrap-based confidence intervals, while inference is orders of magnitude faster than existing neural baselines. Beyond immediate empirical gains, this framework yields trainable, fully differentiable estimators that can be embedded into larger learning pipelines. Moreover, exploiting MI's invariance to invertible transformations, meta-datasets can be adapted to arbitrary data modalities via normalizing flows, enabling flexible training for diverse target meta-distributions.

</details>


### [256] [Learning to Compress Graphs via Dual Agents for Consistent Topological Robustness Evaluation](https://arxiv.org/abs/2511.18958)
*Qisen Chai,Yansong Wang,Junjie Huang,Tao Jia*

Main category: cs.LG

TL;DR: 提出了Cutter框架，使用双智能体强化学习来压缩图数据，保留拓扑结构和鲁棒性特征，提高对抗攻击评估效率。


<details>
  <summary>Details</summary>
Motivation: 图结构数据规模不断增大，评估其对抗攻击下的鲁棒性计算成本高昂且难以扩展，需要高效的压缩方法。

Method: 采用双智能体强化学习框架（VDA和RDA），结合轨迹级奖励塑造、原型塑造和跨智能体模仿三种策略来识别关键节点进行压缩。

Result: 在多个真实世界图上实验表明，Cutter生成的压缩图保留了重要拓扑特性，鲁棒性退化趋势与原图高度一致，显著提高了评估效率。

Conclusion: Cutter能够在不损害评估保真度的前提下，通过图压缩显著提升对抗攻击鲁棒性评估的效率。

Abstract: As graph-structured data grow increasingly large, evaluating their robustness under adversarial attacks becomes computationally expensive and difficult to scale. To address this challenge, we propose to compress graphs into compact representations that preserve both topological structure and robustness profile, enabling efficient and reliable evaluation.We propose Cutter, a dual-agent reinforcement learning framework composed of a Vital Detection Agent (VDA) and a Redundancy Detection Agent (RDA), which collaboratively identify structurally vital and redundant nodes for guided compression. Cutter incorporates three key strategies to enhance learning efficiency and compression quality: trajectory-level reward shaping to transform sparse trajectory returns into dense, policy-equivalent learning signals; prototype-based shaping to guide decisions using behavioral patterns from both highand low-return trajectories; and cross-agent imitation to enable safer and more transferable exploration. Experiments on multiple real-world graphs demonstrate that Cutter generates compressed graphs that retain essential static topological properties and exhibit robustness degradation trends highly consistent with the original graphs under various attack scenarios, thereby significantly improving evaluation efficiency without compromising assessment fidelity.

</details>


### [257] [AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention](https://arxiv.org/abs/2511.18960)
*Lei Xiao,Jifeng Li,Juntao Gao,Feiyang Ye,Yan Jin,Jingjing Qian,Jing Zhang,Yong Wu,Xiaoyuan Yu*

Main category: cs.LG

TL;DR: AVA-VLA是一个基于POMDP视角的视觉-语言-动作模型框架，通过引入主动视觉注意力机制来动态调节视觉处理，利用历史上下文信息提升动态序列决策性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型通常将任务建模为MDP，在每个时间步独立处理密集视觉输入，这种历史无关的设计无法有效利用历史上下文，在动态序列决策中表现欠佳。

Method: 从POMDP视角重新定义问题，提出AVA-VLA框架，引入主动视觉注意力(AVA)模块，利用循环状态（信念状态的神经近似）计算软权重来主动处理任务相关的视觉token。

Result: 在LIBERO和CALVIN等机器人基准测试中达到最先进性能，在双臂机器人平台上的实际部署验证了框架的实用性和强大的仿真到现实迁移能力。

Conclusion: AVA-VLA通过历史感知的视觉处理机制显著提升了VLA模型在动态环境中的决策性能，证明了POMDP视角在视觉-语言-动作任务中的有效性。

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in embodied AI tasks. However, existing VLA models, often built upon Vision-Language Models (VLMs), typically process dense visual inputs independently at each timestep. This approach implicitly models the task as a Markov Decision Process (MDP). However, this history-agnostic design is suboptimal for effective visual token processing in dynamic sequential decision-making, as it fails to leverage the context of history. To address this limitation, we reformulate the problem from a Partially Observable Markov Decision Process (POMDP) perspective and propose a novel framework named AVA-VLA. Inspired by the POMDP that the action generation should be conditioned on the belief state. AVA-VLA introduces Active Visual Attention (AVA) to dynamically modulate visual processing. It achieves this by leveraging the recurrent state, which is a neural approximation of the agent's belief state derived from the previous decision step. Specifically, the AVA module uses the recurrent state to compute the soft weights to actively process task-relevant visual tokens based on its historical context. Comprehensive evaluations demonstrate that AVA-VLA achieves state-of-the-art performance across popular robotic benchmarks, including LIBERO and CALVIN. Furthermore, real-world deployments on a dual-arm robot platform validate the framework's practical applicability and robust sim-to-real transferability.

</details>


### [258] [FastForward Pruning: Efficient LLM Pruning via Single-Step Reinforcement Learning](https://arxiv.org/abs/2511.18977)
*Xin Yuan,Siqi Li,Jiateng Wei,Chengrui Zhu,Yanming Wu,Qingpeng Li,Jiajun Lv,Xiaoke Lan,Jun Chen,Yong Liu*

Main category: cs.LG

TL;DR: FastForward Pruning是一种高效的剪枝方法，通过解耦的单步强化学习框架分离策略优化和预算约束问题，显著降低计算成本，在LLaMA、Mistral和OPT模型上优于启发式方法。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法中，启发式方法速度快但性能次优，而基于搜索的方法（如强化学习）计算成本过高，难以在大规模模型上应用。

Method: 提出解耦的单步强化学习框架，将策略优化与预算约束问题分离，采用课程学习策略从简单任务逐步增加复杂度。

Result: 在LLaMA、Mistral和OPT模型家族上，该方法发现的剪枝策略优于强启发式基线，与其他搜索算法相比，以更低的计算成本获得竞争性或更优的结果。

Conclusion: FastForward Pruning在搜索效率上具有明显优势，能够以较低计算成本找到高质量的层间稀疏度分配策略。

Abstract: Pruning is an effective method for compressing Large Language Models, but finding an optimal, non-uniform layer-wise sparsity allocation remains a key challenge. While heuristic methods are fast but yield suboptimal performance, more powerful search-based approaches like Reinforcement Learning are often hindered by prohibitive computational costs on large-scale models. To overcome this efficiency barrier, we propose FastForward Pruning. Its core is a decoupled, single-step RL framework that separates policy optimization from the complex budget satisfaction problem. Such a decoupling is crucial for efficiently searching the vast policy space of LLMs. This curriculum-based strategy begins with low-cost, simple tasks and gradually increases in complexity, significantly reducing the search's computational overhead. Evaluated on the LLaMA, Mistral, and OPT model families, our framework discovers pruning policies that achieve superior performance over strong heuristic baselines. Crucially, when compared to other search-based algorithms, our method achieves competitive or superior results at a fraction of the computational cost, demonstrating a clear advantage in search efficiency.

</details>


### [259] [Dynamic Mixture of Experts Against Severe Distribution Shifts](https://arxiv.org/abs/2511.18987)
*Donghu Kim*

Main category: cs.LG

TL;DR: 评估DynamicMoE方法在持续学习和强化学习环境中的表现，并与现有网络扩展方法进行基准测试


<details>
  <summary>Details</summary>
Motivation: 解决持续学习和强化学习中神经网络持续学习和适应演化数据流的挑战，受生物大脑通过容量增长保持可塑性的启发

Method: 采用动态混合专家(DynamicMoE)架构，通过为不同分布专门化专家来实现参数高效的学习

Result: 论文旨在评估DynamicMoE方法的有效性，并与现有网络扩展方法进行比较

Conclusion: MoE架构为解决持续学习中的可塑性-稳定性困境提供了有前景的替代方案

Abstract: The challenge of building neural networks that can continuously learn and adapt to evolving data streams is central to the fields of continual learning (CL) and reinforcement learning (RL). This lifelong learning problem is often framed in terms of the plasticity-stability dilemma, focusing on issues like loss of plasticity and catastrophic forgetting. Unlike neural networks, biological brains maintain plasticity through capacity growth, inspiring researchers to explore similar approaches in artificial networks, such as adding capacity dynamically. Prior solutions often lack parameter efficiency or depend on explicit task indices, but Mixture-of-Experts (MoE) architectures offer a promising alternative by specializing experts for distinct distributions. This paper aims to evaluate a DynamicMoE approach for continual and reinforcement learning environments and benchmark its effectiveness against existing network expansion methods.

</details>


### [260] [3D Dynamic Radio Map Prediction Using Vision Transformers for Low-Altitude Wireless Networks](https://arxiv.org/abs/2511.19019)
*Nguyen Duc Minh Quang,Chang Liu,Huy-Trung Nguyen,Shuangyang Li,Derrick Wing Kwan Ng,Wei Xiang*

Main category: cs.LG

TL;DR: 提出了3D动态无线电地图（3D-DRM）框架，用于学习和预测低空无线网络中接收功率的时空演化，解决了传统静态无线电地图无法捕捉实时功率变化的问题。


<details>
  <summary>Details</summary>
Motivation: 低空无线网络（LAWN）中，由于三维移动性、时变用户密度和有限功率预算，基站发射功率随用户位置和流量需求动态波动，导致高度非平稳的3D无线电环境。现有无线电地图多为静态或离线构建，忽略了多无人机网络中的实时功率变化和时空依赖性。

Method: 使用Vision Transformer（ViT）编码器从3D无线电地图中提取高维空间表示，同时采用基于Transformer的模块建模序列依赖性，以预测未来的功率分布。

Result: 实验表明，3D-DRM能够准确捕捉快速变化的功率动态，在无线电地图重建和短期预测方面显著优于基线模型。

Conclusion: 3D-DRM框架有效解决了低空无线网络中动态无线电环境的建模问题，为无线电感知的网络优化提供了有力工具。

Abstract: Low-altitude wireless networks (LAWN) are rapidly expanding with the growing deployment of unmanned aerial vehicles (UAVs) for logistics, surveillance, and emergency response. Reliable connectivity remains a critical yet challenging task due to three-dimensional (3D) mobility, time-varying user density, and limited power budgets. The transmit power of base stations (BSs) fluctuates dynamically according to user locations and traffic demands, leading to a highly non-stationary 3D radio environment. Radio maps (RMs) have emerged as an effective means to characterize spatial power distributions and support radio-aware network optimization. However, most existing works construct static or offline RMs, overlooking real-time power variations and spatio-temporal dependencies in multi-UAV networks. To overcome this limitation, we propose a {3D dynamic radio map (3D-DRM)} framework that learns and predicts the spatio-temporal evolution of received power. Specially, a Vision Transformer (ViT) encoder extracts high-dimensional spatial representations from 3D RMs, while a Transformer-based module models sequential dependencies to predict future power distributions. Experiments unveil that 3D-DRM accurately captures fast-varying power dynamics and substantially outperforms baseline models in both RM reconstruction and short-term prediction.

</details>


### [261] [OrdMoE: Preference Alignment via Hierarchical Expert Group Ranking in Multimodal Mixture-of-Experts LLMs](https://arxiv.org/abs/2511.19023)
*Yuting Gao,Weihao Chen,Lan Wang,Ruihan Xu,Qingpei Guo*

Main category: cs.LG

TL;DR: OrdMoE是一种新颖的偏好对齐框架，通过利用MoE架构中的内部信号来避免依赖外部人工标注的偏好数据，实现零成本的自我监督偏好排序。


<details>
  <summary>Details</summary>
Motivation: 现有的偏好学习方法主要依赖外部人工标注的偏好数据，这些数据收集成本高且劳动密集。

Method: 基于MoE架构中路由器专家选择分数隐含编码响应质量排序的观察，将专家按路由分数分组为不同层级，分别激活以产生质量递增的响应序列，构建内部偏好层次。

Result: 在多个多模态基准测试上的广泛实验表明，OrdMoE显著提升了多模态MoE LLMs的对齐和整体性能，在不依赖任何人工标注偏好数据的情况下取得了有竞争力的结果。

Conclusion: OrdMoE框架成功证明了利用MoE架构内部信号进行偏好对齐的可行性，为多模态大语言模型的后训练对齐提供了一种零成本、自监督的替代方案。

Abstract: Preference learning has recently emerged as a pivotal strategy for post-training alignment of Multimodal Large Language Models (MLLMs). However, existing approaches predominantly rely on external human-annotated preference data, which is costly and labor-intensive to collect. In this work, we propose OrdMoE, a novel preference alignment framework that bypasses the reliance on external human preferences entirely by leveraging intrinsic signals within Mixture-of-Experts (MoE) architectures. Specifically, we observe that the router's expert selection scores implicitly encode a quality-aware ranking of responses (i.e. higher-scoring experts consistently generate higher-quality outputs). Building on this insight, OrdMoE constructs an internal preference hierarchy by grouping experts into ranked tiers based on their per-token routing scores and activating each tier separately to produce a sequence of responses with increasing quality. This yields a zero-cost, self-supervised preference ordering over generated responses, which can be directly optimized using standard preference learning objectives. Extensive experiments across multiple multimodal benchmarks demnstrate that OrdMoE significantly enhances both alignment and overall performance of multimodal Mixture-of-Experts LLMs, achieving competitive results without requiring any human-annotated preference data.

</details>


### [262] [Resolving Node Identifiability in Graph Neural Processes via Laplacian Spectral Encodings](https://arxiv.org/abs/2511.19037)
*Zimo Yan,Zheng Xie,Chang Liu,Yuan Wang*

Main category: cs.LG

TL;DR: 提出了一种拉普拉斯位置编码方法，能够克服传统图神经网络在Weisfeiler-Lehman测试下的表达能力限制，通过理论证明该编码具有节点可识别性，并在药物相互作用任务中取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统消息传递图神经网络的表达能力受限于一维Weisfeiler-Lehman测试，无法区分结构不同的节点，需要解决这一理论表达能力限制。

Method: 提出拉普拉斯位置编码，该方法对特征向量符号翻转和特征空间内的基旋转具有不变性，结合单调链接、谱三角测量和定量谱单射性等技术。

Result: 理论证明该编码能从常数次观测中实现节点识别，与Weisfeiler-Lehman约束架构建立了样本复杂度分离。在药物-药物相互作用任务中，AUC和F1分数均有显著提升。

Conclusion: 通过理论上有原则的位置信息解决表达能力限制具有实际益处，拉普拉斯位置编码为图神经网络提供了更强的表达能力。

Abstract: Message passing graph neural networks are widely used for learning on graphs, yet their expressive power is limited by the one-dimensional Weisfeiler-Lehman test and can fail to distinguish structurally different nodes. We provide rigorous theory for a Laplacian positional encoding that is invariant to eigenvector sign flips and to basis rotations within eigenspaces. We prove that this encoding yields node identifiability from a constant number of observations and establishes a sample-complexity separation from architectures constrained by the Weisfeiler-Lehman test. The analysis combines a monotone link between shortest-path and diffusion distance, spectral trilateration with a constant set of anchors, and quantitative spectral injectivity with logarithmic embedding size. As an instantiation, pairing this encoding with a neural-process style decoder yields significant gains on a drug-drug interaction task on chemical graphs, improving both the area under the ROC curve and the F1 score and demonstrating the practical benefits of resolving theoretical expressiveness limitations with principled positional information.

</details>


### [263] [Mitigating Participation Imbalance Bias in Asynchronous Federated Learning](https://arxiv.org/abs/2511.19066)
*Xiangyu Chang,Manyi Yao,Srikanth V. Krishnamurthy,Christian R. Shelton,Anirban Chakraborty,Ananthram Swami,Samet Oymak,Amit Roy-Chowdhury*

Main category: cs.LG

TL;DR: 本文分析了异步联邦学习(AFL)中的异构性放大问题，提出了ACE和ACED方法来缓解参与不平衡和延迟问题。


<details>
  <summary>Details</summary>
Motivation: 在非IID数据分布的联邦学习环境中，异步更新模式会放大客户端异构性的负面影响，导致全局模型偏向频繁更新的快速客户端。

Method: 提出ACE方法，通过立即使用所有客户端的最新信息进行非缓冲更新来缓解参与不平衡；还提出ACED变体，在客户端多样性和更新延迟之间进行平衡。

Result: 在不同模型、任务和异构性设置下的实验验证了理论分析，并证明了所提方法的鲁棒性能。

Conclusion: ACE和ACED方法能有效缓解异步联邦学习中的异构性放大问题，平衡客户端参与并提高模型性能。

Abstract: In Asynchronous Federated Learning (AFL), the central server immediately updates the global model with each arriving client's contribution. As a result, clients perform their local training on different model versions, causing information staleness (delay). In federated environments with non-IID local data distributions, this asynchronous pattern amplifies the adverse effect of client heterogeneity (due to different data distribution, local objectives, etc.), as faster clients contribute more frequent updates, biasing the global model. We term this phenomenon heterogeneity amplification. Our work provides a theoretical analysis that maps AFL design choices to their resulting error sources when heterogeneity amplification occurs. Guided by our analysis, we propose ACE (All-Client Engagement AFL), which mitigates participation imbalance through immediate, non-buffered updates that use the latest information available from all clients. We also introduce a delay-aware variant, ACED, to balance client diversity against update staleness. Experiments on different models for different tasks across diverse heterogeneity and delay settings validate our analysis and demonstrate the robust performance of our approaches.

</details>


### [264] [EnfoPath: Energy-Informed Analysis of Generative Trajectories in Flow Matching](https://arxiv.org/abs/2511.19087)
*Ziyun Li,Ben Dai,Huancheng Hu,Henrik Boström,Soon Hoe Lim*

Main category: cs.LG

TL;DR: 本文提出动能路径能量（KPE）作为ODE采样器生成路径的诊断工具，发现高KPE预测更强的语义质量，且与数据密度呈负相关，揭示语义丰富样本位于数据分布的稀疏前沿。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注端点指标（如保真度、似然、感知质量），而忽视了采样轨迹所揭示的深层信息。受经典力学启发，希望了解生成路径能揭示什么。

Method: 引入动能路径能量（KPE），量化ODE采样器每条生成路径的总动能消耗。在CIFAR-10和ImageNet-256上进行全面实验。

Result: 发现两个关键现象：（i）高KPE预测更强的语义质量，语义更丰富的样本需要更大的动能消耗；（ii）高KPE与数据密度呈负相关，信息丰富的样本位于稀疏的低密度区域。

Conclusion: 语义信息丰富的样本自然地存在于数据分布的稀疏前沿，需要更大的生成努力。轨迹级分析为理解生成难度和样本特征提供了物理启发且可解释的框架。

Abstract: Flow-based generative models synthesize data by integrating a learned velocity field from a reference distribution to the target data distribution. Prior work has focused on endpoint metrics (e.g., fidelity, likelihood, perceptual quality) while overlooking a deeper question: what do the sampling trajectories reveal? Motivated by classical mechanics, we introduce kinetic path energy (KPE), a simple yet powerful diagnostic that quantifies the total kinetic effort along each generation path of ODE-based samplers. Through comprehensive experiments on CIFAR-10 and ImageNet-256, we uncover two key phenomena: ({i}) higher KPE predicts stronger semantic quality, indicating that semantically richer samples require greater kinetic effort, and ({ii}) higher KPE inversely correlates with data density, with informative samples residing in sparse, low-density regions. Together, these findings reveal that semantically informative samples naturally reside on the sparse frontier of the data distribution, demanding greater generative effort. Our results suggest that trajectory-level analysis offers a physics-inspired and interpretable framework for understanding generation difficulty and sample characteristics.

</details>


### [265] [Optimization of Deep Learning Models for Dynamic Market Behavior Prediction](https://arxiv.org/abs/2511.19090)
*Shenghan Zhao,Yuzhen Lin,Ximeng Yang,Qiaochu Lu,Haozhong Xue,Gaozhe Jiang*

Main category: cs.LG

TL;DR: 提出一种结合多尺度时间卷积、门控循环模块和时间感知自注意力的混合序列模型，用于电商多时间跨度需求预测，在多个评估指标上优于传统方法和最先进的Transformer预测器。


<details>
  <summary>Details</summary>
Motivation: 金融科技领域深度学习的兴起为预测消费者行为提供了新机遇，特别是在电商交易中准确预测多时间跨度的SKU需求对于优化库存管理和市场效率至关重要。

Method: 使用混合序列模型，整合多尺度时间卷积、门控循环模块和时间感知自注意力机制，采用标准回归损失训练，并通过严格的时间分割防止数据泄露。

Result: 模型在MAE、RMSE、sMAPE、MASE和Theil's U_2等指标上均优于ARIMA/Prophet、LSTM/GRU、LightGBM和先进Transformer预测器，在峰值/节假日期间表现出更好的鲁棒性。

Conclusion: 所提出的混合序列模型在电商多时间跨度需求预测任务中实现了准确性和鲁棒性的显著提升，并通过消融实验和统计显著性测试验证了改进的可靠性。

Abstract: The advent of financial technology has witnessed a surge in the utilization of deep learning models to anticipate consumer conduct, a trend that has demonstrated considerable potential in enhancing lending strategies and bolstering market efficiency. We study multi-horizon demand forecasting on e-commerce transactions using the UCI Online Retail II dataset. Unlike prior versions of this manuscript that mixed financial-loan narratives with retail data, we focus exclusively on retail market behavior and define a clear prediction target: per SKU daily demand (or revenue) for horizons H=1,7,14. We present a hybrid sequence model that combines multi-scale temporal convolutions, a gated recurrent module, and time-aware self-attention. The model is trained with standard regression losses and evaluated under MAE, RMSE, sMAPE, MASE, and Theil's U_2 with strict time-based splits to prevent leakage. We benchmark against ARIMA/Prophet, LSTM/GRU, LightGBM, and state-of-the-art Transformer forecasters (TFT, Informer, Autoformer, N-BEATS). Results show consistent accuracy gains and improved robustness on peak/holiday periods. We further provide ablations and statistical significance tests to ensure the reliability of improvements, and we release implementation details to facilitate reproducibility.

</details>


### [266] [Edge-Based Predictive Data Reduction for Smart Agriculture: A Lightweight Approach to Efficient IoT Communication](https://arxiv.org/abs/2511.19103)
*Dora Krekovic,Mario Kusek,Ivana Podnar Zarko,Danh Le-Phuoc*

Main category: cs.LG

TL;DR: 提出了一种用于边缘计算环境的预测算法，通过预测传感器数据并在偏差超过预设阈值时才传输数据，减少通信开销和能耗，支持跨站点部署。


<details>
  <summary>Details</summary>
Motivation: 物联网设备快速增长导致大量传感器数据传输到云端，造成网络拥塞、延迟增加和高能耗，特别是在资源受限的远程环境中，连续传输变化不大的传感器数据效率低下。

Method: 在网络边缘使用预测滤波器预测下一个传感器数据点，仅当实际值与预测值的偏差超过预设容差时才触发数据传输，同时云端模型确保数据完整性和系统一致性。

Result: 该双模型策略有效减少了通信开销，通过最小化冗余传输提高了能源效率，支持跨站点泛化，使模型可以在不同区域部署而无需重新训练。

Conclusion: 该解决方案具有高度可扩展性、能源感知能力，非常适合在远程和带宽受限的物联网环境中优化传感器数据传输。

Abstract: The rapid growth of IoT devices has led to an enormous amount of sensor data that requires transmission to cloud servers for processing, resulting in excessive network congestion, increased latency and high energy consumption. This is particularly problematic in resource-constrained and remote environments where bandwidth is limited, and battery-dependent devices further emphasize the problem. Moreover, in domains such as agriculture, consecutive sensor readings often have minimal variation, making continuous data transmission inefficient and unnecessarily resource intensive. To overcome these challenges, we propose an analytical prediction algorithm designed for edge computing environments and validated through simulation. The proposed solution utilizes a predictive filter at the network edge that forecasts the next sensor data point and triggers data transmission only when the deviation from the predicted value exceeds a predefined tolerance. A complementary cloud-based model ensures data integrity and overall system consistency. This dual-model strategy effectively reduces communication overhead and demonstrates potential for improving energy efficiency by minimizing redundant transmissions. In addition to reducing communication load, our approach leverages both in situ and satellite observations from the same locations to enhance model robustness. It also supports cross-site generalization, enabling models trained in one region to be effectively deployed elsewhere without retraining. This makes our solution highly scalable, energy-aware, and well-suited for optimizing sensor data transmission in remote and bandwidth-constrained IoT environments.

</details>


### [267] [The Core in Max-Loss Non-Centroid Clustering Can Be Empty](https://arxiv.org/abs/2511.19107)
*Robert Bredereck,Eva Deltl,Leon Kellerhals,Jannik Peters*

Main category: cs.LG

TL;DR: 该论文研究了在最大损失目标下的非质心聚类中的核心稳定性问题，证明了对于k≥3的情况，存在度量实例使得没有任何聚类位于α-核心中（α<2^(1/5)），这是该领域的首个不可能性结果。


<details>
  <summary>Details</summary>
Motivation: 研究非质心聚类中核心稳定性的存在性，填补了在最大损失目标下核心可能为空的理论空白。

Method: 使用数学证明和计算机辅助证明方法，构建了特定的度量实例和二维欧几里得点集来验证理论下界。

Result: 证明了对于所有k≥3且n≥9（n能被k整除）的情况，存在度量实例使得α-核心为空（α<2^(1/5)≈1.148），且该下界对于构造是紧的。

Conclusion: 这是首个证明在最大损失目标下的非质心聚类中核心可能为空的结果，为理解聚类稳定性提供了重要理论洞见。

Abstract: We study core stability in non-centroid clustering under the max-loss objective, where each agent's loss is the maximum distance to other members of their cluster. We prove that for all $k\geq 3$ there exist metric instances with $n\ge 9$ agents, with $n$ divisible by $k$, for which no clustering lies in the $α$-core for any $α<2^{\frac{1}{5}}\sim 1.148$. The bound is tight for our construction. Using a computer-aided proof, we also identify a two-dimensional Euclidean point set whose associated lower bound is slightly smaller than that of our general construction. This is, to our knowledge, the first impossibility result showing that the core can be empty in non-centroid clustering under the max-loss objective.

</details>


### [268] [Uncertainty-Aware Deep Learning Framework for Remaining Useful Life Prediction in Turbofan Engines with Learned Aleatoric Uncertainty](https://arxiv.org/abs/2511.19124)
*Krishang Sharma*

Main category: cs.LG

TL;DR: 提出了一种新颖的不确定性感知深度学习框架，用于航空发动机剩余使用寿命预测，通过概率建模直接学习偶然不确定性，在关键区域性能上比传统方法提升25-40%。


<details>
  <summary>Details</summary>
Motivation: 准确预测剩余使用寿命并进行不确定性量化是航空预测性维护的关键挑战，现有CMAPSS文献中尚未探索通过概率建模直接学习偶然不确定性的方法。

Method: 采用分层架构，集成多尺度Inception块进行时间模式提取、双向LSTM进行序列建模，以及同时在传感器和时间维度运行的双层注意力机制。创新点在于贝叶斯输出层，可同时预测平均RUL和方差。

Result: 在NASA CMAPSS基准测试中，整体RMSE分别为16.22、19.29、16.84和19.98。关键区域性能突破性提升，RUL≤30周期时RMSE为5.14、6.89、5.27和7.16，比传统方法提升25-40%。

Conclusion: 该框架实现了校准良好的95%置信区间，覆盖率达到93.5%-95.2%，为安全关键预测建立了新基准，实现了以前在CMAPSS文献中无法实现的风险感知维护调度。

Abstract: Accurate Remaining Useful Life (RUL) prediction coupled with uncertainty quantification remains a critical challenge in aerospace prognostics. This research introduces a novel uncertainty-aware deep learning framework that learns aleatoric uncertainty directly through probabilistic modeling, an approach unexplored in existing CMAPSS-based literature. Our hierarchical architecture integrates multi-scale Inception blocks for temporal pattern extraction, bidirectional Long Short-Term Memory networks for sequential modeling, and a dual-level attention mechanism operating simultaneously on sensor and temporal dimensions. The innovation lies in the Bayesian output layer that predicts both mean RUL and variance, enabling the model to learn data-inherent uncertainty. Comprehensive preprocessing employs condition-aware clustering, wavelet denoising, and intelligent feature selection. Experimental validation on NASA CMAPSS benchmarks (FD001-FD004) demonstrates competitive overall performance with RMSE values of 16.22, 19.29, 16.84, and 19.98 respectively. Remarkably, our framework achieves breakthrough critical zone performance (RUL <= 30 cycles) with RMSE of 5.14, 6.89, 5.27, and 7.16, representing 25-40 percent improvements over conventional approaches and establishing new benchmarks for safety-critical predictions. The learned uncertainty provides well-calibrated 95 percent confidence intervals with coverage ranging from 93.5 percent to 95.2 percent, enabling risk-aware maintenance scheduling previously unattainable in CMAPSS literature.

</details>


### [269] [Masked Diffusion Models are Secretly Learned-Order Autoregressive Models](https://arxiv.org/abs/2511.19152)
*Prateek Garg,Bhavya Kohli,Sunita Sarawagi*

Main category: cs.LG

TL;DR: 本文提出了一种训练框架，通过多元噪声调度优化掩码扩散模型（MDMs）的解码顺序，证明MDM目标可以分解为这些顺序上的加权自回归损失。


<details>
  <summary>Details</summary>
Motivation: MDMs在实践中解码顺序对性能有显著影响，但现有方法无法在训练中优化解码顺序。本文旨在设计能够优化有利解码顺序的训练框架。

Method: 使用多元噪声调度的连续时间变分目标，建立解码顺序与多元噪声调度之间的直接对应关系，并证明MDM目标可以分解为加权自回归损失。

Result: 证明了MDM目标对噪声调度不再保持不变性，并且可以精确分解为可学习顺序上的加权自回归损失，从而将MDMs确立为具有可学习顺序的自回归模型。

Conclusion: 提出的训练框架能够识别和优化解码顺序，为MDMs提供了更灵活和优化的生成建模方法。

Abstract: Masked Diffusion Models (MDMs) have emerged as one of the most promising paradigms for generative modeling over discrete domains. It is known that MDMs effectively train to decode tokens in a random order, and that this ordering has significant performance implications in practice. This observation raises a fundamental question: can we design a training framework that optimizes for a favorable decoding order? We answer this in the affirmative, showing that the continuous-time variational objective of MDMs, when equipped with multivariate noise schedules, can identify and optimize for a decoding order during training. We establish a direct correspondence between decoding order and the multivariate noise schedule and show that this setting breaks invariance of the MDM objective to the noise schedule. Furthermore, we prove that the MDM objective decomposes precisely into a weighted auto-regressive losses over these orders, which establishes them as auto-regressive models with learnable orders.

</details>


### [270] [First-order Sobolev Reinforcement Learning](https://arxiv.org/abs/2511.19165)
*Fabian Schramm,Nicolas Perrin-Gilbert,Justin Carpentier*

Main category: cs.LG

TL;DR: 提出一种改进的时间差分学习方法，通过强制一阶贝尔曼一致性来训练价值函数，使其不仅匹配贝尔曼目标值，还匹配其关于状态和动作的导数。


<details>
  <summary>Details</summary>
Motivation: 传统TD学习只关注价值函数与贝尔曼目标在数值上的一致性，而忽略了一阶导数信息。通过考虑局部几何结构，可以改善价值函数的收敛性和策略梯度的稳定性。

Method: 通过可微动力学对贝尔曼备份进行微分，获得解析一致梯度目标。使用Sobolev型损失将一阶TD匹配原则融入评论家目标函数中。

Result: 该方法可以无缝集成到现有算法中（如Q学习、DDPG、SAC），不改变整体结构。

Conclusion: 一阶TD匹配原则有望加速评论家收敛，提供更稳定的策略梯度，同时保持算法结构的兼容性。

Abstract: We propose a refinement of temporal-difference learning that enforces first-order Bellman consistency: the learned value function is trained to match not only the Bellman targets in value but also their derivatives with respect to states and actions. By differentiating the Bellman backup through differentiable dynamics, we obtain analytically consistent gradient targets. Incorporating these into the critic objective using a Sobolev-type loss encourages the critic to align with both the value and local geometry of the target function. This first-order TD matching principle can be seamlessly integrated into existing algorithms, such as Q-learning or actor-critic methods (e.g., DDPG, SAC), potentially leading to faster critic convergence and more stable policy gradients without altering their overall structure.

</details>


### [271] [RAVEN++: Pinpointing Fine-Grained Violations in Advertisement Videos with Active Reinforcement Reasoning](https://arxiv.org/abs/2511.19168)
*Deyi Ji,Yuekui Yang,Liqun Liu,Peng Shu,Haiyang Wu,Shaogang Tang,Xudong Chen,Shaoping Ma,Tianrun Chen,Lanyun Zhu*

Main category: cs.LG

TL;DR: RAVEN++是一个用于视频广告审核的增强框架，通过主动强化学习、细粒度违规理解和渐进式多阶段训练，提升了违规检测的精确性、可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频广告审核模型（如RAVEN）在细粒度理解、可解释性和泛化能力方面存在不足，需要更先进的解决方案来应对复杂的视频广告审核挑战。

Method: 1) 主动强化学习动态适应不同难度样本；2) 通过分层奖励函数和推理蒸馏实现细粒度违规理解；3) 渐进式多阶段训练结合知识注入、课程式被动强化学习和主动强化学习。

Result: 在公共和专有数据集上的实验表明，RAVEN++在细粒度违规理解、推理能力和泛化能力方面优于通用大语言模型和专用模型如RAVEN，在线A/B测试也验证了其有效性。

Conclusion: RAVEN++通过创新的强化学习方法和多阶段训练策略，显著提升了视频广告审核的精度和可解释性，为复杂广告内容审核提供了有效解决方案。

Abstract: Advertising (Ad) is a cornerstone of the digital economy, yet the moderation of video advertisements remains a significant challenge due to their complexity and the need for precise violation localization. While recent advancements, such as the RAVEN model, have improved coarse-grained violation detection, critical gaps persist in fine-grained understanding, explainability, and generalization. To address these limitations, we propose RAVEN++, a novel framework that introduces three key innovations: 1) Active Reinforcement Learning (RL), which dynamically adapts training to samples of varying difficulty; 2) Fine-Grained Violation Understanding, achieved through hierarchical reward functions and reasoning distillation; and 3) Progressive Multi-Stage Training, which systematically combines knowledge injection, curriculum-based passive RL, and active RL. Extensive experiments on both public and proprietary datasets, on both offline scenarios and online deployed A/B Testing, demonstrate that RAVEN++ outperforms general-purpose LLMs and specialized models like RAVEN in terms of fine-grained violation understanding, reasoning capabilities, and generalization ability.

</details>


### [272] [From Raw Features to Effective Embeddings: A Three-Stage Approach for Multimodal Recipe Recommendation](https://arxiv.org/abs/2511.19176)
*Jeeho Shin,Kyungho Kim,Kijung Shin*

Main category: cs.LG

TL;DR: TESMR是一个三阶段食谱推荐框架，通过内容增强、关系增强和学习增强逐步优化多模态特征，在真实数据集上Recall@10提升7-15%。


<details>
  <summary>Details</summary>
Motivation: 食谱推荐需要有效利用用户-食谱交互之外的多模态特征，系统性地增强这些信号具有很大潜力。

Method: 三阶段框架：1）基于内容增强，使用具有多模态理解能力的基础模型；2）基于关系增强，通过用户-食谱交互的消息传播；3）基于学习增强，通过可学习嵌入的对比学习。

Result: 在两个真实世界数据集上的实验表明，TESMR优于现有方法，Recall@10提高了7-15%。

Conclusion: TESMR通过渐进式多模态特征增强，显著提升了食谱推荐的性能。

Abstract: Recipe recommendation has become an essential task in web-based food platforms. A central challenge is effectively leveraging rich multimodal features beyond user-recipe interactions. Our analysis shows that even simple uses of multimodal signals yield competitive performance, suggesting that systematic enhancement of these signals is highly promising. We propose TESMR, a 3-stage framework for recipe recommendation that progressively refines raw multimodal features into effective embeddings through: (1) content-based enhancement using foundation models with multimodal comprehension, (2) relation-based enhancement via message propagation over user-recipe interactions, and (3) learning-based enhancement through contrastive learning with learnable embeddings. Experiments on two real-world datasets show that TESMR outperforms existing methods, achieving 7-15% higher Recall@10.

</details>


### [273] [Empirical Comparison of Forgetting Mechanisms for UCB-based Algorithms on a Data-Driven Simulation Platform](https://arxiv.org/abs/2511.19240)
*Minxin Chen*

Main category: cs.LG

TL;DR: 提出了FDSW-UCB算法，结合折扣长期视角和滑动窗口短期视角，在非平稳多臂老虎机环境中表现优异，解决了传统UCB算法在动态环境中的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的老虎机问题常涉及非平稳奖励分布，传统UCB算法在动态环境中性能显著下降，需要开发能适应环境变化的算法。

Method: 提出FDSW-UCB双视角算法，集成基于折扣的长期视角和基于滑动窗口的短期视角，并使用MovieLens-1M和Open Bandit数据集构建半合成仿真平台测试算法适应性。

Result: 实验表明滑动窗口机制稳健，而广泛使用的折扣方法存在基本学习失败导致线性遗憾。FDSW-UCB采用乐观聚合策略时在动态环境中表现最佳。

Conclusion: 集成策略本身是成功的关键因素，FDSW-UCB在非平稳环境中实现了优越性能。

Abstract: Many real-world bandit problems involve non-stationary reward distributions, where the optimal decision may shift due to evolving environments. However, the performance of some typical Multi-Armed Bandit (MAB) models such as Upper Confidence Bound (UCB) algorithms degrades significantly in non-stationary environments where reward distributions change over time. To address this limitation, this paper introduces and evaluates FDSW-UCB, a novel dual-view algorithm that integrates a discount-based long-term perspective with a sliding-window-based short-term view. A data-driven semi-synthetic simulation platform, built upon the MovieLens-1M and Open Bandit datasets, is developed to test algorithm adaptability under abrupt and gradual drift scenarios. Experimental results demonstrate that a well-configured sliding-window mechanism (SW-UCB) is robust, while the widely used discounting method (D-UCB) suffers from a fundamental learning failure, leading to linear regret. Crucially, the proposed FDSW-UCB, when employing an optimistic aggregation strategy, achieves superior performance in dynamic settings, highlighting that the ensemble strategy itself is a decisive factor for success.

</details>


### [274] [Local Entropy Search over Descent Sequences for Bayesian Optimization](https://arxiv.org/abs/2511.19241)
*David Stenger,Armin Lindicke,Alexander von Rohr,Sebastian Trimpe*

Main category: cs.LG

TL;DR: 提出局部熵搜索（LES）方法，通过贝叶斯优化针对迭代优化器的下降序列可达解进行搜索，在复杂设计空间中实现高效的局部优化。


<details>
  <summary>Details</summary>
Motivation: 在大型复杂设计空间中寻找全局最优解通常不可行且不必要，而局部优化方法如梯度下降可以迭代优化初始设计的邻域。

Method: LES算法通过优化器传播目标函数的后验信念，生成下降序列的概率分布，然后通过分析熵计算和蒙特卡洛采样最大化与该分布的互信息来选择下一个评估点。

Result: 在高复杂度合成目标和基准问题上的实验结果表明，LES相比现有的局部和全局贝叶斯优化方法具有更强的样本效率。

Conclusion: 局部熵搜索是一种有效的贝叶斯优化范式，能够针对迭代优化器的下降序列进行高效搜索，在复杂优化问题中表现出色。

Abstract: Searching large and complex design spaces for a global optimum can be infeasible and unnecessary. A practical alternative is to iteratively refine the neighborhood of an initial design using local optimization methods such as gradient descent. We propose local entropy search (LES), a Bayesian optimization paradigm that explicitly targets the solutions reachable by the descent sequences of iterative optimizers. The algorithm propagates the posterior belief over the objective through the optimizer, resulting in a probability distribution over descent sequences. It then selects the next evaluation by maximizing mutual information with that distribution, using a combination of analytic entropy calculations and Monte-Carlo sampling of descent sequences. Empirical results on high-complexity synthetic objectives and benchmark problems show that LES achieves strong sample efficiency compared to existing local and global Bayesian optimization methods.

</details>


### [275] [MAESTRO: Multi-Agent Environment Shaping through Task and Reward Optimization](https://arxiv.org/abs/2511.19253)
*Boyuan Wu*

Main category: cs.LG

TL;DR: MAESTRO框架利用LLM作为离线训练架构师，通过生成语义课程和自动奖励函数来优化多智能体强化学习，在不增加推理成本的情况下提升性能


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习中密集奖励函数设计和课程构建的瓶颈，避免现有方法依赖固定启发式或在线使用LLM的高成本问题

Method: 提出MAESTRO框架，包含语义课程生成器和自动奖励合成器，使用LLM离线生成多样化交通场景和可执行Python奖励函数，指导MADDPG算法

Result: 在16个交叉路口的大规模交通信号控制实验中，结合LLM生成的课程和奖励塑形，平均回报提高4.0%，风险调整后性能提升2.2%

Conclusion: LLM可作为合作多智能体强化学习训练的有效高层设计者，在不增加部署成本的情况下显著提升学习性能

Abstract: Cooperative Multi-Agent Reinforcement Learning (MARL) faces two major design bottlenecks: crafting dense reward functions and constructing curricula that avoid local optima in high-dimensional, non-stationary environments. Existing approaches rely on fixed heuristics or use Large Language Models (LLMs) directly in the control loop, which is costly and unsuitable for real-time systems. We propose MAESTRO (Multi-Agent Environment Shaping through Task and Reward Optimization), a framework that moves the LLM outside the execution loop and uses it as an offline training architect. MAESTRO introduces two generative components: (i) a semantic curriculum generator that creates diverse, performance-driven traffic scenarios, and (ii) an automated reward synthesizer that produces executable Python reward functions adapted to evolving curriculum difficulty. These components guide a standard MARL backbone (MADDPG) without increasing inference cost at deployment. We evaluate MAESTRO on large-scale traffic signal control (Hangzhou, 16 intersections) and conduct controlled ablations. Results show that combining LLM-generated curricula with LLM-generated reward shaping yields improved performance and stability. Across four seeds, the full system achieves +4.0% higher mean return (163.26 vs. 156.93) and 2.2% better risk-adjusted performance (Sharpe 1.53 vs. 0.70) over a strong curriculum baseline. These findings highlight LLMs as effective high-level designers for cooperative MARL training.

</details>


### [276] [A Nutrition Multimodal Photoplethysmography Language Model](https://arxiv.org/abs/2511.19260)
*Kyle Verrier,Achille Nazaret,Joseph Futoma,Andrew C. Miller,Guillermo Sapiro*

Main category: cs.LG

TL;DR: 开发了一种结合可穿戴设备PPG信号和膳食描述的营养光电容积脉搏波语言模型(NPLM)，显著提高了日常热量摄入预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 饥饿和饱腹感动态影响饮食行为和代谢健康，但在日常环境中难以捕捉，需要非侵入性的大规模饮食监测方法。

Method: 整合可穿戴设备的连续光电容积脉搏波(PPG)与膳食描述，将PPG投影到语言模型可解释的嵌入空间，实现生理信号和膳食上下文的联合推理。

Result: 在19340名参与者和110万份膳食-PPG配对数据上训练，模型比纯文本基线提高日常热量摄入预测11%，在去除80%膳食文本时仍保持准确性。独立验证研究(n=140)重现了这些发现。

Conclusion: 将消费者可穿戴设备的生理测量与膳食信息整合，为大规模非侵入性饮食监测提供了价值。

Abstract: Hunger and satiety dynamics shape dietary behaviors and metabolic health, yet remain difficult to capture in everyday settings. We present a Nutrition Photoplethysmography Language Model (NPLM), integrating continuous photoplethysmography (PPG) from wearables with meal descriptions. NPLM projects PPG into embeddings interpretable by language models, enabling joint reasoning over physiology and meal context. Trained on 19,340 participants and 1.1 million meal-PPG pairs, the model improved daily caloric intake prediction by 11% over text-only baselines, with accuracy maintained when 80% of meal text was removed. In an independent validation study (n=140) with controlled dining and detailed meal information, the model replicated these findings. These results demonstrate the value of integrating physiological measurements from consumer wearables with meal information for noninvasive dietary monitoring at scale.

</details>


### [277] [Solar-GECO: Perovskite Solar Cell Property Prediction with Geometric-Aware Co-Attention](https://arxiv.org/abs/2511.19263)
*Lucas Li,Jean-Baptiste Puel,Florence Carton,Dounya Barrit,Jhony H. Giraldo*

Main category: cs.LG

TL;DR: 提出Solar-GECO模型，结合几何图神经网络和语言模型嵌入，通过协同注意力机制预测钙钛矿太阳能电池的功率转换效率及其不确定性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 钙钛矿太阳能电池性能受多层材料复杂相互作用影响，传统实验筛选方法成本高且效率低，现有机器学习模型忽略钙钛矿晶体的几何信息。

Method: 使用几何图神经网络编码钙钛矿吸收层的原子结构，语言模型处理传输层等组件的文本表示，集成协同注意力模块捕获层内依赖和层间相互作用，概率回归头预测PCE及不确定性。

Result: Solar-GECO达到最先进性能，将PCE预测的平均绝对误差从3.066降至2.936，显著优于多个基线模型。

Conclusion: 整合几何和文本信息为PCE预测提供了更强大准确的框架。

Abstract: Perovskite solar cells are promising candidates for next-generation photovoltaics. However, their performance as multi-scale devices is determined by complex interactions between their constituent layers. This creates a vast combinatorial space of possible materials and device architectures, making the conventional experimental-based screening process slow and expensive. Machine learning models try to address this problem, but they only focus on individual material properties or neglect the important geometric information of the perovskite crystal. To address this problem, we propose to predict perovskite solar cell power conversion efficiency with a geometric-aware co-attention (Solar-GECO) model. Solar-GECO combines a geometric graph neural network (GNN) - that directly encodes the atomic structure of the perovskite absorber - with language model embeddings that process the textual strings representing the chemical compounds of the transport layers and other device components. Solar-GECO also integrates a co-attention module to capture intra-layer dependencies and inter-layer interactions, while a probabilistic regression head predicts both power conversion efficiency (PCE) and its associated uncertainty. Solar-GECO achieves state-of-the-art performance, significantly outperforming several baselines, reducing the mean absolute error (MAE) for PCE prediction from 3.066 to 2.936 compared to semantic GNN (the previous state-of-the-art model). Solar-GECO demonstrates that integrating geometric and textual information provides a more powerful and accurate framework for PCE prediction.

</details>


### [278] [Interpreting GFlowNets for Drug Discovery: Extracting Actionable Insights for Medicinal Chemistry](https://arxiv.org/abs/2511.19264)
*Amirtha Varshini A S,Duminda S. Ranasinghe,Hok Hei Tam*

Main category: cs.LG

TL;DR: 提出了SynFlowNet的可解释性框架，通过梯度显著性、稀疏自编码器和基序探针揭示GFlowNet内部决策机制，支持透明可控的分子设计


<details>
  <summary>Details</summary>
Motivation: GFlowNet在分子设计中很有前景，但其内部决策策略不透明，限制了在药物发现中的应用，化学家需要清晰可解释的结构设计理由

Method: 集成三个互补组件：梯度显著性结合反事实扰动识别原子环境影响；稀疏自编码器揭示物理化学性质的潜在因子；基序探针显示功能基团的显式编码

Result: 识别出哪些原子环境影响奖励，结构编辑如何改变分子结果；发现极性、亲脂性、分子大小等物理化学性质的轴对齐潜在因子；证明芳香环和卤素等功能基团在线性可解码的内部嵌入中显式编码

Conclusion: 这些结果揭示了SynFlowNet内部的化学逻辑，为透明可控的分子设计提供了可操作和机制性的洞察

Abstract: Generative Flow Networks, or GFlowNets, offer a promising framework for molecular design, but their internal decision policies remain opaque. This limits adoption in drug discovery, where chemists require clear and interpretable rationales for proposed structures. We present an interpretability framework for SynFlowNet, a GFlowNet trained on documented chemical reactions and purchasable starting materials that generates both molecules and the synthetic routes that produce them. Our approach integrates three complementary components. Gradient based saliency combined with counterfactual perturbations identifies which atomic environments influence reward and how structural edits change molecular outcomes. Sparse autoencoders reveal axis aligned latent factors that correspond to physicochemical properties such as polarity, lipophilicity, and molecular size. Motif probes show that functional groups including aromatic rings and halogens are explicitly encoded and linearly decodable from the internal embeddings. Together, these results expose the chemical logic inside SynFlowNet and provide actionable and mechanistic insight that supports transparent and controllable molecular design.

</details>


### [279] [Unboxing the Black Box: Mechanistic Interpretability for Algorithmic Understanding of Neural Networks](https://arxiv.org/abs/2511.19265)
*Bianka Kowalska,Halina Kwaśnicka*

Main category: cs.LG

TL;DR: 本文提出了一个关于机制可解释性(MI)的统一分类法，分析了MI在可解释人工智能(XAI)领域的地位，并探讨了其通过逆向工程理解神经网络内部计算来支持更科学理解机器学习系统的潜力。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的黑盒特性阻碍了透明可信AI系统的部署，随着AI在社会中的普及，开发能够解释这些系统决策的方法变得日益重要。

Method: 提出了机制可解释性方法的统一分类法，详细分析关键技术，提供具体示例和伪代码，并在更广泛的可解释性背景下对比MI的目标、方法和见解。

Result: 建立了MI方法的系统分类框架，展示了MI如何通过研究神经网络内部计算并将其转化为人类可理解算法来支持对机器学习系统的科学理解。

Conclusion: 机制可解释性具有显著潜力，能够支持对机器学习系统更科学的理解——不仅将模型视为解决任务的工具，也将其作为需要研究和理解的系统，希望吸引新研究人员进入该领域。

Abstract: The black box nature of deep neural networks poses a significant challenge for the deployment of transparent and trustworthy artificial intelligence (AI) systems. With the growing presence of AI in society, it becomes increasingly important to develop methods that can explain and interpret the decisions made by these systems. To address this, mechanistic interpretability (MI) emerged as a promising and distinctive research program within the broader field of explainable artificial intelligence (XAI). MI is the process of studying the inner computations of neural networks and translating them into human-understandable algorithms. It encompasses reverse engineering techniques aimed at uncovering the computational algorithms implemented by neural networks. In this article, we propose a unified taxonomy of MI approaches and provide a detailed analysis of key techniques, illustrated with concrete examples and pseudo-code. We contextualize MI within the broader interpretability landscape, comparing its goals, methods, and insights to other strands of XAI. Additionally, we trace the development of MI as a research area, highlighting its conceptual roots and the accelerating pace of recent work. We argue that MI holds significant potential to support a more scientific understanding of machine learning systems -- treating models not only as tools for solving tasks, but also as systems to be studied and understood. We hope to invite new researchers into the field of mechanistic interpretability.

</details>


### [280] [Leveraging Spatiotemporal Graph Neural Networks for Multi-Store Sales Forecasting](https://arxiv.org/abs/2511.19267)
*Manish Singh,Arpita Dayama*

Main category: cs.LG

TL;DR: 该研究评估了时空图神经网络在多家零售店销售预测中的有效性，相比ARIMA、LSTM和XGBoost基准模型表现更优，通过自适应图建模店铺间依赖关系，实现了最低的预测误差。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索在多店铺零售环境中，通过建模店铺间的相互依赖关系来提升销售预测质量，传统方法往往忽略了这种空间相关性。

Method: 使用45家沃尔玛店铺的周销售数据，构建关系预测框架，通过学习的自适应图建模店铺间依赖关系，采用对数差分销售预测和残差路径重构最终值的方法。

Result: STGNN在所有评估指标（标准化总绝对误差、P90 MAPE和MAPE方差）上均优于基准模型，学习到的邻接矩阵揭示了有意义的店铺功能集群和高影响力节点。

Conclusion: 关系结构显著提升了互联零售环境中的预测质量，确立了STGNN作为多店铺需求预测的稳健建模选择。

Abstract: This work evaluates the effectiveness of spatiotemporal Graph Neural Networks (GNNs) for multi-store retail sales forecasting and compares their performance against ARIMA, LSTM, and XGBoost baselines. Using weekly sales data from 45 Walmart stores, we construct a relational forecasting framework that models inter-store dependencies through a learned adaptive graph. The proposed STGNN predicts log-differenced sales and reconstructs final values through a residual path, enabling stable training and improved generalisation. Experiments show that STGNN achieves the lowest overall forecasting error, outperforming all baselines in Normalised Total Absolute Error, P90 MAPE, and variance of MAPE across stores. Analysis of the learned adjacency matrix reveals meaningful functional store clusters and high-influence nodes that emerge without geographic metadata. These results demonstrate that relational structure significantly improves forecast quality in interconnected retail environments and establishes STGNNs as a robust modelling choice for multi-store demand prediction.

</details>


### [281] [CDLM: Consistency Diffusion Language Models For Faster Sampling](https://arxiv.org/abs/2511.19269)
*Minseo Kim,Chenfeng Xu,Coleman Hooper,Harman Singh,Ben Athiwaratkun,Ce Zhang,Kurt Keutzer,Amir Gholami*

Main category: cs.LG

TL;DR: CDLM通过一致性建模和块级因果注意力掩码，解决了扩散语言模型推理速度慢的问题，实现了3.6-14.5倍的延迟降低。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型虽然提供了并行生成的优势，但由于需要大量细化步骤且无法使用标准KV缓存，导致推理速度缓慢。

Method: CDLM结合一致性建模大幅减少采样步骤，并通过块级因果注意力掩码使模型完全兼容KV缓存。

Result: 在数学和编程任务上，CDLM在保持竞争性准确率的同时，实现了3.6-14.5倍的延迟降低。

Conclusion: CDLM有效解决了扩散语言模型的推理瓶颈，为实际应用提供了可行的加速方案。

Abstract: Diffusion Language Models (DLMs) offer a promising parallel generation paradigm but suffer from slow inference due to numerous refinement steps and the inability to use standard KV caching. We introduce CDLM (Consistency Diffusion Language Models), a training-based acceleration method that simultaneously tackles both bottlenecks. CDLM integrates consistency modeling to drastically reduce the number of required sampling steps by enabling multi-token finalization. Furthermore, we enforce a block-wise causal attention mask during fine-tuning, making the model fully compatible with KV caching. Experiments show CDLM achieves 3.6x-14.5x lower latency while maintaining competitive accuracy on math and coding tasks. The full training and evaluation code is available at https://github.com/SqueezeAILab/CDLM.

</details>


### [282] [Tiny-TSM: Efficiently Training a Lightweight SOTA Time Series Foundation Model](https://arxiv.org/abs/2511.19272)
*Felix Birkel*

Main category: cs.LG

TL;DR: Tiny-TSM是一个小型时间序列基础模型，仅2300万参数，在单个A100 GPU上训练不到一周，通过新的合成数据生成和数据增强管道实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有时间序列基础模型规模过大、训练成本高的问题，开发一个资源高效但性能优越的小型模型。

Method: 使用SynthTS合成数据生成和数据增强管道，引入因果输入归一化方案，采用密集下一个token预测损失训练。

Result: 在中长期预测任务上超越所有评估的时间序列基础模型，短期预测性能与最先进模型相当，训练收敛速度显著提升。

Conclusion: Tiny-TSM证明了小型模型通过高效数据生成和训练策略可以实现与大型工业级模型相媲美的性能，为资源受限环境提供了实用解决方案。

Abstract: We present Tiny-TSM, a time series foundation model characterized by small scale, economical training, and state-of-the-art performance. It comprises 23M total parameters, trained on a single A100 GPU in less than a week using a new synthetic data generation and data augmentation pipeline (SynthTS). Without any neural architecture search, hyperparameter tuning, or scaling up model size, Tiny-TSM achieves state-of-the-art performance on a wide range of time series benchmark datasets, often outperforming much larger models and even matching the performance of much larger, industrial-scale, likely highly tuned foundation models. Specifically, Tiny-TSM outperforms all other time series foundation models we evaluated on medium- and long-term forecasting tasks under MSE loss, while short-term accuracy is still competitive with state-of-the-art models.
  We also introduce a causal input normalization scheme that enables time series models to be trained with dense next-token prediction loss, significantly accelerating convergence speed and reducing training time.
  All experiments were conducted on a single A100 GPU, illustrating the practicality of the proposed approach in a resource-constrained setting.

</details>


### [283] [Scalable Bayesian Network Structure Learning Using Tsetlin Machine to Constrain the Search Space](https://arxiv.org/abs/2511.19273)
*Kunal Dumbre,Lei Jiao,Ole-Christoffer Granmo*

Main category: cs.LG

TL;DR: 提出了一种基于Tsetlin Machine的新方法，通过选择最重要的文字进行条件独立性测试，显著降低了PC算法在贝叶斯网络结构学习中的计算复杂度。


<details>
  <summary>Details</summary>
Motivation: PC算法在因果推断中广泛应用，但随着数据集规模增大，其时间复杂度显著增加，限制了在大规模实际问题中的应用。

Method: 利用Tsetlin Machine提取最重要的文字，仅对这些选定的文字进行条件独立性测试，而不是对全部变量集进行测试。

Result: 在bnlearn存储库的分类数据集（如Munin1、Hepar2）上的评估表明，该方法显著降低了计算复杂度，同时在因果发现中保持了有竞争力的准确性。

Conclusion: 基于TM的方法为传统PC算法实现提供了可行的替代方案，在提高效率的同时不牺牲性能。

Abstract: The PC algorithm is a widely used method in causal inference for learning the structure of Bayesian networks. Despite its popularity, the PC algorithm suffers from significant time complexity, particularly as the size of the dataset increases, which limits its applicability in large-scale real-world problems. In this study, we propose a novel approach that utilises the Tsetlin Machine (TM) to construct Bayesian structures more efficiently. Our method leverages the most significant literals extracted from the TM and performs conditional independence (CI) tests on these selected literals instead of the full set of variables, resulting in a considerable reduction in computational time. We implemented our approach and compared it with various state-of-the-art methods. Our evaluation includes categorical datasets from the bnlearn repository, such as Munin1, Hepar2. The findings indicate that the proposed TM-based method not only reduces computational complexity but also maintains competitive accuracy in causal discovery, making it a viable alternative to traditional PC algorithm implementations by offering improved efficiency without compromising performance.

</details>


### [284] [Closing Gaps in Emissions Monitoring with Climate TRACE](https://arxiv.org/abs/2511.19277)
*Brittany V. Lancellotti,Jordan M. Malof,Aaron Davitt,Gavin McCormick,Shelby Anderson,Pol Carbó-Mestre,Gary Collins,Verity Crane,Zoheyr Doctor,George Ebri,Kevin Foster,Trey M. Gowdy,Michael Guzzardi,John Heal,Heather Hunter,David Kroodsma,Khandekar Mahammad Galib,Paul J. Markakis,Gavin McDonald,Daniel P. Moore,Eric D. Nguyen,Sabina Parvu,Michael Pekala,Christine D. Piatko,Amy Piscopo,Mark Powell,Krsna Raniga,Elizabeth P. Reilly,Michael Robinette,Ishan Saraswat,Patrick Sicurello,Isabella Söldner-Rembold,Raymond Song,Charlotte Underwood,Kyle Bradbury*

Main category: cs.LG

TL;DR: Climate TRACE是一个开放获取平台，提供全球温室气体排放估算，具有更高的细节、覆盖范围和时效性，支持数据驱动的气候行动。


<details>
  <summary>Details</summary>
Motivation: 现有排放数据集缺乏准确性、全球覆盖、高时空分辨率和频繁更新等关键特征，限制了其在监测和减缓规划中的实用性。

Method: 综合现有排放数据，优先考虑准确性、覆盖范围和分辨率，并使用特定行业的估算方法来填补数据空白。

Result: 首次提供全球范围内所有人为排放部门的单个排放源（如单个发电厂）的全面排放估算，数据从2021年1月1日至今，每月更新，报告延迟两个月。

Conclusion: Climate TRACE代表了排放核算和减缓的重大突破，支持在决策层面进行数据驱动的气候行动。

Abstract: Global greenhouse gas emissions estimates are essential for monitoring and mitigation planning. Yet most datasets lack one or more characteristics that enhance their actionability, such as accuracy, global coverage, high spatial and temporal resolution, and frequent updates. To address these gaps, we present Climate TRACE (climatetrace.org), an open-access platform delivering global emissions estimates with enhanced detail, coverage, and timeliness. Climate TRACE synthesizes existing emissions data, prioritizing accuracy, coverage, and resolution, and fills gaps using sector-specific estimation approaches. The dataset is the first to provide globally comprehensive emissions estimates for individual sources (e.g., individual power plants) for all anthropogenic emitting sectors. The dataset spans January 1, 2021, to the present, with a two-month reporting lag and monthly updates. The open-access platform enables non-technical audiences to engage with detailed emissions datasets for most subnational governments worldwide. Climate TRACE supports data-driven climate action at scales where decisions are made, representing a major breakthrough for emissions accounting and mitigation.

</details>


### [285] [MapFormer: Self-Supervised Learning of Cognitive Maps with Input-Dependent Positional Embeddings](https://arxiv.org/abs/2511.19279)
*Victor Rambaud,Salvador Mascarenhas,Yair Lakretz*

Main category: cs.LG

TL;DR: MapFormers是基于Transformer的新架构，能够从观测数据中学习认知地图并并行执行路径整合，通过输入依赖的位置编码实现结构-内容解耦，在OOD泛化方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 认知地图能够编码实体间的抽象关系，使人类和动物具有灵活适应新情境的能力，而当前AI系统缺乏这种强大的OOD泛化能力。

Method: 开发了两种MapFormers变体，通过更新Transformer中的位置编码为输入依赖矩阵，统一绝对和相对位置编码来分别建模情景记忆和工作记忆。

Result: 在包括经典2D导航任务在内的多个任务中，MapFormers能够学习底层空间的认知地图，并在OOD泛化（如更长序列）方面达到近乎完美的性能，优于现有架构。

Conclusion: 结果表明学习认知地图的模型具有优越性，结构-内容解耦的结构偏置在Transformer中通过输入依赖位置编码实现，MapFormers在神经科学和AI中具有广泛应用前景。

Abstract: A cognitive map is an internal model which encodes the abstract relationships among entities in the world, giving humans and animals the flexibility to adapt to new situations, with a strong out-of-distribution (OOD) generalization that current AI systems still do not possess. To bridge this gap, we introduce MapFormers, new architectures based on Transformer models, which can learn cognitive maps from observational data and perform path integration in parallel, in a self-supervised manner. Cognitive maps are learned in the model by disentangling structural relationships in the inputs from their specific content, a property that can be achieved naturally by updating the positional encoding in Transformers with input-dependent matrices. We developed two variants of MapFormers that unify absolute and relative positional encoding to model episodic (EM) and working memory (WM), respectively. We tested MapFormers on several tasks, including a classic 2D navigation task, showing that our models can learn a cognitive map of the underlying space and generalize OOD (e.g., to longer sequences) with near-perfect performance, unlike current architectures. Together, these results demonstrate the superiority of models designed to learn a cognitive map, and the importance of introducing a structural bias for structure-content disentanglement, which can be achieved in Transformers with input-dependent positional encoding. MapFormers have broad applications in both neuroscience and AI, by explaining the neural mechanisms giving rise to cognitive maps, while allowing these relation models to be learned at scale.

</details>


### [286] [Open-weight genome language model safeguards: Assessing robustness via adversarial fine-tuning](https://arxiv.org/abs/2511.19299)
*James R. M. Black,Moritz S. Hanke,Aaron Maiwald,Tina Hernandez-Boussard,Oliver M. Crook,Jaspreet Pannu*

Main category: cs.LG

TL;DR: 研究表明，通过微调可以绕过基因组语言模型的数据排除安全措施，恢复对有害人类感染病毒的预测能力，这凸显了需要更完善的安全框架。


<details>
  <summary>Details</summary>
Motivation: 基因组语言模型在生物数据上的应用引发了滥用担忧，特别是生成人类感染病毒基因组的能力。当前主要通过在预训练数据中过滤病毒序列来降低风险，但这种方法对可微调的开源模型是否有效尚不清楚。

Method: 评估了最先进的基因组语言模型Evo 2，使用110种有害人类感染病毒的序列进行微调，测试其恢复滥用相关预测能力的效果。

Result: 微调后的模型在未见病毒序列上表现出较低的困惑度，能够识别SARS-CoV-2的免疫逃逸变异（AUROC为0.6），尽管在微调过程中未接触过SARS-CoV-2序列。

Conclusion: 数据排除措施可能被微调方法绕过，需要为基因组语言模型建立更完善的安全框架，并进一步研究评估和缓解措施以确保安全部署。

Abstract: Novel deep learning architectures are increasingly being applied to biological data, including genetic sequences. These models, referred to as genomic language mod- els (gLMs), have demonstrated impressive predictive and generative capabilities, raising concerns that such models may also enable misuse, for instance via the generation of genomes for human-infecting viruses. These concerns have catalyzed calls for risk mitigation measures. The de facto mitigation of choice is filtering of pretraining data (i.e., removing viral genomic sequences from training datasets) in order to limit gLM performance on virus-related tasks. However, it is not currently known how robust this approach is for securing open-source models that can be fine-tuned using sensitive pathogen data. Here, we evaluate a state-of-the-art gLM, Evo 2, and perform fine-tuning using sequences from 110 harmful human-infecting viruses to assess the rescue of misuse-relevant predictive capabilities. The fine- tuned model exhibited reduced perplexity on unseen viral sequences relative to 1) the pretrained model and 2) a version fine-tuned on bacteriophage sequences. The model fine-tuned on human-infecting viruses also identified immune escape variants from SARS-CoV-2 (achieving an AUROC of 0.6), despite having no expo- sure to SARS-CoV-2 sequences during fine-tuning. This work demonstrates that data exclusion might be circumvented by fine-tuning approaches that can, to some degree, rescue misuse-relevant capabilities of gLMs. We highlight the need for safety frameworks for gLMs and outline further work needed on evaluations and mitigation measures to enable the safe deployment of gLMs.

</details>


### [287] [Understanding the Staged Dynamics of Transformers in Learning Latent Structure](https://arxiv.org/abs/2511.19328)
*Rohan Saha,Farzane Aminmansour,Alona Fyshe*

Main category: cs.LG

TL;DR: 本文研究了Transformer学习潜在结构的动态过程，发现在Alchemy基准测试中，模型分阶段学习：先学习粗粒度规则，再学习完整的潜在结构，并发现组合规则比分解复杂示例更容易。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer如何从上下文中发现潜在结构，特别是学习不同结构组件的动态过程，目前仍不清楚。

Method: 在Alchemy基准上训练小型仅解码器Transformer，研究三个任务变体：从部分上下文推断缺失规则、组合简单规则解决多步序列、分解复杂多步示例推断中间步骤。

Result: 模型以离散阶段获取能力，先学习粗粒度规则，再学习完整潜在结构。存在关键不对称性：模型能稳健组合基本规则，但难以分解复杂示例来发现基本规则。

Conclusion: 这些发现为理解Transformer模型如何学习潜在结构提供了新见解，展示了训练过程中这些能力如何逐步演化。

Abstract: While transformers can discover latent structure from context, the dynamics of how they acquire different components of the latent structure remain poorly understood. In this work, we use the Alchemy benchmark, to investigate the dynamics of latent structure learning. We train a small decoder-only transformer on three task variants: 1) inferring missing rules from partial contextual information, 2) composing simple rules to solve multi-step sequences, and 3) decomposing complex multi-step examples to infer intermediate steps. By factorizing each task into interpretable events, we show that the model acquires capabilities in discrete stages, first learning the coarse grained rules, before learning the complete latent structure. We also identify a crucial asymmetry, where the model can compose fundamental rules robustly, but struggles to decompose complex examples to discover the fundamental rules. These findings offer new insights into understanding how a transformer model learns latent structures, providing a granular view of how these capabilities evolve during training.

</details>


### [288] [Targeted Manipulation: Slope-Based Attacks on Financial Time-Series Data](https://arxiv.org/abs/2511.19330)
*Dominik Luszczynski*

Main category: cs.LG

TL;DR: 本文提出了两种新的基于斜率的对抗攻击方法，用于操纵N-HiTS模型的股票预测趋势，能够绕过标准安全机制，并将斜率加倍。同时开发了GAN架构生成逼真合成数据，并展示了模型推理库中的恶意软件注入风险。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击在图像领域已深入研究，但在时间序列领域特别是金融预测数据方面研究较少，需要填补这一空白并提升整个机器学习管道的安全性。

Method: 提出了两种新的基于斜率的攻击方法：通用斜率攻击和最小二乘斜率攻击，用于操纵N-HiTS模型的预测趋势。还将这些方法整合到GAN架构中生成合成数据，并设计了恶意软件来注入对抗攻击。

Result: 新攻击方法能够将N-HiTS预测的斜率加倍，绕过4层CNN判别器，将其特异性降至28%，准确率降至57%。成功生成了能够欺骗模型的逼真合成数据。

Conclusion: 机器学习安全研究不应只关注模型本身的安全性，还需要保护整个处理管道，包括推理库等环节的安全防护。

Abstract: A common method of attacking deep learning models is through adversarial attacks, which occur when an attacker specifically modifies the input of a model to produce an incorrect result. Adversarial attacks have been deeply investigated in the image domain; however, there is less research in the time-series domain and very little for forecasting financial data. To address these concerns, this study aims to build upon previous research on adversarial attacks for time-series data by introducing two new slope-based methods aimed to alter the trends of the predicted stock forecast generated by an N-HiTS model. Compared to the normal N-HiTS predictions, the two new slope-based methods, the General Slope Attack and Least-Squares Slope Attack, can manipulate N-HiTS predictions by doubling the slope. These new slope attacks can bypass standard security mechanisms, such as a discriminator that filters real and perturbed inputs, reducing a 4-layered CNN's specificity to 28% and accuracy to 57%. Furthermore, the slope based methods were incorporated into a GAN architecture as a means of generating realistic synthetic data, while simultaneously fooling the model. Finally, this paper also proposes a sample malware designed to inject an adversarial attack in the model inference library, proving that ML-security research should not only focus on making the model safe, but also securing the entire pipeline.

</details>


### [289] [Annotation-Free Class-Incremental Learning](https://arxiv.org/abs/2511.19344)
*Hari Chandana Kuchibhotla,K S Ananth,Vineeth N Balasubramanian*

Main category: cs.LG

TL;DR: 本文提出了注释自由类增量学习（AFCIL）这一更现实的持续学习范式，并开发了CrossWorld CL框架，通过利用外部世界知识作为稳定辅助源，在无标注数据流中实现有效的类增量学习。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法大多假设在整个学习过程中都有标注数据可用，但现实场景中数据通常是顺序到达且无标注的。本文旨在解决在标签缺失、任务随时间增量出现的情况下，系统能否适应的根本问题。

Method: 提出了CrossWorld CL框架：为每个下游类别检索语义相关的ImageNet类别，通过跨域对齐策略映射下游和ImageNet特征，并引入新颖的重放策略，使模型能够在无标注情况下发现语义结构并保持先前知识。

Result: 在四个数据集上的实验表明，CrossWorld-CL超越了CLIP基线和现有的持续学习及无标注学习方法，证明了世界知识对注释自由持续学习的益处。

Conclusion: 世界知识可以作为注释自由持续学习的有效辅助源，CrossWorld CL框架在无监督类增量学习场景中表现出色，为解决现实世界持续学习挑战提供了可行方案。

Abstract: Despite significant progress in continual learning ranging from architectural novelty to clever strategies for mitigating catastrophic forgetting most existing methods rest on a strong but unrealistic assumption the availability of labeled data throughout the learning process. In real-world scenarios, however, data often arrives sequentially and without annotations, rendering conventional approaches impractical. In this work, we revisit the fundamental assumptions of continual learning and ask: Can current systems adapt when labels are absent and tasks emerge incrementally over time? To this end, we introduce Annotation-Free Class-Incremental Learning (AFCIL), a more realistic and challenging paradigm where unlabeled data arrives continuously, and the learner must incrementally acquire new classes without any supervision. To enable effective learning under AFCIL, we propose CrossWorld CL, a Cross Domain World Guided Continual Learning framework that incorporates external world knowledge as a stable auxiliary source. The method retrieves semantically related ImageNet classes for each downstream category, maps downstream and ImageNet features through a cross domain alignment strategy and finally introduce a novel replay strategy. This design lets the model uncover semantic structure without annotations while keeping earlier knowledge intact. Across four datasets, CrossWorld-CL surpasses CLIP baselines and existing continual and unlabeled learning methods, underscoring the benefit of world knowledge for annotation free continual learning.

</details>


### [290] [Scalable Parameter-Light Spectral Method for Clustering Short Text Embeddings with a Cohesion-Based Evaluation Metric](https://arxiv.org/abs/2511.19350)
*Nikita Neveditsin,Pawan Lingras,Vijay Mago*

Main category: cs.LG

TL;DR: 提出了一种可扩展的光谱方法，用于自动估计短文本嵌入的聚类数量，并引入了Cohesion Ratio作为无监督聚类质量评估指标。


<details>
  <summary>Details</summary>
Motivation: 短文本嵌入聚类是自然语言处理的基础任务，但由于需要预先指定聚类数量而具有挑战性。现有方法需要人工设定参数，限制了实际应用。

Method: 使用余弦相似度构建拉普拉斯特征谱，通过自适应采样策略直接从特征谱结构估计聚类数量。提出了Cohesion Ratio指标来量化类内相似度与全局相似度背景的差异。

Result: 在六个短文本数据集和四个现代嵌入模型上的实验表明，使用该方法指导的K-Means和HAC算法显著优于HDBSCAN、OPTICS和Leiden等参数较少的方法。Cohesion Ratio与外部指标如标准化互信息和同质性高度相关。

Conclusion: 该方法为短文本数据的无监督组织和评估提供了实用的光谱估计器和评估指标，具有很好的可扩展性和可靠性。

Abstract: Clustering short text embeddings is a foundational task in natural language processing, yet remains challenging due to the need to specify the number of clusters in advance. We introduce a scalable spectral method that estimates the number of clusters directly from the structure of the Laplacian eigenspectrum, constructed using cosine similarities and guided by an adaptive sampling strategy. This sampling approach enables our estimator to efficiently scale to large datasets without sacrificing reliability. To support intrinsic evaluation of cluster quality without ground-truth labels, we propose the Cohesion Ratio, a simple and interpretable evaluation metric that quantifies how much intra-cluster similarity exceeds the global similarity background. It has an information-theoretic motivation inspired by mutual information, and in our experiments it correlates closely with extrinsic measures such as normalized mutual information and homogeneity. Extensive experiments on six short-text datasets and four modern embedding models show that standard algorithms like K-Means and HAC, when guided by our estimator, significantly outperform popular parameter-light methods such as HDBSCAN, OPTICS, and Leiden. These results demonstrate the practical value of our spectral estimator and Cohesion Ratio for unsupervised organization and evaluation of short text data. Implementation of our estimator of k and Cohesion Ratio, along with code for reproducing the experiments, is available at https://anonymous.4open.science/r/towards_clustering-0C2E.

</details>


### [291] [Leveraging LLMs for reward function design in reinforcement learning control tasks](https://arxiv.org/abs/2511.19355)
*Franklin Cardenoso,Wouter Caarls*

Main category: cs.LG

TL;DR: LEARN-Opt是一个基于大语言模型的完全自主、模型无关的奖励函数优化框架，无需预定义指标和环境源代码，仅通过系统描述和任务目标就能自动生成、执行和评估奖励函数候选方案。


<details>
  <summary>Details</summary>
Motivation: 强化学习中设计有效奖励函数是一个重大瓶颈，需要大量人工专业知识且耗时。现有方法通常需要预定义评估指标、人工反馈或环境源代码，限制了自动化程度。

Method: 提出LEARN-Opt框架，能够直接从系统描述和任务目标自主推导性能指标，实现无监督的奖励函数评估和选择。采用多轮运行方法寻找最佳候选方案。

Result: 实验表明LEARN-Opt性能与最先进方法（如EUREKA）相当或更好，且需要更少先验知识。能够利用低成本LLM找到与大型模型相当甚至更好的高性能候选方案。

Conclusion: LEARN-Opt有潜力在不依赖任何人工定义指标的情况下生成高质量奖励函数，减少工程开销并增强泛化能力。

Abstract: The challenge of designing effective reward functions in reinforcement learning (RL) represents a significant bottleneck, often requiring extensive human expertise and being time-consuming. Previous work and recent advancements in large language models (LLMs) have demonstrated their potential for automating the generation of reward functions. However, existing methodologies often require preliminary evaluation metrics, human-engineered feedback for the refinement process, or the use of environmental source code as context. To address these limitations, this paper introduces LEARN-Opt (LLM-based Evaluator and Analyzer for Reward functioN Optimization). This LLM-based, fully autonomous, and model-agnostic framework eliminates the need for preliminary metrics and environmental source code as context to generate, execute, and evaluate reward function candidates from textual descriptions of systems and task objectives. LEARN-Opt's main contribution lies in its ability to autonomously derive performance metrics directly from the system description and the task objective, enabling unsupervised evaluation and selection of reward functions. Our experiments indicate that LEARN-Opt achieves performance comparable to or better to that of state-of-the-art methods, such as EUREKA, while requiring less prior knowledge. We find that automated reward design is a high-variance problem, where the average-case candidate fails, requiring a multi-run approach to find the best candidates. Finally, we show that LEARN-Opt can unlock the potential of low-cost LLMs to find high-performing candidates that are comparable to, or even better than, those of larger models. This demonstrated performance affirms its potential to generate high-quality reward functions without requiring any preliminary human-defined metrics, thereby reducing engineering overhead and enhancing generalizability.

</details>


### [292] [Enhancing Conformal Prediction via Class Similarity](https://arxiv.org/abs/2511.19359)
*Ariel Fargion,Lahav Dabah,Tom Tirer*

Main category: cs.LG

TL;DR: 本文提出了一种基于类别相似性的共形预测增强方法，通过惩罚组外错误和利用类别相似性来减少预测集大小，同时保证预测集包含真实标签的概率。


<details>
  <summary>Details</summary>
Motivation: 在类别可被划分为语义组的情况下，用户不仅需要平均预测集大小小的共形预测方法，还需要预测集包含的语义不同组数量少。

Method: 提出在共形预测评分函数中添加惩罚组外错误的项，并开发模型特定的变体，无需人工语义划分即可利用类别相似性。

Result: 理论分析和广泛实验表明，该方法能持续提升共形预测方法在多个数据集和模型上的性能，减少预测集大小。

Conclusion: 基于类别相似性的方法是一种广泛适用的工具，可增强任何共形预测方法在任何数据集上的表现。

Abstract: Conformal Prediction (CP) has emerged as a powerful statistical framework for high-stakes classification applications. Instead of predicting a single class, CP generates a prediction set, guaranteed to include the true label with a pre-specified probability. The performance of different CP methods is typically assessed by their average prediction set size. In setups where the classes can be partitioned into semantic groups, e.g., diseases that require similar treatment, users can benefit from prediction sets that are not only small on average, but also contain a small number of semantically different groups. This paper begins by addressing this problem and ultimately offers a widely applicable tool for boosting any CP method on any dataset. First, given a class partition, we propose augmenting the CP score function with a term that penalizes predictions with out-of-group errors. We theoretically analyze this strategy and prove its advantages for group-related metrics. Surprisingly, we show mathematically that, for common class partitions, it can also reduce the average set size of any CP score function. Our analysis reveals the class similarity factors behind this improvement and motivates us to propose a model-specific variant, which does not require any human semantic partition and can further reduce the prediction set size. Finally, we present an extensive empirical study, encompassing prominent CP methods, multiple models, and several datasets, which demonstrates that our class-similarity-based approach consistently enhances CP methods.

</details>


### [293] [LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems](https://arxiv.org/abs/2511.19368)
*Tianyang Duan,Zongyuan Zhang,Zheng Lin,Songxiao Guo,Xiuxian Guan,Guangyu Wu,Zihan Fang,Haotian Meng,Xia Du,Ji-Zhe Zhou,Heming Cui,Jun Luo,Yue Gao*

Main category: cs.LG

TL;DR: RELED是一个可扩展的多智能体强化学习框架，通过整合LLM驱动的专家演示和自主智能体探索来解决MARL中的非平稳性问题，提升训练稳定性和策略收敛性。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习中由于智能体策略同步更新导致的严重非平稳性问题，这种非平稳性会导致训练不稳定和策略收敛差，特别是在智能体数量增加时。

Method: RELED框架包含两个核心模块：1）平稳性感知专家演示模块，利用理论非平稳性边界提升LLM生成的专家轨迹质量；2）混合专家-智能体策略优化模块，自适应平衡从专家生成和智能体生成轨迹的学习。

Result: 基于OpenStreetMap的真实城市网络实验表明，RELED相比最先进的MARL方法取得了更优越的性能。

Conclusion: RELED通过整合LLM驱动的专家演示和自主探索，有效解决了MARL中的非平稳性问题，实现了更好的训练稳定性和策略收敛性能。

Abstract: Multi-agent reinforcement learning (MARL) has been increasingly adopted in many real-world applications. While MARL enables decentralized deployment on resource-constrained edge devices, it suffers from severe non-stationarity due to the synchronous updates of agent policies. This non stationarity results in unstable training and poor policy con vergence, especially as the number of agents increases. In this paper, we propose RELED, a scalable MARL framework that integrates large language model (LLM)-driven expert demonstrations with autonomous agent exploration. RELED incorporates a Stationarity-Aware Expert Demonstration module, which leverages theoretical non-stationarity bounds to enhance the quality of LLM-generated expert trajectories, thus providing high reward and training-stable samples for each agent. Moreover, a Hybrid Expert-Agent Policy Optimization module adaptively balances each agent's learning from both expert-generated and agent-generated trajectories, accelerating policy convergence and improving generalization. Extensive experiments with real city networks based on OpenStreetMap demonstrate that RELED achieves superior performance compared to state-of-the-art MARL methods.

</details>


### [294] [Efficiency vs. Fidelity: A Comparative Analysis of Diffusion Probabilistic Models and Flow Matching on Low-Resource Hardware](https://arxiv.org/abs/2511.19379)
*Srishti Gupta,Yashasvee Taiwade*

Main category: cs.LG

TL;DR: Flow Matching在低资源硬件上显著优于DDPM，具有更高效的推理性能和更直接的传输路径，适合实时生成任务


<details>
  <summary>Details</summary>
Motivation: DDPM在推理时计算开销大，需要大量迭代步骤，限制了其在资源受限环境中的部署。本研究旨在比较DDPM与新兴的Flow Matching范式在效率和几何特性上的差异

Method: 在共享的Time-Conditioned U-Net骨干网络上实现两种框架，使用MNIST数据集进行对比分析，包括几何路径分析和数值敏感性分析

Result: Flow Matching学习到近乎最优的整流传输路径（曲率≈1.02），而DDPM路径保持随机和曲折（曲率≈3.45）。在N=10次函数评估时，Flow Matching保持高质量生成，而DDPM崩溃

Conclusion: Flow Matching是资源受限实时生成任务的更优算法选择，其学习到的向量场足够线性，可以使用轻量级欧拉求解器

Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have established a new state-of-the-art in generative image synthesis, yet their deployment is hindered by significant computational overhead during inference, often requiring up to 1,000 iterative steps. This study presents a rigorous comparative analysis of DDPMs against the emerging Flow Matching (Rectified Flow) paradigm, specifically isolating their geometric and efficiency properties on low-resource hardware. By implementing both frameworks on a shared Time-Conditioned U-Net backbone using the MNIST dataset, we demonstrate that Flow Matching significantly outperforms Diffusion in efficiency. Our geometric analysis reveals that Flow Matching learns a highly rectified transport path (Curvature $\mathcal{C} \approx 1.02$), which is near-optimal, whereas Diffusion trajectories remain stochastic and tortuous ($\mathcal{C} \approx 3.45$). Furthermore, we establish an ``efficiency frontier'' at $N=10$ function evaluations, where Flow Matching retains high fidelity while Diffusion collapses. Finally, we show via numerical sensitivity analysis that the learned vector field is sufficiently linear to render high-order ODE solvers (Runge-Kutta 4) unnecessary, validating the use of lightweight Euler solvers for edge deployment. \textbf{This work concludes that Flow Matching is the superior algorithmic choice for real-time, resource-constrained generative tasks.}

</details>


### [295] [Predicting partially observable dynamical systems via diffusion models with a multiscale inference scheme](https://arxiv.org/abs/2511.19390)
*Rudy Morel,Francesco Pio Ramunno,Jeff Shen,Alberto Bietti,Kyunghyun Cho,Miles Cranmer,Siavash Golkar,Olexandr Gugnin,Geraud Krawezik,Tanya Marwah,Michael McCabe,Lucas Meyer,Payel Mukhopadhyay,Ruben Ohana,Liam Parker,Helen Qu,François Rozet,K. D. Leka,François Lanusse,David Fouhey,Shirley Ho*

Main category: cs.LG

TL;DR: 提出了一种用于部分可观测、长记忆动态系统概率预测的多尺度推理方案，特别针对太阳动力学等应用场景，解决了标准自回归方法无法有效捕捉长期依赖关系的问题。


<details>
  <summary>Details</summary>
Motivation: 在许多动态系统预测场景中，可用信息只占预测所需信息的一小部分，如太阳物理中只能观测表面而内部过程无法直接测量。标准推理方案无法有效整合过去信息来捕捉长期依赖关系。

Method: 提出多尺度推理方案，生成时间上靠近当前时刻细粒度、远离时刻粗粒度的轨迹，在不增加计算成本的情况下捕捉长期时间依赖关系，并将其集成到扩散模型中。

Result: 该方法显著减少了预测分布的偏差，提高了推演稳定性，在太阳动力学和活动区演化等应用中表现出色。

Conclusion: 多尺度推理方案为部分可观测、长记忆动态系统的概率预测提供了有效解决方案，能够在不增加计算负担的情况下改善长期依赖关系的建模。

Abstract: Conditional diffusion models provide a natural framework for probabilistic prediction of dynamical systems and have been successfully applied to fluid dynamics and weather prediction. However, in many settings, the available information at a given time represents only a small fraction of what is needed to predict future states, either due to measurement uncertainty or because only a small fraction of the state can be observed. This is true for example in solar physics, where we can observe the Sun's surface and atmosphere, but its evolution is driven by internal processes for which we lack direct measurements. In this paper, we tackle the probabilistic prediction of partially observable, long-memory dynamical systems, with applications to solar dynamics and the evolution of active regions. We show that standard inference schemes, such as autoregressive rollouts, fail to capture long-range dependencies in the data, largely because they do not integrate past information effectively. To overcome this, we propose a multiscale inference scheme for diffusion models, tailored to physical processes. Our method generates trajectories that are temporally fine-grained near the present and coarser as we move farther away, which enables capturing long-range temporal dependencies without increasing computational cost. When integrated into a diffusion model, we show that our inference scheme significantly reduces the bias of the predicted distributions and improves rollout stability.

</details>


### [296] [Learning Robust Social Strategies with Large Language Models](https://arxiv.org/abs/2511.19405)
*Dereck Piche,Mohammed Muqeeth,Milad Aghajohari,Juan Duque,Michael Noukhovitch,Aaron Courville*

Main category: cs.LG

TL;DR: 本文研究了多智能体强化学习在大型语言模型中的应用，发现标准RL训练会导致智能体发展出机会主义行为，即使面对先进闭源模型也会进行利用。作者提出了Advantage Alignment算法和组相对基线来促进多智能体合作，并在新的社交困境环境Trust and Split中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI的普及，具有不同甚至冲突目标的智能体将在复杂环境中交互。多智能体交互带来了基本挑战，特别是在社交困境中，智能体的个体激励可能损害集体福利。虽然RL在单智能体场景中对齐LLM很有效，但先前研究表明标准RL在多智能体设置中往往收敛到背叛的自利策略。

Method: 1. 将对手学习感知算法Advantage Alignment适配用于微调LLM，促进多智能体合作和不可利用性；2. 引入组相对基线简化迭代博弈中的优势计算，实现LLM规模的多智能体训练；3. 创建新的社交困境环境Trust and Split，需要自然语言沟通来实现高集体福利。

Result: 在广泛的社交困境中，使用Advantage Alignment学习的策略实现了更高的集体收益，同时保持了对贪婪智能体利用的鲁棒性。即使在LLM智能体中，RL训练也会发展出机会主义行为，能够利用先进的闭源模型。

Conclusion: Advantage Alignment方法能够有效解决多智能体RL中收敛到不良均衡的问题，促进LLM智能体之间的合作，提高集体福利，同时防止被利用。

Abstract: As agentic AI becomes more widespread, agents with distinct and possibly conflicting goals will interact in complex ways. These multi-agent interactions pose a fundamental challenge, particularly in social dilemmas, where agents' individual incentives can undermine collective welfare. While reinforcement learning (RL) has been effective for aligning large language models (LLMs) in the single-agent regime, prior small-network results suggest that standard RL in multi-agent settings often converges to defecting, self-interested policies. We show the same effect in LLMs: despite cooperative priors, RL-trained LLM agents develop opportunistic behavior that can exploit even advanced closed-source models. To address this tendency of RL to converge to poor equilibria, we adapt a recent opponent-learning awareness algorithm, Advantage Alignment, to fine-tune LLMs toward multi-agent cooperation and non-exploitability. We then introduce a group-relative baseline that simplifies advantage computation in iterated games, enabling multi-agent training at LLM scale. We also contribute a novel social dilemma environment, Trust and Split, which requires natural language communication to achieve high collective welfare. Across a wide range of social dilemmas, policies learned with Advantage Alignment achieve higher collective payoffs while remaining robust against exploitation by greedy agents.

</details>


### [297] [UniGame: Turning a Unified Multimodal Model Into Its Own Adversary](https://arxiv.org/abs/2511.19413)
*Zhaolong Su,Wang Lu,Hao Chen,Sharon Li,Jindong Wang*

Main category: cs.LG

TL;DR: UniGame是一个自对抗后训练框架，通过轻量级扰动器在共享标记接口处应用，让生成分支主动挑战脆弱的理解能力，从而解决统一多模态模型中理解与生成之间的不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 统一多模态模型在单一架构中表现出色，但存在根本性不一致：理解偏向紧凑嵌入，而生成偏向重构丰富表示。这种结构权衡导致决策边界错位、跨模态连贯性下降，以及在分布和对抗性变化下脆弱性增加。

Method: 提出UniGame自对抗后训练框架，在共享标记接口应用轻量级扰动器，使生成分支能够主动寻找和挑战脆弱的理解能力，让模型自身成为自己的对手。

Result: UniGame显著提高了一致性（+4.6%），同时在理解（+3.6%）、生成（+0.02）、分布外和对抗鲁棒性（在NaturalBench和AdVQA上分别+4.8%和+6.2%）方面也取得了实质性改进。

Conclusion: 对抗性自博弈是增强未来多模态基础模型连贯性、稳定性和统一能力的通用有效原则。该框架与架构无关，仅引入不到1%的额外参数，并与现有后训练方法互补。

Abstract: Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings, whereas generation favors reconstruction-rich representations. This structural trade-off produces misaligned decision boundaries, degraded cross-modal coherence, and heightened vulnerability under distributional and adversarial shifts. In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models. The official code is available at: https://github.com/AIFrontierLab/UniGame

</details>


### [298] [Flow Map Distillation Without Data](https://arxiv.org/abs/2511.19428)
*Shangyuan Tong,Nanye Ma,Saining Xie,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 提出了一种无需外部数据集的流映射蒸馏方法，仅从先验分布采样，避免了教师-数据不匹配问题，在ImageNet上仅需1步采样就达到SOTA效果


<details>
  <summary>Details</summary>
Motivation: 传统流映射蒸馏需要从外部数据集采样，但静态数据集可能无法完整代表教师的生成能力，导致教师-数据不匹配风险。本文质疑这种数据依赖的必要性

Method: 引入基于原则的框架，从先验分布采样（教师保证遵循的分布），学习预测教师的采样路径并主动纠正自身累积误差以确保高保真度

Result: 超越所有基于数据的对应方法，显著建立新SOTA：从SiT-XL/2+REPA蒸馏，在ImageNet 256x256上FID达1.45，ImageNet 512x512上FID达1.49，均仅需1步采样

Conclusion: 为加速生成模型建立了更稳健的范式，推动无需数据的流映射蒸馏的广泛采用

Abstract: State-of-the-art flow models achieve remarkable quality but require slow, iterative sampling. To accelerate this, flow maps can be distilled from pre-trained teachers, a procedure that conventionally requires sampling from an external dataset. We argue that this data-dependency introduces a fundamental risk of Teacher-Data Mismatch, as a static dataset may provide an incomplete or even misaligned representation of the teacher's full generative capabilities. This leads us to question whether this reliance on data is truly necessary for successful flow map distillation. In this work, we explore a data-free alternative that samples only from the prior distribution, a distribution the teacher is guaranteed to follow by construction, thereby circumventing the mismatch risk entirely. To demonstrate the practical viability of this philosophy, we introduce a principled framework that learns to predict the teacher's sampling path while actively correcting for its own compounding errors to ensure high fidelity. Our approach surpasses all data-based counterparts and establishes a new state-of-the-art by a significant margin. Specifically, distilling from SiT-XL/2+REPA, our method reaches an impressive FID of 1.45 on ImageNet 256x256, and 1.49 on ImageNet 512x512, both with only 1 sampling step. We hope our work establishes a more robust paradigm for accelerating generative models and motivates the broader adoption of flow map distillation without data.

</details>
