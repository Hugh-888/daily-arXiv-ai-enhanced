<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 90]
- [quant-ph](#quant-ph) [Total: 42]
- [gr-qc](#gr-qc) [Total: 18]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [General Fourier Feature Physics-Informed Extreme Learning Machine (GFF-PIELM) for High-Frequency PDEs](https://arxiv.org/abs/2510.12293)
*Fei Ren,Sifan Wang,Pei-Zhi Zhuang,Hai-Sui Yu,He Yang*

Main category: cs.LG

TL;DR: 提出了一种通用的傅里叶特征物理信息极限学习机(GFF-PIELM)，通过集成傅里叶特征映射作为激活函数，解决了传统PIELM在处理高频和变频偏微分方程时的困难。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息极限学习机(PIELM)在求解涉及高频和变频行为的偏微分方程时面临挑战，需要改进以处理这些复杂问题。

Method: 1) 将傅里叶特征映射变体集成到ELM中作为傅里叶基激活函数；2) 为隐藏神经元分配频率系数；3) 开发创新的超参数初始化方法，通过监控ELM输出权重的分布。

Result: 在五个案例研究共十个数值示例中验证了GFF-PIELM的可行性，相比传统PIELM显著提高了预测精度，且不增加训练时间和架构复杂度。

Conclusion: GFF-PIELM成功扩展了PIELM解决高频和变频PDEs的能力，其初始化策略可能启发其他物理信息机器学习框架的进展。

Abstract: Conventional physics-informed extreme learning machine (PIELM) often faces
challenges in solving partial differential equations (PDEs) involving
high-frequency and variable-frequency behaviors. To address these challenges,
we propose a general Fourier feature physics-informed extreme learning machine
(GFF-PIELM). We demonstrate that directly concatenating multiple Fourier
feature mappings (FFMs) and an extreme learning machine (ELM) network makes it
difficult to determine frequency-related hyperparameters. Fortunately, we find
an alternative to establish the GFF-PIELM in three main steps. First, we
integrate a variation of FFM into ELM as the Fourier-based activation function,
so there is still one hidden layer in the GFF-PIELM framework. Second, we
assign a set of frequency coefficients to the hidden neurons, which enables ELM
network to capture diverse frequency components of target solutions. Finally,
we develop an innovative, straightforward initialization method for these
hyperparameters by monitoring the distribution of ELM output weights. GFF-PIELM
not only retains the high accuracy, efficiency, and simplicity of the PIELM
framework but also inherits the ability of FFMs to effectively handle
high-frequency problems. We carry out five case studies with a total of ten
numerical examples to highlight the feasibility and validity of the proposed
GFF-PIELM, involving high frequency, variable frequency, multi-scale behaviour,
irregular boundary and inverse problems. Compared to conventional PIELM, the
GFF-PIELM approach significantly improves predictive accuracy without
additional cost in training time and architecture complexity. Our results
confirm that that PIELM can be extended to solve high-frequency and
variable-frequency PDEs with high accuracy, and our initialization strategy may
further inspire advances in other physics-informed machine learning (PIML)
frameworks.

</details>


### [2] [Think as a Doctor: An Interpretable AI Approach for ICU Mortality Prediction](https://arxiv.org/abs/2510.11745)
*Qingwen Li,Xiaohang Zhao,Xiao Han,Hailiang Huang,Lanjuan Liu*

Main category: cs.LG

TL;DR: ProtoDoctor是一个用于ICU死亡率预测的新框架，通过原型学习实现内在可解释性，整合了临床病程识别、人口异质性和预后意识三个关键决策要素。


<details>
  <summary>Details</summary>
Motivation: ICU死亡率预测需要高准确性和可解释性，现有方法主要关注人口异质性，忽视了临床病程识别和预后意识，而原型学习方法虽然解决了临床病程识别，但未能整合其他要素。

Method: ProtoDoctor包含两个关键创新模块：预后临床病程识别模块（通过原型学习识别临床病程，使用新的正则化机制实现预后意识）和人口异质性识别模块（通过队列特定原型和风险调整建模人口异质性）。

Result: 广泛的实证评估表明ProtoDoctor在预测准确性上优于最先进的基线方法，人工评估进一步证实其解释更具临床意义、可信度更高且在ICU实践中更适用。

Conclusion: ProtoDoctor成功整合了ICU决策实践的三个关键要素，在保持高预测准确性的同时提供了临床上有意义的解释，为ICU死亡率预测提供了更全面的解决方案。

Abstract: Intensive Care Unit (ICU) mortality prediction, which estimates a patient's
mortality status at discharge using EHRs collected early in an ICU admission,
is vital in critical care. For this task, predictive accuracy alone is
insufficient; interpretability is equally essential for building clinical trust
and meeting regulatory standards, a topic that has attracted significant
attention in information system research. Accordingly, an ideal solution should
enable intrinsic interpretability and align its reasoning with three key
elements of the ICU decision-making practices: clinical course identification,
demographic heterogeneity, and prognostication awareness. However, conventional
approaches largely focus on demographic heterogeneity, overlooking clinical
course identification and prognostication awareness. Recent prototype learning
methods address clinical course identification, yet the integration of the
other elements into such frameworks remains underexplored. To address these
gaps, we propose ProtoDoctor, a novel ICU mortality prediction framework that
delivers intrinsic interpretability while integrating all three elements of the
ICU decision-making practices into its reasoning process. Methodologically,
ProtoDoctor features two key innovations: the Prognostic Clinical Course
Identification module and the Demographic Heterogeneity Recognition module. The
former enables the identification of clinical courses via prototype learning
and achieves prognostication awareness using a novel regularization mechanism.
The latter models demographic heterogeneity through cohort-specific prototypes
and risk adjustments. Extensive empirical evaluations demonstrate that
ProtoDoctor outperforms state-of-the-art baselines in predictive accuracy.
Human evaluations further confirm that its interpretations are more clinically
meaningful, trustworthy, and applicable in ICU practice.

</details>


### [3] [GAR: Generative Adversarial Reinforcement Learning for Formal Theorem Proving](https://arxiv.org/abs/2510.11769)
*Ruida Wang,Jiarui Yao,Rui Pan,Shizhe Diao,Tong Zhang*

Main category: cs.LG

TL;DR: 提出GAR框架，通过对抗式训练联合优化问题生成器和求解器，实现隐式课程学习，提升数学定理证明的训练效率和性能


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的数学证明方法依赖固定问题集，训练效率低且难以处理复杂问题

Method: GAR框架：在对抗循环中联合训练问题生成器和求解器，通过隐式课程学习机制使任务难度与证明器能力同步演进

Result: Goedel-Prover-V2-8B和DeepSeek-Prover-V2-7B在MiniF2F-Test上pass@32相对提升4.20%，DeepSeek-Prover-V2在ProofNet-Test上从22.58%提升至25.81%

Conclusion: GAR建立了可验证环境下问题生成与求解协同进化的一般强化学习范式

Abstract: Solving math problems through verifiable languages such as Lean has
significantly impacted both the mathematics and computer science communities.
Current state-of-the-art models are often trained with expensive online
Reinforcement Learning (RL) or expert iteration. However, these approaches rely
on fixed problem sets, which causes inefficient training and limits the model
to tackle complex problems. To overcome these limitations, we propose GAR:
Generative Adversarial Reinforcement learning, a comprehensive RL training
framework that jointly trains the problem composer and solver in an adversarial
loop. GAR introduces an implicit curriculum learning mechanism, which aligns
task difficulty with the prover's evolving capability. It thereby improves the
training efficiency and enables stronger performance of proving advanced
theorems. Experiments show that with GAR training, Goedel-Prover-V2-8B and
DeepSeek-Prover-V2-7B achieve an average relative improvement in pass@32 of
4.20% on MiniF2F-Test benchmark, while DeepSeek-Prover-V2's pass@32 on
ProofNet-Test increases from 22.58% to 25.81%. Beyond formal proving, GAR
establishes a general RL paradigm for co-evolution of problem generation and
solving under verifiable environments.

</details>


### [4] [Combining Euclidean and Hyperbolic Representations for Node-level Anomaly Detection](https://arxiv.org/abs/2510.11827)
*Simone Mungari,Ettore Ritacco,Pietro Sabatino*

Main category: cs.LG

TL;DR: Janus框架通过联合使用欧几里得和双曲图神经网络来检测节点级异常，利用多视图特征和对比学习来识别难以在不同几何空间中协调的异常节点。


<details>
  <summary>Details</summary>
Motivation: 节点级异常检测面临结构模式和特征分布多样性的挑战，在欺诈检测、网络安全和推荐系统等应用中至关重要。现有方法难以捕捉复杂异常模式。

Method: 为每个节点创建两个视图（原始特征和结构特征），分别嵌入到欧几里得和双曲空间，使用多图自编码器框架和对比学习目标来对齐嵌入，识别难以协调的异常节点。

Result: 在四个真实世界数据集上的实验表明，Janus持续优于浅层和深层基线方法，证明结合多种几何表示能有效识别图中微妙复杂的异常。

Conclusion: 结合欧几里得和双曲几何表示为图异常检测提供了鲁棒有效的方法，多几何空间协同能更好地捕捉复杂异常模式。

Abstract: Node-level anomaly detection (NAD) is challenging due to diverse structural
patterns and feature distributions. As such, NAD is a critical task with
several applications which range from fraud detection, cybersecurity, to
recommendation systems. We introduce Janus, a framework that jointly leverages
Euclidean and Hyperbolic Graph Neural Networks to capture complementary aspects
of node representations. Each node is described by two views, composed by the
original features and structural features derived from random walks and
degrees, then embedded into Euclidean and Hyperbolic spaces. A multi
Graph-Autoencoder framework, equipped with a contrastive learning objective as
regularization term, aligns the embeddings across the Euclidean and Hyperbolic
spaces, highlighting nodes whose views are difficult to reconcile and are thus
likely anomalous. Experiments on four real-world datasets show that Janus
consistently outperforms shallow and deep baselines, empirically demonstrating
that combining multiple geometric representations provides a robust and
effective approach for identifying subtle and complex anomalies in graphs.

</details>


### [5] [Schrödinger bridge for generative AI: Soft-constrained formulation and convergence analysis](https://arxiv.org/abs/2510.11829)
*Jin Ma,Ying Tan,Renyuan Xu*

Main category: cs.LG

TL;DR: 本文提出软约束薛定谔桥问题(SCSBP)，通过用惩罚函数替代硬终端约束来解决经典SBP在实践中的不稳定性问题，为生成式AI提供了更灵活和稳健的框架。


<details>
  <summary>Details</summary>
Motivation: 经典薛定谔桥问题(SBP)在生成式AI中具有重要应用，但其硬终端约束在实际实现中（特别是高维或数据稀缺场景）往往导致不稳定性。

Method: 采用软约束薛定谔桥问题(SCSBP)方法，将硬终端约束替换为一般惩罚函数，形成更灵活的McKean-Vlasov型随机控制公式。

Result: 证明了所有惩罚水平下最优解的存在性，并证明了随着惩罚增加，控制和价值函数以线性速率收敛到经典SBP的结果。

Conclusion: 软约束桥不仅提供了首个定量收敛保证，还揭示了惩罚正则化如何实现稳健的生成建模、微调和迁移学习。

Abstract: Generative AI can be framed as the problem of learning a model that maps
simple reference measures into complex data distributions, and it has recently
found a strong connection to the classical theory of the Schr\"odinger bridge
problems (SBPs) due partly to their common nature of interpolating between
prescribed marginals via entropy-regularized stochastic dynamics. However, the
classical SBP enforces hard terminal constraints, which often leads to
instability in practical implementations, especially in high-dimensional or
data-scarce regimes. To address this challenge, we follow the idea of the
so-called soft-constrained Schr\"odinger bridge problem (SCSBP), in which the
terminal constraint is replaced by a general penalty function. This relaxation
leads to a more flexible stochastic control formulation of McKean-Vlasov type.
  We establish the existence of optimal solutions for all penalty levels and
prove that, as the penalty grows, both the controls and value functions
converge to those of the classical SBP at a linear rate. Our analysis builds on
Doob's h-transform representations, the stability results of Schr\"odinger
potentials, Gamma-convergence, and a novel fixed-point argument that couples an
optimization problem over the space of measures with an auxiliary entropic
optimal transport problem. These results not only provide the first
quantitative convergence guarantees for soft-constrained bridges but also shed
light on how penalty regularization enables robust generative modeling,
fine-tuning, and transfer learning.

</details>


### [6] [Z0-Inf: Zeroth Order Approximation for Data Influence](https://arxiv.org/abs/2510.11832)
*Narine Kokhlikyan,Kamalika Chaudhuri,Saeed Mahloujifar*

Main category: cs.LG

TL;DR: 提出了一种高效的零阶近似方法，用于估计训练数据对模型预测行为的影响，仅需中间检查点的损失值和检查点本身，计算成本远低于现有方法。


<details>
  <summary>Details</summary>
Motivation: 理解训练数据如何影响机器学习模型预测行为对分析和改进系统至关重要，但现有方法在大模型上存在精度低或计算成本高的问题。

Method: 使用零阶近似方法，仅依赖训练和测试数据在中间检查点上的损失值以及检查点本身，无需梯度或逆Hessian计算。

Result: 该方法在估计自影响方面达到更高精度，在估计训练-测试影响方面达到相当或改进的精度，计算时间和内存占用显著降低。

Conclusion: 该方法为大规模语言模型提供了可扩展且实用的训练数据分析工具，即使对于不可微损失函数也适用。

Abstract: A critical aspect of analyzing and improving modern machine learning systems
lies in understanding how individual training examples influence a model's
predictive behavior. Estimating this influence enables critical applications,
including data selection and model debugging; in particular, self-influence,
which quantifies the influence of a training point on itself, has found many
uses in data quality assessment and outlier detection. Existing methods for
measuring data influence, however, are often impractical for large models due
to low accuracy or prohibitive computational costs: most approaches either
provide poor approximations or rely on gradients and inverse-Hessian
computations that remain challenging to scale. In this work, we introduce a
highly efficient zeroth-order approximation for estimating the influence of
training data that requires only a fraction of the time and memory footprint of
prior methods. Notably, our method relies solely on loss values of intermediate
checkpoints on the training and test data, along with the checkpoints
themselves, making it broadly applicable even when the loss function of
interest is non-differentiable. Beyond its computational efficiency, our
approach achieves superior accuracy in estimating self-influence and comparable
or improved accuracy in estimating train-test influence for fine-tuned large
language models, enabling scalable and practical analysis of how training data
shapes model behavior.

</details>


### [7] [Don't Walk the Line: Boundary Guidance for Filtered Generation](https://arxiv.org/abs/2510.11834)
*Sarah Ball,Andreas Haupt*

Main category: cs.LG

TL;DR: 提出Boundary Guidance方法，通过强化学习微调生成模型，使其远离安全分类器的决策边界，从而同时提升输出的安全性和实用性。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过微调生成模型来降低被安全分类器过滤的概率，但这往往使模型产生靠近分类器决策边界的样本，增加了误报和漏报。

Method: 使用强化学习微调方法，明确引导生成远离分类器的边界区域。

Result: 在越狱和模糊提示基准测试中，Boundary Guidance通过LLM-as-a-Judge评估，同时提高了输出的安全性和实用性。

Conclusion: 该方法在不同模型规模和奖励设计下都表现出鲁棒性，为生成模型的安全部署提供了有效解决方案。

Abstract: Generative models are increasingly paired with safety classifiers that filter
harmful or undesirable outputs. A common strategy is to fine-tune the generator
to reduce the probability of being filtered, but this can be suboptimal: it
often pushes the model toward producing samples near the classifier's decision
boundary, increasing both false positives and false negatives. We propose
Boundary Guidance, a reinforcement learning fine-tuning method that explicitly
steers generation away from the classifier's margin. On a benchmark of
jailbreak and ambiguous prompts, Boundary Guidance improves both the safety and
the utility of outputs, as judged by LLM-as-a-Judge evaluations. Comprehensive
ablations across model scales and reward designs demonstrate the robustness of
our approach.

</details>


### [8] [WaveletDiff: Multilevel Wavelet Diffusion For Time Series Generation](https://arxiv.org/abs/2510.11839)
*Yu-Hsiang Wang,Olgica Milenkovic*

Main category: cs.LG

TL;DR: WaveletDiff是一个在时间序列生成方面的新框架，通过在离散小波变换系数上训练扩散模型，利用时间序列固有的多分辨率结构，在多个真实数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列生成模型局限于时域或频域，难以复现真实世界时间序列固有的多尺度结构，而大型高质量时间序列数据集稀缺。

Method: 在离散小波变换系数上训练扩散模型，结合每个分解层的专用transformer和跨层注意力机制，通过自适应门控实现时频尺度间的选择性信息交换，并基于Parseval定理加入能量保持约束。

Result: 在能源、金融和神经科学等六个真实数据集上的测试表明，WaveletDiff在五个不同性能指标上始终优于最先进的时域和频域生成方法，其判别分数和Context-FID分数平均比第二好基线小3倍。

Conclusion: WaveletDiff通过在小波域训练扩散模型，有效利用了时间序列的多分辨率特性，在时间序列生成任务上取得了显著优势。

Abstract: Time series are ubiquitous in many applications that involve forecasting,
classification and causal inference tasks, such as healthcare, finance, audio
signal processing and climate sciences. Still, large, high-quality time series
datasets remain scarce. Synthetic generation can address this limitation;
however, current models confined either to the time or frequency domains
struggle to reproduce the inherently multi-scaled structure of real-world time
series. We introduce WaveletDiff, a novel framework that trains diffusion
models directly on wavelet coefficients to exploit the inherent
multi-resolution structure of time series data. The model combines dedicated
transformers for each decomposition level with cross-level attention mechanisms
that enable selective information exchange between temporal and frequency
scales through adaptive gating. It also incorporates energy preservation
constraints for individual levels based on Parseval's theorem to preserve
spectral fidelity throughout the diffusion process. Comprehensive tests across
six real-world datasets from energy, finance, and neuroscience domains
demonstrate that WaveletDiff consistently outperforms state-of-the-art
time-domain and frequency-domain generative methods on both short and long time
series across five diverse performance metrics. For example, WaveletDiff
achieves discriminative scores and Context-FID scores that are $3\times$
smaller on average than the second-best baseline across all datasets.

</details>


### [9] [Balancing Synthetic Data and Replay for Enhancing Task-Specific Capabilities](https://arxiv.org/abs/2510.11842)
*Urs Spiegelhalter,Jörg K. H. Franke,Frank Hutter*

Main category: cs.LG

TL;DR: 该论文研究了在语言模型持续预训练中，如何通过优化回放比例配置来平衡新任务学习与现有知识保留，在计算预算约束下找到最优配置。


<details>
  <summary>Details</summary>
Motivation: 语言模型在适应新任务时面临基本权衡：既要学习新能力，又要避免灾难性遗忘现有知识。现有研究对合成数据生成技术有所探讨，但在计算约束下平衡任务性能和知识保留的最佳回放比例仍不清楚。

Method: 使用bAbI推理任务作为目标，应用合成数据生成技术，系统评估不同总token预算和回放比例配置，分析它们对任务掌握和通用知识保留的影响。

Result: 实验揭示了能够平衡任务特定性能与通用知识保留的最优配置。基于发现，提供了基于计算预算选择回放比例的实证指导原则。

Conclusion: 研究为实践者提供了在显著降低训练成本的同时实现强大任务适应的指导，使语言模型适应新任务时能在计算约束下有效平衡学习与遗忘问题。

Abstract: Adapting language models to new tasks through continued pretraining faces a
fundamental trade-off: models must learn new capabilities while avoiding
catastrophic forgetting of existing knowledge. While prior work has studied
synthetic data generation techniques, the optimal replay ratios for balancing
task performance and knowledge retention under computational constraints remain
poorly understood. We present a comprehensive empirical study investigating the
interplay between replay ratio configuration and computational budget when
adapting language models to new tasks. Using the bAbI reasoning tasks as our
target objective, we apply synthetic data generation and systematically
evaluate different total token budgets and replay ratio configurations. We
analyze their effects on both task mastery and general knowledge retention. Our
experiments reveal an optimal configuration that balances task-specific
performance with general knowledge retention. Based on our findings, we provide
empirically-grounded guidelines for selecting replay ratios based on
computational budget, enabling practitioners to achieve strong task adaptation
with significantly reduced training costs.

</details>


### [10] [Evaluating Open-Source Vision-Language Models for Multimodal Sarcasm Detection](https://arxiv.org/abs/2510.11852)
*Saroj Basnet,Shafkat Farabi,Tharindu Ranasinghe,Diptesh Kanoji,Marcos Zampieri*

Main category: cs.LG

TL;DR: 评估7种先进开源视觉语言模型在多模态讽刺检测和解释生成方面的能力，发现在二元讽刺检测方面表现中等，但在生成高质量解释方面仍需任务特定微调。


<details>
  <summary>Details</summary>
Motivation: 利用开源视觉语言模型理解复杂主观的多模态现象（如讽刺），评估其在多模态讽刺检测和解释生成方面的能力。

Method: 使用零样本、单样本和少样本提示方法，在三个基准讽刺数据集（Muse、MMSD2.0和SarcNet）上评估7种VLMs（BLIP2、InstructBLIP、OpenFlamingo、LLaVA、PaliGemma、Gemma3和Qwen-VL）。

Result: 当前模型在二元讽刺检测方面取得中等成功，但在生成高质量解释方面能力有限，需要任务特定微调。

Conclusion: 开源视觉语言模型在多模态讽刺检测方面有一定潜力，但在解释生成方面仍需改进，任务特定微调对提升解释质量至关重要。

Abstract: Recent advances in open-source vision-language models (VLMs) offer new
opportunities for understanding complex and subjective multimodal phenomena
such as sarcasm. In this work, we evaluate seven state-of-the-art VLMs - BLIP2,
InstructBLIP, OpenFlamingo, LLaVA, PaliGemma, Gemma3, and Qwen-VL - on their
ability to detect multimodal sarcasm using zero-, one-, and few-shot prompting.
Furthermore, we evaluate the models' capabilities in generating explanations to
sarcastic instances. We evaluate the capabilities of VLMs on three benchmark
sarcasm datasets (Muse, MMSD2.0, and SarcNet). Our primary objectives are
twofold: (1) to quantify each model's performance in detecting sarcastic
image-caption pairs, and (2) to assess their ability to generate human-quality
explanations that highlight the visual-textual incongruities driving sarcasm.
Our results indicate that, while current models achieve moderate success in
binary sarcasm detection, they are still not able to generate high-quality
explanations without task-specific finetuning.

</details>


### [11] [Actor-Enriched Time Series Forecasting of Process Performance](https://arxiv.org/abs/2510.11856)
*Aurelie Leribaux,Rafael Oyamada,Johannes De Smedt,Zahra Dasht Bozorgi,Artem Polyvyanyy,Jochen De Weerdt*

Main category: cs.LG

TL;DR: 该研究探讨了将参与者行为信息作为时间序列纳入预测流程监控中，是否能提高吞吐时间预测模型的性能。结果表明，包含参与者行为的模型在RMSE、MAE和R2指标上均优于仅包含吞吐时间特征的基线模型。


<details>
  <summary>Details</summary>
Motivation: 流程通常是资源驱动的，理解并纳入参与者行为对预测至关重要。现有研究虽然纳入了参与者行为的某些方面，但将其作为时变信号在预测流程监控中的作用仍然有限。

Method: 使用真实事件日志构建多变量时间序列，包括吞吐时间以及参与者中心特征（参与者参与度、继续行为频率、中断行为频率、交接行为频率及其持续时间）。训练和比较多个模型来研究添加参与者行为的好处。

Result: 参与者增强模型在RMSE、MAE和R2指标上持续优于仅包含吞吐时间特征的基线模型。

Conclusion: 将参与者行为建模为时间序列并纳入预测模型，可以显著提高性能指标预测的准确性。

Abstract: Predictive Process Monitoring (PPM) is a key task in Process Mining that aims
to predict future behavior, outcomes, or performance indicators. Accurate
prediction of the latter is critical for proactive decision-making. Given that
processes are often resource-driven, understanding and incorporating actor
behavior in forecasting is crucial. Although existing research has incorporated
aspects of actor behavior, its role as a time-varying signal in PPM remains
limited. This study investigates whether incorporating actor behavior
information, modeled as time series, can improve the predictive performance of
throughput time (TT) forecasting models. Using real-life event logs, we
construct multivariate time series that include TT alongside actor-centric
features, i.e., actor involvement, the frequency of continuation, interruption,
and handover behaviors, and the duration of these behaviors. We train and
compare several models to study the benefits of adding actor behavior. The
results show that actor-enriched models consistently outperform baseline
models, which only include TT features, in terms of RMSE, MAE, and R2. These
findings demonstrate that modeling actor behavior over time and incorporating
this information into forecasting models enhances performance indicator
predictions.

</details>


### [12] [Improving Knowledge Graph Embeddings through Contrastive Learning with Negative Statements](https://arxiv.org/abs/2510.11868)
*Rita T. Sousa,Heiko Paulheim*

Main category: cs.LG

TL;DR: 提出一种集成显式负陈述的知识图嵌入方法，使用双模型架构分别训练正负样本，通过相互评分生成负样本，在链接预测和三元组分类任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有知识图嵌入方法大多基于封闭世界假设，将缺失三元组视为假，这与现实世界的开放世界假设不符。显式负陈述能帮助区分假三元组和未知三元组，但很少被包含在知识图中且常被忽略。

Method: 采用双模型架构，两个嵌入模型并行训练：一个处理正陈述，一个处理负陈述。训练时每个模型通过破坏正样本生成负样本，并由另一个模型评分选择最可能的候选。

Result: 在通用和领域特定知识图上进行广泛实验，在链接预测和三元组分类任务上显示出优于最先进嵌入模型的预测性能。

Conclusion: 将有意义负知识集成到嵌入学习中能显著提升预测性能，证明了负知识在知识图嵌入中的价值。

Abstract: Knowledge graphs represent information as structured triples and serve as the
backbone for a wide range of applications, including question answering, link
prediction, and recommendation systems. A prominent line of research for
exploring knowledge graphs involves graph embedding methods, where entities and
relations are represented in low-dimensional vector spaces that capture
underlying semantics and structure. However, most existing methods rely on
assumptions such as the Closed World Assumption or Local Closed World
Assumption, treating missing triples as false. This contrasts with the Open
World Assumption underlying many real-world knowledge graphs. Furthermore,
while explicitly stated negative statements can help distinguish between false
and unknown triples, they are rarely included in knowledge graphs and are often
overlooked during embedding training.
  In this work, we introduce a novel approach that integrates explicitly
declared negative statements into the knowledge embedding learning process. Our
approach employs a dual-model architecture, where two embedding models are
trained in parallel, one on positive statements and the other on negative
statements. During training, each model generates negative samples by
corrupting positive samples and selecting the most likely candidates as scored
by the other model. The proposed approach is evaluated on both general-purpose
and domain-specific knowledge graphs, with a focus on link prediction and
triple classification tasks. The extensive experiments demonstrate that our
approach improves predictive performance over state-of-the-art embedding
models, demonstrating the value of integrating meaningful negative knowledge
into embedding learning.

</details>


### [13] [Robust Adversarial Reinforcement Learning in Stochastic Games via Sequence Modeling](https://arxiv.org/abs/2510.11877)
*Xiaohang Tang,Zhuowen Cheng,Satyabrat Kumar*

Main category: cs.LG

TL;DR: 提出了CART框架，这是首个增强决策变换器在对抗性随机游戏中鲁棒性的方法，通过将每个阶段的交互建模为阶段博弈，并基于NashQ值生成策略。


<details>
  <summary>Details</summary>
Motivation: 虽然Transformer架构已用于序列决策，但基于序列建模的强化学习方法在对抗鲁棒性方面尚未充分探索，特别是在对抗性随机游戏环境中。

Method: 将每个阶段的交互建模为阶段博弈，定义基于后续状态期望最大值的支付函数，通过基于阶段博弈NashQ值调节Transformer策略来生成鲁棒策略。

Result: CART在对抗性随机游戏中实现了更准确的极小极大值估计，并在各种环境下持续获得更优的最坏情况回报。

Conclusion: CART框架成功增强了决策变换器的对抗鲁棒性，同时保持了对状态转移不确定性的保守性，为序列决策模型的鲁棒性研究提供了新方向。

Abstract: The Transformer, a highly expressive architecture for sequence modeling, has
recently been adapted to solve sequential decision-making, most notably through
the Decision Transformer (DT), which learns policies by conditioning on desired
returns. Yet, the adversarial robustness of reinforcement learning methods
based on sequence modeling remains largely unexplored. Here we introduce the
Conservative Adversarially Robust Decision Transformer (CART), to our knowledge
the first framework designed to enhance the robustness of DT in adversarial
stochastic games. We formulate the interaction between the protagonist and the
adversary at each stage as a stage game, where the payoff is defined as the
expected maximum value over subsequent states, thereby explicitly incorporating
stochastic state transitions. By conditioning Transformer policies on the NashQ
value derived from these stage games, CART generates policy that are
simultaneously less exploitable (adversarially robust) and conservative to
transition uncertainty. Empirically, CART achieves more accurate minimax value
estimation and consistently attains superior worst-case returns across a range
of adversarial stochastic games.

</details>


### [14] [ADARL: Adaptive Low-Rank Structures for Robust Policy Learning under Uncertainty](https://arxiv.org/abs/2510.11899)
*Chenliang Li,Junyu Leng,Jiaxiang Li,Youbang Sun,Shixiang Chen,Shahin Shahrampour,Alfredo Garcia*

Main category: cs.LG

TL;DR: AdaRL提出了一种自适应秩表示的双层优化框架，通过将策略复杂度与任务内在维度对齐来提升鲁棒强化学习的性能，避免传统min-max优化的计算负担和保守性。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒强化学习方法依赖嵌套min-max优化，计算昂贵且产生过于保守的策略，需要更高效的方法来处理环境动态中的认知不确定性。

Method: 采用双层优化框架：下层在固定秩约束下进行策略优化，从Wasserstein球中采样动态；上层自适应调整秩以平衡偏差-方差权衡，将策略参数投影到低秩流形上。

Result: 在MuJoCo连续控制基准测试中，AdaRL不仅优于固定秩基线和最先进的鲁棒RL方法，还能收敛到任务的内在秩。

Conclusion: 自适应低秩策略表示为模型不确定性下的鲁棒RL提供了高效且原则性的替代方案。

Abstract: Robust reinforcement learning (Robust RL) seeks to handle epistemic
uncertainty in environment dynamics, but existing approaches often rely on
nested min--max optimization, which is computationally expensive and yields
overly conservative policies. We propose \textbf{Adaptive Rank Representation
(AdaRL)}, a bi-level optimization framework that improves robustness by
aligning policy complexity with the intrinsic dimension of the task. At the
lower level, AdaRL performs policy optimization under fixed-rank constraints
with dynamics sampled from a Wasserstein ball around a centroid model. At the
upper level, it adaptively adjusts the rank to balance the bias--variance
trade-off, projecting policy parameters onto a low-rank manifold. This design
avoids solving adversarial worst-case dynamics while ensuring robustness
without over-parameterization. Empirical results on MuJoCo continuous control
benchmarks demonstrate that AdaRL not only consistently outperforms fixed-rank
baselines (e.g., SAC) and state-of-the-art robust RL methods (e.g., RNAC,
Parseval), but also converges toward the intrinsic rank of the underlying
tasks. These results highlight that adaptive low-rank policy representations
provide an efficient and principled alternative for robust RL under model
uncertainty.

</details>


### [15] [Integrating Sequential and Relational Modeling for User Events: Datasets and Prediction Tasks](https://arxiv.org/abs/2510.11903)
*Rizal Fathony,Igor Melnyk,Owen Reinert,Nam H. Nguyen,Daniele Rosa,C. Bayan Bruss*

Main category: cs.LG

TL;DR: 该论文提出了一个统一框架来同时建模个人事件和关系事件，填补了现有方法仅单独处理这两种事件类型的空白。


<details>
  <summary>Details</summary>
Motivation: 现实世界系统中需要同时捕捉个人事件和关系事件，但现有工作很少将它们一起考虑，通常将用户行为简化为单一形式化表示（序列或图）。

Method: 引入包含两种事件类型的数据集集合，提出统一的形式化框架，并实证研究结合两种事件类型的模型效果。

Result: 实验结果表明，结合两种事件类型的模型表现更好，但当前方法仍有显著改进空间。

Conclusion: 该研究为统一用户事件建模提供了数据集和基准任务，鼓励在该方向上的进一步研究进展。

Abstract: User event modeling plays a central role in many machine learning
applications, with use cases spanning e-commerce, social media, finance,
cybersecurity, and other domains. User events can be broadly categorized into
personal events, which involve individual actions, and relational events, which
involve interactions between two users. These two types of events are typically
modeled separately, using sequence-based methods for personal events and
graph-based methods for relational events. Despite the need to capture both
event types in real-world systems, prior work has rarely considered them
together. This is often due to the convenient simplification that user behavior
can be adequately represented by a single formalization, either as a sequence
or a graph. To address this gap, there is a need for public datasets and
prediction tasks that explicitly incorporate both personal and relational
events. In this work, we introduce a collection of such datasets, propose a
unified formalization, and empirically show that models benefit from
incorporating both event types. Our results also indicate that current methods
leave a notable room for improvements. We release these resources to support
further research in unified user event modeling and encourage progress in this
direction.

</details>


### [16] [Variational Mixture of Graph Neural Experts for Alzheimer's Disease Biomarker Recognition in EEG Brain Networks](https://arxiv.org/abs/2510.11917)
*Jun-En Ding,Anna Zilverstand,Shihao Yang,Albert Chih-Chieh Yang,Feng Liu*

Main category: cs.LG

TL;DR: 提出VMoGE方法，通过变分图神经网络专家模型整合频率特异性生物标志物识别和结构化变分推理，显著提升痴呆症亚型和严重程度分期的诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病和额颞叶痴呆等痴呆症在EEG中表现出重叠的电生理特征，现有基于EEG的方法受限于全频带分析，难以精确区分痴呆亚型和严重程度阶段。

Method: VMoGE采用多粒度Transformer提取四个频带的多尺度时间模式，然后使用高斯马尔可夫随机场先验的变分图卷积编码器，通过结构化变分推理和自适应门控将神经特化与生理意义的EEG频带联系起来。

Result: 在两个不同数据集上的评估显示，VMoGE在亚型分类和严重程度分期方面均优于现有方法，AUC提高了4%到10%，并提供可解释的专家权重和空间模式。

Conclusion: VMoGE通过频率特异性生物标志物识别和结构化变分推理，为痴呆症的全面诊断和监测提供了有效的EEG生物标志物发现工具。

Abstract: Dementia disorders such as Alzheimer's disease (AD) and frontotemporal
dementia (FTD) exhibit overlapping electrophysiological signatures in EEG that
challenge accurate diagnosis. Existing EEG-based methods are limited by
full-band frequency analysis that hinders precise differentiation of dementia
subtypes and severity stages. We propose a variational mixture of graph neural
experts (VMoGE) that integrates frequency-specific biomarker identification
with structured variational inference for enhanced dementia diagnosis and
staging. VMoGE employs a multi-granularity transformer to extract multi-scale
temporal patterns across four frequency bands, followed by a variational graph
convolutional encoder using Gaussian Markov Random Field priors. Through
structured variational inference and adaptive gating, VMoGE links neural
specialization to physiologically meaningful EEG frequency bands. Evaluated on
two diverse datasets for both subtype classification and severity staging,
VMoGE achieves superior performance with AUC improvements of +4% to +10% over
state-of-the-art methods. Moreover, VMoGE provides interpretable insights
through expert weights that correlate with clinical indicators and spatial
patterns aligned with neuropathological signatures, facilitating EEG biomarker
discovery for comprehensive dementia diagnosis and monitoring.

</details>


### [17] [Indoor Localization using Compact, Telemetry-Agnostic, Transfer-Learning Enabled Decoder-Only Transformer](https://arxiv.org/abs/2510.11926)
*Nayan Sanjay Bhatia,Pranay Kocheta,Russell Elliott,Harikrishna S. Kuttivelil,Katia Obraczka*

Main category: cs.LG

TL;DR: Locaris是一种基于解码器大型语言模型的室内定位系统，将Wi-Fi接入点测量视为标记，无需预处理即可处理原始Wi-Fi遥测数据，实现轻量级且可泛化的室内定位。


<details>
  <summary>Details</summary>
Motivation: 传统室内Wi-Fi定位方法对环境动态、信道传播特性和硬件异构性高度敏感，需要大量人工校准且性能容易退化。

Method: 通过在不同Wi-Fi数据集上微调LLM，将每个接入点测量作为标记，直接从原始信号学习到设备位置的映射。

Result: 实验表明Locaris在多种遥测类型上匹配或超越现有技术，仅用几百个样本即可实现亚米级精度，在缺失AP情况下保持鲁棒性能。

Conclusion: 紧凑型LLM可作为免校准回归模型用于室内定位，在大规模部署中提供可扩展和鲁棒的跨环境性能。

Abstract: Indoor Wi-Fi positioning remains a challenging problem due to the high
sensitivity of radio signals to environmental dynamics, channel propagation
characteristics, and hardware heterogeneity. Conventional fingerprinting and
model-based approaches typically require labor-intensive calibration and suffer
rapid performance degradation when devices, channel or deployment conditions
change. In this paper, we introduce Locaris, a decoder-only large language
model (LLM) for indoor localization. Locaris treats each access point (AP)
measurement as a token, enabling the ingestion of raw Wi-Fi telemetry without
pre-processing. By fine-tuning its LLM on different Wi-Fi datasets, Locaris
learns a lightweight and generalizable mapping from raw signals directly to
device location. Our experimental study comparing Locaris with state-of-the-art
methods consistently shows that Locaris matches or surpasses existing
techniques for various types of telemetry. Our results demonstrate that compact
LLMs can serve as calibration-free regression models for indoor localization,
offering scalable and robust cross-environment performance in heterogeneous
Wi-Fi deployments. Few-shot adaptation experiments, using only a handful of
calibration points per device, further show that Locaris maintains high
accuracy when applied to previously unseen devices and deployment scenarios.
This yields sub-meter accuracy with just a few hundred samples, robust
performance under missing APs and supports any and all available telemetry. Our
findings highlight the practical viability of Locaris for indoor positioning in
the real-world scenarios, particularly in large-scale deployments where
extensive calibration is infeasible.

</details>


### [18] [Efficient Restarts in Non-Stationary Model-Free Reinforcement Learning](https://arxiv.org/abs/2510.11933)
*Hiroshi Nonaka,Simon Ambrozak,Sofia R. Miskala-Dinc,Amedeo Ercole,Aviva Prins*

Main category: cs.LG

TL;DR: 提出了三种高效的重启范式用于无模型非平稳强化学习，解决了RestartQ-UCB算法中完全遗忘和固定时间重启的问题，显著降低了动态遗憾。


<details>
  <summary>Details</summary>
Motivation: 现有RestartQ-UCB算法存在两个核心问题：完全遗忘（重启后丢失所有环境信息）和固定时间重启（不考虑策略与当前环境动态的不兼容性），需要改进重启设计。

Method: 引入了三种重启方法：部分重启、自适应重启和选择性重启，用于改进RestartQ-UCB和RANDOMIZEDQ算法。

Result: 在多种不同环境中实现了接近最优的实证性能，相对于RestartQ-UCB将动态遗憾降低了高达91%。

Conclusion: 提出的三种重启范式有效解决了非平稳强化学习中的重启设计问题，显著提升了算法性能。

Abstract: In this work, we propose three efficient restart paradigms for model-free
non-stationary reinforcement learning (RL). We identify two core issues with
the restart design of Mao et al. (2022)'s RestartQ-UCB algorithm: (1) complete
forgetting, where all the information learned about an environment is lost
after a restart, and (2) scheduled restarts, in which restarts occur only at
predefined timings, regardless of the incompatibility of the policy with the
current environment dynamics. We introduce three approaches, which we call
partial, adaptive, and selective restarts to modify the algorithms RestartQ-UCB
and RANDOMIZEDQ (Wang et al., 2025). We find near-optimal empirical performance
in multiple different environments, decreasing dynamic regret by up to $91$%
relative to RestartQ-UCB.

</details>


### [19] [On efficiently computable functions, deep networks and sparse compositionality](https://arxiv.org/abs/2510.11942)
*Tomaso Poggio*

Main category: cs.LG

TL;DR: 该论文证明高效图灵可计算性意味着存在组合稀疏的DAG表示和相应的神经网络近似器，能够达到目标精度。


<details>
  <summary>Details</summary>
Motivation: 研究高效可计算函数与神经网络表示之间的关系，探索如何将计算复杂性理论结果转化为神经网络架构设计。

Method: 通过将高效可计算函数转换为有界扇入布尔电路，然后用常数大小神经网络模拟器替换每个门，构建深度网络。

Result: 对于在比特深度上多项式时间可计算的函数，可以构建大小和深度为多项式的深度网络，达到指数级精度。

Conclusion: 高效可计算性与组合稀疏神经网络表示之间存在紧密联系，这为理解深度学习的能力和优化提供了理论基础。

Abstract: We show that \emph{efficient Turing computability} at any fixed input/output
precision implies the existence of \emph{compositionally sparse}
(bounded-fan-in, polynomial-size) DAG representations and of corresponding
neural approximants achieving the target precision. Concretely: if
$f:[0,1]^d\to\R^m$ is computable in time polynomial in the bit-depths, then for
every pair of precisions $(n,m_{\mathrm{out}})$ there exists a bounded-fan-in
Boolean circuit of size and depth $\poly(n+m_{\mathrm{out}})$ computing the
discretized map; replacing each gate by a constant-size neural emulator yields
a deep network of size/depth $\poly(n+m_{\mathrm{out}})$ that achieves accuracy
$\varepsilon=2^{-m_{\mathrm{out}}}$. We also relate these constructions to
compositional approximation rates
\cite{MhaskarPoggio2016b,poggio_deep_shallow_2017,Poggio2017,Poggio2023HowDS}
and to optimization viewed as hierarchical search over sparse structures.

</details>


### [20] [Sculpting Latent Spaces With MMD: Disentanglement With Programmable Priors](https://arxiv.org/abs/2510.11953)
*Quentin Fruytier,Akshay Malhotra,Shahab Hamidi-Rad,Aditya Sant,Aryan Mokhtari,Sujay Sanghavi*

Main category: cs.LG

TL;DR: 本文揭示了VAE中KL散度正则化器在强制潜在空间匹配因子化高斯先验方面的不可靠性，并提出了基于最大均值差异(MMD)的可编程先验框架，实现了在复杂数据集上的最先进解缠效果。


<details>
  <summary>Details</summary>
Motivation: 变分自编码器(VAE)框架使用KL散度惩罚来鼓励潜在空间匹配因子化高斯先验，但作者发现这种基于KL的正则化机制不可靠，无法在聚合后验上强制执行目标分布。

Method: 提出了可编程先验框架，该方法基于最大均值差异(MMD)，允许研究人员显式地塑造潜在空间，并引入了无监督的潜在可预测性评分(LPS)来量化解缠效果。

Result: 在CIFAR-10和Tiny ImageNet等复杂数据集上实现了最先进的相互独立性，且没有常见的重建权衡问题，还能构建复杂先验来改善与语义特征的对应关系。

Conclusion: 这项工作为表示工程提供了基础工具，为模型可识别性和因果推理开辟了新途径。

Abstract: Learning disentangled representations, where distinct factors of variation
are captured by independent latent variables, is a central goal in machine
learning. The dominant approach has been the Variational Autoencoder (VAE)
framework, which uses a Kullback-Leibler (KL) divergence penalty to encourage
the latent space to match a factorized Gaussian prior. In this work, however,
we provide direct evidence that this KL-based regularizer is an unreliable
mechanism, consistently failing to enforce the target distribution on the
aggregate posterior. We validate this and quantify the resulting entanglement
using our novel, unsupervised Latent Predictability Score (LPS). To address
this failure, we introduce the Programmable Prior Framework, a method built on
the Maximum Mean Discrepancy (MMD). Our framework allows practitioners to
explicitly sculpt the latent space, achieving state-of-the-art mutual
independence on complex datasets like CIFAR-10 and Tiny ImageNet without the
common reconstruction trade-off. Furthermore, we demonstrate how this
programmability can be used to engineer sophisticated priors that improve
alignment with semantically meaningful features. Ultimately, our work provides
a foundational tool for representation engineering, opening new avenues for
model identifiability and causal reasoning.

</details>


### [21] [Y-shaped Generative Flows](https://arxiv.org/abs/2510.11955)
*Arip Asadulaev,Semyon Semenov,Abduragim Shtanchaev,Eric Moulines,Fakhri Karray,Martin Takac*

Main category: cs.LG

TL;DR: 该论文提出了Y形生成流，通过共享路径移动概率质量，然后分支到特定端点，改进了传统V形传输方法。


<details>
  <summary>Details</summary>
Motivation: 现代连续时间生成模型通常采用V形传输，样本独立沿直线轨迹从先验到数据移动，忽略了共享结构。

Method: 引入Y形生成流，基于具有次线性指数的新型速度驱动传输成本，在可扩展的神经ODE训练目标中实例化该思想。

Result: 在合成、图像和生物学数据集上，Y流恢复了层次感知结构，改进了基于流的强基线方法的分布度量，并以更少的积分步骤达到目标。

Conclusion: Y形生成流通过共享路径和分支机制，有效利用了数据中的共享结构，提高了生成效率和质量。

Abstract: Modern continuous-time generative models often induce V-shaped transport:
each sample travels independently along nearly straight trajectories from prior
to data, overlooking shared structure. We introduce Y-shaped generative flows,
which move probability mass together along shared pathways before branching to
target-specific endpoints. Our formulation is based on novel velocity-powered
transport cost with a sublinear exponent (between zero and one). this concave
dependence rewards joint and fast mass movement. Practically, we instantiate
the idea in a scalable neural ODE training objective. On synthetic, image, and
biology datasets, Y-flows recover hierarchy-aware structure, improve
distributional metrics over strong flow-based baselines, and reach targets with
fewer integration steps.

</details>


### [22] [MosaicDiff: Training-free Structural Pruning for Diffusion Model Acceleration Reflecting Pretraining Dynamics](https://arxiv.org/abs/2510.11962)
*Bowei Guo,Shengkun Tang,Cong Zeng,Zhiqiang Shen*

Main category: cs.LG

TL;DR: MosaicDiff是一个新颖的扩散模型加速框架，通过轨迹感知的结构剪枝将预训练动态与后训练采样加速对齐，实现显著加速而不损失输出质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型预训练过程中存在不同的学习速度阶段，但之前的后训练加速方法完全忽视了这一现象。

Method: 采用自适应剪枝机制，根据预训练不同阶段的学习速度进行不同程度的剪枝：中间快速学习阶段保守剪枝以保留关键特征，早期和后期慢速学习阶段则采用更激进的剪枝策略。

Result: 在DiT和SDXL上的广泛实验表明，该方法实现了显著的采样加速，且不损害输出质量，大幅优于之前的SOTA方法。

Conclusion: 该方法为更高效和鲁棒的无训练扩散加速提供了新视角，首次明确反映了扩散预训练固有的学习速度变化。

Abstract: Diffusion models are renowned for their generative capabilities, yet their
pretraining processes exhibit distinct phases of learning speed that have been
entirely overlooked in prior post-training acceleration efforts in the
community. In this study, we introduce a novel framework called MosaicDiff that
aligns diffusion pretraining dynamics with post-training sampling acceleration
via trajectory-aware structural pruning. Our approach leverages the observation
that the middle, fast-learning stage of diffusion pretraining requires more
conservative pruning to preserve critical model features, while the early and
later, slow-learning stages benefit from a more aggressive pruning strategy.
This adaptive pruning mechanism is the first to explicitly mirror the inherent
learning speed variations of diffusion pretraining, thereby harmonizing the
model's inner training dynamics with its accelerated sampling process.
Extensive experiments on DiT and SDXL demonstrate that our method achieves
significant speed-ups in sampling without compromising output quality,
outperforming previous state-of-the-art methods by large margins, also
providing a new viewpoint for more efficient and robust training-free diffusion
acceleration.

</details>


### [23] [QLENS: Towards A Quantum Perspective of Language Transformers](https://arxiv.org/abs/2510.11963)
*Aditya Gupta,Kirandeep Kaur,Vinayak Gupta*

Main category: cs.LG

TL;DR: 提出QLENS框架，将Transformer模型重新解释为量子力学系统，通过将隐藏层重新定义为幺正算子和哈密顿量，为理解Transformer的生成过程提供基于物理学的数学框架。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer理解方法只作为有限的诊断检查点，缺乏数学框架来机制性地建模各层如何促进状态转换。受量子力学与语言模型概率属性的相似性启发，寻求跨学科视角。

Method: 将Transformer的潜在激活转换为希尔伯特空间中的状态向量，将隐藏层重新定义为幺正算子和哈密顿量，最终通过玻恩规则获得概率分布。

Result: 通过玩具Transformer的概念验证，展示了QLENS在分析模型预测轨迹中各层影响的潜力。

Conclusion: QLENS为跨领域洞察提供了基础，有助于更广泛地理解Transformer模型。

Abstract: In natural language processing, current methods for understanding
Transformers are successful at identifying intermediate predictions during a
model's inference. However, these approaches function as limited diagnostic
checkpoints, lacking a mathematical framework for mechanistically modeling how
each layer facilitates transitions between these evolving states. This
interpretability gap and past successes of interdisciplinary outlooks inspire
us to turn to physics in search of a descriptive mathematical framework for
Transformers. We observe that language models are intrinsically probabilistic,
an attribute that is echoed in the core postulates of quantum mechanics. This
parallel inspires us to translate insights from this discipline to that of
natural language processing. Towards this objective, we propose QLENS a novel
attempt to develop a physics-based perspective on the Transformer generation
process. Under QLENS, a Transformer is studied by converting its latent
activations into a state vector in a Hilbert space derived from the model's
output units. This state subsequently evolves through hidden layers -
reformulated as unitary operators and analogously defined Hamiltonians - during
inference. The model's final probability distribution is obtained by applying
the Born rule to the end state using a specific measurement operator. To
demonstrate QLENS's potential, we conduct a proof-of-concept by probing a toy
Transformer to investigate the influence of individual layers in a model's
prediction trajectory. We present our work as a foundation for cross-domain
insights to be leveraged towards a broader understanding of Transformers.

</details>


### [24] [Learning Dynamics of VLM Finetuning](https://arxiv.org/abs/2510.11978)
*Jusheng Zhang,Kaitong Cai,Jing Yang,Keze Wang*

Main category: cs.LG

TL;DR: CW-DPO是一种两阶段的视觉语言模型对齐方法，通过温和负样本的监督微调阶段和带冷却权重的DPO阶段，稳定训练过程并提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统的基于偏好的视觉语言模型微调方法存在训练不稳定的问题，特别是当负样本包含无信息梯度时会破坏训练过程。

Method: 第一阶段使用温和负样本进行监督微调，第二阶段应用带冷却权重的DPO目标，冷却权重基于模型对每个负样本的平均token对数概率计算，抑制无信息梯度。

Result: 在多种VLM任务中，CW-DPO相比仅使用SFT和普通DPO，实现了更稳定的优化、更好的校准、更高的配对胜率，且收敛步数更少。

Conclusion: 在冷却偏好之前平滑学习动态是进行稳健VLM对齐的简单通用原则。

Abstract: Preference-based finetuning of vision--language models (VLMs) is brittle:
trivially wrong negatives inject uninformative gradients that destabilize
training. We recast alignment as \textbf{learning-dynamics--aware optimization}
and introduce \textbf{Cooling-Weighted DPO (CW-DPO)}, a two-stage recipe that
explicitly models and exploits the training trajectory. \textbf{Stage 1}
performs supervised finetuning with \textbf{gentle negatives}:
\textbf{low-weight smoothed supervision} that regularizes the base policy and
curbs overconfidence without explicit penalties. \textbf{Stage 2} applies a DPO
objective in which the \textbf{negative term is scaled by a cooling weight}
computed from the model's \textbf{average token log-probability} on each
negative, suppressing uninformative gradients from easy or off-distribution
samples while preserving signal from hard negatives. In practice, we emphasize
\textbf{on-policy negatives} and allow \textbf{mixed negatives} by blending a
controllable fraction of dataset negatives to maintain contrast freshness.
Throughout, we instrument training with $\Delta\!\log p$ probes on positives
and negatives as first-class signals for early stopping, curriculum design, and
failure diagnosis. Across diverse VLM tasks, CW-DPO yields \textbf{more stable
optimization}, \textbf{better calibration}, and \textbf{higher pairwise
win-rates} than SFT-only and vanilla DPO, while \textbf{converging in fewer
steps}. Ablations isolate the \textbf{cooling-weight mechanism} as the primary
driver of these gains and show complementary benefits from mixing on-policy and
dataset negatives. Taken together, our results show that \textbf{smoothing
learning dynamics before cooling preferences} is a simple, general principle
for robust VLM alignment.

</details>


### [25] [Learning by Steering the Neural Dynamics: A Statistical Mechanics Perspective](https://arxiv.org/abs/2510.11984)
*Mattia Scardecchia*

Main category: cs.LG

TL;DR: 该研究通过统计力学方法分析随机不对称循环网络中动态吸引子的出现条件，揭示了自耦合强度对固定点结构的相变现象，并提出了一种生物合理的监督学习算法，该算法能将输入映射到动态固定点，并在MNIST等基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前基于梯度优化的深度神经网络与生物学习机制存在根本差异，研究旨在弥合当代AI与计算神经科学之间的差距，探索自然界如何以最小能量成本实现鲁棒、样本高效的学习，以及如何在不使用反向传播的情况下解决信用分配问题。

Method: 使用统计力学工具分析随机不对称循环网络中动态吸引子的出现条件，推导固定点数量与自耦合强度的闭式表达式，并提出基于局部可塑性的生物合理监督学习算法，通过瞬态外部刺激将输入映射到动态固定点。

Result: 发现了固定点结构的相变现象：在临界自耦合强度以下，孤立固定点与指数级多窄簇共存；以上则出现次主导但密集且广泛的簇。提出的算法能在纠缠版MNIST上学习，利用深度发展层次表示并增加异联想能力，适用于多种架构。

Conclusion: 算法性能与揭示的相变现象密切相关，研究为理解生物学习机制提供了新视角，并提出了皮层启发的自耦合替代方案，为弥合AI与神经科学差距迈出了重要一步。

Abstract: Despite the striking successes of deep neural networks trained with
gradient-based optimization, these methods differ fundamentally from their
biological counterparts. This gap raises key questions about how nature
achieves robust, sample-efficient learning at minimal energy costs and solves
the credit-assignment problem without backpropagation. We take a step toward
bridging contemporary AI and computational neuroscience by studying how neural
dynamics can support fully local, distributed learning that scales to simple
machine-learning benchmarks. Using tools from statistical mechanics, we
identify conditions for the emergence of robust dynamical attractors in random
asymmetric recurrent networks. We derive a closed-form expression for the
number of fixed points as a function of self-coupling strength, and we reveal a
phase transition in their structure: below a critical self-coupling, isolated
fixed points coexist with exponentially many narrow clusters showing the
overlap-gap property; above it, subdominant yet dense and extensive clusters
appear. These fixed points become accessible, including to a simple
asynchronous dynamical rule, after an algorithm-dependent self-coupling
threshold. Building on this analysis, we propose a biologically plausible
algorithm for supervised learning with any binary recurrent network. Inputs are
mapped to fixed points of the dynamics, by relaxing under transient external
stimuli and stabilizing the resulting configurations via local plasticity. We
show that our algorithm can learn an entangled version of MNIST, leverages
depth to develop hierarchical representations and increase hetero-association
capacity, and is applicable to several architectures. Finally, we highlight the
strong connection between algorithm performance and the unveiled phase
transition, and we suggest a cortex-inspired alternative to self-couplings for
its emergence.

</details>


### [26] [Nonlinear discretizations and Newton's method: characterizing stationary points of regression objectives](https://arxiv.org/abs/2510.11987)
*Conor Rowan*

Main category: cs.LG

TL;DR: 论文发现使用精确二阶信息（真实Hessian矩阵）进行神经网络训练会可靠地失败，这与使用近似Hessian的拟牛顿方法形成对比，挑战了损失函数充满局部最小值的传统观点。


<details>
  <summary>Details</summary>
Motivation: 研究二阶方法在神经网络训练中的潜力，探索使用精确曲率信息而非近似方法的优势和局限。

Method: 比较使用真实Hessian矩阵和拟牛顿方法（近似Hessian）在神经网络训练中的表现。

Result: 使用精确曲率信息进行神经网络训练会可靠地失败，而拟牛顿方法表现良好。失败模式揭示了非线性离散化的几何特性。

Conclusion: 研究结果质疑了损失函数充满局部最小值的传统观点，为理解神经网络优化几何提供了新视角。

Abstract: Second-order methods are emerging as promising alternatives to standard
first-order optimizers such as gradient descent and ADAM for training neural
networks. Though the advantages of including curvature information in computing
optimization steps have been celebrated in the scientific machine learning
literature, the only second-order methods that have been studied are
quasi-Newton, meaning that the Hessian matrix of the objective function is
approximated. Though one would expect only to gain from using the true Hessian
in place of its approximation, we show that neural network training reliably
fails when relying on exact curvature information. The failure modes provide
insight both into the geometry of nonlinear discretizations as well as the
distribution of stationary points in the loss landscape, leading us to question
the conventional wisdom that the loss landscape is replete with local minima.

</details>


### [27] [Mamaba Can Learn Low-Dimensional Targets In-Context via Test-Time Feature Learning](https://arxiv.org/abs/2510.12026)
*Junsoo Oh,Wei Huang,Taiji Suzuki*

Main category: cs.LG

TL;DR: 本文对Mamba模型的上下文学习能力进行了理论分析，证明其通过测试时特征学习能够高效地从上下文示例中提取相关特征方向，在单索引模型任务上达到与非线性Transformer相当的性能。


<details>
  <summary>Details</summary>
Motivation: Mamba作为一种线性时间序列模型，虽然计算效率高且实证表现优异，但其底层机制的理论理解仍然有限。本文旨在从理论上分析Mamba的上下文学习能力。

Method: 研究Mamba在低维非线性目标函数任务中的上下文学习能力，特别关注单索引模型y≈g*(⟨β,x⟩)，其中β是相关特征方向。通过梯度预训练方法分析Mamba的特征提取机制。

Result: 证明Mamba能够通过测试时特征学习从上下文示例中提取相关方向，其测试时样本复杂度优于线性Transformer（表现为核方法），且与非线性Transformer相当，能够超越相关统计查询下界并接近信息理论最优速率。

Conclusion: 非线性门控机制是Mamba实现特征提取的关键因素，这是Mamba能够同时实现计算效率和高性能的根本驱动力。

Abstract: Mamba, a recently proposed linear-time sequence model, has attracted
significant attention for its computational efficiency and strong empirical
performance. However, a rigorous theoretical understanding of its underlying
mechanisms remains limited. In this work, we provide a theoretical analysis of
Mamba's in-context learning (ICL) capability by focusing on tasks defined by
low-dimensional nonlinear target functions. Specifically, we study in-context
learning of a single-index model $y \approx g_*(\langle \boldsymbol{\beta},
\boldsymbol{x} \rangle)$, which depends on only a single relevant direction
$\boldsymbol{\beta}$, referred to as feature. We prove that Mamba, pretrained
by gradient-based methods, can achieve efficient ICL via test-time feature
learning, extracting the relevant direction directly from context examples.
Consequently, we establish a test-time sample complexity that improves upon
linear Transformers -- analyzed to behave like kernel methods -- and is
comparable to nonlinear Transformers, which have been shown to surpass the
Correlational Statistical Query (CSQ) lower bound and achieve near
information-theoretically optimal rate in previous works. Our analysis reveals
the crucial role of the nonlinear gating mechanism in Mamba for feature
extraction, highlighting it as the fundamental driver behind Mamba's ability to
achieve both computational efficiency and high performance.

</details>


### [28] [Your VAR Model is Secretly an Efficient and Explainable Generative Classifier](https://arxiv.org/abs/2510.12060)
*Yi-Chung Chen,David I. Inouye,Jing Gao*

Main category: cs.LG

TL;DR: 提出基于视觉自回归模型的新型生成式分类器A-VARC+，相比扩散模型在准确性和推理速度间取得更好平衡，并具有可解释性和抗灾难性遗忘特性。


<details>
  <summary>Details</summary>
Motivation: 当前生成式分类器主要依赖计算成本高的扩散模型，限制了可扩展性和对生成式分类器的深入理解，需要探索更高效的替代方法。

Method: 利用视觉自回归建模的最新进展构建生成式分类器，并提出自适应VAR分类器A-VARC+来优化性能与速度的权衡。

Result: A-VARC+在准确性和推理速度方面达到优越平衡，显著提升实用性；VAR方法具有可处理的似然度，支持基于token互信息的可视化解释，并在类增量学习中表现出抗灾难性遗忘能力。

Conclusion: 基于VAR的生成式分类器为研究生成式分类提供了新视角，相比扩散模型具有根本不同的特性，在可解释性和持续学习方面具有独特优势。

Abstract: Generative classifiers, which leverage conditional generative models for
classification, have recently demonstrated desirable properties such as
robustness to distribution shifts. However, recent progress in this area has
been largely driven by diffusion-based models, whose substantial computational
cost severely limits scalability. This exclusive focus on diffusion-based
methods has also constrained our understanding of generative classifiers. In
this work, we propose a novel generative classifier built on recent advances in
visual autoregressive (VAR) modeling, which offers a new perspective for
studying generative classifiers. To further enhance its performance, we
introduce the Adaptive VAR Classifier$^+$ (A-VARC$^+$), which achieves a
superior trade-off between accuracy and inference speed, thereby significantly
improving practical applicability. Moreover, we show that the VAR-based method
exhibits fundamentally different properties from diffusion-based methods. In
particular, due to its tractable likelihood, the VAR-based classifier enables
visual explainability via token-wise mutual information and demonstrates
inherent resistance to catastrophic forgetting in class-incremental learning
tasks.

</details>


### [29] [MEASURE: Multi-scale Minimal Sufficient Representation Learning for Domain Generalization in Sleep Staging](https://arxiv.org/abs/2510.12070)
*Sangmin Jo,Jee Seok Yoon,Wootaek Jeong,Kwanseok Oh,Heung-Il Suk*

Main category: cs.LG

TL;DR: 提出MEASURE框架解决睡眠分期中的领域泛化问题，通过多尺度最小充分表示学习减少领域相关信息，在SleepEDF-20和MASS数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习睡眠分期模型在未见受试者上泛化能力差，现有对比学习方法未能充分提取领域不变特征，需要解决嵌入在样本非共享信息中的领域特征问题。

Method: 提出MEASURE框架，通过多尺度最小充分表示学习，在减少领域相关信息的同时保留睡眠分期所需的关键时间和频谱特征。

Result: 在SleepEDF-20和MASS公开睡眠分期基准数据集上的实验表明，该方法持续优于最先进方法。

Conclusion: MEASURE框架能有效减少领域相关信息，同时保留睡眠分期所需的关键特征，在领域泛化方面表现优异。

Abstract: Deep learning-based automatic sleep staging has significantly advanced in
performance and plays a crucial role in the diagnosis of sleep disorders.
However, those models often struggle to generalize on unseen subjects due to
variability in physiological signals, resulting in degraded performance in
out-of-distribution scenarios. To address this issue, domain generalization
approaches have recently been studied to ensure generalized performance on
unseen domains during training. Among those techniques, contrastive learning
has proven its validity in learning domain-invariant features by aligning
samples of the same class across different domains. Despite its potential, many
existing methods are insufficient to extract adequately domain-invariant
representations, as they do not explicitly address domain characteristics
embedded within the unshared information across samples. In this paper, we
posit that mitigating such domain-relevant attributes-referred to as excess
domain-relevant information-is key to bridging the domain gap. However, the
direct strategy to mitigate the domain-relevant attributes often overfits
features at the high-level information, limiting their ability to leverage the
diverse temporal and spectral information encoded in the multiple feature
levels. To address these limitations, we propose a novel MEASURE (Multi-scalE
minimAl SUfficient Representation lEarning) framework, which effectively
reduces domain-relevant information while preserving essential temporal and
spectral features for sleep stage classification. In our exhaustive experiments
on publicly available sleep staging benchmark datasets, SleepEDF-20 and MASS,
our proposed method consistently outperformed state-of-the-art methods. Our
code is available at : https://github.com/ku-milab/Measure

</details>


### [30] [Influence Dynamics and Stagewise Data Attribution](https://arxiv.org/abs/2510.12071)
*Jin Hwa Lee,Matthew Smith,Maxwell Adam,Jesse Hoogland*

Main category: cs.LG

TL;DR: 本文提出了一个基于奇异学习理论的分阶段数据归因框架，揭示了神经网络训练过程中样本间影响力的动态变化模式。


<details>
  <summary>Details</summary>
Motivation: 当前的数据归因方法假设样本间影响力是静态的，但神经网络学习过程包含不同阶段，这些阶段表现出变化的影响力模式。

Method: 基于奇异学习理论构建分阶段数据归因框架，通过玩具模型进行理论和实证验证，并在语言模型中大规模演示。

Result: 发现影响力会发生非单调变化，包括符号翻转和在发展过渡期的急剧峰值，这些动态变化与模型学习语义层次结构的进程直接对应。

Conclusion: 神经网络学习过程中的影响力是动态变化的，分阶段数据归因框架能够捕捉到与已知发展阶段对齐的影响力变化模式。

Abstract: Current training data attribution (TDA) methods treat the influence one
sample has on another as static, but neural networks learn in distinct stages
that exhibit changing patterns of influence. In this work, we introduce a
framework for stagewise data attribution grounded in singular learning theory.
We predict that influence can change non-monotonically, including sign flips
and sharp peaks at developmental transitions. We first validate these
predictions analytically and empirically in a toy model, showing that dynamic
shifts in influence directly map to the model's progressive learning of a
semantic hierarchy. Finally, we demonstrate these phenomena at scale in
language models, where token-level influence changes align with known
developmental stages.

</details>


### [31] [GraphShaper: Geometry-aware Alignment for Improving Transfer Learning in Text-Attributed Graphs](https://arxiv.org/abs/2510.12085)
*Heng Zhang,Tianyi Zhang,Yuling Shi,Xiaodong Gu,Yaomin Shen,Haochen You,Zijian Zhang,Yilei Yuan,Jin Huang*

Main category: cs.LG

TL;DR: GraphShaper是一个几何感知的图基础模型框架，通过多几何专业化解决现有方法在结构边界处的性能下降问题，在零样本设置下显著提升准确率。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型假设所有图结构都可以在单一欧几里得空间中编码，但在结构边界处（不同拓扑模式交汇处）会出现超过20个百分点的准确率损失，因为树结构需要双曲几何而环模式需要球面几何。

Method: GraphShaper采用针对不同几何空间的专家网络，动态计算融合权重，基于局部结构特征自适应地整合几何特性，在文本嵌入对齐前保持结构完整性。

Result: 在零样本设置下，GraphShaper在引文网络上实现9.47%的准确率提升，在社交网络上实现7.63%的准确率提升。

Conclusion: 通过尊重图结构的内在几何多样性，GraphShaper框架能够有效解决结构边界处的性能退化问题，显著提升图基础模型的表示能力。

Abstract: Graph foundation models represent a transformative paradigm for learning
transferable representations across diverse graph domains. Recent methods
leverage large language models to unify graph and text modalities into a shared
representation space using contrastive learning. However, systematic
evaluations reveal significant performance degradation at structural boundaries
where distinct topological patterns converge, with accuracy losses exceeding 20
percentage points. This issue arises from a key limitation: current methods
assume all graph structures can be encoded within a single Euclidean space. In
reality, tree structures require hyperbolic geometry to preserve hierarchical
branching, while cyclic patterns depend on spherical geometry for closure
properties. At structural boundaries, nodes experience conflicting geometric
constraints that uniform encoding spaces cannot resolve. This raises a crucial
challenge: \textbf{Can alignment frameworks be designed to respect the
intrinsic geometric diversity of graph structures?} We introduce
\textbf{GraphShaper}, a geometry-aware framework that enhances graph encoding
through multi-geometric specialization. Our approach employs expert networks
tailored to different geometric spaces, dynamically computing fusion weights to
adaptively integrate geometric properties based on local structural
characteristics. This adaptive fusion preserves structural integrity before
alignment with text embeddings. Extensive experiments demonstrate that
GraphShaper achieves 9.47\% accuracy improvements on citation networks and
7.63\% on social networks in zero-shot settings.

</details>


### [32] [H4G: Unlocking Faithful Inference for Zero-Shot Graph Learning in Hyperbolic Space](https://arxiv.org/abs/2510.12094)
*Heng Zhang,Tianyi Zhang,Zijun Liu,Yuling Shi,Yaomin Shen,Haochen You,Haichuan Hu,Lubin Gan,Jin Huang*

Main category: cs.LG

TL;DR: 论文提出H4G框架，通过降低双曲空间中的嵌入半径来解决文本属性图中的过抽象问题，在异配图和同配图上分别实现了12.8%和8.4%的零样本性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理需要细粒度模式识别的任务时表现不佳，特别是在异配图上。通过实证和理论分析发现存在过抽象问题，即当前方法在过大的双曲半径下操作，将多尺度结构信息压缩为统一的高级抽象，导致关键局部模式丢失。

Method: 提出H4G框架，使用可学习的块对角缩放矩阵和Möbius矩阵乘法系统性地降低嵌入半径，恢复对细粒度模式的访问，同时以最小计算开销保持全局感知能力。

Result: 实验显示H4G在异配图上实现12.8%的零样本性能提升，在同配图上实现8.4%的提升，达到最先进水平。

Conclusion: 半径减少能够实现忠实多尺度表示，推动零样本图学习的发展。

Abstract: Text-attributed graphs are widely used across domains, offering rich
opportunities for zero-shot learning via graph-text alignment. However,
existing methods struggle with tasks requiring fine-grained pattern
recognition, particularly on heterophilic graphs. Through empirical and
theoretical analysis, we identify an \textbf{over-abstraction problem}: current
approaches operate at excessively large hyperbolic radii, compressing
multi-scale structural information into uniform high-level abstractions. This
abstraction-induced information loss obscures critical local patterns essential
for accurate predictions. By analyzing embeddings in hyperbolic space, we
demonstrate that optimal graph learning requires \textbf{faithful preservation}
of fine-grained structural details, better retained by representations
positioned closer to the origin. To address this, we propose \textbf{H4G}, a
framework that systematically reduces embedding radii using learnable
block-diagonal scaling matrices and M\"obius matrix multiplication. This
approach restores access to fine-grained patterns while maintaining global
receptive ability with minimal computational overhead. Experiments show H4G
achieves state-of-the-art zero-shot performance with \textbf{12.8\%}
improvement on heterophilic graphs and \textbf{8.4\%} on homophilic graphs,
confirming that radius reduction enables faithful multi-scale representation
for advancing zero-shot graph learning.

</details>


### [33] [Rethinking the Role of Dynamic Sparse Training for Scalable Deep Reinforcement Learning](https://arxiv.org/abs/2510.12096)
*Guozheng Ma,Lu Li,Zilin Wang,Haoyu Wang,Shengchao Hu,Leszek Rutkowski,Dacheng Tao*

Main category: cs.LG

TL;DR: 该论文提出模块特定训练(MST)框架，通过动态稀疏训练策略解决深度强化学习中模型规模扩大导致的性能下降问题，在不同模块中应用定制化训练方法，显著提升可扩展性。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习中模型规模扩大往往导致性能下降，现有动态训练方法存在三个关键局限：对所有模块应用统一策略、缺乏架构改进与动态训练重要性比较、缺少不同动态方法的系统对比。

Method: 通过跨模块和架构的综合研究，开发了模块特定训练(MST)框架，针对编码器、评论家和行动者等不同模块采用定制化的动态稀疏训练策略。

Result: 研究表明动态稀疏训练策略提供模块特定的优势，与架构改进形成互补，MST框架在不修改算法的情况下显著提升了多种RL算法的可扩展性。

Conclusion: 模块特定训练框架有效解决了深度强化学习的可扩展性问题，通过结合架构改进和动态训练策略，为大规模RL模型提供了实用的解决方案。

Abstract: Scaling neural networks has driven breakthrough advances in machine learning,
yet this paradigm fails in deep reinforcement learning (DRL), where larger
models often degrade performance due to unique optimization pathologies such as
plasticity loss. While recent works show that dynamically adapting network
topology during training can mitigate these issues, existing studies have three
critical limitations: (1) applying uniform dynamic training strategies across
all modules despite encoder, critic, and actor following distinct learning
paradigms, (2) focusing evaluation on basic architectures without clarifying
the relative importance and interaction between dynamic training and
architectural improvements, and (3) lacking systematic comparison between
different dynamic approaches including sparse-to-sparse, dense-to-sparse, and
sparse-to-dense. Through comprehensive investigation across modules and
architectures, we reveal that dynamic sparse training strategies provide
module-specific benefits that complement the primary scalability foundation
established by architectural improvements. We finally distill these insights
into Module-Specific Training (MST), a practical framework that further
exploits the benefits of architectural improvements and demonstrates
substantial scalability gains across diverse RL algorithms without algorithmic
modifications.

</details>


### [34] [Chimera: State Space Models Beyond Sequences](https://arxiv.org/abs/2510.12111)
*Aakash Lahoti,Tanya Marwah,Ratish Puduppully,Albert Gu*

Main category: cs.LG

TL;DR: Chimera是一个统一模型，通过状态空间模型直接融入数据拓扑结构，无需领域特定偏置，在语言、视觉和图数据上均取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer方法依赖自注意力机制，将数据视为无序集合，忽略了数据的邻域结构或图拓扑，需要设计任务特定的偏置（如位置嵌入、随机游走）来融入拓扑，但这需要大量努力且可能引入副作用。

Method: 提出Chimera模型，将状态空间模型推广到捕获任意图拓扑，无需位置嵌入。针对有向无环图实现线性时间递归，针对一般图通过数学松弛达到Transformer的二次复杂度。

Result: 在GLUE上比BERT提升0.7分，在ImageNet-1k上比ViT提升2.6%，在Long Range Graph Benchmark上超越所有基线。

Conclusion: 验证了Chimera的核心贡献，支持数据拓扑是跨模态的强大归纳偏置这一观点。

Abstract: Transformer-based deep learning methods have become the standard approach for
modeling diverse data such as sequences, images, and graphs. These methods rely
on self-attention, which treats data as an unordered set of elements. This
ignores the neighborhood structure or graph topology of the data and requires
inductive biases--such as position embeddings in sequences and images, or
random walks in graphs--to incorporate topology. However, designing such
task-specific biases requires significant effort and can introduce side effects
that hinder generalization. We introduce Chimera, a unified model that directly
incorporates data topology in a principled way, removing the need for
domain-specific biases. The key idea is that state space models--which
naturally do not require position embeddings--can be generalized to capture any
graph topology. Our experiments show that Chimera achieves strong performance
across language, vision, and graph domains, outperforming BERT on GLUE by 0.7
points, ViT on ImageNet-1k by 2.6%, and all baselines on the Long Range Graph
Benchmark. We further propose algorithmic optimizations to improve Chimera's
efficiency: (1) for Directed Acyclic Graphs, Chimera can be implemented as a
linear-time recurrence; (2) for general graphs, a simple mathematical
relaxation achieves Transformer's quadratic complexity without domain-specific
heuristics. These results validate Chimera's core contribution and support the
idea that data topology is a powerful inductive bias across modalities.

</details>


### [35] [nuGPR: GPU-Accelerated Gaussian Process Regression with Iterative Algorithms and Low-Rank Approximations](https://arxiv.org/abs/2510.12128)
*Ziqi Zhao,Vivek Sarin*

Main category: cs.LG

TL;DR: nuGPR是一个新的高斯过程回归框架，通过数值线性代数技术显著降低训练计算成本，包括使用预条件共轭梯度法、数据聚类识别协方差矩阵块对角结构、数值梯度优化超参数，并在GPU上并行化实现。


<details>
  <summary>Details</summary>
Motivation: 解决高斯过程回归训练计算成本高的挑战，特别是在大规模数据集上的应用受限问题。

Method: 结合预条件共轭梯度法加速线性求解，利用数据聚类识别协方差矩阵块对角结构并构造低秩近似，使用数值梯度优化超参数避免反向传播，在GPU上并行化训练。

Result: 与现有最佳GPU GPR实现相比，总训练时间减少高达2倍，峰值内存消耗减少高达12倍。

Conclusion: nuGPR框架通过数值线性代数优化有效解决了GPR的高计算成本问题，在大规模数据集上具有显著优势。

Abstract: Gaussian Process Regression (GPR) is an important type of supervised machine
learning model with inherent uncertainty measure in its predictions. We propose
a new framework, nuGPR, to address the well-known challenge of high computation
cost associated with GPR training. Our framework includes several ideas from
numerical linear algebra to reduce the amount of computation in key steps of
GPR, and we combine them to establish an end-to-end training algorithm.
Specifically, we leverage the preconditioned conjugate gradient method to
accelerate the convergence of the linear solves required in GPR. We exploit
clustering in the input data to identify block-diagonal structure of the
covariance matrix and subsequently construct low-rank approximations of the
off-diagonal blocks. These enhancements significantly reduce the time and space
complexity of our computations. In addition, unlike other frameworks that rely
on exact differentiation, we employ numerical gradients to optimize the
hyperparameters of our GPR model, further reducing the training cost by
eliminating the need for backpropagation. Lastly, we leverage the CUDA Toolkit
to efficiently parallelize the training procedure on NVIDIA GPUs. As a result,
nuGPR reduces total training time by up to 2x and peak memory consumption by up
to 12x on various synthetic and real-world datasets when compared to the best
existing GPU-based GPR implementation.

</details>


### [36] [Graph Few-Shot Learning via Adaptive Spectrum Experts and Cross-Set Distribution Calibration](https://arxiv.org/abs/2510.12140)
*Yonghao Liu,Yajun Wang,Chunli Guo,Wei Pang,Ximing Li,Fausto Giunchiglia,Xiaoyue Feng,Renchu Guan*

Main category: cs.LG

TL;DR: 提出GRACE框架，通过自适应频谱专家和跨集分布校准技术解决图小样本学习中的局部结构异质性和分布差异问题


<details>
  <summary>Details</summary>
Motivation: 现有图小样本学习方法存在两个主要局限：1) 使用预定义的统一图滤波器无法适应真实图中局部拓扑结构的异质性；2) 假设支持集和查询集来自相同分布，但在小样本条件下有限标签数据可能无法充分捕捉查询集的复杂分布

Method: GRACE框架整合自适应频谱专家和跨集分布校准技术，前者适应局部结构变化，后者处理支持集与查询集之间的分布差异

Result: 理论分析表明该方法通过适应局部结构变化和跨集分布校准增强了模型泛化能力；实证结果显示GRACE在各种实验设置下始终优于最先进的基线方法

Conclusion: GRACE框架有效解决了图小样本学习中的局部结构异质性和分布差异挑战，显著提升了模型性能

Abstract: Graph few-shot learning has attracted increasing attention due to its ability
to rapidly adapt models to new tasks with only limited labeled nodes. Despite
the remarkable progress made by existing graph few-shot learning methods,
several key limitations remain. First, most current approaches rely on
predefined and unified graph filters (e.g., low-pass or high-pass filters) to
globally enhance or suppress node frequency signals. Such fixed spectral
operations fail to account for the heterogeneity of local topological
structures inherent in real-world graphs. Moreover, these methods often assume
that the support and query sets are drawn from the same distribution. However,
under few-shot conditions, the limited labeled data in the support set may not
sufficiently capture the complex distribution of the query set, leading to
suboptimal generalization. To address these challenges, we propose GRACE, a
novel Graph few-shot leaRning framework that integrates Adaptive spectrum
experts with Cross-sEt distribution calibration techniques. Theoretically, the
proposed approach enhances model generalization by adapting to both local
structural variations and cross-set distribution calibration. Empirically,
GRACE consistently outperforms state-of-the-art baselines across a wide range
of experimental settings. Our code can be found here.

</details>


### [37] [Fairness-Constrained Optimization Attack in Federated Learning](https://arxiv.org/abs/2510.12143)
*Harsh Kasyap,Minghong Fang,Zhuqing Liu,Carsten Maple,Somanath Tripathy*

Main category: cs.LG

TL;DR: 本文提出了一种联邦学习中的公平性攻击，恶意客户端通过增加公平性损失来发送有偏见的模型，即使数据分布均匀也能增加系统偏见，同时保持全局准确性难以检测。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然保护了数据隐私，但由于参与者对训练数据的独立性，容易受到投毒攻击。同时，不同数据分布或历史偏见会导致偏见在参与者间传播，即使是无意的。

Method: 提出了一种有意的公平性攻击，恶意客户端在训练时增加公平性损失，通过解决公平性指标（如人口统计均等和均等化几率）的优化问题来计算公平性损失。

Result: 攻击效果显著，即使系统中只有一个恶意客户端，也能将偏见增加高达90%。该攻击对最先进的拜占庭鲁棒和公平感知聚合方案都有效。

Conclusion: 这种公平性攻击具有隐蔽性且难以检测，因为它能在增加偏见的同时保持全局准确性，揭示了联邦学习系统在公平性方面的安全漏洞。

Abstract: Federated learning (FL) is a privacy-preserving machine learning technique
that facilitates collaboration among participants across demographics. FL
enables model sharing, while restricting the movement of data. Since FL
provides participants with independence over their training data, it becomes
susceptible to poisoning attacks. Such collaboration also propagates bias among
the participants, even unintentionally, due to different data distribution or
historical bias present in the data. This paper proposes an intentional
fairness attack, where a client maliciously sends a biased model, by increasing
the fairness loss while training, even considering homogeneous data
distribution. The fairness loss is calculated by solving an optimization
problem for fairness metrics such as demographic parity and equalized odds. The
attack is insidious and hard to detect, as it maintains global accuracy even
after increasing the bias. We evaluate our attack against the state-of-the-art
Byzantine-robust and fairness-aware aggregation schemes over different
datasets, in various settings. The empirical results demonstrate the attack
efficacy by increasing the bias up to 90\%, even in the presence of a single
malicious client in the FL system.

</details>


### [38] [Budget-constrained Active Learning to Effectively De-censor Survival Data](https://arxiv.org/abs/2510.12144)
*Ali Parsaee,Bei Jiang,Zachary Friggstad,Russell Greiner*

Main category: cs.LG

TL;DR: 该论文研究在生存数据集中应用预算学习，允许学习者使用预算来获取部分标记的删失实例，从而改进生存模型。


<details>
  <summary>Details</summary>
Motivation: 标准监督学习需要完整标记的数据集，但在生存数据中存在大量右删失实例，仅知道事件发生时间的下界。预算学习可以模拟现实世界数据收集过程，通过支付预算来获取删失实例的更多信息。

Method: 将最先进的预算学习算法应用于生存数据，允许学习者支付预算来获取删失实例的部分标记信息，如将(3年，删失)更新为(7.2年，未删失)或其他变体。

Result: 实验和理论分析表明，该方法在多个生存任务基准测试中优于其他潜在方法，提供了与标准主动学习方法BatchBALD渐近等价的时间复杂度和边界。

Conclusion: 预算学习在生存数据分析中具有实际应用价值，能够有效利用有限预算获取关键信息来改进模型性能，为现实世界医疗数据收集提供了理论框架。

Abstract: Standard supervised learners attempt to learn a model from a labeled dataset.
Given a small set of labeled instances, and a pool of unlabeled instances, a
budgeted learner can use its given budget to pay to acquire the labels of some
unlabeled instances, which it can then use to produce a model. Here, we explore
budgeted learning in the context of survival datasets, which include (right)
censored instances, where we know only a lower bound on an instance's
time-to-event. Here, that learner can pay to (partially) label a censored
instance -- e.g., to acquire the actual time for an instance [perhaps go from
(3 yr, censored) to (7.2 yr, uncensored)], or other variants [e.g., learn about
one more year, so go from (3 yr, censored) to either (4 yr, censored) or
perhaps (3.2 yr, uncensored)]. This serves as a model of real world data
collection, where follow-up with censored patients does not always lead to
uncensoring, and how much information is given to the learner model during data
collection is a function of the budget and the nature of the data itself. We
provide both experimental and theoretical results for how to apply
state-of-the-art budgeted learning algorithms to survival data and the
respective limitations that exist in doing so. Our approach provides bounds and
time complexity asymptotically equivalent to the standard active learning
method BatchBALD. Moreover, empirical analysis on several survival tasks show
that our model performs better than other potential approaches on several
benchmarks.

</details>


### [39] [Self-Verifying Reflection Helps Transformers with CoT Reasoning](https://arxiv.org/abs/2510.12157)
*Zhongwei Yu,Wannian Xia,Xue Yan,Bo Xu,Haifeng Zhang,Yali Du,Jun Wang*

Main category: cs.LG

TL;DR: 论文提出了一个简约的推理框架，支持小型transformer进行基本的自我验证反思，证明自我验证能保证性能提升，实验显示微小的transformer在整数乘法和数独任务中达到LLM级别的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在推理过程中会进行反思和自我验证，但LLMs在CoT中检测错误的能力有限，反思如何带来实际改进仍不清楚。

Method: 构建一个简约的推理框架，支持小型transformer进行基本的自我验证反思，避免使用自然语言以确保分析清晰度并降低实验成本。

Result: 理论证明如果验证误差得到适当限制，自我验证反思能保证性能提升；实验显示仅几百万参数的小型transformer在整数乘法和数独任务中达到LLM级别的性能。

Conclusion: 将生成式transformer与判别式验证相结合，本质上促进了CoT推理，这种效果与模型规模和自然语言无关。

Abstract: Advanced large language models (LLMs) frequently reflect in reasoning
chain-of-thoughts (CoTs), where they self-verify the correctness of current
solutions and explore alternatives. However, given recent findings that LLMs
detect limited errors in CoTs, how reflection contributes to empirical
improvements remains unclear. To analyze this issue, in this paper, we present
a minimalistic reasoning framework to support basic self-verifying reflection
for small transformers without natural language, which ensures analytic clarity
and reduces the cost of comprehensive experiments. Theoretically, we prove that
self-verifying reflection guarantees improvements if verification errors are
properly bounded. Experimentally, we show that tiny transformers, with only a
few million parameters, benefit from self-verification in both training and
reflective execution, reaching remarkable LLM-level performance in integer
multiplication and Sudoku. Similar to LLM results, we find that reinforcement
learning (RL) improves in-distribution performance and incentivizes frequent
reflection for tiny transformers, yet RL mainly optimizes shallow statistical
patterns without faithfully reducing verification errors. In conclusion,
integrating generative transformers with discriminative verification inherently
facilitates CoT reasoning, regardless of scaling and natural language.

</details>


### [40] [Revisiting Meta-Learning with Noisy Labels: Reweighting Dynamics and Theoretical Guarantees](https://arxiv.org/abs/2510.12209)
*Yiming Zhang,Chester Holtz,Gal Mishne,Alex Cloninger*

Main category: cs.LG

TL;DR: 该论文对元学习样本重加权方法在标签噪声下的训练动态进行了理论分析，揭示了其三个训练阶段，并提出了一种轻量级替代方法。


<details>
  <summary>Details</summary>
Motivation: 元学习样本重加权方法虽然能利用小规模干净数据集缓解标签噪声问题，但其训练行为和动态缺乏理论理解，需要深入分析其工作机制。

Method: 首先对元重加权方法进行严格理论分析，揭示其训练轨迹的三个阶段；然后基于分析提出一种轻量级替代方法，整合均值中心化、行偏移和标签符号调制，避免昂贵的双层优化。

Result: 在合成和真实标签噪声基准测试中，所提出的方法在性能稳定性方面持续优于强重加权/选择基线方法。

Conclusion: 元重加权的训练动态可分为三个明确阶段，其机制依赖于训练信号与干净子集信号的相似性加权耦合；提出的轻量级替代方法能获得更稳定的性能，同时避免计算开销。

Abstract: Learning with noisy labels remains challenging because over-parameterized
networks memorize corrupted supervision. Meta-learning-based sample reweighting
mitigates this by using a small clean subset to guide training, yet its
behavior and training dynamics lack theoretical understanding. We provide a
rigorous theoretical analysis of meta-reweighting under label noise and show
that its training trajectory unfolds in three phases: (i) an alignment phase
that amplifies examples consistent with a clean subset and suppresses
conflicting ones; (ii) a filtering phase driving noisy example weights toward
zero until the clean subset loss plateaus; and (iii) a post-filtering phase in
which noise filtration becomes perturbation-sensitive. The mechanism is a
similarity-weighted coupling between training and clean subset signals together
with clean subset training loss contraction; in the post-filtering regime where
the clean-subset loss is sufficiently small, the coupling term vanishes and
meta-reweighting loses discriminatory power. Guided by this analysis, we
propose a lightweight surrogate for meta-reweighting that integrates
mean-centering, row shifting, and label-signed modulation, yielding more stable
performance while avoiding expensive bi-level optimization. Across synthetic
and real noisy-label benchmarks, our method consistently outperforms strong
reweighting/selection baselines.

</details>


### [41] [DE3S: Dual-Enhanced Soft-Sparse-Shape Learning for Medical Early Time-Series Classification](https://arxiv.org/abs/2510.12214)
*Tao Xie,Zexi Tan,Haoyi Xiao,Binbin Sun,Yiqun Zhang*

Main category: cs.LG

TL;DR: DE3S提出了一种用于医疗早期时间序列分类的双重增强软稀疏形状学习框架，通过双重增强策略、软形状子稀疏化机制和双路径专家混合网络架构，解决了准确性和早期性之间的冲突。


<details>
  <summary>Details</summary>
Motivation: 医疗应用中的早期时间序列分类（如ICU中的脓毒症预测）对时间敏感场景至关重要，但现有方法在准确性和早期性之间存在权衡，难以捕捉早期微弱信号和类别不平衡问题。

Method: 采用双重增强策略（传统时间增强+注意力全局增强）、基于注意力得分的软形状子稀疏化机制、双路径MoE和Inception模块融合架构，使用加权交叉熵损失处理类别不平衡。

Result: 在六个真实医疗数据集上的实验显示达到了最先进的性能，消融研究确认了各组成部分的有效性。

Conclusion: DE3S框架通过精确识别形状子，有效解决了医疗早期时间序列分类中的准确性和早期性冲突问题，具有鲁棒性和高解释性。

Abstract: Early time-series classification (ETSC) in medical applications is crucial
for time-sensitive scenarios such as sepsis prediction in intensive care units
(ICUs), where a large number of deaths are caused by delayed prediction. ETSC
can significantly improve ICU resource utilization efficiency and healthcare
precision. However, it faces conflicting goals of accuracy and earliness, with
existing methods often trading one for the other, struggling to capture subtle
early-stage patterns due to weak initial signals and class imbalance. The key
to solve these challenges is to find shapelets, which are discriminative
subsequences (or shapes) with high interpretability in time-series
classification. This paper proposes Dual-Enhanced Soft-Sparse-Shape Learning
for Medical Early Time-Series Classification (DE3S), which introduces a novel
Dual-Enhanced Soft-Shape Learning framework to figure out shapelets precisely
through three innovations: (1) a comprehensive dual-enhancement strategy
combines traditional temporal augmentation with attention-based global temporal
enhancement for robust representation learning, (2) an attention-score-based
soft shapelet sparsification mechanism dynamically preserves discriminative
patterns while aggregating less important shapelets into representative tokens,
and (3) a dual-path Mixture of Experts Network (MoE) and Inception modules
fusion architecture where MoE performs local learning within shapelets and
multi-scale Inception modules capture global patterns across shapelets. The
framework employs weighted cross-entropy loss for class imbalance handling and
demonstrates robustness on subject-consistency datasets. Extensive experiments
on six real-world medical datasets show state-of-the-art performance, with
ablation studies confirming component efficacy.

</details>


### [42] [Hierarchical Koopman Diffusion: Fast Generation with Interpretable Diffusion Trajectory](https://arxiv.org/abs/2510.12220)
*Hanru Bai,Weiyang Ding,Difan Zou*

Main category: cs.LG

TL;DR: 提出分层Koopman扩散框架，在保持一步采样的同时实现可解释的生成轨迹，解决了扩散模型在快速采样与可解释性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然能生成高质量图像，但迭代去噪过程导致采样速度慢。现有一步方法虽然加速了推理，但牺牲了扩散动力学的可解释性和精细控制能力。

Method: 基于Koopman算子理论，将非线性扩散动态提升到潜在空间，使用全局线性算子控制演化，实现闭式轨迹解。设计了分层架构，通过尺度特定的Koopman子空间在多分辨率上解耦生成动态。

Result: 分层Koopman扩散不仅实现了有竞争力的一步生成性能，还通过谱分析提供了理解和操作生成过程的原理机制。

Conclusion: 该框架弥合了扩散模型中快速采样与可解释性之间的差距，为生成建模中的可解释图像合成开辟了新途径。

Abstract: Diffusion models have achieved impressive success in high-fidelity image
generation but suffer from slow sampling due to their inherently iterative
denoising process. While recent one-step methods accelerate inference by
learning direct noise-to-image mappings, they sacrifice the interpretability
and fine-grained control intrinsic to diffusion dynamics, key advantages that
enable applications like editable generation. To resolve this dichotomy, we
introduce \textbf{Hierarchical Koopman Diffusion}, a novel framework that
achieves both one-step sampling and interpretable generative trajectories.
Grounded in Koopman operator theory, our method lifts the nonlinear diffusion
dynamics into a latent space where evolution is governed by globally linear
operators, enabling closed-form trajectory solutions. This formulation not only
eliminates iterative sampling but also provides full access to intermediate
states, allowing manual intervention during generation. To model the
multi-scale nature of images, we design a hierarchical architecture that
disentangles generative dynamics across spatial resolutions via scale-specific
Koopman subspaces, capturing coarse-to-fine details systematically. We
empirically show that the Hierarchical Koopman Diffusion not only achieves
competitive one-step generation performance but also provides a principled
mechanism for interpreting and manipulating the generative process through
spectral analysis. Our framework bridges the gap between fast sampling and
interpretability in diffusion models, paving the way for explainable image
synthesis in generative modeling.

</details>


### [43] [Unveiling the Vulnerability of Graph-LLMs: An Interpretable Multi-Dimensional Adversarial Attack on TAGs](https://arxiv.org/abs/2510.12233)
*Bowen Fan,Zhilin Guo,Xunkai Li,Yihan Zhou,Bing Zhou,Zhenjun Li,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 提出了IMDGA框架，统一了图结构和文本特征的多维对抗攻击，通过三个紧密集成的模块实现可解释性强、效果显著的攻击，揭示了Graph-LLM在语义维度上的脆弱性。


<details>
  <summary>Details</summary>
Motivation: Graph-LLM虽然通过结合图神经网络和大语言模型增强了表达能力，但这种复杂协同引入了关键脆弱性，现有攻击方法只针对单一维度，缺乏统一的多维攻击框架。

Method: IMDGA框架包含三个紧密集成的模块，能够协调图结构和文本特征的多层次扰动，在保持可解释性的同时实现攻击效果。

Result: 在多个数据集和架构上的实证评估表明，IMDGA在可解释性、攻击效果、隐蔽性和鲁棒性方面均优于现有方法。

Conclusion: 这项工作揭示了Graph-LLM在语义维度上先前未被充分探索的脆弱性，为提升其鲁棒性提供了宝贵见解。

Abstract: Graph Neural Networks (GNNs) have become a pivotal framework for modeling
graph-structured data, enabling a wide range of applications from social
network analysis to molecular chemistry. By integrating large language models
(LLMs), text-attributed graphs (TAGs) enhance node representations with rich
textual semantics, significantly boosting the expressive power of graph-based
learning. However, this sophisticated synergy introduces critical
vulnerabilities, as Graph-LLMs are susceptible to adversarial attacks on both
their structural topology and textual attributes. Although specialized attack
methods have been designed for each of these aspects, no work has yet unified
them into a comprehensive approach. In this work, we propose the Interpretable
Multi-Dimensional Graph Attack (IMDGA), a novel human-centric adversarial
attack framework designed to orchestrate multi-level perturbations across both
graph structure and textual features. IMDGA utilizes three tightly integrated
modules to craft attacks that balance interpretability and impact, enabling a
deeper understanding of Graph-LLM vulnerabilities. Through rigorous theoretical
analysis and comprehensive empirical evaluations on diverse datasets and
architectures, IMDGA demonstrates superior interpretability, attack
effectiveness, stealthiness, and robustness compared to existing methods. By
exposing critical weaknesses in TAG representation learning, this work uncovers
a previously underexplored semantic dimension of vulnerability in Graph-LLMs,
offering valuable insights for improving their resilience. Our code and
resources are publicly available at
https://anonymous.4open.science/r/IMDGA-7289.

</details>


### [44] [MoRA: On-the-fly Molecule-aware Low-Rank Adaptation Framework for LLM-based Multi-Modal Molecular Assistant](https://arxiv.org/abs/2510.12245)
*Tao Yin,Xiaohong Zhang,Jiacheng Zhang,Li Huang,Zhibin Zhang,Yuansong Zeng,Jin Xie,Meng Yan*

Main category: cs.LG

TL;DR: 提出了Molecule-aware Low-Rank Adaptation (MoRA)方法，通过为每个输入分子图生成独特的低秩适配权重，动态注入冻结的LLM中，实现实例特定的参数空间对齐。


<details>
  <summary>Details</summary>
Motivation: 现有多模态对齐方法在处理分子图结构时存在两个主要局限：1）优化所有分子输入的共享参数空间，限制了模型捕捉实例特定结构特征的能力；2）为分子任务微调LLM会导致灾难性遗忘，削弱其通用推理能力。

Method: MoRA为每个输入分子图生成独特的低秩适配权重，这些权重动态注入冻结的LLM中，使模型能够根据每个分子输入的结构调整其推理，同时保留LLM的核心知识。

Result: 在关键分子任务上的实验表明，MoRA的实例特定动态适应优于静态适应的基线方法，包括化学反应预测的精确匹配相对提高14.1%，量子性质预测误差减少22%。

Conclusion: MoRA通过实例特定的动态适应方法，有效解决了分子图结构与LLM集成中的挑战，在保持LLM通用推理能力的同时提升了分子任务的性能。

Abstract: Effectively integrating molecular graph structures with Large Language Models
(LLMs) is a key challenge in drug discovery. Most existing multi-modal
alignment methods typically process these structures by fine-tuning the LLM or
adding a static adapter simultaneously. However, these approaches have two main
limitations: (1) it optimizes a shared parameter space across all molecular
inputs, limiting the model's ability to capture instance-specific structural
features; and (2) fine-tuning the LLM for molecular tasks can lead to
catastrophic forgetting, undermining its general reasoning capabilities. In
this paper, instead of static task-oriented adaptation, we propose an
instance-specific parameter space alignment approach for each molecule
on-the-fly. To this end, we introduce Molecule-aware Low-Rank Adaptation (MoRA)
that produces a unique set of low-rank adaptation weights for each input
molecular graph. These weights are then dynamically injected into a frozen LLM,
allowing the model to adapt its reasoning to the structure of each molecular
input, while preserving the LLM's core knowledge. Extensive experiments
demonstrate that on key molecular tasks, such as chemical reaction prediction
and molecular captioning, MoRA's instance-specific dynamic adaptation
outperforms statically adapted baselines, including a 14.1% relative
improvement in reaction prediction exact match and a 22% reduction in error for
quantum property prediction. The code is available at
https://github.com/jk-sounds/MoRA.

</details>


### [45] [Optimal Regularization for Performative Learning](https://arxiv.org/abs/2510.12249)
*Edwige Cyffers,Alireza Mirrokni,Marco Mondelli*

Main category: cs.LG

TL;DR: 本文研究了在表现性学习中使用正则化处理数据分布随模型部署变化的问题，发现在过参数化情况下表现性效应可能有益，并确定了正则化强度应与表现性效应强度成比例。


<details>
  <summary>Details</summary>
Motivation: 在表现性学习中，数据分布会因部署的模型而发生变化（如用户策略性调整特征），这比传统监督学习更复杂。需要同时优化当前数据模型并考虑模型可能引导分布变化的方向。

Method: 研究正则化在高维岭回归中处理表现性效应的作用，通过理论分析和在合成及真实数据集上的实证评估来验证。

Result: 表现性效应在总体设置下会恶化测试风险，但在过参数化（特征数多于样本数）情况下可能有益。最优正则化强度与表现性效应整体强度成比例。

Conclusion: 正则化可以有效应对表现性效应，特别是在过参数化情况下，且可以通过预期表现性效应强度来设置正则化参数。

Abstract: In performative learning, the data distribution reacts to the deployed model
- for example, because strategic users adapt their features to game it - which
creates a more complex dynamic than in classical supervised learning. One
should thus not only optimize the model for the current data but also take into
account that the model might steer the distribution in a new direction, without
knowing the exact nature of the potential shift. We explore how regularization
can help cope with performative effects by studying its impact in
high-dimensional ridge regression. We show that, while performative effects
worsen the test risk in the population setting, they can be beneficial in the
over-parameterized regime where the number of features exceeds the number of
samples. We show that the optimal regularization scales with the overall
strength of the performative effect, making it possible to set the
regularization in anticipation of this effect. We illustrate this finding
through empirical evaluations of the optimal regularization parameter on both
synthetic and real-world datasets.

</details>


### [46] [Diffusion Models for Reinforcement Learning: Foundations, Taxonomy, and Development](https://arxiv.org/abs/2510.12253)
*Changfu Xu,Jianxiong Guo,Yuzhu Liang,Haiyang Huang,Haodong Zou,Xi Zheng,Shui Yu,Xiaowen Chu,Jiannong Cao,Tian Wang*

Main category: cs.LG

TL;DR: 这篇综述论文系统性地探讨了扩散模型在强化学习中的应用，建立了双轴分类法来组织该领域，并分析了从单智能体到多智能体领域的进展。


<details>
  <summary>Details</summary>
Motivation: 扩散模型作为领先的生成模型，为强化学习提供了多模态表达能力、稳定训练和轨迹级规划等关键优势，需要系统性地梳理这一新兴交叉领域。

Method: 建立了函数导向和技术导向的双轴分类法，分别从扩散模型在强化学习流程中的角色以及在线与离线学习实现两个维度进行组织。

Result: 提供了扩散强化学习的全面分析，涵盖单智能体到多智能体的应用框架，并在多个领域展示了成功的应用案例。

Conclusion: 扩散模型与强化学习的结合是一个有前景的研究方向，需要进一步探索以推动该领域的发展，并维护了相关的GitHub资源库。

Abstract: Diffusion Models (DMs), as a leading class of generative models, offer key
advantages for reinforcement learning (RL), including multi-modal
expressiveness, stable training, and trajectory-level planning. This survey
delivers a comprehensive and up-to-date synthesis of diffusion-based RL. We
first provide an overview of RL, highlighting its challenges, and then
introduce the fundamental concepts of DMs, investigating how they are
integrated into RL frameworks to address key challenges in this research field.
We establish a dual-axis taxonomy that organizes the field along two orthogonal
dimensions: a function-oriented taxonomy that clarifies the roles DMs play
within the RL pipeline, and a technique-oriented taxonomy that situates
implementations across online versus offline learning regimes. We also provide
a comprehensive examination of this progression from single-agent to
multi-agent domains, thereby forming several frameworks for DM-RL integration
and highlighting their practical utility. Furthermore, we outline several
categories of successful applications of diffusion-based RL across diverse
domains, discuss open research issues of current methodologies, and highlight
key directions for future research to advance the field. Finally, we summarize
the survey to identify promising future development directions. We are actively
maintaining a GitHub repository (https://github.com/ChangfuXu/D4RL-FTD) for
papers and other related resources to apply DMs for RL.

</details>


### [47] [FedMMKT:Co-Enhancing a Server Text-to-Image Model and Client Task Models in Multi-Modal Federated Learning](https://arxiv.org/abs/2510.12254)
*Ningxin He,Yang Liu,Wei Sun,Xiaozhou Ye,Ye Ouyang,Tiegang Gao,Zehui Zhang*

Main category: cs.LG

TL;DR: 提出了FedMMKT框架，通过联邦学习实现服务器T2I模型和客户端任务特定模型的协同增强，保护数据隐私的同时利用分散多模态数据。


<details>
  <summary>Details</summary>
Motivation: T2I模型在专业任务适配中受限于任务特定数据的可用性，而现代移动系统和物联网基础设施中的多模态数据提供了重要机会，但存在隐私担忧。

Method: FedMMKT框架，在保护数据隐私的前提下，利用分散的多模态数据协同增强服务器T2I模型和客户端任务特定模型。

Result: 成功实现了服务器和客户端模型的协同增强，同时保护了数据隐私。

Conclusion: FedMMKT为利用分散多模态数据增强T2I模型提供了一种有效的隐私保护解决方案。

Abstract: Text-to-Image (T2I) models have demonstrated their versatility in a wide
range of applications. However, adaptation of T2I models to specialized tasks
is often limited by the availability of task-specific data due to privacy
concerns. On the other hand, harnessing the power of rich multimodal data from
modern mobile systems and IoT infrastructures presents a great opportunity.
This paper introduces Federated Multi-modal Knowledge Transfer (FedMMKT), a
novel framework that enables co-enhancement of a server T2I model and client
task-specific models using decentralized multimodal data without compromising
data privacy.

</details>


### [48] [HiLoRA: Adaptive Hierarchical LoRA Routing for Training-Free Domain Generalization](https://arxiv.org/abs/2510.12266)
*Ziyi Han,Huanyu Wang,Zeyu Zhang,Xiangxiang Dai,Xutong Liu,John C. S. Lui*

Main category: cs.LG

TL;DR: HiLoRA是一个无需训练的分层路由框架，通过自适应选择LoRA模块和细粒度参数激活，解决了现有方法在领域泛化中的参数冗余或不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA复用方法依赖显式任务标签或额外训练，且通常激活固定数量的整个LoRA模块，导致参数冗余或不足，影响性能。

Method: 提出HiLoRA框架，定义rank-one components作为独立单元，在序列级自适应选择LoRA子集并确定ROC分配，在token级进一步细化路由，仅激活最相关的ROCs。

Result: 实验显示HiLoRA在领域泛化方面取得显著改进，准确率比最先进基线提升高达55%，同时保持相当的推理吞吐量。

Conclusion: HiLoRA提供了一种无需训练的高效LoRA复用方法，通过分层路由机制实现了更好的领域泛化性能。

Abstract: Low-Rank Adaptation (LoRA) has emerged as a widely used technique for
adapting large language models (LLMs) to new domains, due to its modular design
and broad availability on platforms such as HuggingFace. This availability has
motivated efforts to reuse existing LoRAs for domain generalization.
  However, existing methods often rely on explicit task labels or additional
training, which are impractical for deployment. Moreover, they typically
activate a fixed number of entire LoRA modules, leading to parameter redundancy
or insufficiency that degrade performance.
  In this paper, we propose \texttt{HiLoRA}, a training-free framework that
performs adaptive hierarchical routing over LoRA pools. Drawing on structural
properties of LoRA, we define rank-one components (ROCs), in which each rank
parameter is regarded as an independent unit. For a given input sequence,
\texttt{HiLoRA} first adaptively selects a subset of LoRAs and determines their
ROC allocation based on Gaussian likelihoods at the sequence level. At the
token level, it further refines routing by activating only the most informative
ROCs.
  We further provide theoretical guarantees that \texttt{HiLoRA} selects the
most relevant LoRAs with high probability.
  Extensive experiments show that \texttt{HiLoRA} achieves substantial
improvements in domain generalization, with accuracy gains of up to {\small
$55\%$} over state-of-the-art baselines, while maintaining comparable inference
throughput.

</details>


### [49] [Multi-Action Self-Improvement for Neural Combinatorial Optimization](https://arxiv.org/abs/2510.12273)
*Laurin Luttmann,Lin Xie*

Main category: cs.LG

TL;DR: 该论文提出了一种基于多智能体联合行动的自改进方法，通过预测完整的智能体-任务分配并利用集合预测损失来提升神经组合优化的效率和协调能力。


<details>
  <summary>Details</summary>
Motivation: 现有自改进方法在神经组合优化中存在计算成本高、无法有效利用多智能体协调结构的问题，且忽略了智能体排列对称性，限制了泛化能力和协调行为学习。

Method: 扩展自改进框架以处理联合多智能体行动，模型架构在每个决策步骤联合预测完整的智能体-任务分配，采用集合预测损失来监督策略学习多个专家分配。

Result: 在多个组合优化问题上验证了方法的有效性，相比标准自改进方法，在最终解质量和生成延迟方面均有显著提升。

Conclusion: 通过多智能体联合行动和集合预测损失，该方法成功提升了自改进在神经组合优化中的样本效率和协调能力，同时大幅加速了解决方案生成过程。

Abstract: Self-improvement has emerged as a state-of-the-art paradigm in Neural
Combinatorial Optimization (NCO), where models iteratively refine their
policies by generating and imitating high-quality solutions. Despite strong
empirical performance, existing methods face key limitations. Training is
computationally expensive, as policy updates require sampling numerous
candidate solutions per instance to extract a single expert trajectory. More
fundamentally, these approaches fail to exploit the structure of combinatorial
problems involving the coordination of multiple agents, such as vehicles in
min-max routing or machines in scheduling. By supervising on single-action
trajectories, they fail to exploit agent-permutation symmetries, where distinct
sequences of actions yield identical solutions, hindering generalization and
the ability to learn coordinated behavior.
  We address these challenges by extending self-improvement to operate over
joint multi-agent actions. Our model architecture predicts complete agent-task
assignments jointly at each decision step. To explicitly leverage symmetries,
we employ a set-prediction loss, which supervises the policy on multiple expert
assignments for any given state. This approach enhances sample efficiency and
the model's ability to learn coordinated behavior. Furthermore, by generating
multi-agent actions in parallel, it drastically accelerates the solution
generation phase of the self-improvement loop. Empirically, we validate our
method on several combinatorial problems, demonstrating consistent improvements
in the quality of the final solution and a reduced generation latency compared
to standard self-improvement.

</details>


### [50] [Deep SPI: Safe Policy Improvement via World Models](https://arxiv.org/abs/2510.12312)
*Florent Delgrange,Raphael Avalos,Willem Röpke*

Main category: cs.LG

TL;DR: 本文提出了DeepSPI算法，将安全策略改进理论扩展到在线深度强化学习场景，结合世界模型和表示学习，提供理论保证的单调改进和收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有安全策略改进理论主要关注离线表格强化学习，缺乏对在线、深度强化学习场景的理论支持，需要将SPI扩展到更一般的在线设置。

Method: 开发理论框架限制策略更新在当前策略的邻域内，结合局部转移和奖励损失与正则化策略更新，提出DeepSPI算法。

Result: 在ALE-57基准测试中，DeepSPI匹配或超越了PPO和DeepMDPs等强基线方法，同时保持理论保证。

Conclusion: 成功将安全策略改进理论扩展到在线深度强化学习，为结合世界模型和表示学习的策略优化提供了理论保证。

Abstract: Safe policy improvement (SPI) offers theoretical control over policy updates,
yet existing guarantees largely concern offline, tabular reinforcement learning
(RL). We study SPI in general online settings, when combined with world model
and representation learning. We develop a theoretical framework showing that
restricting policy updates to a well-defined neighborhood of the current policy
ensures monotonic improvement and convergence. This analysis links transition
and reward prediction losses to representation quality, yielding online, "deep"
analogues of classical SPI theorems from the offline RL literature. Building on
these results, we introduce DeepSPI, a principled on-policy algorithm that
couples local transition and reward losses with regularised policy updates. On
the ALE-57 benchmark, DeepSPI matches or exceeds strong baselines, including
PPO and DeepMDPs, while retaining theoretical guarantees.

</details>


### [51] [Leveraging Teleconnections with Physics-Informed Graph Attention Networks for Long-Range Extreme Rainfall Forecasting in Thailand](https://arxiv.org/abs/2510.12328)
*Kiattikun Chobtham,Kanoksri Sarinnapakorn,Kritanai Torsri,Prattana Deeprasertkul,Jirawan Kamma*

Main category: cs.LG

TL;DR: 提出结合物理信息图神经网络与极值分析技术的新方法，用于改进泰国雨量站的降雨预测，特别关注极端事件预测。


<details>
  <summary>Details</summary>
Motivation: 准确预测降雨特别是极端事件在气候学和地球系统中仍具挑战性，需要改进传统方法的局限性。

Method: 使用图注意力网络与LSTM结合，构建雨量站图结构捕捉时空模式，采用基于地形降水物理的边缘特征，并通过空间季节感知GPD方法进行POT映射。

Result: 该方法在大多数区域（包括极端事件易发区）优于现有基准模型，与SEAS5操作预报系统相比，极端事件预测得到改善。

Conclusion: 该方法为长期水资源管理决策提供了实用的高分辨率地图增强，在极端降雨预测方面具有实际应用价值。

Abstract: Accurate rainfall forecasting, particularly for extreme events, remains a
significant challenge in climatology and the Earth system. This paper presents
novel physics-informed Graph Neural Networks (GNNs) combined with extreme-value
analysis techniques to improve gauge-station rainfall predictions across
Thailand. The model leverages a graph-structured representation of gauge
stations to capture complex spatiotemporal patterns, and it offers
explainability through teleconnections. We preprocess relevant climate indices
that potentially influence regional rainfall. The proposed Graph Attention
Network with Long Short-Term Memory (Attention-LSTM) applies the attention
mechanism using initial edge features derived from simple
orographic-precipitation physics formulation. The embeddings are subsequently
processed by LSTM layers. To address extremes, we perform Peak-Over-Threshold
(POT) mapping using the novel Spatial Season-aware Generalized Pareto
Distribution (GPD) method, which overcomes limitations of traditional
machine-learning models. Experiments demonstrate that our method outperforms
well-established baselines across most regions, including areas prone to
extremes, and remains strongly competitive with the state of the art. Compared
with the operational forecasting system SEAS5, our real-world application
improves extreme-event prediction and offers a practical enhancement to produce
fine-resolution maps that support decision-making in long-term water
management.

</details>


### [52] [Finite-time Convergence Analysis of Actor-Critic with Evolving Reward](https://arxiv.org/abs/2510.12334)
*Rui Hu,Yu Chen,Longbo Huang*

Main category: cs.LG

TL;DR: 首次对马尔可夫采样下具有演化奖励函数的单时间尺度actor-critic算法进行有限时间收敛分析，证明了在奖励参数缓慢变化时仍能达到O(1/√T)的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 许多实用的强化学习算法使用演化奖励函数（如奖励塑造、熵正则化、课程学习），但其理论基础尚不完善。

Method: 分析单时间尺度actor-critic算法在马尔可夫采样下奖励参数随时间步变化的情况，考虑奖励更新对策略优化和价值估计的影响。

Result: 在标准假设下，推导了actor和critic误差的非渐近界，证明当奖励参数演化足够慢时，可以达到O(1/√T)的收敛速率，与静态奖励的最佳已知速率匹配。

Conclusion: 为许多流行的RL技术提供了理论基础，同时在静态奖励情况下改进了分布不匹配分析，将最佳已知速率提高了log²T倍。

Abstract: Many popular practical reinforcement learning (RL) algorithms employ evolving
reward functions-through techniques such as reward shaping, entropy
regularization, or curriculum learning-yet their theoretical foundations remain
underdeveloped. This paper provides the first finite-time convergence analysis
of a single-timescale actor-critic algorithm in the presence of an evolving
reward function under Markovian sampling. We consider a setting where the
reward parameters may change at each time step, affecting both policy
optimization and value estimation. Under standard assumptions, we derive
non-asymptotic bounds for both actor and critic errors. Our result shows that
an $O(1/\sqrt{T})$ convergence rate is achievable, matching the best-known rate
for static rewards, provided the reward parameters evolve slowly enough. This
rate is preserved when the reward is updated via a gradient-based rule with
bounded gradient and on the same timescale as the actor and critic, offering a
theoretical foundation for many popular RL techniques. As a secondary
contribution, we introduce a novel analysis of distribution mismatch under
Markovian sampling, improving the best-known rate by a factor of $\log^2T$ in
the static-reward case.

</details>


### [53] [Traveling Salesman-Based Token Ordering Improves Stability in Homomorphically Encrypted Language Models](https://arxiv.org/abs/2510.12343)
*Donghwan Rho,Sieun Seo,Hyewon Sung,Chohong Min,Ernest K. Ryu*

Main category: cs.LG

TL;DR: 提出基于TSP的token重排序策略和误差后处理，解决同态加密下LLM文本生成的挑战，实现隐私保护的模型推理


<details>
  <summary>Details</summary>
Motivation: 随着用户使用私有信息与LLM交互，需要安全的加密通信。同态加密虽然支持加密数据计算，但文本生成特别是下一词预测仍是实际应用的主要障碍

Method: 采用基于旅行商问题(TSP)的token重排序策略，结合后处理步骤进一步减少近似误差，解决加密文本生成的困难

Result: 理论分析和实验结果表明，该方法防止了生成崩溃，提高了生成文本的连贯性，并在整个过程中保持了数据隐私

Conclusion: 这项工作推进了实用且保护隐私的LLM推理的可行性

Abstract: As users increasingly interact with large language models (LLMs) using
private information, secure and encrypted communication becomes essential.
Homomorphic encryption (HE) provides a principled solution by enabling
computation directly on encrypted data. Although prior work has explored
aspects of running LLMs under HE, the challenge of text generation,
particularly next-token prediction, has received limited attention and remains
a key obstacle to practical encrypted interaction. In this work, we propose a
TSP-based token reordering strategy to address the difficulties of encrypted
text generation, together with a post-processing step that further reduces
approximation error. Theoretical analysis and experimental results demonstrate
that our method prevents collapse, improves coherence in generated text, and
preserves data privacy throughout. Overall, our contributions advance the
feasibility of practical and privacy-preserving LLM inference.

</details>


### [54] [Towards Cross-Modal Error Detection with Tables and Images](https://arxiv.org/abs/2510.12383)
*Olga Ovcharenko,Sebastian Schelter*

Main category: cs.LG

TL;DR: 本文针对多模态数据中的跨模态错误检测问题，在表格数据上对多种方法进行了基准测试，发现Cleanlab和DataScope结合AutoML框架表现最佳，但现有方法在处理真实世界重尾数据时仍有局限。


<details>
  <summary>Details</summary>
Motivation: 大型组织中确保数据质量面临持续挑战，传统错误检测方法通常只关注单一模态（如表），而忽略了电商、医疗等领域中常见的跨模态错误（图像、表格和文本数据共存）。

Method: 在表格数据上进行跨模态错误检测的基准测试，评估了四个数据集和五种基线方法，重点关注Cleanlab（标签错误检测框架）和DataScope（数据估值方法）与AutoML框架的结合。

Result: Cleanlab和DataScope在结合强大的AutoML框架时表现最佳，获得了最高的F1分数。

Conclusion: 当前方法在处理真实世界重尾数据时仍然有限，需要在这一领域进行进一步研究。

Abstract: Ensuring data quality at scale remains a persistent challenge for large
organizations. Despite recent advances, maintaining accurate and consistent
data is still complex, especially when dealing with multiple data modalities.
Traditional error detection and correction methods tend to focus on a single
modality, typically a table, and often miss cross-modal errors that are common
in domains like e-Commerce and healthcare, where image, tabular, and text data
co-exist. To address this gap, we take an initial step towards cross-modal
error detection in tabular data, by benchmarking several methods. Our
evaluation spans four datasets and five baseline approaches. Among them,
Cleanlab, a label error detection framework, and DataScope, a data valuation
method, perform the best when paired with a strong AutoML framework, achieving
the highest F1 scores. Our findings indicate that current methods remain
limited, particularly when applied to heavy-tailed real-world data, motivating
further research in this area.

</details>


### [55] [Enhanced Pre-training of Graph Neural Networks for Million-Scale Heterogeneous Graphs](https://arxiv.org/abs/2510.12401)
*Shengyin Sun,Chen Ma,Jiehao Chen*

Main category: cs.LG

TL;DR: 提出了一种用于大规模异构图预训练图神经网络的有效框架，包含结构感知和语义感知两个预训练任务，以解决异构图预训练中的语义不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有的图神经网络预训练方法主要针对同构图设计，而现实世界中的图大多是异构图。此外，现有方法没有考虑原始数据与理想数据之间的语义不匹配问题，这限制了预训练模型的迁移能力。

Method: 设计了结构感知预训练任务来捕捉异构图的结构特性，以及语义感知预训练任务来处理语义不匹配问题。通过构建由语义邻居组成的扰动子空间，使模型更关注语义空间中的通用知识，从而学习具有更好可迁移性的知识。

Result: 在真实世界的大规模异构图数据集上进行了广泛实验，结果表明该方法优于现有的最先进基线方法。

Conclusion: 提出的框架能够有效预训练异构图神经网络，通过结构感知和语义感知任务的结合，解决了语义不匹配问题，提升了模型在下游任务中的迁移性能。

Abstract: In recent years, graph neural networks (GNNs) have facilitated the
development of graph data mining. However, training GNNs requires sufficient
labeled task-specific data, which is expensive and sometimes unavailable. To be
less dependent on labeled data, recent studies propose to pre-train GNNs in a
self-supervised manner and then apply the pre-trained GNNs to downstream tasks
with limited labeled data. However, most existing methods are designed solely
for homogeneous graphs (real-world graphs are mostly heterogeneous) and do not
consider semantic mismatch (the semantic difference between the original data
and the ideal data containing more transferable semantic information). In this
paper, we propose an effective framework to pre-train GNNs on the large-scale
heterogeneous graph. We first design a structure-aware pre-training task, which
aims to capture structural properties in heterogeneous graphs. Then, we design
a semantic-aware pre-training task to tackle the mismatch. Specifically, we
construct a perturbation subspace composed of semantic neighbors to help deal
with the semantic mismatch. Semantic neighbors make the model focus more on the
general knowledge in the semantic space, which in turn assists the model in
learning knowledge with better transferability. Finally, extensive experiments
are conducted on real-world large-scale heterogeneous graphs to demonstrate the
superiority of the proposed method over state-of-the-art baselines. Code
available at https://github.com/sunshy-1/PHE.

</details>


### [56] [Cautious Weight Decay](https://arxiv.org/abs/2510.12402)
*Lizhang Chen,Jonathan Li,Kaizhao Liang,Baiyu Su,Cong Xie,Nuo Wang Pierse,Chen Liang,Ni Lao,Qiang Liu*

Main category: cs.LG

TL;DR: 提出Cautious Weight Decay(CWD)，一种优化器无关的权重衰减方法，只对与优化器更新方向一致的参数坐标应用权重衰减，无需额外超参数调优。


<details>
  <summary>Details</summary>
Motivation: 标准解耦权重衰减隐式优化正则化或约束目标，CWD旨在保持原始损失函数，寻找未修改目标的局部帕累托最优稳定点。

Method: CWD是一种一行代码修改，仅对与优化器更新方向一致的参数坐标应用权重衰减，具有双层优化解释，在达到稳定流形时诱导滑动模式行为。

Result: 在语言模型预训练和ImageNet分类任务中，CWD在百万到十亿参数规模下持续改善最终损失和准确率。

Conclusion: CWD是AdamW、Lion、Muon等优化器的即插即用改进，无需新超参数或额外调优，能有效提升模型性能。

Abstract: We introduce Cautious Weight Decay (CWD), a one-line, optimizer-agnostic
modification that applies weight decay only to parameter coordinates whose
signs align with the optimizer update. Unlike standard decoupled decay, which
implicitly optimizes a regularized or constrained objective, CWD preserves the
original loss and admits a bilevel interpretation: it induces sliding-mode
behavior upon reaching the stationary manifold, allowing it to search for
locally Pareto-optimal stationary points of the unmodified objective. In
practice, CWD is a drop-in change for optimizers such as AdamW, Lion, and Muon,
requiring no new hyperparameters or additional tuning. For language model
pre-training and ImageNet classification, CWD consistently improves final loss
and accuracy at million- to billion-parameter scales.

</details>


### [57] [Continuous Uniqueness and Novelty Metrics for Generative Modeling of Inorganic Crystals](https://arxiv.org/abs/2510.12405)
*Masahiro Negishi,Hyunsoo Park,Kinga O. Mastej,Aron Walsh*

Main category: cs.LG

TL;DR: 提出两种连续距离函数来评估生成式AI模型生成的无机晶体材料的独特性和新颖性，克服传统晶体距离函数的四个局限性。


<details>
  <summary>Details</summary>
Motivation: 传统晶体距离函数存在四个主要问题：无法量化相似度、不能区分成分和结构差异、缺乏Lipschitz连续性、独特性度量对样本排列不具不变性，这限制了生成模型的有效评估。

Method: 提出两种连续距离函数来评估晶体材料的独特性和新颖性，这些函数理论上克服了传统距离函数的局限性。

Result: 实验表明，新提出的距离函数能够揭示传统距离函数遗漏的洞察，为评估和比较无机晶体生成模型提供了更可靠的基础。

Conclusion: 连续距离函数为评估生成式AI模型在无机晶体材料发现方面提供了更准确和可靠的评估框架。

Abstract: To address pressing scientific challenges such as climate change,
increasingly sophisticated generative artificial intelligence models are being
developed that can efficiently sample the large chemical space of possible
functional materials. These models can quickly sample new chemical compositions
paired with crystal structures. They are typically evaluated using uniqueness
and novelty metrics, which depend on a chosen crystal distance function.
However, the most prevalent distance function has four limitations: it fails to
quantify the degree of similarity between compounds, cannot distinguish
compositional difference and structural difference, lacks Lipschitz continuity
against shifts in atomic coordinates, and results in a uniqueness metric that
is not invariant against the permutation of generated samples. In this work, we
propose using two continuous distance functions to evaluate uniqueness and
novelty, which theoretically overcome these limitations. Our experiments show
that these distances reveal insights missed by traditional distance functions,
providing a more reliable basis for evaluating and comparing generative models
for inorganic crystals.

</details>


### [58] [Bayesian Optimization for Dynamic Pricing and Learning](https://arxiv.org/abs/2510.12447)
*Anush Anand,Pranav Agrawal,Tejas Bodas*

Main category: cs.LG

TL;DR: 提出基于高斯过程的非参数动态定价方法，使用贝叶斯优化来学习未知需求函数，在无限和有限库存场景下都优于传统强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统动态定价方法通常假设需求函数具有特定参数形式，这在现实场景中可能不成立。需要一种不依赖严格建模假设的方法来处理未知需求函数。

Method: 使用高斯过程对需求函数进行非参数建模，基于贝叶斯优化开发定价算法，适用于无限和有限库存两种场景。

Result: 实验表明，基于贝叶斯优化的方法在收益方面优于多种最先进的强化学习算法，需要更少假设且具有更强鲁棒性。

Conclusion: 贝叶斯优化是复杂不确定环境中动态定价的强大实用工具，能够有效学习未知需求函数并实现收益最大化。

Abstract: Dynamic pricing is the practice of adjusting the selling price of a product
to maximize a firm's revenue by responding to market demand. The literature
typically distinguishes between two settings: infinite inventory, where the
firm has unlimited stock and time to sell, and finite inventory, where both
inventory and selling horizon are limited. In both cases, the central challenge
lies in the fact that the demand function -- how sales respond to price -- is
unknown and must be learned from data. Traditional approaches often assume a
specific parametric form for the demand function, enabling the use of
reinforcement learning (RL) to identify near-optimal pricing strategies.
However, such assumptions may not hold in real-world scenarios, limiting the
applicability of these methods. In this work, we propose a Gaussian Process
(GP) based nonparametric approach to dynamic pricing that avoids restrictive
modeling assumptions. We treat the demand function as a black-box function of
the price and develop pricing algorithms based on Bayesian Optimization (BO) --
a sample-efficient method for optimizing unknown functions. We present BO-based
algorithms tailored for both infinite and finite inventory settings and provide
regret guarantees for both regimes, thereby quantifying the learning efficiency
of our methods. Through extensive experiments, we demonstrate that our BO-based
methods outperform several state-of-the-art RL algorithms in terms of revenue,
while requiring fewer assumptions and offering greater robustness. This
highlights Bayesian Optimization as a powerful and practical tool for dynamic
pricing in complex, uncertain environments.

</details>


### [59] [A Function Centric Perspective On Flat and Sharp Minima](https://arxiv.org/abs/2510.12451)
*Israel Mason-Williams,Gabryel Mason-Williams,Helen Yannakoudakis*

Main category: cs.LG

TL;DR: 本文重新审视了平坦最小值与泛化性能的关系，发现正则化会导致更尖锐的最小值，但这些尖锐最小值反而与更好的泛化、校准、鲁棒性和功能一致性相关。


<details>
  <summary>Details</summary>
Motivation: 平坦最小值与泛化性能的关联在近期研究中变得更加复杂，存在理论反例和实证例外。本文旨在重新评估尖锐度在模型性能中的作用。

Method: 进行了广泛的实证研究，从单目标优化到现代图像分类任务，比较了使用不同正则化方法（如SAM、权重衰减、数据增强）的模型。

Result: 发现正则化模型倾向于收敛到更尖锐的最小值，但这些尖锐最小值在泛化、校准、鲁棒性和功能一致性方面表现更好。无正则化的基线模型收敛到更平坦的最小值但性能更差。

Conclusion: 函数复杂性而非平坦度本身决定了解的几何形状，尖锐最小值可以反映更合适的归纳偏置，需要对损失景观几何进行以函数为中心的重新评估。

Abstract: Flat minima are widely believed to correlate with improved generalisation in
deep neural networks. However, this connection has proven more nuanced in
recent studies, with both theoretical counterexamples and empirical exceptions
emerging in the literature. In this paper, we revisit the role of sharpness in
model performance, proposing that sharpness is better understood as a
function-dependent property rather than a reliable indicator of poor
generalisation. We conduct extensive empirical studies, from single-objective
optimisation to modern image classification tasks, showing that sharper minima
often emerge when models are regularised (e.g., via SAM, weight decay, or data
augmentation), and that these sharp minima can coincide with better
generalisation, calibration, robustness, and functional consistency. Across a
range of models and datasets, we find that baselines without regularisation
tend to converge to flatter minima yet often perform worse across all safety
metrics. Our findings demonstrate that function complexity, rather than
flatness alone, governs the geometry of solutions, and that sharper minima can
reflect more appropriate inductive biases (especially under regularisation),
calling for a function-centric reappraisal of loss landscape geometry.

</details>


### [60] [Time-Correlated Video Bridge Matching](https://arxiv.org/abs/2510.12453)
*Viacheslav Vasilev,Arseny Ivanov,Nikita Gushchin,Maria Kovaleva,Alexander Korotin*

Main category: cs.LG

TL;DR: 提出了TCVBM框架，将桥匹配扩展到时间相关视频序列，显式建模序列间依赖关系，在视频帧插值、图像到视频生成和视频超分辨率任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 扩散模型擅长噪声到数据的生成，但在复杂分布间的转换任务中表现不佳，特别是在需要保持时间一致性的视频任务中。桥匹配模型虽然能解决分布间转换问题，但尚未应用于时间相关数据序列。

Method: TCVBM框架扩展桥匹配到时间相关视频序列，在扩散桥中显式建模序列间依赖关系，直接将时间相关性纳入采样过程。

Result: 在视频帧插值、图像到视频生成和视频超分辨率三个任务中，TCVBM在多个定量指标上均优于基于桥匹配和扩散模型的传统方法，展现出更好的生成质量和重建保真度。

Conclusion: TCVBM成功将桥匹配扩展到时间相关视频序列，通过显式建模时间相关性，显著提升了视频相关任务的性能。

Abstract: Diffusion models excel in noise-to-data generation tasks, providing a mapping
from a Gaussian distribution to a more complex data distribution. However they
struggle to model translations between complex distributions, limiting their
effectiveness in data-to-data tasks. While Bridge Matching (BM) models address
this by finding the translation between data distributions, their application
to time-correlated data sequences remains unexplored. This is a critical
limitation for video generation and manipulation tasks, where maintaining
temporal coherence is particularly important. To address this gap, we propose
Time-Correlated Video Bridge Matching (TCVBM), a framework that extends BM to
time-correlated data sequences in the video domain. TCVBM explicitly models
inter-sequence dependencies within the diffusion bridge, directly incorporating
temporal correlations into the sampling process. We compare our approach to
classical methods based on bridge matching and diffusion models for three
video-related tasks: frame interpolation, image-to-video generation, and video
super-resolution. TCVBM achieves superior performance across multiple
quantitative metrics, demonstrating enhanced generation quality and
reconstruction fidelity.

</details>


### [61] [CrossAD: Time Series Anomaly Detection with Cross-scale Associations and Cross-window Modeling](https://arxiv.org/abs/2510.12489)
*Beibu Li,Qichao Shentu,Yang Shu,Hui Zhang,Ming Li,Ning Jin,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: 提出CrossAD框架，通过跨尺度关联和跨窗口建模进行时间序列异常检测，在多个真实数据集上实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有方法独立建模多尺度信息或使用简单特征融合策略，忽略了异常期间跨尺度关联的动态变化，且基于固定滑动窗口限制了上下文信息捕获能力

Method: 提出跨尺度重构从粗粒度序列重构细粒度序列以显式捕获跨尺度关联，设计查询库并融入全局多尺度上下文以克服固定窗口限制

Result: 在多个真实数据集上使用九个评估指标进行广泛实验，验证了CrossAD的有效性

Conclusion: CrossAD通过考虑跨尺度关联和跨窗口建模，在时间序列异常检测中表现出色，达到最先进性能

Abstract: Time series anomaly detection plays a crucial role in a wide range of
real-world applications. Given that time series data can exhibit different
patterns at different sampling granularities, multi-scale modeling has proven
beneficial for uncovering latent anomaly patterns that may not be apparent at a
single scale. However, existing methods often model multi-scale information
independently or rely on simple feature fusion strategies, neglecting the
dynamic changes in cross-scale associations that occur during anomalies.
Moreover, most approaches perform multi-scale modeling based on fixed sliding
windows, which limits their ability to capture comprehensive contextual
information. In this work, we propose CrossAD, a novel framework for time
series Anomaly Detection that takes Cross-scale associations and Cross-window
modeling into account. We propose a cross-scale reconstruction that
reconstructs fine-grained series from coarser series, explicitly capturing
cross-scale associations. Furthermore, we design a query library and
incorporate global multi-scale context to overcome the limitations imposed by
fixed window sizes. Extensive experiments conducted on multiple real-world
datasets using nine evaluation metrics validate the effectiveness of CrossAD,
demonstrating state-of-the-art performance in anomaly detection.

</details>


### [62] [PubSub-VFL: Towards Efficient Two-Party Split Learning in Heterogeneous Environments via Publisher/Subscriber Architecture](https://arxiv.org/abs/2510.12494)
*Yi Liu,Yang Liu,Leqian Zheng,Jue Hong,Junjie Shi,Qingyou Yang,Ye Wu,Cong Wang*

Main category: cs.LG

TL;DR: 提出PubSub-VFL，一种基于发布/订阅架构的新型垂直联邦学习范式，通过分层异步机制解决传统VFL计算资源利用率低和训练效率差的问题，在保持精度的同时实现2-7倍加速和高达91.07%的资源利用率。


<details>
  <summary>Details</summary>
Motivation: 数字经济快速发展推动组织间数据协作，但隐私问题使直接数据共享不可行。现有两方分割学习（垂直联邦学习）存在计算资源利用率低、训练效率差的问题，同步依赖设计增加训练延迟，参与方资源和数据异构性进一步阻碍高效计算。

Method: 利用发布/订阅架构的解耦能力和参数服务器架构的数据并行性，设计分层异步机制；基于参与方系统配置文件形式化优化问题，选择最优超参数同时保护隐私；理论分析证明稳定收敛性和与差分隐私等安全协议的兼容性。

Result: 在五个基准数据集上的案例研究表明，相比最先进基线方法，PubSub-VFL在不损失精度的情况下实现2-7倍训练加速，计算资源利用率高达91.07%。

Conclusion: PubSub-VFL通过发布/订阅架构有效解决了VFL中的效率和资源利用问题，为安全高效的跨组织协作学习提供了可行解决方案。

Abstract: With the rapid advancement of the digital economy, data collaboration between
organizations has become a well-established business model, driving the growth
of various industries. However, privacy concerns make direct data sharing
impractical. To address this, Two-Party Split Learning (a.k.a. Vertical
Federated Learning (VFL)) has emerged as a promising solution for secure
collaborative learning. Despite its advantages, this architecture still suffers
from low computational resource utilization and training efficiency.
Specifically, its synchronous dependency design increases training latency,
while resource and data heterogeneity among participants further hinder
efficient computation. To overcome these challenges, we propose PubSub-VFL, a
novel VFL paradigm with a Publisher/Subscriber architecture optimized for
two-party collaborative learning with high computational efficiency. PubSub-VFL
leverages the decoupling capabilities of the Pub/Sub architecture and the data
parallelism of the parameter server architecture to design a hierarchical
asynchronous mechanism, reducing training latency and improving system
efficiency. Additionally, to mitigate the training imbalance caused by resource
and data heterogeneity, we formalize an optimization problem based on
participants' system profiles, enabling the selection of optimal
hyperparameters while preserving privacy. We conduct a theoretical analysis to
demonstrate that PubSub-VFL achieves stable convergence and is compatible with
security protocols such as differential privacy. Extensive case studies on five
benchmark datasets further validate its effectiveness, showing that, compared
to state-of-the-art baselines, PubSub-VFL not only accelerates training by $2
\sim 7\times$ without compromising accuracy, but also achieves a computational
resource utilization rate of up to 91.07%.

</details>


### [63] [Mitigating the Noise Shift for Denoising Generative Models via Noise Awareness Guidance](https://arxiv.org/abs/2510.12497)
*Jincheng Zhong,Boyuan Jiang,Xin Tao,Pengfei Wan,Kun Gai,Mingsheng Long*

Main category: cs.LG

TL;DR: 本文发现扩散模型采样过程中存在噪声偏移问题，即预定义噪声水平与中间状态实际噪声水平不匹配，并提出噪声感知引导(NAG)方法来纠正这一问题。


<details>
  <summary>Details</summary>
Motivation: 现有去噪生成模型在采样过程中存在长期被忽视但普遍的问题：预定义噪声水平与中间状态实际编码噪声水平之间的不匹配，这导致次优生成质量。

Method: 提出噪声感知引导(NAG)方法，显式引导采样轨迹与预定义噪声调度保持一致；并引入无分类器变体NAG，通过噪声条件丢弃联合训练噪声条件和非条件模型。

Result: 在ImageNet生成和各种监督微调任务上的广泛实验表明，NAG能持续缓解噪声偏移，显著提升主流扩散模型的生成质量。

Conclusion: 噪声偏移是扩散模型中的系统性问题，NAG提供了一种简单有效的解决方案，能显著改善生成性能。

Abstract: Existing denoising generative models rely on solving discretized reverse-time
SDEs or ODEs. In this paper, we identify a long-overlooked yet pervasive issue
in this family of models: a misalignment between the pre-defined noise level
and the actual noise level encoded in intermediate states during sampling. We
refer to this misalignment as noise shift. Through empirical analysis, we
demonstrate that noise shift is widespread in modern diffusion models and
exhibits a systematic bias, leading to sub-optimal generation due to both
out-of-distribution generalization and inaccurate denoising updates. To address
this problem, we propose Noise Awareness Guidance (NAG), a simple yet effective
correction method that explicitly steers sampling trajectories to remain
consistent with the pre-defined noise schedule. We further introduce a
classifier-free variant of NAG, which jointly trains a noise-conditional and a
noise-unconditional model via noise-condition dropout, thereby eliminating the
need for external classifiers. Extensive experiments, including ImageNet
generation and various supervised fine-tuning tasks, show that NAG consistently
mitigates noise shift and substantially improves the generation quality of
mainstream diffusion models.

</details>


### [64] [The Robustness of Differentiable Causal Discovery in Misspecified Scenarios](https://arxiv.org/abs/2510.12503)
*Huiyang Yi,Yanyan He,Duxin Chen,Mingyu Kang,He Wang,Wenwu Yu*

Main category: cs.LG

TL;DR: 本文对主流因果发现算法在模型假设违反情况下的实证表现进行了全面基准测试，发现可微分因果发现方法在多种挑战场景下表现出鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 因果发现算法通常依赖难以在真实数据中满足的因果假设，这限制了其在现实场景中的广泛应用。

Method: 在八种模型假设违反情况下，对多种主流因果发现算法进行广泛基准测试，重点关注可微分因果发现方法。

Result: 实验结果显示，可微分因果发现方法在结构汉明距离和结构干预距离指标下，在常用挑战场景中表现出鲁棒性（除了尺度变化情况）。

Conclusion: 本文旨在为因果发现的合理评估提供标准，并进一步促进其在现实场景中的应用。

Abstract: Causal discovery aims to learn causal relationships between variables from
targeted data, making it a fundamental task in machine learning. However,
causal discovery algorithms often rely on unverifiable causal assumptions,
which are usually difficult to satisfy in real-world data, thereby limiting the
broad application of causal discovery in practical scenarios. Inspired by these
considerations, this work extensively benchmarks the empirical performance of
various mainstream causal discovery algorithms, which assume i.i.d. data, under
eight model assumption violations. Our experimental results show that
differentiable causal discovery methods exhibit robustness under the metrics of
Structural Hamming Distance and Structural Intervention Distance of the
inferred graphs in commonly used challenging scenarios, except for scale
variation. We also provide the theoretical explanations for the performance of
differentiable causal discovery methods. Finally, our work aims to
comprehensively benchmark the performance of recent differentiable causal
discovery methods under model assumption violations, and provide the standard
for reasonable evaluation of causal discovery, as well as to further promote
its application in real-world scenarios.

</details>


### [65] [Multi-Armed Bandits with Minimum Aggregated Revenue Constraints](https://arxiv.org/abs/2510.12523)
*Ahmed Ben Yahmed,Hafedh El Ferchichi,Marc Abeille,Vianney Perchet*

Main category: cs.LG

TL;DR: 该论文研究了具有上下文信息的多臂老虎机问题，目标是在确保每个臂在不同上下文中获得最小聚合奖励的同时，最大化总累积奖励。该框架适用于需要公平收入分配且存在上下文变化的实际应用场景。


<details>
  <summary>Details</summary>
Motivation: 解决现实应用中需要公平收入分配的问题，其中上下文变化是固有的，且需要确保每个臂获得最小奖励约束。跨上下文的最小奖励约束聚合虽然能提升性能和可行性，但带来了技术挑战，特别是缺乏标准MAB设置中常见的闭式最优分配。

Method: 设计并分析了两种算法：乐观优先性能算法和悲观强制约束满足算法。针对每种算法，推导了问题相关的遗憾和约束违反的上界。

Result: 建立了问题相关的遗憾和约束违反上界，并证明了对时间范围的依赖在一般情况下是最优的。揭示了先前工作中利用的自由探索原则的基本局限性。

Conclusion: 该研究为具有跨上下文最小奖励约束的多臂老虎机问题提供了有效的算法解决方案，并建立了理论保证，同时指出了现有方法的局限性。

Abstract: We examine a multi-armed bandit problem with contextual information, where
the objective is to ensure that each arm receives a minimum aggregated reward
across contexts while simultaneously maximizing the total cumulative reward.
This framework captures a broad class of real-world applications where fair
revenue allocation is critical and contextual variation is inherent. The
cross-context aggregation of minimum reward constraints, while enabling better
performance and easier feasibility, introduces significant technical challenges
-- particularly the absence of closed-form optimal allocations typically
available in standard MAB settings. We design and analyze algorithms that
either optimistically prioritize performance or pessimistically enforce
constraint satisfaction. For each algorithm, we derive problem-dependent upper
bounds on both regret and constraint violations. Furthermore, we establish a
lower bound demonstrating that the dependence on the time horizon in our
results is optimal in general and revealing fundamental limitations of the free
exploration principle leveraged in prior work.

</details>


### [66] [Evaluation of Real-Time Preprocessing Methods in AI-Based ECG Signal Analysis](https://arxiv.org/abs/2510.12541)
*Jasmin Freudenberg,Kai Hahn,Christian Weber,Madjid Fathi*

Main category: cs.LG

TL;DR: 分析ECG信号预处理方法在边缘计算环境中的适用性，重点关注能效、处理能力和实时性要求


<details>
  <summary>Details</summary>
Motivation: 便携式ECG系统日益普及，对隐私合规、节能的实时分析需求增长，需要在数据采集点进行信号处理

Method: 分析各种ECG信号预处理步骤，基于能效、处理能力和实时性标准选择适合边缘计算的方法

Result: 为FACE项目开发了结合边缘和云计算优势的机器学习解决方案，用于长期心电图分析

Conclusion: 边缘计算在ECG分析中具有重要作用，不仅降低延迟，还提高数据安全水平

Abstract: The increasing popularity of portable ECG systems and the growing demand for
privacy-compliant, energy-efficient real-time analysis require new approaches
to signal processing at the point of data acquisition. In this context, the
edge domain is acquiring increasing importance, as it not only reduces latency
times, but also enables an increased level of data security. The FACE project
aims to develop an innovative machine learning solution for analysing long-term
electrocardiograms that synergistically combines the strengths of edge and
cloud computing. In this thesis, various pre-processing steps of ECG signals
are analysed with regard to their applicability in the project. The selection
of suitable methods in the edge area is based in particular on criteria such as
energy efficiency, processing capability and real-time capability.

</details>


### [67] [Research in Collaborative Learning Does Not Serve Cross-Silo Federated Learning in Practice](https://arxiv.org/abs/2510.12595)
*Kevin Kuo,Chhavi Yadav,Virginia Smith*

Main category: cs.LG

TL;DR: 本文通过访谈研究揭示了跨组织联邦学习(FL)在实际应用中面临的挑战，包括模型性能担忧、组织间激励和信任问题等，这些挑战与跨设备FL有显著不同。


<details>
  <summary>Details</summary>
Motivation: 尽管GDPR和HIPAA等数据保护法规推动了对跨组织联邦学习的兴趣，但其实际应用仍然有限。本文旨在了解阻碍跨组织联邦学习实际采用的实际挑战。

Method: 采用访谈研究方法，采访了用户组织、软件提供商和学术研究人员等不同利益相关者，收集关于跨组织联邦学习采用障碍的第一手资料。

Result: 研究发现跨组织联邦学习面临一系列现有研究尚未充分捕捉的挑战，包括模型性能担忧、组织间激励问题、信任建立困难等，这些挑战与跨设备联邦学习有本质区别。

Conclusion: 跨组织联邦学习面临独特的实际挑战，需要未来研究关注这些特定问题，以促进该技术的实际应用和推广。

Abstract: Cross-silo federated learning (FL) is a promising approach to enable
cross-organization collaboration in machine learning model development without
directly sharing private data. Despite growing organizational interest driven
by data protection regulations such as GDPR and HIPAA, the adoption of
cross-silo FL remains limited in practice. In this paper, we conduct an
interview study to understand the practical challenges associated with
cross-silo FL adoption. With interviews spanning a diverse set of stakeholders
such as user organizations, software providers, and academic researchers, we
uncover various barriers, from concerns about model performance to questions of
incentives and trust between participating organizations. Our study shows that
cross-silo FL faces a set of challenges that have yet to be well-captured by
existing research in the area and are quite distinct from other forms of
federated learning such as cross-device FL. We end with a discussion on future
research directions that can help overcome these challenges.

</details>


### [68] [Rethinking Knowledge Distillation: A Data Dependent Regulariser With a Negative Asymmetric Payoff](https://arxiv.org/abs/2510.12615)
*Israel Mason-Williams,Gabryel Mason-Williams,Helen Yannakoudakis*

Main category: cs.LG

TL;DR: 知识蒸馏的功能性影响被重新评估，研究发现其作为压缩机制的效果有限，更多表现为数据依赖的正则化器，且存在负知识不对称转移的安全风险。


<details>
  <summary>Details</summary>
Motivation: 理解知识蒸馏的功能性影响，量化其压缩能力和知识转移机制，解耦压缩与架构缩减的关系。

Method: 采用假设检验、控制组和随机控制蒸馏方法，分析多种蒸馏变体和模型规模下的蒸馏缩放规律。

Result: 在某些模态和架构中存在显著知识转移，但程度低于预期；显著转移时存在严重的负知识不对称转移；知识蒸馏主要作为数据依赖的正则化器而非压缩机制。

Conclusion: 知识蒸馏的功能性压缩能力有限，更多起到正则化作用，且存在负知识不对称转移的安全隐患，需要在应用中谨慎考虑。

Abstract: Knowledge distillation is often considered a compression mechanism when
judged on the resulting student's accuracy and loss, yet its functional impact
is poorly understood. In this work, we quantify the compression capacity of
knowledge distillation and the resulting knowledge transfer from a functional
perspective, decoupling compression from architectural reduction, which
provides an improved understanding of knowledge distillation. We employ
hypothesis testing, controls, and random control distillation to understand
knowledge transfer mechanisms across data modalities. To rigorously test the
breadth and limits of our analyses, we explore multiple distillation variants
and analyse distillation scaling laws across model sizes. Our findings
demonstrate that, while there is statistically significant knowledge transfer
in some modalities and architectures, the extent of this transfer is less
pronounced than anticipated, even under conditions designed to maximise
knowledge sharing. Notably, in cases of significant knowledge transfer, we
identify a consistent and severe asymmetric transfer of negative knowledge to
the student, raising safety concerns in knowledge distillation applications.
Across 12 experimental setups, 9 architectures, and 7 datasets, our findings
show that knowledge distillation functions less as a compression mechanism and
more as a data-dependent regulariser with a negative asymmetric payoff.

</details>


### [69] [Towards Fast Coarse-graining and Equation Discovery with Foundation Inference Models](https://arxiv.org/abs/2510.12618)
*Manuel Hinz,Maximilian Mauel,Patrick Seifner,David Berghaus,Kostadin Cvejoski,Ramses J. Sanchez*

Main category: cs.LG

TL;DR: 提出使用预训练的基础推理模型来解耦高维动力学系统的降维问题，将变量发现与动力学建模分离，实现快速可重用的粗粒化流程


<details>
  <summary>Details</summary>
Motivation: 高维动力学过程通常由低维流形上的少量有效变量控制，传统方法需要同时解决变量发现和动力学建模两个耦合问题，训练过程复杂且不稳定

Method: 利用预训练的基础推理模型来估计动力学系统的无穷小生成器，冻结模型权重，仅训练编码器-解码器映射，定义简单的模拟一致性损失

Result: 在嵌入合成视频数据的随机双井势系统上进行了概念验证，展示了该方法在快速可重用粗粒化流程中的潜力

Conclusion: 通过解耦变量发现与动力学建模，利用预训练模型可以稳定表示学习，为复杂系统的粗粒化提供更高效的解决方案

Abstract: High-dimensional recordings of dynamical processes are often characterized by
a much smaller set of effective variables, evolving on low-dimensional
manifolds. Identifying these latent dynamics requires solving two intertwined
problems: discovering appropriate coarse-grained variables and simultaneously
fitting the governing equations. Most machine learning approaches tackle these
tasks jointly by training autoencoders together with models that enforce
dynamical consistency. We propose to decouple the two problems by leveraging
the recently introduced Foundation Inference Models (FIMs). FIMs are pretrained
models that estimate the infinitesimal generators of dynamical systems (e.g.,
the drift and diffusion of a stochastic differential equation) in zero-shot
mode. By amortizing the inference of the dynamics through a FIM with frozen
weights, and training only the encoder-decoder map, we define a simple,
simulation-consistent loss that stabilizes representation learning. A proof of
concept on a stochastic double-well system with semicircle diffusion, embedded
into synthetic video data, illustrates the potential of this approach for fast
and reusable coarse-graining pipelines.

</details>


### [70] [Learning-To-Measure: In-context Active Feature Acquisition](https://arxiv.org/abs/2510.12624)
*Yuta Kobayashi,Zilin Jing,Jiayu Yao,Hongseok Namkoong,Shalmali Joshi*

Main category: cs.LG

TL;DR: 本文提出了元主动特征获取（meta-AFA）问题和Learning-to-Measure（L2M）方法，通过跨任务学习特征获取策略，在标签稀缺和高缺失率情况下优于任务特定基线。


<details>
  <summary>Details</summary>
Motivation: 传统主动特征获取方法通常针对单一预定任务，缺乏可扩展性。实际应用中，模型需要从具有系统性特征缺失和有限任务特定标签的回顾性数据中学习。

Method: L2M包含：i）对未见任务的可靠不确定性量化；ii）基于不确定性的贪婪特征获取代理，最大化条件互信息。采用序列建模或自回归预训练方法支持任意缺失模式下的不确定性量化。

Result: 在合成和真实世界表格基准测试中，L2M匹配或超越任务特定基线方法，特别是在标签稀缺和高缺失率情况下表现更优。

Conclusion: L2M能够直接在具有回顾性缺失的数据集上操作，执行元AFA任务的上下文学习，无需每个任务重新训练，展示了跨任务特征获取策略学习的有效性。

Abstract: Active feature acquisition (AFA) is a sequential decision-making problem
where the goal is to improve model performance for test instances by adaptively
selecting which features to acquire. In practice, AFA methods often learn from
retrospective data with systematic missingness in the features and limited
task-specific labels. Most prior work addresses acquisition for a single
predetermined task, limiting scalability. To address this limitation, we
formalize the meta-AFA problem, where the goal is to learn acquisition policies
across various tasks. We introduce Learning-to-Measure (L2M), which consists of
i) reliable uncertainty quantification over unseen tasks, and ii) an
uncertainty-guided greedy feature acquisition agent that maximizes conditional
mutual information. We demonstrate a sequence-modeling or autoregressive
pre-training approach that underpins reliable uncertainty quantification for
tasks with arbitrary missingness. L2M operates directly on datasets with
retrospective missingness and performs the meta-AFA task in-context,
eliminating per-task retraining. Across synthetic and real-world tabular
benchmarks, L2M matches or surpasses task-specific baselines, particularly
under scarce labels and high missingness.

</details>


### [71] [Laminar: A Scalable Asynchronous RL Post-Training Framework](https://arxiv.org/abs/2510.12633)
*Guangming Sheng,Yuxuan Tong,Borui Wan,Wang Zhang,Chaobo Jia,Xibin Wu,Yuqi Wu,Xiang Li,Chi Zhang,Yanghua Peng,Haibin Lin,Xin Liu,Chuan Wu*

Main category: cs.LG

TL;DR: Laminar是一个用于大规模语言模型强化学习后训练的系统，通过完全解耦的架构解决了现有RL框架因轨迹生成延迟高度偏斜导致的GPU利用率低下问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习框架在大型集群上扩展性有限，因为RL轨迹生成的极端长尾偏斜性导致严重的GPU利用率低下。当前的异步RL系统依赖全局权重同步，这种刚性模型更新计划不适合RL训练中高度偏斜和演化的轨迹生成延迟分布。

Method: 1. 用中继工作者层替换全局更新，作为分布式参数服务，实现异步和细粒度权重同步；2. 动态重新打包机制将长尾轨迹整合到少数专用rollout上，最大化生成吞吐量；3. 完全解耦设计隔离故障，确保长时间运行作业的鲁棒性。

Result: 在1024-GPU集群上的评估显示，Laminar相比最先进系统实现了高达5.48倍的训练吞吐量加速，同时减少了模型收敛时间。

Conclusion: Laminar通过轨迹级异步和完全解耦架构，有效解决了大规模RL后训练中的可扩展性问题，显著提升了训练效率和鲁棒性。

Abstract: Reinforcement learning (RL) post-training for Large Language Models (LLMs) is
now scaling to large clusters and running for extended durations to enhance
model reasoning performance. However, the scalability of existing RL frameworks
is limited, as extreme long-tail skewness in RL trajectory generation causes
severe GPU underutilization. Current asynchronous RL systems attempt to
mitigate this, but they rely on global weight synchronization between the actor
and all rollouts, which creates a rigid model update schedule. This global
synchronization is ill-suited for the highly skewed and evolving distribution
of trajectory generation latency in RL training, crippling training efficiency.
Our key insight is that efficient scaling requires breaking this lockstep
through trajectory-level asynchrony, which generates and consumes each
trajectory independently. We propose Laminar, a scalable and robust RL
post-training system built on a fully decoupled architecture. First, we replace
global updates with a tier of relay workers acting as a distributed parameter
service. This enables asynchronous and fine-grained weight synchronization,
allowing rollouts to pull the latest weight anytime without stalling the
actor's training loop. Second, a dynamic repack mechanism consolidates
long-tail trajectories onto a few dedicated rollouts, maximizing generation
throughput. The fully decoupled design also isolates failures, ensuring
robustness for long-running jobs. Our evaluation on a 1024-GPU cluster shows
that Laminar achieves up to 5.48$\times$ training throughput speedup over
state-of-the-art systems, while reducing model convergence time.

</details>


### [72] [Expert or not? assessing data quality in offline reinforcement learning](https://arxiv.org/abs/2510.12638)
*Arip Asadulaev,Fakhri Karray,Martin Takac*

Main category: cs.LG

TL;DR: 提出Bellman Wasserstein距离(BWD)来评估离线强化学习数据集质量，无需训练智能体即可预测算法性能，并能作为正则化器提升策略优化效果


<details>
  <summary>Details</summary>
Motivation: 离线强化学习数据集质量差异很大，但事先难以评估数据质量，因为数据来源和技能组成未知，需要一种无需训练智能体就能估计数据集质量的方法

Method: 引入Bellman Wasserstein距离(BWD)，这是一种基于价值感知的最优传输评分，通过行为评论家和状态条件最优传输公式计算，衡量数据集行为策略与随机参考策略的差异

Result: 在D4RL MuJoCo任务中，BWD与多个离线RL算法的oracle性能得分强相关，能有效预测标准智能体在给定数据集上的表现，作为正则化器使用时能提升回报

Conclusion: BWD等基于价值感知的分布信号是筛选离线RL数据集和优化策略的实用工具

Abstract: Offline reinforcement learning (RL) learns exclusively from static datasets,
without further interaction with the environment. In practice, such datasets
vary widely in quality, often mixing expert, suboptimal, and even random
trajectories. The choice of algorithm therefore depends on dataset fidelity.
Behavior cloning can suffice on high-quality data, whereas mixed- or
low-quality data typically benefits from offline RL methods that stitch useful
behavior across trajectories. Yet in the wild it is difficult to assess dataset
quality a priori because the data's provenance and skill composition are
unknown. We address the problem of estimating offline dataset quality without
training an agent. We study a spectrum of proxies from simple cumulative
rewards to learned value based estimators, and introduce the Bellman
Wasserstein distance (BWD), a value aware optimal transport score that measures
how dissimilar a dataset's behavioral policy is from a random reference policy.
BWD is computed from a behavioral critic and a state conditional OT
formulation, requiring no environment interaction or full policy optimization.
Across D4RL MuJoCo tasks, BWD strongly correlates with an oracle performance
score that aggregates multiple offline RL algorithms, enabling efficient
prediction of how well standard agents will perform on a given dataset. Beyond
prediction, integrating BWD as a regularizer during policy optimization
explicitly pushes the learned policy away from random behavior and improves
returns. These results indicate that value aware, distributional signals such
as BWD are practical tools for triaging offline RL datasets and policy
optimization.

</details>


### [73] [On Foundation Models for Temporal Point Processes to Accelerate Scientific Discovery](https://arxiv.org/abs/2510.12640)
*David Berghaus,Patrick Seifner,Kostadin Cvejoski,Ramses J. Sanchez*

Main category: cs.LG

TL;DR: 提出了一种用于事件序列分析的基础模型，通过在大规模模拟数据上训练，能够无需重新训练即可快速适应新的科学数据集。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型需要为每个新数据集从头构建和训练，过程缓慢且成本高昂，限制了科学发现的速度。

Method: 训练一个基础模型，在数百万个模拟事件序列上学习事件的底层模式，使其具备通用的事件理解能力。

Result: 该模型能够即时分析新的科学数据而无需重新训练，仅需查看少量示例即可适应，并且可以快速微调以获得更高精度。

Conclusion: 这种方法使复杂的事件分析更加易于获取，并加速了科学发现的步伐。

Abstract: Many scientific fields, from medicine to seismology, rely on analyzing
sequences of events over time to understand complex systems. Traditionally,
machine learning models must be built and trained from scratch for each new
dataset, which is a slow and costly process. We introduce a new approach: a
single, powerful model that learns the underlying patterns of event data in
context. We trained this "foundation model" on millions of simulated event
sequences, teaching it a general-purpose understanding of how events can
unfold. As a result, our model can analyze new scientific data instantly,
without retraining, simply by looking at a few examples from the dataset. It
can also be quickly fine-tuned for even higher accuracy. This approach makes
sophisticated event analysis more accessible and accelerates the pace of
scientific discovery.

</details>


### [74] [Towards Foundation Inference Models that Learn ODEs In-Context](https://arxiv.org/abs/2510.12650)
*Maximilian Mauel,Manuel Hinz,Patrick Seifner,David Berghaus,Ramses J. Sanchez*

Main category: cs.LG

TL;DR: FIM-ODE是一个预训练的神经模型，能够从稀疏和噪声观测中零样本推断常微分方程。


<details>
  <summary>Details</summary>
Motivation: 常微分方程描述了确定性连续时间动态系统，但在稀疏或噪声数据下的数据驱动建模仍然具有挑战性。

Method: 使用合成数据训练，采用灵活的神经算子进行鲁棒的ODE推断，即使从损坏数据中也能工作。

Result: FIM-ODE提供了准确的估计，与神经最先进方法相当，并定性比较了估计向量场的结构。

Conclusion: FIM-ODE是一个有效的零样本ODE推断模型，在稀疏和噪声数据下表现良好。

Abstract: Ordinary differential equations (ODEs) describe dynamical systems evolving
deterministically in continuous time. Accurate data-driven modeling of systems
as ODEs, a central problem across the natural sciences, remains challenging,
especially if the data is sparse or noisy. We introduce FIM-ODE (Foundation
Inference Model for ODEs), a pretrained neural model designed to estimate ODEs
zero-shot (i.e., in context) from sparse and noisy observations. Trained on
synthetic data, the model utilizes a flexible neural operator for robust ODE
inference, even from corrupted data. We empirically verify that FIM-ODE
provides accurate estimates, on par with a neural state-of-the-art method, and
qualitatively compare the structure of their estimated vector fields.

</details>


### [75] [SG-XDEAT: Sparsity-Guided Cross-Dimensional and Cross-Encoding Attention with Target-Aware Conditioning in Tabular Learning](https://arxiv.org/abs/2510.12659)
*Chih-Chuan Cheng,Yi-Ju Tseng*

Main category: cs.LG

TL;DR: SG-XDEAT是一个用于表格数据监督学习的新框架，通过双流编码器将特征分解为原始值和目标感知两个并行表示，结合交叉维度注意力、交叉编码注意力和自适应稀疏注意力机制，在多个基准测试中表现出优于基线的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的表格数据深度学习方法在处理噪声和有效利用目标信息方面存在不足，需要一种能够同时建模原始特征和目标感知特征、并能自适应过滤噪声的鲁棒框架。

Method: 采用双流编码器分解特征为原始值流和目标感知流，使用交叉维度注意力捕获流内特征依赖，交叉编码注意力实现双向交互，自适应稀疏注意力机制动态抑制低效用标记。

Result: 在多个公开基准测试中取得了持续的性能提升，验证了联合建模原始视图和目标感知视图并自适应过滤噪声的有效性。

Conclusion: SG-XDEAT通过双流表示学习和自适应噪声抑制，提供了一个更鲁棒的深度表格学习解决方案，证明了联合建模原始和目标感知特征的重要性。

Abstract: We propose SG-XDEAT (Sparsity-Guided Cross Dimensional and Cross-Encoding
Attention with Target Aware Conditioning), a novel framework designed for
supervised learning on tabular data. At its core, SG-XDEAT employs a
dual-stream encoder that decomposes each input feature into two parallel
representations: a raw value stream and a target-conditioned (label-aware)
stream. These dual representations are then propagated through a hierarchical
stack of attention-based modules. SG-XDEAT integrates three key components: (i)
Cross-Dimensional self-attention, which captures intra-view dependencies among
features within each stream; (ii) Cross-Encoding self-attention, which enables
bidirectional interaction between raw and target-aware representations; and
(iii) an Adaptive Sparse Self-Attention (ASSA) mechanism, which dynamically
suppresses low-utility tokens by driving their attention weights toward
zero--thereby mitigating the impact of noise. Empirical results on multiple
public benchmarks show consistent gains over strong baselines, confirming that
jointly modeling raw and target-aware views--while adaptively filtering
noise--yields a more robust deep tabular learner.

</details>


### [76] [Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models](https://arxiv.org/abs/2510.12666)
*Prasenjit K Mudi,Anshi Sachan,Dahlia Devapriya,Sheetal Kalyani*

Main category: cs.LG

TL;DR: 提出了一种针对Whisper模型的轻量化框架，通过结构化稀疏和权重感知剪枝，在保持WER性能的同时显著减少模型参数、内存消耗和FLOPs。


<details>
  <summary>Details</summary>
Motivation: Whisper模型在语音识别方面取得了显著进展，但其大尺寸限制了在资源受限的边缘设备上的部署。

Method: 使用稀疏组LASSO惩罚作为损失正则化器来强制结构化稀疏，提出权重统计感知剪枝算法，并设计了自定义文本规范化器用于WER评估。

Result: 在Common Voice 11.0印地语数据集上，Whisper-small参数减少35.4%，内存消耗降低14.25%，FLOPs减少18.5%；Whisper-medium参数减少31%，内存降低15.29%，FLOPs减少16.95%；大幅优于现有迭代幅度剪枝方法，多剪枝18.7%参数且WER降低12.31%。

Conclusion: 该框架成功实现了Whisper模型的轻量化，在不降低WER性能的前提下显著减少了计算和存储需求，为边缘设备部署提供了可行方案。

Abstract: Whisper models have achieved remarkable progress in speech recognition; yet
their large size remains a bottleneck for deployment on resource-constrained
edge devices. This paper proposes a framework to design fine-tuned variants of
Whisper which address the above problem. Structured sparsity is enforced via
the Sparse Group LASSO penalty as a loss regularizer, to reduce the number of
FLOating Point operations (FLOPs). Further, a weight statistics aware pruning
algorithm is proposed. We also design our custom text normalizer for WER
evaluation. On Common Voice 11.0 Hindi dataset, we obtain, without degrading
WER, (a) 35.4% reduction in model parameters, 14.25% lower memory consumption
and 18.5% fewer FLOPs on Whisper-small, and (b) 31% reduction in model
parameters, 15.29% lower memory consumption and 16.95% fewer FLOPs on
Whisper-medium; and, (c) substantially outperform the state-of-the-art
Iterative Magnitude Pruning based method by pruning 18.7% more parameters along
with a 12.31 reduction in WER.

</details>


### [77] [Structure-Aware Spectral Sparsification via Uniform Edge Sampling](https://arxiv.org/abs/2510.12669)
*Kaiwen He,Petros Drineas,Rajiv Khanna*

Main category: cs.LG

TL;DR: 该论文证明在具有良好分离k聚类的图中，均匀边采样可以替代传统基于有效电阻的谱稀疏化方法，保留谱聚类所需的谱子空间。


<details>
  <summary>Details</summary>
Motivation: 传统谱聚类依赖特征向量计算，在大规模图上扩展性差。基于有效电阻的稀疏化方法需要昂贵的预处理，研究是否简单的均匀边采样就能满足谱聚类需求。

Method: 使用均匀边采样方法，采样O(γ²n log n/ε²)条边，其中γ是拉普拉斯条件数。引入新的簇内边电阻边界、秩(n-k)有效电阻公式和适应主导特征空间的矩阵切尔诺夫边界。

Result: 证明在具有良好分离k聚类的图中，均匀采样得到的稀疏化器保持了用于聚类的谱子空间，确保谱嵌入的忠实性和聚类质量。

Conclusion: 在强聚类性条件下，均匀边采样具有结构感知能力，为均匀边采样在结构保持谱聚类中的有效性提供了首个理论保证。

Abstract: Spectral clustering is a fundamental method for graph partitioning, but its
reliance on eigenvector computation limits scalability to massive graphs.
Classical sparsification methods preserve spectral properties by sampling edges
proportionally to their effective resistances, but require expensive
preprocessing to estimate these resistances. We study whether uniform edge
sampling-a simple, structure-agnostic strategy-can suffice for spectral
clustering. Our main result shows that for graphs admitting a well-separated
$k$-clustering, characterized by a large structure ratio $\Upsilon(k) =
\lambda_{k+1} / \rho_G(k)$, uniform sampling preserves the spectral subspace
used for clustering. Specifically, we prove that uniformly sampling $O(\gamma^2
n \log n / \epsilon^2)$ edges, where $\gamma$ is the Laplacian condition
number, yields a sparsifier whose top $(n-k)$-dimensional eigenspace is
approximately orthogonal to the cluster indicators. This ensures that the
spectral embedding remains faithful, and clustering quality is preserved. Our
analysis introduces new resistance bounds for intra-cluster edges, a
rank-$(n-k)$ effective resistance formulation, and a matrix Chernoff bound
adapted to the dominant eigenspace. These tools allow us to bypass importance
sampling entirely. Conceptually, our result connects recent coreset-based
clustering theory to spectral sparsification, showing that under strong
clusterability, even uniform sampling is structure-aware. This provides the
first provable guarantee that uniform edge sampling suffices for
structure-preserving spectral clustering.

</details>


### [78] [Keep Calm and Avoid Harmful Content: Concept Alignment and Latent Manipulation Towards Safer Answers](https://arxiv.org/abs/2510.12672)
*Ruben Belo,Claudia Soares,Marta Guimaraes*

Main category: cs.LG

TL;DR: CALM是一种推理时方法，通过修改模型最后一层的潜在表示来抑制有害概念，无需重新训练，结合了计算机视觉中的CW技术和正交投影。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易受到越狱攻击，这些攻击会绕过内置的安全防护机制（例如通过对抗性提示欺骗模型）。

Method: 使用计算机视觉中的CW技术结合正交投影，移除与有害内容相关的潜在方向，同时保持模型性能。

Result: CALM减少了有害输出，在大多数指标上优于基线方法，仅带来较小的推理计算开销。

Conclusion: CALM提供了一种轻量级的AI安全方法，无需额外训练数据或模型微调，仅需在推理时进行少量计算。

Abstract: Large Language Models are susceptible to jailbreak attacks that bypass
built-in safety guardrails (e.g., by tricking the model with adversarial
prompts). We propose Concept Alignment and Concept Manipulation \textbf{CALM},
an inference-time method that suppresses harmful concepts by modifying latent
representations of the last layer of the model, without retraining. Leveraging
\gls*{cw} technique from Computer Vision combined with orthogonal projection,
CALM removes unwanted latent directions associated with harmful content while
preserving model performance. Experiments show that CALM reduces harmful
outputs and outperforms baseline methods in most metrics, offering a
lightweight approach to AI safety with no additional training data or model
fine-tuning, while incurring only a small computational overhead at inference.

</details>


### [79] [Demystifying Hybrid Thinking: Can LLMs Truly Switch Between Think and No-Think?](https://arxiv.org/abs/2510.12680)
*Shouren Wang,Wang Yang,Xianxuan Long,Qifan Wang,Vipin Chaudhary,Xiaotian Han*

Main category: cs.LG

TL;DR: 本文分析了当前混合思维LLMs存在的问题，发现推理行为会泄露到非思维模式中，提出了影响可控性的四个关键因素，并设计了一个实用方案来改善模式分离效果。


<details>
  <summary>Details</summary>
Motivation: 混合思维LLMs能够在推理和直接回答之间切换，平衡效率和推理能力，但现有模型存在模式分离不彻底的问题，推理行为会泄露到非思维模式中。

Method: 通过实验分析影响可控性的因素，发现四个关键因素：更大的数据规模、使用不同问题的思维和非思维答案、适度增加非思维数据量、采用两阶段训练策略。基于这些发现提出了改进方案。

Result: 提出的方案相比标准训练，能在保持两种模式准确率的同时，显著减少非思维模式的输出长度（从1085降至585）和推理支持性标记的出现次数（从5917降至522）。

Conclusion: 当前混合思维存在局限性，研究结果为加强其可控性提供了方向，提出的实用方案能有效改善模式分离效果。

Abstract: Hybrid thinking enables LLMs to switch between reasoning and direct
answering, offering a balance between efficiency and reasoning capability. Yet
our experiments reveal that current hybrid thinking LLMs only achieve partial
mode separation: reasoning behaviors often leak into the no-think mode. To
understand and mitigate this, we analyze the factors influencing
controllability and identify four that matter most: (1) larger data scale, (2)
using think and no-think answers from different questions rather than the same
question, (3) a moderate increase in no-think data number, and (4) a two-phase
strategy that first trains reasoning ability and then applies hybrid think
training. Building on these findings, we propose a practical recipe that,
compared to standard training, can maintain accuracy in both modes while
significantly reducing no-think output length (from $1085$ to $585$ on MATH500)
and occurrences of reasoning-supportive tokens such as ``\texttt{wait}'' (from
$5917$ to $522$ on MATH500). Our findings highlight the limitations of current
hybrid thinking and offer directions for strengthening its controllability.

</details>


### [80] [CoRA: Covariate-Aware Adaptation of Time Series Foundation Models](https://arxiv.org/abs/2510.12681)
*Guo Qin,Zhi Chen,Yong Liu,Zhiyuan Shi,Haixuan Liu,Xiangdong Huang,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

TL;DR: 提出了CoRA框架，通过Granger因果嵌入和零初始化条件注入机制，将多模态外生协变量整合到时间序列基础模型中，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型大多在单变量时间序列上预训练，无法充分利用现实预测任务中的多变量协变量信息，限制了模型性能。

Method: CoRA框架保持基础模型骨干网络冻结作为特征提取器，使用Granger因果嵌入自动评估协变量的因果预测能力，并通过零初始化条件注入机制逐步整合外生信息。

Result: 在协变量感知预测任务上实现了31.1%的MSE降低，在完整样本和少样本训练场景下均优于现有方法，且与多种先进TSFM兼容。

Conclusion: CoRA为时间序列基础模型的实际应用提供了实用范式，能够有效整合多模态协变量信息，避免灾难性遗忘，显著提升预测性能。

Abstract: Time Series Foundation Models (TSFMs) have shown significant impact through
their model capacity, scalability, and zero-shot generalization. However, due
to the heterogeneity of inter-variate dependencies and the backbone scalability
on large-scale multivariate datasets, most TSFMs are typically pre-trained on
univariate time series. This limitation renders them oblivious to crucial
information from diverse covariates in real-world forecasting tasks. To further
enhance the performance of TSFMs, we propose a general covariate-aware
adaptation (CoRA) framework for TSFMs. It leverages pre-trained backbones of
foundation models while effectively incorporating exogenous covariates from
various modalities, including time series, language, and images, to improve the
quality of predictions. Technically, CoRA maintains the equivalence of
initialization and parameter consistency during adaptation. With preserved
backbones of foundation models as frozen feature extractors, the outcome
embeddings from foundation models are empirically demonstrated more informative
than raw data. Further, CoRA employs a novel Granger Causality Embedding (GCE)
to automatically evaluate covariates regarding their causal predictability with
respect to the target variate. We incorporate these weighted embeddings with a
zero-initialized condition-injection mechanism, avoiding catastrophic
forgetting of pre-trained foundation models and gradually integrates exogenous
information. Extensive experiments show that CoRA of TSFMs surpasses
state-of-the-art covariate-aware deep forecasters with full or few-shot
training samples, achieving 31.1% MSE reduction on covariate-aware forecasting.
Compared to other adaptation methods, CoRA exhibits strong compatibility with
various advanced TSFMs and extends the scope of covariates to other modalities,
presenting a practical paradigm for the application of TSFMs.

</details>


### [81] [Few Shot Semi-Supervised Learning for Abnormal Stop Detection from Sparse GPS Trajectories](https://arxiv.org/abs/2510.12686)
*Muhammad Ayub Sabir,Junbiao Pang,Jiaqi Wu,Fatima Ashraf*

Main category: cs.LG

TL;DR: 提出了一种针对城际客车异常停靠检测的稀疏感知方法，通过自适应分割、局部时间指标引导调整和图神经网络，在稀疏GPS数据和有限标签条件下实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 城际客车异常停靠检测对乘客安全和运营可靠性至关重要，但面临GPS轨迹稀疏和标注数据有限两大挑战，现有方法假设密集采样或规则运动模式，适用性受限。

Method: 1) 稀疏感知分割(SAS)基于局部时空密度定义段边界；2) 三个领域特定指标捕捉异常停靠行为；3) 局部时间指标引导调整(LTIGA)通过局部相似图平滑指标；4) 构建时空图，标签传播扩展弱监督，GCN学习关系模式；5) 自训练模块集成高置信度伪标签迭代改进预测。

Result: 在真实客车数据上的实验显示，仅使用10个标注实例就达到AUC 0.854和AP 0.866，优于现有方法。

Conclusion: 该方法有效解决了数据稀疏和标签稀缺问题，在城际客车异常停靠检测中表现出色，代码和数据集已公开。

Abstract: Abnormal stop detection (ASD) in intercity coach transportation is critical
for ensuring passenger safety, operational reliability, and regulatory
compliance. However, two key challenges hinder ASD effectiveness: sparse GPS
trajectories, which obscure short or unauthorized stops, and limited labeled
data, which restricts supervised learning. Existing methods often assume dense
sampling or regular movement patterns, limiting their applicability. To address
data sparsity, we propose a Sparsity-Aware Segmentation (SAS) method that
adaptively defines segment boundaries based on local spatial-temporal density.
Building upon these segments, we introduce three domain-specific indicators to
capture abnormal stop behaviors. To further mitigate the impact of sparsity, we
develop Locally Temporal-Indicator Guided Adjustment (LTIGA), which smooths
these indicators via local similarity graphs. To overcome label scarcity, we
construct a spatial-temporal graph where each segment is a node with
LTIGA-refined features. We apply label propagation to expand weak supervision
across the graph, followed by a GCN to learn relational patterns. A final
self-training module incorporates high-confidence pseudo-labels to iteratively
improve predictions. Experiments on real-world coach data show an AUC of 0.854
and AP of 0.866 using only 10 labeled instances, outperforming prior methods.
The code and dataset are publicly available at
\href{https://github.com/pangjunbiao/Abnormal-Stop-Detection-SSL.git}

</details>


### [82] [DiffEM: Learning from Corrupted Data with Diffusion Models via Expectation Maximization](https://arxiv.org/abs/2510.12691)
*Danial Hosseintabar,Fan Chen,Giannis Daras,Antonio Torralba,Constantinos Daskalakis*

Main category: cs.LG

TL;DR: 提出DiffEM方法，使用期望最大化(EM)从损坏数据训练扩散模型，通过条件扩散模型重建干净数据并迭代优化


<details>
  <summary>Details</summary>
Motivation: 扩散模型在高维逆问题中表现出强大能力，但仅从损坏或噪声观测中学习扩散模型仍然具有挑战性

Method: DiffEM方法结合期望最大化：E步使用条件扩散模型从观测重建干净数据，M步用重建数据精炼条件扩散模型

Result: 理论上为DiffEM迭代提供了单调收敛保证，并在多种图像重建任务中验证了方法的有效性

Conclusion: DiffEM方法能够有效从损坏数据中训练扩散模型，为高维逆问题提供了新的解决方案

Abstract: Diffusion models have emerged as powerful generative priors for
high-dimensional inverse problems, yet learning them when only corrupted or
noisy observations are available remains challenging. In this work, we propose
a new method for training diffusion models with Expectation-Maximization (EM)
from corrupted data. Our proposed method, DiffEM, utilizes conditional
diffusion models to reconstruct clean data from observations in the E-step, and
then uses the reconstructed data to refine the conditional diffusion model in
the M-step. Theoretically, we provide monotonic convergence guarantees for the
DiffEM iteration, assuming appropriate statistical conditions. We demonstrate
the effectiveness of our approach through experiments on various image
reconstruction tasks.

</details>


### [83] [Topological Signatures of ReLU Neural Network Activation Patterns](https://arxiv.org/abs/2510.12700)
*Vicente Bosca,Tatum Rask,Sunia Tanweer,Andrew R. Tawfeek,Branden Stone*

Main category: cs.LG

TL;DR: 该论文研究了ReLU神经网络激活模式的拓扑特征，分析了网络在特征空间中诱导的多面体分解，发现二元分类中Fiedler划分与决策边界相关，回归任务中同调性与训练损失存在相似模式。


<details>
  <summary>Details</summary>
Motivation: 探索ReLU神经网络激活模式的拓扑结构特征，理解网络在特征空间中的几何分解如何反映其决策行为。

Method: 分析前馈ReLU网络在特征空间中的多面体分解，研究二元分类中双图的Fiedler划分与决策边界的关系，计算回归任务中胞腔分解的同调性。

Result: 发现二元分类中Fiedler划分与决策边界存在相关性，回归任务中训练损失与多面体胞腔数量呈现相似的变化模式。

Conclusion: ReLU神经网络的拓扑特征能够反映其决策行为，为理解神经网络内部工作机制提供了新的几何视角。

Abstract: This paper explores the topological signatures of ReLU neural network
activation patterns. We consider feedforward neural networks with ReLU
activation functions and analyze the polytope decomposition of the feature
space induced by the network. Mainly, we investigate how the Fiedler partition
of the dual graph and show that it appears to correlate with the decision
boundary -- in the case of binary classification. Additionally, we compute the
homology of the cellular decomposition -- in a regression task -- to draw
similar patterns in behavior between the training loss and polyhedral
cell-count, as the model is trained.

</details>


### [84] [Multitask finetuning and acceleration of chemical pretrained models for small molecule drug property prediction](https://arxiv.org/abs/2510.12719)
*Matthew Adrian,Yunsie Chung,Kevin Boyd,Saee Paliwal,Srimukh Prasad Veccham,Alan C. Cheng*

Main category: cs.LG

TL;DR: 该论文研究了在化学预训练图神经网络模型上进行多任务微调的方法，发现这种方法能显著提升药物发现中关键端点的预测性能，特别是在大数据量时效果更明显。


<details>
  <summary>Details</summary>
Motivation: 利用化学预训练模型提取的通用化学知识来改进药物发现关键端点（如靶点效力和ADMET性质）的预测，并探索多任务学习在微调过程中的作用。

Method: 在化学预训练图神经网络模型（如KERMT和KGPT）上进行多任务微调，并与未经预训练的图神经网络模型进行对比。

Result: 多任务微调显著提升了性能，且在大数据量时效果最为明显。同时发布了两个多任务ADMET数据集分割用于基准测试。

Conclusion: 多任务微调化学预训练模型是提升药物性质预测性能的有效方法，特别是在大数据场景下，为工业药物发现工作流程提供了加速实现。

Abstract: Chemical pretrained models, sometimes referred to as foundation models, are
receiving considerable interest for drug discovery applications. The general
chemical knowledge extracted from self-supervised training has the potential to
improve predictions for critical drug discovery endpoints, including on-target
potency and ADMET properties. Multi-task learning has previously been
successfully leveraged to improve predictive models. Here, we show that
enabling multitasking in finetuning of chemical pretrained graph neural network
models such as Kinetic GROVER Multi-Task (KERMT), an enhanced version of the
GROVER model, and Knowledge-guided Pre-training of Graph Transformer (KGPT)
significantly improves performance over non-pretrained graph neural network
models. Surprisingly, we find that the performance improvement from finetuning
KERMT in a multitask manner is most significant at larger data sizes.
Additionally, we publish two multitask ADMET data splits to enable more
accurate benchmarking of multitask deep learning methods for drug property
prediction. Finally, we provide an accelerated implementation of the KERMT
model on GitHub, unlocking large-scale pretraining, finetuning, and inference
in industrial drug discovery workflows.

</details>


### [85] [CARVQ: Corrective Adaptor with Group Residual Vector Quantization for LLM Embedding Compression](https://arxiv.org/abs/2510.12721)
*Dayin Gou,Sanghyun Byun,Nilesh Malpeddi,Gabrielle De Micheli,Prathamesh Vaste,Jacob Song,Woo Seong Chung*

Main category: cs.LG

TL;DR: CARVQ是一种结合校正适配器和分组残差向量量化的后训练压缩方法，可将LLM嵌入层压缩至约1.6比特，无需专用硬件支持，显著减少内存占用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的词嵌入层参数庞大，导致存储需求高和内存占用大，特别是在边缘设备上成为内存瓶颈。压缩嵌入层可以释放内存带宽并加速推理。

Method: 采用校正适配器与分组残差向量量化相结合的方法，通过线性和非线性映射的组合来模拟原始模型嵌入，实现高效压缩。

Result: 在LLaMA-3.2-1B、LLaMA-3.2-3B、Qwen2.5-7B等多个模型上测试，CARVQ在大多数情况下能实现更低的平均比特数，同时保持合理的困惑度和准确率，优于标量化方法。

Conclusion: CARVQ提供了一种与最先进变换器量化方法兼容的压缩技术，可在支持4位内存的任何硬件上无缝集成，是实现LLM在边缘设备高效部署的重要进展。

Abstract: Large Language Models (LLMs) typically rely on a large number of parameters
for token embedding, leading to substantial storage requirements and memory
footprints. In particular, LLMs deployed on edge devices are memory-bound, and
reducing the memory footprint by compressing the embedding layer not only frees
up the memory bandwidth but also speeds up inference. To address this, we
introduce CARVQ, a post-training novel Corrective Adaptor combined with group
Residual Vector Quantization. CARVQ relies on the composition of both linear
and non-linear maps and mimics the original model embedding to compress to
approximately 1.6 bits without requiring specialized hardware to support
lower-bit storage. We test our method on pre-trained LLMs such as LLaMA-3.2-1B,
LLaMA-3.2-3B, LLaMA-3.2-3B-Instruct, LLaMA-3.1-8B, Qwen2.5-7B, Qwen2.5-Math-7B
and Phi-4, evaluating on common generative, discriminative, math and reasoning
tasks. We show that in most cases, CARVQ can achieve lower average
bitwidth-per-parameter while maintaining reasonable perplexity and accuracy
compared to scalar quantization. Our contributions include a novel compression
technique that is compatible with state-of-the-art transformer quantization
methods and can be seamlessly integrated into any hardware supporting 4-bit
memory to reduce the model's memory footprint in memory-constrained devices.
This work demonstrates a crucial step toward the efficient deployment of LLMs
on edge devices.

</details>


### [86] [Improving Decision Trees through the Lens of Parameterized Local Search](https://arxiv.org/abs/2510.12726)
*Juha Harviainen,Frank Sommer,Manuel Sorge*

Main category: cs.LG

TL;DR: 该论文研究了决策树中局部搜索操作（调整切割阈值或交换特征）的复杂性，发现这些问题是NP完全的，但通过参数化复杂性分析确定了使问题可处理的组合条件。


<details>
  <summary>Details</summary>
Motivation: 研究决策树学习算法中常见的局部搜索操作（调整切割阈值和交换特征）的复杂性，以理解哪些因素导致问题困难，哪些因素使问题可解。

Method: 使用参数化复杂性分析，考虑特征数量d和域大小D等参数，开发了时间复杂度为(D+1)^(2d)·|I|^O(1)的算法，并进行了概念验证实现和实证评估。

Result: 发现这些问题在一般情况下是NP完全的，但当特征数量d和域大小D都很小时，问题变得固定参数可解。算法实现验证了理论分析的有效性。

Conclusion: 决策树局部搜索操作的复杂性取决于特征数量和域大小的组合，当两者都较小时，问题可以在多项式时间内解决，这为实际应用提供了理论指导。

Abstract: Algorithms for learning decision trees often include heuristic local-search
operations such as (1) adjusting the threshold of a cut or (2) also exchanging
the feature of that cut. We study minimizing the number of classification
errors by performing a fixed number of a single type of these operations.
Although we discover that the corresponding problems are NP-complete in
general, we provide a comprehensive parameterized-complexity analysis with the
aim of determining those properties of the problems that explain the hardness
and those that make the problems tractable. For instance, we show that the
problems remain hard for a small number $d$ of features or small domain size
$D$ but the combination of both yields fixed-parameter tractability. That is,
the problems are solvable in $(D + 1)^{2d} \cdot |I|^{O(1)}$ time, where $|I|$
is the size of the input. We also provide a proof-of-concept implementation of
this algorithm and report on empirical results.

</details>


### [87] [Hierarchical Federated Learning for Crop Yield Prediction in Smart Agricultural Production Systems](https://arxiv.org/abs/2510.12727)
*Anas Abouaomar,Mohammed El hanjri,Abdellatif Kobbane,Anis Laouiti,Khalid Nafil*

Main category: cs.LG

TL;DR: 提出了一种用于智能农业和作物产量预测的分层联邦学习架构，通过季节性订阅机制和三层结构实现作物特定模型的本地专业化与全局泛化。


<details>
  <summary>Details</summary>
Motivation: 解决农业环境中异构农场数据和隐私敏感数据的问题，同时实现作物特定模型的本地专业化与跨作物知识的全局集成。

Method: 采用三层架构：客户端智能农场、作物特定聚合器和全局模型聚合器，结合季节性订阅机制让农场在每季开始时加入作物特定集群。

Result: 实验表明本地和作物层模型能紧密跟踪实际产量模式，显著优于标准机器学习模型，验证了分层联邦学习在农业环境中的优势。

Conclusion: 分层联邦学习架构在农业场景中有效平衡了本地专业化与全局泛化，同时保护数据隐私并减少通信开销。

Abstract: In this paper, we presents a novel hierarchical federated learning
architecture specifically designed for smart agricultural production systems
and crop yield prediction. Our approach introduces a seasonal subscription
mechanism where farms join crop-specific clusters at the beginning of each
agricultural season. The proposed three-layer architecture consists of
individual smart farms at the client level, crop-specific aggregators at the
middle layer, and a global model aggregator at the top level. Within each crop
cluster, clients collaboratively train specialized models tailored to specific
crop types, which are then aggregated to produce a higher-level global model
that integrates knowledge across multiple crops. This hierarchical design
enables both local specialization for individual crop types and global
generalization across diverse agricultural contexts while preserving data
privacy and reducing communication overhead. Experiments demonstrate the
effectiveness of the proposed system, showing that local and crop-layer models
closely follow actual yield patterns with consistent alignment, significantly
outperforming standard machine learning models. The results validate the
advantages of hierarchical federated learning in the agricultural context,
particularly for scenarios involving heterogeneous farming environments and
privacy-sensitive agricultural data.

</details>


### [88] [Doctor Rashomon and the UNIVERSE of Madness: Variable Importance with Unobserved Confounding and the Rashomon Effect](https://arxiv.org/abs/2510.12734)
*Jon Donnelly,Srikar Katta,Emanuele Borgonovo,Cynthia Rudin*

Main category: cs.LG

TL;DR: UNIVERSE方法通过使用Rashomon集合（近似最优模型集合）来处理变量重要性分析中的未观测变量问题，为真实变量重要性提供边界估计。


<details>
  <summary>Details</summary>
Motivation: 传统变量重要性方法依赖于单一模型和观测到的特征，但实际中重要变量常被遗漏，且不同模型会给出不同的变量重要性（Rashomon效应）。

Method: 采用Rashomon集合方法，考虑所有近似最优模型，在存在未观测变量的情况下为变量重要性提供稳健的边界估计。

Result: 理论保证了方法的稳健性，在半合成模拟中表现良好，并在信用风险评估任务中展示了实用性。

Conclusion: UNIVERSE方法能够有效处理变量重要性分析中的未观测变量问题和Rashomon效应，提供更可靠的变量重要性估计。

Abstract: Variable importance (VI) methods are often used for hypothesis generation,
feature selection, and scientific validation. In the standard VI pipeline, an
analyst estimates VI for a single predictive model with only the observed
features. However, the importance of a feature depends heavily on which other
variables are included in the model, and essential variables are often omitted
from observational datasets. Moreover, the VI estimated for one model is often
not the same as the VI estimated for another equally-good model - a phenomenon
known as the Rashomon Effect. We address these gaps by introducing
UNobservables and Inference for Variable importancE using Rashomon SEts
(UNIVERSE). Our approach adapts Rashomon sets - the sets of near-optimal models
in a dataset - to produce bounds on the true VI even with missing features. We
theoretically guarantee the robustness of our approach, show strong performance
on semi-synthetic simulations, and demonstrate its utility in a credit risk
task.

</details>


### [89] [KoALA: KL-L0 Adversarial Detector via Label Agreement](https://arxiv.org/abs/2510.12752)
*Siqi Li,Yasser Shoukry*

Main category: cs.LG

TL;DR: KoALA是一种无需语义信息、无需架构修改或对抗训练的新型对抗性检测器，通过KL散度和L0相似度两种互补度量之间的预测分歧来检测对抗攻击。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络极易受到对抗攻击的威胁，这对安全和关键应用构成重大风险，需要开发轻量级、即插即用的检测解决方案。

Method: 使用KL散度（对密集低幅扰动敏感）和L0相似度（对稀疏高影响变化敏感）两种互补度量，当两种度量的类别预测不一致时检测到对抗攻击。仅需在预训练图像编码器上使用干净图像进行简单微调。

Result: 在ResNet/CIFAR-10上达到0.94的精确度和0.81的召回率，在CLIP/Tiny-ImageNet上达到0.66的精确度和0.85的召回率。当定理条件满足时，能持续有效检测对抗样本。

Conclusion: KoALA提供了一种轻量级、即插即用的对抗检测解决方案，适用于现有模型和多种数据模态，无需对抗训练或架构修改。

Abstract: Deep neural networks are highly susceptible to adversarial attacks, which
pose significant risks to security- and safety-critical applications. We
present KoALA (KL-L0 Adversarial detection via Label Agreement), a novel,
semantics-free adversarial detector that requires no architectural changes or
adversarial retraining. KoALA operates on a simple principle: it detects an
adversarial attack when class predictions from two complementary similarity
metrics disagree. These metrics-KL divergence and an L0-based similarity-are
specifically chosen to detect different types of perturbations. The KL
divergence metric is sensitive to dense, low-amplitude shifts, while the
L0-based similarity is designed for sparse, high-impact changes. We provide a
formal proof of correctness for our approach. The only training required is a
simple fine-tuning step on a pre-trained image encoder using clean images to
ensure the embeddings align well with both metrics. This makes KOALA a
lightweight, plug-and-play solution for existing models and various data
modalities. Our extensive experiments on ResNet/CIFAR-10 and CLIP/Tiny-ImageNet
confirm our theoretical claims. When the theorem's conditions are met, KoALA
consistently and effectively detects adversarial examples. On the full test
sets, KoALA achieves a precision of 0.94 and a recall of 0.81 on
ResNet/CIFAR-10, and a precision of 0.66 and a recall of 0.85 on
CLIP/Tiny-ImageNet.

</details>


### [90] [Sample-Efficient Omniprediction for Proper Losses](https://arxiv.org/abs/2510.12769)
*Isaac Gibbs,Ryan J. Tibshirani*

Main category: cs.LG

TL;DR: 本文研究了为多个决策者构建概率预测的问题，目标是设计一个单一预测器，能同时最小化多个损失函数，实现omniprediction。


<details>
  <summary>Details</summary>
Motivation: 现有omniprediction方法存在局限性：基于多校准的方法样本复杂度次优，而基于对抗游戏的方法会产生复杂的随机化预测器。需要开发更直接、非随机化的高效算法。

Method: 提出了一种新的非随机化算法，利用proper loss集合的结构特性，避免了现有方法中的在线到批量转换和随机化过程。

Result: 证明了多校准比omniprediction更难，前者必然导致次优样本复杂度。新算法在保持样本效率的同时，返回更简单的确定性预测器。

Conclusion: 通过利用proper loss的结构特性，可以设计出比现有方法更直接、更简单的omniprediction算法，避免了复杂随机化过程，同时保持样本效率。

Abstract: We consider the problem of constructing probabilistic predictions that lead
to accurate decisions when employed by downstream users to inform actions. For
a single decision maker, designing an optimal predictor is equivalent to
minimizing a proper loss function corresponding to the negative utility of that
individual. For multiple decision makers, our problem can be viewed as a
variant of omniprediction in which the goal is to design a single predictor
that simultaneously minimizes multiple losses. Existing algorithms for
achieving omniprediction broadly fall into two categories: 1) boosting methods
that optimize other auxiliary targets such as multicalibration and obtain
omniprediction as a corollary, and 2) adversarial two-player game based
approaches that estimate and respond to the ``worst-case" loss in an online
fashion. We give lower bounds demonstrating that multicalibration is a strictly
more difficult problem than omniprediction and thus the former approach must
incur suboptimal sample complexity. For the latter approach, we discuss how
these ideas can be used to obtain a sample-efficient algorithm through an
online-to-batch conversion. This conversion has the downside of returning a
complex, randomized predictor. We improve on this method by designing a more
direct, unrandomized algorithm that exploits structural elements of the set of
proper losses.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [91] [Mathematical aspects of the decomposition of diagonal U(N) operators](https://arxiv.org/abs/2510.11735)
*M. M. Fedin,A. A. Morozov*

Main category: quant-ph

TL;DR: 本文证明了任意对角算子可分解为较小矩阵的张量和矩阵乘积，重点分析了所得公式的解析结构及其内在对称性，并引入图示表示法来清晰展示分解结构。


<details>
  <summary>Details</summary>
Motivation: 研究对角算子的分解结构及其对称性，为量子计算算法优化、复杂生物分析、晶体学、AI模型优化等多个领域提供数学工具。

Method: 使用张量和矩阵乘积分解方法，引入图示表示法来可视化分解结构，并分析分解中的对称性。

Result: 成功证明了任意对角算子可分解为较小矩阵的张量和矩阵乘积，建立了清晰的分解公式和图示表示方法。

Conclusion: 提出的分解方法和图示表示具有广泛的应用价值，可用于量子计算、生物分析、晶体学和AI优化等多个领域。

Abstract: We prove the decomposition of arbitrary diagonal operators into tensor and
matrix products of smaller matrices, focusing on the analytic structure of the
resulting formulas and their inherent symmetries. Diagrammatic representations
are introduced, providing clear visualizations of the structure of these
decompositions. We also discuss symmetries of the suggested decomposition.
Methods and representations developed in this paper can be applied in different
areas, including optimization of quantum computing algorithms, complex
biological analysis, crystallography, optimization of AI models, and others.

</details>


### [92] [Quantum Kernel Methods: Convergence Theory, Separation Bounds and Applications to Marketing Analytics](https://arxiv.org/abs/2510.11744)
*Laura Sáez-Ortuño,Santiago Forgas-Coll,Massimiliano Ferrara*

Main category: quant-ph

TL;DR: 该研究探索了在NISQ时代将量子核方法应用于真实消费者分类任务的可行性，提出了结合量子核SVM和量子特征提取的混合管道，在模拟和有限硬件运行中展示了相对于经典方法的竞争优势。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索在NISQ（含噪声中等规模量子）时代，量子核方法在真实世界分类任务中的应用潜力，为量子-经典混合工作流程和硬件集成提供初步验证。

Method: 采用混合管道方法，结合量子核支持向量机（Q-SVM）和量子特征提取模块（QFE），在模拟和浅层深度硬件运行中与经典和量子基线进行基准测试。

Result: 在固定超参数下，Q-SVM达到0.7790准确率、0.7647精确率、0.8609召回率、0.8100 F1分数和0.83 ROC AUC，相比经典SVM展现出更高的灵敏度同时保持竞争力的精确度。

Conclusion: 这些结果应被视为NISQ时代工作流程和硬件集成的初步指标和具体起点，而非决定性基准。方法学上，研究支持通过浅层但富有表达力的量子嵌入来实现稳健可分离性，尽管存在硬件噪声约束。

Abstract: This work studies the feasibility of applying quantum kernel methods to a
real consumer classification task in the NISQ regime. We present a hybrid
pipeline that combines a quantum-kernel Support Vector Machine (Q-SVM) with a
quantum feature extraction module (QFE), and benchmark it against classical and
quantum baselines in simulation and with limited shallow-depth hardware runs.
With fixed hyperparameters, the proposed Q-SVM attains 0.7790 accuracy, 0.7647
precision, 0.8609 recall, 0.8100 F1, and 0.83 ROC AUC, exhibiting higher
sensitivity while maintaining competitive precision relative to classical SVM.
We interpret these results as an initial indicator and a concrete starting
point for NISQ-era workflows and hardware integration, rather than a definitive
benchmark. Methodologically, our design aligns with recent work that formalizes
quantum-classical separations and verifies resources via XEB-style approaches,
motivating shallow yet expressive quantum embeddings to achieve robust
separability despite hardware noise constraints.

</details>


### [93] [Back-reflection in dipole fields and beyond](https://arxiv.org/abs/2510.11764)
*Maksim Valialshchikov,Felix Karbstein,Daniel Seipt,Matt Zepf*

Main category: quant-ph

TL;DR: 本文研究了偶极脉冲中的光-光散射，重点关注四波混频产生的背反射信号，通过贝叶斯优化确定了最大化信号可探测性的三光束碰撞配置。


<details>
  <summary>Details</summary>
Motivation: 探索量子反射在实际应用中的实现可能性，特别是在背反射通道中，利用偶极脉冲实现最紧密的光聚焦。

Method: 使用偶极泵浦和高斯探针的全光学设置，研究多聚焦脉冲配置（带形配置）作为理想偶极脉冲的近似，应用贝叶斯优化方法确定最优参数。

Result: 优化结果表明三光束碰撞配置能最大化背反射信号的可探测性，该配置通过数值和解析方法进行了深入研究。

Conclusion: 三光束碰撞配置是实现量子反射背反射信号探测的最优方案，为量子真空效应的实验验证提供了可行路径。

Abstract: Quantum reflection is a fascinating signature of the quantum vacuum that
emerges from inhomogeneities in the electromagnetic fields. In pursuit of the
prospective real-world implementation of quantum reflection in the
back-reflection channel, we provide the first numerical estimates for the
light-by-light scattering with dipole pulses, which are known to provide the
tightest focusing of light possible. For an all-optical setup with a dipole
pump and Gaussian probe of the same frequency, we find that the dominant signal
signature is related mainly to the back-reflection channel from 4-wave mixing.
Focusing on this, we study the particular case of a multiple focusing pulses
configuration (belt configuration) as an approximation to the idealized dipole
pulse. Using Bayesian optimization methods, we determine optimal parameters
that maximize the detectability of a discernible back-reflection signal. Our
study indicates that the optimization favors a three-beam collision setup,
which we further investigate both numerically and analytically.

</details>


### [94] [Exact WKB method for radial Schrödinger equation](https://arxiv.org/abs/2510.11766)
*Okuto Morikawa,Shoya Ogawa*

Main category: quant-ph

TL;DR: 该论文重新审视了径向薛定谔问题的精确WKB量子化，重点讨论了如何选择和解释"物理上有意义"的量子化路径，解决了基于复兴的量子化中路径选择的争议。


<details>
  <summary>Details</summary>
Motivation: 从现代复兴视角重新审视径向薛定谔问题的精确WKB量子化，澄清数学单值性数据与物理边界条件的匹配关系，解决近期关于路径选择的争论。

Method: 使用简单转折点和正则奇点处的连接公式，计算非平凡循环数据来得到能谱。对3维谐振子和3维库仑势，显式计算从正无穷出发、环绕原点后返回正无穷的闭合路径。

Result: 提出了闭合路径的适当切片提供了r=0处的物理局域基，该基被从原点到无穷的开路径使用。分析阐明了在径向设置中数学单值性数据与物理边界条件如何协调一致。

Conclusion: 该研究解决了基于复兴的量子化中路径选择的争议，澄清了径向设置中数学单值性数据与物理边界条件的匹配关系，为WKB量子化提供了更清晰的物理解释。

Abstract: We revisit exact WKB quantization for radial Schr\"odinger problems from the
modern resurgence perspective, with emphasis on how ``physically meaningful''
quantization paths should be chosen and interpreted. Using connection formulae
at simple turning points and at regular singular points, we show that the
nontrivial-cycle data give the spectrum. In particular, for the $3$-dimensional
harmonic oscillator and the $3$-dimensional Coulomb potential, we explicitly
compute a closed contour which starts at $+\infty$, bulges into the $r<0$
sector to encircle the origin, and returns to $+\infty$. Also we propose that
the appropriate slice of the closed path provides a physical local basis at
$r=0$, which is used by an origin-to-$\infty$ open path. Our analysis
clarifies, in radial settings, how mathematical monodromy data and physical
boundary conditions dovetail, thereby addressing recent debates on path choices
in resurgence-based quantization.

</details>


### [95] [Qiboml: towards the orchestration of quantum-classical machine learning](https://arxiv.org/abs/2510.11773)
*Matteo Robbiati,Andrea Papaluca,Andrea Pasquale,Edoardo Pedicillo,Renato M. S. Farias,Alejandro Sopena,Mattia Robbiano,Ghaith Alramahi,Simone Bordoni,Alessandro Candido,Niccolò Laurora,Jogi Suda Neto,Yuanzheng Paul Tan,Michele Grossi,Stefano Carrazza*

Main category: quant-ph

TL;DR: Qiboml是一个开源软件库，用于编排混合机器学习工作流中的量子和经典组件，支持多种模拟选项和真实量子硬件。


<details>
  <summary>Details</summary>
Motivation: 构建一个能够统一管理量子和经典组件的软件库，使混合机器学习模型能够在广泛的硬件后端上运行。

Method: 基于Qibo的量子计算能力，集成TensorFlow和PyTorch等流行机器学习框架，提供多线程CPU、GPU、多GPU系统的模拟以及真实量子处理单元支持。

Result: 开发出具有多样化模拟选项、噪声感知模拟、实时错误缓解和校准功能的Qiboml库。

Conclusion: Qiboml成功实现了混合机器学习工作流的编排，为量子-经典混合模型提供了统一的开发平台。

Abstract: We present Qiboml, an open-source software library for orchestrating quantum
and classical components in hybrid machine learning workflows. Building on
Qibo's quantum computing capabilities and integrating with popular machine
learning frameworks such as TensorFlow and PyTorch, Qiboml enables the
construction of quantum and hybrid models that can run on a broad range of
backends: (i) multi-threaded CPUs, GPUs, and multi-GPU systems for simulation
with statevector or tensor network methods; (ii) quantum processing units, both
on-premise and through cloud providers. In this paper, we showcase its
functionalities, including diverse simulation options, noise-aware simulations,
and real-time error mitigation and calibration.

</details>


### [96] [Quantum State-Aware Query Complexity: Krylov Compression and Polynomial Query Duality](https://arxiv.org/abs/2510.11786)
*Kiran Adhikari Chhetriya*

Main category: quant-ph

TL;DR: 本文证明了制备f(H)|ψ₀⟩的最小查询复杂度恰好等于f在L²(μ)空间中的最优多项式逼近度，其中μ是(H,|ψ₀⟩)的谱测度。


<details>
  <summary>Details</summary>
Motivation: 通过状态感知的视角来优化量子查询复杂度，利用状态依赖的谱结构来显著节省查询成本，超越传统的均匀设计方法。

Method: 将Krylov/Favard近似与量子查询统一起来，基于谱测度理论分析最优多项式逼近度。

Result: 建立了最小查询复杂度与最优多项式逼近度之间的精确等价关系，揭示了状态依赖谱结构带来的显著效率提升。

Conclusion: 状态感知方法能够显著改进最坏情况界限，为量子算法设计提供了更精细的复杂度分析框架。

Abstract: We show that the minimal query complexity for preparing $f(H)\ket{\psi_0}$ is
exactly the optimal polynomial approximation degree of $f$ in $L^2(\mu)$, where
$\mu$ is the spectral measure of $(H,\ket{\psi_0})$. This state-aware
perspective refines the worst-case bounds, unifies Krylov/Favard approximation
with quantum queries, and explains how state-dependent spectral structure can
yield substantial savings over uniform designs.

</details>


### [97] [Bound on entanglement in neural quantum states](https://arxiv.org/abs/2510.11797)
*Nisarga Paul*

Main category: quant-ph

TL;DR: 该论文证明了前馈神经量子态在满足特定解析性假设时，其纠缠熵服从上界约束：S ≤ ck log n，其中k是非线性操作数量，n是自旋数。这建立了神经量子态的类面积律约束，排除了O(1)非线性操作下的体积律纠缠。


<details>
  <summary>Details</summary>
Motivation: 研究神经量子态的基本约束，因为虽然变分波函数能绕过多体希尔伯特空间的指数复杂度，但其表达能力常受限制。矩阵乘积态受限于面积律纠缠态，而神经量子态被认为能克服这种限制，但其基本约束尚不清楚。

Method: 通过数学证明，在前馈神经量子态满足特定解析性假设的条件下，分析其纠缠熵的边界约束。同时通过解析和数值方法验证了该约束的紧致性。

Result: 证明了神经量子态的纠缠熵上界为S ≤ ck log n，其中c为常数，k是非线性操作数量，n是自旋数。该约束对多种网络设计都适用，且n的标度是紧致的。

Conclusion: 建立了神经量子态的基本约束，类似于矩阵乘积态的面积律约束，同时确认了神经量子态具有显著的表达能力。这一约束适用于广泛的网络设计，为理解神经量子态的表达能力提供了理论基础。

Abstract: Variational wavefunctions offer a practical route around the exponential
complexity of many-body Hilbert spaces, but their expressive power is often
sharply constrained. Matrix product states, for instance, are efficient but
limited to area law entangled states. Neural quantum states (NQS) are widely
believed to overcome such limitations, yet little is known about their
fundamental constraints. Here we prove that feed-forward neural quantum states
acting on $n$ spins with $k$ scalar nonlinearities, under certain analyticity
assumptions, obey a bound on entanglement entropy for any subregion: $S \leq c
k\log n$, for a constant $c$. This establishes an NQS analog of the area law
constraint for matrix product states and rules out volume law entanglement for
NQS with $O(1)$ nonlinearities. We demonstrate analytically and numerically
that the scaling with $n$ is tight for a wide variety of NQS. Our work
establishes a fundamental constraint on NQS that applies broadly across
different network designs, while reinforcing their substantial expressive
power.

</details>


### [98] [Secret communication games and a hierarchy of quasiparticle statistics in 3 + 1D topological phases](https://arxiv.org/abs/2510.11818)
*Zhiyuan Wang*

Main category: quant-ph

TL;DR: 该论文通过秘密通信挑战游戏建立了一个三维拓扑相中准粒子统计的层次结构，发现R-准粒子是实现噪声鲁棒获胜策略的关键，这种特殊粒子与基于奇异有限群的去禁闭规范理论相关。


<details>
  <summary>Details</summary>
Motivation: 研究三维拓扑相中准粒子统计的层次结构，通过秘密通信游戏来识别和分类奇异的交换统计，探索R-准粒子在拓扑量子物质中的物理意义。

Method: 使用秘密通信挑战游戏作为诊断工具，结合范畴论描述拓扑相，分析R-准粒子的性质及其在游戏中的获胜策略，并引入扭曲变体来排除任意子。

Result: 发现只有R-准粒子能在三维挑战中实现噪声鲁棒的获胜策略，且策略本质上是唯一的；在二维中，通过扭曲变体也能将R-准粒子与任意子区分开来。

Conclusion: 秘密通信挑战游戏是识别和分类拓扑量子物质中奇异交换统计的有效诊断工具，R-准粒子在存在适当缺陷时具有非平凡的物理后果。

Abstract: We show that a family of secret communication challenge games naturally
define a hierarchy of emergent quasiparticle statistics in three-dimensional
(3D) topological phases. The winning strategies exploit a special class of the
recently proposed $R$-paraparticles to allow nonlocal secret communication
between the two participating players. We first give a high-level, axiomatic
description of emergent $R$-paraparticles, and show that any physical system
hosting such particles admits a winning strategy. We then analyze the games
using the categorical description of topological phases (where point-like
excitations in 3D are described by symmetric fusion categories), and show that
only $R$-paraparticles can win the 3D challenge in a noise-robust way, and the
winning strategy is essentially unique. This analysis associates emergent
$R$-paraparticles to deconfined gauge theories based on an exotic class of
finite groups. Thus, even though this special class of $R$-paraparticles are
fermions or bosons under the categorical classification, their exchange
statistics can still have nontrivial physical consequences in the presence of
appropriate defects, and the $R$-paraparticle language offers a more convenient
description of the winning strategies. Finally, while a subclass of non-Abelian
anyons can win the game in 2D, we introduce twisted variants that exclude
anyons, thereby singling out $R$-paraparticles in 2D as well. Our results
establish the secret communication challenge as a versatile diagnostic for both
identifying and classifying exotic exchange statistics in topological quantum
matter.

</details>


### [99] [Non-Hermitian Realization of Quantum Dynamics on Embedded Manifolds](https://arxiv.org/abs/2510.11845)
*Samuel Alperin*

Main category: quant-ph

TL;DR: 该论文证明，由一般时间周期虚势驱动的量子粒子的Floquet哈密顿量在频闪时间下精确等价于嵌入固定曲面的弯曲黎曼流形上的自由粒子哈密顿量。


<details>
  <summary>Details</summary>
Motivation: 统一非厄米Floquet物理与谱几何，为在嵌入流形上设计量子动力学提供通用方法。

Method: 通过分析时间周期虚势驱动的量子粒子的Floquet哈密顿量，建立其与弯曲黎曼流形上自由粒子哈密顿量的精确等价关系。

Result: 成功展示了正弦驱动和旋转环面的构造，证明了两种系统在频闪时间下的等价性。

Conclusion: 该框架为实验设计弯曲空间量子动力学提供了指导，建立了非厄米Floquet物理与谱几何的统一理论。

Abstract: We show that the Floquet Hamiltonian of a quantum particle driven by a
general time-periodic imaginary potential is exactly equivalent, at
stroboscopic times, to the Hamiltonian of a free particle constrained to a
curved Riemannian manifold with fixed embedding. We illustrate the construction
for a sinusoidal drive and for the torus of revolution, and outline how the
framework can guide experimental design of curved-space quantum dynamics. Our
results unify non-Hermitian Floquet physics with spectral geometry and provide
a general recipe for engineering quantum dynamics on embedded manifolds.

</details>


### [100] [Optimal and efficient inference tools for field tracking with precessing spins](https://arxiv.org/abs/2510.11884)
*Klaudia Dilcher,Piotr Bania,Diana Mendez-Avalos,Aleksandra Sierant,Morgan W. Mitchell,Jan Kolodynski*

Main category: quant-ph

TL;DR: 本文研究了用于自旋进动磁强计(SPM)的贝叶斯信号恢复方法，重点分析了扩展卡尔曼滤波(EKF)和容积卡尔曼滤波(CKF)在自由感应衰减模式下的性能，证明它们能够实现接近最优的磁场变化跟踪。


<details>
  <summary>Details</summary>
Motivation: 精确、实时的磁场监测在磁导航和超出标准模型的物理搜索等应用中至关重要。自旋进动磁强计通过观测电子、原子核、色心或μ子自旋在局部磁场中的进动来监测磁场变化。

Method: 研究了贝叶斯信号恢复方法，推导了贝叶斯克拉美-罗界作为拉莫尔频率估计的最终精度基准。比较了预测误差法(PEM)、扩展卡尔曼滤波(EKF)和容积卡尔曼滤波(CKF)的性能。

Result: 扩展卡尔曼滤波和容积卡尔曼滤波提供了接近最优且计算效率高的跟踪性能，其中容积卡尔曼滤波仅在大自旋数情况下表现更好。扩展卡尔曼滤波能够准确跟踪波动和未知瞬态信号。

Conclusion: 扩展卡尔曼滤波是实现自旋进动磁强计高效磁场跟踪的有效方法，该方法可轻松适应其他经历非线性耗散动力学和固有高斯类随机噪声的传感器类型。

Abstract: Precise, real-time monitoring of magnetic field evolution is important in
applications including magnetic navigation and searches for physics beyond the
standard model. One main field-monitoring technique, the spin-precession
magnetometer (SPM), observes electron, nucleus, color center, or muon spins as
they precess in response to their local magnetic field. Here, we study Bayesian
signal-recovery methods for SPMs in the free-induction decay (FID) mode. In
particular, we study tracking of field changes well within the coherence time
of the spin system, and thus well beyond the response bandwidth, as in [Phys.
Rev. Lett. 120, 040503 (2018)]. We derive the Bayesian Cram\'{e}r-Rao bound
that dictates the ultimate precision in estimating the Larmor frequency, which
we show to be attained by the computationally-expensive prediction error method
(PEM). Relative to this benchmark, we show that the extended Kalman filter
(EKF) and cubature Kalman filter (CKF) offer near-optimal tracking that is also
computationally efficient, with the use of the latter giving better results
only for large spin number. Focusing thus on the EKF, we show that it is
sufficient to accurately track fluctuating and unknown transient signals. Our
methods can be easily adapted to other types of sensors undergoing non-linear
dissipative dynamics and experiencing intrinsic Gaussian-like stochastic
noises.

</details>


### [101] [One-dimensional tunneling of the two-body bound state](https://arxiv.org/abs/2510.11932)
*N. Shypka,O. Hryhorchak,V. Pastukhov*

Main category: quant-ph

TL;DR: 研究一维二聚体中两个不同原子与原点处零程势相互作用时的束缚态和散射态，分析无二体束缚态塌陷的系统特性，发现二聚体反射增强现象。


<details>
  <summary>Details</summary>
Motivation: 研究两个耦合非全同原子形成的一维二聚体与原点零程势相互作用时的量子态特性，特别关注避免二体束缚态塌陷的情况。

Method: 通过计算二聚体的局域化波函数和散射波函数来分析系统性质。

Result: 预测二聚体反射相比单个原子有所增强，但在外势吸引侧的一个狭窄区域内例外。

Conclusion: 一维二聚体与零程势相互作用时通常表现出增强的反射特性，但在特定参数区域存在例外情况。

Abstract: We consider bound and scattering states of the one-dimensional dimer formed
by two coupled non-identical atoms when one of them also interacts with the
zero-range potential located at the origin. By calculating the dimer localized
and scattering wave functions, we identify properties of the system without the
two-body bound-state collapse. In general, we predict an enhancement of the
dimer reflection compared to a single atom, except for a narrow region on the
attractive side of the external potential.

</details>


### [102] [Open Quantum Dynamics Theory for Coulomb Potentials: Hierarchical Equations of Motion for Atomic Orbitals (AO-HEOM)](https://arxiv.org/abs/2510.11981)
*Yankai Zhang,oshitaka Tanimura*

Main category: quant-ph

TL;DR: 开发了三维旋转不变系统-浴模型(3D-RISB)，推导出原子轨道层次运动方程(AO-HEOM)，用于研究库仑势系统在热浴中的量子动力学。


<details>
  <summary>Details</summary>
Motivation: 研究库仑势系统在热浴中的量子动力学，特别关注保持整个系统（包括热浴）的旋转对称性。

Method: 采用三维旋转不变系统-浴模型(3D-RISB)，推导出原子轨道层次运动方程(AO-HEOM)，能够非微扰、非马尔可夫地处理系统-浴相互作用。

Result: 计算了各向同性热环境中原子系统的线性吸收光谱，系统性地改变了系统-浴耦合强度和温度参数。

Conclusion: 该形式主义能够准确描述库仑势系统在热浴中的量子动力学行为，为研究此类系统提供了有效的数值工具。

Abstract: We investigate the quantum dynamics of Coulomb potential systems in thermal
baths. We study these systems within the framework of open quantum dynamics
theory, focusing on preserving the rotational symmetry of the entire system,
including the baths. Thus, we employ a three-dimensional rotationally invariant
system-bath (3D-RISB) model to derive numerically ``exact'' hierarchical
equations of motion for atomic orbitals (AO-HEOM) that enable a
non-perturbative and non-Markovian treatment of system-bath interactions at
finite temperatures. To assess the formalism, we calculated the linear
absorption spectrum of an atomic system under isotropic thermal environment,
with systematic variation of system-bath coupling strength and temperature.

</details>


### [103] [Superradiance and Superabsorption Engine of $N$ Two-Level Systems: $N^{2}$-Power Scaling at Near-Unity Efficiency](https://arxiv.org/abs/2510.12017)
*L. F. Alves da Silva,H. Sanchez,M. A. Ponte,M. H. Y. Moussa,Norton G. de Almeida*

Main category: quant-ph

TL;DR: 提出了一种利用N个二能级原子的合作超辐射和超吸收效应的热机，通过集体泵浦和衰变循环运行，平均功率输出与系统尺寸呈二次方关系。


<details>
  <summary>Details</summary>
Motivation: 探索基于集体效应的可扩展且高效的量子热机，利用合作超辐射和超吸收现象来提升热机性能。

Method: 使用有效平均场哈密顿量描述多体动力学，设计保持绝热性的优化驱动脉冲，通过集体泵浦和衰变循环运行。

Result: 实现了平均功率输出P ∝ N²的二次方缩放，效率可接近1，数值模拟验证了分析模型的有效性。

Conclusion: 这种超引擎为基于集体效应的可扩展、高效量子热机开辟了新途径。

Abstract: We present a thermal engine that exploits the \emph{cooperative
superradiance} and \emph{superabsorption} of a sample of \(N\) two-level atoms.
This engine operates using a single cold reservoir via cycles of collective
pumping followed by decay. Using an effective mean-field Hamiltonian to
describe the many-body dynamics, we design optimized drive pulses that preserve
adiabaticity and achieve an average power output scaling quadratically with the
system size, \(P \propto N^2\). An experimentally measurable figure of merit
demonstrates that the efficiency of this superengine can approach unity. The
resulting analytical model, which yields a representative Hamiltonian for the
sample within the mean-field formalism, is validated by numerical simulations.
Our results pave the way for scalable and highly efficient quantum heat engines
based on collective effects.

</details>


### [104] [Characterizing and Harnessing Correlations Featuring Independent Qubit Devices](https://arxiv.org/abs/2510.12022)
*Liang-Liang Sun,Xiang Zhou,Chengjie Zhang,Zizhu Wang,Yong-Shun Song,Sixia Yu*

Main category: quant-ph

TL;DR: 提出了一个框架来表征量子比特系统中贝尔场景和准备-测量场景的相关性，能够处理非凸问题，并引入了设备验证和纠缠检测协议。


<details>
  <summary>Details</summary>
Motivation: 传统线性方法需要极端或非局域相关性，而该方法能够处理更一般的相关性情况，扩展了设备验证和纠缠检测的能力。

Method: 基于量子比特系统特有的不确定性关系推导相关性准则，能够捕捉贝尔场景和准备-测量场景中相关性的非凸性质，并将推断信息整合到NPA层次结构中。

Result: 该方法能够验证设备并检测纠缠，即使对于局部相关性也能有效工作，扩展了传统仅使用极端相关性的协议。

Conclusion: 该框架为标准量子信息协议提供了基础，适用于广泛的量子信息科学应用，特别是设备推断这一核心问题。

Abstract: We propose a framework to characterize the correlations in qubit systems for
Bell and prepare-and-measure scenarios with independent devices -- a typically
non-convex problem. Based on this result, we introduce protocols for referring
devices and detecting entanglement with correlation that are not necessarily
extreme or nonlocal, as required by common linear approach. } Specifically, our
correlation criterion, derived from uncertainty relation specific to qubit
systems, can capture the non-convex nature of the set of correlations arising
from Bell and prepare-and-measure scenarios, as demonstrated through concrete
examples. Conversely, when given an observed correlation, our framework can
refer potential measurements and quantum states -- which are sometimes uniquely
determined -- even with correlations that are not extreme. This extends common
protocols that merely verify devices using extreme correlations. We then
enhance entanglement detection for qubit system by incorporating the inferred
information in Navascu\'{e}s-Pironio-Ac\'{i}n (NPA) hierarchy, showing that
some local correlations can also verify entanglement. Since the scenarios
considered here are standard platforms for most quantum information protocols
and device inference is a central issue in quantum information science, our
methodology, which is well-suited to these tasks, may provide a foundation for
a broad range of applications.

</details>


### [105] [Spectral analysis of hierarchical continuous-time quantum walks](https://arxiv.org/abs/2510.12043)
*Jirô Akahori,Yusuke Ide,Tomoki Kato,Norio Konno,Shuhei Mano,Akihiro Narimatsu*

Main category: quant-ph

TL;DR: 本文提出了分层随机游走模型，包含全局和局部两种随机游走器，并构建了相应的连续时间量子游走，讨论了其谱结构，最后定义了多维连续时间量子游走。


<details>
  <summary>Details</summary>
Motivation: 研究分层随机游走模型及其量子版本，探索多维度量子游走的数学结构和性质。

Method: 使用全局和局部两种随机游走器构建分层随机游走模型，然后转化为连续时间量子游走，并通过取全局游走器的边际分布来定义多维连续时间量子游走。

Result: 构建了分层随机游走模型及其量子版本，分析了其谱结构，并成功定义了多维连续时间量子游走。

Conclusion: 分层随机游走模型为研究多维度量子游走提供了有效的数学框架，能够揭示量子游走的复杂结构特性。

Abstract: In this paper, we introduce hierarchical random walks at first. In this
model, we use two types of random walkers, {global and local} walkers. The
global walker chooses a local walker at every step, then the chosen local
walker moves a single step. After that we construct the corresponding
continuous-time quantum walks and discuss its spectral structures. Then we
define multi-dimensional continuous-time quantum walk by taking a marginal
distribution respect to the global walker.

</details>


### [106] [Engineering atomic superradiance scaling in cavity QED system with collective and individual emission channels](https://arxiv.org/abs/2510.12086)
*Ruijin Sun,Xiang Guo,Andreas Ruschhaupt,Zhihai Wang*

Main category: quant-ph

TL;DR: 研究揭示了腔QED系统中原子-光子耦合如何改变发射行为，抑制集体超辐射标度而增强单个原子发射标度。


<details>
  <summary>Details</summary>
Motivation: 超辐射是量子光学中的核心现象，但在量子信息处理和精密测量中，如何有效控制超辐射相对于参与原子数的标度关系仍未被充分探索。

Method: 研究了一个腔QED系统，分析原子-光子耦合对发射行为的影响。

Result: 发现原子-光子耦合可以显著抑制集体超辐射标度，同时增强单个原子发射的标度。

Conclusion: 这项研究为在最先进的实验平台上实现可控集体发射提供了一条途径。

Abstract: The coherent emission of multiple atoms gives rise to superradiance, a
cornerstone phenomenon in quantum optics with wide-ranging applications in
quantum information processing and precision metrology. Despite its importance,
how the superradiant scaling with respect to the number of participating atoms
can be effectively controlled remains largely unexplored. In this work, we
investigate a cavity-QED system and demonstrate that atom-photon coupling can
significantly alter the emission behavior--suppressing the collective
superradiant scaling while enhancing the scaling associated with individual
atomic emissions. Our study provides a pathway toward controllable collective
emission in state-of-the-art experimental platforms.

</details>


### [107] [Digital adiabatic evolution is universally accurate](https://arxiv.org/abs/2510.12237)
*Yangyu Lu,Yifei Huang,Dong An,Qi Zhao,Dingshun Lv,Xiao Yuan*

Main category: quant-ph

TL;DR: 数字绝热演化具有内在准确性，对模拟误差具有鲁棒性，其模拟误差不会随时间增长，比先前假设的效率高得多。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为数字模拟绝热过程成本高昂，因为算法误差会在长时间演化中累积，需要极深的电路来保持精度。

Method: 分析了两种哈密顿量模拟方法——Trotter分解和广义量子信号处理，并通过数值模拟分子系统和线性方程验证理论。

Result: 发现一阶Trotter分解误差估计比先前分析紧致10^6倍（即使在少于6个量子比特的横向场伊辛模型中），数字绝热演化效率远超预期。

Conclusion: 研究确立了数字绝热演化的基本鲁棒性，为在容错和潜在近期量子平台上实现准确高效实现提供了基础。

Abstract: Adiabatic evolution is a central paradigm in quantum physics. Digital
simulations of adiabatic processes are generally viewed as costly, since
algorithmic errors typically accumulate over the long evolution time, requiring
exceptionally deep circuits to maintain accuracy. This work demonstrates that
digital adiabatic evolution is intrinsically accurate and robust to simulation
errors. We analyze two Hamiltonian simulation methods -- Trotterization and
generalized quantum signal processing -- and prove that the simulation error
does not increase with time. Numerical simulations of molecular systems and
linear equations confirm the theory, revealing that digital adiabatic evolution
is substantially more efficient than previously assumed. Remarkably, our
estimation for the first-order Trotterization error can be 10^6 times tighter
than previous analyses for the transverse field Ising model even with less than
6 qubits. The findings establish fundamental robustness of digital adiabatic
evolution and provide a basis for accurate, efficient implementations on
fault-tolerant -- and potentially near-term -- quantum platforms.

</details>


### [108] [Faster State Preparation with Randomization](https://arxiv.org/abs/2510.12247)
*Yue Wang,Xiao-Ming Zhang,Xiao Yuan,Qi Zhao*

Main category: quant-ph

TL;DR: 提出了一种随机化协议，通过准备简单电路集合来改进具有分层振幅结构量子态的制备精度-成本权衡，相比确定性截断方法实现了误差的二次改进。


<details>
  <summary>Details</summary>
Motivation: 量子态制备是许多量子算法的主要成本来源，特别是对于具有分层振幅结构（指数衰减或幂律衰减）的态，需要改进精度与成本之间的权衡关系。

Method: 采用随机化协议，准备一个简单电路集合：每个电路保留所有大振幅，同时放大单个小振幅，而不是常用的确定性截断小振幅方法。

Result: 严格证明了随机化集合在迹距离误差上比确定性截断方法实现了二次改进，对于指数衰减将编码振幅数量减半，对于幂律衰减实现多项式减少，直接转化为电路深度的减少。

Conclusion: 该方法扩展了在近期和容错量子设备上可制备的态类别，在分子波函数、多体基态和机器学习参数等应用中验证了有效性。

Abstract: Quantum state preparation remains a dominant cost in many quantum algorithms.
We introduce a randomized protocol that fundamentally improves the
accuracy-cost trade-off for states with hierarchical amplitude structures,
where amplitudes decay exponentially or by a power law with exponent greater
than one. Rather than commonly employed deterministically truncating small
amplitudes, we prepare an ensemble of simple circuits: each retains all large
amplitudes while amplifying a single small one. We rigorously prove that the
randomized ensemble achieves a quadratic improvement in trace-distance error
over the deterministic truncation method. This quadratic scaling halves the
number of encoded amplitudes for exponential decay and yields polynomial
reductions for power-law decay, which directly translates to a reduction in
circuit depth. Demonstrations on molecular wavefunctions (LiH), many-body
ground states (transverse-field Ising), and machine-learning parameters
(ResNet) validate the approach across diverse applications. The method broadens
the class of states preparable on near-term and fault-tolerant quantum devices.

</details>


### [109] [High-efficiency and long-distance quantum memory-assisted device-independent quantum secret sharing with single photon sources](https://arxiv.org/abs/2510.12288)
*Qi Zhang,Jia-Wei Ying,Shi-Pu Gu,Xing-Fu Wang,Lan Zhou,Yu-Bo Sheng*

Main category: quant-ph

TL;DR: 提出了基于单光子源的量子存储器辅助设备无关量子秘密共享协议，通过量子存储技术和前导架构显著提高了安全传输距离和密钥生成效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统设备无关量子秘密共享协议中光子传输损耗和低多体纠缠生成率导致的安全传输距离受限和密钥效率低的问题。

Method: 使用单光子源构建长距离多体纠缠通道，结合量子存储器技术和前导架构，优化光子透射率和多光子同步效率。

Result: 协议在理论上可实现无限安全光子传输距离，密钥生成效率比现有协议提高了6-7个数量级，具有模块化特性且在当前实验条件下可行。

Conclusion: 该协议有望推动长距离高效率设备无关量子网络的发展，结合随机密钥生成基策略可有效降低对实验设备的要求。

Abstract: Quantum secret sharing (QSS) plays a critical role in building the
distributed quantum networks. Device-independent (DI) QSS provides the highest
security level for QSS. However, the photon transmission loss and extremely low
multipartite entanglement generation rate largely limit DI QSS's secure photon
transmission distance and practical key generation efficiency. To address the
above drawbacks, we propose the quantum memory-assisted (QMA) DI QSS protocol
based on single photon sources (SPSs). The single photons from the SPSs are
used to construct long-distance multipartite entanglement channels with the
help of the heralded architecture. The heralded architecture enables our
protocol to have an infinite secure photon transmission distance in theory. The
QMA technology can not only increase the multi-photon synchronization
efficiency, but also optimize the photon transmittance to maximize the
construction efficiency of the multipartite entanglement channels. Our protocol
achieves the practical key generation efficiency seven orders of magnitude
higher than that of the existing DI QSS protocols based on cascaded spontaneous
parametric down-conversion sources and six orders of magnitude higher than that
of the DI QSS based on SPSs without QMA. Our protocol has modular
characteristics and is feasible under the current experimental technical
conditions. Combining with the advanced random key generation basis strategy,
the requirement on experimental devices can be effectively reduced. Our
protocol is expected to promote the development of long-distance and
high-efficiency DI quantum network in the future.

</details>


### [110] [Hybrid Vision Transformer and Quantum Convolutional Neural Network for Image Classification](https://arxiv.org/abs/2510.12291)
*Mingzhu Wang,Yun Shang*

Main category: quant-ph

TL;DR: ViT-QCNN-FT混合框架将微调Vision Transformer与量子卷积神经网络结合，在CIFAR-10上达到99.77%准确率，证明了量子优势。


<details>
  <summary>Details</summary>
Motivation: 解决量子机器学习在真实世界任务中因经典预处理和噪声设备而进展缓慢的问题。

Method: 集成微调Vision Transformer与量子卷积神经网络，压缩高维图像特征以适应NISQ设备，系统探究纠缠分布。

Result: 均匀纠缠熵的ansatz表现最佳，量子噪声在某些情况下提升准确率(+2.71%)，量子版本比经典同等参数模型准确率高29.36%。

Conclusion: 建立了经典与量子架构协同设计的原理性路径，指向能够处理复杂高维学习任务的实用量子机器学习。

Abstract: Quantum machine learning (QML) holds promise for computational advantage, yet
progress on real-world tasks is hindered by classical preprocessing and noisy
devices. We introduce ViT-QCNN-FT, a hybrid framework that integrates a
fine-tuned Vision Transformer with a quantum convolutional neural network
(QCNN) to compress high-dimensional images into features suited for noisy
intermediate-scale quantum (NISQ) devices. By systematically probing
entanglement, we show that ansatzes with uniformly distributed entanglement
entropy consistently deliver superior non-local feature fusion and
state-of-the-art accuracy (99.77% on CIFAR-10). Surprisingly, quantum noise
emerges as a double-edged factor: in some cases, it enhances accuracy (+2.71%
under amplitude damping). Strikingly, substituting the QCNN with classical
counterparts of equal parameter count leads to a dramatic 29.36% drop,
providing unambiguous evidence of quantum advantage. Our study establishes a
principled pathway for co-designing classical and quantum architectures,
pointing toward practical QML capable of tackling complex, high-dimensional
learning tasks.

</details>


### [111] [Metrological approach to the emergence of classical objectivity](https://arxiv.org/abs/2510.12313)
*Anthony Kiely,Diana A. Chisholm,Akram Touil,Sebastian Deffner,Gabriel Landi,Steve Campbell*

Main category: quant-ph

TL;DR: 该论文结合量子达尔文主义和量子计量学工具，提出了一个精确描述经典性出现的框架，使用量子Fisher信息作为评估经典客观性出现速率的度量标准。


<details>
  <summary>Details</summary>
Motivation: 旨在精确刻画经典性的出现，将量子达尔文主义形式化与量子计量学工具相结合，为量子达尔文主义提供精确的操作性描述。

Method: 使用量子Fisher信息作为度量标准，分析不同测量选择对系统状态确定精度的影响，以自旋星模型为例进行演示。

Result: 最优测量导致经典性以指数速率出现，虽然次优测量出现较慢，但仍能饱和Cramér-Rao界。

Conclusion: 通过将涌现的经典性重新定义为信息获取协议，该框架为量子达尔文主义提供了精确的操作性描述。

Abstract: We present a precise characterization of the onset of classicality that
combines the formalism of quantum Darwinism with the tools from quantum
metrology. We show that the quantum Fisher information provides a useful metric
for assessing the rate at which classical objectivity emerges. Furthermore, our
formalism allows us to explore how the choice of measurement impacts the
precision with which an observer can determine the state of the system. For a
paradigmatic example of the spin-star model, we demonstrate that optimal
measurements lead to the emergence of classicality at an exponential rate.
Although other measurements necessarily lead to slower emergence, we
importantly show that suboptimal measurements can still saturate the
Cram\'{e}r-Rao bound. By recasting emergent classicality as an information
acquisition protocol, our framework provides a precise operational description
of quantum Darwinism.

</details>


### [112] [Implementing the Quantum Approximate Optimization Algorithms for QUBO problems Across Quantum Hardware Platforms: Performance Analysis, Challenges, and Strategies](https://arxiv.org/abs/2510.12336)
*Teemu Pihkakoski,Aravind Plathanam Babu,Pauli Taipale,Petri Liimatta,Matti Silveri*

Main category: quant-ph

TL;DR: ADAPT-QAOA在解决困难QUBO问题时显著优于标准QAOA，特别是在金融特征选择应用中。标准QAOA在简单问题上仍保持高效。超导量子计算机在时间效率上优于离子阱设备，但后者有更好的错误率表现。


<details>
  <summary>Details</summary>
Motivation: 研究量子计算机在解决复杂优化问题（特别是QUBO问题）上的优势，重点关注QAOA算法在金融特征选择等实际应用中的性能表现。

Method: 比较标准QAOA和自适应ADAPT-QAOA算法在不同规模和难度QUBO问题上的表现，基于真实设备校准数据进行扩展性分析，评估超导和离子阱量子硬件平台的性能。

Result: ADAPT-QAOA在困难问题（α=0.6）上显著优于标准QAOA，在近似比和求解时间方面表现更好。标准QAOA在简单问题上仍高效。超导量子计算机提供更短的求解时间，而离子阱设备有更优的错误率。

Conclusion: 研究为在近期量子硬件上部署QAOA方法提供了挑战、权衡和策略的全面概述，强调了算法选择和硬件平台的重要性。

Abstract: Quantum computers are expected to offer significant advantages in solving
complex optimization problems that are challenging for classical computers.
Quadratic Unconstrained Binary Optimization (QUBO) problems represent an
important class of problems with relevance in finance and logistics. The
Quantum Approximate Optimization Algorithm (QAOA) is a prominent candidate for
solving QUBO problems on near-term quantum devices. In this paper, we
investigate the performance of both the standard QAOA and the adaptive
derivative assembled problem tailored QAOA (ADAPT-QAOA) to solve QUBO problems
of varying sizes and hardnesses with a focus on its practical applications in
financial feature selection problems. Our main observation is that ADAPT-QAOA
significantly outperforms QAOA with hard problems (trade-off parameter {\alpha}
= 0.6) when comparing approximation ratio and time-to-solution. However, the
standard QAOA remains efficient for simpler problems. Additionally, we
investigate the practical feasibility and limitations of QAOA by scaling
analysis based on the real-device calibration data for various hardware
platforms. Our estimates indicate that standard QAOA implemented on
superconducting quantum computers provides a shorter time-to-solution compared
to trapped-ion devices. However, trapped-ion devices are expected to yield more
favorable error rates. Our findings provide a comprehensive overview of the
challenges, trade-offs, and strategies for deploying QAOA-based methods on
near-term quantum hardware.

</details>


### [113] [Snapshot renormalization group for quantum matter](https://arxiv.org/abs/2510.12415)
*Laurin Brunner,Tobias Wiener,Tiago Mendes-Santos,Reyhaneh Khasseh,Markus Heyl*

Main category: quant-ph

TL;DR: 提出了一种直接应用于量子快照数据集的精确重整化群变换方法SnapshotRG，能够在实空间和抽象数据空间中进行重整化，揭示了量子相变中快照数据集的自相似性。


<details>
  <summary>Details</summary>
Motivation: 量子模拟器实验的最新进展使得通过单个多体构型的快照测量能够前所未有地访问量子多体态，需要开发能够直接处理此类快照数据的重整化群方法。

Method: 引入SnapshotRG方法，这是一种精确的重整化群变换，可直接应用于任何快照数据集，既可在实空间操作，也可在测量构型的抽象数据空间中实现重整化。

Result: 证明在连续相变中快照数据集表现出自相似性，解释了最近观察到的波函数网络的尺度无关性，表明尺度不变性扩展到了量子态的完整统计结构。

Conclusion: SnapshotRG方法可轻松应用于神经网络量子态或任何量子模拟平台生成的快照数据，为表征量子相变和量子物质中的临界现象提供了一个通用工具。

Abstract: Recent advances in quantum simulator experiments enable unprecedented access
to quantum many-body states through snapshot measurements of individual
many-body configurations. Here, we introduce an exact renormalization group
(RG) transformation that can be directly applied to any such snapshot dataset.
Our SnapshotRG operates in real space, but can also be directly translated to
an RG in the abstract dataspace of measurement configurations, providing a
framework for the characterization of quantum many-body systems on a more
general level. We demonstrate that snapshot datasets in dataspace exhibit
self-similarity at continuous phase transitions, providing an explanation for
the recently observed scale-freeness of so-called wavefunction networks. As a
consequence, scale invariance extends beyond traditional low-order correlation
functions to encompass the full statistical structure of quantum states as
contained in their snapshot datasets. Our SnapshotRG can be readily implemented
with snapshot data generated by numerical method such as neural quantum states
or any quantum simulation platform, offering a versatile tool for
characterizing quantum phase transitions and critical phenomena in quantum
matter.

</details>


### [114] [Neural Guided Sampling for Quantum Circuit Optimization](https://arxiv.org/abs/2510.12430)
*Bodo Rosenhahn,Tobias J. Osborne,Christoph Hirche*

Main category: quant-ph

TL;DR: 提出了一种基于2D神经引导采样的量子电路优化方法，通过神经网络预测可约简的量子门组，显著减少量子电路缩减的计算时间。


<details>
  <summary>Details</summary>
Motivation: 量子电路在特定硬件拓扑上的编译会导致电路长度显著增加，由于退相干效应，这会严重影响计算质量。现有随机优化方法存在采样效率低、优化时间长的问题。

Method: 使用2D神经网络引导采样，给定量子电路的2D表示，神经网络预测电路中可能可约简的量子门组，从而创建采样先验来加速优化过程。

Result: 实验证明该方法优于qiskit和BQSKit的不同优化级别，能够更有效地减少量子电路。

Conclusion: 2D神经引导采样方法能够显著提高量子电路优化的效率，减少计算时间和能耗。

Abstract: Translating a general quantum circuit on a specific hardware topology with a
reduced set of available gates, also known as transpilation, comes with a
substantial increase in the length of the equivalent circuit. Due to
decoherence, the quality of the computational outcome can degrade seriously
with increasing circuit length. Thus, there is major interest to reduce a
quantum circuit to an equivalent circuit which is in its gate count as short as
possible. One method to address efficient transpilation is based on approaches
known from stochastic optimization, e.g. by using random sampling and token
replacement strategies. Here, a core challenge is that these methods can suffer
from sampling efficiency, causing long and energy consuming optimization time.
As a remedy, we propose in this work 2D neural guided sampling. Thus, given a
2D representation of a quantum circuit, a neural network predicts groups of
gates in the quantum circuit, which are likely reducible. Thus, it leads to a
sampling prior which can heavily reduce the compute time for quantum circuit
reduction. In several experiments, we demonstrate that our method is superior
to results obtained from different qiskit or BQSKit optimization levels.

</details>


### [115] [Experimental verification of multi-copy activation of genuine multipartite entanglement](https://arxiv.org/abs/2510.12457)
*Robert Stárek,Tim Gollerthan,Olga Leskovjanová,Michael Meth,Peter Tirler,Nicolai Friis,Martin Ringbauer,Ladislav Mišta Jr*

Main category: quant-ph

TL;DR: 实验演示了从两个双可分三量子比特态中激活真正的多体纠缠，挑战了量子资源概念并展示了多副本的潜力。


<details>
  <summary>Details</summary>
Motivation: 真正的多体纠缠(GME)是量子信息处理中的重要概念，可用于表征复杂量子系统和量子通信应用。研究发现GME可以从多个双可分量子态副本中激活，这挑战了传统量子资源观念。

Method: 在囚禁离子量子处理器中，使用两个双可分三量子比特态副本进行实验，展示GME激活现象。

Result: 实验明确证明了从两个双可分三量子比特态中激活真正的多体纠缠，这是首次在实验上观察到这种现象。

Conclusion: 这项工作不仅挑战了量子资源的基本概念，还突显了使用量子态多副本实现单个副本无法完成任务的潜力。

Abstract: A central concept in quantum information processing is genuine multipartite
entanglement (GME), a type of correlation beyond biseparability, that is,
correlations that cannot be explained by statistical mixtures of partially
separable states. GME is relevant for characterizing and benchmarking complex
quantum systems, and it is an important resource for applications such as
quantum communication. Remarkably, it has been found that GME can be activated
from multiple copies of biseparable quantum states, which do not possess GME
individually. Here, we experimentally demonstrate unambiguous evidence of such
GME activation from two copies of a biseparable three-qubit state in a
trapped-ion quantum processor. These results not only challenge notions of
quantum resources but also highlight the potential of using multiple copies of
quantum states to achieve tasks beyond the capabilities of the individual
copies.

</details>


### [116] [The logic of quantum mechanics](https://arxiv.org/abs/2510.12502)
*Eric Buffenoir*

Main category: quant-ph

TL;DR: 本文重新审视了量子逻辑程序，通过将张量积和星对合作为先决条件来定义状态空间，构建的量子逻辑与不可约希尔伯特几何有密切联系，并证明了上下文性、无广播定理和贝尔非局域性等量子特性。


<details>
  <summary>Details</summary>
Motivation: 由于现有量子逻辑框架存在张量积限制和无法处理纠缠态等问题，导致Birkhoff和von Neumann的量子逻辑程序被忽视。本文旨在通过反转视角来解决这些问题。

Method: 将张量积和星对合的存在作为定义状态空间的先决条件，构建新的量子逻辑框架，不预先强加希尔伯特几何结构。

Result: 构建的量子逻辑与不可约希尔伯特几何有密切联系，并成功证明了上下文性、无广播定理和贝尔非局域性等基本量子特性。

Conclusion: 该量子逻辑程序能够实现Birkhoff和von Neumann最初建立量子理论的雄心，为量子理论提供了新的基础框架。

Abstract: The quantum logic program originated in a 1936 article by G. Birkhoff and J.
von Neumann. This program is generally disregarded due to no-go theorems
restricting the existence of the tensor product of elementary quantum logics
and, above all, the impossibility of considering entangled states and Bell
non-local states within the framework of these composite quantum logics. We
revisit this study from the beginning and reverse the perspective. Here, the
existence of a tensor product and a star involution are the only prerequisites
for the definition of the state spaces. Surprisingly, the quantum logics
constructed in this way turn out to have a close connection with irreducible
Hilbert geometries, even though we did not impose this sort of structure ab
initio. Endly, the existence of some basic quantum-like properties is
explicitly proven in our framework : contextuality, no-broadcasting theorem,
and Bell non-locality. These elements demonstrate that our quantum logic
program is capable of achieving G. Birkhoff and J. von Neumann's initial
ambition of founding quantum theory.

</details>


### [117] [Detection of quantum information masking via machine learning](https://arxiv.org/abs/2510.12507)
*Sheng-Ao Mao,Lin Zhang,Bo Li*

Main category: quant-ph

TL;DR: 本文研究使用监督机器学习检测量子信息掩蔽，针对纯态和混合态量子比特分别训练XGBoost模型，通过优化训练样本选择提高分类准确率。


<details>
  <summary>Details</summary>
Motivation: 机器学习在量子信息领域应用广泛，但少有研究关注量子信息掩蔽检测。本文旨在填补这一空白，探索机器学习在量子信息掩蔽检测中的应用。

Method: 对于纯态量子比特，随机生成密度矩阵并训练XGBoost模型；对于混合态量子比特，优化训练样本选择改进XGBoost方法。

Result: 实验结果表明该方法获得了更高的分类准确率，接收者操作特征曲线下面积分析进一步验证了其分类性能。

Conclusion: 监督机器学习方法在量子信息掩蔽检测中表现良好，特别是通过优化训练样本选择可以显著提升混合态量子比特的分类性能。

Abstract: Recently, machine learning has been widely applied in the field of quantum
information, notably in tasks such as entanglement detection, steering
characterization, and nonlocality verification. However, few studies have
focused on utilizing machine learning to detect quantum information masking. In
this work, we investigate supervised machine learning for detecting quantum
information masking in both pure and mixed qubit states. For pure qubit states,
we randomly generate the corresponding density matrices and train an XGBoost
model to detect quantum information masking. For mixed qubit states, we improve
the XGBoost method by optimizing the selection of training samples. The
experimental results demonstrate that our approach achieves higher
classification accuracy. Furthermore, we analyze the area under the curve (AUC)
of the receiver operating characteristic curve for this method, which further
confirms its classification performance.

</details>


### [118] [Semiclassical analytical solutions of the eigenstate thermalization hypothesis in a quantum billiard](https://arxiv.org/abs/2510.12517)
*Yaoqi Ye,Chengkai Lin,Xiao Wang*

Main category: quant-ph

TL;DR: 本文推导了四分之一体育场量子台球系统中本征态热化假说(ETH)对角和非对角函数的半经典解析解，揭示了ETH带宽缩放与经典动力学之间的联系。


<details>
  <summary>Details</summary>
Motivation: 研究本征态热化假说在单粒子和少体系统中的物理意义，特别是热化如何表现为初始条件信息的丢失。

Method: 使用半经典分析方法推导ETH对角和非对角函数的解析解，获得局部贡献和相空间关联项的分离表达式，并与Berry猜想进行等价性证明。

Result: 获得了可预测可观测量矩阵带结构的解析表达式，证明了Berry猜想捕捉了本征态空间中的能量长波行为，而解析解描述了半经典极限下f函数的渐近行为。

Conclusion: ETH在单粒子和少体系统中具有重要物理意义，热化表现为初始条件信息的丢失，带宽缩放与基础经典动力学密切相关。

Abstract: We derive semiclassical analytical solutions for both the diagonal and
off-diagonal functions in the eigenstate thermalization hypothesis (ETH) in a
quarter-stadium quantum billiard. For a representative observable, we obtain an
explicit expression and an asymptotic closed-form solution that naturally
separate into a local contribution and a phase-space correlation term. These
analytical results predict the band structure of the observable matrix,
including its bandwidth and scaling behavior. We further demonstrate that our
analytical formula is equivalent to the prediction of Berry's conjecture.
Supported by numerical evidence, we show that Berry's conjecture captures the
energetic long-wavelength behavior in the space of eigenstates, while our
analytical solution describes the asymptotic behavior of the f function in the
semiclassical limit. Finally, by revealing the connection between the bandwidth
scaling and the underlying classical dynamics, our results suggest that the ETH
carries important physical implications in single-particle and few-body
systems, where "thermalization" manifests as the loss of information about
initial conditions.

</details>


### [119] [A universal approach to saddle-point methods in attosecond science](https://arxiv.org/abs/2510.12545)
*Anne Weber,Job Feldbrugge,Emilio Pisanty*

Main category: quant-ph

TL;DR: 本文介绍Picard-Lefschetz理论作为评估强场高次谐波产生中时间积分的通用方法，解决了多色激光场中鞍点方法应用的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决多色激光场驱动的高次谐波产生中，传统鞍点方法难以处理无明确动力学对称性的问题，为量子轨道方法提供严格数学基础。

Method: 使用Picard-Lefschetz理论，通过连续变形积分路径到Lefschetz thimbles来精确评估积分，并引入"项链算法"确定二维积分中鞍点的相关性。

Result: 成功分析了双色激光场驱动的高次谐波产生中的Stokes跃迁和光谱焦散现象，展示了量子轨道分析在颜色转换过程中的应用。

Conclusion: 为阿秒科学中基于量子轨道的方法奠定了严格基础，能够解释最先进的实验装置并指导未来实验设计。

Abstract: Light-matter interactions within the strong-field regime, where intense laser
fields can ionise a target via tunnelling, give rise to fascinating phenomena
such as the generation of high-order harmonic radiation (HHG). On the atomic
scale, these strong-field processes are described in terms of
highly-oscillatory time integrals which are often approximated using
saddle-point methods. These methods simultaneously simplify the calculations
and let us understand the physical processes in terms of semi-classical
electron trajectories, or quantum orbits. However, applying saddle-point
methods for HHG driven by polychromatic laser fields without clear dynamical
symmetries has remained challenging. Here we introduce Picard-Lefschetz theory
as a universal and robust link between the time integrals and the
semi-classical trajectories. The continuous deformation of the integration
contour towards so-called Lefschetz thimbles allows an exact evaluation of the
integral, as well as the identification of relevant quantum orbits, for
arbitrary driving fields. The latter is realised via the ``necklace
algorithm'', a novel solution to the open problem of determining the relevance
of saddle points for a two-dimensional integral, which we introduce here. We
demonstrate the versatility and rigour of Picard-Lefschetz methods by studying
Stokes transitions and spectral caustics arising in HHG driven by two-colour
laser fields. For example, we showcase a quantum-orbit analysis of the colour
switchover, which links the regime of perturbative two-colour fields with that
of fully bichromatic driving fields. With this work, we set the foundation for
a rigorous application of quantum-orbit based approaches in attosecond science
that enables the interpretation of state-of-the-art experimental setups, and
guides the design of future ones.

</details>


### [120] [Optimization of the time-multiplexed SPDC source at 900-950 nm range](https://arxiv.org/abs/2510.12556)
*V. O. Gotovtsev,I. V. Dyakonov,O. V. Borzenkova,K. A. Taratorin,T. B. Dugarnimaev,A. A. Korneev,S. P. Kulik,S. S. Straupe*

Main category: quant-ph

TL;DR: 本文演示了一种基于SPDC过程的时间复用HSPS，包括对关键特性（纯度和预示效率）的精确计算和建模，并对时间复用后的单光子概率进行了分析和近似。


<details>
  <summary>Details</summary>
Motivation: 在量子技术领域，单光子是重要资源，但基于SPDC的HSPS单光子生成概率低，时间复用原理被提出作为解决方案。

Method: 基于SPDC过程的时间复用HSPS，包括对纯度和预示效率的精确计算和建模，以及单光子概率的分析和近似。

Result: 实现了时间复用HSPS的演示，并提供了关键特性的准确计算模型。

Conclusion: 时间复用方法可以有效提高HSPS的单光子生成概率，为量子技术应用提供了改进的单光子源解决方案。

Abstract: In the field of quantum technology, single photons have emerged as a pivotal
resource, prompting the development of heralded single photon sources (HSPS)
with enhanced generation probability. The majority of such sources are based on
spontaneous parametric down-conversion (SPDC), but they exhibit a low single
photon generation probability. The multiplexing principle
(arXiv:quant-ph/0205103) has been proposed as a solution to this problem. This
paper presents a demonstration of a time-multiplexed HSPS based on the SPDC
process, including accurate calculations and modeling of key source
characteristics, specifically purity and heralding efficiency. Furthermore, the
paper provides an analysis and approximation of the probability of a single
photon post-application of time multiplexing.

</details>


### [121] [Multi-Copy Security in Unclonable Cryptography](https://arxiv.org/abs/2510.12626)
*Alper Çakan,Vipul Goyal,Fuyuki Kitagawa,Ryo Nishimaki,Takashi Yamakawa*

Main category: quant-ph

TL;DR: 本文提出了一个通用编译器，可将抗共谋的不可克隆密码学原语升级为多副本安全，仅需单向函数假设。获得了首个多副本安全的公钥量子货币、单解密器加密、不可克隆加密等构造，并引入了可升级量子货币的新概念。


<details>
  <summary>Details</summary>
Motivation: 现有不可克隆密码学主要解决单副本安全性，而更强的多副本安全性研究较少。本文旨在填补这一空白，提供通用的多副本安全升级方案。

Method: 设计通用编译器，将抗共谋不可克隆原语升级为多副本安全；构建可升级量子货币，支持从弱验证升级到完全公开验证；开发升级单副本安全单解密器加密到抗共谋版本的编译器。

Result: 获得了首个多副本安全的公钥量子货币、单解密器加密、不可克隆加密等构造；实现了可升级量子货币；构建了首个多挑战安全的不可克隆加密方案。

Conclusion: 本文提供了系统性的多副本安全不可克隆密码学框架，显著推进了该领域的发展，所提出的通用编译器和新型构造具有独立的理论价值。

Abstract: Unclonable cryptography leverages the quantum no-cloning principle to
copy-protect cryptographic functionalities. While most existing works address
the basic single-copy security, the stronger notion of multi-copy security
remains largely unexplored.
  We introduce a generic compiler that upgrades collusion-resistant unclonable
primitives to achieve multi-copy security, assuming only one-way functions.
Using this framework, we obtain the first multi-copy secure constructions of
public-key quantum money (termed quantum coins), single-decryptor encryption,
unclonable encryption, and more. We also introduce an extended notion of
quantum coins, called upgradable quantum coins, which allow weak
(almost-public) verification under weaker assumptions and can be upgraded to
full public verification under stronger assumptions by the bank simply
publishing additional classical information.
  Along the way, we give a generic compiler that upgrades single-copy secure
single-decryptor encryption to a collusion-resistant one, assuming the
existence of functional encryption, and construct the first multi-challenge
secure unclonable encryption scheme, which we believe are of independent
interest.

</details>


### [122] [Quantum Network-Based Prediction of Cancer Driver Genes](https://arxiv.org/abs/2510.12628)
*Patricia Marques,Andreas Wichert,Duarte Magano,Bruno Coutinho*

Main category: quant-ph

TL;DR: 提出了一种监督量子框架QMME，通过量子多阶矩嵌入将突变评分与网络拓扑结合，用于癌症驱动基因识别，在PPI网络上实现了比经典方法更好的性能。


<details>
  <summary>Details</summary>
Motivation: 整合突变谱与蛋白质相互作用网络识别癌症驱动基因具有前景，但大规模网络计算需求高，量子计算能提供紧凑表示和复杂度降低。

Method: QMME编码节点及其邻居的突变评分低阶统计矩到量子态，使用基于核的量子二分类器区分已知驱动基因与其他基因。

Result: 在经验PPI网络上的模拟显示竞争性性能，相比经典基线召回率提升12.6%，无需经典训练即可实现高效端到端量子工作流。

Conclusion: 该方法展示了监督量子图学习框架在生物发现中的潜力，复杂度分析表明可能在网络癌症基因预测中实现量子加速。

Abstract: Identification of cancer driver genes is fundamental for the development of
targeted therapeutic interventions. The integration of mutational profiles with
protein-protein interaction (PPI) networks offers a promising avenue for their
detection [ 1, 2], but scaling to large network datasets is computationally
demanding. Quantum computing offers compact representations and potential
complexity reductions. Motivated by the classical method of Gumpinger et al.
[3], in this work we introduce a supervised quantum framework that combines
mutation scores with network topology via a novel state preparation scheme,
Quantum Multi-order Moment Embedding (QMME). QMME encodes low-order statistical
moments over the mutation scores of a node's immediate and second-order
neighbors, and encodes this information into quantum states. These are used as
inputs to a kernel-based quantum binary classifier that discriminates known
driver genes from others. Simulations on an empirical PPI network demonstrate
competitive performance, with a 12.6% recall gain over a classical baseline.
The pipeline performs explicit quantum state preparation and requires no
classical training, enabling an efficient, nearly end-to-end quantum workflow.
A brief complexity analysis suggests the approach could achieve a quantum
speedup in network-based cancer gene prediction. This work underscores the
potential of supervised quantum graph learning frameworks to advance biological
discovery.

</details>


### [123] [Variational Quantum Eigensolver Models of Molecular Quantum Dot Cellular Automata](https://arxiv.org/abs/2510.12656)
*Nischal Binod Gautam,Enrique P. Blair*

Main category: quant-ph

TL;DR: 使用NISQ时代的变分量子本征求解器(VQE)来模拟分子量子点元胞自动机(QCA)电路的基态，包括二进制线、反相器和多数门等逻辑电路，验证了VQE在QCA建模中的可行性。


<details>
  <summary>Details</summary>
Motivation: 分子QCA可能提供低功耗、高速的经典信息处理硬件，但完全相干模型随设备数量呈指数级增长，需要近似方法。在NISQ时代，探索使用VQE方法来估计QCA电路的基态。

Method: 使用变分量子本征求解器(VQE)方法，在理想模拟器、噪声模拟器和实际量子硬件上对QCA逻辑电路进行建模，包括二进制线、反相器和多数门。

Result: 研究表明VQE确实可以用于模拟分子QCA电路，但现代NISQ硬件的结果对噪声仍然很敏感，需要采取措施最小化噪声影响。

Conclusion: VQE方法可用于QCA电路建模，但在当前NISQ硬件上需要简化ansatz电路和使用低噪声硬件来应对噪声敏感性问题。

Abstract: Molecular quantum-dot Cellular Automata (QCA) may provide low-power,
high-speed computational hardware for processing classical information.
Simulation and modeling play an important role in the design of QCA circuits
because fully-coherent models of QCA scale exponentially with the number of
devices, and such models are severely limited in size. For larger circuits,
approximations become necessary. In the era of fault-tolerant quantum
computation, however, it may become possible to model large QCA circuits
without such limitations. Presently, this work explores the use of the
noisy-intermediate scale quantum (NISQ) variational quantum eigensolver (VQE)
method for estimating the ground state of QCA circuits. This is relevant
because the computational result of a QCA calculation is encoded in the
circuit's ground state. In this study, VQE is used to model logic circuits,
including binary wires,
  inverters, and majority gates. VQE models are performed ideal simulators,
noisy simulators, and actual quantum hardware. This study demonstrates that VQE
may indeed be used to model molecular QCA circuits. It is observed that using
modern NISQ hardware, results are still quite sensitive to noise, so measures
should be taken to minimize noise. These include simplifying the ansatz circuit
whenever possible, and using low-noise hardware.

</details>


### [124] [Decoding Multimode Gottesman-Kitaev-Preskill Codes with Noisy Auxiliary States](https://arxiv.org/abs/2510.12677)
*Marc-Antoine Roy,Thomas Pousset,Baptiste Royer*

Main category: quant-ph

TL;DR: 提出了一种考虑辅助态噪声和相关性的多模GKP码解码器，通过跟踪误差传播相关性，可将逻辑错误概率降低至少一个数量级。


<details>
  <summary>Details</summary>
Motivation: 为了实现容错量子计算，需要保护逻辑信息免受退相干影响。多模GKP编码是一种有前景的方法，但需要有效的解码策略来处理Steane型量子纠错协议中的测量结果。

Method: 提出了一种解码器，考虑辅助态中的噪声，特别是跟踪误差在纠错电路中不同模式间传播的相关性。利用测量结果与多模GKP态实际误差之间的相关性。

Result: 通过利用测量结果与误差之间的相关性，逻辑错误概率可以降低至少一个数量级。

Conclusion: 该方法能够产生更稳健的量子计算，通过考虑误差相关性显著提高了多模GKP码的纠错性能。

Abstract: In order to achieve fault-tolerant quantum computing, we make use of quantum
error correction schemes designed to protect the logical information of the
system from decoherence. A promising way to preserve such information is to use
the multimode Gottesman-Kitaev-Preskill (GKP) encoding, which encodes logical
qubits into several harmonic oscillators. In this work, we focus on decoding
the measurements obtained from Steane-type quantum error correction protocols
for multimode GKP codes. We propose a decoder that considers the noise present
on the auxiliary states, more specifically by tracking the correlations between
errors on different modes spreading throughout the error-correction circuit. We
show that leveraging the correlations between measurement results and the
actual error affecting the multimode GKP state can decrease the logical error
probability by at least an order of magnitude, yielding more robust quantum
computation.

</details>


### [125] [Probabilistic Links Between Quantum Classification of Patterns of Boolean Functions and Hamming Distance](https://arxiv.org/abs/2510.12736)
*Theodore Andronikos,Constantinos Bitsakos,Konstantinos Nikas,Georgios I. Goumas,Nectarios Koziris*

Main category: quant-ph

TL;DR: 该研究探讨了布尔函数量子分类与汉明距离之间的概率关系，发现分类成功概率随汉明距离单调递减，但存在特定类别的系统性偏差。研究首次界定了分类概率的精确汉明距离区间，为量子分类提供了新的概率评估工具。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索量子计算中布尔函数分类与汉明距离之间的概率关系，旨在提高量子分类算法的可靠性和决策能力。

Method: 通过整合量子计算、信息论和组合数学的概念，使用汉明距离作为分析函数分类偏差的度量指标，并进行广泛的实验验证。

Result: 实验结果表明，分类成功概率随汉明距离单调递减，但在特定类别中存在系统性可预测的偏差。研究成功量化了这些不规则性，并首次界定了分类概率的精确汉明距离区间。

Conclusion: 该研究为量子分类提供了新的概率评估框架，使实践者能够以高确定性支持或拒绝分类结果，显著增强了量子分类算法的可靠性和决策能力。

Abstract: This article investigates the probabilistic relationship between quantum
classification of Boolean functions and their Hamming distance. By integrating
concepts from quantum computing, information theory, and combinatorics, we
explore how Hamming distance serves as a metric for analyzing deviations in
function classification. Our extensive experimental results confirm that the
Hamming distance is a pivotal metric for validating nearest neighbors in the
process of classifying random functions. One of the significant conclusions we
arrived is that the successful classification probability decreases
monotonically with the Hamming distance. However, key exceptions were found in
specific classes, revealing intra-class heterogeneity. We have established that
these deviations are not random but are systemic and predictable. Furthermore,
we were able to quantify these irregularities, turning potential errors into
manageable phenomena. The most important novelty of this work is the
demarcation, for the first time to the best of our knowledge, of precise
Hamming distance intervals for the classification probability. These intervals
bound the possible values the probability can assume, and provide a new
foundational tool for probabilistic assessment in quantum classification.
Practitioners can now endorse classification results with high certainty or
dismiss them with confidence. This framework can significantly enhance any
quantum classification algorithm's reliability and decision-making capability.

</details>


### [126] [Time-dependent Variational Principles for Hybrid Non-Unitary Dynamics: Application to Driven-Dissipative Superconductors](https://arxiv.org/abs/2510.12737)
*Pasquale Filice,Marco Schirò,Giacomo Mazza*

Main category: quant-ph

TL;DR: 本文引入时间依赖变分原理研究开放量子多体系统的非幺正动力学，包括完整Lindblad主方程、完全后选择量子轨迹的非厄米动力学，以及混合Lindbladian动力学。应用在耗散BCS超导体中，发现非厄米极限导致动力学行为的急剧变化，包括密度衰减从幂律到指数衰减的转变，以及非厄米芝诺效应和负温度态的出现。


<details>
  <summary>Details</summary>
Motivation: 研究开放量子多体系统的非幺正动力学，特别是探索非厄米极限作为混合耗散动力学的奇异极限，揭示其如何显著改变系统向驱动耗散稳态的普适趋近方式。

Method: 采用时间依赖变分原理分析开放量子多体系统的动力学，包括完整Lindblad主方程、非厄米动力学和混合Lindbladian动力学。应用在具有两体损失和两体泵浦的耗散BCS超导体模型中。

Result: 在非厄米极限下，密度动力学从普适幂律衰减急剧转变为指数衰减，形成准稳态平台，粒子损耗冻结。准稳态密度随耗散率增加而增加，出现非厄米芝诺效应。在驱动耗散情况下，系统被困在有效负温度态，避免了有限量子跳跃贡献下的无限温度稳态。

Conclusion: 非厄米极限作为混合耗散动力学的奇异极限，通过抑制赝自旋长度的有效单粒子损失和泵浦，导致动力学行为的显著变化，包括非厄米芝诺效应和负温度态的形成。

Abstract: We introduce time-dependent variational principles to study the non-unitary
dynamics of open quantum many-body systems, including dynamics described by the
full Lindblad master equation, the non-Hermitian dynamics corresponding to the
no-click limit of the fully post-selected quantum trajectories, and the
dynamics described by a hybrid Lindbladian with a control parameter $\alpha$
which interpolates between the full post-selection and averaging over all
quantum trajectories. As an application we study the non-unitary dynamics of a
lossy or driven-dissipative BCS superconductors, evolving in presence of
two-body losses and two-body pumps. We show that the non-Hermitian limit acts
as a singular limit of the hybrid dissipative dynamics, leading to a sharp
modification of the universal approach to the driven-dissipative steady-states.
By considering the dissipative dynamics with pair losses, we show that, as the
non-Hermitian limit is approached, the density dynamics sharply evolves from a
universal power-law to exponential decay that converges towards a quasi-steady
plateau characterized by the freezing of the particle depletion due to pair
losses. The reached quasi-stationary density increases as a function of the
dissipation rate highlighting the emergence of a non-Hermitian Zeno effect in
the lossy dynamics. For the driven-dissipative case, we show that, in the
non-Hermitian limit, the system gets trapped into an effective negative
temperature state, thus skipping the infinite temperature steady-state reached
in the presence of finite contribution of the quantum jumps. We rationalize
these findings in terms of the conservation of the length of the pseudospins
which, in the non-Hermitian limit, suppresses the effective single-particle
losses and pumps acting on the non-condensed particles.

</details>


### [127] [Measurement-induced entanglement in noisy 2D random Clifford circuits](https://arxiv.org/abs/2510.12743)
*Zhi-Yuan Wei,Jon Nelson,Joel Rajakumar,Esther Cruz,Alexey V. Gorshkov,Michael J. Gullans,Daniel Malz*

Main category: quant-ph

TL;DR: 研究噪声2D随机Clifford电路中测量诱导纠缠的相变，发现恒定局部噪声会破坏长程体积律纠缠，并提出了高效采样的条件。


<details>
  <summary>Details</summary>
Motivation: 探索在存在噪声的情况下，2D随机Clifford电路中测量诱导纠缠的相变行为，特别是噪声如何影响纠缠的标度规律。

Method: 通过列采样分析噪声2D随机Clifford电路的算子纠缠，研究稳定子生成元的空间分布和条件互信息衰减。

Result: 在无噪声极限下观察到从面积律到体积律的有限深度相变；在恒定噪声率下，最大算子纠缠服从面积律，且与T/p近似线性相关；稳定子生成元呈指数局域化。

Conclusion: 恒定局部噪声会破坏2D随机Clifford电路中的长程体积律测量诱导纠缠，并基于标度行为推测了高效张量网络采样的可行性条件。

Abstract: We study measurement-induced entanglement generated by column-by-column
sampling of noisy 2D random Clifford circuits of size $N$ and depth $T$.
Focusing on the operator entanglement $S_{\rm op}$ of the sampling-induced
boundary state, first, we reproduce in the noiseless limit a finite-depth
transition from area- to volume-law scaling. With on-site probablistic trace
noise at any constant rate $p>0$, the maximal $S_{\rm op}$ attained along the
sampling trajectory obeys an area law in the boundary length and scales
approximately linearly with $T/p$. By analyzing the spatial distribution of
stabilizer generators, we observe exponential localization of stabilizer
generators; this both accounts for the scaling of the maximal $S_{\rm op}$ and
implies an exponential decay of conditional mutual information across buffered
tripartitions, which we also confirm numerically. Together, these results
indicate that constant local noise destroys long-range, volume-law
measurement-induced entanglement in 2D random Clifford circuits. Finally, based
on the observed scaling, we conjecture that a tensor-network-based algorithm
can efficiently sample from noisy 2D random Clifford circuits (i) at
sub-logarithmic depths $T = o(\log N)$ for any constant noise rate $p =
\Omega(1)$, and (ii) at constant depths $T = O(1)$ for noise rates $p =
\Omega(\log^{-1}N)$.

</details>


### [128] [Contextuality-based quantum key distribution with deterministic single-photon sources](https://arxiv.org/abs/2510.12761)
*Yu Meng,Debashis Saha,Mikkel Thorbjørn Mikkelsen,Clara Henke,Ying Wang,Nikolai Bart,Arne Ludwig,Adán Cabello,Leonardo Midolo*

Main category: quant-ph

TL;DR: 利用量子点单光子源实现量子上下文性，开发了半设备无关的量子密钥分发协议


<details>
  <summary>Details</summary>
Motivation: 光子是量子技术的核心，量子点能够按需生成单光子，这是实现长距离量子网络的关键能力

Method: 使用基于自组装InAs(Ga)As量子点的高纯度单光子源作为量子信息载体，证明这类按需单光子可以产生量子上下文性

Result: 实现了基于量子上下文性的半设备无关量子密钥分发协议，该协议不需要理想或完美投影测量

Conclusion: 为稳健实用的量子通信开辟了新途径

Abstract: Photons are central to quantum technologies, with photonic qubits offering a
promising platform for quantum communication. Semiconductor quantum dots stand
out for their ability to generate single photons on demand, a key capability
for enabling long-distance quantum networks. In this work, we utilize
high-purity single-photon sources based on self-assembled InAs(Ga)As quantum
dots as quantum information carriers. We demonstrate that such on-demand single
photons can generate quantum contextuality. This capability enables a novel
protocol for semi-device-independent quantum key distribution over free-space
channels. Crucially, our method does not require ideal or perfectly projective
measurements, opening a new pathway for robust and practical quantum
communication.

</details>


### [129] [Trajectory-Protected Quantum Computing](https://arxiv.org/abs/2510.12771)
*Barbara Šoda,Pierre-Antoine Graham,T. Rick Perche,Gurpahul Singh*

Main category: quant-ph

TL;DR: 提出一种同时隔离量子计算机退相干并实现可控计算门的新方法，利用量子比特的运动来保护其免受退相干影响。


<details>
  <summary>Details</summary>
Motivation: 解决量子计算中的退相干问题，同时实现可控的计算门操作，探索量子误差保护的基本限制。

Method: 使用Unruh-DeWitt探测器模型，通过控制量子比特的轨迹来消除主要退相干通道，利用加速度诱导透明技术关闭旋转波项，通过非共振跃迁实现单量子比特门，从压缩态量子场中提取纠缠实现双量子比特门。

Result: 成功演示了能够同时进行量子误差保护和计算门操作的量子计算模型。

Conclusion: 该方法在量子误差保护和门操作速度之间建立了基本权衡，类似于量子纠错中的Eastin-Knill定理。

Abstract: We introduce a novel method that simultaneously isolates a quantum computer
from decoherence and enables the controlled implementation of computational
gates. We demonstrate a quantum computing model that utilizes a qubit's motion
to protect it from decoherence. We model a qubit interacting with a quantum
field via the standard light-matter interaction model: an Unruh-DeWitt
detector, i.e., the qubit, follows a prescribed classical trajectory while
interacting with a scalar quantum field. We switch off the rotating-wave terms,
i.e., the resonant transitions, using the technique of acceleration-induced
transparency which eliminates the dominant decoherence channels by controlling
the qubit's trajectory. We are able to perform one-qubit gates by stimulating
the counter-rotating wave terms (i.e., the non-resonant transitions) and
two-qubit gates by extracting the entanglement from the quantum field prepared
in a squeezed state. Finally, we discuss the fundamental limits on quantum
error protection: on the trade-off between isolating a quantum computer from
decoherence, and the speed with which entangling gates may be applied,
comparable to the Eastin-Knill theorem for quantum error correction.

</details>


### [130] [Performance of Gaussian Boson Sampling on Planted Bipartite Clique Detection](https://arxiv.org/abs/2510.12774)
*Yu-Zhen Janice Chen,Laurent Massoulié,Don Towsley*

Main category: quant-ph

TL;DR: 本文研究了高斯玻色子采样(GBS)在解决种植二部团问题上的计算优势，发现当种植结构较小时，GBS节点权重的自然波动会主导偏差信号，使得检测不可靠。


<details>
  <summary>Details</summary>
Motivation: 探索GBS在经典困难问题上的理论性能，特别是种植二部团问题，该问题在种植结构较小时被认为经典计算困难。

Method: 分析GBS输出的节点权重统计量，研究该信号是否能区分种植二部团节点和背景节点，并量化种植结构引入的偏差。

Result: 当种植二部团大小处于推测的困难区域时，节点权重的自然波动主导偏差信号，使得使用简单排序策略的检测不可靠。

Conclusion: 这是首个严格证据表明种植二部团检测在GBS量子计算下可能仍然计算困难，需要进一步研究更先进的GBS算法或其他量子方法。

Abstract: We investigate whether Gaussian Boson Sampling (GBS) can provide a
computational advantage for solving the planted biclique problem, which is a
graph problem widely believed to be classically hard when the planted structure
is small. Although GBS has been heuristically and experimentally observed to
favor sampling dense subgraphs, its theoretical performance on this classically
hard problem remains largely unexplored. We focus on a natural statistic
derived from GBS output: the frequency with which a node appears in GBS
samples, referred to as the node weight. We rigorously analyze whether this
signal is strong enough to distinguish planted biclique nodes from background
nodes. Our analysis characterizes the distribution of node weights under GBS
and quantifies the bias introduced by the planted structure. The results reveal
a sharp limitation: when the planted biclique size falls within the conjectured
hard regime, the natural fluctuations in node weights dominate the bias signal,
making detection unreliable using simple ranking strategies. These findings
provide the first rigorous evidence that planted biclique detection may remain
computationally hard even under GBS-based quantum computing, and they motivate
further investigation into more advanced GBS-based algorithms or other quantum
approaches for this problem.

</details>


### [131] [Thermodynamics of quantum processes: An operational framework for free energy and reversible athermality](https://arxiv.org/abs/2510.12790)
*Himanshu Badhani,Dhanuja G S,Siddhartha Das*

Main category: quant-ph

TL;DR: 本文通过公理化方法引入量子过程的自由能概念，建立了量子过程非热性的资源理论，并证明了其在量子信息处理和热力学任务中的操作意义。


<details>
  <summary>Details</summary>
Motivation: 探索量子过程的热力学特性，将经典热力学概念扩展到量子通道，建立量子过程非热性的资源理论框架。

Method: 使用量子相对熵定义量子通道的自由能，构建以吉布斯保持超通道为自由操作的资源理论，利用假设检验和最大相对熵精确表征量子通道的单次蒸馏和形成。

Result: 建立了量子过程自由能的操作解释，证明了非热性的渐近可逆性，揭示了量子过程非热性与私有随机性、纯度蒸馏以及擦除、功提取等热力学任务之间的直接联系。

Conclusion: 该工作将自由能、能量、熵和最大可提取功等核心热力学概念与量子过程的信息处理能力联系起来，为量子热力学提供了统一的理论框架。

Abstract: We explore the thermodynamics of quantum processes (quantum channels) by
axiomatically introducing the free energy for channels, defined via the quantum
relative entropy with an absolutely thermal channel whose fixed output is in
equilibrium with a thermal reservoir. This definition finds strong support
through its operational interpretations in designated quantum information and
thermodynamic tasks. We construct a resource theory of athermality for quantum
processes, where free operations are Gibbs preserving superchannels and golden
units are unitary channels with respect to absolutely thermal channel having
fully degenerate output Hamiltonian. We exactly characterize the one-shot
distillation and formation of quantum channels using hypothesis-testing and
max-relative entropy with respect to the absolutely thermal channel. These
rates converge asymptotically to the channel free energy (up to a
multiplicative factor of half the inverse temperature), establishing its
operational meaning and proving the asymptotic reversibility of the
athermality. We show the direct relation between the resource theory of
athermality and quantum information tasks such as private randomness and purity
distillation and thermodynamic tasks of erasure and work extraction. Our work
connects the core thermodynamic concepts of free energy, energy, entropy, and
maximal extractable work of quantum processes to their information processing
capabilities.

</details>


### [132] [Prethermal gauge structure and surface growth in $\mathbb{Z}_2$ lattice gauge theories](https://arxiv.org/abs/2510.12800)
*Lukas Homeier,Andrea Pizzi,Hongzheng Zhao,Jad C. Halimeh,Fabian Grusdt,Ana Maria Rey*

Main category: quant-ph

TL;DR: 该论文研究了(2+1)D自旋系统中通过双体Ising相互作用稳定预热的ℤ₂规范结构，发现其具有指数长寿命，最终通过Gauss定律缺陷增殖而衰变，展现出KPZ普适类的时空关联特性。


<details>
  <summary>Details</summary>
Motivation: 研究相互作用多体系统中热化的普适特性，探索如何在微观层面理解涌现现象，特别是预热规范结构的稳定性和衰变机制。

Method: 使用平均场动力学模拟数千个自旋的(2+1)D系统，采用双体Ising相互作用，并通过半经典离散时间Wigner近似(DTWA)和精确对角化(ED)在小系统中进行基准测试。

Result: 发现实验可行的双体Ising相互作用可以稳定具有指数长寿命的预热ℤ₂规范结构，该结构最终通过Gauss定律缺陷增殖而衰变，衰变过程中的时空关联符合(1+1)D KPZ普适类。

Conclusion: 该模型为量子模拟器提供了测试平台，可直接在大规模Rydberg原子阵列中实现，揭示了局部对称性对热化路径的强烈影响。

Abstract: Universal aspects of thermalization in interacting many-body systems are
typically challenging to derive microscopically, yet provide a powerful
framework for understanding emergent phenomena. Here, we numerically study the
mean-field dynamics of a $(2+1)$D spin system with thousands of spins and show
that experimentally-feasible two-body Ising interactions can stabilize a
prethermal $\mathbb{Z}_2$ lattice gauge structure with dynamical matter,
manifested by a gauge-invariant plateau with exponentially long lifetime.
Eventually, the metastable prethermal $\mathbb{Z}_2$ gauge structure breaks
down via a proliferation of Gauss' law defects, similar to bubble formation in
false vacuum decay. In this regime, we discover spatio-temporal correlations
described by a non-linear surface growth consistent with the $(1+1)$D
Kardar-Parisi-Zhang (KPZ) universality class. We benchmark our results in small
systems against semi-classical discrete time Wigner approximation (DTWA) and
exact diagonalization (ED), where the breakdown of DTWA signals the emergence
of an extensive number of local symmetries that strongly influence the
thermalization pathway. Our model provides a testbed for quantum simulators and
is directly implementable in large-scale arrays of Rydberg atoms.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [133] [Floquet resonances and redshift-enhanced acceleration radiation from vibrating atoms in Schwarzschild spacetime](https://arxiv.org/abs/2510.11761)
*Reggie C. Pantig,Ali Övgün,Syed Masood,Li-Gang Wang*

Main category: gr-qc

TL;DR: 研究史瓦西黑洞外两能级Unruh-DeWitt探测器在小振幅径向振荡下的加速辐射，在Boulware真空中量化无质量标量场，发现粒子发射谱呈现热Bose-Einstein型分布，并解析展示了曲率/红移对加速辐射的放大效应。


<details>
  <summary>Details</summary>
Motivation: 建立平直时空"振动原子"方案与黑洞时空之间的解析联系，研究曲率如何调制加速辐射效应，同时避免热Hawking背景的干扰。

Method: 在(1+1)径向简化下，使用一阶含时微扰理论计算周期平均跃迁率（Floquet跃迁率），考虑探测器在固定平均半径R0处的小振幅径向振荡。

Result: 粒子发射谱呈现热Bose-Einstein型分布，Floquet共振条件为nΩ>ω0，获得Floquet跃迁率的闭合形式表达式。当R0→∞时还原为平直闵可夫斯基时空结果。近视界处f(R0)<1通过1/√f(R0)增强有效贝塞尔参数，展示曲率/红移对加速辐射的放大。

Conclusion: 该研究提供了平直时空与黑洞时空之间解析可处理的联系，揭示了曲率对加速辐射的调制效应，并讨论了Boulware真空态在视界附近的病态行为。

Abstract: We study acceleration radiation from a two-level Unruh-DeWitt detector that
undergoes small-amplitude radial oscillations at fixed mean radius $R_0$
outside a Schwarzschild black hole. The massless scalar field is quantized in
the Boulware vacuum to isolate curvature-modulated acceleration effects without
a thermal Hawking background. Working in a (1+1) radial reduction and using
first-order time-dependent perturbation, we evaluate the period-averaged
transition rate (or the "Floquet" transition rate). The resulting particle
emission spectrum exhibits a thermal Bose-Einstein-type profile with periodic
trajectory yielding a Floquet resonance condition $n\Omega > \omega_0$ and a
closed-form expression for the Floquet transition rate $\overline{P}_n$ which
reduces to the flat Minkowski spacetime result as $R_0\to\infty$. Near the
horizon, $f(R_0)<1$ enhances the effective Bessel argument by
$1/\sqrt{f(R_0)}$, providing a simple analytic demonstration of
curvature/redshift amplification of acceleration radiation. In particular, the
spectrum weighted by Bessel function becomes ill-defined near the black hole
horizon as $R_{0}\rightarrow 2M$, possibly manifesting the well-known
pathological behavior of Boulware vacuum state. We discuss the regime of
validity (small amplitude, $R_0$ away from the horizon) and outline the
extensions to (3+1) dimensions, including density-of-states and greybody
factors, and to alternative vacuum choices. Our results offer an analytically
tractable link between flat-space "vibrating atom" proposals and black-hole
spacetimes.

</details>


### [134] [Cosmology in the Hoyle Narlikar gravity](https://arxiv.org/abs/2510.11762)
*J. K. Singh,Sonal Aggarwal,Shaily,Hamid Shabani*

Main category: gr-qc

TL;DR: 在Hoyle Narlikar引力框架下研究Quintessence模型的晚期宇宙加速，发现含创造场的理论能更好地解释哈勃张力和宇宙加速膨胀，相比ΛCDM模型更具优势。


<details>
  <summary>Details</summary>
Motivation: 研究Hoyle Narlikar引力框架下含创造场的Quintessence模型，旨在解释晚期宇宙加速现象，并解决哈勃张力问题。

Method: 使用哈勃张力作为物质、辐射和暗能量密度参数的函数，在平坦、均匀、各向同性时空中推导引力场方程，分析各种观测数据集（哈勃数据集、Pantheon+、Pantheon++BAO联合数据）的约束。

Result: 发现含非最小物质相互作用的创造场理论能给出更紧凑的哈勃张力和密度参数约束，比ΛCDM模型更好地解释宇宙加速膨胀，w-dw相分析显示模型具有交替解冻和冻结行为。

Conclusion: Hoyle Narlikar引力框架下的创造场理论能很好地解释晚期宇宙加速，与观测数据一致，且比ΛCDM模型更具优势，模型稳定且观测一致。

Abstract: In this paper, we study the late time cosmic acceleration of the Quintessence
model within the framework of Hoyle Narlikar Gravity, consisting of a creation
field. Using the Hubble tension as a function of the density parameter for
matter, the density parameter for radiation, and the density parameter for dark
energy in the covariant formulation, we find the gravitational field equations
in the spatially flat, homogeneous, and isotropic spacetime to examine the
dynamical mechanism that leads to cosmic acceleration in the late-time
universe. We analyze the observational constraints on the late-time density
parameters using various recent observational datasets, including the Hubble
datasets, Pantheon+, and joint compilation, Pantheon++BAO. Consequently, it is
explicitly demonstrated that late-time cosmic acceleration can be consistent
with recent observational data in Hoyle Narlikar Gravity with non-minimal
matter interaction. In contrast with other modified theories of gravity, it is
observed that the creation field theory with non-minimal matter interaction
renders more compact constraints on the Hubble tension together with density
parameters, and extensively explains the accelerating expansion of the
universe, which makes it a more plausible option compared to the {\Lambda}CDM
model. Furthermore, the w dw phase analysis confirms alternating thawing and
freezing behaviour of the model, with all trajectories ultimately converging
toward the {\Lambda}CDM point, thereby confirming the model stability and the
observational consistency.

</details>


### [135] [Black Hole Ringdown Amplitudescopy](https://arxiv.org/abs/2510.11782)
*Francesco Crescimbeni,Xisco Jimenez-Forteza,Paolo Pani*

Main category: gr-qc

TL;DR: 该论文研究了广义相对论扩展中黑洞环降的额外场诱导模式对引力波信号解释的影响，重点分析了动力学Chern-Simons和爱因斯坦-标量-Gauss-Bonnet理论，发现包含这些模式能改进对理论的约束。


<details>
  <summary>Details</summary>
Motivation: 在广义相对论的扩展理论中，黑洞环降通常会出现两种特征：标准准正规模的理论依赖性偏移和来自额外基本场的附加模式。准确建模这两种效应对于在环降区域进行稳健的广义相对论测试至关重要，而额外场诱导模式在标准环降分析中常被忽略。

Method: 聚焦于动力学Chern-Simons和爱因斯坦-标量-Gauss-Bonnet理论，这两种理论分别通过动态标量场与二次曲率不变量之间的宇称奇偶和宇称偶耦合来表征，研究了额外场诱导模式对引力波信号解释的影响。

Result: 研究表明，包含额外场诱导模式相比标准光谱学能改进对这些理论的约束，并且允许进行基于准正规模偏移的同等约束互补测试。

Conclusion: 分析强调了在环降模板中纳入额外场诱导模式的重要性，并评估了它们在偏置或增强对广义相对论偏差约束方面的潜力。

Abstract: Black hole ringdowns in extensions of General Relativity (GR) generically
exhibit two distinct signatures: (1) theory-dependent shifts in the standard
black-hole quasinormal modes, and (2) additional modes arising from extra
fundamental fields --such as scalar, vector, or tensor degrees of freedom--
that can also contribute to the gravitational-wave signal. As recently argued,
in general both effects are present simultaneously, and accurately modeling
them is essential for robust tests of GR in the ringdown regime. In this work,
we investigate the impact of extra field-induced modes, which are often
neglected in standard ringdown analyses, on the interpretation of
gravitational-wave signals. To provide some concrete examples, we focus on
dynamical Chern-Simons and Einstein-scalar-Gauss-Bonnet theories,
well-motivated extensions of GR, characterized respectively by a parity-odd and
a parity-even coupling between a dynamical scalar field and quadratic curvature
invariants. We show that including extra field-induced modes improves the
bounds on these theories compared to standard spectroscopy and also allows for
equally constraining complementary tests not based on quasinormal mode shifts.
Our analysis highlights the relevance of incorporating extra field-induced
modes in ringdown templates and assesses their potential to either bias or
enhance constraints on GR deviations.

</details>


### [136] [Quasinormal modes from numerical relativity with Bayesian inference](https://arxiv.org/abs/2510.11783)
*Richard Dyer,Christopher J. Moore*

Main category: gr-qc

TL;DR: 提出了一种用于数值相对论波形中数值不确定性的高斯过程模型，以及高效采样准正规模模型后验分布的方法，应用于引力波数据分析。


<details>
  <summary>Details</summary>
Motivation: 数值相对论波形存在不可避免的数值不确定性，量化这些不确定性对于研究合并和环降阶段的次主导或非线性效应至关重要。

Method: 开发了灵活的高斯过程模型来描述数值相对论波形中所有球谐模式的数值不确定性，并设计了高效采样准正规模模型后验分布的程序。

Result: 该方法成功应用于柯西特征演化波形的准正规模分析，证明了其有效性。

Conclusion: 该高斯过程模型使许多贝叶斯数据分析技术能够应用于数值相对论波形，为引力波研究提供了重要工具。

Abstract: Numerical relativity (NR) enables the study of physics in strong and
dynamical gravitational fields and provides predictions for the
gravitational-wave signals produced by merging black holes. Despite the
impressive accuracy of modern codes, the resulting waveforms inevitably contain
numerical uncertainties. Quantifying these uncertainties is important,
especially for studies probing subdominant or nonlinear effects around the
merger and ringdown. This paper describes a flexible Gaussian-process model for
the numerical uncertainties in all the spherical-harmonic waveform modes across
a state-of-the-art catalog of NR waveforms and a highly efficient procedure for
sampling the posteriors of quasinormal mode models without the need for
expensive Markov chain Monte Carlo. The Gaussian-process model is used to
define a likelihood function which allows many Bayesian data analysis
techniques - already widely used in the analysis of experimental gravitational
wave data - to be applied to NR waveforms as well. The efficacy of this
approach is demonstrated by applying it to the analysis of quasinormal modes in
Cauchy-characteristic evolved waveforms.

</details>


### [137] [False Alarm Rates in Detecting Gravitational Wave Lensing from Astrophysical Coincidences: Insights with Model-Independent Technique GLANCE](https://arxiv.org/abs/2510.11790)
*Aniruddha Chakraborty,Suvodip Mukherjee*

Main category: gr-qc

TL;DR: 该研究探讨了引力波强透镜效应检测中的天体物理不确定性，特别是非透镜事件被误判为透镜事件的假警报率问题。通过GLANCE流程对模拟的二元黑洞合并群体进行分析，发现在当前LIGO探测器灵敏度下，约0.01%的事件对可能被错误分类为透镜事件。


<details>
  <summary>Details</summary>
Motivation: 引力波强透镜效应是广义相对论的必然结果，但确认检测这种独特天体物理现象具有挑战性，主要源于探测器噪声和天体物理不确定性。需要开发能够减轻噪声污染的稳健模型独立搜索技术。

Method: 使用GLANCE流程对模拟的合并二元黑洞群体进行模型独立测试，分析不同参数空间下的假警报率分布。

Result: 研究发现约0.01%的事件对可能被错误分类为透镜事件，主要出现在时间延迟约1000天或更长的条件下。展示了在GW源质量、延迟时间和透镜放大参数空间上的假警报率分布。

Conclusion: 该模型独立技术GLANCE在当前LIGO探测器灵敏度下能够自信地检测透镜引力波对，未来将有助于理解下一代引力波探测器的假警报率，这些探测器将能观测到更多引力波源。

Abstract: The strong lensing gravitational waves (GWs) due to intervening massive
astrophysical systems between the source and an observer are an inevitable
consequence of the general theory of relativity, which can produce multiple GW
events in overlapping sky localization error. However, the confirmed detection
of such a unique astrophysical phenomenon is challenging due to several sources
of contamination, arising from detector noise to astrophysical uncertainties.
Robust model-independent search techniques that can mitigate noise
contamination were developed in the past. In this study, we explore the
astrophysical uncertainty associated with incorrectly classifying a pair of
unlensed GW events as a lensed event, and the associated False Alarm Rate (FAR)
depending on the GW source properties. To understand the effect of unlensed
astrophysical GW sources in producing false lensing detections, we have
performed a model-independent test using the pipeline GLANCE on a simulated
population of merging binary-black holes (BBHs). We find that $\sim$ 0.01\% of
the event pairs can be falsely classified as lensed with a lensing threshold
signal-to-noise ratio of 1.5, appearing at a time delay between the event pairs
of $\sim$ 1000 days or more. We show the FAR distribution for the parameter
space of GW source masses, delay time, and lensing magnification parameter over
which the model-independent technique GLANCE can confidently detect lensed GW
pair with the current LIGO detector sensitivity. In the future, this technique
will be useful for understanding the FAR of the upcoming next-generation GW
detectors, which can observe many more GW sources.

</details>


### [138] [Black hole mergers beyond general relativity: a self-force approach](https://arxiv.org/abs/2510.11793)
*Ayush Roy,Lorenzo Küchler,Adam Pound,Rodrigo Panosso Macedo*

Main category: gr-qc

TL;DR: 提出了一种基于自洽力理论的新方法，用于在广义相对论之外的引力理论中模拟黑洞合并和衰荡过程，特别适用于大小黑洞不对称的情况。


<details>
  <summary>Details</summary>
Motivation: 引力波观测为检验广义相对论和黑洞的克尔描述提供了机会，但超越广义相对论的黑洞合并数值模拟一直存在困难，且覆盖完整参数范围的模拟不可行。

Method: 利用自洽力理论，假设一个黑洞远小于另一个，计算合并过程中的自洽力效应，并构建模块化的快速合并-衰荡波形模型。

Result: 首次计算了自洽力对合并波形的影响，证明了该公式能够模块化计算超越广义相对论的效应，并易于集成到快速波形模型中。

Conclusion: 该方法为在广泛引力理论中研究黑洞合并提供了新的第一性原理途径，特别适用于参数范围难以完全覆盖的情况。

Abstract: Gravitational waves from binary black hole mergers provide a glimpse of
gravitational dynamics in its most extreme observable regime, potentially
enabling precision tests of general relativity (GR) and of the Kerr description
of black holes. However, until recently, numerical simulations of black hole
mergers have not been possible in theories beyond GR. While recent
breakthroughs have overcome that obstacle, simulations covering the full,
interesting range of binary parameters remain unfeasible. Here we present a new
first-principles approach to this problem. We show how self-force theory can be
used to model the merger and ringdown of black holes in a broad class of
gravitational theories, assuming one object is much smaller than the other. We
calculate self-force effects on the merger waveform for the first time, and we
demonstrate how our formulation allows us to modularly compute beyond-GR
effects and readily incorporate them into a fast merger-ringdown waveform
model.

</details>


### [139] [Aspects of holographic complexity and volume of the black holes](https://arxiv.org/abs/2510.11833)
*Suraj Maurya,Sashideep Gutti,Rahul Nigam,Swastik Bhattacharya*

Main category: gr-qc

TL;DR: 该论文研究了多种黑洞的复杂性增长率，验证了其与黑洞温度和熵的乘积成正比，并探讨了在不同物理过程中复杂性增长率的变化规律。


<details>
  <summary>Details</summary>
Motivation: 验证Susskind关于黑洞复杂性增长率与温度和熵乘积成正比的猜想，并研究不同物理过程对复杂性增长率的影响。

Method: 使用复杂性-体积(CV)和复杂性-作用量(CA)对偶方法，分析Banados-Teitlboim-Zanelli、Schwarzschild、Reissner-Nordstrom和Kerr黑洞的复杂性增长。

Result: 复杂性增长率确实与黑洞温度和熵的乘积成正比。在不同物理过程中，复杂性增长率变化不同：Penrose过程和超辐射过程中总是增加；粒子吸积过程中可增加、为零或减少；霍金辐射过程中表现出非平凡行为。

Conclusion: 黑洞复杂性增长率与温度和熵乘积成正比，但在粒子吸积等过程中需要考虑扰动黑洞视界动力学的贡献才能获得可靠估计。

Abstract: In this article, we study the complexity growth rate for Banados Teitlboim
Zanelli, Schwarzschild, Reissner Nordstrom, and Kerr black holes using
complexity-volume (CV) and complexity-action (CA) dualities and verify that it
is proportional to the product of the horizon temperature and entropy of the
black holes as conjectured by Susskind. Furthermore, we explore the variation
in the complexity growth rate $\delta \dot{\mathcal{C}}$ under various physical
processes, including the Penrose process, superradiance, particle accretion,
and Hawking radiation, and demonstrate that $\delta \dot{\mathcal{C}}$ exhibits
non-trivial behavior. Under the Penrose process and superradiance, $\delta
\dot{\mathcal{C}}$ always increases, and under particle accretion, $\delta
\dot{\mathcal{C}}$ can increase, remain zero, or decrease depending upon the
direction of angular momentum of an infalling particle. For the cases of
particle accretion, where we find $\delta \dot{\mathcal{C}}$ to be negative, we
argue that for a reliable estimate, one has to take into account the
contribution of the horizon dynamics of the perturbed black hole to the growth
of its complexity.

</details>


### [140] [Shift vector symmetry in the Alcubierre warp drive spacetime geometry](https://arxiv.org/abs/2510.11836)
*Osvaldo L. Santos-Pereira,Everton M. C. Abreu,Marcelo B. Ribeiro*

Main category: gr-qc

TL;DR: 该研究分析了带有宇宙学常数的Alcubierre曲率驱动度规中的爱因斯坦场方程，发现它们可简化为Burgers型方程和热传导型方程，表明曲率驱动泡可解释为传播激波前沿的几何模拟。


<details>
  <summary>Details</summary>
Motivation: 探索Alcubierre曲率驱动度规在包含宇宙学常数情况下的真空解，为理解超光速曲率驱动提供新的理论框架。

Method: 通过适当的假设简化爱因斯坦场方程，将其转化为Burgers型方程和热传导型方程进行分析。

Result: 发现曲率驱动泡可以被解释为传播激波前沿的几何模拟，这为处理超光速曲率驱动提供了新的理论视角。

Conclusion: Alcubierre曲率驱动可能对应于几何激波传播，这为超光速曲率驱动理论开辟了新的研究途径。

Abstract: This work explores the set of coupled partial differential equations of the
Einstein equations yielding vacuum solutions in the original Alcubierre warp
drive metric with the cosmological constant. It is shown that under an
appropriate ansatz they reveal a Burgers-type equation and a heat-type
equation. These results indicate that the spacetime distortion carrying a mass
particle at superluminal speeds, the Alcubierre warp bubble, may be interpreted
as a geometric analog of a propagating shock front, which suggests a possible
novel theoretical framework to deal with superluminal warp speeds.

</details>


### [141] [Lorentz Covariant Supertranslation Frames for the Angular Momentum Aspect](https://arxiv.org/abs/2510.11849)
*Reza Javadinezhad,Massimo Porrati*

Main category: gr-qc

TL;DR: 本文重新审视了广义相对论中角动量和质量偶极子通量定义的模糊性问题，通过定义不变电荷来解决这一模糊性，并提出了确定超平移和时空平移参考框架的条件。


<details>
  <summary>Details</summary>
Motivation: 广义相对论中角动量和质量偶极子通量的定义存在模糊性，这阻碍了对这些物理量的精确测量和理解。本文旨在解决这一长期存在的模糊性问题。

Method: 通过定义不变电荷来消除模糊性，找到固定超平移和时空平移参考框架的条件，并提出测量角动量方面的基本方法。对点粒子产生的度规在牛顿常数的一阶非平凡阶次上明确计算了超平移框架固定条件。

Result: 成功解决了角动量和质量偶极子通量定义的模糊性问题，建立了明确的框架固定条件，使得这些物理量能够被精确测量。

Conclusion: 通过引入不变电荷和框架固定条件，本文为广义相对论中角动量和质量偶极子通量的精确定义和测量提供了系统的方法，解决了长期存在的模糊性问题。

Abstract: In this letter, we review the well known ambiguity in defining angular
momentum (and mass dipole) fluxes in general relativity and we reinterpret
recent works that resolve the ambiguity by defining invariant charges. We
resolve the ambiguity by finding the conditions that fix a frame for
supertranslation and for space-time translation. We also present an elementary
method for measuring the angular momentum aspect and work out explicitly the
supertranslation frame-fixing conditions for the metric created by point
particles to first nontrivial order in the Newton constant.

</details>


### [142] [Impact of facility timing and coordination for next-generation gravitational-wave detectors](https://arxiv.org/abs/2510.11861)
*Ssohrab Borhanian,Arianna Renzini,Philippa S. Cole,Costantino Pacilio,Michele Mancarella,Davide Gerosa*

Main category: gr-qc

TL;DR: 研究下一代引力波探测器网络中设施延迟对科学能力的影响，发现灵敏度指标受影响较小，但定位能力对网络探测器数量高度敏感，延迟会严重影响多信使科学和随机背景搜索。


<details>
  <summary>Details</summary>
Motivation: 爱因斯坦望远镜和宇宙探索者等下一代地面引力波探测器需要联合观测才能充分发挥科学潜力，研究长期延迟对探测器网络科学能力的影响至关重要。

Method: 使用Fisher信息形式模拟探测器网络性能，通过自举模拟方法分析不同网络配置下达到科学目标所需的观测时间，评估灵敏度和定位指标。

Result: 纯灵敏度指标如信噪比受延迟影响较小，但定位指标对网络探测器数量高度敏感，单个探测器延迟相当于网络中断。当前代探测器如LIGO印度可显著缓解延迟对定位的负面影响。

Conclusion: 探测器网络协调对多信使科学和随机背景搜索至关重要，支持性当前代探测器可有效减轻下一代设施延迟带来的负面影响。

Abstract: While the Einstein Telescope and Cosmic Explorer proposals for
next-generation, ground-based detectors promise vastly improved sensitivities
to gravitational-wave signals, only joint observations are expected to enable
the full scientific potential of these facilities, making timing and
coordination between the efforts crucial to avoid missed opportunities. This
study investigates the impact of long-term delays on the scientific
capabilities of next-generation detector networks. We use the Fisher
information formalism to simulate the performance of a set of detector networks
for large, fiducial populations of binary black holes, binary neutron stars,
and primordial black-hole binaries. Bootstrapping the simulated populations, we
map the expected observation times required to reach a number of observations
fulfilling scientific targets for key sensitivity and localization metrics
across various network configurations. We also investigate the sensitivity to
stochastic backgrounds. We find that purely sensitivity-driven metrics such as
the signal-to-noise ratio are not strongly affected by delays between
facilities. This is contrasted by the localization metrics, which are very
sensitive to the number of detectors in the network and, by extension, to
delayed observation campaigns for a detector. Effectively, delays in one
detector behave like network-wide interruptions for the localization metrics
for networks consisting of two next-generation facilities. We examine the
impact of a supporting, current-generation detector such as LIGO India
operating concurrently with next-generation facilities and find such an
addition will greatly mitigate the negative effects of delays for localization
metrics, with important consequences on multi-messenger science and stochastic
searches.

</details>


### [143] [Non-linear causal bulk viscosity in Unified Dark Matter Cosmologies](https://arxiv.org/abs/2510.11900)
*Guillermo Palma,Gabriel Gomez*

Main category: gr-qc

TL;DR: 提出了一种基于非线性以色列-斯图尔特理论的大体积粘性统一暗物质模型，该模型允许粘性流体远离平衡态，能够描述粘性驱动的加速膨胀。通过将宇宙学方程重构为自治动力系统，发现了三种不同的晚期吸引子类型，包括精质解、德西特吸引子和类幽灵解。


<details>
  <summary>Details</summary>
Motivation: 为了构建物理上一致的粘性驱动加速膨胀描述，需要允许粘性流体远离平衡态，而传统的埃卡特理论无法满足这一要求。

Method: 采用非线性扩展的完整因果性以色列-斯图尔特理论，使用标准参数化$\xi = \\xi_{0} \\rho_{m}^{s}$，将宇宙学方程重构为自治动力系统进行分析。

Result: 发现了三种晚期吸引子：$s=1/2$时的精质解（包含德西特行为）、$s<1/2$时的全局精确德西特吸引子、$s\\ge1/2$时的类幽灵解。$s=1/2$时德西特和幽灵吸引子可以共存。

Conclusion: 所有解都满足熵产生条件，宇宙加速膨胀独立于$\\xi_{0}$，这放宽了基于埃卡特理论的粘性模型中$\\xi_{0} \\sim \\mathcal{O}(1)$的强约束要求。

Abstract: We propose a bulk viscous unified dark matter scenario based on a nonlinear
extension of the full causal Israel-Stewart theory. This framework allows the
viscous fluid to remain far from equilibrium, an essential feature for a
physically consistent description of viscosity-driven accelerated expansion. We
adopt the standard parametrization for the bulk viscosity, $\xi = \xi_{0}
\rho_{m}^{s}$, treating $s$ as a free parameter, and study the model in a
spatially flat Friedmann-Robertson-Walker background. By reformulating the
cosmological equations as an autonomous dynamical system, we obtain both
asymptotic analytical solutions and a numerical characterization of the phase
space. At early times, the viscous component can mimic a stiff fluid, while at
intermediate epochs it behaves like dark matter. With a suitable choice of
dynamical variables, the system admits three distinct classes of late-time
attractors. Two of them are separated by a basin-boundary saddle point: (i) a
generic quintessence solution for $s = 1/2$, which encompasses a de Sitter-like
behavior when $\xi_{0}$ satisfies a specific relation involving the nonlinear
parameters; (ii) a global exact de Sitter attractor for $s < 1/2$; and (iii) a
phantom-like solution that emerges for $s \ge 1/2$. In contrast to the generic
$s \ne 1/2$ case, the $s = 1/2$ scenario exhibits a qualitatively different
stability structure, allowing de Sitter and phantom attractors to coexist. All
solutions respect entropy production, and cosmic acceleration emerges
independently of $\xi_{0}$, relaxing the strong bounds $\xi_{0} \sim
\mathcal{O}(1)$ required in Eckart-based viscous models.

</details>


### [144] [Information paradox and island of covariant black holes in LQG](https://arxiv.org/abs/2510.11921)
*Yongbin Du,Jia-Rui Sun,Xiangdong Zhang*

Main category: gr-qc

TL;DR: 研究LQG启发的四维协变黑洞信息悖论，分析两种解法的蒸发速率差异和量子极值表面存在性。


<details>
  <summary>Details</summary>
Motivation: 研究LQG协变黑洞的信息悖论问题，探索不同解法的物理行为差异。

Method: 在Hartle-Hawking态下计算辐射熵，考虑质量损失和灰体因子，应用岛屿处方分析量子极值表面。

Result: 解1中LQG参数ζ增强近视界势垒，加速蒸发；解2在小质量时蒸发缓慢，可能形成残余或黑白洞转变。解1存在量子极值表面，ζ抑制晚期熵增长保持幺正性。

Conclusion: 协变LQG黑洞没有统一的晚期行为，不同解法表现出显著不同的蒸发特性和信息悖论解决方案。

Abstract: We study information paradox of four dimensional covariant black holes
inspired by loop quantum gravity (LQG) with two well motivated solutions. We
first prepare the spacetime in the Hartle-Hawking state, compute the radiation
entropy and recover a linear growth at late time. When considering the mass
loss and incorporating greybody factors, we show that for Solution~1 the LQG
parameter $\zeta$ leaves temperature and Planckian factor of the spectrum
unchanged but enhances the near-horizon barrier, leading to a faster
evaporation rate as $M$ decreases. This behavior contrasts sharply with
Solution~2, which has slow evaporation rate at small $M$ and admits a
non-singular continuation suggestive of a remnant or a black-to-white-hole
transition. We then apply the island prescription on the eternal background and
find that quantum extremal surfaces exist in solution 1 geometries; $\zeta$
primarily shifts the island boundary and suppresses the late time entropy
growth, preserving unitarity. Our results highlight that covariance-respecting
LQG black hole do not exhibit a universal late time behavior.

</details>


### [145] [Phase space analysis of an exponential model in $f(Q)$ gravity including linear dark-sector interactions](https://arxiv.org/abs/2510.12020)
*Ivan R. Vasquez,A. Oliveros*

Main category: gr-qc

TL;DR: 对指数型f(Q)引力模型进行宇宙学分析，通过动力系统形式主义研究其临界点和稳定性，并扩展到包含暗能量-暗物质线性相互作用的情况。


<details>
  <summary>Details</summary>
Motivation: 研究指数型f(Q)引力模型的宇宙学行为，特别是通过动力系统方法分析其临界点和稳定性，以理解宇宙演化中的主要支配时期和晚期de Sitter吸引子的影响。

Method: 采用Böhmer等人引入的方法，将修正的Friedmann方程简化为自治系统。对于指数型f(Q)函数，通过近似处理超越方程来识别临界点，并进行一般稳定性分析。

Result: 模型展现了宇宙的三个主要支配时期（辐射、物质、暗能量），并发现对晚期de Sitter吸引子有非平凡影响。在包含暗能量-暗物质相互作用的情况下，找到了相应的平衡点及其稳定性特性。

Conclusion: 指数型f(Q)引力模型能够成功描述宇宙演化的主要特征，包括三个支配时期和晚期加速膨胀，暗能量-暗物质相互作用进一步丰富了模型的动力学行为。

Abstract: We present a cosmological analysis of an exponential $f(Q)$ gravity model,
within the dynamical systems formalism. Following the method introduced by
B\"ohmer \textit{et al} [Universe \textbf{9} no.4, 166 (2023)], the modified
Friedmann modified equations are successfully reduced to an autonomous system.
Given the exponential form of $f(Q)$, the equilibrium conditions result in
transcendental equations, which we approximate to identify the critical points.
We therefore perform a general stability analysis of these points in terms of
the model parameters. Finally, we extend the model by including a linear dark
energy-dark matter interaction, where the equilibrium points are found with
their stability properties. The model exhibits the three main domination epochs
in the Universe, as well as a non-trivial impact on the late-time de Sitter
attractor.

</details>


### [146] [Coexistence of Spectrally Stable and Unstable Modes in Black Hole Ringdowns](https://arxiv.org/abs/2510.12135)
*Peng Wang,Tianshu Wu*

Main category: gr-qc

TL;DR: 黑洞准正规模式谱的不稳定性可能在外势阱消失后仍然存在，但早期时间域模拟显示稳定的模式主导可观测的铃荡信号。


<details>
  <summary>Details</summary>
Motivation: 研究黑洞准正规模式谱的不稳定性及其对黑洞光谱学的影响，特别是外势阱消失后不稳定性是否持续存在。

Method: 通过时间域模拟分析黑洞准正规模式的稳定性，比较稳定模式与不稳定模式在铃荡信号中的贡献。

Result: 发现谱不稳定性在外势阱消失后仍然存在，但早期铃荡波形主要由稳定模式主导，不稳定模式只有次要贡献。

Conclusion: 黑洞光谱学具有鲁棒性，因为可观测的铃荡信号主要受最稳定的准正规模式控制。

Abstract: Recent studies have shown that a secondary potential barrier, forming a
potential well outside the event horizon, can destabilize the Quasinormal Mode
(QNM) spectrum of black holes. We find that spectral instability may persist
even after the potential well vanishes, giving rise to a distinct family of
spectrally unstable QNMs that differ from the spectrally stable modes localized
near the potential peak and associated with the photon sphere. Nevertheless,
time-domain simulations reveal that early-time ringdown waveforms remain
dominated by stable modes, while unstable modes have only a subdominant
contribution. These results highlight the robustness of black hole
spectroscopy, as the observable ringdown signal is primarily governed by the
most stable QNMs.

</details>


### [147] [The importance of being non-minimally coupled: scalar Hawking radiation from regular black holes](https://arxiv.org/abs/2510.12257)
*Marco Calzà,Massimiliano Rinaldi,Sunny Vagnozzi*

Main category: gr-qc

TL;DR: 该论文研究了在正则黑洞中考虑标量场与曲率的非最小耦合(ξφ²R)对霍金辐射的影响，发现在某些情况下辐射谱可增强或抑制数个数量级。


<details>
  <summary>Details</summary>
Motivation: 在正则黑洞研究中，标量场通常作为几何探针引入，但几乎总是忽略其与曲率的非最小耦合，尽管在弯曲时空这种耦合是普遍存在的。

Method: 研究了四个代表性正则黑洞(Bardeen、Hayward、Simpson-Visser、D'Ambrosio-Rovelli)，在两个基准情况下(共形耦合ξ=1/6和大负值ξ=-10⁴)计算灰体因子和发射谱。

Result: 非最小耦合可显著改变霍金辐射谱，增强或抑制数个数量级。关键因素是ξfR的符号，它决定非最小耦合是抑制还是增强几何势垒。在D'Ambrosio-Rovelli情况下，大负ξ使低能发射谱增强达五个数量级。

Conclusion: 非最小耦合对霍金辐射有重要影响，特别是在原始正则黑洞作为暗物质候选者的情况下，这种耦合会显著影响其蒸发历史。

Abstract: In curved space-time, a scalar field $\phi$ is generically expected to couple
to curvature, via a coupling of the form $\xi\phi^2R$. Yet in the study of
Hawking emission from regular black holes (RBHs), where scalar fields are often
introduced as simple probes of the geometry, and the Ricci scalar is
generically non-zero, this non-minimal coupling is almost always ignored. We
revisit this assumption by studying scalar Hawking emission from four
representative RBHs (the Bardeen, Hayward, Simpson-Visser, and
D'Ambrosio-Rovelli space-times), within two benchmark cases: the conformal case
$\xi=1/6$, and a large negative value $\xi=-10^4$ motivated by Higgs inflation.
We compute the graybody factors and emission spectra, showing that the latter
can be either enhanced or suppressed, even by several orders of magnitude. A
crucial role is played by the sign of the term $\xi fR$, with $f(r)=-g_{tt}$ in
Schwarzschild-like coordinates, as it determines whether the non-minimal
coupling suppresses or enhances the geometric potential barrier. For the
D'Ambrosio-Rovelli case with large negative $\xi$, the low-energy emission
spectrum is enhanced by up to five orders of magnitude, since $\xi fR<0$
throughout the space-time, leading to a deep potential well which broadens the
transmissive window. The deviations we find can be particularly relevant in the
case where primordial RBHs are dark matter candidates, given the impact of the
non-minimal coupling on their evaporation history.

</details>


### [148] [Observational Constraints on Chaplygin Gas Models in Non-Minimally Coupled Power Law $f(Q)$ Gravity with Quasars](https://arxiv.org/abs/2510.12472)
*Nakul Aggarwal,Ali Pourmand,Fatimah Shojai,Harish Parthasarathy*

Main category: gr-qc

TL;DR: 在f(Q)引力框架下研究非最小耦合到物质的宇宙学影响，使用Chaplygin气体变体作为暗能量模型，结合OHD、BAO和QSO观测数据约束模型参数，发现这些模型与ΛCDM存在偏离。


<details>
  <summary>Details</summary>
Motivation: 受Chaplygin气体模型在解释暗能量方面的成功启发，在f(Q)引力框架下探索非最小耦合到物质的宇宙学影响，旨在理解宇宙加速膨胀的机制。

Method: 使用广义Chaplygin气体(GCG)、修正Chaplygin气体(MCG)和可变Chaplygin气体(VCG)作为背景流体，结合OHD、BAO和QSO观测数据进行联合分析，提出QSO数据中共动距离误差的解析表达式。

Result: 联合分析显示，GCG、MCG和VCG模型的宇宙从减速到加速膨胀的转变红移分别为0.620、0.537和0.470，均与ΛCDM模型存在偏离。

Conclusion: 在f(Q)引力框架下，Chaplygin气体变体模型能够解释宇宙加速膨胀，但与标准ΛCDM模型存在显著差异，表明非最小耦合和修正引力效应在宇宙演化中起重要作用。

Abstract: In the framework of $f(Q)$ gravity, where gravity emerges from non-metricity
$Q$, we explore the cosmological implications of its non-minimal coupling to
matter. Inspired by the recent success of Chaplygin gas models in explaining
dark energy, we consider a background fluid composed of baryonic matter,
radiation, and a family of Chaplygin gas variants namely Generalized Chaplygin
Gas (GCG), Modified Chaplygin Gas (MCG), and Variable Chaplygin Gas (VCG). We
constrain these models with three recent observational datasets: Observational
Hubble Data (OHD), Baryonic Acoustic Oscillation (BAO) measurements, and
Quasi-Stellar Objects (QSO) data. For the QSO dataset, we propose an analytical
expression for errors in comoving distance to circumvent the reliance on Monte
Carlo simulations. Using kinematic diagnostics such as the deceleration and
jerk parameters and Om diagnostic, we assess deviations of the proposed models
from $\Lambda$CDM. Our joint analysis of the three datasets reveals that the
transition redshift from a decelerated to an accelerated expansion of the
universe for the GCG, MCG and VCG models is $0.620^{+0.018}_{-0.017}$,
$0.537^{+0.017}_{-0.017}$ and $0.470^{+0.012}_{-0.012}$ respectively,
indicating a departure from $\Lambda$CDM.

</details>


### [149] [OCTOPUS: A Versatile, User-Friendly, and Extensible Public Code for General-Relativistic Ray-Tracing in Spherically Symmetric and Static Spacetimes](https://arxiv.org/abs/2510.12585)
*Shiyang Hu,Shijie Tan,Dan Li,Lina Zhang,Chen Deng,Wenfu Cao*

Main category: gr-qc

TL;DR: OCTOPUS是一个相对论性光线追踪算法，用于计算黑洞事件视界、光子环、临界曲线等关键相对论特征，并模拟黑洞阴影、红移分布、吸积盘图像等观测特征。


<details>
  <summary>Details</summary>
Motivation: 为研究黑洞可观测量和电磁与引力信号的多信使关联提供自动化、模块化的解决方案，降低用户使用门槛。

Method: 基于Fortran和OpenMP加速的框架，使用度量势及其径向导数作为输入，在渐近平坦的球对称弯曲时空中进行光线追踪。

Result: 验证了算法在施瓦西黑洞被Dehnen型暗物质晕包围情况下的精度和效率，发现暗物质晕尺度和密度的增加会增强时空引力场，这反映在黑洞图像和热点光变曲线中。

Conclusion: OCTOPUS是一个高效、精确且易于使用的相对论光线追踪工具，未来计划扩展到轴对称时空。

Abstract: This paper presents OCTOPUS, a relativistic ray-tracing algorithm developed
within a Fortran-based, OpenMP-accelerated framework, designed for
asymptotically flat, spherically symmetric curved spacetimes. The code
efficiently and accurately computes key relativistic features -- including the
black hole event horizon, photon rings, critical curves, and innermost stable
circular orbits -- and simulates black hole shadows, redshift factor
distributions, accretion disk images, toroidal images, as well as gravitational
lensing, light curves, and gravitational radiation from hot-spots. OCTOPUS
provides an automated, modular solution for qualitative studies of black hole
observables and multi-messenger correlations between electromagnetic and
gravitational signals in curved spacetime. Its implementation requires only the
metric potential and its first-, second-, and third-order radial derivatives as
input, ensuring low user barriers while remaining highly extensible and
adaptable. Using a Schwarzschild black hole surrounded by a Dehnen-type dark
matter halo, we thoroughly validate the algorithm's precision, efficiency, and
functionality, and investigate how dark matter halo parameters affect
observational signatures. Our results demonstrate that increasing the scale and
density of the dark matter halo strengthens the spacetime's gravitational
field, an effect clearly reflected in black hole images and supported by
hot-spot light curve signatures. A future version of OCTOPUS, with expanded
capabilities for axisymmetric spacetimes, is planned for release.

</details>


### [150] [Hessian in the spinfoam models with cosmological constant](https://arxiv.org/abs/2510.12755)
*Wojciech Kamiński,Qiaoyin Pan*

Main category: gr-qc

TL;DR: 提出了一种证明自旋泡沫顶点振幅中Hessian矩阵非退化性的通用方法，并将其应用于带宇宙学常数的自旋泡沫模型。


<details>
  <summary>Details</summary>
Motivation: Hessian矩阵的非退化性是应用驻相法的重要必要条件，这对于验证自旋泡沫模型与半经典引力的联系至关重要。

Method: 通过将问题重新表述为相空间中某些子流形的横向相交问题，在平坦SL(2,C)连接的相空间中进行分析。

Result: 证明了对于de Sitter或反de Sitter空间中非退化几何4-单纯形的临界点，Hessian矩阵是非退化的。

Conclusion: 该方法不仅确认了Λ-SF模型与半经典引力的联系，还表明不存在Barrett-Crane模型中出现的异常构型主导贡献，且该方法可推广到其他自旋泡沫模型。

Abstract: In this paper, we introduce a general method to prove the non-degeneracy of
the Hessian in the spinfoam vertex amplitude for quantum gravity and apply it
to the spinfoam models with a cosmological constant ($\Lambda$-SF models). By
reformulating the problem in terms of the transverse intersection of some
submanifolds in the phase space of flat ${\rm SL}(2,\mathbb{C})$ connections,
we demonstrate that the Hessian is non-degenerate for critical points
corresponding to non-degenerate, geometric 4-simplices in de Sitter or anti-de
Sitter space. Non-degeneracy of the Hessian is an important necessary condition
for the stationary phase method to be applicable. With a non-degenerate
Hessian, this method not only confirms the connection of the $\Lambda$-SF model
to semiclassical gravity, but also shows that there are no dominant
contributions from exceptional configurations as in the Barrett-Crane model.
Given its general nature, we expect our criterion to be applicable to other
spinfoam models under mild adjustments.

</details>
