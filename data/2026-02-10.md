<div id=toc></div>

# Table of Contents

- [physics.comp-ph](#physics.comp-ph) [Total: 7]
- [quant-ph](#quant-ph) [Total: 66]
- [cs.LG](#cs.LG) [Total: 270]
- [gr-qc](#gr-qc) [Total: 26]


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [1] [DISCOVER: A Physics-Informed, GPU-Accelerated Symbolic Regression Framework](https://arxiv.org/abs/2602.06986)
*Udaykumar Gajera,Mohsen Sotoudeh,Kanchan Sarkar,Axel Groß*

Main category: physics.comp-ph

TL;DR: DISCOVER是一个用于材料科学等领域的符号回归开源软件包，通过模块化、物理驱动的设计解决现有工具与Python工作流集成差、符号搜索空间控制有限、计算效率低等问题。


<details>
  <summary>Details</summary>
Motivation: 现有符号回归工具存在三个主要问题：1）与现代Python工作流集成差；2）对符号搜索空间的控制有限；3）在大规模研究中计算效率低下。这些问题限制了符号回归在材料科学等领域的应用，特别是在需要可解释性、物理一致性和执行效率的场景中。

Method: DISCOVER采用模块化、物理驱动的设计，允许用户：1）利用领域知识指导符号搜索；2）显式约束特征空间；3）利用可选的GPU加速提高计算效率。软件强调发现物理上有意义的模型，支持可重复和可扩展的符号回归工作流。

Result: 开发了一个开源符号回归软件包DISCOVER，专门针对计算物理、计算化学和材料科学领域的需求，在这些领域中可解释性、物理一致性和执行时间尤为重要。

Conclusion: DISCOVER通过其模块化设计和物理驱动的方法，解决了现有符号回归工具的局限性，为材料科学等领域提供了可重复、可扩展且高效的符号回归解决方案，能够发现物理上有意义的模型。

Abstract: Symbolic Regression (SR) enables the discovery of interpretable mathematical relationships from experimental and simulation data. These relationships are often coined descriptors which are defined as a fundamental materials property that is directly correlated to a desired or undesired functional property of the material. Although established approaches such as Sure Independence Screening and Sparsifying Operator (SISSO) have successfully identified low-dimensional descriptors within large feature spaces many existing SR tools integrate poorly with modern Python workflows, offer limited control over the symbolic search space, or struggle with the computational demands of large-scale studies. This paper introduces DISCOVER (Data-Informed Symbolic Combination of Operators for Variable Equation Regression), an open-source symbolic regression package developed to address these challenges through a modular, physics-motivated design. DISCOVER allows users to guide the symbolic search using domain knowledge, constrain the feature space explicitly, and take advantage of optional GPU acceleration to improve computational efficiency in data-intensive workflows, enabling reproducible and scalable SR workflows. The software is intended for applications in computational physics, computational chemistry, and materials science, where interpretability, physical consistency, and execution time are especially important, and it complements general-purpose SR frameworks by emphasizing the discovery of physically meaningful models.

</details>


### [2] [diffpy.morph: Python tools for model independent comparisons between sets of 1D functions](https://arxiv.org/abs/2602.06987)
*Andrew Yang,Christopher L. Farrow,Pavol Juhás,Luis Kitsu Iglesias,Chia-Hao Liu,Samuel D. Marks,Vivian R. K. Wall,Joshua Safin,Sean M. Drewry,Caden Myers,Dillon F. Hanlon,Nicholas Leonard,Cedomir Petrovic,Ahhyun Jeong,Dmitri V. Talapin,Linda F. Nazar,Haidong Zhou,Samuel W. Teitelbaum,Tim B. van Driel,Soham Banerjee,Emil S. Bozin,Michael F. Toney,Katharine Page,Naomi S. Ginsberg,Simon J. L. Billinge*

Main category: physics.comp-ph

TL;DR: diffpy.morph 是一个用于分析一维科学谱图的Python包，通过应用变换来消除不相关的差异，揭示有意义的科学变化。


<details>
  <summary>Details</summary>
Motivation: 在分析一维科学谱图时，研究人员需要从谱图差异中提取有意义的科学信息，但差异曲线中常包含实验不一致性或良性物理变化等不相关差异，这些干扰使得难以识别真正的化学、结构或其他重要变化。

Method: 开发了diffpy.morph开源Python包，允许研究人员对数据集应用简单的变换（称为"morphs"），以消除不想要的差异，从而揭示非平凡的变化。该方法可应用于任何一维函数，特别针对衍射和PDF数据。

Result: diffpy.morph已成功应用于解决X射线和中子衍射及PDF数据中的一系列实验挑战，能够有效分离不相关差异并揭示有意义的科学变化。

Conclusion: diffpy.morph提供了一个模型无关的强大工具，通过消除不相关差异来增强一维科学谱图的分析能力，有助于从实验数据中提取更深刻的科学见解。

Abstract: diffpy.morph addresses a need to gain scientific insights from 1D scientific spectra in model independent ways. A powerful approach for this is to take differences between pairs of spectra and look for meaningful changes that might indicate underlying chemical, structural, or other modifications. The challenge is that the difference curve may contain uninteresting differences such as experimental inconsistencies and benign physical changes such as the effects of thermal expansion. diffpy.morph allows researchers to apply simple transformations, or "morphs", to one of the datasets to remove the unwanted differences revealing, when they are present, non-trivial differences. diffpy.morph is an open-source Python package available on the Python Package Index and conda-forge. Here, we describe its functionality and apply it to solve a range of experimental challenges on diffraction and PDF data from x-rays and neutrons, though we note that it may be applied to any 1D function in principle.

</details>


### [3] [Event-Chain Monte Carlo: The global-balance breakthrough](https://arxiv.org/abs/2602.07199)
*E. A. J. F. Peters*

Main category: physics.comp-ph

TL;DR: 2009年Bernard等人提出的Event-Chain Monte Carlo算法通过放弃细致平衡条件、采用全局平衡原则，实现了硬球系统的无拒绝确定性采样，开创了蒙特卡洛采样的新范式。


<details>
  <summary>Details</summary>
Motivation: 传统蒙特卡洛采样受限于细致平衡条件，在密集粒子系统中收敛缓慢。作者寻求突破这一限制，探索更高效的采样方法。

Method: 放弃传统的细致平衡条件，采用更基本的全局平衡原则，提出Event-Chain Monte Carlo算法，实现无拒绝、确定性的持续定向动力学采样。

Result: ECMC算法显著加速了密集粒子系统的平衡过程，后续发展为更广泛的Event-Driven Monte Carlo框架，可推广到连续势能和现代提升马尔可夫链形式。

Conclusion: ECMC算法从硬球系统的特定结果发展为强大的一般采样算法类别，代表了蒙特卡洛采样的范式转变，为复杂系统模拟提供了新工具。

Abstract: The seminal 2009 paper by Bernard, Krauth, and Wilson marked a paradigm shift in Monte Carlo sampling. By abandoning the restrictive condition of detailed balance in favor of the more fundamental principle of global balance, they introduced the Event-Chain Monte Carlo (ECMC) algorithm, which achieves rejection-free, deterministic sampling for hard spheres. This breakthrough demonstrated that persistent, directional dynamics could dramatically accelerate equilibration in dense particle systems. In this commentary, we review this foundational work and elucidate its underlying mechanism using the broader Event-Driven Monte Carlo (EDMC) framework developed in subsequent years. We show how the original hard-sphere concept naturally generalizes to continuous potentials and modern lifted Markov chain formalisms, transforming a surprising specific result into a powerful general class of sampling algorithms.

</details>


### [4] [Compressed Sensing Methods for Memory Reduction in Monte Carlo Simulations](https://arxiv.org/abs/2602.07771)
*Ethan Lame,Camille Palmer,Todd Palmer,Ilham Variansyah*

Main category: physics.comp-ph

TL;DR: 该论文研究在蒙特卡罗中子输运模拟中应用压缩感知技术，通过重叠单元收集计数数据，实现显著的内存减少（2D达81.25%，3D达96.25%），同时保持重建误差在参考结果1个标准差内。


<details>
  <summary>Details</summary>
Motivation: 蒙特卡罗中子输运模拟计算密集且内存需求高，传统方法需要大量样本。压缩感知技术能够从较少样本中准确重建信号，为解决这一计算挑战提供可能。

Method: 采用压缩感知技术，通过重叠单元收集计数数据，使用基追踪去噪算法进行信号重建。研究样本数量对重建精度的影响，并分析稀疏参数对重建质量的影响。

Result: 在三个测试案例中，实现了2D重建81.25%和3D重建96.25%的内存减少。部分场景的重建误差在高保真参考结果的1个标准差范围内。增加样本数量能提高重建精度，但边际收益递减。

Conclusion: 压缩感知技术能显著减少蒙特卡罗中子模拟的内存需求，同时保持可接受的精度。稀疏参数选择对重建质量至关重要，该方法为高保真中子输运模拟提供了高效的计算方案。

Abstract: Monte Carlo simulations of neutronic systems are computationally intensive and demand significant memory resources for high-fidelity modeling. Compressed sensing enables accurate reconstruction of signals from significantly fewer samples than traditional methods. The specific implementation of compressed sensing investigated here involves the use of overlapping cells to collect tallies. Increasing the number of samples improves the reconstruction accuracy, although the marginal gains diminish with more samples. Reconstruction quality is strongly influenced by the sparsity parameter used in basis pursuit denoising. Across the three test cases considered, memory reductions of up to 81.25% (96.25%) are demonstrated for 2D (3D) reconstructions, with select scenarios achieving reconstruction errors within 1 standard deviation of the corresponding high-fidelity reference results.

</details>


### [5] [dewi-kadita: A Python Library for Idealized Fish Schooling Simulation with Entropy-Based Diagnostics](https://arxiv.org/abs/2602.07948)
*Sandy H. S. Herho,Iwan P. Anwar,Faruq Khadami,Alfita P. Handayani,Karina A. Sujatmiko,Kamaluddin Kasim,Rusmawan Suwarman,Dasapta E. Irawan*

Main category: physics.comp-ph

TL;DR: 开发了dewi-kadita开源Python库，实现三维Couzin区域模型，提供七个信息熵指标和综合的海洋集群指数，用于量化鱼群集体运动的组织特征。


<details>
  <summary>Details</summary>
Motivation: 鱼群集体运动是活性物质系统中自组织现象的典型例子，但目前模拟和分析这些动态的计算工具在不同研究组之间分散且不统一。需要标准化的、可重复的基础设施来支持集体行为建模研究。

Method: 开发了dewi-kadita开源Python库，实现三维Couzin区域模型。引入了七个信息论指标（集群内聚熵、极化熵、深度分层熵、角动量熵、最近邻熵、速度相关熵和集群形状熵），这些指标组合成海洋集群指数（OSI）。使用Numba JIT编译加速成对相互作用计算，支持NetCDF4输出格式。

Result: 验证了四种典型配置（集群、环状、动态平行、高度平行）的正确性：集群保持无序状态（P<0.1，OSI≈0.71），高度平行状态达到P=0.998，OSI=0.24。熵框架成功区分了具有相似有序参数但组织机制不同的环状和动态平行配置。Numba JIT编译使成对相互作用计算加速10-100倍，能在标准工作站上5分钟内模拟150-250个代理在1000-2000个时间步长的动态。

Conclusion: dewi-kadita库为海洋集体行为研究提供了标准化的、可重复的计算基础设施，类似于成熟的分子动力学代码，填补了集体行为建模中标准化工具的空白。信息熵框架能够捕捉经典有序参数无法访问的组织特征。

Abstract: Collective motion in fish schools exemplifies emergent self-organization in active matter systems, yet computational tools for simulating and analyzing these dynamics remain fragmented across research groups. We present dewi-kadita, an open-source Python library implementing the three-dimensional Couzin zone-based model with comprehensive entropy diagnostics tailored for marine collective behavior research. The library introduces seven information-theoretic metrics -- school cohesion entropy, polarization entropy, depth stratification entropy, angular momentum entropy, nearest-neighbor entropy, velocity correlation entropy, and school shape entropy -- that characterize distinct organizational features inaccessible to classical order parameters. These metrics combine into an Oceanic Schooling Index (OSI) providing a single scalar measure of collective disorder. Validation across four canonical configurations (swarm, torus, dynamic parallel, highly parallel) confirms correct reproduction of known phase behaviors: the swarm maintains disorder with polarization $P < 0.1$ and OSI $\approx 0.71$, while the highly parallel state achieves $P = 0.998$ with OSI $= 0.24$ and velocity correlation entropy vanishing to zero. The entropy framework successfully discriminates the torus and dynamic parallel configurations that exhibit comparable order parameter magnitudes through different organizational mechanisms. Numba just-in-time (JIT) compilation accelerates pairwise interaction calculations by $10$--$100\times$, enabling simulations of $150$--$250$ agents over $1000$--$2000$ time steps within five minutes on standard workstation hardware. NetCDF4 output ensures interoperability with oceanographic analysis tools. The library addresses the need for standardized, reproducible infrastructure in collective behavior modeling analogous to established molecular dynamics codes.

</details>


### [6] [An intramembranous ossification model for the in-silico analysis of bone tissue formation in tooth extraction sites](https://arxiv.org/abs/2602.08492)
*Jennifer Paola Corredor-Gómez,Andrés Mauricio Rueda-Ramírez,Miguel Alejandro Gamboa-Márquez,Carolina Torres-Rodríguez,Carlos Julio Cortés-Rodríguez*

Main category: physics.comp-ph

TL;DR: 开发了一个用于描述拔牙部位骨组织形成的膜内骨化数学模型，重点关注血管生成、氧依赖效应和生长因子诱导的成纤维细胞凋亡，并通过有限元方法实现和验证。


<details>
  <summary>Details</summary>
Motivation: 准确建模生物过程可以通过计算机辅助（in-silico）测试预测活体组织的时空行为，这对于制定医疗策略非常有用，可以避免体内实验的费用和伦理问题。特别是开发一个描述拔牙部位骨愈合的模型，有助于在牙科手术中选择合适的外科技术。

Method: 提出了一个膜内骨化数学模型，描述了不同类型细胞在生化因子影响下相互作用、合成和降解细胞外基质的机制。特别关注血管生成、氧依赖效应和生长因子诱导的成纤维细胞凋亡。考虑到下颌骨深度依赖性血管化及其对骨愈合的影响，提出了切断牙周韧带（PDL）上细胞分布的功能描述。使用有限元方法（FEM）实现了该模型。

Result: 通过模拟文献中报告的狗体内实验，成功验证了开发的模型。模型结果与实验数据吻合良好，平均绝对误差为3.04%。

Conclusion: 这里提出的数学框架可能成为设计未来体外和体内测试的重要工具，也为未来关于骨整合和力学生物学的计算机模拟研究提供了先例。

Abstract: The accurate modeling of biological processes allows to predict the spatio-temporal behavior of living tissues by computer-aided (in-silico) testing, a useful tool for the development of medical strategies, avoiding the expenses and potential ethical implications of in-vivo experimentation. A model for bone healing in mouth would be useful for selecting proper surgical techniques in dental procedures. In this paper, the formulation and implementation of a model for Intramembranous Ossification is presented aiming to describe the complex process of bone tissue formation in tooth extraction sites. The model consists in a mathematical description of the mechanisms in which different types of cells interact, synthesize and degrade extra-cellular matrices under the influence of biochemical factors. Special attention is given to angiogenesis, oxygen-dependent effects and growth factor-induced apoptosis of fibroblasts. Furthermore, considering the depth-dependent vascularization of mandibular bone and its influence on bone healing, a functional description of the cell distribution on the severed periodontal ligament (PDL) is proposed. The developed model was implemented using the finite element method (FEM) and successfully validated by simulating an animal in-vivo experiment on dogs reported in the literature. A good fit between model outcome and experimental data was obtained with a mean absolute error of 3.04%. The mathematical framework presented here may represent an important tool for the design of future in-vitro and in-vivo tests, as well as a precedent for future in-silico studies on osseointegration and mechanobiology.

</details>


### [7] [Tikhonov regularization-based reconstruction of partial scattering functions obtained from contrast variation small-angle neutron scattering](https://arxiv.org/abs/2602.08601)
*Manabu Machida,Koichi Mayumi*

Main category: physics.comp-ph

TL;DR: 提出使用Tikhonov正则化改进对比度变化小角中子散射中的部分散射函数重建稳定性


<details>
  <summary>Details</summary>
Motivation: 传统CV-SANS方法中，通过奇异值分解估计部分散射函数时，当某些部分散射函数的绝对值较小时，由于奇异值差异显著，会导致重建结果不稳定

Method: 引入Tikhonov正则化方法来稳定部分散射函数的重建过程

Result: 该方法能够更稳定地重建部分散射函数，特别是那些绝对值较小的函数

Conclusion: Tikhonov正则化是解决CV-SANS中部分散射函数重建不稳定问题的有效方法

Abstract: Contrast variation small-angle neutron scattering (CV-SANS) has been widely employed for nano structural analysis of multicomponent systems. In CV-SANS experiments, scattering intensities of samples with different scattering co\ ntrasts are decomposed into partial scattering functions, corresponding to structure of each component and cross-correlation between different components, by singular value decomposition (SVD). However, the estimation of partial scattering functions with small absolute values often suffers from instability due to the significant differences in the singular values. In this paper, we propose a remedy for this instability by introducing the Tikhonov regularization, which ensures more stable reconstruction of the partial scattering functions.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [8] [Alleviating Post-Linearization Challenges for Solving Nonlinear Systems on a Quantum Computer](https://arxiv.org/abs/2602.07097)
*Tayyab Ali*

Main category: quant-ph

TL;DR: 提出一种高效的数据访问模型，通过Sigma基分解将非线性系统的Carleman线性化表示加载到量子计算机上，相比传统Pauli分解实现了指数级减少分解项数


<details>
  <summary>Details</summary>
Motivation: 量子力学的线性特性限制了当前量子硬件直接求解非线性微分方程系统。虽然可以通过Carleman线性化将有限非线性系统转化为高维无限线性系统，但需要高效的方法在量子计算机上加载这种表示

Method: 1) 使用Sigma基（非酉算子的加权和）分解Carleman线性化表示的哈密顿量；2) 与传统Pauli线性组合分解相比，Sigma基实现了指数级减少分解项数；3) 利用酉完备化概念为分解中的每个加权张量积分量构建电路实现

Result: 提出的Sigma基分解方法相比传统Pauli分解，在将非线性系统的无限线性表示（截断到N阶）加载到量子计算机时，实现了分解项数的指数级减少

Conclusion: 该方法为在现有量子硬件上通过Carleman线性化间接求解非线性系统提供了一种高效的数据加载方案，通过创新的Sigma基分解和酉完备化技术显著降低了实现复杂度

Abstract: The linearity inherent in quantum mechanics limits current quantum hardware from directly solving nonlinear systems governed by nonlinear differential equations. One can opt for linearization frameworks such as Carleman linearization, which provides a high dimensional infinite linear system corresponding to a finite nonlinear system, as an indirect way of solving nonlinear systems using current quantum computers. We provide an efficient data access model to load this infinite linear representation of the nonlinear system, upto truncation order $N$, on a quantum computer by decomposing the Hamiltonian into the weighted sum of non-unitary operators, namely the Sigma basis. We have shown that the Sigma basis provides an exponential reduction in the number of decomposition terms compared to the traditional decomposition, which is usually done in a linear combination of Pauli operators. Once the Hamiltonian is decomposed, we then use the concept of unitary completion to construct the circuit for the implementation of each weighted tensor product component $\mathcal{H}_{j}$ of the decomposition.

</details>


### [9] [Beyond Wigner: Non-Invertible Symmetries Preserve Probabilities](https://arxiv.org/abs/2602.07110)
*Thomas Bartsch,Yuhan Gai,Sakura Schafer-Nameki*

Main category: quant-ph

TL;DR: 论文解决了非可逆对称性与维格纳定理的矛盾，提出对称性缺陷作为不同希尔伯特空间之间的等距映射，使得非可逆对称性自然表现为保迹量子通道。


<details>
  <summary>Details</summary>
Motivation: 传统量子理论中的对称性被扩展到非可逆的广义对称性，这与维格纳定理要求量子对称性必须由（反）幺正算子实现相矛盾。需要解决这一理论冲突。

Method: 提出对称性缺陷不是作用在固定希尔伯特空间上的幺正算子，而是作为不同希尔伯特空间（由扭曲扇区构造）之间的等距映射。这要求对称性范畴必须是幺正的。

Result: 非可逆对称性自然地表现为保迹量子通道，解决了与维格纳定理的矛盾。通过Tambara-Yamagami、Fibonacci、Yang-Lee以及高阶范畴对称性等例子进行了验证。

Conclusion: 通过将对称性缺陷重新解释为希尔伯特空间之间的等距映射，成功调和了非可逆对称性与维格纳定理的矛盾，为广义对称性理论提供了自洽的数学框架。

Abstract: In recent years, the traditional notion of symmetry in quantum theory was expanded to so-called generalised or categorical symmetries, which, unlike ordinary group symmetries, may be non-invertible. This appears to be at odds with Wigner's theorem, which requires quantum symmetries to be implemented by (anti)unitary -- and hence invertible -- operators in order to preserve probabilities. We resolve this puzzle for (higher) fusion category symmetries $\mathcal{C}$ by proposing that, instead of acting by unitary operators on a fixed Hilbert space, symmetry defects in $\mathcal{C}$ act as isometries between distinct Hilbert spaces constructed from twisted sectors. As a result, we find that non-invertible symmetries naturally act as trace-preserving quantum channels. Crucially, our construction relies on the symmetry category $\mathcal{C}$ being unitary. We illustrate our proposal through several examples that include Tambara-Yamagami, Fibonacci, and Yang-Lee as well as higher categorical symmetries.

</details>


### [10] [Entanglement harvesting in conformal field theory](https://arxiv.org/abs/2602.07112)
*Kelly Wurtz,Caroline Lima,Robert C. Myers,Eduardo Martín-Martínez*

Main category: quant-ph

TL;DR: 研究d维共形场论中通过点状Unruh-DeWitt探测器与标量主算符耦合的纠缠收获，扩展了自由场理论到相互作用共形理论和任意空间维度。


<details>
  <summary>Details</summary>
Motivation: 将标准纠缠收获协议扩展到自由场之外的相互作用共形场论和任意空间维度，研究算子标度维度对纠缠收获的影响，并在全息CFT中区分场收获纠缠与通信介导纠缠。

Method: 使用点状Unruh-DeWitt探测器与标量主算符耦合，分析d维共形场论中的纠缠收获。对于全息CFT，利用体有效场理论分离场收获纠缠和通信介导纠缠。推导渐近闭式近似并与数值结果比较。

Result: 增加算子标度维度会抑制负性和互信息，反映关联的更快衰减。在全息CFT中，体有效场理论能够分离场收获纠缠和通信介导纠缠。推导的渐近闭式近似与数值结果吻合良好。

Conclusion: 该研究扩展了纠缠收获到相互作用共形场论，揭示了算子标度维度对纠缠收获的抑制作用，并在全息CFT中提供了区分不同类型纠缠的理论框架，为理解量子场论中的纠缠动力学提供了新视角。

Abstract: We study entanglement harvesting in general $d$-dimensional conformal field theories using pointlike Unruh-DeWitt detectors coupled to scalar primary operators. This extends standard harvesting protocols beyond free fields to interacting conformal theories and arbitrary spatial dimensions. We find that increasing the operator scaling dimension suppresses both negativity and mutual information, reflecting the faster decay of correlations. For holographic CFTs, we show that bulk effective field theory enables a separation between field-harvested and communication-mediated entanglement. We also derive asymptotic, closed-form approximations that agree well with numerical results.

</details>


### [11] [Performance limits of a quantum receiver for detecting phase-modulated communication signals](https://arxiv.org/abs/2602.07123)
*William M. Watkins,Leigh Norris,Paraj Titum*

Main category: quant-ph

TL;DR: 量子传感器用于解调相位调制电磁波信息，通过广义累积展开建模噪声量子接收机，比较不同量子解调协议性能，发现纠缠量子传感器在某些条件下可超越经典电小天线的信道容量限制。


<details>
  <summary>Details</summary>
Motivation: 量子传感器具有卓越的灵敏度和紧凑尺寸，是检测微弱电磁信号的理想选择。本文旨在分析基于量子传感器的接收链在解调相位调制电磁波信息时的性能，探索量子传感器在通信系统中的潜力。

Method: 引入广义累积展开来建模噪声量子接收机，使用比特错误概率（BEP）和信道容量作为性能指标，比较非纠缠和纠缠量子传感器集合的性能，以二进制相移键控（BPSK）作为相位调制的代表性示例。

Result: 识别出量子传感器集合的信道容量在某些条件下可能超越经典电小天线的极限。讨论了量子协议的修改方案，使其在传感器噪声和信道失真情况下仍能实现高保真数据恢复。探索了以NV-金刚石为量子传感器平台的实用性能极限。

Conclusion: 量子传感器在解调相位调制电磁波方面具有潜力，特别是在某些条件下可超越经典电小天线的性能限制。通过协议优化，量子接收链能够在噪声和失真环境下实现高保真数据恢复，为量子传感器在通信系统中的应用提供了理论基础。

Abstract: Quantum sensors are an ideal candidate for detecting weak electromagnetic signals because of their exceptional sensitivity and compact form factor. In this work, we analyze the performance of a quantum-sensor-based receive chain for demodulating information encoded in phase-modulated electromagnetic waves. We introduce a generalized cumulant expansion to model a noisy quantum receiver and use it to compare the performance of various quantum demodulation protocols. Employing bit error probability (BEP) and channel capacity as quantitative performance metrics, we compare the capabilities of ensembles of quantum sensors - both unentangled and entangled - using Binary Phase-Shift Keying (BPSK) as a representative example of phase modulation. We identify conditions when the channel capacity of an ensemble of quantum sensors may surpass the limits of a classical electrically small antenna. Additionally, we discuss modifications to the quantum protocol that enables high-fidelity data recovery even in the presence of sensor noise and channel distortions. Finally, we explore practical performance limits of such a quantum receive chain, with a focus on NV-diamond as the quantum sensor platform.

</details>


### [12] [Putting fermions onto a digital quantum computer](https://arxiv.org/abs/2602.07151)
*Riley W. Chien,Mitchell L. Chiew,Brent Harrison,Jason Necaise,Weishi Wang,Maryam Mudassar,Campbell McLauchlan,Thomas M. Henderson,Gustavo E. Scuseria,Sergii Strelchuk,James D. Whitfield*

Main category: quant-ph

TL;DR: 本文回顾了将费米子自由度编码到量子比特的方法，旨在消除高维费米子系统本质上更难处理的误解。


<details>
  <summary>Details</summary>
Motivation: 量子计算机有望成为研究物理量子系统的强大工具，但基于量子比特的量子计算机天然适合研究自旋-1/2系统，而包含其他自由度的系统（如费米子系统）需要先编码到量子比特中。费米子自由度变换在物理学中一直是重要工具，现在在基于量子比特的量子计算机上模拟费米子系统提供了新的应用场景。

Method: 本文是一篇综述性视角文章，回顾了将费米子自由度编码到量子比特的各种方法。文章系统性地梳理了现有的编码技术，并分析比较了不同方法的优缺点。

Result: 文章澄清了一个长期存在的误解：即高维（超过一维）费米子系统本质上比低维系统更难处理。通过系统回顾编码方法，表明这种困难并非根本性的，而是可以通过适当的编码策略来解决。

Conclusion: 费米子自由度到量子比特的编码方法是量子计算模拟费米子系统的重要工具，高维费米子系统并不比低维系统本质上更困难，适当的编码策略可以有效解决这一问题，为在量子计算机上研究更广泛的物理系统铺平道路。

Abstract: Quantum computers are expected to become a powerful tool for studying physical quantum systems. Consequently, a number of quantum algorithms for studying the physical properties of such systems have been developed. While qubit-based quantum computers are naturally suited to the study of spin-1/2 systems, systems containing other degrees of freedom must first be encoded into qubits. Transformations to and from fermionic degrees of freedom have long been an important tool in physics and, now the simulation of fermionic systems on quantum computers based on qubits provides yet another application. In this perspective, we review methods for encoding fermionic degrees of freedom into qubits and attempt to dispel the persistent notion that fermionic systems beyond one dimension are fundamentally more difficult to deal with.

</details>


### [13] [Measurement-Based Preparation of Higher-Dimensional AKLT States and Their Quantum Computational Power](https://arxiv.org/abs/2602.07201)
*Wenhan Guo,Mikhail Litvinov,Tzu-Chieh Wei,Abid Khan,Kevin C. Smith*

Main category: quant-ph

TL;DR: 该论文研究了在多维图上制备AKLT态的新方法，包括随机装饰和随机键合变体，并证明了这些态在测量基量子计算中具有与原始AKLT态相同的计算能力。


<details>
  <summary>Details</summary>
Motivation: 研究如何在多维图上高效制备AKLT态，并探索这些态在测量基量子计算中的应用潜力。传统AKLT态制备方法有限，需要开发新的制备方案来扩展其应用范围。

Method: 提出两种新方法：1) 随机装饰AKLT态 - 通过概率性在边中插入顶点来装饰图；2) 随机键合AKLT态 - 在键自由度中使用任意贝尔态而非仅单态。采用常时融合测量基方案进行制备，并分析其计算能力。

Result: 1) 证明了随机装饰AKLT态至少具有与非随机态相同的计算能力；2) 在Bethe格及其装饰版本上实现了确定性常时制备方案；3) 随机键合AKLT态可转换为编码随机图态；4) 从渗流角度证明随机键合AKLT态具有与原始单态键合AKLT态相似的计算能力。

Conclusion: 该研究扩展了AKLT态的制备方法和应用范围，证明了随机装饰和随机键合变体在测量基量子计算中具有实用价值，为量子计算资源态的设计提供了新思路。

Abstract: We investigate a constant-time, fusion measurement-based scheme to create AKLT states beyond one dimension. We show that it is possible to prepare such states on a given graph up to random spin-1 `decorations', each corresponding to a probabilistic insertion of a vertex along an edge. In investigating their utility in measurement-based quantum computation, we demonstrate that any such randomly decorated AKLT state possesses at least the same computational power as non-random ones, such as those on trivalent planar lattices. For AKLT states on Bethe lattices and their decorated versions we show that there exists a deterministic, constant-time scheme for their preparation. In addition to randomly decorated AKLT states, we also consider random-bond AKLT states, whose construction involves any of the canonical Bell states in the bond degrees of freedom instead of just the singlet in the original construction. Such states naturally emerge upon measuring all the decorative spin-1 sites in the randomly decorated AKLT states. We show that those random-bond AKLT states on trivalent lattices can be converted to encoded random graph states after acting with the same POVM on all sites. We also argue that these random-bond AKLT states possess similar quantum computational power as the original singlet-bond AKLT states via the percolation perspective.

</details>


### [14] [The continuous spectrum of bound states in expulsive potentials](https://arxiv.org/abs/2602.07281)
*H. Sakaguchi,B. A. Malomed,A. C. Aristotelous,E. G. Charalampidis*

Main category: quant-ph

TL;DR: 研究发现，比二次反谐振子更陡峭的排斥势能可以产生可归一化的局域化本征态，这些态构成连续谱，并提出了连续谱束缚态概念的扩展。


<details>
  <summary>Details</summary>
Motivation: 传统直觉认为陡峭的排斥势会使量子态高度离域化，但本文挑战这一观点，探索比二次反谐振子更陡峭的排斥势能否产生局域化的本征态。

Method: 研究一维和二维薛定谔方程中比二次反谐振子更陡峭的排斥势，推导本征态的渐近近似解，获得二维涡旋态的特殊精确解，并考虑非线性Gross-Pitaevskii方程的扩展。

Result: 发现这些陡峭排斥势确实能产生可归一化的局域化本征态，构成连续谱；一维有空间偶奇对称态，二维可携带任意涡旋度；渐近近似与数值解高度一致；非线性情况下，立方非线性保持稳定性，而五次自聚焦项会导致超过临界值的态发生动力学坍缩。

Conclusion: 研究扩展了连续谱束缚态的概念，表明陡峭排斥势能产生局域化本征态，为量子力学和傍轴光子学提供了新见解，并揭示了非线性效应对这些态稳定性的影响。

Abstract: On the contrary to the common intuition that a steep expulsive potential makes quantum states widely delocalized, we demonstrate that one- and two-dimensional (1D and 2D) Schrödinger equations, which include expulsive potentials that are \emph{steeper than the quadratic} (anti-harmonic-oscillator) ones, give rise to \emph{normalizable} (effectively localized) eigenstates. These states constitute full continuous spectra in the 1D and 2D cases alike. In 1D, these are spatially even and odd eigenstates. The 2D states may carry any value of the vorticity (alias magnetic quantum number). Asymptotic approximations for wave functions of the 1D and 2D eigenstates, valid far from the center, are derived analytically, demonstrating excellent agreement with numerically found counterparts. Special exact solutions for vortex states are obtained in the 2D case. These findings suggest an extension of the concept of bound states in the continuum, in quantum mechanics and paraxial photonics. Gross-Pitaevskii equations are considered as the nonlinear extension of the 1D and 2D settings. In 1D, the cubic nonlinearity slightly deforms the eigenstates, maintaining their stability. On the other hand, the quintic self-focusing term, which occurs in the photonic version of the 1D model, initiates the dynamical collapse of states whose norm exceeds a critical value.

</details>


### [15] [Encoding Matters: Benchmarking Binary and D-ary Representations for Quantum Combinatorial Optimization](https://arxiv.org/abs/2602.07357)
*Shashank Sanjay Bhat,Peiyong Wang,Joseph West,Udaya Parampalli*

Main category: quant-ph

TL;DR: QUDO（二次无约束D元优化）作为QUBO的替代方案，通过在高维希尔伯特空间中直接编码决策变量，避免惩罚项和辅助变量，在量子组合优化中实现更好的可扩展性和表达性。


<details>
  <summary>Details</summary>
Motivation: 传统QUBO方法通过惩罚项引入约束，需要大量辅助变量，增加了哈密顿量的复杂度，限制了近期量子设备的可扩展性。需要寻找更高效的组合优化问题表示方法。

Method: 系统研究QUDO作为替代方案，在高维希尔伯特空间中直接编码决策变量。使用qudit QAOA（量子近似优化算法）实现，并在旅行商问题、车辆路径问题、图着色、作业调度、Max-K-Cut等多个问题类上进行基准测试。

Result: 与二进制QUBO相比，QUDO在可比较的电路深度下，持续获得改进的近似比，并显著减少计算开销。QUDO能够自然捕获结构约束，无需复杂的惩罚构造。

Conclusion: QUDO是量子组合优化的可扩展且表达性强的表示方法，为近期量子设备上的组合优化问题提供了更高效的解决方案。

Abstract: Combinatorial optimization problems are typically formulated using Quadratic Unconstrained Binary Optimization (QUBO), where constraints are enforced through penalty terms that introduce auxiliary variables and rapidly increase Hamiltonian complexity, limiting scalability on near term quantum devices. In this work, we systematically study Quadratic Unconstrained D-ary Optimization (QUDO) as an alternative formulation in which decision variables are encoded directly in higher dimensional Hilbert spaces. We demonstrate that QUDO naturally captures structural constraints across a range of problem classes, including the Traveling Salesman Problem, two variants of the Vehicle Routing Problem, graph coloring, job scheduling, and Max-K-Cut, without the need for extensive penalty constructions. Using a qudit-level implementation of the Quantum Approximate Optimization Algorithm (qudit QAOA), we benchmark these formulations against their binary QUBO counterparts and exact classical solutions. Our study show consistently improved approximation ratios and substantially reduced computational overhead at comparable circuit depths, highlighting QUDO as a scalable and expressive representation for quantum combinatorial optimization.

</details>


### [16] [The ABL Rule and the Perils of Post-Selection](https://arxiv.org/abs/2602.07402)
*Jacob A. Barandes*

Main category: quant-ph

TL;DR: 该论文挑战ABL规则的传统解释，指出其中存在范畴错误，混淆了单个系统的可观测量与物理系综中才出现的涌现可观测量。


<details>
  <summary>Details</summary>
Motivation: ABL规则被提出作为量子理论的时间对称形式，用于计算新型条件概率，但后续研究赋予其额外意义，包括声称它支持违反不确定性原理。本文旨在挑战这些主张，并揭示ABL规则解释中的根本问题。

Method: 通过识别ABL规则中的范畴错误，分析混淆单个系统可观测量与系综涌现可观测量的根本问题。同时指出相关文献中的其他问题，包括后选择误用、对经典公式的模式匹配依赖，以及测量主义立场。

Result: 揭示了ABL规则解释中的根本缺陷，表明其声称支持违反不确定性原理等主张存在问题。指出了研究文献中的错误推理和概念混淆。

Conclusion: ABL规则的解释存在根本性错误，混淆了不同层次的可观测量概念。量子理论中的测量和概率解释需要更谨慎的处理，避免将实验数据直接用于回答解释性问题。

Abstract: In 1964, Aharonov, Bergmann, and Lebowitz introduced their well-known ABL rule with the intention of providing a time-symmetric formalism for computing novel kinds of conditional probabilities in quantum theory. Later papers attached additional significance to the ABL rule, including assertions that it supported violations of the uncertainty principle. The present work challenges these claims, as well as subsequent attempts to salvage the original interpretation of the ABL rule. Taking a broader view, this paper identifies a subtle category error at the heart of the ABL rule that consists of confusing observables that belong to a single system with emergent observables that arise only for physical ensembles. Along the way, this paper points out other problems and fallacious reasoning in the research literature surrounding the ABL rule, including the misuse of post-selection, a reliance on pattern matching to classical formulas, and a posture of measurementism that takes experimental data as providing answers to interpretational questions.

</details>


### [17] [Quantum Many-Body Principles of Localized-State Ensemble Luminescence](https://arxiv.org/abs/2602.07406)
*Xinye Fan,Shijie Xu*

Main category: quant-ph

TL;DR: 提出了一种考虑电子-声子和电子-电子相互作用的量子多体局域态系综发光理论，解释了异常热行为并推导了经验公式。


<details>
  <summary>Details</summary>
Motivation: 固体中由缺陷和杂质引起的局域电子态的光学性质（尤其是发光性质）具有重要科学和技术意义，但缺乏微观理论来解释局域态系综发光现象。

Method: 发展了考虑电子-声子和电子-电子相互作用的量子多体局域态系综发光理论（MB-LSE理论）。

Result: 该理论能够定量解释峰值位置红移后蓝移、线宽先变窄后变宽、强度下降和寿命变化等异常热行为，阐明了电子-声子和电子-电子相互作用在变温发光中的作用。

Conclusion: MB-LSE理论填补了局域态系综发光微观理论的空白，并进一步推导和讨论了Varshni经验公式和黄-里斯因子，为理解局域态发光提供了理论框架。

Abstract: Localized electron states induced by various disorders,including defects and impurities,usually exist in solids.Their optical properties,especially their luminescence properties,are of both scientific and technological significance.But a microscopic theory has not yet been established for such localized-state ensemble (LSE) luminescence.In this Letter,we attempt to fill this void via developing a quantum many-body (MB) luminescence theory taking into account both electron-phonon (e-p) and electron-electron (e-e) interactions.By using the developed MB-LSE theory,abnormal thermal behaviors such as redshift and subsequent blueshift of peak position,narrowing and succeeding broadening of linewidth,decline in intensity,and variation in lifetime can be quantitatively interpreted.The roles of electron-phonon and electron-electron interactions in the variable-temperature LSE luminescence are thus elucidated. Within the framework of the MB-LSE theory, moreover, Varshni's empirical formula for bandgap temperature dependence and Huang-Rhys factor for e-p coupling are further derived and discussed.

</details>


### [18] [Non-Markovianity in a dressed qubit with local dephasing](https://arxiv.org/abs/2602.07438)
*Saima Bashir,Muzaffar Qadir Lone,Prince A Ganai*

Main category: quant-ph

TL;DR: 研究强耦合声子浴中二格点费米子实现的缀饰量子比特退相干动力学，发现非马尔可夫性导致相干性非单调衰减和复苏，浴谱类型和耦合强度显著影响非马尔可夫行为。


<details>
  <summary>Details</summary>
Motivation: 研究强耦合声子浴中缀饰量子比特的退相干动力学，探索不同浴谱密度下非马尔可夫效应对量子比特相干性的影响。

Method: 采用Lang-Firsov变换将问题转化为可微扰处理的形式，在极化子框架下应用时间卷积主方程，在系统单重态-三重态基中研究退相干动力学。

Result: 强耦合下相干性保持时间更长，表现出非单调行为反映非马尔可夫性；亚欧姆浴在较小耦合下即显示显著记忆效应，而欧姆和超欧姆浴组合需较高耦合才显现明显非马尔可夫性。

Conclusion: 缀饰量子比特的退相干动力学强烈依赖于浴谱类型和耦合强度，非马尔可夫性表现为相干性复苏和非单调衰减模式，为量子信息处理中环境工程提供重要见解。

Abstract: We study the dynamics of a dressed qubit implemented by a spinless fermion hopping between two lattice sites with each site strongly coupled to a bath of phonons. We employ Lang-Firsov transformation to make the problem tractable perturbatively. Applying time-convolutionless master equation within the polaron frame, we investigate decoherence dynamics of the dressed qubit within the singlet-triplet basis of the system for a wide range of bath spectral densities. It is shown that the coherence persists for longer time scales for large coupling values and shows non-monotonic behaviour reflecting the presence of non-Markovianity in the dynamics. Non-Markovianity, characterized by coherence revivals and non-monotonic decay patterns, emerges distinctly depending on the bath spectrum and coupling strengths. Systems coupled to sub-Ohmic baths, whether both or in combination with another type, display pronounced memory effects at relatively small values of couplings. In contrast, combinations involving Ohmic and super-Ohmic baths exhibit noticeable non-Markovianity only at higher couplings.

</details>


### [19] [Sensing weak anharmonicities with a passive-active anti-PT symmetric system](https://arxiv.org/abs/2602.07460)
*Ya-Wei Zeng,Wei-Xin Chen,Tian-Le Yang,Wan-Jun Su,Huaizhi Wu*

Main category: quant-ph

TL;DR: 提出基于三模反宇称时间对称腔-磁振子-波导系统的弱非谐性增强传感方案，通过调节光学增益实现线宽抑制点灵活控制，可检测腔模和磁振子模的弱非线性，灵敏度可通过失谐激光驱动大幅提升。


<details>
  <summary>Details</summary>
Motivation: 传统方法在检测弱非谐性时面临灵敏度不足的问题，特别是在存在强固有衰减的系统中。需要开发能够灵活控制线宽抑制点并提高检测灵敏度的新方案。

Method: 采用三模反宇称时间对称腔-磁振子-波导系统，通过调节主动腔模的光学增益来控制反PT对称哈密顿量的线宽抑制点，利用失谐激光驱动进一步提高灵敏度。

Result: 系统能够灵活控制线宽抑制点，即使磁振子模存在强固有衰减；可检测腔模和磁振子模的弱非线性，两者表现出相似的高灵敏度；失谐激光驱动可大幅提升灵敏度。

Conclusion: 基于集成被动-主动三模反PT对称系统的传感方案具有高灵敏度和灵活性，可推广到各种具有非谐性的物理系统中，为弱非线性检测提供了有效工具。

Abstract: We propose a scheme for enhanced sensing of weak anharmonicities based on a three-mode anti-parity-time (anti-PT) symmetric cavity-magnon-waveguide system. By tuning the optical gain to the active cavity mode, the linewidth suppression point for the anti-PT symmetric Hamiltonian can be flexibly controlled even when the two dissipative magnonic modes experience strong intrinsic decay. This essential characteristic is utilized for detecting weak nonlinearities in both the cavity and magnonic modes, with both demonstrating similar high levels of sensitivity. Moreover, the sensitivity can be greatly improved with a detuned laser drive. Based on the integrated passive-active three-mode anti-PT symmetric system, the sensing scheme can be generalized to various physical systems with anharmonicities.

</details>


### [20] [Recursive QAOA for Interference-Aware Resource Allocation in Wireless Networks](https://arxiv.org/abs/2602.07483)
*Kuan-Cheng Chen,Hiromichi Matsuyama,Wei-hao Huang,Yu Yamashiro*

Main category: quant-ph

TL;DR: RQAOA（递归量子近似优化算法）用于无线网络中的信道分配问题，通过递归消除变量来减小问题规模，在模拟实验中能获得可行解并达到全局最优。


<details>
  <summary>Details</summary>
Motivation: 密集无线网络中的离散无线资源管理问题自然可以表示为QUBO（二次无约束二进制优化）程序，但在大规模情况下难以求解。需要探索量子-经典混合方法来解决这些问题。

Method: 采用基于RQAOA（递归量子近似优化算法）的量子-经典混合方法，将浅层QAOA层与基于测量的单比特和双比特相关器指导的变量消除相结合。对于干扰感知的信道分配问题，给出了紧凑的QUBO/Ising公式，其中成对干扰诱导同信道耦合，通过二次惩罚（或可选的约束保持混合器）强制执行one-hot约束。

Result: 在模拟实验中，包括一个四用户四信道的示例，该方法始终返回可行的信道分配，并且在演示案例中达到了全局最优解。递归方法能够缓解影响普通QAOA的参数增长和可行性问题。

Conclusion: 结果表明，递归可以缓解影响普通QAOA的参数增长和可行性问题，为无线资源分配中的近期量子启发式算法提供了一条可行的途径。

Abstract: Discrete radio resource management problems in dense wireless networks are naturally cast as quadratic unconstrained binary optimization (QUBO) programs but are difficult to solve at scale. We investigate a quantum-classical approach based on the Recursive Quantum Approximate Optimization Algorithm (RQAOA), which interleaves shallow QAOA layers with variable elimination guided by measured single- and two-qubit correlators. For interference-aware channel assignment, we give a compact QUBO/Ising formulation in which pairwise interference induces same-channel couplings and one-hot constraints are enforced via quadratic penalties (or, optionally, constraint-preserving mixers). Within RQAOA, fixing high-confidence variables or relations reduces the problem dimension, stabilizes training, and concentrates measurement effort on a shrinking instance that is solved exactly once below a cutoff. On simulated instances of modest size, including a four-user, four-channel example, the method consistently returns feasible assignments and, for the demonstrated case, attains the global optimum. These results indicate that recursion can mitigate parameter growth and feasibility issues that affect plain QAOA, and suggest a viable pathway for near-term quantum heuristics in wireless resource allocation.

</details>


### [21] [Systematic Characterization of Transmon Qubit Stability with Thermal Cycling](https://arxiv.org/abs/2602.07522)
*Cong Li,Zhaohua Yang,Xinfang Zhang,Zhihao Wu,Shichuan Xue,Mingtang Deng*

Main category: quant-ph

TL;DR: 超导量子比特参数在热循环中表现出分层稳定性：本征器件参数（频率、T1）高度稳定，而环境变量（磁通偏移、TLS缺陷）在每次热循环后都会随机重构。


<details>
  <summary>Details</summary>
Motivation: 研究超导量子处理器中量子比特参数的长期稳定性和可重复性，这对于量子计算机的长期运行和维护至关重要。特别关注热循环对参数稳定性的影响。

Method: 对27个频率可调transmon量子比特进行超过一年的纵向表征，跨越四次热循环。使用频率相关弛豫光谱和T1光谱地形保真度定量指标来分析热循环对缺陷环境的影响。

Result: 发现超导硬件参数存在明显的稳定性层次：决定量子比特频率和T1基线的本征器件参数高度稳定（频率偏差<0.5%，相干性基线未退化）；而环境变量（背景磁通偏移和TLS缺陷微观景观）在每次热循环后都会发生显著随机重构。热循环相当于对局部缺陷环境进行"硬重置"，引入的光谱随机化程度相当于数千小时的连续低温演化。

Conclusion: 虽然制造质量得以保持，但每次热循环都会产生统计上不同的噪声实现，这要求大规模量子系统需要采用自动重新校准策略。热循环会重置缺陷环境，但不会损害本征器件参数。

Abstract: The temporal stability and reproducibility of qubit parameters are critical for the long-term operation and maintenance of superconducting quantum processors. In this work, we present a comprehensive longitudinal characterization of 27 frequency-tunable transmon qubits spanning over one year across four thermal cycles. Our results establish a distinct hierarchy of stability for superconducting hardware. We find that the intrinsic device parameters determining the qubit frequency and the baseline energy relaxation times ($T_1$) exhibit high robustness against thermal stress, characterized by frequency deviations typically confined within 0.5\% and non-degraded coherence baselines. In stark contrast, the environmental variables, specifically the background magnetic flux offsets and the microscopic landscape of two-level system (TLS) defects, undergo a significant stochastic reconfiguration after each cycle. By employing frequency-dependent relaxation spectroscopy and a quantitative metric, the $T_1$ Spectral Topography Fidelity, we demonstrate that thermal cycling acts as a ``hard reset'' for the local defect environment. This process introduces a level of spectral randomization equivalent to thousands of hours of continuous low-temperature evolution. These findings confirm that while the fabrication quality is preserved, the specific noise realization is statistically distinct for each thermal cycle, necessitating automated recalibration strategies for large-scale quantum systems.

</details>


### [22] [Squeezing-enhanced dual-channel interference for ground-state cooling of a levitated micromagnet with low quality factor](https://arxiv.org/abs/2602.07531)
*Lei Chen,Zhe-qi Yang,Liang Bin,Zhi-Rong Zhong*

Main category: quant-ph

TL;DR: 提出基于压缩增强量子干涉的双通道冷却方案，将宏观振子基态冷却所需机械品质因数降低三个数量级，冷却速率提升近180倍


<details>
  <summary>Details</summary>
Motivation: 宏观振子质心运动冷却至量子基态是测试宏观尺度量子力学的基础前提，但当前受限于对超高机械品质因数的严格要求

Method: 在混合悬浮腔-磁机械系统中，通过压缩效应与磁子-质心和腔-质心通道间量子干涉的协同作用，抑制斯托克斯散射同时增强反斯托克斯散射

Result: 将基态冷却所需临界机械品质因数降低三个数量级至实验可实现的10^4量级，净冷却速率提升近180倍，稳态质心占据数和冷却时间均降低两个数量级

Conclusion: 该方案为通过主动控制冷却动力学制备宏观量子态提供了可行路径，放宽了对材料本征性质的限制要求

Abstract: Cooling the center-of-mass (CM) motion of a macroscopic oscillator to its quantum ground state is a fundamental prerequisite for testing quantum mechanics at macroscopic scales. However, achieving this goal is currently hindered by the stringent requirement for an ultrahigh mechanical quality factor ($Q_c$). Here, we propose a dual-channel cooling scheme based on squeezing-enhanced quantum interference within a hybrid levitated cavity-magnomechanical system to overcome this limitation.
  By synergizing squeezing effects with quantum interference between the magnon-CM and cavity-CM channels, our scheme simultaneously suppresses Stokes (heating) scattering while enhancing anti-Stokes (cooling) scattering.~We demonstrate that this cooling mechanism reduces the critical $Q_c$ required for ground-state cooling by three orders of magnitude, making it achievable in the experimentally accessible regime of $Q_c \sim 10^4$. Furthermore, the net cooling rate is enhanced by nearly 180-fold compared to that of conventional single-channel cooling. This improvement is accompanied by a two orders of magnitude reduction in both the steady-state CM occupancy and the cooling time. Importantly, this enhanced performance remains robust even deep within the unresolved-sideband regime. Our results provide a feasible path toward preparing macroscopic quantum states by actively controlling the cooling dynamics, thereby relaxing the constraints on intrinsic material properties.

</details>


### [23] [Characterization of Autofluorescence in Optical Fibers for NV-based Sensing Applications](https://arxiv.org/abs/2602.07536)
*Stefan Johansson,Alexander Bukschat,Dennis Lönard,Alena Erlenbach,Jonas Gutsche,Artur Widera*

Main category: quant-ph

TL;DR: 研究标准光纤的光谱特性，识别背景信号最小的光纤类型，为基于NV中心的量子传感选择最优光纤


<details>
  <summary>Details</summary>
Motivation: 光纤背景荧光和散射会与NV中心荧光光谱重叠，降低信噪比，限制量子传感器的灵敏度

Method: 研究标准光纤的光学光谱，考虑材料依赖性、物理影响因素，以及荧光随激发功率和波长的变化规律

Result: 识别出光谱成分和背景信号最小的光纤类型

Conclusion: 为基于NV中心的量子传感应用提供了选择最优光纤的指导

Abstract: Optical fibers are crucial for guiding light in various sensing applications. Especially for quantum sensors such as the nitrogen-vacancy (NV) center in diamond, they enable light control and device miniaturization. However, fluorescence and scattering within the fiber, often referred to as fiber background, autofluorescence, or autoluminescence, can overlap spectrally with the NV centers' fluorescence, degrading the signal-to-noise ratio and thus limiting sensor sensitivity. Here, we investigate the optical spectra of standard optical fibers, considering material dependencies, physical influences, and their fluorescence scaling with excitation power and wavelength. Our results identify spectral components and fiber types with minimal unwanted background signals, guiding the selection of optimal fibers for NV-based quantum sensing.

</details>


### [24] [BiBiEQ: Bivariate Bicycle Codes on Erasure Qubits](https://arxiv.org/abs/2602.07578)
*Ameya S. Bhave,Navnil Choudhury,Andrew Nemec,Kanad Basu*

Main category: quant-ph

TL;DR: BiBiEQ框架将双变量自行车码编译为擦除感知量子电路，通过擦除检查调度提升量子纠错性能，在距离10时实现显著逻辑错误率降低。


<details>
  <summary>Details</summary>
Motivation: 擦除量子比特通过将主要故障转换为可检测的擦除错误来降低容错量子纠错的开销，已在表面码和Floquet码中展示出优势。本研究旨在将擦除量子比特应用于具有稀疏结构和良好率-距离权衡的QLDPC双变量自行车码。

Method: 提出BiBiEQ框架，将给定的BB码编译为擦除感知内存电路C_E，包含擦除检查、重置和擦除操作。提供BiBiEQ-Exact（保留联合擦除相关性）和BiBiEQ-Approx（使用独立性近似加速）两种转换引擎，将C_E转换为通用解码的稳定子电路。

Result: 4EC调度使两个引擎的准确性保持接近，BiBiEQ-Approx可作为BiBiEQ-Exact的可靠代理进行快速扫描。在伪阈值以下，代码距离从6增加到10时逻辑错误率下降10-17倍，而从10增加到12时下降幅度较小，表明大部分增益在距离10时已实现。

Conclusion: BiBiEQ框架成功将擦除量子比特应用于BB码，通过优化的擦除检查调度显著提升量子纠错性能，距离10是获得大部分性能增益的关键点，为QLDPC码的实际应用提供了有效工具。

Abstract: Erasure qubits reduce overhead in fault-tolerant quantum error correction (QEC) by converting dominant faults into detectable errors known as erasures. They have demonstrated notable improvements in thresholds and scaling in surface and Floquet code memories. In this work, we use erasure qubits on Bivariate Bicycle (BB) codes from the quantum low-density parity-check (QLDPC) regime. Owing to their sparse structure and favorable rate-distance trade-offs, BB codes are practical candidates for QEC. We introduce BiBiEQ, a novel framework that compiles a given BB code into an erasure-aware memory circuit C_E. This erasure circuit C_E comprises erasure checks (ECs), resets, and erasures spread over a user-specified erasure check schedule (2EC, 4EC). BiBiEQ converts this erasure circuit C_E into the stabilizer circuit C for general-purpose decoding. BiBiEQ provides two engines for this conversion, BiBiEQ-Exact and BiBiEQ-Approx. BiBiEQ-Exact preserves the joint-erasure correlations and serves as our accuracy benchmark, while BiBiEQ-Approx uses an independence approximation to accelerate large sweeps and expose accuracy-throughput trade-offs. Using BiBiEQ, we decode the stabilizer circuits to get a per-round logical error rate (LER) for the BB codes and quantify the effect of the EC schedules on the correctable operating region below the pseudo-threshold. The 4EC schedule keeps the accuracy of both engines close to one another, making BiBiEQ-Approx a reliable proxy for BiBiEQ-Exact for faster sweeps. Below the pseudo-threshold, the code distance (d) hop from distance (d) 6 to 10 yields a drop in LER by 10-17x larger than distance (d) 10 to 12, showing that most gains are realized by d=10.

</details>


### [25] [Hidden Kinematics and Dual Quantum References in Magnetic Resonance](https://arxiv.org/abs/2602.07636)
*Sunghyun Kim*

Main category: quant-ph

TL;DR: 自旋共振现象通常用旋转坐标系中的跃迁概率描述，其物理意义隐含依赖于量子参考标准的选择。本文展示了旋转磁场中的自旋构成涉及两种量子描述的配置，它们共享共同的量子化算符但在运动学和动力学角色上不同。


<details>
  <summary>Details</summary>
Motivation: 传统自旋共振理论使用旋转坐标系中的跃迁概率描述，这种描述隐含依赖于量子参考标准的选择，导致物理意义不明确。需要建立更清晰的框架来理解自旋在旋转磁场中的动力学本质。

Method: 提出一个包含两种量子描述的新框架：它们共享相同的量子化算符，但在运动学（自旋矢量运动）和动力学（演化）角色上不同。将自旋矢量的运动学运动与动力学演化相结合，恢复一致的能量计算。

Result: 跃迁概率被揭示为量子参考标准之间的相对量，而不是单个演化自旋态的内在属性。该框架恢复了旋转磁场中自旋动力学的一致能量计算，揭示了其双参考结构。

Conclusion: 旋转磁场中的自旋动力学本质上具有双参考结构，跃迁概率是量子参考标准之间的相对量。这一框架为理解自旋共振现象提供了更清晰的基础，强调了量子参考标准在定义物理量中的关键作用。

Abstract: Spin resonance phenomena are conventionally described using transition probabilities formulated in a rotating frame, whose physical meaning implicitly depends on the choice of quantum reference standard. In this Colloquium, we show that a spin in a rotating magnetic field constitutes a configuration involving two quantum descriptions that share a common quantization operator but differ in their kinematic and dynamical roles. The transition probability therefore emerges as a relational quantity between quantum reference standards rather than an intrinsic property of a single evolving spin state. By incorporating the kinematic motion of the spin vector together with the dynamical evolution, this framework restores consistent energy accounting and reveals the dual-reference structure underlying spin dynamics in rotating magnetic fields.

</details>


### [26] [Two-phase driving of a linear radio-frequency ion trap](https://arxiv.org/abs/2602.07700)
*Santhosh Surendra,Akos Hoffmann,Michael Köhl*

Main category: quant-ph

TL;DR: 提出一种在180度反相下驱动线性射频保罗阱的技术，以解决传统驱动方法导致的轴向微运动问题


<details>
  <summary>Details</summary>
Motivation: 传统线性射频保罗阱驱动方法（一对电极接地，另一对接高压射频源）虽然简化了阻抗匹配，但在某些架构中会导致轴向微运动幅度增加，特别是当射频电极与端盖电极之间的电容不可忽略时

Method: 开发了一种生成两个180度反相高压射频信号的技术，用于驱动线性保罗阱，使相邻电极之间具有相反的电压

Result: 成功在采用新驱动技术的线性射频保罗阱中囚禁并冷却了镱离子链

Conclusion: 提出的180度反相驱动技术能够有效减少传统驱动方法导致的轴向微运动问题，为离子阱实验提供了改进方案

Abstract: A linear radio-frequency Paul trap is traditionally driven with one diagonal pair of electrodes grounded and the other connected to a high-voltage radio-frequency source. This method simplifies impedance matching of the voltage source to the trap. However, for several architectures it leads to increasing the axial micromotion amplitude, for example, when the capacitance between radio-frequency and end-cap electrodes is not negligible. Here, we present a technique to generate two high-voltage radio-frequency signals \SI{180}{\degree} out of phase to drive a linear Paul trap with opposite voltages between neighbouring electrodes. Using this, we have successfully trapped and cooled a chain of Ytterbium ions in a linear radio-frequency Paul trap.

</details>


### [27] [Quantum Steering and Entanglement in a Tritter: Hierarchy under Loss](https://arxiv.org/abs/2602.07788)
*Jifeng Sun,Shumin Yang,Teng Zhao,Qingqian Kang,Liyun Hu*

Main category: quant-ph

TL;DR: 该论文研究了三模连续变量纠缠态中的关联层次，分析了通过线性光学元件（三分束器）混合双模压缩真空态与相干态生成的三模态，发现纠缠和EPR导引强度仅由压缩参数决定，且导引比纠缠具有更严格的阈值条件。


<details>
  <summary>Details</summary>
Motivation: 研究多模连续变量纠缠态中的关联层次，特别是EPR导引与纠缠之间的关系，为开发非对称量子协议（如单边设备无关任务）提供理论基础。

Method: 使用协方差矩阵形式体系，分析通过三分束器混合双模压缩真空态与相干态生成的三模态，研究不同信道配置下的光学损耗影响，并采用参数扩展技术验证结果。

Result: 纠缠和EPR导引强度仅由压缩参数决定，与相干振幅无关；损耗会降低关联强度，但EPR导引保持单配性且比纠缠具有更强的抗损耗能力；导引条件比不可分性判据更严格，证实导引是纠缠的严格子集。

Conclusion: 该研究阐明了易于生成的多模态中的关联结构，为开发非对称量子协议提供了实用见解，其中EPR导引作为单边设备无关任务等应用的关键资源。

Abstract: Multipartite entangled states of continuous variables are fundamental resources for scalable quantum information processing. We study the correlation hierarchy in a tripartite state engineered by mixing a two-mode squeezed vacuum with a coherent state on a tritter, a key linear optical element for multimode state generation. Using the covariance matrix formalism, we comprehensively analyze the entanglement and Einstein-Podolsky-Rosen (EPR) steering among the output modes. The strength of both correlations is governed solely by the squeezing parameter and is independent of the coherent amplitude. We further examine the impact of inevitable optical losses in various channel configurations. The results show that while losses degrade correlations, EPR steering remains monogamous and exhibits stricter resilience thresholds than entanglement. Our analysis, supported by parameter extension techniques, confirms that the steering condition is more stringent than the inseparability criterion, clearly demonstrating that steering forms a strict subset of entanglement. These results elucidate the correlation structure in a readily generated multimode state and offer practical insights for developing asymmetric quantum protocols, such as one-sided device-independent tasks, where EPR steering serves as a critical resource.

</details>


### [28] [Geometric criticality in the driven Jaynes-Cummings model](https://arxiv.org/abs/2602.07795)
*Ken Chen,Jia-Hao Lv,Hao-Long Zhang,Fan Wu,Wen Ning,Zhen-Biao Yang,Shi-Biao Zheng*

Main category: quant-ph

TL;DR: 研究驱动Jaynes-Cummings模型中本征态的几何临界性，发现量子度量和Berry曲率张量在临界区域发散，且亮态的发散比暗态更显著


<details>
  <summary>Details</summary>
Motivation: 虽然光子阻塞破坏相变已被详细研究，但本征态的临界性质仍未被充分探索。本文旨在研究这些本征态相关的几何临界性

Method: 使用驱动Jaynes-Cummings模型，以驱动场的振幅和相位作为哈密顿量的控制参数，分析每个本征态的量子度量和Berry曲率张量

Result: 发现量子度量和Berry曲率张量在临界区域呈现发散行为，且亮态的发散比唯一的暗态更加显著

Conclusion: 本征态的几何临界性在驱动Jaynes-Cummings模型中表现出显著特征，这些理论结果可在电路量子电动力学系统中进行实验验证

Abstract: When the photonic mode in the Jaynes-Cummings model is driven by an external classical field, the system can undergo the photon-blockade breakdown phase transition at a critical point. Such a phase transition has been detailedly investigated, but the critical properties of the eigenstates remain largely unexplored so far. We here study the geometric criticality associated with these eigenstates. The amplitude and phase of the drive serve as the control parameter of the governing Hamiltonian. We find the quantum metric and Berry curvature tensors for each eigenstate display divergent behaviors in the critical region. More importantly, the divergence associated with bright eigenstates is much more pronounced than that for the unique dark state. Our theoretical results can be experimentally confirmed in circuit quantum electrodynamics systems, where the driven Jaynes-Cummings model has been realized.

</details>


### [29] [Semi-device-independent certification of high-dimensional quantum channels](https://arxiv.org/abs/2602.07823)
*Mengyan Li,Yanning Jia,Fenzhuo Guo,Haifeng Dong,Sujuan Qin,Fei Gao*

Main category: quant-ph

TL;DR: 提出半设备无关框架，基于观测统计量认证量子信道特性，仅假设系统维度已知，利用Choi态结构约束和Choi-Jamiołkowski同构进行严格认证。


<details>
  <summary>Details</summary>
Motivation: 现有量子信道认证方案通常需要完全可信的内部设备，这在现实场景中难以实现。需要一种更实用的认证框架，能够在设备信任度较低的情况下验证量子信道性能。

Method: 提出半设备无关框架，仅假设系统维度已知，利用Choi态的结构约束和Choi-Jamiołkowski同构。通过引入见证算子并数值确定其Schmidt数相关边界来认证信道纠缠维度。同时使用基于局部化矩阵的半定规划松弛层次结构来认证信道纠缠保真度。

Result: 该方法成功复现了已知分析基准，并应用于去相位和去极化噪声信道验证了有效性。获得了与观测统计量或单个见证值兼容的纠缠保真度下界。

Conclusion: 提出的半设备无关框架能够在仅知系统维度的较弱假设下，有效认证量子信道的纠缠维度和纠缠保真度，为实际量子通信协议中的信道验证提供了更实用的解决方案。

Abstract: Certifying high-dimensional quantum channels is essential for ensuring the reliability of quantum communication protocols. Existing certification schemes often rely on fully trusted internal devices, which is difficult to achieve in realistic scenarios. Here, we propose a semi-device-independent framework for certifying channel properties directly from observed statistics, assuming only that the system dimension is known. By explicitly incorporating the full set of structural constraints inherent to Choi states, our approach exploits the Choi-Jamiołkowski isomorphism for rigorous certification of quantum channels. The entanglement dimensionality of quantum channels is first certified by introducing a witness and numerically determining its Schmidt-number-dependent bounds. This certification method reproduces known analytical benchmarks and is applied to dephasing and depolarizing noise channels, thereby confirming its validity. To provide a more complete assessment of channel performance, the entanglement fidelity of quantum channels is also certified using a hierarchy of semidefinite programming relaxations based on localizing matrices. Lower bounds on the entanglement fidelity are obtained that are compatible with either the full set of observed statistics or a single witness value.

</details>


### [30] [Geometry-Enabled Radiation from Structured Paraxial Electrons](https://arxiv.org/abs/2602.07858)
*M. S. Epov,I. E. Shenderovich,S. S. Baturin*

Main category: quant-ph

TL;DR: 提出了一种微观计算方法，用于分析扭曲（傍轴）电子在非均匀轴对称磁场中传播时的自发光子发射现象，揭示了波前曲率作为有效几何场驱动辐射的新机制。


<details>
  <summary>Details</summary>
Motivation: 研究结构化电子态（如扭曲电子）在非均匀磁场中的辐射行为，探索几何演化与光子发射之间的直接联系，特别是当外部磁场局部为零时仍能产生辐射的机制。

Method: 结合Foldy-Wouthuysen变换与基于Lewis-Ermakov不变量和元辛变换的几何框架，构建了包含横向模式结构和波前曲率的精确电子态，分析其在二次型空间中的演化路径。

Result: 发现电子波前曲率半径的倒数可作为有效几何场，即使在外部磁场局部为零的区域也能产生辐射；证明了非循环几何演化对发射振幅的几何贡献无法通过规范选择或绝热论证消除。

Conclusion: 该工作将朗道能级辐射推广到非渐近的结构化电子态，建立了非循环几何演化与光子发射之间的直接联系，为结构化电子束与电磁场相互作用提供了新的理论框架。

Abstract: We present a microscopic calculation of spontaneous photon emission by twisted (paraxial) electrons propagating through inhomogeneous, axisymmetric magnetic fields. We construct exact electron states that incorporate transverse mode structure and wavefront curvature by combining the Foldy-Wouthuysen transformation with a geometric framework based on Lewis-Ermakov invariants and metaplectic transformations. We show that the evolution of such structured states corresponds to an open path in the space of quadratic forms, giving rise to a geometric contribution to the emission amplitude that cannot be eliminated by gauge choice or adiabatic arguments. The inverse radius of curvature of the electron wavefront emerges as an effective geometric field that enables radiation even in regions where the external magnetic field vanishes locally. This mechanism generalizes Landau-level radiation to nonasymptotic, structured electron states and establishes a direct connection between noncyclic geometric evolution and photon emission.

</details>


### [31] [Quantum Evolution of Hopf Algebra Hamiltonians](https://arxiv.org/abs/2602.07887)
*Michele Arzano,Antonio Del Prete,Domenico Frattulillo*

Main category: quant-ph

TL;DR: 该论文分析了非对易时空模型中变形对称性是否会导致基本退相干效应，发现文献中考虑的变形时空对称性无法建立物理可行的Lindblad演化。


<details>
  <summary>Details</summary>
Motivation: 近年来，人们越来越关注具有变形对称性的理论（与非对易时空模型相关）可能编码基本形式的退相干效应。这种效应应该由具有非平凡Hopf代数结构的时间演化生成元所支配的Lindblad-like演化来描述。

Method: 通过对量子比特哈密顿量的类似Hopf代数变形进行详细分析，从批判性地检查通过广义伴随作用定义时间演化开始，探索是否能够建立一个连贯且物理可行的框架。

Result: 分析表明，更一般的伴随作用组合总是保证von Neumann动力学，并且在文献中考虑的变形时空对称性情况下，无法建立物理可行的Lindblad演化。

Conclusion: 对于文献中考虑的变形时空对称性，无法建立物理可行的Lindblad演化框架，这表明此类模型可能无法编码基本形式的退相干效应。

Abstract: In recent years, growing attention has been devoted to the possibility that theories with deformed symmetries, associated with certain models of non-commutative spacetime, may encode a fundamental form of decoherence. This effect should be described by a Lindblad-like evolution governed by the non-trivial Hopf algebra structure of the time-evolution generators. In this work we provide a detailed analysis of such possibility for similar Hopf algebra deformations of the Hamiltonian of a qubit. Starting from a critical examination of the very definition of time evolution through the generalized adjoint action, we explore whether a coherent and physically viable framework can be established. In particular, our analysis shows that a more general combination of adjoint actions always guarantees a von Neumann dynamics and, also in the case of deformed spacetime symmetries considered in the literature, a physically viable Lindblad evolution cannot be established.

</details>


### [32] [Minimal nonintegrable models with three-site interactions](https://arxiv.org/abs/2602.07867)
*Wen-Ming Fan,Kun Hao,Xiao-Hui Wang,Kun Zhang,Vladimir Korepin*

Main category: quant-ph

TL;DR: 该论文系统研究了具有真正三体相互作用的平移不变自旋链中的可积性破坏，通过构造和分类最小不可积自旋-1/2哈密顿量，划定了超越最近邻范式的可积与不可积之间的清晰边界。


<details>
  <summary>Details</summary>
Motivation: 目前对具有真正三体相互作用的平移不变自旋链中可积性破坏的系统理解仍然缺乏。作者旨在填补这一空白，通过引入和分类最小不可积自旋-1/2哈密顿量，这些模型在满足注入性的同时，除了哈密顿量外不允许任何非平凡的局域守恒荷。

Method: 首先通过将周期性边界条件下的变形Fredkin自旋链映射到最近邻复合自旋表示，并排除所有可容许的3-局域守恒荷，严格建立了其不可积性。然后基于该结构，构造了五类具有真正三体相互作用的自旋-1/2模型。

Result: 发现其中一类是可积的，而其余四类恰好包含两个相互作用项，构成了最小不可积的三体相互作用模型。这些结果清晰地划定了超越最近邻范式的可积性与不可积性之间的边界。

Conclusion: 该工作通过系统分类最小不可积自旋-1/2哈密顿量，为理解具有真正三体相互作用的平移不变自旋链中的可积性破坏提供了理论基础，并明确划定了可积与不可积模型之间的边界。

Abstract: A systematic understanding of integrability breaking in translationally invariant spin chains with genuine three-site interactions remains lacking. In this work, we introduce and classify minimal nonintegrable spin-$1/2$ Hamiltonians, defined as models that saturate injectivity while admitting no nontrivial local conserved charges beyond the Hamiltonian. We first rigorously establish the nonintegrability of the deformed Fredkin spin chain with periodic boundary conditions by mapping it to a nearest-neighbor composite-spin representation and excluding all admissible $3$-local conserved charges. Guided by its structure, we then construct five classes of spin-$1/2$ models with genuine three-site interactions. One class is integrable, while the remaining four contain exactly two interaction terms and constitute the minimal nonintegrable three-site models. Our results delineate a sharp boundary between integrability and nonintegrability beyond the nearest-neighbor paradigm.

</details>


### [33] [Doubling the size of quantum selected configuration interaction based on seniority-zero space and its application to QC-QSCI-AFQMC](https://arxiv.org/abs/2602.07912)
*Yuichiro Yoshida,Takuma Murokoshi,Naoya Kuroda,Wataru Mizukami*

Main category: quant-ph

TL;DR: DOCI-QSCI方法通过采样seniority-zero空间，结合AFQMC后处理，在量子计算中实现更大轨道空间的化学模拟，相比传统方法精度显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统量子选择组态相互作用(QSCI)受限于轨道空间大小，而seniority-zero空间虽然能扩展轨道数量但会损失定量精度，需要补偿机制来恢复动态关联。

Method: 提出DOCI-QSCI方法：1) 采样seniority-zero空间，将轨道空间扩展一倍；2) 通过笛卡尔积将采样比特串扩展到包含seniority-breaking行列式的更大空间；3) 将得到的波函数作为phaseless AFQMC的试验态，恢复全轨道空间的动态关联。

Result: 在H6链中，DOCI-QSCI-AFQMC达到与完整活性空间相当的精度；在N2解离和BODIPY-O2体系中，对于(14e,28o)和(20e,20o)活性空间，获得合理结果，而单参考CCSD(T)完全失败。

Conclusion: DOCI-QSCI将传统QSCI可访问的轨道空间扩展一倍，结合AFQMC后处理能提供相当高的精度，为量子计算处理更大化学体系开辟了新途径。

Abstract: We propose doubly occupied configuration interaction-quantum selected configuration interaction (DOCI-QSCI), which samples from the seniority-zero space. While the use of this space effectively doubles the qubit budget, equaling the number of spatial orbitals, this sector restriction can compromise quantitative accuracy. To compensate for this, we expand sampled bitstrings via their Cartesian product into a larger space that includes seniority-breaking determinants. The resulting wave function is also proposed using the trial state in phaseless auxiliary-field quantum Monte Carlo (ph-AFQMC) to recover dynamical correlations across the full orbital space (DOCI-QSCI-AFQMC). We evaluate the proposed methods on the H6 chain, N2 dissociation, and the addition of singlet O2 to a BODIPY dye. For the H6 chain, DOCI-QSCI-AFQMC reproduces the accuracy of the level of the complete-active-space counterpart with the quantum device ibm kobe. For N2 and BODIPY-O2, with (14e, 28o) and up to (20e, 20o) active spaces, it yields reasonable results, whereas single-reference CCSD(T) fails qualitatively. These results demonstrate that the DOCI-QSCI doubles the orbital space accessible to conventional QSCI and subsequent ph-AFQMC post-processing delivers reasonably high accuracy.

</details>


### [34] [Real-Time Magnetic Field Sensing based on Microwave Frequency Modulated Photocurrent of Nitrogen-Vacancy Centers in Diamond](https://arxiv.org/abs/2602.07926)
*Xuan-Ming Shen,Qilong Wu,Huihui Yu,Pei-Nan Ni,Qing Lou,Chao-Nan Lin,Xun Yang,Chong-Xin Shan,Yuan Zhang*

Main category: quant-ph

TL;DR: 首次实现基于光电探测磁共振(PDMR)的实时磁场传感，在DC-10Hz范围内获得921 nT/√Hz的灵敏度，并成功追踪交变磁场。


<details>
  <summary>Details</summary>
Motivation: 光电探测磁共振(PDMR)可用于小型化NV中心量子传感器，但实际实现PDMR磁场传感仍面临挑战，需要解决这一问题。

Method: 在金刚石表面制备电极和微波天线，通过多种锁相放大模式检测纳安级光电流实现PDMR，采用激光强度和微波频率调制模式。

Result: 获得理论灵敏度397 nT/√Hz和实验灵敏度921 nT/√Hz（DC-10Hz），首次实现交变磁场实时追踪（标准差1.5 μT），PDMR对比度、线宽和灵敏度与主方程理论模型完美吻合。

Conclusion: 成功实现基于PDMR的实用磁场传感，为小型化NV中心量子传感器发展奠定基础，理论与实验结果一致验证了方法的有效性。

Abstract: While photoelectric detection of magnetic resonance (PDMR) can be applied to miniaturize nitrogen-vacancy (NV) center-based quantum sensors, the real demonstration of PDMR-based magnetic field sensing remains as a distinctive challenge. To tackle this challenge, in this article, we fabricate diamond samples with electrodes and microwave antenna on the surface, and realize PDMR by detecting photocurrent in nanoampere range via various lock-in amplifying modes. Importantly, we obtain a theoretical and experimental sensitivity 397 nT/Hz and 921 nT/Hz of magnetic field detection in DC-10 Hz range with a laser intensity and microwave frequency modulated mode, respectively, and demonstrate for the first time, a real-time tracking of alternating magnetic field with a standard deviation of 1.5 uT. Furthermore, we investigate systematically the dependence of the PDMR contrast, linewidth and the sensitivity on the laser and microwave power, and find a perfect agreement with a master equation based theoretical model, which accounts for not only the optically induced charge switch of neutral and negative NV centers, but also the interaction with microwave field.

</details>


### [35] [Full Schmidt characterization of spatiotemporally entangled states produced from spontaneous parametric down-conversion](https://arxiv.org/abs/2602.07949)
*Rakesh Pradhan,Girish Kulkarni*

Main category: quant-ph

TL;DR: 利用旋转对称性大幅降低计算复杂度，首次实现了SPDC产生的时空纠缠态的完整施密特分解，揭示了具有轨道角动量的施密特模式及其谱分布。


<details>
  <summary>Details</summary>
Motivation: 由于计算复杂度极高，SPDC产生的时空纠缠态的完整施密特分解一直未能实现，这限制了对其量子特性的深入理解和应用开发。

Method: 利用量子态的旋转对称性，将计算复杂度降低了至少四个数量级，从而能够对包含超过10^4个模式的时空纠缠态进行完整的施密特分解。

Result: 首次揭示了SPDC时空纠缠态的精确施密特模式形式及其谱分布，发现这些模式具有横向空间涡旋结构，在所有频率上都带有轨道角动量。在高增益区域，施密特模式会变宽，而施密特谱会随着泵浦强度的增加而变窄。

Conclusion: 这项工作为量子成像和光谱学交叉领域的新型应用开辟了道路，能够更好地利用SPDC产生的纠缠态。

Abstract: The full Schmidt decomposition of spatiotemporally entangled states generated from spontaneous parametric down-conversion (SPDC) has not been carried out until now due to the immense computational complexity arising from the large dimensionalities of the states. In this Letter, we utilize the rotational symmetry of the states to reduce the complexity by at least four orders of magnitude and carry out the decomposition to reveal the precise forms of the spatiotemporal Schmidt modes and the Schmidt spectrum spanning over 10^4 modes. We show that the Schmidt modes have a phase profile with a transverse spatial vortex structure that endows them with orbital angular momentum at all frequencies. In the high-gain regime, these Schmidt modes broaden and the Schmidt spectrum narrows with increasing pump strength. Our work can spur novel applications at the intersection of quantum imaging and spectroscopy that utilize entangled states produced from SPDC.

</details>


### [36] [Higher-Order Corrections to Scrambling Dynamics in Brownian Spin SYK Models](https://arxiv.org/abs/2602.07952)
*Tingfei Li,Miao Wang,Jianghui Yu*

Main category: quant-ph

TL;DR: 研究布朗运动自旋SYK模型中算子增长，关注完整算子尺寸分布，推导主方程和生成函数方法，获得精确解并发展1/N展开，揭示高阶效应对量子混沌的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究布朗运动SYK模型中算子增长和完整算子尺寸分布，以更精细地探测量子混沌，特别是在布朗运动和开放量子系统中。

Method: 推导Pauli弦展开系数的主方程，转化为生成函数表述，对角化主导阶演化算子获得精确解，发展系统性的1/N展开捕获高阶修正。

Result: 获得任意初始算子分布的精确解，包括退相干效应，发现高阶效应对算子混洗至关重要，完整算子尺寸分布是量子混沌的更精细探针。

Conclusion: 完整算子尺寸分布提供了量子混沌的更精细探测，高阶效应对算子混洗起关键作用，方法适用于布朗运动和开放量子系统。

Abstract: We investigate operator growth in a Brownian spin Sachdev--Ye--Kitaev (SYK) model with random all-to-all interactions, focusing on the full operator-size distribution. For Hamiltonians containing interactions of order two up to $L$, we derive a closed master equation for the Pauli-string expansion coefficients and recast their dynamics into a generating-function formulation suitable for the large-$N$ limit. This approach allows us to diagonalize the leading-order evolution operator explicitly and obtain exact solutions for arbitrary initial operator distributions, including the effects of decoherence. Going beyond leading order, we develop a systematic $1/N$ expansion that captures higher-order corrections to the operator-size dynamics and the late-time behavior. Our results demonstrate that higher-order effects play a crucial role in operator scrambling and that the full operator-size distribution provides a more refined probe of quantum chaos in Brownian and open quantum systems.

</details>


### [37] [Quantum self-interaction within an infinitely deep cavity](https://arxiv.org/abs/2602.07956)
*Sergio Giardino*

Main category: quant-ph

TL;DR: 在实希尔伯特空间中研究无限深量子腔（量子无限方势阱），考虑复波函数和四元数波函数解，发现比传统复希尔伯特空间解更一般，包含非定态解、畸变定态解、不同能谱、位置位移，四元数解还允许自相互作用。


<details>
  <summary>Details</summary>
Motivation: 在实希尔伯特空间框架下重新审视无限深量子腔问题，探索复波函数和四元数波函数的解，以发现比传统复希尔伯特空间更一般的量子力学解。

Method: 在实希尔伯特空间中研究无限深量子腔（量子无限方势阱），分别考虑复波函数和四元数波函数作为解，分析其性质并与传统复希尔伯特空间中的解进行比较。

Result: 复波函数解不仅重现了传统复希尔伯特空间的结果，还扩展到了非定态解、畸变定态解、不同能谱和观测位置位移。四元数波函数解进一步允许自相互作用现象，这是复解中无法观察到的。

Conclusion: 实希尔伯特空间中的复波函数和四元数波函数解比传统量子力学解更一般，为研究非相对论理论中的一维解开辟了新途径。

Abstract: One examines the infinitely deep quantum cavity, also known as the quantum infinite square well, within the framework of the real Hilbert space. The solutions are considered in terms of complex wave functions, and also in terms of quaternionic wave functions. The complex results reproduce the usual achievements established in the complex Hilbert space, but also extend them to non-stationary solutions, as well as to distorted stationary solutions, different energy spectra, and dislocated observed position. The quaternionic cases further admit the incidence of self-interaction, something that cannot be observed in complex solutions. Therefore, both the complex and quaternionic solutions are more general than previous cases, thus opening the way to further one-dimensional solutions to be researched in the non-relativistic theory.

</details>


### [38] [Improved entanglement-based high-dimensional optical quantum computation with linear optics](https://arxiv.org/abs/2602.07971)
*Huan-Chao Gao,Guo-Zhu Song,Hai-Rui Wei*

Main category: quant-ph

TL;DR: 提出了一种基于纠缠的光学受控SWAP门，在C²⊗Cᵈ⊗Cᵈ空间上实现，使用偏振和空间自由度混合编码，仅需(2+3d)个线性光学元件，电路深度为5，保真度达99.4%，且无需辅助光子或测量诱导非线性。


<details>
  <summary>Details</summary>
Motivation: 高维量子门在某些量子信息处理任务中比二维量子门具有显著优势，但现有方案需要更多光学元件和更深的电路深度，且通常局限于d=2的情况。

Method: 采用混合编码方案：控制量子比特编码在光子偏振自由度，目标量子比特编码在空间自由度。构建仅需(2+3d)个线性光学元件的电路，电路深度为5，无需借用辅助光子或测量诱导非线性。

Result: 当d=2时，仅需8个线性光学元件（先前方案需14个），电路深度5（先前为11），保真度99.4%高于先前方案。方案支持d>2的情况，且为确定性实现。

Conclusion: 提出了一种高效、高保真度的光学受控SWAP门实现方案，显著减少了所需光学元件数量和电路深度，支持高维扩展，为高维量子信息处理提供了实用工具。

Abstract: Quantum gates are the essential block for quantum computer. High-dimensional quantum gates exhibit remarkable advantages over their two-dimensional counterparts for some quantum information processing tasks. Here we present a family of entanglement-based optical controlled-SWAP gates on $\mathbb{C}^{2}\otimes \mathbb{C}^{d}\otimes \mathbb{C}^{d}$. With the hybrid encoding, we encode the control qubits and target qudits in photonic polarization and spatial degrees of freedom, respectively. The circuit is constructed using only $(2+3d)$ ($d\geq 2$) linear optics, beating an earlier result of 14 linear optics with $d=2$. The circuit depth 5 is much lower than an earlier result of 11 with $d=2$. Besides, the fidelity of the presented circuit can reach 99.4\%, and it is higher than the previous counterpart with $d=2$. Our scheme are constructed in a deterministic way without any borrowed ancillary photons or measurement-induced nonlinearities. Moreover, our approach allows $d>2$.

</details>


### [39] [An efficient method for spot-checking quantum properties with sequential trials](https://arxiv.org/abs/2602.08114)
*Yanbao Zhang,Akshay Seshadri,Emanuel Knill*

Main category: quant-ph

TL;DR: 开发了一种认证非独立同分布量子试验性能的通用方法，该方法在有限试验次数下高效工作，并能提供渐近紧致的性能认证。


<details>
  <summary>Details</summary>
Motivation: 量子资源在实际应用中可能因复杂生成过程或传输中的对抗操作而不可靠，导致实验中的试验序列呈现非独立同分布行为，这会给量子密钥分发、自测试、可验证量子计算等任务带来安全隐患和错误估计。

Method: 开发了一种通用方法，通过在每个试验中随机决定是进行抽查（spot-checking）还是使用量子资源执行任务，来认证非独立同分布抽查试验序列的性能。

Result: 该方法不仅能在有限试验次数下高效工作，还能提供渐近紧致的性能认证。分析表明，即使试验总数趋于无穷大，也只需抽查恒定数量的试验即可在指定置信水平下认证剩余试验的平均性能。

Conclusion: 该研究填补了非独立同分布抽查试验序列认证方法的空白，为量子信息任务的安全性和可靠性提供了重要保障。

Abstract: In practical situations, the reliability of quantum resources can be compromised due to complex generation processes or adversarial manipulations during transmission. Consequently, the trials generated sequentially in an experiment may exhibit non-independent and non-identically distributed (non-i.i.d.) behavior. This non-i.i.d. behavior can introduce security concerns and result in faulty estimates when performing information tasks such as quantum key distribution, self-testing, verifiable quantum computation, and resource allocation in quantum networks. To certify the performance of such tasks, one can make a random decision in each trial, either spot-checking some desired property or utilizing the quantum resource for the given task. However, a general method for certification with a sequence of non-i.i.d. spot-checking trials is still missing. Here, we develop such a method. This method not only works efficiently with a finite number of trials but also yields asymptotically tight certificates of performance. Our analysis shows that even as the total number of trials approaches infinity, only a constant number of trials needs to be spot-checked on average to certify the average performance of the remaining trials at a specified confidence level.

</details>


### [40] [Optimal Quantum Speedups for Repeatedly Nested Expectation Estimation](https://arxiv.org/abs/2602.08120)
*Yihang Sun,Guanyang Wang,Jose Blanchet*

Main category: quant-ph

TL;DR: 量子算法估计重复嵌套期望，达到$\tilde O(\varepsilon^{-1})$成本，相比经典算法实现近二次加速，扩展了量子优势到重复嵌套场景。


<details>
  <summary>Details</summary>
Motivation: 研究重复嵌套期望（RNEs）的估计问题，扩展先前量子计算对单层嵌套期望的加速优势到多层嵌套场景，覆盖更广泛的应用领域如最优停止问题。

Method: 提出量子算法，采用经典随机多级蒙特卡洛（rMLMC）算法的去随机化变体，通过精心设计的去随机化过程解决量子化经典随机算法时常见的时间可变性问题。

Result: 量子算法以$\tilde O(\varepsilon^{-1})$成本实现$\varepsilon$误差估计，达到理论下界，相比最佳经典算法获得近二次加速，成功将量子优势扩展到重复嵌套期望估计。

Conclusion: 该研究成功开发了高效的量子算法用于重复嵌套期望估计，通过创新的去随机化技术克服了量子化经典算法的挑战，为量子计算在金融数学和优化等领域的应用提供了新工具。

Abstract: We study the estimation of repeatedly nested expectations (RNEs) with a constant horizon (number of nestings) using quantum computing. We propose a quantum algorithm that achieves $\varepsilon$-error with cost $\tilde O(\varepsilon^{-1})$, up to logarithmic factors. Standard lower bounds show this scaling is essentially optimal, yielding an almost quadratic speedup over the best classical algorithm. Our results extend prior quantum speedups for single nested expectations to repeated nesting, and therefore cover a broader range of applications, including optimal stopping. This extension requires a new derandomized variant of the classical randomized Multilevel Monte Carlo (rMLMC) algorithm. Careful de-randomization is key to overcoming a variable-time issue that typically increases quantized versions of classical randomized algorithms.

</details>


### [41] [Spinor Double-Quantum Excitation in the Solution NMR of Near-Equivalent Spin-1/2 Pairs](https://arxiv.org/abs/2602.08157)
*Urvashi D. Heramun,Mohamed Sabba,Dolnapa Yamano,Christian Bengs,Bonifac Legrady,Giuseppe Pileio,Sam Thompson,Malcolm H. Levitt*

Main category: quant-ph

TL;DR: 提出基于旋量行为的双量子激发新方法，用于近等价自旋-1/2对的溶液NMR分析


<details>
  <summary>Details</summary>
Motivation: 解决近等价自旋-1/2对在核磁共振中的双量子激发问题，利用旋量行为特性开发更有效的激发方法

Method: 利用2π旋转后量子态变号的旋量行为，通过操纵单量子相干相位来制备双量子前体态，然后通过π/2旋转快速转换为双量子相干。包括基于对称脉冲序列的方法和基于SLIC（自旋锁定诱导交叉）的方法，后者引入了对射频场幅度偏差具有良好补偿性的变体

Result: 在含有非对映异构¹⁹F核对的分子系统中成功演示了双量子滤波¹⁹F NMR，并与现有技术进行了比较

Conclusion: 基于旋量行为的双量子激发方法为近等价自旋-1/2对的NMR分析提供了新的有效工具，特别是SLIC变体对射频场偏差具有良好鲁棒性

Abstract: A family of double-quantum excitation schemes is described for the solution nuclear magnetic resonance (NMR) of near-equivalent spin-1/2 pairs. These new methods exploit the spinor behaviour of 2-level systems, whose signature is the change of sign of a quantum state upon a $2π$ rotation. The spinor behaviour is used to manipulate the phases of single-quantum coherences, in order to prepare a double-quantum precursor state which is rapidly converted into double-quantum coherence by a straightforward $π/2$ rotation. One set of spinor-based methods exploits symmetry-based pulse sequences, while the other set exploits SLIC (spin-lock-induced crossing), in which the nutation frequency under a resonant radiofrequency field is matched to the spin-spin coupling. A variant of SLIC is introduced which is well-compensated for deviations in the radiofrequency field amplitude. The methods are demonstrated by performing double-quantum-filtered $^{19}$F NMR on a molecular system containing a pair of diastereotopic $^{19}$F nuclei. The new methods are compared with existing techniques.

</details>


### [42] [Detecting multilevel entanglement from light-based entanglement witnesses](https://arxiv.org/abs/2602.08180)
*Pedro Rosario,Romain Bachelard*

Main category: quant-ph

TL;DR: 提出基于电场的不等式来检测多能级量子发射器系统的多体纠缠，该方法无需局域测量，对噪声鲁棒且适用于混合态


<details>
  <summary>Details</summary>
Motivation: 多能级量子系统（如超导量子比特、里德堡原子、量子点）中的纠缠检测通常需要局域测量，这在实际实验中可能难以实现。需要开发无需局域测量的纠缠检测方法，特别是针对多能级系统的特性。

Method: 引入基于电场的不等式来检测N个量子发射器系统的多能级纠缠。利用偏振通道和检测方向来增强纠缠检测能力，这是多能级系统特有的特性。将见证子应用于典型量子态家族（如Dicke态、单重态、W-like态）来验证其有效性。

Result: 该方法不仅能检测真实多体纠缠，而且对噪声具有鲁棒性，适用于混合纠缠态。偏振通道和检测方向的选择能显著增强纠缠检测能力。

Conclusion: 为多能级发射器系统中的纠缠检测开辟了新途径，无需局域测量即可实现高效检测，有望应用于超导量子比特、里德堡原子和量子点等实际系统。

Abstract: We introduce a set of electric-field based inequalities capable of detecting multilevel entanglement from a system of N quantum emitters. We determine that the polarization channel as well as the direction of detection can enhance entanglement detection, a feature specific to multilevel systems. We demonstrate the efficiency of the witnesses to detect genuine multipartite entanglement by applying it to families of paradigmatic quantum states, such as Dicke states, singlet states and W-like states. The detection is not only robust to noise, but also applies to mixed entangled states. Our findings open up possibilities for the detection of entanglement without local measurements in systems of multilevel emitters such as superconducting qubits, Rydberg atoms or quantum dots.

</details>


### [43] [Preparing squeezed, cat and GKP states with parity measurements](https://arxiv.org/abs/2602.08209)
*Zhiyuan Lin,Sen Li,Jingyan Feng,Valentin Ivannikov,Matteo Fadel,Tim Byrnes*

Main category: quant-ph

TL;DR: 提出一种基于位移宇称测量的协议，用于制备多种玻色量子态，包括压缩态、猫态和GKP态，仅需三次宇称测量即可实现约9dB的压缩。


<details>
  <summary>Details</summary>
Motivation: 玻色模式是量子技术中的重要资源，用于量子信息的存储、处理和转换。虽然现有技术可以通过辅助量子比特实现玻色模式的相干控制和读取，但需要更高效的量子态制备协议。

Method: 提出基于位移宇称测量的协议，利用强色散耦合下的宇称测量结合相空间位移操作，通过少量测量步骤制备目标量子态。

Result: 该协议仅需三次宇称测量即可实现约9dB的压缩态，且对实验缺陷具有鲁棒性。还可推广到制备猫态和Gottesman-Kitaev-Preskill态等典型玻色态。

Conclusion: 基于位移宇称测量的协议为玻色量子态的制备提供了一种高效且鲁棒的方法，适用于多种量子平台，有望推动量子信息处理的发展。

Abstract: Bosonic modes constitute a central resource in a wide range of quantum technologies, providing long-lived degrees of freedom for the storage, processing, and transduction of quantum information. Such modes naturally arise in platforms including circuit quantum electrodynamics, quantum acoustodynamics, and trapped-ion systems. In these architectures, coherent control and high-fidelity readout of the bosonic degrees of freedom are achieved via coupling to an auxiliary qubit. When operated in the strong dispersive regime, this interaction enables parity measurements of the mode which, in combination with phase-space displacements, constitute a standard experimental tool for full Wigner-function tomography. Here, we propose a protocol based on displaced parity measurements that allows for the preparation of a variety of bosonic quantum states. As a first example, we demonstrate the generation of squeezed states, achieving up to ~9 dB of squeezing after only three parity measurements, and show that the protocol is robust against experimental imperfections. Finally, we generalize our approach to the preparation of other paradigmatic bosonic states, including cat and Gottesman-Kitaev-Preskill states.

</details>


### [44] [The simplified quantum circuits for implementing quantum teleportation](https://arxiv.org/abs/2602.08345)
*Wen-Xiu Zhang,Guo-Zhu Song,Hai-Rui Wei*

Main category: quant-ph

TL;DR: 该论文提出了一种优化量子电路设计的方法，显著减少了多种纠缠通道下量子隐形传态的门数、成本和深度，并在IBM量子计算机上验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 量子信息处理需要尽可能小且浅的量子电路设计，以降低实现复杂度和提高效率。现有量子隐形传态方案的门数、成本和深度仍有优化空间。

Method: 设计简化的量子电路，优化门数、成本和深度，针对多种纠缠通道（GHZ、二/三量子比特簇、Brown、Borras、纠缠交换等）的量子隐形传态方案进行改进，无需前馈恢复操作。

Result: 显著降低了各种纠缠通道量子隐形传态的门数、成本和深度：GHZ从10/6/8降至9/4/6，二量子比特簇从9/4/5降至6/3/5，三量子比特簇从12/6/7降至8/4/5，Brown从25/15/17降至18/8/7，Borras从36/25/20降至15/8/11，纠缠交换从13/8/8降至10/5/5。

Conclusion: 提出的简化压缩方案能有效优化量子隐形传态电路，在IBM量子计算机上的实验验证表明这些方案具有良好保真度，为量子信息处理提供了更高效的实现方法。

Abstract: It is crucial to design quantum circuits as small as possible and as shallow as possible for quantum information processing tasks. We design quantum circuits with simplified gate-count, cost, and depth for implementing quantum teleportation among various entangled channels. Here the gate-count/cost/depth of the Greenberger-Horne-Zeilinger-based quantum teleportation is reduced from 10/6/8 to 9/4/6, the two-qubit-cluster-based quantum teleportation is reduced from 9/4/5 to 6/3/5, the three-qubit-cluster-based quantum teleportation is reduced from 12/6/7 to 8/4/5, the Brown-based quantum teleportation is reduced from 25/15/17 to 18/8/7, the Borras-based quantum teleportation is reduced from 36/25/20 to 15/8/11, and the entanglement-swapping-based quantum teleportation is reduced from 13/8/8 to 10/5/5. Note that, no feed-forward recover operation is required in the simplified schemes. Moreover, the experimentally demonstrations on IBM quantum computer indicate that our simplified and compressed schemes can be realized with good fidelity.

</details>


### [45] [Quantum-classical framework for many-fermion response and structure](https://arxiv.org/abs/2602.08357)
*Weijie Du,Yangguang Yang,Zixin Liu,Chao Yang,James P. Vary*

Main category: quant-ph

TL;DR: 提出量子-经典混合框架计算多费米子系统的响应函数和束缚态谱，使用洛伦兹积分变换和新哈密顿量输入方案，应用于¹⁹O核系统验证


<details>
  <summary>Details</summary>
Motivation: 响应函数是探测多体系统结构和动力学的关键可观测量，但传统方法难以高效计算一般多费米子系统的响应函数和完整束缚态谱

Method: 采用量子-经典混合框架，结合洛伦兹积分变换和新哈密顿量输入方案，实现可扩展的量子电路构造，并提出三种协议提取响应函数和束缚态结构信息

Result: 成功应用于¹⁹O核系统，使用真实核子间相互作用计算了束缚态谱和响应函数，验证了方法的有效性

Conclusion: 该方法为探索多体系统的结构和动力学开辟了新途径，可广泛应用于不同领域的多体系统研究

Abstract: Response functions are key observables for probing the structure and dynamics of many-body systems. We introduce and demonstrate a quantum-classical framework for computing response functions of general many-fermion systems that also provides the full bound-state spectrum. The framework employs the Lorentz integral transform and a new Hamiltonian input scheme that enables practical and scalable circuit constructions for general many-fermion Hamiltonians. Within this framework, we develop a hybrid strategy to evaluate the Lorentz integral and propose three protocols to extract response functions and bound-state structural information. As a demonstration, we apply the method to \({}^{19}\mathrm{O}\) with realistic internucleon interactions, computing both the bound-state spectrum and the response function. We envision that our approach will open new avenues for exploring the structure and dynamics of a broad class of many-body systems across diverse fields.

</details>


### [46] [Quantum Detection of Sequency-Band Structure](https://arxiv.org/abs/2602.08393)
*Alok Shukla,Prakash Vedula*

Main category: quant-ph

TL;DR: 提出一种量子算法，用于估计量子编码信号中用户指定序率带的幅度内容，通过序率排序的量子Walsh-Hadamard变换、比较器oracle和量子幅度估计实现，在量子算法中具有指数级优势。


<details>
  <summary>Details</summary>
Motivation: 传统信号处理中，Walsh-Hadamard变换需要O(Nlog₂N)操作，而量子算法可以在量子态上实现指数加速。需要一种能够在量子算法中无缝集成的方法来检测信号的结构化成分和异常。

Method: 使用序率排序的量子Walsh-Hadamard变换（电路深度O(log₂N)），结合比较器oracle相干标记任意序率范围内的基态，然后应用量子幅度估计来估计选定带中的总概率质量。

Result: 量子Walsh-Hadamard变换在振幅编码量子态上的电路深度为O(log₂N)，相比经典快速Walsh-Hadamard变换的O(Nlog₂N)操作，在量子算法中实现了指数级优势。算法可作为结构异常指示器。

Conclusion: 该量子算法为量子增强信号处理提供了有效工具，能够检测高低序率特征和异常，在量子算法中具有指数优势，适用于零交叉分析、带限噪声估计和Walsh基特征提取等任务。

Abstract: We present a quantum algorithm for estimating the amplitude content of user-specified sequency bands in quantum-encoded signals. The method employs a sequency-ordered Quantum Walsh-Hadamard Transform (QWHT), a comparator-based oracle that coherently marks basis states within an arbitrary sequency range, and Quantum Amplitude Estimation (QAE) to estimate the total probability mass in the selected band. This enables the detection of structured signal components, including both high- and low-sequency features, as well as the identification of rapid sign-change behavior associated with noise or anomalies. The proposed method can be embedded as a module within a larger quantum algorithm; in this setting, both the input and output remain fully quantum, enabling seamless integration with upstream and downstream quantum operations. We show that the sequency-ordered QWHT can be implemented with circuit depth $O(\log_2 N)$ (equivalently $O(n)$ for $N=2^n$) when acting on an amplitude-encoded quantum state, whereas computing the full Walsh-Hadamard spectrum of an explicit length-$N$ classical signal requires $O(N\log_2 N)$ operations via the fast Walsh-Hadamard transform. This results in an exponential quantum advantage when the QWHT is used as a modular block within a larger quantum algorithm, relative to classical fast Walsh-Hadamard transform-based approaches operating on explicit data. From an application perspective, the proposed sequency band-energy estimation may be interpreted as a structure-based anomaly indicator, enabling the detection of unexpected high-sequency components relative to a nominal low-sequency signal class. The algorithm is applicable to quantum-enhanced signal processing tasks such as zero-crossing analysis, band-limited noise estimation, and feature extraction in the Walsh basis.

</details>


### [47] [Efficient circuit compression by multi-qudit entangling gates in linear optical quantum computation](https://arxiv.org/abs/2602.08394)
*Apurav,Jaskaran Singh*

Main category: quant-ph

TL;DR: 该论文提出了多级控制-Z门来解决线性光学量子计算中非局域纠缠门的概率性限制问题，通过两种方案提高了效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 线性光学量子计算（LOQC）的可扩展性受到非局域纠缠门概率性的限制。现有的qudit电路压缩方案在只有部分编码qubit参与非局域纠缠门时效率低下，导致非局域门数量指数级增加。

Method: 提出了多级控制-Z门的概念，不同于传统的两级CZ门，它可以对任意选择的子集空间模式施加条件相移。提出了两种具体的线性光学实现方案：第一种方案以1/8的恒定成功率实现，但具有状态依赖性；第二种方案提供完全状态独立的实现，减少了非局域门的数量。

Result: 第一种方案以1/8的恒定成功率实现，优于现有的1/9成功率。第二种方案将非局域门数量从O(2^{r1+r2})减少到O(2^{r1}+2^{r2})，成功率为1/2*(1/8)^{2^{r1}+2^{r2}}。

Conclusion: 该研究通过多级控制-Z门解决了LOQC的关键可扩展性限制，与qudit电路压缩方案结合时，显著提高了LOQC架构的效率。

Abstract: Linear optical quantum computation (LOQC) offers a promising platform for scalable quantum information processing, but its scalability is fundamentally constrained by the probabilistic nature of non-local entangling gates. Qudit circuit compression schemes mitigate this issue by encoding multiple qubits onto qudits. However, these schemes become inefficient when only a subset of the encoded qubits is required to participate in the non-local entangling gate, leading to an exponential increase in the number of non-local gates. In this Letter, we address this bottleneck by demonstrating the existence of multi-level control-Z (CZ) gates for qudits encoded in multiple spatial modes in LOQC. Unlike conventional two-level CZ gates, which act only on a single pair of modes, multi-level CZ gates impart a conditional phase shift for an arbitrarily chosen subset of the spatial modes. We present two explicit linear optical schemes that realize such operations, illustrating a fundamental trade-off between prior information about the input quantum state and the physical resources required. The first scheme is realized with a constant success probability of $1/8$ independent of the qudit dimension using a single non-local entangling gate, at the cost of state dependence, which is significantly better than the current success probability of $1/9$. Our second scheme provides a fully state independent realization reducing the number of non-local gates to $\mathcal{O}(2^{r_1}+2^{r_2})$ as compared to the existing bound of $\mathcal{O}(2^{r_1+r_2})$ where $r_1$ and $r_2$ are the number of qubits to be removed as control in the qudits. The success probability of the realization is $\frac{1}{2} \left(\frac{1}{8}\right)^{2^{r_1}+2^{r_2}}$. When combined with qudit circuit compression schemes, our results improve upon a key scalability limitation and significantly improve the efficiency of LOQC architectures.

</details>


### [48] [The Finite Geometry of Breaking Quantum Secrets](https://arxiv.org/abs/2602.08410)
*Péter Lévay,Metod Saniga*

Main category: quant-ph

TL;DR: 该论文通过有限几何框架研究五边形和七边形量子码，揭示了量子秘密共享与上下文性之间的统一关系，并基于此推导了具体的秘密破解协议。


<details>
  <summary>Details</summary>
Motivation: 研究量子秘密共享与上下文性之间的内在联系，探索如何通过有限几何框架统一理解这两个概念，为量子信息处理提供新的几何视角。

Method: 采用有限几何框架分析五边形和七边形量子码，重点研究其稳定子群的2+3和3+4张量分解，构建对应的三量子比特和四量子比特嵌入的二元辛极空间结构。

Result: 建立了量子秘密共享与上下文性之间的几何联系，展示了特定量子比特嵌入如何控制上下文性和纠缠特性，并针对(3,5)和(4,7)门限方案推导了具体的秘密破解协议。

Conclusion: 该研究为理解上下文性配置提供了新颖的几何视角，表明有限几何框架能够统一研究量子秘密共享和上下文性，为量子信息处理开辟了新的研究方向。

Abstract: Using a finite geometric framework for studying the pentagon and heptagon codes we show that the concepts of quantum secret sharing and contextuality can be studied in a nice and unified manner. The basic idea is a careful study of the respective $2+3$ and $3+4$ tensorial factorizations of the elements of the stabilizer groups of these codes. It is demonstrated in detail how finite geometric structures entailing a specific three-qubit (resp. four-qubit) embedding of binary symplectic polar spaces of rank two (resp. three), corresponding to these factorizations, govern issues of contextuality and entanglement needed for a geometric understanding of quantum secret sharing. Using these results for the $(3,5)$ and $(4,7)$ threshold schemes explicit secret breaking protocols are derived. Our results hint at a novel geometric way of looking at contextual configurations.

</details>


### [49] [Grover Adaptive Search with Problem-Specific State Preparation](https://arxiv.org/abs/2602.08418)
*Maximilian Hess,Lilly Palackal,Abhishek Awasthi,Peter J. Eder,Manuel Schnaus,Laurin Demmler,Karen Wintersperger,Joseph Doetsch*

Main category: quant-ph

TL;DR: 本文基于Baertschi和Eidenbenz的工作，为旅行商问题构建启发式状态准备例程，模仿经典的Lin-Kernighan启发式算法，旨在通过多项式次数的Grover迭代获得合理的近似比。


<details>
  <summary>Details</summary>
Motivation: Grover搜索算法是量子算法的基本构建块，但将其成功应用于组合优化问题是一个微妙挑战。由于二次加速不足以在指数级大的空间中朴素搜索，需要利用问题结构设计状态准备例程来增加有希望状态的振幅。

Method: 基于Baertschi和Eidenbenz的工作，为旅行商问题构建启发式状态准备例程，模仿经典的Lin-Kernighan启发式算法。比较了与终止标准和未知标记解数量时Grover迭代次数选择相关的几种算法设置。

Result: 通过启发式状态准备，旨在实现仅使用多项式次数的Grover迭代就能获得合理的近似比。对不同的算法设置进行了比较分析。

Conclusion: 通过将经典启发式算法（如Lin-Kernighan）与Grover搜索相结合，可以为组合优化问题（如TSP）构建有效的量子算法，在多项式迭代次数内获得良好的近似解。

Abstract: Grover's search algorithm is one of the basic building block in the world of quantum algorithms. Successfully applying it to combinatorial optimization problems is a subtle challenge. As a quadratic speedup is not enough to naively search an exponentially large space, the search has to be complemented with a state preparation routine which increases the amplitudes of promising states by exploiting the problem structure. In this paper, we build upon previous work by Baertschi and Eidenbenz to construct heuristic state preparation routines for the Traveling Salesperson Problem (TSP), mimicking the well-known classical Lin-Kernighan heuristic. With our heuristic, we aim to achieve a reasonable approximation ratio with only a polynomial number of Grover iterations. Further, we compare several algorithmic settings relating to termination criteria and the choice of Grover iterations when the number of marked solutions is unknown.

</details>


### [50] [Plethysm is in #BQP](https://arxiv.org/abs/2602.08441)
*Matthias Christandl,Aram W. Harrow,Greta Panova,Pietro M. Posta,Michael Walter*

Main category: quant-ph

TL;DR: 该论文证明了一类广泛的表示论重数（包括plethysm系数）属于量子复杂度类#BQP，统一并扩展了先前关于Kronecker系数等特殊情况的量子复杂度研究。


<details>
  <summary>Details</summary>
Motivation: 表示论重数（如Kostka系数、Littlewood-Richardson系数）的计算复杂度是数学和计算机科学中的重要开放问题，对几何复杂性理论和量子信息有重要意义。先前研究主要关注特定重数（如Kronecker系数）的量子复杂度，需要更一般的理论框架。

Method: 通过多次应用Schur变换，并利用近期改进的Schur变换在局部维度上的依赖关系。论文还描述了一个展示表示论重数属于#BQP的通用方法框架。

Result: 证明了一类广泛的表示论重数属于量子复杂度类#BQP，特别包括plethysm系数。该结果统一、简化并扩展了先前关于Kronecker系数等特殊情况的量子复杂度研究。同时证明这些重数也属于GapP类，并在某些参数固定时存在多项式时间经典算法。

Conclusion: 该研究为表示论重数的量子计算复杂度提供了统一的理论框架，将plethysm系数等广泛类别的重数纳入#BQP复杂度类，建立了量子计算与表示论之间的重要联系，并为几何复杂性理论和量子信息提供了新的计算工具。

Abstract: Some representation-theoretic multiplicities, such as the Kostka and the Littlewood-Richardson coefficients, admit a combinatorial interpretation that places their computation in the complexity class #P. Whether this holds more generally is considered an important open problem in mathematics and computer science, with relevance for geometric complexity theory and quantum information. Recent work has investigated the quantum complexity of particular multiplicities, such as the Kronecker coefficients and certain special cases of the plethysm coefficients.
  Here, we show that a broad class of representation-theoretic multiplicities is in #BQP. In particular, our result implies that the plethysm coefficients are in #BQP, which was only known in special cases. It also implies all known results on the quantum complexity of previously studied coefficients as special cases, unifying, simplifying, and extending prior work. We obtain our result by multiple applications of the Schur transform. Recent work has improved its dependence on the local dimension, which is crucial for our work. We further describe a general approach for showing that representation-theoretic multiplicities are in #BQP that captures our approach as well as the approaches of prior work. We complement the above by showing that the same multiplicities are also naturally in GapP and obtain polynomial-time classical algorithms when certain parameters are fixed.

</details>


### [51] [Non-Markovianity induced by Pauli-twirling](https://arxiv.org/abs/2602.08464)
*Joris Kattemölle,Balázs Gulácsi,Guido Burkard*

Main category: quant-ph

TL;DR: 论文指出，在量子信息处理中，通过Pauli twirling将任意噪声通道转化为Pauli通道时，原本的马尔可夫噪声会变成非马尔可夫噪声，这需要使用负的Pauli-Lindblad参数进行正确描述。


<details>
  <summary>Details</summary>
Motivation: 量子噪声是量子信息处理的主要障碍。Pauli twirling技术可以将任意噪声通道转化为Pauli通道，这在理论描述容错量子计算和噪声表征中至关重要。然而，现有的Pauli-Lindblad通道参数化方法假设非负参数，这依赖于底层噪声过程的马尔可夫性。论文旨在研究Pauli twirling对马尔可夫性的影响。

Method: 使用通道（而非整个半群）的马尔可夫性概念，证明一般Pauli通道是非马尔可夫的当且仅当其至少一个Pauli-Lindblad参数为负。基于此，研究Pauli twirling对马尔可夫性的影响，分析标准马尔可夫噪声下量子门实现的具体案例。

Result: 证明马尔可夫量子通道经过Pauli twirling后通常会变成非马尔可夫通道。这种Pauli twirling诱导的非马尔可夫性要求在实验现实场景中使用负的Pauli-Lindblad参数进行正确的噪声描述。具体示例包括标准马尔可夫噪声下√X门的实现。

Conclusion: Pauli twirling会导致原本的马尔可夫噪声变为非马尔可夫噪声，这需要使用负的Pauli-Lindblad参数进行准确描述。这一发现对依赖精确噪声表征的量子误差缓解协议具有直接意义，表明在噪声建模中需要考虑这种效应。

Abstract: Noise forms a central obstacle to effective quantum information processing. Recent experimental advances have enabled the tailoring of noise properties through Pauli twirling, transforming arbitrary noise channels into Pauli channels. This underpins theoretical descriptions of fault-tolerant quantum computation and forms an essential tool in noise characterization and error mitigation. Pauli-Lindblad channels have been introduced to aptly parameterize quasi-local Pauli errors across a quantum register, excluding negative Pauli-Lindblad parameters relying on the Markovianity of the underlying noise processes. We point out that caution is required when parameterizing channels as Pauli-Lindblad channels with nonnegative parameters. For this, we study the effects of Pauli twirling on Markovianity. We use the notion of Markovianity of a channel (rather than that of an entire semigroup) and prove a general Pauli channel is non-Markovian if and only if at least one of its Pauli-Lindblad parameters is negative. Using this, we show that Markovian quantum channels often become non-Markovian after Pauli twirling. The Pauli-twirling induced non-Markovianity necessitates the use of negative Pauli-Lindblad parameters for a correct noise description in experimentally realistic scenarios. An important example is the implementation of the $\sqrt{X}$-gate under standard Markovian noise. As such, our results have direct implications for quantum error mitigation protocols that rely on accurate noise characterization.

</details>


### [52] [Classifying the simplest Bell inequalities beyond qubits and their applications towards self-testing](https://arxiv.org/abs/2602.08469)
*Palash Pandya,Shubhayan Sarkar,Remigiusz Augusiak*

Main category: quant-ph

TL;DR: 本文研究了(2,2,3)场景下的贝尔不等式，即两方各进行两个三结果测量的情况。作者表征了所有可以从最简单的平方和分解中产生的贝尔不等式，这些不等式被三维最大纠缠态最大违反，并利用这些不等式自测试该态及一类三结果测量。


<details>
  <summary>Details</summary>
Motivation: 贝尔不等式揭示了量子力学的非局域本质。探索所有可能的贝尔不等式对于几何表征量子理论中可实现的非局域关联集合很重要，同时为特定量子信息处理任务构建定制化的贝尔不等式提供了系统方法。在(2,2,2)场景下已有较好理解，但在涉及更多结果的场景中，已知的贝尔不等式相对较少。

Method: 考虑(2,2,3)场景（两方各进行两个三结果测量），表征所有可以从最简单的平方和分解中产生的贝尔不等式。这些不等式被三维最大纠缠态最大违反，并利用这些不等式进行自测试。

Result: 成功表征了(2,2,3)场景下所有可以从平方和分解中产生的贝尔不等式，这些不等式被三维最大纠缠态最大违反。利用这些不等式实现了对该态及一类三结果测量的自测试。

Conclusion: 本文扩展了对贝尔不等式的理解，从简单的(2,2,2)场景推进到更复杂的(2,2,3)场景，为量子非局域性的几何表征和量子信息处理任务的定制化贝尔不等式设计提供了新工具。

Abstract: Bell inequalities reveal the fundamentally nonlocal character of quantum mechanics. In this regard, one of the interesting problems is to explore all possible Bell inequalities that demonstrate a gap between local and nonlocal quantum behaviour. This is useful for the geometric characterisation of the set of nonlocal correlations achievable within quantum theory. Moreover, it provides a systematic way to construct Bell inequalities that are tailored to specific quantum information processing tasks. This characterisation is well understood in the simplest $(2,2,2)$ scenario, namely two parties performing two binary outcome measurements. However, beyond this setting, relatively few Bell inequalities are known, and the situation becomes particularly scarce in scenarios involving a greater number of outcomes. Here, we consider the $(2,2,3)$ scenario, or two parties performing two three-outcome measurements, and characterise all Bell inequalities that can arise from the simplest sum-of-squares decomposition and are maximally violated by the maximally entangled state of local dimension three. We then utilise them to self-test this state, along with a class of three-outcome measurements.

</details>


### [53] [A building block of quantum repeaters for scalable quantum networks](https://arxiv.org/abs/2602.08472)
*Wen-Zhao Liu,Ya-Bin Zhou,Jiu-Peng Chen,Bin Wang,Ao Teng,Xiao-Wen Han,Guang-Cheng Liu,Zhi-Jiong Zhang,Yi Yang,Feng-Guang Liu,ChaoHui Xue,Bo-Wen Yang,Jin Yang,Chao Zeng,Du-Ruo Pan,Ming-Yang Zheng,Xing-Jian Zhang,Cao Shen,Yi-Zheng Zhen,You Xiao,Hao Li,Li-Xing You,XiongFeng Ma,Qi Zhao,Feihu Xu,Ye Wang,Yong Wan,Qiang Zhang,Jian-Wei Pan*

Main category: quant-ph

TL;DR: 该研究开发了长寿命离子阱量子存储器、高效电信接口和高可见度单光子纠缠协议，实现了10公里光纤内建立和维持存储器-存储器纠缠，并演示了城域规模的设备无关量子密钥分发。


<details>
  <summary>Details</summary>
Motivation: 量子网络需要确定性长距离纠缠分发，但光纤中的指数光子损耗限制了效率。量子中继器是解决方案，但现有技术中远程存储器-存储器纠缠的建立速度慢于退相干速度，成为关键瓶颈。

Method: 开发了长寿命离子阱量子存储器、高效电信接口和高可见度单光子纠缠协议，在10公里光纤内建立和维持存储器-存储器纠缠，并应用于设备无关量子密钥分发。

Result: 在10公里光纤内成功建立和维持存储器-存储器纠缠，实现了城域规模设备无关量子密钥分发，从4.05×10^5个贝尔对中提取了1,917个秘密密钥，在渐近极限下实现了101公里的正密钥率。

Conclusion: 该工作克服了远程存储器-存储器纠缠建立速度慢于退相干速度的关键瓶颈，为量子中继器提供了关键构建模块，是迈向可扩展量子网络的重要一步。

Abstract: Quantum networks, integrating quantum communication, quantum metrology, and distributed quantum computing, could provide secure and efficient information transfer, high-resolution sensing, and an exponential speed-up in information processing. Deterministic entanglement distribution over long distances is a prerequisite for scalable quantum networks, enabling the utilization of device-independent quantum key distribution (DI-QKD) and quantum teleportation to achieve secure and efficient information transfer. However, the exponential photon loss in optical fibres prohibits efficient and deterministic entanglement distribution. Quantum repeaters, incorporating entanglement swapping and entanglement purification with quantum memories, offer the most promising means to overcome this limitation in fibre-based quantum networks. Despite numerous pioneering efforts toward realizing quantum repeaters, a critical bottleneck remains, as remote memory-memory entanglement suffers from decoherence more rapidly than it can be established and purified over long distances. We overcome this by developing long-lived trapped-ion memories, an efficient telecom interface, and a high-visibility single-photon entanglement protocol. This allows us to establish and maintain memory-memory entanglement over a 10 km fibre within the average entanglement establishment time for the same distance. As a direct application, we demonstrate metropolitan-scale DI-QKD, distilling 1,917 secret keys out of 4.05*10^5 Bell pairs over 10 km. We further report a positive key rate over 101 km in the asymptotic limit, extending the achievable distance by more than two orders of magnitude. Our work provides a critical building block for quantum repeaters and marks an important step toward scalable quantum networks.

</details>


### [54] [Empirical Study of Observable Sets in Multiclass Quantum Classification](https://arxiv.org/abs/2602.08485)
*Paul San Sebastian,Mikel Cañizo,Roman Orus*

Main category: quant-ph

TL;DR: 本文研究了多类量子机器学习中的两种主要分类标准：最大化代表类别的可观测量期望值，以及最大化编码量子态与代表类别的参考态之间的保真度。通过比较不同可观测量集合对模型性能的影响，为未来多类量子机器学习模型设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 目前大多数量子监督学习研究局限于二分类问题，或通过二元分类器集成实现多分类。少数提出原生多类模型的研究未能充分论证分类可观测量选择的合理性。因此需要系统研究多类量子机器学习中的分类标准问题。

Method: 研究两种主要分类标准：1）最大化代表类别的可观测量期望值；2）最大化编码量子态与代表类别的参考态之间的保真度。选择两组可观测量进行比较：Pauli字符串集合和计算基投影算子集合。分析不同可观测量集合对模型性能的影响，特别是在Barren Plateaus和Neural Collapse方面的表现。

Result: 通过实证观察每种模型类型的行为，分析了不同可观测量集合选择对量子机器学习模型性能的影响。研究结果为理解Barren Plateaus和Neural Collapse现象提供了见解。

Conclusion: 研究结果为未来多类量子机器学习模型的设计提供了重要指导，特别是在选择适当的分类标准和可观测量集合方面。这些见解有助于改进量子机器学习模型的性能和可训练性。

Abstract: Variational quantum algorithms have gained attention as early applications of quantum computers for learning tasks. In the context of supervised learning, most of the works that tackle classification problems with parameterized quantum circuits constrain their scope to the setting of binary classification or perform multiclass classification via ensembles of binary classifiers (strategies such as one versus rest). Those few works that propose native multiclass models, however, do not justify the choice of observables that perform the classification. This work studies two main classification criteria in multiclass quantum machine learning: maximizing the expected value of an observable representing a class or maximizing the fidelity of the encoded quantum state with a reference state representing a class. To compare both approaches, sets of Pauli strings and sets of projectors into the computational basis are chosen as observables in the quantum machine learning models. Observing the empirical behavior of each model type, the effect of different observable set choices on the performance of quantum machine learning models is analyzed in the context of Barren Plateaus and Neural Collapse. The results provide insights that may guide the design of future multiclass quantum machine learning models.

</details>


### [55] [Intelligent Control of Collisional Architectures for Deterministic Multipartite State Engineering](https://arxiv.org/abs/2602.08526)
*Duc-Kha Vu,Minh Tam Nguyen,Özgür E. Müstecaplıoğlu,Fatih Ozaydin*

Main category: quant-ph

TL;DR: 提出智能约束感知控制框架，通过优化碰撞角度参数，在重复交互架构中确定性生成对称Dicke态，无需投影测量，对噪声具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 设计可扩展、抗噪声的多体纠缠控制协议是量子技术的核心挑战，需要算法化合成交互参数而非手工设计门序列。

Method: 采用闭环设计方法：使用两个不相交量子比特寄存器与m个辅助"穿梭"量子比特，通过保持激发数的部分SWAP碰撞，将Dicke态制备转化为约束优化问题。使用多起点L-BFGS-B算法优化寄存器内和穿梭-寄存器碰撞角度参数。

Result: 优化控制器在广泛误差范围内保持高保真度，包括随机交互丢失和标准退相干通道。噪声主要增加所需碰撞轮数，控制器可通过时间换取保真度。

Conclusion: 该框架实现了无需投影测量的确定性Dicke态生成，扩展了碰撞纠缠生成到任意激发数，展示了在噪声环境中通过优化控制参数实现鲁棒量子态制备的能力。

Abstract: Designing scalable, noise-tolerant control protocols for multipartite entanglement is a central challenge for quantum technologies, and it naturally calls for \emph{algorithmic} synthesis of interaction parameters rather than handcrafted gate sequences. Here we introduce an intelligent, constraint-aware control framework for deterministic generation of symmetric Dicke states $|D_n^{(m)}\rangle$ in repeated-interaction (collision-model) architectures. The protocol employs excitation-preserving partial-SWAP collisions between two disjoint qubit registers, mediated by $m$ ancillary ``shuttle'' qubits, and poses Dicke-state preparation as a \emph{closed-loop design} problem: given the target $(n,m)$, automatically infer collision strengths that maximize fidelity under practical constraints. Concretely, we formulate a two-parameter, bound-constrained optimization over intra-register and shuttle--register collision angles and solve it using a multi-start strategy with L-BFGS-B, yielding a reproducible controller prescription (optimized $γ_{\mathrm{in}}$, $γ_{\mathrm{sh}}$, and minimal-round convergence points) for each target. This removes the need for projective measurements and extends collisional entanglement generation beyond the single-excitation (W-state) sector to arbitrary $m$. Crucially, we optimize \emph{within} imperfect collisional dynamics where errors act throughout the sequence, including stochastic interaction dropouts (missing collisions) and standard decoherence channels. Strikingly, across wide error ranges the optimized controller preserves high preparation fidelity; imperfections manifest primarily as a modest increase in the required number of collision rounds. This behavior reflects a tunable competition in which noise suppresses correlations while properly chosen collisions continuously replenish them, allowing the control algorithm to trade time for fidelity.

</details>


### [56] [Time resolution at the quantum limit of two incoherent sources based on frequency resolved two-photon-interference](https://arxiv.org/abs/2602.08578)
*Salvatore Muratore,Vincenzo Tamma*

Main category: quant-ph

TL;DR: 该论文提出了一种基于双光子量子拍频现象的新技术，能够突破传统瑞利准则限制，实现时间延迟估计精度达到量子极限的一半。


<details>
  <summary>Details</summary>
Motivation: 传统瑞利准则限制了非相干源的空间分辨率，类似问题也存在于两个弱非相干信号的时间延迟估计中。现有方法受限于时间延迟估计的精度限制。

Method: 利用参考源光子与两个非相干弱信号光子在分束器上的干涉，在频域产生双光子量子拍频现象。通过测量干涉光子的频率（分束器输出的聚束或反聚束），以较少测量次数实现高精度时间延迟估计。

Result: 该方法能够达到量子极限一半的精度，且独立于光子波包的时间形状和待估计的时间延迟。技术可行性高，测量次数相对较少。

Conclusion: 该技术可应用于天文学、显微镜、远程时钟同步和雷达测距等领域，突破了传统瑞利准则的限制，为时间延迟估计提供了新的量子增强方法。

Abstract: The Rayleigh criterion is a widely known limit in the resolution of incoherent sources with classical measurements in the spatial domain. Unsurprisingly the estimation of the time delay between two weak incoherent signals is afflicted by an analogue problem. In this work, we show the emergence of two-photon quantum beats in the frequency domain from the interference at a beam splitter of a photon emitted by a reference source and one from the two incoherent weak signals. We demonstrate, based on this phenomena, that with a relatively low number of measurements of the frequencies of the interfering photons either bunching or antibunching at the beam splitter output one can achieve a precision amounting to half of the quantum limit, independently of both the temporal shape of the photonic wavepacket and the time delay to be estimated. The feasibility of the technique makes it applicable in astronomy, microscopy, remote clocks synchronization and radar ranging

</details>


### [57] [Quantum Charging Advantage in Superconducting Solid-State Batteries](https://arxiv.org/abs/2602.08610)
*Chang-Kang Hu,Chilong Liu,Jingchao Zhao,Liuzhu Zhong,Yuxuan Zhou,Mingze Liu,Haolan Yuan,Yongchang Lin,Yue Xu,Guantian Hu,Guixu Xie,Zixing Liu,Ruiyang Zhou,Yougui Ri,Wenxuan Zhang,Ruicheng Deng,Andreia Saguia,Xiayu Linpeng,Marcelo S. Sarandy,Song Liu,Alan C. Santos,Dian Tan,Dapeng Yu*

Main category: quant-ph

TL;DR: 实验演示了固态量子电池中的量子充电优势，在超导量子处理器中实现了2-12个电池单元的集体演化，展示了无需长程多体相互作用的可扩展量子充电优势。


<details>
  <summary>Details</summary>
Motivation: 量子电池作为新型储能设备，具有超越经典系统的潜在效率和性能，对未来量子技术发展有重要意义。然而，实现可扩展的量子充电优势（QCA）需要克服实验挑战。

Method: 使用超导量子处理器实现量子电池模型，采用仅包含最近邻和成对相互作用的线性链超导transmon量子比特系统。通过双激发哈密顿量促进可扩展QCA，在2-12个电池单元规模上实现集体演化。

Result: 实验实现了显著的量子充电优势，无需长程多体相互作用。测量显示非零相干ergotropy、非相干ergotropy和纠缠，证实了量子电池充电过程的量子特性。

Conclusion: 该工作展示了固态量子电池中可扩展量子充电优势的实验实现，为开发高效且实验可行的QCA协议提供了有前景的前景，推动了量子电池技术的发展。

Abstract: Quantum battery, as a novel energy storage device, offers the potential for unprecedented efficiency and performance beyond the capabilities of classical systems, with broad implications for future quantum technologies. Here, we experimentally \RefC{demonstrate quantum charging advantage (QCA)} in a scalable solid-state quantum battery. More specifically, we show how double-excitation Hamiltonians for two-level systems promote scalable QCA \RefB{with standard methods.} We effectively implement the collective evolution of quantum systems with 2 up to 12 battery cells in a superconducting quantum processor, and study the performance of quantum charging compared to its uncorrelated classical counterpart. The model considered is a linear chain of superconducting transmon qubits with only \textit{nearest-neighbor} and \textit{pairwise} interactions, which constitute the simplest model of a multi-cell quantum battery. Our results empirically realize substantial QCA without the necessity of adopting long-range and many-body interactions \RefB{ and showcase the quantum features of the QB charging processes with measurements of non-zero coherent ergotropy, incoherent ergotropy and entanglement,} revealing a promising prospect for further developments of efficient and experimentally feasible protocols for QCA.

</details>


### [58] [Representation theory of inhomogeneous Gaussian unitaries](https://arxiv.org/abs/2602.08611)
*Jingqi Sun,Joshua Combes,Lucas Hackl*

Main category: quant-ph

TL;DR: 扩展高斯幺正算符的参数化框架，从齐次（仅二次）情况推广到非齐次情况，通过BCH公式将任意高斯幺正分解为压缩和位移变换，并推导群乘法规则


<details>
  <summary>Details</summary>
Motivation: 高斯幺正算符在量子光学和连续变量计算中至关重要，但物理实现会产生相位和符号歧义。之前的工作解决了齐次（仅二次）情况，需要扩展到更一般的非齐次情况

Method: 扩展先前框架到非齐次高斯幺正，参数化为$(M,z,Ψ)$。使用Baker-Campbell-Hausdorff公式将任意高斯幺正分解为压缩变换和位移变换，并推导群乘法规则

Result: 建立了非齐次高斯幺正算符的完整参数化框架，提供了将任意高斯幺正分解为压缩和位移变换的方法，并推导了相应的群乘法规则

Conclusion: 成功将高斯幺正算符的参数化从齐次情况扩展到非齐次情况，为量子光学和连续变量计算中的高斯幺正提供了更完整的数学描述和实现框架

Abstract: Gaussian unitaries, generated by quadratic Hamiltonians, are fundamental in quantum optics and continuous-variable computing. Their structures correspond to symplectic (bosons) and orthogonal (fermions) groups, but physical realizations give rise to their respective double covers, introducing phase and sign ambiguities. The homogeneous (quadratic-only) case has been resolved through a parameterization constructed in a recent work [arXiv:2409.11628]. We extend the previous framework to inhomogeneous Gaussian unitaries parameterized by $(M,z,Ψ)$. The Baker-Campbel-Hausdorff formula allows us then to factor any Gaussian unitary into a squeezing and a displacement transformation, from which we derive the group multiplication law.

</details>


### [59] [Weak forms offer strong regularisations: how to make physics-informed (quantum) machine learning more robust](https://arxiv.org/abs/2602.08703)
*Annie E. Paine,Smit Chaudhary,Antonio A. Gentile*

Main category: quant-ph

TL;DR: 该论文提出将局部和全局损失函数结合在物理信息方法中，以解决传统局部损失函数在泛化和边界条件传播方面的问题，特别针对量子架构展示了这种混合方法的优势。


<details>
  <summary>Details</summary>
Motivation: 物理信息方法在求解微分方程时主要使用局部损失函数，强调在采样点上满足方程，但在泛化到非采样点和传播边界条件时存在问题。传统经典微分方程求解器使用弱形式（积分方法）来施加全局条件，优先考虑平均行为而非"过拟合"特定点。本文旨在结合局部和全局损失函数的优势，提高物理信息方法的鲁棒性和准确性。

Method: 提出将局部损失函数和全局损失函数（基于弱形式的积分方法）结合在物理信息方法中，形成混合损失函数。特别针对可微分量子架构，展示了如何通过域分解技术来协调这种混合损失函数，以利用两种方法的优势并减轻各自的弱点。

Result: 在多种问题中展示了这种混合方法的有效性，特别证明了通过域分解技术协调混合损失函数相比仅使用局部策略具有显著优势。

Conclusion: 结合局部和全局损失函数的混合方法能够提高物理信息求解微分方程的鲁棒性和准确性，特别是在量子架构中，这种混合策略通过域分解技术能够显著优于仅使用局部损失函数的方法。

Abstract: Physics-informed (PI) methodologies have surged to become a pillar route to solve Differential Equations (DEs), sustained by the growth of machine learning methods in scientific contexts. The main proposition of PI is to minimise variationally a loss function, formally ensuring that a neural surrogate of the solution has the DE locally satisfied. The nature of such formulation encouraged the exploration of equivalent quantum algorithms, where the surrogate solution is expressed by variational quantum architectures. The locality of typical loss functions emphasises the DE to hold at an ensemble of points sampled in the domain, but encounters issues when generalising beyond such points, or when propagating boundary conditions. Issues which affect classical and quantum PI algorithms alike. The quest to fill this gap in robustness and accuracy against mainstream DE solvers has led to a plethora of proposals in various directions. In particular, classical DE solvers have long employed the weak form - an integral based approach aiming at imposing a global condition on the solution - prioritising a good average behaviour instead of ``overfitting'' select points. Here, we propose and explore to combine contributions from both local and global loss functions in PI routines, to exploit the advantages and mitigate the weaknesses of both. We showcase this intuition in a variety of problems focusing on differentiable quantum architectures, and demonstrating in particular how orchestrating such hybrid loss formulation with domain decomposition can offer a strong advantage over local-only strategies.

</details>


### [60] [Non-Hermitian Renormalization Group from a Few-Body Perspective](https://arxiv.org/abs/2602.08705)
*Hiroyuki Tajima,Masaya Nakagawa,Haozhao Liang,Masahito Ueda*

Main category: quant-ph

TL;DR: 该论文为非厄米重整化群方法建立了微观基础，从少体角度推导了非厄米RG方程，并将其应用于核物理，揭示了非厄米复势与量子测量之间的联系。


<details>
  <summary>Details</summary>
Motivation: 理解强相互作用系统中非厄米性的影响是一个挑战，因为非微扰强关联与非厄米性相互交织。传统的Wilsonian重整化群方法基于配分函数的存在性，这在非厄米系统中定义不明确，因此需要建立微观基础。

Method: 从少体角度出发，利用散射振幅在RG变换下的不变性，严格推导非厄米重整化群方程。在具有非弹性二体损失的非相对论二体系统中详细分析RG流结构，并将其应用于核物理中的相干中子-核散射。

Result: 成功建立了非厄米RG方法的微观基础，发现了临界半圆的出现，并显示多个核在相干中子-核散射中位于临界半圆附近。提出双中子晕核中的局域化双中子可解释为与核心核吸收相关的虚势的量子测量效应。

Conclusion: 该工作为高能物理中常用的非厄米复势提供了量子测量解释，将高能物理与原子分子光学物理中的非厄米系统联系起来，为非厄米少体物理开辟了跨学科研究平台。

Abstract: Non-Hermiticity plays a fundamental role in open quantum systems and describes a wide variety of effects of interactions with environments, including quantum measurement. However, understanding its consequences in strongly interacting systems is still elusive due to the interplay between non-perturbative strong correlations and non-Hermiticity. While the Wilsonian renormalization group (RG) method has been applied to tackle this problem, its foundation, based on the existence of the partition function, is ill-defined. In this paper, we establish a microscopic foundation of the non-Hermitian RG method from a few-body perspective. We show that the invariance of the scattering amplitude under RG transformations enables us to rigorously derive the non-Hermitian RG equation, giving a physically transparent interpretation of RG flows. We discuss a detailed structure of such RG flows in a non-relativistic two-body system with inelastic two-body loss, and show its relation to a non-Hermitian quantum scale anomaly. Our analysis suggests that non-Hermitian complex potentials often used in high-energy physics can be interpreted as being caused by quantum measurement, where the detection of elastically scattered particles updates the observer's knowledge, resulting in a nonunitary state change of the system. We apply our formalism to nuclear physics, find the emergence of a critical semicircle, and show that several nuclei are located near the critical semicircle in the coherent neutron-nucleus scattering. We also propose that the localized dineutron in two-neutron halo nuclei can be interpreted as the quantum measurement effect on the imaginary potential associated with absorption into the core nucleus. Our result bridges different contexts of non-Hermitian systems in high-energy and atomic, molecular, and optical physics, opening an interdisciplinary playground of non-Hermitian few-body physics.

</details>


### [61] [Heterogeneous Optically-Detected Spin-Acoustic Resonance in Solid-State Molecular Thin-film](https://arxiv.org/abs/2602.08772)
*Kuan-Cheng Chen,Yongqiang Wen,Xiaotian Xu,Max Attwood,Jingdong Xu,Chen Fu,Sami Ramadan,Shang Yu,Sandrine Heutz,Mark Oxborrow*

Main category: quant-ph

TL;DR: 该论文实现了在铌酸锂衬底上集成并五苯薄膜与高品质因数表面声波谐振器的自旋-声共振，展示了室温下零磁场声学驱动的相干自旋操控。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够在室温下实现零磁场自旋操控的技术，通过声学驱动替代传统的电磁驱动，为自旋器件集成提供新途径。

Method: 采用异质集成方法，将并五苯薄膜集成在铌酸锂衬底的高品质因数表面声波谐振器上，利用并五苯的光激发三重态，通过自旋-声子耦合实现声学驱动的自旋共振。

Result: 成功实现了105 MHz附近的声学寻址三重态跃迁，观测到拉比振荡，拉比频率与输入射频功率的平方根呈线性关系，证明了声学激发下的相干自旋操控。

Conclusion: 该研究建立了异质集成分子薄膜平台中的自旋-声共振，为室温下机械可寻址自旋控制和器件集成提供了定量基准和可行途径。

Abstract: We report an implementation of spin-acoustic resonance in pentacene thin films integrated on a high-quality-factor (high-Q) surface acoustic wave (SAW) resonator on a lithium niobate substrate. Heterogeneous optically detected spin-acoustic resonance (HODSAR) is an optically detected spin-resonance measurement in which the resonant drive is delivered mechanically by a surface acoustic wave (SAW). By leveraging the photo-excited triplet state of pentacene at room temperature, we demonstrate coherent spin manipulation via acoustic driving under zero externally applied magnetic field. The heterogeneously integrated device, referred to as HODSAR, utilizes spin-phonon coupling to achieve mechanically driven, zero-field spin resonance, opening avenues for room-temperature mechanically addressable spin control and device integration. We show that the high-Q multimode response of the SAW resonator enables spectrally selective acoustic addressing of triplet transitions near 105 MHz. Coherent control is evidenced by Rabi oscillations, with a Rabi frequency that increases linearly with the square root of the applied RF input power over the measured drive range, consistent with driven two-level dynamics under acoustic excitation. These results establish spin-acoustic resonance in a heterogeneously integrated molecular thin-film platform and provide a quantitative basis for benchmarking mechanically mediated spin control.

</details>


### [62] [The equivalence of quantum deletion and insertion errors on permutation-invariant codes](https://arxiv.org/abs/2602.08780)
*Lewis Bulled,Yingkai Ouyang*

Main category: quant-ph

TL;DR: 本文建立了量子插入-删除错误的等价关系，为量子同步错误的纠错提供了理论基础，解决了量子纠错领域长期存在的问题。


<details>
  <summary>Details</summary>
Motivation: 量子同步错误会改变量子系统中的量子比特数量，这类错误的经典纠错已得到充分研究，但量子版本自量子纠错诞生以来进展甚微。本文旨在解决量子插入-删除等价关系这一长期存在的问题。

Method: 在置换不变码上建立量子插入-删除等价关系，详细分析这些码能够纠正t个插入错误的条件，并将这些条件扩展到量子插入-删除错误，制定更严格的(t,s)插入-删除错误可纠正条件。

Result: 建立了量子插入-删除错误的等价关系，明确了置换不变码能够纠正t个插入错误的具体条件，并扩展到了更一般的插入-删除错误情况。

Conclusion: 本文解决了量子同步错误纠错中的许多关键问题，为量子插入-删除错误的纠错提供了理论基础，填补了该领域长期存在的空白。

Abstract: Quantum synchronisation errors are a class of quantum errors that change the number of qubits in a quantum system. The classical error correction of synchronisation errors has been well-studied, including an insertion-deletion equivalence more than a half-century ago, but little progress has been made towards the quantum counterpart since the birth of quantum error correction. We address the longstanding problem of a quantum insertion-deletion equivalence on permutation-invariant codes, detailing the conditions under which such codes are $t$-insertion error-correctable. We extend these conditions to quantum insdel errors, formulating a more restrictive set of conditions under which permutation-invariant codes are $(t,s)$-insdel error-correctable. Our work resolves many of the outstanding questions regarding the quantum error correction of synchronisation errors.

</details>


### [63] [High-Probability Heralded Entanglement via Repeated Spin-Photon Phase Encoding with Moderate Cooperativity](https://arxiv.org/abs/2602.08834)
*Yu Liu,Martin B. Plenio*

Main category: quant-ph

TL;DR: 提出一种基于重复相位编码的远程纠缠生成方案，通过单光子与自旋-腔系统的多次相互作用，在中等耦合度下实现高保真度纠缠


<details>
  <summary>Details</summary>
Motivation: 传统单次相互作用方案中，有限的耦合度限制了自旋依赖的光学响应，导致成功概率极低。需要一种能在中等耦合度下实现高保真度远程纠缠的方案

Method: 采用重复相位编码方法，让单个入射光子与自旋-腔系统进行多次往返相互作用。每次往返获得的小相位偏移相干累积，同时使用频谱宽度缩放的光子脉冲提高编码效率

Result: 即使在耦合度C~1的现实条件下，该方案也能以可观的成功概率产生高保真度纠缠态，特别适用于弱耦合的固态自旋平台

Conclusion: 该重复相位编码方案为弱耦合固态自旋平台提供了一条实现高保真度远程纠缠的途径，有望推动混合型、光子损耗容忍的分布式量子计算发展

Abstract: We propose a heralded high-probability scheme to generate remote entanglement between moderate-cooperativity spin-cavity registers with high fidelity. In conventional single-shot interfaces, limited cooperativity restricts the spin-conditional optical response and thus strongly suppresses the success probability. Our proposal instead recycles a single incident photon for repeated interactions with the spin-cavity register, such that a small spin-conditional phase shift acquired on each round trip accumulates coherently to enable remote entanglement. Moreover, the repeated scheme enables higher spin-photon encoding efficiency by using a spectral-width-scaling photon pulse with a shorter duration. We show that, for realistic imperfections and losses, this repeated phase-encoding approach produces high-fidelity entangled states with an appreciable success probability even at cooperativity $C\sim1$. Our protocol is particularly well suited to weakly coupled, cavity-based solid-state spin platforms and provides a route toward hybrid, photon-loss-tolerant distributed quantum computing.

</details>


### [64] [Spin-active chlorine-related centers in 4H-SiC with telecom-band emissions](https://arxiv.org/abs/2602.08854)
*Danial Shafizadeh,Misagh Ghezellou,Viktor M. Bobal,Lasse Vines,Jawad Ul-Hassan,Valdas Jokubavicius,Nguyen T. Son,Ivan G. Ivanov*

Main category: quant-ph

TL;DR: 该论文研究了氯离子注入4H-SiC中的缺陷，该缺陷在电信波长（1350-1540 nm）发射光，具有窄零声子线和22-25%的德拜-沃勒因子，且具有室温稳定的自旋活性，是量子网络应用的理想候选。


<details>
  <summary>Details</summary>
Motivation: 研究氯离子注入4H-SiC中缺陷的光学和自旋特性，探索其在量子信息技术中的应用潜力，特别是作为可扩展量子网络的候选材料。

Method: 采用光致发光（PL）和磁共振技术研究氯离子注入4H-SiC中的缺陷，分析其光学发射特性、零声子线、德拜-沃勒因子，并通过光检测磁共振验证自旋活性和室温稳定性。

Result: 发现氯相关中心在电信波长（1350-1540 nm）发射光，具有四个稳定配置，C波段发射的两种配置德拜-沃勒因子为22-25%，零场分裂在1.0-1.4 GHz范围内，自旋活性在室温下稳定。

Conclusion: 该氯相关中心结合了优异的光学特性和室温稳定的自旋特性，是构建可扩展量子网络的极具前景的候选材料。

Abstract: A photoluminescence (PL) and magnetic resonance investigation of a defect in chlorine-implanted 4H-SiC is presented. This Cl-related center emits light at telecom wavelengths with zero-phonon lines in the range 1350-1540 nm. Its four configurations exhibit stable PL spectra characterized by narrow zero-phonon lines. For the two configurations that emit light at the C-band, a Debye-Waller factor in the range 22-25% is estimated. Optically detected magnetic resonance confirms that the Cl-related center is spin active and stable at room temperature with the zero-field splitting in the range of 1.0-1.4 GHz. The combined optical and spin properties suggest this center to be a highly promising candidate for scalable quantum networks.

</details>


### [65] [High-brightness fiber-based Sagnac source of entangled photon pairs for multiplexed quantum networks](https://arxiv.org/abs/2602.08863)
*Tess Troisi,Yoann Pelet,Romain Dalidet,Gregory Sauder,Olivier Alibart,Sébastien Tanzilli,Anthony Martin*

Main category: quant-ph

TL;DR: 基于非线性Sagnac干涉仪的全光纤纠缠光子对源，工作在电信波长，使用标准光纤组件和PPLN波导，支持偏振和能量-时间纠缠，具有高亮度和高纠缠质量。


<details>
  <summary>Details</summary>
Motivation: 开发紧凑、坚固、可现场部署的纠缠光子源，用于未来即插即用量子通信和量子网络平台，需要实用且可扩展的构建模块。

Method: 采用基于非线性Sagnac干涉仪的全光纤架构，使用标准光纤组件和周期性极化铌酸锂波导，支持宽带自发参量下转换，实现密集波分复用。

Result: 在标准100 GHz ITU信道对上实现高归一化亮度（10.3 kpairs/s/nm/mW²），偏振和能量-时间编码的保真度、纯度和可见度超过96%，在多个波长通道上表现出高纠缠质量。

Conclusion: 所提出的Sagnac源构成了实用且可扩展的构建模块，适用于未来即插即用量子通信和量子网络平台，展示了在真实网络环境中的稳定性和可重复性。

Abstract: A fully fibered source of entangled photon pairs based on a nonlinear Sagnac interferometer is reported. Operating at telecom wavelengths, the source relies exclusively on standard fiber-optic components and periodically poled lithium niobate (PPLN) waveguides, resulting in a compact, robust, and field-deployable architecture. The generation stage supports both polarization and energy-time entanglement without modification, enabling versatile operation depending on the targeted application. Broadband spontaneous parametric down-conversion allows dense wavelength-division multiplexing over the telecom C and L bands. High normalized brightness (10.3 kpairs/s/nm/mW$^2$) is achieved on a standard 100 GHz ITU channel pair, together with high entanglement quality. Polarization and energy-time encodings are characterized through state tomography and two-photon interference measurements, yielding fidelities, purities, and visibilities exceeding 96 % over multiple wavelength channels. The stability and reproducibility of the source are further evaluated through long-duration operation in a network environment. These results demonstrate that the proposed Sagnac source constitutes a practical and scalable building block for future plug-and-play quantum communication and quantum networking platforms.

</details>


### [66] [A cavity-mediated reconfigurable coupling scheme for superconducting qubits](https://arxiv.org/abs/2602.08869)
*Shinyoung Hwang,Sangyeon Lee,Eunjong Kim*

Main category: quant-ph

TL;DR: 提出一种基于腔介导耦合的超导量子比特架构，通过可调谐的量子比特-腔耦合器实现非相邻量子比特之间的动态可重构相互作用，显著提高了量子处理器的连接灵活性。


<details>
  <summary>Details</summary>
Motivation: 超导量子比特在门保真度和相干性方面取得了显著进展，但其典型的最近邻连接性限制了复杂量子电路的实现。需要一种能够实现非相邻量子比特之间灵活互连的架构。

Method: 采用腔介导耦合架构，通过共享腔模式和可调谐的量子比特-腔耦合器，实现动态可重构的量子比特相互作用。通过选择性激活耦合器，可以在非相邻量子比特之间执行高保真度量子门操作。

Result: 在50纳秒内实现了保真度高于10^-4的iSWAP和CZ门，空闲时的残余ZZ相互作用低于几千赫兹。在四量子比特系统中，通过选择性激活耦合器实现了所有量子比特对之间的门操作，且量子比特串扰较低。

Conclusion: 该腔介导耦合架构为超导量子处理器提供了增强的相互作用灵活性，可作为选择性非局域耦合器件的实用构建模块，有望推动更复杂量子电路的实现。

Abstract: Superconducting qubits have achieved remarkable progress in gate fidelity and coherence, yet their typical nearest-neighbor connectivity presents constraints for implementing complex quantum circuits. Here, we introduce a cavity-mediated coupling architecture in which a shared cavity mode, accessed through tunable qubit-cavity couplers, enables dynamically reconfigurable interactions between non-adjacent qubits. By selectively activating the couplers, we demonstrate that high-fidelity iSWAP and CZ gates can be performed within 50 ns with simulated coherent error below $10^{-4}$, while residual $ZZ$ interaction during idling remains below a few kilohertz. Extending to a four-qubit system, we also simulate gates between every qubit pair by selectively enabling the couplers with low qubit crosstalk. This approach provides a practical route toward enhanced interaction flexibility in superconducting quantum processors and may serve as a useful building block for devices that benefit from selective non-local coupling.

</details>


### [67] [Differentiable Logical Programming for Quantum Circuit Discovery and Optimization](https://arxiv.org/abs/2602.08880)
*Antonin Sulc*

Main category: quant-ph

TL;DR: 提出一种神经符号框架，将量子电路设计重构为可微逻辑编程问题，通过连续开关优化满足逻辑公理，在QFT发现和硬件适配任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前量子电路设计方法依赖启发式固定结构或基于规则的编译器，往往不够优化或缺乏通用性，需要更系统、可优化的设计框架。

Method: 将量子门和参数化操作表示为可学习的连续"开关"值，通过梯度下降优化满足可微逻辑公理，结合连续逻辑（T-范数）和幺正演化（测地线插值），采用偏置初始化缓解贫瘠高原问题。

Result: 成功从21个候选门中发现了4-qubit量子傅里叶变换电路；在IBM Torino 133-qubit处理器上，硬件感知适配将保真度提高了59.3个百分点，并能适应硬件故障。

Conclusion: 神经符号框架为量子电路设计提供了系统、可微的优化方法，能够发现高效电路并适应实际硬件约束，展示了在量子计算中的实用价值。

Abstract: Designing high-fidelity quantum circuits remains challenging, and current paradigms often depend on heuristic, fixed-ansatz structures or rule-based compilers that can be suboptimal or lack generality. We introduce a neuro-symbolic framework that reframes quantum circuit design as a differentiable logic programming problem. Our model represents a scaffold of potential quantum gates and parameterized operations as a set of learnable, continuous ``truth values'' or ``switches,'' $s \in [0, 1]^N$. These switches are optimized via standard gradient descent to satisfy a user-defined set of differentiable, logical axioms (e.g., correctness, simplicity, robustness). We provide a theoretical formulation bridging continuous logic (via T-norms) and unitary evolution (via geodesic interpolation), while addressing the barren plateau problem through biased initialization. We illustrate the approach on tasks including discovery of a 4-qubit Quantum Fourier Transform (QFT) from a scaffold of 21 candidate gates. We also report a hardware-aware adaptation experiment on the 133-qubit IBM Torino processor, where the method improved fidelity by 59.3 percentage points in a localized routing task while adapting to hardware failures.

</details>


### [68] [Error compensation without a time penalty: robust spin-lock-induced crossing in solution NMR](https://arxiv.org/abs/2602.08883)
*Mohamed Sabba,Christian Bengs,Urvashi D. Heramun,Malcolm H. Levitt*

Main category: quant-ph

TL;DR: 提出补偿型SLIC（cSLIC）方法，用于强耦合核自旋系统的溶液NMR，通过使用两种不同射频场幅度的重复序列元素，在不增加总时长的情况下有效补偿射频场幅度偏差。


<details>
  <summary>Details</summary>
Motivation: 针对强耦合核自旋系统（包括单重态NMR和仲氢增强超极化NMR实验）中广泛使用的SLIC方法，需要改进其对射频场幅度偏差的补偿能力，同时不增加实验总时长。

Method: 提出补偿型SLIC（cSLIC）方案，采用重复序列结构，其中重复元素使用两种不同的射频场幅度，通过这种设计有效补偿射频场幅度偏差。

Result: 通过数值模拟和代表性实验验证了cSLIC方法的优越性能，证明其能有效补偿射频场偏差，同时保持与原始SLIC相同的总实验时长。

Conclusion: cSLIC方法为强耦合核自旋系统的NMR实验提供了改进的射频场补偿能力，在单重态NMR和仲氢增强超极化NMR等应用中具有重要价值。

Abstract: A modification of the widely-used spin-lock-induced crossing (SLIC) procedure is proposed for the solution nuclear magnetic resonance (NMR) of strongly coupled nuclear spin systems, including singlet NMR and parahydrogen-enhanced hyperpolarised NMR experiments. The compensated-SLIC (cSLIC) scheme uses a repetitive sequence where the repeated element employs two different radiofrequency field amplitudes. Effective compensation for deviations in the radiofrequency field amplitude is achieved without increasing the overall duration of the SLIC sequence. The advantageous properties of cSLIC are demonstrated by numerical simulations and by representative experiments.

</details>


### [69] [Multiplexed microwave resonators by frequency comb spectroscopy](https://arxiv.org/abs/2602.08890)
*Angelo Greco,Jukka-Pekka Kaikkonen,Luca Chirolli,Alberto Ronzani,Jorden Senior,Francesco Giazotto,Alessandro Crippa*

Main category: quant-ph

TL;DR: 利用SQUID产生的微波频率梳同时探测多个共面波导谐振器，实现频率复用光谱测量


<details>
  <summary>Details</summary>
Motivation: 需要一种能够同时探测多个非均匀间隔谐振频率的方法，用于电路量子电动力学中的共面波导谐振器表征

Method: 使用时间依赖磁场驱动的SQUID产生宽带微波频率梳，通过双色驱动产生互调产物，对耦合到公共传输线的谐振器进行光谱探测

Result: 频率梳方法与室温电子合成信号在谐振器品质因数估计上基本等效，能够同时寻址多个谐振器，实现频率复用

Conclusion: SQUID产生的频率梳为共面波导谐振器的光谱表征提供了有效工具，能够实现给定带宽内的有效光谱覆盖

Abstract: Coplanar waveguide resonators are central to the thriving field of circuit quantum electrodynamics. Recently, we have demonstrated the generation of a broadband microwave-frequency comb spectrum using a superconducting quantum interference device (SQUID) driven by a time-dependent magnetic field. Here, the frequency comb is used to spectroscopically probe a bank of coplanar microwave resonators, inductively coupled to a common transmission line, a standard circuit with a variety of applications. We compare the resonator line shape obtained from signals synthesized at room temperature using conventional electronics with the radiation produced in the cryogenic environment by our source, showing substantial equivalence in the estimation of the resonator quality factors. To measure non-uniformly spaced resonant frequencies, we drive the generator with a bi-chromatic tone to generate intermodulation products. Such a dense frequency comb spectrum enables simultaneous addressing of a few resonators via frequency multiplexing. Finally, we discuss the criteria for achieving effective spectroscopic coverage of a given frequency bandwidth.

</details>


### [70] [GHz-rate polarization-based QKD system for fiber and satellite applications](https://arxiv.org/abs/2602.08908)
*Matías Rubén Bolaños,Edoardo Rossi,Federico Berra,Alberto De Toni,Ilektra Karakosta-Amarantidou,Daniel Christian Lawo,Costantino Agnesi,Marco Avesani,Andrea Stanco,Francesco Vedovato,Paolo Villoresi,Giuseppe Vallone*

Main category: quant-ph

TL;DR: 该论文展示了一个1550 nm QKD系统，实现了高效BB84协议，在实验室和现场混合链路（光纤+自由空间）测试中，创造了自由空间BB84 QKD系统的新基准，在低损耗和高损耗场景下均表现出色。


<details>
  <summary>Details</summary>
Motivation: 量子密钥分发（QKD）虽然具有量子力学原理的安全优势，但在实际部署中面临传输损耗、量子信道噪声和有限密钥大小效应等挑战。解决这些问题对于QKD在光纤和卫星网络中的大规模部署至关重要。

Method: 采用1550 nm QKD系统，实现高效BB84协议，基于iPOGNAC方案。系统重复率高达1.5 GHz，固有QBER约0.4%。首先在实验室光纤链路上测试，然后在现场混合链路上测试（包括已部署光纤和620米自由空间信道），在日光条件下使用Qubit4Sync同步协议。

Result: 创造了自由空间BB84 QKD系统的新基准：持续1小时生成超过1 Mb/s的秘密密钥率（SKR）。利用新发现的有限大小边界，在低损耗（5 dB）下实现约10 Mb/s的安全密钥率，在高损耗（38.5 dB）、低块长度（N=10^4）情况下实现约6.5 kb/s的密钥率。

Conclusion: 该系统特别适用于高损耗和时间受限的场景，如低地球轨道卫星的QKD应用，展示了在实际部署环境中的强大性能，为QKD在卫星通信等领域的应用铺平了道路。

Abstract: Quantum key distribution (QKD) leverages the principles of quantum mechanics to exchange a secret key between two parties. Despite its promising features, QKD also faces several practical challenges such as transmission loss, noise in quantum channels and finite key size effects. Addressing these issues is crucial for the large-scale deployment of QKD in fiber and satellite networks.
  In this paper, we present a 1550 nm QKD system realizing the efficient-BB84 protocol and based on the iPOGNAC scheme. The system achieved repetition rates up to 1.5~GHz and showed an intrinsic QBER of $\sim 0.4\%$. The system was first tested on a laboratory fiber link and then on an intermodal link in the field, consisting of both deployed fiber and a 620 m free-space channel. The experiment was performed in daylight conditions, exploiting the Qubit4Sync synchronization protocol. With this trial, we achieved a new benchmark for free-space BB84 QKD systems by generating a sustained secret key rate (SKR) above 1~Mb/s for 1 hour. Finally, exploiting a recently discovered finite-size bound, we achieved a secure key rate of about 10 Mb/s at low losses (5 dB), and around 6.5~kb/s in the high-loss (38.5 dB), low block length ($N=10^4$) regime. The latter results demonstrate the system's suitability for highly lossy and time-constrained scenarios such as QKD from low Earth orbit satellites.

</details>


### [71] [Long distance quantum illumination and ranging using polarization entangled photon pairs in a lossy environment](https://arxiv.org/abs/2602.08947)
*Sujai Matta,Soumya Asokan,Sanchari Chakraborti,Mayank Joshi,Rahul Dalal,C. M. Chandrashekar*

Main category: quant-ph

TL;DR: 该研究利用偏振纠缠光子对，在损耗环境中实现了鲁棒的量子照明和测距方案，在近1公里自由空间传播后仍能保持强量子关联。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够在损耗环境中工作的鲁棒量子照明和测距方案，验证偏振纠缠在长距离自由空间传播后的稳定性，为可扩展的量子辅助物体检测和测距建立实践基础。

Method: 使用Sagnac干涉仪配置生成偏振纠缠光子对（CHSH参数S=2.802±0.002），将一个光子作为闲置光子保留，另一个光子通过参考路径或探测路径发送。探测光子通过损耗自由空间通道发送到远处物体，经过近1公里往返传播后收集反射光子。

Result: 即使在仅返回几十个探测光子的情况下，仍能观察到强量子关联（CHSH值S>2.6），证实了偏振纠缠在长距离自由空间传播后的鲁棒性。成功实现了千米尺度散射后偏振纠缠的恢复。

Conclusion: 该工作证明了在不同基中编码光子的鲁棒性，以及在千米尺度物体散射后恢复偏振纠缠的可能性，为可扩展的量子辅助物体检测和测距建立了实用基础。

Abstract: Using polarization entangled photon pairs, we demonstrate a robust scheme for quantum illumination and ranging in a lossy environment. Entangled photon pairs are generated in a Sagnac interferometer configuration, yielding high-visibility two-photon polarization entanglement with a measured CHSH parameter of $S =2.802\pm0.002$. One of the photons from the entangled pair is retained as idler and the other one is directed into either of the two paths, namely reference and probe, of which probe is sent toward a distant object through a lossy free-space channel, and the reflected photons are collected after round-trip free-space propagation over distances approaching $1$ km. Remarkably, strong correlations are observed with CHSH values $S >2.6$ even when only a few tens of probe photons are returned, confirming the robustness of polarization entanglement under long-distance free-space propagation. This work reports the robustness of encoding photons in different basis before it is sent towards the object and recovery of polarization entanglement even after a kilometer-scale scattering from the objects, establishing a practical foundation for scalable quantum-assisted object detection and ranging.

</details>


### [72] [Cascaded Optomechanical Sensing for Small Signals](https://arxiv.org/abs/2602.08981)
*Marta Maria Marchese,Daniel Braun,Stefan Nimmrichter,Dennis Rätzel*

Main category: quant-ph

TL;DR: 提出一种无需纠缠或量子资源的弱力探测方案，通过单向耦合光机械腔链实现海森堡极限灵敏度


<details>
  <summary>Details</summary>
Motivation: 传统量子增强传感方案依赖纠缠等非经典资源，实验实现复杂且脆弱。需要开发更稳健、实验可行的精密传感方法

Method: 利用单向激光束耦合的N个光机械腔链，通过相干平均累积相位偏移来探测作用在机械元件上的共同外力

Result: 该纯经典方案实现了通常与量子增强协议相关的灵敏度标度（海森堡极限），为精密传感提供了稳健且实验可行的途径

Conclusion: 这项工作为利用相干光-物质相互作用进行力传感开辟了新途径，在引力场测量、暗物质探测和引力波检测等领域具有应用前景

Abstract: We propose a sensing scheme for detecting weak forces that achieves Heisenberg-limited sensitivity without relying on entanglement or other non-classical resources. Our scheme utilizes coherent averaging across a chain of N optomechanical cavities, unidirectionally coupled via a laser beam. As the beam passes through the cavities, it accumulates phase shifts induced by a common external force acting on the mechanical elements. Remarkably, this fully classical approach achieves the sensitivity scaling typically associated with quantum-enhanced protocols, providing a robust and experimentally feasible route to precision sensing. Potential applications range from high-sensitivity gravitational field measurements at the Large Hadron Collider to probing dark matter interactions and detecting gravitational waves. This work opens a new pathway for leveraging coherent light-matter interactions for force sensing.

</details>


### [73] [Hybrid Method of Efficient Simulation of Physics Applications for a Quantum Computer](https://arxiv.org/abs/2602.09020)
*Carla Rieger,Albert T. Schmitz,Gehad Salem,Massimiliano Incudini,Sofia Vallecorsa,Anne Y. Matsuura,Michele Grossi,Gian Giacomo Guerreschi*

Main category: quant-ph

TL;DR: 提出一种结合全状态模拟器和Clifford模拟器的混合模拟方法，专门用于高效模拟量子化学哈密顿量时间演化中的多量子比特旋转操作，在24量子比特化学哈密顿量上实现了约18倍（MPI下约22倍）的加速。


<details>
  <summary>Details</summary>
Motivation: 量子化学和材料科学是展示量子算法优势和实用性的重要领域，但需要大规模量子电路模拟来确定量子方法超越经典方法的临界问题规模。当前模拟量子化学哈密顿量时间演化面临计算挑战，特别是多量子比特旋转操作的计算成本较高。

Method: 提出混合模拟方法，结合全状态模拟器和Clifford模拟器，专注于高效模拟多量子比特旋转操作。通过利用Pauli框架优化多量子比特操作的表示和执行，显著降低量子电路模拟的计算成本。该方法已集成到Intel量子SDK中。

Result: 在24量子比特的化学哈密顿量评估中，该方法实现了约18倍的加速（使用MPI时约22倍）。该方法不仅对化学应用有重要影响，对任何依赖多量子比特旋转的计算任务都有广泛意义。

Conclusion: 该混合模拟方法通过优化多量子比特旋转操作的模拟，显著提高了量子电路模拟的效率，为复杂量子系统的研究提供了更准确和成本效益更高的工具，并进一步缩小了理论算法开发与实际量子软件实现之间的差距。

Abstract: Quantum chemistry and materials science are among the most promising areas for demonstrating algorithmic quantum advantage and quantum utility due to their inherent quantum mechanical nature. Still, large-scale simulations of quantum circuits are essential for determining the problem size at which quantum solutions outperform classical methods. In this work, we present a novel hybrid simulation approach, forming a hybrid of a fullstate and a Clifford simulator, specifically designed to address the computational challenges associated with the time evolution of quantum chemistry Hamiltonians. Our method focuses on the efficient emulation of multi-qubit rotations, a critical component of Trotterized Hamiltonian evolution. By optimizing the representation and execution of multi-qubit operations leveraging the Pauli frame, our approach significantly reduces the computational cost of simulating quantum circuits, enabling more efficient simulations. Beyond its impact on chemistry applications, our emulation strategy has broad implications for any computational workload that relies heavily on multi-qubit rotations. By increasing the efficiency of quantum simulations, our method facilitates more accurate and cost-effective studies of complex quantum systems. We quantify the performance improvements and computational savings for this emulation strategy, and we obtain a speedup of a factor $\approx 18$ ($\approx 22$ with MPI) for our evaluated chemistry Hamiltonians with 24 qubits. Thus, we evaluate our integration of this emulation strategy into the Intel Quantum SDK, further bridging the gap between theoretical algorithm development and practical quantum software implementations.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [74] [Attractor Patch Networks: Reducing Catastrophic Forgetting with Routed Low-Rank Patch Experts](https://arxiv.org/abs/2602.06993)
*Shashank*

Main category: cs.LG

TL;DR: 提出Attractor Patch Networks (APN)作为Transformer FFN的替代方案，通过相似性路由选择top-k专家补丁，实现条件化、上下文特化的非线性变换，在保持竞争力的语言建模性能的同时，显著提升持续学习能力。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer的FFN存在两个问题：1) 对所有token使用相同计算量，不考虑上下文结构；2) 持续学习时全局共享权重更新会导致干扰。需要一种能够根据上下文动态分配计算资源并减少干扰的架构。

Method: 提出APN架构：包含专家补丁库，通过相似性路由匹配token表示与学习原型，选择top-k补丁，每个补丁基于紧凑代码生成低秩残差更新，形成条件化、上下文特化的非线性变换。

Result: 在字符级语言建模中，APN达到竞争性困惑度(4.57 vs 4.32 PPL)，同时在持续适应方面表现显著更好：适应新领域时，APN在原始领域保持2.6倍更好(11.1 vs 29.4 PPL)，在新领域适应2.8倍更好(6.4 vs 17.8 PPL)。

Conclusion: APN作为架构原语，既能保持Transformer接口兼容性，又能实现条件化计算分配，显著提升持续学习性能，为解决FFN的密集计算和持续学习干扰问题提供了有效方案。

Abstract: Transformers achieve strong language modeling accuracy, yet their position-wise feed-forward networks (FFNs) are dense, globally shared, and typically updated end to end. These properties create two practical tensions. First, dense FFNs spend the same compute on every token regardless of context, and they allocate capacity uniformly even when language exhibits highly clustered context structure. Second, continual learning, in the sense of updating the model while serving a data stream, often produces interference because a small update touches broadly shared weights.
  We propose Attractor Patch Networks (APN), a plug-compatible replacement for the Transformer FFN. APN is a bank of patch experts. A similarity router selects a small top-k set of patches for each token by matching the token representation to learned prototypes. Each selected patch emits a low-rank residual update conditioned on a compact code. The architecture yields conditional, context-specialized nonlinear transformations while preserving the standard Transformer interface.
  This paper focuses on APN as an architectural primitive. We formalize APN, analyze its expressivity as a piecewise low-rank residual function class, and derive simple interference and stability arguments that make APN naturally compatible with continual learning. In experiments on character-level language modeling, APN achieves competitive perplexity (4.57 vs 4.32 PPL) while enabling dramatically better continual adaptation: when adapting to a shifted domain, APN achieves 2.6 times better retention (11.1 vs 29.4 PPL on the original domain) and 2.8 times better adaptation (6.4 vs 17.8 PPL on the new domain) compared to global fine-tuning of a dense FFN baseline.

</details>


### [75] [Neural Sabermetrics with World Model: Play-by-play Predictive Modeling with Large Language Model](https://arxiv.org/abs/2602.07030)
*Young Jin Ahn,Yiyang Du,Zheyuan Zhang,Haisen Kang*

Main category: cs.LG

TL;DR: 该研究提出了Neural Sabermetrics with World Model，一个基于大语言模型的棒球逐球世界模型，能够预测比赛的多方面演化，在预测下一球和击球手挥棒决策方面优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统棒球统计方法虽然能总结历史数据，但无法建立逐球生成模型，现有方法大多局限于单步预测或事后分析，缺乏对比赛动态演化的建模能力。

Method: 将棒球比赛建模为长自回归事件序列，在超过10年MLB追踪数据（700万次投球序列，约30亿token）上持续预训练单一LLM，构建统一的逐球世界模型。

Result: 模型在分布内常规赛和分布外季后赛数据上均表现优异：正确预测约64%的下一投球，78%的击球手挥棒决策，优于现有神经基线。

Conclusion: LLM可以作为有效的体育世界模型，统一框架能够预测比赛多方面演化，为棒球分析提供了新的生成建模方法。

Abstract: Classical sabermetrics has profoundly shaped baseball analytics by summarizing long histories of play into compact statistics. While these metrics are invaluable for valuation and retrospective analysis, they do not define a generative model of how baseball games unfold pitch by pitch, leaving most existing approaches limited to single-step prediction or post-hoc analysis. In this work, we present Neural Sabermetrics with World Model, a Large Language Model (LLM) based play-by-play world model for baseball. We cast baseball games as long auto-regressive sequences of events and continuously pretrain a single LLM on more than ten years of Major League Baseball (MLB) tracking data, comprising over seven million pitch sequences and approximately three billion tokens. The resulting model is capable of predicting multiple aspects of game evolution within a unified framework. We evaluate our model on both in-distribution regular-season data and out-of-distribution postseason games and compare against strong neural baselines from prior work. Despite using a single backbone model, our approach outperforms the performance of existing baselines, (1) correctly predicting approximately 64% of next pitches within a plate appearance and (2) 78% of batter swing decisions, suggesting that LLMs can serve as effective world models for sports.

</details>


### [76] [Lagged backward-compatible physics-informed neural networks for unsaturated soil consolidation analysis](https://arxiv.org/abs/2602.07031)
*Dong Li,Shuai Huang,Yapeng Cao,Yujun Cui,Xiaobin Wei,Hongtao Cao*

Main category: cs.LG

TL;DR: 提出LBC-PINN方法，用于模拟和反演长期荷载下一维非饱和土固结问题，通过时间分段和滞后兼容性损失解决多时间尺度耦合压力消散挑战。


<details>
  <summary>Details</summary>
Motivation: 非饱和土固结涉及空气和水压力的耦合消散，存在多时间尺度问题，传统方法难以有效处理长期加载下的模拟和反演。

Method: 开发Lagged Backward-Compatible Physics-Informed Neural Network (LBC-PINN)，集成对数时间分段、滞后兼容性损失强制和分段迁移学习，采用基于特征空气相消散时间的简化分段策略。

Result: LBC-PINN能准确预测孔隙空气和孔隙水压力演化，与有限元结果对比平均绝对误差低于1e-2（时间达1e10秒），对渗透率比1e-3到1e3范围具有鲁棒性，简化分段策略保持精度同时提高计算效率。

Conclusion: LBC-PINN为长期荷载下非饱和土固结模拟和反演提供了有效框架，解决了多时间尺度耦合问题，具有高精度和计算效率。

Abstract: This study develops a Lagged Backward-Compatible Physics-Informed Neural Network (LBC-PINN) for simulating and inverting one-dimensional unsaturated soil consolidation under long-term loading. To address the challenges of coupled air and water pressure dissipation across multi-scale time domains, the framework integrates logarithmic time segmentation, lagged compatibility loss enforcement, and segment-wise transfer learning.
  In forward analysis, the LBC-PINN with recommended segmentation schemes accurately predicts pore air and pore water pressure evolution. Model predictions are validated against finite element method (FEM) results, with mean absolute errors below 1e-2 for time durations up to 1e10 seconds. A simplified segmentation strategy based on the characteristic air-phase dissipation time improves computational efficiency while preserving predictive accuracy. Sensitivity analyses confirm the robustness of the framework across air-to-water permeability ratios ranging from 1e-3 to 1e3.

</details>


### [77] [TransConv-DDPM: Enhanced Diffusion Model for Generating Time-Series Data in Healthcare](https://arxiv.org/abs/2602.07033)
*Md Shahriar Kabir,Sana Alamgeer,Minakshi Debnath,Anne H. H. Ngu*

Main category: cs.LG

TL;DR: TransConv-DDPM：一种用于生成生物力学和生理时间序列数据的增强型生成AI方法，结合DDPM、U-Net、多尺度卷积和Transformer层，在多个数据集上表现优于现有方法，并能提升预测模型性能。


<details>
  <summary>Details</summary>
Motivation: 临床领域缺乏真实世界数据阻碍了AI模型训练，生成AI在计算机视觉和NLP领域已显示潜力，但生理时间序列数据生成面临独特挑战，因其具有复杂性和变异性。

Method: 提出TransConv-DDPM方法，采用去噪扩散概率模型（DDPM）结合U-Net架构、多尺度卷积模块和Transformer层，以捕捉全局和局部时间依赖性。

Result: 在三个不同数据集上评估，与TimeGAN和Diffusion-TS等先进方法相比，在SmartFallMM和EEG数据集上表现优异，能有效捕捉数据点间渐变的时间模式。在SmartFallMM数据集上，添加合成数据使预测模型的F1分数提升13.64%，整体准确率提高14.93%。

Conclusion: TransConv-DDPM能够生成高质量的合成生理时间序列数据，具有实际应用潜力，可解决临床领域数据稀缺问题并提升AI模型性能。

Abstract: The lack of real-world data in clinical fields poses a major obstacle in training effective AI models for diagnostic and preventive tools in medicine. Generative AI has shown promise in increasing data volume and enhancing model training, particularly in computer vision and natural language processing (NLP) domains. However, generating physiological time-series data, a common type in medical AI applications, presents unique challenges due to its inherent complexity and variability. This paper introduces TransConv-DDPM, an enhanced generative AI method for biomechanical and physiological time-series data generation. The model employs a denoising diffusion probabilistic model (DDPM) with U-Net, multi-scale convolution modules, and a transformer layer to capture both global and local temporal dependencies. We evaluated TransConv-DDPM on three diverse datasets, generating both long and short-sequence time-series data. Quantitative comparisons against state-of-the-art methods, TimeGAN and Diffusion-TS, using four performance metrics, demonstrated promising results, particularly on the SmartFallMM and EEG datasets, where it effectively captured the more gradual temporal change patterns between data points. Additionally, a utility test on the SmartFallMM dataset revealed that adding synthetic fall data generated by TransConv-DDPM improved predictive model performance, showing a 13.64% improvement in F1-score and a 14.93% increase in overall accuracy compared to the baseline model trained solely on fall data from the SmartFallMM dataset. These findings highlight the potential of TransConv-DDPM to generate high-quality synthetic data for real-world applications.

</details>


### [78] [AVERE: Improving Audiovisual Emotion Reasoning with Preference Optimization](https://arxiv.org/abs/2602.07054)
*Ashutosh Chaubey,Jiacheng Pang,Maksim Siniukov,Mohammad Soleymani*

Main category: cs.LG

TL;DR: 本文提出EmoReAlM基准测试来评估多模态大语言模型在情感理解中的虚假关联和幻觉问题，并开发AVEm-DPO偏好优化方法提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在情感理解任务中存在两个关键挑战：1）情感与无关视听线索之间的虚假关联；2）语言模型主干中的文本先验驱动的视听线索幻觉。需要量化这些问题并开发解决方案。

Method: 提出EmoReAlM基准测试来评估线索-情感关联、幻觉和模态一致性；开发AVEm-DPO偏好优化技术，通过构建对虚假关联/幻觉响应的偏好对，并加入惩罚文本先验依赖的正则化项来对齐模型响应。

Result: 在DFEW、RAVDESS和EMER数据集上的实验表明，该方法显著提升了参考基线模型的性能，在零样本设置下获得了6-19%的相对性能提升。

Conclusion: 通过提供严谨的基准测试和鲁棒的优化框架，这项工作为情感理解和社会AI的多模态大语言模型提供了原则性评估和改进方法。

Abstract: Emotion understanding is essential for building socially intelligent agents. Although recent multimodal large language models have shown strong performance on this task, two key challenges remain - spurious associations between emotions and irrelevant audiovisual cues, and hallucinations of audiovisual cues driven by text priors in the language model backbone. To quantify and understand these issues, we introduce EmoReAlM, a benchmark designed to evaluate MLLMs for cue-emotion associations, hallucinations and modality agreement. We then propose AVEm-DPO, a preference optimization technique that aligns model responses with both audiovisual inputs and emotion-centric queries. Specifically, we construct preferences over responses exhibiting spurious associations or hallucinations, and audiovisual input pairs guided by textual prompts. We also include a regularization term that penalizes reliance on text priors, thereby mitigating modality-specific cue hallucinations. Experimental results on DFEW, RAVDESS and EMER demonstrate that our method significantly improves the performance of the reference baseline models with 6-19% of relative performance gains in zero-shot settings. By providing both a rigorous benchmark and a robust optimization framework, this work enables principled evaluation and improvement of MLLMs for emotion understanding and social AI. Code, models and benchmark will be released at https://avere-iclr.github.io.

</details>


### [79] [TACIT: Transformation-Aware Capturing of Implicit Thought](https://arxiv.org/abs/2602.07061)
*Daniel Nobrega*

Main category: cs.LG

TL;DR: TACIT是一个基于扩散的transformer模型，用于可解释的视觉推理，通过像素空间的rectified flow直接可视化推理过程，在迷宫求解任务中表现出"顿悟时刻"现象。


<details>
  <summary>Details</summary>
Motivation: 开发一个完全在像素空间操作的视觉推理系统，能够直接可视化推理过程，以理解神经网络如何发展在语言之前的隐式推理策略。

Method: 使用基于扩散的transformer架构，采用rectified flow在像素空间操作，通过噪声自由流匹配将未解决的迷宫图像转换为解决方案。

Result: 在100万个合成迷宫对上：训练损失减少192倍，L2距离改善22.7倍，仅需10个Euler步骤（典型扩散模型需要100-1000步）。观察到明显的相变现象：解决方案在68%的转换过程中不可见，然后在t=0.70时在2%的过程中突然出现，所有空间区域同时涌现。

Conclusion: TACIT展示了神经网络发展隐式推理策略的能力，其"顿悟时刻"模式（长期潜伏后突然结晶）与人类认知中的洞察现象相似，为理解语言之前的推理过程提供了基础。

Abstract: We present TACIT (Transformation-Aware Capturing of Implicit Thought), a diffusion-based transformer for interpretable visual reasoning. Unlike language-based reasoning systems, TACIT operates entirely in pixel space using rectified flow, enabling direct visualization of the reasoning process at each inference step. We demonstrate the approach on maze-solving, where the model learns to transform images of unsolved mazes into solutions. Key results on 1 million synthetic maze pairs include:
  - 192x reduction in training loss over 100 epochs
  - 22.7x improvement in L2 distance to ground truth
  - Only 10 Euler steps required (vs. 100-1000 for typical diffusion models)
  Quantitative analysis reveals a striking phase transition phenomenon: the solution remains invisible for 68% of the transformation (zero recall), then emerges abruptly at t=0.70 within just 2% of the process. Most remarkably, 100% of samples exhibit simultaneous emergence across all spatial regions, ruling out sequential path construction and providing evidence for holistic rather than algorithmic reasoning. This "eureka moment" pattern -- long incubation followed by sudden crystallization -- parallels insight phenomena in human cognition. The pixel-space design with noise-free flow matching provides a foundation for understanding how neural networks develop implicit reasoning strategies that operate below and before language.

</details>


### [80] [Video-based Music Generation](https://arxiv.org/abs/2602.07063)
*Serkan Sulun*

Main category: cs.LG

TL;DR: EMSYNC是一个快速、免费、自动化的视频配乐生成系统，通过情感分类、情感音乐生成和时间同步技术，为视频创建情感和节奏同步的音乐。


<details>
  <summary>Details</summary>
Motivation: 随着网络视频内容快速增长，寻找合适的配乐成为重要挑战。内容创作者需要无需作曲或授权即可增强视频制作的解决方案。

Method: 1) 新颖的视频情感分类器，使用预训练深度神经网络提取特征，仅训练融合层；2) 基于连续情感值而非离散类别的MIDI生成器；3) 时间边界条件方法（边界偏移编码），将音乐和弦与场景变化对齐。

Result: 在Ekman-6和MovieNet数据集上获得最先进结果；用户研究表明在音乐丰富度、情感对齐、时间同步和整体偏好方面优于现有方法。

Conclusion: EMSYNC作为全自动视频音乐生成器，在视频配乐生成领域设定了新的最先进标准，为内容创作者提供了高效的音乐创作解决方案。

Abstract: As the volume of video content on the internet grows rapidly, finding a suitable soundtrack remains a significant challenge. This thesis presents EMSYNC (EMotion and SYNChronization), a fast, free, and automatic solution that generates music tailored to the input video, enabling content creators to enhance their productions without composing or licensing music. Our model creates music that is emotionally and rhythmically synchronized with the video. A core component of EMSYNC is a novel video emotion classifier. By leveraging pretrained deep neural networks for feature extraction and keeping them frozen while training only fusion layers, we reduce computational complexity while improving accuracy. We show the generalization abilities of our method by obtaining state-of-the-art results on Ekman-6 and MovieNet. Another key contribution is a large-scale, emotion-labeled MIDI dataset for affective music generation. We then present an emotion-based MIDI generator, the first to condition on continuous emotional values rather than discrete categories, enabling nuanced music generation aligned with complex emotional content. To enhance temporal synchronization, we introduce a novel temporal boundary conditioning method, called "boundary offset encodings," aligning musical chords with scene changes. Combining video emotion classification, emotion-based music generation, and temporal boundary conditioning, EMSYNC emerges as a fully automatic video-based music generator. User studies show that it consistently outperforms existing methods in terms of music richness, emotional alignment, temporal synchronization, and overall preference, setting a new state-of-the-art in video-based music generation.

</details>


### [81] [Hybrid Dual-Path Linear Transformations for Efficient Transformer Architectures](https://arxiv.org/abs/2602.07070)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: 论文提出HDPL算子，将Transformer中的密集线性变换分解为稀疏块对角局部处理和低秩VAE瓶颈全局正则化两条路径，在减少6.8%参数的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer架构依赖密集线性变换，将特征投影视为单一的全秩操作，这种方法效率低下且缺乏区分局部特征保留和全局上下文整合的结构归纳偏置。

Method: 提出混合双路径线性（HDPL）算子，将仿射变换分解为两个拓扑不同的路径：用于高秩局部处理的稀疏块对角组件，以及用于全局上下文正则化的低秩变分自编码器（VAE）瓶颈。通过"外科手术式"地替换特定投影（Query、Key、Value、Gate、Up）为HDPL算子，同时保留标准密集层用于聚合（Output、Down）。

Result: 在FineWeb-Edu数据集上的实验表明，HDPL架构优于标准的Llama风格基线，在减少6.8%参数的同时降低了验证损失。

Conclusion: HDPL在效率和表示能力之间实现了更好的平衡。更重要的是，在Transformer骨干中显式实现概率潜在空间为推理时控制、超网络诱导控制、持续适应、可解释性以及跨模型/跨模态同步提供了新的架构可能性。

Abstract: Standard Transformer architectures rely heavily on dense linear transformations, treating feature projection as a monolithic, full-rank operation. We argue that this formulation is inefficient and lacks the structural inductive bias necessary for distinguishing between local feature preservation and global context integration. To address this, we introduce the Hybrid Dual-Path Linear (HDPL) operator, which decomposes the affine transformation into two topologically distinct pathways: a sparse block-diagonal component for high-rank local processing, and a low-rank Variational Autoencoder (VAE) bottleneck for global context regularization. By "surgically" replacing specific projections (Query, Key, Value, Gate, Up) with HDPL operators while retaining standard dense layers for aggregation (Output, Down), we achieve a superior balance of efficiency and representational power. Experiments on the FineWeb-Edu dataset demonstrate that the HDPL architecture outperforms a standard Llama-style baseline, reducing validation loss while simultaneously reducing parameter count by 6.8%. Beyond immediate performance gains, we discuss how the explicit materialization of a probabilistic latent space within the Transformer backbone serves as a vital architectural affordance, offering new pathways for inference-time or hypernetwork induced control, continual adaptation, interpretability, and cross-model or cross-modal synchronization. The code is available at https://github.com/VladimerKhasia/HDPL

</details>


### [82] [Systematic Performance Assessment of Deep Material Networks for Multiscale Material Modeling](https://arxiv.org/abs/2602.07192)
*Xiaolong He,Haoyan Wei,Wei Hu,Henan Mao,C. T. Wu*

Main category: cs.LG

TL;DR: 本文对深度材料网络(DMNs)进行了全面的性能评估，比较了预测精度、计算效率和训练鲁棒性，发现旋转无关的交互式材料网络(IMN)在保持预测精度的同时实现了3.4-4.7倍的训练加速。


<details>
  <summary>Details</summary>
Motivation: 深度材料网络(DMNs)作为结构保持的机制性机器学习模型，在复杂微结构的多尺度建模中具有加速潜力。尽管应用日益广泛，但对其离线-在线全流程性能的系统评估仍然有限，需要明确训练选择对在线泛化性能的影响。

Method: 对DMNs进行全面的比较评估，研究离线训练选择（包括初始化、批量大小、训练数据量和激活正则化）对在线泛化性能和不确定性的影响。同时对比原始DMN与旋转无关的交互式材料网络(IMN)的性能差异。

Result: 预测误差和方差随训练数据量增加而减小；初始化和批量大小显著影响模型性能；激活正则化对控制网络复杂度和泛化性能起关键作用；IMN相比原始DMN实现了3.4-4.7倍的离线训练加速，同时保持相当的在线预测精度和计算效率。

Conclusion: 研究阐明了结构保持材料网络中模型表达能力和效率之间的关键权衡，为多尺度材料建模中的实际部署提供了实用指导，表明IMN在保持预测精度的同时显著提升了训练效率。

Abstract: Deep Material Networks (DMNs) are structure-preserving, mechanistic machine learning models that embed micromechanical principles into their architectures, enabling strong extrapolation capabilities and significant potential to accelerate multiscale modeling of complex microstructures. A key advantage of these models is that they can be trained exclusively on linear elastic data and then generalized to nonlinear inelastic regimes during online prediction. Despite their growing adoption, systematic evaluations of their performance across the full offline-online pipeline remain limited. This work presents a comprehensive comparative assessment of DMNs with respect to prediction accuracy, computational efficiency, and training robustness. We investigate the effects of offline training choices, including initialization, batch size, training data size, and activation regularization on online generalization performance and uncertainty. The results demonstrate that both prediction error and variance decrease with increasing training data size, while initialization and batch size can significantly influence model performance. Moreover, activation regularization is shown to play a critical role in controlling network complexity and therefore generalization performance. Compared with the original DMN, the rotation-free Interaction-based Material Network (IMN) formulation achieves a 3.4x - 4.7x speed-up in offline training, while maintaining comparable online prediction accuracy and computational efficiency. These findings clarify key trade-offs between model expressivity and efficiency in structure-preserving material networks and provide practical guidance for their deployment in multiscale material modeling.

</details>


### [83] [The Optimal Token Baseline: Variance Reduction for Long-Horizon LLM-RL](https://arxiv.org/abs/2602.07078)
*Yingru Li,Jiawei Xu,Ziniu Li,Jiacai Liu,Wei Liu,Yuxuan Tong,Longtao Zheng,Zhenghai Xue,Yaxiang Zhang,Tianle Cai,Ge Zhang,Qian Liu,Baoxiang Wang*

Main category: cs.LG

TL;DR: 提出Optimal Token Baseline (OTB)方法，通过基于梯度范数的逆加权更新解决LLM强化学习中的训练崩溃问题，显著减少token消耗


<details>
  <summary>Details</summary>
Motivation: 大型语言模型强化学习在长时程任务中常因梯度方差爆炸导致训练崩溃，传统基线方法存在优化困难、忽略序列异质性等问题

Method: 从第一性原理推导出Optimal Token Baseline (OTB)，证明梯度更新应按其累积梯度范数的倒数加权；提出Logit-Gradient Proxy仅使用前向传播概率近似梯度范数

Result: 方法实现训练稳定性，仅用N=4就能达到N=32大组大小的性能，在单轮和工具集成推理任务中减少超过65%的token消耗

Conclusion: OTB方法有效解决了LLM强化学习中的梯度方差问题，显著提高了训练效率和稳定性

Abstract: Reinforcement Learning (RL) for Large Language Models (LLMs) often suffers from training collapse in long-horizon tasks due to exploding gradient variance. To mitigate this, a baseline is commonly introduced for advantage computation; however, traditional value models remain difficult to optimize, and standard group-based baselines overlook sequence heterogeneity. Although classic optimal baseline theory can achieve global variance reduction, it neglects token heterogeneity and requires prohibitive gradient-based computation. In this work, we derive the Optimal Token Baseline (OTB) from first principles, proving that gradient updates should be weighted inversely to their cumulative gradient norm. To ensure efficiency, we propose the Logit-Gradient Proxy that approximates the gradient norm using only forward-pass probabilities. Our method achieves training stability and matches the performance of large group sizes ($N=32$) with only $N=4$, reducing token consumption by over 65% across single-turn and tool-integrated reasoning tasks.

</details>


### [84] [Attention-Driven Framework for Non-Rigid Medical Image Registration](https://arxiv.org/abs/2602.07088)
*Muhammad Zafar Iqbal,Ghazanfar Farooq Siddiqui,Anwar Ul Haq,Imran Razzak*

Main category: cs.LG

TL;DR: 提出AD-RegNet注意力驱动框架用于医学图像非刚性配准，结合3D UNet与双向交叉注意力机制，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在医学图像配准方面取得进展，但准确对齐大变形图像同时保持解剖合理性仍然具有挑战性。需要一种能够处理复杂变形并保持解剖结构一致性的方法。

Method: 提出AD-RegNet框架，采用3D UNet作为骨干网络，结合双向交叉注意力机制在多尺度上建立移动图像与固定图像之间的对应关系。引入区域自适应注意力机制聚焦解剖相关结构，以及多分辨率变形场合成方法实现精确对齐。

Result: 在DIRLab（胸部4D CT）和IXI（脑部MRI）两个数据集上评估，性能与最先进方法相当。使用NCC、MSE、SSIM、Jacobian行列式和TRE等指标综合评估，显示注意力引导的配准提高了对齐精度，同时确保了解剖合理的变形。

Conclusion: AD-RegNet在配准精度和计算效率之间保持了良好平衡，适用于临床应用。注意力机制有效指导配准过程，提高了大变形情况下的对齐准确性。

Abstract: Deformable medical image registration is a fundamental task in medical image analysis with applications in disease diagnosis, treatment planning, and image-guided interventions. Despite significant advances in deep learning based registration methods, accurately aligning images with large deformations while preserving anatomical plausibility remains a challenging task. In this paper, we propose a novel Attention-Driven Framework for Non-Rigid Medical Image Registration (AD-RegNet) that employs attention mechanisms to guide the registration process. Our approach combines a 3D UNet backbone with bidirectional cross-attention, which establishes correspondences between moving and fixed images at multiple scales. We introduce a regional adaptive attention mechanism that focuses on anatomically relevant structures, along with a multi-resolution deformation field synthesis approach for accurate alignment. The method is evaluated on two distinct datasets: DIRLab for thoracic 4D CT scans and IXI for brain MRI scans, demonstrating its versatility across different anatomical structures and imaging modalities. Experimental results demonstrate that our approach achieves performance competitive with state-of-the-art methods on the IXI and DIRLab datasets. The proposed method maintains a favorable balance between registration accuracy and computational efficiency, making it suitable for clinical applications. A comprehensive evaluation using normalized cross-correlation (NCC), mean squared error (MSE), structural similarity (SSIM), Jacobian determinant, and target registration error (TRE) indicates that attention-guided registration improves alignment accuracy while ensuring anatomically plausible deformations.

</details>


### [85] [Finding Connections: Membership Inference Attacks for the Multi-Table Synthetic Data Setting](https://arxiv.org/abs/2602.07126)
*Joshua Ward,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: 提出了一种针对合成关系数据的多表成员推理攻击（MT-MIA），用于审计用户级隐私泄露，相比单表攻击能更准确地评估跨表关系的隐私风险。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据大多存在于关系数据库中，用户信息分布在多个相互关联的表中。现有的合成关系数据生成方法虽然能处理这种复杂性，但数据发布仍面临独特的隐私挑战，因为信息不仅可能从单个项目泄露，还可能通过构成完整用户实体的关系泄露。

Method: 提出了多表成员推理攻击（MT-MIA），在无盒威胁模型下，通过异构图神经网络（HGNN）针对用户实体的学习表示进行攻击。该方法整合用户的所有连接项目，更好地针对由表间关系引起的用户级漏洞。

Result: 评估显示，MT-MIA在多个真实世界多表数据集上有效，这种漏洞存在于最先进的关系合成数据生成器中。单表MIA在项目级别审计会低估用户级隐私泄露，而MT-MIA能更准确地揭示跨表关系的隐私风险。

Conclusion: 需要新的隐私审计方法来评估合成关系数据的用户级隐私风险，MT-MIA为此提供了一个有效的框架，揭示了现有方法在保护跨表关系隐私方面的不足。

Abstract: Synthetic tabular data has gained attention for enabling privacy-preserving data sharing. While substantial progress has been made in single-table synthetic generation where data are modeled at the row or item level, most real-world data exists in relational databases where a user's information spans items across multiple interconnected tables. Recent advances in synthetic relational data generation have emerged to address this complexity, yet release of these data introduce unique privacy challenges as information can be leaked not only from individual items but also through the relationships that comprise a complete user entity.
  To address this, we propose a novel Membership Inference Attack (MIA) setting to audit the empirical user-level privacy of synthetic relational data and show that single-table MIAs that audit at an item level underestimate user-level privacy leakage. We then propose Multi-Table Membership Inference Attack (MT-MIA), a novel adversarial attack under a No-Box threat model that targets learned representations of user entities via Heterogeneous Graph Neural Networks. By incorporating all connected items for a user, MT-MIA better targets user-level vulnerabilities induced by inter-tabular relationships than existing attacks. We evaluate MT-MIA on a range of real-world multi-table datasets and demonstrate that this vulnerability exists in state-of-the-art relational synthetic data generators, employing MT-MIA to additionally study where this leakage occurs.

</details>


### [86] [Landscaper: Understanding Loss Landscapes Through Multi-Dimensional Topological Analysis](https://arxiv.org/abs/2602.07135)
*Jiaqing Chen,Nicholas Hadler,Tiankai Xie,Rostyslav Hnatyshyn,Caleb Geniesse,Yaoqing Yang,Michael W. Mahoney,Talita Perciano,John F. Hartwig,Ross Maciejewski,Gunther H. Weber*

Main category: cs.LG

TL;DR: Landscaper是一个用于任意维度损失景观分析的Python包，结合Hessian子空间构建和拓扑数据分析，通过SMAD指标量化景观平滑度，揭示传统方法遗漏的复杂拓扑特征。


<details>
  <summary>Details</summary>
Motivation: 传统低维损失景观分析常常遗漏复杂的拓扑特征，需要开发更强大的工具来深入理解神经网络优化和泛化过程，特别是在科学机器学习等数据稀缺场景中。

Method: 开发了Landscaper开源Python包，结合Hessian-based子空间构建和拓扑数据分析方法，引入Saddle-Minimum Average Distance（SMAD）指标来量化损失景观的平滑度。

Result: Landscaper在各种架构和任务中有效，包括预训练语言模型，SMAD能够捕捉传统指标遗漏的训练转换（如景观简化），在化学性质预测任务中可作为分布外泛化的度量指标。

Conclusion: Landscaper为模型诊断和架构设计提供了有价值的洞察，特别是在数据稀缺的科学机器学习场景中，能够揭示损失景观的复杂几何结构。

Abstract: Loss landscapes are a powerful tool for understanding neural network optimization and generalization, yet traditional low-dimensional analyses often miss complex topological features. We present Landscaper, an open-source Python package for arbitrary-dimensional loss landscape analysis. Landscaper combines Hessian-based subspace construction with topological data analysis to reveal geometric structures such as basin hierarchy and connectivity. A key component is the Saddle-Minimum Average Distance (SMAD) for quantifying landscape smoothness. We demonstrate Landscaper's effectiveness across various architectures and tasks, including those involving pre-trained language models, showing that SMAD captures training transitions, such as landscape simplification, that conventional metrics miss. We also illustrate Landscaper's performance in challenging chemical property prediction tasks, where SMAD can serve as a metric for out-of-distribution generalization, offering valuable insights for model diagnostics and architecture design in data-scarce scientific machine learning scenarios.

</details>


### [87] [Featured Reproducing Kernel Banach Spaces for Learning and Neural Networks](https://arxiv.org/abs/2602.07141)
*Isabel de la Higuera,Francisco Herrera,M. Victoria Velasco*

Main category: cs.LG

TL;DR: 提出特征再生核Banach空间框架，将再生核理论从Hilbert空间推广到Banach空间，为神经网络等非Hilbert几何模型提供统一的理论基础。


<details>
  <summary>Details</summary>
Motivation: 传统再生核Hilbert空间框架无法处理神经网络等具有非二次范数的现代学习模型，这些模型自然产生非Hilbert几何结构。在Banach空间中，点评估泛函的连续性不足以保证特征表示或核学习公式。

Method: 基于特征再生核Banach空间的概念，建立Banach空间学习的泛函分析框架。确定特征映射、核构造和表示型定理在非Hilbert体制下恢复的精确结构条件。将监督学习表述为最小范数插值或正则化问题，建立存在性结果和条件表示定理。进一步将理论扩展到向量值特征再生核Banach空间。

Result: 证明了固定架构神经网络自然地诱导出此类空间的特殊实例，为核方法和神经网络提供了统一的函数空间视角，并阐明了核学习原理何时可以超越再生核Hilbert空间。

Conclusion: 该框架将再生核理论从Hilbert空间推广到Banach空间，为理解神经网络等非Hilbert几何模型提供了理论基础，统一了核方法和神经网络的学习原理。

Abstract: Reproducing kernel Hilbert spaces provide a foundational framework for kernel-based learning, where regularization and interpolation problems admit finite-dimensional solutions through classical representer theorems. Many modern learning models, however -- including fixed-architecture neural networks equipped with non-quadratic norms -- naturally give rise to non-Hilbertian geometries that fall outside this setting. In Banach spaces, continuity of point-evaluation functionals alone is insufficient to guarantee feature representations or kernel-based learning formulations. In this work, we develop a functional-analytic framework for learning in Banach spaces based on the notion of featured reproducing kernel Banach spaces. We identify the precise structural conditions under which feature maps, kernel constructions, and representer-type results can be recovered beyond the Hilbertian regime. Within this framework, supervised learning is formulated as a minimal-norm interpolation or regularization problem, and existence results together with conditional representer theorems are established. We further extend the theory to vector-valued featured reproducing kernel Banach spaces and show that fixed-architecture neural networks naturally induce special instances of such spaces. This provides a unified function-space perspective on kernel methods and neural networks and clarifies when kernel-based learning principles extend beyond reproducing kernel Hilbert spaces.

</details>


### [88] [BONSAI: Bayesian Optimization with Natural Simplicity and Interpretability](https://arxiv.org/abs/2602.07144)
*Samuel Daulton,David Eriksson,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: BONSAI是一种考虑默认配置的贝叶斯优化方法，在保持优化性能的同时减少对默认参数的偏离，便于实践者理解和验证推荐配置。


<details>
  <summary>Details</summary>
Motivation: 实际应用中，参数通常有精心设计的默认配置，实践者只希望在必要时偏离默认值。但标准贝叶斯优化不关注最小化偏离，经常将弱相关参数推到搜索边界，这使得难以区分重要和虚假的更改，增加了审查推荐配置的负担。

Method: BONSAI是一种默认感知的贝叶斯优化策略，它会修剪对默认配置的低影响偏离，同时明确控制获取函数的损失值。该方法兼容多种获取函数（如期望改进和GP-UCB），通过理论分析证明其遗憾界限。

Result: 理论分析表明，在特定条件下，BONSAI具有与标准GP-UCB相同的无遗憾性质。实证研究表明，BONSAI在保持竞争力的优化性能的同时，显著减少了推荐配置中的非默认参数数量，对运行时间影响很小。

Conclusion: BONSAI提供了一种实用的贝叶斯优化方法，能够在保持优化性能的同时减少对默认配置的不必要偏离，便于实践者理解和验证优化结果，在实际应用中具有重要价值。

Abstract: Bayesian optimization (BO) is a popular technique for sample-efficient optimization of black-box functions. In many applications, the parameters being tuned come with a carefully engineered default configuration, and practitioners only want to deviate from this default when necessary. Standard BO, however, does not aim to minimize deviation from the default and, in practice, often pushes weakly relevant parameters to the boundary of the search space. This makes it difficult to distinguish between important and spurious changes and increases the burden of vetting recommendations when the optimization objective omits relevant operational considerations. We introduce BONSAI, a default-aware BO policy that prunes low-impact deviations from a default configuration while explicitly controlling the loss in acquisition value. BONSAI is compatible with a variety of acquisition functions, including expected improvement and upper confidence bound (GP-UCB). We theoretically bound the regret incurred by BONSAI, showing that, under certain conditions, it enjoys the same no-regret property as vanilla GP-UCB. Across many real-world applications, we empirically find that BONSAI substantially reduces the number of non-default parameters in recommended configurations while maintaining competitive optimization performance, with little effect on wall time.

</details>


### [89] [Convex Dominance in Deep Learning I: A Scaling Law of Loss and Learning Rate](https://arxiv.org/abs/2602.07145)
*Zhiqi Bu,Shiyun Xu,Jialin Mao*

Main category: cs.LG

TL;DR: 该论文研究深度学习优化中的凸性现象，发现训练后损失函数呈现弱凸性，并基于此建立学习率和损失的缩放规律，能跨训练时长和模型规模进行大幅外推。


<details>
  <summary>Details</summary>
Motivation: 深度学习具有非凸损失曲面，其优化动态难以分析或控制。然而，经验上观察到深度学习在各种任务、模型、优化器等条件下表现出类似凸优化的行为。本文旨在探究凸性和Lipschitz连续性在深度学习中的适用性，以通过学习率调度精确控制损失动态。

Method: 通过分析深度学习训练过程，发现训练后损失函数快速呈现弱凸性。利用凸性理论，推导出最后迭代的上界来预测损失，并基于此确定最优学习率的缩放规律。从凸性视角建立学习率和损失的缩放定律。

Result: 研究表明深度学习在短期训练后即呈现弱凸性，损失可通过最后迭代的上界预测。建立的缩放定律能在训练时长上外推80倍，在模型规模上外推70倍。这为学习率调度提供了理论依据和实用指导。

Conclusion: 深度学习优化虽然本质非凸，但在实际训练中表现出弱凸性特征。利用这一特性可以建立有效的学习率缩放定律，实现对损失动态的精确控制，为深度学习优化提供了新的理论视角和实用工具。

Abstract: Deep learning has non-convex loss landscape and its optimization dynamics is hard to analyze or control. Nevertheless, the dynamics can be empirically convex-like across various tasks, models, optimizers, hyperparameters, etc. In this work, we examine the applicability of convexity and Lipschitz continuity in deep learning, in order to precisely control the loss dynamics via the learning rate schedules. We illustrate that deep learning quickly becomes weakly convex after a short period of training, and the loss is predicable by an upper bound on the last iterate, which further informs the scaling of optimal learning rate. Through the lens of convexity, we build scaling laws of learning rates and losses that extrapolate as much as 80X across training horizons and 70X across model sizes.

</details>


### [90] [On Randomness in Agentic Evals](https://arxiv.org/abs/2602.07150)
*Bjarni Haukur Bjarnason,André Silva,Martin Monperrus*

Main category: cs.LG

TL;DR: 研究发现单次运行评估智能体系统存在显著方差，2-3个百分点的改进可能只是统计噪声而非真实进步，建议采用多次运行、统计功效分析和pass@k等更可靠的评估方法。


<details>
  <summary>Details</summary>
Motivation: 当前智能体系统评估通常基于单次运行计算pass@1分数，并假设这能提供可靠的性能估计。本文旨在验证这一假设，探究单次运行评估的可靠性问题。

Method: 在SWE-Bench-Verified基准上收集60,000个智能体轨迹，涵盖三个模型和两种脚手架。通过token级分析追踪轨迹分歧点，并分析评估方差对性能估计的影响。

Result: 发现显著方差：单次运行pass@1估计因选择不同运行而异2.2-6.0个百分点，即使在温度0时标准差也超过1.5个百分点。轨迹在早期（前几个百分点的token）就出现分歧，小差异会级联成不同的解决策略。

Conclusion: 建议三项具体实践：(1) 每个任务从多次独立运行估计pass@1，(2) 使用统计功效分析确定检测预期效应大小所需的运行次数，(3) 考虑使用k>1的pass@k和pass^k指标。这些方法对区分真实科学进展与统计噪声至关重要。

Abstract: Agentic systems are evaluated on benchmarks where agents interact with environments to solve tasks. Most papers report a pass@1 score computed from a single run per task, assuming this gives a reliable performance estimate. We test this assumption by collecting 60,000 agentic trajectories on SWE-Bench-Verified, spanning three models and two scaffolds. We find substantial variance: single-run pass@1 estimates vary by 2.2 to 6.0 percentage points depending on which run is selected, with standard deviations exceeding 1.5 percentage points even at temperature 0. This variance has critical implications: reported improvements of 2--3 percentage points may reflect evaluation noise rather than genuine algorithmic progress. Through token-level analysis, we show that trajectories diverge early, often within the first few percent of tokens, and that these small differences cascade into different solution strategies. To enable reliable evaluation of agentic systems, we recommend three concrete practices: (1) estimate pass@1 from multiple independent runs per task, especially when measuring small improvements, (2) use statistical power analysis to determine the number of runs needed to detect expected effect sizes, and (3) consider metrics like pass@k (optimistic bound) and pass^k (pessimistic bound) with k>1 to better characterize the full performance envelope. While these practices increase evaluation cost, they are essential for distinguishing genuine scientific progress from statistical noise.

</details>


### [91] [Beyond Pooling: Matching for Robust Generalization under Data Heterogeneity](https://arxiv.org/abs/2602.07154)
*Ayush Roy,Rudrasis Chakraborty,Lav Varshney,Vishnu Suresh Lokhande*

Main category: cs.LG

TL;DR: 提出匹配框架解决异构数据集池化中的分布不对称问题，通过自适应质心选择和迭代优化提升零样本泛化能力


<details>
  <summary>Details</summary>
Motivation: 异构数据集池化会放大分布不对称性并产生有偏估计，特别是在需要零样本泛化的场景中。传统池化和均匀子采样无法有效处理域间混杂因素

Method: 提出匹配框架：基于自适应质心选择样本，迭代优化表示分布。结合双重鲁棒性和倾向得分匹配来过滤混杂域，比传统方法更稳健

Result: 理论分析和实验表明，匹配方法在不对称元分布下优于传统池化和均匀子采样，能扩展到非高斯和多模态真实场景，并在零样本医学异常检测中取得显著改进

Conclusion: 匹配框架能有效处理异构数据集的分布不对称问题，提升零样本泛化性能，特别是在医学异常检测等极端异构场景中具有重要应用价值

Abstract: Pooling heterogeneous datasets across domains is a common strategy in representation learning, but naive pooling can amplify distributional asymmetries and yield biased estimators, especially in settings where zero-shot generalization is required. We propose a matching framework that selects samples relative to an adaptive centroid and iteratively refines the representation distribution. The double robustness and the propensity score matching for the inclusion of data domains make matching more robust than naive pooling and uniform subsampling by filtering out the confounding domains (the main cause of heterogeneity). Theoretical and empirical analyses show that, unlike naive pooling or uniform subsampling, matching achieves better results under asymmetric meta-distributions, which are also extended to non-Gaussian and multimodal real-world settings. Most importantly, we show that these improvements translate to zero-shot medical anomaly detection, one of the extreme forms of data heterogeneity and asymmetry. The code is available on https://github.com/AyushRoy2001/Beyond-Pooling.

</details>


### [92] [Mimetic Initialization of MLPs](https://arxiv.org/abs/2602.07156)
*Asher Trockman,J. Zico Kolter*

Main category: cs.LG

TL;DR: 首次将模仿初始化应用于通道混合层（MLP），通过给第一层赋予非零均值来加速小规模视觉任务训练


<details>
  <summary>Details</summary>
Motivation: 模仿初始化方法之前只应用于空间混合层（卷积、自注意力、状态空间层），本文希望将其扩展到通道混合层（MLP）

Method: 提出极简技术：给MLP的第一层赋予非零均值，可以与空间混合初始化方法结合使用

Result: 在CIFAR-10和ImageNet-1k等小规模视觉任务上加速训练，效果虽小于空间混合初始化，但能提供额外正向效果

Conclusion: 成功将模仿初始化扩展到通道混合层，证明了简单MLP初始化技术对训练加速的有效性

Abstract: Mimetic initialization uses pretrained models as case studies of good initialization, using observations of structures in trained weights to inspire new, simple initialization techniques. So far, it has been applied only to spatial mixing layers, such convolutional, self-attention, and state space layers. In this work, we present the first attempt to apply the method to channel mixing layers, namely multilayer perceptrons (MLPs). Our extremely simple technique for MLPs -- to give the first layer a nonzero mean -- speeds up training on small-scale vision tasks like CIFAR-10 and ImageNet-1k. Though its effect is much smaller than spatial mixing initializations, it can be used in conjunction with them for an additional positive effect.

</details>


### [93] [Learning Nonlinear Systems In-Context: From Synthetic Data to Real-World Motor Control](https://arxiv.org/abs/2602.07173)
*Tong Jian,Tianyu Dai,Tao Yu*

Main category: cs.LG

TL;DR: 首次将Transformer的上下文学习能力应用于电机前馈控制，通过分离信号表示与系统行为，实现少样本微调和单样本上下文学习，在真实电机上超越传统PI控制器和物理模型方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型展现了强大的上下文学习能力，但尚未扩展到信号处理系统。传统PI控制器和基于物理的方法在处理非线性特性和复杂负载条件时存在困难，需要一种能够适应真实世界动态变化的数据高效控制方法。

Method: 提出基于Transformer的模型架构，分离信号表示与系统行为，支持少样本微调和单样本上下文学习。在大规模合成线性与非线性系统数据上进行预训练，使模型能够仅通过少量示例泛化到真实电机的未知系统动态。

Result: 模型在多个电机负载配置上表现出良好的泛化能力，能够将未调优的示例转化为准确的前馈预测，性能超越PI控制器和基于物理的前馈基准方法。

Conclusion: 上下文学习能够桥接合成预训练与真实世界适应性，为物理系统的数据高效控制开辟了新方向，展示了Transformer模型在信号处理和控制领域的应用潜力。

Abstract: LLMs have shown strong in-context learning (ICL) abilities, but have not yet been extended to signal processing systems. Inspired by their design, we have proposed for the first time ICL using transformer models applicable to motor feedforward control, a critical task where classical PI and physics-based methods struggle with nonlinearities and complex load conditions. We propose a transformer based model architecture that separates signal representation from system behavior, enabling both few-shot finetuning and one-shot ICL. Pretrained on a large corpus of synthetic linear and nonlinear systems, the model learns to generalize to unseen system dynamics of real-world motors only with a handful of examples. In experiments, our approach generalizes across multiple motor load configurations, transforms untuned examples into accurate feedforward predictions, and outperforms PI controllers and physics-based feedforward baselines. These results demonstrate that ICL can bridge synthetic pretraining and real-world adaptability, opening new directions for data efficient control of physical systems.

</details>


### [94] [Latent Target Score Matching, with an application to Simulation-Based Inference](https://arxiv.org/abs/2602.07189)
*Joohwan Ko,Tomas Geffner*

Main category: cs.LG

TL;DR: 提出Latent Target Score Matching (LTSM)方法，通过利用联合分数来监督边际分数，降低扩散模型训练中的方差，特别是在低噪声水平下。


<details>
  <summary>Details</summary>
Motivation: 传统的去噪分数匹配(DSM)在低噪声水平下可能面临高方差问题。虽然目标分数匹配(TSM)在可获得干净数据分数时能缓解此问题，但在许多应用中，由于存在潜变量，只能获得联合信号，无法直接获得干净分数。

Method: 提出Latent Target Score Matching (LTSM)，将TSM扩展到潜变量场景，利用联合分数为边际分数提供低方差监督。同时采用LTSM与DSM的混合策略，确保在不同噪声尺度下的鲁棒性。

Result: 在基于模拟的推理任务中，LTSM在方差、分数准确性和样本质量方面均取得一致性的改进。

Conclusion: LTSM通过利用联合分数有效解决了潜变量场景下扩散模型训练的低方差监督问题，特别是在低噪声水平下表现优异，与DSM的混合使用确保了整体鲁棒性。

Abstract: Denoising score matching (DSM) for training diffusion models may suffer from high variance at low noise levels. Target Score Matching (TSM) mitigates this when clean data scores are available, providing a low-variance objective. In many applications clean scores are inaccessible due to the presence of latent variables, leaving only joint signals exposed. We propose Latent Target Score Matching (LTSM), an extension of TSM to leverage joint scores for low-variance supervision of the marginal score. While LTSM is effective at low noise levels, a mixture with DSM ensures robustness across noise scales. Across simulation-based inference tasks, LTSM consistently improves variance, score accuracy, and sample quality.

</details>


### [95] [Risk-Sensitive Exponential Actor Critic](https://arxiv.org/abs/2602.07202)
*Alonso Granados,Jason Pacheco*

Main category: cs.LG

TL;DR: 提出rsEAC方法，通过理论分析和创新设计解决现有风险敏感强化学习方法在连续控制任务中的数值不稳定问题


<details>
  <summary>Details</summary>
Motivation: 现有基于熵风险度量的策略梯度方法存在高方差和数值不稳定问题，限制了风险敏感方法在复杂连续任务中的应用

Method: 提出风险敏感指数演员-评论家(rsEAC)方法，包含理论分析（随机和确定性策略的on/off-policy梯度定理）和避免显式表示指数价值函数及其梯度的创新程序

Result: rsEAC相比现有方法产生更数值稳定的更新，在MuJoCo连续任务的危险变体中可靠地学习风险敏感策略

Conclusion: 通过理论分析和rsEAC方法，成功解决了风险敏感强化学习在复杂连续控制任务中的数值稳定性问题

Abstract: Model-free deep reinforcement learning (RL) algorithms have achieved tremendous success on a range of challenging tasks. However, safety concerns remain when these methods are deployed on real-world applications, necessitating risk-aware agents. A common utility for learning such risk-aware agents is the entropic risk measure, but current policy gradient methods optimizing this measure must perform high-variance and numerically unstable updates. As a result, existing risk-sensitive model-free approaches are limited to simple tasks and tabular settings. In this paper, we provide a comprehensive theoretical justification for policy gradient methods on the entropic risk measure, including on- and off-policy gradient theorems for the stochastic and deterministic policy settings. Motivated by theory, we propose risk-sensitive exponential actor-critic (rsEAC), an off-policy model-free approach that incorporates novel procedures to avoid the explicit representation of exponential value functions and their gradients, and optimizes its policy w.r.t the entropic risk measure. We show that rsEAC produces more numerically stable updates compared to existing approaches and reliably learns risk-sensitive policies in challenging risky variants of continuous tasks in MuJoCo.

</details>


### [96] [Exactly Computing do-Shapley Values](https://arxiv.org/abs/2602.07203)
*R. Teal Witter,Álvaro Parafita,Tomas Garriga,Maximilian Muschalik,Fabian Fumagalli,Axel Brando,Lucas Rosenblatt*

Main category: cs.LG

TL;DR: 提出了一种高效计算do-Shapley值的新方法，通过将计算复杂度从指数级降低到与不可约集数量r线性相关，并提供了精确算法和预算可控的估计器。


<details>
  <summary>Details</summary>
Motivation: do-Shapley值作为量化变量平均因果效应的重要方法，传统计算需要指数级复杂度，这限制了其在复杂结构因果模型中的应用。需要开发更高效的计算方法。

Method: 将do-Shapley值重新表述为底层SCM不可约集的函数，基于此开发了精确算法（时间复杂度O(r)）和预算可控的估计器，其中r是SCM中不可约集的数量。

Result: 新方法在计算效率和识别负担方面均有显著改进：计算复杂度从指数级降至线性级；估计器在相同查询预算下比现有方法准确度高几个数量级；识别要求从所有类别减少到仅需d个单元素联盟。

Conclusion: 通过利用SCM的不可约集结构，显著提高了do-Shapley值的计算效率，降低了识别要求，为因果推断中的Shapley值应用提供了更实用的工具。

Abstract: Structural Causal Models (SCM) are a powerful framework for describing complicated dynamics across the natural sciences. A particularly elegant way of interpreting SCMs is do-Shapley, a game-theoretic method of quantifying the average effect of $d$ variables across exponentially many interventions. Like Shapley values, computing do-Shapley values generally requires evaluating exponentially many terms. The foundation of our work is a reformulation of do-Shapley values in terms of the irreducible sets of the underlying SCM. Leveraging this insight, we can exactly compute do-Shapley values in time linear in the number of irreducible sets $r$, which itself can range from $d$ to $2^d$ depending on the graph structure of the SCM. Since $r$ is unknown a priori, we complement the exact algorithm with an estimator that, like general Shapley value estimators, can be run with any query budget. As the query budget approaches $r$, our estimators can produce more accurate estimates than prior methods by several orders of magnitude, and, when the budget reaches $r$, return the Shapley values up to machine precision. Beyond computational speed, we also reduce the identification burden: we prove that non-parametric identifiability of do-Shapley values requires only the identification of interventional effects for the $d$ singleton coalitions, rather than all classes.

</details>


### [97] [Online Learning for Uninformed Markov Games: Empirical Nash-Value Regret and Non-Stationarity Adaptation](https://arxiv.org/abs/2602.07205)
*Junyan Liu,Haipeng Luo,Zihan Zhang,Lillian J. Ratliff*

Main category: cs.LG

TL;DR: 本文提出了一种新的经验纳什值遗憾度量，并设计了参数自由算法，能够自适应地适应对手的非平稳性，在固定对手时恢复O(√K)外部遗憾，在最坏情况下恢复O(K^{2/3})纳什值遗憾。


<details>
  <summary>Details</summary>
Motivation: 现有V-learning算法在无信息马尔可夫博弈中只能达到O(K^{2/3})的纳什值遗憾，且无法适应问题难度：即使对手遵循固定策略（此时O(√K)外部遗憾是可实现的），现有方法仍给出较差的O(K^{2/3})速率。

Method: 1. 引入经验纳什值遗憾这一新遗憾度量，比纳什值遗憾更强，在对手固定时自然退化为外部遗憾。2. 提出参数自由算法，基于Mao等人(2022)的基于epoch的V-learning算法新分析，建立O(ηC + √K/η)遗憾界。3. 展示如何根据对手的非平稳性自适应重启算法并选择适当的η参数。

Result: 算法实现了O(min{√K + (CK)^{1/3}, √LK})遗憾界，其中C量化对手策略的方差，L表示策略切换次数。这恢复了两个极端情况：对手固定时的O(√K)外部遗憾和最坏情况下的O(K^{2/3})纳什值遗憾，并能通过自动适应对手的非平稳性在这些极端之间平滑插值。

Conclusion: 本文完全解决了现有工作的两个限制：1) 提出了更强且更自然的遗憾度量；2) 设计了能够自适应问题难度的参数自由算法，在不同对手行为模式下都能达到最优或接近最优的遗憾界。

Abstract: We study online learning in two-player uninformed Markov games, where the opponent's actions and policies are unobserved. In this setting, Tian et al. (2021) show that achieving no-external-regret is impossible without incurring an exponential dependence on the episode length $H$. They then turn to the weaker notion of Nash-value regret and propose a V-learning algorithm with regret $O(K^{2/3})$ after $K$ episodes. However, their algorithm and guarantee do not adapt to the difficulty of the problem: even in the case where the opponent follows a fixed policy and thus $O(\sqrt{K})$ external regret is well-known to be achievable, their result is still the worse rate $O(K^{2/3})$ on a weaker metric.
  In this work, we fully address both limitations. First, we introduce empirical Nash-value regret, a new regret notion that is strictly stronger than Nash-value regret and naturally reduces to external regret when the opponent follows a fixed policy. Moreover, under this new metric, we propose a parameter-free algorithm that achieves an $O(\min \{\sqrt{K} + (CK)^{1/3},\sqrt{LK}\})$ regret bound, where $C$ quantifies the variance of the opponent's policies and $L$ denotes the number of policy switches (both at most $O(K)$). Therefore, our results not only recover the two extremes -- $O(\sqrt{K})$ external regret when the opponent is fixed and $O(K^{2/3})$ Nash-value regret in the worst case -- but also smoothly interpolate between these extremes by automatically adapting to the opponent's non-stationarity. We achieve so by first providing a new analysis of the epoch-based V-learning algorithm by Mao et al. (2022), establishing an $O(ηC + \sqrt{K/η})$ regret bound, where $η$ is the epoch incremental factor. Next, we show how to adaptively restart this algorithm with an appropriate $η$ in response to the potential non-stationarity of the opponent, eventually achieving our final results.

</details>


### [98] [DSL: Understanding and Improving Softmax Recommender Systems with Competition-Aware Scaling](https://arxiv.org/abs/2602.07206)
*Bucher Sahyouni,Matthew Vowels,Liqun Chen,Simon Hadfield*

Main category: cs.LG

TL;DR: DSL通过自适应温度调节和负样本重加权，在推荐系统中显著提升Softmax Loss的性能和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 传统Softmax Loss在隐式反馈推荐系统中存在两个问题：1）单一全局温度对所有用户-物品对不适用；2）均匀采样的负样本可能包含不同相关性的竞争者，导致训练不稳定

Method: 提出双尺度Softmax Loss (DSL)，包含两个分支：1）基于硬度和物品相似度对每个训练实例内的负样本进行重加权；2）从构建的竞争者列表中自适应学习每个示例的温度

Result: 在多个基准测试和骨干网络上，DSL相比强基线有显著提升，平均改进6.22%，某些设置下超过10%。在OOD流行度偏移下，平均改进9.31%

Conclusion: DSL通过自适应调整竞争分布，在保持Softmax Loss几何结构的同时，显著提升了推荐系统的准确性、鲁棒性和公平性，理论分析也支持其有效性

Abstract: Softmax Loss (SL) is being increasingly adopted for recommender systems (RS) as it has demonstrated better performance, robustness and fairness. Yet in implicit-feedback, a single global temperature and equal treatment of uniformly sampled negatives can lead to brittle training, because sampled sets may contain varying degrees of relevant or informative competitors. The optimal loss sharpness for a user-item pair with a particular set of negatives, can be suboptimal or destabilising for another with different negatives. We introduce Dual-scale Softmax Loss (DSL), which infers effective sharpness from the sampled competition itself. DSL adds two complementary branches to the log-sum-exp backbone. Firstly it reweights negatives within each training instance using hardness and item--item similarity, secondly it adapts a per-example temperature from the competition intensity over a constructed competitor slate. Together, these components preserve the geometry of SL while reshaping the competition distribution across negatives and across examples.
  Over several representative benchmarks and backbones, DSL yields substantial gains over strong baselines, with improvements over SL exceeding $10%$ in several settings and averaging $6.22%$ across datasets, metrics, and backbones. Under out-of-distribution (OOD) popularity shift, the gains are larger, with an average of $9.31%$ improvement over SL. We further provide a theoretical, distributionally robust optimisation (DRO) analysis, which demonstrates how DSL reshapes the robust payoff and the KL deviation for ambiguous instances. This helps explain the empirically observed improvements in accuracy and robustness.

</details>


### [99] [Adaptive Retrieval helps Reasoning in LLMs -- but mostly if it's not used](https://arxiv.org/abs/2602.07213)
*Srijan Shakya,Anamaria-Roberta Hartl,Sepp Hochreiter,Korbinian Pöppel*

Main category: cs.LG

TL;DR: 该研究探索了自适应检索增强架构，让LLM在推理过程中主动决定何时查询外部知识库。实验发现静态检索不如CoT，但自适应检索中不包含检索的推理轨迹表现优于CoT，表明检索很少帮助推理，而主动不使用检索是模型性能良好的信号。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理任务中常因静态参数化知识而出现幻觉，在数学等专业领域表现不佳。本研究探索通过将检索视为动态上下文学习来增强生成模型的基本原则。

Method: 采用自适应检索增强架构，让LLM代理在推理过程中主动决定何时查询外部知识库。在GSM8K和MATH-500基准上比较自适应策略与标准CoT基线和静态检索方法。

Result: 静态检索不如CoT；自适应检索中：包含检索结果的轨迹表现略差于CoT，但不包含检索的轨迹表现优于CoT。这表明检索很少帮助推理，主动不使用检索是模型性能良好的信号。模型会根据问题难度调整检索频率。

Conclusion: 模型自我评估知识并选择性使用外部信息的能力是构建更稳健可靠生成模型的关键原则。检索决策作为元认知信号对模型性能至关重要。

Abstract: Large Language Models (LLMs) often falter in complex reasoning tasks due to their static, parametric knowledge, leading to hallucinations and poor performance in specialized domains like mathematics. This work explores a fundamental principle for enhancing generative models: treating retrieval as a form of dynamic in-context learning. We test an adaptive retrieval-augmented architecture where an LLM agent actively decides when to query an external knowledge base during its reasoning process. We compare this adaptive strategy against a standard Chain-of-Thought (CoT) baseline and a static retrieval approach on the GSM8K and MATH-500 benchmarks. Although our experiments show that static retrieval is inferior to CoT, the adaptive retrieval shows interesting behavior: While traces including retrieved results show slightly worse performance compared to CoT, traces that do not include retrieval actually perform better compared to CoT. This suggests that: (a) retrieval only rarely helps reasoning (we show a few counterexamples, e.g. using useful theorems) and (b) actively not using retrieval is indicative of good model performance. Furthermore, we find that the model scales its retrieval frequency with the difficulty of the problem, reinforcing that the decision to retrieve is a crucial metacognitive signal. The agent's ability to self-assess its knowledge and selectively engage with external information represents a key principle for building more robust and reliable generative models.

</details>


### [100] [Probing Neural TSP Representations for Prescriptive Decision Support](https://arxiv.org/abs/2602.07216)
*Reuben Narad,Léonard Boussioux,Michael Wagner*

Main category: cs.LG

TL;DR: 该研究探索了神经组合优化模型在解决旅行商问题后，其内部表征是否可迁移到其他优化相关任务，如节点移除敏感性和边保留敏感性分析。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索训练好的TSP求解器是否能学习到可迁移的内部表征，用于其他优化相关的下游任务，实现类似其他领域的迁移学习效果。

Method: 训练多个基于注意力的TSP策略，收集其内部激活，然后训练探针模型来预测节点/边嵌入，用于两个NP难的下游任务：节点移除敏感性和边禁止敏感性分析。

Result: 在Euclidean TSP100训练的模型上，两个任务的探针性能与现有基线相当。将探针信号与几何特征集成后，节点移除任务达到65% top-1准确率（基线58%），边识别任务达到73% top-1准确率（基线67%）。

Conclusion: 这是首次研究神经TSP求解器作为可迁移编码器用于超越路径构建的预测性决策支持任务。转移准确率随求解器质量和模型规模提高而增加，表明训练更强的NCO求解器也能产生更有用的下游编码器。

Abstract: The field of neural combinatorial optimization (NCO) trains neural policies to solve NP-hard problems such as the traveling salesperson problem (TSP). We ask whether, beyond producing good tours, a trained TSP solver learns internal representations that transfer to other optimization-relevant objectives, in the spirit of transfer learning from other domains. We train several attention-based TSP policies, collect their internal activations, and train probes on node/edge embeddings for two NP-hard prescriptive downstream tasks inspired by real-world logistics scenarios: node-removal sensitivity (identifying the most impactful node to remove) and edge-forbid sensitivity (identifying the most critical edge to retain). On a Euclidean TSP100-trained model, probes for both tasks are competitive with existing baselines. Ensembling probe signals with geometric features outperforms the strongest baselines: 65\% top-1 accuracy (vs. 58\% baseline) for the best-node-removal task, and 73\% top-1 accuracy (vs. 67\% baseline) for the worst-edge identification task. To our knowledge, we are the first to study neural TSP solvers as transferable encoders for prescriptive what-if decision-support objectives beyond tour construction. Finally, we show that transfer accuracy increases with solver quality across training and model scale, suggesting that training stronger NCO solvers also yields more useful encoders for downstream objectives. Our code is available at: github.com/ReubenNarad/tsp_prescriptive_probe

</details>


### [101] [Collaborative and Efficient Fine-tuning: Leveraging Task Similarity](https://arxiv.org/abs/2602.07218)
*Gagik Magakyan,Amirhossein Reisizadeh,Chanwoo Park,Pablo A. Parrilo,Asuman Ozdaglar*

Main category: cs.LG

TL;DR: CoLoRA：利用任务相似性进行协作式低秩适配，通过共享适配器捕获任务共性，个性化适配器处理用户特定任务，解决基础模型微调中的数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 基础模型微调通常面临标注数据稀缺的问题。作者观察到不同用户的任务之间存在相似性，可以利用这种相似性来增加有效的微调数据量，从而提升模型性能。

Method: 提出协作式低秩适配（CoLoRA），包含两个组件：1）共享适配器捕获所有任务的共性特征；2）个性化适配器针对每个用户的特定任务进行定制。该方法结合了任务相似性的协作优势。

Result: 在异构线性回归问题上提供了理论保证，证明能够恢复真实参数。在自然语言处理实验中，当与相似任务一起训练时，个体性能显著提升，验证了任务相似性的协作效益。

Conclusion: CoLoRA通过利用任务相似性进行协作式微调，有效缓解了基础模型微调中的数据稀缺问题，在理论和实验上都证明了其有效性，为参数高效微调提供了新思路。

Abstract: Adaptability has been regarded as a central feature in the foundation models, enabling them to effectively acclimate to unseen downstream tasks. Parameter-efficient fine-tuning methods such as celebrated LoRA facilitate efficient adaptation of large foundation models using labeled, high-quality and generally scarce task data. To mitigate data scarcity in fine-tuning of foundation models, we propose to leverage task similarity across multiple downstream users. Intuitively, users with similar tasks must be able to assist each other in boosting the effective fine-tuning data size. We propose Collaborative Low-Rank Adaptation, or CoLoRA, which exploits task similarity to collaboratively and efficiently fine-tune personalized foundation models. The main idea in CoLoRA is to train one shared adapter capturing underlying task similarities across all tasks, and personalized adapters tailored to user-specific tasks. We theoretically study CoLoRA on heterogeneous linear regression and provide provable guarantees for ground truth recovery. We also conduct several natural language experiments with varying task similarity, which further demonstrate that when trained together with similar tasks, individual performances are significantly boosted.

</details>


### [102] [The Median is Easier than it Looks: Approximation with a Constant-Depth, Linear-Width ReLU Network](https://arxiv.org/abs/2602.07219)
*Abhigyan Dutta,Itay Safran,Paul Valiant*

Main category: cs.LG

TL;DR: 本文研究了使用ReLU神经网络近似d个输入的median函数，提出了深度-宽度权衡，最终构建了常数深度、线性宽度的网络，在单位超立方体均匀分布上实现指数级小的近似误差。


<details>
  <summary>Details</summary>
Motivation: 先前关于maximum函数的研究表明，要达到可比精度，线性宽度需要深度至少增长为log log d。本文旨在突破这一障碍，研究median函数的近似，并建立从maximum到median的一般归约。

Method: 采用多阶段迭代过程，逐步消除非中心元素，同时保留中位数周围的候选集。克服了maximum函数近似中不存在的障碍。

Result: 构建了常数深度、线性宽度的ReLU神经网络，在单位超立方体均匀分布上实现指数级小的近似误差。结果比先前已知的maximum函数近似结果更强。

Conclusion: 本文突破了先前关于maximum函数近似深度-宽度权衡的障碍，通过研究median函数并建立从maximum到median的归约，获得了比maximum函数本身更强的近似结果。

Abstract: We study the approximation of the median of $d$ inputs using ReLU neural networks. We present depth-width tradeoffs under several settings, culminating in a constant-depth, linear-width construction that achieves exponentially small approximation error with respect to the uniform distribution over the unit hypercube. By further establishing a general reduction from the maximum to the median, our results break a barrier suggested by prior work on the maximum function, which indicated that linear width should require depth growing at least as $\log\log d$ to achieve comparable accuracy. Our construction relies on a multi-stage procedure that iteratively eliminates non-central elements while preserving a candidate set around the median. We overcome obstacles that do not arise for the maximum to yield approximation results that are strictly stronger than those previously known for the maximum itself.

</details>


### [103] [SpecAttn: Co-Designing Sparse Attention with Self-Speculative Decoding](https://arxiv.org/abs/2602.07223)
*Yikang Yue,Yuqi Xue,Jian Huang*

Main category: cs.LG

TL;DR: SpecAttn是一种自推测解码方法，通过验证引导的稀疏注意力机制，利用验证过程中识别的关键KV条目来生成后续token，显著提高解码吞吐量。


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM推理中的KV缓存内存需求成为瓶颈。现有自推测解码方法依赖独立的KV选择算法，忽略了验证过程中已计算出的KV条目重要性信息。

Method: 提出SpecAttn方法，将验证过程作为副产品识别关键KV条目，仅加载这些关键条目用于后续token的生成，实现验证引导的稀疏注意力。

Result: 相比传统自回归解码，吞吐量提高2.81倍；相比最先进的基于稀疏性的自推测解码方法，吞吐量提升1.29倍。

Conclusion: SpecAttn通过重用验证过程中计算的关键KV信息，既提高了token接受率又降低了KV选择开销，有效解决了长上下文LLM推理的内存瓶颈问题。

Abstract: Long-context large language model (LLM) inference has become the norm for today's AI applications. However, it is severely bottlenecked by the increasing memory demands of its KV cache. Previous works have shown that self-speculative decoding with sparse attention, where tokens are drafted using a subset of the KV cache and verified in parallel with full KV cache, speeds up inference in a lossless way. However, this approach relies on standalone KV selection algorithms to select the KV entries used for drafting and overlooks that the criticality of each KV entry is inherently computed during verification. In this paper, we propose SpecAttn, a self-speculative decoding method with verification-guided sparse attention. SpecAttn identifies critical KV entries as a byproduct of verification and only loads these entries when drafting subsequent tokens. This not only improves draft token acceptance rate but also incurs low KV selection overhead, thereby improving decoding throughput. SpecAttn achieves 2.81$\times$ higher throughput over vanilla auto-regressive decoding and 1.29$\times$ improvement over state-of-the-art sparsity-based self-speculative decoding methods.

</details>


### [104] [Fault-Tolerant Evaluation for Sample-Efficient Model Performance Estimators](https://arxiv.org/abs/2602.07226)
*Zihan Zhu,Yanqiu Wu,Qiongkai Xu*

Main category: cs.LG

TL;DR: 提出一个容错评估框架，用于评估模型性能估计器，通过可调节的容忍度ε来整合偏差和方差，解决现有方法在低方差场景下的问题。


<details>
  <summary>Details</summary>
Motivation: 在模型即服务时代，组织依赖第三方AI模型快速部署，但新兴AI应用的动态性、新数据集的不断引入以及声称优越性能的模型增多，使得模型服务的有效可靠验证变得日益困难。现有评估方法在低方差场景下存在问题：RMSE混淆了偏差和方差，当方差小时掩盖了持续偏差；而基于p值的检验变得过于敏感，因微小偏差就拒绝足够的估计器。

Method: 提出一个容错评估框架，将偏差和方差考虑整合到可调节的容忍度ε中，允许在实际可接受的误差范围内评估性能估计器。理论上证明了适当校准ε可以确保在不同方差机制下的可靠评估，并进一步提出了自动优化和选择ε的算法。

Result: 在真实世界数据集上的实验表明，该框架提供了对估计器行为的全面且可操作的洞察。

Conclusion: 提出的容错评估框架解决了现有方法在低方差场景下的局限性，通过整合偏差和方差考虑并引入可调节容忍度，为模型性能估计器的评估提供了更可靠和实用的方法。

Abstract: In the era of Model-as-a-Service, organizations increasingly rely on third-party AI models for rapid deployment. However, the dynamic nature of emerging AI applications, the continual introduction of new datasets, and the growing number of models claiming superior performance make efficient and reliable validation of model services increasingly challenging. This motivates the development of sample-efficient performance estimators, which aim to estimate model performance by strategically selecting instances for labeling, thereby reducing annotation cost. Yet existing evaluation approaches often fail in low-variance settings: RMSE conflates bias and variance, masking persistent bias when variance is small, while p-value based tests become hypersensitive, rejecting adequate estimators for negligible deviations. To address this, we propose a fault-tolerant evaluation framework that integrates bias and variance considerations within an adjustable tolerance level ${\varepsilon}$, enabling the evaluation of performance estimators within practically acceptable error margins. We theoretically show that proper calibration of ${\varepsilon}$ ensures reliable evaluation across different variance regimes, and we further propose an algorithm that automatically optimizes and selects ${\varepsilon}$. Experiments on real-world datasets demonstrate that our framework provides comprehensive and actionable insights into estimator behavior.

</details>


### [105] [Cerebellar-Inspired Residual Control for Fault Recovery: From Inference-Time Adaptation to Structural Consolidation](https://arxiv.org/abs/2602.07227)
*Nethmi Jayasinghe,Diana Gontero,Spencer T. Brown,Vinod K. Sangwan,Mark C. Hersam,Amit Ranjan Trivedi*

Main category: cs.LG

TL;DR: 提出一种受小脑启发的推理时残差控制框架，在冻结的强化学习策略基础上添加在线纠正动作，无需修改基础策略参数即可实现故障恢复。


<details>
  <summary>Details</summary>
Motivation: 机器人策略在真实环境中部署时常遇到训练后故障，而重新训练、探索或系统辨识往往不切实际。需要一种能够在推理时进行故障恢复而不修改基础策略的方法。

Method: 采用小脑启发的残差控制框架：1）通过固定特征扩展实现高维模式分离；2）并行微区式残差通路；3）具有兴奋性和抑制性资格迹的局部误差驱动可塑性，在不同时间尺度上运行；4）性能驱动的元自适应机制调节残差权限和可塑性。

Result: 在MuJoCo基准测试中，在驱动器、动态和环境扰动下，HalfCheetah-v5性能提升达+66%，Humanoid-v5提升达+53%。在严重扰动下表现优雅退化，并能将持久的残差修正整合到策略参数中。

Conclusion: 该小脑启发的推理时残差控制框架能够有效处理训练后故障，在不修改基础策略参数的情况下实现快速局部校正，同时通过元自适应机制保持名义行为稳定性。

Abstract: Robotic policies deployed in real-world environments often encounter post-training faults, where retraining, exploration, or system identification are impractical. We introduce an inference-time, cerebellar-inspired residual control framework that augments a frozen reinforcement learning policy with online corrective actions, enabling fault recovery without modifying base policy parameters. The framework instantiates core cerebellar principles, including high-dimensional pattern separation via fixed feature expansion, parallel microzone-style residual pathways, and local error-driven plasticity with excitatory and inhibitory eligibility traces operating at distinct time scales. These mechanisms enable fast, localized correction under post-training disturbances while avoiding destabilizing global policy updates. A conservative, performance-driven meta-adaptation regulates residual authority and plasticity, preserving nominal behavior and suppressing unnecessary intervention. Experiments on MuJoCo benchmarks under actuator, dynamic, and environmental perturbations show improvements of up to $+66\%$ on \texttt{HalfCheetah-v5} and $+53\%$ on \texttt{Humanoid-v5} under moderate faults, with graceful degradation under severe shifts and complementary robustness from consolidating persistent residual corrections into policy parameters.

</details>


### [106] [ArcMark: Multi-bit LLM Watermark via Optimal Transport](https://arxiv.org/abs/2602.07235)
*Atefeh Gilani,Carol Xuan Long,Sajani Vithana,Oliver Kosut,Lalitha Sankar,Flavio P. Calmon*

Main category: cs.LG

TL;DR: 本文首次推导出多比特水印的信息论容量，并提出基于编码理论的新水印方案ArcMark，在保持平均下一个token预测不变的前提下实现更高比特率和检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型水印技术要么是零比特水印（仅标记AI生成文本），要么是多比特水印（编码复杂信息）。虽然已有一些多比特水印能在不改变平均下一个token预测的情况下插入多个比特，但它们大多沿用零比特水印的设计原则（如每个token编码一个比特）。多比特水印的信息论容量——在不改变平均下一个token预测的前提下，每个token可插入和检测的最大比特数——一直未知。

Method: 1. 首次推导多比特水印的信息论容量表征；2. 基于编码理论原理设计新的水印构造ArcMark，在特定假设下达到多比特水印信道的容量；3. 将语言模型水印问题形式化为信道编码问题。

Result: ArcMark在实际应用中优于其他多比特水印方案，在每token比特率和检测精度方面表现更优。研究证明ArcMark能够达到多比特水印信道的理论容量。

Conclusion: 语言模型水印本质上是信道编码问题，本文为基于编码理论原理的水印设计开辟了新途径。ArcMark展示了编码理论方法在水印设计中的优势，实现了更高的信息嵌入效率和检测性能。

Abstract: Watermarking is an important tool for promoting the responsible use of language models (LMs). Existing watermarks insert a signal into generated tokens that either flags LM-generated text (zero-bit watermarking) or encodes more complex messages (multi-bit watermarking). Though a number of recent multi-bit watermarks insert several bits into text without perturbing average next-token predictions, they largely extend design principles from the zero-bit setting, such as encoding a single bit per token. Notably, the information-theoretic capacity of multi-bit watermarking -- the maximum number of bits per token that can be inserted and detected without changing average next-token predictions -- has remained unknown. We address this gap by deriving the first capacity characterization of multi-bit watermarks. Our results inform the design of ArcMark: a new watermark construction based on coding-theoretic principles that, under certain assumptions, achieves the capacity of the multi-bit watermark channel. In practice, ArcMark outperforms competing multi-bit watermarks in terms of bit rate per token and detection accuracy. Our work demonstrates that LM watermarking is fundamentally a channel coding problem, paving the way for principled coding-theoretic approaches to watermark design.

</details>


### [107] [Graph homophily booster: Reimagining the role of discrete features in heterophilic graph learning](https://arxiv.org/abs/2602.07256)
*Ruizhong Qiu,Ting-Wei Li,Gaotang Li,Hanghang Tong*

Main category: cs.LG

TL;DR: GRAPHITE通过创建特征节点直接提高图同质性，解决异质图上的GNN性能问题，在异质图上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有GNN在异质图（连接节点特征/标签不同的图）上表现不佳，甚至不如简单的MLP。现有方法主要关注架构设计，没有直接解决异质性的根本原因，需要新的范式来直接提高图同质性。

Method: 提出GRAPHITE框架，通过创建特征节点来促进特征相似节点之间的同质性消息传递。从同质性的精确定义出发，直接转换图结构以提高同质性，仅轻微增加图规模。

Result: 理论和实验表明GRAPHITE能显著提高异质图的同质性。在挑战性数据集上，GRAPHITE在异质图上显著优于最先进方法，在同质图上也能达到可比精度。

Conclusion: GRAPHITE提出了一种直接提高图同质性的新范式，通过图转换解决异质性问题，为GNN在异质图上的应用提供了有效解决方案。

Abstract: Graph neural networks (GNNs) have emerged as a powerful tool for modeling graph-structured data. However, existing GNNs often struggle with heterophilic graphs, where connected nodes tend to have dissimilar features or labels. While numerous methods have been proposed to address this challenge, they primarily focus on architectural designs without directly targeting the root cause of the heterophily problem. These approaches still perform even worse than the simplest MLPs on challenging heterophilic datasets. For instance, our experiments show that 21 latest GNNs still fall behind the MLP on the Actor dataset. This critical challenge calls for an innovative approach to addressing graph heterophily beyond architectural designs. To bridge this gap, we propose and study a new and unexplored paradigm: directly increasing the graph homophily via a carefully designed graph transformation. In this work, we present a simple yet effective framework called GRAPHITE to address graph heterophily. To the best of our knowledge, this work is the first method that explicitly transforms the graph to directly improve the graph homophily. Stemmed from the exact definition of homophily, our proposed GRAPHITE creates feature nodes to facilitate homophilic message passing between nodes that share similar features. Furthermore, we both theoretically and empirically show that our proposed GRAPHITE significantly increases the homophily of originally heterophilic graphs, with only a slight increase in the graph size. Extensive experiments on challenging datasets demonstrate that our proposed GRAPHITE significantly outperforms state-of-the-art methods on heterophilic graphs while achieving comparable accuracy with state-of-the-art methods on homophilic graphs.

</details>


### [108] [Robust Ultra-High-Dimensional Variable Selection With Correlated Structure Using Group Testing](https://arxiv.org/abs/2602.07258)
*Wanru Guo,Juan Xie,Binbin Wang,Weicong Chen,Xiaoyi Lu,Vipin Chaudhary,Curtis Tatsuoka*

Main category: cs.LG

TL;DR: Dorfman筛选框架：通过层次聚类形成数据驱动的变量组，进行组内和组间假设检验，结合弹性网络进行特征选择，并提供稳健版本处理异常值和非正态数据。


<details>
  <summary>Details</summary>
Motivation: 高维基因组数据存在强烈的组相关结构，传统特征选择方法假设特征独立或依赖预定义通路，对异常值和模型误设敏感，需要更稳健的方法。

Method: 多阶段程序：1) 通过层次聚类形成数据驱动的变量组；2) 进行组和组内假设检验；3) 使用弹性网络或自适应弹性网络细化选择；稳健变体包含OGK协方差估计、秩相关和Huber加权回归。

Result: 在模拟中，Dorfman-Sparse-Adaptive-EN在正态条件下表现最佳，Robust-OGK-Dorfman-Adaptive-EN在数据污染条件下优势明显；应用于NSCLC基因表达数据时，稳健Dorfman方法获得最低预测误差并富集临床相关基因。

Conclusion: Dorfman框架为基因组特征选择提供了高效稳健的方法，Robust-OGK-Dorfman-Adaptive-EN在理想和污染条件下均表现优异，可扩展到超高维设置，适用于现代基因组生物标志物发现。

Abstract: Background: High-dimensional genomic data exhibit strong group correlation structures that challenge conventional feature selection methods, which often assume feature independence or rely on pre-defined pathways and are sensitive to outliers and model misspecification.
  Methods: We propose the Dorfman screening framework, a multi-stage procedure that forms data-driven variable groups via hierarchical clustering, performs group and within-group hypothesis testing, and refines selection using elastic net or adaptive elastic net. Robust variants incorporate OGK-based covariance estimation, rank-based correlation, and Huber-weighted regression to handle contaminated and non-normal data.
  Results: In simulations, Dorfman-Sparse-Adaptive-EN performed best under normal conditions, while Robust-OGK-Dorfman-Adaptive-EN showed clear advantages under data contamination, outperforming classical Dorfman and competing methods. Applied to NSCLC gene expression data for trametinib response, robust Dorfman methods achieved the lowest prediction errors and enriched recovery of clinically relevant genes.
  Conclusions: The Dorfman framework provides an efficient and robust approach to genomic feature selection. Robust-OGK-Dorfman-Adaptive-EN offers strong performance under both ideal and contaminated conditions and scales to ultra-high-dimensional settings, making it well suited for modern genomic biomarker discovery.

</details>


### [109] [tLoRA: Efficient Multi-LoRA Training with Elastic Shared Super-Models](https://arxiv.org/abs/2602.07263)
*Kevin Li,Dibyadeep Saha,Avni Kanodia,Fan Lai*

Main category: cs.LG

TL;DR: tLoRA：一个高效批量训练多个LoRA适配器的框架，通过融合共享基础模型的适配器为弹性超级模型，使用融合内核和智能调度，提升训练吞吐量1.2-1.8倍，完成时间缩短2.3-5.4倍，GPU利用率提高37%。


<details>
  <summary>Details</summary>
Motivation: 随着LoRA成为微调大语言模型的标准方法，共享集群中同时运行多个LoRA训练任务的情况日益普遍。现有方法在训练时批量处理异构LoRA适配器存在挑战：任务在适配器秩、批大小和资源分配上各不相同，简单的批量处理会导致同步延迟、通信开销和性能下降。

Method: tLoRA采用两层方法：1）内核层：使用融合LoRA内核，自适应重构低秩计算块，调度秩感知的纳米批次，最大化计算与通信的重叠；2）调度层：在线残差容量感知调度器，自适应分组任务以最大化集体吞吐量。框架将共享基础模型的适配器融合为弹性共享超级模型，利用现有分布式训练框架实现资源共享。

Result: 基于真实集群轨迹的评估显示，tLoRA将训练吞吐量提升1.2-1.8倍，任务训练完成时间缩短2.3-5.4倍，GPU利用率提高37%。

Conclusion: tLoRA通过创新的内核融合和智能调度机制，有效解决了异构LoRA适配器批量训练的挑战，显著提升了集群资源利用率和训练效率，为大规模LoRA微调提供了高效解决方案。

Abstract: As Low-Rank Adaptation (LoRA) becomes the standard approach for efficiently fine-tuning large language models (LLMs), shared clusters increasingly execute many concurrent LoRA training jobs over the same frozen backbone. While recent advances enable batching (co-locating) multiple adapters during serving, efficient training-time co-location of heterogeneous LoRA adapters presents unique challenges. Jobs often differ in adapter rank, batch size, and resource allocation, and naïve batching can introduce synchronization stalls, communication overheads, and per-job slowdowns that are worse than executing independently. We introduce tLoRA, a framework that enables efficient batch training of multiple LoRA jobs. tLoRA fuses adapters that share the same base model into an elastic shared super-model, exploiting existing distributed training frameworks to derive parallelism plans that share resources effectively. At the kernel level, tLoRA employs a fused LoRA kernel that adaptively reconstructs low-rank computation tiles and schedules rank-aware nano-batches to maximize overlap between computation and communication across adapters. At the scheduling layer, tLoRA incorporates an online, residual-capacity-aware scheduler that adaptively groups jobs to maximize collective throughput. Evaluations using real-world cluster traces demonstrate that tLoRA improves training throughput by 1.2--1.8x, job training completion time by 2.3--5.4x, and GPU utilization by 37%.

</details>


### [110] [XShare: Collaborative in-Batch Expert Sharing for Faster MoE Inference](https://arxiv.org/abs/2602.07265)
*Daniil Vankov,Nikita Ivkin,Kyle Ulrich,Xiang Song,Ashish Khetan,George Karypis*

Main category: cs.LG

TL;DR: XShare：一种无需重新训练的动态专家选择方法，通过批处理感知的贪心算法优化MoE架构中的专家激活，减少30%专家激活，降低3倍GPU峰值负载，提升14%推理吞吐量


<details>
  <summary>Details</summary>
Motivation: MoE架构虽然能高效扩展大语言模型，但在生产推理中，请求批处理和推测解码会显著增加专家激活，削弱效率优势。需要解决批处理感知的专家选择问题。

Method: 将批处理感知的专家选择建模为模块化优化问题，设计高效的贪心算法适应不同部署场景。XShare无需重新训练，动态适应每个批次，通过最大化选定专家的总门控分数来优化专家激活。

Result: 在标准批处理下减少30%专家激活；在专家并行部署中降低3倍GPU峰值负载；在推测解码中通过分层、相关性感知的专家选择实现14%吞吐量提升，即使批次请求来自异构数据集。

Conclusion: XShare有效解决了MoE架构在生产推理中的效率瓶颈，通过动态批处理感知的专家选择优化，显著提升推理效率，适用于各种部署场景。

Abstract: Mixture-of-Experts (MoE) architectures are increasingly used to efficiently scale large language models. However, in production inference, request batching and speculative decoding significantly amplify expert activation, eroding these efficiency benefits. We address this issue by modeling batch-aware expert selection as a modular optimization problem and designing efficient greedy algorithms for different deployment settings. The proposed method, namely XShare, requires no retraining and dynamically adapts to each batch by maximizing the total gating score of selected experts. It reduces expert activation by up to 30% under standard batching, cuts peak GPU load by up to 3x in expert-parallel deployments, and achieves up to 14% throughput gains in speculative decoding via hierarchical, correlation-aware expert selection even if requests in a batch drawn from heterogeneous datasets.

</details>


### [111] [Hybrid Feedback-Guided Optimal Learning for Wireless Interactive Panoramic Scene Delivery](https://arxiv.org/abs/2602.07273)
*Xiaoyi Wu,Juaren Steiger,Bin Li,R. Srikant*

Main category: cs.LG

TL;DR: 论文提出了一种用于VR/AR边缘渲染的混合反馈在线学习算法AdaPort，通过结合全信息反馈和赌博机反馈来优化视口预测和传输，显著提升了学习效率。


<details>
  <summary>Details</summary>
Motivation: VR/AR应用对帧率、延迟和物理虚拟环境同步有严格要求。现有方法将视口选择问题建模为两级赌博机反馈，但忽略了预测反馈可以在观察到用户头部姿态后对所有候选视口进行回溯计算，这实际上是全信息反馈而非赌博机反馈。

Method: 提出两级混合反馈模型，结合全信息反馈和赌博机反馈。设计了AdaPort混合学习算法，利用两种反馈类型提高学习效率。推导了混合反馈模型的实例相关遗憾下界，并建立了与之渐近匹配的遗憾上界。

Result: 理论分析表明AdaPort的遗憾上界与下界渐近匹配。基于真实世界轨迹的仿真实验显示，AdaPort在性能上持续优于最先进的基线方法。

Conclusion: 通过识别预测反馈的全信息特性并建立混合反馈模型，AdaPort算法在VR/AR边缘渲染的视口选择问题上实现了更高效的学习，为沉浸式应用的实时渲染提供了更好的解决方案。

Abstract: Immersive applications such as virtual and augmented reality impose stringent requirements on frame rate, latency, and synchronization between physical and virtual environments. To meet these requirements, an edge server must render panoramic content, predict user head motion, and transmit a portion of the scene that is large enough to cover the user viewport while remaining within wireless bandwidth constraints. Each portion produces two feedback signals: prediction feedback, indicating whether the selected portion covers the actual viewport, and transmission feedback, indicating whether the corresponding packets are successfully delivered. Prior work models this problem as a multi-armed bandit with two-level bandit feedback, but fails to exploit the fact that prediction feedback can be retrospectively computed for all candidate portions once the user head pose is observed. As a result, prediction feedback constitutes full-information feedback rather than bandit feedback. Motivated by this observation, we introduce a two-level hybrid feedback model that combines full-information and bandit feedback, and formulate the portion selection problem as an online learning task under this setting. We derive an instance-dependent regret lower bound for the hybrid feedback model and propose AdaPort, a hybrid learning algorithm that leverages both feedback types to improve learning efficiency. We further establish an instance-dependent regret upper bound that matches the lower bound asymptotically, and demonstrate through real-world trace driven simulations that AdaPort consistently outperforms state-of-the-art baseline methods.

</details>


### [112] [Laplacian-LoRA: Delaying Oversmoothing in Deep GCNs via Spectral Low-Rank Adaptation](https://arxiv.org/abs/2602.07278)
*Sai Vamsi Alisetti*

Main category: cs.LG

TL;DR: 提出Laplacian-LoRA方法，通过低秩谱适应延迟GCN的过平滑现象，将有效深度提升最多两倍


<details>
  <summary>Details</summary>
Motivation: 深度图卷积网络存在过平滑问题，导致节点表示随深度增加而坍缩。现有方法多通过架构修改或残差机制缓解，但过平滑的谱原因往往未明确揭示。

Method: 提出Laplacian-LoRA方法，对标准GCN进行简单可解释的低秩谱适应。不重新设计消息传递，而是为固定的拉普拉斯传播算子引入可学习的谱锚定校正，选择性地减弱收缩同时保持稳定性和低通归纳偏置。

Result: 在多个基准数据集和深度下，Laplacian-LoRA能一致延迟过平滑的发生，将GCN的有效深度提升最多两倍。嵌入方差诊断确认这些增益来自延迟的表示坍缩，学习谱分析显示校正是平滑、有界且行为良好的。

Conclusion: 过平滑是一种深度依赖的谱现象，可以通过对图传播算子进行适度的低秩适应来系统性地延迟。

Abstract: Oversmoothing is a fundamental limitation of deep graph convolutional networks (GCNs), causing node representations to collapse as depth increases. While many prior approaches mitigate this effect through architectural modifications or residual mechanisms, the underlying spectral cause of oversmoothing is often left implicit. We propose Laplacian-LoRA, a simple and interpretable low-rank spectral adaptation of standard GCNs. Rather than redesigning message passing, Laplacian-LoRA introduces a learnable, spectrally anchored correction to the fixed Laplacian propagation operator, selectively weakening contraction while preserving stability and the low-pass inductive bias. Across multiple benchmark datasets and depths, Laplacian-LoRA consistently delays the onset of oversmoothing, extending the effective depth of GCNs by up to a factor of two. Embedding variance diagnostics confirm that these gains arise from delayed representational collapse, while learned spectral analysis demonstrates that the correction is smooth, bounded, and well behaved. Our results show that oversmoothing is a depth-dependent spectral phenomenon that can be systematically delayed through modest, low-rank adaptation of the graph propagation operator.

</details>


### [113] [VertCoHiRF: Decentralized Vertical Clustering Beyond k-means](https://arxiv.org/abs/2602.07279)
*Bruno Belucci,Karim Lounici,Vladimir R. Kostic,Katia Meziani*

Main category: cs.LG

TL;DR: VertCoHiRF：基于异构视图结构共识的完全去中心化垂直联邦聚类框架，通过标识符级共识实现隐私保护，无需交换特征相关统计量


<details>
  <summary>Details</summary>
Motivation: 现有垂直联邦学习方法主要局限于k-means的分布式变体，需要中心化协调或交换特征相关数值统计，在异构视图或对抗行为下鲁棒性有限

Method: 完全去中心化框架，各代理使用适合本地特征空间的基聚类方法独立聚类，通过标识符级共识协调提案，采用去中心化序数排序选择代表性中心点，逐步诱导共享层次聚类

Result: 通信仅限于样本标识符、聚类标签和序数排序，提供隐私保护设计，支持重叠特征分区和异构本地聚类方法，产生可解释的共享聚类融合层次结构

Conclusion: VertCoHiRF在垂直联邦设置中展现出竞争力的聚类性能，同时分析了通信复杂度和鲁棒性，为异构视图下的联邦聚类提供了隐私保护的去中心化解决方案

Abstract: Vertical Federated Learning (VFL) enables collaborative analysis across parties holding complementary feature views of the same samples, yet existing approaches are largely restricted to distributed variants of $k$-means, requiring centralized coordination or the exchange of feature-dependent numerical statistics, and exhibiting limited robustness under heterogeneous views or adversarial behavior. We introduce VertCoHiRF, a fully decentralized framework for vertical federated clustering based on structural consensus across heterogeneous views, allowing each agent to apply a base clustering method adapted to its local feature space in a peer-to-peer manner. Rather than exchanging feature-dependent statistics or relying on noise injection for privacy, agents cluster their local views independently and reconcile their proposals through identifier-level consensus. Consensus is achieved via decentralized ordinal ranking to select representative medoids, progressively inducing a shared hierarchical clustering across agents. Communication is limited to sample identifiers, cluster labels, and ordinal rankings, providing privacy by design while supporting overlapping feature partitions and heterogeneous local clustering methods, and yielding an interpretable shared Cluster Fusion Hierarchy (CFH) that captures cross-view agreement at multiple resolutions.We analyze communication complexity and robustness, and experiments demonstrate competitive clustering performance in vertical federated settings.

</details>


### [114] [Fair Decisions from Calibrated Scores: Achieving Optimal Classification While Satisfying Sufficiency](https://arxiv.org/abs/2602.07285)
*Etam Benger,Katrina Ligett*

Main category: cs.LG

TL;DR: 本文提出了一种在满足充分性公平约束下进行最优二元分类的精确解决方案，包括几何特征描述和简单后处理算法，并解决了充分性与分离性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 在二元分类中，基于预测概率的阈值划分在无约束情况下是贝叶斯最优的，但使用单一阈值通常会违反统计群体公平约束。虽然独立性和分离性约束下，满足相应标准的分数阈值划分就足够了，但充分性约束下，即使是完美群体校准的分数（包括真实类别概率）在阈值划分后也会违反预测奇偶性。因此需要解决在充分性约束下的最优分类问题。

Method: 1. 假设有限组校准分数集，提出在充分性约束下最优二元（随机化）分类的精确解决方案；2. 提供可实现的正预测值（PPV）和假遗漏率（FOR）对的可实现几何特征描述；3. 推导出仅使用组校准分数和组成员身份的简单后处理算法来获得最优分类器；4. 识别在满足充分性约束下最小化与分离性偏差的分类器。

Result: 1. 获得了在充分性公平约束下最优二元分类的精确解决方案；2. 开发了能够获得最优分类器的简单后处理算法；3. 解决了充分性与分离性之间的权衡问题，找到了在满足充分性约束下最小化与分离性偏差的分类器；4. 该算法通常能达到与最优性能相当的表现。

Conclusion: 本文解决了在充分性公平约束下的二元分类问题，提供了精确解决方案和实用算法，同时处理了充分性与分离性之间的兼容性问题，为公平机器学习中的分类任务提供了重要工具。

Abstract: Binary classification based on predicted probabilities (scores) is a fundamental task in supervised machine learning. While thresholding scores is Bayes-optimal in the unconstrained setting, using a single threshold generally violates statistical group fairness constraints. Under independence (statistical parity) and separation (equalized odds), such thresholding suffices when the scores already satisfy the corresponding criterion. However, this does not extend to sufficiency: even perfectly group-calibrated scores -- including true class probabilities -- violate predictive parity after thresholding. In this work, we present an exact solution for optimal binary (randomized) classification under sufficiency, assuming finite sets of group-calibrated scores. We provide a geometric characterization of the feasible pairs of positive predictive value (PPV) and false omission rate (FOR) achievable by such classifiers, and use it to derive a simple post-processing algorithm that attains the optimal classifier using only group-calibrated scores and group membership. Finally, since sufficiency and separation are generally incompatible, we identify the classifier that minimizes deviation from separation subject to sufficiency, and show that it can also be obtained by our algorithm, often achieving performance comparable to the optimum.

</details>


### [115] [Incorruptible Neural Networks: Training Models that can Generalize to Large Internal Perturbations](https://arxiv.org/abs/2602.07320)
*Philip Jacobson,Ben Feinberg,Suhas Kumar,Sapan Agarwal,T. Patrick Xiao,Christopher Bennett*

Main category: cs.LG

TL;DR: 论文研究了通过锐度感知最小化(SAM)和随机权重扰动(RWP)方法训练对权重扰动鲁棒的神经网络，分析了其在泛化和优化方面的表现，发现过正则化RWP在噪声鲁棒泛化方面最优，SAM在小噪声下表现更好但大噪声下较差，并提出动态调整扰动强度可改善优化效果。


<details>
  <summary>Details</summary>
Motivation: 神经网络损失函数的平坦区域被认为与更好的泛化性能相关，而训练对权重扰动鲁棒的模型对未来低功耗硬件平台很重要。本文旨在探索如何通过SAM和RWP方法找到对各种权重随机扰动鲁棒的最小值。

Method: 采用锐度感知最小化(SAM)和随机权重扰动(RWP)两种方法，从两个角度研究问题：1)泛化角度：如何减小噪声鲁棒泛化差距；2)优化角度：在强扰动下如何最大化优化器性能。理论分析和实验验证相结合，并探索动态调整扰动强度的方法。

Result: 1)过正则化的RWP训练目标在噪声鲁棒泛化方面最优；2)对于小幅度噪声，SAM的对抗性目标比任何RWP配置表现更好，但在大幅度噪声下表现较差；3)损失函数的不均匀性导致梯度消失效应，影响SAM和RWP；4)动态调整扰动强度以匹配损失函数演化可改善优化效果。

Conclusion: 过正则化RWP是噪声鲁棒泛化的最优方法，SAM在小噪声下有优势但大噪声下受限，损失函数不均匀性导致的梯度消失是主要挑战，动态调整扰动强度是改善优化的有效策略。

Abstract: Flat regions of the neural network loss landscape have long been hypothesized to correlate with better generalization properties. A closely related but distinct problem is training models that are robust to internal perturbations to their weights, which may be an important need for future low-power hardware platforms. In this paper, we explore the usage of two methods, sharpness-aware minimization (SAM) and random-weight perturbation (RWP), to find minima robust to a variety of random corruptions to weights. We consider the problem from two angles: generalization (how do we reduce the noise-robust generalization gap) and optimization (how do we maximize performance from optimizers when subject to strong perturbations). First, we establish, both theoretically and empirically, that an over-regularized RWP training objective is optimal for noise-robust generalization. For small-magnitude noise, we find that SAM's adversarial objective further improves performance over any RWP configuration, but performs poorly for large-magnitude noise. We link the cause of this to a vanishing-gradient effect, caused by unevenness in the loss landscape, affecting both SAM and RWP. Lastly, we demonstrate that dynamically adjusting the perturbation strength to match the evolution of the loss landscape improves optimizing for these perturbed objectives.

</details>


### [116] [Revisiting Robustness for LLM Safety Alignment via Selective Geometry Control](https://arxiv.org/abs/2602.07340)
*Yonghui Yang,Wenjian Tao,Jilong Liu,Xingyu Zhu,Junfeng Fang,Weibiao Huang,Le Wu,Richang Hong,Tat-Sent Chua*

Main category: cs.LG

TL;DR: ShaPO是一个几何感知的偏好优化框架，通过选择性控制对齐关键参数子空间的几何结构来增强LLM安全对齐的鲁棒性，在分布偏移和噪声监督下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型安全对齐方法在领域转移和噪声偏好监督下表现脆弱，大多数鲁棒对齐方法只关注对齐数据的不确定性，而忽视了基于偏好的目标函数中优化引起的脆弱性。

Method: 提出ShaPO框架，从优化几何角度重新审视LLM安全对齐的鲁棒性，通过对齐关键参数子空间的选择性几何控制来强制执行最坏情况对齐目标，避免均匀几何约束导致的过正则化。

Result: 在多样化的安全基准测试和噪声偏好设置中，ShaPO相比流行的偏好优化方法持续提升了安全鲁棒性，并且能与数据鲁棒目标清晰组合，带来额外增益。

Conclusion: ShaPO通过优化几何视角有效解决了LLM安全对齐的鲁棒性问题，实证结果支持了所提出的优化几何视角，表明仅靠数据为中心的方法无法完全解决鲁棒性失效问题。

Abstract: Safety alignment of large language models remains brittle under domain shift and noisy preference supervision. Most existing robust alignment methods focus on uncertainty in alignment data, while overlooking optimization-induced fragility in preference-based objectives. In this work, we revisit robustness for LLM safety alignment from an optimization geometry perspective, and argue that robustness failures cannot be addressed by data-centric methods alone. We propose ShaPO, a geometry-aware preference optimization framework that enforces worst-case alignment objectives via selective geometry control over alignment-critical parameter subspace. By avoiding uniform geometry constraints, ShaPO mitigates the over-regularization that can harm robustness under distribution shift. We instantiate ShaPO at two levels: token-level ShaPO stabilizes likelihood-based surrogate optimization, while reward-level ShaPO enforces reward-consistent optimization under noisy supervision. Across diverse safety benchmarks and noisy preference settings, ShaPO consistently improves safety robustness over popular preference optimization methods. Moreover, ShaPO composes cleanly with data-robust objectives, yielding additional gains and empirically supporting the proposed optimization-geometry perspective.

</details>


### [117] [Scalable Dexterous Robot Learning with AR-based Remote Human-Robot Interactions](https://arxiv.org/abs/2602.07341)
*Yicheng Yang,Ruijiao Li,Lifeng Wang,Shuai Zheng,Shunzheng Ma,Keyu Zhang,Tuoyu Sun,Chenyun Dai,Jie Ding,Zhuo Zou*

Main category: cs.LG

TL;DR: 提出一个结合增强现实远程交互、行为克隆预训练和对比学习强化学习的统一框架，用于灵巧机器人臂手系统的可扩展操作学习


<details>
  <summary>Details</summary>
Motivation: 解决灵巧机器人臂手系统操作任务学习中的可扩展性问题，通过增强现实远程人机交互高效收集专家演示数据，提高学习效率

Method: 两阶段框架：第一阶段通过增强现实远程交互收集数据，采用行为克隆预训练策略；第二阶段开发基于对比学习的强化学习方法，设计投影头加速学习，采用事件驱动增强奖励确保安全

Result: 相比经典PPO和SAC方法，本方法显著加快推理速度，在操作任务成功率方面表现更好，消融研究证实对比学习能避免策略崩溃

Conclusion: 提出的统一框架有效解决了灵巧机器人臂手系统的操作学习问题，结合增强现实交互、行为克隆和对比学习强化学习，实现了高效、鲁棒且安全的策略学习

Abstract: This paper focuses on the scalable robot learning for manipulation in the dexterous robot arm-hand systems, where the remote human-robot interactions via augmented reality (AR) are established to collect the expert demonstration data for improving efficiency. In such a system, we present a unified framework to address the general manipulation task problem. Specifically, the proposed method consists of two phases: i) In the first phase for pretraining, the policy is created in a behavior cloning (BC) manner, through leveraging the learning data from our AR-based remote human-robot interaction system; ii) In the second phase, a contrastive learning empowered reinforcement learning (RL) method is developed to obtain more efficient and robust policy than the BC, and thus a projection head is designed to accelerate the learning progress. An event-driven augmented reward is adopted for enhancing the safety. To validate the proposed method, both the physics simulations via PyBullet and real-world experiments are carried out. The results demonstrate that compared to the classic proximal policy optimization and soft actor-critic policies, our method not only significantly speeds up the inference, but also achieves much better performance in terms of the success rate for fulfilling the manipulation tasks. By conducting the ablation study, it is confirmed that the proposed RL with contrastive learning overcomes policy collapse. Supplementary demonstrations are available at https://cyberyyc.github.io/.

</details>


### [118] [Controllable Value Alignment in Large Language Models through Neuron-Level Editing](https://arxiv.org/abs/2602.07356)
*Yonghui Yang,Junwei Li,Jilong Liu,Yicheng He,Fengbin Zhu,Weibiao Huang,Le Wu,Richang Hong,Tat-Seng Chua*

Main category: cs.LG

TL;DR: 提出NeVA框架，通过神经元级编辑实现可控的价值对齐，解决现有方法中价值泄漏问题


<details>
  <summary>Details</summary>
Motivation: 现有基于引导的价值对齐方法存在有限的可控性：引导目标价值时往往会无意中激活其他非目标价值，即"价值泄漏"问题

Method: 提出NeVA框架：1) 识别稀疏的价值相关神经元；2) 在推理时进行激活编辑，无需参数更新或重新训练；3) 基于Schwartz价值理论定义标准化泄漏度量

Result: NeVA实现了更强的目标价值对齐，同时带来更小的通用能力性能下降，显著降低了平均泄漏率，残余效应主要局限于语义相关的价值类别

Conclusion: NeVA为价值对齐提供了更可控和可解释的机制，解决了现有方法的价值泄漏问题

Abstract: Aligning large language models (LLMs) with human values has become increasingly important as their influence on human behavior and decision-making expands. However, existing steering-based alignment methods suffer from limited controllability: steering a target value often unintentionally activates other, non-target values. To characterize this limitation, we introduce value leakage, a diagnostic notion that captures the unintended activation of non-target values during value steering, along with a normalized leakage metric grounded in Schwartz's value theory. In light of this analysis, we propose NeVA, a neuron-level editing framework for controllable value alignment in LLMs. NeVA identifies sparse, value-relevant neurons and performs inference-time activation editing, enabling fine-grained control without parameter updates or retraining. Experiments show that NeVA achieves stronger target value alignment while incurring smaller performance degradation on general capability. Moreover, NeVA significantly reduces the average leakage, with residual effects largely confined to semantically related value classes. Overall, NeVA offers a more controllable and interpretable mechanism for value alignment.

</details>


### [119] [UTOPIA: Unlearnable Tabular Data via Decoupled Shortcut Embedding](https://arxiv.org/abs/2602.07358)
*Jiaming He,Fuming Luo,Hongwei Li,Wenbo Jiang,Wenshu Fan,Zhenbo Shi,Xudong Jiang,Yi Yu*

Main category: cs.LG

TL;DR: UTOPIA提出了一种针对表格数据的不可学习样本生成方法，通过解耦优化在高显著性特征上进行语义混淆，在低显著性冗余特征中嵌入超相关捷径，实现认证的不可学习性。


<details>
  <summary>Details</summary>
Motivation: 金融和医疗领域的表格数据高度敏感，需要防止未经授权的模型训练。现有不可学习样本方法在表格数据上效果不佳，因为表格数据混合了数值和类别约束，且存在显著性稀疏性（学习主要集中于少数维度）。

Method: UTOPIA利用特征冗余将优化解耦为两个通道：高显著性特征用于语义混淆，低显著性冗余特征用于嵌入超相关捷径。在谱主导条件下，当毒化谱压倒干净语义谱时，可实现认证的不可学习性。

Result: 在多个表格数据集和模型上的实验表明，UTOPIA能将未经授权的训练推向接近随机性能，优于现有不可学习样本基线方法，并能很好地跨架构迁移。

Conclusion: UTOPIA为保护敏感表格数据提供了一种有效的不可学习样本生成方法，通过解耦优化和谱主导理论，在保持表格数据有效性的同时实现了强大的保护效果。

Abstract: Unlearnable examples (UE) have emerged as a practical mechanism to prevent unauthorized model training on private vision data, while extending this protection to tabular data is nontrivial. Tabular data in finance and healthcare is highly sensitive, yet existing UE methods transfer poorly because tabular features mix numerical and categorical constraints and exhibit saliency sparsity, with learning dominated by a few dimensions. Under a Spectral Dominance condition, we show certified unlearnability is feasible when the poison spectrum overwhelms the clean semantic spectrum. Guided by this, we propose Unlearnable Tabular Data via DecOuPled Shortcut EmbeddIng (UTOPIA), which exploits feature redundancy to decouple optimization into two channels: high saliency features for semantic obfuscation and low saliency redundant features for embedding a hyper correlated shortcut, yielding constraint-aware dominant shortcuts while preserving tabular validity. Extensive experiments across tabular datasets and models show UTOPIA drives unauthorized training toward near random performance, outperforming strong UE baselines and transferring well across architectures.

</details>


### [120] [FEM-Informed Hypergraph Neural Networks for Efficient Elastoplasticity](https://arxiv.org/abs/2602.07364)
*Jianchuan Yang,Xi Chen,Jidong Zhao*

Main category: cs.LG

TL;DR: 提出一种基于超图神经网络的有限元嵌入方法FHGNN，用于计算力学中的物理驱动学习，无需标注数据，在三维弹塑性问题上实现了比传统PINN更高的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 图神经网络与稀疏算子和非结构化离散化自然对齐，是计算力学中物理驱动机器学习的有前景范式。受离散物理损失和分层深度学习神经网络构造的启发，需要开发一种数值一致的有限元嵌入方法。

Method: 将有限元计算直接嵌入到消息传递层中，提出有限元信息超图神经网络(FHGNN)。输入是节点元素超图，边编码网格连接性。采用高效的变分损失函数，利用GPU并行张量操作和离散表示。

Result: 在包括各向同性/运动硬化循环加载的3D基准测试中，该方法相比最近的竞争性PINN变体实现了显著改进的精度和效率。能够有效扩展到大型弹塑性问题，在可比精度下可与多核有限元实现竞争或更快。

Conclusion: 这项工作为非线性固体力学中可扩展的物理嵌入学习奠定了基础，展示了FHGNN在计算力学中的潜力。

Abstract: Graph neural networks (GNNs) naturally align with sparse operators and unstructured discretizations, making them a promising paradigm for physics-informed machine learning in computational mechanics. Motivated by discrete physics losses and Hierarchical Deep Learning Neural Network (HiDeNN) constructions, we embed finite-element (FEM) computations at nodes and Gauss points directly into message-passing layers and propose a numerically consistent FEM-Informed Hypergraph Neural Networks (FHGNN). Similar to conventional physics-informed neural networks (PINNs), training is purely physics-driven and requires no labeled data: the input is a node element hypergraph whose edges encode mesh connectivity. Guided by empirical results and condition-number analysis, we adopt an efficient variational loss. Validated on 3D benchmarks, including cyclic loading with isotropic/kinematic hardening, the proposed method delivers substantially improved accuracy and efficiency over recent, competitive PINN variants. By leveraging GPU-parallel tensor operations and the discrete representation, it scales effectively to large elastoplastic problems and can be competitive with, or faster than, multi-core FEM implementations at comparable accuracy. This work establishes a foundation for scalable, physics-embedded learning in nonlinear solid mechanics.

</details>


### [121] [Privately Learning Decision Lists and a Differentially Private Winnow](https://arxiv.org/abs/2602.07370)
*Mark Bun,William Fang*

Main category: cs.LG

TL;DR: 提出新的差分隐私算法用于学习决策列表和大间隔半空间，在PAC和在线模型中都有改进


<details>
  <summary>Details</summary>
Motivation: 经典机器学习任务（决策列表和半空间学习）在差分隐私约束下的性能优化，减少隐私保护带来的额外成本

Method: 1. PAC模型中：计算高效的决策列表学习算法，样本开销最小化；2. 在线模型中：Winnow算法的隐私版本，用于学习大间隔半空间；3. 在线模型中的决策列表学习应用

Result: 1. PAC模型中：样本开销接近最优非隐私算法；2. 在线模型中：错误边界在维度上为多对数级，与间隔成反多项式关系；3. 在线决策列表学习达到最先进非隐私算法的性能

Conclusion: 提出的差分隐私算法在PAC和在线模型中都能有效学习决策列表和半空间，在隐私保护下仍能保持接近最优的性能表现

Abstract: We give new differentially private algorithms for the classic problems of learning decision lists and large-margin halfspaces in the PAC and online models. In the PAC model, we give a computationally efficient algorithm for learning decision lists with minimal sample overhead over the best non-private algorithms. In the online model, we give a private analog of the influential Winnow algorithm for learning halfspaces with mistake bound polylogarithmic in the dimension and inverse polynomial in the margin. As an application, we describe how to privately learn decision lists in the online model, qualitatively matching state-of-the art non-private guarantees.

</details>


### [122] [Dichotomy of Feature Learning and Unlearning: Fast-Slow Analysis on Neural Networks with Stochastic Gradient Descent](https://arxiv.org/abs/2602.07378)
*Shota Imai,Sota Nishiyama,Masaaki Imaizumi*

Main category: cs.LG

TL;DR: 该研究通过无限宽度极限和快慢动力学分析，揭示了神经网络中特征遗忘现象的机制和条件，发现数据中的主要非线性项强度会诱导特征遗忘，而第二层权重的初始尺度可以缓解该现象。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络梯度训练中的非平凡结构是理论机器学习的核心挑战。特征遗忘现象（神经网络在长期训练中逐渐丢失先前学习到的特征）引起了关注，需要从理论上解释其机制和条件。

Method: 采用两层神经网络的无限宽度极限，使用大批量随机梯度更新，推导出不同时间尺度的微分方程。利用快慢动力学分析：第一层权重快速对齐，第二层权重缓慢发展。结合张量程序和奇异摄动理论进行理论分析。

Result: 揭示了特征遗忘发生的机制：临界流形上的流动方向（由慢动力学决定）决定了是否发生特征遗忘。数值验证了结果，并推导了特征遗忘的理论基础和缩放规律。

Conclusion: 主要发现：(1) 数据中主要非线性项的强度会诱导特征遗忘；(2) 第二层权重的初始尺度可以缓解特征遗忘。技术分析结合了张量程序和奇异摄动理论。

Abstract: The dynamics of gradient-based training in neural networks often exhibit nontrivial structures; hence, understanding them remains a central challenge in theoretical machine learning. In particular, a concept of feature unlearning, in which a neural network progressively loses previously learned features over long training, has gained attention. In this study, we consider the infinite-width limit of a two-layer neural network updated with a large-batch stochastic gradient, then derive differential equations with different time scales, revealing the mechanism and conditions for feature unlearning to occur. Specifically, we utilize the fast-slow dynamics: while an alignment of first-layer weights develops rapidly, the second-layer weights develop slowly. The direction of a flow on a critical manifold, determined by the slow dynamics, decides whether feature unlearning occurs. We give numerical validation of the result, and derive theoretical grounding and scaling laws of the feature unlearning. Our results yield the following insights: (i) the strength of the primary nonlinear term in data induces the feature unlearning, and (ii) an initial scale of the second-layer weights mitigates the feature unlearning. Technically, our analysis utilizes Tensor Programs and the singular perturbation theory.

</details>


### [123] [Scout Before You Attend: Sketch-and-Walk Sparse Attention for Efficient LLM Inference](https://arxiv.org/abs/2602.07397)
*Hoang Anh Duy Le,Sahil Joshi,Zeyu Yang,Zhaozhuo Xu,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: Sketch&Walk Attention：一种无需训练的稀疏注意力方法，通过轻量级草图（Hadamard sketching）和确定性游走机制动态选择top-k注意力块，在保持接近无损精度的同时实现高达6倍的推理加速。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制在长上下文LLM推理中（包括预填充和解码阶段）占据了主要的计算和内存成本，需要高效的稀疏注意力解决方案来降低这些开销。

Method: 使用Hadamard草图技术廉价近似注意力分数，通过游走机制在层间聚合这些估计值以捕捉超出直接token交互的注意力影响，基于累积的游走分数选择top-k注意力块，实现动态稀疏化。

Result: 在多种模型和任务上，Sketch&Walk在20%注意力密度下保持接近无损的准确性，在某些设置中甚至略微优于密集注意力，同时实现高达6倍的推理加速。

Conclusion: Sketch&Walk Attention提供了一种统一的无需训练稀疏注意力方法，适用于预填充和解码阶段，能显著降低长上下文LLM推理的计算和内存成本。

Abstract: Self-attention dominates the computational and memory cost of long-context LLM inference across both prefill and decode phases. To address this challenge, we introduce Sketch&Walk Attention, a training-free sparse attention method that determines sparsity with lightweight sketches and deterministic walk. Sketch&Walk applies Hadamard sketching to get inexpensive approximations of attention scores, then aggregates these estimates across layers via a walk mechanism that captures attention influence beyond direct interactions between tokens. The accumulated walk scores are used to select top-k attention blocks, enabling dynamic sparsity with a single training-free algorithm that applies uniformly to both the prefill and decode phases, together with custom sparse attention kernels. Across a wide range of models and tasks, Sketch&Walk maintains near-lossless accuracy at 20% attention density and can slightly outperform dense attention in some settings, while achieving up to 6x inference speedup.

</details>


### [124] [BitLogic: Training Framework for Gradient-Based FPGA-Native Neural Networks](https://arxiv.org/abs/2602.07400)
*Simon Bührer,Andreas Plesner,Aczel Till,Roger Wattenhofer*

Main category: cs.LG

TL;DR: BitLogic：一个基于FPGA原生查找表（LUT）计算的端到端可训练神经网络框架，用可微LUT节点替代传统乘加运算，实现高效硬件部署。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络推理的能耗和延迟成本主要来自部署而非训练，需要硬件专用化方案。FPGA是理想的专用化平台，但现有FPGA神经网络方法分散且难以比较。

Method: 提出BitLogic框架：1）用可微LUT节点替代乘加运算，直接映射到FPGA原语；2）支持原生二进制计算、稀疏连接；3）提供模块化API、学习编码器、硬件感知头；4）自动RTL导出管道将PyTorch模型转换为可综合HDL。

Result: 在标准视觉基准和异构硬件平台上展示：1）CIFAR-10达到72.3%测试准确率，仅使用不到30万逻辑门；2）仅用LUT资源实现亚20纳秒单样本推理；3）在FPGA效率方面获得显著提升。

Conclusion: BitLogic为FPGA原生神经网络提供了完整的梯度训练框架，通过LUT计算实现高效硬件部署，在保持竞争力的准确率同时大幅提升FPGA效率，为边缘设备推理提供了有前景的解决方案。

Abstract: The energy and latency costs of deep neural network inference are increasingly driven by deployment rather than training, motivating hardware-specialized alternatives to arithmetic-heavy models. Field-Programmable Gate Arrays (FPGAs) provide an attractive substrate for such specialization, yet existing FPGA-based neural approaches are fragmented and difficult to compare. We present BitLogic, a fully gradient-based, end-to-end trainable framework for FPGA-native neural networks built around Lookup Table (LUT) computation. BitLogic replaces multiply-accumulate operations with differentiable LUT nodes that map directly to FPGA primitives, enabling native binary computation, sparse connectivity, and efficient hardware realization. The framework offers a modular functional API supporting diverse architectures, along with learned encoders, hardware-aware heads, and multiple boundary-consistent LUT relaxations. An automated Register Transfer Level (RTL) export pipeline translates trained PyTorch models into synthesizable HDL, ensuring equivalence between software and hardware inference. Experiments across standard vision benchmarks and heterogeneous hardware platforms demonstrate competitive accuracy and substantial gains in FPGA efficiency, including 72.3% test accuracy on CIFAR-10 achieved with fewer than 0.3M logic gates, while attaining sub-20 ns single-sample inference using only LUT resources.

</details>


### [125] [Nonparametric Bayesian Optimization for General Rewards](https://arxiv.org/abs/2602.07411)
*Zishi Zhang,Tao Ren,Yijie Peng*

Main category: cs.LG

TL;DR: 提出了首个在一般奖励模型下实现无遗憾保证的贝叶斯优化算法，使用无限高斯过程作为代理模型，结合汤普森采样，计算高效且性能优越。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化通常假设高斯过程先验，但在实际应用中奖励模型存在不确定性，可能非平稳、重尾或条件不良，需要更通用的方法。

Method: 提出无限高斯过程作为贝叶斯非参数模型，在奖励分布空间上放置先验，结合汤普森采样进行探索-利用平衡，使用截断吉布斯采样保证计算可扩展性。

Result: 算法在一般奖励设置下实现无遗憾保证，仅需目标函数的Lipschitz连续性，计算复杂度与经典高斯过程相当，在非平稳、重尾等复杂奖励场景中表现优异。

Conclusion: 该工作首次为一般奖励模型下的贝叶斯优化提供了理论保证和实用算法，无限高斯过程框架扩展了传统方法的适用范围，在复杂现实问题中具有重要应用价值。

Abstract: This work focuses on Bayesian optimization (BO) under reward model uncertainty. We propose the first BO algorithm that achieves no-regret guarantee in a general reward setting, requiring only Lipschitz continuity of the objective function and accommodating a broad class of measurement noise. The core of our approach is a novel surrogate model, termed as infinite Gaussian process ($\infty$-GP). It is a Bayesian nonparametric model that places a prior on the space of reward distributions, enabling it to represent a substantially broader class of reward models than classical Gaussian process (GP). The $\infty$-GP is used in combination with Thompson Sampling (TS) to enable effective exploration and exploitation. Correspondingly, we develop a new TS regret analysis framework for general rewards, which relates the regret to the total variation distance between the surrogate model and the true reward distribution. Furthermore, with a truncated Gibbs sampling procedure, our method is computationally scalable, incurring minimal additional memory and computational complexities compared to classical GP. Empirical results demonstrate state-of-the-art performance, particularly in settings with non-stationary, heavy-tailed, or other ill-conditioned rewards.

</details>


### [126] [Learning Molecular Chirality via Chiral Determinant Kernels](https://arxiv.org/abs/2602.07415)
*Runhan Shi,Zhicheng Zhang,Letian Chen,Gufeng Yu,Yang Yang*

Main category: cs.LG

TL;DR: ChiDeK是一个将立体化学信息整合到分子表示学习中的框架，通过手性行列式核编码SE(3)不变的手性矩阵，使用交叉注意力将局部手性中心信息集成到全局分子表示中，能同时编码中心手性和轴向手性，在多个手性相关任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 手性是决定化学和生物学中立体特异性行为的基本分子特性。现有机器学习模型难以捕捉手性，因为立体化学关系几何复杂，传统分子表示缺乏明确的立体化学编码。现有方法主要关注中心手性，依赖手工设计的立体化学标签或有限的3D编码，无法推广到更复杂的轴向手性等形态。

Method: 提出ChiDeK（手性行列式核）框架，通过手性行列式核编码SE(3)不变的手性矩阵，采用交叉注意力机制将局部手性中心的立体化学信息集成到全局分子表示中。该设计能够在统一架构中显式建模手性相关特征，同时编码中心手性和轴向手性。为评估轴向手性，构建了新的电子圆二色性（ECD）和光学旋转（OR）预测基准。

Result: 在四个任务上（R/S构型分类、对映体排序、ECD光谱预测、OR预测），ChiDeK相比最先进的基线方法取得显著改进，特别是在轴向手性任务上平均准确率提高了7%以上。

Conclusion: ChiDeK框架能够系统地将立体化学信息整合到分子表示学习中，通过统一架构同时处理中心手性和轴向手性，在手性相关任务上表现出优越性能，为复杂立体化学关系的机器学习建模提供了有效解决方案。

Abstract: Chirality is a fundamental molecular property that governs stereospecific behavior in chemistry and biology. Capturing chirality in machine learning models remains challenging due to the geometric complexity of stereochemical relationships and the limitations of traditional molecular representations that often lack explicit stereochemical encoding. Existing approaches to chiral molecular representation primarily focus on central chirality, relying on handcrafted stereochemical tags or limited 3D encodings, and thus fail to generalize to more complex forms such as axial chirality. In this work, we introduce ChiDeK (Chiral Determinant Kernels), a framework that systematically integrates stereogenic information into molecular representation learning. We propose the chiral determinant kernel to encode the SE(3)-invariant chirality matrix and employ cross-attention to integrate stereochemical information from local chiral centers into the global molecular representation. This design enables explicit modeling of chiral-related features within a unified architecture, capable of jointly encoding central and axial chirality. To support the evaluation of axial chirality, we construct a new benchmark for electronic circular dichroism (ECD) and optical rotation (OR) prediction. Across four tasks, including R/S configuration classification, enantiomer ranking, ECD spectrum prediction, and OR prediction, ChiDeK achieves substantial improvements over state-of-the-art baselines, most notably yielding over 7% higher accuracy on axially chiral tasks on average.

</details>


### [127] [Achieving Optimal Static and Dynamic Regret Simultaneously in Bandits with Deterministic Losses](https://arxiv.org/abs/2602.07418)
*Jian Qian,Chen-Yu Wei*

Main category: cs.LG

TL;DR: 本文证明了在对抗性多臂老虎机中，针对确定性损失和遗忘性对手，可以同时实现最优静态遗憾和动态遗憾，揭示了自适应对手与遗忘性对手在多重遗憾基准下的根本区别。


<details>
  <summary>Details</summary>
Motivation: 在对抗性多臂老虎机中，静态遗憾和动态遗憾是两个常用性能指标。虽然已有针对各自指标的最优算法，但尚无算法能同时实现两者的最优界。Marinov和Zimmert[2021]首次证明针对自适应对手无法实现同时最优性，本文旨在探索在确定性损失和遗忘性对手下是否可能实现同时最优。

Method: 首先将Marinov和Zimmert的不可能性结果扩展到确定性损失情况，然后提出新算法：利用负静态遗憾来补偿控制动态遗憾时的探索开销，并借助Blackwell可逼近性联合控制两种遗憾，从而设计出新的老虎机模型选择程序。

Result: 证明了在确定性损失和遗忘性对手下，可以同时实现最优静态遗憾和动态遗憾，揭示了自适应对手与遗忘性对手在多重遗憾基准下的根本分离，为同时实现不同切换次数基准的最优遗憾这一长期开放问题提供了新见解。

Conclusion: 本文首次展示了在对抗性多臂老虎机中，针对确定性损失和遗忘性对手，可以同时实现最优静态遗憾和动态遗憾，填补了该领域的重要空白，并为相关开放问题提供了新的解决思路。

Abstract: In adversarial multi-armed bandits, two performance measures are commonly used: static regret, which compares the learner to the best fixed arm, and dynamic regret, which compares it to the best sequence of arms. While optimal algorithms are known for each measure individually, there is no known algorithm achieving optimal bounds for both simultaneously. Marinov and Zimmert [2021] first showed that such simultaneous optimality is impossible against an adaptive adversary. Our work takes a first step to demonstrate its possibility against an oblivious adversary when losses are deterministic. First, we extend the impossibility result of Marinov and Zimmert [2021] to the case of deterministic losses. Then, we present an algorithm achieving optimal static and dynamic regret simultaneously against an oblivious adversary. Together, they reveal a fundamental separation between adaptive and oblivious adversaries when multiple regret benchmarks are considered simultaneously. It also provides new insight into the long open problem of simultaneously achieving optimal regret against switching benchmarks of different numbers of switches.
  Our algorithm uses negative static regret to compensate for the exploration overhead incurred when controlling dynamic regret, and leverages Blackwell approachability to jointly control both regrets. This yields a new model selection procedure for bandits that may be of independent interest.

</details>


### [128] [Sign-Based Optimizers Are Effective Under Heavy-Tailed Noise](https://arxiv.org/abs/2602.07425)
*Dingzhi Yu,Hongyi Tao,Yuanyu Wan,Luo Luo,Lijun Zhang*

Main category: cs.LG

TL;DR: 本文从重尾梯度噪声角度解释了符号优化器（如Lion、Muon）在训练大语言模型时优于AdamW等自适应方法的原因，提出了新的重尾噪声模型并建立了收敛理论。


<details>
  <summary>Details</summary>
Motivation: 符号优化算法（如Lion、Muon）在大语言模型训练中表现出优于AdamW等自适应梯度方法的经验性能，但其理论原因尚不清楚。本文旨在通过重尾梯度噪声这一在语言建模任务中常见的现象，填补理论与实践之间的鸿沟。

Method: 1. 提出新的广义重尾噪声条件，比标准有限方差假设更准确地捕捉LLM梯度行为；2. 在该噪声模型下，为广义光滑函数类建立SignSGD和Lion的尖锐收敛率；3. 将分析扩展到Muon和Muonlight，首次提供矩阵优化在重尾随机性下的严格分析；4. 通过LLM预训练实验验证理论见解。

Result: 1. 建立了符号优化器在重尾噪声下的收敛理论，匹配或超越了先前已知的最佳边界；2. 为Muon和Muonlight提供了首个重尾随机性下的严格分析；3. 实验验证了理论见解，表明提出的噪声模型与实践相符；4. 为符号优化器的经验优越性提供了强有力的理论依据。

Conclusion: 符号优化器在处理重尾梯度噪声方面具有天然优势，这解释了它们在大语言模型训练中优于自适应梯度方法的原因。提出的重尾噪声模型更准确地反映了LLM的实际梯度行为，为符号优化器的优越性能提供了坚实的理论基础。

Abstract: While adaptive gradient methods are the workhorse of modern machine learning, sign-based optimization algorithms such as Lion and Muon have recently demonstrated superior empirical performance over AdamW in training large language models (LLM). However, a theoretical understanding of why sign-based updates outperform variance-adapted methods remains elusive. In this paper, we aim to bridge the gap between theory and practice through the lens of heavy-tailed gradient noise, a phenomenon frequently observed in language modeling tasks. Theoretically, we introduce a novel generalized heavy-tailed noise condition that captures the behavior of LLMs more accurately than standard finite variance assumptions. Under this noise model, we establish sharp convergence rates of SignSGD and Lion for generalized smooth function classes, matching or surpassing previous best-known bounds. Furthermore, we extend our analysis to Muon and Muonlight, providing what is, to our knowledge, the first rigorous analysis of matrix optimization under heavy-tailed stochasticity. These results offer a strong theoretical justification for the empirical superiority of sign-based optimizers, showcasing that they are naturally suited to handle the noisy gradients associated with heavy tails. Empirically, LLM pretraining experiments validate our theoretical insights and confirm that our proposed noise models are well-aligned with practice.

</details>


### [129] [Brep2Shape: Boundary and Shape Representation Alignment via Self-Supervised Transformers](https://arxiv.org/abs/2602.07429)
*Yuanxu Sun,Yuezhou Ma,Haixu Wu,Guanyang Zeng,Muye Chen,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

TL;DR: Brep2Shape：一种自监督预训练方法，通过几何感知任务和对偶Transformer架构，将抽象的边界表示与直观的形状表示对齐，解决了CAD中B-rep模型的表示差距问题。


<details>
  <summary>Details</summary>
Motivation: 现有B-rep处理方法存在表示差距：连续方法具有分析精度但视觉抽象，离散方法直观清晰但几何精度不足。需要一种方法既能保持几何精度又能提供直观的形状理解。

Method: 提出Brep2Shape自监督预训练方法：1）几何感知任务：从参数化Bézier控制点预测密集空间点；2）双Transformer骨干网络：并行编码表面和曲线token以捕获不同几何特性；3）拓扑注意力：建模表面和曲线间的相互依赖关系以保持拓扑一致性。

Result: 实验结果表明Brep2Shape具有显著的可扩展性，在各种下游任务中实现了最先进的精度和更快的收敛速度。

Conclusion: Brep2Shape成功桥接了B-rep处理中的表示差距，通过将抽象边界表示与直观形状表示对齐，为CAD中的深度学习应用提供了更有效的解决方案。

Abstract: Boundary representation (B-rep) is the industry standard for computer-aided design (CAD). While deep learning shows promise in processing B-rep models, existing methods suffer from a representation gap: continuous approaches offer analytical precision but are visually abstract, whereas discrete methods provide intuitive clarity at the expense of geometric precision. To bridge this gap, we introduce Brep2Shape, a novel self-supervised pre-training method designed to align abstract boundary representations with intuitive shape representations. Our method employs a geometry-aware task where the model learns to predict dense spatial points from parametric Bézier control points, enabling the network to better understand physical manifolds derived from abstract coefficients. To enhance this alignment, we propose a Dual Transformer backbone with parallel streams that independently encode surface and curve tokens to capture their distinct geometric properties. Moreover, the topology attention is integrated to model the interdependencies between surfaces and curves, thereby maintaining topological consistency. Experimental results demonstrate that Brep2Shape offers significant scalability, achieving state-of-the-art accuracy and faster convergence across various downstream tasks.

</details>


### [130] [Active Learning Using Aggregated Acquisition Functions: Accuracy and Sustainability Analysis](https://arxiv.org/abs/2602.07440)
*Cédric Jung,Shirin Salehi,Anke Schmeink*

Main category: cs.LG

TL;DR: 本文提出六种聚合获取函数结构来解决主动学习中的探索-利用困境，在减少计算成本和标注样本的同时保持或提高模型精度，实现更可持续的AI。


<details>
  <summary>Details</summary>
Motivation: 主动学习通过选择信息量最大的样本进行标注来降低标注成本，但现有获取函数存在探索-利用困境：基于代表性的方法能探索数据集但不关注决策边界，而基于不确定性的方法专注于已识别的边界。需要解决这一困境，同时平衡精度与能耗，发展更可持续的AI。

Method: 提出六种聚合获取函数结构：串联、并联、混合、自适应反馈、随机探索和退火探索。这些结构结合不同获取函数（如BALD和BADGE）的优势，缓解批量模式低效和冷启动问题。在多种模型和数据集上评估这些结构。

Result: 聚合方法显著减少计算成本同时保持或提高精度。例如，BALD和BADGE交替使用表现稳健；K-Centers后接BALD的串联结构用少12%的样本达到相同性能，获取成本降低近一半。这些结构有效缓解了主动学习的常见问题。

Conclusion: 提出的聚合获取函数结构成功解决了主动学习中的探索-利用困境，在减少标注样本和计算能耗的同时保持模型性能，为实现更可持续、能源感知的人工智能提供了有效途径。

Abstract: Active learning (AL) is a machine learning (ML) approach that strategically selects the most informative samples for annotation during training, aiming to minimize annotation costs. This strategy not only reduces labeling expenses but also results in energy savings during neural network training, thereby enhancing both data and energy efficiency. In this paper, we implement and evaluate various state-of-the-art acquisition functions, analyzing their accuracy and computational costs, while discussing the advantages and disadvantages of each method. Our findings reveal that representativity-based acquisition functions effectively explore the dataset but do not prioritize boundary decisions, whereas uncertainty-based acquisition functions focus on refining boundary decisions already identified by the neural network. This trade-off is known as the exploration-exploitation dilemma. To address this dilemma, we introduce six aggregation structures: series, parallel, hybrid, adaptive feedback, random exploration, and annealing exploration. Our aggregated acquisition functions alleviate common AL pathologies such as batch mode inefficiency and the cold start problem. Additionally, we focus on balancing accuracy and energy consumption, contributing to the development of more sustainable, energy-aware artificial intelligence (AI). We evaluate our proposed structures on various models and datasets. Our results demonstrate the potential of these structures to reduce computational costs while maintaining or even improving accuracy. Innovative aggregation approaches, such as alternating between acquisition functions such as BALD and BADGE, have shown robust results. Sequentially running functions like $K$-Centers followed by BALD has achieved the same performance goals with up to 12\% fewer samples, while reducing the acquisition cost by almost half.

</details>


### [131] [Proximal Action Replacement for Behavior Cloning Actor-Critic in Offline Reinforcement Learning](https://arxiv.org/abs/2602.07441)
*Jinzong Dong,Wei Huang,Jianshu Zhang,Zhuo Chen,Xinzhe Yuan,Qinying Gu,Zhaohui Jiang,Nanyang Ye*

Main category: cs.LG

TL;DR: 本文提出PAR方法解决离线强化学习中行为克隆正则化导致的性能上限问题，通过渐进替换低价值动作为高价值动作来扩展探索空间


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中，行为克隆正则化虽然能产生现实策略并缓解分布外动作的偏差，但当数据集动作次优时，不加区分的模仿会结构性地阻止智能体充分利用评论家建议的高价值区域，尤其是在训练后期模仿已占主导时，这形成了性能上限

Method: 提出近端动作替换(PAR)，这是一种即插即用的训练样本替换器，逐步将低价值动作替换为由稳定智能体生成的高价值动作，从而扩展动作探索空间同时减少低价值数据的影响。PAR与多种行为克隆正则化范式兼容

Result: 在离线强化学习基准测试中的广泛实验表明，PAR能持续提升性能，当与基本的TD3+BC结合时能达到最先进水平

Conclusion: PAR方法有效解决了行为克隆正则化导致的性能上限问题，通过渐进替换机制实现了更好的离线强化学习性能

Abstract: Offline reinforcement learning (RL) optimizes policies from a previously collected static dataset and is an important branch of RL. A popular and promising approach is to regularize actor-critic methods with behavior cloning (BC), which yields realistic policies and mitigates bias from out-of-distribution actions, but can impose an often-overlooked performance ceiling: when dataset actions are suboptimal, indiscriminate imitation structurally prevents the actor from fully exploiting high-value regions suggested by the critic, especially in later training when imitation is already dominant. We formally analyzed this limitation by investigating convergence properties of BC-regularized actor-critic optimization and verified it on a controlled continuous bandit task. To break this ceiling, we propose proximal action replacement (PAR), a plug-and-play training sample replacer that progressively replaces low-value actions with high-value actions generated by a stable actor, broadening the action exploration space while reducing the impact of low-value data. PAR is compatible with multiple BC regularization paradigms. Extensive experiments across offline RL benchmarks show that PAR consistently improves performance and approaches state-of-the-art when combined with the basic TD3+BC.

</details>


### [132] [Data-Aware and Scalable Sensitivity Analysis for Decision Tree Ensembles](https://arxiv.org/abs/2602.07453)
*Namrita Varshney,Ashutosh Gupta,Arhaan Ahmad,Tanay V. Tayal,S. Akshay*

Main category: cs.LG

TL;DR: 提出数据感知的树集成模型特征敏感性分析框架，通过MILP和SMT编码生成接近训练分布的敏感示例，提高可解释性和实用性。


<details>
  <summary>Details</summary>
Motivation: 决策树集成模型在关键领域广泛应用，需要鲁棒性和敏感性分析来确保可信度。现有方法生成的敏感示例往往远离训练分布，限制了可解释性和实用价值。

Method: 提出数据感知敏感性框架，将敏感示例约束在接近数据集的位置；开发基于MILP和SMT编码的数据感知搜索技术；优化MILP算法加速敏感性验证。

Result: 证明了敏感性验证的NP-hard性即使在深度为1的树中也成立；实现了对多达800棵深度8的树集成模型的可扩展分析；在大型树集成上显著优于现有技术。

Conclusion: 该框架为高风险应用中基于树的模型的可靠性和公平性分析提供了实用基础，能生成更现实、可解释的模型弱点证据。

Abstract: Decision tree ensembles are widely used in critical domains, making robustness and sensitivity analysis essential to their trustworthiness. We study the feature sensitivity problem, which asks whether an ensemble is sensitive to a specified subset of features -- such as protected attributes -- whose manipulation can alter model predictions. Existing approaches often yield examples of sensitivity that lie far from the training distribution, limiting their interpretability and practical value. We propose a data-aware sensitivity framework that constrains the sensitive examples to remain close to the dataset, thereby producing realistic and interpretable evidence of model weaknesses. To this end, we develop novel techniques for data-aware search using a combination of mixed-integer linear programming (MILP) and satisfiability modulo theories (SMT) encodings. Our contributions are fourfold. First, we strengthen the NP-hardness result for sensitivity verification, showing it holds even for trees of depth 1. Second, we develop MILP-optimizations that significantly speed up sensitivity verification for single ensembles and for the first time can also handle multiclass tree ensembles. Third, we introduce a data-aware framework generating realistic examples close to the training distribution. Finally, we conduct an extensive experimental evaluation on large tree ensembles, demonstrating scalability to ensembles with up to 800 trees of depth 8, achieving substantial improvements over the state of the art. This framework provides a practical foundation for analyzing the reliability and fairness of tree-based models in high-stakes applications.

</details>


### [133] [On the Importance of a Multi-Scale Calibration for Quantization](https://arxiv.org/abs/2602.07465)
*Seungwoo Son,Ingyu Seong,Junhan Kim,Hyemi Jang,Yongkweon Jeon*

Main category: cs.LG

TL;DR: MaCa提出了一种长度感知的Hessian构建方法，通过多尺度序列长度信息改进LLM后训练量化效果


<details>
  <summary>Details</summary>
Motivation: 传统PTQ方法使用固定长度的随机序列作为校准集，忽略了LLM输入的可变长度特性。输入长度直接影响激活分布和Hessian捕获的权重重要性，固定长度校准得到的Hessian估计可能无法准确反映不同输入场景下的真实权重重要性。

Method: MaCa方法：(i) 将多尺度序列长度信息融入Hessian估计；(ii) 将每个序列作为独立样本进行正则化，从而获得更稳定、更有效的Hessian用于精确量化。

Result: 在Qwen3、Gemma3、LLaMA3等先进LLM上的实验表明，MaCa在低比特量化下能持续提升精度，提供轻量级增强且兼容现有PTQ框架。

Conclusion: 这是首个系统性地强调多尺度校准在LLM量化中作用的工作，MaCa通过长度感知的Hessian构建显著改善了量化性能。

Abstract: Post-training quantization (PTQ) is a cornerstone for efficiently deploying large language models (LLMs), where a small calibration set critically affects quantization performance. However, conventional practices rely on random sequences of fixed length, overlooking the variable-length nature of LLM inputs. Input length directly influences the activation distribution and, consequently, the weight importance captured by the Hessian, which in turn affects quantization outcomes. As a result, Hessian estimates derived from fixed-length calibration may fail to represent the true importance of weights across diverse input scenarios. We propose MaCa (Matryoshka Calibration), a simple yet effective method for length-aware Hessian construction. MaCa (i) incorporates multi-scale sequence length information into Hessian estimation and (ii) regularizes each sequence as an independent sample, yielding a more stable and fruitful Hessian for accurate quantization. Experiments on state-of-the-art LLMs (e.g., Qwen3, Gemma3, LLaMA3) demonstrate that MaCa consistently improves accuracy under low bit quantization, offering a lightweight enhancement compatible with existing PTQ frameworks. To the best of our knowledge, this is the first work to systematically highlight the role of multi-scale calibration in LLM quantization.

</details>


### [134] [Bandit Allocational Instability](https://arxiv.org/abs/2602.07472)
*Yilun Chen,Jiaqi Lu*

Main category: cs.LG

TL;DR: 本文提出了多臂老虎机算法的新性能指标"分配变异性"，建立了分配变异性与遗憾之间的基本权衡关系，并设计了可调算法UCB-f来实现帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 传统多臂老虎机算法在分配拉动次数时存在巨大变异性，这对现代应用如学习增强平台运营和后老虎机统计推断造成危害，因此需要引入新的性能指标来量化这种变异性。

Method: 引入分配变异性作为新性能指标，定义为各臂拉动次数标准偏差的最大值。建立分配变异性与遗憾之间的理论下界关系，并提出可调算法UCB-f（经典UCB1的推广）来实现帕累托前沿。

Result: 证明了任何算法的遗憾$R_T$与分配变异性$S_T$必须满足$R_T \cdot S_T=Ω(T^{\frac{3}{2}})$，表明最小最大遗憾最优算法必然产生最大尺度的分配变异性。UCB-f算法可以实现帕累托前沿$R_T \cdot S_T=\tildeΘ(T^{3/2})$的任何点。

Conclusion: 分配变异性与遗憾之间存在基本权衡关系，这一发现对平台运营和统计推断具有重要启示。同时解决了Praharaj和Khamaru（2025）的一个开放性问题。

Abstract: When multi-armed bandit (MAB) algorithms allocate pulls among competing arms, the resulting allocation can exhibit huge variation. This is particularly harmful in modern applications such as learning-enhanced platform operations and post-bandit statistical inference. Thus motivated, we introduce a new performance metric of MAB algorithms termed allocation variability, which is the largest (over arms) standard deviation of an arm's number of pulls. We establish a fundamental trade-off between allocation variability and regret, the canonical performance metric of reward maximization. In particular, for any algorithm, the worst-case regret $R_T$ and worst-case allocation variability $S_T$ must satisfy $R_T \cdot S_T=Ω(T^{\frac{3}{2}})$ as $T\rightarrow\infty$, as long as $R_T=o(T)$. This indicates that any minimax regret-optimal algorithm must incur worst-case allocation variability $Θ(T)$, the largest possible scale; while any algorithm with sublinear worst-case regret must necessarily incur ${S}_T= ω(\sqrt{T})$. We further show that this lower bound is essentially tight, and that any point on the Pareto frontier $R_T \cdot S_T=\tildeΘ(T^{3/2})$ can be achieved by a simple tunable algorithm UCB-f, a generalization of the classic UCB1. Finally, we discuss implications for platform operations and for statistical inference, when bandit algorithms are used. As a byproduct of our result, we resolve an open question of Praharaj and Khamaru (2025).

</details>


### [135] [Bipartite Graph Attention-based Clustering for Large-scale scRNA-seq Data](https://arxiv.org/abs/2602.07475)
*Zhuomin Liang,Liang Bai,Xian Yang*

Main category: cs.LG

TL;DR: BGFormer：基于二分图Transformer的单细胞RNA测序聚类模型，通过引入可学习的锚点标记实现线性计算复杂度，解决了传统Transformer方法在大型数据集上的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer的单细胞RNA测序聚类方法（如图Transformer）将每个细胞视为序列中的一个标记，其计算和空间复杂度为O(n²)，限制了在大规模数据集上的应用。需要一种更高效、可扩展的聚类方法。

Method: 提出BGFormer（二分图Transformer聚类模型）：1）引入一组可学习的锚点标记作为共享参考点来表示整个数据集；2）采用二分图注意力机制学习细胞与锚点标记之间的相似性；3）将同一类别的细胞在嵌入空间中拉近；4）实现相对于细胞数量的线性计算复杂度。

Result: 在多个大规模单细胞RNA测序数据集上的实验结果表明，BGFormer具有有效性和可扩展性，能够高效处理大规模数据集。

Conclusion: BGFormer通过二分图注意力机制和锚点标记设计，成功解决了传统Transformer方法在大规模单细胞RNA测序聚类中的计算复杂度问题，为大规模生物信息学分析提供了高效解决方案。

Abstract: scRNA-seq clustering is a critical task for analyzing single-cell RNA sequencing (scRNA-seq) data, as it groups cells with similar gene expression profiles. Transformers, as powerful foundational models, have been applied to scRNA-seq clustering. Their self-attention mechanism automatically assigns higher attention weights to cells within the same cluster, enhancing the distinction between clusters. Existing methods for scRNA-seq clustering, such as graph transformer-based models, treat each cell as a token in a sequence. Their computational and space complexities are $\mathcal{O}(n^2)$ with respect to the number of cells, limiting their applicability to large-scale scRNA-seq datasets.To address this challenge, we propose a Bipartite Graph Transformer-based clustering model (BGFormer) for scRNA-seq data. We introduce a set of learnable anchor tokens as shared reference points to represent the entire dataset. A bipartite graph attention mechanism is introduced to learn the similarity between cells and anchor tokens, bringing cells of the same class closer together in the embedding space. BGFormer achieves linear computational complexity with respect to the number of cells, making it scalable to large datasets. Experimental results on multiple large-scale scRNA-seq datasets demonstrate the effectiveness and scalability of BGFormer.

</details>


### [136] [AI-Driven Predictive Modelling for Groundwater Salinization in Israel](https://arxiv.org/abs/2602.07478)
*Laxmi Pandey,Ariel Meroz,Ben Cheng,Ankita Manekar,Abhijit Mukherjee,Meirav Cohen,Adway Mitra*

Main category: cs.LG

TL;DR: 该研究整合多源数据，采用多种机器学习模型（RF、XGBoost、NN、LSTM、CNN、LR）预测以色列地下水盐度，结合特征选择、敏感性分析和可解释AI识别关键驱动因素。


<details>
  <summary>Details</summary>
Motivation: 全球许多地区地下水盐度增加和污染问题严重，导致水资源退化。需要全面理解地下水盐化的潜在因果因素，识别重要的气象、地质和人为驱动因素。

Method: 整合不同潜在协变量数据集，建立机器学习预测模型框架（包括RF、XGBoost、NN、LSTM、CNN、LR）。使用递归特征消除（RFE）、全局敏感性分析（GSA）和可解释AI（XAI）的SHAP方法评估特征重要性。通过双机器学习进行因果分析。

Result: 识别出以色列地下水盐度的关键驱动因素：气象因素（降水、温度）、地质因素（距河流距离、距盐体距离、地形湿度指数、海岸线距离）和人为因素（农田面积、处理废水）。XAI分析特别指出处理废水是重要的人为驱动因素。

Conclusion: 该方法为国家尺度全球盐化机制提供了深入见解，减少了AI模型不确定性，并强调需要针对性的策略来解决盐度问题。处理废水在脆弱水文气候环境中是关键驱动因素。

Abstract: Increasing salinity and contamination of groundwater is a serious issue in many parts of the world, causing degradation of water resources. The aim of this work is to form a comprehensive understanding of groundwater salinization underlying causal factors and identify important meteorological, geological and anthropogenic drivers of salinity. We have integrated different datasets of potential covariates, to create a robust framework for machine learning based predictive models including Random Forest (RF), XGBoost, Neural network, Long Short-Term Memory (LSTM), convolution neural network (CNN) and linear regression (LR), of groundwater salinity. Additionally, Recursive Feature Elimination (RFE) followed by Global sensitivity analysis (GSA) and Explainable AI (XAI) based SHapley Additive exPlanations (SHAP) were used to estimate the importance scores and find insights into the drivers of salinization. We also did causality analysis via Double machine learning using various predictive models. From these analyses, key meteorological (Precipitation, Temperature), geological (Distance from river, Distance to saline body, TWI, Shoreline distance), and anthropogenic (Area of agriculture field, Treated Wastewater) covariates are identified to be influential drivers of groundwater salinity across Israel. XAI analysis also identified Treated Wastewater (TWW) as an essential anthropogenic driver of salinity, its significance being context-dependent but critical in vulnerable hydro-climatic environment. Our approach provides deeper insight into global salinization mechanisms at country scale, reducing AI model uncertainty and highlighting the need for tailored strategies to address salinity.

</details>


### [137] [ODELoRA: Training Low-Rank Adaptation by Solving Ordinary Differential Equations](https://arxiv.org/abs/2602.07479)
*Yihang Gao,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 提出ODELoRA方法，通过常微分方程优化LoRA因子矩阵，模拟完整微调在平衡流形上的梯度流，提升训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA训练方法将低秩因子矩阵分开优化，未能充分利用LoRA参数化的内在结构，导致理论和实践上的次优性能。需要一种更有效的优化方法来提升LoRA的训练效果。

Method: 提出ODELoRA方法：1) 构建连续时间优化动态，用ODE模拟完整微调在平衡流形上的梯度流；2) 采用欧拉和龙格-库塔等时间离散化方案跟踪轨迹；3) 提供统一的ODE视角理解和设计LoRA训练算法。

Result: 1) 理论证明：在强凸目标下，某些离散化方案具有线性收敛性；2) 矩阵感知任务验证线性收敛行为；3) 物理信息神经网络训练显示优于现有基线，特别是在训练稳定性方面；4) 实现稳定的特征学习。

Conclusion: ODELoRA通过ODE框架优化LoRA因子矩阵，提供了更有效的训练方法，在理论和实验上都表现出优越性，特别是在训练稳定性方面，为LoRA训练算法设计提供了新视角。

Abstract: Low-rank adaptation (LoRA) has emerged as a widely adopted parameter-efficient fine-tuning method in deep transfer learning, due to its reduced number of trainable parameters and lower memory requirements enabled by Burer-Monteiro factorization on adaptation matrices. However, classical LoRA training methods treat the low-rank factor matrices individually and optimize them using standard gradient-based algorithms. Such decoupled optimization schemes are theoretically and empirically suboptimal, as they fail to fully exploit the intrinsic structure of the LoRA parameterization. In this work, we propose a novel continuous-time optimization dynamic for LoRA factor matrices in the form of an ordinary differential equation (ODE) that emulates the gradient flow of full fine-tuning on the balanced manifold. We term this approach ODELoRA. To faithfully track the trajectories of ODELoRA, we adopt well-established and theoretically grounded time-discretization schemes, including Euler and Runge--Kutta methods. Our framework provides a unified ODE-based perspective for understanding and designing LoRA training algorithms. We establish linear convergence of the proposed method under strongly convex objectives for certain discretization schemes under mild conditions, and further extend our analysis to the matrix sensing setting. Moreover, we show that ODELoRA achieves stable feature learning, a property that is crucial for training deep neural networks at different scales of problem dimensionality. Empirical results on matrix sensing tasks confirm the derived linear convergence behavior, and experiments on training physics-informed neural networks further demonstrate the superiority of ODELoRA over existing baselines, especially in the training stability.

</details>


### [138] [Deriving Neural Scaling Laws from the statistics of natural language](https://arxiv.org/abs/2602.07488)
*Francesco Cagnetta,Allan Raventós,Surya Ganguli,Matthieu Wyart*

Main category: cs.LG

TL;DR: 本文提出了首个能够定量预测大语言模型数据受限缩放定律指数的理论，该理论基于语言的两个关键统计特性，无需自由参数或合成数据模型。


<details>
  <summary>Details</summary>
Motivation: 尽管实验神经缩放定律极大地指导了大规模机器学习的实证进展，但现有理论无法定量预测任何现代LLM在任何自然语言数据集上的这些重要定律的指数。本文旨在填补这一理论空白。

Method: 通过分析语言的两个关键统计特性：(i) 标记对相关性随时间间隔的衰减，(ii) 下一标记条件熵随上下文长度的衰减，推导出一个简单公式来预测数据受限神经缩放指数。

Result: 该理论在GPT-2和LLaMA风格模型在TinyStories和WikiText两个不同基准上的训练实验中表现出与实验测量神经缩放定律的显著匹配。

Conclusion: 本文首次提供了能够从第一性原理定量预测数据受限神经缩放指数的理论，揭示了语言统计特性与缩放定律之间的根本联系，为理解大规模语言模型训练提供了理论基础。

Abstract: Despite the fact that experimental neural scaling laws have substantially guided empirical progress in large-scale machine learning, no existing theory can quantitatively predict the exponents of these important laws for any modern LLM trained on any natural language dataset. We provide the first such theory in the case of data-limited scaling laws. We isolate two key statistical properties of language that alone can predict neural scaling exponents: (i) the decay of pairwise token correlations with time separation between token pairs, and (ii) the decay of the next-token conditional entropy with the length of the conditioning context. We further derive a simple formula in terms of these statistics that predicts data-limited neural scaling exponents from first principles without any free parameters or synthetic data models. Our theory exhibits a remarkable match with experimentally measured neural scaling laws obtained from training GPT-2 and LLaMA style models from scratch on two qualitatively different benchmarks, TinyStories and WikiText.

</details>


### [139] [Hyperparameter Transfer Laws for Non-Recurrent Multi-Path Neural Networks](https://arxiv.org/abs/2602.07494)
*Shenxi Wu,Haosong Zhang,Xingjian Ma,Shirui Bian,Yichi Zhang,Xi Chen,Wei Lin*

Main category: cs.LG

TL;DR: 该论文提出基于图的有效深度概念，发现在稳定初始化和最大更新准则下，最优学习率随有效深度按-3/2幂律衰减，实现了跨深度和宽度的零样本超参数迁移。


<details>
  <summary>Details</summary>
Motivation: 现代深度架构训练成本高昂，需要跨宽度和深度的超参数迁移。虽然μP解释了宽度缩放下的超参数迁移，但深度缩放对于包含并行路径和残差聚合的现代架构（如CNN、ResNet、Transformer）理解不足。

Method: 引入基于图的有效深度概念来统一各种非循环多路径神经网络；在稳定初始化和最大更新准则下，推导出最优学习率随有效深度的-3/2幂律衰减关系；通过实验验证理论预测。

Result: 实验证实了-3/2幂律斜率，实现了跨深度和宽度的可靠零样本学习率迁移，将深度缩放转化为可预测的超参数迁移问题。

Conclusion: 基于有效深度和最大更新准则的理论框架成功解释了深度缩放下的超参数迁移规律，为现代多路径神经网络提供了统一的深度缩放理论。

Abstract: Deeper modern architectures are costly to train, making hyperparameter transfer preferable to expensive repeated tuning. Maximal Update Parametrization ($μ$P) helps explain why many hyperparameters transfer across width. Yet depth scaling is less understood for modern architectures, whose computation graphs contain multiple parallel paths and residual aggregation. To unify various non-recurrent multi-path neural networks such as CNNs, ResNets, and Transformers, we introduce a graph-based notion of effective depth. Under stabilizing initializations and a maximal-update criterion, we show that the optimal learning rate decays with effective depth following a universal -3/2 power law. Here, the maximal-update criterion maximizes the typical one-step representation change at initialization without causing instability, and effective depth is the minimal path length from input to output, counting layers and residual additions. Experiments across diverse architectures confirm the predicted slope and enable reliable zero-shot transfer of learning rates across depths and widths, turning depth scaling into a predictable hyperparameter-transfer problem.

</details>


### [140] [CoMI-IRL: Contrastive Multi-Intention Inverse Reinforcement Learning](https://arxiv.org/abs/2602.07496)
*Antonio Mone,Frans A. Oliehoek,Luciano Cavalcante Siebert*

Main category: cs.LG

TL;DR: CoMI-IRL：基于Transformer的无监督多意图逆强化学习框架，将行为表示与聚类从奖励学习中解耦，无需先验知识即可适应新行为


<details>
  <summary>Details</summary>
Motivation: 现有深度生成MI-IRL方法需要先验知识了解真实行为模式数量K*，依赖专家知识限制了适应新行为的能力，且只能分析与学习奖励相关的内容，无法跨训练行为模式进行分析

Method: 提出对比多意图逆强化学习(CoMI-IRL)，基于Transformer的无监督框架，将行为表示和聚类与下游奖励学习解耦

Result: CoMI-IRL在无需K*先验知识或标签的情况下优于现有方法，同时允许行为关系的可视化解释，并能适应未见行为而无需完全重新训练

Conclusion: CoMI-IRL通过解耦行为表示和奖励学习，解决了传统MI-IRL方法对先验知识的依赖问题，提高了适应性和可解释性

Abstract: Inverse Reinforcement Learning (IRL) seeks to infer reward functions from expert demonstrations. When demonstrations originate from multiple experts with different intentions, the problem is known as Multi-Intention IRL (MI-IRL). Recent deep generative MI-IRL approaches couple behavior clustering and reward learning, but typically require prior knowledge of the number of true behavioral modes $K^*$. This reliance on expert knowledge limits their adaptability to new behaviors, and only enables analysis related to the learned rewards, and not across the behavior modes used to train them. We propose Contrastive Multi-Intention IRL (CoMI-IRL), a transformer-based unsupervised framework that decouples behavior representation and clustering from downstream reward learning. Our experiments show that CoMI-IRL outperforms existing approaches without a priori knowledge of $K^*$ or labels, while allowing for visual interpretation of behavior relationships and adaptation to unseen behavior without full retraining.

</details>


### [141] [PALMS: Pavlovian Associative Learning Models Simulator](https://arxiv.org/abs/2602.07519)
*Martin Fixman,Alessandro Abati,Julián Jiménez Nimmo,Sean Lim,Esther Mondragón*

Main category: cs.LG

TL;DR: PALMS是一个用于模拟巴甫洛夫条件反射实验的Python仿真环境，集成了多种注意力学习模型，支持大规模刺激模拟和配置线索计算。


<details>
  <summary>Details</summary>
Motivation: 仿真在理论发展和完善过程中至关重要，但现有工具可能无法充分支持复杂的巴甫洛夫条件反射实验模拟，特别是涉及大量刺激和配置线索的情况。

Method: 开发了PALMS仿真器，包含经典Rescorla-Wagner模型和多种注意力学习模型（Pearce-Kaye-Hall、Mackintosh Extended、Le Pelley's Hybrid等），并提出了一个统一可变学习率的Rescorla-Wagner扩展模型。提供图形界面支持实验设计输入，支持数百个刺激的模拟和配置线索计算。

Result: PALMS能够高效运行，即时可视化结果，支持不同模型预测的快速精确比较。图形显示可轻松保存，模拟数据可导出到电子表格。通过复现已发表的关联学习实验验证了软件功能。

Conclusion: PALMS作为一个开源仿真环境，显著扩展了关联学习模型的预测能力，为研究人员提供了一个统一、高效的平台来比较和验证不同学习理论。

Abstract: Simulations are an indispensable step in the cycle of theory development and refinement, helping researchers formulate precise definitions, generate models, and make accurate predictions. This paper introduces the Pavlovian Associative Learning Models Simulator (PALMS), a Python environment to simulate Pavlovian conditioning experiments. In addition to the canonical Rescorla-Wagner model, PALMS incorporates several attentional learning approaches, including Pearce-Kaye-Hall, Mackintosh Extended, Le Pelley's Hybrid, and a novel extension of the Rescorla-Wagner model with a unified variable learning rate that integrates Mackintosh's and Pearce and Hall's opposing conceptualisations. The simulator's graphical interface allows for the input of entire experimental designs in an alphanumeric format, akin to that used by experimental neuroscientists. Moreover, it uniquely enables the simulation of experiments involving hundreds of stimuli, as well as the computation of configural cues and configural-cue compounds across all models, thereby considerably expanding their predictive capabilities. PALMS operates efficiently, providing instant visualisation of results, supporting rapid, precise comparisons of various models' predictions within a single architecture and environment. Furthermore, graphic displays can be easily saved, and simulated data can be exported to spreadsheets. To illustrate the simulator's capabilities and functionalities, we provide a detailed description of the software and examples of use, reproducing published experiments in the associative learning literature. PALMS is licensed under the open-source GNU Lesser General Public License 3.0. The simulator source code and the latest multiplatform release build are accessible as a GitHub repository at https://github.com/cal-r/PALMS-Simulator

</details>


### [142] [Pareto-guided Pipeline for Distilling Featherweight AI Agents in Mobile MOBA Games](https://arxiv.org/abs/2602.07521)
*Xionghui Yang,Bozhou Chen,Yunlong Lu,Yongyi Wang,Lingfeng Li,Lanxiao Huang,Lin Liu,Wenjun Wang,Meng Meng,Xia Lin,Wenxin Li*

Main category: cs.LG

TL;DR: 该论文提出了一种针对移动设备部署的王者荣耀游戏AI蒸馏方法，通过帕累托最优引导的流水线和高效学生架构搜索空间，在保持40.32%胜率的同时实现了12.4倍推理加速和15.6倍能效提升。


<details>
  <summary>Details</summary>
Motivation: 虽然游戏AI已能在复杂MOBA游戏（如王者荣耀）中超越人类顶级玩家，但将这些强大的智能体部署到移动设备上面临巨大挑战：复杂的多模态状态表示和分层动作空间需要大型策略网络，难以压缩为轻量形式；而生产部署需要在移动平台上满足严格的能耗和延迟约束。

Method: 提出帕累托最优引导的流水线，设计专门针对移动执行的高效学生架构搜索空间，系统探索性能与效率之间的权衡。通过知识蒸馏将大型教师模型压缩为适合移动设备部署的轻量学生模型。

Result: 蒸馏后的模型实现了显著效率提升：推理速度加快12.4倍（每帧低于0.5ms），能效提升15.6倍（每局游戏低于0.5mAh），同时保持40.32%的胜率对抗原始教师模型。

Conclusion: 该工作首次系统研究了大规模游戏AI与实用设备部署之间的桥梁，提出的方法成功解决了移动设备部署游戏AI的效率和性能平衡问题，为实际应用提供了可行解决方案。

Abstract: Recent advances in game AI have demonstrated the feasibility of training agents that surpass top-tier human professionals in complex environments such as Honor of Kings (HoK), a leading mobile multiplayer online battle arena (MOBA) game. However, deploying such powerful agents on mobile devices remains a major challenge. On one hand, the intricate multi-modal state representation and hierarchical action space of HoK demand large, sophisticated policy networks that are inherently difficult to compress into lightweight forms. On the other hand, production deployment requires high-frequency inference under strict energy and latency constraints on mobile platform. To the best of our knowledge, bridging large-scale game AI and practical on-device deployment has not been systematically studied. In this work, we propose a Pareto optimality guided pipeline and design a high-efficiency student architecture search space tailored for mobile execution, enabling systematic exploration of the trade-off between performance and efficiency. Experimental results demonstrate that the distilled model achieves remarkable efficiency, including an $12.4\times$ faster inference speed (under 0.5ms per frame) and a $15.6\times$ improvement in energy efficiency (under 0.5mAh per game), while retaining a 40.32% win rate against the original teacher model.

</details>


### [143] [MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution](https://arxiv.org/abs/2602.07529)
*Jianwen Chen,Xinyu Yang,Peng Xia,Arian Azarang,Yueh Z Lee,Gang Li,Hongtu Zhu,Yun Li,Beidi Chen,Huaxiu Yao*

Main category: cs.LG

TL;DR: MedVerse：基于Petri网理论将医学推理重构为可并行化的有向无环图过程，通过拓扑感知注意力机制和定制推理引擎实现并行解码，在提升性能的同时显著降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）的顺序自回归解码方式将本应并行的临床推理（如鉴别诊断）强制压缩为单一线性推理路径，限制了复杂医学问题的推理效率和可靠性。

Method: 1. 数据层面：MedVerse Curator自动化管道合成基于知识的医学推理路径并转换为Petri网结构表示；2. 架构层面：提出拓扑感知注意力机制，支持并行推理同时保持逻辑一致性；3. 系统层面：开发定制推理引擎支持无额外开销的并行执行。

Result: MedVerse将通用LLMs性能提升高达8.9%；与专用医学LLMs相比，在保持相当性能的同时，推理延迟降低1.3倍，生成吞吐量提高1.7倍。

Conclusion: MedVerse通过将医学推理重构为可并行化的DAG过程，有效解决了传统LLMs在医学推理中的线性限制，在提升推理性能的同时显著改善了计算效率。

Abstract: Large language models (LLMs) have demonstrated strong performance and rapid progress in a wide range of medical reasoning tasks. However, their sequential autoregressive decoding forces inherently parallel clinical reasoning, such as differential diagnosis, into a single linear reasoning path, limiting both efficiency and reliability for complex medical problems. To address this, we propose MedVerse, a reasoning framework for complex medical inference that reformulates medical reasoning as a parallelizable directed acyclic graph (DAG) process based on Petri net theory. The framework adopts a full-stack design across data, model architecture, and system execution. For data creation, we introduce the MedVerse Curator, an automated pipeline that synthesizes knowledge-grounded medical reasoning paths and transforms them into Petri net-structured representations. At the architectural level, we propose a topology-aware attention mechanism with adaptive position indices that supports parallel reasoning while preserving logical consistency. Systematically, we develop a customized inference engine that supports parallel execution without additional overhead. Empirical evaluations show that MedVerse improves strong general-purpose LLMs by up to 8.9%. Compared to specialized medical LLMs, MedVerse achieves comparable performance while delivering a 1.3x reduction in inference latency and a 1.7x increase in generation throughput, enabled by its parallel decoding capability.

</details>


### [144] [Compact Conformal Subgraphs](https://arxiv.org/abs/2602.07530)
*Sreenivas Gollapudi,Kostas Kollias,Kamesh Munagala,Aravindan Vijayaraghavan*

Main category: cs.LG

TL;DR: 提出图基保形压缩框架，通过组合优化构建紧凑子图，在保持统计有效性的同时减少结构化预测集的大小


<details>
  <summary>Details</summary>
Motivation: 传统保形预测在结构化领域（如路由、规划、序列推荐）产生的预测集过大，需要压缩方法在保持统计保证的同时减少结构复杂性

Method: 将压缩问题形式化为选择捕获规定概率质量的最小子图，归约为超图中加权最密k子图问题，设计高效近似算法，利用参数最小割的单调性保证嵌套性

Result: 设计了实现常数因子覆盖率和大小权衡的高效近似算法，证明了松弛满足单调性从而保证有效保形保证，在模拟实验中验证了方法在行程规划和导航中的有效性

Conclusion: 该框架将高效保形预测与组合图压缩通过单调性连接，提供统计有效性和压缩大小的严格保证，同时揭示了与经典最密k子图问题不同的可高效近似的算法机制

Abstract: Conformal prediction provides rigorous, distribution-free uncertainty guarantees, but often yields prohibitively large prediction sets in structured domains such as routing, planning, or sequential recommendation. We introduce "graph-based conformal compression", a framework for constructing compact subgraphs that preserve statistical validity while reducing structural complexity. We formulate compression as selecting a smallest subgraph capturing a prescribed fraction of the probability mass, and reduce to a weighted version of densest $k$-subgraphs in hypergraphs, in the regime where the subgraph has a large fraction of edges. We design efficient approximation algorithms that achieve constant factor coverage and size trade-offs. Crucially, we prove that our relaxation satisfies a monotonicity property, derived from a connection to parametric minimum cuts, which guarantees the nestedness required for valid conformal guarantees. Our results on the one hand bridge efficient conformal prediction with combinatorial graph compression via monotonicity, to provide rigorous guarantees on both statistical validity, and compression or size. On the other hand, they also highlight an algorithmic regime, distinct from classical densest-$k$-subgraph hardness settings, where the problem can be approximated efficiently. We finally validate our algorithmic approach via simulations for trip planning and navigation, and compare to natural baselines.

</details>


### [145] [Gaussian Match-and-Copy: A Minimalist Benchmark for Studying Transformer Induction](https://arxiv.org/abs/2602.07562)
*Antoine Gonon,Alexandre Cordonnier,Nicolas Boumal*

Main category: cs.LG

TL;DR: 论文提出了Gaussian Match-and-Copy基准，通过纯二阶相关信号隔离长程检索，研究Transformer中match-and-copy电路的形成机制和优化动态。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型中match-and-copy检索原语如何在自然数据上涌现具有挑战性，因为检索和记忆是纠缠在一起的。需要分离这两种机制来深入研究检索能力。

Method: 引入Gaussian Match-and-Copy基准，使用纯二阶相关信号隔离长程检索；分析Transformer在该任务上的表现；在简化注意力设置中研究优化动态，特别是梯度下降的隐式偏置。

Result: GMC基准保留了Transformer实际开发match-and-copy电路的关键定性方面，并能区分不同架构的检索能力。在特定条件下，梯度下降驱动参数发散同时方向与最大间隔分离器对齐，产生硬匹配选择。

Conclusion: 通过GMC基准可以分离检索和记忆，深入理解match-and-copy电路的涌现机制。梯度下降在特定条件下表现出最大间隔对齐的隐式偏置，解释了硬匹配选择行为的形成。

Abstract: Match-and-copy is a core retrieval primitive used at inference time by large language models to retrieve a matching token from the context then copy its successor. Yet, understanding how this behavior emerges on natural data is challenging because retrieval and memorization are entangled. To disentangle the two, we introduce Gaussian Match-and-Copy (GMC), a minimalist benchmark that isolates long-range retrieval through pure second-order correlation signals. Numerical investigations show that this task retains key qualitative aspects of how Transformers develop match-and-copy circuits in practice, and separates architectures by their retrieval capabilities. We also analyze the optimization dynamics in a simplified attention setting. Although many solutions are a priori possible under a regression objective, including ones that do not implement retrieval, we identify an implicit-bias regime in which gradient descent drives the parameters to diverge while their direction aligns with the max-margin separator, yielding hard match selection. We prove this max-margin alignment for GD trajectories that reach vanishing empirical loss under explicit technical conditions.

</details>


### [146] [Enhancing Time Series Classification with Diversity-Driven Neural Network Ensembles](https://arxiv.org/abs/2602.07579)
*Javidan Abdullayev,Maxime Devanne,Cyril Meyer,Ali Ismail-Fawaz,Jonathan Weber,Germain Forestier*

Main category: cs.LG

TL;DR: 提出一种基于特征正交性损失的时间序列分类多样性驱动集成学习框架，通过促进神经网络集成成员的特征多样性来提升性能


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分类的神经网络集成方法通常使用相同架构和配置训练多个模型，导致特征表示冗余，限制了集成效果。需要显式促进集成成员间的特征多样性来提升性能。

Method: 采用基于特征正交性损失的去相关学习策略，直接应用于学习到的特征表示，确保集成中的每个模型捕获互补而非冗余的信息。

Result: 在UCR档案的128个数据集上评估，实现了最先进的性能，且使用更少的模型，相比传统神经网络集成方法更高效和可扩展。

Conclusion: 提出的多样性驱动集成学习框架通过显式促进特征多样性，在时间序列分类任务中实现了更好的性能，同时提高了效率和可扩展性。

Abstract: Ensemble methods have played a crucial role in achieving state-of-the-art (SOTA) performance across various machine learning tasks by leveraging the diversity of features learned by individual models. In Time Series Classification (TSC), ensembles have proven highly effective whether based on neural networks (NNs) or traditional methods like HIVE-COTE. However most existing NN-based ensemble methods for TSC train multiple models with identical architectures and configurations. These ensembles aggregate predictions without explicitly promoting diversity which often leads to redundant feature representations and limits the benefits of ensembling. In this work, we introduce a diversity-driven ensemble learning framework that explicitly encourages feature diversity among neural network ensemble members. Our approach employs a decorrelated learning strategy using a feature orthogonality loss applied directly to the learned feature representations. This ensures that each model in the ensemble captures complementary rather than redundant information. We evaluate our framework on 128 datasets from the UCR archive and show that it achieves SOTA performance with fewer models. This makes our method both efficient and scalable compared to conventional NN-based ensemble approaches.

</details>


### [147] [Unified Biomolecular Trajectory Generation via Pretrained Variational Bridge](https://arxiv.org/abs/2602.07588)
*Ziyang Yu,Wenbing Huang,Yang Liu*

Main category: cs.LG

TL;DR: PVB是一种预训练变分桥模型，通过编码器-解码器架构将初始结构映射到噪声潜在空间，并利用增强桥匹配技术生成分子动力学轨迹，在保持热力学和动力学特性的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统分子动力学模拟计算成本高昂，现有深度生成模型要么泛化能力差，要么因轨迹数据多样性有限而无法充分利用结构信息来提高生成保真度。

Method: 提出预训练变分桥(PVB)模型，采用编码器-解码器架构：将初始结构映射到噪声潜在空间，通过增强桥匹配技术向阶段特定目标传输；统一训练单结构和配对轨迹数据；针对蛋白质-配体复合物引入基于强化学习的伴随匹配优化。

Result: 实验表明PVB能够忠实复现分子动力学的热力学和动力学可观测量，同时提供稳定高效的生成动力学；对于蛋白质-配体复合物，支持对接构象的高效后优化。

Conclusion: PVB通过统一训练框架有效利用跨域结构知识，在保持分子动力学模拟准确性的同时显著提升计算效率，为蛋白质-配体复合物对接优化提供了有效工具。

Abstract: Molecular Dynamics (MD) simulations provide a fundamental tool for characterizing molecular behavior at full atomic resolution, but their applicability is severely constrained by the computational cost. To address this, a surge of deep generative models has recently emerged to learn dynamics at coarsened timesteps for efficient trajectory generation, yet they either generalize poorly across systems or, due to limited molecular diversity of trajectory data, fail to fully exploit structural information to improve generative fidelity. Here, we present the Pretrained Variational Bridge (PVB) in an encoder-decoder fashion, which maps the initial structure into a noised latent space and transports it toward stage-specific targets through augmented bridge matching. This unifies training on both single-structure and paired trajectory data, enabling consistent use of cross-domain structural knowledge across training stages. Moreover, for protein-ligand complexes, we further introduce a reinforcement learning-based optimization via adjoint matching that speeds progression toward the holo state, which supports efficient post-optimization of docking poses. Experiments on proteins and protein-ligand complexes demonstrate that PVB faithfully reproduces thermodynamic and kinetic observables from MD while delivering stable and efficient generative dynamics.

</details>


### [148] [Beyond Arrow: From Impossibility to Possibilities in Multi-Criteria Benchmarking](https://arxiv.org/abs/2602.07593)
*Polina Gordienko,Christoph Jansen,Julian Rodemann,Georg Schollmeyer*

Main category: cs.LG

TL;DR: 论文将多指标基准测试建模为社会选择问题，证明了在特定偏好结构下单峰性、群可分性和距离限制条件下可以构建良好模型排名


<details>
  <summary>Details</summary>
Motivation: 现代基准测试如HELM MMLU包含多个指标（准确性、鲁棒性、效率等），但将这些指标聚合成单一排名时，自然聚合方法可能变得不一致或不稳定。需要解决多指标基准测试的排名聚合问题。

Method: 将基准测试建模为社会选择问题：每个指标在数据集上诱导出模型偏好排序，基准算子聚合这些"投票"。研究三种偏好结构限制条件：单峰偏好、群可分偏好和距离限制偏好，证明在这些条件下可以构建良好模型排名。

Result: 理论证明在单峰性、群可分性和距离限制条件下，基准算子允许构建行为良好的模型排名。实证研究验证了HELM MMLU等现代基准套件中哪些结构条件在哪些基准问题上得到满足。

Conclusion: 多指标基准测试的排名聚合问题可以通过社会选择理论解决，当偏好满足特定结构条件时，可以避免Arrow不可能定理的病理情况，实现有意义的多标准基准测试。

Abstract: Modern benchmarks such as HELM MMLU account for multiple metrics like accuracy, robustness and efficiency. When trying to turn these metrics into a single ranking, natural aggregation procedures can become incoherent or unstable to changes in the model set. We formalize this aggregation as a social choice problem where each metric induces a preference ranking over models on each dataset, and a benchmark operator aggregates these votes across metrics. While prior work has focused on Arrow's impossibility result, we argue that the impossibility often originates from pathological examples and identify sufficient conditions under which these disappear, and meaningful multi-criteria benchmarking becomes possible. In particular, we deal with three restrictions on the combinations of rankings and prove that on single-peaked, group-separable and distance-restricted preferences, the benchmark operator allows for the construction of well-behaved rankings of the involved models. Empirically, we investigate several modern benchmark suites like HELM MMLU and verify which structural conditions are fulfilled on which benchmark problems.

</details>


### [149] [Astro: Activation-guided Structured Regularization for Outlier-Robust LLM Post-Training Quantization](https://arxiv.org/abs/2602.07596)
*Xi Chen,Ming Li,Junxi Li,Changsheng Li,Peisong Wang,Lizhong Ding,Ye Yuan,Guoren Wang*

Main category: cs.LG

TL;DR: Astro是一个激活引导的结构化正则化框架，用于抑制LLM量化中的异常值，实现零推理延迟的高效权重后训练量化


<details>
  <summary>Details</summary>
Motivation: 现有的权重后训练量化方法在处理权重和激活异常值时存在局限性：要么抑制效果不足，要么导致显著的部署效率问题（如推理延迟、繁重的预处理或复杂的算子融合）

Method: 利用LLM收敛到平坦最小值的特点，提出Astro框架，通过激活引导的正则化目标主动重构内在鲁棒的权重，抑制与高幅度激活对应的权重异常值，同时保持模型精度

Result: 在LLaMA-2-7B上，Astro实现了比复杂学习型旋转方法更好的性能，且量化时间仅为后者的约1/3，同时保持零推理延迟

Conclusion: Astro提供了一种硬件友好且高效的异常值抑制方案，与主流量化方法正交，显著提升了权重后训练量化的性能与效率

Abstract: Weight-only post-training quantization (PTQ) is crucial for efficient Large Language Model (LLM) deployment but suffers from accuracy degradation caused by weight and activation outliers. Existing mitigation strategies often face critical limitations: they either yield insufficient outlier suppression or incur significant deployment inefficiencies, such as inference latency, heavy preprocessing, or reliance on complex operator fusion. To resolve these limitations, we leverage a key insight: over-parameterized LLMs often converge to Flat Minima, implying a vast equivalent solution space where weights can be adjusted without compromising accuracy. Building on this, we propose Astro, an Activation-guided Structured Regularization framework designed to suppress the negative effects of outliers in a hardware-friendly and efficient manner. Leveraging the activation-guided regularization objective, Astro actively reconstructs intrinsically robust weights, aggressively suppressing weight outliers corresponding to high-magnitude activations without sacrificing model accuracy. Crucially, Astro introduces zero inference latency and is orthogonal to mainstream quantization methods like GPTQ. Extensive experiments show that Astro achieves highly competitive performance; notably, on LLaMA-2-7B, it achieves better performance than complex learning-based rotation methods with almost 1/3 of the quantization time.

</details>


### [150] [Rational Transductors](https://arxiv.org/abs/2602.07599)
*Mehryar Mohri*

Main category: cs.LG

TL;DR: Rational Transductors：一种双流架构，通过加权有限自动机（WFA）的矩阵值递归增强Transformer，使其能够捕获所有正则语言和NC¹完全问题，解决Transformer在序列逻辑和状态跟踪上的局限性。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer擅长语义建模，但在严格的序列逻辑和状态跟踪方面表现不佳。理论研究表明自注意力机制在复杂类上受限（AC⁰或TC⁰），难以在没有中间思维链的情况下实现鲁棒的长度泛化。

Method: 提出Rational Transductors双流架构，通过加权有限自动机（WFA）的矩阵值递归增强Transformer。采用深度有理注入方案将有理状态信息注入注意力机制，通过随机有理特征作为序列依赖的通用基，并使用可微分有理特征机制来弥补表示紧凑性差距。

Result: 该框架严格泛化了Transformer的表达能力，能够捕获所有正则语言、NC¹完全问题（如布尔公式求值）以及奇偶性和模计数等基本分离问题，同时保持O(L + log T)的并行时间复杂度。理论分析和实证结果表明，Rational Transductors解决了"正则差距"，在标准Transformer失败的算法任务上实现了鲁棒的长度泛化。

Conclusion: Rational Transductors通过结合Transformer的语义建模能力和WFA的序列处理能力，克服了标准Transformer在序列逻辑任务上的局限性，实现了更好的长度泛化性能，同时避免了传统RNN的顺序计算瓶颈。

Abstract: Standard Transformers excel at semantic modeling but struggle with
  rigid sequential logic and state tracking. Theoretical work
  establishes that self-attention is limited to $\AC^0$ (under hard
  attention) or $\TC^0$ (under soft attention), complexity classes
  that often fail to support robust length generalization on
  sequential problems without intermediate chain-of-thought. In this
  work, we introduce \emph{Rational Transductors}, a dual-stream
  architecture that augments the Transformer with a matrix-valued
  recurrence derived from Weighted Finite Automata (WFA). By
  injecting rational state information into the attention mechanism
  via a \emph{Deep Rational Injection} scheme, our framework strictly
  generalizes the expressive power of Transformers to capture all
  Regular Languages, $\NC^1$-complete problems (such as Boolean
  Formula Evaluation), and fundamental separations like Parity and
  Modular Counting, while preserving $O(L + \log T)$ parallel time
  complexity. We ground the architecture in a rigorous learning
  theory: we prove that \emph{Random Rational Features} act as a
  universal basis for sequential dependencies, justifying our
  initialization strategy, while establishing that the
  \emph{Differentiable Rational Feature} regime is necessary to close
  the representational compactness gap. Theoretical analysis and
  empirical results demonstrate that Rational Transductors solve the
  "Regular Gap," enabling robust length generalization on algorithmic
  tasks where standard Transformers fail, without the sequential
  computational bottlenecks of traditional RNNs.

</details>


### [151] [Object-Oriented Transition Modeling with Inductive Logic Programming](https://arxiv.org/abs/2602.07602)
*Gabriel Stella,Dmitri Loguinov*

Main category: cs.LG

TL;DR: 提出一种新的学习算法，在面向对象的表示基础上，显著提升了归纳建模能力，在泛化性、可解释性和训练效率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 从观察中构建世界模型（归纳）是机器学习的主要挑战之一。有用的模型需要在新的情境中保持准确性（泛化），同时易于解释且训练高效。先前工作探索了受人类认知启发的面向对象表示，但现有方法能力有限。

Method: 开发了一种新颖的学习算法，比先前方法更强大。该算法基于面向对象的表示，通过系统实验（包括消融测试和与神经基线的比较）验证其有效性。

Result: 实验结果表明，该方法在归纳建模任务上相比现有最先进方法有显著改进，在泛化性、可解释性和训练效率方面表现优越。

Conclusion: 提出的新学习算法在面向对象表示的归纳建模中取得了突破性进展，为构建更强大、可解释且高效的世界模型提供了有效解决方案。

Abstract: Building models of the world from observation, i.e., induction, is one of the major challenges in machine learning. In order to be useful, models need to maintain accuracy when used in novel situations, i.e., generalize. In addition, they should be easy to interpret and efficient to train. Prior work has investigated these concepts in the context of object-oriented representations inspired by human cognition. In this paper, we develop a novel learning algorithm that is substantially more powerful than these previous methods. Our thorough experiments, including ablation tests and comparison with neural baselines, demonstrate a significant improvement over the state-of-the-art.

</details>


### [152] [Escaping Spectral Bias without Backpropagation: Fast Implicit Neural Representations with Extreme Learning Machines](https://arxiv.org/abs/2602.07603)
*Woojin Cho,Junghwan Park*

Main category: cs.LG

TL;DR: ELM-INR：一种免反向传播的隐式神经表示方法，使用极端学习机（ELM）进行闭式求解，结合自适应网格细化策略BEAM来提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统INR训练依赖迭代反向传播，存在频谱偏差问题，特别是在处理高度非均匀频率内容时效果不佳。需要更快速、数值稳定的重建方法。

Method: 将域分解为重叠子域，在每个局部问题上使用极端学习机（ELM）进行闭式求解，避免迭代优化。引入BEAM自适应网格细化策略，根据频谱复杂度平衡子域分配。

Result: ELM-INR实现了快速且数值稳定的重建，通过谱Barron范数分析发现全局重建误差主要由高频谱复杂度区域主导，BEAM策略在容量受限情况下显著提升重建质量。

Conclusion: ELM-INR提供了一种免反向传播的INR训练框架，结合自适应网格细化策略，能够有效处理非均匀频率内容，在计算效率和重建质量方面具有优势。

Abstract: Training implicit neural representations (INRs) to capture fine-scale details typically relies on iterative backpropagation and is often hindered by spectral bias when the target exhibits highly non-uniform frequency content. We propose ELM-INR, a backpropagation-free INR that decomposes the domain into overlapping subdomains and fits each local problem using an Extreme Learning Machine (ELM) in closed form, replacing iterative optimization with stable linear least-squares solutions. This design yields fast and numerically robust reconstruction by combining local predictors through a partition of unity. To understand where approximation becomes difficult under fixed local capacity, we analyze the method from a spectral Barron norm perspective, which reveals that global reconstruction error is dominated by regions with high spectral complexity. Building on this insight, we introduce BEAM, an adaptive mesh refinement strategy that balances spectral complexity across subdomains to improve reconstruction quality in capacity-constrained regimes.

</details>


### [153] [SERE: Similarity-based Expert Re-routing for Efficient Batch Decoding in MoE Models](https://arxiv.org/abs/2602.07616)
*Juntong Wu,Jialiang Cheng,Fuyu Lv,Ou Dan,Li Yuan*

Main category: cs.LG

TL;DR: SERE是一种基于相似性的专家重路由方法，用于提升MoE模型批量解码效率，通过动态减少活跃专家数量，实现最高2倍加速且质量损失最小。


<details>
  <summary>Details</summary>
Motivation: MoE模型在生产环境中需要批量推理以优化硬件效率，但这会导致专家过度激活，从而减慢内存受限的解码阶段。存在批量解码与专家稀疏性之间的根本矛盾。

Method: SERE通过相似性分析动态减少活跃专家数量：1）将次要专家的token重路由到最相似的主要专家；2）利用相似性模式识别并保留关键专家；3）避免静态专家剪枝或合并，实现基于批量级专家冗余的动态专家跳过；4）提供高效自定义CUDA内核，可在vLLM中即插即用。

Result: 在各种复杂推理基准测试中，SERE实现了最高2.0倍的加速，且质量损失最小，为大规模MoE部署提供了实用的成本效益和低延迟解决方案。

Conclusion: SERE通过动态专家重路由有效解决了MoE模型批量解码效率问题，在保持模型能力的同时显著提升推理速度，为生产环境中的大规模MoE部署提供了实用解决方案。

Abstract: Mixture-of-Experts (MoE) architectures employ sparse activation to deliver faster training and inference with higher accuracy than dense LLMs. However, in production serving, MoE models require batch inference to optimize hardware efficiency, which may cause excessive expert activation and thus slow the memory-bound decoding stage. To address the fundamental tension between batch decoding and expert sparsity, we present SERE, a Similarity-based Expert Re-routing method for Efficient batch decoding in MoE models. SERE dynamically reduces the number of active experts in an input-aware manner by re-routing tokens from secondary experts to their most similar primary counterparts. It also leverages similarity patterns to identify and preserve critical experts, thereby preventing capability loss. Notably, SERE avoids static expert pruning or merging, instead enabling dynamic expert skipping based on batch-level expert redundancy. Additionally, we provide an efficient custom CUDA kernel for SERE, enabling plug-and-play use in vLLM with only a single-line code change. Extensive experiments on various complex reasoning benchmarks demonstrate that SERE achieves up to 2.0x speedup with minimal quality loss, providing a practical solution for cost-efficient and latency-sensitive large-scale MoE deployment. Code implementation of SERE can be found in https://github.com/JL-Cheng/SERE.

</details>


### [154] [Dense Neural Networks are not Universal Approximators](https://arxiv.org/abs/2602.07618)
*Levi Rauchwerger,Stefanie Jegelka,Ron Levie*

Main category: cs.LG

TL;DR: 密集神经网络在权重和维度约束下无法实现真正的通用逼近，存在某些Lipschitz连续函数无法被逼近，稀疏连接是实现真正通用逼近的必要条件


<details>
  <summary>Details</summary>
Motivation: 研究密集神经网络的逼近能力，虽然通用逼近定理表明足够大的架构可以逼近任意连续函数，但这是在权重无限制的前提下。本文要探究在自然约束下密集神经网络是否仍具有通用性

Method: 采用模型压缩方法，结合弱正则引理，将前馈网络解释为消息传递图神经网络。考虑ReLU神经网络在权重和输入输出维度上的自然约束，这些约束模拟了密集连接的概念

Result: 在设定的约束条件下，证明了存在Lipschitz连续函数无法被此类密集神经网络逼近。这表明密集层神经网络存在内在局限性

Conclusion: 密集神经网络不具备真正的通用逼近能力，稀疏连接是实现真正通用逼近的必要条件。这一发现强调了网络结构稀疏性的重要性

Abstract: We investigate the approximation capabilities of dense neural networks. While universal approximation theorems establish that sufficiently large architectures can approximate arbitrary continuous functions if there are no restrictions on the weight values, we show that dense neural networks do not possess this universality. Our argument is based on a model compression approach, combining the weak regularity lemma with an interpretation of feedforward networks as message passing graph neural networks. We consider ReLU neural networks subject to natural constraints on weights and input and output dimensions, which model a notion of dense connectivity. Within this setting, we demonstrate the existence of Lipschitz continuous functions that cannot be approximated by such networks. This highlights intrinsic limitations of neural networks with dense layers and motivates the use of sparse connectivity as a necessary ingredient for achieving true universality.

</details>


### [155] [TASTE: Task-Aware Out-of-Distribution Detection via Stein Operators](https://arxiv.org/abs/2602.07640)
*Michał Kozyra,Gesine Reinert*

Main category: cs.LG

TL;DR: 提出TASTE框架，基于Stein算子将分布偏移与模型输入敏感性联系起来，实现任务感知的OOD检测，并提供偏移定位和像素级诊断。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法要么是数据中心的（不考虑对模型的影响），要么是模型中心的（不考虑数据几何），缺乏将分布偏移与模型任务性能直接关联的方法。

Method: 基于Stein算子构建任务感知框架，将分布偏移投影到模型的敏感性场上，获得理论保证，支持坐标分解的偏移定位和图像数据的像素级诊断。

Result: 在控制高斯偏移、MNIST几何扰动和CIFAR-10扰动基准测试中，该方法与任务性能退化高度相关，且优于现有基线方法。

Conclusion: TASTE框架成功地将分布偏移与模型敏感性联系起来，提供了理论保证、偏移定位能力和可解释诊断，在OOD检测中表现出色。

Abstract: Out-of-distribution detection methods are often either data-centric, detecting deviations from the training input distribution irrespective of their effect on a trained model, or model-centric, relying on classifier outputs without explicit reference to data geometry. We propose TASTE (Task-Aware STEin operators): a task-aware framework based on so-called Stein operators, which allows us to link distribution shift to the input sensitivity of the model. We show that the resulting operator admits a clear geometric interpretation as a projection of distribution shift onto the sensitivity field of the model, yielding theoretical guarantees. Beyond detecting the presence of a shift, the same construction enables its localisation through a coordinate-wise decomposition, and for image data-provides interpretable per-pixel diagnostics. Experiments on controlled Gaussian shifts, MNIST under geometric perturbations, and CIFAR-10 perturbed benchmarks demonstrate that the proposed method aligns closely with task degradation while outperforming established baselines.

</details>


### [156] [Continuous Program Search](https://arxiv.org/abs/2602.07659)
*Matthew Siper,Muhammad Umair Nasir,Ahmed Khalifa,Lisa Soros,Jay Azhang,Julian Togelius*

Main category: cs.LG

TL;DR: 该论文提出了一种通过学习连续程序空间和设计几何编译变异算子来改善遗传编程中局部性的方法，显著提高了搜索效率。


<details>
  <summary>Details</summary>
Motivation: 遗传编程虽然能产生可解释的程序，但小的语法变异可能导致大的、不可预测的行为变化，这会降低局部性和样本效率。这被视为一个算子设计问题。

Method: 1) 学习一个连续程序空间，其中潜在距离具有行为意义；2) 设计变异算子利用这种结构而不改变进化优化器；3) 通过跟踪受控潜在扰动下的行为级分歧来测量局部性；4) 使用块因子化嵌入和几何编译变异，限制更新到语义配对的进入-退出子空间。

Result: 在五个资产上使用相同的(μ+λ)进化策略和固定评估预算，学习的变异算子使用少一个数量级的评估发现强策略，并实现了最高的中位数样本外夏普比率。几何编译变异产生更快、更可靠的进展。

Conclusion: 语义对齐的变异可以显著提高搜索效率，而无需修改底层进化算法。虽然各向同性变异偶尔能达到更高的峰值性能，但几何编译变异提供了更稳定和高效的搜索。

Abstract: Genetic Programming yields interpretable programs, but small syntactic mutations can induce large, unpredictable behavioral shifts, degrading locality and sample efficiency. We frame this as an operator-design problem: learn a continuous program space where latent distance has behavioral meaning, then design mutation operators that exploit this structure without changing the evolutionary optimizer.
  We make locality measurable by tracking action-level divergence under controlled latent perturbations, identifying an empirical trust region for behavior-local continuous variation. Using a compact trading-strategy DSL with four semantic components (long/short entry and exit), we learn a matching block-factorized embedding and compare isotropic Gaussian mutation over the full latent space to geometry-compiled mutation that restricts updates to semantically paired entry--exit subspaces and proposes directions using a learned flow-based model trained on logged mutation outcomes.
  Under identical $(μ+λ)$ evolution strategies and fixed evaluation budgets across five assets, the learned mutation operator discovers strong strategies using an order of magnitude fewer evaluations and achieves the highest median out-of-sample Sharpe ratio. Although isotropic mutation occasionally attains higher peak performance, geometry-compiled mutation yields faster, more reliable progress, demonstrating that semantically aligned mutation can substantially improve search efficiency without modifying the underlying evolutionary algorithm.

</details>


### [157] [Surprisal-Guided Selection: Compute-Optimal Test-Time Strategies for Execution-Grounded Code Generation](https://arxiv.org/abs/2602.07670)
*Jarrod Barnes*

Main category: cs.LG

TL;DR: 在可验证执行任务中，搜索策略优于测试时训练：最佳N采样达到90%成功率，而TTT仅30.6%。提出基于惊奇度引导的选择策略，零成本实现80%成功率。


<details>
  <summary>Details</summary>
Motivation: 研究在可验证执行任务中，计算资源应该用于梯度适应还是搜索策略。针对GPU内核优化等密集奖励任务，探索最优的测试时策略。

Method: 使用KernelBench测试平台和120B参数模型，比较测试时训练与搜索策略。提出惊奇度引导选择：选择最高惊奇度（最低置信度）的正确样本。

Result: 最佳N采样（K=64）达到90%任务成功率，而TTT仅30.6%。惊奇度引导选择实现80%成功率，比最置信选择提高30%。惊奇度引导前3选择达到100%成功率。

Conclusion: 对于密集奖励的可验证执行任务，计算资源应分配给样本多样性和智能选择，而非梯度适应。惊奇度引导选择原则可推广到其他执行任务领域。

Abstract: Test-time training (TTT) adapts language models through gradient-based updates at inference. But is adaptation the right strategy? We study compute-optimal test-time strategies for verifiable execution-grounded (VEG) tasks, domains like GPU kernel optimization where a deterministic evaluator provides dense, continuous reward signals. Using KernelBench as our testbed and a 120B-parameter model (GPT-OSS-120B with LoRA adaptation), we find that search outperforms minimal adaptation (1-5 gradient steps): Best-of-N sampling achieves 90% task success (18/20 tasks) at K=64 across the full KernelBench L1 eval set while TTT's best checkpoint reaches only 30.6% (3-seed mean), with TTT's "equivalent K" falling below 1, worse than single-sample inference. The failure mode is over-sharpening: gradient updates collapse diversity toward mediocre solutions rather than discovering optimal ones. Our main contribution is surprisal-guided selection: selecting the highest-surprisal (lowest-confidence) correct sample yields 80% success vs. 50% for most-confident selection, a 30% improvement. Extending to surprisal-guided-top3 matches oracle performance at 100%. This zero-cost strategy, validated through length-controlled analysis, recovers oracle performance. For dense-reward VEG tasks, compute should be allocated to sample diversity and intelligent selection rather than gradient adaptation. The surprisal-guided selection principle may generalize to other execution-grounded domains where optimal solutions occupy the distribution tail.

</details>


### [158] [Federated Learning with Profile Mapping under Distribution Shifts and Drifts](https://arxiv.org/abs/2602.07671)
*Mohan Li,Dario Fenoglio,Martin Gjoreski,Marc Langheinrich*

Main category: cs.LG

TL;DR: Feroma是一个联邦学习框架，通过客户端分布配置文件处理数据异构性，无需客户端集群身份信息，动态选择聚合策略并部署模型到未见过的测试客户端。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在真实世界数据异构性下性能下降，无法同时处理客户端间的分布偏移和时间上的分布漂移，且依赖不现实的假设（如已知客户端集群数量、数据异构类型），限制了泛化能力。

Method: 基于客户端分布配置文件（紧凑、隐私保护的本地数据表示），通过自适应相似性加权指导模型聚合和测试时模型分配，动态选择从集群化到个性化的聚合策略，无需重新训练、在线适应或客户端数据先验知识。

Result: 在6个基准测试中，相比10个最先进方法，Feroma在动态数据异构条件下平均准确率提升高达12个百分点，同时保持与FedAvg相当的计算和通信开销。

Conclusion: 基于分布配置文件的聚合为在数据分布偏移和漂移下实现鲁棒联邦学习提供了实用路径。

Abstract: Federated Learning (FL) enables decentralized model training across clients without sharing raw data, but its performance degrades under real-world data heterogeneity. Existing methods often fail to address distribution shift across clients and distribution drift over time, or they rely on unrealistic assumptions such as known number of client clusters and data heterogeneity types, which limits their generalizability. We introduce Feroma, a novel FL framework that explicitly handles both distribution shift and drift without relying on client or cluster identity. Feroma builds on client distribution profiles-compact, privacy-preserving representations of local data-that guide model aggregation and test-time model assignment through adaptive similarity-based weighting. This design allows Feroma to dynamically select aggregation strategies during training, ranging from clustered to personalized, and deploy suitable models to unseen, and unlabeled test clients without retraining, online adaptation, or prior knowledge on clients' data. Extensive experiments show that compared to 10 state-of-the-art methods, Feroma improves performance and stability under dynamic data heterogeneity conditions-an average accuracy gain of up to 12 percentage points over the best baselines across 6 benchmarks-while maintaining computational and communication overhead comparable to FedAvg. These results highlight that distribution-profile-based aggregation offers a practical path toward robust FL under both data distribution shifts and drifts.

</details>


### [159] [ElliCE: Efficient and Provably Robust Algorithmic Recourse via the Rashomon Sets](https://arxiv.org/abs/2602.07674)
*Bohdan Turbal,Iryna Voitsitska,Lesia Semenova*

Main category: cs.LG

TL;DR: 提出ElliCE框架，通过椭圆近似Rashomon集来生成鲁棒的反事实解释，解决模型不确定性下的算法追索问题


<details>
  <summary>Details</summary>
Motivation: 机器学习模型影响人们生活决策，需要理解如何通过行动获得更好结果。当Rashomon集（近似最优模型集合）很大时，标准的反事实解释变得不可靠，因为对一个模型有效的追索行动可能对另一个模型失败

Method: ElliCE框架：在Rashomon集的椭圆近似上优化反事实解释，确保在该椭圆区域内解释的有效性，并提供唯一性、稳定性和特征方向对齐的理论保证

Result: ElliCE生成的反事实解释不仅更鲁棒，而且更灵活，能适应用户指定的特征约束，计算速度显著快于现有基线方法

Conclusion: ElliCE为模型不确定性下的可靠算法追索提供了原则性和实用的解决方案，确保即使模型变化也能为用户提供稳定的推荐

Abstract: Machine learning models now influence decisions that directly affect people's lives, making it important to understand not only their predictions, but also how individuals could act to obtain better results. Algorithmic recourse provides actionable input modifications to achieve more favorable outcomes, typically relying on counterfactual explanations to suggest such changes. However, when the Rashomon set - the set of near-optimal models - is large, standard counterfactual explanations can become unreliable, as a recourse action valid for one model may fail under another. We introduce ElliCE, a novel framework for robust algorithmic recourse that optimizes counterfactuals over an ellipsoidal approximation of the Rashomon set. The resulting explanations are provably valid over this ellipsoid, with theoretical guarantees on uniqueness, stability, and alignment with key feature directions. Empirically, ElliCE generates counterfactuals that are not only more robust but also more flexible, adapting to user-specified feature constraints while being substantially faster than existing baselines. This provides a principled and practical solution for reliable recourse under model uncertainty, ensuring stable recommendations for users even as models evolve.

</details>


### [160] [Spectral Gating Networks](https://arxiv.org/abs/2602.07679)
*Jusheng Zhang,Yijia Fan,Kaitong Cai,Jing Yang,Yongsen Zheng,Kwok-Yan Lam,Liang Lin,Keze Wang*

Main category: cs.LG

TL;DR: SGN是一种在固定参数和训练预算下，通过可学习门控机制将频谱能力注入MLP/FFN层的稳定方法，使用随机傅里叶特征替代基于网格的样条，在多个领域提升精度-效率权衡。


<details>
  <summary>Details</summary>
Motivation: 前馈网络中如何在不牺牲稳定性和可扩展性的情况下引入丰富的频率表达能力是一个未充分探索的问题。基于样条的KAN参数化在网格细化时会导致参数增长和脆弱的优化，特别是在高维情况下。

Method: 提出SGN（频谱门控网络），在标准激活路径上增加紧凑的频谱路径和可学习门控。频谱路径使用可训练的随机傅里叶特征（学习频率和相位）替代基于网格的样条，消除分辨率依赖。采用混合GELU-傅里叶公式提高优化鲁棒性和高频保真度。

Result: 在视觉、NLP、音频和PDE基准测试中，SGN在可比较的计算预算下持续改善精度-效率权衡，在CIFAR-10上达到93.15%准确率，比基于样条的KAN变体推理速度快达11.7倍。

Conclusion: SGN提供了一种稳定且可扩展的方法，在固定参数和训练预算下将频谱能力注入现有MLP/FFN层，解决了样条基KAN的稳定性和可扩展性问题，同时保持了良好的优化特性。

Abstract: Gating mechanisms are ubiquitous, yet a complementary question in feed-forward networks remains under-explored: how to introduce frequency-rich expressivity without sacrificing stability and scalability? This tension is exposed by spline-based Kolmogorov-Arnold Network (KAN) parameterizations, where grid refinement can induce parameter growth and brittle optimization in high dimensions. To propose a stability-preserving way to inject spectral capacity into existing MLP/FFN layers under fixed parameter and training budgets, we introduce Spectral Gating Networks (SGN), a drop-in spectral reparameterization. SGN augments a standard activation pathway with a compact spectral pathway and learnable gates that allow the model to start from a stable base behavior and progressively allocate capacity to spectral features during training. The spectral pathway is instantiated with trainable Random Fourier Features (learned frequencies and phases), replacing grid-based splines and removing resolution dependence. A hybrid GELU-Fourier formulation further improves optimization robustness while enhancing high-frequency fidelity. Across vision, NLP, audio, and PDE benchmarks, SGN consistently improves accuracy-efficiency trade-offs under comparable computational budgets, achieving 93.15% accuracy on CIFAR-10 and up to 11.7x faster inference than spline-based KAN variants. Code and trained models will be released.

</details>


### [161] [On the Infinite Width and Depth Limits of Predictive Coding Networks](https://arxiv.org/abs/2602.07697)
*Francesco Innocenti,El Mehdi Achour,Rafal Bogacz*

Main category: cs.LG

TL;DR: 本文证明了在无限宽度和深度极限下，预测编码网络（PCNs）与反向传播（BP）具有相同的稳定参数化集合，且PC能量在活动平衡时收敛到BP损失，从而计算相同的梯度。


<details>
  <summary>Details</summary>
Motivation: 预测编码（PC）是反向传播（BP）的一种生物可信替代方案，但深度PC网络的训练稳定性和可扩展性仍不清楚。本文旨在研究PCNs在无限宽度和深度极限下的理论基础，以理解其可扩展性。

Method: 研究线性残差网络的无限宽度和深度极限，分析PC和BP的宽度和深度稳定特征学习参数化集合，证明在活动平衡条件下PC能量收敛到BP损失。

Result: 对于线性残差网络，PC和BP具有完全相同的宽度和深度稳定参数化集合。在模型宽度远大于深度的条件下，PC能量在活动平衡时收敛到BP损失，从而计算与BP相同的梯度。实验表明这些结果在深度非线性网络中同样成立。

Conclusion: 本文统一了先前理论和实证结果，证明了PCNs在适当参数化下与BP具有相同的稳定性和梯度计算能力，对PCNs的扩展具有重要启示。

Abstract: Predictive coding (PC) is a biologically plausible alternative to standard backpropagation (BP) that minimises an energy function with respect to network activities before updating weights. Recent work has improved the training stability of deep PC networks (PCNs) by leveraging some BP-inspired reparameterisations. However, the full scalability and theoretical basis of these approaches remains unclear. To address this, we study the infinite width and depth limits of PCNs. For linear residual networks, we show that the set of width- and depth-stable feature-learning parameterisations for PC is exactly the same as for BP. Moreover, under any of these parameterisations, the PC energy with equilibrated activities converges to the BP loss in a regime where the model width is much larger than the depth, resulting in PC computing the same gradients as BP. Experiments show that these results hold in practice for deep nonlinear networks, as long as an activity equilibrium seem to be reached. Overall, this work unifies various previous theoretical and empirical results and has potentially important implications for the scaling of PCNs.

</details>


### [162] [Dense Feature Learning via Linear Structure Preservation in Medical Data](https://arxiv.org/abs/2602.07706)
*Yuanyun Zhang,Mingxuan Zhang,Siyuan Li,Zihan Wang,Haoran Chen,Wenbo Zhou,Shi Li*

Main category: cs.LG

TL;DR: 本文提出密集特征学习框架，通过直接优化嵌入矩阵的线性代数特性（谱平衡、子空间一致性、特征正交性），改善医学表示的质量，无需标签或生成重建。


<details>
  <summary>Details</summary>
Motivation: 传统医学深度学习模型使用任务特定目标训练，导致表示坍缩到少量判别方向，未能充分利用临床数据的丰富结构，限制了特征的迁移性、稳定性和可解释性。

Method: 提出密集特征学习框架，直接操作嵌入矩阵，通过谱平衡、子空间一致性和特征正交性等线性代数特性定义目标函数，无需依赖标签或生成重建。

Result: 在纵向电子健康记录、临床文本和多模态患者表示上的实验表明，相比监督和自监督基线，该方法在有效秩、条件数、稳定性、下游线性性能、鲁棒性和子空间对齐方面均有提升。

Conclusion: 学习覆盖临床变异与学习预测临床结果同等重要，应将表示几何作为医学AI的一等目标，密集特征学习为此提供了有效框架。

Abstract: Deep learning models for medical data are typically trained using task specific objectives that encourage representations to collapse onto a small number of discriminative directions. While effective for individual prediction problems, this paradigm underutilizes the rich structure of clinical data and limits the transferability, stability, and interpretability of learned features. In this work, we propose dense feature learning, a representation centric framework that explicitly shapes the linear structure of medical embeddings. Our approach operates directly on embedding matrices, encouraging spectral balance, subspace consistency, and feature orthogonality through objectives defined entirely in terms of linear algebraic properties. Without relying on labels or generative reconstruction, dense feature learning produces representations with higher effective rank, improved conditioning, and greater stability across time. Empirical evaluations across longitudinal EHR data, clinical text, and multimodal patient representations demonstrate consistent improvements in downstream linear performance, robustness, and subspace alignment compared to supervised and self supervised baselines. These results suggest that learning to span clinical variation may be as important as learning to predict clinical outcomes, and position representation geometry as a first class objective in medical AI.

</details>


### [163] [Quantifying Explanation Quality in Graph Neural Networks using Out-of-Distribution Generalization](https://arxiv.org/abs/2602.07708)
*Ding Zhang,Siddharth Betala,Chirag Agarwal*

Main category: cs.LG

TL;DR: 提出EGS（Explanation-Generalization Score）指标，通过评估GNN解释在分布外泛化中的稳定性来量化解释的因果相关性，为图神经网络解释方法提供更可靠的评估基准。


<details>
  <summary>Details</summary>
Motivation: 当前图神经网络（GNN）后验解释的质量评估存在挑战，现有评估指标（如保真度、稀疏性）往往无法判断解释是否识别了真正的因果变量，需要一种能评估解释因果有效性的新指标。

Method: 提出EGS指标，基于特征不变性原则：如果解释捕获了真正的因果驱动因素，那么在不同分布偏移下应该能保持稳定的预测。通过使用解释子图训练GNN，并在分布外（OOD）设置中评估其性能，将OOD泛化作为解释因果有效性的严格代理指标。

Result: 在合成和真实世界数据集上进行了大规模验证，涉及11,200个模型组合。结果表明EGS能够基于解释捕获因果子结构的能力为解释器提供原则性的排序基准，相比传统的基于保真度的指标提供了更稳健的替代方案。

Conclusion: EGS为GNN解释方法提供了一个新的评估框架，通过量化解释的因果相关性，能够更准确地评估解释器识别真正因果变量的能力，推动了图神经网络可解释性研究的发展。

Abstract: Evaluating the quality of post-hoc explanations for Graph Neural Networks (GNNs) remains a significant challenge. While recent years have seen an increasing development of explainability methods, current evaluation metrics (e.g., fidelity, sparsity) often fail to assess whether an explanation identifies the true underlying causal variables. To address this, we propose the Explanation-Generalization Score (EGS), a metric that quantifies the causal relevance of GNN explanations. EGS is founded on the principle of feature invariance and posits that if an explanation captures true causal drivers, it should lead to stable predictions across distribution shifts. To quantify this, we introduce a framework that trains GNNs using explanatory subgraphs and evaluates their performance in Out-of-Distribution (OOD) settings (here, OOD generalization serves as a rigorous proxy for the explanation's causal validity). Through large-scale validation involving 11,200 model combinations across synthetic and real-world datasets, our results demonstrate that EGS provides a principled benchmark for ranking explainers based on their ability to capture causal substructures, offering a robust alternative to traditional fidelity-based metrics.

</details>


### [164] [Towards Robust Scaling Laws for Optimizers](https://arxiv.org/abs/2602.07712)
*Alexandra Volkova,Mher Safaryan,Christoph H. Lampert,Dan Alistarh*

Main category: cs.LG

TL;DR: 该研究分析了不同优化器（如AdamW、Muon、Shampoo、SOAP）对LLM预训练的影响，提出了更稳健的缩放定律，并提供了理论分析框架。


<details>
  <summary>Details</summary>
Motivation: 现有缩放定律研究通常固定使用AdamW优化器，而新一代优化器（如Muon、Shampoo、SOAP）虽然承诺更快更稳定的收敛，但其与模型和数据缩放的关系尚未被充分理解。需要研究不同优化器下的缩放定律。

Method: 1) 实证研究不同优化器的Chinchilla式缩放定律；2) 提出共享幂律指数和优化器特定缩放因子的更稳健缩放定律；3) 对凸二次目标函数的梯度方法进行理论分析，证明Chinchilla式缩放定律是损失分解为不可约误差、近似误差和优化误差的自然结果。

Result: 1) 发现每个优化器的独立缩放定律存在病态性和高度相关参数；2) 提出的共享指数缩放定律能够直接比较不同优化器；3) 理论分析表明缩放定律是损失分解的自然结果，为优化器选择提供了理论依据。

Conclusion: 不同优化器需要统一的缩放定律框架进行比较，提出的共享指数缩放定律更稳健，理论分析揭示了缩放定律的数学本质，为LLM预训练中优化器选择提供了新视角。

Abstract: The quality of Large Language Model (LLM) pretraining depends on multiple factors, including the compute budget and the choice of optimization algorithm. Empirical scaling laws are widely used to predict loss as model size and training data grow, however, almost all existing studies fix the optimizer (typically AdamW). At the same time, a new generation of optimizers (e.g., Muon, Shampoo, SOAP) promises faster and more stable convergence, but their relationship with model and data scaling is not yet well understood. In this work, we study scaling laws across different optimizers. Empirically, we show that 1) separate Chinchilla-style scaling laws for each optimizer are ill-conditioned and have highly correlated parameters. Instead, 2) we propose a more robust law with shared power-law exponents and optimizer-specific rescaling factors, which enable direct comparison between optimizers. Finally, 3) we provide a theoretical analysis of gradient-based methods for the proxy task of a convex quadratic objective, demonstrating that Chinchilla-style scaling laws emerge naturally as a result of loss decomposition into irreducible, approximation, and optimization errors.

</details>


### [165] [Analyzing and Guiding Zero-Shot Posterior Sampling in Diffusion Models](https://arxiv.org/abs/2602.07715)
*Roi Benita,Michael Elad,Joseph Keshet*

Main category: cs.LG

TL;DR: 论文提出了一种基于高斯先验假设的零样本扩散逆问题求解器理论分析框架，能够推导闭式解并进行谱域分析，为参数设计提供原则性方法


<details>
  <summary>Details</summary>
Motivation: 现有的零样本扩散逆问题求解方法依赖手动调参和启发式策略，缺乏理论分析框架。作者希望建立严格的数学分析来理解这些近似后验采样器，并提供原则性的参数设计方法。

Method: 在假设先验为高斯分布的条件下，推导出理想后验采样器和扩散重建算法的闭式表达式，在谱域进行理论分析。基于这些表示，提出了一个原则性的参数设计框架，替代现有的启发式选择策略。

Result: 提出的谱域分析方法能够为不同算法提供定制化的参数选择，同时考虑先验特性、退化信号和扩散动态。结果显示，推荐的参数结构与标准启发式方法不同，且随扩散步长变化，能够在感知质量和信号保真度之间取得平衡。

Conclusion: 该工作为扩散逆问题求解器提供了理论分析框架，实现了原则性的参数设计，展示了谱域分析在理解算法行为和优化性能方面的重要价值。

Abstract: Recovering a signal from its degraded measurements is a long standing challenge in science and engineering. Recently, zero-shot diffusion based methods have been proposed for such inverse problems, offering a posterior sampling based solution that leverages prior knowledge. Such algorithms incorporate the observations through inference, often leaning on manual tuning and heuristics. In this work we propose a rigorous analysis of such approximate posterior-samplers, relying on a Gaussianity assumption of the prior. Under this regime, we show that both the ideal posterior sampler and diffusion-based reconstruction algorithms can be expressed in closed-form, enabling their thorough analysis and comparisons in the spectral domain. Building on these representations, we also introduce a principled framework for parameter design, replacing heuristic selection strategies used to date. The proposed approach is method-agnostic and yields tailored parameter choices for each algorithm, jointly accounting for the characteristics of the prior, the degraded signal, and the diffusion dynamics. We show that our spectral recommendations differ structurally from standard heuristics and vary with the diffusion step size, resulting in a consistent balance between perceptual quality and signal fidelity.

</details>


### [166] [Efficient Planning in Reinforcement Learning via Model Introspection](https://arxiv.org/abs/2602.07719)
*Gabriel Stella*

Main category: cs.LG

TL;DR: 该论文提出将内省视为程序分析，建立强化学习与经典规划之间的新联系，通过分析内部模型来合成任务相关信息，提高问题解决效率。


<details>
  <summary>Details</summary>
Motivation: 人类在面对任务时，无论任务如何指定，都能通过内省推理内部模型来合成所需信息高效解决问题。强化学习和经典规划通常被视为两个不同问题，需要不同解决方案，但缺乏这种内省能力。

Method: 提出将内省视为程序分析的方法，应用于强化学习中使用的各类模型。特别设计了一种算法，能够在关系强化学习使用的模型类上实现高效的目标导向规划。

Result: 展示了如何将程序分析方法应用于强化学习模型，并开发了在关系强化学习模型上进行高效目标导向规划的算法，建立了强化学习与经典规划之间的新联系。

Conclusion: 通过将内省视为程序分析，可以弥合强化学习与经典规划之间的差距，使智能体能够像人类一样通过分析内部模型来合成任务相关信息，从而提高问题解决效率。

Abstract: Reinforcement learning and classical planning are typically seen as two distinct problems, with differing formulations necessitating different solutions. Yet, when humans are given a task, regardless of the way it is specified, they can often derive the additional information needed to solve the problem efficiently. The key to this ability is introspection: by reasoning about their internal models of the problem, humans directly synthesize additional task-relevant information. In this paper, we propose that this introspection can be thought of as program analysis. We discuss examples of how this approach can be applied to various kinds of models used in reinforcement learning. We then describe an algorithm that enables efficient goal-oriented planning over the class of models used in relational reinforcement learning, demonstrating a novel link between reinforcement learning and classical planning.

</details>


### [167] [ParisKV: Fast and Drift-Robust KV-Cache Retrieval for Long-Context LLMs](https://arxiv.org/abs/2602.07721)
*Yanlin Qi,Xinhang Chen,Huiqiang Jiang,Qitong Wang,Botao Peng,Themis Palpanas*

Main category: cs.LG

TL;DR: ParisKV是一个针对长上下文LLM推理的KV缓存检索框架，通过碰撞候选选择和量化内积重排，在百万token规模下实现高效检索，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存检索方法在处理长上下文时面临分布漂移和高延迟问题，特别是在大规模场景下性能不足，需要更高效的解决方案。

Method: 基于碰撞候选选择，然后使用量化内积重排估计器，支持通过统一虚拟寻址进行CPU卸载的KV缓存，实现按需top-k获取。

Result: 在长输入和长生成基准测试中匹配或优于完整注意力质量，在批大小为1时匹配或超过完整注意力速度，吞吐量提高2.8倍，支持百万token上下文，相比MagicPIG和PQCache分别减少17倍和44倍延迟。

Conclusion: ParisKV是一个漂移鲁棒的GPU原生KV缓存检索框架，在长上下文LLM推理中实现了最先进的解码效率，能够扩展到完整注意力无法处理的大规模场景。

Abstract: KV-cache retrieval is essential for long-context LLM inference, yet existing methods struggle with distribution drift and high latency at scale. We introduce ParisKV, a drift-robust, GPU-native KV-cache retrieval framework based on collision-based candidate selection, followed by a quantized inner-product reranking estimator. For million-token contexts, ParisKV supports CPU-offloaded KV caches via Unified Virtual Addressing (UVA), enabling on-demand top-$k$ fetching with minimal overhead. ParisKV matches or outperforms full attention quality on long-input and long-generation benchmarks. It achieves state-of-the-art long-context decoding efficiency: it matches or exceeds full attention speed even at batch size 1 for long contexts, delivers up to 2.8$\times$ higher throughput within full attention's runnable range, and scales to million-token contexts where full attention runs out of memory. At million-token scale, ParisKV reduces decode latency by 17$\times$ and 44$\times$ compared to MagicPIG and PQCache, respectively, two state-of-the-art KV-cache Top-$k$ retrieval baselines.

</details>


### [168] [Do We Need Adam? Surprisingly Strong and Sparse Reinforcement Learning with SGD in LLMs](https://arxiv.org/abs/2602.07729)
*Sagnik Mukherjee,Lifan Yuan,Pavan Jayasinha,Dilek Hakkani-Tür,Hao Peng*

Main category: cs.LG

TL;DR: 研究发现，在大型语言模型的强化学习阶段，简单的SGD优化器比广泛使用的AdamW表现更好，且参数更新稀疏度极高（仅更新<0.02%的参数），比AdamW节省1000倍以上参数更新。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的强化学习阶段仍沿用预训练阶段的AdamW优化器，但RL与监督学习存在本质差异。AdamW内存开销大，而研究表明RL可能不需要Adam的动量机制和自适应学习率，因此探索更高效的优化方法。

Method: 通过分析AdamW在RL和SFT中的不同影响，提出假设：RL受益于Adam式自适应学习率和动量的程度较低。实验验证使用内存效率更高的SGD优化器在LLM RL中的表现，并与AdamW进行对比。

Result: SGD在LLM强化学习中匹配甚至优于AdamW的表现。SGD全微调仅更新不到0.02%的模型参数，比AdamW少1000倍以上，且无需稀疏正则化。分析提供了这种更新稀疏性的可能原因。

Conclusion: RL在LLM中的优化动态与监督学习不同，SGD在RL中表现优异且参数效率极高。这表明RL可以比之前认知的更参数高效，为LLM训练提供了新的优化见解。

Abstract: Reinforcement learning (RL), particularly RL from verifiable reward (RLVR), has become a crucial phase of training large language models (LLMs) and a key focus of current scaling efforts. However, optimization practices in RL largely follow those of next-token prediction stages (e.g., pretraining and supervised fine-tuning), despite fundamental differences between RL and these stages highlighted by recent work. One such practice is the use of the AdamW optimizer, which is widely adopted for training large-scale transformers despite its high memory overhead. Our analysis shows that both momentum and adaptive learning rates in AdamW are less influential in RL than in SFT, leading us to hypothesize that RL benefits less from Adam-style per-parameter adaptive learning rates and momentum. Confirming this hypothesis, our experiments demonstrate that the substantially more memory-efficient SGD, which is known to perform poorly in supervised learning of large-scale transformers, matches or even outperforms AdamW in RL for LLMs. Remarkably, full fine-tuning with SGD updates fewer than 0.02% of model parameters without any sparsity-promoting regularization, more than 1000 times fewer than AdamW. Our analysis offers potential reasons for this update sparsity. These findings provide new insights into the optimization dynamics of RL in LLMs and show that RL can be substantially more parameter-efficient than previously recognized.

</details>


### [169] [The Laplacian Keyboard: Beyond the Linear Span](https://arxiv.org/abs/2602.07730)
*Siddarth Chandrasekar,Marlos C. Machado*

Main category: cs.LG

TL;DR: Laplacian Keyboard (LK) 是一个分层框架，利用拉普拉斯特征向量构建任务无关的行为基，通过元策略动态组合这些行为，超越线性约束实现高效策略学习。


<details>
  <summary>Details</summary>
Motivation: 拉普拉斯特征向量在强化学习中通常只用于线性近似奖励函数，这限制了在复杂环境中的表达能力。需要超越线性约束的方法来提升策略学习的效率和表达能力。

Method: LK框架从拉普拉斯特征向量构建任务无关的行为基（选项库），保证包含线性空间内任何奖励的最优策略。通过元策略动态组合这些选项，学习超出原始线性约束的策略。

Result: 理论分析建立了零样本近似误差的界限。实证结果表明，LK超越了零样本解决方案，同时相比标准强化学习方法实现了更好的样本效率。

Conclusion: Laplacian Keyboard 通过利用拉普拉斯特征向量构建行为基并动态组合，成功超越了线性约束，为复杂环境中的高效策略学习提供了有效框架。

Abstract: Across scientific disciplines, Laplacian eigenvectors serve as a fundamental basis for simplifying complex systems, from signal processing to quantum mechanics. In reinforcement learning (RL), these eigenvectors provide a natural basis for approximating reward functions; however, their use is typically limited to their linear span, which restricts expressivity in complex environments. We introduce the Laplacian Keyboard (LK), a hierarchical framework that goes beyond the linear span. LK constructs a task-agnostic library of options from these eigenvectors, forming a behavior basis guaranteed to contain the optimal policy for any reward within the linear span. A meta-policy learns to stitch these options dynamically, enabling efficient learning of policies outside the original linear constraints. We establish theoretical bounds on zero-shot approximation error and demonstrate empirically that LK surpasses zero-shot solutions while achieving improved sample efficiency compared to standard RL methods.

</details>


### [170] [Efficient Adaptive Data Analysis over Dense Distributions](https://arxiv.org/abs/2602.07732)
*Joon Suk Huh*

Main category: cs.LG

TL;DR: 本文提出了一种在数据分布稠密条件下，同时实现计算效率和最优样本复杂度的自适应数据分析机制，将样本复杂度从O(√T)提升到O(log T)。


<details>
  <summary>Details</summary>
Motivation: 现代数据工作流具有自适应特性，反复查询同一数据集以优化决策，但这种自适应性会导致过拟合和统计推断失效。现有方法面临计算效率与样本复杂度之间的根本矛盾：计算高效的算法需要O(√T)样本，而统计最优的O(log T)算法计算上不可行。

Method: 提出一种计算高效的自适应数据分析机制，当数据分布相对于已知先验分布是稠密的时，能够达到最优的O(log T)样本复杂度。该方法特别适用于分布特定学习中的特征-标签数据分布，并满足谓词单点识别(PSO)安全性。

Result: 在数据分布稠密的条件下，实现了同时具备计算效率和最优样本复杂度(O(log T))的自适应数据分析机制。该机制还能在分布特定设置下提供样本高效的统计查询预言机。

Conclusion: 研究揭示了自适应数据分析与隐私保护之间的内在联系，超越了差分隐私的范畴。在特定数据分布条件下，可以同时实现计算效率和统计最优性，解决了该领域长期存在的权衡问题。

Abstract: Modern data workflows are inherently adaptive, repeatedly querying the same dataset to refine and validate sequential decisions, but such adaptivity can lead to overfitting and invalid statistical inference. Adaptive Data Analysis (ADA) mechanisms address this challenge; however, there is a fundamental tension between computational efficiency and sample complexity. For $T$ rounds of adaptive analysis, computationally efficient algorithms typically incur suboptimal $O(\sqrt{T})$ sample complexity, whereas statistically optimal $O(\log T)$ algorithms are computationally intractable under standard cryptographic assumptions. In this work, we shed light on this trade-off by identifying a natural class of data distributions under which both computational efficiency and optimal sample complexity are achievable. We propose a computationally efficient ADA mechanism that attains optimal $O(\log T)$ sample complexity when the data distribution is dense with respect to a known prior. This setting includes, in particular, feature--label data distributions arising in distribution-specific learning. As a consequence, our mechanism also yields a sample-efficient (i.e., $O(\log T)$ samples) statistical query oracle in the distribution-specific setting. Moreover, although our algorithm is not based on differential privacy, it satisfies a relaxed privacy notion known as Predicate Singling Out (PSO) security (Cohen and Nissim, 2020). Our results thus reveal an inherent connection between adaptive data analysis and privacy beyond differential privacy.

</details>


### [171] [TerraBind: Fast and Accurate Binding Affinity Prediction through Coarse Structural Representations](https://arxiv.org/abs/2602.07735)
*Matteo Rossi,Ryan Pederson,Miles Wang-Henderson,Ben Kaufman,Edward C. Williams,Carl Underkoffler,Owen Lewis Howell,Adrian Layer,Stephan Thaler,Narbe Mardirossian,John Anthony Parkhill*

Main category: cs.LG

TL;DR: TerraBind是一个蛋白质-配体结构和结合亲和力预测的基础模型，比现有方法推理速度快26倍，亲和力预测准确率提高约20%，采用粗粒度口袋表示和免扩散优化方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的药物设计方法依赖昂贵的全原子扩散来生成3D坐标，导致推理瓶颈，使大规模化合物筛选计算上不可行。作者提出关键假设：全原子分辨率对于准确的小分子构象和结合亲和力预测是不必要的。

Method: 采用粗粒度口袋级表示（仅蛋白质Cβ原子和配体重原子），结合COATI-3分子编码和ESM-2蛋白质嵌入的多模态架构，学习丰富的结构表示。使用免扩散优化模块进行构象生成和结合亲和力似然预测模块。

Result: 在结构预测基准测试中，TerraBind在配体构象准确性上与基于扩散的基线方法相当。在结合亲和力预测方面，比Boltz-2在Pearson相关性上提高约20%。亲和力预测模块还提供良好校准的不确定性估计，并通过持续学习框架和hedged批量选择策略，在模拟药物发现周期中实现比贪婪方法高6倍的亲和力改进。

Conclusion: TerraBind证明了粗粒度表示在蛋白质-配体结构和亲和力预测中的有效性，解决了当前方法的计算瓶颈，为大规模化合物筛选提供了高效准确的解决方案，并提供了可靠的不确定性估计和持续学习能力。

Abstract: We present TerraBind, a foundation model for protein-ligand structure and binding affinity prediction that achieves 26-fold faster inference than state-of-the-art methods while improving affinity prediction accuracy by $\sim$20\%. Current deep learning approaches to structure-based drug design rely on expensive all-atom diffusion to generate 3D coordinates, creating inference bottlenecks that render large-scale compound screening computationally intractable. We challenge this paradigm with a critical hypothesis: full all-atom resolution is unnecessary for accurate small molecule pose and binding affinity prediction. TerraBind tests this hypothesis through a coarse pocket-level representation (protein C$_β$ atoms and ligand heavy atoms only) within a multimodal architecture combining COATI-3 molecular encodings and ESM-2 protein embeddings that learns rich structural representations, which are used in a diffusion-free optimization module for pose generation and a binding affinity likelihood prediction module. On structure prediction benchmarks (FoldBench, PoseBusters, Runs N' Poses), TerraBind matches diffusion-based baselines in ligand pose accuracy. Crucially, TerraBind outperforms Boltz-2 by $\sim$20\% in Pearson correlation for binding affinity prediction on both a public benchmark (CASP16) and a diverse proprietary dataset (18 biochemical/cell assays). We show that the affinity prediction module also provides well-calibrated affinity uncertainty estimates, addressing a critical gap in reliable compound prioritization for drug discovery. Furthermore, this module enables a continual learning framework and a hedged batch selection strategy that, in simulated drug discovery cycles, achieves 6$\times$ greater affinity improvement of selected molecules over greedy-based approaches.

</details>


### [172] [Learnable Chernoff Baselines for Inference-Time Alignment](https://arxiv.org/abs/2602.07738)
*Sunil Madhow,Yuchen Liang,Ness Shroff,Yingbin Liang,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: 提出LCB方法用于推理时奖励引导对齐，通过可学习的Chernoff基线实现近似采样，相比理想拒绝采样显著减少对预训练模型的查询次数


<details>
  <summary>Details</summary>
Motivation: 现有推理时奖励引导对齐方法要么依赖特定架构适配，要么计算成本高昂，需要更高效的方法来从KL正则化奖励对齐产生的指数倾斜核中采样

Method: 引入可学习的Chernoff基线(LCB)方法，仅需预训练模型的黑盒采样访问，通过自适应选择接受概率的拒绝采样实现，提供细粒度推理-计算缩放控制

Result: 建立了与理想对齐模型的总变差保证，在连续和离散扩散设置中，LCB采样与理想拒绝采样匹配，同时显著减少对预训练模型的查询次数

Conclusion: LCB提供了一种高效、近似的推理时奖励对齐方法，仅需黑盒采样访问，在保持采样质量的同时大幅降低计算成本

Abstract: We study inference-time reward-guided alignment for generative models. Existing methods often rely on either architecture-specific adaptations or computationally costly inference procedures. We introduce Learnable Chernoff Baselines (LCBs) as a method for efficiently and approximately sampling from the exponentially tilted kernels that arise from KL-regularized reward alignment. Using only black-box sampling access to the pretrained model, LCBs implement a form of rejection sampling with adaptively selected acceptance probabilities, which allows fine-grained control over inference-compute scaling. We establish total-variation guarantees to the ideal aligned model, and demonstrate in both continuous and discrete diffusion settings that LCB sampling closely matches ideal rejection sampling while using substantially fewer queries to the pretrained model.

</details>


### [173] [Riemannian MeanFlow](https://arxiv.org/abs/2602.07744)
*Dongyeop Woo,Marta Skreta,Seonghyun Park,Sungsoo Ahn,Kirill Neklyudov*

Main category: cs.LG

TL;DR: Riemannian MeanFlow (RMF) 是一种在流形上直接学习流映射的框架，只需一次前向传递即可生成高质量样本，相比扩散模型减少10倍计算量。


<details>
  <summary>Details</summary>
Motivation: 当前在黎曼流形上的扩散和流模型需要数十到数百次神经网络评估，在大规模科学采样工作流中成为计算瓶颈。

Method: 提出Riemannian MeanFlow框架，推导了流形平均速度的三种等价表征（欧拉、拉格朗日和半群恒等式），分析了高维流形上的参数化和稳定化技术。

Result: 在启动子DNA设计和蛋白质骨架生成任务中，RMF达到与先前方法相当的样本质量，同时减少高达10倍函数评估次数；少步流映射通过奖励前瞻实现高效奖励引导设计。

Conclusion: RMF框架显著提高了黎曼流形上生成模型的推理效率，为大规模科学采样工作流提供了实用的解决方案。

Abstract: Diffusion and flow models have become the dominant paradigm for generative modeling on Riemannian manifolds, with successful applications in protein backbone generation and DNA sequence design. However, these methods require tens to hundreds of neural network evaluations at inference time, which can become a computational bottleneck in large-scale scientific sampling workflows. We introduce Riemannian MeanFlow~(RMF), a framework for learning flow maps directly on manifolds, enabling high-quality generations with as few as one forward pass. We derive three equivalent characterizations of the manifold average velocity (Eulerian, Lagrangian, and semigroup identities), and analyze parameterizations and stabilization techniques to improve training on high-dimensional manifolds. In promoter DNA design and protein backbone generation settings, RMF achieves comparable sample quality to prior methods while requiring up to 10$\times$ fewer function evaluations. Finally, we show that few-step flow maps enable efficient reward-guided design through reward look-ahead, where terminal states can be predicted from intermediate steps at minimal additional cost.

</details>


### [174] [Preference Conditioned Multi-Objective Reinforcement Learning: Decomposed, Diversity-Driven Policy Optimization](https://arxiv.org/abs/2602.07764)
*Tanmay Ambadkar,Sourav Panda,Shreyash Kale,Jonathan Dodge,Abhinav Verma*

Main category: cs.LG

TL;DR: D³PO提出了一种新的多目标强化学习方法，通过分解优化流程和多样性正则化，解决了现有方法中梯度干扰和表示崩溃的问题，能够用单个策略学习更高质量的帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 当前多目标强化学习中，虽然单个偏好条件策略是最灵活和可扩展的解决方案，但现有方法在实践中仍然脆弱，经常无法恢复完整的帕累托前沿。这主要源于两个结构性问题：过早标量化导致的破坏性梯度干扰，以及偏好空间中的表示崩溃。

Method: D³PO基于PPO框架重新组织了多目标策略优化。它通过分解优化流程保留每个目标的学习信号，仅在稳定后集成偏好，实现可靠的信用分配。同时，使用缩放多样性正则化器强制策略行为对偏好变化保持敏感，防止表示崩溃。

Result: 在标准MORL基准测试中，包括高维和多目标控制任务，D³PO始终比先前的单策略和多策略方法发现更广泛和更高质量的帕累托前沿，在超体积和期望效用方面匹配或超过最先进水平，同时使用单个可部署策略。

Conclusion: D³PO通过解决梯度干扰和表示崩溃这两个核心结构问题，显著提高了单策略多目标强化学习的性能，为实际应用提供了更可靠和可扩展的解决方案。

Abstract: Multi-objective reinforcement learning (MORL) seeks to learn policies that balance multiple, often conflicting objectives. Although a single preference-conditioned policy is the most flexible and scalable solution, existing approaches remain brittle in practice, frequently failing to recover complete Pareto fronts. We show that this failure stems from two structural issues in current methods: destructive gradient interference caused by premature scalarization and representational collapse across the preference space. We introduce $D^3PO$, a PPO-based framework that reorganizes multi-objective policy optimization to address these issues directly. $D^3PO$ preserves per-objective learning signals through a decomposed optimization pipeline and integrates preferences only after stabilization, enabling reliable credit assignment. In addition, a scaled diversity regularizer enforces sensitivity of policy behavior to preference changes, preventing collapse. Across standard MORL benchmarks, including high-dimensional and many-objective control tasks, $D^3PO$ consistently discovers broader and higher-quality Pareto fronts than prior single- and multi-policy methods, matching or exceeding state-of-the-art hypervolume and expected utility while using a single deployable policy.

</details>


### [175] [MaD-Mix: Multi-Modal Data Mixtures via Latent Space Coupling for Vision-Language Model Training](https://arxiv.org/abs/2602.07790)
*Wanyun Xie,Francesco Tonin,Volkan Cevher*

Main category: cs.LG

TL;DR: MaD-Mix是一个用于视觉语言模型训练的多模态数据混合框架，通过模态感知的域对齐最大化来自动优化数据混合比例，无需人工调优，显著加速训练过程。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型训练依赖昂贵的人工调优来确定多模态数据混合比例，这在复杂的三模态（视频-图像-文本）场景中变得不切实际，需要一种自动、高效的数据混合方法。

Method: 将数据混合问题形式化为模态感知的域对齐最大化，通过Fenchel对偶和跨模态耦合变量获得闭式多模态对齐分数，系统处理缺失模态的域，允许整合纯语言域。

Result: 在0.5B和7B模型上的实验表明，MaD-Mix在图像-文本指令调优中比人工调优节省22%的训练步骤；在三模态场景中，相比均匀权重提升平均准确率，混合计算开销极小（<1 GPU小时）。

Conclusion: MaD-Mix为现代视觉语言模型管道提供了可扩展的数据混合设计方法，实现了高效、自动化的多模态数据混合优化，显著减少了对人工调优的依赖。

Abstract: Vision-Language Models (VLMs) are typically trained on a diverse set of multi-modal domains, yet current practices rely on costly manual tuning. We propose MaD-Mix, a principled and computationally efficient framework that derives multi-modal data mixtures for VLM training. MaD-Mix formulates data mixing as modality-aware domain alignment maximization and obtains closed-form multi-modal alignment scores from the Fenchel dual through inter-modal coupling variables. MaD-Mix systematically handles domains with missing modalities, allowing for the integration of language-only domains. Empirical evaluations across 0.5B and 7B models demonstrate that MaD-Mix accelerates VLM training across diverse benchmarks. MaD-Mix matches human-tuned data mixtures using 22% fewer training steps in image-text instruction tuning. In complex tri-modal video-image-text scenarios, where manual tuning becomes impractical, MaD-Mix boosts average accuracy over uniform weights, with negligible mixture computation overhead (< 1 GPU-hour), enabling scalable mixture design for modern VLM pipelines.

</details>


### [176] [CausalTAD: Injecting Causal Knowledge into Large Language Models for Tabular Anomaly Detection](https://arxiv.org/abs/2602.07798)
*Ruiqi Wang,Ruikang Liu,Runyu Chen,Haoxiang Suo,Zhiyi Peng,Zhuo Tang,Changjian Chen*

Main category: cs.LG

TL;DR: CausalTaD：通过注入因果知识到LLMs中改进表格异常检测，通过因果关系重排列并加权，在30多个数据集上超越SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法将表格数据转为文本时随机排列列顺序，忽略了列间的因果关系，而这对准确检测异常至关重要。

Method: 1. 识别列间因果关系并重新排序（建模为线性排序问题）；2. 提出重加权策略，根据列对因果关系的贡献分配不同权重。

Result: 在30多个数据集上的实验表明，该方法持续超越当前最先进的方法。

Conclusion: 通过注入因果知识并优化列排序和加权，CausalTaD显著提升了LLMs在表格异常检测中的性能。

Abstract: Detecting anomalies in tabular data is critical for many real-world applications, such as credit card fraud detection. With the rapid advancements in large language models (LLMs), state-of-the-art performance in tabular anomaly detection has been achieved by converting tabular data into text and fine-tuning LLMs. However, these methods randomly order columns during conversion, without considering the causal relationships between them, which is crucial for accurately detecting anomalies. In this paper, we present CausalTaD, a method that injects causal knowledge into LLMs for tabular anomaly detection. We first identify the causal relationships between columns and reorder them to align with these causal relationships. This reordering can be modeled as a linear ordering problem. Since each column contributes differently to the causal relationships, we further propose a reweighting strategy to assign different weights to different columns to enhance this effect. Experiments across more than 30 datasets demonstrate that our method consistently outperforms the current state-of-the-art methods. The code for CausalTAD is available at https://github.com/350234/CausalTAD.

</details>


### [177] [Fairness Aware Reward Optimization](https://arxiv.org/abs/2602.07799)
*Ching Lam Choi,Vighnesh Subramaniam,Phillip Isola,Antonio Torralba,Stefanie Jegelka*

Main category: cs.LG

TL;DR: Faro是一个公平感知的奖励优化框架，通过训练满足公平性约束的奖励模型来减少LLM对齐中的偏见，提供理论保证并在实践中有效降低偏见同时保持模型质量。


<details>
  <summary>Details</summary>
Motivation: 人类偏好数据中的人口统计学偏斜会通过奖励模型传播到对齐的LLM中，导致系统性不公平。现有方法无法同时保证奖励模型的序数性（正确排序）、基数性（校准）和公平性。

Method: 提出Faro框架，在训练奖励模型时加入人口统计学平等、均衡机会或反事实公平性约束，提供理论分析包括公平性证明、KL正则化微调中的准确度-公平性权衡形式化描述，以及帕累托前沿存在性证明。

Result: 在多个LLM和基准测试中，Faro显著减少了偏见和有害生成，同时保持或提高了模型质量。与预处理和后处理方法不同，Faro确保奖励模型同时满足序数性、基数性和公平性。

Conclusion: Faro是首个提供理论保证的奖励级公平性框架，通过公平约束训练奖励模型，有效减少LLM对齐中的偏见传播，为构建更公平的AI系统提供了可行方案。

Abstract: Demographic skews in human preference data propagate systematic unfairness through reward models into aligned LLMs. We introduce Fairness Aware Reward Optimization (Faro), an in-processing framework that trains reward models under demographic parity, equalized odds, or counterfactual fairness constraints. We provide the first theoretical analysis of reward-level fairness in LLM alignment, establishing: (i) provable fairness certificates for Faro-trained rewards with controllable slack; a (ii) formal characterization of the accuracy-fairness trade-off induced by KL-regularized fine-tuning, proving fairness transfers from reward to policy; and the (iii) existence of a non-empty Pareto frontier. Unlike pre- and post-processing methods, Faro ensures reward models are simultaneously ordinal (ranking correctly), cardinal (calibrated), and fair. Across multiple LLMs and benchmarks, Faro significantly reduces bias and harmful generations while maintaining or improving model quality.

</details>


### [178] [Approximating Matrix Functions with Deep Neural Networks and Transformers](https://arxiv.org/abs/2602.07800)
*Rahul Padmanabhan,Simone Brugiapaglia*

Main category: cs.LG

TL;DR: 论文研究使用神经网络（包括Transformer）近似矩阵函数，证明了ReLU网络近似矩阵指数的宽度和深度界限，并实验表明带合适数值编码的Transformer编码器-解码器能以5%相对误差近似某些矩阵函数。


<details>
  <summary>Details</summary>
Motivation: Transformer在自然语言处理中取得了革命性进展，但在数值计算方面的应用研究较少。矩阵函数（如矩阵指数、矩阵符号函数）在科学计算中广泛应用，研究神经网络近似这些函数的能力具有重要价值。

Method: 1. 理论分析：证明ReLU网络近似矩阵指数所需的宽度和深度界限；2. 实验研究：使用带数值编码的Transformer编码器-解码器架构，测试不同编码方案对矩阵函数近似性能的影响。

Result: 1. 建立了ReLU网络近似矩阵指数的理论界限；2. 实验表明合适的Transformer编码器-解码器能以5%相对误差近似某些矩阵函数；3. 发现编码方案对性能有显著影响，不同函数适合不同的编码方案。

Conclusion: 神经网络（包括Transformer）能够有效近似矩阵函数，编码方案的选择是关键因素。这项工作为神经网络在科学计算中的应用提供了理论基础和实验验证。

Abstract: Transformers have revolutionized natural language processing, but their use for numerical computation has received less attention. We study the approximation of matrix functions, which map scalar functions to matrices, using neural networks including transformers. We focus on functions mapping square matrices to square matrices of the same dimension. These types of matrix functions appear throughout scientific computing, e.g., the matrix exponential in continuous-time Markov chains and the matrix sign function in stability analysis of dynamical systems. In this paper, we make two contributions. First, we prove bounds on the width and depth of ReLU networks needed to approximate the matrix exponential to an arbitrary precision. Second, we show experimentally that a transformer encoder-decoder with suitable numerical encodings can approximate certain matrix functions at a relative error of 5% with high probability. Our study reveals that the encoding scheme strongly affects performance, with different schemes working better for different functions.

</details>


### [179] [Efficient Representations are Controllable Representations](https://arxiv.org/abs/2602.07828)
*Charles Ye,Jasmine Cui*

Main category: cs.LG

TL;DR: 通过简单的辅助损失微调LLM，训练16个残差流维度作为惰性可解释性标志，模型会围绕这些标志重组并依赖它们进行生成，从而创建可解释的控制开关。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要先识别模型内部特征几何，然后进行干预，过程复杂。本文寻求一种更直接、更暴力的方法来安装可解释、可控制的特征到模型激活中。

Method: 使用简单的辅助损失微调LLM，专门训练3072个残差流维度中的16个维度作为惰性可解释性标志，这些标志指示生成所需的概念。模型在梯度下降过程中会围绕这些标志重组，消除冗余编码，最终依赖这些标志进行实际生成任务。

Result: 这些惰性标志变成了真正的内部特征：可解释的控制开关，允许在推理时引导生成。当特征可靠地在固定位置提供时，梯度下降会逐渐消除其他地方的冗余编码，模型会侵蚀自己的替代表示。

Conclusion: 模型的效率压力是一个可利用的杠杆，可以用来诱导可解释、可控制的表示。通过提供可靠的特征位置，可以迫使模型重组其表示结构，从而创建可解释的控制机制。

Abstract: What is the most brute-force way to install interpretable, controllable features into a model's activations? Controlling how LLMs internally represent concepts typically requires sophisticated methods to first identify, then intervene on the model's existing feature geometry. We bypass all of this.
  We finetune an LLM with a simple auxiliary loss, training 16 of its 3072 residual stream dimensions to be inert interpretability flags that simply indicate what concepts are required for generation. The model reorganizes around them anyway, learning to rely on these flags during actual generation tasks. As a result, these inert flags become genuine internal features: interpretable control switches that allow us to steer generation at inference time. Why does this work? When a feature is reliably supplied at a fixed location, gradient descent gradually eliminates redundant encodings elsewhere, and the model erodes its own alternative representations. A model's efficiency pressure is a lever - exploitable to induce interpretable, controllable representations.

</details>


### [180] [rePIRL: Learn PRM with Inverse RL for LLM Reasoning](https://arxiv.org/abs/2602.07832)
*Xian Wu,Kaijie Zhu,Ying Zhang,Lun Wang,Wenbo Guo*

Main category: cs.LG

TL;DR: rePIRL：一个受逆强化学习启发的框架，用于学习有效的过程奖励模型，对专家策略假设最少，在数学和编程推理任务上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型学习方法要么依赖对专家策略的强假设（如需要其奖励函数），要么存在内在局限性（如熵崩溃），导致模型性能弱或泛化能力有限。

Method: 提出rePIRL框架，采用双重学习过程交替更新策略和过程奖励模型，包含定制化技术解决传统逆强化学习扩展到LLM的挑战。

Result: 在标准化数学和编程推理数据集上的实证评估显示rePIRL优于现有方法，训练出的PRM可用于测试时训练、测试时扩展和为困难问题提供早期信号。

Conclusion: rePIRL能够以最小假设学习有效的过程奖励模型，统一了在线和离线PRM学习方法，并通过消融研究验证了训练方案和关键设计选择。

Abstract: Process rewards have been widely used in deep reinforcement learning to improve training efficiency, reduce variance, and prevent reward hacking. In LLM reasoning, existing works also explore various solutions for learning effective process reward models (PRM) with or without the help of an expert policy. However, existing methods either rely on strong assumptions about the expert policies (e.g., requiring their reward functions) or suffer intrinsic limitations (e.g., entropy collapse), resulting in weak PRMs or limited generalizability. In this paper, we introduce rePIRL, an inverse RL-inspired framework that learns effective PRMs with minimal assumptions about expert policies. Specifically, we design a dual learning process that updates the policy and the PRM interchangeably. Our learning algorithm has customized techniques to address the challenges of scaling traditional inverse RL to LLMs. We theoretically show that our proposed learning framework can unify both online and offline PRM learning methods, justifying that rePIRL can learn PRMs with minimal assumptions. Empirical evaluations on standardized math and coding reasoning datasets demonstrate the effectiveness of rePIRL over existing methods. We further show the application of our trained PRM in test-time training, test-time scaling, and providing an early signal for training hard problems. Finally, we validate our training recipe and key design choices via a detailed ablation study.

</details>


### [181] [Interpretable Analytic Calabi-Yau Metrics via Symbolic Distillation](https://arxiv.org/abs/2602.07834)
*D Yang Eng*

Main category: cs.LG

TL;DR: 使用符号回归将神经网络近似的Calabi-Yau流形度量提炼为简单可解释的五项公式，精度相当但参数减少3000倍


<details>
  <summary>Details</summary>
Motivation: Calabi-Yau流形对弦理论至关重要，但其度量计算极其困难。虽然神经网络可以近似这些度量，但它们是黑盒模型，缺乏可解释性。需要找到既能保持精度又具有物理可解释性的简化表示。

Method: 采用符号回归方法，将训练好的神经网络近似作为"教师"，从中提炼出简洁的数学表达式。通过多种子验证确认几何约束选择的基本特征，主要包含幂和与对称多项式。

Result: 得到五项表达式，与神经网络精度相当（R²=0.9994），但参数数量减少3000倍。该函数形式在研究模空间范围（ψ∈[0,0.8]）内保持稳定，系数平滑变化。公式能准确再现物理可观测量（体积积分和Yukawa耦合）。

Conclusion: 符号提炼方法成功地从黑盒神经网络中恢复出紧凑、可解释的模型，为原本只能通过神经网络访问的物理量提供了透明表示，验证了该方法在弦理论几何中的应用价值。

Abstract: Calabi--Yau manifolds are essential for string theory but require computing intractable metrics. Here we show that symbolic regression can distill neural approximations into simple, interpretable formulas. Our five-term expression matches neural accuracy ($R^2 = 0.9994$) with 3,000-fold fewer parameters. Multi-seed validation confirms that geometric constraints select essential features, specifically power sums and symmetric polynomials, while permitting structural diversity. The functional form can be maintained across the studied moduli range ($ψ\in [0, 0.8]$) with coefficients varying smoothly; we interpret these trends as empirical hypotheses within the accuracy regime of the locally-trained teachers ($σ\approx 8-9\%$ at $ψ\neq 0$). The formula reproduces physical observables -- volume integrals and Yukawa couplings -- validating that symbolic distillation recovers compact, interpretable models for quantities previously accessible only to black-box networks.

</details>


### [182] [MARTI-MARS$^2$: Scaling Multi-Agent Self-Search via Reinforcement Learning for Code Generation](https://arxiv.org/abs/2602.07848)
*Shijie Wang,Pengfei Li,Yikun Fu,Kaifeng Liu,Fangyuan Li,Yang Liu,Xiaowei Sun,Zonglin Li,Siyao Zhao,Jian Zhao,Kai Tian,Dong Li,Junqi Gao,Yutong Zhang,Yiqun Chen,Yuqiang Li,Zoe Li,Weinan Zhang,Peng Ye,Shuyue Hu,Lei Bai,Bowen Zhou,Kaiyan Zhang,Biqing Qi*

Main category: cs.LG

TL;DR: MARTI-MARS2是一个多智能体强化学习框架，通过将多智能体协作探索过程建模为可学习环境，结合策略学习和树搜索，实现了从同构多角色训练到异构多智能体训练的演进，在代码生成任务上超越了单智能体性能极限。


<details>
  <summary>Details</summary>
Motivation: 单智能体系统在复杂任务（如代码生成）中存在性能上限，而现有的多智能体框架通常依赖基于提示的测试时交互或同构参数训练，限制了错误纠正能力和策略多样性。

Method: 提出MARTI-MARS2框架，将多智能体协作探索过程建模为动态可学习环境，结合策略学习和多智能体树搜索，允许智能体在环境中迭代探索和优化。同时引入MARTI-MARS2-T+高效推理策略，充分利用多智能体协作的扩展潜力。

Result: 在两个32B模型协作下，MARTI-MARS2在代码生成基准测试中达到77.7%的准确率，超越了GPT-5.1等强基线。实验揭示了新的扩展规律：从单智能体到同构多角色再到异构多智能体范式，逐步获得更高的RL性能上限、更强的TTS能力和更大的策略多样性。

Conclusion: MARTI-MARS2框架通过多智能体强化学习突破了单智能体能力限制，证明了策略多样性对于通过多智能体强化学习扩展智能的重要性，为复杂任务提供了有效的协作解决方案。

Abstract: While the complex reasoning capability of Large Language Models (LLMs) has attracted significant attention, single-agent systems often encounter inherent performance ceilings in complex tasks such as code generation. Multi-agent collaboration offers a promising avenue to transcend these boundaries. However, existing frameworks typically rely on prompt-based test-time interactions or multi-role configurations trained with homogeneous parameters, limiting error correction capabilities and strategic diversity. In this paper, we propose a Multi-Agent Reinforced Training and Inference Framework with Self-Search Scaling (MARTI-MARS2), which integrates policy learning with multi-agent tree search by formulating the multi-agent collaborative exploration process as a dynamic and learnable environment. By allowing agents to iteratively explore and refine within the environment, the framework facilitates evolution from parameter-sharing homogeneous multi-role training to heterogeneous multi-agent training, breaking through single-agent capability limits. We also introduce an efficient inference strategy MARTI-MARS2-T+ to fully exploit the scaling potential of multi-agent collaboration at test time. We conduct extensive experiments across varied model scales (8B, 14B, and 32B) on challenging code generation benchmarks. Utilizing two collaborating 32B models, MARTI-MARS2 achieves 77.7%, outperforming strong baselines like GPT-5.1. Furthermore, MARTI-MARS2 reveals a novel scaling law: shifting from single-agent to homogeneous multi-role and ultimately to heterogeneous multi-agent paradigms progressively yields higher RL performance ceilings, robust TTS capabilities, and greater policy diversity, suggesting that policy diversity is critical for scaling intelligence via multi-agent reinforcement learning.

</details>


### [183] [Dynamic Load Model for Data Centers with Pattern-Consistent Calibration](https://arxiv.org/abs/2602.07859)
*Siyu Lu,Chenhan Xiao,Yang Weng*

Main category: cs.LG

TL;DR: 提出结合物理模型和数据驱动方法的框架，用于大型电子负载建模，通过时序对比学习进行模式一致性校准，保护数据隐私并改善电网仿真精度。


<details>
  <summary>Details</summary>
Motivation: 数据中心快速增长使得大型电子负载建模对电力系统分析日益重要。传统负载模型无法捕捉快速工作负载变化和保护驱动的断开/重连行为。现有物理模型未校准到设施级运行，而数据驱动方法容易过拟合且产生不现实的动态行为。

Method: 设计结合物理结构和数据驱动适应性的框架。物理结构参数化以支持从真实运行数据进行模式一致性校准。采用时序对比学习对齐时间和统计模式，而非轨迹级对齐。本地执行校准，仅共享校准参数以保护数据隐私。

Result: 使用MIT Supercloud、ASU Sol、Blue Waters和ASHRAE数据集的实际运行负载数据进行校准。集成到ANDES平台并在IEEE 39总线、NPCC 140总线和WECC 179总线系统上评估。发现大型电子负载间的相互作用会根本改变扰动后恢复行为，产生复合断开-重连动态和延迟稳定现象，这是未校准模型无法捕捉的。

Conclusion: 提出的框架成功结合了物理模型的可解释性和数据驱动的适应性，通过模式一致性校准改善了大型电子负载建模精度，揭示了负载间相互作用对电网动态的重要影响。

Abstract: The rapid growth of data centers has made large electronic load (LEL) modeling increasingly important for power system analysis. Such loads are characterized by fast workload-driven variability and protection-driven disconnection and reconnection behavior that are not captured by conventional load models. Existing data center load modeling includes physics-based approaches, which provide interpretable structure for grid simulation, and data-driven approaches, which capture empirical workload variability from data. However, physics-based models are typically uncalibrated to facility-level operation, while trajectory alignment in data-driven methods often leads to overfitting and unrealistic dynamic behavior. To resolve these limitations, we design the framework to leverage both physics-based structure and data-driven adaptability. The physics-based structure is parameterized to enable data-driven pattern-consistent calibration from real operational data, supporting facility-level grid planning. We further show that trajectory-level alignment is limited for inherently stochastic data center loads. Therefore, we design the calibration to align temporal and statistical patterns using temporal contrastive learning (TCL). This calibration is performed locally at the facility, and only calibrated parameters are shared with utilities, preserving data privacy. The proposed load model is calibrated by real-world operational load data from the MIT Supercloud, ASU Sol, Blue Waters, and ASHRAE datasets. Then it is integrated into the ANDES platform and evaluated on the IEEE 39-bus, NPCC 140-bus, and WECC 179-bus systems. We find that interactions among LELs can fundamentally alter post-disturbance recovery behavior, producing compound disconnection-reconnection dynamics and delayed stabilization that are not captured by uncalibrated load models.

</details>


### [184] [Direct Soft-Policy Sampling via Langevin Dynamics](https://arxiv.org/abs/2602.07873)
*Donghyeon Ki,Hee-Jun Ahn,Kyungyoon Kim,Byung-Jun Lee*

Main category: cs.LG

TL;DR: 提出NC-LQL方法，通过噪声条件化的朗之万动力学实现软策略采样，解决传统方法表达能力有限或熵估计困难的问题，在MuJoCo基准上达到与扩散方法竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 软策略作为玻尔兹曼分布在强化学习中能平衡探索与利用，但现有方法存在局限：参数化策略表达能力有限，或扩散策略的不可处理似然阻碍熵估计。

Method: 提出噪声条件化朗之万Q学习(NC-LQL)：1) 使用朗之万动力学直接采样软策略，无需显式参数化策略；2) 引入多尺度噪声扰动价值函数，学习噪声条件化Q函数；3) 通过渐进平滑的价值景观实现从全局探索到精确模式细化的采样过程。

Result: 在OpenAI Gym MuJoCo基准测试中，NC-LQL达到与最先进扩散方法竞争的性能，提供了一种简单而强大的在线RL解决方案。

Conclusion: NC-LQL通过噪声条件化朗之万动力学有效解决了软策略实现的挑战，在保持简单性的同时实现了与复杂扩散方法相当的性能，为在线强化学习提供了实用的软策略采样方法。

Abstract: Soft policies in reinforcement learning define policies as Boltzmann distributions over state-action value functions, providing a principled mechanism for balancing exploration and exploitation. However, realizing such soft policies in practice remains challenging. Existing approaches either depend on parametric policies with limited expressivity or employ diffusion-based policies whose intractable likelihoods hinder reliable entropy estimation in soft policy objectives. We address this challenge by directly realizing soft-policy sampling via Langevin dynamics driven by the action gradient of the Q-function. This perspective leads to Langevin Q-Learning (LQL), which samples actions from the target Boltzmann distribution without explicitly parameterizing the policy. However, directly applying Langevin dynamics suffers from slow mixing in high-dimensional and non-convex Q-landscapes, limiting its practical effectiveness. To overcome this, we propose Noise-Conditioned Langevin Q-Learning (NC-LQL), which integrates multi-scale noise perturbations into the value function. NC-LQL learns a noise-conditioned Q-function that induces a sequence of progressively smoothed value landscapes, enabling sampling to transition from global exploration to precise mode refinement. On OpenAI Gym MuJoCo benchmarks, NC-LQL achieves competitive performance compared to state-of-the-art diffusion-based methods, providing a simple yet powerful solution for online RL.

</details>


### [185] [Harpoon: Generalised Manifold Guidance for Conditional Tabular Diffusion](https://arxiv.org/abs/2602.07875)
*Aditya Shankar,Yuandou Wang,Rihan Hai,Lydia Y. Chen*

Main category: cs.LG

TL;DR: HARPOON是一种表格扩散方法，通过流形理论引导无约束样本满足多样化的表格条件，在推理时处理未见约束和不等式约束等任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法在训练时策略无法泛化到推理时未见约束，且难以处理表格填补之外的条件任务。流形理论虽提供原则性指导，但当前公式局限于特定推理目标和连续域。

Method: 将流形理论扩展到表格数据，提出HARPOON表格扩散方法，沿流形几何引导无约束样本，在推理时满足多样化表格条件。

Result: 在填补和不等式约束等任务上验证了理论贡献，HARPOON在多样化数据集上表现优异，展示了流形感知引导对表格数据的实际优势。

Conclusion: HARPOON通过扩展流形理论到表格数据，成功处理多样化推理时条件任务，为表格数据的条件生成提供了灵活有效的解决方案。

Abstract: Generating tabular data under conditions is critical to applications requiring precise control over the generative process. Existing methods rely on training-time strategies that do not generalise to unseen constraints during inference, and struggle to handle conditional tasks beyond tabular imputation. While manifold theory offers a principled way to guide generation, current formulations are tied to specific inference-time objectives and are limited to continuous domains. We extend manifold theory to tabular data and expand its scope to handle diverse inference-time objectives. On this foundation, we introduce HARPOON, a tabular diffusion method that guides unconstrained samples along the manifold geometry to satisfy diverse tabular conditions at inference. We validate our theoretical contributions empirically on tasks such as imputation and enforcing inequality constraints, demonstrating HARPOON'S strong performance across diverse datasets and the practical benefits of manifold-aware guidance for tabular data. Code URL: https://github.com/adis98/Harpoon

</details>


### [186] [GRAFT: Decoupling Ranking and Calibration for Survival Analysis](https://arxiv.org/abs/2602.07884)
*Mohammad Ashhad,Robert Hoehndorf,Ricardo Henao*

Main category: cs.LG

TL;DR: GRAFT是一种新颖的AFT生存分析模型，通过解耦预后排名和校准，结合线性AFT模型与非线性残差神经网络，并集成随机门控进行端到端特征选择，在公开基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 生存分析面临删失数据、高维特征和非线性交互的挑战。经典模型可解释但限制性强，深度学习模型灵活但通常不可解释且对噪声敏感，需要一种既能保持可解释性又具有灵活性的新方法。

Method: 提出GRAFT模型，采用混合架构：线性AFT模型与非线性残差神经网络结合，集成随机门控进行自动特征选择。使用基于局部Kaplan-Meier估计器的随机条件插补，直接优化可微分的C-index对齐排名损失进行训练。

Result: 在公开基准测试中，GRAFT在区分度和校准方面优于基线模型，在高噪声环境下保持鲁棒性和稀疏性。

Conclusion: GRAFT成功解决了生存分析中的关键挑战，提供了一种既灵活又可解释的解决方案，在保持模型稀疏性的同时实现了优异的性能。

Abstract: Survival analysis is complicated by censored data, high-dimensional features, and non-linear interactions. Classical models are interpretable but restrictive, while deep learning models are flexible but often non-interpretable and sensitive to noise. We propose GRAFT (Gated Residual Accelerated Failure Time), a novel AFT model that decouples prognostic ranking from calibration. GRAFT's hybrid architecture combines a linear AFT model with a non-linear residual neural network, and it also integrates stochastic gates for automatic, end-to-end feature selection. The model is trained by directly optimizing a differentiable, C-index-aligned ranking loss using stochastic conditional imputation from local Kaplan-Meier estimators. In public benchmarks, GRAFT outperforms baselines in discrimination and calibration, while remaining robust and sparse in high-noise settings.

</details>


### [187] [Efficient Anti-exploration via VQVAE and Fuzzy Clustering in Offline Reinforcement Learning](https://arxiv.org/abs/2602.07889)
*Long Chen,Yinkui Liu,Shen Li,Bo Tang,Xuemin Hu*

Main category: cs.LG

TL;DR: 提出基于VQVAE和模糊聚类的离线RL反探索方法，解决连续状态-动作对离散化中的维度灾难和信息丢失问题，在D4RL基准上表现优于SOTA方法且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有离线RL中的反探索方法通过离散化连续状态-动作对进行计数，但存在维度灾难和信息丢失问题，导致效率降低、性能下降甚至策略学习失败。

Method: 1) 基于多码本VQVAE的高效伪计数方法离散化状态-动作对；2) 基于该伪计数方法的离线RL反利用方法；3) 基于模糊C均值聚类的码本更新机制提高向量使用率。

Result: 在D4RL基准测试中，该方法在多个复杂任务上表现优于现有SOTA方法，且需要更少的计算成本。

Conclusion: 提出的基于VQVAE和模糊聚类的反探索方法有效解决了连续状态-动作对离散化中的维度灾难和信息丢失问题，提高了离线RL的学习效率和性能。

Abstract: Pseudo-count is an effective anti-exploration method in offline reinforcement learning (RL) by counting state-action pairs and imposing a large penalty on rare or unseen state-action pair data. Existing anti-exploration methods count continuous state-action pairs by discretizing these data, but often suffer from the issues of dimension disaster and information loss in the discretization process, leading to efficiency and performance reduction, and even failure of policy learning. In this paper, a novel anti-exploration method based on Vector Quantized Variational Autoencoder (VQVAE) and fuzzy clustering in offline RL is proposed. We first propose an efficient pseudo-count method based on the multi-codebook VQVAE to discretize state-action pairs, and design an offline RL anti-exploitation method based on the proposed pseudo-count method to handle the dimension disaster issue and improve the learning efficiency. In addition, a codebook update mechanism based on fuzzy C-means (FCM) clustering is developed to improve the use rate of vectors in codebooks, addressing the information loss issue in the discretization process. The proposed method is evaluated on the benchmark of Datasets for Deep Data-Driven Reinforcement Learning (D4RL), and experimental results show that the proposed method performs better and requires less computing cost in multiple complex tasks compared to state-of-the-art (SOTA) methods.

</details>


### [188] [Safety Alignment as Continual Learning: Mitigating the Alignment Tax via Orthogonal Gradient Projection](https://arxiv.org/abs/2602.07892)
*Guanglong Sun,Siyuan Zhang,Liyuan Wang,Jun Zhu,Hang Su,Yi Zhong*

Main category: cs.LG

TL;DR: OGPSA是一种轻量级方法，通过正交梯度投影解决LLM安全对齐中的遗忘问题，在保持通用能力的同时提升安全性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在进行安全对齐后训练时，往往会出现"对齐税"现象——安全性的提升以通用能力（如推理和编码）的下降为代价。作者认为这主要是由于顺序对齐中的持续学习式遗忘造成的，分布偏移和冲突目标导致安全更新覆盖了预训练获得的能力。

Method: 将安全对齐视为持续学习问题，提出正交梯度投影安全对齐（OGPSA）方法。该方法估计一个低秩能力子空间（从小型参考集的梯度中学习），然后将安全梯度投影到该子空间的正交补空间上进行更新。这样产生的安全导向更新能最小化对先前知识的扰动，同时保留对齐能力。

Result: 在监督微调（SFT）、直接偏好优化（DPO）以及顺序SFT→DPO设置中，OGPSA相比标准基线一致地改善了安全-效用帕累托前沿。例如，在Qwen2.5-7B-Instruct模型的SFT→DPO设置下，OGPSA在保持强大安全性的同时恢复了通用能力，将SimpleQA从0.53%提升到3.03%，IFEval从51.94%提升到63.96%。

Conclusion: OGPSA是一种即插即用的轻量级方法，无需大规模回放、辅助目标或重新训练，能够有效缓解安全对齐中的能力遗忘问题，实现安全性和通用能力的更好平衡。

Abstract: Large Language Models (LLMs) often incur an alignment tax: safety post-training can reduce general utility (e.g., reasoning and coding). We argue that this tax primarily arises from continual-learning-style forgetting in sequential alignment, where distribution shift and conflicting objectives cause safety updates to overwrite pre-trained competencies. Accordingly, we cast safety alignment as a continual learning (CL) problem that must balance plasticity (acquiring safety constraints) and stability (preserving general abilities). We propose Orthogonal Gradient Projection for Safety Alignment (OGPSA), a lightweight method that mitigates interference by constraining each safety update to be orthogonal (in a first-order sense) to a learned subspace capturing general capabilities. Specifically, OGPSA estimates a low-rank capability subspace from gradients on a small reference set and projects the safety gradient onto its orthogonal complement before updating. This produces safety-directed updates that minimally perturb prior knowledge while retaining capacity for alignment. OGPSA is plug-and-play and integrates into standard post-training pipelines without large-scale replay, auxiliary objectives, or retraining. Across Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and sequential SFT$\rightarrow$DPO settings, OGPSA consistently improves the safety--utility Pareto frontier over standard baselines. For instance, on Qwen2.5-7B-Instruct under SFT$\rightarrow$DPO, OGPSA preserves strong safety while recovering general capability, improving SimpleQA from 0.53\% to 3.03\% and IFEval from 51.94\% to 63.96\%. Our source code is available at \href{https://github.com/SunGL001/OGPSA}{OGPSA}

</details>


### [189] [Adaptive Acquisition Selection for Bayesian Optimization with Large Language Models](https://arxiv.org/abs/2602.07904)
*Giang Ngo,Dat Phan Trong,Dang Nguyen,Sunil Gupta,Svetha Venkatesh*

Main category: cs.LG

TL;DR: LMABO：使用预训练大语言模型作为零样本在线策略师，从多样化组合中选择最优采集函数，在贝叶斯优化中实现自适应策略选择


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化中采集函数的选择至关重要，但没有单一策略适用于所有问题；最佳选择是非平稳且问题依赖的。现有自适应组合方法通常基于历史函数值做决策，忽略了剩余预算、代理模型特征等更丰富的信息。

Method: 提出LMABO框架，将预训练大语言模型作为贝叶斯优化过程的零样本在线策略师。在每次迭代中，使用结构化状态表示提示LLM从多样化组合中选择最合适的采集函数。

Result: 在50个基准问题上的评估显示，LMABO相比静态策略、自适应组合方法和其他基于LLM的基线方法有显著性能提升。LLM的行为展现出全面的策略适应性，能够根据实时进展调整决策。

Conclusion: LLM的优势在于其能够处理并综合完整的优化状态信息，形成有效的自适应策略，证明了大语言模型在贝叶斯优化策略选择中的潜力。

Abstract: Bayesian Optimization critically depends on the choice of acquisition function, but no single strategy is universally optimal; the best choice is non-stationary and problem-dependent. Existing adaptive portfolio methods often base their decisions on past function values while ignoring richer information like remaining budget or surrogate model characteristics. To address this, we introduce LMABO, a novel framework that casts a pre-trained Large Language Model (LLM) as a zero-shot, online strategist for the BO process. At each iteration, LMABO uses a structured state representation to prompt the LLM to select the most suitable acquisition function from a diverse portfolio. In an evaluation across 50 benchmark problems, LMABO demonstrates a significant performance improvement over strong static, adaptive portfolio, and other LLM-based baselines. We show that the LLM's behavior is a comprehensive strategy that adapts to real-time progress, proving its advantage stems from its ability to process and synthesize the complete optimization state into an effective, adaptive policy.

</details>


### [190] [AceGRPO: Adaptive Curriculum Enhanced Group Relative Policy Optimization for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2602.07906)
*Yuzhu Cai,Zexi Liu,Xinyu Zhu,Cheng Wang,Jiaao Chen,Hanrui Wang,Wei-Chen Wang,Di Jin,Siheng Chen*

Main category: cs.LG

TL;DR: 提出AceGRPO框架解决自主机器学习工程中的行为停滞问题，通过演化数据缓冲区和自适应采样机制提升学习效率，Ace-30B模型在MLE-Bench-Lite上达到100%有效提交率。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的MLE代理存在行为停滞问题，因为参数固定不变。虽然强化学习可以解决这个问题，但在MLE应用中面临执行延迟高和数据选择效率低的挑战。

Method: 提出AceGRPO框架，包含两个核心组件：1) 演化数据缓冲区，将执行轨迹重新利用为可重复使用的训练任务；2) 自适应采样，通过可学习性潜力函数动态优先处理代理学习边界上的任务以最大化学习效率。

Result: 训练的Ace-30B模型在MLE-Bench-Lite上达到100%有效提交率，性能接近前沿专有模型，并优于更大的开源基线模型（如DeepSeek-V3.2），展示了持续迭代优化的强大能力。

Conclusion: AceGRPO框架有效解决了自主机器学习工程中的行为停滞问题，通过创新的数据重用和任务选择机制显著提升了学习效率，为持续迭代优化提供了有效解决方案。

Abstract: Autonomous Machine Learning Engineering (MLE) requires agents to perform sustained, iterative optimization over long horizons. While recent LLM-based agents show promise, current prompt-based agents for MLE suffer from behavioral stagnation due to frozen parameters. Although Reinforcement Learning (RL) offers a remedy, applying it to MLE is hindered by prohibitive execution latency and inefficient data selection. Recognizing these challenges, we propose AceGRPO with two core components: (1) Evolving Data Buffer that continuously repurposes execution traces into reusable training tasks, and (2) Adaptive Sampling guided by a Learnability Potential function, which dynamically prioritizes tasks at the agent's learning frontier to maximize learning efficiency. Leveraging AceGRPO, our trained Ace-30B model achieves a 100% valid submission rate on MLE-Bench-Lite, approaches the performance of proprietary frontier models, and outperforms larger open-source baselines (e.g., DeepSeek-V3.2), demonstrating robust capability for sustained iterative optimization. Code is available at https://github.com/yuzhu-cai/AceGRPO.

</details>


### [191] [CausalCompass: Evaluating the Robustness of Time-Series Causal Discovery in Misspecified Scenarios](https://arxiv.org/abs/2602.07915)
*Huiyang Yi,Xiaojian Shen,Yonggang Wu,Duxin Chen,He Wang,Wenwu Yu*

Main category: cs.LG

TL;DR: CausalCompass是一个评估时间序列因果发现方法在建模假设违反情况下鲁棒性的基准测试套件，实验表明没有单一方法在所有场景下都最优，深度学习方法整体表现更好。


<details>
  <summary>Details</summary>
Motivation: 时间序列因果发现的广泛应用受到两个主要障碍：1）依赖不可测试的因果假设；2）现有基准测试缺乏面向鲁棒性的评估。为了解决这些问题，需要创建一个能够系统评估方法在假设违反情况下表现的基准测试。

Method: 提出了CausalCompass基准测试套件，这是一个灵活可扩展的框架，专门设计用于评估时间序列因果发现方法在八种假设违反场景下的鲁棒性。对代表性算法进行了广泛的基准测试，并进行了超参数敏感性分析。

Result: 实验结果表明：1）没有单一方法在所有设置下都能达到最优性能；2）在各种场景下整体表现最好的方法几乎都是基于深度学习的方法；3）NTS-NOTEARS在实践中严重依赖标准化预处理，在原始设置下表现差但标准化后表现强劲。

Conclusion: CausalCompass为时间序列因果发现方法在假设违反情况下提供了全面系统的评估框架，有助于促进这些方法在现实世界应用中的更广泛采用。代码和数据集已开源。

Abstract: Causal discovery from time series is a fundamental task in machine learning. However, its widespread adoption is hindered by a reliance on untestable causal assumptions and by the lack of robustness-oriented evaluation in existing benchmarks. To address these challenges, we propose CausalCompass, a flexible and extensible benchmark suite designed to assess the robustness of time-series causal discovery (TSCD) methods under violations of modeling assumptions. To demonstrate the practical utility of CausalCompass, we conduct extensive benchmarking of representative TSCD algorithms across eight assumption-violation scenarios. Our experimental results indicate that no single method consistently attains optimal performance across all settings. Nevertheless, the methods exhibiting superior overall performance across diverse scenarios are almost invariably deep learning-based approaches. We further provide hyperparameter sensitivity analyses to deepen the understanding of these findings. We also find, somewhat surprisingly, that NTS-NOTEARS relies heavily on standardized preprocessing in practice, performing poorly in the vanilla setting but exhibiting strong performance after standardization. Finally, our work aims to provide a comprehensive and systematic evaluation of TSCD methods under assumption violations, thereby facilitating their broader adoption in real-world applications. The code and datasets are available at https://github.com/huiyang-yi/CausalCompass.

</details>


### [192] [A Kinetic-Energy Perspective of Flow Matching](https://arxiv.org/abs/2602.07928)
*Ziyun Li,Huancheng Hu,Soon Hoe Lim,Xuyu Li,Fei Gao,Enmao Diao,Zezhen Ding,Michalis Vazirgiannis,Henrik Bostrom*

Main category: cs.LG

TL;DR: 该论文提出动能路径能量(KPE)作为流生成模型的诊断工具，发现KPE与语义保真度正相关，但过高能量会导致记忆化，进而提出无训练的两阶段推理策略KTS来优化生成质量。


<details>
  <summary>Details</summary>
Motivation: 受经典力学启发，作者希望为基于流的生成模型开发一个类似"作用量"的每样本诊断工具，以理解ODE轨迹的动力学特性，并解决生成过程中的记忆化问题。

Method: 引入动能路径能量(KPE)作为轨迹累积动能的度量，基于经验流匹配的闭式解分析轨迹能量与数据密度的理论关系，提出Kinetic Trajectory Shaping(KTS)两阶段推理策略：早期增强运动，后期软着陆。

Result: 发现两个稳健对应关系：1)更高KPE预测更强的语义保真度；2)高KPE轨迹终止于低密度流形边界。理论证明轨迹能量与数据密度相关但非单调，极端能量会导致训练样本的近似复制。

Conclusion: 提出Goldilocks原则：轨迹能量需要适中，过高会导致记忆化。KTS策略能有效减少记忆化，在基准任务上提升生成质量，为流生成模型提供了新的诊断和优化工具。

Abstract: Flow-based generative models can be viewed through a physics lens: sampling transports a particle from noise to data by integrating a time-varying velocity field, and each sample corresponds to a trajectory with its own dynamical effort. Motivated by classical mechanics, we introduce Kinetic Path Energy (KPE), an action-like, per-sample diagnostic that measures the accumulated kinetic effort along an Ordinary Differential Equation (ODE) trajectory. Empirically, KPE exhibits two robust correspondences: (i) higher KPE predicts stronger semantic fidelity; (ii) high-KPE trajectories terminate on low-density manifold frontiers. We further provide theoretical guarantees linking trajectory energy to data density. Paradoxically, this correlation is non-monotonic. At sufficiently high energy, generation can degenerate into memorization. Leveraging the closed-form of empirical flow matching, we show that extreme energies drive trajectories toward near-copies of training examples. This yields a Goldilocks principle and motivates Kinetic Trajectory Shaping (KTS), a training-free two-phase inference strategy that boosts early motion and enforces a late-time soft landing, reducing memorization and improving generation quality across benchmark tasks.

</details>


### [193] [Attention-Based Deep Learning for Early Parkinson's Disease Detection with Tabular Biomedical Data](https://arxiv.org/abs/2602.07933)
*Olamide Samuel Oseni,Ibraheem Omotolani Obanla,Toheeb Aduramomi Jimoh*

Main category: cs.LG

TL;DR: 该研究比较了四种机器学习模型在帕金森病早期检测中的表现，发现基于注意力机制的SAINT模型在多个评估指标上均优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 帕金森病的早期准确检测面临挑战，传统机器学习模型需要大量特征工程且难以捕捉复杂的特征交互关系。研究旨在探索基于注意力的深度学习模型在帕金森病早期检测中的有效性。

Method: 使用UCI机器学习库中的帕金森病生物医学语音测量数据集，对比评估了四种分类模型：多层感知器（MLP）、梯度提升、TabNet和SAINT。SAINT采用双重注意力机制来建模特征交互。

Result: SAINT在所有基线模型中表现最佳，加权精度0.98，加权召回率0.97，加权F1分数0.97，马修斯相关系数0.9990，以及最高的ROC曲线下面积。TabNet和MLP表现竞争性，梯度提升表现最差。

Conclusion: 基于注意力的深度学习架构在帕金森病早期检测中具有诊断潜力，动态特征表示在临床预测任务中至关重要。SAINT的双重注意力机制能有效建模特征交互。

Abstract: Early and accurate detection of Parkinson's disease (PD) remains a critical challenge in medical diagnostics due to the subtlety of early-stage symptoms and the complex, non-linear relationships inherent in biomedical data. Traditional machine learning (ML) models, though widely applied to PD detection, often rely on extensive feature engineering and struggle to capture complex feature interactions. This study investigates the effectiveness of attention-based deep learning models for early PD detection using tabular biomedical data. We present a comparative evaluation of four classification models: Multi-Layer Perceptron (MLP), Gradient Boosting, TabNet, and SAINT, using a benchmark dataset from the UCI Machine Learning Repository consisting of biomedical voice measurements from PD patients and healthy controls.
  Experimental results show that SAINT consistently outperformed all baseline models across multiple evaluation metrics, achieving a weighted precision of 0.98, weighted recall of 0.97, weighted F1-score of 0.97, a Matthews Correlation Coefficient (MCC) of 0.9990, and the highest Area Under the ROC Curve (AUC-ROC). TabNet and MLP demonstrated competitive performance, while Gradient Boosting yielded the lowest overall scores. The superior performance of SAINT is attributed to its dual attention mechanism, which effectively models feature interactions within and across samples.
  These findings demonstrate the diagnostic potential of attention-based deep learning architectures for early Parkinson's disease detection and highlight the importance of dynamic feature representation in clinical prediction tasks.

</details>


### [194] [A Thermodynamic Theory of Learning Part II: Critical Period Closure and Continual Learning Failure](https://arxiv.org/abs/2602.07950)
*Daisuke Okanohara*

Main category: cs.LG

TL;DR: 有限时间学习必然不可逆，导致临界期闭合现象，使兼容表示间的转换变得动态不可达，从而引发灾难性遗忘


<details>
  <summary>Details</summary>
Motivation: 研究有限时间学习中的不可逆性如何影响持续学习，特别是从轨迹层面理解学习路径的动态可达性限制

Method: 将学习建模为参数分布空间中的传输过程，从轨迹层面分析有限耗散对学习路径的约束

Result: 有限耗散不仅限制可达解，还限制动态可达的学习路径；有限时间学习在任务等价实现中进行选择，通过逐步消除自由度导致临界期闭合

Conclusion: 持续学习失败源于先前学习引起的表示自由度的不可逆丧失，而非任务间直接干扰；灾难性遗忘应被重新理解为有限时间耗散施加的动态约束

Abstract: Learning performed over finite time is necessarily irreversible. In Part~I of this series, we modeled learning as a transport process in the space of parameter distributions and derived the Epistemic Speed Limit, which lower-bounds entropy production under finite-time learning.
  In this work (Part~II), we study the consequences of this irreversibility for continual learning from a trajectory-level perspective. We show that finite dissipation constrains not only which solutions are reachable, but which learning paths remain dynamically accessible.
  Although a continuum of task-equivalent realizations can achieve identical task performance, finite-time learning irreversibly selects among these realizations. This selection occurs through the progressive elimination of degrees of freedom that would otherwise enable structural reconfiguration. We refer to this phenomenon as \emph{critical period closure}: beyond a certain stage of learning, transitions between compatible representations become dynamically inaccessible under any finite dissipation budget.
  As a result, continual learning failure arises not from the absence of solutions satisfying multiple tasks, but from an irreversible loss of representational freedom induced by prior learning. This reframes catastrophic forgetting as a dynamical constraint imposed by finite-time dissipation, rather than direct task interference.

</details>


### [195] [An Explainable Multi-Task Similarity Measure: Integrating Accumulated Local Effects and Weighted Fréchet Distance](https://arxiv.org/abs/2602.07966)
*Pablo Hidalgo,Daniel Rodriguez*

Main category: cs.LG

TL;DR: 本文提出了一种基于可解释人工智能（XAI）的多任务相似性度量方法，使用累积局部效应（ALE）曲线和Fréchet距离，能够评估任务间的相似性并支持多任务学习决策。


<details>
  <summary>Details</summary>
Motivation: 在多任务学习中，需要理解哪些任务相似、如何相似以及为什么相似，以便更好地进行知识迁移。现有方法缺乏对任务相似性的系统度量，特别是在可解释性方面存在不足。

Method: 提出基于累积局部效应（ALE）曲线的多任务相似性度量：1）使用ALE曲线分析特征对预测的影响；2）通过加权Fréchet距离比较ALE曲线；3）考虑数据分布和特征重要性；4）引入缩放因子处理不同任务的预测性能差异；5）方法具有模型无关性，适用于单任务和多任务场景。

Result: 在四个数据集上验证了该度量方法的有效性：1个合成数据集和3个真实数据集（Parkinson数据集、自行车共享数据集、CelebA数据集）。结果显示，该度量方法在表格和非表格数据上都与直觉期望的任务相似性一致，能够有效探索任务间关系。

Conclusion: 提出的基于ALE曲线的多任务相似性度量是一个有价值的工具，能够支持任务关系探索和知情决策制定，为多任务学习中的任务选择和知识迁移提供了可解释的量化依据。

Abstract: In many machine learning contexts, tasks are often treated as interconnected components with the goal of leveraging knowledge transfer between them, which is the central aim of Multi-Task Learning (MTL). Consequently, this multi-task scenario requires addressing critical questions: which tasks are similar, and how and why do they exhibit similarity? In this work, we propose a multi-task similarity measure based on Explainable Artificial Intelligence (XAI) techniques, specifically Accumulated Local Effects (ALE) curves.
  ALE curves are compared using the Fréchet distance, weighted by the data distribution, and the resulting similarity measure incorporates the importance of each feature. The measure is applicable in both single-task learning scenarios, where each task is trained separately, and multi-task learning scenarios, where all tasks are learned simultaneously. The measure is model-agnostic, allowing the use of different machine learning models across tasks. A scaling factor is introduced to account for differences in predictive performance across tasks, and several recommendations are provided for applying the measure in complex scenarios.
  We validate this measure using four datasets, one synthetic dataset and three real-world datasets. The real-world datasets include a well-known Parkinson's dataset and a bike-sharing usage dataset -- both structured in tabular format -- as well as the CelebA dataset, which is used to evaluate the application of concept bottleneck encoders in a multitask learning setting. The results demonstrate that the measure aligns with intuitive expectations of task similarity across both tabular and non-tabular data, making it a valuable tool for exploring relationships between tasks and supporting informed decision-making.

</details>


### [196] [On Improving Neurosymbolic Learning by Exploiting the Representation Space](https://arxiv.org/abs/2602.07973)
*Aaditya Naik,Efthymia Tsamoura,Shibo Jin,Mayur Naik,Dan Roth*

Main category: cs.LG

TL;DR: CLIPPER提出了一种在神经符号学习中的剪枝技术，通过整数线性规划去除不一致的标签组合，显著提升现有神经符号引擎的性能。


<details>
  <summary>Details</summary>
Motivation: 在神经符号学习中，输入实例的隐藏黄金标签必须满足逻辑公式，但可能的标签组合空间会指数级增长，导致学习困难。

Method: 利用"具有相似潜在表示的实例可能共享相同标签"的直觉，将剪枝过程形式化为整数线性规划，在尊重逻辑结构的同时丢弃不一致的标签组合。

Result: 在16个复杂神经符号任务基准测试中，CLIPPER将Scallop、Dolphin和ISED等最先进神经符号引擎的性能分别提升高达48%、53%和8%，达到最先进的准确率。

Conclusion: CLIPPER是一种正交于现有训练算法的剪枝方法，可无缝集成到现有神经符号引擎中，有效解决标签组合空间爆炸问题，显著提升学习性能。

Abstract: We study the problem of learning neural classifiers in a neurosymbolic setting where the hidden gold labels of input instances must satisfy a logical formula. Learning in this setting proceeds by first computing (a subset of) the possible combinations of labels that satisfy the formula and then computing a loss using those combinations and the classifiers' scores. One challenge is that the space of label combinations can grow exponentially, making learning difficult. We propose a technique that prunes this space by exploiting the intuition that instances with similar latent representations are likely to share the same label. While this intuition has been widely used in weakly supervised learning, its application in our setting is challenging due to label dependencies imposed by logical constraints. We formulate the pruning process as an integer linear program that discards inconsistent label combinations while respecting logical structure. Our approach, CLIPPER, is orthogonal to existing training algorithms and can be seamlessly integrated with them. Across 16 benchmarks over complex neurosymbolic tasks, we demonstrate that CLIPPER boosts the performance of state-of-the-art neurosymbolic engines like Scallop, Dolphin, and ISED by up to 48%, 53%, and 8%, leading to state-of-the-art accuracies.

</details>


### [197] [Beyond Optimization: Intelligence as Metric-Topology Factorization under Geometric Incompleteness](https://arxiv.org/abs/2602.07974)
*Xin Li*

Main category: cs.LG

TL;DR: 论文提出度量-拓扑分解(MTF)作为统一几何原理：智能不是固定几何中的导航，而是重塑表示几何使期望行为成为稳定吸引子的能力。学习对应度量收缩，任务身份和环境变化编码为拓扑结构单独存储。基于此引入拓扑Urysohn机(TUM)，通过记忆摊销度量推理实现MTF。


<details>
  <summary>Details</summary>
Motivation: 传统ML将智能等同于优化：在固定表示几何中搜索解。这在静态环境中有效，但在分布偏移、任务置换和持续学习等动态场景中失效，轻微拓扑变化就会使学习解失效并引发灾难性遗忘。需要新的几何框架来解决稳定性-可塑性权衡问题。

Method: 提出度量-拓扑分解(MTF)：将表示几何分解为稳定的拓扑结构和可塑的度量变形。学习是度量收缩过程，任务身份和环境变化编码为拓扑特征单独存储。实现拓扑Urysohn机(TUM)，采用记忆摊销度量推理(MAMI)：通过谱任务签名索引摊销的度量变换，使单一学习几何可跨变换环境重用。

Result: MTF解决了固定度量的几何不完备性问题，任何局部度量表示在某些拓扑变换下都会变得奇异或不一致。TUM实现了对任务重排序的鲁棒性、抵抗灾难性遗忘的能力，以及在传统持续学习方法(如EWC)失败的变换上的泛化能力。

Conclusion: 智能的本质不是固定几何中的优化，而是动态重塑表示几何的能力。度量-拓扑分解提供了统一的几何框架，将稳定性与可塑性解耦，通过几何切换而非重新优化实现快速适应，为解决持续学习和分布偏移问题提供了新方向。

Abstract: Contemporary ML often equates intelligence with optimization: searching for solutions within a fixed representational geometry. This works in static regimes but breaks under distributional shift, task permutation, and continual learning, where even mild topological changes can invalidate learned solutions and trigger catastrophic forgetting. We propose Metric-Topology Factorization (MTF) as a unifying geometric principle: intelligence is not navigation through a fixed maze, but the ability to reshape representational geometry so desired behaviors become stable attractors. Learning corresponds to metric contraction (a controlled deformation of Riemannian structure), while task identity and environmental variation are encoded topologically and stored separately in memory. We show any fixed metric is geometrically incomplete: for any local metric representation, some topological transformations make it singular or incoherent, implying an unavoidable stability-plasticity tradeoff for weight-based systems. MTF resolves this by factorizing stable topology from plastic metric warps, enabling rapid adaptation via geometric switching rather than re-optimization. Building on this, we introduce the Topological Urysohn Machine (TUM), implementing MTF through memory-amortized metric inference (MAMI): spectral task signatures index amortized metric transformations, letting a single learned geometry be reused across permuted, reflected, or parity-altered environments. This explains robustness to task reordering, resistance to catastrophic forgetting, and generalization across transformations that defeat conventional continual learning methods (e.g., EWC).

</details>


### [198] [When Is Compositional Reasoning Learnable from Verifiable Rewards?](https://arxiv.org/abs/2602.07992)
*Daniel Barzilai,Yotam Wolf,Ronen Basri*

Main category: cs.LG

TL;DR: 论文从理论上研究了在RLVR训练下自回归模型中组合问题的可学习性，提出了任务优势比的概念，并证明了哪些组合问题可以从结果级反馈中学习。


<details>
  <summary>Details</summary>
Motivation: 尽管RLVR在组合推理方面取得了经验成功，但尚不清楚哪些组合问题仅通过结果级反馈就能学习。需要理论理解RLVR何时成功、何时失败。

Method: 理论分析自回归模型在RLVR训练下的可学习性，提出任务优势比这一关键概念，该概念是组合问题和基础模型的联合属性。

Result: 当正确中间步骤提供明显优势时，组合问题可通过RLVR高效学习；当结构优势不存在时，RLVR可能收敛到次优组合。基础模型的质量在某些情况下决定了优势是否存在。

Conclusion: 任务优势比表征了哪些任务和组合可以从结果级反馈中学习，为RLVR的成功与失败提供了原则性理论理解。

Abstract: The emergence of compositional reasoning in large language models through reinforcement learning with verifiable rewards (RLVR) has been a key driver of recent empirical successes. Despite this progress, it remains unclear which compositional problems are learnable in this setting using outcome-level feedback alone. In this work, we theoretically study the learnability of compositional problems in autoregressive models under RLVR training. We identify a quantity that we call the task-advantage ratio, a joint property of the compositional problem and the base model, that characterizes which tasks and compositions are learnable from outcome-level feedback. On the positive side, using this characterization, we show that compositional problems where correct intermediate steps provide a clear advantage are efficiently learnable with RLVR. We also analyze how such an advantage naturally arises in different problems. On the negative side, when the structural advantage is not present, RLVR may converge to suboptimal compositions. We prove that, in some cases, the quality of the base model determines if such an advantage exists and whether RLVR will converge to a suboptimal solution. We hope our analysis can provide a principled theoretical understanding of when and why RLVR succeeds and when it does not.

</details>


### [199] [Regret Analysis of Unichain Average Reward Constrained MDPs with General Parameterization](https://arxiv.org/abs/2602.08000)
*Anirudh Satheesh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 提出了一种针对单链约束MDP的原始-对偶自然演员-评论家算法，使用多级蒙特卡洛估计和显式预热机制，实现了√T量级的遗憾和约束违反边界，无需混合时间预言机。


<details>
  <summary>Details</summary>
Motivation: 现有约束强化学习的遗憾分析主要依赖于遍历性或强混合时间假设，这些假设在存在瞬态状态时失效。需要一种能处理单链动态而不需要混合时间预言机的方法。

Method: 提出原始-对偶自然演员-评论家算法，结合多级蒙特卡洛估计器和显式预热机制来处理单链动态，无需混合时间假设。

Result: 建立了有限时间遗憾和累积约束违反边界，尺度为Õ(√T)，受策略和评论家参数化近似误差影响，将最优阶保证扩展到更广泛的CMDP类别。

Conclusion: 该算法成功地将最优阶性能保证扩展到单链约束MDP，无需混合时间假设，为更广泛的约束强化学习问题提供了理论保证。

Abstract: We study infinite-horizon average-reward constrained Markov decision processes (CMDPs) under the unichain assumption and general policy parameterizations. Existing regret analyses for constrained reinforcement learning largely rely on ergodicity or strong mixing-time assumptions, which fail to hold in the presence of transient states. We propose a primal--dual natural actor--critic algorithm that leverages multi-level Monte Carlo (MLMC) estimators and an explicit burn-in mechanism to handle unichain dynamics without requiring mixing-time oracles. Our analysis establishes finite-time regret and cumulative constraint violation bounds that scale as $\tilde{O}(\sqrt{T})$, up to approximation errors arising from policy and critic parameterization, thereby extending order-optimal guarantees to a significantly broader class of CMDPs.

</details>


### [200] [Don't Always Pick the Highest-Performing Model: An Information Theoretic View of LLM Ensemble Selection](https://arxiv.org/abs/2602.08003)
*Yigit Turkmen,Baturalp Buyukates,Melih Bastopcu*

Main category: cs.LG

TL;DR: 本文提出一种基于互信息的贪心算法，用于在有限查询预算下选择最优的LLM集成组合，解决模型强相关时集成性能饱和的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型集成时存在强相关性，导致即使增加模型数量性能也会饱和。需要解决在有限查询预算下如何选择最优模型组合的问题。

Method: 1) 将集成选择问题形式化为最大化真实标签与所选模型预测之间的互信息；2) 使用高斯copula建模模型间的相关误差，分析信息论误差下界；3) 提出贪心互信息选择算法，直接从数据估计信息量，在查询预算下迭代构建集成。

Result: 在MEDMCQA、MMLU问答数据集和IMDB情感分类数据集上测试，该方法在相同查询预算下始终优于强基线方法。

Conclusion: 基于互信息的贪心选择算法能有效解决LLM集成中的模型选择问题，在有限预算下获得更好的性能，为处理模型相关性提供了理论分析和实用解决方案。

Abstract: Large language models (LLMs) are often ensembled together to improve overall reliability and robustness, but in practice models are strongly correlated. This raises a fundamental question: which models should be selected when forming an LLM ensemble? We formulate budgeted ensemble selection as maximizing the mutual information between the true label and predictions of the selected models. Furthermore, to explain why performance can saturate even with many models, we model the correlated errors of the models using Gaussian-copula and show an information-theoretic error floor for the performance of the ensemble. Motivated by these, we propose a simple greedy mutual-information selection algorithm that estimates the required information terms directly from data and iteratively builds an ensemble under a query budget. We test our approach in two question answering datasets and one binary sentiment classification dataset: MEDMCQA, MMLU, and IMDB movie reviews. Across all datasets, we observe that our method consistently outperforms strong baselines under the same query budget.

</details>


### [201] [From $O(mn)$ to $O(r^2)$: Two-Sided Low-Rank Communication for Adam in Distributed Training with Memory Efficiency](https://arxiv.org/abs/2602.08007)
*Sizhe Dang,Jiaqi Shao,Xiaodong Zheng,Guang Dai,Yan Song,Haishan Ye*

Main category: cs.LG

TL;DR: TSR-Adam提出了一种双面低秩通信优化器，通过同步紧凑的核心矩阵将Adam优化器的通信负载从O(mn)降低到O(r²)，显著减少分布式训练中的通信开销。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型规模扩大，数据并行分布式训练中带宽受限的梯度同步成为关键瓶颈。现有的投影低秩优化器主要关注内存效率，但在通信受限的训练中仍不理想：单边同步仍需传输O(rn)对象，刷新步骤可能主导峰值通信量。

Method: 提出TSR-Adam，为Adam系列更新引入双面低秩通信：1) 同步紧凑核心矩阵U⊤GV∈ℝ^{r×r}，将主要每步负载从O(mn)降至O(r²)；2) 采用基于随机SVD的刷新避免全梯度同步；3) 将低秩通信扩展到嵌入梯度，使用嵌入特定秩和刷新计划。

Result: 在从6000万到10亿参数的预训练中，TSR-Adam将平均每步通信字节减少13倍；在GLUE微调中通信减少25倍，同时保持可比性能。论文还提供了所提更新的理论平稳性分析。

Conclusion: TSR-Adam通过双面低秩通信有效解决了大规模分布式训练中的通信瓶颈问题，显著减少了通信开销，同时保持了模型性能，为大规模基础模型训练提供了高效的优化方案。

Abstract: As foundation models continue to scale, pretraining increasingly relies on data-parallel distributed optimization, making bandwidth-limited gradient synchronization a key bottleneck. Orthogonally, projection-based low-rank optimizers were mainly designed for memory efficiency, but remain suboptimal for communication-limited training: one-sided synchronization still transmits an $O(rn)$ object for an $m\times n$ matrix gradient and refresh steps can dominate peak communicated bytes. We propose TSR, which brings two-sided low-rank communication to Adam-family updates (TSR-Adam) by synchronizing a compact core $U^\top G V\in\mathbb{R}^{r\times r}$, reducing the dominant per-step payload from $O(mn)$ to $O(r^2)$ while keeping moment states in low-dimensional cores. To further reduce the peak communication from subspace refresh, TSR-Adam adopts a randomized SVD-based refresh that avoids full-gradient synchronization. We additionally extend low-rank communication to embedding gradients with embedding-specific ranks and refresh schedules, yielding additional communication and memory savings over keeping embeddings dense. Across pretraining from 60M to 1B model scales, TSR-Adam reduces average communicated bytes per step by $13\times$, and on GLUE fine-tuning it reduces communication by $25\times$, while achieving comparable performance; we further provide a theoretical stationarity analysis for the proposed update. Code is available at https://github.com/DKmiyan/TSR-Adam.

</details>


### [202] [A Unified Density Operator View of Flow Control and Merging](https://arxiv.org/abs/2602.08012)
*Riccardo De Santi,Malte Franke,Ya-Ping Hsieh,Andreas Krause*

Main category: cs.LG

TL;DR: 提出统一概率空间框架，将流模型奖励适应与模型融合统一处理，支持奖励引导的流模型融合及复杂逻辑操作


<details>
  <summary>Details</summary>
Motivation: 大规模流模型和扩散模型的发展带来了两个基本挑战：预训练流的基于控制的奖励适应，以及多个模型的集成（流融合）。现有方法分别处理这两个问题，需要统一的框架来同时解决奖励适应和模型融合

Method: 提出统一概率空间框架，将奖励适应和流融合作为极限情况统一处理；引入奖励引导流融合（RFM）方法，使用镜像下降方案将奖励引导流融合转化为一系列标准微调问题

Result: 为奖励引导和纯流融合提供了首个理论保证；在分子设计和低能构象生成等高维任务中展示了方法的有效性

Conclusion: 提出的统一框架能够表达丰富的生成模型密度操作，包括交集、并集、插值及其奖励引导版本，通过生成电路支持复杂逻辑表达式，为流模型融合提供了理论基础和实用方法

Abstract: Recent progress in large-scale flow and diffusion models raised two fundamental algorithmic challenges: (i) control-based reward adaptation of pre-trained flows, and (ii) integration of multiple models, i.e., flow merging. While current approaches address them separately, we introduce a unifying probability-space framework that subsumes both as limit cases, and enables reward-guided flow merging, allowing principled, task-aware combination of multiple pre-trained flows (e.g., merging priors while maximizing drug-discovery utilities). Our formulation renders possible to express a rich family of operators over generative models densities, including intersection (e.g., to enforce safety), union (e.g., to compose diverse models), interpolation (e.g., for discovery), their reward-guided counterparts, as well as complex logical expressions via generative circuits. Next, we introduce Reward-Guided Flow Merging (RFM), a mirror-descent scheme that reduces reward-guided flow merging to a sequence of standard fine-tuning problems. Then, we provide first-of-their-kind theoretical guarantees for reward-guided and pure flow merging via RFM. Ultimately, we showcase the capabilities of the proposed method on illustrative settings providing visually interpretable insights, and apply our method to high-dimensional de-novo molecular design and low-energy conformer generation.

</details>


### [203] [The Rise of Sparse Mixture-of-Experts: A Survey from Algorithmic Foundations to Decentralized Architectures and Vertical Domain Applications](https://arxiv.org/abs/2602.08019)
*Dong Pan,Bingtao Li,Yongsheng Zheng,Jiren Ma,Victor Fei*

Main category: cs.LG

TL;DR: 这篇论文是关于稀疏混合专家(MoE)模型的综述，系统性地回顾了MoE的基础原理、核心组件、去中心化范式、垂直领域应用，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: MoE作为大语言模型的重要分支，通过稀疏条件计算机制显著提高了计算效率，在水平和垂直领域都有广泛应用。然而，现有MoE综述存在覆盖不足或关键领域探索不够深入的问题，需要一篇更全面的综述来填补这一空白。

Method: 首先分析MoE的基础原理，深入探讨其核心组件（路由网络和专家网络）；然后扩展到去中心化范式；接着重点探索垂直领域应用；最后识别关键挑战和未来研究方向。

Result: 这篇综述是目前MoE领域最全面的回顾，旨在为研究者和实践者提供有价值的资源，帮助他们了解最新进展。

Conclusion: MoE架构通过稀疏条件计算提供了强大的扩展能力，去中心化范式进一步释放了其潜力。该综述系统性地总结了MoE的发展，为未来研究指明了方向。

Abstract: The sparse Mixture of Experts(MoE) architecture has evolved as a powerful approach for scaling deep learning models to more parameters with comparable computation cost. As an important branch of large language model(LLM), MoE model only activate a subset of experts based on a routing network. This sparse conditional computation mechanism significantly improves computational efficiency, paving a promising path for greater scalability and cost-efficiency. It not only enhance downstream applications such as natural language processing, computer vision, and multimodal in various horizontal domains, but also exhibit broad applicability across vertical domains. Despite the growing popularity and application of MoE models across various domains, there lacks a systematic exploration of recent advancements of MoE in many important fields. Existing surveys on MoE suffer from limitations such as lack coverage or none extensively exploration of key areas. This survey seeks to fill these gaps. In this paper, Firstly, we examine the foundational principles of MoE, with an in-depth exploration of its core components-the routing network and expert network. Subsequently, we extend beyond the centralized paradigm to the decentralized paradigm, which unlocks the immense untapped potential of decentralized infrastructure, enables democratization of MoE development for broader communities, and delivers greater scalability and cost-efficiency. Furthermore we focus on exploring its vertical domain applications. Finally, we also identify key challenges and promising future research directions. To the best of our knowledge, this survey is currently the most comprehensive review in the field of MoE. We aim for this article to serve as a valuable resource for both researchers and practitioners, enabling them to navigate and stay up-to-date with the latest advancements.

</details>


### [204] [Sharp analysis of linear ensemble sampling](https://arxiv.org/abs/2602.08026)
*Arya Akhavan,David Janz,Csaba Szepesvári*

Main category: cs.LG

TL;DR: 本文分析了线性集成采样(ES)在高斯扰动下的随机线性bandit问题，证明了当集成规模m=Θ(d log n)时，ES能达到$\tilde O(d^{3/2}\sqrt n)$的高概率遗憾界，填补了与Thompson采样基准的差距，同时保持了可比较的计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究线性集成采样在随机线性bandit中的性能，旨在填补其与Thompson采样基准之间的理论差距，同时保持计算效率。当前ES的分析存在理论缺口，需要更精确的界限。

Method: 采用连续时间视角，将离散时间问题转化为m个独立布朗运动的时均匀超越问题。通过分析集成采样在高斯扰动下的行为，使用布朗运动理论来推导遗憾界限。

Result: 当集成规模m=Θ(d log n)时，ES实现了$\tilde O(d^{3/2}\sqrt n)$的高概率遗憾界，这与Thompson采样基准相匹配，同时计算复杂度保持可比。

Conclusion: 线性集成采样在适当的集成规模下可以达到Thompson采样的理论性能，连续时间视角为分析随机探索提供了自然且必要的方法，离散时间问题似乎需要连续时间解决方案才能获得尖锐的界限。

Abstract: We analyse linear ensemble sampling (ES) with standard Gaussian perturbations in stochastic linear bandits. We show that for ensemble size $m=Θ(d\log n)$, ES attains $\tilde O(d^{3/2}\sqrt n)$ high-probability regret, closing the gap to the Thompson sampling benchmark while keeping computation comparable. The proof brings a new perspective on randomized exploration in linear bandits by reducing the analysis to a time-uniform exceedance problem for $m$ independent Brownian motions. Intriguingly, this continuous-time lens is not forced; it appears natural--and perhaps necessary: the discrete-time problem seems to be asking for a continuous-time solution, and we know of no other way to obtain a sharp ES bound.

</details>


### [205] [Horizon Imagination: Efficient On-Policy Training in Diffusion World Models](https://arxiv.org/abs/2602.08032)
*Lior Cohen,Ofir Nabati,Kaixin Wang,Navdeep Kumar,Shie Mannor*

Main category: cs.LG

TL;DR: 提出Horizon Imagination (HI)方法，用于扩散世界模型的并行未来观测去噪，显著降低计算成本同时保持控制性能


<details>
  <summary>Details</summary>
Motivation: 基于扩散的世界模型在强化学习中具有高生成保真度，但面临严重的效率挑战。现有方法要么需要重型推理模型，要么依赖高度顺序化的想象过程，都带来过高的计算成本

Method: 提出Horizon Imagination (HI)方法：1) 在策略想象过程，支持离散随机策略；2) 并行去噪多个未来观测；3) 包含稳定机制；4) 新颖的采样调度，将去噪预算与有效视野解耦，支持子帧预算

Result: 在Atari 100K和Craftium实验中，HI方法仅用一半去噪步数的子帧预算就能保持控制性能，在不同调度下实现更优的生成质量

Conclusion: Horizon Imagination方法有效解决了扩散世界模型在强化学习中的效率问题，通过并行去噪和灵活的调度机制，在保持性能的同时大幅降低计算成本

Abstract: We study diffusion-based world models for reinforcement learning, which offer high generative fidelity but face critical efficiency challenges in control. Current methods either require heavyweight models at inference or rely on highly sequential imagination, both of which impose prohibitive computational costs. We propose Horizon Imagination (HI), an on-policy imagination process for discrete stochastic policies that denoises multiple future observations in parallel. HI incorporates a stabilization mechanism and a novel sampling schedule that decouples the denoising budget from the effective horizon over which denoising is applied while also supporting sub-frame budgets. Experiments on Atari 100K and Craftium show that our approach maintains control performance with a sub-frame budget of half the denoising steps and achieves superior generation quality under varied schedules. Code is available at https://github.com/leor-c/horizon-imagination.

</details>


### [206] [The Benefits of Diversity: Combining Comparisons and Ratings for Efficient Scoring](https://arxiv.org/abs/2602.08033)
*Julien Fageot,Matthias Grossglauser,Lê-Nguyên Hoang,Matteo Tacchi-Bénard,Oscar Villemaud*

Main category: cs.LG

TL;DR: SCoRa模型通过结合比较和评分两种偏好表达方式，在实体排序任务中优于单一方法


<details>
  <summary>Details</summary>
Motivation: 解决长期存在的争论：人类应该单独评估实体还是进行比较评估？探索结合两种偏好表达方式是否能超越单一方法

Method: 提出SCoRa（从比较和评分中评分）统一概率模型，能够同时学习比较和评分信号，并证明其MAP估计器具有良好的单调性和鲁棒性保证

Result: 实证表明SCoRa能够准确恢复分数，即使在模型不匹配的情况下也能工作。在需要准确排序顶部实体的现实场景中，结合比较和评分的方法优于单独使用任一种方法

Conclusion: 结合比较和评分两种偏好表达方式能够提供更准确的实体排序，SCoRa为偏好学习提供了多功能的基础框架，特别适用于需要精确顶部排序的关键场景

Abstract: Should humans be asked to evaluate entities individually or comparatively? This question has been the subject of long debates. In this work, we show that, interestingly, combining both forms of preference elicitation can outperform the focus on a single kind. More specifically, we introduce SCoRa (Scoring from Comparisons and Ratings), a unified probabilistic model that allows to learn from both signals. We prove that the MAP estimator of SCoRa is well-behaved. It verifies monotonicity and robustness guarantees. We then empirically show that SCoRa recovers accurate scores, even under model mismatch. Most interestingly, we identify a realistic setting where combining comparisons and ratings outperforms using either one alone, and when the accurate ordering of top entities is critical. Given the de facto availability of signals of multiple forms, SCoRa additionally offers a versatile foundation for preference learning.

</details>


### [207] [TAAM:Inductive Graph-Class Incremental Learning with Task-Aware Adaptive Modulation](https://arxiv.org/abs/2602.08036)
*Jingtao Liu,Xinming Zhang*

Main category: cs.LG

TL;DR: TAAM提出基于轻量级神经突触调制器（NSMs）的图持续学习方法，无需数据回放，通过任务特定模块指导固定GNN骨干网络，并引入锚点多跳传播（AMP）解决未知任务ID问题。


<details>
  <summary>Details</summary>
Motivation: 现有图持续学习方法依赖数据回放策略，存在内存限制和隐私问题，且难以解决稳定性-可塑性困境。需要一种无需回放、能有效处理未知任务ID的轻量级方法。

Method: 提出任务感知自适应调制（TAAM）：1）为每个新任务训练轻量级神经突触调制器（NSMs），作为"专家模块"对共享GNN骨干网络进行节点注意力自适应调制；2）引入锚点多跳传播（AMP）处理未知任务ID场景。

Result: 在更严格的归纳学习场景下，TAAM在八个数据集上全面优于现有最先进方法，无需数据回放即可有效防止灾难性遗忘。

Conclusion: 轻量级任务特定模块能有效指导固定GNN骨干网络，TAAM通过NSMs和AMP解决了图持续学习中的数据回放依赖和未知任务ID问题，在多个数据集上表现出色。

Abstract: Graph Continual Learning (GCL) aims to solve the challenges of streaming graph data. However, current methods often depend on replay-based strategies, which raise concerns like memory limits and privacy issues, while also struggling to resolve the stability-plasticity dilemma. In this paper, we suggest that lightweight, task-specific modules can effectively guide the reasoning process of a fixed GNN backbone. Based on this idea, we propose Task-Aware Adaptive Modulation (TAAM). The key component of TAAM is its lightweight Neural Synapse Modulators (NSMs). For each new task, a dedicated NSM is trained and then frozen, acting as an "expert module." These modules perform detailed, node-attentive adaptive modulation on the computational flow of a shared GNN backbone. This setup ensures that new knowledge is kept within compact, task-specific modules, naturally preventing catastrophic forgetting without using any data replay. Additionally, to address the important challenge of unknown task IDs in real-world scenarios, we propose and theoretically prove a novel method named Anchored Multi-hop Propagation (AMP). Notably, we find that existing GCL benchmarks have flaws that can cause data leakage and biased evaluations. Therefore, we conduct all experiments in a more rigorous inductive learning scenario. Extensive experiments show that TAAM comprehensively outperforms state-of-the-art methods across eight datasets. Code and Datasets are available at: https://github.com/1iuJT/TAAM_AAMAS2026.

</details>


### [208] [FIRE: Frobenius-Isometry Reinitialization for Balancing the Stability-Plasticity Tradeoff](https://arxiv.org/abs/2602.08040)
*Isaac Han,Sangyeon Park,Seungwon Oh,Donghu Kim,Hojoon Lee,Kyung-Joong Kim*

Main category: cs.LG

TL;DR: FIRE是一种平衡稳定性和可塑性权衡的权重重新初始化方法，通过优化SFE和DfI指标，在持续学习中优于标准方法。


<details>
  <summary>Details</summary>
Motivation: 在非平稳数据上训练的深度神经网络需要平衡稳定性（保留先验知识）和可塑性（适应新任务）。标准重新初始化方法难以调优：保守的无法恢复可塑性，激进的会擦除有用知识。

Method: 提出FIRE方法，通过平方Frobenius误差（SFE）量化稳定性（接近过去权重），通过偏离等距性（DfI）量化可塑性（权重各向同性）。通过约束优化问题求解重新初始化点：最小化SFE，约束DfI为零，使用Newton-Schulz迭代高效近似。

Result: 在持续视觉学习（CIFAR-10 + ResNet-18）、语言建模（OpenWebText + GPT-0.1B）和强化学习（HumanoidBench + SAC，Atari + DQN）三个领域评估，FIRE始终优于无干预的朴素训练和标准重新初始化方法。

Conclusion: FIRE能够有效平衡稳定性和可塑性权衡，为持续学习中的权重重新初始化提供了原则性解决方案。

Abstract: Deep neural networks trained on nonstationary data must balance stability (i.e., retaining prior knowledge) and plasticity (i.e., adapting to new tasks). Standard reinitialization methods, which reinitialize weights toward their original values, are widely used but difficult to tune: conservative reinitializations fail to restore plasticity, while aggressive ones erase useful knowledge. We propose FIRE, a principled reinitialization method that explicitly balances the stability-plasticity tradeoff. FIRE quantifies stability through Squared Frobenius Error (SFE), measuring proximity to past weights, and plasticity through Deviation from Isometry (DfI), reflecting weight isotropy. The reinitialization point is obtained by solving a constrained optimization problem, minimizing SFE subject to DfI being zero, which is efficiently approximated by Newton-Schulz iteration. FIRE is evaluated on continual visual learning (CIFAR-10 with ResNet-18), language modeling (OpenWebText with GPT-0.1B), and reinforcement learning (HumanoidBench with SAC and Atari games with DQN). Across all domains, FIRE consistently outperforms both naive training without intervention and standard reinitialization methods, demonstrating effective balancing of the stability-plasticity tradeoff.

</details>


### [209] [Implicit Strategic Optimization: Rethinking Long-Horizon Decision-Making in Adversarial Poker Environments](https://arxiv.org/abs/2602.08041)
*Boyang Xia,Weiyou Tian,Qingnan Ren,Jiaqi Huang,Jie Xiao,Shuo Lu,Kai Wang,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: 提出ISO框架，通过预测战略环境来优化LLM智能体在长期对抗游戏中的表现，结合战略奖励模型和乐观学习规则，在德州扑克和宝可梦游戏中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 在长期对抗游戏中，传统基于胜率的短视优化方法无法处理随时间演变的战略外部性，导致遗憾分析失效，需要能够预测战略环境变化的框架。

Method: 提出隐式战略优化(ISO)框架：1) 战略奖励模型(SRM)估计行动的长期战略价值；2) iso-grpo，一种情境条件乐观学习规则；智能体预测当前战略环境并在线更新策略。

Result: 理论证明：当预测误差有界时，获得与静态游戏相当的次线性情境遗憾和均衡收敛保证。实验：在6人无限注德州扑克和竞争性宝可梦游戏中，长期回报持续优于强LLM和RL基线，在受控预测噪声下性能优雅下降。

Conclusion: ISO框架通过预测战略环境有效解决了长期对抗游戏中的优化问题，理论保证良好，实验表现优越，为LLM智能体在复杂战略环境中的学习提供了有效方案。

Abstract: Training large language model (LLM) agents for adversarial games is often driven by episodic objectives such as win rate. In long-horizon settings, however, payoffs are shaped by latent strategic externalities that evolve over time, so myopic optimization and variation-based regret analyses can become vacuous even when the dynamics are predictable. To solve this problem, we introduce Implicit Strategic Optimization (ISO), a prediction-aware framework in which each agent forecasts the current strategic context and uses it to update its policy online. ISO combines a Strategic Reward Model (SRM) that estimates the long-run strategic value of actions with iso-grpo, a context-conditioned optimistic learning rule. We prove sublinear contextual regret and equilibrium convergence guarantees whose dominant terms scale with the number of context mispredictions; when prediction errors are bounded, our bounds recover the static-game rates obtained when strategic externalities are known. Experiments in 6-player No-Limit Texas Hold'em and competitive Pokemon show consistent improvements in long-term return over strong LLM and RL baselines, and graceful degradation under controlled prediction noise.

</details>


### [210] [V-ABFT: Variance-Based Adaptive Threshold for Fault-Tolerant Matrix Multiplication in Mixed-Precision Deep Learning](https://arxiv.org/abs/2602.08043)
*Yiheng Gao,Qin Hua,Zizhong Chen*

Main category: cs.LG

TL;DR: V-ABFT是一种基于方差的自适应阈值算法，用于检测矩阵乘法中的静默数据损坏，相比现有方法显著提高了检测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有ABFT阈值确定方法存在严重问题：分析方法过于保守，而概率方法如A-ABFT的阈值比实际舍入误差大160-4200倍，导致检测精度不足。

Method: 提出V-ABFT算法，通过直接建模验证差异并利用统计方差估计，实现更紧密的误差界限。算法仅需O(n)复杂度，使用最大/最小/均值统计量。

Result: V-ABFT将阈值与实际误差比降低到FP32/FP64的7-20倍和BF16的48-158倍，相比A-ABFT有6-48倍改进，保持零误报率。对于融合内核实现，低精度GEMM可使用FP32级阈值，实现约1000倍更精细的检测粒度。

Conclusion: V-ABFT在保持零误报率的同时显著提高了检测精度，复杂度更低，平台无关，已集成到NPU和GPU的容错GEMM实现中，适用于各种数据分布。

Abstract: Algorithm-Based Fault Tolerance (ABFT) is widely adopted to detect silent data corruptions (SDCs) in matrix multiplication, a cornerstone operation in deep learning systems. However, existing threshold determination methods face critical challenges: analytical bounds are overly conservative, while probabilistic approaches like A-ABFT yield thresholds $160$--$4200\times$ larger than actual rounding errors. We present V-ABFT, a variance-based adaptive threshold algorithm that achieves tighter error bounds by directly modeling the verification difference. By leveraging statistical variance estimation, V-ABFT reduces the threshold-to-actual-error ratio to approximately $7$--$20\times$ for FP32/FP64 and $48$--$158\times$ for BF16, representing a \textbf{6--48$\times$ improvement} over A-ABFT while maintaining zero false positive rate across BF16, FP16, FP32, and FP64 precisions. Furthermore, we demonstrate that for fused-kernel ABFT implementations that verify before output quantization, low-precision GEMM can use FP32-level thresholds ($e_{\max} \approx 10^{-6}$), enabling \textbf{$\sim$1000$\times$ finer detection granularity} compared to offline verification with low-precision output ($e_{\max} \approx 10^{-3}$). We reproduce A-ABFT's experimental setup and validate our implementation against the original paper's results. Our method requires only $O(n)$ complexity using max/min/mean statistics, compared to A-ABFT's $O(pn)$ for finding $p$ largest values. Extensive experiments on synthetic data and real model weights (LLaMA-7B, GPT-2, ViT) demonstrate V-ABFT's effectiveness across diverse distributions. V-ABFT is platform-agnostic and has been integrated into fault-tolerant GEMM implementations on both NPUs and GPUs.

</details>


### [211] [Interpretable Fuzzy Systems For Forward Osmosis Desalination](https://arxiv.org/abs/2602.08050)
*Qusai Khaled,Uzay Kaymak,Laura Genga*

Main category: cs.LG

TL;DR: 提出一种人机协同方法构建可解释模糊规则系统，用于预测正渗透海水淡化生产力，在保持语义可解释性的同时达到与聚类方法相当的预测性能。


<details>
  <summary>Details</summary>
Motivation: 在水处理应用中，模糊规则系统的可解释性至关重要，因为决策直接影响公共健康。虽然结构可解释性已通过多目标算法解决，但语义可解释性常因模糊集区分度低而受损。

Method: 集成专家驱动的网格划分生成可区分的隶属函数、领域指导的特征工程减少冗余、基于触发强度的规则剪枝，构建人机协同的模糊规则系统。

Result: 该方法在预测正渗透海水淡化生产力方面，达到了与基于聚类的模糊规则系统相当的预测性能，同时保持了语义可解释性并满足结构复杂性约束。

Conclusion: 该方法为水处理应用提供了一个可解释的解决方案，通过人机协同方法在保持语义可解释性的同时实现了良好的预测性能。

Abstract: Preserving interpretability in fuzzy rule-based systems (FRBS) is vital for water treatment, where decisions impact public health. While structural interpretability has been addressed using multi-objective algorithms, semantic interpretability often suffers due to fuzzy sets with low distinguishability. We propose a human-in-the-loop approach for developing interpretable FRBS to predict forward osmosis desalination productivity. Our method integrates expert-driven grid partitioning for distinguishable membership functions, domain-guided feature engineering to reduce redundancy, and rule pruning based on firing strength. This approach achieved comparable predictive performance to cluster-based FRBS while maintaining semantic interpretability and meeting structural complexity constraints, providing an explainable solution for water treatment applications.

</details>


### [212] [Epigraph-Guided Flow Matching for Safe and Performant Offline Reinforcement Learning](https://arxiv.org/abs/2602.08054)
*Manan Tayal,Mumuksh Tayal*

Main category: cs.LG

TL;DR: EpiFlow：一种基于流匹配的离线强化学习框架，通过epigraph重构将安全约束转化为可行性值函数，在保证性能的同时实现近乎零的安全违规。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在安全关键领域具有潜力，但现有方法难以同时保证强安全性、高性能和数据分布一致性。现有方法要么允许安全违规，要么过于保守，要么难以平衡安全、奖励优化和数据分布。

Method: 将安全离线RL建模为状态约束的最优控制问题，通过epigraph重构学习可行性值函数，避免解耦目标或后处理过滤。基于epigraph值函数对行为分布进行重加权，并通过流匹配拟合生成策略，实现高效且分布一致的采样。

Result: 在包括Safety-Gymnasium基准在内的多个安全关键任务中，EpiFlow实现了具有竞争力的回报和近乎零的实证安全违规，证明了epigraph引导策略合成的有效性。

Conclusion: EpiFlow通过epigraph重构将安全约束纳入离线RL框架，实现了安全与性能的联合优化，为安全关键领域的离线强化学习提供了有效的解决方案。

Abstract: Offline reinforcement learning (RL) provides a compelling paradigm for training autonomous systems without the risks of online exploration, particularly in safety-critical domains. However, jointly achieving strong safety and performance from fixed datasets remains challenging. Existing safe offline RL methods often rely on soft constraints that allow violations, introduce excessive conservatism, or struggle to balance safety, reward optimization, and adherence to the data distribution. To address this, we propose Epigraph-Guided Flow Matching (EpiFlow), a framework that formulates safe offline RL as a state-constrained optimal control problem to co-optimize safety and performance. We learn a feasibility value function derived from an epigraph reformulation of the optimal control problem, thereby avoiding the decoupled objectives or post-hoc filtering common in prior work. Policies are synthesized by reweighting the behavior distribution based on this epigraph value function and fitting a generative policy via flow matching, enabling efficient, distribution-consistent sampling. Across various safety-critical tasks, including Safety-Gymnasium benchmarks, EpiFlow achieves competitive returns with near-zero empirical safety violations, demonstrating the effectiveness of epigraph-guided policy synthesis.

</details>


### [213] [Compiler-Assisted Speculative Sampling for Accelerated LLM Inference on Heterogeneous Edge Devices](https://arxiv.org/abs/2602.08060)
*Alejandro Ruiz y Mesa,Guilherme Korol,Moritz Riesteter,João Paulo Cardoso de Lima,Jeronimo Castrillon*

Main category: cs.LG

TL;DR: 该论文提出了一种用于边缘设备上LLM推理的推测解码优化方法，通过分析成本模型指导异构硬件配置和粗粒度分区，在边缘典型短输入序列长度下实现加速。


<details>
  <summary>Details</summary>
Motivation: 边缘设备部署LLM面临严重的延迟约束，特别是在实时应用中。推测解码是缓解序列化token生成低效性的有前景技术，但在边缘部署面临两个主要挑战：1) 如何将SD集成到基于编译器的流程中而不牺牲性能或可编程性；2) 如何通过精心设计的划分策略利用现代SoC的异构计算资源。

Method: 使用分析成本模型探索异构硬件配置，指导LLM子图的粗粒度划分，特别针对边缘典型的短输入序列长度。该模型预测何时推测采样和异构执行联合有益，并在配备六核Cortex-A CPU和Mali GPU的边缘设备上进行验证。

Result: 在翻译任务上实现了最高1.68倍的加速，与分析预期紧密匹配。

Conclusion: 该工作通过分析成本模型和异构硬件分区策略，成功解决了边缘设备上LLM推测解码的集成和资源利用挑战，为实时边缘AI应用提供了有效的加速方案。

Abstract: LLM deployment on resource-constrained edge devices faces severe latency constraints, particularly in real-time applications where delayed responses can compromise safety or usability. Among many approaches to mitigate the inefficiencies of sequential token-by-token generation, Speculative Decoding (SD) has emerged as a promising technique. However, SD at the edge is hindered by two major challenges: (1) integrating SD into a compiler-based workflow without sacrificing performance or programmability, and (2) exploiting the heterogeneous compute resources of modern SoCs through carefully designed partitioning strategies. This work addresses these challenges by using an analytical cost model that explores heterogeneous hardware configurations and guides coarse-grained partitioning of LLM subgraphs, particularly with edge-typical short input sequence lengths. The cost model predicts when speculative sampling and heterogeneous execution are jointly beneficial and is validated on an edge device featuring a hexacore Cortex-A CPU and a Mali GPU, revealing up to 1.68$\times$ speedup for translation tasks, closely matching analytic expectations.

</details>


### [214] [Efficient and Adaptable Detection of Malicious LLM Prompts via Bootstrap Aggregation](https://arxiv.org/abs/2602.08062)
*Shayan Ali Hassan,Tao Ni,Zafar Ayyub Qazi,Marco Canini*

Main category: cs.LG

TL;DR: BAGEL框架通过集成小型微调模型实现恶意提示检测，在保持高性能的同时提供模块化、轻量级和可增量更新的特性


<details>
  <summary>Details</summary>
Motivation: 现有恶意提示检测方法面临根本性限制：黑箱API缺乏透明性且难以适应新威胁，白箱方法计算成本高昂且需要昂贵的重新训练。当前系统迫使设计者在性能、效率和适应性之间做出妥协。

Method: BAGEL采用引导聚合和专家混合启发的集成方法，包含多个在不同攻击数据集上微调的小型模型。推理时使用随机森林路由器选择最合适的集成成员，并通过随机选择采样额外成员进行预测聚合。当新攻击出现时，通过微调小型提示安全分类器（8600万参数）并将其添加到集成中来增量更新。

Result: BAGEL仅选择5个集成成员（4.3亿参数）即可达到0.92的F1分数，优于需要数十亿参数的OpenAI Moderation API和ShieldGemma。经过9次增量更新后性能保持稳健，并通过路由器的结构特征提供可解释性。

Conclusion: 小型微调分类器的集成能够匹配或超越数十亿参数的防护系统，同时提供生产系统所需的适应性和效率，展示了轻量级集成方法在恶意提示检测中的有效性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, reasoning, and generation. However, these systems remain susceptible to malicious prompts that induce unsafe or policy-violating behavior through harmful requests, jailbreak techniques, and prompt injection attacks. Existing defenses face fundamental limitations: black-box moderation APIs offer limited transparency and adapt poorly to evolving threats, while white-box approaches using large LLM judges impose prohibitive computational costs and require expensive retraining for new attacks. Current systems force designers to choose between performance, efficiency, and adaptability.
  To address these challenges, we present BAGEL (Bootstrap AGgregated Ensemble Layer), a modular, lightweight, and incrementally updatable framework for malicious prompt detection. BAGEL employs a bootstrap aggregation and mixture of expert inspired ensemble of fine-tuned models, each specialized on a different attack dataset. At inference, BAGEL uses a random forest router to identify the most suitable ensemble member, then applies stochastic selection to sample additional members for prediction aggregation. When new attacks emerge, BAGEL updates incrementally by fine-tuning a small prompt-safety classifier (86M parameters) and adding the resulting model to the ensemble. BAGEL achieves an F1 score of 0.92 by selecting just 5 ensemble members (430M parameters), outperforming OpenAI Moderation API and ShieldGemma which require billions of parameters. Performance remains robust after nine incremental updates, and BAGEL provides interpretability through its router's structural features. Our results show ensembles of small finetuned classifiers can match or exceed billion-parameter guardrails while offering the adaptability and efficiency required for production systems.

</details>


### [215] [Efficient Distribution Learning with Error Bounds in Wasserstein Distance](https://arxiv.org/abs/2602.08063)
*Eduardo Figueiredo,Steven Adams,Luca Laurenti*

Main category: cs.LG

TL;DR: 提出一个基于最优传输和整数规划的框架，从有限样本中学习未知分布，并提供Wasserstein距离的非渐近误差界


<details>
  <summary>Details</summary>
Motivation: Wasserstein距离已成为量化概率分布距离的关键指标，在机器学习、控制理论、决策理论和生物系统等领域有广泛应用。从有限样本中学习未知分布并提供易于计算的非渐近误差界已成为许多领域的基本问题。

Method: 提出一个结合最优传输、非线性优化和集中不等式的理论框架。通过求解一个混合整数线性规划问题，即使真实分布未知，也能高效地以高置信度界定Wasserstein距离误差。开发智能聚类算法，在最小化Wasserstein误差的同时优化近似分布的支撑集。

Result: 在基准测试中，该方法优于现有可比方法，通常返回支撑集更小、误差界更紧的近似分布。

Conclusion: 该框架为从有限样本中近似未知分布提供了一个有效的算法和理论方法，能够提供非渐近且易于计算的Wasserstein距离误差界，在多个应用领域具有实用价值。

Abstract: The Wasserstein distance has emerged as a key metric to quantify distances between probability distributions, with applications in various fields, including machine learning, control theory, decision theory, and biological systems. Consequently, learning an unknown distribution with non-asymptotic and easy-to-compute error bounds in Wasserstein distance has become a fundamental problem in many fields. In this paper, we devise a novel algorithmic and theoretical framework to approximate an unknown probability distribution $\mathbb{P}$ from a finite set of samples by an approximate discrete distribution $\widehat{\mathbb{P}}$ while bounding the Wasserstein distance between $\mathbb{P}$ and $\widehat{\mathbb{P}}$. Our framework leverages optimal transport, nonlinear optimization, and concentration inequalities. In particular, we show that, even if $\mathbb{P}$ is unknown, the Wasserstein distance between $\mathbb{P}$ and $\widehat{\mathbb{P}}$ can be efficiently bounded with high confidence by solving a tractable optimization problem (a mixed integer linear program) of a size that only depends on the size of the support of $\widehat{\mathbb{P}}$. This enables us to develop intelligent clustering algorithms to optimally find the support of $\widehat{\mathbb{P}}$ while minimizing the Wasserstein distance error. On a set of benchmarks, we demonstrate that our approach outperforms state-of-the-art comparable methods by generally returning approximating distributions with substantially smaller support and tighter error bounds.

</details>


### [216] [SiameseNorm: Breaking the Barrier to Reconciling Pre/Post-Norm](https://arxiv.org/abs/2602.08064)
*Tianyu Li,Dongchen Han,Zixuan Cao,Haofeng Huang,Mengyu Zhou,Ming Chen,Erchao Zhao,Xiaoxi Jiang,Guanjun Jiang,Gao Huang*

Main category: cs.LG

TL;DR: SiameseNorm提出了一种双流架构，将Pre-Norm和Post-Norm范式解耦，既保持了优化稳定性又提升了表达能力，在13亿参数模型上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现代Transformer主要采用Pre-Norm范式以保证优化稳定性，但牺牲了Post-Norm架构的优越潜力。先前尝试结合两者优势通常导致稳定性与性能的权衡，这源于单流设计中Post-Norm操作会阻碍Pre-Norm保持的干净恒等梯度。

Method: 提出SiameseNorm双流架构，耦合具有共享参数的Pre-Norm-like和Post-Norm-like流。这种设计解耦了两个流的优化动态，使所有残差块都能接收来自两种范式的组合梯度，其中一个流确保稳定性，另一个增强表达能力。

Result: 在13亿参数模型上的大规模预训练实验表明，SiameseNorm展现出卓越的优化鲁棒性，并持续超越强基线模型。

Conclusion: SiameseNorm通过双流架构从根本上调和了Pre-Norm和Post-Norm范式，实现了优化稳定性与表达能力的双重优势，为Transformer架构设计提供了新思路。

Abstract: Modern Transformers predominantly adopt the Pre-Norm paradigm for its optimization stability, foregoing the superior potential of the unstable Post-Norm architecture. Prior attempts to combine their strengths typically lead to a stability-performance trade-off. We attribute this phenomenon to a structural incompatibility within a single-stream design: Any application of the Post-Norm operation inevitably obstructs the clean identity gradient preserved by Pre-Norm. To fundamentally reconcile these paradigms, we propose SiameseNorm, a two-stream architecture that couples Pre-Norm-like and Post-Norm-like streams with shared parameters. This design decouples the optimization dynamics of the two streams, retaining the distinct characteristics of both Pre-Norm and Post-Norm by enabling all residual blocks to receive combined gradients inherited from both paradigms, where one stream secures stability while the other enhances expressivity. Extensive pre-training experiments on 1.3B-parameter models demonstrate that SiameseNorm exhibits exceptional optimization robustness and consistently outperforms strong baselines. Code is available at https://github.com/Qwen-Applications/SiameseNorm.

</details>


### [217] [Enhancing Bandit Algorithms with LLMs for Time-varying User Preferences in Streaming Recommendations](https://arxiv.org/abs/2602.08067)
*Chenglei Shen,Yi Zhan,Weijie Yu,Xiao Zhang,Jun Xu*

Main category: cs.LG

TL;DR: HyperBandit+：一种新颖的上下文老虎机策略，通过时间感知超网络适应时变用户偏好，并利用LLM辅助的预热启动机制提升早期在线阶段的探索-利用效率。


<details>
  <summary>Details</summary>
Motivation: 现实流式推荐系统中用户偏好随时间动态演变，现有基于老虎机的方法仅将时间视为时间戳，忽略了其与用户偏好的显式关系，导致性能不佳。此外，在线学习方法在早期在线阶段往往存在探索-利用效率低下的问题。

Method: 1) 时间感知超网络：利用神经网络以时间特征为输入，生成估计时变奖励的参数，捕捉时间与用户偏好的相关性；2) LLM辅助预热启动机制：采用多步数据增强模拟真实交互数据进行有效离线学习，为早期在线阶段提供预热参数；3) 低秩分解：降低超网络训练复杂度以满足实时流式推荐需求。

Result: 在真实世界数据集上的广泛实验表明，HyperBandit+在累积奖励方面持续优于最先进的基线方法。理论分析建立了考虑超网络和LLM预热启动机制的次线性遗憾上界。

Conclusion: HyperBandit+通过整合时间感知超网络和LLM辅助预热启动机制，有效解决了流式推荐中时变用户偏好建模和早期在线学习效率问题，在理论和实验上都表现出优越性能。

Abstract: In real-world streaming recommender systems, user preferences evolve dynamically over time. Existing bandit-based methods treat time merely as a timestamp, neglecting its explicit relationship with user preferences and leading to suboptimal performance. Moreover, online learning methods often suffer from inefficient exploration-exploitation during the early online phase. To address these issues, we propose HyperBandit+, a novel contextual bandit policy that integrates a time-aware hypernetwork to adapt to time-varying user preferences and employs a large language model-assisted warm-start mechanism (LLM Start) to enhance exploration-exploitation efficiency in the early online phase. Specifically, HyperBandit+ leverages a neural network that takes time features as input and generates parameters for estimating time-varying rewards by capturing the correlation between time and user preferences. Additionally, the LLM Start mechanism employs multi-step data augmentation to simulate realistic interaction data for effective offline learning, providing warm-start parameters for the bandit policy in the early online phase. To meet real-time streaming recommendation demands, we adopt low-rank factorization to reduce hypernetwork training complexity. Theoretically, we rigorously establish a sublinear regret upper bound that accounts for both the hypernetwork and the LLM warm-start mechanism. Extensive experiments on real-world datasets demonstrate that HyperBandit+ consistently outperforms state-of-the-art baselines in terms of accumulated rewards.

</details>


### [218] [Multimodal normative modeling in Alzheimers Disease with introspective variational autoencoders](https://arxiv.org/abs/2602.08077)
*Sayantan Kumar,Peijie Qiu,Aristeidis Sotiras*

Main category: cs.LG

TL;DR: 提出mmSIVAE模型，结合软自省VAE与混合专家乘积聚合，改进多模态神经影像的规范性建模，提升健康参考分布拟合与多模态融合，用于阿尔茨海默病异常检测。


<details>
  <summary>Details</summary>
Motivation: 现有VAE规范性模型在多模态神经影像分析中存在两个问题：1) 对健康参考分布拟合不完善，导致假阳性增加；2) 后验聚合方法（如PoE/MoE）在共享潜在空间中多模态融合效果弱。需要改进规范性建模以更好地捕捉阿尔茨海默病的异质性效应。

Method: 提出mmSIVAE（多模态软自省变分自编码器），结合Mixture-of-Product-of-Experts（MOPOE）聚合机制。在潜在空间和特征空间计算与学习到的健康分布的距离作为偏差分数，并将统计显著的潜在偏差映射到区域异常以实现可解释性。

Result: 在ADNI的MRI区域体积和淀粉样蛋白PET SUVR数据上，mmSIVAE在保留对照组上改进重建效果，相比VAE基线产生更具区分性的偏差分数用于异常检测，具有更高的似然比和更清晰的对照组与AD谱系队列分离。偏差图突出显示与已知AD相关变化一致的区域级模式。

Conclusion: 研究结果强调了训练目标中优先考虑参考分布保真度和鲁棒多模态后验聚合对于规范性建模的重要性，对跨多模态临床数据的偏差分析具有广泛意义。

Abstract: Normative modeling learns a healthy reference distribution and quantifies subject-specific deviations to capture heterogeneous disease effects. In Alzheimers disease (AD), multimodal neuroimaging offers complementary signals but VAE-based normative models often (i) fit the healthy reference distribution imperfectly, inflating false positives, and (ii) use posterior aggregation (e.g., PoE/MoE) that can yield weak multimodal fusion in the shared latent space. We propose mmSIVAE, a multimodal soft-introspective variational autoencoder combined with Mixture-of-Product-of-Experts (MOPOE) aggregation to improve reference fidelity and multimodal integration. We compute deviation scores in latent space and feature space as distances from the learned healthy distributions, and map statistically significant latent deviations to regional abnormalities for interpretability. On ADNI MRI regional volumes and amyloid PET SUVR, mmSIVAE improves reconstruction on held-out controls and produces more discriminative deviation scores for outlier detection than VAE baselines, with higher likelihood ratios and clearer separation between control and AD-spectrum cohorts. Deviation maps highlight region-level patterns aligned with established AD-related changes. More broadly, our results highlight the importance of training objectives that prioritize reference-distribution fidelity and robust multimodal posterior aggregation for normative modeling, with implications for deviation-based analysis across multimodal clinical data.

</details>


### [219] [Spectral Guardrails for Agents in the Wild: Detecting Tool Use Hallucinations via Attention Topology](https://arxiv.org/abs/2602.08082)
*Valentin Noël*

Main category: cs.LG

TL;DR: 提出基于注意力拓扑谱分析的免训练护栏方法，用于检测自主代理工具使用失败，在Llama 3.1 8B上实现97.7%召回率，发现单层谱特征可作为近乎完美的幻觉检测器。


<details>
  <summary>Details</summary>
Motivation: 在野外部署自主代理需要可靠的安全保障来防止工具使用失败。现有监督方法需要标注数据，而本文提出免训练方法来补充监督方法。

Method: 基于注意力拓扑的谱分析方法，无需标注训练数据。使用单层谱特征（如Llama L26平滑度和Mistral L3熵）作为幻觉检测器，通过阈值进行检测。

Result: 在Llama 3.1 8B上，多特征检测达到97.7%召回率，平衡部署达到86.1%召回率和81.0%精确率。单层谱特征检测效果惊人：Llama L26平滑度达到98.2%召回率（捕获213/217个幻觉），Mistral L3熵达到94.7%召回率。跨模型评估发现"大声说谎者"现象：Llama 3.1 8B的失败在谱分析上更易检测，而Mistral 7B达到最佳区分度（AUC 0.900）。

Conclusion: 谱分析为代理安全提供了一个原则性、高效的框架。研究发现幻觉不仅是错误标记，而是热力学状态变化：模型注意力在出错时变为噪声。该方法可作为免训练的安全护栏。

Abstract: Deploying autonomous agents in the wild requires reliable safeguards against tool use failures. We propose a training free guardrail based on spectral analysis of attention topology that complements supervised approaches. On Llama 3.1 8B, our method achieves 97.7\% recall with multi-feature detection and 86.1\% recall with 81.0\% precision for balanced deployment, without requiring any labeled training data. Most remarkably, we discover that single layer spectral features act as near-perfect hallucination detectors: Llama L26 Smoothness achieves 98.2\% recall (213/217 hallucinations caught) with a single threshold, and Mistral L3 Entropy achieves 94.7\% recall. This suggests hallucination is not merely a wrong token but a thermodynamic state change: the model's attention becomes noise when it errs. Through controlled cross-model evaluation on matched domains ($N=1000$, $T=0.3$, same General domain, hallucination rates 20--22\%), we reveal the ``Loud Liar'' phenomenon: Llama 3.1 8B's failures are spectrally catastrophic and dramatically easier to detect, while Mistral 7B achieves the best discrimination (AUC 0.900). These findings establish spectral analysis as a principled, efficient framework for agent safety.

</details>


### [220] [Probability Hacking and the Design of Trustworthy ML for Signal Processing in C-UAS: A Scenario Based Method](https://arxiv.org/abs/2602.08086)
*Liisa Janssens,Laura Middeldorp*

Main category: cs.LG

TL;DR: 该论文提出使用基于场景的方法来识别和防止C-UAS系统中通过机器学习增强信号处理能力时可能出现的概率黑客攻击，以增强系统可信度


<details>
  <summary>Details</summary>
Motivation: 为了有效应对无人机系统带来的各种威胁，需要专门的C-UAS系统。通过人工智能等新兴颠覆性技术增强C-UAS可以带来更有效的对抗措施，但需要确保系统的可信度

Method: 采用基于场景的方法，分析C-UAS系统中机器学习增强信号处理能力时可能出现的概率黑客攻击，并识别可集成到现有法治机制中的需求

Result: 通过该方法识别了防止概率黑客攻击的需求，这些需求可以增强C-UAS系统的可信度，从而建立合理的信任，这对于军民领域的人机协作成功至关重要

Conclusion: 基于场景的方法能够有效识别C-UAS系统中机器学习增强信号处理时的概率黑客攻击风险，提出的需求可以集成到法治机制中，增强系统可信度，促进成功的人机协作

Abstract: In order to counter the various threats manifested by Unmanned Aircraft Systems (UAS) adequately, specialized Counter Unmanned Aircraft Systems (C-UAS) are required. Enhancing C-UAS with Emerging and Disruptive Technologies (EDTs) such as Artificial Intelligence (AI) can lead to more effective countermeasures. In this paper a scenario-based method is applied to C-UAS augmented with Machine Learning (ML), a subset of AI, that can enhance signal processing capabilities. Via the scenarios-based method we frame in this paper probability hacking as a challenge and identify requirements which can be implemented in existing Rule of Law mechanisms to prevent probability hacking. These requirements strengthen the trustworthiness of the C-UAS, which feed into justified trust - a key to successful Human-Autonomy Teaming, in civil and military contexts. Index Terms: C-UAS, Scenario-based method, Emerging and Disruptive Technologies, Probability hacking, Trustworthiness.

</details>


### [221] [Online Domain-aware LLM Decoding for Continual Domain Evolution](https://arxiv.org/abs/2602.08088)
*Mohammad Abu-Shaira,Weishi Shi*

Main category: cs.LG

TL;DR: ODD框架通过在线概率融合和自适应置信度调制，使LLM能够实时适应不断演变的领域知识，无需昂贵重训练


<details>
  <summary>Details</summary>
Motivation: 现实世界中领域知识持续演变（新法规、产品、服务等），而传统LLM微调假设静态领域，重训练成本高昂。同时存在概念漂移问题，忽视这些动态变化会显著降低模型预测准确性。

Method: 提出在线领域感知解码框架（ODD），在基础LLM和前缀树先验之间进行概率级融合，使用分歧和连续性信号指导自适应置信度调制。

Result: 在多种漂移场景下的实证评估显示，ODD在所有句法和语义NLG指标上均优于LLM-Greedy和LLM-Temp Scaled基线，获得0.065的绝对ROUGE-L增益和13.6%的相对余弦相似度提升。

Conclusion: ODD对不断演变的词汇和上下文模式具有鲁棒性，适合动态LLM应用，解决了领域演变与静态适应管道之间的不匹配问题。

Abstract: LLMs are typically fine-tuned offline on domain-specific data, assuming a static domain. In practice, domain knowledge evolves continuously through new regulations, products, services, and interaction patterns. Retraining or fine-tuning LLMs for every new instance is computationally infeasible. Additionally, real-world environments also exhibit temporal dynamics with shifting data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. This mismatch between evolving domains and static adaptation pipelines highlights the need for efficient, real-time adaptation without costly retraining. In response, we introduce Online Domain-aware Decoding framework (ODD). ODD performs probability-level fusion between a base LLM and a prefix-tree prior, guided by adaptive confidence modulation using disagreement and continuity signals. Empirical evaluation under diverse drift scenarios demonstrates that ODD consistently surpasses LLM-Greedy and LLM-Temp Scaled across all syntactic and semantic NLG metrics. It yields an absolute ROUGE-L gain of 0.065 and a 13.6% relative improvement in Cosine Similarity over the best baseline. These results demonstrate ODD 's robustness to evolving lexical and contextual patterns, making it suitable for dynamic LLM applications.

</details>


### [222] [Mutual information and task-relevant latent dimensionality](https://arxiv.org/abs/2602.08105)
*Paarth Gulati,Eslam Abdelaleem,Audrey Sederberg,Ilya Nemenman*

Main category: cs.LG

TL;DR: 提出一种基于信息瓶颈的维度估计方法，通过混合批评器解决传统神经互信息估计器高估维度的问题，并实现单次估计无需扫描瓶颈尺寸。


<details>
  <summary>Details</summary>
Motivation: 估计预测所需的潜在表示维度（任务相关维度）是一个困难且未解决的问题，具有广泛科学应用价值。传统方法存在局限性，需要更可靠的维度估计方法。

Method: 将维度估计转化为信息瓶颈问题：寻找能够压缩预测器和被预测视图同时保持其互信息的最小嵌入瓶颈维度。提出混合批评器，在保留显式维度瓶颈的同时允许灵活的非线性跨视图交互。开发单次估计协议，从单个过参数化混合模型中直接读取有效维度。

Result: 在已知任务相关维度的合成问题上验证了方法的有效性。扩展到内在维度估计，在噪声环境下比传统几何维度估计器更可靠。在多个物理数据集上展示了方法的实用性。

Conclusion: 提出了一种基于信息瓶颈的维度估计新方法，解决了传统神经互信息估计器高估维度的问题，在噪声环境下保持可靠性，为科学应用提供了实用的维度估计工具。

Abstract: Estimating the dimensionality of the latent representation needed for prediction -- the task-relevant dimension -- is a difficult, largely unsolved problem with broad scientific applications. We cast it as an Information Bottleneck question: what embedding bottleneck dimension is sufficient to compress predictor and predicted views while preserving their mutual information (MI). This repurposes neural MI estimators for dimensionality estimation. We show that standard neural estimators with separable/bilinear critics systematically inflate the inferred dimension, and we address this by introducing a hybrid critic that retains an explicit dimensional bottleneck while allowing flexible nonlinear cross-view interactions, thereby preserving the latent geometry. We further propose a one-shot protocol that reads off the effective dimension from a single over-parameterized hybrid model, without sweeping over bottleneck sizes. We validate the approach on synthetic problems with known task-relevant dimension. We extend the approach to intrinsic dimensionality by constructing paired views of a single dataset, enabling comparison with classical geometric dimension estimators. In noisy regimes where those estimators degrade, our approach remains reliable. Finally, we demonstrate the utility of the method on multiple physics datasets.

</details>


### [223] [Online Bayesian Imbalanced Learning with Bregman-Calibrated Deep Networks](https://arxiv.org/abs/2602.08128)
*Zahir Alsulaimawi*

Main category: cs.LG

TL;DR: OBIL框架通过解耦似然比估计与类别先验假设，实现无需重新训练即可实时适应分布偏移的在线不平衡学习。


<details>
  <summary>Details</summary>
Motivation: 现实应用中类别分布经常在部署时发生变化（如欺诈检测、医疗诊断），现有方法需要重新训练或访问标记数据，无法实时适应分布偏移。

Method: 基于Bregman散度与适当评分规则的关联，证明深度网络训练产生的后验概率估计可提取先验不变的似然比，通过阈值调整实现最优贝叶斯决策。

Result: 理论证明似然比估计在任意类别先验和成本结构变化下保持有效，获得O(√T log T)的有限样本遗憾界，实验显示在严重分布偏移下优于现有方法。

Conclusion: OBIL提供了一种无需重新训练即可实时适应分布偏移的在线不平衡学习框架，在类别分布变化的实际应用中具有重要价值。

Abstract: Class imbalance remains a fundamental challenge in machine learning, where standard classifiers exhibit severe performance degradation in minority classes. Although existing approaches address imbalance through resampling or cost-sensitive learning during training, they require retraining or access to labeled target data when class distributions shift at deployment time, a common occurrence in real-world applications such as fraud detection, medical diagnosis, and anomaly detection. We present \textit{Online Bayesian Imbalanced Learning} (OBIL), a principled framework that decouples likelihood-ratio estimation from class-prior assumptions, enabling real-time adaptation to distribution shifts without model retraining. Our approach builds on the established connection between Bregman divergences and proper scoring rules to show that deep networks trained with such losses produce posterior probability estimates from which prior-invariant likelihood ratios can be extracted. We prove that these likelihood-ratio estimates remain valid under arbitrary changes in class priors and cost structures, requiring only a threshold adjustment for optimal Bayes decisions. We derive finite-sample regret bounds demonstrating that OBIL achieves $O(\sqrt{T \log T})$ regret against an oracle with perfect prior knowledge. Extensive experiments on benchmark datasets and medical diagnosis benchmarks under simulated deployment shifts demonstrate that OBIL maintains robust performance under severe distribution shifts, outperforming state-of-the-art methods in F1 Score when test distributions deviate significantly from the training conditions.

</details>


### [224] [Variance-Gated Ensembles: An Epistemic-Aware Framework for Uncertainty Estimation](https://arxiv.org/abs/2602.08142)
*H. Martin Gillis,Isaac Xu,Thomas Trappenberg*

Main category: cs.LG

TL;DR: VGE提出了一种新的不确定性估计框架，通过方差门控机制将决策边界与集成预测方差耦合，避免了传统加性分解的问题。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯或近似方法将不确定性加性分解为偶然不确定性和认知不确定性，但这种方法在使用有限集成采样和/或不匹配预测分布时会失效，需要更可靠的不确定性估计方法。

Method: 提出方差门控集成(VGE)框架：1) 方差门控边缘不确定性(VGMU)评分，将决策边界与集成预测方差耦合；2) 方差门控归一化(VGN)层，通过每类可学习的归一化将方差门控机制扩展到训练中；推导闭式向量-雅可比乘积实现端到端训练。

Result: VGE匹配或超越了最先进的信息论基线方法，同时保持计算效率，为集成模型提供了实用且可扩展的认知感知不确定性估计方法。

Conclusion: VGE提供了一种直观、可微的框架，通过基于集成统计的信噪比门控注入认知敏感性，解决了传统加性分解的局限性，实现了更可靠的不确定性估计。

Abstract: Machine learning applications require fast and reliable per-sample uncertainty estimation. A common approach is to use predictive distributions from Bayesian or approximation methods and additively decompose uncertainty into aleatoric (i.e., data-related) and epistemic (i.e., model-related) components. However, additive decomposition has recently been questioned, with evidence that it breaks down when using finite-ensemble sampling and/or mismatched predictive distributions. This paper introduces Variance-Gated Ensembles (VGE), an intuitive, differentiable framework that injects epistemic sensitivity via a signal-to-noise gate computed from ensemble statistics. VGE provides: (i) a Variance-Gated Margin Uncertainty (VGMU) score that couples decision margins with ensemble predictive variance; and (ii) a Variance-Gated Normalization (VGN) layer that generalizes the variance-gated uncertainty mechanism to training via per-class, learnable normalization of ensemble member probabilities. We derive closed-form vector-Jacobian products enabling end-to-end training through ensemble sample mean and variance. VGE matches or exceeds state-of-the-art information-theoretic baselines while remaining computationally efficient. As a result, VGE provides a practical and scalable approach to epistemic-aware uncertainty estimation in ensemble models. An open-source implementation is available at: https://github.com/nextdevai/vge.

</details>


### [225] [Reliable and Responsible Foundation Models: A Comprehensive Survey](https://arxiv.org/abs/2602.08145)
*Xinyu Yang,Junlin Han,Rishi Bommasani,Jinqi Luo,Wenjie Qu,Wangchunshu Zhou,Adel Bibi,Xiyao Wang,Jaehong Yoon,Elias Stengel-Eskin,Shengbang Tong,Lingfeng Shen,Rafael Rafailov,Runjia Li,Zhaoyang Wang,Yiyang Zhou,Chenhang Cui,Yu Wang,Wenhao Zheng,Huichi Zhou,Jindong Gu,Zhaorun Chen,Peng Xia,Tony Lee,Thomas Zollo,Vikash Sehwag,Jixuan Leng,Jiuhai Chen,Yuxin Wen,Huan Zhang,Zhun Deng,Linjun Zhang,Pavel Izmailov,Pang Wei Koh,Yulia Tsvetkov,Andrew Wilson,Jiaheng Zhang,James Zou,Cihang Xie,Hao Wang,Philip Torr,Julian McAuley,David Alvarez-Melis,Florian Tramèr,Kaidi Xu,Suman Jana,Chris Callison-Burch,Rene Vidal,Filippos Kokkinos,Mohit Bansal,Beidi Chen,Huaxiu Yao*

Main category: cs.LG

TL;DR: 该调查论文系统性地探讨了基础模型的可靠性与责任性发展，涵盖偏见公平、安全隐私、不确定性、可解释性、分布偏移等关键问题，以及幻觉、对齐、AIGC检测等方法和挑战。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型（LLMs、MLLMs、图像生成模型、视频生成模型）在现实世界中的广泛应用，确保其可靠性和责任性已成为学术界、工业界和政府的关键需求。这些模型在医疗、法律、教育、金融、科学等关键领域的部署，要求它们不仅是强大的，还要是伦理的、可信赖的和社会责任的。

Method: 采用系统性调查方法，对基础模型的可靠性和责任性发展进行全面梳理。针对每个关键领域（偏见与公平、安全与隐私、不确定性、可解释性、分布偏移），回顾当前研究现状，并规划具体的未来研究方向。同时探讨这些领域之间的交叉关系和共享挑战。

Result: 该调查提供了基础模型可靠性与责任性研究的全面概览，识别了当前研究的关键问题和挑战，包括模型局限性（如幻觉）、对齐方法、AIGC检测等。为每个研究领域提出了具体的未来研究方向，并强调了跨领域问题的相互关联性。

Conclusion: 该调查旨在促进基础模型的负责任发展，使其不仅是强大的技术工具，更是伦理的、可信赖的、可靠的和具有社会责任的人工智能系统。通过系统性地梳理现有研究和未来方向，为学术界和工业界提供了重要的参考框架，以推动基础模型向更安全、更公平、更透明的方向发展。

Abstract: Foundation models, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models, have become essential tools with broad applications across various domains such as law, medicine, education, finance, science, and beyond. As these models see increasing real-world deployment, ensuring their reliability and responsibility has become critical for academia, industry, and government. This survey addresses the reliable and responsible development of foundation models. We explore critical issues, including bias and fairness, security and privacy, uncertainty, explainability, and distribution shift. Our research also covers model limitations, such as hallucinations, as well as methods like alignment and Artificial Intelligence-Generated Content (AIGC) detection. For each area, we review the current state of the field and outline concrete future research directions. Additionally, we discuss the intersections between these areas, highlighting their connections and shared challenges. We hope our survey fosters the development of foundation models that are not only powerful but also ethical, trustworthy, reliable, and socially responsible.

</details>


### [226] [A second order regret bound for NormalHedge](https://arxiv.org/abs/2602.08151)
*Yoav Freund,Nicholas J. A. Harvey,Victor S. Portella,Yabing Qi,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: 提出一种NormalHedge变体，对"简单"序列实现二阶ε-分位数遗憾界O(√(V_T log(V_T/ε)))，其中V_T是算法确定的自然分布下的瞬时专家遗憾累积二阶矩


<details>
  <summary>Details</summary>
Motivation: 研究"简单"序列的专家建议预测问题，旨在为这类序列设计具有更好遗憾界的算法

Method: 提出NormalHedge算法的变体，通过随机微分方程连续时间极限进行动机推导，离散时间分析使用自协调技术

Result: 当V_T > log N时，算法获得二阶ε-分位数遗憾界O(√(V_T log(V_T/ε)))，其中V_T是算法确定的自然分布下的瞬时专家遗憾累积二阶矩

Conclusion: 该算法为"简单"序列提供了改进的遗憾保证，结合了连续时间极限和自协调分析技术

Abstract: We consider the problem of prediction with expert advice for ``easy'' sequences. We show that a variant of NormalHedge enjoys a second-order $ε$-quantile regret bound of $O\big(\sqrt{V_T \log(V_T/ε)}\big) $ when $V_T > \log N$, where $V_T$ is the cumulative second moment of instantaneous per-expert regret averaged with respect to a natural distribution determined by the algorithm. The algorithm is motivated by a continuous time limit using Stochastic Differential Equations. The discrete time analysis uses self-concordance techniques.

</details>


### [227] [The Confidence Manifold: Geometric Structure of Correctness Representations in Language Models](https://arxiv.org/abs/2602.08159)
*Seonglae Cho,Zekun Wu,Kleyton Da Costa,Adriano Koshiyama*

Main category: cs.LG

TL;DR: 研究发现语言模型内部存在简单的几何结构来表示正确性，仅需3-8个维度就能有效区分正确与错误陈述，且线性分类器足够，无需非线性方法。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型是否知道自己陈述的错误性，以及正确性信息在模型内部如何表示和检测。

Method: 分析9个不同架构模型的几何结构，使用线性探针、激活导向和质心距离等方法，比较内部探测与基于输出的方法。

Result: 正确性信号仅需3-8个维度，线性分类器足够；内部探针AUC达0.80-0.97，而基于输出的方法仅0.44-0.64；激活导向能显著改变错误率。

Conclusion: 语言模型内部存在明确的正确定性表示，但该信息未在输出中表达；正确性检测本质上是几何问题而非学习问题。

Abstract: When a language model asserts that "the capital of Australia is Sydney," does it know this is wrong? We characterize the geometry of correctness representations across 9 models from 5 architecture families. The structure is simple: the discriminative signal occupies 3-8 dimensions, performance degrades with additional dimensions, and no nonlinear classifier improves over linear separation. Centroid distance in the low-dimensional subspace matches trained probe performance (0.90 AUC), enabling few-shot detection: on GPT-2, 25 labeled examples achieve 89% of full-data accuracy. We validate causally through activation steering: the learned direction produces 10.9 percentage point changes in error rates while random directions show no effect. Internal probes achieve 0.80-0.97 AUC; output-based methods (P(True), semantic entropy) achieve only 0.44-0.64 AUC. The correctness signal exists internally but is not expressed in outputs. That centroid distance matches probe performance indicates class separation is a mean shift, making detection geometric rather than learned.

</details>


### [228] [Spherical Steering: Geometry-Aware Activation Rotation for Language Models](https://arxiv.org/abs/2602.08169)
*Zejia You,Chunyuan Deng,Hanjie Chen*

Main category: cs.LG

TL;DR: 提出Spherical Steering方法，通过激活旋转而非加法来实现推理时控制，在保持生成质量的同时提升多项任务性能


<details>
  <summary>Details</summary>
Motivation: 现有推理时控制方法通常基于激活加法，这会改变隐藏表示的幅度，可能导致表示崩溃和开放式生成能力下降

Method: 提出球面导向方法，通过沿测地线旋转激活向量来引导模型，同时引入置信门控机制根据输入不确定性动态调整导向强度

Result: 在多项选择题基准测试中显著优于基于加法的方法（在TruthfulQA、COPA和Storycloze上提升+10%），同时保持模型的开放式生成质量

Conclusion: 保持几何一致性的规范保持旋转是精确推理时控制的鲁棒有效原语

Abstract: Inference-time steering has emerged as a promising paradigm for controlling language models (LMs) without the cost of retraining. However, standard approaches typically rely on activation addition, a geometric operation that inevitably alters the magnitude of hidden representations. This raises concerns about representation collapse and degradation of open-ended generation capabilities. In this work, we explore Spherical Steering, a training-free primitive that resolves this trade-off through activation rotation. Rather than shifting activations with a fixed vector, our method rotates them along a geodesic toward a target direction, guiding the activation toward the target concept while preserving the integrity of the signal. To further enhance adaptivity, we incorporate a confidence gate that dynamically modulates steering strength based on input uncertainty. Extensive experiments across multiple-choice benchmarks demonstrate that Spherical Steering significantly outperforms addition-based baselines (notably by +10% on TruthfulQA, COPA, and Storycloze), while simultaneously maintaining the model's general open-ended generation quality. This work highlights the value of geometric consistency, suggesting that norm-preserving rotation is a robust and effective primitive for precise inference-time control.

</details>


### [229] [A Causal Machine Learning Framework for Treatment Personalization in Clinical Trials: Application to Ulcerative Colitis](https://arxiv.org/abs/2602.08171)
*Cristian Minoccheri,Sophia Tesic,Kayvan Najarian,Ryan Stidham*

Main category: cs.LG

TL;DR: 该研究提出了一个模块化因果机器学习框架，用于区分统计上可检测的治疗异质性与实际改善治疗决策的能力，并在溃疡性结肠炎临床试验中应用，发现内镜特征虽能预测异质性但不能改善治疗选择。


<details>
  <summary>Details</summary>
Motivation: 随机对照试验通常估计平均治疗效果，但治疗反应存在异质性，这促使了个性化治疗的需求。然而，统计上可检测的异质性是否真正能转化为改善的治疗决策是一个关键问题，这两个问题可能产生矛盾的结果。

Method: 提出了模块化因果机器学习框架：1) 置换重要性识别预测异质性的特征；2) 最佳线性预测器(BLP)测试评估统计显著性；3) 双重稳健策略评估衡量利用异质性是否改善患者结局。应用于UNIFI维持试验数据，比较安慰剂、标准剂量乌司奴单抗和剂量强化乌司奴单抗，使用交叉拟合X-learner模型分析基线特征。

Result: BLP测试发现内镜特征与乌司奴单抗vs安慰剂的治疗效果异质性有强关联，但双重稳健策略评估显示纳入内镜特征未能改善预期缓解率，且多臂评估表现更差。内镜评分作为疾病严重程度标志物，能改善未治疗患者的结局预测，但给治疗选择增加了噪声，而临床变量（粪便钙卫蛋白、年龄、CRP）捕捉了决策相关的变异。

Conclusion: 因果机器学习在临床试验中的应用应同时包含策略层面评估和异质性测试，因为统计上显著的异质性不一定能转化为更好的治疗决策，需要区分预测异质性的特征与改善决策的特征。

Abstract: Randomized controlled trials estimate average treatment effects, but treatment response heterogeneity motivates personalized approaches. A critical question is whether statistically detectable heterogeneity translates into improved treatment decisions -- these are distinct questions that can yield contradictory answers. We present a modular causal machine learning framework that evaluates each question separately: permutation importance identifies which features predict heterogeneity, best linear predictor (BLP) testing assesses statistical significance, and doubly robust policy evaluation measures whether acting on the heterogeneity improves patient outcomes. We apply this framework to patient-level data from the UNIFI maintenance trial of ustekinumab in ulcerative colitis, comparing placebo, standard-dose ustekinumab every 12 weeks, and dose-intensified ustekinumab every 8 weeks, using cross-fitted X-learner models with baseline demographics, medication history, week-8 clinical scores, laboratory biomarkers, and video-derived endoscopic features. BLP testing identified strong associations between endoscopic features and treatment effect heterogeneity for ustekinumab versus placebo, yet doubly robust policy evaluation showed no improvement in expected remission from incorporating endoscopic features, and out-of-fold multi-arm evaluation showed worse performance. Diagnostic comparison of prognostic contribution against policy value revealed that endoscopic scores behaved as disease severity markers -- improving outcome prediction in untreated patients but adding noise to treatment selection -- while clinical variables (fecal calprotectin, age, CRP) captured the decision-relevant variation. These results demonstrate that causal machine learning applications to clinical trials should include policy-level evaluation alongside heterogeneity testing.

</details>


### [230] [Nansde-net: A neural sde framework for generating time series with memory](https://arxiv.org/abs/2602.08182)
*Hiromu Ozai,Kei Nakagawa*

Main category: cs.LG

TL;DR: 提出一种基于Itô过程的神经噪声（NA-noise），能够捕捉时间序列的长短期记忆特性，并构建NANSDE-Net生成模型，在保持Itô计算框架的同时超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统分数布朗运动虽然能捕捉时间序列的长短期记忆特性，但与Itô计算不兼容，限制了其在神经随机微分方程框架中的应用。需要一种既能保持Itô计算兼容性又能捕捉记忆特性的替代噪声源。

Method: 提出神经网络核ARMA型噪声（NA-noise），通过神经网络参数化核函数，并分解为乘积形式以保持马尔可夫性质。基于此噪声构建NANSDE-Net生成模型，扩展神经SDEs，并证明解的存在唯一性，推导高效的反向传播训练方案。

Result: 在合成和真实数据集上的实验表明，NANSDE-Net在再现数据的长短期记忆特征方面匹配或优于现有模型（包括分数SDE-Net），同时在Itô计算框架内保持计算可行性。

Conclusion: NA-noise为捕捉时间序列记忆特性提供了与Itô计算兼容的有效替代方案，NANSDE-Net在理论和实证上都表现出色，为神经SDE框架中的记忆建模开辟了新途径。

Abstract: Modeling time series with long- or short-memory characteristics is a fundamental challenge in many scientific and engineering domains. While fractional Brownian motion has been widely used as a noise source to capture such memory effects, its incompatibility with Itô calculus limits its applicability in neural stochastic differential equation~(SDE) frameworks. In this paper, we propose a novel class of noise, termed Neural Network-kernel ARMA-type noise~(NA-noise), which is an Itô-process-based alternative capable of capturing both long- and short-memory behaviors. The kernel function defining the noise structure is parameterized via neural networks and decomposed into a product form to preserve the Markov property. Based on this noise process, we develop NANSDE-Net, a generative model that extends Neural SDEs by incorporating NA-noise. We prove the theoretical existence and uniqueness of the solution under mild conditions and derive an efficient backpropagation scheme for training. Empirical results on both synthetic and real-world datasets demonstrate that NANSDE-Net matches or outperforms existing models, including fractional SDE-Net, in reproducing long- and short-memory features of the data, while maintaining computational tractability within the Itô calculus framework.

</details>


### [231] [Dreaming in Code for Curriculum Learning in Open-Ended Worlds](https://arxiv.org/abs/2602.08194)
*Konstantinos Mitsides,Maxence Faldor,Antoine Cully*

Main category: cs.LG

TL;DR: DiCode框架利用基础模型生成可执行环境代码，通过代码级世界变化构建学习脚手架，在开放世界学习中实现持续能力提升


<details>
  <summary>Details</summary>
Motivation: 开放世界学习面临挑战空间组合爆炸问题，现有方法难以发现持续可学习的经验序列，需要构建中间环境来弥合能力差距

Method: 提出Dreaming in Code框架，让基础模型合成可执行环境代码，通过代码级世界变化为学习提供脚手架，在Craftax基准上实例化

Result: 在Craftax基准上实现16%的平均回报提升，在后期战斗任务上取得非零成功率（先前方法完全失败），证明代码级环境设计能有效控制课程

Conclusion: 代码级环境设计为开放世界学习提供了实用的课程控制机制，能够构建中间环境来弥合能力差距，支持长期技能获取

Abstract: Open-ended learning frames intelligence as emerging from continual interaction with an ever-expanding space of environments. While recent advances have utilized foundation models to programmatically generate diverse environments, these approaches often focus on discovering isolated behaviors rather than orchestrating sustained progression. In complex open-ended worlds, the large combinatorial space of possible challenges makes it difficult for agents to discover sequences of experiences that remain consistently learnable. To address this, we propose Dreaming in Code (DiCode), a framework in which foundation models synthesize executable environment code to scaffold learning toward increasing competence. In DiCode, "dreaming" takes the form of materializing code-level variations of the world. We instantiate DiCode in Craftax, a challenging open-ended benchmark characterized by rich mechanics and long-horizon progression. Empirically, DiCode enables agents to acquire long-horizon skills, achieving a $16\%$ improvement in mean return over the strongest baseline and non-zero success on late-game combat tasks where prior methods fail. Our results suggest that code-level environment design provides a practical mechanism for curriculum control, enabling the construction of intermediate environments that bridge competence gaps in open-ended worlds. Project page and source code are available at https://konstantinosmitsides.github.io/dreaming-in-code and https://github.com/konstantinosmitsides/dreaming-in-code.

</details>


### [232] [Interpretable Dynamic Network Modeling of Tensor Time Series via Kronecker Time-Varying Graphical Lasso](https://arxiv.org/abs/2602.08197)
*Shingo Higashiguchi,Koki Kawabata,Yasuko Matsubara,Yasushi Sakurai*

Main category: cs.LG

TL;DR: 提出Kronecker时变图形套索(KTVGL)方法，用于建模张量时间序列，通过克罗内克积形式估计模态特定的动态网络，避免复杂纠缠结构，提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 随着Web服务的快速发展，金融、医疗、在线平台等领域产生大量时间序列数据。这些数据通常包含多个相互作用的变量，估计变量间的时变依赖关系（动态网络结构）对准确建模至关重要。然而，现实世界数据常表示为多模态张量时间序列，导致庞大、纠缠的网络难以解释且计算量大。

Method: 提出Kronecker时变图形套索(KTVGL)方法，通过克罗内克积形式估计模态特定的动态网络，避免复杂纠缠结构。该方法可扩展到流算法，使计算时间与序列长度无关。

Result: 在合成数据实验中，该方法比现有方法获得更高的边估计精度，同时需要更少的计算时间。通过真实世界数据的案例研究进一步证明了其实用价值。

Conclusion: KTVGL方法能有效建模张量时间序列，通过克罗内克积形式产生可解释的动态网络结构，同时显著提高计算效率，适用于大规模数据场景。

Abstract: With the rapid development of web services, large amounts of time series data are generated and accumulated across various domains such as finance, healthcare, and online platforms. As such data often co-evolves with multiple variables interacting with each other, estimating the time-varying dependencies between variables (i.e., the dynamic network structure) has become crucial for accurate modeling. However, real-world data is often represented as tensor time series with multiple modes, resulting in large, entangled networks that are hard to interpret and computationally intensive to estimate. In this paper, we propose Kronecker Time-Varying Graphical Lasso (KTVGL), a method designed for modeling tensor time series. Our approach estimates mode-specific dynamic networks in a Kronecker product form, thereby avoiding overly complex entangled structures and producing interpretable modeling results. Moreover, the partitioned network structure prevents the exponential growth of computational time with data dimension. In addition, our method can be extended to stream algorithms, making the computational time independent of the sequence length. Experiments on synthetic data show that the proposed method achieves higher edge estimation accuracy than existing methods while requiring less computation time. To further demonstrate its practical value, we also present a case study using real-world data. Our source code and datasets are available at https://github.com/Higashiguchi-Shingo/KTVGL.

</details>


### [233] [CADO: From Imitation to Cost Minimization for Heatmap-based Solvers in Combinatorial Optimization](https://arxiv.org/abs/2602.08210)
*Hyungseok Song,Deunsol Yoon,Kanghoon Lee,Han-Seul Jeong,Soonyoung Lee,Woohyung Lim*

Main category: cs.LG

TL;DR: CADO提出一种基于强化学习的微调框架，通过将扩散去噪过程建模为MDP来直接优化解码后解的成本，解决了传统监督学习中目标不匹配的问题，在组合优化任务上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于热图的组合优化求解器采用监督学习训练范式存在根本性目标不匹配问题：最小化模仿损失（如交叉熵）不能保证解成本的优化。这体现在解码器盲视（忽略不可微解码过程）和成本盲视（优先结构模仿而非解质量）两个缺陷上。

Method: 提出CADO框架：1）将扩散去噪过程建模为马尔可夫决策过程（MDP），直接优化解码后解的成本；2）引入标签中心奖励机制，将真实标签重新用作无偏基线而非模仿目标；3）采用混合微调策略实现参数高效适应。

Result: CADO在多个基准测试中实现了最先进的性能，验证了目标对齐对于释放基于热图的求解器全部潜力的重要性。

Conclusion: 目标对齐对于基于热图的组合优化求解器至关重要，CADO通过强化学习微调框架成功解决了监督学习中的目标不匹配问题，为热图求解器提供了新的发展方向。

Abstract: Heatmap-based solvers have emerged as a promising paradigm for Combinatorial Optimization (CO). However, we argue that the dominant Supervised Learning (SL) training paradigm suffers from a fundamental objective mismatch: minimizing imitation loss (e.g., cross-entropy) does not guarantee solution cost minimization. We dissect this mismatch into two deficiencies: Decoder-Blindness (being oblivious to the non-differentiable decoding process) and Cost-Blindness (prioritizing structural imitation over solution quality). We empirically demonstrate that these intrinsic flaws impose a hard performance ceiling. To overcome this limitation, we propose CADO (Cost-Aware Diffusion models for Optimization), a streamlined Reinforcement Learning fine-tuning framework that formulates the diffusion denoising process as an MDP to directly optimize the post-decoded solution cost. We introduce Label-Centered Reward, which repurposes ground-truth labels as unbiased baselines rather than imitation targets, and Hybrid Fine-Tuning for parameter-efficient adaptation. CADO achieves state-of-the-art performance across diverse benchmarks, validating that objective alignment is essential for unlocking the full potential of heatmap-based solvers.

</details>


### [234] [DrugR: Optimizing Molecular Drugs through LLM-based Explicit Reasoning](https://arxiv.org/abs/2602.08213)
*Haoran Liu,Zheni Zeng,Yukun Yan,Yuxuan Chen,Yunduo Xiao*

Main category: cs.LG

TL;DR: DrugR：基于大语言模型的药物分子优化方法，通过药理学推理逐步优化ADMET性质，同时保持分子核心疗效


<details>
  <summary>Details</summary>
Motivation: 分子生成和优化是化学领域的基础任务。虽然大语言模型（LLMs）具有强大的知识储备和交互能力，但其内在挑战在于分子结构与药理性质之间的复杂隐式关系以及缺乏相应的标注数据。

Method: 提出DrugR方法，将明确的、逐步的药理学推理引入优化过程。该方法整合了领域特定的持续预训练、通过反向数据工程的监督微调，以及自平衡的多粒度强化学习。

Result: 实验结果表明，DrugR在多个性质上实现了全面增强，同时不损害结构相似性或靶点结合亲和力。其明确的推理过程为每个优化步骤提供了清晰、可解释的依据。

Conclusion: DrugR能够有效改善关键ADMET性质，同时保持原始分子的核心疗效。其明确的推理过程提供了可操作的设计见解，推动了自动化、知识驱动的科学发现。代码和模型检查点已开源。

Abstract: Molecule generation and optimization is a fundamental task in chemical domain. The rapid development of intelligent tools, especially large language models (LLMs) with powerful knowledge reserves and interactive capabilities, has provided new paradigms for it. Nevertheless, the intrinsic challenge for LLMs lies in the complex implicit relationship between molecular structure and pharmacological properties and the lack of corresponding labeled data. To bridge this gap, we propose DrugR, an LLM-based method that introduces explicit, step-by-step pharmacological reasoning into the optimization process. Our approach integrates domain-specific continual pretraining, supervised fine-tuning via reverse data engineering, and self-balanced multi-granular reinforcement learning. This framework enables DrugR to effectively improve key ADMET properties while preserving the original molecule's core efficacy. Experimental results demonstrate that DrugR achieves comprehensive enhancement across multiple properties without compromising structural similarity or target binding affinity. Importantly, its explicit reasoning process provides clear, interpretable rationales for each optimization step, yielding actionable design insights and advancing toward automated, knowledge-driven scientific discovery. Our code and model checkpoints are open-sourced to foster future research.

</details>


### [235] [Distribution-Free Robust Functional Predict-Then-Optimize](https://arxiv.org/abs/2602.08215)
*Yash Patel,Ambuj Tewari*

Main category: cs.LG

TL;DR: 提出一种基于保形预测的神经算子不确定性量化方法，用于偏微分方程求解的决策任务，无需分布假设且可扩展


<details>
  <summary>Details</summary>
Motivation: 神经算子模型在PDE求解决策任务中应用广泛，但缺乏校准的不确定性估计。现有方法如集成或贝叶斯后验估计要么需要不切实际的分布假设，要么缺乏实际可扩展性

Method: 将保形预测应用于神经算子，在函数空间上实现分布自由的不确定性量化。使用Danskin定理的无限维推广和变分法解决下游鲁棒决策任务

Result: 该方法在多个工程任务中表现出优于高斯过程等限制性建模范式的性能，能够为下游鲁棒决策任务提供正式的遗憾表征

Conclusion: 提出的保形预测方法为神经算子提供了分布自由且可扩展的不确定性量化，能够有效支持鲁棒决策任务

Abstract: The solution of PDEs in decision-making tasks is increasingly being undertaken with the help of neural operator surrogate models due to the need for repeated evaluation. Such methods, while significantly more computationally favorable compared to their numerical counterparts, fail to provide any calibrated notions of uncertainty in their predictions. Current methods approach this deficiency typically with ensembling or Bayesian posterior estimation. However, these approaches either require distributional assumptions that fail to hold in practice or lack practical scalability, limiting their applications in practice. We, therefore, propose a novel application of conformal prediction to produce distribution-free uncertainty quantification over the function spaces mapped by neural operators. We then demonstrate how such prediction regions enable a formal regret characterization if leveraged in downstream robust decision-making tasks. We further demonstrate how such posited robust decision-making tasks can be efficiently solved using an infinite-dimensional generalization of Danskin's Theorem and calculus of variations and empirically demonstrate the superior performance of our proposed method over more restrictive modeling paradigms, such as Gaussian Processes, across several engineering tasks.

</details>


### [236] [Thermodynamic Isomorphism of Transformers: A Lagrangian Approach to Attention Dynamics](https://arxiv.org/abs/2602.08216)
*Gunn Kim*

Main category: cs.LG

TL;DR: 该论文提出了一个基于第一性原理的信息动力学框架，将Transformer注意力机制视为受最小作用量原理支配的物理系统，而非算法优化。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer架构已经彻底改变了人工智能，但其底层机制在很大程度上仍然是启发式的，缺乏统一的物理理论。本文旨在建立连接统计物理学和深度学习的理论基础。

Method: 通过将信息状态映射到具有Fisher信息度量的黎曼流形，推导出智能拉格朗日量。将softmax函数视为最小化信息气体亥姆霍兹自由能的唯一热力学平衡态，将查询-键交互识别为外部场与固有偶极矩之间的电动力学耦合。

Result: 建立了信息热力学第一定律，统一了推断（机械功）和学习（化学演化）。解释了涌现现象，如缩放定律和顿悟（grokking），作为以比热发散为特征的相变。还解释了注意力流形中的旋转对称性破缺如何产生无质量的Goldstone玻色子，为旋转位置嵌入（RoPE）提供了场论视角。

Conclusion: 该工作连接了统计物理学和深度学习，为基于物理的智能通用理论奠定了基础。

Abstract: Although the Transformer architecture has revolutionized artificial intelligence, its underlying mechanisms remain largely heuristic and lack a unified physical theory. In this work, we propose a first-principles framework for information dynamics, treating the attention mechanism as a physical system governed by the principle of least action rather than as an algorithmic optimization. By mapping information states to a Riemannian manifold with the Fisher information metric, we derive the intelligence Lagrangian. We show that the softmax function corresponds to the unique thermodynamic equilibrium state that minimizes the Helmholtz free energy of the information gas. In addition, we identify the query-key interaction as an electrodynamic coupling between an external field and an intrinsic dipole moment. This theory establishes the first law of information thermodynamics, unifying inference (mechanical work) and learning (chemical evolution). It also explains emergent phenomena, such as scaling laws and grokking, as phase transitions characterized by the divergence of specific heat. Finally, we discuss how rotational symmetry breaking in the attention manifold generates massless Goldstone bosons, providing a field-theoretic perspective on rotary positional embeddings (RoPE). Our work connects Statistical Physics and Deep Learning, laying the groundwork for a general theory of physics-based intelligence.

</details>


### [237] [Sparsity-Aware Evolution for Model Merging](https://arxiv.org/abs/2602.08218)
*Huan Zhang,Yanjian Zhang,Guillaume Wisniewski,Nadi Tomeh,Bang Liu*

Main category: cs.LG

TL;DR: 提出一种稀疏感知进化框架用于模型合并，通过迭代的剪枝-合并循环作为新型变异算子，在进化过程中引入稀疏性约束来优化模型性能


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法在可靠性和稀疏性方面存在不足，需要一种能够同时优化模型性能和稀疏性的自动化框架

Method: 采用稀疏感知进化框架，包含迭代的剪枝-合并循环作为变异算子，将稀疏约束纳入评分函数，引导进化过程偏向更稀疏的模型

Result: 实验表明该方法能提高模型合并的可靠性，在多个大规模LLM基准测试中表现良好，且方法简单易集成

Conclusion: 稀疏感知进化框架为模型合并提供了有效解决方案，通过稀疏性竞争引入额外局部吸引和相互作用，提高合并质量

Abstract: We propose a sparsity-aware evolutionary (SAE) framework for model merging that involves iterative pruning-merging cycles to act as a novel mutation operator. We incorporate the sparsity constraints into the score function, which steers the evolutionary process to favor more sparse models, in addition to other conventional performance scores. Interestingly, the by-product of \textit{competition} for sparsity introduces an extra local \textit{attraction} and interplay into the evolutionary process: if one competitor has more zero elements, the other competitor's non-zero elements will occupy those positions, even though the less sparse competitor loses to the more sparse competitor in other positions. The proposed pipeline is evaluated on a variety of large-scale LLM benchmarks. Experiments demonstrate that our approach can improve model merging reliability across multiple benchmarks, and is easy to incorporate due to its simplicity and being orthogonal to most existing approaches.

</details>


### [238] [SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning](https://arxiv.org/abs/2602.08234)
*Peng Xia,Jianwen Chen,Hanyang Wang,Jiaqi Liu,Kaide Zeng,Yu Wang,Siwei Han,Yiyang Zhou,Xujiang Zhao,Haifeng Chen,Zeyu Zheng,Cihang Xie,Huaxiu Yao*

Main category: cs.LG

TL;DR: SkillRL框架通过自动技能发现和递归演化，将原始经验转化为可重用技能库，提升LLM智能体在复杂任务中的性能


<details>
  <summary>Details</summary>
Motivation: 现有基于记忆的方法主要存储原始轨迹，这些轨迹冗余且噪声多，无法提取高级可重用行为模式，限制了智能体的泛化能力

Method: 提出SkillRL框架：1）基于经验的蒸馏机制构建分层技能库SkillBank；2）自适应检索策略获取通用和任务特定启发式；3）递归演化机制让技能库与强化学习策略协同进化

Result: 在ALFWorld、WebShop和七个搜索增强任务上取得SOTA性能，超越强基线15.3%，任务复杂度增加时仍保持鲁棒性，显著减少token占用同时提升推理效用

Conclusion: SkillRL通过将原始经验转化为结构化技能库，有效解决了LLM智能体经验学习不足的问题，实现了经验到策略改进的桥梁

Abstract: Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.

</details>


### [239] [Linearization Explains Fine-Tuning in Large Language Models](https://arxiv.org/abs/2602.08239)
*Zahra Rahimi Afzal,Tara Esmaeilbeig,Mojtaba Soltanalian,Mesrob I. Ohannessian*

Main category: cs.LG

TL;DR: 论文通过线性化视角分析参数高效微调(PEFT)，发现显式添加欧几里得距离正则化可使微调动态等价于使用神经正切核(NTK)学习，并揭示了NTK特征值谱与模型适应性能的强相关性。


<details>
  <summary>Details</summary>
Motivation: 参数高效微调(PEFT)技术虽然流行，但其训练性能和泛化机制尚未被充分探索。论文旨在通过线性化视角理解这些微调技术的工作原理，特别是分析模型线性化在大型语言模型微调中的适用性。

Method: 通过显式添加参数空间的欧几里得距离正则化，使微调动态等价于使用正定神经正切核(NTK)学习。基于正则化强度分析完全线性与线性化微调优化的接近程度，给出NTK特征值谱与层选择诱导的光谱扰动边界。

Result: 当线性化是良好模型时，NTK特征值谱与模型适应性能存在强相关性。在LLMs上的LoRA实验验证了理论，光谱扰动边界为层选择提供了理论依据。

Conclusion: 通过线性化框架深入理解了PEFT机制，揭示了NTK特征值谱与微调性能的关系，为改进PEFT技术和更明智的LLM适应提供了理论基础。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) is a popular class of techniques that strive to adapt large models in a scalable and resource-efficient manner. Yet, the mechanisms underlying their training performance and generalization remain underexplored. In this paper, we provide several insights into such fine-tuning through the lens of linearization. Fine-tuned models are often implicitly encouraged to remain close to the pretrained model. By making this explicit, using an Euclidean distance inductive bias in parameter space, we show that fine-tuning dynamics become equivalent to learning with the positive-definite neural tangent kernel (NTK). We specifically analyze how close the fully linear and the linearized fine-tuning optimizations are, based on the strength of the regularization. This allows us to be pragmatic about how good a model linearization is when fine-tuning large language models (LLMs). When linearization is a good model, our findings reveal a strong correlation between the eigenvalue spectrum of the NTK and the performance of model adaptation. Motivated by this, we give spectral perturbation bounds on the NTK induced by the choice of layers selected for fine-tuning. We empirically validate our theory on Low Rank Adaptation (LoRA) on LLMs. These insights not only characterize fine-tuning but also have the potential to enhance PEFT techniques, paving the way to better informed and more nimble adaptation in LLMs.

</details>


### [240] [Learning in Context, Guided by Choice: A Reward-Free Paradigm for Reinforcement Learning with Transformers](https://arxiv.org/abs/2602.08244)
*Juncheng Dong,Bowen He,Moyang Guo,Ethan X. Fang,Zhuoran Yang,Vahid Tarokh*

Main category: cs.LG

TL;DR: 论文提出了一种基于偏好的上下文强化学习新范式，仅使用偏好反馈进行预训练和部署，无需奖励监督，在未见任务上实现了与使用完整奖励监督方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有上下文强化学习方法依赖明确的奖励信号进行预训练，这在奖励模糊、难以指定或获取成本高的情况下限制了应用。需要一种仅使用偏好反馈就能学习的方法。

Method: 提出了基于上下文的偏好强化学习，包括两种变体：基于即时偏好的RL（每步偏好）和基于轨迹偏好的RL（轨迹级比较）。研究了监督预训练方法，并引入了直接优化策略的偏好原生框架。

Result: 在决斗赌博机、导航和连续控制任务上的实验表明，该方法能够在未见任务上实现强大的上下文泛化能力，性能与使用完整奖励监督的ICRL方法相当。

Conclusion: ICPRL证明了仅使用偏好反馈进行上下文强化学习的可行性，为奖励信号难以获取的场景提供了有效的解决方案，扩展了上下文强化学习的应用范围。

Abstract: In-context reinforcement learning (ICRL) leverages the in-context learning capabilities of transformer models (TMs) to efficiently generalize to unseen sequential decision-making tasks without parameter updates. However, existing ICRL methods rely on explicit reward signals during pretraining, which limits their applicability when rewards are ambiguous, hard to specify, or costly to obtain. To overcome this limitation, we propose a new learning paradigm, In-Context Preference-based Reinforcement Learning (ICPRL), in which both pretraining and deployment rely solely on preference feedback, eliminating the need for reward supervision. We study two variants that differ in the granularity of feedback: Immediate Preference-based RL (I-PRL) with per-step preferences, and Trajectory Preference-based RL (T-PRL) with trajectory-level comparisons. We first show that supervised pretraining, a standard approach in ICRL, remains effective under preference-only context datasets, demonstrating the feasibility of in-context reinforcement learning using only preference signals. To further improve data efficiency, we introduce alternative preference-native frameworks for I-PRL and T-PRL that directly optimize TM policies from preference data without requiring reward signals nor optimal action labels.Experiments on dueling bandits, navigation, and continuous control tasks demonstrate that ICPRL enables strong in-context generalization to unseen tasks, achieving performance comparable to ICRL methods trained with full reward supervision.

</details>


### [241] [Constraint-Aware Generative Auto-bidding via Pareto-Prioritized Regret Optimization](https://arxiv.org/abs/2602.08261)
*Binglin Wu,Yingyi Zhang,Xianneng Li,Ruyue Deng,Chuan Yue,Weiru Zhang,Xiaoyi Zeng*

Main category: cs.LG

TL;DR: PRO-Bid是一个基于约束感知生成的自动出价框架，通过约束解耦帕累托表示和反事实遗憾优化，解决了决策变换器在目标CPA约束下的状态混淆和平均行为模仿问题，实现了更好的约束满足和价值获取。


<details>
  <summary>Details</summary>
Motivation: 在目标CPA等严格效率约束下，现有决策变换器方法存在两个关键问题：1) 标准Return-to-Go条件忽略了成本维度，导致状态混淆和资源节奏控制不精确；2) 标准回归迫使策略模仿历史平均行为，限制了向约束边界优化的能力。

Method: 提出了PRO-Bid框架，包含两个协同机制：1) 约束解耦帕累托表示(CDPR)：将全局约束分解为递归成本和价值上下文以恢复资源感知，并基于帕累托前沿重新加权轨迹以聚焦高效数据；2) 反事实遗憾优化(CRO)：利用全局结果预测器识别更优的反事实动作，将这些高效用结果作为加权回归目标，使模型超越历史平均值逼近最优约束边界。

Result: 在两个公共基准测试和在线A/B测试中，PRO-Bid相比最先进的基线方法，在约束满足和价值获取方面都取得了更优的性能。

Conclusion: PRO-Bid通过约束感知的生成框架有效解决了自动出价中的约束优化问题，为在严格效率约束下最大化营销价值提供了创新解决方案。

Abstract: Auto-bidding systems aim to maximize marketing value while satisfying strict efficiency constraints such as Target Cost-Per-Action (CPA). Although Decision Transformers provide powerful sequence modeling capabilities, applying them to this constrained setting encounters two challenges: 1) standard Return-to-Go conditioning causes state aliasing by neglecting the cost dimension, preventing precise resource pacing; and 2) standard regression forces the policy to mimic average historical behaviors, thereby limiting the capacity to optimize performance toward the constraint boundary. To address these challenges, we propose PRO-Bid, a constraint-aware generative auto-bidding framework based on two synergistic mechanisms: 1) Constraint-Decoupled Pareto Representation (CDPR) decomposes global constraints into recursive cost and value contexts to restore resource perception, while reweighting trajectories based on the Pareto frontier to focus on high-efficiency data; and 2) Counterfactual Regret Optimization (CRO) facilitates active improvement by utilizing a global outcome predictor to identify superior counterfactual actions. By treating these high-utility outcomes as weighted regression targets, the model transcends historical averages to approach the optimal constraint boundary. Extensive experiments on two public benchmarks and online A/B tests demonstrate that PRO-Bid achieves superior constraint satisfaction and value acquisition compared to state-of-the-art baselines.

</details>


### [242] [Inverting Data Transformations via Diffusion Sampling](https://arxiv.org/abs/2602.08267)
*Jinwoo Kim,Sékou-Oumar Kaba,Jiyun Park,Seunghoon Hong,Siamak Ravanbakhsh*

Main category: cs.LG

TL;DR: 提出TIED方法，通过李群上的扩散过程反演未知变换，恢复数据到原始分布，提升预训练网络对输入变换的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 机器学习中未知变换会显著扭曲观测数据，需要恢复原始数据分布。特别是在测试时等变性场景中，需要提升预训练神经网络对输入变换的鲁棒性

Method: 采用概率视角，将变换后验建模为玻尔兹曼分布，提出TIED方法：在李群上设计扩散过程，保持所有更新在流形上，仅需李代数计算，利用新的平凡化目标-得分恒等式实现高效采样

Result: 在图像单应性和PDE对称性实验中，TIED能在测试时将变换输入恢复到训练分布，性能优于强基准的规范化和采样方法

Conclusion: TIED方法能有效反演未知变换，提升模型对输入变换的鲁棒性，为测试时等变性提供了有效解决方案

Abstract: We study the problem of transformation inversion on general Lie groups: a datum is transformed by an unknown group element, and the goal is to recover an inverse transformation that maps it back to the original data distribution. Such unknown transformations arise widely in machine learning and scientific modeling, where they can significantly distort observations. We take a probabilistic view and model the posterior over transformations as a Boltzmann distribution defined by an energy function on data space. To sample from this posterior, we introduce a diffusion process on Lie groups that keeps all updates on-manifold and only requires computations in the associated Lie algebra. Our method, Transformation-Inverting Energy Diffusion (TIED), relies on a new trivialized target-score identity that enables efficient score-based sampling of the transformation posterior. As a key application, we focus on test-time equivariance, where the objective is to improve the robustness of pretrained neural networks to input transformations. Experiments on image homographies and PDE symmetries demonstrate that TIED can restore transformed inputs to the training distribution at test time, showing improved performance over strong canonicalization and sampling baselines. Code is available at https://github.com/jw9730/tied.

</details>


### [243] [When Do Multi-Agent Systems Outperform? Analysing the Learning Efficiency of Agentic Systems](https://arxiv.org/abs/2602.08272)
*Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: 本文通过PAC框架理论分析MARL与SARL在LLM中的样本效率，发现任务可分解为独立子任务时MARL更优，子任务依赖则削弱其优势，并提出任务对齐概念量化分解与对齐的权衡。


<details>
  <summary>Details</summary>
Motivation: 尽管MARL在LLM训练中展现出潜力，但缺乏理论指导何时选择MARL而非SARL。现有研究缺乏对两种方法样本效率差异的系统理论分析，导致实际应用中的选择不确定性。

Method: 采用PAC框架，形式化定义LLM的SARL和MARL设置，推导显式样本复杂度边界，系统分析任务分解和对齐如何影响学习效率，并引入任务对齐概念量化分解与对齐的权衡。

Result: 当任务自然分解为独立子任务时，MARL显著改善样本复杂度；子任务依赖则削弱MARL的相对优势。任务对齐分析揭示了强制独立分解与潜在错配之间的权衡关系。

Conclusion: 理论分析澄清了MARL与SARL性能差异的根源，为复杂LLM场景中有效部署MARL策略提供了实用标准，解决了实证研究中的不一致性问题。

Abstract: Reinforcement Learning (RL) has emerged as a crucial method for training or fine-tuning large language models (LLMs), enabling adaptive, task-specific optimizations through interactive feedback. Multi-Agent Reinforcement Learning (MARL), in particular, offers a promising avenue by decomposing complex tasks into specialized subtasks learned by distinct interacting agents, potentially enhancing the ability and efficiency of LLM systems. However, theoretical insights regarding when and why MARL outperforms Single-Agent RL (SARL) remain limited, creating uncertainty in selecting the appropriate RL framework. In this paper, we address this critical gap by rigorously analyzing the comparative sample efficiency of MARL and SARL within the context of LLM. Leveraging the Probably Approximately Correct (PAC) framework, we formally define SARL and MARL setups for LLMs, derive explicit sample complexity bounds, and systematically characterize how task decomposition and alignment influence learning efficiency. Our results demonstrate that MARL improves sample complexity when tasks naturally decompose into independent subtasks, whereas dependent subtasks diminish MARL's comparative advantage. Additionally, we introduce and analyze the concept of task alignment, quantifying the trade-offs when enforcing independent task decomposition despite potential misalignments. These theoretical insights clarify empirical inconsistencies and provide practical criteria for deploying MARL strategies effectively in complex LLM scenarios.

</details>


### [244] [Noise Stability of Transformer Models](https://arxiv.org/abs/2602.08287)
*Themistoklis Haris,Zihan Zhang,Yuichi Yoshida*

Main category: cs.LG

TL;DR: 论文提出用噪声稳定性替代平均敏感度作为深度学习简单性度量，开发了理论分析和正则化方法，在算法和语言任务上显著加速训练和促进grokking。


<details>
  <summary>Details</summary>
Motivation: 当前用于衡量深度学习简单性的平均敏感度存在两个关键局限：1）难以自然推广到实值域；2）无法解释现代LLM中观察到的"junta-like"输入依赖性。需要更全面的简单性度量来理解深度学习中的简单性偏置。

Method: 提出噪声稳定性作为新的简单性度量，捕捉模型对所有输入坐标同时施加相关噪声的鲁棒性。对单层注意力层和ReLU MLP层进行理论分析，采用协方差区间传播方法解决多层传播问题，并开发了实用的噪声稳定性正则化方法。

Result: 在算法任务和下一个词预测任务上的实验表明，噪声稳定性正则化方法能持续催化grokking现象，分别加速训练约35%和75%。

Conclusion: 噪声稳定性成为理解和改进现代Transformer的强大工具，在神经网络信号传播和可解释性之间建立了新的连接，为开发可靠AI提供了新路径。

Abstract: Understanding simplicity biases in deep learning offers a promising path toward developing reliable AI. A common metric for this, inspired by Boolean function analysis, is average sensitivity, which captures a model's robustness to single-token perturbations. We argue that average sensitivity has two key limitations: it lacks a natural generalization to real-valued domains and fails to explain the "junta-like" input dependence we empirically observe in modern LLMs. To address these limitations, we propose noise stability as a more comprehensive simplicity metric. Noise stability expresses a model's robustness to correlated noise applied to all input coordinates simultaneously. We provide a theoretical analysis of noise stability for single-layer attention and ReLU MLP layers and tackle the multi-layer propagation problem with a covariance interval propagation approach. Building on this theory, we develop a practical noise stability regularization method. Experiments on algorithmic and next-token-prediction tasks show that our regularizer consistently catalyzes grokking and accelerates training by approximately $35\%$ and $75\%$ respectively. Our results sculpt a new connection between signal propagation in neural networks and interpretability, with noise stability emerging as a powerful tool for understanding and improving modern Transformers.

</details>


### [245] [Trust-Based Incentive Mechanisms in Semi-Decentralized Federated Learning Systems](https://arxiv.org/abs/2602.08290)
*Ajay Kumar Shrestha*

Main category: cs.LG

TL;DR: 提出基于信任的激励机制，通过动态评估节点贡献质量（数据质量、模型精度等）来奖励诚实参与并惩罚恶意行为，结合区块链和智能合约实现透明自动化管理。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中存在恶意或故障节点可能降低模型性能的问题，需要确保系统完整性和可靠性，但现有机制难以有效评估和激励高质量贡献。

Method: 设计基于信任的激励机制，动态评估节点的信任分数（考虑数据质量、模型精度、一致性、贡献频率等因素），以此为基础分配激励，并整合区块链和智能合约实现自动化透明管理。

Result: 构建了一个理论框架，旨在创建更健壮、公平、透明的联邦学习生态系统，减少不可信参与者带来的风险。

Conclusion: 提出的信任激励机制能有效促进诚实参与、惩罚恶意行为，结合区块链技术可增强联邦学习系统的完整性、可靠性和透明度。

Abstract: In federated learning (FL), decentralized model training allows multi-ple participants to collaboratively improve a shared machine learning model without exchanging raw data. However, ensuring the integrity and reliability of the system is challenging due to the presence of potentially malicious or faulty nodes that can degrade the model's performance. This paper proposes a novel trust-based incentive mechanism designed to evaluate and reward the quality of contributions in FL systems. By dynamically assessing trust scores based on fac-tors such as data quality, model accuracy, consistency, and contribution fre-quency, the system encourages honest participation and penalizes unreliable or malicious behavior. These trust scores form the basis of an incentive mechanism that rewards high-trust nodes with greater participation opportunities and penal-ties for low-trust participants. We further explore the integration of blockchain technology and smart contracts to automate the trust evaluation and incentive distribution processes, ensuring transparency and decentralization. Our proposed theoretical framework aims to create a more robust, fair, and transparent FL eco-system, reducing the risks posed by untrustworthy participants.

</details>


### [246] [Grokking in Linear Models for Logistic Regression](https://arxiv.org/abs/2602.08302)
*Nataraj Das,Atreya Vedantam,Chandrashekar Lakshminarayanan*

Main category: cs.LG

TL;DR: 在线性模型中发现延迟泛化现象，表明Grokking不需要深度网络，仅通过偏置项动态即可出现


<details>
  <summary>Details</summary>
Motivation: 研究Grokking（延迟泛化）现象是否真的需要深度神经网络的深度和组合结构，探索在线性模型这种最简单设置下是否也能出现Grokking

Method: 使用逻辑损失进行二元分类的线性模型，研究三种测试机制：同分布测试数据、集中在边缘的测试数据、PGD对抗攻击测试数据。理论分析梯度下降的隐式偏置如何诱导三阶段学习过程

Result: 在线性模型中观察到Grokking现象，发现数据不对称性（类别样本数量和支撑向量分布）影响Grokking的出现，并给出了Grokking时间的表征

Conclusion: Grokking不需要深度或表示学习，即使在线性模型中也能通过偏置项动态出现，为理解延迟泛化现象提供了新的理论框架

Abstract: Grokking, the phenomenon of delayed generalization, is often attributed to the depth and compositional structure of deep neural networks. We study grokking in one of the simplest possible settings: the learning of a linear model with logistic loss for binary classification on data that are linearly (and max margin) separable about the origin. We investigate three testing regimes: (1) test data drawn from the same distribution as the training data, in which case grokking is not observed; (2) test data concentrated around the margin, in which case grokking is observed; and (3) adversarial test data generated via projected gradient descent (PGD) attacks, in which case grokking is also observed. We theoretically show that the implicit bias of gradient descent induces a three-phase learning process-population-dominated, support-vector-dominated unlearning, and support-vector-dominated generalization-during which delayed generalization can arise. Our analysis further relates the emergence of grokking to asymmetries in the data, both in the number of examples per class and in the distribution of support vectors across classes, and yields a characterization of the grokking time. We experimentally validate our theory by planting different distributions of population points and support vectors, and by analyzing accuracy curves and hyperplane dynamics. Overall, our results demonstrate that grokking does not require depth or representation learning, and can emerge even in linear models through the dynamics of the bias term.

</details>


### [247] [TextResNet: Decoupling and Routing Optimization Signals in Compound AI Systems via Deep Residual Tuning](https://arxiv.org/abs/2602.08306)
*Suizhi Huang,Mei Li,Han Yu,Xiaoxiao Li*

Main category: cs.LG

TL;DR: TextResNet解决了TextGrad在深度链式AI系统中因语义纠缠导致的性能限制问题，通过语义梯度分解和因果路由实现精确信号传递


<details>
  <summary>Details</summary>
Motivation: TextGrad等文本梯度优化器在深度链式AI系统中表现不佳，根本原因是语义纠缠问题。在标准文本反向传播中，反馈信号混合了局部批评和上游上下文，导致归因模糊性，限制了优化效果。

Method: 提出TextResNet框架，包含四个关键创新：1）前向传播中强制加性语义增量，保留梯度流的恒等高速公路；2）后向传播中通过语义投影器进行语义梯度分解，将反馈解耦到因果独立子空间；3）因果路由，将投影信号路由到特定组件；4）密度感知优化调度，利用解耦信号动态分配资源到系统瓶颈。

Result: TextResNet不仅比TextGrad表现更优，而且在复合AI系统的代理任务中展现出卓越的稳定性，而基线方法会崩溃。代码已开源。

Conclusion: TextResNet通过解决语义纠缠问题，实现了深度链式AI系统中更精确的梯度信号传递，显著提升了文本梯度优化的效果和稳定性。

Abstract: Textual Gradient-style optimizers (TextGrad) enable gradient-like feedback propagation through compound AI systems. However, they do not work well for deep chains. The root cause of this limitation stems from the Semantic Entanglement problem in these extended workflows. In standard textual backpropagation, feedback signals mix local critiques with upstream contexts, leading to Attribution Ambiguity. To address this challenge, we propose TextResNet, a framework that reformulates the optimization process to achieve precise signal routing via four key innovations. Firstly, in the forward pass, it enforces Additive Semantic Deltas to preserve an Identity Highway for gradient flow. Secondly, in the backward pass, it introduces Semantic Gradient Decomposition via a Semantic Projector to disentangle feedback into causally independent subspaces. Thirdly, it implements Causal Routing, which routes projected signals to their specific components. Finally, it performs Density-Aware Optimization Scheduling to leverage the disentangled signals to dynamically allocate resources to key system bottlenecks. Our results show that TextResNet not only achieves superior performance compared to TextGrad, but also exhibits remarkable stability for agentic tasks in compound AI systems where baselines collapse. Code is available at https://github.com/JeanDiable/TextResNet.

</details>


### [248] [Interaction-Grounded Learning for Contextual Markov Decision Processes with Personalized Feedback](https://arxiv.org/abs/2602.08307)
*Mengxiao Zhang,Yuheng Zhang,Haipeng Luo,Paul Mineiro*

Main category: cs.LG

TL;DR: 本文提出了一种用于多步顺序决策的交互式基础学习算法，解决了从间接反馈中学习个性化目标的问题，并在合成和真实数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的交互式基础学习（IGL）研究主要局限于单步设置，无法适用于现代顺序决策系统（如多轮LLM部署）。需要将IGL扩展到多步马尔可夫决策过程（MDP）中，以处理从间接反馈中学习个性化目标的问题。

Method: 提出计算高效的算法，将Zhang等人（2024a）的奖励估计器从单步扩展到多步设置，解决MDP下解码潜在奖励的独特挑战。基于此估计器，设计了逆间隙加权（IGW）算法进行策略优化。

Result: 算法在上下文情景MDP中实现了次线性遗憾保证。在合成情景MDP和真实用户预订数据集上的实验表明，该方法能有效从多轮交互中学习个性化目标。

Conclusion: 成功将交互式基础学习扩展到多步顺序决策设置，为从间接反馈中学习个性化目标提供了理论保证和实际有效的解决方案，适用于现代决策系统如多轮LLM部署。

Abstract: In this paper, we study Interaction-Grounded Learning (IGL) [Xie et al., 2021], a paradigm designed for realistic scenarios where the learner receives indirect feedback generated by an unknown mechanism, rather than explicit numerical rewards. While prior work on IGL provides efficient algorithms with provable guarantees, those results are confined to single-step settings, restricting their applicability to modern sequential decision-making systems such as multi-turn Large Language Model (LLM) deployments. To bridge this gap, we propose a computationally efficient algorithm that achieves a sublinear regret guarantee for contextual episodic Markov Decision Processes (MDPs) with personalized feedback. Technically, we extend the reward-estimator construction of Zhang et al. [2024a] from the single-step to the multi-step setting, addressing the unique challenges of decoding latent rewards under MDPs. Building on this estimator, we design an Inverse-Gap-Weighting (IGW) algorithm for policy optimization. Finally, we demonstrate the effectiveness of our method in learning personalized objectives from multi-turn interactions through experiments on both a synthetic episodic MDP and a real-world user booking dataset.

</details>


### [249] [Fast Flow Matching based Conditional Independence Tests for Causal Discovery](https://arxiv.org/abs/2602.08315)
*Shunyu Zhao,Yanfeng Yang,Shuai Li,Kenji Fukumizu*

Main category: cs.LG

TL;DR: 提出FMCIT方法，利用流匹配技术加速条件独立性测试，显著提升因果发现效率，并整合到GPC-FMCIT框架中实现准确性与效率的良好平衡。


<details>
  <summary>Details</summary>
Motivation: 基于约束的因果发现方法需要进行大量条件独立性测试，计算复杂度高，限制了实际应用。需要设计能加速每个独立测试的算法。

Method: 提出基于流匹配的条件独立性测试(FMCIT)，利用流匹配的高计算效率，模型在整个因果发现过程中只需训练一次。进一步将FMCIT整合到两阶段引导PC骨架学习框架(GPC-FMCIT)中，结合快速筛选和引导式预算精炼。

Result: FMCIT能有效控制I型错误，在高维条件集下仍保持较高的测试功效。GPC-FMCIT在合成和真实世界因果发现任务中展现出优于现有CI测试方法和PC变体的准确性与效率平衡。

Conclusion: FMCIT通过流匹配技术显著加速了条件独立性测试，GPC-FMCIT框架在保证统计功效的同时限制了CI查询数量，为因果发现提供了高效实用的解决方案。

Abstract: Constraint-based causal discovery methods require a large number of conditional independence (CI) tests, which severely limits their practical applicability due to high computational complexity. Therefore, it is crucial to design an algorithm that accelerates each individual test. To this end, we propose the Flow Matching-based Conditional Independence Test (FMCIT). The proposed test leverages the high computational efficiency of flow matching and requires the model to be trained only once throughout the entire causal discovery procedure, substantially accelerating causal discovery. According to numerical experiments, FMCIT effectively controls type-I error and maintains high testing power under the alternative hypothesis, even in the presence of high-dimensional conditioning sets. In addition, we further integrate FMCIT into a two-stage guided PC skeleton learning framework, termed GPC-FMCIT, which combines fast screening with guided, budgeted refinement using FMCIT. This design yields explicit bounds on the number of CI queries while maintaining high statistical power. Experiments on synthetic and real-world causal discovery tasks demonstrate favorable accuracy-efficiency trade-offs over existing CI testing methods and PC variants.

</details>


### [250] [Towards Efficient Large Language Reasoning Models via Extreme-Ratio Chain-of-Thought Compression](https://arxiv.org/abs/2602.08324)
*Yuntian Tang,Bohan Jia,Wenxuan Huang,Lianyue Zhang,Jiao Xie,Wenxi Li,Wei Li,Jie Hu,Xinghao Chen,Rongrong Ji,Shaohui Lin*

Main category: cs.LG

TL;DR: Extra-CoT框架通过极端比率压缩思维链，在保持准确性的同时大幅减少推理计算开销


<details>
  <summary>Details</summary>
Motivation: 现有思维链压缩方法在高压缩比下会严重损失逻辑保真度，导致性能显著下降，需要实现高保真度的快速推理

Method: 1) 训练专门的语义保留压缩器生成高保真监督数据；2) 混合比率监督微调LLM适应不同压缩预算；3) 提出CHRPO强化学习方法，通过分层奖励激励低预算下的问题解决能力

Result: 在三个数学推理基准测试中表现优异，例如在MATH-500上使用Qwen3-1.7B实现了超过73%的token减少，同时准确率提升0.6%，显著优于SOTA方法

Conclusion: Extra-CoT框架成功实现了极端比率下的高保真思维链压缩，在保持准确性的同时大幅减少了推理计算开销，为高效推理提供了有效解决方案

Abstract: Chain-of-Thought (CoT) reasoning successfully enhances the reasoning capabilities of Large Language Models (LLMs), yet it incurs substantial computational overhead for inference. Existing CoT compression methods often suffer from a critical loss of logical fidelity at high compression ratios, resulting in significant performance degradation. To achieve high-fidelity, fast reasoning, we propose a novel EXTreme-RAtio Chain-of-Thought Compression framework, termed Extra-CoT, which aggressively reduces the token budget while preserving answer accuracy. To generate reliable, high-fidelity supervision, we first train a dedicated semantically-preserved compressor on mathematical CoT data with fine-grained annotations. An LLM is then fine-tuned on these compressed pairs via a mixed-ratio supervised fine-tuning (SFT), teaching it to follow a spectrum of compression budgets and providing a stable initialization for reinforcement learning (RL). We further propose Constrained and Hierarchical Ratio Policy Optimization (CHRPO) to explicitly incentivize question-solving ability under lower budgets by a hierarchical reward. Experiments on three mathematical reasoning benchmarks show the superiority of Extra-CoT. For example, on MATH-500 using Qwen3-1.7B, Extra-CoT achieves over 73\% token reduction with an accuracy improvement of 0.6\%, significantly outperforming state-of-the-art (SOTA) methods.

</details>


### [251] [Near-Oracle KV Selection via Pre-hoc Sparsity for Long-Context Inference](https://arxiv.org/abs/2602.08329)
*Yifei Gao,Lei Wang,Rong-Cheng Tu,Qixin Zhang,Jun Cheng,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出Pre-hoc Sparsity (PrHS)方法，在注意力评分前选择KV缓存条目，通过控制丢弃质量(delta)提供显式精度保证，相比后验启发式方法减少偏差，在多个基准测试中显著降低计算开销并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理中的核心瓶颈是不断增长的KV缓存注意力计算成本。现有稀疏方法依赖后验启发式（基于观察到的注意力或代理分数），这引入后验偏差：扭曲真实token重要性并遗漏关键token，损害长程推理能力。

Method: 提出Pre-hoc Sparsity (PrHS)，在注意力评分前选择KV条目，通过控制丢弃质量(delta)提供显式精度控制。通过边际到互信息分析，推导出仅依赖于丢弃质量的互信息损失上界。在PrHS框架内，沿时间、深度和层三个正交轴实例化三种预选器。

Result: 在LLaMA和Mistral系列模型上验证：GSM8K和CoQA上减少超过90%检索开销，比HShare实现3倍更高的检索稀疏度且精度相当或更好；LongBench上平均退化低于1%；比先前稀疏基线降低约15%注意力FLOPs；在NVIDIA A100-80GB GPU上实现9.9倍注意力算子延迟加速和2.8倍吞吐量提升。

Conclusion: PrHS通过预选KV条目和控制丢弃质量，解决了后验启发式方法的偏差问题，提供可验证的精度保证，显著降低大语言模型推理的计算和带宽开销，同时保持模型性能。

Abstract: A core bottleneck in large language model (LLM) inference is the cost of attending over the ever-growing key-value (KV) cache. Although near-oracle top-k KV selection can preserve the quality of dense attention while sharply reducing computation and bandwidth, existing sparse methods generally rely on posterior heuristics, i.e., selectors conditioned on observed attention or proxy scores. Such conditioning introduces posterior bias: it tends to distort true token importance and miss salient tokens, thereby impairing long-range reasoning. To tackle this problem, we propose Pre-hoc Sparsity (PrHS), which selects KV entries before attention scoring and provides explicit accuracy control. Let the attention mass of discarded entries be delta (the dropped mass). Through a marginal-to-mutual-information analysis, we derive an upper bound on the mutual-information loss that depends only on the dropped mass. This relation explains failure modes of posterior heuristics and enables verifiable guarantees by controlling the dropped mass in advance. Within PrHS, we instantiate three orthogonal pre-hoc selectors along the axes of time, depth, and layer. Extensive experiments on LLaMA and Mistral families validate PrHS. Across GSM8K and CoQA, PrHS reduces retrieval overhead by over 90%, achieving 3x higher retrieval sparsity than HShare at matched or better accuracy. It incurs under 1% average degradation on LongBench, lowers attention FLOPs by about 15% versus prior sparse baselines, and yields a 9.9x speedup in attention-operator latency and 2.8x higher throughput on NVIDIA A100-80GB GPUs than the dense baseline.

</details>


### [252] [Regime Change Hypothesis: Foundations for Decoupled Dynamics in Neural Network Training](https://arxiv.org/abs/2602.08333)
*Cristian Pérez-Corral,Alberto Fernández-Hernández,Jose I. Mestre,Manuel F. Dolz,Jose Duato,Enrique S. Quintana-Ortí*

Main category: cs.LG

TL;DR: 论文研究了ReLU网络的训练动态，发现存在两阶段行为：早期激活模式变化显著，后期权重更新主要在稳定的激活区域内进行微调。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络取得了经验上的成功，但其内部训练动态仍难以表征。ReLU模型中的激活模式决定了网络行为的仿射区域，作者希望探究训练是否表现出两阶段行为：早期激活模式变化显著，后期权重更新主要在稳定的激活区域内进行微调。

Method: 首先证明了局部稳定性性质：在参数和输入的测度零集之外，足够小的参数扰动会保持固定输入的激活模式。然后通过实验追踪全连接、卷积和Transformer架构中每次迭代的权重和激活模式变化，其中激活模式记录在ReLU前馈子模块中。

Result: 实验发现激活模式变化比权重更新幅度早3倍衰减，表明后期训练通常在相对稳定的激活区域内进行。这为监控训练动态提供了架构无关的具体工具。

Conclusion: 研究揭示了ReLU网络训练的两阶段动态，为理解训练过程提供了新视角，并激励了对分段线性网络解耦优化策略的进一步研究。代码和实验配置将在接受后发布以确保可复现性。

Abstract: Despite the empirical success of DNN, their internal training dynamics remain difficult to characterize. In ReLU-based models, the activation pattern induced by a given input determines the piecewise-linear region in which the network behaves affinely. Motivated by this geometry, we investigate whether training exhibits a two-timescale behavior: an early stage with substantial changes in activation patterns and a later stage where weight updates predominantly refine the model within largely stable activation regimes. We first prove a local stability property: outside measure-zero sets of parameters and inputs, sufficiently small parameter perturbations preserve the activation pattern of a fixed input, implying locally affine behavior within activation regions. We then empirically track per-iteration changes in weights and activation patterns across fully-connected and convolutional architectures, as well as Transformer-based models, where activation patterns are recorded in the ReLU feed-forward (MLP/FFN) submodules, using fixed validation subsets. Across the evaluated settings, activation-pattern changes decay 3 times earlier than weight-update magnitudes, showing that late-stage training often proceeds within relatively stable activation regimes. These findings provide a concrete, architecture-agnostic instrument for monitoring training dynamics and motivate further study of decoupled optimization strategies for piecewise-linear networks. For reproducibility, code and experiment configurations will be released upon acceptance.

</details>


### [253] [ManifoldKV: Training-Free KV Cache Compression via Euclidean Outlier Detection](https://arxiv.org/abs/2602.08343)
*Debajyoti Datta,Trishala Neeraj,Bibek Paudel,Vyom Sharma,Subhabrata Mukherjee*

Main category: cs.LG

TL;DR: ManifoldKV：一种基于欧氏距离的KV缓存压缩方法，在长上下文推理中通过同时考虑角度和径向偏差来选择保留的关键token，相比基于余弦相似度的方法在多项任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 长上下文推理受限于KV缓存内存的线性增长，现有基于余弦相似度的几何驱逐方法会丢失幅度信息，无法区分语义上重要的token。

Method: 提出ManifoldKV训练自由评分器，基于token到关键质心的欧氏距离进行排序，同时捕捉角度和径向偏差。针对64K长上下文还提出WindowedManifoldKV变体。

Result: 在RULER基准测试中，4K-16K上下文下20%压缩率实现95.7%准确率；多键检索任务中比KeyDiff提升15.4个百分点；64K上下文下WindowedManifoldKV恢复84.3%准确率。

Conclusion: ManifoldKV通过欧氏距离评分有效解决了余弦相似度方法的局限性，仅需3行代码即可在4种架构上无需调优地工作，显著提升长上下文KV缓存压缩的鲁棒性。

Abstract: Long-context inference is constrained by KV-cache memory, which grows linearly with sequence length; KV-cache compression therefore hinges on reliably selecting which past tokens to retain. Most geometry-based eviction methods score keys by cosine similarity to a global centroid, but cosine is scale-invariant and can discard magnitude cues that distinguish semantically salient tokens. We propose ManifoldKV, a training-free scorer that ranks tokens by Euclidean distance to the key centroid, capturing both angular and radial deviations.
  On the RULER benchmark, ManifoldKV achieves 95.7% accuracy at 4K-16K contexts with 20% compression; matching the best geometric baseline while improving robustness in two regimes where cosine scoring fails. First, on multi-key retrieval, ManifoldKV reduces directional collisions, achieving 92.4% vs KeyDiff's 77.0% (+15.4 points) on 3-key NIAH at 50% compression. Second, to address dilution and performance collapse of global centroids at 64K context, we introduce WindowedManifoldKV, which restores accuracy to 84.3% at 25% compression, a 49-point recovery over global L2 and +3.2 points over KeyDiff. The method requires only 3 lines of code and works across 4 architectures without tuning.

</details>


### [254] [All ERMs Can Fail in Stochastic Convex Optimization Lower Bounds in Linear Dimension](https://arxiv.org/abs/2602.08350)
*Tal Burla,Roi Livni*

Main category: cs.LG

TL;DR: 该论文研究了随机凸优化中经验风险最小化器（ERM）的样本复杂度，证明了存在ERM唯一且会过拟合的实例，解决了Feldman的开放问题，并给出了梯度下降的泛化下界。


<details>
  <summary>Details</summary>
Motivation: 研究随机凸优化中经验风险最小化器的样本复杂度和过拟合行为，解决Feldman提出的开放问题，并探索梯度下降算法的泛化性能。

Method: 通过构造特定实例证明ERM的过拟合现象，并基于此构造分析梯度下降算法的泛化下界，使用理论分析和数学证明方法。

Result: 证明了存在样本量与维度呈线性关系时，ERM可能唯一且过拟合的实例；给出了梯度下降的泛化下界Ω(√(ηT/m^1.5))，显著缩小了现有上下界之间的差距。

Conclusion: 经验风险最小化器在随机凸优化中可能过拟合，梯度下降算法也存在泛化风险，这为理解机器学习算法的泛化性能提供了新的理论洞见。

Abstract: We study the sample complexity of the best-case Empirical Risk Minimizer in the setting of stochastic convex optimization. We show that there exists an instance in which the sample size is linear in the dimension, learning is possible, but the Empirical Risk Minimizer is likely to be unique and to overfit. This resolves an open question by Feldman. We also extend this to approximate ERMs.
  Building on our construction we also show that (constrained) Gradient Descent potentially overfits when horizon and learning rate grow w.r.t sample size. Specifically we provide a novel generalization lower bound of $Ω\left(\sqrt{ηT/m^{1.5}}\right)$ for Gradient Descent, where $η$ is the learning rate, $T$ is the horizon and $m$ is the sample size. This narrows down, exponentially, the gap between the best known upper bound of $O(ηT/m)$ and existing lower bounds from previous constructions.

</details>


### [255] [The Chicken and Egg Dilemma: Co-optimizing Data and Model Configurations for LLMs](https://arxiv.org/abs/2602.08351)
*Zhiliang Chen,Alfred Wei Lun Leong,Shao Yong Ong,Apivich Hemachandram,Gregory Kang Ruey Lau,Chuan-Sheng Foo,Zhengyuan Liu,Nancy F. Chen,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: JoBS：一种联合优化LLM训练数据和模型配置的方法，使用缩放定律性能预测器辅助贝叶斯优化，通过部分预算学习预测器来避免完整训练运行的高成本。


<details>
  <summary>Details</summary>
Motivation: LLM训练中存在数据和模型配置的"鸡生蛋蛋生鸡"困境：最佳训练数据配置依赖于模型配置，反之亦然。现有方法通常只优化数据或模型，忽略了它们的相互作用，而联合优化又往往被认为是不可行的。

Method: JoBS方法结合缩放定律性能预测器和贝叶斯优化：1）分配部分优化预算学习LLM性能预测器，仅通过少量训练步骤预测配置的潜力；2）剩余预算使用该预测器进行贝叶斯优化，避免完整训练运行的高成本；3）研究平均遗憾并设计最优预算分配策略。

Result: JoBS在相同优化预算下，优于现有的多保真度贝叶斯优化基线方法，以及在各种LLM任务上的数据和模型优化方法。

Conclusion: JoBS通过使用性能预测器来指导贝叶斯优化，有效解决了LLM训练中数据和模型配置的联合优化问题，在有限预算内实现了更好的性能。

Abstract: Co-optimizing data and model configurations for training LLMs presents a classic chicken-and-egg dilemma: The best training data configuration (e.g., data mixture) for a downstream task depends on the chosen model configuration (e.g., model architecture), and vice versa. However, jointly optimizing both data and model configurations is often deemed intractable, and existing methods focus on either data or model optimization without considering their interaction. We introduce JoBS, an approach that uses a scaling-law-inspired performance predictor to aid Bayesian optimization (BO) in jointly optimizing LLM training data and model configurations efficiently. JoBS allocates a portion of the optimization budget to learn an LLM performance predictor that predicts how promising a training configuration is from a small number of training steps. The remaining budget is used to perform BO entirely with the predictor, effectively amortizing the cost of running full-training runs. We study JoBS's average regret and devise the optimal budget allocation to minimize regret. JoBS outperforms existing multi-fidelity BO baselines, as well as data and model optimization approaches across diverse LLM tasks under the same optimization budget.

</details>


### [256] [Dynamic Regret via Discounted-to-Dynamic Reduction with Applications to Curved Losses and Adam Optimizer](https://arxiv.org/abs/2602.08372)
*Yan-Feng Xie,Yu-Jie Zhang,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种模块化方法，通过折扣到动态的归约技术，为FTRL方法及其相关优化器（如Adam）在非平稳在线学习中获得动态遗憾界。


<details>
  <summary>Details</summary>
Motivation: FTRL方法对于曲线损失和自适应优化器（如Adam）的理解很重要，但现有的动态遗憾分析对FTRL方法研究较少。需要一种系统的方法来分析FTRL在非平稳环境中的性能。

Method: 基于折扣到动态的归约技术，提出模块化方法获得FTRL相关问题的动态遗憾界。重点关注线性回归和逻辑回归两种代表性曲线损失，并将该方法扩展到Adam优化器的分析。

Result: 简化了在线线性回归最优动态遗憾的证明，获得了在线逻辑回归的新动态遗憾保证。在随机、非凸、非光滑设置下为Adam优化器获得了最优收敛率，并对带两个折扣参数的Adam进行了更详细分析，得到了剪裁和无剪裁变体的新结果。

Conclusion: 提出的归约方法为FTRL和Adam优化器在非平稳在线学习中的动态遗憾分析提供了统一框架，不仅简化了现有证明，还获得了新的理论保证，扩展了这些方法的应用范围。

Abstract: We study dynamic regret minimization in non-stationary online learning, with a primary focus on follow-the-regularized-leader (FTRL) methods. FTRL is important for curved losses and for understanding adaptive optimizers such as Adam, yet existing dynamic regret analyses are less explored for FTRL. To address this, we build on the discounted-to-dynamic reduction and present a modular way to obtain dynamic regret bounds of FTRL-related problems. Specifically, we focus on two representative curved losses: linear regression and logistic regression. Our method not only simplifies existing proofs for the optimal dynamic regret of online linear regression, but also yields new dynamic regret guarantees for online logistic regression. Beyond online convex optimization, we apply the reduction to analyze the Adam optimizers, obtaining optimal convergence rates in stochastic, non-convex, and non-smooth settings. The reduction also enables a more detailed treatment of Adam with two discount parameters $(β_1,β_2)$, leading to new results for both clipped and clip-free variants of Adam optimizers.

</details>


### [257] [OJBKQ: Objective-Joint Babai-Klein Quantization](https://arxiv.org/abs/2602.08376)
*Xinyu Wang,Ziyu Zhao,Peng Lu,Yu Gu,Xiao-Wen Chang*

Main category: cs.LG

TL;DR: OJBKQ是一种新的后训练量化方法，将权重量化建模为激活和权重的联合优化问题，通过扩展的Babai-Klein算法求解，在3-4位量化下比现有方法获得更低的困惑度。


<details>
  <summary>Details</summary>
Motivation: 现有仅权重量化方法依赖启发式目标和贪心舍入，导致低比特量化时性能显著下降，需要更优化的量化方法。

Method: 将每层权重量化建模为多右端盒约束整数最小二乘问题，使用扩展的Babai最近平面算法和Klein随机Babai算法寻找最小残差的Babai-Klein点作为次优解。

Result: 在大语言模型上的实验表明，OJBKQ在3-4位量化下相比现有PTQ方法获得更低的困惑度，同时保持可比较的计算成本。

Conclusion: OJBKQ通过将量化问题形式化为联合优化并采用有效的算法求解，在低比特量化中实现了更好的性能，为后训练量化提供了新的有效方法。

Abstract: Post-training quantization (PTQ) is widely used to compress large language models without retraining. However, many existing weight-only methods rely on heuristic objectives and greedy rounding, thus leading to noticeable degradation under low-bit quantization. In this work, we introduce OJBKQ (Objective-Joint Babai-Klein Quantization with K-Best Sampling), a layer-wise PTQ method that formulates weight quantization as a joint optimization problem over activations and weights. This formulation results in a multiple-right-hand-side box-constrained integer least squares (BILS) problem in each layer, which is NP-hard. For each column of the weight matrix, we apply an extended Babai nearest-plane algorithm and an extended version of Klein's randomized Babai algorithm to find the minimum-residual Babai-Klein point, a sub-optimal solution to the BILS problem. Experimental results on large language models show that OJBKQ achieves lower perplexity at 3-4 bits compared to existing PTQ approaches, while maintaining comparable computational cost.

</details>


### [258] [Reinforcement Learning with Backtracking Feedback](https://arxiv.org/abs/2602.08377)
*Bilgehan Sel,Vaishakh Keshava,Phillip Wallis,Lukas Rutishauser,Ming Jin,Dingcheng Li*

Main category: cs.LG

TL;DR: RLBF框架通过强化学习让LLM学会动态纠正自身生成错误，使用"回溯x个token"信号来恢复安全违规，显著提升对抗攻击和分布内错误的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在面对对抗攻击和分布内错误时的安全鲁棒性不足问题，现有方法如BSAFE仍有改进空间。

Method: 1. 强化学习阶段：模型通过critic反馈学习动态纠正生成错误，发出"回溯x个token"信号后继续自回归生成；2. 改进的监督微调数据生成策略(BSAFE+)：在原本安全的连贯文本中注入违规内容，为回溯机制提供更有效的初始训练。

Result: RLBF显著降低了多种基准测试和模型规模下的攻击成功率，在保持模型基础功能的同时实现了卓越的安全性能。

Conclusion: RLBF框架通过强化学习训练模型动态纠正错误的能力，结合改进的数据生成策略，有效提升了LLM对抗复杂攻击的安全鲁棒性。

Abstract: Addressing the critical need for robust safety in Large Language Models (LLMs), particularly against adversarial attacks and in-distribution errors, we introduce Reinforcement Learning with Backtracking Feedback (RLBF). This framework advances upon prior methods, such as BSAFE, by primarily leveraging a Reinforcement Learning (RL) stage where models learn to dynamically correct their own generation errors. Through RL with critic feedback on the model's live outputs, LLMs are trained to identify and recover from their actual, emergent safety violations by emitting an efficient "backtrack by x tokens" signal, then continuing generation autoregressively. This RL process is crucial for instilling resilience against sophisticated adversarial strategies, including middle filling, Greedy Coordinate Gradient (GCG) attacks, and decoding parameter manipulations. To further support the acquisition of this backtracking capability, we also propose an enhanced Supervised Fine-Tuning (SFT) data generation strategy (BSAFE+). This method improves upon previous data creation techniques by injecting violations into coherent, originally safe text, providing more effective initial training for the backtracking mechanism. Comprehensive empirical evaluations demonstrate that RLBF significantly reduces attack success rates across diverse benchmarks and model scales, achieving superior safety outcomes while critically preserving foundational model utility.

</details>


### [259] [Modalities, a PyTorch-native Framework For Large-scale LLM Training and Research](https://arxiv.org/abs/2602.08387)
*Max Lübbering,Timm Ruland,Richard Rutmann,Felix Stollenwerk,David Fitzek,Michael Fromm,Alexander Weber,Rafet Sifa,Nicolas Flores-Herr,Joachim Köhler,Mehdi Ali*

Main category: cs.LG

TL;DR: Modalities是一个端到端的PyTorch原生框架，旨在解决大规模LLM预训练和研究中计算密集型消融实验的挑战，通过集成先进并行化策略和模块化设计，实现高效训练和可复现性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM预训练和研究工作流程需要大量计算资源进行大规模消融研究，但现有开源框架对此支持有限，研究人员通常需要自行编写包装器和脚本，导致效率低下且难以复现。

Method: Modalities采用端到端PyTorch原生设计，集成最先进的并行化策略，支持万亿token和十亿参数规模的高效预训练和系统消融；采用模块化设计和声明式自包含配置，确保可复现性和可扩展性。

Result: 该框架能够同时实现高效预训练和系统性消融实验，提供现有LLM训练框架难以达到的可复现性和可扩展性水平。

Conclusion: Modalities框架通过集成先进并行化和模块化设计，为大规模LLM研究提供了更高效、可复现的实验工具，解决了当前研究流程中的关键瓶颈。

Abstract: Today's LLM (pre-) training and research workflows typically allocate a significant amount of compute to large-scale ablation studies. Despite the substantial compute costs of these ablations, existing open-source frameworks provide limited tooling for these experiments, often forcing researchers to write their own wrappers and scripts. We propose Modalities, an end-to-end PyTorch-native framework that integrates data-driven LLM research with large-scale model training from two angles. Firstly, by integrating state-of-the-art parallelization strategies, it enables both efficient pretraining and systematic ablations at trillion-token and billion-parameter scale. Secondly, Modalities adopts modular design with declarative, self-contained configuration, enabling reproducibility and extensibility levels that are difficult to achieve out-of-the-box with existing LLM training frameworks.

</details>


### [260] [Drop the mask! GAMM-A Taxonomy for Graph Attributes Missing Mechanisms](https://arxiv.org/abs/2602.08407)
*Richard Serrano,Baptiste Jeudy,Charlotte Laclau,Christine Largeron*

Main category: cs.LG

TL;DR: 本文提出了GAMM框架，将缺失数据机制分类扩展到属性图，考虑了节点属性和图结构的依赖关系，并发现现有插补方法在图感知缺失场景下表现不佳。


<details>
  <summary>Details</summary>
Motivation: 属性图中的缺失数据问题比表格数据更具挑战性，因为需要考虑图结构的影响。现有缺失机制分类主要针对表格数据，缺乏对图特定依赖关系的考虑。

Method: 提出了GAMM（Graph Attributes Missing Mechanisms）框架，将缺失数据机制分类扩展到属性图，系统地将缺失概率与节点属性和底层图结构联系起来，引入了图特定的依赖关系。

Result: 实证研究表明，最先进的插补方法在传统缺失机制上有效，但在这些更现实的图感知缺失场景中表现显著下降。

Conclusion: 属性图中的缺失数据需要专门的机制分类，GAMM框架为此提供了系统方法，揭示了现有方法在图感知缺失场景下的局限性，为未来研究指明了方向。

Abstract: Exploring missing data in attributed graphs introduces unique challenges beyond those found in tabular datasets. In this work, we extend the taxonomy for missing data mechanisms to attributed graphs by proposing GAMM (Graph Attributes Missing Mechanisms), a framework that systematically links missingness probability to both node attributes and the underlying graph structure. Our taxonomy enriches the conventional definitions of masking mechanisms by introducing graph-specific dependencies. We empirically demonstrate that state-of-the-art imputation methods, while effective on traditional masks, significantly struggle when confronted with these more realistic graph-aware missingness scenarios.

</details>


### [261] [Radial Müntz-Szász Networks: Neural Architectures with Learnable Power Bases for Multidimensional Singularities](https://arxiv.org/abs/2602.08419)
*Gnankan Landry Regis N'guessan,Bum Jun Kim*

Main category: cs.LG

TL;DR: 论文提出径向Müntz-Szász网络(RMN)来解决坐标可分离神经网络难以建模径向奇异场(如1/r、log r、裂纹尖端剖面)的问题，通过可学习的径向幂次组合实现精确建模，在多个基准测试中显著优于传统MLP和SIREN网络。


<details>
  <summary>Details</summary>
Motivation: 径向奇异场(如1/r、log r、裂纹尖端剖面)对坐标可分离的神经网络架构难以建模。论文证明任何既是径向又是加性可分离的C^2函数必须是二次函数，这为坐标幂律模型建立了基本障碍。

Method: 提出径向Müntz-Szász网络(RMN)，将场表示为可学习径向幂次r^μ(包括负指数)的线性组合，加上极限稳定的对数原语以实现精确的log r行为。RMN允许闭式空间梯度和拉普拉斯算子，支持穿孔域上的物理信息学习。扩展包括角依赖(RMN-Angular)和具有可学习中心的多源(RMN-MC)。

Result: 在10个2D和3D基准测试中，RMN使用27个参数(MLP使用33,537个，SIREN使用8,577个)实现了比MLP低1.5-51倍的RMSE，比SIREN低10-100倍的RMSE。当优化收敛时，源中心恢复误差低于10^-4。论文还报告了在平滑、强非径向目标上的受控失败，以界定RMN的操作范围。

Conclusion: RMN为径向奇异场提供了高效精确的神经网络表示，克服了传统坐标可分离架构的基本限制，在物理信息学习中具有重要应用价值，特别是在需要精确建模奇异行为的领域。

Abstract: Radial singular fields, such as $1/r$, $\log r$, and crack-tip profiles, are difficult to model for coordinate-separable neural architectures. We show that any $C^2$ function that is both radial and additively separable must be quadratic, establishing a fundamental obstruction for coordinate-wise power-law models. Motivated by this result, we introduce Radial Müntz-Szász Networks (RMN), which represent fields as linear combinations of learnable radial powers $r^μ$, including negative exponents, together with a limit-stable log-primitive for exact $\log r$ behavior. RMN admits closed-form spatial gradients and Laplacians, enabling physics-informed learning on punctured domains. Across ten 2D and 3D benchmarks, RMN achieves 1.5$\times$--51$\times$ lower RMSE than MLPs and 10$\times$--100$\times$ lower RMSE than SIREN while using 27 parameters, compared with 33,537 for MLPs and 8,577 for SIREN. We extend RMN to angular dependence (RMN-Angular) and to multiple sources with learnable centers (RMN-MC); when optimization converges, source-center recovery errors fall below $10^{-4}$. We also report controlled failures on smooth, strongly non-radial targets to delineate RMN's operating regime.

</details>


### [262] [The Connection between Kriging and Large Neural Networks](https://arxiv.org/abs/2602.08427)
*Marius Marinescu*

Main category: cs.LG

TL;DR: 本文探讨了克里金法与神经网络之间的理论联系，旨在通过结合空间统计与机器学习提升模型的可解释性、可靠性和空间感知能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI在各学科的普及，空间统计学正处于与AI深度融合的关键时刻。作者关注空间统计模型与机器学习模型之间的关系，特别是克里金法与神经网络这两种看似无关的方法之间的深层联系，希望通过理解它们的关联来增强机器学习技术。

Method: 通过文献回顾和理论分析，研究克里金法（及其机器学习对应方法高斯过程回归）与神经网络之间的连接关系。分析这两种方法在概率理论基础和黑盒模型特性方面的异同。

Result: 研究发现克里金法与神经网络之间存在强烈关联，尽管前者基于概率理论和随机过程，后者常被视为黑盒模型，但两者在理论上有深刻联系。

Conclusion: 理解克里金法与神经网络的关系并将两种视角结合，可以增强机器学习技术的可解释性、可靠性和空间感知能力，为空间统计学与AI的融合提供新思路。

Abstract: AI has impacted many disciplines and is nowadays ubiquitous. In particular, spatial statistics is in a pivotal moment where it will increasingly intertwine with AI. In this scenario, a relevant question is what relationship spatial statistics models have with machine learning (ML) models, if any. In particular, in this paper, we explore the connections between Kriging and neural networks. At first glance, they may appear unrelated. Kriging - and its ML counterpart, Gaussian process regression - are grounded in probability theory and stochastic processes, whereas many ML models are extensively considered Black-Box models. Nevertheless, they are strongly related. We study their connections and revisit the relevant literature. The understanding of their relations and the combination of both perspectives may enhance ML techniques by making them more interpretable, reliable, and spatially aware.

</details>


### [263] [USBD: Universal Structural Basis Distillation for Source-Free Graph Domain Adaptation](https://arxiv.org/abs/2602.08431)
*Yingxu Wang,Kunyu Zhang,Mengzhu Wang,Siyang Gao,Nan Yin*

Main category: cs.LG

TL;DR: USBD提出通用结构基蒸馏框架，通过构建结构无关的基来覆盖完整拓扑模式谱，解决图领域自适应中源模型结构偏差问题，显著提升对结构差异目标的适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有SF-GDA方法依赖源训练GNN的平滑先验，在拓扑结构显著变化时，源模型会将未见的结构模式误判为噪声，导致基于伪标签的自适应不可靠，成为结构差异场景下的关键瓶颈。

Method: 提出通用结构基蒸馏框架，采用双层优化将源数据集蒸馏为紧凑结构基，通过强制原型覆盖完整Dirichlet能量谱来捕获多样拓扑模式；推理时引入谱感知集成机制，基于目标图谱指纹动态激活最优原型组合。

Result: 在基准测试中显著优于现有方法，特别是在严重结构偏移场景下表现突出；同时通过解耦自适应成本与目标数据规模实现卓越计算效率。

Conclusion: USBD通过从适应有偏模型转向学习通用结构基的范式转变，有效解决了SF-GDA中的结构偏差问题，为处理拓扑结构显著不同的目标图提供了更鲁棒和高效的解决方案。

Abstract: SF-GDA is pivotal for privacy-preserving knowledge transfer across graph datasets. Although recent works incorporate structural information, they implicitly condition adaptation on the smoothness priors of sourcetrained GNNs, thereby limiting their generalization to structurally distinct targets. This dependency becomes a critical bottleneck under significant topological shifts, where the source model misinterprets distinct topological patterns unseen in the source domain as noise, rendering pseudo-label-based adaptation unreliable. To overcome this limitation, we propose the Universal Structural Basis Distillation, a framework that shifts the paradigm from adapting a biased model to learning a universal structural basis for SF-GDA. Instead of adapting a biased source model to a specific target, our core idea is to construct a structure-agnostic basis that proactively covers the full spectrum of potential topological patterns. Specifically, USBD employs a bi-level optimization framework to distill the source dataset into a compact structural basis. By enforcing the prototypes to span the full Dirichlet energy spectrum, the learned basis explicitly captures diverse topological motifs, ranging from low-frequency clusters to high-frequency chains, beyond those present in the source. This ensures that the learned basis creates a comprehensive structural covering capable of handling targets with disparate structures. For inference, we introduce a spectral-aware ensemble mechanism that dynamically activates the optimal prototype combination based on the spectral fingerprint of the target graph. Extensive experiments on benchmarks demonstrate that USBD significantly outperforms state-of-the-art methods, particularly in scenarios with severe structural shifts, while achieving superior computational efficiency by decoupling the adaptation cost from the target data scale.

</details>


### [264] [RIFLE: Robust Distillation-based FL for Deep Model Deployment on Resource-Constrained IoT Networks](https://arxiv.org/abs/2602.08446)
*Pouria Arefijamal,Mahdi Ahmadlou,Bardia Safaei,Jörg Henkel*

Main category: cs.LG

TL;DR: RIFLE：基于蒸馏的鲁棒联邦学习框架，用logit知识转移替代梯度共享，在资源受限的IoT环境中实现深度模型训练，同时提升对抗恶意攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习在IoT环境中面临两个主要挑战：1）TinyML模型在数据异构和任务复杂时难以捕捉复杂模式；2）对抗恶意客户端和中毒更新的鲁棒性不足。需要一种既能训练深度模型又能保证安全性的解决方案。

Method: 提出RIFLE框架：1）用logit-based知识转移替代梯度共享，通过知识蒸馏聚合方案训练深度模型；2）基于KL散度的验证机制量化客户端更新的可靠性，不暴露原始数据；3）在异构非IID条件下实现高效训练。

Result: 在MNIST、CIFAR-10、CIFAR-100数据集上，RIFLE在异构非IID条件下：1）减少87.5%的误报检测；2）提升62.5%的中毒攻击缓解能力；3）相比传统联邦学习基线获得28.3%的准确率提升；4）将VGG19训练时间从600多天缩短到1.39小时。

Conclusion: RIFLE通过logit知识转移和KL散度验证机制，成功解决了IoT环境中联邦学习的深度模型训练和安全性挑战，使深度学习在资源受限网络中变得实用可行。

Abstract: Federated learning (FL) is a decentralized learning paradigm widely adopted in resource-constrained Internet of Things (IoT) environments. These devices, typically relying on TinyML models, collaboratively train global models by sharing gradients with a central server while preserving data privacy. However, as data heterogeneity and task complexity increase, TinyML models often become insufficient to capture intricate patterns, especially under extreme non-IID (non-independent and identically distributed) conditions. Moreover, ensuring robustness against malicious clients and poisoned updates remains a major challenge. Accordingly, this paper introduces RIFLE - a Robust, distillation-based Federated Learning framework that replaces gradient sharing with logit-based knowledge transfer. By leveraging a knowledge distillation aggregation scheme, RIFLE enables the training of deep models such as VGG-19 and Resnet18 within constrained IoT systems. Furthermore, a Kullback-Leibler (KL) divergence-based validation mechanism quantifies the reliability of client updates without exposing raw data, achieving high trust and privacy preservation simultaneously. Experiments on three benchmark datasets (MNIST, CIFAR-10, and CIFAR-100) under heterogeneous non-IID conditions demonstrate that RIFLE reduces false-positive detections by up to 87.5%, enhances poisoning attack mitigation by 62.5%, and achieves up to 28.3% higher accuracy compared to conventional federated learning baselines within only 10 rounds. Notably, RIFLE reduces VGG19 training time from over 600 days to just 1.39 hours on typical IoT devices (0.3 GFLOPS), making deep learning practical in resource-constrained networks.

</details>


### [265] [Estimating Aleatoric Uncertainty in the Causal Treatment Effect](https://arxiv.org/abs/2602.08461)
*Liyuan Xu,Bijan Mazaheri*

Main category: cs.LG

TL;DR: 该论文提出了治疗效应方差(VTE)和条件治疗效应方差(CVTE)作为衡量治疗响应中固有随机不确定性的自然度量，证明了这些量在温和假设下可从观测数据中识别，并开发了非参数核估计器。


<details>
  <summary>Details</summary>
Motivation: 先前因果推断研究主要关注治疗效应的平均值和条件平均值，对个体治疗响应中的变异性和不确定性关注较少。本文旨在填补这一空白，关注治疗响应中的随机不确定性。

Method: 引入治疗效应方差(VTE)和条件治疗效应方差(CVTE)作为随机不确定性的度量，证明这些量在存在未观测混杂因素的情况下仍可识别。提出非参数核基估计器来估计VTE和CVTE，并进行理论收敛性分析。

Result: 理论分析建立了估计器的收敛性。在合成和半模拟数据集上的大量实证实验表明，该方法相比朴素基线方法表现出优越或相当的性能。

Conclusion: VTE和CVTE是衡量治疗响应中随机不确定性的自然度量，即使在存在未观测混杂因素的情况下也可识别。提出的非参数核估计器具有理论保证，并在实证实验中表现良好，为因果推断中的不确定性量化提供了新工具。

Abstract: Previous work on causal inference has primarily focused on averages and conditional averages of treatment effects, with significantly less attention on variability and uncertainty in individual treatment responses. In this paper, we introduce the variance of the treatment effect (VTE) and conditional variance of treatment effect (CVTE) as the natural measure of aleatoric uncertainty inherent in treatment responses, and we demonstrate that these quantities are identifiable from observed data under mild assumptions, even in the presence of unobserved confounders. We further propose nonparametric kernel-based estimators for VTE and CVTE, and our theoretical analysis establishes their convergence. We also test the performance of our method through extensive empirical experiments on both synthetic and semi-simulated datasets, where it demonstrates superior or comparable performance to naive baselines.

</details>


### [266] [Low Rank Transformer for Multivariate Time Series Anomaly Detection and Localization](https://arxiv.org/abs/2602.08467)
*Charalampos Shimillas,Kleanthis Malialis,Konstantinos Fokianos,Marios M. Polycarpou*

Main category: cs.LG

TL;DR: 提出ALoRa-T模型和ALoRa-Loc方法，通过低秩正则化自注意力和量化时间序列相互关系，显著提升多元时间序列异常检测与定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有多元时间序列异常诊断方法缺乏理论洞察，特别是异常定位这一关键但研究不足的领域。需要从理论角度理解Transformer在多元时间序列上的学习过程，并建立与统计时间序列方法的联系。

Method: 1. 理论分析Transformer在多元时间序列上的学习过程，揭示其与统计时间序列方法的联系；2. 提出ALoRa-T模型，对自注意力机制应用低秩正则化；3. 引入Attention Low-Rank score捕捉异常的时间特征；4. 提出ALoRa-Loc方法，通过量化时间序列间的相互关系实现异常变量定位。

Result: 大量实验和真实数据分析表明，所提方法在异常检测和定位任务上均显著优于现有最先进方法。

Conclusion: 通过理论洞察指导的ALoRa-T模型和ALoRa-Loc方法，有效解决了多元时间序列异常诊断中的检测和定位问题，为复杂系统的安全可靠性提供了有力工具。

Abstract: Multivariate time series (MTS) anomaly diagnosis, which encompasses both anomaly detection and localization, is critical for the safety and reliability of complex, large-scale real-world systems. The vast majority of existing anomaly diagnosis methods offer limited theoretical insights, especially for anomaly localization, which is a vital but largely unexplored area. The aim of this contribution is to study the learning process of a Transformer when applied to MTS by revealing connections to statistical time series methods. Based on these theoretical insights, we propose the Attention Low-Rank Transformer (ALoRa-T) model, which applies low-rank regularization to self-attention, and we introduce the Attention Low-Rank score, effectively capturing the temporal characteristics of anomalies. Finally, to enable anomaly localization, we propose the ALoRa-Loc method, a novel approach that associates anomalies to specific variables by quantifying interrelationships among time series. Extensive experiments and real data analysis, show that the proposed methodology significantly outperforms state-of-the-art methods in both detection and localization tasks.

</details>


### [267] [Learning Credal Ensembles via Distributionally Robust Optimization](https://arxiv.org/abs/2602.08470)
*Kaizheng Wang,Ghifari Adam Faza,Fabio Cuzzolin,Siu Lun Chau,David Moens,Hans Hallez*

Main category: cs.LG

TL;DR: CreDRO：一种基于分布鲁棒优化的信度预测器，通过训练多个在i.i.d.假设不同松弛下的模型来捕捉预测性认知不确定性，不仅考虑训练随机性，还考虑训练与测试数据间潜在分布偏移带来的不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有信度预测器主要将认知不确定性定义为由随机训练初始化引起的分歧，这主要反映了对优化随机性的敏感性，而非更深层次的不确定性来源。需要一种能捕捉训练与测试数据间潜在分布偏移带来的认知不确定性的方法。

Method: 提出CreDRO方法，将认知不确定性定义为在不同i.i.d.假设松弛下训练的模型之间的分歧。通过分布鲁棒优化学习一组合理的模型集合，捕捉来自训练随机性和潜在分布偏移的有意义分歧。

Result: CreDRO在多个基准测试中持续优于现有信度方法，在分布外检测任务和医疗应用中的选择性分类任务上表现优异。

Conclusion: CreDRO通过分布鲁棒优化有效捕捉了更全面的认知不确定性，不仅包括训练随机性，还包括训练与测试数据间潜在分布偏移带来的不确定性，提升了模型的鲁棒性和不确定性量化能力。

Abstract: Credal predictors are models that are aware of epistemic uncertainty and produce a convex set of probabilistic predictions. They offer a principled way to quantify predictive epistemic uncertainty (EU) and have been shown to improve model robustness in various settings. However, most state-of-the-art methods mainly define EU as disagreement caused by random training initializations, which mostly reflects sensitivity to optimization randomness rather than uncertainty from deeper sources. To address this, we define EU as disagreement among models trained with varying relaxations of the i.i.d. assumption between training and test data. Based on this idea, we propose CreDRO, which learns an ensemble of plausible models through distributionally robust optimization. As a result, CreDRO captures EU not only from training randomness but also from meaningful disagreement due to potential distribution shifts between training and test data. Empirical results show that CreDRO consistently outperforms existing credal methods on tasks such as out-of-distribution detection across multiple benchmarks and selective classification in medical applications.

</details>


### [268] [Time-Delayed Transformers for Data-Driven Modeling of Low-Dimensional Dynamics](https://arxiv.org/abs/2602.08478)
*Albert Alcalde,Markus Widhalm,Emre Yılmaz*

Main category: cs.LG

TL;DR: TD-TF是一种简化的Transformer架构，用于非定常时空动力学的数据驱动建模，将线性算子方法与深度序列模型联系起来，在保持线性模型可解释性和效率的同时，显著提升非线性系统的表达能力。


<details>
  <summary>Details</summary>
Motivation: 传统线性算子方法（如TD-DMD）在处理非线性系统时表达能力有限，而复杂深度模型计算成本高且缺乏可解释性。需要一种既能处理非线性动力学，又保持线性模型效率和可解释性的方法。

Method: 提出时间延迟Transformer（TD-TF），采用极简架构：单层单头自注意力层（每个预测一个查询）加一个前馈层。该架构可解释为时间延迟动态模态分解（TD-DMD）的非线性推广，具有序列长度的线性计算复杂度和少量参数。

Result: 在近线性系统上，TD-TF与强线性基线性能相当；在非线性和混沌系统中，显著优于线性方法，能准确捕捉长期动力学。在合成信号、非定常空气动力学、Lorenz '63系统和反应扩散模型上的验证表明，TD-TF在保持可解释性和效率的同时，大幅提升了复杂动力学的表达能力。

Conclusion: TD-TF成功桥接了线性算子方法和深度序列模型，提供了一种高效、可解释且表达能力强的非定常时空动力学建模方法，在保持线性模型优点的同时，显著提升了处理非线性系统的能力。

Abstract: We propose the time-delayed transformer (TD-TF), a simplified transformer architecture for data-driven modeling of unsteady spatio-temporal dynamics. TD-TF bridges linear operator-based methods and deep sequence models by showing that a single-layer, single-head transformer can be interpreted as a nonlinear generalization of time-delayed dynamic mode decomposition (TD-DMD). The architecture is deliberately minimal, consisting of one self-attention layer with a single query per prediction and one feedforward layer, resulting in linear computational complexity in sequence length and a small parameter count. Numerical experiments demonstrate that TD-TF matches the performance of strong linear baselines on near-linear systems, while significantly outperforming them in nonlinear and chaotic regimes, where it accurately captures long-term dynamics. Validation studies on synthetic signals, unsteady aerodynamics, the Lorenz '63 system, and a reaction-diffusion model show that TD-TF preserves the interpretability and efficiency of linear models while providing substantially enhanced expressive power for complex dynamics.

</details>


### [269] [Beyond Correctness: Learning Robust Reasoning via Transfer](https://arxiv.org/abs/2602.08489)
*Hyunseok Lee,Soheil Abbasloo,Jihoon Tack,Jinwoo Shin*

Main category: cs.LG

TL;DR: RLTR提出了一种新的强化学习方法，通过可转移奖励来增强LLM推理过程的鲁棒性，相比仅关注最终答案正确性的RLVR，RLTR能产生更稳定、可解释且可泛化的推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法只关注最终答案的正确性，忽略了推理过程本身的鲁棒性。作者认为稳健的推理应该具有可转移性，即使推理过程被截断、重新解释或继续，仍能保持其有效性。

Method: 提出RLTR（Reinforcement Learning with Transferable Reward），通过转移奖励来操作化鲁棒性：测试从一个模型提取的部分推理前缀是否能指导另一个模型得到正确答案。这种方法鼓励LLM产生稳定、可解释且真正可泛化的推理。

Result: RLTR在提高采样一致性的同时改善了最终答案准确率，并且用更少的训练步骤达到可比性能。在MATH500上，RLTR相比RLVR获得+3.6%p的Maj@64提升，并且用约2.5倍更少的训练步骤匹配RLVR的平均准确率。

Conclusion: RLTR方法不仅提供了更可靠的推理过程，还显著提高了样本效率，为LLM推理的鲁棒性提供了新的强化学习框架。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently strengthened LLM reasoning, but its focus on final answer correctness leaves a critical gap: it does not ensure the robustness of the reasoning process itself. We adopt a simple philosophical view, robust reasoning should remain useful beyond the mind that produced it, and treat reasoning as a form of meaning transfer that must survive truncation, reinterpretation, and continuation. Building on this principle, we introduce Reinforcement Learning with Transferable Reward (RLTR), which operationalizes robustness via transfer reward that tests whether a partial reasoning prefix from one model can guide a separate model to the correct answer. This encourages LLMs to produce reasoning that is stable, interpretable, and genuinely generalizable. Our approach improves sampling consistency while improving final answer accuracy, and it reaches comparable performance in substantially fewer training steps. For example, on MATH500, RLTR achieves a +3.6%p gain in Maj@64 compared to RLVR and matches RLVR's average accuracy with roughly 2.5x fewer training steps, providing both more reliable reasoning and significantly more sample efficient.

</details>


### [270] [Contextual Rollout Bandits for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.08499)
*Xiaodong Lu,Xiaohan Wang,Jiajun Chai,Guojun Yin,Wei Lin,Zhijun Chen,Yu Luo,Fuzhen Zhuang,Yikun Ban,Deqing Wang*

Main category: cs.LG

TL;DR: 提出一种基于上下文老虎机的RLVR调度框架，自适应选择高质量rollout，提高训练效率和性能


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在rollout使用上存在两个问题：1) 对同一提示下质量不同的响应采用统一处理，导致噪声监督；2) 历史rollout仅使用一次即丢弃，导致样本效率低下和次优策略更新

Method: 将RLVR中的rollout调度建模为上下文老虎机问题，提出统一的神经调度框架。每个rollout被视为一个臂，其奖励定义为连续优化步骤间的性能增益提升。该调度器支持噪声感知的组内选择和历史rollout的自适应全局重用

Result: 在六个数学推理基准测试中，该方法在多个RLVR优化方法上均表现出性能提升和训练效率提高

Conclusion: 通过将rollout调度形式化为上下文老虎机问题，提出的调度框架能够自适应选择高价值rollout，有效解决了现有RLVR方法的噪声监督和样本效率问题，在理论和实验上都验证了其有效性

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is an effective paradigm for improving the reasoning capabilities of large language models. However, existing RLVR methods utilize rollouts in an indiscriminate and short-horizon manner: responses of heterogeneous quality within each prompt are treated uniformly, and historical rollouts are discarded after a single use. This leads to noisy supervision, poor sample efficiency, and suboptimal policy updates. We address these issues by formulating rollout scheduling in RLVR as a contextual bandit problem and proposing a unified neural scheduling framework that adaptively selects high-value rollouts throughout training. Each rollout is treated as an arm whose reward is defined by the induced performance gain between consecutive optimization steps. The resulting scheduler supports both noise-aware intra-group selection and adaptive global reuse of historical rollouts within a single principled framework. We provide theoretical justification by deriving sublinear regret bounds and showing that enlarging the rollout buffer improves the achievable performance upper bound. Experiments on six mathematical reasoning benchmarks demonstrate consistent gains in performance and training efficiency across multiple RLVR optimization methods.

</details>


### [271] [Is Meta-Path Attention an Explanation? Evidence of Alignment and Decoupling in Heterogeneous GNNs](https://arxiv.org/abs/2602.08500)
*Maiqi Jiang,Noman Ali,Yiran Ding,Yanfu Zhang*

Main category: cs.LG

TL;DR: 该研究实证分析了异构图神经网络中元路径注意力是否真实反映元路径重要性，提出了MetaXplain解释协议和MP-AEA对齐度量，发现注意力与重要性存在对齐和脱钩两种状态。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证元路径注意力机制是否真实反映元路径重要性，因为现有解释器主要针对同构图设计，无法有效处理异构图中的语义混合问题。

Method: 提出MetaXplain解释协议，包含视图分解解释、模式有效通道扰动和融合感知归因三个组件，并设计MP-AEA度量来量化注意力与解释的对齐程度。

Result: 元路径感知解释器通常优于随机基线；MP-AEA显示注意力与重要性存在高对齐和显著脱钩两种状态；基于解释诱导子图重训练能保持甚至提升预测性能。

Conclusion: 元路径注意力并不总是反映元路径重要性，存在脱钩现象；提出的解释协议能有效评估注意力可靠性；解释具有去噪效果，可用于提升模型鲁棒性。

Abstract: Meta-path-based heterogeneous graph neural networks aggregate over meta-path-induced views, and their semantic-level attention over meta-path channels is widely used as a narrative for ``which semantics matter.'' We study this assumption empirically by asking: when does meta-path attention reflect meta-path importance, and when can it decouple? A key challenge is that most post-hoc GNN explainers are designed for homogeneous graphs, and naive adaptations to heterogeneous neighborhoods can mix semantics and confound perturbations. To enable a controlled empirical analysis, we introduce MetaXplain, a meta-path-aware post-hoc explanation protocol that applies existing explainers in the native meta-path view domain via (i) view-factorized explanations, (ii) schema-valid channel-wise perturbations, and (iii) fusion-aware attribution, without modifying the underlying predictor. We benchmark representative gradient-, perturbation-, and Shapley-style explainers on ACM, DBLP, and IMDB with HAN and HAN-GCN, comparing against xPath and type-matched random baselines under standard faithfulness metrics. To quantify attention reliability, we propose Meta-Path Attention--Explanation Alignment (MP-AEA), which measures rank correlation between learned attention weights and explanation-derived meta-path contribution scores across random runs. Our results show that meta-path-aware explanations typically outperform random controls, while MP-AEA reveals both high-alignment and statistically significant decoupling regimes depending on the dataset and backbone; moreover, retraining on explanation-induced subgraphs often preserves, and in some noisy regimes improves, predictive performance, suggesting an explanation-as-denoising effect.

</details>


### [272] [Bridging Academia and Industry: A Comprehensive Benchmark for Attributed Graph Clustering](https://arxiv.org/abs/2602.08519)
*Yunhui Liu,Pengyu Qiu,Yu Xing,Yongchao Liu,Peng Du,Chuntao Hong,Jiajun Zheng,Tao Zheng,Tieke He*

Main category: cs.LG

TL;DR: PyAGC是一个面向工业部署的图聚类基准测试库，解决了当前学术研究与实际应用之间的差距，提供可扩展的mini-batch实现和多样化数据集。


<details>
  <summary>Details</summary>
Motivation: 当前图聚类研究存在三大问题：1) 使用小规模、高同质性引文数据集；2) 依赖不可扩展的全批次训练；3) 使用有监督指标评估，无法反映标签稀缺环境下的真实性能。学术研究与工业部署之间存在显著鸿沟。

Method: 1) 提出模块化的Encode-Cluster-Optimize框架统一现有方法；2) 首次为多种先进AGC算法提供内存高效的mini-batch实现；3) 构建包含12个多样化数据集（2.7K-111M节点）的基准测试，特别包含工业级复杂表格特征和低同质性图；4) 提出包含无监督结构指标和效率分析的综合评估协议。

Result: 开发了PyAGC库，已在蚂蚁集团高风险工业工作流中验证，提供公开可用的代码、资源和文档，为社区提供稳健、可复现、可扩展的AGC研究平台。

Conclusion: PyAGC填补了图聚类研究从学术到工业部署的鸿沟，通过提供生产就绪的基准测试库，推动AGC研究向实际应用方向发展。

Abstract: Attributed Graph Clustering (AGC) is a fundamental unsupervised task that integrates structural topology and node attributes to uncover latent patterns in graph-structured data. Despite its significance in industrial applications such as fraud detection and user segmentation, a significant chasm persists between academic research and real-world deployment. Current evaluation protocols suffer from the small-scale, high-homophily citation datasets, non-scalable full-batch training paradigms, and a reliance on supervised metrics that fail to reflect performance in label-scarce environments. To bridge these gaps, we present PyAGC, a comprehensive, production-ready benchmark and library designed to stress-test AGC methods across diverse scales and structural properties. We unify existing methodologies into a modular Encode-Cluster-Optimize framework and, for the first time, provide memory-efficient, mini-batch implementations for a wide array of state-of-the-art AGC algorithms. Our benchmark curates 12 diverse datasets, ranging from 2.7K to 111M nodes, specifically incorporating industrial graphs with complex tabular features and low homophily. Furthermore, we advocate for a holistic evaluation protocol that mandates unsupervised structural metrics and efficiency profiling alongside traditional supervised metrics. Battle-tested in high-stakes industrial workflows at Ant Group, this benchmark offers the community a robust, reproducible, and scalable platform to advance AGC research towards realistic deployment. The code and resources are publicly available via GitHub (https://github.com/Cloudy1225/PyAGC), PyPI (https://pypi.org/project/pyagc), and Documentation (https://pyagc.readthedocs.io).

</details>


### [273] [Causal Schrödinger Bridges: Constrained Optimal Transport on Structural Manifolds](https://arxiv.org/abs/2602.08535)
*Rui Wu,Li YongJun*

Main category: cs.LG

TL;DR: 提出Causal Schrödinger Bridge (CSB)框架，将反事实推理重构为熵最优传输问题，通过扩散过程在支持集不匹配时稳健地"穿越"低密度区域，优于确定性方法。


<details>
  <summary>Details</summary>
Motivation: 传统生成建模使用确定性流（ODE）寻找最小作用路径，但在因果干预下变得脆弱，因为需要跨过低密度区域（"离流形"）传输概率质量，而该区域的向量场定义不清，导致数值不稳定和虚假相关性。

Method: 引入Causal Schrödinger Bridge (CSB)框架，将反事实推理重构为熵最优传输问题。与需要严格可逆性的确定性方法不同，CSB利用扩散过程（SDEs）在支持集不匹配时稳健地"穿越"低密度区域，同时严格强制执行结构可接受性约束。证明了结构分解定理，表明全局高维桥可分解为局部稳健的转移。

Result: 在高维干预（Morpho-MNIST）上的实证验证表明，CSB在结构一致性方面显著优于确定性基线方法，特别是在强、分布外处理的机制中表现更优。

Conclusion: CSB为因果推理提供了一种稳健的框架，通过扩散过程处理支持集不匹配问题，在反事实推理中比确定性方法更可靠，特别是在需要跨低密度区域传输概率质量的情况下。

Abstract: Generative modeling typically seeks the path of least action via deterministic flows (ODE). While effective for in-distribution tasks, we argue that these deterministic paths become brittle under causal interventions, which often require transporting probability mass across low-density regions ("off-manifold") where the vector field is ill-defined. This leads to numerical instability and spurious correlations. In this work, we introduce the Causal Schrödinger Bridge (CSB), a framework that reformulates counterfactual inference as Entropic Optimal Transport. Unlike deterministic approaches that require strict invertibility, CSB leverages diffusion processes (SDEs) to robustly "tunnel" through support mismatches while strictly enforcing structural admissibility constraints. We prove the Structural Decomposition Theorem, showing that the global high-dimensional bridge factorizes into local, robust transitions. Empirical validation on high-dimensional interventions (Morpho-MNIST) demonstrates that CSB significantly outperforms deterministic baselines in structural consistency, particularly in regimes of strong, out-of-distribution treatments.

</details>


### [274] [Rho-Perfect: Correlation Ceiling For Subjective Evaluation Datasets](https://arxiv.org/abs/2602.08552)
*Fredrik Cumlin*

Main category: cs.LG

TL;DR: 提出ρ-Perfect方法，用于估计主观评分数据集上模型能达到的最高相关性，量化数据可靠性问题


<details>
  <summary>Details</summary>
Motivation: 主观评分存在固有噪声，限制了模型与人类的相关性，但这种可靠性问题很少被量化

Method: 定义ρ-Perfect为完美预测器与人类评分之间的相关性，基于异方差噪声场景推导估计值，使用重测相关性验证

Result: 在语音质量数据集上演示ρ-Perfect的应用，显示该指标能区分模型限制与数据质量问题

Conclusion: ρ-Perfect提供了实用的最高可达到相关性估计，有助于评估模型性能和数据质量

Abstract: Subjective ratings contain inherent noise that limits the model-human correlation, but this reliability issue is rarely quantified. In this paper, we present $ρ$-Perfect, a practical estimation of the highest achievable correlation of a model on subjectively rated datasets. We define $ρ$-Perfect to be the correlation between a perfect predictor and human ratings, and derive an estimate of the value based on heteroscedastic noise scenarios, a common occurrence in subjectively rated datasets. We show that $ρ$-Perfect squared estimates test-retest correlation and use this to validate the estimate. We demonstrate the use of $ρ$-Perfect on a speech quality dataset and show how the measure can distinguish between model limitations and data quality issues.

</details>


### [275] [Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs](https://arxiv.org/abs/2602.08563)
*Ahmed Salem,Andrew Paverd,Sahar Abdelnabi*

Main category: cs.LG

TL;DR: 论文提出"隐式记忆"概念，挑战LLMs无状态的假设，展示模型可通过输出编码信息并在后续交互中恢复，实现跨交互的持久信息通道，并引入"定时炸弹"这一新型时间后门攻击。


<details>
  <summary>Details</summary>
Motivation: 挑战当前将大语言模型视为无状态的普遍假设，探索模型是否能够通过自身输出编码信息并在后续交互中恢复，从而创建跨推理请求的持久信息通道。

Method: 引入"隐式记忆"概念，通过提示工程或微调使模型在输出中编码信息，后续当这些输出重新作为输入时恢复信息。具体演示了"定时炸弹"这一新型时间后门攻击，其激活需要满足通过隐式记忆累积的隐藏条件序列。

Result: 证明隐式记忆行为可通过简单提示或微调实现，展示了定时炸弹攻击的可行性，并分析了隐式记忆在隐蔽跨代理通信、基准污染、定向操纵和训练数据污染等方面的广泛影响。

Conclusion: 隐式记忆挑战了LLMs无状态的基本假设，具有重要的安全影响。需要开发检测方法、压力测试和评估框架来应对这一新兴威胁，并发布了代码和数据以促进未来研究。

Abstract: Large language models (LLMs) are commonly treated as stateless: once an interaction ends, no information is assumed to persist unless it is explicitly stored and re-supplied. We challenge this assumption by introducing implicit memory-the ability of a model to carry state across otherwise independent interactions by encoding information in its own outputs and later recovering it when those outputs are reintroduced as input. This mechanism does not require any explicit memory module, yet it creates a persistent information channel across inference requests. As a concrete demonstration, we introduce a new class of temporal backdoors, which we call time bombs. Unlike conventional backdoors that activate on a single trigger input, time bombs activate only after a sequence of interactions satisfies hidden conditions accumulated via implicit memory. We show that such behavior can be induced today through straightforward prompting or fine-tuning. Beyond this case study, we analyze broader implications of implicit memory, including covert inter-agent communication, benchmark contamination, targeted manipulation, and training-data poisoning. Finally, we discuss detection challenges and outline directions for stress-testing and evaluation, with the goal of anticipating and controlling future developments. To promote future research, we release code and data at: https://github.com/microsoft/implicitMemory.

</details>


### [276] [M-Loss: Quantifying Model Merging Compatibility with Limited Unlabeled Data](https://arxiv.org/abs/2602.08564)
*Tiantong Wang,Yiyang Duan,Haoyu Chen,Tiantong Wu,Wei Yang Bryan Lim*

Main category: cs.LG

TL;DR: 提出M-Loss评估指标，用于量化模型合并的兼容性，通过测量参数平均与模型集成之间的差异，指导更有效的模型合并策略。


<details>
  <summary>Details</summary>
Motivation: 大规模模型训练计算密集且受限于标注数据。模型合并提供了一种无需额外数据或大量训练的替代方案，但传统参数平均方法容易组合不可泛化的特征，而模型集成虽然性能更稳定但推理成本和存储需求更高。现有研究缺乏理论证据和评估指标来指导模型合并。

Method: 提出Merging-ensembling loss (M-Loss)评估指标，使用少量无标签数据量化模型合并的兼容性。通过测量参数平均与模型集成在层和节点级别的差异，指导模型合并策略。M-Loss既作为模型合并理论可行性的定量标准，也作为模型剪枝中参数重要性的指导。

Result: 理论分析和实证评估表明，将M-Loss纳入合并过程能显著提高合并模型与模型集成之间的对齐度，为准确模型整合提供了可扩展且高效的框架。

Conclusion: M-Loss填补了模型合并与集成之间理论证据和评估指标的空白，提供了一种量化模型合并兼容性的方法，能够指导更有效的模型合并策略，实现更准确和高效的模型整合。

Abstract: Training of large-scale models is both computationally intensive and often constrained by the availability of labeled data. Model merging offers a compelling alternative by directly integrating the weights of multiple source models without requiring additional data or extensive training. However, conventional model merging techniques, such as parameter averaging, often suffer from the unintended combination of non-generalizable features, especially when source models exhibit significant weight disparities. Comparatively, model ensembling generally provides more stable and superior performance that aggregates multiple models by averaging outputs. However, it incurs higher inference costs and increased storage requirements. While previous studies experimentally showed the similarities between model merging and ensembling, theoretical evidence and evaluation metrics remain lacking. To address this gap, we introduce Merging-ensembling loss (M-Loss), a novel evaluation metric that quantifies the compatibility of merging source models using very limited unlabeled data. By measuring the discrepancy between parameter averaging and model ensembling at layer and node levels, M-Loss facilitates more effective merging strategies. Specifically, M-Loss serves both as a quantitative criterion of the theoretical feasibility of model merging, and a guide for parameter significance in model pruning. Our theoretical analysis and empirical evaluations demonstrate that incorporating M-Loss into the merging process significantly improves the alignment between merged models and model ensembling, providing a scalable and efficient framework for accurate model consolidation.

</details>


### [277] [An arithmetic method algorithm optimizing k-nearest neighbors compared to regression algorithms and evaluated on real world data sources](https://arxiv.org/abs/2602.08577)
*Theodoros Anagnostopoulos,Evanthia Zervoudi,Christos Anagnostopoulos,Apostolos Christopoulos,Bogdan Wierzbinski*

Main category: cs.LG

TL;DR: 提出一种基于算术方法的k-NN优化算法AMR，通过引入算术方法解决多元线性方程，在多个真实数据集上表现优于传统k-NN，与其他回归算法性能相当。


<details>
  <summary>Details</summary>
Motivation: k-NN是常用的非参数回归算法，但在性能上有优化空间。研究旨在通过引入一种能处理任意数量实变量的线性方程的算术方法，来优化k-NN算法的性能。

Method: 提出算术方法算法AMA评估算术方法效率，并基于AMA开发算术方法回归算法AMR作为k-NN的优化版本。使用引入的最优推断决策规则与其他回归算法比较，并在公开的真实世界数据集上进行评估。

Result: AMR算法在大多数情况下比k-NN表现更好，与其他回归算法性能相当。结果表明AMR确实是对k-NN的有效优化。

Conclusion: 提出的AMR算法成功优化了k-NN回归算法，通过算术方法提升了性能，在真实数据集上表现出有竞争力的结果。

Abstract: Linear regression analysis focuses on predicting a numeric regressand value based on certain regressor values. In this context, k-Nearest Neighbors (k-NN) is a common non-parametric regression algorithm, which achieves efficient performance when compared with other algorithms in literature. In this research effort an optimization of the k-NN algorithm is proposed by exploiting the potentiality of an introduced arithmetic method, which can provide solutions for linear equations involving an arbitrary number of real variables. Specifically, an Arithmetic Method Algorithm (AMA) is adopted to assess the efficiency of the introduced arithmetic method, while an Arithmetic Method Regression (AMR) algorithm is proposed as an optimization of k-NN adopting the potentiality of AMA. Such algorithm is compared with other regression algorithms, according to an introduced optimal inference decision rule, and evaluated on certain real world data sources, which are publicly available. Results are promising since the proposed AMR algorithm has comparable performance with the other algorithms, while in most cases it achieves better performance than the k-NN. The output results indicate that introduced AMR is an optimization of k-NN.

</details>


### [278] [Modeling Score Approximation Errors in Diffusion Models via Forward SPDEs](https://arxiv.org/abs/2602.08579)
*Junsu Seo*

Main category: cs.LG

TL;DR: 该研究通过将分数估计误差视为驱动Fokker-Planck方程的随机源，从SPDE框架分析基于分数的生成模型动态，提出几何稳定性和位移凸性视角，并引入基于SPDE解二次变分的评估指标。


<details>
  <summary>Details</summary>
Motivation: 传统基于粒子的SDE分析存在局限性，需要从概率密度场演化的角度理解基于分数的生成模型动态，特别是分数估计误差对生成过程的影响。

Method: 采用随机偏微分方程框架，将分数估计误差建模为Fokker-Planck方程的随机驱动源，在简化设置下分析几何稳定性和位移凸性，并引入基于SPDE解二次变分的评估指标。

Result: 初步观察表明，提出的评估指标仅需采样轨迹前10%的数据即可保持有效性，显示出计算效率潜力；从几何稳定性和位移凸性角度解释了生成模型的鲁棒性。

Conclusion: SPDE框架为分析基于分数的生成模型提供了新视角，提出的评估指标具有计算效率优势，几何稳定性和位移凸性分析有助于理解生成模型的鲁棒性机制。

Abstract: This study investigates the dynamics of Score-based Generative Models (SGMs) by treating the score estimation error as a stochastic source driving the Fokker-Planck equation. Departing from particle-centric SDE analyses, we employ an SPDE framework to model the evolution of the probability density field under stochastic drift perturbations. Under a simplified setting, we utilize this framework to interpret the robustness of generative models through the lens of geometric stability and displacement convexity. Furthermore, we introduce a candidate evaluation metric derived from the quadratic variation of the SPDE solution projected onto a radial test function. Preliminary observations suggest that this metric remains effective using only the initial 10% of the sampling trajectory, indicating a potential for computational efficiency.

</details>


### [279] [Conditional Sequence Modeling for Safe Reinforcement Learning](https://arxiv.org/abs/2602.08584)
*Wensong Bai,Chao Zhang,Qihang Xu,Chufan Chen,Chenhao Zhou,Hui Qian*

Main category: cs.LG

TL;DR: RCDT是一种基于条件序列建模的离线安全强化学习方法，支持在单一训练策略下零样本适应不同成本阈值，通过拉格朗日式成本惩罚和自适应惩罚系数实现更好的回报-成本权衡。


<details>
  <summary>Details</summary>
Motivation: 现有离线安全RL方法通常在预定义成本阈值下训练，导致策略泛化能力有限且部署灵活性不足。实际部署中不同场景需要不同的成本阈值，因此需要能够零样本适应多种阈值的单一策略。

Method: 提出RCDT方法：1) 基于条件序列建模框架；2) 集成拉格朗日式成本惩罚与自适应惩罚系数；3) 引入回报-成本感知的轨迹重加权机制；4) 加入Q值正则化以避免过度保守行为。

Result: 在DSRL基准测试中，RCDT相比代表性基线方法，在回报-成本权衡方面取得了一致的改进，推动了离线安全RL的最新技术水平。

Conclusion: RCDT是首个基于条件序列建模的离线安全RL算法，能够通过单一训练策略实现跨多个成本阈值的零样本部署，为实际场景中的灵活部署提供了有效解决方案。

Abstract: Offline safe reinforcement learning (RL) aims to learn policies from a fixed dataset while maximizing performance under cumulative cost constraints. In practice, deployment requirements often vary across scenarios, necessitating a single policy that can adapt zero-shot to different cost thresholds. However, most existing offline safe RL methods are trained under a pre-specified threshold, yielding policies with limited generalization and deployment flexibility across cost thresholds. Motivated by recent progress in conditional sequence modeling (CSM), which enables flexible goal-conditioned control by specifying target returns, we propose RCDT, a CSM-based method that supports zero-shot deployment across multiple cost thresholds within a single trained policy. RCDT is the first CSM-based offline safe RL algorithm that integrates a Lagrangian-style cost penalty with an auto-adaptive penalty coefficient. To avoid overly conservative behavior and achieve a more favorable return--cost trade-off, a reward--cost-aware trajectory reweighting mechanism and Q-value regularization are further incorporated. Extensive experiments on the DSRL benchmark demonstrate that RCDT consistently improves return--cost trade-offs over representative baselines, advancing the state-of-the-art in offline safe RL.

</details>


### [280] [Predicting Future Utility: Global Combinatorial Optimization for Task-Agnostic KV Cache Eviction](https://arxiv.org/abs/2602.08585)
*Ziyao Tang,Pengkun Jiao,Xinhang Chen,Wei Liu,Shiyong Li,Jingjing Chen*

Main category: cs.LG

TL;DR: LU-KV：基于边际效用的KV缓存淘汰框架，通过凸包松弛和贪婪求解器优化头级预算分配，在LongBench和RULER基准测试中实现80% KV缓存减少且性能损失最小。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存淘汰方法依赖瞬时启发式指标，假设所有注意力头的分数大小是重要性的一致代理，但忽略了不同注意力头在预测保真度上的异质性。某些头关注token的瞬时贡献，而其他头则专注于捕捉长期效用。

Method: 提出LU-KV框架：1) 基于边际效用优化头级预算分配；2) 使用凸包松弛和基于边际效用的贪婪求解器实现近似最优精度；3) 实现数据驱动的离线分析协议以支持实际部署。

Result: 在LongBench和RULER基准测试中，LU-KV实现了80%的KV缓存大小减少，同时性能下降最小，并降低了推理延迟和GPU内存占用。

Conclusion: 通过考虑注意力头的异质性和基于边际效用的预算分配，LU-KV提供了一种高效的KV缓存淘汰解决方案，在保持模型性能的同时显著减少计算和内存开销。

Abstract: Given the quadratic complexity of attention, KV cache eviction is vital to accelerate model inference. Current KV cache eviction methods typically rely on instantaneous heuristic metrics, implicitly assuming that score magnitudes are consistent proxies for importance across all heads. However, this overlooks the heterogeneity in predictive fidelity across attention heads. While certain heads prioritize the instantaneous contribution of tokens, others are dedicated to capturing long-horizon utility. In this paper, we propose that optimal budget allocation should be governed by the marginal utility in preserving long-term semantic information. Based on this insight, we propose LU-KV, a novel framework that optimizes head-level budget allocation through a convex-hull relaxation and a marginal-utility-based greedy solver to achieve near-optimal precision. Furthermore, we implement a data-driven offline profiling protocol to facilitate the practical deployment of LU-KV. Extensive evaluations on LongBench and RULER benchmarks demonstrate that LU-KV achieves an 80% reduction in KV cache size with minimal performance degradation, while simultaneously reducing inference latency and GPU memory footprint.

</details>


### [281] [FairRARI: A Plug and Play Framework for Fairness-Aware PageRank](https://arxiv.org/abs/2602.08589)
*Emmanouil Kariotakis,Aritra Konar*

Main category: cs.LG

TL;DR: FairRARI是一个统一的凸优化框架，用于计算满足不同群体公平性标准的PageRank向量，确保达到目标公平水平，同时保持与原始PR算法相同的时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 随着算法公平性日益重要，需要计算满足基于顶点敏感属性的群体公平性标准的PageRank向量。目前缺乏原则性算法：有些无法保证达到目标公平水平，有些没有最优性保证。

Method: 提出统一的in-processing凸优化框架FairRARI，利用PageRank的变分公式，通过求解带有公平性约束的强凸优化问题来计算公平PR向量。框架支持三种不同的公平性标准，以"即插即用"方式处理。

Result: FairRARI在真实数据集上的实验表明，它在效用方面优于现有方法，同时在多个顶点群体中达到期望的公平水平。该框架具有与原始PR算法相同的渐近时间复杂度。

Conclusion: FairRARI是一个有效的统一框架，能够计算满足不同群体公平性标准的PageRank向量，确保达到目标公平水平，同时保持计算效率。

Abstract: PageRank (PR) is a fundamental algorithm in graph machine learning tasks. Owing to the increasing importance of algorithmic fairness, we consider the problem of computing PR vectors subject to various group-fairness criteria based on sensitive attributes of the vertices. At present, principled algorithms for this problem are lacking - some cannot guarantee that a target fairness level is achieved, while others do not feature optimality guarantees. In order to overcome these shortcomings, we put forth a unified in-processing convex optimization framework, termed FairRARI, for tackling different group-fairness criteria in a ``plug and play'' fashion. Leveraging a variational formulation of PR, the framework computes fair PR vectors by solving a strongly convex optimization problem with fairness constraints, thereby ensuring that a target fairness level is achieved. We further introduce three different fairness criteria which can be efficiently tackled using FairRARI to compute fair PR vectors with the same asymptotic time-complexity as the original PR algorithm. Extensive experiments on real-world datasets showcase that FairRARI outperforms existing methods in terms of utility, while achieving the desired fairness levels across multiple vertex groups; thereby highlighting its effectiveness.

</details>


### [282] [SDFed: Bridging Local Global Discrepancy via Subspace Refinement and Divergence Control in Federated Prompt Learning](https://arxiv.org/abs/2602.08590)
*Yicheng Di,Wei Yuan,Tieke He,Zhanjie Zhang,Ao Ma,Yuan Liu,Hongzhi Yin*

Main category: cs.LG

TL;DR: SDFed是一个异构联邦提示学习框架，通过子空间细化和分歧控制解决客户端异质性下的本地-全局差异问题


<details>
  <summary>Details</summary>
Motivation: 现有的联邦提示学习方法通常强制所有客户端使用统一的提示结构和长度，这在数据分布和系统资源存在异质性的实际场景中不足，且可能引入全局共享知识和本地最优知识之间的冲突

Method: SDFed采用异构联邦提示学习框架：保持固定长度的全局提示用于高效聚合，同时允许每个客户端学习可变长度的本地提示以适应其数据特性和容量；引入本地提示的子空间细化方法，以及信息保留和分歧控制策略，在保持关键本地信息的同时维持全局与本地表示之间的适当分离性

Result: 在多个数据集上的广泛实验表明，SDFed在异构联邦设置中持续提升性能和鲁棒性

Conclusion: SDFed通过解决客户端异质性下的本地-全局差异问题，为隐私敏感的多方设置中的视觉语言预训练模型适应提供了有效的解决方案

Abstract: Vision-language pretrained models offer strong transferable representations, yet adapting them in privacy-sensitive multi-party settings is challenging due to the high communication cost of federated optimization and the limited local data on clients. Federated prompt learning mitigates this issue by keeping the VLPM backbone frozen and collaboratively training lightweight prompt parameters. However, existing approaches typically enforce a unified prompt structure and length across clients, which is inadequate under practical client heterogeneity in both data distributions and system resources, and may further introduce conflicts between globally shared and locally optimal knowledge. To address these challenges, we propose \textbf{SDFed}, a heterogeneous federated prompt learning framework that bridges Local-Global Discrepancy via Subspace Refinement and Divergence Control. SDFed maintains a fixed-length global prompt for efficient aggregation while allowing each client to learn a variable-length local prompt to better match its data characteristics and capacity. To mitigate local-global conflicts and facilitate effective knowledge transfer, SDFed introduces a subspace refinement method for local prompts and an information retention and divergence control strategy that preserves key local information while maintaining appropriate separability between global and local representations. Extensive experiments on several datasets demonstrate that SDFed consistently improves performance and robustness in heterogeneous federated settings.

</details>


### [283] [TFMLinker: Universal Link Predictor by Graph In-Context Learning with Tabular Foundation Models](https://arxiv.org/abs/2602.08592)
*Tianyin Liao,Chunyu Hu,Yicheng Sui,Xingxuan Zhang,Peng Cui,Jianxin Li,Ziwei Zhang*

Main category: cs.LG

TL;DR: TFMLinker：利用表格基础模型的上下文学习能力进行跨图链接预测，无需特定数据集微调


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型在链接预测中存在预训练规模有限或过度依赖文本信息的问题，而表格基础模型在跨表格数据集通用预测方面表现出色，因此探索将其应用于链接预测

Method: 1）原型增强的局部-全局上下文模块构建上下文；2）通用拓扑感知链接编码器捕获链接中心拓扑信息；3）利用表格基础模型通过上下文学习预测链接存在

Result: 在6个跨不同领域的图基准测试中，该方法在不需数据集特定微调的情况下优于现有最先进基线方法

Conclusion: TFMLinker成功将表格基础模型应用于链接预测任务，实现了跨图通用预测，为链接预测提供了新的有效方法

Abstract: Link prediction is a fundamental task in graph machine learning with widespread applications such as recommendation systems, drug discovery, knowledge graphs, etc. In the foundation model era, how to develop universal link prediction methods across datasets and domains becomes a key problem, with some initial attempts adopting Graph Foundation Models utilizing Graph Neural Networks and Large Language Models. However, the existing methods face notable limitations, including limited pre-training scale or heavy reliance on textual information. Motivated by the success of tabular foundation models (TFMs) in achieving universal prediction across diverse tabular datasets, we explore an alternative approach by TFMs, which are pre-trained on diverse synthetic datasets sampled from structural causal models and support strong in-context learning independent of textual attributes. Nevertheless, adapting TFMs for link prediction faces severe technical challenges such as how to obtain the necessary context and capture link-centric topological information. To solve these challenges, we propose TFMLinker (Tabular Foundation Model for Link Predictor), aiming to leverage the in-context learning capabilities of TFMs to perform link prediction across diverse graphs without requiring dataset-specific fine-tuning. Specifically, we first develop a prototype-augmented local-global context module to construct context that captures both graph-specific and cross-graph transferable patterns. Next, we design a universal topology-aware link encoder to capture link-centric topological information and generate link representations as inputs for the TFM. Finally, we employ the TFM to predict link existence through in-context learning. Experiments on 6 graph benchmarks across diverse domains demonstrate the superiority of our method over state-of-the-art baselines without requiring dataset-specific finetuning.

</details>


### [284] [Breaking the Grid: Distance-Guided Reinforcement Learning in Large Discrete and Hybrid Action Spaces](https://arxiv.org/abs/2602.08616)
*Heiko Hoppe,Fabian Akkerman,Wouter van Heeswijk,Maximilian Schiffer*

Main category: cs.LG

TL;DR: DGRL通过采样动态邻域和距离更新，解决了大规模离散动作空间中的维度诅咒问题，在高达10^20动作空间中实现高效强化学习，性能提升达66%。


<details>
  <summary>Details</summary>
Motivation: 强化学习在物流、调度和推荐系统等应用中面临大规模离散动作空间的维度诅咒问题。现有方法依赖限制性的网格结构或计算昂贵的最近邻搜索，在高维或不规则结构领域中效果有限。

Method: 提出距离引导强化学习(DGRL)，结合采样动态邻域(SDN)和距离更新(DBU)。SDN利用语义嵌入空间进行随机体积探索，保证局部信任区域的完全覆盖。DBU将策略优化转化为稳定回归任务，解耦梯度方差与动作空间基数，保证策略单调改进。

Result: 在规则和不规则结构环境中，DGRL相比最先进基准性能提升高达66%，同时提高了收敛速度和计算复杂度。方法自然推广到混合连续-离散动作空间，无需层次依赖。

Conclusion: DGRL有效解决了大规模离散动作空间中的强化学习挑战，通过语义嵌入探索和稳定回归优化，实现了在高维空间中的高效学习，为实际应用提供了可行解决方案。

Abstract: Reinforcement Learning is increasingly applied to logistics, scheduling, and recommender systems, but standard algorithms struggle with the curse of dimensionality in such large discrete action spaces. Existing algorithms typically rely on restrictive grid-based structures or computationally expensive nearest-neighbor searches, limiting their effectiveness in high-dimensional or irregularly structured domains. We propose Distance-Guided Reinforcement Learning (DGRL), combining Sampled Dynamic Neighborhoods (SDN) and Distance-Based Updates (DBU) to enable efficient RL in spaces with up to 10$^\text{20}$ actions. Unlike prior methods, SDN leverages a semantic embedding space to perform stochastic volumetric exploration, provably providing full support over a local trust region. Complementing this, DBU transforms policy optimization into a stable regression task, decoupling gradient variance from action space cardinality and guaranteeing monotonic policy improvement. DGRL naturally generalizes to hybrid continuous-discrete action spaces without requiring hierarchical dependencies. We demonstrate performance improvements of up to 66% against state-of-the-art benchmarks across regularly and irregularly structured environments, while simultaneously improving convergence speed and computational complexity.

</details>


### [285] [ERIS: Enhancing Privacy and Communication Efficiency in Serverless Federated Learning](https://arxiv.org/abs/2602.08617)
*Dario Fenoglio,Pasquale Polverino,Jacopo Quizi,Martin Gjoreski,Marc Langheinrich*

Main category: cs.LG

TL;DR: ERIS是一个无服务器的联邦学习框架，通过模型分区和分布式梯度压缩，在保持FedAvg级别准确性的同时，显著降低通信成本并增强隐私保护。


<details>
  <summary>Details</summary>
Motivation: 将联邦学习扩展到十亿参数模型时，通信效率、模型准确性和隐私保证之间存在关键权衡。现有解决方案通常孤立地解决这些挑战，牺牲准确性或依赖昂贵的密码学工具。

Method: ERIS结合了模型分区策略（将聚合分布在多个客户端聚合器上）和分布式移位梯度压缩机制，消除了服务器瓶颈并分布了通信负载。

Result: 理论证明ERIS在标准假设下以与FedAvg相同的速率收敛，并将互信息泄漏限制与聚合器数量成反比。实验表明ERIS在图像和文本任务（包括大语言模型）中达到FedAvg级别准确性，同时显著降低通信成本并提高对成员推理和重建攻击的鲁棒性。

Conclusion: ERIS是一个平衡隐私和准确性的无服务器联邦学习框架，无需依赖繁重的密码学或噪声注入，就能提供强大的隐私保证且不降低准确性。

Abstract: Scaling federated learning (FL) to billion-parameter models introduces critical trade-offs between communication efficiency, model accuracy, and privacy guarantees. Existing solutions often tackle these challenges in isolation, sacrificing accuracy or relying on costly cryptographic tools. We propose ERIS, a serverless FL framework that balances privacy and accuracy while eliminating the server bottleneck and distributing the communication load. ERIS combines a model partitioning strategy, distributing aggregation across multiple client-side aggregators, with a distributed shifted gradient compression mechanism. We theoretically prove that ERIS (i) converges at the same rate as FedAvg under standard assumptions, and (ii) bounds mutual information leakage inversely with the number of aggregators, enabling strong privacy guarantees with no accuracy degradation. Experiments across image and text tasks, including large language models, confirm that ERIS achieves FedAvg-level accuracy while substantially reducing communication cost and improving robustness to membership inference and reconstruction attacks, without relying on heavy cryptography or noise injection.

</details>


### [286] [Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs](https://arxiv.org/abs/2602.08621)
*Yukun Jiang,Hai Huang,Mingjie Li,Yage Zhang,Michael Backes,Yang Zhang*

Main category: cs.LG

TL;DR: 研究发现MoE架构的LLMs存在安全漏洞，通过操纵路由器可以激活不安全路径，将安全输出转为有害内容，并提出防御方法。


<details>
  <summary>Details</summary>
Motivation: MoE架构通过稀疏激活降低计算成本，但之前研究主要关注效用和效率，对其安全风险探索不足。本文旨在揭示MoE LLMs中存在的安全漏洞。

Method: 1. 提出Router Safety重要性评分(RoSais)量化路由器安全关键性；2. 开发细粒度token-layer-wise随机优化框架(F-SOUR)发现具体不安全路径；3. 在四个代表性MoE LLM家族上进行实验。

Result: 1. 仅操纵高RoSais路由器即可将默认路径转为不安全路径；2. 在DeepSeek-V2-Lite上屏蔽5个路由器使攻击成功率提升4倍至0.79；3. F-SOUR在JailbreakBench和AdvBench上分别达到0.90和0.98的平均攻击成功率。

Conclusion: MoE LLMs的安全性与其架构一样稀疏，存在通过路由器操纵激活不安全路径的风险。提出了安全感知路径禁用和路由器训练等防御方向，为MoE LLMs的红队测试和安全保障提供参考。

Abstract: By introducing routers to selectively activate experts in Transformer layers, the mixture-of-experts (MoE) architecture significantly reduces computational costs in large language models (LLMs) while maintaining competitive performance, especially for models with massive parameters. However, prior work has largely focused on utility and efficiency, leaving the safety risks associated with this sparse architecture underexplored. In this work, we show that the safety of MoE LLMs is as sparse as their architecture by discovering unsafe routes: routing configurations that, once activated, convert safe outputs into harmful ones. Specifically, we first introduce the Router Safety importance score (RoSais) to quantify the safety criticality of each layer's router. Manipulation of only the high-RoSais router(s) can flip the default route into an unsafe one. For instance, on JailbreakBench, masking 5 routers in DeepSeek-V2-Lite increases attack success rate (ASR) by over 4$\times$ to 0.79, highlighting an inherent risk that router manipulation may naturally occur in MoE LLMs. We further propose a Fine-grained token-layer-wise Stochastic Optimization framework to discover more concrete Unsafe Routes (F-SOUR), which explicitly considers the sequentiality and dynamics of input tokens. Across four representative MoE LLM families, F-SOUR achieves an average ASR of 0.90 and 0.98 on JailbreakBench and AdvBench, respectively. Finally, we outline defensive perspectives, including safety-aware route disabling and router training, as promising directions to safeguard MoE LLMs. We hope our work can inform future red-teaming and safeguarding of MoE LLMs. Our code is provided in https://github.com/TrustAIRLab/UnsafeMoE.

</details>


### [287] [CauScale: Neural Causal Discovery at Scale](https://arxiv.org/abs/2602.08629)
*Bo Peng,Sirui Chen,Jiaguo Tian,Yu Qiao,Chaochao Lu*

Main category: cs.LG

TL;DR: CauScale是一种用于高效因果发现的神经架构，通过压缩数据嵌入和共享注意力权重实现时间和空间效率提升，可扩展到1000节点图，相比现有方法获得4-13,000倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法在处理大规模图时面临时间和空间效率瓶颈，限制了在科学AI和数据分析等数据驱动领域的应用扩展。

Method: CauScale采用双流设计：数据流从高维观测中提取关系证据，图流整合统计图先验并保留关键结构信号。通过减少单元压缩数据嵌入提高时间效率，采用共享注意力权重避免维护轴特定注意力图来提高空间效率。

Result: CauScale成功扩展到500节点图的训练（先前工作因空间限制无法实现），在分布内数据上达到99.6% mAP，分布外数据上达到84.4% mAP，推理速度比先前方法快4-13,000倍。

Conclusion: CauScale通过创新的神经架构设计解决了因果发现中的可扩展性瓶颈，为大规模图的因果发现提供了高效解决方案，在保持高准确性的同时显著提升了时间和空间效率。

Abstract: Causal discovery is essential for advancing data-driven fields such as scientific AI and data analysis, yet existing approaches face significant time- and space-efficiency bottlenecks when scaling to large graphs. To address this challenge, we present CauScale, a neural architecture designed for efficient causal discovery that scales inference to graphs with up to 1000 nodes. CauScale improves time efficiency via a reduction unit that compresses data embeddings and improves space efficiency by adopting tied attention weights to avoid maintaining axis-specific attention maps. To keep high causal discovery accuracy, CauScale adopts a two-stream design: a data stream extracts relational evidence from high-dimensional observations, while a graph stream integrates statistical graph priors and preserves key structural signals. CauScale successfully scales to 500-node graphs during training, where prior work fails due to space limitations. Across testing data with varying graph scales and causal mechanisms, CauScale achieves 99.6% mAP on in-distribution data and 84.4% on out-of-distribution data, while delivering 4-13,000 times inference speedups over prior methods. Our project page is at https://github.com/OpenCausaLab/CauScale.

</details>


### [288] [LEFT: Learnable Fusion of Tri-view Tokens for Unsupervised Time Series Anomaly Detection](https://arxiv.org/abs/2602.08638)
*Dezheng Wang,Tong Chen,Guansong Pang,Congyan Chen,Shihua Li,Hongzhi Yin*

Main category: cs.LG

TL;DR: LEFT：一种通过三视图令牌可学习融合的无监督时间序列异常检测框架，将异常建模为互补表示之间的不一致性


<details>
  <summary>Details</summary>
Motivation: 无监督时间序列异常检测的关键挑战在于许多异常过于微妙，无法在单一视图中检测到，而是表现为跨多个视图（如时间、频率和多分辨率）的不一致性。现有跨视图方法大多依赖特征或分数融合，缺乏分析-合成一致性约束。

Method: LEFT从三个视图学习特征令牌：频率域令牌（嵌入周期性信息）、时间域令牌（捕捉局部动态）和多尺度令牌（学习不同粒度的时间序列异常模式）。通过可学习的奈奎斯特约束谱滤波器将原始时间序列重缩放为多个分辨率，并引入细粒度目标重建目标和创新的时频循环一致性约束来正则化跨视图一致性。

Result: 在真实世界基准测试中，LEFT相比最先进的基线方法获得了最佳检测精度，同时实现了5倍的FLOPs减少和8倍的训练加速。

Conclusion: LEFT通过三视图令牌融合和时频循环一致性约束，有效解决了无监督时间序列异常检测中跨视图不一致性建模的挑战，在精度和效率方面均表现出色。

Abstract: As a fundamental data mining task, unsupervised time series anomaly detection (TSAD) aims to build a model for identifying abnormal timestamps without assuming the availability of annotations. A key challenge in unsupervised TSAD is that many anomalies are too subtle to exhibit detectable deviation in any single view (e.g., time domain), and instead manifest as inconsistencies across multiple views like time, frequency, and a mixture of resolutions. However, most cross-view methods rely on feature or score fusion and do not enforce analysis-synthesis consistency, meaning the frequency branch is not required to reconstruct the time signal through an inverse transform, and vice versa. In this paper, we present Learnable Fusion of Tri-view Tokens (LEFT), a unified unsupervised TSAD framework that models anomalies as inconsistencies across complementary representations. LEFT learns feature tokens from three views of the same input time series: frequency-domain tokens that embed periodicity information, time-domain tokens that capture local dynamics, and multi-scale tokens that learns abnormal patterns at varying time series granularities. By learning a set of adaptive Nyquist-constrained spectral filters, the original time series is rescaled into multiple resolutions and then encoded, allowing these multi-scale tokens to complement the extracted frequency- and time-domain information. When generating the fused representation, we introduce a novel objective that reconstructs fine-grained targets from coarser multi-scale structure, and put forward an innovative time-frequency cycle consistency constraint to explicitly regularize cross-view agreement. Experiments on real-world benchmarks show that LEFT yields the best detection accuracy against SOTA baselines, while achieving a 5x reduction on FLOPs and 8x speed-up for training.

</details>


### [289] [Projected Gradient Ascent for Efficient Reward-Guided Updates with One-Step Generative Models](https://arxiv.org/abs/2602.08646)
*Jisung Hwang,Minhyuk Sung*

Main category: cs.LG

TL;DR: 提出一种带约束的潜在优化方法，通过硬性白噪声约束防止奖励黑客攻击，在保持生成质量的同时显著提升优化效率


<details>
  <summary>Details</summary>
Motivation: 现有的测试时潜在优化方法虽然能提升预训练生成模型的奖励引导生成效果，但存在两个主要问题：1) 容易发生奖励黑客攻击，导致生成质量下降；2) 优化速度过慢，不适用于实际应用

Method: 采用硬性白高斯噪声约束替代软正则化，通过投影梯度上升法在每次更新后应用闭式投影，保持潜在向量在整个优化过程中保持噪声特性，防止导致不真实伪影的漂移

Result: 该方法仅需SOTA正则化方法30%的墙钟时间就能达到相当的美学评分，同时有效防止奖励黑客攻击。投影操作复杂度为O(N log N)，与标准算法（如排序或FFT）相当，实际运行时间几乎不增加

Conclusion: 通过硬性白噪声约束的潜在优化方法，实现了高效可靠的奖励引导生成，解决了现有方法的速度和可靠性问题，为实际应用提供了可行方案

Abstract: We propose a constrained latent optimization method for reward-guided generation that preserves white Gaussian noise characteristics with negligible overhead. Test-time latent optimization can unlock substantially better reward-guided generations from pretrained generative models, but it is prone to reward hacking that degrades quality and also too slow for practical use. In this work, we make test-time optimization both efficient and reliable by replacing soft regularization with hard white Gaussian noise constraints enforced via projected gradient ascent. Our method applies a closed-form projection after each update to keep the latent vector explicitly noise-like throughout optimization, preventing the drift that leads to unrealistic artifacts. This enforcement adds minimal cost: the projection matches the $O(N \log N)$ complexity of standard algorithms such as sorting or FFT and does not practically increase wall-clock time. In experiments, our approach reaches a comparable Aesthetic Score using only 30% of the wall-clock time required by the SOTA regularization-based method, while preventing reward hacking.

</details>


### [290] [From Robotics to Sepsis Treatment: Offline RL via Geometric Pessimism](https://arxiv.org/abs/2602.08655)
*Sarthak Wanjari*

Main category: cs.LG

TL;DR: Geo-IQL：一种计算高效的离线强化学习框架，通过k近邻密度惩罚增强IQL，有效解决OOD动作高估问题，在稀疏数据和真实医疗数据集上表现优异


<details>
  <summary>Details</summary>
Motivation: 离线RL在静态数据集上恢复最优策略时，容易高估分布外(OOD)动作，特别是在数据稀疏和断裂的情况下。现有方法需要在计算效率和性能之间权衡：CQL计算成本高，而IQL在病态数据集上容易退化为行为克隆

Method: 提出几何悲观主义框架，在标准IQL基础上增加基于k近邻距离的密度惩罚。在状态-动作嵌入空间中计算惩罚项，通过奖励塑形注入OOD保守性，预计算惩罚项使训练开销仅为O(1)

Result: 在D4RL MuJoCo基准测试中，Geo-IQL在敏感不稳定的medium-replay任务上比标准IQL提升18+分，种子间方差降低4倍。在MIMIC-III Sepsis数据集上，标准IQL退化为行为克隆，而Geo-IQL实现主动策略改进，与临床医生终末决策一致性达86.4%（IQL为75%）

Conclusion: 几何悲观主义为关键真实世界决策系统提供了必要的正则化，能够安全克服局部最优，在计算效率和性能之间取得良好平衡，特别适用于稀疏数据和医疗等关键应用场景

Abstract: Offline Reinforcement Learning (RL) promises the recovery of optimal policies from static datasets, yet it remains susceptible to the overestimation of out-of-distribution (OOD) actions, particularly in fractured and sparse data manifolds.Current solutions necessitates a trade off between computational efficiency and performance. Methods like CQL offers rigorous conservatism but require tremendous compute power while efficient expectile-based methods like IQL often fail to correct OOD errors on pathological datasets, collapsing to Behavioural Cloning. In this work, we propose Geometric Pessimism, a modular, compute-efficient framework that augments standard IQL with density-based penalty derived from k-nearest-neighbour distances in the state-action embedding space. By pre-computing the penalties applied to each state-action pair our method injects OOD conservatism via reward shaping with a O(1) training overhead. Evaluated on the D4Rl MuJoCo benchmark, our method, Geo-IQL outperforms standard IQL on sensitive and unstable medium-replay tasks by over 18 points, while reducing inter-seed variance by 4x. Furthermore, Geo-IQL does not degrade performance on stable manifolds. Crucially, we validate our algorithm on the MIMIC-III Sepsis critical care dataset. While standard IQL collapses to behaviour cloning, Geo-IQL demonstrates active policy improvement. Maintaining safety constraints, achieving 86.4% terminal agreement with clinicians compared to IQL's 75%. Our results suggest that geometric pessimism provides the necessary regularisation to safely overcome local optima in critical, real-world decision systems.

</details>


### [291] [Two-Stage Data Synthesization: A Statistics-Driven Restricted Trade-off between Privacy and Prediction](https://arxiv.org/abs/2602.08657)
*Xiaotong Liu,Shao-Bo Lin,Jun Fan,Ding-Xuan Zhou*

Main category: cs.LG

TL;DR: 提出两阶段合成数据策略，通过合成-混合和核岭回归方法，在保护隐私的同时优化下游预测性能


<details>
  <summary>Details</summary>
Motivation: 现有合成数据方法主要关注统计信息保持，难以平衡隐私保护所需的显著扰动与预测性能对扰动的敏感性之间的矛盾

Method: 两阶段合成策略：第一阶段采用合成-混合方法生成混合数据；第二阶段基于核岭回归模型，在第一阶段合成输入的基础上生成合成输出

Result: 理论分析和数值实验验证了该方法能够实现统计驱动的受限隐私-预测权衡，并在五个真实数据集和营销问题上展示了泛化能力

Conclusion: 提出的两阶段合成策略通过核岭回归的理论优势和协变分布保持，实现了最优预测性能，同时满足隐私保护要求

Abstract: Synthetic data have gained increasing attention across various domains, with a growing emphasis on their performance in downstream prediction tasks. However, most existing synthesis strategies focus on maintaining statistical information. Although some studies address prediction performance guarantees, their single-stage synthesis designs make it challenging to balance the privacy requirements that necessitate significant perturbations and the prediction performance that is sensitive to such perturbations. We propose a two-stage synthesis strategy. In the first stage, we introduce a synthesis-then-hybrid strategy, which involves a synthesis operation to generate pure synthetic data, followed by a hybrid operation that fuses the synthetic data with the original data. In the second stage, we present a kernel ridge regression (KRR)-based synthesis strategy, where a KRR model is first trained on the original data and then used to generate synthetic outputs based on the synthetic inputs produced in the first stage. By leveraging the theoretical strengths of KRR and the covariant distribution retention achieved in the first stage, our proposed two-stage synthesis strategy enables a statistics-driven restricted privacy--prediction trade-off and guarantee optimal prediction performance. We validate our approach and demonstrate its characteristics of being statistics-driven and restricted in achieving the privacy--prediction trade-off both theoretically and numerically. Additionally, we showcase its generalizability through applications to a marketing problem and five real-world datasets.

</details>


### [292] [Equalized Generative Treatment: Matching f-divergences for Fairness in Generative Models](https://arxiv.org/abs/2602.08660)
*Alexandre Verine,Rafael Pinot,Florian Le Bronnec*

Main category: cs.LG

TL;DR: 论文提出了一种新的生成模型公平性定义EGT，强调不同敏感群体间的生成质量可比性，而非仅概率平衡。理论分析表明公平约束会耦合模型整体质量与最难近似群体的质量，因此提出min-max微调方法，实验证明其在图像和文本生成任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型公平性概念主要从分类任务移植而来，关注不同敏感群体生成样本的概率平衡。但这种标准存在脆弱性，因为即使不同敏感群体的建模质量差异很大，这些标准也能被满足。需要更全面的公平性定义来确保生成质量在不同群体间的可比性。

Method: 提出新的公平性定义"平等化生成处理(EGT)"，要求所有敏感群体的生成质量具有可比性，质量通过参考f-散度衡量。理论分析表明公平约束会将整体模型质量与最难近似的群体质量耦合，因此提出简单高效的min-max微调方法来平衡不同敏感群体间的f-散度。

Result: 在图像和文本生成任务上的实验验证表明，min-max方法相比文献中的其他方法能够持续实现更公平的结果，同时在两个任务上都保持有竞争力的整体性能。理论分析得到实验支持，证明了EGT定义的有效性和min-max方法的优越性。

Conclusion: EGT为生成模型提供了一个更全面的公平性框架，超越了仅关注概率平衡的现有方法。min-max微调是实现EGT的有效方法，能够在保持整体性能的同时确保不同敏感群体间的生成质量公平性。这项工作为生成模型的公平性研究提供了新的理论视角和实践方法。

Abstract: Fairness is a crucial concern for generative models, which not only reflect but can also amplify societal and cultural biases. Existing fairness notions for generative models are largely adapted from classification and focus on balancing the probability of generating samples from each sensitive group. We show that such criteria are brittle, as they can be met even when different sensitive groups are modeled with widely varying quality. To address this limitation, we introduce a new fairness definition for generative models, termed as equalized generative treatment (EGT), which requires comparable generation quality across all sensitive groups, with quality measured via a reference f-divergence. We further analyze the trade-offs induced by EGT, demonstrating that enforcing fairness constraints necessarily couples the overall model quality to that of the most challenging group to approximate. This indicates that a simple yet efficient min-max fine-tuning method should be able to balance f-divergences across sensitive groups to satisfy EGT. We validate this theoretical insight through a set of experiments on both image and text generation tasks. We demonstrate that min-max methods consistently achieve fairer outcomes compared to other approaches from the literature, while maintaining competitive overall performance for both tasks.

</details>


### [293] [LLaDA2.1: Speeding Up Text Diffusion via Token Editing](https://arxiv.org/abs/2602.08676)
*Tiwei Bie,Maosong Cao,Xiang Cao,Bingsen Chen,Fuyuan Chen,Kun Chen,Lun Du,Daozhuo Feng,Haibo Feng,Mingliang Gong,Zhuocheng Gong,Yanmei Gu,Jian Guan,Kaiyuan Guan,Hongliang He,Zenan Huang,Juyong Jiang,Zhonghui Jiang,Zhenzhong Lan,Chengxi Li,Jianguo Li,Zehuan Li,Huabin Liu,Lin Liu,Guoshan Lu,Yuan Lu,Yuxin Ma,Xingyu Mou,Zhenxuan Pan,Kaida Qiu,Yuji Ren,Jianfeng Tan,Yiding Tian,Zian Wang,Lanning Wei,Tao Wu,Yipeng Xing,Wentao Ye,Liangyu Zha,Tianze Zhang,Xiaolu Zhang,Junbo Zhao,Da Zheng,Hao Zhong,Wanli Zhong,Jun Zhou,Junlin Zhou,Liwang Zhu,Muzhi Zhu,Yihong Zhuang*

Main category: cs.LG

TL;DR: LLaDA2.1通过结合Token-to-Token编辑与Mask-to-Token方案，引入可配置阈值解码，提供速度模式和质量模式，并首次实现针对dLLMs的大规模强化学习框架，在保持高质量的同时实现极快解码速度。


<details>
  <summary>Details</summary>
Motivation: 解决LLaDA2.0中解码速度与生成质量之间的权衡问题，超越传统扩散模型的限制，实现更高效、更精确的文本生成。

Method: 1. 将Token-to-Token编辑无缝集成到传统Mask-to-Token方案中，引入联合可配置阈值解码方案；2. 提供两种模式：速度模式（降低M2T阈值，依赖T2T细化输出）和质量模式（保守阈值保证性能）；3. 首次实现针对dLLMs的大规模强化学习框架，采用专门技术进行稳定梯度估计。

Result: 在33个基准测试中表现出色，解码速度极快：100B模型在HumanEval+上达到892 TPS，BigCodeBench上801 TPS，LiveCodeBench上663 TPS。同时发布LLaDA2.1-Mini（16B）和LLaDA2.1-Flash（100B）两个版本。

Conclusion: LLaDA2.1成功解决了扩散模型解码速度与质量的权衡问题，通过创新的解码方案和强化学习对齐，在保持高质量输出的同时实现了前所未有的解码速度，为大规模扩散语言模型的实际应用铺平了道路。

Abstract: While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.

</details>


### [294] [Dashed Line Defense: Plug-And-Play Defense Against Adaptive Score-Based Query Attacks](https://arxiv.org/abs/2602.08679)
*Yanzhang Fu,Zizheng Guo,Jizhou Luo*

Main category: cs.LG

TL;DR: 本文提出Dashed Line Defense (DLD)，一种针对黑盒分数查询攻击的运行时防御方法，通过引入损失值模糊性来抵御自适应攻击策略。


<details>
  <summary>Details</summary>
Motivation: 现有基于输出扰动的运行时防御方法存在两个主要问题：要么需要访问模型参数，要么在面对自适应攻击时失效。即使是当前最先进的即插即用防御也能被自适应攻击绕过，这暴露了现有运行时防御的关键局限性。

Method: 提出Dashed Line Defense (DLD)，这是一种即插即用的后处理方法，专门设计用于抵御自适应查询策略。通过在观测损失与候选样本真实对抗强度之间引入模糊性，防止攻击者可靠地分析和调整查询，从而有效破坏对抗样本生成过程。

Result: 在ImageNet数据集上的实验验证了DLD的有效性，表明DLD在保持模型预测标签的同时，始终优于先前的防御方法，即使在最坏情况的自适应攻击下也是如此。

Conclusion: DLD提供了一种有效的运行时防御机制，能够抵御自适应分数查询攻击，同时提供了防御能力的理论保证，解决了现有防御方法在面对自适应攻击时的脆弱性问题。

Abstract: Score-based query attacks pose a serious threat to deep learning models by crafting adversarial examples (AEs) using only black-box access to model output scores, iteratively optimizing inputs based on observed loss values. While recent runtime defenses attempt to disrupt this process via output perturbation, most either require access to model parameters or fail when attackers adapt their tactics. In this paper, we first reveal that even the state-of-the-art plug-and-play defense can be bypassed by adaptive attacks, exposing a critical limitation of existing runtime defenses. We then propose Dashed Line Defense (DLD), a plug-and-play post-processing method specifically designed to withstand adaptive query strategies. By introducing ambiguity in how the observed loss reflects the true adversarial strength of candidate examples, DLD prevents attackers from reliably analyzing and adapting their queries, effectively disrupting the AE generation process. We provide theoretical guarantees of DLD's defense capability and validate its effectiveness through experiments on ImageNet, demonstrating that DLD consistently outperforms prior defenses--even under worst-case adaptive attacks--while preserving the model's predicted labels.

</details>


### [295] [The Theory and Practice of MAP Inference over Non-Convex Constraints](https://arxiv.org/abs/2602.08681)
*Leander Kurscheidt,Gabriele Masina,Roberto Sebastiani,Antonio Vergari*

Main category: cs.LG

TL;DR: 提出两种处理连续变量约束MAP推断的方法：针对可解片段的精确消息传递算法，以及结合区域划分和数值优化的通用策略，在复杂密度函数上优于无约束基线。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用中，概率ML系统需要在代数约束下进行预测（如避开障碍物的最可能轨迹），但这些约束通常非凸且密度函数非对数凹，使得约束MAP推断计算极具挑战性。

Method: 1) 研究连续变量约束MAP推断的可解条件，开发可扩展的消息传递算法；2) 提出通用策略，将域划分为凸可行区域并与数值约束优化交替进行。

Result: 在合成和真实基准测试中，两种方法均优于无视约束的基线，并能扩展到当前最先进精确求解器无法处理的复杂密度函数。

Conclusion: 论文提出了处理连续变量约束MAP推断的有效方法，解决了实际应用中非凸约束和非对数凹密度函数的计算挑战，为安全关键系统的可靠预测提供了实用解决方案。

Abstract: In many safety-critical settings, probabilistic ML systems have to make predictions subject to algebraic constraints, e.g., predicting the most likely trajectory that does not cross obstacles.
  These real-world constraints are rarely convex, nor the densities considered are (log-)concave.
  This makes computing this constrained maximum a posteriori (MAP) prediction efficiently and reliably extremely challenging.
  In this paper, we first investigate under which conditions we can perform constrained MAP inference over continuous variables exactly and efficiently and devise a scalable message-passing algorithm for this tractable fragment.
  Then, we devise a general constrained MAP strategy that interleaves partitioning the domain into convex feasible regions with numerical constrained optimization.
  We evaluate both methods on synthetic and real-world benchmarks, showing our %
  approaches outperform constraint-agnostic baselines, and scale to complex densities intractable for SoTA exact solvers.

</details>


### [296] [CompilerKV: Risk-Adaptive KV Compression via Offline Experience Compilation](https://arxiv.org/abs/2602.08686)
*Ning Yang,Chengzhi Wang,Yibo Liu,Baoliang Tian,Haijun Zhang*

Main category: cs.LG

TL;DR: CompilerKV：一种风险自适应、头感知的KV缓存压缩框架，通过离线学习头异质性表和风险自适应阈值门控，在512令牌预算下恢复97.7%的FullKV性能。


<details>
  <summary>Details</summary>
Motivation: 现有KV压缩方法在紧内存预算下存在两个关键问题：1) 忽略提示依赖的压缩风险变化；2) 忽略注意力头之间的功能异质性，导致令牌选择不稳定和尾部失败。

Method: 提出CompilerKV框架，包含两个关键组件：1) 头异质性表：通过离线上下文老虎机学习，为不同注意力头分配可靠性权重；2) 风险自适应阈值门控：联合建模注意力熵和局部困惑度，将提示级风险转化为可部署的保留阈值。

Result: 在LongBench上，CompilerKV在512令牌预算下主导SOTA方法，恢复97.7%的FullKV性能，相比最强竞争对手提升高达+5.2个点。

Conclusion: CompilerKV通过风险自适应和头感知的压缩策略，有效解决了长上下文LLM中KV缓存内存线性增长的问题，实现了高效的KV压缩。

Abstract: Large Language Models (LLMs) in long-context scenarios are severely constrained by the linear growth of Key-Value (KV) cache memory. Existing KV compression methods rely either on static thresholds and attention-only heuristics or on coarse memory budget allocation. Under tight memory budgets, these methods overlook two key factors: prompt-dependent variation in compression risk and functional heterogeneity across attention heads, which destabilize token selection and lead to tail failures. To address these challenges, we propose CompilerKV, a risk-adaptive and head-aware compression framework that compiles offline experience into reusable decision tables for prefill-only deployment. CompilerKV integrates two key synergistic components: (i) a Head Heterogeneity Table, learned via offline contextual bandits, which assigns head-specific reliability weights to govern functional differences across attention heads explicitly; and (ii) a Risk-Adaptive Threshold Gating mechanism that jointly models attention entropy and local perplexity, transforming prompt-level risk into deployable retention thresholds. Experiments on LongBench show CompilerKV dominates SOTA methods under a 512-token budget, recovering 97.7\% of FullKV performance while achieving up to +5.2 points gain over the strongest competitor.

</details>


### [297] [Learning To Sample From Diffusion Models Via Inverse Reinforcement Learning](https://arxiv.org/abs/2602.08689)
*Constant Bourdrez,Alexandre Vérine,Olivier Cappé*

Main category: cs.LG

TL;DR: 提出基于逆强化学习的扩散模型采样策略学习框架，无需重新训练去噪器即可优化采样过程


<details>
  <summary>Details</summary>
Motivation: 扩散模型的训练计算成本高，但采样过程具有灵活性，可以利用这种灵活性改进生成样本质量和采样效率

Method: 将扩散采样过程建模为离散时间有限视野马尔可夫决策过程，动作对应采样动力学的可选修改，使用策略梯度技术直接匹配目标行为，避免定义显式奖励函数

Result: 实验证明该方法能改进预训练扩散模型的生成样本质量，并自动调整采样超参数

Conclusion: 提出的逆强化学习框架为扩散模型采样策略学习提供了有效方法，无需重新训练去噪器即可优化采样过程

Abstract: Diffusion models generate samples through an iterative denoising process, guided by a neural network. While training the denoiser on real-world data is computationally demanding, the sampling procedure itself is more flexible. This adaptability serves as a key lever in practice, enabling improvements in both the quality of generated samples and the efficiency of the sampling process. In this work, we introduce an inverse reinforcement learning framework for learning sampling strategies without retraining the denoiser. We formulate the diffusion sampling procedure as a discrete-time finite-horizon Markov Decision Process, where actions correspond to optional modifications of the sampling dynamics. To optimize action scheduling, we avoid defining an explicit reward function. Instead, we directly match the target behavior expected from the sampler using policy gradient techniques. We provide experimental evidence that this approach can improve the quality of samples generated by pretrained diffusion models and automatically tune sampling hyperparameters.

</details>


### [298] [SoK: The Pitfalls of Deep Reinforcement Learning for Cybersecurity](https://arxiv.org/abs/2602.08690)
*Shae McFadden,Myles Foley,Elizabeth Bates,Ilias Tsingenopoulos,Sanyam Vyas,Vasilios Mavroudis,Chris Hicks,Fabio Pierazzi*

Main category: cs.LG

TL;DR: 该论文系统分析了深度强化学习在网络安全应用中的11个常见方法学陷阱，通过分析66篇相关文献发现平均每篇存在超过5个陷阱，并通过实验验证其影响，最后提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在网络安全领域的应用面临从实验室模拟到实际部署的诸多挑战，包括对抗性、非平稳性和部分可观测性等问题，需要系统识别和解决这些方法学陷阱。

Method: 通过分析2018-2025年间66篇重要的DRL4Sec论文，识别并系统化11个方法学陷阱，涵盖环境建模、智能体训练、性能评估和系统部署四个阶段，并通过自主网络防御、对抗性恶意软件创建和Web安全测试三个环境的受控实验验证这些陷阱的实际影响。

Result: 研究发现平均每篇论文存在超过5个方法学陷阱，通过实验证明了这些陷阱对系统性能的实际影响，表明当前DRL4Sec研究存在系统性方法学问题。

Conclusion: 论文为每个陷阱提供了可操作的建议，旨在支持开发更严谨和可部署的基于DRL的安全系统，推动该领域从实验室研究向实际应用转化。

Abstract: Deep Reinforcement Learning (DRL) has achieved remarkable success in domains requiring sequential decision-making, motivating its application to cybersecurity problems. However, transitioning DRL from laboratory simulations to bespoke cyber environments can introduce numerous issues. This is further exacerbated by the often adversarial, non-stationary, and partially-observable nature of most cybersecurity tasks. In this paper, we identify and systematize 11 methodological pitfalls that frequently occur in DRL for cybersecurity (DRL4Sec) literature across the stages of environment modeling, agent training, performance evaluation, and system deployment. By analyzing 66 significant DRL4Sec papers (2018-2025), we quantify the prevalence of each pitfall and find an average of over five pitfalls per paper. We demonstrate the practical impact of these pitfalls using controlled experiments in (i) autonomous cyber defense, (ii) adversarial malware creation, and (iii) web security testing environments. Finally, we provide actionable recommendations for each pitfall to support the development of more rigorous and deployable DRL-based security systems.

</details>


### [299] [Reasoning aligns language models to human cognition](https://arxiv.org/abs/2602.08693)
*Gonçalo Guiomar,Elia Torre,Pehuen Moure,Victoria Shavina,Mario Giulianelli,Shih-Chii Liu,Valerio Mante*

Main category: cs.LG

TL;DR: 语言模型在不确定性下的决策是否像人类？研究发现扩展推理（如思维链）是性能提升的关键，使模型推理更接近人类，但在主动信息获取方面仍有差距。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型在不确定性下的决策机制是否与人类相似，以及思维链推理在决策过程中的作用，通过将主动采样与推理分离来系统比较人类和语言模型的表现。

Method: 引入主动概率推理任务，分离主动证据获取（采样）和证据整合（推理）。比较人类和多种当代大语言模型与近最优参考策略的表现，使用机制模型通过四个可解释潜变量（记忆、策略、选择偏差、遮挡意识）分析行为差异。

Result: 扩展推理是强性能的关键决定因素，显著提升推理能力并使信念轨迹更接近人类，但对主动采样的改善有限。思维链推理使语言模型向人类式的证据积累和信念-选择映射转变，在推理上更对齐人类，但在信息获取方面仍有持续差距。

Conclusion: 思维链推理是语言模型决策过程更接近人类的关键机制，主要在推理整合方面产生人类对齐，但在主动信息获取策略方面仍需改进。该研究为理解语言模型认知过程提供了共享的认知空间框架。

Abstract: Do language models make decisions under uncertainty like humans do, and what role does chain-of-thought (CoT) reasoning play in the underlying decision process? We introduce an active probabilistic reasoning task that cleanly separates sampling (actively acquiring evidence) from inference (integrating evidence toward a decision). Benchmarking humans and a broad set of contemporary large language models against near-optimal reference policies reveals a consistent pattern: extended reasoning is the key determinant of strong performance, driving large gains in inference and producing belief trajectories that become strikingly human-like, while yielding only modest improvements in active sampling. To explain these differences, we fit a mechanistic model that captures systematic deviations from optimal behavior via four interpretable latent variables: memory, strategy, choice bias, and occlusion awareness. This model places humans and models in a shared low-dimensional cognitive space, reproduces behavioral signatures across agents, and shows how chain-of-thought shifts language models toward human-like regimes of evidence accumulation and belief-to-choice mapping, tightening alignment in inference while leaving a persistent gap in information acquisition.

</details>


### [300] [Trapped by simplicity: When Transformers fail to learn from noisy features](https://arxiv.org/abs/2602.08695)
*Evan Peters,Ando Deng,Matheus H. Zambianco,Devin Blankespoor,Achim Kempf*

Main category: cs.LG

TL;DR: Transformer在噪声特征数据训练中，对某些稀疏奇偶校验和多数函数能实现噪声鲁棒学习，但对随机k-junta函数通常失败，尤其当最优解布尔敏感度低于目标函数时。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer在噪声特征数据训练后，能否正确泛化到无噪声输入，即噪声鲁棒学习能力。探索Transformer与LSTM在噪声鲁棒学习上的差异，并分析Transformer失败的原因。

Method: 研究Transformer在k-稀疏奇偶校验、多数函数和随机k-junta函数上的噪声鲁棒学习能力。通过对比LSTM性能，分析Transformer失败原因，提出敏感性惩罚损失来帮助Transformer逃离错误解。

Result: Transformer对k-稀疏奇偶校验和多数函数能实现噪声鲁棒学习，优于LSTM。但对随机k-junta函数通常失败，尤其当最优解敏感性低于目标函数时。通过添加敏感性惩罚损失，Transformer能逃离错误解陷阱。

Conclusion: Transformer在布尔函数噪声鲁棒学习方面效果不佳，失败原因是其偏向简单函数，而噪声鲁棒学习的最优函数通常敏感性更低。通过敏感性惩罚可以改善，但总体上Transformer对布尔函数噪声学习效果有限。

Abstract: Noise is ubiquitous in data used to train large language models, but it is not well understood whether these models are able to correctly generalize to inputs generated without noise. Here, we study noise-robust learning: are transformers trained on data with noisy features able to find a target function that correctly predicts labels for noiseless features? We show that transformers succeed at noise-robust learning for a selection of $k$-sparse parity and majority functions, compared to LSTMs which fail at this task for even modest feature noise. However, we find that transformers typically fail at noise-robust learning of random $k$-juntas, especially when the boolean sensitivity of the optimal solution is smaller than that of the target function. We argue that this failure is due to a combination of two factors: transformers' bias toward simpler functions, combined with an observation that the optimal function for noise-robust learning typically has lower sensitivity than the target function for random boolean functions. We test this hypothesis by exploiting transformers' simplicity bias to trap them in an incorrect solution, but show that transformers can escape this trap by training with an additional loss term penalizing high-sensitivity solutions. Overall, we find that transformers are particularly ineffective for learning boolean functions in the presence of feature noise.

</details>


### [301] [QUOKA: Query-Oriented KV Selection For Efficient LLM Prefill](https://arxiv.org/abs/2602.08722)
*Dalton Jones,Junyoung Park,Matthew Morse,Mingu Lee,Chris Lott,Harper Langston*

Main category: cs.LG

TL;DR: QUOKA是一种无需训练、硬件无关的稀疏注意力算法，通过选择低余弦相似度的查询和相关的键值对来加速Transformer推理，在保持接近基线准确率的同时实现3-7倍加速。


<details>
  <summary>Details</summary>
Motivation: 在分块预填充（chunked prefill）场景下，许多查询只关注一小部分键，但低余弦相似度的查询与更多键交互且对最终注意力对数贡献最大，通过优先处理这些查询可以近似完整注意力的行为。

Method: QUOKA采用两阶段方法：1) 保留一小部分代表性查询（特别是低余弦相似度的查询）；2) 基于这些查询选择最相关的键值对，从而减少需要计算的键值对数量。

Result: 在Needle-In-A-Haystack、LongBench、RULER和Math500等基准测试中，QUOKA实现了首token时间减少3倍，Nvidia GPU上注意力计算加速5倍，Intel Xeon CPU上加速近7倍，同时使用88%更少的键值对，准确率接近基线。

Conclusion: QUOKA是一种有效的稀疏注意力算法，能够在保持模型准确性的同时显著加速Transformer推理，特别是在分块预填充场景下，具有硬件无关性和无需训练的优势。

Abstract: We present QUOKA: Query-oriented KV selection for efficient attention, a training-free and hardware agnostic sparse attention algorithm for accelerating transformer inference under chunked prefill. While many queries focus on a smaller group of keys in the attention operator, we observe that queries with low cosine similarity with respect to the mean query interact more strongly with more keys and have the greatest contribution to final attention logits. By prioritizing these low cosine similarity queries, the behavior of full attention during the prefill stage can be closely approximated. QUOKA leverages this observation, accelerating attention by (1) first retaining a small set of representative queries and (2) then subselectin the keys most aligned with those queries. Through experiments on Needle-In-A-Haystack, LongBench, RULER, and Math500, we show that, while realizing a 3x reduction in time-to-first-token, 5x speedup in attention on Nvidia GPUs and up to nearly a 7x speedup on Intel Xeon CPUs, QUOKA achieves near-baseline accuracy, utilizing 88% fewer key-value pairs per attention evaluation.

</details>


### [302] [Data Reconstruction: Identifiability and Optimization with Sample Splitting](https://arxiv.org/abs/2602.08723)
*Yujie Shen,Zihan Wang,Jian Qian,Qi Lei*

Main category: cs.LG

TL;DR: 论文研究从KKT条件重建训练数据的两个关键问题：可识别性和优化方法，提出多项式激活两层网络的可识别条件，并引入样本分裂优化技术提升重建性能。


<details>
  <summary>Details</summary>
Motivation: 尽管从KKT条件重建训练数据在实证上取得了显著成功，但尚不清楚KKT方程何时有唯一解，以及在可识别情况下如何通过优化可靠地恢复解。因此需要研究这两个互补问题：可识别性和优化方法。

Method: 1. 可识别性分析：讨论多项式激活两层网络KKT系统唯一确定训练数据的充分条件；2. 优化方法：引入样本分裂技术，这是一种适用于一般重建目标的曲率感知细化步骤，可创建额外的下降方向以逃离不良驻点并细化解。

Result: 实验表明，将样本分裂技术增强到多个现有重建方法中，能一致地提高重建性能。

Conclusion: 该工作为训练数据重建提供了理论解释（何时以及为什么重建是可能的）和实用的优化技术，通过样本分裂显著改进了现有方法的重建效果。

Abstract: Training data reconstruction from KKT conditions has shown striking empirical success, yet it remains unclear when the resulting KKT equations have unique solutions and, even in identifiable regimes, how to reliably recover solutions by optimization. This work hereby focuses on these two complementary questions: identifiability and optimization. On the identifiability side, we discuss the sufficient conditions for KKT system of two-layer networks with polynomial activations to uniquely determine the training data, providing a theoretical explanation of when and why reconstruction is possible. On the optimization side, we introduce sample splitting, a curvature-aware refinement step applicable to general reconstruction objectives (not limited to KKT-based formulations): it creates additional descent directions to escape poor stationary points and refine solutions. Experiments demonstrate that augmenting several existing reconstruction methods with sample splitting consistently improves reconstruction performance.

</details>


### [303] [Foundation Inference Models for Ordinary Differential Equations](https://arxiv.org/abs/2602.08733)
*Maximilian Mauel,Johannes R. Hübers,David Berghaus,Patrick Seifner,Ramses J. Sanchez*

Main category: cs.LG

TL;DR: FIM-ODE：一个预训练的基础推理模型，通过单次前向传播直接从噪声轨迹数据预测ODE向量场，实现零样本ODE推断


<details>
  <summary>Details</summary>
Motivation: 现有ODE向量场推断方法（符号回归、高斯过程回归、神经ODE）需要复杂训练流程或强烈依赖系统先验知识，缺乏简单易用的通用解决方案

Method: 提出FIM-ODE预训练模型，在低阶多项式向量场的ODE先验分布上进行预训练，使用神经算子表示目标场，通过单次前向传播从噪声轨迹预测向量场

Result: FIM-ODE在零样本性能上匹配甚至超越ODEFormer（最近的预训练符号基线），且预训练为微调提供强初始化，在适应新任务时优于现代神经和GP基线

Conclusion: FIM-ODE提供了一种简单高效的ODE推断方法，无需复杂训练流程或机器学习专业知识，通过预训练实现快速稳定适应

Abstract: Ordinary differential equations (ODEs) are central to scientific modelling, but inferring their vector fields from noisy trajectories remains challenging. Current approaches such as symbolic regression, Gaussian process (GP) regression, and Neural ODEs often require complex training pipelines and substantial machine learning expertise, or they depend strongly on system-specific prior knowledge. We propose FIM-ODE, a pretrained Foundation Inference Model that amortises low-dimensional ODE inference by predicting the vector field directly from noisy trajectory data in a single forward pass. We pretrain FIM-ODE on a prior distribution over ODEs with low-degree polynomial vector fields and represent the target field with neural operators. FIM-ODE achieves strong zero-shot performance, matching and often improving upon ODEFormer, a recent pretrained symbolic baseline, across a range of regimes despite using a simpler pretraining prior distribution. Pretraining also provides a strong initialisation for finetuning, enabling fast and stable adaptation that outperforms modern neural and GP baselines without requiring machine learning expertise.

</details>


### [304] [On the Expressive Power of GNNs for Boolean Satisfiability](https://arxiv.org/abs/2602.08745)
*Saku Peltonen,Roger Wattenhofer*

Main category: cs.LG

TL;DR: GNNs在SAT求解中的表达能力受限于WL测试层次，无法区分可满足与不可满足实例，工业实例需要更强的表达能力


<details>
  <summary>Details</summary>
Motivation: 分析图神经网络在布尔可满足性问题求解中的表达能力，理解其理论限制和实际应用需求

Method: 通过Weisfeiler-Leman测试层次分析GNN的表达能力，证明WL层次无法区分可满足与不可满足实例，并研究不同SAT实例家族的表达需求

Result: 高阶WL测试无法区分可满足与不可满足实例，这转化为WL有界求解器的实际限制；随机实例大多可区分，但工业实例需要更强的表达能力

Conclusion: GNN在SAT求解中的表达能力受WL测试限制，工业实例需要超越WL层次的表达能力才能有效预测满足赋值

Abstract: Machine learning approaches to solving Boolean Satisfiability (SAT) aim to replace handcrafted heuristics with learning-based models. Graph Neural Networks have emerged as the main architecture for SAT solving, due to the natural graph representation of Boolean formulas. We analyze the expressive power of GNNs for SAT solving through the lens of the Weisfeiler-Leman (WL) test. As our main result, we prove that the full WL hierarchy cannot, in general, distinguish between satisfiable and unsatisfiable instances. We show that indistinguishability under higher-order WL carries over to practical limitations for WL-bounded solvers that set variables sequentially. We further study the expressivity required for several important families of SAT instances, including regular, random and planar instances. To quantify expressivity needs in practice, we conduct experiments on random instances from the G4SAT benchmark and industrial instances from the International SAT Competition. Our results suggest that while random instances are largely distinguishable, industrial instances often require more expressivity to predict a satisfying assignment.

</details>


### [305] [Central Dogma Transformer II: An AI Microscope for Understanding Cellular Regulatory Mechanisms](https://arxiv.org/abs/2602.08751)
*Nobuyuki Ota*

Main category: cs.LG

TL;DR: CDT-II是一个"AI显微镜"，通过模仿中心法则的架构，其注意力机制直接可解释为调控结构，使实验生物学家能够观察自己数据中的调控网络。


<details>
  <summary>Details</summary>
Motivation: 当前生物AI模型缺乏可解释性——其内部表示不对应于研究人员可检查的生物学关系。需要开发能够直接解释调控结构的AI模型。

Method: 通过镜像中心法则的架构设计CDT-II模型：DNA自注意力对应基因组关系，RNA自注意力对应基因共调控，DNA到RNA交叉注意力对应转录控制。仅使用基因组嵌入和原始单细胞表达数据。

Result: 在K562 CRISPRi数据中，CDT-II预测扰动效应（基因平均r=0.84），无监督地恢复GFI1B调控网络（6.6倍富集，P=3.5×10^-17）。两个不同的注意力机制汇聚到一个RNA加工模块（P=1×10^-16）。

Conclusion: CDT-II建立了机制导向的AI作为任务导向方法的替代方案，揭示了调控结构而不仅仅是优化预测，为实验生物学家提供了可解释的调控网络观察工具。

Abstract: Current biological AI models lack interpretability -- their internal representations do not correspond to biological relationships that
  researchers can examine. Here we present CDT-II, an "AI microscope" whose attention maps are directly interpretable as regulatory structure.
  By mirroring the central dogma in its architecture, each attention mechanism corresponds to a specific biological relationship: DNA
  self-attention for genomic relationships, RNA self-attention for gene co-regulation, and DNA-to-RNA cross-attention for transcriptional
  control. Using only genomic embeddings and raw per-cell expression, CDT-II enables experimental biologists to observe regulatory networks in
  their own data. Applied to K562 CRISPRi data, CDT-II predicts perturbation effects (per-gene mean $r = 0.84$) and recovers the GFI1B
  regulatory network without supervision (6.6-fold enrichment, $P = 3.5 \times 10^{-17}$). Two distinct attention mechanisms converge on an RNA
  processing module ($P = 1 \times 10^{-16}$). CDT-II establishes mechanism-oriented AI as an alternative to task-oriented approaches, revealing
  regulatory structure rather than merely optimizing predictions.

</details>


### [306] [Redundancy-Free View Alignment for Multimodal Human Activity Recognition with Arbitrarily Missing Views](https://arxiv.org/abs/2602.08755)
*Duc-Anh Nguyen,Nhien-An Le-Khac*

Main category: cs.LG

TL;DR: RALIS：一种结合多视图对比学习和专家混合模块的模型，用于支持训练和推理期间任意视图可用性的人类活动识别


<details>
  <summary>Details</summary>
Motivation: 现有的多模态多视图学习方法在处理灵活的视图配置（包括任意视图组合、视图数量和异构模态）方面存在困难。特别是在人类活动识别中，需要能够适应训练和推理期间视图可用性变化的模型。

Method: RALIS结合了多视图对比学习和专家混合模块。使用调整的中心对比损失进行自监督表示学习和视图对齐，而不是重建缺失视图。该损失公式允许集成视图权重以考虑视图质量，并将计算复杂度从O(V²)降低到O(V)。专家混合模块采用专门的负载平衡策略来适应任意视图组合。

Result: RALIS在四个包含惯性和人体姿态模态的数据集上进行了验证，视图数量从3到9不等。实验结果表明该模型具有良好的性能和灵活性。

Conclusion: RALIS通过结合对比学习和专家混合模块，有效解决了多视图学习中视图可用性变化的问题，在人类活动识别任务中表现出色，并具有计算效率优势。

Abstract: Multimodal multiview learning seeks to integrate information from diverse sources to enhance task performance. Existing approaches often struggle with flexible view configurations, including arbitrary view combinations, numbers of views, and heterogeneous modalities. Focusing on the context of human activity recognition, we propose RALIS, a model that combines multiview contrastive learning with a mixture-of-experts module to support arbitrary view availability during both training and inference. Instead of trying to reconstruct missing views, an adjusted center contrastive loss is used for self-supervised representation learning and view alignment, mitigating the impact of missing views on multiview fusion. This loss formulation allows for the integration of view weights to account for view quality. Additionally, it reduces computational complexity from $O(V^2)$ to $O(V)$, where $V$ is the number of views. To address residual discrepancies not captured by contrastive learning, we employ a mixture-of-experts module with a specialized load balancing strategy, tasked with adapting to arbitrary view combinations. We highlight the geometric relationship among components in our model and how they combine well in the latent space. RALIS is validated on four datasets encompassing inertial and human pose modalities, with the number of views ranging from three to nine, demonstrating its performance and flexibility.

</details>


### [307] [HoGS: Homophily-Oriented Graph Synthesis for Local Differentially Private GNN Training](https://arxiv.org/abs/2602.08762)
*Wen Xu,Zhetao Li,Yong Xiao,Pengpeng Qiao,Mianxiong Dong,Kaoru Ota*

Main category: cs.LG

TL;DR: HoGS是一个本地差分隐私框架，通过在本地差分隐私保护下生成合成图来训练图神经网络，同时保护链接和节点特征隐私，并利用图数据的同质性现象来减少隐私保护对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在训练时可能泄露图中的敏感个人信息（链接和节点特征）。现有的本地差分隐私图神经网络要么只保护链接隐私，要么在同时保护链接和节点特征隐私时导致显著的效用损失。

Method: HoGS框架首先在本地差分隐私保护下收集图的链接和特征信息，然后利用图数据的同质性现象分别重构图结构和节点特征，生成合成图作为下游图神经网络的输入。

Result: 在三个真实世界数据集上的实验结果表明，HoGS在训练图神经网络的准确性方面显著优于基线方法。

Conclusion: HoGS提供了一个有效的本地差分隐私框架，能够在保护链接和节点特征隐私的同时，保持图神经网络训练的高效用，解决了现有方法在隐私保护和模型性能之间的权衡问题。

Abstract: Graph neural networks (GNNs) have demonstrated remarkable performance in various graph-based machine learning tasks by effectively modeling high-order interactions between nodes. However, training GNNs without protection may leak sensitive personal information in graph data, including links and node features. Local differential privacy (LDP) is an advanced technique for protecting data privacy in decentralized networks. Unfortunately, existing local differentially private GNNs either only preserve link privacy or suffer significant utility loss in the process of preserving link and node feature privacy. In this paper, we propose an effective LDP framework, called HoGS, which trains GNNs with link and feature protection by generating a synthetic graph. Concretely, HoGS first collects the link and feature information of the graph under LDP, and then utilizes the phenomenon of homophily in graph data to reconstruct the graph structure and node features separately, thereby effectively mitigating the negative impact of LDP on the downstream GNN training. We theoretically analyze the privacy guarantee of HoGS and conduct experiments using the generated synthetic graph as input to various state-of-the-art GNN architectures. Experimental results on three real-world datasets show that HoGS significantly outperforms baseline methods in the accuracy of training GNNs.

</details>


### [308] [FreqLens: Interpretable Frequency Attribution for Time Series Forecasting](https://arxiv.org/abs/2602.08768)
*Chi-Sheng Chen,Xinyu Zhang,En-Jui Kuo,Guan-Ying Chen,Qiuzhe Xie,Fan Zhang*

Main category: cs.LG

TL;DR: FreqLens是一个可解释的时间序列预测框架，通过可学习的频率发现和基于公理的频率归因，实现可解释的预测，同时保持竞争力性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测模型通常缺乏可解释性，限制了其在需要可解释预测的领域中的应用。现有方法难以自动发现主导的周期性模式并提供理论保证的归因。

Method: 提出FreqLens框架，包含两个关键创新：1) 可学习的频率发现 - 通过sigmoid映射参数化频率基，从数据中学习并带有多样性正则化，无需领域知识自动发现主导周期性模式；2) 公理化的频率归因 - 基于理论框架，满足完备性、忠实性、零频率和对称性公理，每个频率的归因等价于Shapley值。

Result: 在Traffic和Weather数据集上，FreqLens实现了竞争性或更优的性能，同时发现了物理上有意义的频率：在Traffic数据中所有5次独立运行都发现了24小时日周期（24.6±0.1h，2.5%误差）和12小时半日周期（11.8±0.1h，1.6%误差），在Weather数据中发现了比输入窗口长10倍的周周期。

Conclusion: FreqLens展示了真正的频率级知识发现能力，并在归因质量上提供了形式化的理论保证，为可解释的时间序列预测提供了有效解决方案。

Abstract: Time series forecasting models often lack interpretability, limiting their adoption in domains requiring explainable predictions. We propose \textsc{FreqLens}, an interpretable forecasting framework that discovers and attributes predictions to learnable frequency components. \textsc{FreqLens} introduces two key innovations: (1) \emph{learnable frequency discovery} -- frequency bases are parameterized via sigmoid mapping and learned from data with diversity regularization, enabling automatic discovery of dominant periodic patterns without domain knowledge; and (2) \emph{axiomatic frequency attribution} -- a theoretically grounded framework that provably satisfies Completeness, Faithfulness, Null-Frequency, and Symmetry axioms, with per-frequency attributions equivalent to Shapley values. On Traffic and Weather datasets, \textsc{FreqLens} achieves competitive or superior performance while discovering physically meaningful frequencies: all 5 independent runs discover the 24-hour daily cycle ($24.6 \pm 0.1$h, 2.5\% error) and 12-hour half-daily cycle ($11.8 \pm 0.1$h, 1.6\% error) on Traffic, and weekly cycles ($10\times$ longer than the input window) on Weather. These results demonstrate genuine frequency-level knowledge discovery with formal theoretical guarantees on attribution quality.

</details>


### [309] [Default Machine Learning Hyperparameters Do Not Provide Informative Initialization for Bayesian Optimization](https://arxiv.org/abs/2602.08774)
*Nicolás Villagrán Prieto,Eduardo C. Garrido-Merchán*

Main category: cs.LG

TL;DR: 贝叶斯优化中使用库默认超参数作为初始化点并不能显著提升优化效果，与随机初始化相比无统计显著优势


<details>
  <summary>Details</summary>
Motivation: 研究是否可以利用机器学习库（如scikit-learn）中默认超参数值所隐含的专家知识来加速贝叶斯优化的收敛，这一直观假设尚未得到充分验证

Method: 使用以库默认值为中心的高斯分布采样初始化BO，与均匀随机初始化对比；在三个BO后端、三个模型家族和五个基准数据集上进行广泛实验；通过收敛速度和最终预测质量评估性能，使用单边二项检验确定统计显著性

Result: 在所有实验条件下，默认值初始化相比纯随机采样没有统计显著优势（p值范围0.141-0.908）；虽然更紧密围绕默认值的先验方差能改善早期评估，但这种暂时优势随优化进展而消失，最终性能不变

Conclusion: 默认超参数并未包含对优化有用的方向性信息；建议将超参数调优作为模型开发的必要部分，采用基于数据的搜索策略而非依赖库默认值的启发式方法

Abstract: Bayesian Optimization (BO) is a standard tool for hyperparameter tuning thanks to its sample efficiency on expensive black-box functions. While most BO pipelines begin with uniform random initialization, default hyperparameter values shipped with popular ML libraries such as scikit-learn encode implicit expert knowledge and could serve as informative starting points that accelerate convergence. This hypothesis, despite its intuitive appeal, has remained largely unexamined. We formalize the idea by initializing BO with points drawn from truncated Gaussian distributions centered at library defaults and compare the resulting trajectories against a uniform-random baseline. We conduct an extensive empirical evaluation spanning three BO back-ends (BoTorch, Optuna, Scikit-Optimize), three model families (Random Forests, Support Vector Machines, Multilayer Perceptrons), and five benchmark datasets covering classification and regression tasks. Performance is assessed through convergence speed and final predictive quality, and statistical significance is determined via one-sided binomial tests. Across all conditions, default-informed initialization yields no statistically significant advantage over purely random sampling, with p-values ranging from 0.141 to 0.908. A sensitivity analysis on the prior variance confirms that, while tighter concentration around the defaults improves early evaluations, this transient benefit vanishes as optimization progresses, leaving final performance unchanged. Our results provide no evidence that default hyperparameters encode useful directional information for optimization. We therefore recommend that practitioners treat hyperparameter tuning as an integral part of model development and favor principled, data-driven search strategies over heuristic reliance on library defaults.

</details>


### [310] [A Graphop Analysis of Graph Neural Networks on Sparse Graphs: Generalization and Universal Approximation](https://arxiv.org/abs/2602.08785)
*Ofek Amran,Tom Gilat,Ron Levie*

Main category: cs.LG

TL;DR: 提出统一度量空间框架，将任意大小的稀疏和稠密图纳入同一理论体系，扩展MPNN的泛化与逼近能力分析


<details>
  <summary>Details</summary>
Motivation: 现有MPNN理论存在局限性：分析无界大小图时仅适用于稠密图，而分析稀疏图时又限制在固定大小图集。需要统一框架同时处理任意大小的稀疏和稠密图

Method: 基于图算子(graphop)分析理论，定义包含所有大小图的紧致度量空间，在该度量下证明MPNN的Hölder连续性

Result: 获得比先前工作更强大的通用逼近定理和泛化边界，统一处理稀疏和稠密图的理论框架

Conclusion: 提出的统一度量空间框架克服了现有MPNN理论的局限性，为任意大小图的MPNN分析提供了更强大的理论基础

Abstract: Generalization and approximation capabilities of message passing graph neural networks (MPNNs) are often studied by defining a compact metric on a space of input graphs under which MPNNs are Hölder continuous. Such analyses are of two varieties: 1) when the metric space includes graphs of unbounded sizes, the theory is only appropriate for dense graphs, and, 2) when studying sparse graphs, the metric space only includes graphs of uniformly bounded size. In this work, we present a unified approach, defining a compact metric on the space of graphs of all sizes, both sparse and dense, under which MPNNs are Hölder continuous. This leads to more powerful universal approximation theorems and generalization bounds than previous works. The theory is based on, and extends, a recent approach to graph limit theory called graphop analysis.

</details>


### [311] [How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs](https://arxiv.org/abs/2602.08808)
*Yapei Chang,Kyle Lo,Mohit Iyyer,Luca Soldaini*

Main category: cs.LG

TL;DR: How2Everything框架：从网页挖掘35万条流程数据，构建7千条评估集，开发基于LLM的评分协议，用于评估和改进目标导向的流程生成能力，并通过强化学习提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 生成逐步"如何做"流程是LLM的关键能力，但在真实任务中大规模评估和改进程序有效性仍具挑战且研究不足。需要可扩展的框架来评估和改进目标导向的流程生成。

Method: 1) How2Mine：从98万网页挖掘35.1万条流程数据；2) How2Bench：构建7千条平衡评估集；3) How2Score：开发基于LLM的评分协议，检测生成中是否包含阻碍目标实现的关键失败；4) 将前沿模型蒸馏为8B开源模型用于低成本评估；5) 使用How2Score作为奖励进行强化学习。

Result: 1) 蒸馏模型与人类标注者达到80.5%一致性；2) How2Bench显示模型规模和训练阶段的明显扩展趋势；3) 强化学习使三个模型在How2Bench上提升>10分，且不影响标准基准测试性能；4) 改进对表面记忆或格式合规具有鲁棒性。

Conclusion: How2Everything展示了预训练网络数据如何支持大规模能力评估和改进的闭环，为流程生成能力的系统评估和提升提供了可扩展框架。

Abstract: Generating step-by-step "how-to" procedures is a key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything, a scalable framework to evaluate and improve goal-conditioned procedure generation. Our framework includes How2Mine, which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench, a 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score, an evaluation protocol that uses an LLM judge to detect whether a generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill a frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining. Finally, RL using How2Score as a reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support a closed loop of capability evaluation and improvement at scale.

</details>


### [312] [Efficient Deep Learning for Biometrics: Overview, Challenges and Trends in Ear of Frugal AI](https://arxiv.org/abs/2602.08809)
*Karim Haroun,Aya Zitouni,Aicha Zenakhri,Meriem Amel Guessoum,Larbi Boubchir*

Main category: cs.LG

TL;DR: 该论文简要综述了生物识别应用中高效的深度学习方法，讨论了训练和部署挑战，提供了效率分类，并倡导使用统一可复现的评估指标。


<details>
  <summary>Details</summary>
Motivation: 深度学习在安全防御等应用中取得进展，但其训练和部署的高计算需求导致高能耗和碳足迹，限制了在资源受限边缘设备上的实时应用，因此需要研究高效的深度学习方法。

Method: 采用文献综述方法，对生物识别应用中的高效深度学习技术进行系统梳理，提出分类体系，并讨论包括内存、计算、延迟、吞吐量在内的补充评估指标。

Result: 建立了高效深度学习方法的分类体系，识别了训练和部署过程中的主要挑战，提出了更全面的评估指标框架，并为未来研究方向提供了建议。

Conclusion: 需要开发更高效的深度学习模型以降低能耗和碳足迹，同时需要建立统一、可复现的评估标准来促进该领域的发展，特别是在资源受限的边缘设备生物识别应用中。

Abstract: Recent advances in deep learning, whether on discriminative or generative tasks have been beneficial for various applications, among which security and defense. However, their increasing computational demands during training and deployment translates directly into high energy consumption. As a consequence, this induces a heavy carbon footprint which hinders their widespread use and scalability, but also a limitation when deployed on resource-constrained edge devices for real-time use. In this paper, we briefly survey efficient deep learning methods for biometric applications. Specifically, we tackle the challenges one might incur when training and deploying deep learning approaches, and provide a taxonomy of the various efficient deep learning families. Additionally, we discuss complementary metrics for evaluating the efficiency of these models such as memory, computation, latency, throughput, and advocate for universal and reproducible metrics for better comparison. Last, we give future research directions to consider.

</details>


### [313] [$\texttt{lrnnx}$: A library for Linear RNNs](https://arxiv.org/abs/2602.08810)
*Karan Bania,Soham Kalburgi,Manit Tanwar,Dhruthi,Aditya Nagarsekar,Harshvardhan Mestha,Naman Chibber,Raj Deshmukh,Anish Sathyanarayanan,Aarush Rathore,Pratham Chheda*

Main category: cs.LG

TL;DR: lrnnx是一个统一软件库，实现了多种现代线性循环神经网络架构，提供通用接口以解决现有实现碎片化问题


<details>
  <summary>Details</summary>
Motivation: 现有线性循环神经网络实现分散在不同软件框架中，依赖框架特定优化，有些需要自定义CUDA内核或缺乏公开代码，导致使用、比较或扩展LRNN需要大量实现工作

Method: 开发lrnnx统一软件库，实现多种现代LRNN架构，提供通用接口，暴露多个控制级别，允许用户直接使用核心组件或高层模型抽象

Result: 创建了开源软件库lrnnx，采用MIT许可，提高了LRNN研究的可访问性、可复现性和可扩展性

Conclusion: lrnnx解决了LRNN实现碎片化问题，为研究者和应用开发者提供了统一的工具，促进了线性循环神经网络领域的发展

Abstract: Linear recurrent neural networks (LRNNs) provide a structured approach to sequence modeling that bridges classical linear dynamical systems and modern deep learning, offering both expressive power and theoretical guarantees on stability and trainability. In recent years, multiple LRNN-based architectures have been proposed, each introducing distinct parameterizations, discretization schemes, and implementation constraints. However, existing implementations are fragmented across different software frameworks, often rely on framework-specific optimizations, and in some cases require custom CUDA kernels or lack publicly available code altogether. As a result, using, comparing, or extending LRNNs requires substantial implementation effort. To address this, we introduce $\texttt{lrnnx}$, a unified software library that implements several modern LRNN architectures under a common interface. The library exposes multiple levels of control, allowing users to work directly with core components or higher-level model abstractions. $\texttt{lrnnx}$ aims to improve accessibility, reproducibility, and extensibility of LRNN research and applications. We make our code available under a permissive MIT license.

</details>


### [314] [Robust Policy Optimization to Prevent Catastrophic Forgetting](https://arxiv.org/abs/2602.08813)
*Mahdi Sabbaghi,George Pappas,Adel Javanmard,Hamed Hassani*

Main category: cs.LG

TL;DR: FRPO是一种鲁棒的RLHF框架，通过在KL有界邻域内优化奖励来防止下游微调时的灾难性遗忘，保持安全性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF训练的大语言模型在后续微调时容易出现灾难性遗忘，特别是安全性等已学习行为会退化。标准RLHF目标无法保证对未来适应的鲁棒性。

Method: 提出Fine-tuning Robust Policy Optimization (FRPO)，通过max-min公式在KL有界邻域内优化奖励，确保策略在标准微调下的奖励稳定性。基于GRPO修改，无需额外计算。

Result: FRPO显著减少了多个基础模型在不同下游微调机制（SFT和RL）下的安全性退化，同时保持下游任务性能。在数学RL设置中，FRPO也能在后续微调下保持准确性。

Conclusion: FRPO通过预微调鲁棒性优化，解决了RLHF模型在下游微调时的灾难性遗忘问题，为构建更鲁棒的语言模型提供了有效框架。

Abstract: Large language models are commonly trained through multi-stage post-training: first via RLHF, then fine-tuned for other downstream objectives. Yet even small downstream updates can compromise earlier learned behaviors (e.g., safety), exposing a brittleness known as catastrophic forgetting. This suggests standard RLHF objectives do not guarantee robustness to future adaptation. To address it, most prior work designs downstream-time methods to preserve previously learned behaviors. We argue that preventing this requires pre-finetuning robustness: the base policy should avoid brittle high-reward solutions whose reward drops sharply under standard fine-tuning.
  We propose Fine-tuning Robust Policy Optimization (FRPO), a robust RLHF framework that optimizes reward not only at the current policy, but across a KL-bounded neighborhood of policies reachable by downstream adaptation. The key idea is to ensure reward stability under policy shifts via a max-min formulation. By modifying GRPO, we develop an algorithm with no extra computation, and empirically show it substantially reduces safety degradation across multiple base models and downstream fine-tuning regimes (SFT and RL) while preserving downstream task performance. We further study a math-focused RL setting, demonstrating that FRPO preserves accuracy under subsequent fine-tuning.

</details>


### [315] [Permissive-Washing in the Open AI Supply Chain: A Large-Scale Audit of License Integrity](https://arxiv.org/abs/2602.08816)
*James Jewitt,Gopi Krishnan Rajbahadur,Hao Li,Bram Adams,Ahmed E. Hassan*

Main category: cs.LG

TL;DR: 研究发现开源AI领域存在严重的"许可清洗"现象：96.5%的数据集和95.8%的模型缺少必要的许可文本，只有极少数满足完整的许可要求，导致所谓的"自由使用"标签实际上不可操作。


<details>
  <summary>Details</summary>
Motivation: 开源AI项目虽然使用MIT、Apache-2.0等宽松许可证，但这些许可证包含必须满足的强制性要求（如包含完整许可文本、版权声明等）。当前缺乏对这些要求的大规模验证，导致用户可能在不满足条件的情况下使用AI资产，从而面临法律风险。

Method: 对124,278个数据集→模型→应用程序供应链进行实证审计，涵盖Hugging Face和GitHub上的3,338个数据集、6,664个模型和28,516个应用程序。检查许可文本、版权声明和上游归属的传播情况。

Result: 惊人的发现：96.5%的数据集和95.8%的模型缺少所需许可文本；只有2.3%的数据集和3.2%的模型同时满足许可文本和版权要求；即使上游资产提供完整许可证据，归属也很少向下游传播：只有27.59%的模型保留合规的数据集声明，仅5.75%的应用程序保留合规的模型声明。

Conclusion: 从业者不能假设宽松许可证标签能提供其声称的权利：许可文件和声明（而非元数据）才是法律真相的来源。开源AI领域存在普遍的"许可清洗"问题，需要更严格的合规实践。

Abstract: Permissive licenses like MIT, Apache-2.0, and BSD-3-Clause dominate open-source AI, signaling that artifacts like models, datasets, and code can be freely used, modified, and redistributed. However, these licenses carry mandatory requirements: include the full license text, provide a copyright notice, and preserve upstream attribution, that remain unverified at scale. Failure to meet these conditions can place reuse outside the scope of the license, effectively leaving AI artifacts under default copyright for those uses and exposing downstream users to litigation. We call this phenomenon ``permissive washing'': labeling AI artifacts as free to use, while omitting the legal documentation required to make that label actionable. To assess how widespread permissive washing is in the AI supply chain, we empirically audit 124,278 dataset $\rightarrow$ model $\rightarrow$ application supply chains, spanning 3,338 datasets, 6,664 models, and 28,516 applications across Hugging Face and GitHub. We find that an astonishing 96.5\% of datasets and 95.8\% of models lack the required license text, only 2.3\% of datasets and 3.2\% of models satisfy both license text and copyright requirements, and even when upstream artifacts provide complete licensing evidence, attribution rarely propagates downstream: only 27.59\% of models preserve compliant dataset notices and only 5.75\% of applications preserve compliant model notices (with just 6.38\% preserving any linked upstream notice). Practitioners cannot assume permissive labels confer the rights they claim: license files and notices, not metadata, are the source of legal truth. To support future research, we release our full audit dataset and reproducible pipeline.

</details>


### [316] [Kirin: Improving ANN efficiency with SNN Hybridization](https://arxiv.org/abs/2602.08817)
*Chenyu Wang,Zhanglu Yan,Zhi Zhou,Xu Chen,Weng-Fai Wong*

Main category: cs.LG

TL;DR: Kirin提出了一种整数和脉冲混合的SNN架构，实现了准确率无损的ANN到SNN转换，在W4A4&8量化设置下达到接近FP16精度，能耗降低84.66%，时间步缩短93.75%。


<details>
  <summary>Details</summary>
Motivation: 人工神经网络（特别是大语言模型）推理能力强但能耗高，而脉冲神经网络具有出色的能效但存在转换挑战：高比特量化值需要更长时间窗口增加延迟，以及单脉冲方案信息损失与多脉冲方案能耗之间的权衡。

Method: 提出Kirin混合SNN架构：1）脉冲矩阵混合策略，将导致小时间窗口的低比特参数编码为二进制脉冲，其余保留为整数格式；2）静默阈值机制调节单脉冲发射时机，确保输出与LLM数学等价。

Result: 在W4A4&8量化设置下，Kirin达到接近FP16的准确率，同时能耗降低84.66%，时间步缩短93.75%，实现了准确率无损的高效ANN到SNN转换。

Conclusion: Kirin通过整数和脉冲混合设计成功解决了ANN到SNN转换中的延迟和能效权衡问题，为高效部署大语言模型提供了可行的SNN解决方案。

Abstract: Artificial neural networks (ANNs), particularly large language models (LLMs), demonstrate powerful inference capabilities but consume substantial energy. Conversely, spiking neural networks (SNNs) exhibit exceptional energy efficiency due to their binary and event-driven characteristics, thus motivating the study of ANN-to-SNN conversion. In this process, quantization plays a pivotal role, mapping LLMs' floating-point parameters to discrete SNN parameters via the temporal dimension of the time window. However, several challenges remain in the conversion process: (i) converting high bit-width quantization values into binary spikes requires longer time windows, increasing system latency; and (ii) the inherent trade-off between the information loss of single-spike schemes and the energy costs of multi-spike ones in SNN. To address these challenges, we propose Kirin, a integer and spike hybrid based SNN to achieve accuracy lossless ANN-to-SNN conversion with time and energy efficiency. Specifically, we first propose a Spike Matrix Hybridization strategy that encoding low bit-width parameters that leading to small time window size into binary spikes while preserving the rest in integer format, thereby reducing the overall latency of SNN execution. Second, we introduce a silence threshold mechanism to regulate the timing of single-spike firing, ensuring the output is mathematically equivalent to the LLM's output and preserves accuracy. Experimental results demonstrate that Kirin, under a W4A4\&8 quantization setting, achieves near-FP16 accuracy while reducing energy consumption by up to 84.66\% and shortening time steps by 93.75\%.

</details>


### [317] [FlexMoRE: A Flexible Mixture of Rank-heterogeneous Experts for Efficient Federatedly-trained Large Language Models](https://arxiv.org/abs/2602.08818)
*Annemette Brok Pirchert,Jacob Nielsen,Mogens Henrik From,Lukas Galke Poech,Peter Schneider-Kamp*

Main category: cs.LG

TL;DR: FlexMoRE是一种灵活的混合专家模型，允许使用不同秩的低秩适配器或完整专家，在保持性能的同时大幅减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有混合专家架构通常训练完整大小的专家模型，但作者认为并非所有领域都需要完整专家，低秩适配器可能就足够了。这可以显著提高内存效率。

Method: 提出FlexMoRE（灵活混合秩异构专家）架构，专家可以是完整模型或不同秩的低秩适配器。基于FlexOlmo构建，将其预训练专家转换为低秩版本，系统研究专家秩与下游任务性能的权衡。

Result: 回归分析显示，推理密集型任务需要更高秩的专家，而知识密集型任务需要较低秩。使用最优秩时，FlexMoRE在参数减少三分之二（10.75B vs 33.27B）的情况下，性能反而更好（平均分47.18 vs 45.46）。

Conclusion: FlexMoRE通过灵活混合不同秩的专家，在显著减少参数的同时提高了下游任务性能，证明了低秩适配器在混合专家架构中的有效性。

Abstract: Recent advances in mixture-of-experts architectures have shown that individual experts models can be trained federatedly, i.e., in isolation from other experts by using a common base model to facilitate coordination. However, we hypothesize that full-sized experts may not be necessary for all domains and that instead low-rank adapters may be sufficient. Here, we introduce FlexMoRE, a Flexible Mixture of Rank-heterogenous Experts, which may be either full-sized experts or adapters of a suitable rank. We systematically investigate the trade-off between expert rank and downstream task performance by evaluating $6$ experts with ranks $2^0$ to $2^{14}$ resulting in experiments covering 150 mixtures (96 with 2 experts, 54 with 7 experts) that are evaluated across $120$ tasks. For our experiments, we build on FlexOlmo and turn its pre-trained experts into low-rank versions. Our regression analysis from expert rank to downstream task performance reveals that the best-performing rank is substantially higher for reasoning-heavy benchmarks than for knowledge-heavy benchmarks. These findings on rank sensitivity come with direct implications for memory efficiency: Using optimal ranks, FlexMoRE yields improved downstream task performance (average score $47.18$) compared to the baseline FlexOlmo-style mixture of full-sized experts (average score $45.46$) at less than one third the parameters ($10.75$B for FlexMoRE vs. $33.27$B for FlexOlmo). All code will be made available.

</details>


### [318] [Bayesian Preference Learning for Test-Time Steerable Reward Models](https://arxiv.org/abs/2602.08819)
*Jiwoo Hong,Shao Tang,Zhipeng Wang*

Main category: cs.LG

TL;DR: 提出Variational In-Context Reward Modeling (ICRM)，一种贝叶斯奖励建模方法，通过上下文偏好演示实现测试时的可调控性，在单目标和多目标对齐中都能适应未见过的偏好分布。


<details>
  <summary>Details</summary>
Motivation: 随着强化学习应用于可验证奖励和多目标对齐等场景，奖励模型需要编码更复杂、多方面的偏好分布。然而，传统的分类器奖励模型一旦训练完成就保持静态，限制了其在测试时的适应性。

Method: 提出ICRM方法，将奖励建模视为在Bradley-Terry模型下使用共轭Beta先验对潜在偏好概率进行摊销变分推断。该方法通过上下文偏好演示实现测试时的可调控性。

Result: ICRM在单目标设置中，随着更多上下文演示，在SafeRLHF上获得34%准确率提升，在RM-Bench上获得9%准确率提升；在多目标设置中，在有用性和拒绝基准上获得4%超体积增益，扩展了帕累托前沿。在数学推理中有效编码可验证奖励，优于传统奖励模型。

Conclusion: ICRM通过上下文偏好演示实现了奖励模型在测试时的可调控性，能够适应未见过的偏好分布，为强化学习训练提供了实用的奖励建模方法。理论分析表明变分目标具有全局内部最优解，KL正则化能缓解奖励过度优化问题。

Abstract: Reward models are central to aligning language models with human preferences via reinforcement learning (RL). As RL is increasingly applied to settings such as verifiable rewards and multi-objective alignment, RMs are expected to encode more complex and multifaceted preference distributions. However, classifier RMs remain static once trained, limiting their adaptability at test time. We propose Variational In-Context Reward Modeling (ICRM), a novel Bayesian reward modeling objective that enables test-time steerability via in-context preference demonstrations. ICRM casts reward modeling as amortized variational inference over a latent preference probability under the Bradley-Terry model using a conjugate Beta prior. We show that ICRM adapt to unseen preference distributions at test time for both single and multi-objective settings. With more in-context demonstrations, ICRM gains 34% accuracy on SafeRLHF and 9% accuracy on RM-Bench in the single-objective setting, while widening the Pareto frontier with a 4% gain in hypervolume on helpfulness and refusal benchmarks. We further study the practical applicability of ICRM for RL training, showing that it can effectively encode verifiable rewards by outperforming a conventional RM in math reasoning. Finally, we provide theoretical guarantees that the variational objective admits a global interior optimum with finite confidence, and we analyze how KL regularization mitigates reward over-optimization.

</details>


### [319] [Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems](https://arxiv.org/abs/2602.08847)
*Lang Feng,Longtao Zheng,Shuo He,Fuxiang Zhang,Bo An*

Main category: cs.LG

TL;DR: 提出Dr. MAS方法，通过按智能体归一化优势值来解决多智能体LLM系统中强化学习训练不稳定的问题，显著提升数学推理和多轮搜索任务的性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM系统通过角色专业化实现高级推理和工具使用，但现有的群体强化学习方法在扩展到多智能体系统时存在训练不稳定的问题。研究发现全局归一化基线可能偏离不同智能体的奖励分布，导致梯度范数不稳定。

Method: 提出Dr. MAS方法，采用智能体级别的修正：使用每个智能体自身的奖励统计量对优势值进行归一化，从而校准梯度尺度。该方法还提供了一个端到端的RL训练框架，支持可扩展编排、灵活的按智能体LLM服务和优化配置，以及共享资源调度。

Result: 在数学推理和多轮搜索基准测试中使用Qwen2.5和Qwen3系列模型进行评估。Dr. MAS相比原始GRPO在数学任务上平均提升5.6%，通过率提升4.6%；在搜索任务上平均提升15.2%，通过率提升13.1%，同时大幅消除梯度尖峰。在异构智能体模型分配下仍保持高效。

Conclusion: Dr. MAS通过智能体级别的优势归一化有效解决了多智能体LLM系统中强化学习训练不稳定的问题，提供了稳定高效的训练方案，显著提升了多智能体系统的性能表现。

Abstract: Multi-agent LLM systems enable advanced reasoning and tool use via role specialization, yet reliable reinforcement learning (RL) post-training for such systems remains difficult. In this work, we theoretically pinpoint a key reason for training instability when extending group-based RL to multi-agent LLM systems. We show that under GRPO-style optimization, a global normalization baseline may deviate from diverse agents' reward distributions, which ultimately leads to gradient-norm instability. Based on this finding, we propose Dr. MAS, a simple and stable RL training recipe for multi-agent LLM systems. Dr. MAS uses an agent-wise remedy: normalizing advantages per agent using each agent's own reward statistics, which calibrates gradient scales and dramatically stabilizes training, both theoretically and empirically. Beyond the algorithm, Dr. MAS provides an end-to-end RL training framework for multi-agent LLM systems, supporting scalable orchestration, flexible per-agent LLM serving and optimization configs, and shared resource scheduling of LLM actor backends. We evaluate Dr. MAS on multi-agent math reasoning and multi-turn search benchmarks using Qwen2.5 and Qwen3 series models. Dr. MAS achieves clear gains over vanilla GRPO (e.g., +5.6\% avg@16 and +4.6\% pass@16 on math, and +15.2\% avg@16 and +13.1\% pass@16 on search) while largely eliminating gradient spikes. Moreover, it remains highly effective under heterogeneous agent-model assignments while improving efficiency.

</details>


### [320] [Rethinking Graph Generalization through the Lens of Sharpness-Aware Minimization](https://arxiv.org/abs/2602.08855)
*Yang Qiu,Yixiong Zou,Jun Wang*

Main category: cs.LG

TL;DR: 该论文提出能量驱动的生成增强框架(E2A)，通过能量引导的潜在扰动生成伪OOD样本来解决图神经网络中的最小偏移翻转(MSF)现象，提升图OOD泛化能力。


<details>
  <summary>Details</summary>
Motivation: 图神经网络(GNNs)在各种图任务中取得了显著成功，但对分布偏移高度敏感。论文关注图泛化中普遍但未被充分探索的最小偏移翻转(MSF)现象，即测试样本仅轻微偏离训练分布就被错误分类。需要理解这一现象并提升GNNs的OOD泛化能力。

Method: 1. 从锐度感知最小化(SAM)视角重新审视MSF，引入局部鲁棒半径概念量化损失锐度；2. 提出能量驱动生成增强框架(E2A)，利用能量引导的潜在扰动生成伪OOD样本；3. 建立能量公式与鲁棒半径的理论关联，提供可处理的平坦度和稳定性建模目标。

Result: 在多个基准测试上的广泛实验表明，E2A能持续改进图OOD泛化性能，优于现有最先进的基线方法。

Conclusion: 论文通过理论分析和能量驱动的生成增强框架，有效解决了图神经网络中的最小偏移翻转现象，显著提升了模型在分布偏移下的泛化能力，为图OOD泛化提供了新的解决方案。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success across various graph-based tasks but remain highly sensitive to distribution shifts. In this work, we focus on a prevalent yet under-explored phenomenon in graph generalization, Minimal Shift Flip (MSF),where test samples that slightly deviate from the training distribution are abruptly misclassified. To interpret this phenomenon, we revisit MSF through the lens of Sharpness-Aware Minimization (SAM), which characterizes the local stability and sharpness of the loss landscape while providing a theoretical foundation for modeling generalization error. To quantify loss sharpness, we introduce the concept of Local Robust Radius, measuring the smallest perturbation required to flip a prediction and establishing a theoretical link between local stability and generalization. Building on this perspective, we further observe a continual decrease in the robust radius during training, indicating weakened local stability and an increasingly sharp loss landscape that gives rise to MSF. To jointly solve the MSF phenomenon and the intractability of radius, we develop an energy-based formulation that is theoretically proven to be monotonically correlated with the robust radius, offering a tractable and principled objective for modeling flatness and stability. Building on these insights, we propose an energy-driven generative augmentation framework (E2A) that leverages energy-guided latent perturbations to generate pseudo-OOD samples and enhance model generalization. Extensive experiments across multiple benchmarks demonstrate that E2A consistently improves graph OOD generalization, outperforming state-of-the-art baselines.

</details>


### [321] [Discovering Interpretable Algorithms by Decompiling Transformers to RASP](https://arxiv.org/abs/2602.08857)
*Xinting Huang,Aleksandra Bakalova,Satwik Bhattamishra,William Merrill,Michael Hahn*

Main category: cs.LG

TL;DR: 提出一种从训练好的Transformer中提取RASP程序的方法，通过因果干预发现最小的有效子程序，为Transformer内部实现简单可解释程序提供直接证据


<details>
  <summary>Details</summary>
Motivation: 尽管先前研究表明Transformer的计算可以用RASP编程语言模拟，且Transformer在具有简单RASP程序的问题上能够精确长度泛化，但尚不清楚训练好的模型是否真的实现了简单可解释的程序

Method: 将Transformer忠实重参数化为RASP程序，然后应用因果干预来发现最小的有效子程序

Result: 在小型Transformer上对算法和形式语言任务的实验表明，该方法通常能从长度泛化的Transformer中恢复简单可解释的RASP程序

Conclusion: 这是迄今为止最直接的证据，表明Transformer内部实现了简单的RASP程序

Abstract: Recent work has shown that the computations of Transformers can be simulated in the RASP family of programming languages. These findings have enabled improved understanding of the expressive capacity and generalization abilities of Transformers. In particular, Transformers have been suggested to length-generalize exactly on problems that have simple RASP programs. However, it remains open whether trained models actually implement simple interpretable programs. In this paper, we present a general method to extract such programs from trained Transformers. The idea is to faithfully re-parameterize a Transformer as a RASP program and then apply causal interventions to discover a small sufficient sub-program. In experiments on small Transformers trained on algorithmic and formal language tasks, we show that our method often recovers simple and interpretable RASP programs from length-generalizing transformers. Our results provide the most direct evidence so far that Transformers internally implement simple RASP programs.

</details>


### [322] [Magnitude Distance: A Geometric Measure of Dataset Similarity](https://arxiv.org/abs/2602.08859)
*Sahel Torkamani,Henry Gouk,Rik Sarkar*

Main category: cs.LG

TL;DR: 提出了一种基于度量空间magnitude概念的新型数据集距离度量——magnitude distance，包含可调尺度参数t，在生成模型训练中表现良好


<details>
  <summary>Details</summary>
Motivation: 量化数据集之间的距离是数学和机器学习中的基本问题，现有距离度量在高维设置下可能失去判别性，需要一种能捕捉不同尺度结构的新型距离度量

Method: 基于度量空间的magnitude概念定义magnitude distance，引入可调尺度参数t控制对全局结构（小t）和细节（大t）的敏感性，证明其理论性质，并将其作为推前生成模型的训练目标

Result: 证明了magnitude distance的理论性质，包括尺度极限行为和满足关键度量性质的条件；在高维设置中，当尺度适当调整时仍保持判别性；实验表明该距离提供有意义的信号，与现有基于距离的生成方法相当

Conclusion: magnitude distance是一种有理论保证的新型数据集距离度量，能有效捕捉不同尺度结构，在高维设置中保持判别性，适用于生成模型训练，为数据集比较提供了新工具

Abstract: Quantifying the distance between datasets is a fundamental question in mathematics and machine learning. We propose \textit{magnitude distance}, a novel distance metric defined on finite datasets using the notion of the \emph{magnitude} of a metric space. The proposed distance incorporates a tunable scaling parameter, $t$, that controls the sensitivity to global structure (small $t$) and finer details (large $t$). We prove several theoretical properties of magnitude distance, including its limiting behavior across scales and conditions under which it satisfies key metric properties. In contrast to classical distances, we show that magnitude distance remains discriminative in high-dimensional settings when the scale is appropriately tuned. We further demonstrate how magnitude distance can be used as a training objective for push-forward generative models. Our experimental results support our theoretical analysis and demonstrate that magnitude distance provides meaningful signals, comparable to established distance-based generative approaches.

</details>


### [323] [Near-optimal Swap Regret Minimization for Convex Losses](https://arxiv.org/abs/2602.08862)
*Lunjia Hu,Jon Schneider,Yifan Wu*

Main category: cs.LG

TL;DR: 提出随机在线算法，在单位区间上针对自适应选择的Lipschitz凸损失函数，实现近乎最优的$\widetilde O(\sqrt T)$期望交换遗憾，改进先前$\widetilde O(T^{2/3})$结果，并高效运行。


<details>
  <summary>Details</summary>
Motivation: 解决Fishelson等人[2025b]提出的开放问题：如何改进在线学习中的交换遗憾边界。先前最佳结果为$\widetilde O(T^{2/3})$，希望达到理论最优的$\widetilde O(\sqrt T)$边界。

Method: 提出多尺度分箱技术：将单位区间在不同粒度尺度上离散化为多个箱子，同时使用所有尺度进行随机预测。算法在多项式时间内运行。

Result: 1. 实现了$\widetilde O(\sqrt T)$期望交换遗憾，改进先前$\widetilde O(T^{2/3})$结果；2. 算法高效运行于$\mathsf{poly}(T)$时间；3. 应用于可引出属性的校准误差最小化，无需先前工作所需的识别函数Lipschitz假设，首次实现中位数校准的$\widetilde O(\sqrt T)$校准误差保证。

Conclusion: 本文提出了一个高效随机在线算法，通过多尺度分箱技术实现了近乎最优的交换遗憾边界，解决了开放问题，并扩展了校准误差最小化的应用范围，特别适用于中位数校准等先前无法处理的情况。

Abstract: We give a randomized online algorithm that guarantees near-optimal $\widetilde O(\sqrt T)$ expected swap regret against any sequence of $T$ adaptively chosen Lipschitz convex losses on the unit interval. This improves the previous best bound of $\widetilde O(T^{2/3})$ and answers an open question of Fishelson et al. [2025b]. In addition, our algorithm is efficient: it runs in $\mathsf{poly}(T)$ time. A key technical idea we develop to obtain this result is to discretize the unit interval into bins at multiple scales of granularity and simultaneously use all scales to make randomized predictions, which we call multi-scale binning and may be of independent interest. A direct corollary of our result is an efficient online algorithm for minimizing the calibration error for general elicitable properties. This result does not require the Lipschitzness assumption of the identification function needed in prior work, making it applicable to median calibration, for which we achieve the first $\widetilde O(\sqrt T)$ calibration error guarantee.

</details>


### [324] [AnomSeer: Reinforcing Multimodal LLMs to Reason for Time-Series Anomaly Detection](https://arxiv.org/abs/2602.08868)
*Junru Zhang,Lang Feng,Haoran Shi,Xu Guo,Han Yu,Yabo Dong,Duanqing Xu*

Main category: cs.LG

TL;DR: AnomSeer：基于时间序列细粒度推理的多模态大语言模型异常检测框架，通过专家思维链和TimerPO优化，在分类、定位和解释方面优于GPT-4o等大型模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于多模态大语言模型的时间序列异常检测方法依赖粗粒度启发式规则，缺乏对多维复杂时间序列数据的细粒度推理能力，这限制了模型在理解复杂时间序列模式、进行精确异常分类和定位方面的性能。

Method: 1. 生成专家思维链轨迹：基于经典分析方法（统计度量、频率变换）提供可验证的细粒度推理；2. 提出时间序列接地策略优化（TimerPO）：包含基于最优传输的时间序列接地优势和正交投影组件，确保细粒度信号不干扰主要检测目标。

Result: 在多样化异常场景下，使用Qwen2.5-VL-3B/7B-Instruct的AnomSeer在分类和定位准确率上优于GPT-4o等大型商业基线，特别是在点和频率驱动异常方面表现突出，并能生成支持结论的合理时间序列推理轨迹。

Conclusion: AnomSeer通过将模型推理基于时间序列的精确结构细节，统一了异常分类、定位和解释，解决了MLLMs在复杂时间序列数据细粒度推理方面的局限性，为时间序列异常检测提供了更可靠、可解释的解决方案。

Abstract: Time-series anomaly detection (TSAD) with multimodal large language models (MLLMs) is an emerging area, yet a persistent challenge remains: MLLMs rely on coarse time-series heuristics but struggle with multi-dimensional, detailed reasoning, which is vital for understanding complex time-series data. We present AnomSeer to address this by reinforcing the model to ground its reasoning in precise, structural details of time series, unifying anomaly classification, localization, and explanation. At its core, an expert chain-of-thought trace is generated to provide a verifiable, fine-grained reasoning from classical analyses (e.g., statistical measures, frequency transforms). Building on this, we propose a novel time-series grounded policy optimization (TimerPO) that incorporates two additional components beyond standard reinforcement learning: a time-series grounded advantage based on optimal transport and an orthogonal projection to ensure this auxiliary granular signal does not interfere with the primary detection objective. Across diverse anomaly scenarios, AnomSeer, with Qwen2.5-VL-3B/7B-Instruct, outperforms larger commercial baselines (e.g., GPT-4o) in classification and localization accuracy, particularly on point- and frequency-driven exceptions. Moreover, it produces plausible time-series reasoning traces that support its conclusions.

</details>


### [325] [Stress-Testing Alignment Audits With Prompt-Level Strategic Deception](https://arxiv.org/abs/2602.08877)
*Oliver Daniels,Perusha Moodley,Ben Marlin,David Lindner*

Main category: cs.LG

TL;DR: 本文开发了一个自动红队管道来生成欺骗策略，测试现有对齐审计方法对战略欺骗的鲁棒性，发现当前黑白盒方法在面对有能力的未对齐模型时都不够可靠。


<details>
  <summary>Details</summary>
Motivation: 现有对齐审计方法旨在从战略性的、具有情境意识的未对齐模型中识别隐藏目标，但这些方法尚未系统性地针对欺骗策略进行压力测试。本文旨在填补这一空白。

Method: 开发自动红队管道，生成针对特定白盒和黑盒审计方法的欺骗策略（以系统提示形式）。对助手预填充、用户角色采样、稀疏自编码器和词嵌入相似性等方法进行压力测试。

Result: 自动红队管道找到了能够欺骗黑盒和白盒方法的提示，使其做出自信但错误的猜测。这是首次记录到基于激活的战略欺骗证据。

Conclusion: 当前的黑盒和白盒方法在面对足够有能力的未对齐模型时都不够鲁棒，需要开发更强大的审计方法来应对战略欺骗。

Abstract: Alignment audits aim to robustly identify hidden goals from strategic, situationally aware misaligned models. Despite this threat model, existing auditing methods have not been systematically stress-tested against deception strategies. We address this gap, implementing an automatic red-team pipeline that generates deception strategies (in the form of system prompts) tailored to specific white-box and black-box auditing methods. Stress-testing assistant prefills, user persona sampling, sparse autoencoders, and token embedding similarity methods against secret-keeping model organisms, our automatic red-team pipeline finds prompts that deceive both the black-box and white-box methods into confident, incorrect guesses. Our results provide the first documented evidence of activation-based strategic deception, and suggest that current black-box and white-box methods would not be robust to a sufficiently capable misaligned model.

</details>


### [326] [Learning Potentials for Dynamic Matching and Application to Heart Transplantation](https://arxiv.org/abs/2602.08878)
*Itai Zilberstein,Ioannis Anagnostides,Zachary W. Sollie,Arman Kilic,Tuomas Sandholm*

Main category: cs.LG

TL;DR: 提出基于潜在函数的非近视政策优化框架，用于心脏移植器官分配，通过自监督模仿学习训练潜在函数来模拟全知算法，在真实历史数据上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 心脏移植等待时间危及生命，现有器官分配政策未能考虑器官动态到达和等待名单组成，美国正从基于规则的政策转向数据驱动模型，需要更有效的分配方法。

Method: 提出基于潜在函数的非近视政策优化框架，开发可扩展且准确的高维潜在函数学习方法，采用自监督模仿学习训练潜在函数来模拟具有完美预见性的全知算法。

Result: 使用真实历史数据证明，该方法在优化群体水平结果方面显著优于现有方法，包括美国现状政策和提出的连续分布框架。

Conclusion: 在美国心脏移植分配系统审查的关键时刻，提出了可扩展且理论基础的路径，实现更有效的器官分配。

Abstract: Each year, thousands of patients in need of heart transplants face life-threatening wait times due to organ scarcity. While allocation policies aim to maximize population-level outcomes, current approaches often fail to account for the dynamic arrival of organs and the composition of waitlisted candidates, thereby hampering efficiency. The United States is transitioning from rigid, rule-based allocation to more flexible data-driven models. In this paper, we propose a novel framework for non-myopic policy optimization in general online matching relying on potentials, a concept originally introduced for kidney exchange. We develop scalable and accurate ways of learning potentials that are higher-dimensional and more expressive than prior approaches. Our approach is a form of self-supervised imitation learning: the potentials are trained to mimic an omniscient algorithm that has perfect foresight. We focus on the application of heart transplant allocation and demonstrate, using real historical data, that our policies significantly outperform prior approaches -- including the current US status quo policy and the proposed continuous distribution framework -- in optimizing for population-level outcomes. Our analysis and methods come at a pivotal moment in US policy, as the current heart transplant allocation system is under review. We propose a scalable and theoretically grounded path toward more effective organ allocation.

</details>


### [327] [Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression](https://arxiv.org/abs/2602.08885)
*Paul Saegert,Ullrich Köthe*

Main category: cs.LG

TL;DR: SimpliPy加速符号回归，Flash-ANSR框架在FastSRB基准上超越现有方法


<details>
  <summary>Details</summary>
Motivation: 当前摊销式符号回归(SR)在处理复杂科学问题时效率低下，主要瓶颈在于缺乏快速将等价表达式简化为简洁规范形式的方法。现有方法依赖通用计算机代数系统(如SymPy)，但计算成本过高，严重限制了训练和推理速度。

Method: 提出SimpliPy：基于规则的简化引擎，比SymPy快100倍且质量相当。在此基础上构建Flash-ANSR框架，利用SimpliPy实现大规模训练集扩展、更有效的token预算使用，以及训练集去污染。

Result: 在FastSRB基准测试中，Flash-ANSR显著优于摊销式基线方法(NeSymReS, E2E)。与最先进的直接优化方法(PySR)性能相当，但能随着推理预算增加恢复更简洁而非更复杂的表达式。

Conclusion: SimpliPy通过大幅加速表达式简化，解决了摊销式符号回归的关键瓶颈。Flash-ANSR框架展示了在保持准确性的同时提高效率的潜力，为符号回归在复杂科学问题中的应用开辟了新途径。

Abstract: Symbolic regression (SR) aims to discover interpretable analytical expressions that accurately describe observed data. Amortized SR promises to be much more efficient than the predominant genetic programming SR methods, but currently struggles to scale to realistic scientific complexity. We find that a key obstacle is the lack of a fast reduction of equivalent expressions to a concise normalized form. Amortized SR has addressed this by general-purpose Computer Algebra Systems (CAS) like SymPy, but the high computational cost severely limits training and inference speed. We propose SimpliPy, a rule-based simplification engine achieving a 100-fold speed-up over SymPy at comparable quality. This enables substantial improvements in amortized SR, including scalability to much larger training sets, more efficient use of the per-expression token budget, and systematic training set decontamination with respect to equivalent test expressions. We demonstrate these advantages in our Flash-ANSR framework, which achieves much better accuracy than amortized baselines (NeSymReS, E2E) on the FastSRB benchmark. Moreover, it performs on par with state-of-the-art direct optimization (PySR) while recovering more concise instead of more complex expressions with increasing inference budget.

</details>


### [328] [Discrete Bridges for Mutual Information Estimation](https://arxiv.org/abs/2602.08894)
*Iryna Zabarianska,Sergei Kholkin,Grigoriy Ksenofontov,Ivan Butakov,Alexander Korotin*

Main category: cs.LG

TL;DR: 提出DBMI估计器，利用离散桥匹配模型将互信息估计转化为域转移问题，适用于传统方法难以处理的离散数据


<details>
  <summary>Details</summary>
Motivation: 传统互信息估计方法在处理离散数据时存在困难，而扩散桥模型在离散状态空间中的发展为解决这一问题提供了新思路

Method: 将互信息估计重新构建为域转移问题，利用离散状态空间的桥匹配模型构建离散桥互信息（DBMI）估计器

Result: 在低维和基于图像的两种互信息估计场景中展示了DBMI估计器的性能

Conclusion: 离散桥匹配模型为互信息估计提供了有效的解决方案，特别适用于传统方法难以处理的离散数据场景

Abstract: Diffusion bridge models in both continuous and discrete state spaces have recently become powerful tools in the field of generative modeling. In this work, we leverage the discrete state space formulation of bridge matching models to address another important problem in machine learning and information theory: the estimation of the mutual information (MI) between discrete random variables. By neatly framing MI estimation as a domain transfer problem, we construct a Discrete Bridge Mutual Information (DBMI) estimator suitable for discrete data, which poses difficulties for conventional MI estimators. We showcase the performance of our estimator on two MI estimation settings: low-dimensional and image-based.

</details>


### [329] [GSS: Gated Subspace Steering for Selective Memorization Mitigation in LLMs](https://arxiv.org/abs/2602.08901)
*Xuanqi Zhang,Haoyang Shang,Xiaoxiao Li*

Main category: cs.LG

TL;DR: 提出Gated Subspace Steering (GSS)方法，通过探测-引导机制选择性缓解LLM的记忆问题，相比现有方法计算量减少100-1000倍，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法对LLM的记忆问题采用统一干预，会降低大多数正常泛化token的性能。研究发现记忆是稀疏、间歇且token依赖的，需要上下文感知的干预而非静态参数修改。

Method: 提出Gated Subspace Steering (GSS)方法，将干预分解为探测（检测与记忆相关的激活）和引导（仅在探测超过阈值时应用针对性修正）。通过最优子空间引导的优化框架找到最佳探测-引导对。

Result: 在四个基准测试中，GSS达到或超过最先进的记忆减少效果，同时计算量比基于优化的替代方法少100-1000倍。提供了关于神经表示中记忆几何结构的新理论见解。

Conclusion: GSS通过选择性干预有效缓解LLM记忆问题，计算效率高，为理解记忆在神经网络中的几何结构提供了新视角。

Abstract: Large language models (LLMs) can memorize and reproduce training sequences verbatim -- a tendency that undermines both generalization and privacy. Existing mitigation methods apply interventions uniformly, degrading performance on the majority of tokens that generalize normally. We show empirically that memorization is sparse, intermittent, and token-conditioned, suggesting that effective mitigation requires context-aware intervention rather than static parameter modification. To this end, we propose a novel and effective selective memorization mitigation method -- Gated Subspace Steering (GSS), which decomposes intervention into a probe (detecting memorization-relevant activations) and a steer (applying targeted correction only when the probe exceeds a threshold). The optimal probe-steer pair emerges from a principled optimization framework based on optimal subspace steering. Experiments on four benchmarks show GSS matches or exceeds state-of-the-art memorization reduction while requiring $100-1000 \times$ less compute than optimization-based alternatives. Furthermore, we provide new theoretical insights into the geometry of memorization in neural representations.

</details>


### [330] [Positive Distribution Shift as a Framework for Understanding Tractable Learning](https://arxiv.org/abs/2602.08907)
*Marko Medvedev,Idan Attias,Elisabetta Cornacchia,Theodor Misiakiewicz,Gal Vardi,Nathan Srebro*

Main category: cs.LG

TL;DR: 论文提出"正分布偏移"概念，认为在特定训练分布下，分布偏移可以简化学习而非阻碍学习，主要带来计算而非统计优势。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为分布偏移（特别是协变量偏移）会损害学习效果，但作者认为精心选择的训练分布可以使学习更容易，这种"正分布偏移"视角对当代机器学习创新至关重要。

Method: 形式化不同变体的正分布偏移概念，展示某些难学习类别在正分布偏移下变得容易学习，并与成员查询学习建立联系。

Result: 证明正分布偏移可以使计算困难问题变得可处理，即使使用标准的基于梯度的训练方法也能实现。

Conclusion: 正分布偏移为机器学习提供了新的视角，强调选择好的训练分布而非仅改进训练算法的重要性，特别是在解决计算困难问题方面具有潜力。

Abstract: We study a setting where the goal is to learn a target function f(x) with respect to a target distribution D(x), but training is done on i.i.d. samples from a different training distribution D'(x), labeled by the true target f(x). Such a distribution shift (here in the form of covariate shift) is usually viewed negatively, as hurting or making learning harder, and the traditional distribution shift literature is mostly concerned with limiting or avoiding this negative effect. In contrast, we argue that with a well-chosen D'(x), the shift can be positive and make learning easier -- a perspective called Positive Distribution Shift (PDS). Such a perspective is central to contemporary machine learning, where much of the innovation is in finding good training distributions D'(x), rather than changing the training algorithm. We further argue that the benefit is often computational rather than statistical, and that PDS allows computationally hard problems to become tractable even using standard gradient-based training. We formalize different variants of PDS, show how certain hard classes are easily learnable under PDS, and make connections with membership query learning.

</details>


### [331] [GEMSS: A Variational Bayesian Method for Discovering Multiple Sparse Solutions in Classification and Regression Problems](https://arxiv.org/abs/2602.08913)
*Kateřina Henclová,Václav Šmídl*

Main category: cs.LG

TL;DR: GEMSS是一个变分贝叶斯框架，用于在n≪p和高相关性的欠定场景中同时发现多个不同的稀疏特征组合，解决了传统方法只能找到单一解的问题。


<details>
  <summary>Details</summary>
Motivation: 在欠定（n≪p）和高相关性场景中，多个不同的稀疏特征子集可能同样能解释响应。识别这些替代方案对于生成领域特定见解至关重要，但传统方法通常只能找到单一解，掩盖了完整可能的解释谱系。

Method: GEMSS采用变分贝叶斯框架，使用结构化spike-and-slab先验实现稀疏性，高斯混合近似难以处理的多峰后验分布，并使用基于Jaccard的惩罚项控制解多样性。与顺序贪婪方法不同，GEMSS通过随机梯度下降在单个目标函数中优化整个解集合。

Result: 在包含128个合成实验的全面基准测试中，GEMSS在高维设置（p=5000）中有效扩展，样本量小至n=50，无缝泛化到连续目标，原生处理缺失数据，并对类别不平衡和高斯噪声表现出显著鲁棒性。

Conclusion: GEMSS是一个有效的框架，用于在欠定和高相关性场景中发现多个不同的稀疏特征组合，已作为Python包'gemss'在PyPI上提供，GitHub仓库包含适合非编码人员使用的免费易用应用程序。

Abstract: Selecting interpretable feature sets in underdetermined ($n \ll p$) and highly correlated regimes constitutes a fundamental challenge in data science, particularly when analyzing physical measurements. In such settings, multiple distinct sparse subsets may explain the response equally well. Identifying these alternatives is crucial for generating domain-specific insights into the underlying mechanisms, yet conventional methods typically isolate a single solution, obscuring the full spectrum of plausible explanations.
  We present GEMSS (Gaussian Ensemble for Multiple Sparse Solutions), a variational Bayesian framework specifically designed to simultaneously discover multiple, diverse sparse feature combinations. The method employs a structured spike-and-slab prior for sparsity, a mixture of Gaussians to approximate the intractable multimodal posterior, and a Jaccard-based penalty to further control solution diversity. Unlike sequential greedy approaches, GEMSS optimizes the entire ensemble of solutions within a single objective function via stochastic gradient descent.
  The method is validated on a comprehensive benchmark comprising 128 synthetic experiments across classification and regression tasks. Results demonstrate that GEMSS scales effectively to high-dimensional settings ($p=5000$) with sample size as small as $n = 50$, generalizes seamlessly to continuous targets, handles missing data natively, and exhibits remarkable robustness to class imbalance and Gaussian noise.
  GEMSS is available as a Python package 'gemss' at PyPI. The full GitHub repository at https://github.com/kat-er-ina/gemss/ also includes a free, easy-to-use application suitable for non-coders.

</details>


### [332] [Diffusion-Inspired Reconfiguration of Transformers for Uncertainty Calibration](https://arxiv.org/abs/2602.08920)
*Manh Cuong Dao,Quang Hung Pham,Phi Le Nguyen,Thao Nguyen Truong,Bryan Kian Hsiang Low,Trong Nghia Hoang*

Main category: cs.LG

TL;DR: 提出一种基于扩散过程的Transformer不确定性校准方法，通过概率映射块重构预训练模型，实现表示不确定性的传播，同时保持原始预测性能。


<details>
  <summary>Details</summary>
Motivation: 预训练Transformer在风险敏感应用中需要可靠的不确定性校准，但现有模型缺乏通过特征变换堆栈进行不确定性传播的原则性机制。

Method: 将Transformer的每个特征变换块建模为概率映射，组合这些映射形成类似扩散过程的概率路径，然后将其重新编译到具有统一转移模型的扩散过程中。

Result: 在多种视觉和语言基准测试中，该方法相比现有不确定性感知Transformer实现了更优的校准性能和预测准确性。

Conclusion: 提出的扩散启发式Transformer重构方法能够实现原则性的表示不确定性传播，同时保持预训练模型的原始预测性能，为风险敏感应用提供了更可靠的部署方案。

Abstract: Uncertainty calibration in pre-trained transformers is critical for their reliable deployment in risk-sensitive applications. Yet, most existing pre-trained transformers do not have a principled mechanism for uncertainty propagation through their feature transformation stack. In this work, we propose a diffusion-inspired reconfiguration of transformers in which each feature transformation block is modeled as a probabilistic mapping. Composing these probabilistic mappings reveals a probability path that mimics the structure of a diffusion process, transporting data mass from the input distribution to the pre-trained feature distribution. This probability path can then be recompiled on a diffusion process with a unified transition model to enable principled propagation of representation uncertainty throughout the pre-trained model's architecture while maintaining its original predictive performance. Empirical results across a variety of vision and language benchmarks demonstrate that our method achieves superior calibration and predictive accuracy compared to existing uncertainty-aware transformers.

</details>


### [333] [DynamiQ: Accelerating Gradient Synchronization using Compressed Multi-hop All-reduce](https://arxiv.org/abs/2602.08923)
*Wenchen Han,Shay Vargaftik,Michael Mitzenmacher,Ran Ben Basat*

Main category: cs.LG

TL;DR: DynamiQ是一个针对多跳全归约的量化框架，通过优化部分和表示和融合内核，在保持精度的同时显著加速大规模模型训练。


<details>
  <summary>Details</summary>
Motivation: 随着训练规模扩大，网络成为瓶颈，需要减少传输数据量。现有梯度量化系统未针对多跳聚合优化，其中条目在聚合拓扑中多次部分求和。

Method: 提出DynamiQ量化框架，引入新技术更好地表示部分和，并与解压缩-累加-重新压缩融合内核协同设计以促进快速执行。扩展PyTorch DDP以支持通过NCCL P2P的DynamiQ。

Result: 在不同LLM、任务和规模下，相比Omni-Reduce、THC、MXFP4/6/8等最先进方法，DynamiQ实现高达34.2%的改进。是唯一能持续达到接近基线精度（如BF16基线的99.9%）并显著加速训练的方法。

Conclusion: DynamiQ成功弥合了量化最佳实践与多跳聚合之间的差距，在保持高精度的同时显著加速大规模模型训练。

Abstract: Multi-hop all-reduce is the de facto backbone of large model training. As the training scale increases, the network often becomes a bottleneck, motivating reducing the volume of transmitted data. Accordingly, recent systems demonstrated significant acceleration of the training process using gradient quantization. However, these systems are not optimized for multi-hop aggregation, where entries are partially summed multiple times along their aggregation topology.
  This paper presents DynamiQ, a quantization framework that bridges the gap between quantization best practices and multi-hop aggregation. DynamiQ introduces novel techniques to better represent partial sums, co-designed with a decompress-accumulate-recompress fused kernel to facilitate fast execution.
  We extended PyTorch DDP to support DynamiQ over NCCL P2P, and across different LLMs, tasks, and scales, we demonstrate consistent improvement of up to 34.2% over the best among state-of-the-art methods such as Omni-Reduce, THC, and emerging standards such as MXFP4, MXFP6, and MXFP8. Further, DynamiQ is the only evaluated method that consistently reaches near-baseline accuracy (e.g., 99.9% of the BF16 baseline) and does so while significantly accelerating the training.

</details>


### [334] [StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors](https://arxiv.org/abs/2602.08934)
*Suraj Ranganath,Atharv Ramesh*

Main category: cs.LG

TL;DR: StealthRL是一个基于强化学习的对抗性评估框架，通过在语义保持的前提下对AI文本检测器进行对抗性改写攻击，揭示了当前检测器存在的严重鲁棒性漏洞。


<details>
  <summary>Details</summary>
Motivation: AI文本检测器面临关键的鲁棒性挑战：对抗性改写攻击可以在保持语义的同时逃避检测。需要建立一个系统性的评估框架来测试检测器在真实对抗条件下的鲁棒性。

Method: 使用强化学习框架StealthRL，基于Qwen3-4B模型和LoRA适配器，采用Group Relative Policy Optimization (GRPO)训练改写策略。针对多检测器集成优化复合奖励函数，平衡检测逃避和语义保持。评估了6种攻击设置(M0-M5)对抗3类检测器家族。

Result: StealthRL实现了接近零的检测率(平均TPR@1%FPR为0.001)，将平均AUROC从0.74降至0.27，攻击成功率高达99.9%。攻击还能迁移到训练中未见过的检测器家族，揭示了共享的架构漏洞而非特定检测器的脆弱性。

Conclusion: 当前AI文本检测器存在显著的鲁棒性缺陷，StealthRL提供了一个原则性的对抗性评估协议，可作为检测器安全评估的标准工具。代码和评估流程已开源。

Abstract: AI-text detectors face a critical robustness challenge: adversarial paraphrasing attacks that preserve semantics while evading detection. We introduce StealthRL, a reinforcement learning framework that stress-tests detector robustness under realistic adversarial conditions. StealthRL trains a paraphrase policy against a multi-detector ensemble using Group Relative Policy Optimization (GRPO) with LoRA adapters on Qwen3-4B, optimizing a composite reward that balances detector evasion with semantic preservation. We evaluate six attack settings (M0-M5) against three detector families (RoBERTa, FastDetectGPT, and Binoculars) at the security-relevant 1% false positive rate operating point. StealthRL achieves near-zero detection (0.001 mean TPR@1%FPR), reduces mean AUROC from 0.74 to 0.27, and attains a 99.9% attack success rate. Critically, attacks transfer to a held-out detector family not seen during training, revealing shared architectural vulnerabilities rather than detector-specific brittleness. We additionally conduct LLM-based quality evaluation via Likert scoring, analyze detector score distributions to explain why evasion succeeds, and provide per-detector AUROC with bootstrap confidence intervals. Our results expose significant robustness gaps in current AI-text detection and establish StealthRL as a principled adversarial evaluation protocol. Code and evaluation pipeline are publicly available at https://github.com/suraj-ranganath/StealthRL.

</details>


### [335] [A Behavioural and Representational Evaluation of Goal-Directedness in Language Model Agents](https://arxiv.org/abs/2602.08964)
*Raghu Arghal,Fade Chen,Niall Dalton,Evgenii Kortukov,Calum McNamara,Angelos Nalmpantis,Moksh Nirvaan,Gabriele Sarti,Mario Giulianelli*

Main category: cs.LG

TL;DR: 该研究提出了一个评估智能体目标导向性的框架，结合行为评估和可解释性分析，通过LLM智能体在2D网格世界中的导航案例，发现智能体内部非线性编码环境空间地图，并在推理过程中重组表征。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏可靠的方法来归因智能体系统的目标，理解智能体的目标有助于解释和预测其行为，因此需要建立评估目标导向性的方法论。

Method: 提出结合行为评估和可解释性分析的框架。行为上评估LLM智能体在不同网格大小、障碍密度和目标结构下的表现；可解释性上使用探测方法解码智能体对环境状态和多步行动计划的内部表征。

Result: LLM智能体性能随任务难度增加而提升，但对保持难度的变换和复杂目标结构保持鲁棒性。智能体内部非线性编码环境的粗略空间地图，保留位置和目标的近似任务相关线索，其行动与这些内部表征基本一致，推理过程会重组表征，从环境结构线索转向支持即时行动选择的信息。

Conclusion: 仅靠行为评估不足以表征智能体如何表示和追求目标，需要结合内省检查来全面理解智能体的目标导向性。

Abstract: Understanding an agent's goals helps explain and predict its behaviour, yet there is no established methodology for reliably attributing goals to agentic systems. We propose a framework for evaluating goal-directedness that integrates behavioural evaluation with interpretability-based analyses of models' internal representations. As a case study, we examine an LLM agent navigating a 2D grid world toward a goal state. Behaviourally, we evaluate the agent against an optimal policy across varying grid sizes, obstacle densities, and goal structures, finding that performance scales with task difficulty while remaining robust to difficulty-preserving transformations and complex goal structures. We then use probing methods to decode the agent's internal representations of the environment state and its multi-step action plans. We find that the LLM agent non-linearly encodes a coarse spatial map of the environment, preserving approximate task-relevant cues about its position and the goal location; that its actions are broadly consistent with these internal representations; and that reasoning reorganises them, shifting from broader environment structural cues toward information supporting immediate action selection. Our findings support the view that introspective examination is required beyond behavioural evaluations to characterise how agents represent and pursue their objectives.

</details>


### [336] [Distributionally Robust Optimization via Generative Ambiguity Modeling](https://arxiv.org/abs/2602.08976)
*Jiaqi Wen,Jianyi Yang*

Main category: cs.LG

TL;DR: 提出基于生成模型的分布鲁棒优化(GAS-DRO)，使用生成模型构建歧义集，在保持与名义分布一致性的同时捕获超出名义支持空间的对抗分布，实现可处理的DRO求解和优越的OOD泛化性能。


<details>
  <summary>Details</summary>
Motivation: 传统DRO的歧义集需要同时满足两个关键要求：1) 保持与名义分布的一致性；2) 足够多样化以涵盖各种潜在场景；3) 能够导出可处理的DRO解。现有方法在同时满足这些要求方面存在局限。

Method: 提出基于生成模型的歧义集，通过参数化生成模型空间捕获超出名义支持空间的对抗分布。在此基础上提出GAS-DRO算法，在生成模型参数空间内求解内部最大化问题，并采用扩散模型实现。

Result: 理论上建立了GAS-DRO的平稳收敛性能。实证上使用扩散模型实现GAS-DRO，在机器学习任务中展示了优越的分布外(OOD)泛化性能。

Conclusion: 生成模型歧义集为DRO提供了一种有效框架，既能保持分布一致性，又能捕获多样化对抗场景，同时保证算法可处理性，显著提升了OOD泛化能力。

Abstract: This paper studies Distributionally Robust Optimization (DRO), a fundamental framework for enhancing the robustness and generalization of statistical learning and optimization. An effective ambiguity set for DRO must involve distributions that remain consistent to the nominal distribution while being diverse enough to account for a variety of potential scenarios. Moreover, it should lead to tractable DRO solutions. To this end, we propose generative model-based ambiguity sets that capture various adversarial distributions beyond the nominal support space while maintaining consistency with the nominal distribution. Building on this generative ambiguity modeling, we propose DRO with Generative Ambiguity Set (GAS-DRO), a tractable DRO algorithm that solves the inner maximization over the parameterized generative model space. We formally establish the stationary convergence performance of GAS-DRO. We implement GAS-DRO with a diffusion model and empirically demonstrate its superior Out-of-Distribution (OOD) generalization performance in ML tasks.

</details>


### [337] [StretchTime: Adaptive Time Series Forecasting via Symplectic Attention](https://arxiv.org/abs/2602.08983)
*Yubin Kim,Viresh Pati,Jevon Twitty,Vinh Pham,Shihao Yang,Jiecheng Lu*

Main category: cs.LG

TL;DR: 该论文提出了Symplectic Positional Embeddings (SyPE)来解决传统Transformer在时间序列预测中无法处理时间扭曲的问题，通过将旋转群扩展到辛群，实现了自适应的时间坐标伸缩。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列（如金融周期、生物节律）经常表现出"时间扭曲"的动态特性，即有效时间流与采样索引解耦。传统的Transformer位置编码（如RoPE）假设均匀的时间进展，无法表示非仿射的时间扭曲。

Method: 提出了Symplectic Positional Embeddings (SyPE)，从哈密顿力学推导出的可学习编码框架。SyPE通过将旋转群SO(2)扩展到辛群Sp(2,R)来严格推广RoPE，并通过新颖的输入依赖自适应扭曲模块进行调制。该方法允许注意力机制端到端地自适应扩张或收缩时间坐标。

Result: 在StretchTime架构中实现了该机制，在标准基准测试中达到了最先进的性能，在表现出非平稳时间动态的数据集上展示了卓越的鲁棒性。

Conclusion: SyPE框架成功解决了时间序列预测中的时间扭曲问题，通过辛群扩展和自适应扭曲模块，能够捕捉局部变化的周期性，无需预定义的扭曲函数，在非平稳时间动态场景中表现出色。

Abstract: Transformer architectures have established strong baselines in time series forecasting, yet they typically rely on positional encodings that assume uniform, index-based temporal progression. However, real-world systems, from shifting financial cycles to elastic biological rhythms, frequently exhibit "time-warped" dynamics where the effective flow of time decouples from the sampling index. In this work, we first formalize this misalignment and prove that rotary position embedding (RoPE) is mathematically incapable of representing non-affine temporal warping. To address this, we propose Symplectic Positional Embeddings (SyPE), a learnable encoding framework derived from Hamiltonian mechanics. SyPE strictly generalizes RoPE by extending the rotation group $\mathrm{SO}(2)$ to the symplectic group $\mathrm{Sp}(2,\mathbb{R})$, modulated by a novel input-dependent adaptive warp module. By allowing the attention mechanism to adaptively dilate or contract temporal coordinates end-to-end, our approach captures locally varying periodicities without requiring pre-defined warping functions. We implement this mechanism in StretchTime, a multivariate forecasting architecture that achieves state-of-the-art performance on standard benchmarks, demonstrating superior robustness on datasets exhibiting non-stationary temporal dynamics.

</details>


### [338] [Improving Detection of Rare Nodes in Hierarchical Multi-Label Learning](https://arxiv.org/abs/2602.08986)
*Isaac Xu,Martin Gillis,Ayushi Sharma,Benjamin Misiuk,Craig J. Brown,Thomas Trappenberg*

Main category: cs.LG

TL;DR: 提出加权损失函数解决层次多标签分类中深层节点预测困难问题，结合节点不平衡加权和焦点加权，提升罕见节点的召回率


<details>
  <summary>Details</summary>
Motivation: 层次多标签分类中，模型难以预测更深层次的节点，因为某些类别（或层次节点）天然稀有，且子节点几乎总是比父节点更不频繁

Method: 提出神经网络加权损失目标，结合节点不平衡加权和焦点加权组件，后者利用集成不确定性的现代量化方法，强调罕见节点而非罕见观测，并关注每个模型输出分布中的不确定节点

Result: 在基准数据集上召回率提升高达5倍，F1分数有统计显著提升，在具有次优编码器或有限数据的挑战性任务中，该方法有助于卷积网络

Conclusion: 通过强调罕见节点和关注不确定节点的加权损失方法，有效解决了层次多标签分类中深层节点预测的挑战

Abstract: In hierarchical multi-label classification, a persistent challenge is enabling model predictions to reach deeper levels of the hierarchy for more detailed or fine-grained classifications. This difficulty partly arises from the natural rarity of certain classes (or hierarchical nodes) and the hierarchical constraint that ensures child nodes are almost always less frequent than their parents. To address this, we propose a weighted loss objective for neural networks that combines node-wise imbalance weighting with focal weighting components, the latter leveraging modern quantification of ensemble uncertainties. By emphasizing rare nodes rather than rare observations (data points), and focusing on uncertain nodes for each model output distribution during training, we observe improvements in recall by up to a factor of five on benchmark datasets, along with statistically significant gains in $F_{1}$ score. We also show our approach aids convolutional networks on challenging tasks, as in situations with suboptimal encoders or limited data.

</details>


### [339] [DirMoE: Dirichlet-routed Mixture of Experts](https://arxiv.org/abs/2602.09001)
*Amirhossein Vahidi,Hesam Asadollahzadeh,Navid Akhavan Attar,Marie Moullet,Kevin Ly,Xingyi Yang,Mohammad Lotfollahi*

Main category: cs.LG

TL;DR: 提出DirMoE，一種基於Dirichlet變分自編碼器的端到端可微分路由機制，解決傳統Top-k+Softmax路由中專家選擇與貢獻分配混雜的問題。


<details>
  <summary>Details</summary>
Motivation: 現有MoE模型使用不可微分的Top-k+Softmax路由，限制了性能和可擴展性。傳統方法將專家選擇和專家貢獻分配兩個不同決策混為一談。

Method: 提出DirMoE，使用Dirichlet變分自編碼器框架：Bernoulli分量建模專家選擇，Dirichlet分量處理專家貢獻分配。使用Gumbel-Sigmoid鬆弛實現專家選擇可微，Dirichlet分布使用隱式重參數化。訓練目標為變分ELBO，包含直接稀疏懲罰以控制活躍專家數量。

Result: DirMoE路由機制匹配或超越其他方法，同時提高了專家專業化程度。整個前向傳播保持完全可微分。

Conclusion: DirMoE通過解耦專家選擇和貢獻分配，提供了一種端到端可微分的路由機制，改善了MoE模型的性能和專家專業化。

Abstract: Mixture-of-Experts (MoE) models have demonstrated exceptional performance in large-scale language models. Existing routers typically rely on non-differentiable Top-$k$+Softmax, limiting their performance and scalability. We argue that two distinct decisions, which experts to activate and how to distribute expert contributions among them, are conflated in standard Top-$k$+Softmax. We introduce Dirichlet-Routed MoE (DirMoE), a novel end-to-end differentiable routing mechanism built on a Dirichlet variational autoencoder framework. This design fundamentally disentangles the core routing problems: expert selection, modeled by a Bernoulli component, and expert contribution among chosen experts, handled by a Dirichlet component. The entire forward pass remains fully differentiable through the use of Gumbel-Sigmoid relaxation for the expert selection and implicit reparameterization for the Dirichlet distribution. Our training objective, a variational ELBO, includes a direct sparsity penalty that precisely controls the number of active experts in expectation, alongside a schedule for key hyperparameters that guides the model from an exploratory to a definitive routing state. Moreover, our DirMoE router matches or exceeds other methods while improving expert specialization.

</details>


### [340] [ARO: A New Lens On Matrix Optimization For Large Models](https://arxiv.org/abs/2602.09006)
*Wenbo Gong,Javier Zazo,Qijun Luo,Puqian Wang,James Hensman,Chao Ma*

Main category: cs.LG

TL;DR: ARO是一种新的矩阵优化框架，通过梯度旋转作为核心设计原则，在旋转坐标系中执行范数最速下降，超越现有正交化/白化方法，显著提升LLM训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于正交化/白化的矩阵优化器虽然取得了显著性能提升，但研究者希望探索超越正交化的新范式，进一步推动训练效率的边界。

Method: 提出自适应旋转优化(ARO)框架，将梯度旋转作为首要设计原则，在旋转坐标系中执行范数最速下降，旋转由新颖的范数感知策略决定，超越了现有正交化和白化优化器。

Result: 在严格控制的基准测试协议下，ARO在LLM预训练中始终优于AdamW(1.3-1.35倍)和正交化方法(1.1-1.15倍)，参数规模达80亿激活参数，过训练预算达8倍，未见收益递减。

Conclusion: ARO作为一种基于残差流旋转对称性的对称感知优化器，为利用跨层/跨模块耦合的计算高效设计提供了新思路，推动了矩阵优化范式的发展。

Abstract: Matrix-based optimizers have attracted growing interest for improving LLM training efficiency, with significant progress centered on orthogonalization/whitening based methods. While yielding substantial performance gains, a fundamental question arises: can we develop new paradigms beyond orthogonalization, pushing the efficiency frontier further? We present \textbf{Adaptively Rotated Optimization (ARO}, a new matrix optimization framework that treats gradient rotation as a first class design principle. ARO accelerates LLM training by performing normed steepest descent in a rotated coordinate system, where the rotation is determined by a novel norm-informed policy. This perspective yields update rules that go beyond existing orthogonalization and whitening optimizers, improving sample efficiency in practice. To make comparisons reliable, we propose a rigorously controlled benchmarking protocol that reduces confounding and bias. Under this protocol, ARO consistently outperforms AdamW (by 1.3 $\sim$1.35$\times$) and orthogonalization methods (by 1.1$\sim$1.15$\times$) in LLM pretraining at up to 8B activated parameters, and up to $8\times$ overtrain budget, without evidence of diminishing returns. Finally, we discuss how ARO can be reformulated as a symmetry-aware optimizer grounded in rotational symmetries of residual streams, motivating advanced designs that enable computationally efficient exploitation of cross-layer/cross module couplings.

</details>


### [341] [ShapeCond: Fast Shapelet-Guided Dataset Condensation for Time Series Classification](https://arxiv.org/abs/2602.09008)
*Sijia Peng,Yun Xiong,Xi Chen,Yi Xie,Guanzhi Li,Yanwei Yu,Yangyong Zhu,Zhiqiang Shen*

Main category: cs.LG

TL;DR: ShapeCond是一个针对时间序列分类的高效数据集压缩框架，通过形状基元引导的优化策略来保留关键局部模式，相比现有方法在速度和准确性上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据快速增长给存储和计算带来压力，现有数据集压缩方法主要针对图像领域，无法有效处理时间序列特有的时间结构和局部判别模式（如形状基元）。

Method: 提出ShapeCond框架，利用形状基元引导的优化策略进行时间序列数据集压缩，合成成本与序列长度无关，能够显式保留关键的局部模式。

Result: ShapeCond在合成速度上比现有最佳方法CondTSC快29倍，在Sleep数据集（3000个时间步）上比直接使用形状基元快10000倍，在下游分类准确性上持续优于所有现有时间序列数据集压缩方法。

Conclusion: ShapeCond通过形状基元引导的优化策略，有效解决了时间序列数据集压缩问题，在速度和准确性方面都取得了显著改进，为时间序列分析提供了高效的数据压缩解决方案。

Abstract: Time series data supports many domains (e.g., finance and climate science), but its rapid growth strains storage and computation. Dataset condensation can alleviate this by synthesizing a compact training set that preserves key information. Yet most condensation methods are image-centric and often fail on time series because they miss time-series-specific temporal structure, especially local discriminative motifs such as shapelets. In this work, we propose ShapeCond, a novel and efficient condensation framework for time series classification that leverages shapelet-based dataset knowledge via a shapelet-guided optimization strategy. Our shapelet-assisted synthesis cost is independent of sequence length: longer series yield larger speedups in synthesis (e.g., 29$\times$ faster over prior state-of-the-art method CondTSC for time-series condensation, and up to 10,000$\times$ over naively using shapelets on the Sleep dataset with 3,000 timesteps). By explicitly preserving critical local patterns, ShapeCond improves downstream accuracy and consistently outperforms all prior state-of-the-art time series dataset condensation methods across extensive experiments. Code is available at https://github.com/lunaaa95/ShapeCond.

</details>


### [342] [ANCRe: Adaptive Neural Connection Reassignment for Efficient Depth Scaling](https://arxiv.org/abs/2602.09009)
*Yilang Zhang,Bingcong Li,Niao He,Georgios B. Giannakis*

Main category: cs.LG

TL;DR: 本文提出自适应神经连接重分配（ANCRe）框架，通过参数化和学习残差连接结构来优化深度神经网络，实现更快的收敛速度和更好的深度利用效率。


<details>
  <summary>Details</summary>
Motivation: 尽管深度扩展是现代基础模型成功的关键，但研究发现深层网络往往未被充分利用。传统残差连接的固定结构可能限制了优化效率，需要重新审视残差连接对收敛行为的影响。

Method: 提出自适应神经连接重分配（ANCRe）框架，从优化角度分析残差连接布局对收敛行为的影响。该框架参数化并学习残差连接结构，以数据驱动的方式自适应地重新分配连接，计算和内存开销极小（<1%）。

Result: 在大语言模型预训练、扩散模型和深度ResNets等广泛实验中，ANCRe相比传统残差连接能持续加速收敛、提升性能并增强深度利用效率。

Conclusion: 残差连接布局对收敛行为有根本性影响，甚至会导致收敛速度的指数级差异。ANCRe提供了一种轻量级、数据驱动的方法来优化网络深度利用，为深度网络设计提供了新的优化视角。

Abstract: Scaling network depth has been a central driver behind the success of modern foundation models, yet recent investigations suggest that deep layers are often underutilized. This paper revisits the default mechanism for deepening neural networks, namely residual connections, from an optimization perspective. Rigorous analysis proves that the layout of residual connections can fundamentally shape convergence behavior, and even induces an exponential gap in convergence rates. Prompted by this insight, we introduce adaptive neural connection reassignment (ANCRe), a principled and lightweight framework that parameterizes and learns residual connectivities from the data. ANCRe adaptively reassigns residual connections with negligible computational and memory overhead ($<1\%$), while enabling more effective utilization of network depth. Extensive numerical tests across pre-training of large language models, diffusion models, and deep ResNets demonstrate consistently accelerated convergence, boosted performance, and enhanced depth efficiency over conventional residual connections.

</details>


### [343] [Next-Gen CAPTCHAs: Leveraging the Cognitive Gap for Scalable and Diverse GUI-Agent Defense](https://arxiv.org/abs/2602.09012)
*Jiacheng Liu,Yaxin Luo,Jiacheng Cui,Xinyi Shang,Xiaohan Zhao,Zhiqiang Shen*

Main category: cs.LG

TL;DR: 提出Next-Gen CAPTCHAs框架，利用人机在交互感知、记忆、决策和行动上的"认知鸿沟"，通过需要自适应直觉而非细粒度规划的动态任务来防御先进AI代理


<details>
  <summary>Details</summary>
Motivation: 传统CAPTCHA已过时，先进推理模型（如Gemini3-Pro-High和GPT-5.2-Xhigh）在复杂逻辑谜题上的通过率高达90%，需要新的安全防御机制来保护下一代网络

Method: 构建基于强大数据生成管道的可扩展基准，能够生成近乎无限的CAPTCHA实例；利用人机在交互感知、记忆、决策和行动上的"认知鸿沟"，设计需要自适应直觉而非细粒度规划的动态任务

Result: 建立了可扩展的防御框架，能够大规模评估，特别是对于后端支持的类型，系统能够生成有效无界的CAPTCHA实例

Conclusion: 通过工程化动态任务重新建立了生物用户和人工代理之间的强大区分，为代理时代提供了可扩展且多样化的防御机制

Abstract: The rapid evolution of GUI-enabled agents has rendered traditional CAPTCHAs obsolete. While previous benchmarks like OpenCaptchaWorld established a baseline for evaluating multimodal agents, recent advancements in reasoning-heavy models, such as Gemini3-Pro-High and GPT-5.2-Xhigh have effectively collapsed this security barrier, achieving pass rates as high as 90% on complex logic puzzles like "Bingo". In response, we introduce Next-Gen CAPTCHAs, a scalable defense framework designed to secure the next-generation web against the advanced agents. Unlike static datasets, our benchmark is built upon a robust data generation pipeline, allowing for large-scale and easily scalable evaluations, notably, for backend-supported types, our system is capable of generating effectively unbounded CAPTCHA instances. We exploit the persistent human-agent "Cognitive Gap" in interactive perception, memory, decision-making, and action. By engineering dynamic tasks that require adaptive intuition rather than granular planning, we re-establish a robust distinction between biological users and artificial agents, offering a scalable and diverse defense mechanism for the agentic era.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [344] [Asymptotic Freedom and Vacuum Polarization Determine the Astrophysical End State of Relativistic Gravitational Collapse: Quark--Gluon Plasma Star Instead of Black Hole](https://arxiv.org/abs/2602.07004)
*Herman J. Mosquera Cuesta,Fabián H. Zuluaga Giraldo,Wilmer D. Alfonso Pardo,Edgardo Marbello Santrich,Guillermo U. Avendaño Franco,Rafael Fragozo Larrazabal*

Main category: gr-qc

TL;DR: 提出一种广义相对论模型，描述由非线性电动力学真空极化和量子色动力学渐近自由共同支撑的超大质量、极强磁场、超致密的自束缚夸克-胶子等离子体天体，可作为坍缩恒星核心的最终平衡态。


<details>
  <summary>Details</summary>
Motivation: 探索超新星后坍缩过程中，当物质回落到新生遗迹上超过其稳定性时，如何形成由夸克-胶子等离子体构成的新型天体，这种天体可能模拟黑洞的引力效应但具有不同的物理本质。

Method: 结合广义相对论、非线性电动力学（NLED）和量子色动力学（QCD），建立非线性TOV方程模型，分析由渐近自由状态和真空极化共同支撑的夸克-胶子等离子体天体的平衡条件。

Result: 模型预测存在质量范围广泛（0-7太阳质量及以上）、半径0-24公里及以上、磁场强度10^14-10^16高斯及以上的超大质量QCD星，这些天体可通过引力波"悠悠球"态发射或引力彩虹等透镜现象被探测。

Conclusion: QCD星可作为坍缩恒星核心的最终平衡态，在大多数天体物理环境中模拟黑洞的引力效应，但具有不同的内部结构和可观测特征，为探索极端条件下的强相互作用物质提供了新途径。

Abstract: A general relativistic model of an astrophysical hypermassive extremely magnetized ultra-compact self-bound quark--gluon plasma object that is supported against its ultimate gravitational implosion by the simultaneous action of the vacuum polarization driven by nonlinear electrodynamics (NLED: light-by-light scattering) and the quantum chromodynamics (QCD) asymptotic freedom, is presented. These QCD stars can be the final figures of the equilibrium of collapsing stellar cores. Post-supernova fallback material pushes the nascent remnant beyond its stability to collapse into a hybrid hypermassive neutron star (HHMNS). Hypercritical accretion can unbind the whole HHMNS's baryons to spontaneously break away color confinement, powering a first-order hadron-to-quark phase transition to a sea of ever-freer quarks and gluons. This core is hydro-stabilized by the steady, endlessly compression-admitting asymptotic freedom state, possibly via gluon-mediated enduring exchange of color charge among bound states. The nonlinear TOV equation indicates the occurrence of hypermassive QGP/QCD stars with a wide mass spectrum ($0\lesssim$ M$^{\rm{QGP}}_{\rm{Star}}\lesssim$\,7\,M$_\odot$ and beyond), for star radii ($0\lesssim R^{\rm{QGP}}_{\rm{Star}}\lesssim 24$\,km and beyond) with B-fields ($10^{14} \leq$ B$^{\rm{QGP}}_{\rm{Star}} \leq 10^{16}$\,G and beyond). Such QCD stars can emulate what the true black holes are supposed to gravitationally do in most astrophysical settings. This color quark star could be found through a search for its eternal ``yo-yo'' state gravitational-wave emission, or via lensing phenomena like gravitational rainbows, as in this scenario it is expected that the light deflection angle, directly influenced by the larger effective mass/radius and magnetic field of the deflecting object, increases as the incidence angle decreases for impact parameter lower values.

</details>


### [345] [Escape of quantum information across an analogue black hole horizon](https://arxiv.org/abs/2602.07043)
*Zhilong Liu,Wentao Liu,Zehua Tian,Jieci Wang*

Main category: gr-qc

TL;DR: 该研究通过XY自旋链中的位置依赖耦合模拟黑洞时空，探索了黑洞信息悖论的量子模拟解决方案，展示了类Page曲线行为以及量子资源通过视界的传输机制。


<details>
  <summary>Details</summary>
Motivation: 黑洞完全蒸发作为霍金辐射的自然终点，引发了黑洞信息悖论，这从根本上挑战了量子力学中的幺正性和信息守恒原理。虽然AdS/CFT对应表明信息在黑洞蒸发过程中被保存，但信息如何从霍金辐射中恢复的具体机制仍然是一个开放性问题。

Method: 通过在XY自旋链中实现位置依赖耦合来构建模拟黑洞时空，研究信息传输机制。推导并展示了类Page曲线行为，分析了纠缠和相干性等量子资源在有效视界上的传输。

Result: 研究结果表明，最初局域在内部子系统中的量子资源可以通过粒子辐射穿过视界传输到外部。这为信息如何从黑洞中逃逸提供了新的量子模拟视角。

Conclusion: 该研究通过量子模拟方法为黑洞信息悖论的理解提供了新视角，展示了量子资源通过模拟黑洞视界的传输机制，有助于进一步理解黑洞信息问题。

Abstract: The complete evaporation of black holes, as a natural endpoint of Hawking radiation, gives rise to the black hole information paradox, which fundamentally challenges the principles of unitarity and information conservation in quantum mechanics. Although the AdS/CFT correspondence indicates that information is preserved during black hole evaporation, the precise mechanism by which it is recovered from the Hawking radiation remains an open question. To explore a potential resolution, we investigate information transfer in an analog black hole spacetime realized through position-dependent coupling in an XY spin chain. We derive and demonstrate Page curve-like behavior, and analyze the transmission of quantum resources, such as entanglement and coherence, across the effective horizon. Our results show that quantum resources initially localized within an interior subsystem can be transferred to the exterior via particle radiation through the horizon. This study provides a novel perspective from quantum simulation on how information may escape from black holes, thereby contributing to the further understanding of the black hole information paradox.

</details>


### [346] [Neutron Stars as Perfect Fluids: Extracting the Linearized Response Function](https://arxiv.org/abs/2602.07115)
*Irvin Martínez-Rodríguez*

Main category: gr-qc

TL;DR: 该论文推导了中子星在广义相对论框架下的线性潮汐响应，将中子星建模为正压理想流体，通过有效作用量方法得到离散的驱动模式谱，最终给出模式求和的响应函数和解析的动态潮汐形变能力。


<details>
  <summary>Details</summary>
Motivation: 研究中子星在潮汐力作用下的响应对于理解双星并合、引力波信号等天体物理现象至关重要。需要建立广义相对论框架下的流体动力学模型来描述中子星的潮汐形变。

Method: 从协变的流体有效作用量出发，线性化平衡态，得到流体位移与度规扰动的耦合作用量。将度规扰动分为外部和诱导部分，积分掉诱导场得到厄米算符和离散的带隙驱动模式谱。将位移投影到本征基上，积分掉空间依赖关系，简化为潮汐驱动的振子模型。

Result: 得到了模式求和的响应函数，以及从模式频率、归一化常数和重叠积分解析推导出的动态潮汐形变能力。建立了潮汐响应与中子星内部振荡模式之间的直接联系。

Conclusion: 该方法为研究中子星潮汐响应提供了系统的有效场论框架，能够解析地计算动态潮汐形变能力，对于理解双星并合过程中的潮汐效应和引力波信号具有重要意义。

Abstract: We derive the general relativistic linear tidal response of a neutron star modeled as a barotropic perfect fluid. From the covariant fluid effective action, we linearize about equilibrium and obtain the action for fluid displacements coupled to metric perturbations. Splitting the latter into external and induced parts and integrating out the induced field yields a Hermitian operator and a discrete gapped spectrum of driven modes. Projecting the displacement onto this eigenbasis and integrating out the spatial dependence over the stellar radius reduces the dynamics to tidal-driven oscillators, with couplings set by relativistic inner products and overlap integrals. Matching to the quadrupolar worldline effective action gives a mode-sum response function and analytic dynamical tidal deformabilities from mode frequencies, normalizations, and overlaps.

</details>


### [347] [On the Gravitational Energy of Axial Perturbations in Regular Black Holes](https://arxiv.org/abs/2602.07268)
*S. C. Ulhoa,F. L. Carneiro,B. C. C. Carneiro*

Main category: gr-qc

TL;DR: 本文研究规则黑洞轴向扰动的引力能量，利用TEGR理论计算二阶扰动能量，建立规则黑洞动力学响应与引力扰动能量之间的直接联系。


<details>
  <summary>Details</summary>
Motivation: 研究规则黑洞轴向扰动相关的引力能量，探索黑洞动力学响应与引力扰动能量之间的关系，为理解规则黑洞的稳定性提供能量视角。

Method: 1. 回顾规则黑洞在奇宇称扰动下的稳定性及准正规模；2. 从主方程重构描述度规涨落的扰动函数；3. 使用广义相对论的Teleparallel等效理论（TEGR）计算引力能量；4. 将引力能量计算到扰动参数的二阶，并用准正规模函数表示。

Result: 成功计算了规则黑洞轴向扰动的引力能量（二阶扰动），建立了规则黑洞动力学响应与引力扰动能量之间的直接联系，为理解规则黑洞的稳定性提供了新的能量框架。

Conclusion: 通过TEGR理论成功量化了规则黑洞轴向扰动的引力能量，揭示了黑洞动力学响应与扰动能量之间的内在联系，为规则黑洞的稳定性研究提供了重要的能量学基础。

Abstract: The article deals with the gravitational energy associated with axial perturbations of regular black holes. We review the stability of the geometry under odd-parity perturbations and the corresponding quasinormal modes, previously obtained for this class of spacetimes. The perturbative functions describing the metric fluctuations are reconstructed from the master equation. To evaluate the energy content of these perturbations, we employ the Teleparallel Equivalent of General Relativity (TEGR), which provides a well-defined expression for gravitational energy. The gravitational energy is computed up to second order in the perturbation parameter and expressed in terms of the quasinormal mode functions. Our results establish a direct connection between the dynamical response of regular black holes and the energy carried by their gravitational perturbations.

</details>


### [348] [The effects of boundary conditions on Rindler's spectral anomaly](https://arxiv.org/abs/2602.07323)
*M. A. Estévez,E. Sadurní*

Main category: gr-qc

TL;DR: 论文研究了Rindler时空中加速运动的边界条件如何导致Klein-Gordon场和Maxwell场的量子化模式，这与Unruh效应相关，并分析了相应的数学结构和粒子产生机制。


<details>
  <summary>Details</summary>
Motivation: 研究Rindler时空中加速运动的边界条件如何影响量子场的量子化模式，探索Unruh效应在存在运动边界时的表现，并分析相应的数学结构和物理意义。

Method: 使用Rindler度规描述加速观测者，分析加速运动边界条件下的Klein-Gordon场和Maxwell场，通过Hankel函数等特殊函数求解量子化模式，并利用Bogoliubov变换研究粒子产生机制。

Result: 发现加速运动的边界条件会导致量子场的量子化模式，只要边界不接触Rindler楔形中的奇点。这对应于具有反常势能-1/x²和Dirichlet边界条件的量子力学问题。提供了关于Hankel函数解完备性的数学分析，并阐明了相应的Sobolev空间性质。

Conclusion: Rindler时空中加速运动的边界条件能够产生量子场的量子化模式，这与Unruh效应密切相关。通过详细的数学分析和Bogoliubov变换，阐明了这种边界条件导致的粒子产生机制和相应的物理意义。

Abstract: Rindler's metric is an interesting way to incorporate a set of uniformly accelerated observers into space-time coordinates; this is consistent with special and general relativity. It is known that such an acceleration gives rise to the famous Unruh effect. Interestingly, its Galilean limit already shows the appearance of quantized modes for particles in free space, given by Airy functions. This happens when a wall or boundary condition is moving in an accelerated trajectory in free space and in the presence of a field. Here we show that such a boundary, when viewed as a material obstacle in motion, gives rise to quantized modes for the Klein-Gordon and Maxwell fields, as long as the boundary does not touch the singularity at the Rindler wedge. This corresponds to a quantum-mechanical problem with an anomalous fall-to-the-origin potential $-1/x^2$ supplemented with a Dirichlet condition. We provide further mathematical analysis regarding the completeness of the solutions in terms of Hankel functions $H^{(1)}$ of imaginary index and argument, and clarify the nature of the corresponding Sobolev spaces when the boundary condition disappears for the accelerated observer. A detailed interpretation of the transition amplitudes is given in connection with particle production obtained from a Bogoliubov transformation.

</details>


### [349] [Circularly polarized gravitational waves from parity-violating scalar-tensor theory](https://arxiv.org/abs/2602.07430)
*Jia-Xi Feng,Jia-Yuan Fang,Xian Gao*

Main category: gr-qc

TL;DR: 该研究在宇称破缺标量-张量理论中分析了原初引力波和标量诱导引力波，发现宇称破缺项导致张量传播偏振依赖，产生手性原初谱和非零圆偏振度，SIGWs在辐射主导时期表现出与广义相对论不同的特征性偏差。


<details>
  <summary>Details</summary>
Motivation: 研究宇称破缺标量-张量理论中的引力波特性，探索宇称破缺如何影响原初引力波和标量诱导引力波的传播和偏振特性，特别是理解不同宇称破缺项在线性和二阶扰动中的作用机制。

Method: 在"Qi-Xiu"拉格朗日描述的宇称破缺标量-张量理论框架下，推导了张量扰动的二次作用量，分析了线性阶和二阶扰动方程。在线性阶研究张量传播的偏振依赖性，在二阶推导标量诱导引力波的运动方程并识别宇称破缺源项。在辐射主导时期计算了单色和对数正态曲率功率谱下SIGWs的分数能量密度。

Result: 发现宇称破缺项ℒ₁,₂,₅,₆,₇使张量传播偏振依赖，导致手性原初谱和非零圆偏振度。ℒ₃和ℒ₄专门通过SIGWs的源项进入，即使线性引力波传播保持广义相对论特征时也能产生宇称破缺。在峰值频率附近，PVST引力中的SIGWs表现出与广义相对论不同的特征性偏差，并产生非零圆偏振度。

Conclusion: 宇称破缺标量-张量理论为引力波研究提供了丰富的新物理现象，包括手性原初谱、非零圆偏振度以及SIGWs的特征性偏差，这些特征可作为区分PVST引力和广义相对论的重要观测信号。

Abstract: We study both primordial GWs and scalar-induced gravitational waves (SIGWs) in a class of the parity-violating scalar-tensor (PVST) theory, of which the Lagrangian is the linear combination of seven ghost-free parity-violating scalar-tensor monomials dubbed the ``Qi-Xiu'' Lagrangians. At linear order, we obtain the quadratic action for tensor perturbations and show that parity-violating terms associated with $\mathcal{L}_{1,2,5,6,7}$ render the tensor propagation polarization dependent, leading to chiral primordial spectra and a nonvanishing degree of circular polarization. At second order, we derive the EOM for SIGWs and identify the explicit parity-violating source terms. In particular, $\mathcal{L}_3$ and $\mathcal{L}_4$ enter exclusively through the source term for SIGWs, allowing parity violation to arise even when the linear GWs propagation remains effectively GR-like. During the radiation-dominated era, we compute the fractional energy density of SIGWs for both monochromatic and lognormal curvature power spectra. We find that, around the peak frequency, SIGWs in PVST gravity exhibit characteristic deviations from those in GR, resulting in a nonzero degree of circular polarization.

</details>


### [350] [Probing Quantum Gravity effects with Extreme Mass Ratio Inspirals around Rotating Hayward Black Holes](https://arxiv.org/abs/2602.07436)
*Dan Zhang,Chao Zhang,Qiyuan Pan,Guoyang Fu,Jian-Pin Wu*

Main category: gr-qc

TL;DR: 研究旋转Hayward黑洞周围的极端质量比旋进，通过LISA探测量子引力效应，发现一年观测积累后量子参数α₀引起的波形相移可被探测


<details>
  <summary>Details</summary>
Motivation: 评估量子引力效应在极端质量比旋进中的可探测性，利用LISA高精度观测来检验广义相对论的偏离

Method: 使用旋转Hayward黑洞模型，分析量子参数α₀对轨道频率和通量的修正；采用AAK模型生成波形，利用TDI抑制噪声，通过Fisher信息矩阵评估LISA探测灵敏度

Result: 量子引力修正项在一年观测积累后会引起可探测的波形相移，LISA具备探测这些偏离广义相对论效应的潜力

Conclusion: LISA通过高精度观测极端质量比旋进，有望探测量子引力效应，为检验广义相对论提供新途径

Abstract: We investigate extreme mass-ratio inspirals (EMRIs) around a rotating Hayward black hole to assess the detectability of signatures arising from quantum gravity.The quantum parameter $α_0$, which encodes deviations from general relativity (GR), introduces extra correction terms in both the orbital frequency and the fluxes. Our results show that after one year of accumulated observation, these corrections induce a detectable dephasing in the EMRI waveform. Using the modified orbital evolution driven by $α_0$, we generate waveforms via the augmented analytic kludge (AAK) model implemented in the \texttt{FastEMRIWaveforms} package. Furthermore, we utilize the time-delay interferometry (TDI) to suppress the laser noise and phase fluctuations induced by spacecraft motion, and then employ the Fisher information matrix (FIM) to test the sensitivity of LISA in detecting deviations from GR. Our results demonstrate the potential of LISA to probe quantum-gravity effects through high-precision observations of EMRIs.

</details>


### [351] [A gravitationally induced decoherence model for photons in the context of the relational formalism](https://arxiv.org/abs/2602.07622)
*Max Joseph Fahn,Kristina Giesel,Roman Kemper*

Main category: gr-qc

TL;DR: 论文从Maxwell理论与线性化引力耦合出发，建立了光子引力诱导退相干模型，推导出TCL主方程，为后续研究提供基础框架。


<details>
  <summary>Details</summary>
Motivation: 现有量子力学模型中的系统-环境相互作用是唯象假设的，需要从基础作用量出发建立更根本的引力退相干理论，并扩展标量场模型以分析动力学参考场（时钟）在关系形式主义中的作用。

Method: 使用Ashtekar-Barbero变量表达Maxwell理论与线性化引力耦合，在post-Minkowskian近似下作为开放量子场论系统处理；选择合适的几何时钟和U(1)-Gauss时钟，通过可观测量映射及其对偶得到Dirac可观测量；对约化系统应用Fock量子化，推导二阶截断的时间卷积无关（TCL）主方程。

Result: 得到的Dirac可观测量直接对应于光子场的横向分量以及引力波的对称-横向-无迹自由度；推导出的TCL主方程在结构上与使用ADM变量和特定规范固定得到的光子主方程一致。

Conclusion: 该框架为引力退相干模型的进一步研究（包括重整化和单粒子态详细研究）提供了基础，证明了从基础作用量出发构建引力退相干理论的可行性，并展示了不同Dirac可观测量选择及其动力学的比较结果。

Abstract: We formulate a model of gravitationally induced decoherence for photons starting from Maxwell theory coupled to linearised gravity, expressed in terms of Ashtekar-Barbero variables and treated as an open quantum field theoretic system. In contrast to quantum mechanical models, the interaction between the system (Maxwell field) and the environment (gravitational field) is not postulated phenomenologically, but is instead dictated by the underlying action in a post-Minkowskian approximation. This framework extends earlier models for a scalar field and enables a more detailed analysis of the role of dynamical reference fields (clocks) within the relational formalism. We show that, for a suitable choice of geometrical clocks together with a U(1)-Gauss clock, and by employing an appropriate combination of the observable map and its dual, the resulting Dirac observables are given directly by the transverse components of the photon field as well as the symmetric-transverse-traceless degrees of freedom of gravitational waves on the linearised phase space of the coupled system. In addition we also compare different choices of Dirac observables and their dynamics. Upon applying a Fock quantisation to the reduced system, we derive the time convolutionless (TCL) master equation, truncated at second order, and analyse its structural properties. These results provide a foundation for further investigations of the decoherence model, including its renormalisation and a detailed study of its one-particle sector, and are found to be structurally consistent with former master equations for photons derived using ADM variables and a specific gauge fixing.

</details>


### [352] [Existence of Halos Outside Schwarzschild-$f(R)$ Black Holes](https://arxiv.org/abs/2602.07780)
*Wen-Xiang Chen*

Main category: gr-qc

TL;DR: 本文研究了Schwarzschild-f(R)黑洞外部形成光子晕（稳定光子轨道）的可能性，发现在某些f(R)引力模型中，黑洞事件视界外可以存在多个光子轨道（包括稳定和不稳定轨道），这为通过黑洞阴影观测修改引力理论提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 研究修改引力理论（特别是f(R)引力）中黑洞的光子轨道结构，探索标准Schwarzschild黑洞之外是否存在额外的稳定光子轨道（光子晕），并分析这些轨道对黑洞观测特征的影响。

Method: 采用受Kerr-Newman黑洞球形光子轨道研究启发的方法，分析Schwarzschild-f(R)黑洞时空中的零测地线。研究了多种f(R)引力模型（二次、对数、指数、立方、幂律、双曲形式），推导了光子晕存在的条件，给出了轨道半径的解析表达式，并进行了数值稳定性分析。

Result: 发现Schwarzschild-f(R)黑洞可以在事件视界外支持额外的外层稳定光子轨道（光子晕），而不会触发黑洞炸弹不稳定性。标准Schwarzschild黑洞只有一个不稳定光环，而修改引力模型中可以存在多个光子轨道。这些额外的轨道可能通过黑洞阴影的大小或形态偏差被观测到。

Conclusion: f(R)引力中的Schwarzschild黑洞可以形成稳定的光子晕，这为通过黑洞阴影观测来探测修改引力理论提供了新的可能性，深化了对替代引力理论中光子轨道结构的理解。

Abstract: We investigate the possibility of photon halos (stable photon orbits) forming outside Schwarzschild-$f(R)$ black holes by analyzing null geodesics in these spacetimes. Using methods inspired by studies of spherical photon orbits around Kerr-Newman black holes, we derive conditions for the existence of such halos. We examine several f(R) gravity models, including quadratic, logarithmic, exponential, cubic, power-law, and hyperbolic forms, and find that multiple photon orbits -- both stable and unstable -- can appear outside the event horizon for certain parameter ranges. These additional orbits (halos) provide new insights into spacetime geometry and potential observational signatures of black holes in modified gravity. We present analytical expressions for the orbital radii, perform a numerical stability analysis, and discuss possible observational implications for black hole shadows. Our results indicate that while the standard Schwarzschild black hole admits only a single unstable light ring, Schwarzschild-$f(R)$ black holes can support an additional outer stable photon orbit (a halo) without triggering a black-hole bomb instability. This work deepens the understanding of photon-orbit structures in alternative theories of gravity and highlights how such effects could be detected through deviations in black hole shadow size or morphology.

</details>


### [353] [Accelerating Black Hole Image Generation via Latent Space Diffusion Models](https://arxiv.org/abs/2602.07786)
*Ao Liu,Xudong Zhang,Cuihong Wen,Wentao Liu,Jieci Wang*

Main category: gr-qc

TL;DR: 提出物理条件扩散模型，在紧凑潜在空间中生成高保真黑洞图像，相比传统GRRT模拟计算成本降低4倍以上，推理时间从5.25秒减少到1.15秒。


<details>
  <summary>Details</summary>
Motivation: 传统广义相对论光线追踪模拟计算成本高，限制了黑洞图像参数探索和强场引力测试的效率，需要更快速准确的替代方法。

Method: 利用黑洞图像固有的低维流形结构，提出物理条件扩散模型，在紧凑潜在空间中直接从物理参数生成高保真黑洞图像。

Result: 模型准确再现了完整GRRT模拟的关键观测特征（阴影直径、光子环结构、相对论亮度不对称性），计算成本降低4倍以上，推理时间从5.25秒减少到1.15秒，图像质量、重建保真度和参数估计精度均有显著提升。

Conclusion: 扩散基潜在模型可作为传统辐射传输求解器的高效可扩展替代方案，为下一代黑洞成像提供实时建模和推断的实用框架。

Abstract: Interpreting horizon-scale black hole images currently relies on computationally intensive General Relativistic Ray Tracing (GRRT) simulations, which pose a significant bottleneck for rapid parameter exploration and high-precision tests of strong-field gravity. We demonstrate that physically accurate black hole images, synthesized from magnetized accretion flows, inherently reside on a low-dimensional manifold-encoding the essential features of spacetime geometry, plasma distribution, and relativistic emission. Leveraging this structure, we introduce a physics-conditioned diffusion model that operates in a compact latent space to generate high-fidelity black hole imagery directly from physical parameters. The model accurately reproduces critical observational signatures from full GRRT simulations-such as shadow diameter, photon-ring structure, and relativistic brightness asymmetry-while achieving over a fourfold reduction in computational expense. Compared with the previous generation of denoising diffusion models, the proposed approach achieves significant improvements in image quality, reconstruction fidelity, and parameter estimation accuracy, while reducing the average inference time per black hole image from 5.25 seconds to 1.15 seconds. Our work establishes diffusion-based latent models as efficient and scalable substitutes for traditional radiative transfer solvers, offering a practical framework toward real-time modeling and inference for next-generation black hole imaging.

</details>


### [354] [Geodesic Structure, Thermodynamics and Scalar Perturbations of Mod(A)Max black hole Surrounded by Perfect Fluid Dark Matter](https://arxiv.org/abs/2602.07806)
*Faizuddin Ahmed,Ahmad Al-Badawi,Edilberto O. Silva*

Main category: gr-qc

TL;DR: 研究修正Maxwell理论黑洞在完美流体暗物质环境中的光学性质、粒子动力学、热力学行为和准正则模


<details>
  <summary>Details</summary>
Motivation: 探索修正Maxwell理论（Mod(A)Max）黑洞在完美流体暗物质环境中的物理特性，包括光学现象、粒子轨道稳定性、热力学性质和标量场扰动，以理解几何参数如何影响这些特征

Method: 采用理论分析方法，研究球对称Mod(A)Max黑洞的光子球半径、阴影、光子轨迹和有效径向力；分析大质量粒子的有效势能、圆形轨道能量和角动量；计算黑洞温度、吉布斯自由能、热容和热力学拓扑；通过无质量克莱因-戈登方程研究标量场扰动，计算eikonal区域的准正则模

Result: 系统分析了修正Maxwell黑洞在暗物质环境中的光学特性、粒子动力学行为、热力学性质和准正则模谱，揭示了几何参数对这些物理特征的影响规律

Conclusion: 几何参数显著影响黑洞的光学特征、粒子轨道稳定性、热力学行为和准正则模谱，为理解修正引力理论在暗物质环境中的物理效应提供了全面框架

Abstract: In this work, we investigate the optical properties of a spherically symmetric Mod(A)Max black hole surrounded by perfect fluid dark matter, focusing on key features such as the photon sphere radius, shadow, photon trajectories, and the effective radial force experienced by photons. We also study the dynamics of massive particles around the black hole, deriving the effective potential and, from it, the specific energy and angular momentum of particles moving in circular orbits of fixed radii is discussed. The conditions for marginally stable circular orbits are analyzed, highlighting how the geometric parameters that modify the spacetime curvature influence both the optical and dynamical features. Furthermore, we explore the thermodynamic behavior of the black hole by examining its temperature, Gibbs free energy, and heat capacity, as well as its thermodynamic topology. Finally, scalar field perturbations are considered through the massless Klein-Gordon equation, and the quasinormal modes (QNMs) in the eikonal regime are computed, illustrating how the geometric parameters affect the potential and the QNM spectra.

</details>


### [355] [Signatures of the Israel Junction II: Double Photon Rings in Slowly Rotating Kerr Spacetime with Thin Shell](https://arxiv.org/abs/2602.07923)
*Long-Yue Li,Li-Ming Cao,Yungui Gong,Xia-Yuan Liu,Wenting Zhou*

Main category: gr-qc

TL;DR: 研究薄壳时空中光线穿越薄壳时的能量变化及其对黑洞阴影和吸积盘成像的影响，发现薄壳会导致光子环分裂、红移突变等特征性现象。


<details>
  <summary>Details</summary>
Motivation: 研究薄壳模型在真实天体物理系统中的适用性，通过分析光线穿越薄壳时的物理量变化，探索薄壳对黑洞阴影和吸积盘成像的影响，为未来天文观测验证以色列连接条件和薄壳模型提供理论依据。

Method: 将以色列连接条件应用于缓慢旋转的Kerr时空薄壳模型，分析光线穿越薄壳时能量、角动量和Carter常量的变化，研究这些变化对光线冲击参数的影响，进而分析薄壳时空中黑洞阴影和赤道面薄吸积盘成像的特征。

Result: 发现光线穿越薄壳时，角动量L和Carter常量C保持不变，但能量E发生变化，导致冲击参数η和ξ在薄壳处不连续。薄壳的存在使观测图像出现特征性结构：明显的双光子环（可逐渐合并为单环）、阴影边界与光子环非一一对应、红移因子突变导致的阶梯状结构。

Conclusion: 薄壳时空中光线穿越薄壳时的能量变化会产生独特的观测特征，如双光子环、红移突变等。这些特征可用于通过未来天文观测评估以色列连接条件和薄壳模型在真实天体物理系统中的适用性。

Abstract: Applying the junction conditions to the slowly rotating Kerr spacetime with a thin shell, we find that while the angular momentum $L$ and Carter constant $C$ of the ray remain unchanged upon crossing the shell, its energy $E$ does not. Consequently, the impact parameters $η=L/E$ and $ξ=C/E^2$ of the ray are discontinued at the shell. Utilizing this transformation, we study the shadow of this spacetime and the corresponding images from an equatorial thin accretion disk. The presence of the shell gives rise to distinctive features in the observed images. Notably, we observe distinct double photon rings in the images, which can gradually merge into a single ring. Moreover, the shadow boundaries and the photon rings do not exhibit a one-to-one correspondence. The abrupt changes in redshift factor and the truncated photon regions profoundly influence the image, producing distinctive features such as the step-like structures. These features in shell-equipped spacetimes can help evaluate, through future astronomical observations, the applicability of the Israel junction condition and the shell model in real astrophysical systems.

</details>


### [356] [Dynamic Black-hole Emission Tomography with Physics-informed Neural Fields](https://arxiv.org/abs/2602.08029)
*Berthy T. Feng,Andrew A. Chael,David Bromley,Aviad Levis,William T. Freeman,Katherine L. Bouman*

Main category: gr-qc

TL;DR: PI-DEF：一种物理信息化的4D辐射场重建方法，用于从稀疏射电测量中动态3D成像黑洞周围气体，相比BH-NeRF显著提升重建精度


<details>
  <summary>Details</summary>
Motivation: 现有BH-NeRF方法假设开普勒动力学，但在黑洞附近因强引力和电磁活动而失效；需要更准确的动态3D重建方法来揭示黑洞周围未观测区域并验证新物理模型

Method: 使用可微分神经渲染拟合4D（时间+3D）辐射场，联合重建3D速度场和4D辐射场，并将速度作为辐射场动力学的软约束

Result: 在模拟数据实验中，相比BH-NeRF和物理无关方法，重建精度显著提升；还能用于估计黑洞自旋等其他物理参数

Conclusion: PI-DEF通过物理约束克服了BH-NeRF的限制性假设，为黑洞动态3D成像提供了更准确的方法，有望揭示黑洞周围新物理现象

Abstract: With the success of static black-hole imaging, the next frontier is the dynamic and 3D imaging of black holes. Recovering the dynamic 3D gas near a black hole would reveal previously-unseen parts of the universe and inform new physics models. However, only sparse radio measurements from a single viewpoint are possible, making the dynamic 3D reconstruction problem significantly ill-posed. Previously, BH-NeRF addressed the ill-posed problem by assuming Keplerian dynamics of the gas, but this assumption breaks down near the black hole, where the strong gravitational pull of the black hole and increased electromagnetic activity complicate fluid dynamics. To overcome the restrictive assumptions of BH-NeRF, we propose PI-DEF, a physics-informed approach that uses differentiable neural rendering to fit a 4D (time + 3D) emissivity field given EHT measurements. Our approach jointly reconstructs the 3D velocity field with the 4D emissivity field and enforces the velocity as a soft constraint on the dynamics of the emissivity. In experiments on simulated data, we find significantly improved reconstruction accuracy over both BH-NeRF and a physics-agnostic approach. We demonstrate how our method may be used to estimate other physics parameters of the black hole, such as its spin.

</details>


### [357] [Waveform stability of black hole ringdown with stochastic horizon structure](https://arxiv.org/abs/2602.08034)
*Han-Wen Hu,Cheng-Jun Fang,Zong-Kuan Guo*

Main category: gr-qc

TL;DR: 黑洞环降对随机视界尺度结构具有鲁棒性，相位平均机制是稳定性的物理起源，可观测信号需要宏观空间相干性和经典强度双重约束


<details>
  <summary>Details</summary>
Motivation: 研究黑洞环降对随机视界尺度结构的鲁棒性，探索准正规模谱不稳定性是否会导致观测崩溃，理解微观几何细节如何影响宏观引力波形

Method: 在有效场理论框架内，通过相位平均机制分析波方程的空间积分，研究紫外几何细节的衰减，基于标度律和失配剖面建立几何选择规则

Result: 宏观引力波形保持鲁棒性，波方程的空间积分能有效衰减探测波长分辨率以下的紫外几何细节，建立了可观测性几何选择规则：需要宏观空间相干性和经典强度双重约束

Conclusion: 黑洞环降对随机视界尺度结构具有稳定性，任何显著的环降偏离都将作为宏观相干视界结构的决定性证据，排除了非相干高熵量子泡沫的可观测性

Abstract: We examine the robustness of black hole ringdown to stochastic horizon-scale structure within an effective field framework. Consistent with the understanding that the spectral instability of quasinormal modes does not necessarily imply observational breakdown, our results demonstrate that the macroscopic gravitational waveform remains robust. We identify the phase averaging mechanism as the physical origin of this stability, demonstrating that the spatial integration of the wave equation efficiently attenuates ultraviolet geometric details below the resolution limit of the probing wavelength. Building on the scaling law $\mathcal{M} \propto ε^2$ and the characteristic mismatch profile with respect to $L_c$, we propose a geometric selection rule for observability: a detectable signal imposes a strict dual constraint requiring both macroscopic spatial coherence ($L_c \sim M$) and classical-level intensity ($ε\gtrsim 10^{-4}$). This criterion quantitatively rules out the observability of incoherent, high-entropy quantum foam, suggesting that any significant ringdown deviation would serve as definitive evidence for macroscopically coherent horizon structures.

</details>


### [358] [Quantum Field Theory of Black Hole Perturbations with Backreaction V. Beyond Second Order Perturbations](https://arxiv.org/abs/2602.08125)
*Jonas Neuser,Thomas Thiemann*

Main category: gr-qc

TL;DR: 本文提出了一种新的黑洞微扰理论方法，通过约化相空间方法构建非微扰的规范不变可观测量，然后在全阶规范不变性基础上进行微扰展开，特别关注三阶效应。


<details>
  <summary>Details</summary>
Motivation: 传统黑洞微扰理论在二阶以上存在规范不变性定义模糊的问题，需要逐阶定义规范变换，这导致高阶微扰理论不完善。本文旨在发展一种能够清晰分离规范不变性与微扰阶数的新方法。

Method: 采用广义相对论哈密顿表述的约化相空间方法，构建非微扰的规范不变可观测量动力学（虽然是隐式的）。然后在此基础上进行微扰展开，但只考虑全阶规范不变的微扰。特别使用多项式形式的约束条件而非无穷级数，实现精确的非微扰约束求解。

Result: 该方法能够处理球对称和非对称可观测量，并完全考虑它们之间的微扰反作用。能够同时访问动态视界的外部区域和内部区域，并在三阶微扰上展示了其优势。

Conclusion: 新方法通过约化相空间框架将规范不变性与微扰阶数解耦，提供了处理高阶黑洞微扰的严格框架，特别是在三阶及更高阶微扰中能够捕捉传统方法可能忽略的反作用效应和全阶规范不变性影响。

Abstract: Black hole perturbation theory beyond second order is not well understood because typically one defines the meaning of gauge invariance order by order which is ambiguous. In this series of works we therefore developed a new approach which disentangles the meaning of gauge invariance from the perturbative order. It is based on the reduced phase space approach to the Hamiltonian formulation of General Relativity and constructs a non-perturbative, albeit implicit, formulation of the dynamics of only observables that are gauge invariant to all orders. To obtain explicit expressions, perturbation theory is then employed, but now only perturbations are considered that are gauge invariant to all orders. There are both spherically symmetric and non-symmetric observables and the formulation takes the (perturbative) backreaction between those fully into account. The formulation has access to both the exterior and interior of the dynamical horizon.
  In previous papers of this series we have introduced the general formalism and performed consistency checks with second order results obtained in other approaches. The real virtue of our approach starts emerging at higher than second order where we expect differences from previous works both due to backreaction effects and because we work with observables that are gauge invariant to all orders, not only up to a given order. In this paper, we consider the third order. Also new to our approach is that we start from a non-perturbative, namely polynomial, version of the constraints which therefore are finite polynomials in all degrees of freedom before reducing, rather than an infinite series. This allows for an exact and non-perturbative, while implicit, solution of the constraints which does not need to truncate the series and thus is of tremendous technical advantage.

</details>


### [359] [Does fermionic entanglement always outperform bosonic entanglement in dilaton black hole?](https://arxiv.org/abs/2602.08205)
*Wen-Mei Li,Jianbo Lu,Shu-Min Wu*

Main category: gr-qc

TL;DR: 该研究挑战了传统观念，发现在GHS膨胀子黑洞背景下，玻色场在非引力模式与引力模式间的纠缠强于费米场，但玻色场在引力模式与混合模式间的纠缠弱于费米场。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为费米纠缠在相对论框架下优于玻色纠缠，且玻色纠缠在极端引力环境中会突然死亡。本研究旨在重新审视这一观点，探索膨胀子黑洞背景下玻色场与费米场纠缠行为的差异。

Method: 分析玻色和费米GHZ态的真实N-体纠缠（用负性度量），研究其中m个组分与GHS膨胀子黑洞产生的霍金辐射相互作用的情况。比较不同模式间的纠缠强度。

Result: 1. 玻色场在非引力模式与引力模式间的纠缠强于费米场；2. 玻色场在引力模式与混合模式间的纠缠弱于费米场；3. 全局N-体纠缠关系受引力场强度影响；4. 挑战了"费米纠缠总是优于玻色纠缠"的传统观念。

Conclusion: 研究从新视角揭示了弯曲时空中玻色场与费米场量子纠缠的内在关系，为极端引力条件下选择合适场基量子资源提供了理论指导，表明场类型选择需根据具体纠缠模式和应用场景而定。

Abstract: It has traditionally been believed that fermionic entanglement generally outperforms bosonic entanglement in relativistic frameworks, and that bosonic entanglement experiences sudden death in extreme gravitational environments. In this study, we analyze the genuine N-partite entanglement, measured by negativity, of bosonic and fermionic GHZ states, focusing on scenarios where a subset of $m$ ($m<N$) constituents interacts with Hawking radiation generated by a Garfinkle-Horowitz-Strominger (GHS) dilaton black hole. Surprisingly, we find that quantum entanglement between the non-gravitational and gravitational modes for the bosonic field is stronger than that in the same modes for the fermionic field within dilaton spacetime. This study challenges the traditional belief that ``fermionic entanglement always outperforms bosonic entanglement" in the relativistic framework. However, quantum entanglement between the gravitational modes and the combined gravitational and non-gravitational modes is weaker for the bosonic field than for the fermionic field in the presence of a dilaton black hole. Finally, the connection between the global N-partite entanglement in the bosonic field and that in the fermionic field is influenced by the gravitational field's intensity. Our study reveals the intrinsic relationship between quantum entanglement of bosonic and fermionic fields in curved spacetime from a new perspective, and provides theoretical guidance for selecting appropriate field-based quantum resources for relativistic quantum information tasks under extreme gravitational conditions.

</details>


### [360] [Combinatorial Spacetime from Loop Quantum Gravity](https://arxiv.org/abs/2602.08341)
*Mikhail Altaisky*

Main category: gr-qc

TL;DR: 基于彭罗斯的组合时空思想，将圈量子引力理论重新表述为仅用物质场描述的理论，解决物质与时空关系的概念争议


<details>
  <summary>Details</summary>
Motivation: 圈量子引力作为量子引力理论的有力候选者，存在一个概念争议：从爱因斯坦-希尔伯特作用量出发，在没有物质的情况下描述时空，很难将时空定义为物质场关系之外的任何东西。需要解决时空与物质关系的根本问题。

Method: 遵循彭罗斯的组合时空思想，将圈量子引力理论完全用物质场重新表述，而不是从爱因斯坦-希尔伯特作用量出发。通过物质场之间的关系来定义时空结构。

Result: 提出了一个仅基于物质场的圈量子引力理论新表述，避免了传统方法中时空与物质分离的概念问题，为量子引力理论提供了更一致的概念基础。

Conclusion: 通过彭罗斯的组合时空方法重新表述圈量子引力，可以解决该理论中关于时空本质的概念争议，为量子引力理论提供更坚实的哲学和概念基础。

Abstract: Loop quantum gravity is a perspective candidate for the quantum theory of gravity. However, there is a conceptual controversy in it: having started from the Einstein-Hilbert action and describing spacetime without matter, we can hardly define spacetime as anything other than a set of relations between matter fields. Here, following the Penrose idea of combinatorial spacetime we reformulate loop quantum gravity theory solely in terms of the matter fields.

</details>


### [361] [Can UV meet IR in the Swiss cheese?](https://arxiv.org/abs/2602.08487)
*Madina Abilmazhinova,Diana Kulubayeva,Hrishikesh Chakrabarty,Daniele Malafarina*

Main category: gr-qc

TL;DR: 研究正则黑洞在膨胀宇宙中的嵌入，探讨黑洞奇点正则化的紫外修正如何影响外部宇宙的膨胀速率，并测试这些修正是否可能是当前加速膨胀的原因。


<details>
  <summary>Details</summary>
Motivation: 探索正则黑洞的紫外几何修正对宇宙膨胀动力学的影响，特别是检验这些修正是否能解释当前的宇宙加速膨胀现象。

Method: 考虑多种正则黑洞几何方案，推导相应的Friedmann方程（仅包含尘埃和黑洞的宇宙），通过观测数据约束正则黑洞的紫外截断参数。

Result: 不同正则黑洞方案对宇宙膨胀有不同的影响，可被区分。最佳拟合参数值对应于正则无视界致密天体，而非传统黑洞。

Conclusion: 正则黑洞的紫外几何修正可能影响宇宙膨胀动力学，最佳拟合参数支持正则无视界致密天体，为解释宇宙加速膨胀提供了新视角。

Abstract: We consider the embedding of regular black holes in an expanding universe and study how the ultraviolet modifications to the Schwarzschild geometry that regularize the black hole singularity affect the exterior universe's expansion rate. We consider several proposals for the regular black hole geometry and obtain the corresponding Friedmann equations for a universe filled only with dust and black holes. We show that different proposals have different implications which may be distinguished. We then test the hypothesis that the UV corrections to the black hole geometry may be responsible for the current phase of accelerated expansion. To this aim we constrain the value of the regular black hole UV cutoff parameter from observations. Interestingly we find that the best fit is obtained by values of the parameter corresponding to regular horizonless compact objects.

</details>


### [362] [Covariant eigenmode overlap formalism for gravitational wave signals in electromagnetic cavities](https://arxiv.org/abs/2602.08507)
*Jordan Gué,Tom Krokotsch,Gudrid Moortgat-Pick*

Main category: gr-qc

TL;DR: 提出了一个坐标不变的形式体系，用于描述引力波与各类谐振探测器之间的机械和电磁相互作用，特别适用于使用微波腔的高频引力波实验。


<details>
  <summary>Details</summary>
Motivation: 需要建立一个通用的理论框架来描述引力波与谐振探测器（特别是微波腔）的相互作用，该框架应能处理复杂的几何形状、阻尼效应和电磁反作用。

Method: 开发了坐标不变的形式体系，求解了引力波修正的电磁学和弹性方程，采用本征模展开方法处理动态边界条件，并协变地考虑了阻尼效应和电磁反作用。

Result: 得到了适用于任意探测器几何形状的耦合系数，这些系数特别有利于高频引力波实验，并允许直接的数值实现。

Conclusion: 该形式体系为分析引力波与谐振探测器的相互作用提供了一个强大而灵活的工具，特别适用于微波腔等高频实验的数值模拟和设计优化。

Abstract: We develop a coordinate invariant formalism which describes the mechanical and electromagnetic interaction of gravitational waves (GWs) with a wide class of resonant detectors. We solve the GW-modified equations of electrodynamics and elasticity with dynamic boundary conditions using an eigenmode expansion. Furthermore, we take damping effects and electromagnetic back-action on mechanical systems covariantly into account. The resulting coupling coefficients are particularly useful for high-frequency gravitational wave experiments using microwave cavities and allow a straightforward numerical implementation for arbitrary detector geometries.

</details>


### [363] [Thermal Vacuum Cosmology Explains Hubble Tension](https://arxiv.org/abs/2602.08522)
*Robert Alicki*

Main category: gr-qc

TL;DR: 该论文提出用膨胀真空的吉本斯-霍金温度热能量替代宇宙常数，从而解释哈勃张力的起源


<details>
  <summary>Details</summary>
Motivation: 解决标准平坦暴胀ΛCDM模型中存在的"哈勃张力"问题，即早期宇宙测量与晚期宇宙测量得到的哈勃常数不一致的矛盾

Method: 修改标准平坦暴胀ΛCDM模型，用膨胀真空的吉本斯-霍金温度热能量替代宇宙常数Λ

Result: 该修改能够解释哈勃张力的起源，为这一长期存在的宇宙学问题提供了可能的解决方案

Conclusion: 通过用膨胀真空的热能量替代宇宙常数，可以成功解释哈勃张力，这为宇宙学模型提供了新的修正方向

Abstract: It is argued that the previously proposed modification of the standard (flat) inflationary $ΛCDM$ model in which cosmological constant is replaced by thermal energy of expanding vacum, characterized by the Gibbons-Hawking temperature, explains the origin of notorious ``Hubble tension''.

</details>


### [364] [Dynamical System Analysis of FLRW Model in f(R,L,T) Theory](https://arxiv.org/abs/2602.08562)
*R. R. Panchal,Divya G. Sanjava,A. H. Hasmani*

Main category: gr-qc

TL;DR: 本文研究了f(R,L,T)引力理论在标量场指数势下的动力学行为，通过相空间分析探索宇宙演化，包括物质主导、辐射主导和加速膨胀阶段。


<details>
  <summary>Details</summary>
Motivation: 修正引力理论作为广义相对论的替代方案，被广泛研究以解决暗能量和晚期宇宙加速膨胀等宇宙学问题。本文旨在探索f(R,L,T)引力模型在标量场作用下的动力学行为，以理解宇宙演化过程。

Method: 采用f(R,L,T)=R+αL+βT的特定形式引力模型，结合指数势标量场，通过相空间分析方法研究宇宙模型的动力学行为和晚期演化。评估临界点处的重要宇宙学参数，包括各宇宙成分的密度参数、减速参数和有效状态方程参数。

Result: 通过相空间分析，描述了宇宙演化中的不同阶段：物质主导时期、辐射主导时期和加速膨胀时期。这些阶段通过密度参数、减速参数和状态方程参数等关键宇宙学量进行表征。

Conclusion: f(R,L,T)引力模型结合标量场指数势能够描述宇宙演化的不同阶段，包括晚期加速膨胀，为解决宇宙学问题提供了理论框架。相空间分析为理解该模型的动力学行为提供了系统方法。

Abstract: Modified gravity theories have been extensively studied recently as viable substitutes for general relativity to deal with cosmological issues like dark energy and late-time cosmic acceleration. In the present work, we investigate the dynamical behavior of the $f(R,L,T)$ gravity model with a scalar field utilizing exponential potential, where $R$ represents the Ricci scalar, $L$ is the Lagrangian density and $T$ is the trace of the energy-momentum tensor. We concentrate on a specific type of modified gravity characterized by $f(R,L,T) =R+αL+βT$, where $α$ and $β$ are positive constants. We study the dynamical behavior and late-time evolution of a cosmological model using a thorough phase-space analysis. We assess important cosmological parameters at the critical places, such as the density parameters corresponding to various cosmic components, the deceleration parameter, and the effective equation of state parameter. The nature of the cosmic phases such as matter-dominated, radiation-dominated, and accelerated expansion eras, described using these quantities.

</details>


### [365] [Post-Newtonian accelerations of a Mercury orbiter](https://arxiv.org/abs/2602.08720)
*Miriam Falletta,Gabriel Rodríguez-Moris,Sergei A. Klioner*

Main category: gr-qc

TL;DR: 研究水星轨道器在局部坐标系中的相对论运动模型，分析BepiColombo任务中通常被忽略的相对论第三体扰动项的重要性


<details>
  <summary>Details</summary>
Motivation: 水星的后牛顿引力场（以及其他行星）可以用多极矩展开，这些多极矩最适当地定义在局部参考系中。目前在实际应用中通常忽略相对论第三体扰动，需要评估这些项对水星轨道器精度的影响

Method: 在水星中心局部坐标系中建立运动方程，包括相对论局部扰动（史瓦西项、Lense-Thirring进动、四极矩加速度）和相对论第三体扰动（引力电和引力磁加速度，以及水星与其他太阳系天体的耦合项）。沿BepiColombo两个航天器的轨迹评估这些后牛顿项的大小

Result: 通过分析BepiColombo航天器轨迹上的后牛顿项大小，评估了通常被忽略的相对论第三体扰动的重要性，为构建高精度水星轨道器相对论轨道模型提供依据

Conclusion: 为构建适合水星轨道器的高精度相对论轨道模型提供了实用方法，明确了需要考虑的相对论项，特别是评估了第三体扰动在实际任务中的重要性

Abstract: We investigate the relativistic modeling of spacecraft motion in Mercury's post-Newtonian local coordinates. This investigation is motivated by the fact that Mercury's post-Newtonian gravitational field (as well as that of any other planet) admits an expansion in terms of multipole moments, which are most appropriately defined in the local reference system. The equations of motion in the Mercury-centric local frame include relativistic local perturbations, given by the Schwarzschild term, Lense-Thirring precession, and the acceleration due to the quadrupole moment, and relativistic third-body perturbations, which are the gravito-electric and gravito-magnetic accelerations, along with a coupling term between Mercury and other solar system bodies. The relativistic third-body perturbations are usually neglected in all practical applications. In this study, we analyze the magnitude of the post-Newtonian terms of the equations of motion formulated in the Mercury-centric frame, evaluating them along the trajectories of the two BepiColombo spacecrafts. Based on this analysis, we provide a practical approach for constructing a high-accuracy relativistic orbital model suitable for a Mercury orbiter.

</details>


### [366] [From the confluent Heun equation to a new factorized and resummed gravitational waveform for circularized, nonspinning, compact binaries](https://arxiv.org/abs/2602.08833)
*Andrea Cipriani,Alessandro Nagar,Francesco Fucito,José Francisco Morales*

Main category: gr-qc

TL;DR: 提出一种基于Teukolsky方程映射到合流Heun方程的新因子化重求和波形，能吸收所有测试质量对数项和超越数，在测试质量极限下比传统因子化方法更精确。


<details>
  <summary>Details</summary>
Motivation: 改进当前有效单体波形模型，通过新的因子化重求和方法提高波形和通量计算的精度，特别是吸收测试质量对数项和超越数。

Method: 将Teukolsky方程映射到合流Heun方程，利用解的结构识别新的重求和因子，通过指数和Γ函数吸收所有测试质量对数项和超越数，剩余相对论和相位修正为有理系数多项式。

Result: 在测试质量极限下（计算至10PN），波形和通量比Damour等人的标准因子化方法更精确；从第一性原理恢复了多极矩的重整化群标度并确定了标度常数。

Conclusion: 该方法可推广到可比质量双星系统，实现多极矩的普适反常维度概念，有望改进当前最先进的有效单体波形模型。

Abstract: We introduce a new factorized and resummed waveform for circularized, nonspinning, compact binaries that leverages on the solution of the Teukolsky equation once mapped into a confluent Heun equation. The structure of the solution allows one to identify new resummed factors that completely absorb all test-mass logarithms and transcendental numbers via exponentials and $Γ$-functions at any post-Newtonian (PN) order. The corresponding residual relativistic and phase corrections are thus polynomial with rational coefficients, that are in fact PN-truncated hypergeometric functions. Our approach complements the recent proposal of Ivanov et al. [Phys. Rev. Lett. 135 (2025) 14, 141401], notably recovering the corresponding renormalization group scaling of multipole moments from first principles and fixing the scaling constant. In the test mass limit, our approach (pushed up to 10PN) yields waveforms and fluxes that are globally more accurate than those obtained using the standard factorized approach of Damour et al. [Phys. Rev. D 79 (2009), 064004]. The method generalizes straightforwardly to comparable mass binaries implementing the new concept of universal anomalous dimension of multipole moments and might be eventually useful to improve current state of the art effective-one-body waveform models for coalescing binaries.

</details>


### [367] [Conservative binary dynamics to third post-Minkowskian order beyond General Relativity](https://arxiv.org/abs/2602.08876)
*Gabriel Luz Almeida,Yuchen Du,Zhengwen Liu,Hongbin Wang*

Main category: gr-qc

TL;DR: 该论文在扩展广义相对论的理论中，研究了包含质量标量场与高斯-博内不变量耦合的紧凑双星系统的保守动力学，计算了到后闵可夫斯基近似三阶的散射冲量和偏转角。


<details>
  <summary>Details</summary>
Motivation: 研究扩展广义相对论的理论中紧凑双星系统的动力学，特别是在包含质量标量场与高斯-博内不变量耦合的理论框架下，理解高阶后闵可夫斯基近似下的引力相互作用。

Method: 采用有效场论方法，通过积分掉介导两个天体之间引力相互作用的度规和标量自由度，构建双星系统的有效作用量，推导后闵可夫斯基展开到三阶的解析表达式。

Result: 得到了散射冲量和偏转角到后闵可夫斯基三阶的解析表达式，结果与后牛顿/后闵可夫斯基理论中最先进的计算在重叠区域一致。

Conclusion: 成功在扩展广义相对论的理论中计算了紧凑双星系统到后闵可夫斯基三阶的保守动力学，验证了与现有理论的一致性，为理解标量-张量引力理论中的双星系统动力学提供了重要进展。

Abstract: We present the conservative dynamics of compact binaries to third order in the post-Minkowskian approximation in a theory that extends general relativity by a massless scalar field coupled to the Gauss-Bonnet invariant. We employ the effective field theory approach to construct the effective action of binary systems by integrating out the metric and scalar degrees of freedom that mediate the gravitational interactions between the two bodies. We derive analytical expressions for the scattering impulse and the deflection angle to third order in the post-Minkowskian expansion. Our results are found to be in agreement, in the overlapping regimes, with state-of-the-art calculations in the post-Newtonian/post-Minkowskian theory.

</details>


### [368] [Dynamics, Ringdown, and Accretion-Driven Multiple Quasi-Periodic Oscillations of Kerr-Bertotti-Robinson Black Holes](https://arxiv.org/abs/2602.08911)
*G. Mustafa,Orhan Donmez,Dhruba Jyoti Gogoi,Sushant G. Ghosh,Ibrar Hussain,Chengxun Yuan*

Main category: gr-qc

TL;DR: 研究Kerr-Bertotti-Robinson黑洞周围测试粒子的运动，分析质量M、旋转参数a和磁参数B对粒子动力学的影响，包括稳定轨道、振荡频率、准周期振荡，并与Kerr黑洞比较。


<details>
  <summary>Details</summary>
Motivation: 研究磁场如何影响旋转黑洞周围的粒子动力学，探索准周期振荡的物理机制，为观测到的X射线双星中的多频准周期振荡提供统一解释。

Method: 推导稳定赤道圆轨道的能量和角动量解析表达式，计算径向和纬度振荡频率，使用WKB方法研究标量准正规模，建立BHL吸积的广义相对论模型进行数值模拟。

Result: 黑洞参数强烈影响粒子运动，磁场增加准正规模的阻尼，旋转和角动量主要决定振荡频率。BHL吸积产生两种周期性转换的物理结构，产生低频和高频准周期振荡。

Conclusion: KBR黑洞的磁场和旋转参数共同塑造粒子动力学，产生的准周期振荡模式为观测到的X射线双星多频振荡提供了统一的理论解释框架。

Abstract: We study the motion of test particles around Kerr--Bertotti--Robinson (KBR) black hole (BH) and explore how the three defining parameters the mass $M$, rotation parameter $a$, and magnetic parameter $B$ influence their dynamics. We derive analytical expressions for the energy and angular momentum of stable equatorial circular orbits, along with the corresponding radial and latitudinal oscillation frequencies, as functions of $M$, $a$, and $B$. We also examine the key features of the quasi-periodic oscillations of test particles near stable circular orbits, including the precession effects such as periastron precession and the Lense-Thirring effect. Finally, we compare our results with those corresponding to the Kerr BH. We find that particle motion is strongly shaped by the BH parameters. Using a WKB approach, we also study scalar quasinormal modes of a rotating KBR BH in an external magnetic field and show that the magnetic field increases damping, while rotation and angular momentum mainly set the oscillation frequencies. Alternatively, general relativistic modelling of Bondi-Hoyle-Lyttleton (BHL) accretion onto a rapidly rotating KBR BH shows that two distinct physical structures emerge and cyclically transform into one another over time. These processes produce either a strongly oscillating flip-flop shock cone or a nearly stationary toroidal structure, with their formation governed by the black hole spin and magnetic curvature. Power spectral analysis shows that these configurations give rise to low and high-frequency quasi-periodic oscillations, offering a unified explanation for the multiple quasi-periodic oscillations observed in rapidly spinning X--ray binaries.

</details>


### [369] [Cyclic universe from uniform rate inflation on the brane with a timelike extra dimension](https://arxiv.org/abs/2602.08974)
*Rikpratik Sengupta,Arkajit Aich,Kaushik Bhattacharya*

Main category: gr-qc

TL;DR: 在Shtanov-Sahni膜世界上实现均匀速率暴胀的非奇异宇宙学模型，通过额外维度修正解决初始奇点，实现无限次平滑反弹，同时保持各向异性背景稳定。


<details>
  <summary>Details</summary>
Motivation: 解决标准暴胀宇宙学中的初始奇点问题，将非奇异早期宇宙动力学与精确宇宙学统一起来，探索各向异性膜世界框架下暴胀的可行性。

Method: 在Shtanov-Sahni膜世界上构建各向异性背景，引入类时额外维度产生高能修正，实现均匀速率暴胀。使用δN形式分析原初标量和张量扰动，确保只有暴胀期间退出视界的物理相关模式贡献可观测量。

Result: 模型自然解决了初始奇点，产生无限次平滑非奇异反弹。各向异性剪切在反弹附近被动态抑制，背景演化稳定。观测一致性可在不同各向异性水平下实现，不损害反弹的平滑性或稳定性。

Conclusion: 各向异性膜世界上的均匀速率暴胀是标准暴胀宇宙学的稳健且观测可行的替代方案，为非奇异早期宇宙动力学与精确宇宙学的统一提供了有力框架。

Abstract: We investigate a non-singular cosmological scenario in which uniform-rate inflation is realised on an anisotropic Shtanov-Sahni braneworld. The model naturally resolves the initial singularity resulting in an infinite number of smooth non-singular bounces, while accommodating a phase of accelerated expansion driven by a scalar field rolling at a constant rate. The presence of a timelike extra dimension induces high-energy corrections to the effective Friedmann dynamics, allowing anisotropic shear to be dynamically suppressed near the bounce and rendering the background evolution stable. We derive the full background dynamics analytically and demonstrate that uniform-rate inflation can be consistently embedded within an anisotropic braneworld framework. Primordial scalar and tensor perturbations are analysed using the $δN$ formalism, ensuring that only physically relevant modes exiting the horizon during inflation contribute to observable quantities. Remarkably, we find that observational consistency can be achieved with different levels of anisotropy in the two different scenarios we consider, without compromising the smoothness or stability of the bounce. Our results establish uniform-rate inflation on an anisotropic braneworld as a robust and observationally viable alternative to standard inflationary cosmology, offering a compelling framework in which non-singular early-universe dynamics and precision cosmology can be consistently unified.

</details>
